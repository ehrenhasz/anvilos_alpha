{
  "module_name": "cacheinfo.c",
  "hash_id": "5384bb8d15d09992e2b552637db3715672f074bfa7ddfa419ad109473f33d9b7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/base/cacheinfo.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/acpi.h>\n#include <linux/bitops.h>\n#include <linux/cacheinfo.h>\n#include <linux/compiler.h>\n#include <linux/cpu.h>\n#include <linux/device.h>\n#include <linux/init.h>\n#include <linux/of.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/smp.h>\n#include <linux/sysfs.h>\n\n \nstatic DEFINE_PER_CPU(struct cpu_cacheinfo, ci_cpu_cacheinfo);\n#define ci_cacheinfo(cpu)\t(&per_cpu(ci_cpu_cacheinfo, cpu))\n#define cache_leaves(cpu)\t(ci_cacheinfo(cpu)->num_leaves)\n#define per_cpu_cacheinfo(cpu)\t(ci_cacheinfo(cpu)->info_list)\n#define per_cpu_cacheinfo_idx(cpu, idx)\t\t\\\n\t\t\t\t(per_cpu_cacheinfo(cpu) + (idx))\n\n \nstatic bool use_arch_info;\n\nstruct cpu_cacheinfo *get_cpu_cacheinfo(unsigned int cpu)\n{\n\treturn ci_cacheinfo(cpu);\n}\n\nstatic inline bool cache_leaves_are_shared(struct cacheinfo *this_leaf,\n\t\t\t\t\t   struct cacheinfo *sib_leaf)\n{\n\t \n\tif (!(IS_ENABLED(CONFIG_OF) || IS_ENABLED(CONFIG_ACPI)) ||\n\t    use_arch_info)\n\t\treturn (this_leaf->level != 1) && (sib_leaf->level != 1);\n\n\tif ((sib_leaf->attributes & CACHE_ID) &&\n\t    (this_leaf->attributes & CACHE_ID))\n\t\treturn sib_leaf->id == this_leaf->id;\n\n\treturn sib_leaf->fw_token == this_leaf->fw_token;\n}\n\nbool last_level_cache_is_valid(unsigned int cpu)\n{\n\tstruct cacheinfo *llc;\n\n\tif (!cache_leaves(cpu))\n\t\treturn false;\n\n\tllc = per_cpu_cacheinfo_idx(cpu, cache_leaves(cpu) - 1);\n\n\treturn (llc->attributes & CACHE_ID) || !!llc->fw_token;\n\n}\n\nbool last_level_cache_is_shared(unsigned int cpu_x, unsigned int cpu_y)\n{\n\tstruct cacheinfo *llc_x, *llc_y;\n\n\tif (!last_level_cache_is_valid(cpu_x) ||\n\t    !last_level_cache_is_valid(cpu_y))\n\t\treturn false;\n\n\tllc_x = per_cpu_cacheinfo_idx(cpu_x, cache_leaves(cpu_x) - 1);\n\tllc_y = per_cpu_cacheinfo_idx(cpu_y, cache_leaves(cpu_y) - 1);\n\n\treturn cache_leaves_are_shared(llc_x, llc_y);\n}\n\n#ifdef CONFIG_OF\n\nstatic bool of_check_cache_nodes(struct device_node *np);\n\n \nstruct cache_type_info {\n\tconst char *size_prop;\n\tconst char *line_size_props[2];\n\tconst char *nr_sets_prop;\n};\n\nstatic const struct cache_type_info cache_type_info[] = {\n\t{\n\t\t.size_prop       = \"cache-size\",\n\t\t.line_size_props = { \"cache-line-size\",\n\t\t\t\t     \"cache-block-size\", },\n\t\t.nr_sets_prop    = \"cache-sets\",\n\t}, {\n\t\t.size_prop       = \"i-cache-size\",\n\t\t.line_size_props = { \"i-cache-line-size\",\n\t\t\t\t     \"i-cache-block-size\", },\n\t\t.nr_sets_prop    = \"i-cache-sets\",\n\t}, {\n\t\t.size_prop       = \"d-cache-size\",\n\t\t.line_size_props = { \"d-cache-line-size\",\n\t\t\t\t     \"d-cache-block-size\", },\n\t\t.nr_sets_prop    = \"d-cache-sets\",\n\t},\n};\n\nstatic inline int get_cacheinfo_idx(enum cache_type type)\n{\n\tif (type == CACHE_TYPE_UNIFIED)\n\t\treturn 0;\n\treturn type;\n}\n\nstatic void cache_size(struct cacheinfo *this_leaf, struct device_node *np)\n{\n\tconst char *propname;\n\tint ct_idx;\n\n\tct_idx = get_cacheinfo_idx(this_leaf->type);\n\tpropname = cache_type_info[ct_idx].size_prop;\n\n\tof_property_read_u32(np, propname, &this_leaf->size);\n}\n\n \nstatic void cache_get_line_size(struct cacheinfo *this_leaf,\n\t\t\t\tstruct device_node *np)\n{\n\tint i, lim, ct_idx;\n\n\tct_idx = get_cacheinfo_idx(this_leaf->type);\n\tlim = ARRAY_SIZE(cache_type_info[ct_idx].line_size_props);\n\n\tfor (i = 0; i < lim; i++) {\n\t\tint ret;\n\t\tu32 line_size;\n\t\tconst char *propname;\n\n\t\tpropname = cache_type_info[ct_idx].line_size_props[i];\n\t\tret = of_property_read_u32(np, propname, &line_size);\n\t\tif (!ret) {\n\t\t\tthis_leaf->coherency_line_size = line_size;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void cache_nr_sets(struct cacheinfo *this_leaf, struct device_node *np)\n{\n\tconst char *propname;\n\tint ct_idx;\n\n\tct_idx = get_cacheinfo_idx(this_leaf->type);\n\tpropname = cache_type_info[ct_idx].nr_sets_prop;\n\n\tof_property_read_u32(np, propname, &this_leaf->number_of_sets);\n}\n\nstatic void cache_associativity(struct cacheinfo *this_leaf)\n{\n\tunsigned int line_size = this_leaf->coherency_line_size;\n\tunsigned int nr_sets = this_leaf->number_of_sets;\n\tunsigned int size = this_leaf->size;\n\n\t \n\tif (!(nr_sets == 1) && (nr_sets > 0 && size > 0 && line_size > 0))\n\t\tthis_leaf->ways_of_associativity = (size / nr_sets) / line_size;\n}\n\nstatic bool cache_node_is_unified(struct cacheinfo *this_leaf,\n\t\t\t\t  struct device_node *np)\n{\n\treturn of_property_read_bool(np, \"cache-unified\");\n}\n\nstatic void cache_of_set_props(struct cacheinfo *this_leaf,\n\t\t\t       struct device_node *np)\n{\n\t \n\tif (this_leaf->type == CACHE_TYPE_NOCACHE &&\n\t    cache_node_is_unified(this_leaf, np))\n\t\tthis_leaf->type = CACHE_TYPE_UNIFIED;\n\tcache_size(this_leaf, np);\n\tcache_get_line_size(this_leaf, np);\n\tcache_nr_sets(this_leaf, np);\n\tcache_associativity(this_leaf);\n}\n\nstatic int cache_setup_of_node(unsigned int cpu)\n{\n\tstruct device_node *np, *prev;\n\tstruct cacheinfo *this_leaf;\n\tunsigned int index = 0;\n\n\tnp = of_cpu_device_node_get(cpu);\n\tif (!np) {\n\t\tpr_err(\"Failed to find cpu%d device node\\n\", cpu);\n\t\treturn -ENOENT;\n\t}\n\n\tif (!of_check_cache_nodes(np)) {\n\t\tof_node_put(np);\n\t\treturn -ENOENT;\n\t}\n\n\tprev = np;\n\n\twhile (index < cache_leaves(cpu)) {\n\t\tthis_leaf = per_cpu_cacheinfo_idx(cpu, index);\n\t\tif (this_leaf->level != 1) {\n\t\t\tnp = of_find_next_cache_node(np);\n\t\t\tof_node_put(prev);\n\t\t\tprev = np;\n\t\t\tif (!np)\n\t\t\t\tbreak;\n\t\t}\n\t\tcache_of_set_props(this_leaf, np);\n\t\tthis_leaf->fw_token = np;\n\t\tindex++;\n\t}\n\n\tof_node_put(np);\n\n\tif (index != cache_leaves(cpu))  \n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic bool of_check_cache_nodes(struct device_node *np)\n{\n\tstruct device_node *next;\n\n\tif (of_property_present(np, \"cache-size\")   ||\n\t    of_property_present(np, \"i-cache-size\") ||\n\t    of_property_present(np, \"d-cache-size\") ||\n\t    of_property_present(np, \"cache-unified\"))\n\t\treturn true;\n\n\tnext = of_find_next_cache_node(np);\n\tif (next) {\n\t\tof_node_put(next);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int of_count_cache_leaves(struct device_node *np)\n{\n\tunsigned int leaves = 0;\n\n\tif (of_property_read_bool(np, \"cache-size\"))\n\t\t++leaves;\n\tif (of_property_read_bool(np, \"i-cache-size\"))\n\t\t++leaves;\n\tif (of_property_read_bool(np, \"d-cache-size\"))\n\t\t++leaves;\n\n\tif (!leaves) {\n\t\t \n\t\tif (of_property_read_bool(np, \"cache-unified\"))\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn 2;\n\t}\n\n\treturn leaves;\n}\n\nint init_of_cache_level(unsigned int cpu)\n{\n\tstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\n\tstruct device_node *np = of_cpu_device_node_get(cpu);\n\tstruct device_node *prev = NULL;\n\tunsigned int levels = 0, leaves, level;\n\n\tif (!of_check_cache_nodes(np)) {\n\t\tof_node_put(np);\n\t\treturn -ENOENT;\n\t}\n\n\tleaves = of_count_cache_leaves(np);\n\tif (leaves > 0)\n\t\tlevels = 1;\n\n\tprev = np;\n\twhile ((np = of_find_next_cache_node(np))) {\n\t\tof_node_put(prev);\n\t\tprev = np;\n\t\tif (!of_device_is_compatible(np, \"cache\"))\n\t\t\tgoto err_out;\n\t\tif (of_property_read_u32(np, \"cache-level\", &level))\n\t\t\tgoto err_out;\n\t\tif (level <= levels)\n\t\t\tgoto err_out;\n\n\t\tleaves += of_count_cache_leaves(np);\n\t\tlevels = level;\n\t}\n\n\tof_node_put(np);\n\tthis_cpu_ci->num_levels = levels;\n\tthis_cpu_ci->num_leaves = leaves;\n\n\treturn 0;\n\nerr_out:\n\tof_node_put(np);\n\treturn -EINVAL;\n}\n\n#else\nstatic inline int cache_setup_of_node(unsigned int cpu) { return 0; }\nint init_of_cache_level(unsigned int cpu) { return 0; }\n#endif\n\nint __weak cache_setup_acpi(unsigned int cpu)\n{\n\treturn -ENOTSUPP;\n}\n\nunsigned int coherency_max_size;\n\nstatic int cache_setup_properties(unsigned int cpu)\n{\n\tint ret = 0;\n\n\tif (of_have_populated_dt())\n\t\tret = cache_setup_of_node(cpu);\n\telse if (!acpi_disabled)\n\t\tret = cache_setup_acpi(cpu);\n\n\t \n\tif (ret && use_arch_cache_info())\n\t\tuse_arch_info = true;\n\n\treturn ret;\n}\n\nstatic int cache_shared_cpu_map_setup(unsigned int cpu)\n{\n\tstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\n\tstruct cacheinfo *this_leaf, *sib_leaf;\n\tunsigned int index, sib_index;\n\tint ret = 0;\n\n\tif (this_cpu_ci->cpu_map_populated)\n\t\treturn 0;\n\n\t \n\tif (!last_level_cache_is_valid(cpu) && !use_arch_info) {\n\t\tret = cache_setup_properties(cpu);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor (index = 0; index < cache_leaves(cpu); index++) {\n\t\tunsigned int i;\n\n\t\tthis_leaf = per_cpu_cacheinfo_idx(cpu, index);\n\n\t\tcpumask_set_cpu(cpu, &this_leaf->shared_cpu_map);\n\t\tfor_each_online_cpu(i) {\n\t\t\tstruct cpu_cacheinfo *sib_cpu_ci = get_cpu_cacheinfo(i);\n\n\t\t\tif (i == cpu || !sib_cpu_ci->info_list)\n\t\t\t\tcontinue; \n\t\t\tfor (sib_index = 0; sib_index < cache_leaves(i); sib_index++) {\n\t\t\t\tsib_leaf = per_cpu_cacheinfo_idx(i, sib_index);\n\n\t\t\t\t \n\t\t\t\tif (sib_leaf->level != this_leaf->level ||\n\t\t\t\t    sib_leaf->type != this_leaf->type)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (cache_leaves_are_shared(this_leaf, sib_leaf)) {\n\t\t\t\t\tcpumask_set_cpu(cpu, &sib_leaf->shared_cpu_map);\n\t\t\t\t\tcpumask_set_cpu(i, &this_leaf->shared_cpu_map);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (this_leaf->coherency_line_size > coherency_max_size)\n\t\t\tcoherency_max_size = this_leaf->coherency_line_size;\n\t}\n\n\t \n\tthis_cpu_ci->cpu_map_populated = true;\n\treturn 0;\n}\n\nstatic void cache_shared_cpu_map_remove(unsigned int cpu)\n{\n\tstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\n\tstruct cacheinfo *this_leaf, *sib_leaf;\n\tunsigned int sibling, index, sib_index;\n\n\tfor (index = 0; index < cache_leaves(cpu); index++) {\n\t\tthis_leaf = per_cpu_cacheinfo_idx(cpu, index);\n\t\tfor_each_cpu(sibling, &this_leaf->shared_cpu_map) {\n\t\t\tstruct cpu_cacheinfo *sib_cpu_ci =\n\t\t\t\t\t\tget_cpu_cacheinfo(sibling);\n\n\t\t\tif (sibling == cpu || !sib_cpu_ci->info_list)\n\t\t\t\tcontinue; \n\n\t\t\tfor (sib_index = 0; sib_index < cache_leaves(sibling); sib_index++) {\n\t\t\t\tsib_leaf = per_cpu_cacheinfo_idx(sibling, sib_index);\n\n\t\t\t\t \n\t\t\t\tif (sib_leaf->level != this_leaf->level ||\n\t\t\t\t    sib_leaf->type != this_leaf->type)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (cache_leaves_are_shared(this_leaf, sib_leaf)) {\n\t\t\t\t\tcpumask_clear_cpu(cpu, &sib_leaf->shared_cpu_map);\n\t\t\t\t\tcpumask_clear_cpu(sibling, &this_leaf->shared_cpu_map);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tthis_cpu_ci->cpu_map_populated = false;\n}\n\nstatic void free_cache_attributes(unsigned int cpu)\n{\n\tif (!per_cpu_cacheinfo(cpu))\n\t\treturn;\n\n\tcache_shared_cpu_map_remove(cpu);\n}\n\nint __weak early_cache_level(unsigned int cpu)\n{\n\treturn -ENOENT;\n}\n\nint __weak init_cache_level(unsigned int cpu)\n{\n\treturn -ENOENT;\n}\n\nint __weak populate_cache_leaves(unsigned int cpu)\n{\n\treturn -ENOENT;\n}\n\nstatic inline\nint allocate_cache_info(int cpu)\n{\n\tper_cpu_cacheinfo(cpu) = kcalloc(cache_leaves(cpu),\n\t\t\t\t\t sizeof(struct cacheinfo), GFP_ATOMIC);\n\tif (!per_cpu_cacheinfo(cpu)) {\n\t\tcache_leaves(cpu) = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint fetch_cache_info(unsigned int cpu)\n{\n\tstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\n\tunsigned int levels = 0, split_levels = 0;\n\tint ret;\n\n\tif (acpi_disabled) {\n\t\tret = init_of_cache_level(cpu);\n\t} else {\n\t\tret = acpi_get_cache_info(cpu, &levels, &split_levels);\n\t\tif (!ret) {\n\t\t\tthis_cpu_ci->num_levels = levels;\n\t\t\t \n\t\t\tthis_cpu_ci->num_leaves = levels + split_levels;\n\t\t}\n\t}\n\n\tif (ret || !cache_leaves(cpu)) {\n\t\tret = early_cache_level(cpu);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!cache_leaves(cpu))\n\t\t\treturn -ENOENT;\n\n\t\tthis_cpu_ci->early_ci_levels = true;\n\t}\n\n\treturn allocate_cache_info(cpu);\n}\n\nstatic inline int init_level_allocate_ci(unsigned int cpu)\n{\n\tunsigned int early_leaves = cache_leaves(cpu);\n\n\t \n\tif (per_cpu_cacheinfo(cpu) && !ci_cacheinfo(cpu)->early_ci_levels)\n\t\treturn 0;\n\n\tif (init_cache_level(cpu) || !cache_leaves(cpu))\n\t\treturn -ENOENT;\n\n\t \n\tci_cacheinfo(cpu)->early_ci_levels = false;\n\n\tif (cache_leaves(cpu) <= early_leaves)\n\t\treturn 0;\n\n\tkfree(per_cpu_cacheinfo(cpu));\n\treturn allocate_cache_info(cpu);\n}\n\nint detect_cache_attributes(unsigned int cpu)\n{\n\tint ret;\n\n\tret = init_level_allocate_ci(cpu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!last_level_cache_is_valid(cpu)) {\n\t\t \n\t\tret = populate_cache_leaves(cpu);\n\t\tif (ret)\n\t\t\tgoto free_ci;\n\t}\n\n\t \n\tret = cache_shared_cpu_map_setup(cpu);\n\tif (ret) {\n\t\tpr_warn(\"Unable to detect cache hierarchy for CPU %d\\n\", cpu);\n\t\tgoto free_ci;\n\t}\n\n\treturn 0;\n\nfree_ci:\n\tfree_cache_attributes(cpu);\n\treturn ret;\n}\n\n \nstatic DEFINE_PER_CPU(struct device *, ci_cache_dev);\n#define per_cpu_cache_dev(cpu)\t(per_cpu(ci_cache_dev, cpu))\n\nstatic cpumask_t cache_dev_map;\n\n \nstatic DEFINE_PER_CPU(struct device **, ci_index_dev);\n#define per_cpu_index_dev(cpu)\t(per_cpu(ci_index_dev, cpu))\n#define per_cache_index_dev(cpu, idx)\t((per_cpu_index_dev(cpu))[idx])\n\n#define show_one(file_name, object)\t\t\t\t\\\nstatic ssize_t file_name##_show(struct device *dev,\t\t\\\n\t\tstruct device_attribute *attr, char *buf)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\t\\\n\treturn sysfs_emit(buf, \"%u\\n\", this_leaf->object);\t\\\n}\n\nshow_one(id, id);\nshow_one(level, level);\nshow_one(coherency_line_size, coherency_line_size);\nshow_one(number_of_sets, number_of_sets);\nshow_one(physical_line_partition, physical_line_partition);\nshow_one(ways_of_associativity, ways_of_associativity);\n\nstatic ssize_t size_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\n\treturn sysfs_emit(buf, \"%uK\\n\", this_leaf->size >> 10);\n}\n\nstatic ssize_t shared_cpu_map_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tconst struct cpumask *mask = &this_leaf->shared_cpu_map;\n\n\treturn sysfs_emit(buf, \"%*pb\\n\", nr_cpu_ids, mask);\n}\n\nstatic ssize_t shared_cpu_list_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tconst struct cpumask *mask = &this_leaf->shared_cpu_map;\n\n\treturn sysfs_emit(buf, \"%*pbl\\n\", nr_cpu_ids, mask);\n}\n\nstatic ssize_t type_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tconst char *output;\n\n\tswitch (this_leaf->type) {\n\tcase CACHE_TYPE_DATA:\n\t\toutput = \"Data\";\n\t\tbreak;\n\tcase CACHE_TYPE_INST:\n\t\toutput = \"Instruction\";\n\t\tbreak;\n\tcase CACHE_TYPE_UNIFIED:\n\t\toutput = \"Unified\";\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn sysfs_emit(buf, \"%s\\n\", output);\n}\n\nstatic ssize_t allocation_policy_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tunsigned int ci_attr = this_leaf->attributes;\n\tconst char *output;\n\n\tif ((ci_attr & CACHE_READ_ALLOCATE) && (ci_attr & CACHE_WRITE_ALLOCATE))\n\t\toutput = \"ReadWriteAllocate\";\n\telse if (ci_attr & CACHE_READ_ALLOCATE)\n\t\toutput = \"ReadAllocate\";\n\telse if (ci_attr & CACHE_WRITE_ALLOCATE)\n\t\toutput = \"WriteAllocate\";\n\telse\n\t\treturn 0;\n\n\treturn sysfs_emit(buf, \"%s\\n\", output);\n}\n\nstatic ssize_t write_policy_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tunsigned int ci_attr = this_leaf->attributes;\n\tint n = 0;\n\n\tif (ci_attr & CACHE_WRITE_THROUGH)\n\t\tn = sysfs_emit(buf, \"WriteThrough\\n\");\n\telse if (ci_attr & CACHE_WRITE_BACK)\n\t\tn = sysfs_emit(buf, \"WriteBack\\n\");\n\treturn n;\n}\n\nstatic DEVICE_ATTR_RO(id);\nstatic DEVICE_ATTR_RO(level);\nstatic DEVICE_ATTR_RO(type);\nstatic DEVICE_ATTR_RO(coherency_line_size);\nstatic DEVICE_ATTR_RO(ways_of_associativity);\nstatic DEVICE_ATTR_RO(number_of_sets);\nstatic DEVICE_ATTR_RO(size);\nstatic DEVICE_ATTR_RO(allocation_policy);\nstatic DEVICE_ATTR_RO(write_policy);\nstatic DEVICE_ATTR_RO(shared_cpu_map);\nstatic DEVICE_ATTR_RO(shared_cpu_list);\nstatic DEVICE_ATTR_RO(physical_line_partition);\n\nstatic struct attribute *cache_default_attrs[] = {\n\t&dev_attr_id.attr,\n\t&dev_attr_type.attr,\n\t&dev_attr_level.attr,\n\t&dev_attr_shared_cpu_map.attr,\n\t&dev_attr_shared_cpu_list.attr,\n\t&dev_attr_coherency_line_size.attr,\n\t&dev_attr_ways_of_associativity.attr,\n\t&dev_attr_number_of_sets.attr,\n\t&dev_attr_size.attr,\n\t&dev_attr_allocation_policy.attr,\n\t&dev_attr_write_policy.attr,\n\t&dev_attr_physical_line_partition.attr,\n\tNULL\n};\n\nstatic umode_t\ncache_default_attrs_is_visible(struct kobject *kobj,\n\t\t\t       struct attribute *attr, int unused)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\n\tconst struct cpumask *mask = &this_leaf->shared_cpu_map;\n\tumode_t mode = attr->mode;\n\n\tif ((attr == &dev_attr_id.attr) && (this_leaf->attributes & CACHE_ID))\n\t\treturn mode;\n\tif ((attr == &dev_attr_type.attr) && this_leaf->type)\n\t\treturn mode;\n\tif ((attr == &dev_attr_level.attr) && this_leaf->level)\n\t\treturn mode;\n\tif ((attr == &dev_attr_shared_cpu_map.attr) && !cpumask_empty(mask))\n\t\treturn mode;\n\tif ((attr == &dev_attr_shared_cpu_list.attr) && !cpumask_empty(mask))\n\t\treturn mode;\n\tif ((attr == &dev_attr_coherency_line_size.attr) &&\n\t    this_leaf->coherency_line_size)\n\t\treturn mode;\n\tif ((attr == &dev_attr_ways_of_associativity.attr) &&\n\t    this_leaf->size)  \n\t\treturn mode;\n\tif ((attr == &dev_attr_number_of_sets.attr) &&\n\t    this_leaf->number_of_sets)\n\t\treturn mode;\n\tif ((attr == &dev_attr_size.attr) && this_leaf->size)\n\t\treturn mode;\n\tif ((attr == &dev_attr_write_policy.attr) &&\n\t    (this_leaf->attributes & CACHE_WRITE_POLICY_MASK))\n\t\treturn mode;\n\tif ((attr == &dev_attr_allocation_policy.attr) &&\n\t    (this_leaf->attributes & CACHE_ALLOCATE_POLICY_MASK))\n\t\treturn mode;\n\tif ((attr == &dev_attr_physical_line_partition.attr) &&\n\t    this_leaf->physical_line_partition)\n\t\treturn mode;\n\n\treturn 0;\n}\n\nstatic const struct attribute_group cache_default_group = {\n\t.attrs = cache_default_attrs,\n\t.is_visible = cache_default_attrs_is_visible,\n};\n\nstatic const struct attribute_group *cache_default_groups[] = {\n\t&cache_default_group,\n\tNULL,\n};\n\nstatic const struct attribute_group *cache_private_groups[] = {\n\t&cache_default_group,\n\tNULL,  \n\tNULL,\n};\n\nconst struct attribute_group *\n__weak cache_get_priv_group(struct cacheinfo *this_leaf)\n{\n\treturn NULL;\n}\n\nstatic const struct attribute_group **\ncache_get_attribute_groups(struct cacheinfo *this_leaf)\n{\n\tconst struct attribute_group *priv_group =\n\t\t\tcache_get_priv_group(this_leaf);\n\n\tif (!priv_group)\n\t\treturn cache_default_groups;\n\n\tif (!cache_private_groups[1])\n\t\tcache_private_groups[1] = priv_group;\n\n\treturn cache_private_groups;\n}\n\n \nstatic void cpu_cache_sysfs_exit(unsigned int cpu)\n{\n\tint i;\n\tstruct device *ci_dev;\n\n\tif (per_cpu_index_dev(cpu)) {\n\t\tfor (i = 0; i < cache_leaves(cpu); i++) {\n\t\t\tci_dev = per_cache_index_dev(cpu, i);\n\t\t\tif (!ci_dev)\n\t\t\t\tcontinue;\n\t\t\tdevice_unregister(ci_dev);\n\t\t}\n\t\tkfree(per_cpu_index_dev(cpu));\n\t\tper_cpu_index_dev(cpu) = NULL;\n\t}\n\tdevice_unregister(per_cpu_cache_dev(cpu));\n\tper_cpu_cache_dev(cpu) = NULL;\n}\n\nstatic int cpu_cache_sysfs_init(unsigned int cpu)\n{\n\tstruct device *dev = get_cpu_device(cpu);\n\n\tif (per_cpu_cacheinfo(cpu) == NULL)\n\t\treturn -ENOENT;\n\n\tper_cpu_cache_dev(cpu) = cpu_device_create(dev, NULL, NULL, \"cache\");\n\tif (IS_ERR(per_cpu_cache_dev(cpu)))\n\t\treturn PTR_ERR(per_cpu_cache_dev(cpu));\n\n\t \n\tper_cpu_index_dev(cpu) = kcalloc(cache_leaves(cpu),\n\t\t\t\t\t sizeof(struct device *), GFP_KERNEL);\n\tif (unlikely(per_cpu_index_dev(cpu) == NULL))\n\t\tgoto err_out;\n\n\treturn 0;\n\nerr_out:\n\tcpu_cache_sysfs_exit(cpu);\n\treturn -ENOMEM;\n}\n\nstatic int cache_add_dev(unsigned int cpu)\n{\n\tunsigned int i;\n\tint rc;\n\tstruct device *ci_dev, *parent;\n\tstruct cacheinfo *this_leaf;\n\tconst struct attribute_group **cache_groups;\n\n\trc = cpu_cache_sysfs_init(cpu);\n\tif (unlikely(rc < 0))\n\t\treturn rc;\n\n\tparent = per_cpu_cache_dev(cpu);\n\tfor (i = 0; i < cache_leaves(cpu); i++) {\n\t\tthis_leaf = per_cpu_cacheinfo_idx(cpu, i);\n\t\tif (this_leaf->disable_sysfs)\n\t\t\tcontinue;\n\t\tif (this_leaf->type == CACHE_TYPE_NOCACHE)\n\t\t\tbreak;\n\t\tcache_groups = cache_get_attribute_groups(this_leaf);\n\t\tci_dev = cpu_device_create(parent, this_leaf, cache_groups,\n\t\t\t\t\t   \"index%1u\", i);\n\t\tif (IS_ERR(ci_dev)) {\n\t\t\trc = PTR_ERR(ci_dev);\n\t\t\tgoto err;\n\t\t}\n\t\tper_cache_index_dev(cpu, i) = ci_dev;\n\t}\n\tcpumask_set_cpu(cpu, &cache_dev_map);\n\n\treturn 0;\nerr:\n\tcpu_cache_sysfs_exit(cpu);\n\treturn rc;\n}\n\nstatic int cacheinfo_cpu_online(unsigned int cpu)\n{\n\tint rc = detect_cache_attributes(cpu);\n\n\tif (rc)\n\t\treturn rc;\n\trc = cache_add_dev(cpu);\n\tif (rc)\n\t\tfree_cache_attributes(cpu);\n\treturn rc;\n}\n\nstatic int cacheinfo_cpu_pre_down(unsigned int cpu)\n{\n\tif (cpumask_test_and_clear_cpu(cpu, &cache_dev_map))\n\t\tcpu_cache_sysfs_exit(cpu);\n\n\tfree_cache_attributes(cpu);\n\treturn 0;\n}\n\nstatic int __init cacheinfo_sysfs_init(void)\n{\n\treturn cpuhp_setup_state(CPUHP_AP_BASE_CACHEINFO_ONLINE,\n\t\t\t\t \"base/cacheinfo:online\",\n\t\t\t\t cacheinfo_cpu_online, cacheinfo_cpu_pre_down);\n}\ndevice_initcall(cacheinfo_sysfs_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}