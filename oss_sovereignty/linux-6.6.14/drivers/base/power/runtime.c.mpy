{
  "module_name": "runtime.c",
  "hash_id": "6bb6f04c6f9f601ea7b555abeb238e2ef07ec4b36a365972383bce64f1d63824",
  "original_prompt": "Ingested from linux-6.6.14/drivers/base/power/runtime.c",
  "human_readable_source": "\n \n#include <linux/sched/mm.h>\n#include <linux/ktime.h>\n#include <linux/hrtimer.h>\n#include <linux/export.h>\n#include <linux/pm_runtime.h>\n#include <linux/pm_wakeirq.h>\n#include <trace/events/rpm.h>\n\n#include \"../base.h\"\n#include \"power.h\"\n\ntypedef int (*pm_callback_t)(struct device *);\n\nstatic pm_callback_t __rpm_get_callback(struct device *dev, size_t cb_offset)\n{\n\tpm_callback_t cb;\n\tconst struct dev_pm_ops *ops;\n\n\tif (dev->pm_domain)\n\t\tops = &dev->pm_domain->ops;\n\telse if (dev->type && dev->type->pm)\n\t\tops = dev->type->pm;\n\telse if (dev->class && dev->class->pm)\n\t\tops = dev->class->pm;\n\telse if (dev->bus && dev->bus->pm)\n\t\tops = dev->bus->pm;\n\telse\n\t\tops = NULL;\n\n\tif (ops)\n\t\tcb = *(pm_callback_t *)((void *)ops + cb_offset);\n\telse\n\t\tcb = NULL;\n\n\tif (!cb && dev->driver && dev->driver->pm)\n\t\tcb = *(pm_callback_t *)((void *)dev->driver->pm + cb_offset);\n\n\treturn cb;\n}\n\n#define RPM_GET_CALLBACK(dev, callback) \\\n\t\t__rpm_get_callback(dev, offsetof(struct dev_pm_ops, callback))\n\nstatic int rpm_resume(struct device *dev, int rpmflags);\nstatic int rpm_suspend(struct device *dev, int rpmflags);\n\n \nstatic void update_pm_runtime_accounting(struct device *dev)\n{\n\tu64 now, last, delta;\n\n\tif (dev->power.disable_depth > 0)\n\t\treturn;\n\n\tlast = dev->power.accounting_timestamp;\n\n\tnow = ktime_get_mono_fast_ns();\n\tdev->power.accounting_timestamp = now;\n\n\t \n\tif (now < last)\n\t\treturn;\n\n\tdelta = now - last;\n\n\tif (dev->power.runtime_status == RPM_SUSPENDED)\n\t\tdev->power.suspended_time += delta;\n\telse\n\t\tdev->power.active_time += delta;\n}\n\nstatic void __update_runtime_status(struct device *dev, enum rpm_status status)\n{\n\tupdate_pm_runtime_accounting(dev);\n\tdev->power.runtime_status = status;\n}\n\nstatic u64 rpm_get_accounted_time(struct device *dev, bool suspended)\n{\n\tu64 time;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\tupdate_pm_runtime_accounting(dev);\n\ttime = suspended ? dev->power.suspended_time : dev->power.active_time;\n\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn time;\n}\n\nu64 pm_runtime_active_time(struct device *dev)\n{\n\treturn rpm_get_accounted_time(dev, false);\n}\n\nu64 pm_runtime_suspended_time(struct device *dev)\n{\n\treturn rpm_get_accounted_time(dev, true);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_suspended_time);\n\n \nstatic void pm_runtime_deactivate_timer(struct device *dev)\n{\n\tif (dev->power.timer_expires > 0) {\n\t\thrtimer_try_to_cancel(&dev->power.suspend_timer);\n\t\tdev->power.timer_expires = 0;\n\t}\n}\n\n \nstatic void pm_runtime_cancel_pending(struct device *dev)\n{\n\tpm_runtime_deactivate_timer(dev);\n\t \n\tdev->power.request = RPM_REQ_NONE;\n}\n\n \nu64 pm_runtime_autosuspend_expiration(struct device *dev)\n{\n\tint autosuspend_delay;\n\tu64 expires;\n\n\tif (!dev->power.use_autosuspend)\n\t\treturn 0;\n\n\tautosuspend_delay = READ_ONCE(dev->power.autosuspend_delay);\n\tif (autosuspend_delay < 0)\n\t\treturn 0;\n\n\texpires  = READ_ONCE(dev->power.last_busy);\n\texpires += (u64)autosuspend_delay * NSEC_PER_MSEC;\n\tif (expires > ktime_get_mono_fast_ns())\n\t\treturn expires;\t \n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pm_runtime_autosuspend_expiration);\n\nstatic int dev_memalloc_noio(struct device *dev, void *data)\n{\n\treturn dev->power.memalloc_noio;\n}\n\n \nvoid pm_runtime_set_memalloc_noio(struct device *dev, bool enable)\n{\n\tstatic DEFINE_MUTEX(dev_hotplug_mutex);\n\n\tmutex_lock(&dev_hotplug_mutex);\n\tfor (;;) {\n\t\tbool enabled;\n\n\t\t \n\t\tspin_lock_irq(&dev->power.lock);\n\t\tenabled = dev->power.memalloc_noio;\n\t\tdev->power.memalloc_noio = enable;\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\t \n\t\tif (enabled && enable)\n\t\t\tbreak;\n\n\t\tdev = dev->parent;\n\n\t\t \n\t\tif (!dev || (!enable &&\n\t\t    device_for_each_child(dev, NULL, dev_memalloc_noio)))\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&dev_hotplug_mutex);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_set_memalloc_noio);\n\n \nstatic int rpm_check_suspend_allowed(struct device *dev)\n{\n\tint retval = 0;\n\n\tif (dev->power.runtime_error)\n\t\tretval = -EINVAL;\n\telse if (dev->power.disable_depth > 0)\n\t\tretval = -EACCES;\n\telse if (atomic_read(&dev->power.usage_count))\n\t\tretval = -EAGAIN;\n\telse if (!dev->power.ignore_children && atomic_read(&dev->power.child_count))\n\t\tretval = -EBUSY;\n\n\t \n\telse if ((dev->power.deferred_resume &&\n\t    dev->power.runtime_status == RPM_SUSPENDING) ||\n\t    (dev->power.request_pending && dev->power.request == RPM_REQ_RESUME))\n\t\tretval = -EAGAIN;\n\telse if (__dev_pm_qos_resume_latency(dev) == 0)\n\t\tretval = -EPERM;\n\telse if (dev->power.runtime_status == RPM_SUSPENDED)\n\t\tretval = 1;\n\n\treturn retval;\n}\n\nstatic int rpm_get_suppliers(struct device *dev)\n{\n\tstruct device_link *link;\n\n\tlist_for_each_entry_rcu(link, &dev->links.suppliers, c_node,\n\t\t\t\tdevice_links_read_lock_held()) {\n\t\tint retval;\n\n\t\tif (!(link->flags & DL_FLAG_PM_RUNTIME))\n\t\t\tcontinue;\n\n\t\tretval = pm_runtime_get_sync(link->supplier);\n\t\t \n\t\tif (retval < 0 && retval != -EACCES) {\n\t\t\tpm_runtime_put_noidle(link->supplier);\n\t\t\treturn retval;\n\t\t}\n\t\trefcount_inc(&link->rpm_active);\n\t}\n\treturn 0;\n}\n\n \nvoid pm_runtime_release_supplier(struct device_link *link)\n{\n\tstruct device *supplier = link->supplier;\n\n\t \n\twhile (refcount_dec_not_one(&link->rpm_active) &&\n\t       atomic_read(&supplier->power.usage_count) > 0)\n\t\tpm_runtime_put_noidle(supplier);\n}\n\nstatic void __rpm_put_suppliers(struct device *dev, bool try_to_suspend)\n{\n\tstruct device_link *link;\n\n\tlist_for_each_entry_rcu(link, &dev->links.suppliers, c_node,\n\t\t\t\tdevice_links_read_lock_held()) {\n\t\tpm_runtime_release_supplier(link);\n\t\tif (try_to_suspend)\n\t\t\tpm_request_idle(link->supplier);\n\t}\n}\n\nstatic void rpm_put_suppliers(struct device *dev)\n{\n\t__rpm_put_suppliers(dev, true);\n}\n\nstatic void rpm_suspend_suppliers(struct device *dev)\n{\n\tstruct device_link *link;\n\tint idx = device_links_read_lock();\n\n\tlist_for_each_entry_rcu(link, &dev->links.suppliers, c_node,\n\t\t\t\tdevice_links_read_lock_held())\n\t\tpm_request_idle(link->supplier);\n\n\tdevice_links_read_unlock(idx);\n}\n\n \nstatic int __rpm_callback(int (*cb)(struct device *), struct device *dev)\n\t__releases(&dev->power.lock) __acquires(&dev->power.lock)\n{\n\tint retval = 0, idx;\n\tbool use_links = dev->power.links_count > 0;\n\n\tif (dev->power.irq_safe) {\n\t\tspin_unlock(&dev->power.lock);\n\t} else {\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\t \n\t\tif (use_links && dev->power.runtime_status == RPM_RESUMING) {\n\t\t\tidx = device_links_read_lock();\n\n\t\t\tretval = rpm_get_suppliers(dev);\n\t\t\tif (retval) {\n\t\t\t\trpm_put_suppliers(dev);\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tdevice_links_read_unlock(idx);\n\t\t}\n\t}\n\n\tif (cb)\n\t\tretval = cb(dev);\n\n\tif (dev->power.irq_safe) {\n\t\tspin_lock(&dev->power.lock);\n\t} else {\n\t\t \n\t\tif (use_links &&\n\t\t    ((dev->power.runtime_status == RPM_SUSPENDING && !retval) ||\n\t\t    (dev->power.runtime_status == RPM_RESUMING && retval))) {\n\t\t\tidx = device_links_read_lock();\n\n\t\t\t__rpm_put_suppliers(dev, false);\n\nfail:\n\t\t\tdevice_links_read_unlock(idx);\n\t\t}\n\n\t\tspin_lock_irq(&dev->power.lock);\n\t}\n\n\treturn retval;\n}\n\n \nstatic int rpm_callback(int (*cb)(struct device *), struct device *dev)\n{\n\tint retval;\n\n\tif (dev->power.memalloc_noio) {\n\t\tunsigned int noio_flag;\n\n\t\t \n\t\tnoio_flag = memalloc_noio_save();\n\t\tretval = __rpm_callback(cb, dev);\n\t\tmemalloc_noio_restore(noio_flag);\n\t} else {\n\t\tretval = __rpm_callback(cb, dev);\n\t}\n\n\tdev->power.runtime_error = retval;\n\treturn retval != -EACCES ? retval : -EIO;\n}\n\n \nstatic int rpm_idle(struct device *dev, int rpmflags)\n{\n\tint (*callback)(struct device *);\n\tint retval;\n\n\ttrace_rpm_idle(dev, rpmflags);\n\tretval = rpm_check_suspend_allowed(dev);\n\tif (retval < 0)\n\t\t;\t \n\n\t \n\telse if (dev->power.runtime_status != RPM_ACTIVE)\n\t\tretval = -EAGAIN;\n\n\t \n\telse if (dev->power.request_pending &&\n\t    dev->power.request > RPM_REQ_IDLE)\n\t\tretval = -EAGAIN;\n\n\t \n\telse if (dev->power.idle_notification)\n\t\tretval = -EINPROGRESS;\n\n\tif (retval)\n\t\tgoto out;\n\n\t \n\tdev->power.request = RPM_REQ_NONE;\n\n\tcallback = RPM_GET_CALLBACK(dev, runtime_idle);\n\n\t \n\tif (!callback || dev->power.no_callbacks)\n\t\tgoto out;\n\n\t \n\tif (rpmflags & RPM_ASYNC) {\n\t\tdev->power.request = RPM_REQ_IDLE;\n\t\tif (!dev->power.request_pending) {\n\t\t\tdev->power.request_pending = true;\n\t\t\tqueue_work(pm_wq, &dev->power.work);\n\t\t}\n\t\ttrace_rpm_return_int(dev, _THIS_IP_, 0);\n\t\treturn 0;\n\t}\n\n\tdev->power.idle_notification = true;\n\n\tif (dev->power.irq_safe)\n\t\tspin_unlock(&dev->power.lock);\n\telse\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\tretval = callback(dev);\n\n\tif (dev->power.irq_safe)\n\t\tspin_lock(&dev->power.lock);\n\telse\n\t\tspin_lock_irq(&dev->power.lock);\n\n\tdev->power.idle_notification = false;\n\twake_up_all(&dev->power.wait_queue);\n\n out:\n\ttrace_rpm_return_int(dev, _THIS_IP_, retval);\n\treturn retval ? retval : rpm_suspend(dev, rpmflags | RPM_AUTO);\n}\n\n \nstatic int rpm_suspend(struct device *dev, int rpmflags)\n\t__releases(&dev->power.lock) __acquires(&dev->power.lock)\n{\n\tint (*callback)(struct device *);\n\tstruct device *parent = NULL;\n\tint retval;\n\n\ttrace_rpm_suspend(dev, rpmflags);\n\n repeat:\n\tretval = rpm_check_suspend_allowed(dev);\n\tif (retval < 0)\n\t\tgoto out;\t \n\n\t \n\tif (dev->power.runtime_status == RPM_RESUMING && !(rpmflags & RPM_ASYNC))\n\t\tretval = -EAGAIN;\n\n\tif (retval)\n\t\tgoto out;\n\n\t \n\tif ((rpmflags & RPM_AUTO) && dev->power.runtime_status != RPM_SUSPENDING) {\n\t\tu64 expires = pm_runtime_autosuspend_expiration(dev);\n\n\t\tif (expires != 0) {\n\t\t\t \n\t\t\tdev->power.request = RPM_REQ_NONE;\n\n\t\t\t \n\t\t\tif (!(dev->power.timer_expires &&\n\t\t\t    dev->power.timer_expires <= expires)) {\n\t\t\t\t \n\t\t\t\tu64 slack = (u64)READ_ONCE(dev->power.autosuspend_delay) *\n\t\t\t\t\t\t    (NSEC_PER_MSEC >> 2);\n\n\t\t\t\tdev->power.timer_expires = expires;\n\t\t\t\thrtimer_start_range_ns(&dev->power.suspend_timer,\n\t\t\t\t\t\t       ns_to_ktime(expires),\n\t\t\t\t\t\t       slack,\n\t\t\t\t\t\t       HRTIMER_MODE_ABS);\n\t\t\t}\n\t\t\tdev->power.timer_autosuspends = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tpm_runtime_cancel_pending(dev);\n\n\tif (dev->power.runtime_status == RPM_SUSPENDING) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tif (rpmflags & (RPM_ASYNC | RPM_NOWAIT)) {\n\t\t\tretval = -EINPROGRESS;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (dev->power.irq_safe) {\n\t\t\tspin_unlock(&dev->power.lock);\n\n\t\t\tcpu_relax();\n\n\t\t\tspin_lock(&dev->power.lock);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t \n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&dev->power.wait_queue, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\tif (dev->power.runtime_status != RPM_SUSPENDING)\n\t\t\t\tbreak;\n\n\t\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\t\tschedule();\n\n\t\t\tspin_lock_irq(&dev->power.lock);\n\t\t}\n\t\tfinish_wait(&dev->power.wait_queue, &wait);\n\t\tgoto repeat;\n\t}\n\n\tif (dev->power.no_callbacks)\n\t\tgoto no_callback;\t \n\n\t \n\tif (rpmflags & RPM_ASYNC) {\n\t\tdev->power.request = (rpmflags & RPM_AUTO) ?\n\t\t    RPM_REQ_AUTOSUSPEND : RPM_REQ_SUSPEND;\n\t\tif (!dev->power.request_pending) {\n\t\t\tdev->power.request_pending = true;\n\t\t\tqueue_work(pm_wq, &dev->power.work);\n\t\t}\n\t\tgoto out;\n\t}\n\n\t__update_runtime_status(dev, RPM_SUSPENDING);\n\n\tcallback = RPM_GET_CALLBACK(dev, runtime_suspend);\n\n\tdev_pm_enable_wake_irq_check(dev, true);\n\tretval = rpm_callback(callback, dev);\n\tif (retval)\n\t\tgoto fail;\n\n\tdev_pm_enable_wake_irq_complete(dev);\n\n no_callback:\n\t__update_runtime_status(dev, RPM_SUSPENDED);\n\tpm_runtime_deactivate_timer(dev);\n\n\tif (dev->parent) {\n\t\tparent = dev->parent;\n\t\tatomic_add_unless(&parent->power.child_count, -1, 0);\n\t}\n\twake_up_all(&dev->power.wait_queue);\n\n\tif (dev->power.deferred_resume) {\n\t\tdev->power.deferred_resume = false;\n\t\trpm_resume(dev, 0);\n\t\tretval = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (dev->power.irq_safe)\n\t\tgoto out;\n\n\t \n\tif (parent && !parent->power.ignore_children) {\n\t\tspin_unlock(&dev->power.lock);\n\n\t\tspin_lock(&parent->power.lock);\n\t\trpm_idle(parent, RPM_ASYNC);\n\t\tspin_unlock(&parent->power.lock);\n\n\t\tspin_lock(&dev->power.lock);\n\t}\n\t \n\tif (dev->power.links_count > 0) {\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\trpm_suspend_suppliers(dev);\n\n\t\tspin_lock_irq(&dev->power.lock);\n\t}\n\n out:\n\ttrace_rpm_return_int(dev, _THIS_IP_, retval);\n\n\treturn retval;\n\n fail:\n\tdev_pm_disable_wake_irq_check(dev, true);\n\t__update_runtime_status(dev, RPM_ACTIVE);\n\tdev->power.deferred_resume = false;\n\twake_up_all(&dev->power.wait_queue);\n\n\tif (retval == -EAGAIN || retval == -EBUSY) {\n\t\tdev->power.runtime_error = 0;\n\n\t\t \n\t\tif ((rpmflags & RPM_AUTO) &&\n\t\t    pm_runtime_autosuspend_expiration(dev) != 0)\n\t\t\tgoto repeat;\n\t} else {\n\t\tpm_runtime_cancel_pending(dev);\n\t}\n\tgoto out;\n}\n\n \nstatic int rpm_resume(struct device *dev, int rpmflags)\n\t__releases(&dev->power.lock) __acquires(&dev->power.lock)\n{\n\tint (*callback)(struct device *);\n\tstruct device *parent = NULL;\n\tint retval = 0;\n\n\ttrace_rpm_resume(dev, rpmflags);\n\n repeat:\n\tif (dev->power.runtime_error) {\n\t\tretval = -EINVAL;\n\t} else if (dev->power.disable_depth > 0) {\n\t\tif (dev->power.runtime_status == RPM_ACTIVE &&\n\t\t    dev->power.last_status == RPM_ACTIVE)\n\t\t\tretval = 1;\n\t\telse\n\t\t\tretval = -EACCES;\n\t}\n\tif (retval)\n\t\tgoto out;\n\n\t \n\tdev->power.request = RPM_REQ_NONE;\n\tif (!dev->power.timer_autosuspends)\n\t\tpm_runtime_deactivate_timer(dev);\n\n\tif (dev->power.runtime_status == RPM_ACTIVE) {\n\t\tretval = 1;\n\t\tgoto out;\n\t}\n\n\tif (dev->power.runtime_status == RPM_RESUMING ||\n\t    dev->power.runtime_status == RPM_SUSPENDING) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tif (rpmflags & (RPM_ASYNC | RPM_NOWAIT)) {\n\t\t\tif (dev->power.runtime_status == RPM_SUSPENDING) {\n\t\t\t\tdev->power.deferred_resume = true;\n\t\t\t\tif (rpmflags & RPM_NOWAIT)\n\t\t\t\t\tretval = -EINPROGRESS;\n\t\t\t} else {\n\t\t\t\tretval = -EINPROGRESS;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (dev->power.irq_safe) {\n\t\t\tspin_unlock(&dev->power.lock);\n\n\t\t\tcpu_relax();\n\n\t\t\tspin_lock(&dev->power.lock);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t \n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&dev->power.wait_queue, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\tif (dev->power.runtime_status != RPM_RESUMING &&\n\t\t\t    dev->power.runtime_status != RPM_SUSPENDING)\n\t\t\t\tbreak;\n\n\t\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\t\tschedule();\n\n\t\t\tspin_lock_irq(&dev->power.lock);\n\t\t}\n\t\tfinish_wait(&dev->power.wait_queue, &wait);\n\t\tgoto repeat;\n\t}\n\n\t \n\tif (dev->power.no_callbacks && !parent && dev->parent) {\n\t\tspin_lock_nested(&dev->parent->power.lock, SINGLE_DEPTH_NESTING);\n\t\tif (dev->parent->power.disable_depth > 0 ||\n\t\t    dev->parent->power.ignore_children ||\n\t\t    dev->parent->power.runtime_status == RPM_ACTIVE) {\n\t\t\tatomic_inc(&dev->parent->power.child_count);\n\t\t\tspin_unlock(&dev->parent->power.lock);\n\t\t\tretval = 1;\n\t\t\tgoto no_callback;\t \n\t\t}\n\t\tspin_unlock(&dev->parent->power.lock);\n\t}\n\n\t \n\tif (rpmflags & RPM_ASYNC) {\n\t\tdev->power.request = RPM_REQ_RESUME;\n\t\tif (!dev->power.request_pending) {\n\t\t\tdev->power.request_pending = true;\n\t\t\tqueue_work(pm_wq, &dev->power.work);\n\t\t}\n\t\tretval = 0;\n\t\tgoto out;\n\t}\n\n\tif (!parent && dev->parent) {\n\t\t \n\t\tparent = dev->parent;\n\t\tif (dev->power.irq_safe)\n\t\t\tgoto skip_parent;\n\n\t\tspin_unlock(&dev->power.lock);\n\n\t\tpm_runtime_get_noresume(parent);\n\n\t\tspin_lock(&parent->power.lock);\n\t\t \n\t\tif (!parent->power.disable_depth &&\n\t\t    !parent->power.ignore_children) {\n\t\t\trpm_resume(parent, 0);\n\t\t\tif (parent->power.runtime_status != RPM_ACTIVE)\n\t\t\t\tretval = -EBUSY;\n\t\t}\n\t\tspin_unlock(&parent->power.lock);\n\n\t\tspin_lock(&dev->power.lock);\n\t\tif (retval)\n\t\t\tgoto out;\n\n\t\tgoto repeat;\n\t}\n skip_parent:\n\n\tif (dev->power.no_callbacks)\n\t\tgoto no_callback;\t \n\n\t__update_runtime_status(dev, RPM_RESUMING);\n\n\tcallback = RPM_GET_CALLBACK(dev, runtime_resume);\n\n\tdev_pm_disable_wake_irq_check(dev, false);\n\tretval = rpm_callback(callback, dev);\n\tif (retval) {\n\t\t__update_runtime_status(dev, RPM_SUSPENDED);\n\t\tpm_runtime_cancel_pending(dev);\n\t\tdev_pm_enable_wake_irq_check(dev, false);\n\t} else {\n no_callback:\n\t\t__update_runtime_status(dev, RPM_ACTIVE);\n\t\tpm_runtime_mark_last_busy(dev);\n\t\tif (parent)\n\t\t\tatomic_inc(&parent->power.child_count);\n\t}\n\twake_up_all(&dev->power.wait_queue);\n\n\tif (retval >= 0)\n\t\trpm_idle(dev, RPM_ASYNC);\n\n out:\n\tif (parent && !dev->power.irq_safe) {\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\tpm_runtime_put(parent);\n\n\t\tspin_lock_irq(&dev->power.lock);\n\t}\n\n\ttrace_rpm_return_int(dev, _THIS_IP_, retval);\n\n\treturn retval;\n}\n\n \nstatic void pm_runtime_work(struct work_struct *work)\n{\n\tstruct device *dev = container_of(work, struct device, power.work);\n\tenum rpm_request req;\n\n\tspin_lock_irq(&dev->power.lock);\n\n\tif (!dev->power.request_pending)\n\t\tgoto out;\n\n\treq = dev->power.request;\n\tdev->power.request = RPM_REQ_NONE;\n\tdev->power.request_pending = false;\n\n\tswitch (req) {\n\tcase RPM_REQ_NONE:\n\t\tbreak;\n\tcase RPM_REQ_IDLE:\n\t\trpm_idle(dev, RPM_NOWAIT);\n\t\tbreak;\n\tcase RPM_REQ_SUSPEND:\n\t\trpm_suspend(dev, RPM_NOWAIT);\n\t\tbreak;\n\tcase RPM_REQ_AUTOSUSPEND:\n\t\trpm_suspend(dev, RPM_NOWAIT | RPM_AUTO);\n\t\tbreak;\n\tcase RPM_REQ_RESUME:\n\t\trpm_resume(dev, RPM_NOWAIT);\n\t\tbreak;\n\t}\n\n out:\n\tspin_unlock_irq(&dev->power.lock);\n}\n\n \nstatic enum hrtimer_restart  pm_suspend_timer_fn(struct hrtimer *timer)\n{\n\tstruct device *dev = container_of(timer, struct device, power.suspend_timer);\n\tunsigned long flags;\n\tu64 expires;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\texpires = dev->power.timer_expires;\n\t \n\tif (expires > 0 && expires < ktime_get_mono_fast_ns()) {\n\t\tdev->power.timer_expires = 0;\n\t\trpm_suspend(dev, dev->power.timer_autosuspends ?\n\t\t    (RPM_ASYNC | RPM_AUTO) : RPM_ASYNC);\n\t}\n\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn HRTIMER_NORESTART;\n}\n\n \nint pm_schedule_suspend(struct device *dev, unsigned int delay)\n{\n\tunsigned long flags;\n\tu64 expires;\n\tint retval;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\tif (!delay) {\n\t\tretval = rpm_suspend(dev, RPM_ASYNC);\n\t\tgoto out;\n\t}\n\n\tretval = rpm_check_suspend_allowed(dev);\n\tif (retval)\n\t\tgoto out;\n\n\t \n\tpm_runtime_cancel_pending(dev);\n\n\texpires = ktime_get_mono_fast_ns() + (u64)delay * NSEC_PER_MSEC;\n\tdev->power.timer_expires = expires;\n\tdev->power.timer_autosuspends = 0;\n\thrtimer_start(&dev->power.suspend_timer, expires, HRTIMER_MODE_ABS);\n\n out:\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(pm_schedule_suspend);\n\nstatic int rpm_drop_usage_count(struct device *dev)\n{\n\tint ret;\n\n\tret = atomic_sub_return(1, &dev->power.usage_count);\n\tif (ret >= 0)\n\t\treturn ret;\n\n\t \n\tatomic_inc(&dev->power.usage_count);\n\tdev_warn(dev, \"Runtime PM usage count underflow!\\n\");\n\treturn -EINVAL;\n}\n\n \nint __pm_runtime_idle(struct device *dev, int rpmflags)\n{\n\tunsigned long flags;\n\tint retval;\n\n\tif (rpmflags & RPM_GET_PUT) {\n\t\tretval = rpm_drop_usage_count(dev);\n\t\tif (retval < 0) {\n\t\t\treturn retval;\n\t\t} else if (retval > 0) {\n\t\t\ttrace_rpm_usage(dev, rpmflags);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe);\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\tretval = rpm_idle(dev, rpmflags);\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_idle);\n\n \nint __pm_runtime_suspend(struct device *dev, int rpmflags)\n{\n\tunsigned long flags;\n\tint retval;\n\n\tif (rpmflags & RPM_GET_PUT) {\n\t\tretval = rpm_drop_usage_count(dev);\n\t\tif (retval < 0) {\n\t\t\treturn retval;\n\t\t} else if (retval > 0) {\n\t\t\ttrace_rpm_usage(dev, rpmflags);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe);\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\tretval = rpm_suspend(dev, rpmflags);\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_suspend);\n\n \nint __pm_runtime_resume(struct device *dev, int rpmflags)\n{\n\tunsigned long flags;\n\tint retval;\n\n\tmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe &&\n\t\t\tdev->power.runtime_status != RPM_ACTIVE);\n\n\tif (rpmflags & RPM_GET_PUT)\n\t\tatomic_inc(&dev->power.usage_count);\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\tretval = rpm_resume(dev, rpmflags);\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_resume);\n\n \nint pm_runtime_get_if_active(struct device *dev, bool ign_usage_count)\n{\n\tunsigned long flags;\n\tint retval;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\tif (dev->power.disable_depth > 0) {\n\t\tretval = -EINVAL;\n\t} else if (dev->power.runtime_status != RPM_ACTIVE) {\n\t\tretval = 0;\n\t} else if (ign_usage_count) {\n\t\tretval = 1;\n\t\tatomic_inc(&dev->power.usage_count);\n\t} else {\n\t\tretval = atomic_inc_not_zero(&dev->power.usage_count);\n\t}\n\ttrace_rpm_usage(dev, 0);\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(pm_runtime_get_if_active);\n\n \nint __pm_runtime_set_status(struct device *dev, unsigned int status)\n{\n\tstruct device *parent = dev->parent;\n\tbool notify_parent = false;\n\tunsigned long flags;\n\tint error = 0;\n\n\tif (status != RPM_ACTIVE && status != RPM_SUSPENDED)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\t \n\tif (dev->power.runtime_error || dev->power.disable_depth)\n\t\tdev->power.disable_depth++;\n\telse\n\t\terror = -EAGAIN;\n\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\tif (error)\n\t\treturn error;\n\n\t \n\tif (status == RPM_ACTIVE) {\n\t\tint idx = device_links_read_lock();\n\n\t\terror = rpm_get_suppliers(dev);\n\t\tif (error)\n\t\t\tstatus = RPM_SUSPENDED;\n\n\t\tdevice_links_read_unlock(idx);\n\t}\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\tif (dev->power.runtime_status == status || !parent)\n\t\tgoto out_set;\n\n\tif (status == RPM_SUSPENDED) {\n\t\tatomic_add_unless(&parent->power.child_count, -1, 0);\n\t\tnotify_parent = !parent->power.ignore_children;\n\t} else {\n\t\tspin_lock_nested(&parent->power.lock, SINGLE_DEPTH_NESTING);\n\n\t\t \n\t\tif (!parent->power.disable_depth &&\n\t\t    !parent->power.ignore_children &&\n\t\t    parent->power.runtime_status != RPM_ACTIVE) {\n\t\t\tdev_err(dev, \"runtime PM trying to activate child device %s but parent (%s) is not active\\n\",\n\t\t\t\tdev_name(dev),\n\t\t\t\tdev_name(parent));\n\t\t\terror = -EBUSY;\n\t\t} else if (dev->power.runtime_status == RPM_SUSPENDED) {\n\t\t\tatomic_inc(&parent->power.child_count);\n\t\t}\n\n\t\tspin_unlock(&parent->power.lock);\n\n\t\tif (error) {\n\t\t\tstatus = RPM_SUSPENDED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n out_set:\n\t__update_runtime_status(dev, status);\n\tif (!error)\n\t\tdev->power.runtime_error = 0;\n\n out:\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\tif (notify_parent)\n\t\tpm_request_idle(parent);\n\n\tif (status == RPM_SUSPENDED) {\n\t\tint idx = device_links_read_lock();\n\n\t\trpm_put_suppliers(dev);\n\n\t\tdevice_links_read_unlock(idx);\n\t}\n\n\tpm_runtime_enable(dev);\n\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_set_status);\n\n \nstatic void __pm_runtime_barrier(struct device *dev)\n{\n\tpm_runtime_deactivate_timer(dev);\n\n\tif (dev->power.request_pending) {\n\t\tdev->power.request = RPM_REQ_NONE;\n\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\tcancel_work_sync(&dev->power.work);\n\n\t\tspin_lock_irq(&dev->power.lock);\n\t\tdev->power.request_pending = false;\n\t}\n\n\tif (dev->power.runtime_status == RPM_SUSPENDING ||\n\t    dev->power.runtime_status == RPM_RESUMING ||\n\t    dev->power.idle_notification) {\n\t\tDEFINE_WAIT(wait);\n\n\t\t \n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&dev->power.wait_queue, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\tif (dev->power.runtime_status != RPM_SUSPENDING\n\t\t\t    && dev->power.runtime_status != RPM_RESUMING\n\t\t\t    && !dev->power.idle_notification)\n\t\t\t\tbreak;\n\t\t\tspin_unlock_irq(&dev->power.lock);\n\n\t\t\tschedule();\n\n\t\t\tspin_lock_irq(&dev->power.lock);\n\t\t}\n\t\tfinish_wait(&dev->power.wait_queue, &wait);\n\t}\n}\n\n \nint pm_runtime_barrier(struct device *dev)\n{\n\tint retval = 0;\n\n\tpm_runtime_get_noresume(dev);\n\tspin_lock_irq(&dev->power.lock);\n\n\tif (dev->power.request_pending\n\t    && dev->power.request == RPM_REQ_RESUME) {\n\t\trpm_resume(dev, 0);\n\t\tretval = 1;\n\t}\n\n\t__pm_runtime_barrier(dev);\n\n\tspin_unlock_irq(&dev->power.lock);\n\tpm_runtime_put_noidle(dev);\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(pm_runtime_barrier);\n\n \nvoid __pm_runtime_disable(struct device *dev, bool check_resume)\n{\n\tspin_lock_irq(&dev->power.lock);\n\n\tif (dev->power.disable_depth > 0) {\n\t\tdev->power.disable_depth++;\n\t\tgoto out;\n\t}\n\n\t \n\tif (check_resume && dev->power.request_pending &&\n\t    dev->power.request == RPM_REQ_RESUME) {\n\t\t \n\t\tpm_runtime_get_noresume(dev);\n\n\t\trpm_resume(dev, 0);\n\n\t\tpm_runtime_put_noidle(dev);\n\t}\n\n\t \n\tupdate_pm_runtime_accounting(dev);\n\n\tif (!dev->power.disable_depth++) {\n\t\t__pm_runtime_barrier(dev);\n\t\tdev->power.last_status = dev->power.runtime_status;\n\t}\n\n out:\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_disable);\n\n \nvoid pm_runtime_enable(struct device *dev)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\tif (!dev->power.disable_depth) {\n\t\tdev_warn(dev, \"Unbalanced %s!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tif (--dev->power.disable_depth > 0)\n\t\tgoto out;\n\n\tdev->power.last_status = RPM_INVALID;\n\tdev->power.accounting_timestamp = ktime_get_mono_fast_ns();\n\n\tif (dev->power.runtime_status == RPM_SUSPENDED &&\n\t    !dev->power.ignore_children &&\n\t    atomic_read(&dev->power.child_count) > 0)\n\t\tdev_warn(dev, \"Enabling runtime PM for inactive device with active children\\n\");\n\nout:\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_enable);\n\nstatic void pm_runtime_disable_action(void *data)\n{\n\tpm_runtime_dont_use_autosuspend(data);\n\tpm_runtime_disable(data);\n}\n\n \nint devm_pm_runtime_enable(struct device *dev)\n{\n\tpm_runtime_enable(dev);\n\n\treturn devm_add_action_or_reset(dev, pm_runtime_disable_action, dev);\n}\nEXPORT_SYMBOL_GPL(devm_pm_runtime_enable);\n\n \nvoid pm_runtime_forbid(struct device *dev)\n{\n\tspin_lock_irq(&dev->power.lock);\n\tif (!dev->power.runtime_auto)\n\t\tgoto out;\n\n\tdev->power.runtime_auto = false;\n\tatomic_inc(&dev->power.usage_count);\n\trpm_resume(dev, 0);\n\n out:\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_forbid);\n\n \nvoid pm_runtime_allow(struct device *dev)\n{\n\tint ret;\n\n\tspin_lock_irq(&dev->power.lock);\n\tif (dev->power.runtime_auto)\n\t\tgoto out;\n\n\tdev->power.runtime_auto = true;\n\tret = rpm_drop_usage_count(dev);\n\tif (ret == 0)\n\t\trpm_idle(dev, RPM_AUTO | RPM_ASYNC);\n\telse if (ret > 0)\n\t\ttrace_rpm_usage(dev, RPM_AUTO | RPM_ASYNC);\n\n out:\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_allow);\n\n \nvoid pm_runtime_no_callbacks(struct device *dev)\n{\n\tspin_lock_irq(&dev->power.lock);\n\tdev->power.no_callbacks = 1;\n\tspin_unlock_irq(&dev->power.lock);\n\tif (device_is_registered(dev))\n\t\trpm_sysfs_remove(dev);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_no_callbacks);\n\n \nvoid pm_runtime_irq_safe(struct device *dev)\n{\n\tif (dev->parent)\n\t\tpm_runtime_get_sync(dev->parent);\n\n\tspin_lock_irq(&dev->power.lock);\n\tdev->power.irq_safe = 1;\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_irq_safe);\n\n \nstatic void update_autosuspend(struct device *dev, int old_delay, int old_use)\n{\n\tint delay = dev->power.autosuspend_delay;\n\n\t \n\tif (dev->power.use_autosuspend && delay < 0) {\n\n\t\t \n\t\tif (!old_use || old_delay >= 0) {\n\t\t\tatomic_inc(&dev->power.usage_count);\n\t\t\trpm_resume(dev, 0);\n\t\t} else {\n\t\t\ttrace_rpm_usage(dev, 0);\n\t\t}\n\t}\n\n\t \n\telse {\n\n\t\t \n\t\tif (old_use && old_delay < 0)\n\t\t\tatomic_dec(&dev->power.usage_count);\n\n\t\t \n\t\trpm_idle(dev, RPM_AUTO);\n\t}\n}\n\n \nvoid pm_runtime_set_autosuspend_delay(struct device *dev, int delay)\n{\n\tint old_delay, old_use;\n\n\tspin_lock_irq(&dev->power.lock);\n\told_delay = dev->power.autosuspend_delay;\n\told_use = dev->power.use_autosuspend;\n\tdev->power.autosuspend_delay = delay;\n\tupdate_autosuspend(dev, old_delay, old_use);\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(pm_runtime_set_autosuspend_delay);\n\n \nvoid __pm_runtime_use_autosuspend(struct device *dev, bool use)\n{\n\tint old_delay, old_use;\n\n\tspin_lock_irq(&dev->power.lock);\n\told_delay = dev->power.autosuspend_delay;\n\told_use = dev->power.use_autosuspend;\n\tdev->power.use_autosuspend = use;\n\tupdate_autosuspend(dev, old_delay, old_use);\n\tspin_unlock_irq(&dev->power.lock);\n}\nEXPORT_SYMBOL_GPL(__pm_runtime_use_autosuspend);\n\n \nvoid pm_runtime_init(struct device *dev)\n{\n\tdev->power.runtime_status = RPM_SUSPENDED;\n\tdev->power.last_status = RPM_INVALID;\n\tdev->power.idle_notification = false;\n\n\tdev->power.disable_depth = 1;\n\tatomic_set(&dev->power.usage_count, 0);\n\n\tdev->power.runtime_error = 0;\n\n\tatomic_set(&dev->power.child_count, 0);\n\tpm_suspend_ignore_children(dev, false);\n\tdev->power.runtime_auto = true;\n\n\tdev->power.request_pending = false;\n\tdev->power.request = RPM_REQ_NONE;\n\tdev->power.deferred_resume = false;\n\tdev->power.needs_force_resume = 0;\n\tINIT_WORK(&dev->power.work, pm_runtime_work);\n\n\tdev->power.timer_expires = 0;\n\thrtimer_init(&dev->power.suspend_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);\n\tdev->power.suspend_timer.function = pm_suspend_timer_fn;\n\n\tinit_waitqueue_head(&dev->power.wait_queue);\n}\n\n \nvoid pm_runtime_reinit(struct device *dev)\n{\n\tif (!pm_runtime_enabled(dev)) {\n\t\tif (dev->power.runtime_status == RPM_ACTIVE)\n\t\t\tpm_runtime_set_suspended(dev);\n\t\tif (dev->power.irq_safe) {\n\t\t\tspin_lock_irq(&dev->power.lock);\n\t\t\tdev->power.irq_safe = 0;\n\t\t\tspin_unlock_irq(&dev->power.lock);\n\t\t\tif (dev->parent)\n\t\t\t\tpm_runtime_put(dev->parent);\n\t\t}\n\t}\n}\n\n \nvoid pm_runtime_remove(struct device *dev)\n{\n\t__pm_runtime_disable(dev, false);\n\tpm_runtime_reinit(dev);\n}\n\n \nvoid pm_runtime_get_suppliers(struct device *dev)\n{\n\tstruct device_link *link;\n\tint idx;\n\n\tidx = device_links_read_lock();\n\n\tlist_for_each_entry_rcu(link, &dev->links.suppliers, c_node,\n\t\t\t\tdevice_links_read_lock_held())\n\t\tif (link->flags & DL_FLAG_PM_RUNTIME) {\n\t\t\tlink->supplier_preactivated = true;\n\t\t\tpm_runtime_get_sync(link->supplier);\n\t\t}\n\n\tdevice_links_read_unlock(idx);\n}\n\n \nvoid pm_runtime_put_suppliers(struct device *dev)\n{\n\tstruct device_link *link;\n\tint idx;\n\n\tidx = device_links_read_lock();\n\n\tlist_for_each_entry_rcu(link, &dev->links.suppliers, c_node,\n\t\t\t\tdevice_links_read_lock_held())\n\t\tif (link->supplier_preactivated) {\n\t\t\tlink->supplier_preactivated = false;\n\t\t\tpm_runtime_put(link->supplier);\n\t\t}\n\n\tdevice_links_read_unlock(idx);\n}\n\nvoid pm_runtime_new_link(struct device *dev)\n{\n\tspin_lock_irq(&dev->power.lock);\n\tdev->power.links_count++;\n\tspin_unlock_irq(&dev->power.lock);\n}\n\nstatic void pm_runtime_drop_link_count(struct device *dev)\n{\n\tspin_lock_irq(&dev->power.lock);\n\tWARN_ON(dev->power.links_count == 0);\n\tdev->power.links_count--;\n\tspin_unlock_irq(&dev->power.lock);\n}\n\n \nvoid pm_runtime_drop_link(struct device_link *link)\n{\n\tif (!(link->flags & DL_FLAG_PM_RUNTIME))\n\t\treturn;\n\n\tpm_runtime_drop_link_count(link->consumer);\n\tpm_runtime_release_supplier(link);\n\tpm_request_idle(link->supplier);\n}\n\nstatic bool pm_runtime_need_not_resume(struct device *dev)\n{\n\treturn atomic_read(&dev->power.usage_count) <= 1 &&\n\t\t(atomic_read(&dev->power.child_count) == 0 ||\n\t\t dev->power.ignore_children);\n}\n\n \nint pm_runtime_force_suspend(struct device *dev)\n{\n\tint (*callback)(struct device *);\n\tint ret;\n\n\tpm_runtime_disable(dev);\n\tif (pm_runtime_status_suspended(dev))\n\t\treturn 0;\n\n\tcallback = RPM_GET_CALLBACK(dev, runtime_suspend);\n\n\tdev_pm_enable_wake_irq_check(dev, true);\n\tret = callback ? callback(dev) : 0;\n\tif (ret)\n\t\tgoto err;\n\n\tdev_pm_enable_wake_irq_complete(dev);\n\n\t \n\tif (pm_runtime_need_not_resume(dev)) {\n\t\tpm_runtime_set_suspended(dev);\n\t} else {\n\t\t__update_runtime_status(dev, RPM_SUSPENDED);\n\t\tdev->power.needs_force_resume = 1;\n\t}\n\n\treturn 0;\n\nerr:\n\tdev_pm_disable_wake_irq_check(dev, true);\n\tpm_runtime_enable(dev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pm_runtime_force_suspend);\n\n \nint pm_runtime_force_resume(struct device *dev)\n{\n\tint (*callback)(struct device *);\n\tint ret = 0;\n\n\tif (!pm_runtime_status_suspended(dev) || !dev->power.needs_force_resume)\n\t\tgoto out;\n\n\t \n\t__update_runtime_status(dev, RPM_ACTIVE);\n\n\tcallback = RPM_GET_CALLBACK(dev, runtime_resume);\n\n\tdev_pm_disable_wake_irq_check(dev, false);\n\tret = callback ? callback(dev) : 0;\n\tif (ret) {\n\t\tpm_runtime_set_suspended(dev);\n\t\tdev_pm_enable_wake_irq_check(dev, false);\n\t\tgoto out;\n\t}\n\n\tpm_runtime_mark_last_busy(dev);\nout:\n\tdev->power.needs_force_resume = 0;\n\tpm_runtime_enable(dev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pm_runtime_force_resume);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}