{
  "module_name": "qos.c",
  "hash_id": "f31194e393d2a28e4f275e88644cc8276c6aeb05db913aa98310356a379aa170",
  "original_prompt": "Ingested from linux-6.6.14/drivers/base/power/qos.c",
  "human_readable_source": "\n \n\n#include <linux/pm_qos.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/device.h>\n#include <linux/mutex.h>\n#include <linux/export.h>\n#include <linux/pm_runtime.h>\n#include <linux/err.h>\n#include <trace/events/power.h>\n\n#include \"power.h\"\n\nstatic DEFINE_MUTEX(dev_pm_qos_mtx);\nstatic DEFINE_MUTEX(dev_pm_qos_sysfs_mtx);\n\n \nenum pm_qos_flags_status __dev_pm_qos_flags(struct device *dev, s32 mask)\n{\n\tstruct dev_pm_qos *qos = dev->power.qos;\n\tstruct pm_qos_flags *pqf;\n\ts32 val;\n\n\tlockdep_assert_held(&dev->power.lock);\n\n\tif (IS_ERR_OR_NULL(qos))\n\t\treturn PM_QOS_FLAGS_UNDEFINED;\n\n\tpqf = &qos->flags;\n\tif (list_empty(&pqf->list))\n\t\treturn PM_QOS_FLAGS_UNDEFINED;\n\n\tval = pqf->effective_flags & mask;\n\tif (val)\n\t\treturn (val == mask) ? PM_QOS_FLAGS_ALL : PM_QOS_FLAGS_SOME;\n\n\treturn PM_QOS_FLAGS_NONE;\n}\n\n \nenum pm_qos_flags_status dev_pm_qos_flags(struct device *dev, s32 mask)\n{\n\tunsigned long irqflags;\n\tenum pm_qos_flags_status ret;\n\n\tspin_lock_irqsave(&dev->power.lock, irqflags);\n\tret = __dev_pm_qos_flags(dev, mask);\n\tspin_unlock_irqrestore(&dev->power.lock, irqflags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_flags);\n\n \ns32 __dev_pm_qos_resume_latency(struct device *dev)\n{\n\tlockdep_assert_held(&dev->power.lock);\n\n\treturn dev_pm_qos_raw_resume_latency(dev);\n}\n\n \ns32 dev_pm_qos_read_value(struct device *dev, enum dev_pm_qos_req_type type)\n{\n\tstruct dev_pm_qos *qos = dev->power.qos;\n\tunsigned long flags;\n\ts32 ret;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\n\tswitch (type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\tret = IS_ERR_OR_NULL(qos) ? PM_QOS_RESUME_LATENCY_NO_CONSTRAINT\n\t\t\t: pm_qos_read_value(&qos->resume_latency);\n\t\tbreak;\n\tcase DEV_PM_QOS_MIN_FREQUENCY:\n\t\tret = IS_ERR_OR_NULL(qos) ? PM_QOS_MIN_FREQUENCY_DEFAULT_VALUE\n\t\t\t: freq_qos_read_value(&qos->freq, FREQ_QOS_MIN);\n\t\tbreak;\n\tcase DEV_PM_QOS_MAX_FREQUENCY:\n\t\tret = IS_ERR_OR_NULL(qos) ? PM_QOS_MAX_FREQUENCY_DEFAULT_VALUE\n\t\t\t: freq_qos_read_value(&qos->freq, FREQ_QOS_MAX);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tret = 0;\n\t}\n\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int apply_constraint(struct dev_pm_qos_request *req,\n\t\t\t    enum pm_qos_req_action action, s32 value)\n{\n\tstruct dev_pm_qos *qos = req->dev->power.qos;\n\tint ret;\n\n\tswitch(req->type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\tif (WARN_ON(action != PM_QOS_REMOVE_REQ && value < 0))\n\t\t\tvalue = 0;\n\n\t\tret = pm_qos_update_target(&qos->resume_latency,\n\t\t\t\t\t   &req->data.pnode, action, value);\n\t\tbreak;\n\tcase DEV_PM_QOS_LATENCY_TOLERANCE:\n\t\tret = pm_qos_update_target(&qos->latency_tolerance,\n\t\t\t\t\t   &req->data.pnode, action, value);\n\t\tif (ret) {\n\t\t\tvalue = pm_qos_read_value(&qos->latency_tolerance);\n\t\t\treq->dev->power.set_latency_tolerance(req->dev, value);\n\t\t}\n\t\tbreak;\n\tcase DEV_PM_QOS_MIN_FREQUENCY:\n\tcase DEV_PM_QOS_MAX_FREQUENCY:\n\t\tret = freq_qos_apply(&req->data.freq, action, value);\n\t\tbreak;\n\tcase DEV_PM_QOS_FLAGS:\n\t\tret = pm_qos_update_flags(&qos->flags, &req->data.flr,\n\t\t\t\t\t  action, value);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int dev_pm_qos_constraints_allocate(struct device *dev)\n{\n\tstruct dev_pm_qos *qos;\n\tstruct pm_qos_constraints *c;\n\tstruct blocking_notifier_head *n;\n\n\tqos = kzalloc(sizeof(*qos), GFP_KERNEL);\n\tif (!qos)\n\t\treturn -ENOMEM;\n\n\tn = kzalloc(3 * sizeof(*n), GFP_KERNEL);\n\tif (!n) {\n\t\tkfree(qos);\n\t\treturn -ENOMEM;\n\t}\n\n\tc = &qos->resume_latency;\n\tplist_head_init(&c->list);\n\tc->target_value = PM_QOS_RESUME_LATENCY_DEFAULT_VALUE;\n\tc->default_value = PM_QOS_RESUME_LATENCY_DEFAULT_VALUE;\n\tc->no_constraint_value = PM_QOS_RESUME_LATENCY_NO_CONSTRAINT;\n\tc->type = PM_QOS_MIN;\n\tc->notifiers = n;\n\tBLOCKING_INIT_NOTIFIER_HEAD(n);\n\n\tc = &qos->latency_tolerance;\n\tplist_head_init(&c->list);\n\tc->target_value = PM_QOS_LATENCY_TOLERANCE_DEFAULT_VALUE;\n\tc->default_value = PM_QOS_LATENCY_TOLERANCE_DEFAULT_VALUE;\n\tc->no_constraint_value = PM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT;\n\tc->type = PM_QOS_MIN;\n\n\tfreq_constraints_init(&qos->freq);\n\n\tINIT_LIST_HEAD(&qos->flags.list);\n\n\tspin_lock_irq(&dev->power.lock);\n\tdev->power.qos = qos;\n\tspin_unlock_irq(&dev->power.lock);\n\n\treturn 0;\n}\n\nstatic void __dev_pm_qos_hide_latency_limit(struct device *dev);\nstatic void __dev_pm_qos_hide_flags(struct device *dev);\n\n \nvoid dev_pm_qos_constraints_destroy(struct device *dev)\n{\n\tstruct dev_pm_qos *qos;\n\tstruct dev_pm_qos_request *req, *tmp;\n\tstruct pm_qos_constraints *c;\n\tstruct pm_qos_flags *f;\n\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\n\t \n\tpm_qos_sysfs_remove_resume_latency(dev);\n\tpm_qos_sysfs_remove_flags(dev);\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\t__dev_pm_qos_hide_latency_limit(dev);\n\t__dev_pm_qos_hide_flags(dev);\n\n\tqos = dev->power.qos;\n\tif (!qos)\n\t\tgoto out;\n\n\t \n\tc = &qos->resume_latency;\n\tplist_for_each_entry_safe(req, tmp, &c->list, data.pnode) {\n\t\t \n\t\tapply_constraint(req, PM_QOS_REMOVE_REQ, PM_QOS_DEFAULT_VALUE);\n\t\tmemset(req, 0, sizeof(*req));\n\t}\n\n\tc = &qos->latency_tolerance;\n\tplist_for_each_entry_safe(req, tmp, &c->list, data.pnode) {\n\t\tapply_constraint(req, PM_QOS_REMOVE_REQ, PM_QOS_DEFAULT_VALUE);\n\t\tmemset(req, 0, sizeof(*req));\n\t}\n\n\tc = &qos->freq.min_freq;\n\tplist_for_each_entry_safe(req, tmp, &c->list, data.freq.pnode) {\n\t\tapply_constraint(req, PM_QOS_REMOVE_REQ,\n\t\t\t\t PM_QOS_MIN_FREQUENCY_DEFAULT_VALUE);\n\t\tmemset(req, 0, sizeof(*req));\n\t}\n\n\tc = &qos->freq.max_freq;\n\tplist_for_each_entry_safe(req, tmp, &c->list, data.freq.pnode) {\n\t\tapply_constraint(req, PM_QOS_REMOVE_REQ,\n\t\t\t\t PM_QOS_MAX_FREQUENCY_DEFAULT_VALUE);\n\t\tmemset(req, 0, sizeof(*req));\n\t}\n\n\tf = &qos->flags;\n\tlist_for_each_entry_safe(req, tmp, &f->list, data.flr.node) {\n\t\tapply_constraint(req, PM_QOS_REMOVE_REQ, PM_QOS_DEFAULT_VALUE);\n\t\tmemset(req, 0, sizeof(*req));\n\t}\n\n\tspin_lock_irq(&dev->power.lock);\n\tdev->power.qos = ERR_PTR(-ENODEV);\n\tspin_unlock_irq(&dev->power.lock);\n\n\tkfree(qos->resume_latency.notifiers);\n\tkfree(qos);\n\n out:\n\tmutex_unlock(&dev_pm_qos_mtx);\n\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n}\n\nstatic bool dev_pm_qos_invalid_req_type(struct device *dev,\n\t\t\t\t\tenum dev_pm_qos_req_type type)\n{\n\treturn type == DEV_PM_QOS_LATENCY_TOLERANCE &&\n\t       !dev->power.set_latency_tolerance;\n}\n\nstatic int __dev_pm_qos_add_request(struct device *dev,\n\t\t\t\t    struct dev_pm_qos_request *req,\n\t\t\t\t    enum dev_pm_qos_req_type type, s32 value)\n{\n\tint ret = 0;\n\n\tif (!dev || !req || dev_pm_qos_invalid_req_type(dev, type))\n\t\treturn -EINVAL;\n\n\tif (WARN(dev_pm_qos_request_active(req),\n\t\t \"%s() called for already added request\\n\", __func__))\n\t\treturn -EINVAL;\n\n\tif (IS_ERR(dev->power.qos))\n\t\tret = -ENODEV;\n\telse if (!dev->power.qos)\n\t\tret = dev_pm_qos_constraints_allocate(dev);\n\n\ttrace_dev_pm_qos_add_request(dev_name(dev), type, value);\n\tif (ret)\n\t\treturn ret;\n\n\treq->dev = dev;\n\treq->type = type;\n\tif (req->type == DEV_PM_QOS_MIN_FREQUENCY)\n\t\tret = freq_qos_add_request(&dev->power.qos->freq,\n\t\t\t\t\t   &req->data.freq,\n\t\t\t\t\t   FREQ_QOS_MIN, value);\n\telse if (req->type == DEV_PM_QOS_MAX_FREQUENCY)\n\t\tret = freq_qos_add_request(&dev->power.qos->freq,\n\t\t\t\t\t   &req->data.freq,\n\t\t\t\t\t   FREQ_QOS_MAX, value);\n\telse\n\t\tret = apply_constraint(req, PM_QOS_ADD_REQ, value);\n\n\treturn ret;\n}\n\n \nint dev_pm_qos_add_request(struct device *dev, struct dev_pm_qos_request *req,\n\t\t\t   enum dev_pm_qos_req_type type, s32 value)\n{\n\tint ret;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\tret = __dev_pm_qos_add_request(dev, req, type, value);\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_add_request);\n\n \nstatic int __dev_pm_qos_update_request(struct dev_pm_qos_request *req,\n\t\t\t\t       s32 new_value)\n{\n\ts32 curr_value;\n\tint ret = 0;\n\n\tif (!req)  \n\t\treturn -EINVAL;\n\n\tif (WARN(!dev_pm_qos_request_active(req),\n\t\t \"%s() called for unknown object\\n\", __func__))\n\t\treturn -EINVAL;\n\n\tif (IS_ERR_OR_NULL(req->dev->power.qos))\n\t\treturn -ENODEV;\n\n\tswitch(req->type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\tcase DEV_PM_QOS_LATENCY_TOLERANCE:\n\t\tcurr_value = req->data.pnode.prio;\n\t\tbreak;\n\tcase DEV_PM_QOS_MIN_FREQUENCY:\n\tcase DEV_PM_QOS_MAX_FREQUENCY:\n\t\tcurr_value = req->data.freq.pnode.prio;\n\t\tbreak;\n\tcase DEV_PM_QOS_FLAGS:\n\t\tcurr_value = req->data.flr.flags;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\ttrace_dev_pm_qos_update_request(dev_name(req->dev), req->type,\n\t\t\t\t\tnew_value);\n\tif (curr_value != new_value)\n\t\tret = apply_constraint(req, PM_QOS_UPDATE_REQ, new_value);\n\n\treturn ret;\n}\n\n \nint dev_pm_qos_update_request(struct dev_pm_qos_request *req, s32 new_value)\n{\n\tint ret;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\tret = __dev_pm_qos_update_request(req, new_value);\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_update_request);\n\nstatic int __dev_pm_qos_remove_request(struct dev_pm_qos_request *req)\n{\n\tint ret;\n\n\tif (!req)  \n\t\treturn -EINVAL;\n\n\tif (WARN(!dev_pm_qos_request_active(req),\n\t\t \"%s() called for unknown object\\n\", __func__))\n\t\treturn -EINVAL;\n\n\tif (IS_ERR_OR_NULL(req->dev->power.qos))\n\t\treturn -ENODEV;\n\n\ttrace_dev_pm_qos_remove_request(dev_name(req->dev), req->type,\n\t\t\t\t\tPM_QOS_DEFAULT_VALUE);\n\tret = apply_constraint(req, PM_QOS_REMOVE_REQ, PM_QOS_DEFAULT_VALUE);\n\tmemset(req, 0, sizeof(*req));\n\treturn ret;\n}\n\n \nint dev_pm_qos_remove_request(struct dev_pm_qos_request *req)\n{\n\tint ret;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\tret = __dev_pm_qos_remove_request(req);\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_remove_request);\n\n \nint dev_pm_qos_add_notifier(struct device *dev, struct notifier_block *notifier,\n\t\t\t    enum dev_pm_qos_req_type type)\n{\n\tint ret = 0;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\tif (IS_ERR(dev->power.qos))\n\t\tret = -ENODEV;\n\telse if (!dev->power.qos)\n\t\tret = dev_pm_qos_constraints_allocate(dev);\n\n\tif (ret)\n\t\tgoto unlock;\n\n\tswitch (type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\tret = blocking_notifier_chain_register(dev->power.qos->resume_latency.notifiers,\n\t\t\t\t\t\t       notifier);\n\t\tbreak;\n\tcase DEV_PM_QOS_MIN_FREQUENCY:\n\t\tret = freq_qos_add_notifier(&dev->power.qos->freq,\n\t\t\t\t\t    FREQ_QOS_MIN, notifier);\n\t\tbreak;\n\tcase DEV_PM_QOS_MAX_FREQUENCY:\n\t\tret = freq_qos_add_notifier(&dev->power.qos->freq,\n\t\t\t\t\t    FREQ_QOS_MAX, notifier);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tret = -EINVAL;\n\t}\n\nunlock:\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_add_notifier);\n\n \nint dev_pm_qos_remove_notifier(struct device *dev,\n\t\t\t       struct notifier_block *notifier,\n\t\t\t       enum dev_pm_qos_req_type type)\n{\n\tint ret = 0;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\t \n\tif (IS_ERR_OR_NULL(dev->power.qos))\n\t\tgoto unlock;\n\n\tswitch (type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\tret = blocking_notifier_chain_unregister(dev->power.qos->resume_latency.notifiers,\n\t\t\t\t\t\t\t notifier);\n\t\tbreak;\n\tcase DEV_PM_QOS_MIN_FREQUENCY:\n\t\tret = freq_qos_remove_notifier(&dev->power.qos->freq,\n\t\t\t\t\t       FREQ_QOS_MIN, notifier);\n\t\tbreak;\n\tcase DEV_PM_QOS_MAX_FREQUENCY:\n\t\tret = freq_qos_remove_notifier(&dev->power.qos->freq,\n\t\t\t\t\t       FREQ_QOS_MAX, notifier);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tret = -EINVAL;\n\t}\n\nunlock:\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_remove_notifier);\n\n \nint dev_pm_qos_add_ancestor_request(struct device *dev,\n\t\t\t\t    struct dev_pm_qos_request *req,\n\t\t\t\t    enum dev_pm_qos_req_type type, s32 value)\n{\n\tstruct device *ancestor = dev->parent;\n\tint ret = -ENODEV;\n\n\tswitch (type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\twhile (ancestor && !ancestor->power.ignore_children)\n\t\t\tancestor = ancestor->parent;\n\n\t\tbreak;\n\tcase DEV_PM_QOS_LATENCY_TOLERANCE:\n\t\twhile (ancestor && !ancestor->power.set_latency_tolerance)\n\t\t\tancestor = ancestor->parent;\n\n\t\tbreak;\n\tdefault:\n\t\tancestor = NULL;\n\t}\n\tif (ancestor)\n\t\tret = dev_pm_qos_add_request(ancestor, req, type, value);\n\n\tif (ret < 0)\n\t\treq->dev = NULL;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_add_ancestor_request);\n\nstatic void __dev_pm_qos_drop_user_request(struct device *dev,\n\t\t\t\t\t   enum dev_pm_qos_req_type type)\n{\n\tstruct dev_pm_qos_request *req = NULL;\n\n\tswitch(type) {\n\tcase DEV_PM_QOS_RESUME_LATENCY:\n\t\treq = dev->power.qos->resume_latency_req;\n\t\tdev->power.qos->resume_latency_req = NULL;\n\t\tbreak;\n\tcase DEV_PM_QOS_LATENCY_TOLERANCE:\n\t\treq = dev->power.qos->latency_tolerance_req;\n\t\tdev->power.qos->latency_tolerance_req = NULL;\n\t\tbreak;\n\tcase DEV_PM_QOS_FLAGS:\n\t\treq = dev->power.qos->flags_req;\n\t\tdev->power.qos->flags_req = NULL;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\t__dev_pm_qos_remove_request(req);\n\tkfree(req);\n}\n\nstatic void dev_pm_qos_drop_user_request(struct device *dev,\n\t\t\t\t\t enum dev_pm_qos_req_type type)\n{\n\tmutex_lock(&dev_pm_qos_mtx);\n\t__dev_pm_qos_drop_user_request(dev, type);\n\tmutex_unlock(&dev_pm_qos_mtx);\n}\n\n \nint dev_pm_qos_expose_latency_limit(struct device *dev, s32 value)\n{\n\tstruct dev_pm_qos_request *req;\n\tint ret;\n\n\tif (!device_is_registered(dev) || value < 0)\n\t\treturn -EINVAL;\n\n\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tret = dev_pm_qos_add_request(dev, req, DEV_PM_QOS_RESUME_LATENCY, value);\n\tif (ret < 0) {\n\t\tkfree(req);\n\t\treturn ret;\n\t}\n\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\tif (IS_ERR_OR_NULL(dev->power.qos))\n\t\tret = -ENODEV;\n\telse if (dev->power.qos->resume_latency_req)\n\t\tret = -EEXIST;\n\n\tif (ret < 0) {\n\t\t__dev_pm_qos_remove_request(req);\n\t\tkfree(req);\n\t\tmutex_unlock(&dev_pm_qos_mtx);\n\t\tgoto out;\n\t}\n\tdev->power.qos->resume_latency_req = req;\n\n\tmutex_unlock(&dev_pm_qos_mtx);\n\n\tret = pm_qos_sysfs_add_resume_latency(dev);\n\tif (ret)\n\t\tdev_pm_qos_drop_user_request(dev, DEV_PM_QOS_RESUME_LATENCY);\n\n out:\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_expose_latency_limit);\n\nstatic void __dev_pm_qos_hide_latency_limit(struct device *dev)\n{\n\tif (!IS_ERR_OR_NULL(dev->power.qos) && dev->power.qos->resume_latency_req)\n\t\t__dev_pm_qos_drop_user_request(dev, DEV_PM_QOS_RESUME_LATENCY);\n}\n\n \nvoid dev_pm_qos_hide_latency_limit(struct device *dev)\n{\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\n\tpm_qos_sysfs_remove_resume_latency(dev);\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\t__dev_pm_qos_hide_latency_limit(dev);\n\tmutex_unlock(&dev_pm_qos_mtx);\n\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_hide_latency_limit);\n\n \nint dev_pm_qos_expose_flags(struct device *dev, s32 val)\n{\n\tstruct dev_pm_qos_request *req;\n\tint ret;\n\n\tif (!device_is_registered(dev))\n\t\treturn -EINVAL;\n\n\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tret = dev_pm_qos_add_request(dev, req, DEV_PM_QOS_FLAGS, val);\n\tif (ret < 0) {\n\t\tkfree(req);\n\t\treturn ret;\n\t}\n\n\tpm_runtime_get_sync(dev);\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\tif (IS_ERR_OR_NULL(dev->power.qos))\n\t\tret = -ENODEV;\n\telse if (dev->power.qos->flags_req)\n\t\tret = -EEXIST;\n\n\tif (ret < 0) {\n\t\t__dev_pm_qos_remove_request(req);\n\t\tkfree(req);\n\t\tmutex_unlock(&dev_pm_qos_mtx);\n\t\tgoto out;\n\t}\n\tdev->power.qos->flags_req = req;\n\n\tmutex_unlock(&dev_pm_qos_mtx);\n\n\tret = pm_qos_sysfs_add_flags(dev);\n\tif (ret)\n\t\tdev_pm_qos_drop_user_request(dev, DEV_PM_QOS_FLAGS);\n\n out:\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n\tpm_runtime_put(dev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_expose_flags);\n\nstatic void __dev_pm_qos_hide_flags(struct device *dev)\n{\n\tif (!IS_ERR_OR_NULL(dev->power.qos) && dev->power.qos->flags_req)\n\t\t__dev_pm_qos_drop_user_request(dev, DEV_PM_QOS_FLAGS);\n}\n\n \nvoid dev_pm_qos_hide_flags(struct device *dev)\n{\n\tpm_runtime_get_sync(dev);\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\n\tpm_qos_sysfs_remove_flags(dev);\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\t__dev_pm_qos_hide_flags(dev);\n\tmutex_unlock(&dev_pm_qos_mtx);\n\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n\tpm_runtime_put(dev);\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_hide_flags);\n\n \nint dev_pm_qos_update_flags(struct device *dev, s32 mask, bool set)\n{\n\ts32 value;\n\tint ret;\n\n\tpm_runtime_get_sync(dev);\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\tif (IS_ERR_OR_NULL(dev->power.qos) || !dev->power.qos->flags_req) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tvalue = dev_pm_qos_requested_flags(dev);\n\tif (set)\n\t\tvalue |= mask;\n\telse\n\t\tvalue &= ~mask;\n\n\tret = __dev_pm_qos_update_request(dev->power.qos->flags_req, value);\n\n out:\n\tmutex_unlock(&dev_pm_qos_mtx);\n\tpm_runtime_put(dev);\n\treturn ret;\n}\n\n \ns32 dev_pm_qos_get_user_latency_tolerance(struct device *dev)\n{\n\ts32 ret;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\tret = IS_ERR_OR_NULL(dev->power.qos)\n\t\t|| !dev->power.qos->latency_tolerance_req ?\n\t\t\tPM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT :\n\t\t\tdev->power.qos->latency_tolerance_req->data.pnode.prio;\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\n\n \nint dev_pm_qos_update_user_latency_tolerance(struct device *dev, s32 val)\n{\n\tint ret;\n\n\tmutex_lock(&dev_pm_qos_mtx);\n\n\tif (IS_ERR_OR_NULL(dev->power.qos)\n\t    || !dev->power.qos->latency_tolerance_req) {\n\t\tstruct dev_pm_qos_request *req;\n\n\t\tif (val < 0) {\n\t\t\tif (val == PM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT)\n\t\t\t\tret = 0;\n\t\t\telse\n\t\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\t\tif (!req) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = __dev_pm_qos_add_request(dev, req, DEV_PM_QOS_LATENCY_TOLERANCE, val);\n\t\tif (ret < 0) {\n\t\t\tkfree(req);\n\t\t\tgoto out;\n\t\t}\n\t\tdev->power.qos->latency_tolerance_req = req;\n\t} else {\n\t\tif (val < 0) {\n\t\t\t__dev_pm_qos_drop_user_request(dev, DEV_PM_QOS_LATENCY_TOLERANCE);\n\t\t\tret = 0;\n\t\t} else {\n\t\t\tret = __dev_pm_qos_update_request(dev->power.qos->latency_tolerance_req, val);\n\t\t}\n\t}\n\n out:\n\tmutex_unlock(&dev_pm_qos_mtx);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_update_user_latency_tolerance);\n\n \nint dev_pm_qos_expose_latency_tolerance(struct device *dev)\n{\n\tint ret;\n\n\tif (!dev->power.set_latency_tolerance)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\tret = pm_qos_sysfs_add_latency_tolerance(dev);\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_expose_latency_tolerance);\n\n \nvoid dev_pm_qos_hide_latency_tolerance(struct device *dev)\n{\n\tmutex_lock(&dev_pm_qos_sysfs_mtx);\n\tpm_qos_sysfs_remove_latency_tolerance(dev);\n\tmutex_unlock(&dev_pm_qos_sysfs_mtx);\n\n\t \n\tpm_runtime_get_sync(dev);\n\tdev_pm_qos_update_user_latency_tolerance(dev,\n\t\tPM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT);\n\tpm_runtime_put(dev);\n}\nEXPORT_SYMBOL_GPL(dev_pm_qos_hide_latency_tolerance);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}