{
  "module_name": "memory.c",
  "hash_id": "f1b3e90ccbfbaf56d5b79996d490fb933708b3e145796ef7aa9ff0ebc237404e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/base/memory.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/topology.h>\n#include <linux/capability.h>\n#include <linux/device.h>\n#include <linux/memory.h>\n#include <linux/memory_hotplug.h>\n#include <linux/mm.h>\n#include <linux/stat.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n\n#include <linux/atomic.h>\n#include <linux/uaccess.h>\n\n#define MEMORY_CLASS_NAME\t\"memory\"\n\nstatic const char *const online_type_to_str[] = {\n\t[MMOP_OFFLINE] = \"offline\",\n\t[MMOP_ONLINE] = \"online\",\n\t[MMOP_ONLINE_KERNEL] = \"online_kernel\",\n\t[MMOP_ONLINE_MOVABLE] = \"online_movable\",\n};\n\nint mhp_online_type_from_str(const char *str)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(online_type_to_str); i++) {\n\t\tif (sysfs_streq(str, online_type_to_str[i]))\n\t\t\treturn i;\n\t}\n\treturn -EINVAL;\n}\n\n#define to_memory_block(dev) container_of(dev, struct memory_block, dev)\n\nstatic int sections_per_block;\n\nstatic inline unsigned long memory_block_id(unsigned long section_nr)\n{\n\treturn section_nr / sections_per_block;\n}\n\nstatic inline unsigned long pfn_to_block_id(unsigned long pfn)\n{\n\treturn memory_block_id(pfn_to_section_nr(pfn));\n}\n\nstatic inline unsigned long phys_to_block_id(unsigned long phys)\n{\n\treturn pfn_to_block_id(PFN_DOWN(phys));\n}\n\nstatic int memory_subsys_online(struct device *dev);\nstatic int memory_subsys_offline(struct device *dev);\n\nstatic struct bus_type memory_subsys = {\n\t.name = MEMORY_CLASS_NAME,\n\t.dev_name = MEMORY_CLASS_NAME,\n\t.online = memory_subsys_online,\n\t.offline = memory_subsys_offline,\n};\n\n \nstatic DEFINE_XARRAY(memory_blocks);\n\n \nstatic DEFINE_XARRAY_FLAGS(memory_groups, XA_FLAGS_ALLOC);\n#define MEMORY_GROUP_MARK_DYNAMIC\tXA_MARK_1\n\nstatic BLOCKING_NOTIFIER_HEAD(memory_chain);\n\nint register_memory_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&memory_chain, nb);\n}\nEXPORT_SYMBOL(register_memory_notifier);\n\nvoid unregister_memory_notifier(struct notifier_block *nb)\n{\n\tblocking_notifier_chain_unregister(&memory_chain, nb);\n}\nEXPORT_SYMBOL(unregister_memory_notifier);\n\nstatic void memory_block_release(struct device *dev)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\t \n\tWARN_ON(mem->altmap);\n\tkfree(mem);\n}\n\nunsigned long __weak memory_block_size_bytes(void)\n{\n\treturn MIN_MEMORY_BLOCK_SIZE;\n}\nEXPORT_SYMBOL_GPL(memory_block_size_bytes);\n\n \nstatic ssize_t phys_index_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\n\treturn sysfs_emit(buf, \"%08lx\\n\", memory_block_id(mem->start_section_nr));\n}\n\n \nstatic ssize_t removable_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", (int)IS_ENABLED(CONFIG_MEMORY_HOTREMOVE));\n}\n\n \nstatic ssize_t state_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\tconst char *output;\n\n\t \n\tswitch (mem->state) {\n\tcase MEM_ONLINE:\n\t\toutput = \"online\";\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\toutput = \"offline\";\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\toutput = \"going-offline\";\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn sysfs_emit(buf, \"ERROR-UNKNOWN-%ld\\n\", mem->state);\n\t}\n\n\treturn sysfs_emit(buf, \"%s\\n\", output);\n}\n\nint memory_notify(unsigned long val, void *v)\n{\n\treturn blocking_notifier_call_chain(&memory_chain, val, v);\n}\n\n#if defined(CONFIG_MEMORY_FAILURE) && defined(CONFIG_MEMORY_HOTPLUG)\nstatic unsigned long memblk_nr_poison(struct memory_block *mem);\n#else\nstatic inline unsigned long memblk_nr_poison(struct memory_block *mem)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic int memory_block_online(struct memory_block *mem)\n{\n\tunsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);\n\tunsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;\n\tunsigned long nr_vmemmap_pages = 0;\n\tstruct zone *zone;\n\tint ret;\n\n\tif (memblk_nr_poison(mem))\n\t\treturn -EHWPOISON;\n\n\tzone = zone_for_pfn_range(mem->online_type, mem->nid, mem->group,\n\t\t\t\t  start_pfn, nr_pages);\n\n\t \n\tif (mem->altmap)\n\t\tnr_vmemmap_pages = mem->altmap->free;\n\n\tmem_hotplug_begin();\n\tif (nr_vmemmap_pages) {\n\t\tret = mhp_init_memmap_on_memory(start_pfn, nr_vmemmap_pages, zone);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = online_pages(start_pfn + nr_vmemmap_pages,\n\t\t\t   nr_pages - nr_vmemmap_pages, zone, mem->group);\n\tif (ret) {\n\t\tif (nr_vmemmap_pages)\n\t\t\tmhp_deinit_memmap_on_memory(start_pfn, nr_vmemmap_pages);\n\t\tgoto out;\n\t}\n\n\t \n\tif (nr_vmemmap_pages)\n\t\tadjust_present_page_count(pfn_to_page(start_pfn), mem->group,\n\t\t\t\t\t  nr_vmemmap_pages);\n\n\tmem->zone = zone;\nout:\n\tmem_hotplug_done();\n\treturn ret;\n}\n\n \nstatic int memory_block_offline(struct memory_block *mem)\n{\n\tunsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);\n\tunsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;\n\tunsigned long nr_vmemmap_pages = 0;\n\tint ret;\n\n\tif (!mem->zone)\n\t\treturn -EINVAL;\n\n\t \n\tif (mem->altmap)\n\t\tnr_vmemmap_pages = mem->altmap->free;\n\n\tmem_hotplug_begin();\n\tif (nr_vmemmap_pages)\n\t\tadjust_present_page_count(pfn_to_page(start_pfn), mem->group,\n\t\t\t\t\t  -nr_vmemmap_pages);\n\n\tret = offline_pages(start_pfn + nr_vmemmap_pages,\n\t\t\t    nr_pages - nr_vmemmap_pages, mem->zone, mem->group);\n\tif (ret) {\n\t\t \n\t\tif (nr_vmemmap_pages)\n\t\t\tadjust_present_page_count(pfn_to_page(start_pfn),\n\t\t\t\t\t\t  mem->group, nr_vmemmap_pages);\n\t\tgoto out;\n\t}\n\n\tif (nr_vmemmap_pages)\n\t\tmhp_deinit_memmap_on_memory(start_pfn, nr_vmemmap_pages);\n\n\tmem->zone = NULL;\nout:\n\tmem_hotplug_done();\n\treturn ret;\n}\n\n \nstatic int\nmemory_block_action(struct memory_block *mem, unsigned long action)\n{\n\tint ret;\n\n\tswitch (action) {\n\tcase MEM_ONLINE:\n\t\tret = memory_block_online(mem);\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\tret = memory_block_offline(mem);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, KERN_WARNING \"%s(%ld, %ld) unknown action: \"\n\t\t     \"%ld\\n\", __func__, mem->start_section_nr, action, action);\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int memory_block_change_state(struct memory_block *mem,\n\t\tunsigned long to_state, unsigned long from_state_req)\n{\n\tint ret = 0;\n\n\tif (mem->state != from_state_req)\n\t\treturn -EINVAL;\n\n\tif (to_state == MEM_OFFLINE)\n\t\tmem->state = MEM_GOING_OFFLINE;\n\n\tret = memory_block_action(mem, to_state);\n\tmem->state = ret ? from_state_req : to_state;\n\n\treturn ret;\n}\n\n \nstatic int memory_subsys_online(struct device *dev)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\tint ret;\n\n\tif (mem->state == MEM_ONLINE)\n\t\treturn 0;\n\n\t \n\tif (mem->online_type == MMOP_OFFLINE)\n\t\tmem->online_type = MMOP_ONLINE;\n\n\tret = memory_block_change_state(mem, MEM_ONLINE, MEM_OFFLINE);\n\tmem->online_type = MMOP_OFFLINE;\n\n\treturn ret;\n}\n\nstatic int memory_subsys_offline(struct device *dev)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\n\tif (mem->state == MEM_OFFLINE)\n\t\treturn 0;\n\n\treturn memory_block_change_state(mem, MEM_OFFLINE, MEM_ONLINE);\n}\n\nstatic ssize_t state_store(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tconst int online_type = mhp_online_type_from_str(buf);\n\tstruct memory_block *mem = to_memory_block(dev);\n\tint ret;\n\n\tif (online_type < 0)\n\t\treturn -EINVAL;\n\n\tret = lock_device_hotplug_sysfs();\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (online_type) {\n\tcase MMOP_ONLINE_KERNEL:\n\tcase MMOP_ONLINE_MOVABLE:\n\tcase MMOP_ONLINE:\n\t\t \n\t\tmem->online_type = online_type;\n\t\tret = device_online(&mem->dev);\n\t\tbreak;\n\tcase MMOP_OFFLINE:\n\t\tret = device_offline(&mem->dev);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;  \n\t}\n\n\tunlock_device_hotplug();\n\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret)\n\t\treturn -EINVAL;\n\n\treturn count;\n}\n\n \nstatic ssize_t phys_device_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\tunsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);\n\n\treturn sysfs_emit(buf, \"%d\\n\",\n\t\t\t  arch_get_memory_phys_device(start_pfn));\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\nstatic int print_allowed_zone(char *buf, int len, int nid,\n\t\t\t      struct memory_group *group,\n\t\t\t      unsigned long start_pfn, unsigned long nr_pages,\n\t\t\t      int online_type, struct zone *default_zone)\n{\n\tstruct zone *zone;\n\n\tzone = zone_for_pfn_range(online_type, nid, group, start_pfn, nr_pages);\n\tif (zone == default_zone)\n\t\treturn 0;\n\n\treturn sysfs_emit_at(buf, len, \" %s\", zone->name);\n}\n\nstatic ssize_t valid_zones_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\tunsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);\n\tunsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;\n\tstruct memory_group *group = mem->group;\n\tstruct zone *default_zone;\n\tint nid = mem->nid;\n\tint len = 0;\n\n\t \n\tif (mem->state == MEM_ONLINE) {\n\t\t \n\t\tdefault_zone = mem->zone;\n\t\tif (!default_zone)\n\t\t\treturn sysfs_emit(buf, \"%s\\n\", \"none\");\n\t\tlen += sysfs_emit_at(buf, len, \"%s\", default_zone->name);\n\t\tgoto out;\n\t}\n\n\tdefault_zone = zone_for_pfn_range(MMOP_ONLINE, nid, group,\n\t\t\t\t\t  start_pfn, nr_pages);\n\n\tlen += sysfs_emit_at(buf, len, \"%s\", default_zone->name);\n\tlen += print_allowed_zone(buf, len, nid, group, start_pfn, nr_pages,\n\t\t\t\t  MMOP_ONLINE_KERNEL, default_zone);\n\tlen += print_allowed_zone(buf, len, nid, group, start_pfn, nr_pages,\n\t\t\t\t  MMOP_ONLINE_MOVABLE, default_zone);\nout:\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\treturn len;\n}\nstatic DEVICE_ATTR_RO(valid_zones);\n#endif\n\nstatic DEVICE_ATTR_RO(phys_index);\nstatic DEVICE_ATTR_RW(state);\nstatic DEVICE_ATTR_RO(phys_device);\nstatic DEVICE_ATTR_RO(removable);\n\n \nstatic ssize_t block_size_bytes_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lx\\n\", memory_block_size_bytes());\n}\n\nstatic DEVICE_ATTR_RO(block_size_bytes);\n\n \n\nstatic ssize_t auto_online_blocks_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  online_type_to_str[mhp_default_online_type]);\n}\n\nstatic ssize_t auto_online_blocks_store(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tconst int online_type = mhp_online_type_from_str(buf);\n\n\tif (online_type < 0)\n\t\treturn -EINVAL;\n\n\tmhp_default_online_type = online_type;\n\treturn count;\n}\n\nstatic DEVICE_ATTR_RW(auto_online_blocks);\n\n#ifdef CONFIG_CRASH_HOTPLUG\n#include <linux/kexec.h>\nstatic ssize_t crash_hotplug_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", crash_hotplug_memory_support());\n}\nstatic DEVICE_ATTR_RO(crash_hotplug);\n#endif\n\n \n#ifdef CONFIG_ARCH_MEMORY_PROBE\nstatic ssize_t probe_store(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tu64 phys_addr;\n\tint nid, ret;\n\tunsigned long pages_per_block = PAGES_PER_SECTION * sections_per_block;\n\n\tret = kstrtoull(buf, 0, &phys_addr);\n\tif (ret)\n\t\treturn ret;\n\n\tif (phys_addr & ((pages_per_block << PAGE_SHIFT) - 1))\n\t\treturn -EINVAL;\n\n\tret = lock_device_hotplug_sysfs();\n\tif (ret)\n\t\treturn ret;\n\n\tnid = memory_add_physaddr_to_nid(phys_addr);\n\tret = __add_memory(nid, phys_addr,\n\t\t\t   MIN_MEMORY_BLOCK_SIZE * sections_per_block,\n\t\t\t   MHP_NONE);\n\n\tif (ret)\n\t\tgoto out;\n\n\tret = count;\nout:\n\tunlock_device_hotplug();\n\treturn ret;\n}\n\nstatic DEVICE_ATTR_WO(probe);\n#endif\n\n#ifdef CONFIG_MEMORY_FAILURE\n \n\n \nstatic ssize_t soft_offline_page_store(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tint ret;\n\tu64 pfn;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tif (kstrtoull(buf, 0, &pfn) < 0)\n\t\treturn -EINVAL;\n\tpfn >>= PAGE_SHIFT;\n\tret = soft_offline_page(pfn, 0);\n\treturn ret == 0 ? count : ret;\n}\n\n \nstatic ssize_t hard_offline_page_store(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tint ret;\n\tu64 pfn;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tif (kstrtoull(buf, 0, &pfn) < 0)\n\t\treturn -EINVAL;\n\tpfn >>= PAGE_SHIFT;\n\tret = memory_failure(pfn, MF_SW_SIMULATED);\n\tif (ret == -EOPNOTSUPP)\n\t\tret = 0;\n\treturn ret ? ret : count;\n}\n\nstatic DEVICE_ATTR_WO(soft_offline_page);\nstatic DEVICE_ATTR_WO(hard_offline_page);\n#endif\n\n \nint __weak arch_get_memory_phys_device(unsigned long start_pfn)\n{\n\treturn 0;\n}\n\n \nstatic struct memory_block *find_memory_block_by_id(unsigned long block_id)\n{\n\tstruct memory_block *mem;\n\n\tmem = xa_load(&memory_blocks, block_id);\n\tif (mem)\n\t\tget_device(&mem->dev);\n\treturn mem;\n}\n\n \nstruct memory_block *find_memory_block(unsigned long section_nr)\n{\n\tunsigned long block_id = memory_block_id(section_nr);\n\n\treturn find_memory_block_by_id(block_id);\n}\n\nstatic struct attribute *memory_memblk_attrs[] = {\n\t&dev_attr_phys_index.attr,\n\t&dev_attr_state.attr,\n\t&dev_attr_phys_device.attr,\n\t&dev_attr_removable.attr,\n#ifdef CONFIG_MEMORY_HOTREMOVE\n\t&dev_attr_valid_zones.attr,\n#endif\n\tNULL\n};\n\nstatic const struct attribute_group memory_memblk_attr_group = {\n\t.attrs = memory_memblk_attrs,\n};\n\nstatic const struct attribute_group *memory_memblk_attr_groups[] = {\n\t&memory_memblk_attr_group,\n\tNULL,\n};\n\nstatic int __add_memory_block(struct memory_block *memory)\n{\n\tint ret;\n\n\tmemory->dev.bus = &memory_subsys;\n\tmemory->dev.id = memory->start_section_nr / sections_per_block;\n\tmemory->dev.release = memory_block_release;\n\tmemory->dev.groups = memory_memblk_attr_groups;\n\tmemory->dev.offline = memory->state == MEM_OFFLINE;\n\n\tret = device_register(&memory->dev);\n\tif (ret) {\n\t\tput_device(&memory->dev);\n\t\treturn ret;\n\t}\n\tret = xa_err(xa_store(&memory_blocks, memory->dev.id, memory,\n\t\t\t      GFP_KERNEL));\n\tif (ret)\n\t\tdevice_unregister(&memory->dev);\n\n\treturn ret;\n}\n\nstatic struct zone *early_node_zone_for_memory_block(struct memory_block *mem,\n\t\t\t\t\t\t     int nid)\n{\n\tconst unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);\n\tconst unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;\n\tstruct zone *zone, *matching_zone = NULL;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tint i;\n\n\t \n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tzone = pgdat->node_zones + i;\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\t\tif (!zone_intersects(zone, start_pfn, nr_pages))\n\t\t\tcontinue;\n\t\tif (!matching_zone) {\n\t\t\tmatching_zone = zone;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tmatching_zone = NULL;\n\t\tbreak;\n\t}\n\treturn matching_zone;\n}\n\n#ifdef CONFIG_NUMA\n \nvoid memory_block_add_nid(struct memory_block *mem, int nid,\n\t\t\t  enum meminit_context context)\n{\n\tif (context == MEMINIT_EARLY && mem->nid != nid) {\n\t\t \n\t\tif (mem->nid == NUMA_NO_NODE)\n\t\t\tmem->zone = early_node_zone_for_memory_block(mem, nid);\n\t\telse\n\t\t\tmem->zone = NULL;\n\t}\n\n\t \n\tmem->nid = nid;\n}\n#endif\n\nstatic int add_memory_block(unsigned long block_id, unsigned long state,\n\t\t\t    struct vmem_altmap *altmap,\n\t\t\t    struct memory_group *group)\n{\n\tstruct memory_block *mem;\n\tint ret = 0;\n\n\tmem = find_memory_block_by_id(block_id);\n\tif (mem) {\n\t\tput_device(&mem->dev);\n\t\treturn -EEXIST;\n\t}\n\tmem = kzalloc(sizeof(*mem), GFP_KERNEL);\n\tif (!mem)\n\t\treturn -ENOMEM;\n\n\tmem->start_section_nr = block_id * sections_per_block;\n\tmem->state = state;\n\tmem->nid = NUMA_NO_NODE;\n\tmem->altmap = altmap;\n\tINIT_LIST_HEAD(&mem->group_next);\n\n#ifndef CONFIG_NUMA\n\tif (state == MEM_ONLINE)\n\t\t \n\t\tmem->zone = early_node_zone_for_memory_block(mem, NUMA_NO_NODE);\n#endif  \n\n\tret = __add_memory_block(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tif (group) {\n\t\tmem->group = group;\n\t\tlist_add(&mem->group_next, &group->memory_blocks);\n\t}\n\n\treturn 0;\n}\n\nstatic int __init add_boot_memory_block(unsigned long base_section_nr)\n{\n\tint section_count = 0;\n\tunsigned long nr;\n\n\tfor (nr = base_section_nr; nr < base_section_nr + sections_per_block;\n\t     nr++)\n\t\tif (present_section_nr(nr))\n\t\t\tsection_count++;\n\n\tif (section_count == 0)\n\t\treturn 0;\n\treturn add_memory_block(memory_block_id(base_section_nr),\n\t\t\t\tMEM_ONLINE, NULL,  NULL);\n}\n\nstatic int add_hotplug_memory_block(unsigned long block_id,\n\t\t\t\t    struct vmem_altmap *altmap,\n\t\t\t\t    struct memory_group *group)\n{\n\treturn add_memory_block(block_id, MEM_OFFLINE, altmap, group);\n}\n\nstatic void remove_memory_block(struct memory_block *memory)\n{\n\tif (WARN_ON_ONCE(memory->dev.bus != &memory_subsys))\n\t\treturn;\n\n\tWARN_ON(xa_erase(&memory_blocks, memory->dev.id) == NULL);\n\n\tif (memory->group) {\n\t\tlist_del(&memory->group_next);\n\t\tmemory->group = NULL;\n\t}\n\n\t \n\tput_device(&memory->dev);\n\tdevice_unregister(&memory->dev);\n}\n\n \nint create_memory_block_devices(unsigned long start, unsigned long size,\n\t\t\t\tstruct vmem_altmap *altmap,\n\t\t\t\tstruct memory_group *group)\n{\n\tconst unsigned long start_block_id = pfn_to_block_id(PFN_DOWN(start));\n\tunsigned long end_block_id = pfn_to_block_id(PFN_DOWN(start + size));\n\tstruct memory_block *mem;\n\tunsigned long block_id;\n\tint ret = 0;\n\n\tif (WARN_ON_ONCE(!IS_ALIGNED(start, memory_block_size_bytes()) ||\n\t\t\t !IS_ALIGNED(size, memory_block_size_bytes())))\n\t\treturn -EINVAL;\n\n\tfor (block_id = start_block_id; block_id != end_block_id; block_id++) {\n\t\tret = add_hotplug_memory_block(block_id, altmap, group);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (ret) {\n\t\tend_block_id = block_id;\n\t\tfor (block_id = start_block_id; block_id != end_block_id;\n\t\t     block_id++) {\n\t\t\tmem = find_memory_block_by_id(block_id);\n\t\t\tif (WARN_ON_ONCE(!mem))\n\t\t\t\tcontinue;\n\t\t\tremove_memory_block(mem);\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nvoid remove_memory_block_devices(unsigned long start, unsigned long size)\n{\n\tconst unsigned long start_block_id = pfn_to_block_id(PFN_DOWN(start));\n\tconst unsigned long end_block_id = pfn_to_block_id(PFN_DOWN(start + size));\n\tstruct memory_block *mem;\n\tunsigned long block_id;\n\n\tif (WARN_ON_ONCE(!IS_ALIGNED(start, memory_block_size_bytes()) ||\n\t\t\t !IS_ALIGNED(size, memory_block_size_bytes())))\n\t\treturn;\n\n\tfor (block_id = start_block_id; block_id != end_block_id; block_id++) {\n\t\tmem = find_memory_block_by_id(block_id);\n\t\tif (WARN_ON_ONCE(!mem))\n\t\t\tcontinue;\n\t\tnum_poisoned_pages_sub(-1UL, memblk_nr_poison(mem));\n\t\tunregister_memory_block_under_nodes(mem);\n\t\tremove_memory_block(mem);\n\t}\n}\n\nstatic struct attribute *memory_root_attrs[] = {\n#ifdef CONFIG_ARCH_MEMORY_PROBE\n\t&dev_attr_probe.attr,\n#endif\n\n#ifdef CONFIG_MEMORY_FAILURE\n\t&dev_attr_soft_offline_page.attr,\n\t&dev_attr_hard_offline_page.attr,\n#endif\n\n\t&dev_attr_block_size_bytes.attr,\n\t&dev_attr_auto_online_blocks.attr,\n#ifdef CONFIG_CRASH_HOTPLUG\n\t&dev_attr_crash_hotplug.attr,\n#endif\n\tNULL\n};\n\nstatic const struct attribute_group memory_root_attr_group = {\n\t.attrs = memory_root_attrs,\n};\n\nstatic const struct attribute_group *memory_root_attr_groups[] = {\n\t&memory_root_attr_group,\n\tNULL,\n};\n\n \nvoid __init memory_dev_init(void)\n{\n\tint ret;\n\tunsigned long block_sz, nr;\n\n\t \n\tblock_sz = memory_block_size_bytes();\n\tif (!is_power_of_2(block_sz) || block_sz < MIN_MEMORY_BLOCK_SIZE)\n\t\tpanic(\"Memory block size not suitable: 0x%lx\\n\", block_sz);\n\tsections_per_block = block_sz / MIN_MEMORY_BLOCK_SIZE;\n\n\tret = subsys_system_register(&memory_subsys, memory_root_attr_groups);\n\tif (ret)\n\t\tpanic(\"%s() failed to register subsystem: %d\\n\", __func__, ret);\n\n\t \n\tfor (nr = 0; nr <= __highest_present_section_nr;\n\t     nr += sections_per_block) {\n\t\tret = add_boot_memory_block(nr);\n\t\tif (ret)\n\t\t\tpanic(\"%s() failed to add memory block: %d\\n\", __func__,\n\t\t\t      ret);\n\t}\n}\n\n \nint walk_memory_blocks(unsigned long start, unsigned long size,\n\t\t       void *arg, walk_memory_blocks_func_t func)\n{\n\tconst unsigned long start_block_id = phys_to_block_id(start);\n\tconst unsigned long end_block_id = phys_to_block_id(start + size - 1);\n\tstruct memory_block *mem;\n\tunsigned long block_id;\n\tint ret = 0;\n\n\tif (!size)\n\t\treturn 0;\n\n\tfor (block_id = start_block_id; block_id <= end_block_id; block_id++) {\n\t\tmem = find_memory_block_by_id(block_id);\n\t\tif (!mem)\n\t\t\tcontinue;\n\n\t\tret = func(mem, arg);\n\t\tput_device(&mem->dev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstruct for_each_memory_block_cb_data {\n\twalk_memory_blocks_func_t func;\n\tvoid *arg;\n};\n\nstatic int for_each_memory_block_cb(struct device *dev, void *data)\n{\n\tstruct memory_block *mem = to_memory_block(dev);\n\tstruct for_each_memory_block_cb_data *cb_data = data;\n\n\treturn cb_data->func(mem, cb_data->arg);\n}\n\n \nint for_each_memory_block(void *arg, walk_memory_blocks_func_t func)\n{\n\tstruct for_each_memory_block_cb_data cb_data = {\n\t\t.func = func,\n\t\t.arg = arg,\n\t};\n\n\treturn bus_for_each_dev(&memory_subsys, NULL, &cb_data,\n\t\t\t\tfor_each_memory_block_cb);\n}\n\n \nstatic int memory_group_register(struct memory_group group)\n{\n\tstruct memory_group *new_group;\n\tuint32_t mgid;\n\tint ret;\n\n\tif (!node_possible(group.nid))\n\t\treturn -EINVAL;\n\n\tnew_group = kzalloc(sizeof(group), GFP_KERNEL);\n\tif (!new_group)\n\t\treturn -ENOMEM;\n\t*new_group = group;\n\tINIT_LIST_HEAD(&new_group->memory_blocks);\n\n\tret = xa_alloc(&memory_groups, &mgid, new_group, xa_limit_31b,\n\t\t       GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(new_group);\n\t\treturn ret;\n\t} else if (group.is_dynamic) {\n\t\txa_set_mark(&memory_groups, mgid, MEMORY_GROUP_MARK_DYNAMIC);\n\t}\n\treturn mgid;\n}\n\n \nint memory_group_register_static(int nid, unsigned long max_pages)\n{\n\tstruct memory_group group = {\n\t\t.nid = nid,\n\t\t.s = {\n\t\t\t.max_pages = max_pages,\n\t\t},\n\t};\n\n\tif (!max_pages)\n\t\treturn -EINVAL;\n\treturn memory_group_register(group);\n}\nEXPORT_SYMBOL_GPL(memory_group_register_static);\n\n \nint memory_group_register_dynamic(int nid, unsigned long unit_pages)\n{\n\tstruct memory_group group = {\n\t\t.nid = nid,\n\t\t.is_dynamic = true,\n\t\t.d = {\n\t\t\t.unit_pages = unit_pages,\n\t\t},\n\t};\n\n\tif (!unit_pages || !is_power_of_2(unit_pages) ||\n\t    unit_pages < PHYS_PFN(memory_block_size_bytes()))\n\t\treturn -EINVAL;\n\treturn memory_group_register(group);\n}\nEXPORT_SYMBOL_GPL(memory_group_register_dynamic);\n\n \nint memory_group_unregister(int mgid)\n{\n\tstruct memory_group *group;\n\n\tif (mgid < 0)\n\t\treturn -EINVAL;\n\n\tgroup = xa_load(&memory_groups, mgid);\n\tif (!group)\n\t\treturn -EINVAL;\n\tif (!list_empty(&group->memory_blocks))\n\t\treturn -EBUSY;\n\txa_erase(&memory_groups, mgid);\n\tkfree(group);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(memory_group_unregister);\n\n \nstruct memory_group *memory_group_find_by_id(int mgid)\n{\n\treturn xa_load(&memory_groups, mgid);\n}\n\n \nint walk_dynamic_memory_groups(int nid, walk_memory_groups_func_t func,\n\t\t\t       struct memory_group *excluded, void *arg)\n{\n\tstruct memory_group *group;\n\tunsigned long index;\n\tint ret = 0;\n\n\txa_for_each_marked(&memory_groups, index, group,\n\t\t\t   MEMORY_GROUP_MARK_DYNAMIC) {\n\t\tif (group == excluded)\n\t\t\tcontinue;\n#ifdef CONFIG_NUMA\n\t\tif (nid != NUMA_NO_NODE && group->nid != nid)\n\t\t\tcontinue;\n#endif  \n\t\tret = func(group, arg);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#if defined(CONFIG_MEMORY_FAILURE) && defined(CONFIG_MEMORY_HOTPLUG)\nvoid memblk_nr_poison_inc(unsigned long pfn)\n{\n\tconst unsigned long block_id = pfn_to_block_id(pfn);\n\tstruct memory_block *mem = find_memory_block_by_id(block_id);\n\n\tif (mem)\n\t\tatomic_long_inc(&mem->nr_hwpoison);\n}\n\nvoid memblk_nr_poison_sub(unsigned long pfn, long i)\n{\n\tconst unsigned long block_id = pfn_to_block_id(pfn);\n\tstruct memory_block *mem = find_memory_block_by_id(block_id);\n\n\tif (mem)\n\t\tatomic_long_sub(i, &mem->nr_hwpoison);\n}\n\nstatic unsigned long memblk_nr_poison(struct memory_block *mem)\n{\n\treturn atomic_long_read(&mem->nr_hwpoison);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}