{
  "module_name": "coupled.c",
  "hash_id": "8a4a4d86b62f56273b2248f69c3e2e09f66767689e0557ede2331e20d8823293",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cpuidle/coupled.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/cpu.h>\n#include <linux/cpuidle.h>\n#include <linux/mutex.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n#include \"cpuidle.h\"\n\n \n\n \nstruct cpuidle_coupled {\n\tcpumask_t coupled_cpus;\n\tint requested_state[NR_CPUS];\n\tatomic_t ready_waiting_counts;\n\tatomic_t abort_barrier;\n\tint online_count;\n\tint refcnt;\n\tint prevent;\n};\n\n#define WAITING_BITS 16\n#define MAX_WAITING_CPUS (1 << WAITING_BITS)\n#define WAITING_MASK (MAX_WAITING_CPUS - 1)\n#define READY_MASK (~WAITING_MASK)\n\n#define CPUIDLE_COUPLED_NOT_IDLE\t(-1)\n\nstatic DEFINE_PER_CPU(call_single_data_t, cpuidle_coupled_poke_cb);\n\n \nstatic cpumask_t cpuidle_coupled_poke_pending;\n\n \nstatic cpumask_t cpuidle_coupled_poked;\n\n \nvoid cpuidle_coupled_parallel_barrier(struct cpuidle_device *dev, atomic_t *a)\n{\n\tint n = dev->coupled->online_count;\n\n\tsmp_mb__before_atomic();\n\tatomic_inc(a);\n\n\twhile (atomic_read(a) < n)\n\t\tcpu_relax();\n\n\tif (atomic_inc_return(a) == n * 2) {\n\t\tatomic_set(a, 0);\n\t\treturn;\n\t}\n\n\twhile (atomic_read(a) > n)\n\t\tcpu_relax();\n}\n\n \nbool cpuidle_state_is_coupled(struct cpuidle_driver *drv, int state)\n{\n\treturn drv->states[state].flags & CPUIDLE_FLAG_COUPLED;\n}\n\n \nint cpuidle_coupled_state_verify(struct cpuidle_driver *drv)\n{\n\tint i;\n\n\tfor (i = drv->state_count - 1; i >= 0; i--) {\n\t\tif (cpuidle_state_is_coupled(drv, i) &&\n\t\t    (drv->safe_state_index == i ||\n\t\t     drv->safe_state_index < 0 ||\n\t\t     drv->safe_state_index >= drv->state_count))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline void cpuidle_coupled_set_ready(struct cpuidle_coupled *coupled)\n{\n\tatomic_add(MAX_WAITING_CPUS, &coupled->ready_waiting_counts);\n}\n\n \nstatic\ninline int cpuidle_coupled_set_not_ready(struct cpuidle_coupled *coupled)\n{\n\tint all;\n\tint ret;\n\n\tall = coupled->online_count | (coupled->online_count << WAITING_BITS);\n\tret = atomic_add_unless(&coupled->ready_waiting_counts,\n\t\t-MAX_WAITING_CPUS, all);\n\n\treturn ret ? 0 : -EINVAL;\n}\n\n \nstatic inline int cpuidle_coupled_no_cpus_ready(struct cpuidle_coupled *coupled)\n{\n\tint r = atomic_read(&coupled->ready_waiting_counts) >> WAITING_BITS;\n\treturn r == 0;\n}\n\n \nstatic inline bool cpuidle_coupled_cpus_ready(struct cpuidle_coupled *coupled)\n{\n\tint r = atomic_read(&coupled->ready_waiting_counts) >> WAITING_BITS;\n\treturn r == coupled->online_count;\n}\n\n \nstatic inline bool cpuidle_coupled_cpus_waiting(struct cpuidle_coupled *coupled)\n{\n\tint w = atomic_read(&coupled->ready_waiting_counts) & WAITING_MASK;\n\treturn w == coupled->online_count;\n}\n\n \nstatic inline int cpuidle_coupled_no_cpus_waiting(struct cpuidle_coupled *coupled)\n{\n\tint w = atomic_read(&coupled->ready_waiting_counts) & WAITING_MASK;\n\treturn w == 0;\n}\n\n \nstatic inline int cpuidle_coupled_get_state(struct cpuidle_device *dev,\n\t\tstruct cpuidle_coupled *coupled)\n{\n\tint i;\n\tint state = INT_MAX;\n\n\t \n\tsmp_rmb();\n\n\tfor_each_cpu(i, &coupled->coupled_cpus)\n\t\tif (cpu_online(i) && coupled->requested_state[i] < state)\n\t\t\tstate = coupled->requested_state[i];\n\n\treturn state;\n}\n\nstatic void cpuidle_coupled_handle_poke(void *info)\n{\n\tint cpu = (unsigned long)info;\n\tcpumask_set_cpu(cpu, &cpuidle_coupled_poked);\n\tcpumask_clear_cpu(cpu, &cpuidle_coupled_poke_pending);\n}\n\n \nstatic void cpuidle_coupled_poke(int cpu)\n{\n\tcall_single_data_t *csd = &per_cpu(cpuidle_coupled_poke_cb, cpu);\n\n\tif (!cpumask_test_and_set_cpu(cpu, &cpuidle_coupled_poke_pending))\n\t\tsmp_call_function_single_async(cpu, csd);\n}\n\n \nstatic void cpuidle_coupled_poke_others(int this_cpu,\n\t\tstruct cpuidle_coupled *coupled)\n{\n\tint cpu;\n\n\tfor_each_cpu(cpu, &coupled->coupled_cpus)\n\t\tif (cpu != this_cpu && cpu_online(cpu))\n\t\t\tcpuidle_coupled_poke(cpu);\n}\n\n \nstatic int cpuidle_coupled_set_waiting(int cpu,\n\t\tstruct cpuidle_coupled *coupled, int next_state)\n{\n\tcoupled->requested_state[cpu] = next_state;\n\n\t \n\treturn atomic_inc_return(&coupled->ready_waiting_counts) & WAITING_MASK;\n}\n\n \nstatic void cpuidle_coupled_set_not_waiting(int cpu,\n\t\tstruct cpuidle_coupled *coupled)\n{\n\t \n\tatomic_dec(&coupled->ready_waiting_counts);\n\n\tcoupled->requested_state[cpu] = CPUIDLE_COUPLED_NOT_IDLE;\n}\n\n \nstatic void cpuidle_coupled_set_done(int cpu, struct cpuidle_coupled *coupled)\n{\n\tcpuidle_coupled_set_not_waiting(cpu, coupled);\n\tatomic_sub(MAX_WAITING_CPUS, &coupled->ready_waiting_counts);\n}\n\n \nstatic int cpuidle_coupled_clear_pokes(int cpu)\n{\n\tif (!cpumask_test_cpu(cpu, &cpuidle_coupled_poke_pending))\n\t\treturn 0;\n\n\tlocal_irq_enable();\n\twhile (cpumask_test_cpu(cpu, &cpuidle_coupled_poke_pending))\n\t\tcpu_relax();\n\tlocal_irq_disable();\n\n\treturn 1;\n}\n\nstatic bool cpuidle_coupled_any_pokes_pending(struct cpuidle_coupled *coupled)\n{\n\tcpumask_t cpus;\n\tint ret;\n\n\tcpumask_and(&cpus, cpu_online_mask, &coupled->coupled_cpus);\n\tret = cpumask_and(&cpus, &cpuidle_coupled_poke_pending, &cpus);\n\n\treturn ret;\n}\n\n \nint cpuidle_enter_state_coupled(struct cpuidle_device *dev,\n\t\tstruct cpuidle_driver *drv, int next_state)\n{\n\tint entered_state = -1;\n\tstruct cpuidle_coupled *coupled = dev->coupled;\n\tint w;\n\n\tif (!coupled)\n\t\treturn -EINVAL;\n\n\twhile (coupled->prevent) {\n\t\tcpuidle_coupled_clear_pokes(dev->cpu);\n\t\tif (need_resched()) {\n\t\t\tlocal_irq_enable();\n\t\t\treturn entered_state;\n\t\t}\n\t\tentered_state = cpuidle_enter_state(dev, drv,\n\t\t\tdrv->safe_state_index);\n\t\tlocal_irq_disable();\n\t}\n\n\t \n\tsmp_rmb();\n\nreset:\n\tcpumask_clear_cpu(dev->cpu, &cpuidle_coupled_poked);\n\n\tw = cpuidle_coupled_set_waiting(dev->cpu, coupled, next_state);\n\t \n\tif (w == coupled->online_count) {\n\t\tcpumask_set_cpu(dev->cpu, &cpuidle_coupled_poked);\n\t\tcpuidle_coupled_poke_others(dev->cpu, coupled);\n\t}\n\nretry:\n\t \n\twhile (!cpuidle_coupled_cpus_waiting(coupled) ||\n\t\t\t!cpumask_test_cpu(dev->cpu, &cpuidle_coupled_poked)) {\n\t\tif (cpuidle_coupled_clear_pokes(dev->cpu))\n\t\t\tcontinue;\n\n\t\tif (need_resched()) {\n\t\t\tcpuidle_coupled_set_not_waiting(dev->cpu, coupled);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (coupled->prevent) {\n\t\t\tcpuidle_coupled_set_not_waiting(dev->cpu, coupled);\n\t\t\tgoto out;\n\t\t}\n\n\t\tentered_state = cpuidle_enter_state(dev, drv,\n\t\t\tdrv->safe_state_index);\n\t\tlocal_irq_disable();\n\t}\n\n\tcpuidle_coupled_clear_pokes(dev->cpu);\n\tif (need_resched()) {\n\t\tcpuidle_coupled_set_not_waiting(dev->cpu, coupled);\n\t\tgoto out;\n\t}\n\n\t \n\tsmp_wmb();\n\n\t \n\n\tcpuidle_coupled_set_ready(coupled);\n\twhile (!cpuidle_coupled_cpus_ready(coupled)) {\n\t\t \n\t\tif (!cpuidle_coupled_cpus_waiting(coupled))\n\t\t\tif (!cpuidle_coupled_set_not_ready(coupled))\n\t\t\t\tgoto retry;\n\n\t\tcpu_relax();\n\t}\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (cpuidle_coupled_any_pokes_pending(coupled)) {\n\t\tcpuidle_coupled_set_done(dev->cpu, coupled);\n\t\t \n\t\tcpuidle_coupled_parallel_barrier(dev, &coupled->abort_barrier);\n\t\tgoto reset;\n\t}\n\n\t \n\tnext_state = cpuidle_coupled_get_state(dev, coupled);\n\n\tentered_state = cpuidle_enter_state(dev, drv, next_state);\n\n\tcpuidle_coupled_set_done(dev->cpu, coupled);\n\nout:\n\t \n\tlocal_irq_enable();\n\n\t \n\twhile (!cpuidle_coupled_no_cpus_ready(coupled))\n\t\tcpu_relax();\n\n\treturn entered_state;\n}\n\nstatic void cpuidle_coupled_update_online_cpus(struct cpuidle_coupled *coupled)\n{\n\tcpumask_t cpus;\n\tcpumask_and(&cpus, cpu_online_mask, &coupled->coupled_cpus);\n\tcoupled->online_count = cpumask_weight(&cpus);\n}\n\n \nint cpuidle_coupled_register_device(struct cpuidle_device *dev)\n{\n\tint cpu;\n\tstruct cpuidle_device *other_dev;\n\tcall_single_data_t *csd;\n\tstruct cpuidle_coupled *coupled;\n\n\tif (cpumask_empty(&dev->coupled_cpus))\n\t\treturn 0;\n\n\tfor_each_cpu(cpu, &dev->coupled_cpus) {\n\t\tother_dev = per_cpu(cpuidle_devices, cpu);\n\t\tif (other_dev && other_dev->coupled) {\n\t\t\tcoupled = other_dev->coupled;\n\t\t\tgoto have_coupled;\n\t\t}\n\t}\n\n\t \n\tcoupled = kzalloc(sizeof(struct cpuidle_coupled), GFP_KERNEL);\n\tif (!coupled)\n\t\treturn -ENOMEM;\n\n\tcoupled->coupled_cpus = dev->coupled_cpus;\n\nhave_coupled:\n\tdev->coupled = coupled;\n\tif (WARN_ON(!cpumask_equal(&dev->coupled_cpus, &coupled->coupled_cpus)))\n\t\tcoupled->prevent++;\n\n\tcpuidle_coupled_update_online_cpus(coupled);\n\n\tcoupled->refcnt++;\n\n\tcsd = &per_cpu(cpuidle_coupled_poke_cb, dev->cpu);\n\tINIT_CSD(csd, cpuidle_coupled_handle_poke, (void *)(unsigned long)dev->cpu);\n\n\treturn 0;\n}\n\n \nvoid cpuidle_coupled_unregister_device(struct cpuidle_device *dev)\n{\n\tstruct cpuidle_coupled *coupled = dev->coupled;\n\n\tif (cpumask_empty(&dev->coupled_cpus))\n\t\treturn;\n\n\tif (--coupled->refcnt)\n\t\tkfree(coupled);\n\tdev->coupled = NULL;\n}\n\n \nstatic void cpuidle_coupled_prevent_idle(struct cpuidle_coupled *coupled)\n{\n\tint cpu = get_cpu();\n\n\t \n\tcoupled->prevent++;\n\tcpuidle_coupled_poke_others(cpu, coupled);\n\tput_cpu();\n\twhile (!cpuidle_coupled_no_cpus_waiting(coupled))\n\t\tcpu_relax();\n}\n\n \nstatic void cpuidle_coupled_allow_idle(struct cpuidle_coupled *coupled)\n{\n\tint cpu = get_cpu();\n\n\t \n\tsmp_wmb();\n\tcoupled->prevent--;\n\t \n\tcpuidle_coupled_poke_others(cpu, coupled);\n\tput_cpu();\n}\n\nstatic int coupled_cpu_online(unsigned int cpu)\n{\n\tstruct cpuidle_device *dev;\n\n\tmutex_lock(&cpuidle_lock);\n\n\tdev = per_cpu(cpuidle_devices, cpu);\n\tif (dev && dev->coupled) {\n\t\tcpuidle_coupled_update_online_cpus(dev->coupled);\n\t\tcpuidle_coupled_allow_idle(dev->coupled);\n\t}\n\n\tmutex_unlock(&cpuidle_lock);\n\treturn 0;\n}\n\nstatic int coupled_cpu_up_prepare(unsigned int cpu)\n{\n\tstruct cpuidle_device *dev;\n\n\tmutex_lock(&cpuidle_lock);\n\n\tdev = per_cpu(cpuidle_devices, cpu);\n\tif (dev && dev->coupled)\n\t\tcpuidle_coupled_prevent_idle(dev->coupled);\n\n\tmutex_unlock(&cpuidle_lock);\n\treturn 0;\n}\n\nstatic int __init cpuidle_coupled_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_CPUIDLE_COUPLED_PREPARE,\n\t\t\t\t\t\"cpuidle/coupled:prepare\",\n\t\t\t\t\tcoupled_cpu_up_prepare,\n\t\t\t\t\tcoupled_cpu_online);\n\tif (ret)\n\t\treturn ret;\n\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t\"cpuidle/coupled:online\",\n\t\t\t\t\tcoupled_cpu_online,\n\t\t\t\t\tcoupled_cpu_up_prepare);\n\tif (ret < 0)\n\t\tcpuhp_remove_state_nocalls(CPUHP_CPUIDLE_COUPLED_PREPARE);\n\treturn ret;\n}\ncore_initcall(cpuidle_coupled_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}