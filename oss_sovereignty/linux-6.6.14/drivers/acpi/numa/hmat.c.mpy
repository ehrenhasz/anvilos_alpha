{
  "module_name": "hmat.c",
  "hash_id": "8acb2e01d8858314e7b837a29581d8210c828d7a8333a08f62e7642cc913b5fc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/acpi/numa/hmat.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"acpi/hmat: \" fmt\n\n#include <linux/acpi.h>\n#include <linux/bitops.h>\n#include <linux/device.h>\n#include <linux/init.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/platform_device.h>\n#include <linux/list_sort.h>\n#include <linux/memregion.h>\n#include <linux/memory.h>\n#include <linux/mutex.h>\n#include <linux/node.h>\n#include <linux/sysfs.h>\n#include <linux/dax.h>\n\nstatic u8 hmat_revision;\nstatic int hmat_disable __initdata;\n\nvoid __init disable_hmat(void)\n{\n\thmat_disable = 1;\n}\n\nstatic LIST_HEAD(targets);\nstatic LIST_HEAD(initiators);\nstatic LIST_HEAD(localities);\n\nstatic DEFINE_MUTEX(target_lock);\n\n \nenum locality_types {\n\tWRITE_LATENCY,\n\tREAD_LATENCY,\n\tWRITE_BANDWIDTH,\n\tREAD_BANDWIDTH,\n};\n\nstatic struct memory_locality *localities_types[4];\n\nstruct target_cache {\n\tstruct list_head node;\n\tstruct node_cache_attrs cache_attrs;\n};\n\nstruct memory_target {\n\tstruct list_head node;\n\tunsigned int memory_pxm;\n\tunsigned int processor_pxm;\n\tstruct resource memregions;\n\tstruct node_hmem_attrs hmem_attrs[2];\n\tstruct list_head caches;\n\tstruct node_cache_attrs cache_attrs;\n\tbool registered;\n};\n\nstruct memory_initiator {\n\tstruct list_head node;\n\tunsigned int processor_pxm;\n\tbool has_cpu;\n};\n\nstruct memory_locality {\n\tstruct list_head node;\n\tstruct acpi_hmat_locality *hmat_loc;\n};\n\nstatic struct memory_initiator *find_mem_initiator(unsigned int cpu_pxm)\n{\n\tstruct memory_initiator *initiator;\n\n\tlist_for_each_entry(initiator, &initiators, node)\n\t\tif (initiator->processor_pxm == cpu_pxm)\n\t\t\treturn initiator;\n\treturn NULL;\n}\n\nstatic struct memory_target *find_mem_target(unsigned int mem_pxm)\n{\n\tstruct memory_target *target;\n\n\tlist_for_each_entry(target, &targets, node)\n\t\tif (target->memory_pxm == mem_pxm)\n\t\t\treturn target;\n\treturn NULL;\n}\n\nstatic __init void alloc_memory_initiator(unsigned int cpu_pxm)\n{\n\tstruct memory_initiator *initiator;\n\n\tif (pxm_to_node(cpu_pxm) == NUMA_NO_NODE)\n\t\treturn;\n\n\tinitiator = find_mem_initiator(cpu_pxm);\n\tif (initiator)\n\t\treturn;\n\n\tinitiator = kzalloc(sizeof(*initiator), GFP_KERNEL);\n\tif (!initiator)\n\t\treturn;\n\n\tinitiator->processor_pxm = cpu_pxm;\n\tinitiator->has_cpu = node_state(pxm_to_node(cpu_pxm), N_CPU);\n\tlist_add_tail(&initiator->node, &initiators);\n}\n\nstatic __init void alloc_memory_target(unsigned int mem_pxm,\n\t\tresource_size_t start, resource_size_t len)\n{\n\tstruct memory_target *target;\n\n\ttarget = find_mem_target(mem_pxm);\n\tif (!target) {\n\t\ttarget = kzalloc(sizeof(*target), GFP_KERNEL);\n\t\tif (!target)\n\t\t\treturn;\n\t\ttarget->memory_pxm = mem_pxm;\n\t\ttarget->processor_pxm = PXM_INVAL;\n\t\ttarget->memregions = (struct resource) {\n\t\t\t.name\t= \"ACPI mem\",\n\t\t\t.start\t= 0,\n\t\t\t.end\t= -1,\n\t\t\t.flags\t= IORESOURCE_MEM,\n\t\t};\n\t\tlist_add_tail(&target->node, &targets);\n\t\tINIT_LIST_HEAD(&target->caches);\n\t}\n\n\t \n\tif (!__request_region(&target->memregions, start, len, \"memory target\",\n\t\t\t\tIORESOURCE_MEM))\n\t\tpr_warn(\"failed to reserve %#llx - %#llx in pxm: %d\\n\",\n\t\t\t\tstart, start + len, mem_pxm);\n}\n\nstatic __init const char *hmat_data_type(u8 type)\n{\n\tswitch (type) {\n\tcase ACPI_HMAT_ACCESS_LATENCY:\n\t\treturn \"Access Latency\";\n\tcase ACPI_HMAT_READ_LATENCY:\n\t\treturn \"Read Latency\";\n\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\treturn \"Write Latency\";\n\tcase ACPI_HMAT_ACCESS_BANDWIDTH:\n\t\treturn \"Access Bandwidth\";\n\tcase ACPI_HMAT_READ_BANDWIDTH:\n\t\treturn \"Read Bandwidth\";\n\tcase ACPI_HMAT_WRITE_BANDWIDTH:\n\t\treturn \"Write Bandwidth\";\n\tdefault:\n\t\treturn \"Reserved\";\n\t}\n}\n\nstatic __init const char *hmat_data_type_suffix(u8 type)\n{\n\tswitch (type) {\n\tcase ACPI_HMAT_ACCESS_LATENCY:\n\tcase ACPI_HMAT_READ_LATENCY:\n\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\treturn \" nsec\";\n\tcase ACPI_HMAT_ACCESS_BANDWIDTH:\n\tcase ACPI_HMAT_READ_BANDWIDTH:\n\tcase ACPI_HMAT_WRITE_BANDWIDTH:\n\t\treturn \" MB/s\";\n\tdefault:\n\t\treturn \"\";\n\t}\n}\n\nstatic u32 hmat_normalize(u16 entry, u64 base, u8 type)\n{\n\tu32 value;\n\n\t \n\tif (entry == 0xffff || !entry)\n\t\treturn 0;\n\telse if (base > (UINT_MAX / (entry)))\n\t\treturn 0;\n\n\t \n\tvalue = entry * base;\n\tif (hmat_revision == 1) {\n\t\tif (value < 10)\n\t\t\treturn 0;\n\t\tvalue = DIV_ROUND_UP(value, 10);\n\t} else if (hmat_revision == 2) {\n\t\tswitch (type) {\n\t\tcase ACPI_HMAT_ACCESS_LATENCY:\n\t\tcase ACPI_HMAT_READ_LATENCY:\n\t\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\t\tvalue = DIV_ROUND_UP(value, 1000);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn value;\n}\n\nstatic void hmat_update_target_access(struct memory_target *target,\n\t\t\t\t      u8 type, u32 value, int access)\n{\n\tswitch (type) {\n\tcase ACPI_HMAT_ACCESS_LATENCY:\n\t\ttarget->hmem_attrs[access].read_latency = value;\n\t\ttarget->hmem_attrs[access].write_latency = value;\n\t\tbreak;\n\tcase ACPI_HMAT_READ_LATENCY:\n\t\ttarget->hmem_attrs[access].read_latency = value;\n\t\tbreak;\n\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\ttarget->hmem_attrs[access].write_latency = value;\n\t\tbreak;\n\tcase ACPI_HMAT_ACCESS_BANDWIDTH:\n\t\ttarget->hmem_attrs[access].read_bandwidth = value;\n\t\ttarget->hmem_attrs[access].write_bandwidth = value;\n\t\tbreak;\n\tcase ACPI_HMAT_READ_BANDWIDTH:\n\t\ttarget->hmem_attrs[access].read_bandwidth = value;\n\t\tbreak;\n\tcase ACPI_HMAT_WRITE_BANDWIDTH:\n\t\ttarget->hmem_attrs[access].write_bandwidth = value;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic __init void hmat_add_locality(struct acpi_hmat_locality *hmat_loc)\n{\n\tstruct memory_locality *loc;\n\n\tloc = kzalloc(sizeof(*loc), GFP_KERNEL);\n\tif (!loc) {\n\t\tpr_notice_once(\"Failed to allocate HMAT locality\\n\");\n\t\treturn;\n\t}\n\n\tloc->hmat_loc = hmat_loc;\n\tlist_add_tail(&loc->node, &localities);\n\n\tswitch (hmat_loc->data_type) {\n\tcase ACPI_HMAT_ACCESS_LATENCY:\n\t\tlocalities_types[READ_LATENCY] = loc;\n\t\tlocalities_types[WRITE_LATENCY] = loc;\n\t\tbreak;\n\tcase ACPI_HMAT_READ_LATENCY:\n\t\tlocalities_types[READ_LATENCY] = loc;\n\t\tbreak;\n\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\tlocalities_types[WRITE_LATENCY] = loc;\n\t\tbreak;\n\tcase ACPI_HMAT_ACCESS_BANDWIDTH:\n\t\tlocalities_types[READ_BANDWIDTH] = loc;\n\t\tlocalities_types[WRITE_BANDWIDTH] = loc;\n\t\tbreak;\n\tcase ACPI_HMAT_READ_BANDWIDTH:\n\t\tlocalities_types[READ_BANDWIDTH] = loc;\n\t\tbreak;\n\tcase ACPI_HMAT_WRITE_BANDWIDTH:\n\t\tlocalities_types[WRITE_BANDWIDTH] = loc;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic __init int hmat_parse_locality(union acpi_subtable_headers *header,\n\t\t\t\t      const unsigned long end)\n{\n\tstruct acpi_hmat_locality *hmat_loc = (void *)header;\n\tstruct memory_target *target;\n\tunsigned int init, targ, total_size, ipds, tpds;\n\tu32 *inits, *targs, value;\n\tu16 *entries;\n\tu8 type, mem_hier;\n\n\tif (hmat_loc->header.length < sizeof(*hmat_loc)) {\n\t\tpr_notice(\"Unexpected locality header length: %u\\n\",\n\t\t\t hmat_loc->header.length);\n\t\treturn -EINVAL;\n\t}\n\n\ttype = hmat_loc->data_type;\n\tmem_hier = hmat_loc->flags & ACPI_HMAT_MEMORY_HIERARCHY;\n\tipds = hmat_loc->number_of_initiator_Pds;\n\ttpds = hmat_loc->number_of_target_Pds;\n\ttotal_size = sizeof(*hmat_loc) + sizeof(*entries) * ipds * tpds +\n\t\t     sizeof(*inits) * ipds + sizeof(*targs) * tpds;\n\tif (hmat_loc->header.length < total_size) {\n\t\tpr_notice(\"Unexpected locality header length:%u, minimum required:%u\\n\",\n\t\t\t hmat_loc->header.length, total_size);\n\t\treturn -EINVAL;\n\t}\n\n\tpr_info(\"Locality: Flags:%02x Type:%s Initiator Domains:%u Target Domains:%u Base:%lld\\n\",\n\t\thmat_loc->flags, hmat_data_type(type), ipds, tpds,\n\t\thmat_loc->entry_base_unit);\n\n\tinits = (u32 *)(hmat_loc + 1);\n\ttargs = inits + ipds;\n\tentries = (u16 *)(targs + tpds);\n\tfor (init = 0; init < ipds; init++) {\n\t\talloc_memory_initiator(inits[init]);\n\t\tfor (targ = 0; targ < tpds; targ++) {\n\t\t\tvalue = hmat_normalize(entries[init * tpds + targ],\n\t\t\t\t\t       hmat_loc->entry_base_unit,\n\t\t\t\t\t       type);\n\t\t\tpr_info(\"  Initiator-Target[%u-%u]:%u%s\\n\",\n\t\t\t\tinits[init], targs[targ], value,\n\t\t\t\thmat_data_type_suffix(type));\n\n\t\t\tif (mem_hier == ACPI_HMAT_MEMORY) {\n\t\t\t\ttarget = find_mem_target(targs[targ]);\n\t\t\t\tif (target && target->processor_pxm == inits[init]) {\n\t\t\t\t\thmat_update_target_access(target, type, value, 0);\n\t\t\t\t\t \n\t\t\t\t\tif (node_state(pxm_to_node(inits[init]), N_CPU))\n\t\t\t\t\t\thmat_update_target_access(target, type, value, 1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (mem_hier == ACPI_HMAT_MEMORY)\n\t\thmat_add_locality(hmat_loc);\n\n\treturn 0;\n}\n\nstatic __init int hmat_parse_cache(union acpi_subtable_headers *header,\n\t\t\t\t   const unsigned long end)\n{\n\tstruct acpi_hmat_cache *cache = (void *)header;\n\tstruct memory_target *target;\n\tstruct target_cache *tcache;\n\tu32 attrs;\n\n\tif (cache->header.length < sizeof(*cache)) {\n\t\tpr_notice(\"Unexpected cache header length: %u\\n\",\n\t\t\t cache->header.length);\n\t\treturn -EINVAL;\n\t}\n\n\tattrs = cache->cache_attributes;\n\tpr_info(\"Cache: Domain:%u Size:%llu Attrs:%08x SMBIOS Handles:%d\\n\",\n\t\tcache->memory_PD, cache->cache_size, attrs,\n\t\tcache->number_of_SMBIOShandles);\n\n\ttarget = find_mem_target(cache->memory_PD);\n\tif (!target)\n\t\treturn 0;\n\n\ttcache = kzalloc(sizeof(*tcache), GFP_KERNEL);\n\tif (!tcache) {\n\t\tpr_notice_once(\"Failed to allocate HMAT cache info\\n\");\n\t\treturn 0;\n\t}\n\n\ttcache->cache_attrs.size = cache->cache_size;\n\ttcache->cache_attrs.level = (attrs & ACPI_HMAT_CACHE_LEVEL) >> 4;\n\ttcache->cache_attrs.line_size = (attrs & ACPI_HMAT_CACHE_LINE_SIZE) >> 16;\n\n\tswitch ((attrs & ACPI_HMAT_CACHE_ASSOCIATIVITY) >> 8) {\n\tcase ACPI_HMAT_CA_DIRECT_MAPPED:\n\t\ttcache->cache_attrs.indexing = NODE_CACHE_DIRECT_MAP;\n\t\tbreak;\n\tcase ACPI_HMAT_CA_COMPLEX_CACHE_INDEXING:\n\t\ttcache->cache_attrs.indexing = NODE_CACHE_INDEXED;\n\t\tbreak;\n\tcase ACPI_HMAT_CA_NONE:\n\tdefault:\n\t\ttcache->cache_attrs.indexing = NODE_CACHE_OTHER;\n\t\tbreak;\n\t}\n\n\tswitch ((attrs & ACPI_HMAT_WRITE_POLICY) >> 12) {\n\tcase ACPI_HMAT_CP_WB:\n\t\ttcache->cache_attrs.write_policy = NODE_CACHE_WRITE_BACK;\n\t\tbreak;\n\tcase ACPI_HMAT_CP_WT:\n\t\ttcache->cache_attrs.write_policy = NODE_CACHE_WRITE_THROUGH;\n\t\tbreak;\n\tcase ACPI_HMAT_CP_NONE:\n\tdefault:\n\t\ttcache->cache_attrs.write_policy = NODE_CACHE_WRITE_OTHER;\n\t\tbreak;\n\t}\n\tlist_add_tail(&tcache->node, &target->caches);\n\n\treturn 0;\n}\n\nstatic int __init hmat_parse_proximity_domain(union acpi_subtable_headers *header,\n\t\t\t\t\t      const unsigned long end)\n{\n\tstruct acpi_hmat_proximity_domain *p = (void *)header;\n\tstruct memory_target *target = NULL;\n\n\tif (p->header.length != sizeof(*p)) {\n\t\tpr_notice(\"Unexpected address range header length: %u\\n\",\n\t\t\t p->header.length);\n\t\treturn -EINVAL;\n\t}\n\n\tif (hmat_revision == 1)\n\t\tpr_info(\"Memory (%#llx length %#llx) Flags:%04x Processor Domain:%u Memory Domain:%u\\n\",\n\t\t\tp->reserved3, p->reserved4, p->flags, p->processor_PD,\n\t\t\tp->memory_PD);\n\telse\n\t\tpr_info(\"Memory Flags:%04x Processor Domain:%u Memory Domain:%u\\n\",\n\t\t\tp->flags, p->processor_PD, p->memory_PD);\n\n\tif ((hmat_revision == 1 && p->flags & ACPI_HMAT_MEMORY_PD_VALID) ||\n\t    hmat_revision > 1) {\n\t\ttarget = find_mem_target(p->memory_PD);\n\t\tif (!target) {\n\t\t\tpr_debug(\"Memory Domain missing from SRAT\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (target && p->flags & ACPI_HMAT_PROCESSOR_PD_VALID) {\n\t\tint p_node = pxm_to_node(p->processor_PD);\n\n\t\tif (p_node == NUMA_NO_NODE) {\n\t\t\tpr_debug(\"Invalid Processor Domain\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttarget->processor_pxm = p->processor_PD;\n\t}\n\n\treturn 0;\n}\n\nstatic int __init hmat_parse_subtable(union acpi_subtable_headers *header,\n\t\t\t\t      const unsigned long end)\n{\n\tstruct acpi_hmat_structure *hdr = (void *)header;\n\n\tif (!hdr)\n\t\treturn -EINVAL;\n\n\tswitch (hdr->type) {\n\tcase ACPI_HMAT_TYPE_PROXIMITY:\n\t\treturn hmat_parse_proximity_domain(header, end);\n\tcase ACPI_HMAT_TYPE_LOCALITY:\n\t\treturn hmat_parse_locality(header, end);\n\tcase ACPI_HMAT_TYPE_CACHE:\n\t\treturn hmat_parse_cache(header, end);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic __init int srat_parse_mem_affinity(union acpi_subtable_headers *header,\n\t\t\t\t\t  const unsigned long end)\n{\n\tstruct acpi_srat_mem_affinity *ma = (void *)header;\n\n\tif (!ma)\n\t\treturn -EINVAL;\n\tif (!(ma->flags & ACPI_SRAT_MEM_ENABLED))\n\t\treturn 0;\n\talloc_memory_target(ma->proximity_domain, ma->base_address, ma->length);\n\treturn 0;\n}\n\nstatic u32 hmat_initiator_perf(struct memory_target *target,\n\t\t\t       struct memory_initiator *initiator,\n\t\t\t       struct acpi_hmat_locality *hmat_loc)\n{\n\tunsigned int ipds, tpds, i, idx = 0, tdx = 0;\n\tu32 *inits, *targs;\n\tu16 *entries;\n\n\tipds = hmat_loc->number_of_initiator_Pds;\n\ttpds = hmat_loc->number_of_target_Pds;\n\tinits = (u32 *)(hmat_loc + 1);\n\ttargs = inits + ipds;\n\tentries = (u16 *)(targs + tpds);\n\n\tfor (i = 0; i < ipds; i++) {\n\t\tif (inits[i] == initiator->processor_pxm) {\n\t\t\tidx = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == ipds)\n\t\treturn 0;\n\n\tfor (i = 0; i < tpds; i++) {\n\t\tif (targs[i] == target->memory_pxm) {\n\t\t\ttdx = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (i == tpds)\n\t\treturn 0;\n\n\treturn hmat_normalize(entries[idx * tpds + tdx],\n\t\t\t      hmat_loc->entry_base_unit,\n\t\t\t      hmat_loc->data_type);\n}\n\nstatic bool hmat_update_best(u8 type, u32 value, u32 *best)\n{\n\tbool updated = false;\n\n\tif (!value)\n\t\treturn false;\n\n\tswitch (type) {\n\tcase ACPI_HMAT_ACCESS_LATENCY:\n\tcase ACPI_HMAT_READ_LATENCY:\n\tcase ACPI_HMAT_WRITE_LATENCY:\n\t\tif (!*best || *best > value) {\n\t\t\t*best = value;\n\t\t\tupdated = true;\n\t\t}\n\t\tbreak;\n\tcase ACPI_HMAT_ACCESS_BANDWIDTH:\n\tcase ACPI_HMAT_READ_BANDWIDTH:\n\tcase ACPI_HMAT_WRITE_BANDWIDTH:\n\t\tif (!*best || *best < value) {\n\t\t\t*best = value;\n\t\t\tupdated = true;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn updated;\n}\n\nstatic int initiator_cmp(void *priv, const struct list_head *a,\n\t\t\t const struct list_head *b)\n{\n\tstruct memory_initiator *ia;\n\tstruct memory_initiator *ib;\n\n\tia = list_entry(a, struct memory_initiator, node);\n\tib = list_entry(b, struct memory_initiator, node);\n\n\treturn ia->processor_pxm - ib->processor_pxm;\n}\n\nstatic int initiators_to_nodemask(unsigned long *p_nodes)\n{\n\tstruct memory_initiator *initiator;\n\n\tif (list_empty(&initiators))\n\t\treturn -ENXIO;\n\n\tlist_for_each_entry(initiator, &initiators, node)\n\t\tset_bit(initiator->processor_pxm, p_nodes);\n\n\treturn 0;\n}\n\nstatic void hmat_register_target_initiators(struct memory_target *target)\n{\n\tstatic DECLARE_BITMAP(p_nodes, MAX_NUMNODES);\n\tstruct memory_initiator *initiator;\n\tunsigned int mem_nid, cpu_nid;\n\tstruct memory_locality *loc = NULL;\n\tu32 best = 0;\n\tbool access0done = false;\n\tint i;\n\n\tmem_nid = pxm_to_node(target->memory_pxm);\n\t \n\tif (target->processor_pxm != PXM_INVAL) {\n\t\tcpu_nid = pxm_to_node(target->processor_pxm);\n\t\tregister_memory_node_under_compute_node(mem_nid, cpu_nid, 0);\n\t\taccess0done = true;\n\t\tif (node_state(cpu_nid, N_CPU)) {\n\t\t\tregister_memory_node_under_compute_node(mem_nid, cpu_nid, 1);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (list_empty(&localities))\n\t\treturn;\n\n\t \n\tbitmap_zero(p_nodes, MAX_NUMNODES);\n\tlist_sort(NULL, &initiators, initiator_cmp);\n\tif (initiators_to_nodemask(p_nodes) < 0)\n\t\treturn;\n\n\tif (!access0done) {\n\t\tfor (i = WRITE_LATENCY; i <= READ_BANDWIDTH; i++) {\n\t\t\tloc = localities_types[i];\n\t\t\tif (!loc)\n\t\t\t\tcontinue;\n\n\t\t\tbest = 0;\n\t\t\tlist_for_each_entry(initiator, &initiators, node) {\n\t\t\t\tu32 value;\n\n\t\t\t\tif (!test_bit(initiator->processor_pxm, p_nodes))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tvalue = hmat_initiator_perf(target, initiator,\n\t\t\t\t\t\t\t    loc->hmat_loc);\n\t\t\t\tif (hmat_update_best(loc->hmat_loc->data_type, value, &best))\n\t\t\t\t\tbitmap_clear(p_nodes, 0, initiator->processor_pxm);\n\t\t\t\tif (value != best)\n\t\t\t\t\tclear_bit(initiator->processor_pxm, p_nodes);\n\t\t\t}\n\t\t\tif (best)\n\t\t\t\thmat_update_target_access(target, loc->hmat_loc->data_type,\n\t\t\t\t\t\t\t  best, 0);\n\t\t}\n\n\t\tfor_each_set_bit(i, p_nodes, MAX_NUMNODES) {\n\t\t\tcpu_nid = pxm_to_node(i);\n\t\t\tregister_memory_node_under_compute_node(mem_nid, cpu_nid, 0);\n\t\t}\n\t}\n\n\t \n\tbitmap_zero(p_nodes, MAX_NUMNODES);\n\tif (initiators_to_nodemask(p_nodes) < 0)\n\t\treturn;\n\n\tfor (i = WRITE_LATENCY; i <= READ_BANDWIDTH; i++) {\n\t\tloc = localities_types[i];\n\t\tif (!loc)\n\t\t\tcontinue;\n\n\t\tbest = 0;\n\t\tlist_for_each_entry(initiator, &initiators, node) {\n\t\t\tu32 value;\n\n\t\t\tif (!initiator->has_cpu) {\n\t\t\t\tclear_bit(initiator->processor_pxm, p_nodes);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!test_bit(initiator->processor_pxm, p_nodes))\n\t\t\t\tcontinue;\n\n\t\t\tvalue = hmat_initiator_perf(target, initiator, loc->hmat_loc);\n\t\t\tif (hmat_update_best(loc->hmat_loc->data_type, value, &best))\n\t\t\t\tbitmap_clear(p_nodes, 0, initiator->processor_pxm);\n\t\t\tif (value != best)\n\t\t\t\tclear_bit(initiator->processor_pxm, p_nodes);\n\t\t}\n\t\tif (best)\n\t\t\thmat_update_target_access(target, loc->hmat_loc->data_type, best, 1);\n\t}\n\tfor_each_set_bit(i, p_nodes, MAX_NUMNODES) {\n\t\tcpu_nid = pxm_to_node(i);\n\t\tregister_memory_node_under_compute_node(mem_nid, cpu_nid, 1);\n\t}\n}\n\nstatic void hmat_register_target_cache(struct memory_target *target)\n{\n\tunsigned mem_nid = pxm_to_node(target->memory_pxm);\n\tstruct target_cache *tcache;\n\n\tlist_for_each_entry(tcache, &target->caches, node)\n\t\tnode_add_cache(mem_nid, &tcache->cache_attrs);\n}\n\nstatic void hmat_register_target_perf(struct memory_target *target, int access)\n{\n\tunsigned mem_nid = pxm_to_node(target->memory_pxm);\n\tnode_set_perf_attrs(mem_nid, &target->hmem_attrs[access], access);\n}\n\nstatic void hmat_register_target_devices(struct memory_target *target)\n{\n\tstruct resource *res;\n\n\t \n\tif (!IS_ENABLED(CONFIG_DEV_DAX_HMEM))\n\t\treturn;\n\n\tfor (res = target->memregions.child; res; res = res->sibling) {\n\t\tint target_nid = pxm_to_node(target->memory_pxm);\n\n\t\thmem_register_resource(target_nid, res);\n\t}\n}\n\nstatic void hmat_register_target(struct memory_target *target)\n{\n\tint nid = pxm_to_node(target->memory_pxm);\n\n\t \n\thmat_register_target_devices(target);\n\n\t \n\tif (nid == NUMA_NO_NODE || !node_online(nid))\n\t\treturn;\n\n\tmutex_lock(&target_lock);\n\tif (!target->registered) {\n\t\thmat_register_target_initiators(target);\n\t\thmat_register_target_cache(target);\n\t\thmat_register_target_perf(target, 0);\n\t\thmat_register_target_perf(target, 1);\n\t\ttarget->registered = true;\n\t}\n\tmutex_unlock(&target_lock);\n}\n\nstatic void hmat_register_targets(void)\n{\n\tstruct memory_target *target;\n\n\tlist_for_each_entry(target, &targets, node)\n\t\thmat_register_target(target);\n}\n\nstatic int hmat_callback(struct notifier_block *self,\n\t\t\t unsigned long action, void *arg)\n{\n\tstruct memory_target *target;\n\tstruct memory_notify *mnb = arg;\n\tint pxm, nid = mnb->status_change_nid;\n\n\tif (nid == NUMA_NO_NODE || action != MEM_ONLINE)\n\t\treturn NOTIFY_OK;\n\n\tpxm = node_to_pxm(nid);\n\ttarget = find_mem_target(pxm);\n\tif (!target)\n\t\treturn NOTIFY_OK;\n\n\thmat_register_target(target);\n\treturn NOTIFY_OK;\n}\n\nstatic __init void hmat_free_structures(void)\n{\n\tstruct memory_target *target, *tnext;\n\tstruct memory_locality *loc, *lnext;\n\tstruct memory_initiator *initiator, *inext;\n\tstruct target_cache *tcache, *cnext;\n\n\tlist_for_each_entry_safe(target, tnext, &targets, node) {\n\t\tstruct resource *res, *res_next;\n\n\t\tlist_for_each_entry_safe(tcache, cnext, &target->caches, node) {\n\t\t\tlist_del(&tcache->node);\n\t\t\tkfree(tcache);\n\t\t}\n\n\t\tlist_del(&target->node);\n\t\tres = target->memregions.child;\n\t\twhile (res) {\n\t\t\tres_next = res->sibling;\n\t\t\t__release_region(&target->memregions, res->start,\n\t\t\t\t\tresource_size(res));\n\t\t\tres = res_next;\n\t\t}\n\t\tkfree(target);\n\t}\n\n\tlist_for_each_entry_safe(initiator, inext, &initiators, node) {\n\t\tlist_del(&initiator->node);\n\t\tkfree(initiator);\n\t}\n\n\tlist_for_each_entry_safe(loc, lnext, &localities, node) {\n\t\tlist_del(&loc->node);\n\t\tkfree(loc);\n\t}\n}\n\nstatic __init int hmat_init(void)\n{\n\tstruct acpi_table_header *tbl;\n\tenum acpi_hmat_type i;\n\tacpi_status status;\n\n\tif (srat_disabled() || hmat_disable)\n\t\treturn 0;\n\n\tstatus = acpi_get_table(ACPI_SIG_SRAT, 0, &tbl);\n\tif (ACPI_FAILURE(status))\n\t\treturn 0;\n\n\tif (acpi_table_parse_entries(ACPI_SIG_SRAT,\n\t\t\t\tsizeof(struct acpi_table_srat),\n\t\t\t\tACPI_SRAT_TYPE_MEMORY_AFFINITY,\n\t\t\t\tsrat_parse_mem_affinity, 0) < 0)\n\t\tgoto out_put;\n\tacpi_put_table(tbl);\n\n\tstatus = acpi_get_table(ACPI_SIG_HMAT, 0, &tbl);\n\tif (ACPI_FAILURE(status))\n\t\tgoto out_put;\n\n\thmat_revision = tbl->revision;\n\tswitch (hmat_revision) {\n\tcase 1:\n\tcase 2:\n\t\tbreak;\n\tdefault:\n\t\tpr_notice(\"Ignoring: Unknown revision:%d\\n\", hmat_revision);\n\t\tgoto out_put;\n\t}\n\n\tfor (i = ACPI_HMAT_TYPE_PROXIMITY; i < ACPI_HMAT_TYPE_RESERVED; i++) {\n\t\tif (acpi_table_parse_entries(ACPI_SIG_HMAT,\n\t\t\t\t\t     sizeof(struct acpi_table_hmat), i,\n\t\t\t\t\t     hmat_parse_subtable, 0) < 0) {\n\t\t\tpr_notice(\"Ignoring: Invalid table\");\n\t\t\tgoto out_put;\n\t\t}\n\t}\n\thmat_register_targets();\n\n\t \n\tif (!hotplug_memory_notifier(hmat_callback, HMAT_CALLBACK_PRI))\n\t\treturn 0;\nout_put:\n\thmat_free_structures();\n\tacpi_put_table(tbl);\n\treturn 0;\n}\nsubsys_initcall(hmat_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}