{
  "module_name": "iort.c",
  "hash_id": "2a1c613a25fe32811965decaf4b99c4ca002bd8e0eb5230f2fec08d0f3db6f10",
  "original_prompt": "Ingested from linux-6.6.14/drivers/acpi/arm64/iort.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"ACPI: IORT: \" fmt\n\n#include <linux/acpi_iort.h>\n#include <linux/bitfield.h>\n#include <linux/iommu.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/dma-map-ops.h>\n#include \"init.h\"\n\n#define IORT_TYPE_MASK(type)\t(1 << (type))\n#define IORT_MSI_TYPE\t\t(1 << ACPI_IORT_NODE_ITS_GROUP)\n#define IORT_IOMMU_TYPE\t\t((1 << ACPI_IORT_NODE_SMMU) |\t\\\n\t\t\t\t(1 << ACPI_IORT_NODE_SMMU_V3))\n\nstruct iort_its_msi_chip {\n\tstruct list_head\tlist;\n\tstruct fwnode_handle\t*fw_node;\n\tphys_addr_t\t\tbase_addr;\n\tu32\t\t\ttranslation_id;\n};\n\nstruct iort_fwnode {\n\tstruct list_head list;\n\tstruct acpi_iort_node *iort_node;\n\tstruct fwnode_handle *fwnode;\n};\nstatic LIST_HEAD(iort_fwnode_list);\nstatic DEFINE_SPINLOCK(iort_fwnode_lock);\n\n \nstatic inline int iort_set_fwnode(struct acpi_iort_node *iort_node,\n\t\t\t\t  struct fwnode_handle *fwnode)\n{\n\tstruct iort_fwnode *np;\n\n\tnp = kzalloc(sizeof(struct iort_fwnode), GFP_ATOMIC);\n\n\tif (WARN_ON(!np))\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&np->list);\n\tnp->iort_node = iort_node;\n\tnp->fwnode = fwnode;\n\n\tspin_lock(&iort_fwnode_lock);\n\tlist_add_tail(&np->list, &iort_fwnode_list);\n\tspin_unlock(&iort_fwnode_lock);\n\n\treturn 0;\n}\n\n \nstatic inline struct fwnode_handle *iort_get_fwnode(\n\t\t\tstruct acpi_iort_node *node)\n{\n\tstruct iort_fwnode *curr;\n\tstruct fwnode_handle *fwnode = NULL;\n\n\tspin_lock(&iort_fwnode_lock);\n\tlist_for_each_entry(curr, &iort_fwnode_list, list) {\n\t\tif (curr->iort_node == node) {\n\t\t\tfwnode = curr->fwnode;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_fwnode_lock);\n\n\treturn fwnode;\n}\n\n \nstatic inline void iort_delete_fwnode(struct acpi_iort_node *node)\n{\n\tstruct iort_fwnode *curr, *tmp;\n\n\tspin_lock(&iort_fwnode_lock);\n\tlist_for_each_entry_safe(curr, tmp, &iort_fwnode_list, list) {\n\t\tif (curr->iort_node == node) {\n\t\t\tlist_del(&curr->list);\n\t\t\tkfree(curr);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_fwnode_lock);\n}\n\n \nstatic inline struct acpi_iort_node *iort_get_iort_node(\n\t\t\tstruct fwnode_handle *fwnode)\n{\n\tstruct iort_fwnode *curr;\n\tstruct acpi_iort_node *iort_node = NULL;\n\n\tspin_lock(&iort_fwnode_lock);\n\tlist_for_each_entry(curr, &iort_fwnode_list, list) {\n\t\tif (curr->fwnode == fwnode) {\n\t\t\tiort_node = curr->iort_node;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_fwnode_lock);\n\n\treturn iort_node;\n}\n\ntypedef acpi_status (*iort_find_node_callback)\n\t(struct acpi_iort_node *node, void *context);\n\n \nstatic struct acpi_table_header *iort_table;\n\nstatic LIST_HEAD(iort_msi_chip_list);\nstatic DEFINE_SPINLOCK(iort_msi_chip_lock);\n\n \nint iort_register_domain_token(int trans_id, phys_addr_t base,\n\t\t\t       struct fwnode_handle *fw_node)\n{\n\tstruct iort_its_msi_chip *its_msi_chip;\n\n\tits_msi_chip = kzalloc(sizeof(*its_msi_chip), GFP_KERNEL);\n\tif (!its_msi_chip)\n\t\treturn -ENOMEM;\n\n\tits_msi_chip->fw_node = fw_node;\n\tits_msi_chip->translation_id = trans_id;\n\tits_msi_chip->base_addr = base;\n\n\tspin_lock(&iort_msi_chip_lock);\n\tlist_add(&its_msi_chip->list, &iort_msi_chip_list);\n\tspin_unlock(&iort_msi_chip_lock);\n\n\treturn 0;\n}\n\n \nvoid iort_deregister_domain_token(int trans_id)\n{\n\tstruct iort_its_msi_chip *its_msi_chip, *t;\n\n\tspin_lock(&iort_msi_chip_lock);\n\tlist_for_each_entry_safe(its_msi_chip, t, &iort_msi_chip_list, list) {\n\t\tif (its_msi_chip->translation_id == trans_id) {\n\t\t\tlist_del(&its_msi_chip->list);\n\t\t\tkfree(its_msi_chip);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_msi_chip_lock);\n}\n\n \nstruct fwnode_handle *iort_find_domain_token(int trans_id)\n{\n\tstruct fwnode_handle *fw_node = NULL;\n\tstruct iort_its_msi_chip *its_msi_chip;\n\n\tspin_lock(&iort_msi_chip_lock);\n\tlist_for_each_entry(its_msi_chip, &iort_msi_chip_list, list) {\n\t\tif (its_msi_chip->translation_id == trans_id) {\n\t\t\tfw_node = its_msi_chip->fw_node;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_msi_chip_lock);\n\n\treturn fw_node;\n}\n\nstatic struct acpi_iort_node *iort_scan_node(enum acpi_iort_node_type type,\n\t\t\t\t\t     iort_find_node_callback callback,\n\t\t\t\t\t     void *context)\n{\n\tstruct acpi_iort_node *iort_node, *iort_end;\n\tstruct acpi_table_iort *iort;\n\tint i;\n\n\tif (!iort_table)\n\t\treturn NULL;\n\n\t \n\tiort = (struct acpi_table_iort *)iort_table;\n\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort,\n\t\t\t\t iort->node_offset);\n\tiort_end = ACPI_ADD_PTR(struct acpi_iort_node, iort_table,\n\t\t\t\tiort_table->length);\n\n\tfor (i = 0; i < iort->node_count; i++) {\n\t\tif (WARN_TAINT(iort_node >= iort_end, TAINT_FIRMWARE_WORKAROUND,\n\t\t\t       \"IORT node pointer overflows, bad table!\\n\"))\n\t\t\treturn NULL;\n\n\t\tif (iort_node->type == type &&\n\t\t    ACPI_SUCCESS(callback(iort_node, context)))\n\t\t\treturn iort_node;\n\n\t\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort_node,\n\t\t\t\t\t iort_node->length);\n\t}\n\n\treturn NULL;\n}\n\nstatic acpi_status iort_match_node_callback(struct acpi_iort_node *node,\n\t\t\t\t\t    void *context)\n{\n\tstruct device *dev = context;\n\tacpi_status status = AE_NOT_FOUND;\n\n\tif (node->type == ACPI_IORT_NODE_NAMED_COMPONENT) {\n\t\tstruct acpi_buffer buf = { ACPI_ALLOCATE_BUFFER, NULL };\n\t\tstruct acpi_device *adev;\n\t\tstruct acpi_iort_named_component *ncomp;\n\t\tstruct device *nc_dev = dev;\n\n\t\t \n\t\tdo {\n\t\t\tadev = ACPI_COMPANION(nc_dev);\n\t\t\tif (adev)\n\t\t\t\tbreak;\n\n\t\t\tnc_dev = nc_dev->parent;\n\t\t} while (nc_dev);\n\n\t\tif (!adev)\n\t\t\tgoto out;\n\n\t\tstatus = acpi_get_name(adev->handle, ACPI_FULL_PATHNAME, &buf);\n\t\tif (ACPI_FAILURE(status)) {\n\t\t\tdev_warn(nc_dev, \"Can't get device full path name\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tncomp = (struct acpi_iort_named_component *)node->node_data;\n\t\tstatus = !strcmp(ncomp->device_name, buf.pointer) ?\n\t\t\t\t\t\t\tAE_OK : AE_NOT_FOUND;\n\t\tacpi_os_free(buf.pointer);\n\t} else if (node->type == ACPI_IORT_NODE_PCI_ROOT_COMPLEX) {\n\t\tstruct acpi_iort_root_complex *pci_rc;\n\t\tstruct pci_bus *bus;\n\n\t\tbus = to_pci_bus(dev);\n\t\tpci_rc = (struct acpi_iort_root_complex *)node->node_data;\n\n\t\t \n\t\tstatus = pci_rc->pci_segment_number == pci_domain_nr(bus) ?\n\t\t\t\t\t\t\tAE_OK : AE_NOT_FOUND;\n\t}\nout:\n\treturn status;\n}\n\nstatic int iort_id_map(struct acpi_iort_id_mapping *map, u8 type, u32 rid_in,\n\t\t       u32 *rid_out, bool check_overlap)\n{\n\t \n\tif (map->flags & ACPI_IORT_ID_SINGLE_MAPPING) {\n\t\tif (type == ACPI_IORT_NODE_NAMED_COMPONENT ||\n\t\t    type == ACPI_IORT_NODE_PCI_ROOT_COMPLEX) {\n\t\t\t*rid_out = map->output_base;\n\t\t\treturn 0;\n\t\t}\n\n\t\tpr_warn(FW_BUG \"[map %p] SINGLE MAPPING flag not allowed for node type %d, skipping ID map\\n\",\n\t\t\tmap, type);\n\t\treturn -ENXIO;\n\t}\n\n\tif (rid_in < map->input_base ||\n\t    (rid_in > map->input_base + map->id_count))\n\t\treturn -ENXIO;\n\n\tif (check_overlap) {\n\t\t \n\t\tpr_err(FW_BUG \"[map %p] conflicting mapping for input ID 0x%x\\n\",\n\t\t       map, rid_in);\n\t\tif (rid_in != map->input_base)\n\t\t\treturn -ENXIO;\n\n\t\tpr_err(FW_BUG \"applying workaround.\\n\");\n\t}\n\n\t*rid_out = map->output_base + (rid_in - map->input_base);\n\n\t \n\tif (map->id_count > 0 && rid_in == map->input_base + map->id_count)\n\t\treturn -EAGAIN;\n\treturn 0;\n}\n\nstatic struct acpi_iort_node *iort_node_get_id(struct acpi_iort_node *node,\n\t\t\t\t\t       u32 *id_out, int index)\n{\n\tstruct acpi_iort_node *parent;\n\tstruct acpi_iort_id_mapping *map;\n\n\tif (!node->mapping_offset || !node->mapping_count ||\n\t\t\t\t     index >= node->mapping_count)\n\t\treturn NULL;\n\n\tmap = ACPI_ADD_PTR(struct acpi_iort_id_mapping, node,\n\t\t\t   node->mapping_offset + index * sizeof(*map));\n\n\t \n\tif (!map->output_reference) {\n\t\tpr_err(FW_BUG \"[node %p type %d] ID map has NULL parent reference\\n\",\n\t\t       node, node->type);\n\t\treturn NULL;\n\t}\n\n\tparent = ACPI_ADD_PTR(struct acpi_iort_node, iort_table,\n\t\t\t       map->output_reference);\n\n\tif (map->flags & ACPI_IORT_ID_SINGLE_MAPPING) {\n\t\tif (node->type == ACPI_IORT_NODE_NAMED_COMPONENT ||\n\t\t    node->type == ACPI_IORT_NODE_PCI_ROOT_COMPLEX ||\n\t\t    node->type == ACPI_IORT_NODE_SMMU_V3 ||\n\t\t    node->type == ACPI_IORT_NODE_PMCG) {\n\t\t\t*id_out = map->output_base;\n\t\t\treturn parent;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n#ifndef ACPI_IORT_SMMU_V3_DEVICEID_VALID\n#define ACPI_IORT_SMMU_V3_DEVICEID_VALID (1 << 4)\n#endif\n\nstatic int iort_get_id_mapping_index(struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu_v3 *smmu;\n\tstruct acpi_iort_pmcg *pmcg;\n\n\tswitch (node->type) {\n\tcase ACPI_IORT_NODE_SMMU_V3:\n\t\t \n\t\tif (node->revision < 1)\n\t\t\treturn -EINVAL;\n\n\t\tsmmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\t\t \n\t\tif (node->revision < 5) {\n\t\t\tif (smmu->event_gsiv && smmu->pri_gsiv &&\n\t\t\t    smmu->gerr_gsiv && smmu->sync_gsiv)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (!(smmu->flags & ACPI_IORT_SMMU_V3_DEVICEID_VALID)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (smmu->id_mapping_index >= node->mapping_count) {\n\t\t\tpr_err(FW_BUG \"[node %p type %d] ID mapping index overflows valid mappings\\n\",\n\t\t\t       node, node->type);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\treturn smmu->id_mapping_index;\n\tcase ACPI_IORT_NODE_PMCG:\n\t\tpmcg = (struct acpi_iort_pmcg *)node->node_data;\n\t\tif (pmcg->overflow_gsiv || node->mapping_count == 0)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic struct acpi_iort_node *iort_node_map_id(struct acpi_iort_node *node,\n\t\t\t\t\t       u32 id_in, u32 *id_out,\n\t\t\t\t\t       u8 type_mask)\n{\n\tu32 id = id_in;\n\n\t \n\twhile (node) {\n\t\tstruct acpi_iort_id_mapping *map;\n\t\tint i, index, rc = 0;\n\t\tu32 out_ref = 0, map_id = id;\n\n\t\tif (IORT_TYPE_MASK(node->type) & type_mask) {\n\t\t\tif (id_out)\n\t\t\t\t*id_out = id;\n\t\t\treturn node;\n\t\t}\n\n\t\tif (!node->mapping_offset || !node->mapping_count)\n\t\t\tgoto fail_map;\n\n\t\tmap = ACPI_ADD_PTR(struct acpi_iort_id_mapping, node,\n\t\t\t\t   node->mapping_offset);\n\n\t\t \n\t\tif (!map->output_reference) {\n\t\t\tpr_err(FW_BUG \"[node %p type %d] ID map has NULL parent reference\\n\",\n\t\t\t       node, node->type);\n\t\t\tgoto fail_map;\n\t\t}\n\n\t\t \n\t\tindex = iort_get_id_mapping_index(node);\n\n\t\t \n\t\tfor (i = 0; i < node->mapping_count; i++, map++) {\n\t\t\t \n\t\t\tif (i == index)\n\t\t\t\tcontinue;\n\n\t\t\trc = iort_id_map(map, node->type, map_id, &id, out_ref);\n\t\t\tif (!rc)\n\t\t\t\tbreak;\n\t\t\tif (rc == -EAGAIN)\n\t\t\t\tout_ref = map->output_reference;\n\t\t}\n\n\t\tif (i == node->mapping_count && !out_ref)\n\t\t\tgoto fail_map;\n\n\t\tnode = ACPI_ADD_PTR(struct acpi_iort_node, iort_table,\n\t\t\t\t    rc ? out_ref : map->output_reference);\n\t}\n\nfail_map:\n\t \n\tif (id_out)\n\t\t*id_out = id_in;\n\n\treturn NULL;\n}\n\nstatic struct acpi_iort_node *iort_node_map_platform_id(\n\t\tstruct acpi_iort_node *node, u32 *id_out, u8 type_mask,\n\t\tint index)\n{\n\tstruct acpi_iort_node *parent;\n\tu32 id;\n\n\t \n\tparent = iort_node_get_id(node, &id, index);\n\tif (!parent)\n\t\treturn NULL;\n\n\t \n\tif (!(IORT_TYPE_MASK(parent->type) & type_mask))\n\t\tparent = iort_node_map_id(parent, id, id_out, type_mask);\n\telse\n\t\tif (id_out)\n\t\t\t*id_out = id;\n\n\treturn parent;\n}\n\nstatic struct acpi_iort_node *iort_find_dev_node(struct device *dev)\n{\n\tstruct pci_bus *pbus;\n\n\tif (!dev_is_pci(dev)) {\n\t\tstruct acpi_iort_node *node;\n\t\t \n\t\tnode = iort_get_iort_node(dev->fwnode);\n\t\tif (node)\n\t\t\treturn node;\n\t\t \n\t\treturn iort_scan_node(ACPI_IORT_NODE_NAMED_COMPONENT,\n\t\t\t\t      iort_match_node_callback, dev);\n\t}\n\n\tpbus = to_pci_dev(dev)->bus;\n\n\treturn iort_scan_node(ACPI_IORT_NODE_PCI_ROOT_COMPLEX,\n\t\t\t      iort_match_node_callback, &pbus->dev);\n}\n\n \nu32 iort_msi_map_id(struct device *dev, u32 input_id)\n{\n\tstruct acpi_iort_node *node;\n\tu32 dev_id;\n\n\tnode = iort_find_dev_node(dev);\n\tif (!node)\n\t\treturn input_id;\n\n\tiort_node_map_id(node, input_id, &dev_id, IORT_MSI_TYPE);\n\treturn dev_id;\n}\n\n \nint iort_pmsi_get_dev_id(struct device *dev, u32 *dev_id)\n{\n\tint i, index;\n\tstruct acpi_iort_node *node;\n\n\tnode = iort_find_dev_node(dev);\n\tif (!node)\n\t\treturn -ENODEV;\n\n\tindex = iort_get_id_mapping_index(node);\n\t \n\tif (index >= 0) {\n\t\tif (iort_node_get_id(node, dev_id, index))\n\t\t\treturn 0;\n\t} else {\n\t\tfor (i = 0; i < node->mapping_count; i++) {\n\t\t\tif (iort_node_map_platform_id(node, dev_id,\n\t\t\t\t\t\t      IORT_MSI_TYPE, i))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENODEV;\n}\n\nstatic int __maybe_unused iort_find_its_base(u32 its_id, phys_addr_t *base)\n{\n\tstruct iort_its_msi_chip *its_msi_chip;\n\tint ret = -ENODEV;\n\n\tspin_lock(&iort_msi_chip_lock);\n\tlist_for_each_entry(its_msi_chip, &iort_msi_chip_list, list) {\n\t\tif (its_msi_chip->translation_id == its_id) {\n\t\t\t*base = its_msi_chip->base_addr;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&iort_msi_chip_lock);\n\n\treturn ret;\n}\n\n \nstatic int iort_dev_find_its_id(struct device *dev, u32 id,\n\t\t\t\tunsigned int idx, int *its_id)\n{\n\tstruct acpi_iort_its_group *its;\n\tstruct acpi_iort_node *node;\n\n\tnode = iort_find_dev_node(dev);\n\tif (!node)\n\t\treturn -ENXIO;\n\n\tnode = iort_node_map_id(node, id, NULL, IORT_MSI_TYPE);\n\tif (!node)\n\t\treturn -ENXIO;\n\n\t \n\tits = (struct acpi_iort_its_group *)node->node_data;\n\tif (idx >= its->its_count) {\n\t\tdev_err(dev, \"requested ITS ID index [%d] overruns ITS entries [%d]\\n\",\n\t\t\tidx, its->its_count);\n\t\treturn -ENXIO;\n\t}\n\n\t*its_id = its->identifiers[idx];\n\treturn 0;\n}\n\n \nstruct irq_domain *iort_get_device_domain(struct device *dev, u32 id,\n\t\t\t\t\t  enum irq_domain_bus_token bus_token)\n{\n\tstruct fwnode_handle *handle;\n\tint its_id;\n\n\tif (iort_dev_find_its_id(dev, id, 0, &its_id))\n\t\treturn NULL;\n\n\thandle = iort_find_domain_token(its_id);\n\tif (!handle)\n\t\treturn NULL;\n\n\treturn irq_find_matching_fwnode(handle, bus_token);\n}\n\nstatic void iort_set_device_domain(struct device *dev,\n\t\t\t\t   struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_its_group *its;\n\tstruct acpi_iort_node *msi_parent;\n\tstruct acpi_iort_id_mapping *map;\n\tstruct fwnode_handle *iort_fwnode;\n\tstruct irq_domain *domain;\n\tint index;\n\n\tindex = iort_get_id_mapping_index(node);\n\tif (index < 0)\n\t\treturn;\n\n\tmap = ACPI_ADD_PTR(struct acpi_iort_id_mapping, node,\n\t\t\t   node->mapping_offset + index * sizeof(*map));\n\n\t \n\tif (!map->output_reference ||\n\t    !(map->flags & ACPI_IORT_ID_SINGLE_MAPPING)) {\n\t\tpr_err(FW_BUG \"[node %p type %d] Invalid MSI mapping\\n\",\n\t\t       node, node->type);\n\t\treturn;\n\t}\n\n\tmsi_parent = ACPI_ADD_PTR(struct acpi_iort_node, iort_table,\n\t\t\t\t  map->output_reference);\n\n\tif (!msi_parent || msi_parent->type != ACPI_IORT_NODE_ITS_GROUP)\n\t\treturn;\n\n\t \n\tits = (struct acpi_iort_its_group *)msi_parent->node_data;\n\n\tiort_fwnode = iort_find_domain_token(its->identifiers[0]);\n\tif (!iort_fwnode)\n\t\treturn;\n\n\tdomain = irq_find_matching_fwnode(iort_fwnode, DOMAIN_BUS_PLATFORM_MSI);\n\tif (domain)\n\t\tdev_set_msi_domain(dev, domain);\n}\n\n \nstatic struct irq_domain *iort_get_platform_device_domain(struct device *dev)\n{\n\tstruct acpi_iort_node *node, *msi_parent = NULL;\n\tstruct fwnode_handle *iort_fwnode;\n\tstruct acpi_iort_its_group *its;\n\tint i;\n\n\t \n\tnode = iort_scan_node(ACPI_IORT_NODE_NAMED_COMPONENT,\n\t\t\t      iort_match_node_callback, dev);\n\tif (!node)\n\t\treturn NULL;\n\n\t \n\tfor (i = 0; i < node->mapping_count; i++) {\n\t\tmsi_parent = iort_node_map_platform_id(node, NULL,\n\t\t\t\t\t\t       IORT_MSI_TYPE, i);\n\t\tif (msi_parent)\n\t\t\tbreak;\n\t}\n\n\tif (!msi_parent)\n\t\treturn NULL;\n\n\t \n\tits = (struct acpi_iort_its_group *)msi_parent->node_data;\n\n\tiort_fwnode = iort_find_domain_token(its->identifiers[0]);\n\tif (!iort_fwnode)\n\t\treturn NULL;\n\n\treturn irq_find_matching_fwnode(iort_fwnode, DOMAIN_BUS_PLATFORM_MSI);\n}\n\nvoid acpi_configure_pmsi_domain(struct device *dev)\n{\n\tstruct irq_domain *msi_domain;\n\n\tmsi_domain = iort_get_platform_device_domain(dev);\n\tif (msi_domain)\n\t\tdev_set_msi_domain(dev, msi_domain);\n}\n\n#ifdef CONFIG_IOMMU_API\nstatic void iort_rmr_free(struct device *dev,\n\t\t\t  struct iommu_resv_region *region)\n{\n\tstruct iommu_iort_rmr_data *rmr_data;\n\n\trmr_data = container_of(region, struct iommu_iort_rmr_data, rr);\n\tkfree(rmr_data->sids);\n\tkfree(rmr_data);\n}\n\nstatic struct iommu_iort_rmr_data *iort_rmr_alloc(\n\t\t\t\t\tstruct acpi_iort_rmr_desc *rmr_desc,\n\t\t\t\t\tint prot, enum iommu_resv_type type,\n\t\t\t\t\tu32 *sids, u32 num_sids)\n{\n\tstruct iommu_iort_rmr_data *rmr_data;\n\tstruct iommu_resv_region *region;\n\tu32 *sids_copy;\n\tu64 addr = rmr_desc->base_address, size = rmr_desc->length;\n\n\trmr_data = kmalloc(sizeof(*rmr_data), GFP_KERNEL);\n\tif (!rmr_data)\n\t\treturn NULL;\n\n\t \n\tsids_copy = kmemdup(sids, num_sids * sizeof(*sids), GFP_KERNEL);\n\tif (!sids_copy) {\n\t\tkfree(rmr_data);\n\t\treturn NULL;\n\t}\n\trmr_data->sids = sids_copy;\n\trmr_data->num_sids = num_sids;\n\n\tif (!IS_ALIGNED(addr, SZ_64K) || !IS_ALIGNED(size, SZ_64K)) {\n\t\t \n\t\taddr &= PAGE_MASK;\n\t\tsize = PAGE_ALIGN(size + offset_in_page(rmr_desc->base_address));\n\n\t\tpr_err(FW_BUG \"RMR descriptor[0x%llx - 0x%llx] not aligned to 64K, continue with [0x%llx - 0x%llx]\\n\",\n\t\t       rmr_desc->base_address,\n\t\t       rmr_desc->base_address + rmr_desc->length - 1,\n\t\t       addr, addr + size - 1);\n\t}\n\n\tregion = &rmr_data->rr;\n\tINIT_LIST_HEAD(&region->list);\n\tregion->start = addr;\n\tregion->length = size;\n\tregion->prot = prot;\n\tregion->type = type;\n\tregion->free = iort_rmr_free;\n\n\treturn rmr_data;\n}\n\nstatic void iort_rmr_desc_check_overlap(struct acpi_iort_rmr_desc *desc,\n\t\t\t\t\tu32 count)\n{\n\tint i, j;\n\n\tfor (i = 0; i < count; i++) {\n\t\tu64 end, start = desc[i].base_address, length = desc[i].length;\n\n\t\tif (!length) {\n\t\t\tpr_err(FW_BUG \"RMR descriptor[0x%llx] with zero length, continue anyway\\n\",\n\t\t\t       start);\n\t\t\tcontinue;\n\t\t}\n\n\t\tend = start + length - 1;\n\n\t\t \n\t\tfor (j = i + 1; j < count; j++) {\n\t\t\tu64 e_start = desc[j].base_address;\n\t\t\tu64 e_end = e_start + desc[j].length - 1;\n\n\t\t\tif (start <= e_end && end >= e_start)\n\t\t\t\tpr_err(FW_BUG \"RMR descriptor[0x%llx - 0x%llx] overlaps, continue anyway\\n\",\n\t\t\t\t       start, end);\n\t\t}\n\t}\n}\n\n \nstatic void iort_get_rmrs(struct acpi_iort_node *node,\n\t\t\t  struct acpi_iort_node *smmu,\n\t\t\t  u32 *sids, u32 num_sids,\n\t\t\t  struct list_head *head)\n{\n\tstruct acpi_iort_rmr *rmr = (struct acpi_iort_rmr *)node->node_data;\n\tstruct acpi_iort_rmr_desc *rmr_desc;\n\tint i;\n\n\trmr_desc = ACPI_ADD_PTR(struct acpi_iort_rmr_desc, node,\n\t\t\t\trmr->rmr_offset);\n\n\tiort_rmr_desc_check_overlap(rmr_desc, rmr->rmr_count);\n\n\tfor (i = 0; i < rmr->rmr_count; i++, rmr_desc++) {\n\t\tstruct iommu_iort_rmr_data *rmr_data;\n\t\tenum iommu_resv_type type;\n\t\tint prot = IOMMU_READ | IOMMU_WRITE;\n\n\t\tif (rmr->flags & ACPI_IORT_RMR_REMAP_PERMITTED)\n\t\t\ttype = IOMMU_RESV_DIRECT_RELAXABLE;\n\t\telse\n\t\t\ttype = IOMMU_RESV_DIRECT;\n\n\t\tif (rmr->flags & ACPI_IORT_RMR_ACCESS_PRIVILEGE)\n\t\t\tprot |= IOMMU_PRIV;\n\n\t\t \n\t\tif (ACPI_IORT_RMR_ACCESS_ATTRIBUTES(rmr->flags) <=\n\t\t\t\tACPI_IORT_RMR_ATTR_DEVICE_GRE)\n\t\t\tprot |= IOMMU_MMIO;\n\t\telse if (ACPI_IORT_RMR_ACCESS_ATTRIBUTES(rmr->flags) ==\n\t\t\t\tACPI_IORT_RMR_ATTR_NORMAL_IWB_OWB)\n\t\t\tprot |= IOMMU_CACHE;\n\n\t\trmr_data = iort_rmr_alloc(rmr_desc, prot, type,\n\t\t\t\t\t  sids, num_sids);\n\t\tif (!rmr_data)\n\t\t\treturn;\n\n\t\tlist_add_tail(&rmr_data->rr.list, head);\n\t}\n}\n\nstatic u32 *iort_rmr_alloc_sids(u32 *sids, u32 count, u32 id_start,\n\t\t\t\tu32 new_count)\n{\n\tu32 *new_sids;\n\tu32 total_count = count + new_count;\n\tint i;\n\n\tnew_sids = krealloc_array(sids, count + new_count,\n\t\t\t\t  sizeof(*new_sids), GFP_KERNEL);\n\tif (!new_sids)\n\t\treturn NULL;\n\n\tfor (i = count; i < total_count; i++)\n\t\tnew_sids[i] = id_start++;\n\n\treturn new_sids;\n}\n\nstatic bool iort_rmr_has_dev(struct device *dev, u32 id_start,\n\t\t\t     u32 id_count)\n{\n\tint i;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\t \n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\t\tstruct pci_host_bridge *host = pci_find_host_bridge(pdev->bus);\n\n\t\tif (!host->preserve_config)\n\t\t\treturn false;\n\t}\n\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\tif (fwspec->ids[i] >= id_start &&\n\t\t    fwspec->ids[i] <= id_start + id_count)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void iort_node_get_rmr_info(struct acpi_iort_node *node,\n\t\t\t\t   struct acpi_iort_node *iommu,\n\t\t\t\t   struct device *dev, struct list_head *head)\n{\n\tstruct acpi_iort_node *smmu = NULL;\n\tstruct acpi_iort_rmr *rmr;\n\tstruct acpi_iort_id_mapping *map;\n\tu32 *sids = NULL;\n\tu32 num_sids = 0;\n\tint i;\n\n\tif (!node->mapping_offset || !node->mapping_count) {\n\t\tpr_err(FW_BUG \"Invalid ID mapping, skipping RMR node %p\\n\",\n\t\t       node);\n\t\treturn;\n\t}\n\n\trmr = (struct acpi_iort_rmr *)node->node_data;\n\tif (!rmr->rmr_offset || !rmr->rmr_count)\n\t\treturn;\n\n\tmap = ACPI_ADD_PTR(struct acpi_iort_id_mapping, node,\n\t\t\t   node->mapping_offset);\n\n\t \n\tfor (i = 0; i < node->mapping_count; i++, map++) {\n\t\tstruct acpi_iort_node *parent;\n\n\t\tparent = ACPI_ADD_PTR(struct acpi_iort_node, iort_table,\n\t\t\t\t      map->output_reference);\n\t\tif (parent != iommu)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (dev && !iort_rmr_has_dev(dev, map->output_base,\n\t\t\t\t\t     map->id_count))\n\t\t\tcontinue;\n\n\t\t \n\t\tsids = iort_rmr_alloc_sids(sids, num_sids, map->output_base,\n\t\t\t\t\t   map->id_count + 1);\n\t\tif (!sids)\n\t\t\treturn;\n\n\t\tnum_sids += map->id_count + 1;\n\t}\n\n\tif (!sids)\n\t\treturn;\n\n\tiort_get_rmrs(node, smmu, sids, num_sids, head);\n\tkfree(sids);\n}\n\nstatic void iort_find_rmrs(struct acpi_iort_node *iommu, struct device *dev,\n\t\t\t   struct list_head *head)\n{\n\tstruct acpi_table_iort *iort;\n\tstruct acpi_iort_node *iort_node, *iort_end;\n\tint i;\n\n\t \n\tif (iort_table->revision < 5)\n\t\treturn;\n\n\tiort = (struct acpi_table_iort *)iort_table;\n\n\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort,\n\t\t\t\t iort->node_offset);\n\tiort_end = ACPI_ADD_PTR(struct acpi_iort_node, iort,\n\t\t\t\tiort_table->length);\n\n\tfor (i = 0; i < iort->node_count; i++) {\n\t\tif (WARN_TAINT(iort_node >= iort_end, TAINT_FIRMWARE_WORKAROUND,\n\t\t\t       \"IORT node pointer overflows, bad table!\\n\"))\n\t\t\treturn;\n\n\t\tif (iort_node->type == ACPI_IORT_NODE_RMR)\n\t\t\tiort_node_get_rmr_info(iort_node, iommu, dev, head);\n\n\t\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort_node,\n\t\t\t\t\t iort_node->length);\n\t}\n}\n\n \nstatic void iort_iommu_rmr_get_resv_regions(struct fwnode_handle *iommu_fwnode,\n\t\t\t\t\t    struct device *dev,\n\t\t\t\t\t    struct list_head *head)\n{\n\tstruct acpi_iort_node *iommu;\n\n\tiommu = iort_get_iort_node(iommu_fwnode);\n\tif (!iommu)\n\t\treturn;\n\n\tiort_find_rmrs(iommu, dev, head);\n}\n\nstatic struct acpi_iort_node *iort_get_msi_resv_iommu(struct device *dev)\n{\n\tstruct acpi_iort_node *iommu;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tiommu = iort_get_iort_node(fwspec->iommu_fwnode);\n\n\tif (iommu && (iommu->type == ACPI_IORT_NODE_SMMU_V3)) {\n\t\tstruct acpi_iort_smmu_v3 *smmu;\n\n\t\tsmmu = (struct acpi_iort_smmu_v3 *)iommu->node_data;\n\t\tif (smmu->model == ACPI_IORT_SMMU_V3_HISILICON_HI161X)\n\t\t\treturn iommu;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void iort_iommu_msi_get_resv_regions(struct device *dev,\n\t\t\t\t\t    struct list_head *head)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct acpi_iort_its_group *its;\n\tstruct acpi_iort_node *iommu_node, *its_node = NULL;\n\tint i;\n\n\tiommu_node = iort_get_msi_resv_iommu(dev);\n\tif (!iommu_node)\n\t\treturn;\n\n\t \n\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\tits_node = iort_node_map_id(iommu_node,\n\t\t\t\t\tfwspec->ids[i],\n\t\t\t\t\tNULL, IORT_MSI_TYPE);\n\t\tif (its_node)\n\t\t\tbreak;\n\t}\n\n\tif (!its_node)\n\t\treturn;\n\n\t \n\tits = (struct acpi_iort_its_group *)its_node->node_data;\n\n\tfor (i = 0; i < its->its_count; i++) {\n\t\tphys_addr_t base;\n\n\t\tif (!iort_find_its_base(its->identifiers[i], &base)) {\n\t\t\tint prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\t\t\tstruct iommu_resv_region *region;\n\n\t\t\tregion = iommu_alloc_resv_region(base + SZ_64K, SZ_64K,\n\t\t\t\t\t\t\t prot, IOMMU_RESV_MSI,\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (region)\n\t\t\t\tlist_add_tail(&region->list, head);\n\t\t}\n\t}\n}\n\n \nvoid iort_iommu_get_resv_regions(struct device *dev, struct list_head *head)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tiort_iommu_msi_get_resv_regions(dev, head);\n\tiort_iommu_rmr_get_resv_regions(fwspec->iommu_fwnode, dev, head);\n}\n\n \nvoid iort_get_rmr_sids(struct fwnode_handle *iommu_fwnode,\n\t\t       struct list_head *head)\n{\n\tiort_iommu_rmr_get_resv_regions(iommu_fwnode, NULL, head);\n}\nEXPORT_SYMBOL_GPL(iort_get_rmr_sids);\n\n \nvoid iort_put_rmr_sids(struct fwnode_handle *iommu_fwnode,\n\t\t       struct list_head *head)\n{\n\tstruct iommu_resv_region *entry, *next;\n\n\tlist_for_each_entry_safe(entry, next, head, list)\n\t\tentry->free(NULL, entry);\n}\nEXPORT_SYMBOL_GPL(iort_put_rmr_sids);\n\nstatic inline bool iort_iommu_driver_enabled(u8 type)\n{\n\tswitch (type) {\n\tcase ACPI_IORT_NODE_SMMU_V3:\n\t\treturn IS_ENABLED(CONFIG_ARM_SMMU_V3);\n\tcase ACPI_IORT_NODE_SMMU:\n\t\treturn IS_ENABLED(CONFIG_ARM_SMMU);\n\tdefault:\n\t\tpr_warn(\"IORT node type %u does not describe an SMMU\\n\", type);\n\t\treturn false;\n\t}\n}\n\nstatic bool iort_pci_rc_supports_ats(struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_root_complex *pci_rc;\n\n\tpci_rc = (struct acpi_iort_root_complex *)node->node_data;\n\treturn pci_rc->ats_attribute & ACPI_IORT_ATS_SUPPORTED;\n}\n\nstatic int iort_iommu_xlate(struct device *dev, struct acpi_iort_node *node,\n\t\t\t    u32 streamid)\n{\n\tconst struct iommu_ops *ops;\n\tstruct fwnode_handle *iort_fwnode;\n\n\tif (!node)\n\t\treturn -ENODEV;\n\n\tiort_fwnode = iort_get_fwnode(node);\n\tif (!iort_fwnode)\n\t\treturn -ENODEV;\n\n\t \n\tops = iommu_ops_from_fwnode(iort_fwnode);\n\tif (!ops)\n\t\treturn iort_iommu_driver_enabled(node->type) ?\n\t\t       -EPROBE_DEFER : -ENODEV;\n\n\treturn acpi_iommu_fwspec_init(dev, streamid, iort_fwnode, ops);\n}\n\nstruct iort_pci_alias_info {\n\tstruct device *dev;\n\tstruct acpi_iort_node *node;\n};\n\nstatic int iort_pci_iommu_init(struct pci_dev *pdev, u16 alias, void *data)\n{\n\tstruct iort_pci_alias_info *info = data;\n\tstruct acpi_iort_node *parent;\n\tu32 streamid;\n\n\tparent = iort_node_map_id(info->node, alias, &streamid,\n\t\t\t\t  IORT_IOMMU_TYPE);\n\treturn iort_iommu_xlate(info->dev, parent, streamid);\n}\n\nstatic void iort_named_component_init(struct device *dev,\n\t\t\t\t      struct acpi_iort_node *node)\n{\n\tstruct property_entry props[3] = {};\n\tstruct acpi_iort_named_component *nc;\n\n\tnc = (struct acpi_iort_named_component *)node->node_data;\n\tprops[0] = PROPERTY_ENTRY_U32(\"pasid-num-bits\",\n\t\t\t\t      FIELD_GET(ACPI_IORT_NC_PASID_BITS,\n\t\t\t\t\t\tnc->node_flags));\n\tif (nc->node_flags & ACPI_IORT_NC_STALL_SUPPORTED)\n\t\tprops[1] = PROPERTY_ENTRY_BOOL(\"dma-can-stall\");\n\n\tif (device_create_managed_software_node(dev, props, NULL))\n\t\tdev_warn(dev, \"Could not add device properties\\n\");\n}\n\nstatic int iort_nc_iommu_map(struct device *dev, struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_node *parent;\n\tint err = -ENODEV, i = 0;\n\tu32 streamid = 0;\n\n\tdo {\n\n\t\tparent = iort_node_map_platform_id(node, &streamid,\n\t\t\t\t\t\t   IORT_IOMMU_TYPE,\n\t\t\t\t\t\t   i++);\n\n\t\tif (parent)\n\t\t\terr = iort_iommu_xlate(dev, parent, streamid);\n\t} while (parent && !err);\n\n\treturn err;\n}\n\nstatic int iort_nc_iommu_map_id(struct device *dev,\n\t\t\t\tstruct acpi_iort_node *node,\n\t\t\t\tconst u32 *in_id)\n{\n\tstruct acpi_iort_node *parent;\n\tu32 streamid;\n\n\tparent = iort_node_map_id(node, *in_id, &streamid, IORT_IOMMU_TYPE);\n\tif (parent)\n\t\treturn iort_iommu_xlate(dev, parent, streamid);\n\n\treturn -ENODEV;\n}\n\n\n \nint iort_iommu_configure_id(struct device *dev, const u32 *id_in)\n{\n\tstruct acpi_iort_node *node;\n\tint err = -ENODEV;\n\n\tif (dev_is_pci(dev)) {\n\t\tstruct iommu_fwspec *fwspec;\n\t\tstruct pci_bus *bus = to_pci_dev(dev)->bus;\n\t\tstruct iort_pci_alias_info info = { .dev = dev };\n\n\t\tnode = iort_scan_node(ACPI_IORT_NODE_PCI_ROOT_COMPLEX,\n\t\t\t\t      iort_match_node_callback, &bus->dev);\n\t\tif (!node)\n\t\t\treturn -ENODEV;\n\n\t\tinfo.node = node;\n\t\terr = pci_for_each_dma_alias(to_pci_dev(dev),\n\t\t\t\t\t     iort_pci_iommu_init, &info);\n\n\t\tfwspec = dev_iommu_fwspec_get(dev);\n\t\tif (fwspec && iort_pci_rc_supports_ats(node))\n\t\t\tfwspec->flags |= IOMMU_FWSPEC_PCI_RC_ATS;\n\t} else {\n\t\tnode = iort_scan_node(ACPI_IORT_NODE_NAMED_COMPONENT,\n\t\t\t\t      iort_match_node_callback, dev);\n\t\tif (!node)\n\t\t\treturn -ENODEV;\n\n\t\terr = id_in ? iort_nc_iommu_map_id(dev, node, id_in) :\n\t\t\t      iort_nc_iommu_map(dev, node);\n\n\t\tif (!err)\n\t\t\tiort_named_component_init(dev, node);\n\t}\n\n\treturn err;\n}\n\n#else\nvoid iort_iommu_get_resv_regions(struct device *dev, struct list_head *head)\n{ }\nint iort_iommu_configure_id(struct device *dev, const u32 *input_id)\n{ return -ENODEV; }\n#endif\n\nstatic int nc_dma_get_range(struct device *dev, u64 *size)\n{\n\tstruct acpi_iort_node *node;\n\tstruct acpi_iort_named_component *ncomp;\n\n\tnode = iort_scan_node(ACPI_IORT_NODE_NAMED_COMPONENT,\n\t\t\t      iort_match_node_callback, dev);\n\tif (!node)\n\t\treturn -ENODEV;\n\n\tncomp = (struct acpi_iort_named_component *)node->node_data;\n\n\tif (!ncomp->memory_address_limit) {\n\t\tpr_warn(FW_BUG \"Named component missing memory address limit\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t*size = ncomp->memory_address_limit >= 64 ? U64_MAX :\n\t\t\t1ULL<<ncomp->memory_address_limit;\n\n\treturn 0;\n}\n\nstatic int rc_dma_get_range(struct device *dev, u64 *size)\n{\n\tstruct acpi_iort_node *node;\n\tstruct acpi_iort_root_complex *rc;\n\tstruct pci_bus *pbus = to_pci_dev(dev)->bus;\n\n\tnode = iort_scan_node(ACPI_IORT_NODE_PCI_ROOT_COMPLEX,\n\t\t\t      iort_match_node_callback, &pbus->dev);\n\tif (!node || node->revision < 1)\n\t\treturn -ENODEV;\n\n\trc = (struct acpi_iort_root_complex *)node->node_data;\n\n\tif (!rc->memory_address_limit) {\n\t\tpr_warn(FW_BUG \"Root complex missing memory address limit\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t*size = rc->memory_address_limit >= 64 ? U64_MAX :\n\t\t\t1ULL<<rc->memory_address_limit;\n\n\treturn 0;\n}\n\n \nint iort_dma_get_ranges(struct device *dev, u64 *size)\n{\n\tif (dev_is_pci(dev))\n\t\treturn rc_dma_get_range(dev, size);\n\telse\n\t\treturn nc_dma_get_range(dev, size);\n}\n\nstatic void __init acpi_iort_register_irq(int hwirq, const char *name,\n\t\t\t\t\t  int trigger,\n\t\t\t\t\t  struct resource *res)\n{\n\tint irq = acpi_register_gsi(NULL, hwirq, trigger,\n\t\t\t\t    ACPI_ACTIVE_HIGH);\n\n\tif (irq <= 0) {\n\t\tpr_err(\"could not register gsi hwirq %d name [%s]\\n\", hwirq,\n\t\t\t\t\t\t\t\t      name);\n\t\treturn;\n\t}\n\n\tres->start = irq;\n\tres->end = irq;\n\tres->flags = IORESOURCE_IRQ;\n\tres->name = name;\n}\n\nstatic int __init arm_smmu_v3_count_resources(struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu_v3 *smmu;\n\t \n\tint num_res = 1;\n\n\t \n\tsmmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\n\tif (smmu->event_gsiv)\n\t\tnum_res++;\n\n\tif (smmu->pri_gsiv)\n\t\tnum_res++;\n\n\tif (smmu->gerr_gsiv)\n\t\tnum_res++;\n\n\tif (smmu->sync_gsiv)\n\t\tnum_res++;\n\n\treturn num_res;\n}\n\nstatic bool arm_smmu_v3_is_combined_irq(struct acpi_iort_smmu_v3 *smmu)\n{\n\t \n\tif (smmu->model != ACPI_IORT_SMMU_V3_CAVIUM_CN99XX)\n\t\treturn false;\n\n\t \n\treturn smmu->event_gsiv == smmu->pri_gsiv &&\n\t       smmu->event_gsiv == smmu->gerr_gsiv &&\n\t       smmu->event_gsiv == smmu->sync_gsiv;\n}\n\nstatic unsigned long arm_smmu_v3_resource_size(struct acpi_iort_smmu_v3 *smmu)\n{\n\t \n\tif (smmu->model == ACPI_IORT_SMMU_V3_CAVIUM_CN99XX)\n\t\treturn SZ_64K;\n\n\treturn SZ_128K;\n}\n\nstatic void __init arm_smmu_v3_init_resources(struct resource *res,\n\t\t\t\t\t      struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu_v3 *smmu;\n\tint num_res = 0;\n\n\t \n\tsmmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\n\tres[num_res].start = smmu->base_address;\n\tres[num_res].end = smmu->base_address +\n\t\t\t\tarm_smmu_v3_resource_size(smmu) - 1;\n\tres[num_res].flags = IORESOURCE_MEM;\n\n\tnum_res++;\n\tif (arm_smmu_v3_is_combined_irq(smmu)) {\n\t\tif (smmu->event_gsiv)\n\t\t\tacpi_iort_register_irq(smmu->event_gsiv, \"combined\",\n\t\t\t\t\t       ACPI_EDGE_SENSITIVE,\n\t\t\t\t\t       &res[num_res++]);\n\t} else {\n\n\t\tif (smmu->event_gsiv)\n\t\t\tacpi_iort_register_irq(smmu->event_gsiv, \"eventq\",\n\t\t\t\t\t       ACPI_EDGE_SENSITIVE,\n\t\t\t\t\t       &res[num_res++]);\n\n\t\tif (smmu->pri_gsiv)\n\t\t\tacpi_iort_register_irq(smmu->pri_gsiv, \"priq\",\n\t\t\t\t\t       ACPI_EDGE_SENSITIVE,\n\t\t\t\t\t       &res[num_res++]);\n\n\t\tif (smmu->gerr_gsiv)\n\t\t\tacpi_iort_register_irq(smmu->gerr_gsiv, \"gerror\",\n\t\t\t\t\t       ACPI_EDGE_SENSITIVE,\n\t\t\t\t\t       &res[num_res++]);\n\n\t\tif (smmu->sync_gsiv)\n\t\t\tacpi_iort_register_irq(smmu->sync_gsiv, \"cmdq-sync\",\n\t\t\t\t\t       ACPI_EDGE_SENSITIVE,\n\t\t\t\t\t       &res[num_res++]);\n\t}\n}\n\nstatic void __init arm_smmu_v3_dma_configure(struct device *dev,\n\t\t\t\t\t     struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu_v3 *smmu;\n\tenum dev_dma_attr attr;\n\n\t \n\tsmmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\n\tattr = (smmu->flags & ACPI_IORT_SMMU_V3_COHACC_OVERRIDE) ?\n\t\t\tDEV_DMA_COHERENT : DEV_DMA_NON_COHERENT;\n\n\t \n\tdev->dma_mask = &dev->coherent_dma_mask;\n\n\t \n\tacpi_dma_configure(dev, attr);\n}\n\n#if defined(CONFIG_ACPI_NUMA)\n \nstatic int  __init arm_smmu_v3_set_proximity(struct device *dev,\n\t\t\t\t\t      struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu_v3 *smmu;\n\n\tsmmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\tif (smmu->flags & ACPI_IORT_SMMU_V3_PXM_VALID) {\n\t\tint dev_node = pxm_to_node(smmu->pxm);\n\n\t\tif (dev_node != NUMA_NO_NODE && !node_online(dev_node))\n\t\t\treturn -EINVAL;\n\n\t\tset_dev_node(dev, dev_node);\n\t\tpr_info(\"SMMU-v3[%llx] Mapped to Proximity domain %d\\n\",\n\t\t\tsmmu->base_address,\n\t\t\tsmmu->pxm);\n\t}\n\treturn 0;\n}\n#else\n#define arm_smmu_v3_set_proximity NULL\n#endif\n\nstatic int __init arm_smmu_count_resources(struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu *smmu;\n\n\t \n\tsmmu = (struct acpi_iort_smmu *)node->node_data;\n\n\t \n\treturn smmu->context_interrupt_count + 2;\n}\n\nstatic void __init arm_smmu_init_resources(struct resource *res,\n\t\t\t\t\t   struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu *smmu;\n\tint i, hw_irq, trigger, num_res = 0;\n\tu64 *ctx_irq, *glb_irq;\n\n\t \n\tsmmu = (struct acpi_iort_smmu *)node->node_data;\n\n\tres[num_res].start = smmu->base_address;\n\tres[num_res].end = smmu->base_address + smmu->span - 1;\n\tres[num_res].flags = IORESOURCE_MEM;\n\tnum_res++;\n\n\tglb_irq = ACPI_ADD_PTR(u64, node, smmu->global_interrupt_offset);\n\t \n\thw_irq = IORT_IRQ_MASK(glb_irq[0]);\n\ttrigger = IORT_IRQ_TRIGGER_MASK(glb_irq[0]);\n\n\tacpi_iort_register_irq(hw_irq, \"arm-smmu-global\", trigger,\n\t\t\t\t     &res[num_res++]);\n\n\t \n\tctx_irq = ACPI_ADD_PTR(u64, node, smmu->context_interrupt_offset);\n\tfor (i = 0; i < smmu->context_interrupt_count; i++) {\n\t\thw_irq = IORT_IRQ_MASK(ctx_irq[i]);\n\t\ttrigger = IORT_IRQ_TRIGGER_MASK(ctx_irq[i]);\n\n\t\tacpi_iort_register_irq(hw_irq, \"arm-smmu-context\", trigger,\n\t\t\t\t       &res[num_res++]);\n\t}\n}\n\nstatic void __init arm_smmu_dma_configure(struct device *dev,\n\t\t\t\t\t  struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_smmu *smmu;\n\tenum dev_dma_attr attr;\n\n\t \n\tsmmu = (struct acpi_iort_smmu *)node->node_data;\n\n\tattr = (smmu->flags & ACPI_IORT_SMMU_COHERENT_WALK) ?\n\t\t\tDEV_DMA_COHERENT : DEV_DMA_NON_COHERENT;\n\n\t \n\tdev->dma_mask = &dev->coherent_dma_mask;\n\n\t \n\tacpi_dma_configure(dev, attr);\n}\n\nstatic int __init arm_smmu_v3_pmcg_count_resources(struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_pmcg *pmcg;\n\n\t \n\tpmcg = (struct acpi_iort_pmcg *)node->node_data;\n\n\t \n\treturn pmcg->overflow_gsiv ? 3 : 2;\n}\n\nstatic void __init arm_smmu_v3_pmcg_init_resources(struct resource *res,\n\t\t\t\t\t\t   struct acpi_iort_node *node)\n{\n\tstruct acpi_iort_pmcg *pmcg;\n\n\t \n\tpmcg = (struct acpi_iort_pmcg *)node->node_data;\n\n\tres[0].start = pmcg->page0_base_address;\n\tres[0].end = pmcg->page0_base_address + SZ_4K - 1;\n\tres[0].flags = IORESOURCE_MEM;\n\t \n\tif (node->revision > 0) {\n\t\tres[1].start = pmcg->page1_base_address;\n\t\tres[1].end = pmcg->page1_base_address + SZ_4K - 1;\n\t\tres[1].flags = IORESOURCE_MEM;\n\t}\n\n\tif (pmcg->overflow_gsiv)\n\t\tacpi_iort_register_irq(pmcg->overflow_gsiv, \"overflow\",\n\t\t\t\t       ACPI_EDGE_SENSITIVE, &res[2]);\n}\n\nstatic struct acpi_platform_list pmcg_plat_info[] __initdata = {\n\t \n\t{\"HISI  \", \"HIP08   \", 0, ACPI_SIG_IORT, greater_than_or_equal,\n\t \"Erratum #162001800, Erratum #162001900\", IORT_SMMU_V3_PMCG_HISI_HIP08},\n\t \n\t{\"HISI  \", \"HIP09   \", 0, ACPI_SIG_IORT, greater_than_or_equal,\n\t \"Erratum #162001900\", IORT_SMMU_V3_PMCG_HISI_HIP09},\n\t{ }\n};\n\nstatic int __init arm_smmu_v3_pmcg_add_platdata(struct platform_device *pdev)\n{\n\tu32 model;\n\tint idx;\n\n\tidx = acpi_match_platform_list(pmcg_plat_info);\n\tif (idx >= 0)\n\t\tmodel = pmcg_plat_info[idx].data;\n\telse\n\t\tmodel = IORT_SMMU_V3_PMCG_GENERIC;\n\n\treturn platform_device_add_data(pdev, &model, sizeof(model));\n}\n\nstruct iort_dev_config {\n\tconst char *name;\n\tint (*dev_init)(struct acpi_iort_node *node);\n\tvoid (*dev_dma_configure)(struct device *dev,\n\t\t\t\t  struct acpi_iort_node *node);\n\tint (*dev_count_resources)(struct acpi_iort_node *node);\n\tvoid (*dev_init_resources)(struct resource *res,\n\t\t\t\t     struct acpi_iort_node *node);\n\tint (*dev_set_proximity)(struct device *dev,\n\t\t\t\t    struct acpi_iort_node *node);\n\tint (*dev_add_platdata)(struct platform_device *pdev);\n};\n\nstatic const struct iort_dev_config iort_arm_smmu_v3_cfg __initconst = {\n\t.name = \"arm-smmu-v3\",\n\t.dev_dma_configure = arm_smmu_v3_dma_configure,\n\t.dev_count_resources = arm_smmu_v3_count_resources,\n\t.dev_init_resources = arm_smmu_v3_init_resources,\n\t.dev_set_proximity = arm_smmu_v3_set_proximity,\n};\n\nstatic const struct iort_dev_config iort_arm_smmu_cfg __initconst = {\n\t.name = \"arm-smmu\",\n\t.dev_dma_configure = arm_smmu_dma_configure,\n\t.dev_count_resources = arm_smmu_count_resources,\n\t.dev_init_resources = arm_smmu_init_resources,\n};\n\nstatic const struct iort_dev_config iort_arm_smmu_v3_pmcg_cfg __initconst = {\n\t.name = \"arm-smmu-v3-pmcg\",\n\t.dev_count_resources = arm_smmu_v3_pmcg_count_resources,\n\t.dev_init_resources = arm_smmu_v3_pmcg_init_resources,\n\t.dev_add_platdata = arm_smmu_v3_pmcg_add_platdata,\n};\n\nstatic __init const struct iort_dev_config *iort_get_dev_cfg(\n\t\t\tstruct acpi_iort_node *node)\n{\n\tswitch (node->type) {\n\tcase ACPI_IORT_NODE_SMMU_V3:\n\t\treturn &iort_arm_smmu_v3_cfg;\n\tcase ACPI_IORT_NODE_SMMU:\n\t\treturn &iort_arm_smmu_cfg;\n\tcase ACPI_IORT_NODE_PMCG:\n\t\treturn &iort_arm_smmu_v3_pmcg_cfg;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\n \nstatic int __init iort_add_platform_device(struct acpi_iort_node *node,\n\t\t\t\t\t   const struct iort_dev_config *ops)\n{\n\tstruct fwnode_handle *fwnode;\n\tstruct platform_device *pdev;\n\tstruct resource *r;\n\tint ret, count;\n\n\tpdev = platform_device_alloc(ops->name, PLATFORM_DEVID_AUTO);\n\tif (!pdev)\n\t\treturn -ENOMEM;\n\n\tif (ops->dev_set_proximity) {\n\t\tret = ops->dev_set_proximity(&pdev->dev, node);\n\t\tif (ret)\n\t\t\tgoto dev_put;\n\t}\n\n\tcount = ops->dev_count_resources(node);\n\n\tr = kcalloc(count, sizeof(*r), GFP_KERNEL);\n\tif (!r) {\n\t\tret = -ENOMEM;\n\t\tgoto dev_put;\n\t}\n\n\tops->dev_init_resources(r, node);\n\n\tret = platform_device_add_resources(pdev, r, count);\n\t \n\tkfree(r);\n\n\tif (ret)\n\t\tgoto dev_put;\n\n\t \n\tif (ops->dev_add_platdata)\n\t\tret = ops->dev_add_platdata(pdev);\n\telse\n\t\tret = platform_device_add_data(pdev, &node, sizeof(node));\n\n\tif (ret)\n\t\tgoto dev_put;\n\n\tfwnode = iort_get_fwnode(node);\n\n\tif (!fwnode) {\n\t\tret = -ENODEV;\n\t\tgoto dev_put;\n\t}\n\n\tpdev->dev.fwnode = fwnode;\n\n\tif (ops->dev_dma_configure)\n\t\tops->dev_dma_configure(&pdev->dev, node);\n\n\tiort_set_device_domain(&pdev->dev, node);\n\n\tret = platform_device_add(pdev);\n\tif (ret)\n\t\tgoto dma_deconfigure;\n\n\treturn 0;\n\ndma_deconfigure:\n\tarch_teardown_dma_ops(&pdev->dev);\ndev_put:\n\tplatform_device_put(pdev);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_PCI\nstatic void __init iort_enable_acs(struct acpi_iort_node *iort_node)\n{\n\tstatic bool acs_enabled __initdata;\n\n\tif (acs_enabled)\n\t\treturn;\n\n\tif (iort_node->type == ACPI_IORT_NODE_PCI_ROOT_COMPLEX) {\n\t\tstruct acpi_iort_node *parent;\n\t\tstruct acpi_iort_id_mapping *map;\n\t\tint i;\n\n\t\tmap = ACPI_ADD_PTR(struct acpi_iort_id_mapping, iort_node,\n\t\t\t\t   iort_node->mapping_offset);\n\n\t\tfor (i = 0; i < iort_node->mapping_count; i++, map++) {\n\t\t\tif (!map->output_reference)\n\t\t\t\tcontinue;\n\n\t\t\tparent = ACPI_ADD_PTR(struct acpi_iort_node,\n\t\t\t\t\tiort_table,  map->output_reference);\n\t\t\t \n\t\t\tif ((parent->type == ACPI_IORT_NODE_SMMU) ||\n\t\t\t\t(parent->type == ACPI_IORT_NODE_SMMU_V3)) {\n\t\t\t\tpci_request_acs();\n\t\t\t\tacs_enabled = true;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n#else\nstatic inline void iort_enable_acs(struct acpi_iort_node *iort_node) { }\n#endif\n\nstatic void __init iort_init_platform_devices(void)\n{\n\tstruct acpi_iort_node *iort_node, *iort_end;\n\tstruct acpi_table_iort *iort;\n\tstruct fwnode_handle *fwnode;\n\tint i, ret;\n\tconst struct iort_dev_config *ops;\n\n\t \n\tiort = (struct acpi_table_iort *)iort_table;\n\n\t \n\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort,\n\t\t\t\t iort->node_offset);\n\tiort_end = ACPI_ADD_PTR(struct acpi_iort_node, iort,\n\t\t\t\tiort_table->length);\n\n\tfor (i = 0; i < iort->node_count; i++) {\n\t\tif (iort_node >= iort_end) {\n\t\t\tpr_err(\"iort node pointer overflows, bad table\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tiort_enable_acs(iort_node);\n\n\t\tops = iort_get_dev_cfg(iort_node);\n\t\tif (ops) {\n\t\t\tfwnode = acpi_alloc_fwnode_static();\n\t\t\tif (!fwnode)\n\t\t\t\treturn;\n\n\t\t\tiort_set_fwnode(iort_node, fwnode);\n\n\t\t\tret = iort_add_platform_device(iort_node, ops);\n\t\t\tif (ret) {\n\t\t\t\tiort_delete_fwnode(iort_node);\n\t\t\t\tacpi_free_fwnode_static(fwnode);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tiort_node = ACPI_ADD_PTR(struct acpi_iort_node, iort_node,\n\t\t\t\t\t iort_node->length);\n\t}\n}\n\nvoid __init acpi_iort_init(void)\n{\n\tacpi_status status;\n\n\t \n\tstatus = acpi_get_table(ACPI_SIG_IORT, 0, &iort_table);\n\tif (ACPI_FAILURE(status)) {\n\t\tif (status != AE_NOT_FOUND) {\n\t\t\tconst char *msg = acpi_format_exception(status);\n\n\t\t\tpr_err(\"Failed to get table, %s\\n\", msg);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tiort_init_platform_devices();\n}\n\n#ifdef CONFIG_ZONE_DMA\n \nphys_addr_t __init acpi_iort_dma_get_max_cpu_address(void)\n{\n\tphys_addr_t limit = PHYS_ADDR_MAX;\n\tstruct acpi_iort_node *node, *end;\n\tstruct acpi_table_iort *iort;\n\tacpi_status status;\n\tint i;\n\n\tif (acpi_disabled)\n\t\treturn limit;\n\n\tstatus = acpi_get_table(ACPI_SIG_IORT, 0,\n\t\t\t\t(struct acpi_table_header **)&iort);\n\tif (ACPI_FAILURE(status))\n\t\treturn limit;\n\n\tnode = ACPI_ADD_PTR(struct acpi_iort_node, iort, iort->node_offset);\n\tend = ACPI_ADD_PTR(struct acpi_iort_node, iort, iort->header.length);\n\n\tfor (i = 0; i < iort->node_count; i++) {\n\t\tif (node >= end)\n\t\t\tbreak;\n\n\t\tswitch (node->type) {\n\t\t\tstruct acpi_iort_named_component *ncomp;\n\t\t\tstruct acpi_iort_root_complex *rc;\n\t\t\tphys_addr_t local_limit;\n\n\t\tcase ACPI_IORT_NODE_NAMED_COMPONENT:\n\t\t\tncomp = (struct acpi_iort_named_component *)node->node_data;\n\t\t\tlocal_limit = DMA_BIT_MASK(ncomp->memory_address_limit);\n\t\t\tlimit = min_not_zero(limit, local_limit);\n\t\t\tbreak;\n\n\t\tcase ACPI_IORT_NODE_PCI_ROOT_COMPLEX:\n\t\t\tif (node->revision < 1)\n\t\t\t\tbreak;\n\n\t\t\trc = (struct acpi_iort_root_complex *)node->node_data;\n\t\t\tlocal_limit = DMA_BIT_MASK(rc->memory_address_limit);\n\t\t\tlimit = min_not_zero(limit, local_limit);\n\t\t\tbreak;\n\t\t}\n\t\tnode = ACPI_ADD_PTR(struct acpi_iort_node, node, node->length);\n\t}\n\tacpi_put_table(&iort->header);\n\treturn limit;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}