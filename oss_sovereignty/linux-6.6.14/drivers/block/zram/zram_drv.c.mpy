{
  "module_name": "zram_drv.c",
  "hash_id": "8b17ed6959468a78eb7bb44dfe47e7f1a46a926344b792f994d97fe0af225073",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/zram/zram_drv.c",
  "human_readable_source": " \n\n#define KMSG_COMPONENT \"zram\"\n#define pr_fmt(fmt) KMSG_COMPONENT \": \" fmt\n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/bitops.h>\n#include <linux/blkdev.h>\n#include <linux/buffer_head.h>\n#include <linux/device.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/string.h>\n#include <linux/vmalloc.h>\n#include <linux/err.h>\n#include <linux/idr.h>\n#include <linux/sysfs.h>\n#include <linux/debugfs.h>\n#include <linux/cpuhotplug.h>\n#include <linux/part_stat.h>\n\n#include \"zram_drv.h\"\n\nstatic DEFINE_IDR(zram_index_idr);\n \nstatic DEFINE_MUTEX(zram_index_mutex);\n\nstatic int zram_major;\nstatic const char *default_compressor = CONFIG_ZRAM_DEF_COMP;\n\n \nstatic unsigned int num_devices = 1;\n \nstatic size_t huge_class_size;\n\nstatic const struct block_device_operations zram_devops;\n\nstatic void zram_free_page(struct zram *zram, size_t index);\nstatic int zram_read_page(struct zram *zram, struct page *page, u32 index,\n\t\t\t  struct bio *parent);\n\nstatic int zram_slot_trylock(struct zram *zram, u32 index)\n{\n\treturn bit_spin_trylock(ZRAM_LOCK, &zram->table[index].flags);\n}\n\nstatic void zram_slot_lock(struct zram *zram, u32 index)\n{\n\tbit_spin_lock(ZRAM_LOCK, &zram->table[index].flags);\n}\n\nstatic void zram_slot_unlock(struct zram *zram, u32 index)\n{\n\tbit_spin_unlock(ZRAM_LOCK, &zram->table[index].flags);\n}\n\nstatic inline bool init_done(struct zram *zram)\n{\n\treturn zram->disksize;\n}\n\nstatic inline struct zram *dev_to_zram(struct device *dev)\n{\n\treturn (struct zram *)dev_to_disk(dev)->private_data;\n}\n\nstatic unsigned long zram_get_handle(struct zram *zram, u32 index)\n{\n\treturn zram->table[index].handle;\n}\n\nstatic void zram_set_handle(struct zram *zram, u32 index, unsigned long handle)\n{\n\tzram->table[index].handle = handle;\n}\n\n \nstatic bool zram_test_flag(struct zram *zram, u32 index,\n\t\t\tenum zram_pageflags flag)\n{\n\treturn zram->table[index].flags & BIT(flag);\n}\n\nstatic void zram_set_flag(struct zram *zram, u32 index,\n\t\t\tenum zram_pageflags flag)\n{\n\tzram->table[index].flags |= BIT(flag);\n}\n\nstatic void zram_clear_flag(struct zram *zram, u32 index,\n\t\t\tenum zram_pageflags flag)\n{\n\tzram->table[index].flags &= ~BIT(flag);\n}\n\nstatic inline void zram_set_element(struct zram *zram, u32 index,\n\t\t\tunsigned long element)\n{\n\tzram->table[index].element = element;\n}\n\nstatic unsigned long zram_get_element(struct zram *zram, u32 index)\n{\n\treturn zram->table[index].element;\n}\n\nstatic size_t zram_get_obj_size(struct zram *zram, u32 index)\n{\n\treturn zram->table[index].flags & (BIT(ZRAM_FLAG_SHIFT) - 1);\n}\n\nstatic void zram_set_obj_size(struct zram *zram,\n\t\t\t\t\tu32 index, size_t size)\n{\n\tunsigned long flags = zram->table[index].flags >> ZRAM_FLAG_SHIFT;\n\n\tzram->table[index].flags = (flags << ZRAM_FLAG_SHIFT) | size;\n}\n\nstatic inline bool zram_allocated(struct zram *zram, u32 index)\n{\n\treturn zram_get_obj_size(zram, index) ||\n\t\t\tzram_test_flag(zram, index, ZRAM_SAME) ||\n\t\t\tzram_test_flag(zram, index, ZRAM_WB);\n}\n\n#if PAGE_SIZE != 4096\nstatic inline bool is_partial_io(struct bio_vec *bvec)\n{\n\treturn bvec->bv_len != PAGE_SIZE;\n}\n#define ZRAM_PARTIAL_IO\t\t1\n#else\nstatic inline bool is_partial_io(struct bio_vec *bvec)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void zram_set_priority(struct zram *zram, u32 index, u32 prio)\n{\n\tprio &= ZRAM_COMP_PRIORITY_MASK;\n\t \n\tzram->table[index].flags &= ~(ZRAM_COMP_PRIORITY_MASK <<\n\t\t\t\t      ZRAM_COMP_PRIORITY_BIT1);\n\tzram->table[index].flags |= (prio << ZRAM_COMP_PRIORITY_BIT1);\n}\n\nstatic inline u32 zram_get_priority(struct zram *zram, u32 index)\n{\n\tu32 prio = zram->table[index].flags >> ZRAM_COMP_PRIORITY_BIT1;\n\n\treturn prio & ZRAM_COMP_PRIORITY_MASK;\n}\n\nstatic inline void update_used_max(struct zram *zram,\n\t\t\t\t\tconst unsigned long pages)\n{\n\tunsigned long cur_max = atomic_long_read(&zram->stats.max_used_pages);\n\n\tdo {\n\t\tif (cur_max >= pages)\n\t\t\treturn;\n\t} while (!atomic_long_try_cmpxchg(&zram->stats.max_used_pages,\n\t\t\t\t\t  &cur_max, pages));\n}\n\nstatic inline void zram_fill_page(void *ptr, unsigned long len,\n\t\t\t\t\tunsigned long value)\n{\n\tWARN_ON_ONCE(!IS_ALIGNED(len, sizeof(unsigned long)));\n\tmemset_l(ptr, value, len / sizeof(unsigned long));\n}\n\nstatic bool page_same_filled(void *ptr, unsigned long *element)\n{\n\tunsigned long *page;\n\tunsigned long val;\n\tunsigned int pos, last_pos = PAGE_SIZE / sizeof(*page) - 1;\n\n\tpage = (unsigned long *)ptr;\n\tval = page[0];\n\n\tif (val != page[last_pos])\n\t\treturn false;\n\n\tfor (pos = 1; pos < last_pos; pos++) {\n\t\tif (val != page[pos])\n\t\t\treturn false;\n\t}\n\n\t*element = val;\n\n\treturn true;\n}\n\nstatic ssize_t initstate_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tu32 val;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tdown_read(&zram->init_lock);\n\tval = init_done(zram);\n\tup_read(&zram->init_lock);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%u\\n\", val);\n}\n\nstatic ssize_t disksize_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%llu\\n\", zram->disksize);\n}\n\nstatic ssize_t mem_limit_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tu64 limit;\n\tchar *tmp;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tlimit = memparse(buf, &tmp);\n\tif (buf == tmp)  \n\t\treturn -EINVAL;\n\n\tdown_write(&zram->init_lock);\n\tzram->limit_pages = PAGE_ALIGN(limit) >> PAGE_SHIFT;\n\tup_write(&zram->init_lock);\n\n\treturn len;\n}\n\nstatic ssize_t mem_used_max_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tint err;\n\tunsigned long val;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\terr = kstrtoul(buf, 10, &val);\n\tif (err || val != 0)\n\t\treturn -EINVAL;\n\n\tdown_read(&zram->init_lock);\n\tif (init_done(zram)) {\n\t\tatomic_long_set(&zram->stats.max_used_pages,\n\t\t\t\tzs_get_total_pages(zram->mem_pool));\n\t}\n\tup_read(&zram->init_lock);\n\n\treturn len;\n}\n\n \nstatic void mark_idle(struct zram *zram, ktime_t cutoff)\n{\n\tint is_idle = 1;\n\tunsigned long nr_pages = zram->disksize >> PAGE_SHIFT;\n\tint index;\n\n\tfor (index = 0; index < nr_pages; index++) {\n\t\t \n\t\tzram_slot_lock(zram, index);\n\t\tif (zram_allocated(zram, index) &&\n\t\t\t\t!zram_test_flag(zram, index, ZRAM_UNDER_WB)) {\n#ifdef CONFIG_ZRAM_MEMORY_TRACKING\n\t\t\tis_idle = !cutoff || ktime_after(cutoff, zram->table[index].ac_time);\n#endif\n\t\t\tif (is_idle)\n\t\t\t\tzram_set_flag(zram, index, ZRAM_IDLE);\n\t\t}\n\t\tzram_slot_unlock(zram, index);\n\t}\n}\n\nstatic ssize_t idle_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tktime_t cutoff_time = 0;\n\tssize_t rv = -EINVAL;\n\n\tif (!sysfs_streq(buf, \"all\")) {\n\t\t \n\t\tu64 age_sec;\n\n\t\tif (IS_ENABLED(CONFIG_ZRAM_MEMORY_TRACKING) && !kstrtoull(buf, 0, &age_sec))\n\t\t\tcutoff_time = ktime_sub(ktime_get_boottime(),\n\t\t\t\t\tns_to_ktime(age_sec * NSEC_PER_SEC));\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tdown_read(&zram->init_lock);\n\tif (!init_done(zram))\n\t\tgoto out_unlock;\n\n\t \n\tmark_idle(zram, cutoff_time);\n\trv = len;\n\nout_unlock:\n\tup_read(&zram->init_lock);\nout:\n\treturn rv;\n}\n\n#ifdef CONFIG_ZRAM_WRITEBACK\nstatic ssize_t writeback_limit_enable_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tu64 val;\n\tssize_t ret = -EINVAL;\n\n\tif (kstrtoull(buf, 10, &val))\n\t\treturn ret;\n\n\tdown_read(&zram->init_lock);\n\tspin_lock(&zram->wb_limit_lock);\n\tzram->wb_limit_enable = val;\n\tspin_unlock(&zram->wb_limit_lock);\n\tup_read(&zram->init_lock);\n\tret = len;\n\n\treturn ret;\n}\n\nstatic ssize_t writeback_limit_enable_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tbool val;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tdown_read(&zram->init_lock);\n\tspin_lock(&zram->wb_limit_lock);\n\tval = zram->wb_limit_enable;\n\tspin_unlock(&zram->wb_limit_lock);\n\tup_read(&zram->init_lock);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", val);\n}\n\nstatic ssize_t writeback_limit_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tu64 val;\n\tssize_t ret = -EINVAL;\n\n\tif (kstrtoull(buf, 10, &val))\n\t\treturn ret;\n\n\tdown_read(&zram->init_lock);\n\tspin_lock(&zram->wb_limit_lock);\n\tzram->bd_wb_limit = val;\n\tspin_unlock(&zram->wb_limit_lock);\n\tup_read(&zram->init_lock);\n\tret = len;\n\n\treturn ret;\n}\n\nstatic ssize_t writeback_limit_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tu64 val;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tdown_read(&zram->init_lock);\n\tspin_lock(&zram->wb_limit_lock);\n\tval = zram->bd_wb_limit;\n\tspin_unlock(&zram->wb_limit_lock);\n\tup_read(&zram->init_lock);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%llu\\n\", val);\n}\n\nstatic void reset_bdev(struct zram *zram)\n{\n\tstruct block_device *bdev;\n\n\tif (!zram->backing_dev)\n\t\treturn;\n\n\tbdev = zram->bdev;\n\tblkdev_put(bdev, zram);\n\t \n\tfilp_close(zram->backing_dev, NULL);\n\tzram->backing_dev = NULL;\n\tzram->bdev = NULL;\n\tzram->disk->fops = &zram_devops;\n\tkvfree(zram->bitmap);\n\tzram->bitmap = NULL;\n}\n\nstatic ssize_t backing_dev_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct file *file;\n\tstruct zram *zram = dev_to_zram(dev);\n\tchar *p;\n\tssize_t ret;\n\n\tdown_read(&zram->init_lock);\n\tfile = zram->backing_dev;\n\tif (!file) {\n\t\tmemcpy(buf, \"none\\n\", 5);\n\t\tup_read(&zram->init_lock);\n\t\treturn 5;\n\t}\n\n\tp = file_path(file, buf, PAGE_SIZE - 1);\n\tif (IS_ERR(p)) {\n\t\tret = PTR_ERR(p);\n\t\tgoto out;\n\t}\n\n\tret = strlen(p);\n\tmemmove(buf, p, ret);\n\tbuf[ret++] = '\\n';\nout:\n\tup_read(&zram->init_lock);\n\treturn ret;\n}\n\nstatic ssize_t backing_dev_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tchar *file_name;\n\tsize_t sz;\n\tstruct file *backing_dev = NULL;\n\tstruct inode *inode;\n\tstruct address_space *mapping;\n\tunsigned int bitmap_sz;\n\tunsigned long nr_pages, *bitmap = NULL;\n\tstruct block_device *bdev = NULL;\n\tint err;\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tfile_name = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!file_name)\n\t\treturn -ENOMEM;\n\n\tdown_write(&zram->init_lock);\n\tif (init_done(zram)) {\n\t\tpr_info(\"Can't setup backing device for initialized device\\n\");\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tstrscpy(file_name, buf, PATH_MAX);\n\t \n\tsz = strlen(file_name);\n\tif (sz > 0 && file_name[sz - 1] == '\\n')\n\t\tfile_name[sz - 1] = 0x00;\n\n\tbacking_dev = filp_open(file_name, O_RDWR|O_LARGEFILE, 0);\n\tif (IS_ERR(backing_dev)) {\n\t\terr = PTR_ERR(backing_dev);\n\t\tbacking_dev = NULL;\n\t\tgoto out;\n\t}\n\n\tmapping = backing_dev->f_mapping;\n\tinode = mapping->host;\n\n\t \n\tif (!S_ISBLK(inode->i_mode)) {\n\t\terr = -ENOTBLK;\n\t\tgoto out;\n\t}\n\n\tbdev = blkdev_get_by_dev(inode->i_rdev, BLK_OPEN_READ | BLK_OPEN_WRITE,\n\t\t\t\t zram, NULL);\n\tif (IS_ERR(bdev)) {\n\t\terr = PTR_ERR(bdev);\n\t\tbdev = NULL;\n\t\tgoto out;\n\t}\n\n\tnr_pages = i_size_read(inode) >> PAGE_SHIFT;\n\tbitmap_sz = BITS_TO_LONGS(nr_pages) * sizeof(long);\n\tbitmap = kvzalloc(bitmap_sz, GFP_KERNEL);\n\tif (!bitmap) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\treset_bdev(zram);\n\n\tzram->bdev = bdev;\n\tzram->backing_dev = backing_dev;\n\tzram->bitmap = bitmap;\n\tzram->nr_pages = nr_pages;\n\tup_write(&zram->init_lock);\n\n\tpr_info(\"setup backing device %s\\n\", file_name);\n\tkfree(file_name);\n\n\treturn len;\nout:\n\tkvfree(bitmap);\n\n\tif (bdev)\n\t\tblkdev_put(bdev, zram);\n\n\tif (backing_dev)\n\t\tfilp_close(backing_dev, NULL);\n\n\tup_write(&zram->init_lock);\n\n\tkfree(file_name);\n\n\treturn err;\n}\n\nstatic unsigned long alloc_block_bdev(struct zram *zram)\n{\n\tunsigned long blk_idx = 1;\nretry:\n\t \n\tblk_idx = find_next_zero_bit(zram->bitmap, zram->nr_pages, blk_idx);\n\tif (blk_idx == zram->nr_pages)\n\t\treturn 0;\n\n\tif (test_and_set_bit(blk_idx, zram->bitmap))\n\t\tgoto retry;\n\n\tatomic64_inc(&zram->stats.bd_count);\n\treturn blk_idx;\n}\n\nstatic void free_block_bdev(struct zram *zram, unsigned long blk_idx)\n{\n\tint was_set;\n\n\twas_set = test_and_clear_bit(blk_idx, zram->bitmap);\n\tWARN_ON_ONCE(!was_set);\n\tatomic64_dec(&zram->stats.bd_count);\n}\n\nstatic void read_from_bdev_async(struct zram *zram, struct page *page,\n\t\t\tunsigned long entry, struct bio *parent)\n{\n\tstruct bio *bio;\n\n\tbio = bio_alloc(zram->bdev, 1, parent->bi_opf, GFP_NOIO);\n\tbio->bi_iter.bi_sector = entry * (PAGE_SIZE >> 9);\n\t__bio_add_page(bio, page, PAGE_SIZE, 0);\n\tbio_chain(bio, parent);\n\tsubmit_bio(bio);\n}\n\n#define PAGE_WB_SIG \"page_index=\"\n\n#define PAGE_WRITEBACK\t\t\t0\n#define HUGE_WRITEBACK\t\t\t(1<<0)\n#define IDLE_WRITEBACK\t\t\t(1<<1)\n#define INCOMPRESSIBLE_WRITEBACK\t(1<<2)\n\nstatic ssize_t writeback_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tunsigned long nr_pages = zram->disksize >> PAGE_SHIFT;\n\tunsigned long index = 0;\n\tstruct bio bio;\n\tstruct bio_vec bio_vec;\n\tstruct page *page;\n\tssize_t ret = len;\n\tint mode, err;\n\tunsigned long blk_idx = 0;\n\n\tif (sysfs_streq(buf, \"idle\"))\n\t\tmode = IDLE_WRITEBACK;\n\telse if (sysfs_streq(buf, \"huge\"))\n\t\tmode = HUGE_WRITEBACK;\n\telse if (sysfs_streq(buf, \"huge_idle\"))\n\t\tmode = IDLE_WRITEBACK | HUGE_WRITEBACK;\n\telse if (sysfs_streq(buf, \"incompressible\"))\n\t\tmode = INCOMPRESSIBLE_WRITEBACK;\n\telse {\n\t\tif (strncmp(buf, PAGE_WB_SIG, sizeof(PAGE_WB_SIG) - 1))\n\t\t\treturn -EINVAL;\n\n\t\tif (kstrtol(buf + sizeof(PAGE_WB_SIG) - 1, 10, &index) ||\n\t\t\t\tindex >= nr_pages)\n\t\t\treturn -EINVAL;\n\n\t\tnr_pages = 1;\n\t\tmode = PAGE_WRITEBACK;\n\t}\n\n\tdown_read(&zram->init_lock);\n\tif (!init_done(zram)) {\n\t\tret = -EINVAL;\n\t\tgoto release_init_lock;\n\t}\n\n\tif (!zram->backing_dev) {\n\t\tret = -ENODEV;\n\t\tgoto release_init_lock;\n\t}\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto release_init_lock;\n\t}\n\n\tfor (; nr_pages != 0; index++, nr_pages--) {\n\t\tspin_lock(&zram->wb_limit_lock);\n\t\tif (zram->wb_limit_enable && !zram->bd_wb_limit) {\n\t\t\tspin_unlock(&zram->wb_limit_lock);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&zram->wb_limit_lock);\n\n\t\tif (!blk_idx) {\n\t\t\tblk_idx = alloc_block_bdev(zram);\n\t\t\tif (!blk_idx) {\n\t\t\t\tret = -ENOSPC;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tzram_slot_lock(zram, index);\n\t\tif (!zram_allocated(zram, index))\n\t\t\tgoto next;\n\n\t\tif (zram_test_flag(zram, index, ZRAM_WB) ||\n\t\t\t\tzram_test_flag(zram, index, ZRAM_SAME) ||\n\t\t\t\tzram_test_flag(zram, index, ZRAM_UNDER_WB))\n\t\t\tgoto next;\n\n\t\tif (mode & IDLE_WRITEBACK &&\n\t\t    !zram_test_flag(zram, index, ZRAM_IDLE))\n\t\t\tgoto next;\n\t\tif (mode & HUGE_WRITEBACK &&\n\t\t    !zram_test_flag(zram, index, ZRAM_HUGE))\n\t\t\tgoto next;\n\t\tif (mode & INCOMPRESSIBLE_WRITEBACK &&\n\t\t    !zram_test_flag(zram, index, ZRAM_INCOMPRESSIBLE))\n\t\t\tgoto next;\n\n\t\t \n\t\tzram_set_flag(zram, index, ZRAM_UNDER_WB);\n\t\t \n\t\tzram_set_flag(zram, index, ZRAM_IDLE);\n\t\tzram_slot_unlock(zram, index);\n\t\tif (zram_read_page(zram, page, index, NULL)) {\n\t\t\tzram_slot_lock(zram, index);\n\t\t\tzram_clear_flag(zram, index, ZRAM_UNDER_WB);\n\t\t\tzram_clear_flag(zram, index, ZRAM_IDLE);\n\t\t\tzram_slot_unlock(zram, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbio_init(&bio, zram->bdev, &bio_vec, 1,\n\t\t\t REQ_OP_WRITE | REQ_SYNC);\n\t\tbio.bi_iter.bi_sector = blk_idx * (PAGE_SIZE >> 9);\n\t\t__bio_add_page(&bio, page, PAGE_SIZE, 0);\n\n\t\t \n\t\terr = submit_bio_wait(&bio);\n\t\tif (err) {\n\t\t\tzram_slot_lock(zram, index);\n\t\t\tzram_clear_flag(zram, index, ZRAM_UNDER_WB);\n\t\t\tzram_clear_flag(zram, index, ZRAM_IDLE);\n\t\t\tzram_slot_unlock(zram, index);\n\t\t\t \n\t\t\tret = err;\n\t\t\tcontinue;\n\t\t}\n\n\t\tatomic64_inc(&zram->stats.bd_writes);\n\t\t \n\t\tzram_slot_lock(zram, index);\n\t\tif (!zram_allocated(zram, index) ||\n\t\t\t  !zram_test_flag(zram, index, ZRAM_IDLE)) {\n\t\t\tzram_clear_flag(zram, index, ZRAM_UNDER_WB);\n\t\t\tzram_clear_flag(zram, index, ZRAM_IDLE);\n\t\t\tgoto next;\n\t\t}\n\n\t\tzram_free_page(zram, index);\n\t\tzram_clear_flag(zram, index, ZRAM_UNDER_WB);\n\t\tzram_set_flag(zram, index, ZRAM_WB);\n\t\tzram_set_element(zram, index, blk_idx);\n\t\tblk_idx = 0;\n\t\tatomic64_inc(&zram->stats.pages_stored);\n\t\tspin_lock(&zram->wb_limit_lock);\n\t\tif (zram->wb_limit_enable && zram->bd_wb_limit > 0)\n\t\t\tzram->bd_wb_limit -=  1UL << (PAGE_SHIFT - 12);\n\t\tspin_unlock(&zram->wb_limit_lock);\nnext:\n\t\tzram_slot_unlock(zram, index);\n\t}\n\n\tif (blk_idx)\n\t\tfree_block_bdev(zram, blk_idx);\n\t__free_page(page);\nrelease_init_lock:\n\tup_read(&zram->init_lock);\n\n\treturn ret;\n}\n\nstruct zram_work {\n\tstruct work_struct work;\n\tstruct zram *zram;\n\tunsigned long entry;\n\tstruct page *page;\n\tint error;\n};\n\nstatic void zram_sync_read(struct work_struct *work)\n{\n\tstruct zram_work *zw = container_of(work, struct zram_work, work);\n\tstruct bio_vec bv;\n\tstruct bio bio;\n\n\tbio_init(&bio, zw->zram->bdev, &bv, 1, REQ_OP_READ);\n\tbio.bi_iter.bi_sector = zw->entry * (PAGE_SIZE >> 9);\n\t__bio_add_page(&bio, zw->page, PAGE_SIZE, 0);\n\tzw->error = submit_bio_wait(&bio);\n}\n\n \nstatic int read_from_bdev_sync(struct zram *zram, struct page *page,\n\t\t\t\tunsigned long entry)\n{\n\tstruct zram_work work;\n\n\twork.page = page;\n\twork.zram = zram;\n\twork.entry = entry;\n\n\tINIT_WORK_ONSTACK(&work.work, zram_sync_read);\n\tqueue_work(system_unbound_wq, &work.work);\n\tflush_work(&work.work);\n\tdestroy_work_on_stack(&work.work);\n\n\treturn work.error;\n}\n\nstatic int read_from_bdev(struct zram *zram, struct page *page,\n\t\t\tunsigned long entry, struct bio *parent)\n{\n\tatomic64_inc(&zram->stats.bd_reads);\n\tif (!parent) {\n\t\tif (WARN_ON_ONCE(!IS_ENABLED(ZRAM_PARTIAL_IO)))\n\t\t\treturn -EIO;\n\t\treturn read_from_bdev_sync(zram, page, entry);\n\t}\n\tread_from_bdev_async(zram, page, entry, parent);\n\treturn 0;\n}\n#else\nstatic inline void reset_bdev(struct zram *zram) {};\nstatic int read_from_bdev(struct zram *zram, struct page *page,\n\t\t\tunsigned long entry, struct bio *parent)\n{\n\treturn -EIO;\n}\n\nstatic void free_block_bdev(struct zram *zram, unsigned long blk_idx) {};\n#endif\n\n#ifdef CONFIG_ZRAM_MEMORY_TRACKING\n\nstatic struct dentry *zram_debugfs_root;\n\nstatic void zram_debugfs_create(void)\n{\n\tzram_debugfs_root = debugfs_create_dir(\"zram\", NULL);\n}\n\nstatic void zram_debugfs_destroy(void)\n{\n\tdebugfs_remove_recursive(zram_debugfs_root);\n}\n\nstatic void zram_accessed(struct zram *zram, u32 index)\n{\n\tzram_clear_flag(zram, index, ZRAM_IDLE);\n\tzram->table[index].ac_time = ktime_get_boottime();\n}\n\nstatic ssize_t read_block_state(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *kbuf;\n\tssize_t index, written = 0;\n\tstruct zram *zram = file->private_data;\n\tunsigned long nr_pages = zram->disksize >> PAGE_SHIFT;\n\tstruct timespec64 ts;\n\n\tkbuf = kvmalloc(count, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\tdown_read(&zram->init_lock);\n\tif (!init_done(zram)) {\n\t\tup_read(&zram->init_lock);\n\t\tkvfree(kbuf);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (index = *ppos; index < nr_pages; index++) {\n\t\tint copied;\n\n\t\tzram_slot_lock(zram, index);\n\t\tif (!zram_allocated(zram, index))\n\t\t\tgoto next;\n\n\t\tts = ktime_to_timespec64(zram->table[index].ac_time);\n\t\tcopied = snprintf(kbuf + written, count,\n\t\t\t\"%12zd %12lld.%06lu %c%c%c%c%c%c\\n\",\n\t\t\tindex, (s64)ts.tv_sec,\n\t\t\tts.tv_nsec / NSEC_PER_USEC,\n\t\t\tzram_test_flag(zram, index, ZRAM_SAME) ? 's' : '.',\n\t\t\tzram_test_flag(zram, index, ZRAM_WB) ? 'w' : '.',\n\t\t\tzram_test_flag(zram, index, ZRAM_HUGE) ? 'h' : '.',\n\t\t\tzram_test_flag(zram, index, ZRAM_IDLE) ? 'i' : '.',\n\t\t\tzram_get_priority(zram, index) ? 'r' : '.',\n\t\t\tzram_test_flag(zram, index,\n\t\t\t\t       ZRAM_INCOMPRESSIBLE) ? 'n' : '.');\n\n\t\tif (count <= copied) {\n\t\t\tzram_slot_unlock(zram, index);\n\t\t\tbreak;\n\t\t}\n\t\twritten += copied;\n\t\tcount -= copied;\nnext:\n\t\tzram_slot_unlock(zram, index);\n\t\t*ppos += 1;\n\t}\n\n\tup_read(&zram->init_lock);\n\tif (copy_to_user(buf, kbuf, written))\n\t\twritten = -EFAULT;\n\tkvfree(kbuf);\n\n\treturn written;\n}\n\nstatic const struct file_operations proc_zram_block_state_op = {\n\t.open = simple_open,\n\t.read = read_block_state,\n\t.llseek = default_llseek,\n};\n\nstatic void zram_debugfs_register(struct zram *zram)\n{\n\tif (!zram_debugfs_root)\n\t\treturn;\n\n\tzram->debugfs_dir = debugfs_create_dir(zram->disk->disk_name,\n\t\t\t\t\t\tzram_debugfs_root);\n\tdebugfs_create_file(\"block_state\", 0400, zram->debugfs_dir,\n\t\t\t\tzram, &proc_zram_block_state_op);\n}\n\nstatic void zram_debugfs_unregister(struct zram *zram)\n{\n\tdebugfs_remove_recursive(zram->debugfs_dir);\n}\n#else\nstatic void zram_debugfs_create(void) {};\nstatic void zram_debugfs_destroy(void) {};\nstatic void zram_accessed(struct zram *zram, u32 index)\n{\n\tzram_clear_flag(zram, index, ZRAM_IDLE);\n};\nstatic void zram_debugfs_register(struct zram *zram) {};\nstatic void zram_debugfs_unregister(struct zram *zram) {};\n#endif\n\n \nstatic ssize_t max_comp_streams_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", num_online_cpus());\n}\n\nstatic ssize_t max_comp_streams_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\treturn len;\n}\n\nstatic void comp_algorithm_set(struct zram *zram, u32 prio, const char *alg)\n{\n\t \n\tif (zram->comp_algs[prio] != default_compressor)\n\t\tkfree(zram->comp_algs[prio]);\n\n\tzram->comp_algs[prio] = alg;\n}\n\nstatic ssize_t __comp_algorithm_show(struct zram *zram, u32 prio, char *buf)\n{\n\tssize_t sz;\n\n\tdown_read(&zram->init_lock);\n\tsz = zcomp_available_show(zram->comp_algs[prio], buf);\n\tup_read(&zram->init_lock);\n\n\treturn sz;\n}\n\nstatic int __comp_algorithm_store(struct zram *zram, u32 prio, const char *buf)\n{\n\tchar *compressor;\n\tsize_t sz;\n\n\tsz = strlen(buf);\n\tif (sz >= CRYPTO_MAX_ALG_NAME)\n\t\treturn -E2BIG;\n\n\tcompressor = kstrdup(buf, GFP_KERNEL);\n\tif (!compressor)\n\t\treturn -ENOMEM;\n\n\t \n\tif (sz > 0 && compressor[sz - 1] == '\\n')\n\t\tcompressor[sz - 1] = 0x00;\n\n\tif (!zcomp_available_algorithm(compressor)) {\n\t\tkfree(compressor);\n\t\treturn -EINVAL;\n\t}\n\n\tdown_write(&zram->init_lock);\n\tif (init_done(zram)) {\n\t\tup_write(&zram->init_lock);\n\t\tkfree(compressor);\n\t\tpr_info(\"Can't change algorithm for initialized device\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tcomp_algorithm_set(zram, prio, compressor);\n\tup_write(&zram->init_lock);\n\treturn 0;\n}\n\nstatic ssize_t comp_algorithm_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\n\treturn __comp_algorithm_show(zram, ZRAM_PRIMARY_COMP, buf);\n}\n\nstatic ssize_t comp_algorithm_store(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    const char *buf,\n\t\t\t\t    size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tint ret;\n\n\tret = __comp_algorithm_store(zram, ZRAM_PRIMARY_COMP, buf);\n\treturn ret ? ret : len;\n}\n\n#ifdef CONFIG_ZRAM_MULTI_COMP\nstatic ssize_t recomp_algorithm_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tssize_t sz = 0;\n\tu32 prio;\n\n\tfor (prio = ZRAM_SECONDARY_COMP; prio < ZRAM_MAX_COMPS; prio++) {\n\t\tif (!zram->comp_algs[prio])\n\t\t\tcontinue;\n\n\t\tsz += scnprintf(buf + sz, PAGE_SIZE - sz - 2, \"#%d: \", prio);\n\t\tsz += __comp_algorithm_show(zram, prio, buf + sz);\n\t}\n\n\treturn sz;\n}\n\nstatic ssize_t recomp_algorithm_store(struct device *dev,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf,\n\t\t\t\t      size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tint prio = ZRAM_SECONDARY_COMP;\n\tchar *args, *param, *val;\n\tchar *alg = NULL;\n\tint ret;\n\n\targs = skip_spaces(buf);\n\twhile (*args) {\n\t\targs = next_arg(args, &param, &val);\n\n\t\tif (!val || !*val)\n\t\t\treturn -EINVAL;\n\n\t\tif (!strcmp(param, \"algo\")) {\n\t\t\talg = val;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcmp(param, \"priority\")) {\n\t\t\tret = kstrtoint(val, 10, &prio);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (!alg)\n\t\treturn -EINVAL;\n\n\tif (prio < ZRAM_SECONDARY_COMP || prio >= ZRAM_MAX_COMPS)\n\t\treturn -EINVAL;\n\n\tret = __comp_algorithm_store(zram, prio, alg);\n\treturn ret ? ret : len;\n}\n#endif\n\nstatic ssize_t compact_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\n\tdown_read(&zram->init_lock);\n\tif (!init_done(zram)) {\n\t\tup_read(&zram->init_lock);\n\t\treturn -EINVAL;\n\t}\n\n\tzs_compact(zram->mem_pool);\n\tup_read(&zram->init_lock);\n\n\treturn len;\n}\n\nstatic ssize_t io_stat_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tssize_t ret;\n\n\tdown_read(&zram->init_lock);\n\tret = scnprintf(buf, PAGE_SIZE,\n\t\t\t\"%8llu %8llu 0 %8llu\\n\",\n\t\t\t(u64)atomic64_read(&zram->stats.failed_reads),\n\t\t\t(u64)atomic64_read(&zram->stats.failed_writes),\n\t\t\t(u64)atomic64_read(&zram->stats.notify_free));\n\tup_read(&zram->init_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t mm_stat_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tstruct zs_pool_stats pool_stats;\n\tu64 orig_size, mem_used = 0;\n\tlong max_used;\n\tssize_t ret;\n\n\tmemset(&pool_stats, 0x00, sizeof(struct zs_pool_stats));\n\n\tdown_read(&zram->init_lock);\n\tif (init_done(zram)) {\n\t\tmem_used = zs_get_total_pages(zram->mem_pool);\n\t\tzs_pool_stats(zram->mem_pool, &pool_stats);\n\t}\n\n\torig_size = atomic64_read(&zram->stats.pages_stored);\n\tmax_used = atomic_long_read(&zram->stats.max_used_pages);\n\n\tret = scnprintf(buf, PAGE_SIZE,\n\t\t\t\"%8llu %8llu %8llu %8lu %8ld %8llu %8lu %8llu %8llu\\n\",\n\t\t\torig_size << PAGE_SHIFT,\n\t\t\t(u64)atomic64_read(&zram->stats.compr_data_size),\n\t\t\tmem_used << PAGE_SHIFT,\n\t\t\tzram->limit_pages << PAGE_SHIFT,\n\t\t\tmax_used << PAGE_SHIFT,\n\t\t\t(u64)atomic64_read(&zram->stats.same_pages),\n\t\t\tatomic_long_read(&pool_stats.pages_compacted),\n\t\t\t(u64)atomic64_read(&zram->stats.huge_pages),\n\t\t\t(u64)atomic64_read(&zram->stats.huge_pages_since));\n\tup_read(&zram->init_lock);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_ZRAM_WRITEBACK\n#define FOUR_K(x) ((x) * (1 << (PAGE_SHIFT - 12)))\nstatic ssize_t bd_stat_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct zram *zram = dev_to_zram(dev);\n\tssize_t ret;\n\n\tdown_read(&zram->init_lock);\n\tret = scnprintf(buf, PAGE_SIZE,\n\t\t\"%8llu %8llu %8llu\\n\",\n\t\t\tFOUR_K((u64)atomic64_read(&zram->stats.bd_count)),\n\t\t\tFOUR_K((u64)atomic64_read(&zram->stats.bd_reads)),\n\t\t\tFOUR_K((u64)atomic64_read(&zram->stats.bd_writes)));\n\tup_read(&zram->init_lock);\n\n\treturn ret;\n}\n#endif\n\nstatic ssize_t debug_stat_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tint version = 1;\n\tstruct zram *zram = dev_to_zram(dev);\n\tssize_t ret;\n\n\tdown_read(&zram->init_lock);\n\tret = scnprintf(buf, PAGE_SIZE,\n\t\t\t\"version: %d\\n%8llu %8llu\\n\",\n\t\t\tversion,\n\t\t\t(u64)atomic64_read(&zram->stats.writestall),\n\t\t\t(u64)atomic64_read(&zram->stats.miss_free));\n\tup_read(&zram->init_lock);\n\n\treturn ret;\n}\n\nstatic DEVICE_ATTR_RO(io_stat);\nstatic DEVICE_ATTR_RO(mm_stat);\n#ifdef CONFIG_ZRAM_WRITEBACK\nstatic DEVICE_ATTR_RO(bd_stat);\n#endif\nstatic DEVICE_ATTR_RO(debug_stat);\n\nstatic void zram_meta_free(struct zram *zram, u64 disksize)\n{\n\tsize_t num_pages = disksize >> PAGE_SHIFT;\n\tsize_t index;\n\n\t \n\tfor (index = 0; index < num_pages; index++)\n\t\tzram_free_page(zram, index);\n\n\tzs_destroy_pool(zram->mem_pool);\n\tvfree(zram->table);\n}\n\nstatic bool zram_meta_alloc(struct zram *zram, u64 disksize)\n{\n\tsize_t num_pages;\n\n\tnum_pages = disksize >> PAGE_SHIFT;\n\tzram->table = vzalloc(array_size(num_pages, sizeof(*zram->table)));\n\tif (!zram->table)\n\t\treturn false;\n\n\tzram->mem_pool = zs_create_pool(zram->disk->disk_name);\n\tif (!zram->mem_pool) {\n\t\tvfree(zram->table);\n\t\treturn false;\n\t}\n\n\tif (!huge_class_size)\n\t\thuge_class_size = zs_huge_class_size(zram->mem_pool);\n\treturn true;\n}\n\n \nstatic void zram_free_page(struct zram *zram, size_t index)\n{\n\tunsigned long handle;\n\n#ifdef CONFIG_ZRAM_MEMORY_TRACKING\n\tzram->table[index].ac_time = 0;\n#endif\n\tif (zram_test_flag(zram, index, ZRAM_IDLE))\n\t\tzram_clear_flag(zram, index, ZRAM_IDLE);\n\n\tif (zram_test_flag(zram, index, ZRAM_HUGE)) {\n\t\tzram_clear_flag(zram, index, ZRAM_HUGE);\n\t\tatomic64_dec(&zram->stats.huge_pages);\n\t}\n\n\tif (zram_test_flag(zram, index, ZRAM_INCOMPRESSIBLE))\n\t\tzram_clear_flag(zram, index, ZRAM_INCOMPRESSIBLE);\n\n\tzram_set_priority(zram, index, 0);\n\n\tif (zram_test_flag(zram, index, ZRAM_WB)) {\n\t\tzram_clear_flag(zram, index, ZRAM_WB);\n\t\tfree_block_bdev(zram, zram_get_element(zram, index));\n\t\tgoto out;\n\t}\n\n\t \n\tif (zram_test_flag(zram, index, ZRAM_SAME)) {\n\t\tzram_clear_flag(zram, index, ZRAM_SAME);\n\t\tatomic64_dec(&zram->stats.same_pages);\n\t\tgoto out;\n\t}\n\n\thandle = zram_get_handle(zram, index);\n\tif (!handle)\n\t\treturn;\n\n\tzs_free(zram->mem_pool, handle);\n\n\tatomic64_sub(zram_get_obj_size(zram, index),\n\t\t\t&zram->stats.compr_data_size);\nout:\n\tatomic64_dec(&zram->stats.pages_stored);\n\tzram_set_handle(zram, index, 0);\n\tzram_set_obj_size(zram, index, 0);\n\tWARN_ON_ONCE(zram->table[index].flags &\n\t\t~(1UL << ZRAM_LOCK | 1UL << ZRAM_UNDER_WB));\n}\n\n \nstatic int zram_read_from_zspool(struct zram *zram, struct page *page,\n\t\t\t\t u32 index)\n{\n\tstruct zcomp_strm *zstrm;\n\tunsigned long handle;\n\tunsigned int size;\n\tvoid *src, *dst;\n\tu32 prio;\n\tint ret;\n\n\thandle = zram_get_handle(zram, index);\n\tif (!handle || zram_test_flag(zram, index, ZRAM_SAME)) {\n\t\tunsigned long value;\n\t\tvoid *mem;\n\n\t\tvalue = handle ? zram_get_element(zram, index) : 0;\n\t\tmem = kmap_atomic(page);\n\t\tzram_fill_page(mem, PAGE_SIZE, value);\n\t\tkunmap_atomic(mem);\n\t\treturn 0;\n\t}\n\n\tsize = zram_get_obj_size(zram, index);\n\n\tif (size != PAGE_SIZE) {\n\t\tprio = zram_get_priority(zram, index);\n\t\tzstrm = zcomp_stream_get(zram->comps[prio]);\n\t}\n\n\tsrc = zs_map_object(zram->mem_pool, handle, ZS_MM_RO);\n\tif (size == PAGE_SIZE) {\n\t\tdst = kmap_atomic(page);\n\t\tmemcpy(dst, src, PAGE_SIZE);\n\t\tkunmap_atomic(dst);\n\t\tret = 0;\n\t} else {\n\t\tdst = kmap_atomic(page);\n\t\tret = zcomp_decompress(zstrm, src, size, dst);\n\t\tkunmap_atomic(dst);\n\t\tzcomp_stream_put(zram->comps[prio]);\n\t}\n\tzs_unmap_object(zram->mem_pool, handle);\n\treturn ret;\n}\n\nstatic int zram_read_page(struct zram *zram, struct page *page, u32 index,\n\t\t\t  struct bio *parent)\n{\n\tint ret;\n\n\tzram_slot_lock(zram, index);\n\tif (!zram_test_flag(zram, index, ZRAM_WB)) {\n\t\t \n\t\tret = zram_read_from_zspool(zram, page, index);\n\t\tzram_slot_unlock(zram, index);\n\t} else {\n\t\t \n\t\tzram_slot_unlock(zram, index);\n\n\t\tret = read_from_bdev(zram, page, zram_get_element(zram, index),\n\t\t\t\t     parent);\n\t}\n\n\t \n\tif (WARN_ON(ret < 0))\n\t\tpr_err(\"Decompression failed! err=%d, page=%u\\n\", ret, index);\n\n\treturn ret;\n}\n\n \nstatic int zram_bvec_read_partial(struct zram *zram, struct bio_vec *bvec,\n\t\t\t\t  u32 index, int offset)\n{\n\tstruct page *page = alloc_page(GFP_NOIO);\n\tint ret;\n\n\tif (!page)\n\t\treturn -ENOMEM;\n\tret = zram_read_page(zram, page, index, NULL);\n\tif (likely(!ret))\n\t\tmemcpy_to_bvec(bvec, page_address(page) + offset);\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,\n\t\t\t  u32 index, int offset, struct bio *bio)\n{\n\tif (is_partial_io(bvec))\n\t\treturn zram_bvec_read_partial(zram, bvec, index, offset);\n\treturn zram_read_page(zram, bvec->bv_page, index, bio);\n}\n\nstatic int zram_write_page(struct zram *zram, struct page *page, u32 index)\n{\n\tint ret = 0;\n\tunsigned long alloced_pages;\n\tunsigned long handle = -ENOMEM;\n\tunsigned int comp_len = 0;\n\tvoid *src, *dst, *mem;\n\tstruct zcomp_strm *zstrm;\n\tunsigned long element = 0;\n\tenum zram_pageflags flags = 0;\n\n\tmem = kmap_atomic(page);\n\tif (page_same_filled(mem, &element)) {\n\t\tkunmap_atomic(mem);\n\t\t \n\t\tflags = ZRAM_SAME;\n\t\tatomic64_inc(&zram->stats.same_pages);\n\t\tgoto out;\n\t}\n\tkunmap_atomic(mem);\n\ncompress_again:\n\tzstrm = zcomp_stream_get(zram->comps[ZRAM_PRIMARY_COMP]);\n\tsrc = kmap_atomic(page);\n\tret = zcomp_compress(zstrm, src, &comp_len);\n\tkunmap_atomic(src);\n\n\tif (unlikely(ret)) {\n\t\tzcomp_stream_put(zram->comps[ZRAM_PRIMARY_COMP]);\n\t\tpr_err(\"Compression failed! err=%d\\n\", ret);\n\t\tzs_free(zram->mem_pool, handle);\n\t\treturn ret;\n\t}\n\n\tif (comp_len >= huge_class_size)\n\t\tcomp_len = PAGE_SIZE;\n\t \n\tif (IS_ERR_VALUE(handle))\n\t\thandle = zs_malloc(zram->mem_pool, comp_len,\n\t\t\t\t__GFP_KSWAPD_RECLAIM |\n\t\t\t\t__GFP_NOWARN |\n\t\t\t\t__GFP_HIGHMEM |\n\t\t\t\t__GFP_MOVABLE);\n\tif (IS_ERR_VALUE(handle)) {\n\t\tzcomp_stream_put(zram->comps[ZRAM_PRIMARY_COMP]);\n\t\tatomic64_inc(&zram->stats.writestall);\n\t\thandle = zs_malloc(zram->mem_pool, comp_len,\n\t\t\t\tGFP_NOIO | __GFP_HIGHMEM |\n\t\t\t\t__GFP_MOVABLE);\n\t\tif (IS_ERR_VALUE(handle))\n\t\t\treturn PTR_ERR((void *)handle);\n\n\t\tif (comp_len != PAGE_SIZE)\n\t\t\tgoto compress_again;\n\t\t \n\t\tzstrm = zcomp_stream_get(zram->comps[ZRAM_PRIMARY_COMP]);\n\t}\n\n\talloced_pages = zs_get_total_pages(zram->mem_pool);\n\tupdate_used_max(zram, alloced_pages);\n\n\tif (zram->limit_pages && alloced_pages > zram->limit_pages) {\n\t\tzcomp_stream_put(zram->comps[ZRAM_PRIMARY_COMP]);\n\t\tzs_free(zram->mem_pool, handle);\n\t\treturn -ENOMEM;\n\t}\n\n\tdst = zs_map_object(zram->mem_pool, handle, ZS_MM_WO);\n\n\tsrc = zstrm->buffer;\n\tif (comp_len == PAGE_SIZE)\n\t\tsrc = kmap_atomic(page);\n\tmemcpy(dst, src, comp_len);\n\tif (comp_len == PAGE_SIZE)\n\t\tkunmap_atomic(src);\n\n\tzcomp_stream_put(zram->comps[ZRAM_PRIMARY_COMP]);\n\tzs_unmap_object(zram->mem_pool, handle);\n\tatomic64_add(comp_len, &zram->stats.compr_data_size);\nout:\n\t \n\tzram_slot_lock(zram, index);\n\tzram_free_page(zram, index);\n\n\tif (comp_len == PAGE_SIZE) {\n\t\tzram_set_flag(zram, index, ZRAM_HUGE);\n\t\tatomic64_inc(&zram->stats.huge_pages);\n\t\tatomic64_inc(&zram->stats.huge_pages_since);\n\t}\n\n\tif (flags) {\n\t\tzram_set_flag(zram, index, flags);\n\t\tzram_set_element(zram, index, element);\n\t}  else {\n\t\tzram_set_handle(zram, index, handle);\n\t\tzram_set_obj_size(zram, index, comp_len);\n\t}\n\tzram_slot_unlock(zram, index);\n\n\t \n\tatomic64_inc(&zram->stats.pages_stored);\n\treturn ret;\n}\n\n \nstatic int zram_bvec_write_partial(struct zram *zram, struct bio_vec *bvec,\n\t\t\t\t   u32 index, int offset, struct bio *bio)\n{\n\tstruct page *page = alloc_page(GFP_NOIO);\n\tint ret;\n\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tret = zram_read_page(zram, page, index, bio);\n\tif (!ret) {\n\t\tmemcpy_from_bvec(page_address(page) + offset, bvec);\n\t\tret = zram_write_page(zram, page, index);\n\t}\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int zram_bvec_write(struct zram *zram, struct bio_vec *bvec,\n\t\t\t   u32 index, int offset, struct bio *bio)\n{\n\tif (is_partial_io(bvec))\n\t\treturn zram_bvec_write_partial(zram, bvec, index, offset, bio);\n\treturn zram_write_page(zram, bvec->bv_page, index);\n}\n\n#ifdef CONFIG_ZRAM_MULTI_COMP\n \nstatic int zram_recompress(struct zram *zram, u32 index, struct page *page,\n\t\t\t   u32 threshold, u32 prio, u32 prio_max)\n{\n\tstruct zcomp_strm *zstrm = NULL;\n\tunsigned long handle_old;\n\tunsigned long handle_new;\n\tunsigned int comp_len_old;\n\tunsigned int comp_len_new;\n\tunsigned int class_index_old;\n\tunsigned int class_index_new;\n\tu32 num_recomps = 0;\n\tvoid *src, *dst;\n\tint ret;\n\n\thandle_old = zram_get_handle(zram, index);\n\tif (!handle_old)\n\t\treturn -EINVAL;\n\n\tcomp_len_old = zram_get_obj_size(zram, index);\n\t \n\tif (comp_len_old < threshold)\n\t\treturn 0;\n\n\tret = zram_read_from_zspool(zram, page, index);\n\tif (ret)\n\t\treturn ret;\n\n\tclass_index_old = zs_lookup_class_index(zram->mem_pool, comp_len_old);\n\t \n\tfor (; prio < prio_max; prio++) {\n\t\tif (!zram->comps[prio])\n\t\t\tcontinue;\n\n\t\t \n\t\tif (prio <= zram_get_priority(zram, index))\n\t\t\tcontinue;\n\n\t\tnum_recomps++;\n\t\tzstrm = zcomp_stream_get(zram->comps[prio]);\n\t\tsrc = kmap_atomic(page);\n\t\tret = zcomp_compress(zstrm, src, &comp_len_new);\n\t\tkunmap_atomic(src);\n\n\t\tif (ret) {\n\t\t\tzcomp_stream_put(zram->comps[prio]);\n\t\t\treturn ret;\n\t\t}\n\n\t\tclass_index_new = zs_lookup_class_index(zram->mem_pool,\n\t\t\t\t\t\t\tcomp_len_new);\n\n\t\t \n\t\tif (class_index_new >= class_index_old ||\n\t\t    (threshold && comp_len_new >= threshold)) {\n\t\t\tzcomp_stream_put(zram->comps[prio]);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tbreak;\n\t}\n\n\t \n\tif (!zstrm)\n\t\treturn 0;\n\n\tif (class_index_new >= class_index_old) {\n\t\t \n\t\tif (num_recomps == zram->num_active_comps - 1)\n\t\t\tzram_set_flag(zram, index, ZRAM_INCOMPRESSIBLE);\n\t\treturn 0;\n\t}\n\n\t \n\tif (threshold && comp_len_new >= threshold)\n\t\treturn 0;\n\n\t \n\thandle_new = zs_malloc(zram->mem_pool, comp_len_new,\n\t\t\t       __GFP_KSWAPD_RECLAIM |\n\t\t\t       __GFP_NOWARN |\n\t\t\t       __GFP_HIGHMEM |\n\t\t\t       __GFP_MOVABLE);\n\tif (IS_ERR_VALUE(handle_new)) {\n\t\tzcomp_stream_put(zram->comps[prio]);\n\t\treturn PTR_ERR((void *)handle_new);\n\t}\n\n\tdst = zs_map_object(zram->mem_pool, handle_new, ZS_MM_WO);\n\tmemcpy(dst, zstrm->buffer, comp_len_new);\n\tzcomp_stream_put(zram->comps[prio]);\n\n\tzs_unmap_object(zram->mem_pool, handle_new);\n\n\tzram_free_page(zram, index);\n\tzram_set_handle(zram, index, handle_new);\n\tzram_set_obj_size(zram, index, comp_len_new);\n\tzram_set_priority(zram, index, prio);\n\n\tatomic64_add(comp_len_new, &zram->stats.compr_data_size);\n\tatomic64_inc(&zram->stats.pages_stored);\n\n\treturn 0;\n}\n\n#define RECOMPRESS_IDLE\t\t(1 << 0)\n#define RECOMPRESS_HUGE\t\t(1 << 1)\n\nstatic ssize_t recompress_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tconst char *buf, size_t len)\n{\n\tu32 prio = ZRAM_SECONDARY_COMP, prio_max = ZRAM_MAX_COMPS;\n\tstruct zram *zram = dev_to_zram(dev);\n\tunsigned long nr_pages = zram->disksize >> PAGE_SHIFT;\n\tchar *args, *param, *val, *algo = NULL;\n\tu32 mode = 0, threshold = 0;\n\tunsigned long index;\n\tstruct page *page;\n\tssize_t ret;\n\n\targs = skip_spaces(buf);\n\twhile (*args) {\n\t\targs = next_arg(args, &param, &val);\n\n\t\tif (!val || !*val)\n\t\t\treturn -EINVAL;\n\n\t\tif (!strcmp(param, \"type\")) {\n\t\t\tif (!strcmp(val, \"idle\"))\n\t\t\t\tmode = RECOMPRESS_IDLE;\n\t\t\tif (!strcmp(val, \"huge\"))\n\t\t\t\tmode = RECOMPRESS_HUGE;\n\t\t\tif (!strcmp(val, \"huge_idle\"))\n\t\t\t\tmode = RECOMPRESS_IDLE | RECOMPRESS_HUGE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcmp(param, \"threshold\")) {\n\t\t\t \n\t\t\tret = kstrtouint(val, 10, &threshold);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcmp(param, \"algo\")) {\n\t\t\talgo = val;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (threshold >= huge_class_size)\n\t\treturn -EINVAL;\n\n\tdown_read(&zram->init_lock);\n\tif (!init_done(zram)) {\n\t\tret = -EINVAL;\n\t\tgoto release_init_lock;\n\t}\n\n\tif (algo) {\n\t\tbool found = false;\n\n\t\tfor (; prio < ZRAM_MAX_COMPS; prio++) {\n\t\t\tif (!zram->comp_algs[prio])\n\t\t\t\tcontinue;\n\n\t\t\tif (!strcmp(zram->comp_algs[prio], algo)) {\n\t\t\t\tprio_max = min(prio + 1, ZRAM_MAX_COMPS);\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto release_init_lock;\n\t\t}\n\t}\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto release_init_lock;\n\t}\n\n\tret = len;\n\tfor (index = 0; index < nr_pages; index++) {\n\t\tint err = 0;\n\n\t\tzram_slot_lock(zram, index);\n\n\t\tif (!zram_allocated(zram, index))\n\t\t\tgoto next;\n\n\t\tif (mode & RECOMPRESS_IDLE &&\n\t\t    !zram_test_flag(zram, index, ZRAM_IDLE))\n\t\t\tgoto next;\n\n\t\tif (mode & RECOMPRESS_HUGE &&\n\t\t    !zram_test_flag(zram, index, ZRAM_HUGE))\n\t\t\tgoto next;\n\n\t\tif (zram_test_flag(zram, index, ZRAM_WB) ||\n\t\t    zram_test_flag(zram, index, ZRAM_UNDER_WB) ||\n\t\t    zram_test_flag(zram, index, ZRAM_SAME) ||\n\t\t    zram_test_flag(zram, index, ZRAM_INCOMPRESSIBLE))\n\t\t\tgoto next;\n\n\t\terr = zram_recompress(zram, index, page, threshold,\n\t\t\t\t      prio, prio_max);\nnext:\n\t\tzram_slot_unlock(zram, index);\n\t\tif (err) {\n\t\t\tret = err;\n\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\t__free_page(page);\n\nrelease_init_lock:\n\tup_read(&zram->init_lock);\n\treturn ret;\n}\n#endif\n\nstatic void zram_bio_discard(struct zram *zram, struct bio *bio)\n{\n\tsize_t n = bio->bi_iter.bi_size;\n\tu32 index = bio->bi_iter.bi_sector >> SECTORS_PER_PAGE_SHIFT;\n\tu32 offset = (bio->bi_iter.bi_sector & (SECTORS_PER_PAGE - 1)) <<\n\t\t\tSECTOR_SHIFT;\n\n\t \n\tif (offset) {\n\t\tif (n <= (PAGE_SIZE - offset))\n\t\t\treturn;\n\n\t\tn -= (PAGE_SIZE - offset);\n\t\tindex++;\n\t}\n\n\twhile (n >= PAGE_SIZE) {\n\t\tzram_slot_lock(zram, index);\n\t\tzram_free_page(zram, index);\n\t\tzram_slot_unlock(zram, index);\n\t\tatomic64_inc(&zram->stats.notify_free);\n\t\tindex++;\n\t\tn -= PAGE_SIZE;\n\t}\n\n\tbio_endio(bio);\n}\n\nstatic void zram_bio_read(struct zram *zram, struct bio *bio)\n{\n\tunsigned long start_time = bio_start_io_acct(bio);\n\tstruct bvec_iter iter = bio->bi_iter;\n\n\tdo {\n\t\tu32 index = iter.bi_sector >> SECTORS_PER_PAGE_SHIFT;\n\t\tu32 offset = (iter.bi_sector & (SECTORS_PER_PAGE - 1)) <<\n\t\t\t\tSECTOR_SHIFT;\n\t\tstruct bio_vec bv = bio_iter_iovec(bio, iter);\n\n\t\tbv.bv_len = min_t(u32, bv.bv_len, PAGE_SIZE - offset);\n\n\t\tif (zram_bvec_read(zram, &bv, index, offset, bio) < 0) {\n\t\t\tatomic64_inc(&zram->stats.failed_reads);\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbreak;\n\t\t}\n\t\tflush_dcache_page(bv.bv_page);\n\n\t\tzram_slot_lock(zram, index);\n\t\tzram_accessed(zram, index);\n\t\tzram_slot_unlock(zram, index);\n\n\t\tbio_advance_iter_single(bio, &iter, bv.bv_len);\n\t} while (iter.bi_size);\n\n\tbio_end_io_acct(bio, start_time);\n\tbio_endio(bio);\n}\n\nstatic void zram_bio_write(struct zram *zram, struct bio *bio)\n{\n\tunsigned long start_time = bio_start_io_acct(bio);\n\tstruct bvec_iter iter = bio->bi_iter;\n\n\tdo {\n\t\tu32 index = iter.bi_sector >> SECTORS_PER_PAGE_SHIFT;\n\t\tu32 offset = (iter.bi_sector & (SECTORS_PER_PAGE - 1)) <<\n\t\t\t\tSECTOR_SHIFT;\n\t\tstruct bio_vec bv = bio_iter_iovec(bio, iter);\n\n\t\tbv.bv_len = min_t(u32, bv.bv_len, PAGE_SIZE - offset);\n\n\t\tif (zram_bvec_write(zram, &bv, index, offset, bio) < 0) {\n\t\t\tatomic64_inc(&zram->stats.failed_writes);\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbreak;\n\t\t}\n\n\t\tzram_slot_lock(zram, index);\n\t\tzram_accessed(zram, index);\n\t\tzram_slot_unlock(zram, index);\n\n\t\tbio_advance_iter_single(bio, &iter, bv.bv_len);\n\t} while (iter.bi_size);\n\n\tbio_end_io_acct(bio, start_time);\n\tbio_endio(bio);\n}\n\n \nstatic void zram_submit_bio(struct bio *bio)\n{\n\tstruct zram *zram = bio->bi_bdev->bd_disk->private_data;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_READ:\n\t\tzram_bio_read(zram, bio);\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\tzram_bio_write(zram, bio);\n\t\tbreak;\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tzram_bio_discard(zram, bio);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbio_endio(bio);\n\t}\n}\n\nstatic void zram_slot_free_notify(struct block_device *bdev,\n\t\t\t\tunsigned long index)\n{\n\tstruct zram *zram;\n\n\tzram = bdev->bd_disk->private_data;\n\n\tatomic64_inc(&zram->stats.notify_free);\n\tif (!zram_slot_trylock(zram, index)) {\n\t\tatomic64_inc(&zram->stats.miss_free);\n\t\treturn;\n\t}\n\n\tzram_free_page(zram, index);\n\tzram_slot_unlock(zram, index);\n}\n\nstatic void zram_destroy_comps(struct zram *zram)\n{\n\tu32 prio;\n\n\tfor (prio = 0; prio < ZRAM_MAX_COMPS; prio++) {\n\t\tstruct zcomp *comp = zram->comps[prio];\n\n\t\tzram->comps[prio] = NULL;\n\t\tif (!comp)\n\t\t\tcontinue;\n\t\tzcomp_destroy(comp);\n\t\tzram->num_active_comps--;\n\t}\n}\n\nstatic void zram_reset_device(struct zram *zram)\n{\n\tdown_write(&zram->init_lock);\n\n\tzram->limit_pages = 0;\n\n\tif (!init_done(zram)) {\n\t\tup_write(&zram->init_lock);\n\t\treturn;\n\t}\n\n\tset_capacity_and_notify(zram->disk, 0);\n\tpart_stat_set_all(zram->disk->part0, 0);\n\n\t \n\tzram_meta_free(zram, zram->disksize);\n\tzram->disksize = 0;\n\tzram_destroy_comps(zram);\n\tmemset(&zram->stats, 0, sizeof(zram->stats));\n\treset_bdev(zram);\n\n\tcomp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor);\n\tup_write(&zram->init_lock);\n}\n\nstatic ssize_t disksize_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tu64 disksize;\n\tstruct zcomp *comp;\n\tstruct zram *zram = dev_to_zram(dev);\n\tint err;\n\tu32 prio;\n\n\tdisksize = memparse(buf, NULL);\n\tif (!disksize)\n\t\treturn -EINVAL;\n\n\tdown_write(&zram->init_lock);\n\tif (init_done(zram)) {\n\t\tpr_info(\"Cannot change disksize for initialized device\\n\");\n\t\terr = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tdisksize = PAGE_ALIGN(disksize);\n\tif (!zram_meta_alloc(zram, disksize)) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tfor (prio = 0; prio < ZRAM_MAX_COMPS; prio++) {\n\t\tif (!zram->comp_algs[prio])\n\t\t\tcontinue;\n\n\t\tcomp = zcomp_create(zram->comp_algs[prio]);\n\t\tif (IS_ERR(comp)) {\n\t\t\tpr_err(\"Cannot initialise %s compressing backend\\n\",\n\t\t\t       zram->comp_algs[prio]);\n\t\t\terr = PTR_ERR(comp);\n\t\t\tgoto out_free_comps;\n\t\t}\n\n\t\tzram->comps[prio] = comp;\n\t\tzram->num_active_comps++;\n\t}\n\tzram->disksize = disksize;\n\tset_capacity_and_notify(zram->disk, zram->disksize >> SECTOR_SHIFT);\n\tup_write(&zram->init_lock);\n\n\treturn len;\n\nout_free_comps:\n\tzram_destroy_comps(zram);\n\tzram_meta_free(zram, disksize);\nout_unlock:\n\tup_write(&zram->init_lock);\n\treturn err;\n}\n\nstatic ssize_t reset_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tint ret;\n\tunsigned short do_reset;\n\tstruct zram *zram;\n\tstruct gendisk *disk;\n\n\tret = kstrtou16(buf, 10, &do_reset);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!do_reset)\n\t\treturn -EINVAL;\n\n\tzram = dev_to_zram(dev);\n\tdisk = zram->disk;\n\n\tmutex_lock(&disk->open_mutex);\n\t \n\tif (disk_openers(disk) || zram->claim) {\n\t\tmutex_unlock(&disk->open_mutex);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tzram->claim = true;\n\tmutex_unlock(&disk->open_mutex);\n\n\t \n\tsync_blockdev(disk->part0);\n\tzram_reset_device(zram);\n\n\tmutex_lock(&disk->open_mutex);\n\tzram->claim = false;\n\tmutex_unlock(&disk->open_mutex);\n\n\treturn len;\n}\n\nstatic int zram_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct zram *zram = disk->private_data;\n\n\tWARN_ON(!mutex_is_locked(&disk->open_mutex));\n\n\t \n\tif (zram->claim)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\nstatic const struct block_device_operations zram_devops = {\n\t.open = zram_open,\n\t.submit_bio = zram_submit_bio,\n\t.swap_slot_free_notify = zram_slot_free_notify,\n\t.owner = THIS_MODULE\n};\n\nstatic DEVICE_ATTR_WO(compact);\nstatic DEVICE_ATTR_RW(disksize);\nstatic DEVICE_ATTR_RO(initstate);\nstatic DEVICE_ATTR_WO(reset);\nstatic DEVICE_ATTR_WO(mem_limit);\nstatic DEVICE_ATTR_WO(mem_used_max);\nstatic DEVICE_ATTR_WO(idle);\nstatic DEVICE_ATTR_RW(max_comp_streams);\nstatic DEVICE_ATTR_RW(comp_algorithm);\n#ifdef CONFIG_ZRAM_WRITEBACK\nstatic DEVICE_ATTR_RW(backing_dev);\nstatic DEVICE_ATTR_WO(writeback);\nstatic DEVICE_ATTR_RW(writeback_limit);\nstatic DEVICE_ATTR_RW(writeback_limit_enable);\n#endif\n#ifdef CONFIG_ZRAM_MULTI_COMP\nstatic DEVICE_ATTR_RW(recomp_algorithm);\nstatic DEVICE_ATTR_WO(recompress);\n#endif\n\nstatic struct attribute *zram_disk_attrs[] = {\n\t&dev_attr_disksize.attr,\n\t&dev_attr_initstate.attr,\n\t&dev_attr_reset.attr,\n\t&dev_attr_compact.attr,\n\t&dev_attr_mem_limit.attr,\n\t&dev_attr_mem_used_max.attr,\n\t&dev_attr_idle.attr,\n\t&dev_attr_max_comp_streams.attr,\n\t&dev_attr_comp_algorithm.attr,\n#ifdef CONFIG_ZRAM_WRITEBACK\n\t&dev_attr_backing_dev.attr,\n\t&dev_attr_writeback.attr,\n\t&dev_attr_writeback_limit.attr,\n\t&dev_attr_writeback_limit_enable.attr,\n#endif\n\t&dev_attr_io_stat.attr,\n\t&dev_attr_mm_stat.attr,\n#ifdef CONFIG_ZRAM_WRITEBACK\n\t&dev_attr_bd_stat.attr,\n#endif\n\t&dev_attr_debug_stat.attr,\n#ifdef CONFIG_ZRAM_MULTI_COMP\n\t&dev_attr_recomp_algorithm.attr,\n\t&dev_attr_recompress.attr,\n#endif\n\tNULL,\n};\n\nATTRIBUTE_GROUPS(zram_disk);\n\n \nstatic int zram_add(void)\n{\n\tstruct zram *zram;\n\tint ret, device_id;\n\n\tzram = kzalloc(sizeof(struct zram), GFP_KERNEL);\n\tif (!zram)\n\t\treturn -ENOMEM;\n\n\tret = idr_alloc(&zram_index_idr, zram, 0, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto out_free_dev;\n\tdevice_id = ret;\n\n\tinit_rwsem(&zram->init_lock);\n#ifdef CONFIG_ZRAM_WRITEBACK\n\tspin_lock_init(&zram->wb_limit_lock);\n#endif\n\n\t \n\tzram->disk = blk_alloc_disk(NUMA_NO_NODE);\n\tif (!zram->disk) {\n\t\tpr_err(\"Error allocating disk structure for device %d\\n\",\n\t\t\tdevice_id);\n\t\tret = -ENOMEM;\n\t\tgoto out_free_idr;\n\t}\n\n\tzram->disk->major = zram_major;\n\tzram->disk->first_minor = device_id;\n\tzram->disk->minors = 1;\n\tzram->disk->flags |= GENHD_FL_NO_PART;\n\tzram->disk->fops = &zram_devops;\n\tzram->disk->private_data = zram;\n\tsnprintf(zram->disk->disk_name, 16, \"zram%d\", device_id);\n\n\t \n\tset_capacity(zram->disk, 0);\n\t \n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, zram->disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_SYNCHRONOUS, zram->disk->queue);\n\n\t \n\tblk_queue_physical_block_size(zram->disk->queue, PAGE_SIZE);\n\tblk_queue_logical_block_size(zram->disk->queue,\n\t\t\t\t\tZRAM_LOGICAL_BLOCK_SIZE);\n\tblk_queue_io_min(zram->disk->queue, PAGE_SIZE);\n\tblk_queue_io_opt(zram->disk->queue, PAGE_SIZE);\n\tzram->disk->queue->limits.discard_granularity = PAGE_SIZE;\n\tblk_queue_max_discard_sectors(zram->disk->queue, UINT_MAX);\n\n\t \n\tif (ZRAM_LOGICAL_BLOCK_SIZE == PAGE_SIZE)\n\t\tblk_queue_max_write_zeroes_sectors(zram->disk->queue, UINT_MAX);\n\n\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, zram->disk->queue);\n\tret = device_add_disk(NULL, zram->disk, zram_disk_groups);\n\tif (ret)\n\t\tgoto out_cleanup_disk;\n\n\tcomp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor);\n\n\tzram_debugfs_register(zram);\n\tpr_info(\"Added device: %s\\n\", zram->disk->disk_name);\n\treturn device_id;\n\nout_cleanup_disk:\n\tput_disk(zram->disk);\nout_free_idr:\n\tidr_remove(&zram_index_idr, device_id);\nout_free_dev:\n\tkfree(zram);\n\treturn ret;\n}\n\nstatic int zram_remove(struct zram *zram)\n{\n\tbool claimed;\n\n\tmutex_lock(&zram->disk->open_mutex);\n\tif (disk_openers(zram->disk)) {\n\t\tmutex_unlock(&zram->disk->open_mutex);\n\t\treturn -EBUSY;\n\t}\n\n\tclaimed = zram->claim;\n\tif (!claimed)\n\t\tzram->claim = true;\n\tmutex_unlock(&zram->disk->open_mutex);\n\n\tzram_debugfs_unregister(zram);\n\n\tif (claimed) {\n\t\t \n\t\t;\n\t} else {\n\t\t \n\t\tsync_blockdev(zram->disk->part0);\n\t\tzram_reset_device(zram);\n\t}\n\n\tpr_info(\"Removed device: %s\\n\", zram->disk->disk_name);\n\n\tdel_gendisk(zram->disk);\n\n\t \n\tWARN_ON_ONCE(claimed && zram->claim);\n\n\t \n\tzram_reset_device(zram);\n\n\tput_disk(zram->disk);\n\tkfree(zram);\n\treturn 0;\n}\n\n \n\n \nstatic ssize_t hot_add_show(const struct class *class,\n\t\t\tconst struct class_attribute *attr,\n\t\t\tchar *buf)\n{\n\tint ret;\n\n\tmutex_lock(&zram_index_mutex);\n\tret = zram_add();\n\tmutex_unlock(&zram_index_mutex);\n\n\tif (ret < 0)\n\t\treturn ret;\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", ret);\n}\n \nstatic struct class_attribute class_attr_hot_add =\n\t__ATTR(hot_add, 0400, hot_add_show, NULL);\n\nstatic ssize_t hot_remove_store(const struct class *class,\n\t\t\tconst struct class_attribute *attr,\n\t\t\tconst char *buf,\n\t\t\tsize_t count)\n{\n\tstruct zram *zram;\n\tint ret, dev_id;\n\n\t \n\tret = kstrtoint(buf, 10, &dev_id);\n\tif (ret)\n\t\treturn ret;\n\tif (dev_id < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&zram_index_mutex);\n\n\tzram = idr_find(&zram_index_idr, dev_id);\n\tif (zram) {\n\t\tret = zram_remove(zram);\n\t\tif (!ret)\n\t\t\tidr_remove(&zram_index_idr, dev_id);\n\t} else {\n\t\tret = -ENODEV;\n\t}\n\n\tmutex_unlock(&zram_index_mutex);\n\treturn ret ? ret : count;\n}\nstatic CLASS_ATTR_WO(hot_remove);\n\nstatic struct attribute *zram_control_class_attrs[] = {\n\t&class_attr_hot_add.attr,\n\t&class_attr_hot_remove.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(zram_control_class);\n\nstatic struct class zram_control_class = {\n\t.name\t\t= \"zram-control\",\n\t.class_groups\t= zram_control_class_groups,\n};\n\nstatic int zram_remove_cb(int id, void *ptr, void *data)\n{\n\tWARN_ON_ONCE(zram_remove(ptr));\n\treturn 0;\n}\n\nstatic void destroy_devices(void)\n{\n\tclass_unregister(&zram_control_class);\n\tidr_for_each(&zram_index_idr, &zram_remove_cb, NULL);\n\tzram_debugfs_destroy();\n\tidr_destroy(&zram_index_idr);\n\tunregister_blkdev(zram_major, \"zram\");\n\tcpuhp_remove_multi_state(CPUHP_ZCOMP_PREPARE);\n}\n\nstatic int __init zram_init(void)\n{\n\tint ret;\n\n\tBUILD_BUG_ON(__NR_ZRAM_PAGEFLAGS > BITS_PER_LONG);\n\n\tret = cpuhp_setup_state_multi(CPUHP_ZCOMP_PREPARE, \"block/zram:prepare\",\n\t\t\t\t      zcomp_cpu_up_prepare, zcomp_cpu_dead);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = class_register(&zram_control_class);\n\tif (ret) {\n\t\tpr_err(\"Unable to register zram-control class\\n\");\n\t\tcpuhp_remove_multi_state(CPUHP_ZCOMP_PREPARE);\n\t\treturn ret;\n\t}\n\n\tzram_debugfs_create();\n\tzram_major = register_blkdev(0, \"zram\");\n\tif (zram_major <= 0) {\n\t\tpr_err(\"Unable to get major number\\n\");\n\t\tclass_unregister(&zram_control_class);\n\t\tcpuhp_remove_multi_state(CPUHP_ZCOMP_PREPARE);\n\t\treturn -EBUSY;\n\t}\n\n\twhile (num_devices != 0) {\n\t\tmutex_lock(&zram_index_mutex);\n\t\tret = zram_add();\n\t\tmutex_unlock(&zram_index_mutex);\n\t\tif (ret < 0)\n\t\t\tgoto out_error;\n\t\tnum_devices--;\n\t}\n\n\treturn 0;\n\nout_error:\n\tdestroy_devices();\n\treturn ret;\n}\n\nstatic void __exit zram_exit(void)\n{\n\tdestroy_devices();\n}\n\nmodule_init(zram_init);\nmodule_exit(zram_exit);\n\nmodule_param(num_devices, uint, 0);\nMODULE_PARM_DESC(num_devices, \"Number of pre-created zram devices\");\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Nitin Gupta <ngupta@vflare.org>\");\nMODULE_DESCRIPTION(\"Compressed RAM Block Device\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}