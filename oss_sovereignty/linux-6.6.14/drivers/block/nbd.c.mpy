{
  "module_name": "nbd.c",
  "hash_id": "f5197ded63906c4096fbb60242597558ed87b2c2be0ffba71ce091f85e0b489c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/nbd.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"nbd: \" fmt\n\n#include <linux/major.h>\n\n#include <linux/blkdev.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/fs.h>\n#include <linux/bio.h>\n#include <linux/stat.h>\n#include <linux/errno.h>\n#include <linux/file.h>\n#include <linux/ioctl.h>\n#include <linux/mutex.h>\n#include <linux/compiler.h>\n#include <linux/completion.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <linux/net.h>\n#include <linux/kthread.h>\n#include <linux/types.h>\n#include <linux/debugfs.h>\n#include <linux/blk-mq.h>\n\n#include <linux/uaccess.h>\n#include <asm/types.h>\n\n#include <linux/nbd.h>\n#include <linux/nbd-netlink.h>\n#include <net/genetlink.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/nbd.h>\n\nstatic DEFINE_IDR(nbd_index_idr);\nstatic DEFINE_MUTEX(nbd_index_mutex);\nstatic struct workqueue_struct *nbd_del_wq;\nstatic int nbd_total_devices = 0;\n\nstruct nbd_sock {\n\tstruct socket *sock;\n\tstruct mutex tx_lock;\n\tstruct request *pending;\n\tint sent;\n\tbool dead;\n\tint fallback_index;\n\tint cookie;\n};\n\nstruct recv_thread_args {\n\tstruct work_struct work;\n\tstruct nbd_device *nbd;\n\tstruct nbd_sock *nsock;\n\tint index;\n};\n\nstruct link_dead_args {\n\tstruct work_struct work;\n\tint index;\n};\n\n#define NBD_RT_TIMEDOUT\t\t\t0\n#define NBD_RT_DISCONNECT_REQUESTED\t1\n#define NBD_RT_DISCONNECTED\t\t2\n#define NBD_RT_HAS_PID_FILE\t\t3\n#define NBD_RT_HAS_CONFIG_REF\t\t4\n#define NBD_RT_BOUND\t\t\t5\n#define NBD_RT_DISCONNECT_ON_CLOSE\t6\n#define NBD_RT_HAS_BACKEND_FILE\t\t7\n\n#define NBD_DESTROY_ON_DISCONNECT\t0\n#define NBD_DISCONNECT_REQUESTED\t1\n\nstruct nbd_config {\n\tu32 flags;\n\tunsigned long runtime_flags;\n\tu64 dead_conn_timeout;\n\n\tstruct nbd_sock **socks;\n\tint num_connections;\n\tatomic_t live_connections;\n\twait_queue_head_t conn_wait;\n\n\tatomic_t recv_threads;\n\twait_queue_head_t recv_wq;\n\tunsigned int blksize_bits;\n\tloff_t bytesize;\n#if IS_ENABLED(CONFIG_DEBUG_FS)\n\tstruct dentry *dbg_dir;\n#endif\n};\n\nstatic inline unsigned int nbd_blksize(struct nbd_config *config)\n{\n\treturn 1u << config->blksize_bits;\n}\n\nstruct nbd_device {\n\tstruct blk_mq_tag_set tag_set;\n\n\tint index;\n\trefcount_t config_refs;\n\trefcount_t refs;\n\tstruct nbd_config *config;\n\tstruct mutex config_lock;\n\tstruct gendisk *disk;\n\tstruct workqueue_struct *recv_workq;\n\tstruct work_struct remove_work;\n\n\tstruct list_head list;\n\tstruct task_struct *task_setup;\n\n\tunsigned long flags;\n\tpid_t pid;  \n\n\tchar *backend;\n};\n\n#define NBD_CMD_REQUEUED\t1\n \n#define NBD_CMD_INFLIGHT\t2\n\nstruct nbd_cmd {\n\tstruct nbd_device *nbd;\n\tstruct mutex lock;\n\tint index;\n\tint cookie;\n\tint retries;\n\tblk_status_t status;\n\tunsigned long flags;\n\tu32 cmd_cookie;\n};\n\n#if IS_ENABLED(CONFIG_DEBUG_FS)\nstatic struct dentry *nbd_dbg_dir;\n#endif\n\n#define nbd_name(nbd) ((nbd)->disk->disk_name)\n\n#define NBD_DEF_BLKSIZE_BITS 10\n\nstatic unsigned int nbds_max = 16;\nstatic int max_part = 16;\nstatic int part_shift;\n\nstatic int nbd_dev_dbg_init(struct nbd_device *nbd);\nstatic void nbd_dev_dbg_close(struct nbd_device *nbd);\nstatic void nbd_config_put(struct nbd_device *nbd);\nstatic void nbd_connect_reply(struct genl_info *info, int index);\nstatic int nbd_genl_status(struct sk_buff *skb, struct genl_info *info);\nstatic void nbd_dead_link_work(struct work_struct *work);\nstatic void nbd_disconnect_and_put(struct nbd_device *nbd);\n\nstatic inline struct device *nbd_to_dev(struct nbd_device *nbd)\n{\n\treturn disk_to_dev(nbd->disk);\n}\n\nstatic void nbd_requeue_cmd(struct nbd_cmd *cmd)\n{\n\tstruct request *req = blk_mq_rq_from_pdu(cmd);\n\n\tif (!test_and_set_bit(NBD_CMD_REQUEUED, &cmd->flags))\n\t\tblk_mq_requeue_request(req, true);\n}\n\n#define NBD_COOKIE_BITS 32\n\nstatic u64 nbd_cmd_handle(struct nbd_cmd *cmd)\n{\n\tstruct request *req = blk_mq_rq_from_pdu(cmd);\n\tu32 tag = blk_mq_unique_tag(req);\n\tu64 cookie = cmd->cmd_cookie;\n\n\treturn (cookie << NBD_COOKIE_BITS) | tag;\n}\n\nstatic u32 nbd_handle_to_tag(u64 handle)\n{\n\treturn (u32)handle;\n}\n\nstatic u32 nbd_handle_to_cookie(u64 handle)\n{\n\treturn (u32)(handle >> NBD_COOKIE_BITS);\n}\n\nstatic const char *nbdcmd_to_ascii(int cmd)\n{\n\tswitch (cmd) {\n\tcase  NBD_CMD_READ: return \"read\";\n\tcase NBD_CMD_WRITE: return \"write\";\n\tcase  NBD_CMD_DISC: return \"disconnect\";\n\tcase NBD_CMD_FLUSH: return \"flush\";\n\tcase  NBD_CMD_TRIM: return \"trim/discard\";\n\t}\n\treturn \"invalid\";\n}\n\nstatic ssize_t pid_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct nbd_device *nbd = (struct nbd_device *)disk->private_data;\n\n\treturn sprintf(buf, \"%d\\n\", nbd->pid);\n}\n\nstatic const struct device_attribute pid_attr = {\n\t.attr = { .name = \"pid\", .mode = 0444},\n\t.show = pid_show,\n};\n\nstatic ssize_t backend_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct nbd_device *nbd = (struct nbd_device *)disk->private_data;\n\n\treturn sprintf(buf, \"%s\\n\", nbd->backend ?: \"\");\n}\n\nstatic const struct device_attribute backend_attr = {\n\t.attr = { .name = \"backend\", .mode = 0444},\n\t.show = backend_show,\n};\n\nstatic void nbd_dev_remove(struct nbd_device *nbd)\n{\n\tstruct gendisk *disk = nbd->disk;\n\n\tdel_gendisk(disk);\n\tblk_mq_free_tag_set(&nbd->tag_set);\n\n\t \n\tmutex_lock(&nbd_index_mutex);\n\tidr_remove(&nbd_index_idr, nbd->index);\n\tmutex_unlock(&nbd_index_mutex);\n\tdestroy_workqueue(nbd->recv_workq);\n\tput_disk(disk);\n}\n\nstatic void nbd_dev_remove_work(struct work_struct *work)\n{\n\tnbd_dev_remove(container_of(work, struct nbd_device, remove_work));\n}\n\nstatic void nbd_put(struct nbd_device *nbd)\n{\n\tif (!refcount_dec_and_test(&nbd->refs))\n\t\treturn;\n\n\t \n\tif (test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags))\n\t\tqueue_work(nbd_del_wq, &nbd->remove_work);\n\telse\n\t\tnbd_dev_remove(nbd);\n}\n\nstatic int nbd_disconnected(struct nbd_config *config)\n{\n\treturn test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags) ||\n\t\ttest_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags);\n}\n\nstatic void nbd_mark_nsock_dead(struct nbd_device *nbd, struct nbd_sock *nsock,\n\t\t\t\tint notify)\n{\n\tif (!nsock->dead && notify && !nbd_disconnected(nbd->config)) {\n\t\tstruct link_dead_args *args;\n\t\targs = kmalloc(sizeof(struct link_dead_args), GFP_NOIO);\n\t\tif (args) {\n\t\t\tINIT_WORK(&args->work, nbd_dead_link_work);\n\t\t\targs->index = nbd->index;\n\t\t\tqueue_work(system_wq, &args->work);\n\t\t}\n\t}\n\tif (!nsock->dead) {\n\t\tkernel_sock_shutdown(nsock->sock, SHUT_RDWR);\n\t\tif (atomic_dec_return(&nbd->config->live_connections) == 0) {\n\t\t\tif (test_and_clear_bit(NBD_RT_DISCONNECT_REQUESTED,\n\t\t\t\t\t       &nbd->config->runtime_flags)) {\n\t\t\t\tset_bit(NBD_RT_DISCONNECTED,\n\t\t\t\t\t&nbd->config->runtime_flags);\n\t\t\t\tdev_info(nbd_to_dev(nbd),\n\t\t\t\t\t\"Disconnected due to user request.\\n\");\n\t\t\t}\n\t\t}\n\t}\n\tnsock->dead = true;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n}\n\nstatic int nbd_set_size(struct nbd_device *nbd, loff_t bytesize,\n\t\tloff_t blksize)\n{\n\tif (!blksize)\n\t\tblksize = 1u << NBD_DEF_BLKSIZE_BITS;\n\n\tif (blk_validate_block_size(blksize))\n\t\treturn -EINVAL;\n\n\tif (bytesize < 0)\n\t\treturn -EINVAL;\n\n\tnbd->config->bytesize = bytesize;\n\tnbd->config->blksize_bits = __ffs(blksize);\n\n\tif (!nbd->pid)\n\t\treturn 0;\n\n\tif (nbd->config->flags & NBD_FLAG_SEND_TRIM) {\n\t\tnbd->disk->queue->limits.discard_granularity = blksize;\n\t\tblk_queue_max_discard_sectors(nbd->disk->queue, UINT_MAX);\n\t}\n\tblk_queue_logical_block_size(nbd->disk->queue, blksize);\n\tblk_queue_physical_block_size(nbd->disk->queue, blksize);\n\n\tif (max_part)\n\t\tset_bit(GD_NEED_PART_SCAN, &nbd->disk->state);\n\tif (!set_capacity_and_notify(nbd->disk, bytesize >> 9))\n\t\tkobject_uevent(&nbd_to_dev(nbd)->kobj, KOBJ_CHANGE);\n\treturn 0;\n}\n\nstatic void nbd_complete_rq(struct request *req)\n{\n\tstruct nbd_cmd *cmd = blk_mq_rq_to_pdu(req);\n\n\tdev_dbg(nbd_to_dev(cmd->nbd), \"request %p: %s\\n\", req,\n\t\tcmd->status ? \"failed\" : \"done\");\n\n\tblk_mq_end_request(req, cmd->status);\n}\n\n \nstatic void sock_shutdown(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tint i;\n\n\tif (config->num_connections == 0)\n\t\treturn;\n\tif (test_and_set_bit(NBD_RT_DISCONNECTED, &config->runtime_flags))\n\t\treturn;\n\n\tfor (i = 0; i < config->num_connections; i++) {\n\t\tstruct nbd_sock *nsock = config->socks[i];\n\t\tmutex_lock(&nsock->tx_lock);\n\t\tnbd_mark_nsock_dead(nbd, nsock, 0);\n\t\tmutex_unlock(&nsock->tx_lock);\n\t}\n\tdev_warn(disk_to_dev(nbd->disk), \"shutting down sockets\\n\");\n}\n\nstatic u32 req_to_nbd_cmd_type(struct request *req)\n{\n\tswitch (req_op(req)) {\n\tcase REQ_OP_DISCARD:\n\t\treturn NBD_CMD_TRIM;\n\tcase REQ_OP_FLUSH:\n\t\treturn NBD_CMD_FLUSH;\n\tcase REQ_OP_WRITE:\n\t\treturn NBD_CMD_WRITE;\n\tcase REQ_OP_READ:\n\t\treturn NBD_CMD_READ;\n\tdefault:\n\t\treturn U32_MAX;\n\t}\n}\n\nstatic struct nbd_config *nbd_get_config_unlocked(struct nbd_device *nbd)\n{\n\tif (refcount_inc_not_zero(&nbd->config_refs)) {\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\treturn nbd->config;\n\t}\n\n\treturn NULL;\n}\n\nstatic enum blk_eh_timer_return nbd_xmit_timeout(struct request *req)\n{\n\tstruct nbd_cmd *cmd = blk_mq_rq_to_pdu(req);\n\tstruct nbd_device *nbd = cmd->nbd;\n\tstruct nbd_config *config;\n\n\tif (!mutex_trylock(&cmd->lock))\n\t\treturn BLK_EH_RESET_TIMER;\n\n\tif (!test_bit(NBD_CMD_INFLIGHT, &cmd->flags)) {\n\t\tmutex_unlock(&cmd->lock);\n\t\treturn BLK_EH_DONE;\n\t}\n\n\tconfig = nbd_get_config_unlocked(nbd);\n\tif (!config) {\n\t\tcmd->status = BLK_STS_TIMEOUT;\n\t\t__clear_bit(NBD_CMD_INFLIGHT, &cmd->flags);\n\t\tmutex_unlock(&cmd->lock);\n\t\tgoto done;\n\t}\n\n\tif (config->num_connections > 1 ||\n\t    (config->num_connections == 1 && nbd->tag_set.timeout)) {\n\t\tdev_err_ratelimited(nbd_to_dev(nbd),\n\t\t\t\t    \"Connection timed out, retrying (%d/%d alive)\\n\",\n\t\t\t\t    atomic_read(&config->live_connections),\n\t\t\t\t    config->num_connections);\n\t\t \n\t\tif (config->socks) {\n\t\t\tif (cmd->index < config->num_connections) {\n\t\t\t\tstruct nbd_sock *nsock =\n\t\t\t\t\tconfig->socks[cmd->index];\n\t\t\t\tmutex_lock(&nsock->tx_lock);\n\t\t\t\t \n\t\t\t\tif (cmd->cookie == nsock->cookie)\n\t\t\t\t\tnbd_mark_nsock_dead(nbd, nsock, 1);\n\t\t\t\tmutex_unlock(&nsock->tx_lock);\n\t\t\t}\n\t\t\tmutex_unlock(&cmd->lock);\n\t\t\tnbd_requeue_cmd(cmd);\n\t\t\tnbd_config_put(nbd);\n\t\t\treturn BLK_EH_DONE;\n\t\t}\n\t}\n\n\tif (!nbd->tag_set.timeout) {\n\t\t \n\t\tstruct nbd_sock *nsock = config->socks[cmd->index];\n\t\tcmd->retries++;\n\t\tdev_info(nbd_to_dev(nbd), \"Possible stuck request %p: control (%s@%llu,%uB). Runtime %u seconds\\n\",\n\t\t\treq, nbdcmd_to_ascii(req_to_nbd_cmd_type(req)),\n\t\t\t(unsigned long long)blk_rq_pos(req) << 9,\n\t\t\tblk_rq_bytes(req), (req->timeout / HZ) * cmd->retries);\n\n\t\tmutex_lock(&nsock->tx_lock);\n\t\tif (cmd->cookie != nsock->cookie) {\n\t\t\tnbd_requeue_cmd(cmd);\n\t\t\tmutex_unlock(&nsock->tx_lock);\n\t\t\tmutex_unlock(&cmd->lock);\n\t\t\tnbd_config_put(nbd);\n\t\t\treturn BLK_EH_DONE;\n\t\t}\n\t\tmutex_unlock(&nsock->tx_lock);\n\t\tmutex_unlock(&cmd->lock);\n\t\tnbd_config_put(nbd);\n\t\treturn BLK_EH_RESET_TIMER;\n\t}\n\n\tdev_err_ratelimited(nbd_to_dev(nbd), \"Connection timed out\\n\");\n\tset_bit(NBD_RT_TIMEDOUT, &config->runtime_flags);\n\tcmd->status = BLK_STS_IOERR;\n\t__clear_bit(NBD_CMD_INFLIGHT, &cmd->flags);\n\tmutex_unlock(&cmd->lock);\n\tsock_shutdown(nbd);\n\tnbd_config_put(nbd);\ndone:\n\tblk_mq_complete_request(req);\n\treturn BLK_EH_DONE;\n}\n\nstatic int __sock_xmit(struct nbd_device *nbd, struct socket *sock, int send,\n\t\t       struct iov_iter *iter, int msg_flags, int *sent)\n{\n\tint result;\n\tstruct msghdr msg;\n\tunsigned int noreclaim_flag;\n\n\tif (unlikely(!sock)) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\"Attempted %s on closed socket in sock_xmit\\n\",\n\t\t\t(send ? \"send\" : \"recv\"));\n\t\treturn -EINVAL;\n\t}\n\n\tmsg.msg_iter = *iter;\n\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tdo {\n\t\tsock->sk->sk_allocation = GFP_NOIO | __GFP_MEMALLOC;\n\t\tsock->sk->sk_use_task_frag = false;\n\t\tmsg.msg_name = NULL;\n\t\tmsg.msg_namelen = 0;\n\t\tmsg.msg_control = NULL;\n\t\tmsg.msg_controllen = 0;\n\t\tmsg.msg_flags = msg_flags | MSG_NOSIGNAL;\n\n\t\tif (send)\n\t\t\tresult = sock_sendmsg(sock, &msg);\n\t\telse\n\t\t\tresult = sock_recvmsg(sock, &msg, msg.msg_flags);\n\n\t\tif (result <= 0) {\n\t\t\tif (result == 0)\n\t\t\t\tresult = -EPIPE;  \n\t\t\tbreak;\n\t\t}\n\t\tif (sent)\n\t\t\t*sent += result;\n\t} while (msg_data_left(&msg));\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\n\treturn result;\n}\n\n \nstatic int sock_xmit(struct nbd_device *nbd, int index, int send,\n\t\t     struct iov_iter *iter, int msg_flags, int *sent)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock = config->socks[index]->sock;\n\n\treturn __sock_xmit(nbd, sock, send, iter, msg_flags, sent);\n}\n\n \nstatic inline int was_interrupted(int result)\n{\n\treturn result == -ERESTARTSYS || result == -EINTR;\n}\n\n \nstatic int nbd_send_cmd(struct nbd_device *nbd, struct nbd_cmd *cmd, int index)\n{\n\tstruct request *req = blk_mq_rq_from_pdu(cmd);\n\tstruct nbd_config *config = nbd->config;\n\tstruct nbd_sock *nsock = config->socks[index];\n\tint result;\n\tstruct nbd_request request = {.magic = htonl(NBD_REQUEST_MAGIC)};\n\tstruct kvec iov = {.iov_base = &request, .iov_len = sizeof(request)};\n\tstruct iov_iter from;\n\tunsigned long size = blk_rq_bytes(req);\n\tstruct bio *bio;\n\tu64 handle;\n\tu32 type;\n\tu32 nbd_cmd_flags = 0;\n\tint sent = nsock->sent, skip = 0;\n\n\tiov_iter_kvec(&from, ITER_SOURCE, &iov, 1, sizeof(request));\n\n\ttype = req_to_nbd_cmd_type(req);\n\tif (type == U32_MAX)\n\t\treturn -EIO;\n\n\tif (rq_data_dir(req) == WRITE &&\n\t    (config->flags & NBD_FLAG_READ_ONLY)) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t    \"Write on read-only\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (req->cmd_flags & REQ_FUA)\n\t\tnbd_cmd_flags |= NBD_CMD_FLAG_FUA;\n\n\t \n\tif (sent) {\n\t\tif (sent >= sizeof(request)) {\n\t\t\tskip = sent - sizeof(request);\n\n\t\t\t \n\t\t\thandle = nbd_cmd_handle(cmd);\n\n\t\t\tgoto send_pages;\n\t\t}\n\t\tiov_iter_advance(&from, sent);\n\t} else {\n\t\tcmd->cmd_cookie++;\n\t}\n\tcmd->index = index;\n\tcmd->cookie = nsock->cookie;\n\tcmd->retries = 0;\n\trequest.type = htonl(type | nbd_cmd_flags);\n\tif (type != NBD_CMD_FLUSH) {\n\t\trequest.from = cpu_to_be64((u64)blk_rq_pos(req) << 9);\n\t\trequest.len = htonl(size);\n\t}\n\thandle = nbd_cmd_handle(cmd);\n\trequest.cookie = cpu_to_be64(handle);\n\n\ttrace_nbd_send_request(&request, nbd->index, blk_mq_rq_from_pdu(cmd));\n\n\tdev_dbg(nbd_to_dev(nbd), \"request %p: sending control (%s@%llu,%uB)\\n\",\n\t\treq, nbdcmd_to_ascii(type),\n\t\t(unsigned long long)blk_rq_pos(req) << 9, blk_rq_bytes(req));\n\tresult = sock_xmit(nbd, index, 1, &from,\n\t\t\t(type == NBD_CMD_WRITE) ? MSG_MORE : 0, &sent);\n\ttrace_nbd_header_sent(req, handle);\n\tif (result < 0) {\n\t\tif (was_interrupted(result)) {\n\t\t\t \n\t\t\tif (sent) {\n\t\t\t\tnsock->pending = req;\n\t\t\t\tnsock->sent = sent;\n\t\t\t}\n\t\t\tset_bit(NBD_CMD_REQUEUED, &cmd->flags);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\"Send control failed (result %d)\\n\", result);\n\t\treturn -EAGAIN;\n\t}\nsend_pages:\n\tif (type != NBD_CMD_WRITE)\n\t\tgoto out;\n\n\tbio = req->bio;\n\twhile (bio) {\n\t\tstruct bio *next = bio->bi_next;\n\t\tstruct bvec_iter iter;\n\t\tstruct bio_vec bvec;\n\n\t\tbio_for_each_segment(bvec, bio, iter) {\n\t\t\tbool is_last = !next && bio_iter_last(bvec, iter);\n\t\t\tint flags = is_last ? 0 : MSG_MORE;\n\n\t\t\tdev_dbg(nbd_to_dev(nbd), \"request %p: sending %d bytes data\\n\",\n\t\t\t\treq, bvec.bv_len);\n\t\t\tiov_iter_bvec(&from, ITER_SOURCE, &bvec, 1, bvec.bv_len);\n\t\t\tif (skip) {\n\t\t\t\tif (skip >= iov_iter_count(&from)) {\n\t\t\t\t\tskip -= iov_iter_count(&from);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tiov_iter_advance(&from, skip);\n\t\t\t\tskip = 0;\n\t\t\t}\n\t\t\tresult = sock_xmit(nbd, index, 1, &from, flags, &sent);\n\t\t\tif (result < 0) {\n\t\t\t\tif (was_interrupted(result)) {\n\t\t\t\t\t \n\t\t\t\t\tnsock->pending = req;\n\t\t\t\t\tnsock->sent = sent;\n\t\t\t\t\tset_bit(NBD_CMD_REQUEUED, &cmd->flags);\n\t\t\t\t\treturn BLK_STS_RESOURCE;\n\t\t\t\t}\n\t\t\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\t\t\"Send data failed (result %d)\\n\",\n\t\t\t\t\tresult);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t\t \n\t\t\tif (is_last)\n\t\t\t\tbreak;\n\t\t}\n\t\tbio = next;\n\t}\nout:\n\ttrace_nbd_payload_sent(req, handle);\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\treturn 0;\n}\n\nstatic int nbd_read_reply(struct nbd_device *nbd, struct socket *sock,\n\t\t\t  struct nbd_reply *reply)\n{\n\tstruct kvec iov = {.iov_base = reply, .iov_len = sizeof(*reply)};\n\tstruct iov_iter to;\n\tint result;\n\n\treply->magic = 0;\n\tiov_iter_kvec(&to, ITER_DEST, &iov, 1, sizeof(*reply));\n\tresult = __sock_xmit(nbd, sock, 0, &to, MSG_WAITALL, NULL);\n\tif (result < 0) {\n\t\tif (!nbd_disconnected(nbd->config))\n\t\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\t\"Receive control failed (result %d)\\n\", result);\n\t\treturn result;\n\t}\n\n\tif (ntohl(reply->magic) != NBD_REPLY_MAGIC) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Wrong magic (0x%lx)\\n\",\n\t\t\t\t(unsigned long)ntohl(reply->magic));\n\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct nbd_cmd *nbd_handle_reply(struct nbd_device *nbd, int index,\n\t\t\t\t\tstruct nbd_reply *reply)\n{\n\tint result;\n\tstruct nbd_cmd *cmd;\n\tstruct request *req = NULL;\n\tu64 handle;\n\tu16 hwq;\n\tu32 tag;\n\tint ret = 0;\n\n\thandle = be64_to_cpu(reply->cookie);\n\ttag = nbd_handle_to_tag(handle);\n\thwq = blk_mq_unique_tag_to_hwq(tag);\n\tif (hwq < nbd->tag_set.nr_hw_queues)\n\t\treq = blk_mq_tag_to_rq(nbd->tag_set.tags[hwq],\n\t\t\t\t       blk_mq_unique_tag_to_tag(tag));\n\tif (!req || !blk_mq_request_started(req)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Unexpected reply (%d) %p\\n\",\n\t\t\ttag, req);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\ttrace_nbd_header_received(req, handle);\n\tcmd = blk_mq_rq_to_pdu(req);\n\n\tmutex_lock(&cmd->lock);\n\tif (!test_bit(NBD_CMD_INFLIGHT, &cmd->flags)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Suspicious reply %d (status %u flags %lu)\",\n\t\t\ttag, cmd->status, cmd->flags);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (cmd->index != index) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Unexpected reply %d from different sock %d (expected %d)\",\n\t\t\ttag, index, cmd->index);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (cmd->cmd_cookie != nbd_handle_to_cookie(handle)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Double reply on req %p, cmd_cookie %u, handle cookie %u\\n\",\n\t\t\treq, cmd->cmd_cookie, nbd_handle_to_cookie(handle));\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (cmd->status != BLK_STS_OK) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Command already handled %p\\n\",\n\t\t\treq);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (test_bit(NBD_CMD_REQUEUED, &cmd->flags)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Raced with timeout on req %p\\n\",\n\t\t\treq);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (ntohl(reply->error)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Other side returned error (%d)\\n\",\n\t\t\tntohl(reply->error));\n\t\tcmd->status = BLK_STS_IOERR;\n\t\tgoto out;\n\t}\n\n\tdev_dbg(nbd_to_dev(nbd), \"request %p: got reply\\n\", req);\n\tif (rq_data_dir(req) != WRITE) {\n\t\tstruct req_iterator iter;\n\t\tstruct bio_vec bvec;\n\t\tstruct iov_iter to;\n\n\t\trq_for_each_segment(bvec, req, iter) {\n\t\t\tiov_iter_bvec(&to, ITER_DEST, &bvec, 1, bvec.bv_len);\n\t\t\tresult = sock_xmit(nbd, index, 0, &to, MSG_WAITALL, NULL);\n\t\t\tif (result < 0) {\n\t\t\t\tdev_err(disk_to_dev(nbd->disk), \"Receive data failed (result %d)\\n\",\n\t\t\t\t\tresult);\n\t\t\t\t \n\t\t\t\tif (nbd_disconnected(nbd->config)) {\n\t\t\t\t\tcmd->status = BLK_STS_IOERR;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdev_dbg(nbd_to_dev(nbd), \"request %p: got %d bytes data\\n\",\n\t\t\t\treq, bvec.bv_len);\n\t\t}\n\t}\nout:\n\ttrace_nbd_payload_received(req, handle);\n\tmutex_unlock(&cmd->lock);\n\treturn ret ? ERR_PTR(ret) : cmd;\n}\n\nstatic void recv_work(struct work_struct *work)\n{\n\tstruct recv_thread_args *args = container_of(work,\n\t\t\t\t\t\t     struct recv_thread_args,\n\t\t\t\t\t\t     work);\n\tstruct nbd_device *nbd = args->nbd;\n\tstruct nbd_config *config = nbd->config;\n\tstruct request_queue *q = nbd->disk->queue;\n\tstruct nbd_sock *nsock = args->nsock;\n\tstruct nbd_cmd *cmd;\n\tstruct request *rq;\n\n\twhile (1) {\n\t\tstruct nbd_reply reply;\n\n\t\tif (nbd_read_reply(nbd, nsock->sock, &reply))\n\t\t\tbreak;\n\n\t\t \n\t\tif (!percpu_ref_tryget(&q->q_usage_counter)) {\n\t\t\tdev_err(disk_to_dev(nbd->disk), \"%s: no io inflight\\n\",\n\t\t\t\t__func__);\n\t\t\tbreak;\n\t\t}\n\n\t\tcmd = nbd_handle_reply(nbd, args->index, &reply);\n\t\tif (IS_ERR(cmd)) {\n\t\t\tpercpu_ref_put(&q->q_usage_counter);\n\t\t\tbreak;\n\t\t}\n\n\t\trq = blk_mq_rq_from_pdu(cmd);\n\t\tif (likely(!blk_should_fake_timeout(rq->q))) {\n\t\t\tbool complete;\n\n\t\t\tmutex_lock(&cmd->lock);\n\t\t\tcomplete = __test_and_clear_bit(NBD_CMD_INFLIGHT,\n\t\t\t\t\t\t\t&cmd->flags);\n\t\t\tmutex_unlock(&cmd->lock);\n\t\t\tif (complete)\n\t\t\t\tblk_mq_complete_request(rq);\n\t\t}\n\t\tpercpu_ref_put(&q->q_usage_counter);\n\t}\n\n\tmutex_lock(&nsock->tx_lock);\n\tnbd_mark_nsock_dead(nbd, nsock, 1);\n\tmutex_unlock(&nsock->tx_lock);\n\n\tnbd_config_put(nbd);\n\tatomic_dec(&config->recv_threads);\n\twake_up(&config->recv_wq);\n\tkfree(args);\n}\n\nstatic bool nbd_clear_req(struct request *req, void *data)\n{\n\tstruct nbd_cmd *cmd = blk_mq_rq_to_pdu(req);\n\n\t \n\tif (blk_mq_request_completed(req))\n\t\treturn true;\n\n\tmutex_lock(&cmd->lock);\n\tif (!__test_and_clear_bit(NBD_CMD_INFLIGHT, &cmd->flags)) {\n\t\tmutex_unlock(&cmd->lock);\n\t\treturn true;\n\t}\n\tcmd->status = BLK_STS_IOERR;\n\tmutex_unlock(&cmd->lock);\n\n\tblk_mq_complete_request(req);\n\treturn true;\n}\n\nstatic void nbd_clear_que(struct nbd_device *nbd)\n{\n\tblk_mq_quiesce_queue(nbd->disk->queue);\n\tblk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);\n\tblk_mq_unquiesce_queue(nbd->disk->queue);\n\tdev_dbg(disk_to_dev(nbd->disk), \"queue cleared\\n\");\n}\n\nstatic int find_fallback(struct nbd_device *nbd, int index)\n{\n\tstruct nbd_config *config = nbd->config;\n\tint new_index = -1;\n\tstruct nbd_sock *nsock = config->socks[index];\n\tint fallback = nsock->fallback_index;\n\n\tif (test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags))\n\t\treturn new_index;\n\n\tif (config->num_connections <= 1) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t    \"Dead connection, failed to find a fallback\\n\");\n\t\treturn new_index;\n\t}\n\n\tif (fallback >= 0 && fallback < config->num_connections &&\n\t    !config->socks[fallback]->dead)\n\t\treturn fallback;\n\n\tif (nsock->fallback_index < 0 ||\n\t    nsock->fallback_index >= config->num_connections ||\n\t    config->socks[nsock->fallback_index]->dead) {\n\t\tint i;\n\t\tfor (i = 0; i < config->num_connections; i++) {\n\t\t\tif (i == index)\n\t\t\t\tcontinue;\n\t\t\tif (!config->socks[i]->dead) {\n\t\t\t\tnew_index = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnsock->fallback_index = new_index;\n\t\tif (new_index < 0) {\n\t\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t\t    \"Dead connection, failed to find a fallback\\n\");\n\t\t\treturn new_index;\n\t\t}\n\t}\n\tnew_index = nsock->fallback_index;\n\treturn new_index;\n}\n\nstatic int wait_for_reconnect(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tif (!config->dead_conn_timeout)\n\t\treturn 0;\n\n\tif (!wait_event_timeout(config->conn_wait,\n\t\t\t\ttest_bit(NBD_RT_DISCONNECTED,\n\t\t\t\t\t &config->runtime_flags) ||\n\t\t\t\tatomic_read(&config->live_connections) > 0,\n\t\t\t\tconfig->dead_conn_timeout))\n\t\treturn 0;\n\n\treturn !test_bit(NBD_RT_DISCONNECTED, &config->runtime_flags);\n}\n\nstatic int nbd_handle_cmd(struct nbd_cmd *cmd, int index)\n{\n\tstruct request *req = blk_mq_rq_from_pdu(cmd);\n\tstruct nbd_device *nbd = cmd->nbd;\n\tstruct nbd_config *config;\n\tstruct nbd_sock *nsock;\n\tint ret;\n\n\tconfig = nbd_get_config_unlocked(nbd);\n\tif (!config) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t    \"Socks array is empty\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (index >= config->num_connections) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t    \"Attempted send on invalid socket\\n\");\n\t\tnbd_config_put(nbd);\n\t\treturn -EINVAL;\n\t}\n\tcmd->status = BLK_STS_OK;\nagain:\n\tnsock = config->socks[index];\n\tmutex_lock(&nsock->tx_lock);\n\tif (nsock->dead) {\n\t\tint old_index = index;\n\t\tindex = find_fallback(nbd, index);\n\t\tmutex_unlock(&nsock->tx_lock);\n\t\tif (index < 0) {\n\t\t\tif (wait_for_reconnect(nbd)) {\n\t\t\t\tindex = old_index;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\t \n\t\t\tsock_shutdown(nbd);\n\t\t\tnbd_config_put(nbd);\n\t\t\treturn -EIO;\n\t\t}\n\t\tgoto again;\n\t}\n\n\t \n\tblk_mq_start_request(req);\n\tif (unlikely(nsock->pending && nsock->pending != req)) {\n\t\tnbd_requeue_cmd(cmd);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\t \n\tret = nbd_send_cmd(nbd, cmd, index);\n\t \n\tif (!ret)\n\t\t__set_bit(NBD_CMD_INFLIGHT, &cmd->flags);\n\telse if (ret == -EAGAIN) {\n\t\tdev_err_ratelimited(disk_to_dev(nbd->disk),\n\t\t\t\t    \"Request send failed, requeueing\\n\");\n\t\tnbd_mark_nsock_dead(nbd, nsock, 1);\n\t\tnbd_requeue_cmd(cmd);\n\t\tret = 0;\n\t}\nout:\n\tmutex_unlock(&nsock->tx_lock);\n\tnbd_config_put(nbd);\n\treturn ret;\n}\n\nstatic blk_status_t nbd_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct nbd_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);\n\tint ret;\n\n\t \n\tmutex_lock(&cmd->lock);\n\tclear_bit(NBD_CMD_REQUEUED, &cmd->flags);\n\n\t \n\tret = nbd_handle_cmd(cmd, hctx->queue_num);\n\tif (ret < 0)\n\t\tret = BLK_STS_IOERR;\n\telse if (!ret)\n\t\tret = BLK_STS_OK;\n\tmutex_unlock(&cmd->lock);\n\n\treturn ret;\n}\n\nstatic struct socket *nbd_get_socket(struct nbd_device *nbd, unsigned long fd,\n\t\t\t\t     int *err)\n{\n\tstruct socket *sock;\n\n\t*err = 0;\n\tsock = sockfd_lookup(fd, err);\n\tif (!sock)\n\t\treturn NULL;\n\n\tif (sock->ops->shutdown == sock_no_shutdown) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Unsupported socket: shutdown callout must be supported.\\n\");\n\t\t*err = -EINVAL;\n\t\tsockfd_put(sock);\n\t\treturn NULL;\n\t}\n\n\treturn sock;\n}\n\nstatic int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\t \n\tif (arg > INT_MAX)\n\t\treturn -EINVAL;\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\t \n\tblk_mq_freeze_queue(nbd->disk->queue);\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\n\treturn 0;\n\nput_socket:\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\tsockfd_put(sock);\n\treturn err;\n}\n\nstatic int nbd_reconnect_socket(struct nbd_device *nbd, unsigned long arg)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock, *old;\n\tstruct recv_thread_args *args;\n\tint i;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\targs = kzalloc(sizeof(*args), GFP_KERNEL);\n\tif (!args) {\n\t\tsockfd_put(sock);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < config->num_connections; i++) {\n\t\tstruct nbd_sock *nsock = config->socks[i];\n\n\t\tif (!nsock->dead)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&nsock->tx_lock);\n\t\tif (!nsock->dead) {\n\t\t\tmutex_unlock(&nsock->tx_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tsk_set_memalloc(sock->sk);\n\t\tif (nbd->tag_set.timeout)\n\t\t\tsock->sk->sk_sndtimeo = nbd->tag_set.timeout;\n\t\tatomic_inc(&config->recv_threads);\n\t\trefcount_inc(&nbd->config_refs);\n\t\told = nsock->sock;\n\t\tnsock->fallback_index = -1;\n\t\tnsock->sock = sock;\n\t\tnsock->dead = false;\n\t\tINIT_WORK(&args->work, recv_work);\n\t\targs->index = i;\n\t\targs->nbd = nbd;\n\t\targs->nsock = nsock;\n\t\tnsock->cookie++;\n\t\tmutex_unlock(&nsock->tx_lock);\n\t\tsockfd_put(old);\n\n\t\tclear_bit(NBD_RT_DISCONNECTED, &config->runtime_flags);\n\n\t\t \n\t\tqueue_work(nbd->recv_workq, &args->work);\n\n\t\tatomic_inc(&config->live_connections);\n\t\twake_up(&config->conn_wait);\n\t\treturn 0;\n\t}\n\tsockfd_put(sock);\n\tkfree(args);\n\treturn -ENOSPC;\n}\n\nstatic void nbd_bdev_reset(struct nbd_device *nbd)\n{\n\tif (disk_openers(nbd->disk) > 1)\n\t\treturn;\n\tset_capacity(nbd->disk, 0);\n}\n\nstatic void nbd_parse_flags(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tif (config->flags & NBD_FLAG_READ_ONLY)\n\t\tset_disk_ro(nbd->disk, true);\n\telse\n\t\tset_disk_ro(nbd->disk, false);\n\tif (config->flags & NBD_FLAG_SEND_FLUSH) {\n\t\tif (config->flags & NBD_FLAG_SEND_FUA)\n\t\t\tblk_queue_write_cache(nbd->disk->queue, true, true);\n\t\telse\n\t\t\tblk_queue_write_cache(nbd->disk->queue, true, false);\n\t}\n\telse\n\t\tblk_queue_write_cache(nbd->disk->queue, false, false);\n}\n\nstatic void send_disconnects(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct nbd_request request = {\n\t\t.magic = htonl(NBD_REQUEST_MAGIC),\n\t\t.type = htonl(NBD_CMD_DISC),\n\t};\n\tstruct kvec iov = {.iov_base = &request, .iov_len = sizeof(request)};\n\tstruct iov_iter from;\n\tint i, ret;\n\n\tfor (i = 0; i < config->num_connections; i++) {\n\t\tstruct nbd_sock *nsock = config->socks[i];\n\n\t\tiov_iter_kvec(&from, ITER_SOURCE, &iov, 1, sizeof(request));\n\t\tmutex_lock(&nsock->tx_lock);\n\t\tret = sock_xmit(nbd, i, 1, &from, 0, NULL);\n\t\tif (ret < 0)\n\t\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\t\"Send disconnect failed %d\\n\", ret);\n\t\tmutex_unlock(&nsock->tx_lock);\n\t}\n}\n\nstatic int nbd_disconnect(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\n\tdev_info(disk_to_dev(nbd->disk), \"NBD_DISCONNECT\\n\");\n\tset_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags);\n\tset_bit(NBD_DISCONNECT_REQUESTED, &nbd->flags);\n\tsend_disconnects(nbd);\n\treturn 0;\n}\n\nstatic void nbd_clear_sock(struct nbd_device *nbd)\n{\n\tsock_shutdown(nbd);\n\tnbd_clear_que(nbd);\n\tnbd->task_setup = NULL;\n}\n\nstatic void nbd_config_put(struct nbd_device *nbd)\n{\n\tif (refcount_dec_and_mutex_lock(&nbd->config_refs,\n\t\t\t\t\t&nbd->config_lock)) {\n\t\tstruct nbd_config *config = nbd->config;\n\t\tnbd_dev_dbg_close(nbd);\n\t\tinvalidate_disk(nbd->disk);\n\t\tif (nbd->config->bytesize)\n\t\t\tkobject_uevent(&nbd_to_dev(nbd)->kobj, KOBJ_CHANGE);\n\t\tif (test_and_clear_bit(NBD_RT_HAS_PID_FILE,\n\t\t\t\t       &config->runtime_flags))\n\t\t\tdevice_remove_file(disk_to_dev(nbd->disk), &pid_attr);\n\t\tnbd->pid = 0;\n\t\tif (test_and_clear_bit(NBD_RT_HAS_BACKEND_FILE,\n\t\t\t\t       &config->runtime_flags)) {\n\t\t\tdevice_remove_file(disk_to_dev(nbd->disk), &backend_attr);\n\t\t\tkfree(nbd->backend);\n\t\t\tnbd->backend = NULL;\n\t\t}\n\t\tnbd_clear_sock(nbd);\n\t\tif (config->num_connections) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < config->num_connections; i++) {\n\t\t\t\tsockfd_put(config->socks[i]->sock);\n\t\t\t\tkfree(config->socks[i]);\n\t\t\t}\n\t\t\tkfree(config->socks);\n\t\t}\n\t\tkfree(nbd->config);\n\t\tnbd->config = NULL;\n\n\t\tnbd->tag_set.timeout = 0;\n\t\tnbd->disk->queue->limits.discard_granularity = 0;\n\t\tblk_queue_max_discard_sectors(nbd->disk->queue, 0);\n\n\t\tmutex_unlock(&nbd->config_lock);\n\t\tnbd_put(nbd);\n\t\tmodule_put(THIS_MODULE);\n\t}\n}\n\nstatic int nbd_start_device(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tint num_connections = config->num_connections;\n\tint error = 0, i;\n\n\tif (nbd->pid)\n\t\treturn -EBUSY;\n\tif (!config->socks)\n\t\treturn -EINVAL;\n\tif (num_connections > 1 &&\n\t    !(config->flags & NBD_FLAG_CAN_MULTI_CONN)) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"server does not support multiple connections per device.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tblk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);\n\tnbd->pid = task_pid_nr(current);\n\n\tnbd_parse_flags(nbd);\n\n\terror = device_create_file(disk_to_dev(nbd->disk), &pid_attr);\n\tif (error) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"device_create_file failed for pid!\\n\");\n\t\treturn error;\n\t}\n\tset_bit(NBD_RT_HAS_PID_FILE, &config->runtime_flags);\n\n\tnbd_dev_dbg_init(nbd);\n\tfor (i = 0; i < num_connections; i++) {\n\t\tstruct recv_thread_args *args;\n\n\t\targs = kzalloc(sizeof(*args), GFP_KERNEL);\n\t\tif (!args) {\n\t\t\tsock_shutdown(nbd);\n\t\t\t \n\t\t\tif (i)\n\t\t\t\tflush_workqueue(nbd->recv_workq);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsk_set_memalloc(config->socks[i]->sock->sk);\n\t\tif (nbd->tag_set.timeout)\n\t\t\tconfig->socks[i]->sock->sk->sk_sndtimeo =\n\t\t\t\tnbd->tag_set.timeout;\n\t\tatomic_inc(&config->recv_threads);\n\t\trefcount_inc(&nbd->config_refs);\n\t\tINIT_WORK(&args->work, recv_work);\n\t\targs->nbd = nbd;\n\t\targs->nsock = config->socks[i];\n\t\targs->index = i;\n\t\tqueue_work(nbd->recv_workq, &args->work);\n\t}\n\treturn nbd_set_size(nbd, config->bytesize, nbd_blksize(config));\n}\n\nstatic int nbd_start_device_ioctl(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tint ret;\n\n\tret = nbd_start_device(nbd);\n\tif (ret)\n\t\treturn ret;\n\n\tif (max_part)\n\t\tset_bit(GD_NEED_PART_SCAN, &nbd->disk->state);\n\tmutex_unlock(&nbd->config_lock);\n\tret = wait_event_interruptible(config->recv_wq,\n\t\t\t\t\t atomic_read(&config->recv_threads) == 0);\n\tif (ret) {\n\t\tsock_shutdown(nbd);\n\t\tnbd_clear_que(nbd);\n\t}\n\n\tflush_workqueue(nbd->recv_workq);\n\tmutex_lock(&nbd->config_lock);\n\tnbd_bdev_reset(nbd);\n\t \n\tif (test_bit(NBD_RT_DISCONNECT_REQUESTED, &config->runtime_flags))\n\t\tret = 0;\n\tif (test_bit(NBD_RT_TIMEDOUT, &config->runtime_flags))\n\t\tret = -ETIMEDOUT;\n\treturn ret;\n}\n\nstatic void nbd_clear_sock_ioctl(struct nbd_device *nbd)\n{\n\tnbd_clear_sock(nbd);\n\tdisk_force_media_change(nbd->disk);\n\tnbd_bdev_reset(nbd);\n\tif (test_and_clear_bit(NBD_RT_HAS_CONFIG_REF,\n\t\t\t       &nbd->config->runtime_flags))\n\t\tnbd_config_put(nbd);\n}\n\nstatic void nbd_set_cmd_timeout(struct nbd_device *nbd, u64 timeout)\n{\n\tnbd->tag_set.timeout = timeout * HZ;\n\tif (timeout)\n\t\tblk_queue_rq_timeout(nbd->disk->queue, timeout * HZ);\n\telse\n\t\tblk_queue_rq_timeout(nbd->disk->queue, 30 * HZ);\n}\n\n \nstatic int __nbd_ioctl(struct block_device *bdev, struct nbd_device *nbd,\n\t\t       unsigned int cmd, unsigned long arg)\n{\n\tstruct nbd_config *config = nbd->config;\n\tloff_t bytesize;\n\n\tswitch (cmd) {\n\tcase NBD_DISCONNECT:\n\t\treturn nbd_disconnect(nbd);\n\tcase NBD_CLEAR_SOCK:\n\t\tnbd_clear_sock_ioctl(nbd);\n\t\treturn 0;\n\tcase NBD_SET_SOCK:\n\t\treturn nbd_add_socket(nbd, arg, false);\n\tcase NBD_SET_BLKSIZE:\n\t\treturn nbd_set_size(nbd, config->bytesize, arg);\n\tcase NBD_SET_SIZE:\n\t\treturn nbd_set_size(nbd, arg, nbd_blksize(config));\n\tcase NBD_SET_SIZE_BLOCKS:\n\t\tif (check_shl_overflow(arg, config->blksize_bits, &bytesize))\n\t\t\treturn -EINVAL;\n\t\treturn nbd_set_size(nbd, bytesize, nbd_blksize(config));\n\tcase NBD_SET_TIMEOUT:\n\t\tnbd_set_cmd_timeout(nbd, arg);\n\t\treturn 0;\n\n\tcase NBD_SET_FLAGS:\n\t\tconfig->flags = arg;\n\t\treturn 0;\n\tcase NBD_DO_IT:\n\t\treturn nbd_start_device_ioctl(nbd);\n\tcase NBD_CLEAR_QUE:\n\t\t \n\t\treturn 0;\n\tcase NBD_PRINT_DEBUG:\n\t\t \n\t\treturn 0;\n\t}\n\treturn -ENOTTY;\n}\n\nstatic int nbd_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct nbd_device *nbd = bdev->bd_disk->private_data;\n\tstruct nbd_config *config = nbd->config;\n\tint error = -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t \n\tif (_IOC_TYPE(cmd) != 0xab)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&nbd->config_lock);\n\n\t \n\tif (!test_bit(NBD_RT_BOUND, &config->runtime_flags) ||\n\t    (cmd == NBD_DISCONNECT || cmd == NBD_CLEAR_SOCK))\n\t\terror = __nbd_ioctl(bdev, nbd, cmd, arg);\n\telse\n\t\tdev_err(nbd_to_dev(nbd), \"Cannot use ioctl interface on a netlink controlled device.\\n\");\n\tmutex_unlock(&nbd->config_lock);\n\treturn error;\n}\n\nstatic int nbd_alloc_and_init_config(struct nbd_device *nbd)\n{\n\tstruct nbd_config *config;\n\n\tif (WARN_ON(nbd->config))\n\t\treturn -EINVAL;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\tconfig = kzalloc(sizeof(struct nbd_config), GFP_NOFS);\n\tif (!config) {\n\t\tmodule_put(THIS_MODULE);\n\t\treturn -ENOMEM;\n\t}\n\n\tatomic_set(&config->recv_threads, 0);\n\tinit_waitqueue_head(&config->recv_wq);\n\tinit_waitqueue_head(&config->conn_wait);\n\tconfig->blksize_bits = NBD_DEF_BLKSIZE_BITS;\n\tatomic_set(&config->live_connections, 0);\n\n\tnbd->config = config;\n\t \n\tsmp_mb__before_atomic();\n\trefcount_set(&nbd->config_refs, 1);\n\n\treturn 0;\n}\n\nstatic int nbd_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct nbd_device *nbd;\n\tstruct nbd_config *config;\n\tint ret = 0;\n\n\tmutex_lock(&nbd_index_mutex);\n\tnbd = disk->private_data;\n\tif (!nbd) {\n\t\tret = -ENXIO;\n\t\tgoto out;\n\t}\n\tif (!refcount_inc_not_zero(&nbd->refs)) {\n\t\tret = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tconfig = nbd_get_config_unlocked(nbd);\n\tif (!config) {\n\t\tmutex_lock(&nbd->config_lock);\n\t\tif (refcount_inc_not_zero(&nbd->config_refs)) {\n\t\t\tmutex_unlock(&nbd->config_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tret = nbd_alloc_and_init_config(nbd);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&nbd->config_lock);\n\t\t\tgoto out;\n\t\t}\n\n\t\trefcount_inc(&nbd->refs);\n\t\tmutex_unlock(&nbd->config_lock);\n\t\tif (max_part)\n\t\t\tset_bit(GD_NEED_PART_SCAN, &disk->state);\n\t} else if (nbd_disconnected(config)) {\n\t\tif (max_part)\n\t\t\tset_bit(GD_NEED_PART_SCAN, &disk->state);\n\t}\nout:\n\tmutex_unlock(&nbd_index_mutex);\n\treturn ret;\n}\n\nstatic void nbd_release(struct gendisk *disk)\n{\n\tstruct nbd_device *nbd = disk->private_data;\n\n\tif (test_bit(NBD_RT_DISCONNECT_ON_CLOSE, &nbd->config->runtime_flags) &&\n\t\t\tdisk_openers(disk) == 0)\n\t\tnbd_disconnect_and_put(nbd);\n\n\tnbd_config_put(nbd);\n\tnbd_put(nbd);\n}\n\nstatic void nbd_free_disk(struct gendisk *disk)\n{\n\tstruct nbd_device *nbd = disk->private_data;\n\n\tkfree(nbd);\n}\n\nstatic const struct block_device_operations nbd_fops =\n{\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\tnbd_open,\n\t.release =\tnbd_release,\n\t.ioctl =\tnbd_ioctl,\n\t.compat_ioctl =\tnbd_ioctl,\n\t.free_disk =\tnbd_free_disk,\n};\n\n#if IS_ENABLED(CONFIG_DEBUG_FS)\n\nstatic int nbd_dbg_tasks_show(struct seq_file *s, void *unused)\n{\n\tstruct nbd_device *nbd = s->private;\n\n\tif (nbd->pid)\n\t\tseq_printf(s, \"recv: %d\\n\", nbd->pid);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(nbd_dbg_tasks);\n\nstatic int nbd_dbg_flags_show(struct seq_file *s, void *unused)\n{\n\tstruct nbd_device *nbd = s->private;\n\tu32 flags = nbd->config->flags;\n\n\tseq_printf(s, \"Hex: 0x%08x\\n\\n\", flags);\n\n\tseq_puts(s, \"Known flags:\\n\");\n\n\tif (flags & NBD_FLAG_HAS_FLAGS)\n\t\tseq_puts(s, \"NBD_FLAG_HAS_FLAGS\\n\");\n\tif (flags & NBD_FLAG_READ_ONLY)\n\t\tseq_puts(s, \"NBD_FLAG_READ_ONLY\\n\");\n\tif (flags & NBD_FLAG_SEND_FLUSH)\n\t\tseq_puts(s, \"NBD_FLAG_SEND_FLUSH\\n\");\n\tif (flags & NBD_FLAG_SEND_FUA)\n\t\tseq_puts(s, \"NBD_FLAG_SEND_FUA\\n\");\n\tif (flags & NBD_FLAG_SEND_TRIM)\n\t\tseq_puts(s, \"NBD_FLAG_SEND_TRIM\\n\");\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(nbd_dbg_flags);\n\nstatic int nbd_dev_dbg_init(struct nbd_device *nbd)\n{\n\tstruct dentry *dir;\n\tstruct nbd_config *config = nbd->config;\n\n\tif (!nbd_dbg_dir)\n\t\treturn -EIO;\n\n\tdir = debugfs_create_dir(nbd_name(nbd), nbd_dbg_dir);\n\tif (IS_ERR(dir)) {\n\t\tdev_err(nbd_to_dev(nbd), \"Failed to create debugfs dir for '%s'\\n\",\n\t\t\tnbd_name(nbd));\n\t\treturn -EIO;\n\t}\n\tconfig->dbg_dir = dir;\n\n\tdebugfs_create_file(\"tasks\", 0444, dir, nbd, &nbd_dbg_tasks_fops);\n\tdebugfs_create_u64(\"size_bytes\", 0444, dir, &config->bytesize);\n\tdebugfs_create_u32(\"timeout\", 0444, dir, &nbd->tag_set.timeout);\n\tdebugfs_create_u32(\"blocksize_bits\", 0444, dir, &config->blksize_bits);\n\tdebugfs_create_file(\"flags\", 0444, dir, nbd, &nbd_dbg_flags_fops);\n\n\treturn 0;\n}\n\nstatic void nbd_dev_dbg_close(struct nbd_device *nbd)\n{\n\tdebugfs_remove_recursive(nbd->config->dbg_dir);\n}\n\nstatic int nbd_dbg_init(void)\n{\n\tstruct dentry *dbg_dir;\n\n\tdbg_dir = debugfs_create_dir(\"nbd\", NULL);\n\tif (IS_ERR(dbg_dir))\n\t\treturn -EIO;\n\n\tnbd_dbg_dir = dbg_dir;\n\n\treturn 0;\n}\n\nstatic void nbd_dbg_close(void)\n{\n\tdebugfs_remove_recursive(nbd_dbg_dir);\n}\n\n#else   \n\nstatic int nbd_dev_dbg_init(struct nbd_device *nbd)\n{\n\treturn 0;\n}\n\nstatic void nbd_dev_dbg_close(struct nbd_device *nbd)\n{\n}\n\nstatic int nbd_dbg_init(void)\n{\n\treturn 0;\n}\n\nstatic void nbd_dbg_close(void)\n{\n}\n\n#endif\n\nstatic int nbd_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t    unsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct nbd_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\tcmd->nbd = set->driver_data;\n\tcmd->flags = 0;\n\tmutex_init(&cmd->lock);\n\treturn 0;\n}\n\nstatic const struct blk_mq_ops nbd_mq_ops = {\n\t.queue_rq\t= nbd_queue_rq,\n\t.complete\t= nbd_complete_rq,\n\t.init_request\t= nbd_init_request,\n\t.timeout\t= nbd_xmit_timeout,\n};\n\nstatic struct nbd_device *nbd_dev_add(int index, unsigned int refs)\n{\n\tstruct nbd_device *nbd;\n\tstruct gendisk *disk;\n\tint err = -ENOMEM;\n\n\tnbd = kzalloc(sizeof(struct nbd_device), GFP_KERNEL);\n\tif (!nbd)\n\t\tgoto out;\n\n\tnbd->tag_set.ops = &nbd_mq_ops;\n\tnbd->tag_set.nr_hw_queues = 1;\n\tnbd->tag_set.queue_depth = 128;\n\tnbd->tag_set.numa_node = NUMA_NO_NODE;\n\tnbd->tag_set.cmd_size = sizeof(struct nbd_cmd);\n\tnbd->tag_set.flags = BLK_MQ_F_SHOULD_MERGE |\n\t\tBLK_MQ_F_BLOCKING;\n\tnbd->tag_set.driver_data = nbd;\n\tINIT_WORK(&nbd->remove_work, nbd_dev_remove_work);\n\tnbd->backend = NULL;\n\n\terr = blk_mq_alloc_tag_set(&nbd->tag_set);\n\tif (err)\n\t\tgoto out_free_nbd;\n\n\tmutex_lock(&nbd_index_mutex);\n\tif (index >= 0) {\n\t\terr = idr_alloc(&nbd_index_idr, nbd, index, index + 1,\n\t\t\t\tGFP_KERNEL);\n\t\tif (err == -ENOSPC)\n\t\t\terr = -EEXIST;\n\t} else {\n\t\terr = idr_alloc(&nbd_index_idr, nbd, 0,\n\t\t\t\t(MINORMASK >> part_shift) + 1, GFP_KERNEL);\n\t\tif (err >= 0)\n\t\t\tindex = err;\n\t}\n\tnbd->index = index;\n\tmutex_unlock(&nbd_index_mutex);\n\tif (err < 0)\n\t\tgoto out_free_tags;\n\n\tdisk = blk_mq_alloc_disk(&nbd->tag_set, NULL);\n\tif (IS_ERR(disk)) {\n\t\terr = PTR_ERR(disk);\n\t\tgoto out_free_idr;\n\t}\n\tnbd->disk = disk;\n\n\tnbd->recv_workq = alloc_workqueue(\"nbd%d-recv\",\n\t\t\t\t\t  WQ_MEM_RECLAIM | WQ_HIGHPRI |\n\t\t\t\t\t  WQ_UNBOUND, 0, nbd->index);\n\tif (!nbd->recv_workq) {\n\t\tdev_err(disk_to_dev(nbd->disk), \"Could not allocate knbd recv work queue.\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_err_disk;\n\t}\n\n\t \n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, disk->queue);\n\tdisk->queue->limits.discard_granularity = 0;\n\tblk_queue_max_discard_sectors(disk->queue, 0);\n\tblk_queue_max_segment_size(disk->queue, UINT_MAX);\n\tblk_queue_max_segments(disk->queue, USHRT_MAX);\n\tblk_queue_max_hw_sectors(disk->queue, 65536);\n\tdisk->queue->limits.max_sectors = 256;\n\n\tmutex_init(&nbd->config_lock);\n\trefcount_set(&nbd->config_refs, 0);\n\t \n\trefcount_set(&nbd->refs, 0);\n\tINIT_LIST_HEAD(&nbd->list);\n\tdisk->major = NBD_MAJOR;\n\tdisk->first_minor = index << part_shift;\n\tdisk->minors = 1 << part_shift;\n\tdisk->fops = &nbd_fops;\n\tdisk->private_data = nbd;\n\tsprintf(disk->disk_name, \"nbd%d\", index);\n\terr = add_disk(disk);\n\tif (err)\n\t\tgoto out_free_work;\n\n\t \n\trefcount_set(&nbd->refs, refs);\n\tnbd_total_devices++;\n\treturn nbd;\n\nout_free_work:\n\tdestroy_workqueue(nbd->recv_workq);\nout_err_disk:\n\tput_disk(disk);\nout_free_idr:\n\tmutex_lock(&nbd_index_mutex);\n\tidr_remove(&nbd_index_idr, index);\n\tmutex_unlock(&nbd_index_mutex);\nout_free_tags:\n\tblk_mq_free_tag_set(&nbd->tag_set);\nout_free_nbd:\n\tkfree(nbd);\nout:\n\treturn ERR_PTR(err);\n}\n\nstatic struct nbd_device *nbd_find_get_unused(void)\n{\n\tstruct nbd_device *nbd;\n\tint id;\n\n\tlockdep_assert_held(&nbd_index_mutex);\n\n\tidr_for_each_entry(&nbd_index_idr, nbd, id) {\n\t\tif (refcount_read(&nbd->config_refs) ||\n\t\t    test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags))\n\t\t\tcontinue;\n\t\tif (refcount_inc_not_zero(&nbd->refs))\n\t\t\treturn nbd;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic const struct nla_policy nbd_attr_policy[NBD_ATTR_MAX + 1] = {\n\t[NBD_ATTR_INDEX]\t\t=\t{ .type = NLA_U32 },\n\t[NBD_ATTR_SIZE_BYTES]\t\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_BLOCK_SIZE_BYTES]\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_TIMEOUT]\t\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_SERVER_FLAGS]\t\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_CLIENT_FLAGS]\t\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_SOCKETS]\t\t=\t{ .type = NLA_NESTED},\n\t[NBD_ATTR_DEAD_CONN_TIMEOUT]\t=\t{ .type = NLA_U64 },\n\t[NBD_ATTR_DEVICE_LIST]\t\t=\t{ .type = NLA_NESTED},\n\t[NBD_ATTR_BACKEND_IDENTIFIER]\t=\t{ .type = NLA_STRING},\n};\n\nstatic const struct nla_policy nbd_sock_policy[NBD_SOCK_MAX + 1] = {\n\t[NBD_SOCK_FD]\t\t\t=\t{ .type = NLA_U32 },\n};\n\n \nstatic const struct nla_policy __attribute__((unused))\nnbd_device_policy[NBD_DEVICE_ATTR_MAX + 1] = {\n\t[NBD_DEVICE_INDEX]\t\t=\t{ .type = NLA_U32 },\n\t[NBD_DEVICE_CONNECTED]\t\t=\t{ .type = NLA_U8 },\n};\n\nstatic int nbd_genl_size_set(struct genl_info *info, struct nbd_device *nbd)\n{\n\tstruct nbd_config *config = nbd->config;\n\tu64 bsize = nbd_blksize(config);\n\tu64 bytes = config->bytesize;\n\n\tif (info->attrs[NBD_ATTR_SIZE_BYTES])\n\t\tbytes = nla_get_u64(info->attrs[NBD_ATTR_SIZE_BYTES]);\n\n\tif (info->attrs[NBD_ATTR_BLOCK_SIZE_BYTES])\n\t\tbsize = nla_get_u64(info->attrs[NBD_ATTR_BLOCK_SIZE_BYTES]);\n\n\tif (bytes != config->bytesize || bsize != nbd_blksize(config))\n\t\treturn nbd_set_size(nbd, bytes, bsize);\n\treturn 0;\n}\n\nstatic int nbd_genl_connect(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nbd_device *nbd;\n\tstruct nbd_config *config;\n\tint index = -1;\n\tint ret;\n\tbool put_dev = false;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (info->attrs[NBD_ATTR_INDEX]) {\n\t\tindex = nla_get_u32(info->attrs[NBD_ATTR_INDEX]);\n\n\t\t \n\t\tif (index < 0 || index > MINORMASK >> part_shift) {\n\t\t\tpr_err(\"illegal input index %d\\n\", index);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (GENL_REQ_ATTR_CHECK(info, NBD_ATTR_SOCKETS)) {\n\t\tpr_err(\"must specify at least one socket\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (GENL_REQ_ATTR_CHECK(info, NBD_ATTR_SIZE_BYTES)) {\n\t\tpr_err(\"must specify a size in bytes for the device\\n\");\n\t\treturn -EINVAL;\n\t}\nagain:\n\tmutex_lock(&nbd_index_mutex);\n\tif (index == -1) {\n\t\tnbd = nbd_find_get_unused();\n\t} else {\n\t\tnbd = idr_find(&nbd_index_idr, index);\n\t\tif (nbd) {\n\t\t\tif ((test_bit(NBD_DESTROY_ON_DISCONNECT, &nbd->flags) &&\n\t\t\t     test_bit(NBD_DISCONNECT_REQUESTED, &nbd->flags)) ||\n\t\t\t    !refcount_inc_not_zero(&nbd->refs)) {\n\t\t\t\tmutex_unlock(&nbd_index_mutex);\n\t\t\t\tpr_err(\"device at index %d is going down\\n\",\n\t\t\t\t\tindex);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&nbd_index_mutex);\n\n\tif (!nbd) {\n\t\tnbd = nbd_dev_add(index, 2);\n\t\tif (IS_ERR(nbd)) {\n\t\t\tpr_err(\"failed to add new device\\n\");\n\t\t\treturn PTR_ERR(nbd);\n\t\t}\n\t}\n\n\tmutex_lock(&nbd->config_lock);\n\tif (refcount_read(&nbd->config_refs)) {\n\t\tmutex_unlock(&nbd->config_lock);\n\t\tnbd_put(nbd);\n\t\tif (index == -1)\n\t\t\tgoto again;\n\t\tpr_err(\"nbd%d already in use\\n\", index);\n\t\treturn -EBUSY;\n\t}\n\n\tret = nbd_alloc_and_init_config(nbd);\n\tif (ret) {\n\t\tmutex_unlock(&nbd->config_lock);\n\t\tnbd_put(nbd);\n\t\tpr_err(\"couldn't allocate config\\n\");\n\t\treturn ret;\n\t}\n\n\tconfig = nbd->config;\n\tset_bit(NBD_RT_BOUND, &config->runtime_flags);\n\tret = nbd_genl_size_set(info, nbd);\n\tif (ret)\n\t\tgoto out;\n\n\tif (info->attrs[NBD_ATTR_TIMEOUT])\n\t\tnbd_set_cmd_timeout(nbd,\n\t\t\t\t    nla_get_u64(info->attrs[NBD_ATTR_TIMEOUT]));\n\tif (info->attrs[NBD_ATTR_DEAD_CONN_TIMEOUT]) {\n\t\tconfig->dead_conn_timeout =\n\t\t\tnla_get_u64(info->attrs[NBD_ATTR_DEAD_CONN_TIMEOUT]);\n\t\tconfig->dead_conn_timeout *= HZ;\n\t}\n\tif (info->attrs[NBD_ATTR_SERVER_FLAGS])\n\t\tconfig->flags =\n\t\t\tnla_get_u64(info->attrs[NBD_ATTR_SERVER_FLAGS]);\n\tif (info->attrs[NBD_ATTR_CLIENT_FLAGS]) {\n\t\tu64 flags = nla_get_u64(info->attrs[NBD_ATTR_CLIENT_FLAGS]);\n\t\tif (flags & NBD_CFLAG_DESTROY_ON_DISCONNECT) {\n\t\t\t \n\t\t\tif (!test_and_set_bit(NBD_DESTROY_ON_DISCONNECT,\n\t\t\t\t\t      &nbd->flags))\n\t\t\t\tput_dev = true;\n\t\t} else {\n\t\t\tif (test_and_clear_bit(NBD_DESTROY_ON_DISCONNECT,\n\t\t\t\t\t       &nbd->flags))\n\t\t\t\trefcount_inc(&nbd->refs);\n\t\t}\n\t\tif (flags & NBD_CFLAG_DISCONNECT_ON_CLOSE) {\n\t\t\tset_bit(NBD_RT_DISCONNECT_ON_CLOSE,\n\t\t\t\t&config->runtime_flags);\n\t\t}\n\t}\n\n\tif (info->attrs[NBD_ATTR_SOCKETS]) {\n\t\tstruct nlattr *attr;\n\t\tint rem, fd;\n\n\t\tnla_for_each_nested(attr, info->attrs[NBD_ATTR_SOCKETS],\n\t\t\t\t    rem) {\n\t\t\tstruct nlattr *socks[NBD_SOCK_MAX+1];\n\n\t\t\tif (nla_type(attr) != NBD_SOCK_ITEM) {\n\t\t\t\tpr_err(\"socks must be embedded in a SOCK_ITEM attr\\n\");\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = nla_parse_nested_deprecated(socks, NBD_SOCK_MAX,\n\t\t\t\t\t\t\t  attr,\n\t\t\t\t\t\t\t  nbd_sock_policy,\n\t\t\t\t\t\t\t  info->extack);\n\t\t\tif (ret != 0) {\n\t\t\t\tpr_err(\"error processing sock list\\n\");\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!socks[NBD_SOCK_FD])\n\t\t\t\tcontinue;\n\t\t\tfd = (int)nla_get_u32(socks[NBD_SOCK_FD]);\n\t\t\tret = nbd_add_socket(nbd, fd, true);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\tret = nbd_start_device(nbd);\n\tif (ret)\n\t\tgoto out;\n\tif (info->attrs[NBD_ATTR_BACKEND_IDENTIFIER]) {\n\t\tnbd->backend = nla_strdup(info->attrs[NBD_ATTR_BACKEND_IDENTIFIER],\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!nbd->backend) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = device_create_file(disk_to_dev(nbd->disk), &backend_attr);\n\tif (ret) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"device_create_file failed for backend!\\n\");\n\t\tgoto out;\n\t}\n\tset_bit(NBD_RT_HAS_BACKEND_FILE, &config->runtime_flags);\nout:\n\tmutex_unlock(&nbd->config_lock);\n\tif (!ret) {\n\t\tset_bit(NBD_RT_HAS_CONFIG_REF, &config->runtime_flags);\n\t\trefcount_inc(&nbd->config_refs);\n\t\tnbd_connect_reply(info, nbd->index);\n\t}\n\tnbd_config_put(nbd);\n\tif (put_dev)\n\t\tnbd_put(nbd);\n\treturn ret;\n}\n\nstatic void nbd_disconnect_and_put(struct nbd_device *nbd)\n{\n\tmutex_lock(&nbd->config_lock);\n\tnbd_disconnect(nbd);\n\tsock_shutdown(nbd);\n\twake_up(&nbd->config->conn_wait);\n\t \n\tflush_workqueue(nbd->recv_workq);\n\tnbd_clear_que(nbd);\n\tnbd->task_setup = NULL;\n\tmutex_unlock(&nbd->config_lock);\n\n\tif (test_and_clear_bit(NBD_RT_HAS_CONFIG_REF,\n\t\t\t       &nbd->config->runtime_flags))\n\t\tnbd_config_put(nbd);\n}\n\nstatic int nbd_genl_disconnect(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nbd_device *nbd;\n\tint index;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (GENL_REQ_ATTR_CHECK(info, NBD_ATTR_INDEX)) {\n\t\tpr_err(\"must specify an index to disconnect\\n\");\n\t\treturn -EINVAL;\n\t}\n\tindex = nla_get_u32(info->attrs[NBD_ATTR_INDEX]);\n\tmutex_lock(&nbd_index_mutex);\n\tnbd = idr_find(&nbd_index_idr, index);\n\tif (!nbd) {\n\t\tmutex_unlock(&nbd_index_mutex);\n\t\tpr_err(\"couldn't find device at index %d\\n\", index);\n\t\treturn -EINVAL;\n\t}\n\tif (!refcount_inc_not_zero(&nbd->refs)) {\n\t\tmutex_unlock(&nbd_index_mutex);\n\t\tpr_err(\"device at index %d is going down\\n\", index);\n\t\treturn -EINVAL;\n\t}\n\tmutex_unlock(&nbd_index_mutex);\n\tif (!refcount_inc_not_zero(&nbd->config_refs))\n\t\tgoto put_nbd;\n\tnbd_disconnect_and_put(nbd);\n\tnbd_config_put(nbd);\nput_nbd:\n\tnbd_put(nbd);\n\treturn 0;\n}\n\nstatic int nbd_genl_reconfigure(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nbd_device *nbd = NULL;\n\tstruct nbd_config *config;\n\tint index;\n\tint ret = 0;\n\tbool put_dev = false;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (GENL_REQ_ATTR_CHECK(info, NBD_ATTR_INDEX)) {\n\t\tpr_err(\"must specify a device to reconfigure\\n\");\n\t\treturn -EINVAL;\n\t}\n\tindex = nla_get_u32(info->attrs[NBD_ATTR_INDEX]);\n\tmutex_lock(&nbd_index_mutex);\n\tnbd = idr_find(&nbd_index_idr, index);\n\tif (!nbd) {\n\t\tmutex_unlock(&nbd_index_mutex);\n\t\tpr_err(\"couldn't find a device at index %d\\n\", index);\n\t\treturn -EINVAL;\n\t}\n\tif (nbd->backend) {\n\t\tif (info->attrs[NBD_ATTR_BACKEND_IDENTIFIER]) {\n\t\t\tif (nla_strcmp(info->attrs[NBD_ATTR_BACKEND_IDENTIFIER],\n\t\t\t\t       nbd->backend)) {\n\t\t\t\tmutex_unlock(&nbd_index_mutex);\n\t\t\t\tdev_err(nbd_to_dev(nbd),\n\t\t\t\t\t\"backend image doesn't match with %s\\n\",\n\t\t\t\t\tnbd->backend);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tmutex_unlock(&nbd_index_mutex);\n\t\t\tdev_err(nbd_to_dev(nbd), \"must specify backend\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (!refcount_inc_not_zero(&nbd->refs)) {\n\t\tmutex_unlock(&nbd_index_mutex);\n\t\tpr_err(\"device at index %d is going down\\n\", index);\n\t\treturn -EINVAL;\n\t}\n\tmutex_unlock(&nbd_index_mutex);\n\n\tconfig = nbd_get_config_unlocked(nbd);\n\tif (!config) {\n\t\tdev_err(nbd_to_dev(nbd),\n\t\t\t\"not configured, cannot reconfigure\\n\");\n\t\tnbd_put(nbd);\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&nbd->config_lock);\n\tif (!test_bit(NBD_RT_BOUND, &config->runtime_flags) ||\n\t    !nbd->pid) {\n\t\tdev_err(nbd_to_dev(nbd),\n\t\t\t\"not configured, cannot reconfigure\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = nbd_genl_size_set(info, nbd);\n\tif (ret)\n\t\tgoto out;\n\n\tif (info->attrs[NBD_ATTR_TIMEOUT])\n\t\tnbd_set_cmd_timeout(nbd,\n\t\t\t\t    nla_get_u64(info->attrs[NBD_ATTR_TIMEOUT]));\n\tif (info->attrs[NBD_ATTR_DEAD_CONN_TIMEOUT]) {\n\t\tconfig->dead_conn_timeout =\n\t\t\tnla_get_u64(info->attrs[NBD_ATTR_DEAD_CONN_TIMEOUT]);\n\t\tconfig->dead_conn_timeout *= HZ;\n\t}\n\tif (info->attrs[NBD_ATTR_CLIENT_FLAGS]) {\n\t\tu64 flags = nla_get_u64(info->attrs[NBD_ATTR_CLIENT_FLAGS]);\n\t\tif (flags & NBD_CFLAG_DESTROY_ON_DISCONNECT) {\n\t\t\tif (!test_and_set_bit(NBD_DESTROY_ON_DISCONNECT,\n\t\t\t\t\t      &nbd->flags))\n\t\t\t\tput_dev = true;\n\t\t} else {\n\t\t\tif (test_and_clear_bit(NBD_DESTROY_ON_DISCONNECT,\n\t\t\t\t\t       &nbd->flags))\n\t\t\t\trefcount_inc(&nbd->refs);\n\t\t}\n\n\t\tif (flags & NBD_CFLAG_DISCONNECT_ON_CLOSE) {\n\t\t\tset_bit(NBD_RT_DISCONNECT_ON_CLOSE,\n\t\t\t\t\t&config->runtime_flags);\n\t\t} else {\n\t\t\tclear_bit(NBD_RT_DISCONNECT_ON_CLOSE,\n\t\t\t\t\t&config->runtime_flags);\n\t\t}\n\t}\n\n\tif (info->attrs[NBD_ATTR_SOCKETS]) {\n\t\tstruct nlattr *attr;\n\t\tint rem, fd;\n\n\t\tnla_for_each_nested(attr, info->attrs[NBD_ATTR_SOCKETS],\n\t\t\t\t    rem) {\n\t\t\tstruct nlattr *socks[NBD_SOCK_MAX+1];\n\n\t\t\tif (nla_type(attr) != NBD_SOCK_ITEM) {\n\t\t\t\tpr_err(\"socks must be embedded in a SOCK_ITEM attr\\n\");\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = nla_parse_nested_deprecated(socks, NBD_SOCK_MAX,\n\t\t\t\t\t\t\t  attr,\n\t\t\t\t\t\t\t  nbd_sock_policy,\n\t\t\t\t\t\t\t  info->extack);\n\t\t\tif (ret != 0) {\n\t\t\t\tpr_err(\"error processing sock list\\n\");\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!socks[NBD_SOCK_FD])\n\t\t\t\tcontinue;\n\t\t\tfd = (int)nla_get_u32(socks[NBD_SOCK_FD]);\n\t\t\tret = nbd_reconnect_socket(nbd, fd);\n\t\t\tif (ret) {\n\t\t\t\tif (ret == -ENOSPC)\n\t\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdev_info(nbd_to_dev(nbd), \"reconnected socket\\n\");\n\t\t}\n\t}\nout:\n\tmutex_unlock(&nbd->config_lock);\n\tnbd_config_put(nbd);\n\tnbd_put(nbd);\n\tif (put_dev)\n\t\tnbd_put(nbd);\n\treturn ret;\n}\n\nstatic const struct genl_small_ops nbd_connect_genl_ops[] = {\n\t{\n\t\t.cmd\t= NBD_CMD_CONNECT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit\t= nbd_genl_connect,\n\t},\n\t{\n\t\t.cmd\t= NBD_CMD_DISCONNECT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit\t= nbd_genl_disconnect,\n\t},\n\t{\n\t\t.cmd\t= NBD_CMD_RECONFIGURE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit\t= nbd_genl_reconfigure,\n\t},\n\t{\n\t\t.cmd\t= NBD_CMD_STATUS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit\t= nbd_genl_status,\n\t},\n};\n\nstatic const struct genl_multicast_group nbd_mcast_grps[] = {\n\t{ .name = NBD_GENL_MCAST_GROUP_NAME, },\n};\n\nstatic struct genl_family nbd_genl_family __ro_after_init = {\n\t.hdrsize\t= 0,\n\t.name\t\t= NBD_GENL_FAMILY_NAME,\n\t.version\t= NBD_GENL_VERSION,\n\t.module\t\t= THIS_MODULE,\n\t.small_ops\t= nbd_connect_genl_ops,\n\t.n_small_ops\t= ARRAY_SIZE(nbd_connect_genl_ops),\n\t.resv_start_op\t= NBD_CMD_STATUS + 1,\n\t.maxattr\t= NBD_ATTR_MAX,\n\t.netnsok\t= 1,\n\t.policy = nbd_attr_policy,\n\t.mcgrps\t\t= nbd_mcast_grps,\n\t.n_mcgrps\t= ARRAY_SIZE(nbd_mcast_grps),\n};\nMODULE_ALIAS_GENL_FAMILY(NBD_GENL_FAMILY_NAME);\n\nstatic int populate_nbd_status(struct nbd_device *nbd, struct sk_buff *reply)\n{\n\tstruct nlattr *dev_opt;\n\tu8 connected = 0;\n\tint ret;\n\n\t \n\tif (refcount_read(&nbd->config_refs))\n\t\tconnected = 1;\n\tdev_opt = nla_nest_start_noflag(reply, NBD_DEVICE_ITEM);\n\tif (!dev_opt)\n\t\treturn -EMSGSIZE;\n\tret = nla_put_u32(reply, NBD_DEVICE_INDEX, nbd->index);\n\tif (ret)\n\t\treturn -EMSGSIZE;\n\tret = nla_put_u8(reply, NBD_DEVICE_CONNECTED,\n\t\t\t connected);\n\tif (ret)\n\t\treturn -EMSGSIZE;\n\tnla_nest_end(reply, dev_opt);\n\treturn 0;\n}\n\nstatic int status_cb(int id, void *ptr, void *data)\n{\n\tstruct nbd_device *nbd = ptr;\n\treturn populate_nbd_status(nbd, (struct sk_buff *)data);\n}\n\nstatic int nbd_genl_status(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nlattr *dev_list;\n\tstruct sk_buff *reply;\n\tvoid *reply_head;\n\tsize_t msg_size;\n\tint index = -1;\n\tint ret = -ENOMEM;\n\n\tif (info->attrs[NBD_ATTR_INDEX])\n\t\tindex = nla_get_u32(info->attrs[NBD_ATTR_INDEX]);\n\n\tmutex_lock(&nbd_index_mutex);\n\n\tmsg_size = nla_total_size(nla_attr_size(sizeof(u32)) +\n\t\t\t\t  nla_attr_size(sizeof(u8)));\n\tmsg_size *= (index == -1) ? nbd_total_devices : 1;\n\n\treply = genlmsg_new(msg_size, GFP_KERNEL);\n\tif (!reply)\n\t\tgoto out;\n\treply_head = genlmsg_put_reply(reply, info, &nbd_genl_family, 0,\n\t\t\t\t       NBD_CMD_STATUS);\n\tif (!reply_head) {\n\t\tnlmsg_free(reply);\n\t\tgoto out;\n\t}\n\n\tdev_list = nla_nest_start_noflag(reply, NBD_ATTR_DEVICE_LIST);\n\tif (index == -1) {\n\t\tret = idr_for_each(&nbd_index_idr, &status_cb, reply);\n\t\tif (ret) {\n\t\t\tnlmsg_free(reply);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tstruct nbd_device *nbd;\n\t\tnbd = idr_find(&nbd_index_idr, index);\n\t\tif (nbd) {\n\t\t\tret = populate_nbd_status(nbd, reply);\n\t\t\tif (ret) {\n\t\t\t\tnlmsg_free(reply);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tnla_nest_end(reply, dev_list);\n\tgenlmsg_end(reply, reply_head);\n\tret = genlmsg_reply(reply, info);\nout:\n\tmutex_unlock(&nbd_index_mutex);\n\treturn ret;\n}\n\nstatic void nbd_connect_reply(struct genl_info *info, int index)\n{\n\tstruct sk_buff *skb;\n\tvoid *msg_head;\n\tint ret;\n\n\tskb = genlmsg_new(nla_total_size(sizeof(u32)), GFP_KERNEL);\n\tif (!skb)\n\t\treturn;\n\tmsg_head = genlmsg_put_reply(skb, info, &nbd_genl_family, 0,\n\t\t\t\t     NBD_CMD_CONNECT);\n\tif (!msg_head) {\n\t\tnlmsg_free(skb);\n\t\treturn;\n\t}\n\tret = nla_put_u32(skb, NBD_ATTR_INDEX, index);\n\tif (ret) {\n\t\tnlmsg_free(skb);\n\t\treturn;\n\t}\n\tgenlmsg_end(skb, msg_head);\n\tgenlmsg_reply(skb, info);\n}\n\nstatic void nbd_mcast_index(int index)\n{\n\tstruct sk_buff *skb;\n\tvoid *msg_head;\n\tint ret;\n\n\tskb = genlmsg_new(nla_total_size(sizeof(u32)), GFP_KERNEL);\n\tif (!skb)\n\t\treturn;\n\tmsg_head = genlmsg_put(skb, 0, 0, &nbd_genl_family, 0,\n\t\t\t\t     NBD_CMD_LINK_DEAD);\n\tif (!msg_head) {\n\t\tnlmsg_free(skb);\n\t\treturn;\n\t}\n\tret = nla_put_u32(skb, NBD_ATTR_INDEX, index);\n\tif (ret) {\n\t\tnlmsg_free(skb);\n\t\treturn;\n\t}\n\tgenlmsg_end(skb, msg_head);\n\tgenlmsg_multicast(&nbd_genl_family, skb, 0, 0, GFP_KERNEL);\n}\n\nstatic void nbd_dead_link_work(struct work_struct *work)\n{\n\tstruct link_dead_args *args = container_of(work, struct link_dead_args,\n\t\t\t\t\t\t   work);\n\tnbd_mcast_index(args->index);\n\tkfree(args);\n}\n\nstatic int __init nbd_init(void)\n{\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(struct nbd_request) != 28);\n\n\tif (max_part < 0) {\n\t\tpr_err(\"max_part must be >= 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpart_shift = 0;\n\tif (max_part > 0) {\n\t\tpart_shift = fls(max_part);\n\n\t\t \n\t\tmax_part = (1UL << part_shift) - 1;\n\t}\n\n\tif ((1UL << part_shift) > DISK_MAX_PARTS)\n\t\treturn -EINVAL;\n\n\tif (nbds_max > 1UL << (MINORBITS - part_shift))\n\t\treturn -EINVAL;\n\n\tif (register_blkdev(NBD_MAJOR, \"nbd\"))\n\t\treturn -EIO;\n\n\tnbd_del_wq = alloc_workqueue(\"nbd-del\", WQ_UNBOUND, 0);\n\tif (!nbd_del_wq) {\n\t\tunregister_blkdev(NBD_MAJOR, \"nbd\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (genl_register_family(&nbd_genl_family)) {\n\t\tdestroy_workqueue(nbd_del_wq);\n\t\tunregister_blkdev(NBD_MAJOR, \"nbd\");\n\t\treturn -EINVAL;\n\t}\n\tnbd_dbg_init();\n\n\tfor (i = 0; i < nbds_max; i++)\n\t\tnbd_dev_add(i, 1);\n\treturn 0;\n}\n\nstatic int nbd_exit_cb(int id, void *ptr, void *data)\n{\n\tstruct list_head *list = (struct list_head *)data;\n\tstruct nbd_device *nbd = ptr;\n\n\t \n\tif (refcount_read(&nbd->refs))\n\t\tlist_add_tail(&nbd->list, list);\n\n\treturn 0;\n}\n\nstatic void __exit nbd_cleanup(void)\n{\n\tstruct nbd_device *nbd;\n\tLIST_HEAD(del_list);\n\n\t \n\tgenl_unregister_family(&nbd_genl_family);\n\n\tnbd_dbg_close();\n\n\tmutex_lock(&nbd_index_mutex);\n\tidr_for_each(&nbd_index_idr, &nbd_exit_cb, &del_list);\n\tmutex_unlock(&nbd_index_mutex);\n\n\twhile (!list_empty(&del_list)) {\n\t\tnbd = list_first_entry(&del_list, struct nbd_device, list);\n\t\tlist_del_init(&nbd->list);\n\t\tif (refcount_read(&nbd->config_refs))\n\t\t\tpr_err(\"possibly leaking nbd_config (ref %d)\\n\",\n\t\t\t\t\trefcount_read(&nbd->config_refs));\n\t\tif (refcount_read(&nbd->refs) != 1)\n\t\t\tpr_err(\"possibly leaking a device\\n\");\n\t\tnbd_put(nbd);\n\t}\n\n\t \n\tdestroy_workqueue(nbd_del_wq);\n\n\tidr_destroy(&nbd_index_idr);\n\tunregister_blkdev(NBD_MAJOR, \"nbd\");\n}\n\nmodule_init(nbd_init);\nmodule_exit(nbd_cleanup);\n\nMODULE_DESCRIPTION(\"Network Block Device\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_param(nbds_max, int, 0444);\nMODULE_PARM_DESC(nbds_max, \"number of network block devices to initialize (default: 16)\");\nmodule_param(max_part, int, 0444);\nMODULE_PARM_DESC(max_part, \"number of partitions per device (default: 16)\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}