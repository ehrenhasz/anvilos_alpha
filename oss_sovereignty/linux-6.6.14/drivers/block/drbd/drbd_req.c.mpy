{
  "module_name": "drbd_req.c",
  "hash_id": "0c19a7d8a872c06de8ea51e763743badcaa9ffa75c16a269381d7e29bf64c19c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/drbd/drbd_req.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n\n#include <linux/slab.h>\n#include <linux/drbd.h>\n#include \"drbd_int.h\"\n#include \"drbd_req.h\"\n\n\nstatic bool drbd_may_do_local_read(struct drbd_device *device, sector_t sector, int size);\n\nstatic struct drbd_request *drbd_req_new(struct drbd_device *device, struct bio *bio_src)\n{\n\tstruct drbd_request *req;\n\n\treq = mempool_alloc(&drbd_request_mempool, GFP_NOIO);\n\tif (!req)\n\t\treturn NULL;\n\tmemset(req, 0, sizeof(*req));\n\n\treq->rq_state = (bio_data_dir(bio_src) == WRITE ? RQ_WRITE : 0)\n\t\t      | (bio_op(bio_src) == REQ_OP_WRITE_ZEROES ? RQ_ZEROES : 0)\n\t\t      | (bio_op(bio_src) == REQ_OP_DISCARD ? RQ_UNMAP : 0);\n\treq->device = device;\n\treq->master_bio = bio_src;\n\treq->epoch = 0;\n\n\tdrbd_clear_interval(&req->i);\n\treq->i.sector     = bio_src->bi_iter.bi_sector;\n\treq->i.size      = bio_src->bi_iter.bi_size;\n\treq->i.local = true;\n\treq->i.waiting = false;\n\n\tINIT_LIST_HEAD(&req->tl_requests);\n\tINIT_LIST_HEAD(&req->w.list);\n\tINIT_LIST_HEAD(&req->req_pending_master_completion);\n\tINIT_LIST_HEAD(&req->req_pending_local);\n\n\t \n\tatomic_set(&req->completion_ref, 1);\n\t \n\tkref_init(&req->kref);\n\treturn req;\n}\n\nstatic void drbd_remove_request_interval(struct rb_root *root,\n\t\t\t\t\t struct drbd_request *req)\n{\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_interval *i = &req->i;\n\n\tdrbd_remove_interval(root, i);\n\n\t \n\tif (i->waiting)\n\t\twake_up(&device->misc_wait);\n}\n\nvoid drbd_req_destroy(struct kref *kref)\n{\n\tstruct drbd_request *req = container_of(kref, struct drbd_request, kref);\n\tstruct drbd_device *device = req->device;\n\tconst unsigned s = req->rq_state;\n\n\tif ((req->master_bio && !(s & RQ_POSTPONED)) ||\n\t\tatomic_read(&req->completion_ref) ||\n\t\t(s & RQ_LOCAL_PENDING) ||\n\t\t((s & RQ_NET_MASK) && !(s & RQ_NET_DONE))) {\n\t\tdrbd_err(device, \"drbd_req_destroy: Logic BUG rq_state = 0x%x, completion_ref = %d\\n\",\n\t\t\t\ts, atomic_read(&req->completion_ref));\n\t\treturn;\n\t}\n\n\t \n\tlist_del_init(&req->tl_requests);\n\n\t \n\tif (!drbd_interval_empty(&req->i)) {\n\t\tstruct rb_root *root;\n\n\t\tif (s & RQ_WRITE)\n\t\t\troot = &device->write_requests;\n\t\telse\n\t\t\troot = &device->read_requests;\n\t\tdrbd_remove_request_interval(root, req);\n\t} else if (s & (RQ_NET_MASK & ~RQ_NET_DONE) && req->i.size != 0)\n\t\tdrbd_err(device, \"drbd_req_destroy: Logic BUG: interval empty, but: rq_state=0x%x, sect=%llu, size=%u\\n\",\n\t\t\ts, (unsigned long long)req->i.sector, req->i.size);\n\n\t \n\tif (s & RQ_WRITE) {\n\t\t \n\n\t\t \n\t\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\t\tif ((s & (RQ_POSTPONED|RQ_LOCAL_MASK|RQ_NET_MASK)) != RQ_POSTPONED) {\n\t\t\tif (!(s & RQ_NET_OK) || !(s & RQ_LOCAL_OK))\n\t\t\t\tdrbd_set_out_of_sync(peer_device, req->i.sector, req->i.size);\n\n\t\t\tif ((s & RQ_NET_OK) && (s & RQ_LOCAL_OK) && (s & RQ_NET_SIS))\n\t\t\t\tdrbd_set_in_sync(peer_device, req->i.sector, req->i.size);\n\t\t}\n\n\t\t \n\t\tif (s & RQ_IN_ACT_LOG) {\n\t\t\tif (get_ldev_if_state(device, D_FAILED)) {\n\t\t\t\tdrbd_al_complete_io(device, &req->i);\n\t\t\t\tput_ldev(device);\n\t\t\t} else if (drbd_ratelimit()) {\n\t\t\t\tdrbd_warn(device, \"Should have called drbd_al_complete_io(, %llu, %u), \"\n\t\t\t\t\t \"but my Disk seems to have failed :(\\n\",\n\t\t\t\t\t (unsigned long long) req->i.sector, req->i.size);\n\t\t\t}\n\t\t}\n\t}\n\n\tmempool_free(req, &drbd_request_mempool);\n}\n\nstatic void wake_all_senders(struct drbd_connection *connection)\n{\n\twake_up(&connection->sender_work.q_wait);\n}\n\n \nvoid start_new_tl_epoch(struct drbd_connection *connection)\n{\n\t \n\tif (connection->current_tle_writes == 0)\n\t\treturn;\n\n\tconnection->current_tle_writes = 0;\n\tatomic_inc(&connection->current_tle_nr);\n\twake_all_senders(connection);\n}\n\nvoid complete_master_bio(struct drbd_device *device,\n\t\tstruct bio_and_error *m)\n{\n\tif (unlikely(m->error))\n\t\tm->bio->bi_status = errno_to_blk_status(m->error);\n\tbio_endio(m->bio);\n\tdec_ap_bio(device);\n}\n\n\n \nstatic\nvoid drbd_req_complete(struct drbd_request *req, struct bio_and_error *m)\n{\n\tconst unsigned s = req->rq_state;\n\tstruct drbd_device *device = req->device;\n\tint error, ok;\n\n\t \n\tif ((s & RQ_LOCAL_PENDING && !(s & RQ_LOCAL_ABORTED)) ||\n\t    (s & RQ_NET_QUEUED) || (s & RQ_NET_PENDING) ||\n\t    (s & RQ_COMPLETION_SUSP)) {\n\t\tdrbd_err(device, \"drbd_req_complete: Logic BUG rq_state = 0x%x\\n\", s);\n\t\treturn;\n\t}\n\n\tif (!req->master_bio) {\n\t\tdrbd_err(device, \"drbd_req_complete: Logic BUG, master_bio == NULL!\\n\");\n\t\treturn;\n\t}\n\n\t \n\tok = (s & RQ_LOCAL_OK) || (s & RQ_NET_OK);\n\terror = PTR_ERR(req->private_bio);\n\n\t \n\tif (op_is_write(bio_op(req->master_bio)) &&\n\t    req->epoch == atomic_read(&first_peer_device(device)->connection->current_tle_nr))\n\t\tstart_new_tl_epoch(first_peer_device(device)->connection);\n\n\t \n\tbio_end_io_acct(req->master_bio, req->start_jif);\n\n\t \n\tif (!ok &&\n\t    bio_op(req->master_bio) == REQ_OP_READ &&\n\t    !(req->master_bio->bi_opf & REQ_RAHEAD) &&\n\t    !list_empty(&req->tl_requests))\n\t\treq->rq_state |= RQ_POSTPONED;\n\n\tif (!(req->rq_state & RQ_POSTPONED)) {\n\t\tm->error = ok ? 0 : (error ?: -EIO);\n\t\tm->bio = req->master_bio;\n\t\treq->master_bio = NULL;\n\t\t \n\t\treq->i.completed = true;\n\t}\n\n\tif (req->i.waiting)\n\t\twake_up(&device->misc_wait);\n\n\t \n\tlist_del_init(&req->req_pending_master_completion);\n}\n\n \nstatic void drbd_req_put_completion_ref(struct drbd_request *req, struct bio_and_error *m, int put)\n{\n\tstruct drbd_device *device = req->device;\n\tD_ASSERT(device, m || (req->rq_state & RQ_POSTPONED));\n\n\tif (!put)\n\t\treturn;\n\n\tif (!atomic_sub_and_test(put, &req->completion_ref))\n\t\treturn;\n\n\tdrbd_req_complete(req, m);\n\n\t \n\tif (req->rq_state & RQ_LOCAL_ABORTED)\n\t\treturn;\n\n\tif (req->rq_state & RQ_POSTPONED) {\n\t\t \n\t\tdrbd_restart_request(req);\n\t\treturn;\n\t}\n\n\tkref_put(&req->kref, drbd_req_destroy);\n}\n\nstatic void set_if_null_req_next(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_next == NULL)\n\t\tconnection->req_next = req;\n}\n\nstatic void advance_conn_req_next(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tstruct drbd_request *iter = req;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_next != req)\n\t\treturn;\n\n\treq = NULL;\n\tlist_for_each_entry_continue(iter, &connection->transfer_log, tl_requests) {\n\t\tconst unsigned int s = iter->rq_state;\n\n\t\tif (s & RQ_NET_QUEUED) {\n\t\t\treq = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tconnection->req_next = req;\n}\n\nstatic void set_if_null_req_ack_pending(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_ack_pending == NULL)\n\t\tconnection->req_ack_pending = req;\n}\n\nstatic void advance_conn_req_ack_pending(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tstruct drbd_request *iter = req;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_ack_pending != req)\n\t\treturn;\n\n\treq = NULL;\n\tlist_for_each_entry_continue(iter, &connection->transfer_log, tl_requests) {\n\t\tconst unsigned int s = iter->rq_state;\n\n\t\tif ((s & RQ_NET_SENT) && (s & RQ_NET_PENDING)) {\n\t\t\treq = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tconnection->req_ack_pending = req;\n}\n\nstatic void set_if_null_req_not_net_done(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_not_net_done == NULL)\n\t\tconnection->req_not_net_done = req;\n}\n\nstatic void advance_conn_req_not_net_done(struct drbd_peer_device *peer_device, struct drbd_request *req)\n{\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tstruct drbd_request *iter = req;\n\tif (!connection)\n\t\treturn;\n\tif (connection->req_not_net_done != req)\n\t\treturn;\n\n\treq = NULL;\n\tlist_for_each_entry_continue(iter, &connection->transfer_log, tl_requests) {\n\t\tconst unsigned int s = iter->rq_state;\n\n\t\tif ((s & RQ_NET_SENT) && !(s & RQ_NET_DONE)) {\n\t\t\treq = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tconnection->req_not_net_done = req;\n}\n\n \nstatic void mod_rq_state(struct drbd_request *req, struct bio_and_error *m,\n\t\tint clear, int set)\n{\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\tunsigned s = req->rq_state;\n\tint c_put = 0;\n\n\tif (drbd_suspended(device) && !((s | clear) & RQ_COMPLETION_SUSP))\n\t\tset |= RQ_COMPLETION_SUSP;\n\n\t \n\n\treq->rq_state &= ~clear;\n\treq->rq_state |= set;\n\n\t \n\tif (req->rq_state == s)\n\t\treturn;\n\n\t \n\n\tkref_get(&req->kref);\n\n\tif (!(s & RQ_LOCAL_PENDING) && (set & RQ_LOCAL_PENDING))\n\t\tatomic_inc(&req->completion_ref);\n\n\tif (!(s & RQ_NET_PENDING) && (set & RQ_NET_PENDING)) {\n\t\tinc_ap_pending(device);\n\t\tatomic_inc(&req->completion_ref);\n\t}\n\n\tif (!(s & RQ_NET_QUEUED) && (set & RQ_NET_QUEUED)) {\n\t\tatomic_inc(&req->completion_ref);\n\t\tset_if_null_req_next(peer_device, req);\n\t}\n\n\tif (!(s & RQ_EXP_BARR_ACK) && (set & RQ_EXP_BARR_ACK))\n\t\tkref_get(&req->kref);  \n\n\tif (!(s & RQ_NET_SENT) && (set & RQ_NET_SENT)) {\n\t\t \n\t\tif (!(s & RQ_NET_DONE)) {\n\t\t\tatomic_add(req->i.size >> 9, &device->ap_in_flight);\n\t\t\tset_if_null_req_not_net_done(peer_device, req);\n\t\t}\n\t\tif (req->rq_state & RQ_NET_PENDING)\n\t\t\tset_if_null_req_ack_pending(peer_device, req);\n\t}\n\n\tif (!(s & RQ_COMPLETION_SUSP) && (set & RQ_COMPLETION_SUSP))\n\t\tatomic_inc(&req->completion_ref);\n\n\t \n\n\tif ((s & RQ_COMPLETION_SUSP) && (clear & RQ_COMPLETION_SUSP))\n\t\t++c_put;\n\n\tif (!(s & RQ_LOCAL_ABORTED) && (set & RQ_LOCAL_ABORTED)) {\n\t\tD_ASSERT(device, req->rq_state & RQ_LOCAL_PENDING);\n\t\t++c_put;\n\t}\n\n\tif ((s & RQ_LOCAL_PENDING) && (clear & RQ_LOCAL_PENDING)) {\n\t\tif (req->rq_state & RQ_LOCAL_ABORTED)\n\t\t\tkref_put(&req->kref, drbd_req_destroy);\n\t\telse\n\t\t\t++c_put;\n\t\tlist_del_init(&req->req_pending_local);\n\t}\n\n\tif ((s & RQ_NET_PENDING) && (clear & RQ_NET_PENDING)) {\n\t\tdec_ap_pending(device);\n\t\t++c_put;\n\t\treq->acked_jif = jiffies;\n\t\tadvance_conn_req_ack_pending(peer_device, req);\n\t}\n\n\tif ((s & RQ_NET_QUEUED) && (clear & RQ_NET_QUEUED)) {\n\t\t++c_put;\n\t\tadvance_conn_req_next(peer_device, req);\n\t}\n\n\tif (!(s & RQ_NET_DONE) && (set & RQ_NET_DONE)) {\n\t\tif (s & RQ_NET_SENT)\n\t\t\tatomic_sub(req->i.size >> 9, &device->ap_in_flight);\n\t\tif (s & RQ_EXP_BARR_ACK)\n\t\t\tkref_put(&req->kref, drbd_req_destroy);\n\t\treq->net_done_jif = jiffies;\n\n\t\t \n\t\tadvance_conn_req_next(peer_device, req);\n\t\tadvance_conn_req_ack_pending(peer_device, req);\n\t\tadvance_conn_req_not_net_done(peer_device, req);\n\t}\n\n\t \n\n\t \n\tif (req->i.waiting)\n\t\twake_up(&device->misc_wait);\n\n\tdrbd_req_put_completion_ref(req, m, c_put);\n\tkref_put(&req->kref, drbd_req_destroy);\n}\n\nstatic void drbd_report_io_error(struct drbd_device *device, struct drbd_request *req)\n{\n\tif (!drbd_ratelimit())\n\t\treturn;\n\n\tdrbd_warn(device, \"local %s IO error sector %llu+%u on %pg\\n\",\n\t\t\t(req->rq_state & RQ_WRITE) ? \"WRITE\" : \"READ\",\n\t\t\t(unsigned long long)req->i.sector,\n\t\t\treq->i.size >> 9,\n\t\t\tdevice->ldev->backing_bdev);\n}\n\n \nstatic inline bool is_pending_write_protocol_A(struct drbd_request *req)\n{\n\treturn (req->rq_state &\n\t\t   (RQ_WRITE|RQ_NET_PENDING|RQ_EXP_WRITE_ACK|RQ_EXP_RECEIVE_ACK))\n\t\t== (RQ_WRITE|RQ_NET_PENDING);\n}\n\n \nint __req_mod(struct drbd_request *req, enum drbd_req_event what,\n\t\tstruct drbd_peer_device *peer_device,\n\t\tstruct bio_and_error *m)\n{\n\tstruct drbd_device *const device = req->device;\n\tstruct drbd_connection *const connection = peer_device ? peer_device->connection : NULL;\n\tstruct net_conf *nc;\n\tint p, rv = 0;\n\n\tif (m)\n\t\tm->bio = NULL;\n\n\tswitch (what) {\n\tdefault:\n\t\tdrbd_err(device, \"LOGIC BUG in %s:%u\\n\", __FILE__ , __LINE__);\n\t\tbreak;\n\n\t \n\n\tcase TO_BE_SENT:  \n\t\t \n\t\tD_ASSERT(device, !(req->rq_state & RQ_NET_MASK));\n\t\trcu_read_lock();\n\t\tnc = rcu_dereference(connection->net_conf);\n\t\tp = nc->wire_protocol;\n\t\trcu_read_unlock();\n\t\treq->rq_state |=\n\t\t\tp == DRBD_PROT_C ? RQ_EXP_WRITE_ACK :\n\t\t\tp == DRBD_PROT_B ? RQ_EXP_RECEIVE_ACK : 0;\n\t\tmod_rq_state(req, m, 0, RQ_NET_PENDING);\n\t\tbreak;\n\n\tcase TO_BE_SUBMITTED:  \n\t\t \n\t\tD_ASSERT(device, !(req->rq_state & RQ_LOCAL_MASK));\n\t\tmod_rq_state(req, m, 0, RQ_LOCAL_PENDING);\n\t\tbreak;\n\n\tcase COMPLETED_OK:\n\t\tif (req->rq_state & RQ_WRITE)\n\t\t\tdevice->writ_cnt += req->i.size >> 9;\n\t\telse\n\t\t\tdevice->read_cnt += req->i.size >> 9;\n\n\t\tmod_rq_state(req, m, RQ_LOCAL_PENDING,\n\t\t\t\tRQ_LOCAL_COMPLETED|RQ_LOCAL_OK);\n\t\tbreak;\n\n\tcase ABORT_DISK_IO:\n\t\tmod_rq_state(req, m, 0, RQ_LOCAL_ABORTED);\n\t\tbreak;\n\n\tcase WRITE_COMPLETED_WITH_ERROR:\n\t\tdrbd_report_io_error(device, req);\n\t\t__drbd_chk_io_error(device, DRBD_WRITE_ERROR);\n\t\tmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\n\t\tbreak;\n\n\tcase READ_COMPLETED_WITH_ERROR:\n\t\tdrbd_set_out_of_sync(peer_device, req->i.sector, req->i.size);\n\t\tdrbd_report_io_error(device, req);\n\t\t__drbd_chk_io_error(device, DRBD_READ_ERROR);\n\t\tfallthrough;\n\tcase READ_AHEAD_COMPLETED_WITH_ERROR:\n\t\t \n\t\tmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\n\t\tbreak;\n\n\tcase DISCARD_COMPLETED_NOTSUPP:\n\tcase DISCARD_COMPLETED_WITH_ERROR:\n\t\t \n\t\tmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\n\t\tbreak;\n\n\tcase QUEUE_FOR_NET_READ:\n\t\t \n\t\t \n\n\t\t \n\t\tD_ASSERT(device, drbd_interval_empty(&req->i));\n\t\tdrbd_insert_interval(&device->read_requests, &req->i);\n\n\t\tset_bit(UNPLUG_REMOTE, &device->flags);\n\n\t\tD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\n\t\tD_ASSERT(device, (req->rq_state & RQ_LOCAL_MASK) == 0);\n\t\tmod_rq_state(req, m, 0, RQ_NET_QUEUED);\n\t\treq->w.cb = w_send_read_req;\n\t\tdrbd_queue_work(&connection->sender_work,\n\t\t\t\t&req->w);\n\t\tbreak;\n\n\tcase QUEUE_FOR_NET_WRITE:\n\t\t \n\t\t \n\n\t\t \n\t\tD_ASSERT(device, drbd_interval_empty(&req->i));\n\t\tdrbd_insert_interval(&device->write_requests, &req->i);\n\n\t\t \n\n\t\t \n\t\tset_bit(UNPLUG_REMOTE, &device->flags);\n\n\t\t \n\t\tD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\n\t\tmod_rq_state(req, m, 0, RQ_NET_QUEUED|RQ_EXP_BARR_ACK);\n\t\treq->w.cb =  w_send_dblock;\n\t\tdrbd_queue_work(&connection->sender_work,\n\t\t\t\t&req->w);\n\n\t\t \n\t\trcu_read_lock();\n\t\tnc = rcu_dereference(connection->net_conf);\n\t\tp = nc->max_epoch_size;\n\t\trcu_read_unlock();\n\t\tif (connection->current_tle_writes >= p)\n\t\t\tstart_new_tl_epoch(connection);\n\n\t\tbreak;\n\n\tcase QUEUE_FOR_SEND_OOS:\n\t\tmod_rq_state(req, m, 0, RQ_NET_QUEUED);\n\t\treq->w.cb =  w_send_out_of_sync;\n\t\tdrbd_queue_work(&connection->sender_work,\n\t\t\t\t&req->w);\n\t\tbreak;\n\n\tcase READ_RETRY_REMOTE_CANCELED:\n\tcase SEND_CANCELED:\n\tcase SEND_FAILED:\n\t\t \n\t\tmod_rq_state(req, m, RQ_NET_QUEUED, 0);\n\t\tbreak;\n\n\tcase HANDED_OVER_TO_NETWORK:\n\t\t \n\t\tif (is_pending_write_protocol_A(req))\n\t\t\t \n\t\t\tmod_rq_state(req, m, RQ_NET_QUEUED|RQ_NET_PENDING,\n\t\t\t\t\t\tRQ_NET_SENT|RQ_NET_OK);\n\t\telse\n\t\t\tmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_SENT);\n\t\t \n\t\tbreak;\n\n\tcase OOS_HANDED_TO_NETWORK:\n\t\t \n\t\tmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_DONE);\n\t\tbreak;\n\n\tcase CONNECTION_LOST_WHILE_PENDING:\n\t\t \n\t\tmod_rq_state(req, m,\n\t\t\t\tRQ_NET_OK|RQ_NET_PENDING|RQ_COMPLETION_SUSP,\n\t\t\t\tRQ_NET_DONE);\n\t\tbreak;\n\n\tcase CONFLICT_RESOLVED:\n\t\t \n\t\tD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\n\t\tD_ASSERT(device, req->rq_state & RQ_EXP_WRITE_ACK);\n\t\tmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_DONE|RQ_NET_OK);\n\t\tbreak;\n\n\tcase WRITE_ACKED_BY_PEER_AND_SIS:\n\t\treq->rq_state |= RQ_NET_SIS;\n\t\tfallthrough;\n\tcase WRITE_ACKED_BY_PEER:\n\t\t \n\t\tgoto ack_common;\n\tcase RECV_ACKED_BY_PEER:\n\t\tD_ASSERT(device, req->rq_state & RQ_EXP_RECEIVE_ACK);\n\t\t \n\tack_common:\n\t\tmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK);\n\t\tbreak;\n\n\tcase POSTPONE_WRITE:\n\t\tD_ASSERT(device, req->rq_state & RQ_EXP_WRITE_ACK);\n\t\t \n\t\tD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\n\t\treq->rq_state |= RQ_POSTPONED;\n\t\tif (req->i.waiting)\n\t\t\twake_up(&device->misc_wait);\n\t\t \n\t\tbreak;\n\n\tcase NEG_ACKED:\n\t\tmod_rq_state(req, m, RQ_NET_OK|RQ_NET_PENDING, 0);\n\t\tbreak;\n\n\tcase FAIL_FROZEN_DISK_IO:\n\t\tif (!(req->rq_state & RQ_LOCAL_COMPLETED))\n\t\t\tbreak;\n\t\tmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\n\t\tbreak;\n\n\tcase RESTART_FROZEN_DISK_IO:\n\t\tif (!(req->rq_state & RQ_LOCAL_COMPLETED))\n\t\t\tbreak;\n\n\t\tmod_rq_state(req, m,\n\t\t\t\tRQ_COMPLETION_SUSP|RQ_LOCAL_COMPLETED,\n\t\t\t\tRQ_LOCAL_PENDING);\n\n\t\trv = MR_READ;\n\t\tif (bio_data_dir(req->master_bio) == WRITE)\n\t\t\trv = MR_WRITE;\n\n\t\tget_ldev(device);  \n\t\treq->w.cb = w_restart_disk_io;\n\t\tdrbd_queue_work(&connection->sender_work,\n\t\t\t\t&req->w);\n\t\tbreak;\n\n\tcase RESEND:\n\t\t \n\t\tif (!(req->rq_state & RQ_WRITE) && !req->w.cb) {\n\t\t\tmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!(req->rq_state & RQ_NET_OK)) {\n\t\t\t \n\n\t\t\tmod_rq_state(req, m, RQ_COMPLETION_SUSP, RQ_NET_QUEUED|RQ_NET_PENDING);\n\t\t\tif (req->w.cb) {\n\t\t\t\t \n\t\t\t\tdrbd_queue_work(&connection->sender_work,\n\t\t\t\t\t\t&req->w);\n\t\t\t\trv = req->rq_state & RQ_WRITE ? MR_WRITE : MR_READ;\n\t\t\t}  \n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\t \n\n\tcase BARRIER_ACKED:\n\t\t \n\t\tif (!(req->rq_state & RQ_WRITE))\n\t\t\tbreak;\n\n\t\tif (req->rq_state & RQ_NET_PENDING) {\n\t\t\t \n\t\t\tdrbd_err(device, \"FIXME (BARRIER_ACKED but pending)\\n\");\n\t\t}\n\t\t \n\t\tmod_rq_state(req, m, RQ_COMPLETION_SUSP,\n\t\t\t\t(req->rq_state & RQ_NET_MASK) ? RQ_NET_DONE : 0);\n\t\tbreak;\n\n\tcase DATA_RECEIVED:\n\t\tD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\n\t\tmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK|RQ_NET_DONE);\n\t\tbreak;\n\n\tcase QUEUE_AS_DRBD_BARRIER:\n\t\tstart_new_tl_epoch(connection);\n\t\tmod_rq_state(req, m, 0, RQ_NET_OK|RQ_NET_DONE);\n\t\tbreak;\n\t}\n\n\treturn rv;\n}\n\n \nstatic bool drbd_may_do_local_read(struct drbd_device *device, sector_t sector, int size)\n{\n\tunsigned long sbnr, ebnr;\n\tsector_t esector, nr_sectors;\n\n\tif (device->state.disk == D_UP_TO_DATE)\n\t\treturn true;\n\tif (device->state.disk != D_INCONSISTENT)\n\t\treturn false;\n\tesector = sector + (size >> 9) - 1;\n\tnr_sectors = get_capacity(device->vdisk);\n\tD_ASSERT(device, sector  < nr_sectors);\n\tD_ASSERT(device, esector < nr_sectors);\n\n\tsbnr = BM_SECT_TO_BIT(sector);\n\tebnr = BM_SECT_TO_BIT(esector);\n\n\treturn drbd_bm_count_bits(device, sbnr, ebnr) == 0;\n}\n\nstatic bool remote_due_to_read_balancing(struct drbd_device *device, sector_t sector,\n\t\tenum drbd_read_balancing rbm)\n{\n\tint stripe_shift;\n\n\tswitch (rbm) {\n\tcase RB_CONGESTED_REMOTE:\n\t\treturn false;\n\tcase RB_LEAST_PENDING:\n\t\treturn atomic_read(&device->local_cnt) >\n\t\t\tatomic_read(&device->ap_pending_cnt) + atomic_read(&device->rs_pending_cnt);\n\tcase RB_32K_STRIPING:   \n\tcase RB_64K_STRIPING:\n\tcase RB_128K_STRIPING:\n\tcase RB_256K_STRIPING:\n\tcase RB_512K_STRIPING:\n\tcase RB_1M_STRIPING:    \n\t\tstripe_shift = (rbm - RB_32K_STRIPING + 15);\n\t\treturn (sector >> (stripe_shift - 9)) & 1;\n\tcase RB_ROUND_ROBIN:\n\t\treturn test_and_change_bit(READ_BALANCE_RR, &device->flags);\n\tcase RB_PREFER_REMOTE:\n\t\treturn true;\n\tcase RB_PREFER_LOCAL:\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic void complete_conflicting_writes(struct drbd_request *req)\n{\n\tDEFINE_WAIT(wait);\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_interval *i;\n\tsector_t sector = req->i.sector;\n\tint size = req->i.size;\n\n\tfor (;;) {\n\t\tdrbd_for_each_overlap(i, &device->write_requests, sector, size) {\n\t\t\t \n\t\t\tif (i->completed)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tif (!i)\t \n\t\t\tbreak;\n\n\t\t \n\t\tprepare_to_wait(&device->misc_wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\ti->waiting = true;\n\t\tspin_unlock_irq(&device->resource->req_lock);\n\t\tschedule();\n\t\tspin_lock_irq(&device->resource->req_lock);\n\t}\n\tfinish_wait(&device->misc_wait, &wait);\n}\n\n \nstatic void maybe_pull_ahead(struct drbd_device *device)\n{\n\tstruct drbd_connection *connection = first_peer_device(device)->connection;\n\tstruct net_conf *nc;\n\tbool congested = false;\n\tenum drbd_on_congestion on_congestion;\n\n\trcu_read_lock();\n\tnc = rcu_dereference(connection->net_conf);\n\ton_congestion = nc ? nc->on_congestion : OC_BLOCK;\n\trcu_read_unlock();\n\tif (on_congestion == OC_BLOCK ||\n\t    connection->agreed_pro_version < 96)\n\t\treturn;\n\n\tif (on_congestion == OC_PULL_AHEAD && device->state.conn == C_AHEAD)\n\t\treturn;  \n\n\t \n\tif (!get_ldev_if_state(device, D_UP_TO_DATE))\n\t\treturn;\n\n\tif (nc->cong_fill &&\n\t    atomic_read(&device->ap_in_flight) >= nc->cong_fill) {\n\t\tdrbd_info(device, \"Congestion-fill threshold reached\\n\");\n\t\tcongested = true;\n\t}\n\n\tif (device->act_log->used >= nc->cong_extents) {\n\t\tdrbd_info(device, \"Congestion-extents threshold reached\\n\");\n\t\tcongested = true;\n\t}\n\n\tif (congested) {\n\t\t \n\t\tstart_new_tl_epoch(first_peer_device(device)->connection);\n\n\t\tif (on_congestion == OC_PULL_AHEAD)\n\t\t\t_drbd_set_state(_NS(device, conn, C_AHEAD), 0, NULL);\n\t\telse   \n\t\t\t_drbd_set_state(_NS(device, conn, C_DISCONNECTING), 0, NULL);\n\t}\n\tput_ldev(device);\n}\n\n \nstatic bool do_remote_read(struct drbd_request *req)\n{\n\tstruct drbd_device *device = req->device;\n\tenum drbd_read_balancing rbm;\n\n\tif (req->private_bio) {\n\t\tif (!drbd_may_do_local_read(device,\n\t\t\t\t\treq->i.sector, req->i.size)) {\n\t\t\tbio_put(req->private_bio);\n\t\t\treq->private_bio = NULL;\n\t\t\tput_ldev(device);\n\t\t}\n\t}\n\n\tif (device->state.pdsk != D_UP_TO_DATE)\n\t\treturn false;\n\n\tif (req->private_bio == NULL)\n\t\treturn true;\n\n\t \n\n\trcu_read_lock();\n\trbm = rcu_dereference(device->ldev->disk_conf)->read_balancing;\n\trcu_read_unlock();\n\n\tif (rbm == RB_PREFER_LOCAL && req->private_bio)\n\t\treturn false;  \n\n\tif (remote_due_to_read_balancing(device, req->i.sector, rbm)) {\n\t\tif (req->private_bio) {\n\t\t\tbio_put(req->private_bio);\n\t\t\treq->private_bio = NULL;\n\t\t\tput_ldev(device);\n\t\t}\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool drbd_should_do_remote(union drbd_dev_state s)\n{\n\treturn s.pdsk == D_UP_TO_DATE ||\n\t\t(s.pdsk >= D_INCONSISTENT &&\n\t\t s.conn >= C_WF_BITMAP_T &&\n\t\t s.conn < C_AHEAD);\n\t \n}\n\nstatic bool drbd_should_send_out_of_sync(union drbd_dev_state s)\n{\n\treturn s.conn == C_AHEAD || s.conn == C_WF_BITMAP_S;\n\t \n}\n\n \nstatic int drbd_process_write_request(struct drbd_request *req)\n{\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\tint remote, send_oos;\n\n\tremote = drbd_should_do_remote(device->state);\n\tsend_oos = drbd_should_send_out_of_sync(device->state);\n\n\t \n\tif (unlikely(req->i.size == 0)) {\n\t\t \n\t\tD_ASSERT(device, req->master_bio->bi_opf & REQ_PREFLUSH);\n\t\tif (remote)\n\t\t\t_req_mod(req, QUEUE_AS_DRBD_BARRIER, peer_device);\n\t\treturn remote;\n\t}\n\n\tif (!remote && !send_oos)\n\t\treturn 0;\n\n\tD_ASSERT(device, !(remote && send_oos));\n\n\tif (remote) {\n\t\t_req_mod(req, TO_BE_SENT, peer_device);\n\t\t_req_mod(req, QUEUE_FOR_NET_WRITE, peer_device);\n\t} else if (drbd_set_out_of_sync(peer_device, req->i.sector, req->i.size))\n\t\t_req_mod(req, QUEUE_FOR_SEND_OOS, peer_device);\n\n\treturn remote;\n}\n\nstatic void drbd_process_discard_or_zeroes_req(struct drbd_request *req, int flags)\n{\n\tint err = drbd_issue_discard_or_zero_out(req->device,\n\t\t\t\treq->i.sector, req->i.size >> 9, flags);\n\tif (err)\n\t\treq->private_bio->bi_status = BLK_STS_IOERR;\n\tbio_endio(req->private_bio);\n}\n\nstatic void\ndrbd_submit_req_private_bio(struct drbd_request *req)\n{\n\tstruct drbd_device *device = req->device;\n\tstruct bio *bio = req->private_bio;\n\tunsigned int type;\n\n\tif (bio_op(bio) != REQ_OP_READ)\n\t\ttype = DRBD_FAULT_DT_WR;\n\telse if (bio->bi_opf & REQ_RAHEAD)\n\t\ttype = DRBD_FAULT_DT_RA;\n\telse\n\t\ttype = DRBD_FAULT_DT_RD;\n\n\t \n\tif (get_ldev(device)) {\n\t\tif (drbd_insert_fault(device, type))\n\t\t\tbio_io_error(bio);\n\t\telse if (bio_op(bio) == REQ_OP_WRITE_ZEROES)\n\t\t\tdrbd_process_discard_or_zeroes_req(req, EE_ZEROOUT |\n\t\t\t    ((bio->bi_opf & REQ_NOUNMAP) ? 0 : EE_TRIM));\n\t\telse if (bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tdrbd_process_discard_or_zeroes_req(req, EE_TRIM);\n\t\telse\n\t\t\tsubmit_bio_noacct(bio);\n\t\tput_ldev(device);\n\t} else\n\t\tbio_io_error(bio);\n}\n\nstatic void drbd_queue_write(struct drbd_device *device, struct drbd_request *req)\n{\n\tspin_lock_irq(&device->resource->req_lock);\n\tlist_add_tail(&req->tl_requests, &device->submit.writes);\n\tlist_add_tail(&req->req_pending_master_completion,\n\t\t\t&device->pending_master_completion[1  ]);\n\tspin_unlock_irq(&device->resource->req_lock);\n\tqueue_work(device->submit.wq, &device->submit.worker);\n\t \n\twake_up(&device->al_wait);\n}\n\n \nstatic struct drbd_request *\ndrbd_request_prepare(struct drbd_device *device, struct bio *bio)\n{\n\tconst int rw = bio_data_dir(bio);\n\tstruct drbd_request *req;\n\n\t \n\treq = drbd_req_new(device, bio);\n\tif (!req) {\n\t\tdec_ap_bio(device);\n\t\t \n\t\tdrbd_err(device, \"could not kmalloc() req\\n\");\n\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\tbio_endio(bio);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\treq->start_jif = bio_start_io_acct(req->master_bio);\n\n\tif (get_ldev(device)) {\n\t\treq->private_bio = bio_alloc_clone(device->ldev->backing_bdev,\n\t\t\t\t\t\t   bio, GFP_NOIO,\n\t\t\t\t\t\t   &drbd_io_bio_set);\n\t\treq->private_bio->bi_private = req;\n\t\treq->private_bio->bi_end_io = drbd_request_endio;\n\t}\n\n\t \n\tif (bio_op(bio) == REQ_OP_WRITE_ZEROES ||\n\t    bio_op(bio) == REQ_OP_DISCARD)\n\t\tgoto queue_for_submitter_thread;\n\n\tif (rw == WRITE && req->private_bio && req->i.size\n\t&& !test_bit(AL_SUSPENDED, &device->flags)) {\n\t\tif (!drbd_al_begin_io_fastpath(device, &req->i))\n\t\t\tgoto queue_for_submitter_thread;\n\t\treq->rq_state |= RQ_IN_ACT_LOG;\n\t\treq->in_actlog_jif = jiffies;\n\t}\n\treturn req;\n\n queue_for_submitter_thread:\n\tatomic_inc(&device->ap_actlog_cnt);\n\tdrbd_queue_write(device, req);\n\treturn NULL;\n}\n\n \nstatic bool may_do_writes(struct drbd_device *device)\n{\n\tconst union drbd_dev_state s = device->state;\n\treturn s.disk == D_UP_TO_DATE || s.pdsk == D_UP_TO_DATE;\n}\n\nstruct drbd_plug_cb {\n\tstruct blk_plug_cb cb;\n\tstruct drbd_request *most_recent_req;\n\t \n};\n\nstatic void drbd_unplug(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct drbd_plug_cb *plug = container_of(cb, struct drbd_plug_cb, cb);\n\tstruct drbd_resource *resource = plug->cb.data;\n\tstruct drbd_request *req = plug->most_recent_req;\n\n\tkfree(cb);\n\tif (!req)\n\t\treturn;\n\n\tspin_lock_irq(&resource->req_lock);\n\t \n\treq->rq_state |= RQ_UNPLUG;\n\t \n\tdrbd_queue_unplug(req->device);\n\tkref_put(&req->kref, drbd_req_destroy);\n\tspin_unlock_irq(&resource->req_lock);\n}\n\nstatic struct drbd_plug_cb* drbd_check_plugged(struct drbd_resource *resource)\n{\n\t \n\tstruct drbd_plug_cb *plug;\n\tstruct blk_plug_cb *cb = blk_check_plugged(drbd_unplug, resource, sizeof(*plug));\n\n\tif (cb)\n\t\tplug = container_of(cb, struct drbd_plug_cb, cb);\n\telse\n\t\tplug = NULL;\n\treturn plug;\n}\n\nstatic void drbd_update_plug(struct drbd_plug_cb *plug, struct drbd_request *req)\n{\n\tstruct drbd_request *tmp = plug->most_recent_req;\n\t \n\tkref_get(&req->kref);\n\tplug->most_recent_req = req;\n\tif (tmp)\n\t\tkref_put(&tmp->kref, drbd_req_destroy);\n}\n\nstatic void drbd_send_and_submit(struct drbd_device *device, struct drbd_request *req)\n{\n\tstruct drbd_resource *resource = device->resource;\n\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\tconst int rw = bio_data_dir(req->master_bio);\n\tstruct bio_and_error m = { NULL, };\n\tbool no_remote = false;\n\tbool submit_private_bio = false;\n\n\tspin_lock_irq(&resource->req_lock);\n\tif (rw == WRITE) {\n\t\t \n\t\tcomplete_conflicting_writes(req);\n\t\t \n\n\t\t \n\t\tmaybe_pull_ahead(device);\n\t}\n\n\n\tif (drbd_suspended(device)) {\n\t\t \n\t\treq->rq_state |= RQ_POSTPONED;\n\t\tif (req->private_bio) {\n\t\t\tbio_put(req->private_bio);\n\t\t\treq->private_bio = NULL;\n\t\t\tput_ldev(device);\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tif (rw != WRITE) {\n\t\tif (!do_remote_read(req) && !req->private_bio)\n\t\t\tgoto nodata;\n\t}\n\n\t \n\treq->epoch = atomic_read(&first_peer_device(device)->connection->current_tle_nr);\n\n\t \n\tif (likely(req->i.size!=0)) {\n\t\tif (rw == WRITE)\n\t\t\tfirst_peer_device(device)->connection->current_tle_writes++;\n\n\t\tlist_add_tail(&req->tl_requests, &first_peer_device(device)->connection->transfer_log);\n\t}\n\n\tif (rw == WRITE) {\n\t\tif (req->private_bio && !may_do_writes(device)) {\n\t\t\tbio_put(req->private_bio);\n\t\t\treq->private_bio = NULL;\n\t\t\tput_ldev(device);\n\t\t\tgoto nodata;\n\t\t}\n\t\tif (!drbd_process_write_request(req))\n\t\t\tno_remote = true;\n\t} else {\n\t\t \n\t\tif (req->private_bio == NULL) {\n\t\t\t_req_mod(req, TO_BE_SENT, peer_device);\n\t\t\t_req_mod(req, QUEUE_FOR_NET_READ, peer_device);\n\t\t} else\n\t\t\tno_remote = true;\n\t}\n\n\tif (no_remote == false) {\n\t\tstruct drbd_plug_cb *plug = drbd_check_plugged(resource);\n\t\tif (plug)\n\t\t\tdrbd_update_plug(plug, req);\n\t}\n\n\t \n\tif (list_empty(&req->req_pending_master_completion))\n\t\tlist_add_tail(&req->req_pending_master_completion,\n\t\t\t&device->pending_master_completion[rw == WRITE]);\n\tif (req->private_bio) {\n\t\t \n\t\treq->pre_submit_jif = jiffies;\n\t\tlist_add_tail(&req->req_pending_local,\n\t\t\t&device->pending_completion[rw == WRITE]);\n\t\t_req_mod(req, TO_BE_SUBMITTED, NULL);\n\t\t \n\t\tsubmit_private_bio = true;\n\t} else if (no_remote) {\nnodata:\n\t\tif (drbd_ratelimit())\n\t\t\tdrbd_err(device, \"IO ERROR: neither local nor remote data, sector %llu+%u\\n\",\n\t\t\t\t\t(unsigned long long)req->i.sector, req->i.size >> 9);\n\t\t \n\t}\n\nout:\n\tdrbd_req_put_completion_ref(req, &m, 1);\n\tspin_unlock_irq(&resource->req_lock);\n\n\t \n\tif (submit_private_bio)\n\t\tdrbd_submit_req_private_bio(req);\n\tif (m.bio)\n\t\tcomplete_master_bio(device, &m);\n}\n\nvoid __drbd_make_request(struct drbd_device *device, struct bio *bio)\n{\n\tstruct drbd_request *req = drbd_request_prepare(device, bio);\n\tif (IS_ERR_OR_NULL(req))\n\t\treturn;\n\tdrbd_send_and_submit(device, req);\n}\n\nstatic void submit_fast_path(struct drbd_device *device, struct list_head *incoming)\n{\n\tstruct blk_plug plug;\n\tstruct drbd_request *req, *tmp;\n\n\tblk_start_plug(&plug);\n\tlist_for_each_entry_safe(req, tmp, incoming, tl_requests) {\n\t\tconst int rw = bio_data_dir(req->master_bio);\n\n\t\tif (rw == WRITE  \n\t\t&& req->private_bio && req->i.size\n\t\t&& !test_bit(AL_SUSPENDED, &device->flags)) {\n\t\t\tif (!drbd_al_begin_io_fastpath(device, &req->i))\n\t\t\t\tcontinue;\n\n\t\t\treq->rq_state |= RQ_IN_ACT_LOG;\n\t\t\treq->in_actlog_jif = jiffies;\n\t\t\tatomic_dec(&device->ap_actlog_cnt);\n\t\t}\n\n\t\tlist_del_init(&req->tl_requests);\n\t\tdrbd_send_and_submit(device, req);\n\t}\n\tblk_finish_plug(&plug);\n}\n\nstatic bool prepare_al_transaction_nonblock(struct drbd_device *device,\n\t\t\t\t\t    struct list_head *incoming,\n\t\t\t\t\t    struct list_head *pending,\n\t\t\t\t\t    struct list_head *later)\n{\n\tstruct drbd_request *req;\n\tint wake = 0;\n\tint err;\n\n\tspin_lock_irq(&device->al_lock);\n\twhile ((req = list_first_entry_or_null(incoming, struct drbd_request, tl_requests))) {\n\t\terr = drbd_al_begin_io_nonblock(device, &req->i);\n\t\tif (err == -ENOBUFS)\n\t\t\tbreak;\n\t\tif (err == -EBUSY)\n\t\t\twake = 1;\n\t\tif (err)\n\t\t\tlist_move_tail(&req->tl_requests, later);\n\t\telse\n\t\t\tlist_move_tail(&req->tl_requests, pending);\n\t}\n\tspin_unlock_irq(&device->al_lock);\n\tif (wake)\n\t\twake_up(&device->al_wait);\n\treturn !list_empty(pending);\n}\n\nstatic void send_and_submit_pending(struct drbd_device *device, struct list_head *pending)\n{\n\tstruct blk_plug plug;\n\tstruct drbd_request *req;\n\n\tblk_start_plug(&plug);\n\twhile ((req = list_first_entry_or_null(pending, struct drbd_request, tl_requests))) {\n\t\treq->rq_state |= RQ_IN_ACT_LOG;\n\t\treq->in_actlog_jif = jiffies;\n\t\tatomic_dec(&device->ap_actlog_cnt);\n\t\tlist_del_init(&req->tl_requests);\n\t\tdrbd_send_and_submit(device, req);\n\t}\n\tblk_finish_plug(&plug);\n}\n\nvoid do_submit(struct work_struct *ws)\n{\n\tstruct drbd_device *device = container_of(ws, struct drbd_device, submit.worker);\n\tLIST_HEAD(incoming);\t \n\tLIST_HEAD(pending);\t \n\tLIST_HEAD(busy);\t \n\n\t \n\tspin_lock_irq(&device->resource->req_lock);\n\tlist_splice_tail_init(&device->submit.writes, &incoming);\n\tspin_unlock_irq(&device->resource->req_lock);\n\n\tfor (;;) {\n\t\tDEFINE_WAIT(wait);\n\n\t\t \n\t\tlist_splice_init(&busy, &incoming);\n\t\tsubmit_fast_path(device, &incoming);\n\t\tif (list_empty(&incoming))\n\t\t\tbreak;\n\n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&device->al_wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\t\tlist_splice_init(&busy, &incoming);\n\t\t\tprepare_al_transaction_nonblock(device, &incoming, &pending, &busy);\n\t\t\tif (!list_empty(&pending))\n\t\t\t\tbreak;\n\n\t\t\tschedule();\n\n\t\t\t \n\t\t\tif (!list_empty(&incoming))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tspin_lock_irq(&device->resource->req_lock);\n\t\t\tlist_splice_tail_init(&device->submit.writes, &incoming);\n\t\t\tspin_unlock_irq(&device->resource->req_lock);\n\t\t}\n\t\tfinish_wait(&device->al_wait, &wait);\n\n\t\t \n\n\t\twhile (list_empty(&incoming)) {\n\t\t\tLIST_HEAD(more_pending);\n\t\t\tLIST_HEAD(more_incoming);\n\t\t\tbool made_progress;\n\n\t\t\t \n\t\t\tif (list_empty(&device->submit.writes))\n\t\t\t\tbreak;\n\n\t\t\tspin_lock_irq(&device->resource->req_lock);\n\t\t\tlist_splice_tail_init(&device->submit.writes, &more_incoming);\n\t\t\tspin_unlock_irq(&device->resource->req_lock);\n\n\t\t\tif (list_empty(&more_incoming))\n\t\t\t\tbreak;\n\n\t\t\tmade_progress = prepare_al_transaction_nonblock(device, &more_incoming, &more_pending, &busy);\n\n\t\t\tlist_splice_tail_init(&more_pending, &pending);\n\t\t\tlist_splice_tail_init(&more_incoming, &incoming);\n\t\t\tif (!made_progress)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdrbd_al_begin_io_commit(device);\n\t\tsend_and_submit_pending(device, &pending);\n\t}\n}\n\nvoid drbd_submit_bio(struct bio *bio)\n{\n\tstruct drbd_device *device = bio->bi_bdev->bd_disk->private_data;\n\n\tbio = bio_split_to_limits(bio);\n\tif (!bio)\n\t\treturn;\n\n\t \n\tD_ASSERT(device, IS_ALIGNED(bio->bi_iter.bi_size, 512));\n\n\tinc_ap_bio(device);\n\t__drbd_make_request(device, bio);\n}\n\nstatic bool net_timeout_reached(struct drbd_request *net_req,\n\t\tstruct drbd_connection *connection,\n\t\tunsigned long now, unsigned long ent,\n\t\tunsigned int ko_count, unsigned int timeout)\n{\n\tstruct drbd_device *device = net_req->device;\n\n\tif (!time_after(now, net_req->pre_send_jif + ent))\n\t\treturn false;\n\n\tif (time_in_range(now, connection->last_reconnect_jif, connection->last_reconnect_jif + ent))\n\t\treturn false;\n\n\tif (net_req->rq_state & RQ_NET_PENDING) {\n\t\tdrbd_warn(device, \"Remote failed to finish a request within %ums > ko-count (%u) * timeout (%u * 0.1s)\\n\",\n\t\t\tjiffies_to_msecs(now - net_req->pre_send_jif), ko_count, timeout);\n\t\treturn true;\n\t}\n\n\t \n\tif (net_req->epoch == connection->send.current_epoch_nr) {\n\t\tdrbd_warn(device,\n\t\t\t\"We did not send a P_BARRIER for %ums > ko-count (%u) * timeout (%u * 0.1s); drbd kernel thread blocked?\\n\",\n\t\t\tjiffies_to_msecs(now - net_req->pre_send_jif), ko_count, timeout);\n\t\treturn false;\n\t}\n\n\t \n\tif (time_after(now, connection->send.last_sent_barrier_jif + ent)) {\n\t\tdrbd_warn(device, \"Remote failed to answer a P_BARRIER (sent at %lu jif; now=%lu jif) within %ums > ko-count (%u) * timeout (%u * 0.1s)\\n\",\n\t\t\tconnection->send.last_sent_barrier_jif, now,\n\t\t\tjiffies_to_msecs(now - connection->send.last_sent_barrier_jif), ko_count, timeout);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \n\nvoid request_timer_fn(struct timer_list *t)\n{\n\tstruct drbd_device *device = from_timer(device, t, request_timer);\n\tstruct drbd_connection *connection = first_peer_device(device)->connection;\n\tstruct drbd_request *req_read, *req_write, *req_peer;  \n\tstruct net_conf *nc;\n\tunsigned long oldest_submit_jif;\n\tunsigned long ent = 0, dt = 0, et, nt;  \n\tunsigned long now;\n\tunsigned int ko_count = 0, timeout = 0;\n\n\trcu_read_lock();\n\tnc = rcu_dereference(connection->net_conf);\n\tif (nc && device->state.conn >= C_WF_REPORT_PARAMS) {\n\t\tko_count = nc->ko_count;\n\t\ttimeout = nc->timeout;\n\t}\n\n\tif (get_ldev(device)) {  \n\t\tdt = rcu_dereference(device->ldev->disk_conf)->disk_timeout * HZ / 10;\n\t\tput_ldev(device);\n\t}\n\trcu_read_unlock();\n\n\n\tent = timeout * HZ/10 * ko_count;\n\tet = min_not_zero(dt, ent);\n\n\tif (!et)\n\t\treturn;  \n\n\tnow = jiffies;\n\tnt = now + et;\n\n\tspin_lock_irq(&device->resource->req_lock);\n\treq_read = list_first_entry_or_null(&device->pending_completion[0], struct drbd_request, req_pending_local);\n\treq_write = list_first_entry_or_null(&device->pending_completion[1], struct drbd_request, req_pending_local);\n\n\t \n\n\t \n\treq_peer = connection->req_ack_pending;\n\n\t \n\tif (!req_peer)\n\t\treq_peer = connection->req_not_net_done;\n\n\t \n\tif (req_peer && req_peer->device != device)\n\t\treq_peer = NULL;\n\n\t \n\tif (req_peer == NULL && req_write == NULL && req_read == NULL)\n\t\tgoto out;\n\n\toldest_submit_jif =\n\t\t(req_write && req_read)\n\t\t? ( time_before(req_write->pre_submit_jif, req_read->pre_submit_jif)\n\t\t  ? req_write->pre_submit_jif : req_read->pre_submit_jif )\n\t\t: req_write ? req_write->pre_submit_jif\n\t\t: req_read ? req_read->pre_submit_jif : now;\n\n\tif (ent && req_peer && net_timeout_reached(req_peer, connection, now, ent, ko_count, timeout))\n\t\t_conn_request_state(connection, NS(conn, C_TIMEOUT), CS_VERBOSE | CS_HARD);\n\n\tif (dt && oldest_submit_jif != now &&\n\t\t time_after(now, oldest_submit_jif + dt) &&\n\t\t!time_in_range(now, device->last_reattach_jif, device->last_reattach_jif + dt)) {\n\t\tdrbd_warn(device, \"Local backing device failed to meet the disk-timeout\\n\");\n\t\t__drbd_chk_io_error(device, DRBD_FORCE_DETACH);\n\t}\n\n\t \n\tent = (ent && req_peer && time_before(now, req_peer->pre_send_jif + ent))\n\t\t? req_peer->pre_send_jif + ent : now + et;\n\tdt = (dt && oldest_submit_jif != now && time_before(now, oldest_submit_jif + dt))\n\t\t? oldest_submit_jif + dt : now + et;\n\tnt = time_before(ent, dt) ? ent : dt;\nout:\n\tspin_unlock_irq(&device->resource->req_lock);\n\tmod_timer(&device->request_timer, nt);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}