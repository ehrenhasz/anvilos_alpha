{
  "module_name": "drbd_int.h",
  "hash_id": "fa7632a46fd9a22f5b53ac71d39d0fdde81631a2b6bdf04fd6e3cf24a69b2850",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/drbd/drbd_int.h",
  "human_readable_source": " \n \n\n#ifndef _DRBD_INT_H\n#define _DRBD_INT_H\n\n#include <crypto/hash.h>\n#include <linux/compiler.h>\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/sched/signal.h>\n#include <linux/bitops.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/tcp.h>\n#include <linux/mutex.h>\n#include <linux/major.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/idr.h>\n#include <linux/dynamic_debug.h>\n#include <net/tcp.h>\n#include <linux/lru_cache.h>\n#include <linux/prefetch.h>\n#include <linux/drbd_genl_api.h>\n#include <linux/drbd.h>\n#include <linux/drbd_config.h>\n#include \"drbd_strings.h\"\n#include \"drbd_state.h\"\n#include \"drbd_protocol.h\"\n#include \"drbd_polymorph_printk.h\"\n\n \n#ifdef CONFIG_DRBD_FAULT_INJECTION\nextern int drbd_enable_faults;\nextern int drbd_fault_rate;\n#endif\n\nextern unsigned int drbd_minor_count;\nextern char drbd_usermode_helper[];\nextern int drbd_proc_details;\n\n\n \n#define DRBD_SIGKILL SIGHUP\n\n#define ID_IN_SYNC      (4711ULL)\n#define ID_OUT_OF_SYNC  (4712ULL)\n#define ID_SYNCER (-1ULL)\n\n#define UUID_NEW_BM_OFFSET ((u64)0x0001000000000000ULL)\n\nstruct drbd_device;\nstruct drbd_connection;\nstruct drbd_peer_device;\n\n \nenum {\n\tDRBD_FAULT_MD_WR = 0,\t \n\tDRBD_FAULT_MD_RD = 1,\t \n\tDRBD_FAULT_RS_WR = 2,\t \n\tDRBD_FAULT_RS_RD = 3,\n\tDRBD_FAULT_DT_WR = 4,\t \n\tDRBD_FAULT_DT_RD = 5,\n\tDRBD_FAULT_DT_RA = 6,\t \n\tDRBD_FAULT_BM_ALLOC = 7,\t \n\tDRBD_FAULT_AL_EE = 8,\t \n\tDRBD_FAULT_RECEIVE = 9,  \n\n\tDRBD_FAULT_MAX,\n};\n\nextern unsigned int\n_drbd_insert_fault(struct drbd_device *device, unsigned int type);\n\nstatic inline int\ndrbd_insert_fault(struct drbd_device *device, unsigned int type) {\n#ifdef CONFIG_DRBD_FAULT_INJECTION\n\treturn drbd_fault_rate &&\n\t\t(drbd_enable_faults & (1<<type)) &&\n\t\t_drbd_insert_fault(device, type);\n#else\n\treturn 0;\n#endif\n}\n\n \n#define div_ceil(A, B) ((A)/(B) + ((A)%(B) ? 1 : 0))\n \n#define div_floor(A, B) ((A)/(B))\n\nextern struct ratelimit_state drbd_ratelimit_state;\nextern struct idr drbd_devices;  \nextern struct list_head drbd_resources;  \n\nextern const char *cmdname(enum drbd_packet cmd);\n\n \nstruct bm_xfer_ctx {\n\t \n\tunsigned long bm_bits;\n\tunsigned long bm_words;\n\t \n\tunsigned long bit_offset;\n\tunsigned long word_offset;\n\n\t \n\tunsigned packets[2];\n\tunsigned bytes[2];\n};\n\nextern void INFO_bm_xfer_stats(struct drbd_peer_device *peer_device,\n\t\t\t       const char *direction, struct bm_xfer_ctx *c);\n\nstatic inline void bm_xfer_ctx_bit_to_word_offset(struct bm_xfer_ctx *c)\n{\n\t \n#if BITS_PER_LONG == 64\n\tc->word_offset = c->bit_offset >> 6;\n#elif BITS_PER_LONG == 32\n\tc->word_offset = c->bit_offset >> 5;\n\tc->word_offset &= ~(1UL);\n#else\n# error \"unsupported BITS_PER_LONG\"\n#endif\n}\n\nextern unsigned int drbd_header_size(struct drbd_connection *connection);\n\n \nenum drbd_thread_state {\n\tNONE,\n\tRUNNING,\n\tEXITING,\n\tRESTARTING\n};\n\nstruct drbd_thread {\n\tspinlock_t t_lock;\n\tstruct task_struct *task;\n\tstruct completion stop;\n\tenum drbd_thread_state t_state;\n\tint (*function) (struct drbd_thread *);\n\tstruct drbd_resource *resource;\n\tstruct drbd_connection *connection;\n\tint reset_cpu_mask;\n\tconst char *name;\n};\n\nstatic inline enum drbd_thread_state get_t_state(struct drbd_thread *thi)\n{\n\t \n\n\tsmp_rmb();\n\treturn thi->t_state;\n}\n\nstruct drbd_work {\n\tstruct list_head list;\n\tint (*cb)(struct drbd_work *, int cancel);\n};\n\nstruct drbd_device_work {\n\tstruct drbd_work w;\n\tstruct drbd_device *device;\n};\n\n#include \"drbd_interval.h\"\n\nextern int drbd_wait_misc(struct drbd_device *, struct drbd_interval *);\n\nextern void lock_all_resources(void);\nextern void unlock_all_resources(void);\n\nstruct drbd_request {\n\tstruct drbd_work w;\n\tstruct drbd_device *device;\n\n\t \n\tstruct bio *private_bio;\n\n\tstruct drbd_interval i;\n\n\t \n\tunsigned int epoch;\n\n\tstruct list_head tl_requests;  \n\tstruct bio *master_bio;        \n\n\t \n\tstruct list_head req_pending_master_completion;\n\tstruct list_head req_pending_local;\n\n\t \n\tunsigned long start_jif;\n\n\t \n\n\t \n\n\t \n\tunsigned long in_actlog_jif;\n\n\t \n\tunsigned long pre_submit_jif;\n\n\t \n\tunsigned long pre_send_jif;\n\tunsigned long acked_jif;\n\tunsigned long net_done_jif;\n\n\t \n\n\n\t \n\tatomic_t completion_ref;\n\t \n\tstruct kref kref;\n\n\tunsigned rq_state;  \n};\n\nstruct drbd_epoch {\n\tstruct drbd_connection *connection;\n\tstruct list_head list;\n\tunsigned int barrier_nr;\n\tatomic_t epoch_size;  \n\tatomic_t active;      \n\tunsigned long flags;\n};\n\n \nint drbdd_init(struct drbd_thread *);\nint drbd_asender(struct drbd_thread *);\n\n \nenum {\n\tDE_HAVE_BARRIER_NUMBER,\n};\n\nenum epoch_event {\n\tEV_PUT,\n\tEV_GOT_BARRIER_NR,\n\tEV_BECAME_LAST,\n\tEV_CLEANUP = 32,  \n};\n\nstruct digest_info {\n\tint digest_size;\n\tvoid *digest;\n};\n\nstruct drbd_peer_request {\n\tstruct drbd_work w;\n\tstruct drbd_peer_device *peer_device;\n\tstruct drbd_epoch *epoch;  \n\tstruct page *pages;\n\tblk_opf_t opf;\n\tatomic_t pending_bios;\n\tstruct drbd_interval i;\n\t \n\tunsigned long flags;\n\tunsigned long submit_jif;\n\tunion {\n\t\tu64 block_id;\n\t\tstruct digest_info *digest;\n\t};\n};\n\n \n#define peer_req_op(peer_req) \\\n\t((peer_req)->opf & REQ_OP_MASK)\n\n \nenum {\n\t__EE_CALL_AL_COMPLETE_IO,\n\t__EE_MAY_SET_IN_SYNC,\n\n\t \n\t__EE_TRIM,\n\t \n\t__EE_ZEROOUT,\n\n\t \n\t__EE_RESUBMITTED,\n\n\t \n\t__EE_WAS_ERROR,\n\n\t \n\t__EE_HAS_DIGEST,\n\n\t \n\t__EE_RESTART_REQUESTS,\n\n\t \n\t__EE_SEND_WRITE_ACK,\n\n\t \n\t__EE_IN_INTERVAL_TREE,\n\n\t \n\t \n\t__EE_SUBMITTED,\n\n\t \n\t__EE_WRITE,\n\n\t \n\t__EE_WRITE_SAME,\n\n\t \n\t__EE_APPLICATION,\n\n\t \n\t__EE_RS_THIN_REQ,\n};\n#define EE_CALL_AL_COMPLETE_IO (1<<__EE_CALL_AL_COMPLETE_IO)\n#define EE_MAY_SET_IN_SYNC     (1<<__EE_MAY_SET_IN_SYNC)\n#define EE_TRIM                (1<<__EE_TRIM)\n#define EE_ZEROOUT             (1<<__EE_ZEROOUT)\n#define EE_RESUBMITTED         (1<<__EE_RESUBMITTED)\n#define EE_WAS_ERROR           (1<<__EE_WAS_ERROR)\n#define EE_HAS_DIGEST          (1<<__EE_HAS_DIGEST)\n#define EE_RESTART_REQUESTS\t(1<<__EE_RESTART_REQUESTS)\n#define EE_SEND_WRITE_ACK\t(1<<__EE_SEND_WRITE_ACK)\n#define EE_IN_INTERVAL_TREE\t(1<<__EE_IN_INTERVAL_TREE)\n#define EE_SUBMITTED\t\t(1<<__EE_SUBMITTED)\n#define EE_WRITE\t\t(1<<__EE_WRITE)\n#define EE_WRITE_SAME\t\t(1<<__EE_WRITE_SAME)\n#define EE_APPLICATION\t\t(1<<__EE_APPLICATION)\n#define EE_RS_THIN_REQ\t\t(1<<__EE_RS_THIN_REQ)\n\n \nenum {\n\tUNPLUG_REMOTE,\t\t \n\tMD_DIRTY,\t\t \n\tUSE_DEGR_WFC_T,\t\t \n\tCL_ST_CHG_SUCCESS,\n\tCL_ST_CHG_FAIL,\n\tCRASHED_PRIMARY,\t \n\tCONSIDER_RESYNC,\n\n\tMD_NO_FUA,\t\t \n\n\tBITMAP_IO,\t\t \n\tBITMAP_IO_QUEUED,        \n\tWAS_IO_ERROR,\t\t \n\tWAS_READ_ERROR,\t\t \n\tFORCE_DETACH,\t\t \n\tRESYNC_AFTER_NEG,        \n\tRESIZE_PENDING,\t\t \n\tNEW_CUR_UUID,\t\t \n\tAL_SUSPENDED,\t\t \n\tAHEAD_TO_SYNC_SOURCE,    \n\tB_RS_H_DONE,\t\t \n\tDISCARD_MY_DATA,\t \n\tREAD_BALANCE_RR,\n\n\tFLUSH_PENDING,\t\t \n\n\t \n\tGOING_DISKLESS,\t\t \n\n\t \n\tGO_DISKLESS,\t\t \n\tDESTROY_DISK,\t\t \n\tMD_SYNC,\t\t \n\tRS_START,\t\t \n\tRS_PROGRESS,\t\t \n\tRS_DONE,\t\t \n};\n\nstruct drbd_bitmap;  \n\n \nenum bm_flag {\n\t \n\tBM_LOCKED_MASK = 0xf,\n\n\t \n\tBM_DONT_CLEAR = 0x1,\n\tBM_DONT_SET   = 0x2,\n\tBM_DONT_TEST  = 0x4,\n\n\t \n\tBM_IS_LOCKED  = 0x8,\n\n\t \n\tBM_LOCKED_TEST_ALLOWED = BM_DONT_CLEAR | BM_DONT_SET | BM_IS_LOCKED,\n\n\t \n\tBM_LOCKED_SET_ALLOWED = BM_DONT_CLEAR | BM_IS_LOCKED,\n\n\t \n\tBM_LOCKED_CHANGE_ALLOWED = BM_IS_LOCKED,\n};\n\nstruct drbd_work_queue {\n\tstruct list_head q;\n\tspinlock_t q_lock;   \n\twait_queue_head_t q_wait;\n};\n\nstruct drbd_socket {\n\tstruct mutex mutex;\n\tstruct socket    *socket;\n\t \n\tvoid *sbuf;\n\tvoid *rbuf;\n};\n\nstruct drbd_md {\n\tu64 md_offset;\t\t \n\n\tu64 la_size_sect;\t \n\tspinlock_t uuid_lock;\n\tu64 uuid[UI_SIZE];\n\tu64 device_uuid;\n\tu32 flags;\n\tu32 md_size_sect;\n\n\ts32 al_offset;\t \n\ts32 bm_offset;\t \n\n\t \n\ts32 meta_dev_idx;\n\n\t \n\tu32 al_stripes;\n\tu32 al_stripe_size_4k;\n\tu32 al_size_4k;  \n};\n\nstruct drbd_backing_dev {\n\tstruct block_device *backing_bdev;\n\tstruct block_device *md_bdev;\n\tstruct drbd_md md;\n\tstruct disk_conf *disk_conf;  \n\tsector_t known_size;  \n};\n\nstruct drbd_md_io {\n\tstruct page *page;\n\tunsigned long start_jif;\t \n\tunsigned long submit_jif;\t \n\tconst char *current_use;\n\tatomic_t in_use;\n\tunsigned int done;\n\tint error;\n};\n\nstruct bm_io_work {\n\tstruct drbd_work w;\n\tstruct drbd_peer_device *peer_device;\n\tchar *why;\n\tenum bm_flag flags;\n\tint (*io_fn)(struct drbd_device *device, struct drbd_peer_device *peer_device);\n\tvoid (*done)(struct drbd_device *device, int rv);\n};\n\nstruct fifo_buffer {\n\tunsigned int head_index;\n\tunsigned int size;\n\tint total;  \n\tint values[];\n};\nextern struct fifo_buffer *fifo_alloc(unsigned int fifo_size);\n\n \nenum {\n\tNET_CONGESTED,\t\t \n\tRESOLVE_CONFLICTS,\t \n\tSEND_PING,\n\tGOT_PING_ACK,\t\t \n\tCONN_WD_ST_CHG_REQ,\t \n\tCONN_WD_ST_CHG_OKAY,\n\tCONN_WD_ST_CHG_FAIL,\n\tCONN_DRY_RUN,\t\t \n\tCREATE_BARRIER,\t\t \n\tSTATE_SENT,\t\t \n\tCALLBACK_PENDING,\t \n\tDISCONNECT_SENT,\n\n\tDEVICE_WORK_PENDING,\t \n};\n\nenum which_state { NOW, OLD = NOW, NEW };\n\nstruct drbd_resource {\n\tchar *name;\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry *debugfs_res;\n\tstruct dentry *debugfs_res_volumes;\n\tstruct dentry *debugfs_res_connections;\n\tstruct dentry *debugfs_res_in_flight_summary;\n#endif\n\tstruct kref kref;\n\tstruct idr devices;\t\t \n\tstruct list_head connections;\n\tstruct list_head resources;\n\tstruct res_opts res_opts;\n\tstruct mutex conf_update;\t \n\tstruct mutex adm_mutex;\t\t \n\tspinlock_t req_lock;\n\n\tunsigned susp:1;\t\t \n\tunsigned susp_nod:1;\t\t \n\tunsigned susp_fen:1;\t\t \n\n\tenum write_ordering_e write_ordering;\n\n\tcpumask_var_t cpu_mask;\n};\n\nstruct drbd_thread_timing_details\n{\n\tunsigned long start_jif;\n\tvoid *cb_addr;\n\tconst char *caller_fn;\n\tunsigned int line;\n\tunsigned int cb_nr;\n};\n\nstruct drbd_connection {\n\tstruct list_head connections;\n\tstruct drbd_resource *resource;\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry *debugfs_conn;\n\tstruct dentry *debugfs_conn_callback_history;\n\tstruct dentry *debugfs_conn_oldest_requests;\n#endif\n\tstruct kref kref;\n\tstruct idr peer_devices;\t \n\tenum drbd_conns cstate;\t\t \n\tstruct mutex cstate_mutex;\t \n\tunsigned int connect_cnt;\t \n\n\tunsigned long flags;\n\tstruct net_conf *net_conf;\t \n\twait_queue_head_t ping_wait;\t \n\n\tstruct sockaddr_storage my_addr;\n\tint my_addr_len;\n\tstruct sockaddr_storage peer_addr;\n\tint peer_addr_len;\n\n\tstruct drbd_socket data;\t \n\tstruct drbd_socket meta;\t \n\tint agreed_pro_version;\t\t \n\tu32 agreed_features;\n\tunsigned long last_received;\t \n\tunsigned int ko_count;\n\n\tstruct list_head transfer_log;\t \n\n\tstruct crypto_shash *cram_hmac_tfm;\n\tstruct crypto_shash *integrity_tfm;   \n\tstruct crypto_shash *peer_integrity_tfm;   \n\tstruct crypto_shash *csums_tfm;\n\tstruct crypto_shash *verify_tfm;\n\tvoid *int_dig_in;\n\tvoid *int_dig_vv;\n\n\t \n\tstruct drbd_epoch *current_epoch;\n\tspinlock_t epoch_lock;\n\tunsigned int epochs;\n\tatomic_t current_tle_nr;\t \n\tunsigned current_tle_writes;\t \n\n\tunsigned long last_reconnect_jif;\n\t \n\tstruct blk_plug receiver_plug;\n\tstruct drbd_thread receiver;\n\tstruct drbd_thread worker;\n\tstruct drbd_thread ack_receiver;\n\tstruct workqueue_struct *ack_sender;\n\n\t \n\tstruct drbd_request *req_next;  \n\tstruct drbd_request *req_ack_pending;\n\tstruct drbd_request *req_not_net_done;\n\n\t \n\tstruct drbd_work_queue sender_work;\n\n#define DRBD_THREAD_DETAILS_HIST\t16\n\tunsigned int w_cb_nr;  \n\tunsigned int r_cb_nr;  \n\tstruct drbd_thread_timing_details w_timing_details[DRBD_THREAD_DETAILS_HIST];\n\tstruct drbd_thread_timing_details r_timing_details[DRBD_THREAD_DETAILS_HIST];\n\n\tstruct {\n\t\tunsigned long last_sent_barrier_jif;\n\n\t\t \n\t\tbool seen_any_write_yet;\n\n\t\t \n\t\tint current_epoch_nr;\n\n\t\t \n\t\tunsigned current_epoch_writes;\n\t} send;\n};\n\nstatic inline bool has_net_conf(struct drbd_connection *connection)\n{\n\tbool has_net_conf;\n\n\trcu_read_lock();\n\thas_net_conf = rcu_dereference(connection->net_conf);\n\trcu_read_unlock();\n\n\treturn has_net_conf;\n}\n\nvoid __update_timing_details(\n\t\tstruct drbd_thread_timing_details *tdp,\n\t\tunsigned int *cb_nr,\n\t\tvoid *cb,\n\t\tconst char *fn, const unsigned int line);\n\n#define update_worker_timing_details(c, cb) \\\n\t__update_timing_details(c->w_timing_details, &c->w_cb_nr, cb, __func__ , __LINE__ )\n#define update_receiver_timing_details(c, cb) \\\n\t__update_timing_details(c->r_timing_details, &c->r_cb_nr, cb, __func__ , __LINE__ )\n\nstruct submit_worker {\n\tstruct workqueue_struct *wq;\n\tstruct work_struct worker;\n\n\t \n\tstruct list_head writes;\n};\n\nstruct drbd_peer_device {\n\tstruct list_head peer_devices;\n\tstruct drbd_device *device;\n\tstruct drbd_connection *connection;\n\tstruct work_struct send_acks_work;\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry *debugfs_peer_dev;\n#endif\n};\n\nstruct drbd_device {\n\tstruct drbd_resource *resource;\n\tstruct list_head peer_devices;\n\tstruct list_head pending_bitmap_io;\n\n\tunsigned long flush_jif;\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry *debugfs_minor;\n\tstruct dentry *debugfs_vol;\n\tstruct dentry *debugfs_vol_oldest_requests;\n\tstruct dentry *debugfs_vol_act_log_extents;\n\tstruct dentry *debugfs_vol_resync_extents;\n\tstruct dentry *debugfs_vol_data_gen_id;\n\tstruct dentry *debugfs_vol_ed_gen_id;\n#endif\n\n\tunsigned int vnr;\t \n\tunsigned int minor;\t \n\n\tstruct kref kref;\n\n\t \n\tunsigned long flags;\n\n\t \n\tstruct drbd_backing_dev *ldev;\n\n\tsector_t p_size;      \n\tstruct request_queue *rq_queue;\n\tstruct gendisk\t    *vdisk;\n\n\tunsigned long last_reattach_jif;\n\tstruct drbd_work resync_work;\n\tstruct drbd_work unplug_work;\n\tstruct timer_list resync_timer;\n\tstruct timer_list md_sync_timer;\n\tstruct timer_list start_resync_timer;\n\tstruct timer_list request_timer;\n\n\t \n\tunion drbd_state new_state_tmp;\n\n\tunion drbd_dev_state state;\n\twait_queue_head_t misc_wait;\n\twait_queue_head_t state_wait;   \n\tunsigned int send_cnt;\n\tunsigned int recv_cnt;\n\tunsigned int read_cnt;\n\tunsigned int writ_cnt;\n\tunsigned int al_writ_cnt;\n\tunsigned int bm_writ_cnt;\n\tatomic_t ap_bio_cnt;\t  \n\tatomic_t ap_actlog_cnt;   \n\tatomic_t ap_pending_cnt;  \n\tatomic_t rs_pending_cnt;  \n\tatomic_t unacked_cnt;\t  \n\tatomic_t local_cnt;\t  \n\tatomic_t suspend_cnt;\n\n\t \n\tstruct rb_root read_requests;\n\tstruct rb_root write_requests;\n\n\t \n\t \n\tstruct list_head pending_master_completion[2];\n\tstruct list_head pending_completion[2];\n\n\t \n\tbool use_csums;\n\t \n\tunsigned long rs_total;\n\t \n\tunsigned long rs_failed;\n\t \n\tunsigned long rs_start;\n\t \n\tunsigned long rs_paused;\n\t \n\tunsigned long rs_same_csum;\n#define DRBD_SYNC_MARKS 8\n#define DRBD_SYNC_MARK_STEP (3*HZ)\n\t \n\tunsigned long rs_mark_left[DRBD_SYNC_MARKS];\n\t \n\tunsigned long rs_mark_time[DRBD_SYNC_MARKS];\n\t \n\tint rs_last_mark;\n\tunsigned long rs_last_bcast;  \n\n\t \n\tsector_t ov_start_sector;\n\tsector_t ov_stop_sector;\n\t \n\tsector_t ov_position;\n\t \n\tsector_t ov_last_oos_start;\n\t \n\tsector_t ov_last_oos_size;\n\tunsigned long ov_left;  \n\n\tstruct drbd_bitmap *bitmap;\n\tunsigned long bm_resync_fo;  \n\n\t \n\tstruct lru_cache *resync;\n\t \n\tunsigned int resync_locked;\n\t \n\tunsigned int resync_wenr;\n\n\tint open_cnt;\n\tu64 *p_uuid;\n\n\tstruct list_head active_ee;  \n\tstruct list_head sync_ee;    \n\tstruct list_head done_ee;    \n\tstruct list_head read_ee;    \n\tstruct list_head net_ee;     \n\n\tint next_barrier_nr;\n\tstruct list_head resync_reads;\n\tatomic_t pp_in_use;\t\t \n\tatomic_t pp_in_use_by_net;\t \n\twait_queue_head_t ee_wait;\n\tstruct drbd_md_io md_io;\n\tspinlock_t al_lock;\n\twait_queue_head_t al_wait;\n\tstruct lru_cache *act_log;\t \n\tunsigned int al_tr_number;\n\tint al_tr_cycle;\n\twait_queue_head_t seq_wait;\n\tatomic_t packet_seq;\n\tunsigned int peer_seq;\n\tspinlock_t peer_seq_lock;\n\tunsigned long comm_bm_set;  \n\tstruct bm_io_work bm_io_work;\n\tu64 ed_uuid;  \n\tstruct mutex own_state_mutex;\n\tstruct mutex *state_mutex;  \n\tchar congestion_reason;   \n\tatomic_t rs_sect_in;  \n\tatomic_t rs_sect_ev;  \n\tint rs_last_sect_ev;  \n\tint rs_last_events;   \n\tint c_sync_rate;  \n\tstruct fifo_buffer *rs_plan_s;  \n\tint rs_in_flight;  \n\tatomic_t ap_in_flight;  \n\tunsigned int peer_max_bio_size;\n\tunsigned int local_max_bio_size;\n\n\t \n\tstruct submit_worker submit;\n};\n\nstruct drbd_bm_aio_ctx {\n\tstruct drbd_device *device;\n\tstruct list_head list;  ;\n\tunsigned long start_jif;\n\tatomic_t in_flight;\n\tunsigned int done;\n\tunsigned flags;\n#define BM_AIO_COPY_PAGES\t1\n#define BM_AIO_WRITE_HINTED\t2\n#define BM_AIO_WRITE_ALL_PAGES\t4\n#define BM_AIO_READ\t\t8\n\tint error;\n\tstruct kref kref;\n};\n\nstruct drbd_config_context {\n\t \n\tunsigned int minor;\n\t \n\tunsigned int volume;\n#define VOLUME_UNSPECIFIED\t\t(-1U)\n\t \n\tchar *resource_name;\n\tstruct nlattr *my_addr;\n\tstruct nlattr *peer_addr;\n\n\t \n\tstruct sk_buff *reply_skb;\n\t \n\tstruct drbd_genlmsghdr *reply_dh;\n\t \n\tstruct drbd_device *device;\n\tstruct drbd_resource *resource;\n\tstruct drbd_connection *connection;\n};\n\nstatic inline struct drbd_device *minor_to_device(unsigned int minor)\n{\n\treturn (struct drbd_device *)idr_find(&drbd_devices, minor);\n}\n\nstatic inline struct drbd_peer_device *first_peer_device(struct drbd_device *device)\n{\n\treturn list_first_entry_or_null(&device->peer_devices, struct drbd_peer_device, peer_devices);\n}\n\nstatic inline struct drbd_peer_device *\nconn_peer_device(struct drbd_connection *connection, int volume_number)\n{\n\treturn idr_find(&connection->peer_devices, volume_number);\n}\n\n#define for_each_resource(resource, _resources) \\\n\tlist_for_each_entry(resource, _resources, resources)\n\n#define for_each_resource_rcu(resource, _resources) \\\n\tlist_for_each_entry_rcu(resource, _resources, resources)\n\n#define for_each_resource_safe(resource, tmp, _resources) \\\n\tlist_for_each_entry_safe(resource, tmp, _resources, resources)\n\n#define for_each_connection(connection, resource) \\\n\tlist_for_each_entry(connection, &resource->connections, connections)\n\n#define for_each_connection_rcu(connection, resource) \\\n\tlist_for_each_entry_rcu(connection, &resource->connections, connections)\n\n#define for_each_connection_safe(connection, tmp, resource) \\\n\tlist_for_each_entry_safe(connection, tmp, &resource->connections, connections)\n\n#define for_each_peer_device(peer_device, device) \\\n\tlist_for_each_entry(peer_device, &device->peer_devices, peer_devices)\n\n#define for_each_peer_device_rcu(peer_device, device) \\\n\tlist_for_each_entry_rcu(peer_device, &device->peer_devices, peer_devices)\n\n#define for_each_peer_device_safe(peer_device, tmp, device) \\\n\tlist_for_each_entry_safe(peer_device, tmp, &device->peer_devices, peer_devices)\n\nstatic inline unsigned int device_to_minor(struct drbd_device *device)\n{\n\treturn device->minor;\n}\n\n \n\n \n\nenum dds_flags {\n\tDDSF_FORCED    = 1,\n\tDDSF_NO_RESYNC = 2,  \n};\n\nextern void drbd_init_set_defaults(struct drbd_device *device);\nextern int  drbd_thread_start(struct drbd_thread *thi);\nextern void _drbd_thread_stop(struct drbd_thread *thi, int restart, int wait);\n#ifdef CONFIG_SMP\nextern void drbd_thread_current_set_cpu(struct drbd_thread *thi);\n#else\n#define drbd_thread_current_set_cpu(A) ({})\n#endif\nextern void tl_release(struct drbd_connection *, unsigned int barrier_nr,\n\t\t       unsigned int set_size);\nextern void tl_clear(struct drbd_connection *);\nextern void drbd_free_sock(struct drbd_connection *connection);\nextern int drbd_send(struct drbd_connection *connection, struct socket *sock,\n\t\t     void *buf, size_t size, unsigned msg_flags);\nextern int drbd_send_all(struct drbd_connection *, struct socket *, void *, size_t,\n\t\t\t unsigned);\n\nextern int __drbd_send_protocol(struct drbd_connection *connection, enum drbd_packet cmd);\nextern int drbd_send_protocol(struct drbd_connection *connection);\nextern int drbd_send_uuids(struct drbd_peer_device *);\nextern int drbd_send_uuids_skip_initial_sync(struct drbd_peer_device *);\nextern void drbd_gen_and_send_sync_uuid(struct drbd_peer_device *);\nextern int drbd_send_sizes(struct drbd_peer_device *, int trigger_reply, enum dds_flags flags);\nextern int drbd_send_state(struct drbd_peer_device *, union drbd_state s);\nextern int drbd_send_current_state(struct drbd_peer_device *);\nextern int drbd_send_sync_param(struct drbd_peer_device *);\nextern void drbd_send_b_ack(struct drbd_connection *connection, u32 barrier_nr,\n\t\t\t    u32 set_size);\nextern int drbd_send_ack(struct drbd_peer_device *, enum drbd_packet,\n\t\t\t struct drbd_peer_request *);\nextern void drbd_send_ack_rp(struct drbd_peer_device *, enum drbd_packet,\n\t\t\t     struct p_block_req *rp);\nextern void drbd_send_ack_dp(struct drbd_peer_device *, enum drbd_packet,\n\t\t\t     struct p_data *dp, int data_size);\nextern int drbd_send_ack_ex(struct drbd_peer_device *, enum drbd_packet,\n\t\t\t    sector_t sector, int blksize, u64 block_id);\nextern int drbd_send_out_of_sync(struct drbd_peer_device *, struct drbd_request *);\nextern int drbd_send_block(struct drbd_peer_device *, enum drbd_packet,\n\t\t\t   struct drbd_peer_request *);\nextern int drbd_send_dblock(struct drbd_peer_device *, struct drbd_request *req);\nextern int drbd_send_drequest(struct drbd_peer_device *, int cmd,\n\t\t\t      sector_t sector, int size, u64 block_id);\nextern int drbd_send_drequest_csum(struct drbd_peer_device *, sector_t sector,\n\t\t\t\t   int size, void *digest, int digest_size,\n\t\t\t\t   enum drbd_packet cmd);\nextern int drbd_send_ov_request(struct drbd_peer_device *, sector_t sector, int size);\n\nextern int drbd_send_bitmap(struct drbd_device *device, struct drbd_peer_device *peer_device);\nextern void drbd_send_sr_reply(struct drbd_peer_device *, enum drbd_state_rv retcode);\nextern void conn_send_sr_reply(struct drbd_connection *connection, enum drbd_state_rv retcode);\nextern int drbd_send_rs_deallocated(struct drbd_peer_device *, struct drbd_peer_request *);\nextern void drbd_backing_dev_free(struct drbd_device *device, struct drbd_backing_dev *ldev);\nextern void drbd_device_cleanup(struct drbd_device *device);\nextern void drbd_print_uuids(struct drbd_device *device, const char *text);\nextern void drbd_queue_unplug(struct drbd_device *device);\n\nextern void conn_md_sync(struct drbd_connection *connection);\nextern void drbd_md_write(struct drbd_device *device, void *buffer);\nextern void drbd_md_sync(struct drbd_device *device);\nextern int  drbd_md_read(struct drbd_device *device, struct drbd_backing_dev *bdev);\nextern void drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local);\nextern void _drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local);\nextern void drbd_uuid_new_current(struct drbd_device *device) __must_hold(local);\nextern void drbd_uuid_set_bm(struct drbd_device *device, u64 val) __must_hold(local);\nextern void drbd_uuid_move_history(struct drbd_device *device) __must_hold(local);\nextern void __drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local);\nextern void drbd_md_set_flag(struct drbd_device *device, int flags) __must_hold(local);\nextern void drbd_md_clear_flag(struct drbd_device *device, int flags)__must_hold(local);\nextern int drbd_md_test_flag(struct drbd_backing_dev *, int);\nextern void drbd_md_mark_dirty(struct drbd_device *device);\nextern void drbd_queue_bitmap_io(struct drbd_device *device,\n\t\t\t\t int (*io_fn)(struct drbd_device *, struct drbd_peer_device *),\n\t\t\t\t void (*done)(struct drbd_device *, int),\n\t\t\t\t char *why, enum bm_flag flags,\n\t\t\t\t struct drbd_peer_device *peer_device);\nextern int drbd_bitmap_io(struct drbd_device *device,\n\t\tint (*io_fn)(struct drbd_device *, struct drbd_peer_device *),\n\t\tchar *why, enum bm_flag flags,\n\t\tstruct drbd_peer_device *peer_device);\nextern int drbd_bitmap_io_from_worker(struct drbd_device *device,\n\t\tint (*io_fn)(struct drbd_device *, struct drbd_peer_device *),\n\t\tchar *why, enum bm_flag flags,\n\t\tstruct drbd_peer_device *peer_device);\nextern int drbd_bmio_set_n_write(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\nextern int drbd_bmio_clear_n_write(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\n\n \n\n \n#define MD_128MB_SECT (128LLU << 11)   \n#define MD_4kB_SECT\t 8\n#define MD_32kB_SECT\t64\n\n \n#define AL_EXTENT_SHIFT 22\n#define AL_EXTENT_SIZE (1<<AL_EXTENT_SHIFT)\n\n \n#define AL_UPDATES_PER_TRANSACTION\t 64\t\n#define AL_CONTEXT_PER_TRANSACTION\t919\t\n\n#if BITS_PER_LONG == 32\n#define LN2_BPL 5\n#define cpu_to_lel(A) cpu_to_le32(A)\n#define lel_to_cpu(A) le32_to_cpu(A)\n#elif BITS_PER_LONG == 64\n#define LN2_BPL 6\n#define cpu_to_lel(A) cpu_to_le64(A)\n#define lel_to_cpu(A) le64_to_cpu(A)\n#else\n#error \"LN2 of BITS_PER_LONG unknown!\"\n#endif\n\n \n \nstruct bm_extent {\n\tint rs_left;  \n\tint rs_failed;  \n\tunsigned long flags;\n\tstruct lc_element lce;\n};\n\n#define BME_NO_WRITES  0   \n#define BME_LOCKED     1   \n#define BME_PRIORITY   2   \n\n \n \n\n#define SLEEP_TIME (HZ/10)\n\n \n#define BM_BLOCK_SHIFT\t12\t\t\t  \n#define BM_BLOCK_SIZE\t (1<<BM_BLOCK_SHIFT)\n \n#define BM_EXT_SHIFT\t 24\t \n#define BM_EXT_SIZE\t (1<<BM_EXT_SHIFT)\n\n#if (BM_EXT_SHIFT != 24) || (BM_BLOCK_SHIFT != 12)\n#error \"HAVE YOU FIXED drbdmeta AS WELL??\"\n#endif\n\n \n#define BM_SECT_TO_BIT(x)   ((x)>>(BM_BLOCK_SHIFT-9))\n#define BM_BIT_TO_SECT(x)   ((sector_t)(x)<<(BM_BLOCK_SHIFT-9))\n#define BM_SECT_PER_BIT     BM_BIT_TO_SECT(1)\n\n \n#define Bit2KB(bits) ((bits)<<(BM_BLOCK_SHIFT-10))\n\n \n#define BM_SECT_TO_EXT(x)   ((x)>>(BM_EXT_SHIFT-9))\n#define BM_BIT_TO_EXT(x)    ((x) >> (BM_EXT_SHIFT - BM_BLOCK_SHIFT))\n\n \n#define BM_EXT_TO_SECT(x)   ((sector_t)(x) << (BM_EXT_SHIFT-9))\n \n#define BM_SECT_PER_EXT     BM_EXT_TO_SECT(1)\n \n#define BM_BITS_PER_EXT     (1UL << (BM_EXT_SHIFT - BM_BLOCK_SHIFT))\n\n#define BM_BLOCKS_PER_BM_EXT_MASK  (BM_BITS_PER_EXT - 1)\n\n\n \n#define AL_EXT_PER_BM_SECT  (1 << (BM_EXT_SHIFT - AL_EXTENT_SHIFT))\n\n \n\n#define DRBD_MAX_SECTORS_32 (0xffffffffLU)\n \n\n#define DRBD_MAX_SECTORS_FIXED_BM \\\n\t  ((MD_128MB_SECT - MD_32kB_SECT - MD_4kB_SECT) * (1LL<<(BM_EXT_SHIFT-9)))\n#define DRBD_MAX_SECTORS      DRBD_MAX_SECTORS_FIXED_BM\n \n#if BITS_PER_LONG == 32\n \n#define DRBD_MAX_SECTORS_FLEX BM_BIT_TO_SECT(0xffff7fff)\n#else\n \n#define DRBD_MAX_SECTORS_FLEX (1UL << 51)\n \n#endif\n\n \n#define DRBD_MAX_BIO_SIZE (1U << 20)\n#if DRBD_MAX_BIO_SIZE > (BIO_MAX_VECS << PAGE_SHIFT)\n#error Architecture not supported: DRBD_MAX_BIO_SIZE > BIO_MAX_SIZE\n#endif\n#define DRBD_MAX_BIO_SIZE_SAFE (1U << 12)        \n\n#define DRBD_MAX_SIZE_H80_PACKET (1U << 15)  \n#define DRBD_MAX_BIO_SIZE_P95    (1U << 17)  \n\n \n#define DRBD_MAX_BATCH_BIO_SIZE\t (AL_UPDATES_PER_TRANSACTION/2*AL_EXTENT_SIZE)\n#define DRBD_MAX_BBIO_SECTORS    (DRBD_MAX_BATCH_BIO_SIZE >> 9)\n\nextern int  drbd_bm_init(struct drbd_device *device);\nextern int  drbd_bm_resize(struct drbd_device *device, sector_t sectors, int set_new_bits);\nextern void drbd_bm_cleanup(struct drbd_device *device);\nextern void drbd_bm_set_all(struct drbd_device *device);\nextern void drbd_bm_clear_all(struct drbd_device *device);\n \nextern int  drbd_bm_set_bits(\n\t\tstruct drbd_device *device, unsigned long s, unsigned long e);\nextern int  drbd_bm_clear_bits(\n\t\tstruct drbd_device *device, unsigned long s, unsigned long e);\nextern int drbd_bm_count_bits(\n\tstruct drbd_device *device, const unsigned long s, const unsigned long e);\n \nextern void _drbd_bm_set_bits(struct drbd_device *device,\n\t\tconst unsigned long s, const unsigned long e);\nextern int  drbd_bm_test_bit(struct drbd_device *device, unsigned long bitnr);\nextern int  drbd_bm_e_weight(struct drbd_device *device, unsigned long enr);\nextern int  drbd_bm_read(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\nextern void drbd_bm_mark_for_writeout(struct drbd_device *device, int page_nr);\nextern int  drbd_bm_write(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\nextern void drbd_bm_reset_al_hints(struct drbd_device *device) __must_hold(local);\nextern int  drbd_bm_write_hinted(struct drbd_device *device) __must_hold(local);\nextern int  drbd_bm_write_lazy(struct drbd_device *device, unsigned upper_idx) __must_hold(local);\nextern int drbd_bm_write_all(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\nextern int  drbd_bm_write_copy_pages(struct drbd_device *device,\n\t\tstruct drbd_peer_device *peer_device) __must_hold(local);\nextern size_t\t     drbd_bm_words(struct drbd_device *device);\nextern unsigned long drbd_bm_bits(struct drbd_device *device);\nextern sector_t      drbd_bm_capacity(struct drbd_device *device);\n\n#define DRBD_END_OF_BITMAP\t(~(unsigned long)0)\nextern unsigned long drbd_bm_find_next(struct drbd_device *device, unsigned long bm_fo);\n \nextern unsigned long _drbd_bm_find_next(struct drbd_device *device, unsigned long bm_fo);\nextern unsigned long _drbd_bm_find_next_zero(struct drbd_device *device, unsigned long bm_fo);\nextern unsigned long _drbd_bm_total_weight(struct drbd_device *device);\nextern unsigned long drbd_bm_total_weight(struct drbd_device *device);\n \nextern void drbd_bm_merge_lel(struct drbd_device *device, size_t offset,\n\t\tsize_t number, unsigned long *buffer);\n \nextern void drbd_bm_get_lel(struct drbd_device *device, size_t offset,\n\t\tsize_t number, unsigned long *buffer);\n\nextern void drbd_bm_lock(struct drbd_device *device, char *why, enum bm_flag flags);\nextern void drbd_bm_unlock(struct drbd_device *device);\n \n\nextern struct kmem_cache *drbd_request_cache;\nextern struct kmem_cache *drbd_ee_cache;\t \nextern struct kmem_cache *drbd_bm_ext_cache;\t \nextern struct kmem_cache *drbd_al_ext_cache;\t \nextern mempool_t drbd_request_mempool;\nextern mempool_t drbd_ee_mempool;\n\n \nextern struct page *drbd_pp_pool;\nextern spinlock_t   drbd_pp_lock;\nextern int\t    drbd_pp_vacant;\nextern wait_queue_head_t drbd_pp_wait;\n\n \n#define DRBD_MIN_POOL_PAGES\t128\nextern mempool_t drbd_md_io_page_pool;\n\n \nextern struct bio_set drbd_md_io_bio_set;\n\n \nextern struct bio_set drbd_io_bio_set;\n\nextern struct mutex resources_mutex;\n\nextern int conn_lowest_minor(struct drbd_connection *connection);\nextern enum drbd_ret_code drbd_create_device(struct drbd_config_context *adm_ctx, unsigned int minor);\nextern void drbd_destroy_device(struct kref *kref);\nextern void drbd_delete_device(struct drbd_device *device);\n\nextern struct drbd_resource *drbd_create_resource(const char *name);\nextern void drbd_free_resource(struct drbd_resource *resource);\n\nextern int set_resource_options(struct drbd_resource *resource, struct res_opts *res_opts);\nextern struct drbd_connection *conn_create(const char *name, struct res_opts *res_opts);\nextern void drbd_destroy_connection(struct kref *kref);\nextern struct drbd_connection *conn_get_by_addrs(void *my_addr, int my_addr_len,\n\t\t\t\t\t    void *peer_addr, int peer_addr_len);\nextern struct drbd_resource *drbd_find_resource(const char *name);\nextern void drbd_destroy_resource(struct kref *kref);\nextern void conn_free_crypto(struct drbd_connection *connection);\n\n \nextern void do_submit(struct work_struct *ws);\nextern void __drbd_make_request(struct drbd_device *, struct bio *);\nvoid drbd_submit_bio(struct bio *bio);\nextern int drbd_read_remote(struct drbd_device *device, struct drbd_request *req);\nextern int is_valid_ar_handle(struct drbd_request *, sector_t);\n\n\n \n\nextern struct mutex notification_mutex;\n\nextern void drbd_suspend_io(struct drbd_device *device);\nextern void drbd_resume_io(struct drbd_device *device);\nextern char *ppsize(char *buf, unsigned long long size);\nextern sector_t drbd_new_dev_size(struct drbd_device *, struct drbd_backing_dev *, sector_t, int);\nenum determine_dev_size {\n\tDS_ERROR_SHRINK = -3,\n\tDS_ERROR_SPACE_MD = -2,\n\tDS_ERROR = -1,\n\tDS_UNCHANGED = 0,\n\tDS_SHRUNK = 1,\n\tDS_GREW = 2,\n\tDS_GREW_FROM_ZERO = 3,\n};\nextern enum determine_dev_size\ndrbd_determine_dev_size(struct drbd_device *, enum dds_flags, struct resize_parms *) __must_hold(local);\nextern void resync_after_online_grow(struct drbd_device *);\nextern void drbd_reconsider_queue_parameters(struct drbd_device *device,\n\t\t\tstruct drbd_backing_dev *bdev, struct o_qlim *o);\nextern enum drbd_state_rv drbd_set_role(struct drbd_device *device,\n\t\t\t\t\tenum drbd_role new_role,\n\t\t\t\t\tint force);\nextern bool conn_try_outdate_peer(struct drbd_connection *connection);\nextern void conn_try_outdate_peer_async(struct drbd_connection *connection);\nextern enum drbd_peer_state conn_khelper(struct drbd_connection *connection, char *cmd);\nextern int drbd_khelper(struct drbd_device *device, char *cmd);\n\n \n \nextern void drbd_md_endio(struct bio *bio);\nextern void drbd_peer_request_endio(struct bio *bio);\nextern void drbd_request_endio(struct bio *bio);\nextern int drbd_worker(struct drbd_thread *thi);\nenum drbd_ret_code drbd_resync_after_valid(struct drbd_device *device, int o_minor);\nvoid drbd_resync_after_changed(struct drbd_device *device);\nextern void drbd_start_resync(struct drbd_device *device, enum drbd_conns side);\nextern void resume_next_sg(struct drbd_device *device);\nextern void suspend_other_sg(struct drbd_device *device);\nextern int drbd_resync_finished(struct drbd_peer_device *peer_device);\n \nextern void *drbd_md_get_buffer(struct drbd_device *device, const char *intent);\nextern void drbd_md_put_buffer(struct drbd_device *device);\nextern int drbd_md_sync_page_io(struct drbd_device *device,\n\t\tstruct drbd_backing_dev *bdev, sector_t sector, enum req_op op);\nextern void drbd_ov_out_of_sync_found(struct drbd_peer_device *peer_device,\n\t\tsector_t sector, int size);\nextern void wait_until_done_or_force_detached(struct drbd_device *device,\n\t\tstruct drbd_backing_dev *bdev, unsigned int *done);\nextern void drbd_rs_controller_reset(struct drbd_peer_device *peer_device);\n\nstatic inline void ov_out_of_sync_print(struct drbd_peer_device *peer_device)\n{\n\tstruct drbd_device *device = peer_device->device;\n\n\tif (device->ov_last_oos_size) {\n\t\tdrbd_err(peer_device, \"Out of sync: start=%llu, size=%lu (sectors)\\n\",\n\t\t     (unsigned long long)device->ov_last_oos_start,\n\t\t     (unsigned long)device->ov_last_oos_size);\n\t}\n\tdevice->ov_last_oos_size = 0;\n}\n\n\nextern void drbd_csum_bio(struct crypto_shash *, struct bio *, void *);\nextern void drbd_csum_ee(struct crypto_shash *, struct drbd_peer_request *,\n\t\t\t void *);\n \nextern int w_e_end_data_req(struct drbd_work *, int);\nextern int w_e_end_rsdata_req(struct drbd_work *, int);\nextern int w_e_end_csum_rs_req(struct drbd_work *, int);\nextern int w_e_end_ov_reply(struct drbd_work *, int);\nextern int w_e_end_ov_req(struct drbd_work *, int);\nextern int w_ov_finished(struct drbd_work *, int);\nextern int w_resync_timer(struct drbd_work *, int);\nextern int w_send_write_hint(struct drbd_work *, int);\nextern int w_send_dblock(struct drbd_work *, int);\nextern int w_send_read_req(struct drbd_work *, int);\nextern int w_e_reissue(struct drbd_work *, int);\nextern int w_restart_disk_io(struct drbd_work *, int);\nextern int w_send_out_of_sync(struct drbd_work *, int);\n\nextern void resync_timer_fn(struct timer_list *t);\nextern void start_resync_timer_fn(struct timer_list *t);\n\nextern void drbd_endio_write_sec_final(struct drbd_peer_request *peer_req);\n\n \nextern int drbd_issue_discard_or_zero_out(struct drbd_device *device,\n\t\tsector_t start, unsigned int nr_sectors, int flags);\nextern int drbd_receiver(struct drbd_thread *thi);\nextern int drbd_ack_receiver(struct drbd_thread *thi);\nextern void drbd_send_ping_wf(struct work_struct *ws);\nextern void drbd_send_acks_wf(struct work_struct *ws);\nextern bool drbd_rs_c_min_rate_throttle(struct drbd_device *device);\nextern bool drbd_rs_should_slow_down(struct drbd_peer_device *peer_device, sector_t sector,\n\t\tbool throttle_if_app_is_waiting);\nextern int drbd_submit_peer_request(struct drbd_peer_request *peer_req);\nextern int drbd_free_peer_reqs(struct drbd_device *, struct list_head *);\nextern struct drbd_peer_request *drbd_alloc_peer_req(struct drbd_peer_device *, u64,\n\t\t\t\t\t\t     sector_t, unsigned int,\n\t\t\t\t\t\t     unsigned int,\n\t\t\t\t\t\t     gfp_t) __must_hold(local);\nextern void __drbd_free_peer_req(struct drbd_device *, struct drbd_peer_request *,\n\t\t\t\t int);\n#define drbd_free_peer_req(m,e) __drbd_free_peer_req(m, e, 0)\n#define drbd_free_net_peer_req(m,e) __drbd_free_peer_req(m, e, 1)\nextern struct page *drbd_alloc_pages(struct drbd_peer_device *, unsigned int, bool);\nextern void drbd_set_recv_tcq(struct drbd_device *device, int tcq_enabled);\nextern void _drbd_clear_done_ee(struct drbd_device *device, struct list_head *to_be_freed);\nextern int drbd_connected(struct drbd_peer_device *);\n\n \nvoid drbd_set_my_capacity(struct drbd_device *device, sector_t size);\n\n \nstatic inline void drbd_submit_bio_noacct(struct drbd_device *device,\n\t\t\t\t\t     int fault_type, struct bio *bio)\n{\n\t__release(local);\n\tif (!bio->bi_bdev) {\n\t\tdrbd_err(device, \"drbd_submit_bio_noacct: bio->bi_bdev == NULL\\n\");\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tif (drbd_insert_fault(device, fault_type))\n\t\tbio_io_error(bio);\n\telse\n\t\tsubmit_bio_noacct(bio);\n}\n\nvoid drbd_bump_write_ordering(struct drbd_resource *resource, struct drbd_backing_dev *bdev,\n\t\t\t      enum write_ordering_e wo);\n\n \nextern struct proc_dir_entry *drbd_proc;\nint drbd_seq_show(struct seq_file *seq, void *v);\n\n \nextern bool drbd_al_begin_io_prepare(struct drbd_device *device, struct drbd_interval *i);\nextern int drbd_al_begin_io_nonblock(struct drbd_device *device, struct drbd_interval *i);\nextern void drbd_al_begin_io_commit(struct drbd_device *device);\nextern bool drbd_al_begin_io_fastpath(struct drbd_device *device, struct drbd_interval *i);\nextern void drbd_al_begin_io(struct drbd_device *device, struct drbd_interval *i);\nextern void drbd_al_complete_io(struct drbd_device *device, struct drbd_interval *i);\nextern void drbd_rs_complete_io(struct drbd_device *device, sector_t sector);\nextern int drbd_rs_begin_io(struct drbd_device *device, sector_t sector);\nextern int drbd_try_rs_begin_io(struct drbd_peer_device *peer_device, sector_t sector);\nextern void drbd_rs_cancel_all(struct drbd_device *device);\nextern int drbd_rs_del_all(struct drbd_device *device);\nextern void drbd_rs_failed_io(struct drbd_peer_device *peer_device,\n\t\tsector_t sector, int size);\nextern void drbd_advance_rs_marks(struct drbd_peer_device *peer_device, unsigned long still_to_go);\n\nenum update_sync_bits_mode { RECORD_RS_FAILED, SET_OUT_OF_SYNC, SET_IN_SYNC };\nextern int __drbd_change_sync(struct drbd_peer_device *peer_device, sector_t sector, int size,\n\t\tenum update_sync_bits_mode mode);\n#define drbd_set_in_sync(peer_device, sector, size) \\\n\t__drbd_change_sync(peer_device, sector, size, SET_IN_SYNC)\n#define drbd_set_out_of_sync(peer_device, sector, size) \\\n\t__drbd_change_sync(peer_device, sector, size, SET_OUT_OF_SYNC)\n#define drbd_rs_failed_io(peer_device, sector, size) \\\n\t__drbd_change_sync(peer_device, sector, size, RECORD_RS_FAILED)\nextern void drbd_al_shrink(struct drbd_device *device);\nextern int drbd_al_initialize(struct drbd_device *, void *);\n\n \n \nstruct sib_info {\n\tenum drbd_state_info_bcast_reason sib_reason;\n\tunion {\n\t\tstruct {\n\t\t\tchar *helper_name;\n\t\t\tunsigned helper_exit_code;\n\t\t};\n\t\tstruct {\n\t\t\tunion drbd_state os;\n\t\t\tunion drbd_state ns;\n\t\t};\n\t};\n};\nvoid drbd_bcast_event(struct drbd_device *device, const struct sib_info *sib);\n\nextern int notify_resource_state(struct sk_buff *,\n\t\t\t\t  unsigned int,\n\t\t\t\t  struct drbd_resource *,\n\t\t\t\t  struct resource_info *,\n\t\t\t\t  enum drbd_notification_type);\nextern int notify_device_state(struct sk_buff *,\n\t\t\t\tunsigned int,\n\t\t\t\tstruct drbd_device *,\n\t\t\t\tstruct device_info *,\n\t\t\t\tenum drbd_notification_type);\nextern int notify_connection_state(struct sk_buff *,\n\t\t\t\t    unsigned int,\n\t\t\t\t    struct drbd_connection *,\n\t\t\t\t    struct connection_info *,\n\t\t\t\t    enum drbd_notification_type);\nextern int notify_peer_device_state(struct sk_buff *,\n\t\t\t\t     unsigned int,\n\t\t\t\t     struct drbd_peer_device *,\n\t\t\t\t     struct peer_device_info *,\n\t\t\t\t     enum drbd_notification_type);\nextern void notify_helper(enum drbd_notification_type, struct drbd_device *,\n\t\t\t  struct drbd_connection *, const char *, int);\n\n \n\n \nstatic inline struct page *page_chain_next(struct page *page)\n{\n\treturn (struct page *)page_private(page);\n}\n#define page_chain_for_each(page) \\\n\tfor (; page && ({ prefetch(page_chain_next(page)); 1; }); \\\n\t\t\tpage = page_chain_next(page))\n#define page_chain_for_each_safe(page, n) \\\n\tfor (; page && ({ n = page_chain_next(page); 1; }); page = n)\n\n\nstatic inline int drbd_peer_req_has_active_page(struct drbd_peer_request *peer_req)\n{\n\tstruct page *page = peer_req->pages;\n\tpage_chain_for_each(page) {\n\t\tif (page_count(page) > 1)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline union drbd_state drbd_read_state(struct drbd_device *device)\n{\n\tstruct drbd_resource *resource = device->resource;\n\tunion drbd_state rv;\n\n\trv.i = device->state.i;\n\trv.susp = resource->susp;\n\trv.susp_nod = resource->susp_nod;\n\trv.susp_fen = resource->susp_fen;\n\n\treturn rv;\n}\n\nenum drbd_force_detach_flags {\n\tDRBD_READ_ERROR,\n\tDRBD_WRITE_ERROR,\n\tDRBD_META_IO_ERROR,\n\tDRBD_FORCE_DETACH,\n};\n\n#define __drbd_chk_io_error(m,f) __drbd_chk_io_error_(m,f, __func__)\nstatic inline void __drbd_chk_io_error_(struct drbd_device *device,\n\t\tenum drbd_force_detach_flags df,\n\t\tconst char *where)\n{\n\tenum drbd_io_error_p ep;\n\n\trcu_read_lock();\n\tep = rcu_dereference(device->ldev->disk_conf)->on_io_error;\n\trcu_read_unlock();\n\tswitch (ep) {\n\tcase EP_PASS_ON:  \n\t\tif (df == DRBD_READ_ERROR || df == DRBD_WRITE_ERROR) {\n\t\t\tif (drbd_ratelimit())\n\t\t\t\tdrbd_err(device, \"Local IO failed in %s.\\n\", where);\n\t\t\tif (device->state.disk > D_INCONSISTENT)\n\t\t\t\t_drbd_set_state(_NS(device, disk, D_INCONSISTENT), CS_HARD, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\t \n\tcase EP_DETACH:\n\tcase EP_CALL_HELPER:\n\t\t \n\t\tset_bit(WAS_IO_ERROR, &device->flags);\n\t\tif (df == DRBD_READ_ERROR)\n\t\t\tset_bit(WAS_READ_ERROR, &device->flags);\n\t\tif (df == DRBD_FORCE_DETACH)\n\t\t\tset_bit(FORCE_DETACH, &device->flags);\n\t\tif (device->state.disk > D_FAILED) {\n\t\t\t_drbd_set_state(_NS(device, disk, D_FAILED), CS_HARD, NULL);\n\t\t\tdrbd_err(device,\n\t\t\t\t\"Local IO failed in %s. Detaching...\\n\", where);\n\t\t}\n\t\tbreak;\n\t}\n}\n\n \n#define drbd_chk_io_error(m,e,f) drbd_chk_io_error_(m,e,f, __func__)\nstatic inline void drbd_chk_io_error_(struct drbd_device *device,\n\tint error, enum drbd_force_detach_flags forcedetach, const char *where)\n{\n\tif (error) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&device->resource->req_lock, flags);\n\t\t__drbd_chk_io_error_(device, forcedetach, where);\n\t\tspin_unlock_irqrestore(&device->resource->req_lock, flags);\n\t}\n}\n\n\n \nstatic inline sector_t drbd_md_first_sector(struct drbd_backing_dev *bdev)\n{\n\tswitch (bdev->md.meta_dev_idx) {\n\tcase DRBD_MD_INDEX_INTERNAL:\n\tcase DRBD_MD_INDEX_FLEX_INT:\n\t\treturn bdev->md.md_offset + bdev->md.bm_offset;\n\tcase DRBD_MD_INDEX_FLEX_EXT:\n\tdefault:\n\t\treturn bdev->md.md_offset;\n\t}\n}\n\n \nstatic inline sector_t drbd_md_last_sector(struct drbd_backing_dev *bdev)\n{\n\tswitch (bdev->md.meta_dev_idx) {\n\tcase DRBD_MD_INDEX_INTERNAL:\n\tcase DRBD_MD_INDEX_FLEX_INT:\n\t\treturn bdev->md.md_offset + MD_4kB_SECT -1;\n\tcase DRBD_MD_INDEX_FLEX_EXT:\n\tdefault:\n\t\treturn bdev->md.md_offset + bdev->md.md_size_sect -1;\n\t}\n}\n\n \nstatic inline sector_t drbd_get_capacity(struct block_device *bdev)\n{\n\treturn bdev ? bdev_nr_sectors(bdev) : 0;\n}\n\n \nstatic inline sector_t drbd_get_max_capacity(struct drbd_backing_dev *bdev)\n{\n\tsector_t s;\n\n\tswitch (bdev->md.meta_dev_idx) {\n\tcase DRBD_MD_INDEX_INTERNAL:\n\tcase DRBD_MD_INDEX_FLEX_INT:\n\t\ts = drbd_get_capacity(bdev->backing_bdev)\n\t\t\t? min_t(sector_t, DRBD_MAX_SECTORS_FLEX,\n\t\t\t\tdrbd_md_first_sector(bdev))\n\t\t\t: 0;\n\t\tbreak;\n\tcase DRBD_MD_INDEX_FLEX_EXT:\n\t\ts = min_t(sector_t, DRBD_MAX_SECTORS_FLEX,\n\t\t\t\tdrbd_get_capacity(bdev->backing_bdev));\n\t\t \n\t\ts = min_t(sector_t, s,\n\t\t\tBM_EXT_TO_SECT(bdev->md.md_size_sect\n\t\t\t\t     - bdev->md.bm_offset));\n\t\tbreak;\n\tdefault:\n\t\ts = min_t(sector_t, DRBD_MAX_SECTORS,\n\t\t\t\tdrbd_get_capacity(bdev->backing_bdev));\n\t}\n\treturn s;\n}\n\n \nstatic inline sector_t drbd_md_ss(struct drbd_backing_dev *bdev)\n{\n\tconst int meta_dev_idx = bdev->md.meta_dev_idx;\n\n\tif (meta_dev_idx == DRBD_MD_INDEX_FLEX_EXT)\n\t\treturn 0;\n\n\t \n\tif (meta_dev_idx == DRBD_MD_INDEX_INTERNAL ||\n\t    meta_dev_idx == DRBD_MD_INDEX_FLEX_INT)\n\t\treturn (drbd_get_capacity(bdev->backing_bdev) & ~7ULL) - 8;\n\n\t \n\treturn MD_128MB_SECT * bdev->md.meta_dev_idx;\n}\n\nstatic inline void\ndrbd_queue_work(struct drbd_work_queue *q, struct drbd_work *w)\n{\n\tunsigned long flags;\n\tspin_lock_irqsave(&q->q_lock, flags);\n\tlist_add_tail(&w->list, &q->q);\n\tspin_unlock_irqrestore(&q->q_lock, flags);\n\twake_up(&q->q_wait);\n}\n\nstatic inline void\ndrbd_queue_work_if_unqueued(struct drbd_work_queue *q, struct drbd_work *w)\n{\n\tunsigned long flags;\n\tspin_lock_irqsave(&q->q_lock, flags);\n\tif (list_empty_careful(&w->list))\n\t\tlist_add_tail(&w->list, &q->q);\n\tspin_unlock_irqrestore(&q->q_lock, flags);\n\twake_up(&q->q_wait);\n}\n\nstatic inline void\ndrbd_device_post_work(struct drbd_device *device, int work_bit)\n{\n\tif (!test_and_set_bit(work_bit, &device->flags)) {\n\t\tstruct drbd_connection *connection =\n\t\t\tfirst_peer_device(device)->connection;\n\t\tstruct drbd_work_queue *q = &connection->sender_work;\n\t\tif (!test_and_set_bit(DEVICE_WORK_PENDING, &connection->flags))\n\t\t\twake_up(&q->q_wait);\n\t}\n}\n\nextern void drbd_flush_workqueue(struct drbd_work_queue *work_queue);\n\n \nstatic inline void wake_ack_receiver(struct drbd_connection *connection)\n{\n\tstruct task_struct *task = connection->ack_receiver.task;\n\tif (task && get_t_state(&connection->ack_receiver) == RUNNING)\n\t\tsend_sig(SIGXCPU, task, 1);\n}\n\nstatic inline void request_ping(struct drbd_connection *connection)\n{\n\tset_bit(SEND_PING, &connection->flags);\n\twake_ack_receiver(connection);\n}\n\nextern void *conn_prepare_command(struct drbd_connection *, struct drbd_socket *);\nextern void *drbd_prepare_command(struct drbd_peer_device *, struct drbd_socket *);\nextern int conn_send_command(struct drbd_connection *, struct drbd_socket *,\n\t\t\t     enum drbd_packet, unsigned int, void *,\n\t\t\t     unsigned int);\nextern int drbd_send_command(struct drbd_peer_device *, struct drbd_socket *,\n\t\t\t     enum drbd_packet, unsigned int, void *,\n\t\t\t     unsigned int);\n\nextern int drbd_send_ping(struct drbd_connection *connection);\nextern int drbd_send_ping_ack(struct drbd_connection *connection);\nextern int drbd_send_state_req(struct drbd_peer_device *, union drbd_state, union drbd_state);\nextern int conn_send_state_req(struct drbd_connection *, union drbd_state, union drbd_state);\n\nstatic inline void drbd_thread_stop(struct drbd_thread *thi)\n{\n\t_drbd_thread_stop(thi, false, true);\n}\n\nstatic inline void drbd_thread_stop_nowait(struct drbd_thread *thi)\n{\n\t_drbd_thread_stop(thi, false, false);\n}\n\nstatic inline void drbd_thread_restart_nowait(struct drbd_thread *thi)\n{\n\t_drbd_thread_stop(thi, true, false);\n}\n\n \nstatic inline void inc_ap_pending(struct drbd_device *device)\n{\n\tatomic_inc(&device->ap_pending_cnt);\n}\n\n#define dec_ap_pending(device) ((void)expect((device), __dec_ap_pending(device) >= 0))\nstatic inline int __dec_ap_pending(struct drbd_device *device)\n{\n\tint ap_pending_cnt = atomic_dec_return(&device->ap_pending_cnt);\n\n\tif (ap_pending_cnt == 0)\n\t\twake_up(&device->misc_wait);\n\treturn ap_pending_cnt;\n}\n\n \nstatic inline void inc_rs_pending(struct drbd_peer_device *peer_device)\n{\n\tatomic_inc(&peer_device->device->rs_pending_cnt);\n}\n\n#define dec_rs_pending(peer_device) \\\n\t((void)expect((peer_device), __dec_rs_pending(peer_device) >= 0))\nstatic inline int __dec_rs_pending(struct drbd_peer_device *peer_device)\n{\n\treturn atomic_dec_return(&peer_device->device->rs_pending_cnt);\n}\n\n \nstatic inline void inc_unacked(struct drbd_device *device)\n{\n\tatomic_inc(&device->unacked_cnt);\n}\n\n#define dec_unacked(device) ((void)expect(device, __dec_unacked(device) >= 0))\nstatic inline int __dec_unacked(struct drbd_device *device)\n{\n\treturn atomic_dec_return(&device->unacked_cnt);\n}\n\n#define sub_unacked(device, n) ((void)expect(device, __sub_unacked(device) >= 0))\nstatic inline int __sub_unacked(struct drbd_device *device, int n)\n{\n\treturn atomic_sub_return(n, &device->unacked_cnt);\n}\n\nstatic inline bool is_sync_target_state(enum drbd_conns connection_state)\n{\n\treturn\tconnection_state == C_SYNC_TARGET ||\n\t\tconnection_state == C_PAUSED_SYNC_T;\n}\n\nstatic inline bool is_sync_source_state(enum drbd_conns connection_state)\n{\n\treturn\tconnection_state == C_SYNC_SOURCE ||\n\t\tconnection_state == C_PAUSED_SYNC_S;\n}\n\nstatic inline bool is_sync_state(enum drbd_conns connection_state)\n{\n\treturn\tis_sync_source_state(connection_state) ||\n\t\tis_sync_target_state(connection_state);\n}\n\n \n#define get_ldev_if_state(_device, _min_state)\t\t\t\t\\\n\t(_get_ldev_if_state((_device), (_min_state)) ?\t\t\t\\\n\t ({ __acquire(x); true; }) : false)\n#define get_ldev(_device) get_ldev_if_state(_device, D_INCONSISTENT)\n\nstatic inline void put_ldev(struct drbd_device *device)\n{\n\tenum drbd_disk_state disk_state = device->state.disk;\n\t \n\tint i = atomic_dec_return(&device->local_cnt);\n\n\t \n\n\t__release(local);\n\tD_ASSERT(device, i >= 0);\n\tif (i == 0) {\n\t\tif (disk_state == D_DISKLESS)\n\t\t\t \n\t\t\tdrbd_device_post_work(device, DESTROY_DISK);\n\t\tif (disk_state == D_FAILED)\n\t\t\t \n\t\t\tif (!test_and_set_bit(GOING_DISKLESS, &device->flags))\n\t\t\t\tdrbd_device_post_work(device, GO_DISKLESS);\n\t\twake_up(&device->misc_wait);\n\t}\n}\n\n#ifndef __CHECKER__\nstatic inline int _get_ldev_if_state(struct drbd_device *device, enum drbd_disk_state mins)\n{\n\tint io_allowed;\n\n\t \n\tif (device->state.disk == D_DISKLESS)\n\t\treturn 0;\n\n\tatomic_inc(&device->local_cnt);\n\tio_allowed = (device->state.disk >= mins);\n\tif (!io_allowed)\n\t\tput_ldev(device);\n\treturn io_allowed;\n}\n#else\nextern int _get_ldev_if_state(struct drbd_device *device, enum drbd_disk_state mins);\n#endif\n\n \nstatic inline int drbd_get_max_buffers(struct drbd_device *device)\n{\n\tstruct net_conf *nc;\n\tint mxb;\n\n\trcu_read_lock();\n\tnc = rcu_dereference(first_peer_device(device)->connection->net_conf);\n\tmxb = nc ? nc->max_buffers : 1000000;   \n\trcu_read_unlock();\n\n\treturn mxb;\n}\n\nstatic inline int drbd_state_is_stable(struct drbd_device *device)\n{\n\tunion drbd_dev_state s = device->state;\n\n\t \n\n\tswitch ((enum drbd_conns)s.conn) {\n\t \n\tcase C_STANDALONE:\n\tcase C_WF_CONNECTION:\n\t \n\tcase C_CONNECTED:\n\tcase C_SYNC_SOURCE:\n\tcase C_SYNC_TARGET:\n\tcase C_VERIFY_S:\n\tcase C_VERIFY_T:\n\tcase C_PAUSED_SYNC_S:\n\tcase C_PAUSED_SYNC_T:\n\tcase C_AHEAD:\n\tcase C_BEHIND:\n\t\t \n\tcase C_DISCONNECTING:\n\tcase C_UNCONNECTED:\n\tcase C_TIMEOUT:\n\tcase C_BROKEN_PIPE:\n\tcase C_NETWORK_FAILURE:\n\tcase C_PROTOCOL_ERROR:\n\tcase C_TEAR_DOWN:\n\tcase C_WF_REPORT_PARAMS:\n\tcase C_STARTING_SYNC_S:\n\tcase C_STARTING_SYNC_T:\n\t\tbreak;\n\n\t\t \n\tcase C_WF_BITMAP_S:\n\t\tif (first_peer_device(device)->connection->agreed_pro_version < 96)\n\t\t\treturn 0;\n\t\tbreak;\n\n\t\t \n\tcase C_WF_BITMAP_T:\n\tcase C_WF_SYNC_UUID:\n\tcase C_MASK:\n\t\t \n\t\treturn 0;\n\t}\n\n\tswitch ((enum drbd_disk_state)s.disk) {\n\tcase D_DISKLESS:\n\tcase D_INCONSISTENT:\n\tcase D_OUTDATED:\n\tcase D_CONSISTENT:\n\tcase D_UP_TO_DATE:\n\tcase D_FAILED:\n\t\t \n\t\tbreak;\n\n\t \n\tcase D_ATTACHING:\n\tcase D_NEGOTIATING:\n\tcase D_UNKNOWN:\n\tcase D_MASK:\n\t\t \n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic inline int drbd_suspended(struct drbd_device *device)\n{\n\tstruct drbd_resource *resource = device->resource;\n\n\treturn resource->susp || resource->susp_fen || resource->susp_nod;\n}\n\nstatic inline bool may_inc_ap_bio(struct drbd_device *device)\n{\n\tint mxb = drbd_get_max_buffers(device);\n\n\tif (drbd_suspended(device))\n\t\treturn false;\n\tif (atomic_read(&device->suspend_cnt))\n\t\treturn false;\n\n\t \n\n\t \n\tif (!drbd_state_is_stable(device))\n\t\treturn false;\n\n\t \n\tif (atomic_read(&device->ap_bio_cnt) > mxb)\n\t\treturn false;\n\tif (test_bit(BITMAP_IO, &device->flags))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool inc_ap_bio_cond(struct drbd_device *device)\n{\n\tbool rv = false;\n\n\tspin_lock_irq(&device->resource->req_lock);\n\trv = may_inc_ap_bio(device);\n\tif (rv)\n\t\tatomic_inc(&device->ap_bio_cnt);\n\tspin_unlock_irq(&device->resource->req_lock);\n\n\treturn rv;\n}\n\nstatic inline void inc_ap_bio(struct drbd_device *device)\n{\n\t \n\n\twait_event(device->misc_wait, inc_ap_bio_cond(device));\n}\n\nstatic inline void dec_ap_bio(struct drbd_device *device)\n{\n\tint mxb = drbd_get_max_buffers(device);\n\tint ap_bio = atomic_dec_return(&device->ap_bio_cnt);\n\n\tD_ASSERT(device, ap_bio >= 0);\n\n\tif (ap_bio == 0 && test_bit(BITMAP_IO, &device->flags)) {\n\t\tif (!test_and_set_bit(BITMAP_IO_QUEUED, &device->flags))\n\t\t\tdrbd_queue_work(&first_peer_device(device)->\n\t\t\t\tconnection->sender_work,\n\t\t\t\t&device->bm_io_work.w);\n\t}\n\n\t \n\tif (ap_bio < mxb)\n\t\twake_up(&device->misc_wait);\n}\n\nstatic inline bool verify_can_do_stop_sector(struct drbd_device *device)\n{\n\treturn first_peer_device(device)->connection->agreed_pro_version >= 97 &&\n\t\tfirst_peer_device(device)->connection->agreed_pro_version != 100;\n}\n\nstatic inline int drbd_set_ed_uuid(struct drbd_device *device, u64 val)\n{\n\tint changed = device->ed_uuid != val;\n\tdevice->ed_uuid = val;\n\treturn changed;\n}\n\nstatic inline int drbd_queue_order_type(struct drbd_device *device)\n{\n\t \n#ifndef QUEUE_ORDERED_NONE\n#define QUEUE_ORDERED_NONE 0\n#endif\n\treturn QUEUE_ORDERED_NONE;\n}\n\nstatic inline struct drbd_connection *first_connection(struct drbd_resource *resource)\n{\n\treturn list_first_entry_or_null(&resource->connections,\n\t\t\t\tstruct drbd_connection, connections);\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}