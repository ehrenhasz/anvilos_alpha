{
  "module_name": "drbd_worker.c",
  "hash_id": "d37641e96ab69424cd203ca9eac1f55311f874dc518ca514c8daa9d49763ec1e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/drbd/drbd_worker.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/drbd.h>\n#include <linux/sched/signal.h>\n#include <linux/wait.h>\n#include <linux/mm.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/string.h>\n#include <linux/scatterlist.h>\n#include <linux/part_stat.h>\n\n#include \"drbd_int.h\"\n#include \"drbd_protocol.h\"\n#include \"drbd_req.h\"\n\nstatic int make_ov_request(struct drbd_peer_device *, int);\nstatic int make_resync_request(struct drbd_peer_device *, int);\n\n \n\n \nvoid drbd_md_endio(struct bio *bio)\n{\n\tstruct drbd_device *device;\n\n\tdevice = bio->bi_private;\n\tdevice->md_io.error = blk_status_to_errno(bio->bi_status);\n\n\t \n\tif (device->ldev)\n\t\tput_ldev(device);\n\tbio_put(bio);\n\n\t \n\tdrbd_md_put_buffer(device);\n\tdevice->md_io.done = 1;\n\twake_up(&device->misc_wait);\n}\n\n \nstatic void drbd_endio_read_sec_final(struct drbd_peer_request *peer_req) __releases(local)\n{\n\tunsigned long flags = 0;\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\n\tspin_lock_irqsave(&device->resource->req_lock, flags);\n\tdevice->read_cnt += peer_req->i.size >> 9;\n\tlist_del(&peer_req->w.list);\n\tif (list_empty(&device->read_ee))\n\t\twake_up(&device->ee_wait);\n\tif (test_bit(__EE_WAS_ERROR, &peer_req->flags))\n\t\t__drbd_chk_io_error(device, DRBD_READ_ERROR);\n\tspin_unlock_irqrestore(&device->resource->req_lock, flags);\n\n\tdrbd_queue_work(&peer_device->connection->sender_work, &peer_req->w);\n\tput_ldev(device);\n}\n\n \nvoid drbd_endio_write_sec_final(struct drbd_peer_request *peer_req) __releases(local)\n{\n\tunsigned long flags = 0;\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tstruct drbd_connection *connection = peer_device->connection;\n\tstruct drbd_interval i;\n\tint do_wake;\n\tu64 block_id;\n\tint do_al_complete_io;\n\n\t \n\ti = peer_req->i;\n\tdo_al_complete_io = peer_req->flags & EE_CALL_AL_COMPLETE_IO;\n\tblock_id = peer_req->block_id;\n\tpeer_req->flags &= ~EE_CALL_AL_COMPLETE_IO;\n\n\tif (peer_req->flags & EE_WAS_ERROR) {\n\t\t \n\t\tif (!__test_and_set_bit(__EE_SEND_WRITE_ACK, &peer_req->flags))\n\t\t\tinc_unacked(device);\n\t\tdrbd_set_out_of_sync(peer_device, peer_req->i.sector, peer_req->i.size);\n\t}\n\n\tspin_lock_irqsave(&device->resource->req_lock, flags);\n\tdevice->writ_cnt += peer_req->i.size >> 9;\n\tlist_move_tail(&peer_req->w.list, &device->done_ee);\n\n\t \n\n\tdo_wake = list_empty(block_id == ID_SYNCER ? &device->sync_ee : &device->active_ee);\n\n\t \n\tif (peer_req->flags & EE_WAS_ERROR)\n\t\t__drbd_chk_io_error(device, DRBD_WRITE_ERROR);\n\n\tif (connection->cstate >= C_WF_REPORT_PARAMS) {\n\t\tkref_get(&device->kref);  \n\t\tif (!queue_work(connection->ack_sender, &peer_device->send_acks_work))\n\t\t\tkref_put(&device->kref, drbd_destroy_device);\n\t}\n\tspin_unlock_irqrestore(&device->resource->req_lock, flags);\n\n\tif (block_id == ID_SYNCER)\n\t\tdrbd_rs_complete_io(device, i.sector);\n\n\tif (do_wake)\n\t\twake_up(&device->ee_wait);\n\n\tif (do_al_complete_io)\n\t\tdrbd_al_complete_io(device, &i);\n\n\tput_ldev(device);\n}\n\n \nvoid drbd_peer_request_endio(struct bio *bio)\n{\n\tstruct drbd_peer_request *peer_req = bio->bi_private;\n\tstruct drbd_device *device = peer_req->peer_device->device;\n\tbool is_write = bio_data_dir(bio) == WRITE;\n\tbool is_discard = bio_op(bio) == REQ_OP_WRITE_ZEROES ||\n\t\t\t  bio_op(bio) == REQ_OP_DISCARD;\n\n\tif (bio->bi_status && drbd_ratelimit())\n\t\tdrbd_warn(device, \"%s: error=%d s=%llus\\n\",\n\t\t\t\tis_write ? (is_discard ? \"discard\" : \"write\")\n\t\t\t\t\t: \"read\", bio->bi_status,\n\t\t\t\t(unsigned long long)peer_req->i.sector);\n\n\tif (bio->bi_status)\n\t\tset_bit(__EE_WAS_ERROR, &peer_req->flags);\n\n\tbio_put(bio);  \n\tif (atomic_dec_and_test(&peer_req->pending_bios)) {\n\t\tif (is_write)\n\t\t\tdrbd_endio_write_sec_final(peer_req);\n\t\telse\n\t\t\tdrbd_endio_read_sec_final(peer_req);\n\t}\n}\n\nstatic void\ndrbd_panic_after_delayed_completion_of_aborted_request(struct drbd_device *device)\n{\n\tpanic(\"drbd%u %s/%u potential random memory corruption caused by delayed completion of aborted local request\\n\",\n\t\tdevice->minor, device->resource->name, device->vnr);\n}\n\n \nvoid drbd_request_endio(struct bio *bio)\n{\n\tunsigned long flags;\n\tstruct drbd_request *req = bio->bi_private;\n\tstruct drbd_device *device = req->device;\n\tstruct bio_and_error m;\n\tenum drbd_req_event what;\n\n\t \n\tif (unlikely(req->rq_state & RQ_LOCAL_ABORTED)) {\n\t\tif (drbd_ratelimit())\n\t\t\tdrbd_emerg(device, \"delayed completion of aborted local request; disk-timeout may be too aggressive\\n\");\n\n\t\tif (!bio->bi_status)\n\t\t\tdrbd_panic_after_delayed_completion_of_aborted_request(device);\n\t}\n\n\t \n\tif (unlikely(bio->bi_status)) {\n\t\tswitch (bio_op(bio)) {\n\t\tcase REQ_OP_WRITE_ZEROES:\n\t\tcase REQ_OP_DISCARD:\n\t\t\tif (bio->bi_status == BLK_STS_NOTSUPP)\n\t\t\t\twhat = DISCARD_COMPLETED_NOTSUPP;\n\t\t\telse\n\t\t\t\twhat = DISCARD_COMPLETED_WITH_ERROR;\n\t\t\tbreak;\n\t\tcase REQ_OP_READ:\n\t\t\tif (bio->bi_opf & REQ_RAHEAD)\n\t\t\t\twhat = READ_AHEAD_COMPLETED_WITH_ERROR;\n\t\t\telse\n\t\t\t\twhat = READ_COMPLETED_WITH_ERROR;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\twhat = WRITE_COMPLETED_WITH_ERROR;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\twhat = COMPLETED_OK;\n\t}\n\n\treq->private_bio = ERR_PTR(blk_status_to_errno(bio->bi_status));\n\tbio_put(bio);\n\n\t \n\tspin_lock_irqsave(&device->resource->req_lock, flags);\n\t__req_mod(req, what, NULL, &m);\n\tspin_unlock_irqrestore(&device->resource->req_lock, flags);\n\tput_ldev(device);\n\n\tif (m.bio)\n\t\tcomplete_master_bio(device, &m);\n}\n\nvoid drbd_csum_ee(struct crypto_shash *tfm, struct drbd_peer_request *peer_req, void *digest)\n{\n\tSHASH_DESC_ON_STACK(desc, tfm);\n\tstruct page *page = peer_req->pages;\n\tstruct page *tmp;\n\tunsigned len;\n\tvoid *src;\n\n\tdesc->tfm = tfm;\n\n\tcrypto_shash_init(desc);\n\n\tsrc = kmap_atomic(page);\n\twhile ((tmp = page_chain_next(page))) {\n\t\t \n\t\tcrypto_shash_update(desc, src, PAGE_SIZE);\n\t\tkunmap_atomic(src);\n\t\tpage = tmp;\n\t\tsrc = kmap_atomic(page);\n\t}\n\t \n\tlen = peer_req->i.size & (PAGE_SIZE - 1);\n\tcrypto_shash_update(desc, src, len ?: PAGE_SIZE);\n\tkunmap_atomic(src);\n\n\tcrypto_shash_final(desc, digest);\n\tshash_desc_zero(desc);\n}\n\nvoid drbd_csum_bio(struct crypto_shash *tfm, struct bio *bio, void *digest)\n{\n\tSHASH_DESC_ON_STACK(desc, tfm);\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\n\tdesc->tfm = tfm;\n\n\tcrypto_shash_init(desc);\n\n\tbio_for_each_segment(bvec, bio, iter) {\n\t\tu8 *src;\n\n\t\tsrc = bvec_kmap_local(&bvec);\n\t\tcrypto_shash_update(desc, src, bvec.bv_len);\n\t\tkunmap_local(src);\n\t}\n\tcrypto_shash_final(desc, digest);\n\tshash_desc_zero(desc);\n}\n\n \nstatic int w_e_send_csum(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tint digest_size;\n\tvoid *digest;\n\tint err = 0;\n\n\tif (unlikely(cancel))\n\t\tgoto out;\n\n\tif (unlikely((peer_req->flags & EE_WAS_ERROR) != 0))\n\t\tgoto out;\n\n\tdigest_size = crypto_shash_digestsize(peer_device->connection->csums_tfm);\n\tdigest = kmalloc(digest_size, GFP_NOIO);\n\tif (digest) {\n\t\tsector_t sector = peer_req->i.sector;\n\t\tunsigned int size = peer_req->i.size;\n\t\tdrbd_csum_ee(peer_device->connection->csums_tfm, peer_req, digest);\n\t\t \n\t\tdrbd_free_peer_req(device, peer_req);\n\t\tpeer_req = NULL;\n\t\tinc_rs_pending(peer_device);\n\t\terr = drbd_send_drequest_csum(peer_device, sector, size,\n\t\t\t\t\t      digest, digest_size,\n\t\t\t\t\t      P_CSUM_RS_REQUEST);\n\t\tkfree(digest);\n\t} else {\n\t\tdrbd_err(device, \"kmalloc() of digest failed.\\n\");\n\t\terr = -ENOMEM;\n\t}\n\nout:\n\tif (peer_req)\n\t\tdrbd_free_peer_req(device, peer_req);\n\n\tif (unlikely(err))\n\t\tdrbd_err(device, \"drbd_send_drequest(..., csum) failed\\n\");\n\treturn err;\n}\n\n#define GFP_TRY\t(__GFP_HIGHMEM | __GFP_NOWARN)\n\nstatic int read_for_csum(struct drbd_peer_device *peer_device, sector_t sector, int size)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tstruct drbd_peer_request *peer_req;\n\n\tif (!get_ldev(device))\n\t\treturn -EIO;\n\n\t \n\tpeer_req = drbd_alloc_peer_req(peer_device, ID_SYNCER  , sector,\n\t\t\t\t       size, size, GFP_TRY);\n\tif (!peer_req)\n\t\tgoto defer;\n\n\tpeer_req->w.cb = w_e_send_csum;\n\tpeer_req->opf = REQ_OP_READ;\n\tspin_lock_irq(&device->resource->req_lock);\n\tlist_add_tail(&peer_req->w.list, &device->read_ee);\n\tspin_unlock_irq(&device->resource->req_lock);\n\n\tatomic_add(size >> 9, &device->rs_sect_ev);\n\tif (drbd_submit_peer_request(peer_req) == 0)\n\t\treturn 0;\n\n\t \n\tspin_lock_irq(&device->resource->req_lock);\n\tlist_del(&peer_req->w.list);\n\tspin_unlock_irq(&device->resource->req_lock);\n\n\tdrbd_free_peer_req(device, peer_req);\ndefer:\n\tput_ldev(device);\n\treturn -EAGAIN;\n}\n\nint w_resync_timer(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_device *device =\n\t\tcontainer_of(w, struct drbd_device, resync_work);\n\n\tswitch (device->state.conn) {\n\tcase C_VERIFY_S:\n\t\tmake_ov_request(first_peer_device(device), cancel);\n\t\tbreak;\n\tcase C_SYNC_TARGET:\n\t\tmake_resync_request(first_peer_device(device), cancel);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nvoid resync_timer_fn(struct timer_list *t)\n{\n\tstruct drbd_device *device = from_timer(device, t, resync_timer);\n\n\tdrbd_queue_work_if_unqueued(\n\t\t&first_peer_device(device)->connection->sender_work,\n\t\t&device->resync_work);\n}\n\nstatic void fifo_set(struct fifo_buffer *fb, int value)\n{\n\tint i;\n\n\tfor (i = 0; i < fb->size; i++)\n\t\tfb->values[i] = value;\n}\n\nstatic int fifo_push(struct fifo_buffer *fb, int value)\n{\n\tint ov;\n\n\tov = fb->values[fb->head_index];\n\tfb->values[fb->head_index++] = value;\n\n\tif (fb->head_index >= fb->size)\n\t\tfb->head_index = 0;\n\n\treturn ov;\n}\n\nstatic void fifo_add_val(struct fifo_buffer *fb, int value)\n{\n\tint i;\n\n\tfor (i = 0; i < fb->size; i++)\n\t\tfb->values[i] += value;\n}\n\nstruct fifo_buffer *fifo_alloc(unsigned int fifo_size)\n{\n\tstruct fifo_buffer *fb;\n\n\tfb = kzalloc(struct_size(fb, values, fifo_size), GFP_NOIO);\n\tif (!fb)\n\t\treturn NULL;\n\n\tfb->head_index = 0;\n\tfb->size = fifo_size;\n\tfb->total = 0;\n\n\treturn fb;\n}\n\nstatic int drbd_rs_controller(struct drbd_peer_device *peer_device, unsigned int sect_in)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tstruct disk_conf *dc;\n\tunsigned int want;      \n\tint req_sect;  \n\tint correction;  \n\tint cps;  \n\tint steps;  \n\tint curr_corr;\n\tint max_sect;\n\tstruct fifo_buffer *plan;\n\n\tdc = rcu_dereference(device->ldev->disk_conf);\n\tplan = rcu_dereference(device->rs_plan_s);\n\n\tsteps = plan->size;  \n\n\tif (device->rs_in_flight + sect_in == 0) {  \n\t\twant = ((dc->resync_rate * 2 * SLEEP_TIME) / HZ) * steps;\n\t} else {  \n\t\twant = dc->c_fill_target ? dc->c_fill_target :\n\t\t\tsect_in * dc->c_delay_target * HZ / (SLEEP_TIME * 10);\n\t}\n\n\tcorrection = want - device->rs_in_flight - plan->total;\n\n\t \n\tcps = correction / steps;\n\tfifo_add_val(plan, cps);\n\tplan->total += cps * steps;\n\n\t \n\tcurr_corr = fifo_push(plan, 0);\n\tplan->total -= curr_corr;\n\n\treq_sect = sect_in + curr_corr;\n\tif (req_sect < 0)\n\t\treq_sect = 0;\n\n\tmax_sect = (dc->c_max_rate * 2 * SLEEP_TIME) / HZ;\n\tif (req_sect > max_sect)\n\t\treq_sect = max_sect;\n\n\t \n\n\treturn req_sect;\n}\n\nstatic int drbd_rs_number_requests(struct drbd_peer_device *peer_device)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tunsigned int sect_in;   \n\tint number, mxb;\n\n\tsect_in = atomic_xchg(&device->rs_sect_in, 0);\n\tdevice->rs_in_flight -= sect_in;\n\n\trcu_read_lock();\n\tmxb = drbd_get_max_buffers(device) / 2;\n\tif (rcu_dereference(device->rs_plan_s)->size) {\n\t\tnumber = drbd_rs_controller(peer_device, sect_in) >> (BM_BLOCK_SHIFT - 9);\n\t\tdevice->c_sync_rate = number * HZ * (BM_BLOCK_SIZE / 1024) / SLEEP_TIME;\n\t} else {\n\t\tdevice->c_sync_rate = rcu_dereference(device->ldev->disk_conf)->resync_rate;\n\t\tnumber = SLEEP_TIME * device->c_sync_rate  / ((BM_BLOCK_SIZE / 1024) * HZ);\n\t}\n\trcu_read_unlock();\n\n\t \n\n\t \n\tif (mxb - device->rs_in_flight/8 < number)\n\t\tnumber = mxb - device->rs_in_flight/8;\n\n\treturn number;\n}\n\nstatic int make_resync_request(struct drbd_peer_device *const peer_device, int cancel)\n{\n\tstruct drbd_device *const device = peer_device->device;\n\tstruct drbd_connection *const connection = peer_device ? peer_device->connection : NULL;\n\tunsigned long bit;\n\tsector_t sector;\n\tconst sector_t capacity = get_capacity(device->vdisk);\n\tint max_bio_size;\n\tint number, rollback_i, size;\n\tint align, requeue = 0;\n\tint i = 0;\n\tint discard_granularity = 0;\n\n\tif (unlikely(cancel))\n\t\treturn 0;\n\n\tif (device->rs_total == 0) {\n\t\t \n\t\tdrbd_resync_finished(peer_device);\n\t\treturn 0;\n\t}\n\n\tif (!get_ldev(device)) {\n\t\t \n\t\tdrbd_err(device, \"Disk broke down during resync!\\n\");\n\t\treturn 0;\n\t}\n\n\tif (connection->agreed_features & DRBD_FF_THIN_RESYNC) {\n\t\trcu_read_lock();\n\t\tdiscard_granularity = rcu_dereference(device->ldev->disk_conf)->rs_discard_granularity;\n\t\trcu_read_unlock();\n\t}\n\n\tmax_bio_size = queue_max_hw_sectors(device->rq_queue) << 9;\n\tnumber = drbd_rs_number_requests(peer_device);\n\tif (number <= 0)\n\t\tgoto requeue;\n\n\tfor (i = 0; i < number; i++) {\n\t\t \n\t\tmutex_lock(&connection->data.mutex);\n\t\tif (connection->data.socket) {\n\t\t\tstruct sock *sk = connection->data.socket->sk;\n\t\t\tint queued = sk->sk_wmem_queued;\n\t\t\tint sndbuf = sk->sk_sndbuf;\n\t\t\tif (queued > sndbuf / 2) {\n\t\t\t\trequeue = 1;\n\t\t\t\tif (sk->sk_socket)\n\t\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\t\t}\n\t\t} else\n\t\t\trequeue = 1;\n\t\tmutex_unlock(&connection->data.mutex);\n\t\tif (requeue)\n\t\t\tgoto requeue;\n\nnext_sector:\n\t\tsize = BM_BLOCK_SIZE;\n\t\tbit  = drbd_bm_find_next(device, device->bm_resync_fo);\n\n\t\tif (bit == DRBD_END_OF_BITMAP) {\n\t\t\tdevice->bm_resync_fo = drbd_bm_bits(device);\n\t\t\tput_ldev(device);\n\t\t\treturn 0;\n\t\t}\n\n\t\tsector = BM_BIT_TO_SECT(bit);\n\n\t\tif (drbd_try_rs_begin_io(peer_device, sector)) {\n\t\t\tdevice->bm_resync_fo = bit;\n\t\t\tgoto requeue;\n\t\t}\n\t\tdevice->bm_resync_fo = bit + 1;\n\n\t\tif (unlikely(drbd_bm_test_bit(device, bit) == 0)) {\n\t\t\tdrbd_rs_complete_io(device, sector);\n\t\t\tgoto next_sector;\n\t\t}\n\n#if DRBD_MAX_BIO_SIZE > BM_BLOCK_SIZE\n\t\t \n\t\talign = 1;\n\t\trollback_i = i;\n\t\twhile (i < number) {\n\t\t\tif (size + BM_BLOCK_SIZE > max_bio_size)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (sector & ((1<<(align+3))-1))\n\t\t\t\tbreak;\n\n\t\t\tif (discard_granularity && size == discard_granularity)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (((bit+1) & BM_BLOCKS_PER_BM_EXT_MASK) == 0)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (drbd_bm_test_bit(device, bit+1) != 1)\n\t\t\t\tbreak;\n\t\t\tbit++;\n\t\t\tsize += BM_BLOCK_SIZE;\n\t\t\tif ((BM_BLOCK_SIZE << align) <= size)\n\t\t\t\talign++;\n\t\t\ti++;\n\t\t}\n\t\t \n\t\tif (size > BM_BLOCK_SIZE)\n\t\t\tdevice->bm_resync_fo = bit + 1;\n#endif\n\n\t\t \n\t\tif (sector + (size>>9) > capacity)\n\t\t\tsize = (capacity-sector)<<9;\n\n\t\tif (device->use_csums) {\n\t\t\tswitch (read_for_csum(peer_device, sector, size)) {\n\t\t\tcase -EIO:  \n\t\t\t\tput_ldev(device);\n\t\t\t\treturn -EIO;\n\t\t\tcase -EAGAIN:  \n\t\t\t\tdrbd_rs_complete_io(device, sector);\n\t\t\t\tdevice->bm_resync_fo = BM_SECT_TO_BIT(sector);\n\t\t\t\ti = rollback_i;\n\t\t\t\tgoto requeue;\n\t\t\tcase 0:\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tBUG();\n\t\t\t}\n\t\t} else {\n\t\t\tint err;\n\n\t\t\tinc_rs_pending(peer_device);\n\t\t\terr = drbd_send_drequest(peer_device,\n\t\t\t\t\t\t size == discard_granularity ? P_RS_THIN_REQ : P_RS_DATA_REQUEST,\n\t\t\t\t\t\t sector, size, ID_SYNCER);\n\t\t\tif (err) {\n\t\t\t\tdrbd_err(device, \"drbd_send_drequest() failed, aborting...\\n\");\n\t\t\t\tdec_rs_pending(peer_device);\n\t\t\t\tput_ldev(device);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (device->bm_resync_fo >= drbd_bm_bits(device)) {\n\t\t \n\t\tput_ldev(device);\n\t\treturn 0;\n\t}\n\n requeue:\n\tdevice->rs_in_flight += (i << (BM_BLOCK_SHIFT - 9));\n\tmod_timer(&device->resync_timer, jiffies + SLEEP_TIME);\n\tput_ldev(device);\n\treturn 0;\n}\n\nstatic int make_ov_request(struct drbd_peer_device *peer_device, int cancel)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tint number, i, size;\n\tsector_t sector;\n\tconst sector_t capacity = get_capacity(device->vdisk);\n\tbool stop_sector_reached = false;\n\n\tif (unlikely(cancel))\n\t\treturn 1;\n\n\tnumber = drbd_rs_number_requests(peer_device);\n\n\tsector = device->ov_position;\n\tfor (i = 0; i < number; i++) {\n\t\tif (sector >= capacity)\n\t\t\treturn 1;\n\n\t\t \n\t\tstop_sector_reached = i > 0\n\t\t\t&& verify_can_do_stop_sector(device)\n\t\t\t&& sector >= device->ov_stop_sector;\n\t\tif (stop_sector_reached)\n\t\t\tbreak;\n\n\t\tsize = BM_BLOCK_SIZE;\n\n\t\tif (drbd_try_rs_begin_io(peer_device, sector)) {\n\t\t\tdevice->ov_position = sector;\n\t\t\tgoto requeue;\n\t\t}\n\n\t\tif (sector + (size>>9) > capacity)\n\t\t\tsize = (capacity-sector)<<9;\n\n\t\tinc_rs_pending(peer_device);\n\t\tif (drbd_send_ov_request(first_peer_device(device), sector, size)) {\n\t\t\tdec_rs_pending(peer_device);\n\t\t\treturn 0;\n\t\t}\n\t\tsector += BM_SECT_PER_BIT;\n\t}\n\tdevice->ov_position = sector;\n\n requeue:\n\tdevice->rs_in_flight += (i << (BM_BLOCK_SHIFT - 9));\n\tif (i == 0 || !stop_sector_reached)\n\t\tmod_timer(&device->resync_timer, jiffies + SLEEP_TIME);\n\treturn 1;\n}\n\nint w_ov_finished(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_device_work *dw =\n\t\tcontainer_of(w, struct drbd_device_work, w);\n\tstruct drbd_device *device = dw->device;\n\tkfree(dw);\n\tov_out_of_sync_print(first_peer_device(device));\n\tdrbd_resync_finished(first_peer_device(device));\n\n\treturn 0;\n}\n\nstatic int w_resync_finished(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_device_work *dw =\n\t\tcontainer_of(w, struct drbd_device_work, w);\n\tstruct drbd_device *device = dw->device;\n\tkfree(dw);\n\n\tdrbd_resync_finished(first_peer_device(device));\n\n\treturn 0;\n}\n\nstatic void ping_peer(struct drbd_device *device)\n{\n\tstruct drbd_connection *connection = first_peer_device(device)->connection;\n\n\tclear_bit(GOT_PING_ACK, &connection->flags);\n\trequest_ping(connection);\n\twait_event(connection->ping_wait,\n\t\t   test_bit(GOT_PING_ACK, &connection->flags) || device->state.conn < C_CONNECTED);\n}\n\nint drbd_resync_finished(struct drbd_peer_device *peer_device)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tstruct drbd_connection *connection = peer_device->connection;\n\tunsigned long db, dt, dbdt;\n\tunsigned long n_oos;\n\tunion drbd_state os, ns;\n\tstruct drbd_device_work *dw;\n\tchar *khelper_cmd = NULL;\n\tint verify_done = 0;\n\n\t \n\tif (drbd_rs_del_all(device)) {\n\t\t \n\n\t\tschedule_timeout_interruptible(HZ / 10);\n\t\tdw = kmalloc(sizeof(struct drbd_device_work), GFP_ATOMIC);\n\t\tif (dw) {\n\t\t\tdw->w.cb = w_resync_finished;\n\t\t\tdw->device = device;\n\t\t\tdrbd_queue_work(&connection->sender_work, &dw->w);\n\t\t\treturn 1;\n\t\t}\n\t\tdrbd_err(device, \"Warn failed to drbd_rs_del_all() and to kmalloc(dw).\\n\");\n\t}\n\n\tdt = (jiffies - device->rs_start - device->rs_paused) / HZ;\n\tif (dt <= 0)\n\t\tdt = 1;\n\n\tdb = device->rs_total;\n\t \n\tif (device->state.conn == C_VERIFY_S || device->state.conn == C_VERIFY_T)\n\t\tdb -= device->ov_left;\n\n\tdbdt = Bit2KB(db/dt);\n\tdevice->rs_paused /= HZ;\n\n\tif (!get_ldev(device))\n\t\tgoto out;\n\n\tping_peer(device);\n\n\tspin_lock_irq(&device->resource->req_lock);\n\tos = drbd_read_state(device);\n\n\tverify_done = (os.conn == C_VERIFY_S || os.conn == C_VERIFY_T);\n\n\t \n\tif (os.conn <= C_CONNECTED)\n\t\tgoto out_unlock;\n\n\tns = os;\n\tns.conn = C_CONNECTED;\n\n\tdrbd_info(device, \"%s done (total %lu sec; paused %lu sec; %lu K/sec)\\n\",\n\t     verify_done ? \"Online verify\" : \"Resync\",\n\t     dt + device->rs_paused, device->rs_paused, dbdt);\n\n\tn_oos = drbd_bm_total_weight(device);\n\n\tif (os.conn == C_VERIFY_S || os.conn == C_VERIFY_T) {\n\t\tif (n_oos) {\n\t\t\tdrbd_alert(device, \"Online verify found %lu %dk block out of sync!\\n\",\n\t\t\t      n_oos, Bit2KB(1));\n\t\t\tkhelper_cmd = \"out-of-sync\";\n\t\t}\n\t} else {\n\t\tD_ASSERT(device, (n_oos - device->rs_failed) == 0);\n\n\t\tif (os.conn == C_SYNC_TARGET || os.conn == C_PAUSED_SYNC_T)\n\t\t\tkhelper_cmd = \"after-resync-target\";\n\n\t\tif (device->use_csums && device->rs_total) {\n\t\t\tconst unsigned long s = device->rs_same_csum;\n\t\t\tconst unsigned long t = device->rs_total;\n\t\t\tconst int ratio =\n\t\t\t\t(t == 0)     ? 0 :\n\t\t\t(t < 100000) ? ((s*100)/t) : (s/(t/100));\n\t\t\tdrbd_info(device, \"%u %% had equal checksums, eliminated: %luK; \"\n\t\t\t     \"transferred %luK total %luK\\n\",\n\t\t\t     ratio,\n\t\t\t     Bit2KB(device->rs_same_csum),\n\t\t\t     Bit2KB(device->rs_total - device->rs_same_csum),\n\t\t\t     Bit2KB(device->rs_total));\n\t\t}\n\t}\n\n\tif (device->rs_failed) {\n\t\tdrbd_info(device, \"            %lu failed blocks\\n\", device->rs_failed);\n\n\t\tif (os.conn == C_SYNC_TARGET || os.conn == C_PAUSED_SYNC_T) {\n\t\t\tns.disk = D_INCONSISTENT;\n\t\t\tns.pdsk = D_UP_TO_DATE;\n\t\t} else {\n\t\t\tns.disk = D_UP_TO_DATE;\n\t\t\tns.pdsk = D_INCONSISTENT;\n\t\t}\n\t} else {\n\t\tns.disk = D_UP_TO_DATE;\n\t\tns.pdsk = D_UP_TO_DATE;\n\n\t\tif (os.conn == C_SYNC_TARGET || os.conn == C_PAUSED_SYNC_T) {\n\t\t\tif (device->p_uuid) {\n\t\t\t\tint i;\n\t\t\t\tfor (i = UI_BITMAP ; i <= UI_HISTORY_END ; i++)\n\t\t\t\t\t_drbd_uuid_set(device, i, device->p_uuid[i]);\n\t\t\t\tdrbd_uuid_set(device, UI_BITMAP, device->ldev->md.uuid[UI_CURRENT]);\n\t\t\t\t_drbd_uuid_set(device, UI_CURRENT, device->p_uuid[UI_CURRENT]);\n\t\t\t} else {\n\t\t\t\tdrbd_err(device, \"device->p_uuid is NULL! BUG\\n\");\n\t\t\t}\n\t\t}\n\n\t\tif (!(os.conn == C_VERIFY_S || os.conn == C_VERIFY_T)) {\n\t\t\t \n\t\t\tdrbd_uuid_set_bm(device, 0UL);\n\t\t\tdrbd_print_uuids(device, \"updated UUIDs\");\n\t\t\tif (device->p_uuid) {\n\t\t\t\t \n\t\t\t\tint i;\n\t\t\t\tfor (i = UI_CURRENT ; i <= UI_HISTORY_END ; i++)\n\t\t\t\t\tdevice->p_uuid[i] = device->ldev->md.uuid[i];\n\t\t\t}\n\t\t}\n\t}\n\n\t_drbd_set_state(device, ns, CS_VERBOSE, NULL);\nout_unlock:\n\tspin_unlock_irq(&device->resource->req_lock);\n\n\t \n\tif (os.conn == C_SYNC_SOURCE) {\n\t\tenum drbd_disk_state disk_state = D_MASK;\n\t\tenum drbd_disk_state pdsk_state = D_MASK;\n\t\tenum drbd_fencing_p fp = FP_DONT_CARE;\n\n\t\trcu_read_lock();\n\t\tfp = rcu_dereference(device->ldev->disk_conf)->fencing;\n\t\tif (fp != FP_DONT_CARE) {\n\t\t\tstruct drbd_peer_device *peer_device;\n\t\t\tint vnr;\n\t\t\tidr_for_each_entry(&connection->peer_devices, peer_device, vnr) {\n\t\t\t\tstruct drbd_device *device = peer_device->device;\n\t\t\t\tdisk_state = min_t(enum drbd_disk_state, disk_state, device->state.disk);\n\t\t\t\tpdsk_state = min_t(enum drbd_disk_state, pdsk_state, device->state.pdsk);\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tif (disk_state == D_UP_TO_DATE && pdsk_state == D_UP_TO_DATE)\n\t\t\tconn_khelper(connection, \"unfence-peer\");\n\t}\n\n\tput_ldev(device);\nout:\n\tdevice->rs_total  = 0;\n\tdevice->rs_failed = 0;\n\tdevice->rs_paused = 0;\n\n\t \n\tif (verify_done && device->ov_left == 0)\n\t\tdevice->ov_start_sector = 0;\n\n\tdrbd_md_sync(device);\n\n\tif (khelper_cmd)\n\t\tdrbd_khelper(device, khelper_cmd);\n\n\treturn 1;\n}\n\n \nstatic void move_to_net_ee_or_free(struct drbd_device *device, struct drbd_peer_request *peer_req)\n{\n\tif (drbd_peer_req_has_active_page(peer_req)) {\n\t\t \n\t\tint i = PFN_UP(peer_req->i.size);\n\t\tatomic_add(i, &device->pp_in_use_by_net);\n\t\tatomic_sub(i, &device->pp_in_use);\n\t\tspin_lock_irq(&device->resource->req_lock);\n\t\tlist_add_tail(&peer_req->w.list, &device->net_ee);\n\t\tspin_unlock_irq(&device->resource->req_lock);\n\t\twake_up(&drbd_pp_wait);\n\t} else\n\t\tdrbd_free_peer_req(device, peer_req);\n}\n\n \nint w_e_end_data_req(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tint err;\n\n\tif (unlikely(cancel)) {\n\t\tdrbd_free_peer_req(device, peer_req);\n\t\tdec_unacked(device);\n\t\treturn 0;\n\t}\n\n\tif (likely((peer_req->flags & EE_WAS_ERROR) == 0)) {\n\t\terr = drbd_send_block(peer_device, P_DATA_REPLY, peer_req);\n\t} else {\n\t\tif (drbd_ratelimit())\n\t\t\tdrbd_err(device, \"Sending NegDReply. sector=%llus.\\n\",\n\t\t\t    (unsigned long long)peer_req->i.sector);\n\n\t\terr = drbd_send_ack(peer_device, P_NEG_DREPLY, peer_req);\n\t}\n\n\tdec_unacked(device);\n\n\tmove_to_net_ee_or_free(device, peer_req);\n\n\tif (unlikely(err))\n\t\tdrbd_err(device, \"drbd_send_block() failed\\n\");\n\treturn err;\n}\n\nstatic bool all_zero(struct drbd_peer_request *peer_req)\n{\n\tstruct page *page = peer_req->pages;\n\tunsigned int len = peer_req->i.size;\n\n\tpage_chain_for_each(page) {\n\t\tunsigned int l = min_t(unsigned int, len, PAGE_SIZE);\n\t\tunsigned int i, words = l / sizeof(long);\n\t\tunsigned long *d;\n\n\t\td = kmap_atomic(page);\n\t\tfor (i = 0; i < words; i++) {\n\t\t\tif (d[i]) {\n\t\t\t\tkunmap_atomic(d);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tkunmap_atomic(d);\n\t\tlen -= l;\n\t}\n\n\treturn true;\n}\n\n \nint w_e_end_rsdata_req(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tint err;\n\n\tif (unlikely(cancel)) {\n\t\tdrbd_free_peer_req(device, peer_req);\n\t\tdec_unacked(device);\n\t\treturn 0;\n\t}\n\n\tif (get_ldev_if_state(device, D_FAILED)) {\n\t\tdrbd_rs_complete_io(device, peer_req->i.sector);\n\t\tput_ldev(device);\n\t}\n\n\tif (device->state.conn == C_AHEAD) {\n\t\terr = drbd_send_ack(peer_device, P_RS_CANCEL, peer_req);\n\t} else if (likely((peer_req->flags & EE_WAS_ERROR) == 0)) {\n\t\tif (likely(device->state.pdsk >= D_INCONSISTENT)) {\n\t\t\tinc_rs_pending(peer_device);\n\t\t\tif (peer_req->flags & EE_RS_THIN_REQ && all_zero(peer_req))\n\t\t\t\terr = drbd_send_rs_deallocated(peer_device, peer_req);\n\t\t\telse\n\t\t\t\terr = drbd_send_block(peer_device, P_RS_DATA_REPLY, peer_req);\n\t\t} else {\n\t\t\tif (drbd_ratelimit())\n\t\t\t\tdrbd_err(device, \"Not sending RSDataReply, \"\n\t\t\t\t    \"partner DISKLESS!\\n\");\n\t\t\terr = 0;\n\t\t}\n\t} else {\n\t\tif (drbd_ratelimit())\n\t\t\tdrbd_err(device, \"Sending NegRSDReply. sector %llus.\\n\",\n\t\t\t    (unsigned long long)peer_req->i.sector);\n\n\t\terr = drbd_send_ack(peer_device, P_NEG_RS_DREPLY, peer_req);\n\n\t\t \n\t\tdrbd_rs_failed_io(peer_device, peer_req->i.sector, peer_req->i.size);\n\t}\n\n\tdec_unacked(device);\n\n\tmove_to_net_ee_or_free(device, peer_req);\n\n\tif (unlikely(err))\n\t\tdrbd_err(device, \"drbd_send_block() failed\\n\");\n\treturn err;\n}\n\nint w_e_end_csum_rs_req(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tstruct digest_info *di;\n\tint digest_size;\n\tvoid *digest = NULL;\n\tint err, eq = 0;\n\n\tif (unlikely(cancel)) {\n\t\tdrbd_free_peer_req(device, peer_req);\n\t\tdec_unacked(device);\n\t\treturn 0;\n\t}\n\n\tif (get_ldev(device)) {\n\t\tdrbd_rs_complete_io(device, peer_req->i.sector);\n\t\tput_ldev(device);\n\t}\n\n\tdi = peer_req->digest;\n\n\tif (likely((peer_req->flags & EE_WAS_ERROR) == 0)) {\n\t\t \n\t\tif (peer_device->connection->csums_tfm) {\n\t\t\tdigest_size = crypto_shash_digestsize(peer_device->connection->csums_tfm);\n\t\t\tD_ASSERT(device, digest_size == di->digest_size);\n\t\t\tdigest = kmalloc(digest_size, GFP_NOIO);\n\t\t}\n\t\tif (digest) {\n\t\t\tdrbd_csum_ee(peer_device->connection->csums_tfm, peer_req, digest);\n\t\t\teq = !memcmp(digest, di->digest, digest_size);\n\t\t\tkfree(digest);\n\t\t}\n\n\t\tif (eq) {\n\t\t\tdrbd_set_in_sync(peer_device, peer_req->i.sector, peer_req->i.size);\n\t\t\t \n\t\t\tdevice->rs_same_csum += peer_req->i.size >> BM_BLOCK_SHIFT;\n\t\t\terr = drbd_send_ack(peer_device, P_RS_IS_IN_SYNC, peer_req);\n\t\t} else {\n\t\t\tinc_rs_pending(peer_device);\n\t\t\tpeer_req->block_id = ID_SYNCER;  \n\t\t\tpeer_req->flags &= ~EE_HAS_DIGEST;  \n\t\t\tkfree(di);\n\t\t\terr = drbd_send_block(peer_device, P_RS_DATA_REPLY, peer_req);\n\t\t}\n\t} else {\n\t\terr = drbd_send_ack(peer_device, P_NEG_RS_DREPLY, peer_req);\n\t\tif (drbd_ratelimit())\n\t\t\tdrbd_err(device, \"Sending NegDReply. I guess it gets messy.\\n\");\n\t}\n\n\tdec_unacked(device);\n\tmove_to_net_ee_or_free(device, peer_req);\n\n\tif (unlikely(err))\n\t\tdrbd_err(device, \"drbd_send_block/ack() failed\\n\");\n\treturn err;\n}\n\nint w_e_end_ov_req(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tsector_t sector = peer_req->i.sector;\n\tunsigned int size = peer_req->i.size;\n\tint digest_size;\n\tvoid *digest;\n\tint err = 0;\n\n\tif (unlikely(cancel))\n\t\tgoto out;\n\n\tdigest_size = crypto_shash_digestsize(peer_device->connection->verify_tfm);\n\tdigest = kmalloc(digest_size, GFP_NOIO);\n\tif (!digest) {\n\t\terr = 1;\t \n\t\tgoto out;\n\t}\n\n\tif (likely(!(peer_req->flags & EE_WAS_ERROR)))\n\t\tdrbd_csum_ee(peer_device->connection->verify_tfm, peer_req, digest);\n\telse\n\t\tmemset(digest, 0, digest_size);\n\n\t \n\tdrbd_free_peer_req(device, peer_req);\n\tpeer_req = NULL;\n\tinc_rs_pending(peer_device);\n\terr = drbd_send_drequest_csum(peer_device, sector, size, digest, digest_size, P_OV_REPLY);\n\tif (err)\n\t\tdec_rs_pending(peer_device);\n\tkfree(digest);\n\nout:\n\tif (peer_req)\n\t\tdrbd_free_peer_req(device, peer_req);\n\tdec_unacked(device);\n\treturn err;\n}\n\nvoid drbd_ov_out_of_sync_found(struct drbd_peer_device *peer_device, sector_t sector, int size)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tif (device->ov_last_oos_start + device->ov_last_oos_size == sector) {\n\t\tdevice->ov_last_oos_size += size>>9;\n\t} else {\n\t\tdevice->ov_last_oos_start = sector;\n\t\tdevice->ov_last_oos_size = size>>9;\n\t}\n\tdrbd_set_out_of_sync(peer_device, sector, size);\n}\n\nint w_e_end_ov_reply(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_peer_request *peer_req = container_of(w, struct drbd_peer_request, w);\n\tstruct drbd_peer_device *peer_device = peer_req->peer_device;\n\tstruct drbd_device *device = peer_device->device;\n\tstruct digest_info *di;\n\tvoid *digest;\n\tsector_t sector = peer_req->i.sector;\n\tunsigned int size = peer_req->i.size;\n\tint digest_size;\n\tint err, eq = 0;\n\tbool stop_sector_reached = false;\n\n\tif (unlikely(cancel)) {\n\t\tdrbd_free_peer_req(device, peer_req);\n\t\tdec_unacked(device);\n\t\treturn 0;\n\t}\n\n\t \n\tif (get_ldev(device)) {\n\t\tdrbd_rs_complete_io(device, peer_req->i.sector);\n\t\tput_ldev(device);\n\t}\n\n\tdi = peer_req->digest;\n\n\tif (likely((peer_req->flags & EE_WAS_ERROR) == 0)) {\n\t\tdigest_size = crypto_shash_digestsize(peer_device->connection->verify_tfm);\n\t\tdigest = kmalloc(digest_size, GFP_NOIO);\n\t\tif (digest) {\n\t\t\tdrbd_csum_ee(peer_device->connection->verify_tfm, peer_req, digest);\n\n\t\t\tD_ASSERT(device, digest_size == di->digest_size);\n\t\t\teq = !memcmp(digest, di->digest, digest_size);\n\t\t\tkfree(digest);\n\t\t}\n\t}\n\n\t \n\tdrbd_free_peer_req(device, peer_req);\n\tif (!eq)\n\t\tdrbd_ov_out_of_sync_found(peer_device, sector, size);\n\telse\n\t\tov_out_of_sync_print(peer_device);\n\n\terr = drbd_send_ack_ex(peer_device, P_OV_RESULT, sector, size,\n\t\t\t       eq ? ID_IN_SYNC : ID_OUT_OF_SYNC);\n\n\tdec_unacked(device);\n\n\t--device->ov_left;\n\n\t \n\tif ((device->ov_left & 0x200) == 0x200)\n\t\tdrbd_advance_rs_marks(peer_device, device->ov_left);\n\n\tstop_sector_reached = verify_can_do_stop_sector(device) &&\n\t\t(sector + (size>>9)) >= device->ov_stop_sector;\n\n\tif (device->ov_left == 0 || stop_sector_reached) {\n\t\tov_out_of_sync_print(peer_device);\n\t\tdrbd_resync_finished(peer_device);\n\t}\n\n\treturn err;\n}\n\n \nstatic int drbd_send_barrier(struct drbd_connection *connection)\n{\n\tstruct p_barrier *p;\n\tstruct drbd_socket *sock;\n\n\tsock = &connection->data;\n\tp = conn_prepare_command(connection, sock);\n\tif (!p)\n\t\treturn -EIO;\n\tp->barrier = connection->send.current_epoch_nr;\n\tp->pad = 0;\n\tconnection->send.current_epoch_writes = 0;\n\tconnection->send.last_sent_barrier_jif = jiffies;\n\n\treturn conn_send_command(connection, sock, P_BARRIER, sizeof(*p), NULL, 0);\n}\n\nstatic int pd_send_unplug_remote(struct drbd_peer_device *pd)\n{\n\tstruct drbd_socket *sock = &pd->connection->data;\n\tif (!drbd_prepare_command(pd, sock))\n\t\treturn -EIO;\n\treturn drbd_send_command(pd, sock, P_UNPLUG_REMOTE, 0, NULL, 0);\n}\n\nint w_send_write_hint(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_device *device =\n\t\tcontainer_of(w, struct drbd_device, unplug_work);\n\n\tif (cancel)\n\t\treturn 0;\n\treturn pd_send_unplug_remote(first_peer_device(device));\n}\n\nstatic void re_init_if_first_write(struct drbd_connection *connection, unsigned int epoch)\n{\n\tif (!connection->send.seen_any_write_yet) {\n\t\tconnection->send.seen_any_write_yet = true;\n\t\tconnection->send.current_epoch_nr = epoch;\n\t\tconnection->send.current_epoch_writes = 0;\n\t\tconnection->send.last_sent_barrier_jif = jiffies;\n\t}\n}\n\nstatic void maybe_send_barrier(struct drbd_connection *connection, unsigned int epoch)\n{\n\t \n\tif (!connection->send.seen_any_write_yet)\n\t\treturn;\n\tif (connection->send.current_epoch_nr != epoch) {\n\t\tif (connection->send.current_epoch_writes)\n\t\t\tdrbd_send_barrier(connection);\n\t\tconnection->send.current_epoch_nr = epoch;\n\t}\n}\n\nint w_send_out_of_sync(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_request *req = container_of(w, struct drbd_request, w);\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_peer_device *const peer_device = first_peer_device(device);\n\tstruct drbd_connection *const connection = peer_device->connection;\n\tint err;\n\n\tif (unlikely(cancel)) {\n\t\treq_mod(req, SEND_CANCELED, peer_device);\n\t\treturn 0;\n\t}\n\treq->pre_send_jif = jiffies;\n\n\t \n\tmaybe_send_barrier(connection, req->epoch);\n\n\terr = drbd_send_out_of_sync(peer_device, req);\n\treq_mod(req, OOS_HANDED_TO_NETWORK, peer_device);\n\n\treturn err;\n}\n\n \nint w_send_dblock(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_request *req = container_of(w, struct drbd_request, w);\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_peer_device *const peer_device = first_peer_device(device);\n\tstruct drbd_connection *connection = peer_device->connection;\n\tbool do_send_unplug = req->rq_state & RQ_UNPLUG;\n\tint err;\n\n\tif (unlikely(cancel)) {\n\t\treq_mod(req, SEND_CANCELED, peer_device);\n\t\treturn 0;\n\t}\n\treq->pre_send_jif = jiffies;\n\n\tre_init_if_first_write(connection, req->epoch);\n\tmaybe_send_barrier(connection, req->epoch);\n\tconnection->send.current_epoch_writes++;\n\n\terr = drbd_send_dblock(peer_device, req);\n\treq_mod(req, err ? SEND_FAILED : HANDED_OVER_TO_NETWORK, peer_device);\n\n\tif (do_send_unplug && !err)\n\t\tpd_send_unplug_remote(peer_device);\n\n\treturn err;\n}\n\n \nint w_send_read_req(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_request *req = container_of(w, struct drbd_request, w);\n\tstruct drbd_device *device = req->device;\n\tstruct drbd_peer_device *const peer_device = first_peer_device(device);\n\tstruct drbd_connection *connection = peer_device->connection;\n\tbool do_send_unplug = req->rq_state & RQ_UNPLUG;\n\tint err;\n\n\tif (unlikely(cancel)) {\n\t\treq_mod(req, SEND_CANCELED, peer_device);\n\t\treturn 0;\n\t}\n\treq->pre_send_jif = jiffies;\n\n\t \n\tmaybe_send_barrier(connection, req->epoch);\n\n\terr = drbd_send_drequest(peer_device, P_DATA_REQUEST, req->i.sector, req->i.size,\n\t\t\t\t (unsigned long)req);\n\n\treq_mod(req, err ? SEND_FAILED : HANDED_OVER_TO_NETWORK, peer_device);\n\n\tif (do_send_unplug && !err)\n\t\tpd_send_unplug_remote(peer_device);\n\n\treturn err;\n}\n\nint w_restart_disk_io(struct drbd_work *w, int cancel)\n{\n\tstruct drbd_request *req = container_of(w, struct drbd_request, w);\n\tstruct drbd_device *device = req->device;\n\n\tif (bio_data_dir(req->master_bio) == WRITE && req->rq_state & RQ_IN_ACT_LOG)\n\t\tdrbd_al_begin_io(device, &req->i);\n\n\treq->private_bio = bio_alloc_clone(device->ldev->backing_bdev,\n\t\t\t\t\t   req->master_bio, GFP_NOIO,\n\t\t\t\t\t  &drbd_io_bio_set);\n\treq->private_bio->bi_private = req;\n\treq->private_bio->bi_end_io = drbd_request_endio;\n\tsubmit_bio_noacct(req->private_bio);\n\n\treturn 0;\n}\n\nstatic int _drbd_may_sync_now(struct drbd_device *device)\n{\n\tstruct drbd_device *odev = device;\n\tint resync_after;\n\n\twhile (1) {\n\t\tif (!odev->ldev || odev->state.disk == D_DISKLESS)\n\t\t\treturn 1;\n\t\trcu_read_lock();\n\t\tresync_after = rcu_dereference(odev->ldev->disk_conf)->resync_after;\n\t\trcu_read_unlock();\n\t\tif (resync_after == -1)\n\t\t\treturn 1;\n\t\todev = minor_to_device(resync_after);\n\t\tif (!odev)\n\t\t\treturn 1;\n\t\tif ((odev->state.conn >= C_SYNC_SOURCE &&\n\t\t     odev->state.conn <= C_PAUSED_SYNC_T) ||\n\t\t    odev->state.aftr_isp || odev->state.peer_isp ||\n\t\t    odev->state.user_isp)\n\t\t\treturn 0;\n\t}\n}\n\n \nstatic bool drbd_pause_after(struct drbd_device *device)\n{\n\tbool changed = false;\n\tstruct drbd_device *odev;\n\tint i;\n\n\trcu_read_lock();\n\tidr_for_each_entry(&drbd_devices, odev, i) {\n\t\tif (odev->state.conn == C_STANDALONE && odev->state.disk == D_DISKLESS)\n\t\t\tcontinue;\n\t\tif (!_drbd_may_sync_now(odev) &&\n\t\t    _drbd_set_state(_NS(odev, aftr_isp, 1),\n\t\t\t\t    CS_HARD, NULL) != SS_NOTHING_TO_DO)\n\t\t\tchanged = true;\n\t}\n\trcu_read_unlock();\n\n\treturn changed;\n}\n\n \nstatic bool drbd_resume_next(struct drbd_device *device)\n{\n\tbool changed = false;\n\tstruct drbd_device *odev;\n\tint i;\n\n\trcu_read_lock();\n\tidr_for_each_entry(&drbd_devices, odev, i) {\n\t\tif (odev->state.conn == C_STANDALONE && odev->state.disk == D_DISKLESS)\n\t\t\tcontinue;\n\t\tif (odev->state.aftr_isp) {\n\t\t\tif (_drbd_may_sync_now(odev) &&\n\t\t\t    _drbd_set_state(_NS(odev, aftr_isp, 0),\n\t\t\t\t\t    CS_HARD, NULL) != SS_NOTHING_TO_DO)\n\t\t\t\tchanged = true;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn changed;\n}\n\nvoid resume_next_sg(struct drbd_device *device)\n{\n\tlock_all_resources();\n\tdrbd_resume_next(device);\n\tunlock_all_resources();\n}\n\nvoid suspend_other_sg(struct drbd_device *device)\n{\n\tlock_all_resources();\n\tdrbd_pause_after(device);\n\tunlock_all_resources();\n}\n\n \nenum drbd_ret_code drbd_resync_after_valid(struct drbd_device *device, int o_minor)\n{\n\tstruct drbd_device *odev;\n\tint resync_after;\n\n\tif (o_minor == -1)\n\t\treturn NO_ERROR;\n\tif (o_minor < -1 || o_minor > MINORMASK)\n\t\treturn ERR_RESYNC_AFTER;\n\n\t \n\todev = minor_to_device(o_minor);\n\twhile (1) {\n\t\tif (odev == device)\n\t\t\treturn ERR_RESYNC_AFTER_CYCLE;\n\n\t\t \n\t\tif (!odev || !odev->ldev || odev->state.disk == D_DISKLESS)\n\t\t\treturn NO_ERROR;\n\n\t\trcu_read_lock();\n\t\tresync_after = rcu_dereference(odev->ldev->disk_conf)->resync_after;\n\t\trcu_read_unlock();\n\t\t \n\t\tif (resync_after == -1)\n\t\t\treturn NO_ERROR;\n\n\t\t \n\t\todev = minor_to_device(resync_after);\n\t}\n}\n\n \nvoid drbd_resync_after_changed(struct drbd_device *device)\n{\n\tint changed;\n\n\tdo {\n\t\tchanged  = drbd_pause_after(device);\n\t\tchanged |= drbd_resume_next(device);\n\t} while (changed);\n}\n\nvoid drbd_rs_controller_reset(struct drbd_peer_device *peer_device)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tstruct gendisk *disk = device->ldev->backing_bdev->bd_disk;\n\tstruct fifo_buffer *plan;\n\n\tatomic_set(&device->rs_sect_in, 0);\n\tatomic_set(&device->rs_sect_ev, 0);\n\tdevice->rs_in_flight = 0;\n\tdevice->rs_last_events =\n\t\t(int)part_stat_read_accum(disk->part0, sectors);\n\n\t \n\trcu_read_lock();\n\tplan = rcu_dereference(device->rs_plan_s);\n\tplan->total = 0;\n\tfifo_set(plan, 0);\n\trcu_read_unlock();\n}\n\nvoid start_resync_timer_fn(struct timer_list *t)\n{\n\tstruct drbd_device *device = from_timer(device, t, start_resync_timer);\n\tdrbd_device_post_work(device, RS_START);\n}\n\nstatic void do_start_resync(struct drbd_device *device)\n{\n\tif (atomic_read(&device->unacked_cnt) || atomic_read(&device->rs_pending_cnt)) {\n\t\tdrbd_warn(device, \"postponing start_resync ...\\n\");\n\t\tdevice->start_resync_timer.expires = jiffies + HZ/10;\n\t\tadd_timer(&device->start_resync_timer);\n\t\treturn;\n\t}\n\n\tdrbd_start_resync(device, C_SYNC_SOURCE);\n\tclear_bit(AHEAD_TO_SYNC_SOURCE, &device->flags);\n}\n\nstatic bool use_checksum_based_resync(struct drbd_connection *connection, struct drbd_device *device)\n{\n\tbool csums_after_crash_only;\n\trcu_read_lock();\n\tcsums_after_crash_only = rcu_dereference(connection->net_conf)->csums_after_crash_only;\n\trcu_read_unlock();\n\treturn connection->agreed_pro_version >= 89 &&\t\t \n\t\tconnection->csums_tfm &&\t\t\t \n\t\t(csums_after_crash_only == false\t\t \n\t\t || test_bit(CRASHED_PRIMARY, &device->flags));\t \n}\n\n \nvoid drbd_start_resync(struct drbd_device *device, enum drbd_conns side)\n{\n\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\tstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\n\tunion drbd_state ns;\n\tint r;\n\n\tif (device->state.conn >= C_SYNC_SOURCE && device->state.conn < C_AHEAD) {\n\t\tdrbd_err(device, \"Resync already running!\\n\");\n\t\treturn;\n\t}\n\n\tif (!connection) {\n\t\tdrbd_err(device, \"No connection to peer, aborting!\\n\");\n\t\treturn;\n\t}\n\n\tif (!test_bit(B_RS_H_DONE, &device->flags)) {\n\t\tif (side == C_SYNC_TARGET) {\n\t\t\t \n\t\t\tr = drbd_khelper(device, \"before-resync-target\");\n\t\t\tr = (r >> 8) & 0xff;\n\t\t\tif (r > 0) {\n\t\t\t\tdrbd_info(device, \"before-resync-target handler returned %d, \"\n\t\t\t\t\t \"dropping connection.\\n\", r);\n\t\t\t\tconn_request_state(connection, NS(conn, C_DISCONNECTING), CS_HARD);\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else   {\n\t\t\tr = drbd_khelper(device, \"before-resync-source\");\n\t\t\tr = (r >> 8) & 0xff;\n\t\t\tif (r > 0) {\n\t\t\t\tif (r == 3) {\n\t\t\t\t\tdrbd_info(device, \"before-resync-source handler returned %d, \"\n\t\t\t\t\t\t \"ignoring. Old userland tools?\", r);\n\t\t\t\t} else {\n\t\t\t\t\tdrbd_info(device, \"before-resync-source handler returned %d, \"\n\t\t\t\t\t\t \"dropping connection.\\n\", r);\n\t\t\t\t\tconn_request_state(connection,\n\t\t\t\t\t\t\t   NS(conn, C_DISCONNECTING), CS_HARD);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (current == connection->worker.task) {\n\t\t \n\t\tif (!mutex_trylock(device->state_mutex)) {\n\t\t\tset_bit(B_RS_H_DONE, &device->flags);\n\t\t\tdevice->start_resync_timer.expires = jiffies + HZ/5;\n\t\t\tadd_timer(&device->start_resync_timer);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tmutex_lock(device->state_mutex);\n\t}\n\n\tlock_all_resources();\n\tclear_bit(B_RS_H_DONE, &device->flags);\n\t \n\tif (device->state.conn < C_CONNECTED\n\t|| !get_ldev_if_state(device, D_NEGOTIATING)) {\n\t\tunlock_all_resources();\n\t\tgoto out;\n\t}\n\n\tns = drbd_read_state(device);\n\n\tns.aftr_isp = !_drbd_may_sync_now(device);\n\n\tns.conn = side;\n\n\tif (side == C_SYNC_TARGET)\n\t\tns.disk = D_INCONSISTENT;\n\telse  \n\t\tns.pdsk = D_INCONSISTENT;\n\n\tr = _drbd_set_state(device, ns, CS_VERBOSE, NULL);\n\tns = drbd_read_state(device);\n\n\tif (ns.conn < C_CONNECTED)\n\t\tr = SS_UNKNOWN_ERROR;\n\n\tif (r == SS_SUCCESS) {\n\t\tunsigned long tw = drbd_bm_total_weight(device);\n\t\tunsigned long now = jiffies;\n\t\tint i;\n\n\t\tdevice->rs_failed    = 0;\n\t\tdevice->rs_paused    = 0;\n\t\tdevice->rs_same_csum = 0;\n\t\tdevice->rs_last_sect_ev = 0;\n\t\tdevice->rs_total     = tw;\n\t\tdevice->rs_start     = now;\n\t\tfor (i = 0; i < DRBD_SYNC_MARKS; i++) {\n\t\t\tdevice->rs_mark_left[i] = tw;\n\t\t\tdevice->rs_mark_time[i] = now;\n\t\t}\n\t\tdrbd_pause_after(device);\n\t\t \n\t\tspin_lock(&device->al_lock);\n\t\tlc_reset(device->resync);\n\t\tdevice->resync_locked = 0;\n\t\tdevice->resync_wenr = LC_FREE;\n\t\tspin_unlock(&device->al_lock);\n\t}\n\tunlock_all_resources();\n\n\tif (r == SS_SUCCESS) {\n\t\twake_up(&device->al_wait);  \n\t\t \n\t\tdevice->rs_last_bcast = jiffies - HZ;\n\n\t\tdrbd_info(device, \"Began resync as %s (will sync %lu KB [%lu bits set]).\\n\",\n\t\t     drbd_conn_str(ns.conn),\n\t\t     (unsigned long) device->rs_total << (BM_BLOCK_SHIFT-10),\n\t\t     (unsigned long) device->rs_total);\n\t\tif (side == C_SYNC_TARGET) {\n\t\t\tdevice->bm_resync_fo = 0;\n\t\t\tdevice->use_csums = use_checksum_based_resync(connection, device);\n\t\t} else {\n\t\t\tdevice->use_csums = false;\n\t\t}\n\n\t\t \n\t\tif (side == C_SYNC_SOURCE && connection->agreed_pro_version < 96)\n\t\t\tdrbd_gen_and_send_sync_uuid(peer_device);\n\n\t\tif (connection->agreed_pro_version < 95 && device->rs_total == 0) {\n\t\t\t \n\t\t\tif (side == C_SYNC_SOURCE) {\n\t\t\t\tstruct net_conf *nc;\n\t\t\t\tint timeo;\n\n\t\t\t\trcu_read_lock();\n\t\t\t\tnc = rcu_dereference(connection->net_conf);\n\t\t\t\ttimeo = nc->ping_int * HZ + nc->ping_timeo * HZ / 9;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tschedule_timeout_interruptible(timeo);\n\t\t\t}\n\t\t\tdrbd_resync_finished(peer_device);\n\t\t}\n\n\t\tdrbd_rs_controller_reset(peer_device);\n\t\t \n\t\tif (ns.conn == C_SYNC_TARGET)\n\t\t\tmod_timer(&device->resync_timer, jiffies);\n\n\t\tdrbd_md_sync(device);\n\t}\n\tput_ldev(device);\nout:\n\tmutex_unlock(device->state_mutex);\n}\n\nstatic void update_on_disk_bitmap(struct drbd_peer_device *peer_device, bool resync_done)\n{\n\tstruct drbd_device *device = peer_device->device;\n\tstruct sib_info sib = { .sib_reason = SIB_SYNC_PROGRESS, };\n\tdevice->rs_last_bcast = jiffies;\n\n\tif (!get_ldev(device))\n\t\treturn;\n\n\tdrbd_bm_write_lazy(device, 0);\n\tif (resync_done && is_sync_state(device->state.conn))\n\t\tdrbd_resync_finished(peer_device);\n\n\tdrbd_bcast_event(device, &sib);\n\t \n\tdevice->rs_last_bcast = jiffies;\n\tput_ldev(device);\n}\n\nstatic void drbd_ldev_destroy(struct drbd_device *device)\n{\n\tlc_destroy(device->resync);\n\tdevice->resync = NULL;\n\tlc_destroy(device->act_log);\n\tdevice->act_log = NULL;\n\n\t__acquire(local);\n\tdrbd_backing_dev_free(device, device->ldev);\n\tdevice->ldev = NULL;\n\t__release(local);\n\n\tclear_bit(GOING_DISKLESS, &device->flags);\n\twake_up(&device->misc_wait);\n}\n\nstatic void go_diskless(struct drbd_device *device)\n{\n\tstruct drbd_peer_device *peer_device = first_peer_device(device);\n\tD_ASSERT(device, device->state.disk == D_FAILED);\n\t \n\n\t \n\tif (device->bitmap && device->ldev) {\n\t\t \n\t\tif (drbd_bitmap_io_from_worker(device, drbd_bm_write,\n\t\t\t\t\t\"detach\", BM_LOCKED_TEST_ALLOWED, peer_device)) {\n\t\t\tif (test_bit(WAS_READ_ERROR, &device->flags)) {\n\t\t\t\tdrbd_md_set_flag(device, MDF_FULL_SYNC);\n\t\t\t\tdrbd_md_sync(device);\n\t\t\t}\n\t\t}\n\t}\n\n\tdrbd_force_state(device, NS(disk, D_DISKLESS));\n}\n\nstatic int do_md_sync(struct drbd_device *device)\n{\n\tdrbd_warn(device, \"md_sync_timer expired! Worker calls drbd_md_sync().\\n\");\n\tdrbd_md_sync(device);\n\treturn 0;\n}\n\n \nvoid __update_timing_details(\n\t\tstruct drbd_thread_timing_details *tdp,\n\t\tunsigned int *cb_nr,\n\t\tvoid *cb,\n\t\tconst char *fn, const unsigned int line)\n{\n\tunsigned int i = *cb_nr % DRBD_THREAD_DETAILS_HIST;\n\tstruct drbd_thread_timing_details *td = tdp + i;\n\n\ttd->start_jif = jiffies;\n\ttd->cb_addr = cb;\n\ttd->caller_fn = fn;\n\ttd->line = line;\n\ttd->cb_nr = *cb_nr;\n\n\ti = (i+1) % DRBD_THREAD_DETAILS_HIST;\n\ttd = tdp + i;\n\tmemset(td, 0, sizeof(*td));\n\n\t++(*cb_nr);\n}\n\nstatic void do_device_work(struct drbd_device *device, const unsigned long todo)\n{\n\tif (test_bit(MD_SYNC, &todo))\n\t\tdo_md_sync(device);\n\tif (test_bit(RS_DONE, &todo) ||\n\t    test_bit(RS_PROGRESS, &todo))\n\t\tupdate_on_disk_bitmap(first_peer_device(device), test_bit(RS_DONE, &todo));\n\tif (test_bit(GO_DISKLESS, &todo))\n\t\tgo_diskless(device);\n\tif (test_bit(DESTROY_DISK, &todo))\n\t\tdrbd_ldev_destroy(device);\n\tif (test_bit(RS_START, &todo))\n\t\tdo_start_resync(device);\n}\n\n#define DRBD_DEVICE_WORK_MASK\t\\\n\t((1UL << GO_DISKLESS)\t\\\n\t|(1UL << DESTROY_DISK)\t\\\n\t|(1UL << MD_SYNC)\t\\\n\t|(1UL << RS_START)\t\\\n\t|(1UL << RS_PROGRESS)\t\\\n\t|(1UL << RS_DONE)\t\\\n\t)\n\nstatic unsigned long get_work_bits(unsigned long *flags)\n{\n\tunsigned long old, new;\n\tdo {\n\t\told = *flags;\n\t\tnew = old & ~DRBD_DEVICE_WORK_MASK;\n\t} while (cmpxchg(flags, old, new) != old);\n\treturn old & DRBD_DEVICE_WORK_MASK;\n}\n\nstatic void do_unqueued_work(struct drbd_connection *connection)\n{\n\tstruct drbd_peer_device *peer_device;\n\tint vnr;\n\n\trcu_read_lock();\n\tidr_for_each_entry(&connection->peer_devices, peer_device, vnr) {\n\t\tstruct drbd_device *device = peer_device->device;\n\t\tunsigned long todo = get_work_bits(&device->flags);\n\t\tif (!todo)\n\t\t\tcontinue;\n\n\t\tkref_get(&device->kref);\n\t\trcu_read_unlock();\n\t\tdo_device_work(device, todo);\n\t\tkref_put(&device->kref, drbd_destroy_device);\n\t\trcu_read_lock();\n\t}\n\trcu_read_unlock();\n}\n\nstatic bool dequeue_work_batch(struct drbd_work_queue *queue, struct list_head *work_list)\n{\n\tspin_lock_irq(&queue->q_lock);\n\tlist_splice_tail_init(&queue->q, work_list);\n\tspin_unlock_irq(&queue->q_lock);\n\treturn !list_empty(work_list);\n}\n\nstatic void wait_for_work(struct drbd_connection *connection, struct list_head *work_list)\n{\n\tDEFINE_WAIT(wait);\n\tstruct net_conf *nc;\n\tint uncork, cork;\n\n\tdequeue_work_batch(&connection->sender_work, work_list);\n\tif (!list_empty(work_list))\n\t\treturn;\n\n\t \n\trcu_read_lock();\n\tnc = rcu_dereference(connection->net_conf);\n\tuncork = nc ? nc->tcp_cork : 0;\n\trcu_read_unlock();\n\tif (uncork) {\n\t\tmutex_lock(&connection->data.mutex);\n\t\tif (connection->data.socket)\n\t\t\ttcp_sock_set_cork(connection->data.socket->sk, false);\n\t\tmutex_unlock(&connection->data.mutex);\n\t}\n\n\tfor (;;) {\n\t\tint send_barrier;\n\t\tprepare_to_wait(&connection->sender_work.q_wait, &wait, TASK_INTERRUPTIBLE);\n\t\tspin_lock_irq(&connection->resource->req_lock);\n\t\tspin_lock(&connection->sender_work.q_lock);\t \n\t\tif (!list_empty(&connection->sender_work.q))\n\t\t\tlist_splice_tail_init(&connection->sender_work.q, work_list);\n\t\tspin_unlock(&connection->sender_work.q_lock);\t \n\t\tif (!list_empty(work_list) || signal_pending(current)) {\n\t\t\tspin_unlock_irq(&connection->resource->req_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tsend_barrier =\n\t\t\tatomic_read(&connection->current_tle_nr) !=\n\t\t\tconnection->send.current_epoch_nr;\n\t\tspin_unlock_irq(&connection->resource->req_lock);\n\n\t\tif (send_barrier)\n\t\t\tmaybe_send_barrier(connection,\n\t\t\t\t\tconnection->send.current_epoch_nr + 1);\n\n\t\tif (test_bit(DEVICE_WORK_PENDING, &connection->flags))\n\t\t\tbreak;\n\n\t\t \n\t\tif (get_t_state(&connection->worker) != RUNNING)\n\t\t\tbreak;\n\n\t\tschedule();\n\t\t \n\t}\n\tfinish_wait(&connection->sender_work.q_wait, &wait);\n\n\t \n\trcu_read_lock();\n\tnc = rcu_dereference(connection->net_conf);\n\tcork = nc ? nc->tcp_cork : 0;\n\trcu_read_unlock();\n\tmutex_lock(&connection->data.mutex);\n\tif (connection->data.socket) {\n\t\tif (cork)\n\t\t\ttcp_sock_set_cork(connection->data.socket->sk, true);\n\t\telse if (!uncork)\n\t\t\ttcp_sock_set_cork(connection->data.socket->sk, false);\n\t}\n\tmutex_unlock(&connection->data.mutex);\n}\n\nint drbd_worker(struct drbd_thread *thi)\n{\n\tstruct drbd_connection *connection = thi->connection;\n\tstruct drbd_work *w = NULL;\n\tstruct drbd_peer_device *peer_device;\n\tLIST_HEAD(work_list);\n\tint vnr;\n\n\twhile (get_t_state(thi) == RUNNING) {\n\t\tdrbd_thread_current_set_cpu(thi);\n\n\t\tif (list_empty(&work_list)) {\n\t\t\tupdate_worker_timing_details(connection, wait_for_work);\n\t\t\twait_for_work(connection, &work_list);\n\t\t}\n\n\t\tif (test_and_clear_bit(DEVICE_WORK_PENDING, &connection->flags)) {\n\t\t\tupdate_worker_timing_details(connection, do_unqueued_work);\n\t\t\tdo_unqueued_work(connection);\n\t\t}\n\n\t\tif (signal_pending(current)) {\n\t\t\tflush_signals(current);\n\t\t\tif (get_t_state(thi) == RUNNING) {\n\t\t\t\tdrbd_warn(connection, \"Worker got an unexpected signal\\n\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tif (get_t_state(thi) != RUNNING)\n\t\t\tbreak;\n\n\t\tif (!list_empty(&work_list)) {\n\t\t\tw = list_first_entry(&work_list, struct drbd_work, list);\n\t\t\tlist_del_init(&w->list);\n\t\t\tupdate_worker_timing_details(connection, w->cb);\n\t\t\tif (w->cb(w, connection->cstate < C_WF_REPORT_PARAMS) == 0)\n\t\t\t\tcontinue;\n\t\t\tif (connection->cstate >= C_WF_REPORT_PARAMS)\n\t\t\t\tconn_request_state(connection, NS(conn, C_NETWORK_FAILURE), CS_HARD);\n\t\t}\n\t}\n\n\tdo {\n\t\tif (test_and_clear_bit(DEVICE_WORK_PENDING, &connection->flags)) {\n\t\t\tupdate_worker_timing_details(connection, do_unqueued_work);\n\t\t\tdo_unqueued_work(connection);\n\t\t}\n\t\tif (!list_empty(&work_list)) {\n\t\t\tw = list_first_entry(&work_list, struct drbd_work, list);\n\t\t\tlist_del_init(&w->list);\n\t\t\tupdate_worker_timing_details(connection, w->cb);\n\t\t\tw->cb(w, 1);\n\t\t} else\n\t\t\tdequeue_work_batch(&connection->sender_work, &work_list);\n\t} while (!list_empty(&work_list) || test_bit(DEVICE_WORK_PENDING, &connection->flags));\n\n\trcu_read_lock();\n\tidr_for_each_entry(&connection->peer_devices, peer_device, vnr) {\n\t\tstruct drbd_device *device = peer_device->device;\n\t\tD_ASSERT(device, device->state.disk == D_DISKLESS && device->state.conn == C_STANDALONE);\n\t\tkref_get(&device->kref);\n\t\trcu_read_unlock();\n\t\tdrbd_device_cleanup(device);\n\t\tkref_put(&device->kref, drbd_destroy_device);\n\t\trcu_read_lock();\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}