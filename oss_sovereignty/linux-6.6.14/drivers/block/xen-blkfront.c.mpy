{
  "module_name": "xen-blkfront.c",
  "hash_id": "1ae5135cf40c0e8f062266694c14d2d9f38ff6a82770ccbed72389eee8f7202d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/xen-blkfront.c",
  "human_readable_source": " \n\n#include <linux/interrupt.h>\n#include <linux/blkdev.h>\n#include <linux/blk-mq.h>\n#include <linux/hdreg.h>\n#include <linux/cdrom.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/major.h>\n#include <linux/mutex.h>\n#include <linux/scatterlist.h>\n#include <linux/bitmap.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/sched/mm.h>\n\n#include <xen/xen.h>\n#include <xen/xenbus.h>\n#include <xen/grant_table.h>\n#include <xen/events.h>\n#include <xen/page.h>\n#include <xen/platform_pci.h>\n\n#include <xen/interface/grant_table.h>\n#include <xen/interface/io/blkif.h>\n#include <xen/interface/io/protocols.h>\n\n#include <asm/xen/hypervisor.h>\n\n \n#define HAS_EXTRA_REQ (BLKIF_MAX_SEGMENTS_PER_REQUEST < XEN_PFN_PER_PAGE)\n\nenum blkif_state {\n\tBLKIF_STATE_DISCONNECTED,\n\tBLKIF_STATE_CONNECTED,\n\tBLKIF_STATE_SUSPENDED,\n\tBLKIF_STATE_ERROR,\n};\n\nstruct grant {\n\tgrant_ref_t gref;\n\tstruct page *page;\n\tstruct list_head node;\n};\n\nenum blk_req_status {\n\tREQ_PROCESSING,\n\tREQ_WAITING,\n\tREQ_DONE,\n\tREQ_ERROR,\n\tREQ_EOPNOTSUPP,\n};\n\nstruct blk_shadow {\n\tstruct blkif_request req;\n\tstruct request *request;\n\tstruct grant **grants_used;\n\tstruct grant **indirect_grants;\n\tstruct scatterlist *sg;\n\tunsigned int num_sg;\n\tenum blk_req_status status;\n\n\t#define NO_ASSOCIATED_ID ~0UL\n\t \n\tunsigned long associated_id;\n};\n\nstruct blkif_req {\n\tblk_status_t\terror;\n};\n\nstatic inline struct blkif_req *blkif_req(struct request *rq)\n{\n\treturn blk_mq_rq_to_pdu(rq);\n}\n\nstatic DEFINE_MUTEX(blkfront_mutex);\nstatic const struct block_device_operations xlvbd_block_fops;\nstatic struct delayed_work blkfront_work;\nstatic LIST_HEAD(info_list);\n\n \n\nstatic unsigned int xen_blkif_max_segments = 32;\nmodule_param_named(max_indirect_segments, xen_blkif_max_segments, uint, 0444);\nMODULE_PARM_DESC(max_indirect_segments,\n\t\t \"Maximum amount of segments in indirect requests (default is 32)\");\n\nstatic unsigned int xen_blkif_max_queues = 4;\nmodule_param_named(max_queues, xen_blkif_max_queues, uint, 0444);\nMODULE_PARM_DESC(max_queues, \"Maximum number of hardware queues/rings used per virtual disk\");\n\n \nstatic unsigned int xen_blkif_max_ring_order;\nmodule_param_named(max_ring_page_order, xen_blkif_max_ring_order, int, 0444);\nMODULE_PARM_DESC(max_ring_page_order, \"Maximum order of pages to be used for the shared ring\");\n\nstatic bool __read_mostly xen_blkif_trusted = true;\nmodule_param_named(trusted, xen_blkif_trusted, bool, 0644);\nMODULE_PARM_DESC(trusted, \"Is the backend trusted\");\n\n#define BLK_RING_SIZE(info)\t\\\n\t__CONST_RING_SIZE(blkif, XEN_PAGE_SIZE * (info)->nr_ring_pages)\n\n \n#define RINGREF_NAME_LEN (20)\n \n#define QUEUE_NAME_LEN (17)\n\n \nstruct blkfront_ring_info {\n\t \n\tspinlock_t ring_lock;\n\tstruct blkif_front_ring ring;\n\tunsigned int ring_ref[XENBUS_MAX_RING_GRANTS];\n\tunsigned int evtchn, irq;\n\tstruct work_struct work;\n\tstruct gnttab_free_callback callback;\n\tstruct list_head indirect_pages;\n\tstruct list_head grants;\n\tunsigned int persistent_gnts_c;\n\tunsigned long shadow_free;\n\tstruct blkfront_info *dev_info;\n\tstruct blk_shadow shadow[];\n};\n\n \nstruct blkfront_info\n{\n\tstruct mutex mutex;\n\tstruct xenbus_device *xbdev;\n\tstruct gendisk *gd;\n\tu16 sector_size;\n\tunsigned int physical_sector_size;\n\tunsigned long vdisk_info;\n\tint vdevice;\n\tblkif_vdev_t handle;\n\tenum blkif_state connected;\n\t \n\tunsigned int nr_ring_pages;\n\tstruct request_queue *rq;\n\tunsigned int feature_flush:1;\n\tunsigned int feature_fua:1;\n\tunsigned int feature_discard:1;\n\tunsigned int feature_secdiscard:1;\n\t \n\tunsigned int feature_persistent_parm:1;\n\t \n\tunsigned int feature_persistent:1;\n\tunsigned int bounce:1;\n\tunsigned int discard_granularity;\n\tunsigned int discard_alignment;\n\t \n\tunsigned int max_indirect_segments;\n\tint is_ready;\n\tstruct blk_mq_tag_set tag_set;\n\tstruct blkfront_ring_info *rinfo;\n\tunsigned int nr_rings;\n\tunsigned int rinfo_size;\n\t \n\tstruct list_head requests;\n\tstruct bio_list bio_list;\n\tstruct list_head info_list;\n};\n\nstatic unsigned int nr_minors;\nstatic unsigned long *minors;\nstatic DEFINE_SPINLOCK(minor_lock);\n\n#define PARTS_PER_DISK\t\t16\n#define PARTS_PER_EXT_DISK      256\n\n#define BLKIF_MAJOR(dev) ((dev)>>8)\n#define BLKIF_MINOR(dev) ((dev) & 0xff)\n\n#define EXT_SHIFT 28\n#define EXTENDED (1<<EXT_SHIFT)\n#define VDEV_IS_EXTENDED(dev) ((dev)&(EXTENDED))\n#define BLKIF_MINOR_EXT(dev) ((dev)&(~EXTENDED))\n#define EMULATED_HD_DISK_MINOR_OFFSET (0)\n#define EMULATED_HD_DISK_NAME_OFFSET (EMULATED_HD_DISK_MINOR_OFFSET / 256)\n#define EMULATED_SD_DISK_MINOR_OFFSET (0)\n#define EMULATED_SD_DISK_NAME_OFFSET (EMULATED_SD_DISK_MINOR_OFFSET / 256)\n\n#define DEV_NAME\t\"xvd\"\t \n\n \n#define GRANTS_PER_PSEG\t(PAGE_SIZE / XEN_PAGE_SIZE)\n\n#define GRANTS_PER_INDIRECT_FRAME \\\n\t(XEN_PAGE_SIZE / sizeof(struct blkif_request_segment))\n\n#define INDIRECT_GREFS(_grants)\t\t\\\n\tDIV_ROUND_UP(_grants, GRANTS_PER_INDIRECT_FRAME)\n\nstatic int blkfront_setup_indirect(struct blkfront_ring_info *rinfo);\nstatic void blkfront_gather_backend_features(struct blkfront_info *info);\nstatic int negotiate_mq(struct blkfront_info *info);\n\n#define for_each_rinfo(info, ptr, idx)\t\t\t\t\\\n\tfor ((ptr) = (info)->rinfo, (idx) = 0;\t\t\t\\\n\t     (idx) < (info)->nr_rings;\t\t\t\t\\\n\t     (idx)++, (ptr) = (void *)(ptr) + (info)->rinfo_size)\n\nstatic inline struct blkfront_ring_info *\nget_rinfo(const struct blkfront_info *info, unsigned int i)\n{\n\tBUG_ON(i >= info->nr_rings);\n\treturn (void *)info->rinfo + i * info->rinfo_size;\n}\n\nstatic int get_id_from_freelist(struct blkfront_ring_info *rinfo)\n{\n\tunsigned long free = rinfo->shadow_free;\n\n\tBUG_ON(free >= BLK_RING_SIZE(rinfo->dev_info));\n\trinfo->shadow_free = rinfo->shadow[free].req.u.rw.id;\n\trinfo->shadow[free].req.u.rw.id = 0x0fffffee;  \n\treturn free;\n}\n\nstatic int add_id_to_freelist(struct blkfront_ring_info *rinfo,\n\t\t\t      unsigned long id)\n{\n\tif (rinfo->shadow[id].req.u.rw.id != id)\n\t\treturn -EINVAL;\n\tif (rinfo->shadow[id].request == NULL)\n\t\treturn -EINVAL;\n\trinfo->shadow[id].req.u.rw.id  = rinfo->shadow_free;\n\trinfo->shadow[id].request = NULL;\n\trinfo->shadow_free = id;\n\treturn 0;\n}\n\nstatic int fill_grant_buffer(struct blkfront_ring_info *rinfo, int num)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct page *granted_page;\n\tstruct grant *gnt_list_entry, *n;\n\tint i = 0;\n\n\twhile (i < num) {\n\t\tgnt_list_entry = kzalloc(sizeof(struct grant), GFP_NOIO);\n\t\tif (!gnt_list_entry)\n\t\t\tgoto out_of_memory;\n\n\t\tif (info->bounce) {\n\t\t\tgranted_page = alloc_page(GFP_NOIO | __GFP_ZERO);\n\t\t\tif (!granted_page) {\n\t\t\t\tkfree(gnt_list_entry);\n\t\t\t\tgoto out_of_memory;\n\t\t\t}\n\t\t\tgnt_list_entry->page = granted_page;\n\t\t}\n\n\t\tgnt_list_entry->gref = INVALID_GRANT_REF;\n\t\tlist_add(&gnt_list_entry->node, &rinfo->grants);\n\t\ti++;\n\t}\n\n\treturn 0;\n\nout_of_memory:\n\tlist_for_each_entry_safe(gnt_list_entry, n,\n\t                         &rinfo->grants, node) {\n\t\tlist_del(&gnt_list_entry->node);\n\t\tif (info->bounce)\n\t\t\t__free_page(gnt_list_entry->page);\n\t\tkfree(gnt_list_entry);\n\t\ti--;\n\t}\n\tBUG_ON(i != 0);\n\treturn -ENOMEM;\n}\n\nstatic struct grant *get_free_grant(struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry;\n\n\tBUG_ON(list_empty(&rinfo->grants));\n\tgnt_list_entry = list_first_entry(&rinfo->grants, struct grant,\n\t\t\t\t\t  node);\n\tlist_del(&gnt_list_entry->node);\n\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\trinfo->persistent_gnts_c--;\n\n\treturn gnt_list_entry;\n}\n\nstatic inline void grant_foreign_access(const struct grant *gnt_list_entry,\n\t\t\t\t\tconst struct blkfront_info *info)\n{\n\tgnttab_page_grant_foreign_access_ref_one(gnt_list_entry->gref,\n\t\t\t\t\t\t info->xbdev->otherend_id,\n\t\t\t\t\t\t gnt_list_entry->page,\n\t\t\t\t\t\t 0);\n}\n\nstatic struct grant *get_grant(grant_ref_t *gref_head,\n\t\t\t       unsigned long gfn,\n\t\t\t       struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\n\t \n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (info->bounce)\n\t\tgrant_foreign_access(gnt_list_entry, info);\n\telse {\n\t\t \n\t\tgnttab_grant_foreign_access_ref(gnt_list_entry->gref,\n\t\t\t\t\t\tinfo->xbdev->otherend_id,\n\t\t\t\t\t\tgfn, 0);\n\t}\n\n\treturn gnt_list_entry;\n}\n\nstatic struct grant *get_indirect_grant(grant_ref_t *gref_head,\n\t\t\t\t\tstruct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\n\t \n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (!info->bounce) {\n\t\tstruct page *indirect_page;\n\n\t\t \n\t\tBUG_ON(list_empty(&rinfo->indirect_pages));\n\t\tindirect_page = list_first_entry(&rinfo->indirect_pages,\n\t\t\t\t\t\t struct page, lru);\n\t\tlist_del(&indirect_page->lru);\n\t\tgnt_list_entry->page = indirect_page;\n\t}\n\tgrant_foreign_access(gnt_list_entry, info);\n\n\treturn gnt_list_entry;\n}\n\nstatic const char *op_name(int op)\n{\n\tstatic const char *const names[] = {\n\t\t[BLKIF_OP_READ] = \"read\",\n\t\t[BLKIF_OP_WRITE] = \"write\",\n\t\t[BLKIF_OP_WRITE_BARRIER] = \"barrier\",\n\t\t[BLKIF_OP_FLUSH_DISKCACHE] = \"flush\",\n\t\t[BLKIF_OP_DISCARD] = \"discard\" };\n\n\tif (op < 0 || op >= ARRAY_SIZE(names))\n\t\treturn \"unknown\";\n\n\tif (!names[op])\n\t\treturn \"reserved\";\n\n\treturn names[op];\n}\nstatic int xlbd_reserve_minors(unsigned int minor, unsigned int nr)\n{\n\tunsigned int end = minor + nr;\n\tint rc;\n\n\tif (end > nr_minors) {\n\t\tunsigned long *bitmap, *old;\n\n\t\tbitmap = kcalloc(BITS_TO_LONGS(end), sizeof(*bitmap),\n\t\t\t\t GFP_KERNEL);\n\t\tif (bitmap == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tspin_lock(&minor_lock);\n\t\tif (end > nr_minors) {\n\t\t\told = minors;\n\t\t\tmemcpy(bitmap, minors,\n\t\t\t       BITS_TO_LONGS(nr_minors) * sizeof(*bitmap));\n\t\t\tminors = bitmap;\n\t\t\tnr_minors = BITS_TO_LONGS(end) * BITS_PER_LONG;\n\t\t} else\n\t\t\told = bitmap;\n\t\tspin_unlock(&minor_lock);\n\t\tkfree(old);\n\t}\n\n\tspin_lock(&minor_lock);\n\tif (find_next_bit(minors, end, minor) >= end) {\n\t\tbitmap_set(minors, minor, nr);\n\t\trc = 0;\n\t} else\n\t\trc = -EBUSY;\n\tspin_unlock(&minor_lock);\n\n\treturn rc;\n}\n\nstatic void xlbd_release_minors(unsigned int minor, unsigned int nr)\n{\n\tunsigned int end = minor + nr;\n\n\tBUG_ON(end > nr_minors);\n\tspin_lock(&minor_lock);\n\tbitmap_clear(minors,  minor, nr);\n\tspin_unlock(&minor_lock);\n}\n\nstatic void blkif_restart_queue_callback(void *arg)\n{\n\tstruct blkfront_ring_info *rinfo = (struct blkfront_ring_info *)arg;\n\tschedule_work(&rinfo->work);\n}\n\nstatic int blkif_getgeo(struct block_device *bd, struct hd_geometry *hg)\n{\n\t \n\tsector_t nsect = get_capacity(bd->bd_disk);\n\tsector_t cylinders = nsect;\n\n\thg->heads = 0xff;\n\thg->sectors = 0x3f;\n\tsector_div(cylinders, hg->heads * hg->sectors);\n\thg->cylinders = cylinders;\n\tif ((sector_t)(hg->cylinders + 1) * hg->heads * hg->sectors < nsect)\n\t\thg->cylinders = 0xffff;\n\treturn 0;\n}\n\nstatic int blkif_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\t       unsigned command, unsigned long argument)\n{\n\tstruct blkfront_info *info = bdev->bd_disk->private_data;\n\tint i;\n\n\tswitch (command) {\n\tcase CDROMMULTISESSION:\n\t\tfor (i = 0; i < sizeof(struct cdrom_multisession); i++)\n\t\t\tif (put_user(0, (char __user *)(argument + i)))\n\t\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase CDROM_GET_CAPABILITY:\n\t\tif (!(info->vdisk_info & VDISK_CDROM))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic unsigned long blkif_ring_get_request(struct blkfront_ring_info *rinfo,\n\t\t\t\t\t    struct request *req,\n\t\t\t\t\t    struct blkif_request **ring_req)\n{\n\tunsigned long id;\n\n\t*ring_req = RING_GET_REQUEST(&rinfo->ring, rinfo->ring.req_prod_pvt);\n\trinfo->ring.req_prod_pvt++;\n\n\tid = get_id_from_freelist(rinfo);\n\trinfo->shadow[id].request = req;\n\trinfo->shadow[id].status = REQ_PROCESSING;\n\trinfo->shadow[id].associated_id = NO_ASSOCIATED_ID;\n\n\trinfo->shadow[id].req.u.rw.id = id;\n\n\treturn id;\n}\n\nstatic int blkif_queue_discard_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *final_ring_req;\n\tunsigned long id;\n\n\t \n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tring_req->operation = BLKIF_OP_DISCARD;\n\tring_req->u.discard.nr_sectors = blk_rq_sectors(req);\n\tring_req->u.discard.id = id;\n\tring_req->u.discard.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\tif (req_op(req) == REQ_OP_SECURE_ERASE && info->feature_secdiscard)\n\t\tring_req->u.discard.flag = BLKIF_DISCARD_SECURE;\n\telse\n\t\tring_req->u.discard.flag = 0;\n\n\t \n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\n\treturn 0;\n}\n\nstruct setup_rw_req {\n\tunsigned int grant_idx;\n\tstruct blkif_request_segment *segments;\n\tstruct blkfront_ring_info *rinfo;\n\tstruct blkif_request *ring_req;\n\tgrant_ref_t gref_head;\n\tunsigned int id;\n\t \n\tbool need_copy;\n\tunsigned int bvec_off;\n\tchar *bvec_data;\n\n\tbool require_extra_req;\n\tstruct blkif_request *extra_ring_req;\n};\n\nstatic void blkif_setup_rw_req_grant(unsigned long gfn, unsigned int offset,\n\t\t\t\t     unsigned int len, void *data)\n{\n\tstruct setup_rw_req *setup = data;\n\tint n, ref;\n\tstruct grant *gnt_list_entry;\n\tunsigned int fsect, lsect;\n\t \n\tunsigned int grant_idx = setup->grant_idx;\n\tstruct blkif_request *ring_req = setup->ring_req;\n\tstruct blkfront_ring_info *rinfo = setup->rinfo;\n\t \n\tstruct blk_shadow *shadow = &rinfo->shadow[setup->id];\n\n\tif (unlikely(setup->require_extra_req &&\n\t\t     grant_idx >= BLKIF_MAX_SEGMENTS_PER_REQUEST)) {\n\t\t \n\t\tgrant_idx -= BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\tring_req = setup->extra_ring_req;\n\t}\n\n\tif ((ring_req->operation == BLKIF_OP_INDIRECT) &&\n\t    (grant_idx % GRANTS_PER_INDIRECT_FRAME == 0)) {\n\t\tif (setup->segments)\n\t\t\tkunmap_atomic(setup->segments);\n\n\t\tn = grant_idx / GRANTS_PER_INDIRECT_FRAME;\n\t\tgnt_list_entry = get_indirect_grant(&setup->gref_head, rinfo);\n\t\tshadow->indirect_grants[n] = gnt_list_entry;\n\t\tsetup->segments = kmap_atomic(gnt_list_entry->page);\n\t\tring_req->u.indirect.indirect_grefs[n] = gnt_list_entry->gref;\n\t}\n\n\tgnt_list_entry = get_grant(&setup->gref_head, gfn, rinfo);\n\tref = gnt_list_entry->gref;\n\t \n\tshadow->grants_used[setup->grant_idx] = gnt_list_entry;\n\n\tif (setup->need_copy) {\n\t\tvoid *shared_data;\n\n\t\tshared_data = kmap_atomic(gnt_list_entry->page);\n\t\t \n\t\tmemcpy(shared_data + offset,\n\t\t       setup->bvec_data + setup->bvec_off,\n\t\t       len);\n\n\t\tkunmap_atomic(shared_data);\n\t\tsetup->bvec_off += len;\n\t}\n\n\tfsect = offset >> 9;\n\tlsect = fsect + (len >> 9) - 1;\n\tif (ring_req->operation != BLKIF_OP_INDIRECT) {\n\t\tring_req->u.rw.seg[grant_idx] =\n\t\t\t(struct blkif_request_segment) {\n\t\t\t\t.gref       = ref,\n\t\t\t\t.first_sect = fsect,\n\t\t\t\t.last_sect  = lsect };\n\t} else {\n\t\tsetup->segments[grant_idx % GRANTS_PER_INDIRECT_FRAME] =\n\t\t\t(struct blkif_request_segment) {\n\t\t\t\t.gref       = ref,\n\t\t\t\t.first_sect = fsect,\n\t\t\t\t.last_sect  = lsect };\n\t}\n\n\t(setup->grant_idx)++;\n}\n\nstatic void blkif_setup_extra_req(struct blkif_request *first,\n\t\t\t\t  struct blkif_request *second)\n{\n\tuint16_t nr_segments = first->u.rw.nr_segments;\n\n\t \n\tfirst->u.rw.nr_segments = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\n\tsecond->u.rw.nr_segments = nr_segments - BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tsecond->u.rw.sector_number = first->u.rw.sector_number +\n\t\t(BLKIF_MAX_SEGMENTS_PER_REQUEST * XEN_PAGE_SIZE) / 512;\n\n\tsecond->u.rw.handle = first->u.rw.handle;\n\tsecond->operation = first->operation;\n}\n\nstatic int blkif_queue_rw_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *extra_ring_req = NULL;\n\tstruct blkif_request *final_ring_req, *final_extra_ring_req = NULL;\n\tunsigned long id, extra_id = NO_ASSOCIATED_ID;\n\tbool require_extra_req = false;\n\tint i;\n\tstruct setup_rw_req setup = {\n\t\t.grant_idx = 0,\n\t\t.segments = NULL,\n\t\t.rinfo = rinfo,\n\t\t.need_copy = rq_data_dir(req) && info->bounce,\n\t};\n\n\t \n\tbool new_persistent_gnts = false;\n\tstruct scatterlist *sg;\n\tint num_sg, max_grefs, num_grant;\n\n\tmax_grefs = req->nr_phys_segments * GRANTS_PER_PSEG;\n\tif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t \n\t\tmax_grefs += INDIRECT_GREFS(max_grefs);\n\n\t \n\tif (rinfo->persistent_gnts_c < max_grefs) {\n\t\tnew_persistent_gnts = true;\n\n\t\tif (gnttab_alloc_grant_references(\n\t\t    max_grefs - rinfo->persistent_gnts_c,\n\t\t    &setup.gref_head) < 0) {\n\t\t\tgnttab_request_free_callback(\n\t\t\t\t&rinfo->callback,\n\t\t\t\tblkif_restart_queue_callback,\n\t\t\t\trinfo,\n\t\t\t\tmax_grefs - rinfo->persistent_gnts_c);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t \n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tnum_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);\n\tnum_grant = 0;\n\t \n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i)\n\t       num_grant += gnttab_count_grant(sg->offset, sg->length);\n\n\trequire_extra_req = info->max_indirect_segments == 0 &&\n\t\tnum_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tBUG_ON(!HAS_EXTRA_REQ && require_extra_req);\n\n\trinfo->shadow[id].num_sg = num_sg;\n\tif (num_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST &&\n\t    likely(!require_extra_req)) {\n\t\t \n\t\tBUG_ON(req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA);\n\t\tring_req->operation = BLKIF_OP_INDIRECT;\n\t\tring_req->u.indirect.indirect_op = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.indirect.handle = info->handle;\n\t\tring_req->u.indirect.nr_segments = num_grant;\n\t} else {\n\t\tring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.rw.handle = info->handle;\n\t\tring_req->operation = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tif (req_op(req) == REQ_OP_FLUSH ||\n\t\t    (req_op(req) == REQ_OP_WRITE && (req->cmd_flags & REQ_FUA))) {\n\t\t\t \n\t\t\tif (info->feature_flush && info->feature_fua)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_WRITE_BARRIER;\n\t\t\telse if (info->feature_flush)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_FLUSH_DISKCACHE;\n\t\t\telse\n\t\t\t\tring_req->operation = 0;\n\t\t}\n\t\tring_req->u.rw.nr_segments = num_grant;\n\t\tif (unlikely(require_extra_req)) {\n\t\t\textra_id = blkif_ring_get_request(rinfo, req,\n\t\t\t\t\t\t\t  &final_extra_ring_req);\n\t\t\textra_ring_req = &rinfo->shadow[extra_id].req;\n\n\t\t\t \n\t\t\trinfo->shadow[extra_id].num_sg = 0;\n\n\t\t\tblkif_setup_extra_req(ring_req, extra_ring_req);\n\n\t\t\t \n\t\t\trinfo->shadow[extra_id].associated_id = id;\n\t\t\trinfo->shadow[id].associated_id = extra_id;\n\t\t}\n\t}\n\n\tsetup.ring_req = ring_req;\n\tsetup.id = id;\n\n\tsetup.require_extra_req = require_extra_req;\n\tif (unlikely(require_extra_req))\n\t\tsetup.extra_ring_req = extra_ring_req;\n\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i) {\n\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\tif (setup.need_copy) {\n\t\t\tsetup.bvec_off = sg->offset;\n\t\t\tsetup.bvec_data = kmap_atomic(sg_page(sg));\n\t\t}\n\n\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t      sg->offset,\n\t\t\t\t\t      sg->length,\n\t\t\t\t\t      blkif_setup_rw_req_grant,\n\t\t\t\t\t      &setup);\n\n\t\tif (setup.need_copy)\n\t\t\tkunmap_atomic(setup.bvec_data);\n\t}\n\tif (setup.segments)\n\t\tkunmap_atomic(setup.segments);\n\n\t \n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\tif (unlikely(require_extra_req)) {\n\t\t*final_extra_ring_req = *extra_ring_req;\n\t\trinfo->shadow[extra_id].status = REQ_WAITING;\n\t}\n\n\tif (new_persistent_gnts)\n\t\tgnttab_free_grant_references(setup.gref_head);\n\n\treturn 0;\n}\n\n \nstatic int blkif_queue_request(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tif (unlikely(rinfo->dev_info->connected != BLKIF_STATE_CONNECTED))\n\t\treturn 1;\n\n\tif (unlikely(req_op(req) == REQ_OP_DISCARD ||\n\t\t     req_op(req) == REQ_OP_SECURE_ERASE))\n\t\treturn blkif_queue_discard_req(req, rinfo);\n\telse\n\t\treturn blkif_queue_rw_req(req, rinfo);\n}\n\nstatic inline void flush_requests(struct blkfront_ring_info *rinfo)\n{\n\tint notify;\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&rinfo->ring, notify);\n\n\tif (notify)\n\t\tnotify_remote_via_irq(rinfo->irq);\n}\n\nstatic inline bool blkif_request_flush_invalid(struct request *req,\n\t\t\t\t\t       struct blkfront_info *info)\n{\n\treturn (blk_rq_is_passthrough(req) ||\n\t\t((req_op(req) == REQ_OP_FLUSH) &&\n\t\t !info->feature_flush) ||\n\t\t((req->cmd_flags & REQ_FUA) &&\n\t\t !info->feature_fua));\n}\n\nstatic blk_status_t blkif_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t  const struct blk_mq_queue_data *qd)\n{\n\tunsigned long flags;\n\tint qid = hctx->queue_num;\n\tstruct blkfront_info *info = hctx->queue->queuedata;\n\tstruct blkfront_ring_info *rinfo = NULL;\n\n\trinfo = get_rinfo(info, qid);\n\tblk_mq_start_request(qd->rq);\n\tspin_lock_irqsave(&rinfo->ring_lock, flags);\n\tif (RING_FULL(&rinfo->ring))\n\t\tgoto out_busy;\n\n\tif (blkif_request_flush_invalid(qd->rq, rinfo->dev_info))\n\t\tgoto out_err;\n\n\tif (blkif_queue_request(qd->rq, rinfo))\n\t\tgoto out_busy;\n\n\tflush_requests(rinfo);\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\treturn BLK_STS_OK;\n\nout_err:\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\treturn BLK_STS_IOERR;\n\nout_busy:\n\tblk_mq_stop_hw_queue(hctx);\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\treturn BLK_STS_DEV_RESOURCE;\n}\n\nstatic void blkif_complete_rq(struct request *rq)\n{\n\tblk_mq_end_request(rq, blkif_req(rq)->error);\n}\n\nstatic const struct blk_mq_ops blkfront_mq_ops = {\n\t.queue_rq = blkif_queue_rq,\n\t.complete = blkif_complete_rq,\n};\n\nstatic void blkif_set_queue_limits(struct blkfront_info *info)\n{\n\tstruct request_queue *rq = info->rq;\n\tstruct gendisk *gd = info->gd;\n\tunsigned int segments = info->max_indirect_segments ? :\n\t\t\t\tBLKIF_MAX_SEGMENTS_PER_REQUEST;\n\n\tblk_queue_flag_set(QUEUE_FLAG_VIRT, rq);\n\n\tif (info->feature_discard) {\n\t\tblk_queue_max_discard_sectors(rq, get_capacity(gd));\n\t\trq->limits.discard_granularity = info->discard_granularity ?:\n\t\t\t\t\t\t info->physical_sector_size;\n\t\trq->limits.discard_alignment = info->discard_alignment;\n\t\tif (info->feature_secdiscard)\n\t\t\tblk_queue_max_secure_erase_sectors(rq,\n\t\t\t\t\t\t\t   get_capacity(gd));\n\t}\n\n\t \n\tblk_queue_logical_block_size(rq, info->sector_size);\n\tblk_queue_physical_block_size(rq, info->physical_sector_size);\n\tblk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);\n\n\t \n\tblk_queue_segment_boundary(rq, PAGE_SIZE - 1);\n\tblk_queue_max_segment_size(rq, PAGE_SIZE);\n\n\t \n\tblk_queue_max_segments(rq, segments / GRANTS_PER_PSEG);\n\n\t \n\tblk_queue_dma_alignment(rq, 511);\n}\n\nstatic const char *flush_info(struct blkfront_info *info)\n{\n\tif (info->feature_flush && info->feature_fua)\n\t\treturn \"barrier: enabled;\";\n\telse if (info->feature_flush)\n\t\treturn \"flush diskcache: enabled;\";\n\telse\n\t\treturn \"barrier or flush: disabled;\";\n}\n\nstatic void xlvbd_flush(struct blkfront_info *info)\n{\n\tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n\t\t\t      info->feature_fua ? true : false);\n\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",\n\t\tinfo->gd->disk_name, flush_info(info),\n\t\t\"persistent grants:\", info->feature_persistent ?\n\t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",\n\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");\n}\n\nstatic int xen_translate_vdev(int vdevice, int *minor, unsigned int *offset)\n{\n\tint major;\n\tmajor = BLKIF_MAJOR(vdevice);\n\t*minor = BLKIF_MINOR(vdevice);\n\tswitch (major) {\n\t\tcase XEN_IDE0_MAJOR:\n\t\t\t*offset = (*minor / 64) + EMULATED_HD_DISK_NAME_OFFSET;\n\t\t\t*minor = ((*minor / 64) * PARTS_PER_DISK) +\n\t\t\t\tEMULATED_HD_DISK_MINOR_OFFSET;\n\t\t\tbreak;\n\t\tcase XEN_IDE1_MAJOR:\n\t\t\t*offset = (*minor / 64) + 2 + EMULATED_HD_DISK_NAME_OFFSET;\n\t\t\t*minor = (((*minor / 64) + 2) * PARTS_PER_DISK) +\n\t\t\t\tEMULATED_HD_DISK_MINOR_OFFSET;\n\t\t\tbreak;\n\t\tcase XEN_SCSI_DISK0_MAJOR:\n\t\t\t*offset = (*minor / PARTS_PER_DISK) + EMULATED_SD_DISK_NAME_OFFSET;\n\t\t\t*minor = *minor + EMULATED_SD_DISK_MINOR_OFFSET;\n\t\t\tbreak;\n\t\tcase XEN_SCSI_DISK1_MAJOR:\n\t\tcase XEN_SCSI_DISK2_MAJOR:\n\t\tcase XEN_SCSI_DISK3_MAJOR:\n\t\tcase XEN_SCSI_DISK4_MAJOR:\n\t\tcase XEN_SCSI_DISK5_MAJOR:\n\t\tcase XEN_SCSI_DISK6_MAJOR:\n\t\tcase XEN_SCSI_DISK7_MAJOR:\n\t\t\t*offset = (*minor / PARTS_PER_DISK) + \n\t\t\t\t((major - XEN_SCSI_DISK1_MAJOR + 1) * 16) +\n\t\t\t\tEMULATED_SD_DISK_NAME_OFFSET;\n\t\t\t*minor = *minor +\n\t\t\t\t((major - XEN_SCSI_DISK1_MAJOR + 1) * 16 * PARTS_PER_DISK) +\n\t\t\t\tEMULATED_SD_DISK_MINOR_OFFSET;\n\t\t\tbreak;\n\t\tcase XEN_SCSI_DISK8_MAJOR:\n\t\tcase XEN_SCSI_DISK9_MAJOR:\n\t\tcase XEN_SCSI_DISK10_MAJOR:\n\t\tcase XEN_SCSI_DISK11_MAJOR:\n\t\tcase XEN_SCSI_DISK12_MAJOR:\n\t\tcase XEN_SCSI_DISK13_MAJOR:\n\t\tcase XEN_SCSI_DISK14_MAJOR:\n\t\tcase XEN_SCSI_DISK15_MAJOR:\n\t\t\t*offset = (*minor / PARTS_PER_DISK) + \n\t\t\t\t((major - XEN_SCSI_DISK8_MAJOR + 8) * 16) +\n\t\t\t\tEMULATED_SD_DISK_NAME_OFFSET;\n\t\t\t*minor = *minor +\n\t\t\t\t((major - XEN_SCSI_DISK8_MAJOR + 8) * 16 * PARTS_PER_DISK) +\n\t\t\t\tEMULATED_SD_DISK_MINOR_OFFSET;\n\t\t\tbreak;\n\t\tcase XENVBD_MAJOR:\n\t\t\t*offset = *minor / PARTS_PER_DISK;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprintk(KERN_WARNING \"blkfront: your disk configuration is \"\n\t\t\t\t\t\"incorrect, please use an xvd device instead\\n\");\n\t\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic char *encode_disk_name(char *ptr, unsigned int n)\n{\n\tif (n >= 26)\n\t\tptr = encode_disk_name(ptr, n / 26 - 1);\n\t*ptr = 'a' + n % 26;\n\treturn ptr + 1;\n}\n\nstatic int xlvbd_alloc_gendisk(blkif_sector_t capacity,\n\t\tstruct blkfront_info *info, u16 sector_size,\n\t\tunsigned int physical_sector_size)\n{\n\tstruct gendisk *gd;\n\tint nr_minors = 1;\n\tint err;\n\tunsigned int offset;\n\tint minor;\n\tint nr_parts;\n\tchar *ptr;\n\n\tBUG_ON(info->gd != NULL);\n\tBUG_ON(info->rq != NULL);\n\n\tif ((info->vdevice>>EXT_SHIFT) > 1) {\n\t\t \n\t\tprintk(KERN_WARNING \"blkfront: vdevice 0x%x is above the extended range; ignoring\\n\", info->vdevice);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!VDEV_IS_EXTENDED(info->vdevice)) {\n\t\terr = xen_translate_vdev(info->vdevice, &minor, &offset);\n\t\tif (err)\n\t\t\treturn err;\n\t\tnr_parts = PARTS_PER_DISK;\n\t} else {\n\t\tminor = BLKIF_MINOR_EXT(info->vdevice);\n\t\tnr_parts = PARTS_PER_EXT_DISK;\n\t\toffset = minor / nr_parts;\n\t\tif (xen_hvm_domain() && offset < EMULATED_HD_DISK_NAME_OFFSET + 4)\n\t\t\tprintk(KERN_WARNING \"blkfront: vdevice 0x%x might conflict with \"\n\t\t\t\t\t\"emulated IDE disks,\\n\\t choose an xvd device name\"\n\t\t\t\t\t\"from xvde on\\n\", info->vdevice);\n\t}\n\tif (minor >> MINORBITS) {\n\t\tpr_warn(\"blkfront: %#x's minor (%#x) out of range; ignoring\\n\",\n\t\t\tinfo->vdevice, minor);\n\t\treturn -ENODEV;\n\t}\n\n\tif ((minor % nr_parts) == 0)\n\t\tnr_minors = nr_parts;\n\n\terr = xlbd_reserve_minors(minor, nr_minors);\n\tif (err)\n\t\treturn err;\n\n\tmemset(&info->tag_set, 0, sizeof(info->tag_set));\n\tinfo->tag_set.ops = &blkfront_mq_ops;\n\tinfo->tag_set.nr_hw_queues = info->nr_rings;\n\tif (HAS_EXTRA_REQ && info->max_indirect_segments == 0) {\n\t\t \n\t\tinfo->tag_set.queue_depth =  BLK_RING_SIZE(info) / 2;\n\t} else\n\t\tinfo->tag_set.queue_depth = BLK_RING_SIZE(info);\n\tinfo->tag_set.numa_node = NUMA_NO_NODE;\n\tinfo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\n\tinfo->tag_set.cmd_size = sizeof(struct blkif_req);\n\tinfo->tag_set.driver_data = info;\n\n\terr = blk_mq_alloc_tag_set(&info->tag_set);\n\tif (err)\n\t\tgoto out_release_minors;\n\n\tgd = blk_mq_alloc_disk(&info->tag_set, info);\n\tif (IS_ERR(gd)) {\n\t\terr = PTR_ERR(gd);\n\t\tgoto out_free_tag_set;\n\t}\n\n\tstrcpy(gd->disk_name, DEV_NAME);\n\tptr = encode_disk_name(gd->disk_name + sizeof(DEV_NAME) - 1, offset);\n\tBUG_ON(ptr >= gd->disk_name + DISK_NAME_LEN);\n\tif (nr_minors > 1)\n\t\t*ptr = 0;\n\telse\n\t\tsnprintf(ptr, gd->disk_name + DISK_NAME_LEN - ptr,\n\t\t\t \"%d\", minor & (nr_parts - 1));\n\n\tgd->major = XENVBD_MAJOR;\n\tgd->first_minor = minor;\n\tgd->minors = nr_minors;\n\tgd->fops = &xlvbd_block_fops;\n\tgd->private_data = info;\n\tset_capacity(gd, capacity);\n\n\tinfo->rq = gd->queue;\n\tinfo->gd = gd;\n\tinfo->sector_size = sector_size;\n\tinfo->physical_sector_size = physical_sector_size;\n\tblkif_set_queue_limits(info);\n\n\txlvbd_flush(info);\n\n\tif (info->vdisk_info & VDISK_READONLY)\n\t\tset_disk_ro(gd, 1);\n\tif (info->vdisk_info & VDISK_REMOVABLE)\n\t\tgd->flags |= GENHD_FL_REMOVABLE;\n\n\treturn 0;\n\nout_free_tag_set:\n\tblk_mq_free_tag_set(&info->tag_set);\nout_release_minors:\n\txlbd_release_minors(minor, nr_minors);\n\treturn err;\n}\n\n \nstatic inline void kick_pending_request_queues_locked(struct blkfront_ring_info *rinfo)\n{\n\tif (!RING_FULL(&rinfo->ring))\n\t\tblk_mq_start_stopped_hw_queues(rinfo->dev_info->rq, true);\n}\n\nstatic void kick_pending_request_queues(struct blkfront_ring_info *rinfo)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rinfo->ring_lock, flags);\n\tkick_pending_request_queues_locked(rinfo);\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n}\n\nstatic void blkif_restart_queue(struct work_struct *work)\n{\n\tstruct blkfront_ring_info *rinfo = container_of(work, struct blkfront_ring_info, work);\n\n\tif (rinfo->dev_info->connected == BLKIF_STATE_CONNECTED)\n\t\tkick_pending_request_queues(rinfo);\n}\n\nstatic void blkif_free_ring(struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *persistent_gnt, *n;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tint i, j, segs;\n\n\t \n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\n\t\tBUG_ON(info->bounce);\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\t \n\tif (!list_empty(&rinfo->grants)) {\n\t\tlist_for_each_entry_safe(persistent_gnt, n,\n\t\t\t\t\t &rinfo->grants, node) {\n\t\t\tlist_del(&persistent_gnt->node);\n\t\t\tif (persistent_gnt->gref != INVALID_GRANT_REF) {\n\t\t\t\tgnttab_end_foreign_access(persistent_gnt->gref,\n\t\t\t\t\t\t\t  NULL);\n\t\t\t\trinfo->persistent_gnts_c--;\n\t\t\t}\n\t\t\tif (info->bounce)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\t}\n\tBUG_ON(rinfo->persistent_gnts_c != 0);\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\t \n\t\tif (!rinfo->shadow[i].request)\n\t\t\tgoto free_shadow;\n\n\t\tsegs = rinfo->shadow[i].req.operation == BLKIF_OP_INDIRECT ?\n\t\t       rinfo->shadow[i].req.u.indirect.nr_segments :\n\t\t       rinfo->shadow[i].req.u.rw.nr_segments;\n\t\tfor (j = 0; j < segs; j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].grants_used[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\tif (info->bounce)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\n\t\tif (rinfo->shadow[i].req.operation != BLKIF_OP_INDIRECT)\n\t\t\t \n\t\t\tgoto free_shadow;\n\n\t\tfor (j = 0; j < INDIRECT_GREFS(segs); j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].indirect_grants[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\nfree_shadow:\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t}\n\n\t \n\tgnttab_cancel_free_callback(&rinfo->callback);\n\n\t \n\tflush_work(&rinfo->work);\n\n\t \n\txenbus_teardown_ring((void **)&rinfo->ring.sring, info->nr_ring_pages,\n\t\t\t     rinfo->ring_ref);\n\n\tif (rinfo->irq)\n\t\tunbind_from_irqhandler(rinfo->irq, rinfo);\n\trinfo->evtchn = rinfo->irq = 0;\n}\n\nstatic void blkif_free(struct blkfront_info *info, int suspend)\n{\n\tunsigned int i;\n\tstruct blkfront_ring_info *rinfo;\n\n\t \n\tinfo->connected = suspend ?\n\t\tBLKIF_STATE_SUSPENDED : BLKIF_STATE_DISCONNECTED;\n\t \n\tif (info->rq)\n\t\tblk_mq_stop_hw_queues(info->rq);\n\n\tfor_each_rinfo(info, rinfo, i)\n\t\tblkif_free_ring(rinfo);\n\n\tkvfree(info->rinfo);\n\tinfo->rinfo = NULL;\n\tinfo->nr_rings = 0;\n}\n\nstruct copy_from_grant {\n\tconst struct blk_shadow *s;\n\tunsigned int grant_idx;\n\tunsigned int bvec_offset;\n\tchar *bvec_data;\n};\n\nstatic void blkif_copy_from_grant(unsigned long gfn, unsigned int offset,\n\t\t\t\t  unsigned int len, void *data)\n{\n\tstruct copy_from_grant *info = data;\n\tchar *shared_data;\n\t \n\tconst struct blk_shadow *s = info->s;\n\n\tshared_data = kmap_atomic(s->grants_used[info->grant_idx]->page);\n\n\tmemcpy(info->bvec_data + info->bvec_offset,\n\t       shared_data + offset, len);\n\n\tinfo->bvec_offset += len;\n\tinfo->grant_idx++;\n\n\tkunmap_atomic(shared_data);\n}\n\nstatic enum blk_req_status blkif_rsp_to_req_status(int rsp)\n{\n\tswitch (rsp)\n\t{\n\tcase BLKIF_RSP_OKAY:\n\t\treturn REQ_DONE;\n\tcase BLKIF_RSP_EOPNOTSUPP:\n\t\treturn REQ_EOPNOTSUPP;\n\tcase BLKIF_RSP_ERROR:\n\tdefault:\n\t\treturn REQ_ERROR;\n\t}\n}\n\n \nstatic int blkif_get_final_status(enum blk_req_status s1,\n\t\t\t\t  enum blk_req_status s2)\n{\n\tBUG_ON(s1 < REQ_DONE);\n\tBUG_ON(s2 < REQ_DONE);\n\n\tif (s1 == REQ_ERROR || s2 == REQ_ERROR)\n\t\treturn BLKIF_RSP_ERROR;\n\telse if (s1 == REQ_EOPNOTSUPP || s2 == REQ_EOPNOTSUPP)\n\t\treturn BLKIF_RSP_EOPNOTSUPP;\n\treturn BLKIF_RSP_OKAY;\n}\n\n \nstatic int blkif_completion(unsigned long *id,\n\t\t\t    struct blkfront_ring_info *rinfo,\n\t\t\t    struct blkif_response *bret)\n{\n\tint i = 0;\n\tstruct scatterlist *sg;\n\tint num_sg, num_grant;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blk_shadow *s = &rinfo->shadow[*id];\n\tstruct copy_from_grant data = {\n\t\t.grant_idx = 0,\n\t};\n\n\tnum_grant = s->req.operation == BLKIF_OP_INDIRECT ?\n\t\ts->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\n\n\t \n\tif (unlikely(s->associated_id != NO_ASSOCIATED_ID)) {\n\t\tstruct blk_shadow *s2 = &rinfo->shadow[s->associated_id];\n\n\t\t \n\t\ts->status = blkif_rsp_to_req_status(bret->status);\n\n\t\t \n\t\tif (s2->status < REQ_DONE)\n\t\t\treturn 0;\n\n\t\tbret->status = blkif_get_final_status(s->status,\n\t\t\t\t\t\t      s2->status);\n\n\t\t \n\t\tnum_grant += s2->req.u.rw.nr_segments;\n\n\t\t \n\t\tif (s2->num_sg != 0) {\n\t\t\t \n\t\t\t*id = s->associated_id;\n\t\t\ts = s2;\n\t\t}\n\n\t\t \n\t\tif (add_id_to_freelist(rinfo, s->associated_id))\n\t\t\tWARN(1, \"%s: can't recycle the second part (id = %ld) of the request\\n\",\n\t\t\t     info->gd->disk_name, s->associated_id);\n\t}\n\n\tdata.s = s;\n\tnum_sg = s->num_sg;\n\n\tif (bret->operation == BLKIF_OP_READ && info->bounce) {\n\t\tfor_each_sg(s->sg, sg, num_sg, i) {\n\t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\t\tdata.bvec_offset = sg->offset;\n\t\t\tdata.bvec_data = kmap_atomic(sg_page(sg));\n\n\t\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t\t      sg->offset,\n\t\t\t\t\t\t      sg->length,\n\t\t\t\t\t\t      blkif_copy_from_grant,\n\t\t\t\t\t\t      &data);\n\n\t\t\tkunmap_atomic(data.bvec_data);\n\t\t}\n\t}\n\t \n\tfor (i = 0; i < num_grant; i++) {\n\t\tif (!gnttab_try_end_foreign_access(s->grants_used[i]->gref)) {\n\t\t\t \n\t\t\tif (!info->feature_persistent) {\n\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t s->grants_used[i]->gref);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tlist_add(&s->grants_used[i]->node, &rinfo->grants);\n\t\t\trinfo->persistent_gnts_c++;\n\t\t} else {\n\t\t\t \n\t\t\ts->grants_used[i]->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&s->grants_used[i]->node, &rinfo->grants);\n\t\t}\n\t}\n\tif (s->req.operation == BLKIF_OP_INDIRECT) {\n\t\tfor (i = 0; i < INDIRECT_GREFS(num_grant); i++) {\n\t\t\tif (!gnttab_try_end_foreign_access(s->indirect_grants[i]->gref)) {\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t\t s->indirect_grants[i]->gref);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tlist_add(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t\trinfo->persistent_gnts_c++;\n\t\t\t} else {\n\t\t\t\tstruct page *indirect_page;\n\n\t\t\t\t \n\t\t\t\tif (!info->bounce) {\n\t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n\t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t\t\t}\n\t\t\t\ts->indirect_grants[i]->gref = INVALID_GRANT_REF;\n\t\t\t\tlist_add_tail(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nstatic irqreturn_t blkif_interrupt(int irq, void *dev_id)\n{\n\tstruct request *req;\n\tstruct blkif_response bret;\n\tRING_IDX i, rp;\n\tunsigned long flags;\n\tstruct blkfront_ring_info *rinfo = (struct blkfront_ring_info *)dev_id;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tunsigned int eoiflag = XEN_EOI_FLAG_SPURIOUS;\n\n\tif (unlikely(info->connected != BLKIF_STATE_CONNECTED)) {\n\t\txen_irq_lateeoi(irq, XEN_EOI_FLAG_SPURIOUS);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tspin_lock_irqsave(&rinfo->ring_lock, flags);\n again:\n\trp = READ_ONCE(rinfo->ring.sring->rsp_prod);\n\tvirt_rmb();  \n\tif (RING_RESPONSE_PROD_OVERFLOW(&rinfo->ring, rp)) {\n\t\tpr_alert(\"%s: illegal number of responses %u\\n\",\n\t\t\t info->gd->disk_name, rp - rinfo->ring.rsp_cons);\n\t\tgoto err;\n\t}\n\n\tfor (i = rinfo->ring.rsp_cons; i != rp; i++) {\n\t\tunsigned long id;\n\t\tunsigned int op;\n\n\t\teoiflag = 0;\n\n\t\tRING_COPY_RESPONSE(&rinfo->ring, i, &bret);\n\t\tid = bret.id;\n\n\t\t \n\t\tif (id >= BLK_RING_SIZE(info)) {\n\t\t\tpr_alert(\"%s: response has incorrect id (%ld)\\n\",\n\t\t\t\t info->gd->disk_name, id);\n\t\t\tgoto err;\n\t\t}\n\t\tif (rinfo->shadow[id].status != REQ_WAITING) {\n\t\t\tpr_alert(\"%s: response references no pending request\\n\",\n\t\t\t\t info->gd->disk_name);\n\t\t\tgoto err;\n\t\t}\n\n\t\trinfo->shadow[id].status = REQ_PROCESSING;\n\t\treq  = rinfo->shadow[id].request;\n\n\t\top = rinfo->shadow[id].req.operation;\n\t\tif (op == BLKIF_OP_INDIRECT)\n\t\t\top = rinfo->shadow[id].req.u.indirect.indirect_op;\n\t\tif (bret.operation != op) {\n\t\t\tpr_alert(\"%s: response has wrong operation (%u instead of %u)\\n\",\n\t\t\t\t info->gd->disk_name, bret.operation, op);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (bret.operation != BLKIF_OP_DISCARD) {\n\t\t\tint ret;\n\n\t\t\t \n\t\t\tret = blkif_completion(&id, rinfo, &bret);\n\t\t\tif (!ret)\n\t\t\t\tcontinue;\n\t\t\tif (unlikely(ret < 0))\n\t\t\t\tgoto err;\n\t\t}\n\n\t\tif (add_id_to_freelist(rinfo, id)) {\n\t\t\tWARN(1, \"%s: response to %s (id %ld) couldn't be recycled!\\n\",\n\t\t\t     info->gd->disk_name, op_name(bret.operation), id);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bret.status == BLKIF_RSP_OKAY)\n\t\t\tblkif_req(req)->error = BLK_STS_OK;\n\t\telse\n\t\t\tblkif_req(req)->error = BLK_STS_IOERR;\n\n\t\tswitch (bret.operation) {\n\t\tcase BLKIF_OP_DISCARD:\n\t\t\tif (unlikely(bret.status == BLKIF_RSP_EOPNOTSUPP)) {\n\t\t\t\tstruct request_queue *rq = info->rq;\n\n\t\t\t\tpr_warn_ratelimited(\"blkfront: %s: %s op failed\\n\",\n\t\t\t\t\t   info->gd->disk_name, op_name(bret.operation));\n\t\t\t\tblkif_req(req)->error = BLK_STS_NOTSUPP;\n\t\t\t\tinfo->feature_discard = 0;\n\t\t\t\tinfo->feature_secdiscard = 0;\n\t\t\t\tblk_queue_max_discard_sectors(rq, 0);\n\t\t\t\tblk_queue_max_secure_erase_sectors(rq, 0);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BLKIF_OP_FLUSH_DISKCACHE:\n\t\tcase BLKIF_OP_WRITE_BARRIER:\n\t\t\tif (unlikely(bret.status == BLKIF_RSP_EOPNOTSUPP)) {\n\t\t\t\tpr_warn_ratelimited(\"blkfront: %s: %s op failed\\n\",\n\t\t\t\t       info->gd->disk_name, op_name(bret.operation));\n\t\t\t\tblkif_req(req)->error = BLK_STS_NOTSUPP;\n\t\t\t}\n\t\t\tif (unlikely(bret.status == BLKIF_RSP_ERROR &&\n\t\t\t\t     rinfo->shadow[id].req.u.rw.nr_segments == 0)) {\n\t\t\t\tpr_warn_ratelimited(\"blkfront: %s: empty %s op failed\\n\",\n\t\t\t\t       info->gd->disk_name, op_name(bret.operation));\n\t\t\t\tblkif_req(req)->error = BLK_STS_NOTSUPP;\n\t\t\t}\n\t\t\tif (unlikely(blkif_req(req)->error)) {\n\t\t\t\tif (blkif_req(req)->error == BLK_STS_NOTSUPP)\n\t\t\t\t\tblkif_req(req)->error = BLK_STS_OK;\n\t\t\t\tinfo->feature_fua = 0;\n\t\t\t\tinfo->feature_flush = 0;\n\t\t\t\txlvbd_flush(info);\n\t\t\t}\n\t\t\tfallthrough;\n\t\tcase BLKIF_OP_READ:\n\t\tcase BLKIF_OP_WRITE:\n\t\t\tif (unlikely(bret.status != BLKIF_RSP_OKAY))\n\t\t\t\tdev_dbg_ratelimited(&info->xbdev->dev,\n\t\t\t\t\t\"Bad return from blkdev data request: %#x\\n\",\n\t\t\t\t\tbret.status);\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tif (likely(!blk_should_fake_timeout(req->q)))\n\t\t\tblk_mq_complete_request(req);\n\t}\n\n\trinfo->ring.rsp_cons = i;\n\n\tif (i != rinfo->ring.req_prod_pvt) {\n\t\tint more_to_do;\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&rinfo->ring, more_to_do);\n\t\tif (more_to_do)\n\t\t\tgoto again;\n\t} else\n\t\trinfo->ring.sring->rsp_event = i + 1;\n\n\tkick_pending_request_queues_locked(rinfo);\n\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\n\txen_irq_lateeoi(irq, eoiflag);\n\n\treturn IRQ_HANDLED;\n\n err:\n\tinfo->connected = BLKIF_STATE_ERROR;\n\n\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\n\t \n\n\tpr_alert(\"%s disabled for further use\\n\", info->gd->disk_name);\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int setup_blkring(struct xenbus_device *dev,\n\t\t\t struct blkfront_ring_info *rinfo)\n{\n\tstruct blkif_sring *sring;\n\tint err;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tunsigned long ring_size = info->nr_ring_pages * XEN_PAGE_SIZE;\n\n\terr = xenbus_setup_ring(dev, GFP_NOIO, (void **)&sring,\n\t\t\t\tinfo->nr_ring_pages, rinfo->ring_ref);\n\tif (err)\n\t\tgoto fail;\n\n\tXEN_FRONT_RING_INIT(&rinfo->ring, sring, ring_size);\n\n\terr = xenbus_alloc_evtchn(dev, &rinfo->evtchn);\n\tif (err)\n\t\tgoto fail;\n\n\terr = bind_evtchn_to_irqhandler_lateeoi(rinfo->evtchn, blkif_interrupt,\n\t\t\t\t\t\t0, \"blkif\", rinfo);\n\tif (err <= 0) {\n\t\txenbus_dev_fatal(dev, err,\n\t\t\t\t \"bind_evtchn_to_irqhandler failed\");\n\t\tgoto fail;\n\t}\n\trinfo->irq = err;\n\n\treturn 0;\nfail:\n\tblkif_free(info, 0);\n\treturn err;\n}\n\n \nstatic int write_per_ring_nodes(struct xenbus_transaction xbt,\n\t\t\t\tstruct blkfront_ring_info *rinfo, const char *dir)\n{\n\tint err;\n\tunsigned int i;\n\tconst char *message = NULL;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tif (info->nr_ring_pages == 1) {\n\t\terr = xenbus_printf(xbt, dir, \"ring-ref\", \"%u\", rinfo->ring_ref[0]);\n\t\tif (err) {\n\t\t\tmessage = \"writing ring-ref\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < info->nr_ring_pages; i++) {\n\t\t\tchar ring_ref_name[RINGREF_NAME_LEN];\n\n\t\t\tsnprintf(ring_ref_name, RINGREF_NAME_LEN, \"ring-ref%u\", i);\n\t\t\terr = xenbus_printf(xbt, dir, ring_ref_name,\n\t\t\t\t\t    \"%u\", rinfo->ring_ref[i]);\n\t\t\tif (err) {\n\t\t\t\tmessage = \"writing ring-ref\";\n\t\t\t\tgoto abort_transaction;\n\t\t\t}\n\t\t}\n\t}\n\n\terr = xenbus_printf(xbt, dir, \"event-channel\", \"%u\", rinfo->evtchn);\n\tif (err) {\n\t\tmessage = \"writing event-channel\";\n\t\tgoto abort_transaction;\n\t}\n\n\treturn 0;\n\nabort_transaction:\n\txenbus_transaction_end(xbt, 1);\n\tif (message)\n\t\txenbus_dev_fatal(info->xbdev, err, \"%s\", message);\n\n\treturn err;\n}\n\n \nstatic bool feature_persistent = true;\nmodule_param(feature_persistent, bool, 0644);\nMODULE_PARM_DESC(feature_persistent,\n\t\t\"Enables the persistent grants feature\");\n\n \nstatic int talk_to_blkback(struct xenbus_device *dev,\n\t\t\t   struct blkfront_info *info)\n{\n\tconst char *message = NULL;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int i, max_page_order;\n\tunsigned int ring_page_order;\n\tstruct blkfront_ring_info *rinfo;\n\n\tif (!info)\n\t\treturn -ENODEV;\n\n\t \n\tinfo->bounce = !xen_blkif_trusted ||\n\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n\n\tmax_page_order = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t      \"max-ring-page-order\", 0);\n\tring_page_order = min(xen_blkif_max_ring_order, max_page_order);\n\tinfo->nr_ring_pages = 1 << ring_page_order;\n\n\terr = negotiate_mq(info);\n\tif (err)\n\t\tgoto destroy_blkring;\n\n\tfor_each_rinfo(info, rinfo, i) {\n\t\t \n\t\terr = setup_blkring(dev, rinfo);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t}\n\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\n\tif (info->nr_ring_pages > 1) {\n\t\terr = xenbus_printf(xbt, dev->nodename, \"ring-page-order\", \"%u\",\n\t\t\t\t    ring_page_order);\n\t\tif (err) {\n\t\t\tmessage = \"writing ring-page-order\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t}\n\n\t \n\tif (info->nr_rings == 1) {\n\t\terr = write_per_ring_nodes(xbt, info->rinfo, dev->nodename);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t} else {\n\t\tchar *path;\n\t\tsize_t pathsize;\n\n\t\terr = xenbus_printf(xbt, dev->nodename, \"multi-queue-num-queues\", \"%u\",\n\t\t\t\t    info->nr_rings);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\n\t\tpathsize = strlen(dev->nodename) + QUEUE_NAME_LEN;\n\t\tpath = kmalloc(pathsize, GFP_KERNEL);\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tmessage = \"ENOMEM while writing ring references\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\n\t\tfor_each_rinfo(info, rinfo, i) {\n\t\t\tmemset(path, 0, pathsize);\n\t\t\tsnprintf(path, pathsize, \"%s/queue-%u\", dev->nodename, i);\n\t\t\terr = write_per_ring_nodes(xbt, rinfo, path);\n\t\t\tif (err) {\n\t\t\t\tkfree(path);\n\t\t\t\tgoto destroy_blkring;\n\t\t\t}\n\t\t}\n\t\tkfree(path);\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"protocol\", \"%s\",\n\t\t\t    XEN_IO_PROTO_ABI_NATIVE);\n\tif (err) {\n\t\tmessage = \"writing protocol\";\n\t\tgoto abort_transaction;\n\t}\n\tinfo->feature_persistent_parm = feature_persistent;\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-persistent\", \"%u\",\n\t\t\tinfo->feature_persistent_parm);\n\tif (err)\n\t\tdev_warn(&dev->dev,\n\t\t\t \"writing persistent grants feature to xenbus\");\n\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tunsigned int j;\n\n\t\tfor (j = 0; j < BLK_RING_SIZE(info); j++)\n\t\t\trinfo->shadow[j].req.u.rw.id = j + 1;\n\t\trinfo->shadow[BLK_RING_SIZE(info)-1].req.u.rw.id = 0x0fffffff;\n\t}\n\txenbus_switch_state(dev, XenbusStateInitialised);\n\n\treturn 0;\n\n abort_transaction:\n\txenbus_transaction_end(xbt, 1);\n\tif (message)\n\t\txenbus_dev_fatal(dev, err, \"%s\", message);\n destroy_blkring:\n\tblkif_free(info, 0);\n\treturn err;\n}\n\nstatic int negotiate_mq(struct blkfront_info *info)\n{\n\tunsigned int backend_max_queues;\n\tunsigned int i;\n\tstruct blkfront_ring_info *rinfo;\n\n\tBUG_ON(info->nr_rings);\n\n\t \n\tbackend_max_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tinfo->nr_rings = min(backend_max_queues, xen_blkif_max_queues);\n\t \n\tif (!info->nr_rings)\n\t\tinfo->nr_rings = 1;\n\n\tinfo->rinfo_size = struct_size(info->rinfo, shadow,\n\t\t\t\t       BLK_RING_SIZE(info));\n\tinfo->rinfo = kvcalloc(info->nr_rings, info->rinfo_size, GFP_KERNEL);\n\tif (!info->rinfo) {\n\t\txenbus_dev_fatal(info->xbdev, -ENOMEM, \"allocating ring_info structure\");\n\t\tinfo->nr_rings = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tINIT_LIST_HEAD(&rinfo->indirect_pages);\n\t\tINIT_LIST_HEAD(&rinfo->grants);\n\t\trinfo->dev_info = info;\n\t\tINIT_WORK(&rinfo->work, blkif_restart_queue);\n\t\tspin_lock_init(&rinfo->ring_lock);\n\t}\n\treturn 0;\n}\n\n \nstatic int blkfront_probe(struct xenbus_device *dev,\n\t\t\t  const struct xenbus_device_id *id)\n{\n\tint err, vdevice;\n\tstruct blkfront_info *info;\n\n\t \n\terr = xenbus_scanf(XBT_NIL, dev->nodename,\n\t\t\t   \"virtual-device\", \"%i\", &vdevice);\n\tif (err != 1) {\n\t\t \n\t\terr = xenbus_scanf(XBT_NIL, dev->nodename, \"virtual-device-ext\",\n\t\t\t\t   \"%i\", &vdevice);\n\t\tif (err != 1) {\n\t\t\txenbus_dev_fatal(dev, err, \"reading virtual-device\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (xen_hvm_domain()) {\n\t\tchar *type;\n\t\tint len;\n\t\t \n\t\tif (xen_has_pv_and_legacy_disk_devices()) {\n\t\t\tint major;\n\n\t\t\tif (!VDEV_IS_EXTENDED(vdevice))\n\t\t\t\tmajor = BLKIF_MAJOR(vdevice);\n\t\t\telse\n\t\t\t\tmajor = XENVBD_MAJOR;\n\n\t\t\tif (major != XENVBD_MAJOR) {\n\t\t\t\tprintk(KERN_INFO\n\t\t\t\t\t\t\"%s: HVM does not support vbd %d as xen block device\\n\",\n\t\t\t\t\t\t__func__, vdevice);\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\t\t}\n\t\t \n\t\ttype = xenbus_read(XBT_NIL, dev->nodename, \"device-type\", &len);\n\t\tif (IS_ERR(type))\n\t\t\treturn -ENODEV;\n\t\tif (strncmp(type, \"cdrom\", 5) == 0) {\n\t\t\tkfree(type);\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tkfree(type);\n\t}\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\txenbus_dev_fatal(dev, -ENOMEM, \"allocating info structure\");\n\t\treturn -ENOMEM;\n\t}\n\n\tinfo->xbdev = dev;\n\n\tmutex_init(&info->mutex);\n\tinfo->vdevice = vdevice;\n\tinfo->connected = BLKIF_STATE_DISCONNECTED;\n\n\t \n\tinfo->handle = simple_strtoul(strrchr(dev->nodename, '/')+1, NULL, 0);\n\tdev_set_drvdata(&dev->dev, info);\n\n\tmutex_lock(&blkfront_mutex);\n\tlist_add(&info->info_list, &info_list);\n\tmutex_unlock(&blkfront_mutex);\n\n\treturn 0;\n}\n\nstatic int blkif_recover(struct blkfront_info *info)\n{\n\tunsigned int r_index;\n\tstruct request *req, *n;\n\tint rc;\n\tstruct bio *bio;\n\tunsigned int segs;\n\tstruct blkfront_ring_info *rinfo;\n\n\tblkfront_gather_backend_features(info);\n\t \n\tblkif_set_queue_limits(info);\n\tsegs = info->max_indirect_segments ? : BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tblk_queue_max_segments(info->rq, segs / GRANTS_PER_PSEG);\n\n\tfor_each_rinfo(info, rinfo, r_index) {\n\t\trc = blkfront_setup_indirect(rinfo);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\txenbus_switch_state(info->xbdev, XenbusStateConnected);\n\n\t \n\tinfo->connected = BLKIF_STATE_CONNECTED;\n\n\tfor_each_rinfo(info, rinfo, r_index) {\n\t\t \n\t\tkick_pending_request_queues(rinfo);\n\t}\n\n\tlist_for_each_entry_safe(req, n, &info->requests, queuelist) {\n\t\t \n\t\tlist_del_init(&req->queuelist);\n\t\tBUG_ON(req->nr_phys_segments > segs);\n\t\tblk_mq_requeue_request(req, false);\n\t}\n\tblk_mq_start_stopped_hw_queues(info->rq, true);\n\tblk_mq_kick_requeue_list(info->rq);\n\n\twhile ((bio = bio_list_pop(&info->bio_list)) != NULL) {\n\t\t \n\t\tsubmit_bio(bio);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int blkfront_resume(struct xenbus_device *dev)\n{\n\tstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\n\tint err = 0;\n\tunsigned int i, j;\n\tstruct blkfront_ring_info *rinfo;\n\n\tdev_dbg(&dev->dev, \"blkfront_resume: %s\\n\", dev->nodename);\n\n\tbio_list_init(&info->bio_list);\n\tINIT_LIST_HEAD(&info->requests);\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tstruct bio_list merge_bio;\n\t\tstruct blk_shadow *shadow = rinfo->shadow;\n\n\t\tfor (j = 0; j < BLK_RING_SIZE(info); j++) {\n\t\t\t \n\t\t\tif (!shadow[j].request)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (req_op(shadow[j].request) == REQ_OP_FLUSH ||\n\t\t\t    req_op(shadow[j].request) == REQ_OP_DISCARD ||\n\t\t\t    req_op(shadow[j].request) == REQ_OP_SECURE_ERASE ||\n\t\t\t    shadow[j].request->cmd_flags & REQ_FUA) {\n\t\t\t\t \n\t\t\t\tlist_add(&shadow[j].request->queuelist, &info->requests);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmerge_bio.head = shadow[j].request->bio;\n\t\t\tmerge_bio.tail = shadow[j].request->biotail;\n\t\t\tbio_list_merge(&info->bio_list, &merge_bio);\n\t\t\tshadow[j].request->bio = NULL;\n\t\t\tblk_mq_end_request(shadow[j].request, BLK_STS_OK);\n\t\t}\n\t}\n\n\tblkif_free(info, info->connected == BLKIF_STATE_CONNECTED);\n\n\terr = talk_to_blkback(dev, info);\n\tif (!err)\n\t\tblk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);\n\n\t \n\n\treturn err;\n}\n\nstatic void blkfront_closing(struct blkfront_info *info)\n{\n\tstruct xenbus_device *xbdev = info->xbdev;\n\tstruct blkfront_ring_info *rinfo;\n\tunsigned int i;\n\n\tif (xbdev->state == XenbusStateClosing)\n\t\treturn;\n\n\t \n\tif (info->rq && info->gd) {\n\t\tblk_mq_stop_hw_queues(info->rq);\n\t\tblk_mark_disk_dead(info->gd);\n\t}\n\n\tfor_each_rinfo(info, rinfo, i) {\n\t\t \n\t\tgnttab_cancel_free_callback(&rinfo->callback);\n\n\t\t \n\t\tflush_work(&rinfo->work);\n\t}\n\n\txenbus_frontend_closed(xbdev);\n}\n\nstatic void blkfront_setup_discard(struct blkfront_info *info)\n{\n\tinfo->feature_discard = 1;\n\tinfo->discard_granularity = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t \"discard-granularity\",\n\t\t\t\t\t\t\t 0);\n\tinfo->discard_alignment = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t       \"discard-alignment\", 0);\n\tinfo->feature_secdiscard =\n\t\t!!xenbus_read_unsigned(info->xbdev->otherend, \"discard-secure\",\n\t\t\t\t       0);\n}\n\nstatic int blkfront_setup_indirect(struct blkfront_ring_info *rinfo)\n{\n\tunsigned int psegs, grants, memflags;\n\tint err, i;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tmemflags = memalloc_noio_save();\n\n\tif (info->max_indirect_segments == 0) {\n\t\tif (!HAS_EXTRA_REQ)\n\t\t\tgrants = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\telse {\n\t\t\t \n\t\t\tgrants = GRANTS_PER_PSEG;\n\t\t}\n\t}\n\telse\n\t\tgrants = info->max_indirect_segments;\n\tpsegs = DIV_ROUND_UP(grants, GRANTS_PER_PSEG);\n\n\terr = fill_grant_buffer(rinfo,\n\t\t\t\t(grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));\n\tif (err)\n\t\tgoto out_of_memory;\n\n\tif (!info->bounce && info->max_indirect_segments) {\n\t\t \n\t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);\n\n\t\tBUG_ON(!list_empty(&rinfo->indirect_pages));\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tstruct page *indirect_page = alloc_page(GFP_KERNEL |\n\t\t\t\t\t\t\t\t__GFP_ZERO);\n\t\t\tif (!indirect_page)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\trinfo->shadow[i].grants_used =\n\t\t\tkvcalloc(grants,\n\t\t\t\t sizeof(rinfo->shadow[i].grants_used[0]),\n\t\t\t\t GFP_KERNEL);\n\t\trinfo->shadow[i].sg = kvcalloc(psegs,\n\t\t\t\t\t       sizeof(rinfo->shadow[i].sg[0]),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (info->max_indirect_segments)\n\t\t\trinfo->shadow[i].indirect_grants =\n\t\t\t\tkvcalloc(INDIRECT_GREFS(grants),\n\t\t\t\t\t sizeof(rinfo->shadow[i].indirect_grants[0]),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif ((rinfo->shadow[i].grants_used == NULL) ||\n\t\t\t(rinfo->shadow[i].sg == NULL) ||\n\t\t     (info->max_indirect_segments &&\n\t\t     (rinfo->shadow[i].indirect_grants == NULL)))\n\t\t\tgoto out_of_memory;\n\t\tsg_init_table(rinfo->shadow[i].sg, psegs);\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn 0;\n\nout_of_memory:\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t}\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn -ENOMEM;\n}\n\n \nstatic void blkfront_gather_backend_features(struct blkfront_info *info)\n{\n\tunsigned int indirect_segments;\n\n\tinfo->feature_flush = 0;\n\tinfo->feature_fua = 0;\n\n\t \n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-barrier\", 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 1;\n\t}\n\n\t \n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-flush-cache\",\n\t\t\t\t 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 0;\n\t}\n\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-discard\", 0))\n\t\tblkfront_setup_discard(info);\n\n\tif (info->feature_persistent_parm)\n\t\tinfo->feature_persistent =\n\t\t\t!!xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t       \"feature-persistent\", 0);\n\tif (info->feature_persistent)\n\t\tinfo->bounce = true;\n\n\tindirect_segments = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-max-indirect-segments\", 0);\n\tif (indirect_segments > xen_blkif_max_segments)\n\t\tindirect_segments = xen_blkif_max_segments;\n\tif (indirect_segments <= BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\tindirect_segments = 0;\n\tinfo->max_indirect_segments = indirect_segments;\n\n\tif (info->feature_persistent) {\n\t\tmutex_lock(&blkfront_mutex);\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\t\tmutex_unlock(&blkfront_mutex);\n\t}\n}\n\n \nstatic void blkfront_connect(struct blkfront_info *info)\n{\n\tunsigned long long sectors;\n\tunsigned long sector_size;\n\tunsigned int physical_sector_size;\n\tint err, i;\n\tstruct blkfront_ring_info *rinfo;\n\n\tswitch (info->connected) {\n\tcase BLKIF_STATE_CONNECTED:\n\t\t \n\t\terr = xenbus_scanf(XBT_NIL, info->xbdev->otherend,\n\t\t\t\t   \"sectors\", \"%Lu\", &sectors);\n\t\tif (XENBUS_EXIST_ERR(err))\n\t\t\treturn;\n\t\tprintk(KERN_INFO \"Setting capacity to %Lu\\n\",\n\t\t       sectors);\n\t\tset_capacity_and_notify(info->gd, sectors);\n\n\t\treturn;\n\tcase BLKIF_STATE_SUSPENDED:\n\t\t \n\t\tblkif_recover(info);\n\t\treturn;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tdev_dbg(&info->xbdev->dev, \"%s:%s.\\n\",\n\t\t__func__, info->xbdev->otherend);\n\n\terr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\n\t\t\t    \"sectors\", \"%llu\", &sectors,\n\t\t\t    \"info\", \"%u\", &info->vdisk_info,\n\t\t\t    \"sector-size\", \"%lu\", &sector_size,\n\t\t\t    NULL);\n\tif (err) {\n\t\txenbus_dev_fatal(info->xbdev, err,\n\t\t\t\t \"reading backend fields at %s\",\n\t\t\t\t info->xbdev->otherend);\n\t\treturn;\n\t}\n\n\t \n\tphysical_sector_size = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t    \"physical-sector-size\",\n\t\t\t\t\t\t    sector_size);\n\tblkfront_gather_backend_features(info);\n\tfor_each_rinfo(info, rinfo, i) {\n\t\terr = blkfront_setup_indirect(rinfo);\n\t\tif (err) {\n\t\t\txenbus_dev_fatal(info->xbdev, err, \"setup_indirect at %s\",\n\t\t\t\t\t info->xbdev->otherend);\n\t\t\tblkif_free(info, 0);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = xlvbd_alloc_gendisk(sectors, info, sector_size,\n\t\t\t\t  physical_sector_size);\n\tif (err) {\n\t\txenbus_dev_fatal(info->xbdev, err, \"xlvbd_add at %s\",\n\t\t\t\t info->xbdev->otherend);\n\t\tgoto fail;\n\t}\n\n\txenbus_switch_state(info->xbdev, XenbusStateConnected);\n\n\t \n\tinfo->connected = BLKIF_STATE_CONNECTED;\n\tfor_each_rinfo(info, rinfo, i)\n\t\tkick_pending_request_queues(rinfo);\n\n\terr = device_add_disk(&info->xbdev->dev, info->gd, NULL);\n\tif (err) {\n\t\tput_disk(info->gd);\n\t\tblk_mq_free_tag_set(&info->tag_set);\n\t\tinfo->rq = NULL;\n\t\tgoto fail;\n\t}\n\n\tinfo->is_ready = 1;\n\treturn;\n\nfail:\n\tblkif_free(info, 0);\n\treturn;\n}\n\n \nstatic void blkback_changed(struct xenbus_device *dev,\n\t\t\t    enum xenbus_state backend_state)\n{\n\tstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\n\n\tdev_dbg(&dev->dev, \"blkfront:blkback_changed to state %d.\\n\", backend_state);\n\n\tswitch (backend_state) {\n\tcase XenbusStateInitWait:\n\t\tif (dev->state != XenbusStateInitialising)\n\t\t\tbreak;\n\t\tif (talk_to_blkback(dev, info))\n\t\t\tbreak;\n\t\tbreak;\n\tcase XenbusStateInitialising:\n\tcase XenbusStateInitialised:\n\tcase XenbusStateReconfiguring:\n\tcase XenbusStateReconfigured:\n\tcase XenbusStateUnknown:\n\t\tbreak;\n\n\tcase XenbusStateConnected:\n\t\t \n\t\tif ((dev->state != XenbusStateInitialised) &&\n\t\t    (dev->state != XenbusStateConnected)) {\n\t\t\tif (talk_to_blkback(dev, info))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tblkfront_connect(info);\n\t\tbreak;\n\n\tcase XenbusStateClosed:\n\t\tif (dev->state == XenbusStateClosed)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase XenbusStateClosing:\n\t\tblkfront_closing(info);\n\t\tbreak;\n\t}\n}\n\nstatic void blkfront_remove(struct xenbus_device *xbdev)\n{\n\tstruct blkfront_info *info = dev_get_drvdata(&xbdev->dev);\n\n\tdev_dbg(&xbdev->dev, \"%s removed\", xbdev->nodename);\n\n\tif (info->gd)\n\t\tdel_gendisk(info->gd);\n\n\tmutex_lock(&blkfront_mutex);\n\tlist_del(&info->info_list);\n\tmutex_unlock(&blkfront_mutex);\n\n\tblkif_free(info, 0);\n\tif (info->gd) {\n\t\txlbd_release_minors(info->gd->first_minor, info->gd->minors);\n\t\tput_disk(info->gd);\n\t\tblk_mq_free_tag_set(&info->tag_set);\n\t}\n\n\tkfree(info);\n}\n\nstatic int blkfront_is_ready(struct xenbus_device *dev)\n{\n\tstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\n\n\treturn info->is_ready && info->xbdev;\n}\n\nstatic const struct block_device_operations xlvbd_block_fops =\n{\n\t.owner = THIS_MODULE,\n\t.getgeo = blkif_getgeo,\n\t.ioctl = blkif_ioctl,\n\t.compat_ioctl = blkdev_compat_ptr_ioctl,\n};\n\n\nstatic const struct xenbus_device_id blkfront_ids[] = {\n\t{ \"vbd\" },\n\t{ \"\" }\n};\n\nstatic struct xenbus_driver blkfront_driver = {\n\t.ids  = blkfront_ids,\n\t.probe = blkfront_probe,\n\t.remove = blkfront_remove,\n\t.resume = blkfront_resume,\n\t.otherend_changed = blkback_changed,\n\t.is_ready = blkfront_is_ready,\n};\n\nstatic void purge_persistent_grants(struct blkfront_info *info)\n{\n\tunsigned int i;\n\tunsigned long flags;\n\tstruct blkfront_ring_info *rinfo;\n\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tstruct grant *gnt_list_entry, *tmp;\n\t\tLIST_HEAD(grants);\n\n\t\tspin_lock_irqsave(&rinfo->ring_lock, flags);\n\n\t\tif (rinfo->persistent_gnts_c == 0) {\n\t\t\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_for_each_entry_safe(gnt_list_entry, tmp, &rinfo->grants,\n\t\t\t\t\t node) {\n\t\t\tif (gnt_list_entry->gref == INVALID_GRANT_REF ||\n\t\t\t    !gnttab_try_end_foreign_access(gnt_list_entry->gref))\n\t\t\t\tcontinue;\n\n\t\t\tlist_del(&gnt_list_entry->node);\n\t\t\trinfo->persistent_gnts_c--;\n\t\t\tgnt_list_entry->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&gnt_list_entry->node, &grants);\n\t\t}\n\n\t\tlist_splice_tail(&grants, &rinfo->grants);\n\n\t\tspin_unlock_irqrestore(&rinfo->ring_lock, flags);\n\t}\n}\n\nstatic void blkfront_delay_work(struct work_struct *work)\n{\n\tstruct blkfront_info *info;\n\tbool need_schedule_work = false;\n\n\t \n\n\tmutex_lock(&blkfront_mutex);\n\n\tlist_for_each_entry(info, &info_list, info_list) {\n\t\tif (info->feature_persistent) {\n\t\t\tneed_schedule_work = true;\n\t\t\tmutex_lock(&info->mutex);\n\t\t\tpurge_persistent_grants(info);\n\t\t\tmutex_unlock(&info->mutex);\n\t\t}\n\t}\n\n\tif (need_schedule_work)\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\n\tmutex_unlock(&blkfront_mutex);\n}\n\nstatic int __init xlblk_init(void)\n{\n\tint ret;\n\tint nr_cpus = num_online_cpus();\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\tif (!xen_has_pv_disk_devices())\n\t\treturn -ENODEV;\n\n\tif (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {\n\t\tpr_warn(\"xen_blk: can't get major %d with name %s\\n\",\n\t\t\tXENVBD_MAJOR, DEV_NAME);\n\t\treturn -ENODEV;\n\t}\n\n\tif (xen_blkif_max_segments < BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\txen_blkif_max_segments = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\n\tif (xen_blkif_max_ring_order > XENBUS_MAX_RING_GRANT_ORDER) {\n\t\tpr_info(\"Invalid max_ring_order (%d), will use default max: %d.\\n\",\n\t\t\txen_blkif_max_ring_order, XENBUS_MAX_RING_GRANT_ORDER);\n\t\txen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;\n\t}\n\n\tif (xen_blkif_max_queues > nr_cpus) {\n\t\tpr_info(\"Invalid max_queues (%d), will use default max: %d.\\n\",\n\t\t\txen_blkif_max_queues, nr_cpus);\n\t\txen_blkif_max_queues = nr_cpus;\n\t}\n\n\tINIT_DELAYED_WORK(&blkfront_work, blkfront_delay_work);\n\n\tret = xenbus_register_frontend(&blkfront_driver);\n\tif (ret) {\n\t\tunregister_blkdev(XENVBD_MAJOR, DEV_NAME);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nmodule_init(xlblk_init);\n\n\nstatic void __exit xlblk_exit(void)\n{\n\tcancel_delayed_work_sync(&blkfront_work);\n\n\txenbus_unregister_driver(&blkfront_driver);\n\tunregister_blkdev(XENVBD_MAJOR, DEV_NAME);\n\tkfree(minors);\n}\nmodule_exit(xlblk_exit);\n\nMODULE_DESCRIPTION(\"Xen virtual block device frontend\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_BLOCKDEV_MAJOR(XENVBD_MAJOR);\nMODULE_ALIAS(\"xen:vbd\");\nMODULE_ALIAS(\"xenblk\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}