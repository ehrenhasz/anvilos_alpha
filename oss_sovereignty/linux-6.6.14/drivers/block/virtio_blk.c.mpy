{
  "module_name": "virtio_blk.c",
  "hash_id": "7fd42549f8d1193f159ddb2440c14053af08e46483552cd68dbced3ebfb005c5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/virtio_blk.c",
  "human_readable_source": "\n\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/hdreg.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/interrupt.h>\n#include <linux/virtio.h>\n#include <linux/virtio_blk.h>\n#include <linux/scatterlist.h>\n#include <linux/string_helpers.h>\n#include <linux/idr.h>\n#include <linux/blk-mq.h>\n#include <linux/blk-mq-virtio.h>\n#include <linux/numa.h>\n#include <linux/vmalloc.h>\n#include <uapi/linux/virtio_ring.h>\n\n#define PART_BITS 4\n#define VQ_NAME_LEN 16\n#define MAX_DISCARD_SEGMENTS 256u\n\n \n#define VIRTIO_BLK_MAX_SG_ELEMS 32768\n\n#ifdef CONFIG_ARCH_NO_SG_CHAIN\n#define VIRTIO_BLK_INLINE_SG_CNT\t0\n#else\n#define VIRTIO_BLK_INLINE_SG_CNT\t2\n#endif\n\nstatic unsigned int num_request_queues;\nmodule_param(num_request_queues, uint, 0644);\nMODULE_PARM_DESC(num_request_queues,\n\t\t \"Limit the number of request queues to use for blk device. \"\n\t\t \"0 for no limit. \"\n\t\t \"Values > nr_cpu_ids truncated to nr_cpu_ids.\");\n\nstatic unsigned int poll_queues;\nmodule_param(poll_queues, uint, 0644);\nMODULE_PARM_DESC(poll_queues, \"The number of dedicated virtqueues for polling I/O\");\n\nstatic int major;\nstatic DEFINE_IDA(vd_index_ida);\n\nstatic struct workqueue_struct *virtblk_wq;\n\nstruct virtio_blk_vq {\n\tstruct virtqueue *vq;\n\tspinlock_t lock;\n\tchar name[VQ_NAME_LEN];\n} ____cacheline_aligned_in_smp;\n\nstruct virtio_blk {\n\t \n\tstruct mutex vdev_mutex;\n\tstruct virtio_device *vdev;\n\n\t \n\tstruct gendisk *disk;\n\n\t \n\tstruct blk_mq_tag_set tag_set;\n\n\t \n\tstruct work_struct config_work;\n\n\t \n\tint index;\n\n\t \n\tint num_vqs;\n\tint io_queues[HCTX_MAX_TYPES];\n\tstruct virtio_blk_vq *vqs;\n\n\t \n\tunsigned int zone_sectors;\n};\n\nstruct virtblk_req {\n\t \n\tstruct virtio_blk_outhdr out_hdr;\n\n\t \n\tunion {\n\t\tu8 status;\n\n\t\t \n\t\tstruct {\n\t\t\t__virtio64 sector;\n\t\t\tu8 status;\n\t\t} zone_append;\n\t} in_hdr;\n\n\tsize_t in_hdr_len;\n\n\tstruct sg_table sg_table;\n\tstruct scatterlist sg[];\n};\n\nstatic inline blk_status_t virtblk_result(u8 status)\n{\n\tswitch (status) {\n\tcase VIRTIO_BLK_S_OK:\n\t\treturn BLK_STS_OK;\n\tcase VIRTIO_BLK_S_UNSUPP:\n\t\treturn BLK_STS_NOTSUPP;\n\tcase VIRTIO_BLK_S_ZONE_OPEN_RESOURCE:\n\t\treturn BLK_STS_ZONE_OPEN_RESOURCE;\n\tcase VIRTIO_BLK_S_ZONE_ACTIVE_RESOURCE:\n\t\treturn BLK_STS_ZONE_ACTIVE_RESOURCE;\n\tcase VIRTIO_BLK_S_IOERR:\n\tcase VIRTIO_BLK_S_ZONE_UNALIGNED_WP:\n\tdefault:\n\t\treturn BLK_STS_IOERR;\n\t}\n}\n\nstatic inline struct virtio_blk_vq *get_virtio_blk_vq(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct virtio_blk *vblk = hctx->queue->queuedata;\n\tstruct virtio_blk_vq *vq = &vblk->vqs[hctx->queue_num];\n\n\treturn vq;\n}\n\nstatic int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr)\n{\n\tstruct scatterlist out_hdr, in_hdr, *sgs[3];\n\tunsigned int num_out = 0, num_in = 0;\n\n\tsg_init_one(&out_hdr, &vbr->out_hdr, sizeof(vbr->out_hdr));\n\tsgs[num_out++] = &out_hdr;\n\n\tif (vbr->sg_table.nents) {\n\t\tif (vbr->out_hdr.type & cpu_to_virtio32(vq->vdev, VIRTIO_BLK_T_OUT))\n\t\t\tsgs[num_out++] = vbr->sg_table.sgl;\n\t\telse\n\t\t\tsgs[num_out + num_in++] = vbr->sg_table.sgl;\n\t}\n\n\tsg_init_one(&in_hdr, &vbr->in_hdr.status, vbr->in_hdr_len);\n\tsgs[num_out + num_in++] = &in_hdr;\n\n\treturn virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);\n}\n\nstatic int virtblk_setup_discard_write_zeroes_erase(struct request *req, bool unmap)\n{\n\tunsigned short segments = blk_rq_nr_discard_segments(req);\n\tunsigned short n = 0;\n\tstruct virtio_blk_discard_write_zeroes *range;\n\tstruct bio *bio;\n\tu32 flags = 0;\n\n\tif (unmap)\n\t\tflags |= VIRTIO_BLK_WRITE_ZEROES_FLAG_UNMAP;\n\n\trange = kmalloc_array(segments, sizeof(*range), GFP_ATOMIC);\n\tif (!range)\n\t\treturn -ENOMEM;\n\n\t \n\tif (queue_max_discard_segments(req->q) == 1) {\n\t\trange[0].flags = cpu_to_le32(flags);\n\t\trange[0].num_sectors = cpu_to_le32(blk_rq_sectors(req));\n\t\trange[0].sector = cpu_to_le64(blk_rq_pos(req));\n\t\tn = 1;\n\t} else {\n\t\t__rq_for_each_bio(bio, req) {\n\t\t\tu64 sector = bio->bi_iter.bi_sector;\n\t\t\tu32 num_sectors = bio->bi_iter.bi_size >> SECTOR_SHIFT;\n\n\t\t\trange[n].flags = cpu_to_le32(flags);\n\t\t\trange[n].num_sectors = cpu_to_le32(num_sectors);\n\t\t\trange[n].sector = cpu_to_le64(sector);\n\t\t\tn++;\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(n != segments);\n\n\tbvec_set_virt(&req->special_vec, range, sizeof(*range) * segments);\n\treq->rq_flags |= RQF_SPECIAL_PAYLOAD;\n\n\treturn 0;\n}\n\nstatic void virtblk_unmap_data(struct request *req, struct virtblk_req *vbr)\n{\n\tif (blk_rq_nr_phys_segments(req))\n\t\tsg_free_table_chained(&vbr->sg_table,\n\t\t\t\t      VIRTIO_BLK_INLINE_SG_CNT);\n}\n\nstatic int virtblk_map_data(struct blk_mq_hw_ctx *hctx, struct request *req,\n\t\tstruct virtblk_req *vbr)\n{\n\tint err;\n\n\tif (!blk_rq_nr_phys_segments(req))\n\t\treturn 0;\n\n\tvbr->sg_table.sgl = vbr->sg;\n\terr = sg_alloc_table_chained(&vbr->sg_table,\n\t\t\t\t     blk_rq_nr_phys_segments(req),\n\t\t\t\t     vbr->sg_table.sgl,\n\t\t\t\t     VIRTIO_BLK_INLINE_SG_CNT);\n\tif (unlikely(err))\n\t\treturn -ENOMEM;\n\n\treturn blk_rq_map_sg(hctx->queue, req, vbr->sg_table.sgl);\n}\n\nstatic void virtblk_cleanup_cmd(struct request *req)\n{\n\tif (req->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\tkfree(bvec_virt(&req->special_vec));\n}\n\nstatic blk_status_t virtblk_setup_cmd(struct virtio_device *vdev,\n\t\t\t\t      struct request *req,\n\t\t\t\t      struct virtblk_req *vbr)\n{\n\tsize_t in_hdr_len = sizeof(vbr->in_hdr.status);\n\tbool unmap = false;\n\tu32 type;\n\tu64 sector = 0;\n\n\tif (!IS_ENABLED(CONFIG_BLK_DEV_ZONED) && op_is_zone_mgmt(req_op(req)))\n\t\treturn BLK_STS_NOTSUPP;\n\n\t \n\tvbr->out_hdr.ioprio = cpu_to_virtio32(vdev, req_get_ioprio(req));\n\n\tswitch (req_op(req)) {\n\tcase REQ_OP_READ:\n\t\ttype = VIRTIO_BLK_T_IN;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\ttype = VIRTIO_BLK_T_OUT;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_FLUSH:\n\t\ttype = VIRTIO_BLK_T_FLUSH;\n\t\tbreak;\n\tcase REQ_OP_DISCARD:\n\t\ttype = VIRTIO_BLK_T_DISCARD;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\ttype = VIRTIO_BLK_T_WRITE_ZEROES;\n\t\tunmap = !(req->cmd_flags & REQ_NOUNMAP);\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\ttype = VIRTIO_BLK_T_SECURE_ERASE;\n\t\tbreak;\n\tcase REQ_OP_ZONE_OPEN:\n\t\ttype = VIRTIO_BLK_T_ZONE_OPEN;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_ZONE_CLOSE:\n\t\ttype = VIRTIO_BLK_T_ZONE_CLOSE;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_ZONE_FINISH:\n\t\ttype = VIRTIO_BLK_T_ZONE_FINISH;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_ZONE_APPEND:\n\t\ttype = VIRTIO_BLK_T_ZONE_APPEND;\n\t\tsector = blk_rq_pos(req);\n\t\tin_hdr_len = sizeof(vbr->in_hdr.zone_append);\n\t\tbreak;\n\tcase REQ_OP_ZONE_RESET:\n\t\ttype = VIRTIO_BLK_T_ZONE_RESET;\n\t\tsector = blk_rq_pos(req);\n\t\tbreak;\n\tcase REQ_OP_ZONE_RESET_ALL:\n\t\ttype = VIRTIO_BLK_T_ZONE_RESET_ALL;\n\t\tbreak;\n\tcase REQ_OP_DRV_IN:\n\t\t \n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\t \n\tvbr->in_hdr_len = in_hdr_len;\n\tvbr->out_hdr.type = cpu_to_virtio32(vdev, type);\n\tvbr->out_hdr.sector = cpu_to_virtio64(vdev, sector);\n\n\tif (type == VIRTIO_BLK_T_DISCARD || type == VIRTIO_BLK_T_WRITE_ZEROES ||\n\t    type == VIRTIO_BLK_T_SECURE_ERASE) {\n\t\tif (virtblk_setup_discard_write_zeroes_erase(req, unmap))\n\t\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline u8 virtblk_vbr_status(struct virtblk_req *vbr)\n{\n\treturn *((u8 *)&vbr->in_hdr + vbr->in_hdr_len - 1);\n}\n\nstatic inline void virtblk_request_done(struct request *req)\n{\n\tstruct virtblk_req *vbr = blk_mq_rq_to_pdu(req);\n\tblk_status_t status = virtblk_result(virtblk_vbr_status(vbr));\n\tstruct virtio_blk *vblk = req->mq_hctx->queue->queuedata;\n\n\tvirtblk_unmap_data(req, vbr);\n\tvirtblk_cleanup_cmd(req);\n\n\tif (req_op(req) == REQ_OP_ZONE_APPEND)\n\t\treq->__sector = virtio64_to_cpu(vblk->vdev,\n\t\t\t\t\t\tvbr->in_hdr.zone_append.sector);\n\n\tblk_mq_end_request(req, status);\n}\n\nstatic void virtblk_done(struct virtqueue *vq)\n{\n\tstruct virtio_blk *vblk = vq->vdev->priv;\n\tbool req_done = false;\n\tint qid = vq->index;\n\tstruct virtblk_req *vbr;\n\tunsigned long flags;\n\tunsigned int len;\n\n\tspin_lock_irqsave(&vblk->vqs[qid].lock, flags);\n\tdo {\n\t\tvirtqueue_disable_cb(vq);\n\t\twhile ((vbr = virtqueue_get_buf(vblk->vqs[qid].vq, &len)) != NULL) {\n\t\t\tstruct request *req = blk_mq_rq_from_pdu(vbr);\n\n\t\t\tif (likely(!blk_should_fake_timeout(req->q)))\n\t\t\t\tblk_mq_complete_request(req);\n\t\t\treq_done = true;\n\t\t}\n\t\tif (unlikely(virtqueue_is_broken(vq)))\n\t\t\tbreak;\n\t} while (!virtqueue_enable_cb(vq));\n\n\t \n\tif (req_done)\n\t\tblk_mq_start_stopped_hw_queues(vblk->disk->queue, true);\n\tspin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);\n}\n\nstatic void virtio_commit_rqs(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct virtio_blk *vblk = hctx->queue->queuedata;\n\tstruct virtio_blk_vq *vq = &vblk->vqs[hctx->queue_num];\n\tbool kick;\n\n\tspin_lock_irq(&vq->lock);\n\tkick = virtqueue_kick_prepare(vq->vq);\n\tspin_unlock_irq(&vq->lock);\n\n\tif (kick)\n\t\tvirtqueue_notify(vq->vq);\n}\n\nstatic blk_status_t virtblk_fail_to_queue(struct request *req, int rc)\n{\n\tvirtblk_cleanup_cmd(req);\n\tswitch (rc) {\n\tcase -ENOSPC:\n\t\treturn BLK_STS_DEV_RESOURCE;\n\tcase -ENOMEM:\n\t\treturn BLK_STS_RESOURCE;\n\tdefault:\n\t\treturn BLK_STS_IOERR;\n\t}\n}\n\nstatic blk_status_t virtblk_prep_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\tstruct virtio_blk *vblk,\n\t\t\t\t\tstruct request *req,\n\t\t\t\t\tstruct virtblk_req *vbr)\n{\n\tblk_status_t status;\n\tint num;\n\n\tstatus = virtblk_setup_cmd(vblk->vdev, req, vbr);\n\tif (unlikely(status))\n\t\treturn status;\n\n\tnum = virtblk_map_data(hctx, req, vbr);\n\tif (unlikely(num < 0))\n\t\treturn virtblk_fail_to_queue(req, -ENOMEM);\n\tvbr->sg_table.nents = num;\n\n\tblk_mq_start_request(req);\n\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t   const struct blk_mq_queue_data *bd)\n{\n\tstruct virtio_blk *vblk = hctx->queue->queuedata;\n\tstruct request *req = bd->rq;\n\tstruct virtblk_req *vbr = blk_mq_rq_to_pdu(req);\n\tunsigned long flags;\n\tint qid = hctx->queue_num;\n\tbool notify = false;\n\tblk_status_t status;\n\tint err;\n\n\tstatus = virtblk_prep_rq(hctx, vblk, req, vbr);\n\tif (unlikely(status))\n\t\treturn status;\n\n\tspin_lock_irqsave(&vblk->vqs[qid].lock, flags);\n\terr = virtblk_add_req(vblk->vqs[qid].vq, vbr);\n\tif (err) {\n\t\tvirtqueue_kick(vblk->vqs[qid].vq);\n\t\t \n\t\tif (err == -ENOSPC)\n\t\t\tblk_mq_stop_hw_queue(hctx);\n\t\tspin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);\n\t\tvirtblk_unmap_data(req, vbr);\n\t\treturn virtblk_fail_to_queue(req, err);\n\t}\n\n\tif (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))\n\t\tnotify = true;\n\tspin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);\n\n\tif (notify)\n\t\tvirtqueue_notify(vblk->vqs[qid].vq);\n\treturn BLK_STS_OK;\n}\n\nstatic bool virtblk_prep_rq_batch(struct request *req)\n{\n\tstruct virtio_blk *vblk = req->mq_hctx->queue->queuedata;\n\tstruct virtblk_req *vbr = blk_mq_rq_to_pdu(req);\n\n\treq->mq_hctx->tags->rqs[req->tag] = req;\n\n\treturn virtblk_prep_rq(req->mq_hctx, vblk, req, vbr) == BLK_STS_OK;\n}\n\nstatic bool virtblk_add_req_batch(struct virtio_blk_vq *vq,\n\t\t\t\t\tstruct request **rqlist)\n{\n\tunsigned long flags;\n\tint err;\n\tbool kick;\n\n\tspin_lock_irqsave(&vq->lock, flags);\n\n\twhile (!rq_list_empty(*rqlist)) {\n\t\tstruct request *req = rq_list_pop(rqlist);\n\t\tstruct virtblk_req *vbr = blk_mq_rq_to_pdu(req);\n\n\t\terr = virtblk_add_req(vq->vq, vbr);\n\t\tif (err) {\n\t\t\tvirtblk_unmap_data(req, vbr);\n\t\t\tvirtblk_cleanup_cmd(req);\n\t\t\tblk_mq_requeue_request(req, true);\n\t\t}\n\t}\n\n\tkick = virtqueue_kick_prepare(vq->vq);\n\tspin_unlock_irqrestore(&vq->lock, flags);\n\n\treturn kick;\n}\n\nstatic void virtio_queue_rqs(struct request **rqlist)\n{\n\tstruct request *req, *next, *prev = NULL;\n\tstruct request *requeue_list = NULL;\n\n\trq_list_for_each_safe(rqlist, req, next) {\n\t\tstruct virtio_blk_vq *vq = get_virtio_blk_vq(req->mq_hctx);\n\t\tbool kick;\n\n\t\tif (!virtblk_prep_rq_batch(req)) {\n\t\t\trq_list_move(rqlist, &requeue_list, req, prev);\n\t\t\treq = prev;\n\t\t\tif (!req)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (!next || req->mq_hctx != next->mq_hctx) {\n\t\t\treq->rq_next = NULL;\n\t\t\tkick = virtblk_add_req_batch(vq, rqlist);\n\t\t\tif (kick)\n\t\t\t\tvirtqueue_notify(vq->vq);\n\n\t\t\t*rqlist = next;\n\t\t\tprev = NULL;\n\t\t} else\n\t\t\tprev = req;\n\t}\n\n\t*rqlist = requeue_list;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic void *virtblk_alloc_report_buffer(struct virtio_blk *vblk,\n\t\t\t\t\t  unsigned int nr_zones,\n\t\t\t\t\t  size_t *buflen)\n{\n\tstruct request_queue *q = vblk->disk->queue;\n\tsize_t bufsize;\n\tvoid *buf;\n\n\tnr_zones = min_t(unsigned int, nr_zones,\n\t\t\t get_capacity(vblk->disk) >> ilog2(vblk->zone_sectors));\n\n\tbufsize = sizeof(struct virtio_blk_zone_report) +\n\t\tnr_zones * sizeof(struct virtio_blk_zone_descriptor);\n\tbufsize = min_t(size_t, bufsize,\n\t\t\tqueue_max_hw_sectors(q) << SECTOR_SHIFT);\n\tbufsize = min_t(size_t, bufsize, queue_max_segments(q) << PAGE_SHIFT);\n\n\twhile (bufsize >= sizeof(struct virtio_blk_zone_report)) {\n\t\tbuf = __vmalloc(bufsize, GFP_KERNEL | __GFP_NORETRY);\n\t\tif (buf) {\n\t\t\t*buflen = bufsize;\n\t\t\treturn buf;\n\t\t}\n\t\tbufsize >>= 1;\n\t}\n\n\treturn NULL;\n}\n\nstatic int virtblk_submit_zone_report(struct virtio_blk *vblk,\n\t\t\t\t       char *report_buf, size_t report_len,\n\t\t\t\t       sector_t sector)\n{\n\tstruct request_queue *q = vblk->disk->queue;\n\tstruct request *req;\n\tstruct virtblk_req *vbr;\n\tint err;\n\n\treq = blk_mq_alloc_request(q, REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\tvbr = blk_mq_rq_to_pdu(req);\n\tvbr->in_hdr_len = sizeof(vbr->in_hdr.status);\n\tvbr->out_hdr.type = cpu_to_virtio32(vblk->vdev, VIRTIO_BLK_T_ZONE_REPORT);\n\tvbr->out_hdr.sector = cpu_to_virtio64(vblk->vdev, sector);\n\n\terr = blk_rq_map_kern(q, req, report_buf, report_len, GFP_KERNEL);\n\tif (err)\n\t\tgoto out;\n\n\tblk_execute_rq(req, false);\n\terr = blk_status_to_errno(virtblk_result(vbr->in_hdr.status));\nout:\n\tblk_mq_free_request(req);\n\treturn err;\n}\n\nstatic int virtblk_parse_zone(struct virtio_blk *vblk,\n\t\t\t       struct virtio_blk_zone_descriptor *entry,\n\t\t\t       unsigned int idx, report_zones_cb cb, void *data)\n{\n\tstruct blk_zone zone = { };\n\n\tzone.start = virtio64_to_cpu(vblk->vdev, entry->z_start);\n\tif (zone.start + vblk->zone_sectors <= get_capacity(vblk->disk))\n\t\tzone.len = vblk->zone_sectors;\n\telse\n\t\tzone.len = get_capacity(vblk->disk) - zone.start;\n\tzone.capacity = virtio64_to_cpu(vblk->vdev, entry->z_cap);\n\tzone.wp = virtio64_to_cpu(vblk->vdev, entry->z_wp);\n\n\tswitch (entry->z_type) {\n\tcase VIRTIO_BLK_ZT_SWR:\n\t\tzone.type = BLK_ZONE_TYPE_SEQWRITE_REQ;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZT_SWP:\n\t\tzone.type = BLK_ZONE_TYPE_SEQWRITE_PREF;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZT_CONV:\n\t\tzone.type = BLK_ZONE_TYPE_CONVENTIONAL;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&vblk->vdev->dev, \"zone %llu: invalid type %#x\\n\",\n\t\t\tzone.start, entry->z_type);\n\t\treturn -EIO;\n\t}\n\n\tswitch (entry->z_state) {\n\tcase VIRTIO_BLK_ZS_EMPTY:\n\t\tzone.cond = BLK_ZONE_COND_EMPTY;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_CLOSED:\n\t\tzone.cond = BLK_ZONE_COND_CLOSED;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_FULL:\n\t\tzone.cond = BLK_ZONE_COND_FULL;\n\t\tzone.wp = zone.start + zone.len;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_EOPEN:\n\t\tzone.cond = BLK_ZONE_COND_EXP_OPEN;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_IOPEN:\n\t\tzone.cond = BLK_ZONE_COND_IMP_OPEN;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_NOT_WP:\n\t\tzone.cond = BLK_ZONE_COND_NOT_WP;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_RDONLY:\n\t\tzone.cond = BLK_ZONE_COND_READONLY;\n\t\tzone.wp = ULONG_MAX;\n\t\tbreak;\n\tcase VIRTIO_BLK_ZS_OFFLINE:\n\t\tzone.cond = BLK_ZONE_COND_OFFLINE;\n\t\tzone.wp = ULONG_MAX;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&vblk->vdev->dev, \"zone %llu: invalid condition %#x\\n\",\n\t\t\tzone.start, entry->z_state);\n\t\treturn -EIO;\n\t}\n\n\t \n\treturn cb(&zone, idx, data);\n}\n\nstatic int virtblk_report_zones(struct gendisk *disk, sector_t sector,\n\t\t\t\t unsigned int nr_zones, report_zones_cb cb,\n\t\t\t\t void *data)\n{\n\tstruct virtio_blk *vblk = disk->private_data;\n\tstruct virtio_blk_zone_report *report;\n\tunsigned long long nz, i;\n\tsize_t buflen;\n\tunsigned int zone_idx = 0;\n\tint ret;\n\n\tif (WARN_ON_ONCE(!vblk->zone_sectors))\n\t\treturn -EOPNOTSUPP;\n\n\treport = virtblk_alloc_report_buffer(vblk, nr_zones, &buflen);\n\tif (!report)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&vblk->vdev_mutex);\n\n\tif (!vblk->vdev) {\n\t\tret = -ENXIO;\n\t\tgoto fail_report;\n\t}\n\n\twhile (zone_idx < nr_zones && sector < get_capacity(vblk->disk)) {\n\t\tmemset(report, 0, buflen);\n\n\t\tret = virtblk_submit_zone_report(vblk, (char *)report,\n\t\t\t\t\t\t buflen, sector);\n\t\tif (ret)\n\t\t\tgoto fail_report;\n\n\t\tnz = min_t(u64, virtio64_to_cpu(vblk->vdev, report->nr_zones),\n\t\t\t   nr_zones);\n\t\tif (!nz)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nz && zone_idx < nr_zones; i++) {\n\t\t\tret = virtblk_parse_zone(vblk, &report->zones[i],\n\t\t\t\t\t\t zone_idx, cb, data);\n\t\t\tif (ret)\n\t\t\t\tgoto fail_report;\n\n\t\t\tsector = virtio64_to_cpu(vblk->vdev,\n\t\t\t\t\t\t report->zones[i].z_start) +\n\t\t\t\t vblk->zone_sectors;\n\t\t\tzone_idx++;\n\t\t}\n\t}\n\n\tif (zone_idx > 0)\n\t\tret = zone_idx;\n\telse\n\t\tret = -EINVAL;\nfail_report:\n\tmutex_unlock(&vblk->vdev_mutex);\n\tkvfree(report);\n\treturn ret;\n}\n\nstatic void virtblk_revalidate_zones(struct virtio_blk *vblk)\n{\n\tu8 model;\n\n\tvirtio_cread(vblk->vdev, struct virtio_blk_config,\n\t\t     zoned.model, &model);\n\tswitch (model) {\n\tdefault:\n\t\tdev_err(&vblk->vdev->dev, \"unknown zone model %d\\n\", model);\n\t\tfallthrough;\n\tcase VIRTIO_BLK_Z_NONE:\n\tcase VIRTIO_BLK_Z_HA:\n\t\tdisk_set_zoned(vblk->disk, BLK_ZONED_NONE);\n\t\treturn;\n\tcase VIRTIO_BLK_Z_HM:\n\t\tWARN_ON_ONCE(!vblk->zone_sectors);\n\t\tif (!blk_revalidate_disk_zones(vblk->disk, NULL))\n\t\t\tset_capacity_and_notify(vblk->disk, 0);\n\t}\n}\n\nstatic int virtblk_probe_zoned_device(struct virtio_device *vdev,\n\t\t\t\t       struct virtio_blk *vblk,\n\t\t\t\t       struct request_queue *q)\n{\n\tu32 v, wg;\n\tu8 model;\n\n\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t     zoned.model, &model);\n\n\tswitch (model) {\n\tcase VIRTIO_BLK_Z_NONE:\n\tcase VIRTIO_BLK_Z_HA:\n\t\t \n\t\treturn 0;\n\tcase VIRTIO_BLK_Z_HM:\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&vdev->dev, \"unsupported zone model %d\\n\", model);\n\t\treturn -EINVAL;\n\t}\n\n\tdev_dbg(&vdev->dev, \"probing host-managed zoned device\\n\");\n\n\tdisk_set_zoned(vblk->disk, BLK_ZONED_HM);\n\tblk_queue_flag_set(QUEUE_FLAG_ZONE_RESETALL, q);\n\n\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t     zoned.max_open_zones, &v);\n\tdisk_set_max_open_zones(vblk->disk, v);\n\tdev_dbg(&vdev->dev, \"max open zones = %u\\n\", v);\n\n\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t     zoned.max_active_zones, &v);\n\tdisk_set_max_active_zones(vblk->disk, v);\n\tdev_dbg(&vdev->dev, \"max active zones = %u\\n\", v);\n\n\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t     zoned.write_granularity, &wg);\n\tif (!wg) {\n\t\tdev_warn(&vdev->dev, \"zero write granularity reported\\n\");\n\t\treturn -ENODEV;\n\t}\n\tblk_queue_physical_block_size(q, wg);\n\tblk_queue_io_min(q, wg);\n\n\tdev_dbg(&vdev->dev, \"write granularity = %u\\n\", wg);\n\n\t \n\tvirtio_cread(vdev, struct virtio_blk_config, zoned.zone_sectors,\n\t\t     &vblk->zone_sectors);\n\tif (vblk->zone_sectors == 0 || !is_power_of_2(vblk->zone_sectors)) {\n\t\tdev_err(&vdev->dev,\n\t\t\t\"zoned device with non power of two zone size %u\\n\",\n\t\t\tvblk->zone_sectors);\n\t\treturn -ENODEV;\n\t}\n\tblk_queue_chunk_sectors(q, vblk->zone_sectors);\n\tdev_dbg(&vdev->dev, \"zone sectors = %u\\n\", vblk->zone_sectors);\n\n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_DISCARD)) {\n\t\tdev_warn(&vblk->vdev->dev,\n\t\t\t \"ignoring negotiated F_DISCARD for zoned device\\n\");\n\t\tblk_queue_max_discard_sectors(q, 0);\n\t}\n\n\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t     zoned.max_append_sectors, &v);\n\tif (!v) {\n\t\tdev_warn(&vdev->dev, \"zero max_append_sectors reported\\n\");\n\t\treturn -ENODEV;\n\t}\n\tif ((v << SECTOR_SHIFT) < wg) {\n\t\tdev_err(&vdev->dev,\n\t\t\t\"write granularity %u exceeds max_append_sectors %u limit\\n\",\n\t\t\twg, v);\n\t\treturn -ENODEV;\n\t}\n\tblk_queue_max_zone_append_sectors(q, v);\n\tdev_dbg(&vdev->dev, \"max append sectors = %u\\n\", v);\n\n\treturn blk_revalidate_disk_zones(vblk->disk, NULL);\n}\n\n#else\n\n \n#define virtblk_report_zones       NULL\n\nstatic inline void virtblk_revalidate_zones(struct virtio_blk *vblk)\n{\n}\n\nstatic inline int virtblk_probe_zoned_device(struct virtio_device *vdev,\n\t\t\tstruct virtio_blk *vblk, struct request_queue *q)\n{\n\tu8 model;\n\n\tvirtio_cread(vdev, struct virtio_blk_config, zoned.model, &model);\n\tif (model == VIRTIO_BLK_Z_HM) {\n\t\tdev_err(&vdev->dev,\n\t\t\t\"virtio_blk: zoned devices are not supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n#endif  \n\n \nstatic int virtblk_get_id(struct gendisk *disk, char *id_str)\n{\n\tstruct virtio_blk *vblk = disk->private_data;\n\tstruct request_queue *q = vblk->disk->queue;\n\tstruct request *req;\n\tstruct virtblk_req *vbr;\n\tint err;\n\n\treq = blk_mq_alloc_request(q, REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\tvbr = blk_mq_rq_to_pdu(req);\n\tvbr->in_hdr_len = sizeof(vbr->in_hdr.status);\n\tvbr->out_hdr.type = cpu_to_virtio32(vblk->vdev, VIRTIO_BLK_T_GET_ID);\n\tvbr->out_hdr.sector = 0;\n\n\terr = blk_rq_map_kern(q, req, id_str, VIRTIO_BLK_ID_BYTES, GFP_KERNEL);\n\tif (err)\n\t\tgoto out;\n\n\tblk_execute_rq(req, false);\n\terr = blk_status_to_errno(virtblk_result(vbr->in_hdr.status));\nout:\n\tblk_mq_free_request(req);\n\treturn err;\n}\n\n \nstatic int virtblk_getgeo(struct block_device *bd, struct hd_geometry *geo)\n{\n\tstruct virtio_blk *vblk = bd->bd_disk->private_data;\n\tint ret = 0;\n\n\tmutex_lock(&vblk->vdev_mutex);\n\n\tif (!vblk->vdev) {\n\t\tret = -ENXIO;\n\t\tgoto out;\n\t}\n\n\t \n\tif (virtio_has_feature(vblk->vdev, VIRTIO_BLK_F_GEOMETRY)) {\n\t\tvirtio_cread(vblk->vdev, struct virtio_blk_config,\n\t\t\t     geometry.cylinders, &geo->cylinders);\n\t\tvirtio_cread(vblk->vdev, struct virtio_blk_config,\n\t\t\t     geometry.heads, &geo->heads);\n\t\tvirtio_cread(vblk->vdev, struct virtio_blk_config,\n\t\t\t     geometry.sectors, &geo->sectors);\n\t} else {\n\t\t \n\t\tgeo->heads = 1 << 6;\n\t\tgeo->sectors = 1 << 5;\n\t\tgeo->cylinders = get_capacity(bd->bd_disk) >> 11;\n\t}\nout:\n\tmutex_unlock(&vblk->vdev_mutex);\n\treturn ret;\n}\n\nstatic void virtblk_free_disk(struct gendisk *disk)\n{\n\tstruct virtio_blk *vblk = disk->private_data;\n\n\tida_free(&vd_index_ida, vblk->index);\n\tmutex_destroy(&vblk->vdev_mutex);\n\tkfree(vblk);\n}\n\nstatic const struct block_device_operations virtblk_fops = {\n\t.owner  \t= THIS_MODULE,\n\t.getgeo\t\t= virtblk_getgeo,\n\t.free_disk\t= virtblk_free_disk,\n\t.report_zones\t= virtblk_report_zones,\n};\n\nstatic int index_to_minor(int index)\n{\n\treturn index << PART_BITS;\n}\n\nstatic int minor_to_index(int minor)\n{\n\treturn minor >> PART_BITS;\n}\n\nstatic ssize_t serial_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tint err;\n\n\t \n\tBUILD_BUG_ON(PAGE_SIZE < VIRTIO_BLK_ID_BYTES);\n\n\tbuf[VIRTIO_BLK_ID_BYTES] = '\\0';\n\terr = virtblk_get_id(disk, buf);\n\tif (!err)\n\t\treturn strlen(buf);\n\n\tif (err == -EIO)  \n\t\treturn 0;\n\n\treturn err;\n}\n\nstatic DEVICE_ATTR_RO(serial);\n\n \nstatic void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)\n{\n\tstruct virtio_device *vdev = vblk->vdev;\n\tstruct request_queue *q = vblk->disk->queue;\n\tchar cap_str_2[10], cap_str_10[10];\n\tunsigned long long nblocks;\n\tu64 capacity;\n\n\t \n\tvirtio_cread(vdev, struct virtio_blk_config, capacity, &capacity);\n\n\tnblocks = DIV_ROUND_UP_ULL(capacity, queue_logical_block_size(q) >> 9);\n\n\tstring_get_size(nblocks, queue_logical_block_size(q),\n\t\t\tSTRING_UNITS_2, cap_str_2, sizeof(cap_str_2));\n\tstring_get_size(nblocks, queue_logical_block_size(q),\n\t\t\tSTRING_UNITS_10, cap_str_10, sizeof(cap_str_10));\n\n\tdev_notice(&vdev->dev,\n\t\t   \"[%s] %s%llu %d-byte logical blocks (%s/%s)\\n\",\n\t\t   vblk->disk->disk_name,\n\t\t   resize ? \"new size: \" : \"\",\n\t\t   nblocks,\n\t\t   queue_logical_block_size(q),\n\t\t   cap_str_10,\n\t\t   cap_str_2);\n\n\tset_capacity_and_notify(vblk->disk, capacity);\n}\n\nstatic void virtblk_config_changed_work(struct work_struct *work)\n{\n\tstruct virtio_blk *vblk =\n\t\tcontainer_of(work, struct virtio_blk, config_work);\n\n\tvirtblk_revalidate_zones(vblk);\n\tvirtblk_update_capacity(vblk, true);\n}\n\nstatic void virtblk_config_changed(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk = vdev->priv;\n\n\tqueue_work(virtblk_wq, &vblk->config_work);\n}\n\nstatic int init_vq(struct virtio_blk *vblk)\n{\n\tint err;\n\tunsigned short i;\n\tvq_callback_t **callbacks;\n\tconst char **names;\n\tstruct virtqueue **vqs;\n\tunsigned short num_vqs;\n\tunsigned short num_poll_vqs;\n\tstruct virtio_device *vdev = vblk->vdev;\n\tstruct irq_affinity desc = { 0, };\n\n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_MQ,\n\t\t\t\t   struct virtio_blk_config, num_queues,\n\t\t\t\t   &num_vqs);\n\tif (err)\n\t\tnum_vqs = 1;\n\n\tif (!err && !num_vqs) {\n\t\tdev_err(&vdev->dev, \"MQ advertised but zero queues reported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tnum_vqs = min_t(unsigned int,\n\t\t\tmin_not_zero(num_request_queues, nr_cpu_ids),\n\t\t\tnum_vqs);\n\n\tnum_poll_vqs = min_t(unsigned int, poll_queues, num_vqs - 1);\n\n\tvblk->io_queues[HCTX_TYPE_DEFAULT] = num_vqs - num_poll_vqs;\n\tvblk->io_queues[HCTX_TYPE_READ] = 0;\n\tvblk->io_queues[HCTX_TYPE_POLL] = num_poll_vqs;\n\n\tdev_info(&vdev->dev, \"%d/%d/%d default/read/poll queues\\n\",\n\t\t\t\tvblk->io_queues[HCTX_TYPE_DEFAULT],\n\t\t\t\tvblk->io_queues[HCTX_TYPE_READ],\n\t\t\t\tvblk->io_queues[HCTX_TYPE_POLL]);\n\n\tvblk->vqs = kmalloc_array(num_vqs, sizeof(*vblk->vqs), GFP_KERNEL);\n\tif (!vblk->vqs)\n\t\treturn -ENOMEM;\n\n\tnames = kmalloc_array(num_vqs, sizeof(*names), GFP_KERNEL);\n\tcallbacks = kmalloc_array(num_vqs, sizeof(*callbacks), GFP_KERNEL);\n\tvqs = kmalloc_array(num_vqs, sizeof(*vqs), GFP_KERNEL);\n\tif (!names || !callbacks || !vqs) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_vqs - num_poll_vqs; i++) {\n\t\tcallbacks[i] = virtblk_done;\n\t\tsnprintf(vblk->vqs[i].name, VQ_NAME_LEN, \"req.%u\", i);\n\t\tnames[i] = vblk->vqs[i].name;\n\t}\n\n\tfor (; i < num_vqs; i++) {\n\t\tcallbacks[i] = NULL;\n\t\tsnprintf(vblk->vqs[i].name, VQ_NAME_LEN, \"req_poll.%u\", i);\n\t\tnames[i] = vblk->vqs[i].name;\n\t}\n\n\t \n\terr = virtio_find_vqs(vdev, num_vqs, vqs, callbacks, names, &desc);\n\tif (err)\n\t\tgoto out;\n\n\tfor (i = 0; i < num_vqs; i++) {\n\t\tspin_lock_init(&vblk->vqs[i].lock);\n\t\tvblk->vqs[i].vq = vqs[i];\n\t}\n\tvblk->num_vqs = num_vqs;\n\nout:\n\tkfree(vqs);\n\tkfree(callbacks);\n\tkfree(names);\n\tif (err)\n\t\tkfree(vblk->vqs);\n\treturn err;\n}\n\n \nstatic int virtblk_name_format(char *prefix, int index, char *buf, int buflen)\n{\n\tconst int base = 'z' - 'a' + 1;\n\tchar *begin = buf + strlen(prefix);\n\tchar *end = buf + buflen;\n\tchar *p;\n\tint unit;\n\n\tp = end - 1;\n\t*p = '\\0';\n\tunit = base;\n\tdo {\n\t\tif (p == begin)\n\t\t\treturn -EINVAL;\n\t\t*--p = 'a' + (index % unit);\n\t\tindex = (index / unit) - 1;\n\t} while (index >= 0);\n\n\tmemmove(begin, p, end - p);\n\tmemcpy(buf, prefix, strlen(prefix));\n\n\treturn 0;\n}\n\nstatic int virtblk_get_cache_mode(struct virtio_device *vdev)\n{\n\tu8 writeback;\n\tint err;\n\n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_CONFIG_WCE,\n\t\t\t\t   struct virtio_blk_config, wce,\n\t\t\t\t   &writeback);\n\n\t \n\tif (err)\n\t\twriteback = virtio_has_feature(vdev, VIRTIO_BLK_F_FLUSH);\n\n\treturn writeback;\n}\n\nstatic void virtblk_update_cache_mode(struct virtio_device *vdev)\n{\n\tu8 writeback = virtblk_get_cache_mode(vdev);\n\tstruct virtio_blk *vblk = vdev->priv;\n\n\tblk_queue_write_cache(vblk->disk->queue, writeback, false);\n}\n\nstatic const char *const virtblk_cache_types[] = {\n\t\"write through\", \"write back\"\n};\n\nstatic ssize_t\ncache_type_store(struct device *dev, struct device_attribute *attr,\n\t\t const char *buf, size_t count)\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct virtio_blk *vblk = disk->private_data;\n\tstruct virtio_device *vdev = vblk->vdev;\n\tint i;\n\n\tBUG_ON(!virtio_has_feature(vblk->vdev, VIRTIO_BLK_F_CONFIG_WCE));\n\ti = sysfs_match_string(virtblk_cache_types, buf);\n\tif (i < 0)\n\t\treturn i;\n\n\tvirtio_cwrite8(vdev, offsetof(struct virtio_blk_config, wce), i);\n\tvirtblk_update_cache_mode(vdev);\n\treturn count;\n}\n\nstatic ssize_t\ncache_type_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct virtio_blk *vblk = disk->private_data;\n\tu8 writeback = virtblk_get_cache_mode(vblk->vdev);\n\n\tBUG_ON(writeback >= ARRAY_SIZE(virtblk_cache_types));\n\treturn sysfs_emit(buf, \"%s\\n\", virtblk_cache_types[writeback]);\n}\n\nstatic DEVICE_ATTR_RW(cache_type);\n\nstatic struct attribute *virtblk_attrs[] = {\n\t&dev_attr_serial.attr,\n\t&dev_attr_cache_type.attr,\n\tNULL,\n};\n\nstatic umode_t virtblk_attrs_are_visible(struct kobject *kobj,\n\t\tstruct attribute *a, int n)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct virtio_blk *vblk = disk->private_data;\n\tstruct virtio_device *vdev = vblk->vdev;\n\n\tif (a == &dev_attr_cache_type.attr &&\n\t    !virtio_has_feature(vdev, VIRTIO_BLK_F_CONFIG_WCE))\n\t\treturn S_IRUGO;\n\n\treturn a->mode;\n}\n\nstatic const struct attribute_group virtblk_attr_group = {\n\t.attrs = virtblk_attrs,\n\t.is_visible = virtblk_attrs_are_visible,\n};\n\nstatic const struct attribute_group *virtblk_attr_groups[] = {\n\t&virtblk_attr_group,\n\tNULL,\n};\n\nstatic void virtblk_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct virtio_blk *vblk = set->driver_data;\n\tint i, qoff;\n\n\tfor (i = 0, qoff = 0; i < set->nr_maps; i++) {\n\t\tstruct blk_mq_queue_map *map = &set->map[i];\n\n\t\tmap->nr_queues = vblk->io_queues[i];\n\t\tmap->queue_offset = qoff;\n\t\tqoff += map->nr_queues;\n\n\t\tif (map->nr_queues == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (i == HCTX_TYPE_POLL)\n\t\t\tblk_mq_map_queues(&set->map[i]);\n\t\telse\n\t\t\tblk_mq_virtio_map_queues(&set->map[i], vblk->vdev, 0);\n\t}\n}\n\nstatic void virtblk_complete_batch(struct io_comp_batch *iob)\n{\n\tstruct request *req;\n\n\trq_list_for_each(&iob->req_list, req) {\n\t\tvirtblk_unmap_data(req, blk_mq_rq_to_pdu(req));\n\t\tvirtblk_cleanup_cmd(req);\n\t}\n\tblk_mq_end_request_batch(iob);\n}\n\nstatic int virtblk_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct virtio_blk *vblk = hctx->queue->queuedata;\n\tstruct virtio_blk_vq *vq = get_virtio_blk_vq(hctx);\n\tstruct virtblk_req *vbr;\n\tunsigned long flags;\n\tunsigned int len;\n\tint found = 0;\n\n\tspin_lock_irqsave(&vq->lock, flags);\n\n\twhile ((vbr = virtqueue_get_buf(vq->vq, &len)) != NULL) {\n\t\tstruct request *req = blk_mq_rq_from_pdu(vbr);\n\n\t\tfound++;\n\t\tif (!blk_mq_complete_request_remote(req) &&\n\t\t    !blk_mq_add_to_batch(req, iob, virtblk_vbr_status(vbr),\n\t\t\t\t\t\tvirtblk_complete_batch))\n\t\t\tvirtblk_request_done(req);\n\t}\n\n\tif (found)\n\t\tblk_mq_start_stopped_hw_queues(vblk->disk->queue, true);\n\n\tspin_unlock_irqrestore(&vq->lock, flags);\n\n\treturn found;\n}\n\nstatic const struct blk_mq_ops virtio_mq_ops = {\n\t.queue_rq\t= virtio_queue_rq,\n\t.queue_rqs\t= virtio_queue_rqs,\n\t.commit_rqs\t= virtio_commit_rqs,\n\t.complete\t= virtblk_request_done,\n\t.map_queues\t= virtblk_map_queues,\n\t.poll\t\t= virtblk_poll,\n};\n\nstatic unsigned int virtblk_queue_depth;\nmodule_param_named(queue_depth, virtblk_queue_depth, uint, 0444);\n\nstatic int virtblk_probe(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk;\n\tstruct request_queue *q;\n\tint err, index;\n\n\tu32 v, blk_size, max_size, sg_elems, opt_io_size;\n\tu32 max_discard_segs = 0;\n\tu32 discard_granularity = 0;\n\tu16 min_io_size;\n\tu8 physical_block_exp, alignment_offset;\n\tunsigned int queue_depth;\n\tsize_t max_dma_size;\n\n\tif (!vdev->config->get) {\n\t\tdev_err(&vdev->dev, \"%s failure: config access disabled\\n\",\n\t\t\t__func__);\n\t\treturn -EINVAL;\n\t}\n\n\terr = ida_alloc_range(&vd_index_ida, 0,\n\t\t\t      minor_to_index(1 << MINORBITS) - 1, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto out;\n\tindex = err;\n\n\t \n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_SEG_MAX,\n\t\t\t\t   struct virtio_blk_config, seg_max,\n\t\t\t\t   &sg_elems);\n\n\t \n\tif (err || !sg_elems)\n\t\tsg_elems = 1;\n\n\t \n\tsg_elems = min_t(u32, sg_elems, VIRTIO_BLK_MAX_SG_ELEMS - 2);\n\n\tvdev->priv = vblk = kmalloc(sizeof(*vblk), GFP_KERNEL);\n\tif (!vblk) {\n\t\terr = -ENOMEM;\n\t\tgoto out_free_index;\n\t}\n\n\tmutex_init(&vblk->vdev_mutex);\n\n\tvblk->vdev = vdev;\n\n\tINIT_WORK(&vblk->config_work, virtblk_config_changed_work);\n\n\terr = init_vq(vblk);\n\tif (err)\n\t\tgoto out_free_vblk;\n\n\t \n\tif (!virtblk_queue_depth) {\n\t\tqueue_depth = vblk->vqs[0].vq->num_free;\n\t\t \n\t\tif (!virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC))\n\t\t\tqueue_depth /= 2;\n\t} else {\n\t\tqueue_depth = virtblk_queue_depth;\n\t}\n\n\tmemset(&vblk->tag_set, 0, sizeof(vblk->tag_set));\n\tvblk->tag_set.ops = &virtio_mq_ops;\n\tvblk->tag_set.queue_depth = queue_depth;\n\tvblk->tag_set.numa_node = NUMA_NO_NODE;\n\tvblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\n\tvblk->tag_set.cmd_size =\n\t\tsizeof(struct virtblk_req) +\n\t\tsizeof(struct scatterlist) * VIRTIO_BLK_INLINE_SG_CNT;\n\tvblk->tag_set.driver_data = vblk;\n\tvblk->tag_set.nr_hw_queues = vblk->num_vqs;\n\tvblk->tag_set.nr_maps = 1;\n\tif (vblk->io_queues[HCTX_TYPE_POLL])\n\t\tvblk->tag_set.nr_maps = 3;\n\n\terr = blk_mq_alloc_tag_set(&vblk->tag_set);\n\tif (err)\n\t\tgoto out_free_vq;\n\n\tvblk->disk = blk_mq_alloc_disk(&vblk->tag_set, vblk);\n\tif (IS_ERR(vblk->disk)) {\n\t\terr = PTR_ERR(vblk->disk);\n\t\tgoto out_free_tags;\n\t}\n\tq = vblk->disk->queue;\n\n\tvirtblk_name_format(\"vd\", index, vblk->disk->disk_name, DISK_NAME_LEN);\n\n\tvblk->disk->major = major;\n\tvblk->disk->first_minor = index_to_minor(index);\n\tvblk->disk->minors = 1 << PART_BITS;\n\tvblk->disk->private_data = vblk;\n\tvblk->disk->fops = &virtblk_fops;\n\tvblk->index = index;\n\n\t \n\tvirtblk_update_cache_mode(vdev);\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_RO))\n\t\tset_disk_ro(vblk->disk, 1);\n\n\t \n\tblk_queue_max_segments(q, sg_elems);\n\n\t \n\tblk_queue_max_hw_sectors(q, UINT_MAX);\n\n\tmax_dma_size = virtio_max_dma_size(vdev);\n\tmax_size = max_dma_size > U32_MAX ? U32_MAX : max_dma_size;\n\n\t \n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_SIZE_MAX,\n\t\t\t\t   struct virtio_blk_config, size_max, &v);\n\tif (!err)\n\t\tmax_size = min(max_size, v);\n\n\tblk_queue_max_segment_size(q, max_size);\n\n\t \n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_BLK_SIZE,\n\t\t\t\t   struct virtio_blk_config, blk_size,\n\t\t\t\t   &blk_size);\n\tif (!err) {\n\t\terr = blk_validate_block_size(blk_size);\n\t\tif (err) {\n\t\t\tdev_err(&vdev->dev,\n\t\t\t\t\"virtio_blk: invalid block size: 0x%x\\n\",\n\t\t\t\tblk_size);\n\t\t\tgoto out_cleanup_disk;\n\t\t}\n\n\t\tblk_queue_logical_block_size(q, blk_size);\n\t} else\n\t\tblk_size = queue_logical_block_size(q);\n\n\t \n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_TOPOLOGY,\n\t\t\t\t   struct virtio_blk_config, physical_block_exp,\n\t\t\t\t   &physical_block_exp);\n\tif (!err && physical_block_exp)\n\t\tblk_queue_physical_block_size(q,\n\t\t\t\tblk_size * (1 << physical_block_exp));\n\n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_TOPOLOGY,\n\t\t\t\t   struct virtio_blk_config, alignment_offset,\n\t\t\t\t   &alignment_offset);\n\tif (!err && alignment_offset)\n\t\tblk_queue_alignment_offset(q, blk_size * alignment_offset);\n\n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_TOPOLOGY,\n\t\t\t\t   struct virtio_blk_config, min_io_size,\n\t\t\t\t   &min_io_size);\n\tif (!err && min_io_size)\n\t\tblk_queue_io_min(q, blk_size * min_io_size);\n\n\terr = virtio_cread_feature(vdev, VIRTIO_BLK_F_TOPOLOGY,\n\t\t\t\t   struct virtio_blk_config, opt_io_size,\n\t\t\t\t   &opt_io_size);\n\tif (!err && opt_io_size)\n\t\tblk_queue_io_opt(q, blk_size * opt_io_size);\n\n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_DISCARD)) {\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     discard_sector_alignment, &discard_granularity);\n\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     max_discard_sectors, &v);\n\t\tblk_queue_max_discard_sectors(q, v ? v : UINT_MAX);\n\n\t\tvirtio_cread(vdev, struct virtio_blk_config, max_discard_seg,\n\t\t\t     &max_discard_segs);\n\t}\n\n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_WRITE_ZEROES)) {\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     max_write_zeroes_sectors, &v);\n\t\tblk_queue_max_write_zeroes_sectors(q, v ? v : UINT_MAX);\n\t}\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_SECURE_ERASE)) {\n\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     secure_erase_sector_alignment, &v);\n\n\t\t \n\t\tif (!v) {\n\t\t\tdev_err(&vdev->dev,\n\t\t\t\t\"virtio_blk: secure_erase_sector_alignment can't be 0\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_cleanup_disk;\n\t\t}\n\n\t\tdiscard_granularity = min_not_zero(discard_granularity, v);\n\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     max_secure_erase_sectors, &v);\n\n\t\t \n\t\tif (!v) {\n\t\t\tdev_err(&vdev->dev,\n\t\t\t\t\"virtio_blk: max_secure_erase_sectors can't be 0\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_cleanup_disk;\n\t\t}\n\n\t\tblk_queue_max_secure_erase_sectors(q, v);\n\n\t\tvirtio_cread(vdev, struct virtio_blk_config,\n\t\t\t     max_secure_erase_seg, &v);\n\n\t\t \n\t\tif (!v) {\n\t\t\tdev_err(&vdev->dev,\n\t\t\t\t\"virtio_blk: max_secure_erase_seg can't be 0\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_cleanup_disk;\n\t\t}\n\n\t\tmax_discard_segs = min_not_zero(max_discard_segs, v);\n\t}\n\n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_DISCARD) ||\n\t    virtio_has_feature(vdev, VIRTIO_BLK_F_SECURE_ERASE)) {\n\t\t \n\t\tif (!max_discard_segs)\n\t\t\tmax_discard_segs = sg_elems;\n\n\t\tblk_queue_max_discard_segments(q,\n\t\t\t\t\t       min(max_discard_segs, MAX_DISCARD_SEGMENTS));\n\n\t\tif (discard_granularity)\n\t\t\tq->limits.discard_granularity = discard_granularity << SECTOR_SHIFT;\n\t\telse\n\t\t\tq->limits.discard_granularity = blk_size;\n\t}\n\n\tvirtblk_update_capacity(vblk, false);\n\tvirtio_device_ready(vdev);\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_BLK_F_ZONED)) {\n\t\terr = virtblk_probe_zoned_device(vdev, vblk, q);\n\t\tif (err)\n\t\t\tgoto out_cleanup_disk;\n\t}\n\n\terr = device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);\n\tif (err)\n\t\tgoto out_cleanup_disk;\n\n\treturn 0;\n\nout_cleanup_disk:\n\tput_disk(vblk->disk);\nout_free_tags:\n\tblk_mq_free_tag_set(&vblk->tag_set);\nout_free_vq:\n\tvdev->config->del_vqs(vdev);\n\tkfree(vblk->vqs);\nout_free_vblk:\n\tkfree(vblk);\nout_free_index:\n\tida_free(&vd_index_ida, index);\nout:\n\treturn err;\n}\n\nstatic void virtblk_remove(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk = vdev->priv;\n\n\t \n\tflush_work(&vblk->config_work);\n\n\tdel_gendisk(vblk->disk);\n\tblk_mq_free_tag_set(&vblk->tag_set);\n\n\tmutex_lock(&vblk->vdev_mutex);\n\n\t \n\tvirtio_reset_device(vdev);\n\n\t \n\tvblk->vdev = NULL;\n\n\tvdev->config->del_vqs(vdev);\n\tkfree(vblk->vqs);\n\n\tmutex_unlock(&vblk->vdev_mutex);\n\n\tput_disk(vblk->disk);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int virtblk_freeze(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk = vdev->priv;\n\n\t \n\tvirtio_reset_device(vdev);\n\n\t \n\tflush_work(&vblk->config_work);\n\n\tblk_mq_quiesce_queue(vblk->disk->queue);\n\n\tvdev->config->del_vqs(vdev);\n\tkfree(vblk->vqs);\n\n\treturn 0;\n}\n\nstatic int virtblk_restore(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk = vdev->priv;\n\tint ret;\n\n\tret = init_vq(vdev->priv);\n\tif (ret)\n\t\treturn ret;\n\n\tvirtio_device_ready(vdev);\n\n\tblk_mq_unquiesce_queue(vblk->disk->queue);\n\treturn 0;\n}\n#endif\n\nstatic const struct virtio_device_id id_table[] = {\n\t{ VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },\n\t{ 0 },\n};\n\nstatic unsigned int features_legacy[] = {\n\tVIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,\n\tVIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,\n\tVIRTIO_BLK_F_FLUSH, VIRTIO_BLK_F_TOPOLOGY, VIRTIO_BLK_F_CONFIG_WCE,\n\tVIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,\n\tVIRTIO_BLK_F_SECURE_ERASE,\n}\n;\nstatic unsigned int features[] = {\n\tVIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,\n\tVIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,\n\tVIRTIO_BLK_F_FLUSH, VIRTIO_BLK_F_TOPOLOGY, VIRTIO_BLK_F_CONFIG_WCE,\n\tVIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,\n\tVIRTIO_BLK_F_SECURE_ERASE, VIRTIO_BLK_F_ZONED,\n};\n\nstatic struct virtio_driver virtio_blk = {\n\t.feature_table\t\t\t= features,\n\t.feature_table_size\t\t= ARRAY_SIZE(features),\n\t.feature_table_legacy\t\t= features_legacy,\n\t.feature_table_size_legacy\t= ARRAY_SIZE(features_legacy),\n\t.driver.name\t\t\t= KBUILD_MODNAME,\n\t.driver.owner\t\t\t= THIS_MODULE,\n\t.id_table\t\t\t= id_table,\n\t.probe\t\t\t\t= virtblk_probe,\n\t.remove\t\t\t\t= virtblk_remove,\n\t.config_changed\t\t\t= virtblk_config_changed,\n#ifdef CONFIG_PM_SLEEP\n\t.freeze\t\t\t\t= virtblk_freeze,\n\t.restore\t\t\t= virtblk_restore,\n#endif\n};\n\nstatic int __init virtio_blk_init(void)\n{\n\tint error;\n\n\tvirtblk_wq = alloc_workqueue(\"virtio-blk\", 0, 0);\n\tif (!virtblk_wq)\n\t\treturn -ENOMEM;\n\n\tmajor = register_blkdev(0, \"virtblk\");\n\tif (major < 0) {\n\t\terror = major;\n\t\tgoto out_destroy_workqueue;\n\t}\n\n\terror = register_virtio_driver(&virtio_blk);\n\tif (error)\n\t\tgoto out_unregister_blkdev;\n\treturn 0;\n\nout_unregister_blkdev:\n\tunregister_blkdev(major, \"virtblk\");\nout_destroy_workqueue:\n\tdestroy_workqueue(virtblk_wq);\n\treturn error;\n}\n\nstatic void __exit virtio_blk_fini(void)\n{\n\tunregister_virtio_driver(&virtio_blk);\n\tunregister_blkdev(major, \"virtblk\");\n\tdestroy_workqueue(virtblk_wq);\n}\nmodule_init(virtio_blk_init);\nmodule_exit(virtio_blk_fini);\n\nMODULE_DEVICE_TABLE(virtio, id_table);\nMODULE_DESCRIPTION(\"Virtio block driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}