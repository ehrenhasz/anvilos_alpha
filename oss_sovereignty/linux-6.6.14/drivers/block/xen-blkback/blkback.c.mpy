{
  "module_name": "blkback.c",
  "hash_id": "efe4c4e70ba7b27dbe70fb078385289927425835980575cb9f8bd4be6b7dd8b0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/xen-blkback/blkback.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) \"xen-blkback: \" fmt\n\n#include <linux/spinlock.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/delay.h>\n#include <linux/freezer.h>\n#include <linux/bitmap.h>\n\n#include <xen/events.h>\n#include <xen/page.h>\n#include <xen/xen.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/hypercall.h>\n#include <xen/balloon.h>\n#include <xen/grant_table.h>\n#include \"common.h\"\n\n \n\nstatic int max_buffer_pages = 1024;\nmodule_param_named(max_buffer_pages, max_buffer_pages, int, 0644);\nMODULE_PARM_DESC(max_buffer_pages,\n\"Maximum number of free pages to keep in each block backend buffer\");\n\n \n\nstatic int max_pgrants = 1056;\nmodule_param_named(max_persistent_grants, max_pgrants, int, 0644);\nMODULE_PARM_DESC(max_persistent_grants,\n                 \"Maximum number of grants to map persistently\");\n\n \n\nstatic unsigned int pgrant_timeout = 60;\nmodule_param_named(persistent_grant_unused_seconds, pgrant_timeout,\n\t\t   uint, 0644);\nMODULE_PARM_DESC(persistent_grant_unused_seconds,\n\t\t \"Time in seconds an unused persistent grant is allowed to \"\n\t\t \"remain allocated. Default is 60, 0 means unlimited.\");\n\n \nunsigned int xenblk_max_queues;\nmodule_param_named(max_queues, xenblk_max_queues, uint, 0644);\nMODULE_PARM_DESC(max_queues,\n\t\t \"Maximum number of hardware queues per virtual disk.\" \\\n\t\t \"By default it is the number of online CPUs.\");\n\n \nunsigned int xen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;\nmodule_param_named(max_ring_page_order, xen_blkif_max_ring_order, int, 0444);\nMODULE_PARM_DESC(max_ring_page_order, \"Maximum order of pages to be used for the shared ring\");\n \n#define LRU_INTERVAL 100\n\n \n#define LRU_PERCENT_CLEAN 5\n\n \nstatic unsigned int log_stats;\nmodule_param(log_stats, int, 0644);\n\n#define BLKBACK_INVALID_HANDLE (~0)\n\nstatic inline bool persistent_gnt_timeout(struct persistent_gnt *persistent_gnt)\n{\n\treturn pgrant_timeout && (jiffies - persistent_gnt->last_used >=\n\t\t\tHZ * pgrant_timeout);\n}\n\n#define vaddr(page) ((unsigned long)pfn_to_kaddr(page_to_pfn(page)))\n\nstatic int do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags);\nstatic int dispatch_rw_block_io(struct xen_blkif_ring *ring,\n\t\t\t\tstruct blkif_request *req,\n\t\t\t\tstruct pending_req *pending_req);\nstatic void make_response(struct xen_blkif_ring *ring, u64 id,\n\t\t\t  unsigned short op, int st);\n\n#define foreach_grant_safe(pos, n, rbtree, node) \\\n\tfor ((pos) = container_of(rb_first((rbtree)), typeof(*(pos)), node), \\\n\t     (n) = (&(pos)->node != NULL) ? rb_next(&(pos)->node) : NULL; \\\n\t     &(pos)->node != NULL; \\\n\t     (pos) = container_of(n, typeof(*(pos)), node), \\\n\t     (n) = (&(pos)->node != NULL) ? rb_next(&(pos)->node) : NULL)\n\n\n \nstatic int add_persistent_gnt(struct xen_blkif_ring *ring,\n\t\t\t       struct persistent_gnt *persistent_gnt)\n{\n\tstruct rb_node **new = NULL, *parent = NULL;\n\tstruct persistent_gnt *this;\n\tstruct xen_blkif *blkif = ring->blkif;\n\n\tif (ring->persistent_gnt_c >= max_pgrants) {\n\t\tif (!blkif->vbd.overflow_max_grants)\n\t\t\tblkif->vbd.overflow_max_grants = 1;\n\t\treturn -EBUSY;\n\t}\n\t \n\tnew = &ring->persistent_gnts.rb_node;\n\twhile (*new) {\n\t\tthis = container_of(*new, struct persistent_gnt, node);\n\n\t\tparent = *new;\n\t\tif (persistent_gnt->gnt < this->gnt)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse if (persistent_gnt->gnt > this->gnt)\n\t\t\tnew = &((*new)->rb_right);\n\t\telse {\n\t\t\tpr_alert_ratelimited(\"trying to add a gref that's already in the tree\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tpersistent_gnt->active = true;\n\t \n\trb_link_node(&(persistent_gnt->node), parent, new);\n\trb_insert_color(&(persistent_gnt->node), &ring->persistent_gnts);\n\tring->persistent_gnt_c++;\n\tatomic_inc(&ring->persistent_gnt_in_use);\n\treturn 0;\n}\n\nstatic struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,\n\t\t\t\t\t\t grant_ref_t gref)\n{\n\tstruct persistent_gnt *data;\n\tstruct rb_node *node = NULL;\n\n\tnode = ring->persistent_gnts.rb_node;\n\twhile (node) {\n\t\tdata = container_of(node, struct persistent_gnt, node);\n\n\t\tif (gref < data->gnt)\n\t\t\tnode = node->rb_left;\n\t\telse if (gref > data->gnt)\n\t\t\tnode = node->rb_right;\n\t\telse {\n\t\t\tif (data->active) {\n\t\t\t\tpr_alert_ratelimited(\"requesting a grant already in use\\n\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tdata->active = true;\n\t\t\tatomic_inc(&ring->persistent_gnt_in_use);\n\t\t\treturn data;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void put_persistent_gnt(struct xen_blkif_ring *ring,\n                               struct persistent_gnt *persistent_gnt)\n{\n\tif (!persistent_gnt->active)\n\t\tpr_alert_ratelimited(\"freeing a grant already unused\\n\");\n\tpersistent_gnt->last_used = jiffies;\n\tpersistent_gnt->active = false;\n\tatomic_dec(&ring->persistent_gnt_in_use);\n}\n\nstatic void free_persistent_gnts(struct xen_blkif_ring *ring)\n{\n\tstruct rb_root *root = &ring->persistent_gnts;\n\tstruct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct page *pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct persistent_gnt *persistent_gnt;\n\tstruct rb_node *n;\n\tint segs_to_unmap = 0;\n\tstruct gntab_unmap_queue_data unmap_data;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn;\n\n\tunmap_data.pages = pages;\n\tunmap_data.unmap_ops = unmap;\n\tunmap_data.kunmap_ops = NULL;\n\n\tforeach_grant_safe(persistent_gnt, n, root, node) {\n\t\tBUG_ON(persistent_gnt->handle ==\n\t\t\tBLKBACK_INVALID_HANDLE);\n\t\tgnttab_set_unmap_op(&unmap[segs_to_unmap],\n\t\t\t(unsigned long) pfn_to_kaddr(page_to_pfn(\n\t\t\t\tpersistent_gnt->page)),\n\t\t\tGNTMAP_host_map,\n\t\t\tpersistent_gnt->handle);\n\n\t\tpages[segs_to_unmap] = persistent_gnt->page;\n\n\t\tif (++segs_to_unmap == BLKIF_MAX_SEGMENTS_PER_REQUEST ||\n\t\t\t!rb_next(&persistent_gnt->node)) {\n\n\t\t\tunmap_data.count = segs_to_unmap;\n\t\t\tBUG_ON(gnttab_unmap_refs_sync(&unmap_data));\n\n\t\t\tgnttab_page_cache_put(&ring->free_pages, pages,\n\t\t\t\t\t      segs_to_unmap);\n\t\t\tsegs_to_unmap = 0;\n\t\t}\n\n\t\trb_erase(&persistent_gnt->node, root);\n\t\tkfree(persistent_gnt);\n\t\tring->persistent_gnt_c--;\n\t}\n\n\tBUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));\n\tBUG_ON(ring->persistent_gnt_c != 0);\n}\n\nvoid xen_blkbk_unmap_purged_grants(struct work_struct *work)\n{\n\tstruct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct page *pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct persistent_gnt *persistent_gnt;\n\tint segs_to_unmap = 0;\n\tstruct xen_blkif_ring *ring = container_of(work, typeof(*ring), persistent_purge_work);\n\tstruct gntab_unmap_queue_data unmap_data;\n\n\tunmap_data.pages = pages;\n\tunmap_data.unmap_ops = unmap;\n\tunmap_data.kunmap_ops = NULL;\n\n\twhile(!list_empty(&ring->persistent_purge_list)) {\n\t\tpersistent_gnt = list_first_entry(&ring->persistent_purge_list,\n\t\t                                  struct persistent_gnt,\n\t\t                                  remove_node);\n\t\tlist_del(&persistent_gnt->remove_node);\n\n\t\tgnttab_set_unmap_op(&unmap[segs_to_unmap],\n\t\t\tvaddr(persistent_gnt->page),\n\t\t\tGNTMAP_host_map,\n\t\t\tpersistent_gnt->handle);\n\n\t\tpages[segs_to_unmap] = persistent_gnt->page;\n\n\t\tif (++segs_to_unmap == BLKIF_MAX_SEGMENTS_PER_REQUEST) {\n\t\t\tunmap_data.count = segs_to_unmap;\n\t\t\tBUG_ON(gnttab_unmap_refs_sync(&unmap_data));\n\t\t\tgnttab_page_cache_put(&ring->free_pages, pages,\n\t\t\t\t\t      segs_to_unmap);\n\t\t\tsegs_to_unmap = 0;\n\t\t}\n\t\tkfree(persistent_gnt);\n\t}\n\tif (segs_to_unmap > 0) {\n\t\tunmap_data.count = segs_to_unmap;\n\t\tBUG_ON(gnttab_unmap_refs_sync(&unmap_data));\n\t\tgnttab_page_cache_put(&ring->free_pages, pages, segs_to_unmap);\n\t}\n}\n\nstatic void purge_persistent_gnt(struct xen_blkif_ring *ring)\n{\n\tstruct persistent_gnt *persistent_gnt;\n\tstruct rb_node *n;\n\tunsigned int num_clean, total;\n\tbool scan_used = false;\n\tstruct rb_root *root;\n\n\tif (work_busy(&ring->persistent_purge_work)) {\n\t\tpr_alert_ratelimited(\"Scheduled work from previous purge is still busy, cannot purge list\\n\");\n\t\tgoto out;\n\t}\n\n\tif (ring->persistent_gnt_c < max_pgrants ||\n\t    (ring->persistent_gnt_c == max_pgrants &&\n\t    !ring->blkif->vbd.overflow_max_grants)) {\n\t\tnum_clean = 0;\n\t} else {\n\t\tnum_clean = (max_pgrants / 100) * LRU_PERCENT_CLEAN;\n\t\tnum_clean = ring->persistent_gnt_c - max_pgrants + num_clean;\n\t\tnum_clean = min(ring->persistent_gnt_c, num_clean);\n\t\tpr_debug(\"Going to purge at least %u persistent grants\\n\",\n\t\t\t num_clean);\n\t}\n\n\t \n\n\ttotal = 0;\n\n\tBUG_ON(!list_empty(&ring->persistent_purge_list));\n\troot = &ring->persistent_gnts;\npurge_list:\n\tforeach_grant_safe(persistent_gnt, n, root, node) {\n\t\tBUG_ON(persistent_gnt->handle ==\n\t\t\tBLKBACK_INVALID_HANDLE);\n\n\t\tif (persistent_gnt->active)\n\t\t\tcontinue;\n\t\tif (!scan_used && !persistent_gnt_timeout(persistent_gnt))\n\t\t\tcontinue;\n\t\tif (scan_used && total >= num_clean)\n\t\t\tcontinue;\n\n\t\trb_erase(&persistent_gnt->node, root);\n\t\tlist_add(&persistent_gnt->remove_node,\n\t\t\t &ring->persistent_purge_list);\n\t\ttotal++;\n\t}\n\t \n\tif (!scan_used && total < num_clean) {\n\t\tpr_debug(\"Still missing %u purged frames\\n\", num_clean - total);\n\t\tscan_used = true;\n\t\tgoto purge_list;\n\t}\n\n\tif (total) {\n\t\tring->persistent_gnt_c -= total;\n\t\tring->blkif->vbd.overflow_max_grants = 0;\n\n\t\t \n\t\tschedule_work(&ring->persistent_purge_work);\n\t\tpr_debug(\"Purged %u/%u\\n\", num_clean, total);\n\t}\n\nout:\n\treturn;\n}\n\n \nstatic struct pending_req *alloc_req(struct xen_blkif_ring *ring)\n{\n\tstruct pending_req *req = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ring->pending_free_lock, flags);\n\tif (!list_empty(&ring->pending_free)) {\n\t\treq = list_entry(ring->pending_free.next, struct pending_req,\n\t\t\t\t free_list);\n\t\tlist_del(&req->free_list);\n\t}\n\tspin_unlock_irqrestore(&ring->pending_free_lock, flags);\n\treturn req;\n}\n\n \nstatic void free_req(struct xen_blkif_ring *ring, struct pending_req *req)\n{\n\tunsigned long flags;\n\tint was_empty;\n\n\tspin_lock_irqsave(&ring->pending_free_lock, flags);\n\twas_empty = list_empty(&ring->pending_free);\n\tlist_add(&req->free_list, &ring->pending_free);\n\tspin_unlock_irqrestore(&ring->pending_free_lock, flags);\n\tif (was_empty)\n\t\twake_up(&ring->pending_free_wq);\n}\n\n \nstatic int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,\n\t\t\t     enum req_op operation)\n{\n\tstruct xen_vbd *vbd = &blkif->vbd;\n\tint rc = -EACCES;\n\n\tif ((operation != REQ_OP_READ) && vbd->readonly)\n\t\tgoto out;\n\n\tif (likely(req->nr_sects)) {\n\t\tblkif_sector_t end = req->sector_number + req->nr_sects;\n\n\t\tif (unlikely(end < req->sector_number))\n\t\t\tgoto out;\n\t\tif (unlikely(end > vbd_sz(vbd)))\n\t\t\tgoto out;\n\t}\n\n\treq->dev  = vbd->pdevice;\n\treq->bdev = vbd->bdev;\n\trc = 0;\n\n out:\n\treturn rc;\n}\n\nstatic void xen_vbd_resize(struct xen_blkif *blkif)\n{\n\tstruct xen_vbd *vbd = &blkif->vbd;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tstruct xenbus_device *dev = xen_blkbk_xenbus(blkif->be);\n\tunsigned long long new_size = vbd_sz(vbd);\n\n\tpr_info(\"VBD Resize: Domid: %d, Device: (%d, %d)\\n\",\n\t\tblkif->domid, MAJOR(vbd->pdevice), MINOR(vbd->pdevice));\n\tpr_info(\"VBD Resize: new size %llu\\n\", new_size);\n\tvbd->size = new_size;\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\tpr_warn(\"Error starting transaction\\n\");\n\t\treturn;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"sectors\", \"%llu\",\n\t\t\t    (unsigned long long)vbd_sz(vbd));\n\tif (err) {\n\t\tpr_warn(\"Error writing new size\\n\");\n\t\tgoto abort;\n\t}\n\t \n\terr = xenbus_printf(xbt, dev->nodename, \"state\", \"%d\", dev->state);\n\tif (err) {\n\t\tpr_warn(\"Error writing the state\\n\");\n\t\tgoto abort;\n\t}\n\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\tif (err)\n\t\tpr_warn(\"Error ending transaction\\n\");\n\treturn;\nabort:\n\txenbus_transaction_end(xbt, 1);\n}\n\n \nstatic void blkif_notify_work(struct xen_blkif_ring *ring)\n{\n\tring->waiting_reqs = 1;\n\twake_up(&ring->wq);\n}\n\nirqreturn_t xen_blkif_be_int(int irq, void *dev_id)\n{\n\tblkif_notify_work(dev_id);\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic void print_stats(struct xen_blkif_ring *ring)\n{\n\tpr_info(\"(%s): oo %3llu  |  rd %4llu  |  wr %4llu  |  f %4llu\"\n\t\t \"  |  ds %4llu | pg: %4u/%4d\\n\",\n\t\t current->comm, ring->st_oo_req,\n\t\t ring->st_rd_req, ring->st_wr_req,\n\t\t ring->st_f_req, ring->st_ds_req,\n\t\t ring->persistent_gnt_c, max_pgrants);\n\tring->st_print = jiffies + msecs_to_jiffies(10 * 1000);\n\tring->st_rd_req = 0;\n\tring->st_wr_req = 0;\n\tring->st_oo_req = 0;\n\tring->st_ds_req = 0;\n}\n\nint xen_blkif_schedule(void *arg)\n{\n\tstruct xen_blkif_ring *ring = arg;\n\tstruct xen_blkif *blkif = ring->blkif;\n\tstruct xen_vbd *vbd = &blkif->vbd;\n\tunsigned long timeout;\n\tint ret;\n\tbool do_eoi;\n\tunsigned int eoi_flags = XEN_EOI_FLAG_SPURIOUS;\n\n\tset_freezable();\n\twhile (!kthread_should_stop()) {\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (unlikely(vbd->size != vbd_sz(vbd)))\n\t\t\txen_vbd_resize(blkif);\n\n\t\ttimeout = msecs_to_jiffies(LRU_INTERVAL);\n\n\t\ttimeout = wait_event_interruptible_timeout(\n\t\t\tring->wq,\n\t\t\tring->waiting_reqs || kthread_should_stop(),\n\t\t\ttimeout);\n\t\tif (timeout == 0)\n\t\t\tgoto purge_gnt_list;\n\t\ttimeout = wait_event_interruptible_timeout(\n\t\t\tring->pending_free_wq,\n\t\t\t!list_empty(&ring->pending_free) ||\n\t\t\tkthread_should_stop(),\n\t\t\ttimeout);\n\t\tif (timeout == 0)\n\t\t\tgoto purge_gnt_list;\n\n\t\tdo_eoi = ring->waiting_reqs;\n\n\t\tring->waiting_reqs = 0;\n\t\tsmp_mb();  \n\n\t\tret = do_block_io_op(ring, &eoi_flags);\n\t\tif (ret > 0)\n\t\t\tring->waiting_reqs = 1;\n\t\tif (ret == -EACCES)\n\t\t\twait_event_interruptible(ring->shutdown_wq,\n\t\t\t\t\t\t kthread_should_stop());\n\n\t\tif (do_eoi && !ring->waiting_reqs) {\n\t\t\txen_irq_lateeoi(ring->irq, eoi_flags);\n\t\t\teoi_flags |= XEN_EOI_FLAG_SPURIOUS;\n\t\t}\n\npurge_gnt_list:\n\t\tif (blkif->vbd.feature_gnt_persistent &&\n\t\t    time_after(jiffies, ring->next_lru)) {\n\t\t\tpurge_persistent_gnt(ring);\n\t\t\tring->next_lru = jiffies + msecs_to_jiffies(LRU_INTERVAL);\n\t\t}\n\n\t\t \n\t\tif (time_before(jiffies, blkif->buffer_squeeze_end))\n\t\t\tgnttab_page_cache_shrink(&ring->free_pages, 0);\n\t\telse\n\t\t\tgnttab_page_cache_shrink(&ring->free_pages,\n\t\t\t\t\t\t max_buffer_pages);\n\n\t\tif (log_stats && time_after(jiffies, ring->st_print))\n\t\t\tprint_stats(ring);\n\t}\n\n\t \n\tflush_work(&ring->persistent_purge_work);\n\n\tif (log_stats)\n\t\tprint_stats(ring);\n\n\tring->xenblkd = NULL;\n\n\treturn 0;\n}\n\n \nvoid xen_blkbk_free_caches(struct xen_blkif_ring *ring)\n{\n\t \n\tfree_persistent_gnts(ring);\n\n\t \n\tgnttab_page_cache_shrink(&ring->free_pages, 0  );\n}\n\nstatic unsigned int xen_blkbk_unmap_prepare(\n\tstruct xen_blkif_ring *ring,\n\tstruct grant_page **pages,\n\tunsigned int num,\n\tstruct gnttab_unmap_grant_ref *unmap_ops,\n\tstruct page **unmap_pages)\n{\n\tunsigned int i, invcount = 0;\n\n\tfor (i = 0; i < num; i++) {\n\t\tif (pages[i]->persistent_gnt != NULL) {\n\t\t\tput_persistent_gnt(ring, pages[i]->persistent_gnt);\n\t\t\tcontinue;\n\t\t}\n\t\tif (pages[i]->handle == BLKBACK_INVALID_HANDLE)\n\t\t\tcontinue;\n\t\tunmap_pages[invcount] = pages[i]->page;\n\t\tgnttab_set_unmap_op(&unmap_ops[invcount], vaddr(pages[i]->page),\n\t\t\t\t    GNTMAP_host_map, pages[i]->handle);\n\t\tpages[i]->handle = BLKBACK_INVALID_HANDLE;\n\t\tinvcount++;\n\t}\n\n\treturn invcount;\n}\n\nstatic void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_queue_data *data)\n{\n\tstruct pending_req *pending_req = (struct pending_req *)(data->data);\n\tstruct xen_blkif_ring *ring = pending_req->ring;\n\tstruct xen_blkif *blkif = ring->blkif;\n\n\t \n\tBUG_ON(result);\n\n\tgnttab_page_cache_put(&ring->free_pages, data->pages, data->count);\n\tmake_response(ring, pending_req->id,\n\t\t      pending_req->operation, pending_req->status);\n\tfree_req(ring, pending_req);\n\t \n\tif (atomic_dec_and_test(&ring->inflight) && atomic_read(&blkif->drain)) {\n\t\tcomplete(&blkif->drain_complete);\n\t}\n\txen_blkif_put(blkif);\n}\n\nstatic void xen_blkbk_unmap_and_respond(struct pending_req *req)\n{\n\tstruct gntab_unmap_queue_data* work = &req->gnttab_unmap_data;\n\tstruct xen_blkif_ring *ring = req->ring;\n\tstruct grant_page **pages = req->segments;\n\tunsigned int invcount;\n\n\tinvcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,\n\t\t\t\t\t   req->unmap, req->unmap_pages);\n\n\twork->data = req;\n\twork->done = xen_blkbk_unmap_and_respond_callback;\n\twork->unmap_ops = req->unmap;\n\twork->kunmap_ops = NULL;\n\twork->pages = req->unmap_pages;\n\twork->count = invcount;\n\n\tgnttab_unmap_refs_async(&req->gnttab_unmap_data);\n}\n\n\n \nstatic void xen_blkbk_unmap(struct xen_blkif_ring *ring,\n                            struct grant_page *pages[],\n                            int num)\n{\n\tstruct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct page *unmap_pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tunsigned int invcount = 0;\n\tint ret;\n\n\twhile (num) {\n\t\tunsigned int batch = min(num, BLKIF_MAX_SEGMENTS_PER_REQUEST);\n\n\t\tinvcount = xen_blkbk_unmap_prepare(ring, pages, batch,\n\t\t\t\t\t\t   unmap, unmap_pages);\n\t\tif (invcount) {\n\t\t\tret = gnttab_unmap_refs(unmap, NULL, unmap_pages, invcount);\n\t\t\tBUG_ON(ret);\n\t\t\tgnttab_page_cache_put(&ring->free_pages, unmap_pages,\n\t\t\t\t\t      invcount);\n\t\t}\n\t\tpages += batch;\n\t\tnum -= batch;\n\t}\n}\n\nstatic int xen_blkbk_map(struct xen_blkif_ring *ring,\n\t\t\t struct grant_page *pages[],\n\t\t\t int num, bool ro)\n{\n\tstruct gnttab_map_grant_ref map[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct page *pages_to_gnt[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n\tstruct persistent_gnt *persistent_gnt = NULL;\n\tphys_addr_t addr = 0;\n\tint i, seg_idx, new_map_idx;\n\tint segs_to_map = 0;\n\tint ret = 0;\n\tint last_map = 0, map_until = 0;\n\tint use_persistent_gnts;\n\tstruct xen_blkif *blkif = ring->blkif;\n\n\tuse_persistent_gnts = (blkif->vbd.feature_gnt_persistent);\n\n\t \nagain:\n\tfor (i = map_until; i < num; i++) {\n\t\tuint32_t flags;\n\n\t\tif (use_persistent_gnts) {\n\t\t\tpersistent_gnt = get_persistent_gnt(\n\t\t\t\tring,\n\t\t\t\tpages[i]->gref);\n\t\t}\n\n\t\tif (persistent_gnt) {\n\t\t\t \n\t\t\tpages[i]->page = persistent_gnt->page;\n\t\t\tpages[i]->persistent_gnt = persistent_gnt;\n\t\t} else {\n\t\t\tif (gnttab_page_cache_get(&ring->free_pages,\n\t\t\t\t\t\t  &pages[i]->page)) {\n\t\t\t\tgnttab_page_cache_put(&ring->free_pages,\n\t\t\t\t\t\t      pages_to_gnt,\n\t\t\t\t\t\t      segs_to_map);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taddr = vaddr(pages[i]->page);\n\t\t\tpages_to_gnt[segs_to_map] = pages[i]->page;\n\t\t\tpages[i]->persistent_gnt = NULL;\n\t\t\tflags = GNTMAP_host_map;\n\t\t\tif (!use_persistent_gnts && ro)\n\t\t\t\tflags |= GNTMAP_readonly;\n\t\t\tgnttab_set_map_op(&map[segs_to_map++], addr,\n\t\t\t\t\t  flags, pages[i]->gref,\n\t\t\t\t\t  blkif->domid);\n\t\t}\n\t\tmap_until = i + 1;\n\t\tif (segs_to_map == BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t\tbreak;\n\t}\n\n\tif (segs_to_map)\n\t\tret = gnttab_map_refs(map, NULL, pages_to_gnt, segs_to_map);\n\n\t \n\tfor (seg_idx = last_map, new_map_idx = 0; seg_idx < map_until; seg_idx++) {\n\t\tif (!pages[seg_idx]->persistent_gnt) {\n\t\t\t \n\t\t\tBUG_ON(new_map_idx >= segs_to_map);\n\t\t\tif (unlikely(map[new_map_idx].status != 0)) {\n\t\t\t\tpr_debug(\"invalid buffer -- could not remap it\\n\");\n\t\t\t\tgnttab_page_cache_put(&ring->free_pages,\n\t\t\t\t\t\t      &pages[seg_idx]->page, 1);\n\t\t\t\tpages[seg_idx]->handle = BLKBACK_INVALID_HANDLE;\n\t\t\t\tret |= !ret;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tpages[seg_idx]->handle = map[new_map_idx].handle;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\t\tif (use_persistent_gnts &&\n\t\t    ring->persistent_gnt_c < max_pgrants) {\n\t\t\t \n\t\t\tpersistent_gnt = kmalloc(sizeof(struct persistent_gnt),\n\t\t\t\t                 GFP_KERNEL);\n\t\t\tif (!persistent_gnt) {\n\t\t\t\t \n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tpersistent_gnt->gnt = map[new_map_idx].ref;\n\t\t\tpersistent_gnt->handle = map[new_map_idx].handle;\n\t\t\tpersistent_gnt->page = pages[seg_idx]->page;\n\t\t\tif (add_persistent_gnt(ring,\n\t\t\t                       persistent_gnt)) {\n\t\t\t\tkfree(persistent_gnt);\n\t\t\t\tpersistent_gnt = NULL;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tpages[seg_idx]->persistent_gnt = persistent_gnt;\n\t\t\tpr_debug(\"grant %u added to the tree of persistent grants, using %u/%u\\n\",\n\t\t\t\t persistent_gnt->gnt, ring->persistent_gnt_c,\n\t\t\t\t max_pgrants);\n\t\t\tgoto next;\n\t\t}\n\t\tif (use_persistent_gnts && !blkif->vbd.overflow_max_grants) {\n\t\t\tblkif->vbd.overflow_max_grants = 1;\n\t\t\tpr_debug(\"domain %u, device %#x is using maximum number of persistent grants\\n\",\n\t\t\t         blkif->domid, blkif->vbd.handle);\n\t\t}\n\t\t \nnext:\n\t\tnew_map_idx++;\n\t}\n\tsegs_to_map = 0;\n\tlast_map = map_until;\n\tif (!ret && map_until != num)\n\t\tgoto again;\n\nout:\n\tfor (i = last_map; i < num; i++) {\n\t\t \n\t\tif (i >= map_until)\n\t\t\tpages[i]->persistent_gnt = NULL;\n\t\tpages[i]->handle = BLKBACK_INVALID_HANDLE;\n\t}\n\n\treturn ret;\n}\n\nstatic int xen_blkbk_map_seg(struct pending_req *pending_req)\n{\n\tint rc;\n\n\trc = xen_blkbk_map(pending_req->ring, pending_req->segments,\n\t\t\t   pending_req->nr_segs,\n\t                   (pending_req->operation != BLKIF_OP_READ));\n\n\treturn rc;\n}\n\nstatic int xen_blkbk_parse_indirect(struct blkif_request *req,\n\t\t\t\t    struct pending_req *pending_req,\n\t\t\t\t    struct seg_buf seg[],\n\t\t\t\t    struct phys_req *preq)\n{\n\tstruct grant_page **pages = pending_req->indirect_pages;\n\tstruct xen_blkif_ring *ring = pending_req->ring;\n\tint indirect_grefs, rc, n, nseg, i;\n\tstruct blkif_request_segment *segments = NULL;\n\n\tnseg = pending_req->nr_segs;\n\tindirect_grefs = INDIRECT_PAGES(nseg);\n\tBUG_ON(indirect_grefs > BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST);\n\n\tfor (i = 0; i < indirect_grefs; i++)\n\t\tpages[i]->gref = req->u.indirect.indirect_grefs[i];\n\n\trc = xen_blkbk_map(ring, pages, indirect_grefs, true);\n\tif (rc)\n\t\tgoto unmap;\n\n\tfor (n = 0; n < nseg; n++) {\n\t\tuint8_t first_sect, last_sect;\n\n\t\tif ((n % SEGS_PER_INDIRECT_FRAME) == 0) {\n\t\t\t \n\t\t\tif (segments)\n\t\t\t\tkunmap_atomic(segments);\n\t\t\tsegments = kmap_atomic(pages[n/SEGS_PER_INDIRECT_FRAME]->page);\n\t\t}\n\t\ti = n % SEGS_PER_INDIRECT_FRAME;\n\n\t\tpending_req->segments[n]->gref = segments[i].gref;\n\n\t\tfirst_sect = READ_ONCE(segments[i].first_sect);\n\t\tlast_sect = READ_ONCE(segments[i].last_sect);\n\t\tif (last_sect >= (XEN_PAGE_SIZE >> 9) || last_sect < first_sect) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto unmap;\n\t\t}\n\n\t\tseg[n].nsec = last_sect - first_sect + 1;\n\t\tseg[n].offset = first_sect << 9;\n\t\tpreq->nr_sects += seg[n].nsec;\n\t}\n\nunmap:\n\tif (segments)\n\t\tkunmap_atomic(segments);\n\txen_blkbk_unmap(ring, pages, indirect_grefs);\n\treturn rc;\n}\n\nstatic int dispatch_discard_io(struct xen_blkif_ring *ring,\n\t\t\t\tstruct blkif_request *req)\n{\n\tint err = 0;\n\tint status = BLKIF_RSP_OKAY;\n\tstruct xen_blkif *blkif = ring->blkif;\n\tstruct block_device *bdev = blkif->vbd.bdev;\n\tstruct phys_req preq;\n\n\txen_blkif_get(blkif);\n\n\tpreq.sector_number = req->u.discard.sector_number;\n\tpreq.nr_sects      = req->u.discard.nr_sectors;\n\n\terr = xen_vbd_translate(&preq, blkif, REQ_OP_WRITE);\n\tif (err) {\n\t\tpr_warn(\"access denied: DISCARD [%llu->%llu] on dev=%04x\\n\",\n\t\t\tpreq.sector_number,\n\t\t\tpreq.sector_number + preq.nr_sects, blkif->vbd.pdevice);\n\t\tgoto fail_response;\n\t}\n\tring->st_ds_req++;\n\n\tif (blkif->vbd.discard_secure &&\n\t    (req->u.discard.flag & BLKIF_DISCARD_SECURE))\n\t\terr = blkdev_issue_secure_erase(bdev,\n\t\t\t\treq->u.discard.sector_number,\n\t\t\t\treq->u.discard.nr_sectors, GFP_KERNEL);\n\telse\n\t\terr = blkdev_issue_discard(bdev, req->u.discard.sector_number,\n\t\t\t\treq->u.discard.nr_sectors, GFP_KERNEL);\n\nfail_response:\n\tif (err == -EOPNOTSUPP) {\n\t\tpr_debug(\"discard op failed, not supported\\n\");\n\t\tstatus = BLKIF_RSP_EOPNOTSUPP;\n\t} else if (err)\n\t\tstatus = BLKIF_RSP_ERROR;\n\n\tmake_response(ring, req->u.discard.id, req->operation, status);\n\txen_blkif_put(blkif);\n\treturn err;\n}\n\nstatic int dispatch_other_io(struct xen_blkif_ring *ring,\n\t\t\t     struct blkif_request *req,\n\t\t\t     struct pending_req *pending_req)\n{\n\tfree_req(ring, pending_req);\n\tmake_response(ring, req->u.other.id, req->operation,\n\t\t      BLKIF_RSP_EOPNOTSUPP);\n\treturn -EIO;\n}\n\nstatic void xen_blk_drain_io(struct xen_blkif_ring *ring)\n{\n\tstruct xen_blkif *blkif = ring->blkif;\n\n\tatomic_set(&blkif->drain, 1);\n\tdo {\n\t\tif (atomic_read(&ring->inflight) == 0)\n\t\t\tbreak;\n\t\twait_for_completion_interruptible_timeout(\n\t\t\t\t&blkif->drain_complete, HZ);\n\n\t\tif (!atomic_read(&blkif->drain))\n\t\t\tbreak;\n\t} while (!kthread_should_stop());\n\tatomic_set(&blkif->drain, 0);\n}\n\nstatic void __end_block_io_op(struct pending_req *pending_req,\n\t\tblk_status_t error)\n{\n\t \n\tif (pending_req->operation == BLKIF_OP_FLUSH_DISKCACHE &&\n\t    error == BLK_STS_NOTSUPP) {\n\t\tpr_debug(\"flush diskcache op failed, not supported\\n\");\n\t\txen_blkbk_flush_diskcache(XBT_NIL, pending_req->ring->blkif->be, 0);\n\t\tpending_req->status = BLKIF_RSP_EOPNOTSUPP;\n\t} else if (pending_req->operation == BLKIF_OP_WRITE_BARRIER &&\n\t\t   error == BLK_STS_NOTSUPP) {\n\t\tpr_debug(\"write barrier op failed, not supported\\n\");\n\t\txen_blkbk_barrier(XBT_NIL, pending_req->ring->blkif->be, 0);\n\t\tpending_req->status = BLKIF_RSP_EOPNOTSUPP;\n\t} else if (error) {\n\t\tpr_debug(\"Buffer not up-to-date at end of operation,\"\n\t\t\t \" error=%d\\n\", error);\n\t\tpending_req->status = BLKIF_RSP_ERROR;\n\t}\n\n\t \n\tif (atomic_dec_and_test(&pending_req->pendcnt))\n\t\txen_blkbk_unmap_and_respond(pending_req);\n}\n\n \nstatic void end_block_io_op(struct bio *bio)\n{\n\t__end_block_io_op(bio->bi_private, bio->bi_status);\n\tbio_put(bio);\n}\n\nstatic void blkif_get_x86_32_req(struct blkif_request *dst,\n\t\t\t\t const struct blkif_x86_32_request *src)\n{\n\tunsigned int i, n;\n\n\tdst->operation = READ_ONCE(src->operation);\n\n\tswitch (dst->operation) {\n\tcase BLKIF_OP_READ:\n\tcase BLKIF_OP_WRITE:\n\tcase BLKIF_OP_WRITE_BARRIER:\n\tcase BLKIF_OP_FLUSH_DISKCACHE:\n\t\tdst->u.rw.nr_segments = READ_ONCE(src->u.rw.nr_segments);\n\t\tdst->u.rw.handle = src->u.rw.handle;\n\t\tdst->u.rw.id = src->u.rw.id;\n\t\tdst->u.rw.sector_number = src->u.rw.sector_number;\n\t\tn = min_t(unsigned int, BLKIF_MAX_SEGMENTS_PER_REQUEST,\n\t\t\t  dst->u.rw.nr_segments);\n\t\tfor (i = 0; i < n; i++)\n\t\t\tdst->u.rw.seg[i] = src->u.rw.seg[i];\n\t\tbreak;\n\n\tcase BLKIF_OP_DISCARD:\n\t\tdst->u.discard.flag = src->u.discard.flag;\n\t\tdst->u.discard.id = src->u.discard.id;\n\t\tdst->u.discard.sector_number = src->u.discard.sector_number;\n\t\tdst->u.discard.nr_sectors = src->u.discard.nr_sectors;\n\t\tbreak;\n\n\tcase BLKIF_OP_INDIRECT:\n\t\tdst->u.indirect.indirect_op = src->u.indirect.indirect_op;\n\t\tdst->u.indirect.nr_segments =\n\t\t\tREAD_ONCE(src->u.indirect.nr_segments);\n\t\tdst->u.indirect.handle = src->u.indirect.handle;\n\t\tdst->u.indirect.id = src->u.indirect.id;\n\t\tdst->u.indirect.sector_number = src->u.indirect.sector_number;\n\t\tn = min(MAX_INDIRECT_PAGES,\n\t\t\tINDIRECT_PAGES(dst->u.indirect.nr_segments));\n\t\tfor (i = 0; i < n; i++)\n\t\t\tdst->u.indirect.indirect_grefs[i] =\n\t\t\t\tsrc->u.indirect.indirect_grefs[i];\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tdst->u.other.id = src->u.other.id;\n\t\tbreak;\n\t}\n}\n\nstatic void blkif_get_x86_64_req(struct blkif_request *dst,\n\t\t\t\t const struct blkif_x86_64_request *src)\n{\n\tunsigned int i, n;\n\n\tdst->operation = READ_ONCE(src->operation);\n\n\tswitch (dst->operation) {\n\tcase BLKIF_OP_READ:\n\tcase BLKIF_OP_WRITE:\n\tcase BLKIF_OP_WRITE_BARRIER:\n\tcase BLKIF_OP_FLUSH_DISKCACHE:\n\t\tdst->u.rw.nr_segments = READ_ONCE(src->u.rw.nr_segments);\n\t\tdst->u.rw.handle = src->u.rw.handle;\n\t\tdst->u.rw.id = src->u.rw.id;\n\t\tdst->u.rw.sector_number = src->u.rw.sector_number;\n\t\tn = min_t(unsigned int, BLKIF_MAX_SEGMENTS_PER_REQUEST,\n\t\t\t  dst->u.rw.nr_segments);\n\t\tfor (i = 0; i < n; i++)\n\t\t\tdst->u.rw.seg[i] = src->u.rw.seg[i];\n\t\tbreak;\n\n\tcase BLKIF_OP_DISCARD:\n\t\tdst->u.discard.flag = src->u.discard.flag;\n\t\tdst->u.discard.id = src->u.discard.id;\n\t\tdst->u.discard.sector_number = src->u.discard.sector_number;\n\t\tdst->u.discard.nr_sectors = src->u.discard.nr_sectors;\n\t\tbreak;\n\n\tcase BLKIF_OP_INDIRECT:\n\t\tdst->u.indirect.indirect_op = src->u.indirect.indirect_op;\n\t\tdst->u.indirect.nr_segments =\n\t\t\tREAD_ONCE(src->u.indirect.nr_segments);\n\t\tdst->u.indirect.handle = src->u.indirect.handle;\n\t\tdst->u.indirect.id = src->u.indirect.id;\n\t\tdst->u.indirect.sector_number = src->u.indirect.sector_number;\n\t\tn = min(MAX_INDIRECT_PAGES,\n\t\t\tINDIRECT_PAGES(dst->u.indirect.nr_segments));\n\t\tfor (i = 0; i < n; i++)\n\t\t\tdst->u.indirect.indirect_grefs[i] =\n\t\t\t\tsrc->u.indirect.indirect_grefs[i];\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tdst->u.other.id = src->u.other.id;\n\t\tbreak;\n\t}\n}\n\n \nstatic int\n__do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)\n{\n\tunion blkif_back_rings *blk_rings = &ring->blk_rings;\n\tstruct blkif_request req;\n\tstruct pending_req *pending_req;\n\tRING_IDX rc, rp;\n\tint more_to_do = 0;\n\n\trc = blk_rings->common.req_cons;\n\trp = blk_rings->common.sring->req_prod;\n\trmb();  \n\n\tif (RING_REQUEST_PROD_OVERFLOW(&blk_rings->common, rp)) {\n\t\trc = blk_rings->common.rsp_prod_pvt;\n\t\tpr_warn(\"Frontend provided bogus ring requests (%d - %d = %d). Halting ring processing on dev=%04x\\n\",\n\t\t\trp, rc, rp - rc, ring->blkif->vbd.pdevice);\n\t\treturn -EACCES;\n\t}\n\twhile (rc != rp) {\n\n\t\tif (RING_REQUEST_CONS_OVERFLOW(&blk_rings->common, rc))\n\t\t\tbreak;\n\n\t\t \n\t\t*eoi_flags &= ~XEN_EOI_FLAG_SPURIOUS;\n\n\t\tif (kthread_should_stop()) {\n\t\t\tmore_to_do = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tpending_req = alloc_req(ring);\n\t\tif (NULL == pending_req) {\n\t\t\tring->st_oo_req++;\n\t\t\tmore_to_do = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (ring->blkif->blk_protocol) {\n\t\tcase BLKIF_PROTOCOL_NATIVE:\n\t\t\tmemcpy(&req, RING_GET_REQUEST(&blk_rings->native, rc), sizeof(req));\n\t\t\tbreak;\n\t\tcase BLKIF_PROTOCOL_X86_32:\n\t\t\tblkif_get_x86_32_req(&req, RING_GET_REQUEST(&blk_rings->x86_32, rc));\n\t\t\tbreak;\n\t\tcase BLKIF_PROTOCOL_X86_64:\n\t\t\tblkif_get_x86_64_req(&req, RING_GET_REQUEST(&blk_rings->x86_64, rc));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tblk_rings->common.req_cons = ++rc;  \n\n\t\t \n\t\tbarrier();\n\n\t\tswitch (req.operation) {\n\t\tcase BLKIF_OP_READ:\n\t\tcase BLKIF_OP_WRITE:\n\t\tcase BLKIF_OP_WRITE_BARRIER:\n\t\tcase BLKIF_OP_FLUSH_DISKCACHE:\n\t\tcase BLKIF_OP_INDIRECT:\n\t\t\tif (dispatch_rw_block_io(ring, &req, pending_req))\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\tcase BLKIF_OP_DISCARD:\n\t\t\tfree_req(ring, pending_req);\n\t\t\tif (dispatch_discard_io(ring, &req))\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (dispatch_other_io(ring, &req, pending_req))\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tcond_resched();\n\t}\ndone:\n\treturn more_to_do;\n}\n\nstatic int\ndo_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)\n{\n\tunion blkif_back_rings *blk_rings = &ring->blk_rings;\n\tint more_to_do;\n\n\tdo {\n\t\tmore_to_do = __do_block_io_op(ring, eoi_flags);\n\t\tif (more_to_do)\n\t\t\tbreak;\n\n\t\tRING_FINAL_CHECK_FOR_REQUESTS(&blk_rings->common, more_to_do);\n\t} while (more_to_do);\n\n\treturn more_to_do;\n}\n \nstatic int dispatch_rw_block_io(struct xen_blkif_ring *ring,\n\t\t\t\tstruct blkif_request *req,\n\t\t\t\tstruct pending_req *pending_req)\n{\n\tstruct phys_req preq;\n\tstruct seg_buf *seg = pending_req->seg;\n\tunsigned int nseg;\n\tstruct bio *bio = NULL;\n\tstruct bio **biolist = pending_req->biolist;\n\tint i, nbio = 0;\n\tenum req_op operation;\n\tblk_opf_t operation_flags = 0;\n\tstruct blk_plug plug;\n\tbool drain = false;\n\tstruct grant_page **pages = pending_req->segments;\n\tunsigned short req_operation;\n\n\treq_operation = req->operation == BLKIF_OP_INDIRECT ?\n\t\t\treq->u.indirect.indirect_op : req->operation;\n\n\tif ((req->operation == BLKIF_OP_INDIRECT) &&\n\t    (req_operation != BLKIF_OP_READ) &&\n\t    (req_operation != BLKIF_OP_WRITE)) {\n\t\tpr_debug(\"Invalid indirect operation (%u)\\n\", req_operation);\n\t\tgoto fail_response;\n\t}\n\n\tswitch (req_operation) {\n\tcase BLKIF_OP_READ:\n\t\tring->st_rd_req++;\n\t\toperation = REQ_OP_READ;\n\t\tbreak;\n\tcase BLKIF_OP_WRITE:\n\t\tring->st_wr_req++;\n\t\toperation = REQ_OP_WRITE;\n\t\toperation_flags = REQ_SYNC | REQ_IDLE;\n\t\tbreak;\n\tcase BLKIF_OP_WRITE_BARRIER:\n\t\tdrain = true;\n\t\tfallthrough;\n\tcase BLKIF_OP_FLUSH_DISKCACHE:\n\t\tring->st_f_req++;\n\t\toperation = REQ_OP_WRITE;\n\t\toperation_flags = REQ_PREFLUSH;\n\t\tbreak;\n\tdefault:\n\t\toperation = 0;  \n\t\tgoto fail_response;\n\t\tbreak;\n\t}\n\n\t \n\tnseg = req->operation == BLKIF_OP_INDIRECT ?\n\t       req->u.indirect.nr_segments : req->u.rw.nr_segments;\n\n\tif (unlikely(nseg == 0 && operation_flags != REQ_PREFLUSH) ||\n\t    unlikely((req->operation != BLKIF_OP_INDIRECT) &&\n\t\t     (nseg > BLKIF_MAX_SEGMENTS_PER_REQUEST)) ||\n\t    unlikely((req->operation == BLKIF_OP_INDIRECT) &&\n\t\t     (nseg > MAX_INDIRECT_SEGMENTS))) {\n\t\tpr_debug(\"Bad number of segments in request (%d)\\n\", nseg);\n\t\t \n\t\tgoto fail_response;\n\t}\n\n\tpreq.nr_sects      = 0;\n\n\tpending_req->ring      = ring;\n\tpending_req->id        = req->u.rw.id;\n\tpending_req->operation = req_operation;\n\tpending_req->status    = BLKIF_RSP_OKAY;\n\tpending_req->nr_segs   = nseg;\n\n\tif (req->operation != BLKIF_OP_INDIRECT) {\n\t\tpreq.dev               = req->u.rw.handle;\n\t\tpreq.sector_number     = req->u.rw.sector_number;\n\t\tfor (i = 0; i < nseg; i++) {\n\t\t\tpages[i]->gref = req->u.rw.seg[i].gref;\n\t\t\tseg[i].nsec = req->u.rw.seg[i].last_sect -\n\t\t\t\treq->u.rw.seg[i].first_sect + 1;\n\t\t\tseg[i].offset = (req->u.rw.seg[i].first_sect << 9);\n\t\t\tif ((req->u.rw.seg[i].last_sect >= (XEN_PAGE_SIZE >> 9)) ||\n\t\t\t    (req->u.rw.seg[i].last_sect <\n\t\t\t     req->u.rw.seg[i].first_sect))\n\t\t\t\tgoto fail_response;\n\t\t\tpreq.nr_sects += seg[i].nsec;\n\t\t}\n\t} else {\n\t\tpreq.dev               = req->u.indirect.handle;\n\t\tpreq.sector_number     = req->u.indirect.sector_number;\n\t\tif (xen_blkbk_parse_indirect(req, pending_req, seg, &preq))\n\t\t\tgoto fail_response;\n\t}\n\n\tif (xen_vbd_translate(&preq, ring->blkif, operation) != 0) {\n\t\tpr_debug(\"access denied: %s of [%llu,%llu] on dev=%04x\\n\",\n\t\t\t operation == REQ_OP_READ ? \"read\" : \"write\",\n\t\t\t preq.sector_number,\n\t\t\t preq.sector_number + preq.nr_sects,\n\t\t\t ring->blkif->vbd.pdevice);\n\t\tgoto fail_response;\n\t}\n\n\t \n\tfor (i = 0; i < nseg; i++) {\n\t\tif (((int)preq.sector_number|(int)seg[i].nsec) &\n\t\t    ((bdev_logical_block_size(preq.bdev) >> 9) - 1)) {\n\t\t\tpr_debug(\"Misaligned I/O request from domain %d\\n\",\n\t\t\t\t ring->blkif->domid);\n\t\t\tgoto fail_response;\n\t\t}\n\t}\n\n\t \n\tif (drain)\n\t\txen_blk_drain_io(pending_req->ring);\n\n\t \n\tif (xen_blkbk_map_seg(pending_req))\n\t\tgoto fail_flush;\n\n\t \n\txen_blkif_get(ring->blkif);\n\tatomic_inc(&ring->inflight);\n\n\tfor (i = 0; i < nseg; i++) {\n\t\twhile ((bio == NULL) ||\n\t\t       (bio_add_page(bio,\n\t\t\t\t     pages[i]->page,\n\t\t\t\t     seg[i].nsec << 9,\n\t\t\t\t     seg[i].offset) == 0)) {\n\t\t\tbio = bio_alloc(preq.bdev, bio_max_segs(nseg - i),\n\t\t\t\t\toperation | operation_flags,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tbiolist[nbio++] = bio;\n\t\t\tbio->bi_private = pending_req;\n\t\t\tbio->bi_end_io  = end_block_io_op;\n\t\t\tbio->bi_iter.bi_sector  = preq.sector_number;\n\t\t}\n\n\t\tpreq.sector_number += seg[i].nsec;\n\t}\n\n\t \n\tif (!bio) {\n\t\tBUG_ON(operation_flags != REQ_PREFLUSH);\n\n\t\tbio = bio_alloc(preq.bdev, 0, operation | operation_flags,\n\t\t\t\tGFP_KERNEL);\n\t\tbiolist[nbio++] = bio;\n\t\tbio->bi_private = pending_req;\n\t\tbio->bi_end_io  = end_block_io_op;\n\t}\n\n\tatomic_set(&pending_req->pendcnt, nbio);\n\tblk_start_plug(&plug);\n\n\tfor (i = 0; i < nbio; i++)\n\t\tsubmit_bio(biolist[i]);\n\n\t \n\tblk_finish_plug(&plug);\n\n\tif (operation == REQ_OP_READ)\n\t\tring->st_rd_sect += preq.nr_sects;\n\telse if (operation == REQ_OP_WRITE)\n\t\tring->st_wr_sect += preq.nr_sects;\n\n\treturn 0;\n\n fail_flush:\n\txen_blkbk_unmap(ring, pending_req->segments,\n\t                pending_req->nr_segs);\n fail_response:\n\t \n\tmake_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);\n\tfree_req(ring, pending_req);\n\tmsleep(1);  \n\treturn -EIO;\n}\n\n\n\n \nstatic void make_response(struct xen_blkif_ring *ring, u64 id,\n\t\t\t  unsigned short op, int st)\n{\n\tstruct blkif_response *resp;\n\tunsigned long     flags;\n\tunion blkif_back_rings *blk_rings;\n\tint notify;\n\n\tspin_lock_irqsave(&ring->blk_ring_lock, flags);\n\tblk_rings = &ring->blk_rings;\n\t \n\tswitch (ring->blkif->blk_protocol) {\n\tcase BLKIF_PROTOCOL_NATIVE:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->native,\n\t\t\t\t\t blk_rings->native.rsp_prod_pvt);\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_32:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_32,\n\t\t\t\t\t blk_rings->x86_32.rsp_prod_pvt);\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_64:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_64,\n\t\t\t\t\t blk_rings->x86_64.rsp_prod_pvt);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tresp->id        = id;\n\tresp->operation = op;\n\tresp->status    = st;\n\n\tblk_rings->common.rsp_prod_pvt++;\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);\n\tspin_unlock_irqrestore(&ring->blk_ring_lock, flags);\n\tif (notify)\n\t\tnotify_remote_via_irq(ring->irq);\n}\n\nstatic int __init xen_blkif_init(void)\n{\n\tint rc = 0;\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\tif (xen_blkif_max_ring_order > XENBUS_MAX_RING_GRANT_ORDER) {\n\t\tpr_info(\"Invalid max_ring_order (%d), will use default max: %d.\\n\",\n\t\t\txen_blkif_max_ring_order, XENBUS_MAX_RING_GRANT_ORDER);\n\t\txen_blkif_max_ring_order = XENBUS_MAX_RING_GRANT_ORDER;\n\t}\n\n\tif (xenblk_max_queues == 0)\n\t\txenblk_max_queues = num_online_cpus();\n\n\trc = xen_blkif_interface_init();\n\tif (rc)\n\t\tgoto failed_init;\n\n\trc = xen_blkif_xenbus_init();\n\tif (rc)\n\t\tgoto failed_init;\n\n failed_init:\n\treturn rc;\n}\n\nmodule_init(xen_blkif_init);\n\nstatic void __exit xen_blkif_fini(void)\n{\n\txen_blkif_xenbus_fini();\n\txen_blkif_interface_fini();\n}\n\nmodule_exit(xen_blkif_fini);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_ALIAS(\"xen-backend:vbd\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}