{
  "module_name": "common.h",
  "hash_id": "fb38e6f68bca96515356b4bf6c12041920230a7a363c7bb28220ff584e9227b2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/xen-blkback/common.h",
  "human_readable_source": " \n\n#ifndef __XEN_BLKIF__BACKEND__COMMON_H__\n#define __XEN_BLKIF__BACKEND__COMMON_H__\n\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include <linux/wait.h>\n#include <linux/io.h>\n#include <linux/rbtree.h>\n#include <asm/setup.h>\n#include <asm/hypervisor.h>\n#include <xen/grant_table.h>\n#include <xen/page.h>\n#include <xen/xenbus.h>\n#include <xen/interface/io/ring.h>\n#include <xen/interface/io/blkif.h>\n#include <xen/interface/io/protocols.h>\n\nextern unsigned int xen_blkif_max_ring_order;\nextern unsigned int xenblk_max_queues;\n \n#define MAX_INDIRECT_SEGMENTS 256\n\n \n#define XEN_PAGES_PER_SEGMENT   (PAGE_SIZE / XEN_PAGE_SIZE)\n\n#define XEN_PAGES_PER_INDIRECT_FRAME \\\n\t(XEN_PAGE_SIZE/sizeof(struct blkif_request_segment))\n#define SEGS_PER_INDIRECT_FRAME\t\\\n\t(XEN_PAGES_PER_INDIRECT_FRAME / XEN_PAGES_PER_SEGMENT)\n\n#define MAX_INDIRECT_PAGES \\\n\t((MAX_INDIRECT_SEGMENTS + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)\n#define INDIRECT_PAGES(_segs) DIV_ROUND_UP(_segs, XEN_PAGES_PER_INDIRECT_FRAME)\n\n \nstruct blkif_common_request {\n\tchar dummy;\n};\n\n \n\nstruct blkif_x86_32_request_rw {\n\tuint8_t        nr_segments;   \n\tblkif_vdev_t   handle;        \n\tuint64_t       id;            \n\tblkif_sector_t sector_number; \n\tstruct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n} __attribute__((__packed__));\n\nstruct blkif_x86_32_request_discard {\n\tuint8_t        flag;          \n\tblkif_vdev_t   _pad1;         \n\tuint64_t       id;            \n\tblkif_sector_t sector_number; \n\tuint64_t       nr_sectors;\n} __attribute__((__packed__));\n\nstruct blkif_x86_32_request_other {\n\tuint8_t        _pad1;\n\tblkif_vdev_t   _pad2;\n\tuint64_t       id;            \n} __attribute__((__packed__));\n\nstruct blkif_x86_32_request_indirect {\n\tuint8_t        indirect_op;\n\tuint16_t       nr_segments;\n\tuint64_t       id;\n\tblkif_sector_t sector_number;\n\tblkif_vdev_t   handle;\n\tuint16_t       _pad1;\n\tgrant_ref_t    indirect_grefs[BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST];\n\t \n\tuint64_t       _pad2;         \n} __attribute__((__packed__));\n\nstruct blkif_x86_32_request {\n\tuint8_t        operation;     \n\tunion {\n\t\tstruct blkif_x86_32_request_rw rw;\n\t\tstruct blkif_x86_32_request_discard discard;\n\t\tstruct blkif_x86_32_request_other other;\n\t\tstruct blkif_x86_32_request_indirect indirect;\n\t} u;\n} __attribute__((__packed__));\n\n \n\nstruct blkif_x86_64_request_rw {\n\tuint8_t        nr_segments;   \n\tblkif_vdev_t   handle;        \n\tuint32_t       _pad1;         \n\tuint64_t       id;\n\tblkif_sector_t sector_number; \n\tstruct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];\n} __attribute__((__packed__));\n\nstruct blkif_x86_64_request_discard {\n\tuint8_t        flag;          \n\tblkif_vdev_t   _pad1;         \n        uint32_t       _pad2;         \n\tuint64_t       id;\n\tblkif_sector_t sector_number; \n\tuint64_t       nr_sectors;\n} __attribute__((__packed__));\n\nstruct blkif_x86_64_request_other {\n\tuint8_t        _pad1;\n\tblkif_vdev_t   _pad2;\n\tuint32_t       _pad3;         \n\tuint64_t       id;            \n} __attribute__((__packed__));\n\nstruct blkif_x86_64_request_indirect {\n\tuint8_t        indirect_op;\n\tuint16_t       nr_segments;\n\tuint32_t       _pad1;         \n\tuint64_t       id;\n\tblkif_sector_t sector_number;\n\tblkif_vdev_t   handle;\n\tuint16_t       _pad2;\n\tgrant_ref_t    indirect_grefs[BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST];\n\t \n\tuint32_t       _pad3;         \n} __attribute__((__packed__));\n\nstruct blkif_x86_64_request {\n\tuint8_t        operation;     \n\tunion {\n\t\tstruct blkif_x86_64_request_rw rw;\n\t\tstruct blkif_x86_64_request_discard discard;\n\t\tstruct blkif_x86_64_request_other other;\n\t\tstruct blkif_x86_64_request_indirect indirect;\n\t} u;\n} __attribute__((__packed__));\n\nDEFINE_RING_TYPES(blkif_common, struct blkif_common_request,\n\t\t  struct blkif_response);\nDEFINE_RING_TYPES(blkif_x86_32, struct blkif_x86_32_request,\n\t\t  struct blkif_response __packed);\nDEFINE_RING_TYPES(blkif_x86_64, struct blkif_x86_64_request,\n\t\t  struct blkif_response);\n\nunion blkif_back_rings {\n\tstruct blkif_back_ring        native;\n\tstruct blkif_common_back_ring common;\n\tstruct blkif_x86_32_back_ring x86_32;\n\tstruct blkif_x86_64_back_ring x86_64;\n};\n\nenum blkif_protocol {\n\tBLKIF_PROTOCOL_NATIVE = 1,\n\tBLKIF_PROTOCOL_X86_32 = 2,\n\tBLKIF_PROTOCOL_X86_64 = 3,\n};\n\n \n#ifdef CONFIG_X86\n#  define BLKIF_PROTOCOL_DEFAULT BLKIF_PROTOCOL_X86_32\n#else\n#  define BLKIF_PROTOCOL_DEFAULT BLKIF_PROTOCOL_NATIVE\n#endif\n\nstruct xen_vbd {\n\t \n\tblkif_vdev_t\t\thandle;\n\t \n\tunsigned char\t\treadonly;\n\t \n\tunsigned char\t\ttype;\n\t \n\tu32\t\t\tpdevice;\n\tstruct block_device\t*bdev;\n\t \n\tsector_t\t\tsize;\n\tunsigned int\t\tflush_support:1;\n\tunsigned int\t\tdiscard_secure:1;\n\t \n\tunsigned int\t\tfeature_gnt_persistent_parm:1;\n\t \n\tunsigned int\t\tfeature_gnt_persistent:1;\n\tunsigned int\t\toverflow_max_grants:1;\n};\n\nstruct backend_info;\n\n \n#define XEN_BLKIF_REQS_PER_PAGE\t\t32\n\nstruct persistent_gnt {\n\tstruct page *page;\n\tgrant_ref_t gnt;\n\tgrant_handle_t handle;\n\tunsigned long last_used;\n\tbool active;\n\tstruct rb_node node;\n\tstruct list_head remove_node;\n};\n\n \nstruct xen_blkif_ring {\n\t \n\tunsigned int\t\tirq;\n\tunion blkif_back_rings\tblk_rings;\n\tvoid\t\t\t*blk_ring;\n\t \n\tspinlock_t\t\tblk_ring_lock;\n\n\twait_queue_head_t\twq;\n\tatomic_t\t\tinflight;\n\tbool\t\t\tactive;\n\t \n\tstruct task_struct\t*xenblkd;\n\tunsigned int\t\twaiting_reqs;\n\n\t \n\tstruct list_head\tpending_free;\n\t \n\tspinlock_t\t\tpending_free_lock;\n\twait_queue_head_t\tpending_free_wq;\n\n\t \n\tstruct rb_root\t\tpersistent_gnts;\n\tunsigned int\t\tpersistent_gnt_c;\n\tatomic_t\t\tpersistent_gnt_in_use;\n\tunsigned long           next_lru;\n\n\t \n\tunsigned long\t\tst_print;\n\tunsigned long long\tst_rd_req;\n\tunsigned long long\tst_wr_req;\n\tunsigned long long\tst_oo_req;\n\tunsigned long long\tst_f_req;\n\tunsigned long long\tst_ds_req;\n\tunsigned long long\tst_rd_sect;\n\tunsigned long long\tst_wr_sect;\n\n\t \n\tstruct list_head\tpersistent_purge_list;\n\tstruct work_struct\tpersistent_purge_work;\n\n\t \n\tstruct gnttab_page_cache free_pages;\n\n\tstruct work_struct\tfree_work;\n\t \n\twait_queue_head_t\tshutdown_wq;\n\tstruct xen_blkif\t*blkif;\n};\n\nstruct xen_blkif {\n\t \n\tdomid_t\t\t\tdomid;\n\tunsigned int\t\thandle;\n\t \n\tenum blkif_protocol\tblk_protocol;\n\t \n\tstruct xen_vbd\t\tvbd;\n\t \n\tstruct backend_info\t*be;\n\tatomic_t\t\trefcnt;\n\t \n\tstruct completion\tdrain_complete;\n\tatomic_t\t\tdrain;\n\n\tstruct work_struct\tfree_work;\n\tunsigned int\t\tnr_ring_pages;\n\tbool\t\t\tmulti_ref;\n\t \n\tstruct xen_blkif_ring\t*rings;\n\tunsigned int\t\tnr_rings;\n\tunsigned long\t\tbuffer_squeeze_end;\n};\n\nstruct seg_buf {\n\tunsigned long offset;\n\tunsigned int nsec;\n};\n\nstruct grant_page {\n\tstruct page\t\t*page;\n\tstruct persistent_gnt\t*persistent_gnt;\n\tgrant_handle_t\t\thandle;\n\tgrant_ref_t\t\tgref;\n};\n\n \nstruct pending_req {\n\tstruct xen_blkif_ring   *ring;\n\tu64\t\t\tid;\n\tint\t\t\tnr_segs;\n\tatomic_t\t\tpendcnt;\n\tunsigned short\t\toperation;\n\tint\t\t\tstatus;\n\tstruct list_head\tfree_list;\n\tstruct grant_page\t*segments[MAX_INDIRECT_SEGMENTS];\n\t \n\tstruct grant_page\t*indirect_pages[MAX_INDIRECT_PAGES];\n\tstruct seg_buf\t\tseg[MAX_INDIRECT_SEGMENTS];\n\tstruct bio\t\t*biolist[MAX_INDIRECT_SEGMENTS];\n\tstruct gnttab_unmap_grant_ref unmap[MAX_INDIRECT_SEGMENTS];\n\tstruct page                   *unmap_pages[MAX_INDIRECT_SEGMENTS];\n\tstruct gntab_unmap_queue_data gnttab_unmap_data;\n};\n\n\n#define vbd_sz(_v)\tbdev_nr_sectors((_v)->bdev)\n\n#define xen_blkif_get(_b) (atomic_inc(&(_b)->refcnt))\n#define xen_blkif_put(_b)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (atomic_dec_and_test(&(_b)->refcnt))\t\\\n\t\t\tschedule_work(&(_b)->free_work);\\\n\t} while (0)\n\nstruct phys_req {\n\tunsigned short\t\tdev;\n\tblkif_sector_t\t\tnr_sects;\n\tstruct block_device\t*bdev;\n\tblkif_sector_t\t\tsector_number;\n};\n\nint xen_blkif_interface_init(void);\nvoid xen_blkif_interface_fini(void);\n\nint xen_blkif_xenbus_init(void);\nvoid xen_blkif_xenbus_fini(void);\n\nirqreturn_t xen_blkif_be_int(int irq, void *dev_id);\nint xen_blkif_schedule(void *arg);\nvoid xen_blkbk_free_caches(struct xen_blkif_ring *ring);\n\nint xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,\n\t\t\t      struct backend_info *be, int state);\n\nint xen_blkbk_barrier(struct xenbus_transaction xbt,\n\t\t      struct backend_info *be, int state);\nstruct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);\nvoid xen_blkbk_unmap_purged_grants(struct work_struct *work);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}