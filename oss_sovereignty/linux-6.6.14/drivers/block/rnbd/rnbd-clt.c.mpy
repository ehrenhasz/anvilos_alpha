{
  "module_name": "rnbd-clt.c",
  "hash_id": "38cd77a5a7e04609ce92f2101accdd564b4ccd8533358b48ffd5023ba94c3180",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/rnbd/rnbd-clt.c",
  "human_readable_source": "\n \n\n#undef pr_fmt\n#define pr_fmt(fmt) KBUILD_MODNAME \" L\" __stringify(__LINE__) \": \" fmt\n\n#include <linux/module.h>\n#include <linux/blkdev.h>\n#include <linux/hdreg.h>\n#include <linux/scatterlist.h>\n#include <linux/idr.h>\n\n#include \"rnbd-clt.h\"\n\nMODULE_DESCRIPTION(\"RDMA Network Block Device Client\");\nMODULE_LICENSE(\"GPL\");\n\nstatic int rnbd_client_major;\nstatic DEFINE_IDA(index_ida);\nstatic DEFINE_MUTEX(sess_lock);\nstatic LIST_HEAD(sess_list);\nstatic struct workqueue_struct *rnbd_clt_wq;\n\n \n#define RNBD_PART_BITS\t\t6\n\nstatic inline bool rnbd_clt_get_sess(struct rnbd_clt_session *sess)\n{\n\treturn refcount_inc_not_zero(&sess->refcount);\n}\n\nstatic void free_sess(struct rnbd_clt_session *sess);\n\nstatic void rnbd_clt_put_sess(struct rnbd_clt_session *sess)\n{\n\tmight_sleep();\n\n\tif (refcount_dec_and_test(&sess->refcount))\n\t\tfree_sess(sess);\n}\n\nstatic void rnbd_clt_put_dev(struct rnbd_clt_dev *dev)\n{\n\tmight_sleep();\n\n\tif (!refcount_dec_and_test(&dev->refcount))\n\t\treturn;\n\n\tida_free(&index_ida, dev->clt_device_id);\n\tkfree(dev->hw_queues);\n\tkfree(dev->pathname);\n\trnbd_clt_put_sess(dev->sess);\n\tmutex_destroy(&dev->lock);\n\tkfree(dev);\n}\n\nstatic inline bool rnbd_clt_get_dev(struct rnbd_clt_dev *dev)\n{\n\treturn refcount_inc_not_zero(&dev->refcount);\n}\n\nstatic void rnbd_clt_change_capacity(struct rnbd_clt_dev *dev,\n\t\t\t\t    sector_t new_nsectors)\n{\n\tif (get_capacity(dev->gd) == new_nsectors)\n\t\treturn;\n\n\t \n\trnbd_clt_info(dev, \"Device size changed from %llu to %llu sectors\\n\",\n\t\t      get_capacity(dev->gd), new_nsectors);\n\tset_capacity_and_notify(dev->gd, new_nsectors);\n}\n\nstatic int process_msg_open_rsp(struct rnbd_clt_dev *dev,\n\t\t\t\tstruct rnbd_msg_open_rsp *rsp)\n{\n\tstruct kobject *gd_kobj;\n\tint err = 0;\n\n\tmutex_lock(&dev->lock);\n\tif (dev->dev_state == DEV_STATE_UNMAPPED) {\n\t\trnbd_clt_info(dev,\n\t\t\t       \"Ignoring Open-Response message from server for  unmapped device\\n\");\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (dev->dev_state == DEV_STATE_MAPPED_DISCONNECTED) {\n\t\tu64 nsectors = le64_to_cpu(rsp->nsectors);\n\n\t\trnbd_clt_change_capacity(dev, nsectors);\n\t\tgd_kobj = &disk_to_dev(dev->gd)->kobj;\n\t\tkobject_uevent(gd_kobj, KOBJ_ONLINE);\n\t\trnbd_clt_info(dev, \"Device online, device remapped successfully\\n\");\n\t}\n\tif (!rsp->logical_block_size) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tdev->device_id = le32_to_cpu(rsp->device_id);\n\tdev->dev_state = DEV_STATE_MAPPED;\n\nout:\n\tmutex_unlock(&dev->lock);\n\n\treturn err;\n}\n\nint rnbd_clt_resize_disk(struct rnbd_clt_dev *dev, sector_t newsize)\n{\n\tint ret = 0;\n\n\tmutex_lock(&dev->lock);\n\tif (dev->dev_state != DEV_STATE_MAPPED) {\n\t\tpr_err(\"Failed to set new size of the device, device is not opened\\n\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\trnbd_clt_change_capacity(dev, newsize);\n\nout:\n\tmutex_unlock(&dev->lock);\n\n\treturn ret;\n}\n\nstatic inline void rnbd_clt_dev_requeue(struct rnbd_queue *q)\n{\n\tif (WARN_ON(!q->hctx))\n\t\treturn;\n\n\t \n\tblk_mq_run_hw_queue(q->hctx, true);\n}\n\nenum {\n\tRNBD_DELAY_IFBUSY = -1,\n};\n\n \nstatic struct rnbd_cpu_qlist *\nrnbd_get_cpu_qlist(struct rnbd_clt_session *sess, int cpu)\n{\n\tint bit;\n\n\t \n\tbit = find_next_bit(sess->cpu_queues_bm, nr_cpu_ids, cpu);\n\tif (bit < nr_cpu_ids) {\n\t\treturn per_cpu_ptr(sess->cpu_queues, bit);\n\t} else if (cpu != 0) {\n\t\t \n\t\tbit = find_first_bit(sess->cpu_queues_bm, cpu);\n\t\tif (bit < cpu)\n\t\t\treturn per_cpu_ptr(sess->cpu_queues, bit);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline int nxt_cpu(int cpu)\n{\n\treturn (cpu + 1) % nr_cpu_ids;\n}\n\n \nstatic bool rnbd_rerun_if_needed(struct rnbd_clt_session *sess)\n{\n\tstruct rnbd_queue *q = NULL;\n\tstruct rnbd_cpu_qlist *cpu_q;\n\tunsigned long flags;\n\tint *cpup;\n\n\t \n\tcpup = get_cpu_ptr(sess->cpu_rr);\n\tfor (cpu_q = rnbd_get_cpu_qlist(sess, nxt_cpu(*cpup)); cpu_q;\n\t     cpu_q = rnbd_get_cpu_qlist(sess, nxt_cpu(cpu_q->cpu))) {\n\t\tif (!spin_trylock_irqsave(&cpu_q->requeue_lock, flags))\n\t\t\tcontinue;\n\t\tif (!test_bit(cpu_q->cpu, sess->cpu_queues_bm))\n\t\t\tgoto unlock;\n\t\tq = list_first_entry_or_null(&cpu_q->requeue_list,\n\t\t\t\t\t     typeof(*q), requeue_list);\n\t\tif (WARN_ON(!q))\n\t\t\tgoto clear_bit;\n\t\tlist_del_init(&q->requeue_list);\n\t\tclear_bit_unlock(0, &q->in_list);\n\n\t\tif (list_empty(&cpu_q->requeue_list)) {\n\t\t\t \nclear_bit:\n\t\t\tclear_bit(cpu_q->cpu, sess->cpu_queues_bm);\n\t\t}\nunlock:\n\t\tspin_unlock_irqrestore(&cpu_q->requeue_lock, flags);\n\n\t\tif (q)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (cpu_q)\n\t\t*cpup = cpu_q->cpu;\n\tput_cpu_ptr(sess->cpu_rr);\n\n\tif (q)\n\t\trnbd_clt_dev_requeue(q);\n\n\treturn q;\n}\n\n \nstatic void rnbd_rerun_all_if_idle(struct rnbd_clt_session *sess)\n{\n\tbool requeued;\n\n\tdo {\n\t\trequeued = rnbd_rerun_if_needed(sess);\n\t} while (atomic_read(&sess->busy) == 0 && requeued);\n}\n\nstatic struct rtrs_permit *rnbd_get_permit(struct rnbd_clt_session *sess,\n\t\t\t\t\t     enum rtrs_clt_con_type con_type,\n\t\t\t\t\t     enum wait_type wait)\n{\n\tstruct rtrs_permit *permit;\n\n\tpermit = rtrs_clt_get_permit(sess->rtrs, con_type, wait);\n\tif (permit)\n\t\t \n\t\tatomic_inc(&sess->busy);\n\n\treturn permit;\n}\n\nstatic void rnbd_put_permit(struct rnbd_clt_session *sess,\n\t\t\t     struct rtrs_permit *permit)\n{\n\trtrs_clt_put_permit(sess->rtrs, permit);\n\tatomic_dec(&sess->busy);\n\t \n\tsmp_mb__after_atomic();\n\trnbd_rerun_all_if_idle(sess);\n}\n\nstatic struct rnbd_iu *rnbd_get_iu(struct rnbd_clt_session *sess,\n\t\t\t\t     enum rtrs_clt_con_type con_type,\n\t\t\t\t     enum wait_type wait)\n{\n\tstruct rnbd_iu *iu;\n\tstruct rtrs_permit *permit;\n\n\tiu = kzalloc(sizeof(*iu), GFP_KERNEL);\n\tif (!iu)\n\t\treturn NULL;\n\n\tpermit = rnbd_get_permit(sess, con_type, wait);\n\tif (!permit) {\n\t\tkfree(iu);\n\t\treturn NULL;\n\t}\n\n\tiu->permit = permit;\n\t \n\tatomic_set(&iu->refcount, 2);\n\tinit_waitqueue_head(&iu->comp.wait);\n\tiu->comp.errno = INT_MAX;\n\n\tif (sg_alloc_table(&iu->sgt, 1, GFP_KERNEL)) {\n\t\trnbd_put_permit(sess, permit);\n\t\tkfree(iu);\n\t\treturn NULL;\n\t}\n\n\treturn iu;\n}\n\nstatic void rnbd_put_iu(struct rnbd_clt_session *sess, struct rnbd_iu *iu)\n{\n\tif (atomic_dec_and_test(&iu->refcount)) {\n\t\tsg_free_table(&iu->sgt);\n\t\trnbd_put_permit(sess, iu->permit);\n\t\tkfree(iu);\n\t}\n}\n\nstatic void rnbd_softirq_done_fn(struct request *rq)\n{\n\tstruct rnbd_clt_dev *dev\t= rq->q->disk->private_data;\n\tstruct rnbd_clt_session *sess\t= dev->sess;\n\tstruct rnbd_iu *iu;\n\n\tiu = blk_mq_rq_to_pdu(rq);\n\tsg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);\n\trnbd_put_permit(sess, iu->permit);\n\tblk_mq_end_request(rq, errno_to_blk_status(iu->errno));\n}\n\nstatic void msg_io_conf(void *priv, int errno)\n{\n\tstruct rnbd_iu *iu = priv;\n\tstruct rnbd_clt_dev *dev = iu->dev;\n\tstruct request *rq = iu->rq;\n\tint rw = rq_data_dir(rq);\n\n\tiu->errno = errno;\n\n\tblk_mq_complete_request(rq);\n\n\tif (errno)\n\t\trnbd_clt_info_rl(dev, \"%s I/O failed with err: %d\\n\",\n\t\t\t\t rw == READ ? \"read\" : \"write\", errno);\n}\n\nstatic void wake_up_iu_comp(struct rnbd_iu *iu, int errno)\n{\n\tiu->comp.errno = errno;\n\twake_up(&iu->comp.wait);\n}\n\nstatic void msg_conf(void *priv, int errno)\n{\n\tstruct rnbd_iu *iu = priv;\n\n\tiu->errno = errno;\n\tschedule_work(&iu->work);\n}\n\nstatic int send_usr_msg(struct rtrs_clt_sess *rtrs, int dir,\n\t\t\tstruct rnbd_iu *iu, struct kvec *vec,\n\t\t\tsize_t len, struct scatterlist *sg, unsigned int sg_len,\n\t\t\tvoid (*conf)(struct work_struct *work),\n\t\t\tint *errno, int wait)\n{\n\tint err;\n\tstruct rtrs_clt_req_ops req_ops;\n\n\tINIT_WORK(&iu->work, conf);\n\treq_ops = (struct rtrs_clt_req_ops) {\n\t\t.priv = iu,\n\t\t.conf_fn = msg_conf,\n\t};\n\terr = rtrs_clt_request(dir, &req_ops, rtrs, iu->permit,\n\t\t\t\tvec, 1, len, sg, sg_len);\n\tif (!err && wait) {\n\t\twait_event(iu->comp.wait, iu->comp.errno != INT_MAX);\n\t\t*errno = iu->comp.errno;\n\t} else {\n\t\t*errno = 0;\n\t}\n\n\treturn err;\n}\n\nstatic void msg_close_conf(struct work_struct *work)\n{\n\tstruct rnbd_iu *iu = container_of(work, struct rnbd_iu, work);\n\tstruct rnbd_clt_dev *dev = iu->dev;\n\n\twake_up_iu_comp(iu, iu->errno);\n\trnbd_put_iu(dev->sess, iu);\n\trnbd_clt_put_dev(dev);\n}\n\nstatic int send_msg_close(struct rnbd_clt_dev *dev, u32 device_id,\n\t\t\t  enum wait_type wait)\n{\n\tstruct rnbd_clt_session *sess = dev->sess;\n\tstruct rnbd_msg_close msg;\n\tstruct rnbd_iu *iu;\n\tstruct kvec vec = {\n\t\t.iov_base = &msg,\n\t\t.iov_len  = sizeof(msg)\n\t};\n\tint err, errno;\n\n\tiu = rnbd_get_iu(sess, RTRS_ADMIN_CON, RTRS_PERMIT_WAIT);\n\tif (!iu)\n\t\treturn -ENOMEM;\n\n\tiu->buf = NULL;\n\tiu->dev = dev;\n\n\tmsg.hdr.type\t= cpu_to_le16(RNBD_MSG_CLOSE);\n\tmsg.device_id\t= cpu_to_le32(device_id);\n\n\tWARN_ON(!rnbd_clt_get_dev(dev));\n\terr = send_usr_msg(sess->rtrs, WRITE, iu, &vec, 0, NULL, 0,\n\t\t\t   msg_close_conf, &errno, wait);\n\tif (err) {\n\t\trnbd_clt_put_dev(dev);\n\t\trnbd_put_iu(sess, iu);\n\t} else {\n\t\terr = errno;\n\t}\n\n\trnbd_put_iu(sess, iu);\n\treturn err;\n}\n\nstatic void msg_open_conf(struct work_struct *work)\n{\n\tstruct rnbd_iu *iu = container_of(work, struct rnbd_iu, work);\n\tstruct rnbd_msg_open_rsp *rsp = iu->buf;\n\tstruct rnbd_clt_dev *dev = iu->dev;\n\tint errno = iu->errno;\n\tbool from_map = false;\n\n\t \n\tif (dev->dev_state == DEV_STATE_INIT)\n\t\tfrom_map = true;\n\n\tif (errno) {\n\t\trnbd_clt_err(dev,\n\t\t\t      \"Opening failed, server responded: %d\\n\",\n\t\t\t      errno);\n\t} else {\n\t\terrno = process_msg_open_rsp(dev, rsp);\n\t\tif (errno) {\n\t\t\tu32 device_id = le32_to_cpu(rsp->device_id);\n\t\t\t \n\t\t\tsend_msg_close(dev, device_id, RTRS_PERMIT_NOWAIT);\n\t\t}\n\t}\n\t \n\tif (!from_map)\n\t\tkfree(rsp);\n\twake_up_iu_comp(iu, errno);\n\trnbd_put_iu(dev->sess, iu);\n\trnbd_clt_put_dev(dev);\n}\n\nstatic void msg_sess_info_conf(struct work_struct *work)\n{\n\tstruct rnbd_iu *iu = container_of(work, struct rnbd_iu, work);\n\tstruct rnbd_msg_sess_info_rsp *rsp = iu->buf;\n\tstruct rnbd_clt_session *sess = iu->sess;\n\n\tif (!iu->errno)\n\t\tsess->ver = min_t(u8, rsp->ver, RNBD_PROTO_VER_MAJOR);\n\n\tkfree(rsp);\n\twake_up_iu_comp(iu, iu->errno);\n\trnbd_put_iu(sess, iu);\n\trnbd_clt_put_sess(sess);\n}\n\nstatic int send_msg_open(struct rnbd_clt_dev *dev, enum wait_type wait)\n{\n\tstruct rnbd_clt_session *sess = dev->sess;\n\tstruct rnbd_msg_open_rsp *rsp;\n\tstruct rnbd_msg_open msg;\n\tstruct rnbd_iu *iu;\n\tstruct kvec vec = {\n\t\t.iov_base = &msg,\n\t\t.iov_len  = sizeof(msg)\n\t};\n\tint err, errno;\n\n\trsp = kzalloc(sizeof(*rsp), GFP_KERNEL);\n\tif (!rsp)\n\t\treturn -ENOMEM;\n\n\tiu = rnbd_get_iu(sess, RTRS_ADMIN_CON, RTRS_PERMIT_WAIT);\n\tif (!iu) {\n\t\tkfree(rsp);\n\t\treturn -ENOMEM;\n\t}\n\n\tiu->buf = rsp;\n\tiu->dev = dev;\n\n\tsg_init_one(iu->sgt.sgl, rsp, sizeof(*rsp));\n\n\tmsg.hdr.type\t= cpu_to_le16(RNBD_MSG_OPEN);\n\tmsg.access_mode\t= dev->access_mode;\n\tstrscpy(msg.dev_name, dev->pathname, sizeof(msg.dev_name));\n\n\tWARN_ON(!rnbd_clt_get_dev(dev));\n\terr = send_usr_msg(sess->rtrs, READ, iu,\n\t\t\t   &vec, sizeof(*rsp), iu->sgt.sgl, 1,\n\t\t\t   msg_open_conf, &errno, wait);\n\tif (err) {\n\t\trnbd_clt_put_dev(dev);\n\t\trnbd_put_iu(sess, iu);\n\t\tkfree(rsp);\n\t} else {\n\t\terr = errno;\n\t}\n\n\trnbd_put_iu(sess, iu);\n\treturn err;\n}\n\nstatic int send_msg_sess_info(struct rnbd_clt_session *sess, enum wait_type wait)\n{\n\tstruct rnbd_msg_sess_info_rsp *rsp;\n\tstruct rnbd_msg_sess_info msg;\n\tstruct rnbd_iu *iu;\n\tstruct kvec vec = {\n\t\t.iov_base = &msg,\n\t\t.iov_len  = sizeof(msg)\n\t};\n\tint err, errno;\n\n\trsp = kzalloc(sizeof(*rsp), GFP_KERNEL);\n\tif (!rsp)\n\t\treturn -ENOMEM;\n\n\tiu = rnbd_get_iu(sess, RTRS_ADMIN_CON, RTRS_PERMIT_WAIT);\n\tif (!iu) {\n\t\tkfree(rsp);\n\t\treturn -ENOMEM;\n\t}\n\n\tiu->buf = rsp;\n\tiu->sess = sess;\n\tsg_init_one(iu->sgt.sgl, rsp, sizeof(*rsp));\n\n\tmsg.hdr.type = cpu_to_le16(RNBD_MSG_SESS_INFO);\n\tmsg.ver      = RNBD_PROTO_VER_MAJOR;\n\n\tif (!rnbd_clt_get_sess(sess)) {\n\t\t \n\t\terr = -ENODEV;\n\t\tgoto put_iu;\n\t}\n\terr = send_usr_msg(sess->rtrs, READ, iu,\n\t\t\t   &vec, sizeof(*rsp), iu->sgt.sgl, 1,\n\t\t\t   msg_sess_info_conf, &errno, wait);\n\tif (err) {\n\t\trnbd_clt_put_sess(sess);\nput_iu:\n\t\trnbd_put_iu(sess, iu);\n\t\tkfree(rsp);\n\t} else {\n\t\terr = errno;\n\t}\n\trnbd_put_iu(sess, iu);\n\treturn err;\n}\n\nstatic void set_dev_states_to_disconnected(struct rnbd_clt_session *sess)\n{\n\tstruct rnbd_clt_dev *dev;\n\tstruct kobject *gd_kobj;\n\n\tmutex_lock(&sess->lock);\n\tlist_for_each_entry(dev, &sess->devs_list, list) {\n\t\trnbd_clt_err(dev, \"Device disconnected.\\n\");\n\n\t\tmutex_lock(&dev->lock);\n\t\tif (dev->dev_state == DEV_STATE_MAPPED) {\n\t\t\tdev->dev_state = DEV_STATE_MAPPED_DISCONNECTED;\n\t\t\tgd_kobj = &disk_to_dev(dev->gd)->kobj;\n\t\t\tkobject_uevent(gd_kobj, KOBJ_OFFLINE);\n\t\t}\n\t\tmutex_unlock(&dev->lock);\n\t}\n\tmutex_unlock(&sess->lock);\n}\n\nstatic void remap_devs(struct rnbd_clt_session *sess)\n{\n\tstruct rnbd_clt_dev *dev;\n\tstruct rtrs_attrs attrs;\n\tint err;\n\n\t \n\n\terr = send_msg_sess_info(sess, RTRS_PERMIT_NOWAIT);\n\tif (err) {\n\t\tpr_err(\"send_msg_sess_info(\\\"%s\\\"): %d\\n\", sess->sessname, err);\n\t\treturn;\n\t}\n\n\terr = rtrs_clt_query(sess->rtrs, &attrs);\n\tif (err) {\n\t\tpr_err(\"rtrs_clt_query(\\\"%s\\\"): %d\\n\", sess->sessname, err);\n\t\treturn;\n\t}\n\tmutex_lock(&sess->lock);\n\tsess->max_io_size = attrs.max_io_size;\n\n\tlist_for_each_entry(dev, &sess->devs_list, list) {\n\t\tbool skip;\n\n\t\tmutex_lock(&dev->lock);\n\t\tskip = (dev->dev_state == DEV_STATE_INIT);\n\t\tmutex_unlock(&dev->lock);\n\t\tif (skip)\n\t\t\t \n\t\t\tcontinue;\n\n\t\trnbd_clt_info(dev, \"session reconnected, remapping device\\n\");\n\t\terr = send_msg_open(dev, RTRS_PERMIT_NOWAIT);\n\t\tif (err) {\n\t\t\trnbd_clt_err(dev, \"send_msg_open(): %d\\n\", err);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&sess->lock);\n}\n\nstatic void rnbd_clt_link_ev(void *priv, enum rtrs_clt_link_ev ev)\n{\n\tstruct rnbd_clt_session *sess = priv;\n\n\tswitch (ev) {\n\tcase RTRS_CLT_LINK_EV_DISCONNECTED:\n\t\tset_dev_states_to_disconnected(sess);\n\t\tbreak;\n\tcase RTRS_CLT_LINK_EV_RECONNECTED:\n\t\tremap_devs(sess);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Unknown session event received (%d), session: %s\\n\",\n\t\t       ev, sess->sessname);\n\t}\n}\n\nstatic void rnbd_init_cpu_qlists(struct rnbd_cpu_qlist __percpu *cpu_queues)\n{\n\tunsigned int cpu;\n\tstruct rnbd_cpu_qlist *cpu_q;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_q = per_cpu_ptr(cpu_queues, cpu);\n\n\t\tcpu_q->cpu = cpu;\n\t\tINIT_LIST_HEAD(&cpu_q->requeue_list);\n\t\tspin_lock_init(&cpu_q->requeue_lock);\n\t}\n}\n\nstatic void destroy_mq_tags(struct rnbd_clt_session *sess)\n{\n\tif (sess->tag_set.tags)\n\t\tblk_mq_free_tag_set(&sess->tag_set);\n}\n\nstatic inline void wake_up_rtrs_waiters(struct rnbd_clt_session *sess)\n{\n\tsess->rtrs_ready = true;\n\twake_up_all(&sess->rtrs_waitq);\n}\n\nstatic void close_rtrs(struct rnbd_clt_session *sess)\n{\n\tmight_sleep();\n\n\tif (!IS_ERR_OR_NULL(sess->rtrs)) {\n\t\trtrs_clt_close(sess->rtrs);\n\t\tsess->rtrs = NULL;\n\t\twake_up_rtrs_waiters(sess);\n\t}\n}\n\nstatic void free_sess(struct rnbd_clt_session *sess)\n{\n\tWARN_ON(!list_empty(&sess->devs_list));\n\n\tmight_sleep();\n\n\tclose_rtrs(sess);\n\tdestroy_mq_tags(sess);\n\tif (!list_empty(&sess->list)) {\n\t\tmutex_lock(&sess_lock);\n\t\tlist_del(&sess->list);\n\t\tmutex_unlock(&sess_lock);\n\t}\n\tfree_percpu(sess->cpu_queues);\n\tfree_percpu(sess->cpu_rr);\n\tmutex_destroy(&sess->lock);\n\tkfree(sess);\n}\n\nstatic struct rnbd_clt_session *alloc_sess(const char *sessname)\n{\n\tstruct rnbd_clt_session *sess;\n\tint err, cpu;\n\n\tsess = kzalloc_node(sizeof(*sess), GFP_KERNEL, NUMA_NO_NODE);\n\tif (!sess)\n\t\treturn ERR_PTR(-ENOMEM);\n\tstrscpy(sess->sessname, sessname, sizeof(sess->sessname));\n\tatomic_set(&sess->busy, 0);\n\tmutex_init(&sess->lock);\n\tINIT_LIST_HEAD(&sess->devs_list);\n\tINIT_LIST_HEAD(&sess->list);\n\tbitmap_zero(sess->cpu_queues_bm, num_possible_cpus());\n\tinit_waitqueue_head(&sess->rtrs_waitq);\n\trefcount_set(&sess->refcount, 1);\n\n\tsess->cpu_queues = alloc_percpu(struct rnbd_cpu_qlist);\n\tif (!sess->cpu_queues) {\n\t\terr = -ENOMEM;\n\t\tgoto err;\n\t}\n\trnbd_init_cpu_qlists(sess->cpu_queues);\n\n\t \n\tsess->cpu_rr = alloc_percpu(int);\n\tif (!sess->cpu_rr) {\n\t\terr = -ENOMEM;\n\t\tgoto err;\n\t}\n\tfor_each_possible_cpu(cpu)\n\t\t* per_cpu_ptr(sess->cpu_rr, cpu) = cpu;\n\n\treturn sess;\n\nerr:\n\tfree_sess(sess);\n\n\treturn ERR_PTR(err);\n}\n\nstatic int wait_for_rtrs_connection(struct rnbd_clt_session *sess)\n{\n\twait_event(sess->rtrs_waitq, sess->rtrs_ready);\n\tif (IS_ERR_OR_NULL(sess->rtrs))\n\t\treturn -ECONNRESET;\n\n\treturn 0;\n}\n\nstatic void wait_for_rtrs_disconnection(struct rnbd_clt_session *sess)\n\t__releases(&sess_lock)\n\t__acquires(&sess_lock)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(&sess->rtrs_waitq, &wait, TASK_UNINTERRUPTIBLE);\n\tif (IS_ERR_OR_NULL(sess->rtrs)) {\n\t\tfinish_wait(&sess->rtrs_waitq, &wait);\n\t\treturn;\n\t}\n\tmutex_unlock(&sess_lock);\n\t \n\tschedule();\n\tmutex_lock(&sess_lock);\n}\n\nstatic struct rnbd_clt_session *__find_and_get_sess(const char *sessname)\n\t__releases(&sess_lock)\n\t__acquires(&sess_lock)\n{\n\tstruct rnbd_clt_session *sess, *sn;\n\tint err;\n\nagain:\n\tlist_for_each_entry_safe(sess, sn, &sess_list, list) {\n\t\tif (strcmp(sessname, sess->sessname))\n\t\t\tcontinue;\n\n\t\tif (sess->rtrs_ready && IS_ERR_OR_NULL(sess->rtrs))\n\t\t\t \n\t\t\tcontinue;\n\n\t\tif (rnbd_clt_get_sess(sess)) {\n\t\t\t \n\t\t\tmutex_unlock(&sess_lock);\n\t\t\terr = wait_for_rtrs_connection(sess);\n\t\t\tif (err)\n\t\t\t\trnbd_clt_put_sess(sess);\n\t\t\tmutex_lock(&sess_lock);\n\n\t\t\tif (err)\n\t\t\t\t \n\t\t\t\tgoto again;\n\n\t\t\treturn sess;\n\t\t}\n\t\t \n\t\twait_for_rtrs_disconnection(sess);\n\t\t \n\t\tgoto again;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct\nrnbd_clt_session *find_or_create_sess(const char *sessname, bool *first)\n{\n\tstruct rnbd_clt_session *sess = NULL;\n\n\tmutex_lock(&sess_lock);\n\tsess = __find_and_get_sess(sessname);\n\tif (!sess) {\n\t\tsess = alloc_sess(sessname);\n\t\tif (IS_ERR(sess)) {\n\t\t\tmutex_unlock(&sess_lock);\n\t\t\treturn sess;\n\t\t}\n\t\tlist_add(&sess->list, &sess_list);\n\t\t*first = true;\n\t}\n\tmutex_unlock(&sess_lock);\n\n\treturn sess;\n}\n\nstatic int rnbd_client_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct rnbd_clt_dev *dev = disk->private_data;\n\n\tif (get_disk_ro(dev->gd) && (mode & BLK_OPEN_WRITE))\n\t\treturn -EPERM;\n\n\tif (dev->dev_state == DEV_STATE_UNMAPPED ||\n\t    !rnbd_clt_get_dev(dev))\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic void rnbd_client_release(struct gendisk *gen)\n{\n\tstruct rnbd_clt_dev *dev = gen->private_data;\n\n\trnbd_clt_put_dev(dev);\n}\n\nstatic int rnbd_client_getgeo(struct block_device *block_device,\n\t\t\t      struct hd_geometry *geo)\n{\n\tu64 size;\n\tstruct rnbd_clt_dev *dev = block_device->bd_disk->private_data;\n\tstruct queue_limits *limit = &dev->queue->limits;\n\n\tsize = dev->size * (limit->logical_block_size / SECTOR_SIZE);\n\tgeo->cylinders\t= size >> 6;\t \n\tgeo->heads\t= 4;\n\tgeo->sectors\t= 16;\n\tgeo->start\t= 0;\n\n\treturn 0;\n}\n\nstatic const struct block_device_operations rnbd_client_ops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= rnbd_client_open,\n\t.release\t= rnbd_client_release,\n\t.getgeo\t\t= rnbd_client_getgeo\n};\n\n \nstatic size_t rnbd_clt_get_sg_size(struct scatterlist *sglist, u32 len)\n{\n\tstruct scatterlist *sg;\n\tsize_t tsize = 0;\n\tint i;\n\n\tfor_each_sg(sglist, sg, len, i)\n\t\ttsize += sg->length;\n\treturn tsize;\n}\n\nstatic int rnbd_client_xfer_request(struct rnbd_clt_dev *dev,\n\t\t\t\t     struct request *rq,\n\t\t\t\t     struct rnbd_iu *iu)\n{\n\tstruct rtrs_clt_sess *rtrs = dev->sess->rtrs;\n\tstruct rtrs_permit *permit = iu->permit;\n\tstruct rnbd_msg_io msg;\n\tstruct rtrs_clt_req_ops req_ops;\n\tunsigned int sg_cnt = 0;\n\tstruct kvec vec;\n\tsize_t size;\n\tint err;\n\n\tiu->rq\t\t= rq;\n\tiu->dev\t\t= dev;\n\tmsg.sector\t= cpu_to_le64(blk_rq_pos(rq));\n\tmsg.bi_size\t= cpu_to_le32(blk_rq_bytes(rq));\n\tmsg.rw\t\t= cpu_to_le32(rq_to_rnbd_flags(rq));\n\tmsg.prio\t= cpu_to_le16(req_get_ioprio(rq));\n\n\t \n\tif (req_op(rq) != REQ_OP_DISCARD)\n\t\tsg_cnt = blk_rq_map_sg(dev->queue, rq, iu->sgt.sgl);\n\n\tif (sg_cnt == 0)\n\t\tsg_mark_end(&iu->sgt.sgl[0]);\n\n\tmsg.hdr.type\t= cpu_to_le16(RNBD_MSG_IO);\n\tmsg.device_id\t= cpu_to_le32(dev->device_id);\n\n\tvec = (struct kvec) {\n\t\t.iov_base = &msg,\n\t\t.iov_len  = sizeof(msg)\n\t};\n\tsize = rnbd_clt_get_sg_size(iu->sgt.sgl, sg_cnt);\n\treq_ops = (struct rtrs_clt_req_ops) {\n\t\t.priv = iu,\n\t\t.conf_fn = msg_io_conf,\n\t};\n\terr = rtrs_clt_request(rq_data_dir(rq), &req_ops, rtrs, permit,\n\t\t\t       &vec, 1, size, iu->sgt.sgl, sg_cnt);\n\tif (err) {\n\t\trnbd_clt_err_rl(dev, \"RTRS failed to transfer IO, err: %d\\n\",\n\t\t\t\t err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool rnbd_clt_dev_add_to_requeue(struct rnbd_clt_dev *dev,\n\t\t\t\t\t\tstruct rnbd_queue *q)\n{\n\tstruct rnbd_clt_session *sess = dev->sess;\n\tstruct rnbd_cpu_qlist *cpu_q;\n\tunsigned long flags;\n\tbool added = true;\n\tbool need_set;\n\n\tcpu_q = get_cpu_ptr(sess->cpu_queues);\n\tspin_lock_irqsave(&cpu_q->requeue_lock, flags);\n\n\tif (!test_and_set_bit_lock(0, &q->in_list)) {\n\t\tif (WARN_ON(!list_empty(&q->requeue_list)))\n\t\t\tgoto unlock;\n\n\t\tneed_set = !test_bit(cpu_q->cpu, sess->cpu_queues_bm);\n\t\tif (need_set) {\n\t\t\tset_bit(cpu_q->cpu, sess->cpu_queues_bm);\n\t\t\t \n\t\t\tsmp_mb__before_atomic();\n\t\t}\n\t\tif (atomic_read(&sess->busy)) {\n\t\t\tlist_add_tail(&q->requeue_list, &cpu_q->requeue_list);\n\t\t} else {\n\t\t\t \n\t\t\tif (need_set)\n\t\t\t\tclear_bit(cpu_q->cpu, sess->cpu_queues_bm);\n\t\t\tclear_bit_unlock(0, &q->in_list);\n\t\t\tadded = false;\n\t\t}\n\t}\nunlock:\n\tspin_unlock_irqrestore(&cpu_q->requeue_lock, flags);\n\tput_cpu_ptr(sess->cpu_queues);\n\n\treturn added;\n}\n\nstatic void rnbd_clt_dev_kick_mq_queue(struct rnbd_clt_dev *dev,\n\t\t\t\t\tstruct blk_mq_hw_ctx *hctx,\n\t\t\t\t\tint delay)\n{\n\tstruct rnbd_queue *q = hctx->driver_data;\n\n\tif (delay != RNBD_DELAY_IFBUSY)\n\t\tblk_mq_delay_run_hw_queue(hctx, delay);\n\telse if (!rnbd_clt_dev_add_to_requeue(dev, q))\n\t\t \n\t\tblk_mq_delay_run_hw_queue(hctx, 10 );\n}\n\nstatic blk_status_t rnbd_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t   const struct blk_mq_queue_data *bd)\n{\n\tstruct request *rq = bd->rq;\n\tstruct rnbd_clt_dev *dev = rq->q->disk->private_data;\n\tstruct rnbd_iu *iu = blk_mq_rq_to_pdu(rq);\n\tint err;\n\tblk_status_t ret = BLK_STS_IOERR;\n\n\tif (dev->dev_state != DEV_STATE_MAPPED)\n\t\treturn BLK_STS_IOERR;\n\n\tiu->permit = rnbd_get_permit(dev->sess, RTRS_IO_CON,\n\t\t\t\t      RTRS_PERMIT_NOWAIT);\n\tif (!iu->permit) {\n\t\trnbd_clt_dev_kick_mq_queue(dev, hctx, RNBD_DELAY_IFBUSY);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\tiu->sgt.sgl = iu->first_sgl;\n\terr = sg_alloc_table_chained(&iu->sgt,\n\t\t\t\t      \n\t\t\t\t     blk_rq_nr_phys_segments(rq) ? : 1,\n\t\t\t\t     iu->sgt.sgl,\n\t\t\t\t     RNBD_INLINE_SG_CNT);\n\tif (err) {\n\t\trnbd_clt_err_rl(dev, \"sg_alloc_table_chained ret=%d\\n\", err);\n\t\trnbd_clt_dev_kick_mq_queue(dev, hctx, 10 );\n\t\trnbd_put_permit(dev->sess, iu->permit);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\tblk_mq_start_request(rq);\n\terr = rnbd_client_xfer_request(dev, rq, iu);\n\tif (err == 0)\n\t\treturn BLK_STS_OK;\n\tif (err == -EAGAIN || err == -ENOMEM) {\n\t\trnbd_clt_dev_kick_mq_queue(dev, hctx, 10 );\n\t\tret = BLK_STS_RESOURCE;\n\t}\n\tsg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);\n\trnbd_put_permit(dev->sess, iu->permit);\n\treturn ret;\n}\n\nstatic int rnbd_rdma_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct rnbd_queue *q = hctx->driver_data;\n\tstruct rnbd_clt_dev *dev = q->dev;\n\n\treturn rtrs_clt_rdma_cq_direct(dev->sess->rtrs, hctx->queue_num);\n}\n\nstatic void rnbd_rdma_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct rnbd_clt_session *sess = set->driver_data;\n\n\t \n\tset->map[HCTX_TYPE_DEFAULT].nr_queues = num_online_cpus();\n\tset->map[HCTX_TYPE_DEFAULT].queue_offset = 0;\n\tset->map[HCTX_TYPE_READ].nr_queues = num_online_cpus();\n\tset->map[HCTX_TYPE_READ].queue_offset = 0;\n\tblk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);\n\tblk_mq_map_queues(&set->map[HCTX_TYPE_READ]);\n\n\tif (sess->nr_poll_queues) {\n\t\t \n\t\tset->map[HCTX_TYPE_POLL].nr_queues = sess->nr_poll_queues;\n\t\tset->map[HCTX_TYPE_POLL].queue_offset = set->map[HCTX_TYPE_READ].queue_offset +\n\t\t\tset->map[HCTX_TYPE_READ].nr_queues;\n\t\tblk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);\n\t\tpr_info(\"[session=%s] mapped %d/%d/%d default/read/poll queues.\\n\",\n\t\t\tsess->sessname,\n\t\t\tset->map[HCTX_TYPE_DEFAULT].nr_queues,\n\t\t\tset->map[HCTX_TYPE_READ].nr_queues,\n\t\t\tset->map[HCTX_TYPE_POLL].nr_queues);\n\t} else {\n\t\tpr_info(\"[session=%s] mapped %d/%d default/read queues.\\n\",\n\t\t\tsess->sessname,\n\t\t\tset->map[HCTX_TYPE_DEFAULT].nr_queues,\n\t\t\tset->map[HCTX_TYPE_READ].nr_queues);\n\t}\n}\n\nstatic struct blk_mq_ops rnbd_mq_ops = {\n\t.queue_rq\t= rnbd_queue_rq,\n\t.complete\t= rnbd_softirq_done_fn,\n\t.map_queues     = rnbd_rdma_map_queues,\n\t.poll           = rnbd_rdma_poll,\n};\n\nstatic int setup_mq_tags(struct rnbd_clt_session *sess)\n{\n\tstruct blk_mq_tag_set *tag_set = &sess->tag_set;\n\n\tmemset(tag_set, 0, sizeof(*tag_set));\n\ttag_set->ops\t\t= &rnbd_mq_ops;\n\ttag_set->queue_depth\t= sess->queue_depth;\n\ttag_set->numa_node\t\t= NUMA_NO_NODE;\n\ttag_set->flags\t\t= BLK_MQ_F_SHOULD_MERGE |\n\t\t\t\t  BLK_MQ_F_TAG_QUEUE_SHARED;\n\ttag_set->cmd_size\t= sizeof(struct rnbd_iu) + RNBD_RDMA_SGL_SIZE;\n\n\t \n\ttag_set->nr_maps        = sess->nr_poll_queues ? HCTX_MAX_TYPES : 2;\n\t \n\ttag_set->nr_hw_queues\t= num_online_cpus() + sess->nr_poll_queues;\n\ttag_set->driver_data    = sess;\n\n\treturn blk_mq_alloc_tag_set(tag_set);\n}\n\nstatic struct rnbd_clt_session *\nfind_and_get_or_create_sess(const char *sessname,\n\t\t\t    const struct rtrs_addr *paths,\n\t\t\t    size_t path_cnt, u16 port_nr, u32 nr_poll_queues)\n{\n\tstruct rnbd_clt_session *sess;\n\tstruct rtrs_attrs attrs;\n\tint err;\n\tbool first = false;\n\tstruct rtrs_clt_ops rtrs_ops;\n\n\tsess = find_or_create_sess(sessname, &first);\n\tif (sess == ERR_PTR(-ENOMEM)) {\n\t\treturn ERR_PTR(-ENOMEM);\n\t} else if ((nr_poll_queues && !first) ||  (!nr_poll_queues && sess->nr_poll_queues)) {\n\t\t \n\t\terr = -EINVAL;\n\t\tgoto put_sess;\n\t}\n\n\tif (!first)\n\t\treturn sess;\n\n\tif (!path_cnt) {\n\t\tpr_err(\"Session %s not found, and path parameter not given\", sessname);\n\t\terr = -ENXIO;\n\t\tgoto put_sess;\n\t}\n\n\trtrs_ops = (struct rtrs_clt_ops) {\n\t\t.priv = sess,\n\t\t.link_ev = rnbd_clt_link_ev,\n\t};\n\t \n\tsess->rtrs = rtrs_clt_open(&rtrs_ops, sessname,\n\t\t\t\t   paths, path_cnt, port_nr,\n\t\t\t\t   0,  \n\t\t\t\t   RECONNECT_DELAY,\n\t\t\t\t   MAX_RECONNECTS, nr_poll_queues);\n\tif (IS_ERR(sess->rtrs)) {\n\t\terr = PTR_ERR(sess->rtrs);\n\t\tgoto wake_up_and_put;\n\t}\n\n\terr = rtrs_clt_query(sess->rtrs, &attrs);\n\tif (err)\n\t\tgoto close_rtrs;\n\n\tsess->max_io_size = attrs.max_io_size;\n\tsess->queue_depth = attrs.queue_depth;\n\tsess->nr_poll_queues = nr_poll_queues;\n\tsess->max_segments = attrs.max_segments;\n\n\terr = setup_mq_tags(sess);\n\tif (err)\n\t\tgoto close_rtrs;\n\n\terr = send_msg_sess_info(sess, RTRS_PERMIT_WAIT);\n\tif (err)\n\t\tgoto close_rtrs;\n\n\twake_up_rtrs_waiters(sess);\n\n\treturn sess;\n\nclose_rtrs:\n\tclose_rtrs(sess);\nput_sess:\n\trnbd_clt_put_sess(sess);\n\n\treturn ERR_PTR(err);\n\nwake_up_and_put:\n\twake_up_rtrs_waiters(sess);\n\tgoto put_sess;\n}\n\nstatic inline void rnbd_init_hw_queue(struct rnbd_clt_dev *dev,\n\t\t\t\t       struct rnbd_queue *q,\n\t\t\t\t       struct blk_mq_hw_ctx *hctx)\n{\n\tINIT_LIST_HEAD(&q->requeue_list);\n\tq->dev  = dev;\n\tq->hctx = hctx;\n}\n\nstatic void rnbd_init_mq_hw_queues(struct rnbd_clt_dev *dev)\n{\n\tunsigned long i;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct rnbd_queue *q;\n\n\tqueue_for_each_hw_ctx(dev->queue, hctx, i) {\n\t\tq = &dev->hw_queues[i];\n\t\trnbd_init_hw_queue(dev, q, hctx);\n\t\thctx->driver_data = q;\n\t}\n}\n\nstatic void setup_request_queue(struct rnbd_clt_dev *dev,\n\t\t\t\tstruct rnbd_msg_open_rsp *rsp)\n{\n\tblk_queue_logical_block_size(dev->queue,\n\t\t\t\t     le16_to_cpu(rsp->logical_block_size));\n\tblk_queue_physical_block_size(dev->queue,\n\t\t\t\t      le16_to_cpu(rsp->physical_block_size));\n\tblk_queue_max_hw_sectors(dev->queue,\n\t\t\t\t dev->sess->max_io_size / SECTOR_SIZE);\n\n\t \n\tblk_queue_max_discard_segments(dev->queue, 1);\n\n\tblk_queue_max_discard_sectors(dev->queue,\n\t\t\t\t      le32_to_cpu(rsp->max_discard_sectors));\n\tdev->queue->limits.discard_granularity =\n\t\t\t\t\tle32_to_cpu(rsp->discard_granularity);\n\tdev->queue->limits.discard_alignment =\n\t\t\t\t\tle32_to_cpu(rsp->discard_alignment);\n\tif (le16_to_cpu(rsp->secure_discard))\n\t\tblk_queue_max_secure_erase_sectors(dev->queue,\n\t\t\t\t\tle32_to_cpu(rsp->max_discard_sectors));\n\tblk_queue_flag_set(QUEUE_FLAG_SAME_COMP, dev->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_SAME_FORCE, dev->queue);\n\tblk_queue_max_segments(dev->queue, dev->sess->max_segments);\n\tblk_queue_io_opt(dev->queue, dev->sess->max_io_size);\n\tblk_queue_virt_boundary(dev->queue, SZ_4K - 1);\n\tblk_queue_write_cache(dev->queue,\n\t\t\t      !!(rsp->cache_policy & RNBD_WRITEBACK),\n\t\t\t      !!(rsp->cache_policy & RNBD_FUA));\n}\n\nstatic int rnbd_clt_setup_gen_disk(struct rnbd_clt_dev *dev,\n\t\t\t\t   struct rnbd_msg_open_rsp *rsp, int idx)\n{\n\tint err;\n\n\tdev->gd->major\t\t= rnbd_client_major;\n\tdev->gd->first_minor\t= idx << RNBD_PART_BITS;\n\tdev->gd->minors\t\t= 1 << RNBD_PART_BITS;\n\tdev->gd->fops\t\t= &rnbd_client_ops;\n\tdev->gd->queue\t\t= dev->queue;\n\tdev->gd->private_data\t= dev;\n\tsnprintf(dev->gd->disk_name, sizeof(dev->gd->disk_name), \"rnbd%d\",\n\t\t idx);\n\tpr_debug(\"disk_name=%s, capacity=%llu\\n\",\n\t\t dev->gd->disk_name,\n\t\t le64_to_cpu(rsp->nsectors) *\n\t\t (le16_to_cpu(rsp->logical_block_size) / SECTOR_SIZE));\n\n\tset_capacity(dev->gd, le64_to_cpu(rsp->nsectors));\n\n\tif (dev->access_mode == RNBD_ACCESS_RO)\n\t\tset_disk_ro(dev->gd, true);\n\n\t \n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, dev->queue);\n\terr = add_disk(dev->gd);\n\tif (err)\n\t\tput_disk(dev->gd);\n\n\treturn err;\n}\n\nstatic int rnbd_client_setup_device(struct rnbd_clt_dev *dev,\n\t\t\t\t    struct rnbd_msg_open_rsp *rsp)\n{\n\tint idx = dev->clt_device_id;\n\n\tdev->size = le64_to_cpu(rsp->nsectors) *\n\t\t\tle16_to_cpu(rsp->logical_block_size);\n\n\tdev->gd = blk_mq_alloc_disk(&dev->sess->tag_set, dev);\n\tif (IS_ERR(dev->gd))\n\t\treturn PTR_ERR(dev->gd);\n\tdev->queue = dev->gd->queue;\n\trnbd_init_mq_hw_queues(dev);\n\n\tsetup_request_queue(dev, rsp);\n\treturn rnbd_clt_setup_gen_disk(dev, rsp, idx);\n}\n\nstatic struct rnbd_clt_dev *init_dev(struct rnbd_clt_session *sess,\n\t\t\t\t      enum rnbd_access_mode access_mode,\n\t\t\t\t      const char *pathname,\n\t\t\t\t      u32 nr_poll_queues)\n{\n\tstruct rnbd_clt_dev *dev;\n\tint ret;\n\n\tdev = kzalloc_node(sizeof(*dev), GFP_KERNEL, NUMA_NO_NODE);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tdev->hw_queues = kcalloc(nr_cpu_ids + nr_poll_queues,\n\t\t\t\t sizeof(*dev->hw_queues),\n\t\t\t\t GFP_KERNEL);\n\tif (!dev->hw_queues) {\n\t\tret = -ENOMEM;\n\t\tgoto out_alloc;\n\t}\n\n\tret = ida_alloc_max(&index_ida, (1 << (MINORBITS - RNBD_PART_BITS)) - 1,\n\t\t\t    GFP_KERNEL);\n\tif (ret < 0) {\n\t\tpr_err(\"Failed to initialize device '%s' from session %s, allocating idr failed, err: %d\\n\",\n\t\t       pathname, sess->sessname, ret);\n\t\tgoto out_queues;\n\t}\n\n\tdev->pathname = kstrdup(pathname, GFP_KERNEL);\n\tif (!dev->pathname) {\n\t\tret = -ENOMEM;\n\t\tgoto out_queues;\n\t}\n\n\tdev->clt_device_id\t= ret;\n\tdev->sess\t\t= sess;\n\tdev->access_mode\t= access_mode;\n\tdev->nr_poll_queues\t= nr_poll_queues;\n\tmutex_init(&dev->lock);\n\trefcount_set(&dev->refcount, 1);\n\tdev->dev_state = DEV_STATE_INIT;\n\n\t \n\tWARN_ON(!rnbd_clt_get_sess(sess));\n\n\treturn dev;\n\nout_queues:\n\tkfree(dev->hw_queues);\nout_alloc:\n\tkfree(dev);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool __exists_dev(const char *pathname, const char *sessname)\n{\n\tstruct rnbd_clt_session *sess;\n\tstruct rnbd_clt_dev *dev;\n\tbool found = false;\n\n\tlist_for_each_entry(sess, &sess_list, list) {\n\t\tif (sessname && strncmp(sess->sessname, sessname,\n\t\t\t\t\tsizeof(sess->sessname)))\n\t\t\tcontinue;\n\t\tmutex_lock(&sess->lock);\n\t\tlist_for_each_entry(dev, &sess->devs_list, list) {\n\t\t\tif (strlen(dev->pathname) == strlen(pathname) &&\n\t\t\t    !strcmp(dev->pathname, pathname)) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&sess->lock);\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\n\treturn found;\n}\n\nstatic bool exists_devpath(const char *pathname, const char *sessname)\n{\n\tbool found;\n\n\tmutex_lock(&sess_lock);\n\tfound = __exists_dev(pathname, sessname);\n\tmutex_unlock(&sess_lock);\n\n\treturn found;\n}\n\nstatic bool insert_dev_if_not_exists_devpath(struct rnbd_clt_dev *dev)\n{\n\tbool found;\n\tstruct rnbd_clt_session *sess = dev->sess;\n\n\tmutex_lock(&sess_lock);\n\tfound = __exists_dev(dev->pathname, sess->sessname);\n\tif (!found) {\n\t\tmutex_lock(&sess->lock);\n\t\tlist_add_tail(&dev->list, &sess->devs_list);\n\t\tmutex_unlock(&sess->lock);\n\t}\n\tmutex_unlock(&sess_lock);\n\n\treturn found;\n}\n\nstatic void delete_dev(struct rnbd_clt_dev *dev)\n{\n\tstruct rnbd_clt_session *sess = dev->sess;\n\n\tmutex_lock(&sess->lock);\n\tlist_del(&dev->list);\n\tmutex_unlock(&sess->lock);\n}\n\nstruct rnbd_clt_dev *rnbd_clt_map_device(const char *sessname,\n\t\t\t\t\t   struct rtrs_addr *paths,\n\t\t\t\t\t   size_t path_cnt, u16 port_nr,\n\t\t\t\t\t   const char *pathname,\n\t\t\t\t\t   enum rnbd_access_mode access_mode,\n\t\t\t\t\t   u32 nr_poll_queues)\n{\n\tstruct rnbd_clt_session *sess;\n\tstruct rnbd_clt_dev *dev;\n\tint ret, errno;\n\tstruct rnbd_msg_open_rsp *rsp;\n\tstruct rnbd_msg_open msg;\n\tstruct rnbd_iu *iu;\n\tstruct kvec vec = {\n\t\t.iov_base = &msg,\n\t\t.iov_len  = sizeof(msg)\n\t};\n\n\tif (exists_devpath(pathname, sessname))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tsess = find_and_get_or_create_sess(sessname, paths, path_cnt, port_nr, nr_poll_queues);\n\tif (IS_ERR(sess))\n\t\treturn ERR_CAST(sess);\n\n\tdev = init_dev(sess, access_mode, pathname, nr_poll_queues);\n\tif (IS_ERR(dev)) {\n\t\tpr_err(\"map_device: failed to map device '%s' from session %s, can't initialize device, err: %ld\\n\",\n\t\t       pathname, sess->sessname, PTR_ERR(dev));\n\t\tret = PTR_ERR(dev);\n\t\tgoto put_sess;\n\t}\n\tif (insert_dev_if_not_exists_devpath(dev)) {\n\t\tret = -EEXIST;\n\t\tgoto put_dev;\n\t}\n\n\trsp = kzalloc(sizeof(*rsp), GFP_KERNEL);\n\tif (!rsp) {\n\t\tret = -ENOMEM;\n\t\tgoto del_dev;\n\t}\n\n\tiu = rnbd_get_iu(sess, RTRS_ADMIN_CON, RTRS_PERMIT_WAIT);\n\tif (!iu) {\n\t\tret = -ENOMEM;\n\t\tkfree(rsp);\n\t\tgoto del_dev;\n\t}\n\tiu->buf = rsp;\n\tiu->dev = dev;\n\tsg_init_one(iu->sgt.sgl, rsp, sizeof(*rsp));\n\n\tmsg.hdr.type    = cpu_to_le16(RNBD_MSG_OPEN);\n\tmsg.access_mode = dev->access_mode;\n\tstrscpy(msg.dev_name, dev->pathname, sizeof(msg.dev_name));\n\n\tWARN_ON(!rnbd_clt_get_dev(dev));\n\tret = send_usr_msg(sess->rtrs, READ, iu,\n\t\t\t   &vec, sizeof(*rsp), iu->sgt.sgl, 1,\n\t\t\t   msg_open_conf, &errno, RTRS_PERMIT_WAIT);\n\tif (ret) {\n\t\trnbd_clt_put_dev(dev);\n\t\trnbd_put_iu(sess, iu);\n\t} else {\n\t\tret = errno;\n\t}\n\tif (ret) {\n\t\trnbd_clt_err(dev,\n\t\t\t      \"map_device: failed, can't open remote device, err: %d\\n\",\n\t\t\t      ret);\n\t\tgoto put_iu;\n\t}\n\tmutex_lock(&dev->lock);\n\tpr_debug(\"Opened remote device: session=%s, path='%s'\\n\",\n\t\t sess->sessname, pathname);\n\tret = rnbd_client_setup_device(dev, rsp);\n\tif (ret) {\n\t\trnbd_clt_err(dev,\n\t\t\t      \"map_device: Failed to configure device, err: %d\\n\",\n\t\t\t      ret);\n\t\tmutex_unlock(&dev->lock);\n\t\tgoto send_close;\n\t}\n\n\trnbd_clt_info(dev,\n\t\t       \"map_device: Device mapped as %s (nsectors: %llu, logical_block_size: %d, physical_block_size: %d, max_discard_sectors: %d, discard_granularity: %d, discard_alignment: %d, secure_discard: %d, max_segments: %d, max_hw_sectors: %d, wc: %d, fua: %d)\\n\",\n\t\t       dev->gd->disk_name, le64_to_cpu(rsp->nsectors),\n\t\t       le16_to_cpu(rsp->logical_block_size),\n\t\t       le16_to_cpu(rsp->physical_block_size),\n\t\t       le32_to_cpu(rsp->max_discard_sectors),\n\t\t       le32_to_cpu(rsp->discard_granularity),\n\t\t       le32_to_cpu(rsp->discard_alignment),\n\t\t       le16_to_cpu(rsp->secure_discard),\n\t\t       sess->max_segments, sess->max_io_size / SECTOR_SIZE,\n\t\t       !!(rsp->cache_policy & RNBD_WRITEBACK),\n\t\t       !!(rsp->cache_policy & RNBD_FUA));\n\n\tmutex_unlock(&dev->lock);\n\tkfree(rsp);\n\trnbd_put_iu(sess, iu);\n\trnbd_clt_put_sess(sess);\n\n\treturn dev;\n\nsend_close:\n\tsend_msg_close(dev, dev->device_id, RTRS_PERMIT_WAIT);\nput_iu:\n\tkfree(rsp);\n\trnbd_put_iu(sess, iu);\ndel_dev:\n\tdelete_dev(dev);\nput_dev:\n\trnbd_clt_put_dev(dev);\nput_sess:\n\trnbd_clt_put_sess(sess);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic void destroy_gen_disk(struct rnbd_clt_dev *dev)\n{\n\tdel_gendisk(dev->gd);\n\tput_disk(dev->gd);\n}\n\nstatic void destroy_sysfs(struct rnbd_clt_dev *dev,\n\t\t\t  const struct attribute *sysfs_self)\n{\n\trnbd_clt_remove_dev_symlink(dev);\n\tif (dev->kobj.state_initialized) {\n\t\tif (sysfs_self)\n\t\t\t \n\t\t\tsysfs_remove_file_self(&dev->kobj, sysfs_self);\n\t\tkobject_del(&dev->kobj);\n\t\tkobject_put(&dev->kobj);\n\t}\n}\n\nint rnbd_clt_unmap_device(struct rnbd_clt_dev *dev, bool force,\n\t\t\t   const struct attribute *sysfs_self)\n{\n\tstruct rnbd_clt_session *sess = dev->sess;\n\tint refcount, ret = 0;\n\tbool was_mapped;\n\n\tmutex_lock(&dev->lock);\n\tif (dev->dev_state == DEV_STATE_UNMAPPED) {\n\t\trnbd_clt_info(dev, \"Device is already being unmapped\\n\");\n\t\tret = -EALREADY;\n\t\tgoto err;\n\t}\n\trefcount = refcount_read(&dev->refcount);\n\tif (!force && refcount > 1) {\n\t\trnbd_clt_err(dev,\n\t\t\t      \"Closing device failed, device is in use, (%d device users)\\n\",\n\t\t\t      refcount - 1);\n\t\tret = -EBUSY;\n\t\tgoto err;\n\t}\n\twas_mapped = (dev->dev_state == DEV_STATE_MAPPED);\n\tdev->dev_state = DEV_STATE_UNMAPPED;\n\tmutex_unlock(&dev->lock);\n\n\tdelete_dev(dev);\n\tdestroy_sysfs(dev, sysfs_self);\n\tdestroy_gen_disk(dev);\n\tif (was_mapped && sess->rtrs)\n\t\tsend_msg_close(dev, dev->device_id, RTRS_PERMIT_WAIT);\n\n\trnbd_clt_info(dev, \"Device is unmapped\\n\");\n\n\t \n\trnbd_clt_put_dev(dev);\n\n\t \n\n\treturn 0;\nerr:\n\tmutex_unlock(&dev->lock);\n\n\treturn ret;\n}\n\nint rnbd_clt_remap_device(struct rnbd_clt_dev *dev)\n{\n\tint err;\n\n\tmutex_lock(&dev->lock);\n\tif (dev->dev_state == DEV_STATE_MAPPED_DISCONNECTED)\n\t\terr = 0;\n\telse if (dev->dev_state == DEV_STATE_UNMAPPED)\n\t\terr = -ENODEV;\n\telse if (dev->dev_state == DEV_STATE_MAPPED)\n\t\terr = -EALREADY;\n\telse\n\t\terr = -EBUSY;\n\tmutex_unlock(&dev->lock);\n\tif (!err) {\n\t\trnbd_clt_info(dev, \"Remapping device.\\n\");\n\t\terr = send_msg_open(dev, RTRS_PERMIT_WAIT);\n\t\tif (err)\n\t\t\trnbd_clt_err(dev, \"remap_device: %d\\n\", err);\n\t}\n\n\treturn err;\n}\n\nstatic void unmap_device_work(struct work_struct *work)\n{\n\tstruct rnbd_clt_dev *dev;\n\n\tdev = container_of(work, typeof(*dev), unmap_on_rmmod_work);\n\trnbd_clt_unmap_device(dev, true, NULL);\n}\n\nstatic void rnbd_destroy_sessions(void)\n{\n\tstruct rnbd_clt_session *sess, *sn;\n\tstruct rnbd_clt_dev *dev, *tn;\n\n\t \n\trnbd_clt_destroy_sysfs_files();\n\n\t \n\n\tlist_for_each_entry_safe(sess, sn, &sess_list, list) {\n\t\tif (!rnbd_clt_get_sess(sess))\n\t\t\tcontinue;\n\t\tclose_rtrs(sess);\n\t\tlist_for_each_entry_safe(dev, tn, &sess->devs_list, list) {\n\t\t\t \n\t\t\tINIT_WORK(&dev->unmap_on_rmmod_work, unmap_device_work);\n\t\t\tqueue_work(rnbd_clt_wq, &dev->unmap_on_rmmod_work);\n\t\t}\n\t\trnbd_clt_put_sess(sess);\n\t}\n\t \n\tflush_workqueue(rnbd_clt_wq);\n\tWARN_ON(!list_empty(&sess_list));\n}\n\nstatic int __init rnbd_client_init(void)\n{\n\tint err = 0;\n\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_hdr) != 4);\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_sess_info) != 36);\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_sess_info_rsp) != 36);\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_open) != 264);\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_close) != 8);\n\tBUILD_BUG_ON(sizeof(struct rnbd_msg_open_rsp) != 56);\n\trnbd_client_major = register_blkdev(rnbd_client_major, \"rnbd\");\n\tif (rnbd_client_major <= 0) {\n\t\tpr_err(\"Failed to load module, block device registration failed\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\terr = rnbd_clt_create_sysfs_files();\n\tif (err) {\n\t\tpr_err(\"Failed to load module, creating sysfs device files failed, err: %d\\n\",\n\t\t       err);\n\t\tunregister_blkdev(rnbd_client_major, \"rnbd\");\n\t\treturn err;\n\t}\n\trnbd_clt_wq = alloc_workqueue(\"rnbd_clt_wq\", 0, 0);\n\tif (!rnbd_clt_wq) {\n\t\tpr_err(\"Failed to load module, alloc_workqueue failed.\\n\");\n\t\trnbd_clt_destroy_sysfs_files();\n\t\tunregister_blkdev(rnbd_client_major, \"rnbd\");\n\t\terr = -ENOMEM;\n\t}\n\n\treturn err;\n}\n\nstatic void __exit rnbd_client_exit(void)\n{\n\trnbd_destroy_sessions();\n\tunregister_blkdev(rnbd_client_major, \"rnbd\");\n\tida_destroy(&index_ida);\n\tdestroy_workqueue(rnbd_clt_wq);\n}\n\nmodule_init(rnbd_client_init);\nmodule_exit(rnbd_client_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}