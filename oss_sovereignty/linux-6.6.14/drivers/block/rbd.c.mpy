{
  "module_name": "rbd.c",
  "hash_id": "9520e7eddf6b624e024c09a6e88d72817e93a66809509ef5c47b3f5d9533f383",
  "original_prompt": "Ingested from linux-6.6.14/drivers/block/rbd.c",
  "human_readable_source": "\n \n\n#include <linux/ceph/libceph.h>\n#include <linux/ceph/osd_client.h>\n#include <linux/ceph/mon_client.h>\n#include <linux/ceph/cls_lock_client.h>\n#include <linux/ceph/striper.h>\n#include <linux/ceph/decode.h>\n#include <linux/fs_parser.h>\n#include <linux/bsearch.h>\n\n#include <linux/kernel.h>\n#include <linux/device.h>\n#include <linux/module.h>\n#include <linux/blk-mq.h>\n#include <linux/fs.h>\n#include <linux/blkdev.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/workqueue.h>\n\n#include \"rbd_types.h\"\n\n#define RBD_DEBUG\t \n\n \nstatic int atomic_inc_return_safe(atomic_t *v)\n{\n\tunsigned int counter;\n\n\tcounter = (unsigned int)atomic_fetch_add_unless(v, 1, 0);\n\tif (counter <= (unsigned int)INT_MAX)\n\t\treturn (int)counter;\n\n\tatomic_dec(v);\n\n\treturn -EINVAL;\n}\n\n \nstatic int atomic_dec_return_safe(atomic_t *v)\n{\n\tint counter;\n\n\tcounter = atomic_dec_return(v);\n\tif (counter >= 0)\n\t\treturn counter;\n\n\tatomic_inc(v);\n\n\treturn -EINVAL;\n}\n\n#define RBD_DRV_NAME \"rbd\"\n\n#define RBD_MINORS_PER_MAJOR\t\t256\n#define RBD_SINGLE_MAJOR_PART_SHIFT\t4\n\n#define RBD_MAX_PARENT_CHAIN_LEN\t16\n\n#define RBD_SNAP_DEV_NAME_PREFIX\t\"snap_\"\n#define RBD_MAX_SNAP_NAME_LEN\t\\\n\t\t\t(NAME_MAX - (sizeof (RBD_SNAP_DEV_NAME_PREFIX) - 1))\n\n#define RBD_MAX_SNAP_COUNT\t510\t \n\n#define RBD_SNAP_HEAD_NAME\t\"-\"\n\n#define\tBAD_SNAP_INDEX\tU32_MAX\t\t \n\n \n#define RBD_IMAGE_NAME_LEN_MAX\t(PAGE_SIZE - sizeof (__le32) - 1)\n#define RBD_IMAGE_ID_LEN_MAX\t64\n\n#define RBD_OBJ_PREFIX_LEN_MAX\t64\n\n#define RBD_NOTIFY_TIMEOUT\t5\t \n#define RBD_RETRY_DELAY\t\tmsecs_to_jiffies(1000)\n\n \n\n#define RBD_FEATURE_LAYERING\t\t(1ULL<<0)\n#define RBD_FEATURE_STRIPINGV2\t\t(1ULL<<1)\n#define RBD_FEATURE_EXCLUSIVE_LOCK\t(1ULL<<2)\n#define RBD_FEATURE_OBJECT_MAP\t\t(1ULL<<3)\n#define RBD_FEATURE_FAST_DIFF\t\t(1ULL<<4)\n#define RBD_FEATURE_DEEP_FLATTEN\t(1ULL<<5)\n#define RBD_FEATURE_DATA_POOL\t\t(1ULL<<7)\n#define RBD_FEATURE_OPERATIONS\t\t(1ULL<<8)\n\n#define RBD_FEATURES_ALL\t(RBD_FEATURE_LAYERING |\t\t\\\n\t\t\t\t RBD_FEATURE_STRIPINGV2 |\t\\\n\t\t\t\t RBD_FEATURE_EXCLUSIVE_LOCK |\t\\\n\t\t\t\t RBD_FEATURE_OBJECT_MAP |\t\\\n\t\t\t\t RBD_FEATURE_FAST_DIFF |\t\\\n\t\t\t\t RBD_FEATURE_DEEP_FLATTEN |\t\\\n\t\t\t\t RBD_FEATURE_DATA_POOL |\t\\\n\t\t\t\t RBD_FEATURE_OPERATIONS)\n\n \n\n#define RBD_FEATURES_SUPPORTED\t(RBD_FEATURES_ALL)\n\n \n#define DEV_NAME_LEN\t\t32\n\n \nstruct rbd_image_header {\n\t \n\tchar *object_prefix;\n\t__u8 obj_order;\n\tu64 stripe_unit;\n\tu64 stripe_count;\n\ts64 data_pool_id;\n\tu64 features;\t\t \n\n\t \n\tu64 image_size;\n\tstruct ceph_snap_context *snapc;\n\tchar *snap_names;\t \n\tu64 *snap_sizes;\t \n};\n\n \nstruct rbd_spec {\n\tu64\t\tpool_id;\n\tconst char\t*pool_name;\n\tconst char\t*pool_ns;\t \n\n\tconst char\t*image_id;\n\tconst char\t*image_name;\n\n\tu64\t\tsnap_id;\n\tconst char\t*snap_name;\n\n\tstruct kref\tkref;\n};\n\n \nstruct rbd_client {\n\tstruct ceph_client\t*client;\n\tstruct kref\t\tkref;\n\tstruct list_head\tnode;\n};\n\nstruct pending_result {\n\tint\t\t\tresult;\t\t \n\tint\t\t\tnum_pending;\n};\n\nstruct rbd_img_request;\n\nenum obj_request_type {\n\tOBJ_REQUEST_NODATA = 1,\n\tOBJ_REQUEST_BIO,\t \n\tOBJ_REQUEST_BVECS,\t \n\tOBJ_REQUEST_OWN_BVECS,\t \n};\n\nenum obj_operation_type {\n\tOBJ_OP_READ = 1,\n\tOBJ_OP_WRITE,\n\tOBJ_OP_DISCARD,\n\tOBJ_OP_ZEROOUT,\n};\n\n#define RBD_OBJ_FLAG_DELETION\t\t\t(1U << 0)\n#define RBD_OBJ_FLAG_COPYUP_ENABLED\t\t(1U << 1)\n#define RBD_OBJ_FLAG_COPYUP_ZEROS\t\t(1U << 2)\n#define RBD_OBJ_FLAG_MAY_EXIST\t\t\t(1U << 3)\n#define RBD_OBJ_FLAG_NOOP_FOR_NONEXISTENT\t(1U << 4)\n\nenum rbd_obj_read_state {\n\tRBD_OBJ_READ_START = 1,\n\tRBD_OBJ_READ_OBJECT,\n\tRBD_OBJ_READ_PARENT,\n};\n\n \nenum rbd_obj_write_state {\n\tRBD_OBJ_WRITE_START = 1,\n\tRBD_OBJ_WRITE_PRE_OBJECT_MAP,\n\tRBD_OBJ_WRITE_OBJECT,\n\t__RBD_OBJ_WRITE_COPYUP,\n\tRBD_OBJ_WRITE_COPYUP,\n\tRBD_OBJ_WRITE_POST_OBJECT_MAP,\n};\n\nenum rbd_obj_copyup_state {\n\tRBD_OBJ_COPYUP_START = 1,\n\tRBD_OBJ_COPYUP_READ_PARENT,\n\t__RBD_OBJ_COPYUP_OBJECT_MAPS,\n\tRBD_OBJ_COPYUP_OBJECT_MAPS,\n\t__RBD_OBJ_COPYUP_WRITE_OBJECT,\n\tRBD_OBJ_COPYUP_WRITE_OBJECT,\n};\n\nstruct rbd_obj_request {\n\tstruct ceph_object_extent ex;\n\tunsigned int\t\tflags;\t \n\tunion {\n\t\tenum rbd_obj_read_state\t read_state;\t \n\t\tenum rbd_obj_write_state write_state;\t \n\t};\n\n\tstruct rbd_img_request\t*img_request;\n\tstruct ceph_file_extent\t*img_extents;\n\tu32\t\t\tnum_img_extents;\n\n\tunion {\n\t\tstruct ceph_bio_iter\tbio_pos;\n\t\tstruct {\n\t\t\tstruct ceph_bvec_iter\tbvec_pos;\n\t\t\tu32\t\t\tbvec_count;\n\t\t\tu32\t\t\tbvec_idx;\n\t\t};\n\t};\n\n\tenum rbd_obj_copyup_state copyup_state;\n\tstruct bio_vec\t\t*copyup_bvecs;\n\tu32\t\t\tcopyup_bvec_count;\n\n\tstruct list_head\tosd_reqs;\t \n\n\tstruct mutex\t\tstate_mutex;\n\tstruct pending_result\tpending;\n\tstruct kref\t\tkref;\n};\n\nenum img_req_flags {\n\tIMG_REQ_CHILD,\t\t \n\tIMG_REQ_LAYERED,\t \n};\n\nenum rbd_img_state {\n\tRBD_IMG_START = 1,\n\tRBD_IMG_EXCLUSIVE_LOCK,\n\t__RBD_IMG_OBJECT_REQUESTS,\n\tRBD_IMG_OBJECT_REQUESTS,\n};\n\nstruct rbd_img_request {\n\tstruct rbd_device\t*rbd_dev;\n\tenum obj_operation_type\top_type;\n\tenum obj_request_type\tdata_type;\n\tunsigned long\t\tflags;\n\tenum rbd_img_state\tstate;\n\tunion {\n\t\tu64\t\t\tsnap_id;\t \n\t\tstruct ceph_snap_context *snapc;\t \n\t};\n\tstruct rbd_obj_request\t*obj_request;\t \n\n\tstruct list_head\tlock_item;\n\tstruct list_head\tobject_extents;\t \n\n\tstruct mutex\t\tstate_mutex;\n\tstruct pending_result\tpending;\n\tstruct work_struct\twork;\n\tint\t\t\twork_result;\n};\n\n#define for_each_obj_request(ireq, oreq) \\\n\tlist_for_each_entry(oreq, &(ireq)->object_extents, ex.oe_item)\n#define for_each_obj_request_safe(ireq, oreq, n) \\\n\tlist_for_each_entry_safe(oreq, n, &(ireq)->object_extents, ex.oe_item)\n\nenum rbd_watch_state {\n\tRBD_WATCH_STATE_UNREGISTERED,\n\tRBD_WATCH_STATE_REGISTERED,\n\tRBD_WATCH_STATE_ERROR,\n};\n\nenum rbd_lock_state {\n\tRBD_LOCK_STATE_UNLOCKED,\n\tRBD_LOCK_STATE_LOCKED,\n\tRBD_LOCK_STATE_RELEASING,\n};\n\n \nstruct rbd_client_id {\n\tu64 gid;\n\tu64 handle;\n};\n\nstruct rbd_mapping {\n\tu64                     size;\n};\n\n \nstruct rbd_device {\n\tint\t\t\tdev_id;\t\t \n\n\tint\t\t\tmajor;\t\t \n\tint\t\t\tminor;\n\tstruct gendisk\t\t*disk;\t\t \n\n\tu32\t\t\timage_format;\t \n\tstruct rbd_client\t*rbd_client;\n\n\tchar\t\t\tname[DEV_NAME_LEN];  \n\n\tspinlock_t\t\tlock;\t\t \n\n\tstruct rbd_image_header\theader;\n\tunsigned long\t\tflags;\t\t \n\tstruct rbd_spec\t\t*spec;\n\tstruct rbd_options\t*opts;\n\tchar\t\t\t*config_info;\t \n\n\tstruct ceph_object_id\theader_oid;\n\tstruct ceph_object_locator header_oloc;\n\n\tstruct ceph_file_layout\tlayout;\t\t \n\n\tstruct mutex\t\twatch_mutex;\n\tenum rbd_watch_state\twatch_state;\n\tstruct ceph_osd_linger_request *watch_handle;\n\tu64\t\t\twatch_cookie;\n\tstruct delayed_work\twatch_dwork;\n\n\tstruct rw_semaphore\tlock_rwsem;\n\tenum rbd_lock_state\tlock_state;\n\tchar\t\t\tlock_cookie[32];\n\tstruct rbd_client_id\towner_cid;\n\tstruct work_struct\tacquired_lock_work;\n\tstruct work_struct\treleased_lock_work;\n\tstruct delayed_work\tlock_dwork;\n\tstruct work_struct\tunlock_work;\n\tspinlock_t\t\tlock_lists_lock;\n\tstruct list_head\tacquiring_list;\n\tstruct list_head\trunning_list;\n\tstruct completion\tacquire_wait;\n\tint\t\t\tacquire_err;\n\tstruct completion\treleasing_wait;\n\n\tspinlock_t\t\tobject_map_lock;\n\tu8\t\t\t*object_map;\n\tu64\t\t\tobject_map_size;\t \n\tu64\t\t\tobject_map_flags;\n\n\tstruct workqueue_struct\t*task_wq;\n\n\tstruct rbd_spec\t\t*parent_spec;\n\tu64\t\t\tparent_overlap;\n\tatomic_t\t\tparent_ref;\n\tstruct rbd_device\t*parent;\n\n\t \n\tstruct blk_mq_tag_set\ttag_set;\n\n\t \n\tstruct rw_semaphore     header_rwsem;\n\n\tstruct rbd_mapping\tmapping;\n\n\tstruct list_head\tnode;\n\n\t \n\tstruct device\t\tdev;\n\tunsigned long\t\topen_count;\t \n};\n\n \nenum rbd_dev_flags {\n\tRBD_DEV_FLAG_EXISTS,\t \n\tRBD_DEV_FLAG_REMOVING,\t \n\tRBD_DEV_FLAG_READONLY,   \n};\n\nstatic DEFINE_MUTEX(client_mutex);\t \n\nstatic LIST_HEAD(rbd_dev_list);     \nstatic DEFINE_SPINLOCK(rbd_dev_list_lock);\n\nstatic LIST_HEAD(rbd_client_list);\t\t \nstatic DEFINE_SPINLOCK(rbd_client_list_lock);\n\n \n\nstatic struct kmem_cache\t*rbd_img_request_cache;\nstatic struct kmem_cache\t*rbd_obj_request_cache;\n\nstatic int rbd_major;\nstatic DEFINE_IDA(rbd_dev_id_ida);\n\nstatic struct workqueue_struct *rbd_wq;\n\nstatic struct ceph_snap_context rbd_empty_snapc = {\n\t.nref = REFCOUNT_INIT(1),\n};\n\n \nstatic bool single_major = true;\nmodule_param(single_major, bool, 0444);\nMODULE_PARM_DESC(single_major, \"Use a single major number for all rbd devices (default: true)\");\n\nstatic ssize_t add_store(const struct bus_type *bus, const char *buf, size_t count);\nstatic ssize_t remove_store(const struct bus_type *bus, const char *buf,\n\t\t\t    size_t count);\nstatic ssize_t add_single_major_store(const struct bus_type *bus, const char *buf,\n\t\t\t\t      size_t count);\nstatic ssize_t remove_single_major_store(const struct bus_type *bus, const char *buf,\n\t\t\t\t\t size_t count);\nstatic int rbd_dev_image_probe(struct rbd_device *rbd_dev, int depth);\n\nstatic int rbd_dev_id_to_minor(int dev_id)\n{\n\treturn dev_id << RBD_SINGLE_MAJOR_PART_SHIFT;\n}\n\nstatic int minor_to_rbd_dev_id(int minor)\n{\n\treturn minor >> RBD_SINGLE_MAJOR_PART_SHIFT;\n}\n\nstatic bool rbd_is_ro(struct rbd_device *rbd_dev)\n{\n\treturn test_bit(RBD_DEV_FLAG_READONLY, &rbd_dev->flags);\n}\n\nstatic bool rbd_is_snap(struct rbd_device *rbd_dev)\n{\n\treturn rbd_dev->spec->snap_id != CEPH_NOSNAP;\n}\n\nstatic bool __rbd_is_lock_owner(struct rbd_device *rbd_dev)\n{\n\tlockdep_assert_held(&rbd_dev->lock_rwsem);\n\n\treturn rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED ||\n\t       rbd_dev->lock_state == RBD_LOCK_STATE_RELEASING;\n}\n\nstatic bool rbd_is_lock_owner(struct rbd_device *rbd_dev)\n{\n\tbool is_lock_owner;\n\n\tdown_read(&rbd_dev->lock_rwsem);\n\tis_lock_owner = __rbd_is_lock_owner(rbd_dev);\n\tup_read(&rbd_dev->lock_rwsem);\n\treturn is_lock_owner;\n}\n\nstatic ssize_t supported_features_show(const struct bus_type *bus, char *buf)\n{\n\treturn sprintf(buf, \"0x%llx\\n\", RBD_FEATURES_SUPPORTED);\n}\n\nstatic BUS_ATTR_WO(add);\nstatic BUS_ATTR_WO(remove);\nstatic BUS_ATTR_WO(add_single_major);\nstatic BUS_ATTR_WO(remove_single_major);\nstatic BUS_ATTR_RO(supported_features);\n\nstatic struct attribute *rbd_bus_attrs[] = {\n\t&bus_attr_add.attr,\n\t&bus_attr_remove.attr,\n\t&bus_attr_add_single_major.attr,\n\t&bus_attr_remove_single_major.attr,\n\t&bus_attr_supported_features.attr,\n\tNULL,\n};\n\nstatic umode_t rbd_bus_is_visible(struct kobject *kobj,\n\t\t\t\t  struct attribute *attr, int index)\n{\n\tif (!single_major &&\n\t    (attr == &bus_attr_add_single_major.attr ||\n\t     attr == &bus_attr_remove_single_major.attr))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group rbd_bus_group = {\n\t.attrs = rbd_bus_attrs,\n\t.is_visible = rbd_bus_is_visible,\n};\n__ATTRIBUTE_GROUPS(rbd_bus);\n\nstatic struct bus_type rbd_bus_type = {\n\t.name\t\t= \"rbd\",\n\t.bus_groups\t= rbd_bus_groups,\n};\n\nstatic void rbd_root_dev_release(struct device *dev)\n{\n}\n\nstatic struct device rbd_root_dev = {\n\t.init_name =    \"rbd\",\n\t.release =      rbd_root_dev_release,\n};\n\nstatic __printf(2, 3)\nvoid rbd_warn(struct rbd_device *rbd_dev, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\n\tif (!rbd_dev)\n\t\tprintk(KERN_WARNING \"%s: %pV\\n\", RBD_DRV_NAME, &vaf);\n\telse if (rbd_dev->disk)\n\t\tprintk(KERN_WARNING \"%s: %s: %pV\\n\",\n\t\t\tRBD_DRV_NAME, rbd_dev->disk->disk_name, &vaf);\n\telse if (rbd_dev->spec && rbd_dev->spec->image_name)\n\t\tprintk(KERN_WARNING \"%s: image %s: %pV\\n\",\n\t\t\tRBD_DRV_NAME, rbd_dev->spec->image_name, &vaf);\n\telse if (rbd_dev->spec && rbd_dev->spec->image_id)\n\t\tprintk(KERN_WARNING \"%s: id %s: %pV\\n\",\n\t\t\tRBD_DRV_NAME, rbd_dev->spec->image_id, &vaf);\n\telse\t \n\t\tprintk(KERN_WARNING \"%s: rbd_dev %p: %pV\\n\",\n\t\t\tRBD_DRV_NAME, rbd_dev, &vaf);\n\tva_end(args);\n}\n\n#ifdef RBD_DEBUG\n#define rbd_assert(expr)\t\t\t\t\t\t\\\n\t\tif (unlikely(!(expr))) {\t\t\t\t\\\n\t\t\tprintk(KERN_ERR \"\\nAssertion failure in %s() \"\t\\\n\t\t\t\t\t\t\"at line %d:\\n\\n\"\t\\\n\t\t\t\t\t\"\\trbd_assert(%s);\\n\\n\",\t\\\n\t\t\t\t\t__func__, __LINE__, #expr);\t\\\n\t\t\tBUG();\t\t\t\t\t\t\\\n\t\t}\n#else  \n#  define rbd_assert(expr)\t((void) 0)\n#endif  \n\nstatic void rbd_dev_remove_parent(struct rbd_device *rbd_dev);\n\nstatic int rbd_dev_refresh(struct rbd_device *rbd_dev);\nstatic int rbd_dev_v2_header_onetime(struct rbd_device *rbd_dev,\n\t\t\t\t     struct rbd_image_header *header);\nstatic const char *rbd_dev_v2_snap_name(struct rbd_device *rbd_dev,\n\t\t\t\t\tu64 snap_id);\nstatic int _rbd_dev_v2_snap_size(struct rbd_device *rbd_dev, u64 snap_id,\n\t\t\t\tu8 *order, u64 *snap_size);\nstatic int rbd_dev_v2_get_flags(struct rbd_device *rbd_dev);\n\nstatic void rbd_obj_handle_request(struct rbd_obj_request *obj_req, int result);\nstatic void rbd_img_handle_request(struct rbd_img_request *img_req, int result);\n\n \nstatic bool pending_result_dec(struct pending_result *pending, int *result)\n{\n\trbd_assert(pending->num_pending > 0);\n\n\tif (*result && !pending->result)\n\t\tpending->result = *result;\n\tif (--pending->num_pending)\n\t\treturn false;\n\n\t*result = pending->result;\n\treturn true;\n}\n\nstatic int rbd_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct rbd_device *rbd_dev = disk->private_data;\n\tbool removing = false;\n\n\tspin_lock_irq(&rbd_dev->lock);\n\tif (test_bit(RBD_DEV_FLAG_REMOVING, &rbd_dev->flags))\n\t\tremoving = true;\n\telse\n\t\trbd_dev->open_count++;\n\tspin_unlock_irq(&rbd_dev->lock);\n\tif (removing)\n\t\treturn -ENOENT;\n\n\t(void) get_device(&rbd_dev->dev);\n\n\treturn 0;\n}\n\nstatic void rbd_release(struct gendisk *disk)\n{\n\tstruct rbd_device *rbd_dev = disk->private_data;\n\tunsigned long open_count_before;\n\n\tspin_lock_irq(&rbd_dev->lock);\n\topen_count_before = rbd_dev->open_count--;\n\tspin_unlock_irq(&rbd_dev->lock);\n\trbd_assert(open_count_before > 0);\n\n\tput_device(&rbd_dev->dev);\n}\n\nstatic const struct block_device_operations rbd_bd_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.open\t\t\t= rbd_open,\n\t.release\t\t= rbd_release,\n};\n\n \nstatic struct rbd_client *rbd_client_create(struct ceph_options *ceph_opts)\n{\n\tstruct rbd_client *rbdc;\n\tint ret = -ENOMEM;\n\n\tdout(\"%s:\\n\", __func__);\n\trbdc = kmalloc(sizeof(struct rbd_client), GFP_KERNEL);\n\tif (!rbdc)\n\t\tgoto out_opt;\n\n\tkref_init(&rbdc->kref);\n\tINIT_LIST_HEAD(&rbdc->node);\n\n\trbdc->client = ceph_create_client(ceph_opts, rbdc);\n\tif (IS_ERR(rbdc->client))\n\t\tgoto out_rbdc;\n\tceph_opts = NULL;  \n\n\tret = ceph_open_session(rbdc->client);\n\tif (ret < 0)\n\t\tgoto out_client;\n\n\tspin_lock(&rbd_client_list_lock);\n\tlist_add_tail(&rbdc->node, &rbd_client_list);\n\tspin_unlock(&rbd_client_list_lock);\n\n\tdout(\"%s: rbdc %p\\n\", __func__, rbdc);\n\n\treturn rbdc;\nout_client:\n\tceph_destroy_client(rbdc->client);\nout_rbdc:\n\tkfree(rbdc);\nout_opt:\n\tif (ceph_opts)\n\t\tceph_destroy_options(ceph_opts);\n\tdout(\"%s: error %d\\n\", __func__, ret);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic struct rbd_client *__rbd_get_client(struct rbd_client *rbdc)\n{\n\tkref_get(&rbdc->kref);\n\n\treturn rbdc;\n}\n\n \nstatic struct rbd_client *rbd_client_find(struct ceph_options *ceph_opts)\n{\n\tstruct rbd_client *rbdc = NULL, *iter;\n\n\tif (ceph_opts->flags & CEPH_OPT_NOSHARE)\n\t\treturn NULL;\n\n\tspin_lock(&rbd_client_list_lock);\n\tlist_for_each_entry(iter, &rbd_client_list, node) {\n\t\tif (!ceph_compare_options(ceph_opts, iter->client)) {\n\t\t\t__rbd_get_client(iter);\n\n\t\t\trbdc = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&rbd_client_list_lock);\n\n\treturn rbdc;\n}\n\n \nenum {\n\tOpt_queue_depth,\n\tOpt_alloc_size,\n\tOpt_lock_timeout,\n\t \n\tOpt_pool_ns,\n\tOpt_compression_hint,\n\t \n\tOpt_read_only,\n\tOpt_read_write,\n\tOpt_lock_on_read,\n\tOpt_exclusive,\n\tOpt_notrim,\n};\n\nenum {\n\tOpt_compression_hint_none,\n\tOpt_compression_hint_compressible,\n\tOpt_compression_hint_incompressible,\n};\n\nstatic const struct constant_table rbd_param_compression_hint[] = {\n\t{\"none\",\t\tOpt_compression_hint_none},\n\t{\"compressible\",\tOpt_compression_hint_compressible},\n\t{\"incompressible\",\tOpt_compression_hint_incompressible},\n\t{}\n};\n\nstatic const struct fs_parameter_spec rbd_parameters[] = {\n\tfsparam_u32\t(\"alloc_size\",\t\t\tOpt_alloc_size),\n\tfsparam_enum\t(\"compression_hint\",\t\tOpt_compression_hint,\n\t\t\t rbd_param_compression_hint),\n\tfsparam_flag\t(\"exclusive\",\t\t\tOpt_exclusive),\n\tfsparam_flag\t(\"lock_on_read\",\t\tOpt_lock_on_read),\n\tfsparam_u32\t(\"lock_timeout\",\t\tOpt_lock_timeout),\n\tfsparam_flag\t(\"notrim\",\t\t\tOpt_notrim),\n\tfsparam_string\t(\"_pool_ns\",\t\t\tOpt_pool_ns),\n\tfsparam_u32\t(\"queue_depth\",\t\t\tOpt_queue_depth),\n\tfsparam_flag\t(\"read_only\",\t\t\tOpt_read_only),\n\tfsparam_flag\t(\"read_write\",\t\t\tOpt_read_write),\n\tfsparam_flag\t(\"ro\",\t\t\t\tOpt_read_only),\n\tfsparam_flag\t(\"rw\",\t\t\t\tOpt_read_write),\n\t{}\n};\n\nstruct rbd_options {\n\tint\tqueue_depth;\n\tint\talloc_size;\n\tunsigned long\tlock_timeout;\n\tbool\tread_only;\n\tbool\tlock_on_read;\n\tbool\texclusive;\n\tbool\ttrim;\n\n\tu32 alloc_hint_flags;   \n};\n\n#define RBD_QUEUE_DEPTH_DEFAULT\tBLKDEV_DEFAULT_RQ\n#define RBD_ALLOC_SIZE_DEFAULT\t(64 * 1024)\n#define RBD_LOCK_TIMEOUT_DEFAULT 0   \n#define RBD_READ_ONLY_DEFAULT\tfalse\n#define RBD_LOCK_ON_READ_DEFAULT false\n#define RBD_EXCLUSIVE_DEFAULT\tfalse\n#define RBD_TRIM_DEFAULT\ttrue\n\nstruct rbd_parse_opts_ctx {\n\tstruct rbd_spec\t\t*spec;\n\tstruct ceph_options\t*copts;\n\tstruct rbd_options\t*opts;\n};\n\nstatic char* obj_op_name(enum obj_operation_type op_type)\n{\n\tswitch (op_type) {\n\tcase OBJ_OP_READ:\n\t\treturn \"read\";\n\tcase OBJ_OP_WRITE:\n\t\treturn \"write\";\n\tcase OBJ_OP_DISCARD:\n\t\treturn \"discard\";\n\tcase OBJ_OP_ZEROOUT:\n\t\treturn \"zeroout\";\n\tdefault:\n\t\treturn \"???\";\n\t}\n}\n\n \nstatic void rbd_client_release(struct kref *kref)\n{\n\tstruct rbd_client *rbdc = container_of(kref, struct rbd_client, kref);\n\n\tdout(\"%s: rbdc %p\\n\", __func__, rbdc);\n\tspin_lock(&rbd_client_list_lock);\n\tlist_del(&rbdc->node);\n\tspin_unlock(&rbd_client_list_lock);\n\n\tceph_destroy_client(rbdc->client);\n\tkfree(rbdc);\n}\n\n \nstatic void rbd_put_client(struct rbd_client *rbdc)\n{\n\tif (rbdc)\n\t\tkref_put(&rbdc->kref, rbd_client_release);\n}\n\n \nstatic struct rbd_client *rbd_get_client(struct ceph_options *ceph_opts)\n{\n\tstruct rbd_client *rbdc;\n\tint ret;\n\n\tmutex_lock(&client_mutex);\n\trbdc = rbd_client_find(ceph_opts);\n\tif (rbdc) {\n\t\tceph_destroy_options(ceph_opts);\n\n\t\t \n\t\tret = ceph_wait_for_latest_osdmap(rbdc->client,\n\t\t\t\t\trbdc->client->options->mount_timeout);\n\t\tif (ret) {\n\t\t\trbd_warn(NULL, \"failed to get latest osdmap: %d\", ret);\n\t\t\trbd_put_client(rbdc);\n\t\t\trbdc = ERR_PTR(ret);\n\t\t}\n\t} else {\n\t\trbdc = rbd_client_create(ceph_opts);\n\t}\n\tmutex_unlock(&client_mutex);\n\n\treturn rbdc;\n}\n\nstatic bool rbd_image_format_valid(u32 image_format)\n{\n\treturn image_format == 1 || image_format == 2;\n}\n\nstatic bool rbd_dev_ondisk_valid(struct rbd_image_header_ondisk *ondisk)\n{\n\tsize_t size;\n\tu32 snap_count;\n\n\t \n\tif (memcmp(&ondisk->text, RBD_HEADER_TEXT, sizeof (RBD_HEADER_TEXT)))\n\t\treturn false;\n\n\t \n\n\tif (ondisk->options.order < SECTOR_SHIFT)\n\t\treturn false;\n\n\t \n\n\tif (ondisk->options.order > 8 * sizeof (int) - 1)\n\t\treturn false;\n\n\t \n\tsnap_count = le32_to_cpu(ondisk->snap_count);\n\tsize = SIZE_MAX - sizeof (struct ceph_snap_context);\n\tif (snap_count > size / sizeof (__le64))\n\t\treturn false;\n\n\t \n\tsize -= snap_count * sizeof (__le64);\n\tif ((u64) size < le64_to_cpu(ondisk->snap_names_len))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic u32 rbd_obj_bytes(struct rbd_image_header *header)\n{\n\treturn 1U << header->obj_order;\n}\n\nstatic void rbd_init_layout(struct rbd_device *rbd_dev)\n{\n\tif (rbd_dev->header.stripe_unit == 0 ||\n\t    rbd_dev->header.stripe_count == 0) {\n\t\trbd_dev->header.stripe_unit = rbd_obj_bytes(&rbd_dev->header);\n\t\trbd_dev->header.stripe_count = 1;\n\t}\n\n\trbd_dev->layout.stripe_unit = rbd_dev->header.stripe_unit;\n\trbd_dev->layout.stripe_count = rbd_dev->header.stripe_count;\n\trbd_dev->layout.object_size = rbd_obj_bytes(&rbd_dev->header);\n\trbd_dev->layout.pool_id = rbd_dev->header.data_pool_id == CEPH_NOPOOL ?\n\t\t\t  rbd_dev->spec->pool_id : rbd_dev->header.data_pool_id;\n\tRCU_INIT_POINTER(rbd_dev->layout.pool_ns, NULL);\n}\n\nstatic void rbd_image_header_cleanup(struct rbd_image_header *header)\n{\n\tkfree(header->object_prefix);\n\tceph_put_snap_context(header->snapc);\n\tkfree(header->snap_sizes);\n\tkfree(header->snap_names);\n\n\tmemset(header, 0, sizeof(*header));\n}\n\n \nstatic int rbd_header_from_disk(struct rbd_image_header *header,\n\t\t\t\tstruct rbd_image_header_ondisk *ondisk,\n\t\t\t\tbool first_time)\n{\n\tstruct ceph_snap_context *snapc;\n\tchar *object_prefix = NULL;\n\tchar *snap_names = NULL;\n\tu64 *snap_sizes = NULL;\n\tu32 snap_count;\n\tint ret = -ENOMEM;\n\tu32 i;\n\n\t \n\n\tif (first_time) {\n\t\tobject_prefix = kstrndup(ondisk->object_prefix,\n\t\t\t\t\t sizeof(ondisk->object_prefix),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!object_prefix)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\n\tsnap_count = le32_to_cpu(ondisk->snap_count);\n\tsnapc = ceph_create_snap_context(snap_count, GFP_KERNEL);\n\tif (!snapc)\n\t\tgoto out_err;\n\tsnapc->seq = le64_to_cpu(ondisk->snap_seq);\n\tif (snap_count) {\n\t\tstruct rbd_image_snap_ondisk *snaps;\n\t\tu64 snap_names_len = le64_to_cpu(ondisk->snap_names_len);\n\n\t\t \n\n\t\tif (snap_names_len > (u64)SIZE_MAX)\n\t\t\tgoto out_2big;\n\t\tsnap_names = kmalloc(snap_names_len, GFP_KERNEL);\n\t\tif (!snap_names)\n\t\t\tgoto out_err;\n\n\t\t \n\t\tsnap_sizes = kmalloc_array(snap_count,\n\t\t\t\t\t   sizeof(*header->snap_sizes),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!snap_sizes)\n\t\t\tgoto out_err;\n\n\t\t \n\t\tmemcpy(snap_names, &ondisk->snaps[snap_count], snap_names_len);\n\t\tsnaps = ondisk->snaps;\n\t\tfor (i = 0; i < snap_count; i++) {\n\t\t\tsnapc->snaps[i] = le64_to_cpu(snaps[i].id);\n\t\t\tsnap_sizes[i] = le64_to_cpu(snaps[i].image_size);\n\t\t}\n\t}\n\n\t \n\n\tif (first_time) {\n\t\theader->object_prefix = object_prefix;\n\t\theader->obj_order = ondisk->options.order;\n\t}\n\n\t \n\n\theader->image_size = le64_to_cpu(ondisk->image_size);\n\theader->snapc = snapc;\n\theader->snap_names = snap_names;\n\theader->snap_sizes = snap_sizes;\n\n\treturn 0;\nout_2big:\n\tret = -EIO;\nout_err:\n\tkfree(snap_sizes);\n\tkfree(snap_names);\n\tceph_put_snap_context(snapc);\n\tkfree(object_prefix);\n\n\treturn ret;\n}\n\nstatic const char *_rbd_dev_v1_snap_name(struct rbd_device *rbd_dev, u32 which)\n{\n\tconst char *snap_name;\n\n\trbd_assert(which < rbd_dev->header.snapc->num_snaps);\n\n\t \n\n\tsnap_name = rbd_dev->header.snap_names;\n\twhile (which--)\n\t\tsnap_name += strlen(snap_name) + 1;\n\n\treturn kstrdup(snap_name, GFP_KERNEL);\n}\n\n \nstatic int snapid_compare_reverse(const void *s1, const void *s2)\n{\n\tu64 snap_id1 = *(u64 *)s1;\n\tu64 snap_id2 = *(u64 *)s2;\n\n\tif (snap_id1 < snap_id2)\n\t\treturn 1;\n\treturn snap_id1 == snap_id2 ? 0 : -1;\n}\n\n \nstatic u32 rbd_dev_snap_index(struct rbd_device *rbd_dev, u64 snap_id)\n{\n\tstruct ceph_snap_context *snapc = rbd_dev->header.snapc;\n\tu64 *found;\n\n\tfound = bsearch(&snap_id, &snapc->snaps, snapc->num_snaps,\n\t\t\t\tsizeof (snap_id), snapid_compare_reverse);\n\n\treturn found ? (u32)(found - &snapc->snaps[0]) : BAD_SNAP_INDEX;\n}\n\nstatic const char *rbd_dev_v1_snap_name(struct rbd_device *rbd_dev,\n\t\t\t\t\tu64 snap_id)\n{\n\tu32 which;\n\tconst char *snap_name;\n\n\twhich = rbd_dev_snap_index(rbd_dev, snap_id);\n\tif (which == BAD_SNAP_INDEX)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tsnap_name = _rbd_dev_v1_snap_name(rbd_dev, which);\n\treturn snap_name ? snap_name : ERR_PTR(-ENOMEM);\n}\n\nstatic const char *rbd_snap_name(struct rbd_device *rbd_dev, u64 snap_id)\n{\n\tif (snap_id == CEPH_NOSNAP)\n\t\treturn RBD_SNAP_HEAD_NAME;\n\n\trbd_assert(rbd_image_format_valid(rbd_dev->image_format));\n\tif (rbd_dev->image_format == 1)\n\t\treturn rbd_dev_v1_snap_name(rbd_dev, snap_id);\n\n\treturn rbd_dev_v2_snap_name(rbd_dev, snap_id);\n}\n\nstatic int rbd_snap_size(struct rbd_device *rbd_dev, u64 snap_id,\n\t\t\t\tu64 *snap_size)\n{\n\trbd_assert(rbd_image_format_valid(rbd_dev->image_format));\n\tif (snap_id == CEPH_NOSNAP) {\n\t\t*snap_size = rbd_dev->header.image_size;\n\t} else if (rbd_dev->image_format == 1) {\n\t\tu32 which;\n\n\t\twhich = rbd_dev_snap_index(rbd_dev, snap_id);\n\t\tif (which == BAD_SNAP_INDEX)\n\t\t\treturn -ENOENT;\n\n\t\t*snap_size = rbd_dev->header.snap_sizes[which];\n\t} else {\n\t\tu64 size = 0;\n\t\tint ret;\n\n\t\tret = _rbd_dev_v2_snap_size(rbd_dev, snap_id, NULL, &size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t*snap_size = size;\n\t}\n\treturn 0;\n}\n\nstatic int rbd_dev_mapping_set(struct rbd_device *rbd_dev)\n{\n\tu64 snap_id = rbd_dev->spec->snap_id;\n\tu64 size = 0;\n\tint ret;\n\n\tret = rbd_snap_size(rbd_dev, snap_id, &size);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_dev->mapping.size = size;\n\treturn 0;\n}\n\nstatic void rbd_dev_mapping_clear(struct rbd_device *rbd_dev)\n{\n\trbd_dev->mapping.size = 0;\n}\n\nstatic void zero_bios(struct ceph_bio_iter *bio_pos, u32 off, u32 bytes)\n{\n\tstruct ceph_bio_iter it = *bio_pos;\n\n\tceph_bio_iter_advance(&it, off);\n\tceph_bio_iter_advance_step(&it, bytes, ({\n\t\tmemzero_bvec(&bv);\n\t}));\n}\n\nstatic void zero_bvecs(struct ceph_bvec_iter *bvec_pos, u32 off, u32 bytes)\n{\n\tstruct ceph_bvec_iter it = *bvec_pos;\n\n\tceph_bvec_iter_advance(&it, off);\n\tceph_bvec_iter_advance_step(&it, bytes, ({\n\t\tmemzero_bvec(&bv);\n\t}));\n}\n\n \nstatic void rbd_obj_zero_range(struct rbd_obj_request *obj_req, u32 off,\n\t\t\t       u32 bytes)\n{\n\tdout(\"%s %p data buf %u~%u\\n\", __func__, obj_req, off, bytes);\n\n\tswitch (obj_req->img_request->data_type) {\n\tcase OBJ_REQUEST_BIO:\n\t\tzero_bios(&obj_req->bio_pos, off, bytes);\n\t\tbreak;\n\tcase OBJ_REQUEST_BVECS:\n\tcase OBJ_REQUEST_OWN_BVECS:\n\t\tzero_bvecs(&obj_req->bvec_pos, off, bytes);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void rbd_obj_request_destroy(struct kref *kref);\nstatic void rbd_obj_request_put(struct rbd_obj_request *obj_request)\n{\n\trbd_assert(obj_request != NULL);\n\tdout(\"%s: obj %p (was %d)\\n\", __func__, obj_request,\n\t\tkref_read(&obj_request->kref));\n\tkref_put(&obj_request->kref, rbd_obj_request_destroy);\n}\n\nstatic inline void rbd_img_obj_request_add(struct rbd_img_request *img_request,\n\t\t\t\t\tstruct rbd_obj_request *obj_request)\n{\n\trbd_assert(obj_request->img_request == NULL);\n\n\t \n\tobj_request->img_request = img_request;\n\tdout(\"%s: img %p obj %p\\n\", __func__, img_request, obj_request);\n}\n\nstatic inline void rbd_img_obj_request_del(struct rbd_img_request *img_request,\n\t\t\t\t\tstruct rbd_obj_request *obj_request)\n{\n\tdout(\"%s: img %p obj %p\\n\", __func__, img_request, obj_request);\n\tlist_del(&obj_request->ex.oe_item);\n\trbd_assert(obj_request->img_request == img_request);\n\trbd_obj_request_put(obj_request);\n}\n\nstatic void rbd_osd_submit(struct ceph_osd_request *osd_req)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\n\tdout(\"%s osd_req %p for obj_req %p objno %llu %llu~%llu\\n\",\n\t     __func__, osd_req, obj_req, obj_req->ex.oe_objno,\n\t     obj_req->ex.oe_off, obj_req->ex.oe_len);\n\tceph_osdc_start_request(osd_req->r_osdc, osd_req);\n}\n\n \nstatic void img_request_layered_set(struct rbd_img_request *img_request)\n{\n\tset_bit(IMG_REQ_LAYERED, &img_request->flags);\n}\n\nstatic bool img_request_layered_test(struct rbd_img_request *img_request)\n{\n\treturn test_bit(IMG_REQ_LAYERED, &img_request->flags) != 0;\n}\n\nstatic bool rbd_obj_is_entire(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\n\treturn !obj_req->ex.oe_off &&\n\t       obj_req->ex.oe_len == rbd_dev->layout.object_size;\n}\n\nstatic bool rbd_obj_is_tail(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\n\treturn obj_req->ex.oe_off + obj_req->ex.oe_len ==\n\t\t\t\t\trbd_dev->layout.object_size;\n}\n\n \nstatic void rbd_obj_set_copyup_enabled(struct rbd_obj_request *obj_req)\n{\n\trbd_assert(obj_req->img_request->snapc);\n\n\tif (obj_req->img_request->op_type == OBJ_OP_DISCARD) {\n\t\tdout(\"%s %p objno %llu discard\\n\", __func__, obj_req,\n\t\t     obj_req->ex.oe_objno);\n\t\treturn;\n\t}\n\n\tif (!obj_req->num_img_extents) {\n\t\tdout(\"%s %p objno %llu not overlapping\\n\", __func__, obj_req,\n\t\t     obj_req->ex.oe_objno);\n\t\treturn;\n\t}\n\n\tif (rbd_obj_is_entire(obj_req) &&\n\t    !obj_req->img_request->snapc->num_snaps) {\n\t\tdout(\"%s %p objno %llu entire\\n\", __func__, obj_req,\n\t\t     obj_req->ex.oe_objno);\n\t\treturn;\n\t}\n\n\tobj_req->flags |= RBD_OBJ_FLAG_COPYUP_ENABLED;\n}\n\nstatic u64 rbd_obj_img_extents_bytes(struct rbd_obj_request *obj_req)\n{\n\treturn ceph_file_extents_bytes(obj_req->img_extents,\n\t\t\t\t       obj_req->num_img_extents);\n}\n\nstatic bool rbd_img_is_write(struct rbd_img_request *img_req)\n{\n\tswitch (img_req->op_type) {\n\tcase OBJ_OP_READ:\n\t\treturn false;\n\tcase OBJ_OP_WRITE:\n\tcase OBJ_OP_DISCARD:\n\tcase OBJ_OP_ZEROOUT:\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void rbd_osd_req_callback(struct ceph_osd_request *osd_req)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\tint result;\n\n\tdout(\"%s osd_req %p result %d for obj_req %p\\n\", __func__, osd_req,\n\t     osd_req->r_result, obj_req);\n\n\t \n\tif (osd_req->r_result > 0 && rbd_img_is_write(obj_req->img_request))\n\t\tresult = 0;\n\telse\n\t\tresult = osd_req->r_result;\n\n\trbd_obj_handle_request(obj_req, result);\n}\n\nstatic void rbd_osd_format_read(struct ceph_osd_request *osd_req)\n{\n\tstruct rbd_obj_request *obj_request = osd_req->r_priv;\n\tstruct rbd_device *rbd_dev = obj_request->img_request->rbd_dev;\n\tstruct ceph_options *opt = rbd_dev->rbd_client->client->options;\n\n\tosd_req->r_flags = CEPH_OSD_FLAG_READ | opt->read_from_replica;\n\tosd_req->r_snapid = obj_request->img_request->snap_id;\n}\n\nstatic void rbd_osd_format_write(struct ceph_osd_request *osd_req)\n{\n\tstruct rbd_obj_request *obj_request = osd_req->r_priv;\n\n\tosd_req->r_flags = CEPH_OSD_FLAG_WRITE;\n\tktime_get_real_ts64(&osd_req->r_mtime);\n\tosd_req->r_data_offset = obj_request->ex.oe_off;\n}\n\nstatic struct ceph_osd_request *\n__rbd_obj_add_osd_request(struct rbd_obj_request *obj_req,\n\t\t\t  struct ceph_snap_context *snapc, int num_ops)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_osd_request *req;\n\tconst char *name_format = rbd_dev->image_format == 1 ?\n\t\t\t\t      RBD_V1_DATA_FORMAT : RBD_V2_DATA_FORMAT;\n\tint ret;\n\n\treq = ceph_osdc_alloc_request(osdc, snapc, num_ops, false, GFP_NOIO);\n\tif (!req)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlist_add_tail(&req->r_private_item, &obj_req->osd_reqs);\n\treq->r_callback = rbd_osd_req_callback;\n\treq->r_priv = obj_req;\n\n\t \n\tceph_oloc_copy(&req->r_base_oloc, &rbd_dev->header_oloc);\n\treq->r_base_oloc.pool = rbd_dev->layout.pool_id;\n\n\tret = ceph_oid_aprintf(&req->r_base_oid, GFP_NOIO, name_format,\n\t\t\t       rbd_dev->header.object_prefix,\n\t\t\t       obj_req->ex.oe_objno);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn req;\n}\n\nstatic struct ceph_osd_request *\nrbd_obj_add_osd_request(struct rbd_obj_request *obj_req, int num_ops)\n{\n\trbd_assert(obj_req->img_request->snapc);\n\treturn __rbd_obj_add_osd_request(obj_req, obj_req->img_request->snapc,\n\t\t\t\t\t num_ops);\n}\n\nstatic struct rbd_obj_request *rbd_obj_request_create(void)\n{\n\tstruct rbd_obj_request *obj_request;\n\n\tobj_request = kmem_cache_zalloc(rbd_obj_request_cache, GFP_NOIO);\n\tif (!obj_request)\n\t\treturn NULL;\n\n\tceph_object_extent_init(&obj_request->ex);\n\tINIT_LIST_HEAD(&obj_request->osd_reqs);\n\tmutex_init(&obj_request->state_mutex);\n\tkref_init(&obj_request->kref);\n\n\tdout(\"%s %p\\n\", __func__, obj_request);\n\treturn obj_request;\n}\n\nstatic void rbd_obj_request_destroy(struct kref *kref)\n{\n\tstruct rbd_obj_request *obj_request;\n\tstruct ceph_osd_request *osd_req;\n\tu32 i;\n\n\tobj_request = container_of(kref, struct rbd_obj_request, kref);\n\n\tdout(\"%s: obj %p\\n\", __func__, obj_request);\n\n\twhile (!list_empty(&obj_request->osd_reqs)) {\n\t\tosd_req = list_first_entry(&obj_request->osd_reqs,\n\t\t\t\t    struct ceph_osd_request, r_private_item);\n\t\tlist_del_init(&osd_req->r_private_item);\n\t\tceph_osdc_put_request(osd_req);\n\t}\n\n\tswitch (obj_request->img_request->data_type) {\n\tcase OBJ_REQUEST_NODATA:\n\tcase OBJ_REQUEST_BIO:\n\tcase OBJ_REQUEST_BVECS:\n\t\tbreak;\t\t \n\tcase OBJ_REQUEST_OWN_BVECS:\n\t\tkfree(obj_request->bvec_pos.bvecs);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tkfree(obj_request->img_extents);\n\tif (obj_request->copyup_bvecs) {\n\t\tfor (i = 0; i < obj_request->copyup_bvec_count; i++) {\n\t\t\tif (obj_request->copyup_bvecs[i].bv_page)\n\t\t\t\t__free_page(obj_request->copyup_bvecs[i].bv_page);\n\t\t}\n\t\tkfree(obj_request->copyup_bvecs);\n\t}\n\n\tkmem_cache_free(rbd_obj_request_cache, obj_request);\n}\n\n \n\nstatic void rbd_spec_put(struct rbd_spec *spec);\nstatic void rbd_dev_unparent(struct rbd_device *rbd_dev)\n{\n\trbd_dev_remove_parent(rbd_dev);\n\trbd_spec_put(rbd_dev->parent_spec);\n\trbd_dev->parent_spec = NULL;\n\trbd_dev->parent_overlap = 0;\n}\n\n \nstatic void rbd_dev_parent_put(struct rbd_device *rbd_dev)\n{\n\tint counter;\n\n\tif (!rbd_dev->parent_spec)\n\t\treturn;\n\n\tcounter = atomic_dec_return_safe(&rbd_dev->parent_ref);\n\tif (counter > 0)\n\t\treturn;\n\n\t \n\n\tif (!counter)\n\t\trbd_dev_unparent(rbd_dev);\n\telse\n\t\trbd_warn(rbd_dev, \"parent reference underflow\");\n}\n\n \nstatic bool rbd_dev_parent_get(struct rbd_device *rbd_dev)\n{\n\tint counter = 0;\n\n\tif (!rbd_dev->parent_spec)\n\t\treturn false;\n\n\tif (rbd_dev->parent_overlap)\n\t\tcounter = atomic_inc_return_safe(&rbd_dev->parent_ref);\n\n\tif (counter < 0)\n\t\trbd_warn(rbd_dev, \"parent reference overflow\");\n\n\treturn counter > 0;\n}\n\nstatic void rbd_img_request_init(struct rbd_img_request *img_request,\n\t\t\t\t struct rbd_device *rbd_dev,\n\t\t\t\t enum obj_operation_type op_type)\n{\n\tmemset(img_request, 0, sizeof(*img_request));\n\n\timg_request->rbd_dev = rbd_dev;\n\timg_request->op_type = op_type;\n\n\tINIT_LIST_HEAD(&img_request->lock_item);\n\tINIT_LIST_HEAD(&img_request->object_extents);\n\tmutex_init(&img_request->state_mutex);\n}\n\n \nstatic void rbd_img_capture_header(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\n\tlockdep_assert_held(&rbd_dev->header_rwsem);\n\n\tif (!rbd_img_is_write(img_req))\n\t\timg_req->snap_id = rbd_dev->spec->snap_id;\n\n\tif (rbd_dev_parent_get(rbd_dev))\n\t\timg_request_layered_set(img_req);\n}\n\nstatic void rbd_img_request_destroy(struct rbd_img_request *img_request)\n{\n\tstruct rbd_obj_request *obj_request;\n\tstruct rbd_obj_request *next_obj_request;\n\n\tdout(\"%s: img %p\\n\", __func__, img_request);\n\n\tWARN_ON(!list_empty(&img_request->lock_item));\n\tfor_each_obj_request_safe(img_request, obj_request, next_obj_request)\n\t\trbd_img_obj_request_del(img_request, obj_request);\n\n\tif (img_request_layered_test(img_request))\n\t\trbd_dev_parent_put(img_request->rbd_dev);\n\n\tif (rbd_img_is_write(img_request))\n\t\tceph_put_snap_context(img_request->snapc);\n\n\tif (test_bit(IMG_REQ_CHILD, &img_request->flags))\n\t\tkmem_cache_free(rbd_img_request_cache, img_request);\n}\n\n#define BITS_PER_OBJ\t2\n#define OBJS_PER_BYTE\t(BITS_PER_BYTE / BITS_PER_OBJ)\n#define OBJ_MASK\t((1 << BITS_PER_OBJ) - 1)\n\nstatic void __rbd_object_map_index(struct rbd_device *rbd_dev, u64 objno,\n\t\t\t\t   u64 *index, u8 *shift)\n{\n\tu32 off;\n\n\trbd_assert(objno < rbd_dev->object_map_size);\n\t*index = div_u64_rem(objno, OBJS_PER_BYTE, &off);\n\t*shift = (OBJS_PER_BYTE - off - 1) * BITS_PER_OBJ;\n}\n\nstatic u8 __rbd_object_map_get(struct rbd_device *rbd_dev, u64 objno)\n{\n\tu64 index;\n\tu8 shift;\n\n\tlockdep_assert_held(&rbd_dev->object_map_lock);\n\t__rbd_object_map_index(rbd_dev, objno, &index, &shift);\n\treturn (rbd_dev->object_map[index] >> shift) & OBJ_MASK;\n}\n\nstatic void __rbd_object_map_set(struct rbd_device *rbd_dev, u64 objno, u8 val)\n{\n\tu64 index;\n\tu8 shift;\n\tu8 *p;\n\n\tlockdep_assert_held(&rbd_dev->object_map_lock);\n\trbd_assert(!(val & ~OBJ_MASK));\n\n\t__rbd_object_map_index(rbd_dev, objno, &index, &shift);\n\tp = &rbd_dev->object_map[index];\n\t*p = (*p & ~(OBJ_MASK << shift)) | (val << shift);\n}\n\nstatic u8 rbd_object_map_get(struct rbd_device *rbd_dev, u64 objno)\n{\n\tu8 state;\n\n\tspin_lock(&rbd_dev->object_map_lock);\n\tstate = __rbd_object_map_get(rbd_dev, objno);\n\tspin_unlock(&rbd_dev->object_map_lock);\n\treturn state;\n}\n\nstatic bool use_object_map(struct rbd_device *rbd_dev)\n{\n\t \n\tif (!rbd_is_snap(rbd_dev) && rbd_is_ro(rbd_dev))\n\t\treturn false;\n\n\treturn ((rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP) &&\n\t\t!(rbd_dev->object_map_flags & RBD_FLAG_OBJECT_MAP_INVALID));\n}\n\nstatic bool rbd_object_map_may_exist(struct rbd_device *rbd_dev, u64 objno)\n{\n\tu8 state;\n\n\t \n\tif (!use_object_map(rbd_dev))\n\t\treturn true;\n\n\tstate = rbd_object_map_get(rbd_dev, objno);\n\treturn state != OBJECT_NONEXISTENT;\n}\n\nstatic void rbd_object_map_name(struct rbd_device *rbd_dev, u64 snap_id,\n\t\t\t\tstruct ceph_object_id *oid)\n{\n\tif (snap_id == CEPH_NOSNAP)\n\t\tceph_oid_printf(oid, \"%s%s\", RBD_OBJECT_MAP_PREFIX,\n\t\t\t\trbd_dev->spec->image_id);\n\telse\n\t\tceph_oid_printf(oid, \"%s%s.%016llx\", RBD_OBJECT_MAP_PREFIX,\n\t\t\t\trbd_dev->spec->image_id, snap_id);\n}\n\nstatic int rbd_object_map_lock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tCEPH_DEFINE_OID_ONSTACK(oid);\n\tu8 lock_type;\n\tchar *lock_tag;\n\tstruct ceph_locker *lockers;\n\tu32 num_lockers;\n\tbool broke_lock = false;\n\tint ret;\n\n\trbd_object_map_name(rbd_dev, CEPH_NOSNAP, &oid);\n\nagain:\n\tret = ceph_cls_lock(osdc, &oid, &rbd_dev->header_oloc, RBD_LOCK_NAME,\n\t\t\t    CEPH_CLS_LOCK_EXCLUSIVE, \"\", \"\", \"\", 0);\n\tif (ret != -EBUSY || broke_lock) {\n\t\tif (ret == -EEXIST)\n\t\t\tret = 0;  \n\t\tif (ret)\n\t\t\trbd_warn(rbd_dev, \"failed to lock object map: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ceph_cls_lock_info(osdc, &oid, &rbd_dev->header_oloc,\n\t\t\t\t RBD_LOCK_NAME, &lock_type, &lock_tag,\n\t\t\t\t &lockers, &num_lockers);\n\tif (ret) {\n\t\tif (ret == -ENOENT)\n\t\t\tgoto again;\n\n\t\trbd_warn(rbd_dev, \"failed to get object map lockers: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tkfree(lock_tag);\n\tif (num_lockers == 0)\n\t\tgoto again;\n\n\trbd_warn(rbd_dev, \"breaking object map lock owned by %s%llu\",\n\t\t ENTITY_NAME(lockers[0].id.name));\n\n\tret = ceph_cls_break_lock(osdc, &oid, &rbd_dev->header_oloc,\n\t\t\t\t  RBD_LOCK_NAME, lockers[0].id.cookie,\n\t\t\t\t  &lockers[0].id.name);\n\tceph_free_lockers(lockers, num_lockers);\n\tif (ret) {\n\t\tif (ret == -ENOENT)\n\t\t\tgoto again;\n\n\t\trbd_warn(rbd_dev, \"failed to break object map lock: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tbroke_lock = true;\n\tgoto again;\n}\n\nstatic void rbd_object_map_unlock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tCEPH_DEFINE_OID_ONSTACK(oid);\n\tint ret;\n\n\trbd_object_map_name(rbd_dev, CEPH_NOSNAP, &oid);\n\n\tret = ceph_cls_unlock(osdc, &oid, &rbd_dev->header_oloc, RBD_LOCK_NAME,\n\t\t\t      \"\");\n\tif (ret && ret != -ENOENT)\n\t\trbd_warn(rbd_dev, \"failed to unlock object map: %d\", ret);\n}\n\nstatic int decode_object_map_header(void **p, void *end, u64 *object_map_size)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tu32 header_len;\n\tvoid *header_end;\n\tint ret;\n\n\tceph_decode_32_safe(p, end, header_len, e_inval);\n\theader_end = *p + header_len;\n\n\tret = ceph_start_decoding(p, end, 1, \"BitVector header\", &struct_v,\n\t\t\t\t  &struct_len);\n\tif (ret)\n\t\treturn ret;\n\n\tceph_decode_64_safe(p, end, *object_map_size, e_inval);\n\n\t*p = header_end;\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\nstatic int __rbd_object_map_load(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tCEPH_DEFINE_OID_ONSTACK(oid);\n\tstruct page **pages;\n\tvoid *p, *end;\n\tsize_t reply_len;\n\tu64 num_objects;\n\tu64 object_map_bytes;\n\tu64 object_map_size;\n\tint num_pages;\n\tint ret;\n\n\trbd_assert(!rbd_dev->object_map && !rbd_dev->object_map_size);\n\n\tnum_objects = ceph_get_num_objects(&rbd_dev->layout,\n\t\t\t\t\t   rbd_dev->mapping.size);\n\tobject_map_bytes = DIV_ROUND_UP_ULL(num_objects * BITS_PER_OBJ,\n\t\t\t\t\t    BITS_PER_BYTE);\n\tnum_pages = calc_pages_for(0, object_map_bytes) + 1;\n\tpages = ceph_alloc_page_vector(num_pages, GFP_KERNEL);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\treply_len = num_pages * PAGE_SIZE;\n\trbd_object_map_name(rbd_dev, rbd_dev->spec->snap_id, &oid);\n\tret = ceph_osdc_call(osdc, &oid, &rbd_dev->header_oloc,\n\t\t\t     \"rbd\", \"object_map_load\", CEPH_OSD_FLAG_READ,\n\t\t\t     NULL, 0, pages, &reply_len);\n\tif (ret)\n\t\tgoto out;\n\n\tp = page_address(pages[0]);\n\tend = p + min(reply_len, (size_t)PAGE_SIZE);\n\tret = decode_object_map_header(&p, end, &object_map_size);\n\tif (ret)\n\t\tgoto out;\n\n\tif (object_map_size != num_objects) {\n\t\trbd_warn(rbd_dev, \"object map size mismatch: %llu vs %llu\",\n\t\t\t object_map_size, num_objects);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (offset_in_page(p) + object_map_bytes > reply_len) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\trbd_dev->object_map = kvmalloc(object_map_bytes, GFP_KERNEL);\n\tif (!rbd_dev->object_map) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trbd_dev->object_map_size = object_map_size;\n\tceph_copy_from_page_vector(pages, rbd_dev->object_map,\n\t\t\t\t   offset_in_page(p), object_map_bytes);\n\nout:\n\tceph_release_page_vector(pages, num_pages);\n\treturn ret;\n}\n\nstatic void rbd_object_map_free(struct rbd_device *rbd_dev)\n{\n\tkvfree(rbd_dev->object_map);\n\trbd_dev->object_map = NULL;\n\trbd_dev->object_map_size = 0;\n}\n\nstatic int rbd_object_map_load(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\tret = __rbd_object_map_load(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rbd_dev_v2_get_flags(rbd_dev);\n\tif (ret) {\n\t\trbd_object_map_free(rbd_dev);\n\t\treturn ret;\n\t}\n\n\tif (rbd_dev->object_map_flags & RBD_FLAG_OBJECT_MAP_INVALID)\n\t\trbd_warn(rbd_dev, \"object map is invalid\");\n\n\treturn 0;\n}\n\nstatic int rbd_object_map_open(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\tret = rbd_object_map_lock(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rbd_object_map_load(rbd_dev);\n\tif (ret) {\n\t\trbd_object_map_unlock(rbd_dev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void rbd_object_map_close(struct rbd_device *rbd_dev)\n{\n\trbd_object_map_free(rbd_dev);\n\trbd_object_map_unlock(rbd_dev);\n}\n\n \nstatic int rbd_object_map_update_finish(struct rbd_obj_request *obj_req,\n\t\t\t\t\tstruct ceph_osd_request *osd_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tstruct ceph_osd_data *osd_data;\n\tu64 objno;\n\tu8 state, new_state, current_state;\n\tbool has_current_state;\n\tvoid *p;\n\n\tif (osd_req->r_result)\n\t\treturn osd_req->r_result;\n\n\t \n\tif (osd_req->r_num_ops == 1)\n\t\treturn 0;\n\n\t \n\trbd_assert(osd_req->r_num_ops == 2);\n\tosd_data = osd_req_op_data(osd_req, 1, cls, request_data);\n\trbd_assert(osd_data->type == CEPH_OSD_DATA_TYPE_PAGES);\n\n\tp = page_address(osd_data->pages[0]);\n\tobjno = ceph_decode_64(&p);\n\trbd_assert(objno == obj_req->ex.oe_objno);\n\trbd_assert(ceph_decode_64(&p) == objno + 1);\n\tnew_state = ceph_decode_8(&p);\n\thas_current_state = ceph_decode_8(&p);\n\tif (has_current_state)\n\t\tcurrent_state = ceph_decode_8(&p);\n\n\tspin_lock(&rbd_dev->object_map_lock);\n\tstate = __rbd_object_map_get(rbd_dev, objno);\n\tif (!has_current_state || current_state == state ||\n\t    (current_state == OBJECT_EXISTS && state == OBJECT_EXISTS_CLEAN))\n\t\t__rbd_object_map_set(rbd_dev, objno, new_state);\n\tspin_unlock(&rbd_dev->object_map_lock);\n\n\treturn 0;\n}\n\nstatic void rbd_object_map_callback(struct ceph_osd_request *osd_req)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\tint result;\n\n\tdout(\"%s osd_req %p result %d for obj_req %p\\n\", __func__, osd_req,\n\t     osd_req->r_result, obj_req);\n\n\tresult = rbd_object_map_update_finish(obj_req, osd_req);\n\trbd_obj_handle_request(obj_req, result);\n}\n\nstatic bool update_needed(struct rbd_device *rbd_dev, u64 objno, u8 new_state)\n{\n\tu8 state = rbd_object_map_get(rbd_dev, objno);\n\n\tif (state == new_state ||\n\t    (new_state == OBJECT_PENDING && state == OBJECT_NONEXISTENT) ||\n\t    (new_state == OBJECT_NONEXISTENT && state != OBJECT_PENDING))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int rbd_cls_object_map_update(struct ceph_osd_request *req,\n\t\t\t\t     int which, u64 objno, u8 new_state,\n\t\t\t\t     const u8 *current_state)\n{\n\tstruct page **pages;\n\tvoid *p, *start;\n\tint ret;\n\n\tret = osd_req_op_cls_init(req, which, \"rbd\", \"object_map_update\");\n\tif (ret)\n\t\treturn ret;\n\n\tpages = ceph_alloc_page_vector(1, GFP_NOIO);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\tp = start = page_address(pages[0]);\n\tceph_encode_64(&p, objno);\n\tceph_encode_64(&p, objno + 1);\n\tceph_encode_8(&p, new_state);\n\tif (current_state) {\n\t\tceph_encode_8(&p, 1);\n\t\tceph_encode_8(&p, *current_state);\n\t} else {\n\t\tceph_encode_8(&p, 0);\n\t}\n\n\tosd_req_op_cls_request_data_pages(req, which, pages, p - start, 0,\n\t\t\t\t\t  false, true);\n\treturn 0;\n}\n\n \nstatic int rbd_object_map_update(struct rbd_obj_request *obj_req, u64 snap_id,\n\t\t\t\t u8 new_state, const u8 *current_state)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_osd_request *req;\n\tint num_ops = 1;\n\tint which = 0;\n\tint ret;\n\n\tif (snap_id == CEPH_NOSNAP) {\n\t\tif (!update_needed(rbd_dev, obj_req->ex.oe_objno, new_state))\n\t\t\treturn 1;\n\n\t\tnum_ops++;  \n\t}\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, num_ops, false, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tlist_add_tail(&req->r_private_item, &obj_req->osd_reqs);\n\treq->r_callback = rbd_object_map_callback;\n\treq->r_priv = obj_req;\n\n\trbd_object_map_name(rbd_dev, snap_id, &req->r_base_oid);\n\tceph_oloc_copy(&req->r_base_oloc, &rbd_dev->header_oloc);\n\treq->r_flags = CEPH_OSD_FLAG_WRITE;\n\tktime_get_real_ts64(&req->r_mtime);\n\n\tif (snap_id == CEPH_NOSNAP) {\n\t\t \n\t\tret = ceph_cls_assert_locked(req, which++, RBD_LOCK_NAME,\n\t\t\t\t\t     CEPH_CLS_LOCK_EXCLUSIVE, \"\", \"\");\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = rbd_cls_object_map_update(req, which, obj_req->ex.oe_objno,\n\t\t\t\t\tnew_state, current_state);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tif (ret)\n\t\treturn ret;\n\n\tceph_osdc_start_request(osdc, req);\n\treturn 0;\n}\n\nstatic void prune_extents(struct ceph_file_extent *img_extents,\n\t\t\t  u32 *num_img_extents, u64 overlap)\n{\n\tu32 cnt = *num_img_extents;\n\n\t \n\twhile (cnt && img_extents[cnt - 1].fe_off >= overlap)\n\t\tcnt--;\n\n\tif (cnt) {\n\t\tstruct ceph_file_extent *ex = &img_extents[cnt - 1];\n\n\t\t \n\t\tif (ex->fe_off + ex->fe_len > overlap)\n\t\t\tex->fe_len = overlap - ex->fe_off;\n\t}\n\n\t*num_img_extents = cnt;\n}\n\n \nstatic int rbd_obj_calc_img_extents(struct rbd_obj_request *obj_req,\n\t\t\t\t    bool entire)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tint ret;\n\n\tif (!rbd_dev->parent_overlap)\n\t\treturn 0;\n\n\tret = ceph_extent_to_file(&rbd_dev->layout, obj_req->ex.oe_objno,\n\t\t\t\t  entire ? 0 : obj_req->ex.oe_off,\n\t\t\t\t  entire ? rbd_dev->layout.object_size :\n\t\t\t\t\t\t\tobj_req->ex.oe_len,\n\t\t\t\t  &obj_req->img_extents,\n\t\t\t\t  &obj_req->num_img_extents);\n\tif (ret)\n\t\treturn ret;\n\n\tprune_extents(obj_req->img_extents, &obj_req->num_img_extents,\n\t\t      rbd_dev->parent_overlap);\n\treturn 0;\n}\n\nstatic void rbd_osd_setup_data(struct ceph_osd_request *osd_req, int which)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\n\tswitch (obj_req->img_request->data_type) {\n\tcase OBJ_REQUEST_BIO:\n\t\tosd_req_op_extent_osd_data_bio(osd_req, which,\n\t\t\t\t\t       &obj_req->bio_pos,\n\t\t\t\t\t       obj_req->ex.oe_len);\n\t\tbreak;\n\tcase OBJ_REQUEST_BVECS:\n\tcase OBJ_REQUEST_OWN_BVECS:\n\t\trbd_assert(obj_req->bvec_pos.iter.bi_size ==\n\t\t\t\t\t\t\tobj_req->ex.oe_len);\n\t\trbd_assert(obj_req->bvec_idx == obj_req->bvec_count);\n\t\tosd_req_op_extent_osd_data_bvec_pos(osd_req, which,\n\t\t\t\t\t\t    &obj_req->bvec_pos);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int rbd_osd_setup_stat(struct ceph_osd_request *osd_req, int which)\n{\n\tstruct page **pages;\n\n\t \n\tpages = ceph_alloc_page_vector(1, GFP_NOIO);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\tosd_req_op_init(osd_req, which, CEPH_OSD_OP_STAT, 0);\n\tosd_req_op_raw_data_in_pages(osd_req, which, pages,\n\t\t\t\t     8 + sizeof(struct ceph_timespec),\n\t\t\t\t     0, false, true);\n\treturn 0;\n}\n\nstatic int rbd_osd_setup_copyup(struct ceph_osd_request *osd_req, int which,\n\t\t\t\tu32 bytes)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\tint ret;\n\n\tret = osd_req_op_cls_init(osd_req, which, \"rbd\", \"copyup\");\n\tif (ret)\n\t\treturn ret;\n\n\tosd_req_op_cls_request_data_bvecs(osd_req, which, obj_req->copyup_bvecs,\n\t\t\t\t\t  obj_req->copyup_bvec_count, bytes);\n\treturn 0;\n}\n\nstatic int rbd_obj_init_read(struct rbd_obj_request *obj_req)\n{\n\tobj_req->read_state = RBD_OBJ_READ_START;\n\treturn 0;\n}\n\nstatic void __rbd_osd_setup_write_ops(struct ceph_osd_request *osd_req,\n\t\t\t\t      int which)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tu16 opcode;\n\n\tif (!use_object_map(rbd_dev) ||\n\t    !(obj_req->flags & RBD_OBJ_FLAG_MAY_EXIST)) {\n\t\tosd_req_op_alloc_hint_init(osd_req, which++,\n\t\t\t\t\t   rbd_dev->layout.object_size,\n\t\t\t\t\t   rbd_dev->layout.object_size,\n\t\t\t\t\t   rbd_dev->opts->alloc_hint_flags);\n\t}\n\n\tif (rbd_obj_is_entire(obj_req))\n\t\topcode = CEPH_OSD_OP_WRITEFULL;\n\telse\n\t\topcode = CEPH_OSD_OP_WRITE;\n\n\tosd_req_op_extent_init(osd_req, which, opcode,\n\t\t\t       obj_req->ex.oe_off, obj_req->ex.oe_len, 0, 0);\n\trbd_osd_setup_data(osd_req, which);\n}\n\nstatic int rbd_obj_init_write(struct rbd_obj_request *obj_req)\n{\n\tint ret;\n\n\t \n\tret = rbd_obj_calc_img_extents(obj_req, true);\n\tif (ret)\n\t\treturn ret;\n\n\tobj_req->write_state = RBD_OBJ_WRITE_START;\n\treturn 0;\n}\n\nstatic u16 truncate_or_zero_opcode(struct rbd_obj_request *obj_req)\n{\n\treturn rbd_obj_is_tail(obj_req) ? CEPH_OSD_OP_TRUNCATE :\n\t\t\t\t\t  CEPH_OSD_OP_ZERO;\n}\n\nstatic void __rbd_osd_setup_discard_ops(struct ceph_osd_request *osd_req,\n\t\t\t\t\tint which)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\n\tif (rbd_obj_is_entire(obj_req) && !obj_req->num_img_extents) {\n\t\trbd_assert(obj_req->flags & RBD_OBJ_FLAG_DELETION);\n\t\tosd_req_op_init(osd_req, which, CEPH_OSD_OP_DELETE, 0);\n\t} else {\n\t\tosd_req_op_extent_init(osd_req, which,\n\t\t\t\t       truncate_or_zero_opcode(obj_req),\n\t\t\t\t       obj_req->ex.oe_off, obj_req->ex.oe_len,\n\t\t\t\t       0, 0);\n\t}\n}\n\nstatic int rbd_obj_init_discard(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tu64 off, next_off;\n\tint ret;\n\n\t \n\tif (rbd_dev->opts->alloc_size != rbd_dev->layout.object_size ||\n\t    !rbd_obj_is_tail(obj_req)) {\n\t\toff = round_up(obj_req->ex.oe_off, rbd_dev->opts->alloc_size);\n\t\tnext_off = round_down(obj_req->ex.oe_off + obj_req->ex.oe_len,\n\t\t\t\t      rbd_dev->opts->alloc_size);\n\t\tif (off >= next_off)\n\t\t\treturn 1;\n\n\t\tdout(\"%s %p %llu~%llu -> %llu~%llu\\n\", __func__,\n\t\t     obj_req, obj_req->ex.oe_off, obj_req->ex.oe_len,\n\t\t     off, next_off - off);\n\t\tobj_req->ex.oe_off = off;\n\t\tobj_req->ex.oe_len = next_off - off;\n\t}\n\n\t \n\tret = rbd_obj_calc_img_extents(obj_req, true);\n\tif (ret)\n\t\treturn ret;\n\n\tobj_req->flags |= RBD_OBJ_FLAG_NOOP_FOR_NONEXISTENT;\n\tif (rbd_obj_is_entire(obj_req) && !obj_req->num_img_extents)\n\t\tobj_req->flags |= RBD_OBJ_FLAG_DELETION;\n\n\tobj_req->write_state = RBD_OBJ_WRITE_START;\n\treturn 0;\n}\n\nstatic void __rbd_osd_setup_zeroout_ops(struct ceph_osd_request *osd_req,\n\t\t\t\t\tint which)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\tu16 opcode;\n\n\tif (rbd_obj_is_entire(obj_req)) {\n\t\tif (obj_req->num_img_extents) {\n\t\t\tif (!(obj_req->flags & RBD_OBJ_FLAG_COPYUP_ENABLED))\n\t\t\t\tosd_req_op_init(osd_req, which++,\n\t\t\t\t\t\tCEPH_OSD_OP_CREATE, 0);\n\t\t\topcode = CEPH_OSD_OP_TRUNCATE;\n\t\t} else {\n\t\t\trbd_assert(obj_req->flags & RBD_OBJ_FLAG_DELETION);\n\t\t\tosd_req_op_init(osd_req, which++,\n\t\t\t\t\tCEPH_OSD_OP_DELETE, 0);\n\t\t\topcode = 0;\n\t\t}\n\t} else {\n\t\topcode = truncate_or_zero_opcode(obj_req);\n\t}\n\n\tif (opcode)\n\t\tosd_req_op_extent_init(osd_req, which, opcode,\n\t\t\t\t       obj_req->ex.oe_off, obj_req->ex.oe_len,\n\t\t\t\t       0, 0);\n}\n\nstatic int rbd_obj_init_zeroout(struct rbd_obj_request *obj_req)\n{\n\tint ret;\n\n\t \n\tret = rbd_obj_calc_img_extents(obj_req, true);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!obj_req->num_img_extents) {\n\t\tobj_req->flags |= RBD_OBJ_FLAG_NOOP_FOR_NONEXISTENT;\n\t\tif (rbd_obj_is_entire(obj_req))\n\t\t\tobj_req->flags |= RBD_OBJ_FLAG_DELETION;\n\t}\n\n\tobj_req->write_state = RBD_OBJ_WRITE_START;\n\treturn 0;\n}\n\nstatic int count_write_ops(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_img_request *img_req = obj_req->img_request;\n\n\tswitch (img_req->op_type) {\n\tcase OBJ_OP_WRITE:\n\t\tif (!use_object_map(img_req->rbd_dev) ||\n\t\t    !(obj_req->flags & RBD_OBJ_FLAG_MAY_EXIST))\n\t\t\treturn 2;  \n\n\t\treturn 1;  \n\tcase OBJ_OP_DISCARD:\n\t\treturn 1;  \n\tcase OBJ_OP_ZEROOUT:\n\t\tif (rbd_obj_is_entire(obj_req) && obj_req->num_img_extents &&\n\t\t    !(obj_req->flags & RBD_OBJ_FLAG_COPYUP_ENABLED))\n\t\t\treturn 2;  \n\n\t\treturn 1;  \n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void rbd_osd_setup_write_ops(struct ceph_osd_request *osd_req,\n\t\t\t\t    int which)\n{\n\tstruct rbd_obj_request *obj_req = osd_req->r_priv;\n\n\tswitch (obj_req->img_request->op_type) {\n\tcase OBJ_OP_WRITE:\n\t\t__rbd_osd_setup_write_ops(osd_req, which);\n\t\tbreak;\n\tcase OBJ_OP_DISCARD:\n\t\t__rbd_osd_setup_discard_ops(osd_req, which);\n\t\tbreak;\n\tcase OBJ_OP_ZEROOUT:\n\t\t__rbd_osd_setup_zeroout_ops(osd_req, which);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic int __rbd_img_fill_request(struct rbd_img_request *img_req)\n{\n\tstruct rbd_obj_request *obj_req, *next_obj_req;\n\tint ret;\n\n\tfor_each_obj_request_safe(img_req, obj_req, next_obj_req) {\n\t\tswitch (img_req->op_type) {\n\t\tcase OBJ_OP_READ:\n\t\t\tret = rbd_obj_init_read(obj_req);\n\t\t\tbreak;\n\t\tcase OBJ_OP_WRITE:\n\t\t\tret = rbd_obj_init_write(obj_req);\n\t\t\tbreak;\n\t\tcase OBJ_OP_DISCARD:\n\t\t\tret = rbd_obj_init_discard(obj_req);\n\t\t\tbreak;\n\t\tcase OBJ_OP_ZEROOUT:\n\t\t\tret = rbd_obj_init_zeroout(obj_req);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret > 0) {\n\t\t\trbd_img_obj_request_del(img_req, obj_req);\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\timg_req->state = RBD_IMG_START;\n\treturn 0;\n}\n\nunion rbd_img_fill_iter {\n\tstruct ceph_bio_iter\tbio_iter;\n\tstruct ceph_bvec_iter\tbvec_iter;\n};\n\nstruct rbd_img_fill_ctx {\n\tenum obj_request_type\tpos_type;\n\tunion rbd_img_fill_iter\t*pos;\n\tunion rbd_img_fill_iter\titer;\n\tceph_object_extent_fn_t\tset_pos_fn;\n\tceph_object_extent_fn_t\tcount_fn;\n\tceph_object_extent_fn_t\tcopy_fn;\n};\n\nstatic struct ceph_object_extent *alloc_object_extent(void *arg)\n{\n\tstruct rbd_img_request *img_req = arg;\n\tstruct rbd_obj_request *obj_req;\n\n\tobj_req = rbd_obj_request_create();\n\tif (!obj_req)\n\t\treturn NULL;\n\n\trbd_img_obj_request_add(img_req, obj_req);\n\treturn &obj_req->ex;\n}\n\n \nstatic bool rbd_layout_is_fancy(struct ceph_file_layout *l)\n{\n\treturn l->stripe_unit != l->object_size;\n}\n\nstatic int rbd_img_fill_request_nocopy(struct rbd_img_request *img_req,\n\t\t\t\t       struct ceph_file_extent *img_extents,\n\t\t\t\t       u32 num_img_extents,\n\t\t\t\t       struct rbd_img_fill_ctx *fctx)\n{\n\tu32 i;\n\tint ret;\n\n\timg_req->data_type = fctx->pos_type;\n\n\t \n\tfctx->iter = *fctx->pos;\n\tfor (i = 0; i < num_img_extents; i++) {\n\t\tret = ceph_file_to_extents(&img_req->rbd_dev->layout,\n\t\t\t\t\t   img_extents[i].fe_off,\n\t\t\t\t\t   img_extents[i].fe_len,\n\t\t\t\t\t   &img_req->object_extents,\n\t\t\t\t\t   alloc_object_extent, img_req,\n\t\t\t\t\t   fctx->set_pos_fn, &fctx->iter);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn __rbd_img_fill_request(img_req);\n}\n\n \nstatic int rbd_img_fill_request(struct rbd_img_request *img_req,\n\t\t\t\tstruct ceph_file_extent *img_extents,\n\t\t\t\tu32 num_img_extents,\n\t\t\t\tstruct rbd_img_fill_ctx *fctx)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tstruct rbd_obj_request *obj_req;\n\tu32 i;\n\tint ret;\n\n\tif (fctx->pos_type == OBJ_REQUEST_NODATA ||\n\t    !rbd_layout_is_fancy(&rbd_dev->layout))\n\t\treturn rbd_img_fill_request_nocopy(img_req, img_extents,\n\t\t\t\t\t\t   num_img_extents, fctx);\n\n\timg_req->data_type = OBJ_REQUEST_OWN_BVECS;\n\n\t \n\tfctx->iter = *fctx->pos;\n\tfor (i = 0; i < num_img_extents; i++) {\n\t\tret = ceph_file_to_extents(&rbd_dev->layout,\n\t\t\t\t\t   img_extents[i].fe_off,\n\t\t\t\t\t   img_extents[i].fe_len,\n\t\t\t\t\t   &img_req->object_extents,\n\t\t\t\t\t   alloc_object_extent, img_req,\n\t\t\t\t\t   fctx->count_fn, &fctx->iter);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor_each_obj_request(img_req, obj_req) {\n\t\tobj_req->bvec_pos.bvecs = kmalloc_array(obj_req->bvec_count,\n\t\t\t\t\t      sizeof(*obj_req->bvec_pos.bvecs),\n\t\t\t\t\t      GFP_NOIO);\n\t\tif (!obj_req->bvec_pos.bvecs)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfctx->iter = *fctx->pos;\n\tfor (i = 0; i < num_img_extents; i++) {\n\t\tret = ceph_iterate_extents(&rbd_dev->layout,\n\t\t\t\t\t   img_extents[i].fe_off,\n\t\t\t\t\t   img_extents[i].fe_len,\n\t\t\t\t\t   &img_req->object_extents,\n\t\t\t\t\t   fctx->copy_fn, &fctx->iter);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn __rbd_img_fill_request(img_req);\n}\n\nstatic int rbd_img_fill_nodata(struct rbd_img_request *img_req,\n\t\t\t       u64 off, u64 len)\n{\n\tstruct ceph_file_extent ex = { off, len };\n\tunion rbd_img_fill_iter dummy = {};\n\tstruct rbd_img_fill_ctx fctx = {\n\t\t.pos_type = OBJ_REQUEST_NODATA,\n\t\t.pos = &dummy,\n\t};\n\n\treturn rbd_img_fill_request(img_req, &ex, 1, &fctx);\n}\n\nstatic void set_bio_pos(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bio_iter *it = arg;\n\n\tdout(\"%s objno %llu bytes %u\\n\", __func__, ex->oe_objno, bytes);\n\tobj_req->bio_pos = *it;\n\tceph_bio_iter_advance(it, bytes);\n}\n\nstatic void count_bio_bvecs(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bio_iter *it = arg;\n\n\tdout(\"%s objno %llu bytes %u\\n\", __func__, ex->oe_objno, bytes);\n\tceph_bio_iter_advance_step(it, bytes, ({\n\t\tobj_req->bvec_count++;\n\t}));\n\n}\n\nstatic void copy_bio_bvecs(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bio_iter *it = arg;\n\n\tdout(\"%s objno %llu bytes %u\\n\", __func__, ex->oe_objno, bytes);\n\tceph_bio_iter_advance_step(it, bytes, ({\n\t\tobj_req->bvec_pos.bvecs[obj_req->bvec_idx++] = bv;\n\t\tobj_req->bvec_pos.iter.bi_size += bv.bv_len;\n\t}));\n}\n\nstatic int __rbd_img_fill_from_bio(struct rbd_img_request *img_req,\n\t\t\t\t   struct ceph_file_extent *img_extents,\n\t\t\t\t   u32 num_img_extents,\n\t\t\t\t   struct ceph_bio_iter *bio_pos)\n{\n\tstruct rbd_img_fill_ctx fctx = {\n\t\t.pos_type = OBJ_REQUEST_BIO,\n\t\t.pos = (union rbd_img_fill_iter *)bio_pos,\n\t\t.set_pos_fn = set_bio_pos,\n\t\t.count_fn = count_bio_bvecs,\n\t\t.copy_fn = copy_bio_bvecs,\n\t};\n\n\treturn rbd_img_fill_request(img_req, img_extents, num_img_extents,\n\t\t\t\t    &fctx);\n}\n\nstatic int rbd_img_fill_from_bio(struct rbd_img_request *img_req,\n\t\t\t\t u64 off, u64 len, struct bio *bio)\n{\n\tstruct ceph_file_extent ex = { off, len };\n\tstruct ceph_bio_iter it = { .bio = bio, .iter = bio->bi_iter };\n\n\treturn __rbd_img_fill_from_bio(img_req, &ex, 1, &it);\n}\n\nstatic void set_bvec_pos(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bvec_iter *it = arg;\n\n\tobj_req->bvec_pos = *it;\n\tceph_bvec_iter_shorten(&obj_req->bvec_pos, bytes);\n\tceph_bvec_iter_advance(it, bytes);\n}\n\nstatic void count_bvecs(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bvec_iter *it = arg;\n\n\tceph_bvec_iter_advance_step(it, bytes, ({\n\t\tobj_req->bvec_count++;\n\t}));\n}\n\nstatic void copy_bvecs(struct ceph_object_extent *ex, u32 bytes, void *arg)\n{\n\tstruct rbd_obj_request *obj_req =\n\t    container_of(ex, struct rbd_obj_request, ex);\n\tstruct ceph_bvec_iter *it = arg;\n\n\tceph_bvec_iter_advance_step(it, bytes, ({\n\t\tobj_req->bvec_pos.bvecs[obj_req->bvec_idx++] = bv;\n\t\tobj_req->bvec_pos.iter.bi_size += bv.bv_len;\n\t}));\n}\n\nstatic int __rbd_img_fill_from_bvecs(struct rbd_img_request *img_req,\n\t\t\t\t     struct ceph_file_extent *img_extents,\n\t\t\t\t     u32 num_img_extents,\n\t\t\t\t     struct ceph_bvec_iter *bvec_pos)\n{\n\tstruct rbd_img_fill_ctx fctx = {\n\t\t.pos_type = OBJ_REQUEST_BVECS,\n\t\t.pos = (union rbd_img_fill_iter *)bvec_pos,\n\t\t.set_pos_fn = set_bvec_pos,\n\t\t.count_fn = count_bvecs,\n\t\t.copy_fn = copy_bvecs,\n\t};\n\n\treturn rbd_img_fill_request(img_req, img_extents, num_img_extents,\n\t\t\t\t    &fctx);\n}\n\nstatic int rbd_img_fill_from_bvecs(struct rbd_img_request *img_req,\n\t\t\t\t   struct ceph_file_extent *img_extents,\n\t\t\t\t   u32 num_img_extents,\n\t\t\t\t   struct bio_vec *bvecs)\n{\n\tstruct ceph_bvec_iter it = {\n\t\t.bvecs = bvecs,\n\t\t.iter = { .bi_size = ceph_file_extents_bytes(img_extents,\n\t\t\t\t\t\t\t     num_img_extents) },\n\t};\n\n\treturn __rbd_img_fill_from_bvecs(img_req, img_extents, num_img_extents,\n\t\t\t\t\t &it);\n}\n\nstatic void rbd_img_handle_request_work(struct work_struct *work)\n{\n\tstruct rbd_img_request *img_req =\n\t    container_of(work, struct rbd_img_request, work);\n\n\trbd_img_handle_request(img_req, img_req->work_result);\n}\n\nstatic void rbd_img_schedule(struct rbd_img_request *img_req, int result)\n{\n\tINIT_WORK(&img_req->work, rbd_img_handle_request_work);\n\timg_req->work_result = result;\n\tqueue_work(rbd_wq, &img_req->work);\n}\n\nstatic bool rbd_obj_may_exist(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\n\tif (rbd_object_map_may_exist(rbd_dev, obj_req->ex.oe_objno)) {\n\t\tobj_req->flags |= RBD_OBJ_FLAG_MAY_EXIST;\n\t\treturn true;\n\t}\n\n\tdout(\"%s %p objno %llu assuming dne\\n\", __func__, obj_req,\n\t     obj_req->ex.oe_objno);\n\treturn false;\n}\n\nstatic int rbd_obj_read_object(struct rbd_obj_request *obj_req)\n{\n\tstruct ceph_osd_request *osd_req;\n\tint ret;\n\n\tosd_req = __rbd_obj_add_osd_request(obj_req, NULL, 1);\n\tif (IS_ERR(osd_req))\n\t\treturn PTR_ERR(osd_req);\n\n\tosd_req_op_extent_init(osd_req, 0, CEPH_OSD_OP_READ,\n\t\t\t       obj_req->ex.oe_off, obj_req->ex.oe_len, 0, 0);\n\trbd_osd_setup_data(osd_req, 0);\n\trbd_osd_format_read(osd_req);\n\n\tret = ceph_osdc_alloc_messages(osd_req, GFP_NOIO);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_osd_submit(osd_req);\n\treturn 0;\n}\n\nstatic int rbd_obj_read_from_parent(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_img_request *img_req = obj_req->img_request;\n\tstruct rbd_device *parent = img_req->rbd_dev->parent;\n\tstruct rbd_img_request *child_img_req;\n\tint ret;\n\n\tchild_img_req = kmem_cache_alloc(rbd_img_request_cache, GFP_NOIO);\n\tif (!child_img_req)\n\t\treturn -ENOMEM;\n\n\trbd_img_request_init(child_img_req, parent, OBJ_OP_READ);\n\t__set_bit(IMG_REQ_CHILD, &child_img_req->flags);\n\tchild_img_req->obj_request = obj_req;\n\n\tdown_read(&parent->header_rwsem);\n\trbd_img_capture_header(child_img_req);\n\tup_read(&parent->header_rwsem);\n\n\tdout(\"%s child_img_req %p for obj_req %p\\n\", __func__, child_img_req,\n\t     obj_req);\n\n\tif (!rbd_img_is_write(img_req)) {\n\t\tswitch (img_req->data_type) {\n\t\tcase OBJ_REQUEST_BIO:\n\t\t\tret = __rbd_img_fill_from_bio(child_img_req,\n\t\t\t\t\t\t      obj_req->img_extents,\n\t\t\t\t\t\t      obj_req->num_img_extents,\n\t\t\t\t\t\t      &obj_req->bio_pos);\n\t\t\tbreak;\n\t\tcase OBJ_REQUEST_BVECS:\n\t\tcase OBJ_REQUEST_OWN_BVECS:\n\t\t\tret = __rbd_img_fill_from_bvecs(child_img_req,\n\t\t\t\t\t\t      obj_req->img_extents,\n\t\t\t\t\t\t      obj_req->num_img_extents,\n\t\t\t\t\t\t      &obj_req->bvec_pos);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t} else {\n\t\tret = rbd_img_fill_from_bvecs(child_img_req,\n\t\t\t\t\t      obj_req->img_extents,\n\t\t\t\t\t      obj_req->num_img_extents,\n\t\t\t\t\t      obj_req->copyup_bvecs);\n\t}\n\tif (ret) {\n\t\trbd_img_request_destroy(child_img_req);\n\t\treturn ret;\n\t}\n\n\t \n\trbd_img_schedule(child_img_req, 0);\n\treturn 0;\n}\n\nstatic bool rbd_obj_advance_read(struct rbd_obj_request *obj_req, int *result)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tint ret;\n\nagain:\n\tswitch (obj_req->read_state) {\n\tcase RBD_OBJ_READ_START:\n\t\trbd_assert(!*result);\n\n\t\tif (!rbd_obj_may_exist(obj_req)) {\n\t\t\t*result = -ENOENT;\n\t\t\tobj_req->read_state = RBD_OBJ_READ_OBJECT;\n\t\t\tgoto again;\n\t\t}\n\n\t\tret = rbd_obj_read_object(obj_req);\n\t\tif (ret) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\tobj_req->read_state = RBD_OBJ_READ_OBJECT;\n\t\treturn false;\n\tcase RBD_OBJ_READ_OBJECT:\n\t\tif (*result == -ENOENT && rbd_dev->parent_overlap) {\n\t\t\t \n\t\t\tret = rbd_obj_calc_img_extents(obj_req, false);\n\t\t\tif (ret) {\n\t\t\t\t*result = ret;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tif (obj_req->num_img_extents) {\n\t\t\t\tret = rbd_obj_read_from_parent(obj_req);\n\t\t\t\tif (ret) {\n\t\t\t\t\t*result = ret;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tobj_req->read_state = RBD_OBJ_READ_PARENT;\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (*result == -ENOENT) {\n\t\t\trbd_obj_zero_range(obj_req, 0, obj_req->ex.oe_len);\n\t\t\t*result = 0;\n\t\t} else if (*result >= 0) {\n\t\t\tif (*result < obj_req->ex.oe_len)\n\t\t\t\trbd_obj_zero_range(obj_req, *result,\n\t\t\t\t\t\tobj_req->ex.oe_len - *result);\n\t\t\telse\n\t\t\t\trbd_assert(*result == obj_req->ex.oe_len);\n\t\t\t*result = 0;\n\t\t}\n\t\treturn true;\n\tcase RBD_OBJ_READ_PARENT:\n\t\t \n\t\tif (!*result) {\n\t\t\tu32 obj_overlap = rbd_obj_img_extents_bytes(obj_req);\n\n\t\t\tif (obj_overlap < obj_req->ex.oe_len)\n\t\t\t\trbd_obj_zero_range(obj_req, obj_overlap,\n\t\t\t\t\t    obj_req->ex.oe_len - obj_overlap);\n\t\t}\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic bool rbd_obj_write_is_noop(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\n\tif (rbd_object_map_may_exist(rbd_dev, obj_req->ex.oe_objno))\n\t\tobj_req->flags |= RBD_OBJ_FLAG_MAY_EXIST;\n\n\tif (!(obj_req->flags & RBD_OBJ_FLAG_MAY_EXIST) &&\n\t    (obj_req->flags & RBD_OBJ_FLAG_NOOP_FOR_NONEXISTENT)) {\n\t\tdout(\"%s %p noop for nonexistent\\n\", __func__, obj_req);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int rbd_obj_write_pre_object_map(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tu8 new_state;\n\n\tif (!(rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP))\n\t\treturn 1;\n\n\tif (obj_req->flags & RBD_OBJ_FLAG_DELETION)\n\t\tnew_state = OBJECT_PENDING;\n\telse\n\t\tnew_state = OBJECT_EXISTS;\n\n\treturn rbd_object_map_update(obj_req, CEPH_NOSNAP, new_state, NULL);\n}\n\nstatic int rbd_obj_write_object(struct rbd_obj_request *obj_req)\n{\n\tstruct ceph_osd_request *osd_req;\n\tint num_ops = count_write_ops(obj_req);\n\tint which = 0;\n\tint ret;\n\n\tif (obj_req->flags & RBD_OBJ_FLAG_COPYUP_ENABLED)\n\t\tnum_ops++;  \n\n\tosd_req = rbd_obj_add_osd_request(obj_req, num_ops);\n\tif (IS_ERR(osd_req))\n\t\treturn PTR_ERR(osd_req);\n\n\tif (obj_req->flags & RBD_OBJ_FLAG_COPYUP_ENABLED) {\n\t\tret = rbd_osd_setup_stat(osd_req, which++);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\trbd_osd_setup_write_ops(osd_req, which);\n\trbd_osd_format_write(osd_req);\n\n\tret = ceph_osdc_alloc_messages(osd_req, GFP_NOIO);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_osd_submit(osd_req);\n\treturn 0;\n}\n\n \nstatic bool is_zero_bvecs(struct bio_vec *bvecs, u32 bytes)\n{\n\tstruct ceph_bvec_iter it = {\n\t\t.bvecs = bvecs,\n\t\t.iter = { .bi_size = bytes },\n\t};\n\n\tceph_bvec_iter_advance_step(&it, bytes, ({\n\t\tif (memchr_inv(bvec_virt(&bv), 0, bv.bv_len))\n\t\t\treturn false;\n\t}));\n\treturn true;\n}\n\n#define MODS_ONLY\tU32_MAX\n\nstatic int rbd_obj_copyup_empty_snapc(struct rbd_obj_request *obj_req,\n\t\t\t\t      u32 bytes)\n{\n\tstruct ceph_osd_request *osd_req;\n\tint ret;\n\n\tdout(\"%s obj_req %p bytes %u\\n\", __func__, obj_req, bytes);\n\trbd_assert(bytes > 0 && bytes != MODS_ONLY);\n\n\tosd_req = __rbd_obj_add_osd_request(obj_req, &rbd_empty_snapc, 1);\n\tif (IS_ERR(osd_req))\n\t\treturn PTR_ERR(osd_req);\n\n\tret = rbd_osd_setup_copyup(osd_req, 0, bytes);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_osd_format_write(osd_req);\n\n\tret = ceph_osdc_alloc_messages(osd_req, GFP_NOIO);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_osd_submit(osd_req);\n\treturn 0;\n}\n\nstatic int rbd_obj_copyup_current_snapc(struct rbd_obj_request *obj_req,\n\t\t\t\t\tu32 bytes)\n{\n\tstruct ceph_osd_request *osd_req;\n\tint num_ops = count_write_ops(obj_req);\n\tint which = 0;\n\tint ret;\n\n\tdout(\"%s obj_req %p bytes %u\\n\", __func__, obj_req, bytes);\n\n\tif (bytes != MODS_ONLY)\n\t\tnum_ops++;  \n\n\tosd_req = rbd_obj_add_osd_request(obj_req, num_ops);\n\tif (IS_ERR(osd_req))\n\t\treturn PTR_ERR(osd_req);\n\n\tif (bytes != MODS_ONLY) {\n\t\tret = rbd_osd_setup_copyup(osd_req, which++, bytes);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\trbd_osd_setup_write_ops(osd_req, which);\n\trbd_osd_format_write(osd_req);\n\n\tret = ceph_osdc_alloc_messages(osd_req, GFP_NOIO);\n\tif (ret)\n\t\treturn ret;\n\n\trbd_osd_submit(osd_req);\n\treturn 0;\n}\n\nstatic int setup_copyup_bvecs(struct rbd_obj_request *obj_req, u64 obj_overlap)\n{\n\tu32 i;\n\n\trbd_assert(!obj_req->copyup_bvecs);\n\tobj_req->copyup_bvec_count = calc_pages_for(0, obj_overlap);\n\tobj_req->copyup_bvecs = kcalloc(obj_req->copyup_bvec_count,\n\t\t\t\t\tsizeof(*obj_req->copyup_bvecs),\n\t\t\t\t\tGFP_NOIO);\n\tif (!obj_req->copyup_bvecs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < obj_req->copyup_bvec_count; i++) {\n\t\tunsigned int len = min(obj_overlap, (u64)PAGE_SIZE);\n\t\tstruct page *page = alloc_page(GFP_NOIO);\n\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\tbvec_set_page(&obj_req->copyup_bvecs[i], page, len, 0);\n\t\tobj_overlap -= len;\n\t}\n\n\trbd_assert(!obj_overlap);\n\treturn 0;\n}\n\n \nstatic int rbd_obj_copyup_read_parent(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tint ret;\n\n\trbd_assert(obj_req->num_img_extents);\n\tprune_extents(obj_req->img_extents, &obj_req->num_img_extents,\n\t\t      rbd_dev->parent_overlap);\n\tif (!obj_req->num_img_extents) {\n\t\t \n\t\treturn rbd_obj_copyup_current_snapc(obj_req, MODS_ONLY);\n\t}\n\n\tret = setup_copyup_bvecs(obj_req, rbd_obj_img_extents_bytes(obj_req));\n\tif (ret)\n\t\treturn ret;\n\n\treturn rbd_obj_read_from_parent(obj_req);\n}\n\nstatic void rbd_obj_copyup_object_maps(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tstruct ceph_snap_context *snapc = obj_req->img_request->snapc;\n\tu8 new_state;\n\tu32 i;\n\tint ret;\n\n\trbd_assert(!obj_req->pending.result && !obj_req->pending.num_pending);\n\n\tif (!(rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP))\n\t\treturn;\n\n\tif (obj_req->flags & RBD_OBJ_FLAG_COPYUP_ZEROS)\n\t\treturn;\n\n\tfor (i = 0; i < snapc->num_snaps; i++) {\n\t\tif ((rbd_dev->header.features & RBD_FEATURE_FAST_DIFF) &&\n\t\t    i + 1 < snapc->num_snaps)\n\t\t\tnew_state = OBJECT_EXISTS_CLEAN;\n\t\telse\n\t\t\tnew_state = OBJECT_EXISTS;\n\n\t\tret = rbd_object_map_update(obj_req, snapc->snaps[i],\n\t\t\t\t\t    new_state, NULL);\n\t\tif (ret < 0) {\n\t\t\tobj_req->pending.result = ret;\n\t\t\treturn;\n\t\t}\n\n\t\trbd_assert(!ret);\n\t\tobj_req->pending.num_pending++;\n\t}\n}\n\nstatic void rbd_obj_copyup_write_object(struct rbd_obj_request *obj_req)\n{\n\tu32 bytes = rbd_obj_img_extents_bytes(obj_req);\n\tint ret;\n\n\trbd_assert(!obj_req->pending.result && !obj_req->pending.num_pending);\n\n\t \n\tif (obj_req->flags & RBD_OBJ_FLAG_COPYUP_ZEROS)\n\t\tbytes = 0;\n\n\tif (obj_req->img_request->snapc->num_snaps && bytes > 0) {\n\t\t \n\t\tret = rbd_obj_copyup_empty_snapc(obj_req, bytes);\n\t\tif (ret) {\n\t\t\tobj_req->pending.result = ret;\n\t\t\treturn;\n\t\t}\n\n\t\tobj_req->pending.num_pending++;\n\t\tbytes = MODS_ONLY;\n\t}\n\n\tret = rbd_obj_copyup_current_snapc(obj_req, bytes);\n\tif (ret) {\n\t\tobj_req->pending.result = ret;\n\t\treturn;\n\t}\n\n\tobj_req->pending.num_pending++;\n}\n\nstatic bool rbd_obj_advance_copyup(struct rbd_obj_request *obj_req, int *result)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tint ret;\n\nagain:\n\tswitch (obj_req->copyup_state) {\n\tcase RBD_OBJ_COPYUP_START:\n\t\trbd_assert(!*result);\n\n\t\tret = rbd_obj_copyup_read_parent(obj_req);\n\t\tif (ret) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\tif (obj_req->num_img_extents)\n\t\t\tobj_req->copyup_state = RBD_OBJ_COPYUP_READ_PARENT;\n\t\telse\n\t\t\tobj_req->copyup_state = RBD_OBJ_COPYUP_WRITE_OBJECT;\n\t\treturn false;\n\tcase RBD_OBJ_COPYUP_READ_PARENT:\n\t\tif (*result)\n\t\t\treturn true;\n\n\t\tif (is_zero_bvecs(obj_req->copyup_bvecs,\n\t\t\t\t  rbd_obj_img_extents_bytes(obj_req))) {\n\t\t\tdout(\"%s %p detected zeros\\n\", __func__, obj_req);\n\t\t\tobj_req->flags |= RBD_OBJ_FLAG_COPYUP_ZEROS;\n\t\t}\n\n\t\trbd_obj_copyup_object_maps(obj_req);\n\t\tif (!obj_req->pending.num_pending) {\n\t\t\t*result = obj_req->pending.result;\n\t\t\tobj_req->copyup_state = RBD_OBJ_COPYUP_OBJECT_MAPS;\n\t\t\tgoto again;\n\t\t}\n\t\tobj_req->copyup_state = __RBD_OBJ_COPYUP_OBJECT_MAPS;\n\t\treturn false;\n\tcase __RBD_OBJ_COPYUP_OBJECT_MAPS:\n\t\tif (!pending_result_dec(&obj_req->pending, result))\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase RBD_OBJ_COPYUP_OBJECT_MAPS:\n\t\tif (*result) {\n\t\t\trbd_warn(rbd_dev, \"snap object map update failed: %d\",\n\t\t\t\t *result);\n\t\t\treturn true;\n\t\t}\n\n\t\trbd_obj_copyup_write_object(obj_req);\n\t\tif (!obj_req->pending.num_pending) {\n\t\t\t*result = obj_req->pending.result;\n\t\t\tobj_req->copyup_state = RBD_OBJ_COPYUP_WRITE_OBJECT;\n\t\t\tgoto again;\n\t\t}\n\t\tobj_req->copyup_state = __RBD_OBJ_COPYUP_WRITE_OBJECT;\n\t\treturn false;\n\tcase __RBD_OBJ_COPYUP_WRITE_OBJECT:\n\t\tif (!pending_result_dec(&obj_req->pending, result))\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase RBD_OBJ_COPYUP_WRITE_OBJECT:\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic int rbd_obj_write_post_object_map(struct rbd_obj_request *obj_req)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tu8 current_state = OBJECT_PENDING;\n\n\tif (!(rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP))\n\t\treturn 1;\n\n\tif (!(obj_req->flags & RBD_OBJ_FLAG_DELETION))\n\t\treturn 1;\n\n\treturn rbd_object_map_update(obj_req, CEPH_NOSNAP, OBJECT_NONEXISTENT,\n\t\t\t\t     &current_state);\n}\n\nstatic bool rbd_obj_advance_write(struct rbd_obj_request *obj_req, int *result)\n{\n\tstruct rbd_device *rbd_dev = obj_req->img_request->rbd_dev;\n\tint ret;\n\nagain:\n\tswitch (obj_req->write_state) {\n\tcase RBD_OBJ_WRITE_START:\n\t\trbd_assert(!*result);\n\n\t\trbd_obj_set_copyup_enabled(obj_req);\n\t\tif (rbd_obj_write_is_noop(obj_req))\n\t\t\treturn true;\n\n\t\tret = rbd_obj_write_pre_object_map(obj_req);\n\t\tif (ret < 0) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\tobj_req->write_state = RBD_OBJ_WRITE_PRE_OBJECT_MAP;\n\t\tif (ret > 0)\n\t\t\tgoto again;\n\t\treturn false;\n\tcase RBD_OBJ_WRITE_PRE_OBJECT_MAP:\n\t\tif (*result) {\n\t\t\trbd_warn(rbd_dev, \"pre object map update failed: %d\",\n\t\t\t\t *result);\n\t\t\treturn true;\n\t\t}\n\t\tret = rbd_obj_write_object(obj_req);\n\t\tif (ret) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\tobj_req->write_state = RBD_OBJ_WRITE_OBJECT;\n\t\treturn false;\n\tcase RBD_OBJ_WRITE_OBJECT:\n\t\tif (*result == -ENOENT) {\n\t\t\tif (obj_req->flags & RBD_OBJ_FLAG_COPYUP_ENABLED) {\n\t\t\t\t*result = 0;\n\t\t\t\tobj_req->copyup_state = RBD_OBJ_COPYUP_START;\n\t\t\t\tobj_req->write_state = __RBD_OBJ_WRITE_COPYUP;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\t \n\t\t\tif (obj_req->flags & RBD_OBJ_FLAG_DELETION)\n\t\t\t\t*result = 0;\n\t\t}\n\t\tif (*result)\n\t\t\treturn true;\n\n\t\tobj_req->write_state = RBD_OBJ_WRITE_COPYUP;\n\t\tgoto again;\n\tcase __RBD_OBJ_WRITE_COPYUP:\n\t\tif (!rbd_obj_advance_copyup(obj_req, result))\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase RBD_OBJ_WRITE_COPYUP:\n\t\tif (*result) {\n\t\t\trbd_warn(rbd_dev, \"copyup failed: %d\", *result);\n\t\t\treturn true;\n\t\t}\n\t\tret = rbd_obj_write_post_object_map(obj_req);\n\t\tif (ret < 0) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\tobj_req->write_state = RBD_OBJ_WRITE_POST_OBJECT_MAP;\n\t\tif (ret > 0)\n\t\t\tgoto again;\n\t\treturn false;\n\tcase RBD_OBJ_WRITE_POST_OBJECT_MAP:\n\t\tif (*result)\n\t\t\trbd_warn(rbd_dev, \"post object map update failed: %d\",\n\t\t\t\t *result);\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic bool __rbd_obj_handle_request(struct rbd_obj_request *obj_req,\n\t\t\t\t     int *result)\n{\n\tstruct rbd_img_request *img_req = obj_req->img_request;\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tbool done;\n\n\tmutex_lock(&obj_req->state_mutex);\n\tif (!rbd_img_is_write(img_req))\n\t\tdone = rbd_obj_advance_read(obj_req, result);\n\telse\n\t\tdone = rbd_obj_advance_write(obj_req, result);\n\tmutex_unlock(&obj_req->state_mutex);\n\n\tif (done && *result) {\n\t\trbd_assert(*result < 0);\n\t\trbd_warn(rbd_dev, \"%s at objno %llu %llu~%llu result %d\",\n\t\t\t obj_op_name(img_req->op_type), obj_req->ex.oe_objno,\n\t\t\t obj_req->ex.oe_off, obj_req->ex.oe_len, *result);\n\t}\n\treturn done;\n}\n\n \nstatic void rbd_obj_handle_request(struct rbd_obj_request *obj_req, int result)\n{\n\tif (__rbd_obj_handle_request(obj_req, &result))\n\t\trbd_img_handle_request(obj_req->img_request, result);\n}\n\nstatic bool need_exclusive_lock(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\n\tif (!(rbd_dev->header.features & RBD_FEATURE_EXCLUSIVE_LOCK))\n\t\treturn false;\n\n\tif (rbd_is_ro(rbd_dev))\n\t\treturn false;\n\n\trbd_assert(!test_bit(IMG_REQ_CHILD, &img_req->flags));\n\tif (rbd_dev->opts->lock_on_read ||\n\t    (rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP))\n\t\treturn true;\n\n\treturn rbd_img_is_write(img_req);\n}\n\nstatic bool rbd_lock_add_request(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tbool locked;\n\n\tlockdep_assert_held(&rbd_dev->lock_rwsem);\n\tlocked = rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED;\n\tspin_lock(&rbd_dev->lock_lists_lock);\n\trbd_assert(list_empty(&img_req->lock_item));\n\tif (!locked)\n\t\tlist_add_tail(&img_req->lock_item, &rbd_dev->acquiring_list);\n\telse\n\t\tlist_add_tail(&img_req->lock_item, &rbd_dev->running_list);\n\tspin_unlock(&rbd_dev->lock_lists_lock);\n\treturn locked;\n}\n\nstatic void rbd_lock_del_request(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tbool need_wakeup;\n\n\tlockdep_assert_held(&rbd_dev->lock_rwsem);\n\tspin_lock(&rbd_dev->lock_lists_lock);\n\trbd_assert(!list_empty(&img_req->lock_item));\n\tlist_del_init(&img_req->lock_item);\n\tneed_wakeup = (rbd_dev->lock_state == RBD_LOCK_STATE_RELEASING &&\n\t\t       list_empty(&rbd_dev->running_list));\n\tspin_unlock(&rbd_dev->lock_lists_lock);\n\tif (need_wakeup)\n\t\tcomplete(&rbd_dev->releasing_wait);\n}\n\nstatic int rbd_img_exclusive_lock(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\n\tif (!need_exclusive_lock(img_req))\n\t\treturn 1;\n\n\tif (rbd_lock_add_request(img_req))\n\t\treturn 1;\n\n\tif (rbd_dev->opts->exclusive) {\n\t\tWARN_ON(1);  \n\t\treturn -EROFS;\n\t}\n\n\t \n\tdout(\"%s rbd_dev %p queueing lock_dwork\\n\", __func__, rbd_dev);\n\tqueue_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork, 0);\n\treturn 0;\n}\n\nstatic void rbd_img_object_requests(struct rbd_img_request *img_req)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tstruct rbd_obj_request *obj_req;\n\n\trbd_assert(!img_req->pending.result && !img_req->pending.num_pending);\n\trbd_assert(!need_exclusive_lock(img_req) ||\n\t\t   __rbd_is_lock_owner(rbd_dev));\n\n\tif (rbd_img_is_write(img_req)) {\n\t\trbd_assert(!img_req->snapc);\n\t\tdown_read(&rbd_dev->header_rwsem);\n\t\timg_req->snapc = ceph_get_snap_context(rbd_dev->header.snapc);\n\t\tup_read(&rbd_dev->header_rwsem);\n\t}\n\n\tfor_each_obj_request(img_req, obj_req) {\n\t\tint result = 0;\n\n\t\tif (__rbd_obj_handle_request(obj_req, &result)) {\n\t\t\tif (result) {\n\t\t\t\timg_req->pending.result = result;\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else {\n\t\t\timg_req->pending.num_pending++;\n\t\t}\n\t}\n}\n\nstatic bool rbd_img_advance(struct rbd_img_request *img_req, int *result)\n{\n\tint ret;\n\nagain:\n\tswitch (img_req->state) {\n\tcase RBD_IMG_START:\n\t\trbd_assert(!*result);\n\n\t\tret = rbd_img_exclusive_lock(img_req);\n\t\tif (ret < 0) {\n\t\t\t*result = ret;\n\t\t\treturn true;\n\t\t}\n\t\timg_req->state = RBD_IMG_EXCLUSIVE_LOCK;\n\t\tif (ret > 0)\n\t\t\tgoto again;\n\t\treturn false;\n\tcase RBD_IMG_EXCLUSIVE_LOCK:\n\t\tif (*result)\n\t\t\treturn true;\n\n\t\trbd_img_object_requests(img_req);\n\t\tif (!img_req->pending.num_pending) {\n\t\t\t*result = img_req->pending.result;\n\t\t\timg_req->state = RBD_IMG_OBJECT_REQUESTS;\n\t\t\tgoto again;\n\t\t}\n\t\timg_req->state = __RBD_IMG_OBJECT_REQUESTS;\n\t\treturn false;\n\tcase __RBD_IMG_OBJECT_REQUESTS:\n\t\tif (!pending_result_dec(&img_req->pending, result))\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase RBD_IMG_OBJECT_REQUESTS:\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic bool __rbd_img_handle_request(struct rbd_img_request *img_req,\n\t\t\t\t     int *result)\n{\n\tstruct rbd_device *rbd_dev = img_req->rbd_dev;\n\tbool done;\n\n\tif (need_exclusive_lock(img_req)) {\n\t\tdown_read(&rbd_dev->lock_rwsem);\n\t\tmutex_lock(&img_req->state_mutex);\n\t\tdone = rbd_img_advance(img_req, result);\n\t\tif (done)\n\t\t\trbd_lock_del_request(img_req);\n\t\tmutex_unlock(&img_req->state_mutex);\n\t\tup_read(&rbd_dev->lock_rwsem);\n\t} else {\n\t\tmutex_lock(&img_req->state_mutex);\n\t\tdone = rbd_img_advance(img_req, result);\n\t\tmutex_unlock(&img_req->state_mutex);\n\t}\n\n\tif (done && *result) {\n\t\trbd_assert(*result < 0);\n\t\trbd_warn(rbd_dev, \"%s%s result %d\",\n\t\t      test_bit(IMG_REQ_CHILD, &img_req->flags) ? \"child \" : \"\",\n\t\t      obj_op_name(img_req->op_type), *result);\n\t}\n\treturn done;\n}\n\nstatic void rbd_img_handle_request(struct rbd_img_request *img_req, int result)\n{\nagain:\n\tif (!__rbd_img_handle_request(img_req, &result))\n\t\treturn;\n\n\tif (test_bit(IMG_REQ_CHILD, &img_req->flags)) {\n\t\tstruct rbd_obj_request *obj_req = img_req->obj_request;\n\n\t\trbd_img_request_destroy(img_req);\n\t\tif (__rbd_obj_handle_request(obj_req, &result)) {\n\t\t\timg_req = obj_req->img_request;\n\t\t\tgoto again;\n\t\t}\n\t} else {\n\t\tstruct request *rq = blk_mq_rq_from_pdu(img_req);\n\n\t\trbd_img_request_destroy(img_req);\n\t\tblk_mq_end_request(rq, errno_to_blk_status(result));\n\t}\n}\n\nstatic const struct rbd_client_id rbd_empty_cid;\n\nstatic bool rbd_cid_equal(const struct rbd_client_id *lhs,\n\t\t\t  const struct rbd_client_id *rhs)\n{\n\treturn lhs->gid == rhs->gid && lhs->handle == rhs->handle;\n}\n\nstatic struct rbd_client_id rbd_get_cid(struct rbd_device *rbd_dev)\n{\n\tstruct rbd_client_id cid;\n\n\tmutex_lock(&rbd_dev->watch_mutex);\n\tcid.gid = ceph_client_gid(rbd_dev->rbd_client->client);\n\tcid.handle = rbd_dev->watch_cookie;\n\tmutex_unlock(&rbd_dev->watch_mutex);\n\treturn cid;\n}\n\n \nstatic void rbd_set_owner_cid(struct rbd_device *rbd_dev,\n\t\t\t      const struct rbd_client_id *cid)\n{\n\tdout(\"%s rbd_dev %p %llu-%llu -> %llu-%llu\\n\", __func__, rbd_dev,\n\t     rbd_dev->owner_cid.gid, rbd_dev->owner_cid.handle,\n\t     cid->gid, cid->handle);\n\trbd_dev->owner_cid = *cid;  \n}\n\nstatic void format_lock_cookie(struct rbd_device *rbd_dev, char *buf)\n{\n\tmutex_lock(&rbd_dev->watch_mutex);\n\tsprintf(buf, \"%s %llu\", RBD_LOCK_COOKIE_PREFIX, rbd_dev->watch_cookie);\n\tmutex_unlock(&rbd_dev->watch_mutex);\n}\n\nstatic void __rbd_lock(struct rbd_device *rbd_dev, const char *cookie)\n{\n\tstruct rbd_client_id cid = rbd_get_cid(rbd_dev);\n\n\trbd_dev->lock_state = RBD_LOCK_STATE_LOCKED;\n\tstrcpy(rbd_dev->lock_cookie, cookie);\n\trbd_set_owner_cid(rbd_dev, &cid);\n\tqueue_work(rbd_dev->task_wq, &rbd_dev->acquired_lock_work);\n}\n\n \nstatic int rbd_lock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tchar cookie[32];\n\tint ret;\n\n\tWARN_ON(__rbd_is_lock_owner(rbd_dev) ||\n\t\trbd_dev->lock_cookie[0] != '\\0');\n\n\tformat_lock_cookie(rbd_dev, cookie);\n\tret = ceph_cls_lock(osdc, &rbd_dev->header_oid, &rbd_dev->header_oloc,\n\t\t\t    RBD_LOCK_NAME, CEPH_CLS_LOCK_EXCLUSIVE, cookie,\n\t\t\t    RBD_LOCK_TAG, \"\", 0);\n\tif (ret && ret != -EEXIST)\n\t\treturn ret;\n\n\t__rbd_lock(rbd_dev, cookie);\n\treturn 0;\n}\n\n \nstatic void rbd_unlock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tint ret;\n\n\tWARN_ON(!__rbd_is_lock_owner(rbd_dev) ||\n\t\trbd_dev->lock_cookie[0] == '\\0');\n\n\tret = ceph_cls_unlock(osdc, &rbd_dev->header_oid, &rbd_dev->header_oloc,\n\t\t\t      RBD_LOCK_NAME, rbd_dev->lock_cookie);\n\tif (ret && ret != -ENOENT)\n\t\trbd_warn(rbd_dev, \"failed to unlock header: %d\", ret);\n\n\t \n\trbd_dev->lock_state = RBD_LOCK_STATE_UNLOCKED;\n\trbd_dev->lock_cookie[0] = '\\0';\n\trbd_set_owner_cid(rbd_dev, &rbd_empty_cid);\n\tqueue_work(rbd_dev->task_wq, &rbd_dev->released_lock_work);\n}\n\nstatic int __rbd_notify_op_lock(struct rbd_device *rbd_dev,\n\t\t\t\tenum rbd_notify_op notify_op,\n\t\t\t\tstruct page ***preply_pages,\n\t\t\t\tsize_t *preply_len)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct rbd_client_id cid = rbd_get_cid(rbd_dev);\n\tchar buf[4 + 8 + 8 + CEPH_ENCODING_START_BLK_LEN];\n\tint buf_size = sizeof(buf);\n\tvoid *p = buf;\n\n\tdout(\"%s rbd_dev %p notify_op %d\\n\", __func__, rbd_dev, notify_op);\n\n\t \n\tceph_start_encoding(&p, 2, 1, buf_size - CEPH_ENCODING_START_BLK_LEN);\n\tceph_encode_32(&p, notify_op);\n\tceph_encode_64(&p, cid.gid);\n\tceph_encode_64(&p, cid.handle);\n\n\treturn ceph_osdc_notify(osdc, &rbd_dev->header_oid,\n\t\t\t\t&rbd_dev->header_oloc, buf, buf_size,\n\t\t\t\tRBD_NOTIFY_TIMEOUT, preply_pages, preply_len);\n}\n\nstatic void rbd_notify_op_lock(struct rbd_device *rbd_dev,\n\t\t\t       enum rbd_notify_op notify_op)\n{\n\t__rbd_notify_op_lock(rbd_dev, notify_op, NULL, NULL);\n}\n\nstatic void rbd_notify_acquired_lock(struct work_struct *work)\n{\n\tstruct rbd_device *rbd_dev = container_of(work, struct rbd_device,\n\t\t\t\t\t\t  acquired_lock_work);\n\n\trbd_notify_op_lock(rbd_dev, RBD_NOTIFY_OP_ACQUIRED_LOCK);\n}\n\nstatic void rbd_notify_released_lock(struct work_struct *work)\n{\n\tstruct rbd_device *rbd_dev = container_of(work, struct rbd_device,\n\t\t\t\t\t\t  released_lock_work);\n\n\trbd_notify_op_lock(rbd_dev, RBD_NOTIFY_OP_RELEASED_LOCK);\n}\n\nstatic int rbd_request_lock(struct rbd_device *rbd_dev)\n{\n\tstruct page **reply_pages;\n\tsize_t reply_len;\n\tbool lock_owner_responded = false;\n\tint ret;\n\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\tret = __rbd_notify_op_lock(rbd_dev, RBD_NOTIFY_OP_REQUEST_LOCK,\n\t\t\t\t   &reply_pages, &reply_len);\n\tif (ret && ret != -ETIMEDOUT) {\n\t\trbd_warn(rbd_dev, \"failed to request lock: %d\", ret);\n\t\tgoto out;\n\t}\n\n\tif (reply_len > 0 && reply_len <= PAGE_SIZE) {\n\t\tvoid *p = page_address(reply_pages[0]);\n\t\tvoid *const end = p + reply_len;\n\t\tu32 n;\n\n\t\tceph_decode_32_safe(&p, end, n, e_inval);  \n\t\twhile (n--) {\n\t\t\tu8 struct_v;\n\t\t\tu32 len;\n\n\t\t\tceph_decode_need(&p, end, 8 + 8, e_inval);\n\t\t\tp += 8 + 8;  \n\n\t\t\tceph_decode_32_safe(&p, end, len, e_inval);\n\t\t\tif (!len)\n\t\t\t\tcontinue;\n\n\t\t\tif (lock_owner_responded) {\n\t\t\t\trbd_warn(rbd_dev,\n\t\t\t\t\t \"duplicate lock owners detected\");\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tlock_owner_responded = true;\n\t\t\tret = ceph_start_decoding(&p, end, 1, \"ResponseMessage\",\n\t\t\t\t\t\t  &struct_v, &len);\n\t\t\tif (ret) {\n\t\t\t\trbd_warn(rbd_dev,\n\t\t\t\t\t \"failed to decode ResponseMessage: %d\",\n\t\t\t\t\t ret);\n\t\t\t\tgoto e_inval;\n\t\t\t}\n\n\t\t\tret = ceph_decode_32(&p);\n\t\t}\n\t}\n\n\tif (!lock_owner_responded) {\n\t\trbd_warn(rbd_dev, \"no lock owners detected\");\n\t\tret = -ETIMEDOUT;\n\t}\n\nout:\n\tceph_release_page_vector(reply_pages, calc_pages_for(0, reply_len));\n\treturn ret;\n\ne_inval:\n\tret = -EINVAL;\n\tgoto out;\n}\n\n \nstatic void wake_lock_waiters(struct rbd_device *rbd_dev, int result)\n{\n\tstruct rbd_img_request *img_req;\n\n\tdout(\"%s rbd_dev %p result %d\\n\", __func__, rbd_dev, result);\n\tlockdep_assert_held_write(&rbd_dev->lock_rwsem);\n\n\tcancel_delayed_work(&rbd_dev->lock_dwork);\n\tif (!completion_done(&rbd_dev->acquire_wait)) {\n\t\trbd_assert(list_empty(&rbd_dev->acquiring_list) &&\n\t\t\t   list_empty(&rbd_dev->running_list));\n\t\trbd_dev->acquire_err = result;\n\t\tcomplete_all(&rbd_dev->acquire_wait);\n\t\treturn;\n\t}\n\n\tlist_for_each_entry(img_req, &rbd_dev->acquiring_list, lock_item) {\n\t\tmutex_lock(&img_req->state_mutex);\n\t\trbd_assert(img_req->state == RBD_IMG_EXCLUSIVE_LOCK);\n\t\trbd_img_schedule(img_req, result);\n\t\tmutex_unlock(&img_req->state_mutex);\n\t}\n\n\tlist_splice_tail_init(&rbd_dev->acquiring_list, &rbd_dev->running_list);\n}\n\nstatic bool locker_equal(const struct ceph_locker *lhs,\n\t\t\t const struct ceph_locker *rhs)\n{\n\treturn lhs->id.name.type == rhs->id.name.type &&\n\t       lhs->id.name.num == rhs->id.name.num &&\n\t       !strcmp(lhs->id.cookie, rhs->id.cookie) &&\n\t       ceph_addr_equal_no_type(&lhs->info.addr, &rhs->info.addr);\n}\n\nstatic void free_locker(struct ceph_locker *locker)\n{\n\tif (locker)\n\t\tceph_free_lockers(locker, 1);\n}\n\nstatic struct ceph_locker *get_lock_owner_info(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_locker *lockers;\n\tu32 num_lockers;\n\tu8 lock_type;\n\tchar *lock_tag;\n\tu64 handle;\n\tint ret;\n\n\tret = ceph_cls_lock_info(osdc, &rbd_dev->header_oid,\n\t\t\t\t &rbd_dev->header_oloc, RBD_LOCK_NAME,\n\t\t\t\t &lock_type, &lock_tag, &lockers, &num_lockers);\n\tif (ret) {\n\t\trbd_warn(rbd_dev, \"failed to get header lockers: %d\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (num_lockers == 0) {\n\t\tdout(\"%s rbd_dev %p no lockers detected\\n\", __func__, rbd_dev);\n\t\tlockers = NULL;\n\t\tgoto out;\n\t}\n\n\tif (strcmp(lock_tag, RBD_LOCK_TAG)) {\n\t\trbd_warn(rbd_dev, \"locked by external mechanism, tag %s\",\n\t\t\t lock_tag);\n\t\tgoto err_busy;\n\t}\n\n\tif (lock_type != CEPH_CLS_LOCK_EXCLUSIVE) {\n\t\trbd_warn(rbd_dev, \"incompatible lock type detected\");\n\t\tgoto err_busy;\n\t}\n\n\tWARN_ON(num_lockers != 1);\n\tret = sscanf(lockers[0].id.cookie, RBD_LOCK_COOKIE_PREFIX \" %llu\",\n\t\t     &handle);\n\tif (ret != 1) {\n\t\trbd_warn(rbd_dev, \"locked by external mechanism, cookie %s\",\n\t\t\t lockers[0].id.cookie);\n\t\tgoto err_busy;\n\t}\n\tif (ceph_addr_is_blank(&lockers[0].info.addr)) {\n\t\trbd_warn(rbd_dev, \"locker has a blank address\");\n\t\tgoto err_busy;\n\t}\n\n\tdout(\"%s rbd_dev %p got locker %s%llu@%pISpc/%u handle %llu\\n\",\n\t     __func__, rbd_dev, ENTITY_NAME(lockers[0].id.name),\n\t     &lockers[0].info.addr.in_addr,\n\t     le32_to_cpu(lockers[0].info.addr.nonce), handle);\n\nout:\n\tkfree(lock_tag);\n\treturn lockers;\n\nerr_busy:\n\tkfree(lock_tag);\n\tceph_free_lockers(lockers, num_lockers);\n\treturn ERR_PTR(-EBUSY);\n}\n\nstatic int find_watcher(struct rbd_device *rbd_dev,\n\t\t\tconst struct ceph_locker *locker)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_watch_item *watchers;\n\tu32 num_watchers;\n\tu64 cookie;\n\tint i;\n\tint ret;\n\n\tret = ceph_osdc_list_watchers(osdc, &rbd_dev->header_oid,\n\t\t\t\t      &rbd_dev->header_oloc, &watchers,\n\t\t\t\t      &num_watchers);\n\tif (ret) {\n\t\trbd_warn(rbd_dev, \"failed to get watchers: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tsscanf(locker->id.cookie, RBD_LOCK_COOKIE_PREFIX \" %llu\", &cookie);\n\tfor (i = 0; i < num_watchers; i++) {\n\t\t \n\t\tif (ceph_addr_equal_no_type(&watchers[i].addr,\n\t\t\t\t\t    &locker->info.addr) &&\n\t\t    watchers[i].cookie == cookie) {\n\t\t\tstruct rbd_client_id cid = {\n\t\t\t\t.gid = le64_to_cpu(watchers[i].name.num),\n\t\t\t\t.handle = cookie,\n\t\t\t};\n\n\t\t\tdout(\"%s rbd_dev %p found cid %llu-%llu\\n\", __func__,\n\t\t\t     rbd_dev, cid.gid, cid.handle);\n\t\t\trbd_set_owner_cid(rbd_dev, &cid);\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tdout(\"%s rbd_dev %p no watchers\\n\", __func__, rbd_dev);\n\tret = 0;\nout:\n\tkfree(watchers);\n\treturn ret;\n}\n\n \nstatic int rbd_try_lock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_client *client = rbd_dev->rbd_client->client;\n\tstruct ceph_locker *locker, *refreshed_locker;\n\tint ret;\n\n\tfor (;;) {\n\t\tlocker = refreshed_locker = NULL;\n\n\t\tret = rbd_lock(rbd_dev);\n\t\tif (!ret)\n\t\t\tgoto out;\n\t\tif (ret != -EBUSY) {\n\t\t\trbd_warn(rbd_dev, \"failed to lock header: %d\", ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tlocker = get_lock_owner_info(rbd_dev);\n\t\tif (IS_ERR(locker)) {\n\t\t\tret = PTR_ERR(locker);\n\t\t\tlocker = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!locker)\n\t\t\tgoto again;\n\n\t\tret = find_watcher(rbd_dev, locker);\n\t\tif (ret)\n\t\t\tgoto out;  \n\n\t\trefreshed_locker = get_lock_owner_info(rbd_dev);\n\t\tif (IS_ERR(refreshed_locker)) {\n\t\t\tret = PTR_ERR(refreshed_locker);\n\t\t\trefreshed_locker = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!refreshed_locker ||\n\t\t    !locker_equal(locker, refreshed_locker))\n\t\t\tgoto again;\n\n\t\trbd_warn(rbd_dev, \"breaking header lock owned by %s%llu\",\n\t\t\t ENTITY_NAME(locker->id.name));\n\n\t\tret = ceph_monc_blocklist_add(&client->monc,\n\t\t\t\t\t      &locker->info.addr);\n\t\tif (ret) {\n\t\t\trbd_warn(rbd_dev, \"failed to blocklist %s%llu: %d\",\n\t\t\t\t ENTITY_NAME(locker->id.name), ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = ceph_cls_break_lock(&client->osdc, &rbd_dev->header_oid,\n\t\t\t\t\t  &rbd_dev->header_oloc, RBD_LOCK_NAME,\n\t\t\t\t\t  locker->id.cookie, &locker->id.name);\n\t\tif (ret && ret != -ENOENT) {\n\t\t\trbd_warn(rbd_dev, \"failed to break header lock: %d\",\n\t\t\t\t ret);\n\t\t\tgoto out;\n\t\t}\n\nagain:\n\t\tfree_locker(refreshed_locker);\n\t\tfree_locker(locker);\n\t}\n\nout:\n\tfree_locker(refreshed_locker);\n\tfree_locker(locker);\n\treturn ret;\n}\n\nstatic int rbd_post_acquire_action(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\tret = rbd_dev_refresh(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP) {\n\t\tret = rbd_object_map_open(rbd_dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int rbd_try_acquire_lock(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\tdown_read(&rbd_dev->lock_rwsem);\n\tdout(\"%s rbd_dev %p read lock_state %d\\n\", __func__, rbd_dev,\n\t     rbd_dev->lock_state);\n\tif (__rbd_is_lock_owner(rbd_dev)) {\n\t\tup_read(&rbd_dev->lock_rwsem);\n\t\treturn 0;\n\t}\n\n\tup_read(&rbd_dev->lock_rwsem);\n\tdown_write(&rbd_dev->lock_rwsem);\n\tdout(\"%s rbd_dev %p write lock_state %d\\n\", __func__, rbd_dev,\n\t     rbd_dev->lock_state);\n\tif (__rbd_is_lock_owner(rbd_dev)) {\n\t\tup_write(&rbd_dev->lock_rwsem);\n\t\treturn 0;\n\t}\n\n\tret = rbd_try_lock(rbd_dev);\n\tif (ret < 0) {\n\t\trbd_warn(rbd_dev, \"failed to acquire lock: %d\", ret);\n\t\tgoto out;\n\t}\n\tif (ret > 0) {\n\t\tup_write(&rbd_dev->lock_rwsem);\n\t\treturn ret;\n\t}\n\n\trbd_assert(rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED);\n\trbd_assert(list_empty(&rbd_dev->running_list));\n\n\tret = rbd_post_acquire_action(rbd_dev);\n\tif (ret) {\n\t\trbd_warn(rbd_dev, \"post-acquire action failed: %d\", ret);\n\t\t \n\t\trbd_unlock(rbd_dev);\n\t}\n\nout:\n\twake_lock_waiters(rbd_dev, ret);\n\tup_write(&rbd_dev->lock_rwsem);\n\treturn ret;\n}\n\nstatic void rbd_acquire_lock(struct work_struct *work)\n{\n\tstruct rbd_device *rbd_dev = container_of(to_delayed_work(work),\n\t\t\t\t\t    struct rbd_device, lock_dwork);\n\tint ret;\n\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\nagain:\n\tret = rbd_try_acquire_lock(rbd_dev);\n\tif (ret <= 0) {\n\t\tdout(\"%s rbd_dev %p ret %d - done\\n\", __func__, rbd_dev, ret);\n\t\treturn;\n\t}\n\n\tret = rbd_request_lock(rbd_dev);\n\tif (ret == -ETIMEDOUT) {\n\t\tgoto again;  \n\t} else if (ret == -EROFS) {\n\t\trbd_warn(rbd_dev, \"peer will not release lock\");\n\t\tdown_write(&rbd_dev->lock_rwsem);\n\t\twake_lock_waiters(rbd_dev, ret);\n\t\tup_write(&rbd_dev->lock_rwsem);\n\t} else if (ret < 0) {\n\t\trbd_warn(rbd_dev, \"error requesting lock: %d\", ret);\n\t\tmod_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork,\n\t\t\t\t RBD_RETRY_DELAY);\n\t} else {\n\t\t \n\t\tdout(\"%s rbd_dev %p requeuing lock_dwork\\n\", __func__,\n\t\t     rbd_dev);\n\t\tmod_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork,\n\t\t    msecs_to_jiffies(2 * RBD_NOTIFY_TIMEOUT * MSEC_PER_SEC));\n\t}\n}\n\nstatic bool rbd_quiesce_lock(struct rbd_device *rbd_dev)\n{\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\tlockdep_assert_held_write(&rbd_dev->lock_rwsem);\n\n\tif (rbd_dev->lock_state != RBD_LOCK_STATE_LOCKED)\n\t\treturn false;\n\n\t \n\trbd_dev->lock_state = RBD_LOCK_STATE_RELEASING;\n\trbd_assert(!completion_done(&rbd_dev->releasing_wait));\n\tif (list_empty(&rbd_dev->running_list))\n\t\treturn true;\n\n\tup_write(&rbd_dev->lock_rwsem);\n\twait_for_completion(&rbd_dev->releasing_wait);\n\n\tdown_write(&rbd_dev->lock_rwsem);\n\tif (rbd_dev->lock_state != RBD_LOCK_STATE_RELEASING)\n\t\treturn false;\n\n\trbd_assert(list_empty(&rbd_dev->running_list));\n\treturn true;\n}\n\nstatic void rbd_pre_release_action(struct rbd_device *rbd_dev)\n{\n\tif (rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP)\n\t\trbd_object_map_close(rbd_dev);\n}\n\nstatic void __rbd_release_lock(struct rbd_device *rbd_dev)\n{\n\trbd_assert(list_empty(&rbd_dev->running_list));\n\n\trbd_pre_release_action(rbd_dev);\n\trbd_unlock(rbd_dev);\n}\n\n \nstatic void rbd_release_lock(struct rbd_device *rbd_dev)\n{\n\tif (!rbd_quiesce_lock(rbd_dev))\n\t\treturn;\n\n\t__rbd_release_lock(rbd_dev);\n\n\t \n\tcancel_delayed_work(&rbd_dev->lock_dwork);\n}\n\nstatic void rbd_release_lock_work(struct work_struct *work)\n{\n\tstruct rbd_device *rbd_dev = container_of(work, struct rbd_device,\n\t\t\t\t\t\t  unlock_work);\n\n\tdown_write(&rbd_dev->lock_rwsem);\n\trbd_release_lock(rbd_dev);\n\tup_write(&rbd_dev->lock_rwsem);\n}\n\nstatic void maybe_kick_acquire(struct rbd_device *rbd_dev)\n{\n\tbool have_requests;\n\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\tif (__rbd_is_lock_owner(rbd_dev))\n\t\treturn;\n\n\tspin_lock(&rbd_dev->lock_lists_lock);\n\thave_requests = !list_empty(&rbd_dev->acquiring_list);\n\tspin_unlock(&rbd_dev->lock_lists_lock);\n\tif (have_requests || delayed_work_pending(&rbd_dev->lock_dwork)) {\n\t\tdout(\"%s rbd_dev %p kicking lock_dwork\\n\", __func__, rbd_dev);\n\t\tmod_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork, 0);\n\t}\n}\n\nstatic void rbd_handle_acquired_lock(struct rbd_device *rbd_dev, u8 struct_v,\n\t\t\t\t     void **p)\n{\n\tstruct rbd_client_id cid = { 0 };\n\n\tif (struct_v >= 2) {\n\t\tcid.gid = ceph_decode_64(p);\n\t\tcid.handle = ceph_decode_64(p);\n\t}\n\n\tdout(\"%s rbd_dev %p cid %llu-%llu\\n\", __func__, rbd_dev, cid.gid,\n\t     cid.handle);\n\tif (!rbd_cid_equal(&cid, &rbd_empty_cid)) {\n\t\tdown_write(&rbd_dev->lock_rwsem);\n\t\tif (rbd_cid_equal(&cid, &rbd_dev->owner_cid)) {\n\t\t\tdout(\"%s rbd_dev %p cid %llu-%llu == owner_cid\\n\",\n\t\t\t     __func__, rbd_dev, cid.gid, cid.handle);\n\t\t} else {\n\t\t\trbd_set_owner_cid(rbd_dev, &cid);\n\t\t}\n\t\tdowngrade_write(&rbd_dev->lock_rwsem);\n\t} else {\n\t\tdown_read(&rbd_dev->lock_rwsem);\n\t}\n\n\tmaybe_kick_acquire(rbd_dev);\n\tup_read(&rbd_dev->lock_rwsem);\n}\n\nstatic void rbd_handle_released_lock(struct rbd_device *rbd_dev, u8 struct_v,\n\t\t\t\t     void **p)\n{\n\tstruct rbd_client_id cid = { 0 };\n\n\tif (struct_v >= 2) {\n\t\tcid.gid = ceph_decode_64(p);\n\t\tcid.handle = ceph_decode_64(p);\n\t}\n\n\tdout(\"%s rbd_dev %p cid %llu-%llu\\n\", __func__, rbd_dev, cid.gid,\n\t     cid.handle);\n\tif (!rbd_cid_equal(&cid, &rbd_empty_cid)) {\n\t\tdown_write(&rbd_dev->lock_rwsem);\n\t\tif (!rbd_cid_equal(&cid, &rbd_dev->owner_cid)) {\n\t\t\tdout(\"%s rbd_dev %p cid %llu-%llu != owner_cid %llu-%llu\\n\",\n\t\t\t     __func__, rbd_dev, cid.gid, cid.handle,\n\t\t\t     rbd_dev->owner_cid.gid, rbd_dev->owner_cid.handle);\n\t\t} else {\n\t\t\trbd_set_owner_cid(rbd_dev, &rbd_empty_cid);\n\t\t}\n\t\tdowngrade_write(&rbd_dev->lock_rwsem);\n\t} else {\n\t\tdown_read(&rbd_dev->lock_rwsem);\n\t}\n\n\tmaybe_kick_acquire(rbd_dev);\n\tup_read(&rbd_dev->lock_rwsem);\n}\n\n \nstatic int rbd_handle_request_lock(struct rbd_device *rbd_dev, u8 struct_v,\n\t\t\t\t   void **p)\n{\n\tstruct rbd_client_id my_cid = rbd_get_cid(rbd_dev);\n\tstruct rbd_client_id cid = { 0 };\n\tint result = 1;\n\n\tif (struct_v >= 2) {\n\t\tcid.gid = ceph_decode_64(p);\n\t\tcid.handle = ceph_decode_64(p);\n\t}\n\n\tdout(\"%s rbd_dev %p cid %llu-%llu\\n\", __func__, rbd_dev, cid.gid,\n\t     cid.handle);\n\tif (rbd_cid_equal(&cid, &my_cid))\n\t\treturn result;\n\n\tdown_read(&rbd_dev->lock_rwsem);\n\tif (__rbd_is_lock_owner(rbd_dev)) {\n\t\tif (rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED &&\n\t\t    rbd_cid_equal(&rbd_dev->owner_cid, &rbd_empty_cid))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tresult = 0;\n\n\t\tif (rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED) {\n\t\t\tif (!rbd_dev->opts->exclusive) {\n\t\t\t\tdout(\"%s rbd_dev %p queueing unlock_work\\n\",\n\t\t\t\t     __func__, rbd_dev);\n\t\t\t\tqueue_work(rbd_dev->task_wq,\n\t\t\t\t\t   &rbd_dev->unlock_work);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tresult = -EROFS;\n\t\t\t}\n\t\t}\n\t}\n\nout_unlock:\n\tup_read(&rbd_dev->lock_rwsem);\n\treturn result;\n}\n\nstatic void __rbd_acknowledge_notify(struct rbd_device *rbd_dev,\n\t\t\t\t     u64 notify_id, u64 cookie, s32 *result)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tchar buf[4 + CEPH_ENCODING_START_BLK_LEN];\n\tint buf_size = sizeof(buf);\n\tint ret;\n\n\tif (result) {\n\t\tvoid *p = buf;\n\n\t\t \n\t\tceph_start_encoding(&p, 1, 1,\n\t\t\t\t    buf_size - CEPH_ENCODING_START_BLK_LEN);\n\t\tceph_encode_32(&p, *result);\n\t} else {\n\t\tbuf_size = 0;\n\t}\n\n\tret = ceph_osdc_notify_ack(osdc, &rbd_dev->header_oid,\n\t\t\t\t   &rbd_dev->header_oloc, notify_id, cookie,\n\t\t\t\t   buf, buf_size);\n\tif (ret)\n\t\trbd_warn(rbd_dev, \"acknowledge_notify failed: %d\", ret);\n}\n\nstatic void rbd_acknowledge_notify(struct rbd_device *rbd_dev, u64 notify_id,\n\t\t\t\t   u64 cookie)\n{\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\t__rbd_acknowledge_notify(rbd_dev, notify_id, cookie, NULL);\n}\n\nstatic void rbd_acknowledge_notify_result(struct rbd_device *rbd_dev,\n\t\t\t\t\t  u64 notify_id, u64 cookie, s32 result)\n{\n\tdout(\"%s rbd_dev %p result %d\\n\", __func__, rbd_dev, result);\n\t__rbd_acknowledge_notify(rbd_dev, notify_id, cookie, &result);\n}\n\nstatic void rbd_watch_cb(void *arg, u64 notify_id, u64 cookie,\n\t\t\t u64 notifier_id, void *data, size_t data_len)\n{\n\tstruct rbd_device *rbd_dev = arg;\n\tvoid *p = data;\n\tvoid *const end = p + data_len;\n\tu8 struct_v = 0;\n\tu32 len;\n\tu32 notify_op;\n\tint ret;\n\n\tdout(\"%s rbd_dev %p cookie %llu notify_id %llu data_len %zu\\n\",\n\t     __func__, rbd_dev, cookie, notify_id, data_len);\n\tif (data_len) {\n\t\tret = ceph_start_decoding(&p, end, 1, \"NotifyMessage\",\n\t\t\t\t\t  &struct_v, &len);\n\t\tif (ret) {\n\t\t\trbd_warn(rbd_dev, \"failed to decode NotifyMessage: %d\",\n\t\t\t\t ret);\n\t\t\treturn;\n\t\t}\n\n\t\tnotify_op = ceph_decode_32(&p);\n\t} else {\n\t\t \n\t\tnotify_op = RBD_NOTIFY_OP_HEADER_UPDATE;\n\t\tlen = 0;\n\t}\n\n\tdout(\"%s rbd_dev %p notify_op %u\\n\", __func__, rbd_dev, notify_op);\n\tswitch (notify_op) {\n\tcase RBD_NOTIFY_OP_ACQUIRED_LOCK:\n\t\trbd_handle_acquired_lock(rbd_dev, struct_v, &p);\n\t\trbd_acknowledge_notify(rbd_dev, notify_id, cookie);\n\t\tbreak;\n\tcase RBD_NOTIFY_OP_RELEASED_LOCK:\n\t\trbd_handle_released_lock(rbd_dev, struct_v, &p);\n\t\trbd_acknowledge_notify(rbd_dev, notify_id, cookie);\n\t\tbreak;\n\tcase RBD_NOTIFY_OP_REQUEST_LOCK:\n\t\tret = rbd_handle_request_lock(rbd_dev, struct_v, &p);\n\t\tif (ret <= 0)\n\t\t\trbd_acknowledge_notify_result(rbd_dev, notify_id,\n\t\t\t\t\t\t      cookie, ret);\n\t\telse\n\t\t\trbd_acknowledge_notify(rbd_dev, notify_id, cookie);\n\t\tbreak;\n\tcase RBD_NOTIFY_OP_HEADER_UPDATE:\n\t\tret = rbd_dev_refresh(rbd_dev);\n\t\tif (ret)\n\t\t\trbd_warn(rbd_dev, \"refresh failed: %d\", ret);\n\n\t\trbd_acknowledge_notify(rbd_dev, notify_id, cookie);\n\t\tbreak;\n\tdefault:\n\t\tif (rbd_is_lock_owner(rbd_dev))\n\t\t\trbd_acknowledge_notify_result(rbd_dev, notify_id,\n\t\t\t\t\t\t      cookie, -EOPNOTSUPP);\n\t\telse\n\t\t\trbd_acknowledge_notify(rbd_dev, notify_id, cookie);\n\t\tbreak;\n\t}\n}\n\nstatic void __rbd_unregister_watch(struct rbd_device *rbd_dev);\n\nstatic void rbd_watch_errcb(void *arg, u64 cookie, int err)\n{\n\tstruct rbd_device *rbd_dev = arg;\n\n\trbd_warn(rbd_dev, \"encountered watch error: %d\", err);\n\n\tdown_write(&rbd_dev->lock_rwsem);\n\trbd_set_owner_cid(rbd_dev, &rbd_empty_cid);\n\tup_write(&rbd_dev->lock_rwsem);\n\n\tmutex_lock(&rbd_dev->watch_mutex);\n\tif (rbd_dev->watch_state == RBD_WATCH_STATE_REGISTERED) {\n\t\t__rbd_unregister_watch(rbd_dev);\n\t\trbd_dev->watch_state = RBD_WATCH_STATE_ERROR;\n\n\t\tqueue_delayed_work(rbd_dev->task_wq, &rbd_dev->watch_dwork, 0);\n\t}\n\tmutex_unlock(&rbd_dev->watch_mutex);\n}\n\n \nstatic int __rbd_register_watch(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_osd_linger_request *handle;\n\n\trbd_assert(!rbd_dev->watch_handle);\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\thandle = ceph_osdc_watch(osdc, &rbd_dev->header_oid,\n\t\t\t\t &rbd_dev->header_oloc, rbd_watch_cb,\n\t\t\t\t rbd_watch_errcb, rbd_dev);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\trbd_dev->watch_handle = handle;\n\treturn 0;\n}\n\n \nstatic void __rbd_unregister_watch(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tint ret;\n\n\trbd_assert(rbd_dev->watch_handle);\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\tret = ceph_osdc_unwatch(osdc, rbd_dev->watch_handle);\n\tif (ret)\n\t\trbd_warn(rbd_dev, \"failed to unwatch: %d\", ret);\n\n\trbd_dev->watch_handle = NULL;\n}\n\nstatic int rbd_register_watch(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\tmutex_lock(&rbd_dev->watch_mutex);\n\trbd_assert(rbd_dev->watch_state == RBD_WATCH_STATE_UNREGISTERED);\n\tret = __rbd_register_watch(rbd_dev);\n\tif (ret)\n\t\tgoto out;\n\n\trbd_dev->watch_state = RBD_WATCH_STATE_REGISTERED;\n\trbd_dev->watch_cookie = rbd_dev->watch_handle->linger_id;\n\nout:\n\tmutex_unlock(&rbd_dev->watch_mutex);\n\treturn ret;\n}\n\nstatic void cancel_tasks_sync(struct rbd_device *rbd_dev)\n{\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\tcancel_work_sync(&rbd_dev->acquired_lock_work);\n\tcancel_work_sync(&rbd_dev->released_lock_work);\n\tcancel_delayed_work_sync(&rbd_dev->lock_dwork);\n\tcancel_work_sync(&rbd_dev->unlock_work);\n}\n\n \nstatic void rbd_unregister_watch(struct rbd_device *rbd_dev)\n{\n\tcancel_tasks_sync(rbd_dev);\n\n\tmutex_lock(&rbd_dev->watch_mutex);\n\tif (rbd_dev->watch_state == RBD_WATCH_STATE_REGISTERED)\n\t\t__rbd_unregister_watch(rbd_dev);\n\trbd_dev->watch_state = RBD_WATCH_STATE_UNREGISTERED;\n\tmutex_unlock(&rbd_dev->watch_mutex);\n\n\tcancel_delayed_work_sync(&rbd_dev->watch_dwork);\n\tceph_osdc_flush_notifies(&rbd_dev->rbd_client->client->osdc);\n}\n\n \nstatic void rbd_reacquire_lock(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tchar cookie[32];\n\tint ret;\n\n\tif (!rbd_quiesce_lock(rbd_dev))\n\t\treturn;\n\n\tformat_lock_cookie(rbd_dev, cookie);\n\tret = ceph_cls_set_cookie(osdc, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, RBD_LOCK_NAME,\n\t\t\t\t  CEPH_CLS_LOCK_EXCLUSIVE, rbd_dev->lock_cookie,\n\t\t\t\t  RBD_LOCK_TAG, cookie);\n\tif (ret) {\n\t\tif (ret != -EOPNOTSUPP)\n\t\t\trbd_warn(rbd_dev, \"failed to update lock cookie: %d\",\n\t\t\t\t ret);\n\n\t\t \n\t\t__rbd_release_lock(rbd_dev);\n\t\tqueue_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork, 0);\n\t} else {\n\t\t__rbd_lock(rbd_dev, cookie);\n\t\twake_lock_waiters(rbd_dev, 0);\n\t}\n}\n\nstatic void rbd_reregister_watch(struct work_struct *work)\n{\n\tstruct rbd_device *rbd_dev = container_of(to_delayed_work(work),\n\t\t\t\t\t    struct rbd_device, watch_dwork);\n\tint ret;\n\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\tmutex_lock(&rbd_dev->watch_mutex);\n\tif (rbd_dev->watch_state != RBD_WATCH_STATE_ERROR) {\n\t\tmutex_unlock(&rbd_dev->watch_mutex);\n\t\treturn;\n\t}\n\n\tret = __rbd_register_watch(rbd_dev);\n\tif (ret) {\n\t\trbd_warn(rbd_dev, \"failed to reregister watch: %d\", ret);\n\t\tif (ret != -EBLOCKLISTED && ret != -ENOENT) {\n\t\t\tqueue_delayed_work(rbd_dev->task_wq,\n\t\t\t\t\t   &rbd_dev->watch_dwork,\n\t\t\t\t\t   RBD_RETRY_DELAY);\n\t\t\tmutex_unlock(&rbd_dev->watch_mutex);\n\t\t\treturn;\n\t\t}\n\n\t\tmutex_unlock(&rbd_dev->watch_mutex);\n\t\tdown_write(&rbd_dev->lock_rwsem);\n\t\twake_lock_waiters(rbd_dev, ret);\n\t\tup_write(&rbd_dev->lock_rwsem);\n\t\treturn;\n\t}\n\n\trbd_dev->watch_state = RBD_WATCH_STATE_REGISTERED;\n\trbd_dev->watch_cookie = rbd_dev->watch_handle->linger_id;\n\tmutex_unlock(&rbd_dev->watch_mutex);\n\n\tdown_write(&rbd_dev->lock_rwsem);\n\tif (rbd_dev->lock_state == RBD_LOCK_STATE_LOCKED)\n\t\trbd_reacquire_lock(rbd_dev);\n\tup_write(&rbd_dev->lock_rwsem);\n\n\tret = rbd_dev_refresh(rbd_dev);\n\tif (ret)\n\t\trbd_warn(rbd_dev, \"reregistration refresh failed: %d\", ret);\n}\n\n \nstatic int rbd_obj_method_sync(struct rbd_device *rbd_dev,\n\t\t\t     struct ceph_object_id *oid,\n\t\t\t     struct ceph_object_locator *oloc,\n\t\t\t     const char *method_name,\n\t\t\t     const void *outbound,\n\t\t\t     size_t outbound_size,\n\t\t\t     void *inbound,\n\t\t\t     size_t inbound_size)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct page *req_page = NULL;\n\tstruct page *reply_page;\n\tint ret;\n\n\t \n\tif (outbound) {\n\t\tif (outbound_size > PAGE_SIZE)\n\t\t\treturn -E2BIG;\n\n\t\treq_page = alloc_page(GFP_KERNEL);\n\t\tif (!req_page)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(page_address(req_page), outbound, outbound_size);\n\t}\n\n\treply_page = alloc_page(GFP_KERNEL);\n\tif (!reply_page) {\n\t\tif (req_page)\n\t\t\t__free_page(req_page);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = ceph_osdc_call(osdc, oid, oloc, RBD_DRV_NAME, method_name,\n\t\t\t     CEPH_OSD_FLAG_READ, req_page, outbound_size,\n\t\t\t     &reply_page, &inbound_size);\n\tif (!ret) {\n\t\tmemcpy(inbound, page_address(reply_page), inbound_size);\n\t\tret = inbound_size;\n\t}\n\n\tif (req_page)\n\t\t__free_page(req_page);\n\t__free_page(reply_page);\n\treturn ret;\n}\n\nstatic void rbd_queue_workfn(struct work_struct *work)\n{\n\tstruct rbd_img_request *img_request =\n\t    container_of(work, struct rbd_img_request, work);\n\tstruct rbd_device *rbd_dev = img_request->rbd_dev;\n\tenum obj_operation_type op_type = img_request->op_type;\n\tstruct request *rq = blk_mq_rq_from_pdu(img_request);\n\tu64 offset = (u64)blk_rq_pos(rq) << SECTOR_SHIFT;\n\tu64 length = blk_rq_bytes(rq);\n\tu64 mapping_size;\n\tint result;\n\n\t \n\tif (!length) {\n\t\tdout(\"%s: zero-length request\\n\", __func__);\n\t\tresult = 0;\n\t\tgoto err_img_request;\n\t}\n\n\tblk_mq_start_request(rq);\n\n\tdown_read(&rbd_dev->header_rwsem);\n\tmapping_size = rbd_dev->mapping.size;\n\trbd_img_capture_header(img_request);\n\tup_read(&rbd_dev->header_rwsem);\n\n\tif (offset + length > mapping_size) {\n\t\trbd_warn(rbd_dev, \"beyond EOD (%llu~%llu > %llu)\", offset,\n\t\t\t length, mapping_size);\n\t\tresult = -EIO;\n\t\tgoto err_img_request;\n\t}\n\n\tdout(\"%s rbd_dev %p img_req %p %s %llu~%llu\\n\", __func__, rbd_dev,\n\t     img_request, obj_op_name(op_type), offset, length);\n\n\tif (op_type == OBJ_OP_DISCARD || op_type == OBJ_OP_ZEROOUT)\n\t\tresult = rbd_img_fill_nodata(img_request, offset, length);\n\telse\n\t\tresult = rbd_img_fill_from_bio(img_request, offset, length,\n\t\t\t\t\t       rq->bio);\n\tif (result)\n\t\tgoto err_img_request;\n\n\trbd_img_handle_request(img_request, 0);\n\treturn;\n\nerr_img_request:\n\trbd_img_request_destroy(img_request);\n\tif (result)\n\t\trbd_warn(rbd_dev, \"%s %llx at %llx result %d\",\n\t\t\t obj_op_name(op_type), length, offset, result);\n\tblk_mq_end_request(rq, errno_to_blk_status(result));\n}\n\nstatic blk_status_t rbd_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct rbd_device *rbd_dev = hctx->queue->queuedata;\n\tstruct rbd_img_request *img_req = blk_mq_rq_to_pdu(bd->rq);\n\tenum obj_operation_type op_type;\n\n\tswitch (req_op(bd->rq)) {\n\tcase REQ_OP_DISCARD:\n\t\top_type = OBJ_OP_DISCARD;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\top_type = OBJ_OP_ZEROOUT;\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\top_type = OBJ_OP_WRITE;\n\t\tbreak;\n\tcase REQ_OP_READ:\n\t\top_type = OBJ_OP_READ;\n\t\tbreak;\n\tdefault:\n\t\trbd_warn(rbd_dev, \"unknown req_op %d\", req_op(bd->rq));\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\trbd_img_request_init(img_req, rbd_dev, op_type);\n\n\tif (rbd_img_is_write(img_req)) {\n\t\tif (rbd_is_ro(rbd_dev)) {\n\t\t\trbd_warn(rbd_dev, \"%s on read-only mapping\",\n\t\t\t\t obj_op_name(img_req->op_type));\n\t\t\treturn BLK_STS_IOERR;\n\t\t}\n\t\trbd_assert(!rbd_is_snap(rbd_dev));\n\t}\n\n\tINIT_WORK(&img_req->work, rbd_queue_workfn);\n\tqueue_work(rbd_wq, &img_req->work);\n\treturn BLK_STS_OK;\n}\n\nstatic void rbd_free_disk(struct rbd_device *rbd_dev)\n{\n\tput_disk(rbd_dev->disk);\n\tblk_mq_free_tag_set(&rbd_dev->tag_set);\n\trbd_dev->disk = NULL;\n}\n\nstatic int rbd_obj_read_sync(struct rbd_device *rbd_dev,\n\t\t\t     struct ceph_object_id *oid,\n\t\t\t     struct ceph_object_locator *oloc,\n\t\t\t     void *buf, int buf_len)\n\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct ceph_osd_request *req;\n\tstruct page **pages;\n\tint num_pages = calc_pages_for(0, buf_len);\n\tint ret;\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tceph_oid_copy(&req->r_base_oid, oid);\n\tceph_oloc_copy(&req->r_base_oloc, oloc);\n\treq->r_flags = CEPH_OSD_FLAG_READ;\n\n\tpages = ceph_alloc_page_vector(num_pages, GFP_KERNEL);\n\tif (IS_ERR(pages)) {\n\t\tret = PTR_ERR(pages);\n\t\tgoto out_req;\n\t}\n\n\tosd_req_op_extent_init(req, 0, CEPH_OSD_OP_READ, 0, buf_len, 0, 0);\n\tosd_req_op_extent_osd_data_pages(req, 0, pages, buf_len, 0, false,\n\t\t\t\t\t true);\n\n\tret = ceph_osdc_alloc_messages(req, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_req;\n\n\tceph_osdc_start_request(osdc, req);\n\tret = ceph_osdc_wait_request(osdc, req);\n\tif (ret >= 0)\n\t\tceph_copy_from_page_vector(pages, buf, 0, ret);\n\nout_req:\n\tceph_osdc_put_request(req);\n\treturn ret;\n}\n\n \nstatic int rbd_dev_v1_header_info(struct rbd_device *rbd_dev,\n\t\t\t\t  struct rbd_image_header *header,\n\t\t\t\t  bool first_time)\n{\n\tstruct rbd_image_header_ondisk *ondisk = NULL;\n\tu32 snap_count = 0;\n\tu64 names_size = 0;\n\tu32 want_count;\n\tint ret;\n\n\t \n\tdo {\n\t\tsize_t size;\n\n\t\tkfree(ondisk);\n\n\t\tsize = sizeof (*ondisk);\n\t\tsize += snap_count * sizeof (struct rbd_image_snap_ondisk);\n\t\tsize += names_size;\n\t\tondisk = kmalloc(size, GFP_KERNEL);\n\t\tif (!ondisk)\n\t\t\treturn -ENOMEM;\n\n\t\tret = rbd_obj_read_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t\t&rbd_dev->header_oloc, ondisk, size);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif ((size_t)ret < size) {\n\t\t\tret = -ENXIO;\n\t\t\trbd_warn(rbd_dev, \"short header read (want %zd got %d)\",\n\t\t\t\tsize, ret);\n\t\t\tgoto out;\n\t\t}\n\t\tif (!rbd_dev_ondisk_valid(ondisk)) {\n\t\t\tret = -ENXIO;\n\t\t\trbd_warn(rbd_dev, \"invalid header\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tnames_size = le64_to_cpu(ondisk->snap_names_len);\n\t\twant_count = snap_count;\n\t\tsnap_count = le32_to_cpu(ondisk->snap_count);\n\t} while (snap_count != want_count);\n\n\tret = rbd_header_from_disk(header, ondisk, first_time);\nout:\n\tkfree(ondisk);\n\n\treturn ret;\n}\n\nstatic void rbd_dev_update_size(struct rbd_device *rbd_dev)\n{\n\tsector_t size;\n\n\t \n\tif (test_bit(RBD_DEV_FLAG_EXISTS, &rbd_dev->flags) &&\n\t    !test_bit(RBD_DEV_FLAG_REMOVING, &rbd_dev->flags)) {\n\t\tsize = (sector_t)rbd_dev->mapping.size / SECTOR_SIZE;\n\t\tdout(\"setting size to %llu sectors\", (unsigned long long)size);\n\t\tset_capacity_and_notify(rbd_dev->disk, size);\n\t}\n}\n\nstatic const struct blk_mq_ops rbd_mq_ops = {\n\t.queue_rq\t= rbd_queue_rq,\n};\n\nstatic int rbd_init_disk(struct rbd_device *rbd_dev)\n{\n\tstruct gendisk *disk;\n\tstruct request_queue *q;\n\tunsigned int objset_bytes =\n\t    rbd_dev->layout.object_size * rbd_dev->layout.stripe_count;\n\tint err;\n\n\tmemset(&rbd_dev->tag_set, 0, sizeof(rbd_dev->tag_set));\n\trbd_dev->tag_set.ops = &rbd_mq_ops;\n\trbd_dev->tag_set.queue_depth = rbd_dev->opts->queue_depth;\n\trbd_dev->tag_set.numa_node = NUMA_NO_NODE;\n\trbd_dev->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\n\trbd_dev->tag_set.nr_hw_queues = num_present_cpus();\n\trbd_dev->tag_set.cmd_size = sizeof(struct rbd_img_request);\n\n\terr = blk_mq_alloc_tag_set(&rbd_dev->tag_set);\n\tif (err)\n\t\treturn err;\n\n\tdisk = blk_mq_alloc_disk(&rbd_dev->tag_set, rbd_dev);\n\tif (IS_ERR(disk)) {\n\t\terr = PTR_ERR(disk);\n\t\tgoto out_tag_set;\n\t}\n\tq = disk->queue;\n\n\tsnprintf(disk->disk_name, sizeof(disk->disk_name), RBD_DRV_NAME \"%d\",\n\t\t rbd_dev->dev_id);\n\tdisk->major = rbd_dev->major;\n\tdisk->first_minor = rbd_dev->minor;\n\tif (single_major)\n\t\tdisk->minors = (1 << RBD_SINGLE_MAJOR_PART_SHIFT);\n\telse\n\t\tdisk->minors = RBD_MINORS_PER_MAJOR;\n\tdisk->fops = &rbd_bd_ops;\n\tdisk->private_data = rbd_dev;\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, q);\n\t \n\n\tblk_queue_max_hw_sectors(q, objset_bytes >> SECTOR_SHIFT);\n\tq->limits.max_sectors = queue_max_hw_sectors(q);\n\tblk_queue_max_segments(q, USHRT_MAX);\n\tblk_queue_max_segment_size(q, UINT_MAX);\n\tblk_queue_io_min(q, rbd_dev->opts->alloc_size);\n\tblk_queue_io_opt(q, rbd_dev->opts->alloc_size);\n\n\tif (rbd_dev->opts->trim) {\n\t\tq->limits.discard_granularity = rbd_dev->opts->alloc_size;\n\t\tblk_queue_max_discard_sectors(q, objset_bytes >> SECTOR_SHIFT);\n\t\tblk_queue_max_write_zeroes_sectors(q, objset_bytes >> SECTOR_SHIFT);\n\t}\n\n\tif (!ceph_test_opt(rbd_dev->rbd_client->client, NOCRC))\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, q);\n\n\trbd_dev->disk = disk;\n\n\treturn 0;\nout_tag_set:\n\tblk_mq_free_tag_set(&rbd_dev->tag_set);\n\treturn err;\n}\n\n \n\nstatic struct rbd_device *dev_to_rbd_dev(struct device *dev)\n{\n\treturn container_of(dev, struct rbd_device, dev);\n}\n\nstatic ssize_t rbd_size_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t(unsigned long long)rbd_dev->mapping.size);\n}\n\nstatic ssize_t rbd_features_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"0x%016llx\\n\", rbd_dev->header.features);\n}\n\nstatic ssize_t rbd_major_show(struct device *dev,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\tif (rbd_dev->major)\n\t\treturn sprintf(buf, \"%d\\n\", rbd_dev->major);\n\n\treturn sprintf(buf, \"(none)\\n\");\n}\n\nstatic ssize_t rbd_minor_show(struct device *dev,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%d\\n\", rbd_dev->minor);\n}\n\nstatic ssize_t rbd_client_addr_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tstruct ceph_entity_addr *client_addr =\n\t    ceph_client_addr(rbd_dev->rbd_client->client);\n\n\treturn sprintf(buf, \"%pISpc/%u\\n\", &client_addr->in_addr,\n\t\t       le32_to_cpu(client_addr->nonce));\n}\n\nstatic ssize_t rbd_client_id_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"client%lld\\n\",\n\t\t       ceph_client_gid(rbd_dev->rbd_client->client));\n}\n\nstatic ssize_t rbd_cluster_fsid_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%pU\\n\", &rbd_dev->rbd_client->client->fsid);\n}\n\nstatic ssize_t rbd_config_info_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->config_info);\n}\n\nstatic ssize_t rbd_pool_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->spec->pool_name);\n}\n\nstatic ssize_t rbd_pool_id_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t\t(unsigned long long) rbd_dev->spec->pool_id);\n}\n\nstatic ssize_t rbd_pool_ns_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->spec->pool_ns ?: \"\");\n}\n\nstatic ssize_t rbd_name_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\tif (rbd_dev->spec->image_name)\n\t\treturn sprintf(buf, \"%s\\n\", rbd_dev->spec->image_name);\n\n\treturn sprintf(buf, \"(unknown)\\n\");\n}\n\nstatic ssize_t rbd_image_id_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->spec->image_id);\n}\n\n \nstatic ssize_t rbd_snap_show(struct device *dev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%s\\n\", rbd_dev->spec->snap_name);\n}\n\nstatic ssize_t rbd_snap_id_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\n\treturn sprintf(buf, \"%llu\\n\", rbd_dev->spec->snap_id);\n}\n\n \nstatic ssize_t rbd_parent_show(struct device *dev,\n\t\t\t       struct device_attribute *attr,\n\t\t\t       char *buf)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tssize_t count = 0;\n\n\tif (!rbd_dev->parent)\n\t\treturn sprintf(buf, \"(no parent image)\\n\");\n\n\tfor ( ; rbd_dev->parent; rbd_dev = rbd_dev->parent) {\n\t\tstruct rbd_spec *spec = rbd_dev->parent_spec;\n\n\t\tcount += sprintf(&buf[count], \"%s\"\n\t\t\t    \"pool_id %llu\\npool_name %s\\n\"\n\t\t\t    \"pool_ns %s\\n\"\n\t\t\t    \"image_id %s\\nimage_name %s\\n\"\n\t\t\t    \"snap_id %llu\\nsnap_name %s\\n\"\n\t\t\t    \"overlap %llu\\n\",\n\t\t\t    !count ? \"\" : \"\\n\",  \n\t\t\t    spec->pool_id, spec->pool_name,\n\t\t\t    spec->pool_ns ?: \"\",\n\t\t\t    spec->image_id, spec->image_name ?: \"(unknown)\",\n\t\t\t    spec->snap_id, spec->snap_name,\n\t\t\t    rbd_dev->parent_overlap);\n\t}\n\n\treturn count;\n}\n\nstatic ssize_t rbd_image_refresh(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf,\n\t\t\t\t size_t size)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = rbd_dev_refresh(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\treturn size;\n}\n\nstatic DEVICE_ATTR(size, 0444, rbd_size_show, NULL);\nstatic DEVICE_ATTR(features, 0444, rbd_features_show, NULL);\nstatic DEVICE_ATTR(major, 0444, rbd_major_show, NULL);\nstatic DEVICE_ATTR(minor, 0444, rbd_minor_show, NULL);\nstatic DEVICE_ATTR(client_addr, 0444, rbd_client_addr_show, NULL);\nstatic DEVICE_ATTR(client_id, 0444, rbd_client_id_show, NULL);\nstatic DEVICE_ATTR(cluster_fsid, 0444, rbd_cluster_fsid_show, NULL);\nstatic DEVICE_ATTR(config_info, 0400, rbd_config_info_show, NULL);\nstatic DEVICE_ATTR(pool, 0444, rbd_pool_show, NULL);\nstatic DEVICE_ATTR(pool_id, 0444, rbd_pool_id_show, NULL);\nstatic DEVICE_ATTR(pool_ns, 0444, rbd_pool_ns_show, NULL);\nstatic DEVICE_ATTR(name, 0444, rbd_name_show, NULL);\nstatic DEVICE_ATTR(image_id, 0444, rbd_image_id_show, NULL);\nstatic DEVICE_ATTR(refresh, 0200, NULL, rbd_image_refresh);\nstatic DEVICE_ATTR(current_snap, 0444, rbd_snap_show, NULL);\nstatic DEVICE_ATTR(snap_id, 0444, rbd_snap_id_show, NULL);\nstatic DEVICE_ATTR(parent, 0444, rbd_parent_show, NULL);\n\nstatic struct attribute *rbd_attrs[] = {\n\t&dev_attr_size.attr,\n\t&dev_attr_features.attr,\n\t&dev_attr_major.attr,\n\t&dev_attr_minor.attr,\n\t&dev_attr_client_addr.attr,\n\t&dev_attr_client_id.attr,\n\t&dev_attr_cluster_fsid.attr,\n\t&dev_attr_config_info.attr,\n\t&dev_attr_pool.attr,\n\t&dev_attr_pool_id.attr,\n\t&dev_attr_pool_ns.attr,\n\t&dev_attr_name.attr,\n\t&dev_attr_image_id.attr,\n\t&dev_attr_current_snap.attr,\n\t&dev_attr_snap_id.attr,\n\t&dev_attr_parent.attr,\n\t&dev_attr_refresh.attr,\n\tNULL\n};\n\nstatic struct attribute_group rbd_attr_group = {\n\t.attrs = rbd_attrs,\n};\n\nstatic const struct attribute_group *rbd_attr_groups[] = {\n\t&rbd_attr_group,\n\tNULL\n};\n\nstatic void rbd_dev_release(struct device *dev);\n\nstatic const struct device_type rbd_device_type = {\n\t.name\t\t= \"rbd\",\n\t.groups\t\t= rbd_attr_groups,\n\t.release\t= rbd_dev_release,\n};\n\nstatic struct rbd_spec *rbd_spec_get(struct rbd_spec *spec)\n{\n\tkref_get(&spec->kref);\n\n\treturn spec;\n}\n\nstatic void rbd_spec_free(struct kref *kref);\nstatic void rbd_spec_put(struct rbd_spec *spec)\n{\n\tif (spec)\n\t\tkref_put(&spec->kref, rbd_spec_free);\n}\n\nstatic struct rbd_spec *rbd_spec_alloc(void)\n{\n\tstruct rbd_spec *spec;\n\n\tspec = kzalloc(sizeof (*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn NULL;\n\n\tspec->pool_id = CEPH_NOPOOL;\n\tspec->snap_id = CEPH_NOSNAP;\n\tkref_init(&spec->kref);\n\n\treturn spec;\n}\n\nstatic void rbd_spec_free(struct kref *kref)\n{\n\tstruct rbd_spec *spec = container_of(kref, struct rbd_spec, kref);\n\n\tkfree(spec->pool_name);\n\tkfree(spec->pool_ns);\n\tkfree(spec->image_id);\n\tkfree(spec->image_name);\n\tkfree(spec->snap_name);\n\tkfree(spec);\n}\n\nstatic void rbd_dev_free(struct rbd_device *rbd_dev)\n{\n\tWARN_ON(rbd_dev->watch_state != RBD_WATCH_STATE_UNREGISTERED);\n\tWARN_ON(rbd_dev->lock_state != RBD_LOCK_STATE_UNLOCKED);\n\n\tceph_oid_destroy(&rbd_dev->header_oid);\n\tceph_oloc_destroy(&rbd_dev->header_oloc);\n\tkfree(rbd_dev->config_info);\n\n\trbd_put_client(rbd_dev->rbd_client);\n\trbd_spec_put(rbd_dev->spec);\n\tkfree(rbd_dev->opts);\n\tkfree(rbd_dev);\n}\n\nstatic void rbd_dev_release(struct device *dev)\n{\n\tstruct rbd_device *rbd_dev = dev_to_rbd_dev(dev);\n\tbool need_put = !!rbd_dev->opts;\n\n\tif (need_put) {\n\t\tdestroy_workqueue(rbd_dev->task_wq);\n\t\tida_simple_remove(&rbd_dev_id_ida, rbd_dev->dev_id);\n\t}\n\n\trbd_dev_free(rbd_dev);\n\n\t \n\tif (need_put)\n\t\tmodule_put(THIS_MODULE);\n}\n\nstatic struct rbd_device *__rbd_dev_create(struct rbd_spec *spec)\n{\n\tstruct rbd_device *rbd_dev;\n\n\trbd_dev = kzalloc(sizeof(*rbd_dev), GFP_KERNEL);\n\tif (!rbd_dev)\n\t\treturn NULL;\n\n\tspin_lock_init(&rbd_dev->lock);\n\tINIT_LIST_HEAD(&rbd_dev->node);\n\tinit_rwsem(&rbd_dev->header_rwsem);\n\n\trbd_dev->header.data_pool_id = CEPH_NOPOOL;\n\tceph_oid_init(&rbd_dev->header_oid);\n\trbd_dev->header_oloc.pool = spec->pool_id;\n\tif (spec->pool_ns) {\n\t\tWARN_ON(!*spec->pool_ns);\n\t\trbd_dev->header_oloc.pool_ns =\n\t\t    ceph_find_or_create_string(spec->pool_ns,\n\t\t\t\t\t       strlen(spec->pool_ns));\n\t}\n\n\tmutex_init(&rbd_dev->watch_mutex);\n\trbd_dev->watch_state = RBD_WATCH_STATE_UNREGISTERED;\n\tINIT_DELAYED_WORK(&rbd_dev->watch_dwork, rbd_reregister_watch);\n\n\tinit_rwsem(&rbd_dev->lock_rwsem);\n\trbd_dev->lock_state = RBD_LOCK_STATE_UNLOCKED;\n\tINIT_WORK(&rbd_dev->acquired_lock_work, rbd_notify_acquired_lock);\n\tINIT_WORK(&rbd_dev->released_lock_work, rbd_notify_released_lock);\n\tINIT_DELAYED_WORK(&rbd_dev->lock_dwork, rbd_acquire_lock);\n\tINIT_WORK(&rbd_dev->unlock_work, rbd_release_lock_work);\n\tspin_lock_init(&rbd_dev->lock_lists_lock);\n\tINIT_LIST_HEAD(&rbd_dev->acquiring_list);\n\tINIT_LIST_HEAD(&rbd_dev->running_list);\n\tinit_completion(&rbd_dev->acquire_wait);\n\tinit_completion(&rbd_dev->releasing_wait);\n\n\tspin_lock_init(&rbd_dev->object_map_lock);\n\n\trbd_dev->dev.bus = &rbd_bus_type;\n\trbd_dev->dev.type = &rbd_device_type;\n\trbd_dev->dev.parent = &rbd_root_dev;\n\tdevice_initialize(&rbd_dev->dev);\n\n\treturn rbd_dev;\n}\n\n \nstatic struct rbd_device *rbd_dev_create(struct rbd_client *rbdc,\n\t\t\t\t\t struct rbd_spec *spec,\n\t\t\t\t\t struct rbd_options *opts)\n{\n\tstruct rbd_device *rbd_dev;\n\n\trbd_dev = __rbd_dev_create(spec);\n\tif (!rbd_dev)\n\t\treturn NULL;\n\n\t \n\trbd_dev->dev_id = ida_simple_get(&rbd_dev_id_ida, 0,\n\t\t\t\t\t minor_to_rbd_dev_id(1 << MINORBITS),\n\t\t\t\t\t GFP_KERNEL);\n\tif (rbd_dev->dev_id < 0)\n\t\tgoto fail_rbd_dev;\n\n\tsprintf(rbd_dev->name, RBD_DRV_NAME \"%d\", rbd_dev->dev_id);\n\trbd_dev->task_wq = alloc_ordered_workqueue(\"%s-tasks\", WQ_MEM_RECLAIM,\n\t\t\t\t\t\t   rbd_dev->name);\n\tif (!rbd_dev->task_wq)\n\t\tgoto fail_dev_id;\n\n\t \n\t__module_get(THIS_MODULE);\n\n\trbd_dev->rbd_client = rbdc;\n\trbd_dev->spec = spec;\n\trbd_dev->opts = opts;\n\n\tdout(\"%s rbd_dev %p dev_id %d\\n\", __func__, rbd_dev, rbd_dev->dev_id);\n\treturn rbd_dev;\n\nfail_dev_id:\n\tida_simple_remove(&rbd_dev_id_ida, rbd_dev->dev_id);\nfail_rbd_dev:\n\trbd_dev_free(rbd_dev);\n\treturn NULL;\n}\n\nstatic void rbd_dev_destroy(struct rbd_device *rbd_dev)\n{\n\tif (rbd_dev)\n\t\tput_device(&rbd_dev->dev);\n}\n\n \nstatic int _rbd_dev_v2_snap_size(struct rbd_device *rbd_dev, u64 snap_id,\n\t\t\t\tu8 *order, u64 *snap_size)\n{\n\t__le64 snapid = cpu_to_le64(snap_id);\n\tint ret;\n\tstruct {\n\t\tu8 order;\n\t\t__le64 size;\n\t} __attribute__ ((packed)) size_buf = { 0 };\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_size\",\n\t\t\t\t  &snapid, sizeof(snapid),\n\t\t\t\t  &size_buf, sizeof(size_buf));\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret < sizeof (size_buf))\n\t\treturn -ERANGE;\n\n\tif (order) {\n\t\t*order = size_buf.order;\n\t\tdout(\"  order %u\", (unsigned int)*order);\n\t}\n\t*snap_size = le64_to_cpu(size_buf.size);\n\n\tdout(\"  snap_id 0x%016llx snap_size = %llu\\n\",\n\t\t(unsigned long long)snap_id,\n\t\t(unsigned long long)*snap_size);\n\n\treturn 0;\n}\n\nstatic int rbd_dev_v2_object_prefix(struct rbd_device *rbd_dev,\n\t\t\t\t    char **pobject_prefix)\n{\n\tsize_t size;\n\tvoid *reply_buf;\n\tchar *object_prefix;\n\tint ret;\n\tvoid *p;\n\n\t \n\tsize = sizeof(__le32) + RBD_OBJ_PREFIX_LEN_MAX;\n\treply_buf = kzalloc(size, GFP_KERNEL);\n\tif (!reply_buf)\n\t\treturn -ENOMEM;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_object_prefix\",\n\t\t\t\t  NULL, 0, reply_buf, size);\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tp = reply_buf;\n\tobject_prefix = ceph_extract_encoded_string(&p, p + ret, NULL,\n\t\t\t\t\t\t    GFP_NOIO);\n\tif (IS_ERR(object_prefix)) {\n\t\tret = PTR_ERR(object_prefix);\n\t\tgoto out;\n\t}\n\tret = 0;\n\n\t*pobject_prefix = object_prefix;\n\tdout(\"  object_prefix = %s\\n\", object_prefix);\nout:\n\tkfree(reply_buf);\n\n\treturn ret;\n}\n\nstatic int _rbd_dev_v2_snap_features(struct rbd_device *rbd_dev, u64 snap_id,\n\t\t\t\t     bool read_only, u64 *snap_features)\n{\n\tstruct {\n\t\t__le64 snap_id;\n\t\tu8 read_only;\n\t} features_in;\n\tstruct {\n\t\t__le64 features;\n\t\t__le64 incompat;\n\t} __attribute__ ((packed)) features_buf = { 0 };\n\tu64 unsup;\n\tint ret;\n\n\tfeatures_in.snap_id = cpu_to_le64(snap_id);\n\tfeatures_in.read_only = read_only;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_features\",\n\t\t\t\t  &features_in, sizeof(features_in),\n\t\t\t\t  &features_buf, sizeof(features_buf));\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret < sizeof (features_buf))\n\t\treturn -ERANGE;\n\n\tunsup = le64_to_cpu(features_buf.incompat) & ~RBD_FEATURES_SUPPORTED;\n\tif (unsup) {\n\t\trbd_warn(rbd_dev, \"image uses unsupported features: 0x%llx\",\n\t\t\t unsup);\n\t\treturn -ENXIO;\n\t}\n\n\t*snap_features = le64_to_cpu(features_buf.features);\n\n\tdout(\"  snap_id 0x%016llx features = 0x%016llx incompat = 0x%016llx\\n\",\n\t\t(unsigned long long)snap_id,\n\t\t(unsigned long long)*snap_features,\n\t\t(unsigned long long)le64_to_cpu(features_buf.incompat));\n\n\treturn 0;\n}\n\n \nstatic int rbd_dev_v2_get_flags(struct rbd_device *rbd_dev)\n{\n\t__le64 snapid = cpu_to_le64(rbd_dev->spec->snap_id);\n\t__le64 flags;\n\tint ret;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_flags\",\n\t\t\t\t  &snapid, sizeof(snapid),\n\t\t\t\t  &flags, sizeof(flags));\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret < sizeof(flags))\n\t\treturn -EBADMSG;\n\n\trbd_dev->object_map_flags = le64_to_cpu(flags);\n\treturn 0;\n}\n\nstruct parent_image_info {\n\tu64\t\tpool_id;\n\tconst char\t*pool_ns;\n\tconst char\t*image_id;\n\tu64\t\tsnap_id;\n\n\tbool\t\thas_overlap;\n\tu64\t\toverlap;\n};\n\nstatic void rbd_parent_info_cleanup(struct parent_image_info *pii)\n{\n\tkfree(pii->pool_ns);\n\tkfree(pii->image_id);\n\n\tmemset(pii, 0, sizeof(*pii));\n}\n\n \nstatic int decode_parent_image_spec(void **p, void *end,\n\t\t\t\t    struct parent_image_info *pii)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tint ret;\n\n\tret = ceph_start_decoding(p, end, 1, \"ParentImageSpec\",\n\t\t\t\t  &struct_v, &struct_len);\n\tif (ret)\n\t\treturn ret;\n\n\tceph_decode_64_safe(p, end, pii->pool_id, e_inval);\n\tpii->pool_ns = ceph_extract_encoded_string(p, end, NULL, GFP_KERNEL);\n\tif (IS_ERR(pii->pool_ns)) {\n\t\tret = PTR_ERR(pii->pool_ns);\n\t\tpii->pool_ns = NULL;\n\t\treturn ret;\n\t}\n\tpii->image_id = ceph_extract_encoded_string(p, end, NULL, GFP_KERNEL);\n\tif (IS_ERR(pii->image_id)) {\n\t\tret = PTR_ERR(pii->image_id);\n\t\tpii->image_id = NULL;\n\t\treturn ret;\n\t}\n\tceph_decode_64_safe(p, end, pii->snap_id, e_inval);\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\nstatic int __get_parent_info(struct rbd_device *rbd_dev,\n\t\t\t     struct page *req_page,\n\t\t\t     struct page *reply_page,\n\t\t\t     struct parent_image_info *pii)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tsize_t reply_len = PAGE_SIZE;\n\tvoid *p, *end;\n\tint ret;\n\n\tret = ceph_osdc_call(osdc, &rbd_dev->header_oid, &rbd_dev->header_oloc,\n\t\t\t     \"rbd\", \"parent_get\", CEPH_OSD_FLAG_READ,\n\t\t\t     req_page, sizeof(u64), &reply_page, &reply_len);\n\tif (ret)\n\t\treturn ret == -EOPNOTSUPP ? 1 : ret;\n\n\tp = page_address(reply_page);\n\tend = p + reply_len;\n\tret = decode_parent_image_spec(&p, end, pii);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ceph_osdc_call(osdc, &rbd_dev->header_oid, &rbd_dev->header_oloc,\n\t\t\t     \"rbd\", \"parent_overlap_get\", CEPH_OSD_FLAG_READ,\n\t\t\t     req_page, sizeof(u64), &reply_page, &reply_len);\n\tif (ret)\n\t\treturn ret;\n\n\tp = page_address(reply_page);\n\tend = p + reply_len;\n\tceph_decode_8_safe(&p, end, pii->has_overlap, e_inval);\n\tif (pii->has_overlap)\n\t\tceph_decode_64_safe(&p, end, pii->overlap, e_inval);\n\n\tdout(\"%s pool_id %llu pool_ns %s image_id %s snap_id %llu has_overlap %d overlap %llu\\n\",\n\t     __func__, pii->pool_id, pii->pool_ns, pii->image_id, pii->snap_id,\n\t     pii->has_overlap, pii->overlap);\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\n \nstatic int __get_parent_info_legacy(struct rbd_device *rbd_dev,\n\t\t\t\t    struct page *req_page,\n\t\t\t\t    struct page *reply_page,\n\t\t\t\t    struct parent_image_info *pii)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tsize_t reply_len = PAGE_SIZE;\n\tvoid *p, *end;\n\tint ret;\n\n\tret = ceph_osdc_call(osdc, &rbd_dev->header_oid, &rbd_dev->header_oloc,\n\t\t\t     \"rbd\", \"get_parent\", CEPH_OSD_FLAG_READ,\n\t\t\t     req_page, sizeof(u64), &reply_page, &reply_len);\n\tif (ret)\n\t\treturn ret;\n\n\tp = page_address(reply_page);\n\tend = p + reply_len;\n\tceph_decode_64_safe(&p, end, pii->pool_id, e_inval);\n\tpii->image_id = ceph_extract_encoded_string(&p, end, NULL, GFP_KERNEL);\n\tif (IS_ERR(pii->image_id)) {\n\t\tret = PTR_ERR(pii->image_id);\n\t\tpii->image_id = NULL;\n\t\treturn ret;\n\t}\n\tceph_decode_64_safe(&p, end, pii->snap_id, e_inval);\n\tpii->has_overlap = true;\n\tceph_decode_64_safe(&p, end, pii->overlap, e_inval);\n\n\tdout(\"%s pool_id %llu pool_ns %s image_id %s snap_id %llu has_overlap %d overlap %llu\\n\",\n\t     __func__, pii->pool_id, pii->pool_ns, pii->image_id, pii->snap_id,\n\t     pii->has_overlap, pii->overlap);\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\nstatic int rbd_dev_v2_parent_info(struct rbd_device *rbd_dev,\n\t\t\t\t  struct parent_image_info *pii)\n{\n\tstruct page *req_page, *reply_page;\n\tvoid *p;\n\tint ret;\n\n\treq_page = alloc_page(GFP_KERNEL);\n\tif (!req_page)\n\t\treturn -ENOMEM;\n\n\treply_page = alloc_page(GFP_KERNEL);\n\tif (!reply_page) {\n\t\t__free_page(req_page);\n\t\treturn -ENOMEM;\n\t}\n\n\tp = page_address(req_page);\n\tceph_encode_64(&p, rbd_dev->spec->snap_id);\n\tret = __get_parent_info(rbd_dev, req_page, reply_page, pii);\n\tif (ret > 0)\n\t\tret = __get_parent_info_legacy(rbd_dev, req_page, reply_page,\n\t\t\t\t\t       pii);\n\n\t__free_page(req_page);\n\t__free_page(reply_page);\n\treturn ret;\n}\n\nstatic int rbd_dev_setup_parent(struct rbd_device *rbd_dev)\n{\n\tstruct rbd_spec *parent_spec;\n\tstruct parent_image_info pii = { 0 };\n\tint ret;\n\n\tparent_spec = rbd_spec_alloc();\n\tif (!parent_spec)\n\t\treturn -ENOMEM;\n\n\tret = rbd_dev_v2_parent_info(rbd_dev, &pii);\n\tif (ret)\n\t\tgoto out_err;\n\n\tif (pii.pool_id == CEPH_NOPOOL || !pii.has_overlap)\n\t\tgoto out;\t \n\n\t \n\n\tret = -EIO;\n\tif (pii.pool_id > (u64)U32_MAX) {\n\t\trbd_warn(NULL, \"parent pool id too large (%llu > %u)\",\n\t\t\t(unsigned long long)pii.pool_id, U32_MAX);\n\t\tgoto out_err;\n\t}\n\n\t \n\tparent_spec->pool_id = pii.pool_id;\n\tif (pii.pool_ns && *pii.pool_ns) {\n\t\tparent_spec->pool_ns = pii.pool_ns;\n\t\tpii.pool_ns = NULL;\n\t}\n\tparent_spec->image_id = pii.image_id;\n\tpii.image_id = NULL;\n\tparent_spec->snap_id = pii.snap_id;\n\n\trbd_assert(!rbd_dev->parent_spec);\n\trbd_dev->parent_spec = parent_spec;\n\tparent_spec = NULL;\t \n\n\t \n\tif (!pii.overlap)\n\t\trbd_warn(rbd_dev, \"clone is standalone (overlap 0)\");\n\trbd_dev->parent_overlap = pii.overlap;\n\nout:\n\tret = 0;\nout_err:\n\trbd_parent_info_cleanup(&pii);\n\trbd_spec_put(parent_spec);\n\treturn ret;\n}\n\nstatic int rbd_dev_v2_striping_info(struct rbd_device *rbd_dev,\n\t\t\t\t    u64 *stripe_unit, u64 *stripe_count)\n{\n\tstruct {\n\t\t__le64 stripe_unit;\n\t\t__le64 stripe_count;\n\t} __attribute__ ((packed)) striping_info_buf = { 0 };\n\tsize_t size = sizeof (striping_info_buf);\n\tint ret;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t&rbd_dev->header_oloc, \"get_stripe_unit_count\",\n\t\t\t\tNULL, 0, &striping_info_buf, size);\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret < size)\n\t\treturn -ERANGE;\n\n\t*stripe_unit = le64_to_cpu(striping_info_buf.stripe_unit);\n\t*stripe_count = le64_to_cpu(striping_info_buf.stripe_count);\n\tdout(\"  stripe_unit = %llu stripe_count = %llu\\n\", *stripe_unit,\n\t     *stripe_count);\n\n\treturn 0;\n}\n\nstatic int rbd_dev_v2_data_pool(struct rbd_device *rbd_dev, s64 *data_pool_id)\n{\n\t__le64 data_pool_buf;\n\tint ret;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_data_pool\",\n\t\t\t\t  NULL, 0, &data_pool_buf,\n\t\t\t\t  sizeof(data_pool_buf));\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret < sizeof(data_pool_buf))\n\t\treturn -EBADMSG;\n\n\t*data_pool_id = le64_to_cpu(data_pool_buf);\n\tdout(\"  data_pool_id = %lld\\n\", *data_pool_id);\n\tWARN_ON(*data_pool_id == CEPH_NOPOOL);\n\n\treturn 0;\n}\n\nstatic char *rbd_dev_image_name(struct rbd_device *rbd_dev)\n{\n\tCEPH_DEFINE_OID_ONSTACK(oid);\n\tsize_t image_id_size;\n\tchar *image_id;\n\tvoid *p;\n\tvoid *end;\n\tsize_t size;\n\tvoid *reply_buf = NULL;\n\tsize_t len = 0;\n\tchar *image_name = NULL;\n\tint ret;\n\n\trbd_assert(!rbd_dev->spec->image_name);\n\n\tlen = strlen(rbd_dev->spec->image_id);\n\timage_id_size = sizeof (__le32) + len;\n\timage_id = kmalloc(image_id_size, GFP_KERNEL);\n\tif (!image_id)\n\t\treturn NULL;\n\n\tp = image_id;\n\tend = image_id + image_id_size;\n\tceph_encode_string(&p, end, rbd_dev->spec->image_id, (u32)len);\n\n\tsize = sizeof (__le32) + RBD_IMAGE_NAME_LEN_MAX;\n\treply_buf = kmalloc(size, GFP_KERNEL);\n\tif (!reply_buf)\n\t\tgoto out;\n\n\tceph_oid_printf(&oid, \"%s\", RBD_DIRECTORY);\n\tret = rbd_obj_method_sync(rbd_dev, &oid, &rbd_dev->header_oloc,\n\t\t\t\t  \"dir_get_name\", image_id, image_id_size,\n\t\t\t\t  reply_buf, size);\n\tif (ret < 0)\n\t\tgoto out;\n\tp = reply_buf;\n\tend = reply_buf + ret;\n\n\timage_name = ceph_extract_encoded_string(&p, end, &len, GFP_KERNEL);\n\tif (IS_ERR(image_name))\n\t\timage_name = NULL;\n\telse\n\t\tdout(\"%s: name is %s len is %zd\\n\", __func__, image_name, len);\nout:\n\tkfree(reply_buf);\n\tkfree(image_id);\n\n\treturn image_name;\n}\n\nstatic u64 rbd_v1_snap_id_by_name(struct rbd_device *rbd_dev, const char *name)\n{\n\tstruct ceph_snap_context *snapc = rbd_dev->header.snapc;\n\tconst char *snap_name;\n\tu32 which = 0;\n\n\t \n\n\tsnap_name = rbd_dev->header.snap_names;\n\twhile (which < snapc->num_snaps) {\n\t\tif (!strcmp(name, snap_name))\n\t\t\treturn snapc->snaps[which];\n\t\tsnap_name += strlen(snap_name) + 1;\n\t\twhich++;\n\t}\n\treturn CEPH_NOSNAP;\n}\n\nstatic u64 rbd_v2_snap_id_by_name(struct rbd_device *rbd_dev, const char *name)\n{\n\tstruct ceph_snap_context *snapc = rbd_dev->header.snapc;\n\tu32 which;\n\tbool found = false;\n\tu64 snap_id;\n\n\tfor (which = 0; !found && which < snapc->num_snaps; which++) {\n\t\tconst char *snap_name;\n\n\t\tsnap_id = snapc->snaps[which];\n\t\tsnap_name = rbd_dev_v2_snap_name(rbd_dev, snap_id);\n\t\tif (IS_ERR(snap_name)) {\n\t\t\t \n\t\t\tif (PTR_ERR(snap_name) == -ENOENT)\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t\tfound = !strcmp(name, snap_name);\n\t\tkfree(snap_name);\n\t}\n\treturn found ? snap_id : CEPH_NOSNAP;\n}\n\n \nstatic u64 rbd_snap_id_by_name(struct rbd_device *rbd_dev, const char *name)\n{\n\tif (rbd_dev->image_format == 1)\n\t\treturn rbd_v1_snap_id_by_name(rbd_dev, name);\n\n\treturn rbd_v2_snap_id_by_name(rbd_dev, name);\n}\n\n \nstatic int rbd_spec_fill_snap_id(struct rbd_device *rbd_dev)\n{\n\tstruct rbd_spec *spec = rbd_dev->spec;\n\n\trbd_assert(spec->pool_id != CEPH_NOPOOL && spec->pool_name);\n\trbd_assert(spec->image_id && spec->image_name);\n\trbd_assert(spec->snap_name);\n\n\tif (strcmp(spec->snap_name, RBD_SNAP_HEAD_NAME)) {\n\t\tu64 snap_id;\n\n\t\tsnap_id = rbd_snap_id_by_name(rbd_dev, spec->snap_name);\n\t\tif (snap_id == CEPH_NOSNAP)\n\t\t\treturn -ENOENT;\n\n\t\tspec->snap_id = snap_id;\n\t} else {\n\t\tspec->snap_id = CEPH_NOSNAP;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int rbd_spec_fill_names(struct rbd_device *rbd_dev)\n{\n\tstruct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;\n\tstruct rbd_spec *spec = rbd_dev->spec;\n\tconst char *pool_name;\n\tconst char *image_name;\n\tconst char *snap_name;\n\tint ret;\n\n\trbd_assert(spec->pool_id != CEPH_NOPOOL);\n\trbd_assert(spec->image_id);\n\trbd_assert(spec->snap_id != CEPH_NOSNAP);\n\n\t \n\n\tpool_name = ceph_pg_pool_name_by_id(osdc->osdmap, spec->pool_id);\n\tif (!pool_name) {\n\t\trbd_warn(rbd_dev, \"no pool with id %llu\", spec->pool_id);\n\t\treturn -EIO;\n\t}\n\tpool_name = kstrdup(pool_name, GFP_KERNEL);\n\tif (!pool_name)\n\t\treturn -ENOMEM;\n\n\t \n\n\timage_name = rbd_dev_image_name(rbd_dev);\n\tif (!image_name)\n\t\trbd_warn(rbd_dev, \"unable to get image name\");\n\n\t \n\n\tsnap_name = rbd_snap_name(rbd_dev, spec->snap_id);\n\tif (IS_ERR(snap_name)) {\n\t\tret = PTR_ERR(snap_name);\n\t\tgoto out_err;\n\t}\n\n\tspec->pool_name = pool_name;\n\tspec->image_name = image_name;\n\tspec->snap_name = snap_name;\n\n\treturn 0;\n\nout_err:\n\tkfree(image_name);\n\tkfree(pool_name);\n\treturn ret;\n}\n\nstatic int rbd_dev_v2_snap_context(struct rbd_device *rbd_dev,\n\t\t\t\t   struct ceph_snap_context **psnapc)\n{\n\tsize_t size;\n\tint ret;\n\tvoid *reply_buf;\n\tvoid *p;\n\tvoid *end;\n\tu64 seq;\n\tu32 snap_count;\n\tstruct ceph_snap_context *snapc;\n\tu32 i;\n\n\t \n\tsize = sizeof (__le64) + sizeof (__le32) +\n\t\t\tRBD_MAX_SNAP_COUNT * sizeof (__le64);\n\treply_buf = kzalloc(size, GFP_KERNEL);\n\tif (!reply_buf)\n\t\treturn -ENOMEM;\n\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_snapcontext\",\n\t\t\t\t  NULL, 0, reply_buf, size);\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tp = reply_buf;\n\tend = reply_buf + ret;\n\tret = -ERANGE;\n\tceph_decode_64_safe(&p, end, seq, out);\n\tceph_decode_32_safe(&p, end, snap_count, out);\n\n\t \n\tif (snap_count > (SIZE_MAX - sizeof (struct ceph_snap_context))\n\t\t\t\t / sizeof (u64)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!ceph_has_room(&p, end, snap_count * sizeof (__le64)))\n\t\tgoto out;\n\tret = 0;\n\n\tsnapc = ceph_create_snap_context(snap_count, GFP_KERNEL);\n\tif (!snapc) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsnapc->seq = seq;\n\tfor (i = 0; i < snap_count; i++)\n\t\tsnapc->snaps[i] = ceph_decode_64(&p);\n\n\t*psnapc = snapc;\n\tdout(\"  snap context seq = %llu, snap_count = %u\\n\",\n\t\t(unsigned long long)seq, (unsigned int)snap_count);\nout:\n\tkfree(reply_buf);\n\n\treturn ret;\n}\n\nstatic const char *rbd_dev_v2_snap_name(struct rbd_device *rbd_dev,\n\t\t\t\t\tu64 snap_id)\n{\n\tsize_t size;\n\tvoid *reply_buf;\n\t__le64 snapid;\n\tint ret;\n\tvoid *p;\n\tvoid *end;\n\tchar *snap_name;\n\n\tsize = sizeof (__le32) + RBD_MAX_SNAP_NAME_LEN;\n\treply_buf = kmalloc(size, GFP_KERNEL);\n\tif (!reply_buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tsnapid = cpu_to_le64(snap_id);\n\tret = rbd_obj_method_sync(rbd_dev, &rbd_dev->header_oid,\n\t\t\t\t  &rbd_dev->header_oloc, \"get_snapshot_name\",\n\t\t\t\t  &snapid, sizeof(snapid), reply_buf, size);\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret < 0) {\n\t\tsnap_name = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tp = reply_buf;\n\tend = reply_buf + ret;\n\tsnap_name = ceph_extract_encoded_string(&p, end, NULL, GFP_KERNEL);\n\tif (IS_ERR(snap_name))\n\t\tgoto out;\n\n\tdout(\"  snap_id 0x%016llx snap_name = %s\\n\",\n\t\t(unsigned long long)snap_id, snap_name);\nout:\n\tkfree(reply_buf);\n\n\treturn snap_name;\n}\n\nstatic int rbd_dev_v2_header_info(struct rbd_device *rbd_dev,\n\t\t\t\t  struct rbd_image_header *header,\n\t\t\t\t  bool first_time)\n{\n\tint ret;\n\n\tret = _rbd_dev_v2_snap_size(rbd_dev, CEPH_NOSNAP,\n\t\t\t\t    first_time ? &header->obj_order : NULL,\n\t\t\t\t    &header->image_size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (first_time) {\n\t\tret = rbd_dev_v2_header_onetime(rbd_dev, header);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = rbd_dev_v2_snap_context(rbd_dev, &header->snapc);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic int rbd_dev_header_info(struct rbd_device *rbd_dev,\n\t\t\t       struct rbd_image_header *header,\n\t\t\t       bool first_time)\n{\n\trbd_assert(rbd_image_format_valid(rbd_dev->image_format));\n\trbd_assert(!header->object_prefix && !header->snapc);\n\n\tif (rbd_dev->image_format == 1)\n\t\treturn rbd_dev_v1_header_info(rbd_dev, header, first_time);\n\n\treturn rbd_dev_v2_header_info(rbd_dev, header, first_time);\n}\n\n \nstatic inline size_t next_token(const char **buf)\n{\n         \n\tstatic const char spaces[] = \" \\f\\n\\r\\t\\v\";\n\n        *buf += strspn(*buf, spaces);\t \n\n\treturn strcspn(*buf, spaces);    \n}\n\n \nstatic inline char *dup_token(const char **buf, size_t *lenp)\n{\n\tchar *dup;\n\tsize_t len;\n\n\tlen = next_token(buf);\n\tdup = kmemdup(*buf, len + 1, GFP_KERNEL);\n\tif (!dup)\n\t\treturn NULL;\n\t*(dup + len) = '\\0';\n\t*buf += len;\n\n\tif (lenp)\n\t\t*lenp = len;\n\n\treturn dup;\n}\n\nstatic int rbd_parse_param(struct fs_parameter *param,\n\t\t\t    struct rbd_parse_opts_ctx *pctx)\n{\n\tstruct rbd_options *opt = pctx->opts;\n\tstruct fs_parse_result result;\n\tstruct p_log log = {.prefix = \"rbd\"};\n\tint token, ret;\n\n\tret = ceph_parse_param(param, pctx->copts, NULL);\n\tif (ret != -ENOPARAM)\n\t\treturn ret;\n\n\ttoken = __fs_parse(&log, rbd_parameters, param, &result);\n\tdout(\"%s fs_parse '%s' token %d\\n\", __func__, param->key, token);\n\tif (token < 0) {\n\t\tif (token == -ENOPARAM)\n\t\t\treturn inval_plog(&log, \"Unknown parameter '%s'\",\n\t\t\t\t\t  param->key);\n\t\treturn token;\n\t}\n\n\tswitch (token) {\n\tcase Opt_queue_depth:\n\t\tif (result.uint_32 < 1)\n\t\t\tgoto out_of_range;\n\t\topt->queue_depth = result.uint_32;\n\t\tbreak;\n\tcase Opt_alloc_size:\n\t\tif (result.uint_32 < SECTOR_SIZE)\n\t\t\tgoto out_of_range;\n\t\tif (!is_power_of_2(result.uint_32))\n\t\t\treturn inval_plog(&log, \"alloc_size must be a power of 2\");\n\t\topt->alloc_size = result.uint_32;\n\t\tbreak;\n\tcase Opt_lock_timeout:\n\t\t \n\t\tif (result.uint_32 > INT_MAX / 1000)\n\t\t\tgoto out_of_range;\n\t\topt->lock_timeout = msecs_to_jiffies(result.uint_32 * 1000);\n\t\tbreak;\n\tcase Opt_pool_ns:\n\t\tkfree(pctx->spec->pool_ns);\n\t\tpctx->spec->pool_ns = param->string;\n\t\tparam->string = NULL;\n\t\tbreak;\n\tcase Opt_compression_hint:\n\t\tswitch (result.uint_32) {\n\t\tcase Opt_compression_hint_none:\n\t\t\topt->alloc_hint_flags &=\n\t\t\t    ~(CEPH_OSD_ALLOC_HINT_FLAG_COMPRESSIBLE |\n\t\t\t      CEPH_OSD_ALLOC_HINT_FLAG_INCOMPRESSIBLE);\n\t\t\tbreak;\n\t\tcase Opt_compression_hint_compressible:\n\t\t\topt->alloc_hint_flags |=\n\t\t\t    CEPH_OSD_ALLOC_HINT_FLAG_COMPRESSIBLE;\n\t\t\topt->alloc_hint_flags &=\n\t\t\t    ~CEPH_OSD_ALLOC_HINT_FLAG_INCOMPRESSIBLE;\n\t\t\tbreak;\n\t\tcase Opt_compression_hint_incompressible:\n\t\t\topt->alloc_hint_flags |=\n\t\t\t    CEPH_OSD_ALLOC_HINT_FLAG_INCOMPRESSIBLE;\n\t\t\topt->alloc_hint_flags &=\n\t\t\t    ~CEPH_OSD_ALLOC_HINT_FLAG_COMPRESSIBLE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\tcase Opt_read_only:\n\t\topt->read_only = true;\n\t\tbreak;\n\tcase Opt_read_write:\n\t\topt->read_only = false;\n\t\tbreak;\n\tcase Opt_lock_on_read:\n\t\topt->lock_on_read = true;\n\t\tbreak;\n\tcase Opt_exclusive:\n\t\topt->exclusive = true;\n\t\tbreak;\n\tcase Opt_notrim:\n\t\topt->trim = false;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn 0;\n\nout_of_range:\n\treturn inval_plog(&log, \"%s out of range\", param->key);\n}\n\n \nstatic int rbd_parse_options(char *options, struct rbd_parse_opts_ctx *pctx)\n{\n\tchar *key;\n\tint ret = 0;\n\n\tdout(\"%s '%s'\\n\", __func__, options);\n\twhile ((key = strsep(&options, \",\")) != NULL) {\n\t\tif (*key) {\n\t\t\tstruct fs_parameter param = {\n\t\t\t\t.key\t= key,\n\t\t\t\t.type\t= fs_value_is_flag,\n\t\t\t};\n\t\t\tchar *value = strchr(key, '=');\n\t\t\tsize_t v_len = 0;\n\n\t\t\tif (value) {\n\t\t\t\tif (value == key)\n\t\t\t\t\tcontinue;\n\t\t\t\t*value++ = 0;\n\t\t\t\tv_len = strlen(value);\n\t\t\t\tparam.string = kmemdup_nul(value, v_len,\n\t\t\t\t\t\t\t   GFP_KERNEL);\n\t\t\t\tif (!param.string)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tparam.type = fs_value_is_string;\n\t\t\t}\n\t\t\tparam.size = v_len;\n\n\t\t\tret = rbd_parse_param(&param, pctx);\n\t\t\tkfree(param.string);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int rbd_add_parse_args(const char *buf,\n\t\t\t\tstruct ceph_options **ceph_opts,\n\t\t\t\tstruct rbd_options **opts,\n\t\t\t\tstruct rbd_spec **rbd_spec)\n{\n\tsize_t len;\n\tchar *options;\n\tconst char *mon_addrs;\n\tchar *snap_name;\n\tsize_t mon_addrs_size;\n\tstruct rbd_parse_opts_ctx pctx = { 0 };\n\tint ret;\n\n\t \n\n\tlen = next_token(&buf);\n\tif (!len) {\n\t\trbd_warn(NULL, \"no monitor address(es) provided\");\n\t\treturn -EINVAL;\n\t}\n\tmon_addrs = buf;\n\tmon_addrs_size = len;\n\tbuf += len;\n\n\tret = -EINVAL;\n\toptions = dup_token(&buf, NULL);\n\tif (!options)\n\t\treturn -ENOMEM;\n\tif (!*options) {\n\t\trbd_warn(NULL, \"no options provided\");\n\t\tgoto out_err;\n\t}\n\n\tpctx.spec = rbd_spec_alloc();\n\tif (!pctx.spec)\n\t\tgoto out_mem;\n\n\tpctx.spec->pool_name = dup_token(&buf, NULL);\n\tif (!pctx.spec->pool_name)\n\t\tgoto out_mem;\n\tif (!*pctx.spec->pool_name) {\n\t\trbd_warn(NULL, \"no pool name provided\");\n\t\tgoto out_err;\n\t}\n\n\tpctx.spec->image_name = dup_token(&buf, NULL);\n\tif (!pctx.spec->image_name)\n\t\tgoto out_mem;\n\tif (!*pctx.spec->image_name) {\n\t\trbd_warn(NULL, \"no image name provided\");\n\t\tgoto out_err;\n\t}\n\n\t \n\tlen = next_token(&buf);\n\tif (!len) {\n\t\tbuf = RBD_SNAP_HEAD_NAME;  \n\t\tlen = sizeof (RBD_SNAP_HEAD_NAME) - 1;\n\t} else if (len > RBD_MAX_SNAP_NAME_LEN) {\n\t\tret = -ENAMETOOLONG;\n\t\tgoto out_err;\n\t}\n\tsnap_name = kmemdup(buf, len + 1, GFP_KERNEL);\n\tif (!snap_name)\n\t\tgoto out_mem;\n\t*(snap_name + len) = '\\0';\n\tpctx.spec->snap_name = snap_name;\n\n\tpctx.copts = ceph_alloc_options();\n\tif (!pctx.copts)\n\t\tgoto out_mem;\n\n\t \n\n\tpctx.opts = kzalloc(sizeof(*pctx.opts), GFP_KERNEL);\n\tif (!pctx.opts)\n\t\tgoto out_mem;\n\n\tpctx.opts->read_only = RBD_READ_ONLY_DEFAULT;\n\tpctx.opts->queue_depth = RBD_QUEUE_DEPTH_DEFAULT;\n\tpctx.opts->alloc_size = RBD_ALLOC_SIZE_DEFAULT;\n\tpctx.opts->lock_timeout = RBD_LOCK_TIMEOUT_DEFAULT;\n\tpctx.opts->lock_on_read = RBD_LOCK_ON_READ_DEFAULT;\n\tpctx.opts->exclusive = RBD_EXCLUSIVE_DEFAULT;\n\tpctx.opts->trim = RBD_TRIM_DEFAULT;\n\n\tret = ceph_parse_mon_ips(mon_addrs, mon_addrs_size, pctx.copts, NULL,\n\t\t\t\t ',');\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = rbd_parse_options(options, &pctx);\n\tif (ret)\n\t\tgoto out_err;\n\n\t*ceph_opts = pctx.copts;\n\t*opts = pctx.opts;\n\t*rbd_spec = pctx.spec;\n\tkfree(options);\n\treturn 0;\n\nout_mem:\n\tret = -ENOMEM;\nout_err:\n\tkfree(pctx.opts);\n\tceph_destroy_options(pctx.copts);\n\trbd_spec_put(pctx.spec);\n\tkfree(options);\n\treturn ret;\n}\n\nstatic void rbd_dev_image_unlock(struct rbd_device *rbd_dev)\n{\n\tdown_write(&rbd_dev->lock_rwsem);\n\tif (__rbd_is_lock_owner(rbd_dev))\n\t\t__rbd_release_lock(rbd_dev);\n\tup_write(&rbd_dev->lock_rwsem);\n}\n\n \nstatic int rbd_add_acquire_lock(struct rbd_device *rbd_dev)\n{\n\tlong ret;\n\n\tif (!(rbd_dev->header.features & RBD_FEATURE_EXCLUSIVE_LOCK)) {\n\t\tif (!rbd_dev->opts->exclusive && !rbd_dev->opts->lock_on_read)\n\t\t\treturn 0;\n\n\t\trbd_warn(rbd_dev, \"exclusive-lock feature is not enabled\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (rbd_is_ro(rbd_dev))\n\t\treturn 0;\n\n\trbd_assert(!rbd_is_lock_owner(rbd_dev));\n\tqueue_delayed_work(rbd_dev->task_wq, &rbd_dev->lock_dwork, 0);\n\tret = wait_for_completion_killable_timeout(&rbd_dev->acquire_wait,\n\t\t\t    ceph_timeout_jiffies(rbd_dev->opts->lock_timeout));\n\tif (ret > 0) {\n\t\tret = rbd_dev->acquire_err;\n\t} else {\n\t\tcancel_delayed_work_sync(&rbd_dev->lock_dwork);\n\t\tif (!ret)\n\t\t\tret = -ETIMEDOUT;\n\n\t\trbd_warn(rbd_dev, \"failed to acquire lock: %ld\", ret);\n\t}\n\tif (ret)\n\t\treturn ret;\n\n\t \n\trbd_assert(!rbd_dev->opts->exclusive || rbd_is_lock_owner(rbd_dev));\n\treturn 0;\n}\n\n \nstatic int rbd_dev_image_id(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\tsize_t size;\n\tCEPH_DEFINE_OID_ONSTACK(oid);\n\tvoid *response;\n\tchar *image_id;\n\n\t \n\tif (rbd_dev->spec->image_id) {\n\t\trbd_dev->image_format = *rbd_dev->spec->image_id ? 2 : 1;\n\n\t\treturn 0;\n\t}\n\n\t \n\tret = ceph_oid_aprintf(&oid, GFP_KERNEL, \"%s%s\", RBD_ID_PREFIX,\n\t\t\t       rbd_dev->spec->image_name);\n\tif (ret)\n\t\treturn ret;\n\n\tdout(\"rbd id object name is %s\\n\", oid.name);\n\n\t \n\tsize = sizeof (__le32) + RBD_IMAGE_ID_LEN_MAX;\n\tresponse = kzalloc(size, GFP_NOIO);\n\tif (!response) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\n\tret = rbd_obj_method_sync(rbd_dev, &oid, &rbd_dev->header_oloc,\n\t\t\t\t  \"get_id\", NULL, 0,\n\t\t\t\t  response, size);\n\tdout(\"%s: rbd_obj_method_sync returned %d\\n\", __func__, ret);\n\tif (ret == -ENOENT) {\n\t\timage_id = kstrdup(\"\", GFP_KERNEL);\n\t\tret = image_id ? 0 : -ENOMEM;\n\t\tif (!ret)\n\t\t\trbd_dev->image_format = 1;\n\t} else if (ret >= 0) {\n\t\tvoid *p = response;\n\n\t\timage_id = ceph_extract_encoded_string(&p, p + ret,\n\t\t\t\t\t\tNULL, GFP_NOIO);\n\t\tret = PTR_ERR_OR_ZERO(image_id);\n\t\tif (!ret)\n\t\t\trbd_dev->image_format = 2;\n\t}\n\n\tif (!ret) {\n\t\trbd_dev->spec->image_id = image_id;\n\t\tdout(\"image_id is %s\\n\", image_id);\n\t}\nout:\n\tkfree(response);\n\tceph_oid_destroy(&oid);\n\treturn ret;\n}\n\n \nstatic void rbd_dev_unprobe(struct rbd_device *rbd_dev)\n{\n\trbd_dev_parent_put(rbd_dev);\n\trbd_object_map_free(rbd_dev);\n\trbd_dev_mapping_clear(rbd_dev);\n\n\t \n\n\trbd_image_header_cleanup(&rbd_dev->header);\n}\n\nstatic int rbd_dev_v2_header_onetime(struct rbd_device *rbd_dev,\n\t\t\t\t     struct rbd_image_header *header)\n{\n\tint ret;\n\n\tret = rbd_dev_v2_object_prefix(rbd_dev, &header->object_prefix);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = _rbd_dev_v2_snap_features(rbd_dev, CEPH_NOSNAP,\n\t\t\t\t\trbd_is_ro(rbd_dev), &header->features);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\n\tif (header->features & RBD_FEATURE_STRIPINGV2) {\n\t\tret = rbd_dev_v2_striping_info(rbd_dev, &header->stripe_unit,\n\t\t\t\t\t       &header->stripe_count);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (header->features & RBD_FEATURE_DATA_POOL) {\n\t\tret = rbd_dev_v2_data_pool(rbd_dev, &header->data_pool_id);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int rbd_dev_probe_parent(struct rbd_device *rbd_dev, int depth)\n{\n\tstruct rbd_device *parent = NULL;\n\tint ret;\n\n\tif (!rbd_dev->parent_spec)\n\t\treturn 0;\n\n\tif (++depth > RBD_MAX_PARENT_CHAIN_LEN) {\n\t\tpr_info(\"parent chain is too long (%d)\\n\", depth);\n\t\tret = -EINVAL;\n\t\tgoto out_err;\n\t}\n\n\tparent = __rbd_dev_create(rbd_dev->parent_spec);\n\tif (!parent) {\n\t\tret = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\t \n\tparent->rbd_client = __rbd_get_client(rbd_dev->rbd_client);\n\tparent->spec = rbd_spec_get(rbd_dev->parent_spec);\n\n\t__set_bit(RBD_DEV_FLAG_READONLY, &parent->flags);\n\n\tret = rbd_dev_image_probe(parent, depth);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\trbd_dev->parent = parent;\n\tatomic_set(&rbd_dev->parent_ref, 1);\n\treturn 0;\n\nout_err:\n\trbd_dev_unparent(rbd_dev);\n\trbd_dev_destroy(parent);\n\treturn ret;\n}\n\nstatic void rbd_dev_device_release(struct rbd_device *rbd_dev)\n{\n\tclear_bit(RBD_DEV_FLAG_EXISTS, &rbd_dev->flags);\n\trbd_free_disk(rbd_dev);\n\tif (!single_major)\n\t\tunregister_blkdev(rbd_dev->major, rbd_dev->name);\n}\n\n \nstatic int rbd_dev_device_setup(struct rbd_device *rbd_dev)\n{\n\tint ret;\n\n\t \n\n\tif (!single_major) {\n\t\tret = register_blkdev(0, rbd_dev->name);\n\t\tif (ret < 0)\n\t\t\tgoto err_out_unlock;\n\n\t\trbd_dev->major = ret;\n\t\trbd_dev->minor = 0;\n\t} else {\n\t\trbd_dev->major = rbd_major;\n\t\trbd_dev->minor = rbd_dev_id_to_minor(rbd_dev->dev_id);\n\t}\n\n\t \n\n\tret = rbd_init_disk(rbd_dev);\n\tif (ret)\n\t\tgoto err_out_blkdev;\n\n\tset_capacity(rbd_dev->disk, rbd_dev->mapping.size / SECTOR_SIZE);\n\tset_disk_ro(rbd_dev->disk, rbd_is_ro(rbd_dev));\n\n\tret = dev_set_name(&rbd_dev->dev, \"%d\", rbd_dev->dev_id);\n\tif (ret)\n\t\tgoto err_out_disk;\n\n\tset_bit(RBD_DEV_FLAG_EXISTS, &rbd_dev->flags);\n\tup_write(&rbd_dev->header_rwsem);\n\treturn 0;\n\nerr_out_disk:\n\trbd_free_disk(rbd_dev);\nerr_out_blkdev:\n\tif (!single_major)\n\t\tunregister_blkdev(rbd_dev->major, rbd_dev->name);\nerr_out_unlock:\n\tup_write(&rbd_dev->header_rwsem);\n\treturn ret;\n}\n\nstatic int rbd_dev_header_name(struct rbd_device *rbd_dev)\n{\n\tstruct rbd_spec *spec = rbd_dev->spec;\n\tint ret;\n\n\t \n\n\trbd_assert(rbd_image_format_valid(rbd_dev->image_format));\n\tif (rbd_dev->image_format == 1)\n\t\tret = ceph_oid_aprintf(&rbd_dev->header_oid, GFP_KERNEL, \"%s%s\",\n\t\t\t\t       spec->image_name, RBD_SUFFIX);\n\telse\n\t\tret = ceph_oid_aprintf(&rbd_dev->header_oid, GFP_KERNEL, \"%s%s\",\n\t\t\t\t       RBD_HEADER_PREFIX, spec->image_id);\n\n\treturn ret;\n}\n\nstatic void rbd_print_dne(struct rbd_device *rbd_dev, bool is_snap)\n{\n\tif (!is_snap) {\n\t\tpr_info(\"image %s/%s%s%s does not exist\\n\",\n\t\t\trbd_dev->spec->pool_name,\n\t\t\trbd_dev->spec->pool_ns ?: \"\",\n\t\t\trbd_dev->spec->pool_ns ? \"/\" : \"\",\n\t\t\trbd_dev->spec->image_name);\n\t} else {\n\t\tpr_info(\"snap %s/%s%s%s@%s does not exist\\n\",\n\t\t\trbd_dev->spec->pool_name,\n\t\t\trbd_dev->spec->pool_ns ?: \"\",\n\t\t\trbd_dev->spec->pool_ns ? \"/\" : \"\",\n\t\t\trbd_dev->spec->image_name,\n\t\t\trbd_dev->spec->snap_name);\n\t}\n}\n\nstatic void rbd_dev_image_release(struct rbd_device *rbd_dev)\n{\n\tif (!rbd_is_ro(rbd_dev))\n\t\trbd_unregister_watch(rbd_dev);\n\n\trbd_dev_unprobe(rbd_dev);\n\trbd_dev->image_format = 0;\n\tkfree(rbd_dev->spec->image_id);\n\trbd_dev->spec->image_id = NULL;\n}\n\n \nstatic int rbd_dev_image_probe(struct rbd_device *rbd_dev, int depth)\n{\n\tbool need_watch = !rbd_is_ro(rbd_dev);\n\tint ret;\n\n\t \n\tret = rbd_dev_image_id(rbd_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rbd_dev_header_name(rbd_dev);\n\tif (ret)\n\t\tgoto err_out_format;\n\n\tif (need_watch) {\n\t\tret = rbd_register_watch(rbd_dev);\n\t\tif (ret) {\n\t\t\tif (ret == -ENOENT)\n\t\t\t\trbd_print_dne(rbd_dev, false);\n\t\t\tgoto err_out_format;\n\t\t}\n\t}\n\n\tif (!depth)\n\t\tdown_write(&rbd_dev->header_rwsem);\n\n\tret = rbd_dev_header_info(rbd_dev, &rbd_dev->header, true);\n\tif (ret) {\n\t\tif (ret == -ENOENT && !need_watch)\n\t\t\trbd_print_dne(rbd_dev, false);\n\t\tgoto err_out_probe;\n\t}\n\n\trbd_init_layout(rbd_dev);\n\n\t \n\tif (!depth)\n\t\tret = rbd_spec_fill_snap_id(rbd_dev);\n\telse\n\t\tret = rbd_spec_fill_names(rbd_dev);\n\tif (ret) {\n\t\tif (ret == -ENOENT)\n\t\t\trbd_print_dne(rbd_dev, true);\n\t\tgoto err_out_probe;\n\t}\n\n\tret = rbd_dev_mapping_set(rbd_dev);\n\tif (ret)\n\t\tgoto err_out_probe;\n\n\tif (rbd_is_snap(rbd_dev) &&\n\t    (rbd_dev->header.features & RBD_FEATURE_OBJECT_MAP)) {\n\t\tret = rbd_object_map_load(rbd_dev);\n\t\tif (ret)\n\t\t\tgoto err_out_probe;\n\t}\n\n\tif (rbd_dev->header.features & RBD_FEATURE_LAYERING) {\n\t\tret = rbd_dev_setup_parent(rbd_dev);\n\t\tif (ret)\n\t\t\tgoto err_out_probe;\n\t}\n\n\tret = rbd_dev_probe_parent(rbd_dev, depth);\n\tif (ret)\n\t\tgoto err_out_probe;\n\n\tdout(\"discovered format %u image, header name is %s\\n\",\n\t\trbd_dev->image_format, rbd_dev->header_oid.name);\n\treturn 0;\n\nerr_out_probe:\n\tif (!depth)\n\t\tup_write(&rbd_dev->header_rwsem);\n\tif (need_watch)\n\t\trbd_unregister_watch(rbd_dev);\n\trbd_dev_unprobe(rbd_dev);\nerr_out_format:\n\trbd_dev->image_format = 0;\n\tkfree(rbd_dev->spec->image_id);\n\trbd_dev->spec->image_id = NULL;\n\treturn ret;\n}\n\nstatic void rbd_dev_update_header(struct rbd_device *rbd_dev,\n\t\t\t\t  struct rbd_image_header *header)\n{\n\trbd_assert(rbd_image_format_valid(rbd_dev->image_format));\n\trbd_assert(rbd_dev->header.object_prefix);  \n\n\tif (rbd_dev->header.image_size != header->image_size) {\n\t\trbd_dev->header.image_size = header->image_size;\n\n\t\tif (!rbd_is_snap(rbd_dev)) {\n\t\t\trbd_dev->mapping.size = header->image_size;\n\t\t\trbd_dev_update_size(rbd_dev);\n\t\t}\n\t}\n\n\tceph_put_snap_context(rbd_dev->header.snapc);\n\trbd_dev->header.snapc = header->snapc;\n\theader->snapc = NULL;\n\n\tif (rbd_dev->image_format == 1) {\n\t\tkfree(rbd_dev->header.snap_names);\n\t\trbd_dev->header.snap_names = header->snap_names;\n\t\theader->snap_names = NULL;\n\n\t\tkfree(rbd_dev->header.snap_sizes);\n\t\trbd_dev->header.snap_sizes = header->snap_sizes;\n\t\theader->snap_sizes = NULL;\n\t}\n}\n\nstatic void rbd_dev_update_parent(struct rbd_device *rbd_dev,\n\t\t\t\t  struct parent_image_info *pii)\n{\n\tif (pii->pool_id == CEPH_NOPOOL || !pii->has_overlap) {\n\t\t \n\t\tif (rbd_dev->parent_overlap) {\n\t\t\trbd_dev->parent_overlap = 0;\n\t\t\trbd_dev_parent_put(rbd_dev);\n\t\t\tpr_info(\"%s: clone has been flattened\\n\",\n\t\t\t\trbd_dev->disk->disk_name);\n\t\t}\n\t} else {\n\t\trbd_assert(rbd_dev->parent_spec);\n\n\t\t \n\t\tif (!pii->overlap && rbd_dev->parent_overlap)\n\t\t\trbd_warn(rbd_dev,\n\t\t\t\t \"clone has become standalone (overlap 0)\");\n\t\trbd_dev->parent_overlap = pii->overlap;\n\t}\n}\n\nstatic int rbd_dev_refresh(struct rbd_device *rbd_dev)\n{\n\tstruct rbd_image_header\theader = { 0 };\n\tstruct parent_image_info pii = { 0 };\n\tint ret;\n\n\tdout(\"%s rbd_dev %p\\n\", __func__, rbd_dev);\n\n\tret = rbd_dev_header_info(rbd_dev, &header, false);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (rbd_dev->parent) {\n\t\tret = rbd_dev_v2_parent_info(rbd_dev, &pii);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tdown_write(&rbd_dev->header_rwsem);\n\trbd_dev_update_header(rbd_dev, &header);\n\tif (rbd_dev->parent)\n\t\trbd_dev_update_parent(rbd_dev, &pii);\n\tup_write(&rbd_dev->header_rwsem);\n\nout:\n\trbd_parent_info_cleanup(&pii);\n\trbd_image_header_cleanup(&header);\n\treturn ret;\n}\n\nstatic ssize_t do_rbd_add(const char *buf, size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tstruct ceph_options *ceph_opts = NULL;\n\tstruct rbd_options *rbd_opts = NULL;\n\tstruct rbd_spec *spec = NULL;\n\tstruct rbd_client *rbdc;\n\tint rc;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\t \n\trc = rbd_add_parse_args(buf, &ceph_opts, &rbd_opts, &spec);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trbdc = rbd_get_client(ceph_opts);\n\tif (IS_ERR(rbdc)) {\n\t\trc = PTR_ERR(rbdc);\n\t\tgoto err_out_args;\n\t}\n\n\t \n\trc = ceph_pg_poolid_by_name(rbdc->client->osdc.osdmap, spec->pool_name);\n\tif (rc < 0) {\n\t\tif (rc == -ENOENT)\n\t\t\tpr_info(\"pool %s does not exist\\n\", spec->pool_name);\n\t\tgoto err_out_client;\n\t}\n\tspec->pool_id = (u64)rc;\n\n\trbd_dev = rbd_dev_create(rbdc, spec, rbd_opts);\n\tif (!rbd_dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_client;\n\t}\n\trbdc = NULL;\t\t \n\tspec = NULL;\t\t \n\trbd_opts = NULL;\t \n\n\t \n\tif (rbd_dev->opts->read_only ||\n\t    strcmp(rbd_dev->spec->snap_name, RBD_SNAP_HEAD_NAME))\n\t\t__set_bit(RBD_DEV_FLAG_READONLY, &rbd_dev->flags);\n\n\trbd_dev->config_info = kstrdup(buf, GFP_KERNEL);\n\tif (!rbd_dev->config_info) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_rbd_dev;\n\t}\n\n\trc = rbd_dev_image_probe(rbd_dev, 0);\n\tif (rc < 0)\n\t\tgoto err_out_rbd_dev;\n\n\tif (rbd_dev->opts->alloc_size > rbd_dev->layout.object_size) {\n\t\trbd_warn(rbd_dev, \"alloc_size adjusted to %u\",\n\t\t\t rbd_dev->layout.object_size);\n\t\trbd_dev->opts->alloc_size = rbd_dev->layout.object_size;\n\t}\n\n\trc = rbd_dev_device_setup(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_probe;\n\n\trc = rbd_add_acquire_lock(rbd_dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\t \n\n\trc = device_add(&rbd_dev->dev);\n\tif (rc)\n\t\tgoto err_out_image_lock;\n\n\trc = device_add_disk(&rbd_dev->dev, rbd_dev->disk, NULL);\n\tif (rc)\n\t\tgoto err_out_cleanup_disk;\n\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_add_tail(&rbd_dev->node, &rbd_dev_list);\n\tspin_unlock(&rbd_dev_list_lock);\n\n\tpr_info(\"%s: capacity %llu features 0x%llx\\n\", rbd_dev->disk->disk_name,\n\t\t(unsigned long long)get_capacity(rbd_dev->disk) << SECTOR_SHIFT,\n\t\trbd_dev->header.features);\n\trc = count;\nout:\n\tmodule_put(THIS_MODULE);\n\treturn rc;\n\nerr_out_cleanup_disk:\n\trbd_free_disk(rbd_dev);\nerr_out_image_lock:\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\nerr_out_image_probe:\n\trbd_dev_image_release(rbd_dev);\nerr_out_rbd_dev:\n\trbd_dev_destroy(rbd_dev);\nerr_out_client:\n\trbd_put_client(rbdc);\nerr_out_args:\n\trbd_spec_put(spec);\n\tkfree(rbd_opts);\n\tgoto out;\n}\n\nstatic ssize_t add_store(const struct bus_type *bus, const char *buf, size_t count)\n{\n\tif (single_major)\n\t\treturn -EINVAL;\n\n\treturn do_rbd_add(buf, count);\n}\n\nstatic ssize_t add_single_major_store(const struct bus_type *bus, const char *buf,\n\t\t\t\t      size_t count)\n{\n\treturn do_rbd_add(buf, count);\n}\n\nstatic void rbd_dev_remove_parent(struct rbd_device *rbd_dev)\n{\n\twhile (rbd_dev->parent) {\n\t\tstruct rbd_device *first = rbd_dev;\n\t\tstruct rbd_device *second = first->parent;\n\t\tstruct rbd_device *third;\n\n\t\t \n\t\twhile (second && (third = second->parent)) {\n\t\t\tfirst = second;\n\t\t\tsecond = third;\n\t\t}\n\t\trbd_assert(second);\n\t\trbd_dev_image_release(second);\n\t\trbd_dev_destroy(second);\n\t\tfirst->parent = NULL;\n\t\tfirst->parent_overlap = 0;\n\n\t\trbd_assert(first->parent_spec);\n\t\trbd_spec_put(first->parent_spec);\n\t\tfirst->parent_spec = NULL;\n\t}\n}\n\nstatic ssize_t do_rbd_remove(const char *buf, size_t count)\n{\n\tstruct rbd_device *rbd_dev = NULL;\n\tint dev_id;\n\tchar opt_buf[6];\n\tbool force = false;\n\tint ret;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tdev_id = -1;\n\topt_buf[0] = '\\0';\n\tsscanf(buf, \"%d %5s\", &dev_id, opt_buf);\n\tif (dev_id < 0) {\n\t\tpr_err(\"dev_id out of range\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (opt_buf[0] != '\\0') {\n\t\tif (!strcmp(opt_buf, \"force\")) {\n\t\t\tforce = true;\n\t\t} else {\n\t\t\tpr_err(\"bad remove option at '%s'\\n\", opt_buf);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tret = -ENOENT;\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_for_each_entry(rbd_dev, &rbd_dev_list, node) {\n\t\tif (rbd_dev->dev_id == dev_id) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!ret) {\n\t\tspin_lock_irq(&rbd_dev->lock);\n\t\tif (rbd_dev->open_count && !force)\n\t\t\tret = -EBUSY;\n\t\telse if (test_and_set_bit(RBD_DEV_FLAG_REMOVING,\n\t\t\t\t\t  &rbd_dev->flags))\n\t\t\tret = -EINPROGRESS;\n\t\tspin_unlock_irq(&rbd_dev->lock);\n\t}\n\tspin_unlock(&rbd_dev_list_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tif (force) {\n\t\t \n\t\tblk_mq_freeze_queue(rbd_dev->disk->queue);\n\t\tblk_mark_disk_dead(rbd_dev->disk);\n\t}\n\n\tdel_gendisk(rbd_dev->disk);\n\tspin_lock(&rbd_dev_list_lock);\n\tlist_del_init(&rbd_dev->node);\n\tspin_unlock(&rbd_dev_list_lock);\n\tdevice_del(&rbd_dev->dev);\n\n\trbd_dev_image_unlock(rbd_dev);\n\trbd_dev_device_release(rbd_dev);\n\trbd_dev_image_release(rbd_dev);\n\trbd_dev_destroy(rbd_dev);\n\treturn count;\n}\n\nstatic ssize_t remove_store(const struct bus_type *bus, const char *buf, size_t count)\n{\n\tif (single_major)\n\t\treturn -EINVAL;\n\n\treturn do_rbd_remove(buf, count);\n}\n\nstatic ssize_t remove_single_major_store(const struct bus_type *bus, const char *buf,\n\t\t\t\t\t size_t count)\n{\n\treturn do_rbd_remove(buf, count);\n}\n\n \nstatic int __init rbd_sysfs_init(void)\n{\n\tint ret;\n\n\tret = device_register(&rbd_root_dev);\n\tif (ret < 0) {\n\t\tput_device(&rbd_root_dev);\n\t\treturn ret;\n\t}\n\n\tret = bus_register(&rbd_bus_type);\n\tif (ret < 0)\n\t\tdevice_unregister(&rbd_root_dev);\n\n\treturn ret;\n}\n\nstatic void __exit rbd_sysfs_cleanup(void)\n{\n\tbus_unregister(&rbd_bus_type);\n\tdevice_unregister(&rbd_root_dev);\n}\n\nstatic int __init rbd_slab_init(void)\n{\n\trbd_assert(!rbd_img_request_cache);\n\trbd_img_request_cache = KMEM_CACHE(rbd_img_request, 0);\n\tif (!rbd_img_request_cache)\n\t\treturn -ENOMEM;\n\n\trbd_assert(!rbd_obj_request_cache);\n\trbd_obj_request_cache = KMEM_CACHE(rbd_obj_request, 0);\n\tif (!rbd_obj_request_cache)\n\t\tgoto out_err;\n\n\treturn 0;\n\nout_err:\n\tkmem_cache_destroy(rbd_img_request_cache);\n\trbd_img_request_cache = NULL;\n\treturn -ENOMEM;\n}\n\nstatic void rbd_slab_exit(void)\n{\n\trbd_assert(rbd_obj_request_cache);\n\tkmem_cache_destroy(rbd_obj_request_cache);\n\trbd_obj_request_cache = NULL;\n\n\trbd_assert(rbd_img_request_cache);\n\tkmem_cache_destroy(rbd_img_request_cache);\n\trbd_img_request_cache = NULL;\n}\n\nstatic int __init rbd_init(void)\n{\n\tint rc;\n\n\tif (!libceph_compatible(NULL)) {\n\t\trbd_warn(NULL, \"libceph incompatibility (quitting)\");\n\t\treturn -EINVAL;\n\t}\n\n\trc = rbd_slab_init();\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trbd_wq = alloc_workqueue(RBD_DRV_NAME, WQ_MEM_RECLAIM, 0);\n\tif (!rbd_wq) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out_slab;\n\t}\n\n\tif (single_major) {\n\t\trbd_major = register_blkdev(0, RBD_DRV_NAME);\n\t\tif (rbd_major < 0) {\n\t\t\trc = rbd_major;\n\t\t\tgoto err_out_wq;\n\t\t}\n\t}\n\n\trc = rbd_sysfs_init();\n\tif (rc)\n\t\tgoto err_out_blkdev;\n\n\tif (single_major)\n\t\tpr_info(\"loaded (major %d)\\n\", rbd_major);\n\telse\n\t\tpr_info(\"loaded\\n\");\n\n\treturn 0;\n\nerr_out_blkdev:\n\tif (single_major)\n\t\tunregister_blkdev(rbd_major, RBD_DRV_NAME);\nerr_out_wq:\n\tdestroy_workqueue(rbd_wq);\nerr_out_slab:\n\trbd_slab_exit();\n\treturn rc;\n}\n\nstatic void __exit rbd_exit(void)\n{\n\tida_destroy(&rbd_dev_id_ida);\n\trbd_sysfs_cleanup();\n\tif (single_major)\n\t\tunregister_blkdev(rbd_major, RBD_DRV_NAME);\n\tdestroy_workqueue(rbd_wq);\n\trbd_slab_exit();\n}\n\nmodule_init(rbd_init);\nmodule_exit(rbd_exit);\n\nMODULE_AUTHOR(\"Alex Elder <elder@inktank.com>\");\nMODULE_AUTHOR(\"Sage Weil <sage@newdream.net>\");\nMODULE_AUTHOR(\"Yehuda Sadeh <yehuda@hq.newdream.net>\");\n \nMODULE_AUTHOR(\"Jeff Garzik <jeff@garzik.org>\");\n\nMODULE_DESCRIPTION(\"RADOS Block Device (RBD) driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}