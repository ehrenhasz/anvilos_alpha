{
  "module_name": "ipu3-mmu.c",
  "hash_id": "05bc2b90a1342e6c58ae40d09f0af71453f6bf10acaf7dae8bb0b975affa53b9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/staging/media/ipu3/ipu3-mmu.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/iopoll.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n#include <asm/set_memory.h>\n\n#include \"ipu3-mmu.h\"\n\n#define IPU3_PT_BITS\t\t10\n#define IPU3_PT_PTES\t\t(1UL << IPU3_PT_BITS)\n#define IPU3_PT_SIZE\t\t(IPU3_PT_PTES << 2)\n#define IPU3_PT_ORDER\t\t(IPU3_PT_SIZE >> PAGE_SHIFT)\n\n#define IPU3_ADDR2PTE(addr)\t((addr) >> IPU3_PAGE_SHIFT)\n#define IPU3_PTE2ADDR(pte)\t((phys_addr_t)(pte) << IPU3_PAGE_SHIFT)\n\n#define IPU3_L2PT_SHIFT\t\tIPU3_PT_BITS\n#define IPU3_L2PT_MASK\t\t((1UL << IPU3_L2PT_SHIFT) - 1)\n\n#define IPU3_L1PT_SHIFT\t\tIPU3_PT_BITS\n#define IPU3_L1PT_MASK\t\t((1UL << IPU3_L1PT_SHIFT) - 1)\n\n#define IPU3_MMU_ADDRESS_BITS\t(IPU3_PAGE_SHIFT + \\\n\t\t\t\t IPU3_L2PT_SHIFT + \\\n\t\t\t\t IPU3_L1PT_SHIFT)\n\n#define IMGU_REG_BASE\t\t0x4000\n#define REG_TLB_INVALIDATE\t(IMGU_REG_BASE + 0x300)\n#define TLB_INVALIDATE\t\t1\n#define REG_L1_PHYS\t\t(IMGU_REG_BASE + 0x304)  \n#define REG_GP_HALT\t\t(IMGU_REG_BASE + 0x5dc)\n#define REG_GP_HALTED\t\t(IMGU_REG_BASE + 0x5e0)\n\nstruct imgu_mmu {\n\tstruct device *dev;\n\tvoid __iomem *base;\n\t \n\tspinlock_t lock;\n\n\tvoid *dummy_page;\n\tu32 dummy_page_pteval;\n\n\tu32 *dummy_l2pt;\n\tu32 dummy_l2pt_pteval;\n\n\tu32 **l2pts;\n\tu32 *l1pt;\n\n\tstruct imgu_mmu_info geometry;\n};\n\nstatic inline struct imgu_mmu *to_imgu_mmu(struct imgu_mmu_info *info)\n{\n\treturn container_of(info, struct imgu_mmu, geometry);\n}\n\n \nstatic void imgu_mmu_tlb_invalidate(struct imgu_mmu *mmu)\n{\n\twritel(TLB_INVALIDATE, mmu->base + REG_TLB_INVALIDATE);\n}\n\nstatic void call_if_imgu_is_powered(struct imgu_mmu *mmu,\n\t\t\t\t    void (*func)(struct imgu_mmu *mmu))\n{\n\tif (!pm_runtime_get_if_in_use(mmu->dev))\n\t\treturn;\n\n\tfunc(mmu);\n\tpm_runtime_put(mmu->dev);\n}\n\n \nstatic void imgu_mmu_set_halt(struct imgu_mmu *mmu, bool halt)\n{\n\tint ret;\n\tu32 val;\n\n\twritel(halt, mmu->base + REG_GP_HALT);\n\tret = readl_poll_timeout(mmu->base + REG_GP_HALTED,\n\t\t\t\t val, (val & 1) == halt, 1000, 100000);\n\n\tif (ret)\n\t\tdev_err(mmu->dev, \"failed to %s CIO gate halt\\n\",\n\t\t\thalt ? \"set\" : \"clear\");\n}\n\n \nstatic u32 *imgu_mmu_alloc_page_table(u32 pteval)\n{\n\tu32 *pt;\n\tint pte;\n\n\tpt = (u32 *)__get_free_page(GFP_KERNEL);\n\tif (!pt)\n\t\treturn NULL;\n\n\tfor (pte = 0; pte < IPU3_PT_PTES; pte++)\n\t\tpt[pte] = pteval;\n\n\tset_memory_uc((unsigned long)pt, IPU3_PT_ORDER);\n\n\treturn pt;\n}\n\n \nstatic void imgu_mmu_free_page_table(u32 *pt)\n{\n\tset_memory_wb((unsigned long)pt, IPU3_PT_ORDER);\n\tfree_page((unsigned long)pt);\n}\n\n \nstatic inline void address_to_pte_idx(unsigned long iova, u32 *l1pt_idx,\n\t\t\t\t      u32 *l2pt_idx)\n{\n\tiova >>= IPU3_PAGE_SHIFT;\n\n\tif (l2pt_idx)\n\t\t*l2pt_idx = iova & IPU3_L2PT_MASK;\n\n\tiova >>= IPU3_L2PT_SHIFT;\n\n\tif (l1pt_idx)\n\t\t*l1pt_idx = iova & IPU3_L1PT_MASK;\n}\n\nstatic u32 *imgu_mmu_get_l2pt(struct imgu_mmu *mmu, u32 l1pt_idx)\n{\n\tunsigned long flags;\n\tu32 *l2pt, *new_l2pt;\n\tu32 pteval;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tl2pt = mmu->l2pts[l1pt_idx];\n\tif (l2pt) {\n\t\tspin_unlock_irqrestore(&mmu->lock, flags);\n\t\treturn l2pt;\n\t}\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\n\tnew_l2pt = imgu_mmu_alloc_page_table(mmu->dummy_page_pteval);\n\tif (!new_l2pt)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tdev_dbg(mmu->dev, \"allocated page table %p for l1pt_idx %u\\n\",\n\t\tnew_l2pt, l1pt_idx);\n\n\tl2pt = mmu->l2pts[l1pt_idx];\n\tif (l2pt) {\n\t\tspin_unlock_irqrestore(&mmu->lock, flags);\n\t\timgu_mmu_free_page_table(new_l2pt);\n\t\treturn l2pt;\n\t}\n\n\tl2pt = new_l2pt;\n\tmmu->l2pts[l1pt_idx] = new_l2pt;\n\n\tpteval = IPU3_ADDR2PTE(virt_to_phys(new_l2pt));\n\tmmu->l1pt[l1pt_idx] = pteval;\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\treturn l2pt;\n}\n\nstatic int __imgu_mmu_map(struct imgu_mmu *mmu, unsigned long iova,\n\t\t\t  phys_addr_t paddr)\n{\n\tu32 l1pt_idx, l2pt_idx;\n\tunsigned long flags;\n\tu32 *l2pt;\n\n\tif (!mmu)\n\t\treturn -ENODEV;\n\n\taddress_to_pte_idx(iova, &l1pt_idx, &l2pt_idx);\n\n\tl2pt = imgu_mmu_get_l2pt(mmu, l1pt_idx);\n\tif (!l2pt)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tif (l2pt[l2pt_idx] != mmu->dummy_page_pteval) {\n\t\tspin_unlock_irqrestore(&mmu->lock, flags);\n\t\treturn -EBUSY;\n\t}\n\n\tl2pt[l2pt_idx] = IPU3_ADDR2PTE(paddr);\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\n\treturn 0;\n}\n\n \nint imgu_mmu_map(struct imgu_mmu_info *info, unsigned long iova,\n\t\t phys_addr_t paddr, size_t size)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\tint ret = 0;\n\n\t \n\tif (!IS_ALIGNED(iova | paddr | size, IPU3_PAGE_SIZE)) {\n\t\tdev_err(mmu->dev, \"unaligned: iova 0x%lx pa %pa size 0x%zx\\n\",\n\t\t\tiova, &paddr, size);\n\t\treturn -EINVAL;\n\t}\n\n\tdev_dbg(mmu->dev, \"map: iova 0x%lx pa %pa size 0x%zx\\n\",\n\t\tiova, &paddr, size);\n\n\twhile (size) {\n\t\tdev_dbg(mmu->dev, \"mapping: iova 0x%lx pa %pa\\n\", iova, &paddr);\n\n\t\tret = __imgu_mmu_map(mmu, iova, paddr);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tiova += IPU3_PAGE_SIZE;\n\t\tpaddr += IPU3_PAGE_SIZE;\n\t\tsize -= IPU3_PAGE_SIZE;\n\t}\n\n\tcall_if_imgu_is_powered(mmu, imgu_mmu_tlb_invalidate);\n\n\treturn ret;\n}\n\n \nsize_t imgu_mmu_map_sg(struct imgu_mmu_info *info, unsigned long iova,\n\t\t       struct scatterlist *sg, unsigned int nents)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\tstruct scatterlist *s;\n\tsize_t s_length, mapped = 0;\n\tunsigned int i;\n\tint ret;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tphys_addr_t phys = page_to_phys(sg_page(s)) + s->offset;\n\n\t\ts_length = s->length;\n\n\t\tif (!IS_ALIGNED(s->offset, IPU3_PAGE_SIZE))\n\t\t\tgoto out_err;\n\n\t\t \n\t\tif (i == nents - 1 && !IS_ALIGNED(s->length, IPU3_PAGE_SIZE))\n\t\t\ts_length = PAGE_ALIGN(s->length);\n\n\t\tret = imgu_mmu_map(info, iova + mapped, phys, s_length);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\n\t\tmapped += s_length;\n\t}\n\n\tcall_if_imgu_is_powered(mmu, imgu_mmu_tlb_invalidate);\n\n\treturn mapped;\n\nout_err:\n\t \n\timgu_mmu_unmap(info, iova, mapped);\n\n\treturn 0;\n}\n\nstatic size_t __imgu_mmu_unmap(struct imgu_mmu *mmu,\n\t\t\t       unsigned long iova, size_t size)\n{\n\tu32 l1pt_idx, l2pt_idx;\n\tunsigned long flags;\n\tsize_t unmap = size;\n\tu32 *l2pt;\n\n\tif (!mmu)\n\t\treturn 0;\n\n\taddress_to_pte_idx(iova, &l1pt_idx, &l2pt_idx);\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tl2pt = mmu->l2pts[l1pt_idx];\n\tif (!l2pt) {\n\t\tspin_unlock_irqrestore(&mmu->lock, flags);\n\t\treturn 0;\n\t}\n\n\tif (l2pt[l2pt_idx] == mmu->dummy_page_pteval)\n\t\tunmap = 0;\n\n\tl2pt[l2pt_idx] = mmu->dummy_page_pteval;\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\n\treturn unmap;\n}\n\n \nsize_t imgu_mmu_unmap(struct imgu_mmu_info *info, unsigned long iova,\n\t\t      size_t size)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\tsize_t unmapped_page, unmapped = 0;\n\n\t \n\tif (!IS_ALIGNED(iova | size, IPU3_PAGE_SIZE)) {\n\t\tdev_err(mmu->dev, \"unaligned: iova 0x%lx size 0x%zx\\n\",\n\t\t\tiova, size);\n\t\treturn -EINVAL;\n\t}\n\n\tdev_dbg(mmu->dev, \"unmap this: iova 0x%lx size 0x%zx\\n\", iova, size);\n\n\t \n\twhile (unmapped < size) {\n\t\tunmapped_page = __imgu_mmu_unmap(mmu, iova, IPU3_PAGE_SIZE);\n\t\tif (!unmapped_page)\n\t\t\tbreak;\n\n\t\tdev_dbg(mmu->dev, \"unmapped: iova 0x%lx size 0x%zx\\n\",\n\t\t\tiova, unmapped_page);\n\n\t\tiova += unmapped_page;\n\t\tunmapped += unmapped_page;\n\t}\n\n\tcall_if_imgu_is_powered(mmu, imgu_mmu_tlb_invalidate);\n\n\treturn unmapped;\n}\n\n \nstruct imgu_mmu_info *imgu_mmu_init(struct device *parent, void __iomem *base)\n{\n\tstruct imgu_mmu *mmu;\n\tu32 pteval;\n\n\tmmu = kzalloc(sizeof(*mmu), GFP_KERNEL);\n\tif (!mmu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmmu->dev = parent;\n\tmmu->base = base;\n\tspin_lock_init(&mmu->lock);\n\n\t \n\timgu_mmu_set_halt(mmu, true);\n\n\t \n\tmmu->dummy_page = (void *)__get_free_page(GFP_KERNEL);\n\tif (!mmu->dummy_page)\n\t\tgoto fail_group;\n\tpteval = IPU3_ADDR2PTE(virt_to_phys(mmu->dummy_page));\n\tmmu->dummy_page_pteval = pteval;\n\n\t \n\tmmu->dummy_l2pt = imgu_mmu_alloc_page_table(pteval);\n\tif (!mmu->dummy_l2pt)\n\t\tgoto fail_dummy_page;\n\tpteval = IPU3_ADDR2PTE(virt_to_phys(mmu->dummy_l2pt));\n\tmmu->dummy_l2pt_pteval = pteval;\n\n\t \n\tmmu->l2pts = vzalloc(IPU3_PT_PTES * sizeof(*mmu->l2pts));\n\tif (!mmu->l2pts)\n\t\tgoto fail_l2pt;\n\n\t \n\tmmu->l1pt = imgu_mmu_alloc_page_table(mmu->dummy_l2pt_pteval);\n\tif (!mmu->l1pt)\n\t\tgoto fail_l2pts;\n\n\tpteval = IPU3_ADDR2PTE(virt_to_phys(mmu->l1pt));\n\twritel(pteval, mmu->base + REG_L1_PHYS);\n\timgu_mmu_tlb_invalidate(mmu);\n\timgu_mmu_set_halt(mmu, false);\n\n\tmmu->geometry.aperture_start = 0;\n\tmmu->geometry.aperture_end = DMA_BIT_MASK(IPU3_MMU_ADDRESS_BITS);\n\n\treturn &mmu->geometry;\n\nfail_l2pts:\n\tvfree(mmu->l2pts);\nfail_l2pt:\n\timgu_mmu_free_page_table(mmu->dummy_l2pt);\nfail_dummy_page:\n\tfree_page((unsigned long)mmu->dummy_page);\nfail_group:\n\tkfree(mmu);\n\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nvoid imgu_mmu_exit(struct imgu_mmu_info *info)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\n\t \n\timgu_mmu_set_halt(mmu, true);\n\timgu_mmu_tlb_invalidate(mmu);\n\n\timgu_mmu_free_page_table(mmu->l1pt);\n\tvfree(mmu->l2pts);\n\timgu_mmu_free_page_table(mmu->dummy_l2pt);\n\tfree_page((unsigned long)mmu->dummy_page);\n\tkfree(mmu);\n}\n\nvoid imgu_mmu_suspend(struct imgu_mmu_info *info)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\n\timgu_mmu_set_halt(mmu, true);\n}\n\nvoid imgu_mmu_resume(struct imgu_mmu_info *info)\n{\n\tstruct imgu_mmu *mmu = to_imgu_mmu(info);\n\tu32 pteval;\n\n\timgu_mmu_set_halt(mmu, true);\n\n\tpteval = IPU3_ADDR2PTE(virt_to_phys(mmu->l1pt));\n\twritel(pteval, mmu->base + REG_L1_PHYS);\n\n\timgu_mmu_tlb_invalidate(mmu);\n\timgu_mmu_set_halt(mmu, false);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}