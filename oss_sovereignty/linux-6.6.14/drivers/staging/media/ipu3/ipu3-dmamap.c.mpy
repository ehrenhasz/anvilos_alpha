{
  "module_name": "ipu3-dmamap.c",
  "hash_id": "9c98d559ee06a5972014c6bdd440f014ddca00a3bcf409e8b20717455c54e073",
  "original_prompt": "Ingested from linux-6.6.14/drivers/staging/media/ipu3/ipu3-dmamap.c",
  "human_readable_source": "\n \n\n#include <linux/vmalloc.h>\n\n#include \"ipu3.h\"\n#include \"ipu3-css-pool.h\"\n#include \"ipu3-mmu.h\"\n#include \"ipu3-dmamap.h\"\n\n \nstatic void imgu_dmamap_free_buffer(struct page **pages,\n\t\t\t\t    size_t size)\n{\n\tint count = size >> PAGE_SHIFT;\n\n\twhile (count--)\n\t\t__free_page(pages[count]);\n\tkvfree(pages);\n}\n\n \nstatic struct page **imgu_dmamap_alloc_buffer(size_t size, gfp_t gfp)\n{\n\tstruct page **pages;\n\tunsigned int i = 0, count = size >> PAGE_SHIFT;\n\tunsigned int order_mask = 1;\n\tconst gfp_t high_order_gfp = __GFP_NOWARN | __GFP_NORETRY;\n\n\t \n\tpages = kvmalloc_array(count, sizeof(*pages), GFP_KERNEL);\n\n\tif (!pages)\n\t\treturn NULL;\n\n\tgfp |= __GFP_HIGHMEM | __GFP_ZERO;\n\n\twhile (count) {\n\t\tstruct page *page = NULL;\n\t\tunsigned int order_size;\n\n\t\tfor (order_mask &= (2U << __fls(count)) - 1;\n\t\t     order_mask; order_mask &= ~order_size) {\n\t\t\tunsigned int order = __fls(order_mask);\n\n\t\t\torder_size = 1U << order;\n\t\t\tpage = alloc_pages((order_mask - order_size) ?\n\t\t\t\t\t   gfp | high_order_gfp : gfp, order);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\t\t\tif (!order)\n\t\t\t\tbreak;\n\t\t\tif (!PageCompound(page)) {\n\t\t\t\tsplit_page(page, order);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__free_pages(page, order);\n\t\t}\n\t\tif (!page) {\n\t\t\timgu_dmamap_free_buffer(pages, i << PAGE_SHIFT);\n\t\t\treturn NULL;\n\t\t}\n\t\tcount -= order_size;\n\t\twhile (order_size--)\n\t\t\tpages[i++] = page++;\n\t}\n\n\treturn pages;\n}\n\n \nvoid *imgu_dmamap_alloc(struct imgu_device *imgu, struct imgu_css_map *map,\n\t\t\tsize_t len)\n{\n\tunsigned long shift = iova_shift(&imgu->iova_domain);\n\tstruct device *dev = &imgu->pci_dev->dev;\n\tsize_t size = PAGE_ALIGN(len);\n\tint count = size >> PAGE_SHIFT;\n\tstruct page **pages;\n\tdma_addr_t iovaddr;\n\tstruct iova *iova;\n\tint i, rval;\n\n\tdev_dbg(dev, \"%s: allocating %zu\\n\", __func__, size);\n\n\tiova = alloc_iova(&imgu->iova_domain, size >> shift,\n\t\t\t  imgu->mmu->aperture_end >> shift, 0);\n\tif (!iova)\n\t\treturn NULL;\n\n\tpages = imgu_dmamap_alloc_buffer(size, GFP_KERNEL);\n\tif (!pages)\n\t\tgoto out_free_iova;\n\n\t \n\tiovaddr = iova_dma_addr(&imgu->iova_domain, iova);\n\tfor (i = 0; i < count; ++i) {\n\t\trval = imgu_mmu_map(imgu->mmu, iovaddr,\n\t\t\t\t    page_to_phys(pages[i]), PAGE_SIZE);\n\t\tif (rval)\n\t\t\tgoto out_unmap;\n\n\t\tiovaddr += PAGE_SIZE;\n\t}\n\n\tmap->vaddr = vmap(pages, count, VM_USERMAP, PAGE_KERNEL);\n\tif (!map->vaddr)\n\t\tgoto out_unmap;\n\n\tmap->pages = pages;\n\tmap->size = size;\n\tmap->daddr = iova_dma_addr(&imgu->iova_domain, iova);\n\n\tdev_dbg(dev, \"%s: allocated %zu @ IOVA %pad @ VA %p\\n\", __func__,\n\t\tsize, &map->daddr, map->vaddr);\n\n\treturn map->vaddr;\n\nout_unmap:\n\timgu_dmamap_free_buffer(pages, size);\n\timgu_mmu_unmap(imgu->mmu, iova_dma_addr(&imgu->iova_domain, iova),\n\t\t       i * PAGE_SIZE);\n\nout_free_iova:\n\t__free_iova(&imgu->iova_domain, iova);\n\n\treturn NULL;\n}\n\nvoid imgu_dmamap_unmap(struct imgu_device *imgu, struct imgu_css_map *map)\n{\n\tstruct iova *iova;\n\n\tiova = find_iova(&imgu->iova_domain,\n\t\t\t iova_pfn(&imgu->iova_domain, map->daddr));\n\tif (WARN_ON(!iova))\n\t\treturn;\n\n\timgu_mmu_unmap(imgu->mmu, iova_dma_addr(&imgu->iova_domain, iova),\n\t\t       iova_size(iova) << iova_shift(&imgu->iova_domain));\n\n\t__free_iova(&imgu->iova_domain, iova);\n}\n\n \nvoid imgu_dmamap_free(struct imgu_device *imgu, struct imgu_css_map *map)\n{\n\tdev_dbg(&imgu->pci_dev->dev, \"%s: freeing %zu @ IOVA %pad @ VA %p\\n\",\n\t\t__func__, map->size, &map->daddr, map->vaddr);\n\n\tif (!map->vaddr)\n\t\treturn;\n\n\timgu_dmamap_unmap(imgu, map);\n\n\tvunmap(map->vaddr);\n\timgu_dmamap_free_buffer(map->pages, map->size);\n\tmap->vaddr = NULL;\n}\n\nint imgu_dmamap_map_sg(struct imgu_device *imgu, struct scatterlist *sglist,\n\t\t       int nents, struct imgu_css_map *map)\n{\n\tunsigned long shift = iova_shift(&imgu->iova_domain);\n\tstruct scatterlist *sg;\n\tstruct iova *iova;\n\tsize_t size = 0;\n\tint i;\n\n\tfor_each_sg(sglist, sg, nents, i) {\n\t\tif (sg->offset)\n\t\t\treturn -EINVAL;\n\n\t\tif (i != nents - 1 && !PAGE_ALIGNED(sg->length))\n\t\t\treturn -EINVAL;\n\n\t\tsize += sg->length;\n\t}\n\n\tsize = iova_align(&imgu->iova_domain, size);\n\tdev_dbg(&imgu->pci_dev->dev, \"dmamap: mapping sg %d entries, %zu pages\\n\",\n\t\tnents, size >> shift);\n\n\tiova = alloc_iova(&imgu->iova_domain, size >> shift,\n\t\t\t  imgu->mmu->aperture_end >> shift, 0);\n\tif (!iova)\n\t\treturn -ENOMEM;\n\n\tdev_dbg(&imgu->pci_dev->dev, \"dmamap: iova low pfn %lu, high pfn %lu\\n\",\n\t\tiova->pfn_lo, iova->pfn_hi);\n\n\tif (imgu_mmu_map_sg(imgu->mmu, iova_dma_addr(&imgu->iova_domain, iova),\n\t\t\t    sglist, nents) < size)\n\t\tgoto out_fail;\n\n\tmemset(map, 0, sizeof(*map));\n\tmap->daddr = iova_dma_addr(&imgu->iova_domain, iova);\n\tmap->size = size;\n\n\treturn 0;\n\nout_fail:\n\t__free_iova(&imgu->iova_domain, iova);\n\n\treturn -EFAULT;\n}\n\nint imgu_dmamap_init(struct imgu_device *imgu)\n{\n\tunsigned long order, base_pfn;\n\tint ret = iova_cache_get();\n\n\tif (ret)\n\t\treturn ret;\n\n\torder = __ffs(IPU3_PAGE_SIZE);\n\tbase_pfn = max_t(unsigned long, 1, imgu->mmu->aperture_start >> order);\n\tinit_iova_domain(&imgu->iova_domain, 1UL << order, base_pfn);\n\n\treturn 0;\n}\n\nvoid imgu_dmamap_exit(struct imgu_device *imgu)\n{\n\tput_iova_domain(&imgu->iova_domain);\n\tiova_cache_put();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}