{
  "module_name": "isp_mmu.c",
  "hash_id": "f59d0f7411cda71a6d5cc62095163f1e664851c88f859a927a0c2de316d7899d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/staging/media/atomisp/pci/mmu/isp_mmu.c",
  "human_readable_source": "\n \n \n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/gfp.h>\n#include <linux/mm.h>\t\t \n#include <linux/slab.h>\t\t \n#include <linux/list.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/sizes.h>\n\n#ifdef CONFIG_X86\n#include <asm/set_memory.h>\n#endif\n\n#include \"atomisp_internal.h\"\n#include \"mmu/isp_mmu.h\"\n\n \n#define NR_PAGES_2GB\t(SZ_2G / PAGE_SIZE)\n\nstatic void free_mmu_map(struct isp_mmu *mmu, unsigned int start_isp_virt,\n\t\t\t unsigned int end_isp_virt);\n\nstatic unsigned int atomisp_get_pte(phys_addr_t pt, unsigned int idx)\n{\n\tunsigned int *pt_virt = phys_to_virt(pt);\n\n\treturn *(pt_virt + idx);\n}\n\nstatic void atomisp_set_pte(phys_addr_t pt,\n\t\t\t    unsigned int idx, unsigned int pte)\n{\n\tunsigned int *pt_virt = phys_to_virt(pt);\n\t*(pt_virt + idx) = pte;\n}\n\nstatic void *isp_pt_phys_to_virt(phys_addr_t phys)\n{\n\treturn phys_to_virt(phys);\n}\n\nstatic phys_addr_t isp_pte_to_pgaddr(struct isp_mmu *mmu,\n\t\t\t\t     unsigned int pte)\n{\n\treturn mmu->driver->pte_to_phys(mmu, pte);\n}\n\nstatic unsigned int isp_pgaddr_to_pte_valid(struct isp_mmu *mmu,\n\tphys_addr_t phys)\n{\n\tunsigned int pte = mmu->driver->phys_to_pte(mmu, phys);\n\n\treturn (unsigned int)(pte | ISP_PTE_VALID_MASK(mmu));\n}\n\n \nstatic phys_addr_t alloc_page_table(struct isp_mmu *mmu)\n{\n\tint i;\n\tphys_addr_t page;\n\tvoid *virt;\n\n\tvirt = (void *)__get_free_page(GFP_KERNEL | GFP_DMA32);\n\n\tif (!virt)\n\t\treturn (phys_addr_t)NULL_PAGE;\n\n\t \n#ifdef\tCONFIG_X86\n\tset_memory_uc((unsigned long)virt, 1);\n#endif\n\n\tpage = virt_to_phys(virt);\n\n\tfor (i = 0; i < 1024; i++) {\n\t\t \n\t\tatomisp_set_pte(page, i, mmu->driver->null_pte);\n\t}\n\n\treturn page;\n}\n\nstatic void free_page_table(struct isp_mmu *mmu, phys_addr_t page)\n{\n\tvoid *virt;\n\n\tpage &= ISP_PAGE_MASK;\n\t \n\tvirt = phys_to_virt(page);\n\n#ifdef\tCONFIG_X86\n\tset_memory_wb((unsigned long)virt, 1);\n#endif\n\n\tfree_page((unsigned long)virt);\n}\n\nstatic void mmu_remap_error(struct isp_mmu *mmu,\n\t\t\t    phys_addr_t l1_pt, unsigned int l1_idx,\n\t\t\t    phys_addr_t l2_pt, unsigned int l2_idx,\n\t\t\t    unsigned int isp_virt, phys_addr_t old_phys,\n\t\t\t    phys_addr_t new_phys)\n{\n\tdev_err(atomisp_dev, \"address remap:\\n\\n\"\n\t\t\"\\tL1 PT: virt = %p, phys = 0x%llx, idx = %d\\n\"\n\t\t\"\\tL2 PT: virt = %p, phys = 0x%llx, idx = %d\\n\"\n\t\t\"\\told: isp_virt = 0x%x, phys = 0x%llx\\n\"\n\t\t\"\\tnew: isp_virt = 0x%x, phys = 0x%llx\\n\",\n\t\tisp_pt_phys_to_virt(l1_pt),\n\t\t(u64)l1_pt, l1_idx,\n\t\tisp_pt_phys_to_virt(l2_pt),\n\t\t(u64)l2_pt, l2_idx, isp_virt,\n\t\t(u64)old_phys, isp_virt,\n\t\t(u64)new_phys);\n}\n\nstatic void mmu_unmap_l2_pte_error(struct isp_mmu *mmu,\n\t\t\t\t   phys_addr_t l1_pt, unsigned int l1_idx,\n\t\t\t\t   phys_addr_t l2_pt, unsigned int l2_idx,\n\t\t\t\t   unsigned int isp_virt, unsigned int pte)\n{\n\tdev_err(atomisp_dev, \"unmap invalid L2 pte:\\n\\n\"\n\t\t\"\\tL1 PT: virt = %p, phys = 0x%llx, idx = %d\\n\"\n\t\t\"\\tL2 PT: virt = %p, phys = 0x%llx, idx = %d\\n\"\n\t\t\"\\tisp_virt = 0x%x, pte(page phys) = 0x%x\\n\",\n\t\tisp_pt_phys_to_virt(l1_pt),\n\t\t(u64)l1_pt, l1_idx,\n\t\tisp_pt_phys_to_virt(l2_pt),\n\t\t(u64)l2_pt, l2_idx, isp_virt,\n\t\tpte);\n}\n\nstatic void mmu_unmap_l1_pte_error(struct isp_mmu *mmu,\n\t\t\t\t   phys_addr_t l1_pt, unsigned int l1_idx,\n\t\t\t\t   unsigned int isp_virt, unsigned int pte)\n{\n\tdev_err(atomisp_dev, \"unmap invalid L1 pte (L2 PT):\\n\\n\"\n\t\t\"\\tL1 PT: virt = %p, phys = 0x%llx, idx = %d\\n\"\n\t\t\"\\tisp_virt = 0x%x, l1_pte(L2 PT) = 0x%x\\n\",\n\t\tisp_pt_phys_to_virt(l1_pt),\n\t\t(u64)l1_pt, l1_idx, (unsigned int)isp_virt,\n\t\tpte);\n}\n\nstatic void mmu_unmap_l1_pt_error(struct isp_mmu *mmu, unsigned int pte)\n{\n\tdev_err(atomisp_dev, \"unmap invalid L1PT:\\n\\n\"\n\t\t\"L1PT = 0x%x\\n\", (unsigned int)pte);\n}\n\n \nstatic int mmu_l2_map(struct isp_mmu *mmu, phys_addr_t l1_pt,\n\t\t      unsigned int l1_idx, phys_addr_t l2_pt,\n\t\t      unsigned int start, unsigned int end, phys_addr_t phys)\n{\n\tunsigned int ptr;\n\tunsigned int idx;\n\tunsigned int pte;\n\n\tl2_pt &= ISP_PAGE_MASK;\n\n\tstart = start & ISP_PAGE_MASK;\n\tend = ISP_PAGE_ALIGN(end);\n\tphys &= ISP_PAGE_MASK;\n\n\tptr = start;\n\tdo {\n\t\tidx = ISP_PTR_TO_L2_IDX(ptr);\n\n\t\tpte = atomisp_get_pte(l2_pt, idx);\n\n\t\tif (ISP_PTE_VALID(mmu, pte)) {\n\t\t\tmmu_remap_error(mmu, l1_pt, l1_idx,\n\t\t\t\t\tl2_pt, idx, ptr, pte, phys);\n\n\t\t\t \n\t\t\tfree_mmu_map(mmu, start, ptr);\n\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpte = isp_pgaddr_to_pte_valid(mmu, phys);\n\n\t\tatomisp_set_pte(l2_pt, idx, pte);\n\t\tmmu->l2_pgt_refcount[l1_idx]++;\n\t\tptr += (1U << ISP_L2PT_OFFSET);\n\t\tphys += (1U << ISP_L2PT_OFFSET);\n\t} while (ptr < end && idx < ISP_L2PT_PTES - 1);\n\n\treturn 0;\n}\n\n \nstatic int mmu_l1_map(struct isp_mmu *mmu, phys_addr_t l1_pt,\n\t\t      unsigned int start, unsigned int end,\n\t\t      phys_addr_t phys)\n{\n\tphys_addr_t l2_pt;\n\tunsigned int ptr, l1_aligned;\n\tunsigned int idx;\n\tunsigned int l2_pte;\n\tint ret;\n\n\tl1_pt &= ISP_PAGE_MASK;\n\n\tstart = start & ISP_PAGE_MASK;\n\tend = ISP_PAGE_ALIGN(end);\n\tphys &= ISP_PAGE_MASK;\n\n\tptr = start;\n\tdo {\n\t\tidx = ISP_PTR_TO_L1_IDX(ptr);\n\n\t\tl2_pte = atomisp_get_pte(l1_pt, idx);\n\n\t\tif (!ISP_PTE_VALID(mmu, l2_pte)) {\n\t\t\tl2_pt = alloc_page_table(mmu);\n\t\t\tif (l2_pt == NULL_PAGE) {\n\t\t\t\tdev_err(atomisp_dev,\n\t\t\t\t\t\"alloc page table fail.\\n\");\n\n\t\t\t\t \n\t\t\t\tfree_mmu_map(mmu, start, ptr);\n\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tl2_pte = isp_pgaddr_to_pte_valid(mmu, l2_pt);\n\n\t\t\tatomisp_set_pte(l1_pt, idx, l2_pte);\n\t\t\tmmu->l2_pgt_refcount[idx] = 0;\n\t\t}\n\n\t\tl2_pt = isp_pte_to_pgaddr(mmu, l2_pte);\n\n\t\tl1_aligned = (ptr & ISP_PAGE_MASK) + (1U << ISP_L1PT_OFFSET);\n\n\t\tif (l1_aligned < end) {\n\t\t\tret = mmu_l2_map(mmu, l1_pt, idx,\n\t\t\t\t\t l2_pt, ptr, l1_aligned, phys);\n\t\t\tphys += (l1_aligned - ptr);\n\t\t\tptr = l1_aligned;\n\t\t} else {\n\t\t\tret = mmu_l2_map(mmu, l1_pt, idx,\n\t\t\t\t\t l2_pt, ptr, end, phys);\n\t\t\tphys += (end - ptr);\n\t\t\tptr = end;\n\t\t}\n\n\t\tif (ret) {\n\t\t\tdev_err(atomisp_dev, \"setup mapping in L2PT fail.\\n\");\n\n\t\t\t \n\t\t\tfree_mmu_map(mmu, start, ptr);\n\n\t\t\treturn -EINVAL;\n\t\t}\n\t} while (ptr < end && idx < ISP_L1PT_PTES);\n\n\treturn 0;\n}\n\n \nstatic int mmu_map(struct isp_mmu *mmu, unsigned int isp_virt,\n\t\t   phys_addr_t phys, unsigned int pgnr)\n{\n\tunsigned int start, end;\n\tphys_addr_t l1_pt;\n\tint ret;\n\n\tmutex_lock(&mmu->pt_mutex);\n\tif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\n\t\t \n\t\tl1_pt = alloc_page_table(mmu);\n\t\tif (l1_pt == NULL_PAGE) {\n\t\t\tdev_err(atomisp_dev, \"alloc page table fail.\\n\");\n\t\t\tmutex_unlock(&mmu->pt_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t \n\t\tmmu->base_address = l1_pt;\n\t\tmmu->l1_pte = isp_pgaddr_to_pte_valid(mmu, l1_pt);\n\t\tmemset(mmu->l2_pgt_refcount, 0, sizeof(int) * ISP_L1PT_PTES);\n\t}\n\n\tl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\n\n\tstart = (isp_virt) & ISP_PAGE_MASK;\n\tend = start + (pgnr << ISP_PAGE_OFFSET);\n\tphys &= ISP_PAGE_MASK;\n\n\tret = mmu_l1_map(mmu, l1_pt, start, end, phys);\n\n\tif (ret)\n\t\tdev_err(atomisp_dev, \"setup mapping in L1PT fail.\\n\");\n\n\tmutex_unlock(&mmu->pt_mutex);\n\treturn ret;\n}\n\n \nstatic void mmu_l2_unmap(struct isp_mmu *mmu, phys_addr_t l1_pt,\n\t\t\t unsigned int l1_idx, phys_addr_t l2_pt,\n\t\t\t unsigned int start, unsigned int end)\n{\n\tunsigned int ptr;\n\tunsigned int idx;\n\tunsigned int pte;\n\n\tl2_pt &= ISP_PAGE_MASK;\n\n\tstart = start & ISP_PAGE_MASK;\n\tend = ISP_PAGE_ALIGN(end);\n\n\tptr = start;\n\tdo {\n\t\tidx = ISP_PTR_TO_L2_IDX(ptr);\n\n\t\tpte = atomisp_get_pte(l2_pt, idx);\n\n\t\tif (!ISP_PTE_VALID(mmu, pte))\n\t\t\tmmu_unmap_l2_pte_error(mmu, l1_pt, l1_idx,\n\t\t\t\t\t       l2_pt, idx, ptr, pte);\n\n\t\tatomisp_set_pte(l2_pt, idx, mmu->driver->null_pte);\n\t\tmmu->l2_pgt_refcount[l1_idx]--;\n\t\tptr += (1U << ISP_L2PT_OFFSET);\n\t} while (ptr < end && idx < ISP_L2PT_PTES - 1);\n\n\tif (mmu->l2_pgt_refcount[l1_idx] == 0) {\n\t\tfree_page_table(mmu, l2_pt);\n\t\tatomisp_set_pte(l1_pt, l1_idx, mmu->driver->null_pte);\n\t}\n}\n\n \nstatic void mmu_l1_unmap(struct isp_mmu *mmu, phys_addr_t l1_pt,\n\t\t\t unsigned int start, unsigned int end)\n{\n\tphys_addr_t l2_pt;\n\tunsigned int ptr, l1_aligned;\n\tunsigned int idx;\n\tunsigned int l2_pte;\n\n\tl1_pt &= ISP_PAGE_MASK;\n\n\tstart = start & ISP_PAGE_MASK;\n\tend = ISP_PAGE_ALIGN(end);\n\n\tptr = start;\n\tdo {\n\t\tidx = ISP_PTR_TO_L1_IDX(ptr);\n\n\t\tl2_pte = atomisp_get_pte(l1_pt, idx);\n\n\t\tif (!ISP_PTE_VALID(mmu, l2_pte)) {\n\t\t\tmmu_unmap_l1_pte_error(mmu, l1_pt, idx, ptr, l2_pte);\n\t\t\tcontinue;\n\t\t}\n\n\t\tl2_pt = isp_pte_to_pgaddr(mmu, l2_pte);\n\n\t\tl1_aligned = (ptr & ISP_PAGE_MASK) + (1U << ISP_L1PT_OFFSET);\n\n\t\tif (l1_aligned < end) {\n\t\t\tmmu_l2_unmap(mmu, l1_pt, idx, l2_pt, ptr, l1_aligned);\n\t\t\tptr = l1_aligned;\n\t\t} else {\n\t\t\tmmu_l2_unmap(mmu, l1_pt, idx, l2_pt, ptr, end);\n\t\t\tptr = end;\n\t\t}\n\t\t \n\t\t \n\t} while (ptr < end && idx < ISP_L1PT_PTES);\n}\n\n \nstatic void mmu_unmap(struct isp_mmu *mmu, unsigned int isp_virt,\n\t\t      unsigned int pgnr)\n{\n\tunsigned int start, end;\n\tphys_addr_t l1_pt;\n\n\tmutex_lock(&mmu->pt_mutex);\n\tif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\n\t\tmmu_unmap_l1_pt_error(mmu, mmu->l1_pte);\n\t\tmutex_unlock(&mmu->pt_mutex);\n\t\treturn;\n\t}\n\n\tl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\n\n\tstart = (isp_virt) & ISP_PAGE_MASK;\n\tend = start + (pgnr << ISP_PAGE_OFFSET);\n\n\tmmu_l1_unmap(mmu, l1_pt, start, end);\n\tmutex_unlock(&mmu->pt_mutex);\n}\n\n \nstatic void free_mmu_map(struct isp_mmu *mmu, unsigned int start_isp_virt,\n\t\t\t unsigned int end_isp_virt)\n{\n\tunsigned int pgnr;\n\tunsigned int start, end;\n\n\tstart = (start_isp_virt) & ISP_PAGE_MASK;\n\tend = (end_isp_virt) & ISP_PAGE_MASK;\n\tpgnr = (end - start) >> ISP_PAGE_OFFSET;\n\tmmu_unmap(mmu, start, pgnr);\n}\n\nint isp_mmu_map(struct isp_mmu *mmu, unsigned int isp_virt,\n\t\tphys_addr_t phys, unsigned int pgnr)\n{\n\treturn mmu_map(mmu, isp_virt, phys, pgnr);\n}\n\nvoid isp_mmu_unmap(struct isp_mmu *mmu, unsigned int isp_virt,\n\t\t   unsigned int pgnr)\n{\n\tmmu_unmap(mmu, isp_virt, pgnr);\n}\n\nstatic void isp_mmu_flush_tlb_range_default(struct isp_mmu *mmu,\n\tunsigned int start,\n\tunsigned int size)\n{\n\tisp_mmu_flush_tlb(mmu);\n}\n\n \nint isp_mmu_init(struct isp_mmu *mmu, struct isp_mmu_client *driver)\n{\n\tif (!mmu)\t\t \n\t\treturn -EINVAL;\n\tif (!driver)\t\t \n\t\treturn -EINVAL;\n\n\tif (!driver->name)\n\t\tdev_warn(atomisp_dev, \"NULL name for MMU driver...\\n\");\n\n\tmmu->driver = driver;\n\n\tif (!driver->tlb_flush_all) {\n\t\tdev_err(atomisp_dev, \"tlb_flush_all operation not provided.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!driver->tlb_flush_range)\n\t\tdriver->tlb_flush_range = isp_mmu_flush_tlb_range_default;\n\n\tif (!driver->pte_valid_mask) {\n\t\tdev_err(atomisp_dev, \"PTE_MASK is missing from mmu driver\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmmu->l1_pte = driver->null_pte;\n\n\tmutex_init(&mmu->pt_mutex);\n\n\treturn 0;\n}\n\n \nvoid isp_mmu_exit(struct isp_mmu *mmu)\n{\n\tunsigned int idx;\n\tunsigned int pte;\n\tphys_addr_t l1_pt, l2_pt;\n\n\tif (!mmu)\n\t\treturn;\n\n\tif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\n\t\tdev_warn(atomisp_dev, \"invalid L1PT: pte = 0x%x\\n\",\n\t\t\t (unsigned int)mmu->l1_pte);\n\t\treturn;\n\t}\n\n\tl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\n\n\tfor (idx = 0; idx < ISP_L1PT_PTES; idx++) {\n\t\tpte = atomisp_get_pte(l1_pt, idx);\n\n\t\tif (ISP_PTE_VALID(mmu, pte)) {\n\t\t\tl2_pt = isp_pte_to_pgaddr(mmu, pte);\n\n\t\t\tfree_page_table(mmu, l2_pt);\n\t\t}\n\t}\n\n\tfree_page_table(mmu, l1_pt);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}