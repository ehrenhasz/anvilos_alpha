{
  "module_name": "btt.c",
  "hash_id": "c7f7541abed52a8ac3007de8a3b3ebd79fc8ad57d435c5172d343f2a8ed431ed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvdimm/btt.c",
  "human_readable_source": "\n \n#include <linux/highmem.h>\n#include <linux/debugfs.h>\n#include <linux/blkdev.h>\n#include <linux/pagemap.h>\n#include <linux/module.h>\n#include <linux/device.h>\n#include <linux/mutex.h>\n#include <linux/hdreg.h>\n#include <linux/sizes.h>\n#include <linux/ndctl.h>\n#include <linux/fs.h>\n#include <linux/nd.h>\n#include <linux/backing-dev.h>\n#include \"btt.h\"\n#include \"nd.h\"\n\nenum log_ent_request {\n\tLOG_NEW_ENT = 0,\n\tLOG_OLD_ENT\n};\n\nstatic struct device *to_dev(struct arena_info *arena)\n{\n\treturn &arena->nd_btt->dev;\n}\n\nstatic u64 adjust_initial_offset(struct nd_btt *nd_btt, u64 offset)\n{\n\treturn offset + nd_btt->initial_offset;\n}\n\nstatic int arena_read_bytes(struct arena_info *arena, resource_size_t offset,\n\t\tvoid *buf, size_t n, unsigned long flags)\n{\n\tstruct nd_btt *nd_btt = arena->nd_btt;\n\tstruct nd_namespace_common *ndns = nd_btt->ndns;\n\n\t \n\toffset = adjust_initial_offset(nd_btt, offset);\n\treturn nvdimm_read_bytes(ndns, offset, buf, n, flags);\n}\n\nstatic int arena_write_bytes(struct arena_info *arena, resource_size_t offset,\n\t\tvoid *buf, size_t n, unsigned long flags)\n{\n\tstruct nd_btt *nd_btt = arena->nd_btt;\n\tstruct nd_namespace_common *ndns = nd_btt->ndns;\n\n\t \n\toffset = adjust_initial_offset(nd_btt, offset);\n\treturn nvdimm_write_bytes(ndns, offset, buf, n, flags);\n}\n\nstatic int btt_info_write(struct arena_info *arena, struct btt_sb *super)\n{\n\tint ret;\n\n\t \n\tdev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->infooff, 512),\n\t\t\"arena->infooff: %#llx is unaligned\\n\", arena->infooff);\n\tdev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->info2off, 512),\n\t\t\"arena->info2off: %#llx is unaligned\\n\", arena->info2off);\n\n\tret = arena_write_bytes(arena, arena->info2off, super,\n\t\t\tsizeof(struct btt_sb), 0);\n\tif (ret)\n\t\treturn ret;\n\n\treturn arena_write_bytes(arena, arena->infooff, super,\n\t\t\tsizeof(struct btt_sb), 0);\n}\n\nstatic int btt_info_read(struct arena_info *arena, struct btt_sb *super)\n{\n\treturn arena_read_bytes(arena, arena->infooff, super,\n\t\t\tsizeof(struct btt_sb), 0);\n}\n\n \nstatic int __btt_map_write(struct arena_info *arena, u32 lba, __le32 mapping,\n\t\tunsigned long flags)\n{\n\tu64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);\n\n\tif (unlikely(lba >= arena->external_nlba))\n\t\tdev_err_ratelimited(to_dev(arena),\n\t\t\t\"%s: lba %#x out of range (max: %#x)\\n\",\n\t\t\t__func__, lba, arena->external_nlba);\n\treturn arena_write_bytes(arena, ns_off, &mapping, MAP_ENT_SIZE, flags);\n}\n\nstatic int btt_map_write(struct arena_info *arena, u32 lba, u32 mapping,\n\t\t\tu32 z_flag, u32 e_flag, unsigned long rwb_flags)\n{\n\tu32 ze;\n\t__le32 mapping_le;\n\n\t \n\tmapping = ent_lba(mapping);\n\n\tze = (z_flag << 1) + e_flag;\n\tswitch (ze) {\n\tcase 0:\n\t\t \n\t\tmapping |= MAP_ENT_NORMAL;\n\t\tbreak;\n\tcase 1:\n\t\tmapping |= (1 << MAP_ERR_SHIFT);\n\t\tbreak;\n\tcase 2:\n\t\tmapping |= (1 << MAP_TRIM_SHIFT);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_err_ratelimited(to_dev(arena),\n\t\t\t\"Invalid use of Z and E flags\\n\");\n\t\treturn -EIO;\n\t}\n\n\tmapping_le = cpu_to_le32(mapping);\n\treturn __btt_map_write(arena, lba, mapping_le, rwb_flags);\n}\n\nstatic int btt_map_read(struct arena_info *arena, u32 lba, u32 *mapping,\n\t\t\tint *trim, int *error, unsigned long rwb_flags)\n{\n\tint ret;\n\t__le32 in;\n\tu32 raw_mapping, postmap, ze, z_flag, e_flag;\n\tu64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);\n\n\tif (unlikely(lba >= arena->external_nlba))\n\t\tdev_err_ratelimited(to_dev(arena),\n\t\t\t\"%s: lba %#x out of range (max: %#x)\\n\",\n\t\t\t__func__, lba, arena->external_nlba);\n\n\tret = arena_read_bytes(arena, ns_off, &in, MAP_ENT_SIZE, rwb_flags);\n\tif (ret)\n\t\treturn ret;\n\n\traw_mapping = le32_to_cpu(in);\n\n\tz_flag = ent_z_flag(raw_mapping);\n\te_flag = ent_e_flag(raw_mapping);\n\tze = (z_flag << 1) + e_flag;\n\tpostmap = ent_lba(raw_mapping);\n\n\t \n\tz_flag = 0;\n\te_flag = 0;\n\n\tswitch (ze) {\n\tcase 0:\n\t\t \n\t\t*mapping = lba;\n\t\tbreak;\n\tcase 1:\n\t\t*mapping = postmap;\n\t\te_flag = 1;\n\t\tbreak;\n\tcase 2:\n\t\t*mapping = postmap;\n\t\tz_flag = 1;\n\t\tbreak;\n\tcase 3:\n\t\t*mapping = postmap;\n\t\tbreak;\n\tdefault:\n\t\treturn -EIO;\n\t}\n\n\tif (trim)\n\t\t*trim = z_flag;\n\tif (error)\n\t\t*error = e_flag;\n\n\treturn ret;\n}\n\nstatic int btt_log_group_read(struct arena_info *arena, u32 lane,\n\t\t\tstruct log_group *log)\n{\n\treturn arena_read_bytes(arena,\n\t\t\tarena->logoff + (lane * LOG_GRP_SIZE), log,\n\t\t\tLOG_GRP_SIZE, 0);\n}\n\nstatic struct dentry *debugfs_root;\n\nstatic void arena_debugfs_init(struct arena_info *a, struct dentry *parent,\n\t\t\t\tint idx)\n{\n\tchar dirname[32];\n\tstruct dentry *d;\n\n\t \n\tif (!parent)\n\t\treturn;\n\n\tsnprintf(dirname, 32, \"arena%d\", idx);\n\td = debugfs_create_dir(dirname, parent);\n\tif (IS_ERR_OR_NULL(d))\n\t\treturn;\n\ta->debugfs_dir = d;\n\n\tdebugfs_create_x64(\"size\", S_IRUGO, d, &a->size);\n\tdebugfs_create_x64(\"external_lba_start\", S_IRUGO, d,\n\t\t\t\t&a->external_lba_start);\n\tdebugfs_create_x32(\"internal_nlba\", S_IRUGO, d, &a->internal_nlba);\n\tdebugfs_create_u32(\"internal_lbasize\", S_IRUGO, d,\n\t\t\t\t&a->internal_lbasize);\n\tdebugfs_create_x32(\"external_nlba\", S_IRUGO, d, &a->external_nlba);\n\tdebugfs_create_u32(\"external_lbasize\", S_IRUGO, d,\n\t\t\t\t&a->external_lbasize);\n\tdebugfs_create_u32(\"nfree\", S_IRUGO, d, &a->nfree);\n\tdebugfs_create_u16(\"version_major\", S_IRUGO, d, &a->version_major);\n\tdebugfs_create_u16(\"version_minor\", S_IRUGO, d, &a->version_minor);\n\tdebugfs_create_x64(\"nextoff\", S_IRUGO, d, &a->nextoff);\n\tdebugfs_create_x64(\"infooff\", S_IRUGO, d, &a->infooff);\n\tdebugfs_create_x64(\"dataoff\", S_IRUGO, d, &a->dataoff);\n\tdebugfs_create_x64(\"mapoff\", S_IRUGO, d, &a->mapoff);\n\tdebugfs_create_x64(\"logoff\", S_IRUGO, d, &a->logoff);\n\tdebugfs_create_x64(\"info2off\", S_IRUGO, d, &a->info2off);\n\tdebugfs_create_x32(\"flags\", S_IRUGO, d, &a->flags);\n\tdebugfs_create_u32(\"log_index_0\", S_IRUGO, d, &a->log_index[0]);\n\tdebugfs_create_u32(\"log_index_1\", S_IRUGO, d, &a->log_index[1]);\n}\n\nstatic void btt_debugfs_init(struct btt *btt)\n{\n\tint i = 0;\n\tstruct arena_info *arena;\n\n\tbtt->debugfs_dir = debugfs_create_dir(dev_name(&btt->nd_btt->dev),\n\t\t\t\t\t\tdebugfs_root);\n\tif (IS_ERR_OR_NULL(btt->debugfs_dir))\n\t\treturn;\n\n\tlist_for_each_entry(arena, &btt->arena_list, list) {\n\t\tarena_debugfs_init(arena, btt->debugfs_dir, i);\n\t\ti++;\n\t}\n}\n\nstatic u32 log_seq(struct log_group *log, int log_idx)\n{\n\treturn le32_to_cpu(log->ent[log_idx].seq);\n}\n\n \nstatic int btt_log_get_old(struct arena_info *a, struct log_group *log)\n{\n\tint idx0 = a->log_index[0];\n\tint idx1 = a->log_index[1];\n\tint old;\n\n\t \n\tif (log_seq(log, idx0) == 0) {\n\t\tlog->ent[idx0].seq = cpu_to_le32(1);\n\t\treturn 0;\n\t}\n\n\tif (log_seq(log, idx0) == log_seq(log, idx1))\n\t\treturn -EINVAL;\n\tif (log_seq(log, idx0) + log_seq(log, idx1) > 5)\n\t\treturn -EINVAL;\n\n\tif (log_seq(log, idx0) < log_seq(log, idx1)) {\n\t\tif ((log_seq(log, idx1) - log_seq(log, idx0)) == 1)\n\t\t\told = 0;\n\t\telse\n\t\t\told = 1;\n\t} else {\n\t\tif ((log_seq(log, idx0) - log_seq(log, idx1)) == 1)\n\t\t\told = 1;\n\t\telse\n\t\t\told = 0;\n\t}\n\n\treturn old;\n}\n\n \nstatic int btt_log_read(struct arena_info *arena, u32 lane,\n\t\t\tstruct log_entry *ent, int old_flag)\n{\n\tint ret;\n\tint old_ent, ret_ent;\n\tstruct log_group log;\n\n\tret = btt_log_group_read(arena, lane, &log);\n\tif (ret)\n\t\treturn -EIO;\n\n\told_ent = btt_log_get_old(arena, &log);\n\tif (old_ent < 0 || old_ent > 1) {\n\t\tdev_err(to_dev(arena),\n\t\t\t\t\"log corruption (%d): lane %d seq [%d, %d]\\n\",\n\t\t\t\told_ent, lane, log.ent[arena->log_index[0]].seq,\n\t\t\t\tlog.ent[arena->log_index[1]].seq);\n\t\t \n\t\treturn -EIO;\n\t}\n\n\tret_ent = (old_flag ? old_ent : (1 - old_ent));\n\n\tif (ent != NULL)\n\t\tmemcpy(ent, &log.ent[arena->log_index[ret_ent]], LOG_ENT_SIZE);\n\n\treturn ret_ent;\n}\n\n \nstatic int __btt_log_write(struct arena_info *arena, u32 lane,\n\t\t\tu32 sub, struct log_entry *ent, unsigned long flags)\n{\n\tint ret;\n\tu32 group_slot = arena->log_index[sub];\n\tunsigned int log_half = LOG_ENT_SIZE / 2;\n\tvoid *src = ent;\n\tu64 ns_off;\n\n\tns_off = arena->logoff + (lane * LOG_GRP_SIZE) +\n\t\t(group_slot * LOG_ENT_SIZE);\n\t \n\tret = arena_write_bytes(arena, ns_off, src, log_half, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tns_off += log_half;\n\tsrc += log_half;\n\treturn arena_write_bytes(arena, ns_off, src, log_half, flags);\n}\n\nstatic int btt_flog_write(struct arena_info *arena, u32 lane, u32 sub,\n\t\t\tstruct log_entry *ent)\n{\n\tint ret;\n\n\tret = __btt_log_write(arena, lane, sub, ent, NVDIMM_IO_ATOMIC);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tarena->freelist[lane].sub = 1 - arena->freelist[lane].sub;\n\tif (++(arena->freelist[lane].seq) == 4)\n\t\tarena->freelist[lane].seq = 1;\n\tif (ent_e_flag(le32_to_cpu(ent->old_map)))\n\t\tarena->freelist[lane].has_err = 1;\n\tarena->freelist[lane].block = ent_lba(le32_to_cpu(ent->old_map));\n\n\treturn ret;\n}\n\n \nstatic int btt_map_init(struct arena_info *arena)\n{\n\tint ret = -EINVAL;\n\tvoid *zerobuf;\n\tsize_t offset = 0;\n\tsize_t chunk_size = SZ_2M;\n\tsize_t mapsize = arena->logoff - arena->mapoff;\n\n\tzerobuf = kzalloc(chunk_size, GFP_KERNEL);\n\tif (!zerobuf)\n\t\treturn -ENOMEM;\n\n\t \n\tdev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->mapoff, 512),\n\t\t\"arena->mapoff: %#llx is unaligned\\n\", arena->mapoff);\n\n\twhile (mapsize) {\n\t\tsize_t size = min(mapsize, chunk_size);\n\n\t\tdev_WARN_ONCE(to_dev(arena), size < 512,\n\t\t\t\"chunk size: %#zx is unaligned\\n\", size);\n\t\tret = arena_write_bytes(arena, arena->mapoff + offset, zerobuf,\n\t\t\t\tsize, 0);\n\t\tif (ret)\n\t\t\tgoto free;\n\n\t\toffset += size;\n\t\tmapsize -= size;\n\t\tcond_resched();\n\t}\n\n free:\n\tkfree(zerobuf);\n\treturn ret;\n}\n\n \nstatic int btt_log_init(struct arena_info *arena)\n{\n\tsize_t logsize = arena->info2off - arena->logoff;\n\tsize_t chunk_size = SZ_4K, offset = 0;\n\tstruct log_entry ent;\n\tvoid *zerobuf;\n\tint ret;\n\tu32 i;\n\n\tzerobuf = kzalloc(chunk_size, GFP_KERNEL);\n\tif (!zerobuf)\n\t\treturn -ENOMEM;\n\t \n\tdev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->logoff, 512),\n\t\t\"arena->logoff: %#llx is unaligned\\n\", arena->logoff);\n\n\twhile (logsize) {\n\t\tsize_t size = min(logsize, chunk_size);\n\n\t\tdev_WARN_ONCE(to_dev(arena), size < 512,\n\t\t\t\"chunk size: %#zx is unaligned\\n\", size);\n\t\tret = arena_write_bytes(arena, arena->logoff + offset, zerobuf,\n\t\t\t\tsize, 0);\n\t\tif (ret)\n\t\t\tgoto free;\n\n\t\toffset += size;\n\t\tlogsize -= size;\n\t\tcond_resched();\n\t}\n\n\tfor (i = 0; i < arena->nfree; i++) {\n\t\tent.lba = cpu_to_le32(i);\n\t\tent.old_map = cpu_to_le32(arena->external_nlba + i);\n\t\tent.new_map = cpu_to_le32(arena->external_nlba + i);\n\t\tent.seq = cpu_to_le32(LOG_SEQ_INIT);\n\t\tret = __btt_log_write(arena, i, 0, &ent, 0);\n\t\tif (ret)\n\t\t\tgoto free;\n\t}\n\n free:\n\tkfree(zerobuf);\n\treturn ret;\n}\n\nstatic u64 to_namespace_offset(struct arena_info *arena, u64 lba)\n{\n\treturn arena->dataoff + ((u64)lba * arena->internal_lbasize);\n}\n\nstatic int arena_clear_freelist_error(struct arena_info *arena, u32 lane)\n{\n\tint ret = 0;\n\n\tif (arena->freelist[lane].has_err) {\n\t\tvoid *zero_page = page_address(ZERO_PAGE(0));\n\t\tu32 lba = arena->freelist[lane].block;\n\t\tu64 nsoff = to_namespace_offset(arena, lba);\n\t\tunsigned long len = arena->sector_size;\n\n\t\tmutex_lock(&arena->err_lock);\n\n\t\twhile (len) {\n\t\t\tunsigned long chunk = min(len, PAGE_SIZE);\n\n\t\t\tret = arena_write_bytes(arena, nsoff, zero_page,\n\t\t\t\tchunk, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tlen -= chunk;\n\t\t\tnsoff += chunk;\n\t\t\tif (len == 0)\n\t\t\t\tarena->freelist[lane].has_err = 0;\n\t\t}\n\t\tmutex_unlock(&arena->err_lock);\n\t}\n\treturn ret;\n}\n\nstatic int btt_freelist_init(struct arena_info *arena)\n{\n\tint new, ret;\n\tstruct log_entry log_new;\n\tu32 i, map_entry, log_oldmap, log_newmap;\n\n\tarena->freelist = kcalloc(arena->nfree, sizeof(struct free_entry),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!arena->freelist)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < arena->nfree; i++) {\n\t\tnew = btt_log_read(arena, i, &log_new, LOG_NEW_ENT);\n\t\tif (new < 0)\n\t\t\treturn new;\n\n\t\t \n\t\tlog_oldmap = ent_lba(le32_to_cpu(log_new.old_map));\n\t\tlog_newmap = ent_lba(le32_to_cpu(log_new.new_map));\n\n\t\t \n\t\tarena->freelist[i].sub = 1 - new;\n\t\tarena->freelist[i].seq = nd_inc_seq(le32_to_cpu(log_new.seq));\n\t\tarena->freelist[i].block = log_oldmap;\n\n\t\t \n\t\tif (ent_e_flag(le32_to_cpu(log_new.old_map)) &&\n\t\t    !ent_normal(le32_to_cpu(log_new.old_map))) {\n\t\t\tarena->freelist[i].has_err = 1;\n\t\t\tret = arena_clear_freelist_error(arena, i);\n\t\t\tif (ret)\n\t\t\t\tdev_err_ratelimited(to_dev(arena),\n\t\t\t\t\t\"Unable to clear known errors\\n\");\n\t\t}\n\n\t\t \n\t\tif (log_oldmap == log_newmap)\n\t\t\tcontinue;\n\n\t\t \n\t\tret = btt_map_read(arena, le32_to_cpu(log_new.lba), &map_entry,\n\t\t\t\tNULL, NULL, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif ((log_newmap != map_entry) && (log_oldmap == map_entry)) {\n\t\t\t \n\t\t\tret = btt_map_write(arena, le32_to_cpu(log_new.lba),\n\t\t\t\t\tle32_to_cpu(log_new.new_map), 0, 0, 0);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool ent_is_padding(struct log_entry *ent)\n{\n\treturn (ent->lba == 0) && (ent->old_map == 0) && (ent->new_map == 0)\n\t\t&& (ent->seq == 0);\n}\n\n \nstatic int log_set_indices(struct arena_info *arena)\n{\n\tbool idx_set = false, initial_state = true;\n\tint ret, log_index[2] = {-1, -1};\n\tu32 i, j, next_idx = 0;\n\tstruct log_group log;\n\tu32 pad_count = 0;\n\n\tfor (i = 0; i < arena->nfree; i++) {\n\t\tret = btt_log_group_read(arena, i, &log);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tfor (j = 0; j < 4; j++) {\n\t\t\tif (!idx_set) {\n\t\t\t\tif (ent_is_padding(&log.ent[j])) {\n\t\t\t\t\tpad_count++;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tif ((next_idx == 1) &&\n\t\t\t\t\t\t(j == log_index[0]))\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t \n\t\t\t\t\tlog_index[next_idx] = j;\n\t\t\t\t\tnext_idx++;\n\t\t\t\t}\n\t\t\t\tif (next_idx == 2) {\n\t\t\t\t\t \n\t\t\t\t\tidx_set = true;\n\t\t\t\t} else if (next_idx > 2) {\n\t\t\t\t\t \n\t\t\t\t\treturn -ENXIO;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (j == log_index[0]) {\n\t\t\t\t\t \n\t\t\t\t\tif (ent_is_padding(&log.ent[j]))\n\t\t\t\t\t\treturn -ENXIO;\n\t\t\t\t} else if (j == log_index[1]) {\n\t\t\t\t\t;\n\t\t\t\t\t \n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tif (!ent_is_padding(&log.ent[j]))\n\t\t\t\t\t\treturn -ENXIO;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (pad_count < 3)\n\t\t\tinitial_state = false;\n\t\tpad_count = 0;\n\t}\n\n\tif (!initial_state && !idx_set)\n\t\treturn -ENXIO;\n\n\t \n\tif (initial_state)\n\t\tlog_index[1] = 1;\n\n\t \n\tif ((log_index[0] == 0) && ((log_index[1] == 1) || (log_index[1] == 2)))\n\t\t;  \n\telse {\n\t\tdev_err(to_dev(arena), \"Found an unknown padding scheme\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tarena->log_index[0] = log_index[0];\n\tarena->log_index[1] = log_index[1];\n\tdev_dbg(to_dev(arena), \"log_index_0 = %d\\n\", log_index[0]);\n\tdev_dbg(to_dev(arena), \"log_index_1 = %d\\n\", log_index[1]);\n\treturn 0;\n}\n\nstatic int btt_rtt_init(struct arena_info *arena)\n{\n\tarena->rtt = kcalloc(arena->nfree, sizeof(u32), GFP_KERNEL);\n\tif (arena->rtt == NULL)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int btt_maplocks_init(struct arena_info *arena)\n{\n\tu32 i;\n\n\tarena->map_locks = kcalloc(arena->nfree, sizeof(struct aligned_lock),\n\t\t\t\tGFP_KERNEL);\n\tif (!arena->map_locks)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < arena->nfree; i++)\n\t\tspin_lock_init(&arena->map_locks[i].lock);\n\n\treturn 0;\n}\n\nstatic struct arena_info *alloc_arena(struct btt *btt, size_t size,\n\t\t\t\tsize_t start, size_t arena_off)\n{\n\tstruct arena_info *arena;\n\tu64 logsize, mapsize, datasize;\n\tu64 available = size;\n\n\tarena = kzalloc(sizeof(struct arena_info), GFP_KERNEL);\n\tif (!arena)\n\t\treturn NULL;\n\tarena->nd_btt = btt->nd_btt;\n\tarena->sector_size = btt->sector_size;\n\tmutex_init(&arena->err_lock);\n\n\tif (!size)\n\t\treturn arena;\n\n\tarena->size = size;\n\tarena->external_lba_start = start;\n\tarena->external_lbasize = btt->lbasize;\n\tarena->internal_lbasize = roundup(arena->external_lbasize,\n\t\t\t\t\tINT_LBASIZE_ALIGNMENT);\n\tarena->nfree = BTT_DEFAULT_NFREE;\n\tarena->version_major = btt->nd_btt->version_major;\n\tarena->version_minor = btt->nd_btt->version_minor;\n\n\tif (available % BTT_PG_SIZE)\n\t\tavailable -= (available % BTT_PG_SIZE);\n\n\t \n\tavailable -= 2 * BTT_PG_SIZE;\n\n\t \n\tlogsize = roundup(arena->nfree * LOG_GRP_SIZE, BTT_PG_SIZE);\n\tavailable -= logsize;\n\n\t \n\tarena->internal_nlba = div_u64(available - BTT_PG_SIZE,\n\t\t\tarena->internal_lbasize + MAP_ENT_SIZE);\n\tarena->external_nlba = arena->internal_nlba - arena->nfree;\n\n\tmapsize = roundup((arena->external_nlba * MAP_ENT_SIZE), BTT_PG_SIZE);\n\tdatasize = available - mapsize;\n\n\t \n\tarena->infooff = arena_off;\n\tarena->dataoff = arena->infooff + BTT_PG_SIZE;\n\tarena->mapoff = arena->dataoff + datasize;\n\tarena->logoff = arena->mapoff + mapsize;\n\tarena->info2off = arena->logoff + logsize;\n\n\t \n\tarena->log_index[0] = 0;\n\tarena->log_index[1] = 1;\n\treturn arena;\n}\n\nstatic void free_arenas(struct btt *btt)\n{\n\tstruct arena_info *arena, *next;\n\n\tlist_for_each_entry_safe(arena, next, &btt->arena_list, list) {\n\t\tlist_del(&arena->list);\n\t\tkfree(arena->rtt);\n\t\tkfree(arena->map_locks);\n\t\tkfree(arena->freelist);\n\t\tdebugfs_remove_recursive(arena->debugfs_dir);\n\t\tkfree(arena);\n\t}\n}\n\n \nstatic void parse_arena_meta(struct arena_info *arena, struct btt_sb *super,\n\t\t\t\tu64 arena_off)\n{\n\tarena->internal_nlba = le32_to_cpu(super->internal_nlba);\n\tarena->internal_lbasize = le32_to_cpu(super->internal_lbasize);\n\tarena->external_nlba = le32_to_cpu(super->external_nlba);\n\tarena->external_lbasize = le32_to_cpu(super->external_lbasize);\n\tarena->nfree = le32_to_cpu(super->nfree);\n\tarena->version_major = le16_to_cpu(super->version_major);\n\tarena->version_minor = le16_to_cpu(super->version_minor);\n\n\tarena->nextoff = (super->nextoff == 0) ? 0 : (arena_off +\n\t\t\tle64_to_cpu(super->nextoff));\n\tarena->infooff = arena_off;\n\tarena->dataoff = arena_off + le64_to_cpu(super->dataoff);\n\tarena->mapoff = arena_off + le64_to_cpu(super->mapoff);\n\tarena->logoff = arena_off + le64_to_cpu(super->logoff);\n\tarena->info2off = arena_off + le64_to_cpu(super->info2off);\n\n\tarena->size = (le64_to_cpu(super->nextoff) > 0)\n\t\t? (le64_to_cpu(super->nextoff))\n\t\t: (arena->info2off - arena->infooff + BTT_PG_SIZE);\n\n\tarena->flags = le32_to_cpu(super->flags);\n}\n\nstatic int discover_arenas(struct btt *btt)\n{\n\tint ret = 0;\n\tstruct arena_info *arena;\n\tstruct btt_sb *super;\n\tsize_t remaining = btt->rawsize;\n\tu64 cur_nlba = 0;\n\tsize_t cur_off = 0;\n\tint num_arenas = 0;\n\n\tsuper = kzalloc(sizeof(*super), GFP_KERNEL);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\twhile (remaining) {\n\t\t \n\t\tarena = alloc_arena(btt, 0, 0, 0);\n\t\tif (!arena) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_super;\n\t\t}\n\n\t\tarena->infooff = cur_off;\n\t\tret = btt_info_read(arena, super);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!nd_btt_arena_is_valid(btt->nd_btt, super)) {\n\t\t\tif (remaining == btt->rawsize) {\n\t\t\t\tbtt->init_state = INIT_NOTFOUND;\n\t\t\t\tdev_info(to_dev(arena), \"No existing arenas\\n\");\n\t\t\t\tgoto out;\n\t\t\t} else {\n\t\t\t\tdev_err(to_dev(arena),\n\t\t\t\t\t\t\"Found corrupted metadata!\\n\");\n\t\t\t\tret = -ENODEV;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tarena->external_lba_start = cur_nlba;\n\t\tparse_arena_meta(arena, super, cur_off);\n\n\t\tret = log_set_indices(arena);\n\t\tif (ret) {\n\t\t\tdev_err(to_dev(arena),\n\t\t\t\t\"Unable to deduce log/padding indices\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btt_freelist_init(arena);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = btt_rtt_init(arena);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = btt_maplocks_init(arena);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tlist_add_tail(&arena->list, &btt->arena_list);\n\n\t\tremaining -= arena->size;\n\t\tcur_off += arena->size;\n\t\tcur_nlba += arena->external_nlba;\n\t\tnum_arenas++;\n\n\t\tif (arena->nextoff == 0)\n\t\t\tbreak;\n\t}\n\tbtt->num_arenas = num_arenas;\n\tbtt->nlba = cur_nlba;\n\tbtt->init_state = INIT_READY;\n\n\tkfree(super);\n\treturn ret;\n\n out:\n\tkfree(arena);\n\tfree_arenas(btt);\n out_super:\n\tkfree(super);\n\treturn ret;\n}\n\nstatic int create_arenas(struct btt *btt)\n{\n\tsize_t remaining = btt->rawsize;\n\tsize_t cur_off = 0;\n\n\twhile (remaining) {\n\t\tstruct arena_info *arena;\n\t\tsize_t arena_size = min_t(u64, ARENA_MAX_SIZE, remaining);\n\n\t\tremaining -= arena_size;\n\t\tif (arena_size < ARENA_MIN_SIZE)\n\t\t\tbreak;\n\n\t\tarena = alloc_arena(btt, arena_size, btt->nlba, cur_off);\n\t\tif (!arena) {\n\t\t\tfree_arenas(btt);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tbtt->nlba += arena->external_nlba;\n\t\tif (remaining >= ARENA_MIN_SIZE)\n\t\t\tarena->nextoff = arena->size;\n\t\telse\n\t\t\tarena->nextoff = 0;\n\t\tcur_off += arena_size;\n\t\tlist_add_tail(&arena->list, &btt->arena_list);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int btt_arena_write_layout(struct arena_info *arena)\n{\n\tint ret;\n\tu64 sum;\n\tstruct btt_sb *super;\n\tstruct nd_btt *nd_btt = arena->nd_btt;\n\tconst uuid_t *parent_uuid = nd_dev_to_uuid(&nd_btt->ndns->dev);\n\n\tret = btt_map_init(arena);\n\tif (ret)\n\t\treturn ret;\n\n\tret = btt_log_init(arena);\n\tif (ret)\n\t\treturn ret;\n\n\tsuper = kzalloc(sizeof(struct btt_sb), GFP_NOIO);\n\tif (!super)\n\t\treturn -ENOMEM;\n\n\tstrncpy(super->signature, BTT_SIG, BTT_SIG_LEN);\n\texport_uuid(super->uuid, nd_btt->uuid);\n\texport_uuid(super->parent_uuid, parent_uuid);\n\tsuper->flags = cpu_to_le32(arena->flags);\n\tsuper->version_major = cpu_to_le16(arena->version_major);\n\tsuper->version_minor = cpu_to_le16(arena->version_minor);\n\tsuper->external_lbasize = cpu_to_le32(arena->external_lbasize);\n\tsuper->external_nlba = cpu_to_le32(arena->external_nlba);\n\tsuper->internal_lbasize = cpu_to_le32(arena->internal_lbasize);\n\tsuper->internal_nlba = cpu_to_le32(arena->internal_nlba);\n\tsuper->nfree = cpu_to_le32(arena->nfree);\n\tsuper->infosize = cpu_to_le32(sizeof(struct btt_sb));\n\tsuper->nextoff = cpu_to_le64(arena->nextoff);\n\t \n\tsuper->dataoff = cpu_to_le64(arena->dataoff - arena->infooff);\n\tsuper->mapoff = cpu_to_le64(arena->mapoff - arena->infooff);\n\tsuper->logoff = cpu_to_le64(arena->logoff - arena->infooff);\n\tsuper->info2off = cpu_to_le64(arena->info2off - arena->infooff);\n\n\tsuper->flags = 0;\n\tsum = nd_sb_checksum((struct nd_gen_sb *) super);\n\tsuper->checksum = cpu_to_le64(sum);\n\n\tret = btt_info_write(arena, super);\n\n\tkfree(super);\n\treturn ret;\n}\n\n \nstatic int btt_meta_init(struct btt *btt)\n{\n\tint ret = 0;\n\tstruct arena_info *arena;\n\n\tmutex_lock(&btt->init_lock);\n\tlist_for_each_entry(arena, &btt->arena_list, list) {\n\t\tret = btt_arena_write_layout(arena);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tret = btt_freelist_init(arena);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tret = btt_rtt_init(arena);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tret = btt_maplocks_init(arena);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\n\tbtt->init_state = INIT_READY;\n\n unlock:\n\tmutex_unlock(&btt->init_lock);\n\treturn ret;\n}\n\nstatic u32 btt_meta_size(struct btt *btt)\n{\n\treturn btt->lbasize - btt->sector_size;\n}\n\n \nstatic int lba_to_arena(struct btt *btt, sector_t sector, __u32 *premap,\n\t\t\t\tstruct arena_info **arena)\n{\n\tstruct arena_info *arena_list;\n\t__u64 lba = div_u64(sector << SECTOR_SHIFT, btt->sector_size);\n\n\tlist_for_each_entry(arena_list, &btt->arena_list, list) {\n\t\tif (lba < arena_list->external_nlba) {\n\t\t\t*arena = arena_list;\n\t\t\t*premap = lba;\n\t\t\treturn 0;\n\t\t}\n\t\tlba -= arena_list->external_nlba;\n\t}\n\n\treturn -EIO;\n}\n\n \nstatic void lock_map(struct arena_info *arena, u32 premap)\n\t\t__acquires(&arena->map_locks[idx].lock)\n{\n\tu32 idx = (premap * MAP_ENT_SIZE / L1_CACHE_BYTES) % arena->nfree;\n\n\tspin_lock(&arena->map_locks[idx].lock);\n}\n\nstatic void unlock_map(struct arena_info *arena, u32 premap)\n\t\t__releases(&arena->map_locks[idx].lock)\n{\n\tu32 idx = (premap * MAP_ENT_SIZE / L1_CACHE_BYTES) % arena->nfree;\n\n\tspin_unlock(&arena->map_locks[idx].lock);\n}\n\nstatic int btt_data_read(struct arena_info *arena, struct page *page,\n\t\t\tunsigned int off, u32 lba, u32 len)\n{\n\tint ret;\n\tu64 nsoff = to_namespace_offset(arena, lba);\n\tvoid *mem = kmap_atomic(page);\n\n\tret = arena_read_bytes(arena, nsoff, mem + off, len, NVDIMM_IO_ATOMIC);\n\tkunmap_atomic(mem);\n\n\treturn ret;\n}\n\nstatic int btt_data_write(struct arena_info *arena, u32 lba,\n\t\t\tstruct page *page, unsigned int off, u32 len)\n{\n\tint ret;\n\tu64 nsoff = to_namespace_offset(arena, lba);\n\tvoid *mem = kmap_atomic(page);\n\n\tret = arena_write_bytes(arena, nsoff, mem + off, len, NVDIMM_IO_ATOMIC);\n\tkunmap_atomic(mem);\n\n\treturn ret;\n}\n\nstatic void zero_fill_data(struct page *page, unsigned int off, u32 len)\n{\n\tvoid *mem = kmap_atomic(page);\n\n\tmemset(mem + off, 0, len);\n\tkunmap_atomic(mem);\n}\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\nstatic int btt_rw_integrity(struct btt *btt, struct bio_integrity_payload *bip,\n\t\t\tstruct arena_info *arena, u32 postmap, int rw)\n{\n\tunsigned int len = btt_meta_size(btt);\n\tu64 meta_nsoff;\n\tint ret = 0;\n\n\tif (bip == NULL)\n\t\treturn 0;\n\n\tmeta_nsoff = to_namespace_offset(arena, postmap) + btt->sector_size;\n\n\twhile (len) {\n\t\tunsigned int cur_len;\n\t\tstruct bio_vec bv;\n\t\tvoid *mem;\n\n\t\tbv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);\n\t\t \n\n\t\tcur_len = min(len, bv.bv_len);\n\t\tmem = bvec_kmap_local(&bv);\n\t\tif (rw)\n\t\t\tret = arena_write_bytes(arena, meta_nsoff, mem, cur_len,\n\t\t\t\t\tNVDIMM_IO_ATOMIC);\n\t\telse\n\t\t\tret = arena_read_bytes(arena, meta_nsoff, mem, cur_len,\n\t\t\t\t\tNVDIMM_IO_ATOMIC);\n\n\t\tkunmap_local(mem);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tlen -= cur_len;\n\t\tmeta_nsoff += cur_len;\n\t\tif (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))\n\t\t\treturn -EIO;\n\t}\n\n\treturn ret;\n}\n\n#else  \nstatic int btt_rw_integrity(struct btt *btt, struct bio_integrity_payload *bip,\n\t\t\tstruct arena_info *arena, u32 postmap, int rw)\n{\n\treturn 0;\n}\n#endif\n\nstatic int btt_read_pg(struct btt *btt, struct bio_integrity_payload *bip,\n\t\t\tstruct page *page, unsigned int off, sector_t sector,\n\t\t\tunsigned int len)\n{\n\tint ret = 0;\n\tint t_flag, e_flag;\n\tstruct arena_info *arena = NULL;\n\tu32 lane = 0, premap, postmap;\n\n\twhile (len) {\n\t\tu32 cur_len;\n\n\t\tlane = nd_region_acquire_lane(btt->nd_region);\n\n\t\tret = lba_to_arena(btt, sector, &premap, &arena);\n\t\tif (ret)\n\t\t\tgoto out_lane;\n\n\t\tcur_len = min(btt->sector_size, len);\n\n\t\tret = btt_map_read(arena, premap, &postmap, &t_flag, &e_flag,\n\t\t\t\tNVDIMM_IO_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out_lane;\n\n\t\t \n\t\twhile (1) {\n\t\t\tu32 new_map;\n\t\t\tint new_t, new_e;\n\n\t\t\tif (t_flag) {\n\t\t\t\tzero_fill_data(page, off, cur_len);\n\t\t\t\tgoto out_lane;\n\t\t\t}\n\n\t\t\tif (e_flag) {\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out_lane;\n\t\t\t}\n\n\t\t\tarena->rtt[lane] = RTT_VALID | postmap;\n\t\t\t \n\t\t\tbarrier();\n\n\t\t\tret = btt_map_read(arena, premap, &new_map, &new_t,\n\t\t\t\t\t\t&new_e, NVDIMM_IO_ATOMIC);\n\t\t\tif (ret)\n\t\t\t\tgoto out_rtt;\n\n\t\t\tif ((postmap == new_map) && (t_flag == new_t) &&\n\t\t\t\t\t(e_flag == new_e))\n\t\t\t\tbreak;\n\n\t\t\tpostmap = new_map;\n\t\t\tt_flag = new_t;\n\t\t\te_flag = new_e;\n\t\t}\n\n\t\tret = btt_data_read(arena, page, off, postmap, cur_len);\n\t\tif (ret) {\n\t\t\t \n\t\t\tif (btt_map_write(arena, premap, postmap, 0, 1, NVDIMM_IO_ATOMIC))\n\t\t\t\tdev_warn_ratelimited(to_dev(arena),\n\t\t\t\t\t\"Error persistently tracking bad blocks at %#x\\n\",\n\t\t\t\t\tpremap);\n\t\t\tgoto out_rtt;\n\t\t}\n\n\t\tif (bip) {\n\t\t\tret = btt_rw_integrity(btt, bip, arena, postmap, READ);\n\t\t\tif (ret)\n\t\t\t\tgoto out_rtt;\n\t\t}\n\n\t\tarena->rtt[lane] = RTT_INVALID;\n\t\tnd_region_release_lane(btt->nd_region, lane);\n\n\t\tlen -= cur_len;\n\t\toff += cur_len;\n\t\tsector += btt->sector_size >> SECTOR_SHIFT;\n\t}\n\n\treturn 0;\n\n out_rtt:\n\tarena->rtt[lane] = RTT_INVALID;\n out_lane:\n\tnd_region_release_lane(btt->nd_region, lane);\n\treturn ret;\n}\n\n \nstatic bool btt_is_badblock(struct btt *btt, struct arena_info *arena,\n\t\tu32 postmap)\n{\n\tu64 nsoff = adjust_initial_offset(arena->nd_btt,\n\t\t\tto_namespace_offset(arena, postmap));\n\tsector_t phys_sector = nsoff >> 9;\n\n\treturn is_bad_pmem(btt->phys_bb, phys_sector, arena->internal_lbasize);\n}\n\nstatic int btt_write_pg(struct btt *btt, struct bio_integrity_payload *bip,\n\t\t\tsector_t sector, struct page *page, unsigned int off,\n\t\t\tunsigned int len)\n{\n\tint ret = 0;\n\tstruct arena_info *arena = NULL;\n\tu32 premap = 0, old_postmap, new_postmap, lane = 0, i;\n\tstruct log_entry log;\n\tint sub;\n\n\twhile (len) {\n\t\tu32 cur_len;\n\t\tint e_flag;\n\n retry:\n\t\tlane = nd_region_acquire_lane(btt->nd_region);\n\n\t\tret = lba_to_arena(btt, sector, &premap, &arena);\n\t\tif (ret)\n\t\t\tgoto out_lane;\n\t\tcur_len = min(btt->sector_size, len);\n\n\t\tif ((arena->flags & IB_FLAG_ERROR_MASK) != 0) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_lane;\n\t\t}\n\n\t\tif (btt_is_badblock(btt, arena, arena->freelist[lane].block))\n\t\t\tarena->freelist[lane].has_err = 1;\n\n\t\tif (mutex_is_locked(&arena->err_lock)\n\t\t\t\t|| arena->freelist[lane].has_err) {\n\t\t\tnd_region_release_lane(btt->nd_region, lane);\n\n\t\t\tret = arena_clear_freelist_error(arena, lane);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\t \n\t\t\tgoto retry;\n\t\t}\n\n\t\tnew_postmap = arena->freelist[lane].block;\n\n\t\t \n\t\tfor (i = 0; i < arena->nfree; i++)\n\t\t\twhile (arena->rtt[i] == (RTT_VALID | new_postmap))\n\t\t\t\tcpu_relax();\n\n\n\t\tif (new_postmap >= arena->internal_nlba) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_lane;\n\t\t}\n\n\t\tret = btt_data_write(arena, new_postmap, page, off, cur_len);\n\t\tif (ret)\n\t\t\tgoto out_lane;\n\n\t\tif (bip) {\n\t\t\tret = btt_rw_integrity(btt, bip, arena, new_postmap,\n\t\t\t\t\t\tWRITE);\n\t\t\tif (ret)\n\t\t\t\tgoto out_lane;\n\t\t}\n\n\t\tlock_map(arena, premap);\n\t\tret = btt_map_read(arena, premap, &old_postmap, NULL, &e_flag,\n\t\t\t\tNVDIMM_IO_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out_map;\n\t\tif (old_postmap >= arena->internal_nlba) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_map;\n\t\t}\n\t\tif (e_flag)\n\t\t\tset_e_flag(old_postmap);\n\n\t\tlog.lba = cpu_to_le32(premap);\n\t\tlog.old_map = cpu_to_le32(old_postmap);\n\t\tlog.new_map = cpu_to_le32(new_postmap);\n\t\tlog.seq = cpu_to_le32(arena->freelist[lane].seq);\n\t\tsub = arena->freelist[lane].sub;\n\t\tret = btt_flog_write(arena, lane, sub, &log);\n\t\tif (ret)\n\t\t\tgoto out_map;\n\n\t\tret = btt_map_write(arena, premap, new_postmap, 0, 0,\n\t\t\tNVDIMM_IO_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out_map;\n\n\t\tunlock_map(arena, premap);\n\t\tnd_region_release_lane(btt->nd_region, lane);\n\n\t\tif (e_flag) {\n\t\t\tret = arena_clear_freelist_error(arena, lane);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tlen -= cur_len;\n\t\toff += cur_len;\n\t\tsector += btt->sector_size >> SECTOR_SHIFT;\n\t}\n\n\treturn 0;\n\n out_map:\n\tunlock_map(arena, premap);\n out_lane:\n\tnd_region_release_lane(btt->nd_region, lane);\n\treturn ret;\n}\n\nstatic int btt_do_bvec(struct btt *btt, struct bio_integrity_payload *bip,\n\t\t\tstruct page *page, unsigned int len, unsigned int off,\n\t\t\tenum req_op op, sector_t sector)\n{\n\tint ret;\n\n\tif (!op_is_write(op)) {\n\t\tret = btt_read_pg(btt, bip, page, off, sector, len);\n\t\tflush_dcache_page(page);\n\t} else {\n\t\tflush_dcache_page(page);\n\t\tret = btt_write_pg(btt, bip, sector, page, off, len);\n\t}\n\n\treturn ret;\n}\n\nstatic void btt_submit_bio(struct bio *bio)\n{\n\tstruct bio_integrity_payload *bip = bio_integrity(bio);\n\tstruct btt *btt = bio->bi_bdev->bd_disk->private_data;\n\tstruct bvec_iter iter;\n\tunsigned long start;\n\tstruct bio_vec bvec;\n\tint err = 0;\n\tbool do_acct;\n\n\tif (!bio_integrity_prep(bio))\n\t\treturn;\n\n\tdo_acct = blk_queue_io_stat(bio->bi_bdev->bd_disk->queue);\n\tif (do_acct)\n\t\tstart = bio_start_io_acct(bio);\n\tbio_for_each_segment(bvec, bio, iter) {\n\t\tunsigned int len = bvec.bv_len;\n\n\t\tif (len > PAGE_SIZE || len < btt->sector_size ||\n\t\t\t\tlen % btt->sector_size) {\n\t\t\tdev_err_ratelimited(&btt->nd_btt->dev,\n\t\t\t\t\"unaligned bio segment (len: %d)\\n\", len);\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = btt_do_bvec(btt, bip, bvec.bv_page, len, bvec.bv_offset,\n\t\t\t\t  bio_op(bio), iter.bi_sector);\n\t\tif (err) {\n\t\t\tdev_err(&btt->nd_btt->dev,\n\t\t\t\t\t\"io error in %s sector %lld, len %d,\\n\",\n\t\t\t\t\t(op_is_write(bio_op(bio))) ? \"WRITE\" :\n\t\t\t\t\t\"READ\",\n\t\t\t\t\t(unsigned long long) iter.bi_sector, len);\n\t\t\tbio->bi_status = errno_to_blk_status(err);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (do_acct)\n\t\tbio_end_io_acct(bio, start);\n\n\tbio_endio(bio);\n}\n\nstatic int btt_getgeo(struct block_device *bd, struct hd_geometry *geo)\n{\n\t \n\tgeo->heads = 1 << 6;\n\tgeo->sectors = 1 << 5;\n\tgeo->cylinders = get_capacity(bd->bd_disk) >> 11;\n\treturn 0;\n}\n\nstatic const struct block_device_operations btt_fops = {\n\t.owner =\t\tTHIS_MODULE,\n\t.submit_bio =\t\tbtt_submit_bio,\n\t.getgeo =\t\tbtt_getgeo,\n};\n\nstatic int btt_blk_init(struct btt *btt)\n{\n\tstruct nd_btt *nd_btt = btt->nd_btt;\n\tstruct nd_namespace_common *ndns = nd_btt->ndns;\n\tint rc = -ENOMEM;\n\n\tbtt->btt_disk = blk_alloc_disk(NUMA_NO_NODE);\n\tif (!btt->btt_disk)\n\t\treturn -ENOMEM;\n\n\tnvdimm_namespace_disk_name(ndns, btt->btt_disk->disk_name);\n\tbtt->btt_disk->first_minor = 0;\n\tbtt->btt_disk->fops = &btt_fops;\n\tbtt->btt_disk->private_data = btt;\n\n\tblk_queue_logical_block_size(btt->btt_disk->queue, btt->sector_size);\n\tblk_queue_max_hw_sectors(btt->btt_disk->queue, UINT_MAX);\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, btt->btt_disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_SYNCHRONOUS, btt->btt_disk->queue);\n\n\tif (btt_meta_size(btt)) {\n\t\trc = nd_integrity_init(btt->btt_disk, btt_meta_size(btt));\n\t\tif (rc)\n\t\t\tgoto out_cleanup_disk;\n\t}\n\n\tset_capacity(btt->btt_disk, btt->nlba * btt->sector_size >> 9);\n\trc = device_add_disk(&btt->nd_btt->dev, btt->btt_disk, NULL);\n\tif (rc)\n\t\tgoto out_cleanup_disk;\n\n\tbtt->nd_btt->size = btt->nlba * (u64)btt->sector_size;\n\tnvdimm_check_and_set_ro(btt->btt_disk);\n\n\treturn 0;\n\nout_cleanup_disk:\n\tput_disk(btt->btt_disk);\n\treturn rc;\n}\n\nstatic void btt_blk_cleanup(struct btt *btt)\n{\n\tdel_gendisk(btt->btt_disk);\n\tput_disk(btt->btt_disk);\n}\n\n \nstatic struct btt *btt_init(struct nd_btt *nd_btt, unsigned long long rawsize,\n\t\t\t    u32 lbasize, uuid_t *uuid,\n\t\t\t    struct nd_region *nd_region)\n{\n\tint ret;\n\tstruct btt *btt;\n\tstruct nd_namespace_io *nsio;\n\tstruct device *dev = &nd_btt->dev;\n\n\tbtt = devm_kzalloc(dev, sizeof(struct btt), GFP_KERNEL);\n\tif (!btt)\n\t\treturn NULL;\n\n\tbtt->nd_btt = nd_btt;\n\tbtt->rawsize = rawsize;\n\tbtt->lbasize = lbasize;\n\tbtt->sector_size = ((lbasize >= 4096) ? 4096 : 512);\n\tINIT_LIST_HEAD(&btt->arena_list);\n\tmutex_init(&btt->init_lock);\n\tbtt->nd_region = nd_region;\n\tnsio = to_nd_namespace_io(&nd_btt->ndns->dev);\n\tbtt->phys_bb = &nsio->bb;\n\n\tret = discover_arenas(btt);\n\tif (ret) {\n\t\tdev_err(dev, \"init: error in arena_discover: %d\\n\", ret);\n\t\treturn NULL;\n\t}\n\n\tif (btt->init_state != INIT_READY && nd_region->ro) {\n\t\tdev_warn(dev, \"%s is read-only, unable to init btt metadata\\n\",\n\t\t\t\tdev_name(&nd_region->dev));\n\t\treturn NULL;\n\t} else if (btt->init_state != INIT_READY) {\n\t\tbtt->num_arenas = (rawsize / ARENA_MAX_SIZE) +\n\t\t\t((rawsize % ARENA_MAX_SIZE) ? 1 : 0);\n\t\tdev_dbg(dev, \"init: %d arenas for %llu rawsize\\n\",\n\t\t\t\tbtt->num_arenas, rawsize);\n\n\t\tret = create_arenas(btt);\n\t\tif (ret) {\n\t\t\tdev_info(dev, \"init: create_arenas: %d\\n\", ret);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tret = btt_meta_init(btt);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"init: error in meta_init: %d\\n\", ret);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tret = btt_blk_init(btt);\n\tif (ret) {\n\t\tdev_err(dev, \"init: error in blk_init: %d\\n\", ret);\n\t\treturn NULL;\n\t}\n\n\tbtt_debugfs_init(btt);\n\n\treturn btt;\n}\n\n \nstatic void btt_fini(struct btt *btt)\n{\n\tif (btt) {\n\t\tbtt_blk_cleanup(btt);\n\t\tfree_arenas(btt);\n\t\tdebugfs_remove_recursive(btt->debugfs_dir);\n\t}\n}\n\nint nvdimm_namespace_attach_btt(struct nd_namespace_common *ndns)\n{\n\tstruct nd_btt *nd_btt = to_nd_btt(ndns->claim);\n\tstruct nd_region *nd_region;\n\tstruct btt_sb *btt_sb;\n\tstruct btt *btt;\n\tsize_t size, rawsize;\n\tint rc;\n\n\tif (!nd_btt->uuid || !nd_btt->ndns || !nd_btt->lbasize) {\n\t\tdev_dbg(&nd_btt->dev, \"incomplete btt configuration\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tbtt_sb = devm_kzalloc(&nd_btt->dev, sizeof(*btt_sb), GFP_KERNEL);\n\tif (!btt_sb)\n\t\treturn -ENOMEM;\n\n\tsize = nvdimm_namespace_capacity(ndns);\n\trc = devm_namespace_enable(&nd_btt->dev, ndns, size);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tnd_btt_version(nd_btt, ndns, btt_sb);\n\n\trawsize = size - nd_btt->initial_offset;\n\tif (rawsize < ARENA_MIN_SIZE) {\n\t\tdev_dbg(&nd_btt->dev, \"%s must be at least %ld bytes\\n\",\n\t\t\t\tdev_name(&ndns->dev),\n\t\t\t\tARENA_MIN_SIZE + nd_btt->initial_offset);\n\t\treturn -ENXIO;\n\t}\n\tnd_region = to_nd_region(nd_btt->dev.parent);\n\tbtt = btt_init(nd_btt, rawsize, nd_btt->lbasize, nd_btt->uuid,\n\t\t       nd_region);\n\tif (!btt)\n\t\treturn -ENOMEM;\n\tnd_btt->btt = btt;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(nvdimm_namespace_attach_btt);\n\nint nvdimm_namespace_detach_btt(struct nd_btt *nd_btt)\n{\n\tstruct btt *btt = nd_btt->btt;\n\n\tbtt_fini(btt);\n\tnd_btt->btt = NULL;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(nvdimm_namespace_detach_btt);\n\nstatic int __init nd_btt_init(void)\n{\n\tint rc = 0;\n\n\tdebugfs_root = debugfs_create_dir(\"btt\", NULL);\n\tif (IS_ERR_OR_NULL(debugfs_root))\n\t\trc = -ENXIO;\n\n\treturn rc;\n}\n\nstatic void __exit nd_btt_exit(void)\n{\n\tdebugfs_remove_recursive(debugfs_root);\n}\n\nMODULE_ALIAS_ND_DEVICE(ND_DEVICE_BTT);\nMODULE_AUTHOR(\"Vishal Verma <vishal.l.verma@linux.intel.com>\");\nMODULE_LICENSE(\"GPL v2\");\nmodule_init(nd_btt_init);\nmodule_exit(nd_btt_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}