{
  "module_name": "pmem.c",
  "hash_id": "0f3815a9c37f56fea47d2c57f9b6e2142c7449dd4d3f91670b9d92d2403b8f4a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvdimm/pmem.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/pagemap.h>\n#include <linux/hdreg.h>\n#include <linux/init.h>\n#include <linux/platform_device.h>\n#include <linux/set_memory.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/badblocks.h>\n#include <linux/memremap.h>\n#include <linux/kstrtox.h>\n#include <linux/vmalloc.h>\n#include <linux/blk-mq.h>\n#include <linux/pfn_t.h>\n#include <linux/slab.h>\n#include <linux/uio.h>\n#include <linux/dax.h>\n#include <linux/nd.h>\n#include <linux/mm.h>\n#include <asm/cacheflush.h>\n#include \"pmem.h\"\n#include \"btt.h\"\n#include \"pfn.h\"\n#include \"nd.h\"\n\nstatic struct device *to_dev(struct pmem_device *pmem)\n{\n\t \n\treturn pmem->bb.dev;\n}\n\nstatic struct nd_region *to_region(struct pmem_device *pmem)\n{\n\treturn to_nd_region(to_dev(pmem)->parent);\n}\n\nstatic phys_addr_t pmem_to_phys(struct pmem_device *pmem, phys_addr_t offset)\n{\n\treturn pmem->phys_addr + offset;\n}\n\nstatic sector_t to_sect(struct pmem_device *pmem, phys_addr_t offset)\n{\n\treturn (offset - pmem->data_offset) >> SECTOR_SHIFT;\n}\n\nstatic phys_addr_t to_offset(struct pmem_device *pmem, sector_t sector)\n{\n\treturn (sector << SECTOR_SHIFT) + pmem->data_offset;\n}\n\nstatic void pmem_mkpage_present(struct pmem_device *pmem, phys_addr_t offset,\n\t\tunsigned int len)\n{\n\tphys_addr_t phys = pmem_to_phys(pmem, offset);\n\tunsigned long pfn_start, pfn_end, pfn;\n\n\t \n\tif (is_vmalloc_addr(pmem->virt_addr))\n\t\treturn;\n\n\tpfn_start = PHYS_PFN(phys);\n\tpfn_end = pfn_start + PHYS_PFN(len);\n\tfor (pfn = pfn_start; pfn < pfn_end; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t \n\t\tif (test_and_clear_pmem_poison(page))\n\t\t\tclear_mce_nospec(pfn);\n\t}\n}\n\nstatic void pmem_clear_bb(struct pmem_device *pmem, sector_t sector, long blks)\n{\n\tif (blks == 0)\n\t\treturn;\n\tbadblocks_clear(&pmem->bb, sector, blks);\n\tif (pmem->bb_state)\n\t\tsysfs_notify_dirent(pmem->bb_state);\n}\n\nstatic long __pmem_clear_poison(struct pmem_device *pmem,\n\t\tphys_addr_t offset, unsigned int len)\n{\n\tphys_addr_t phys = pmem_to_phys(pmem, offset);\n\tlong cleared = nvdimm_clear_poison(to_dev(pmem), phys, len);\n\n\tif (cleared > 0) {\n\t\tpmem_mkpage_present(pmem, offset, cleared);\n\t\tarch_invalidate_pmem(pmem->virt_addr + offset, len);\n\t}\n\treturn cleared;\n}\n\nstatic blk_status_t pmem_clear_poison(struct pmem_device *pmem,\n\t\tphys_addr_t offset, unsigned int len)\n{\n\tlong cleared = __pmem_clear_poison(pmem, offset, len);\n\n\tif (cleared < 0)\n\t\treturn BLK_STS_IOERR;\n\n\tpmem_clear_bb(pmem, to_sect(pmem, offset), cleared >> SECTOR_SHIFT);\n\tif (cleared < len)\n\t\treturn BLK_STS_IOERR;\n\treturn BLK_STS_OK;\n}\n\nstatic void write_pmem(void *pmem_addr, struct page *page,\n\t\tunsigned int off, unsigned int len)\n{\n\tunsigned int chunk;\n\tvoid *mem;\n\n\twhile (len) {\n\t\tmem = kmap_atomic(page);\n\t\tchunk = min_t(unsigned int, len, PAGE_SIZE - off);\n\t\tmemcpy_flushcache(pmem_addr, mem + off, chunk);\n\t\tkunmap_atomic(mem);\n\t\tlen -= chunk;\n\t\toff = 0;\n\t\tpage++;\n\t\tpmem_addr += chunk;\n\t}\n}\n\nstatic blk_status_t read_pmem(struct page *page, unsigned int off,\n\t\tvoid *pmem_addr, unsigned int len)\n{\n\tunsigned int chunk;\n\tunsigned long rem;\n\tvoid *mem;\n\n\twhile (len) {\n\t\tmem = kmap_atomic(page);\n\t\tchunk = min_t(unsigned int, len, PAGE_SIZE - off);\n\t\trem = copy_mc_to_kernel(mem + off, pmem_addr, chunk);\n\t\tkunmap_atomic(mem);\n\t\tif (rem)\n\t\t\treturn BLK_STS_IOERR;\n\t\tlen -= chunk;\n\t\toff = 0;\n\t\tpage++;\n\t\tpmem_addr += chunk;\n\t}\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t pmem_do_read(struct pmem_device *pmem,\n\t\t\tstruct page *page, unsigned int page_off,\n\t\t\tsector_t sector, unsigned int len)\n{\n\tblk_status_t rc;\n\tphys_addr_t pmem_off = to_offset(pmem, sector);\n\tvoid *pmem_addr = pmem->virt_addr + pmem_off;\n\n\tif (unlikely(is_bad_pmem(&pmem->bb, sector, len)))\n\t\treturn BLK_STS_IOERR;\n\n\trc = read_pmem(page, page_off, pmem_addr, len);\n\tflush_dcache_page(page);\n\treturn rc;\n}\n\nstatic blk_status_t pmem_do_write(struct pmem_device *pmem,\n\t\t\tstruct page *page, unsigned int page_off,\n\t\t\tsector_t sector, unsigned int len)\n{\n\tphys_addr_t pmem_off = to_offset(pmem, sector);\n\tvoid *pmem_addr = pmem->virt_addr + pmem_off;\n\n\tif (unlikely(is_bad_pmem(&pmem->bb, sector, len))) {\n\t\tblk_status_t rc = pmem_clear_poison(pmem, pmem_off, len);\n\n\t\tif (rc != BLK_STS_OK)\n\t\t\treturn rc;\n\t}\n\n\tflush_dcache_page(page);\n\twrite_pmem(pmem_addr, page, page_off, len);\n\n\treturn BLK_STS_OK;\n}\n\nstatic void pmem_submit_bio(struct bio *bio)\n{\n\tint ret = 0;\n\tblk_status_t rc = 0;\n\tbool do_acct;\n\tunsigned long start;\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\tstruct pmem_device *pmem = bio->bi_bdev->bd_disk->private_data;\n\tstruct nd_region *nd_region = to_region(pmem);\n\n\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\tret = nvdimm_flush(nd_region, bio);\n\n\tdo_acct = blk_queue_io_stat(bio->bi_bdev->bd_disk->queue);\n\tif (do_acct)\n\t\tstart = bio_start_io_acct(bio);\n\tbio_for_each_segment(bvec, bio, iter) {\n\t\tif (op_is_write(bio_op(bio)))\n\t\t\trc = pmem_do_write(pmem, bvec.bv_page, bvec.bv_offset,\n\t\t\t\titer.bi_sector, bvec.bv_len);\n\t\telse\n\t\t\trc = pmem_do_read(pmem, bvec.bv_page, bvec.bv_offset,\n\t\t\t\titer.bi_sector, bvec.bv_len);\n\t\tif (rc) {\n\t\t\tbio->bi_status = rc;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (do_acct)\n\t\tbio_end_io_acct(bio, start);\n\n\tif (bio->bi_opf & REQ_FUA)\n\t\tret = nvdimm_flush(nd_region, bio);\n\n\tif (ret)\n\t\tbio->bi_status = errno_to_blk_status(ret);\n\n\tbio_endio(bio);\n}\n\n \n__weak long __pmem_direct_access(struct pmem_device *pmem, pgoff_t pgoff,\n\t\tlong nr_pages, enum dax_access_mode mode, void **kaddr,\n\t\tpfn_t *pfn)\n{\n\tresource_size_t offset = PFN_PHYS(pgoff) + pmem->data_offset;\n\tsector_t sector = PFN_PHYS(pgoff) >> SECTOR_SHIFT;\n\tunsigned int num = PFN_PHYS(nr_pages) >> SECTOR_SHIFT;\n\tstruct badblocks *bb = &pmem->bb;\n\tsector_t first_bad;\n\tint num_bad;\n\n\tif (kaddr)\n\t\t*kaddr = pmem->virt_addr + offset;\n\tif (pfn)\n\t\t*pfn = phys_to_pfn_t(pmem->phys_addr + offset, pmem->pfn_flags);\n\n\tif (bb->count &&\n\t    badblocks_check(bb, sector, num, &first_bad, &num_bad)) {\n\t\tlong actual_nr;\n\n\t\tif (mode != DAX_RECOVERY_WRITE)\n\t\t\treturn -EHWPOISON;\n\n\t\t \n\t\tactual_nr = PHYS_PFN(\n\t\t\tPAGE_ALIGN((first_bad - sector) << SECTOR_SHIFT));\n\t\tdev_dbg(pmem->bb.dev, \"start sector(%llu), nr_pages(%ld), first_bad(%llu), actual_nr(%ld)\\n\",\n\t\t\t\tsector, nr_pages, first_bad, actual_nr);\n\t\tif (actual_nr)\n\t\t\treturn actual_nr;\n\t\treturn 1;\n\t}\n\n\t \n\tif (bb->count)\n\t\treturn nr_pages;\n\treturn PHYS_PFN(pmem->size - pmem->pfn_pad - offset);\n}\n\nstatic const struct block_device_operations pmem_fops = {\n\t.owner =\t\tTHIS_MODULE,\n\t.submit_bio =\t\tpmem_submit_bio,\n};\n\nstatic int pmem_dax_zero_page_range(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\t\t\t    size_t nr_pages)\n{\n\tstruct pmem_device *pmem = dax_get_private(dax_dev);\n\n\treturn blk_status_to_errno(pmem_do_write(pmem, ZERO_PAGE(0), 0,\n\t\t\t\t   PFN_PHYS(pgoff) >> SECTOR_SHIFT,\n\t\t\t\t   PAGE_SIZE));\n}\n\nstatic long pmem_dax_direct_access(struct dax_device *dax_dev,\n\t\tpgoff_t pgoff, long nr_pages, enum dax_access_mode mode,\n\t\tvoid **kaddr, pfn_t *pfn)\n{\n\tstruct pmem_device *pmem = dax_get_private(dax_dev);\n\n\treturn __pmem_direct_access(pmem, pgoff, nr_pages, mode, kaddr, pfn);\n}\n\n \nstatic size_t pmem_recovery_write(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tvoid *addr, size_t bytes, struct iov_iter *i)\n{\n\tstruct pmem_device *pmem = dax_get_private(dax_dev);\n\tsize_t olen, len, off;\n\tphys_addr_t pmem_off;\n\tstruct device *dev = pmem->bb.dev;\n\tlong cleared;\n\n\toff = offset_in_page(addr);\n\tlen = PFN_PHYS(PFN_UP(off + bytes));\n\tif (!is_bad_pmem(&pmem->bb, PFN_PHYS(pgoff) >> SECTOR_SHIFT, len))\n\t\treturn _copy_from_iter_flushcache(addr, bytes, i);\n\n\t \n\tif (off || !PAGE_ALIGNED(bytes)) {\n\t\tdev_dbg(dev, \"Found poison, but addr(%p) or bytes(%#zx) not page aligned\\n\",\n\t\t\taddr, bytes);\n\t\treturn 0;\n\t}\n\n\tpmem_off = PFN_PHYS(pgoff) + pmem->data_offset;\n\tcleared = __pmem_clear_poison(pmem, pmem_off, len);\n\tif (cleared > 0 && cleared < len) {\n\t\tdev_dbg(dev, \"poison cleared only %ld out of %zu bytes\\n\",\n\t\t\tcleared, len);\n\t\treturn 0;\n\t}\n\tif (cleared < 0) {\n\t\tdev_dbg(dev, \"poison clear failed: %ld\\n\", cleared);\n\t\treturn 0;\n\t}\n\n\tolen = _copy_from_iter_flushcache(addr, bytes, i);\n\tpmem_clear_bb(pmem, to_sect(pmem, pmem_off), cleared >> SECTOR_SHIFT);\n\n\treturn olen;\n}\n\nstatic const struct dax_operations pmem_dax_ops = {\n\t.direct_access = pmem_dax_direct_access,\n\t.zero_page_range = pmem_dax_zero_page_range,\n\t.recovery_write = pmem_recovery_write,\n};\n\nstatic ssize_t write_cache_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct pmem_device *pmem = dev_to_disk(dev)->private_data;\n\n\treturn sprintf(buf, \"%d\\n\", !!dax_write_cache_enabled(pmem->dax_dev));\n}\n\nstatic ssize_t write_cache_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t len)\n{\n\tstruct pmem_device *pmem = dev_to_disk(dev)->private_data;\n\tbool write_cache;\n\tint rc;\n\n\trc = kstrtobool(buf, &write_cache);\n\tif (rc)\n\t\treturn rc;\n\tdax_write_cache(pmem->dax_dev, write_cache);\n\treturn len;\n}\nstatic DEVICE_ATTR_RW(write_cache);\n\nstatic umode_t dax_visible(struct kobject *kobj, struct attribute *a, int n)\n{\n#ifndef CONFIG_ARCH_HAS_PMEM_API\n\tif (a == &dev_attr_write_cache.attr)\n\t\treturn 0;\n#endif\n\treturn a->mode;\n}\n\nstatic struct attribute *dax_attributes[] = {\n\t&dev_attr_write_cache.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group dax_attribute_group = {\n\t.name\t\t= \"dax\",\n\t.attrs\t\t= dax_attributes,\n\t.is_visible\t= dax_visible,\n};\n\nstatic const struct attribute_group *pmem_attribute_groups[] = {\n\t&dax_attribute_group,\n\tNULL,\n};\n\nstatic void pmem_release_disk(void *__pmem)\n{\n\tstruct pmem_device *pmem = __pmem;\n\n\tdax_remove_host(pmem->disk);\n\tkill_dax(pmem->dax_dev);\n\tput_dax(pmem->dax_dev);\n\tdel_gendisk(pmem->disk);\n\n\tput_disk(pmem->disk);\n}\n\nstatic int pmem_pagemap_memory_failure(struct dev_pagemap *pgmap,\n\t\tunsigned long pfn, unsigned long nr_pages, int mf_flags)\n{\n\tstruct pmem_device *pmem =\n\t\t\tcontainer_of(pgmap, struct pmem_device, pgmap);\n\tu64 offset = PFN_PHYS(pfn) - pmem->phys_addr - pmem->data_offset;\n\tu64 len = nr_pages << PAGE_SHIFT;\n\n\treturn dax_holder_notify_failure(pmem->dax_dev, offset, len, mf_flags);\n}\n\nstatic const struct dev_pagemap_ops fsdax_pagemap_ops = {\n\t.memory_failure\t\t= pmem_pagemap_memory_failure,\n};\n\nstatic int pmem_attach_disk(struct device *dev,\n\t\tstruct nd_namespace_common *ndns)\n{\n\tstruct nd_namespace_io *nsio = to_nd_namespace_io(&ndns->dev);\n\tstruct nd_region *nd_region = to_nd_region(dev->parent);\n\tint nid = dev_to_node(dev), fua;\n\tstruct resource *res = &nsio->res;\n\tstruct range bb_range;\n\tstruct nd_pfn *nd_pfn = NULL;\n\tstruct dax_device *dax_dev;\n\tstruct nd_pfn_sb *pfn_sb;\n\tstruct pmem_device *pmem;\n\tstruct request_queue *q;\n\tstruct gendisk *disk;\n\tvoid *addr;\n\tint rc;\n\n\tpmem = devm_kzalloc(dev, sizeof(*pmem), GFP_KERNEL);\n\tif (!pmem)\n\t\treturn -ENOMEM;\n\n\trc = devm_namespace_enable(dev, ndns, nd_info_block_reserve());\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (is_nd_pfn(dev)) {\n\t\tnd_pfn = to_nd_pfn(dev);\n\t\trc = nvdimm_setup_pfn(nd_pfn, &pmem->pgmap);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\t \n\tdevm_namespace_disable(dev, ndns);\n\n\tdev_set_drvdata(dev, pmem);\n\tpmem->phys_addr = res->start;\n\tpmem->size = resource_size(res);\n\tfua = nvdimm_has_flush(nd_region);\n\tif (!IS_ENABLED(CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE) || fua < 0) {\n\t\tdev_warn(dev, \"unable to guarantee persistence of writes\\n\");\n\t\tfua = 0;\n\t}\n\n\tif (!devm_request_mem_region(dev, res->start, resource_size(res),\n\t\t\t\tdev_name(&ndns->dev))) {\n\t\tdev_warn(dev, \"could not reserve region %pR\\n\", res);\n\t\treturn -EBUSY;\n\t}\n\n\tdisk = blk_alloc_disk(nid);\n\tif (!disk)\n\t\treturn -ENOMEM;\n\tq = disk->queue;\n\n\tpmem->disk = disk;\n\tpmem->pgmap.owner = pmem;\n\tpmem->pfn_flags = PFN_DEV;\n\tif (is_nd_pfn(dev)) {\n\t\tpmem->pgmap.type = MEMORY_DEVICE_FS_DAX;\n\t\tpmem->pgmap.ops = &fsdax_pagemap_ops;\n\t\taddr = devm_memremap_pages(dev, &pmem->pgmap);\n\t\tpfn_sb = nd_pfn->pfn_sb;\n\t\tpmem->data_offset = le64_to_cpu(pfn_sb->dataoff);\n\t\tpmem->pfn_pad = resource_size(res) -\n\t\t\trange_len(&pmem->pgmap.range);\n\t\tpmem->pfn_flags |= PFN_MAP;\n\t\tbb_range = pmem->pgmap.range;\n\t\tbb_range.start += pmem->data_offset;\n\t} else if (pmem_should_map_pages(dev)) {\n\t\tpmem->pgmap.range.start = res->start;\n\t\tpmem->pgmap.range.end = res->end;\n\t\tpmem->pgmap.nr_range = 1;\n\t\tpmem->pgmap.type = MEMORY_DEVICE_FS_DAX;\n\t\tpmem->pgmap.ops = &fsdax_pagemap_ops;\n\t\taddr = devm_memremap_pages(dev, &pmem->pgmap);\n\t\tpmem->pfn_flags |= PFN_MAP;\n\t\tbb_range = pmem->pgmap.range;\n\t} else {\n\t\taddr = devm_memremap(dev, pmem->phys_addr,\n\t\t\t\tpmem->size, ARCH_MEMREMAP_PMEM);\n\t\tbb_range.start =  res->start;\n\t\tbb_range.end = res->end;\n\t}\n\n\tif (IS_ERR(addr)) {\n\t\trc = PTR_ERR(addr);\n\t\tgoto out;\n\t}\n\tpmem->virt_addr = addr;\n\n\tblk_queue_write_cache(q, true, fua);\n\tblk_queue_physical_block_size(q, PAGE_SIZE);\n\tblk_queue_logical_block_size(q, pmem_sector_size(ndns));\n\tblk_queue_max_hw_sectors(q, UINT_MAX);\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, q);\n\tblk_queue_flag_set(QUEUE_FLAG_SYNCHRONOUS, q);\n\tif (pmem->pfn_flags & PFN_MAP)\n\t\tblk_queue_flag_set(QUEUE_FLAG_DAX, q);\n\n\tdisk->fops\t\t= &pmem_fops;\n\tdisk->private_data\t= pmem;\n\tnvdimm_namespace_disk_name(ndns, disk->disk_name);\n\tset_capacity(disk, (pmem->size - pmem->pfn_pad - pmem->data_offset)\n\t\t\t/ 512);\n\tif (devm_init_badblocks(dev, &pmem->bb))\n\t\treturn -ENOMEM;\n\tnvdimm_badblocks_populate(nd_region, &pmem->bb, &bb_range);\n\tdisk->bb = &pmem->bb;\n\n\tdax_dev = alloc_dax(pmem, &pmem_dax_ops);\n\tif (IS_ERR(dax_dev)) {\n\t\trc = PTR_ERR(dax_dev);\n\t\tgoto out;\n\t}\n\tset_dax_nocache(dax_dev);\n\tset_dax_nomc(dax_dev);\n\tif (is_nvdimm_sync(nd_region))\n\t\tset_dax_synchronous(dax_dev);\n\trc = dax_add_host(dax_dev, disk);\n\tif (rc)\n\t\tgoto out_cleanup_dax;\n\tdax_write_cache(dax_dev, nvdimm_has_cache(nd_region));\n\tpmem->dax_dev = dax_dev;\n\n\trc = device_add_disk(dev, disk, pmem_attribute_groups);\n\tif (rc)\n\t\tgoto out_remove_host;\n\tif (devm_add_action_or_reset(dev, pmem_release_disk, pmem))\n\t\treturn -ENOMEM;\n\n\tnvdimm_check_and_set_ro(disk);\n\n\tpmem->bb_state = sysfs_get_dirent(disk_to_dev(disk)->kobj.sd,\n\t\t\t\t\t  \"badblocks\");\n\tif (!pmem->bb_state)\n\t\tdev_warn(dev, \"'badblocks' notification disabled\\n\");\n\treturn 0;\n\nout_remove_host:\n\tdax_remove_host(pmem->disk);\nout_cleanup_dax:\n\tkill_dax(pmem->dax_dev);\n\tput_dax(pmem->dax_dev);\nout:\n\tput_disk(pmem->disk);\n\treturn rc;\n}\n\nstatic int nd_pmem_probe(struct device *dev)\n{\n\tint ret;\n\tstruct nd_namespace_common *ndns;\n\n\tndns = nvdimm_namespace_common_probe(dev);\n\tif (IS_ERR(ndns))\n\t\treturn PTR_ERR(ndns);\n\n\tif (is_nd_btt(dev))\n\t\treturn nvdimm_namespace_attach_btt(ndns);\n\n\tif (is_nd_pfn(dev))\n\t\treturn pmem_attach_disk(dev, ndns);\n\n\tret = devm_namespace_enable(dev, ndns, nd_info_block_reserve());\n\tif (ret)\n\t\treturn ret;\n\n\tret = nd_btt_probe(dev, ndns);\n\tif (ret == 0)\n\t\treturn -ENXIO;\n\n\t \n\tret = nd_pfn_probe(dev, ndns);\n\tif (ret == 0)\n\t\treturn -ENXIO;\n\telse if (ret == -EOPNOTSUPP)\n\t\treturn ret;\n\n\tret = nd_dax_probe(dev, ndns);\n\tif (ret == 0)\n\t\treturn -ENXIO;\n\telse if (ret == -EOPNOTSUPP)\n\t\treturn ret;\n\n\t \n\tdevm_namespace_disable(dev, ndns);\n\n\treturn pmem_attach_disk(dev, ndns);\n}\n\nstatic void nd_pmem_remove(struct device *dev)\n{\n\tstruct pmem_device *pmem = dev_get_drvdata(dev);\n\n\tif (is_nd_btt(dev))\n\t\tnvdimm_namespace_detach_btt(to_nd_btt(dev));\n\telse {\n\t\t \n\t\tsysfs_put(pmem->bb_state);\n\t\tpmem->bb_state = NULL;\n\t}\n\tnvdimm_flush(to_nd_region(dev->parent), NULL);\n}\n\nstatic void nd_pmem_shutdown(struct device *dev)\n{\n\tnvdimm_flush(to_nd_region(dev->parent), NULL);\n}\n\nstatic void pmem_revalidate_poison(struct device *dev)\n{\n\tstruct nd_region *nd_region;\n\tresource_size_t offset = 0, end_trunc = 0;\n\tstruct nd_namespace_common *ndns;\n\tstruct nd_namespace_io *nsio;\n\tstruct badblocks *bb;\n\tstruct range range;\n\tstruct kernfs_node *bb_state;\n\n\tif (is_nd_btt(dev)) {\n\t\tstruct nd_btt *nd_btt = to_nd_btt(dev);\n\n\t\tndns = nd_btt->ndns;\n\t\tnd_region = to_nd_region(ndns->dev.parent);\n\t\tnsio = to_nd_namespace_io(&ndns->dev);\n\t\tbb = &nsio->bb;\n\t\tbb_state = NULL;\n\t} else {\n\t\tstruct pmem_device *pmem = dev_get_drvdata(dev);\n\n\t\tnd_region = to_region(pmem);\n\t\tbb = &pmem->bb;\n\t\tbb_state = pmem->bb_state;\n\n\t\tif (is_nd_pfn(dev)) {\n\t\t\tstruct nd_pfn *nd_pfn = to_nd_pfn(dev);\n\t\t\tstruct nd_pfn_sb *pfn_sb = nd_pfn->pfn_sb;\n\n\t\t\tndns = nd_pfn->ndns;\n\t\t\toffset = pmem->data_offset +\n\t\t\t\t\t__le32_to_cpu(pfn_sb->start_pad);\n\t\t\tend_trunc = __le32_to_cpu(pfn_sb->end_trunc);\n\t\t} else {\n\t\t\tndns = to_ndns(dev);\n\t\t}\n\n\t\tnsio = to_nd_namespace_io(&ndns->dev);\n\t}\n\n\trange.start = nsio->res.start + offset;\n\trange.end = nsio->res.end - end_trunc;\n\tnvdimm_badblocks_populate(nd_region, bb, &range);\n\tif (bb_state)\n\t\tsysfs_notify_dirent(bb_state);\n}\n\nstatic void pmem_revalidate_region(struct device *dev)\n{\n\tstruct pmem_device *pmem;\n\n\tif (is_nd_btt(dev)) {\n\t\tstruct nd_btt *nd_btt = to_nd_btt(dev);\n\t\tstruct btt *btt = nd_btt->btt;\n\n\t\tnvdimm_check_and_set_ro(btt->btt_disk);\n\t\treturn;\n\t}\n\n\tpmem = dev_get_drvdata(dev);\n\tnvdimm_check_and_set_ro(pmem->disk);\n}\n\nstatic void nd_pmem_notify(struct device *dev, enum nvdimm_event event)\n{\n\tswitch (event) {\n\tcase NVDIMM_REVALIDATE_POISON:\n\t\tpmem_revalidate_poison(dev);\n\t\tbreak;\n\tcase NVDIMM_REVALIDATE_REGION:\n\t\tpmem_revalidate_region(dev);\n\t\tbreak;\n\tdefault:\n\t\tdev_WARN_ONCE(dev, 1, \"notify: unknown event: %d\\n\", event);\n\t\tbreak;\n\t}\n}\n\nMODULE_ALIAS(\"pmem\");\nMODULE_ALIAS_ND_DEVICE(ND_DEVICE_NAMESPACE_IO);\nMODULE_ALIAS_ND_DEVICE(ND_DEVICE_NAMESPACE_PMEM);\nstatic struct nd_device_driver nd_pmem_driver = {\n\t.probe = nd_pmem_probe,\n\t.remove = nd_pmem_remove,\n\t.notify = nd_pmem_notify,\n\t.shutdown = nd_pmem_shutdown,\n\t.drv = {\n\t\t.name = \"nd_pmem\",\n\t},\n\t.type = ND_DRIVER_NAMESPACE_IO | ND_DRIVER_NAMESPACE_PMEM,\n};\n\nmodule_nd_driver(nd_pmem_driver);\n\nMODULE_AUTHOR(\"Ross Zwisler <ross.zwisler@linux.intel.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}