{
  "module_name": "arm-smmu.c",
  "hash_id": "9ac5397eafa3de17b25b295541e9c426f544656f46edf9cdd463803cfdfb48e9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/arm/arm-smmu/arm-smmu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"arm-smmu: \" fmt\n\n#include <linux/acpi.h>\n#include <linux/acpi_iort.h>\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n\n#include <linux/fsl/mc.h>\n\n#include \"arm-smmu.h\"\n#include \"../../dma-iommu.h\"\n\n \n#define QCOM_DUMMY_VAL -1\n\n#define MSI_IOVA_BASE\t\t\t0x8000000\n#define MSI_IOVA_LENGTH\t\t\t0x100000\n\nstatic int force_stage;\nmodule_param(force_stage, int, S_IRUGO);\nMODULE_PARM_DESC(force_stage,\n\t\"Force SMMU mappings to be installed at a particular stage of translation. A value of '1' or '2' forces the corresponding stage. All other values are ignored (i.e. no stage is forced). Note that selecting a specific stage will disable support for nested translation.\");\nstatic bool disable_bypass =\n\tIS_ENABLED(CONFIG_ARM_SMMU_DISABLE_BYPASS_BY_DEFAULT);\nmodule_param(disable_bypass, bool, S_IRUGO);\nMODULE_PARM_DESC(disable_bypass,\n\t\"Disable bypass streams such that incoming transactions from devices that are not attached to an iommu domain will report an abort back to the device and will not be allowed to pass through the SMMU.\");\n\n#define s2cr_init_val (struct arm_smmu_s2cr){\t\t\t\t\\\n\t.type = disable_bypass ? S2CR_TYPE_FAULT : S2CR_TYPE_BYPASS,\t\\\n}\n\nstatic bool using_legacy_binding, using_generic_binding;\n\nstatic inline int arm_smmu_rpm_get(struct arm_smmu_device *smmu)\n{\n\tif (pm_runtime_enabled(smmu->dev))\n\t\treturn pm_runtime_resume_and_get(smmu->dev);\n\n\treturn 0;\n}\n\nstatic inline void arm_smmu_rpm_put(struct arm_smmu_device *smmu)\n{\n\tif (pm_runtime_enabled(smmu->dev))\n\t\tpm_runtime_put_autosuspend(smmu->dev);\n}\n\nstatic struct arm_smmu_domain *to_smmu_domain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct arm_smmu_domain, domain);\n}\n\nstatic struct platform_driver arm_smmu_driver;\nstatic struct iommu_ops arm_smmu_ops;\n\n#ifdef CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS\nstatic struct device_node *dev_get_dev_node(struct device *dev)\n{\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_bus *bus = to_pci_dev(dev)->bus;\n\n\t\twhile (!pci_is_root_bus(bus))\n\t\t\tbus = bus->parent;\n\t\treturn of_node_get(bus->bridge->parent->of_node);\n\t}\n\n\treturn of_node_get(dev->of_node);\n}\n\nstatic int __arm_smmu_get_pci_sid(struct pci_dev *pdev, u16 alias, void *data)\n{\n\t*((__be32 *)data) = cpu_to_be32(alias);\n\treturn 0;  \n}\n\nstatic int __find_legacy_master_phandle(struct device *dev, void *data)\n{\n\tstruct of_phandle_iterator *it = *(void **)data;\n\tstruct device_node *np = it->node;\n\tint err;\n\n\tof_for_each_phandle(it, err, dev->of_node, \"mmu-masters\",\n\t\t\t    \"#stream-id-cells\", -1)\n\t\tif (it->node == np) {\n\t\t\t*(void **)data = dev;\n\t\t\treturn 1;\n\t\t}\n\tit->node = np;\n\treturn err == -ENOENT ? 0 : err;\n}\n\nstatic int arm_smmu_register_legacy_master(struct device *dev,\n\t\t\t\t\t   struct arm_smmu_device **smmu)\n{\n\tstruct device *smmu_dev;\n\tstruct device_node *np;\n\tstruct of_phandle_iterator it;\n\tvoid *data = &it;\n\tu32 *sids;\n\t__be32 pci_sid;\n\tint err;\n\n\tnp = dev_get_dev_node(dev);\n\tif (!np || !of_property_present(np, \"#stream-id-cells\")) {\n\t\tof_node_put(np);\n\t\treturn -ENODEV;\n\t}\n\n\tit.node = np;\n\terr = driver_for_each_device(&arm_smmu_driver.driver, NULL, &data,\n\t\t\t\t     __find_legacy_master_phandle);\n\tsmmu_dev = data;\n\tof_node_put(np);\n\tif (err == 0)\n\t\treturn -ENODEV;\n\tif (err < 0)\n\t\treturn err;\n\n\tif (dev_is_pci(dev)) {\n\t\t \n\t\tpci_for_each_dma_alias(to_pci_dev(dev), __arm_smmu_get_pci_sid,\n\t\t\t\t       &pci_sid);\n\t\tit.cur = &pci_sid;\n\t\tit.cur_count = 1;\n\t}\n\n\terr = iommu_fwspec_init(dev, &smmu_dev->of_node->fwnode,\n\t\t\t\t&arm_smmu_ops);\n\tif (err)\n\t\treturn err;\n\n\tsids = kcalloc(it.cur_count, sizeof(*sids), GFP_KERNEL);\n\tif (!sids)\n\t\treturn -ENOMEM;\n\n\t*smmu = dev_get_drvdata(smmu_dev);\n\tof_phandle_iterator_args(&it, sids, it.cur_count);\n\terr = iommu_fwspec_add_ids(dev, sids, it.cur_count);\n\tkfree(sids);\n\treturn err;\n}\n#else\nstatic int arm_smmu_register_legacy_master(struct device *dev,\n\t\t\t\t\t   struct arm_smmu_device **smmu)\n{\n\treturn -ENODEV;\n}\n#endif  \n\nstatic void __arm_smmu_free_bitmap(unsigned long *map, int idx)\n{\n\tclear_bit(idx, map);\n}\n\n \nstatic void __arm_smmu_tlb_sync(struct arm_smmu_device *smmu, int page,\n\t\t\t\tint sync, int status)\n{\n\tunsigned int spin_cnt, delay;\n\tu32 reg;\n\n\tif (smmu->impl && unlikely(smmu->impl->tlb_sync))\n\t\treturn smmu->impl->tlb_sync(smmu, page, sync, status);\n\n\tarm_smmu_writel(smmu, page, sync, QCOM_DUMMY_VAL);\n\tfor (delay = 1; delay < TLB_LOOP_TIMEOUT; delay *= 2) {\n\t\tfor (spin_cnt = TLB_SPIN_COUNT; spin_cnt > 0; spin_cnt--) {\n\t\t\treg = arm_smmu_readl(smmu, page, status);\n\t\t\tif (!(reg & ARM_SMMU_sTLBGSTATUS_GSACTIVE))\n\t\t\t\treturn;\n\t\t\tcpu_relax();\n\t\t}\n\t\tudelay(delay);\n\t}\n\tdev_err_ratelimited(smmu->dev,\n\t\t\t    \"TLB sync timed out -- SMMU may be deadlocked\\n\");\n}\n\nstatic void arm_smmu_tlb_sync_global(struct arm_smmu_device *smmu)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&smmu->global_sync_lock, flags);\n\t__arm_smmu_tlb_sync(smmu, ARM_SMMU_GR0, ARM_SMMU_GR0_sTLBGSYNC,\n\t\t\t    ARM_SMMU_GR0_sTLBGSTATUS);\n\tspin_unlock_irqrestore(&smmu->global_sync_lock, flags);\n}\n\nstatic void arm_smmu_tlb_sync_context(struct arm_smmu_domain *smmu_domain)\n{\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&smmu_domain->cb_lock, flags);\n\t__arm_smmu_tlb_sync(smmu, ARM_SMMU_CB(smmu, smmu_domain->cfg.cbndx),\n\t\t\t    ARM_SMMU_CB_TLBSYNC, ARM_SMMU_CB_TLBSTATUS);\n\tspin_unlock_irqrestore(&smmu_domain->cb_lock, flags);\n}\n\nstatic void arm_smmu_tlb_inv_context_s1(void *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\t \n\twmb();\n\tarm_smmu_cb_write(smmu_domain->smmu, smmu_domain->cfg.cbndx,\n\t\t\t  ARM_SMMU_CB_S1_TLBIASID, smmu_domain->cfg.asid);\n\tarm_smmu_tlb_sync_context(smmu_domain);\n}\n\nstatic void arm_smmu_tlb_inv_context_s2(void *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\t \n\twmb();\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIVMID, smmu_domain->cfg.vmid);\n\tarm_smmu_tlb_sync_global(smmu);\n}\n\nstatic void arm_smmu_tlb_inv_range_s1(unsigned long iova, size_t size,\n\t\t\t\t      size_t granule, void *cookie, int reg)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\tint idx = cfg->cbndx;\n\n\tif (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)\n\t\twmb();\n\n\tif (cfg->fmt != ARM_SMMU_CTX_FMT_AARCH64) {\n\t\tiova = (iova >> 12) << 12;\n\t\tiova |= cfg->asid;\n\t\tdo {\n\t\t\tarm_smmu_cb_write(smmu, idx, reg, iova);\n\t\t\tiova += granule;\n\t\t} while (size -= granule);\n\t} else {\n\t\tiova >>= 12;\n\t\tiova |= (u64)cfg->asid << 48;\n\t\tdo {\n\t\t\tarm_smmu_cb_writeq(smmu, idx, reg, iova);\n\t\t\tiova += granule >> 12;\n\t\t} while (size -= granule);\n\t}\n}\n\nstatic void arm_smmu_tlb_inv_range_s2(unsigned long iova, size_t size,\n\t\t\t\t      size_t granule, void *cookie, int reg)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tint idx = smmu_domain->cfg.cbndx;\n\n\tif (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)\n\t\twmb();\n\n\tiova >>= 12;\n\tdo {\n\t\tif (smmu_domain->cfg.fmt == ARM_SMMU_CTX_FMT_AARCH64)\n\t\t\tarm_smmu_cb_writeq(smmu, idx, reg, iova);\n\t\telse\n\t\t\tarm_smmu_cb_write(smmu, idx, reg, iova);\n\t\tiova += granule >> 12;\n\t} while (size -= granule);\n}\n\nstatic void arm_smmu_tlb_inv_walk_s1(unsigned long iova, size_t size,\n\t\t\t\t     size_t granule, void *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\n\tif (cfg->flush_walk_prefer_tlbiasid) {\n\t\tarm_smmu_tlb_inv_context_s1(cookie);\n\t} else {\n\t\tarm_smmu_tlb_inv_range_s1(iova, size, granule, cookie,\n\t\t\t\t\t  ARM_SMMU_CB_S1_TLBIVA);\n\t\tarm_smmu_tlb_sync_context(cookie);\n\t}\n}\n\nstatic void arm_smmu_tlb_add_page_s1(struct iommu_iotlb_gather *gather,\n\t\t\t\t     unsigned long iova, size_t granule,\n\t\t\t\t     void *cookie)\n{\n\tarm_smmu_tlb_inv_range_s1(iova, granule, granule, cookie,\n\t\t\t\t  ARM_SMMU_CB_S1_TLBIVAL);\n}\n\nstatic void arm_smmu_tlb_inv_walk_s2(unsigned long iova, size_t size,\n\t\t\t\t     size_t granule, void *cookie)\n{\n\tarm_smmu_tlb_inv_range_s2(iova, size, granule, cookie,\n\t\t\t\t  ARM_SMMU_CB_S2_TLBIIPAS2);\n\tarm_smmu_tlb_sync_context(cookie);\n}\n\nstatic void arm_smmu_tlb_add_page_s2(struct iommu_iotlb_gather *gather,\n\t\t\t\t     unsigned long iova, size_t granule,\n\t\t\t\t     void *cookie)\n{\n\tarm_smmu_tlb_inv_range_s2(iova, granule, granule, cookie,\n\t\t\t\t  ARM_SMMU_CB_S2_TLBIIPAS2L);\n}\n\nstatic void arm_smmu_tlb_inv_walk_s2_v1(unsigned long iova, size_t size,\n\t\t\t\t\tsize_t granule, void *cookie)\n{\n\tarm_smmu_tlb_inv_context_s2(cookie);\n}\n \nstatic void arm_smmu_tlb_add_page_s2_v1(struct iommu_iotlb_gather *gather,\n\t\t\t\t\tunsigned long iova, size_t granule,\n\t\t\t\t\tvoid *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\tif (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)\n\t\twmb();\n\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIVMID, smmu_domain->cfg.vmid);\n}\n\nstatic const struct iommu_flush_ops arm_smmu_s1_tlb_ops = {\n\t.tlb_flush_all\t= arm_smmu_tlb_inv_context_s1,\n\t.tlb_flush_walk\t= arm_smmu_tlb_inv_walk_s1,\n\t.tlb_add_page\t= arm_smmu_tlb_add_page_s1,\n};\n\nstatic const struct iommu_flush_ops arm_smmu_s2_tlb_ops_v2 = {\n\t.tlb_flush_all\t= arm_smmu_tlb_inv_context_s2,\n\t.tlb_flush_walk\t= arm_smmu_tlb_inv_walk_s2,\n\t.tlb_add_page\t= arm_smmu_tlb_add_page_s2,\n};\n\nstatic const struct iommu_flush_ops arm_smmu_s2_tlb_ops_v1 = {\n\t.tlb_flush_all\t= arm_smmu_tlb_inv_context_s2,\n\t.tlb_flush_walk\t= arm_smmu_tlb_inv_walk_s2_v1,\n\t.tlb_add_page\t= arm_smmu_tlb_add_page_s2_v1,\n};\n\nstatic irqreturn_t arm_smmu_context_fault(int irq, void *dev)\n{\n\tu32 fsr, fsynr, cbfrsynra;\n\tunsigned long iova;\n\tstruct iommu_domain *domain = dev;\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tint idx = smmu_domain->cfg.cbndx;\n\tint ret;\n\n\tfsr = arm_smmu_cb_read(smmu, idx, ARM_SMMU_CB_FSR);\n\tif (!(fsr & ARM_SMMU_FSR_FAULT))\n\t\treturn IRQ_NONE;\n\n\tfsynr = arm_smmu_cb_read(smmu, idx, ARM_SMMU_CB_FSYNR0);\n\tiova = arm_smmu_cb_readq(smmu, idx, ARM_SMMU_CB_FAR);\n\tcbfrsynra = arm_smmu_gr1_read(smmu, ARM_SMMU_GR1_CBFRSYNRA(idx));\n\n\tret = report_iommu_fault(domain, NULL, iova,\n\t\tfsynr & ARM_SMMU_FSYNR0_WNR ? IOMMU_FAULT_WRITE : IOMMU_FAULT_READ);\n\n\tif (ret == -ENOSYS)\n\t\tdev_err_ratelimited(smmu->dev,\n\t\t\"Unhandled context fault: fsr=0x%x, iova=0x%08lx, fsynr=0x%x, cbfrsynra=0x%x, cb=%d\\n\",\n\t\t\t    fsr, iova, fsynr, cbfrsynra, idx);\n\n\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_FSR, fsr);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t arm_smmu_global_fault(int irq, void *dev)\n{\n\tu32 gfsr, gfsynr0, gfsynr1, gfsynr2;\n\tstruct arm_smmu_device *smmu = dev;\n\tstatic DEFINE_RATELIMIT_STATE(rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tgfsr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sGFSR);\n\tgfsynr0 = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sGFSYNR0);\n\tgfsynr1 = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sGFSYNR1);\n\tgfsynr2 = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sGFSYNR2);\n\n\tif (!gfsr)\n\t\treturn IRQ_NONE;\n\n\tif (__ratelimit(&rs)) {\n\t\tif (IS_ENABLED(CONFIG_ARM_SMMU_DISABLE_BYPASS_BY_DEFAULT) &&\n\t\t    (gfsr & ARM_SMMU_sGFSR_USF))\n\t\t\tdev_err(smmu->dev,\n\t\t\t\t\"Blocked unknown Stream ID 0x%hx; boot with \\\"arm-smmu.disable_bypass=0\\\" to allow, but this may have security implications\\n\",\n\t\t\t\t(u16)gfsynr1);\n\t\telse\n\t\t\tdev_err(smmu->dev,\n\t\t\t\t\"Unexpected global fault, this could be serious\\n\");\n\t\tdev_err(smmu->dev,\n\t\t\t\"\\tGFSR 0x%08x, GFSYNR0 0x%08x, GFSYNR1 0x%08x, GFSYNR2 0x%08x\\n\",\n\t\t\tgfsr, gfsynr0, gfsynr1, gfsynr2);\n\t}\n\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sGFSR, gfsr);\n\treturn IRQ_HANDLED;\n}\n\nstatic void arm_smmu_init_context_bank(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t       struct io_pgtable_cfg *pgtbl_cfg)\n{\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\tstruct arm_smmu_cb *cb = &smmu_domain->smmu->cbs[cfg->cbndx];\n\tbool stage1 = cfg->cbar != CBAR_TYPE_S2_TRANS;\n\n\tcb->cfg = cfg;\n\n\t \n\tif (stage1) {\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_S) {\n\t\t\tcb->tcr[0] = pgtbl_cfg->arm_v7s_cfg.tcr;\n\t\t} else {\n\t\t\tcb->tcr[0] = arm_smmu_lpae_tcr(pgtbl_cfg);\n\t\t\tcb->tcr[1] = arm_smmu_lpae_tcr2(pgtbl_cfg);\n\t\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH64)\n\t\t\t\tcb->tcr[1] |= ARM_SMMU_TCR2_AS;\n\t\t\telse\n\t\t\t\tcb->tcr[0] |= ARM_SMMU_TCR_EAE;\n\t\t}\n\t} else {\n\t\tcb->tcr[0] = arm_smmu_lpae_vtcr(pgtbl_cfg);\n\t}\n\n\t \n\tif (stage1) {\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_S) {\n\t\t\tcb->ttbr[0] = pgtbl_cfg->arm_v7s_cfg.ttbr;\n\t\t\tcb->ttbr[1] = 0;\n\t\t} else {\n\t\t\tcb->ttbr[0] = FIELD_PREP(ARM_SMMU_TTBRn_ASID,\n\t\t\t\t\t\t cfg->asid);\n\t\t\tcb->ttbr[1] = FIELD_PREP(ARM_SMMU_TTBRn_ASID,\n\t\t\t\t\t\t cfg->asid);\n\n\t\t\tif (pgtbl_cfg->quirks & IO_PGTABLE_QUIRK_ARM_TTBR1)\n\t\t\t\tcb->ttbr[1] |= pgtbl_cfg->arm_lpae_s1_cfg.ttbr;\n\t\t\telse\n\t\t\t\tcb->ttbr[0] |= pgtbl_cfg->arm_lpae_s1_cfg.ttbr;\n\t\t}\n\t} else {\n\t\tcb->ttbr[0] = pgtbl_cfg->arm_lpae_s2_cfg.vttbr;\n\t}\n\n\t \n\tif (stage1) {\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_S) {\n\t\t\tcb->mair[0] = pgtbl_cfg->arm_v7s_cfg.prrr;\n\t\t\tcb->mair[1] = pgtbl_cfg->arm_v7s_cfg.nmrr;\n\t\t} else {\n\t\t\tcb->mair[0] = pgtbl_cfg->arm_lpae_s1_cfg.mair;\n\t\t\tcb->mair[1] = pgtbl_cfg->arm_lpae_s1_cfg.mair >> 32;\n\t\t}\n\t}\n}\n\nvoid arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)\n{\n\tu32 reg;\n\tbool stage1;\n\tstruct arm_smmu_cb *cb = &smmu->cbs[idx];\n\tstruct arm_smmu_cfg *cfg = cb->cfg;\n\n\t \n\tif (!cfg) {\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_SCTLR, 0);\n\t\treturn;\n\t}\n\n\tstage1 = cfg->cbar != CBAR_TYPE_S2_TRANS;\n\n\t \n\tif (smmu->version > ARM_SMMU_V1) {\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH64)\n\t\t\treg = ARM_SMMU_CBA2R_VA64;\n\t\telse\n\t\t\treg = 0;\n\t\t \n\t\tif (smmu->features & ARM_SMMU_FEAT_VMID16)\n\t\t\treg |= FIELD_PREP(ARM_SMMU_CBA2R_VMID16, cfg->vmid);\n\n\t\tarm_smmu_gr1_write(smmu, ARM_SMMU_GR1_CBA2R(idx), reg);\n\t}\n\n\t \n\treg = FIELD_PREP(ARM_SMMU_CBAR_TYPE, cfg->cbar);\n\tif (smmu->version < ARM_SMMU_V2)\n\t\treg |= FIELD_PREP(ARM_SMMU_CBAR_IRPTNDX, cfg->irptndx);\n\n\t \n\tif (stage1) {\n\t\treg |= FIELD_PREP(ARM_SMMU_CBAR_S1_BPSHCFG,\n\t\t\t\t  ARM_SMMU_CBAR_S1_BPSHCFG_NSH) |\n\t\t       FIELD_PREP(ARM_SMMU_CBAR_S1_MEMATTR,\n\t\t\t\t  ARM_SMMU_CBAR_S1_MEMATTR_WB);\n\t} else if (!(smmu->features & ARM_SMMU_FEAT_VMID16)) {\n\t\t \n\t\treg |= FIELD_PREP(ARM_SMMU_CBAR_VMID, cfg->vmid);\n\t}\n\tarm_smmu_gr1_write(smmu, ARM_SMMU_GR1_CBAR(idx), reg);\n\n\t \n\tif (stage1 && smmu->version > ARM_SMMU_V1)\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_TCR2, cb->tcr[1]);\n\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_TCR, cb->tcr[0]);\n\n\t \n\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_S) {\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_CONTEXTIDR, cfg->asid);\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_TTBR0, cb->ttbr[0]);\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_TTBR1, cb->ttbr[1]);\n\t} else {\n\t\tarm_smmu_cb_writeq(smmu, idx, ARM_SMMU_CB_TTBR0, cb->ttbr[0]);\n\t\tif (stage1)\n\t\t\tarm_smmu_cb_writeq(smmu, idx, ARM_SMMU_CB_TTBR1,\n\t\t\t\t\t   cb->ttbr[1]);\n\t}\n\n\t \n\tif (stage1) {\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_S1_MAIR0, cb->mair[0]);\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_S1_MAIR1, cb->mair[1]);\n\t}\n\n\t \n\treg = ARM_SMMU_SCTLR_CFIE | ARM_SMMU_SCTLR_CFRE | ARM_SMMU_SCTLR_AFE |\n\t      ARM_SMMU_SCTLR_TRE | ARM_SMMU_SCTLR_M;\n\tif (stage1)\n\t\treg |= ARM_SMMU_SCTLR_S1_ASIDPNE;\n\tif (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))\n\t\treg |= ARM_SMMU_SCTLR_E;\n\n\tif (smmu->impl && smmu->impl->write_sctlr)\n\t\tsmmu->impl->write_sctlr(smmu, idx, reg);\n\telse\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_SCTLR, reg);\n}\n\nstatic int arm_smmu_alloc_context_bank(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t       struct arm_smmu_device *smmu,\n\t\t\t\t       struct device *dev, unsigned int start)\n{\n\tif (smmu->impl && smmu->impl->alloc_context_bank)\n\t\treturn smmu->impl->alloc_context_bank(smmu_domain, smmu, dev, start);\n\n\treturn __arm_smmu_alloc_bitmap(smmu->context_map, start, smmu->num_context_banks);\n}\n\nstatic int arm_smmu_init_domain_context(struct iommu_domain *domain,\n\t\t\t\t\tstruct arm_smmu_device *smmu,\n\t\t\t\t\tstruct device *dev)\n{\n\tint irq, start, ret = 0;\n\tunsigned long ias, oas;\n\tstruct io_pgtable_ops *pgtbl_ops;\n\tstruct io_pgtable_cfg pgtbl_cfg;\n\tenum io_pgtable_fmt fmt;\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\tirqreturn_t (*context_fault)(int irq, void *dev);\n\n\tmutex_lock(&smmu_domain->init_mutex);\n\tif (smmu_domain->smmu)\n\t\tgoto out_unlock;\n\n\tif (domain->type == IOMMU_DOMAIN_IDENTITY) {\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_BYPASS;\n\t\tsmmu_domain->smmu = smmu;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (!(smmu->features & ARM_SMMU_FEAT_TRANS_S1))\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_S2;\n\tif (!(smmu->features & ARM_SMMU_FEAT_TRANS_S2))\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_S1;\n\n\t \n\tif (smmu->features & ARM_SMMU_FEAT_FMT_AARCH32_L)\n\t\tcfg->fmt = ARM_SMMU_CTX_FMT_AARCH32_L;\n\tif (IS_ENABLED(CONFIG_IOMMU_IO_PGTABLE_ARMV7S) &&\n\t    !IS_ENABLED(CONFIG_64BIT) && !IS_ENABLED(CONFIG_ARM_LPAE) &&\n\t    (smmu->features & ARM_SMMU_FEAT_FMT_AARCH32_S) &&\n\t    (smmu_domain->stage == ARM_SMMU_DOMAIN_S1))\n\t\tcfg->fmt = ARM_SMMU_CTX_FMT_AARCH32_S;\n\tif ((IS_ENABLED(CONFIG_64BIT) || cfg->fmt == ARM_SMMU_CTX_FMT_NONE) &&\n\t    (smmu->features & (ARM_SMMU_FEAT_FMT_AARCH64_64K |\n\t\t\t       ARM_SMMU_FEAT_FMT_AARCH64_16K |\n\t\t\t       ARM_SMMU_FEAT_FMT_AARCH64_4K)))\n\t\tcfg->fmt = ARM_SMMU_CTX_FMT_AARCH64;\n\n\tif (cfg->fmt == ARM_SMMU_CTX_FMT_NONE) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (smmu_domain->stage) {\n\tcase ARM_SMMU_DOMAIN_S1:\n\t\tcfg->cbar = CBAR_TYPE_S1_TRANS_S2_BYPASS;\n\t\tstart = smmu->num_s2_context_banks;\n\t\tias = smmu->va_size;\n\t\toas = smmu->ipa_size;\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH64) {\n\t\t\tfmt = ARM_64_LPAE_S1;\n\t\t} else if (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_L) {\n\t\t\tfmt = ARM_32_LPAE_S1;\n\t\t\tias = min(ias, 32UL);\n\t\t\toas = min(oas, 40UL);\n\t\t} else {\n\t\t\tfmt = ARM_V7S;\n\t\t\tias = min(ias, 32UL);\n\t\t\toas = min(oas, 32UL);\n\t\t}\n\t\tsmmu_domain->flush_ops = &arm_smmu_s1_tlb_ops;\n\t\tbreak;\n\tcase ARM_SMMU_DOMAIN_NESTED:\n\t\t \n\tcase ARM_SMMU_DOMAIN_S2:\n\t\tcfg->cbar = CBAR_TYPE_S2_TRANS;\n\t\tstart = 0;\n\t\tias = smmu->ipa_size;\n\t\toas = smmu->pa_size;\n\t\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH64) {\n\t\t\tfmt = ARM_64_LPAE_S2;\n\t\t} else {\n\t\t\tfmt = ARM_32_LPAE_S2;\n\t\t\tias = min(ias, 40UL);\n\t\t\toas = min(oas, 40UL);\n\t\t}\n\t\tif (smmu->version == ARM_SMMU_V2)\n\t\t\tsmmu_domain->flush_ops = &arm_smmu_s2_tlb_ops_v2;\n\t\telse\n\t\t\tsmmu_domain->flush_ops = &arm_smmu_s2_tlb_ops_v1;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tret = arm_smmu_alloc_context_bank(smmu_domain, smmu, dev, start);\n\tif (ret < 0) {\n\t\tgoto out_unlock;\n\t}\n\n\tsmmu_domain->smmu = smmu;\n\n\tcfg->cbndx = ret;\n\tif (smmu->version < ARM_SMMU_V2) {\n\t\tcfg->irptndx = atomic_inc_return(&smmu->irptndx);\n\t\tcfg->irptndx %= smmu->num_context_irqs;\n\t} else {\n\t\tcfg->irptndx = cfg->cbndx;\n\t}\n\n\tif (smmu_domain->stage == ARM_SMMU_DOMAIN_S2)\n\t\tcfg->vmid = cfg->cbndx + 1;\n\telse\n\t\tcfg->asid = cfg->cbndx;\n\n\tpgtbl_cfg = (struct io_pgtable_cfg) {\n\t\t.pgsize_bitmap\t= smmu->pgsize_bitmap,\n\t\t.ias\t\t= ias,\n\t\t.oas\t\t= oas,\n\t\t.coherent_walk\t= smmu->features & ARM_SMMU_FEAT_COHERENT_WALK,\n\t\t.tlb\t\t= smmu_domain->flush_ops,\n\t\t.iommu_dev\t= smmu->dev,\n\t};\n\n\tif (smmu->impl && smmu->impl->init_context) {\n\t\tret = smmu->impl->init_context(smmu_domain, &pgtbl_cfg, dev);\n\t\tif (ret)\n\t\t\tgoto out_clear_smmu;\n\t}\n\n\tif (smmu_domain->pgtbl_quirks)\n\t\tpgtbl_cfg.quirks |= smmu_domain->pgtbl_quirks;\n\n\tpgtbl_ops = alloc_io_pgtable_ops(fmt, &pgtbl_cfg, smmu_domain);\n\tif (!pgtbl_ops) {\n\t\tret = -ENOMEM;\n\t\tgoto out_clear_smmu;\n\t}\n\n\t \n\tdomain->pgsize_bitmap = pgtbl_cfg.pgsize_bitmap;\n\n\tif (pgtbl_cfg.quirks & IO_PGTABLE_QUIRK_ARM_TTBR1) {\n\t\tdomain->geometry.aperture_start = ~0UL << ias;\n\t\tdomain->geometry.aperture_end = ~0UL;\n\t} else {\n\t\tdomain->geometry.aperture_end = (1UL << ias) - 1;\n\t}\n\n\tdomain->geometry.force_aperture = true;\n\n\t \n\tarm_smmu_init_context_bank(smmu_domain, &pgtbl_cfg);\n\tarm_smmu_write_context_bank(smmu, cfg->cbndx);\n\n\t \n\tirq = smmu->irqs[cfg->irptndx];\n\n\tif (smmu->impl && smmu->impl->context_fault)\n\t\tcontext_fault = smmu->impl->context_fault;\n\telse\n\t\tcontext_fault = arm_smmu_context_fault;\n\n\tret = devm_request_irq(smmu->dev, irq, context_fault,\n\t\t\t       IRQF_SHARED, \"arm-smmu-context-fault\", domain);\n\tif (ret < 0) {\n\t\tdev_err(smmu->dev, \"failed to request context IRQ %d (%u)\\n\",\n\t\t\tcfg->irptndx, irq);\n\t\tcfg->irptndx = ARM_SMMU_INVALID_IRPTNDX;\n\t}\n\n\tmutex_unlock(&smmu_domain->init_mutex);\n\n\t \n\tsmmu_domain->pgtbl_ops = pgtbl_ops;\n\treturn 0;\n\nout_clear_smmu:\n\t__arm_smmu_free_bitmap(smmu->context_map, cfg->cbndx);\n\tsmmu_domain->smmu = NULL;\nout_unlock:\n\tmutex_unlock(&smmu_domain->init_mutex);\n\treturn ret;\n}\n\nstatic void arm_smmu_destroy_domain_context(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\tint ret, irq;\n\n\tif (!smmu || domain->type == IOMMU_DOMAIN_IDENTITY)\n\t\treturn;\n\n\tret = arm_smmu_rpm_get(smmu);\n\tif (ret < 0)\n\t\treturn;\n\n\t \n\tsmmu->cbs[cfg->cbndx].cfg = NULL;\n\tarm_smmu_write_context_bank(smmu, cfg->cbndx);\n\n\tif (cfg->irptndx != ARM_SMMU_INVALID_IRPTNDX) {\n\t\tirq = smmu->irqs[cfg->irptndx];\n\t\tdevm_free_irq(smmu->dev, irq, domain);\n\t}\n\n\tfree_io_pgtable_ops(smmu_domain->pgtbl_ops);\n\t__arm_smmu_free_bitmap(smmu->context_map, cfg->cbndx);\n\n\tarm_smmu_rpm_put(smmu);\n}\n\nstatic struct iommu_domain *arm_smmu_domain_alloc(unsigned type)\n{\n\tstruct arm_smmu_domain *smmu_domain;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED && type != IOMMU_DOMAIN_IDENTITY) {\n\t\tif (using_legacy_binding || type != IOMMU_DOMAIN_DMA)\n\t\t\treturn NULL;\n\t}\n\t \n\tsmmu_domain = kzalloc(sizeof(*smmu_domain), GFP_KERNEL);\n\tif (!smmu_domain)\n\t\treturn NULL;\n\n\tmutex_init(&smmu_domain->init_mutex);\n\tspin_lock_init(&smmu_domain->cb_lock);\n\n\treturn &smmu_domain->domain;\n}\n\nstatic void arm_smmu_domain_free(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\n\t \n\tarm_smmu_destroy_domain_context(domain);\n\tkfree(smmu_domain);\n}\n\nstatic void arm_smmu_write_smr(struct arm_smmu_device *smmu, int idx)\n{\n\tstruct arm_smmu_smr *smr = smmu->smrs + idx;\n\tu32 reg = FIELD_PREP(ARM_SMMU_SMR_ID, smr->id) |\n\t\t  FIELD_PREP(ARM_SMMU_SMR_MASK, smr->mask);\n\n\tif (!(smmu->features & ARM_SMMU_FEAT_EXIDS) && smr->valid)\n\t\treg |= ARM_SMMU_SMR_VALID;\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(idx), reg);\n}\n\nstatic void arm_smmu_write_s2cr(struct arm_smmu_device *smmu, int idx)\n{\n\tstruct arm_smmu_s2cr *s2cr = smmu->s2crs + idx;\n\tu32 reg;\n\n\tif (smmu->impl && smmu->impl->write_s2cr) {\n\t\tsmmu->impl->write_s2cr(smmu, idx);\n\t\treturn;\n\t}\n\n\treg = FIELD_PREP(ARM_SMMU_S2CR_TYPE, s2cr->type) |\n\t      FIELD_PREP(ARM_SMMU_S2CR_CBNDX, s2cr->cbndx) |\n\t      FIELD_PREP(ARM_SMMU_S2CR_PRIVCFG, s2cr->privcfg);\n\n\tif (smmu->features & ARM_SMMU_FEAT_EXIDS && smmu->smrs &&\n\t    smmu->smrs[idx].valid)\n\t\treg |= ARM_SMMU_S2CR_EXIDVALID;\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_S2CR(idx), reg);\n}\n\nstatic void arm_smmu_write_sme(struct arm_smmu_device *smmu, int idx)\n{\n\tarm_smmu_write_s2cr(smmu, idx);\n\tif (smmu->smrs)\n\t\tarm_smmu_write_smr(smmu, idx);\n}\n\n \nstatic void arm_smmu_test_smr_masks(struct arm_smmu_device *smmu)\n{\n\tu32 smr;\n\tint i;\n\n\tif (!smmu->smrs)\n\t\treturn;\n\t \n\tfor (i = 0; i < smmu->num_mapping_groups; i++)\n\t\tif (!smmu->smrs[i].valid)\n\t\t\tgoto smr_ok;\n\treturn;\nsmr_ok:\n\t \n\tsmr = FIELD_PREP(ARM_SMMU_SMR_ID, smmu->streamid_mask);\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(i), smr);\n\tsmr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(i));\n\tsmmu->streamid_mask = FIELD_GET(ARM_SMMU_SMR_ID, smr);\n\n\tsmr = FIELD_PREP(ARM_SMMU_SMR_MASK, smmu->streamid_mask);\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(i), smr);\n\tsmr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(i));\n\tsmmu->smr_mask_mask = FIELD_GET(ARM_SMMU_SMR_MASK, smr);\n}\n\nstatic int arm_smmu_find_sme(struct arm_smmu_device *smmu, u16 id, u16 mask)\n{\n\tstruct arm_smmu_smr *smrs = smmu->smrs;\n\tint i, free_idx = -ENOSPC;\n\n\t \n\tif (!smrs)\n\t\treturn id;\n\n\t \n\tfor (i = 0; i < smmu->num_mapping_groups; ++i) {\n\t\tif (!smrs[i].valid) {\n\t\t\t \n\t\t\tif (free_idx < 0)\n\t\t\t\tfree_idx = i;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif ((mask & smrs[i].mask) == mask &&\n\t\t    !((id ^ smrs[i].id) & ~smrs[i].mask))\n\t\t\treturn i;\n\t\t \n\t\tif (!((id ^ smrs[i].id) & ~(smrs[i].mask | mask)))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn free_idx;\n}\n\nstatic bool arm_smmu_free_sme(struct arm_smmu_device *smmu, int idx)\n{\n\tif (--smmu->s2crs[idx].count)\n\t\treturn false;\n\n\tsmmu->s2crs[idx] = s2cr_init_val;\n\tif (smmu->smrs)\n\t\tsmmu->smrs[idx].valid = false;\n\n\treturn true;\n}\n\nstatic int arm_smmu_master_alloc_smes(struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);\n\tstruct arm_smmu_device *smmu = cfg->smmu;\n\tstruct arm_smmu_smr *smrs = smmu->smrs;\n\tint i, idx, ret;\n\n\tmutex_lock(&smmu->stream_map_mutex);\n\t \n\tfor_each_cfg_sme(cfg, fwspec, i, idx) {\n\t\tu16 sid = FIELD_GET(ARM_SMMU_SMR_ID, fwspec->ids[i]);\n\t\tu16 mask = FIELD_GET(ARM_SMMU_SMR_MASK, fwspec->ids[i]);\n\n\t\tif (idx != INVALID_SMENDX) {\n\t\t\tret = -EEXIST;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tret = arm_smmu_find_sme(smmu, sid, mask);\n\t\tif (ret < 0)\n\t\t\tgoto out_err;\n\n\t\tidx = ret;\n\t\tif (smrs && smmu->s2crs[idx].count == 0) {\n\t\t\tsmrs[idx].id = sid;\n\t\t\tsmrs[idx].mask = mask;\n\t\t\tsmrs[idx].valid = true;\n\t\t}\n\t\tsmmu->s2crs[idx].count++;\n\t\tcfg->smendx[i] = (s16)idx;\n\t}\n\n\t \n\tfor_each_cfg_sme(cfg, fwspec, i, idx)\n\t\tarm_smmu_write_sme(smmu, idx);\n\n\tmutex_unlock(&smmu->stream_map_mutex);\n\treturn 0;\n\nout_err:\n\twhile (i--) {\n\t\tarm_smmu_free_sme(smmu, cfg->smendx[i]);\n\t\tcfg->smendx[i] = INVALID_SMENDX;\n\t}\n\tmutex_unlock(&smmu->stream_map_mutex);\n\treturn ret;\n}\n\nstatic void arm_smmu_master_free_smes(struct arm_smmu_master_cfg *cfg,\n\t\t\t\t      struct iommu_fwspec *fwspec)\n{\n\tstruct arm_smmu_device *smmu = cfg->smmu;\n\tint i, idx;\n\n\tmutex_lock(&smmu->stream_map_mutex);\n\tfor_each_cfg_sme(cfg, fwspec, i, idx) {\n\t\tif (arm_smmu_free_sme(smmu, idx))\n\t\t\tarm_smmu_write_sme(smmu, idx);\n\t\tcfg->smendx[i] = INVALID_SMENDX;\n\t}\n\tmutex_unlock(&smmu->stream_map_mutex);\n}\n\nstatic int arm_smmu_domain_add_master(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t      struct arm_smmu_master_cfg *cfg,\n\t\t\t\t      struct iommu_fwspec *fwspec)\n{\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_s2cr *s2cr = smmu->s2crs;\n\tu8 cbndx = smmu_domain->cfg.cbndx;\n\tenum arm_smmu_s2cr_type type;\n\tint i, idx;\n\n\tif (smmu_domain->stage == ARM_SMMU_DOMAIN_BYPASS)\n\t\ttype = S2CR_TYPE_BYPASS;\n\telse\n\t\ttype = S2CR_TYPE_TRANS;\n\n\tfor_each_cfg_sme(cfg, fwspec, i, idx) {\n\t\tif (type == s2cr[idx].type && cbndx == s2cr[idx].cbndx)\n\t\t\tcontinue;\n\n\t\ts2cr[idx].type = type;\n\t\ts2cr[idx].privcfg = S2CR_PRIVCFG_DEFAULT;\n\t\ts2cr[idx].cbndx = cbndx;\n\t\tarm_smmu_write_s2cr(smmu, idx);\n\t}\n\treturn 0;\n}\n\nstatic int arm_smmu_attach_dev(struct iommu_domain *domain, struct device *dev)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct arm_smmu_master_cfg *cfg;\n\tstruct arm_smmu_device *smmu;\n\tint ret;\n\n\tif (!fwspec || fwspec->ops != &arm_smmu_ops) {\n\t\tdev_err(dev, \"cannot attach to SMMU, is it on the same bus?\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tcfg = dev_iommu_priv_get(dev);\n\tif (!cfg)\n\t\treturn -ENODEV;\n\n\tsmmu = cfg->smmu;\n\n\tret = arm_smmu_rpm_get(smmu);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tret = arm_smmu_init_domain_context(domain, smmu, dev);\n\tif (ret < 0)\n\t\tgoto rpm_put;\n\n\t \n\tif (smmu_domain->smmu != smmu) {\n\t\tret = -EINVAL;\n\t\tgoto rpm_put;\n\t}\n\n\t \n\tret = arm_smmu_domain_add_master(smmu_domain, cfg, fwspec);\n\n\t \n\tpm_runtime_set_autosuspend_delay(smmu->dev, 20);\n\tpm_runtime_use_autosuspend(smmu->dev);\n\nrpm_put:\n\tarm_smmu_rpm_put(smmu);\n\treturn ret;\n}\n\nstatic int arm_smmu_map_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t      phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t      int prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;\n\tstruct arm_smmu_device *smmu = to_smmu_domain(domain)->smmu;\n\tint ret;\n\n\tif (!ops)\n\t\treturn -ENODEV;\n\n\tarm_smmu_rpm_get(smmu);\n\tret = ops->map_pages(ops, iova, paddr, pgsize, pgcount, prot, gfp, mapped);\n\tarm_smmu_rpm_put(smmu);\n\n\treturn ret;\n}\n\nstatic size_t arm_smmu_unmap_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t\t   size_t pgsize, size_t pgcount,\n\t\t\t\t   struct iommu_iotlb_gather *iotlb_gather)\n{\n\tstruct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;\n\tstruct arm_smmu_device *smmu = to_smmu_domain(domain)->smmu;\n\tsize_t ret;\n\n\tif (!ops)\n\t\treturn 0;\n\n\tarm_smmu_rpm_get(smmu);\n\tret = ops->unmap_pages(ops, iova, pgsize, pgcount, iotlb_gather);\n\tarm_smmu_rpm_put(smmu);\n\n\treturn ret;\n}\n\nstatic void arm_smmu_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\tif (smmu_domain->flush_ops) {\n\t\tarm_smmu_rpm_get(smmu);\n\t\tsmmu_domain->flush_ops->tlb_flush_all(smmu_domain);\n\t\tarm_smmu_rpm_put(smmu);\n\t}\n}\n\nstatic void arm_smmu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\tstruct iommu_iotlb_gather *gather)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\tif (!smmu)\n\t\treturn;\n\n\tarm_smmu_rpm_get(smmu);\n\tif (smmu->version == ARM_SMMU_V2 ||\n\t    smmu_domain->stage == ARM_SMMU_DOMAIN_S1)\n\t\tarm_smmu_tlb_sync_context(smmu_domain);\n\telse\n\t\tarm_smmu_tlb_sync_global(smmu);\n\tarm_smmu_rpm_put(smmu);\n}\n\nstatic phys_addr_t arm_smmu_iova_to_phys_hard(struct iommu_domain *domain,\n\t\t\t\t\t      dma_addr_t iova)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_cfg *cfg = &smmu_domain->cfg;\n\tstruct io_pgtable_ops *ops= smmu_domain->pgtbl_ops;\n\tstruct device *dev = smmu->dev;\n\tvoid __iomem *reg;\n\tu32 tmp;\n\tu64 phys;\n\tunsigned long va, flags;\n\tint ret, idx = cfg->cbndx;\n\tphys_addr_t addr = 0;\n\n\tret = arm_smmu_rpm_get(smmu);\n\tif (ret < 0)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&smmu_domain->cb_lock, flags);\n\tva = iova & ~0xfffUL;\n\tif (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH64)\n\t\tarm_smmu_cb_writeq(smmu, idx, ARM_SMMU_CB_ATS1PR, va);\n\telse\n\t\tarm_smmu_cb_write(smmu, idx, ARM_SMMU_CB_ATS1PR, va);\n\n\treg = arm_smmu_page(smmu, ARM_SMMU_CB(smmu, idx)) + ARM_SMMU_CB_ATSR;\n\tif (readl_poll_timeout_atomic(reg, tmp, !(tmp & ARM_SMMU_ATSR_ACTIVE),\n\t\t\t\t      5, 50)) {\n\t\tspin_unlock_irqrestore(&smmu_domain->cb_lock, flags);\n\t\tdev_err(dev,\n\t\t\t\"iova to phys timed out on %pad. Falling back to software table walk.\\n\",\n\t\t\t&iova);\n\t\tarm_smmu_rpm_put(smmu);\n\t\treturn ops->iova_to_phys(ops, iova);\n\t}\n\n\tphys = arm_smmu_cb_readq(smmu, idx, ARM_SMMU_CB_PAR);\n\tspin_unlock_irqrestore(&smmu_domain->cb_lock, flags);\n\tif (phys & ARM_SMMU_CB_PAR_F) {\n\t\tdev_err(dev, \"translation fault!\\n\");\n\t\tdev_err(dev, \"PAR = 0x%llx\\n\", phys);\n\t\tgoto out;\n\t}\n\n\taddr = (phys & GENMASK_ULL(39, 12)) | (iova & 0xfff);\nout:\n\tarm_smmu_rpm_put(smmu);\n\n\treturn addr;\n}\n\nstatic phys_addr_t arm_smmu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\tdma_addr_t iova)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct io_pgtable_ops *ops = smmu_domain->pgtbl_ops;\n\n\tif (!ops)\n\t\treturn 0;\n\n\tif (smmu_domain->smmu->features & ARM_SMMU_FEAT_TRANS_OPS &&\n\t\t\tsmmu_domain->stage == ARM_SMMU_DOMAIN_S1)\n\t\treturn arm_smmu_iova_to_phys_hard(domain, iova);\n\n\treturn ops->iova_to_phys(ops, iova);\n}\n\nstatic bool arm_smmu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tstruct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);\n\n\tswitch (cap) {\n\tcase IOMMU_CAP_CACHE_COHERENCY:\n\t\t \n\t\treturn cfg->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK ||\n\t\t\tdevice_get_dma_attr(dev) == DEV_DMA_COHERENT;\n\tcase IOMMU_CAP_NOEXEC:\n\tcase IOMMU_CAP_DEFERRED_FLUSH:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic\nstruct arm_smmu_device *arm_smmu_get_by_fwnode(struct fwnode_handle *fwnode)\n{\n\tstruct device *dev = driver_find_device_by_fwnode(&arm_smmu_driver.driver,\n\t\t\t\t\t\t\t  fwnode);\n\tput_device(dev);\n\treturn dev ? dev_get_drvdata(dev) : NULL;\n}\n\nstatic struct iommu_device *arm_smmu_probe_device(struct device *dev)\n{\n\tstruct arm_smmu_device *smmu = NULL;\n\tstruct arm_smmu_master_cfg *cfg;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tint i, ret;\n\n\tif (using_legacy_binding) {\n\t\tret = arm_smmu_register_legacy_master(dev, &smmu);\n\n\t\t \n\t\tfwspec = dev_iommu_fwspec_get(dev);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t} else if (fwspec && fwspec->ops == &arm_smmu_ops) {\n\t\tsmmu = arm_smmu_get_by_fwnode(fwspec->iommu_fwnode);\n\t} else {\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tret = -EINVAL;\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\tu16 sid = FIELD_GET(ARM_SMMU_SMR_ID, fwspec->ids[i]);\n\t\tu16 mask = FIELD_GET(ARM_SMMU_SMR_MASK, fwspec->ids[i]);\n\n\t\tif (sid & ~smmu->streamid_mask) {\n\t\t\tdev_err(dev, \"stream ID 0x%x out of range for SMMU (0x%x)\\n\",\n\t\t\t\tsid, smmu->streamid_mask);\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (mask & ~smmu->smr_mask_mask) {\n\t\t\tdev_err(dev, \"SMR mask 0x%x out of range for SMMU (0x%x)\\n\",\n\t\t\t\tmask, smmu->smr_mask_mask);\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tret = -ENOMEM;\n\tcfg = kzalloc(offsetof(struct arm_smmu_master_cfg, smendx[i]),\n\t\t      GFP_KERNEL);\n\tif (!cfg)\n\t\tgoto out_free;\n\n\tcfg->smmu = smmu;\n\tdev_iommu_priv_set(dev, cfg);\n\twhile (i--)\n\t\tcfg->smendx[i] = INVALID_SMENDX;\n\n\tret = arm_smmu_rpm_get(smmu);\n\tif (ret < 0)\n\t\tgoto out_cfg_free;\n\n\tret = arm_smmu_master_alloc_smes(dev);\n\tarm_smmu_rpm_put(smmu);\n\n\tif (ret)\n\t\tgoto out_cfg_free;\n\n\tdevice_link_add(dev, smmu->dev,\n\t\t\tDL_FLAG_PM_RUNTIME | DL_FLAG_AUTOREMOVE_SUPPLIER);\n\n\treturn &smmu->iommu;\n\nout_cfg_free:\n\tkfree(cfg);\nout_free:\n\tiommu_fwspec_free(dev);\n\treturn ERR_PTR(ret);\n}\n\nstatic void arm_smmu_release_device(struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);\n\tint ret;\n\n\tret = arm_smmu_rpm_get(cfg->smmu);\n\tif (ret < 0)\n\t\treturn;\n\n\tarm_smmu_master_free_smes(cfg, fwspec);\n\n\tarm_smmu_rpm_put(cfg->smmu);\n\n\tdev_iommu_priv_set(dev, NULL);\n\tkfree(cfg);\n}\n\nstatic void arm_smmu_probe_finalize(struct device *dev)\n{\n\tstruct arm_smmu_master_cfg *cfg;\n\tstruct arm_smmu_device *smmu;\n\n\tcfg = dev_iommu_priv_get(dev);\n\tsmmu = cfg->smmu;\n\n\tif (smmu->impl && smmu->impl->probe_finalize)\n\t\tsmmu->impl->probe_finalize(smmu, dev);\n}\n\nstatic struct iommu_group *arm_smmu_device_group(struct device *dev)\n{\n\tstruct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct arm_smmu_device *smmu = cfg->smmu;\n\tstruct iommu_group *group = NULL;\n\tint i, idx;\n\n\tmutex_lock(&smmu->stream_map_mutex);\n\tfor_each_cfg_sme(cfg, fwspec, i, idx) {\n\t\tif (group && smmu->s2crs[idx].group &&\n\t\t    group != smmu->s2crs[idx].group) {\n\t\t\tmutex_unlock(&smmu->stream_map_mutex);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tgroup = smmu->s2crs[idx].group;\n\t}\n\n\tif (group) {\n\t\tmutex_unlock(&smmu->stream_map_mutex);\n\t\treturn iommu_group_ref_get(group);\n\t}\n\n\tif (dev_is_pci(dev))\n\t\tgroup = pci_device_group(dev);\n\telse if (dev_is_fsl_mc(dev))\n\t\tgroup = fsl_mc_device_group(dev);\n\telse\n\t\tgroup = generic_device_group(dev);\n\n\t \n\tif (!IS_ERR(group))\n\t\tfor_each_cfg_sme(cfg, fwspec, i, idx)\n\t\t\tsmmu->s2crs[idx].group = group;\n\n\tmutex_unlock(&smmu->stream_map_mutex);\n\treturn group;\n}\n\nstatic int arm_smmu_enable_nesting(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tint ret = 0;\n\n\tmutex_lock(&smmu_domain->init_mutex);\n\tif (smmu_domain->smmu)\n\t\tret = -EPERM;\n\telse\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_NESTED;\n\tmutex_unlock(&smmu_domain->init_mutex);\n\n\treturn ret;\n}\n\nstatic int arm_smmu_set_pgtable_quirks(struct iommu_domain *domain,\n\t\tunsigned long quirks)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tint ret = 0;\n\n\tmutex_lock(&smmu_domain->init_mutex);\n\tif (smmu_domain->smmu)\n\t\tret = -EPERM;\n\telse\n\t\tsmmu_domain->pgtbl_quirks = quirks;\n\tmutex_unlock(&smmu_domain->init_mutex);\n\n\treturn ret;\n}\n\nstatic int arm_smmu_of_xlate(struct device *dev, struct of_phandle_args *args)\n{\n\tu32 mask, fwid = 0;\n\n\tif (args->args_count > 0)\n\t\tfwid |= FIELD_PREP(ARM_SMMU_SMR_ID, args->args[0]);\n\n\tif (args->args_count > 1)\n\t\tfwid |= FIELD_PREP(ARM_SMMU_SMR_MASK, args->args[1]);\n\telse if (!of_property_read_u32(args->np, \"stream-match-mask\", &mask))\n\t\tfwid |= FIELD_PREP(ARM_SMMU_SMR_MASK, mask);\n\n\treturn iommu_fwspec_add_ids(dev, &fwid, 1);\n}\n\nstatic void arm_smmu_get_resv_regions(struct device *dev,\n\t\t\t\t      struct list_head *head)\n{\n\tstruct iommu_resv_region *region;\n\tint prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\n\tregion = iommu_alloc_resv_region(MSI_IOVA_BASE, MSI_IOVA_LENGTH,\n\t\t\t\t\t prot, IOMMU_RESV_SW_MSI, GFP_KERNEL);\n\tif (!region)\n\t\treturn;\n\n\tlist_add_tail(&region->list, head);\n\n\tiommu_dma_get_resv_regions(dev, head);\n}\n\nstatic int arm_smmu_def_domain_type(struct device *dev)\n{\n\tstruct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);\n\tconst struct arm_smmu_impl *impl = cfg->smmu->impl;\n\n\tif (using_legacy_binding)\n\t\treturn IOMMU_DOMAIN_IDENTITY;\n\n\tif (impl && impl->def_domain_type)\n\t\treturn impl->def_domain_type(dev);\n\n\treturn 0;\n}\n\nstatic struct iommu_ops arm_smmu_ops = {\n\t.capable\t\t= arm_smmu_capable,\n\t.domain_alloc\t\t= arm_smmu_domain_alloc,\n\t.probe_device\t\t= arm_smmu_probe_device,\n\t.release_device\t\t= arm_smmu_release_device,\n\t.probe_finalize\t\t= arm_smmu_probe_finalize,\n\t.device_group\t\t= arm_smmu_device_group,\n\t.of_xlate\t\t= arm_smmu_of_xlate,\n\t.get_resv_regions\t= arm_smmu_get_resv_regions,\n\t.def_domain_type\t= arm_smmu_def_domain_type,\n\t.pgsize_bitmap\t\t= -1UL,  \n\t.owner\t\t\t= THIS_MODULE,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t\t= arm_smmu_attach_dev,\n\t\t.map_pages\t\t= arm_smmu_map_pages,\n\t\t.unmap_pages\t\t= arm_smmu_unmap_pages,\n\t\t.flush_iotlb_all\t= arm_smmu_flush_iotlb_all,\n\t\t.iotlb_sync\t\t= arm_smmu_iotlb_sync,\n\t\t.iova_to_phys\t\t= arm_smmu_iova_to_phys,\n\t\t.enable_nesting\t\t= arm_smmu_enable_nesting,\n\t\t.set_pgtable_quirks\t= arm_smmu_set_pgtable_quirks,\n\t\t.free\t\t\t= arm_smmu_domain_free,\n\t}\n};\n\nstatic void arm_smmu_device_reset(struct arm_smmu_device *smmu)\n{\n\tint i;\n\tu32 reg;\n\n\t \n\treg = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sGFSR);\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sGFSR, reg);\n\n\t \n\tfor (i = 0; i < smmu->num_mapping_groups; ++i)\n\t\tarm_smmu_write_sme(smmu, i);\n\n\t \n\tfor (i = 0; i < smmu->num_context_banks; ++i) {\n\t\tarm_smmu_write_context_bank(smmu, i);\n\t\tarm_smmu_cb_write(smmu, i, ARM_SMMU_CB_FSR, ARM_SMMU_FSR_FAULT);\n\t}\n\n\t \n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIALLH, QCOM_DUMMY_VAL);\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIALLNSNH, QCOM_DUMMY_VAL);\n\n\treg = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sCR0);\n\n\t \n\treg |= (ARM_SMMU_sCR0_GFRE | ARM_SMMU_sCR0_GFIE |\n\t\tARM_SMMU_sCR0_GCFGFRE | ARM_SMMU_sCR0_GCFGFIE);\n\n\t \n\treg |= (ARM_SMMU_sCR0_VMIDPNE | ARM_SMMU_sCR0_PTM);\n\n\t \n\treg &= ~ARM_SMMU_sCR0_CLIENTPD;\n\tif (disable_bypass)\n\t\treg |= ARM_SMMU_sCR0_USFCFG;\n\telse\n\t\treg &= ~ARM_SMMU_sCR0_USFCFG;\n\n\t \n\treg &= ~ARM_SMMU_sCR0_FB;\n\n\t \n\treg &= ~(ARM_SMMU_sCR0_BSU);\n\n\tif (smmu->features & ARM_SMMU_FEAT_VMID16)\n\t\treg |= ARM_SMMU_sCR0_VMID16EN;\n\n\tif (smmu->features & ARM_SMMU_FEAT_EXIDS)\n\t\treg |= ARM_SMMU_sCR0_EXIDENABLE;\n\n\tif (smmu->impl && smmu->impl->reset)\n\t\tsmmu->impl->reset(smmu);\n\n\t \n\tarm_smmu_tlb_sync_global(smmu);\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sCR0, reg);\n}\n\nstatic int arm_smmu_id_size_to_bits(int size)\n{\n\tswitch (size) {\n\tcase 0:\n\t\treturn 32;\n\tcase 1:\n\t\treturn 36;\n\tcase 2:\n\t\treturn 40;\n\tcase 3:\n\t\treturn 42;\n\tcase 4:\n\t\treturn 44;\n\tcase 5:\n\tdefault:\n\t\treturn 48;\n\t}\n}\n\nstatic int arm_smmu_device_cfg_probe(struct arm_smmu_device *smmu)\n{\n\tunsigned int size;\n\tu32 id;\n\tbool cttw_reg, cttw_fw = smmu->features & ARM_SMMU_FEAT_COHERENT_WALK;\n\tint i, ret;\n\n\tdev_notice(smmu->dev, \"probing hardware configuration...\\n\");\n\tdev_notice(smmu->dev, \"SMMUv%d with:\\n\",\n\t\t\tsmmu->version == ARM_SMMU_V2 ? 2 : 1);\n\n\t \n\tid = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_ID0);\n\n\t \n\tif (force_stage == 1)\n\t\tid &= ~(ARM_SMMU_ID0_S2TS | ARM_SMMU_ID0_NTS);\n\telse if (force_stage == 2)\n\t\tid &= ~(ARM_SMMU_ID0_S1TS | ARM_SMMU_ID0_NTS);\n\n\tif (id & ARM_SMMU_ID0_S1TS) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_S1;\n\t\tdev_notice(smmu->dev, \"\\tstage 1 translation\\n\");\n\t}\n\n\tif (id & ARM_SMMU_ID0_S2TS) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_S2;\n\t\tdev_notice(smmu->dev, \"\\tstage 2 translation\\n\");\n\t}\n\n\tif (id & ARM_SMMU_ID0_NTS) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_NESTED;\n\t\tdev_notice(smmu->dev, \"\\tnested translation\\n\");\n\t}\n\n\tif (!(smmu->features &\n\t\t(ARM_SMMU_FEAT_TRANS_S1 | ARM_SMMU_FEAT_TRANS_S2))) {\n\t\tdev_err(smmu->dev, \"\\tno translation support!\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif ((id & ARM_SMMU_ID0_S1TS) &&\n\t    ((smmu->version < ARM_SMMU_V2) || !(id & ARM_SMMU_ID0_ATOSNS))) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_OPS;\n\t\tdev_notice(smmu->dev, \"\\taddress translation ops\\n\");\n\t}\n\n\t \n\tcttw_reg = !!(id & ARM_SMMU_ID0_CTTW);\n\tif (cttw_fw || cttw_reg)\n\t\tdev_notice(smmu->dev, \"\\t%scoherent table walk\\n\",\n\t\t\t   cttw_fw ? \"\" : \"non-\");\n\tif (cttw_fw != cttw_reg)\n\t\tdev_notice(smmu->dev,\n\t\t\t   \"\\t(IDR0.CTTW overridden by FW configuration)\\n\");\n\n\t \n\tif (smmu->version == ARM_SMMU_V2 && id & ARM_SMMU_ID0_EXIDS) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_EXIDS;\n\t\tsize = 1 << 16;\n\t} else {\n\t\tsize = 1 << FIELD_GET(ARM_SMMU_ID0_NUMSIDB, id);\n\t}\n\tsmmu->streamid_mask = size - 1;\n\tif (id & ARM_SMMU_ID0_SMS) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_STREAM_MATCH;\n\t\tsize = FIELD_GET(ARM_SMMU_ID0_NUMSMRG, id);\n\t\tif (size == 0) {\n\t\t\tdev_err(smmu->dev,\n\t\t\t\t\"stream-matching supported, but no SMRs present!\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\t \n\t\tsmmu->smrs = devm_kcalloc(smmu->dev, size, sizeof(*smmu->smrs),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!smmu->smrs)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_notice(smmu->dev,\n\t\t\t   \"\\tstream matching with %u register groups\", size);\n\t}\n\t \n\tsmmu->s2crs = devm_kmalloc_array(smmu->dev, size, sizeof(*smmu->s2crs),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!smmu->s2crs)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < size; i++)\n\t\tsmmu->s2crs[i] = s2cr_init_val;\n\n\tsmmu->num_mapping_groups = size;\n\tmutex_init(&smmu->stream_map_mutex);\n\tspin_lock_init(&smmu->global_sync_lock);\n\n\tif (smmu->version < ARM_SMMU_V2 ||\n\t    !(id & ARM_SMMU_ID0_PTFS_NO_AARCH32)) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH32_L;\n\t\tif (!(id & ARM_SMMU_ID0_PTFS_NO_AARCH32S))\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH32_S;\n\t}\n\n\t \n\tid = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_ID1);\n\tsmmu->pgshift = (id & ARM_SMMU_ID1_PAGESIZE) ? 16 : 12;\n\n\t \n\tsize = 1 << (FIELD_GET(ARM_SMMU_ID1_NUMPAGENDXB, id) + 1);\n\tif (smmu->numpage != 2 * size << smmu->pgshift)\n\t\tdev_warn(smmu->dev,\n\t\t\t\"SMMU address space size (0x%x) differs from mapped region size (0x%x)!\\n\",\n\t\t\t2 * size << smmu->pgshift, smmu->numpage);\n\t \n\tsmmu->numpage = size;\n\n\tsmmu->num_s2_context_banks = FIELD_GET(ARM_SMMU_ID1_NUMS2CB, id);\n\tsmmu->num_context_banks = FIELD_GET(ARM_SMMU_ID1_NUMCB, id);\n\tif (smmu->num_s2_context_banks > smmu->num_context_banks) {\n\t\tdev_err(smmu->dev, \"impossible number of S2 context banks!\\n\");\n\t\treturn -ENODEV;\n\t}\n\tdev_notice(smmu->dev, \"\\t%u context banks (%u stage-2 only)\\n\",\n\t\t   smmu->num_context_banks, smmu->num_s2_context_banks);\n\tsmmu->cbs = devm_kcalloc(smmu->dev, smmu->num_context_banks,\n\t\t\t\t sizeof(*smmu->cbs), GFP_KERNEL);\n\tif (!smmu->cbs)\n\t\treturn -ENOMEM;\n\n\t \n\tid = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_ID2);\n\tsize = arm_smmu_id_size_to_bits(FIELD_GET(ARM_SMMU_ID2_IAS, id));\n\tsmmu->ipa_size = size;\n\n\t \n\tsize = arm_smmu_id_size_to_bits(FIELD_GET(ARM_SMMU_ID2_OAS, id));\n\tsmmu->pa_size = size;\n\n\tif (id & ARM_SMMU_ID2_VMID16)\n\t\tsmmu->features |= ARM_SMMU_FEAT_VMID16;\n\n\t \n\tif (dma_set_mask_and_coherent(smmu->dev, DMA_BIT_MASK(size)))\n\t\tdev_warn(smmu->dev,\n\t\t\t \"failed to set DMA mask for table walker\\n\");\n\n\tif (smmu->version < ARM_SMMU_V2) {\n\t\tsmmu->va_size = smmu->ipa_size;\n\t\tif (smmu->version == ARM_SMMU_V1_64K)\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH64_64K;\n\t} else {\n\t\tsize = FIELD_GET(ARM_SMMU_ID2_UBS, id);\n\t\tsmmu->va_size = arm_smmu_id_size_to_bits(size);\n\t\tif (id & ARM_SMMU_ID2_PTFS_4K)\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH64_4K;\n\t\tif (id & ARM_SMMU_ID2_PTFS_16K)\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH64_16K;\n\t\tif (id & ARM_SMMU_ID2_PTFS_64K)\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_FMT_AARCH64_64K;\n\t}\n\n\tif (smmu->impl && smmu->impl->cfg_probe) {\n\t\tret = smmu->impl->cfg_probe(smmu);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (smmu->features & ARM_SMMU_FEAT_FMT_AARCH32_S)\n\t\tsmmu->pgsize_bitmap |= SZ_4K | SZ_64K | SZ_1M | SZ_16M;\n\tif (smmu->features &\n\t    (ARM_SMMU_FEAT_FMT_AARCH32_L | ARM_SMMU_FEAT_FMT_AARCH64_4K))\n\t\tsmmu->pgsize_bitmap |= SZ_4K | SZ_2M | SZ_1G;\n\tif (smmu->features & ARM_SMMU_FEAT_FMT_AARCH64_16K)\n\t\tsmmu->pgsize_bitmap |= SZ_16K | SZ_32M;\n\tif (smmu->features & ARM_SMMU_FEAT_FMT_AARCH64_64K)\n\t\tsmmu->pgsize_bitmap |= SZ_64K | SZ_512M;\n\n\tif (arm_smmu_ops.pgsize_bitmap == -1UL)\n\t\tarm_smmu_ops.pgsize_bitmap = smmu->pgsize_bitmap;\n\telse\n\t\tarm_smmu_ops.pgsize_bitmap |= smmu->pgsize_bitmap;\n\tdev_notice(smmu->dev, \"\\tSupported page sizes: 0x%08lx\\n\",\n\t\t   smmu->pgsize_bitmap);\n\n\n\tif (smmu->features & ARM_SMMU_FEAT_TRANS_S1)\n\t\tdev_notice(smmu->dev, \"\\tStage-1: %lu-bit VA -> %lu-bit IPA\\n\",\n\t\t\t   smmu->va_size, smmu->ipa_size);\n\n\tif (smmu->features & ARM_SMMU_FEAT_TRANS_S2)\n\t\tdev_notice(smmu->dev, \"\\tStage-2: %lu-bit IPA -> %lu-bit PA\\n\",\n\t\t\t   smmu->ipa_size, smmu->pa_size);\n\n\treturn 0;\n}\n\nstruct arm_smmu_match_data {\n\tenum arm_smmu_arch_version version;\n\tenum arm_smmu_implementation model;\n};\n\n#define ARM_SMMU_MATCH_DATA(name, ver, imp)\t\\\nstatic const struct arm_smmu_match_data name = { .version = ver, .model = imp }\n\nARM_SMMU_MATCH_DATA(smmu_generic_v1, ARM_SMMU_V1, GENERIC_SMMU);\nARM_SMMU_MATCH_DATA(smmu_generic_v2, ARM_SMMU_V2, GENERIC_SMMU);\nARM_SMMU_MATCH_DATA(arm_mmu401, ARM_SMMU_V1_64K, GENERIC_SMMU);\nARM_SMMU_MATCH_DATA(arm_mmu500, ARM_SMMU_V2, ARM_MMU500);\nARM_SMMU_MATCH_DATA(cavium_smmuv2, ARM_SMMU_V2, CAVIUM_SMMUV2);\nARM_SMMU_MATCH_DATA(qcom_smmuv2, ARM_SMMU_V2, QCOM_SMMUV2);\n\nstatic const struct of_device_id arm_smmu_of_match[] = {\n\t{ .compatible = \"arm,smmu-v1\", .data = &smmu_generic_v1 },\n\t{ .compatible = \"arm,smmu-v2\", .data = &smmu_generic_v2 },\n\t{ .compatible = \"arm,mmu-400\", .data = &smmu_generic_v1 },\n\t{ .compatible = \"arm,mmu-401\", .data = &arm_mmu401 },\n\t{ .compatible = \"arm,mmu-500\", .data = &arm_mmu500 },\n\t{ .compatible = \"cavium,smmu-v2\", .data = &cavium_smmuv2 },\n\t{ .compatible = \"nvidia,smmu-500\", .data = &arm_mmu500 },\n\t{ .compatible = \"qcom,smmu-v2\", .data = &qcom_smmuv2 },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, arm_smmu_of_match);\n\n#ifdef CONFIG_ACPI\nstatic int acpi_smmu_get_data(u32 model, struct arm_smmu_device *smmu)\n{\n\tint ret = 0;\n\n\tswitch (model) {\n\tcase ACPI_IORT_SMMU_V1:\n\tcase ACPI_IORT_SMMU_CORELINK_MMU400:\n\t\tsmmu->version = ARM_SMMU_V1;\n\t\tsmmu->model = GENERIC_SMMU;\n\t\tbreak;\n\tcase ACPI_IORT_SMMU_CORELINK_MMU401:\n\t\tsmmu->version = ARM_SMMU_V1_64K;\n\t\tsmmu->model = GENERIC_SMMU;\n\t\tbreak;\n\tcase ACPI_IORT_SMMU_V2:\n\t\tsmmu->version = ARM_SMMU_V2;\n\t\tsmmu->model = GENERIC_SMMU;\n\t\tbreak;\n\tcase ACPI_IORT_SMMU_CORELINK_MMU500:\n\t\tsmmu->version = ARM_SMMU_V2;\n\t\tsmmu->model = ARM_MMU500;\n\t\tbreak;\n\tcase ACPI_IORT_SMMU_CAVIUM_THUNDERX:\n\t\tsmmu->version = ARM_SMMU_V2;\n\t\tsmmu->model = CAVIUM_SMMUV2;\n\t\tbreak;\n\tdefault:\n\t\tret = -ENODEV;\n\t}\n\n\treturn ret;\n}\n\nstatic int arm_smmu_device_acpi_probe(struct arm_smmu_device *smmu,\n\t\t\t\t      u32 *global_irqs, u32 *pmu_irqs)\n{\n\tstruct device *dev = smmu->dev;\n\tstruct acpi_iort_node *node =\n\t\t*(struct acpi_iort_node **)dev_get_platdata(dev);\n\tstruct acpi_iort_smmu *iort_smmu;\n\tint ret;\n\n\t \n\tiort_smmu = (struct acpi_iort_smmu *)node->node_data;\n\n\tret = acpi_smmu_get_data(iort_smmu->model, smmu);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\t*global_irqs = 1;\n\t*pmu_irqs = 0;\n\n\tif (iort_smmu->flags & ACPI_IORT_SMMU_COHERENT_WALK)\n\t\tsmmu->features |= ARM_SMMU_FEAT_COHERENT_WALK;\n\n\treturn 0;\n}\n#else\nstatic inline int arm_smmu_device_acpi_probe(struct arm_smmu_device *smmu,\n\t\t\t\t\t     u32 *global_irqs, u32 *pmu_irqs)\n{\n\treturn -ENODEV;\n}\n#endif\n\nstatic int arm_smmu_device_dt_probe(struct arm_smmu_device *smmu,\n\t\t\t\t    u32 *global_irqs, u32 *pmu_irqs)\n{\n\tconst struct arm_smmu_match_data *data;\n\tstruct device *dev = smmu->dev;\n\tbool legacy_binding;\n\n\tif (of_property_read_u32(dev->of_node, \"#global-interrupts\", global_irqs))\n\t\treturn dev_err_probe(dev, -ENODEV,\n\t\t\t\t     \"missing #global-interrupts property\\n\");\n\t*pmu_irqs = 0;\n\n\tdata = of_device_get_match_data(dev);\n\tsmmu->version = data->version;\n\tsmmu->model = data->model;\n\n\tlegacy_binding = of_find_property(dev->of_node, \"mmu-masters\", NULL);\n\tif (legacy_binding && !using_generic_binding) {\n\t\tif (!using_legacy_binding) {\n\t\t\tpr_notice(\"deprecated \\\"mmu-masters\\\" DT property in use; %s support unavailable\\n\",\n\t\t\t\t  IS_ENABLED(CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS) ? \"DMA API\" : \"SMMU\");\n\t\t}\n\t\tusing_legacy_binding = true;\n\t} else if (!legacy_binding && !using_legacy_binding) {\n\t\tusing_generic_binding = true;\n\t} else {\n\t\tdev_err(dev, \"not probing due to mismatched DT properties\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (of_dma_is_coherent(dev->of_node))\n\t\tsmmu->features |= ARM_SMMU_FEAT_COHERENT_WALK;\n\n\treturn 0;\n}\n\nstatic void arm_smmu_rmr_install_bypass_smr(struct arm_smmu_device *smmu)\n{\n\tstruct list_head rmr_list;\n\tstruct iommu_resv_region *e;\n\tint idx, cnt = 0;\n\tu32 reg;\n\n\tINIT_LIST_HEAD(&rmr_list);\n\tiort_get_rmr_sids(dev_fwnode(smmu->dev), &rmr_list);\n\n\t \n\treg = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sCR0);\n\treg |= ARM_SMMU_sCR0_CLIENTPD;\n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sCR0, reg);\n\n\tlist_for_each_entry(e, &rmr_list, list) {\n\t\tstruct iommu_iort_rmr_data *rmr;\n\t\tint i;\n\n\t\trmr = container_of(e, struct iommu_iort_rmr_data, rr);\n\t\tfor (i = 0; i < rmr->num_sids; i++) {\n\t\t\tidx = arm_smmu_find_sme(smmu, rmr->sids[i], ~0);\n\t\t\tif (idx < 0)\n\t\t\t\tcontinue;\n\n\t\t\tif (smmu->s2crs[idx].count == 0) {\n\t\t\t\tsmmu->smrs[idx].id = rmr->sids[i];\n\t\t\t\tsmmu->smrs[idx].mask = 0;\n\t\t\t\tsmmu->smrs[idx].valid = true;\n\t\t\t}\n\t\t\tsmmu->s2crs[idx].count++;\n\t\t\tsmmu->s2crs[idx].type = S2CR_TYPE_BYPASS;\n\t\t\tsmmu->s2crs[idx].privcfg = S2CR_PRIVCFG_DEFAULT;\n\n\t\t\tcnt++;\n\t\t}\n\t}\n\n\tdev_notice(smmu->dev, \"\\tpreserved %d boot mapping%s\\n\", cnt,\n\t\t   cnt == 1 ? \"\" : \"s\");\n\tiort_put_rmr_sids(dev_fwnode(smmu->dev), &rmr_list);\n}\n\nstatic int arm_smmu_device_probe(struct platform_device *pdev)\n{\n\tstruct resource *res;\n\tstruct arm_smmu_device *smmu;\n\tstruct device *dev = &pdev->dev;\n\tint num_irqs, i, err;\n\tu32 global_irqs, pmu_irqs;\n\tirqreturn_t (*global_fault)(int irq, void *dev);\n\n\tsmmu = devm_kzalloc(dev, sizeof(*smmu), GFP_KERNEL);\n\tif (!smmu) {\n\t\tdev_err(dev, \"failed to allocate arm_smmu_device\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tsmmu->dev = dev;\n\n\tif (dev->of_node)\n\t\terr = arm_smmu_device_dt_probe(smmu, &global_irqs, &pmu_irqs);\n\telse\n\t\terr = arm_smmu_device_acpi_probe(smmu, &global_irqs, &pmu_irqs);\n\tif (err)\n\t\treturn err;\n\n\tsmmu->base = devm_platform_get_and_ioremap_resource(pdev, 0, &res);\n\tif (IS_ERR(smmu->base))\n\t\treturn PTR_ERR(smmu->base);\n\tsmmu->ioaddr = res->start;\n\n\t \n\tsmmu->numpage = resource_size(res);\n\n\tsmmu = arm_smmu_impl_init(smmu);\n\tif (IS_ERR(smmu))\n\t\treturn PTR_ERR(smmu);\n\n\tnum_irqs = platform_irq_count(pdev);\n\n\tsmmu->num_context_irqs = num_irqs - global_irqs - pmu_irqs;\n\tif (smmu->num_context_irqs <= 0)\n\t\treturn dev_err_probe(dev, -ENODEV,\n\t\t\t\t\"found %d interrupts but expected at least %d\\n\",\n\t\t\t\tnum_irqs, global_irqs + pmu_irqs + 1);\n\n\tsmmu->irqs = devm_kcalloc(dev, smmu->num_context_irqs,\n\t\t\t\t  sizeof(*smmu->irqs), GFP_KERNEL);\n\tif (!smmu->irqs)\n\t\treturn dev_err_probe(dev, -ENOMEM, \"failed to allocate %d irqs\\n\",\n\t\t\t\t     smmu->num_context_irqs);\n\n\tfor (i = 0; i < smmu->num_context_irqs; i++) {\n\t\tint irq = platform_get_irq(pdev, global_irqs + pmu_irqs + i);\n\n\t\tif (irq < 0)\n\t\t\treturn irq;\n\t\tsmmu->irqs[i] = irq;\n\t}\n\n\terr = devm_clk_bulk_get_all(dev, &smmu->clks);\n\tif (err < 0) {\n\t\tdev_err(dev, \"failed to get clocks %d\\n\", err);\n\t\treturn err;\n\t}\n\tsmmu->num_clks = err;\n\n\terr = clk_bulk_prepare_enable(smmu->num_clks, smmu->clks);\n\tif (err)\n\t\treturn err;\n\n\terr = arm_smmu_device_cfg_probe(smmu);\n\tif (err)\n\t\treturn err;\n\n\tif (smmu->version == ARM_SMMU_V2) {\n\t\tif (smmu->num_context_banks > smmu->num_context_irqs) {\n\t\t\tdev_err(dev,\n\t\t\t      \"found only %d context irq(s) but %d required\\n\",\n\t\t\t      smmu->num_context_irqs, smmu->num_context_banks);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\t \n\t\tsmmu->num_context_irqs = smmu->num_context_banks;\n\t}\n\n\tif (smmu->impl && smmu->impl->global_fault)\n\t\tglobal_fault = smmu->impl->global_fault;\n\telse\n\t\tglobal_fault = arm_smmu_global_fault;\n\n\tfor (i = 0; i < global_irqs; i++) {\n\t\tint irq = platform_get_irq(pdev, i);\n\n\t\tif (irq < 0)\n\t\t\treturn irq;\n\n\t\terr = devm_request_irq(dev, irq, global_fault, IRQF_SHARED,\n\t\t\t\t       \"arm-smmu global fault\", smmu);\n\t\tif (err)\n\t\t\treturn dev_err_probe(dev, err,\n\t\t\t\t\t\"failed to request global IRQ %d (%u)\\n\",\n\t\t\t\t\ti, irq);\n\t}\n\n\terr = iommu_device_sysfs_add(&smmu->iommu, smmu->dev, NULL,\n\t\t\t\t     \"smmu.%pa\", &smmu->ioaddr);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register iommu in sysfs\\n\");\n\t\treturn err;\n\t}\n\n\terr = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register iommu\\n\");\n\t\tiommu_device_sysfs_remove(&smmu->iommu);\n\t\treturn err;\n\t}\n\n\tplatform_set_drvdata(pdev, smmu);\n\n\t \n\tarm_smmu_rmr_install_bypass_smr(smmu);\n\n\tarm_smmu_device_reset(smmu);\n\tarm_smmu_test_smr_masks(smmu);\n\n\t \n\tif (dev->pm_domain) {\n\t\tpm_runtime_set_active(dev);\n\t\tpm_runtime_enable(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic void arm_smmu_device_shutdown(struct platform_device *pdev)\n{\n\tstruct arm_smmu_device *smmu = platform_get_drvdata(pdev);\n\n\tif (!bitmap_empty(smmu->context_map, ARM_SMMU_MAX_CBS))\n\t\tdev_notice(&pdev->dev, \"disabling translation\\n\");\n\n\tarm_smmu_rpm_get(smmu);\n\t \n\tarm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sCR0, ARM_SMMU_sCR0_CLIENTPD);\n\tarm_smmu_rpm_put(smmu);\n\n\tif (pm_runtime_enabled(smmu->dev))\n\t\tpm_runtime_force_suspend(smmu->dev);\n\telse\n\t\tclk_bulk_disable(smmu->num_clks, smmu->clks);\n\n\tclk_bulk_unprepare(smmu->num_clks, smmu->clks);\n}\n\nstatic void arm_smmu_device_remove(struct platform_device *pdev)\n{\n\tstruct arm_smmu_device *smmu = platform_get_drvdata(pdev);\n\n\tiommu_device_unregister(&smmu->iommu);\n\tiommu_device_sysfs_remove(&smmu->iommu);\n\n\tarm_smmu_device_shutdown(pdev);\n}\n\nstatic int __maybe_unused arm_smmu_runtime_resume(struct device *dev)\n{\n\tstruct arm_smmu_device *smmu = dev_get_drvdata(dev);\n\tint ret;\n\n\tret = clk_bulk_enable(smmu->num_clks, smmu->clks);\n\tif (ret)\n\t\treturn ret;\n\n\tarm_smmu_device_reset(smmu);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused arm_smmu_runtime_suspend(struct device *dev)\n{\n\tstruct arm_smmu_device *smmu = dev_get_drvdata(dev);\n\n\tclk_bulk_disable(smmu->num_clks, smmu->clks);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused arm_smmu_pm_resume(struct device *dev)\n{\n\tint ret;\n\tstruct arm_smmu_device *smmu = dev_get_drvdata(dev);\n\n\tret = clk_bulk_prepare(smmu->num_clks, smmu->clks);\n\tif (ret)\n\t\treturn ret;\n\n\tif (pm_runtime_suspended(dev))\n\t\treturn 0;\n\n\tret = arm_smmu_runtime_resume(dev);\n\tif (ret)\n\t\tclk_bulk_unprepare(smmu->num_clks, smmu->clks);\n\n\treturn ret;\n}\n\nstatic int __maybe_unused arm_smmu_pm_suspend(struct device *dev)\n{\n\tint ret = 0;\n\tstruct arm_smmu_device *smmu = dev_get_drvdata(dev);\n\n\tif (pm_runtime_suspended(dev))\n\t\tgoto clk_unprepare;\n\n\tret = arm_smmu_runtime_suspend(dev);\n\tif (ret)\n\t\treturn ret;\n\nclk_unprepare:\n\tclk_bulk_unprepare(smmu->num_clks, smmu->clks);\n\treturn ret;\n}\n\nstatic const struct dev_pm_ops arm_smmu_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(arm_smmu_pm_suspend, arm_smmu_pm_resume)\n\tSET_RUNTIME_PM_OPS(arm_smmu_runtime_suspend,\n\t\t\t   arm_smmu_runtime_resume, NULL)\n};\n\nstatic struct platform_driver arm_smmu_driver = {\n\t.driver\t= {\n\t\t.name\t\t\t= \"arm-smmu\",\n\t\t.of_match_table\t\t= arm_smmu_of_match,\n\t\t.pm\t\t\t= &arm_smmu_pm_ops,\n\t\t.suppress_bind_attrs    = true,\n\t},\n\t.probe\t= arm_smmu_device_probe,\n\t.remove_new = arm_smmu_device_remove,\n\t.shutdown = arm_smmu_device_shutdown,\n};\nmodule_platform_driver(arm_smmu_driver);\n\nMODULE_DESCRIPTION(\"IOMMU API for ARM architected SMMU implementations\");\nMODULE_AUTHOR(\"Will Deacon <will@kernel.org>\");\nMODULE_ALIAS(\"platform:arm-smmu\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}