{
  "module_name": "arm-smmu-nvidia.c",
  "hash_id": "a3722a9a3920d382f58f3d6c26451ffd079a1ebff0ff3d07e8809cc7426c9fc2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/arm/arm-smmu/arm-smmu-nvidia.c",
  "human_readable_source": "\n\n\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n\n#include <soc/tegra/mc.h>\n\n#include \"arm-smmu.h\"\n\n \n#define MAX_SMMU_INSTANCES 2\n\nstruct nvidia_smmu {\n\tstruct arm_smmu_device smmu;\n\tvoid __iomem *bases[MAX_SMMU_INSTANCES];\n\tunsigned int num_instances;\n\tstruct tegra_mc *mc;\n};\n\nstatic inline struct nvidia_smmu *to_nvidia_smmu(struct arm_smmu_device *smmu)\n{\n\treturn container_of(smmu, struct nvidia_smmu, smmu);\n}\n\nstatic inline void __iomem *nvidia_smmu_page(struct arm_smmu_device *smmu,\n\t\t\t\t\t     unsigned int inst, int page)\n{\n\tstruct nvidia_smmu *nvidia_smmu;\n\n\tnvidia_smmu = container_of(smmu, struct nvidia_smmu, smmu);\n\treturn nvidia_smmu->bases[inst] + (page << smmu->pgshift);\n}\n\nstatic u32 nvidia_smmu_read_reg(struct arm_smmu_device *smmu,\n\t\t\t\tint page, int offset)\n{\n\tvoid __iomem *reg = nvidia_smmu_page(smmu, 0, page) + offset;\n\n\treturn readl_relaxed(reg);\n}\n\nstatic void nvidia_smmu_write_reg(struct arm_smmu_device *smmu,\n\t\t\t\t  int page, int offset, u32 val)\n{\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\tunsigned int i;\n\n\tfor (i = 0; i < nvidia->num_instances; i++) {\n\t\tvoid __iomem *reg = nvidia_smmu_page(smmu, i, page) + offset;\n\n\t\twritel_relaxed(val, reg);\n\t}\n}\n\nstatic u64 nvidia_smmu_read_reg64(struct arm_smmu_device *smmu,\n\t\t\t\t  int page, int offset)\n{\n\tvoid __iomem *reg = nvidia_smmu_page(smmu, 0, page) + offset;\n\n\treturn readq_relaxed(reg);\n}\n\nstatic void nvidia_smmu_write_reg64(struct arm_smmu_device *smmu,\n\t\t\t\t    int page, int offset, u64 val)\n{\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\tunsigned int i;\n\n\tfor (i = 0; i < nvidia->num_instances; i++) {\n\t\tvoid __iomem *reg = nvidia_smmu_page(smmu, i, page) + offset;\n\n\t\twriteq_relaxed(val, reg);\n\t}\n}\n\nstatic void nvidia_smmu_tlb_sync(struct arm_smmu_device *smmu, int page,\n\t\t\t\t int sync, int status)\n{\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\tunsigned int delay;\n\n\tarm_smmu_writel(smmu, page, sync, 0);\n\n\tfor (delay = 1; delay < TLB_LOOP_TIMEOUT; delay *= 2) {\n\t\tunsigned int spin_cnt;\n\n\t\tfor (spin_cnt = TLB_SPIN_COUNT; spin_cnt > 0; spin_cnt--) {\n\t\t\tu32 val = 0;\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < nvidia->num_instances; i++) {\n\t\t\t\tvoid __iomem *reg;\n\n\t\t\t\treg = nvidia_smmu_page(smmu, i, page) + status;\n\t\t\t\tval |= readl_relaxed(reg);\n\t\t\t}\n\n\t\t\tif (!(val & ARM_SMMU_sTLBGSTATUS_GSACTIVE))\n\t\t\t\treturn;\n\n\t\t\tcpu_relax();\n\t\t}\n\n\t\tudelay(delay);\n\t}\n\n\tdev_err_ratelimited(smmu->dev,\n\t\t\t    \"TLB sync timed out -- SMMU may be deadlocked\\n\");\n}\n\nstatic int nvidia_smmu_reset(struct arm_smmu_device *smmu)\n{\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\tunsigned int i;\n\n\tfor (i = 0; i < nvidia->num_instances; i++) {\n\t\tu32 val;\n\t\tvoid __iomem *reg = nvidia_smmu_page(smmu, i, ARM_SMMU_GR0) +\n\t\t\t\t    ARM_SMMU_GR0_sGFSR;\n\n\t\t \n\t\tval = readl_relaxed(reg);\n\t\twritel_relaxed(val, reg);\n\t}\n\n\treturn 0;\n}\n\nstatic irqreturn_t nvidia_smmu_global_fault_inst(int irq,\n\t\t\t\t\t\t struct arm_smmu_device *smmu,\n\t\t\t\t\t\t int inst)\n{\n\tu32 gfsr, gfsynr0, gfsynr1, gfsynr2;\n\tvoid __iomem *gr0_base = nvidia_smmu_page(smmu, inst, 0);\n\n\tgfsr = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSR);\n\tif (!gfsr)\n\t\treturn IRQ_NONE;\n\n\tgfsynr0 = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSYNR0);\n\tgfsynr1 = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSYNR1);\n\tgfsynr2 = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSYNR2);\n\n\tdev_err_ratelimited(smmu->dev,\n\t\t\t    \"Unexpected global fault, this could be serious\\n\");\n\tdev_err_ratelimited(smmu->dev,\n\t\t\t    \"\\tGFSR 0x%08x, GFSYNR0 0x%08x, GFSYNR1 0x%08x, GFSYNR2 0x%08x\\n\",\n\t\t\t    gfsr, gfsynr0, gfsynr1, gfsynr2);\n\n\twritel_relaxed(gfsr, gr0_base + ARM_SMMU_GR0_sGFSR);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nvidia_smmu_global_fault(int irq, void *dev)\n{\n\tunsigned int inst;\n\tirqreturn_t ret = IRQ_NONE;\n\tstruct arm_smmu_device *smmu = dev;\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\n\tfor (inst = 0; inst < nvidia->num_instances; inst++) {\n\t\tirqreturn_t irq_ret;\n\n\t\tirq_ret = nvidia_smmu_global_fault_inst(irq, smmu, inst);\n\t\tif (irq_ret == IRQ_HANDLED)\n\t\t\tret = IRQ_HANDLED;\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t nvidia_smmu_context_fault_bank(int irq,\n\t\t\t\t\t\t  struct arm_smmu_device *smmu,\n\t\t\t\t\t\t  int idx, int inst)\n{\n\tu32 fsr, fsynr, cbfrsynra;\n\tunsigned long iova;\n\tvoid __iomem *gr1_base = nvidia_smmu_page(smmu, inst, 1);\n\tvoid __iomem *cb_base = nvidia_smmu_page(smmu, inst, smmu->numpage + idx);\n\n\tfsr = readl_relaxed(cb_base + ARM_SMMU_CB_FSR);\n\tif (!(fsr & ARM_SMMU_FSR_FAULT))\n\t\treturn IRQ_NONE;\n\n\tfsynr = readl_relaxed(cb_base + ARM_SMMU_CB_FSYNR0);\n\tiova = readq_relaxed(cb_base + ARM_SMMU_CB_FAR);\n\tcbfrsynra = readl_relaxed(gr1_base + ARM_SMMU_GR1_CBFRSYNRA(idx));\n\n\tdev_err_ratelimited(smmu->dev,\n\t\t\t    \"Unhandled context fault: fsr=0x%x, iova=0x%08lx, fsynr=0x%x, cbfrsynra=0x%x, cb=%d\\n\",\n\t\t\t    fsr, iova, fsynr, cbfrsynra, idx);\n\n\twritel_relaxed(fsr, cb_base + ARM_SMMU_CB_FSR);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nvidia_smmu_context_fault(int irq, void *dev)\n{\n\tint idx;\n\tunsigned int inst;\n\tirqreturn_t ret = IRQ_NONE;\n\tstruct arm_smmu_device *smmu;\n\tstruct iommu_domain *domain = dev;\n\tstruct arm_smmu_domain *smmu_domain;\n\tstruct nvidia_smmu *nvidia;\n\n\tsmmu_domain = container_of(domain, struct arm_smmu_domain, domain);\n\tsmmu = smmu_domain->smmu;\n\tnvidia = to_nvidia_smmu(smmu);\n\n\tfor (inst = 0; inst < nvidia->num_instances; inst++) {\n\t\tirqreturn_t irq_ret;\n\n\t\t \n\t\tfor (idx = 0; idx < smmu->num_context_banks; idx++) {\n\t\t\tirq_ret = nvidia_smmu_context_fault_bank(irq, smmu,\n\t\t\t\t\t\t\t\t idx, inst);\n\t\t\tif (irq_ret == IRQ_HANDLED)\n\t\t\t\tret = IRQ_HANDLED;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void nvidia_smmu_probe_finalize(struct arm_smmu_device *smmu, struct device *dev)\n{\n\tstruct nvidia_smmu *nvidia = to_nvidia_smmu(smmu);\n\tint err;\n\n\terr = tegra_mc_probe_device(nvidia->mc, dev);\n\tif (err < 0)\n\t\tdev_err(smmu->dev, \"memory controller probe failed for %s: %d\\n\",\n\t\t\tdev_name(dev), err);\n}\n\nstatic int nvidia_smmu_init_context(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t    struct io_pgtable_cfg *pgtbl_cfg,\n\t\t\t\t    struct device *dev)\n{\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tconst struct device_node *np = smmu->dev->of_node;\n\n\t \n\tif (of_device_is_compatible(np, \"nvidia,tegra234-smmu\") ||\n\t    of_device_is_compatible(np, \"nvidia,tegra194-smmu\")) {\n\t\tsmmu->pgsize_bitmap = PAGE_SIZE;\n\t\tpgtbl_cfg->pgsize_bitmap = smmu->pgsize_bitmap;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct arm_smmu_impl nvidia_smmu_impl = {\n\t.read_reg = nvidia_smmu_read_reg,\n\t.write_reg = nvidia_smmu_write_reg,\n\t.read_reg64 = nvidia_smmu_read_reg64,\n\t.write_reg64 = nvidia_smmu_write_reg64,\n\t.reset = nvidia_smmu_reset,\n\t.tlb_sync = nvidia_smmu_tlb_sync,\n\t.global_fault = nvidia_smmu_global_fault,\n\t.context_fault = nvidia_smmu_context_fault,\n\t.probe_finalize = nvidia_smmu_probe_finalize,\n\t.init_context = nvidia_smmu_init_context,\n};\n\nstatic const struct arm_smmu_impl nvidia_smmu_single_impl = {\n\t.probe_finalize = nvidia_smmu_probe_finalize,\n\t.init_context = nvidia_smmu_init_context,\n};\n\nstruct arm_smmu_device *nvidia_smmu_impl_init(struct arm_smmu_device *smmu)\n{\n\tstruct resource *res;\n\tstruct device *dev = smmu->dev;\n\tstruct nvidia_smmu *nvidia_smmu;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tunsigned int i;\n\n\tnvidia_smmu = devm_krealloc(dev, smmu, sizeof(*nvidia_smmu), GFP_KERNEL);\n\tif (!nvidia_smmu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnvidia_smmu->mc = devm_tegra_memory_controller_get(dev);\n\tif (IS_ERR(nvidia_smmu->mc))\n\t\treturn ERR_CAST(nvidia_smmu->mc);\n\n\t \n\tnvidia_smmu->bases[0] = smmu->base;\n\tnvidia_smmu->num_instances++;\n\n\tfor (i = 1; i < MAX_SMMU_INSTANCES; i++) {\n\t\tres = platform_get_resource(pdev, IORESOURCE_MEM, i);\n\t\tif (!res)\n\t\t\tbreak;\n\n\t\tnvidia_smmu->bases[i] = devm_ioremap_resource(dev, res);\n\t\tif (IS_ERR(nvidia_smmu->bases[i]))\n\t\t\treturn ERR_CAST(nvidia_smmu->bases[i]);\n\n\t\tnvidia_smmu->num_instances++;\n\t}\n\n\tif (nvidia_smmu->num_instances == 1)\n\t\tnvidia_smmu->smmu.impl = &nvidia_smmu_single_impl;\n\telse\n\t\tnvidia_smmu->smmu.impl = &nvidia_smmu_impl;\n\n\treturn &nvidia_smmu->smmu;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}