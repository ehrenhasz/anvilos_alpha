{
  "module_name": "arm-smmu-v3.c",
  "hash_id": "a9b68416629aca00e1c0991f30738fb96a2a10a1d57e845af73e5444cbbb478d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/acpi_iort.h>\n#include <linux/bitops.h>\n#include <linux/crash_dump.h>\n#include <linux/delay.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/io-pgtable.h>\n#include <linux/iopoll.h>\n#include <linux/module.h>\n#include <linux/msi.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_platform.h>\n#include <linux/pci.h>\n#include <linux/pci-ats.h>\n#include <linux/platform_device.h>\n\n#include \"arm-smmu-v3.h\"\n#include \"../../dma-iommu.h\"\n#include \"../../iommu-sva.h\"\n\nstatic bool disable_bypass = true;\nmodule_param(disable_bypass, bool, 0444);\nMODULE_PARM_DESC(disable_bypass,\n\t\"Disable bypass streams such that incoming transactions from devices that are not attached to an iommu domain will report an abort back to the device and will not be allowed to pass through the SMMU.\");\n\nstatic bool disable_msipolling;\nmodule_param(disable_msipolling, bool, 0444);\nMODULE_PARM_DESC(disable_msipolling,\n\t\"Disable MSI-based polling for CMD_SYNC completion.\");\n\nenum arm_smmu_msi_index {\n\tEVTQ_MSI_INDEX,\n\tGERROR_MSI_INDEX,\n\tPRIQ_MSI_INDEX,\n\tARM_SMMU_MAX_MSIS,\n};\n\nstatic phys_addr_t arm_smmu_msi_cfg[ARM_SMMU_MAX_MSIS][3] = {\n\t[EVTQ_MSI_INDEX] = {\n\t\tARM_SMMU_EVTQ_IRQ_CFG0,\n\t\tARM_SMMU_EVTQ_IRQ_CFG1,\n\t\tARM_SMMU_EVTQ_IRQ_CFG2,\n\t},\n\t[GERROR_MSI_INDEX] = {\n\t\tARM_SMMU_GERROR_IRQ_CFG0,\n\t\tARM_SMMU_GERROR_IRQ_CFG1,\n\t\tARM_SMMU_GERROR_IRQ_CFG2,\n\t},\n\t[PRIQ_MSI_INDEX] = {\n\t\tARM_SMMU_PRIQ_IRQ_CFG0,\n\t\tARM_SMMU_PRIQ_IRQ_CFG1,\n\t\tARM_SMMU_PRIQ_IRQ_CFG2,\n\t},\n};\n\nstruct arm_smmu_option_prop {\n\tu32 opt;\n\tconst char *prop;\n};\n\nDEFINE_XARRAY_ALLOC1(arm_smmu_asid_xa);\nDEFINE_MUTEX(arm_smmu_asid_lock);\n\n \nstruct arm_smmu_ctx_desc quiet_cd = { 0 };\n\nstatic struct arm_smmu_option_prop arm_smmu_options[] = {\n\t{ ARM_SMMU_OPT_SKIP_PREFETCH, \"hisilicon,broken-prefetch-cmd\" },\n\t{ ARM_SMMU_OPT_PAGE0_REGS_ONLY, \"cavium,cn9900-broken-page1-regspace\"},\n\t{ 0, NULL},\n};\n\nstatic void parse_driver_options(struct arm_smmu_device *smmu)\n{\n\tint i = 0;\n\n\tdo {\n\t\tif (of_property_read_bool(smmu->dev->of_node,\n\t\t\t\t\t\tarm_smmu_options[i].prop)) {\n\t\t\tsmmu->options |= arm_smmu_options[i].opt;\n\t\t\tdev_notice(smmu->dev, \"option %s\\n\",\n\t\t\t\tarm_smmu_options[i].prop);\n\t\t}\n\t} while (arm_smmu_options[++i].opt);\n}\n\n \nstatic bool queue_has_space(struct arm_smmu_ll_queue *q, u32 n)\n{\n\tu32 space, prod, cons;\n\n\tprod = Q_IDX(q, q->prod);\n\tcons = Q_IDX(q, q->cons);\n\n\tif (Q_WRP(q, q->prod) == Q_WRP(q, q->cons))\n\t\tspace = (1 << q->max_n_shift) - (prod - cons);\n\telse\n\t\tspace = cons - prod;\n\n\treturn space >= n;\n}\n\nstatic bool queue_full(struct arm_smmu_ll_queue *q)\n{\n\treturn Q_IDX(q, q->prod) == Q_IDX(q, q->cons) &&\n\t       Q_WRP(q, q->prod) != Q_WRP(q, q->cons);\n}\n\nstatic bool queue_empty(struct arm_smmu_ll_queue *q)\n{\n\treturn Q_IDX(q, q->prod) == Q_IDX(q, q->cons) &&\n\t       Q_WRP(q, q->prod) == Q_WRP(q, q->cons);\n}\n\nstatic bool queue_consumed(struct arm_smmu_ll_queue *q, u32 prod)\n{\n\treturn ((Q_WRP(q, q->cons) == Q_WRP(q, prod)) &&\n\t\t(Q_IDX(q, q->cons) > Q_IDX(q, prod))) ||\n\t       ((Q_WRP(q, q->cons) != Q_WRP(q, prod)) &&\n\t\t(Q_IDX(q, q->cons) <= Q_IDX(q, prod)));\n}\n\nstatic void queue_sync_cons_out(struct arm_smmu_queue *q)\n{\n\t \n\t__iomb();\n\twritel_relaxed(q->llq.cons, q->cons_reg);\n}\n\nstatic void queue_inc_cons(struct arm_smmu_ll_queue *q)\n{\n\tu32 cons = (Q_WRP(q, q->cons) | Q_IDX(q, q->cons)) + 1;\n\tq->cons = Q_OVF(q->cons) | Q_WRP(q, cons) | Q_IDX(q, cons);\n}\n\nstatic void queue_sync_cons_ovf(struct arm_smmu_queue *q)\n{\n\tstruct arm_smmu_ll_queue *llq = &q->llq;\n\n\tif (likely(Q_OVF(llq->prod) == Q_OVF(llq->cons)))\n\t\treturn;\n\n\tllq->cons = Q_OVF(llq->prod) | Q_WRP(llq, llq->cons) |\n\t\t      Q_IDX(llq, llq->cons);\n\tqueue_sync_cons_out(q);\n}\n\nstatic int queue_sync_prod_in(struct arm_smmu_queue *q)\n{\n\tu32 prod;\n\tint ret = 0;\n\n\t \n\tprod = readl(q->prod_reg);\n\n\tif (Q_OVF(prod) != Q_OVF(q->llq.prod))\n\t\tret = -EOVERFLOW;\n\n\tq->llq.prod = prod;\n\treturn ret;\n}\n\nstatic u32 queue_inc_prod_n(struct arm_smmu_ll_queue *q, int n)\n{\n\tu32 prod = (Q_WRP(q, q->prod) | Q_IDX(q, q->prod)) + n;\n\treturn Q_OVF(q->prod) | Q_WRP(q, prod) | Q_IDX(q, prod);\n}\n\nstatic void queue_poll_init(struct arm_smmu_device *smmu,\n\t\t\t    struct arm_smmu_queue_poll *qp)\n{\n\tqp->delay = 1;\n\tqp->spin_cnt = 0;\n\tqp->wfe = !!(smmu->features & ARM_SMMU_FEAT_SEV);\n\tqp->timeout = ktime_add_us(ktime_get(), ARM_SMMU_POLL_TIMEOUT_US);\n}\n\nstatic int queue_poll(struct arm_smmu_queue_poll *qp)\n{\n\tif (ktime_compare(ktime_get(), qp->timeout) > 0)\n\t\treturn -ETIMEDOUT;\n\n\tif (qp->wfe) {\n\t\twfe();\n\t} else if (++qp->spin_cnt < ARM_SMMU_POLL_SPIN_COUNT) {\n\t\tcpu_relax();\n\t} else {\n\t\tudelay(qp->delay);\n\t\tqp->delay *= 2;\n\t\tqp->spin_cnt = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic void queue_write(__le64 *dst, u64 *src, size_t n_dwords)\n{\n\tint i;\n\n\tfor (i = 0; i < n_dwords; ++i)\n\t\t*dst++ = cpu_to_le64(*src++);\n}\n\nstatic void queue_read(u64 *dst, __le64 *src, size_t n_dwords)\n{\n\tint i;\n\n\tfor (i = 0; i < n_dwords; ++i)\n\t\t*dst++ = le64_to_cpu(*src++);\n}\n\nstatic int queue_remove_raw(struct arm_smmu_queue *q, u64 *ent)\n{\n\tif (queue_empty(&q->llq))\n\t\treturn -EAGAIN;\n\n\tqueue_read(ent, Q_ENT(q, q->llq.cons), q->ent_dwords);\n\tqueue_inc_cons(&q->llq);\n\tqueue_sync_cons_out(q);\n\treturn 0;\n}\n\n \nstatic int arm_smmu_cmdq_build_cmd(u64 *cmd, struct arm_smmu_cmdq_ent *ent)\n{\n\tmemset(cmd, 0, 1 << CMDQ_ENT_SZ_SHIFT);\n\tcmd[0] |= FIELD_PREP(CMDQ_0_OP, ent->opcode);\n\n\tswitch (ent->opcode) {\n\tcase CMDQ_OP_TLBI_EL2_ALL:\n\tcase CMDQ_OP_TLBI_NSNH_ALL:\n\t\tbreak;\n\tcase CMDQ_OP_PREFETCH_CFG:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_PREFETCH_0_SID, ent->prefetch.sid);\n\t\tbreak;\n\tcase CMDQ_OP_CFGI_CD:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_CFGI_0_SSID, ent->cfgi.ssid);\n\t\tfallthrough;\n\tcase CMDQ_OP_CFGI_STE:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_CFGI_0_SID, ent->cfgi.sid);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_CFGI_1_LEAF, ent->cfgi.leaf);\n\t\tbreak;\n\tcase CMDQ_OP_CFGI_CD_ALL:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_CFGI_0_SID, ent->cfgi.sid);\n\t\tbreak;\n\tcase CMDQ_OP_CFGI_ALL:\n\t\t \n\t\tcmd[1] |= FIELD_PREP(CMDQ_CFGI_1_RANGE, 31);\n\t\tbreak;\n\tcase CMDQ_OP_TLBI_NH_VA:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);\n\t\tfallthrough;\n\tcase CMDQ_OP_TLBI_EL2_VA:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_NUM, ent->tlbi.num);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_SCALE, ent->tlbi.scale);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_TTL, ent->tlbi.ttl);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_TG, ent->tlbi.tg);\n\t\tcmd[1] |= ent->tlbi.addr & CMDQ_TLBI_1_VA_MASK;\n\t\tbreak;\n\tcase CMDQ_OP_TLBI_S2_IPA:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_NUM, ent->tlbi.num);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_SCALE, ent->tlbi.scale);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_LEAF, ent->tlbi.leaf);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_TTL, ent->tlbi.ttl);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_TLBI_1_TG, ent->tlbi.tg);\n\t\tcmd[1] |= ent->tlbi.addr & CMDQ_TLBI_1_IPA_MASK;\n\t\tbreak;\n\tcase CMDQ_OP_TLBI_NH_ASID:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);\n\t\tfallthrough;\n\tcase CMDQ_OP_TLBI_S12_VMALL:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_VMID, ent->tlbi.vmid);\n\t\tbreak;\n\tcase CMDQ_OP_TLBI_EL2_ASID:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_TLBI_0_ASID, ent->tlbi.asid);\n\t\tbreak;\n\tcase CMDQ_OP_ATC_INV:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_ATC_0_GLOBAL, ent->atc.global);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_ATC_0_SSID, ent->atc.ssid);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_ATC_0_SID, ent->atc.sid);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_ATC_1_SIZE, ent->atc.size);\n\t\tcmd[1] |= ent->atc.addr & CMDQ_ATC_1_ADDR_MASK;\n\t\tbreak;\n\tcase CMDQ_OP_PRI_RESP:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_0_SSV, ent->substream_valid);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_PRI_0_SSID, ent->pri.ssid);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_PRI_0_SID, ent->pri.sid);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_PRI_1_GRPID, ent->pri.grpid);\n\t\tswitch (ent->pri.resp) {\n\t\tcase PRI_RESP_DENY:\n\t\tcase PRI_RESP_FAIL:\n\t\tcase PRI_RESP_SUCC:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcmd[1] |= FIELD_PREP(CMDQ_PRI_1_RESP, ent->pri.resp);\n\t\tbreak;\n\tcase CMDQ_OP_RESUME:\n\t\tcmd[0] |= FIELD_PREP(CMDQ_RESUME_0_SID, ent->resume.sid);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_RESUME_0_RESP, ent->resume.resp);\n\t\tcmd[1] |= FIELD_PREP(CMDQ_RESUME_1_STAG, ent->resume.stag);\n\t\tbreak;\n\tcase CMDQ_OP_CMD_SYNC:\n\t\tif (ent->sync.msiaddr) {\n\t\t\tcmd[0] |= FIELD_PREP(CMDQ_SYNC_0_CS, CMDQ_SYNC_0_CS_IRQ);\n\t\t\tcmd[1] |= ent->sync.msiaddr & CMDQ_SYNC_1_MSIADDR_MASK;\n\t\t} else {\n\t\t\tcmd[0] |= FIELD_PREP(CMDQ_SYNC_0_CS, CMDQ_SYNC_0_CS_SEV);\n\t\t}\n\t\tcmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSH, ARM_SMMU_SH_ISH);\n\t\tcmd[0] |= FIELD_PREP(CMDQ_SYNC_0_MSIATTR, ARM_SMMU_MEMATTR_OIWB);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\treturn 0;\n}\n\nstatic struct arm_smmu_cmdq *arm_smmu_get_cmdq(struct arm_smmu_device *smmu)\n{\n\treturn &smmu->cmdq;\n}\n\nstatic void arm_smmu_cmdq_build_sync_cmd(u64 *cmd, struct arm_smmu_device *smmu,\n\t\t\t\t\t struct arm_smmu_queue *q, u32 prod)\n{\n\tstruct arm_smmu_cmdq_ent ent = {\n\t\t.opcode = CMDQ_OP_CMD_SYNC,\n\t};\n\n\t \n\tif (smmu->options & ARM_SMMU_OPT_MSIPOLL) {\n\t\tent.sync.msiaddr = q->base_dma + Q_IDX(&q->llq, prod) *\n\t\t\t\t   q->ent_dwords * 8;\n\t}\n\n\tarm_smmu_cmdq_build_cmd(cmd, &ent);\n}\n\nstatic void __arm_smmu_cmdq_skip_err(struct arm_smmu_device *smmu,\n\t\t\t\t     struct arm_smmu_queue *q)\n{\n\tstatic const char * const cerror_str[] = {\n\t\t[CMDQ_ERR_CERROR_NONE_IDX]\t= \"No error\",\n\t\t[CMDQ_ERR_CERROR_ILL_IDX]\t= \"Illegal command\",\n\t\t[CMDQ_ERR_CERROR_ABT_IDX]\t= \"Abort on command fetch\",\n\t\t[CMDQ_ERR_CERROR_ATC_INV_IDX]\t= \"ATC invalidate timeout\",\n\t};\n\n\tint i;\n\tu64 cmd[CMDQ_ENT_DWORDS];\n\tu32 cons = readl_relaxed(q->cons_reg);\n\tu32 idx = FIELD_GET(CMDQ_CONS_ERR, cons);\n\tstruct arm_smmu_cmdq_ent cmd_sync = {\n\t\t.opcode = CMDQ_OP_CMD_SYNC,\n\t};\n\n\tdev_err(smmu->dev, \"CMDQ error (cons 0x%08x): %s\\n\", cons,\n\t\tidx < ARRAY_SIZE(cerror_str) ?  cerror_str[idx] : \"Unknown\");\n\n\tswitch (idx) {\n\tcase CMDQ_ERR_CERROR_ABT_IDX:\n\t\tdev_err(smmu->dev, \"retrying command fetch\\n\");\n\t\treturn;\n\tcase CMDQ_ERR_CERROR_NONE_IDX:\n\t\treturn;\n\tcase CMDQ_ERR_CERROR_ATC_INV_IDX:\n\t\t \n\t\treturn;\n\tcase CMDQ_ERR_CERROR_ILL_IDX:\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tqueue_read(cmd, Q_ENT(q, cons), q->ent_dwords);\n\tdev_err(smmu->dev, \"skipping command in error state:\\n\");\n\tfor (i = 0; i < ARRAY_SIZE(cmd); ++i)\n\t\tdev_err(smmu->dev, \"\\t0x%016llx\\n\", (unsigned long long)cmd[i]);\n\n\t \n\tarm_smmu_cmdq_build_cmd(cmd, &cmd_sync);\n\n\tqueue_write(Q_ENT(q, cons), cmd, q->ent_dwords);\n}\n\nstatic void arm_smmu_cmdq_skip_err(struct arm_smmu_device *smmu)\n{\n\t__arm_smmu_cmdq_skip_err(smmu, &smmu->cmdq.q);\n}\n\n \nstatic void arm_smmu_cmdq_shared_lock(struct arm_smmu_cmdq *cmdq)\n{\n\tint val;\n\n\t \n\tif (atomic_fetch_inc_relaxed(&cmdq->lock) >= 0)\n\t\treturn;\n\n\tdo {\n\t\tval = atomic_cond_read_relaxed(&cmdq->lock, VAL >= 0);\n\t} while (atomic_cmpxchg_relaxed(&cmdq->lock, val, val + 1) != val);\n}\n\nstatic void arm_smmu_cmdq_shared_unlock(struct arm_smmu_cmdq *cmdq)\n{\n\t(void)atomic_dec_return_release(&cmdq->lock);\n}\n\nstatic bool arm_smmu_cmdq_shared_tryunlock(struct arm_smmu_cmdq *cmdq)\n{\n\tif (atomic_read(&cmdq->lock) == 1)\n\t\treturn false;\n\n\tarm_smmu_cmdq_shared_unlock(cmdq);\n\treturn true;\n}\n\n#define arm_smmu_cmdq_exclusive_trylock_irqsave(cmdq, flags)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool __ret;\t\t\t\t\t\t\t\\\n\tlocal_irq_save(flags);\t\t\t\t\t\t\\\n\t__ret = !atomic_cmpxchg_relaxed(&cmdq->lock, 0, INT_MIN);\t\\\n\tif (!__ret)\t\t\t\t\t\t\t\\\n\t\tlocal_irq_restore(flags);\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define arm_smmu_cmdq_exclusive_unlock_irqrestore(cmdq, flags)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tatomic_set_release(&cmdq->lock, 0);\t\t\t\t\\\n\tlocal_irq_restore(flags);\t\t\t\t\t\\\n})\n\n\n \nstatic void __arm_smmu_cmdq_poll_set_valid_map(struct arm_smmu_cmdq *cmdq,\n\t\t\t\t\t       u32 sprod, u32 eprod, bool set)\n{\n\tu32 swidx, sbidx, ewidx, ebidx;\n\tstruct arm_smmu_ll_queue llq = {\n\t\t.max_n_shift\t= cmdq->q.llq.max_n_shift,\n\t\t.prod\t\t= sprod,\n\t};\n\n\tewidx = BIT_WORD(Q_IDX(&llq, eprod));\n\tebidx = Q_IDX(&llq, eprod) % BITS_PER_LONG;\n\n\twhile (llq.prod != eprod) {\n\t\tunsigned long mask;\n\t\tatomic_long_t *ptr;\n\t\tu32 limit = BITS_PER_LONG;\n\n\t\tswidx = BIT_WORD(Q_IDX(&llq, llq.prod));\n\t\tsbidx = Q_IDX(&llq, llq.prod) % BITS_PER_LONG;\n\n\t\tptr = &cmdq->valid_map[swidx];\n\n\t\tif ((swidx == ewidx) && (sbidx < ebidx))\n\t\t\tlimit = ebidx;\n\n\t\tmask = GENMASK(limit - 1, sbidx);\n\n\t\t \n\t\tif (set) {\n\t\t\tatomic_long_xor(mask, ptr);\n\t\t} else {  \n\t\t\tunsigned long valid;\n\n\t\t\tvalid = (ULONG_MAX + !!Q_WRP(&llq, llq.prod)) & mask;\n\t\t\tatomic_long_cond_read_relaxed(ptr, (VAL & mask) == valid);\n\t\t}\n\n\t\tllq.prod = queue_inc_prod_n(&llq, limit - sbidx);\n\t}\n}\n\n \nstatic void arm_smmu_cmdq_set_valid_map(struct arm_smmu_cmdq *cmdq,\n\t\t\t\t\tu32 sprod, u32 eprod)\n{\n\t__arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, true);\n}\n\n \nstatic void arm_smmu_cmdq_poll_valid_map(struct arm_smmu_cmdq *cmdq,\n\t\t\t\t\t u32 sprod, u32 eprod)\n{\n\t__arm_smmu_cmdq_poll_set_valid_map(cmdq, sprod, eprod, false);\n}\n\n \nstatic int arm_smmu_cmdq_poll_until_not_full(struct arm_smmu_device *smmu,\n\t\t\t\t\t     struct arm_smmu_ll_queue *llq)\n{\n\tunsigned long flags;\n\tstruct arm_smmu_queue_poll qp;\n\tstruct arm_smmu_cmdq *cmdq = arm_smmu_get_cmdq(smmu);\n\tint ret = 0;\n\n\t \n\tif (arm_smmu_cmdq_exclusive_trylock_irqsave(cmdq, flags)) {\n\t\tWRITE_ONCE(cmdq->q.llq.cons, readl_relaxed(cmdq->q.cons_reg));\n\t\tarm_smmu_cmdq_exclusive_unlock_irqrestore(cmdq, flags);\n\t\tllq->val = READ_ONCE(cmdq->q.llq.val);\n\t\treturn 0;\n\t}\n\n\tqueue_poll_init(smmu, &qp);\n\tdo {\n\t\tllq->val = READ_ONCE(cmdq->q.llq.val);\n\t\tif (!queue_full(llq))\n\t\t\tbreak;\n\n\t\tret = queue_poll(&qp);\n\t} while (!ret);\n\n\treturn ret;\n}\n\n \nstatic int __arm_smmu_cmdq_poll_until_msi(struct arm_smmu_device *smmu,\n\t\t\t\t\t  struct arm_smmu_ll_queue *llq)\n{\n\tint ret = 0;\n\tstruct arm_smmu_queue_poll qp;\n\tstruct arm_smmu_cmdq *cmdq = arm_smmu_get_cmdq(smmu);\n\tu32 *cmd = (u32 *)(Q_ENT(&cmdq->q, llq->prod));\n\n\tqueue_poll_init(smmu, &qp);\n\n\t \n\tqp.wfe = false;\n\tsmp_cond_load_relaxed(cmd, !VAL || (ret = queue_poll(&qp)));\n\tllq->cons = ret ? llq->prod : queue_inc_prod_n(llq, 1);\n\treturn ret;\n}\n\n \nstatic int __arm_smmu_cmdq_poll_until_consumed(struct arm_smmu_device *smmu,\n\t\t\t\t\t       struct arm_smmu_ll_queue *llq)\n{\n\tstruct arm_smmu_queue_poll qp;\n\tstruct arm_smmu_cmdq *cmdq = arm_smmu_get_cmdq(smmu);\n\tu32 prod = llq->prod;\n\tint ret = 0;\n\n\tqueue_poll_init(smmu, &qp);\n\tllq->val = READ_ONCE(cmdq->q.llq.val);\n\tdo {\n\t\tif (queue_consumed(llq, prod))\n\t\t\tbreak;\n\n\t\tret = queue_poll(&qp);\n\n\t\t \n\t\tllq->cons = readl(cmdq->q.cons_reg);\n\t} while (!ret);\n\n\treturn ret;\n}\n\nstatic int arm_smmu_cmdq_poll_until_sync(struct arm_smmu_device *smmu,\n\t\t\t\t\t struct arm_smmu_ll_queue *llq)\n{\n\tif (smmu->options & ARM_SMMU_OPT_MSIPOLL)\n\t\treturn __arm_smmu_cmdq_poll_until_msi(smmu, llq);\n\n\treturn __arm_smmu_cmdq_poll_until_consumed(smmu, llq);\n}\n\nstatic void arm_smmu_cmdq_write_entries(struct arm_smmu_cmdq *cmdq, u64 *cmds,\n\t\t\t\t\tu32 prod, int n)\n{\n\tint i;\n\tstruct arm_smmu_ll_queue llq = {\n\t\t.max_n_shift\t= cmdq->q.llq.max_n_shift,\n\t\t.prod\t\t= prod,\n\t};\n\n\tfor (i = 0; i < n; ++i) {\n\t\tu64 *cmd = &cmds[i * CMDQ_ENT_DWORDS];\n\n\t\tprod = queue_inc_prod_n(&llq, i);\n\t\tqueue_write(Q_ENT(&cmdq->q, prod), cmd, CMDQ_ENT_DWORDS);\n\t}\n}\n\n \nstatic int arm_smmu_cmdq_issue_cmdlist(struct arm_smmu_device *smmu,\n\t\t\t\t       u64 *cmds, int n, bool sync)\n{\n\tu64 cmd_sync[CMDQ_ENT_DWORDS];\n\tu32 prod;\n\tunsigned long flags;\n\tbool owner;\n\tstruct arm_smmu_cmdq *cmdq = arm_smmu_get_cmdq(smmu);\n\tstruct arm_smmu_ll_queue llq, head;\n\tint ret = 0;\n\n\tllq.max_n_shift = cmdq->q.llq.max_n_shift;\n\n\t \n\tlocal_irq_save(flags);\n\tllq.val = READ_ONCE(cmdq->q.llq.val);\n\tdo {\n\t\tu64 old;\n\n\t\twhile (!queue_has_space(&llq, n + sync)) {\n\t\t\tlocal_irq_restore(flags);\n\t\t\tif (arm_smmu_cmdq_poll_until_not_full(smmu, &llq))\n\t\t\t\tdev_err_ratelimited(smmu->dev, \"CMDQ timeout\\n\");\n\t\t\tlocal_irq_save(flags);\n\t\t}\n\n\t\thead.cons = llq.cons;\n\t\thead.prod = queue_inc_prod_n(&llq, n + sync) |\n\t\t\t\t\t     CMDQ_PROD_OWNED_FLAG;\n\n\t\told = cmpxchg_relaxed(&cmdq->q.llq.val, llq.val, head.val);\n\t\tif (old == llq.val)\n\t\t\tbreak;\n\n\t\tllq.val = old;\n\t} while (1);\n\towner = !(llq.prod & CMDQ_PROD_OWNED_FLAG);\n\thead.prod &= ~CMDQ_PROD_OWNED_FLAG;\n\tllq.prod &= ~CMDQ_PROD_OWNED_FLAG;\n\n\t \n\tarm_smmu_cmdq_write_entries(cmdq, cmds, llq.prod, n);\n\tif (sync) {\n\t\tprod = queue_inc_prod_n(&llq, n);\n\t\tarm_smmu_cmdq_build_sync_cmd(cmd_sync, smmu, &cmdq->q, prod);\n\t\tqueue_write(Q_ENT(&cmdq->q, prod), cmd_sync, CMDQ_ENT_DWORDS);\n\n\t\t \n\t\tarm_smmu_cmdq_shared_lock(cmdq);\n\t}\n\n\t \n\tdma_wmb();\n\tarm_smmu_cmdq_set_valid_map(cmdq, llq.prod, head.prod);\n\n\t \n\tif (owner) {\n\t\t \n\t\tatomic_cond_read_relaxed(&cmdq->owner_prod, VAL == llq.prod);\n\n\t\t \n\t\tprod = atomic_fetch_andnot_relaxed(CMDQ_PROD_OWNED_FLAG,\n\t\t\t\t\t\t   &cmdq->q.llq.atomic.prod);\n\t\tprod &= ~CMDQ_PROD_OWNED_FLAG;\n\n\t\t \n\t\tarm_smmu_cmdq_poll_valid_map(cmdq, llq.prod, prod);\n\n\t\t \n\t\twritel_relaxed(prod, cmdq->q.prod_reg);\n\n\t\t \n\t\tatomic_set_release(&cmdq->owner_prod, prod);\n\t}\n\n\t \n\tif (sync) {\n\t\tllq.prod = queue_inc_prod_n(&llq, n);\n\t\tret = arm_smmu_cmdq_poll_until_sync(smmu, &llq);\n\t\tif (ret) {\n\t\t\tdev_err_ratelimited(smmu->dev,\n\t\t\t\t\t    \"CMD_SYNC timeout at 0x%08x [hwprod 0x%08x, hwcons 0x%08x]\\n\",\n\t\t\t\t\t    llq.prod,\n\t\t\t\t\t    readl_relaxed(cmdq->q.prod_reg),\n\t\t\t\t\t    readl_relaxed(cmdq->q.cons_reg));\n\t\t}\n\n\t\t \n\t\tif (!arm_smmu_cmdq_shared_tryunlock(cmdq)) {\n\t\t\tWRITE_ONCE(cmdq->q.llq.cons, llq.cons);\n\t\t\tarm_smmu_cmdq_shared_unlock(cmdq);\n\t\t}\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\nstatic int __arm_smmu_cmdq_issue_cmd(struct arm_smmu_device *smmu,\n\t\t\t\t     struct arm_smmu_cmdq_ent *ent,\n\t\t\t\t     bool sync)\n{\n\tu64 cmd[CMDQ_ENT_DWORDS];\n\n\tif (unlikely(arm_smmu_cmdq_build_cmd(cmd, ent))) {\n\t\tdev_warn(smmu->dev, \"ignoring unknown CMDQ opcode 0x%x\\n\",\n\t\t\t ent->opcode);\n\t\treturn -EINVAL;\n\t}\n\n\treturn arm_smmu_cmdq_issue_cmdlist(smmu, cmd, 1, sync);\n}\n\nstatic int arm_smmu_cmdq_issue_cmd(struct arm_smmu_device *smmu,\n\t\t\t\t   struct arm_smmu_cmdq_ent *ent)\n{\n\treturn __arm_smmu_cmdq_issue_cmd(smmu, ent, false);\n}\n\nstatic int arm_smmu_cmdq_issue_cmd_with_sync(struct arm_smmu_device *smmu,\n\t\t\t\t\t     struct arm_smmu_cmdq_ent *ent)\n{\n\treturn __arm_smmu_cmdq_issue_cmd(smmu, ent, true);\n}\n\nstatic void arm_smmu_cmdq_batch_add(struct arm_smmu_device *smmu,\n\t\t\t\t    struct arm_smmu_cmdq_batch *cmds,\n\t\t\t\t    struct arm_smmu_cmdq_ent *cmd)\n{\n\tint index;\n\n\tif (cmds->num == CMDQ_BATCH_ENTRIES - 1 &&\n\t    (smmu->options & ARM_SMMU_OPT_CMDQ_FORCE_SYNC)) {\n\t\tarm_smmu_cmdq_issue_cmdlist(smmu, cmds->cmds, cmds->num, true);\n\t\tcmds->num = 0;\n\t}\n\n\tif (cmds->num == CMDQ_BATCH_ENTRIES) {\n\t\tarm_smmu_cmdq_issue_cmdlist(smmu, cmds->cmds, cmds->num, false);\n\t\tcmds->num = 0;\n\t}\n\n\tindex = cmds->num * CMDQ_ENT_DWORDS;\n\tif (unlikely(arm_smmu_cmdq_build_cmd(&cmds->cmds[index], cmd))) {\n\t\tdev_warn(smmu->dev, \"ignoring unknown CMDQ opcode 0x%x\\n\",\n\t\t\t cmd->opcode);\n\t\treturn;\n\t}\n\n\tcmds->num++;\n}\n\nstatic int arm_smmu_cmdq_batch_submit(struct arm_smmu_device *smmu,\n\t\t\t\t      struct arm_smmu_cmdq_batch *cmds)\n{\n\treturn arm_smmu_cmdq_issue_cmdlist(smmu, cmds->cmds, cmds->num, true);\n}\n\nstatic int arm_smmu_page_response(struct device *dev,\n\t\t\t\t  struct iommu_fault_event *unused,\n\t\t\t\t  struct iommu_page_response *resp)\n{\n\tstruct arm_smmu_cmdq_ent cmd = {0};\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\tint sid = master->streams[0].id;\n\n\tif (master->stall_enabled) {\n\t\tcmd.opcode\t\t= CMDQ_OP_RESUME;\n\t\tcmd.resume.sid\t\t= sid;\n\t\tcmd.resume.stag\t\t= resp->grpid;\n\t\tswitch (resp->code) {\n\t\tcase IOMMU_PAGE_RESP_INVALID:\n\t\tcase IOMMU_PAGE_RESP_FAILURE:\n\t\t\tcmd.resume.resp = CMDQ_RESUME_0_RESP_ABORT;\n\t\t\tbreak;\n\t\tcase IOMMU_PAGE_RESP_SUCCESS:\n\t\t\tcmd.resume.resp = CMDQ_RESUME_0_RESP_RETRY;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\treturn -ENODEV;\n\t}\n\n\tarm_smmu_cmdq_issue_cmd(master->smmu, &cmd);\n\t \n\n\treturn 0;\n}\n\n \nvoid arm_smmu_tlb_inv_asid(struct arm_smmu_device *smmu, u16 asid)\n{\n\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t.opcode\t= smmu->features & ARM_SMMU_FEAT_E2H ?\n\t\t\tCMDQ_OP_TLBI_EL2_ASID : CMDQ_OP_TLBI_NH_ASID,\n\t\t.tlbi.asid = asid,\n\t};\n\n\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n}\n\nstatic void arm_smmu_sync_cd(struct arm_smmu_domain *smmu_domain,\n\t\t\t     int ssid, bool leaf)\n{\n\tsize_t i;\n\tunsigned long flags;\n\tstruct arm_smmu_master *master;\n\tstruct arm_smmu_cmdq_batch cmds;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t.opcode\t= CMDQ_OP_CFGI_CD,\n\t\t.cfgi\t= {\n\t\t\t.ssid\t= ssid,\n\t\t\t.leaf\t= leaf,\n\t\t},\n\t};\n\n\tcmds.num = 0;\n\n\tspin_lock_irqsave(&smmu_domain->devices_lock, flags);\n\tlist_for_each_entry(master, &smmu_domain->devices, domain_head) {\n\t\tfor (i = 0; i < master->num_streams; i++) {\n\t\t\tcmd.cfgi.sid = master->streams[i].id;\n\t\t\tarm_smmu_cmdq_batch_add(smmu, &cmds, &cmd);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&smmu_domain->devices_lock, flags);\n\n\tarm_smmu_cmdq_batch_submit(smmu, &cmds);\n}\n\nstatic int arm_smmu_alloc_cd_leaf_table(struct arm_smmu_device *smmu,\n\t\t\t\t\tstruct arm_smmu_l1_ctx_desc *l1_desc)\n{\n\tsize_t size = CTXDESC_L2_ENTRIES * (CTXDESC_CD_DWORDS << 3);\n\n\tl1_desc->l2ptr = dmam_alloc_coherent(smmu->dev, size,\n\t\t\t\t\t     &l1_desc->l2ptr_dma, GFP_KERNEL);\n\tif (!l1_desc->l2ptr) {\n\t\tdev_warn(smmu->dev,\n\t\t\t \"failed to allocate context descriptor table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void arm_smmu_write_cd_l1_desc(__le64 *dst,\n\t\t\t\t      struct arm_smmu_l1_ctx_desc *l1_desc)\n{\n\tu64 val = (l1_desc->l2ptr_dma & CTXDESC_L1_DESC_L2PTR_MASK) |\n\t\t  CTXDESC_L1_DESC_V;\n\n\t \n\tWRITE_ONCE(*dst, cpu_to_le64(val));\n}\n\nstatic __le64 *arm_smmu_get_cd_ptr(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t   u32 ssid)\n{\n\t__le64 *l1ptr;\n\tunsigned int idx;\n\tstruct arm_smmu_l1_ctx_desc *l1_desc;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_ctx_desc_cfg *cdcfg = &smmu_domain->s1_cfg.cdcfg;\n\n\tif (smmu_domain->s1_cfg.s1fmt == STRTAB_STE_0_S1FMT_LINEAR)\n\t\treturn cdcfg->cdtab + ssid * CTXDESC_CD_DWORDS;\n\n\tidx = ssid >> CTXDESC_SPLIT;\n\tl1_desc = &cdcfg->l1_desc[idx];\n\tif (!l1_desc->l2ptr) {\n\t\tif (arm_smmu_alloc_cd_leaf_table(smmu, l1_desc))\n\t\t\treturn NULL;\n\n\t\tl1ptr = cdcfg->cdtab + idx * CTXDESC_L1_DESC_DWORDS;\n\t\tarm_smmu_write_cd_l1_desc(l1ptr, l1_desc);\n\t\t \n\t\tarm_smmu_sync_cd(smmu_domain, ssid, false);\n\t}\n\tidx = ssid & (CTXDESC_L2_ENTRIES - 1);\n\treturn l1_desc->l2ptr + idx * CTXDESC_CD_DWORDS;\n}\n\nint arm_smmu_write_ctx_desc(struct arm_smmu_domain *smmu_domain, int ssid,\n\t\t\t    struct arm_smmu_ctx_desc *cd)\n{\n\t \n\tu64 val;\n\tbool cd_live;\n\t__le64 *cdptr;\n\n\tif (WARN_ON(ssid >= (1 << smmu_domain->s1_cfg.s1cdmax)))\n\t\treturn -E2BIG;\n\n\tcdptr = arm_smmu_get_cd_ptr(smmu_domain, ssid);\n\tif (!cdptr)\n\t\treturn -ENOMEM;\n\n\tval = le64_to_cpu(cdptr[0]);\n\tcd_live = !!(val & CTXDESC_CD_0_V);\n\n\tif (!cd) {  \n\t\tval = 0;\n\t} else if (cd == &quiet_cd) {  \n\t\tval |= CTXDESC_CD_0_TCR_EPD0;\n\t} else if (cd_live) {  \n\t\tval &= ~CTXDESC_CD_0_ASID;\n\t\tval |= FIELD_PREP(CTXDESC_CD_0_ASID, cd->asid);\n\t\t \n\t} else {  \n\t\tcdptr[1] = cpu_to_le64(cd->ttbr & CTXDESC_CD_1_TTB0_MASK);\n\t\tcdptr[2] = 0;\n\t\tcdptr[3] = cpu_to_le64(cd->mair);\n\n\t\t \n\t\tarm_smmu_sync_cd(smmu_domain, ssid, true);\n\n\t\tval = cd->tcr |\n#ifdef __BIG_ENDIAN\n\t\t\tCTXDESC_CD_0_ENDI |\n#endif\n\t\t\tCTXDESC_CD_0_R | CTXDESC_CD_0_A |\n\t\t\t(cd->mm ? 0 : CTXDESC_CD_0_ASET) |\n\t\t\tCTXDESC_CD_0_AA64 |\n\t\t\tFIELD_PREP(CTXDESC_CD_0_ASID, cd->asid) |\n\t\t\tCTXDESC_CD_0_V;\n\n\t\tif (smmu_domain->stall_enabled)\n\t\t\tval |= CTXDESC_CD_0_S;\n\t}\n\n\t \n\tWRITE_ONCE(cdptr[0], cpu_to_le64(val));\n\tarm_smmu_sync_cd(smmu_domain, ssid, true);\n\treturn 0;\n}\n\nstatic int arm_smmu_alloc_cd_tables(struct arm_smmu_domain *smmu_domain)\n{\n\tint ret;\n\tsize_t l1size;\n\tsize_t max_contexts;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_s1_cfg *cfg = &smmu_domain->s1_cfg;\n\tstruct arm_smmu_ctx_desc_cfg *cdcfg = &cfg->cdcfg;\n\n\tmax_contexts = 1 << cfg->s1cdmax;\n\n\tif (!(smmu->features & ARM_SMMU_FEAT_2_LVL_CDTAB) ||\n\t    max_contexts <= CTXDESC_L2_ENTRIES) {\n\t\tcfg->s1fmt = STRTAB_STE_0_S1FMT_LINEAR;\n\t\tcdcfg->num_l1_ents = max_contexts;\n\n\t\tl1size = max_contexts * (CTXDESC_CD_DWORDS << 3);\n\t} else {\n\t\tcfg->s1fmt = STRTAB_STE_0_S1FMT_64K_L2;\n\t\tcdcfg->num_l1_ents = DIV_ROUND_UP(max_contexts,\n\t\t\t\t\t\t  CTXDESC_L2_ENTRIES);\n\n\t\tcdcfg->l1_desc = devm_kcalloc(smmu->dev, cdcfg->num_l1_ents,\n\t\t\t\t\t      sizeof(*cdcfg->l1_desc),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!cdcfg->l1_desc)\n\t\t\treturn -ENOMEM;\n\n\t\tl1size = cdcfg->num_l1_ents * (CTXDESC_L1_DESC_DWORDS << 3);\n\t}\n\n\tcdcfg->cdtab = dmam_alloc_coherent(smmu->dev, l1size, &cdcfg->cdtab_dma,\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!cdcfg->cdtab) {\n\t\tdev_warn(smmu->dev, \"failed to allocate context descriptor\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_free_l1;\n\t}\n\n\treturn 0;\n\nerr_free_l1:\n\tif (cdcfg->l1_desc) {\n\t\tdevm_kfree(smmu->dev, cdcfg->l1_desc);\n\t\tcdcfg->l1_desc = NULL;\n\t}\n\treturn ret;\n}\n\nstatic void arm_smmu_free_cd_tables(struct arm_smmu_domain *smmu_domain)\n{\n\tint i;\n\tsize_t size, l1size;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_ctx_desc_cfg *cdcfg = &smmu_domain->s1_cfg.cdcfg;\n\n\tif (cdcfg->l1_desc) {\n\t\tsize = CTXDESC_L2_ENTRIES * (CTXDESC_CD_DWORDS << 3);\n\n\t\tfor (i = 0; i < cdcfg->num_l1_ents; i++) {\n\t\t\tif (!cdcfg->l1_desc[i].l2ptr)\n\t\t\t\tcontinue;\n\n\t\t\tdmam_free_coherent(smmu->dev, size,\n\t\t\t\t\t   cdcfg->l1_desc[i].l2ptr,\n\t\t\t\t\t   cdcfg->l1_desc[i].l2ptr_dma);\n\t\t}\n\t\tdevm_kfree(smmu->dev, cdcfg->l1_desc);\n\t\tcdcfg->l1_desc = NULL;\n\n\t\tl1size = cdcfg->num_l1_ents * (CTXDESC_L1_DESC_DWORDS << 3);\n\t} else {\n\t\tl1size = cdcfg->num_l1_ents * (CTXDESC_CD_DWORDS << 3);\n\t}\n\n\tdmam_free_coherent(smmu->dev, l1size, cdcfg->cdtab, cdcfg->cdtab_dma);\n\tcdcfg->cdtab_dma = 0;\n\tcdcfg->cdtab = NULL;\n}\n\nbool arm_smmu_free_asid(struct arm_smmu_ctx_desc *cd)\n{\n\tbool free;\n\tstruct arm_smmu_ctx_desc *old_cd;\n\n\tif (!cd->asid)\n\t\treturn false;\n\n\tfree = refcount_dec_and_test(&cd->refs);\n\tif (free) {\n\t\told_cd = xa_erase(&arm_smmu_asid_xa, cd->asid);\n\t\tWARN_ON(old_cd != cd);\n\t}\n\treturn free;\n}\n\n \nstatic void\narm_smmu_write_strtab_l1_desc(__le64 *dst, struct arm_smmu_strtab_l1_desc *desc)\n{\n\tu64 val = 0;\n\n\tval |= FIELD_PREP(STRTAB_L1_DESC_SPAN, desc->span);\n\tval |= desc->l2ptr_dma & STRTAB_L1_DESC_L2PTR_MASK;\n\n\t \n\tWRITE_ONCE(*dst, cpu_to_le64(val));\n}\n\nstatic void arm_smmu_sync_ste_for_sid(struct arm_smmu_device *smmu, u32 sid)\n{\n\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t.opcode\t= CMDQ_OP_CFGI_STE,\n\t\t.cfgi\t= {\n\t\t\t.sid\t= sid,\n\t\t\t.leaf\t= true,\n\t\t},\n\t};\n\n\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n}\n\nstatic void arm_smmu_write_strtab_ent(struct arm_smmu_master *master, u32 sid,\n\t\t\t\t      __le64 *dst)\n{\n\t \n\tu64 val = le64_to_cpu(dst[0]);\n\tbool ste_live = false;\n\tstruct arm_smmu_device *smmu = NULL;\n\tstruct arm_smmu_s1_cfg *s1_cfg = NULL;\n\tstruct arm_smmu_s2_cfg *s2_cfg = NULL;\n\tstruct arm_smmu_domain *smmu_domain = NULL;\n\tstruct arm_smmu_cmdq_ent prefetch_cmd = {\n\t\t.opcode\t\t= CMDQ_OP_PREFETCH_CFG,\n\t\t.prefetch\t= {\n\t\t\t.sid\t= sid,\n\t\t},\n\t};\n\n\tif (master) {\n\t\tsmmu_domain = master->domain;\n\t\tsmmu = master->smmu;\n\t}\n\n\tif (smmu_domain) {\n\t\tswitch (smmu_domain->stage) {\n\t\tcase ARM_SMMU_DOMAIN_S1:\n\t\t\ts1_cfg = &smmu_domain->s1_cfg;\n\t\t\tbreak;\n\t\tcase ARM_SMMU_DOMAIN_S2:\n\t\tcase ARM_SMMU_DOMAIN_NESTED:\n\t\t\ts2_cfg = &smmu_domain->s2_cfg;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (val & STRTAB_STE_0_V) {\n\t\tswitch (FIELD_GET(STRTAB_STE_0_CFG, val)) {\n\t\tcase STRTAB_STE_0_CFG_BYPASS:\n\t\t\tbreak;\n\t\tcase STRTAB_STE_0_CFG_S1_TRANS:\n\t\tcase STRTAB_STE_0_CFG_S2_TRANS:\n\t\t\tste_live = true;\n\t\t\tbreak;\n\t\tcase STRTAB_STE_0_CFG_ABORT:\n\t\t\tBUG_ON(!disable_bypass);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();  \n\t\t}\n\t}\n\n\t \n\tval = STRTAB_STE_0_V;\n\n\t \n\tif (!smmu_domain || !(s1_cfg || s2_cfg)) {\n\t\tif (!smmu_domain && disable_bypass)\n\t\t\tval |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_ABORT);\n\t\telse\n\t\t\tval |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_BYPASS);\n\n\t\tdst[0] = cpu_to_le64(val);\n\t\tdst[1] = cpu_to_le64(FIELD_PREP(STRTAB_STE_1_SHCFG,\n\t\t\t\t\t\tSTRTAB_STE_1_SHCFG_INCOMING));\n\t\tdst[2] = 0;  \n\t\t \n\t\tif (smmu)\n\t\t\tarm_smmu_sync_ste_for_sid(smmu, sid);\n\t\treturn;\n\t}\n\n\tif (s1_cfg) {\n\t\tu64 strw = smmu->features & ARM_SMMU_FEAT_E2H ?\n\t\t\tSTRTAB_STE_1_STRW_EL2 : STRTAB_STE_1_STRW_NSEL1;\n\n\t\tBUG_ON(ste_live);\n\t\tdst[1] = cpu_to_le64(\n\t\t\t FIELD_PREP(STRTAB_STE_1_S1DSS, STRTAB_STE_1_S1DSS_SSID0) |\n\t\t\t FIELD_PREP(STRTAB_STE_1_S1CIR, STRTAB_STE_1_S1C_CACHE_WBRA) |\n\t\t\t FIELD_PREP(STRTAB_STE_1_S1COR, STRTAB_STE_1_S1C_CACHE_WBRA) |\n\t\t\t FIELD_PREP(STRTAB_STE_1_S1CSH, ARM_SMMU_SH_ISH) |\n\t\t\t FIELD_PREP(STRTAB_STE_1_STRW, strw));\n\n\t\tif (smmu->features & ARM_SMMU_FEAT_STALLS &&\n\t\t    !master->stall_enabled)\n\t\t\tdst[1] |= cpu_to_le64(STRTAB_STE_1_S1STALLD);\n\n\t\tval |= (s1_cfg->cdcfg.cdtab_dma & STRTAB_STE_0_S1CTXPTR_MASK) |\n\t\t\tFIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_S1_TRANS) |\n\t\t\tFIELD_PREP(STRTAB_STE_0_S1CDMAX, s1_cfg->s1cdmax) |\n\t\t\tFIELD_PREP(STRTAB_STE_0_S1FMT, s1_cfg->s1fmt);\n\t}\n\n\tif (s2_cfg) {\n\t\tBUG_ON(ste_live);\n\t\tdst[2] = cpu_to_le64(\n\t\t\t FIELD_PREP(STRTAB_STE_2_S2VMID, s2_cfg->vmid) |\n\t\t\t FIELD_PREP(STRTAB_STE_2_VTCR, s2_cfg->vtcr) |\n#ifdef __BIG_ENDIAN\n\t\t\t STRTAB_STE_2_S2ENDI |\n#endif\n\t\t\t STRTAB_STE_2_S2PTW | STRTAB_STE_2_S2AA64 |\n\t\t\t STRTAB_STE_2_S2R);\n\n\t\tdst[3] = cpu_to_le64(s2_cfg->vttbr & STRTAB_STE_3_S2TTB_MASK);\n\n\t\tval |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_S2_TRANS);\n\t}\n\n\tif (master->ats_enabled)\n\t\tdst[1] |= cpu_to_le64(FIELD_PREP(STRTAB_STE_1_EATS,\n\t\t\t\t\t\t STRTAB_STE_1_EATS_TRANS));\n\n\tarm_smmu_sync_ste_for_sid(smmu, sid);\n\t \n\tWRITE_ONCE(dst[0], cpu_to_le64(val));\n\tarm_smmu_sync_ste_for_sid(smmu, sid);\n\n\t \n\tif (!(smmu->options & ARM_SMMU_OPT_SKIP_PREFETCH))\n\t\tarm_smmu_cmdq_issue_cmd(smmu, &prefetch_cmd);\n}\n\nstatic void arm_smmu_init_bypass_stes(__le64 *strtab, unsigned int nent, bool force)\n{\n\tunsigned int i;\n\tu64 val = STRTAB_STE_0_V;\n\n\tif (disable_bypass && !force)\n\t\tval |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_ABORT);\n\telse\n\t\tval |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_BYPASS);\n\n\tfor (i = 0; i < nent; ++i) {\n\t\tstrtab[0] = cpu_to_le64(val);\n\t\tstrtab[1] = cpu_to_le64(FIELD_PREP(STRTAB_STE_1_SHCFG,\n\t\t\t\t\t\t   STRTAB_STE_1_SHCFG_INCOMING));\n\t\tstrtab[2] = 0;\n\t\tstrtab += STRTAB_STE_DWORDS;\n\t}\n}\n\nstatic int arm_smmu_init_l2_strtab(struct arm_smmu_device *smmu, u32 sid)\n{\n\tsize_t size;\n\tvoid *strtab;\n\tstruct arm_smmu_strtab_cfg *cfg = &smmu->strtab_cfg;\n\tstruct arm_smmu_strtab_l1_desc *desc = &cfg->l1_desc[sid >> STRTAB_SPLIT];\n\n\tif (desc->l2ptr)\n\t\treturn 0;\n\n\tsize = 1 << (STRTAB_SPLIT + ilog2(STRTAB_STE_DWORDS) + 3);\n\tstrtab = &cfg->strtab[(sid >> STRTAB_SPLIT) * STRTAB_L1_DESC_DWORDS];\n\n\tdesc->span = STRTAB_SPLIT + 1;\n\tdesc->l2ptr = dmam_alloc_coherent(smmu->dev, size, &desc->l2ptr_dma,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!desc->l2ptr) {\n\t\tdev_err(smmu->dev,\n\t\t\t\"failed to allocate l2 stream table for SID %u\\n\",\n\t\t\tsid);\n\t\treturn -ENOMEM;\n\t}\n\n\tarm_smmu_init_bypass_stes(desc->l2ptr, 1 << STRTAB_SPLIT, false);\n\tarm_smmu_write_strtab_l1_desc(strtab, desc);\n\treturn 0;\n}\n\nstatic struct arm_smmu_master *\narm_smmu_find_master(struct arm_smmu_device *smmu, u32 sid)\n{\n\tstruct rb_node *node;\n\tstruct arm_smmu_stream *stream;\n\n\tlockdep_assert_held(&smmu->streams_mutex);\n\n\tnode = smmu->streams.rb_node;\n\twhile (node) {\n\t\tstream = rb_entry(node, struct arm_smmu_stream, node);\n\t\tif (stream->id < sid)\n\t\t\tnode = node->rb_right;\n\t\telse if (stream->id > sid)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\t\treturn stream->master;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int arm_smmu_handle_evt(struct arm_smmu_device *smmu, u64 *evt)\n{\n\tint ret;\n\tu32 reason;\n\tu32 perm = 0;\n\tstruct arm_smmu_master *master;\n\tbool ssid_valid = evt[0] & EVTQ_0_SSV;\n\tu32 sid = FIELD_GET(EVTQ_0_SID, evt[0]);\n\tstruct iommu_fault_event fault_evt = { };\n\tstruct iommu_fault *flt = &fault_evt.fault;\n\n\tswitch (FIELD_GET(EVTQ_0_ID, evt[0])) {\n\tcase EVT_ID_TRANSLATION_FAULT:\n\t\treason = IOMMU_FAULT_REASON_PTE_FETCH;\n\t\tbreak;\n\tcase EVT_ID_ADDR_SIZE_FAULT:\n\t\treason = IOMMU_FAULT_REASON_OOR_ADDRESS;\n\t\tbreak;\n\tcase EVT_ID_ACCESS_FAULT:\n\t\treason = IOMMU_FAULT_REASON_ACCESS;\n\t\tbreak;\n\tcase EVT_ID_PERMISSION_FAULT:\n\t\treason = IOMMU_FAULT_REASON_PERMISSION;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (evt[1] & EVTQ_1_S2)\n\t\treturn -EFAULT;\n\n\tif (evt[1] & EVTQ_1_RnW)\n\t\tperm |= IOMMU_FAULT_PERM_READ;\n\telse\n\t\tperm |= IOMMU_FAULT_PERM_WRITE;\n\n\tif (evt[1] & EVTQ_1_InD)\n\t\tperm |= IOMMU_FAULT_PERM_EXEC;\n\n\tif (evt[1] & EVTQ_1_PnU)\n\t\tperm |= IOMMU_FAULT_PERM_PRIV;\n\n\tif (evt[1] & EVTQ_1_STALL) {\n\t\tflt->type = IOMMU_FAULT_PAGE_REQ;\n\t\tflt->prm = (struct iommu_fault_page_request) {\n\t\t\t.flags = IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE,\n\t\t\t.grpid = FIELD_GET(EVTQ_1_STAG, evt[1]),\n\t\t\t.perm = perm,\n\t\t\t.addr = FIELD_GET(EVTQ_2_ADDR, evt[2]),\n\t\t};\n\n\t\tif (ssid_valid) {\n\t\t\tflt->prm.flags |= IOMMU_FAULT_PAGE_REQUEST_PASID_VALID;\n\t\t\tflt->prm.pasid = FIELD_GET(EVTQ_0_SSID, evt[0]);\n\t\t}\n\t} else {\n\t\tflt->type = IOMMU_FAULT_DMA_UNRECOV;\n\t\tflt->event = (struct iommu_fault_unrecoverable) {\n\t\t\t.reason = reason,\n\t\t\t.flags = IOMMU_FAULT_UNRECOV_ADDR_VALID,\n\t\t\t.perm = perm,\n\t\t\t.addr = FIELD_GET(EVTQ_2_ADDR, evt[2]),\n\t\t};\n\n\t\tif (ssid_valid) {\n\t\t\tflt->event.flags |= IOMMU_FAULT_UNRECOV_PASID_VALID;\n\t\t\tflt->event.pasid = FIELD_GET(EVTQ_0_SSID, evt[0]);\n\t\t}\n\t}\n\n\tmutex_lock(&smmu->streams_mutex);\n\tmaster = arm_smmu_find_master(smmu, sid);\n\tif (!master) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tret = iommu_report_device_fault(master->dev, &fault_evt);\n\tif (ret && flt->type == IOMMU_FAULT_PAGE_REQ) {\n\t\t \n\t\tstruct iommu_page_response resp = {\n\t\t\t.pasid\t\t= flt->prm.pasid,\n\t\t\t.grpid\t\t= flt->prm.grpid,\n\t\t\t.code\t\t= IOMMU_PAGE_RESP_FAILURE,\n\t\t};\n\t\tarm_smmu_page_response(master->dev, &fault_evt, &resp);\n\t}\n\nout_unlock:\n\tmutex_unlock(&smmu->streams_mutex);\n\treturn ret;\n}\n\nstatic irqreturn_t arm_smmu_evtq_thread(int irq, void *dev)\n{\n\tint i, ret;\n\tstruct arm_smmu_device *smmu = dev;\n\tstruct arm_smmu_queue *q = &smmu->evtq.q;\n\tstruct arm_smmu_ll_queue *llq = &q->llq;\n\tstatic DEFINE_RATELIMIT_STATE(rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tu64 evt[EVTQ_ENT_DWORDS];\n\n\tdo {\n\t\twhile (!queue_remove_raw(q, evt)) {\n\t\t\tu8 id = FIELD_GET(EVTQ_0_ID, evt[0]);\n\n\t\t\tret = arm_smmu_handle_evt(smmu, evt);\n\t\t\tif (!ret || !__ratelimit(&rs))\n\t\t\t\tcontinue;\n\n\t\t\tdev_info(smmu->dev, \"event 0x%02x received:\\n\", id);\n\t\t\tfor (i = 0; i < ARRAY_SIZE(evt); ++i)\n\t\t\t\tdev_info(smmu->dev, \"\\t0x%016llx\\n\",\n\t\t\t\t\t (unsigned long long)evt[i]);\n\n\t\t\tcond_resched();\n\t\t}\n\n\t\t \n\t\tif (queue_sync_prod_in(q) == -EOVERFLOW)\n\t\t\tdev_err(smmu->dev, \"EVTQ overflow detected -- events lost\\n\");\n\t} while (!queue_empty(llq));\n\n\t \n\tqueue_sync_cons_ovf(q);\n\treturn IRQ_HANDLED;\n}\n\nstatic void arm_smmu_handle_ppr(struct arm_smmu_device *smmu, u64 *evt)\n{\n\tu32 sid, ssid;\n\tu16 grpid;\n\tbool ssv, last;\n\n\tsid = FIELD_GET(PRIQ_0_SID, evt[0]);\n\tssv = FIELD_GET(PRIQ_0_SSID_V, evt[0]);\n\tssid = ssv ? FIELD_GET(PRIQ_0_SSID, evt[0]) : IOMMU_NO_PASID;\n\tlast = FIELD_GET(PRIQ_0_PRG_LAST, evt[0]);\n\tgrpid = FIELD_GET(PRIQ_1_PRG_IDX, evt[1]);\n\n\tdev_info(smmu->dev, \"unexpected PRI request received:\\n\");\n\tdev_info(smmu->dev,\n\t\t \"\\tsid 0x%08x.0x%05x: [%u%s] %sprivileged %s%s%s access at iova 0x%016llx\\n\",\n\t\t sid, ssid, grpid, last ? \"L\" : \"\",\n\t\t evt[0] & PRIQ_0_PERM_PRIV ? \"\" : \"un\",\n\t\t evt[0] & PRIQ_0_PERM_READ ? \"R\" : \"\",\n\t\t evt[0] & PRIQ_0_PERM_WRITE ? \"W\" : \"\",\n\t\t evt[0] & PRIQ_0_PERM_EXEC ? \"X\" : \"\",\n\t\t evt[1] & PRIQ_1_ADDR_MASK);\n\n\tif (last) {\n\t\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t\t.opcode\t\t\t= CMDQ_OP_PRI_RESP,\n\t\t\t.substream_valid\t= ssv,\n\t\t\t.pri\t\t\t= {\n\t\t\t\t.sid\t= sid,\n\t\t\t\t.ssid\t= ssid,\n\t\t\t\t.grpid\t= grpid,\n\t\t\t\t.resp\t= PRI_RESP_DENY,\n\t\t\t},\n\t\t};\n\n\t\tarm_smmu_cmdq_issue_cmd(smmu, &cmd);\n\t}\n}\n\nstatic irqreturn_t arm_smmu_priq_thread(int irq, void *dev)\n{\n\tstruct arm_smmu_device *smmu = dev;\n\tstruct arm_smmu_queue *q = &smmu->priq.q;\n\tstruct arm_smmu_ll_queue *llq = &q->llq;\n\tu64 evt[PRIQ_ENT_DWORDS];\n\n\tdo {\n\t\twhile (!queue_remove_raw(q, evt))\n\t\t\tarm_smmu_handle_ppr(smmu, evt);\n\n\t\tif (queue_sync_prod_in(q) == -EOVERFLOW)\n\t\t\tdev_err(smmu->dev, \"PRIQ overflow detected -- requests lost\\n\");\n\t} while (!queue_empty(llq));\n\n\t \n\tqueue_sync_cons_ovf(q);\n\treturn IRQ_HANDLED;\n}\n\nstatic int arm_smmu_device_disable(struct arm_smmu_device *smmu);\n\nstatic irqreturn_t arm_smmu_gerror_handler(int irq, void *dev)\n{\n\tu32 gerror, gerrorn, active;\n\tstruct arm_smmu_device *smmu = dev;\n\n\tgerror = readl_relaxed(smmu->base + ARM_SMMU_GERROR);\n\tgerrorn = readl_relaxed(smmu->base + ARM_SMMU_GERRORN);\n\n\tactive = gerror ^ gerrorn;\n\tif (!(active & GERROR_ERR_MASK))\n\t\treturn IRQ_NONE;  \n\n\tdev_warn(smmu->dev,\n\t\t \"unexpected global error reported (0x%08x), this could be serious\\n\",\n\t\t active);\n\n\tif (active & GERROR_SFM_ERR) {\n\t\tdev_err(smmu->dev, \"device has entered Service Failure Mode!\\n\");\n\t\tarm_smmu_device_disable(smmu);\n\t}\n\n\tif (active & GERROR_MSI_GERROR_ABT_ERR)\n\t\tdev_warn(smmu->dev, \"GERROR MSI write aborted\\n\");\n\n\tif (active & GERROR_MSI_PRIQ_ABT_ERR)\n\t\tdev_warn(smmu->dev, \"PRIQ MSI write aborted\\n\");\n\n\tif (active & GERROR_MSI_EVTQ_ABT_ERR)\n\t\tdev_warn(smmu->dev, \"EVTQ MSI write aborted\\n\");\n\n\tif (active & GERROR_MSI_CMDQ_ABT_ERR)\n\t\tdev_warn(smmu->dev, \"CMDQ MSI write aborted\\n\");\n\n\tif (active & GERROR_PRIQ_ABT_ERR)\n\t\tdev_err(smmu->dev, \"PRIQ write aborted -- events may have been lost\\n\");\n\n\tif (active & GERROR_EVTQ_ABT_ERR)\n\t\tdev_err(smmu->dev, \"EVTQ write aborted -- events may have been lost\\n\");\n\n\tif (active & GERROR_CMDQ_ERR)\n\t\tarm_smmu_cmdq_skip_err(smmu);\n\n\twritel(gerror, smmu->base + ARM_SMMU_GERRORN);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t arm_smmu_combined_irq_thread(int irq, void *dev)\n{\n\tstruct arm_smmu_device *smmu = dev;\n\n\tarm_smmu_evtq_thread(irq, dev);\n\tif (smmu->features & ARM_SMMU_FEAT_PRI)\n\t\tarm_smmu_priq_thread(irq, dev);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t arm_smmu_combined_irq_handler(int irq, void *dev)\n{\n\tarm_smmu_gerror_handler(irq, dev);\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic void\narm_smmu_atc_inv_to_cmd(int ssid, unsigned long iova, size_t size,\n\t\t\tstruct arm_smmu_cmdq_ent *cmd)\n{\n\tsize_t log2_span;\n\tsize_t span_mask;\n\t \n\tsize_t inval_grain_shift = 12;\n\tunsigned long page_start, page_end;\n\n\t \n\t*cmd = (struct arm_smmu_cmdq_ent) {\n\t\t.opcode\t\t\t= CMDQ_OP_ATC_INV,\n\t\t.substream_valid\t= (ssid != IOMMU_NO_PASID),\n\t\t.atc.ssid\t\t= ssid,\n\t};\n\n\tif (!size) {\n\t\tcmd->atc.size = ATC_INV_SIZE_ALL;\n\t\treturn;\n\t}\n\n\tpage_start\t= iova >> inval_grain_shift;\n\tpage_end\t= (iova + size - 1) >> inval_grain_shift;\n\n\t \n\tlog2_span\t= fls_long(page_start ^ page_end);\n\tspan_mask\t= (1ULL << log2_span) - 1;\n\n\tpage_start\t&= ~span_mask;\n\n\tcmd->atc.addr\t= page_start << inval_grain_shift;\n\tcmd->atc.size\t= log2_span;\n}\n\nstatic int arm_smmu_atc_inv_master(struct arm_smmu_master *master)\n{\n\tint i;\n\tstruct arm_smmu_cmdq_ent cmd;\n\tstruct arm_smmu_cmdq_batch cmds;\n\n\tarm_smmu_atc_inv_to_cmd(IOMMU_NO_PASID, 0, 0, &cmd);\n\n\tcmds.num = 0;\n\tfor (i = 0; i < master->num_streams; i++) {\n\t\tcmd.atc.sid = master->streams[i].id;\n\t\tarm_smmu_cmdq_batch_add(master->smmu, &cmds, &cmd);\n\t}\n\n\treturn arm_smmu_cmdq_batch_submit(master->smmu, &cmds);\n}\n\nint arm_smmu_atc_inv_domain(struct arm_smmu_domain *smmu_domain, int ssid,\n\t\t\t    unsigned long iova, size_t size)\n{\n\tint i;\n\tunsigned long flags;\n\tstruct arm_smmu_cmdq_ent cmd;\n\tstruct arm_smmu_master *master;\n\tstruct arm_smmu_cmdq_batch cmds;\n\n\tif (!(smmu_domain->smmu->features & ARM_SMMU_FEAT_ATS))\n\t\treturn 0;\n\n\t \n\tsmp_mb();\n\tif (!atomic_read(&smmu_domain->nr_ats_masters))\n\t\treturn 0;\n\n\tarm_smmu_atc_inv_to_cmd(ssid, iova, size, &cmd);\n\n\tcmds.num = 0;\n\n\tspin_lock_irqsave(&smmu_domain->devices_lock, flags);\n\tlist_for_each_entry(master, &smmu_domain->devices, domain_head) {\n\t\tif (!master->ats_enabled)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < master->num_streams; i++) {\n\t\t\tcmd.atc.sid = master->streams[i].id;\n\t\t\tarm_smmu_cmdq_batch_add(smmu_domain->smmu, &cmds, &cmd);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&smmu_domain->devices_lock, flags);\n\n\treturn arm_smmu_cmdq_batch_submit(smmu_domain->smmu, &cmds);\n}\n\n \nstatic void arm_smmu_tlb_inv_context(void *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_cmdq_ent cmd;\n\n\t \n\tif (smmu_domain->stage == ARM_SMMU_DOMAIN_S1) {\n\t\tarm_smmu_tlb_inv_asid(smmu, smmu_domain->s1_cfg.cd.asid);\n\t} else {\n\t\tcmd.opcode\t= CMDQ_OP_TLBI_S12_VMALL;\n\t\tcmd.tlbi.vmid\t= smmu_domain->s2_cfg.vmid;\n\t\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n\t}\n\tarm_smmu_atc_inv_domain(smmu_domain, IOMMU_NO_PASID, 0, 0);\n}\n\nstatic void __arm_smmu_tlb_inv_range(struct arm_smmu_cmdq_ent *cmd,\n\t\t\t\t     unsigned long iova, size_t size,\n\t\t\t\t     size_t granule,\n\t\t\t\t     struct arm_smmu_domain *smmu_domain)\n{\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tunsigned long end = iova + size, num_pages = 0, tg = 0;\n\tsize_t inv_range = granule;\n\tstruct arm_smmu_cmdq_batch cmds;\n\n\tif (!size)\n\t\treturn;\n\n\tif (smmu->features & ARM_SMMU_FEAT_RANGE_INV) {\n\t\t \n\t\ttg = __ffs(smmu_domain->domain.pgsize_bitmap);\n\n\t\tnum_pages = size >> tg;\n\n\t\t \n\t\tcmd->tlbi.tg = (tg - 10) / 2;\n\n\t\t \n\t\tif (cmd->tlbi.leaf)\n\t\t\tcmd->tlbi.ttl = 4 - ((ilog2(granule) - 3) / (tg - 3));\n\t\telse if ((num_pages & CMDQ_TLBI_RANGE_NUM_MAX) == 1)\n\t\t\tnum_pages++;\n\t}\n\n\tcmds.num = 0;\n\n\twhile (iova < end) {\n\t\tif (smmu->features & ARM_SMMU_FEAT_RANGE_INV) {\n\t\t\t \n\t\t\tunsigned long scale, num;\n\n\t\t\t \n\t\t\tscale = __ffs(num_pages);\n\t\t\tcmd->tlbi.scale = scale;\n\n\t\t\t \n\t\t\tnum = (num_pages >> scale) & CMDQ_TLBI_RANGE_NUM_MAX;\n\t\t\tcmd->tlbi.num = num - 1;\n\n\t\t\t \n\t\t\tinv_range = num << (scale + tg);\n\n\t\t\t \n\t\t\tnum_pages -= num << scale;\n\t\t}\n\n\t\tcmd->tlbi.addr = iova;\n\t\tarm_smmu_cmdq_batch_add(smmu, &cmds, cmd);\n\t\tiova += inv_range;\n\t}\n\tarm_smmu_cmdq_batch_submit(smmu, &cmds);\n}\n\nstatic void arm_smmu_tlb_inv_range_domain(unsigned long iova, size_t size,\n\t\t\t\t\t  size_t granule, bool leaf,\n\t\t\t\t\t  struct arm_smmu_domain *smmu_domain)\n{\n\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t.tlbi = {\n\t\t\t.leaf\t= leaf,\n\t\t},\n\t};\n\n\tif (smmu_domain->stage == ARM_SMMU_DOMAIN_S1) {\n\t\tcmd.opcode\t= smmu_domain->smmu->features & ARM_SMMU_FEAT_E2H ?\n\t\t\t\t  CMDQ_OP_TLBI_EL2_VA : CMDQ_OP_TLBI_NH_VA;\n\t\tcmd.tlbi.asid\t= smmu_domain->s1_cfg.cd.asid;\n\t} else {\n\t\tcmd.opcode\t= CMDQ_OP_TLBI_S2_IPA;\n\t\tcmd.tlbi.vmid\t= smmu_domain->s2_cfg.vmid;\n\t}\n\t__arm_smmu_tlb_inv_range(&cmd, iova, size, granule, smmu_domain);\n\n\t \n\tarm_smmu_atc_inv_domain(smmu_domain, IOMMU_NO_PASID, iova, size);\n}\n\nvoid arm_smmu_tlb_inv_range_asid(unsigned long iova, size_t size, int asid,\n\t\t\t\t size_t granule, bool leaf,\n\t\t\t\t struct arm_smmu_domain *smmu_domain)\n{\n\tstruct arm_smmu_cmdq_ent cmd = {\n\t\t.opcode\t= smmu_domain->smmu->features & ARM_SMMU_FEAT_E2H ?\n\t\t\t  CMDQ_OP_TLBI_EL2_VA : CMDQ_OP_TLBI_NH_VA,\n\t\t.tlbi = {\n\t\t\t.asid\t= asid,\n\t\t\t.leaf\t= leaf,\n\t\t},\n\t};\n\n\t__arm_smmu_tlb_inv_range(&cmd, iova, size, granule, smmu_domain);\n}\n\nstatic void arm_smmu_tlb_inv_page_nosync(struct iommu_iotlb_gather *gather,\n\t\t\t\t\t unsigned long iova, size_t granule,\n\t\t\t\t\t void *cookie)\n{\n\tstruct arm_smmu_domain *smmu_domain = cookie;\n\tstruct iommu_domain *domain = &smmu_domain->domain;\n\n\tiommu_iotlb_gather_add_page(domain, gather, iova, granule);\n}\n\nstatic void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,\n\t\t\t\t  size_t granule, void *cookie)\n{\n\tarm_smmu_tlb_inv_range_domain(iova, size, granule, false, cookie);\n}\n\nstatic const struct iommu_flush_ops arm_smmu_flush_ops = {\n\t.tlb_flush_all\t= arm_smmu_tlb_inv_context,\n\t.tlb_flush_walk = arm_smmu_tlb_inv_walk,\n\t.tlb_add_page\t= arm_smmu_tlb_inv_page_nosync,\n};\n\n \nstatic bool arm_smmu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\n\tswitch (cap) {\n\tcase IOMMU_CAP_CACHE_COHERENCY:\n\t\t \n\t\treturn master->smmu->features & ARM_SMMU_FEAT_COHERENCY;\n\tcase IOMMU_CAP_NOEXEC:\n\tcase IOMMU_CAP_DEFERRED_FLUSH:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct iommu_domain *arm_smmu_domain_alloc(unsigned type)\n{\n\tstruct arm_smmu_domain *smmu_domain;\n\n\tif (type == IOMMU_DOMAIN_SVA)\n\t\treturn arm_smmu_sva_domain_alloc();\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED &&\n\t    type != IOMMU_DOMAIN_DMA &&\n\t    type != IOMMU_DOMAIN_IDENTITY)\n\t\treturn NULL;\n\n\t \n\tsmmu_domain = kzalloc(sizeof(*smmu_domain), GFP_KERNEL);\n\tif (!smmu_domain)\n\t\treturn NULL;\n\n\tmutex_init(&smmu_domain->init_mutex);\n\tINIT_LIST_HEAD(&smmu_domain->devices);\n\tspin_lock_init(&smmu_domain->devices_lock);\n\tINIT_LIST_HEAD(&smmu_domain->mmu_notifiers);\n\n\treturn &smmu_domain->domain;\n}\n\nstatic void arm_smmu_domain_free(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\tfree_io_pgtable_ops(smmu_domain->pgtbl_ops);\n\n\t \n\tif (smmu_domain->stage == ARM_SMMU_DOMAIN_S1) {\n\t\tstruct arm_smmu_s1_cfg *cfg = &smmu_domain->s1_cfg;\n\n\t\t \n\t\tmutex_lock(&arm_smmu_asid_lock);\n\t\tif (cfg->cdcfg.cdtab)\n\t\t\tarm_smmu_free_cd_tables(smmu_domain);\n\t\tarm_smmu_free_asid(&cfg->cd);\n\t\tmutex_unlock(&arm_smmu_asid_lock);\n\t} else {\n\t\tstruct arm_smmu_s2_cfg *cfg = &smmu_domain->s2_cfg;\n\t\tif (cfg->vmid)\n\t\t\tida_free(&smmu->vmid_map, cfg->vmid);\n\t}\n\n\tkfree(smmu_domain);\n}\n\nstatic int arm_smmu_domain_finalise_s1(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t       struct arm_smmu_master *master,\n\t\t\t\t       struct io_pgtable_cfg *pgtbl_cfg)\n{\n\tint ret;\n\tu32 asid;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_s1_cfg *cfg = &smmu_domain->s1_cfg;\n\ttypeof(&pgtbl_cfg->arm_lpae_s1_cfg.tcr) tcr = &pgtbl_cfg->arm_lpae_s1_cfg.tcr;\n\n\trefcount_set(&cfg->cd.refs, 1);\n\n\t \n\tmutex_lock(&arm_smmu_asid_lock);\n\tret = xa_alloc(&arm_smmu_asid_xa, &asid, &cfg->cd,\n\t\t       XA_LIMIT(1, (1 << smmu->asid_bits) - 1), GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tcfg->s1cdmax = master->ssid_bits;\n\n\tsmmu_domain->stall_enabled = master->stall_enabled;\n\n\tret = arm_smmu_alloc_cd_tables(smmu_domain);\n\tif (ret)\n\t\tgoto out_free_asid;\n\n\tcfg->cd.asid\t= (u16)asid;\n\tcfg->cd.ttbr\t= pgtbl_cfg->arm_lpae_s1_cfg.ttbr;\n\tcfg->cd.tcr\t= FIELD_PREP(CTXDESC_CD_0_TCR_T0SZ, tcr->tsz) |\n\t\t\t  FIELD_PREP(CTXDESC_CD_0_TCR_TG0, tcr->tg) |\n\t\t\t  FIELD_PREP(CTXDESC_CD_0_TCR_IRGN0, tcr->irgn) |\n\t\t\t  FIELD_PREP(CTXDESC_CD_0_TCR_ORGN0, tcr->orgn) |\n\t\t\t  FIELD_PREP(CTXDESC_CD_0_TCR_SH0, tcr->sh) |\n\t\t\t  FIELD_PREP(CTXDESC_CD_0_TCR_IPS, tcr->ips) |\n\t\t\t  CTXDESC_CD_0_TCR_EPD1 | CTXDESC_CD_0_AA64;\n\tcfg->cd.mair\t= pgtbl_cfg->arm_lpae_s1_cfg.mair;\n\n\t \n\tret = arm_smmu_write_ctx_desc(smmu_domain, IOMMU_NO_PASID, &cfg->cd);\n\tif (ret)\n\t\tgoto out_free_cd_tables;\n\n\tmutex_unlock(&arm_smmu_asid_lock);\n\treturn 0;\n\nout_free_cd_tables:\n\tarm_smmu_free_cd_tables(smmu_domain);\nout_free_asid:\n\tarm_smmu_free_asid(&cfg->cd);\nout_unlock:\n\tmutex_unlock(&arm_smmu_asid_lock);\n\treturn ret;\n}\n\nstatic int arm_smmu_domain_finalise_s2(struct arm_smmu_domain *smmu_domain,\n\t\t\t\t       struct arm_smmu_master *master,\n\t\t\t\t       struct io_pgtable_cfg *pgtbl_cfg)\n{\n\tint vmid;\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\tstruct arm_smmu_s2_cfg *cfg = &smmu_domain->s2_cfg;\n\ttypeof(&pgtbl_cfg->arm_lpae_s2_cfg.vtcr) vtcr;\n\n\t \n\tvmid = ida_alloc_range(&smmu->vmid_map, 1, (1 << smmu->vmid_bits) - 1,\n\t\t\t       GFP_KERNEL);\n\tif (vmid < 0)\n\t\treturn vmid;\n\n\tvtcr = &pgtbl_cfg->arm_lpae_s2_cfg.vtcr;\n\tcfg->vmid\t= (u16)vmid;\n\tcfg->vttbr\t= pgtbl_cfg->arm_lpae_s2_cfg.vttbr;\n\tcfg->vtcr\t= FIELD_PREP(STRTAB_STE_2_VTCR_S2T0SZ, vtcr->tsz) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2SL0, vtcr->sl) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2IR0, vtcr->irgn) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2OR0, vtcr->orgn) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2SH0, vtcr->sh) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2TG, vtcr->tg) |\n\t\t\t  FIELD_PREP(STRTAB_STE_2_VTCR_S2PS, vtcr->ps);\n\treturn 0;\n}\n\nstatic int arm_smmu_domain_finalise(struct iommu_domain *domain,\n\t\t\t\t    struct arm_smmu_master *master)\n{\n\tint ret;\n\tunsigned long ias, oas;\n\tenum io_pgtable_fmt fmt;\n\tstruct io_pgtable_cfg pgtbl_cfg;\n\tstruct io_pgtable_ops *pgtbl_ops;\n\tint (*finalise_stage_fn)(struct arm_smmu_domain *,\n\t\t\t\t struct arm_smmu_master *,\n\t\t\t\t struct io_pgtable_cfg *);\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_device *smmu = smmu_domain->smmu;\n\n\tif (domain->type == IOMMU_DOMAIN_IDENTITY) {\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_BYPASS;\n\t\treturn 0;\n\t}\n\n\t \n\tif (!(smmu->features & ARM_SMMU_FEAT_TRANS_S1))\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_S2;\n\tif (!(smmu->features & ARM_SMMU_FEAT_TRANS_S2))\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_S1;\n\n\tswitch (smmu_domain->stage) {\n\tcase ARM_SMMU_DOMAIN_S1:\n\t\tias = (smmu->features & ARM_SMMU_FEAT_VAX) ? 52 : 48;\n\t\tias = min_t(unsigned long, ias, VA_BITS);\n\t\toas = smmu->ias;\n\t\tfmt = ARM_64_LPAE_S1;\n\t\tfinalise_stage_fn = arm_smmu_domain_finalise_s1;\n\t\tbreak;\n\tcase ARM_SMMU_DOMAIN_NESTED:\n\tcase ARM_SMMU_DOMAIN_S2:\n\t\tias = smmu->ias;\n\t\toas = smmu->oas;\n\t\tfmt = ARM_64_LPAE_S2;\n\t\tfinalise_stage_fn = arm_smmu_domain_finalise_s2;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tpgtbl_cfg = (struct io_pgtable_cfg) {\n\t\t.pgsize_bitmap\t= smmu->pgsize_bitmap,\n\t\t.ias\t\t= ias,\n\t\t.oas\t\t= oas,\n\t\t.coherent_walk\t= smmu->features & ARM_SMMU_FEAT_COHERENCY,\n\t\t.tlb\t\t= &arm_smmu_flush_ops,\n\t\t.iommu_dev\t= smmu->dev,\n\t};\n\n\tpgtbl_ops = alloc_io_pgtable_ops(fmt, &pgtbl_cfg, smmu_domain);\n\tif (!pgtbl_ops)\n\t\treturn -ENOMEM;\n\n\tdomain->pgsize_bitmap = pgtbl_cfg.pgsize_bitmap;\n\tdomain->geometry.aperture_end = (1UL << pgtbl_cfg.ias) - 1;\n\tdomain->geometry.force_aperture = true;\n\n\tret = finalise_stage_fn(smmu_domain, master, &pgtbl_cfg);\n\tif (ret < 0) {\n\t\tfree_io_pgtable_ops(pgtbl_ops);\n\t\treturn ret;\n\t}\n\n\tsmmu_domain->pgtbl_ops = pgtbl_ops;\n\treturn 0;\n}\n\nstatic __le64 *arm_smmu_get_step_for_sid(struct arm_smmu_device *smmu, u32 sid)\n{\n\t__le64 *step;\n\tstruct arm_smmu_strtab_cfg *cfg = &smmu->strtab_cfg;\n\n\tif (smmu->features & ARM_SMMU_FEAT_2_LVL_STRTAB) {\n\t\tstruct arm_smmu_strtab_l1_desc *l1_desc;\n\t\tint idx;\n\n\t\t \n\t\tidx = (sid >> STRTAB_SPLIT) * STRTAB_L1_DESC_DWORDS;\n\t\tl1_desc = &cfg->l1_desc[idx];\n\t\tidx = (sid & ((1 << STRTAB_SPLIT) - 1)) * STRTAB_STE_DWORDS;\n\t\tstep = &l1_desc->l2ptr[idx];\n\t} else {\n\t\t \n\t\tstep = &cfg->strtab[sid * STRTAB_STE_DWORDS];\n\t}\n\n\treturn step;\n}\n\nstatic void arm_smmu_install_ste_for_dev(struct arm_smmu_master *master)\n{\n\tint i, j;\n\tstruct arm_smmu_device *smmu = master->smmu;\n\n\tfor (i = 0; i < master->num_streams; ++i) {\n\t\tu32 sid = master->streams[i].id;\n\t\t__le64 *step = arm_smmu_get_step_for_sid(smmu, sid);\n\n\t\t \n\t\tfor (j = 0; j < i; j++)\n\t\t\tif (master->streams[j].id == sid)\n\t\t\t\tbreak;\n\t\tif (j < i)\n\t\t\tcontinue;\n\n\t\tarm_smmu_write_strtab_ent(master, sid, step);\n\t}\n}\n\nstatic bool arm_smmu_ats_supported(struct arm_smmu_master *master)\n{\n\tstruct device *dev = master->dev;\n\tstruct arm_smmu_device *smmu = master->smmu;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (!(smmu->features & ARM_SMMU_FEAT_ATS))\n\t\treturn false;\n\n\tif (!(fwspec->flags & IOMMU_FWSPEC_PCI_RC_ATS))\n\t\treturn false;\n\n\treturn dev_is_pci(dev) && pci_ats_supported(to_pci_dev(dev));\n}\n\nstatic void arm_smmu_enable_ats(struct arm_smmu_master *master)\n{\n\tsize_t stu;\n\tstruct pci_dev *pdev;\n\tstruct arm_smmu_device *smmu = master->smmu;\n\tstruct arm_smmu_domain *smmu_domain = master->domain;\n\n\t \n\tif (!master->ats_enabled)\n\t\treturn;\n\n\t \n\tstu = __ffs(smmu->pgsize_bitmap);\n\tpdev = to_pci_dev(master->dev);\n\n\tatomic_inc(&smmu_domain->nr_ats_masters);\n\tarm_smmu_atc_inv_domain(smmu_domain, IOMMU_NO_PASID, 0, 0);\n\tif (pci_enable_ats(pdev, stu))\n\t\tdev_err(master->dev, \"Failed to enable ATS (STU %zu)\\n\", stu);\n}\n\nstatic void arm_smmu_disable_ats(struct arm_smmu_master *master)\n{\n\tstruct arm_smmu_domain *smmu_domain = master->domain;\n\n\tif (!master->ats_enabled)\n\t\treturn;\n\n\tpci_disable_ats(to_pci_dev(master->dev));\n\t \n\twmb();\n\tarm_smmu_atc_inv_master(master);\n\tatomic_dec(&smmu_domain->nr_ats_masters);\n}\n\nstatic int arm_smmu_enable_pasid(struct arm_smmu_master *master)\n{\n\tint ret;\n\tint features;\n\tint num_pasids;\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(master->dev))\n\t\treturn -ENODEV;\n\n\tpdev = to_pci_dev(master->dev);\n\n\tfeatures = pci_pasid_features(pdev);\n\tif (features < 0)\n\t\treturn features;\n\n\tnum_pasids = pci_max_pasids(pdev);\n\tif (num_pasids <= 0)\n\t\treturn num_pasids;\n\n\tret = pci_enable_pasid(pdev, features);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Failed to enable PASID\\n\");\n\t\treturn ret;\n\t}\n\n\tmaster->ssid_bits = min_t(u8, ilog2(num_pasids),\n\t\t\t\t  master->smmu->ssid_bits);\n\treturn 0;\n}\n\nstatic void arm_smmu_disable_pasid(struct arm_smmu_master *master)\n{\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(master->dev))\n\t\treturn;\n\n\tpdev = to_pci_dev(master->dev);\n\n\tif (!pdev->pasid_enabled)\n\t\treturn;\n\n\tmaster->ssid_bits = 0;\n\tpci_disable_pasid(pdev);\n}\n\nstatic void arm_smmu_detach_dev(struct arm_smmu_master *master)\n{\n\tunsigned long flags;\n\tstruct arm_smmu_domain *smmu_domain = master->domain;\n\n\tif (!smmu_domain)\n\t\treturn;\n\n\tarm_smmu_disable_ats(master);\n\n\tspin_lock_irqsave(&smmu_domain->devices_lock, flags);\n\tlist_del(&master->domain_head);\n\tspin_unlock_irqrestore(&smmu_domain->devices_lock, flags);\n\n\tmaster->domain = NULL;\n\tmaster->ats_enabled = false;\n\tarm_smmu_install_ste_for_dev(master);\n}\n\nstatic int arm_smmu_attach_dev(struct iommu_domain *domain, struct device *dev)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct arm_smmu_device *smmu;\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct arm_smmu_master *master;\n\n\tif (!fwspec)\n\t\treturn -ENOENT;\n\n\tmaster = dev_iommu_priv_get(dev);\n\tsmmu = master->smmu;\n\n\t \n\tif (arm_smmu_master_sva_enabled(master)) {\n\t\tdev_err(dev, \"cannot attach - SVA enabled\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tarm_smmu_detach_dev(master);\n\n\tmutex_lock(&smmu_domain->init_mutex);\n\n\tif (!smmu_domain->smmu) {\n\t\tsmmu_domain->smmu = smmu;\n\t\tret = arm_smmu_domain_finalise(domain, master);\n\t\tif (ret) {\n\t\t\tsmmu_domain->smmu = NULL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (smmu_domain->smmu != smmu) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t} else if (smmu_domain->stage == ARM_SMMU_DOMAIN_S1 &&\n\t\t   master->ssid_bits != smmu_domain->s1_cfg.s1cdmax) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t} else if (smmu_domain->stage == ARM_SMMU_DOMAIN_S1 &&\n\t\t   smmu_domain->stall_enabled != master->stall_enabled) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tmaster->domain = smmu_domain;\n\n\t \n\tif (smmu_domain->stage != ARM_SMMU_DOMAIN_BYPASS)\n\t\tmaster->ats_enabled = arm_smmu_ats_supported(master);\n\n\tarm_smmu_install_ste_for_dev(master);\n\n\tspin_lock_irqsave(&smmu_domain->devices_lock, flags);\n\tlist_add(&master->domain_head, &smmu_domain->devices);\n\tspin_unlock_irqrestore(&smmu_domain->devices_lock, flags);\n\n\tarm_smmu_enable_ats(master);\n\nout_unlock:\n\tmutex_unlock(&smmu_domain->init_mutex);\n\treturn ret;\n}\n\nstatic int arm_smmu_map_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t      phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t      int prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;\n\n\tif (!ops)\n\t\treturn -ENODEV;\n\n\treturn ops->map_pages(ops, iova, paddr, pgsize, pgcount, prot, gfp, mapped);\n}\n\nstatic size_t arm_smmu_unmap_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t\t   size_t pgsize, size_t pgcount,\n\t\t\t\t   struct iommu_iotlb_gather *gather)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tstruct io_pgtable_ops *ops = smmu_domain->pgtbl_ops;\n\n\tif (!ops)\n\t\treturn 0;\n\n\treturn ops->unmap_pages(ops, iova, pgsize, pgcount, gather);\n}\n\nstatic void arm_smmu_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\n\tif (smmu_domain->smmu)\n\t\tarm_smmu_tlb_inv_context(smmu_domain);\n}\n\nstatic void arm_smmu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\tstruct iommu_iotlb_gather *gather)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\n\tif (!gather->pgsize)\n\t\treturn;\n\n\tarm_smmu_tlb_inv_range_domain(gather->start,\n\t\t\t\t      gather->end - gather->start + 1,\n\t\t\t\t      gather->pgsize, true, smmu_domain);\n}\n\nstatic phys_addr_t\narm_smmu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova)\n{\n\tstruct io_pgtable_ops *ops = to_smmu_domain(domain)->pgtbl_ops;\n\n\tif (!ops)\n\t\treturn 0;\n\n\treturn ops->iova_to_phys(ops, iova);\n}\n\nstatic struct platform_driver arm_smmu_driver;\n\nstatic\nstruct arm_smmu_device *arm_smmu_get_by_fwnode(struct fwnode_handle *fwnode)\n{\n\tstruct device *dev = driver_find_device_by_fwnode(&arm_smmu_driver.driver,\n\t\t\t\t\t\t\t  fwnode);\n\tput_device(dev);\n\treturn dev ? dev_get_drvdata(dev) : NULL;\n}\n\nstatic bool arm_smmu_sid_in_range(struct arm_smmu_device *smmu, u32 sid)\n{\n\tunsigned long limit = smmu->strtab_cfg.num_l1_ents;\n\n\tif (smmu->features & ARM_SMMU_FEAT_2_LVL_STRTAB)\n\t\tlimit *= 1UL << STRTAB_SPLIT;\n\n\treturn sid < limit;\n}\n\nstatic int arm_smmu_init_sid_strtab(struct arm_smmu_device *smmu, u32 sid)\n{\n\t \n\tif (!arm_smmu_sid_in_range(smmu, sid))\n\t\treturn -ERANGE;\n\n\t \n\tif (smmu->features & ARM_SMMU_FEAT_2_LVL_STRTAB)\n\t\treturn arm_smmu_init_l2_strtab(smmu, sid);\n\n\treturn 0;\n}\n\nstatic int arm_smmu_insert_master(struct arm_smmu_device *smmu,\n\t\t\t\t  struct arm_smmu_master *master)\n{\n\tint i;\n\tint ret = 0;\n\tstruct arm_smmu_stream *new_stream, *cur_stream;\n\tstruct rb_node **new_node, *parent_node = NULL;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(master->dev);\n\n\tmaster->streams = kcalloc(fwspec->num_ids, sizeof(*master->streams),\n\t\t\t\t  GFP_KERNEL);\n\tif (!master->streams)\n\t\treturn -ENOMEM;\n\tmaster->num_streams = fwspec->num_ids;\n\n\tmutex_lock(&smmu->streams_mutex);\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\tu32 sid = fwspec->ids[i];\n\n\t\tnew_stream = &master->streams[i];\n\t\tnew_stream->id = sid;\n\t\tnew_stream->master = master;\n\n\t\tret = arm_smmu_init_sid_strtab(smmu, sid);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tnew_node = &(smmu->streams.rb_node);\n\t\twhile (*new_node) {\n\t\t\tcur_stream = rb_entry(*new_node, struct arm_smmu_stream,\n\t\t\t\t\t      node);\n\t\t\tparent_node = *new_node;\n\t\t\tif (cur_stream->id > new_stream->id) {\n\t\t\t\tnew_node = &((*new_node)->rb_left);\n\t\t\t} else if (cur_stream->id < new_stream->id) {\n\t\t\t\tnew_node = &((*new_node)->rb_right);\n\t\t\t} else {\n\t\t\t\tdev_warn(master->dev,\n\t\t\t\t\t \"stream %u already in tree\\n\",\n\t\t\t\t\t cur_stream->id);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\trb_link_node(&new_stream->node, parent_node, new_node);\n\t\trb_insert_color(&new_stream->node, &smmu->streams);\n\t}\n\n\tif (ret) {\n\t\tfor (i--; i >= 0; i--)\n\t\t\trb_erase(&master->streams[i].node, &smmu->streams);\n\t\tkfree(master->streams);\n\t}\n\tmutex_unlock(&smmu->streams_mutex);\n\n\treturn ret;\n}\n\nstatic void arm_smmu_remove_master(struct arm_smmu_master *master)\n{\n\tint i;\n\tstruct arm_smmu_device *smmu = master->smmu;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(master->dev);\n\n\tif (!smmu || !master->streams)\n\t\treturn;\n\n\tmutex_lock(&smmu->streams_mutex);\n\tfor (i = 0; i < fwspec->num_ids; i++)\n\t\trb_erase(&master->streams[i].node, &smmu->streams);\n\tmutex_unlock(&smmu->streams_mutex);\n\n\tkfree(master->streams);\n}\n\nstatic struct iommu_ops arm_smmu_ops;\n\nstatic struct iommu_device *arm_smmu_probe_device(struct device *dev)\n{\n\tint ret;\n\tstruct arm_smmu_device *smmu;\n\tstruct arm_smmu_master *master;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (!fwspec || fwspec->ops != &arm_smmu_ops)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (WARN_ON_ONCE(dev_iommu_priv_get(dev)))\n\t\treturn ERR_PTR(-EBUSY);\n\n\tsmmu = arm_smmu_get_by_fwnode(fwspec->iommu_fwnode);\n\tif (!smmu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmaster = kzalloc(sizeof(*master), GFP_KERNEL);\n\tif (!master)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmaster->dev = dev;\n\tmaster->smmu = smmu;\n\tINIT_LIST_HEAD(&master->bonds);\n\tdev_iommu_priv_set(dev, master);\n\n\tret = arm_smmu_insert_master(smmu, master);\n\tif (ret)\n\t\tgoto err_free_master;\n\n\tdevice_property_read_u32(dev, \"pasid-num-bits\", &master->ssid_bits);\n\tmaster->ssid_bits = min(smmu->ssid_bits, master->ssid_bits);\n\n\t \n\tarm_smmu_enable_pasid(master);\n\n\tif (!(smmu->features & ARM_SMMU_FEAT_2_LVL_CDTAB))\n\t\tmaster->ssid_bits = min_t(u8, master->ssid_bits,\n\t\t\t\t\t  CTXDESC_LINEAR_CDMAX);\n\n\tif ((smmu->features & ARM_SMMU_FEAT_STALLS &&\n\t     device_property_read_bool(dev, \"dma-can-stall\")) ||\n\t    smmu->features & ARM_SMMU_FEAT_STALL_FORCE)\n\t\tmaster->stall_enabled = true;\n\n\treturn &smmu->iommu;\n\nerr_free_master:\n\tkfree(master);\n\tdev_iommu_priv_set(dev, NULL);\n\treturn ERR_PTR(ret);\n}\n\nstatic void arm_smmu_release_device(struct device *dev)\n{\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\n\tif (WARN_ON(arm_smmu_master_sva_enabled(master)))\n\t\tiopf_queue_remove_device(master->smmu->evtq.iopf, dev);\n\tarm_smmu_detach_dev(master);\n\tarm_smmu_disable_pasid(master);\n\tarm_smmu_remove_master(master);\n\tkfree(master);\n}\n\nstatic struct iommu_group *arm_smmu_device_group(struct device *dev)\n{\n\tstruct iommu_group *group;\n\n\t \n\tif (dev_is_pci(dev))\n\t\tgroup = pci_device_group(dev);\n\telse\n\t\tgroup = generic_device_group(dev);\n\n\treturn group;\n}\n\nstatic int arm_smmu_enable_nesting(struct iommu_domain *domain)\n{\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\tint ret = 0;\n\n\tmutex_lock(&smmu_domain->init_mutex);\n\tif (smmu_domain->smmu)\n\t\tret = -EPERM;\n\telse\n\t\tsmmu_domain->stage = ARM_SMMU_DOMAIN_NESTED;\n\tmutex_unlock(&smmu_domain->init_mutex);\n\n\treturn ret;\n}\n\nstatic int arm_smmu_of_xlate(struct device *dev, struct of_phandle_args *args)\n{\n\treturn iommu_fwspec_add_ids(dev, args->args, 1);\n}\n\nstatic void arm_smmu_get_resv_regions(struct device *dev,\n\t\t\t\t      struct list_head *head)\n{\n\tstruct iommu_resv_region *region;\n\tint prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\n\tregion = iommu_alloc_resv_region(MSI_IOVA_BASE, MSI_IOVA_LENGTH,\n\t\t\t\t\t prot, IOMMU_RESV_SW_MSI, GFP_KERNEL);\n\tif (!region)\n\t\treturn;\n\n\tlist_add_tail(&region->list, head);\n\n\tiommu_dma_get_resv_regions(dev, head);\n}\n\nstatic int arm_smmu_dev_enable_feature(struct device *dev,\n\t\t\t\t       enum iommu_dev_features feat)\n{\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\n\tif (!master)\n\t\treturn -ENODEV;\n\n\tswitch (feat) {\n\tcase IOMMU_DEV_FEAT_IOPF:\n\t\tif (!arm_smmu_master_iopf_supported(master))\n\t\t\treturn -EINVAL;\n\t\tif (master->iopf_enabled)\n\t\t\treturn -EBUSY;\n\t\tmaster->iopf_enabled = true;\n\t\treturn 0;\n\tcase IOMMU_DEV_FEAT_SVA:\n\t\tif (!arm_smmu_master_sva_supported(master))\n\t\t\treturn -EINVAL;\n\t\tif (arm_smmu_master_sva_enabled(master))\n\t\t\treturn -EBUSY;\n\t\treturn arm_smmu_master_enable_sva(master);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int arm_smmu_dev_disable_feature(struct device *dev,\n\t\t\t\t\tenum iommu_dev_features feat)\n{\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\n\tif (!master)\n\t\treturn -EINVAL;\n\n\tswitch (feat) {\n\tcase IOMMU_DEV_FEAT_IOPF:\n\t\tif (!master->iopf_enabled)\n\t\t\treturn -EINVAL;\n\t\tif (master->sva_enabled)\n\t\t\treturn -EBUSY;\n\t\tmaster->iopf_enabled = false;\n\t\treturn 0;\n\tcase IOMMU_DEV_FEAT_SVA:\n\t\tif (!arm_smmu_master_sva_enabled(master))\n\t\t\treturn -EINVAL;\n\t\treturn arm_smmu_master_disable_sva(master);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \n#define IS_HISI_PTT_DEVICE(pdev)\t((pdev)->vendor == PCI_VENDOR_ID_HUAWEI && \\\n\t\t\t\t\t (pdev)->device == 0xa12e)\n\nstatic int arm_smmu_def_domain_type(struct device *dev)\n{\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\n\t\tif (IS_HISI_PTT_DEVICE(pdev))\n\t\t\treturn IOMMU_DOMAIN_IDENTITY;\n\t}\n\n\treturn 0;\n}\n\nstatic void arm_smmu_remove_dev_pasid(struct device *dev, ioasid_t pasid)\n{\n\tstruct iommu_domain *domain;\n\n\tdomain = iommu_get_domain_for_dev_pasid(dev, pasid, IOMMU_DOMAIN_SVA);\n\tif (WARN_ON(IS_ERR(domain)) || !domain)\n\t\treturn;\n\n\tarm_smmu_sva_remove_dev_pasid(domain, dev, pasid);\n}\n\nstatic struct iommu_ops arm_smmu_ops = {\n\t.capable\t\t= arm_smmu_capable,\n\t.domain_alloc\t\t= arm_smmu_domain_alloc,\n\t.probe_device\t\t= arm_smmu_probe_device,\n\t.release_device\t\t= arm_smmu_release_device,\n\t.device_group\t\t= arm_smmu_device_group,\n\t.of_xlate\t\t= arm_smmu_of_xlate,\n\t.get_resv_regions\t= arm_smmu_get_resv_regions,\n\t.remove_dev_pasid\t= arm_smmu_remove_dev_pasid,\n\t.dev_enable_feat\t= arm_smmu_dev_enable_feature,\n\t.dev_disable_feat\t= arm_smmu_dev_disable_feature,\n\t.page_response\t\t= arm_smmu_page_response,\n\t.def_domain_type\t= arm_smmu_def_domain_type,\n\t.pgsize_bitmap\t\t= -1UL,  \n\t.owner\t\t\t= THIS_MODULE,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t\t= arm_smmu_attach_dev,\n\t\t.map_pages\t\t= arm_smmu_map_pages,\n\t\t.unmap_pages\t\t= arm_smmu_unmap_pages,\n\t\t.flush_iotlb_all\t= arm_smmu_flush_iotlb_all,\n\t\t.iotlb_sync\t\t= arm_smmu_iotlb_sync,\n\t\t.iova_to_phys\t\t= arm_smmu_iova_to_phys,\n\t\t.enable_nesting\t\t= arm_smmu_enable_nesting,\n\t\t.free\t\t\t= arm_smmu_domain_free,\n\t}\n};\n\n \nstatic int arm_smmu_init_one_queue(struct arm_smmu_device *smmu,\n\t\t\t\t   struct arm_smmu_queue *q,\n\t\t\t\t   void __iomem *page,\n\t\t\t\t   unsigned long prod_off,\n\t\t\t\t   unsigned long cons_off,\n\t\t\t\t   size_t dwords, const char *name)\n{\n\tsize_t qsz;\n\n\tdo {\n\t\tqsz = ((1 << q->llq.max_n_shift) * dwords) << 3;\n\t\tq->base = dmam_alloc_coherent(smmu->dev, qsz, &q->base_dma,\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (q->base || qsz < PAGE_SIZE)\n\t\t\tbreak;\n\n\t\tq->llq.max_n_shift--;\n\t} while (1);\n\n\tif (!q->base) {\n\t\tdev_err(smmu->dev,\n\t\t\t\"failed to allocate queue (0x%zx bytes) for %s\\n\",\n\t\t\tqsz, name);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (!WARN_ON(q->base_dma & (qsz - 1))) {\n\t\tdev_info(smmu->dev, \"allocated %u entries for %s\\n\",\n\t\t\t 1 << q->llq.max_n_shift, name);\n\t}\n\n\tq->prod_reg\t= page + prod_off;\n\tq->cons_reg\t= page + cons_off;\n\tq->ent_dwords\t= dwords;\n\n\tq->q_base  = Q_BASE_RWA;\n\tq->q_base |= q->base_dma & Q_BASE_ADDR_MASK;\n\tq->q_base |= FIELD_PREP(Q_BASE_LOG2SIZE, q->llq.max_n_shift);\n\n\tq->llq.prod = q->llq.cons = 0;\n\treturn 0;\n}\n\nstatic int arm_smmu_cmdq_init(struct arm_smmu_device *smmu)\n{\n\tstruct arm_smmu_cmdq *cmdq = &smmu->cmdq;\n\tunsigned int nents = 1 << cmdq->q.llq.max_n_shift;\n\n\tatomic_set(&cmdq->owner_prod, 0);\n\tatomic_set(&cmdq->lock, 0);\n\n\tcmdq->valid_map = (atomic_long_t *)devm_bitmap_zalloc(smmu->dev, nents,\n\t\t\t\t\t\t\t      GFP_KERNEL);\n\tif (!cmdq->valid_map)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int arm_smmu_init_queues(struct arm_smmu_device *smmu)\n{\n\tint ret;\n\n\t \n\tret = arm_smmu_init_one_queue(smmu, &smmu->cmdq.q, smmu->base,\n\t\t\t\t      ARM_SMMU_CMDQ_PROD, ARM_SMMU_CMDQ_CONS,\n\t\t\t\t      CMDQ_ENT_DWORDS, \"cmdq\");\n\tif (ret)\n\t\treturn ret;\n\n\tret = arm_smmu_cmdq_init(smmu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = arm_smmu_init_one_queue(smmu, &smmu->evtq.q, smmu->page1,\n\t\t\t\t      ARM_SMMU_EVTQ_PROD, ARM_SMMU_EVTQ_CONS,\n\t\t\t\t      EVTQ_ENT_DWORDS, \"evtq\");\n\tif (ret)\n\t\treturn ret;\n\n\tif ((smmu->features & ARM_SMMU_FEAT_SVA) &&\n\t    (smmu->features & ARM_SMMU_FEAT_STALLS)) {\n\t\tsmmu->evtq.iopf = iopf_queue_alloc(dev_name(smmu->dev));\n\t\tif (!smmu->evtq.iopf)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (!(smmu->features & ARM_SMMU_FEAT_PRI))\n\t\treturn 0;\n\n\treturn arm_smmu_init_one_queue(smmu, &smmu->priq.q, smmu->page1,\n\t\t\t\t       ARM_SMMU_PRIQ_PROD, ARM_SMMU_PRIQ_CONS,\n\t\t\t\t       PRIQ_ENT_DWORDS, \"priq\");\n}\n\nstatic int arm_smmu_init_l1_strtab(struct arm_smmu_device *smmu)\n{\n\tunsigned int i;\n\tstruct arm_smmu_strtab_cfg *cfg = &smmu->strtab_cfg;\n\tvoid *strtab = smmu->strtab_cfg.strtab;\n\n\tcfg->l1_desc = devm_kcalloc(smmu->dev, cfg->num_l1_ents,\n\t\t\t\t    sizeof(*cfg->l1_desc), GFP_KERNEL);\n\tif (!cfg->l1_desc)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < cfg->num_l1_ents; ++i) {\n\t\tarm_smmu_write_strtab_l1_desc(strtab, &cfg->l1_desc[i]);\n\t\tstrtab += STRTAB_L1_DESC_DWORDS << 3;\n\t}\n\n\treturn 0;\n}\n\nstatic int arm_smmu_init_strtab_2lvl(struct arm_smmu_device *smmu)\n{\n\tvoid *strtab;\n\tu64 reg;\n\tu32 size, l1size;\n\tstruct arm_smmu_strtab_cfg *cfg = &smmu->strtab_cfg;\n\n\t \n\tsize = STRTAB_L1_SZ_SHIFT - (ilog2(STRTAB_L1_DESC_DWORDS) + 3);\n\tsize = min(size, smmu->sid_bits - STRTAB_SPLIT);\n\tcfg->num_l1_ents = 1 << size;\n\n\tsize += STRTAB_SPLIT;\n\tif (size < smmu->sid_bits)\n\t\tdev_warn(smmu->dev,\n\t\t\t \"2-level strtab only covers %u/%u bits of SID\\n\",\n\t\t\t size, smmu->sid_bits);\n\n\tl1size = cfg->num_l1_ents * (STRTAB_L1_DESC_DWORDS << 3);\n\tstrtab = dmam_alloc_coherent(smmu->dev, l1size, &cfg->strtab_dma,\n\t\t\t\t     GFP_KERNEL);\n\tif (!strtab) {\n\t\tdev_err(smmu->dev,\n\t\t\t\"failed to allocate l1 stream table (%u bytes)\\n\",\n\t\t\tl1size);\n\t\treturn -ENOMEM;\n\t}\n\tcfg->strtab = strtab;\n\n\t \n\treg  = FIELD_PREP(STRTAB_BASE_CFG_FMT, STRTAB_BASE_CFG_FMT_2LVL);\n\treg |= FIELD_PREP(STRTAB_BASE_CFG_LOG2SIZE, size);\n\treg |= FIELD_PREP(STRTAB_BASE_CFG_SPLIT, STRTAB_SPLIT);\n\tcfg->strtab_base_cfg = reg;\n\n\treturn arm_smmu_init_l1_strtab(smmu);\n}\n\nstatic int arm_smmu_init_strtab_linear(struct arm_smmu_device *smmu)\n{\n\tvoid *strtab;\n\tu64 reg;\n\tu32 size;\n\tstruct arm_smmu_strtab_cfg *cfg = &smmu->strtab_cfg;\n\n\tsize = (1 << smmu->sid_bits) * (STRTAB_STE_DWORDS << 3);\n\tstrtab = dmam_alloc_coherent(smmu->dev, size, &cfg->strtab_dma,\n\t\t\t\t     GFP_KERNEL);\n\tif (!strtab) {\n\t\tdev_err(smmu->dev,\n\t\t\t\"failed to allocate linear stream table (%u bytes)\\n\",\n\t\t\tsize);\n\t\treturn -ENOMEM;\n\t}\n\tcfg->strtab = strtab;\n\tcfg->num_l1_ents = 1 << smmu->sid_bits;\n\n\t \n\treg  = FIELD_PREP(STRTAB_BASE_CFG_FMT, STRTAB_BASE_CFG_FMT_LINEAR);\n\treg |= FIELD_PREP(STRTAB_BASE_CFG_LOG2SIZE, smmu->sid_bits);\n\tcfg->strtab_base_cfg = reg;\n\n\tarm_smmu_init_bypass_stes(strtab, cfg->num_l1_ents, false);\n\treturn 0;\n}\n\nstatic int arm_smmu_init_strtab(struct arm_smmu_device *smmu)\n{\n\tu64 reg;\n\tint ret;\n\n\tif (smmu->features & ARM_SMMU_FEAT_2_LVL_STRTAB)\n\t\tret = arm_smmu_init_strtab_2lvl(smmu);\n\telse\n\t\tret = arm_smmu_init_strtab_linear(smmu);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treg  = smmu->strtab_cfg.strtab_dma & STRTAB_BASE_ADDR_MASK;\n\treg |= STRTAB_BASE_RA;\n\tsmmu->strtab_cfg.strtab_base = reg;\n\n\tida_init(&smmu->vmid_map);\n\n\treturn 0;\n}\n\nstatic int arm_smmu_init_structures(struct arm_smmu_device *smmu)\n{\n\tint ret;\n\n\tmutex_init(&smmu->streams_mutex);\n\tsmmu->streams = RB_ROOT;\n\n\tret = arm_smmu_init_queues(smmu);\n\tif (ret)\n\t\treturn ret;\n\n\treturn arm_smmu_init_strtab(smmu);\n}\n\nstatic int arm_smmu_write_reg_sync(struct arm_smmu_device *smmu, u32 val,\n\t\t\t\t   unsigned int reg_off, unsigned int ack_off)\n{\n\tu32 reg;\n\n\twritel_relaxed(val, smmu->base + reg_off);\n\treturn readl_relaxed_poll_timeout(smmu->base + ack_off, reg, reg == val,\n\t\t\t\t\t  1, ARM_SMMU_POLL_TIMEOUT_US);\n}\n\n \nstatic int arm_smmu_update_gbpa(struct arm_smmu_device *smmu, u32 set, u32 clr)\n{\n\tint ret;\n\tu32 reg, __iomem *gbpa = smmu->base + ARM_SMMU_GBPA;\n\n\tret = readl_relaxed_poll_timeout(gbpa, reg, !(reg & GBPA_UPDATE),\n\t\t\t\t\t 1, ARM_SMMU_POLL_TIMEOUT_US);\n\tif (ret)\n\t\treturn ret;\n\n\treg &= ~clr;\n\treg |= set;\n\twritel_relaxed(reg | GBPA_UPDATE, gbpa);\n\tret = readl_relaxed_poll_timeout(gbpa, reg, !(reg & GBPA_UPDATE),\n\t\t\t\t\t 1, ARM_SMMU_POLL_TIMEOUT_US);\n\n\tif (ret)\n\t\tdev_err(smmu->dev, \"GBPA not responding to update\\n\");\n\treturn ret;\n}\n\nstatic void arm_smmu_free_msis(void *data)\n{\n\tstruct device *dev = data;\n\tplatform_msi_domain_free_irqs(dev);\n}\n\nstatic void arm_smmu_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tphys_addr_t doorbell;\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct arm_smmu_device *smmu = dev_get_drvdata(dev);\n\tphys_addr_t *cfg = arm_smmu_msi_cfg[desc->msi_index];\n\n\tdoorbell = (((u64)msg->address_hi) << 32) | msg->address_lo;\n\tdoorbell &= MSI_CFG0_ADDR_MASK;\n\n\twriteq_relaxed(doorbell, smmu->base + cfg[0]);\n\twritel_relaxed(msg->data, smmu->base + cfg[1]);\n\twritel_relaxed(ARM_SMMU_MEMATTR_DEVICE_nGnRE, smmu->base + cfg[2]);\n}\n\nstatic void arm_smmu_setup_msis(struct arm_smmu_device *smmu)\n{\n\tint ret, nvec = ARM_SMMU_MAX_MSIS;\n\tstruct device *dev = smmu->dev;\n\n\t \n\twriteq_relaxed(0, smmu->base + ARM_SMMU_GERROR_IRQ_CFG0);\n\twriteq_relaxed(0, smmu->base + ARM_SMMU_EVTQ_IRQ_CFG0);\n\n\tif (smmu->features & ARM_SMMU_FEAT_PRI)\n\t\twriteq_relaxed(0, smmu->base + ARM_SMMU_PRIQ_IRQ_CFG0);\n\telse\n\t\tnvec--;\n\n\tif (!(smmu->features & ARM_SMMU_FEAT_MSI))\n\t\treturn;\n\n\tif (!dev->msi.domain) {\n\t\tdev_info(smmu->dev, \"msi_domain absent - falling back to wired irqs\\n\");\n\t\treturn;\n\t}\n\n\t \n\tret = platform_msi_domain_alloc_irqs(dev, nvec, arm_smmu_write_msi_msg);\n\tif (ret) {\n\t\tdev_warn(dev, \"failed to allocate MSIs - falling back to wired irqs\\n\");\n\t\treturn;\n\t}\n\n\tsmmu->evtq.q.irq = msi_get_virq(dev, EVTQ_MSI_INDEX);\n\tsmmu->gerr_irq = msi_get_virq(dev, GERROR_MSI_INDEX);\n\tsmmu->priq.q.irq = msi_get_virq(dev, PRIQ_MSI_INDEX);\n\n\t \n\tdevm_add_action(dev, arm_smmu_free_msis, dev);\n}\n\nstatic void arm_smmu_setup_unique_irqs(struct arm_smmu_device *smmu)\n{\n\tint irq, ret;\n\n\tarm_smmu_setup_msis(smmu);\n\n\t \n\tirq = smmu->evtq.q.irq;\n\tif (irq) {\n\t\tret = devm_request_threaded_irq(smmu->dev, irq, NULL,\n\t\t\t\t\t\tarm_smmu_evtq_thread,\n\t\t\t\t\t\tIRQF_ONESHOT,\n\t\t\t\t\t\t\"arm-smmu-v3-evtq\", smmu);\n\t\tif (ret < 0)\n\t\t\tdev_warn(smmu->dev, \"failed to enable evtq irq\\n\");\n\t} else {\n\t\tdev_warn(smmu->dev, \"no evtq irq - events will not be reported!\\n\");\n\t}\n\n\tirq = smmu->gerr_irq;\n\tif (irq) {\n\t\tret = devm_request_irq(smmu->dev, irq, arm_smmu_gerror_handler,\n\t\t\t\t       0, \"arm-smmu-v3-gerror\", smmu);\n\t\tif (ret < 0)\n\t\t\tdev_warn(smmu->dev, \"failed to enable gerror irq\\n\");\n\t} else {\n\t\tdev_warn(smmu->dev, \"no gerr irq - errors will not be reported!\\n\");\n\t}\n\n\tif (smmu->features & ARM_SMMU_FEAT_PRI) {\n\t\tirq = smmu->priq.q.irq;\n\t\tif (irq) {\n\t\t\tret = devm_request_threaded_irq(smmu->dev, irq, NULL,\n\t\t\t\t\t\t\tarm_smmu_priq_thread,\n\t\t\t\t\t\t\tIRQF_ONESHOT,\n\t\t\t\t\t\t\t\"arm-smmu-v3-priq\",\n\t\t\t\t\t\t\tsmmu);\n\t\t\tif (ret < 0)\n\t\t\t\tdev_warn(smmu->dev,\n\t\t\t\t\t \"failed to enable priq irq\\n\");\n\t\t} else {\n\t\t\tdev_warn(smmu->dev, \"no priq irq - PRI will be broken\\n\");\n\t\t}\n\t}\n}\n\nstatic int arm_smmu_setup_irqs(struct arm_smmu_device *smmu)\n{\n\tint ret, irq;\n\tu32 irqen_flags = IRQ_CTRL_EVTQ_IRQEN | IRQ_CTRL_GERROR_IRQEN;\n\n\t \n\tret = arm_smmu_write_reg_sync(smmu, 0, ARM_SMMU_IRQ_CTRL,\n\t\t\t\t      ARM_SMMU_IRQ_CTRLACK);\n\tif (ret) {\n\t\tdev_err(smmu->dev, \"failed to disable irqs\\n\");\n\t\treturn ret;\n\t}\n\n\tirq = smmu->combined_irq;\n\tif (irq) {\n\t\t \n\t\tret = devm_request_threaded_irq(smmu->dev, irq,\n\t\t\t\t\tarm_smmu_combined_irq_handler,\n\t\t\t\t\tarm_smmu_combined_irq_thread,\n\t\t\t\t\tIRQF_ONESHOT,\n\t\t\t\t\t\"arm-smmu-v3-combined-irq\", smmu);\n\t\tif (ret < 0)\n\t\t\tdev_warn(smmu->dev, \"failed to enable combined irq\\n\");\n\t} else\n\t\tarm_smmu_setup_unique_irqs(smmu);\n\n\tif (smmu->features & ARM_SMMU_FEAT_PRI)\n\t\tirqen_flags |= IRQ_CTRL_PRIQ_IRQEN;\n\n\t \n\tret = arm_smmu_write_reg_sync(smmu, irqen_flags,\n\t\t\t\t      ARM_SMMU_IRQ_CTRL, ARM_SMMU_IRQ_CTRLACK);\n\tif (ret)\n\t\tdev_warn(smmu->dev, \"failed to enable irqs\\n\");\n\n\treturn 0;\n}\n\nstatic int arm_smmu_device_disable(struct arm_smmu_device *smmu)\n{\n\tint ret;\n\n\tret = arm_smmu_write_reg_sync(smmu, 0, ARM_SMMU_CR0, ARM_SMMU_CR0ACK);\n\tif (ret)\n\t\tdev_err(smmu->dev, \"failed to clear cr0\\n\");\n\n\treturn ret;\n}\n\nstatic int arm_smmu_device_reset(struct arm_smmu_device *smmu, bool bypass)\n{\n\tint ret;\n\tu32 reg, enables;\n\tstruct arm_smmu_cmdq_ent cmd;\n\n\t \n\treg = readl_relaxed(smmu->base + ARM_SMMU_CR0);\n\tif (reg & CR0_SMMUEN) {\n\t\tdev_warn(smmu->dev, \"SMMU currently enabled! Resetting...\\n\");\n\t\tWARN_ON(is_kdump_kernel() && !disable_bypass);\n\t\tarm_smmu_update_gbpa(smmu, GBPA_ABORT, 0);\n\t}\n\n\tret = arm_smmu_device_disable(smmu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treg = FIELD_PREP(CR1_TABLE_SH, ARM_SMMU_SH_ISH) |\n\t      FIELD_PREP(CR1_TABLE_OC, CR1_CACHE_WB) |\n\t      FIELD_PREP(CR1_TABLE_IC, CR1_CACHE_WB) |\n\t      FIELD_PREP(CR1_QUEUE_SH, ARM_SMMU_SH_ISH) |\n\t      FIELD_PREP(CR1_QUEUE_OC, CR1_CACHE_WB) |\n\t      FIELD_PREP(CR1_QUEUE_IC, CR1_CACHE_WB);\n\twritel_relaxed(reg, smmu->base + ARM_SMMU_CR1);\n\n\t \n\treg = CR2_PTM | CR2_RECINVSID;\n\n\tif (smmu->features & ARM_SMMU_FEAT_E2H)\n\t\treg |= CR2_E2H;\n\n\twritel_relaxed(reg, smmu->base + ARM_SMMU_CR2);\n\n\t \n\twriteq_relaxed(smmu->strtab_cfg.strtab_base,\n\t\t       smmu->base + ARM_SMMU_STRTAB_BASE);\n\twritel_relaxed(smmu->strtab_cfg.strtab_base_cfg,\n\t\t       smmu->base + ARM_SMMU_STRTAB_BASE_CFG);\n\n\t \n\twriteq_relaxed(smmu->cmdq.q.q_base, smmu->base + ARM_SMMU_CMDQ_BASE);\n\twritel_relaxed(smmu->cmdq.q.llq.prod, smmu->base + ARM_SMMU_CMDQ_PROD);\n\twritel_relaxed(smmu->cmdq.q.llq.cons, smmu->base + ARM_SMMU_CMDQ_CONS);\n\n\tenables = CR0_CMDQEN;\n\tret = arm_smmu_write_reg_sync(smmu, enables, ARM_SMMU_CR0,\n\t\t\t\t      ARM_SMMU_CR0ACK);\n\tif (ret) {\n\t\tdev_err(smmu->dev, \"failed to enable command queue\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tcmd.opcode = CMDQ_OP_CFGI_ALL;\n\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n\n\t \n\tif (smmu->features & ARM_SMMU_FEAT_HYP) {\n\t\tcmd.opcode = CMDQ_OP_TLBI_EL2_ALL;\n\t\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n\t}\n\n\tcmd.opcode = CMDQ_OP_TLBI_NSNH_ALL;\n\tarm_smmu_cmdq_issue_cmd_with_sync(smmu, &cmd);\n\n\t \n\twriteq_relaxed(smmu->evtq.q.q_base, smmu->base + ARM_SMMU_EVTQ_BASE);\n\twritel_relaxed(smmu->evtq.q.llq.prod, smmu->page1 + ARM_SMMU_EVTQ_PROD);\n\twritel_relaxed(smmu->evtq.q.llq.cons, smmu->page1 + ARM_SMMU_EVTQ_CONS);\n\n\tenables |= CR0_EVTQEN;\n\tret = arm_smmu_write_reg_sync(smmu, enables, ARM_SMMU_CR0,\n\t\t\t\t      ARM_SMMU_CR0ACK);\n\tif (ret) {\n\t\tdev_err(smmu->dev, \"failed to enable event queue\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tif (smmu->features & ARM_SMMU_FEAT_PRI) {\n\t\twriteq_relaxed(smmu->priq.q.q_base,\n\t\t\t       smmu->base + ARM_SMMU_PRIQ_BASE);\n\t\twritel_relaxed(smmu->priq.q.llq.prod,\n\t\t\t       smmu->page1 + ARM_SMMU_PRIQ_PROD);\n\t\twritel_relaxed(smmu->priq.q.llq.cons,\n\t\t\t       smmu->page1 + ARM_SMMU_PRIQ_CONS);\n\n\t\tenables |= CR0_PRIQEN;\n\t\tret = arm_smmu_write_reg_sync(smmu, enables, ARM_SMMU_CR0,\n\t\t\t\t\t      ARM_SMMU_CR0ACK);\n\t\tif (ret) {\n\t\t\tdev_err(smmu->dev, \"failed to enable PRI queue\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (smmu->features & ARM_SMMU_FEAT_ATS) {\n\t\tenables |= CR0_ATSCHK;\n\t\tret = arm_smmu_write_reg_sync(smmu, enables, ARM_SMMU_CR0,\n\t\t\t\t\t      ARM_SMMU_CR0ACK);\n\t\tif (ret) {\n\t\t\tdev_err(smmu->dev, \"failed to enable ATS check\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = arm_smmu_setup_irqs(smmu);\n\tif (ret) {\n\t\tdev_err(smmu->dev, \"failed to setup irqs\\n\");\n\t\treturn ret;\n\t}\n\n\tif (is_kdump_kernel())\n\t\tenables &= ~(CR0_EVTQEN | CR0_PRIQEN);\n\n\t \n\tif (!bypass || disable_bypass) {\n\t\tenables |= CR0_SMMUEN;\n\t} else {\n\t\tret = arm_smmu_update_gbpa(smmu, 0, GBPA_ABORT);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tret = arm_smmu_write_reg_sync(smmu, enables, ARM_SMMU_CR0,\n\t\t\t\t      ARM_SMMU_CR0ACK);\n\tif (ret) {\n\t\tdev_err(smmu->dev, \"failed to enable SMMU interface\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n#define IIDR_IMPLEMENTER_ARM\t\t0x43b\n#define IIDR_PRODUCTID_ARM_MMU_600\t0x483\n#define IIDR_PRODUCTID_ARM_MMU_700\t0x487\n\nstatic void arm_smmu_device_iidr_probe(struct arm_smmu_device *smmu)\n{\n\tu32 reg;\n\tunsigned int implementer, productid, variant, revision;\n\n\treg = readl_relaxed(smmu->base + ARM_SMMU_IIDR);\n\timplementer = FIELD_GET(IIDR_IMPLEMENTER, reg);\n\tproductid = FIELD_GET(IIDR_PRODUCTID, reg);\n\tvariant = FIELD_GET(IIDR_VARIANT, reg);\n\trevision = FIELD_GET(IIDR_REVISION, reg);\n\n\tswitch (implementer) {\n\tcase IIDR_IMPLEMENTER_ARM:\n\t\tswitch (productid) {\n\t\tcase IIDR_PRODUCTID_ARM_MMU_600:\n\t\t\t \n\t\t\tif (variant == 0 && revision <= 2)\n\t\t\t\tsmmu->features &= ~ARM_SMMU_FEAT_SEV;\n\t\t\t \n\t\t\tif (variant < 2)\n\t\t\t\tsmmu->features &= ~ARM_SMMU_FEAT_NESTING;\n\t\t\tbreak;\n\t\tcase IIDR_PRODUCTID_ARM_MMU_700:\n\t\t\t \n\t\t\tsmmu->features &= ~ARM_SMMU_FEAT_BTM;\n\t\t\tsmmu->options |= ARM_SMMU_OPT_CMDQ_FORCE_SYNC;\n\t\t\t \n\t\t\tsmmu->features &= ~ARM_SMMU_FEAT_NESTING;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic int arm_smmu_device_hw_probe(struct arm_smmu_device *smmu)\n{\n\tu32 reg;\n\tbool coherent = smmu->features & ARM_SMMU_FEAT_COHERENCY;\n\n\t \n\treg = readl_relaxed(smmu->base + ARM_SMMU_IDR0);\n\n\t \n\tif (FIELD_GET(IDR0_ST_LVL, reg) == IDR0_ST_LVL_2LVL)\n\t\tsmmu->features |= ARM_SMMU_FEAT_2_LVL_STRTAB;\n\n\tif (reg & IDR0_CD2L)\n\t\tsmmu->features |= ARM_SMMU_FEAT_2_LVL_CDTAB;\n\n\t \n\tswitch (FIELD_GET(IDR0_TTENDIAN, reg)) {\n\tcase IDR0_TTENDIAN_MIXED:\n\t\tsmmu->features |= ARM_SMMU_FEAT_TT_LE | ARM_SMMU_FEAT_TT_BE;\n\t\tbreak;\n#ifdef __BIG_ENDIAN\n\tcase IDR0_TTENDIAN_BE:\n\t\tsmmu->features |= ARM_SMMU_FEAT_TT_BE;\n\t\tbreak;\n#else\n\tcase IDR0_TTENDIAN_LE:\n\t\tsmmu->features |= ARM_SMMU_FEAT_TT_LE;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_err(smmu->dev, \"unknown/unsupported TT endianness!\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tif (IS_ENABLED(CONFIG_PCI_PRI) && reg & IDR0_PRI)\n\t\tsmmu->features |= ARM_SMMU_FEAT_PRI;\n\n\tif (IS_ENABLED(CONFIG_PCI_ATS) && reg & IDR0_ATS)\n\t\tsmmu->features |= ARM_SMMU_FEAT_ATS;\n\n\tif (reg & IDR0_SEV)\n\t\tsmmu->features |= ARM_SMMU_FEAT_SEV;\n\n\tif (reg & IDR0_MSI) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_MSI;\n\t\tif (coherent && !disable_msipolling)\n\t\t\tsmmu->options |= ARM_SMMU_OPT_MSIPOLL;\n\t}\n\n\tif (reg & IDR0_HYP) {\n\t\tsmmu->features |= ARM_SMMU_FEAT_HYP;\n\t\tif (cpus_have_cap(ARM64_HAS_VIRT_HOST_EXTN))\n\t\t\tsmmu->features |= ARM_SMMU_FEAT_E2H;\n\t}\n\n\t \n\tif (!!(reg & IDR0_COHACC) != coherent)\n\t\tdev_warn(smmu->dev, \"IDR0.COHACC overridden by FW configuration (%s)\\n\",\n\t\t\t coherent ? \"true\" : \"false\");\n\n\tswitch (FIELD_GET(IDR0_STALL_MODEL, reg)) {\n\tcase IDR0_STALL_MODEL_FORCE:\n\t\tsmmu->features |= ARM_SMMU_FEAT_STALL_FORCE;\n\t\tfallthrough;\n\tcase IDR0_STALL_MODEL_STALL:\n\t\tsmmu->features |= ARM_SMMU_FEAT_STALLS;\n\t}\n\n\tif (reg & IDR0_S1P)\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_S1;\n\n\tif (reg & IDR0_S2P)\n\t\tsmmu->features |= ARM_SMMU_FEAT_TRANS_S2;\n\n\tif (!(reg & (IDR0_S1P | IDR0_S2P))) {\n\t\tdev_err(smmu->dev, \"no translation support!\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tswitch (FIELD_GET(IDR0_TTF, reg)) {\n\tcase IDR0_TTF_AARCH32_64:\n\t\tsmmu->ias = 40;\n\t\tfallthrough;\n\tcase IDR0_TTF_AARCH64:\n\t\tbreak;\n\tdefault:\n\t\tdev_err(smmu->dev, \"AArch64 table format not supported!\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tsmmu->asid_bits = reg & IDR0_ASID16 ? 16 : 8;\n\tsmmu->vmid_bits = reg & IDR0_VMID16 ? 16 : 8;\n\n\t \n\treg = readl_relaxed(smmu->base + ARM_SMMU_IDR1);\n\tif (reg & (IDR1_TABLES_PRESET | IDR1_QUEUES_PRESET | IDR1_REL)) {\n\t\tdev_err(smmu->dev, \"embedded implementation not supported\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tsmmu->cmdq.q.llq.max_n_shift = min_t(u32, CMDQ_MAX_SZ_SHIFT,\n\t\t\t\t\t     FIELD_GET(IDR1_CMDQS, reg));\n\tif (smmu->cmdq.q.llq.max_n_shift <= ilog2(CMDQ_BATCH_ENTRIES)) {\n\t\t \n\t\tdev_err(smmu->dev, \"command queue size <= %d entries not supported\\n\",\n\t\t\tCMDQ_BATCH_ENTRIES);\n\t\treturn -ENXIO;\n\t}\n\n\tsmmu->evtq.q.llq.max_n_shift = min_t(u32, EVTQ_MAX_SZ_SHIFT,\n\t\t\t\t\t     FIELD_GET(IDR1_EVTQS, reg));\n\tsmmu->priq.q.llq.max_n_shift = min_t(u32, PRIQ_MAX_SZ_SHIFT,\n\t\t\t\t\t     FIELD_GET(IDR1_PRIQS, reg));\n\n\t \n\tsmmu->ssid_bits = FIELD_GET(IDR1_SSIDSIZE, reg);\n\tsmmu->sid_bits = FIELD_GET(IDR1_SIDSIZE, reg);\n\tsmmu->iommu.max_pasids = 1UL << smmu->ssid_bits;\n\n\t \n\tif (smmu->sid_bits <= STRTAB_SPLIT)\n\t\tsmmu->features &= ~ARM_SMMU_FEAT_2_LVL_STRTAB;\n\n\t \n\treg = readl_relaxed(smmu->base + ARM_SMMU_IDR3);\n\tif (FIELD_GET(IDR3_RIL, reg))\n\t\tsmmu->features |= ARM_SMMU_FEAT_RANGE_INV;\n\n\t \n\treg = readl_relaxed(smmu->base + ARM_SMMU_IDR5);\n\n\t \n\tsmmu->evtq.max_stalls = FIELD_GET(IDR5_STALL_MAX, reg);\n\n\t \n\tif (reg & IDR5_GRAN64K)\n\t\tsmmu->pgsize_bitmap |= SZ_64K | SZ_512M;\n\tif (reg & IDR5_GRAN16K)\n\t\tsmmu->pgsize_bitmap |= SZ_16K | SZ_32M;\n\tif (reg & IDR5_GRAN4K)\n\t\tsmmu->pgsize_bitmap |= SZ_4K | SZ_2M | SZ_1G;\n\n\t \n\tif (FIELD_GET(IDR5_VAX, reg) == IDR5_VAX_52_BIT)\n\t\tsmmu->features |= ARM_SMMU_FEAT_VAX;\n\n\t \n\tswitch (FIELD_GET(IDR5_OAS, reg)) {\n\tcase IDR5_OAS_32_BIT:\n\t\tsmmu->oas = 32;\n\t\tbreak;\n\tcase IDR5_OAS_36_BIT:\n\t\tsmmu->oas = 36;\n\t\tbreak;\n\tcase IDR5_OAS_40_BIT:\n\t\tsmmu->oas = 40;\n\t\tbreak;\n\tcase IDR5_OAS_42_BIT:\n\t\tsmmu->oas = 42;\n\t\tbreak;\n\tcase IDR5_OAS_44_BIT:\n\t\tsmmu->oas = 44;\n\t\tbreak;\n\tcase IDR5_OAS_52_BIT:\n\t\tsmmu->oas = 52;\n\t\tsmmu->pgsize_bitmap |= 1ULL << 42;  \n\t\tbreak;\n\tdefault:\n\t\tdev_info(smmu->dev,\n\t\t\t\"unknown output address size. Truncating to 48-bit\\n\");\n\t\tfallthrough;\n\tcase IDR5_OAS_48_BIT:\n\t\tsmmu->oas = 48;\n\t}\n\n\tif (arm_smmu_ops.pgsize_bitmap == -1UL)\n\t\tarm_smmu_ops.pgsize_bitmap = smmu->pgsize_bitmap;\n\telse\n\t\tarm_smmu_ops.pgsize_bitmap |= smmu->pgsize_bitmap;\n\n\t \n\tif (dma_set_mask_and_coherent(smmu->dev, DMA_BIT_MASK(smmu->oas)))\n\t\tdev_warn(smmu->dev,\n\t\t\t \"failed to set DMA mask for table walker\\n\");\n\n\tsmmu->ias = max(smmu->ias, smmu->oas);\n\n\tif ((smmu->features & ARM_SMMU_FEAT_TRANS_S1) &&\n\t    (smmu->features & ARM_SMMU_FEAT_TRANS_S2))\n\t\tsmmu->features |= ARM_SMMU_FEAT_NESTING;\n\n\tarm_smmu_device_iidr_probe(smmu);\n\n\tif (arm_smmu_sva_supported(smmu))\n\t\tsmmu->features |= ARM_SMMU_FEAT_SVA;\n\n\tdev_info(smmu->dev, \"ias %lu-bit, oas %lu-bit (features 0x%08x)\\n\",\n\t\t smmu->ias, smmu->oas, smmu->features);\n\treturn 0;\n}\n\n#ifdef CONFIG_ACPI\nstatic void acpi_smmu_get_options(u32 model, struct arm_smmu_device *smmu)\n{\n\tswitch (model) {\n\tcase ACPI_IORT_SMMU_V3_CAVIUM_CN99XX:\n\t\tsmmu->options |= ARM_SMMU_OPT_PAGE0_REGS_ONLY;\n\t\tbreak;\n\tcase ACPI_IORT_SMMU_V3_HISILICON_HI161X:\n\t\tsmmu->options |= ARM_SMMU_OPT_SKIP_PREFETCH;\n\t\tbreak;\n\t}\n\n\tdev_notice(smmu->dev, \"option mask 0x%x\\n\", smmu->options);\n}\n\nstatic int arm_smmu_device_acpi_probe(struct platform_device *pdev,\n\t\t\t\t      struct arm_smmu_device *smmu)\n{\n\tstruct acpi_iort_smmu_v3 *iort_smmu;\n\tstruct device *dev = smmu->dev;\n\tstruct acpi_iort_node *node;\n\n\tnode = *(struct acpi_iort_node **)dev_get_platdata(dev);\n\n\t \n\tiort_smmu = (struct acpi_iort_smmu_v3 *)node->node_data;\n\n\tacpi_smmu_get_options(iort_smmu->model, smmu);\n\n\tif (iort_smmu->flags & ACPI_IORT_SMMU_V3_COHACC_OVERRIDE)\n\t\tsmmu->features |= ARM_SMMU_FEAT_COHERENCY;\n\n\treturn 0;\n}\n#else\nstatic inline int arm_smmu_device_acpi_probe(struct platform_device *pdev,\n\t\t\t\t\t     struct arm_smmu_device *smmu)\n{\n\treturn -ENODEV;\n}\n#endif\n\nstatic int arm_smmu_device_dt_probe(struct platform_device *pdev,\n\t\t\t\t    struct arm_smmu_device *smmu)\n{\n\tstruct device *dev = &pdev->dev;\n\tu32 cells;\n\tint ret = -EINVAL;\n\n\tif (of_property_read_u32(dev->of_node, \"#iommu-cells\", &cells))\n\t\tdev_err(dev, \"missing #iommu-cells property\\n\");\n\telse if (cells != 1)\n\t\tdev_err(dev, \"invalid #iommu-cells value (%d)\\n\", cells);\n\telse\n\t\tret = 0;\n\n\tparse_driver_options(smmu);\n\n\tif (of_dma_is_coherent(dev->of_node))\n\t\tsmmu->features |= ARM_SMMU_FEAT_COHERENCY;\n\n\treturn ret;\n}\n\nstatic unsigned long arm_smmu_resource_size(struct arm_smmu_device *smmu)\n{\n\tif (smmu->options & ARM_SMMU_OPT_PAGE0_REGS_ONLY)\n\t\treturn SZ_64K;\n\telse\n\t\treturn SZ_128K;\n}\n\nstatic void __iomem *arm_smmu_ioremap(struct device *dev, resource_size_t start,\n\t\t\t\t      resource_size_t size)\n{\n\tstruct resource res = DEFINE_RES_MEM(start, size);\n\n\treturn devm_ioremap_resource(dev, &res);\n}\n\nstatic void arm_smmu_rmr_install_bypass_ste(struct arm_smmu_device *smmu)\n{\n\tstruct list_head rmr_list;\n\tstruct iommu_resv_region *e;\n\n\tINIT_LIST_HEAD(&rmr_list);\n\tiort_get_rmr_sids(dev_fwnode(smmu->dev), &rmr_list);\n\n\tlist_for_each_entry(e, &rmr_list, list) {\n\t\t__le64 *step;\n\t\tstruct iommu_iort_rmr_data *rmr;\n\t\tint ret, i;\n\n\t\trmr = container_of(e, struct iommu_iort_rmr_data, rr);\n\t\tfor (i = 0; i < rmr->num_sids; i++) {\n\t\t\tret = arm_smmu_init_sid_strtab(smmu, rmr->sids[i]);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(smmu->dev, \"RMR SID(0x%x) bypass failed\\n\",\n\t\t\t\t\trmr->sids[i]);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstep = arm_smmu_get_step_for_sid(smmu, rmr->sids[i]);\n\t\t\tarm_smmu_init_bypass_stes(step, 1, true);\n\t\t}\n\t}\n\n\tiort_put_rmr_sids(dev_fwnode(smmu->dev), &rmr_list);\n}\n\nstatic int arm_smmu_device_probe(struct platform_device *pdev)\n{\n\tint irq, ret;\n\tstruct resource *res;\n\tresource_size_t ioaddr;\n\tstruct arm_smmu_device *smmu;\n\tstruct device *dev = &pdev->dev;\n\tbool bypass;\n\n\tsmmu = devm_kzalloc(dev, sizeof(*smmu), GFP_KERNEL);\n\tif (!smmu)\n\t\treturn -ENOMEM;\n\tsmmu->dev = dev;\n\n\tif (dev->of_node) {\n\t\tret = arm_smmu_device_dt_probe(pdev, smmu);\n\t} else {\n\t\tret = arm_smmu_device_acpi_probe(pdev, smmu);\n\t\tif (ret == -ENODEV)\n\t\t\treturn ret;\n\t}\n\n\t \n\tbypass = !!ret;\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res)\n\t\treturn -EINVAL;\n\tif (resource_size(res) < arm_smmu_resource_size(smmu)) {\n\t\tdev_err(dev, \"MMIO region too small (%pr)\\n\", res);\n\t\treturn -EINVAL;\n\t}\n\tioaddr = res->start;\n\n\t \n\tsmmu->base = arm_smmu_ioremap(dev, ioaddr, ARM_SMMU_REG_SZ);\n\tif (IS_ERR(smmu->base))\n\t\treturn PTR_ERR(smmu->base);\n\n\tif (arm_smmu_resource_size(smmu) > SZ_64K) {\n\t\tsmmu->page1 = arm_smmu_ioremap(dev, ioaddr + SZ_64K,\n\t\t\t\t\t       ARM_SMMU_REG_SZ);\n\t\tif (IS_ERR(smmu->page1))\n\t\t\treturn PTR_ERR(smmu->page1);\n\t} else {\n\t\tsmmu->page1 = smmu->base;\n\t}\n\n\t \n\n\tirq = platform_get_irq_byname_optional(pdev, \"combined\");\n\tif (irq > 0)\n\t\tsmmu->combined_irq = irq;\n\telse {\n\t\tirq = platform_get_irq_byname_optional(pdev, \"eventq\");\n\t\tif (irq > 0)\n\t\t\tsmmu->evtq.q.irq = irq;\n\n\t\tirq = platform_get_irq_byname_optional(pdev, \"priq\");\n\t\tif (irq > 0)\n\t\t\tsmmu->priq.q.irq = irq;\n\n\t\tirq = platform_get_irq_byname_optional(pdev, \"gerror\");\n\t\tif (irq > 0)\n\t\t\tsmmu->gerr_irq = irq;\n\t}\n\t \n\tret = arm_smmu_device_hw_probe(smmu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = arm_smmu_init_structures(smmu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tplatform_set_drvdata(pdev, smmu);\n\n\t \n\tarm_smmu_rmr_install_bypass_ste(smmu);\n\n\t \n\tret = arm_smmu_device_reset(smmu, bypass);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = iommu_device_sysfs_add(&smmu->iommu, dev, NULL,\n\t\t\t\t     \"smmu3.%pa\", &ioaddr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to register iommu\\n\");\n\t\tiommu_device_sysfs_remove(&smmu->iommu);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void arm_smmu_device_remove(struct platform_device *pdev)\n{\n\tstruct arm_smmu_device *smmu = platform_get_drvdata(pdev);\n\n\tiommu_device_unregister(&smmu->iommu);\n\tiommu_device_sysfs_remove(&smmu->iommu);\n\tarm_smmu_device_disable(smmu);\n\tiopf_queue_free(smmu->evtq.iopf);\n\tida_destroy(&smmu->vmid_map);\n}\n\nstatic void arm_smmu_device_shutdown(struct platform_device *pdev)\n{\n\tstruct arm_smmu_device *smmu = platform_get_drvdata(pdev);\n\n\tarm_smmu_device_disable(smmu);\n}\n\nstatic const struct of_device_id arm_smmu_of_match[] = {\n\t{ .compatible = \"arm,smmu-v3\", },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, arm_smmu_of_match);\n\nstatic void arm_smmu_driver_unregister(struct platform_driver *drv)\n{\n\tarm_smmu_sva_notifier_synchronize();\n\tplatform_driver_unregister(drv);\n}\n\nstatic struct platform_driver arm_smmu_driver = {\n\t.driver\t= {\n\t\t.name\t\t\t= \"arm-smmu-v3\",\n\t\t.of_match_table\t\t= arm_smmu_of_match,\n\t\t.suppress_bind_attrs\t= true,\n\t},\n\t.probe\t= arm_smmu_device_probe,\n\t.remove_new = arm_smmu_device_remove,\n\t.shutdown = arm_smmu_device_shutdown,\n};\nmodule_driver(arm_smmu_driver, platform_driver_register,\n\t      arm_smmu_driver_unregister);\n\nMODULE_DESCRIPTION(\"IOMMU API for ARM architected SMMUv3 implementations\");\nMODULE_AUTHOR(\"Will Deacon <will@kernel.org>\");\nMODULE_ALIAS(\"platform:arm-smmu-v3\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}