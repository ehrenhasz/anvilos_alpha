{
  "module_name": "arm-smmu-v3-sva.c",
  "hash_id": "0e8ca16271c0652236e2c0778cbf9f29443ad6bc83be55c98507d3532834d0dc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/mmu_context.h>\n#include <linux/mmu_notifier.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n\n#include \"arm-smmu-v3.h\"\n#include \"../../iommu-sva.h\"\n#include \"../../io-pgtable-arm.h\"\n\nstruct arm_smmu_mmu_notifier {\n\tstruct mmu_notifier\t\tmn;\n\tstruct arm_smmu_ctx_desc\t*cd;\n\tbool\t\t\t\tcleared;\n\trefcount_t\t\t\trefs;\n\tstruct list_head\t\tlist;\n\tstruct arm_smmu_domain\t\t*domain;\n};\n\n#define mn_to_smmu(mn) container_of(mn, struct arm_smmu_mmu_notifier, mn)\n\nstruct arm_smmu_bond {\n\tstruct iommu_sva\t\tsva;\n\tstruct mm_struct\t\t*mm;\n\tstruct arm_smmu_mmu_notifier\t*smmu_mn;\n\tstruct list_head\t\tlist;\n\trefcount_t\t\t\trefs;\n};\n\n#define sva_to_bond(handle) \\\n\tcontainer_of(handle, struct arm_smmu_bond, sva)\n\nstatic DEFINE_MUTEX(sva_lock);\n\n \nstatic struct arm_smmu_ctx_desc *\narm_smmu_share_asid(struct mm_struct *mm, u16 asid)\n{\n\tint ret;\n\tu32 new_asid;\n\tstruct arm_smmu_ctx_desc *cd;\n\tstruct arm_smmu_device *smmu;\n\tstruct arm_smmu_domain *smmu_domain;\n\n\tcd = xa_load(&arm_smmu_asid_xa, asid);\n\tif (!cd)\n\t\treturn NULL;\n\n\tif (cd->mm) {\n\t\tif (WARN_ON(cd->mm != mm))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t \n\t\trefcount_inc(&cd->refs);\n\t\treturn cd;\n\t}\n\n\tsmmu_domain = container_of(cd, struct arm_smmu_domain, s1_cfg.cd);\n\tsmmu = smmu_domain->smmu;\n\n\tret = xa_alloc(&arm_smmu_asid_xa, &new_asid, cd,\n\t\t       XA_LIMIT(1, (1 << smmu->asid_bits) - 1), GFP_KERNEL);\n\tif (ret)\n\t\treturn ERR_PTR(-ENOSPC);\n\t \n\tcd->asid = new_asid;\n\t \n\tarm_smmu_write_ctx_desc(smmu_domain, IOMMU_NO_PASID, cd);\n\n\t \n\tarm_smmu_tlb_inv_asid(smmu, asid);\n\n\txa_erase(&arm_smmu_asid_xa, asid);\n\treturn NULL;\n}\n\nstatic struct arm_smmu_ctx_desc *arm_smmu_alloc_shared_cd(struct mm_struct *mm)\n{\n\tu16 asid;\n\tint err = 0;\n\tu64 tcr, par, reg;\n\tstruct arm_smmu_ctx_desc *cd;\n\tstruct arm_smmu_ctx_desc *ret = NULL;\n\n\t \n\tmmgrab(mm);\n\n\tasid = arm64_mm_context_get(mm);\n\tif (!asid) {\n\t\terr = -ESRCH;\n\t\tgoto out_drop_mm;\n\t}\n\n\tcd = kzalloc(sizeof(*cd), GFP_KERNEL);\n\tif (!cd) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_context;\n\t}\n\n\trefcount_set(&cd->refs, 1);\n\n\tmutex_lock(&arm_smmu_asid_lock);\n\tret = arm_smmu_share_asid(mm, asid);\n\tif (ret) {\n\t\tmutex_unlock(&arm_smmu_asid_lock);\n\t\tgoto out_free_cd;\n\t}\n\n\terr = xa_insert(&arm_smmu_asid_xa, asid, cd, GFP_KERNEL);\n\tmutex_unlock(&arm_smmu_asid_lock);\n\n\tif (err)\n\t\tgoto out_free_asid;\n\n\ttcr = FIELD_PREP(CTXDESC_CD_0_TCR_T0SZ, 64ULL - vabits_actual) |\n\t      FIELD_PREP(CTXDESC_CD_0_TCR_IRGN0, ARM_LPAE_TCR_RGN_WBWA) |\n\t      FIELD_PREP(CTXDESC_CD_0_TCR_ORGN0, ARM_LPAE_TCR_RGN_WBWA) |\n\t      FIELD_PREP(CTXDESC_CD_0_TCR_SH0, ARM_LPAE_TCR_SH_IS) |\n\t      CTXDESC_CD_0_TCR_EPD1 | CTXDESC_CD_0_AA64;\n\n\tswitch (PAGE_SIZE) {\n\tcase SZ_4K:\n\t\ttcr |= FIELD_PREP(CTXDESC_CD_0_TCR_TG0, ARM_LPAE_TCR_TG0_4K);\n\t\tbreak;\n\tcase SZ_16K:\n\t\ttcr |= FIELD_PREP(CTXDESC_CD_0_TCR_TG0, ARM_LPAE_TCR_TG0_16K);\n\t\tbreak;\n\tcase SZ_64K:\n\t\ttcr |= FIELD_PREP(CTXDESC_CD_0_TCR_TG0, ARM_LPAE_TCR_TG0_64K);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\terr = -EINVAL;\n\t\tgoto out_free_asid;\n\t}\n\n\treg = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);\n\tpar = cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR0_EL1_PARANGE_SHIFT);\n\ttcr |= FIELD_PREP(CTXDESC_CD_0_TCR_IPS, par);\n\n\tcd->ttbr = virt_to_phys(mm->pgd);\n\tcd->tcr = tcr;\n\t \n\tcd->mair = read_sysreg(mair_el1);\n\tcd->asid = asid;\n\tcd->mm = mm;\n\n\treturn cd;\n\nout_free_asid:\n\tarm_smmu_free_asid(cd);\nout_free_cd:\n\tkfree(cd);\nout_put_context:\n\tarm64_mm_context_put(mm);\nout_drop_mm:\n\tmmdrop(mm);\n\treturn err < 0 ? ERR_PTR(err) : ret;\n}\n\nstatic void arm_smmu_free_shared_cd(struct arm_smmu_ctx_desc *cd)\n{\n\tif (arm_smmu_free_asid(cd)) {\n\t\t \n\t\tarm64_mm_context_put(cd->mm);\n\t\tmmdrop(cd->mm);\n\t\tkfree(cd);\n\t}\n}\n\n \n#define CMDQ_MAX_TLBI_OPS\t\t(1 << (PAGE_SHIFT - 3))\n\nstatic void arm_smmu_mm_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long end)\n{\n\tstruct arm_smmu_mmu_notifier *smmu_mn = mn_to_smmu(mn);\n\tstruct arm_smmu_domain *smmu_domain = smmu_mn->domain;\n\tsize_t size;\n\n\t \n\tsize = end - start;\n\tif (!(smmu_domain->smmu->features & ARM_SMMU_FEAT_RANGE_INV)) {\n\t\tif (size >= CMDQ_MAX_TLBI_OPS * PAGE_SIZE)\n\t\t\tsize = 0;\n\t} else {\n\t\tif (size == ULONG_MAX)\n\t\t\tsize = 0;\n\t}\n\n\tif (!(smmu_domain->smmu->features & ARM_SMMU_FEAT_BTM)) {\n\t\tif (!size)\n\t\t\tarm_smmu_tlb_inv_asid(smmu_domain->smmu,\n\t\t\t\t\t      smmu_mn->cd->asid);\n\t\telse\n\t\t\tarm_smmu_tlb_inv_range_asid(start, size,\n\t\t\t\t\t\t    smmu_mn->cd->asid,\n\t\t\t\t\t\t    PAGE_SIZE, false,\n\t\t\t\t\t\t    smmu_domain);\n\t}\n\n\tarm_smmu_atc_inv_domain(smmu_domain, mm->pasid, start, size);\n}\n\nstatic void arm_smmu_mm_release(struct mmu_notifier *mn, struct mm_struct *mm)\n{\n\tstruct arm_smmu_mmu_notifier *smmu_mn = mn_to_smmu(mn);\n\tstruct arm_smmu_domain *smmu_domain = smmu_mn->domain;\n\n\tmutex_lock(&sva_lock);\n\tif (smmu_mn->cleared) {\n\t\tmutex_unlock(&sva_lock);\n\t\treturn;\n\t}\n\n\t \n\tarm_smmu_write_ctx_desc(smmu_domain, mm->pasid, &quiet_cd);\n\n\tarm_smmu_tlb_inv_asid(smmu_domain->smmu, smmu_mn->cd->asid);\n\tarm_smmu_atc_inv_domain(smmu_domain, mm->pasid, 0, 0);\n\n\tsmmu_mn->cleared = true;\n\tmutex_unlock(&sva_lock);\n}\n\nstatic void arm_smmu_mmu_notifier_free(struct mmu_notifier *mn)\n{\n\tkfree(mn_to_smmu(mn));\n}\n\nstatic const struct mmu_notifier_ops arm_smmu_mmu_notifier_ops = {\n\t.arch_invalidate_secondary_tlbs\t= arm_smmu_mm_arch_invalidate_secondary_tlbs,\n\t.release\t\t\t= arm_smmu_mm_release,\n\t.free_notifier\t\t\t= arm_smmu_mmu_notifier_free,\n};\n\n \nstatic struct arm_smmu_mmu_notifier *\narm_smmu_mmu_notifier_get(struct arm_smmu_domain *smmu_domain,\n\t\t\t  struct mm_struct *mm)\n{\n\tint ret;\n\tstruct arm_smmu_ctx_desc *cd;\n\tstruct arm_smmu_mmu_notifier *smmu_mn;\n\n\tlist_for_each_entry(smmu_mn, &smmu_domain->mmu_notifiers, list) {\n\t\tif (smmu_mn->mn.mm == mm) {\n\t\t\trefcount_inc(&smmu_mn->refs);\n\t\t\treturn smmu_mn;\n\t\t}\n\t}\n\n\tcd = arm_smmu_alloc_shared_cd(mm);\n\tif (IS_ERR(cd))\n\t\treturn ERR_CAST(cd);\n\n\tsmmu_mn = kzalloc(sizeof(*smmu_mn), GFP_KERNEL);\n\tif (!smmu_mn) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_cd;\n\t}\n\n\trefcount_set(&smmu_mn->refs, 1);\n\tsmmu_mn->cd = cd;\n\tsmmu_mn->domain = smmu_domain;\n\tsmmu_mn->mn.ops = &arm_smmu_mmu_notifier_ops;\n\n\tret = mmu_notifier_register(&smmu_mn->mn, mm);\n\tif (ret) {\n\t\tkfree(smmu_mn);\n\t\tgoto err_free_cd;\n\t}\n\n\tret = arm_smmu_write_ctx_desc(smmu_domain, mm->pasid, cd);\n\tif (ret)\n\t\tgoto err_put_notifier;\n\n\tlist_add(&smmu_mn->list, &smmu_domain->mmu_notifiers);\n\treturn smmu_mn;\n\nerr_put_notifier:\n\t \n\tmmu_notifier_put(&smmu_mn->mn);\nerr_free_cd:\n\tarm_smmu_free_shared_cd(cd);\n\treturn ERR_PTR(ret);\n}\n\nstatic void arm_smmu_mmu_notifier_put(struct arm_smmu_mmu_notifier *smmu_mn)\n{\n\tstruct mm_struct *mm = smmu_mn->mn.mm;\n\tstruct arm_smmu_ctx_desc *cd = smmu_mn->cd;\n\tstruct arm_smmu_domain *smmu_domain = smmu_mn->domain;\n\n\tif (!refcount_dec_and_test(&smmu_mn->refs))\n\t\treturn;\n\n\tlist_del(&smmu_mn->list);\n\tarm_smmu_write_ctx_desc(smmu_domain, mm->pasid, NULL);\n\n\t \n\tif (!smmu_mn->cleared) {\n\t\tarm_smmu_tlb_inv_asid(smmu_domain->smmu, cd->asid);\n\t\tarm_smmu_atc_inv_domain(smmu_domain, mm->pasid, 0, 0);\n\t}\n\n\t \n\tmmu_notifier_put(&smmu_mn->mn);\n\tarm_smmu_free_shared_cd(cd);\n}\n\nstatic struct iommu_sva *\n__arm_smmu_sva_bind(struct device *dev, struct mm_struct *mm)\n{\n\tint ret;\n\tstruct arm_smmu_bond *bond;\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\tstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\tstruct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);\n\n\tif (!master || !master->sva_enabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\t \n\tlist_for_each_entry(bond, &master->bonds, list) {\n\t\tif (bond->mm == mm) {\n\t\t\trefcount_inc(&bond->refs);\n\t\t\treturn &bond->sva;\n\t\t}\n\t}\n\n\tbond = kzalloc(sizeof(*bond), GFP_KERNEL);\n\tif (!bond)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbond->mm = mm;\n\tbond->sva.dev = dev;\n\trefcount_set(&bond->refs, 1);\n\n\tbond->smmu_mn = arm_smmu_mmu_notifier_get(smmu_domain, mm);\n\tif (IS_ERR(bond->smmu_mn)) {\n\t\tret = PTR_ERR(bond->smmu_mn);\n\t\tgoto err_free_bond;\n\t}\n\n\tlist_add(&bond->list, &master->bonds);\n\treturn &bond->sva;\n\nerr_free_bond:\n\tkfree(bond);\n\treturn ERR_PTR(ret);\n}\n\nbool arm_smmu_sva_supported(struct arm_smmu_device *smmu)\n{\n\tunsigned long reg, fld;\n\tunsigned long oas;\n\tunsigned long asid_bits;\n\tu32 feat_mask = ARM_SMMU_FEAT_COHERENCY;\n\n\tif (vabits_actual == 52)\n\t\tfeat_mask |= ARM_SMMU_FEAT_VAX;\n\n\tif ((smmu->features & feat_mask) != feat_mask)\n\t\treturn false;\n\n\tif (!(smmu->pgsize_bitmap & PAGE_SIZE))\n\t\treturn false;\n\n\t \n\treg = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);\n\tfld = cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR0_EL1_PARANGE_SHIFT);\n\toas = id_aa64mmfr0_parange_to_phys_shift(fld);\n\tif (smmu->oas < oas)\n\t\treturn false;\n\n\t \n\tfld = cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR0_EL1_ASIDBITS_SHIFT);\n\tasid_bits = fld ? 16 : 8;\n\tif (smmu->asid_bits < asid_bits)\n\t\treturn false;\n\n\t \n\tif (arm64_kernel_unmapped_at_el0())\n\t\tasid_bits--;\n\tdev_dbg(smmu->dev, \"%d shared contexts\\n\", (1 << asid_bits) -\n\t\tnum_possible_cpus() - 2);\n\n\treturn true;\n}\n\nbool arm_smmu_master_iopf_supported(struct arm_smmu_master *master)\n{\n\t \n\tif (master->num_streams != 1)\n\t\treturn false;\n\n\treturn master->stall_enabled;\n}\n\nbool arm_smmu_master_sva_supported(struct arm_smmu_master *master)\n{\n\tif (!(master->smmu->features & ARM_SMMU_FEAT_SVA))\n\t\treturn false;\n\n\t \n\treturn master->ssid_bits;\n}\n\nbool arm_smmu_master_sva_enabled(struct arm_smmu_master *master)\n{\n\tbool enabled;\n\n\tmutex_lock(&sva_lock);\n\tenabled = master->sva_enabled;\n\tmutex_unlock(&sva_lock);\n\treturn enabled;\n}\n\nstatic int arm_smmu_master_sva_enable_iopf(struct arm_smmu_master *master)\n{\n\tint ret;\n\tstruct device *dev = master->dev;\n\n\t \n\tif (!arm_smmu_master_iopf_supported(master))\n\t\treturn 0;\n\n\tif (!master->iopf_enabled)\n\t\treturn -EINVAL;\n\n\tret = iopf_queue_add_device(master->smmu->evtq.iopf, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = iommu_register_device_fault_handler(dev, iommu_queue_iopf, dev);\n\tif (ret) {\n\t\tiopf_queue_remove_device(master->smmu->evtq.iopf, dev);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void arm_smmu_master_sva_disable_iopf(struct arm_smmu_master *master)\n{\n\tstruct device *dev = master->dev;\n\n\tif (!master->iopf_enabled)\n\t\treturn;\n\n\tiommu_unregister_device_fault_handler(dev);\n\tiopf_queue_remove_device(master->smmu->evtq.iopf, dev);\n}\n\nint arm_smmu_master_enable_sva(struct arm_smmu_master *master)\n{\n\tint ret;\n\n\tmutex_lock(&sva_lock);\n\tret = arm_smmu_master_sva_enable_iopf(master);\n\tif (!ret)\n\t\tmaster->sva_enabled = true;\n\tmutex_unlock(&sva_lock);\n\n\treturn ret;\n}\n\nint arm_smmu_master_disable_sva(struct arm_smmu_master *master)\n{\n\tmutex_lock(&sva_lock);\n\tif (!list_empty(&master->bonds)) {\n\t\tdev_err(master->dev, \"cannot disable SVA, device is bound\\n\");\n\t\tmutex_unlock(&sva_lock);\n\t\treturn -EBUSY;\n\t}\n\tarm_smmu_master_sva_disable_iopf(master);\n\tmaster->sva_enabled = false;\n\tmutex_unlock(&sva_lock);\n\n\treturn 0;\n}\n\nvoid arm_smmu_sva_notifier_synchronize(void)\n{\n\t \n\tmmu_notifier_synchronize();\n}\n\nvoid arm_smmu_sva_remove_dev_pasid(struct iommu_domain *domain,\n\t\t\t\t   struct device *dev, ioasid_t id)\n{\n\tstruct mm_struct *mm = domain->mm;\n\tstruct arm_smmu_bond *bond = NULL, *t;\n\tstruct arm_smmu_master *master = dev_iommu_priv_get(dev);\n\n\tmutex_lock(&sva_lock);\n\tlist_for_each_entry(t, &master->bonds, list) {\n\t\tif (t->mm == mm) {\n\t\t\tbond = t;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!WARN_ON(!bond) && refcount_dec_and_test(&bond->refs)) {\n\t\tlist_del(&bond->list);\n\t\tarm_smmu_mmu_notifier_put(bond->smmu_mn);\n\t\tkfree(bond);\n\t}\n\tmutex_unlock(&sva_lock);\n}\n\nstatic int arm_smmu_sva_set_dev_pasid(struct iommu_domain *domain,\n\t\t\t\t      struct device *dev, ioasid_t id)\n{\n\tint ret = 0;\n\tstruct iommu_sva *handle;\n\tstruct mm_struct *mm = domain->mm;\n\n\tmutex_lock(&sva_lock);\n\thandle = __arm_smmu_sva_bind(dev, mm);\n\tif (IS_ERR(handle))\n\t\tret = PTR_ERR(handle);\n\tmutex_unlock(&sva_lock);\n\n\treturn ret;\n}\n\nstatic void arm_smmu_sva_domain_free(struct iommu_domain *domain)\n{\n\tkfree(domain);\n}\n\nstatic const struct iommu_domain_ops arm_smmu_sva_domain_ops = {\n\t.set_dev_pasid\t\t= arm_smmu_sva_set_dev_pasid,\n\t.free\t\t\t= arm_smmu_sva_domain_free\n};\n\nstruct iommu_domain *arm_smmu_sva_domain_alloc(void)\n{\n\tstruct iommu_domain *domain;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\tdomain->ops = &arm_smmu_sva_domain_ops;\n\n\treturn domain;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}