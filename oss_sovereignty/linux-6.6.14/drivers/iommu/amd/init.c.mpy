{
  "module_name": "init.c",
  "hash_id": "44d66c12e901a62dc2e7fb7cab6b90197231f8987fa12afd68ed530492f94135",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/amd/init.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"AMD-Vi: \" fmt\n#define dev_fmt(fmt)    pr_fmt(fmt)\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/list.h>\n#include <linux/bitmap.h>\n#include <linux/slab.h>\n#include <linux/syscore_ops.h>\n#include <linux/interrupt.h>\n#include <linux/msi.h>\n#include <linux/irq.h>\n#include <linux/amd-iommu.h>\n#include <linux/export.h>\n#include <linux/kmemleak.h>\n#include <linux/cc_platform.h>\n#include <linux/iopoll.h>\n#include <asm/pci-direct.h>\n#include <asm/iommu.h>\n#include <asm/apic.h>\n#include <asm/gart.h>\n#include <asm/x86_init.h>\n#include <asm/io_apic.h>\n#include <asm/irq_remapping.h>\n#include <asm/set_memory.h>\n\n#include <linux/crash_dump.h>\n\n#include \"amd_iommu.h\"\n#include \"../irq_remapping.h\"\n\n \n#define IVRS_HEADER_LENGTH 48\n\n#define ACPI_IVHD_TYPE_MAX_SUPPORTED\t0x40\n#define ACPI_IVMD_TYPE_ALL              0x20\n#define ACPI_IVMD_TYPE                  0x21\n#define ACPI_IVMD_TYPE_RANGE            0x22\n\n#define IVHD_DEV_ALL                    0x01\n#define IVHD_DEV_SELECT                 0x02\n#define IVHD_DEV_SELECT_RANGE_START     0x03\n#define IVHD_DEV_RANGE_END              0x04\n#define IVHD_DEV_ALIAS                  0x42\n#define IVHD_DEV_ALIAS_RANGE            0x43\n#define IVHD_DEV_EXT_SELECT             0x46\n#define IVHD_DEV_EXT_SELECT_RANGE       0x47\n#define IVHD_DEV_SPECIAL\t\t0x48\n#define IVHD_DEV_ACPI_HID\t\t0xf0\n\n#define UID_NOT_PRESENT                 0\n#define UID_IS_INTEGER                  1\n#define UID_IS_CHARACTER                2\n\n#define IVHD_SPECIAL_IOAPIC\t\t1\n#define IVHD_SPECIAL_HPET\t\t2\n\n#define IVHD_FLAG_HT_TUN_EN_MASK        0x01\n#define IVHD_FLAG_PASSPW_EN_MASK        0x02\n#define IVHD_FLAG_RESPASSPW_EN_MASK     0x04\n#define IVHD_FLAG_ISOC_EN_MASK          0x08\n\n#define IVMD_FLAG_EXCL_RANGE            0x08\n#define IVMD_FLAG_IW                    0x04\n#define IVMD_FLAG_IR                    0x02\n#define IVMD_FLAG_UNITY_MAP             0x01\n\n#define ACPI_DEVFLAG_INITPASS           0x01\n#define ACPI_DEVFLAG_EXTINT             0x02\n#define ACPI_DEVFLAG_NMI                0x04\n#define ACPI_DEVFLAG_SYSMGT1            0x10\n#define ACPI_DEVFLAG_SYSMGT2            0x20\n#define ACPI_DEVFLAG_LINT0              0x40\n#define ACPI_DEVFLAG_LINT1              0x80\n#define ACPI_DEVFLAG_ATSDIS             0x10000000\n\n#define LOOP_TIMEOUT\t2000000\n\n#define IVRS_GET_SBDF_ID(seg, bus, dev, fn)\t(((seg & 0xffff) << 16) | ((bus & 0xff) << 8) \\\n\t\t\t\t\t\t | ((dev & 0x1f) << 3) | (fn & 0x7))\n\n \n\n \nstruct ivhd_header {\n\tu8 type;\n\tu8 flags;\n\tu16 length;\n\tu16 devid;\n\tu16 cap_ptr;\n\tu64 mmio_phys;\n\tu16 pci_seg;\n\tu16 info;\n\tu32 efr_attr;\n\n\t \n\tu64 efr_reg;  \n\tu64 efr_reg2;\n} __attribute__((packed));\n\n \nstruct ivhd_entry {\n\tu8 type;\n\tu16 devid;\n\tu8 flags;\n\tstruct_group(ext_hid,\n\t\tu32 ext;\n\t\tu32 hidh;\n\t);\n\tu64 cid;\n\tu8 uidf;\n\tu8 uidl;\n\tu8 uid;\n} __attribute__((packed));\n\n \nstruct ivmd_header {\n\tu8 type;\n\tu8 flags;\n\tu16 length;\n\tu16 devid;\n\tu16 aux;\n\tu16 pci_seg;\n\tu8  resv[6];\n\tu64 range_start;\n\tu64 range_length;\n} __attribute__((packed));\n\nbool amd_iommu_dump;\nbool amd_iommu_irq_remap __read_mostly;\n\nenum io_pgtable_fmt amd_iommu_pgtable = AMD_IOMMU_V1;\n \nint amd_iommu_gpt_level = PAGE_MODE_4_LEVEL;\n\nint amd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_VAPIC;\nstatic int amd_iommu_xt_mode = IRQ_REMAP_XAPIC_MODE;\n\nstatic bool amd_iommu_detected;\nstatic bool amd_iommu_disabled __initdata;\nstatic bool amd_iommu_force_enable __initdata;\nstatic bool amd_iommu_irtcachedis;\nstatic int amd_iommu_target_ivhd_type;\n\n \nu64 amd_iommu_efr;\nu64 amd_iommu_efr2;\n\n \nbool amd_iommu_snp_en;\nEXPORT_SYMBOL(amd_iommu_snp_en);\n\nLIST_HEAD(amd_iommu_pci_seg_list);\t \nLIST_HEAD(amd_iommu_list);\t\t \n\n \nstruct amd_iommu *amd_iommus[MAX_IOMMUS];\n\n \nstatic int amd_iommus_present;\n\n \nbool amd_iommu_np_cache __read_mostly;\nbool amd_iommu_iotlb_sup __read_mostly = true;\n\nu32 amd_iommu_max_pasid __read_mostly = ~0;\n\nbool amd_iommu_v2_present __read_mostly;\nstatic bool amd_iommu_pc_present __read_mostly;\nbool amdr_ivrs_remap_support __read_mostly;\n\nbool amd_iommu_force_isolation __read_mostly;\n\n \nunsigned long *amd_iommu_pd_alloc_bitmap;\n\nenum iommu_init_state {\n\tIOMMU_START_STATE,\n\tIOMMU_IVRS_DETECTED,\n\tIOMMU_ACPI_FINISHED,\n\tIOMMU_ENABLED,\n\tIOMMU_PCI_INIT,\n\tIOMMU_INTERRUPTS_EN,\n\tIOMMU_INITIALIZED,\n\tIOMMU_NOT_FOUND,\n\tIOMMU_INIT_ERROR,\n\tIOMMU_CMDLINE_DISABLED,\n};\n\n \n#define EARLY_MAP_SIZE\t\t4\nstatic struct devid_map __initdata early_ioapic_map[EARLY_MAP_SIZE];\nstatic struct devid_map __initdata early_hpet_map[EARLY_MAP_SIZE];\nstatic struct acpihid_map_entry __initdata early_acpihid_map[EARLY_MAP_SIZE];\n\nstatic int __initdata early_ioapic_map_size;\nstatic int __initdata early_hpet_map_size;\nstatic int __initdata early_acpihid_map_size;\n\nstatic bool __initdata cmdline_maps;\n\nstatic enum iommu_init_state init_state = IOMMU_START_STATE;\n\nstatic int amd_iommu_enable_interrupts(void);\nstatic int __init iommu_go_to_state(enum iommu_init_state state);\nstatic void init_device_table_dma(struct amd_iommu_pci_seg *pci_seg);\n\nstatic bool amd_iommu_pre_enabled = true;\n\nstatic u32 amd_iommu_ivinfo __initdata;\n\nbool translation_pre_enabled(struct amd_iommu *iommu)\n{\n\treturn (iommu->flags & AMD_IOMMU_FLAG_TRANS_PRE_ENABLED);\n}\n\nstatic void clear_translation_pre_enabled(struct amd_iommu *iommu)\n{\n\tiommu->flags &= ~AMD_IOMMU_FLAG_TRANS_PRE_ENABLED;\n}\n\nstatic void init_translation_status(struct amd_iommu *iommu)\n{\n\tu64 ctrl;\n\n\tctrl = readq(iommu->mmio_base + MMIO_CONTROL_OFFSET);\n\tif (ctrl & (1<<CONTROL_IOMMU_EN))\n\t\tiommu->flags |= AMD_IOMMU_FLAG_TRANS_PRE_ENABLED;\n}\n\nstatic inline unsigned long tbl_size(int entry_size, int last_bdf)\n{\n\tunsigned shift = PAGE_SHIFT +\n\t\t\t get_order((last_bdf + 1) * entry_size);\n\n\treturn 1UL << shift;\n}\n\nint amd_iommu_get_num_iommus(void)\n{\n\treturn amd_iommus_present;\n}\n\n \nstatic void get_global_efr(void)\n{\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu) {\n\t\tu64 tmp = iommu->features;\n\t\tu64 tmp2 = iommu->features2;\n\n\t\tif (list_is_first(&iommu->list, &amd_iommu_list)) {\n\t\t\tamd_iommu_efr = tmp;\n\t\t\tamd_iommu_efr2 = tmp2;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (amd_iommu_efr == tmp &&\n\t\t    amd_iommu_efr2 == tmp2)\n\t\t\tcontinue;\n\n\t\tpr_err(FW_BUG\n\t\t       \"Found inconsistent EFR/EFR2 %#llx,%#llx (global %#llx,%#llx) on iommu%d (%04x:%02x:%02x.%01x).\\n\",\n\t\t       tmp, tmp2, amd_iommu_efr, amd_iommu_efr2,\n\t\t       iommu->index, iommu->pci_seg->id,\n\t\t       PCI_BUS_NUM(iommu->devid), PCI_SLOT(iommu->devid),\n\t\t       PCI_FUNC(iommu->devid));\n\n\t\tamd_iommu_efr &= tmp;\n\t\tamd_iommu_efr2 &= tmp2;\n\t}\n\n\tpr_info(\"Using global IVHD EFR:%#llx, EFR2:%#llx\\n\", amd_iommu_efr, amd_iommu_efr2);\n}\n\nstatic bool check_feature_on_all_iommus(u64 mask)\n{\n\treturn !!(amd_iommu_efr & mask);\n}\n\nstatic inline int check_feature_gpt_level(void)\n{\n\treturn ((amd_iommu_efr >> FEATURE_GATS_SHIFT) & FEATURE_GATS_MASK);\n}\n\n \nstatic void __init early_iommu_features_init(struct amd_iommu *iommu,\n\t\t\t\t\t     struct ivhd_header *h)\n{\n\tif (amd_iommu_ivinfo & IOMMU_IVINFO_EFRSUP) {\n\t\tiommu->features = h->efr_reg;\n\t\tiommu->features2 = h->efr_reg2;\n\t}\n\tif (amd_iommu_ivinfo & IOMMU_IVINFO_DMA_REMAP)\n\t\tamdr_ivrs_remap_support = true;\n}\n\n \n\nstatic u32 iommu_read_l1(struct amd_iommu *iommu, u16 l1, u8 address)\n{\n\tu32 val;\n\n\tpci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\n\tpci_read_config_dword(iommu->dev, 0xfc, &val);\n\treturn val;\n}\n\nstatic void iommu_write_l1(struct amd_iommu *iommu, u16 l1, u8 address, u32 val)\n{\n\tpci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16 | 1 << 31));\n\tpci_write_config_dword(iommu->dev, 0xfc, val);\n\tpci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\n}\n\nstatic u32 iommu_read_l2(struct amd_iommu *iommu, u8 address)\n{\n\tu32 val;\n\n\tpci_write_config_dword(iommu->dev, 0xf0, address);\n\tpci_read_config_dword(iommu->dev, 0xf4, &val);\n\treturn val;\n}\n\nstatic void iommu_write_l2(struct amd_iommu *iommu, u8 address, u32 val)\n{\n\tpci_write_config_dword(iommu->dev, 0xf0, (address | 1 << 8));\n\tpci_write_config_dword(iommu->dev, 0xf4, val);\n}\n\n \n\n \nstatic void iommu_set_exclusion_range(struct amd_iommu *iommu)\n{\n\tu64 start = iommu->exclusion_start & PAGE_MASK;\n\tu64 limit = (start + iommu->exclusion_length - 1) & PAGE_MASK;\n\tu64 entry;\n\n\tif (!iommu->exclusion_start)\n\t\treturn;\n\n\tentry = start | MMIO_EXCL_ENABLE_MASK;\n\tmemcpy_toio(iommu->mmio_base + MMIO_EXCL_BASE_OFFSET,\n\t\t\t&entry, sizeof(entry));\n\n\tentry = limit;\n\tmemcpy_toio(iommu->mmio_base + MMIO_EXCL_LIMIT_OFFSET,\n\t\t\t&entry, sizeof(entry));\n}\n\nstatic void iommu_set_cwwb_range(struct amd_iommu *iommu)\n{\n\tu64 start = iommu_virt_to_phys((void *)iommu->cmd_sem);\n\tu64 entry = start & PM_ADDR_MASK;\n\n\tif (!check_feature_on_all_iommus(FEATURE_SNP))\n\t\treturn;\n\n\t \n\tmemcpy_toio(iommu->mmio_base + MMIO_EXCL_BASE_OFFSET,\n\t\t    &entry, sizeof(entry));\n\n\t \n\tmemcpy_toio(iommu->mmio_base + MMIO_EXCL_LIMIT_OFFSET,\n\t\t    &entry, sizeof(entry));\n}\n\n \nstatic void iommu_set_device_table(struct amd_iommu *iommu)\n{\n\tu64 entry;\n\tu32 dev_table_size = iommu->pci_seg->dev_table_size;\n\tvoid *dev_table = (void *)get_dev_table(iommu);\n\n\tBUG_ON(iommu->mmio_base == NULL);\n\n\tentry = iommu_virt_to_phys(dev_table);\n\tentry |= (dev_table_size >> 12) - 1;\n\tmemcpy_toio(iommu->mmio_base + MMIO_DEV_TABLE_OFFSET,\n\t\t\t&entry, sizeof(entry));\n}\n\n \nstatic void iommu_feature_enable(struct amd_iommu *iommu, u8 bit)\n{\n\tu64 ctrl;\n\n\tctrl = readq(iommu->mmio_base +  MMIO_CONTROL_OFFSET);\n\tctrl |= (1ULL << bit);\n\twriteq(ctrl, iommu->mmio_base +  MMIO_CONTROL_OFFSET);\n}\n\nstatic void iommu_feature_disable(struct amd_iommu *iommu, u8 bit)\n{\n\tu64 ctrl;\n\n\tctrl = readq(iommu->mmio_base + MMIO_CONTROL_OFFSET);\n\tctrl &= ~(1ULL << bit);\n\twriteq(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\n}\n\nstatic void iommu_set_inv_tlb_timeout(struct amd_iommu *iommu, int timeout)\n{\n\tu64 ctrl;\n\n\tctrl = readq(iommu->mmio_base + MMIO_CONTROL_OFFSET);\n\tctrl &= ~CTRL_INV_TO_MASK;\n\tctrl |= (timeout << CONTROL_INV_TIMEOUT) & CTRL_INV_TO_MASK;\n\twriteq(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\n}\n\n \nstatic void iommu_enable(struct amd_iommu *iommu)\n{\n\tiommu_feature_enable(iommu, CONTROL_IOMMU_EN);\n}\n\nstatic void iommu_disable(struct amd_iommu *iommu)\n{\n\tif (!iommu->mmio_base)\n\t\treturn;\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_EVT_INT_EN);\n\tiommu_feature_disable(iommu, CONTROL_EVT_LOG_EN);\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_GALOG_EN);\n\tiommu_feature_disable(iommu, CONTROL_GAINT_EN);\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_PPRLOG_EN);\n\tiommu_feature_disable(iommu, CONTROL_PPRINT_EN);\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_IOMMU_EN);\n\n\t \n\tiommu_feature_disable(iommu, CONTROL_IRTCACHEDIS);\n}\n\n \nstatic u8 __iomem * __init iommu_map_mmio_space(u64 address, u64 end)\n{\n\tif (!request_mem_region(address, end, \"amd_iommu\")) {\n\t\tpr_err(\"Can not reserve memory region %llx-%llx for mmio\\n\",\n\t\t\taddress, end);\n\t\tpr_err(\"This is a BIOS bug. Please contact your hardware vendor\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn (u8 __iomem *)ioremap(address, end);\n}\n\nstatic void __init iommu_unmap_mmio_space(struct amd_iommu *iommu)\n{\n\tif (iommu->mmio_base)\n\t\tiounmap(iommu->mmio_base);\n\trelease_mem_region(iommu->mmio_phys, iommu->mmio_phys_end);\n}\n\nstatic inline u32 get_ivhd_header_size(struct ivhd_header *h)\n{\n\tu32 size = 0;\n\n\tswitch (h->type) {\n\tcase 0x10:\n\t\tsize = 24;\n\t\tbreak;\n\tcase 0x11:\n\tcase 0x40:\n\t\tsize = 40;\n\t\tbreak;\n\t}\n\treturn size;\n}\n\n \n\n \nstatic inline int ivhd_entry_length(u8 *ivhd)\n{\n\tu32 type = ((struct ivhd_entry *)ivhd)->type;\n\n\tif (type < 0x80) {\n\t\treturn 0x04 << (*ivhd >> 6);\n\t} else if (type == IVHD_DEV_ACPI_HID) {\n\t\t \n\t\treturn *((u8 *)ivhd + 21) + 22;\n\t}\n\treturn 0;\n}\n\n \nstatic int __init find_last_devid_from_ivhd(struct ivhd_header *h)\n{\n\tu8 *p = (void *)h, *end = (void *)h;\n\tstruct ivhd_entry *dev;\n\tint last_devid = -EINVAL;\n\n\tu32 ivhd_size = get_ivhd_header_size(h);\n\n\tif (!ivhd_size) {\n\t\tpr_err(\"Unsupported IVHD type %#x\\n\", h->type);\n\t\treturn -EINVAL;\n\t}\n\n\tp += ivhd_size;\n\tend += h->length;\n\n\twhile (p < end) {\n\t\tdev = (struct ivhd_entry *)p;\n\t\tswitch (dev->type) {\n\t\tcase IVHD_DEV_ALL:\n\t\t\t \n\t\t\treturn 0xffff;\n\t\tcase IVHD_DEV_SELECT:\n\t\tcase IVHD_DEV_RANGE_END:\n\t\tcase IVHD_DEV_ALIAS:\n\t\tcase IVHD_DEV_EXT_SELECT:\n\t\t\t \n\t\t\tif (dev->devid > last_devid)\n\t\t\t\tlast_devid = dev->devid;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tp += ivhd_entry_length(p);\n\t}\n\n\tWARN_ON(p != end);\n\n\treturn last_devid;\n}\n\nstatic int __init check_ivrs_checksum(struct acpi_table_header *table)\n{\n\tint i;\n\tu8 checksum = 0, *p = (u8 *)table;\n\n\tfor (i = 0; i < table->length; ++i)\n\t\tchecksum += p[i];\n\tif (checksum != 0) {\n\t\t \n\t\tpr_err(FW_BUG \"IVRS invalid checksum\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init find_last_devid_acpi(struct acpi_table_header *table, u16 pci_seg)\n{\n\tu8 *p = (u8 *)table, *end = (u8 *)table;\n\tstruct ivhd_header *h;\n\tint last_devid, last_bdf = 0;\n\n\tp += IVRS_HEADER_LENGTH;\n\n\tend += table->length;\n\twhile (p < end) {\n\t\th = (struct ivhd_header *)p;\n\t\tif (h->pci_seg == pci_seg &&\n\t\t    h->type == amd_iommu_target_ivhd_type) {\n\t\t\tlast_devid = find_last_devid_from_ivhd(h);\n\n\t\t\tif (last_devid < 0)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (last_devid > last_bdf)\n\t\t\t\tlast_bdf = last_devid;\n\t\t}\n\t\tp += h->length;\n\t}\n\tWARN_ON(p != end);\n\n\treturn last_bdf;\n}\n\n \n\n \nstatic inline int __init alloc_dev_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tpci_seg->dev_table = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO | GFP_DMA32,\n\t\t\t\t\t\t      get_order(pci_seg->dev_table_size));\n\tif (!pci_seg->dev_table)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline void free_dev_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tfree_pages((unsigned long)pci_seg->dev_table,\n\t\t    get_order(pci_seg->dev_table_size));\n\tpci_seg->dev_table = NULL;\n}\n\n \nstatic inline int __init alloc_rlookup_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tpci_seg->rlookup_table = (void *)__get_free_pages(\n\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t\tget_order(pci_seg->rlookup_table_size));\n\tif (pci_seg->rlookup_table == NULL)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline void free_rlookup_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tfree_pages((unsigned long)pci_seg->rlookup_table,\n\t\t   get_order(pci_seg->rlookup_table_size));\n\tpci_seg->rlookup_table = NULL;\n}\n\nstatic inline int __init alloc_irq_lookup_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tpci_seg->irq_lookup_table = (void *)__get_free_pages(\n\t\t\t\t\t     GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t     get_order(pci_seg->rlookup_table_size));\n\tkmemleak_alloc(pci_seg->irq_lookup_table,\n\t\t       pci_seg->rlookup_table_size, 1, GFP_KERNEL);\n\tif (pci_seg->irq_lookup_table == NULL)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline void free_irq_lookup_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tkmemleak_free(pci_seg->irq_lookup_table);\n\tfree_pages((unsigned long)pci_seg->irq_lookup_table,\n\t\t   get_order(pci_seg->rlookup_table_size));\n\tpci_seg->irq_lookup_table = NULL;\n}\n\nstatic int __init alloc_alias_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tint i;\n\n\tpci_seg->alias_table = (void *)__get_free_pages(GFP_KERNEL,\n\t\t\t\t\tget_order(pci_seg->alias_table_size));\n\tif (!pci_seg->alias_table)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i <= pci_seg->last_bdf; ++i)\n\t\tpci_seg->alias_table[i] = i;\n\n\treturn 0;\n}\n\nstatic void __init free_alias_table(struct amd_iommu_pci_seg *pci_seg)\n{\n\tfree_pages((unsigned long)pci_seg->alias_table,\n\t\t   get_order(pci_seg->alias_table_size));\n\tpci_seg->alias_table = NULL;\n}\n\n \nstatic int __init alloc_command_buffer(struct amd_iommu *iommu)\n{\n\tiommu->cmd_buf = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t\t  get_order(CMD_BUFFER_SIZE));\n\n\treturn iommu->cmd_buf ? 0 : -ENOMEM;\n}\n\n \nstatic void amd_iommu_restart_log(struct amd_iommu *iommu, const char *evt_type,\n\t\t\t\t  u8 cntrl_intr, u8 cntrl_log,\n\t\t\t\t  u32 status_run_mask, u32 status_overflow_mask)\n{\n\tu32 status;\n\n\tstatus = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\tif (status & status_run_mask)\n\t\treturn;\n\n\tpr_info_ratelimited(\"IOMMU %s log restarting\\n\", evt_type);\n\n\tiommu_feature_disable(iommu, cntrl_log);\n\tiommu_feature_disable(iommu, cntrl_intr);\n\n\twritel(status_overflow_mask, iommu->mmio_base + MMIO_STATUS_OFFSET);\n\n\tiommu_feature_enable(iommu, cntrl_intr);\n\tiommu_feature_enable(iommu, cntrl_log);\n}\n\n \nvoid amd_iommu_restart_event_logging(struct amd_iommu *iommu)\n{\n\tamd_iommu_restart_log(iommu, \"Event\", CONTROL_EVT_INT_EN,\n\t\t\t      CONTROL_EVT_LOG_EN, MMIO_STATUS_EVT_RUN_MASK,\n\t\t\t      MMIO_STATUS_EVT_OVERFLOW_MASK);\n}\n\n \nvoid amd_iommu_restart_ga_log(struct amd_iommu *iommu)\n{\n\tamd_iommu_restart_log(iommu, \"GA\", CONTROL_GAINT_EN,\n\t\t\t      CONTROL_GALOG_EN, MMIO_STATUS_GALOG_RUN_MASK,\n\t\t\t      MMIO_STATUS_GALOG_OVERFLOW_MASK);\n}\n\n \nvoid amd_iommu_restart_ppr_log(struct amd_iommu *iommu)\n{\n\tamd_iommu_restart_log(iommu, \"PPR\", CONTROL_PPRINT_EN,\n\t\t\t      CONTROL_PPRLOG_EN, MMIO_STATUS_PPR_RUN_MASK,\n\t\t\t      MMIO_STATUS_PPR_OVERFLOW_MASK);\n}\n\n \nstatic void amd_iommu_reset_cmd_buffer(struct amd_iommu *iommu)\n{\n\tiommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\n\n\twritel(0x00, iommu->mmio_base + MMIO_CMD_HEAD_OFFSET);\n\twritel(0x00, iommu->mmio_base + MMIO_CMD_TAIL_OFFSET);\n\tiommu->cmd_buf_head = 0;\n\tiommu->cmd_buf_tail = 0;\n\n\tiommu_feature_enable(iommu, CONTROL_CMDBUF_EN);\n}\n\n \nstatic void iommu_enable_command_buffer(struct amd_iommu *iommu)\n{\n\tu64 entry;\n\n\tBUG_ON(iommu->cmd_buf == NULL);\n\n\tentry = iommu_virt_to_phys(iommu->cmd_buf);\n\tentry |= MMIO_CMD_SIZE_512;\n\n\tmemcpy_toio(iommu->mmio_base + MMIO_CMD_BUF_OFFSET,\n\t\t    &entry, sizeof(entry));\n\n\tamd_iommu_reset_cmd_buffer(iommu);\n}\n\n \nstatic void iommu_disable_command_buffer(struct amd_iommu *iommu)\n{\n\tiommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\n}\n\nstatic void __init free_command_buffer(struct amd_iommu *iommu)\n{\n\tfree_pages((unsigned long)iommu->cmd_buf, get_order(CMD_BUFFER_SIZE));\n}\n\nstatic void *__init iommu_alloc_4k_pages(struct amd_iommu *iommu,\n\t\t\t\t\t gfp_t gfp, size_t size)\n{\n\tint order = get_order(size);\n\tvoid *buf = (void *)__get_free_pages(gfp, order);\n\n\tif (buf &&\n\t    check_feature_on_all_iommus(FEATURE_SNP) &&\n\t    set_memory_4k((unsigned long)buf, (1 << order))) {\n\t\tfree_pages((unsigned long)buf, order);\n\t\tbuf = NULL;\n\t}\n\n\treturn buf;\n}\n\n \nstatic int __init alloc_event_buffer(struct amd_iommu *iommu)\n{\n\tiommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t      EVT_BUFFER_SIZE);\n\n\treturn iommu->evt_buf ? 0 : -ENOMEM;\n}\n\nstatic void iommu_enable_event_buffer(struct amd_iommu *iommu)\n{\n\tu64 entry;\n\n\tBUG_ON(iommu->evt_buf == NULL);\n\n\tentry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;\n\n\tmemcpy_toio(iommu->mmio_base + MMIO_EVT_BUF_OFFSET,\n\t\t    &entry, sizeof(entry));\n\n\t \n\twritel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);\n\twritel(0x00, iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);\n\n\tiommu_feature_enable(iommu, CONTROL_EVT_LOG_EN);\n}\n\n \nstatic void iommu_disable_event_buffer(struct amd_iommu *iommu)\n{\n\tiommu_feature_disable(iommu, CONTROL_EVT_LOG_EN);\n}\n\nstatic void __init free_event_buffer(struct amd_iommu *iommu)\n{\n\tfree_pages((unsigned long)iommu->evt_buf, get_order(EVT_BUFFER_SIZE));\n}\n\n \nstatic int __init alloc_ppr_log(struct amd_iommu *iommu)\n{\n\tiommu->ppr_log = iommu_alloc_4k_pages(iommu, GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t      PPR_LOG_SIZE);\n\n\treturn iommu->ppr_log ? 0 : -ENOMEM;\n}\n\nstatic void iommu_enable_ppr_log(struct amd_iommu *iommu)\n{\n\tu64 entry;\n\n\tif (iommu->ppr_log == NULL)\n\t\treturn;\n\n\tiommu_feature_enable(iommu, CONTROL_PPR_EN);\n\n\tentry = iommu_virt_to_phys(iommu->ppr_log) | PPR_LOG_SIZE_512;\n\n\tmemcpy_toio(iommu->mmio_base + MMIO_PPR_LOG_OFFSET,\n\t\t    &entry, sizeof(entry));\n\n\t \n\twritel(0x00, iommu->mmio_base + MMIO_PPR_HEAD_OFFSET);\n\twritel(0x00, iommu->mmio_base + MMIO_PPR_TAIL_OFFSET);\n\n\tiommu_feature_enable(iommu, CONTROL_PPRLOG_EN);\n\tiommu_feature_enable(iommu, CONTROL_PPRINT_EN);\n}\n\nstatic void __init free_ppr_log(struct amd_iommu *iommu)\n{\n\tfree_pages((unsigned long)iommu->ppr_log, get_order(PPR_LOG_SIZE));\n}\n\nstatic void free_ga_log(struct amd_iommu *iommu)\n{\n#ifdef CONFIG_IRQ_REMAP\n\tfree_pages((unsigned long)iommu->ga_log, get_order(GA_LOG_SIZE));\n\tfree_pages((unsigned long)iommu->ga_log_tail, get_order(8));\n#endif\n}\n\n#ifdef CONFIG_IRQ_REMAP\nstatic int iommu_ga_log_enable(struct amd_iommu *iommu)\n{\n\tu32 status, i;\n\tu64 entry;\n\n\tif (!iommu->ga_log)\n\t\treturn -EINVAL;\n\n\tentry = iommu_virt_to_phys(iommu->ga_log) | GA_LOG_SIZE_512;\n\tmemcpy_toio(iommu->mmio_base + MMIO_GA_LOG_BASE_OFFSET,\n\t\t    &entry, sizeof(entry));\n\tentry = (iommu_virt_to_phys(iommu->ga_log_tail) &\n\t\t (BIT_ULL(52)-1)) & ~7ULL;\n\tmemcpy_toio(iommu->mmio_base + MMIO_GA_LOG_TAIL_OFFSET,\n\t\t    &entry, sizeof(entry));\n\twritel(0x00, iommu->mmio_base + MMIO_GA_HEAD_OFFSET);\n\twritel(0x00, iommu->mmio_base + MMIO_GA_TAIL_OFFSET);\n\n\n\tiommu_feature_enable(iommu, CONTROL_GAINT_EN);\n\tiommu_feature_enable(iommu, CONTROL_GALOG_EN);\n\n\tfor (i = 0; i < LOOP_TIMEOUT; ++i) {\n\t\tstatus = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\t\tif (status & (MMIO_STATUS_GALOG_RUN_MASK))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tif (WARN_ON(i >= LOOP_TIMEOUT))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int iommu_init_ga_log(struct amd_iommu *iommu)\n{\n\tif (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir))\n\t\treturn 0;\n\n\tiommu->ga_log = (u8 *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\tget_order(GA_LOG_SIZE));\n\tif (!iommu->ga_log)\n\t\tgoto err_out;\n\n\tiommu->ga_log_tail = (u8 *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\tget_order(8));\n\tif (!iommu->ga_log_tail)\n\t\tgoto err_out;\n\n\treturn 0;\nerr_out:\n\tfree_ga_log(iommu);\n\treturn -EINVAL;\n}\n#endif  \n\nstatic int __init alloc_cwwb_sem(struct amd_iommu *iommu)\n{\n\tiommu->cmd_sem = iommu_alloc_4k_pages(iommu, GFP_KERNEL | __GFP_ZERO, 1);\n\n\treturn iommu->cmd_sem ? 0 : -ENOMEM;\n}\n\nstatic void __init free_cwwb_sem(struct amd_iommu *iommu)\n{\n\tif (iommu->cmd_sem)\n\t\tfree_page((unsigned long)iommu->cmd_sem);\n}\n\nstatic void iommu_enable_xt(struct amd_iommu *iommu)\n{\n#ifdef CONFIG_IRQ_REMAP\n\t \n\tif (AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir) &&\n\t    amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)\n\t\tiommu_feature_enable(iommu, CONTROL_XT_EN);\n#endif  \n}\n\nstatic void iommu_enable_gt(struct amd_iommu *iommu)\n{\n\tif (!iommu_feature(iommu, FEATURE_GT))\n\t\treturn;\n\n\tiommu_feature_enable(iommu, CONTROL_GT_EN);\n}\n\n \nstatic void __set_dev_entry_bit(struct dev_table_entry *dev_table,\n\t\t\t\tu16 devid, u8 bit)\n{\n\tint i = (bit >> 6) & 0x03;\n\tint _bit = bit & 0x3f;\n\n\tdev_table[devid].data[i] |= (1UL << _bit);\n}\n\nstatic void set_dev_entry_bit(struct amd_iommu *iommu, u16 devid, u8 bit)\n{\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\treturn __set_dev_entry_bit(dev_table, devid, bit);\n}\n\nstatic int __get_dev_entry_bit(struct dev_table_entry *dev_table,\n\t\t\t       u16 devid, u8 bit)\n{\n\tint i = (bit >> 6) & 0x03;\n\tint _bit = bit & 0x3f;\n\n\treturn (dev_table[devid].data[i] & (1UL << _bit)) >> _bit;\n}\n\nstatic int get_dev_entry_bit(struct amd_iommu *iommu, u16 devid, u8 bit)\n{\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\treturn __get_dev_entry_bit(dev_table, devid, bit);\n}\n\nstatic bool __copy_device_table(struct amd_iommu *iommu)\n{\n\tu64 int_ctl, int_tab_len, entry = 0;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\tstruct dev_table_entry *old_devtb = NULL;\n\tu32 lo, hi, devid, old_devtb_size;\n\tphys_addr_t old_devtb_phys;\n\tu16 dom_id, dte_v, irq_v;\n\tgfp_t gfp_flag;\n\tu64 tmp;\n\n\t \n\tlo = readl(iommu->mmio_base + MMIO_DEV_TABLE_OFFSET);\n\thi = readl(iommu->mmio_base + MMIO_DEV_TABLE_OFFSET + 4);\n\tentry = (((u64) hi) << 32) + lo;\n\n\told_devtb_size = ((entry & ~PAGE_MASK) + 1) << 12;\n\tif (old_devtb_size != pci_seg->dev_table_size) {\n\t\tpr_err(\"The device table size of IOMMU:%d is not expected!\\n\",\n\t\t\tiommu->index);\n\t\treturn false;\n\t}\n\n\t \n\told_devtb_phys = __sme_clr(entry) & PAGE_MASK;\n\n\tif (old_devtb_phys >= 0x100000000ULL) {\n\t\tpr_err(\"The address of old device table is above 4G, not trustworthy!\\n\");\n\t\treturn false;\n\t}\n\told_devtb = (cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT) && is_kdump_kernel())\n\t\t    ? (__force void *)ioremap_encrypted(old_devtb_phys,\n\t\t\t\t\t\t\tpci_seg->dev_table_size)\n\t\t    : memremap(old_devtb_phys, pci_seg->dev_table_size, MEMREMAP_WB);\n\n\tif (!old_devtb)\n\t\treturn false;\n\n\tgfp_flag = GFP_KERNEL | __GFP_ZERO | GFP_DMA32;\n\tpci_seg->old_dev_tbl_cpy = (void *)__get_free_pages(gfp_flag,\n\t\t\t\t\t\t    get_order(pci_seg->dev_table_size));\n\tif (pci_seg->old_dev_tbl_cpy == NULL) {\n\t\tpr_err(\"Failed to allocate memory for copying old device table!\\n\");\n\t\tmemunmap(old_devtb);\n\t\treturn false;\n\t}\n\n\tfor (devid = 0; devid <= pci_seg->last_bdf; ++devid) {\n\t\tpci_seg->old_dev_tbl_cpy[devid] = old_devtb[devid];\n\t\tdom_id = old_devtb[devid].data[1] & DEV_DOMID_MASK;\n\t\tdte_v = old_devtb[devid].data[0] & DTE_FLAG_V;\n\n\t\tif (dte_v && dom_id) {\n\t\t\tpci_seg->old_dev_tbl_cpy[devid].data[0] = old_devtb[devid].data[0];\n\t\t\tpci_seg->old_dev_tbl_cpy[devid].data[1] = old_devtb[devid].data[1];\n\t\t\t__set_bit(dom_id, amd_iommu_pd_alloc_bitmap);\n\t\t\t \n\t\t\tif (old_devtb[devid].data[0] & DTE_FLAG_GV) {\n\t\t\t\ttmp = DTE_GCR3_VAL_B(~0ULL) << DTE_GCR3_SHIFT_B;\n\t\t\t\ttmp |= DTE_GCR3_VAL_C(~0ULL) << DTE_GCR3_SHIFT_C;\n\t\t\t\tpci_seg->old_dev_tbl_cpy[devid].data[1] &= ~tmp;\n\t\t\t\ttmp = DTE_GCR3_VAL_A(~0ULL) << DTE_GCR3_SHIFT_A;\n\t\t\t\ttmp |= DTE_FLAG_GV;\n\t\t\t\tpci_seg->old_dev_tbl_cpy[devid].data[0] &= ~tmp;\n\t\t\t}\n\t\t}\n\n\t\tirq_v = old_devtb[devid].data[2] & DTE_IRQ_REMAP_ENABLE;\n\t\tint_ctl = old_devtb[devid].data[2] & DTE_IRQ_REMAP_INTCTL_MASK;\n\t\tint_tab_len = old_devtb[devid].data[2] & DTE_INTTABLEN_MASK;\n\t\tif (irq_v && (int_ctl || int_tab_len)) {\n\t\t\tif ((int_ctl != DTE_IRQ_REMAP_INTCTL) ||\n\t\t\t    (int_tab_len != DTE_INTTABLEN)) {\n\t\t\t\tpr_err(\"Wrong old irq remapping flag: %#x\\n\", devid);\n\t\t\t\tmemunmap(old_devtb);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tpci_seg->old_dev_tbl_cpy[devid].data[2] = old_devtb[devid].data[2];\n\t\t}\n\t}\n\tmemunmap(old_devtb);\n\n\treturn true;\n}\n\nstatic bool copy_device_table(void)\n{\n\tstruct amd_iommu *iommu;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\n\tif (!amd_iommu_pre_enabled)\n\t\treturn false;\n\n\tpr_warn(\"Translation is already enabled - trying to copy translation structures\\n\");\n\n\t \n\tfor_each_pci_segment(pci_seg) {\n\t\tfor_each_iommu(iommu) {\n\t\t\tif (pci_seg->id != iommu->pci_seg->id)\n\t\t\t\tcontinue;\n\t\t\tif (!__copy_device_table(iommu))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nvoid amd_iommu_apply_erratum_63(struct amd_iommu *iommu, u16 devid)\n{\n\tint sysmgt;\n\n\tsysmgt = get_dev_entry_bit(iommu, devid, DEV_ENTRY_SYSMGT1) |\n\t\t (get_dev_entry_bit(iommu, devid, DEV_ENTRY_SYSMGT2) << 1);\n\n\tif (sysmgt == 0x01)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_IW);\n}\n\n \nstatic void __init set_dev_entry_from_acpi(struct amd_iommu *iommu,\n\t\t\t\t\t   u16 devid, u32 flags, u32 ext_flags)\n{\n\tif (flags & ACPI_DEVFLAG_INITPASS)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_INIT_PASS);\n\tif (flags & ACPI_DEVFLAG_EXTINT)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_EINT_PASS);\n\tif (flags & ACPI_DEVFLAG_NMI)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_NMI_PASS);\n\tif (flags & ACPI_DEVFLAG_SYSMGT1)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_SYSMGT1);\n\tif (flags & ACPI_DEVFLAG_SYSMGT2)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_SYSMGT2);\n\tif (flags & ACPI_DEVFLAG_LINT0)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_LINT0_PASS);\n\tif (flags & ACPI_DEVFLAG_LINT1)\n\t\tset_dev_entry_bit(iommu, devid, DEV_ENTRY_LINT1_PASS);\n\n\tamd_iommu_apply_erratum_63(iommu, devid);\n\n\tamd_iommu_set_rlookup_table(iommu, devid);\n}\n\nint __init add_special_device(u8 type, u8 id, u32 *devid, bool cmd_line)\n{\n\tstruct devid_map *entry;\n\tstruct list_head *list;\n\n\tif (type == IVHD_SPECIAL_IOAPIC)\n\t\tlist = &ioapic_map;\n\telse if (type == IVHD_SPECIAL_HPET)\n\t\tlist = &hpet_map;\n\telse\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(entry, list, list) {\n\t\tif (!(entry->id == id && entry->cmd_line))\n\t\t\tcontinue;\n\n\t\tpr_info(\"Command-line override present for %s id %d - ignoring\\n\",\n\t\t\ttype == IVHD_SPECIAL_IOAPIC ? \"IOAPIC\" : \"HPET\", id);\n\n\t\t*devid = entry->devid;\n\n\t\treturn 0;\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tentry->id\t= id;\n\tentry->devid\t= *devid;\n\tentry->cmd_line\t= cmd_line;\n\n\tlist_add_tail(&entry->list, list);\n\n\treturn 0;\n}\n\nstatic int __init add_acpi_hid_device(u8 *hid, u8 *uid, u32 *devid,\n\t\t\t\t      bool cmd_line)\n{\n\tstruct acpihid_map_entry *entry;\n\tstruct list_head *list = &acpihid_map;\n\n\tlist_for_each_entry(entry, list, list) {\n\t\tif (strcmp(entry->hid, hid) ||\n\t\t    (*uid && *entry->uid && strcmp(entry->uid, uid)) ||\n\t\t    !entry->cmd_line)\n\t\t\tcontinue;\n\n\t\tpr_info(\"Command-line override for hid:%s uid:%s\\n\",\n\t\t\thid, uid);\n\t\t*devid = entry->devid;\n\t\treturn 0;\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tmemcpy(entry->uid, uid, strlen(uid));\n\tmemcpy(entry->hid, hid, strlen(hid));\n\tentry->devid = *devid;\n\tentry->cmd_line\t= cmd_line;\n\tentry->root_devid = (entry->devid & (~0x7));\n\n\tpr_info(\"%s, add hid:%s, uid:%s, rdevid:%d\\n\",\n\t\tentry->cmd_line ? \"cmd\" : \"ivrs\",\n\t\tentry->hid, entry->uid, entry->root_devid);\n\n\tlist_add_tail(&entry->list, list);\n\treturn 0;\n}\n\nstatic int __init add_early_maps(void)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < early_ioapic_map_size; ++i) {\n\t\tret = add_special_device(IVHD_SPECIAL_IOAPIC,\n\t\t\t\t\t early_ioapic_map[i].id,\n\t\t\t\t\t &early_ioapic_map[i].devid,\n\t\t\t\t\t early_ioapic_map[i].cmd_line);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < early_hpet_map_size; ++i) {\n\t\tret = add_special_device(IVHD_SPECIAL_HPET,\n\t\t\t\t\t early_hpet_map[i].id,\n\t\t\t\t\t &early_hpet_map[i].devid,\n\t\t\t\t\t early_hpet_map[i].cmd_line);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < early_acpihid_map_size; ++i) {\n\t\tret = add_acpi_hid_device(early_acpihid_map[i].hid,\n\t\t\t\t\t  early_acpihid_map[i].uid,\n\t\t\t\t\t  &early_acpihid_map[i].devid,\n\t\t\t\t\t  early_acpihid_map[i].cmd_line);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init init_iommu_from_acpi(struct amd_iommu *iommu,\n\t\t\t\t\tstruct ivhd_header *h)\n{\n\tu8 *p = (u8 *)h;\n\tu8 *end = p, flags = 0;\n\tu16 devid = 0, devid_start = 0, devid_to = 0, seg_id;\n\tu32 dev_i, ext_flags = 0;\n\tbool alias = false;\n\tstruct ivhd_entry *e;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\tu32 ivhd_size;\n\tint ret;\n\n\n\tret = add_early_maps();\n\tif (ret)\n\t\treturn ret;\n\n\tamd_iommu_apply_ivrs_quirks();\n\n\t \n\tiommu->acpi_flags = h->flags;\n\n\t \n\tivhd_size = get_ivhd_header_size(h);\n\tif (!ivhd_size) {\n\t\tpr_err(\"Unsupported IVHD type %#x\\n\", h->type);\n\t\treturn -EINVAL;\n\t}\n\n\tp += ivhd_size;\n\n\tend += h->length;\n\n\n\twhile (p < end) {\n\t\te = (struct ivhd_entry *)p;\n\t\tseg_id = pci_seg->id;\n\n\t\tswitch (e->type) {\n\t\tcase IVHD_DEV_ALL:\n\n\t\t\tDUMP_printk(\"  DEV_ALL\\t\\t\\tflags: %02x\\n\", e->flags);\n\n\t\t\tfor (dev_i = 0; dev_i <= pci_seg->last_bdf; ++dev_i)\n\t\t\t\tset_dev_entry_from_acpi(iommu, dev_i, e->flags, 0);\n\t\t\tbreak;\n\t\tcase IVHD_DEV_SELECT:\n\n\t\t\tDUMP_printk(\"  DEV_SELECT\\t\\t\\t devid: %04x:%02x:%02x.%x \"\n\t\t\t\t    \"flags: %02x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags);\n\n\t\t\tdevid = e->devid;\n\t\t\tset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\n\t\t\tbreak;\n\t\tcase IVHD_DEV_SELECT_RANGE_START:\n\n\t\t\tDUMP_printk(\"  DEV_SELECT_RANGE_START\\t \"\n\t\t\t\t    \"devid: %04x:%02x:%02x.%x flags: %02x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags);\n\n\t\t\tdevid_start = e->devid;\n\t\t\tflags = e->flags;\n\t\t\text_flags = 0;\n\t\t\talias = false;\n\t\t\tbreak;\n\t\tcase IVHD_DEV_ALIAS:\n\n\t\t\tDUMP_printk(\"  DEV_ALIAS\\t\\t\\t devid: %04x:%02x:%02x.%x \"\n\t\t\t\t    \"flags: %02x devid_to: %02x:%02x.%x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags,\n\t\t\t\t    PCI_BUS_NUM(e->ext >> 8),\n\t\t\t\t    PCI_SLOT(e->ext >> 8),\n\t\t\t\t    PCI_FUNC(e->ext >> 8));\n\n\t\t\tdevid = e->devid;\n\t\t\tdevid_to = e->ext >> 8;\n\t\t\tset_dev_entry_from_acpi(iommu, devid   , e->flags, 0);\n\t\t\tset_dev_entry_from_acpi(iommu, devid_to, e->flags, 0);\n\t\t\tpci_seg->alias_table[devid] = devid_to;\n\t\t\tbreak;\n\t\tcase IVHD_DEV_ALIAS_RANGE:\n\n\t\t\tDUMP_printk(\"  DEV_ALIAS_RANGE\\t\\t \"\n\t\t\t\t    \"devid: %04x:%02x:%02x.%x flags: %02x \"\n\t\t\t\t    \"devid_to: %04x:%02x:%02x.%x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags,\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->ext >> 8),\n\t\t\t\t    PCI_SLOT(e->ext >> 8),\n\t\t\t\t    PCI_FUNC(e->ext >> 8));\n\n\t\t\tdevid_start = e->devid;\n\t\t\tflags = e->flags;\n\t\t\tdevid_to = e->ext >> 8;\n\t\t\text_flags = 0;\n\t\t\talias = true;\n\t\t\tbreak;\n\t\tcase IVHD_DEV_EXT_SELECT:\n\n\t\t\tDUMP_printk(\"  DEV_EXT_SELECT\\t\\t devid: %04x:%02x:%02x.%x \"\n\t\t\t\t    \"flags: %02x ext: %08x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags, e->ext);\n\n\t\t\tdevid = e->devid;\n\t\t\tset_dev_entry_from_acpi(iommu, devid, e->flags,\n\t\t\t\t\t\te->ext);\n\t\t\tbreak;\n\t\tcase IVHD_DEV_EXT_SELECT_RANGE:\n\n\t\t\tDUMP_printk(\"  DEV_EXT_SELECT_RANGE\\t devid: \"\n\t\t\t\t    \"%04x:%02x:%02x.%x flags: %02x ext: %08x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid),\n\t\t\t\t    e->flags, e->ext);\n\n\t\t\tdevid_start = e->devid;\n\t\t\tflags = e->flags;\n\t\t\text_flags = e->ext;\n\t\t\talias = false;\n\t\t\tbreak;\n\t\tcase IVHD_DEV_RANGE_END:\n\n\t\t\tDUMP_printk(\"  DEV_RANGE_END\\t\\t devid: %04x:%02x:%02x.%x\\n\",\n\t\t\t\t    seg_id, PCI_BUS_NUM(e->devid),\n\t\t\t\t    PCI_SLOT(e->devid),\n\t\t\t\t    PCI_FUNC(e->devid));\n\n\t\t\tdevid = e->devid;\n\t\t\tfor (dev_i = devid_start; dev_i <= devid; ++dev_i) {\n\t\t\t\tif (alias) {\n\t\t\t\t\tpci_seg->alias_table[dev_i] = devid_to;\n\t\t\t\t\tset_dev_entry_from_acpi(iommu,\n\t\t\t\t\t\tdevid_to, flags, ext_flags);\n\t\t\t\t}\n\t\t\t\tset_dev_entry_from_acpi(iommu, dev_i,\n\t\t\t\t\t\t\tflags, ext_flags);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase IVHD_DEV_SPECIAL: {\n\t\t\tu8 handle, type;\n\t\t\tconst char *var;\n\t\t\tu32 devid;\n\t\t\tint ret;\n\n\t\t\thandle = e->ext & 0xff;\n\t\t\tdevid = PCI_SEG_DEVID_TO_SBDF(seg_id, (e->ext >> 8));\n\t\t\ttype   = (e->ext >> 24) & 0xff;\n\n\t\t\tif (type == IVHD_SPECIAL_IOAPIC)\n\t\t\t\tvar = \"IOAPIC\";\n\t\t\telse if (type == IVHD_SPECIAL_HPET)\n\t\t\t\tvar = \"HPET\";\n\t\t\telse\n\t\t\t\tvar = \"UNKNOWN\";\n\n\t\t\tDUMP_printk(\"  DEV_SPECIAL(%s[%d])\\t\\tdevid: %04x:%02x:%02x.%x\\n\",\n\t\t\t\t    var, (int)handle,\n\t\t\t\t    seg_id, PCI_BUS_NUM(devid),\n\t\t\t\t    PCI_SLOT(devid),\n\t\t\t\t    PCI_FUNC(devid));\n\n\t\t\tret = add_special_device(type, handle, &devid, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\t \n\t\t\tset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\n\n\t\t\tbreak;\n\t\t}\n\t\tcase IVHD_DEV_ACPI_HID: {\n\t\t\tu32 devid;\n\t\t\tu8 hid[ACPIHID_HID_LEN];\n\t\t\tu8 uid[ACPIHID_UID_LEN];\n\t\t\tint ret;\n\n\t\t\tif (h->type != 0x40) {\n\t\t\t\tpr_err(FW_BUG \"Invalid IVHD device type %#x\\n\",\n\t\t\t\t       e->type);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(sizeof(e->ext_hid) != ACPIHID_HID_LEN - 1);\n\t\t\tmemcpy(hid, &e->ext_hid, ACPIHID_HID_LEN - 1);\n\t\t\thid[ACPIHID_HID_LEN - 1] = '\\0';\n\n\t\t\tif (!(*hid)) {\n\t\t\t\tpr_err(FW_BUG \"Invalid HID.\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuid[0] = '\\0';\n\t\t\tswitch (e->uidf) {\n\t\t\tcase UID_NOT_PRESENT:\n\n\t\t\t\tif (e->uidl != 0)\n\t\t\t\t\tpr_warn(FW_BUG \"Invalid UID length.\\n\");\n\n\t\t\t\tbreak;\n\t\t\tcase UID_IS_INTEGER:\n\n\t\t\t\tsprintf(uid, \"%d\", e->uid);\n\n\t\t\t\tbreak;\n\t\t\tcase UID_IS_CHARACTER:\n\n\t\t\t\tmemcpy(uid, &e->uid, e->uidl);\n\t\t\t\tuid[e->uidl] = '\\0';\n\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdevid = PCI_SEG_DEVID_TO_SBDF(seg_id, e->devid);\n\t\t\tDUMP_printk(\"  DEV_ACPI_HID(%s[%s])\\t\\tdevid: %04x:%02x:%02x.%x\\n\",\n\t\t\t\t    hid, uid, seg_id,\n\t\t\t\t    PCI_BUS_NUM(devid),\n\t\t\t\t    PCI_SLOT(devid),\n\t\t\t\t    PCI_FUNC(devid));\n\n\t\t\tflags = e->flags;\n\n\t\t\tret = add_acpi_hid_device(hid, uid, &devid, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\t \n\t\t\tset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\n\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tp += ivhd_entry_length(p);\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct amd_iommu_pci_seg *__init alloc_pci_segment(u16 id,\n\t\t\t\t\t  struct acpi_table_header *ivrs_base)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tint last_bdf;\n\n\t \n\tlast_bdf = find_last_devid_acpi(ivrs_base, id);\n\tif (last_bdf < 0)\n\t\treturn NULL;\n\n\tpci_seg = kzalloc(sizeof(struct amd_iommu_pci_seg), GFP_KERNEL);\n\tif (pci_seg == NULL)\n\t\treturn NULL;\n\n\tpci_seg->last_bdf = last_bdf;\n\tDUMP_printk(\"PCI segment : 0x%0x, last bdf : 0x%04x\\n\", id, last_bdf);\n\tpci_seg->dev_table_size     = tbl_size(DEV_TABLE_ENTRY_SIZE, last_bdf);\n\tpci_seg->alias_table_size   = tbl_size(ALIAS_TABLE_ENTRY_SIZE, last_bdf);\n\tpci_seg->rlookup_table_size = tbl_size(RLOOKUP_TABLE_ENTRY_SIZE, last_bdf);\n\n\tpci_seg->id = id;\n\tinit_llist_head(&pci_seg->dev_data_list);\n\tINIT_LIST_HEAD(&pci_seg->unity_map);\n\tlist_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);\n\n\tif (alloc_dev_table(pci_seg))\n\t\treturn NULL;\n\tif (alloc_alias_table(pci_seg))\n\t\treturn NULL;\n\tif (alloc_rlookup_table(pci_seg))\n\t\treturn NULL;\n\n\treturn pci_seg;\n}\n\nstatic struct amd_iommu_pci_seg *__init get_pci_segment(u16 id,\n\t\t\t\t\tstruct acpi_table_header *ivrs_base)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\n\tfor_each_pci_segment(pci_seg) {\n\t\tif (pci_seg->id == id)\n\t\t\treturn pci_seg;\n\t}\n\n\treturn alloc_pci_segment(id, ivrs_base);\n}\n\nstatic void __init free_pci_segments(void)\n{\n\tstruct amd_iommu_pci_seg *pci_seg, *next;\n\n\tfor_each_pci_segment_safe(pci_seg, next) {\n\t\tlist_del(&pci_seg->list);\n\t\tfree_irq_lookup_table(pci_seg);\n\t\tfree_rlookup_table(pci_seg);\n\t\tfree_alias_table(pci_seg);\n\t\tfree_dev_table(pci_seg);\n\t\tkfree(pci_seg);\n\t}\n}\n\nstatic void __init free_iommu_one(struct amd_iommu *iommu)\n{\n\tfree_cwwb_sem(iommu);\n\tfree_command_buffer(iommu);\n\tfree_event_buffer(iommu);\n\tfree_ppr_log(iommu);\n\tfree_ga_log(iommu);\n\tiommu_unmap_mmio_space(iommu);\n}\n\nstatic void __init free_iommu_all(void)\n{\n\tstruct amd_iommu *iommu, *next;\n\n\tfor_each_iommu_safe(iommu, next) {\n\t\tlist_del(&iommu->list);\n\t\tfree_iommu_one(iommu);\n\t\tkfree(iommu);\n\t}\n}\n\n \nstatic void amd_iommu_erratum_746_workaround(struct amd_iommu *iommu)\n{\n\tu32 value;\n\n\tif ((boot_cpu_data.x86 != 0x15) ||\n\t    (boot_cpu_data.x86_model < 0x10) ||\n\t    (boot_cpu_data.x86_model > 0x1f))\n\t\treturn;\n\n\tpci_write_config_dword(iommu->dev, 0xf0, 0x90);\n\tpci_read_config_dword(iommu->dev, 0xf4, &value);\n\n\tif (value & BIT(2))\n\t\treturn;\n\n\t \n\tpci_write_config_dword(iommu->dev, 0xf0, 0x90 | (1 << 8));\n\n\tpci_write_config_dword(iommu->dev, 0xf4, value | 0x4);\n\tpci_info(iommu->dev, \"Applying erratum 746 workaround\\n\");\n\n\t \n\tpci_write_config_dword(iommu->dev, 0xf0, 0x90);\n}\n\n \nstatic void amd_iommu_ats_write_check_workaround(struct amd_iommu *iommu)\n{\n\tu32 value;\n\n\tif ((boot_cpu_data.x86 != 0x15) ||\n\t    (boot_cpu_data.x86_model < 0x30) ||\n\t    (boot_cpu_data.x86_model > 0x3f))\n\t\treturn;\n\n\t \n\tvalue = iommu_read_l2(iommu, 0x47);\n\n\tif (value & BIT(0))\n\t\treturn;\n\n\t \n\tiommu_write_l2(iommu, 0x47, value | BIT(0));\n\n\tpci_info(iommu->dev, \"Applying ATS write check workaround\\n\");\n}\n\n \nstatic int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h,\n\t\t\t\t struct acpi_table_header *ivrs_base)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\n\tpci_seg = get_pci_segment(h->pci_seg, ivrs_base);\n\tif (pci_seg == NULL)\n\t\treturn -ENOMEM;\n\tiommu->pci_seg = pci_seg;\n\n\traw_spin_lock_init(&iommu->lock);\n\tatomic64_set(&iommu->cmd_sem_val, 0);\n\n\t \n\tlist_add_tail(&iommu->list, &amd_iommu_list);\n\tiommu->index = amd_iommus_present++;\n\n\tif (unlikely(iommu->index >= MAX_IOMMUS)) {\n\t\tWARN(1, \"System has more IOMMUs than supported by this driver\\n\");\n\t\treturn -ENOSYS;\n\t}\n\n\t \n\tamd_iommus[iommu->index] = iommu;\n\n\t \n\tiommu->devid   = h->devid;\n\tiommu->cap_ptr = h->cap_ptr;\n\tiommu->mmio_phys = h->mmio_phys;\n\n\tswitch (h->type) {\n\tcase 0x10:\n\t\t \n\t\tif ((h->efr_attr != 0) &&\n\t\t    ((h->efr_attr & (0xF << 13)) != 0) &&\n\t\t    ((h->efr_attr & (0x3F << 17)) != 0))\n\t\t\tiommu->mmio_phys_end = MMIO_REG_END_OFFSET;\n\t\telse\n\t\t\tiommu->mmio_phys_end = MMIO_CNTR_CONF_OFFSET;\n\n\t\t \n\t\tif (!boot_cpu_has(X86_FEATURE_CX16) ||\n\t\t    ((h->efr_attr & (0x1 << IOMMU_FEAT_GASUP_SHIFT)) == 0))\n\t\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_LEGACY;\n\t\tbreak;\n\tcase 0x11:\n\tcase 0x40:\n\t\tif (h->efr_reg & (1 << 9))\n\t\t\tiommu->mmio_phys_end = MMIO_REG_END_OFFSET;\n\t\telse\n\t\t\tiommu->mmio_phys_end = MMIO_CNTR_CONF_OFFSET;\n\n\t\t \n\t\tif (!boot_cpu_has(X86_FEATURE_CX16) ||\n\t\t    ((h->efr_reg & (0x1 << IOMMU_EFR_GASUP_SHIFT)) == 0)) {\n\t\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_LEGACY;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (h->efr_reg & BIT(IOMMU_EFR_XTSUP_SHIFT))\n\t\t\tamd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;\n\n\t\tearly_iommu_features_init(iommu, h);\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tiommu->mmio_base = iommu_map_mmio_space(iommu->mmio_phys,\n\t\t\t\t\t\tiommu->mmio_phys_end);\n\tif (!iommu->mmio_base)\n\t\treturn -ENOMEM;\n\n\treturn init_iommu_from_acpi(iommu, h);\n}\n\nstatic int __init init_iommu_one_late(struct amd_iommu *iommu)\n{\n\tint ret;\n\n\tif (alloc_cwwb_sem(iommu))\n\t\treturn -ENOMEM;\n\n\tif (alloc_command_buffer(iommu))\n\t\treturn -ENOMEM;\n\n\tif (alloc_event_buffer(iommu))\n\t\treturn -ENOMEM;\n\n\tiommu->int_enabled = false;\n\n\tinit_translation_status(iommu);\n\tif (translation_pre_enabled(iommu) && !is_kdump_kernel()) {\n\t\tiommu_disable(iommu);\n\t\tclear_translation_pre_enabled(iommu);\n\t\tpr_warn(\"Translation was enabled for IOMMU:%d but we are not in kdump mode\\n\",\n\t\t\tiommu->index);\n\t}\n\tif (amd_iommu_pre_enabled)\n\t\tamd_iommu_pre_enabled = translation_pre_enabled(iommu);\n\n\tif (amd_iommu_irq_remap) {\n\t\tret = amd_iommu_create_irq_domain(iommu);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tiommu->pci_seg->rlookup_table[iommu->devid] = NULL;\n\n\treturn 0;\n}\n\n \nstatic u8 get_highest_supported_ivhd_type(struct acpi_table_header *ivrs)\n{\n\tu8 *base = (u8 *)ivrs;\n\tstruct ivhd_header *ivhd = (struct ivhd_header *)\n\t\t\t\t\t(base + IVRS_HEADER_LENGTH);\n\tu8 last_type = ivhd->type;\n\tu16 devid = ivhd->devid;\n\n\twhile (((u8 *)ivhd - base < ivrs->length) &&\n\t       (ivhd->type <= ACPI_IVHD_TYPE_MAX_SUPPORTED)) {\n\t\tu8 *p = (u8 *) ivhd;\n\n\t\tif (ivhd->devid == devid)\n\t\t\tlast_type = ivhd->type;\n\t\tivhd = (struct ivhd_header *)(p + ivhd->length);\n\t}\n\n\treturn last_type;\n}\n\n \nstatic int __init init_iommu_all(struct acpi_table_header *table)\n{\n\tu8 *p = (u8 *)table, *end = (u8 *)table;\n\tstruct ivhd_header *h;\n\tstruct amd_iommu *iommu;\n\tint ret;\n\n\tend += table->length;\n\tp += IVRS_HEADER_LENGTH;\n\n\t \n\twhile (p < end) {\n\t\th = (struct ivhd_header *)p;\n\t\tif (*p == amd_iommu_target_ivhd_type) {\n\n\t\t\tDUMP_printk(\"device: %04x:%02x:%02x.%01x cap: %04x \"\n\t\t\t\t    \"flags: %01x info %04x\\n\",\n\t\t\t\t    h->pci_seg, PCI_BUS_NUM(h->devid),\n\t\t\t\t    PCI_SLOT(h->devid), PCI_FUNC(h->devid),\n\t\t\t\t    h->cap_ptr, h->flags, h->info);\n\t\t\tDUMP_printk(\"       mmio-addr: %016llx\\n\",\n\t\t\t\t    h->mmio_phys);\n\n\t\t\tiommu = kzalloc(sizeof(struct amd_iommu), GFP_KERNEL);\n\t\t\tif (iommu == NULL)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tret = init_iommu_one(iommu, h, table);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tp += h->length;\n\n\t}\n\tWARN_ON(p != end);\n\n\t \n\tget_global_efr();\n\n\t \n\tfor_each_iommu(iommu) {\n\t\tret = init_iommu_one_late(iommu);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void init_iommu_perf_ctr(struct amd_iommu *iommu)\n{\n\tu64 val;\n\tstruct pci_dev *pdev = iommu->dev;\n\n\tif (!iommu_feature(iommu, FEATURE_PC))\n\t\treturn;\n\n\tamd_iommu_pc_present = true;\n\n\tpci_info(pdev, \"IOMMU performance counters supported\\n\");\n\n\tval = readl(iommu->mmio_base + MMIO_CNTR_CONF_OFFSET);\n\tiommu->max_banks = (u8) ((val >> 12) & 0x3f);\n\tiommu->max_counters = (u8) ((val >> 7) & 0xf);\n\n\treturn;\n}\n\nstatic ssize_t amd_iommu_show_cap(struct device *dev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\tstruct amd_iommu *iommu = dev_to_amd_iommu(dev);\n\treturn sysfs_emit(buf, \"%x\\n\", iommu->cap);\n}\nstatic DEVICE_ATTR(cap, S_IRUGO, amd_iommu_show_cap, NULL);\n\nstatic ssize_t amd_iommu_show_features(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       char *buf)\n{\n\tstruct amd_iommu *iommu = dev_to_amd_iommu(dev);\n\treturn sysfs_emit(buf, \"%llx:%llx\\n\", iommu->features2, iommu->features);\n}\nstatic DEVICE_ATTR(features, S_IRUGO, amd_iommu_show_features, NULL);\n\nstatic struct attribute *amd_iommu_attrs[] = {\n\t&dev_attr_cap.attr,\n\t&dev_attr_features.attr,\n\tNULL,\n};\n\nstatic struct attribute_group amd_iommu_group = {\n\t.name = \"amd-iommu\",\n\t.attrs = amd_iommu_attrs,\n};\n\nstatic const struct attribute_group *amd_iommu_groups[] = {\n\t&amd_iommu_group,\n\tNULL,\n};\n\n \nstatic void __init late_iommu_features_init(struct amd_iommu *iommu)\n{\n\tu64 features, features2;\n\n\tif (!(iommu->cap & (1 << IOMMU_CAP_EFR)))\n\t\treturn;\n\n\t \n\tfeatures = readq(iommu->mmio_base + MMIO_EXT_FEATURES);\n\tfeatures2 = readq(iommu->mmio_base + MMIO_EXT_FEATURES2);\n\n\tif (!iommu->features) {\n\t\tiommu->features = features;\n\t\tiommu->features2 = features2;\n\t\treturn;\n\t}\n\n\t \n\tif (features != iommu->features ||\n\t    features2 != iommu->features2) {\n\t\tpr_warn(FW_WARN\n\t\t\t\"EFR mismatch. Use IVHD EFR (%#llx : %#llx), EFR2 (%#llx : %#llx).\\n\",\n\t\t\tfeatures, iommu->features,\n\t\t\tfeatures2, iommu->features2);\n\t}\n}\n\nstatic int __init iommu_init_pci(struct amd_iommu *iommu)\n{\n\tint cap_ptr = iommu->cap_ptr;\n\tint ret;\n\n\tiommu->dev = pci_get_domain_bus_and_slot(iommu->pci_seg->id,\n\t\t\t\t\t\t PCI_BUS_NUM(iommu->devid),\n\t\t\t\t\t\t iommu->devid & 0xff);\n\tif (!iommu->dev)\n\t\treturn -ENODEV;\n\n\t \n\tiommu->dev->match_driver = false;\n\n\tpci_read_config_dword(iommu->dev, cap_ptr + MMIO_CAP_HDR_OFFSET,\n\t\t\t      &iommu->cap);\n\n\tif (!(iommu->cap & (1 << IOMMU_CAP_IOTLB)))\n\t\tamd_iommu_iotlb_sup = false;\n\n\tlate_iommu_features_init(iommu);\n\n\tif (iommu_feature(iommu, FEATURE_GT)) {\n\t\tint glxval;\n\t\tu32 max_pasid;\n\t\tu64 pasmax;\n\n\t\tpasmax = iommu->features & FEATURE_PASID_MASK;\n\t\tpasmax >>= FEATURE_PASID_SHIFT;\n\t\tmax_pasid  = (1 << (pasmax + 1)) - 1;\n\n\t\tamd_iommu_max_pasid = min(amd_iommu_max_pasid, max_pasid);\n\n\t\tBUG_ON(amd_iommu_max_pasid & ~PASID_MASK);\n\n\t\tglxval   = iommu->features & FEATURE_GLXVAL_MASK;\n\t\tglxval >>= FEATURE_GLXVAL_SHIFT;\n\n\t\tif (amd_iommu_max_glx_val == -1)\n\t\t\tamd_iommu_max_glx_val = glxval;\n\t\telse\n\t\t\tamd_iommu_max_glx_val = min(amd_iommu_max_glx_val, glxval);\n\t}\n\n\tif (iommu_feature(iommu, FEATURE_GT) &&\n\t    iommu_feature(iommu, FEATURE_PPR)) {\n\t\tiommu->is_iommu_v2   = true;\n\t\tamd_iommu_v2_present = true;\n\t}\n\n\tif (iommu_feature(iommu, FEATURE_PPR) && alloc_ppr_log(iommu))\n\t\treturn -ENOMEM;\n\n\tif (iommu->cap & (1UL << IOMMU_CAP_NPCACHE)) {\n\t\tpr_info(\"Using strict mode due to virtualization\\n\");\n\t\tiommu_set_dma_strict();\n\t\tamd_iommu_np_cache = true;\n\t}\n\n\tinit_iommu_perf_ctr(iommu);\n\n\tif (amd_iommu_pgtable == AMD_IOMMU_V2) {\n\t\tif (!iommu_feature(iommu, FEATURE_GIOSUP) ||\n\t\t    !iommu_feature(iommu, FEATURE_GT)) {\n\t\t\tpr_warn(\"Cannot enable v2 page table for DMA-API. Fallback to v1.\\n\");\n\t\t\tamd_iommu_pgtable = AMD_IOMMU_V1;\n\t\t} else if (iommu_default_passthrough()) {\n\t\t\tpr_warn(\"V2 page table doesn't support passthrough mode. Fallback to v1.\\n\");\n\t\t\tamd_iommu_pgtable = AMD_IOMMU_V1;\n\t\t}\n\t}\n\n\tif (is_rd890_iommu(iommu->dev)) {\n\t\tint i, j;\n\n\t\tiommu->root_pdev =\n\t\t\tpci_get_domain_bus_and_slot(iommu->pci_seg->id,\n\t\t\t\t\t\t    iommu->dev->bus->number,\n\t\t\t\t\t\t    PCI_DEVFN(0, 0));\n\n\t\t \n\t\tpci_read_config_dword(iommu->dev, iommu->cap_ptr + 4,\n\t\t\t\t&iommu->stored_addr_lo);\n\t\tpci_read_config_dword(iommu->dev, iommu->cap_ptr + 8,\n\t\t\t\t&iommu->stored_addr_hi);\n\n\t\t \n\t\tiommu->stored_addr_lo &= ~1;\n\n\t\tfor (i = 0; i < 6; i++)\n\t\t\tfor (j = 0; j < 0x12; j++)\n\t\t\t\tiommu->stored_l1[i][j] = iommu_read_l1(iommu, i, j);\n\n\t\tfor (i = 0; i < 0x83; i++)\n\t\t\tiommu->stored_l2[i] = iommu_read_l2(iommu, i);\n\t}\n\n\tamd_iommu_erratum_746_workaround(iommu);\n\tamd_iommu_ats_write_check_workaround(iommu);\n\n\tret = iommu_device_sysfs_add(&iommu->iommu, &iommu->dev->dev,\n\t\t\t       amd_iommu_groups, \"ivhd%d\", iommu->index);\n\tif (ret)\n\t\treturn ret;\n\n\tiommu_device_register(&iommu->iommu, &amd_iommu_ops, NULL);\n\n\treturn pci_enable_device(iommu->dev);\n}\n\nstatic void print_iommu_info(void)\n{\n\tstatic const char * const feat_str[] = {\n\t\t\"PreF\", \"PPR\", \"X2APIC\", \"NX\", \"GT\", \"[5]\",\n\t\t\"IA\", \"GA\", \"HE\", \"PC\"\n\t};\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu) {\n\t\tstruct pci_dev *pdev = iommu->dev;\n\t\tint i;\n\n\t\tpci_info(pdev, \"Found IOMMU cap 0x%x\\n\", iommu->cap_ptr);\n\n\t\tif (iommu->cap & (1 << IOMMU_CAP_EFR)) {\n\t\t\tpr_info(\"Extended features (%#llx, %#llx):\", iommu->features, iommu->features2);\n\n\t\t\tfor (i = 0; i < ARRAY_SIZE(feat_str); ++i) {\n\t\t\t\tif (iommu_feature(iommu, (1ULL << i)))\n\t\t\t\t\tpr_cont(\" %s\", feat_str[i]);\n\t\t\t}\n\n\t\t\tif (iommu->features & FEATURE_GAM_VAPIC)\n\t\t\t\tpr_cont(\" GA_vAPIC\");\n\n\t\t\tif (iommu->features & FEATURE_SNP)\n\t\t\t\tpr_cont(\" SNP\");\n\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t}\n\tif (irq_remapping_enabled) {\n\t\tpr_info(\"Interrupt remapping enabled\\n\");\n\t\tif (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)\n\t\t\tpr_info(\"X2APIC enabled\\n\");\n\t}\n\tif (amd_iommu_pgtable == AMD_IOMMU_V2) {\n\t\tpr_info(\"V2 page table enabled (Paging mode : %d level)\\n\",\n\t\t\tamd_iommu_gpt_level);\n\t}\n}\n\nstatic int __init amd_iommu_init_pci(void)\n{\n\tstruct amd_iommu *iommu;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tint ret;\n\n\tfor_each_iommu(iommu) {\n\t\tret = iommu_init_pci(iommu);\n\t\tif (ret) {\n\t\t\tpr_err(\"IOMMU%d: Failed to initialize IOMMU Hardware (error=%d)!\\n\",\n\t\t\t       iommu->index, ret);\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tiommu_set_cwwb_range(iommu);\n\t}\n\n\t \n\tfor_each_pci_segment(pci_seg)\n\t\tinit_device_table_dma(pci_seg);\n\n\tfor_each_iommu(iommu)\n\t\tiommu_flush_all_caches(iommu);\n\n\tprint_iommu_info();\n\nout:\n\treturn ret;\n}\n\n \n\nstatic int iommu_setup_msi(struct amd_iommu *iommu)\n{\n\tint r;\n\n\tr = pci_enable_msi(iommu->dev);\n\tif (r)\n\t\treturn r;\n\n\tr = request_threaded_irq(iommu->dev->irq,\n\t\t\t\t amd_iommu_int_handler,\n\t\t\t\t amd_iommu_int_thread,\n\t\t\t\t 0, \"AMD-Vi\",\n\t\t\t\t iommu);\n\n\tif (r) {\n\t\tpci_disable_msi(iommu->dev);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nunion intcapxt {\n\tu64\tcapxt;\n\tstruct {\n\t\tu64\treserved_0\t\t:  2,\n\t\t\tdest_mode_logical\t:  1,\n\t\t\treserved_1\t\t:  5,\n\t\t\tdestid_0_23\t\t: 24,\n\t\t\tvector\t\t\t:  8,\n\t\t\treserved_2\t\t: 16,\n\t\t\tdestid_24_31\t\t:  8;\n\t};\n} __attribute__ ((packed));\n\n\nstatic struct irq_chip intcapxt_controller;\n\nstatic int intcapxt_irqdomain_activate(struct irq_domain *domain,\n\t\t\t\t       struct irq_data *irqd, bool reserve)\n{\n\treturn 0;\n}\n\nstatic void intcapxt_irqdomain_deactivate(struct irq_domain *domain,\n\t\t\t\t\t  struct irq_data *irqd)\n{\n}\n\n\nstatic int intcapxt_irqdomain_alloc(struct irq_domain *domain, unsigned int virq,\n\t\t\t\t    unsigned int nr_irqs, void *arg)\n{\n\tstruct irq_alloc_info *info = arg;\n\tint i, ret;\n\n\tif (!info || info->type != X86_IRQ_ALLOC_TYPE_AMDVI)\n\t\treturn -EINVAL;\n\n\tret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tfor (i = virq; i < virq + nr_irqs; i++) {\n\t\tstruct irq_data *irqd = irq_domain_get_irq_data(domain, i);\n\n\t\tirqd->chip = &intcapxt_controller;\n\t\tirqd->hwirq = info->hwirq;\n\t\tirqd->chip_data = info->data;\n\t\t__irq_set_handler(i, handle_edge_irq, 0, \"edge\");\n\t}\n\n\treturn ret;\n}\n\nstatic void intcapxt_irqdomain_free(struct irq_domain *domain, unsigned int virq,\n\t\t\t\t    unsigned int nr_irqs)\n{\n\tirq_domain_free_irqs_top(domain, virq, nr_irqs);\n}\n\n\nstatic void intcapxt_unmask_irq(struct irq_data *irqd)\n{\n\tstruct amd_iommu *iommu = irqd->chip_data;\n\tstruct irq_cfg *cfg = irqd_cfg(irqd);\n\tunion intcapxt xt;\n\n\txt.capxt = 0ULL;\n\txt.dest_mode_logical = apic->dest_mode_logical;\n\txt.vector = cfg->vector;\n\txt.destid_0_23 = cfg->dest_apicid & GENMASK(23, 0);\n\txt.destid_24_31 = cfg->dest_apicid >> 24;\n\n\twriteq(xt.capxt, iommu->mmio_base + irqd->hwirq);\n}\n\nstatic void intcapxt_mask_irq(struct irq_data *irqd)\n{\n\tstruct amd_iommu *iommu = irqd->chip_data;\n\n\twriteq(0, iommu->mmio_base + irqd->hwirq);\n}\n\n\nstatic int intcapxt_set_affinity(struct irq_data *irqd,\n\t\t\t\t const struct cpumask *mask, bool force)\n{\n\tstruct irq_data *parent = irqd->parent_data;\n\tint ret;\n\n\tret = parent->chip->irq_set_affinity(parent, mask, force);\n\tif (ret < 0 || ret == IRQ_SET_MASK_OK_DONE)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic int intcapxt_set_wake(struct irq_data *irqd, unsigned int on)\n{\n\treturn on ? -EOPNOTSUPP : 0;\n}\n\nstatic struct irq_chip intcapxt_controller = {\n\t.name\t\t\t= \"IOMMU-MSI\",\n\t.irq_unmask\t\t= intcapxt_unmask_irq,\n\t.irq_mask\t\t= intcapxt_mask_irq,\n\t.irq_ack\t\t= irq_chip_ack_parent,\n\t.irq_retrigger\t\t= irq_chip_retrigger_hierarchy,\n\t.irq_set_affinity       = intcapxt_set_affinity,\n\t.irq_set_wake\t\t= intcapxt_set_wake,\n\t.flags\t\t\t= IRQCHIP_MASK_ON_SUSPEND,\n};\n\nstatic const struct irq_domain_ops intcapxt_domain_ops = {\n\t.alloc\t\t\t= intcapxt_irqdomain_alloc,\n\t.free\t\t\t= intcapxt_irqdomain_free,\n\t.activate\t\t= intcapxt_irqdomain_activate,\n\t.deactivate\t\t= intcapxt_irqdomain_deactivate,\n};\n\n\nstatic struct irq_domain *iommu_irqdomain;\n\nstatic struct irq_domain *iommu_get_irqdomain(void)\n{\n\tstruct fwnode_handle *fn;\n\n\t \n\tif (iommu_irqdomain)\n\t\treturn iommu_irqdomain;\n\n\tfn = irq_domain_alloc_named_fwnode(\"AMD-Vi-MSI\");\n\tif (!fn)\n\t\treturn NULL;\n\n\tiommu_irqdomain = irq_domain_create_hierarchy(x86_vector_domain, 0, 0,\n\t\t\t\t\t\t      fn, &intcapxt_domain_ops,\n\t\t\t\t\t\t      NULL);\n\tif (!iommu_irqdomain)\n\t\tirq_domain_free_fwnode(fn);\n\n\treturn iommu_irqdomain;\n}\n\nstatic int __iommu_setup_intcapxt(struct amd_iommu *iommu, const char *devname,\n\t\t\t\t  int hwirq, irq_handler_t thread_fn)\n{\n\tstruct irq_domain *domain;\n\tstruct irq_alloc_info info;\n\tint irq, ret;\n\tint node = dev_to_node(&iommu->dev->dev);\n\n\tdomain = iommu_get_irqdomain();\n\tif (!domain)\n\t\treturn -ENXIO;\n\n\tinit_irq_alloc_info(&info, NULL);\n\tinfo.type = X86_IRQ_ALLOC_TYPE_AMDVI;\n\tinfo.data = iommu;\n\tinfo.hwirq = hwirq;\n\n\tirq = irq_domain_alloc_irqs(domain, 1, node, &info);\n\tif (irq < 0) {\n\t\tirq_domain_remove(domain);\n\t\treturn irq;\n\t}\n\n\tret = request_threaded_irq(irq, amd_iommu_int_handler,\n\t\t\t\t   thread_fn, 0, devname, iommu);\n\tif (ret) {\n\t\tirq_domain_free_irqs(irq, 1);\n\t\tirq_domain_remove(domain);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int iommu_setup_intcapxt(struct amd_iommu *iommu)\n{\n\tint ret;\n\n\tsnprintf(iommu->evt_irq_name, sizeof(iommu->evt_irq_name),\n\t\t \"AMD-Vi%d-Evt\", iommu->index);\n\tret = __iommu_setup_intcapxt(iommu, iommu->evt_irq_name,\n\t\t\t\t     MMIO_INTCAPXT_EVT_OFFSET,\n\t\t\t\t     amd_iommu_int_thread_evtlog);\n\tif (ret)\n\t\treturn ret;\n\n\tsnprintf(iommu->ppr_irq_name, sizeof(iommu->ppr_irq_name),\n\t\t \"AMD-Vi%d-PPR\", iommu->index);\n\tret = __iommu_setup_intcapxt(iommu, iommu->ppr_irq_name,\n\t\t\t\t     MMIO_INTCAPXT_PPR_OFFSET,\n\t\t\t\t     amd_iommu_int_thread_pprlog);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_IRQ_REMAP\n\tsnprintf(iommu->ga_irq_name, sizeof(iommu->ga_irq_name),\n\t\t \"AMD-Vi%d-GA\", iommu->index);\n\tret = __iommu_setup_intcapxt(iommu, iommu->ga_irq_name,\n\t\t\t\t     MMIO_INTCAPXT_GALOG_OFFSET,\n\t\t\t\t     amd_iommu_int_thread_galog);\n#endif\n\n\treturn ret;\n}\n\nstatic int iommu_init_irq(struct amd_iommu *iommu)\n{\n\tint ret;\n\n\tif (iommu->int_enabled)\n\t\tgoto enable_faults;\n\n\tif (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)\n\t\tret = iommu_setup_intcapxt(iommu);\n\telse if (iommu->dev->msi_cap)\n\t\tret = iommu_setup_msi(iommu);\n\telse\n\t\tret = -ENODEV;\n\n\tif (ret)\n\t\treturn ret;\n\n\tiommu->int_enabled = true;\nenable_faults:\n\n\tif (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)\n\t\tiommu_feature_enable(iommu, CONTROL_INTCAPXT_EN);\n\n\tiommu_feature_enable(iommu, CONTROL_EVT_INT_EN);\n\n\treturn 0;\n}\n\n \n\nstatic void __init free_unity_maps(void)\n{\n\tstruct unity_map_entry *entry, *next;\n\tstruct amd_iommu_pci_seg *p, *pci_seg;\n\n\tfor_each_pci_segment_safe(pci_seg, p) {\n\t\tlist_for_each_entry_safe(entry, next, &pci_seg->unity_map, list) {\n\t\t\tlist_del(&entry->list);\n\t\t\tkfree(entry);\n\t\t}\n\t}\n}\n\n \nstatic int __init init_unity_map_range(struct ivmd_header *m,\n\t\t\t\t       struct acpi_table_header *ivrs_base)\n{\n\tstruct unity_map_entry *e = NULL;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tchar *s;\n\n\tpci_seg = get_pci_segment(m->pci_seg, ivrs_base);\n\tif (pci_seg == NULL)\n\t\treturn -ENOMEM;\n\n\te = kzalloc(sizeof(*e), GFP_KERNEL);\n\tif (e == NULL)\n\t\treturn -ENOMEM;\n\n\tswitch (m->type) {\n\tdefault:\n\t\tkfree(e);\n\t\treturn 0;\n\tcase ACPI_IVMD_TYPE:\n\t\ts = \"IVMD_TYPEi\\t\\t\\t\";\n\t\te->devid_start = e->devid_end = m->devid;\n\t\tbreak;\n\tcase ACPI_IVMD_TYPE_ALL:\n\t\ts = \"IVMD_TYPE_ALL\\t\\t\";\n\t\te->devid_start = 0;\n\t\te->devid_end = pci_seg->last_bdf;\n\t\tbreak;\n\tcase ACPI_IVMD_TYPE_RANGE:\n\t\ts = \"IVMD_TYPE_RANGE\\t\\t\";\n\t\te->devid_start = m->devid;\n\t\te->devid_end = m->aux;\n\t\tbreak;\n\t}\n\te->address_start = PAGE_ALIGN(m->range_start);\n\te->address_end = e->address_start + PAGE_ALIGN(m->range_length);\n\te->prot = m->flags >> 1;\n\n\t \n\tif (m->flags & IVMD_FLAG_EXCL_RANGE)\n\t\te->prot = (IVMD_FLAG_IW | IVMD_FLAG_IR) >> 1;\n\n\tDUMP_printk(\"%s devid_start: %04x:%02x:%02x.%x devid_end: \"\n\t\t    \"%04x:%02x:%02x.%x range_start: %016llx range_end: %016llx\"\n\t\t    \" flags: %x\\n\", s, m->pci_seg,\n\t\t    PCI_BUS_NUM(e->devid_start), PCI_SLOT(e->devid_start),\n\t\t    PCI_FUNC(e->devid_start), m->pci_seg,\n\t\t    PCI_BUS_NUM(e->devid_end),\n\t\t    PCI_SLOT(e->devid_end), PCI_FUNC(e->devid_end),\n\t\t    e->address_start, e->address_end, m->flags);\n\n\tlist_add_tail(&e->list, &pci_seg->unity_map);\n\n\treturn 0;\n}\n\n \nstatic int __init init_memory_definitions(struct acpi_table_header *table)\n{\n\tu8 *p = (u8 *)table, *end = (u8 *)table;\n\tstruct ivmd_header *m;\n\n\tend += table->length;\n\tp += IVRS_HEADER_LENGTH;\n\n\twhile (p < end) {\n\t\tm = (struct ivmd_header *)p;\n\t\tif (m->flags & (IVMD_FLAG_UNITY_MAP | IVMD_FLAG_EXCL_RANGE))\n\t\t\tinit_unity_map_range(m, table);\n\n\t\tp += m->length;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void init_device_table_dma(struct amd_iommu_pci_seg *pci_seg)\n{\n\tu32 devid;\n\tstruct dev_table_entry *dev_table = pci_seg->dev_table;\n\n\tif (dev_table == NULL)\n\t\treturn;\n\n\tfor (devid = 0; devid <= pci_seg->last_bdf; ++devid) {\n\t\t__set_dev_entry_bit(dev_table, devid, DEV_ENTRY_VALID);\n\t\tif (!amd_iommu_snp_en)\n\t\t\t__set_dev_entry_bit(dev_table, devid, DEV_ENTRY_TRANSLATION);\n\t}\n}\n\nstatic void __init uninit_device_table_dma(struct amd_iommu_pci_seg *pci_seg)\n{\n\tu32 devid;\n\tstruct dev_table_entry *dev_table = pci_seg->dev_table;\n\n\tif (dev_table == NULL)\n\t\treturn;\n\n\tfor (devid = 0; devid <= pci_seg->last_bdf; ++devid) {\n\t\tdev_table[devid].data[0] = 0ULL;\n\t\tdev_table[devid].data[1] = 0ULL;\n\t}\n}\n\nstatic void init_device_table(void)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tu32 devid;\n\n\tif (!amd_iommu_irq_remap)\n\t\treturn;\n\n\tfor_each_pci_segment(pci_seg) {\n\t\tfor (devid = 0; devid <= pci_seg->last_bdf; ++devid)\n\t\t\t__set_dev_entry_bit(pci_seg->dev_table,\n\t\t\t\t\t    devid, DEV_ENTRY_IRQ_TBL_EN);\n\t}\n}\n\nstatic void iommu_init_flags(struct amd_iommu *iommu)\n{\n\tiommu->acpi_flags & IVHD_FLAG_HT_TUN_EN_MASK ?\n\t\tiommu_feature_enable(iommu, CONTROL_HT_TUN_EN) :\n\t\tiommu_feature_disable(iommu, CONTROL_HT_TUN_EN);\n\n\tiommu->acpi_flags & IVHD_FLAG_PASSPW_EN_MASK ?\n\t\tiommu_feature_enable(iommu, CONTROL_PASSPW_EN) :\n\t\tiommu_feature_disable(iommu, CONTROL_PASSPW_EN);\n\n\tiommu->acpi_flags & IVHD_FLAG_RESPASSPW_EN_MASK ?\n\t\tiommu_feature_enable(iommu, CONTROL_RESPASSPW_EN) :\n\t\tiommu_feature_disable(iommu, CONTROL_RESPASSPW_EN);\n\n\tiommu->acpi_flags & IVHD_FLAG_ISOC_EN_MASK ?\n\t\tiommu_feature_enable(iommu, CONTROL_ISOC_EN) :\n\t\tiommu_feature_disable(iommu, CONTROL_ISOC_EN);\n\n\t \n\tiommu_feature_enable(iommu, CONTROL_COHERENT_EN);\n\n\t \n\tiommu_set_inv_tlb_timeout(iommu, CTRL_INV_TO_1S);\n}\n\nstatic void iommu_apply_resume_quirks(struct amd_iommu *iommu)\n{\n\tint i, j;\n\tu32 ioc_feature_control;\n\tstruct pci_dev *pdev = iommu->root_pdev;\n\n\t \n\tif (!is_rd890_iommu(iommu->dev) || !pdev)\n\t\treturn;\n\n\t \n\n\t \n\tpci_write_config_dword(pdev, 0x60, 0x75 | (1 << 7));\n\tpci_read_config_dword(pdev, 0x64, &ioc_feature_control);\n\n\t \n\tif (!(ioc_feature_control & 0x1))\n\t\tpci_write_config_dword(pdev, 0x64, ioc_feature_control | 1);\n\n\t \n\tpci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\n\t\t\t       iommu->stored_addr_lo);\n\tpci_write_config_dword(iommu->dev, iommu->cap_ptr + 8,\n\t\t\t       iommu->stored_addr_hi);\n\n\t \n\tfor (i = 0; i < 6; i++)\n\t\tfor (j = 0; j < 0x12; j++)\n\t\t\tiommu_write_l1(iommu, i, j, iommu->stored_l1[i][j]);\n\n\t \n\tfor (i = 0; i < 0x83; i++)\n\t\tiommu_write_l2(iommu, i, iommu->stored_l2[i]);\n\n\t \n\tpci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\n\t\t\t       iommu->stored_addr_lo | 1);\n}\n\nstatic void iommu_enable_ga(struct amd_iommu *iommu)\n{\n#ifdef CONFIG_IRQ_REMAP\n\tswitch (amd_iommu_guest_ir) {\n\tcase AMD_IOMMU_GUEST_IR_VAPIC:\n\tcase AMD_IOMMU_GUEST_IR_LEGACY_GA:\n\t\tiommu_feature_enable(iommu, CONTROL_GA_EN);\n\t\tiommu->irte_ops = &irte_128_ops;\n\t\tbreak;\n\tdefault:\n\t\tiommu->irte_ops = &irte_32_ops;\n\t\tbreak;\n\t}\n#endif\n}\n\nstatic void iommu_disable_irtcachedis(struct amd_iommu *iommu)\n{\n\tiommu_feature_disable(iommu, CONTROL_IRTCACHEDIS);\n}\n\nstatic void iommu_enable_irtcachedis(struct amd_iommu *iommu)\n{\n\tu64 ctrl;\n\n\tif (!amd_iommu_irtcachedis)\n\t\treturn;\n\n\t \n\tiommu_feature_enable(iommu, CONTROL_IRTCACHEDIS);\n\tctrl = readq(iommu->mmio_base +  MMIO_CONTROL_OFFSET);\n\tctrl &= (1ULL << CONTROL_IRTCACHEDIS);\n\tif (ctrl)\n\t\tiommu->irtcachedis_enabled = true;\n\tpr_info(\"iommu%d (%#06x) : IRT cache is %s\\n\",\n\t\tiommu->index, iommu->devid,\n\t\tiommu->irtcachedis_enabled ? \"disabled\" : \"enabled\");\n}\n\nstatic void early_enable_iommu(struct amd_iommu *iommu)\n{\n\tiommu_disable(iommu);\n\tiommu_init_flags(iommu);\n\tiommu_set_device_table(iommu);\n\tiommu_enable_command_buffer(iommu);\n\tiommu_enable_event_buffer(iommu);\n\tiommu_set_exclusion_range(iommu);\n\tiommu_enable_ga(iommu);\n\tiommu_enable_xt(iommu);\n\tiommu_enable_irtcachedis(iommu);\n\tiommu_enable(iommu);\n\tiommu_flush_all_caches(iommu);\n}\n\n \nstatic void early_enable_iommus(void)\n{\n\tstruct amd_iommu *iommu;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\n\tif (!copy_device_table()) {\n\t\t \n\t\tif (amd_iommu_pre_enabled)\n\t\t\tpr_err(\"Failed to copy DEV table from previous kernel.\\n\");\n\n\t\tfor_each_pci_segment(pci_seg) {\n\t\t\tif (pci_seg->old_dev_tbl_cpy != NULL) {\n\t\t\t\tfree_pages((unsigned long)pci_seg->old_dev_tbl_cpy,\n\t\t\t\t\t\tget_order(pci_seg->dev_table_size));\n\t\t\t\tpci_seg->old_dev_tbl_cpy = NULL;\n\t\t\t}\n\t\t}\n\n\t\tfor_each_iommu(iommu) {\n\t\t\tclear_translation_pre_enabled(iommu);\n\t\t\tearly_enable_iommu(iommu);\n\t\t}\n\t} else {\n\t\tpr_info(\"Copied DEV table from previous kernel.\\n\");\n\n\t\tfor_each_pci_segment(pci_seg) {\n\t\t\tfree_pages((unsigned long)pci_seg->dev_table,\n\t\t\t\t   get_order(pci_seg->dev_table_size));\n\t\t\tpci_seg->dev_table = pci_seg->old_dev_tbl_cpy;\n\t\t}\n\n\t\tfor_each_iommu(iommu) {\n\t\t\tiommu_disable_command_buffer(iommu);\n\t\t\tiommu_disable_event_buffer(iommu);\n\t\t\tiommu_disable_irtcachedis(iommu);\n\t\t\tiommu_enable_command_buffer(iommu);\n\t\t\tiommu_enable_event_buffer(iommu);\n\t\t\tiommu_enable_ga(iommu);\n\t\t\tiommu_enable_xt(iommu);\n\t\t\tiommu_enable_irtcachedis(iommu);\n\t\t\tiommu_set_device_table(iommu);\n\t\t\tiommu_flush_all_caches(iommu);\n\t\t}\n\t}\n}\n\nstatic void enable_iommus_v2(void)\n{\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu) {\n\t\tiommu_enable_ppr_log(iommu);\n\t\tiommu_enable_gt(iommu);\n\t}\n}\n\nstatic void enable_iommus_vapic(void)\n{\n#ifdef CONFIG_IRQ_REMAP\n\tu32 status, i;\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu) {\n\t\t \n\t\tstatus = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\t\tif (!(status & MMIO_STATUS_GALOG_RUN_MASK))\n\t\t\tcontinue;\n\n\t\tiommu_feature_disable(iommu, CONTROL_GALOG_EN);\n\t\tiommu_feature_disable(iommu, CONTROL_GAINT_EN);\n\n\t\t \n\t\tfor (i = 0; i < LOOP_TIMEOUT; ++i) {\n\t\t\tstatus = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\t\t\tif (!(status & MMIO_STATUS_GALOG_RUN_MASK))\n\t\t\t\tbreak;\n\t\t\tudelay(10);\n\t\t}\n\n\t\tif (WARN_ON(i >= LOOP_TIMEOUT))\n\t\t\treturn;\n\t}\n\n\tif (AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) &&\n\t    !check_feature_on_all_iommus(FEATURE_GAM_VAPIC)) {\n\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_LEGACY_GA;\n\t\treturn;\n\t}\n\n\tif (amd_iommu_snp_en &&\n\t    !FEATURE_SNPAVICSUP_GAM(amd_iommu_efr2)) {\n\t\tpr_warn(\"Force to disable Virtual APIC due to SNP\\n\");\n\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_LEGACY_GA;\n\t\treturn;\n\t}\n\n\t \n\tfor_each_iommu(iommu) {\n\t\tif (iommu_init_ga_log(iommu) ||\n\t\t    iommu_ga_log_enable(iommu))\n\t\t\treturn;\n\n\t\tiommu_feature_enable(iommu, CONTROL_GAM_EN);\n\t\tif (amd_iommu_snp_en)\n\t\t\tiommu_feature_enable(iommu, CONTROL_SNPAVIC_EN);\n\t}\n\n\tamd_iommu_irq_ops.capability |= (1 << IRQ_POSTING_CAP);\n\tpr_info(\"Virtual APIC enabled\\n\");\n#endif\n}\n\nstatic void enable_iommus(void)\n{\n\tearly_enable_iommus();\n}\n\nstatic void disable_iommus(void)\n{\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu)\n\t\tiommu_disable(iommu);\n\n#ifdef CONFIG_IRQ_REMAP\n\tif (AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir))\n\t\tamd_iommu_irq_ops.capability &= ~(1 << IRQ_POSTING_CAP);\n#endif\n}\n\n \n\nstatic void amd_iommu_resume(void)\n{\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu)\n\t\tiommu_apply_resume_quirks(iommu);\n\n\t \n\tenable_iommus();\n\n\tamd_iommu_enable_interrupts();\n}\n\nstatic int amd_iommu_suspend(void)\n{\n\t \n\tdisable_iommus();\n\n\treturn 0;\n}\n\nstatic struct syscore_ops amd_iommu_syscore_ops = {\n\t.suspend = amd_iommu_suspend,\n\t.resume = amd_iommu_resume,\n};\n\nstatic void __init free_iommu_resources(void)\n{\n\tkmem_cache_destroy(amd_iommu_irq_cache);\n\tamd_iommu_irq_cache = NULL;\n\n\tfree_iommu_all();\n\tfree_pci_segments();\n}\n\n \n#define IOAPIC_SB_DEVID\t\t((0x00 << 8) | PCI_DEVFN(0x14, 0))\n\nstatic bool __init check_ioapic_information(void)\n{\n\tconst char *fw_bug = FW_BUG;\n\tbool ret, has_sb_ioapic;\n\tint idx;\n\n\thas_sb_ioapic = false;\n\tret           = false;\n\n\t \n\tif (cmdline_maps)\n\t\tfw_bug = \"\";\n\n\tfor (idx = 0; idx < nr_ioapics; idx++) {\n\t\tint devid, id = mpc_ioapic_id(idx);\n\n\t\tdevid = get_ioapic_devid(id);\n\t\tif (devid < 0) {\n\t\t\tpr_err(\"%s: IOAPIC[%d] not in IVRS table\\n\",\n\t\t\t\tfw_bug, id);\n\t\t\tret = false;\n\t\t} else if (devid == IOAPIC_SB_DEVID) {\n\t\t\thas_sb_ioapic = true;\n\t\t\tret           = true;\n\t\t}\n\t}\n\n\tif (!has_sb_ioapic) {\n\t\t \n\t\tpr_err(\"%s: No southbridge IOAPIC found\\n\", fw_bug);\n\t}\n\n\tif (!ret)\n\t\tpr_err(\"Disabling interrupt remapping\\n\");\n\n\treturn ret;\n}\n\nstatic void __init free_dma_resources(void)\n{\n\tfree_pages((unsigned long)amd_iommu_pd_alloc_bitmap,\n\t\t   get_order(MAX_DOMAIN_ID/8));\n\tamd_iommu_pd_alloc_bitmap = NULL;\n\n\tfree_unity_maps();\n}\n\nstatic void __init ivinfo_init(void *ivrs)\n{\n\tamd_iommu_ivinfo = *((u32 *)(ivrs + IOMMU_IVINFO_OFFSET));\n}\n\n \nstatic int __init early_amd_iommu_init(void)\n{\n\tstruct acpi_table_header *ivrs_base;\n\tint remap_cache_sz, ret;\n\tacpi_status status;\n\n\tif (!amd_iommu_detected)\n\t\treturn -ENODEV;\n\n\tstatus = acpi_get_table(\"IVRS\", 0, &ivrs_base);\n\tif (status == AE_NOT_FOUND)\n\t\treturn -ENODEV;\n\telse if (ACPI_FAILURE(status)) {\n\t\tconst char *err = acpi_format_exception(status);\n\t\tpr_err(\"IVRS table error: %s\\n\", err);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = check_ivrs_checksum(ivrs_base);\n\tif (ret)\n\t\tgoto out;\n\n\tivinfo_init(ivrs_base);\n\n\tamd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);\n\tDUMP_printk(\"Using IVHD type %#x\\n\", amd_iommu_target_ivhd_type);\n\n\t \n\tret = -ENOMEM;\n\n\tamd_iommu_pd_alloc_bitmap = (void *)__get_free_pages(\n\t\t\t\t\t    GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t    get_order(MAX_DOMAIN_ID/8));\n\tif (amd_iommu_pd_alloc_bitmap == NULL)\n\t\tgoto out;\n\n\t \n\t__set_bit(0, amd_iommu_pd_alloc_bitmap);\n\n\t \n\tret = init_iommu_all(ivrs_base);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_LA57) &&\n\t    check_feature_gpt_level() == GUEST_PGTABLE_5_LEVEL)\n\t\tamd_iommu_gpt_level = PAGE_MODE_5_LEVEL;\n\n\t \n\tif (!is_kdump_kernel() || amd_iommu_disabled)\n\t\tdisable_iommus();\n\n\tif (amd_iommu_irq_remap)\n\t\tamd_iommu_irq_remap = check_ioapic_information();\n\n\tif (amd_iommu_irq_remap) {\n\t\tstruct amd_iommu_pci_seg *pci_seg;\n\t\t \n\t\tret = -ENOMEM;\n\t\tif (!AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir))\n\t\t\tremap_cache_sz = MAX_IRQS_PER_TABLE * sizeof(u32);\n\t\telse\n\t\t\tremap_cache_sz = MAX_IRQS_PER_TABLE * (sizeof(u64) * 2);\n\t\tamd_iommu_irq_cache = kmem_cache_create(\"irq_remap_cache\",\n\t\t\t\t\t\t\tremap_cache_sz,\n\t\t\t\t\t\t\tDTE_INTTAB_ALIGNMENT,\n\t\t\t\t\t\t\t0, NULL);\n\t\tif (!amd_iommu_irq_cache)\n\t\t\tgoto out;\n\n\t\tfor_each_pci_segment(pci_seg) {\n\t\t\tif (alloc_irq_lookup_table(pci_seg))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = init_memory_definitions(ivrs_base);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tinit_device_table();\n\nout:\n\t \n\tacpi_put_table(ivrs_base);\n\n\treturn ret;\n}\n\nstatic int amd_iommu_enable_interrupts(void)\n{\n\tstruct amd_iommu *iommu;\n\tint ret = 0;\n\n\tfor_each_iommu(iommu) {\n\t\tret = iommu_init_irq(iommu);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t \n\tenable_iommus_vapic();\n\tenable_iommus_v2();\n\nout:\n\treturn ret;\n}\n\nstatic bool __init detect_ivrs(void)\n{\n\tstruct acpi_table_header *ivrs_base;\n\tacpi_status status;\n\tint i;\n\n\tstatus = acpi_get_table(\"IVRS\", 0, &ivrs_base);\n\tif (status == AE_NOT_FOUND)\n\t\treturn false;\n\telse if (ACPI_FAILURE(status)) {\n\t\tconst char *err = acpi_format_exception(status);\n\t\tpr_err(\"IVRS table error: %s\\n\", err);\n\t\treturn false;\n\t}\n\n\tacpi_put_table(ivrs_base);\n\n\tif (amd_iommu_force_enable)\n\t\tgoto out;\n\n\t \n\tfor (i = 0; i < 32; i++) {\n\t\tu32 pci_id;\n\n\t\tpci_id = read_pci_config(0, i, 0, 0);\n\t\tif ((pci_id & 0xffff) == 0x1002 && (pci_id >> 16) == 0x98e4) {\n\t\t\tpr_info(\"Disable IOMMU on Stoney Ridge\\n\");\n\t\t\treturn false;\n\t\t}\n\t}\n\nout:\n\t \n\tpci_request_acs();\n\n\treturn true;\n}\n\n \n\nstatic int __init state_next(void)\n{\n\tint ret = 0;\n\n\tswitch (init_state) {\n\tcase IOMMU_START_STATE:\n\t\tif (!detect_ivrs()) {\n\t\t\tinit_state\t= IOMMU_NOT_FOUND;\n\t\t\tret\t\t= -ENODEV;\n\t\t} else {\n\t\t\tinit_state\t= IOMMU_IVRS_DETECTED;\n\t\t}\n\t\tbreak;\n\tcase IOMMU_IVRS_DETECTED:\n\t\tif (amd_iommu_disabled) {\n\t\t\tinit_state = IOMMU_CMDLINE_DISABLED;\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tret = early_amd_iommu_init();\n\t\t\tinit_state = ret ? IOMMU_INIT_ERROR : IOMMU_ACPI_FINISHED;\n\t\t}\n\t\tbreak;\n\tcase IOMMU_ACPI_FINISHED:\n\t\tearly_enable_iommus();\n\t\tx86_platform.iommu_shutdown = disable_iommus;\n\t\tinit_state = IOMMU_ENABLED;\n\t\tbreak;\n\tcase IOMMU_ENABLED:\n\t\tregister_syscore_ops(&amd_iommu_syscore_ops);\n\t\tret = amd_iommu_init_pci();\n\t\tinit_state = ret ? IOMMU_INIT_ERROR : IOMMU_PCI_INIT;\n\t\tbreak;\n\tcase IOMMU_PCI_INIT:\n\t\tret = amd_iommu_enable_interrupts();\n\t\tinit_state = ret ? IOMMU_INIT_ERROR : IOMMU_INTERRUPTS_EN;\n\t\tbreak;\n\tcase IOMMU_INTERRUPTS_EN:\n\t\tinit_state = IOMMU_INITIALIZED;\n\t\tbreak;\n\tcase IOMMU_INITIALIZED:\n\t\t \n\t\tbreak;\n\tcase IOMMU_NOT_FOUND:\n\tcase IOMMU_INIT_ERROR:\n\tcase IOMMU_CMDLINE_DISABLED:\n\t\t \n\t\tret = -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n\n\tif (ret) {\n\t\tfree_dma_resources();\n\t\tif (!irq_remapping_enabled) {\n\t\t\tdisable_iommus();\n\t\t\tfree_iommu_resources();\n\t\t} else {\n\t\t\tstruct amd_iommu *iommu;\n\t\t\tstruct amd_iommu_pci_seg *pci_seg;\n\n\t\t\tfor_each_pci_segment(pci_seg)\n\t\t\t\tuninit_device_table_dma(pci_seg);\n\n\t\t\tfor_each_iommu(iommu)\n\t\t\t\tiommu_flush_all_caches(iommu);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int __init iommu_go_to_state(enum iommu_init_state state)\n{\n\tint ret = -EINVAL;\n\n\twhile (init_state != state) {\n\t\tif (init_state == IOMMU_NOT_FOUND         ||\n\t\t    init_state == IOMMU_INIT_ERROR        ||\n\t\t    init_state == IOMMU_CMDLINE_DISABLED)\n\t\t\tbreak;\n\t\tret = state_next();\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_IRQ_REMAP\nint __init amd_iommu_prepare(void)\n{\n\tint ret;\n\n\tamd_iommu_irq_remap = true;\n\n\tret = iommu_go_to_state(IOMMU_ACPI_FINISHED);\n\tif (ret) {\n\t\tamd_iommu_irq_remap = false;\n\t\treturn ret;\n\t}\n\n\treturn amd_iommu_irq_remap ? 0 : -ENODEV;\n}\n\nint __init amd_iommu_enable(void)\n{\n\tint ret;\n\n\tret = iommu_go_to_state(IOMMU_ENABLED);\n\tif (ret)\n\t\treturn ret;\n\n\tirq_remapping_enabled = 1;\n\treturn amd_iommu_xt_mode;\n}\n\nvoid amd_iommu_disable(void)\n{\n\tamd_iommu_suspend();\n}\n\nint amd_iommu_reenable(int mode)\n{\n\tamd_iommu_resume();\n\n\treturn 0;\n}\n\nint __init amd_iommu_enable_faulting(void)\n{\n\t \n\treturn 0;\n}\n#endif\n\n \nstatic int __init amd_iommu_init(void)\n{\n\tstruct amd_iommu *iommu;\n\tint ret;\n\n\tret = iommu_go_to_state(IOMMU_INITIALIZED);\n#ifdef CONFIG_GART_IOMMU\n\tif (ret && list_empty(&amd_iommu_list)) {\n\t\t \n\t\tgart_iommu_init();\n\t}\n#endif\n\n\tfor_each_iommu(iommu)\n\t\tamd_iommu_debugfs_setup(iommu);\n\n\treturn ret;\n}\n\nstatic bool amd_iommu_sme_check(void)\n{\n\tif (!cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT) ||\n\t    (boot_cpu_data.x86 != 0x17))\n\t\treturn true;\n\n\t \n\tif (boot_cpu_data.microcode >= 0x08001205)\n\t\treturn true;\n\n\tif ((boot_cpu_data.microcode >= 0x08001126) &&\n\t    (boot_cpu_data.microcode <= 0x080011ff))\n\t\treturn true;\n\n\tpr_notice(\"IOMMU not currently supported when SME is active\\n\");\n\n\treturn false;\n}\n\n \nint __init amd_iommu_detect(void)\n{\n\tint ret;\n\n\tif (no_iommu || (iommu_detected && !gart_iommu_aperture))\n\t\treturn -ENODEV;\n\n\tif (!amd_iommu_sme_check())\n\t\treturn -ENODEV;\n\n\tret = iommu_go_to_state(IOMMU_IVRS_DETECTED);\n\tif (ret)\n\t\treturn ret;\n\n\tamd_iommu_detected = true;\n\tiommu_detected = 1;\n\tx86_init.iommu.iommu_init = amd_iommu_init;\n\n\treturn 1;\n}\n\n \n\nstatic int __init parse_amd_iommu_dump(char *str)\n{\n\tamd_iommu_dump = true;\n\n\treturn 1;\n}\n\nstatic int __init parse_amd_iommu_intr(char *str)\n{\n\tfor (; *str; ++str) {\n\t\tif (strncmp(str, \"legacy\", 6) == 0) {\n\t\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_LEGACY_GA;\n\t\t\tbreak;\n\t\t}\n\t\tif (strncmp(str, \"vapic\", 5) == 0) {\n\t\t\tamd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_VAPIC;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __init parse_amd_iommu_options(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\twhile (*str) {\n\t\tif (strncmp(str, \"fullflush\", 9) == 0) {\n\t\t\tpr_warn(\"amd_iommu=fullflush deprecated; use iommu.strict=1 instead\\n\");\n\t\t\tiommu_set_dma_strict();\n\t\t} else if (strncmp(str, \"force_enable\", 12) == 0) {\n\t\t\tamd_iommu_force_enable = true;\n\t\t} else if (strncmp(str, \"off\", 3) == 0) {\n\t\t\tamd_iommu_disabled = true;\n\t\t} else if (strncmp(str, \"force_isolation\", 15) == 0) {\n\t\t\tamd_iommu_force_isolation = true;\n\t\t} else if (strncmp(str, \"pgtbl_v1\", 8) == 0) {\n\t\t\tamd_iommu_pgtable = AMD_IOMMU_V1;\n\t\t} else if (strncmp(str, \"pgtbl_v2\", 8) == 0) {\n\t\t\tamd_iommu_pgtable = AMD_IOMMU_V2;\n\t\t} else if (strncmp(str, \"irtcachedis\", 11) == 0) {\n\t\t\tamd_iommu_irtcachedis = true;\n\t\t} else {\n\t\t\tpr_notice(\"Unknown option - '%s'\\n\", str);\n\t\t}\n\n\t\tstr += strcspn(str, \",\");\n\t\twhile (*str == ',')\n\t\t\tstr++;\n\t}\n\n\treturn 1;\n}\n\nstatic int __init parse_ivrs_ioapic(char *str)\n{\n\tu32 seg = 0, bus, dev, fn;\n\tint id, i;\n\tu32 devid;\n\n\tif (sscanf(str, \"=%d@%x:%x.%x\", &id, &bus, &dev, &fn) == 4 ||\n\t    sscanf(str, \"=%d@%x:%x:%x.%x\", &id, &seg, &bus, &dev, &fn) == 5)\n\t\tgoto found;\n\n\tif (sscanf(str, \"[%d]=%x:%x.%x\", &id, &bus, &dev, &fn) == 4 ||\n\t    sscanf(str, \"[%d]=%x:%x:%x.%x\", &id, &seg, &bus, &dev, &fn) == 5) {\n\t\tpr_warn(\"ivrs_ioapic%s option format deprecated; use ivrs_ioapic=%d@%04x:%02x:%02x.%d instead\\n\",\n\t\t\tstr, id, seg, bus, dev, fn);\n\t\tgoto found;\n\t}\n\n\tpr_err(\"Invalid command line: ivrs_ioapic%s\\n\", str);\n\treturn 1;\n\nfound:\n\tif (early_ioapic_map_size == EARLY_MAP_SIZE) {\n\t\tpr_err(\"Early IOAPIC map overflow - ignoring ivrs_ioapic%s\\n\",\n\t\t\tstr);\n\t\treturn 1;\n\t}\n\n\tdevid = IVRS_GET_SBDF_ID(seg, bus, dev, fn);\n\n\tcmdline_maps\t\t\t= true;\n\ti\t\t\t\t= early_ioapic_map_size++;\n\tearly_ioapic_map[i].id\t\t= id;\n\tearly_ioapic_map[i].devid\t= devid;\n\tearly_ioapic_map[i].cmd_line\t= true;\n\n\treturn 1;\n}\n\nstatic int __init parse_ivrs_hpet(char *str)\n{\n\tu32 seg = 0, bus, dev, fn;\n\tint id, i;\n\tu32 devid;\n\n\tif (sscanf(str, \"=%d@%x:%x.%x\", &id, &bus, &dev, &fn) == 4 ||\n\t    sscanf(str, \"=%d@%x:%x:%x.%x\", &id, &seg, &bus, &dev, &fn) == 5)\n\t\tgoto found;\n\n\tif (sscanf(str, \"[%d]=%x:%x.%x\", &id, &bus, &dev, &fn) == 4 ||\n\t    sscanf(str, \"[%d]=%x:%x:%x.%x\", &id, &seg, &bus, &dev, &fn) == 5) {\n\t\tpr_warn(\"ivrs_hpet%s option format deprecated; use ivrs_hpet=%d@%04x:%02x:%02x.%d instead\\n\",\n\t\t\tstr, id, seg, bus, dev, fn);\n\t\tgoto found;\n\t}\n\n\tpr_err(\"Invalid command line: ivrs_hpet%s\\n\", str);\n\treturn 1;\n\nfound:\n\tif (early_hpet_map_size == EARLY_MAP_SIZE) {\n\t\tpr_err(\"Early HPET map overflow - ignoring ivrs_hpet%s\\n\",\n\t\t\tstr);\n\t\treturn 1;\n\t}\n\n\tdevid = IVRS_GET_SBDF_ID(seg, bus, dev, fn);\n\n\tcmdline_maps\t\t\t= true;\n\ti\t\t\t\t= early_hpet_map_size++;\n\tearly_hpet_map[i].id\t\t= id;\n\tearly_hpet_map[i].devid\t\t= devid;\n\tearly_hpet_map[i].cmd_line\t= true;\n\n\treturn 1;\n}\n\n#define ACPIID_LEN (ACPIHID_UID_LEN + ACPIHID_HID_LEN)\n\nstatic int __init parse_ivrs_acpihid(char *str)\n{\n\tu32 seg = 0, bus, dev, fn;\n\tchar *hid, *uid, *p, *addr;\n\tchar acpiid[ACPIID_LEN] = {0};\n\tint i;\n\n\taddr = strchr(str, '@');\n\tif (!addr) {\n\t\taddr = strchr(str, '=');\n\t\tif (!addr)\n\t\t\tgoto not_found;\n\n\t\t++addr;\n\n\t\tif (strlen(addr) > ACPIID_LEN)\n\t\t\tgoto not_found;\n\n\t\tif (sscanf(str, \"[%x:%x.%x]=%s\", &bus, &dev, &fn, acpiid) == 4 ||\n\t\t    sscanf(str, \"[%x:%x:%x.%x]=%s\", &seg, &bus, &dev, &fn, acpiid) == 5) {\n\t\t\tpr_warn(\"ivrs_acpihid%s option format deprecated; use ivrs_acpihid=%s@%04x:%02x:%02x.%d instead\\n\",\n\t\t\t\tstr, acpiid, seg, bus, dev, fn);\n\t\t\tgoto found;\n\t\t}\n\t\tgoto not_found;\n\t}\n\n\t \n\t*addr++ = 0;\n\n\tif (strlen(str) > ACPIID_LEN + 1)\n\t\tgoto not_found;\n\n\tif (sscanf(str, \"=%s\", acpiid) != 1)\n\t\tgoto not_found;\n\n\tif (sscanf(addr, \"%x:%x.%x\", &bus, &dev, &fn) == 3 ||\n\t    sscanf(addr, \"%x:%x:%x.%x\", &seg, &bus, &dev, &fn) == 4)\n\t\tgoto found;\n\nnot_found:\n\tpr_err(\"Invalid command line: ivrs_acpihid%s\\n\", str);\n\treturn 1;\n\nfound:\n\tp = acpiid;\n\thid = strsep(&p, \":\");\n\tuid = p;\n\n\tif (!hid || !(*hid) || !uid) {\n\t\tpr_err(\"Invalid command line: hid or uid\\n\");\n\t\treturn 1;\n\t}\n\n\t \n\twhile (*uid == '0' && *(uid + 1))\n\t\tuid++;\n\n\ti = early_acpihid_map_size++;\n\tmemcpy(early_acpihid_map[i].hid, hid, strlen(hid));\n\tmemcpy(early_acpihid_map[i].uid, uid, strlen(uid));\n\tearly_acpihid_map[i].devid = IVRS_GET_SBDF_ID(seg, bus, dev, fn);\n\tearly_acpihid_map[i].cmd_line\t= true;\n\n\treturn 1;\n}\n\n__setup(\"amd_iommu_dump\",\tparse_amd_iommu_dump);\n__setup(\"amd_iommu=\",\t\tparse_amd_iommu_options);\n__setup(\"amd_iommu_intr=\",\tparse_amd_iommu_intr);\n__setup(\"ivrs_ioapic\",\t\tparse_ivrs_ioapic);\n__setup(\"ivrs_hpet\",\t\tparse_ivrs_hpet);\n__setup(\"ivrs_acpihid\",\t\tparse_ivrs_acpihid);\n\nbool amd_iommu_v2_supported(void)\n{\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_LA57) &&\n\t    amd_iommu_gpt_level != PAGE_MODE_5_LEVEL)\n\t\treturn false;\n\n\t \n\treturn amd_iommu_v2_present && !amd_iommu_snp_en;\n}\nEXPORT_SYMBOL(amd_iommu_v2_supported);\n\nstruct amd_iommu *get_amd_iommu(unsigned int idx)\n{\n\tunsigned int i = 0;\n\tstruct amd_iommu *iommu;\n\n\tfor_each_iommu(iommu)\n\t\tif (i++ == idx)\n\t\t\treturn iommu;\n\treturn NULL;\n}\n\n \n\nu8 amd_iommu_pc_get_max_banks(unsigned int idx)\n{\n\tstruct amd_iommu *iommu = get_amd_iommu(idx);\n\n\tif (iommu)\n\t\treturn iommu->max_banks;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(amd_iommu_pc_get_max_banks);\n\nbool amd_iommu_pc_supported(void)\n{\n\treturn amd_iommu_pc_present;\n}\nEXPORT_SYMBOL(amd_iommu_pc_supported);\n\nu8 amd_iommu_pc_get_max_counters(unsigned int idx)\n{\n\tstruct amd_iommu *iommu = get_amd_iommu(idx);\n\n\tif (iommu)\n\t\treturn iommu->max_counters;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(amd_iommu_pc_get_max_counters);\n\nstatic int iommu_pc_get_set_reg(struct amd_iommu *iommu, u8 bank, u8 cntr,\n\t\t\t\tu8 fxn, u64 *value, bool is_write)\n{\n\tu32 offset;\n\tu32 max_offset_lim;\n\n\t \n\tif (!amd_iommu_pc_present)\n\t\treturn -ENODEV;\n\n\t \n\tif (WARN_ON(!iommu || (fxn > 0x28) || (fxn & 7)))\n\t\treturn -ENODEV;\n\n\toffset = (u32)(((0x40 | bank) << 12) | (cntr << 8) | fxn);\n\n\t \n\tmax_offset_lim = (u32)(((0x40 | iommu->max_banks) << 12) |\n\t\t\t\t(iommu->max_counters << 8) | 0x28);\n\tif ((offset < MMIO_CNTR_REG_OFFSET) ||\n\t    (offset > max_offset_lim))\n\t\treturn -EINVAL;\n\n\tif (is_write) {\n\t\tu64 val = *value & GENMASK_ULL(47, 0);\n\n\t\twritel((u32)val, iommu->mmio_base + offset);\n\t\twritel((val >> 32), iommu->mmio_base + offset + 4);\n\t} else {\n\t\t*value = readl(iommu->mmio_base + offset + 4);\n\t\t*value <<= 32;\n\t\t*value |= readl(iommu->mmio_base + offset);\n\t\t*value &= GENMASK_ULL(47, 0);\n\t}\n\n\treturn 0;\n}\n\nint amd_iommu_pc_get_reg(struct amd_iommu *iommu, u8 bank, u8 cntr, u8 fxn, u64 *value)\n{\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\treturn iommu_pc_get_set_reg(iommu, bank, cntr, fxn, value, false);\n}\n\nint amd_iommu_pc_set_reg(struct amd_iommu *iommu, u8 bank, u8 cntr, u8 fxn, u64 *value)\n{\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\treturn iommu_pc_get_set_reg(iommu, bank, cntr, fxn, value, true);\n}\n\n#ifdef CONFIG_AMD_MEM_ENCRYPT\nint amd_iommu_snp_enable(void)\n{\n\t \n\tif (no_iommu || iommu_default_passthrough()) {\n\t\tpr_err(\"SNP: IOMMU is disabled or configured in passthrough mode, SNP cannot be supported\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (init_state > IOMMU_ENABLED) {\n\t\tpr_err(\"SNP: Too late to enable SNP for IOMMU.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tamd_iommu_snp_en = check_feature_on_all_iommus(FEATURE_SNP);\n\tif (!amd_iommu_snp_en)\n\t\treturn -EINVAL;\n\n\tpr_info(\"SNP enabled\\n\");\n\n\t \n\tif (amd_iommu_pgtable != AMD_IOMMU_V1) {\n\t\tpr_warn(\"Force to using AMD IOMMU v1 page table due to SNP\\n\");\n\t\tamd_iommu_pgtable = AMD_IOMMU_V1;\n\t}\n\n\treturn 0;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}