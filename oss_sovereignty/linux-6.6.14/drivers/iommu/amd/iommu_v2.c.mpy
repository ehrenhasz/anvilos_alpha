{
  "module_name": "iommu_v2.c",
  "hash_id": "d09bc4570a4b4f55b877d67bbcce3a3217d84acda92f98d8797453f80500c3bc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/amd/iommu_v2.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"AMD-Vi: \" fmt\n\n#include <linux/refcount.h>\n#include <linux/mmu_notifier.h>\n#include <linux/amd-iommu.h>\n#include <linux/mm_types.h>\n#include <linux/profile.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/wait.h>\n#include <linux/pci.h>\n#include <linux/gfp.h>\n#include <linux/cc_platform.h>\n\n#include \"amd_iommu.h\"\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Joerg Roedel <jroedel@suse.de>\");\n\n#define PRI_QUEUE_SIZE\t\t512\n\nstruct pri_queue {\n\tatomic_t inflight;\n\tbool finish;\n\tint status;\n};\n\nstruct pasid_state {\n\tstruct list_head list;\t\t\t \n\trefcount_t count;\t\t\t\t \n\tunsigned mmu_notifier_count;\t\t \n\tstruct mm_struct *mm;\t\t\t \n\tstruct mmu_notifier mn;                  \n\tstruct pri_queue pri[PRI_QUEUE_SIZE];\t \n\tstruct device_state *device_state;\t \n\tu32 pasid;\t\t\t\t \n\tbool invalid;\t\t\t\t \n\tspinlock_t lock;\t\t\t \n\twait_queue_head_t wq;\t\t\t \n};\n\nstruct device_state {\n\tstruct list_head list;\n\tu32 sbdf;\n\tatomic_t count;\n\tstruct pci_dev *pdev;\n\tstruct pasid_state **states;\n\tstruct iommu_domain *domain;\n\tint pasid_levels;\n\tint max_pasids;\n\tamd_iommu_invalid_ppr_cb inv_ppr_cb;\n\tamd_iommu_invalidate_ctx inv_ctx_cb;\n\tspinlock_t lock;\n\twait_queue_head_t wq;\n};\n\nstruct fault {\n\tstruct work_struct work;\n\tstruct device_state *dev_state;\n\tstruct pasid_state *state;\n\tstruct mm_struct *mm;\n\tu64 address;\n\tu32 pasid;\n\tu16 tag;\n\tu16 finish;\n\tu16 flags;\n};\n\nstatic LIST_HEAD(state_list);\nstatic DEFINE_SPINLOCK(state_lock);\n\nstatic struct workqueue_struct *iommu_wq;\n\nstatic void free_pasid_states(struct device_state *dev_state);\n\nstatic struct device_state *__get_device_state(u32 sbdf)\n{\n\tstruct device_state *dev_state;\n\n\tlist_for_each_entry(dev_state, &state_list, list) {\n\t\tif (dev_state->sbdf == sbdf)\n\t\t\treturn dev_state;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct device_state *get_device_state(u32 sbdf)\n{\n\tstruct device_state *dev_state;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&state_lock, flags);\n\tdev_state = __get_device_state(sbdf);\n\tif (dev_state != NULL)\n\t\tatomic_inc(&dev_state->count);\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\treturn dev_state;\n}\n\nstatic void free_device_state(struct device_state *dev_state)\n{\n\tstruct iommu_group *group;\n\n\t \n\tfree_pasid_states(dev_state);\n\n\t \n\twait_event(dev_state->wq, !atomic_read(&dev_state->count));\n\n\t \n\tgroup = iommu_group_get(&dev_state->pdev->dev);\n\tif (WARN_ON(!group))\n\t\treturn;\n\n\tiommu_detach_group(dev_state->domain, group);\n\n\tiommu_group_put(group);\n\n\t \n\tiommu_domain_free(dev_state->domain);\n\n\t \n\tkfree(dev_state);\n}\n\nstatic void put_device_state(struct device_state *dev_state)\n{\n\tif (atomic_dec_and_test(&dev_state->count))\n\t\twake_up(&dev_state->wq);\n}\n\n \nstatic struct pasid_state **__get_pasid_state_ptr(struct device_state *dev_state,\n\t\t\t\t\t\t  u32 pasid, bool alloc)\n{\n\tstruct pasid_state **root, **ptr;\n\tint level, index;\n\n\tlevel = dev_state->pasid_levels;\n\troot  = dev_state->states;\n\n\twhile (true) {\n\n\t\tindex = (pasid >> (9 * level)) & 0x1ff;\n\t\tptr   = &root[index];\n\n\t\tif (level == 0)\n\t\t\tbreak;\n\n\t\tif (*ptr == NULL) {\n\t\t\tif (!alloc)\n\t\t\t\treturn NULL;\n\n\t\t\t*ptr = (void *)get_zeroed_page(GFP_ATOMIC);\n\t\t\tif (*ptr == NULL)\n\t\t\t\treturn NULL;\n\t\t}\n\n\t\troot   = (struct pasid_state **)*ptr;\n\t\tlevel -= 1;\n\t}\n\n\treturn ptr;\n}\n\nstatic int set_pasid_state(struct device_state *dev_state,\n\t\t\t   struct pasid_state *pasid_state,\n\t\t\t   u32 pasid)\n{\n\tstruct pasid_state **ptr;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev_state->lock, flags);\n\tptr = __get_pasid_state_ptr(dev_state, pasid, true);\n\n\tret = -ENOMEM;\n\tif (ptr == NULL)\n\t\tgoto out_unlock;\n\n\tret = -ENOMEM;\n\tif (*ptr != NULL)\n\t\tgoto out_unlock;\n\n\t*ptr = pasid_state;\n\n\tret = 0;\n\nout_unlock:\n\tspin_unlock_irqrestore(&dev_state->lock, flags);\n\n\treturn ret;\n}\n\nstatic void clear_pasid_state(struct device_state *dev_state, u32 pasid)\n{\n\tstruct pasid_state **ptr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev_state->lock, flags);\n\tptr = __get_pasid_state_ptr(dev_state, pasid, true);\n\n\tif (ptr == NULL)\n\t\tgoto out_unlock;\n\n\t*ptr = NULL;\n\nout_unlock:\n\tspin_unlock_irqrestore(&dev_state->lock, flags);\n}\n\nstatic struct pasid_state *get_pasid_state(struct device_state *dev_state,\n\t\t\t\t\t   u32 pasid)\n{\n\tstruct pasid_state **ptr, *ret = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev_state->lock, flags);\n\tptr = __get_pasid_state_ptr(dev_state, pasid, false);\n\n\tif (ptr == NULL)\n\t\tgoto out_unlock;\n\n\tret = *ptr;\n\tif (ret)\n\t\trefcount_inc(&ret->count);\n\nout_unlock:\n\tspin_unlock_irqrestore(&dev_state->lock, flags);\n\n\treturn ret;\n}\n\nstatic void free_pasid_state(struct pasid_state *pasid_state)\n{\n\tkfree(pasid_state);\n}\n\nstatic void put_pasid_state(struct pasid_state *pasid_state)\n{\n\tif (refcount_dec_and_test(&pasid_state->count))\n\t\twake_up(&pasid_state->wq);\n}\n\nstatic void put_pasid_state_wait(struct pasid_state *pasid_state)\n{\n\tif (!refcount_dec_and_test(&pasid_state->count))\n\t\twait_event(pasid_state->wq, !refcount_read(&pasid_state->count));\n\tfree_pasid_state(pasid_state);\n}\n\nstatic void unbind_pasid(struct pasid_state *pasid_state)\n{\n\tstruct iommu_domain *domain;\n\n\tdomain = pasid_state->device_state->domain;\n\n\t \n\tpasid_state->invalid = true;\n\n\t \n\tsmp_wmb();\n\n\t \n\tamd_iommu_domain_clear_gcr3(domain, pasid_state->pasid);\n\n\t \n\tflush_workqueue(iommu_wq);\n}\n\nstatic void free_pasid_states_level1(struct pasid_state **tbl)\n{\n\tint i;\n\n\tfor (i = 0; i < 512; ++i) {\n\t\tif (tbl[i] == NULL)\n\t\t\tcontinue;\n\n\t\tfree_page((unsigned long)tbl[i]);\n\t}\n}\n\nstatic void free_pasid_states_level2(struct pasid_state **tbl)\n{\n\tstruct pasid_state **ptr;\n\tint i;\n\n\tfor (i = 0; i < 512; ++i) {\n\t\tif (tbl[i] == NULL)\n\t\t\tcontinue;\n\n\t\tptr = (struct pasid_state **)tbl[i];\n\t\tfree_pasid_states_level1(ptr);\n\t}\n}\n\nstatic void free_pasid_states(struct device_state *dev_state)\n{\n\tstruct pasid_state *pasid_state;\n\tint i;\n\n\tfor (i = 0; i < dev_state->max_pasids; ++i) {\n\t\tpasid_state = get_pasid_state(dev_state, i);\n\t\tif (pasid_state == NULL)\n\t\t\tcontinue;\n\n\t\tput_pasid_state(pasid_state);\n\n\t\t \n\t\tclear_pasid_state(dev_state, pasid_state->pasid);\n\n\t\t \n\t\tmmu_notifier_unregister(&pasid_state->mn, pasid_state->mm);\n\n\t\tput_pasid_state_wait(pasid_state);  \n\n\t\t \n\t\tput_device_state(dev_state);\n\t}\n\n\tif (dev_state->pasid_levels == 2)\n\t\tfree_pasid_states_level2(dev_state->states);\n\telse if (dev_state->pasid_levels == 1)\n\t\tfree_pasid_states_level1(dev_state->states);\n\telse\n\t\tBUG_ON(dev_state->pasid_levels != 0);\n\n\tfree_page((unsigned long)dev_state->states);\n}\n\nstatic struct pasid_state *mn_to_state(struct mmu_notifier *mn)\n{\n\treturn container_of(mn, struct pasid_state, mn);\n}\n\nstatic void mn_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct pasid_state *pasid_state;\n\tstruct device_state *dev_state;\n\n\tpasid_state = mn_to_state(mn);\n\tdev_state   = pasid_state->device_state;\n\n\tif ((start ^ (end - 1)) < PAGE_SIZE)\n\t\tamd_iommu_flush_page(dev_state->domain, pasid_state->pasid,\n\t\t\t\t     start);\n\telse\n\t\tamd_iommu_flush_tlb(dev_state->domain, pasid_state->pasid);\n}\n\nstatic void mn_release(struct mmu_notifier *mn, struct mm_struct *mm)\n{\n\tstruct pasid_state *pasid_state;\n\tstruct device_state *dev_state;\n\tbool run_inv_ctx_cb;\n\n\tmight_sleep();\n\n\tpasid_state    = mn_to_state(mn);\n\tdev_state      = pasid_state->device_state;\n\trun_inv_ctx_cb = !pasid_state->invalid;\n\n\tif (run_inv_ctx_cb && dev_state->inv_ctx_cb)\n\t\tdev_state->inv_ctx_cb(dev_state->pdev, pasid_state->pasid);\n\n\tunbind_pasid(pasid_state);\n}\n\nstatic const struct mmu_notifier_ops iommu_mn = {\n\t.release\t\t\t= mn_release,\n\t.arch_invalidate_secondary_tlbs\t= mn_arch_invalidate_secondary_tlbs,\n};\n\nstatic void set_pri_tag_status(struct pasid_state *pasid_state,\n\t\t\t       u16 tag, int status)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pasid_state->lock, flags);\n\tpasid_state->pri[tag].status = status;\n\tspin_unlock_irqrestore(&pasid_state->lock, flags);\n}\n\nstatic void finish_pri_tag(struct device_state *dev_state,\n\t\t\t   struct pasid_state *pasid_state,\n\t\t\t   u16 tag)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pasid_state->lock, flags);\n\tif (atomic_dec_and_test(&pasid_state->pri[tag].inflight) &&\n\t    pasid_state->pri[tag].finish) {\n\t\tamd_iommu_complete_ppr(dev_state->pdev, pasid_state->pasid,\n\t\t\t\t       pasid_state->pri[tag].status, tag);\n\t\tpasid_state->pri[tag].finish = false;\n\t\tpasid_state->pri[tag].status = PPR_SUCCESS;\n\t}\n\tspin_unlock_irqrestore(&pasid_state->lock, flags);\n}\n\nstatic void handle_fault_error(struct fault *fault)\n{\n\tint status;\n\n\tif (!fault->dev_state->inv_ppr_cb) {\n\t\tset_pri_tag_status(fault->state, fault->tag, PPR_INVALID);\n\t\treturn;\n\t}\n\n\tstatus = fault->dev_state->inv_ppr_cb(fault->dev_state->pdev,\n\t\t\t\t\t      fault->pasid,\n\t\t\t\t\t      fault->address,\n\t\t\t\t\t      fault->flags);\n\tswitch (status) {\n\tcase AMD_IOMMU_INV_PRI_RSP_SUCCESS:\n\t\tset_pri_tag_status(fault->state, fault->tag, PPR_SUCCESS);\n\t\tbreak;\n\tcase AMD_IOMMU_INV_PRI_RSP_INVALID:\n\t\tset_pri_tag_status(fault->state, fault->tag, PPR_INVALID);\n\t\tbreak;\n\tcase AMD_IOMMU_INV_PRI_RSP_FAIL:\n\t\tset_pri_tag_status(fault->state, fault->tag, PPR_FAILURE);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic bool access_error(struct vm_area_struct *vma, struct fault *fault)\n{\n\tunsigned long requested = 0;\n\n\tif (fault->flags & PPR_FAULT_EXEC)\n\t\trequested |= VM_EXEC;\n\n\tif (fault->flags & PPR_FAULT_READ)\n\t\trequested |= VM_READ;\n\n\tif (fault->flags & PPR_FAULT_WRITE)\n\t\trequested |= VM_WRITE;\n\n\treturn (requested & ~vma->vm_flags) != 0;\n}\n\nstatic void do_fault(struct work_struct *work)\n{\n\tstruct fault *fault = container_of(work, struct fault, work);\n\tstruct vm_area_struct *vma;\n\tvm_fault_t ret = VM_FAULT_ERROR;\n\tunsigned int flags = 0;\n\tstruct mm_struct *mm;\n\tu64 address;\n\n\tmm = fault->state->mm;\n\taddress = fault->address;\n\n\tif (fault->flags & PPR_FAULT_USER)\n\t\tflags |= FAULT_FLAG_USER;\n\tif (fault->flags & PPR_FAULT_WRITE)\n\t\tflags |= FAULT_FLAG_WRITE;\n\tflags |= FAULT_FLAG_REMOTE;\n\n\tmmap_read_lock(mm);\n\tvma = vma_lookup(mm, address);\n\tif (!vma)\n\t\t \n\t\tgoto out;\n\n\t \n\tif (access_error(vma, fault))\n\t\tgoto out;\n\n\tret = handle_mm_fault(vma, address, flags, NULL);\nout:\n\tmmap_read_unlock(mm);\n\n\tif (ret & VM_FAULT_ERROR)\n\t\t \n\t\thandle_fault_error(fault);\n\n\tfinish_pri_tag(fault->dev_state, fault->state, fault->tag);\n\n\tput_pasid_state(fault->state);\n\n\tkfree(fault);\n}\n\nstatic int ppr_notifier(struct notifier_block *nb, unsigned long e, void *data)\n{\n\tstruct amd_iommu_fault *iommu_fault;\n\tstruct pasid_state *pasid_state;\n\tstruct device_state *dev_state;\n\tstruct pci_dev *pdev = NULL;\n\tunsigned long flags;\n\tstruct fault *fault;\n\tbool finish;\n\tu16 tag, devid, seg_id;\n\tint ret;\n\n\tiommu_fault = data;\n\ttag         = iommu_fault->tag & 0x1ff;\n\tfinish      = (iommu_fault->tag >> 9) & 1;\n\n\tseg_id = PCI_SBDF_TO_SEGID(iommu_fault->sbdf);\n\tdevid = PCI_SBDF_TO_DEVID(iommu_fault->sbdf);\n\tpdev = pci_get_domain_bus_and_slot(seg_id, PCI_BUS_NUM(devid),\n\t\t\t\t\t   devid & 0xff);\n\tif (!pdev)\n\t\treturn -ENODEV;\n\n\tret = NOTIFY_DONE;\n\n\t \n\tif (amd_iommu_is_attach_deferred(&pdev->dev)) {\n\t\tamd_iommu_complete_ppr(pdev, iommu_fault->pasid,\n\t\t\t\t       PPR_INVALID, tag);\n\t\tgoto out;\n\t}\n\n\tdev_state = get_device_state(iommu_fault->sbdf);\n\tif (dev_state == NULL)\n\t\tgoto out;\n\n\tpasid_state = get_pasid_state(dev_state, iommu_fault->pasid);\n\tif (pasid_state == NULL || pasid_state->invalid) {\n\t\t \n\t\tamd_iommu_complete_ppr(dev_state->pdev, iommu_fault->pasid,\n\t\t\t\t       PPR_INVALID, tag);\n\t\tgoto out_drop_state;\n\t}\n\n\tspin_lock_irqsave(&pasid_state->lock, flags);\n\tatomic_inc(&pasid_state->pri[tag].inflight);\n\tif (finish)\n\t\tpasid_state->pri[tag].finish = true;\n\tspin_unlock_irqrestore(&pasid_state->lock, flags);\n\n\tfault = kzalloc(sizeof(*fault), GFP_ATOMIC);\n\tif (fault == NULL) {\n\t\t \n\t\tfinish_pri_tag(dev_state, pasid_state, tag);\n\t\tgoto out_drop_state;\n\t}\n\n\tfault->dev_state = dev_state;\n\tfault->address   = iommu_fault->address;\n\tfault->state     = pasid_state;\n\tfault->tag       = tag;\n\tfault->finish    = finish;\n\tfault->pasid     = iommu_fault->pasid;\n\tfault->flags     = iommu_fault->flags;\n\tINIT_WORK(&fault->work, do_fault);\n\n\tqueue_work(iommu_wq, &fault->work);\n\n\tret = NOTIFY_OK;\n\nout_drop_state:\n\n\tif (ret != NOTIFY_OK && pasid_state)\n\t\tput_pasid_state(pasid_state);\n\n\tput_device_state(dev_state);\n\nout:\n\tpci_dev_put(pdev);\n\treturn ret;\n}\n\nstatic struct notifier_block ppr_nb = {\n\t.notifier_call = ppr_notifier,\n};\n\nint amd_iommu_bind_pasid(struct pci_dev *pdev, u32 pasid,\n\t\t\t struct task_struct *task)\n{\n\tstruct pasid_state *pasid_state;\n\tstruct device_state *dev_state;\n\tstruct mm_struct *mm;\n\tu32 sbdf;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn -ENODEV;\n\n\tsbdf      = get_pci_sbdf_id(pdev);\n\tdev_state = get_device_state(sbdf);\n\n\tif (dev_state == NULL)\n\t\treturn -EINVAL;\n\n\tret = -EINVAL;\n\tif (pasid >= dev_state->max_pasids)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tpasid_state = kzalloc(sizeof(*pasid_state), GFP_KERNEL);\n\tif (pasid_state == NULL)\n\t\tgoto out;\n\n\n\trefcount_set(&pasid_state->count, 1);\n\tinit_waitqueue_head(&pasid_state->wq);\n\tspin_lock_init(&pasid_state->lock);\n\n\tmm                        = get_task_mm(task);\n\tpasid_state->mm           = mm;\n\tpasid_state->device_state = dev_state;\n\tpasid_state->pasid        = pasid;\n\tpasid_state->invalid      = true;  \n\tpasid_state->mn.ops       = &iommu_mn;\n\n\tif (pasid_state->mm == NULL)\n\t\tgoto out_free;\n\n\tret = mmu_notifier_register(&pasid_state->mn, mm);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = set_pasid_state(dev_state, pasid_state, pasid);\n\tif (ret)\n\t\tgoto out_unregister;\n\n\tret = amd_iommu_domain_set_gcr3(dev_state->domain, pasid,\n\t\t\t\t\t__pa(pasid_state->mm->pgd));\n\tif (ret)\n\t\tgoto out_clear_state;\n\n\t \n\tpasid_state->invalid = false;\n\n\t \n\tmmput(mm);\n\n\treturn 0;\n\nout_clear_state:\n\tclear_pasid_state(dev_state, pasid);\n\nout_unregister:\n\tmmu_notifier_unregister(&pasid_state->mn, mm);\n\tmmput(mm);\n\nout_free:\n\tfree_pasid_state(pasid_state);\n\nout:\n\tput_device_state(dev_state);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_bind_pasid);\n\nvoid amd_iommu_unbind_pasid(struct pci_dev *pdev, u32 pasid)\n{\n\tstruct pasid_state *pasid_state;\n\tstruct device_state *dev_state;\n\tu32 sbdf;\n\n\tmight_sleep();\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn;\n\n\tsbdf = get_pci_sbdf_id(pdev);\n\tdev_state = get_device_state(sbdf);\n\tif (dev_state == NULL)\n\t\treturn;\n\n\tif (pasid >= dev_state->max_pasids)\n\t\tgoto out;\n\n\tpasid_state = get_pasid_state(dev_state, pasid);\n\tif (pasid_state == NULL)\n\t\tgoto out;\n\t \n\tput_pasid_state(pasid_state);\n\n\t \n\tclear_pasid_state(dev_state, pasid_state->pasid);\n\n\t \n\tmmu_notifier_unregister(&pasid_state->mn, pasid_state->mm);\n\n\tput_pasid_state_wait(pasid_state);  \nout:\n\t \n\tput_device_state(dev_state);\n\n\t \n\tput_device_state(dev_state);\n}\nEXPORT_SYMBOL(amd_iommu_unbind_pasid);\n\nint amd_iommu_init_device(struct pci_dev *pdev, int pasids)\n{\n\tstruct device_state *dev_state;\n\tstruct iommu_group *group;\n\tunsigned long flags;\n\tint ret, tmp;\n\tu32 sbdf;\n\n\tmight_sleep();\n\n\t \n\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\treturn -ENODEV;\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn -ENODEV;\n\n\tif (pasids <= 0 || pasids > (PASID_MASK + 1))\n\t\treturn -EINVAL;\n\n\tsbdf = get_pci_sbdf_id(pdev);\n\n\tdev_state = kzalloc(sizeof(*dev_state), GFP_KERNEL);\n\tif (dev_state == NULL)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&dev_state->lock);\n\tinit_waitqueue_head(&dev_state->wq);\n\tdev_state->pdev  = pdev;\n\tdev_state->sbdf = sbdf;\n\n\ttmp = pasids;\n\tfor (dev_state->pasid_levels = 0; (tmp - 1) & ~0x1ff; tmp >>= 9)\n\t\tdev_state->pasid_levels += 1;\n\n\tatomic_set(&dev_state->count, 1);\n\tdev_state->max_pasids = pasids;\n\n\tret = -ENOMEM;\n\tdev_state->states = (void *)get_zeroed_page(GFP_KERNEL);\n\tif (dev_state->states == NULL)\n\t\tgoto out_free_dev_state;\n\n\tdev_state->domain = iommu_domain_alloc(&pci_bus_type);\n\tif (dev_state->domain == NULL)\n\t\tgoto out_free_states;\n\n\t \n\tdev_state->domain->type = IOMMU_DOMAIN_IDENTITY;\n\tamd_iommu_domain_direct_map(dev_state->domain);\n\n\tret = amd_iommu_domain_enable_v2(dev_state->domain, pasids);\n\tif (ret)\n\t\tgoto out_free_domain;\n\n\tgroup = iommu_group_get(&pdev->dev);\n\tif (!group) {\n\t\tret = -EINVAL;\n\t\tgoto out_free_domain;\n\t}\n\n\tret = iommu_attach_group(dev_state->domain, group);\n\tif (ret != 0)\n\t\tgoto out_drop_group;\n\n\tiommu_group_put(group);\n\n\tspin_lock_irqsave(&state_lock, flags);\n\n\tif (__get_device_state(sbdf) != NULL) {\n\t\tspin_unlock_irqrestore(&state_lock, flags);\n\t\tret = -EBUSY;\n\t\tgoto out_free_domain;\n\t}\n\n\tlist_add_tail(&dev_state->list, &state_list);\n\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\treturn 0;\n\nout_drop_group:\n\tiommu_group_put(group);\n\nout_free_domain:\n\tiommu_domain_free(dev_state->domain);\n\nout_free_states:\n\tfree_page((unsigned long)dev_state->states);\n\nout_free_dev_state:\n\tkfree(dev_state);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_init_device);\n\nvoid amd_iommu_free_device(struct pci_dev *pdev)\n{\n\tstruct device_state *dev_state;\n\tunsigned long flags;\n\tu32 sbdf;\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn;\n\n\tsbdf = get_pci_sbdf_id(pdev);\n\n\tspin_lock_irqsave(&state_lock, flags);\n\n\tdev_state = __get_device_state(sbdf);\n\tif (dev_state == NULL) {\n\t\tspin_unlock_irqrestore(&state_lock, flags);\n\t\treturn;\n\t}\n\n\tlist_del(&dev_state->list);\n\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\tput_device_state(dev_state);\n\tfree_device_state(dev_state);\n}\nEXPORT_SYMBOL(amd_iommu_free_device);\n\nint amd_iommu_set_invalid_ppr_cb(struct pci_dev *pdev,\n\t\t\t\t amd_iommu_invalid_ppr_cb cb)\n{\n\tstruct device_state *dev_state;\n\tunsigned long flags;\n\tu32 sbdf;\n\tint ret;\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn -ENODEV;\n\n\tsbdf = get_pci_sbdf_id(pdev);\n\n\tspin_lock_irqsave(&state_lock, flags);\n\n\tret = -EINVAL;\n\tdev_state = __get_device_state(sbdf);\n\tif (dev_state == NULL)\n\t\tgoto out_unlock;\n\n\tdev_state->inv_ppr_cb = cb;\n\n\tret = 0;\n\nout_unlock:\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_set_invalid_ppr_cb);\n\nint amd_iommu_set_invalidate_ctx_cb(struct pci_dev *pdev,\n\t\t\t\t    amd_iommu_invalidate_ctx cb)\n{\n\tstruct device_state *dev_state;\n\tunsigned long flags;\n\tu32 sbdf;\n\tint ret;\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn -ENODEV;\n\n\tsbdf = get_pci_sbdf_id(pdev);\n\n\tspin_lock_irqsave(&state_lock, flags);\n\n\tret = -EINVAL;\n\tdev_state = __get_device_state(sbdf);\n\tif (dev_state == NULL)\n\t\tgoto out_unlock;\n\n\tdev_state->inv_ctx_cb = cb;\n\n\tret = 0;\n\nout_unlock:\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_set_invalidate_ctx_cb);\n\nstatic int __init amd_iommu_v2_init(void)\n{\n\tint ret;\n\n\tif (!amd_iommu_v2_supported()) {\n\t\tpr_info(\"AMD IOMMUv2 functionality not available on this system - This is not a bug.\\n\");\n\t\t \n\t\treturn 0;\n\t}\n\n\tret = -ENOMEM;\n\tiommu_wq = alloc_workqueue(\"amd_iommu_v2\", WQ_MEM_RECLAIM, 0);\n\tif (iommu_wq == NULL)\n\t\tgoto out;\n\n\tamd_iommu_register_ppr_notifier(&ppr_nb);\n\n\tpr_info(\"AMD IOMMUv2 loaded and initialized\\n\");\n\n\treturn 0;\n\nout:\n\treturn ret;\n}\n\nstatic void __exit amd_iommu_v2_exit(void)\n{\n\tstruct device_state *dev_state, *next;\n\tunsigned long flags;\n\tLIST_HEAD(freelist);\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn;\n\n\tamd_iommu_unregister_ppr_notifier(&ppr_nb);\n\n\tflush_workqueue(iommu_wq);\n\n\t \n\tspin_lock_irqsave(&state_lock, flags);\n\n\tlist_for_each_entry_safe(dev_state, next, &state_list, list) {\n\t\tWARN_ON_ONCE(1);\n\n\t\tput_device_state(dev_state);\n\t\tlist_del(&dev_state->list);\n\t\tlist_add_tail(&dev_state->list, &freelist);\n\t}\n\n\tspin_unlock_irqrestore(&state_lock, flags);\n\n\t \n\tlist_for_each_entry_safe(dev_state, next, &freelist, list) {\n\t\tlist_del(&dev_state->list);\n\t\tfree_device_state(dev_state);\n\t}\n\n\tdestroy_workqueue(iommu_wq);\n}\n\nmodule_init(amd_iommu_v2_init);\nmodule_exit(amd_iommu_v2_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}