{
  "module_name": "io_pgtable_v2.c",
  "hash_id": "cd8ddc3ce9e42c2d47cda0ecb60915ffa0d2027bb74b51edbaec9759f75541b9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/amd/io_pgtable_v2.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"AMD-Vi: \" fmt\n#define dev_fmt(fmt)\tpr_fmt(fmt)\n\n#include <linux/bitops.h>\n#include <linux/io-pgtable.h>\n#include <linux/kernel.h>\n\n#include <asm/barrier.h>\n\n#include \"amd_iommu_types.h\"\n#include \"amd_iommu.h\"\n\n#define IOMMU_PAGE_PRESENT\tBIT_ULL(0)\t \n#define IOMMU_PAGE_RW\t\tBIT_ULL(1)\t \n#define IOMMU_PAGE_USER\t\tBIT_ULL(2)\t \n#define IOMMU_PAGE_PWT\t\tBIT_ULL(3)\t \n#define IOMMU_PAGE_PCD\t\tBIT_ULL(4)\t \n#define IOMMU_PAGE_ACCESS\tBIT_ULL(5)\t \n#define IOMMU_PAGE_DIRTY\tBIT_ULL(6)\t \n#define IOMMU_PAGE_PSE\t\tBIT_ULL(7)\t \n#define IOMMU_PAGE_NX\t\tBIT_ULL(63)\t \n\n#define MAX_PTRS_PER_PAGE\t512\n\n#define IOMMU_PAGE_SIZE_2M\tBIT_ULL(21)\n#define IOMMU_PAGE_SIZE_1G\tBIT_ULL(30)\n\n\nstatic inline int get_pgtable_level(void)\n{\n\treturn amd_iommu_gpt_level;\n}\n\nstatic inline bool is_large_pte(u64 pte)\n{\n\treturn (pte & IOMMU_PAGE_PSE);\n}\n\nstatic inline u64 set_pgtable_attr(u64 *page)\n{\n\tu64 prot;\n\n\tprot = IOMMU_PAGE_PRESENT | IOMMU_PAGE_RW | IOMMU_PAGE_USER;\n\tprot |= IOMMU_PAGE_ACCESS | IOMMU_PAGE_DIRTY;\n\n\treturn (iommu_virt_to_phys(page) | prot);\n}\n\nstatic inline void *get_pgtable_pte(u64 pte)\n{\n\treturn iommu_phys_to_virt(pte & PM_ADDR_MASK);\n}\n\nstatic u64 set_pte_attr(u64 paddr, u64 pg_size, int prot)\n{\n\tu64 pte;\n\n\tpte = __sme_set(paddr & PM_ADDR_MASK);\n\tpte |= IOMMU_PAGE_PRESENT | IOMMU_PAGE_USER;\n\tpte |= IOMMU_PAGE_ACCESS | IOMMU_PAGE_DIRTY;\n\n\tif (prot & IOMMU_PROT_IW)\n\t\tpte |= IOMMU_PAGE_RW;\n\n\t \n\tif (pg_size == IOMMU_PAGE_SIZE_1G || pg_size == IOMMU_PAGE_SIZE_2M)\n\t\tpte |= IOMMU_PAGE_PSE;\n\n\treturn pte;\n}\n\nstatic inline u64 get_alloc_page_size(u64 size)\n{\n\tif (size >= IOMMU_PAGE_SIZE_1G)\n\t\treturn IOMMU_PAGE_SIZE_1G;\n\n\tif (size >= IOMMU_PAGE_SIZE_2M)\n\t\treturn IOMMU_PAGE_SIZE_2M;\n\n\treturn PAGE_SIZE;\n}\n\nstatic inline int page_size_to_level(u64 pg_size)\n{\n\tif (pg_size == IOMMU_PAGE_SIZE_1G)\n\t\treturn PAGE_MODE_3_LEVEL;\n\tif (pg_size == IOMMU_PAGE_SIZE_2M)\n\t\treturn PAGE_MODE_2_LEVEL;\n\n\treturn PAGE_MODE_1_LEVEL;\n}\n\nstatic inline void free_pgtable_page(u64 *pt)\n{\n\tfree_page((unsigned long)pt);\n}\n\nstatic void free_pgtable(u64 *pt, int level)\n{\n\tu64 *p;\n\tint i;\n\n\tfor (i = 0; i < MAX_PTRS_PER_PAGE; i++) {\n\t\t \n\t\tif (!IOMMU_PTE_PRESENT(pt[i]))\n\t\t\tcontinue;\n\n\t\tif (is_large_pte(pt[i]))\n\t\t\tcontinue;\n\n\t\t \n\t\tp = get_pgtable_pte(pt[i]);\n\t\tif (level > 2)\n\t\t\tfree_pgtable(p, level - 1);\n\t\telse\n\t\t\tfree_pgtable_page(p);\n\t}\n\n\tfree_pgtable_page(pt);\n}\n\n \nstatic u64 *v2_alloc_pte(int nid, u64 *pgd, unsigned long iova,\n\t\t\t unsigned long pg_size, gfp_t gfp, bool *updated)\n{\n\tu64 *pte, *page;\n\tint level, end_level;\n\n\tlevel = get_pgtable_level() - 1;\n\tend_level = page_size_to_level(pg_size);\n\tpte = &pgd[PM_LEVEL_INDEX(level, iova)];\n\tiova = PAGE_SIZE_ALIGN(iova, PAGE_SIZE);\n\n\twhile (level >= end_level) {\n\t\tu64 __pte, __npte;\n\n\t\t__pte = *pte;\n\n\t\tif (IOMMU_PTE_PRESENT(__pte) && is_large_pte(__pte)) {\n\t\t\t \n\t\t\tcmpxchg64(pte, *pte, 0ULL);\n\t\t\t*updated = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!IOMMU_PTE_PRESENT(__pte)) {\n\t\t\tpage = alloc_pgtable_page(nid, gfp);\n\t\t\tif (!page)\n\t\t\t\treturn NULL;\n\n\t\t\t__npte = set_pgtable_attr(page);\n\t\t\t \n\t\t\tif (cmpxchg64(pte, __pte, __npte) != __pte)\n\t\t\t\tfree_pgtable_page(page);\n\t\t\telse if (IOMMU_PTE_PRESENT(__pte))\n\t\t\t\t*updated = true;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tlevel -= 1;\n\t\tpte = get_pgtable_pte(__pte);\n\t\tpte = &pte[PM_LEVEL_INDEX(level, iova)];\n\t}\n\n\t \n\tif (IOMMU_PTE_PRESENT(*pte)) {\n\t\tu64 *__pte;\n\n\t\t*updated = true;\n\t\t__pte = get_pgtable_pte(*pte);\n\t\tcmpxchg64(pte, *pte, 0ULL);\n\t\tif (pg_size == IOMMU_PAGE_SIZE_1G)\n\t\t\tfree_pgtable(__pte, end_level - 1);\n\t\telse if (pg_size == IOMMU_PAGE_SIZE_2M)\n\t\t\tfree_pgtable_page(__pte);\n\t}\n\n\treturn pte;\n}\n\n \nstatic u64 *fetch_pte(struct amd_io_pgtable *pgtable,\n\t\t      unsigned long iova, unsigned long *page_size)\n{\n\tu64 *pte;\n\tint level;\n\n\tlevel = get_pgtable_level() - 1;\n\tpte = &pgtable->pgd[PM_LEVEL_INDEX(level, iova)];\n\t \n\t*page_size = PAGE_SIZE;\n\n\twhile (level) {\n\t\t \n\t\tif (!IOMMU_PTE_PRESENT(*pte))\n\t\t\treturn NULL;\n\n\t\t \n\t\tpte = get_pgtable_pte(*pte);\n\t\tpte = &pte[PM_LEVEL_INDEX(level - 1, iova)];\n\n\t\t \n\t\tif (is_large_pte(*pte)) {\n\t\t\tif (level == PAGE_MODE_3_LEVEL)\n\t\t\t\t*page_size = IOMMU_PAGE_SIZE_1G;\n\t\t\telse if (level == PAGE_MODE_2_LEVEL)\n\t\t\t\t*page_size = IOMMU_PAGE_SIZE_2M;\n\t\t\telse\n\t\t\t\treturn NULL;\t \n\n\t\t\tbreak;\n\t\t}\n\n\t\tlevel -= 1;\n\t}\n\n\treturn pte;\n}\n\nstatic int iommu_v2_map_pages(struct io_pgtable_ops *ops, unsigned long iova,\n\t\t\t      phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t      int prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct protection_domain *pdom = io_pgtable_ops_to_domain(ops);\n\tstruct io_pgtable_cfg *cfg = &pdom->iop.iop.cfg;\n\tu64 *pte;\n\tunsigned long map_size;\n\tunsigned long mapped_size = 0;\n\tunsigned long o_iova = iova;\n\tsize_t size = pgcount << __ffs(pgsize);\n\tint count = 0;\n\tint ret = 0;\n\tbool updated = false;\n\n\tif (WARN_ON(!pgsize || (pgsize & cfg->pgsize_bitmap) != pgsize) || !pgcount)\n\t\treturn -EINVAL;\n\n\tif (!(prot & IOMMU_PROT_MASK))\n\t\treturn -EINVAL;\n\n\twhile (mapped_size < size) {\n\t\tmap_size = get_alloc_page_size(pgsize);\n\t\tpte = v2_alloc_pte(pdom->nid, pdom->iop.pgd,\n\t\t\t\t   iova, map_size, gfp, &updated);\n\t\tif (!pte) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*pte = set_pte_attr(paddr, map_size, prot);\n\n\t\tcount++;\n\t\tiova += map_size;\n\t\tpaddr += map_size;\n\t\tmapped_size += map_size;\n\t}\n\nout:\n\tif (updated) {\n\t\tif (count > 1)\n\t\t\tamd_iommu_flush_tlb(&pdom->domain, 0);\n\t\telse\n\t\t\tamd_iommu_flush_page(&pdom->domain, 0, o_iova);\n\t}\n\n\tif (mapped)\n\t\t*mapped += mapped_size;\n\n\treturn ret;\n}\n\nstatic unsigned long iommu_v2_unmap_pages(struct io_pgtable_ops *ops,\n\t\t\t\t\t  unsigned long iova,\n\t\t\t\t\t  size_t pgsize, size_t pgcount,\n\t\t\t\t\t  struct iommu_iotlb_gather *gather)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_ops_to_data(ops);\n\tstruct io_pgtable_cfg *cfg = &pgtable->iop.cfg;\n\tunsigned long unmap_size;\n\tunsigned long unmapped = 0;\n\tsize_t size = pgcount << __ffs(pgsize);\n\tu64 *pte;\n\n\tif (WARN_ON(!pgsize || (pgsize & cfg->pgsize_bitmap) != pgsize || !pgcount))\n\t\treturn 0;\n\n\twhile (unmapped < size) {\n\t\tpte = fetch_pte(pgtable, iova, &unmap_size);\n\t\tif (!pte)\n\t\t\treturn unmapped;\n\n\t\t*pte = 0ULL;\n\n\t\tiova = (iova & ~(unmap_size - 1)) + unmap_size;\n\t\tunmapped += unmap_size;\n\t}\n\n\treturn unmapped;\n}\n\nstatic phys_addr_t iommu_v2_iova_to_phys(struct io_pgtable_ops *ops, unsigned long iova)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_ops_to_data(ops);\n\tunsigned long offset_mask, pte_pgsize;\n\tu64 *pte, __pte;\n\n\tpte = fetch_pte(pgtable, iova, &pte_pgsize);\n\tif (!pte || !IOMMU_PTE_PRESENT(*pte))\n\t\treturn 0;\n\n\toffset_mask = pte_pgsize - 1;\n\t__pte = __sme_clr(*pte & PM_ADDR_MASK);\n\n\treturn (__pte & ~offset_mask) | (iova & offset_mask);\n}\n\n \nstatic void v2_tlb_flush_all(void *cookie)\n{\n}\n\nstatic void v2_tlb_flush_walk(unsigned long iova, size_t size,\n\t\t\t      size_t granule, void *cookie)\n{\n}\n\nstatic void v2_tlb_add_page(struct iommu_iotlb_gather *gather,\n\t\t\t    unsigned long iova, size_t granule,\n\t\t\t    void *cookie)\n{\n}\n\nstatic const struct iommu_flush_ops v2_flush_ops = {\n\t.tlb_flush_all\t= v2_tlb_flush_all,\n\t.tlb_flush_walk = v2_tlb_flush_walk,\n\t.tlb_add_page\t= v2_tlb_add_page,\n};\n\nstatic void v2_free_pgtable(struct io_pgtable *iop)\n{\n\tstruct protection_domain *pdom;\n\tstruct amd_io_pgtable *pgtable = container_of(iop, struct amd_io_pgtable, iop);\n\n\tpdom = container_of(pgtable, struct protection_domain, iop);\n\tif (!(pdom->flags & PD_IOMMUV2_MASK))\n\t\treturn;\n\n\t \n\tamd_iommu_domain_update(pdom);\n\n\t \n\tfree_pgtable(pgtable->pgd, get_pgtable_level());\n}\n\nstatic struct io_pgtable *v2_alloc_pgtable(struct io_pgtable_cfg *cfg, void *cookie)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_cfg_to_data(cfg);\n\tstruct protection_domain *pdom = (struct protection_domain *)cookie;\n\tint ret;\n\tint ias = IOMMU_IN_ADDR_BIT_SIZE;\n\n\tpgtable->pgd = alloc_pgtable_page(pdom->nid, GFP_ATOMIC);\n\tif (!pgtable->pgd)\n\t\treturn NULL;\n\n\tret = amd_iommu_domain_set_gcr3(&pdom->domain, 0, iommu_virt_to_phys(pgtable->pgd));\n\tif (ret)\n\t\tgoto err_free_pgd;\n\n\tif (get_pgtable_level() == PAGE_MODE_5_LEVEL)\n\t\tias = 57;\n\n\tpgtable->iop.ops.map_pages    = iommu_v2_map_pages;\n\tpgtable->iop.ops.unmap_pages  = iommu_v2_unmap_pages;\n\tpgtable->iop.ops.iova_to_phys = iommu_v2_iova_to_phys;\n\n\tcfg->pgsize_bitmap = AMD_IOMMU_PGSIZES_V2,\n\tcfg->ias           = ias,\n\tcfg->oas           = IOMMU_OUT_ADDR_BIT_SIZE,\n\tcfg->tlb           = &v2_flush_ops;\n\n\treturn &pgtable->iop;\n\nerr_free_pgd:\n\tfree_pgtable_page(pgtable->pgd);\n\n\treturn NULL;\n}\n\nstruct io_pgtable_init_fns io_pgtable_amd_iommu_v2_init_fns = {\n\t.alloc\t= v2_alloc_pgtable,\n\t.free\t= v2_free_pgtable,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}