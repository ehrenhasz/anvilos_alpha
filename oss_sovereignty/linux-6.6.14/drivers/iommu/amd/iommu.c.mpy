{
  "module_name": "iommu.c",
  "hash_id": "ececd2b1efcbf5c62d8828d7fc355a72ba987d332d7f30e934579154cfc94edc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/amd/iommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"AMD-Vi: \" fmt\n#define dev_fmt(fmt)    pr_fmt(fmt)\n\n#include <linux/ratelimit.h>\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/pci-ats.h>\n#include <linux/bitmap.h>\n#include <linux/slab.h>\n#include <linux/debugfs.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-map-ops.h>\n#include <linux/dma-direct.h>\n#include <linux/iommu-helper.h>\n#include <linux/delay.h>\n#include <linux/amd-iommu.h>\n#include <linux/notifier.h>\n#include <linux/export.h>\n#include <linux/irq.h>\n#include <linux/msi.h>\n#include <linux/irqdomain.h>\n#include <linux/percpu.h>\n#include <linux/io-pgtable.h>\n#include <linux/cc_platform.h>\n#include <asm/irq_remapping.h>\n#include <asm/io_apic.h>\n#include <asm/apic.h>\n#include <asm/hw_irq.h>\n#include <asm/proto.h>\n#include <asm/iommu.h>\n#include <asm/gart.h>\n#include <asm/dma.h>\n\n#include \"amd_iommu.h\"\n#include \"../dma-iommu.h\"\n#include \"../irq_remapping.h\"\n\n#define CMD_SET_TYPE(cmd, t) ((cmd)->data[1] |= ((t) << 28))\n\n#define LOOP_TIMEOUT\t100000\n\n \n#define IOVA_START_PFN\t\t(1)\n#define IOVA_PFN(addr)\t\t((addr) >> PAGE_SHIFT)\n\n \n#define MSI_RANGE_START\t\t(0xfee00000)\n#define MSI_RANGE_END\t\t(0xfeefffff)\n#define HT_RANGE_START\t\t(0xfd00000000ULL)\n#define HT_RANGE_END\t\t(0xffffffffffULL)\n\n#define DEFAULT_PGTABLE_LEVEL\tPAGE_MODE_3_LEVEL\n\nstatic DEFINE_SPINLOCK(pd_bitmap_lock);\n\nLIST_HEAD(ioapic_map);\nLIST_HEAD(hpet_map);\nLIST_HEAD(acpihid_map);\n\nconst struct iommu_ops amd_iommu_ops;\n\nstatic ATOMIC_NOTIFIER_HEAD(ppr_notifier);\nint amd_iommu_max_glx_val = -1;\n\n \nstruct iommu_cmd {\n\tu32 data[4];\n};\n\nstruct kmem_cache *amd_iommu_irq_cache;\n\nstatic void detach_device(struct device *dev);\nstatic int domain_enable_v2(struct protection_domain *domain, int pasids);\n\n \n\nstatic inline int get_acpihid_device_id(struct device *dev,\n\t\t\t\t\tstruct acpihid_map_entry **entry)\n{\n\tstruct acpi_device *adev = ACPI_COMPANION(dev);\n\tstruct acpihid_map_entry *p;\n\n\tif (!adev)\n\t\treturn -ENODEV;\n\n\tlist_for_each_entry(p, &acpihid_map, list) {\n\t\tif (acpi_dev_hid_uid_match(adev, p->hid,\n\t\t\t\t\t   p->uid[0] ? p->uid : NULL)) {\n\t\t\tif (entry)\n\t\t\t\t*entry = p;\n\t\t\treturn p->devid;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic inline int get_device_sbdf_id(struct device *dev)\n{\n\tint sbdf;\n\n\tif (dev_is_pci(dev))\n\t\tsbdf = get_pci_sbdf_id(to_pci_dev(dev));\n\telse\n\t\tsbdf = get_acpihid_device_id(dev, NULL);\n\n\treturn sbdf;\n}\n\nstruct dev_table_entry *get_dev_table(struct amd_iommu *iommu)\n{\n\tstruct dev_table_entry *dev_table;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tBUG_ON(pci_seg == NULL);\n\tdev_table = pci_seg->dev_table;\n\tBUG_ON(dev_table == NULL);\n\n\treturn dev_table;\n}\n\nstatic inline u16 get_device_segment(struct device *dev)\n{\n\tu16 seg;\n\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\n\t\tseg = pci_domain_nr(pdev->bus);\n\t} else {\n\t\tu32 devid = get_acpihid_device_id(dev, NULL);\n\n\t\tseg = PCI_SBDF_TO_SEGID(devid);\n\t}\n\n\treturn seg;\n}\n\n \nvoid amd_iommu_set_rlookup_table(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tpci_seg->rlookup_table[devid] = iommu;\n}\n\nstatic struct amd_iommu *__rlookup_amd_iommu(u16 seg, u16 devid)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\n\tfor_each_pci_segment(pci_seg) {\n\t\tif (pci_seg->id == seg)\n\t\t\treturn pci_seg->rlookup_table[devid];\n\t}\n\treturn NULL;\n}\n\nstatic struct amd_iommu *rlookup_amd_iommu(struct device *dev)\n{\n\tu16 seg = get_device_segment(dev);\n\tint devid = get_device_sbdf_id(dev);\n\n\tif (devid < 0)\n\t\treturn NULL;\n\treturn __rlookup_amd_iommu(seg, PCI_SBDF_TO_DEVID(devid));\n}\n\nstatic struct protection_domain *to_pdomain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct protection_domain, domain);\n}\n\nstatic struct iommu_dev_data *alloc_dev_data(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tdev_data = kzalloc(sizeof(*dev_data), GFP_KERNEL);\n\tif (!dev_data)\n\t\treturn NULL;\n\n\tspin_lock_init(&dev_data->lock);\n\tdev_data->devid = devid;\n\tratelimit_default_init(&dev_data->rs);\n\n\tllist_add(&dev_data->dev_data_list, &pci_seg->dev_data_list);\n\treturn dev_data;\n}\n\nstatic struct iommu_dev_data *search_dev_data(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct llist_node *node;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tif (llist_empty(&pci_seg->dev_data_list))\n\t\treturn NULL;\n\n\tnode = pci_seg->dev_data_list.first;\n\tllist_for_each_entry(dev_data, node, dev_data_list) {\n\t\tif (dev_data->devid == devid)\n\t\t\treturn dev_data;\n\t}\n\n\treturn NULL;\n}\n\nstatic int clone_alias(struct pci_dev *pdev, u16 alias, void *data)\n{\n\tstruct amd_iommu *iommu;\n\tstruct dev_table_entry *dev_table;\n\tu16 devid = pci_dev_id(pdev);\n\n\tif (devid == alias)\n\t\treturn 0;\n\n\tiommu = rlookup_amd_iommu(&pdev->dev);\n\tif (!iommu)\n\t\treturn 0;\n\n\tamd_iommu_set_rlookup_table(iommu, alias);\n\tdev_table = get_dev_table(iommu);\n\tmemcpy(dev_table[alias].data,\n\t       dev_table[devid].data,\n\t       sizeof(dev_table[alias].data));\n\n\treturn 0;\n}\n\nstatic void clone_aliases(struct amd_iommu *iommu, struct device *dev)\n{\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(dev))\n\t\treturn;\n\tpdev = to_pci_dev(dev);\n\n\t \n\tclone_alias(pdev, iommu->pci_seg->alias_table[pci_dev_id(pdev)], NULL);\n\n\tpci_for_each_dma_alias(pdev, clone_alias, NULL);\n}\n\nstatic void setup_aliases(struct amd_iommu *iommu, struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\tu16 ivrs_alias;\n\n\t \n\tif (!dev_is_pci(dev))\n\t\treturn;\n\n\t \n\tivrs_alias = pci_seg->alias_table[pci_dev_id(pdev)];\n\tif (ivrs_alias != pci_dev_id(pdev) &&\n\t    PCI_BUS_NUM(ivrs_alias) == pdev->bus->number)\n\t\tpci_add_dma_alias(pdev, ivrs_alias & 0xff, 1);\n\n\tclone_aliases(iommu, dev);\n}\n\nstatic struct iommu_dev_data *find_dev_data(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct iommu_dev_data *dev_data;\n\n\tdev_data = search_dev_data(iommu, devid);\n\n\tif (dev_data == NULL) {\n\t\tdev_data = alloc_dev_data(iommu, devid);\n\t\tif (!dev_data)\n\t\t\treturn NULL;\n\n\t\tif (translation_pre_enabled(iommu))\n\t\t\tdev_data->defer_attach = true;\n\t}\n\n\treturn dev_data;\n}\n\n \nstatic struct iommu_group *acpihid_device_group(struct device *dev)\n{\n\tstruct acpihid_map_entry *p, *entry = NULL;\n\tint devid;\n\n\tdevid = get_acpihid_device_id(dev, &entry);\n\tif (devid < 0)\n\t\treturn ERR_PTR(devid);\n\n\tlist_for_each_entry(p, &acpihid_map, list) {\n\t\tif ((devid == p->devid) && p->group)\n\t\t\tentry->group = p->group;\n\t}\n\n\tif (!entry->group)\n\t\tentry->group = generic_device_group(dev);\n\telse\n\t\tiommu_group_ref_get(entry->group);\n\n\treturn entry->group;\n}\n\nstatic bool pci_iommuv2_capable(struct pci_dev *pdev)\n{\n\tstatic const int caps[] = {\n\t\tPCI_EXT_CAP_ID_PRI,\n\t\tPCI_EXT_CAP_ID_PASID,\n\t};\n\tint i, pos;\n\n\tif (!pci_ats_supported(pdev))\n\t\treturn false;\n\n\tfor (i = 0; i < 2; ++i) {\n\t\tpos = pci_find_ext_capability(pdev, caps[i]);\n\t\tif (pos == 0)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool check_device(struct device *dev)\n{\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tstruct amd_iommu *iommu;\n\tint devid, sbdf;\n\n\tif (!dev)\n\t\treturn false;\n\n\tsbdf = get_device_sbdf_id(dev);\n\tif (sbdf < 0)\n\t\treturn false;\n\tdevid = PCI_SBDF_TO_DEVID(sbdf);\n\n\tiommu = rlookup_amd_iommu(dev);\n\tif (!iommu)\n\t\treturn false;\n\n\t \n\tpci_seg = iommu->pci_seg;\n\tif (devid > pci_seg->last_bdf)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int iommu_init_device(struct amd_iommu *iommu, struct device *dev)\n{\n\tstruct iommu_dev_data *dev_data;\n\tint devid, sbdf;\n\n\tif (dev_iommu_priv_get(dev))\n\t\treturn 0;\n\n\tsbdf = get_device_sbdf_id(dev);\n\tif (sbdf < 0)\n\t\treturn sbdf;\n\n\tdevid = PCI_SBDF_TO_DEVID(sbdf);\n\tdev_data = find_dev_data(iommu, devid);\n\tif (!dev_data)\n\t\treturn -ENOMEM;\n\n\tdev_data->dev = dev;\n\tsetup_aliases(iommu, dev);\n\n\t \n\tif ((iommu_default_passthrough() || !amd_iommu_force_isolation) &&\n\t    dev_is_pci(dev) && pci_iommuv2_capable(to_pci_dev(dev))) {\n\t\tdev_data->iommu_v2 = iommu->is_iommu_v2;\n\t}\n\n\tdev_iommu_priv_set(dev, dev_data);\n\n\treturn 0;\n}\n\nstatic void iommu_ignore_device(struct amd_iommu *iommu, struct device *dev)\n{\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\tint devid, sbdf;\n\n\tsbdf = get_device_sbdf_id(dev);\n\tif (sbdf < 0)\n\t\treturn;\n\n\tdevid = PCI_SBDF_TO_DEVID(sbdf);\n\tpci_seg->rlookup_table[devid] = NULL;\n\tmemset(&dev_table[devid], 0, sizeof(struct dev_table_entry));\n\n\tsetup_aliases(iommu, dev);\n}\n\nstatic void amd_iommu_uninit_device(struct device *dev)\n{\n\tstruct iommu_dev_data *dev_data;\n\n\tdev_data = dev_iommu_priv_get(dev);\n\tif (!dev_data)\n\t\treturn;\n\n\tif (dev_data->domain)\n\t\tdetach_device(dev);\n\n\tdev_iommu_priv_set(dev, NULL);\n\n\t \n}\n\n \n\nstatic void dump_dte_entry(struct amd_iommu *iommu, u16 devid)\n{\n\tint i;\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\tfor (i = 0; i < 4; ++i)\n\t\tpr_err(\"DTE[%d]: %016llx\\n\", i, dev_table[devid].data[i]);\n}\n\nstatic void dump_command(unsigned long phys_addr)\n{\n\tstruct iommu_cmd *cmd = iommu_phys_to_virt(phys_addr);\n\tint i;\n\n\tfor (i = 0; i < 4; ++i)\n\t\tpr_err(\"CMD[%d]: %08x\\n\", i, cmd->data[i]);\n}\n\nstatic void amd_iommu_report_rmp_hw_error(struct amd_iommu *iommu, volatile u32 *event)\n{\n\tstruct iommu_dev_data *dev_data = NULL;\n\tint devid, vmg_tag, flags;\n\tstruct pci_dev *pdev;\n\tu64 spa;\n\n\tdevid   = (event[0] >> EVENT_DEVID_SHIFT) & EVENT_DEVID_MASK;\n\tvmg_tag = (event[1]) & 0xFFFF;\n\tflags   = (event[1] >> EVENT_FLAGS_SHIFT) & EVENT_FLAGS_MASK;\n\tspa     = ((u64)event[3] << 32) | (event[2] & 0xFFFFFFF8);\n\n\tpdev = pci_get_domain_bus_and_slot(iommu->pci_seg->id, PCI_BUS_NUM(devid),\n\t\t\t\t\t   devid & 0xff);\n\tif (pdev)\n\t\tdev_data = dev_iommu_priv_get(&pdev->dev);\n\n\tif (dev_data) {\n\t\tif (__ratelimit(&dev_data->rs)) {\n\t\t\tpci_err(pdev, \"Event logged [RMP_HW_ERROR vmg_tag=0x%04x, spa=0x%llx, flags=0x%04x]\\n\",\n\t\t\t\tvmg_tag, spa, flags);\n\t\t}\n\t} else {\n\t\tpr_err_ratelimited(\"Event logged [RMP_HW_ERROR device=%04x:%02x:%02x.%x, vmg_tag=0x%04x, spa=0x%llx, flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tvmg_tag, spa, flags);\n\t}\n\n\tif (pdev)\n\t\tpci_dev_put(pdev);\n}\n\nstatic void amd_iommu_report_rmp_fault(struct amd_iommu *iommu, volatile u32 *event)\n{\n\tstruct iommu_dev_data *dev_data = NULL;\n\tint devid, flags_rmp, vmg_tag, flags;\n\tstruct pci_dev *pdev;\n\tu64 gpa;\n\n\tdevid     = (event[0] >> EVENT_DEVID_SHIFT) & EVENT_DEVID_MASK;\n\tflags_rmp = (event[0] >> EVENT_FLAGS_SHIFT) & 0xFF;\n\tvmg_tag   = (event[1]) & 0xFFFF;\n\tflags     = (event[1] >> EVENT_FLAGS_SHIFT) & EVENT_FLAGS_MASK;\n\tgpa       = ((u64)event[3] << 32) | event[2];\n\n\tpdev = pci_get_domain_bus_and_slot(iommu->pci_seg->id, PCI_BUS_NUM(devid),\n\t\t\t\t\t   devid & 0xff);\n\tif (pdev)\n\t\tdev_data = dev_iommu_priv_get(&pdev->dev);\n\n\tif (dev_data) {\n\t\tif (__ratelimit(&dev_data->rs)) {\n\t\t\tpci_err(pdev, \"Event logged [RMP_PAGE_FAULT vmg_tag=0x%04x, gpa=0x%llx, flags_rmp=0x%04x, flags=0x%04x]\\n\",\n\t\t\t\tvmg_tag, gpa, flags_rmp, flags);\n\t\t}\n\t} else {\n\t\tpr_err_ratelimited(\"Event logged [RMP_PAGE_FAULT device=%04x:%02x:%02x.%x, vmg_tag=0x%04x, gpa=0x%llx, flags_rmp=0x%04x, flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tvmg_tag, gpa, flags_rmp, flags);\n\t}\n\n\tif (pdev)\n\t\tpci_dev_put(pdev);\n}\n\n#define IS_IOMMU_MEM_TRANSACTION(flags)\t\t\\\n\t(((flags) & EVENT_FLAG_I) == 0)\n\n#define IS_WRITE_REQUEST(flags)\t\t\t\\\n\t((flags) & EVENT_FLAG_RW)\n\nstatic void amd_iommu_report_page_fault(struct amd_iommu *iommu,\n\t\t\t\t\tu16 devid, u16 domain_id,\n\t\t\t\t\tu64 address, int flags)\n{\n\tstruct iommu_dev_data *dev_data = NULL;\n\tstruct pci_dev *pdev;\n\n\tpdev = pci_get_domain_bus_and_slot(iommu->pci_seg->id, PCI_BUS_NUM(devid),\n\t\t\t\t\t   devid & 0xff);\n\tif (pdev)\n\t\tdev_data = dev_iommu_priv_get(&pdev->dev);\n\n\tif (dev_data) {\n\t\t \n\t\tif (IS_IOMMU_MEM_TRANSACTION(flags)) {\n\t\t\t \n\t\t\tif (dev_data->domain == NULL) {\n\t\t\t\tpr_err_ratelimited(\"Event logged [Device not attached to domain properly]\\n\");\n\t\t\t\tpr_err_ratelimited(\"  device=%04x:%02x:%02x.%x domain=0x%04x\\n\",\n\t\t\t\t\t\t   iommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid),\n\t\t\t\t\t\t   PCI_FUNC(devid), domain_id);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (!report_iommu_fault(&dev_data->domain->domain,\n\t\t\t\t\t\t&pdev->dev, address,\n\t\t\t\t\t\tIS_WRITE_REQUEST(flags) ?\n\t\t\t\t\t\t\tIOMMU_FAULT_WRITE :\n\t\t\t\t\t\t\tIOMMU_FAULT_READ))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (__ratelimit(&dev_data->rs)) {\n\t\t\tpci_err(pdev, \"Event logged [IO_PAGE_FAULT domain=0x%04x address=0x%llx flags=0x%04x]\\n\",\n\t\t\t\tdomain_id, address, flags);\n\t\t}\n\t} else {\n\t\tpr_err_ratelimited(\"Event logged [IO_PAGE_FAULT device=%04x:%02x:%02x.%x domain=0x%04x address=0x%llx flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tdomain_id, address, flags);\n\t}\n\nout:\n\tif (pdev)\n\t\tpci_dev_put(pdev);\n}\n\nstatic void iommu_print_event(struct amd_iommu *iommu, void *__evt)\n{\n\tstruct device *dev = iommu->iommu.dev;\n\tint type, devid, flags, tag;\n\tvolatile u32 *event = __evt;\n\tint count = 0;\n\tu64 address;\n\tu32 pasid;\n\nretry:\n\ttype    = (event[1] >> EVENT_TYPE_SHIFT)  & EVENT_TYPE_MASK;\n\tdevid   = (event[0] >> EVENT_DEVID_SHIFT) & EVENT_DEVID_MASK;\n\tpasid   = (event[0] & EVENT_DOMID_MASK_HI) |\n\t\t  (event[1] & EVENT_DOMID_MASK_LO);\n\tflags   = (event[1] >> EVENT_FLAGS_SHIFT) & EVENT_FLAGS_MASK;\n\taddress = (u64)(((u64)event[3]) << 32) | event[2];\n\n\tif (type == 0) {\n\t\t \n\t\tif (++count == LOOP_TIMEOUT) {\n\t\t\tpr_err(\"No event written to event log\\n\");\n\t\t\treturn;\n\t\t}\n\t\tudelay(1);\n\t\tgoto retry;\n\t}\n\n\tif (type == EVENT_TYPE_IO_FAULT) {\n\t\tamd_iommu_report_page_fault(iommu, devid, pasid, address, flags);\n\t\treturn;\n\t}\n\n\tswitch (type) {\n\tcase EVENT_TYPE_ILL_DEV:\n\t\tdev_err(dev, \"Event logged [ILLEGAL_DEV_TABLE_ENTRY device=%04x:%02x:%02x.%x pasid=0x%05x address=0x%llx flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tpasid, address, flags);\n\t\tdump_dte_entry(iommu, devid);\n\t\tbreak;\n\tcase EVENT_TYPE_DEV_TAB_ERR:\n\t\tdev_err(dev, \"Event logged [DEV_TAB_HARDWARE_ERROR device=%04x:%02x:%02x.%x \"\n\t\t\t\"address=0x%llx flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\taddress, flags);\n\t\tbreak;\n\tcase EVENT_TYPE_PAGE_TAB_ERR:\n\t\tdev_err(dev, \"Event logged [PAGE_TAB_HARDWARE_ERROR device=%04x:%02x:%02x.%x pasid=0x%04x address=0x%llx flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tpasid, address, flags);\n\t\tbreak;\n\tcase EVENT_TYPE_ILL_CMD:\n\t\tdev_err(dev, \"Event logged [ILLEGAL_COMMAND_ERROR address=0x%llx]\\n\", address);\n\t\tdump_command(address);\n\t\tbreak;\n\tcase EVENT_TYPE_CMD_HARD_ERR:\n\t\tdev_err(dev, \"Event logged [COMMAND_HARDWARE_ERROR address=0x%llx flags=0x%04x]\\n\",\n\t\t\taddress, flags);\n\t\tbreak;\n\tcase EVENT_TYPE_IOTLB_INV_TO:\n\t\tdev_err(dev, \"Event logged [IOTLB_INV_TIMEOUT device=%04x:%02x:%02x.%x address=0x%llx]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\taddress);\n\t\tbreak;\n\tcase EVENT_TYPE_INV_DEV_REQ:\n\t\tdev_err(dev, \"Event logged [INVALID_DEVICE_REQUEST device=%04x:%02x:%02x.%x pasid=0x%05x address=0x%llx flags=0x%04x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tpasid, address, flags);\n\t\tbreak;\n\tcase EVENT_TYPE_RMP_FAULT:\n\t\tamd_iommu_report_rmp_fault(iommu, event);\n\t\tbreak;\n\tcase EVENT_TYPE_RMP_HW_ERR:\n\t\tamd_iommu_report_rmp_hw_error(iommu, event);\n\t\tbreak;\n\tcase EVENT_TYPE_INV_PPR_REQ:\n\t\tpasid = PPR_PASID(*((u64 *)__evt));\n\t\ttag = event[1] & 0x03FF;\n\t\tdev_err(dev, \"Event logged [INVALID_PPR_REQUEST device=%04x:%02x:%02x.%x pasid=0x%05x address=0x%llx flags=0x%04x tag=0x%03x]\\n\",\n\t\t\tiommu->pci_seg->id, PCI_BUS_NUM(devid), PCI_SLOT(devid), PCI_FUNC(devid),\n\t\t\tpasid, address, flags, tag);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Event logged [UNKNOWN event[0]=0x%08x event[1]=0x%08x event[2]=0x%08x event[3]=0x%08x\\n\",\n\t\t\tevent[0], event[1], event[2], event[3]);\n\t}\n\n\t \n\tif (!amd_iommu_snp_en)\n\t\tmemset(__evt, 0, 4 * sizeof(u32));\n}\n\nstatic void iommu_poll_events(struct amd_iommu *iommu)\n{\n\tu32 head, tail;\n\n\thead = readl(iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);\n\ttail = readl(iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);\n\n\twhile (head != tail) {\n\t\tiommu_print_event(iommu, iommu->evt_buf + head);\n\t\thead = (head + EVENT_ENTRY_SIZE) % EVT_BUFFER_SIZE;\n\t}\n\n\twritel(head, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);\n}\n\nstatic void iommu_handle_ppr_entry(struct amd_iommu *iommu, u64 *raw)\n{\n\tstruct amd_iommu_fault fault;\n\n\tif (PPR_REQ_TYPE(raw[0]) != PPR_REQ_FAULT) {\n\t\tpr_err_ratelimited(\"Unknown PPR request received\\n\");\n\t\treturn;\n\t}\n\n\tfault.address   = raw[1];\n\tfault.pasid     = PPR_PASID(raw[0]);\n\tfault.sbdf      = PCI_SEG_DEVID_TO_SBDF(iommu->pci_seg->id, PPR_DEVID(raw[0]));\n\tfault.tag       = PPR_TAG(raw[0]);\n\tfault.flags     = PPR_FLAGS(raw[0]);\n\n\tatomic_notifier_call_chain(&ppr_notifier, 0, &fault);\n}\n\nstatic void iommu_poll_ppr_log(struct amd_iommu *iommu)\n{\n\tu32 head, tail;\n\n\tif (iommu->ppr_log == NULL)\n\t\treturn;\n\n\thead = readl(iommu->mmio_base + MMIO_PPR_HEAD_OFFSET);\n\ttail = readl(iommu->mmio_base + MMIO_PPR_TAIL_OFFSET);\n\n\twhile (head != tail) {\n\t\tvolatile u64 *raw;\n\t\tu64 entry[2];\n\t\tint i;\n\n\t\traw = (u64 *)(iommu->ppr_log + head);\n\n\t\t \n\t\tfor (i = 0; i < LOOP_TIMEOUT; ++i) {\n\t\t\tif (PPR_REQ_TYPE(raw[0]) != 0)\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t\t \n\t\tentry[0] = raw[0];\n\t\tentry[1] = raw[1];\n\n\t\t \n\t\tif (!amd_iommu_snp_en)\n\t\t\traw[0] = raw[1] = 0UL;\n\n\t\t \n\t\thead = (head + PPR_ENTRY_SIZE) % PPR_LOG_SIZE;\n\t\twritel(head, iommu->mmio_base + MMIO_PPR_HEAD_OFFSET);\n\n\t\t \n\t\tiommu_handle_ppr_entry(iommu, entry);\n\n\t\t \n\t\thead = readl(iommu->mmio_base + MMIO_PPR_HEAD_OFFSET);\n\t\ttail = readl(iommu->mmio_base + MMIO_PPR_TAIL_OFFSET);\n\t}\n}\n\n#ifdef CONFIG_IRQ_REMAP\nstatic int (*iommu_ga_log_notifier)(u32);\n\nint amd_iommu_register_ga_log_notifier(int (*notifier)(u32))\n{\n\tiommu_ga_log_notifier = notifier;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(amd_iommu_register_ga_log_notifier);\n\nstatic void iommu_poll_ga_log(struct amd_iommu *iommu)\n{\n\tu32 head, tail;\n\n\tif (iommu->ga_log == NULL)\n\t\treturn;\n\n\thead = readl(iommu->mmio_base + MMIO_GA_HEAD_OFFSET);\n\ttail = readl(iommu->mmio_base + MMIO_GA_TAIL_OFFSET);\n\n\twhile (head != tail) {\n\t\tvolatile u64 *raw;\n\t\tu64 log_entry;\n\n\t\traw = (u64 *)(iommu->ga_log + head);\n\n\t\t \n\t\tlog_entry = *raw;\n\n\t\t \n\t\thead = (head + GA_ENTRY_SIZE) % GA_LOG_SIZE;\n\t\twritel(head, iommu->mmio_base + MMIO_GA_HEAD_OFFSET);\n\n\t\t \n\t\tswitch (GA_REQ_TYPE(log_entry)) {\n\t\tcase GA_GUEST_NR:\n\t\t\tif (!iommu_ga_log_notifier)\n\t\t\t\tbreak;\n\n\t\t\tpr_debug(\"%s: devid=%#x, ga_tag=%#x\\n\",\n\t\t\t\t __func__, GA_DEVID(log_entry),\n\t\t\t\t GA_TAG(log_entry));\n\n\t\t\tif (iommu_ga_log_notifier(GA_TAG(log_entry)) != 0)\n\t\t\t\tpr_err(\"GA log notifier failed.\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void\namd_iommu_set_pci_msi_domain(struct device *dev, struct amd_iommu *iommu)\n{\n\tif (!irq_remapping_enabled || !dev_is_pci(dev) ||\n\t    !pci_dev_has_default_msi_parent_domain(to_pci_dev(dev)))\n\t\treturn;\n\n\tdev_set_msi_domain(dev, iommu->ir_domain);\n}\n\n#else  \nstatic inline void\namd_iommu_set_pci_msi_domain(struct device *dev, struct amd_iommu *iommu) { }\n#endif  \n\nstatic void amd_iommu_handle_irq(void *data, const char *evt_type,\n\t\t\t\t u32 int_mask, u32 overflow_mask,\n\t\t\t\t void (*int_handler)(struct amd_iommu *),\n\t\t\t\t void (*overflow_handler)(struct amd_iommu *))\n{\n\tstruct amd_iommu *iommu = (struct amd_iommu *) data;\n\tu32 status = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\tu32 mask = int_mask | overflow_mask;\n\n\twhile (status & mask) {\n\t\t \n\t\twritel(mask, iommu->mmio_base + MMIO_STATUS_OFFSET);\n\n\t\tif (int_handler) {\n\t\t\tpr_devel(\"Processing IOMMU (ivhd%d) %s Log\\n\",\n\t\t\t\t iommu->index, evt_type);\n\t\t\tint_handler(iommu);\n\t\t}\n\n\t\tif ((status & overflow_mask) && overflow_handler)\n\t\t\toverflow_handler(iommu);\n\n\t\t \n\t\tstatus = readl(iommu->mmio_base + MMIO_STATUS_OFFSET);\n\t}\n}\n\nirqreturn_t amd_iommu_int_thread_evtlog(int irq, void *data)\n{\n\tamd_iommu_handle_irq(data, \"Evt\", MMIO_STATUS_EVT_INT_MASK,\n\t\t\t     MMIO_STATUS_EVT_OVERFLOW_MASK,\n\t\t\t     iommu_poll_events, amd_iommu_restart_event_logging);\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t amd_iommu_int_thread_pprlog(int irq, void *data)\n{\n\tamd_iommu_handle_irq(data, \"PPR\", MMIO_STATUS_PPR_INT_MASK,\n\t\t\t     MMIO_STATUS_PPR_OVERFLOW_MASK,\n\t\t\t     iommu_poll_ppr_log, amd_iommu_restart_ppr_log);\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t amd_iommu_int_thread_galog(int irq, void *data)\n{\n#ifdef CONFIG_IRQ_REMAP\n\tamd_iommu_handle_irq(data, \"GA\", MMIO_STATUS_GALOG_INT_MASK,\n\t\t\t     MMIO_STATUS_GALOG_OVERFLOW_MASK,\n\t\t\t     iommu_poll_ga_log, amd_iommu_restart_ga_log);\n#endif\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t amd_iommu_int_thread(int irq, void *data)\n{\n\tamd_iommu_int_thread_evtlog(irq, data);\n\tamd_iommu_int_thread_pprlog(irq, data);\n\tamd_iommu_int_thread_galog(irq, data);\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t amd_iommu_int_handler(int irq, void *data)\n{\n\treturn IRQ_WAKE_THREAD;\n}\n\n \n\nstatic int wait_on_sem(struct amd_iommu *iommu, u64 data)\n{\n\tint i = 0;\n\n\twhile (*iommu->cmd_sem != data && i < LOOP_TIMEOUT) {\n\t\tudelay(1);\n\t\ti += 1;\n\t}\n\n\tif (i == LOOP_TIMEOUT) {\n\t\tpr_alert(\"Completion-Wait loop timed out\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void copy_cmd_to_buffer(struct amd_iommu *iommu,\n\t\t\t       struct iommu_cmd *cmd)\n{\n\tu8 *target;\n\tu32 tail;\n\n\t \n\ttail = iommu->cmd_buf_tail;\n\ttarget = iommu->cmd_buf + tail;\n\tmemcpy(target, cmd, sizeof(*cmd));\n\n\ttail = (tail + sizeof(*cmd)) % CMD_BUFFER_SIZE;\n\tiommu->cmd_buf_tail = tail;\n\n\t \n\twritel(tail, iommu->mmio_base + MMIO_CMD_TAIL_OFFSET);\n}\n\nstatic void build_completion_wait(struct iommu_cmd *cmd,\n\t\t\t\t  struct amd_iommu *iommu,\n\t\t\t\t  u64 data)\n{\n\tu64 paddr = iommu_virt_to_phys((void *)iommu->cmd_sem);\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->data[0] = lower_32_bits(paddr) | CMD_COMPL_WAIT_STORE_MASK;\n\tcmd->data[1] = upper_32_bits(paddr);\n\tcmd->data[2] = lower_32_bits(data);\n\tcmd->data[3] = upper_32_bits(data);\n\tCMD_SET_TYPE(cmd, CMD_COMPL_WAIT);\n}\n\nstatic void build_inv_dte(struct iommu_cmd *cmd, u16 devid)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->data[0] = devid;\n\tCMD_SET_TYPE(cmd, CMD_INV_DEV_ENTRY);\n}\n\n \nstatic inline u64 build_inv_address(u64 address, size_t size)\n{\n\tu64 pages, end, msb_diff;\n\n\tpages = iommu_num_pages(address, size, PAGE_SIZE);\n\n\tif (pages == 1)\n\t\treturn address & PAGE_MASK;\n\n\tend = address + size - 1;\n\n\t \n\tmsb_diff = fls64(end ^ address) - 1;\n\n\t \n\tif (unlikely(msb_diff > 51)) {\n\t\taddress = CMD_INV_IOMMU_ALL_PAGES_ADDRESS;\n\t} else {\n\t\t \n\t\taddress |= (1ull << msb_diff) - 1;\n\t}\n\n\t \n\taddress &= PAGE_MASK;\n\n\t \n\treturn address | CMD_INV_IOMMU_PAGES_SIZE_MASK;\n}\n\nstatic void build_inv_iommu_pages(struct iommu_cmd *cmd, u64 address,\n\t\t\t\t  size_t size, u16 domid, int pde)\n{\n\tu64 inv_address = build_inv_address(address, size);\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->data[1] |= domid;\n\tcmd->data[2]  = lower_32_bits(inv_address);\n\tcmd->data[3]  = upper_32_bits(inv_address);\n\tCMD_SET_TYPE(cmd, CMD_INV_IOMMU_PAGES);\n\tif (pde)  \n\t\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_PDE_MASK;\n}\n\nstatic void build_inv_iotlb_pages(struct iommu_cmd *cmd, u16 devid, int qdep,\n\t\t\t\t  u64 address, size_t size)\n{\n\tu64 inv_address = build_inv_address(address, size);\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->data[0]  = devid;\n\tcmd->data[0] |= (qdep & 0xff) << 24;\n\tcmd->data[1]  = devid;\n\tcmd->data[2]  = lower_32_bits(inv_address);\n\tcmd->data[3]  = upper_32_bits(inv_address);\n\tCMD_SET_TYPE(cmd, CMD_INV_IOTLB_PAGES);\n}\n\nstatic void build_inv_iommu_pasid(struct iommu_cmd *cmd, u16 domid, u32 pasid,\n\t\t\t\t  u64 address, bool size)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\n\taddress &= ~(0xfffULL);\n\n\tcmd->data[0]  = pasid;\n\tcmd->data[1]  = domid;\n\tcmd->data[2]  = lower_32_bits(address);\n\tcmd->data[3]  = upper_32_bits(address);\n\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_PDE_MASK;\n\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_GN_MASK;\n\tif (size)\n\t\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_SIZE_MASK;\n\tCMD_SET_TYPE(cmd, CMD_INV_IOMMU_PAGES);\n}\n\nstatic void build_inv_iotlb_pasid(struct iommu_cmd *cmd, u16 devid, u32 pasid,\n\t\t\t\t  int qdep, u64 address, bool size)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\n\taddress &= ~(0xfffULL);\n\n\tcmd->data[0]  = devid;\n\tcmd->data[0] |= ((pasid >> 8) & 0xff) << 16;\n\tcmd->data[0] |= (qdep  & 0xff) << 24;\n\tcmd->data[1]  = devid;\n\tcmd->data[1] |= (pasid & 0xff) << 16;\n\tcmd->data[2]  = lower_32_bits(address);\n\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_GN_MASK;\n\tcmd->data[3]  = upper_32_bits(address);\n\tif (size)\n\t\tcmd->data[2] |= CMD_INV_IOMMU_PAGES_SIZE_MASK;\n\tCMD_SET_TYPE(cmd, CMD_INV_IOTLB_PAGES);\n}\n\nstatic void build_complete_ppr(struct iommu_cmd *cmd, u16 devid, u32 pasid,\n\t\t\t       int status, int tag, bool gn)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\n\tcmd->data[0]  = devid;\n\tif (gn) {\n\t\tcmd->data[1]  = pasid;\n\t\tcmd->data[2]  = CMD_INV_IOMMU_PAGES_GN_MASK;\n\t}\n\tcmd->data[3]  = tag & 0x1ff;\n\tcmd->data[3] |= (status & PPR_STATUS_MASK) << PPR_STATUS_SHIFT;\n\n\tCMD_SET_TYPE(cmd, CMD_COMPLETE_PPR);\n}\n\nstatic void build_inv_all(struct iommu_cmd *cmd)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\tCMD_SET_TYPE(cmd, CMD_INV_ALL);\n}\n\nstatic void build_inv_irt(struct iommu_cmd *cmd, u16 devid)\n{\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->data[0] = devid;\n\tCMD_SET_TYPE(cmd, CMD_INV_IRT);\n}\n\n \nstatic int __iommu_queue_command_sync(struct amd_iommu *iommu,\n\t\t\t\t      struct iommu_cmd *cmd,\n\t\t\t\t      bool sync)\n{\n\tunsigned int count = 0;\n\tu32 left, next_tail;\n\n\tnext_tail = (iommu->cmd_buf_tail + sizeof(*cmd)) % CMD_BUFFER_SIZE;\nagain:\n\tleft      = (iommu->cmd_buf_head - next_tail) % CMD_BUFFER_SIZE;\n\n\tif (left <= 0x20) {\n\t\t \n\t\tif (count++) {\n\t\t\tif (count == LOOP_TIMEOUT) {\n\t\t\t\tpr_err(\"Command buffer timeout\\n\");\n\t\t\t\treturn -EIO;\n\t\t\t}\n\n\t\t\tudelay(1);\n\t\t}\n\n\t\t \n\t\tiommu->cmd_buf_head = readl(iommu->mmio_base +\n\t\t\t\t\t    MMIO_CMD_HEAD_OFFSET);\n\n\t\tgoto again;\n\t}\n\n\tcopy_cmd_to_buffer(iommu, cmd);\n\n\t \n\tiommu->need_sync = sync;\n\n\treturn 0;\n}\n\nstatic int iommu_queue_command_sync(struct amd_iommu *iommu,\n\t\t\t\t    struct iommu_cmd *cmd,\n\t\t\t\t    bool sync)\n{\n\tunsigned long flags;\n\tint ret;\n\n\traw_spin_lock_irqsave(&iommu->lock, flags);\n\tret = __iommu_queue_command_sync(iommu, cmd, sync);\n\traw_spin_unlock_irqrestore(&iommu->lock, flags);\n\n\treturn ret;\n}\n\nstatic int iommu_queue_command(struct amd_iommu *iommu, struct iommu_cmd *cmd)\n{\n\treturn iommu_queue_command_sync(iommu, cmd, true);\n}\n\n \nstatic int iommu_completion_wait(struct amd_iommu *iommu)\n{\n\tstruct iommu_cmd cmd;\n\tunsigned long flags;\n\tint ret;\n\tu64 data;\n\n\tif (!iommu->need_sync)\n\t\treturn 0;\n\n\tdata = atomic64_add_return(1, &iommu->cmd_sem_val);\n\tbuild_completion_wait(&cmd, iommu, data);\n\n\traw_spin_lock_irqsave(&iommu->lock, flags);\n\n\tret = __iommu_queue_command_sync(iommu, &cmd, false);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = wait_on_sem(iommu, data);\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&iommu->lock, flags);\n\n\treturn ret;\n}\n\nstatic int iommu_flush_dte(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct iommu_cmd cmd;\n\n\tbuild_inv_dte(&cmd, devid);\n\n\treturn iommu_queue_command(iommu, &cmd);\n}\n\nstatic void amd_iommu_flush_dte_all(struct amd_iommu *iommu)\n{\n\tu32 devid;\n\tu16 last_bdf = iommu->pci_seg->last_bdf;\n\n\tfor (devid = 0; devid <= last_bdf; ++devid)\n\t\tiommu_flush_dte(iommu, devid);\n\n\tiommu_completion_wait(iommu);\n}\n\n \nstatic void amd_iommu_flush_tlb_all(struct amd_iommu *iommu)\n{\n\tu32 dom_id;\n\tu16 last_bdf = iommu->pci_seg->last_bdf;\n\n\tfor (dom_id = 0; dom_id <= last_bdf; ++dom_id) {\n\t\tstruct iommu_cmd cmd;\n\t\tbuild_inv_iommu_pages(&cmd, 0, CMD_INV_IOMMU_ALL_PAGES_ADDRESS,\n\t\t\t\t      dom_id, 1);\n\t\tiommu_queue_command(iommu, &cmd);\n\t}\n\n\tiommu_completion_wait(iommu);\n}\n\nstatic void amd_iommu_flush_tlb_domid(struct amd_iommu *iommu, u32 dom_id)\n{\n\tstruct iommu_cmd cmd;\n\n\tbuild_inv_iommu_pages(&cmd, 0, CMD_INV_IOMMU_ALL_PAGES_ADDRESS,\n\t\t\t      dom_id, 1);\n\tiommu_queue_command(iommu, &cmd);\n\n\tiommu_completion_wait(iommu);\n}\n\nstatic void amd_iommu_flush_all(struct amd_iommu *iommu)\n{\n\tstruct iommu_cmd cmd;\n\n\tbuild_inv_all(&cmd);\n\n\tiommu_queue_command(iommu, &cmd);\n\tiommu_completion_wait(iommu);\n}\n\nstatic void iommu_flush_irt(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct iommu_cmd cmd;\n\n\tbuild_inv_irt(&cmd, devid);\n\n\tiommu_queue_command(iommu, &cmd);\n}\n\nstatic void amd_iommu_flush_irt_all(struct amd_iommu *iommu)\n{\n\tu32 devid;\n\tu16 last_bdf = iommu->pci_seg->last_bdf;\n\n\tif (iommu->irtcachedis_enabled)\n\t\treturn;\n\n\tfor (devid = 0; devid <= last_bdf; devid++)\n\t\tiommu_flush_irt(iommu, devid);\n\n\tiommu_completion_wait(iommu);\n}\n\nvoid iommu_flush_all_caches(struct amd_iommu *iommu)\n{\n\tif (iommu_feature(iommu, FEATURE_IA)) {\n\t\tamd_iommu_flush_all(iommu);\n\t} else {\n\t\tamd_iommu_flush_dte_all(iommu);\n\t\tamd_iommu_flush_irt_all(iommu);\n\t\tamd_iommu_flush_tlb_all(iommu);\n\t}\n}\n\n \nstatic int device_flush_iotlb(struct iommu_dev_data *dev_data,\n\t\t\t      u64 address, size_t size)\n{\n\tstruct amd_iommu *iommu;\n\tstruct iommu_cmd cmd;\n\tint qdep;\n\n\tqdep     = dev_data->ats.qdep;\n\tiommu    = rlookup_amd_iommu(dev_data->dev);\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\tbuild_inv_iotlb_pages(&cmd, dev_data->devid, qdep, address, size);\n\n\treturn iommu_queue_command(iommu, &cmd);\n}\n\nstatic int device_flush_dte_alias(struct pci_dev *pdev, u16 alias, void *data)\n{\n\tstruct amd_iommu *iommu = data;\n\n\treturn iommu_flush_dte(iommu, alias);\n}\n\n \nstatic int device_flush_dte(struct iommu_dev_data *dev_data)\n{\n\tstruct amd_iommu *iommu;\n\tstruct pci_dev *pdev = NULL;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tu16 alias;\n\tint ret;\n\n\tiommu = rlookup_amd_iommu(dev_data->dev);\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\tif (dev_is_pci(dev_data->dev))\n\t\tpdev = to_pci_dev(dev_data->dev);\n\n\tif (pdev)\n\t\tret = pci_for_each_dma_alias(pdev,\n\t\t\t\t\t     device_flush_dte_alias, iommu);\n\telse\n\t\tret = iommu_flush_dte(iommu, dev_data->devid);\n\tif (ret)\n\t\treturn ret;\n\n\tpci_seg = iommu->pci_seg;\n\talias = pci_seg->alias_table[dev_data->devid];\n\tif (alias != dev_data->devid) {\n\t\tret = iommu_flush_dte(iommu, alias);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (dev_data->ats.enabled)\n\t\tret = device_flush_iotlb(dev_data, 0, ~0UL);\n\n\treturn ret;\n}\n\n \nstatic void __domain_flush_pages(struct protection_domain *domain,\n\t\t\t\t u64 address, size_t size, int pde)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct iommu_cmd cmd;\n\tint ret = 0, i;\n\n\tbuild_inv_iommu_pages(&cmd, address, size, domain->id, pde);\n\n\tfor (i = 0; i < amd_iommu_get_num_iommus(); ++i) {\n\t\tif (!domain->dev_iommu[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tret |= iommu_queue_command(amd_iommus[i], &cmd);\n\t}\n\n\tlist_for_each_entry(dev_data, &domain->dev_list, list) {\n\n\t\tif (!dev_data->ats.enabled)\n\t\t\tcontinue;\n\n\t\tret |= device_flush_iotlb(dev_data, address, size);\n\t}\n\n\tWARN_ON(ret);\n}\n\nstatic void domain_flush_pages(struct protection_domain *domain,\n\t\t\t       u64 address, size_t size, int pde)\n{\n\tif (likely(!amd_iommu_np_cache)) {\n\t\t__domain_flush_pages(domain, address, size, pde);\n\t\treturn;\n\t}\n\n\t \n\twhile (size != 0) {\n\t\tint addr_alignment = __ffs(address);\n\t\tint size_alignment = __fls(size);\n\t\tint min_alignment;\n\t\tsize_t flush_size;\n\n\t\t \n\t\tif (likely((unsigned long)address != 0))\n\t\t\tmin_alignment = min(addr_alignment, size_alignment);\n\t\telse\n\t\t\tmin_alignment = size_alignment;\n\n\t\tflush_size = 1ul << min_alignment;\n\n\t\t__domain_flush_pages(domain, address, flush_size, pde);\n\t\taddress += flush_size;\n\t\tsize -= flush_size;\n\t}\n}\n\n \nvoid amd_iommu_domain_flush_tlb_pde(struct protection_domain *domain)\n{\n\tdomain_flush_pages(domain, 0, CMD_INV_IOMMU_ALL_PAGES_ADDRESS, 1);\n}\n\nvoid amd_iommu_domain_flush_complete(struct protection_domain *domain)\n{\n\tint i;\n\n\tfor (i = 0; i < amd_iommu_get_num_iommus(); ++i) {\n\t\tif (domain && !domain->dev_iommu[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tiommu_completion_wait(amd_iommus[i]);\n\t}\n}\n\n \nstatic void domain_flush_np_cache(struct protection_domain *domain,\n\t\tdma_addr_t iova, size_t size)\n{\n\tif (unlikely(amd_iommu_np_cache)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&domain->lock, flags);\n\t\tdomain_flush_pages(domain, iova, size, 1);\n\t\tamd_iommu_domain_flush_complete(domain);\n\t\tspin_unlock_irqrestore(&domain->lock, flags);\n\t}\n}\n\n\n \nstatic void domain_flush_devices(struct protection_domain *domain)\n{\n\tstruct iommu_dev_data *dev_data;\n\n\tlist_for_each_entry(dev_data, &domain->dev_list, list)\n\t\tdevice_flush_dte(dev_data);\n}\n\n \n\nstatic u16 domain_id_alloc(void)\n{\n\tint id;\n\n\tspin_lock(&pd_bitmap_lock);\n\tid = find_first_zero_bit(amd_iommu_pd_alloc_bitmap, MAX_DOMAIN_ID);\n\tBUG_ON(id == 0);\n\tif (id > 0 && id < MAX_DOMAIN_ID)\n\t\t__set_bit(id, amd_iommu_pd_alloc_bitmap);\n\telse\n\t\tid = 0;\n\tspin_unlock(&pd_bitmap_lock);\n\n\treturn id;\n}\n\nstatic void domain_id_free(int id)\n{\n\tspin_lock(&pd_bitmap_lock);\n\tif (id > 0 && id < MAX_DOMAIN_ID)\n\t\t__clear_bit(id, amd_iommu_pd_alloc_bitmap);\n\tspin_unlock(&pd_bitmap_lock);\n}\n\nstatic void free_gcr3_tbl_level1(u64 *tbl)\n{\n\tu64 *ptr;\n\tint i;\n\n\tfor (i = 0; i < 512; ++i) {\n\t\tif (!(tbl[i] & GCR3_VALID))\n\t\t\tcontinue;\n\n\t\tptr = iommu_phys_to_virt(tbl[i] & PAGE_MASK);\n\n\t\tfree_page((unsigned long)ptr);\n\t}\n}\n\nstatic void free_gcr3_tbl_level2(u64 *tbl)\n{\n\tu64 *ptr;\n\tint i;\n\n\tfor (i = 0; i < 512; ++i) {\n\t\tif (!(tbl[i] & GCR3_VALID))\n\t\t\tcontinue;\n\n\t\tptr = iommu_phys_to_virt(tbl[i] & PAGE_MASK);\n\n\t\tfree_gcr3_tbl_level1(ptr);\n\t}\n}\n\nstatic void free_gcr3_table(struct protection_domain *domain)\n{\n\tif (domain->glx == 2)\n\t\tfree_gcr3_tbl_level2(domain->gcr3_tbl);\n\telse if (domain->glx == 1)\n\t\tfree_gcr3_tbl_level1(domain->gcr3_tbl);\n\telse\n\t\tBUG_ON(domain->glx != 0);\n\n\tfree_page((unsigned long)domain->gcr3_tbl);\n}\n\nstatic void set_dte_entry(struct amd_iommu *iommu, u16 devid,\n\t\t\t  struct protection_domain *domain, bool ats, bool ppr)\n{\n\tu64 pte_root = 0;\n\tu64 flags = 0;\n\tu32 old_domid;\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\tif (domain->iop.mode != PAGE_MODE_NONE)\n\t\tpte_root = iommu_virt_to_phys(domain->iop.root);\n\n\tpte_root |= (domain->iop.mode & DEV_ENTRY_MODE_MASK)\n\t\t    << DEV_ENTRY_MODE_SHIFT;\n\n\tpte_root |= DTE_FLAG_IR | DTE_FLAG_IW | DTE_FLAG_V;\n\n\t \n\tif (!amd_iommu_snp_en || (domain->id != 0))\n\t\tpte_root |= DTE_FLAG_TV;\n\n\tflags = dev_table[devid].data[1];\n\n\tif (ats)\n\t\tflags |= DTE_FLAG_IOTLB;\n\n\tif (ppr) {\n\t\tif (iommu_feature(iommu, FEATURE_EPHSUP))\n\t\t\tpte_root |= 1ULL << DEV_ENTRY_PPR;\n\t}\n\n\tif (domain->flags & PD_IOMMUV2_MASK) {\n\t\tu64 gcr3 = iommu_virt_to_phys(domain->gcr3_tbl);\n\t\tu64 glx  = domain->glx;\n\t\tu64 tmp;\n\n\t\tpte_root |= DTE_FLAG_GV;\n\t\tpte_root |= (glx & DTE_GLX_MASK) << DTE_GLX_SHIFT;\n\n\t\t \n\t\ttmp = DTE_GCR3_VAL_B(~0ULL) << DTE_GCR3_SHIFT_B;\n\t\tflags    &= ~tmp;\n\n\t\ttmp = DTE_GCR3_VAL_C(~0ULL) << DTE_GCR3_SHIFT_C;\n\t\tflags    &= ~tmp;\n\n\t\t \n\t\ttmp = DTE_GCR3_VAL_A(gcr3) << DTE_GCR3_SHIFT_A;\n\t\tpte_root |= tmp;\n\n\t\ttmp = DTE_GCR3_VAL_B(gcr3) << DTE_GCR3_SHIFT_B;\n\t\tflags    |= tmp;\n\n\t\ttmp = DTE_GCR3_VAL_C(gcr3) << DTE_GCR3_SHIFT_C;\n\t\tflags    |= tmp;\n\n\t\tif (amd_iommu_gpt_level == PAGE_MODE_5_LEVEL) {\n\t\t\tdev_table[devid].data[2] |=\n\t\t\t\t((u64)GUEST_PGTABLE_5_LEVEL << DTE_GPT_LEVEL_SHIFT);\n\t\t}\n\n\t\tif (domain->flags & PD_GIOV_MASK)\n\t\t\tpte_root |= DTE_FLAG_GIOV;\n\t}\n\n\tflags &= ~DEV_DOMID_MASK;\n\tflags |= domain->id;\n\n\told_domid = dev_table[devid].data[1] & DEV_DOMID_MASK;\n\tdev_table[devid].data[1]  = flags;\n\tdev_table[devid].data[0]  = pte_root;\n\n\t \n\tif (old_domid) {\n\t\tamd_iommu_flush_tlb_domid(iommu, old_domid);\n\t}\n}\n\nstatic void clear_dte_entry(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\t \n\tdev_table[devid].data[0]  = DTE_FLAG_V;\n\n\tif (!amd_iommu_snp_en)\n\t\tdev_table[devid].data[0] |= DTE_FLAG_TV;\n\n\tdev_table[devid].data[1] &= DTE_FLAG_MASK;\n\n\tamd_iommu_apply_erratum_63(iommu, devid);\n}\n\nstatic void do_attach(struct iommu_dev_data *dev_data,\n\t\t      struct protection_domain *domain)\n{\n\tstruct amd_iommu *iommu;\n\tbool ats;\n\n\tiommu = rlookup_amd_iommu(dev_data->dev);\n\tif (!iommu)\n\t\treturn;\n\tats   = dev_data->ats.enabled;\n\n\t \n\tdev_data->domain = domain;\n\tlist_add(&dev_data->list, &domain->dev_list);\n\n\t \n\tif (domain->nid == NUMA_NO_NODE)\n\t\tdomain->nid = dev_to_node(dev_data->dev);\n\n\t \n\tdomain->dev_iommu[iommu->index] += 1;\n\tdomain->dev_cnt                 += 1;\n\n\t \n\tset_dte_entry(iommu, dev_data->devid, domain,\n\t\t      ats, dev_data->iommu_v2);\n\tclone_aliases(iommu, dev_data->dev);\n\n\tdevice_flush_dte(dev_data);\n}\n\nstatic void do_detach(struct iommu_dev_data *dev_data)\n{\n\tstruct protection_domain *domain = dev_data->domain;\n\tstruct amd_iommu *iommu;\n\n\tiommu = rlookup_amd_iommu(dev_data->dev);\n\tif (!iommu)\n\t\treturn;\n\n\t \n\tdev_data->domain = NULL;\n\tlist_del(&dev_data->list);\n\tclear_dte_entry(iommu, dev_data->devid);\n\tclone_aliases(iommu, dev_data->dev);\n\n\t \n\tdevice_flush_dte(dev_data);\n\n\t \n\tamd_iommu_domain_flush_tlb_pde(domain);\n\n\t \n\tamd_iommu_domain_flush_complete(domain);\n\n\t \n\tdomain->dev_iommu[iommu->index] -= 1;\n\tdomain->dev_cnt                 -= 1;\n}\n\nstatic void pdev_iommuv2_disable(struct pci_dev *pdev)\n{\n\tpci_disable_ats(pdev);\n\tpci_disable_pri(pdev);\n\tpci_disable_pasid(pdev);\n}\n\nstatic int pdev_pri_ats_enable(struct pci_dev *pdev)\n{\n\tint ret;\n\n\t \n\tret = pci_enable_pasid(pdev, 0);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = pci_reset_pri(pdev);\n\tif (ret)\n\t\tgoto out_err_pasid;\n\n\t \n\t \n\tret = pci_enable_pri(pdev, 32);\n\tif (ret)\n\t\tgoto out_err_pasid;\n\n\tret = pci_enable_ats(pdev, PAGE_SHIFT);\n\tif (ret)\n\t\tgoto out_err_pri;\n\n\treturn 0;\n\nout_err_pri:\n\tpci_disable_pri(pdev);\n\nout_err_pasid:\n\tpci_disable_pasid(pdev);\n\n\treturn ret;\n}\n\n \nstatic int attach_device(struct device *dev,\n\t\t\t struct protection_domain *domain)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct pci_dev *pdev;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tdev_data = dev_iommu_priv_get(dev);\n\n\tspin_lock(&dev_data->lock);\n\n\tret = -EBUSY;\n\tif (dev_data->domain != NULL)\n\t\tgoto out;\n\n\tif (!dev_is_pci(dev))\n\t\tgoto skip_ats_check;\n\n\tpdev = to_pci_dev(dev);\n\tif (domain->flags & PD_IOMMUV2_MASK) {\n\t\tstruct iommu_domain *def_domain = iommu_get_dma_domain(dev);\n\n\t\tret = -EINVAL;\n\n\t\t \n\t\tif ((amd_iommu_pgtable == AMD_IOMMU_V1) &&\n\t\t    def_domain->type != IOMMU_DOMAIN_IDENTITY) {\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (dev_data->iommu_v2) {\n\t\t\tif (pdev_pri_ats_enable(pdev) != 0)\n\t\t\t\tgoto out;\n\n\t\t\tdev_data->ats.enabled = true;\n\t\t\tdev_data->ats.qdep    = pci_ats_queue_depth(pdev);\n\t\t\tdev_data->pri_tlp     = pci_prg_resp_pasid_required(pdev);\n\t\t}\n\t} else if (amd_iommu_iotlb_sup &&\n\t\t   pci_enable_ats(pdev, PAGE_SHIFT) == 0) {\n\t\tdev_data->ats.enabled = true;\n\t\tdev_data->ats.qdep    = pci_ats_queue_depth(pdev);\n\t}\n\nskip_ats_check:\n\tret = 0;\n\n\tdo_attach(dev_data, domain);\n\n\t \n\tamd_iommu_domain_flush_tlb_pde(domain);\n\n\tamd_iommu_domain_flush_complete(domain);\n\nout:\n\tspin_unlock(&dev_data->lock);\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn ret;\n}\n\n \nstatic void detach_device(struct device *dev)\n{\n\tstruct protection_domain *domain;\n\tstruct iommu_dev_data *dev_data;\n\tunsigned long flags;\n\n\tdev_data = dev_iommu_priv_get(dev);\n\tdomain   = dev_data->domain;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tspin_lock(&dev_data->lock);\n\n\t \n\tif (WARN_ON(!dev_data->domain))\n\t\tgoto out;\n\n\tdo_detach(dev_data);\n\n\tif (!dev_is_pci(dev))\n\t\tgoto out;\n\n\tif (domain->flags & PD_IOMMUV2_MASK && dev_data->iommu_v2)\n\t\tpdev_iommuv2_disable(to_pci_dev(dev));\n\telse if (dev_data->ats.enabled)\n\t\tpci_disable_ats(to_pci_dev(dev));\n\n\tdev_data->ats.enabled = false;\n\nout:\n\tspin_unlock(&dev_data->lock);\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\nstatic struct iommu_device *amd_iommu_probe_device(struct device *dev)\n{\n\tstruct iommu_device *iommu_dev;\n\tstruct amd_iommu *iommu;\n\tint ret;\n\n\tif (!check_device(dev))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tiommu = rlookup_amd_iommu(dev);\n\tif (!iommu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\t \n\tif (!iommu->iommu.ops)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (dev_iommu_priv_get(dev))\n\t\treturn &iommu->iommu;\n\n\tret = iommu_init_device(iommu, dev);\n\tif (ret) {\n\t\tif (ret != -ENOTSUPP)\n\t\t\tdev_err(dev, \"Failed to initialize - trying to proceed anyway\\n\");\n\t\tiommu_dev = ERR_PTR(ret);\n\t\tiommu_ignore_device(iommu, dev);\n\t} else {\n\t\tamd_iommu_set_pci_msi_domain(dev, iommu);\n\t\tiommu_dev = &iommu->iommu;\n\t}\n\n\tiommu_completion_wait(iommu);\n\n\treturn iommu_dev;\n}\n\nstatic void amd_iommu_probe_finalize(struct device *dev)\n{\n\t \n\tset_dma_ops(dev, NULL);\n\tiommu_setup_dma_ops(dev, 0, U64_MAX);\n}\n\nstatic void amd_iommu_release_device(struct device *dev)\n{\n\tstruct amd_iommu *iommu;\n\n\tif (!check_device(dev))\n\t\treturn;\n\n\tiommu = rlookup_amd_iommu(dev);\n\tif (!iommu)\n\t\treturn;\n\n\tamd_iommu_uninit_device(dev);\n\tiommu_completion_wait(iommu);\n}\n\nstatic struct iommu_group *amd_iommu_device_group(struct device *dev)\n{\n\tif (dev_is_pci(dev))\n\t\treturn pci_device_group(dev);\n\n\treturn acpihid_device_group(dev);\n}\n\n \n\nstatic void update_device_table(struct protection_domain *domain)\n{\n\tstruct iommu_dev_data *dev_data;\n\n\tlist_for_each_entry(dev_data, &domain->dev_list, list) {\n\t\tstruct amd_iommu *iommu = rlookup_amd_iommu(dev_data->dev);\n\n\t\tif (!iommu)\n\t\t\tcontinue;\n\t\tset_dte_entry(iommu, dev_data->devid, domain,\n\t\t\t      dev_data->ats.enabled, dev_data->iommu_v2);\n\t\tclone_aliases(iommu, dev_data->dev);\n\t}\n}\n\nvoid amd_iommu_update_and_flush_device_table(struct protection_domain *domain)\n{\n\tupdate_device_table(domain);\n\tdomain_flush_devices(domain);\n}\n\nvoid amd_iommu_domain_update(struct protection_domain *domain)\n{\n\t \n\tamd_iommu_update_and_flush_device_table(domain);\n\n\t \n\tamd_iommu_domain_flush_tlb_pde(domain);\n\tamd_iommu_domain_flush_complete(domain);\n}\n\n \n\nstatic void cleanup_domain(struct protection_domain *domain)\n{\n\tstruct iommu_dev_data *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\twhile (!list_empty(&domain->dev_list)) {\n\t\tentry = list_first_entry(&domain->dev_list,\n\t\t\t\t\t struct iommu_dev_data, list);\n\t\tBUG_ON(!entry->domain);\n\t\tdo_detach(entry);\n\t}\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\nstatic void protection_domain_free(struct protection_domain *domain)\n{\n\tif (!domain)\n\t\treturn;\n\n\tif (domain->iop.pgtbl_cfg.tlb)\n\t\tfree_io_pgtable_ops(&domain->iop.iop.ops);\n\n\tif (domain->id)\n\t\tdomain_id_free(domain->id);\n\n\tkfree(domain);\n}\n\nstatic int protection_domain_init_v1(struct protection_domain *domain, int mode)\n{\n\tu64 *pt_root = NULL;\n\n\tBUG_ON(mode < PAGE_MODE_NONE || mode > PAGE_MODE_6_LEVEL);\n\n\tspin_lock_init(&domain->lock);\n\tdomain->id = domain_id_alloc();\n\tif (!domain->id)\n\t\treturn -ENOMEM;\n\tINIT_LIST_HEAD(&domain->dev_list);\n\n\tif (mode != PAGE_MODE_NONE) {\n\t\tpt_root = (void *)get_zeroed_page(GFP_KERNEL);\n\t\tif (!pt_root) {\n\t\t\tdomain_id_free(domain->id);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tamd_iommu_domain_set_pgtable(domain, pt_root, mode);\n\n\treturn 0;\n}\n\nstatic int protection_domain_init_v2(struct protection_domain *domain)\n{\n\tspin_lock_init(&domain->lock);\n\tdomain->id = domain_id_alloc();\n\tif (!domain->id)\n\t\treturn -ENOMEM;\n\tINIT_LIST_HEAD(&domain->dev_list);\n\n\tdomain->flags |= PD_GIOV_MASK;\n\n\tdomain->domain.pgsize_bitmap = AMD_IOMMU_PGSIZES_V2;\n\n\tif (domain_enable_v2(domain, 1)) {\n\t\tdomain_id_free(domain->id);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic struct protection_domain *protection_domain_alloc(unsigned int type)\n{\n\tstruct io_pgtable_ops *pgtbl_ops;\n\tstruct protection_domain *domain;\n\tint pgtable;\n\tint mode = DEFAULT_PGTABLE_LEVEL;\n\tint ret;\n\n\t \n\tif (type == IOMMU_DOMAIN_IDENTITY) {\n\t\tpgtable = AMD_IOMMU_V1;\n\t\tmode = PAGE_MODE_NONE;\n\t} else if (type == IOMMU_DOMAIN_UNMANAGED) {\n\t\tpgtable = AMD_IOMMU_V1;\n\t} else if (type == IOMMU_DOMAIN_DMA || type == IOMMU_DOMAIN_DMA_FQ) {\n\t\tpgtable = amd_iommu_pgtable;\n\t} else {\n\t\treturn NULL;\n\t}\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\n\tswitch (pgtable) {\n\tcase AMD_IOMMU_V1:\n\t\tret = protection_domain_init_v1(domain, mode);\n\t\tbreak;\n\tcase AMD_IOMMU_V2:\n\t\tret = protection_domain_init_v2(domain);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tif (ret)\n\t\tgoto out_err;\n\n\t \n\tif (type == IOMMU_DOMAIN_IDENTITY)\n\t\treturn domain;\n\n\tdomain->nid = NUMA_NO_NODE;\n\n\tpgtbl_ops = alloc_io_pgtable_ops(pgtable, &domain->iop.pgtbl_cfg, domain);\n\tif (!pgtbl_ops) {\n\t\tdomain_id_free(domain->id);\n\t\tgoto out_err;\n\t}\n\n\treturn domain;\nout_err:\n\tkfree(domain);\n\treturn NULL;\n}\n\nstatic inline u64 dma_max_address(void)\n{\n\tif (amd_iommu_pgtable == AMD_IOMMU_V1)\n\t\treturn ~0ULL;\n\n\t \n\treturn ((1ULL << PM_LEVEL_SHIFT(amd_iommu_gpt_level)) - 1);\n}\n\nstatic struct iommu_domain *amd_iommu_domain_alloc(unsigned type)\n{\n\tstruct protection_domain *domain;\n\n\t \n\tif (amd_iommu_snp_en && (type == IOMMU_DOMAIN_IDENTITY))\n\t\treturn NULL;\n\n\tdomain = protection_domain_alloc(type);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->domain.geometry.aperture_start = 0;\n\tdomain->domain.geometry.aperture_end   = dma_max_address();\n\tdomain->domain.geometry.force_aperture = true;\n\n\treturn &domain->domain;\n}\n\nstatic void amd_iommu_domain_free(struct iommu_domain *dom)\n{\n\tstruct protection_domain *domain;\n\n\tdomain = to_pdomain(dom);\n\n\tif (domain->dev_cnt > 0)\n\t\tcleanup_domain(domain);\n\n\tBUG_ON(domain->dev_cnt != 0);\n\n\tif (!dom)\n\t\treturn;\n\n\tif (domain->flags & PD_IOMMUV2_MASK)\n\t\tfree_gcr3_table(domain);\n\n\tprotection_domain_free(domain);\n}\n\nstatic int amd_iommu_attach_device(struct iommu_domain *dom,\n\t\t\t\t   struct device *dev)\n{\n\tstruct iommu_dev_data *dev_data = dev_iommu_priv_get(dev);\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tstruct amd_iommu *iommu = rlookup_amd_iommu(dev);\n\tint ret;\n\n\t \n\tif (dev_data->domain == domain)\n\t\treturn 0;\n\n\tdev_data->defer_attach = false;\n\n\tif (dev_data->domain)\n\t\tdetach_device(dev);\n\n\tret = attach_device(dev, domain);\n\n#ifdef CONFIG_IRQ_REMAP\n\tif (AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir)) {\n\t\tif (dom->type == IOMMU_DOMAIN_UNMANAGED)\n\t\t\tdev_data->use_vapic = 1;\n\t\telse\n\t\t\tdev_data->use_vapic = 0;\n\t}\n#endif\n\n\tiommu_completion_wait(iommu);\n\n\treturn ret;\n}\n\nstatic void amd_iommu_iotlb_sync_map(struct iommu_domain *dom,\n\t\t\t\t     unsigned long iova, size_t size)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tstruct io_pgtable_ops *ops = &domain->iop.iop.ops;\n\n\tif (ops->map_pages)\n\t\tdomain_flush_np_cache(domain, iova, size);\n}\n\nstatic int amd_iommu_map_pages(struct iommu_domain *dom, unsigned long iova,\n\t\t\t       phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t       int iommu_prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tstruct io_pgtable_ops *ops = &domain->iop.iop.ops;\n\tint prot = 0;\n\tint ret = -EINVAL;\n\n\tif ((amd_iommu_pgtable == AMD_IOMMU_V1) &&\n\t    (domain->iop.mode == PAGE_MODE_NONE))\n\t\treturn -EINVAL;\n\n\tif (iommu_prot & IOMMU_READ)\n\t\tprot |= IOMMU_PROT_IR;\n\tif (iommu_prot & IOMMU_WRITE)\n\t\tprot |= IOMMU_PROT_IW;\n\n\tif (ops->map_pages) {\n\t\tret = ops->map_pages(ops, iova, paddr, pgsize,\n\t\t\t\t     pgcount, prot, gfp, mapped);\n\t}\n\n\treturn ret;\n}\n\nstatic void amd_iommu_iotlb_gather_add_page(struct iommu_domain *domain,\n\t\t\t\t\t    struct iommu_iotlb_gather *gather,\n\t\t\t\t\t    unsigned long iova, size_t size)\n{\n\t \n\tif (amd_iommu_np_cache &&\n\t    iommu_iotlb_gather_is_disjoint(gather, iova, size))\n\t\tiommu_iotlb_sync(domain, gather);\n\n\tiommu_iotlb_gather_add_range(gather, iova, size);\n}\n\nstatic size_t amd_iommu_unmap_pages(struct iommu_domain *dom, unsigned long iova,\n\t\t\t\t    size_t pgsize, size_t pgcount,\n\t\t\t\t    struct iommu_iotlb_gather *gather)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tstruct io_pgtable_ops *ops = &domain->iop.iop.ops;\n\tsize_t r;\n\n\tif ((amd_iommu_pgtable == AMD_IOMMU_V1) &&\n\t    (domain->iop.mode == PAGE_MODE_NONE))\n\t\treturn 0;\n\n\tr = (ops->unmap_pages) ? ops->unmap_pages(ops, iova, pgsize, pgcount, NULL) : 0;\n\n\tif (r)\n\t\tamd_iommu_iotlb_gather_add_page(dom, gather, iova, r);\n\n\treturn r;\n}\n\nstatic phys_addr_t amd_iommu_iova_to_phys(struct iommu_domain *dom,\n\t\t\t\t\t  dma_addr_t iova)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tstruct io_pgtable_ops *ops = &domain->iop.iop.ops;\n\n\treturn ops->iova_to_phys(ops, iova);\n}\n\nstatic bool amd_iommu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tswitch (cap) {\n\tcase IOMMU_CAP_CACHE_COHERENCY:\n\t\treturn true;\n\tcase IOMMU_CAP_NOEXEC:\n\t\treturn false;\n\tcase IOMMU_CAP_PRE_BOOT_PROTECTION:\n\t\treturn amdr_ivrs_remap_support;\n\tcase IOMMU_CAP_ENFORCE_CACHE_COHERENCY:\n\t\treturn true;\n\tcase IOMMU_CAP_DEFERRED_FLUSH:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstatic void amd_iommu_get_resv_regions(struct device *dev,\n\t\t\t\t       struct list_head *head)\n{\n\tstruct iommu_resv_region *region;\n\tstruct unity_map_entry *entry;\n\tstruct amd_iommu *iommu;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tint devid, sbdf;\n\n\tsbdf = get_device_sbdf_id(dev);\n\tif (sbdf < 0)\n\t\treturn;\n\n\tdevid = PCI_SBDF_TO_DEVID(sbdf);\n\tiommu = rlookup_amd_iommu(dev);\n\tif (!iommu)\n\t\treturn;\n\tpci_seg = iommu->pci_seg;\n\n\tlist_for_each_entry(entry, &pci_seg->unity_map, list) {\n\t\tint type, prot = 0;\n\t\tsize_t length;\n\n\t\tif (devid < entry->devid_start || devid > entry->devid_end)\n\t\t\tcontinue;\n\n\t\ttype   = IOMMU_RESV_DIRECT;\n\t\tlength = entry->address_end - entry->address_start;\n\t\tif (entry->prot & IOMMU_PROT_IR)\n\t\t\tprot |= IOMMU_READ;\n\t\tif (entry->prot & IOMMU_PROT_IW)\n\t\t\tprot |= IOMMU_WRITE;\n\t\tif (entry->prot & IOMMU_UNITY_MAP_FLAG_EXCL_RANGE)\n\t\t\t \n\t\t\ttype = IOMMU_RESV_RESERVED;\n\n\t\tregion = iommu_alloc_resv_region(entry->address_start,\n\t\t\t\t\t\t length, prot, type,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!region) {\n\t\t\tdev_err(dev, \"Out of memory allocating dm-regions\\n\");\n\t\t\treturn;\n\t\t}\n\t\tlist_add_tail(&region->list, head);\n\t}\n\n\tregion = iommu_alloc_resv_region(MSI_RANGE_START,\n\t\t\t\t\t MSI_RANGE_END - MSI_RANGE_START + 1,\n\t\t\t\t\t 0, IOMMU_RESV_MSI, GFP_KERNEL);\n\tif (!region)\n\t\treturn;\n\tlist_add_tail(&region->list, head);\n\n\tregion = iommu_alloc_resv_region(HT_RANGE_START,\n\t\t\t\t\t HT_RANGE_END - HT_RANGE_START + 1,\n\t\t\t\t\t 0, IOMMU_RESV_RESERVED, GFP_KERNEL);\n\tif (!region)\n\t\treturn;\n\tlist_add_tail(&region->list, head);\n}\n\nbool amd_iommu_is_attach_deferred(struct device *dev)\n{\n\tstruct iommu_dev_data *dev_data = dev_iommu_priv_get(dev);\n\n\treturn dev_data->defer_attach;\n}\nEXPORT_SYMBOL_GPL(amd_iommu_is_attach_deferred);\n\nstatic void amd_iommu_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tstruct protection_domain *dom = to_pdomain(domain);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dom->lock, flags);\n\tamd_iommu_domain_flush_tlb_pde(dom);\n\tamd_iommu_domain_flush_complete(dom);\n\tspin_unlock_irqrestore(&dom->lock, flags);\n}\n\nstatic void amd_iommu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\t struct iommu_iotlb_gather *gather)\n{\n\tstruct protection_domain *dom = to_pdomain(domain);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dom->lock, flags);\n\tdomain_flush_pages(dom, gather->start, gather->end - gather->start + 1, 1);\n\tamd_iommu_domain_flush_complete(dom);\n\tspin_unlock_irqrestore(&dom->lock, flags);\n}\n\nstatic int amd_iommu_def_domain_type(struct device *dev)\n{\n\tstruct iommu_dev_data *dev_data;\n\n\tdev_data = dev_iommu_priv_get(dev);\n\tif (!dev_data)\n\t\treturn 0;\n\n\t \n\tif (dev_data->iommu_v2 &&\n\t    !cc_platform_has(CC_ATTR_MEM_ENCRYPT) &&\n\t    !amd_iommu_snp_en) {\n\t\treturn IOMMU_DOMAIN_IDENTITY;\n\t}\n\n\treturn 0;\n}\n\nstatic bool amd_iommu_enforce_cache_coherency(struct iommu_domain *domain)\n{\n\t \n\treturn true;\n}\n\nconst struct iommu_ops amd_iommu_ops = {\n\t.capable = amd_iommu_capable,\n\t.domain_alloc = amd_iommu_domain_alloc,\n\t.probe_device = amd_iommu_probe_device,\n\t.release_device = amd_iommu_release_device,\n\t.probe_finalize = amd_iommu_probe_finalize,\n\t.device_group = amd_iommu_device_group,\n\t.get_resv_regions = amd_iommu_get_resv_regions,\n\t.is_attach_deferred = amd_iommu_is_attach_deferred,\n\t.pgsize_bitmap\t= AMD_IOMMU_PGSIZES,\n\t.def_domain_type = amd_iommu_def_domain_type,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= amd_iommu_attach_device,\n\t\t.map_pages\t= amd_iommu_map_pages,\n\t\t.unmap_pages\t= amd_iommu_unmap_pages,\n\t\t.iotlb_sync_map\t= amd_iommu_iotlb_sync_map,\n\t\t.iova_to_phys\t= amd_iommu_iova_to_phys,\n\t\t.flush_iotlb_all = amd_iommu_flush_iotlb_all,\n\t\t.iotlb_sync\t= amd_iommu_iotlb_sync,\n\t\t.free\t\t= amd_iommu_domain_free,\n\t\t.enforce_cache_coherency = amd_iommu_enforce_cache_coherency,\n\t}\n};\n\n \n\n \nint amd_iommu_register_ppr_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_register(&ppr_notifier, nb);\n}\nEXPORT_SYMBOL(amd_iommu_register_ppr_notifier);\n\nint amd_iommu_unregister_ppr_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_unregister(&ppr_notifier, nb);\n}\nEXPORT_SYMBOL(amd_iommu_unregister_ppr_notifier);\n\nvoid amd_iommu_domain_direct_map(struct iommu_domain *dom)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tif (domain->iop.pgtbl_cfg.tlb)\n\t\tfree_io_pgtable_ops(&domain->iop.iop.ops);\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\nEXPORT_SYMBOL(amd_iommu_domain_direct_map);\n\n \nstatic int domain_enable_v2(struct protection_domain *domain, int pasids)\n{\n\tint levels;\n\n\t \n\tfor (levels = 0; (pasids - 1) & ~0x1ff; pasids >>= 9)\n\t\tlevels += 1;\n\n\tif (levels > amd_iommu_max_glx_val)\n\t\treturn -EINVAL;\n\n\tdomain->gcr3_tbl = (void *)get_zeroed_page(GFP_ATOMIC);\n\tif (domain->gcr3_tbl == NULL)\n\t\treturn -ENOMEM;\n\n\tdomain->glx      = levels;\n\tdomain->flags   |= PD_IOMMUV2_MASK;\n\n\tamd_iommu_domain_update(domain);\n\n\treturn 0;\n}\n\nint amd_iommu_domain_enable_v2(struct iommu_domain *dom, int pasids)\n{\n\tstruct protection_domain *pdom = to_pdomain(dom);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&pdom->lock, flags);\n\n\t \n\tret = -EBUSY;\n\tif (pdom->dev_cnt > 0 || pdom->flags & PD_IOMMUV2_MASK)\n\t\tgoto out;\n\n\tif (!pdom->gcr3_tbl)\n\t\tret = domain_enable_v2(pdom, pasids);\n\nout:\n\tspin_unlock_irqrestore(&pdom->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_domain_enable_v2);\n\nstatic int __flush_pasid(struct protection_domain *domain, u32 pasid,\n\t\t\t u64 address, bool size)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct iommu_cmd cmd;\n\tint i, ret;\n\n\tif (!(domain->flags & PD_IOMMUV2_MASK))\n\t\treturn -EINVAL;\n\n\tbuild_inv_iommu_pasid(&cmd, domain->id, pasid, address, size);\n\n\t \n\tfor (i = 0; i < amd_iommu_get_num_iommus(); ++i) {\n\t\tif (domain->dev_iommu[i] == 0)\n\t\t\tcontinue;\n\n\t\tret = iommu_queue_command(amd_iommus[i], &cmd);\n\t\tif (ret != 0)\n\t\t\tgoto out;\n\t}\n\n\t \n\tamd_iommu_domain_flush_complete(domain);\n\n\t \n\tlist_for_each_entry(dev_data, &domain->dev_list, list) {\n\t\tstruct amd_iommu *iommu;\n\t\tint qdep;\n\n\t\t \n\t\tif (!dev_data->ats.enabled)\n\t\t\tcontinue;\n\n\t\tqdep  = dev_data->ats.qdep;\n\t\tiommu = rlookup_amd_iommu(dev_data->dev);\n\t\tif (!iommu)\n\t\t\tcontinue;\n\t\tbuild_inv_iotlb_pasid(&cmd, dev_data->devid, pasid,\n\t\t\t\t      qdep, address, size);\n\n\t\tret = iommu_queue_command(iommu, &cmd);\n\t\tif (ret != 0)\n\t\t\tgoto out;\n\t}\n\n\t \n\tamd_iommu_domain_flush_complete(domain);\n\n\tret = 0;\n\nout:\n\n\treturn ret;\n}\n\nstatic int __amd_iommu_flush_page(struct protection_domain *domain, u32 pasid,\n\t\t\t\t  u64 address)\n{\n\treturn __flush_pasid(domain, pasid, address, false);\n}\n\nint amd_iommu_flush_page(struct iommu_domain *dom, u32 pasid,\n\t\t\t u64 address)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tret = __amd_iommu_flush_page(domain, pasid, address);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_flush_page);\n\nstatic int __amd_iommu_flush_tlb(struct protection_domain *domain, u32 pasid)\n{\n\treturn __flush_pasid(domain, pasid, CMD_INV_IOMMU_ALL_PAGES_ADDRESS,\n\t\t\t     true);\n}\n\nint amd_iommu_flush_tlb(struct iommu_domain *dom, u32 pasid)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tret = __amd_iommu_flush_tlb(domain, pasid);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_flush_tlb);\n\nstatic u64 *__get_gcr3_pte(u64 *root, int level, u32 pasid, bool alloc)\n{\n\tint index;\n\tu64 *pte;\n\n\twhile (true) {\n\n\t\tindex = (pasid >> (9 * level)) & 0x1ff;\n\t\tpte   = &root[index];\n\n\t\tif (level == 0)\n\t\t\tbreak;\n\n\t\tif (!(*pte & GCR3_VALID)) {\n\t\t\tif (!alloc)\n\t\t\t\treturn NULL;\n\n\t\t\troot = (void *)get_zeroed_page(GFP_ATOMIC);\n\t\t\tif (root == NULL)\n\t\t\t\treturn NULL;\n\n\t\t\t*pte = iommu_virt_to_phys(root) | GCR3_VALID;\n\t\t}\n\n\t\troot = iommu_phys_to_virt(*pte & PAGE_MASK);\n\n\t\tlevel -= 1;\n\t}\n\n\treturn pte;\n}\n\nstatic int __set_gcr3(struct protection_domain *domain, u32 pasid,\n\t\t      unsigned long cr3)\n{\n\tu64 *pte;\n\n\tif (domain->iop.mode != PAGE_MODE_NONE)\n\t\treturn -EINVAL;\n\n\tpte = __get_gcr3_pte(domain->gcr3_tbl, domain->glx, pasid, true);\n\tif (pte == NULL)\n\t\treturn -ENOMEM;\n\n\t*pte = (cr3 & PAGE_MASK) | GCR3_VALID;\n\n\treturn __amd_iommu_flush_tlb(domain, pasid);\n}\n\nstatic int __clear_gcr3(struct protection_domain *domain, u32 pasid)\n{\n\tu64 *pte;\n\n\tif (domain->iop.mode != PAGE_MODE_NONE)\n\t\treturn -EINVAL;\n\n\tpte = __get_gcr3_pte(domain->gcr3_tbl, domain->glx, pasid, false);\n\tif (pte == NULL)\n\t\treturn 0;\n\n\t*pte = 0;\n\n\treturn __amd_iommu_flush_tlb(domain, pasid);\n}\n\nint amd_iommu_domain_set_gcr3(struct iommu_domain *dom, u32 pasid,\n\t\t\t      unsigned long cr3)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tret = __set_gcr3(domain, pasid, cr3);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_domain_set_gcr3);\n\nint amd_iommu_domain_clear_gcr3(struct iommu_domain *dom, u32 pasid)\n{\n\tstruct protection_domain *domain = to_pdomain(dom);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tret = __clear_gcr3(domain, pasid);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(amd_iommu_domain_clear_gcr3);\n\nint amd_iommu_complete_ppr(struct pci_dev *pdev, u32 pasid,\n\t\t\t   int status, int tag)\n{\n\tstruct iommu_dev_data *dev_data;\n\tstruct amd_iommu *iommu;\n\tstruct iommu_cmd cmd;\n\n\tdev_data = dev_iommu_priv_get(&pdev->dev);\n\tiommu    = rlookup_amd_iommu(&pdev->dev);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tbuild_complete_ppr(&cmd, dev_data->devid, pasid, status,\n\t\t\t   tag, dev_data->pri_tlp);\n\n\treturn iommu_queue_command(iommu, &cmd);\n}\nEXPORT_SYMBOL(amd_iommu_complete_ppr);\n\nint amd_iommu_device_info(struct pci_dev *pdev,\n                          struct amd_iommu_device_info *info)\n{\n\tint max_pasids;\n\tint pos;\n\n\tif (pdev == NULL || info == NULL)\n\t\treturn -EINVAL;\n\n\tif (!amd_iommu_v2_supported())\n\t\treturn -EINVAL;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tif (pci_ats_supported(pdev))\n\t\tinfo->flags |= AMD_IOMMU_DEVICE_FLAG_ATS_SUP;\n\n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_PRI);\n\tif (pos)\n\t\tinfo->flags |= AMD_IOMMU_DEVICE_FLAG_PRI_SUP;\n\n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_PASID);\n\tif (pos) {\n\t\tint features;\n\n\t\tmax_pasids = 1 << (9 * (amd_iommu_max_glx_val + 1));\n\t\tmax_pasids = min(max_pasids, (1 << 20));\n\n\t\tinfo->flags |= AMD_IOMMU_DEVICE_FLAG_PASID_SUP;\n\t\tinfo->max_pasids = min(pci_max_pasids(pdev), max_pasids);\n\n\t\tfeatures = pci_pasid_features(pdev);\n\t\tif (features & PCI_PASID_CAP_EXEC)\n\t\t\tinfo->flags |= AMD_IOMMU_DEVICE_FLAG_EXEC_SUP;\n\t\tif (features & PCI_PASID_CAP_PRIV)\n\t\t\tinfo->flags |= AMD_IOMMU_DEVICE_FLAG_PRIV_SUP;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(amd_iommu_device_info);\n\n#ifdef CONFIG_IRQ_REMAP\n\n \n\nstatic struct irq_chip amd_ir_chip;\nstatic DEFINE_SPINLOCK(iommu_table_lock);\n\nstatic void iommu_flush_irt_and_complete(struct amd_iommu *iommu, u16 devid)\n{\n\tint ret;\n\tu64 data;\n\tunsigned long flags;\n\tstruct iommu_cmd cmd, cmd2;\n\n\tif (iommu->irtcachedis_enabled)\n\t\treturn;\n\n\tbuild_inv_irt(&cmd, devid);\n\tdata = atomic64_add_return(1, &iommu->cmd_sem_val);\n\tbuild_completion_wait(&cmd2, iommu, data);\n\n\traw_spin_lock_irqsave(&iommu->lock, flags);\n\tret = __iommu_queue_command_sync(iommu, &cmd, true);\n\tif (ret)\n\t\tgoto out;\n\tret = __iommu_queue_command_sync(iommu, &cmd2, false);\n\tif (ret)\n\t\tgoto out;\n\twait_on_sem(iommu, data);\nout:\n\traw_spin_unlock_irqrestore(&iommu->lock, flags);\n}\n\nstatic void set_dte_irq_entry(struct amd_iommu *iommu, u16 devid,\n\t\t\t      struct irq_remap_table *table)\n{\n\tu64 dte;\n\tstruct dev_table_entry *dev_table = get_dev_table(iommu);\n\n\tdte\t= dev_table[devid].data[2];\n\tdte\t&= ~DTE_IRQ_PHYS_ADDR_MASK;\n\tdte\t|= iommu_virt_to_phys(table->table);\n\tdte\t|= DTE_IRQ_REMAP_INTCTL;\n\tdte\t|= DTE_INTTABLEN;\n\tdte\t|= DTE_IRQ_REMAP_ENABLE;\n\n\tdev_table[devid].data[2] = dte;\n}\n\nstatic struct irq_remap_table *get_irq_table(struct amd_iommu *iommu, u16 devid)\n{\n\tstruct irq_remap_table *table;\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tif (WARN_ONCE(!pci_seg->rlookup_table[devid],\n\t\t      \"%s: no iommu for devid %x:%x\\n\",\n\t\t      __func__, pci_seg->id, devid))\n\t\treturn NULL;\n\n\ttable = pci_seg->irq_lookup_table[devid];\n\tif (WARN_ONCE(!table, \"%s: no table for devid %x:%x\\n\",\n\t\t      __func__, pci_seg->id, devid))\n\t\treturn NULL;\n\n\treturn table;\n}\n\nstatic struct irq_remap_table *__alloc_irq_table(void)\n{\n\tstruct irq_remap_table *table;\n\n\ttable = kzalloc(sizeof(*table), GFP_KERNEL);\n\tif (!table)\n\t\treturn NULL;\n\n\ttable->table = kmem_cache_alloc(amd_iommu_irq_cache, GFP_KERNEL);\n\tif (!table->table) {\n\t\tkfree(table);\n\t\treturn NULL;\n\t}\n\traw_spin_lock_init(&table->lock);\n\n\tif (!AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir))\n\t\tmemset(table->table, 0,\n\t\t       MAX_IRQS_PER_TABLE * sizeof(u32));\n\telse\n\t\tmemset(table->table, 0,\n\t\t       (MAX_IRQS_PER_TABLE * (sizeof(u64) * 2)));\n\treturn table;\n}\n\nstatic void set_remap_table_entry(struct amd_iommu *iommu, u16 devid,\n\t\t\t\t  struct irq_remap_table *table)\n{\n\tstruct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;\n\n\tpci_seg->irq_lookup_table[devid] = table;\n\tset_dte_irq_entry(iommu, devid, table);\n\tiommu_flush_dte(iommu, devid);\n}\n\nstatic int set_remap_table_entry_alias(struct pci_dev *pdev, u16 alias,\n\t\t\t\t       void *data)\n{\n\tstruct irq_remap_table *table = data;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tstruct amd_iommu *iommu = rlookup_amd_iommu(&pdev->dev);\n\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\tpci_seg = iommu->pci_seg;\n\tpci_seg->irq_lookup_table[alias] = table;\n\tset_dte_irq_entry(iommu, alias, table);\n\tiommu_flush_dte(pci_seg->rlookup_table[alias], alias);\n\n\treturn 0;\n}\n\nstatic struct irq_remap_table *alloc_irq_table(struct amd_iommu *iommu,\n\t\t\t\t\t       u16 devid, struct pci_dev *pdev)\n{\n\tstruct irq_remap_table *table = NULL;\n\tstruct irq_remap_table *new_table = NULL;\n\tstruct amd_iommu_pci_seg *pci_seg;\n\tunsigned long flags;\n\tu16 alias;\n\n\tspin_lock_irqsave(&iommu_table_lock, flags);\n\n\tpci_seg = iommu->pci_seg;\n\ttable = pci_seg->irq_lookup_table[devid];\n\tif (table)\n\t\tgoto out_unlock;\n\n\talias = pci_seg->alias_table[devid];\n\ttable = pci_seg->irq_lookup_table[alias];\n\tif (table) {\n\t\tset_remap_table_entry(iommu, devid, table);\n\t\tgoto out_wait;\n\t}\n\tspin_unlock_irqrestore(&iommu_table_lock, flags);\n\n\t \n\tnew_table = __alloc_irq_table();\n\tif (!new_table)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&iommu_table_lock, flags);\n\n\ttable = pci_seg->irq_lookup_table[devid];\n\tif (table)\n\t\tgoto out_unlock;\n\n\ttable = pci_seg->irq_lookup_table[alias];\n\tif (table) {\n\t\tset_remap_table_entry(iommu, devid, table);\n\t\tgoto out_wait;\n\t}\n\n\ttable = new_table;\n\tnew_table = NULL;\n\n\tif (pdev)\n\t\tpci_for_each_dma_alias(pdev, set_remap_table_entry_alias,\n\t\t\t\t       table);\n\telse\n\t\tset_remap_table_entry(iommu, devid, table);\n\n\tif (devid != alias)\n\t\tset_remap_table_entry(iommu, alias, table);\n\nout_wait:\n\tiommu_completion_wait(iommu);\n\nout_unlock:\n\tspin_unlock_irqrestore(&iommu_table_lock, flags);\n\n\tif (new_table) {\n\t\tkmem_cache_free(amd_iommu_irq_cache, new_table->table);\n\t\tkfree(new_table);\n\t}\n\treturn table;\n}\n\nstatic int alloc_irq_index(struct amd_iommu *iommu, u16 devid, int count,\n\t\t\t   bool align, struct pci_dev *pdev)\n{\n\tstruct irq_remap_table *table;\n\tint index, c, alignment = 1;\n\tunsigned long flags;\n\n\ttable = alloc_irq_table(iommu, devid, pdev);\n\tif (!table)\n\t\treturn -ENODEV;\n\n\tif (align)\n\t\talignment = roundup_pow_of_two(count);\n\n\traw_spin_lock_irqsave(&table->lock, flags);\n\n\t \n\tfor (index = ALIGN(table->min_index, alignment), c = 0;\n\t     index < MAX_IRQS_PER_TABLE;) {\n\t\tif (!iommu->irte_ops->is_allocated(table, index)) {\n\t\t\tc += 1;\n\t\t} else {\n\t\t\tc     = 0;\n\t\t\tindex = ALIGN(index + 1, alignment);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (c == count)\t{\n\t\t\tfor (; c != 0; --c)\n\t\t\t\tiommu->irte_ops->set_allocated(table, index - c + 1);\n\n\t\t\tindex -= count - 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tindex++;\n\t}\n\n\tindex = -ENOSPC;\n\nout:\n\traw_spin_unlock_irqrestore(&table->lock, flags);\n\n\treturn index;\n}\n\nstatic int modify_irte_ga(struct amd_iommu *iommu, u16 devid, int index,\n\t\t\t  struct irte_ga *irte)\n{\n\tstruct irq_remap_table *table;\n\tstruct irte_ga *entry;\n\tunsigned long flags;\n\tu128 old;\n\n\ttable = get_irq_table(iommu, devid);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\traw_spin_lock_irqsave(&table->lock, flags);\n\n\tentry = (struct irte_ga *)table->table;\n\tentry = &entry[index];\n\n\t \n\told = entry->irte;\n\tWARN_ON(!try_cmpxchg128(&entry->irte, &old, irte->irte));\n\n\traw_spin_unlock_irqrestore(&table->lock, flags);\n\n\tiommu_flush_irt_and_complete(iommu, devid);\n\n\treturn 0;\n}\n\nstatic int modify_irte(struct amd_iommu *iommu,\n\t\t       u16 devid, int index, union irte *irte)\n{\n\tstruct irq_remap_table *table;\n\tunsigned long flags;\n\n\ttable = get_irq_table(iommu, devid);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\traw_spin_lock_irqsave(&table->lock, flags);\n\ttable->table[index] = irte->val;\n\traw_spin_unlock_irqrestore(&table->lock, flags);\n\n\tiommu_flush_irt_and_complete(iommu, devid);\n\n\treturn 0;\n}\n\nstatic void free_irte(struct amd_iommu *iommu, u16 devid, int index)\n{\n\tstruct irq_remap_table *table;\n\tunsigned long flags;\n\n\ttable = get_irq_table(iommu, devid);\n\tif (!table)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&table->lock, flags);\n\tiommu->irte_ops->clear_allocated(table, index);\n\traw_spin_unlock_irqrestore(&table->lock, flags);\n\n\tiommu_flush_irt_and_complete(iommu, devid);\n}\n\nstatic void irte_prepare(void *entry,\n\t\t\t u32 delivery_mode, bool dest_mode,\n\t\t\t u8 vector, u32 dest_apicid, int devid)\n{\n\tunion irte *irte = (union irte *) entry;\n\n\tirte->val                = 0;\n\tirte->fields.vector      = vector;\n\tirte->fields.int_type    = delivery_mode;\n\tirte->fields.destination = dest_apicid;\n\tirte->fields.dm          = dest_mode;\n\tirte->fields.valid       = 1;\n}\n\nstatic void irte_ga_prepare(void *entry,\n\t\t\t    u32 delivery_mode, bool dest_mode,\n\t\t\t    u8 vector, u32 dest_apicid, int devid)\n{\n\tstruct irte_ga *irte = (struct irte_ga *) entry;\n\n\tirte->lo.val                      = 0;\n\tirte->hi.val                      = 0;\n\tirte->lo.fields_remap.int_type    = delivery_mode;\n\tirte->lo.fields_remap.dm          = dest_mode;\n\tirte->hi.fields.vector            = vector;\n\tirte->lo.fields_remap.destination = APICID_TO_IRTE_DEST_LO(dest_apicid);\n\tirte->hi.fields.destination       = APICID_TO_IRTE_DEST_HI(dest_apicid);\n\tirte->lo.fields_remap.valid       = 1;\n}\n\nstatic void irte_activate(struct amd_iommu *iommu, void *entry, u16 devid, u16 index)\n{\n\tunion irte *irte = (union irte *) entry;\n\n\tirte->fields.valid = 1;\n\tmodify_irte(iommu, devid, index, irte);\n}\n\nstatic void irte_ga_activate(struct amd_iommu *iommu, void *entry, u16 devid, u16 index)\n{\n\tstruct irte_ga *irte = (struct irte_ga *) entry;\n\n\tirte->lo.fields_remap.valid = 1;\n\tmodify_irte_ga(iommu, devid, index, irte);\n}\n\nstatic void irte_deactivate(struct amd_iommu *iommu, void *entry, u16 devid, u16 index)\n{\n\tunion irte *irte = (union irte *) entry;\n\n\tirte->fields.valid = 0;\n\tmodify_irte(iommu, devid, index, irte);\n}\n\nstatic void irte_ga_deactivate(struct amd_iommu *iommu, void *entry, u16 devid, u16 index)\n{\n\tstruct irte_ga *irte = (struct irte_ga *) entry;\n\n\tirte->lo.fields_remap.valid = 0;\n\tmodify_irte_ga(iommu, devid, index, irte);\n}\n\nstatic void irte_set_affinity(struct amd_iommu *iommu, void *entry, u16 devid, u16 index,\n\t\t\t      u8 vector, u32 dest_apicid)\n{\n\tunion irte *irte = (union irte *) entry;\n\n\tirte->fields.vector = vector;\n\tirte->fields.destination = dest_apicid;\n\tmodify_irte(iommu, devid, index, irte);\n}\n\nstatic void irte_ga_set_affinity(struct amd_iommu *iommu, void *entry, u16 devid, u16 index,\n\t\t\t\t u8 vector, u32 dest_apicid)\n{\n\tstruct irte_ga *irte = (struct irte_ga *) entry;\n\n\tif (!irte->lo.fields_remap.guest_mode) {\n\t\tirte->hi.fields.vector = vector;\n\t\tirte->lo.fields_remap.destination =\n\t\t\t\t\tAPICID_TO_IRTE_DEST_LO(dest_apicid);\n\t\tirte->hi.fields.destination =\n\t\t\t\t\tAPICID_TO_IRTE_DEST_HI(dest_apicid);\n\t\tmodify_irte_ga(iommu, devid, index, irte);\n\t}\n}\n\n#define IRTE_ALLOCATED (~1U)\nstatic void irte_set_allocated(struct irq_remap_table *table, int index)\n{\n\ttable->table[index] = IRTE_ALLOCATED;\n}\n\nstatic void irte_ga_set_allocated(struct irq_remap_table *table, int index)\n{\n\tstruct irte_ga *ptr = (struct irte_ga *)table->table;\n\tstruct irte_ga *irte = &ptr[index];\n\n\tmemset(&irte->lo.val, 0, sizeof(u64));\n\tmemset(&irte->hi.val, 0, sizeof(u64));\n\tirte->hi.fields.vector = 0xff;\n}\n\nstatic bool irte_is_allocated(struct irq_remap_table *table, int index)\n{\n\tunion irte *ptr = (union irte *)table->table;\n\tunion irte *irte = &ptr[index];\n\n\treturn irte->val != 0;\n}\n\nstatic bool irte_ga_is_allocated(struct irq_remap_table *table, int index)\n{\n\tstruct irte_ga *ptr = (struct irte_ga *)table->table;\n\tstruct irte_ga *irte = &ptr[index];\n\n\treturn irte->hi.fields.vector != 0;\n}\n\nstatic void irte_clear_allocated(struct irq_remap_table *table, int index)\n{\n\ttable->table[index] = 0;\n}\n\nstatic void irte_ga_clear_allocated(struct irq_remap_table *table, int index)\n{\n\tstruct irte_ga *ptr = (struct irte_ga *)table->table;\n\tstruct irte_ga *irte = &ptr[index];\n\n\tmemset(&irte->lo.val, 0, sizeof(u64));\n\tmemset(&irte->hi.val, 0, sizeof(u64));\n}\n\nstatic int get_devid(struct irq_alloc_info *info)\n{\n\tswitch (info->type) {\n\tcase X86_IRQ_ALLOC_TYPE_IOAPIC:\n\t\treturn get_ioapic_devid(info->devid);\n\tcase X86_IRQ_ALLOC_TYPE_HPET:\n\t\treturn get_hpet_devid(info->devid);\n\tcase X86_IRQ_ALLOC_TYPE_PCI_MSI:\n\tcase X86_IRQ_ALLOC_TYPE_PCI_MSIX:\n\t\treturn get_device_sbdf_id(msi_desc_to_dev(info->desc));\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -1;\n\t}\n}\n\nstruct irq_remap_ops amd_iommu_irq_ops = {\n\t.prepare\t\t= amd_iommu_prepare,\n\t.enable\t\t\t= amd_iommu_enable,\n\t.disable\t\t= amd_iommu_disable,\n\t.reenable\t\t= amd_iommu_reenable,\n\t.enable_faulting\t= amd_iommu_enable_faulting,\n};\n\nstatic void fill_msi_msg(struct msi_msg *msg, u32 index)\n{\n\tmsg->data = index;\n\tmsg->address_lo = 0;\n\tmsg->arch_addr_lo.base_address = X86_MSI_BASE_ADDRESS_LOW;\n\tmsg->address_hi = X86_MSI_BASE_ADDRESS_HIGH;\n}\n\nstatic void irq_remapping_prepare_irte(struct amd_ir_data *data,\n\t\t\t\t       struct irq_cfg *irq_cfg,\n\t\t\t\t       struct irq_alloc_info *info,\n\t\t\t\t       int devid, int index, int sub_handle)\n{\n\tstruct irq_2_irte *irte_info = &data->irq_2_irte;\n\tstruct amd_iommu *iommu = data->iommu;\n\n\tif (!iommu)\n\t\treturn;\n\n\tdata->irq_2_irte.devid = devid;\n\tdata->irq_2_irte.index = index + sub_handle;\n\tiommu->irte_ops->prepare(data->entry, apic->delivery_mode,\n\t\t\t\t apic->dest_mode_logical, irq_cfg->vector,\n\t\t\t\t irq_cfg->dest_apicid, devid);\n\n\tswitch (info->type) {\n\tcase X86_IRQ_ALLOC_TYPE_IOAPIC:\n\tcase X86_IRQ_ALLOC_TYPE_HPET:\n\tcase X86_IRQ_ALLOC_TYPE_PCI_MSI:\n\tcase X86_IRQ_ALLOC_TYPE_PCI_MSIX:\n\t\tfill_msi_msg(&data->msi_entry, irte_info->index);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n}\n\nstruct amd_irte_ops irte_32_ops = {\n\t.prepare = irte_prepare,\n\t.activate = irte_activate,\n\t.deactivate = irte_deactivate,\n\t.set_affinity = irte_set_affinity,\n\t.set_allocated = irte_set_allocated,\n\t.is_allocated = irte_is_allocated,\n\t.clear_allocated = irte_clear_allocated,\n};\n\nstruct amd_irte_ops irte_128_ops = {\n\t.prepare = irte_ga_prepare,\n\t.activate = irte_ga_activate,\n\t.deactivate = irte_ga_deactivate,\n\t.set_affinity = irte_ga_set_affinity,\n\t.set_allocated = irte_ga_set_allocated,\n\t.is_allocated = irte_ga_is_allocated,\n\t.clear_allocated = irte_ga_clear_allocated,\n};\n\nstatic int irq_remapping_alloc(struct irq_domain *domain, unsigned int virq,\n\t\t\t       unsigned int nr_irqs, void *arg)\n{\n\tstruct irq_alloc_info *info = arg;\n\tstruct irq_data *irq_data;\n\tstruct amd_ir_data *data = NULL;\n\tstruct amd_iommu *iommu;\n\tstruct irq_cfg *cfg;\n\tint i, ret, devid, seg, sbdf;\n\tint index;\n\n\tif (!info)\n\t\treturn -EINVAL;\n\tif (nr_irqs > 1 && info->type != X86_IRQ_ALLOC_TYPE_PCI_MSI)\n\t\treturn -EINVAL;\n\n\tsbdf = get_devid(info);\n\tif (sbdf < 0)\n\t\treturn -EINVAL;\n\n\tseg = PCI_SBDF_TO_SEGID(sbdf);\n\tdevid = PCI_SBDF_TO_DEVID(sbdf);\n\tiommu = __rlookup_amd_iommu(seg, devid);\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\tret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (info->type == X86_IRQ_ALLOC_TYPE_IOAPIC) {\n\t\tstruct irq_remap_table *table;\n\n\t\ttable = alloc_irq_table(iommu, devid, NULL);\n\t\tif (table) {\n\t\t\tif (!table->min_index) {\n\t\t\t\t \n\t\t\t\ttable->min_index = 32;\n\t\t\t\tfor (i = 0; i < 32; ++i)\n\t\t\t\t\tiommu->irte_ops->set_allocated(table, i);\n\t\t\t}\n\t\t\tWARN_ON(table->min_index != 32);\n\t\t\tindex = info->ioapic.pin;\n\t\t} else {\n\t\t\tindex = -ENOMEM;\n\t\t}\n\t} else if (info->type == X86_IRQ_ALLOC_TYPE_PCI_MSI ||\n\t\t   info->type == X86_IRQ_ALLOC_TYPE_PCI_MSIX) {\n\t\tbool align = (info->type == X86_IRQ_ALLOC_TYPE_PCI_MSI);\n\n\t\tindex = alloc_irq_index(iommu, devid, nr_irqs, align,\n\t\t\t\t\tmsi_desc_to_pci_dev(info->desc));\n\t} else {\n\t\tindex = alloc_irq_index(iommu, devid, nr_irqs, false, NULL);\n\t}\n\n\tif (index < 0) {\n\t\tpr_warn(\"Failed to allocate IRTE\\n\");\n\t\tret = index;\n\t\tgoto out_free_parent;\n\t}\n\n\tfor (i = 0; i < nr_irqs; i++) {\n\t\tirq_data = irq_domain_get_irq_data(domain, virq + i);\n\t\tcfg = irq_data ? irqd_cfg(irq_data) : NULL;\n\t\tif (!cfg) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free_data;\n\t\t}\n\n\t\tret = -ENOMEM;\n\t\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\t\tif (!data)\n\t\t\tgoto out_free_data;\n\n\t\tif (!AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir))\n\t\t\tdata->entry = kzalloc(sizeof(union irte), GFP_KERNEL);\n\t\telse\n\t\t\tdata->entry = kzalloc(sizeof(struct irte_ga),\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!data->entry) {\n\t\t\tkfree(data);\n\t\t\tgoto out_free_data;\n\t\t}\n\n\t\tdata->iommu = iommu;\n\t\tirq_data->hwirq = (devid << 16) + i;\n\t\tirq_data->chip_data = data;\n\t\tirq_data->chip = &amd_ir_chip;\n\t\tirq_remapping_prepare_irte(data, cfg, info, devid, index, i);\n\t\tirq_set_status_flags(virq + i, IRQ_MOVE_PCNTXT);\n\t}\n\n\treturn 0;\n\nout_free_data:\n\tfor (i--; i >= 0; i--) {\n\t\tirq_data = irq_domain_get_irq_data(domain, virq + i);\n\t\tif (irq_data)\n\t\t\tkfree(irq_data->chip_data);\n\t}\n\tfor (i = 0; i < nr_irqs; i++)\n\t\tfree_irte(iommu, devid, index + i);\nout_free_parent:\n\tirq_domain_free_irqs_common(domain, virq, nr_irqs);\n\treturn ret;\n}\n\nstatic void irq_remapping_free(struct irq_domain *domain, unsigned int virq,\n\t\t\t       unsigned int nr_irqs)\n{\n\tstruct irq_2_irte *irte_info;\n\tstruct irq_data *irq_data;\n\tstruct amd_ir_data *data;\n\tint i;\n\n\tfor (i = 0; i < nr_irqs; i++) {\n\t\tirq_data = irq_domain_get_irq_data(domain, virq  + i);\n\t\tif (irq_data && irq_data->chip_data) {\n\t\t\tdata = irq_data->chip_data;\n\t\t\tirte_info = &data->irq_2_irte;\n\t\t\tfree_irte(data->iommu, irte_info->devid, irte_info->index);\n\t\t\tkfree(data->entry);\n\t\t\tkfree(data);\n\t\t}\n\t}\n\tirq_domain_free_irqs_common(domain, virq, nr_irqs);\n}\n\nstatic void amd_ir_update_irte(struct irq_data *irqd, struct amd_iommu *iommu,\n\t\t\t       struct amd_ir_data *ir_data,\n\t\t\t       struct irq_2_irte *irte_info,\n\t\t\t       struct irq_cfg *cfg);\n\nstatic int irq_remapping_activate(struct irq_domain *domain,\n\t\t\t\t  struct irq_data *irq_data, bool reserve)\n{\n\tstruct amd_ir_data *data = irq_data->chip_data;\n\tstruct irq_2_irte *irte_info = &data->irq_2_irte;\n\tstruct amd_iommu *iommu = data->iommu;\n\tstruct irq_cfg *cfg = irqd_cfg(irq_data);\n\n\tif (!iommu)\n\t\treturn 0;\n\n\tiommu->irte_ops->activate(iommu, data->entry, irte_info->devid,\n\t\t\t\t  irte_info->index);\n\tamd_ir_update_irte(irq_data, iommu, data, irte_info, cfg);\n\treturn 0;\n}\n\nstatic void irq_remapping_deactivate(struct irq_domain *domain,\n\t\t\t\t     struct irq_data *irq_data)\n{\n\tstruct amd_ir_data *data = irq_data->chip_data;\n\tstruct irq_2_irte *irte_info = &data->irq_2_irte;\n\tstruct amd_iommu *iommu = data->iommu;\n\n\tif (iommu)\n\t\tiommu->irte_ops->deactivate(iommu, data->entry, irte_info->devid,\n\t\t\t\t\t    irte_info->index);\n}\n\nstatic int irq_remapping_select(struct irq_domain *d, struct irq_fwspec *fwspec,\n\t\t\t\tenum irq_domain_bus_token bus_token)\n{\n\tstruct amd_iommu *iommu;\n\tint devid = -1;\n\n\tif (!amd_iommu_irq_remap)\n\t\treturn 0;\n\n\tif (x86_fwspec_is_ioapic(fwspec))\n\t\tdevid = get_ioapic_devid(fwspec->param[0]);\n\telse if (x86_fwspec_is_hpet(fwspec))\n\t\tdevid = get_hpet_devid(fwspec->param[0]);\n\n\tif (devid < 0)\n\t\treturn 0;\n\tiommu = __rlookup_amd_iommu((devid >> 16), (devid & 0xffff));\n\n\treturn iommu && iommu->ir_domain == d;\n}\n\nstatic const struct irq_domain_ops amd_ir_domain_ops = {\n\t.select = irq_remapping_select,\n\t.alloc = irq_remapping_alloc,\n\t.free = irq_remapping_free,\n\t.activate = irq_remapping_activate,\n\t.deactivate = irq_remapping_deactivate,\n};\n\nint amd_iommu_activate_guest_mode(void *data)\n{\n\tstruct amd_ir_data *ir_data = (struct amd_ir_data *)data;\n\tstruct irte_ga *entry = (struct irte_ga *) ir_data->entry;\n\tu64 valid;\n\n\tif (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) || !entry)\n\t\treturn 0;\n\n\tvalid = entry->lo.fields_vapic.valid;\n\n\tentry->lo.val = 0;\n\tentry->hi.val = 0;\n\n\tentry->lo.fields_vapic.valid       = valid;\n\tentry->lo.fields_vapic.guest_mode  = 1;\n\tentry->lo.fields_vapic.ga_log_intr = 1;\n\tentry->hi.fields.ga_root_ptr       = ir_data->ga_root_ptr;\n\tentry->hi.fields.vector            = ir_data->ga_vector;\n\tentry->lo.fields_vapic.ga_tag      = ir_data->ga_tag;\n\n\treturn modify_irte_ga(ir_data->iommu, ir_data->irq_2_irte.devid,\n\t\t\t      ir_data->irq_2_irte.index, entry);\n}\nEXPORT_SYMBOL(amd_iommu_activate_guest_mode);\n\nint amd_iommu_deactivate_guest_mode(void *data)\n{\n\tstruct amd_ir_data *ir_data = (struct amd_ir_data *)data;\n\tstruct irte_ga *entry = (struct irte_ga *) ir_data->entry;\n\tstruct irq_cfg *cfg = ir_data->cfg;\n\tu64 valid;\n\n\tif (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) ||\n\t    !entry || !entry->lo.fields_vapic.guest_mode)\n\t\treturn 0;\n\n\tvalid = entry->lo.fields_remap.valid;\n\n\tentry->lo.val = 0;\n\tentry->hi.val = 0;\n\n\tentry->lo.fields_remap.valid       = valid;\n\tentry->lo.fields_remap.dm          = apic->dest_mode_logical;\n\tentry->lo.fields_remap.int_type    = apic->delivery_mode;\n\tentry->hi.fields.vector            = cfg->vector;\n\tentry->lo.fields_remap.destination =\n\t\t\t\tAPICID_TO_IRTE_DEST_LO(cfg->dest_apicid);\n\tentry->hi.fields.destination =\n\t\t\t\tAPICID_TO_IRTE_DEST_HI(cfg->dest_apicid);\n\n\treturn modify_irte_ga(ir_data->iommu, ir_data->irq_2_irte.devid,\n\t\t\t      ir_data->irq_2_irte.index, entry);\n}\nEXPORT_SYMBOL(amd_iommu_deactivate_guest_mode);\n\nstatic int amd_ir_set_vcpu_affinity(struct irq_data *data, void *vcpu_info)\n{\n\tint ret;\n\tstruct amd_iommu_pi_data *pi_data = vcpu_info;\n\tstruct vcpu_data *vcpu_pi_info = pi_data->vcpu_data;\n\tstruct amd_ir_data *ir_data = data->chip_data;\n\tstruct irq_2_irte *irte_info = &ir_data->irq_2_irte;\n\tstruct iommu_dev_data *dev_data;\n\n\tif (ir_data->iommu == NULL)\n\t\treturn -EINVAL;\n\n\tdev_data = search_dev_data(ir_data->iommu, irte_info->devid);\n\n\t \n\tif (!dev_data || !dev_data->use_vapic)\n\t\treturn 0;\n\n\tir_data->cfg = irqd_cfg(data);\n\tpi_data->ir_data = ir_data;\n\n\t \n\tif (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir)) {\n\t\tpr_debug(\"%s: Fall back to using intr legacy remap\\n\",\n\t\t\t __func__);\n\t\tpi_data->is_guest_mode = false;\n\t}\n\n\tpi_data->prev_ga_tag = ir_data->cached_ga_tag;\n\tif (pi_data->is_guest_mode) {\n\t\tir_data->ga_root_ptr = (pi_data->base >> 12);\n\t\tir_data->ga_vector = vcpu_pi_info->vector;\n\t\tir_data->ga_tag = pi_data->ga_tag;\n\t\tret = amd_iommu_activate_guest_mode(ir_data);\n\t\tif (!ret)\n\t\t\tir_data->cached_ga_tag = pi_data->ga_tag;\n\t} else {\n\t\tret = amd_iommu_deactivate_guest_mode(ir_data);\n\n\t\t \n\t\tif (!ret)\n\t\t\tir_data->cached_ga_tag = 0;\n\t}\n\n\treturn ret;\n}\n\n\nstatic void amd_ir_update_irte(struct irq_data *irqd, struct amd_iommu *iommu,\n\t\t\t       struct amd_ir_data *ir_data,\n\t\t\t       struct irq_2_irte *irte_info,\n\t\t\t       struct irq_cfg *cfg)\n{\n\n\t \n\tiommu->irte_ops->set_affinity(iommu, ir_data->entry, irte_info->devid,\n\t\t\t\t      irte_info->index, cfg->vector,\n\t\t\t\t      cfg->dest_apicid);\n}\n\nstatic int amd_ir_set_affinity(struct irq_data *data,\n\t\t\t       const struct cpumask *mask, bool force)\n{\n\tstruct amd_ir_data *ir_data = data->chip_data;\n\tstruct irq_2_irte *irte_info = &ir_data->irq_2_irte;\n\tstruct irq_cfg *cfg = irqd_cfg(data);\n\tstruct irq_data *parent = data->parent_data;\n\tstruct amd_iommu *iommu = ir_data->iommu;\n\tint ret;\n\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tret = parent->chip->irq_set_affinity(parent, mask, force);\n\tif (ret < 0 || ret == IRQ_SET_MASK_OK_DONE)\n\t\treturn ret;\n\n\tamd_ir_update_irte(data, iommu, ir_data, irte_info, cfg);\n\t \n\tvector_schedule_cleanup(cfg);\n\n\treturn IRQ_SET_MASK_OK_DONE;\n}\n\nstatic void ir_compose_msi_msg(struct irq_data *irq_data, struct msi_msg *msg)\n{\n\tstruct amd_ir_data *ir_data = irq_data->chip_data;\n\n\t*msg = ir_data->msi_entry;\n}\n\nstatic struct irq_chip amd_ir_chip = {\n\t.name\t\t\t= \"AMD-IR\",\n\t.irq_ack\t\t= apic_ack_irq,\n\t.irq_set_affinity\t= amd_ir_set_affinity,\n\t.irq_set_vcpu_affinity\t= amd_ir_set_vcpu_affinity,\n\t.irq_compose_msi_msg\t= ir_compose_msi_msg,\n};\n\nstatic const struct msi_parent_ops amdvi_msi_parent_ops = {\n\t.supported_flags\t= X86_VECTOR_MSI_FLAGS_SUPPORTED |\n\t\t\t\t  MSI_FLAG_MULTI_PCI_MSI |\n\t\t\t\t  MSI_FLAG_PCI_IMS,\n\t.prefix\t\t\t= \"IR-\",\n\t.init_dev_msi_info\t= msi_parent_init_dev_msi_info,\n};\n\nstatic const struct msi_parent_ops virt_amdvi_msi_parent_ops = {\n\t.supported_flags\t= X86_VECTOR_MSI_FLAGS_SUPPORTED |\n\t\t\t\t  MSI_FLAG_MULTI_PCI_MSI,\n\t.prefix\t\t\t= \"vIR-\",\n\t.init_dev_msi_info\t= msi_parent_init_dev_msi_info,\n};\n\nint amd_iommu_create_irq_domain(struct amd_iommu *iommu)\n{\n\tstruct fwnode_handle *fn;\n\n\tfn = irq_domain_alloc_named_id_fwnode(\"AMD-IR\", iommu->index);\n\tif (!fn)\n\t\treturn -ENOMEM;\n\tiommu->ir_domain = irq_domain_create_hierarchy(arch_get_ir_parent_domain(), 0, 0,\n\t\t\t\t\t\t       fn, &amd_ir_domain_ops, iommu);\n\tif (!iommu->ir_domain) {\n\t\tirq_domain_free_fwnode(fn);\n\t\treturn -ENOMEM;\n\t}\n\n\tirq_domain_update_bus_token(iommu->ir_domain,  DOMAIN_BUS_AMDVI);\n\tiommu->ir_domain->flags |= IRQ_DOMAIN_FLAG_MSI_PARENT |\n\t\t\t\t   IRQ_DOMAIN_FLAG_ISOLATED_MSI;\n\n\tif (amd_iommu_np_cache)\n\t\tiommu->ir_domain->msi_parent_ops = &virt_amdvi_msi_parent_ops;\n\telse\n\t\tiommu->ir_domain->msi_parent_ops = &amdvi_msi_parent_ops;\n\n\treturn 0;\n}\n\nint amd_iommu_update_ga(int cpu, bool is_run, void *data)\n{\n\tstruct amd_ir_data *ir_data = (struct amd_ir_data *)data;\n\tstruct irte_ga *entry = (struct irte_ga *) ir_data->entry;\n\n\tif (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) ||\n\t    !entry || !entry->lo.fields_vapic.guest_mode)\n\t\treturn 0;\n\n\tif (!ir_data->iommu)\n\t\treturn -ENODEV;\n\n\tif (cpu >= 0) {\n\t\tentry->lo.fields_vapic.destination =\n\t\t\t\t\tAPICID_TO_IRTE_DEST_LO(cpu);\n\t\tentry->hi.fields.destination =\n\t\t\t\t\tAPICID_TO_IRTE_DEST_HI(cpu);\n\t}\n\tentry->lo.fields_vapic.is_run = is_run;\n\n\treturn modify_irte_ga(ir_data->iommu, ir_data->irq_2_irte.devid,\n\t\t\t      ir_data->irq_2_irte.index, entry);\n}\nEXPORT_SYMBOL(amd_iommu_update_ga);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}