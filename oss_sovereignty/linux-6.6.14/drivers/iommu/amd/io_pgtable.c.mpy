{
  "module_name": "io_pgtable.c",
  "hash_id": "ba3ed2227ef3cc83572a65a7ece02fab07302a600843fb51df612d3310523861",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/amd/io_pgtable.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"AMD-Vi: \" fmt\n#define dev_fmt(fmt)    pr_fmt(fmt)\n\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/io-pgtable.h>\n#include <linux/kernel.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/dma-mapping.h>\n\n#include <asm/barrier.h>\n\n#include \"amd_iommu_types.h\"\n#include \"amd_iommu.h\"\n\nstatic void v1_tlb_flush_all(void *cookie)\n{\n}\n\nstatic void v1_tlb_flush_walk(unsigned long iova, size_t size,\n\t\t\t\t  size_t granule, void *cookie)\n{\n}\n\nstatic void v1_tlb_add_page(struct iommu_iotlb_gather *gather,\n\t\t\t\t\t unsigned long iova, size_t granule,\n\t\t\t\t\t void *cookie)\n{\n}\n\nstatic const struct iommu_flush_ops v1_flush_ops = {\n\t.tlb_flush_all\t= v1_tlb_flush_all,\n\t.tlb_flush_walk = v1_tlb_flush_walk,\n\t.tlb_add_page\t= v1_tlb_add_page,\n};\n\n \nstatic u64 *first_pte_l7(u64 *pte, unsigned long *page_size,\n\t\t\t unsigned long *count)\n{\n\tunsigned long pte_mask, pg_size, cnt;\n\tu64 *fpte;\n\n\tpg_size  = PTE_PAGE_SIZE(*pte);\n\tcnt      = PAGE_SIZE_PTE_COUNT(pg_size);\n\tpte_mask = ~((cnt << 3) - 1);\n\tfpte     = (u64 *)(((unsigned long)pte) & pte_mask);\n\n\tif (page_size)\n\t\t*page_size = pg_size;\n\n\tif (count)\n\t\t*count = cnt;\n\n\treturn fpte;\n}\n\n \n\nstatic void free_pt_page(u64 *pt, struct list_head *freelist)\n{\n\tstruct page *p = virt_to_page(pt);\n\n\tlist_add_tail(&p->lru, freelist);\n}\n\nstatic void free_pt_lvl(u64 *pt, struct list_head *freelist, int lvl)\n{\n\tu64 *p;\n\tint i;\n\n\tfor (i = 0; i < 512; ++i) {\n\t\t \n\t\tif (!IOMMU_PTE_PRESENT(pt[i]))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (PM_PTE_LEVEL(pt[i]) == 0 ||\n\t\t    PM_PTE_LEVEL(pt[i]) == 7)\n\t\t\tcontinue;\n\n\t\t \n\t\tp = IOMMU_PTE_PAGE(pt[i]);\n\t\tif (lvl > 2)\n\t\t\tfree_pt_lvl(p, freelist, lvl - 1);\n\t\telse\n\t\t\tfree_pt_page(p, freelist);\n\t}\n\n\tfree_pt_page(pt, freelist);\n}\n\nstatic void free_sub_pt(u64 *root, int mode, struct list_head *freelist)\n{\n\tswitch (mode) {\n\tcase PAGE_MODE_NONE:\n\tcase PAGE_MODE_7_LEVEL:\n\t\tbreak;\n\tcase PAGE_MODE_1_LEVEL:\n\t\tfree_pt_page(root, freelist);\n\t\tbreak;\n\tcase PAGE_MODE_2_LEVEL:\n\tcase PAGE_MODE_3_LEVEL:\n\tcase PAGE_MODE_4_LEVEL:\n\tcase PAGE_MODE_5_LEVEL:\n\tcase PAGE_MODE_6_LEVEL:\n\t\tfree_pt_lvl(root, freelist, mode);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nvoid amd_iommu_domain_set_pgtable(struct protection_domain *domain,\n\t\t\t\t  u64 *root, int mode)\n{\n\tu64 pt_root;\n\n\t \n\tpt_root = mode & 7;\n\tpt_root |= (u64)root;\n\n\tamd_iommu_domain_set_pt_root(domain, pt_root);\n}\n\n \nstatic bool increase_address_space(struct protection_domain *domain,\n\t\t\t\t   unsigned long address,\n\t\t\t\t   gfp_t gfp)\n{\n\tunsigned long flags;\n\tbool ret = true;\n\tu64 *pte;\n\n\tpte = alloc_pgtable_page(domain->nid, gfp);\n\tif (!pte)\n\t\treturn false;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tif (address <= PM_LEVEL_SIZE(domain->iop.mode))\n\t\tgoto out;\n\n\tret = false;\n\tif (WARN_ON_ONCE(domain->iop.mode == PAGE_MODE_6_LEVEL))\n\t\tgoto out;\n\n\t*pte = PM_LEVEL_PDE(domain->iop.mode, iommu_virt_to_phys(domain->iop.root));\n\n\tdomain->iop.root  = pte;\n\tdomain->iop.mode += 1;\n\tamd_iommu_update_and_flush_device_table(domain);\n\tamd_iommu_domain_flush_complete(domain);\n\n\t \n\tamd_iommu_domain_set_pgtable(domain, pte, domain->iop.mode);\n\n\tpte = NULL;\n\tret = true;\n\nout:\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\tfree_page((unsigned long)pte);\n\n\treturn ret;\n}\n\nstatic u64 *alloc_pte(struct protection_domain *domain,\n\t\t      unsigned long address,\n\t\t      unsigned long page_size,\n\t\t      u64 **pte_page,\n\t\t      gfp_t gfp,\n\t\t      bool *updated)\n{\n\tint level, end_lvl;\n\tu64 *pte, *page;\n\n\tBUG_ON(!is_power_of_2(page_size));\n\n\twhile (address > PM_LEVEL_SIZE(domain->iop.mode)) {\n\t\t \n\t\tif (!increase_address_space(domain, address, gfp))\n\t\t\treturn NULL;\n\t}\n\n\n\tlevel   = domain->iop.mode - 1;\n\tpte     = &domain->iop.root[PM_LEVEL_INDEX(level, address)];\n\taddress = PAGE_SIZE_ALIGN(address, page_size);\n\tend_lvl = PAGE_SIZE_LEVEL(page_size);\n\n\twhile (level > end_lvl) {\n\t\tu64 __pte, __npte;\n\t\tint pte_level;\n\n\t\t__pte     = *pte;\n\t\tpte_level = PM_PTE_LEVEL(__pte);\n\n\t\t \n\t\tif (IOMMU_PTE_PRESENT(__pte) &&\n\t\t    pte_level == PAGE_MODE_7_LEVEL) {\n\t\t\tunsigned long count, i;\n\t\t\tu64 *lpte;\n\n\t\t\tlpte = first_pte_l7(pte, NULL, &count);\n\n\t\t\t \n\t\t\tfor (i = 0; i < count; ++i)\n\t\t\t\tcmpxchg64(&lpte[i], __pte, 0ULL);\n\n\t\t\t*updated = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!IOMMU_PTE_PRESENT(__pte) ||\n\t\t    pte_level == PAGE_MODE_NONE) {\n\t\t\tpage = alloc_pgtable_page(domain->nid, gfp);\n\n\t\t\tif (!page)\n\t\t\t\treturn NULL;\n\n\t\t\t__npte = PM_LEVEL_PDE(level, iommu_virt_to_phys(page));\n\n\t\t\t \n\t\t\tif (!try_cmpxchg64(pte, &__pte, __npte))\n\t\t\t\tfree_page((unsigned long)page);\n\t\t\telse if (IOMMU_PTE_PRESENT(__pte))\n\t\t\t\t*updated = true;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (pte_level != level)\n\t\t\treturn NULL;\n\n\t\tlevel -= 1;\n\n\t\tpte = IOMMU_PTE_PAGE(__pte);\n\n\t\tif (pte_page && level == end_lvl)\n\t\t\t*pte_page = pte;\n\n\t\tpte = &pte[PM_LEVEL_INDEX(level, address)];\n\t}\n\n\treturn pte;\n}\n\n \nstatic u64 *fetch_pte(struct amd_io_pgtable *pgtable,\n\t\t      unsigned long address,\n\t\t      unsigned long *page_size)\n{\n\tint level;\n\tu64 *pte;\n\n\t*page_size = 0;\n\n\tif (address > PM_LEVEL_SIZE(pgtable->mode))\n\t\treturn NULL;\n\n\tlevel\t   =  pgtable->mode - 1;\n\tpte\t   = &pgtable->root[PM_LEVEL_INDEX(level, address)];\n\t*page_size =  PTE_LEVEL_PAGE_SIZE(level);\n\n\twhile (level > 0) {\n\n\t\t \n\t\tif (!IOMMU_PTE_PRESENT(*pte))\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (PM_PTE_LEVEL(*pte) == PAGE_MODE_7_LEVEL ||\n\t\t    PM_PTE_LEVEL(*pte) == PAGE_MODE_NONE)\n\t\t\tbreak;\n\n\t\t \n\t\tif (PM_PTE_LEVEL(*pte) != level)\n\t\t\treturn NULL;\n\n\t\tlevel -= 1;\n\n\t\t \n\t\tpte\t   = IOMMU_PTE_PAGE(*pte);\n\t\tpte\t   = &pte[PM_LEVEL_INDEX(level, address)];\n\t\t*page_size = PTE_LEVEL_PAGE_SIZE(level);\n\t}\n\n\t \n\tif (PM_PTE_LEVEL(*pte) == PAGE_MODE_7_LEVEL)\n\t\tpte = first_pte_l7(pte, page_size, NULL);\n\n\treturn pte;\n}\n\nstatic void free_clear_pte(u64 *pte, u64 pteval, struct list_head *freelist)\n{\n\tu64 *pt;\n\tint mode;\n\n\twhile (!try_cmpxchg64(pte, &pteval, 0))\n\t\tpr_warn(\"AMD-Vi: IOMMU pte changed since we read it\\n\");\n\n\tif (!IOMMU_PTE_PRESENT(pteval))\n\t\treturn;\n\n\tpt   = IOMMU_PTE_PAGE(pteval);\n\tmode = IOMMU_PTE_MODE(pteval);\n\n\tfree_sub_pt(pt, mode, freelist);\n}\n\n \nstatic int iommu_v1_map_pages(struct io_pgtable_ops *ops, unsigned long iova,\n\t\t\t      phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t      int prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct protection_domain *dom = io_pgtable_ops_to_domain(ops);\n\tLIST_HEAD(freelist);\n\tbool updated = false;\n\tu64 __pte, *pte;\n\tint ret, i, count;\n\n\tBUG_ON(!IS_ALIGNED(iova, pgsize));\n\tBUG_ON(!IS_ALIGNED(paddr, pgsize));\n\n\tret = -EINVAL;\n\tif (!(prot & IOMMU_PROT_MASK))\n\t\tgoto out;\n\n\twhile (pgcount > 0) {\n\t\tcount = PAGE_SIZE_PTE_COUNT(pgsize);\n\t\tpte   = alloc_pte(dom, iova, pgsize, NULL, gfp, &updated);\n\n\t\tret = -ENOMEM;\n\t\tif (!pte)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < count; ++i)\n\t\t\tfree_clear_pte(&pte[i], pte[i], &freelist);\n\n\t\tif (!list_empty(&freelist))\n\t\t\tupdated = true;\n\n\t\tif (count > 1) {\n\t\t\t__pte = PAGE_SIZE_PTE(__sme_set(paddr), pgsize);\n\t\t\t__pte |= PM_LEVEL_ENC(7) | IOMMU_PTE_PR | IOMMU_PTE_FC;\n\t\t} else\n\t\t\t__pte = __sme_set(paddr) | IOMMU_PTE_PR | IOMMU_PTE_FC;\n\n\t\tif (prot & IOMMU_PROT_IR)\n\t\t\t__pte |= IOMMU_PTE_IR;\n\t\tif (prot & IOMMU_PROT_IW)\n\t\t\t__pte |= IOMMU_PTE_IW;\n\n\t\tfor (i = 0; i < count; ++i)\n\t\t\tpte[i] = __pte;\n\n\t\tiova  += pgsize;\n\t\tpaddr += pgsize;\n\t\tpgcount--;\n\t\tif (mapped)\n\t\t\t*mapped += pgsize;\n\t}\n\n\tret = 0;\n\nout:\n\tif (updated) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&dom->lock, flags);\n\t\t \n\t\tamd_iommu_domain_flush_tlb_pde(dom);\n\t\tamd_iommu_domain_flush_complete(dom);\n\t\tspin_unlock_irqrestore(&dom->lock, flags);\n\t}\n\n\t \n\tput_pages_list(&freelist);\n\n\treturn ret;\n}\n\nstatic unsigned long iommu_v1_unmap_pages(struct io_pgtable_ops *ops,\n\t\t\t\t\t  unsigned long iova,\n\t\t\t\t\t  size_t pgsize, size_t pgcount,\n\t\t\t\t\t  struct iommu_iotlb_gather *gather)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_ops_to_data(ops);\n\tunsigned long long unmapped;\n\tunsigned long unmap_size;\n\tu64 *pte;\n\tsize_t size = pgcount << __ffs(pgsize);\n\n\tBUG_ON(!is_power_of_2(pgsize));\n\n\tunmapped = 0;\n\n\twhile (unmapped < size) {\n\t\tpte = fetch_pte(pgtable, iova, &unmap_size);\n\t\tif (pte) {\n\t\t\tint i, count;\n\n\t\t\tcount = PAGE_SIZE_PTE_COUNT(unmap_size);\n\t\t\tfor (i = 0; i < count; i++)\n\t\t\t\tpte[i] = 0ULL;\n\t\t} else {\n\t\t\treturn unmapped;\n\t\t}\n\n\t\tiova = (iova & ~(unmap_size - 1)) + unmap_size;\n\t\tunmapped += unmap_size;\n\t}\n\n\treturn unmapped;\n}\n\nstatic phys_addr_t iommu_v1_iova_to_phys(struct io_pgtable_ops *ops, unsigned long iova)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_ops_to_data(ops);\n\tunsigned long offset_mask, pte_pgsize;\n\tu64 *pte, __pte;\n\n\tpte = fetch_pte(pgtable, iova, &pte_pgsize);\n\n\tif (!pte || !IOMMU_PTE_PRESENT(*pte))\n\t\treturn 0;\n\n\toffset_mask = pte_pgsize - 1;\n\t__pte\t    = __sme_clr(*pte & PM_ADDR_MASK);\n\n\treturn (__pte & ~offset_mask) | (iova & offset_mask);\n}\n\n \nstatic void v1_free_pgtable(struct io_pgtable *iop)\n{\n\tstruct amd_io_pgtable *pgtable = container_of(iop, struct amd_io_pgtable, iop);\n\tstruct protection_domain *dom;\n\tLIST_HEAD(freelist);\n\n\tif (pgtable->mode == PAGE_MODE_NONE)\n\t\treturn;\n\n\tdom = container_of(pgtable, struct protection_domain, iop);\n\n\t \n\tBUG_ON(pgtable->mode < PAGE_MODE_NONE ||\n\t       pgtable->mode > PAGE_MODE_6_LEVEL);\n\n\tfree_sub_pt(pgtable->root, pgtable->mode, &freelist);\n\n\t \n\tamd_iommu_domain_clr_pt_root(dom);\n\n\t \n\tamd_iommu_domain_update(dom);\n\n\tput_pages_list(&freelist);\n}\n\nstatic struct io_pgtable *v1_alloc_pgtable(struct io_pgtable_cfg *cfg, void *cookie)\n{\n\tstruct amd_io_pgtable *pgtable = io_pgtable_cfg_to_data(cfg);\n\n\tcfg->pgsize_bitmap  = AMD_IOMMU_PGSIZES,\n\tcfg->ias            = IOMMU_IN_ADDR_BIT_SIZE,\n\tcfg->oas            = IOMMU_OUT_ADDR_BIT_SIZE,\n\tcfg->tlb            = &v1_flush_ops;\n\n\tpgtable->iop.ops.map_pages    = iommu_v1_map_pages;\n\tpgtable->iop.ops.unmap_pages  = iommu_v1_unmap_pages;\n\tpgtable->iop.ops.iova_to_phys = iommu_v1_iova_to_phys;\n\n\treturn &pgtable->iop;\n}\n\nstruct io_pgtable_init_fns io_pgtable_amd_iommu_v1_init_fns = {\n\t.alloc\t= v1_alloc_pgtable,\n\t.free\t= v1_free_pgtable,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}