{
  "module_name": "dmar.c",
  "hash_id": "3ec75d0da5f6561e5660b77d58270f85d4767d3262f50c2dd07c9253aed41d87",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/intel/dmar.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"DMAR: \" fmt\n\n#include <linux/pci.h>\n#include <linux/dmar.h>\n#include <linux/iova.h>\n#include <linux/timer.h>\n#include <linux/irq.h>\n#include <linux/interrupt.h>\n#include <linux/tboot.h>\n#include <linux/dmi.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/numa.h>\n#include <linux/limits.h>\n#include <asm/irq_remapping.h>\n\n#include \"iommu.h\"\n#include \"../irq_remapping.h\"\n#include \"perf.h\"\n#include \"trace.h\"\n#include \"perfmon.h\"\n\ntypedef int (*dmar_res_handler_t)(struct acpi_dmar_header *, void *);\nstruct dmar_res_callback {\n\tdmar_res_handler_t\tcb[ACPI_DMAR_TYPE_RESERVED];\n\tvoid\t\t\t*arg[ACPI_DMAR_TYPE_RESERVED];\n\tbool\t\t\tignore_unhandled;\n\tbool\t\t\tprint_entry;\n};\n\n \nDECLARE_RWSEM(dmar_global_lock);\nLIST_HEAD(dmar_drhd_units);\n\nstruct acpi_table_header * __initdata dmar_tbl;\nstatic int dmar_dev_scope_status = 1;\nstatic DEFINE_IDA(dmar_seq_ids);\n\nstatic int alloc_iommu(struct dmar_drhd_unit *drhd);\nstatic void free_iommu(struct intel_iommu *iommu);\n\nstatic void dmar_register_drhd_unit(struct dmar_drhd_unit *drhd)\n{\n\t \n\tif (drhd->include_all)\n\t\tlist_add_tail_rcu(&drhd->list, &dmar_drhd_units);\n\telse\n\t\tlist_add_rcu(&drhd->list, &dmar_drhd_units);\n}\n\nvoid *dmar_alloc_dev_scope(void *start, void *end, int *cnt)\n{\n\tstruct acpi_dmar_device_scope *scope;\n\n\t*cnt = 0;\n\twhile (start < end) {\n\t\tscope = start;\n\t\tif (scope->entry_type == ACPI_DMAR_SCOPE_TYPE_NAMESPACE ||\n\t\t    scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT ||\n\t\t    scope->entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE)\n\t\t\t(*cnt)++;\n\t\telse if (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC &&\n\t\t\tscope->entry_type != ACPI_DMAR_SCOPE_TYPE_HPET) {\n\t\t\tpr_warn(\"Unsupported device scope\\n\");\n\t\t}\n\t\tstart += scope->length;\n\t}\n\tif (*cnt == 0)\n\t\treturn NULL;\n\n\treturn kcalloc(*cnt, sizeof(struct dmar_dev_scope), GFP_KERNEL);\n}\n\nvoid dmar_free_dev_scope(struct dmar_dev_scope **devices, int *cnt)\n{\n\tint i;\n\tstruct device *tmp_dev;\n\n\tif (*devices && *cnt) {\n\t\tfor_each_active_dev_scope(*devices, *cnt, i, tmp_dev)\n\t\t\tput_device(tmp_dev);\n\t\tkfree(*devices);\n\t}\n\n\t*devices = NULL;\n\t*cnt = 0;\n}\n\n \nstatic char dmar_pci_notify_info_buf[64];\n\nstatic struct dmar_pci_notify_info *\ndmar_alloc_pci_notify_info(struct pci_dev *dev, unsigned long event)\n{\n\tint level = 0;\n\tsize_t size;\n\tstruct pci_dev *tmp;\n\tstruct dmar_pci_notify_info *info;\n\n\t \n\tif (pci_domain_nr(dev->bus) > U16_MAX)\n\t\treturn NULL;\n\n\t \n\tif (event == BUS_NOTIFY_ADD_DEVICE)\n\t\tfor (tmp = dev; tmp; tmp = tmp->bus->self)\n\t\t\tlevel++;\n\n\tsize = struct_size(info, path, level);\n\tif (size <= sizeof(dmar_pci_notify_info_buf)) {\n\t\tinfo = (struct dmar_pci_notify_info *)dmar_pci_notify_info_buf;\n\t} else {\n\t\tinfo = kzalloc(size, GFP_KERNEL);\n\t\tif (!info) {\n\t\t\tif (dmar_dev_scope_status == 0)\n\t\t\t\tdmar_dev_scope_status = -ENOMEM;\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tinfo->event = event;\n\tinfo->dev = dev;\n\tinfo->seg = pci_domain_nr(dev->bus);\n\tinfo->level = level;\n\tif (event == BUS_NOTIFY_ADD_DEVICE) {\n\t\tfor (tmp = dev; tmp; tmp = tmp->bus->self) {\n\t\t\tlevel--;\n\t\t\tinfo->path[level].bus = tmp->bus->number;\n\t\t\tinfo->path[level].device = PCI_SLOT(tmp->devfn);\n\t\t\tinfo->path[level].function = PCI_FUNC(tmp->devfn);\n\t\t\tif (pci_is_root_bus(tmp->bus))\n\t\t\t\tinfo->bus = tmp->bus->number;\n\t\t}\n\t}\n\n\treturn info;\n}\n\nstatic inline void dmar_free_pci_notify_info(struct dmar_pci_notify_info *info)\n{\n\tif ((void *)info != dmar_pci_notify_info_buf)\n\t\tkfree(info);\n}\n\nstatic bool dmar_match_pci_path(struct dmar_pci_notify_info *info, int bus,\n\t\t\t\tstruct acpi_dmar_pci_path *path, int count)\n{\n\tint i;\n\n\tif (info->bus != bus)\n\t\tgoto fallback;\n\tif (info->level != count)\n\t\tgoto fallback;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (path[i].device != info->path[i].device ||\n\t\t    path[i].function != info->path[i].function)\n\t\t\tgoto fallback;\n\t}\n\n\treturn true;\n\nfallback:\n\n\tif (count != 1)\n\t\treturn false;\n\n\ti = info->level - 1;\n\tif (bus              == info->path[i].bus &&\n\t    path[0].device   == info->path[i].device &&\n\t    path[0].function == info->path[i].function) {\n\t\tpr_info(FW_BUG \"RMRR entry for device %02x:%02x.%x is broken - applying workaround\\n\",\n\t\t\tbus, path[0].device, path[0].function);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nint dmar_insert_dev_scope(struct dmar_pci_notify_info *info,\n\t\t\t  void *start, void*end, u16 segment,\n\t\t\t  struct dmar_dev_scope *devices,\n\t\t\t  int devices_cnt)\n{\n\tint i, level;\n\tstruct device *tmp, *dev = &info->dev->dev;\n\tstruct acpi_dmar_device_scope *scope;\n\tstruct acpi_dmar_pci_path *path;\n\n\tif (segment != info->seg)\n\t\treturn 0;\n\n\tfor (; start < end; start += scope->length) {\n\t\tscope = start;\n\t\tif (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_ENDPOINT &&\n\t\t    scope->entry_type != ACPI_DMAR_SCOPE_TYPE_BRIDGE)\n\t\t\tcontinue;\n\n\t\tpath = (struct acpi_dmar_pci_path *)(scope + 1);\n\t\tlevel = (scope->length - sizeof(*scope)) / sizeof(*path);\n\t\tif (!dmar_match_pci_path(info, scope->bus, path, level))\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT &&\n\t\t     info->dev->hdr_type != PCI_HEADER_TYPE_NORMAL) ||\n\t\t    (scope->entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE &&\n\t\t     (info->dev->hdr_type == PCI_HEADER_TYPE_NORMAL &&\n\t\t      info->dev->class >> 16 != PCI_BASE_CLASS_BRIDGE))) {\n\t\t\tpr_warn(\"Device scope type does not match for %s\\n\",\n\t\t\t\tpci_name(info->dev));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor_each_dev_scope(devices, devices_cnt, i, tmp)\n\t\t\tif (tmp == NULL) {\n\t\t\t\tdevices[i].bus = info->dev->bus->number;\n\t\t\t\tdevices[i].devfn = info->dev->devfn;\n\t\t\t\trcu_assign_pointer(devices[i].dev,\n\t\t\t\t\t\t   get_device(dev));\n\t\t\t\treturn 1;\n\t\t\t}\n\t\tif (WARN_ON(i >= devices_cnt))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint dmar_remove_dev_scope(struct dmar_pci_notify_info *info, u16 segment,\n\t\t\t  struct dmar_dev_scope *devices, int count)\n{\n\tint index;\n\tstruct device *tmp;\n\n\tif (info->seg != segment)\n\t\treturn 0;\n\n\tfor_each_active_dev_scope(devices, count, index, tmp)\n\t\tif (tmp == &info->dev->dev) {\n\t\t\tRCU_INIT_POINTER(devices[index].dev, NULL);\n\t\t\tsynchronize_rcu();\n\t\t\tput_device(tmp);\n\t\t\treturn 1;\n\t\t}\n\n\treturn 0;\n}\n\nstatic int dmar_pci_bus_add_dev(struct dmar_pci_notify_info *info)\n{\n\tint ret = 0;\n\tstruct dmar_drhd_unit *dmaru;\n\tstruct acpi_dmar_hardware_unit *drhd;\n\n\tfor_each_drhd_unit(dmaru) {\n\t\tif (dmaru->include_all)\n\t\t\tcontinue;\n\n\t\tdrhd = container_of(dmaru->hdr,\n\t\t\t\t    struct acpi_dmar_hardware_unit, header);\n\t\tret = dmar_insert_dev_scope(info, (void *)(drhd + 1),\n\t\t\t\t((void *)drhd) + drhd->header.length,\n\t\t\t\tdmaru->segment,\n\t\t\t\tdmaru->devices, dmaru->devices_cnt);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (ret >= 0)\n\t\tret = dmar_iommu_notify_scope_dev(info);\n\tif (ret < 0 && dmar_dev_scope_status == 0)\n\t\tdmar_dev_scope_status = ret;\n\n\tif (ret >= 0)\n\t\tintel_irq_remap_add_device(info);\n\n\treturn ret;\n}\n\nstatic void  dmar_pci_bus_del_dev(struct dmar_pci_notify_info *info)\n{\n\tstruct dmar_drhd_unit *dmaru;\n\n\tfor_each_drhd_unit(dmaru)\n\t\tif (dmar_remove_dev_scope(info, dmaru->segment,\n\t\t\tdmaru->devices, dmaru->devices_cnt))\n\t\t\tbreak;\n\tdmar_iommu_notify_scope_dev(info);\n}\n\nstatic inline void vf_inherit_msi_domain(struct pci_dev *pdev)\n{\n\tstruct pci_dev *physfn = pci_physfn(pdev);\n\n\tdev_set_msi_domain(&pdev->dev, dev_get_msi_domain(&physfn->dev));\n}\n\nstatic int dmar_pci_bus_notifier(struct notifier_block *nb,\n\t\t\t\t unsigned long action, void *data)\n{\n\tstruct pci_dev *pdev = to_pci_dev(data);\n\tstruct dmar_pci_notify_info *info;\n\n\t \n\tif (pdev->is_virtfn) {\n\t\t \n\t\tif (action == BUS_NOTIFY_ADD_DEVICE)\n\t\t\tvf_inherit_msi_domain(pdev);\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (action != BUS_NOTIFY_ADD_DEVICE &&\n\t    action != BUS_NOTIFY_REMOVED_DEVICE)\n\t\treturn NOTIFY_DONE;\n\n\tinfo = dmar_alloc_pci_notify_info(pdev, action);\n\tif (!info)\n\t\treturn NOTIFY_DONE;\n\n\tdown_write(&dmar_global_lock);\n\tif (action == BUS_NOTIFY_ADD_DEVICE)\n\t\tdmar_pci_bus_add_dev(info);\n\telse if (action == BUS_NOTIFY_REMOVED_DEVICE)\n\t\tdmar_pci_bus_del_dev(info);\n\tup_write(&dmar_global_lock);\n\n\tdmar_free_pci_notify_info(info);\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block dmar_pci_bus_nb = {\n\t.notifier_call = dmar_pci_bus_notifier,\n\t.priority = 1,\n};\n\nstatic struct dmar_drhd_unit *\ndmar_find_dmaru(struct acpi_dmar_hardware_unit *drhd)\n{\n\tstruct dmar_drhd_unit *dmaru;\n\n\tlist_for_each_entry_rcu(dmaru, &dmar_drhd_units, list,\n\t\t\t\tdmar_rcu_check())\n\t\tif (dmaru->segment == drhd->segment &&\n\t\t    dmaru->reg_base_addr == drhd->address)\n\t\t\treturn dmaru;\n\n\treturn NULL;\n}\n\n \nstatic int dmar_parse_one_drhd(struct acpi_dmar_header *header, void *arg)\n{\n\tstruct acpi_dmar_hardware_unit *drhd;\n\tstruct dmar_drhd_unit *dmaru;\n\tint ret;\n\n\tdrhd = (struct acpi_dmar_hardware_unit *)header;\n\tdmaru = dmar_find_dmaru(drhd);\n\tif (dmaru)\n\t\tgoto out;\n\n\tdmaru = kzalloc(sizeof(*dmaru) + header->length, GFP_KERNEL);\n\tif (!dmaru)\n\t\treturn -ENOMEM;\n\n\t \n\tdmaru->hdr = (void *)(dmaru + 1);\n\tmemcpy(dmaru->hdr, header, header->length);\n\tdmaru->reg_base_addr = drhd->address;\n\tdmaru->segment = drhd->segment;\n\t \n\tdmaru->reg_size = 1UL << (drhd->size + 12);\n\tdmaru->include_all = drhd->flags & 0x1;  \n\tdmaru->devices = dmar_alloc_dev_scope((void *)(drhd + 1),\n\t\t\t\t\t      ((void *)drhd) + drhd->header.length,\n\t\t\t\t\t      &dmaru->devices_cnt);\n\tif (dmaru->devices_cnt && dmaru->devices == NULL) {\n\t\tkfree(dmaru);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = alloc_iommu(dmaru);\n\tif (ret) {\n\t\tdmar_free_dev_scope(&dmaru->devices,\n\t\t\t\t    &dmaru->devices_cnt);\n\t\tkfree(dmaru);\n\t\treturn ret;\n\t}\n\tdmar_register_drhd_unit(dmaru);\n\nout:\n\tif (arg)\n\t\t(*(int *)arg)++;\n\n\treturn 0;\n}\n\nstatic void dmar_free_drhd(struct dmar_drhd_unit *dmaru)\n{\n\tif (dmaru->devices && dmaru->devices_cnt)\n\t\tdmar_free_dev_scope(&dmaru->devices, &dmaru->devices_cnt);\n\tif (dmaru->iommu)\n\t\tfree_iommu(dmaru->iommu);\n\tkfree(dmaru);\n}\n\nstatic int __init dmar_parse_one_andd(struct acpi_dmar_header *header,\n\t\t\t\t      void *arg)\n{\n\tstruct acpi_dmar_andd *andd = (void *)header;\n\n\t \n\tif (strnlen(andd->device_name, header->length - 8) == header->length - 8) {\n\t\tpr_warn(FW_BUG\n\t\t\t   \"Your BIOS is broken; ANDD object name is not NUL-terminated\\n\"\n\t\t\t   \"BIOS vendor: %s; Ver: %s; Product Version: %s\\n\",\n\t\t\t   dmi_get_system_info(DMI_BIOS_VENDOR),\n\t\t\t   dmi_get_system_info(DMI_BIOS_VERSION),\n\t\t\t   dmi_get_system_info(DMI_PRODUCT_VERSION));\n\t\tadd_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);\n\t\treturn -EINVAL;\n\t}\n\tpr_info(\"ANDD device: %x name: %s\\n\", andd->device_number,\n\t\tandd->device_name);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic int dmar_parse_one_rhsa(struct acpi_dmar_header *header, void *arg)\n{\n\tstruct acpi_dmar_rhsa *rhsa;\n\tstruct dmar_drhd_unit *drhd;\n\n\trhsa = (struct acpi_dmar_rhsa *)header;\n\tfor_each_drhd_unit(drhd) {\n\t\tif (drhd->reg_base_addr == rhsa->base_address) {\n\t\t\tint node = pxm_to_node(rhsa->proximity_domain);\n\n\t\t\tif (node != NUMA_NO_NODE && !node_online(node))\n\t\t\t\tnode = NUMA_NO_NODE;\n\t\t\tdrhd->iommu->node = node;\n\t\t\treturn 0;\n\t\t}\n\t}\n\tpr_warn(FW_BUG\n\t\t\"Your BIOS is broken; RHSA refers to non-existent DMAR unit at %llx\\n\"\n\t\t\"BIOS vendor: %s; Ver: %s; Product Version: %s\\n\",\n\t\trhsa->base_address,\n\t\tdmi_get_system_info(DMI_BIOS_VENDOR),\n\t\tdmi_get_system_info(DMI_BIOS_VERSION),\n\t\tdmi_get_system_info(DMI_PRODUCT_VERSION));\n\tadd_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);\n\n\treturn 0;\n}\n#else\n#define\tdmar_parse_one_rhsa\t\tdmar_res_noop\n#endif\n\nstatic void\ndmar_table_print_dmar_entry(struct acpi_dmar_header *header)\n{\n\tstruct acpi_dmar_hardware_unit *drhd;\n\tstruct acpi_dmar_reserved_memory *rmrr;\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct acpi_dmar_rhsa *rhsa;\n\tstruct acpi_dmar_satc *satc;\n\n\tswitch (header->type) {\n\tcase ACPI_DMAR_TYPE_HARDWARE_UNIT:\n\t\tdrhd = container_of(header, struct acpi_dmar_hardware_unit,\n\t\t\t\t    header);\n\t\tpr_info(\"DRHD base: %#016Lx flags: %#x\\n\",\n\t\t\t(unsigned long long)drhd->address, drhd->flags);\n\t\tbreak;\n\tcase ACPI_DMAR_TYPE_RESERVED_MEMORY:\n\t\trmrr = container_of(header, struct acpi_dmar_reserved_memory,\n\t\t\t\t    header);\n\t\tpr_info(\"RMRR base: %#016Lx end: %#016Lx\\n\",\n\t\t\t(unsigned long long)rmrr->base_address,\n\t\t\t(unsigned long long)rmrr->end_address);\n\t\tbreak;\n\tcase ACPI_DMAR_TYPE_ROOT_ATS:\n\t\tatsr = container_of(header, struct acpi_dmar_atsr, header);\n\t\tpr_info(\"ATSR flags: %#x\\n\", atsr->flags);\n\t\tbreak;\n\tcase ACPI_DMAR_TYPE_HARDWARE_AFFINITY:\n\t\trhsa = container_of(header, struct acpi_dmar_rhsa, header);\n\t\tpr_info(\"RHSA base: %#016Lx proximity domain: %#x\\n\",\n\t\t       (unsigned long long)rhsa->base_address,\n\t\t       rhsa->proximity_domain);\n\t\tbreak;\n\tcase ACPI_DMAR_TYPE_NAMESPACE:\n\t\t \n\t\tbreak;\n\tcase ACPI_DMAR_TYPE_SATC:\n\t\tsatc = container_of(header, struct acpi_dmar_satc, header);\n\t\tpr_info(\"SATC flags: 0x%x\\n\", satc->flags);\n\t\tbreak;\n\t}\n}\n\n \nstatic int __init dmar_table_detect(void)\n{\n\tacpi_status status = AE_OK;\n\n\t \n\tstatus = acpi_get_table(ACPI_SIG_DMAR, 0, &dmar_tbl);\n\n\tif (ACPI_SUCCESS(status) && !dmar_tbl) {\n\t\tpr_warn(\"Unable to map DMAR\\n\");\n\t\tstatus = AE_NOT_FOUND;\n\t}\n\n\treturn ACPI_SUCCESS(status) ? 0 : -ENOENT;\n}\n\nstatic int dmar_walk_remapping_entries(struct acpi_dmar_header *start,\n\t\t\t\t       size_t len, struct dmar_res_callback *cb)\n{\n\tstruct acpi_dmar_header *iter, *next;\n\tstruct acpi_dmar_header *end = ((void *)start) + len;\n\n\tfor (iter = start; iter < end; iter = next) {\n\t\tnext = (void *)iter + iter->length;\n\t\tif (iter->length == 0) {\n\t\t\t \n\t\t\tpr_debug(FW_BUG \"Invalid 0-length structure\\n\");\n\t\t\tbreak;\n\t\t} else if (next > end) {\n\t\t\t \n\t\t\tpr_warn(FW_BUG \"Record passes table end\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (cb->print_entry)\n\t\t\tdmar_table_print_dmar_entry(iter);\n\n\t\tif (iter->type >= ACPI_DMAR_TYPE_RESERVED) {\n\t\t\t \n\t\t\tpr_debug(\"Unknown DMAR structure type %d\\n\",\n\t\t\t\t iter->type);\n\t\t} else if (cb->cb[iter->type]) {\n\t\t\tint ret;\n\n\t\t\tret = cb->cb[iter->type](iter, cb->arg[iter->type]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else if (!cb->ignore_unhandled) {\n\t\t\tpr_warn(\"No handler for DMAR structure type %d\\n\",\n\t\t\t\titer->type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic inline int dmar_walk_dmar_table(struct acpi_table_dmar *dmar,\n\t\t\t\t       struct dmar_res_callback *cb)\n{\n\treturn dmar_walk_remapping_entries((void *)(dmar + 1),\n\t\t\tdmar->header.length - sizeof(*dmar), cb);\n}\n\n \nstatic int __init\nparse_dmar_table(void)\n{\n\tstruct acpi_table_dmar *dmar;\n\tint drhd_count = 0;\n\tint ret;\n\tstruct dmar_res_callback cb = {\n\t\t.print_entry = true,\n\t\t.ignore_unhandled = true,\n\t\t.arg[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &drhd_count,\n\t\t.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_parse_one_drhd,\n\t\t.cb[ACPI_DMAR_TYPE_RESERVED_MEMORY] = &dmar_parse_one_rmrr,\n\t\t.cb[ACPI_DMAR_TYPE_ROOT_ATS] = &dmar_parse_one_atsr,\n\t\t.cb[ACPI_DMAR_TYPE_HARDWARE_AFFINITY] = &dmar_parse_one_rhsa,\n\t\t.cb[ACPI_DMAR_TYPE_NAMESPACE] = &dmar_parse_one_andd,\n\t\t.cb[ACPI_DMAR_TYPE_SATC] = &dmar_parse_one_satc,\n\t};\n\n\t \n\tdmar_table_detect();\n\n\t \n\tdmar_tbl = tboot_get_dmar_table(dmar_tbl);\n\n\tdmar = (struct acpi_table_dmar *)dmar_tbl;\n\tif (!dmar)\n\t\treturn -ENODEV;\n\n\tif (dmar->width < PAGE_SHIFT - 1) {\n\t\tpr_warn(\"Invalid DMAR haw\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpr_info(\"Host address width %d\\n\", dmar->width + 1);\n\tret = dmar_walk_dmar_table(dmar, &cb);\n\tif (ret == 0 && drhd_count == 0)\n\t\tpr_warn(FW_BUG \"No DRHD structure found in DMAR table\\n\");\n\n\treturn ret;\n}\n\nstatic int dmar_pci_device_match(struct dmar_dev_scope devices[],\n\t\t\t\t int cnt, struct pci_dev *dev)\n{\n\tint index;\n\tstruct device *tmp;\n\n\twhile (dev) {\n\t\tfor_each_active_dev_scope(devices, cnt, index, tmp)\n\t\t\tif (dev_is_pci(tmp) && dev == to_pci_dev(tmp))\n\t\t\t\treturn 1;\n\n\t\t \n\t\tdev = dev->bus->self;\n\t}\n\n\treturn 0;\n}\n\nstruct dmar_drhd_unit *\ndmar_find_matched_drhd_unit(struct pci_dev *dev)\n{\n\tstruct dmar_drhd_unit *dmaru;\n\tstruct acpi_dmar_hardware_unit *drhd;\n\n\tdev = pci_physfn(dev);\n\n\trcu_read_lock();\n\tfor_each_drhd_unit(dmaru) {\n\t\tdrhd = container_of(dmaru->hdr,\n\t\t\t\t    struct acpi_dmar_hardware_unit,\n\t\t\t\t    header);\n\n\t\tif (dmaru->include_all &&\n\t\t    drhd->segment == pci_domain_nr(dev->bus))\n\t\t\tgoto out;\n\n\t\tif (dmar_pci_device_match(dmaru->devices,\n\t\t\t\t\t  dmaru->devices_cnt, dev))\n\t\t\tgoto out;\n\t}\n\tdmaru = NULL;\nout:\n\trcu_read_unlock();\n\n\treturn dmaru;\n}\n\nstatic void __init dmar_acpi_insert_dev_scope(u8 device_number,\n\t\t\t\t\t      struct acpi_device *adev)\n{\n\tstruct dmar_drhd_unit *dmaru;\n\tstruct acpi_dmar_hardware_unit *drhd;\n\tstruct acpi_dmar_device_scope *scope;\n\tstruct device *tmp;\n\tint i;\n\tstruct acpi_dmar_pci_path *path;\n\n\tfor_each_drhd_unit(dmaru) {\n\t\tdrhd = container_of(dmaru->hdr,\n\t\t\t\t    struct acpi_dmar_hardware_unit,\n\t\t\t\t    header);\n\n\t\tfor (scope = (void *)(drhd + 1);\n\t\t     (unsigned long)scope < ((unsigned long)drhd) + drhd->header.length;\n\t\t     scope = ((void *)scope) + scope->length) {\n\t\t\tif (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_NAMESPACE)\n\t\t\t\tcontinue;\n\t\t\tif (scope->enumeration_id != device_number)\n\t\t\t\tcontinue;\n\n\t\t\tpath = (void *)(scope + 1);\n\t\t\tpr_info(\"ACPI device \\\"%s\\\" under DMAR at %llx as %02x:%02x.%d\\n\",\n\t\t\t\tdev_name(&adev->dev), dmaru->reg_base_addr,\n\t\t\t\tscope->bus, path->device, path->function);\n\t\t\tfor_each_dev_scope(dmaru->devices, dmaru->devices_cnt, i, tmp)\n\t\t\t\tif (tmp == NULL) {\n\t\t\t\t\tdmaru->devices[i].bus = scope->bus;\n\t\t\t\t\tdmaru->devices[i].devfn = PCI_DEVFN(path->device,\n\t\t\t\t\t\t\t\t\t    path->function);\n\t\t\t\t\trcu_assign_pointer(dmaru->devices[i].dev,\n\t\t\t\t\t\t\t   get_device(&adev->dev));\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\tBUG_ON(i >= dmaru->devices_cnt);\n\t\t}\n\t}\n\tpr_warn(\"No IOMMU scope found for ANDD enumeration ID %d (%s)\\n\",\n\t\tdevice_number, dev_name(&adev->dev));\n}\n\nstatic int __init dmar_acpi_dev_scope_init(void)\n{\n\tstruct acpi_dmar_andd *andd;\n\n\tif (dmar_tbl == NULL)\n\t\treturn -ENODEV;\n\n\tfor (andd = (void *)dmar_tbl + sizeof(struct acpi_table_dmar);\n\t     ((unsigned long)andd) < ((unsigned long)dmar_tbl) + dmar_tbl->length;\n\t     andd = ((void *)andd) + andd->header.length) {\n\t\tif (andd->header.type == ACPI_DMAR_TYPE_NAMESPACE) {\n\t\t\tacpi_handle h;\n\t\t\tstruct acpi_device *adev;\n\n\t\t\tif (!ACPI_SUCCESS(acpi_get_handle(ACPI_ROOT_OBJECT,\n\t\t\t\t\t\t\t  andd->device_name,\n\t\t\t\t\t\t\t  &h))) {\n\t\t\t\tpr_err(\"Failed to find handle for ACPI object %s\\n\",\n\t\t\t\t       andd->device_name);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tadev = acpi_fetch_acpi_dev(h);\n\t\t\tif (!adev) {\n\t\t\t\tpr_err(\"Failed to get device for ACPI object %s\\n\",\n\t\t\t\t       andd->device_name);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tdmar_acpi_insert_dev_scope(andd->device_number, adev);\n\t\t}\n\t}\n\treturn 0;\n}\n\nint __init dmar_dev_scope_init(void)\n{\n\tstruct pci_dev *dev = NULL;\n\tstruct dmar_pci_notify_info *info;\n\n\tif (dmar_dev_scope_status != 1)\n\t\treturn dmar_dev_scope_status;\n\n\tif (list_empty(&dmar_drhd_units)) {\n\t\tdmar_dev_scope_status = -ENODEV;\n\t} else {\n\t\tdmar_dev_scope_status = 0;\n\n\t\tdmar_acpi_dev_scope_init();\n\n\t\tfor_each_pci_dev(dev) {\n\t\t\tif (dev->is_virtfn)\n\t\t\t\tcontinue;\n\n\t\t\tinfo = dmar_alloc_pci_notify_info(dev,\n\t\t\t\t\tBUS_NOTIFY_ADD_DEVICE);\n\t\t\tif (!info) {\n\t\t\t\tpci_dev_put(dev);\n\t\t\t\treturn dmar_dev_scope_status;\n\t\t\t} else {\n\t\t\t\tdmar_pci_bus_add_dev(info);\n\t\t\t\tdmar_free_pci_notify_info(info);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn dmar_dev_scope_status;\n}\n\nvoid __init dmar_register_bus_notifier(void)\n{\n\tbus_register_notifier(&pci_bus_type, &dmar_pci_bus_nb);\n}\n\n\nint __init dmar_table_init(void)\n{\n\tstatic int dmar_table_initialized;\n\tint ret;\n\n\tif (dmar_table_initialized == 0) {\n\t\tret = parse_dmar_table();\n\t\tif (ret < 0) {\n\t\t\tif (ret != -ENODEV)\n\t\t\t\tpr_info(\"Parse DMAR table failure.\\n\");\n\t\t} else  if (list_empty(&dmar_drhd_units)) {\n\t\t\tpr_info(\"No DMAR devices found\\n\");\n\t\t\tret = -ENODEV;\n\t\t}\n\n\t\tif (ret < 0)\n\t\t\tdmar_table_initialized = ret;\n\t\telse\n\t\t\tdmar_table_initialized = 1;\n\t}\n\n\treturn dmar_table_initialized < 0 ? dmar_table_initialized : 0;\n}\n\nstatic void warn_invalid_dmar(u64 addr, const char *message)\n{\n\tpr_warn_once(FW_BUG\n\t\t\"Your BIOS is broken; DMAR reported at address %llx%s!\\n\"\n\t\t\"BIOS vendor: %s; Ver: %s; Product Version: %s\\n\",\n\t\taddr, message,\n\t\tdmi_get_system_info(DMI_BIOS_VENDOR),\n\t\tdmi_get_system_info(DMI_BIOS_VERSION),\n\t\tdmi_get_system_info(DMI_PRODUCT_VERSION));\n\tadd_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);\n}\n\nstatic int __ref\ndmar_validate_one_drhd(struct acpi_dmar_header *entry, void *arg)\n{\n\tstruct acpi_dmar_hardware_unit *drhd;\n\tvoid __iomem *addr;\n\tu64 cap, ecap;\n\n\tdrhd = (void *)entry;\n\tif (!drhd->address) {\n\t\twarn_invalid_dmar(0, \"\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (arg)\n\t\taddr = ioremap(drhd->address, VTD_PAGE_SIZE);\n\telse\n\t\taddr = early_ioremap(drhd->address, VTD_PAGE_SIZE);\n\tif (!addr) {\n\t\tpr_warn(\"Can't validate DRHD address: %llx\\n\", drhd->address);\n\t\treturn -EINVAL;\n\t}\n\n\tcap = dmar_readq(addr + DMAR_CAP_REG);\n\tecap = dmar_readq(addr + DMAR_ECAP_REG);\n\n\tif (arg)\n\t\tiounmap(addr);\n\telse\n\t\tearly_iounmap(addr, VTD_PAGE_SIZE);\n\n\tif (cap == (uint64_t)-1 && ecap == (uint64_t)-1) {\n\t\twarn_invalid_dmar(drhd->address, \" returns all ones\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid __init detect_intel_iommu(void)\n{\n\tint ret;\n\tstruct dmar_res_callback validate_drhd_cb = {\n\t\t.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_validate_one_drhd,\n\t\t.ignore_unhandled = true,\n\t};\n\n\tdown_write(&dmar_global_lock);\n\tret = dmar_table_detect();\n\tif (!ret)\n\t\tret = dmar_walk_dmar_table((struct acpi_table_dmar *)dmar_tbl,\n\t\t\t\t\t   &validate_drhd_cb);\n\tif (!ret && !no_iommu && !iommu_detected &&\n\t    (!dmar_disabled || dmar_platform_optin())) {\n\t\tiommu_detected = 1;\n\t\t \n\t\tpci_request_acs();\n\t}\n\n#ifdef CONFIG_X86\n\tif (!ret) {\n\t\tx86_init.iommu.iommu_init = intel_iommu_init;\n\t\tx86_platform.iommu_shutdown = intel_iommu_shutdown;\n\t}\n\n#endif\n\n\tif (dmar_tbl) {\n\t\tacpi_put_table(dmar_tbl);\n\t\tdmar_tbl = NULL;\n\t}\n\tup_write(&dmar_global_lock);\n}\n\nstatic void unmap_iommu(struct intel_iommu *iommu)\n{\n\tiounmap(iommu->reg);\n\trelease_mem_region(iommu->reg_phys, iommu->reg_size);\n}\n\n \nstatic int map_iommu(struct intel_iommu *iommu, struct dmar_drhd_unit *drhd)\n{\n\tu64 phys_addr = drhd->reg_base_addr;\n\tint map_size, err=0;\n\n\tiommu->reg_phys = phys_addr;\n\tiommu->reg_size = drhd->reg_size;\n\n\tif (!request_mem_region(iommu->reg_phys, iommu->reg_size, iommu->name)) {\n\t\tpr_err(\"Can't reserve memory\\n\");\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tiommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\n\tif (!iommu->reg) {\n\t\tpr_err(\"Can't map the region\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\tiommu->cap = dmar_readq(iommu->reg + DMAR_CAP_REG);\n\tiommu->ecap = dmar_readq(iommu->reg + DMAR_ECAP_REG);\n\n\tif (iommu->cap == (uint64_t)-1 && iommu->ecap == (uint64_t)-1) {\n\t\terr = -EINVAL;\n\t\twarn_invalid_dmar(phys_addr, \" returns all ones\");\n\t\tgoto unmap;\n\t}\n\n\t \n\tmap_size = max_t(int, ecap_max_iotlb_offset(iommu->ecap),\n\t\t\t cap_max_fault_reg_offset(iommu->cap));\n\tmap_size = VTD_PAGE_ALIGN(map_size);\n\tif (map_size > iommu->reg_size) {\n\t\tiounmap(iommu->reg);\n\t\trelease_mem_region(iommu->reg_phys, iommu->reg_size);\n\t\tiommu->reg_size = map_size;\n\t\tif (!request_mem_region(iommu->reg_phys, iommu->reg_size,\n\t\t\t\t\tiommu->name)) {\n\t\t\tpr_err(\"Can't reserve memory\\n\");\n\t\t\terr = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tiommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\n\t\tif (!iommu->reg) {\n\t\t\tpr_err(\"Can't map the region\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto release;\n\t\t}\n\t}\n\n\tif (cap_ecmds(iommu->cap)) {\n\t\tint i;\n\n\t\tfor (i = 0; i < DMA_MAX_NUM_ECMDCAP; i++) {\n\t\t\tiommu->ecmdcap[i] = dmar_readq(iommu->reg + DMAR_ECCAP_REG +\n\t\t\t\t\t\t       i * DMA_ECMD_REG_STEP);\n\t\t}\n\t}\n\n\terr = 0;\n\tgoto out;\n\nunmap:\n\tiounmap(iommu->reg);\nrelease:\n\trelease_mem_region(iommu->reg_phys, iommu->reg_size);\nout:\n\treturn err;\n}\n\nstatic int alloc_iommu(struct dmar_drhd_unit *drhd)\n{\n\tstruct intel_iommu *iommu;\n\tu32 ver, sts;\n\tint agaw = -1;\n\tint msagaw = -1;\n\tint err;\n\n\tif (!drhd->reg_base_addr) {\n\t\twarn_invalid_dmar(0, \"\");\n\t\treturn -EINVAL;\n\t}\n\n\tiommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\n\tif (!iommu)\n\t\treturn -ENOMEM;\n\n\tiommu->seq_id = ida_alloc_range(&dmar_seq_ids, 0,\n\t\t\t\t\tDMAR_UNITS_SUPPORTED - 1, GFP_KERNEL);\n\tif (iommu->seq_id < 0) {\n\t\tpr_err(\"Failed to allocate seq_id\\n\");\n\t\terr = iommu->seq_id;\n\t\tgoto error;\n\t}\n\tsprintf(iommu->name, \"dmar%d\", iommu->seq_id);\n\n\terr = map_iommu(iommu, drhd);\n\tif (err) {\n\t\tpr_err(\"Failed to map %s\\n\", iommu->name);\n\t\tgoto error_free_seq_id;\n\t}\n\n\terr = -EINVAL;\n\tif (!cap_sagaw(iommu->cap) &&\n\t    (!ecap_smts(iommu->ecap) || ecap_slts(iommu->ecap))) {\n\t\tpr_info(\"%s: No supported address widths. Not attempting DMA translation.\\n\",\n\t\t\tiommu->name);\n\t\tdrhd->ignored = 1;\n\t}\n\n\tif (!drhd->ignored) {\n\t\tagaw = iommu_calculate_agaw(iommu);\n\t\tif (agaw < 0) {\n\t\t\tpr_err(\"Cannot get a valid agaw for iommu (seq_id = %d)\\n\",\n\t\t\t       iommu->seq_id);\n\t\t\tdrhd->ignored = 1;\n\t\t}\n\t}\n\tif (!drhd->ignored) {\n\t\tmsagaw = iommu_calculate_max_sagaw(iommu);\n\t\tif (msagaw < 0) {\n\t\t\tpr_err(\"Cannot get a valid max agaw for iommu (seq_id = %d)\\n\",\n\t\t\t       iommu->seq_id);\n\t\t\tdrhd->ignored = 1;\n\t\t\tagaw = -1;\n\t\t}\n\t}\n\tiommu->agaw = agaw;\n\tiommu->msagaw = msagaw;\n\tiommu->segment = drhd->segment;\n\n\tiommu->node = NUMA_NO_NODE;\n\n\tver = readl(iommu->reg + DMAR_VER_REG);\n\tpr_info(\"%s: reg_base_addr %llx ver %d:%d cap %llx ecap %llx\\n\",\n\t\tiommu->name,\n\t\t(unsigned long long)drhd->reg_base_addr,\n\t\tDMAR_VER_MAJOR(ver), DMAR_VER_MINOR(ver),\n\t\t(unsigned long long)iommu->cap,\n\t\t(unsigned long long)iommu->ecap);\n\n\t \n\tsts = readl(iommu->reg + DMAR_GSTS_REG);\n\tif (sts & DMA_GSTS_IRES)\n\t\tiommu->gcmd |= DMA_GCMD_IRE;\n\tif (sts & DMA_GSTS_TES)\n\t\tiommu->gcmd |= DMA_GCMD_TE;\n\tif (sts & DMA_GSTS_QIES)\n\t\tiommu->gcmd |= DMA_GCMD_QIE;\n\n\tif (alloc_iommu_pmu(iommu))\n\t\tpr_debug(\"Cannot alloc PMU for iommu (seq_id = %d)\\n\", iommu->seq_id);\n\n\traw_spin_lock_init(&iommu->register_lock);\n\n\t \n\tif (pasid_supported(iommu))\n\t\tiommu->iommu.max_pasids = 2UL << ecap_pss(iommu->ecap);\n\n\t \n\tif (intel_iommu_enabled && !drhd->ignored) {\n\t\terr = iommu_device_sysfs_add(&iommu->iommu, NULL,\n\t\t\t\t\t     intel_iommu_groups,\n\t\t\t\t\t     \"%s\", iommu->name);\n\t\tif (err)\n\t\t\tgoto err_unmap;\n\n\t\terr = iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);\n\t\tif (err)\n\t\t\tgoto err_sysfs;\n\n\t\tiommu_pmu_register(iommu);\n\t}\n\n\tdrhd->iommu = iommu;\n\tiommu->drhd = drhd;\n\n\treturn 0;\n\nerr_sysfs:\n\tiommu_device_sysfs_remove(&iommu->iommu);\nerr_unmap:\n\tfree_iommu_pmu(iommu);\n\tunmap_iommu(iommu);\nerror_free_seq_id:\n\tida_free(&dmar_seq_ids, iommu->seq_id);\nerror:\n\tkfree(iommu);\n\treturn err;\n}\n\nstatic void free_iommu(struct intel_iommu *iommu)\n{\n\tif (intel_iommu_enabled && !iommu->drhd->ignored) {\n\t\tiommu_pmu_unregister(iommu);\n\t\tiommu_device_unregister(&iommu->iommu);\n\t\tiommu_device_sysfs_remove(&iommu->iommu);\n\t}\n\n\tfree_iommu_pmu(iommu);\n\n\tif (iommu->irq) {\n\t\tif (iommu->pr_irq) {\n\t\t\tfree_irq(iommu->pr_irq, iommu);\n\t\t\tdmar_free_hwirq(iommu->pr_irq);\n\t\t\tiommu->pr_irq = 0;\n\t\t}\n\t\tfree_irq(iommu->irq, iommu);\n\t\tdmar_free_hwirq(iommu->irq);\n\t\tiommu->irq = 0;\n\t}\n\n\tif (iommu->qi) {\n\t\tfree_page((unsigned long)iommu->qi->desc);\n\t\tkfree(iommu->qi->desc_status);\n\t\tkfree(iommu->qi);\n\t}\n\n\tif (iommu->reg)\n\t\tunmap_iommu(iommu);\n\n\tida_free(&dmar_seq_ids, iommu->seq_id);\n\tkfree(iommu);\n}\n\n \nstatic inline void reclaim_free_desc(struct q_inval *qi)\n{\n\twhile (qi->desc_status[qi->free_tail] == QI_DONE ||\n\t       qi->desc_status[qi->free_tail] == QI_ABORT) {\n\t\tqi->desc_status[qi->free_tail] = QI_FREE;\n\t\tqi->free_tail = (qi->free_tail + 1) % QI_LENGTH;\n\t\tqi->free_cnt++;\n\t}\n}\n\nstatic const char *qi_type_string(u8 type)\n{\n\tswitch (type) {\n\tcase QI_CC_TYPE:\n\t\treturn \"Context-cache Invalidation\";\n\tcase QI_IOTLB_TYPE:\n\t\treturn \"IOTLB Invalidation\";\n\tcase QI_DIOTLB_TYPE:\n\t\treturn \"Device-TLB Invalidation\";\n\tcase QI_IEC_TYPE:\n\t\treturn \"Interrupt Entry Cache Invalidation\";\n\tcase QI_IWD_TYPE:\n\t\treturn \"Invalidation Wait\";\n\tcase QI_EIOTLB_TYPE:\n\t\treturn \"PASID-based IOTLB Invalidation\";\n\tcase QI_PC_TYPE:\n\t\treturn \"PASID-cache Invalidation\";\n\tcase QI_DEIOTLB_TYPE:\n\t\treturn \"PASID-based Device-TLB Invalidation\";\n\tcase QI_PGRP_RESP_TYPE:\n\t\treturn \"Page Group Response\";\n\tdefault:\n\t\treturn \"UNKNOWN\";\n\t}\n}\n\nstatic void qi_dump_fault(struct intel_iommu *iommu, u32 fault)\n{\n\tunsigned int head = dmar_readl(iommu->reg + DMAR_IQH_REG);\n\tu64 iqe_err = dmar_readq(iommu->reg + DMAR_IQER_REG);\n\tstruct qi_desc *desc = iommu->qi->desc + head;\n\n\tif (fault & DMA_FSTS_IQE)\n\t\tpr_err(\"VT-d detected Invalidation Queue Error: Reason %llx\",\n\t\t       DMAR_IQER_REG_IQEI(iqe_err));\n\tif (fault & DMA_FSTS_ITE)\n\t\tpr_err(\"VT-d detected Invalidation Time-out Error: SID %llx\",\n\t\t       DMAR_IQER_REG_ITESID(iqe_err));\n\tif (fault & DMA_FSTS_ICE)\n\t\tpr_err(\"VT-d detected Invalidation Completion Error: SID %llx\",\n\t\t       DMAR_IQER_REG_ICESID(iqe_err));\n\n\tpr_err(\"QI HEAD: %s qw0 = 0x%llx, qw1 = 0x%llx\\n\",\n\t       qi_type_string(desc->qw0 & 0xf),\n\t       (unsigned long long)desc->qw0,\n\t       (unsigned long long)desc->qw1);\n\n\thead = ((head >> qi_shift(iommu)) + QI_LENGTH - 1) % QI_LENGTH;\n\thead <<= qi_shift(iommu);\n\tdesc = iommu->qi->desc + head;\n\n\tpr_err(\"QI PRIOR: %s qw0 = 0x%llx, qw1 = 0x%llx\\n\",\n\t       qi_type_string(desc->qw0 & 0xf),\n\t       (unsigned long long)desc->qw0,\n\t       (unsigned long long)desc->qw1);\n}\n\nstatic int qi_check_fault(struct intel_iommu *iommu, int index, int wait_index)\n{\n\tu32 fault;\n\tint head, tail;\n\tstruct q_inval *qi = iommu->qi;\n\tint shift = qi_shift(iommu);\n\n\tif (qi->desc_status[wait_index] == QI_ABORT)\n\t\treturn -EAGAIN;\n\n\tfault = readl(iommu->reg + DMAR_FSTS_REG);\n\tif (fault & (DMA_FSTS_IQE | DMA_FSTS_ITE | DMA_FSTS_ICE))\n\t\tqi_dump_fault(iommu, fault);\n\n\t \n\tif (fault & DMA_FSTS_IQE) {\n\t\thead = readl(iommu->reg + DMAR_IQH_REG);\n\t\tif ((head >> shift) == index) {\n\t\t\tstruct qi_desc *desc = qi->desc + head;\n\n\t\t\t \n\t\t\tmemcpy(desc, qi->desc + (wait_index << shift),\n\t\t\t       1 << shift);\n\t\t\twritel(DMA_FSTS_IQE, iommu->reg + DMAR_FSTS_REG);\n\t\t\tpr_info(\"Invalidation Queue Error (IQE) cleared\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (fault & DMA_FSTS_ITE) {\n\t\thead = readl(iommu->reg + DMAR_IQH_REG);\n\t\thead = ((head >> shift) - 1 + QI_LENGTH) % QI_LENGTH;\n\t\thead |= 1;\n\t\ttail = readl(iommu->reg + DMAR_IQT_REG);\n\t\ttail = ((tail >> shift) - 1 + QI_LENGTH) % QI_LENGTH;\n\n\t\twritel(DMA_FSTS_ITE, iommu->reg + DMAR_FSTS_REG);\n\t\tpr_info(\"Invalidation Time-out Error (ITE) cleared\\n\");\n\n\t\tdo {\n\t\t\tif (qi->desc_status[head] == QI_IN_USE)\n\t\t\t\tqi->desc_status[head] = QI_ABORT;\n\t\t\thead = (head - 2 + QI_LENGTH) % QI_LENGTH;\n\t\t} while (head != tail);\n\n\t\tif (qi->desc_status[wait_index] == QI_ABORT)\n\t\t\treturn -EAGAIN;\n\t}\n\n\tif (fault & DMA_FSTS_ICE) {\n\t\twritel(DMA_FSTS_ICE, iommu->reg + DMAR_FSTS_REG);\n\t\tpr_info(\"Invalidation Completion Error (ICE) cleared\\n\");\n\t}\n\n\treturn 0;\n}\n\n \nint qi_submit_sync(struct intel_iommu *iommu, struct qi_desc *desc,\n\t\t   unsigned int count, unsigned long options)\n{\n\tstruct q_inval *qi = iommu->qi;\n\ts64 devtlb_start_ktime = 0;\n\ts64 iotlb_start_ktime = 0;\n\ts64 iec_start_ktime = 0;\n\tstruct qi_desc wait_desc;\n\tint wait_index, index;\n\tunsigned long flags;\n\tint offset, shift;\n\tint rc, i;\n\tu64 type;\n\n\tif (!qi)\n\t\treturn 0;\n\n\ttype = desc->qw0 & GENMASK_ULL(3, 0);\n\n\tif ((type == QI_IOTLB_TYPE || type == QI_EIOTLB_TYPE) &&\n\t    dmar_latency_enabled(iommu, DMAR_LATENCY_INV_IOTLB))\n\t\tiotlb_start_ktime = ktime_to_ns(ktime_get());\n\n\tif ((type == QI_DIOTLB_TYPE || type == QI_DEIOTLB_TYPE) &&\n\t    dmar_latency_enabled(iommu, DMAR_LATENCY_INV_DEVTLB))\n\t\tdevtlb_start_ktime = ktime_to_ns(ktime_get());\n\n\tif (type == QI_IEC_TYPE &&\n\t    dmar_latency_enabled(iommu, DMAR_LATENCY_INV_IEC))\n\t\tiec_start_ktime = ktime_to_ns(ktime_get());\n\nrestart:\n\trc = 0;\n\n\traw_spin_lock_irqsave(&qi->q_lock, flags);\n\t \n\twhile (qi->free_cnt < count + 2) {\n\t\traw_spin_unlock_irqrestore(&qi->q_lock, flags);\n\t\tcpu_relax();\n\t\traw_spin_lock_irqsave(&qi->q_lock, flags);\n\t}\n\n\tindex = qi->free_head;\n\twait_index = (index + count) % QI_LENGTH;\n\tshift = qi_shift(iommu);\n\n\tfor (i = 0; i < count; i++) {\n\t\toffset = ((index + i) % QI_LENGTH) << shift;\n\t\tmemcpy(qi->desc + offset, &desc[i], 1 << shift);\n\t\tqi->desc_status[(index + i) % QI_LENGTH] = QI_IN_USE;\n\t\ttrace_qi_submit(iommu, desc[i].qw0, desc[i].qw1,\n\t\t\t\tdesc[i].qw2, desc[i].qw3);\n\t}\n\tqi->desc_status[wait_index] = QI_IN_USE;\n\n\twait_desc.qw0 = QI_IWD_STATUS_DATA(QI_DONE) |\n\t\t\tQI_IWD_STATUS_WRITE | QI_IWD_TYPE;\n\tif (options & QI_OPT_WAIT_DRAIN)\n\t\twait_desc.qw0 |= QI_IWD_PRQ_DRAIN;\n\twait_desc.qw1 = virt_to_phys(&qi->desc_status[wait_index]);\n\twait_desc.qw2 = 0;\n\twait_desc.qw3 = 0;\n\n\toffset = wait_index << shift;\n\tmemcpy(qi->desc + offset, &wait_desc, 1 << shift);\n\n\tqi->free_head = (qi->free_head + count + 1) % QI_LENGTH;\n\tqi->free_cnt -= count + 1;\n\n\t \n\twritel(qi->free_head << shift, iommu->reg + DMAR_IQT_REG);\n\n\twhile (qi->desc_status[wait_index] != QI_DONE) {\n\t\t \n\t\trc = qi_check_fault(iommu, index, wait_index);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\traw_spin_unlock(&qi->q_lock);\n\t\tcpu_relax();\n\t\traw_spin_lock(&qi->q_lock);\n\t}\n\n\tfor (i = 0; i < count; i++)\n\t\tqi->desc_status[(index + i) % QI_LENGTH] = QI_DONE;\n\n\treclaim_free_desc(qi);\n\traw_spin_unlock_irqrestore(&qi->q_lock, flags);\n\n\tif (rc == -EAGAIN)\n\t\tgoto restart;\n\n\tif (iotlb_start_ktime)\n\t\tdmar_latency_update(iommu, DMAR_LATENCY_INV_IOTLB,\n\t\t\t\tktime_to_ns(ktime_get()) - iotlb_start_ktime);\n\n\tif (devtlb_start_ktime)\n\t\tdmar_latency_update(iommu, DMAR_LATENCY_INV_DEVTLB,\n\t\t\t\tktime_to_ns(ktime_get()) - devtlb_start_ktime);\n\n\tif (iec_start_ktime)\n\t\tdmar_latency_update(iommu, DMAR_LATENCY_INV_IEC,\n\t\t\t\tktime_to_ns(ktime_get()) - iec_start_ktime);\n\n\treturn rc;\n}\n\n \nvoid qi_global_iec(struct intel_iommu *iommu)\n{\n\tstruct qi_desc desc;\n\n\tdesc.qw0 = QI_IEC_TYPE;\n\tdesc.qw1 = 0;\n\tdesc.qw2 = 0;\n\tdesc.qw3 = 0;\n\n\t \n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\nvoid qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,\n\t\t      u64 type)\n{\n\tstruct qi_desc desc;\n\n\tdesc.qw0 = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)\n\t\t\t| QI_CC_GRAN(type) | QI_CC_TYPE;\n\tdesc.qw1 = 0;\n\tdesc.qw2 = 0;\n\tdesc.qw3 = 0;\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\nvoid qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,\n\t\t    unsigned int size_order, u64 type)\n{\n\tu8 dw = 0, dr = 0;\n\n\tstruct qi_desc desc;\n\tint ih = 0;\n\n\tif (cap_write_drain(iommu->cap))\n\t\tdw = 1;\n\n\tif (cap_read_drain(iommu->cap))\n\t\tdr = 1;\n\n\tdesc.qw0 = QI_IOTLB_DID(did) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)\n\t\t| QI_IOTLB_GRAN(type) | QI_IOTLB_TYPE;\n\tdesc.qw1 = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)\n\t\t| QI_IOTLB_AM(size_order);\n\tdesc.qw2 = 0;\n\tdesc.qw3 = 0;\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\nvoid qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,\n\t\t\tu16 qdep, u64 addr, unsigned mask)\n{\n\tstruct qi_desc desc;\n\n\t \n\tif (!(iommu->gcmd & DMA_GCMD_TE))\n\t\treturn;\n\n\tif (mask) {\n\t\taddr |= (1ULL << (VTD_PAGE_SHIFT + mask - 1)) - 1;\n\t\tdesc.qw1 = QI_DEV_IOTLB_ADDR(addr) | QI_DEV_IOTLB_SIZE;\n\t} else\n\t\tdesc.qw1 = QI_DEV_IOTLB_ADDR(addr);\n\n\tif (qdep >= QI_DEV_IOTLB_MAX_INVS)\n\t\tqdep = 0;\n\n\tdesc.qw0 = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |\n\t\t   QI_DIOTLB_TYPE | QI_DEV_IOTLB_PFSID(pfsid);\n\tdesc.qw2 = 0;\n\tdesc.qw3 = 0;\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\n \nvoid qi_flush_piotlb(struct intel_iommu *iommu, u16 did, u32 pasid, u64 addr,\n\t\t     unsigned long npages, bool ih)\n{\n\tstruct qi_desc desc = {.qw2 = 0, .qw3 = 0};\n\n\t \n\tif (WARN_ON(!npages)) {\n\t\tpr_err(\"Invalid input npages = %ld\\n\", npages);\n\t\treturn;\n\t}\n\n\tif (npages == -1) {\n\t\tdesc.qw0 = QI_EIOTLB_PASID(pasid) |\n\t\t\t\tQI_EIOTLB_DID(did) |\n\t\t\t\tQI_EIOTLB_GRAN(QI_GRAN_NONG_PASID) |\n\t\t\t\tQI_EIOTLB_TYPE;\n\t\tdesc.qw1 = 0;\n\t} else {\n\t\tint mask = ilog2(__roundup_pow_of_two(npages));\n\t\tunsigned long align = (1ULL << (VTD_PAGE_SHIFT + mask));\n\n\t\tif (WARN_ON_ONCE(!IS_ALIGNED(addr, align)))\n\t\t\taddr = ALIGN_DOWN(addr, align);\n\n\t\tdesc.qw0 = QI_EIOTLB_PASID(pasid) |\n\t\t\t\tQI_EIOTLB_DID(did) |\n\t\t\t\tQI_EIOTLB_GRAN(QI_GRAN_PSI_PASID) |\n\t\t\t\tQI_EIOTLB_TYPE;\n\t\tdesc.qw1 = QI_EIOTLB_ADDR(addr) |\n\t\t\t\tQI_EIOTLB_IH(ih) |\n\t\t\t\tQI_EIOTLB_AM(mask);\n\t}\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\n \nvoid qi_flush_dev_iotlb_pasid(struct intel_iommu *iommu, u16 sid, u16 pfsid,\n\t\t\t      u32 pasid,  u16 qdep, u64 addr, unsigned int size_order)\n{\n\tunsigned long mask = 1UL << (VTD_PAGE_SHIFT + size_order - 1);\n\tstruct qi_desc desc = {.qw1 = 0, .qw2 = 0, .qw3 = 0};\n\n\t \n\tif (!(iommu->gcmd & DMA_GCMD_TE))\n\t\treturn;\n\n\tdesc.qw0 = QI_DEV_EIOTLB_PASID(pasid) | QI_DEV_EIOTLB_SID(sid) |\n\t\tQI_DEV_EIOTLB_QDEP(qdep) | QI_DEIOTLB_TYPE |\n\t\tQI_DEV_IOTLB_PFSID(pfsid);\n\n\t \n\tif (!IS_ALIGNED(addr, VTD_PAGE_SIZE << size_order))\n\t\tpr_warn_ratelimited(\"Invalidate non-aligned address %llx, order %d\\n\",\n\t\t\t\t    addr, size_order);\n\n\t \n\tdesc.qw1 = QI_DEV_EIOTLB_ADDR(addr);\n\n\tif (size_order) {\n\t\t \n\t\tdesc.qw1 |= GENMASK_ULL(size_order + VTD_PAGE_SHIFT - 1,\n\t\t\t\t\tVTD_PAGE_SHIFT);\n\t\t \n\t\tdesc.qw1 &= ~mask;\n\t\t \n\t\tdesc.qw1 |= QI_DEV_EIOTLB_SIZE;\n\t}\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\nvoid qi_flush_pasid_cache(struct intel_iommu *iommu, u16 did,\n\t\t\t  u64 granu, u32 pasid)\n{\n\tstruct qi_desc desc = {.qw1 = 0, .qw2 = 0, .qw3 = 0};\n\n\tdesc.qw0 = QI_PC_PASID(pasid) | QI_PC_DID(did) |\n\t\t\tQI_PC_GRAN(granu) | QI_PC_TYPE;\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\n \nvoid dmar_disable_qi(struct intel_iommu *iommu)\n{\n\tunsigned long flags;\n\tu32 sts;\n\tcycles_t start_time = get_cycles();\n\n\tif (!ecap_qis(iommu->ecap))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flags);\n\n\tsts =  readl(iommu->reg + DMAR_GSTS_REG);\n\tif (!(sts & DMA_GSTS_QIES))\n\t\tgoto end;\n\n\t \n\twhile ((readl(iommu->reg + DMAR_IQT_REG) !=\n\t\treadl(iommu->reg + DMAR_IQH_REG)) &&\n\t\t(DMAR_OPERATION_TIMEOUT > (get_cycles() - start_time)))\n\t\tcpu_relax();\n\n\tiommu->gcmd &= ~DMA_GCMD_QIE;\n\twritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\n\n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl,\n\t\t      !(sts & DMA_GSTS_QIES), sts);\nend:\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flags);\n}\n\n \nstatic void __dmar_enable_qi(struct intel_iommu *iommu)\n{\n\tu32 sts;\n\tunsigned long flags;\n\tstruct q_inval *qi = iommu->qi;\n\tu64 val = virt_to_phys(qi->desc);\n\n\tqi->free_head = qi->free_tail = 0;\n\tqi->free_cnt = QI_LENGTH;\n\n\t \n\tif (ecap_smts(iommu->ecap))\n\t\tval |= BIT_ULL(11) | BIT_ULL(0);\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flags);\n\n\t \n\twritel(0, iommu->reg + DMAR_IQT_REG);\n\n\tdmar_writeq(iommu->reg + DMAR_IQA_REG, val);\n\n\tiommu->gcmd |= DMA_GCMD_QIE;\n\twritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl, (sts & DMA_GSTS_QIES), sts);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flags);\n}\n\n \nint dmar_enable_qi(struct intel_iommu *iommu)\n{\n\tstruct q_inval *qi;\n\tstruct page *desc_page;\n\n\tif (!ecap_qis(iommu->ecap))\n\t\treturn -ENOENT;\n\n\t \n\tif (iommu->qi)\n\t\treturn 0;\n\n\tiommu->qi = kmalloc(sizeof(*qi), GFP_ATOMIC);\n\tif (!iommu->qi)\n\t\treturn -ENOMEM;\n\n\tqi = iommu->qi;\n\n\t \n\tdesc_page = alloc_pages_node(iommu->node, GFP_ATOMIC | __GFP_ZERO,\n\t\t\t\t     !!ecap_smts(iommu->ecap));\n\tif (!desc_page) {\n\t\tkfree(qi);\n\t\tiommu->qi = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tqi->desc = page_address(desc_page);\n\n\tqi->desc_status = kcalloc(QI_LENGTH, sizeof(int), GFP_ATOMIC);\n\tif (!qi->desc_status) {\n\t\tfree_page((unsigned long) qi->desc);\n\t\tkfree(qi);\n\t\tiommu->qi = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\traw_spin_lock_init(&qi->q_lock);\n\n\t__dmar_enable_qi(iommu);\n\n\treturn 0;\n}\n\n \n\nenum faulttype {\n\tDMA_REMAP,\n\tINTR_REMAP,\n\tUNKNOWN,\n};\n\nstatic const char *dma_remap_fault_reasons[] =\n{\n\t\"Software\",\n\t\"Present bit in root entry is clear\",\n\t\"Present bit in context entry is clear\",\n\t\"Invalid context entry\",\n\t\"Access beyond MGAW\",\n\t\"PTE Write access is not set\",\n\t\"PTE Read access is not set\",\n\t\"Next page table ptr is invalid\",\n\t\"Root table address invalid\",\n\t\"Context table ptr is invalid\",\n\t\"non-zero reserved fields in RTP\",\n\t\"non-zero reserved fields in CTP\",\n\t\"non-zero reserved fields in PTE\",\n\t\"PCE for translation request specifies blocking\",\n};\n\nstatic const char * const dma_remap_sm_fault_reasons[] = {\n\t\"SM: Invalid Root Table Address\",\n\t\"SM: TTM 0 for request with PASID\",\n\t\"SM: TTM 0 for page group request\",\n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Error attempting to access Root Entry\",\n\t\"SM: Present bit in Root Entry is clear\",\n\t\"SM: Non-zero reserved field set in Root Entry\",\n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Error attempting to access Context Entry\",\n\t\"SM: Present bit in Context Entry is clear\",\n\t\"SM: Non-zero reserved field set in the Context Entry\",\n\t\"SM: Invalid Context Entry\",\n\t\"SM: DTE field in Context Entry is clear\",\n\t\"SM: PASID Enable field in Context Entry is clear\",\n\t\"SM: PASID is larger than the max in Context Entry\",\n\t\"SM: PRE field in Context-Entry is clear\",\n\t\"SM: RID_PASID field error in Context-Entry\",\n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Error attempting to access the PASID Directory Entry\",\n\t\"SM: Present bit in Directory Entry is clear\",\n\t\"SM: Non-zero reserved field set in PASID Directory Entry\",\n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Error attempting to access PASID Table Entry\",\n\t\"SM: Present bit in PASID Table Entry is clear\",\n\t\"SM: Non-zero reserved field set in PASID Table Entry\",\n\t\"SM: Invalid Scalable-Mode PASID Table Entry\",\n\t\"SM: ERE field is clear in PASID Table Entry\",\n\t\"SM: SRE field is clear in PASID Table Entry\",\n\t\"Unknown\", \"Unknown\", \n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Error attempting to access first-level paging entry\",\n\t\"SM: Present bit in first-level paging entry is clear\",\n\t\"SM: Non-zero reserved field set in first-level paging entry\",\n\t\"SM: Error attempting to access FL-PML4 entry\",\n\t\"SM: First-level entry address beyond MGAW in Nested translation\",\n\t\"SM: Read permission error in FL-PML4 entry in Nested translation\",\n\t\"SM: Read permission error in first-level paging entry in Nested translation\",\n\t\"SM: Write permission error in first-level paging entry in Nested translation\",\n\t\"SM: Error attempting to access second-level paging entry\",\n\t\"SM: Read/Write permission error in second-level paging entry\",\n\t\"SM: Non-zero reserved field set in second-level paging entry\",\n\t\"SM: Invalid second-level page table pointer\",\n\t\"SM: A/D bit update needed in second-level entry when set up in no snoop\",\n\t\"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: Address in first-level translation is not canonical\",\n\t\"SM: U/S set 0 for first-level translation with user privilege\",\n\t\"SM: No execute permission for request with PASID and ER=1\",\n\t\"SM: Address beyond the DMA hardware max\",\n\t\"SM: Second-level entry address beyond the max\",\n\t\"SM: No write permission for Write/AtomicOp request\",\n\t\"SM: No read permission for Read/AtomicOp request\",\n\t\"SM: Invalid address-interrupt address\",\n\t\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\",  \n\t\"SM: A/D bit update needed in first-level entry when set up in no snoop\",\n};\n\nstatic const char *irq_remap_fault_reasons[] =\n{\n\t\"Detected reserved fields in the decoded interrupt-remapped request\",\n\t\"Interrupt index exceeded the interrupt-remapping table size\",\n\t\"Present field in the IRTE entry is clear\",\n\t\"Error accessing interrupt-remapping table pointed by IRTA_REG\",\n\t\"Detected reserved fields in the IRTE entry\",\n\t\"Blocked a compatibility format interrupt request\",\n\t\"Blocked an interrupt request due to source-id verification failure\",\n};\n\nstatic const char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)\n{\n\tif (fault_reason >= 0x20 && (fault_reason - 0x20 <\n\t\t\t\t\tARRAY_SIZE(irq_remap_fault_reasons))) {\n\t\t*fault_type = INTR_REMAP;\n\t\treturn irq_remap_fault_reasons[fault_reason - 0x20];\n\t} else if (fault_reason >= 0x30 && (fault_reason - 0x30 <\n\t\t\tARRAY_SIZE(dma_remap_sm_fault_reasons))) {\n\t\t*fault_type = DMA_REMAP;\n\t\treturn dma_remap_sm_fault_reasons[fault_reason - 0x30];\n\t} else if (fault_reason < ARRAY_SIZE(dma_remap_fault_reasons)) {\n\t\t*fault_type = DMA_REMAP;\n\t\treturn dma_remap_fault_reasons[fault_reason];\n\t} else {\n\t\t*fault_type = UNKNOWN;\n\t\treturn \"Unknown\";\n\t}\n}\n\n\nstatic inline int dmar_msi_reg(struct intel_iommu *iommu, int irq)\n{\n\tif (iommu->irq == irq)\n\t\treturn DMAR_FECTL_REG;\n\telse if (iommu->pr_irq == irq)\n\t\treturn DMAR_PECTL_REG;\n\telse if (iommu->perf_irq == irq)\n\t\treturn DMAR_PERFINTRCTL_REG;\n\telse\n\t\tBUG();\n}\n\nvoid dmar_msi_unmask(struct irq_data *data)\n{\n\tstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\n\tint reg = dmar_msi_reg(iommu, data->irq);\n\tunsigned long flag;\n\n\t \n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\twritel(0, iommu->reg + reg);\n\t \n\treadl(iommu->reg + reg);\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\nvoid dmar_msi_mask(struct irq_data *data)\n{\n\tstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\n\tint reg = dmar_msi_reg(iommu, data->irq);\n\tunsigned long flag;\n\n\t \n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\twritel(DMA_FECTL_IM, iommu->reg + reg);\n\t \n\treadl(iommu->reg + reg);\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\nvoid dmar_msi_write(int irq, struct msi_msg *msg)\n{\n\tstruct intel_iommu *iommu = irq_get_handler_data(irq);\n\tint reg = dmar_msi_reg(iommu, irq);\n\tunsigned long flag;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\twritel(msg->data, iommu->reg + reg + 4);\n\twritel(msg->address_lo, iommu->reg + reg + 8);\n\twritel(msg->address_hi, iommu->reg + reg + 12);\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\nvoid dmar_msi_read(int irq, struct msi_msg *msg)\n{\n\tstruct intel_iommu *iommu = irq_get_handler_data(irq);\n\tint reg = dmar_msi_reg(iommu, irq);\n\tunsigned long flag;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\tmsg->data = readl(iommu->reg + reg + 4);\n\tmsg->address_lo = readl(iommu->reg + reg + 8);\n\tmsg->address_hi = readl(iommu->reg + reg + 12);\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\nstatic int dmar_fault_do_one(struct intel_iommu *iommu, int type,\n\t\tu8 fault_reason, u32 pasid, u16 source_id,\n\t\tunsigned long long addr)\n{\n\tconst char *reason;\n\tint fault_type;\n\n\treason = dmar_get_fault_reason(fault_reason, &fault_type);\n\n\tif (fault_type == INTR_REMAP) {\n\t\tpr_err(\"[INTR-REMAP] Request device [%02x:%02x.%d] fault index 0x%llx [fault reason 0x%02x] %s\\n\",\n\t\t       source_id >> 8, PCI_SLOT(source_id & 0xFF),\n\t\t       PCI_FUNC(source_id & 0xFF), addr >> 48,\n\t\t       fault_reason, reason);\n\n\t\treturn 0;\n\t}\n\n\tif (pasid == IOMMU_PASID_INVALID)\n\t\tpr_err(\"[%s NO_PASID] Request device [%02x:%02x.%d] fault addr 0x%llx [fault reason 0x%02x] %s\\n\",\n\t\t       type ? \"DMA Read\" : \"DMA Write\",\n\t\t       source_id >> 8, PCI_SLOT(source_id & 0xFF),\n\t\t       PCI_FUNC(source_id & 0xFF), addr,\n\t\t       fault_reason, reason);\n\telse\n\t\tpr_err(\"[%s PASID 0x%x] Request device [%02x:%02x.%d] fault addr 0x%llx [fault reason 0x%02x] %s\\n\",\n\t\t       type ? \"DMA Read\" : \"DMA Write\", pasid,\n\t\t       source_id >> 8, PCI_SLOT(source_id & 0xFF),\n\t\t       PCI_FUNC(source_id & 0xFF), addr,\n\t\t       fault_reason, reason);\n\n\tdmar_fault_dump_ptes(iommu, source_id, addr, pasid);\n\n\treturn 0;\n}\n\n#define PRIMARY_FAULT_REG_LEN (16)\nirqreturn_t dmar_fault(int irq, void *dev_id)\n{\n\tstruct intel_iommu *iommu = dev_id;\n\tint reg, fault_index;\n\tu32 fault_status;\n\tunsigned long flag;\n\tstatic DEFINE_RATELIMIT_STATE(rs,\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\tfault_status = readl(iommu->reg + DMAR_FSTS_REG);\n\tif (fault_status && __ratelimit(&rs))\n\t\tpr_err(\"DRHD: handling fault status reg %x\\n\", fault_status);\n\n\t \n\tif (!(fault_status & DMA_FSTS_PPF))\n\t\tgoto unlock_exit;\n\n\tfault_index = dma_fsts_fault_record_index(fault_status);\n\treg = cap_fault_reg_offset(iommu->cap);\n\twhile (1) {\n\t\t \n\t\tbool ratelimited = !__ratelimit(&rs);\n\t\tu8 fault_reason;\n\t\tu16 source_id;\n\t\tu64 guest_addr;\n\t\tu32 pasid;\n\t\tint type;\n\t\tu32 data;\n\t\tbool pasid_present;\n\n\t\t \n\t\tdata = readl(iommu->reg + reg +\n\t\t\t\tfault_index * PRIMARY_FAULT_REG_LEN + 12);\n\t\tif (!(data & DMA_FRCD_F))\n\t\t\tbreak;\n\n\t\tif (!ratelimited) {\n\t\t\tfault_reason = dma_frcd_fault_reason(data);\n\t\t\ttype = dma_frcd_type(data);\n\n\t\t\tpasid = dma_frcd_pasid_value(data);\n\t\t\tdata = readl(iommu->reg + reg +\n\t\t\t\t     fault_index * PRIMARY_FAULT_REG_LEN + 8);\n\t\t\tsource_id = dma_frcd_source_id(data);\n\n\t\t\tpasid_present = dma_frcd_pasid_present(data);\n\t\t\tguest_addr = dmar_readq(iommu->reg + reg +\n\t\t\t\t\tfault_index * PRIMARY_FAULT_REG_LEN);\n\t\t\tguest_addr = dma_frcd_page_addr(guest_addr);\n\t\t}\n\n\t\t \n\t\twritel(DMA_FRCD_F, iommu->reg + reg +\n\t\t\tfault_index * PRIMARY_FAULT_REG_LEN + 12);\n\n\t\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\n\t\tif (!ratelimited)\n\t\t\t \n\t\t\tdmar_fault_do_one(iommu, type, fault_reason,\n\t\t\t\t\t  pasid_present ? pasid : IOMMU_PASID_INVALID,\n\t\t\t\t\t  source_id, guest_addr);\n\n\t\tfault_index++;\n\t\tif (fault_index >= cap_num_fault_regs(iommu->cap))\n\t\t\tfault_index = 0;\n\t\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\t}\n\n\twritel(DMA_FSTS_PFO | DMA_FSTS_PPF | DMA_FSTS_PRO,\n\t       iommu->reg + DMAR_FSTS_REG);\n\nunlock_exit:\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\treturn IRQ_HANDLED;\n}\n\nint dmar_set_interrupt(struct intel_iommu *iommu)\n{\n\tint irq, ret;\n\n\t \n\tif (iommu->irq)\n\t\treturn 0;\n\n\tirq = dmar_alloc_hwirq(iommu->seq_id, iommu->node, iommu);\n\tif (irq > 0) {\n\t\tiommu->irq = irq;\n\t} else {\n\t\tpr_err(\"No free IRQ vectors\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);\n\tif (ret)\n\t\tpr_err(\"Can't request irq\\n\");\n\treturn ret;\n}\n\nint __init enable_drhd_fault_handling(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\n\t \n\tfor_each_iommu(iommu, drhd) {\n\t\tu32 fault_status;\n\t\tint ret = dmar_set_interrupt(iommu);\n\n\t\tif (ret) {\n\t\t\tpr_err(\"DRHD %Lx: failed to enable fault, interrupt, ret %d\\n\",\n\t\t\t       (unsigned long long)drhd->reg_base_addr, ret);\n\t\t\treturn -1;\n\t\t}\n\n\t\t \n\t\tdmar_fault(iommu->irq, iommu);\n\t\tfault_status = readl(iommu->reg + DMAR_FSTS_REG);\n\t\twritel(fault_status, iommu->reg + DMAR_FSTS_REG);\n\t}\n\n\treturn 0;\n}\n\n \nint dmar_reenable_qi(struct intel_iommu *iommu)\n{\n\tif (!ecap_qis(iommu->ecap))\n\t\treturn -ENOENT;\n\n\tif (!iommu->qi)\n\t\treturn -ENOENT;\n\n\t \n\tdmar_disable_qi(iommu);\n\t \n\t__dmar_enable_qi(iommu);\n\n\treturn 0;\n}\n\n \nint __init dmar_ir_support(void)\n{\n\tstruct acpi_table_dmar *dmar;\n\tdmar = (struct acpi_table_dmar *)dmar_tbl;\n\tif (!dmar)\n\t\treturn 0;\n\treturn dmar->flags & 0x1;\n}\n\n \nstatic inline bool dmar_in_use(void)\n{\n\treturn irq_remapping_enabled || intel_iommu_enabled;\n}\n\nstatic int __init dmar_free_unused_resources(void)\n{\n\tstruct dmar_drhd_unit *dmaru, *dmaru_n;\n\n\tif (dmar_in_use())\n\t\treturn 0;\n\n\tif (dmar_dev_scope_status != 1 && !list_empty(&dmar_drhd_units))\n\t\tbus_unregister_notifier(&pci_bus_type, &dmar_pci_bus_nb);\n\n\tdown_write(&dmar_global_lock);\n\tlist_for_each_entry_safe(dmaru, dmaru_n, &dmar_drhd_units, list) {\n\t\tlist_del(&dmaru->list);\n\t\tdmar_free_drhd(dmaru);\n\t}\n\tup_write(&dmar_global_lock);\n\n\treturn 0;\n}\n\nlate_initcall(dmar_free_unused_resources);\n\n \nstatic guid_t dmar_hp_guid =\n\tGUID_INIT(0xD8C1A3A6, 0xBE9B, 0x4C9B,\n\t\t  0x91, 0xBF, 0xC3, 0xCB, 0x81, 0xFC, 0x5D, 0xAF);\n\n \n#define\tDMAR_DSM_REV_ID\t\t\t0\n#define\tDMAR_DSM_FUNC_DRHD\t\t1\n#define\tDMAR_DSM_FUNC_ATSR\t\t2\n#define\tDMAR_DSM_FUNC_RHSA\t\t3\n#define\tDMAR_DSM_FUNC_SATC\t\t4\n\nstatic inline bool dmar_detect_dsm(acpi_handle handle, int func)\n{\n\treturn acpi_check_dsm(handle, &dmar_hp_guid, DMAR_DSM_REV_ID, 1 << func);\n}\n\nstatic int dmar_walk_dsm_resource(acpi_handle handle, int func,\n\t\t\t\t  dmar_res_handler_t handler, void *arg)\n{\n\tint ret = -ENODEV;\n\tunion acpi_object *obj;\n\tstruct acpi_dmar_header *start;\n\tstruct dmar_res_callback callback;\n\tstatic int res_type[] = {\n\t\t[DMAR_DSM_FUNC_DRHD] = ACPI_DMAR_TYPE_HARDWARE_UNIT,\n\t\t[DMAR_DSM_FUNC_ATSR] = ACPI_DMAR_TYPE_ROOT_ATS,\n\t\t[DMAR_DSM_FUNC_RHSA] = ACPI_DMAR_TYPE_HARDWARE_AFFINITY,\n\t\t[DMAR_DSM_FUNC_SATC] = ACPI_DMAR_TYPE_SATC,\n\t};\n\n\tif (!dmar_detect_dsm(handle, func))\n\t\treturn 0;\n\n\tobj = acpi_evaluate_dsm_typed(handle, &dmar_hp_guid, DMAR_DSM_REV_ID,\n\t\t\t\t      func, NULL, ACPI_TYPE_BUFFER);\n\tif (!obj)\n\t\treturn -ENODEV;\n\n\tmemset(&callback, 0, sizeof(callback));\n\tcallback.cb[res_type[func]] = handler;\n\tcallback.arg[res_type[func]] = arg;\n\tstart = (struct acpi_dmar_header *)obj->buffer.pointer;\n\tret = dmar_walk_remapping_entries(start, obj->buffer.length, &callback);\n\n\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int dmar_hp_add_drhd(struct acpi_dmar_header *header, void *arg)\n{\n\tint ret;\n\tstruct dmar_drhd_unit *dmaru;\n\n\tdmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\n\tif (!dmaru)\n\t\treturn -ENODEV;\n\n\tret = dmar_ir_hotplug(dmaru, true);\n\tif (ret == 0)\n\t\tret = dmar_iommu_hotplug(dmaru, true);\n\n\treturn ret;\n}\n\nstatic int dmar_hp_remove_drhd(struct acpi_dmar_header *header, void *arg)\n{\n\tint i, ret;\n\tstruct device *dev;\n\tstruct dmar_drhd_unit *dmaru;\n\n\tdmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\n\tif (!dmaru)\n\t\treturn 0;\n\n\t \n\tif (!dmaru->include_all && dmaru->devices && dmaru->devices_cnt) {\n\t\tfor_each_active_dev_scope(dmaru->devices,\n\t\t\t\t\t  dmaru->devices_cnt, i, dev)\n\t\t\treturn -EBUSY;\n\t}\n\n\tret = dmar_ir_hotplug(dmaru, false);\n\tif (ret == 0)\n\t\tret = dmar_iommu_hotplug(dmaru, false);\n\n\treturn ret;\n}\n\nstatic int dmar_hp_release_drhd(struct acpi_dmar_header *header, void *arg)\n{\n\tstruct dmar_drhd_unit *dmaru;\n\n\tdmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\n\tif (dmaru) {\n\t\tlist_del_rcu(&dmaru->list);\n\t\tsynchronize_rcu();\n\t\tdmar_free_drhd(dmaru);\n\t}\n\n\treturn 0;\n}\n\nstatic int dmar_hotplug_insert(acpi_handle handle)\n{\n\tint ret;\n\tint drhd_count = 0;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t     &dmar_validate_one_drhd, (void *)1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t     &dmar_parse_one_drhd, (void *)&drhd_count);\n\tif (ret == 0 && drhd_count == 0) {\n\t\tpr_warn(FW_BUG \"No DRHD structures in buffer returned by _DSM method\\n\");\n\t\tgoto out;\n\t} else if (ret) {\n\t\tgoto release_drhd;\n\t}\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_RHSA,\n\t\t\t\t     &dmar_parse_one_rhsa, NULL);\n\tif (ret)\n\t\tgoto release_drhd;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\n\t\t\t\t     &dmar_parse_one_atsr, NULL);\n\tif (ret)\n\t\tgoto release_atsr;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t     &dmar_hp_add_drhd, NULL);\n\tif (!ret)\n\t\treturn 0;\n\n\tdmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t       &dmar_hp_remove_drhd, NULL);\nrelease_atsr:\n\tdmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\n\t\t\t       &dmar_release_one_atsr, NULL);\nrelease_drhd:\n\tdmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t       &dmar_hp_release_drhd, NULL);\nout:\n\treturn ret;\n}\n\nstatic int dmar_hotplug_remove(acpi_handle handle)\n{\n\tint ret;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\n\t\t\t\t     &dmar_check_one_atsr, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t     &dmar_hp_remove_drhd, NULL);\n\tif (ret == 0) {\n\t\tWARN_ON(dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\n\t\t\t\t\t       &dmar_release_one_atsr, NULL));\n\t\tWARN_ON(dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t\t       &dmar_hp_release_drhd, NULL));\n\t} else {\n\t\tdmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\n\t\t\t\t       &dmar_hp_add_drhd, NULL);\n\t}\n\n\treturn ret;\n}\n\nstatic acpi_status dmar_get_dsm_handle(acpi_handle handle, u32 lvl,\n\t\t\t\t       void *context, void **retval)\n{\n\tacpi_handle *phdl = retval;\n\n\tif (dmar_detect_dsm(handle, DMAR_DSM_FUNC_DRHD)) {\n\t\t*phdl = handle;\n\t\treturn AE_CTRL_TERMINATE;\n\t}\n\n\treturn AE_OK;\n}\n\nstatic int dmar_device_hotplug(acpi_handle handle, bool insert)\n{\n\tint ret;\n\tacpi_handle tmp = NULL;\n\tacpi_status status;\n\n\tif (!dmar_in_use())\n\t\treturn 0;\n\n\tif (dmar_detect_dsm(handle, DMAR_DSM_FUNC_DRHD)) {\n\t\ttmp = handle;\n\t} else {\n\t\tstatus = acpi_walk_namespace(ACPI_TYPE_DEVICE, handle,\n\t\t\t\t\t     ACPI_UINT32_MAX,\n\t\t\t\t\t     dmar_get_dsm_handle,\n\t\t\t\t\t     NULL, NULL, &tmp);\n\t\tif (ACPI_FAILURE(status)) {\n\t\t\tpr_warn(\"Failed to locate _DSM method.\\n\");\n\t\t\treturn -ENXIO;\n\t\t}\n\t}\n\tif (tmp == NULL)\n\t\treturn 0;\n\n\tdown_write(&dmar_global_lock);\n\tif (insert)\n\t\tret = dmar_hotplug_insert(tmp);\n\telse\n\t\tret = dmar_hotplug_remove(tmp);\n\tup_write(&dmar_global_lock);\n\n\treturn ret;\n}\n\nint dmar_device_add(acpi_handle handle)\n{\n\treturn dmar_device_hotplug(handle, true);\n}\n\nint dmar_device_remove(acpi_handle handle)\n{\n\treturn dmar_device_hotplug(handle, false);\n}\n\n \nbool dmar_platform_optin(void)\n{\n\tstruct acpi_table_dmar *dmar;\n\tacpi_status status;\n\tbool ret;\n\n\tstatus = acpi_get_table(ACPI_SIG_DMAR, 0,\n\t\t\t\t(struct acpi_table_header **)&dmar);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tret = !!(dmar->flags & DMAR_PLATFORM_OPT_IN);\n\tacpi_put_table((struct acpi_table_header *)dmar);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dmar_platform_optin);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}