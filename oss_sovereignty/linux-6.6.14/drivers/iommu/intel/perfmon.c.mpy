{
  "module_name": "perfmon.c",
  "hash_id": "8d71410b27813d819f257cfc8dae510b485199eefd0e3209ee879f60b0a6d0f5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/intel/perfmon.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt)\t\"DMAR: \" fmt\n#define dev_fmt(fmt)\tpr_fmt(fmt)\n\n#include <linux/dmar.h>\n#include \"iommu.h\"\n#include \"perfmon.h\"\n\nPMU_FORMAT_ATTR(event,\t\t\"config:0-27\");\t\t \nPMU_FORMAT_ATTR(event_group,\t\"config:28-31\");\t \n\nstatic struct attribute *iommu_pmu_format_attrs[] = {\n\t&format_attr_event_group.attr,\n\t&format_attr_event.attr,\n\tNULL\n};\n\nstatic struct attribute_group iommu_pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = iommu_pmu_format_attrs,\n};\n\n \nstatic struct attribute *attrs_empty[] = {\n\tNULL\n};\n\nstatic struct attribute_group iommu_pmu_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = attrs_empty,\n};\n\nstatic cpumask_t iommu_pmu_cpu_mask;\n\nstatic ssize_t\ncpumask_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpumap_print_to_pagebuf(true, buf, &iommu_pmu_cpu_mask);\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *iommu_pmu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL\n};\n\nstatic struct attribute_group iommu_pmu_cpumask_attr_group = {\n\t.attrs = iommu_pmu_cpumask_attrs,\n};\n\nstatic const struct attribute_group *iommu_pmu_attr_groups[] = {\n\t&iommu_pmu_format_attr_group,\n\t&iommu_pmu_events_attr_group,\n\t&iommu_pmu_cpumask_attr_group,\n\tNULL\n};\n\nstatic inline struct iommu_pmu *dev_to_iommu_pmu(struct device *dev)\n{\n\t \n\treturn container_of(dev_get_drvdata(dev), struct iommu_pmu, pmu);\n}\n\n#define IOMMU_PMU_ATTR(_name, _format, _filter)\t\t\t\t\\\n\tPMU_FORMAT_ATTR(_name, _format);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic struct attribute *_name##_attr[] = {\t\t\t\t\\\n\t&format_attr_##_name.attr,\t\t\t\t\t\\\n\tNULL\t\t\t\t\t\t\t\t\\\n};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic umode_t\t\t\t\t\t\t\t\t\\\n_name##_is_visible(struct kobject *kobj, struct attribute *attr, int i)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct device *dev = kobj_to_dev(kobj);\t\t\t\t\\\n\tstruct iommu_pmu *iommu_pmu = dev_to_iommu_pmu(dev);\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!iommu_pmu)\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\treturn (iommu_pmu->filter & _filter) ? attr->mode : 0;\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic struct attribute_group _name = {\t\t\t\t\t\\\n\t.name\t\t= \"format\",\t\t\t\t\t\\\n\t.attrs\t\t= _name##_attr,\t\t\t\t\t\\\n\t.is_visible\t= _name##_is_visible,\t\t\t\t\\\n};\n\nIOMMU_PMU_ATTR(filter_requester_id_en,\t\"config1:0\",\t\tIOMMU_PMU_FILTER_REQUESTER_ID);\nIOMMU_PMU_ATTR(filter_domain_en,\t\"config1:1\",\t\tIOMMU_PMU_FILTER_DOMAIN);\nIOMMU_PMU_ATTR(filter_pasid_en,\t\t\"config1:2\",\t\tIOMMU_PMU_FILTER_PASID);\nIOMMU_PMU_ATTR(filter_ats_en,\t\t\"config1:3\",\t\tIOMMU_PMU_FILTER_ATS);\nIOMMU_PMU_ATTR(filter_page_table_en,\t\"config1:4\",\t\tIOMMU_PMU_FILTER_PAGE_TABLE);\nIOMMU_PMU_ATTR(filter_requester_id,\t\"config1:16-31\",\tIOMMU_PMU_FILTER_REQUESTER_ID);\nIOMMU_PMU_ATTR(filter_domain,\t\t\"config1:32-47\",\tIOMMU_PMU_FILTER_DOMAIN);\nIOMMU_PMU_ATTR(filter_pasid,\t\t\"config2:0-21\",\t\tIOMMU_PMU_FILTER_PASID);\nIOMMU_PMU_ATTR(filter_ats,\t\t\"config2:24-28\",\tIOMMU_PMU_FILTER_ATS);\nIOMMU_PMU_ATTR(filter_page_table,\t\"config2:32-36\",\tIOMMU_PMU_FILTER_PAGE_TABLE);\n\n#define iommu_pmu_en_requester_id(e)\t\t((e) & 0x1)\n#define iommu_pmu_en_domain(e)\t\t\t(((e) >> 1) & 0x1)\n#define iommu_pmu_en_pasid(e)\t\t\t(((e) >> 2) & 0x1)\n#define iommu_pmu_en_ats(e)\t\t\t(((e) >> 3) & 0x1)\n#define iommu_pmu_en_page_table(e)\t\t(((e) >> 4) & 0x1)\n#define iommu_pmu_get_requester_id(filter)\t(((filter) >> 16) & 0xffff)\n#define iommu_pmu_get_domain(filter)\t\t(((filter) >> 32) & 0xffff)\n#define iommu_pmu_get_pasid(filter)\t\t((filter) & 0x3fffff)\n#define iommu_pmu_get_ats(filter)\t\t(((filter) >> 24) & 0x1f)\n#define iommu_pmu_get_page_table(filter)\t(((filter) >> 32) & 0x1f)\n\n#define iommu_pmu_set_filter(_name, _config, _filter, _idx, _econfig)\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\\\n\tif ((iommu_pmu->filter & _filter) && iommu_pmu_en_##_name(_econfig)) {\t\\\n\t\tdmar_writel(iommu_pmu->cfg_reg + _idx * IOMMU_PMU_CFG_OFFSET +\t\\\n\t\t\t    IOMMU_PMU_CFG_SIZE +\t\t\t\t\\\n\t\t\t    (ffs(_filter) - 1) * IOMMU_PMU_CFG_FILTERS_OFFSET,\t\\\n\t\t\t    iommu_pmu_get_##_name(_config) | IOMMU_PMU_FILTER_EN);\\\n\t}\t\t\t\t\t\t\t\t\t\\\n}\n\n#define iommu_pmu_clear_filter(_filter, _idx)\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\\\n\tif (iommu_pmu->filter & _filter) {\t\t\t\t\t\\\n\t\tdmar_writel(iommu_pmu->cfg_reg + _idx * IOMMU_PMU_CFG_OFFSET +\t\\\n\t\t\t    IOMMU_PMU_CFG_SIZE +\t\t\t\t\\\n\t\t\t    (ffs(_filter) - 1) * IOMMU_PMU_CFG_FILTERS_OFFSET,\t\\\n\t\t\t    0);\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n}\n\n \n#define IOMMU_PMU_EVENT_ATTR(_name, _string, _g_idx, _event)\t\t\t\\\n\tPMU_EVENT_ATTR_STRING(_name, event_attr_##_name, _string)\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\nstatic struct attribute *_name##_attr[] = {\t\t\t\t\t\\\n\t&event_attr_##_name.attr.attr,\t\t\t\t\t\t\\\n\tNULL\t\t\t\t\t\t\t\t\t\\\n};\t\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\nstatic umode_t\t\t\t\t\t\t\t\t\t\\\n_name##_is_visible(struct kobject *kobj, struct attribute *attr, int i)\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\\\n\tstruct device *dev = kobj_to_dev(kobj);\t\t\t\t\t\\\n\tstruct iommu_pmu *iommu_pmu = dev_to_iommu_pmu(dev);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tif (!iommu_pmu)\t\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\t\\\n\treturn (iommu_pmu->evcap[_g_idx] & _event) ? attr->mode : 0;\t\t\\\n}\t\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\nstatic struct attribute_group _name = {\t\t\t\t\t\t\\\n\t.name\t\t= \"events\",\t\t\t\t\t\t\\\n\t.attrs\t\t= _name##_attr,\t\t\t\t\t\t\\\n\t.is_visible\t= _name##_is_visible,\t\t\t\t\t\\\n};\n\nIOMMU_PMU_EVENT_ATTR(iommu_clocks,\t\t\"event_group=0x0,event=0x001\", 0x0, 0x001)\nIOMMU_PMU_EVENT_ATTR(iommu_requests,\t\t\"event_group=0x0,event=0x002\", 0x0, 0x002)\nIOMMU_PMU_EVENT_ATTR(pw_occupancy,\t\t\"event_group=0x0,event=0x004\", 0x0, 0x004)\nIOMMU_PMU_EVENT_ATTR(ats_blocked,\t\t\"event_group=0x0,event=0x008\", 0x0, 0x008)\nIOMMU_PMU_EVENT_ATTR(iommu_mrds,\t\t\"event_group=0x1,event=0x001\", 0x1, 0x001)\nIOMMU_PMU_EVENT_ATTR(iommu_mem_blocked,\t\t\"event_group=0x1,event=0x020\", 0x1, 0x020)\nIOMMU_PMU_EVENT_ATTR(pg_req_posted,\t\t\"event_group=0x1,event=0x040\", 0x1, 0x040)\nIOMMU_PMU_EVENT_ATTR(ctxt_cache_lookup,\t\t\"event_group=0x2,event=0x001\", 0x2, 0x001)\nIOMMU_PMU_EVENT_ATTR(ctxt_cache_hit,\t\t\"event_group=0x2,event=0x002\", 0x2, 0x002)\nIOMMU_PMU_EVENT_ATTR(pasid_cache_lookup,\t\"event_group=0x2,event=0x004\", 0x2, 0x004)\nIOMMU_PMU_EVENT_ATTR(pasid_cache_hit,\t\t\"event_group=0x2,event=0x008\", 0x2, 0x008)\nIOMMU_PMU_EVENT_ATTR(ss_nonleaf_lookup,\t\t\"event_group=0x2,event=0x010\", 0x2, 0x010)\nIOMMU_PMU_EVENT_ATTR(ss_nonleaf_hit,\t\t\"event_group=0x2,event=0x020\", 0x2, 0x020)\nIOMMU_PMU_EVENT_ATTR(fs_nonleaf_lookup,\t\t\"event_group=0x2,event=0x040\", 0x2, 0x040)\nIOMMU_PMU_EVENT_ATTR(fs_nonleaf_hit,\t\t\"event_group=0x2,event=0x080\", 0x2, 0x080)\nIOMMU_PMU_EVENT_ATTR(hpt_nonleaf_lookup,\t\"event_group=0x2,event=0x100\", 0x2, 0x100)\nIOMMU_PMU_EVENT_ATTR(hpt_nonleaf_hit,\t\t\"event_group=0x2,event=0x200\", 0x2, 0x200)\nIOMMU_PMU_EVENT_ATTR(iotlb_lookup,\t\t\"event_group=0x3,event=0x001\", 0x3, 0x001)\nIOMMU_PMU_EVENT_ATTR(iotlb_hit,\t\t\t\"event_group=0x3,event=0x002\", 0x3, 0x002)\nIOMMU_PMU_EVENT_ATTR(hpt_leaf_lookup,\t\t\"event_group=0x3,event=0x004\", 0x3, 0x004)\nIOMMU_PMU_EVENT_ATTR(hpt_leaf_hit,\t\t\"event_group=0x3,event=0x008\", 0x3, 0x008)\nIOMMU_PMU_EVENT_ATTR(int_cache_lookup,\t\t\"event_group=0x4,event=0x001\", 0x4, 0x001)\nIOMMU_PMU_EVENT_ATTR(int_cache_hit_nonposted,\t\"event_group=0x4,event=0x002\", 0x4, 0x002)\nIOMMU_PMU_EVENT_ATTR(int_cache_hit_posted,\t\"event_group=0x4,event=0x004\", 0x4, 0x004)\n\nstatic const struct attribute_group *iommu_pmu_attr_update[] = {\n\t&filter_requester_id_en,\n\t&filter_domain_en,\n\t&filter_pasid_en,\n\t&filter_ats_en,\n\t&filter_page_table_en,\n\t&filter_requester_id,\n\t&filter_domain,\n\t&filter_pasid,\n\t&filter_ats,\n\t&filter_page_table,\n\t&iommu_clocks,\n\t&iommu_requests,\n\t&pw_occupancy,\n\t&ats_blocked,\n\t&iommu_mrds,\n\t&iommu_mem_blocked,\n\t&pg_req_posted,\n\t&ctxt_cache_lookup,\n\t&ctxt_cache_hit,\n\t&pasid_cache_lookup,\n\t&pasid_cache_hit,\n\t&ss_nonleaf_lookup,\n\t&ss_nonleaf_hit,\n\t&fs_nonleaf_lookup,\n\t&fs_nonleaf_hit,\n\t&hpt_nonleaf_lookup,\n\t&hpt_nonleaf_hit,\n\t&iotlb_lookup,\n\t&iotlb_hit,\n\t&hpt_leaf_lookup,\n\t&hpt_leaf_hit,\n\t&int_cache_lookup,\n\t&int_cache_hit_nonposted,\n\t&int_cache_hit_posted,\n\tNULL\n};\n\nstatic inline void __iomem *\niommu_event_base(struct iommu_pmu *iommu_pmu, int idx)\n{\n\treturn iommu_pmu->cntr_reg + idx * iommu_pmu->cntr_stride;\n}\n\nstatic inline void __iomem *\niommu_config_base(struct iommu_pmu *iommu_pmu, int idx)\n{\n\treturn iommu_pmu->cfg_reg + idx * IOMMU_PMU_CFG_OFFSET;\n}\n\nstatic inline struct iommu_pmu *iommu_event_to_pmu(struct perf_event *event)\n{\n\treturn container_of(event->pmu, struct iommu_pmu, pmu);\n}\n\nstatic inline u64 iommu_event_config(struct perf_event *event)\n{\n\tu64 config = event->attr.config;\n\n\treturn (iommu_event_select(config) << IOMMU_EVENT_CFG_ES_SHIFT) |\n\t       (iommu_event_group(config) << IOMMU_EVENT_CFG_EGI_SHIFT) |\n\t       IOMMU_EVENT_CFG_INT;\n}\n\nstatic inline bool is_iommu_pmu_event(struct iommu_pmu *iommu_pmu,\n\t\t\t\t      struct perf_event *event)\n{\n\treturn event->pmu == &iommu_pmu->pmu;\n}\n\nstatic int iommu_pmu_validate_event(struct perf_event *event)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tu32 event_group = iommu_event_group(event->attr.config);\n\n\tif (event_group >= iommu_pmu->num_eg)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int iommu_pmu_validate_group(struct perf_event *event)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tstruct perf_event *sibling;\n\tint nr = 0;\n\n\t \n\tfor_each_sibling_event(sibling, event->group_leader) {\n\t\tif (!is_iommu_pmu_event(iommu_pmu, sibling) ||\n\t\t    sibling->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (++nr > iommu_pmu->num_cntr)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int iommu_pmu_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (event->attr.sample_period)\n\t\treturn -EINVAL;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\tif (iommu_pmu_validate_event(event))\n\t\treturn -EINVAL;\n\n\thwc->config = iommu_event_config(event);\n\n\treturn iommu_pmu_validate_group(event);\n}\n\nstatic void iommu_pmu_event_update(struct perf_event *event)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 prev_count, new_count, delta;\n\tint shift = 64 - iommu_pmu->cntr_width;\n\nagain:\n\tprev_count = local64_read(&hwc->prev_count);\n\tnew_count = dmar_readq(iommu_event_base(iommu_pmu, hwc->idx));\n\tif (local64_xchg(&hwc->prev_count, new_count) != prev_count)\n\t\tgoto again;\n\n\t \n\tdelta = (new_count << shift) - (prev_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void iommu_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tstruct intel_iommu *iommu = iommu_pmu->iommu;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 count;\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(hwc->idx < 0 || hwc->idx >= IOMMU_PMU_IDX_MAX))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\n\t \n\tcount = dmar_readq(iommu_event_base(iommu_pmu, hwc->idx));\n\tlocal64_set((&hwc->prev_count), count);\n\n\t \n\tecmd_submit_sync(iommu, DMA_ECMD_ENABLE, hwc->idx, 0);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void iommu_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tstruct intel_iommu *iommu = iommu_pmu->iommu;\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tecmd_submit_sync(iommu, DMA_ECMD_DISABLE, hwc->idx, 0);\n\n\t\tiommu_pmu_event_update(event);\n\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic inline int\niommu_pmu_validate_per_cntr_event(struct iommu_pmu *iommu_pmu,\n\t\t\t\t  int idx, struct perf_event *event)\n{\n\tu32 event_group = iommu_event_group(event->attr.config);\n\tu32 select = iommu_event_select(event->attr.config);\n\n\tif (!(iommu_pmu->cntr_evcap[idx][event_group] & select))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int iommu_pmu_assign_event(struct iommu_pmu *iommu_pmu,\n\t\t\t\t  struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\t \n\tfor (idx = iommu_pmu->num_cntr - 1; idx >= 0; idx--) {\n\t\tif (test_and_set_bit(idx, iommu_pmu->used_mask))\n\t\t\tcontinue;\n\t\t \n\t\tif (!iommu_pmu_validate_per_cntr_event(iommu_pmu, idx, event))\n\t\t\tbreak;\n\t\tclear_bit(idx, iommu_pmu->used_mask);\n\t}\n\tif (idx < 0)\n\t\treturn -EINVAL;\n\n\tiommu_pmu->event_list[idx] = event;\n\thwc->idx = idx;\n\n\t \n\tdmar_writeq(iommu_config_base(iommu_pmu, idx), hwc->config);\n\n\tiommu_pmu_set_filter(requester_id, event->attr.config1,\n\t\t\t     IOMMU_PMU_FILTER_REQUESTER_ID, idx,\n\t\t\t     event->attr.config1);\n\tiommu_pmu_set_filter(domain, event->attr.config1,\n\t\t\t     IOMMU_PMU_FILTER_DOMAIN, idx,\n\t\t\t     event->attr.config1);\n\tiommu_pmu_set_filter(pasid, event->attr.config1,\n\t\t\t     IOMMU_PMU_FILTER_PASID, idx,\n\t\t\t     event->attr.config1);\n\tiommu_pmu_set_filter(ats, event->attr.config2,\n\t\t\t     IOMMU_PMU_FILTER_ATS, idx,\n\t\t\t     event->attr.config1);\n\tiommu_pmu_set_filter(page_table, event->attr.config2,\n\t\t\t     IOMMU_PMU_FILTER_PAGE_TABLE, idx,\n\t\t\t     event->attr.config1);\n\n\treturn 0;\n}\n\nstatic int iommu_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret;\n\n\tret = iommu_pmu_assign_event(iommu_pmu, event);\n\tif (ret < 0)\n\t\treturn ret;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\tif (flags & PERF_EF_START)\n\t\tiommu_pmu_start(event, 0);\n\n\treturn 0;\n}\n\nstatic void iommu_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu_event_to_pmu(event);\n\tint idx = event->hw.idx;\n\n\tiommu_pmu_stop(event, PERF_EF_UPDATE);\n\n\tiommu_pmu_clear_filter(IOMMU_PMU_FILTER_REQUESTER_ID, idx);\n\tiommu_pmu_clear_filter(IOMMU_PMU_FILTER_DOMAIN, idx);\n\tiommu_pmu_clear_filter(IOMMU_PMU_FILTER_PASID, idx);\n\tiommu_pmu_clear_filter(IOMMU_PMU_FILTER_ATS, idx);\n\tiommu_pmu_clear_filter(IOMMU_PMU_FILTER_PAGE_TABLE, idx);\n\n\tiommu_pmu->event_list[idx] = NULL;\n\tevent->hw.idx = -1;\n\tclear_bit(idx, iommu_pmu->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void iommu_pmu_enable(struct pmu *pmu)\n{\n\tstruct iommu_pmu *iommu_pmu = container_of(pmu, struct iommu_pmu, pmu);\n\tstruct intel_iommu *iommu = iommu_pmu->iommu;\n\n\tecmd_submit_sync(iommu, DMA_ECMD_UNFREEZE, 0, 0);\n}\n\nstatic void iommu_pmu_disable(struct pmu *pmu)\n{\n\tstruct iommu_pmu *iommu_pmu = container_of(pmu, struct iommu_pmu, pmu);\n\tstruct intel_iommu *iommu = iommu_pmu->iommu;\n\n\tecmd_submit_sync(iommu, DMA_ECMD_FREEZE, 0, 0);\n}\n\nstatic void iommu_pmu_counter_overflow(struct iommu_pmu *iommu_pmu)\n{\n\tstruct perf_event *event;\n\tu64 status;\n\tint i;\n\n\t \n\twhile ((status = dmar_readq(iommu_pmu->overflow))) {\n\t\tfor_each_set_bit(i, (unsigned long *)&status, iommu_pmu->num_cntr) {\n\t\t\t \n\t\t\tevent = iommu_pmu->event_list[i];\n\t\t\tif (!event) {\n\t\t\t\tpr_warn_once(\"Cannot find the assigned event for counter %d\\n\", i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tiommu_pmu_event_update(event);\n\t\t}\n\n\t\tdmar_writeq(iommu_pmu->overflow, status);\n\t}\n}\n\nstatic irqreturn_t iommu_pmu_irq_handler(int irq, void *dev_id)\n{\n\tstruct intel_iommu *iommu = dev_id;\n\n\tif (!dmar_readl(iommu->reg + DMAR_PERFINTRSTS_REG))\n\t\treturn IRQ_NONE;\n\n\tiommu_pmu_counter_overflow(iommu->pmu);\n\n\t \n\tdmar_writel(iommu->reg + DMAR_PERFINTRSTS_REG, DMA_PERFINTRSTS_PIS);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int __iommu_pmu_register(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu->pmu;\n\n\tiommu_pmu->pmu.name\t\t= iommu->name;\n\tiommu_pmu->pmu.task_ctx_nr\t= perf_invalid_context;\n\tiommu_pmu->pmu.event_init\t= iommu_pmu_event_init;\n\tiommu_pmu->pmu.pmu_enable\t= iommu_pmu_enable;\n\tiommu_pmu->pmu.pmu_disable\t= iommu_pmu_disable;\n\tiommu_pmu->pmu.add\t\t= iommu_pmu_add;\n\tiommu_pmu->pmu.del\t\t= iommu_pmu_del;\n\tiommu_pmu->pmu.start\t\t= iommu_pmu_start;\n\tiommu_pmu->pmu.stop\t\t= iommu_pmu_stop;\n\tiommu_pmu->pmu.read\t\t= iommu_pmu_event_update;\n\tiommu_pmu->pmu.attr_groups\t= iommu_pmu_attr_groups;\n\tiommu_pmu->pmu.attr_update\t= iommu_pmu_attr_update;\n\tiommu_pmu->pmu.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE;\n\tiommu_pmu->pmu.module\t\t= THIS_MODULE;\n\n\treturn perf_pmu_register(&iommu_pmu->pmu, iommu_pmu->pmu.name, -1);\n}\n\nstatic inline void __iomem *\nget_perf_reg_address(struct intel_iommu *iommu, u32 offset)\n{\n\tu32 off = dmar_readl(iommu->reg + offset);\n\n\treturn iommu->reg + off;\n}\n\nint alloc_iommu_pmu(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu;\n\tint i, j, ret;\n\tu64 perfcap;\n\tu32 cap;\n\n\tif (!ecap_pms(iommu->ecap))\n\t\treturn 0;\n\n\t \n\tif (!cap_ecmds(iommu->cap))\n\t\treturn -ENODEV;\n\n\tperfcap = dmar_readq(iommu->reg + DMAR_PERFCAP_REG);\n\t \n\tif (!perfcap)\n\t\treturn -ENODEV;\n\n\t \n\tif (!pcap_num_cntr(perfcap) || !pcap_num_event_group(perfcap))\n\t\treturn -ENODEV;\n\n\t \n\tif (!pcap_interrupt(perfcap))\n\t\treturn -ENODEV;\n\n\t \n\tif (!ecmd_has_pmu_essential(iommu))\n\t\treturn -ENODEV;\n\n\tiommu_pmu = kzalloc(sizeof(*iommu_pmu), GFP_KERNEL);\n\tif (!iommu_pmu)\n\t\treturn -ENOMEM;\n\n\tiommu_pmu->num_cntr = pcap_num_cntr(perfcap);\n\tif (iommu_pmu->num_cntr > IOMMU_PMU_IDX_MAX) {\n\t\tpr_warn_once(\"The number of IOMMU counters %d > max(%d), clipping!\",\n\t\t\t     iommu_pmu->num_cntr, IOMMU_PMU_IDX_MAX);\n\t\tiommu_pmu->num_cntr = IOMMU_PMU_IDX_MAX;\n\t}\n\n\tiommu_pmu->cntr_width = pcap_cntr_width(perfcap);\n\tiommu_pmu->filter = pcap_filters_mask(perfcap);\n\tiommu_pmu->cntr_stride = pcap_cntr_stride(perfcap);\n\tiommu_pmu->num_eg = pcap_num_event_group(perfcap);\n\n\tiommu_pmu->evcap = kcalloc(iommu_pmu->num_eg, sizeof(u64), GFP_KERNEL);\n\tif (!iommu_pmu->evcap) {\n\t\tret = -ENOMEM;\n\t\tgoto free_pmu;\n\t}\n\n\t \n\tfor (i = 0; i < iommu_pmu->num_eg; i++) {\n\t\tu64 pcap;\n\n\t\tpcap = dmar_readq(iommu->reg + DMAR_PERFEVNTCAP_REG +\n\t\t\t\t  i * IOMMU_PMU_CAP_REGS_STEP);\n\t\tiommu_pmu->evcap[i] = pecap_es(pcap);\n\t}\n\n\tiommu_pmu->cntr_evcap = kcalloc(iommu_pmu->num_cntr, sizeof(u32 *), GFP_KERNEL);\n\tif (!iommu_pmu->cntr_evcap) {\n\t\tret = -ENOMEM;\n\t\tgoto free_pmu_evcap;\n\t}\n\tfor (i = 0; i < iommu_pmu->num_cntr; i++) {\n\t\tiommu_pmu->cntr_evcap[i] = kcalloc(iommu_pmu->num_eg, sizeof(u32), GFP_KERNEL);\n\t\tif (!iommu_pmu->cntr_evcap[i]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_pmu_cntr_evcap;\n\t\t}\n\t\t \n\t\tfor (j = 0; j < iommu_pmu->num_eg; j++)\n\t\t\tiommu_pmu->cntr_evcap[i][j] = (u32)iommu_pmu->evcap[j];\n\t}\n\n\tiommu_pmu->cfg_reg = get_perf_reg_address(iommu, DMAR_PERFCFGOFF_REG);\n\tiommu_pmu->cntr_reg = get_perf_reg_address(iommu, DMAR_PERFCNTROFF_REG);\n\tiommu_pmu->overflow = get_perf_reg_address(iommu, DMAR_PERFOVFOFF_REG);\n\n\t \n\tfor (i = 0; i < iommu_pmu->num_cntr; i++) {\n\t\tcap = dmar_readl(iommu_pmu->cfg_reg +\n\t\t\t\t i * IOMMU_PMU_CFG_OFFSET +\n\t\t\t\t IOMMU_PMU_CFG_CNTRCAP_OFFSET);\n\t\tif (!iommu_cntrcap_pcc(cap))\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((iommu_cntrcap_cw(cap) != iommu_pmu->cntr_width) ||\n\t\t    !iommu_cntrcap_ios(cap)) {\n\t\t\tiommu_pmu->num_cntr = i;\n\t\t\tpr_warn(\"PMU counter capability inconsistent, counter number reduced to %d\\n\",\n\t\t\t\tiommu_pmu->num_cntr);\n\t\t}\n\n\t\t \n\t\tfor (j = 0; j < iommu_pmu->num_eg; j++)\n\t\t\tiommu_pmu->cntr_evcap[i][j] = 0;\n\n\t\t \n\t\tfor (j = 0; j < iommu_cntrcap_egcnt(cap); j++) {\n\t\t\tcap = dmar_readl(iommu_pmu->cfg_reg + i * IOMMU_PMU_CFG_OFFSET +\n\t\t\t\t\t IOMMU_PMU_CFG_CNTREVCAP_OFFSET +\n\t\t\t\t\t (j * IOMMU_PMU_OFF_REGS_STEP));\n\t\t\tiommu_pmu->cntr_evcap[i][iommu_event_group(cap)] = iommu_event_select(cap);\n\t\t\t \n\t\t\tiommu_pmu->evcap[iommu_event_group(cap)] |= iommu_event_select(cap);\n\t\t}\n\t}\n\n\tiommu_pmu->iommu = iommu;\n\tiommu->pmu = iommu_pmu;\n\n\treturn 0;\n\nfree_pmu_cntr_evcap:\n\tfor (i = 0; i < iommu_pmu->num_cntr; i++)\n\t\tkfree(iommu_pmu->cntr_evcap[i]);\n\tkfree(iommu_pmu->cntr_evcap);\nfree_pmu_evcap:\n\tkfree(iommu_pmu->evcap);\nfree_pmu:\n\tkfree(iommu_pmu);\n\n\treturn ret;\n}\n\nvoid free_iommu_pmu(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu->pmu;\n\n\tif (!iommu_pmu)\n\t\treturn;\n\n\tif (iommu_pmu->evcap) {\n\t\tint i;\n\n\t\tfor (i = 0; i < iommu_pmu->num_cntr; i++)\n\t\t\tkfree(iommu_pmu->cntr_evcap[i]);\n\t\tkfree(iommu_pmu->cntr_evcap);\n\t}\n\tkfree(iommu_pmu->evcap);\n\tkfree(iommu_pmu);\n\tiommu->pmu = NULL;\n}\n\nstatic int iommu_pmu_set_interrupt(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu->pmu;\n\tint irq, ret;\n\n\tirq = dmar_alloc_hwirq(IOMMU_IRQ_ID_OFFSET_PERF + iommu->seq_id, iommu->node, iommu);\n\tif (irq <= 0)\n\t\treturn -EINVAL;\n\n\tsnprintf(iommu_pmu->irq_name, sizeof(iommu_pmu->irq_name), \"dmar%d-perf\", iommu->seq_id);\n\n\tiommu->perf_irq = irq;\n\tret = request_threaded_irq(irq, NULL, iommu_pmu_irq_handler,\n\t\t\t\t   IRQF_ONESHOT, iommu_pmu->irq_name, iommu);\n\tif (ret) {\n\t\tdmar_free_hwirq(irq);\n\t\tiommu->perf_irq = 0;\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void iommu_pmu_unset_interrupt(struct intel_iommu *iommu)\n{\n\tif (!iommu->perf_irq)\n\t\treturn;\n\n\tfree_irq(iommu->perf_irq, iommu);\n\tdmar_free_hwirq(iommu->perf_irq);\n\tiommu->perf_irq = 0;\n}\n\nstatic int iommu_pmu_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct iommu_pmu *iommu_pmu = hlist_entry_safe(node, typeof(*iommu_pmu), cpuhp_node);\n\n\tif (cpumask_empty(&iommu_pmu_cpu_mask))\n\t\tcpumask_set_cpu(cpu, &iommu_pmu_cpu_mask);\n\n\tif (cpumask_test_cpu(cpu, &iommu_pmu_cpu_mask))\n\t\tiommu_pmu->cpu = cpu;\n\n\treturn 0;\n}\n\nstatic int iommu_pmu_cpu_offline(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct iommu_pmu *iommu_pmu = hlist_entry_safe(node, typeof(*iommu_pmu), cpuhp_node);\n\tint target = cpumask_first(&iommu_pmu_cpu_mask);\n\n\t \n\tif (target < nr_cpu_ids && target != iommu_pmu->cpu) {\n\t\tperf_pmu_migrate_context(&iommu_pmu->pmu, cpu, target);\n\t\tiommu_pmu->cpu = target;\n\t\treturn 0;\n\t}\n\n\tif (!cpumask_test_and_clear_cpu(cpu, &iommu_pmu_cpu_mask))\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\n\tif (target < nr_cpu_ids)\n\t\tcpumask_set_cpu(target, &iommu_pmu_cpu_mask);\n\telse\n\t\treturn 0;\n\n\tperf_pmu_migrate_context(&iommu_pmu->pmu, cpu, target);\n\tiommu_pmu->cpu = target;\n\n\treturn 0;\n}\n\nstatic int nr_iommu_pmu;\nstatic enum cpuhp_state iommu_cpuhp_slot;\n\nstatic int iommu_pmu_cpuhp_setup(struct iommu_pmu *iommu_pmu)\n{\n\tint ret;\n\n\tif (!nr_iommu_pmu) {\n\t\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t      \"driver/iommu/intel/perfmon:online\",\n\t\t\t\t\t      iommu_pmu_cpu_online,\n\t\t\t\t\t      iommu_pmu_cpu_offline);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tiommu_cpuhp_slot = ret;\n\t}\n\n\tret = cpuhp_state_add_instance(iommu_cpuhp_slot, &iommu_pmu->cpuhp_node);\n\tif (ret) {\n\t\tif (!nr_iommu_pmu)\n\t\t\tcpuhp_remove_multi_state(iommu_cpuhp_slot);\n\t\treturn ret;\n\t}\n\tnr_iommu_pmu++;\n\n\treturn 0;\n}\n\nstatic void iommu_pmu_cpuhp_free(struct iommu_pmu *iommu_pmu)\n{\n\tcpuhp_state_remove_instance(iommu_cpuhp_slot, &iommu_pmu->cpuhp_node);\n\n\tif (--nr_iommu_pmu)\n\t\treturn;\n\n\tcpuhp_remove_multi_state(iommu_cpuhp_slot);\n}\n\nvoid iommu_pmu_register(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu->pmu;\n\n\tif (!iommu_pmu)\n\t\treturn;\n\n\tif (__iommu_pmu_register(iommu))\n\t\tgoto err;\n\n\tif (iommu_pmu_cpuhp_setup(iommu_pmu))\n\t\tgoto unregister;\n\n\t \n\tif (iommu_pmu_set_interrupt(iommu))\n\t\tgoto cpuhp_free;\n\n\treturn;\n\ncpuhp_free:\n\tiommu_pmu_cpuhp_free(iommu_pmu);\nunregister:\n\tperf_pmu_unregister(&iommu_pmu->pmu);\nerr:\n\tpr_err(\"Failed to register PMU for iommu (seq_id = %d)\\n\", iommu->seq_id);\n\tfree_iommu_pmu(iommu);\n}\n\nvoid iommu_pmu_unregister(struct intel_iommu *iommu)\n{\n\tstruct iommu_pmu *iommu_pmu = iommu->pmu;\n\n\tif (!iommu_pmu)\n\t\treturn;\n\n\tiommu_pmu_unset_interrupt(iommu);\n\tiommu_pmu_cpuhp_free(iommu_pmu);\n\tperf_pmu_unregister(&iommu_pmu->pmu);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}