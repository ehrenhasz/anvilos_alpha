{
  "module_name": "svm.c",
  "hash_id": "fd46e3929c6d0d0b532d1fd52be0ceeb9a2842e5d96ebdfe6c183bffee3b6e1c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/intel/svm.c",
  "human_readable_source": "\n \n\n#include <linux/mmu_notifier.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n#include <linux/rculist.h>\n#include <linux/pci.h>\n#include <linux/pci-ats.h>\n#include <linux/dmar.h>\n#include <linux/interrupt.h>\n#include <linux/mm_types.h>\n#include <linux/xarray.h>\n#include <asm/page.h>\n#include <asm/fpu/api.h>\n\n#include \"iommu.h\"\n#include \"pasid.h\"\n#include \"perf.h\"\n#include \"../iommu-sva.h\"\n#include \"trace.h\"\n\nstatic irqreturn_t prq_event_thread(int irq, void *d);\n\nstatic DEFINE_XARRAY_ALLOC(pasid_private_array);\nstatic int pasid_private_add(ioasid_t pasid, void *priv)\n{\n\treturn xa_alloc(&pasid_private_array, &pasid, priv,\n\t\t\tXA_LIMIT(pasid, pasid), GFP_ATOMIC);\n}\n\nstatic void pasid_private_remove(ioasid_t pasid)\n{\n\txa_erase(&pasid_private_array, pasid);\n}\n\nstatic void *pasid_private_find(ioasid_t pasid)\n{\n\treturn xa_load(&pasid_private_array, pasid);\n}\n\nstatic struct intel_svm_dev *\nsvm_lookup_device_by_dev(struct intel_svm *svm, struct device *dev)\n{\n\tstruct intel_svm_dev *sdev = NULL, *t;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(t, &svm->devs, list) {\n\t\tif (t->dev == dev) {\n\t\t\tsdev = t;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn sdev;\n}\n\nint intel_svm_enable_prq(struct intel_iommu *iommu)\n{\n\tstruct iopf_queue *iopfq;\n\tstruct page *pages;\n\tint irq, ret;\n\n\tpages = alloc_pages(GFP_KERNEL | __GFP_ZERO, PRQ_ORDER);\n\tif (!pages) {\n\t\tpr_warn(\"IOMMU: %s: Failed to allocate page request queue\\n\",\n\t\t\tiommu->name);\n\t\treturn -ENOMEM;\n\t}\n\tiommu->prq = page_address(pages);\n\n\tirq = dmar_alloc_hwirq(IOMMU_IRQ_ID_OFFSET_PRQ + iommu->seq_id, iommu->node, iommu);\n\tif (irq <= 0) {\n\t\tpr_err(\"IOMMU: %s: Failed to create IRQ vector for page request queue\\n\",\n\t\t       iommu->name);\n\t\tret = -EINVAL;\n\t\tgoto free_prq;\n\t}\n\tiommu->pr_irq = irq;\n\n\tsnprintf(iommu->iopfq_name, sizeof(iommu->iopfq_name),\n\t\t \"dmar%d-iopfq\", iommu->seq_id);\n\tiopfq = iopf_queue_alloc(iommu->iopfq_name);\n\tif (!iopfq) {\n\t\tpr_err(\"IOMMU: %s: Failed to allocate iopf queue\\n\", iommu->name);\n\t\tret = -ENOMEM;\n\t\tgoto free_hwirq;\n\t}\n\tiommu->iopf_queue = iopfq;\n\n\tsnprintf(iommu->prq_name, sizeof(iommu->prq_name), \"dmar%d-prq\", iommu->seq_id);\n\n\tret = request_threaded_irq(irq, NULL, prq_event_thread, IRQF_ONESHOT,\n\t\t\t\t   iommu->prq_name, iommu);\n\tif (ret) {\n\t\tpr_err(\"IOMMU: %s: Failed to request IRQ for page request queue\\n\",\n\t\t       iommu->name);\n\t\tgoto free_iopfq;\n\t}\n\tdmar_writeq(iommu->reg + DMAR_PQH_REG, 0ULL);\n\tdmar_writeq(iommu->reg + DMAR_PQT_REG, 0ULL);\n\tdmar_writeq(iommu->reg + DMAR_PQA_REG, virt_to_phys(iommu->prq) | PRQ_ORDER);\n\n\tinit_completion(&iommu->prq_complete);\n\n\treturn 0;\n\nfree_iopfq:\n\tiopf_queue_free(iommu->iopf_queue);\n\tiommu->iopf_queue = NULL;\nfree_hwirq:\n\tdmar_free_hwirq(irq);\n\tiommu->pr_irq = 0;\nfree_prq:\n\tfree_pages((unsigned long)iommu->prq, PRQ_ORDER);\n\tiommu->prq = NULL;\n\n\treturn ret;\n}\n\nint intel_svm_finish_prq(struct intel_iommu *iommu)\n{\n\tdmar_writeq(iommu->reg + DMAR_PQH_REG, 0ULL);\n\tdmar_writeq(iommu->reg + DMAR_PQT_REG, 0ULL);\n\tdmar_writeq(iommu->reg + DMAR_PQA_REG, 0ULL);\n\n\tif (iommu->pr_irq) {\n\t\tfree_irq(iommu->pr_irq, iommu);\n\t\tdmar_free_hwirq(iommu->pr_irq);\n\t\tiommu->pr_irq = 0;\n\t}\n\n\tif (iommu->iopf_queue) {\n\t\tiopf_queue_free(iommu->iopf_queue);\n\t\tiommu->iopf_queue = NULL;\n\t}\n\n\tfree_pages((unsigned long)iommu->prq, PRQ_ORDER);\n\tiommu->prq = NULL;\n\n\treturn 0;\n}\n\nvoid intel_svm_check(struct intel_iommu *iommu)\n{\n\tif (!pasid_supported(iommu))\n\t\treturn;\n\n\tif (cpu_feature_enabled(X86_FEATURE_GBPAGES) &&\n\t    !cap_fl1gp_support(iommu->cap)) {\n\t\tpr_err(\"%s SVM disabled, incompatible 1GB page capability\\n\",\n\t\t       iommu->name);\n\t\treturn;\n\t}\n\n\tif (cpu_feature_enabled(X86_FEATURE_LA57) &&\n\t    !cap_fl5lp_support(iommu->cap)) {\n\t\tpr_err(\"%s SVM disabled, incompatible paging mode\\n\",\n\t\t       iommu->name);\n\t\treturn;\n\t}\n\n\tiommu->flags |= VTD_FLAG_SVM_CAPABLE;\n}\n\nstatic void __flush_svm_range_dev(struct intel_svm *svm,\n\t\t\t\t  struct intel_svm_dev *sdev,\n\t\t\t\t  unsigned long address,\n\t\t\t\t  unsigned long pages, int ih)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(sdev->dev);\n\n\tif (WARN_ON(!pages))\n\t\treturn;\n\n\tqi_flush_piotlb(sdev->iommu, sdev->did, svm->pasid, address, pages, ih);\n\tif (info->ats_enabled) {\n\t\tqi_flush_dev_iotlb_pasid(sdev->iommu, sdev->sid, info->pfsid,\n\t\t\t\t\t svm->pasid, sdev->qdep, address,\n\t\t\t\t\t order_base_2(pages));\n\t\tquirk_extra_dev_tlb_flush(info, address, order_base_2(pages),\n\t\t\t\t\t  svm->pasid, sdev->qdep);\n\t}\n}\n\nstatic void intel_flush_svm_range_dev(struct intel_svm *svm,\n\t\t\t\t      struct intel_svm_dev *sdev,\n\t\t\t\t      unsigned long address,\n\t\t\t\t      unsigned long pages, int ih)\n{\n\tunsigned long shift = ilog2(__roundup_pow_of_two(pages));\n\tunsigned long align = (1ULL << (VTD_PAGE_SHIFT + shift));\n\tunsigned long start = ALIGN_DOWN(address, align);\n\tunsigned long end = ALIGN(address + (pages << VTD_PAGE_SHIFT), align);\n\n\twhile (start < end) {\n\t\t__flush_svm_range_dev(svm, sdev, start, align >> VTD_PAGE_SHIFT, ih);\n\t\tstart += align;\n\t}\n}\n\nstatic void intel_flush_svm_range(struct intel_svm *svm, unsigned long address,\n\t\t\t\tunsigned long pages, int ih)\n{\n\tstruct intel_svm_dev *sdev;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sdev, &svm->devs, list)\n\t\tintel_flush_svm_range_dev(svm, sdev, address, pages, ih);\n\trcu_read_unlock();\n}\n\nstatic void intel_flush_svm_all(struct intel_svm *svm)\n{\n\tstruct device_domain_info *info;\n\tstruct intel_svm_dev *sdev;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sdev, &svm->devs, list) {\n\t\tinfo = dev_iommu_priv_get(sdev->dev);\n\n\t\tqi_flush_piotlb(sdev->iommu, sdev->did, svm->pasid, 0, -1UL, 0);\n\t\tif (info->ats_enabled) {\n\t\t\tqi_flush_dev_iotlb_pasid(sdev->iommu, sdev->sid, info->pfsid,\n\t\t\t\t\t\t svm->pasid, sdev->qdep,\n\t\t\t\t\t\t 0, 64 - VTD_PAGE_SHIFT);\n\t\t\tquirk_extra_dev_tlb_flush(info, 0, 64 - VTD_PAGE_SHIFT,\n\t\t\t\t\t\t  svm->pasid, sdev->qdep);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic void intel_arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\n\n\tif (start == 0 && end == -1UL) {\n\t\tintel_flush_svm_all(svm);\n\t\treturn;\n\t}\n\n\tintel_flush_svm_range(svm, start,\n\t\t\t      (end - start + PAGE_SIZE - 1) >> VTD_PAGE_SHIFT, 0);\n}\n\nstatic void intel_mm_release(struct mmu_notifier *mn, struct mm_struct *mm)\n{\n\tstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\n\tstruct intel_svm_dev *sdev;\n\n\t \n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sdev, &svm->devs, list)\n\t\tintel_pasid_tear_down_entry(sdev->iommu, sdev->dev,\n\t\t\t\t\t    svm->pasid, true);\n\trcu_read_unlock();\n\n}\n\nstatic const struct mmu_notifier_ops intel_mmuops = {\n\t.release = intel_mm_release,\n\t.arch_invalidate_secondary_tlbs = intel_arch_invalidate_secondary_tlbs,\n};\n\nstatic int pasid_to_svm_sdev(struct device *dev, unsigned int pasid,\n\t\t\t     struct intel_svm **rsvm,\n\t\t\t     struct intel_svm_dev **rsdev)\n{\n\tstruct intel_svm_dev *sdev = NULL;\n\tstruct intel_svm *svm;\n\n\tif (pasid == IOMMU_PASID_INVALID || pasid >= PASID_MAX)\n\t\treturn -EINVAL;\n\n\tsvm = pasid_private_find(pasid);\n\tif (IS_ERR(svm))\n\t\treturn PTR_ERR(svm);\n\n\tif (!svm)\n\t\tgoto out;\n\n\t \n\tif (WARN_ON(list_empty(&svm->devs)))\n\t\treturn -EINVAL;\n\tsdev = svm_lookup_device_by_dev(svm, dev);\n\nout:\n\t*rsvm = svm;\n\t*rsdev = sdev;\n\n\treturn 0;\n}\n\nstatic int intel_svm_bind_mm(struct intel_iommu *iommu, struct device *dev,\n\t\t\t     struct mm_struct *mm)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_svm_dev *sdev;\n\tstruct intel_svm *svm;\n\tunsigned long sflags;\n\tint ret = 0;\n\n\tsvm = pasid_private_find(mm->pasid);\n\tif (!svm) {\n\t\tsvm = kzalloc(sizeof(*svm), GFP_KERNEL);\n\t\tif (!svm)\n\t\t\treturn -ENOMEM;\n\n\t\tsvm->pasid = mm->pasid;\n\t\tsvm->mm = mm;\n\t\tINIT_LIST_HEAD_RCU(&svm->devs);\n\n\t\tsvm->notifier.ops = &intel_mmuops;\n\t\tret = mmu_notifier_register(&svm->notifier, mm);\n\t\tif (ret) {\n\t\t\tkfree(svm);\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = pasid_private_add(svm->pasid, svm);\n\t\tif (ret) {\n\t\t\tmmu_notifier_unregister(&svm->notifier, mm);\n\t\t\tkfree(svm);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tsdev = kzalloc(sizeof(*sdev), GFP_KERNEL);\n\tif (!sdev) {\n\t\tret = -ENOMEM;\n\t\tgoto free_svm;\n\t}\n\n\tsdev->dev = dev;\n\tsdev->iommu = iommu;\n\tsdev->did = FLPT_DEFAULT_DID;\n\tsdev->sid = PCI_DEVID(info->bus, info->devfn);\n\tinit_rcu_head(&sdev->rcu);\n\tif (info->ats_enabled) {\n\t\tsdev->qdep = info->ats_qdep;\n\t\tif (sdev->qdep >= QI_DEV_EIOTLB_MAX_INVS)\n\t\t\tsdev->qdep = 0;\n\t}\n\n\t \n\tsflags = cpu_feature_enabled(X86_FEATURE_LA57) ? PASID_FLAG_FL5LP : 0;\n\tret = intel_pasid_setup_first_level(iommu, dev, mm->pgd, mm->pasid,\n\t\t\t\t\t    FLPT_DEFAULT_DID, sflags);\n\tif (ret)\n\t\tgoto free_sdev;\n\n\tlist_add_rcu(&sdev->list, &svm->devs);\n\n\treturn 0;\n\nfree_sdev:\n\tkfree(sdev);\nfree_svm:\n\tif (list_empty(&svm->devs)) {\n\t\tmmu_notifier_unregister(&svm->notifier, mm);\n\t\tpasid_private_remove(mm->pasid);\n\t\tkfree(svm);\n\t}\n\n\treturn ret;\n}\n\nvoid intel_svm_remove_dev_pasid(struct device *dev, u32 pasid)\n{\n\tstruct intel_svm_dev *sdev;\n\tstruct intel_iommu *iommu;\n\tstruct intel_svm *svm;\n\tstruct mm_struct *mm;\n\n\tiommu = device_to_iommu(dev, NULL, NULL);\n\tif (!iommu)\n\t\treturn;\n\n\tif (pasid_to_svm_sdev(dev, pasid, &svm, &sdev))\n\t\treturn;\n\tmm = svm->mm;\n\n\tif (sdev) {\n\t\tlist_del_rcu(&sdev->list);\n\t\tkfree_rcu(sdev, rcu);\n\n\t\tif (list_empty(&svm->devs)) {\n\t\t\tif (svm->notifier.ops)\n\t\t\t\tmmu_notifier_unregister(&svm->notifier, mm);\n\t\t\tpasid_private_remove(svm->pasid);\n\t\t\t \n\t\t\tmemset(svm, 0x6b, sizeof(*svm));\n\t\t\tkfree(svm);\n\t\t}\n\t}\n}\n\n \nstruct page_req_dsc {\n\tunion {\n\t\tstruct {\n\t\t\tu64 type:8;\n\t\t\tu64 pasid_present:1;\n\t\t\tu64 priv_data_present:1;\n\t\t\tu64 rsvd:6;\n\t\t\tu64 rid:16;\n\t\t\tu64 pasid:20;\n\t\t\tu64 exe_req:1;\n\t\t\tu64 pm_req:1;\n\t\t\tu64 rsvd2:10;\n\t\t};\n\t\tu64 qw_0;\n\t};\n\tunion {\n\t\tstruct {\n\t\t\tu64 rd_req:1;\n\t\t\tu64 wr_req:1;\n\t\t\tu64 lpig:1;\n\t\t\tu64 prg_index:9;\n\t\t\tu64 addr:52;\n\t\t};\n\t\tu64 qw_1;\n\t};\n\tu64 priv_data[2];\n};\n\nstatic bool is_canonical_address(u64 addr)\n{\n\tint shift = 64 - (__VIRTUAL_MASK_SHIFT + 1);\n\tlong saddr = (long) addr;\n\n\treturn (((saddr << shift) >> shift) == saddr);\n}\n\n \nvoid intel_drain_pasid_prq(struct device *dev, u32 pasid)\n{\n\tstruct device_domain_info *info;\n\tstruct dmar_domain *domain;\n\tstruct intel_iommu *iommu;\n\tstruct qi_desc desc[3];\n\tstruct pci_dev *pdev;\n\tint head, tail;\n\tu16 sid, did;\n\tint qdep;\n\n\tinfo = dev_iommu_priv_get(dev);\n\tif (WARN_ON(!info || !dev_is_pci(dev)))\n\t\treturn;\n\n\tif (!info->pri_enabled)\n\t\treturn;\n\n\tiommu = info->iommu;\n\tdomain = info->domain;\n\tpdev = to_pci_dev(dev);\n\tsid = PCI_DEVID(info->bus, info->devfn);\n\tdid = domain_id_iommu(domain, iommu);\n\tqdep = pci_ats_queue_depth(pdev);\n\n\t \nprq_retry:\n\treinit_completion(&iommu->prq_complete);\n\ttail = dmar_readq(iommu->reg + DMAR_PQT_REG) & PRQ_RING_MASK;\n\thead = dmar_readq(iommu->reg + DMAR_PQH_REG) & PRQ_RING_MASK;\n\twhile (head != tail) {\n\t\tstruct page_req_dsc *req;\n\n\t\treq = &iommu->prq[head / sizeof(*req)];\n\t\tif (!req->pasid_present || req->pasid != pasid) {\n\t\t\thead = (head + sizeof(*req)) & PRQ_RING_MASK;\n\t\t\tcontinue;\n\t\t}\n\n\t\twait_for_completion(&iommu->prq_complete);\n\t\tgoto prq_retry;\n\t}\n\n\tiopf_queue_flush_dev(dev);\n\n\t \n\tmemset(desc, 0, sizeof(desc));\n\tdesc[0].qw0 = QI_IWD_STATUS_DATA(QI_DONE) |\n\t\t\tQI_IWD_FENCE |\n\t\t\tQI_IWD_TYPE;\n\tdesc[1].qw0 = QI_EIOTLB_PASID(pasid) |\n\t\t\tQI_EIOTLB_DID(did) |\n\t\t\tQI_EIOTLB_GRAN(QI_GRAN_NONG_PASID) |\n\t\t\tQI_EIOTLB_TYPE;\n\tdesc[2].qw0 = QI_DEV_EIOTLB_PASID(pasid) |\n\t\t\tQI_DEV_EIOTLB_SID(sid) |\n\t\t\tQI_DEV_EIOTLB_QDEP(qdep) |\n\t\t\tQI_DEIOTLB_TYPE |\n\t\t\tQI_DEV_IOTLB_PFSID(info->pfsid);\nqi_retry:\n\treinit_completion(&iommu->prq_complete);\n\tqi_submit_sync(iommu, desc, 3, QI_OPT_WAIT_DRAIN);\n\tif (readl(iommu->reg + DMAR_PRS_REG) & DMA_PRS_PRO) {\n\t\twait_for_completion(&iommu->prq_complete);\n\t\tgoto qi_retry;\n\t}\n}\n\nstatic int prq_to_iommu_prot(struct page_req_dsc *req)\n{\n\tint prot = 0;\n\n\tif (req->rd_req)\n\t\tprot |= IOMMU_FAULT_PERM_READ;\n\tif (req->wr_req)\n\t\tprot |= IOMMU_FAULT_PERM_WRITE;\n\tif (req->exe_req)\n\t\tprot |= IOMMU_FAULT_PERM_EXEC;\n\tif (req->pm_req)\n\t\tprot |= IOMMU_FAULT_PERM_PRIV;\n\n\treturn prot;\n}\n\nstatic int intel_svm_prq_report(struct intel_iommu *iommu, struct device *dev,\n\t\t\t\tstruct page_req_dsc *desc)\n{\n\tstruct iommu_fault_event event;\n\n\tif (!dev || !dev_is_pci(dev))\n\t\treturn -ENODEV;\n\n\t \n\tmemset(&event, 0, sizeof(struct iommu_fault_event));\n\tevent.fault.type = IOMMU_FAULT_PAGE_REQ;\n\tevent.fault.prm.addr = (u64)desc->addr << VTD_PAGE_SHIFT;\n\tevent.fault.prm.pasid = desc->pasid;\n\tevent.fault.prm.grpid = desc->prg_index;\n\tevent.fault.prm.perm = prq_to_iommu_prot(desc);\n\n\tif (desc->lpig)\n\t\tevent.fault.prm.flags |= IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE;\n\tif (desc->pasid_present) {\n\t\tevent.fault.prm.flags |= IOMMU_FAULT_PAGE_REQUEST_PASID_VALID;\n\t\tevent.fault.prm.flags |= IOMMU_FAULT_PAGE_RESPONSE_NEEDS_PASID;\n\t}\n\tif (desc->priv_data_present) {\n\t\t \n\t\tevent.fault.prm.flags |= IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE;\n\t\tevent.fault.prm.flags |= IOMMU_FAULT_PAGE_REQUEST_PRIV_DATA;\n\t\tevent.fault.prm.private_data[0] = desc->priv_data[0];\n\t\tevent.fault.prm.private_data[1] = desc->priv_data[1];\n\t} else if (dmar_latency_enabled(iommu, DMAR_LATENCY_PRQ)) {\n\t\t \n\t\tevent.fault.prm.private_data[0] = ktime_to_ns(ktime_get());\n\t}\n\n\treturn iommu_report_device_fault(dev, &event);\n}\n\nstatic void handle_bad_prq_event(struct intel_iommu *iommu,\n\t\t\t\t struct page_req_dsc *req, int result)\n{\n\tstruct qi_desc desc;\n\n\tpr_err(\"%s: Invalid page request: %08llx %08llx\\n\",\n\t       iommu->name, ((unsigned long long *)req)[0],\n\t       ((unsigned long long *)req)[1]);\n\n\t \n\tif (!req->lpig && !req->priv_data_present)\n\t\treturn;\n\n\tdesc.qw0 = QI_PGRP_PASID(req->pasid) |\n\t\t\tQI_PGRP_DID(req->rid) |\n\t\t\tQI_PGRP_PASID_P(req->pasid_present) |\n\t\t\tQI_PGRP_PDP(req->priv_data_present) |\n\t\t\tQI_PGRP_RESP_CODE(result) |\n\t\t\tQI_PGRP_RESP_TYPE;\n\tdesc.qw1 = QI_PGRP_IDX(req->prg_index) |\n\t\t\tQI_PGRP_LPIG(req->lpig);\n\n\tif (req->priv_data_present) {\n\t\tdesc.qw2 = req->priv_data[0];\n\t\tdesc.qw3 = req->priv_data[1];\n\t} else {\n\t\tdesc.qw2 = 0;\n\t\tdesc.qw3 = 0;\n\t}\n\n\tqi_submit_sync(iommu, &desc, 1, 0);\n}\n\nstatic irqreturn_t prq_event_thread(int irq, void *d)\n{\n\tstruct intel_iommu *iommu = d;\n\tstruct page_req_dsc *req;\n\tint head, tail, handled;\n\tstruct pci_dev *pdev;\n\tu64 address;\n\n\t \n\twritel(DMA_PRS_PPR, iommu->reg + DMAR_PRS_REG);\n\n\ttail = dmar_readq(iommu->reg + DMAR_PQT_REG) & PRQ_RING_MASK;\n\thead = dmar_readq(iommu->reg + DMAR_PQH_REG) & PRQ_RING_MASK;\n\thandled = (head != tail);\n\twhile (head != tail) {\n\t\treq = &iommu->prq[head / sizeof(*req)];\n\t\taddress = (u64)req->addr << VTD_PAGE_SHIFT;\n\n\t\tif (unlikely(!req->pasid_present)) {\n\t\t\tpr_err(\"IOMMU: %s: Page request without PASID\\n\",\n\t\t\t       iommu->name);\nbad_req:\n\t\t\thandle_bad_prq_event(iommu, req, QI_RESP_INVALID);\n\t\t\tgoto prq_advance;\n\t\t}\n\n\t\tif (unlikely(!is_canonical_address(address))) {\n\t\t\tpr_err(\"IOMMU: %s: Address is not canonical\\n\",\n\t\t\t       iommu->name);\n\t\t\tgoto bad_req;\n\t\t}\n\n\t\tif (unlikely(req->pm_req && (req->rd_req | req->wr_req))) {\n\t\t\tpr_err(\"IOMMU: %s: Page request in Privilege Mode\\n\",\n\t\t\t       iommu->name);\n\t\t\tgoto bad_req;\n\t\t}\n\n\t\tif (unlikely(req->exe_req && req->rd_req)) {\n\t\t\tpr_err(\"IOMMU: %s: Execution request not supported\\n\",\n\t\t\t       iommu->name);\n\t\t\tgoto bad_req;\n\t\t}\n\n\t\t \n\t\tif (unlikely(req->lpig && !req->rd_req && !req->wr_req))\n\t\t\tgoto prq_advance;\n\n\t\tpdev = pci_get_domain_bus_and_slot(iommu->segment,\n\t\t\t\t\t\t   PCI_BUS_NUM(req->rid),\n\t\t\t\t\t\t   req->rid & 0xff);\n\t\t \n\t\tif (!pdev)\n\t\t\tgoto bad_req;\n\n\t\tif (intel_svm_prq_report(iommu, &pdev->dev, req))\n\t\t\thandle_bad_prq_event(iommu, req, QI_RESP_INVALID);\n\t\telse\n\t\t\ttrace_prq_report(iommu, &pdev->dev, req->qw_0, req->qw_1,\n\t\t\t\t\t req->priv_data[0], req->priv_data[1],\n\t\t\t\t\t iommu->prq_seq_number++);\n\t\tpci_dev_put(pdev);\nprq_advance:\n\t\thead = (head + sizeof(*req)) & PRQ_RING_MASK;\n\t}\n\n\tdmar_writeq(iommu->reg + DMAR_PQH_REG, tail);\n\n\t \n\tif (readl(iommu->reg + DMAR_PRS_REG) & DMA_PRS_PRO) {\n\t\tpr_info_ratelimited(\"IOMMU: %s: PRQ overflow detected\\n\",\n\t\t\t\t    iommu->name);\n\t\thead = dmar_readq(iommu->reg + DMAR_PQH_REG) & PRQ_RING_MASK;\n\t\ttail = dmar_readq(iommu->reg + DMAR_PQT_REG) & PRQ_RING_MASK;\n\t\tif (head == tail) {\n\t\t\tiopf_queue_discard_partial(iommu->iopf_queue);\n\t\t\twritel(DMA_PRS_PRO, iommu->reg + DMAR_PRS_REG);\n\t\t\tpr_info_ratelimited(\"IOMMU: %s: PRQ overflow cleared\",\n\t\t\t\t\t    iommu->name);\n\t\t}\n\t}\n\n\tif (!completion_done(&iommu->prq_complete))\n\t\tcomplete(&iommu->prq_complete);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nint intel_svm_page_response(struct device *dev,\n\t\t\t    struct iommu_fault_event *evt,\n\t\t\t    struct iommu_page_response *msg)\n{\n\tstruct iommu_fault_page_request *prm;\n\tstruct intel_iommu *iommu;\n\tbool private_present;\n\tbool pasid_present;\n\tbool last_page;\n\tu8 bus, devfn;\n\tint ret = 0;\n\tu16 sid;\n\n\tif (!dev || !dev_is_pci(dev))\n\t\treturn -ENODEV;\n\n\tiommu = device_to_iommu(dev, &bus, &devfn);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tif (!msg || !evt)\n\t\treturn -EINVAL;\n\n\tprm = &evt->fault.prm;\n\tsid = PCI_DEVID(bus, devfn);\n\tpasid_present = prm->flags & IOMMU_FAULT_PAGE_REQUEST_PASID_VALID;\n\tprivate_present = prm->flags & IOMMU_FAULT_PAGE_REQUEST_PRIV_DATA;\n\tlast_page = prm->flags & IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE;\n\n\tif (!pasid_present) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (prm->pasid == 0 || prm->pasid >= PASID_MAX) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (last_page || private_present) {\n\t\tstruct qi_desc desc;\n\n\t\tdesc.qw0 = QI_PGRP_PASID(prm->pasid) | QI_PGRP_DID(sid) |\n\t\t\t\tQI_PGRP_PASID_P(pasid_present) |\n\t\t\t\tQI_PGRP_PDP(private_present) |\n\t\t\t\tQI_PGRP_RESP_CODE(msg->code) |\n\t\t\t\tQI_PGRP_RESP_TYPE;\n\t\tdesc.qw1 = QI_PGRP_IDX(prm->grpid) | QI_PGRP_LPIG(last_page);\n\t\tdesc.qw2 = 0;\n\t\tdesc.qw3 = 0;\n\n\t\tif (private_present) {\n\t\t\tdesc.qw2 = prm->private_data[0];\n\t\t\tdesc.qw3 = prm->private_data[1];\n\t\t} else if (prm->private_data[0]) {\n\t\t\tdmar_latency_update(iommu, DMAR_LATENCY_PRQ,\n\t\t\t\tktime_to_ns(ktime_get()) - prm->private_data[0]);\n\t\t}\n\n\t\tqi_submit_sync(iommu, &desc, 1, 0);\n\t}\nout:\n\treturn ret;\n}\n\nstatic int intel_svm_set_dev_pasid(struct iommu_domain *domain,\n\t\t\t\t   struct device *dev, ioasid_t pasid)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu = info->iommu;\n\tstruct mm_struct *mm = domain->mm;\n\n\treturn intel_svm_bind_mm(iommu, dev, mm);\n}\n\nstatic void intel_svm_domain_free(struct iommu_domain *domain)\n{\n\tkfree(to_dmar_domain(domain));\n}\n\nstatic const struct iommu_domain_ops intel_svm_domain_ops = {\n\t.set_dev_pasid\t\t= intel_svm_set_dev_pasid,\n\t.free\t\t\t= intel_svm_domain_free\n};\n\nstruct iommu_domain *intel_svm_domain_alloc(void)\n{\n\tstruct dmar_domain *domain;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\tdomain->domain.ops = &intel_svm_domain_ops;\n\n\treturn &domain->domain;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}