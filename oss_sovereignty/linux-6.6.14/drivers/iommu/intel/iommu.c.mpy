{
  "module_name": "iommu.c",
  "hash_id": "37bfd3f22d3a6a618b4fc0a4e066fc8d3876ceee19d566bab4cd22224c278ea3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/intel/iommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)     \"DMAR: \" fmt\n#define dev_fmt(fmt)    pr_fmt(fmt)\n\n#include <linux/crash_dump.h>\n#include <linux/dma-direct.h>\n#include <linux/dmi.h>\n#include <linux/memory.h>\n#include <linux/pci.h>\n#include <linux/pci-ats.h>\n#include <linux/spinlock.h>\n#include <linux/syscore_ops.h>\n#include <linux/tboot.h>\n#include <uapi/linux/iommufd.h>\n\n#include \"iommu.h\"\n#include \"../dma-iommu.h\"\n#include \"../irq_remapping.h\"\n#include \"../iommu-sva.h\"\n#include \"pasid.h\"\n#include \"cap_audit.h\"\n#include \"perfmon.h\"\n\n#define ROOT_SIZE\t\tVTD_PAGE_SIZE\n#define CONTEXT_SIZE\t\tVTD_PAGE_SIZE\n\n#define IS_GFX_DEVICE(pdev) ((pdev->class >> 16) == PCI_BASE_CLASS_DISPLAY)\n#define IS_USB_DEVICE(pdev) ((pdev->class >> 8) == PCI_CLASS_SERIAL_USB)\n#define IS_ISA_DEVICE(pdev) ((pdev->class >> 8) == PCI_CLASS_BRIDGE_ISA)\n#define IS_AZALIA(pdev) ((pdev)->vendor == 0x8086 && (pdev)->device == 0x3a3e)\n\n#define IOAPIC_RANGE_START\t(0xfee00000)\n#define IOAPIC_RANGE_END\t(0xfeefffff)\n#define IOVA_START_ADDR\t\t(0x1000)\n\n#define DEFAULT_DOMAIN_ADDRESS_WIDTH 57\n\n#define MAX_AGAW_WIDTH 64\n#define MAX_AGAW_PFN_WIDTH\t(MAX_AGAW_WIDTH - VTD_PAGE_SHIFT)\n\n#define __DOMAIN_MAX_PFN(gaw)  ((((uint64_t)1) << ((gaw) - VTD_PAGE_SHIFT)) - 1)\n#define __DOMAIN_MAX_ADDR(gaw) ((((uint64_t)1) << (gaw)) - 1)\n\n \n#define DOMAIN_MAX_PFN(gaw)\t((unsigned long) min_t(uint64_t, \\\n\t\t\t\t__DOMAIN_MAX_PFN(gaw), (unsigned long)-1))\n#define DOMAIN_MAX_ADDR(gaw)\t(((uint64_t)__DOMAIN_MAX_PFN(gaw)) << VTD_PAGE_SHIFT)\n\n \n#define IOVA_START_PFN\t\t(1)\n\n#define IOVA_PFN(addr)\t\t((addr) >> PAGE_SHIFT)\n\n \n#define LEVEL_STRIDE\t\t(9)\n#define LEVEL_MASK\t\t(((u64)1 << LEVEL_STRIDE) - 1)\n\nstatic inline int agaw_to_level(int agaw)\n{\n\treturn agaw + 2;\n}\n\nstatic inline int agaw_to_width(int agaw)\n{\n\treturn min_t(int, 30 + agaw * LEVEL_STRIDE, MAX_AGAW_WIDTH);\n}\n\nstatic inline int width_to_agaw(int width)\n{\n\treturn DIV_ROUND_UP(width - 30, LEVEL_STRIDE);\n}\n\nstatic inline unsigned int level_to_offset_bits(int level)\n{\n\treturn (level - 1) * LEVEL_STRIDE;\n}\n\nstatic inline int pfn_level_offset(u64 pfn, int level)\n{\n\treturn (pfn >> level_to_offset_bits(level)) & LEVEL_MASK;\n}\n\nstatic inline u64 level_mask(int level)\n{\n\treturn -1ULL << level_to_offset_bits(level);\n}\n\nstatic inline u64 level_size(int level)\n{\n\treturn 1ULL << level_to_offset_bits(level);\n}\n\nstatic inline u64 align_to_level(u64 pfn, int level)\n{\n\treturn (pfn + level_size(level) - 1) & level_mask(level);\n}\n\nstatic inline unsigned long lvl_to_nr_pages(unsigned int lvl)\n{\n\treturn 1UL << min_t(int, (lvl - 1) * LEVEL_STRIDE, MAX_AGAW_PFN_WIDTH);\n}\n\n \nstatic inline unsigned long mm_to_dma_pfn_start(unsigned long mm_pfn)\n{\n\treturn mm_pfn << (PAGE_SHIFT - VTD_PAGE_SHIFT);\n}\nstatic inline unsigned long mm_to_dma_pfn_end(unsigned long mm_pfn)\n{\n\treturn ((mm_pfn + 1) << (PAGE_SHIFT - VTD_PAGE_SHIFT)) - 1;\n}\nstatic inline unsigned long page_to_dma_pfn(struct page *pg)\n{\n\treturn mm_to_dma_pfn_start(page_to_pfn(pg));\n}\nstatic inline unsigned long virt_to_dma_pfn(void *p)\n{\n\treturn page_to_dma_pfn(virt_to_page(p));\n}\n\nstatic void __init check_tylersburg_isoch(void);\nstatic int rwbf_quirk;\n\n \nstatic int force_on = 0;\nstatic int intel_iommu_tboot_noforce;\nstatic int no_platform_optin;\n\n#define ROOT_ENTRY_NR (VTD_PAGE_SIZE/sizeof(struct root_entry))\n\n \nstatic phys_addr_t root_entry_lctp(struct root_entry *re)\n{\n\tif (!(re->lo & 1))\n\t\treturn 0;\n\n\treturn re->lo & VTD_PAGE_MASK;\n}\n\n \nstatic phys_addr_t root_entry_uctp(struct root_entry *re)\n{\n\tif (!(re->hi & 1))\n\t\treturn 0;\n\n\treturn re->hi & VTD_PAGE_MASK;\n}\n\nstatic inline void context_set_present(struct context_entry *context)\n{\n\tcontext->lo |= 1;\n}\n\nstatic inline void context_set_fault_enable(struct context_entry *context)\n{\n\tcontext->lo &= (((u64)-1) << 2) | 1;\n}\n\nstatic inline void context_set_translation_type(struct context_entry *context,\n\t\t\t\t\t\tunsigned long value)\n{\n\tcontext->lo &= (((u64)-1) << 4) | 3;\n\tcontext->lo |= (value & 3) << 2;\n}\n\nstatic inline void context_set_address_root(struct context_entry *context,\n\t\t\t\t\t    unsigned long value)\n{\n\tcontext->lo &= ~VTD_PAGE_MASK;\n\tcontext->lo |= value & VTD_PAGE_MASK;\n}\n\nstatic inline void context_set_address_width(struct context_entry *context,\n\t\t\t\t\t     unsigned long value)\n{\n\tcontext->hi |= value & 7;\n}\n\nstatic inline void context_set_domain_id(struct context_entry *context,\n\t\t\t\t\t unsigned long value)\n{\n\tcontext->hi |= (value & ((1 << 16) - 1)) << 8;\n}\n\nstatic inline void context_set_pasid(struct context_entry *context)\n{\n\tcontext->lo |= CONTEXT_PASIDE;\n}\n\nstatic inline int context_domain_id(struct context_entry *c)\n{\n\treturn((c->hi >> 8) & 0xffff);\n}\n\nstatic inline void context_clear_entry(struct context_entry *context)\n{\n\tcontext->lo = 0;\n\tcontext->hi = 0;\n}\n\nstatic inline bool context_copied(struct intel_iommu *iommu, u8 bus, u8 devfn)\n{\n\tif (!iommu->copied_tables)\n\t\treturn false;\n\n\treturn test_bit(((long)bus << 8) | devfn, iommu->copied_tables);\n}\n\nstatic inline void\nset_context_copied(struct intel_iommu *iommu, u8 bus, u8 devfn)\n{\n\tset_bit(((long)bus << 8) | devfn, iommu->copied_tables);\n}\n\nstatic inline void\nclear_context_copied(struct intel_iommu *iommu, u8 bus, u8 devfn)\n{\n\tclear_bit(((long)bus << 8) | devfn, iommu->copied_tables);\n}\n\n \nstatic struct dmar_domain *si_domain;\nstatic int hw_pass_through = 1;\n\nstruct dmar_rmrr_unit {\n\tstruct list_head list;\t\t \n\tstruct acpi_dmar_header *hdr;\t \n\tu64\tbase_address;\t\t \n\tu64\tend_address;\t\t \n\tstruct dmar_dev_scope *devices;\t \n\tint\tdevices_cnt;\t\t \n};\n\nstruct dmar_atsr_unit {\n\tstruct list_head list;\t\t \n\tstruct acpi_dmar_header *hdr;\t \n\tstruct dmar_dev_scope *devices;\t \n\tint devices_cnt;\t\t \n\tu8 include_all:1;\t\t \n};\n\nstruct dmar_satc_unit {\n\tstruct list_head list;\t\t \n\tstruct acpi_dmar_header *hdr;\t \n\tstruct dmar_dev_scope *devices;\t \n\tstruct intel_iommu *iommu;\t \n\tint devices_cnt;\t\t \n\tu8 atc_required:1;\t\t \n};\n\nstatic LIST_HEAD(dmar_atsr_units);\nstatic LIST_HEAD(dmar_rmrr_units);\nstatic LIST_HEAD(dmar_satc_units);\n\n#define for_each_rmrr_units(rmrr) \\\n\tlist_for_each_entry(rmrr, &dmar_rmrr_units, list)\n\nstatic void device_block_translation(struct device *dev);\nstatic void intel_iommu_domain_free(struct iommu_domain *domain);\n\nint dmar_disabled = !IS_ENABLED(CONFIG_INTEL_IOMMU_DEFAULT_ON);\nint intel_iommu_sm = IS_ENABLED(CONFIG_INTEL_IOMMU_SCALABLE_MODE_DEFAULT_ON);\n\nint intel_iommu_enabled = 0;\nEXPORT_SYMBOL_GPL(intel_iommu_enabled);\n\nstatic int dmar_map_gfx = 1;\nstatic int intel_iommu_superpage = 1;\nstatic int iommu_identity_mapping;\nstatic int iommu_skip_te_disable;\n\n#define IDENTMAP_GFX\t\t2\n#define IDENTMAP_AZALIA\t\t4\n\nconst struct iommu_ops intel_iommu_ops;\n\nstatic bool translation_pre_enabled(struct intel_iommu *iommu)\n{\n\treturn (iommu->flags & VTD_FLAG_TRANS_PRE_ENABLED);\n}\n\nstatic void clear_translation_pre_enabled(struct intel_iommu *iommu)\n{\n\tiommu->flags &= ~VTD_FLAG_TRANS_PRE_ENABLED;\n}\n\nstatic void init_translation_status(struct intel_iommu *iommu)\n{\n\tu32 gsts;\n\n\tgsts = readl(iommu->reg + DMAR_GSTS_REG);\n\tif (gsts & DMA_GSTS_TES)\n\t\tiommu->flags |= VTD_FLAG_TRANS_PRE_ENABLED;\n}\n\nstatic int __init intel_iommu_setup(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\twhile (*str) {\n\t\tif (!strncmp(str, \"on\", 2)) {\n\t\t\tdmar_disabled = 0;\n\t\t\tpr_info(\"IOMMU enabled\\n\");\n\t\t} else if (!strncmp(str, \"off\", 3)) {\n\t\t\tdmar_disabled = 1;\n\t\t\tno_platform_optin = 1;\n\t\t\tpr_info(\"IOMMU disabled\\n\");\n\t\t} else if (!strncmp(str, \"igfx_off\", 8)) {\n\t\t\tdmar_map_gfx = 0;\n\t\t\tpr_info(\"Disable GFX device mapping\\n\");\n\t\t} else if (!strncmp(str, \"forcedac\", 8)) {\n\t\t\tpr_warn(\"intel_iommu=forcedac deprecated; use iommu.forcedac instead\\n\");\n\t\t\tiommu_dma_forcedac = true;\n\t\t} else if (!strncmp(str, \"strict\", 6)) {\n\t\t\tpr_warn(\"intel_iommu=strict deprecated; use iommu.strict=1 instead\\n\");\n\t\t\tiommu_set_dma_strict();\n\t\t} else if (!strncmp(str, \"sp_off\", 6)) {\n\t\t\tpr_info(\"Disable supported super page\\n\");\n\t\t\tintel_iommu_superpage = 0;\n\t\t} else if (!strncmp(str, \"sm_on\", 5)) {\n\t\t\tpr_info(\"Enable scalable mode if hardware supports\\n\");\n\t\t\tintel_iommu_sm = 1;\n\t\t} else if (!strncmp(str, \"sm_off\", 6)) {\n\t\t\tpr_info(\"Scalable mode is disallowed\\n\");\n\t\t\tintel_iommu_sm = 0;\n\t\t} else if (!strncmp(str, \"tboot_noforce\", 13)) {\n\t\t\tpr_info(\"Intel-IOMMU: not forcing on after tboot. This could expose security risk for tboot\\n\");\n\t\t\tintel_iommu_tboot_noforce = 1;\n\t\t} else {\n\t\t\tpr_notice(\"Unknown option - '%s'\\n\", str);\n\t\t}\n\n\t\tstr += strcspn(str, \",\");\n\t\twhile (*str == ',')\n\t\t\tstr++;\n\t}\n\n\treturn 1;\n}\n__setup(\"intel_iommu=\", intel_iommu_setup);\n\nvoid *alloc_pgtable_page(int node, gfp_t gfp)\n{\n\tstruct page *page;\n\tvoid *vaddr = NULL;\n\n\tpage = alloc_pages_node(node, gfp | __GFP_ZERO, 0);\n\tif (page)\n\t\tvaddr = page_address(page);\n\treturn vaddr;\n}\n\nvoid free_pgtable_page(void *vaddr)\n{\n\tfree_page((unsigned long)vaddr);\n}\n\nstatic inline int domain_type_is_si(struct dmar_domain *domain)\n{\n\treturn domain->domain.type == IOMMU_DOMAIN_IDENTITY;\n}\n\nstatic inline int domain_pfn_supported(struct dmar_domain *domain,\n\t\t\t\t       unsigned long pfn)\n{\n\tint addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;\n\n\treturn !(addr_width < BITS_PER_LONG && pfn >> addr_width);\n}\n\n \nstatic unsigned long __iommu_calculate_sagaw(struct intel_iommu *iommu)\n{\n\tunsigned long fl_sagaw, sl_sagaw;\n\n\tfl_sagaw = BIT(2) | (cap_fl5lp_support(iommu->cap) ? BIT(3) : 0);\n\tsl_sagaw = cap_sagaw(iommu->cap);\n\n\t \n\tif (!sm_supported(iommu) || !ecap_flts(iommu->ecap))\n\t\treturn sl_sagaw;\n\n\t \n\tif (!ecap_slts(iommu->ecap))\n\t\treturn fl_sagaw;\n\n\treturn fl_sagaw & sl_sagaw;\n}\n\nstatic int __iommu_calculate_agaw(struct intel_iommu *iommu, int max_gaw)\n{\n\tunsigned long sagaw;\n\tint agaw;\n\n\tsagaw = __iommu_calculate_sagaw(iommu);\n\tfor (agaw = width_to_agaw(max_gaw); agaw >= 0; agaw--) {\n\t\tif (test_bit(agaw, &sagaw))\n\t\t\tbreak;\n\t}\n\n\treturn agaw;\n}\n\n \nint iommu_calculate_max_sagaw(struct intel_iommu *iommu)\n{\n\treturn __iommu_calculate_agaw(iommu, MAX_AGAW_WIDTH);\n}\n\n \nint iommu_calculate_agaw(struct intel_iommu *iommu)\n{\n\treturn __iommu_calculate_agaw(iommu, DEFAULT_DOMAIN_ADDRESS_WIDTH);\n}\n\nstatic inline bool iommu_paging_structure_coherency(struct intel_iommu *iommu)\n{\n\treturn sm_supported(iommu) ?\n\t\t\tecap_smpwc(iommu->ecap) : ecap_coherent(iommu->ecap);\n}\n\nstatic void domain_update_iommu_coherency(struct dmar_domain *domain)\n{\n\tstruct iommu_domain_info *info;\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\tbool found = false;\n\tunsigned long i;\n\n\tdomain->iommu_coherency = true;\n\txa_for_each(&domain->iommu_array, i, info) {\n\t\tfound = true;\n\t\tif (!iommu_paging_structure_coherency(info->iommu)) {\n\t\t\tdomain->iommu_coherency = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (found)\n\t\treturn;\n\n\t \n\trcu_read_lock();\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tif (!iommu_paging_structure_coherency(iommu)) {\n\t\t\tdomain->iommu_coherency = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nstatic int domain_update_iommu_superpage(struct dmar_domain *domain,\n\t\t\t\t\t struct intel_iommu *skip)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\tint mask = 0x3;\n\n\tif (!intel_iommu_superpage)\n\t\treturn 0;\n\n\t \n\trcu_read_lock();\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tif (iommu != skip) {\n\t\t\tif (domain && domain->use_first_level) {\n\t\t\t\tif (!cap_fl1gp_support(iommu->cap))\n\t\t\t\t\tmask = 0x1;\n\t\t\t} else {\n\t\t\t\tmask &= cap_super_page_val(iommu->cap);\n\t\t\t}\n\n\t\t\tif (!mask)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn fls(mask);\n}\n\nstatic int domain_update_device_node(struct dmar_domain *domain)\n{\n\tstruct device_domain_info *info;\n\tint nid = NUMA_NO_NODE;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(info, &domain->devices, link) {\n\t\t \n\t\tnid = dev_to_node(info->dev);\n\t\tif (nid != NUMA_NO_NODE)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn nid;\n}\n\nstatic void domain_update_iotlb(struct dmar_domain *domain);\n\n \nstatic unsigned long domain_super_pgsize_bitmap(struct dmar_domain *domain)\n{\n\tunsigned long bitmap = 0;\n\n\t \n\tif (domain->iommu_superpage == 1)\n\t\tbitmap |= SZ_2M;\n\telse if (domain->iommu_superpage == 2)\n\t\tbitmap |= SZ_2M | SZ_1G;\n\n\treturn bitmap;\n}\n\n \nstatic void domain_update_iommu_cap(struct dmar_domain *domain)\n{\n\tdomain_update_iommu_coherency(domain);\n\tdomain->iommu_superpage = domain_update_iommu_superpage(domain, NULL);\n\n\t \n\tif (domain->nid == NUMA_NO_NODE)\n\t\tdomain->nid = domain_update_device_node(domain);\n\n\t \n\tif (domain->use_first_level)\n\t\tdomain->domain.geometry.aperture_end = __DOMAIN_MAX_ADDR(domain->gaw - 1);\n\telse\n\t\tdomain->domain.geometry.aperture_end = __DOMAIN_MAX_ADDR(domain->gaw);\n\n\tdomain->domain.pgsize_bitmap |= domain_super_pgsize_bitmap(domain);\n\tdomain_update_iotlb(domain);\n}\n\nstruct context_entry *iommu_context_addr(struct intel_iommu *iommu, u8 bus,\n\t\t\t\t\t u8 devfn, int alloc)\n{\n\tstruct root_entry *root = &iommu->root_entry[bus];\n\tstruct context_entry *context;\n\tu64 *entry;\n\n\t \n\tif (!alloc && context_copied(iommu, bus, devfn))\n\t\treturn NULL;\n\n\tentry = &root->lo;\n\tif (sm_supported(iommu)) {\n\t\tif (devfn >= 0x80) {\n\t\t\tdevfn -= 0x80;\n\t\t\tentry = &root->hi;\n\t\t}\n\t\tdevfn *= 2;\n\t}\n\tif (*entry & 1)\n\t\tcontext = phys_to_virt(*entry & VTD_PAGE_MASK);\n\telse {\n\t\tunsigned long phy_addr;\n\t\tif (!alloc)\n\t\t\treturn NULL;\n\n\t\tcontext = alloc_pgtable_page(iommu->node, GFP_ATOMIC);\n\t\tif (!context)\n\t\t\treturn NULL;\n\n\t\t__iommu_flush_cache(iommu, (void *)context, CONTEXT_SIZE);\n\t\tphy_addr = virt_to_phys((void *)context);\n\t\t*entry = phy_addr | 1;\n\t\t__iommu_flush_cache(iommu, entry, sizeof(*entry));\n\t}\n\treturn &context[devfn];\n}\n\n \nstatic bool\nis_downstream_to_pci_bridge(struct device *dev, struct device *bridge)\n{\n\tstruct pci_dev *pdev, *pbridge;\n\n\tif (!dev_is_pci(dev) || !dev_is_pci(bridge))\n\t\treturn false;\n\n\tpdev = to_pci_dev(dev);\n\tpbridge = to_pci_dev(bridge);\n\n\tif (pbridge->subordinate &&\n\t    pbridge->subordinate->number <= pdev->bus->number &&\n\t    pbridge->subordinate->busn_res.end >= pdev->bus->number)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool quirk_ioat_snb_local_iommu(struct pci_dev *pdev)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tu32 vtbar;\n\tint rc;\n\n\t \n\trc = pci_bus_read_config_dword(pdev->bus, PCI_DEVFN(0, 0), 0xb0, &vtbar);\n\tif (rc) {\n\t\t \n\t\tdev_info(&pdev->dev, \"failed to run vt-d quirk\\n\");\n\t\treturn false;\n\t}\n\tvtbar &= 0xffff0000;\n\n\t \n\tdrhd = dmar_find_matched_drhd_unit(pdev);\n\tif (!drhd || drhd->reg_base_addr - vtbar != 0xa000) {\n\t\tpr_warn_once(FW_BUG \"BIOS assigned incorrect VT-d unit for Intel(R) QuickData Technology device\\n\");\n\t\tadd_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool iommu_is_dummy(struct intel_iommu *iommu, struct device *dev)\n{\n\tif (!iommu || iommu->drhd->ignored)\n\t\treturn true;\n\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\n\t\tif (pdev->vendor == PCI_VENDOR_ID_INTEL &&\n\t\t    pdev->device == PCI_DEVICE_ID_INTEL_IOAT_SNB &&\n\t\t    quirk_ioat_snb_local_iommu(pdev))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstruct intel_iommu *device_to_iommu(struct device *dev, u8 *bus, u8 *devfn)\n{\n\tstruct dmar_drhd_unit *drhd = NULL;\n\tstruct pci_dev *pdev = NULL;\n\tstruct intel_iommu *iommu;\n\tstruct device *tmp;\n\tu16 segment = 0;\n\tint i;\n\n\tif (!dev)\n\t\treturn NULL;\n\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pf_pdev;\n\n\t\tpdev = pci_real_dma_dev(to_pci_dev(dev));\n\n\t\t \n\t\tpf_pdev = pci_physfn(pdev);\n\t\tdev = &pf_pdev->dev;\n\t\tsegment = pci_domain_nr(pdev->bus);\n\t} else if (has_acpi_companion(dev))\n\t\tdev = &ACPI_COMPANION(dev)->dev;\n\n\trcu_read_lock();\n\tfor_each_iommu(iommu, drhd) {\n\t\tif (pdev && segment != drhd->segment)\n\t\t\tcontinue;\n\n\t\tfor_each_active_dev_scope(drhd->devices,\n\t\t\t\t\t  drhd->devices_cnt, i, tmp) {\n\t\t\tif (tmp == dev) {\n\t\t\t\t \n\t\t\t\tif (pdev && pdev->is_virtfn)\n\t\t\t\t\tgoto got_pdev;\n\n\t\t\t\tif (bus && devfn) {\n\t\t\t\t\t*bus = drhd->devices[i].bus;\n\t\t\t\t\t*devfn = drhd->devices[i].devfn;\n\t\t\t\t}\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (is_downstream_to_pci_bridge(dev, tmp))\n\t\t\t\tgoto got_pdev;\n\t\t}\n\n\t\tif (pdev && drhd->include_all) {\ngot_pdev:\n\t\t\tif (bus && devfn) {\n\t\t\t\t*bus = pdev->bus->number;\n\t\t\t\t*devfn = pdev->devfn;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t}\n\tiommu = NULL;\nout:\n\tif (iommu_is_dummy(iommu, dev))\n\t\tiommu = NULL;\n\n\trcu_read_unlock();\n\n\treturn iommu;\n}\n\nstatic void domain_flush_cache(struct dmar_domain *domain,\n\t\t\t       void *addr, int size)\n{\n\tif (!domain->iommu_coherency)\n\t\tclflush_cache_range(addr, size);\n}\n\nstatic void free_context_table(struct intel_iommu *iommu)\n{\n\tstruct context_entry *context;\n\tint i;\n\n\tif (!iommu->root_entry)\n\t\treturn;\n\n\tfor (i = 0; i < ROOT_ENTRY_NR; i++) {\n\t\tcontext = iommu_context_addr(iommu, i, 0, 0);\n\t\tif (context)\n\t\t\tfree_pgtable_page(context);\n\n\t\tif (!sm_supported(iommu))\n\t\t\tcontinue;\n\n\t\tcontext = iommu_context_addr(iommu, i, 0x80, 0);\n\t\tif (context)\n\t\t\tfree_pgtable_page(context);\n\t}\n\n\tfree_pgtable_page(iommu->root_entry);\n\tiommu->root_entry = NULL;\n}\n\n#ifdef CONFIG_DMAR_DEBUG\nstatic void pgtable_walk(struct intel_iommu *iommu, unsigned long pfn,\n\t\t\t u8 bus, u8 devfn, struct dma_pte *parent, int level)\n{\n\tstruct dma_pte *pte;\n\tint offset;\n\n\twhile (1) {\n\t\toffset = pfn_level_offset(pfn, level);\n\t\tpte = &parent[offset];\n\t\tif (!pte || (dma_pte_superpage(pte) || !dma_pte_present(pte))) {\n\t\t\tpr_info(\"PTE not present at level %d\\n\", level);\n\t\t\tbreak;\n\t\t}\n\n\t\tpr_info(\"pte level: %d, pte value: 0x%016llx\\n\", level, pte->val);\n\n\t\tif (level == 1)\n\t\t\tbreak;\n\n\t\tparent = phys_to_virt(dma_pte_addr(pte));\n\t\tlevel--;\n\t}\n}\n\nvoid dmar_fault_dump_ptes(struct intel_iommu *iommu, u16 source_id,\n\t\t\t  unsigned long long addr, u32 pasid)\n{\n\tstruct pasid_dir_entry *dir, *pde;\n\tstruct pasid_entry *entries, *pte;\n\tstruct context_entry *ctx_entry;\n\tstruct root_entry *rt_entry;\n\tint i, dir_index, index, level;\n\tu8 devfn = source_id & 0xff;\n\tu8 bus = source_id >> 8;\n\tstruct dma_pte *pgtable;\n\n\tpr_info(\"Dump %s table entries for IOVA 0x%llx\\n\", iommu->name, addr);\n\n\t \n\trt_entry = &iommu->root_entry[bus];\n\tif (!rt_entry) {\n\t\tpr_info(\"root table entry is not present\\n\");\n\t\treturn;\n\t}\n\n\tif (sm_supported(iommu))\n\t\tpr_info(\"scalable mode root entry: hi 0x%016llx, low 0x%016llx\\n\",\n\t\t\trt_entry->hi, rt_entry->lo);\n\telse\n\t\tpr_info(\"root entry: 0x%016llx\", rt_entry->lo);\n\n\t \n\tctx_entry = iommu_context_addr(iommu, bus, devfn, 0);\n\tif (!ctx_entry) {\n\t\tpr_info(\"context table entry is not present\\n\");\n\t\treturn;\n\t}\n\n\tpr_info(\"context entry: hi 0x%016llx, low 0x%016llx\\n\",\n\t\tctx_entry->hi, ctx_entry->lo);\n\n\t \n\tif (!sm_supported(iommu)) {\n\t\tlevel = agaw_to_level(ctx_entry->hi & 7);\n\t\tpgtable = phys_to_virt(ctx_entry->lo & VTD_PAGE_MASK);\n\t\tgoto pgtable_walk;\n\t}\n\n\t \n\tdir = phys_to_virt(ctx_entry->lo & VTD_PAGE_MASK);\n\tif (!dir) {\n\t\tpr_info(\"pasid directory entry is not present\\n\");\n\t\treturn;\n\t}\n\t \n\tif (intel_iommu_sm && pasid == IOMMU_PASID_INVALID)\n\t\tpasid = IOMMU_NO_PASID;\n\n\tdir_index = pasid >> PASID_PDE_SHIFT;\n\tpde = &dir[dir_index];\n\tpr_info(\"pasid dir entry: 0x%016llx\\n\", pde->val);\n\n\t \n\tentries = get_pasid_table_from_pde(pde);\n\tif (!entries) {\n\t\tpr_info(\"pasid table entry is not present\\n\");\n\t\treturn;\n\t}\n\tindex = pasid & PASID_PTE_MASK;\n\tpte = &entries[index];\n\tfor (i = 0; i < ARRAY_SIZE(pte->val); i++)\n\t\tpr_info(\"pasid table entry[%d]: 0x%016llx\\n\", i, pte->val[i]);\n\n\tif (pasid_pte_get_pgtt(pte) == PASID_ENTRY_PGTT_FL_ONLY) {\n\t\tlevel = pte->val[2] & BIT_ULL(2) ? 5 : 4;\n\t\tpgtable = phys_to_virt(pte->val[2] & VTD_PAGE_MASK);\n\t} else {\n\t\tlevel = agaw_to_level((pte->val[0] >> 2) & 0x7);\n\t\tpgtable = phys_to_virt(pte->val[0] & VTD_PAGE_MASK);\n\t}\n\npgtable_walk:\n\tpgtable_walk(iommu, addr >> VTD_PAGE_SHIFT, bus, devfn, pgtable, level);\n}\n#endif\n\nstatic struct dma_pte *pfn_to_dma_pte(struct dmar_domain *domain,\n\t\t\t\t      unsigned long pfn, int *target_level,\n\t\t\t\t      gfp_t gfp)\n{\n\tstruct dma_pte *parent, *pte;\n\tint level = agaw_to_level(domain->agaw);\n\tint offset;\n\n\tif (!domain_pfn_supported(domain, pfn))\n\t\t \n\t\treturn NULL;\n\n\tparent = domain->pgd;\n\n\twhile (1) {\n\t\tvoid *tmp_page;\n\n\t\toffset = pfn_level_offset(pfn, level);\n\t\tpte = &parent[offset];\n\t\tif (!*target_level && (dma_pte_superpage(pte) || !dma_pte_present(pte)))\n\t\t\tbreak;\n\t\tif (level == *target_level)\n\t\t\tbreak;\n\n\t\tif (!dma_pte_present(pte)) {\n\t\t\tuint64_t pteval;\n\n\t\t\ttmp_page = alloc_pgtable_page(domain->nid, gfp);\n\n\t\t\tif (!tmp_page)\n\t\t\t\treturn NULL;\n\n\t\t\tdomain_flush_cache(domain, tmp_page, VTD_PAGE_SIZE);\n\t\t\tpteval = ((uint64_t)virt_to_dma_pfn(tmp_page) << VTD_PAGE_SHIFT) | DMA_PTE_READ | DMA_PTE_WRITE;\n\t\t\tif (domain->use_first_level)\n\t\t\t\tpteval |= DMA_FL_PTE_XD | DMA_FL_PTE_US | DMA_FL_PTE_ACCESS;\n\n\t\t\tif (cmpxchg64(&pte->val, 0ULL, pteval))\n\t\t\t\t \n\t\t\t\tfree_pgtable_page(tmp_page);\n\t\t\telse\n\t\t\t\tdomain_flush_cache(domain, pte, sizeof(*pte));\n\t\t}\n\t\tif (level == 1)\n\t\t\tbreak;\n\n\t\tparent = phys_to_virt(dma_pte_addr(pte));\n\t\tlevel--;\n\t}\n\n\tif (!*target_level)\n\t\t*target_level = level;\n\n\treturn pte;\n}\n\n \nstatic struct dma_pte *dma_pfn_level_pte(struct dmar_domain *domain,\n\t\t\t\t\t unsigned long pfn,\n\t\t\t\t\t int level, int *large_page)\n{\n\tstruct dma_pte *parent, *pte;\n\tint total = agaw_to_level(domain->agaw);\n\tint offset;\n\n\tparent = domain->pgd;\n\twhile (level <= total) {\n\t\toffset = pfn_level_offset(pfn, total);\n\t\tpte = &parent[offset];\n\t\tif (level == total)\n\t\t\treturn pte;\n\n\t\tif (!dma_pte_present(pte)) {\n\t\t\t*large_page = total;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (dma_pte_superpage(pte)) {\n\t\t\t*large_page = total;\n\t\t\treturn pte;\n\t\t}\n\n\t\tparent = phys_to_virt(dma_pte_addr(pte));\n\t\ttotal--;\n\t}\n\treturn NULL;\n}\n\n \nstatic void dma_pte_clear_range(struct dmar_domain *domain,\n\t\t\t\tunsigned long start_pfn,\n\t\t\t\tunsigned long last_pfn)\n{\n\tunsigned int large_page;\n\tstruct dma_pte *first_pte, *pte;\n\n\tif (WARN_ON(!domain_pfn_supported(domain, last_pfn)) ||\n\t    WARN_ON(start_pfn > last_pfn))\n\t\treturn;\n\n\t \n\tdo {\n\t\tlarge_page = 1;\n\t\tfirst_pte = pte = dma_pfn_level_pte(domain, start_pfn, 1, &large_page);\n\t\tif (!pte) {\n\t\t\tstart_pfn = align_to_level(start_pfn + 1, large_page + 1);\n\t\t\tcontinue;\n\t\t}\n\t\tdo {\n\t\t\tdma_clear_pte(pte);\n\t\t\tstart_pfn += lvl_to_nr_pages(large_page);\n\t\t\tpte++;\n\t\t} while (start_pfn <= last_pfn && !first_pte_in_page(pte));\n\n\t\tdomain_flush_cache(domain, first_pte,\n\t\t\t\t   (void *)pte - (void *)first_pte);\n\n\t} while (start_pfn && start_pfn <= last_pfn);\n}\n\nstatic void dma_pte_free_level(struct dmar_domain *domain, int level,\n\t\t\t       int retain_level, struct dma_pte *pte,\n\t\t\t       unsigned long pfn, unsigned long start_pfn,\n\t\t\t       unsigned long last_pfn)\n{\n\tpfn = max(start_pfn, pfn);\n\tpte = &pte[pfn_level_offset(pfn, level)];\n\n\tdo {\n\t\tunsigned long level_pfn;\n\t\tstruct dma_pte *level_pte;\n\n\t\tif (!dma_pte_present(pte) || dma_pte_superpage(pte))\n\t\t\tgoto next;\n\n\t\tlevel_pfn = pfn & level_mask(level);\n\t\tlevel_pte = phys_to_virt(dma_pte_addr(pte));\n\n\t\tif (level > 2) {\n\t\t\tdma_pte_free_level(domain, level - 1, retain_level,\n\t\t\t\t\t   level_pte, level_pfn, start_pfn,\n\t\t\t\t\t   last_pfn);\n\t\t}\n\n\t\t \n\t\tif (level < retain_level && !(start_pfn > level_pfn ||\n\t\t      last_pfn < level_pfn + level_size(level) - 1)) {\n\t\t\tdma_clear_pte(pte);\n\t\t\tdomain_flush_cache(domain, pte, sizeof(*pte));\n\t\t\tfree_pgtable_page(level_pte);\n\t\t}\nnext:\n\t\tpfn += level_size(level);\n\t} while (!first_pte_in_page(++pte) && pfn <= last_pfn);\n}\n\n \nstatic void dma_pte_free_pagetable(struct dmar_domain *domain,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long last_pfn,\n\t\t\t\t   int retain_level)\n{\n\tdma_pte_clear_range(domain, start_pfn, last_pfn);\n\n\t \n\tdma_pte_free_level(domain, agaw_to_level(domain->agaw), retain_level,\n\t\t\t   domain->pgd, 0, start_pfn, last_pfn);\n\n\t \n\tif (start_pfn == 0 && last_pfn == DOMAIN_MAX_PFN(domain->gaw)) {\n\t\tfree_pgtable_page(domain->pgd);\n\t\tdomain->pgd = NULL;\n\t}\n}\n\n \nstatic void dma_pte_list_pagetables(struct dmar_domain *domain,\n\t\t\t\t    int level, struct dma_pte *pte,\n\t\t\t\t    struct list_head *freelist)\n{\n\tstruct page *pg;\n\n\tpg = pfn_to_page(dma_pte_addr(pte) >> PAGE_SHIFT);\n\tlist_add_tail(&pg->lru, freelist);\n\n\tif (level == 1)\n\t\treturn;\n\n\tpte = page_address(pg);\n\tdo {\n\t\tif (dma_pte_present(pte) && !dma_pte_superpage(pte))\n\t\t\tdma_pte_list_pagetables(domain, level - 1, pte, freelist);\n\t\tpte++;\n\t} while (!first_pte_in_page(pte));\n}\n\nstatic void dma_pte_clear_level(struct dmar_domain *domain, int level,\n\t\t\t\tstruct dma_pte *pte, unsigned long pfn,\n\t\t\t\tunsigned long start_pfn, unsigned long last_pfn,\n\t\t\t\tstruct list_head *freelist)\n{\n\tstruct dma_pte *first_pte = NULL, *last_pte = NULL;\n\n\tpfn = max(start_pfn, pfn);\n\tpte = &pte[pfn_level_offset(pfn, level)];\n\n\tdo {\n\t\tunsigned long level_pfn = pfn & level_mask(level);\n\n\t\tif (!dma_pte_present(pte))\n\t\t\tgoto next;\n\n\t\t \n\t\tif (start_pfn <= level_pfn &&\n\t\t    last_pfn >= level_pfn + level_size(level) - 1) {\n\t\t\t \n\t\t\tif (level > 1 && !dma_pte_superpage(pte))\n\t\t\t\tdma_pte_list_pagetables(domain, level - 1, pte, freelist);\n\n\t\t\tdma_clear_pte(pte);\n\t\t\tif (!first_pte)\n\t\t\t\tfirst_pte = pte;\n\t\t\tlast_pte = pte;\n\t\t} else if (level > 1) {\n\t\t\t \n\t\t\tdma_pte_clear_level(domain, level - 1,\n\t\t\t\t\t    phys_to_virt(dma_pte_addr(pte)),\n\t\t\t\t\t    level_pfn, start_pfn, last_pfn,\n\t\t\t\t\t    freelist);\n\t\t}\nnext:\n\t\tpfn = level_pfn + level_size(level);\n\t} while (!first_pte_in_page(++pte) && pfn <= last_pfn);\n\n\tif (first_pte)\n\t\tdomain_flush_cache(domain, first_pte,\n\t\t\t\t   (void *)++last_pte - (void *)first_pte);\n}\n\n \nstatic void domain_unmap(struct dmar_domain *domain, unsigned long start_pfn,\n\t\t\t unsigned long last_pfn, struct list_head *freelist)\n{\n\tif (WARN_ON(!domain_pfn_supported(domain, last_pfn)) ||\n\t    WARN_ON(start_pfn > last_pfn))\n\t\treturn;\n\n\t \n\tdma_pte_clear_level(domain, agaw_to_level(domain->agaw),\n\t\t\t    domain->pgd, 0, start_pfn, last_pfn, freelist);\n\n\t \n\tif (start_pfn == 0 && last_pfn == DOMAIN_MAX_PFN(domain->gaw)) {\n\t\tstruct page *pgd_page = virt_to_page(domain->pgd);\n\t\tlist_add_tail(&pgd_page->lru, freelist);\n\t\tdomain->pgd = NULL;\n\t}\n}\n\n \nstatic int iommu_alloc_root_entry(struct intel_iommu *iommu)\n{\n\tstruct root_entry *root;\n\n\troot = alloc_pgtable_page(iommu->node, GFP_ATOMIC);\n\tif (!root) {\n\t\tpr_err(\"Allocating root entry for %s failed\\n\",\n\t\t\tiommu->name);\n\t\treturn -ENOMEM;\n\t}\n\n\t__iommu_flush_cache(iommu, root, ROOT_SIZE);\n\tiommu->root_entry = root;\n\n\treturn 0;\n}\n\nstatic void iommu_set_root_entry(struct intel_iommu *iommu)\n{\n\tu64 addr;\n\tu32 sts;\n\tunsigned long flag;\n\n\taddr = virt_to_phys(iommu->root_entry);\n\tif (sm_supported(iommu))\n\t\taddr |= DMA_RTADDR_SMT;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\tdmar_writeq(iommu->reg + DMAR_RTADDR_REG, addr);\n\n\twritel(iommu->gcmd | DMA_GCMD_SRTP, iommu->reg + DMAR_GCMD_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,\n\t\t      readl, (sts & DMA_GSTS_RTPS), sts);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\n\t \n\tif (cap_esrtps(iommu->cap))\n\t\treturn;\n\n\tiommu->flush.flush_context(iommu, 0, 0, 0, DMA_CCMD_GLOBAL_INVL);\n\tif (sm_supported(iommu))\n\t\tqi_flush_pasid_cache(iommu, 0, QI_PC_GLOBAL, 0);\n\tiommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH);\n}\n\nvoid iommu_flush_write_buffer(struct intel_iommu *iommu)\n{\n\tu32 val;\n\tunsigned long flag;\n\n\tif (!rwbf_quirk && !cap_rwbf(iommu->cap))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\twritel(iommu->gcmd | DMA_GCMD_WBF, iommu->reg + DMAR_GCMD_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,\n\t\t      readl, (!(val & DMA_GSTS_WBFS)), val);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\n \nstatic void __iommu_flush_context(struct intel_iommu *iommu,\n\t\t\t\t  u16 did, u16 source_id, u8 function_mask,\n\t\t\t\t  u64 type)\n{\n\tu64 val = 0;\n\tunsigned long flag;\n\n\tswitch (type) {\n\tcase DMA_CCMD_GLOBAL_INVL:\n\t\tval = DMA_CCMD_GLOBAL_INVL;\n\t\tbreak;\n\tcase DMA_CCMD_DOMAIN_INVL:\n\t\tval = DMA_CCMD_DOMAIN_INVL|DMA_CCMD_DID(did);\n\t\tbreak;\n\tcase DMA_CCMD_DEVICE_INVL:\n\t\tval = DMA_CCMD_DEVICE_INVL|DMA_CCMD_DID(did)\n\t\t\t| DMA_CCMD_SID(source_id) | DMA_CCMD_FM(function_mask);\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"%s: Unexpected context-cache invalidation type 0x%llx\\n\",\n\t\t\tiommu->name, type);\n\t\treturn;\n\t}\n\tval |= DMA_CCMD_ICC;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\tdmar_writeq(iommu->reg + DMAR_CCMD_REG, val);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_CCMD_REG,\n\t\tdmar_readq, (!(val & DMA_CCMD_ICC)), val);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\n \nstatic void __iommu_flush_iotlb(struct intel_iommu *iommu, u16 did,\n\t\t\t\tu64 addr, unsigned int size_order, u64 type)\n{\n\tint tlb_offset = ecap_iotlb_offset(iommu->ecap);\n\tu64 val = 0, val_iva = 0;\n\tunsigned long flag;\n\n\tswitch (type) {\n\tcase DMA_TLB_GLOBAL_FLUSH:\n\t\t \n\t\tval = DMA_TLB_GLOBAL_FLUSH|DMA_TLB_IVT;\n\t\tbreak;\n\tcase DMA_TLB_DSI_FLUSH:\n\t\tval = DMA_TLB_DSI_FLUSH|DMA_TLB_IVT|DMA_TLB_DID(did);\n\t\tbreak;\n\tcase DMA_TLB_PSI_FLUSH:\n\t\tval = DMA_TLB_PSI_FLUSH|DMA_TLB_IVT|DMA_TLB_DID(did);\n\t\t \n\t\tval_iva = size_order | addr;\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"%s: Unexpected iotlb invalidation type 0x%llx\\n\",\n\t\t\tiommu->name, type);\n\t\treturn;\n\t}\n\n\tif (cap_write_drain(iommu->cap))\n\t\tval |= DMA_TLB_WRITE_DRAIN;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\t \n\tif (val_iva)\n\t\tdmar_writeq(iommu->reg + tlb_offset, val_iva);\n\tdmar_writeq(iommu->reg + tlb_offset + 8, val);\n\n\t \n\tIOMMU_WAIT_OP(iommu, tlb_offset + 8,\n\t\tdmar_readq, (!(val & DMA_TLB_IVT)), val);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\n\t \n\tif (DMA_TLB_IAIG(val) == 0)\n\t\tpr_err(\"Flush IOTLB failed\\n\");\n\tif (DMA_TLB_IAIG(val) != DMA_TLB_IIRG(type))\n\t\tpr_debug(\"TLB flush request %Lx, actual %Lx\\n\",\n\t\t\t(unsigned long long)DMA_TLB_IIRG(type),\n\t\t\t(unsigned long long)DMA_TLB_IAIG(val));\n}\n\nstatic struct device_domain_info *\ndomain_lookup_dev_info(struct dmar_domain *domain,\n\t\t       struct intel_iommu *iommu, u8 bus, u8 devfn)\n{\n\tstruct device_domain_info *info;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(info, &domain->devices, link) {\n\t\tif (info->iommu == iommu && info->bus == bus &&\n\t\t    info->devfn == devfn) {\n\t\t\tspin_unlock_irqrestore(&domain->lock, flags);\n\t\t\treturn info;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\treturn NULL;\n}\n\nstatic void domain_update_iotlb(struct dmar_domain *domain)\n{\n\tstruct dev_pasid_info *dev_pasid;\n\tstruct device_domain_info *info;\n\tbool has_iotlb_device = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(info, &domain->devices, link) {\n\t\tif (info->ats_enabled) {\n\t\t\thas_iotlb_device = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlist_for_each_entry(dev_pasid, &domain->dev_pasids, link_domain) {\n\t\tinfo = dev_iommu_priv_get(dev_pasid->dev);\n\t\tif (info->ats_enabled) {\n\t\t\thas_iotlb_device = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tdomain->has_iotlb_device = has_iotlb_device;\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\n \n#define BUGGY_QAT_DEVID_MASK 0x4940\nstatic bool dev_needs_extra_dtlb_flush(struct pci_dev *pdev)\n{\n\tif (pdev->vendor != PCI_VENDOR_ID_INTEL)\n\t\treturn false;\n\n\tif ((pdev->device & 0xfffc) != BUGGY_QAT_DEVID_MASK)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void iommu_enable_pci_caps(struct device_domain_info *info)\n{\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(info->dev))\n\t\treturn;\n\n\tpdev = to_pci_dev(info->dev);\n\n\t \n\tif (info->pasid_supported && !pci_enable_pasid(pdev, info->pasid_supported & ~1))\n\t\tinfo->pasid_enabled = 1;\n\n\tif (info->ats_supported && pci_ats_page_aligned(pdev) &&\n\t    !pci_enable_ats(pdev, VTD_PAGE_SHIFT)) {\n\t\tinfo->ats_enabled = 1;\n\t\tdomain_update_iotlb(info->domain);\n\t}\n}\n\nstatic void iommu_disable_pci_caps(struct device_domain_info *info)\n{\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(info->dev))\n\t\treturn;\n\n\tpdev = to_pci_dev(info->dev);\n\n\tif (info->ats_enabled) {\n\t\tpci_disable_ats(pdev);\n\t\tinfo->ats_enabled = 0;\n\t\tdomain_update_iotlb(info->domain);\n\t}\n\n\tif (info->pasid_enabled) {\n\t\tpci_disable_pasid(pdev);\n\t\tinfo->pasid_enabled = 0;\n\t}\n}\n\nstatic void __iommu_flush_dev_iotlb(struct device_domain_info *info,\n\t\t\t\t    u64 addr, unsigned int mask)\n{\n\tu16 sid, qdep;\n\n\tif (!info || !info->ats_enabled)\n\t\treturn;\n\n\tsid = info->bus << 8 | info->devfn;\n\tqdep = info->ats_qdep;\n\tqi_flush_dev_iotlb(info->iommu, sid, info->pfsid,\n\t\t\t   qdep, addr, mask);\n\tquirk_extra_dev_tlb_flush(info, addr, mask, IOMMU_NO_PASID, qdep);\n}\n\nstatic void iommu_flush_dev_iotlb(struct dmar_domain *domain,\n\t\t\t\t  u64 addr, unsigned mask)\n{\n\tstruct dev_pasid_info *dev_pasid;\n\tstruct device_domain_info *info;\n\tunsigned long flags;\n\n\tif (!domain->has_iotlb_device)\n\t\treturn;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(info, &domain->devices, link)\n\t\t__iommu_flush_dev_iotlb(info, addr, mask);\n\n\tlist_for_each_entry(dev_pasid, &domain->dev_pasids, link_domain) {\n\t\tinfo = dev_iommu_priv_get(dev_pasid->dev);\n\n\t\tif (!info->ats_enabled)\n\t\t\tcontinue;\n\n\t\tqi_flush_dev_iotlb_pasid(info->iommu,\n\t\t\t\t\t PCI_DEVID(info->bus, info->devfn),\n\t\t\t\t\t info->pfsid, dev_pasid->pasid,\n\t\t\t\t\t info->ats_qdep, addr,\n\t\t\t\t\t mask);\n\t}\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\nstatic void domain_flush_pasid_iotlb(struct intel_iommu *iommu,\n\t\t\t\t     struct dmar_domain *domain, u64 addr,\n\t\t\t\t     unsigned long npages, bool ih)\n{\n\tu16 did = domain_id_iommu(domain, iommu);\n\tstruct dev_pasid_info *dev_pasid;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(dev_pasid, &domain->dev_pasids, link_domain)\n\t\tqi_flush_piotlb(iommu, did, dev_pasid->pasid, addr, npages, ih);\n\n\tif (!list_empty(&domain->devices))\n\t\tqi_flush_piotlb(iommu, did, IOMMU_NO_PASID, addr, npages, ih);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\nstatic void iommu_flush_iotlb_psi(struct intel_iommu *iommu,\n\t\t\t\t  struct dmar_domain *domain,\n\t\t\t\t  unsigned long pfn, unsigned int pages,\n\t\t\t\t  int ih, int map)\n{\n\tunsigned int aligned_pages = __roundup_pow_of_two(pages);\n\tunsigned int mask = ilog2(aligned_pages);\n\tuint64_t addr = (uint64_t)pfn << VTD_PAGE_SHIFT;\n\tu16 did = domain_id_iommu(domain, iommu);\n\n\tif (WARN_ON(!pages))\n\t\treturn;\n\n\tif (ih)\n\t\tih = 1 << 6;\n\n\tif (domain->use_first_level) {\n\t\tdomain_flush_pasid_iotlb(iommu, domain, addr, pages, ih);\n\t} else {\n\t\tunsigned long bitmask = aligned_pages - 1;\n\n\t\t \n\t\tif (unlikely(bitmask & pfn)) {\n\t\t\tunsigned long end_pfn = pfn + pages - 1, shared_bits;\n\n\t\t\t \n\t\t\tshared_bits = ~(pfn ^ end_pfn) & ~bitmask;\n\t\t\tmask = shared_bits ? __ffs(shared_bits) : BITS_PER_LONG;\n\t\t}\n\n\t\t \n\t\tif (!cap_pgsel_inv(iommu->cap) ||\n\t\t    mask > cap_max_amask_val(iommu->cap))\n\t\t\tiommu->flush.flush_iotlb(iommu, did, 0, 0,\n\t\t\t\t\t\t\tDMA_TLB_DSI_FLUSH);\n\t\telse\n\t\t\tiommu->flush.flush_iotlb(iommu, did, addr | ih, mask,\n\t\t\t\t\t\t\tDMA_TLB_PSI_FLUSH);\n\t}\n\n\t \n\tif (!cap_caching_mode(iommu->cap) || !map)\n\t\tiommu_flush_dev_iotlb(domain, addr, mask);\n}\n\n \nstatic inline void __mapping_notify_one(struct intel_iommu *iommu,\n\t\t\t\t\tstruct dmar_domain *domain,\n\t\t\t\t\tunsigned long pfn, unsigned int pages)\n{\n\t \n\tif (cap_caching_mode(iommu->cap) && !domain->use_first_level)\n\t\tiommu_flush_iotlb_psi(iommu, domain, pfn, pages, 0, 1);\n\telse\n\t\tiommu_flush_write_buffer(iommu);\n}\n\nstatic void intel_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tstruct iommu_domain_info *info;\n\tunsigned long idx;\n\n\txa_for_each(&dmar_domain->iommu_array, idx, info) {\n\t\tstruct intel_iommu *iommu = info->iommu;\n\t\tu16 did = domain_id_iommu(dmar_domain, iommu);\n\n\t\tif (dmar_domain->use_first_level)\n\t\t\tdomain_flush_pasid_iotlb(iommu, dmar_domain, 0, -1, 0);\n\t\telse\n\t\t\tiommu->flush.flush_iotlb(iommu, did, 0, 0,\n\t\t\t\t\t\t DMA_TLB_DSI_FLUSH);\n\n\t\tif (!cap_caching_mode(iommu->cap))\n\t\t\tiommu_flush_dev_iotlb(dmar_domain, 0, MAX_AGAW_PFN_WIDTH);\n\t}\n}\n\nstatic void iommu_disable_protect_mem_regions(struct intel_iommu *iommu)\n{\n\tu32 pmen;\n\tunsigned long flags;\n\n\tif (!cap_plmr(iommu->cap) && !cap_phmr(iommu->cap))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flags);\n\tpmen = readl(iommu->reg + DMAR_PMEN_REG);\n\tpmen &= ~DMA_PMEN_EPM;\n\twritel(pmen, iommu->reg + DMAR_PMEN_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_PMEN_REG,\n\t\treadl, !(pmen & DMA_PMEN_PRS), pmen);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flags);\n}\n\nstatic void iommu_enable_translation(struct intel_iommu *iommu)\n{\n\tu32 sts;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flags);\n\tiommu->gcmd |= DMA_GCMD_TE;\n\twritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,\n\t\t      readl, (sts & DMA_GSTS_TES), sts);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flags);\n}\n\nstatic void iommu_disable_translation(struct intel_iommu *iommu)\n{\n\tu32 sts;\n\tunsigned long flag;\n\n\tif (iommu_skip_te_disable && iommu->drhd->gfx_dedicated &&\n\t    (cap_read_drain(iommu->cap) || cap_write_drain(iommu->cap)))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\tiommu->gcmd &= ~DMA_GCMD_TE;\n\twritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\n\n\t \n\tIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,\n\t\t      readl, (!(sts & DMA_GSTS_TES)), sts);\n\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n}\n\nstatic int iommu_init_domains(struct intel_iommu *iommu)\n{\n\tu32 ndomains;\n\n\tndomains = cap_ndoms(iommu->cap);\n\tpr_debug(\"%s: Number of Domains supported <%d>\\n\",\n\t\t iommu->name, ndomains);\n\n\tspin_lock_init(&iommu->lock);\n\n\tiommu->domain_ids = bitmap_zalloc(ndomains, GFP_KERNEL);\n\tif (!iommu->domain_ids)\n\t\treturn -ENOMEM;\n\n\t \n\tset_bit(0, iommu->domain_ids);\n\n\t \n\tif (sm_supported(iommu))\n\t\tset_bit(FLPT_DEFAULT_DID, iommu->domain_ids);\n\n\treturn 0;\n}\n\nstatic void disable_dmar_iommu(struct intel_iommu *iommu)\n{\n\tif (!iommu->domain_ids)\n\t\treturn;\n\n\t \n\tif (WARN_ON(bitmap_weight(iommu->domain_ids, cap_ndoms(iommu->cap))\n\t\t    > NUM_RESERVED_DID))\n\t\treturn;\n\n\tif (iommu->gcmd & DMA_GCMD_TE)\n\t\tiommu_disable_translation(iommu);\n}\n\nstatic void free_dmar_iommu(struct intel_iommu *iommu)\n{\n\tif (iommu->domain_ids) {\n\t\tbitmap_free(iommu->domain_ids);\n\t\tiommu->domain_ids = NULL;\n\t}\n\n\tif (iommu->copied_tables) {\n\t\tbitmap_free(iommu->copied_tables);\n\t\tiommu->copied_tables = NULL;\n\t}\n\n\t \n\tfree_context_table(iommu);\n\n#ifdef CONFIG_INTEL_IOMMU_SVM\n\tif (pasid_supported(iommu)) {\n\t\tif (ecap_prs(iommu->ecap))\n\t\t\tintel_svm_finish_prq(iommu);\n\t}\n#endif\n}\n\n \nstatic bool first_level_by_default(unsigned int type)\n{\n\t \n\tif (!scalable_mode_support())\n\t\treturn false;\n\n\t \n\tif (intel_cap_flts_sanity() ^ intel_cap_slts_sanity())\n\t\treturn intel_cap_flts_sanity();\n\n\t \n\treturn type != IOMMU_DOMAIN_UNMANAGED;\n}\n\nstatic struct dmar_domain *alloc_domain(unsigned int type)\n{\n\tstruct dmar_domain *domain;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->nid = NUMA_NO_NODE;\n\tif (first_level_by_default(type))\n\t\tdomain->use_first_level = true;\n\tdomain->has_iotlb_device = false;\n\tINIT_LIST_HEAD(&domain->devices);\n\tINIT_LIST_HEAD(&domain->dev_pasids);\n\tspin_lock_init(&domain->lock);\n\txa_init(&domain->iommu_array);\n\n\treturn domain;\n}\n\nstatic int domain_attach_iommu(struct dmar_domain *domain,\n\t\t\t       struct intel_iommu *iommu)\n{\n\tstruct iommu_domain_info *info, *curr;\n\tunsigned long ndomains;\n\tint num, ret = -ENOSPC;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&iommu->lock);\n\tcurr = xa_load(&domain->iommu_array, iommu->seq_id);\n\tif (curr) {\n\t\tcurr->refcnt++;\n\t\tspin_unlock(&iommu->lock);\n\t\tkfree(info);\n\t\treturn 0;\n\t}\n\n\tndomains = cap_ndoms(iommu->cap);\n\tnum = find_first_zero_bit(iommu->domain_ids, ndomains);\n\tif (num >= ndomains) {\n\t\tpr_err(\"%s: No free domain ids\\n\", iommu->name);\n\t\tgoto err_unlock;\n\t}\n\n\tset_bit(num, iommu->domain_ids);\n\tinfo->refcnt\t= 1;\n\tinfo->did\t= num;\n\tinfo->iommu\t= iommu;\n\tcurr = xa_cmpxchg(&domain->iommu_array, iommu->seq_id,\n\t\t\t  NULL, info, GFP_ATOMIC);\n\tif (curr) {\n\t\tret = xa_err(curr) ? : -EBUSY;\n\t\tgoto err_clear;\n\t}\n\tdomain_update_iommu_cap(domain);\n\n\tspin_unlock(&iommu->lock);\n\treturn 0;\n\nerr_clear:\n\tclear_bit(info->did, iommu->domain_ids);\nerr_unlock:\n\tspin_unlock(&iommu->lock);\n\tkfree(info);\n\treturn ret;\n}\n\nstatic void domain_detach_iommu(struct dmar_domain *domain,\n\t\t\t\tstruct intel_iommu *iommu)\n{\n\tstruct iommu_domain_info *info;\n\n\tspin_lock(&iommu->lock);\n\tinfo = xa_load(&domain->iommu_array, iommu->seq_id);\n\tif (--info->refcnt == 0) {\n\t\tclear_bit(info->did, iommu->domain_ids);\n\t\txa_erase(&domain->iommu_array, iommu->seq_id);\n\t\tdomain->nid = NUMA_NO_NODE;\n\t\tdomain_update_iommu_cap(domain);\n\t\tkfree(info);\n\t}\n\tspin_unlock(&iommu->lock);\n}\n\nstatic inline int guestwidth_to_adjustwidth(int gaw)\n{\n\tint agaw;\n\tint r = (gaw - 12) % 9;\n\n\tif (r == 0)\n\t\tagaw = gaw;\n\telse\n\t\tagaw = gaw + 9 - r;\n\tif (agaw > 64)\n\t\tagaw = 64;\n\treturn agaw;\n}\n\nstatic void domain_exit(struct dmar_domain *domain)\n{\n\tif (domain->pgd) {\n\t\tLIST_HEAD(freelist);\n\n\t\tdomain_unmap(domain, 0, DOMAIN_MAX_PFN(domain->gaw), &freelist);\n\t\tput_pages_list(&freelist);\n\t}\n\n\tif (WARN_ON(!list_empty(&domain->devices)))\n\t\treturn;\n\n\tkfree(domain);\n}\n\n \nstatic inline unsigned long context_get_sm_pds(struct pasid_table *table)\n{\n\tunsigned long pds, max_pde;\n\n\tmax_pde = table->max_pasid >> PASID_PDE_SHIFT;\n\tpds = find_first_bit(&max_pde, MAX_NR_PASID_BITS);\n\tif (pds < 7)\n\t\treturn 0;\n\n\treturn pds - 7;\n}\n\n \nstatic inline void\ncontext_set_sm_rid2pasid(struct context_entry *context, unsigned long pasid)\n{\n\tcontext->hi |= pasid & ((1 << 20) - 1);\n}\n\n \nstatic inline void context_set_sm_dte(struct context_entry *context)\n{\n\tcontext->lo |= BIT_ULL(2);\n}\n\n \nstatic inline void context_set_sm_pre(struct context_entry *context)\n{\n\tcontext->lo |= BIT_ULL(4);\n}\n\n \n#define context_pdts(pds)\t(((pds) & 0x7) << 9)\n\nstatic int domain_context_mapping_one(struct dmar_domain *domain,\n\t\t\t\t      struct intel_iommu *iommu,\n\t\t\t\t      struct pasid_table *table,\n\t\t\t\t      u8 bus, u8 devfn)\n{\n\tstruct device_domain_info *info =\n\t\t\tdomain_lookup_dev_info(domain, iommu, bus, devfn);\n\tu16 did = domain_id_iommu(domain, iommu);\n\tint translation = CONTEXT_TT_MULTI_LEVEL;\n\tstruct context_entry *context;\n\tint ret;\n\n\tif (hw_pass_through && domain_type_is_si(domain))\n\t\ttranslation = CONTEXT_TT_PASS_THROUGH;\n\n\tpr_debug(\"Set context mapping for %02x:%02x.%d\\n\",\n\t\tbus, PCI_SLOT(devfn), PCI_FUNC(devfn));\n\n\tspin_lock(&iommu->lock);\n\tret = -ENOMEM;\n\tcontext = iommu_context_addr(iommu, bus, devfn, 1);\n\tif (!context)\n\t\tgoto out_unlock;\n\n\tret = 0;\n\tif (context_present(context) && !context_copied(iommu, bus, devfn))\n\t\tgoto out_unlock;\n\n\t \n\tif (context_copied(iommu, bus, devfn)) {\n\t\tu16 did_old = context_domain_id(context);\n\n\t\tif (did_old < cap_ndoms(iommu->cap)) {\n\t\t\tiommu->flush.flush_context(iommu, did_old,\n\t\t\t\t\t\t   (((u16)bus) << 8) | devfn,\n\t\t\t\t\t\t   DMA_CCMD_MASK_NOBIT,\n\t\t\t\t\t\t   DMA_CCMD_DEVICE_INVL);\n\t\t\tiommu->flush.flush_iotlb(iommu, did_old, 0, 0,\n\t\t\t\t\t\t DMA_TLB_DSI_FLUSH);\n\t\t}\n\n\t\tclear_context_copied(iommu, bus, devfn);\n\t}\n\n\tcontext_clear_entry(context);\n\n\tif (sm_supported(iommu)) {\n\t\tunsigned long pds;\n\n\t\t \n\t\tpds = context_get_sm_pds(table);\n\t\tcontext->lo = (u64)virt_to_phys(table->table) |\n\t\t\t\tcontext_pdts(pds);\n\n\t\t \n\t\tcontext_set_sm_rid2pasid(context, IOMMU_NO_PASID);\n\n\t\t \n\t\tif (info && info->ats_supported)\n\t\t\tcontext_set_sm_dte(context);\n\t\tif (info && info->pri_supported)\n\t\t\tcontext_set_sm_pre(context);\n\t\tif (info && info->pasid_supported)\n\t\t\tcontext_set_pasid(context);\n\t} else {\n\t\tstruct dma_pte *pgd = domain->pgd;\n\t\tint agaw;\n\n\t\tcontext_set_domain_id(context, did);\n\n\t\tif (translation != CONTEXT_TT_PASS_THROUGH) {\n\t\t\t \n\t\t\tfor (agaw = domain->agaw; agaw > iommu->agaw; agaw--) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tpgd = phys_to_virt(dma_pte_addr(pgd));\n\t\t\t\tif (!dma_pte_present(pgd))\n\t\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tif (info && info->ats_supported)\n\t\t\t\ttranslation = CONTEXT_TT_DEV_IOTLB;\n\t\t\telse\n\t\t\t\ttranslation = CONTEXT_TT_MULTI_LEVEL;\n\n\t\t\tcontext_set_address_root(context, virt_to_phys(pgd));\n\t\t\tcontext_set_address_width(context, agaw);\n\t\t} else {\n\t\t\t \n\t\t\tcontext_set_address_width(context, iommu->msagaw);\n\t\t}\n\n\t\tcontext_set_translation_type(context, translation);\n\t}\n\n\tcontext_set_fault_enable(context);\n\tcontext_set_present(context);\n\tif (!ecap_coherent(iommu->ecap))\n\t\tclflush_cache_range(context, sizeof(*context));\n\n\t \n\tif (cap_caching_mode(iommu->cap)) {\n\t\tiommu->flush.flush_context(iommu, 0,\n\t\t\t\t\t   (((u16)bus) << 8) | devfn,\n\t\t\t\t\t   DMA_CCMD_MASK_NOBIT,\n\t\t\t\t\t   DMA_CCMD_DEVICE_INVL);\n\t\tiommu->flush.flush_iotlb(iommu, did, 0, 0, DMA_TLB_DSI_FLUSH);\n\t} else {\n\t\tiommu_flush_write_buffer(iommu);\n\t}\n\n\tret = 0;\n\nout_unlock:\n\tspin_unlock(&iommu->lock);\n\n\treturn ret;\n}\n\nstruct domain_context_mapping_data {\n\tstruct dmar_domain *domain;\n\tstruct intel_iommu *iommu;\n\tstruct pasid_table *table;\n};\n\nstatic int domain_context_mapping_cb(struct pci_dev *pdev,\n\t\t\t\t     u16 alias, void *opaque)\n{\n\tstruct domain_context_mapping_data *data = opaque;\n\n\treturn domain_context_mapping_one(data->domain, data->iommu,\n\t\t\t\t\t  data->table, PCI_BUS_NUM(alias),\n\t\t\t\t\t  alias & 0xff);\n}\n\nstatic int\ndomain_context_mapping(struct dmar_domain *domain, struct device *dev)\n{\n\tstruct domain_context_mapping_data data;\n\tstruct pasid_table *table;\n\tstruct intel_iommu *iommu;\n\tu8 bus, devfn;\n\n\tiommu = device_to_iommu(dev, &bus, &devfn);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\ttable = intel_pasid_get_table(dev);\n\n\tif (!dev_is_pci(dev))\n\t\treturn domain_context_mapping_one(domain, iommu, table,\n\t\t\t\t\t\t  bus, devfn);\n\n\tdata.domain = domain;\n\tdata.iommu = iommu;\n\tdata.table = table;\n\n\treturn pci_for_each_dma_alias(to_pci_dev(dev),\n\t\t\t\t      &domain_context_mapping_cb, &data);\n}\n\n \nstatic inline unsigned long aligned_nrpages(unsigned long host_addr,\n\t\t\t\t\t    size_t size)\n{\n\thost_addr &= ~PAGE_MASK;\n\treturn PAGE_ALIGN(host_addr + size) >> VTD_PAGE_SHIFT;\n}\n\n \nstatic inline int hardware_largepage_caps(struct dmar_domain *domain,\n\t\t\t\t\t  unsigned long iov_pfn,\n\t\t\t\t\t  unsigned long phy_pfn,\n\t\t\t\t\t  unsigned long pages)\n{\n\tint support, level = 1;\n\tunsigned long pfnmerge;\n\n\tsupport = domain->iommu_superpage;\n\n\t \n\tpfnmerge = iov_pfn | phy_pfn;\n\n\twhile (support && !(pfnmerge & ~VTD_STRIDE_MASK)) {\n\t\tpages >>= VTD_STRIDE_SHIFT;\n\t\tif (!pages)\n\t\t\tbreak;\n\t\tpfnmerge >>= VTD_STRIDE_SHIFT;\n\t\tlevel++;\n\t\tsupport--;\n\t}\n\treturn level;\n}\n\n \nstatic void switch_to_super_page(struct dmar_domain *domain,\n\t\t\t\t unsigned long start_pfn,\n\t\t\t\t unsigned long end_pfn, int level)\n{\n\tunsigned long lvl_pages = lvl_to_nr_pages(level);\n\tstruct iommu_domain_info *info;\n\tstruct dma_pte *pte = NULL;\n\tunsigned long i;\n\n\twhile (start_pfn <= end_pfn) {\n\t\tif (!pte)\n\t\t\tpte = pfn_to_dma_pte(domain, start_pfn, &level,\n\t\t\t\t\t     GFP_ATOMIC);\n\n\t\tif (dma_pte_present(pte)) {\n\t\t\tdma_pte_free_pagetable(domain, start_pfn,\n\t\t\t\t\t       start_pfn + lvl_pages - 1,\n\t\t\t\t\t       level + 1);\n\n\t\t\txa_for_each(&domain->iommu_array, i, info)\n\t\t\t\tiommu_flush_iotlb_psi(info->iommu, domain,\n\t\t\t\t\t\t      start_pfn, lvl_pages,\n\t\t\t\t\t\t      0, 0);\n\t\t}\n\n\t\tpte++;\n\t\tstart_pfn += lvl_pages;\n\t\tif (first_pte_in_page(pte))\n\t\t\tpte = NULL;\n\t}\n}\n\nstatic int\n__domain_mapping(struct dmar_domain *domain, unsigned long iov_pfn,\n\t\t unsigned long phys_pfn, unsigned long nr_pages, int prot,\n\t\t gfp_t gfp)\n{\n\tstruct dma_pte *first_pte = NULL, *pte = NULL;\n\tunsigned int largepage_lvl = 0;\n\tunsigned long lvl_pages = 0;\n\tphys_addr_t pteval;\n\tu64 attr;\n\n\tif (unlikely(!domain_pfn_supported(domain, iov_pfn + nr_pages - 1)))\n\t\treturn -EINVAL;\n\n\tif ((prot & (DMA_PTE_READ|DMA_PTE_WRITE)) == 0)\n\t\treturn -EINVAL;\n\n\tattr = prot & (DMA_PTE_READ | DMA_PTE_WRITE | DMA_PTE_SNP);\n\tattr |= DMA_FL_PTE_PRESENT;\n\tif (domain->use_first_level) {\n\t\tattr |= DMA_FL_PTE_XD | DMA_FL_PTE_US | DMA_FL_PTE_ACCESS;\n\t\tif (prot & DMA_PTE_WRITE)\n\t\t\tattr |= DMA_FL_PTE_DIRTY;\n\t}\n\n\tdomain->has_mappings = true;\n\n\tpteval = ((phys_addr_t)phys_pfn << VTD_PAGE_SHIFT) | attr;\n\n\twhile (nr_pages > 0) {\n\t\tuint64_t tmp;\n\n\t\tif (!pte) {\n\t\t\tlargepage_lvl = hardware_largepage_caps(domain, iov_pfn,\n\t\t\t\t\tphys_pfn, nr_pages);\n\n\t\t\tpte = pfn_to_dma_pte(domain, iov_pfn, &largepage_lvl,\n\t\t\t\t\t     gfp);\n\t\t\tif (!pte)\n\t\t\t\treturn -ENOMEM;\n\t\t\tfirst_pte = pte;\n\n\t\t\tlvl_pages = lvl_to_nr_pages(largepage_lvl);\n\n\t\t\t \n\t\t\tif (largepage_lvl > 1) {\n\t\t\t\tunsigned long end_pfn;\n\t\t\t\tunsigned long pages_to_remove;\n\n\t\t\t\tpteval |= DMA_PTE_LARGE_PAGE;\n\t\t\t\tpages_to_remove = min_t(unsigned long, nr_pages,\n\t\t\t\t\t\t\tnr_pte_to_next_page(pte) * lvl_pages);\n\t\t\t\tend_pfn = iov_pfn + pages_to_remove - 1;\n\t\t\t\tswitch_to_super_page(domain, iov_pfn, end_pfn, largepage_lvl);\n\t\t\t} else {\n\t\t\t\tpteval &= ~(uint64_t)DMA_PTE_LARGE_PAGE;\n\t\t\t}\n\n\t\t}\n\t\t \n\t\ttmp = cmpxchg64_local(&pte->val, 0ULL, pteval);\n\t\tif (tmp) {\n\t\t\tstatic int dumps = 5;\n\t\t\tpr_crit(\"ERROR: DMA PTE for vPFN 0x%lx already set (to %llx not %llx)\\n\",\n\t\t\t\tiov_pfn, tmp, (unsigned long long)pteval);\n\t\t\tif (dumps) {\n\t\t\t\tdumps--;\n\t\t\t\tdebug_dma_dump_mappings(NULL);\n\t\t\t}\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tnr_pages -= lvl_pages;\n\t\tiov_pfn += lvl_pages;\n\t\tphys_pfn += lvl_pages;\n\t\tpteval += lvl_pages * VTD_PAGE_SIZE;\n\n\t\t \n\t\tpte++;\n\t\tif (!nr_pages || first_pte_in_page(pte) ||\n\t\t    (largepage_lvl > 1 && nr_pages < lvl_pages)) {\n\t\t\tdomain_flush_cache(domain, first_pte,\n\t\t\t\t\t   (void *)pte - (void *)first_pte);\n\t\t\tpte = NULL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void domain_context_clear_one(struct device_domain_info *info, u8 bus, u8 devfn)\n{\n\tstruct intel_iommu *iommu = info->iommu;\n\tstruct context_entry *context;\n\tu16 did_old;\n\n\tif (!iommu)\n\t\treturn;\n\n\tspin_lock(&iommu->lock);\n\tcontext = iommu_context_addr(iommu, bus, devfn, 0);\n\tif (!context) {\n\t\tspin_unlock(&iommu->lock);\n\t\treturn;\n\t}\n\n\tif (sm_supported(iommu)) {\n\t\tif (hw_pass_through && domain_type_is_si(info->domain))\n\t\t\tdid_old = FLPT_DEFAULT_DID;\n\t\telse\n\t\t\tdid_old = domain_id_iommu(info->domain, iommu);\n\t} else {\n\t\tdid_old = context_domain_id(context);\n\t}\n\n\tcontext_clear_entry(context);\n\t__iommu_flush_cache(iommu, context, sizeof(*context));\n\tspin_unlock(&iommu->lock);\n\tiommu->flush.flush_context(iommu,\n\t\t\t\t   did_old,\n\t\t\t\t   (((u16)bus) << 8) | devfn,\n\t\t\t\t   DMA_CCMD_MASK_NOBIT,\n\t\t\t\t   DMA_CCMD_DEVICE_INVL);\n\n\tif (sm_supported(iommu))\n\t\tqi_flush_pasid_cache(iommu, did_old, QI_PC_ALL_PASIDS, 0);\n\n\tiommu->flush.flush_iotlb(iommu,\n\t\t\t\t did_old,\n\t\t\t\t 0,\n\t\t\t\t 0,\n\t\t\t\t DMA_TLB_DSI_FLUSH);\n\n\t__iommu_flush_dev_iotlb(info, 0, MAX_AGAW_PFN_WIDTH);\n}\n\nstatic int domain_setup_first_level(struct intel_iommu *iommu,\n\t\t\t\t    struct dmar_domain *domain,\n\t\t\t\t    struct device *dev,\n\t\t\t\t    u32 pasid)\n{\n\tstruct dma_pte *pgd = domain->pgd;\n\tint agaw, level;\n\tint flags = 0;\n\n\t \n\tfor (agaw = domain->agaw; agaw > iommu->agaw; agaw--) {\n\t\tpgd = phys_to_virt(dma_pte_addr(pgd));\n\t\tif (!dma_pte_present(pgd))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tlevel = agaw_to_level(agaw);\n\tif (level != 4 && level != 5)\n\t\treturn -EINVAL;\n\n\tif (level == 5)\n\t\tflags |= PASID_FLAG_FL5LP;\n\n\tif (domain->force_snooping)\n\t\tflags |= PASID_FLAG_PAGE_SNOOP;\n\n\treturn intel_pasid_setup_first_level(iommu, dev, (pgd_t *)pgd, pasid,\n\t\t\t\t\t     domain_id_iommu(domain, iommu),\n\t\t\t\t\t     flags);\n}\n\nstatic bool dev_is_real_dma_subdevice(struct device *dev)\n{\n\treturn dev && dev_is_pci(dev) &&\n\t       pci_real_dma_dev(to_pci_dev(dev)) != to_pci_dev(dev);\n}\n\nstatic int iommu_domain_identity_map(struct dmar_domain *domain,\n\t\t\t\t     unsigned long first_vpfn,\n\t\t\t\t     unsigned long last_vpfn)\n{\n\t \n\tdma_pte_clear_range(domain, first_vpfn, last_vpfn);\n\n\treturn __domain_mapping(domain, first_vpfn,\n\t\t\t\tfirst_vpfn, last_vpfn - first_vpfn + 1,\n\t\t\t\tDMA_PTE_READ|DMA_PTE_WRITE, GFP_KERNEL);\n}\n\nstatic int md_domain_init(struct dmar_domain *domain, int guest_width);\n\nstatic int __init si_domain_init(int hw)\n{\n\tstruct dmar_rmrr_unit *rmrr;\n\tstruct device *dev;\n\tint i, nid, ret;\n\n\tsi_domain = alloc_domain(IOMMU_DOMAIN_IDENTITY);\n\tif (!si_domain)\n\t\treturn -EFAULT;\n\n\tif (md_domain_init(si_domain, DEFAULT_DOMAIN_ADDRESS_WIDTH)) {\n\t\tdomain_exit(si_domain);\n\t\tsi_domain = NULL;\n\t\treturn -EFAULT;\n\t}\n\n\tif (hw)\n\t\treturn 0;\n\n\tfor_each_online_node(nid) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tint i;\n\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tret = iommu_domain_identity_map(si_domain,\n\t\t\t\t\tmm_to_dma_pfn_start(start_pfn),\n\t\t\t\t\tmm_to_dma_pfn_end(end_pfn));\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tfor_each_rmrr_units(rmrr) {\n\t\tfor_each_active_dev_scope(rmrr->devices, rmrr->devices_cnt,\n\t\t\t\t\t  i, dev) {\n\t\t\tunsigned long long start = rmrr->base_address;\n\t\t\tunsigned long long end = rmrr->end_address;\n\n\t\t\tif (WARN_ON(end < start ||\n\t\t\t\t    end >> agaw_to_width(si_domain->agaw)))\n\t\t\t\tcontinue;\n\n\t\t\tret = iommu_domain_identity_map(si_domain,\n\t\t\t\t\tmm_to_dma_pfn_start(start >> PAGE_SHIFT),\n\t\t\t\t\tmm_to_dma_pfn_end(end >> PAGE_SHIFT));\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int dmar_domain_attach_device(struct dmar_domain *domain,\n\t\t\t\t     struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu;\n\tunsigned long flags;\n\tu8 bus, devfn;\n\tint ret;\n\n\tiommu = device_to_iommu(dev, &bus, &devfn);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tret = domain_attach_iommu(domain, iommu);\n\tif (ret)\n\t\treturn ret;\n\tinfo->domain = domain;\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_add(&info->link, &domain->devices);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\t \n\tif (sm_supported(iommu) && !dev_is_real_dma_subdevice(dev)) {\n\t\t \n\t\tif (hw_pass_through && domain_type_is_si(domain))\n\t\t\tret = intel_pasid_setup_pass_through(iommu, domain,\n\t\t\t\t\tdev, IOMMU_NO_PASID);\n\t\telse if (domain->use_first_level)\n\t\t\tret = domain_setup_first_level(iommu, domain, dev,\n\t\t\t\t\tIOMMU_NO_PASID);\n\t\telse\n\t\t\tret = intel_pasid_setup_second_level(iommu, domain,\n\t\t\t\t\tdev, IOMMU_NO_PASID);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Setup RID2PASID failed\\n\");\n\t\t\tdevice_block_translation(dev);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = domain_context_mapping(domain, dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Domain context map failed\\n\");\n\t\tdevice_block_translation(dev);\n\t\treturn ret;\n\t}\n\n\tif (sm_supported(info->iommu) || !domain_type_is_si(info->domain))\n\t\tiommu_enable_pci_caps(info);\n\n\treturn 0;\n}\n\n \nstatic bool device_rmrr_is_relaxable(struct device *dev)\n{\n\tstruct pci_dev *pdev;\n\n\tif (!dev_is_pci(dev))\n\t\treturn false;\n\n\tpdev = to_pci_dev(dev);\n\tif (IS_USB_DEVICE(pdev) || IS_GFX_DEVICE(pdev))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\n \nstatic int device_def_domain_type(struct device *dev)\n{\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\n\t\tif ((iommu_identity_mapping & IDENTMAP_AZALIA) && IS_AZALIA(pdev))\n\t\t\treturn IOMMU_DOMAIN_IDENTITY;\n\n\t\tif ((iommu_identity_mapping & IDENTMAP_GFX) && IS_GFX_DEVICE(pdev))\n\t\t\treturn IOMMU_DOMAIN_IDENTITY;\n\t}\n\n\treturn 0;\n}\n\nstatic void intel_iommu_init_qi(struct intel_iommu *iommu)\n{\n\t \n\tif (!iommu->qi) {\n\t\t \n\t\tdmar_fault(-1, iommu);\n\t\t \n\t\tdmar_disable_qi(iommu);\n\t}\n\n\tif (dmar_enable_qi(iommu)) {\n\t\t \n\t\tiommu->flush.flush_context = __iommu_flush_context;\n\t\tiommu->flush.flush_iotlb = __iommu_flush_iotlb;\n\t\tpr_info(\"%s: Using Register based invalidation\\n\",\n\t\t\tiommu->name);\n\t} else {\n\t\tiommu->flush.flush_context = qi_flush_context;\n\t\tiommu->flush.flush_iotlb = qi_flush_iotlb;\n\t\tpr_info(\"%s: Using Queued invalidation\\n\", iommu->name);\n\t}\n}\n\nstatic int copy_context_table(struct intel_iommu *iommu,\n\t\t\t      struct root_entry *old_re,\n\t\t\t      struct context_entry **tbl,\n\t\t\t      int bus, bool ext)\n{\n\tint tbl_idx, pos = 0, idx, devfn, ret = 0, did;\n\tstruct context_entry *new_ce = NULL, ce;\n\tstruct context_entry *old_ce = NULL;\n\tstruct root_entry re;\n\tphys_addr_t old_ce_phys;\n\n\ttbl_idx = ext ? bus * 2 : bus;\n\tmemcpy(&re, old_re, sizeof(re));\n\n\tfor (devfn = 0; devfn < 256; devfn++) {\n\t\t \n\t\tidx = (ext ? devfn * 2 : devfn) % 256;\n\n\t\tif (idx == 0) {\n\t\t\t \n\t\t\tif (new_ce) {\n\t\t\t\ttbl[tbl_idx] = new_ce;\n\t\t\t\t__iommu_flush_cache(iommu, new_ce,\n\t\t\t\t\t\t    VTD_PAGE_SIZE);\n\t\t\t\tpos = 1;\n\t\t\t}\n\n\t\t\tif (old_ce)\n\t\t\t\tmemunmap(old_ce);\n\n\t\t\tret = 0;\n\t\t\tif (devfn < 0x80)\n\t\t\t\told_ce_phys = root_entry_lctp(&re);\n\t\t\telse\n\t\t\t\told_ce_phys = root_entry_uctp(&re);\n\n\t\t\tif (!old_ce_phys) {\n\t\t\t\tif (ext && devfn == 0) {\n\t\t\t\t\t \n\t\t\t\t\tdevfn = 0x7f;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = -ENOMEM;\n\t\t\told_ce = memremap(old_ce_phys, PAGE_SIZE,\n\t\t\t\t\tMEMREMAP_WB);\n\t\t\tif (!old_ce)\n\t\t\t\tgoto out;\n\n\t\t\tnew_ce = alloc_pgtable_page(iommu->node, GFP_KERNEL);\n\t\t\tif (!new_ce)\n\t\t\t\tgoto out_unmap;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\t \n\t\tmemcpy(&ce, old_ce + idx, sizeof(ce));\n\n\t\tif (!context_present(&ce))\n\t\t\tcontinue;\n\n\t\tdid = context_domain_id(&ce);\n\t\tif (did >= 0 && did < cap_ndoms(iommu->cap))\n\t\t\tset_bit(did, iommu->domain_ids);\n\n\t\tset_context_copied(iommu, bus, devfn);\n\t\tnew_ce[idx] = ce;\n\t}\n\n\ttbl[tbl_idx + pos] = new_ce;\n\n\t__iommu_flush_cache(iommu, new_ce, VTD_PAGE_SIZE);\n\nout_unmap:\n\tmemunmap(old_ce);\n\nout:\n\treturn ret;\n}\n\nstatic int copy_translation_tables(struct intel_iommu *iommu)\n{\n\tstruct context_entry **ctxt_tbls;\n\tstruct root_entry *old_rt;\n\tphys_addr_t old_rt_phys;\n\tint ctxt_table_entries;\n\tu64 rtaddr_reg;\n\tint bus, ret;\n\tbool new_ext, ext;\n\n\trtaddr_reg = dmar_readq(iommu->reg + DMAR_RTADDR_REG);\n\text        = !!(rtaddr_reg & DMA_RTADDR_SMT);\n\tnew_ext    = !!sm_supported(iommu);\n\n\t \n\tif (new_ext != ext)\n\t\treturn -EINVAL;\n\n\tiommu->copied_tables = bitmap_zalloc(BIT_ULL(16), GFP_KERNEL);\n\tif (!iommu->copied_tables)\n\t\treturn -ENOMEM;\n\n\told_rt_phys = rtaddr_reg & VTD_PAGE_MASK;\n\tif (!old_rt_phys)\n\t\treturn -EINVAL;\n\n\told_rt = memremap(old_rt_phys, PAGE_SIZE, MEMREMAP_WB);\n\tif (!old_rt)\n\t\treturn -ENOMEM;\n\n\t \n\tctxt_table_entries = ext ? 512 : 256;\n\tret = -ENOMEM;\n\tctxt_tbls = kcalloc(ctxt_table_entries, sizeof(void *), GFP_KERNEL);\n\tif (!ctxt_tbls)\n\t\tgoto out_unmap;\n\n\tfor (bus = 0; bus < 256; bus++) {\n\t\tret = copy_context_table(iommu, &old_rt[bus],\n\t\t\t\t\t ctxt_tbls, bus, ext);\n\t\tif (ret) {\n\t\t\tpr_err(\"%s: Failed to copy context table for bus %d\\n\",\n\t\t\t\tiommu->name, bus);\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tspin_lock(&iommu->lock);\n\n\t \n\tfor (bus = 0; bus < 256; bus++) {\n\t\tint idx = ext ? bus * 2 : bus;\n\t\tu64 val;\n\n\t\tif (ctxt_tbls[idx]) {\n\t\t\tval = virt_to_phys(ctxt_tbls[idx]) | 1;\n\t\t\tiommu->root_entry[bus].lo = val;\n\t\t}\n\n\t\tif (!ext || !ctxt_tbls[idx + 1])\n\t\t\tcontinue;\n\n\t\tval = virt_to_phys(ctxt_tbls[idx + 1]) | 1;\n\t\tiommu->root_entry[bus].hi = val;\n\t}\n\n\tspin_unlock(&iommu->lock);\n\n\tkfree(ctxt_tbls);\n\n\t__iommu_flush_cache(iommu, iommu->root_entry, PAGE_SIZE);\n\n\tret = 0;\n\nout_unmap:\n\tmemunmap(old_rt);\n\n\treturn ret;\n}\n\nstatic int __init init_dmars(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\tint ret;\n\n\tret = intel_cap_audit(CAP_AUDIT_STATIC_DMAR, NULL);\n\tif (ret)\n\t\tgoto free_iommu;\n\n\tfor_each_iommu(iommu, drhd) {\n\t\tif (drhd->ignored) {\n\t\t\tiommu_disable_translation(iommu);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (pasid_supported(iommu)) {\n\t\t\tu32 temp = 2 << ecap_pss(iommu->ecap);\n\n\t\t\tintel_pasid_max_id = min_t(u32, temp,\n\t\t\t\t\t\t   intel_pasid_max_id);\n\t\t}\n\n\t\tintel_iommu_init_qi(iommu);\n\n\t\tret = iommu_init_domains(iommu);\n\t\tif (ret)\n\t\t\tgoto free_iommu;\n\n\t\tinit_translation_status(iommu);\n\n\t\tif (translation_pre_enabled(iommu) && !is_kdump_kernel()) {\n\t\t\tiommu_disable_translation(iommu);\n\t\t\tclear_translation_pre_enabled(iommu);\n\t\t\tpr_warn(\"Translation was enabled for %s but we are not in kdump mode\\n\",\n\t\t\t\tiommu->name);\n\t\t}\n\n\t\t \n\t\tret = iommu_alloc_root_entry(iommu);\n\t\tif (ret)\n\t\t\tgoto free_iommu;\n\n\t\tif (translation_pre_enabled(iommu)) {\n\t\t\tpr_info(\"Translation already enabled - trying to copy translation structures\\n\");\n\n\t\t\tret = copy_translation_tables(iommu);\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\tpr_err(\"Failed to copy translation tables from previous kernel for %s\\n\",\n\t\t\t\t       iommu->name);\n\t\t\t\tiommu_disable_translation(iommu);\n\t\t\t\tclear_translation_pre_enabled(iommu);\n\t\t\t} else {\n\t\t\t\tpr_info(\"Copied translation tables from previous kernel for %s\\n\",\n\t\t\t\t\tiommu->name);\n\t\t\t}\n\t\t}\n\n\t\tif (!ecap_pass_through(iommu->ecap))\n\t\t\thw_pass_through = 0;\n\t\tintel_svm_check(iommu);\n\t}\n\n\t \n\tfor_each_active_iommu(iommu, drhd) {\n\t\tiommu_flush_write_buffer(iommu);\n\t\tiommu_set_root_entry(iommu);\n\t}\n\n#ifdef CONFIG_INTEL_IOMMU_BROKEN_GFX_WA\n\tdmar_map_gfx = 0;\n#endif\n\n\tif (!dmar_map_gfx)\n\t\tiommu_identity_mapping |= IDENTMAP_GFX;\n\n\tcheck_tylersburg_isoch();\n\n\tret = si_domain_init(hw_pass_through);\n\tif (ret)\n\t\tgoto free_iommu;\n\n\t \n\tfor_each_iommu(iommu, drhd) {\n\t\tif (drhd->ignored) {\n\t\t\t \n\t\t\tif (force_on)\n\t\t\t\tiommu_disable_protect_mem_regions(iommu);\n\t\t\tcontinue;\n\t\t}\n\n\t\tiommu_flush_write_buffer(iommu);\n\n#ifdef CONFIG_INTEL_IOMMU_SVM\n\t\tif (pasid_supported(iommu) && ecap_prs(iommu->ecap)) {\n\t\t\t \n\t\t\tup_write(&dmar_global_lock);\n\t\t\tret = intel_svm_enable_prq(iommu);\n\t\t\tdown_write(&dmar_global_lock);\n\t\t\tif (ret)\n\t\t\t\tgoto free_iommu;\n\t\t}\n#endif\n\t\tret = dmar_set_interrupt(iommu);\n\t\tif (ret)\n\t\t\tgoto free_iommu;\n\t}\n\n\treturn 0;\n\nfree_iommu:\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tdisable_dmar_iommu(iommu);\n\t\tfree_dmar_iommu(iommu);\n\t}\n\tif (si_domain) {\n\t\tdomain_exit(si_domain);\n\t\tsi_domain = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic void __init init_no_remapping_devices(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct device *dev;\n\tint i;\n\n\tfor_each_drhd_unit(drhd) {\n\t\tif (!drhd->include_all) {\n\t\t\tfor_each_active_dev_scope(drhd->devices,\n\t\t\t\t\t\t  drhd->devices_cnt, i, dev)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (i == drhd->devices_cnt)\n\t\t\t\tdrhd->ignored = 1;\n\t\t}\n\t}\n\n\tfor_each_active_drhd_unit(drhd) {\n\t\tif (drhd->include_all)\n\t\t\tcontinue;\n\n\t\tfor_each_active_dev_scope(drhd->devices,\n\t\t\t\t\t  drhd->devices_cnt, i, dev)\n\t\t\tif (!dev_is_pci(dev) || !IS_GFX_DEVICE(to_pci_dev(dev)))\n\t\t\t\tbreak;\n\t\tif (i < drhd->devices_cnt)\n\t\t\tcontinue;\n\n\t\t \n\t\tdrhd->gfx_dedicated = 1;\n\t\tif (!dmar_map_gfx)\n\t\t\tdrhd->ignored = 1;\n\t}\n}\n\n#ifdef CONFIG_SUSPEND\nstatic int init_iommu_hw(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu = NULL;\n\tint ret;\n\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tif (iommu->qi) {\n\t\t\tret = dmar_reenable_qi(iommu);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tfor_each_iommu(iommu, drhd) {\n\t\tif (drhd->ignored) {\n\t\t\t \n\t\t\tif (force_on)\n\t\t\t\tiommu_disable_protect_mem_regions(iommu);\n\t\t\tcontinue;\n\t\t}\n\n\t\tiommu_flush_write_buffer(iommu);\n\t\tiommu_set_root_entry(iommu);\n\t\tiommu_enable_translation(iommu);\n\t\tiommu_disable_protect_mem_regions(iommu);\n\t}\n\n\treturn 0;\n}\n\nstatic void iommu_flush_all(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tiommu->flush.flush_context(iommu, 0, 0, 0,\n\t\t\t\t\t   DMA_CCMD_GLOBAL_INVL);\n\t\tiommu->flush.flush_iotlb(iommu, 0, 0, 0,\n\t\t\t\t\t DMA_TLB_GLOBAL_FLUSH);\n\t}\n}\n\nstatic int iommu_suspend(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu = NULL;\n\tunsigned long flag;\n\n\tiommu_flush_all();\n\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tiommu_disable_translation(iommu);\n\n\t\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\n\t\tiommu->iommu_state[SR_DMAR_FECTL_REG] =\n\t\t\treadl(iommu->reg + DMAR_FECTL_REG);\n\t\tiommu->iommu_state[SR_DMAR_FEDATA_REG] =\n\t\t\treadl(iommu->reg + DMAR_FEDATA_REG);\n\t\tiommu->iommu_state[SR_DMAR_FEADDR_REG] =\n\t\t\treadl(iommu->reg + DMAR_FEADDR_REG);\n\t\tiommu->iommu_state[SR_DMAR_FEUADDR_REG] =\n\t\t\treadl(iommu->reg + DMAR_FEUADDR_REG);\n\n\t\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\t}\n\treturn 0;\n}\n\nstatic void iommu_resume(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu = NULL;\n\tunsigned long flag;\n\n\tif (init_iommu_hw()) {\n\t\tif (force_on)\n\t\t\tpanic(\"tboot: IOMMU setup failed, DMAR can not resume!\\n\");\n\t\telse\n\t\t\tWARN(1, \"IOMMU setup failed, DMAR can not resume!\\n\");\n\t\treturn;\n\t}\n\n\tfor_each_active_iommu(iommu, drhd) {\n\n\t\traw_spin_lock_irqsave(&iommu->register_lock, flag);\n\n\t\twritel(iommu->iommu_state[SR_DMAR_FECTL_REG],\n\t\t\tiommu->reg + DMAR_FECTL_REG);\n\t\twritel(iommu->iommu_state[SR_DMAR_FEDATA_REG],\n\t\t\tiommu->reg + DMAR_FEDATA_REG);\n\t\twritel(iommu->iommu_state[SR_DMAR_FEADDR_REG],\n\t\t\tiommu->reg + DMAR_FEADDR_REG);\n\t\twritel(iommu->iommu_state[SR_DMAR_FEUADDR_REG],\n\t\t\tiommu->reg + DMAR_FEUADDR_REG);\n\n\t\traw_spin_unlock_irqrestore(&iommu->register_lock, flag);\n\t}\n}\n\nstatic struct syscore_ops iommu_syscore_ops = {\n\t.resume\t\t= iommu_resume,\n\t.suspend\t= iommu_suspend,\n};\n\nstatic void __init init_iommu_pm_ops(void)\n{\n\tregister_syscore_ops(&iommu_syscore_ops);\n}\n\n#else\nstatic inline void init_iommu_pm_ops(void) {}\n#endif\t \n\nstatic int __init rmrr_sanity_check(struct acpi_dmar_reserved_memory *rmrr)\n{\n\tif (!IS_ALIGNED(rmrr->base_address, PAGE_SIZE) ||\n\t    !IS_ALIGNED(rmrr->end_address + 1, PAGE_SIZE) ||\n\t    rmrr->end_address <= rmrr->base_address ||\n\t    arch_rmrr_sanity_check(rmrr))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint __init dmar_parse_one_rmrr(struct acpi_dmar_header *header, void *arg)\n{\n\tstruct acpi_dmar_reserved_memory *rmrr;\n\tstruct dmar_rmrr_unit *rmrru;\n\n\trmrr = (struct acpi_dmar_reserved_memory *)header;\n\tif (rmrr_sanity_check(rmrr)) {\n\t\tpr_warn(FW_BUG\n\t\t\t   \"Your BIOS is broken; bad RMRR [%#018Lx-%#018Lx]\\n\"\n\t\t\t   \"BIOS vendor: %s; Ver: %s; Product Version: %s\\n\",\n\t\t\t   rmrr->base_address, rmrr->end_address,\n\t\t\t   dmi_get_system_info(DMI_BIOS_VENDOR),\n\t\t\t   dmi_get_system_info(DMI_BIOS_VERSION),\n\t\t\t   dmi_get_system_info(DMI_PRODUCT_VERSION));\n\t\tadd_taint(TAINT_FIRMWARE_WORKAROUND, LOCKDEP_STILL_OK);\n\t}\n\n\trmrru = kzalloc(sizeof(*rmrru), GFP_KERNEL);\n\tif (!rmrru)\n\t\tgoto out;\n\n\trmrru->hdr = header;\n\n\trmrru->base_address = rmrr->base_address;\n\trmrru->end_address = rmrr->end_address;\n\n\trmrru->devices = dmar_alloc_dev_scope((void *)(rmrr + 1),\n\t\t\t\t((void *)rmrr) + rmrr->header.length,\n\t\t\t\t&rmrru->devices_cnt);\n\tif (rmrru->devices_cnt && rmrru->devices == NULL)\n\t\tgoto free_rmrru;\n\n\tlist_add(&rmrru->list, &dmar_rmrr_units);\n\n\treturn 0;\nfree_rmrru:\n\tkfree(rmrru);\nout:\n\treturn -ENOMEM;\n}\n\nstatic struct dmar_atsr_unit *dmar_find_atsr(struct acpi_dmar_atsr *atsr)\n{\n\tstruct dmar_atsr_unit *atsru;\n\tstruct acpi_dmar_atsr *tmp;\n\n\tlist_for_each_entry_rcu(atsru, &dmar_atsr_units, list,\n\t\t\t\tdmar_rcu_check()) {\n\t\ttmp = (struct acpi_dmar_atsr *)atsru->hdr;\n\t\tif (atsr->segment != tmp->segment)\n\t\t\tcontinue;\n\t\tif (atsr->header.length != tmp->header.length)\n\t\t\tcontinue;\n\t\tif (memcmp(atsr, tmp, atsr->header.length) == 0)\n\t\t\treturn atsru;\n\t}\n\n\treturn NULL;\n}\n\nint dmar_parse_one_atsr(struct acpi_dmar_header *hdr, void *arg)\n{\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct dmar_atsr_unit *atsru;\n\n\tif (system_state >= SYSTEM_RUNNING && !intel_iommu_enabled)\n\t\treturn 0;\n\n\tatsr = container_of(hdr, struct acpi_dmar_atsr, header);\n\tatsru = dmar_find_atsr(atsr);\n\tif (atsru)\n\t\treturn 0;\n\n\tatsru = kzalloc(sizeof(*atsru) + hdr->length, GFP_KERNEL);\n\tif (!atsru)\n\t\treturn -ENOMEM;\n\n\t \n\tatsru->hdr = (void *)(atsru + 1);\n\tmemcpy(atsru->hdr, hdr, hdr->length);\n\tatsru->include_all = atsr->flags & 0x1;\n\tif (!atsru->include_all) {\n\t\tatsru->devices = dmar_alloc_dev_scope((void *)(atsr + 1),\n\t\t\t\t(void *)atsr + atsr->header.length,\n\t\t\t\t&atsru->devices_cnt);\n\t\tif (atsru->devices_cnt && atsru->devices == NULL) {\n\t\t\tkfree(atsru);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tlist_add_rcu(&atsru->list, &dmar_atsr_units);\n\n\treturn 0;\n}\n\nstatic void intel_iommu_free_atsr(struct dmar_atsr_unit *atsru)\n{\n\tdmar_free_dev_scope(&atsru->devices, &atsru->devices_cnt);\n\tkfree(atsru);\n}\n\nint dmar_release_one_atsr(struct acpi_dmar_header *hdr, void *arg)\n{\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct dmar_atsr_unit *atsru;\n\n\tatsr = container_of(hdr, struct acpi_dmar_atsr, header);\n\tatsru = dmar_find_atsr(atsr);\n\tif (atsru) {\n\t\tlist_del_rcu(&atsru->list);\n\t\tsynchronize_rcu();\n\t\tintel_iommu_free_atsr(atsru);\n\t}\n\n\treturn 0;\n}\n\nint dmar_check_one_atsr(struct acpi_dmar_header *hdr, void *arg)\n{\n\tint i;\n\tstruct device *dev;\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct dmar_atsr_unit *atsru;\n\n\tatsr = container_of(hdr, struct acpi_dmar_atsr, header);\n\tatsru = dmar_find_atsr(atsr);\n\tif (!atsru)\n\t\treturn 0;\n\n\tif (!atsru->include_all && atsru->devices && atsru->devices_cnt) {\n\t\tfor_each_active_dev_scope(atsru->devices, atsru->devices_cnt,\n\t\t\t\t\t  i, dev)\n\t\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic struct dmar_satc_unit *dmar_find_satc(struct acpi_dmar_satc *satc)\n{\n\tstruct dmar_satc_unit *satcu;\n\tstruct acpi_dmar_satc *tmp;\n\n\tlist_for_each_entry_rcu(satcu, &dmar_satc_units, list,\n\t\t\t\tdmar_rcu_check()) {\n\t\ttmp = (struct acpi_dmar_satc *)satcu->hdr;\n\t\tif (satc->segment != tmp->segment)\n\t\t\tcontinue;\n\t\tif (satc->header.length != tmp->header.length)\n\t\t\tcontinue;\n\t\tif (memcmp(satc, tmp, satc->header.length) == 0)\n\t\t\treturn satcu;\n\t}\n\n\treturn NULL;\n}\n\nint dmar_parse_one_satc(struct acpi_dmar_header *hdr, void *arg)\n{\n\tstruct acpi_dmar_satc *satc;\n\tstruct dmar_satc_unit *satcu;\n\n\tif (system_state >= SYSTEM_RUNNING && !intel_iommu_enabled)\n\t\treturn 0;\n\n\tsatc = container_of(hdr, struct acpi_dmar_satc, header);\n\tsatcu = dmar_find_satc(satc);\n\tif (satcu)\n\t\treturn 0;\n\n\tsatcu = kzalloc(sizeof(*satcu) + hdr->length, GFP_KERNEL);\n\tif (!satcu)\n\t\treturn -ENOMEM;\n\n\tsatcu->hdr = (void *)(satcu + 1);\n\tmemcpy(satcu->hdr, hdr, hdr->length);\n\tsatcu->atc_required = satc->flags & 0x1;\n\tsatcu->devices = dmar_alloc_dev_scope((void *)(satc + 1),\n\t\t\t\t\t      (void *)satc + satc->header.length,\n\t\t\t\t\t      &satcu->devices_cnt);\n\tif (satcu->devices_cnt && !satcu->devices) {\n\t\tkfree(satcu);\n\t\treturn -ENOMEM;\n\t}\n\tlist_add_rcu(&satcu->list, &dmar_satc_units);\n\n\treturn 0;\n}\n\nstatic int intel_iommu_add(struct dmar_drhd_unit *dmaru)\n{\n\tint sp, ret;\n\tstruct intel_iommu *iommu = dmaru->iommu;\n\n\tret = intel_cap_audit(CAP_AUDIT_HOTPLUG_DMAR, iommu);\n\tif (ret)\n\t\tgoto out;\n\n\tif (hw_pass_through && !ecap_pass_through(iommu->ecap)) {\n\t\tpr_warn(\"%s: Doesn't support hardware pass through.\\n\",\n\t\t\tiommu->name);\n\t\treturn -ENXIO;\n\t}\n\n\tsp = domain_update_iommu_superpage(NULL, iommu) - 1;\n\tif (sp >= 0 && !(cap_super_page_val(iommu->cap) & (1 << sp))) {\n\t\tpr_warn(\"%s: Doesn't support large page.\\n\",\n\t\t\tiommu->name);\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tif (iommu->gcmd & DMA_GCMD_TE)\n\t\tiommu_disable_translation(iommu);\n\n\tret = iommu_init_domains(iommu);\n\tif (ret == 0)\n\t\tret = iommu_alloc_root_entry(iommu);\n\tif (ret)\n\t\tgoto out;\n\n\tintel_svm_check(iommu);\n\n\tif (dmaru->ignored) {\n\t\t \n\t\tif (force_on)\n\t\t\tiommu_disable_protect_mem_regions(iommu);\n\t\treturn 0;\n\t}\n\n\tintel_iommu_init_qi(iommu);\n\tiommu_flush_write_buffer(iommu);\n\n#ifdef CONFIG_INTEL_IOMMU_SVM\n\tif (pasid_supported(iommu) && ecap_prs(iommu->ecap)) {\n\t\tret = intel_svm_enable_prq(iommu);\n\t\tif (ret)\n\t\t\tgoto disable_iommu;\n\t}\n#endif\n\tret = dmar_set_interrupt(iommu);\n\tif (ret)\n\t\tgoto disable_iommu;\n\n\tiommu_set_root_entry(iommu);\n\tiommu_enable_translation(iommu);\n\n\tiommu_disable_protect_mem_regions(iommu);\n\treturn 0;\n\ndisable_iommu:\n\tdisable_dmar_iommu(iommu);\nout:\n\tfree_dmar_iommu(iommu);\n\treturn ret;\n}\n\nint dmar_iommu_hotplug(struct dmar_drhd_unit *dmaru, bool insert)\n{\n\tint ret = 0;\n\tstruct intel_iommu *iommu = dmaru->iommu;\n\n\tif (!intel_iommu_enabled)\n\t\treturn 0;\n\tif (iommu == NULL)\n\t\treturn -EINVAL;\n\n\tif (insert) {\n\t\tret = intel_iommu_add(dmaru);\n\t} else {\n\t\tdisable_dmar_iommu(iommu);\n\t\tfree_dmar_iommu(iommu);\n\t}\n\n\treturn ret;\n}\n\nstatic void intel_iommu_free_dmars(void)\n{\n\tstruct dmar_rmrr_unit *rmrru, *rmrr_n;\n\tstruct dmar_atsr_unit *atsru, *atsr_n;\n\tstruct dmar_satc_unit *satcu, *satc_n;\n\n\tlist_for_each_entry_safe(rmrru, rmrr_n, &dmar_rmrr_units, list) {\n\t\tlist_del(&rmrru->list);\n\t\tdmar_free_dev_scope(&rmrru->devices, &rmrru->devices_cnt);\n\t\tkfree(rmrru);\n\t}\n\n\tlist_for_each_entry_safe(atsru, atsr_n, &dmar_atsr_units, list) {\n\t\tlist_del(&atsru->list);\n\t\tintel_iommu_free_atsr(atsru);\n\t}\n\tlist_for_each_entry_safe(satcu, satc_n, &dmar_satc_units, list) {\n\t\tlist_del(&satcu->list);\n\t\tdmar_free_dev_scope(&satcu->devices, &satcu->devices_cnt);\n\t\tkfree(satcu);\n\t}\n}\n\nstatic struct dmar_satc_unit *dmar_find_matched_satc_unit(struct pci_dev *dev)\n{\n\tstruct dmar_satc_unit *satcu;\n\tstruct acpi_dmar_satc *satc;\n\tstruct device *tmp;\n\tint i;\n\n\tdev = pci_physfn(dev);\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(satcu, &dmar_satc_units, list) {\n\t\tsatc = container_of(satcu->hdr, struct acpi_dmar_satc, header);\n\t\tif (satc->segment != pci_domain_nr(dev->bus))\n\t\t\tcontinue;\n\t\tfor_each_dev_scope(satcu->devices, satcu->devices_cnt, i, tmp)\n\t\t\tif (to_pci_dev(tmp) == dev)\n\t\t\t\tgoto out;\n\t}\n\tsatcu = NULL;\nout:\n\trcu_read_unlock();\n\treturn satcu;\n}\n\nstatic int dmar_ats_supported(struct pci_dev *dev, struct intel_iommu *iommu)\n{\n\tint i, ret = 1;\n\tstruct pci_bus *bus;\n\tstruct pci_dev *bridge = NULL;\n\tstruct device *tmp;\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct dmar_atsr_unit *atsru;\n\tstruct dmar_satc_unit *satcu;\n\n\tdev = pci_physfn(dev);\n\tsatcu = dmar_find_matched_satc_unit(dev);\n\tif (satcu)\n\t\t \n\t\treturn !(satcu->atc_required && !sm_supported(iommu));\n\n\tfor (bus = dev->bus; bus; bus = bus->parent) {\n\t\tbridge = bus->self;\n\t\t \n\t\tif (!bridge)\n\t\t\treturn 1;\n\t\t \n\t\tif (!pci_is_pcie(bridge) ||\n\t\t    pci_pcie_type(bridge) == PCI_EXP_TYPE_PCI_BRIDGE)\n\t\t\treturn 0;\n\t\t \n\t\tif (pci_pcie_type(bridge) == PCI_EXP_TYPE_ROOT_PORT)\n\t\t\tbreak;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(atsru, &dmar_atsr_units, list) {\n\t\tatsr = container_of(atsru->hdr, struct acpi_dmar_atsr, header);\n\t\tif (atsr->segment != pci_domain_nr(dev->bus))\n\t\t\tcontinue;\n\n\t\tfor_each_dev_scope(atsru->devices, atsru->devices_cnt, i, tmp)\n\t\t\tif (tmp == &bridge->dev)\n\t\t\t\tgoto out;\n\n\t\tif (atsru->include_all)\n\t\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nint dmar_iommu_notify_scope_dev(struct dmar_pci_notify_info *info)\n{\n\tint ret;\n\tstruct dmar_rmrr_unit *rmrru;\n\tstruct dmar_atsr_unit *atsru;\n\tstruct dmar_satc_unit *satcu;\n\tstruct acpi_dmar_atsr *atsr;\n\tstruct acpi_dmar_reserved_memory *rmrr;\n\tstruct acpi_dmar_satc *satc;\n\n\tif (!intel_iommu_enabled && system_state >= SYSTEM_RUNNING)\n\t\treturn 0;\n\n\tlist_for_each_entry(rmrru, &dmar_rmrr_units, list) {\n\t\trmrr = container_of(rmrru->hdr,\n\t\t\t\t    struct acpi_dmar_reserved_memory, header);\n\t\tif (info->event == BUS_NOTIFY_ADD_DEVICE) {\n\t\t\tret = dmar_insert_dev_scope(info, (void *)(rmrr + 1),\n\t\t\t\t((void *)rmrr) + rmrr->header.length,\n\t\t\t\trmrr->segment, rmrru->devices,\n\t\t\t\trmrru->devices_cnt);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t} else if (info->event == BUS_NOTIFY_REMOVED_DEVICE) {\n\t\t\tdmar_remove_dev_scope(info, rmrr->segment,\n\t\t\t\trmrru->devices, rmrru->devices_cnt);\n\t\t}\n\t}\n\n\tlist_for_each_entry(atsru, &dmar_atsr_units, list) {\n\t\tif (atsru->include_all)\n\t\t\tcontinue;\n\n\t\tatsr = container_of(atsru->hdr, struct acpi_dmar_atsr, header);\n\t\tif (info->event == BUS_NOTIFY_ADD_DEVICE) {\n\t\t\tret = dmar_insert_dev_scope(info, (void *)(atsr + 1),\n\t\t\t\t\t(void *)atsr + atsr->header.length,\n\t\t\t\t\tatsr->segment, atsru->devices,\n\t\t\t\t\tatsru->devices_cnt);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\telse if (ret < 0)\n\t\t\t\treturn ret;\n\t\t} else if (info->event == BUS_NOTIFY_REMOVED_DEVICE) {\n\t\t\tif (dmar_remove_dev_scope(info, atsr->segment,\n\t\t\t\t\tatsru->devices, atsru->devices_cnt))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tlist_for_each_entry(satcu, &dmar_satc_units, list) {\n\t\tsatc = container_of(satcu->hdr, struct acpi_dmar_satc, header);\n\t\tif (info->event == BUS_NOTIFY_ADD_DEVICE) {\n\t\t\tret = dmar_insert_dev_scope(info, (void *)(satc + 1),\n\t\t\t\t\t(void *)satc + satc->header.length,\n\t\t\t\t\tsatc->segment, satcu->devices,\n\t\t\t\t\tsatcu->devices_cnt);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\telse if (ret < 0)\n\t\t\t\treturn ret;\n\t\t} else if (info->event == BUS_NOTIFY_REMOVED_DEVICE) {\n\t\t\tif (dmar_remove_dev_scope(info, satc->segment,\n\t\t\t\t\tsatcu->devices, satcu->devices_cnt))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_iommu_memory_notifier(struct notifier_block *nb,\n\t\t\t\t       unsigned long val, void *v)\n{\n\tstruct memory_notify *mhp = v;\n\tunsigned long start_vpfn = mm_to_dma_pfn_start(mhp->start_pfn);\n\tunsigned long last_vpfn = mm_to_dma_pfn_end(mhp->start_pfn +\n\t\t\tmhp->nr_pages - 1);\n\n\tswitch (val) {\n\tcase MEM_GOING_ONLINE:\n\t\tif (iommu_domain_identity_map(si_domain,\n\t\t\t\t\t      start_vpfn, last_vpfn)) {\n\t\t\tpr_warn(\"Failed to build identity map for [%lx-%lx]\\n\",\n\t\t\t\tstart_vpfn, last_vpfn);\n\t\t\treturn NOTIFY_BAD;\n\t\t}\n\t\tbreak;\n\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\t\t{\n\t\t\tstruct dmar_drhd_unit *drhd;\n\t\t\tstruct intel_iommu *iommu;\n\t\t\tLIST_HEAD(freelist);\n\n\t\t\tdomain_unmap(si_domain, start_vpfn, last_vpfn, &freelist);\n\n\t\t\trcu_read_lock();\n\t\t\tfor_each_active_iommu(iommu, drhd)\n\t\t\t\tiommu_flush_iotlb_psi(iommu, si_domain,\n\t\t\t\t\tstart_vpfn, mhp->nr_pages,\n\t\t\t\t\tlist_empty(&freelist), 0);\n\t\t\trcu_read_unlock();\n\t\t\tput_pages_list(&freelist);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block intel_iommu_memory_nb = {\n\t.notifier_call = intel_iommu_memory_notifier,\n\t.priority = 0\n};\n\nstatic void intel_disable_iommus(void)\n{\n\tstruct intel_iommu *iommu = NULL;\n\tstruct dmar_drhd_unit *drhd;\n\n\tfor_each_iommu(iommu, drhd)\n\t\tiommu_disable_translation(iommu);\n}\n\nvoid intel_iommu_shutdown(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu = NULL;\n\n\tif (no_iommu || dmar_disabled)\n\t\treturn;\n\n\tdown_write(&dmar_global_lock);\n\n\t \n\tfor_each_iommu(iommu, drhd)\n\t\tiommu_disable_protect_mem_regions(iommu);\n\n\t \n\tintel_disable_iommus();\n\n\tup_write(&dmar_global_lock);\n}\n\nstatic inline struct intel_iommu *dev_to_intel_iommu(struct device *dev)\n{\n\tstruct iommu_device *iommu_dev = dev_to_iommu_device(dev);\n\n\treturn container_of(iommu_dev, struct intel_iommu, iommu);\n}\n\nstatic ssize_t version_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\tu32 ver = readl(iommu->reg + DMAR_VER_REG);\n\treturn sysfs_emit(buf, \"%d:%d\\n\",\n\t\t\t  DMAR_VER_MAJOR(ver), DMAR_VER_MINOR(ver));\n}\nstatic DEVICE_ATTR_RO(version);\n\nstatic ssize_t address_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\treturn sysfs_emit(buf, \"%llx\\n\", iommu->reg_phys);\n}\nstatic DEVICE_ATTR_RO(address);\n\nstatic ssize_t cap_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\treturn sysfs_emit(buf, \"%llx\\n\", iommu->cap);\n}\nstatic DEVICE_ATTR_RO(cap);\n\nstatic ssize_t ecap_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\treturn sysfs_emit(buf, \"%llx\\n\", iommu->ecap);\n}\nstatic DEVICE_ATTR_RO(ecap);\n\nstatic ssize_t domains_supported_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\treturn sysfs_emit(buf, \"%ld\\n\", cap_ndoms(iommu->cap));\n}\nstatic DEVICE_ATTR_RO(domains_supported);\n\nstatic ssize_t domains_used_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct intel_iommu *iommu = dev_to_intel_iommu(dev);\n\treturn sysfs_emit(buf, \"%d\\n\",\n\t\t\t  bitmap_weight(iommu->domain_ids,\n\t\t\t\t\tcap_ndoms(iommu->cap)));\n}\nstatic DEVICE_ATTR_RO(domains_used);\n\nstatic struct attribute *intel_iommu_attrs[] = {\n\t&dev_attr_version.attr,\n\t&dev_attr_address.attr,\n\t&dev_attr_cap.attr,\n\t&dev_attr_ecap.attr,\n\t&dev_attr_domains_supported.attr,\n\t&dev_attr_domains_used.attr,\n\tNULL,\n};\n\nstatic struct attribute_group intel_iommu_group = {\n\t.name = \"intel-iommu\",\n\t.attrs = intel_iommu_attrs,\n};\n\nconst struct attribute_group *intel_iommu_groups[] = {\n\t&intel_iommu_group,\n\tNULL,\n};\n\nstatic inline bool has_external_pci(void)\n{\n\tstruct pci_dev *pdev = NULL;\n\n\tfor_each_pci_dev(pdev)\n\t\tif (pdev->external_facing) {\n\t\t\tpci_dev_put(pdev);\n\t\t\treturn true;\n\t\t}\n\n\treturn false;\n}\n\nstatic int __init platform_optin_force_iommu(void)\n{\n\tif (!dmar_platform_optin() || no_platform_optin || !has_external_pci())\n\t\treturn 0;\n\n\tif (no_iommu || dmar_disabled)\n\t\tpr_info(\"Intel-IOMMU force enabled due to platform opt in\\n\");\n\n\t \n\tif (dmar_disabled)\n\t\tiommu_set_default_passthrough(false);\n\n\tdmar_disabled = 0;\n\tno_iommu = 0;\n\n\treturn 1;\n}\n\nstatic int __init probe_acpi_namespace_devices(void)\n{\n\tstruct dmar_drhd_unit *drhd;\n\t \n\tstruct intel_iommu *iommu __maybe_unused;\n\tstruct device *dev;\n\tint i, ret = 0;\n\n\tfor_each_active_iommu(iommu, drhd) {\n\t\tfor_each_active_dev_scope(drhd->devices,\n\t\t\t\t\t  drhd->devices_cnt, i, dev) {\n\t\t\tstruct acpi_device_physical_node *pn;\n\t\t\tstruct acpi_device *adev;\n\n\t\t\tif (dev->bus != &acpi_bus_type)\n\t\t\t\tcontinue;\n\n\t\t\tadev = to_acpi_device(dev);\n\t\t\tmutex_lock(&adev->physical_node_lock);\n\t\t\tlist_for_each_entry(pn,\n\t\t\t\t\t    &adev->physical_node_list, node) {\n\t\t\t\tret = iommu_probe_device(pn->dev);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmutex_unlock(&adev->physical_node_lock);\n\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic __init int tboot_force_iommu(void)\n{\n\tif (!tboot_enabled())\n\t\treturn 0;\n\n\tif (no_iommu || dmar_disabled)\n\t\tpr_warn(\"Forcing Intel-IOMMU to enabled\\n\");\n\n\tdmar_disabled = 0;\n\tno_iommu = 0;\n\n\treturn 1;\n}\n\nint __init intel_iommu_init(void)\n{\n\tint ret = -ENODEV;\n\tstruct dmar_drhd_unit *drhd;\n\tstruct intel_iommu *iommu;\n\n\t \n\tforce_on = (!intel_iommu_tboot_noforce && tboot_force_iommu()) ||\n\t\t    platform_optin_force_iommu();\n\n\tdown_write(&dmar_global_lock);\n\tif (dmar_table_init()) {\n\t\tif (force_on)\n\t\t\tpanic(\"tboot: Failed to initialize DMAR table\\n\");\n\t\tgoto out_free_dmar;\n\t}\n\n\tif (dmar_dev_scope_init() < 0) {\n\t\tif (force_on)\n\t\t\tpanic(\"tboot: Failed to initialize DMAR device scope\\n\");\n\t\tgoto out_free_dmar;\n\t}\n\n\tup_write(&dmar_global_lock);\n\n\t \n\tdmar_register_bus_notifier();\n\n\tdown_write(&dmar_global_lock);\n\n\tif (!no_iommu)\n\t\tintel_iommu_debugfs_init();\n\n\tif (no_iommu || dmar_disabled) {\n\t\t \n\t\tif (intel_iommu_tboot_noforce) {\n\t\t\tfor_each_iommu(iommu, drhd)\n\t\t\t\tiommu_disable_protect_mem_regions(iommu);\n\t\t}\n\n\t\t \n\t\tintel_disable_iommus();\n\t\tgoto out_free_dmar;\n\t}\n\n\tif (list_empty(&dmar_rmrr_units))\n\t\tpr_info(\"No RMRR found\\n\");\n\n\tif (list_empty(&dmar_atsr_units))\n\t\tpr_info(\"No ATSR found\\n\");\n\n\tif (list_empty(&dmar_satc_units))\n\t\tpr_info(\"No SATC found\\n\");\n\n\tinit_no_remapping_devices();\n\n\tret = init_dmars();\n\tif (ret) {\n\t\tif (force_on)\n\t\t\tpanic(\"tboot: Failed to initialize DMARs\\n\");\n\t\tpr_err(\"Initialization failed\\n\");\n\t\tgoto out_free_dmar;\n\t}\n\tup_write(&dmar_global_lock);\n\n\tinit_iommu_pm_ops();\n\n\tdown_read(&dmar_global_lock);\n\tfor_each_active_iommu(iommu, drhd) {\n\t\t \n\t\tif (cap_caching_mode(iommu->cap) &&\n\t\t    !first_level_by_default(IOMMU_DOMAIN_DMA)) {\n\t\t\tpr_info_once(\"IOMMU batching disallowed due to virtualization\\n\");\n\t\t\tiommu_set_dma_strict();\n\t\t}\n\t\tiommu_device_sysfs_add(&iommu->iommu, NULL,\n\t\t\t\t       intel_iommu_groups,\n\t\t\t\t       \"%s\", iommu->name);\n\t\tiommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);\n\n\t\tiommu_pmu_register(iommu);\n\t}\n\tup_read(&dmar_global_lock);\n\n\tif (si_domain && !hw_pass_through)\n\t\tregister_memory_notifier(&intel_iommu_memory_nb);\n\n\tdown_read(&dmar_global_lock);\n\tif (probe_acpi_namespace_devices())\n\t\tpr_warn(\"ACPI name space devices didn't probe correctly\\n\");\n\n\t \n\tfor_each_iommu(iommu, drhd) {\n\t\tif (!drhd->ignored && !translation_pre_enabled(iommu))\n\t\t\tiommu_enable_translation(iommu);\n\n\t\tiommu_disable_protect_mem_regions(iommu);\n\t}\n\tup_read(&dmar_global_lock);\n\n\tpr_info(\"Intel(R) Virtualization Technology for Directed I/O\\n\");\n\n\tintel_iommu_enabled = 1;\n\n\treturn 0;\n\nout_free_dmar:\n\tintel_iommu_free_dmars();\n\tup_write(&dmar_global_lock);\n\treturn ret;\n}\n\nstatic int domain_context_clear_one_cb(struct pci_dev *pdev, u16 alias, void *opaque)\n{\n\tstruct device_domain_info *info = opaque;\n\n\tdomain_context_clear_one(info, PCI_BUS_NUM(alias), alias & 0xff);\n\treturn 0;\n}\n\n \nstatic void domain_context_clear(struct device_domain_info *info)\n{\n\tif (!dev_is_pci(info->dev))\n\t\tdomain_context_clear_one(info, info->bus, info->devfn);\n\n\tpci_for_each_dma_alias(to_pci_dev(info->dev),\n\t\t\t       &domain_context_clear_one_cb, info);\n}\n\nstatic void dmar_remove_one_dev_info(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct dmar_domain *domain = info->domain;\n\tstruct intel_iommu *iommu = info->iommu;\n\tunsigned long flags;\n\n\tif (!dev_is_real_dma_subdevice(info->dev)) {\n\t\tif (dev_is_pci(info->dev) && sm_supported(iommu))\n\t\t\tintel_pasid_tear_down_entry(iommu, info->dev,\n\t\t\t\t\tIOMMU_NO_PASID, false);\n\n\t\tiommu_disable_pci_caps(info);\n\t\tdomain_context_clear(info);\n\t}\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_del(&info->link);\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\tdomain_detach_iommu(domain, iommu);\n\tinfo->domain = NULL;\n}\n\n \nstatic void device_block_translation(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu = info->iommu;\n\tunsigned long flags;\n\n\tiommu_disable_pci_caps(info);\n\tif (!dev_is_real_dma_subdevice(dev)) {\n\t\tif (sm_supported(iommu))\n\t\t\tintel_pasid_tear_down_entry(iommu, dev,\n\t\t\t\t\t\t    IOMMU_NO_PASID, false);\n\t\telse\n\t\t\tdomain_context_clear(info);\n\t}\n\n\tif (!info->domain)\n\t\treturn;\n\n\tspin_lock_irqsave(&info->domain->lock, flags);\n\tlist_del(&info->link);\n\tspin_unlock_irqrestore(&info->domain->lock, flags);\n\n\tdomain_detach_iommu(info->domain, iommu);\n\tinfo->domain = NULL;\n}\n\nstatic int md_domain_init(struct dmar_domain *domain, int guest_width)\n{\n\tint adjust_width;\n\n\t \n\tdomain->gaw = guest_width;\n\tadjust_width = guestwidth_to_adjustwidth(guest_width);\n\tdomain->agaw = width_to_agaw(adjust_width);\n\n\tdomain->iommu_coherency = false;\n\tdomain->iommu_superpage = 0;\n\tdomain->max_addr = 0;\n\n\t \n\tdomain->pgd = alloc_pgtable_page(domain->nid, GFP_ATOMIC);\n\tif (!domain->pgd)\n\t\treturn -ENOMEM;\n\tdomain_flush_cache(domain, domain->pgd, PAGE_SIZE);\n\treturn 0;\n}\n\nstatic int blocking_domain_attach_dev(struct iommu_domain *domain,\n\t\t\t\t      struct device *dev)\n{\n\tdevice_block_translation(dev);\n\treturn 0;\n}\n\nstatic struct iommu_domain blocking_domain = {\n\t.ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= blocking_domain_attach_dev,\n\t\t.free\t\t= intel_iommu_domain_free\n\t}\n};\n\nstatic struct iommu_domain *intel_iommu_domain_alloc(unsigned type)\n{\n\tstruct dmar_domain *dmar_domain;\n\tstruct iommu_domain *domain;\n\n\tswitch (type) {\n\tcase IOMMU_DOMAIN_BLOCKED:\n\t\treturn &blocking_domain;\n\tcase IOMMU_DOMAIN_DMA:\n\tcase IOMMU_DOMAIN_UNMANAGED:\n\t\tdmar_domain = alloc_domain(type);\n\t\tif (!dmar_domain) {\n\t\t\tpr_err(\"Can't allocate dmar_domain\\n\");\n\t\t\treturn NULL;\n\t\t}\n\t\tif (md_domain_init(dmar_domain, DEFAULT_DOMAIN_ADDRESS_WIDTH)) {\n\t\t\tpr_err(\"Domain initialization failed\\n\");\n\t\t\tdomain_exit(dmar_domain);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tdomain = &dmar_domain->domain;\n\t\tdomain->geometry.aperture_start = 0;\n\t\tdomain->geometry.aperture_end   =\n\t\t\t\t__DOMAIN_MAX_ADDR(dmar_domain->gaw);\n\t\tdomain->geometry.force_aperture = true;\n\n\t\treturn domain;\n\tcase IOMMU_DOMAIN_IDENTITY:\n\t\treturn &si_domain->domain;\n\tcase IOMMU_DOMAIN_SVA:\n\t\treturn intel_svm_domain_alloc();\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\treturn NULL;\n}\n\nstatic void intel_iommu_domain_free(struct iommu_domain *domain)\n{\n\tif (domain != &si_domain->domain && domain != &blocking_domain)\n\t\tdomain_exit(to_dmar_domain(domain));\n}\n\nstatic int prepare_domain_attach_device(struct iommu_domain *domain,\n\t\t\t\t\tstruct device *dev)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tstruct intel_iommu *iommu;\n\tint addr_width;\n\n\tiommu = device_to_iommu(dev, NULL, NULL);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tif (dmar_domain->force_snooping && !ecap_sc_support(iommu->ecap))\n\t\treturn -EINVAL;\n\n\t \n\taddr_width = agaw_to_width(iommu->agaw);\n\tif (addr_width > cap_mgaw(iommu->cap))\n\t\taddr_width = cap_mgaw(iommu->cap);\n\n\tif (dmar_domain->max_addr > (1LL << addr_width))\n\t\treturn -EINVAL;\n\tdmar_domain->gaw = addr_width;\n\n\t \n\twhile (iommu->agaw < dmar_domain->agaw) {\n\t\tstruct dma_pte *pte;\n\n\t\tpte = dmar_domain->pgd;\n\t\tif (dma_pte_present(pte)) {\n\t\t\tdmar_domain->pgd = phys_to_virt(dma_pte_addr(pte));\n\t\t\tfree_pgtable_page(pte);\n\t\t}\n\t\tdmar_domain->agaw--;\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_iommu_attach_device(struct iommu_domain *domain,\n\t\t\t\t     struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tint ret;\n\n\tif (info->domain)\n\t\tdevice_block_translation(dev);\n\n\tret = prepare_domain_attach_device(domain, dev);\n\tif (ret)\n\t\treturn ret;\n\n\treturn dmar_domain_attach_device(to_dmar_domain(domain), dev);\n}\n\nstatic int intel_iommu_map(struct iommu_domain *domain,\n\t\t\t   unsigned long iova, phys_addr_t hpa,\n\t\t\t   size_t size, int iommu_prot, gfp_t gfp)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tu64 max_addr;\n\tint prot = 0;\n\n\tif (iommu_prot & IOMMU_READ)\n\t\tprot |= DMA_PTE_READ;\n\tif (iommu_prot & IOMMU_WRITE)\n\t\tprot |= DMA_PTE_WRITE;\n\tif (dmar_domain->set_pte_snp)\n\t\tprot |= DMA_PTE_SNP;\n\n\tmax_addr = iova + size;\n\tif (dmar_domain->max_addr < max_addr) {\n\t\tu64 end;\n\n\t\t \n\t\tend = __DOMAIN_MAX_ADDR(dmar_domain->gaw) + 1;\n\t\tif (end < max_addr) {\n\t\t\tpr_err(\"%s: iommu width (%d) is not \"\n\t\t\t       \"sufficient for the mapped address (%llx)\\n\",\n\t\t\t       __func__, dmar_domain->gaw, max_addr);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tdmar_domain->max_addr = max_addr;\n\t}\n\t \n\tsize = aligned_nrpages(hpa, size);\n\treturn __domain_mapping(dmar_domain, iova >> VTD_PAGE_SHIFT,\n\t\t\t\thpa >> VTD_PAGE_SHIFT, size, prot, gfp);\n}\n\nstatic int intel_iommu_map_pages(struct iommu_domain *domain,\n\t\t\t\t unsigned long iova, phys_addr_t paddr,\n\t\t\t\t size_t pgsize, size_t pgcount,\n\t\t\t\t int prot, gfp_t gfp, size_t *mapped)\n{\n\tunsigned long pgshift = __ffs(pgsize);\n\tsize_t size = pgcount << pgshift;\n\tint ret;\n\n\tif (pgsize != SZ_4K && pgsize != SZ_2M && pgsize != SZ_1G)\n\t\treturn -EINVAL;\n\n\tif (!IS_ALIGNED(iova | paddr, pgsize))\n\t\treturn -EINVAL;\n\n\tret = intel_iommu_map(domain, iova, paddr, size, prot, gfp);\n\tif (!ret && mapped)\n\t\t*mapped = size;\n\n\treturn ret;\n}\n\nstatic size_t intel_iommu_unmap(struct iommu_domain *domain,\n\t\t\t\tunsigned long iova, size_t size,\n\t\t\t\tstruct iommu_iotlb_gather *gather)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tunsigned long start_pfn, last_pfn;\n\tint level = 0;\n\n\t \n\tif (unlikely(!pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT,\n\t\t\t\t     &level, GFP_ATOMIC)))\n\t\treturn 0;\n\n\tif (size < VTD_PAGE_SIZE << level_to_offset_bits(level))\n\t\tsize = VTD_PAGE_SIZE << level_to_offset_bits(level);\n\n\tstart_pfn = iova >> VTD_PAGE_SHIFT;\n\tlast_pfn = (iova + size - 1) >> VTD_PAGE_SHIFT;\n\n\tdomain_unmap(dmar_domain, start_pfn, last_pfn, &gather->freelist);\n\n\tif (dmar_domain->max_addr == iova + size)\n\t\tdmar_domain->max_addr = iova;\n\n\t \n\tif (!iommu_iotlb_gather_queued(gather))\n\t\tiommu_iotlb_gather_add_page(domain, gather, iova, size);\n\n\treturn size;\n}\n\nstatic size_t intel_iommu_unmap_pages(struct iommu_domain *domain,\n\t\t\t\t      unsigned long iova,\n\t\t\t\t      size_t pgsize, size_t pgcount,\n\t\t\t\t      struct iommu_iotlb_gather *gather)\n{\n\tunsigned long pgshift = __ffs(pgsize);\n\tsize_t size = pgcount << pgshift;\n\n\treturn intel_iommu_unmap(domain, iova, size, gather);\n}\n\nstatic void intel_iommu_tlb_sync(struct iommu_domain *domain,\n\t\t\t\t struct iommu_iotlb_gather *gather)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tunsigned long iova_pfn = IOVA_PFN(gather->start);\n\tsize_t size = gather->end - gather->start;\n\tstruct iommu_domain_info *info;\n\tunsigned long start_pfn;\n\tunsigned long nrpages;\n\tunsigned long i;\n\n\tnrpages = aligned_nrpages(gather->start, size);\n\tstart_pfn = mm_to_dma_pfn_start(iova_pfn);\n\n\txa_for_each(&dmar_domain->iommu_array, i, info)\n\t\tiommu_flush_iotlb_psi(info->iommu, dmar_domain,\n\t\t\t\t      start_pfn, nrpages,\n\t\t\t\t      list_empty(&gather->freelist), 0);\n\n\tput_pages_list(&gather->freelist);\n}\n\nstatic phys_addr_t intel_iommu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\t    dma_addr_t iova)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tstruct dma_pte *pte;\n\tint level = 0;\n\tu64 phys = 0;\n\n\tpte = pfn_to_dma_pte(dmar_domain, iova >> VTD_PAGE_SHIFT, &level,\n\t\t\t     GFP_ATOMIC);\n\tif (pte && dma_pte_present(pte))\n\t\tphys = dma_pte_addr(pte) +\n\t\t\t(iova & (BIT_MASK(level_to_offset_bits(level) +\n\t\t\t\t\t\tVTD_PAGE_SHIFT) - 1));\n\n\treturn phys;\n}\n\nstatic bool domain_support_force_snooping(struct dmar_domain *domain)\n{\n\tstruct device_domain_info *info;\n\tbool support = true;\n\n\tassert_spin_locked(&domain->lock);\n\tlist_for_each_entry(info, &domain->devices, link) {\n\t\tif (!ecap_sc_support(info->iommu->ecap)) {\n\t\t\tsupport = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn support;\n}\n\nstatic void domain_set_force_snooping(struct dmar_domain *domain)\n{\n\tstruct device_domain_info *info;\n\n\tassert_spin_locked(&domain->lock);\n\t \n\tif (!domain->use_first_level) {\n\t\tdomain->set_pte_snp = true;\n\t\treturn;\n\t}\n\n\tlist_for_each_entry(info, &domain->devices, link)\n\t\tintel_pasid_setup_page_snoop_control(info->iommu, info->dev,\n\t\t\t\t\t\t     IOMMU_NO_PASID);\n}\n\nstatic bool intel_iommu_enforce_cache_coherency(struct iommu_domain *domain)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tunsigned long flags;\n\n\tif (dmar_domain->force_snooping)\n\t\treturn true;\n\n\tspin_lock_irqsave(&dmar_domain->lock, flags);\n\tif (!domain_support_force_snooping(dmar_domain) ||\n\t    (!dmar_domain->use_first_level && dmar_domain->has_mappings)) {\n\t\tspin_unlock_irqrestore(&dmar_domain->lock, flags);\n\t\treturn false;\n\t}\n\n\tdomain_set_force_snooping(dmar_domain);\n\tdmar_domain->force_snooping = true;\n\tspin_unlock_irqrestore(&dmar_domain->lock, flags);\n\n\treturn true;\n}\n\nstatic bool intel_iommu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\n\tswitch (cap) {\n\tcase IOMMU_CAP_CACHE_COHERENCY:\n\tcase IOMMU_CAP_DEFERRED_FLUSH:\n\t\treturn true;\n\tcase IOMMU_CAP_PRE_BOOT_PROTECTION:\n\t\treturn dmar_platform_optin();\n\tcase IOMMU_CAP_ENFORCE_CACHE_COHERENCY:\n\t\treturn ecap_sc_support(info->iommu->ecap);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct iommu_device *intel_iommu_probe_device(struct device *dev)\n{\n\tstruct pci_dev *pdev = dev_is_pci(dev) ? to_pci_dev(dev) : NULL;\n\tstruct device_domain_info *info;\n\tstruct intel_iommu *iommu;\n\tu8 bus, devfn;\n\tint ret;\n\n\tiommu = device_to_iommu(dev, &bus, &devfn);\n\tif (!iommu || !iommu->iommu.ops)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (dev_is_real_dma_subdevice(dev)) {\n\t\tinfo->bus = pdev->bus->number;\n\t\tinfo->devfn = pdev->devfn;\n\t\tinfo->segment = pci_domain_nr(pdev->bus);\n\t} else {\n\t\tinfo->bus = bus;\n\t\tinfo->devfn = devfn;\n\t\tinfo->segment = iommu->segment;\n\t}\n\n\tinfo->dev = dev;\n\tinfo->iommu = iommu;\n\tif (dev_is_pci(dev)) {\n\t\tif (ecap_dev_iotlb_support(iommu->ecap) &&\n\t\t    pci_ats_supported(pdev) &&\n\t\t    dmar_ats_supported(pdev, iommu)) {\n\t\t\tinfo->ats_supported = 1;\n\t\t\tinfo->dtlb_extra_inval = dev_needs_extra_dtlb_flush(pdev);\n\n\t\t\t \n\t\t\tif (ecap_dit(iommu->ecap))\n\t\t\t\tinfo->pfsid = pci_dev_id(pci_physfn(pdev));\n\t\t\tinfo->ats_qdep = pci_ats_queue_depth(pdev);\n\t\t}\n\t\tif (sm_supported(iommu)) {\n\t\t\tif (pasid_supported(iommu)) {\n\t\t\t\tint features = pci_pasid_features(pdev);\n\n\t\t\t\tif (features >= 0)\n\t\t\t\t\tinfo->pasid_supported = features | 1;\n\t\t\t}\n\n\t\t\tif (info->ats_supported && ecap_prs(iommu->ecap) &&\n\t\t\t    pci_pri_supported(pdev))\n\t\t\t\tinfo->pri_supported = 1;\n\t\t}\n\t}\n\n\tdev_iommu_priv_set(dev, info);\n\n\tif (sm_supported(iommu) && !dev_is_real_dma_subdevice(dev)) {\n\t\tret = intel_pasid_alloc_table(dev);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"PASID table allocation failed\\n\");\n\t\t\tdev_iommu_priv_set(dev, NULL);\n\t\t\tkfree(info);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\n\treturn &iommu->iommu;\n}\n\nstatic void intel_iommu_release_device(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\n\tdmar_remove_one_dev_info(dev);\n\tintel_pasid_free_table(dev);\n\tdev_iommu_priv_set(dev, NULL);\n\tkfree(info);\n\tset_dma_ops(dev, NULL);\n}\n\nstatic void intel_iommu_probe_finalize(struct device *dev)\n{\n\tset_dma_ops(dev, NULL);\n\tiommu_setup_dma_ops(dev, 0, U64_MAX);\n}\n\nstatic void intel_iommu_get_resv_regions(struct device *device,\n\t\t\t\t\t struct list_head *head)\n{\n\tint prot = DMA_PTE_READ | DMA_PTE_WRITE;\n\tstruct iommu_resv_region *reg;\n\tstruct dmar_rmrr_unit *rmrr;\n\tstruct device *i_dev;\n\tint i;\n\n\trcu_read_lock();\n\tfor_each_rmrr_units(rmrr) {\n\t\tfor_each_active_dev_scope(rmrr->devices, rmrr->devices_cnt,\n\t\t\t\t\t  i, i_dev) {\n\t\t\tstruct iommu_resv_region *resv;\n\t\t\tenum iommu_resv_type type;\n\t\t\tsize_t length;\n\n\t\t\tif (i_dev != device &&\n\t\t\t    !is_downstream_to_pci_bridge(device, i_dev))\n\t\t\t\tcontinue;\n\n\t\t\tlength = rmrr->end_address - rmrr->base_address + 1;\n\n\t\t\ttype = device_rmrr_is_relaxable(device) ?\n\t\t\t\tIOMMU_RESV_DIRECT_RELAXABLE : IOMMU_RESV_DIRECT;\n\n\t\t\tresv = iommu_alloc_resv_region(rmrr->base_address,\n\t\t\t\t\t\t       length, prot, type,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (!resv)\n\t\t\t\tbreak;\n\n\t\t\tlist_add_tail(&resv->list, head);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n#ifdef CONFIG_INTEL_IOMMU_FLOPPY_WA\n\tif (dev_is_pci(device)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(device);\n\n\t\tif ((pdev->class >> 8) == PCI_CLASS_BRIDGE_ISA) {\n\t\t\treg = iommu_alloc_resv_region(0, 1UL << 24, prot,\n\t\t\t\t\tIOMMU_RESV_DIRECT_RELAXABLE,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (reg)\n\t\t\t\tlist_add_tail(&reg->list, head);\n\t\t}\n\t}\n#endif  \n\n\treg = iommu_alloc_resv_region(IOAPIC_RANGE_START,\n\t\t\t\t      IOAPIC_RANGE_END - IOAPIC_RANGE_START + 1,\n\t\t\t\t      0, IOMMU_RESV_MSI, GFP_KERNEL);\n\tif (!reg)\n\t\treturn;\n\tlist_add_tail(&reg->list, head);\n}\n\nstatic struct iommu_group *intel_iommu_device_group(struct device *dev)\n{\n\tif (dev_is_pci(dev))\n\t\treturn pci_device_group(dev);\n\treturn generic_device_group(dev);\n}\n\nstatic int intel_iommu_enable_sva(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu;\n\n\tif (!info || dmar_disabled)\n\t\treturn -EINVAL;\n\n\tiommu = info->iommu;\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\tif (!(iommu->flags & VTD_FLAG_SVM_CAPABLE))\n\t\treturn -ENODEV;\n\n\tif (!info->pasid_enabled || !info->ats_enabled)\n\t\treturn -EINVAL;\n\n\t \n\tif (!info->pri_supported)\n\t\treturn 0;\n\n\t \n\tif (!info->pri_enabled)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int intel_iommu_enable_iopf(struct device *dev)\n{\n\tstruct pci_dev *pdev = dev_is_pci(dev) ? to_pci_dev(dev) : NULL;\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu;\n\tint ret;\n\n\tif (!pdev || !info || !info->ats_enabled || !info->pri_supported)\n\t\treturn -ENODEV;\n\n\tif (info->pri_enabled)\n\t\treturn -EBUSY;\n\n\tiommu = info->iommu;\n\tif (!iommu)\n\t\treturn -EINVAL;\n\n\t \n\tif (info->pasid_enabled && !pci_prg_resp_pasid_required(pdev))\n\t\treturn -EINVAL;\n\n\tret = pci_reset_pri(pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = iopf_queue_add_device(iommu->iopf_queue, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = iommu_register_device_fault_handler(dev, iommu_queue_iopf, dev);\n\tif (ret)\n\t\tgoto iopf_remove_device;\n\n\tret = pci_enable_pri(pdev, PRQ_DEPTH);\n\tif (ret)\n\t\tgoto iopf_unregister_handler;\n\tinfo->pri_enabled = 1;\n\n\treturn 0;\n\niopf_unregister_handler:\n\tiommu_unregister_device_fault_handler(dev);\niopf_remove_device:\n\tiopf_queue_remove_device(iommu->iopf_queue, dev);\n\n\treturn ret;\n}\n\nstatic int intel_iommu_disable_iopf(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu = info->iommu;\n\n\tif (!info->pri_enabled)\n\t\treturn -EINVAL;\n\n\t \n\tpci_disable_pri(to_pci_dev(dev));\n\tinfo->pri_enabled = 0;\n\n\t \n\tWARN_ON(iommu_unregister_device_fault_handler(dev));\n\tWARN_ON(iopf_queue_remove_device(iommu->iopf_queue, dev));\n\n\treturn 0;\n}\n\nstatic int\nintel_iommu_dev_enable_feat(struct device *dev, enum iommu_dev_features feat)\n{\n\tswitch (feat) {\n\tcase IOMMU_DEV_FEAT_IOPF:\n\t\treturn intel_iommu_enable_iopf(dev);\n\n\tcase IOMMU_DEV_FEAT_SVA:\n\t\treturn intel_iommu_enable_sva(dev);\n\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nstatic int\nintel_iommu_dev_disable_feat(struct device *dev, enum iommu_dev_features feat)\n{\n\tswitch (feat) {\n\tcase IOMMU_DEV_FEAT_IOPF:\n\t\treturn intel_iommu_disable_iopf(dev);\n\n\tcase IOMMU_DEV_FEAT_SVA:\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nstatic bool intel_iommu_is_attach_deferred(struct device *dev)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\n\treturn translation_pre_enabled(info->iommu) && !info->domain;\n}\n\n \nstatic bool risky_device(struct pci_dev *pdev)\n{\n\tif (pdev->untrusted) {\n\t\tpci_info(pdev,\n\t\t\t \"Skipping IOMMU quirk for dev [%04X:%04X] on untrusted PCI link\\n\",\n\t\t\t pdev->vendor, pdev->device);\n\t\tpci_info(pdev, \"Please check with your BIOS/Platform vendor about this\\n\");\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void intel_iommu_iotlb_sync_map(struct iommu_domain *domain,\n\t\t\t\t       unsigned long iova, size_t size)\n{\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tunsigned long pages = aligned_nrpages(iova, size);\n\tunsigned long pfn = iova >> VTD_PAGE_SHIFT;\n\tstruct iommu_domain_info *info;\n\tunsigned long i;\n\n\txa_for_each(&dmar_domain->iommu_array, i, info)\n\t\t__mapping_notify_one(info->iommu, dmar_domain, pfn, pages);\n}\n\nstatic void intel_iommu_remove_dev_pasid(struct device *dev, ioasid_t pasid)\n{\n\tstruct intel_iommu *iommu = device_to_iommu(dev, NULL, NULL);\n\tstruct dev_pasid_info *curr, *dev_pasid = NULL;\n\tstruct dmar_domain *dmar_domain;\n\tstruct iommu_domain *domain;\n\tunsigned long flags;\n\n\tdomain = iommu_get_domain_for_dev_pasid(dev, pasid, 0);\n\tif (WARN_ON_ONCE(!domain))\n\t\tgoto out_tear_down;\n\n\t \n\tif (domain->type == IOMMU_DOMAIN_SVA) {\n\t\tintel_svm_remove_dev_pasid(dev, pasid);\n\t\tgoto out_tear_down;\n\t}\n\n\tdmar_domain = to_dmar_domain(domain);\n\tspin_lock_irqsave(&dmar_domain->lock, flags);\n\tlist_for_each_entry(curr, &dmar_domain->dev_pasids, link_domain) {\n\t\tif (curr->dev == dev && curr->pasid == pasid) {\n\t\t\tlist_del(&curr->link_domain);\n\t\t\tdev_pasid = curr;\n\t\t\tbreak;\n\t\t}\n\t}\n\tWARN_ON_ONCE(!dev_pasid);\n\tspin_unlock_irqrestore(&dmar_domain->lock, flags);\n\n\tdomain_detach_iommu(dmar_domain, iommu);\n\tkfree(dev_pasid);\nout_tear_down:\n\tintel_pasid_tear_down_entry(iommu, dev, pasid, false);\n\tintel_drain_pasid_prq(dev, pasid);\n}\n\nstatic int intel_iommu_set_dev_pasid(struct iommu_domain *domain,\n\t\t\t\t     struct device *dev, ioasid_t pasid)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct dmar_domain *dmar_domain = to_dmar_domain(domain);\n\tstruct intel_iommu *iommu = info->iommu;\n\tstruct dev_pasid_info *dev_pasid;\n\tunsigned long flags;\n\tint ret;\n\n\tif (!pasid_supported(iommu) || dev_is_real_dma_subdevice(dev))\n\t\treturn -EOPNOTSUPP;\n\n\tif (context_copied(iommu, info->bus, info->devfn))\n\t\treturn -EBUSY;\n\n\tret = prepare_domain_attach_device(domain, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tdev_pasid = kzalloc(sizeof(*dev_pasid), GFP_KERNEL);\n\tif (!dev_pasid)\n\t\treturn -ENOMEM;\n\n\tret = domain_attach_iommu(dmar_domain, iommu);\n\tif (ret)\n\t\tgoto out_free;\n\n\tif (domain_type_is_si(dmar_domain))\n\t\tret = intel_pasid_setup_pass_through(iommu, dmar_domain,\n\t\t\t\t\t\t     dev, pasid);\n\telse if (dmar_domain->use_first_level)\n\t\tret = domain_setup_first_level(iommu, dmar_domain,\n\t\t\t\t\t       dev, pasid);\n\telse\n\t\tret = intel_pasid_setup_second_level(iommu, dmar_domain,\n\t\t\t\t\t\t     dev, pasid);\n\tif (ret)\n\t\tgoto out_detach_iommu;\n\n\tdev_pasid->dev = dev;\n\tdev_pasid->pasid = pasid;\n\tspin_lock_irqsave(&dmar_domain->lock, flags);\n\tlist_add(&dev_pasid->link_domain, &dmar_domain->dev_pasids);\n\tspin_unlock_irqrestore(&dmar_domain->lock, flags);\n\n\treturn 0;\nout_detach_iommu:\n\tdomain_detach_iommu(dmar_domain, iommu);\nout_free:\n\tkfree(dev_pasid);\n\treturn ret;\n}\n\nstatic void *intel_iommu_hw_info(struct device *dev, u32 *length, u32 *type)\n{\n\tstruct device_domain_info *info = dev_iommu_priv_get(dev);\n\tstruct intel_iommu *iommu = info->iommu;\n\tstruct iommu_hw_info_vtd *vtd;\n\n\tvtd = kzalloc(sizeof(*vtd), GFP_KERNEL);\n\tif (!vtd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvtd->cap_reg = iommu->cap;\n\tvtd->ecap_reg = iommu->ecap;\n\t*length = sizeof(*vtd);\n\t*type = IOMMU_HW_INFO_TYPE_INTEL_VTD;\n\treturn vtd;\n}\n\nconst struct iommu_ops intel_iommu_ops = {\n\t.capable\t\t= intel_iommu_capable,\n\t.hw_info\t\t= intel_iommu_hw_info,\n\t.domain_alloc\t\t= intel_iommu_domain_alloc,\n\t.probe_device\t\t= intel_iommu_probe_device,\n\t.probe_finalize\t\t= intel_iommu_probe_finalize,\n\t.release_device\t\t= intel_iommu_release_device,\n\t.get_resv_regions\t= intel_iommu_get_resv_regions,\n\t.device_group\t\t= intel_iommu_device_group,\n\t.dev_enable_feat\t= intel_iommu_dev_enable_feat,\n\t.dev_disable_feat\t= intel_iommu_dev_disable_feat,\n\t.is_attach_deferred\t= intel_iommu_is_attach_deferred,\n\t.def_domain_type\t= device_def_domain_type,\n\t.remove_dev_pasid\t= intel_iommu_remove_dev_pasid,\n\t.pgsize_bitmap\t\t= SZ_4K,\n#ifdef CONFIG_INTEL_IOMMU_SVM\n\t.page_response\t\t= intel_svm_page_response,\n#endif\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t\t= intel_iommu_attach_device,\n\t\t.set_dev_pasid\t\t= intel_iommu_set_dev_pasid,\n\t\t.map_pages\t\t= intel_iommu_map_pages,\n\t\t.unmap_pages\t\t= intel_iommu_unmap_pages,\n\t\t.iotlb_sync_map\t\t= intel_iommu_iotlb_sync_map,\n\t\t.flush_iotlb_all        = intel_flush_iotlb_all,\n\t\t.iotlb_sync\t\t= intel_iommu_tlb_sync,\n\t\t.iova_to_phys\t\t= intel_iommu_iova_to_phys,\n\t\t.free\t\t\t= intel_iommu_domain_free,\n\t\t.enforce_cache_coherency = intel_iommu_enforce_cache_coherency,\n\t}\n};\n\nstatic void quirk_iommu_igfx(struct pci_dev *dev)\n{\n\tif (risky_device(dev))\n\t\treturn;\n\n\tpci_info(dev, \"Disabling IOMMU for graphics on this chipset\\n\");\n\tdmar_map_gfx = 0;\n}\n\n \nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2a40, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e00, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e10, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e20, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e30, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e40, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e90, quirk_iommu_igfx);\n\n \nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1606, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x160B, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x160E, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1602, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x160A, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x160D, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1616, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x161B, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x161E, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1612, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x161A, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x161D, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1626, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x162B, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x162E, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1622, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x162A, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x162D, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1636, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163B, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163E, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1632, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163A, quirk_iommu_igfx);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163D, quirk_iommu_igfx);\n\nstatic void quirk_iommu_rwbf(struct pci_dev *dev)\n{\n\tif (risky_device(dev))\n\t\treturn;\n\n\t \n\tpci_info(dev, \"Forcing write-buffer flush capability\\n\");\n\trwbf_quirk = 1;\n}\n\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2a40, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e00, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e10, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e20, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e30, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e40, quirk_iommu_rwbf);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e90, quirk_iommu_rwbf);\n\n#define GGC 0x52\n#define GGC_MEMORY_SIZE_MASK\t(0xf << 8)\n#define GGC_MEMORY_SIZE_NONE\t(0x0 << 8)\n#define GGC_MEMORY_SIZE_1M\t(0x1 << 8)\n#define GGC_MEMORY_SIZE_2M\t(0x3 << 8)\n#define GGC_MEMORY_VT_ENABLED\t(0x8 << 8)\n#define GGC_MEMORY_SIZE_2M_VT\t(0x9 << 8)\n#define GGC_MEMORY_SIZE_3M_VT\t(0xa << 8)\n#define GGC_MEMORY_SIZE_4M_VT\t(0xb << 8)\n\nstatic void quirk_calpella_no_shadow_gtt(struct pci_dev *dev)\n{\n\tunsigned short ggc;\n\n\tif (risky_device(dev))\n\t\treturn;\n\n\tif (pci_read_config_word(dev, GGC, &ggc))\n\t\treturn;\n\n\tif (!(ggc & GGC_MEMORY_VT_ENABLED)) {\n\t\tpci_info(dev, \"BIOS has allocated no shadow GTT; disabling IOMMU for graphics\\n\");\n\t\tdmar_map_gfx = 0;\n\t} else if (dmar_map_gfx) {\n\t\t \n\t\tpci_info(dev, \"Disabling batched IOTLB flush on Ironlake\\n\");\n\t\tiommu_set_dma_strict();\n\t}\n}\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x0040, quirk_calpella_no_shadow_gtt);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x0044, quirk_calpella_no_shadow_gtt);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x0062, quirk_calpella_no_shadow_gtt);\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x006a, quirk_calpella_no_shadow_gtt);\n\nstatic void quirk_igfx_skip_te_disable(struct pci_dev *dev)\n{\n\tunsigned short ver;\n\n\tif (!IS_GFX_DEVICE(dev))\n\t\treturn;\n\n\tver = (dev->device >> 8) & 0xff;\n\tif (ver != 0x45 && ver != 0x46 && ver != 0x4c &&\n\t    ver != 0x4e && ver != 0x8a && ver != 0x98 &&\n\t    ver != 0x9a && ver != 0xa7 && ver != 0x7d)\n\t\treturn;\n\n\tif (risky_device(dev))\n\t\treturn;\n\n\tpci_info(dev, \"Skip IOMMU disabling for graphics\\n\");\n\tiommu_skip_te_disable = 1;\n}\nDECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, PCI_ANY_ID, quirk_igfx_skip_te_disable);\n\n \nstatic void __init check_tylersburg_isoch(void)\n{\n\tstruct pci_dev *pdev;\n\tuint32_t vtisochctrl;\n\n\t \n\tpdev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x3a3e, NULL);\n\tif (!pdev)\n\t\treturn;\n\n\tif (risky_device(pdev)) {\n\t\tpci_dev_put(pdev);\n\t\treturn;\n\t}\n\n\tpci_dev_put(pdev);\n\n\t \n\tpdev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x342e, NULL);\n\tif (!pdev)\n\t\treturn;\n\n\tif (risky_device(pdev)) {\n\t\tpci_dev_put(pdev);\n\t\treturn;\n\t}\n\n\tif (pci_read_config_dword(pdev, 0x188, &vtisochctrl)) {\n\t\tpci_dev_put(pdev);\n\t\treturn;\n\t}\n\n\tpci_dev_put(pdev);\n\n\t \n\tif (vtisochctrl & 1)\n\t\treturn;\n\n\t \n\tvtisochctrl &= 0x1c;\n\n\t \n\tif (vtisochctrl == 0x10)\n\t\treturn;\n\n\t \n\tif (!vtisochctrl) {\n\t\tWARN(1, \"Your BIOS is broken; DMA routed to ISOCH DMAR unit but no TLB space.\\n\"\n\t\t     \"BIOS vendor: %s; Ver: %s; Product Version: %s\\n\",\n\t\t     dmi_get_system_info(DMI_BIOS_VENDOR),\n\t\t     dmi_get_system_info(DMI_BIOS_VERSION),\n\t\t     dmi_get_system_info(DMI_PRODUCT_VERSION));\n\t\tiommu_identity_mapping |= IDENTMAP_AZALIA;\n\t\treturn;\n\t}\n\n\tpr_warn(\"Recommended TLB entries for ISOCH unit is 16; your BIOS set %d\\n\",\n\t       vtisochctrl);\n}\n\n \nvoid quirk_extra_dev_tlb_flush(struct device_domain_info *info,\n\t\t\t       unsigned long address, unsigned long mask,\n\t\t\t       u32 pasid, u16 qdep)\n{\n\tu16 sid;\n\n\tif (likely(!info->dtlb_extra_inval))\n\t\treturn;\n\n\tsid = PCI_DEVID(info->bus, info->devfn);\n\tif (pasid == IOMMU_NO_PASID) {\n\t\tqi_flush_dev_iotlb(info->iommu, sid, info->pfsid,\n\t\t\t\t   qdep, address, mask);\n\t} else {\n\t\tqi_flush_dev_iotlb_pasid(info->iommu, sid, info->pfsid,\n\t\t\t\t\t pasid, qdep, address, mask);\n\t}\n}\n\n#define ecmd_get_status_code(res)\t(((res) & 0xff) >> 1)\n\n \nint ecmd_submit_sync(struct intel_iommu *iommu, u8 ecmd, u64 oa, u64 ob)\n{\n\tunsigned long flags;\n\tu64 res;\n\tint ret;\n\n\tif (!cap_ecmds(iommu->cap))\n\t\treturn -ENODEV;\n\n\traw_spin_lock_irqsave(&iommu->register_lock, flags);\n\n\tres = dmar_readq(iommu->reg + DMAR_ECRSP_REG);\n\tif (res & DMA_ECMD_ECRSP_IP) {\n\t\tret = -EBUSY;\n\t\tgoto err;\n\t}\n\n\t \n\tdmar_writeq(iommu->reg + DMAR_ECEO_REG, ob);\n\tdmar_writeq(iommu->reg + DMAR_ECMD_REG, ecmd | (oa << DMA_ECMD_OA_SHIFT));\n\n\tIOMMU_WAIT_OP(iommu, DMAR_ECRSP_REG, dmar_readq,\n\t\t      !(res & DMA_ECMD_ECRSP_IP), res);\n\n\tif (res & DMA_ECMD_ECRSP_IP) {\n\t\tret = -ETIMEDOUT;\n\t\tgoto err;\n\t}\n\n\tret = ecmd_get_status_code(res);\nerr:\n\traw_spin_unlock_irqrestore(&iommu->register_lock, flags);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}