{
  "module_name": "io-pgfault.c",
  "hash_id": "f7a690e0325da4eafb16b626c737227e78f31c80c5b144d47ebd00310630cce7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/io-pgfault.c",
  "human_readable_source": "\n \n\n#include <linux/iommu.h>\n#include <linux/list.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n\n#include \"iommu-sva.h\"\n\n \nstruct iopf_queue {\n\tstruct workqueue_struct\t\t*wq;\n\tstruct list_head\t\tdevices;\n\tstruct mutex\t\t\tlock;\n};\n\n \nstruct iopf_device_param {\n\tstruct device\t\t\t*dev;\n\tstruct iopf_queue\t\t*queue;\n\tstruct list_head\t\tqueue_list;\n\tstruct list_head\t\tpartial;\n};\n\nstruct iopf_fault {\n\tstruct iommu_fault\t\tfault;\n\tstruct list_head\t\tlist;\n};\n\nstruct iopf_group {\n\tstruct iopf_fault\t\tlast_fault;\n\tstruct list_head\t\tfaults;\n\tstruct work_struct\t\twork;\n\tstruct device\t\t\t*dev;\n};\n\nstatic int iopf_complete_group(struct device *dev, struct iopf_fault *iopf,\n\t\t\t       enum iommu_page_response_code status)\n{\n\tstruct iommu_page_response resp = {\n\t\t.version\t\t= IOMMU_PAGE_RESP_VERSION_1,\n\t\t.pasid\t\t\t= iopf->fault.prm.pasid,\n\t\t.grpid\t\t\t= iopf->fault.prm.grpid,\n\t\t.code\t\t\t= status,\n\t};\n\n\tif ((iopf->fault.prm.flags & IOMMU_FAULT_PAGE_REQUEST_PASID_VALID) &&\n\t    (iopf->fault.prm.flags & IOMMU_FAULT_PAGE_RESPONSE_NEEDS_PASID))\n\t\tresp.flags = IOMMU_PAGE_RESP_PASID_VALID;\n\n\treturn iommu_page_response(dev, &resp);\n}\n\nstatic void iopf_handler(struct work_struct *work)\n{\n\tstruct iopf_group *group;\n\tstruct iommu_domain *domain;\n\tstruct iopf_fault *iopf, *next;\n\tenum iommu_page_response_code status = IOMMU_PAGE_RESP_SUCCESS;\n\n\tgroup = container_of(work, struct iopf_group, work);\n\tdomain = iommu_get_domain_for_dev_pasid(group->dev,\n\t\t\t\tgroup->last_fault.fault.prm.pasid, 0);\n\tif (!domain || !domain->iopf_handler)\n\t\tstatus = IOMMU_PAGE_RESP_INVALID;\n\n\tlist_for_each_entry_safe(iopf, next, &group->faults, list) {\n\t\t \n\t\tif (status == IOMMU_PAGE_RESP_SUCCESS)\n\t\t\tstatus = domain->iopf_handler(&iopf->fault,\n\t\t\t\t\t\t      domain->fault_data);\n\n\t\tif (!(iopf->fault.prm.flags &\n\t\t      IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE))\n\t\t\tkfree(iopf);\n\t}\n\n\tiopf_complete_group(group->dev, &group->last_fault, status);\n\tkfree(group);\n}\n\n \nint iommu_queue_iopf(struct iommu_fault *fault, void *cookie)\n{\n\tint ret;\n\tstruct iopf_group *group;\n\tstruct iopf_fault *iopf, *next;\n\tstruct iopf_device_param *iopf_param;\n\n\tstruct device *dev = cookie;\n\tstruct dev_iommu *param = dev->iommu;\n\n\tlockdep_assert_held(&param->lock);\n\n\tif (fault->type != IOMMU_FAULT_PAGE_REQ)\n\t\t \n\t\treturn -EOPNOTSUPP;\n\n\t \n\tiopf_param = param->iopf_param;\n\tif (!iopf_param)\n\t\treturn -ENODEV;\n\n\tif (!(fault->prm.flags & IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE)) {\n\t\tiopf = kzalloc(sizeof(*iopf), GFP_KERNEL);\n\t\tif (!iopf)\n\t\t\treturn -ENOMEM;\n\n\t\tiopf->fault = *fault;\n\n\t\t \n\t\tlist_add(&iopf->list, &iopf_param->partial);\n\n\t\treturn 0;\n\t}\n\n\tgroup = kzalloc(sizeof(*group), GFP_KERNEL);\n\tif (!group) {\n\t\t \n\t\tret = -ENOMEM;\n\t\tgoto cleanup_partial;\n\t}\n\n\tgroup->dev = dev;\n\tgroup->last_fault.fault = *fault;\n\tINIT_LIST_HEAD(&group->faults);\n\tlist_add(&group->last_fault.list, &group->faults);\n\tINIT_WORK(&group->work, iopf_handler);\n\n\t \n\tlist_for_each_entry_safe(iopf, next, &iopf_param->partial, list) {\n\t\tif (iopf->fault.prm.grpid == fault->prm.grpid)\n\t\t\t \n\t\t\tlist_move(&iopf->list, &group->faults);\n\t}\n\n\tqueue_work(iopf_param->queue->wq, &group->work);\n\treturn 0;\n\ncleanup_partial:\n\tlist_for_each_entry_safe(iopf, next, &iopf_param->partial, list) {\n\t\tif (iopf->fault.prm.grpid == fault->prm.grpid) {\n\t\t\tlist_del(&iopf->list);\n\t\t\tkfree(iopf);\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_queue_iopf);\n\n \nint iopf_queue_flush_dev(struct device *dev)\n{\n\tint ret = 0;\n\tstruct iopf_device_param *iopf_param;\n\tstruct dev_iommu *param = dev->iommu;\n\n\tif (!param)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&param->lock);\n\tiopf_param = param->iopf_param;\n\tif (iopf_param)\n\t\tflush_workqueue(iopf_param->queue->wq);\n\telse\n\t\tret = -ENODEV;\n\tmutex_unlock(&param->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iopf_queue_flush_dev);\n\n \nint iopf_queue_discard_partial(struct iopf_queue *queue)\n{\n\tstruct iopf_fault *iopf, *next;\n\tstruct iopf_device_param *iopf_param;\n\n\tif (!queue)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&queue->lock);\n\tlist_for_each_entry(iopf_param, &queue->devices, queue_list) {\n\t\tlist_for_each_entry_safe(iopf, next, &iopf_param->partial,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&iopf->list);\n\t\t\tkfree(iopf);\n\t\t}\n\t}\n\tmutex_unlock(&queue->lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iopf_queue_discard_partial);\n\n \nint iopf_queue_add_device(struct iopf_queue *queue, struct device *dev)\n{\n\tint ret = -EBUSY;\n\tstruct iopf_device_param *iopf_param;\n\tstruct dev_iommu *param = dev->iommu;\n\n\tif (!param)\n\t\treturn -ENODEV;\n\n\tiopf_param = kzalloc(sizeof(*iopf_param), GFP_KERNEL);\n\tif (!iopf_param)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&iopf_param->partial);\n\tiopf_param->queue = queue;\n\tiopf_param->dev = dev;\n\n\tmutex_lock(&queue->lock);\n\tmutex_lock(&param->lock);\n\tif (!param->iopf_param) {\n\t\tlist_add(&iopf_param->queue_list, &queue->devices);\n\t\tparam->iopf_param = iopf_param;\n\t\tret = 0;\n\t}\n\tmutex_unlock(&param->lock);\n\tmutex_unlock(&queue->lock);\n\n\tif (ret)\n\t\tkfree(iopf_param);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iopf_queue_add_device);\n\n \nint iopf_queue_remove_device(struct iopf_queue *queue, struct device *dev)\n{\n\tint ret = -EINVAL;\n\tstruct iopf_fault *iopf, *next;\n\tstruct iopf_device_param *iopf_param;\n\tstruct dev_iommu *param = dev->iommu;\n\n\tif (!param || !queue)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&queue->lock);\n\tmutex_lock(&param->lock);\n\tiopf_param = param->iopf_param;\n\tif (iopf_param && iopf_param->queue == queue) {\n\t\tlist_del(&iopf_param->queue_list);\n\t\tparam->iopf_param = NULL;\n\t\tret = 0;\n\t}\n\tmutex_unlock(&param->lock);\n\tmutex_unlock(&queue->lock);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tlist_for_each_entry_safe(iopf, next, &iopf_param->partial, list)\n\t\tkfree(iopf);\n\n\tkfree(iopf_param);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iopf_queue_remove_device);\n\n \nstruct iopf_queue *iopf_queue_alloc(const char *name)\n{\n\tstruct iopf_queue *queue;\n\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\n\t \n\tqueue->wq = alloc_workqueue(\"iopf_queue/%s\", WQ_UNBOUND, 0, name);\n\tif (!queue->wq) {\n\t\tkfree(queue);\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&queue->devices);\n\tmutex_init(&queue->lock);\n\n\treturn queue;\n}\nEXPORT_SYMBOL_GPL(iopf_queue_alloc);\n\n \nvoid iopf_queue_free(struct iopf_queue *queue)\n{\n\tstruct iopf_device_param *iopf_param, *next;\n\n\tif (!queue)\n\t\treturn;\n\n\tlist_for_each_entry_safe(iopf_param, next, &queue->devices, queue_list)\n\t\tiopf_queue_remove_device(queue, iopf_param->dev);\n\n\tdestroy_workqueue(queue->wq);\n\tkfree(queue);\n}\nEXPORT_SYMBOL_GPL(iopf_queue_free);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}