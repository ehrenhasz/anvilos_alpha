{
  "module_name": "iova.c",
  "hash_id": "b1365d01b21b4d84d3605c9966bfc95b0587ceac80882d99e29368ba84029fd5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/iova.c",
  "human_readable_source": "\n \n\n#include <linux/iova.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/smp.h>\n#include <linux/bitops.h>\n#include <linux/cpu.h>\n\n \n#define IOVA_ANCHOR\t~0UL\n\n#define IOVA_RANGE_CACHE_MAX_SIZE 6\t \n\nstatic bool iova_rcache_insert(struct iova_domain *iovad,\n\t\t\t       unsigned long pfn,\n\t\t\t       unsigned long size);\nstatic unsigned long iova_rcache_get(struct iova_domain *iovad,\n\t\t\t\t     unsigned long size,\n\t\t\t\t     unsigned long limit_pfn);\nstatic void free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad);\nstatic void free_iova_rcaches(struct iova_domain *iovad);\n\nunsigned long iova_rcache_range(void)\n{\n\treturn PAGE_SIZE << (IOVA_RANGE_CACHE_MAX_SIZE - 1);\n}\n\nstatic int iova_cpuhp_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct iova_domain *iovad;\n\n\tiovad = hlist_entry_safe(node, struct iova_domain, cpuhp_dead);\n\n\tfree_cpu_cached_iovas(cpu, iovad);\n\treturn 0;\n}\n\nstatic void free_global_cached_iovas(struct iova_domain *iovad);\n\nstatic struct iova *to_iova(struct rb_node *node)\n{\n\treturn rb_entry(node, struct iova, node);\n}\n\nvoid\ninit_iova_domain(struct iova_domain *iovad, unsigned long granule,\n\tunsigned long start_pfn)\n{\n\t \n\tBUG_ON((granule > PAGE_SIZE) || !is_power_of_2(granule));\n\n\tspin_lock_init(&iovad->iova_rbtree_lock);\n\tiovad->rbroot = RB_ROOT;\n\tiovad->cached_node = &iovad->anchor.node;\n\tiovad->cached32_node = &iovad->anchor.node;\n\tiovad->granule = granule;\n\tiovad->start_pfn = start_pfn;\n\tiovad->dma_32bit_pfn = 1UL << (32 - iova_shift(iovad));\n\tiovad->max32_alloc_size = iovad->dma_32bit_pfn;\n\tiovad->anchor.pfn_lo = iovad->anchor.pfn_hi = IOVA_ANCHOR;\n\trb_link_node(&iovad->anchor.node, NULL, &iovad->rbroot.rb_node);\n\trb_insert_color(&iovad->anchor.node, &iovad->rbroot);\n}\nEXPORT_SYMBOL_GPL(init_iova_domain);\n\nstatic struct rb_node *\n__get_cached_rbnode(struct iova_domain *iovad, unsigned long limit_pfn)\n{\n\tif (limit_pfn <= iovad->dma_32bit_pfn)\n\t\treturn iovad->cached32_node;\n\n\treturn iovad->cached_node;\n}\n\nstatic void\n__cached_rbnode_insert_update(struct iova_domain *iovad, struct iova *new)\n{\n\tif (new->pfn_hi < iovad->dma_32bit_pfn)\n\t\tiovad->cached32_node = &new->node;\n\telse\n\t\tiovad->cached_node = &new->node;\n}\n\nstatic void\n__cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)\n{\n\tstruct iova *cached_iova;\n\n\tcached_iova = to_iova(iovad->cached32_node);\n\tif (free == cached_iova ||\n\t    (free->pfn_hi < iovad->dma_32bit_pfn &&\n\t     free->pfn_lo >= cached_iova->pfn_lo))\n\t\tiovad->cached32_node = rb_next(&free->node);\n\n\tif (free->pfn_lo < iovad->dma_32bit_pfn)\n\t\tiovad->max32_alloc_size = iovad->dma_32bit_pfn;\n\n\tcached_iova = to_iova(iovad->cached_node);\n\tif (free->pfn_lo >= cached_iova->pfn_lo)\n\t\tiovad->cached_node = rb_next(&free->node);\n}\n\nstatic struct rb_node *iova_find_limit(struct iova_domain *iovad, unsigned long limit_pfn)\n{\n\tstruct rb_node *node, *next;\n\t \n\tif (limit_pfn > iovad->dma_32bit_pfn)\n\t\treturn &iovad->anchor.node;\n\n\tnode = iovad->rbroot.rb_node;\n\twhile (to_iova(node)->pfn_hi < limit_pfn)\n\t\tnode = node->rb_right;\n\nsearch_left:\n\twhile (node->rb_left && to_iova(node->rb_left)->pfn_lo >= limit_pfn)\n\t\tnode = node->rb_left;\n\n\tif (!node->rb_left)\n\t\treturn node;\n\n\tnext = node->rb_left;\n\twhile (next->rb_right) {\n\t\tnext = next->rb_right;\n\t\tif (to_iova(next)->pfn_lo >= limit_pfn) {\n\t\t\tnode = next;\n\t\t\tgoto search_left;\n\t\t}\n\t}\n\n\treturn node;\n}\n\n \nstatic void\niova_insert_rbtree(struct rb_root *root, struct iova *iova,\n\t\t   struct rb_node *start)\n{\n\tstruct rb_node **new, *parent = NULL;\n\n\tnew = (start) ? &start : &(root->rb_node);\n\t \n\twhile (*new) {\n\t\tstruct iova *this = to_iova(*new);\n\n\t\tparent = *new;\n\n\t\tif (iova->pfn_lo < this->pfn_lo)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse if (iova->pfn_lo > this->pfn_lo)\n\t\t\tnew = &((*new)->rb_right);\n\t\telse {\n\t\t\tWARN_ON(1);  \n\t\t\treturn;\n\t\t}\n\t}\n\t \n\trb_link_node(&iova->node, parent, new);\n\trb_insert_color(&iova->node, root);\n}\n\nstatic int __alloc_and_insert_iova_range(struct iova_domain *iovad,\n\t\tunsigned long size, unsigned long limit_pfn,\n\t\t\tstruct iova *new, bool size_aligned)\n{\n\tstruct rb_node *curr, *prev;\n\tstruct iova *curr_iova;\n\tunsigned long flags;\n\tunsigned long new_pfn, retry_pfn;\n\tunsigned long align_mask = ~0UL;\n\tunsigned long high_pfn = limit_pfn, low_pfn = iovad->start_pfn;\n\n\tif (size_aligned)\n\t\talign_mask <<= fls_long(size - 1);\n\n\t \n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\tif (limit_pfn <= iovad->dma_32bit_pfn &&\n\t\t\tsize >= iovad->max32_alloc_size)\n\t\tgoto iova32_full;\n\n\tcurr = __get_cached_rbnode(iovad, limit_pfn);\n\tcurr_iova = to_iova(curr);\n\tretry_pfn = curr_iova->pfn_hi;\n\nretry:\n\tdo {\n\t\thigh_pfn = min(high_pfn, curr_iova->pfn_lo);\n\t\tnew_pfn = (high_pfn - size) & align_mask;\n\t\tprev = curr;\n\t\tcurr = rb_prev(curr);\n\t\tcurr_iova = to_iova(curr);\n\t} while (curr && new_pfn <= curr_iova->pfn_hi && new_pfn >= low_pfn);\n\n\tif (high_pfn < size || new_pfn < low_pfn) {\n\t\tif (low_pfn == iovad->start_pfn && retry_pfn < limit_pfn) {\n\t\t\thigh_pfn = limit_pfn;\n\t\t\tlow_pfn = retry_pfn + 1;\n\t\t\tcurr = iova_find_limit(iovad, limit_pfn);\n\t\t\tcurr_iova = to_iova(curr);\n\t\t\tgoto retry;\n\t\t}\n\t\tiovad->max32_alloc_size = size;\n\t\tgoto iova32_full;\n\t}\n\n\t \n\tnew->pfn_lo = new_pfn;\n\tnew->pfn_hi = new->pfn_lo + size - 1;\n\n\t \n\tiova_insert_rbtree(&iovad->rbroot, new, prev);\n\t__cached_rbnode_insert_update(iovad, new);\n\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\treturn 0;\n\niova32_full:\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\treturn -ENOMEM;\n}\n\nstatic struct kmem_cache *iova_cache;\nstatic unsigned int iova_cache_users;\nstatic DEFINE_MUTEX(iova_cache_mutex);\n\nstatic struct iova *alloc_iova_mem(void)\n{\n\treturn kmem_cache_zalloc(iova_cache, GFP_ATOMIC | __GFP_NOWARN);\n}\n\nstatic void free_iova_mem(struct iova *iova)\n{\n\tif (iova->pfn_lo != IOVA_ANCHOR)\n\t\tkmem_cache_free(iova_cache, iova);\n}\n\nint iova_cache_get(void)\n{\n\tmutex_lock(&iova_cache_mutex);\n\tif (!iova_cache_users) {\n\t\tint ret;\n\n\t\tret = cpuhp_setup_state_multi(CPUHP_IOMMU_IOVA_DEAD, \"iommu/iova:dead\", NULL,\n\t\t\t\t\tiova_cpuhp_dead);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&iova_cache_mutex);\n\t\t\tpr_err(\"Couldn't register cpuhp handler\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tiova_cache = kmem_cache_create(\n\t\t\t\"iommu_iova\", sizeof(struct iova), 0,\n\t\t\tSLAB_HWCACHE_ALIGN, NULL);\n\t\tif (!iova_cache) {\n\t\t\tcpuhp_remove_multi_state(CPUHP_IOMMU_IOVA_DEAD);\n\t\t\tmutex_unlock(&iova_cache_mutex);\n\t\t\tpr_err(\"Couldn't create iova cache\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tiova_cache_users++;\n\tmutex_unlock(&iova_cache_mutex);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iova_cache_get);\n\nvoid iova_cache_put(void)\n{\n\tmutex_lock(&iova_cache_mutex);\n\tif (WARN_ON(!iova_cache_users)) {\n\t\tmutex_unlock(&iova_cache_mutex);\n\t\treturn;\n\t}\n\tiova_cache_users--;\n\tif (!iova_cache_users) {\n\t\tcpuhp_remove_multi_state(CPUHP_IOMMU_IOVA_DEAD);\n\t\tkmem_cache_destroy(iova_cache);\n\t}\n\tmutex_unlock(&iova_cache_mutex);\n}\nEXPORT_SYMBOL_GPL(iova_cache_put);\n\n \nstruct iova *\nalloc_iova(struct iova_domain *iovad, unsigned long size,\n\tunsigned long limit_pfn,\n\tbool size_aligned)\n{\n\tstruct iova *new_iova;\n\tint ret;\n\n\tnew_iova = alloc_iova_mem();\n\tif (!new_iova)\n\t\treturn NULL;\n\n\tret = __alloc_and_insert_iova_range(iovad, size, limit_pfn + 1,\n\t\t\tnew_iova, size_aligned);\n\n\tif (ret) {\n\t\tfree_iova_mem(new_iova);\n\t\treturn NULL;\n\t}\n\n\treturn new_iova;\n}\nEXPORT_SYMBOL_GPL(alloc_iova);\n\nstatic struct iova *\nprivate_find_iova(struct iova_domain *iovad, unsigned long pfn)\n{\n\tstruct rb_node *node = iovad->rbroot.rb_node;\n\n\tassert_spin_locked(&iovad->iova_rbtree_lock);\n\n\twhile (node) {\n\t\tstruct iova *iova = to_iova(node);\n\n\t\tif (pfn < iova->pfn_lo)\n\t\t\tnode = node->rb_left;\n\t\telse if (pfn > iova->pfn_hi)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn iova;\t \n\t}\n\n\treturn NULL;\n}\n\nstatic void remove_iova(struct iova_domain *iovad, struct iova *iova)\n{\n\tassert_spin_locked(&iovad->iova_rbtree_lock);\n\t__cached_rbnode_delete_update(iovad, iova);\n\trb_erase(&iova->node, &iovad->rbroot);\n}\n\n \nstruct iova *find_iova(struct iova_domain *iovad, unsigned long pfn)\n{\n\tunsigned long flags;\n\tstruct iova *iova;\n\n\t \n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\tiova = private_find_iova(iovad, pfn);\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\treturn iova;\n}\nEXPORT_SYMBOL_GPL(find_iova);\n\n \nvoid\n__free_iova(struct iova_domain *iovad, struct iova *iova)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\tremove_iova(iovad, iova);\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\tfree_iova_mem(iova);\n}\nEXPORT_SYMBOL_GPL(__free_iova);\n\n \nvoid\nfree_iova(struct iova_domain *iovad, unsigned long pfn)\n{\n\tunsigned long flags;\n\tstruct iova *iova;\n\n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\tiova = private_find_iova(iovad, pfn);\n\tif (!iova) {\n\t\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\t\treturn;\n\t}\n\tremove_iova(iovad, iova);\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\tfree_iova_mem(iova);\n}\nEXPORT_SYMBOL_GPL(free_iova);\n\n \nunsigned long\nalloc_iova_fast(struct iova_domain *iovad, unsigned long size,\n\t\tunsigned long limit_pfn, bool flush_rcache)\n{\n\tunsigned long iova_pfn;\n\tstruct iova *new_iova;\n\n\t \n\tif (size < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))\n\t\tsize = roundup_pow_of_two(size);\n\n\tiova_pfn = iova_rcache_get(iovad, size, limit_pfn + 1);\n\tif (iova_pfn)\n\t\treturn iova_pfn;\n\nretry:\n\tnew_iova = alloc_iova(iovad, size, limit_pfn, true);\n\tif (!new_iova) {\n\t\tunsigned int cpu;\n\n\t\tif (!flush_rcache)\n\t\t\treturn 0;\n\n\t\t \n\t\tflush_rcache = false;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_cpu_cached_iovas(cpu, iovad);\n\t\tfree_global_cached_iovas(iovad);\n\t\tgoto retry;\n\t}\n\n\treturn new_iova->pfn_lo;\n}\nEXPORT_SYMBOL_GPL(alloc_iova_fast);\n\n \nvoid\nfree_iova_fast(struct iova_domain *iovad, unsigned long pfn, unsigned long size)\n{\n\tif (iova_rcache_insert(iovad, pfn, size))\n\t\treturn;\n\n\tfree_iova(iovad, pfn);\n}\nEXPORT_SYMBOL_GPL(free_iova_fast);\n\nstatic void iova_domain_free_rcaches(struct iova_domain *iovad)\n{\n\tcpuhp_state_remove_instance_nocalls(CPUHP_IOMMU_IOVA_DEAD,\n\t\t\t\t\t    &iovad->cpuhp_dead);\n\tfree_iova_rcaches(iovad);\n}\n\n \nvoid put_iova_domain(struct iova_domain *iovad)\n{\n\tstruct iova *iova, *tmp;\n\n\tif (iovad->rcaches)\n\t\tiova_domain_free_rcaches(iovad);\n\n\trbtree_postorder_for_each_entry_safe(iova, tmp, &iovad->rbroot, node)\n\t\tfree_iova_mem(iova);\n}\nEXPORT_SYMBOL_GPL(put_iova_domain);\n\nstatic int\n__is_range_overlap(struct rb_node *node,\n\tunsigned long pfn_lo, unsigned long pfn_hi)\n{\n\tstruct iova *iova = to_iova(node);\n\n\tif ((pfn_lo <= iova->pfn_hi) && (pfn_hi >= iova->pfn_lo))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline struct iova *\nalloc_and_init_iova(unsigned long pfn_lo, unsigned long pfn_hi)\n{\n\tstruct iova *iova;\n\n\tiova = alloc_iova_mem();\n\tif (iova) {\n\t\tiova->pfn_lo = pfn_lo;\n\t\tiova->pfn_hi = pfn_hi;\n\t}\n\n\treturn iova;\n}\n\nstatic struct iova *\n__insert_new_range(struct iova_domain *iovad,\n\tunsigned long pfn_lo, unsigned long pfn_hi)\n{\n\tstruct iova *iova;\n\n\tiova = alloc_and_init_iova(pfn_lo, pfn_hi);\n\tif (iova)\n\t\tiova_insert_rbtree(&iovad->rbroot, iova, NULL);\n\n\treturn iova;\n}\n\nstatic void\n__adjust_overlap_range(struct iova *iova,\n\tunsigned long *pfn_lo, unsigned long *pfn_hi)\n{\n\tif (*pfn_lo < iova->pfn_lo)\n\t\tiova->pfn_lo = *pfn_lo;\n\tif (*pfn_hi > iova->pfn_hi)\n\t\t*pfn_lo = iova->pfn_hi + 1;\n}\n\n \nstruct iova *\nreserve_iova(struct iova_domain *iovad,\n\tunsigned long pfn_lo, unsigned long pfn_hi)\n{\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct iova *iova;\n\tunsigned int overlap = 0;\n\n\t \n\tif (WARN_ON((pfn_hi | pfn_lo) > (ULLONG_MAX >> iova_shift(iovad))))\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\tfor (node = rb_first(&iovad->rbroot); node; node = rb_next(node)) {\n\t\tif (__is_range_overlap(node, pfn_lo, pfn_hi)) {\n\t\t\tiova = to_iova(node);\n\t\t\t__adjust_overlap_range(iova, &pfn_lo, &pfn_hi);\n\t\t\tif ((pfn_lo >= iova->pfn_lo) &&\n\t\t\t\t(pfn_hi <= iova->pfn_hi))\n\t\t\t\tgoto finish;\n\t\t\toverlap = 1;\n\n\t\t} else if (overlap)\n\t\t\t\tbreak;\n\t}\n\n\t \n\tiova = __insert_new_range(iovad, pfn_lo, pfn_hi);\nfinish:\n\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\treturn iova;\n}\nEXPORT_SYMBOL_GPL(reserve_iova);\n\n \n\n \n#define IOVA_MAG_SIZE 127\n#define MAX_GLOBAL_MAGS 32\t \n\nstruct iova_magazine {\n\tunsigned long size;\n\tunsigned long pfns[IOVA_MAG_SIZE];\n};\n\nstruct iova_cpu_rcache {\n\tspinlock_t lock;\n\tstruct iova_magazine *loaded;\n\tstruct iova_magazine *prev;\n};\n\nstruct iova_rcache {\n\tspinlock_t lock;\n\tunsigned long depot_size;\n\tstruct iova_magazine *depot[MAX_GLOBAL_MAGS];\n\tstruct iova_cpu_rcache __percpu *cpu_rcaches;\n};\n\nstatic struct iova_magazine *iova_magazine_alloc(gfp_t flags)\n{\n\tstruct iova_magazine *mag;\n\n\tmag = kmalloc(sizeof(*mag), flags);\n\tif (mag)\n\t\tmag->size = 0;\n\n\treturn mag;\n}\n\nstatic void iova_magazine_free(struct iova_magazine *mag)\n{\n\tkfree(mag);\n}\n\nstatic void\niova_magazine_free_pfns(struct iova_magazine *mag, struct iova_domain *iovad)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\n\n\tfor (i = 0 ; i < mag->size; ++i) {\n\t\tstruct iova *iova = private_find_iova(iovad, mag->pfns[i]);\n\n\t\tif (WARN_ON(!iova))\n\t\t\tcontinue;\n\n\t\tremove_iova(iovad, iova);\n\t\tfree_iova_mem(iova);\n\t}\n\n\tspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\n\n\tmag->size = 0;\n}\n\nstatic bool iova_magazine_full(struct iova_magazine *mag)\n{\n\treturn mag->size == IOVA_MAG_SIZE;\n}\n\nstatic bool iova_magazine_empty(struct iova_magazine *mag)\n{\n\treturn mag->size == 0;\n}\n\nstatic unsigned long iova_magazine_pop(struct iova_magazine *mag,\n\t\t\t\t       unsigned long limit_pfn)\n{\n\tint i;\n\tunsigned long pfn;\n\n\t \n\tfor (i = mag->size - 1; mag->pfns[i] > limit_pfn; i--)\n\t\tif (i == 0)\n\t\t\treturn 0;\n\n\t \n\tpfn = mag->pfns[i];\n\tmag->pfns[i] = mag->pfns[--mag->size];\n\n\treturn pfn;\n}\n\nstatic void iova_magazine_push(struct iova_magazine *mag, unsigned long pfn)\n{\n\tmag->pfns[mag->size++] = pfn;\n}\n\nint iova_domain_init_rcaches(struct iova_domain *iovad)\n{\n\tunsigned int cpu;\n\tint i, ret;\n\n\tiovad->rcaches = kcalloc(IOVA_RANGE_CACHE_MAX_SIZE,\n\t\t\t\t sizeof(struct iova_rcache),\n\t\t\t\t GFP_KERNEL);\n\tif (!iovad->rcaches)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\n\t\tstruct iova_cpu_rcache *cpu_rcache;\n\t\tstruct iova_rcache *rcache;\n\n\t\trcache = &iovad->rcaches[i];\n\t\tspin_lock_init(&rcache->lock);\n\t\trcache->depot_size = 0;\n\t\trcache->cpu_rcaches = __alloc_percpu(sizeof(*cpu_rcache),\n\t\t\t\t\t\t     cache_line_size());\n\t\tif (!rcache->cpu_rcaches) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tcpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\n\n\t\t\tspin_lock_init(&cpu_rcache->lock);\n\t\t\tcpu_rcache->loaded = iova_magazine_alloc(GFP_KERNEL);\n\t\t\tcpu_rcache->prev = iova_magazine_alloc(GFP_KERNEL);\n\t\t\tif (!cpu_rcache->loaded || !cpu_rcache->prev) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = cpuhp_state_add_instance_nocalls(CPUHP_IOMMU_IOVA_DEAD,\n\t\t\t\t\t       &iovad->cpuhp_dead);\n\tif (ret)\n\t\tgoto out_err;\n\treturn 0;\n\nout_err:\n\tfree_iova_rcaches(iovad);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iova_domain_init_rcaches);\n\n \nstatic bool __iova_rcache_insert(struct iova_domain *iovad,\n\t\t\t\t struct iova_rcache *rcache,\n\t\t\t\t unsigned long iova_pfn)\n{\n\tstruct iova_magazine *mag_to_free = NULL;\n\tstruct iova_cpu_rcache *cpu_rcache;\n\tbool can_insert = false;\n\tunsigned long flags;\n\n\tcpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);\n\tspin_lock_irqsave(&cpu_rcache->lock, flags);\n\n\tif (!iova_magazine_full(cpu_rcache->loaded)) {\n\t\tcan_insert = true;\n\t} else if (!iova_magazine_full(cpu_rcache->prev)) {\n\t\tswap(cpu_rcache->prev, cpu_rcache->loaded);\n\t\tcan_insert = true;\n\t} else {\n\t\tstruct iova_magazine *new_mag = iova_magazine_alloc(GFP_ATOMIC);\n\n\t\tif (new_mag) {\n\t\t\tspin_lock(&rcache->lock);\n\t\t\tif (rcache->depot_size < MAX_GLOBAL_MAGS) {\n\t\t\t\trcache->depot[rcache->depot_size++] =\n\t\t\t\t\t\tcpu_rcache->loaded;\n\t\t\t} else {\n\t\t\t\tmag_to_free = cpu_rcache->loaded;\n\t\t\t}\n\t\t\tspin_unlock(&rcache->lock);\n\n\t\t\tcpu_rcache->loaded = new_mag;\n\t\t\tcan_insert = true;\n\t\t}\n\t}\n\n\tif (can_insert)\n\t\tiova_magazine_push(cpu_rcache->loaded, iova_pfn);\n\n\tspin_unlock_irqrestore(&cpu_rcache->lock, flags);\n\n\tif (mag_to_free) {\n\t\tiova_magazine_free_pfns(mag_to_free, iovad);\n\t\tiova_magazine_free(mag_to_free);\n\t}\n\n\treturn can_insert;\n}\n\nstatic bool iova_rcache_insert(struct iova_domain *iovad, unsigned long pfn,\n\t\t\t       unsigned long size)\n{\n\tunsigned int log_size = order_base_2(size);\n\n\tif (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)\n\t\treturn false;\n\n\treturn __iova_rcache_insert(iovad, &iovad->rcaches[log_size], pfn);\n}\n\n \nstatic unsigned long __iova_rcache_get(struct iova_rcache *rcache,\n\t\t\t\t       unsigned long limit_pfn)\n{\n\tstruct iova_cpu_rcache *cpu_rcache;\n\tunsigned long iova_pfn = 0;\n\tbool has_pfn = false;\n\tunsigned long flags;\n\n\tcpu_rcache = raw_cpu_ptr(rcache->cpu_rcaches);\n\tspin_lock_irqsave(&cpu_rcache->lock, flags);\n\n\tif (!iova_magazine_empty(cpu_rcache->loaded)) {\n\t\thas_pfn = true;\n\t} else if (!iova_magazine_empty(cpu_rcache->prev)) {\n\t\tswap(cpu_rcache->prev, cpu_rcache->loaded);\n\t\thas_pfn = true;\n\t} else {\n\t\tspin_lock(&rcache->lock);\n\t\tif (rcache->depot_size > 0) {\n\t\t\tiova_magazine_free(cpu_rcache->loaded);\n\t\t\tcpu_rcache->loaded = rcache->depot[--rcache->depot_size];\n\t\t\thas_pfn = true;\n\t\t}\n\t\tspin_unlock(&rcache->lock);\n\t}\n\n\tif (has_pfn)\n\t\tiova_pfn = iova_magazine_pop(cpu_rcache->loaded, limit_pfn);\n\n\tspin_unlock_irqrestore(&cpu_rcache->lock, flags);\n\n\treturn iova_pfn;\n}\n\n \nstatic unsigned long iova_rcache_get(struct iova_domain *iovad,\n\t\t\t\t     unsigned long size,\n\t\t\t\t     unsigned long limit_pfn)\n{\n\tunsigned int log_size = order_base_2(size);\n\n\tif (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)\n\t\treturn 0;\n\n\treturn __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn - size);\n}\n\n \nstatic void free_iova_rcaches(struct iova_domain *iovad)\n{\n\tstruct iova_rcache *rcache;\n\tstruct iova_cpu_rcache *cpu_rcache;\n\tunsigned int cpu;\n\tint i, j;\n\n\tfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\n\t\trcache = &iovad->rcaches[i];\n\t\tif (!rcache->cpu_rcaches)\n\t\t\tbreak;\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tcpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\n\t\t\tiova_magazine_free(cpu_rcache->loaded);\n\t\t\tiova_magazine_free(cpu_rcache->prev);\n\t\t}\n\t\tfree_percpu(rcache->cpu_rcaches);\n\t\tfor (j = 0; j < rcache->depot_size; ++j)\n\t\t\tiova_magazine_free(rcache->depot[j]);\n\t}\n\n\tkfree(iovad->rcaches);\n\tiovad->rcaches = NULL;\n}\n\n \nstatic void free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad)\n{\n\tstruct iova_cpu_rcache *cpu_rcache;\n\tstruct iova_rcache *rcache;\n\tunsigned long flags;\n\tint i;\n\n\tfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\n\t\trcache = &iovad->rcaches[i];\n\t\tcpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\n\t\tspin_lock_irqsave(&cpu_rcache->lock, flags);\n\t\tiova_magazine_free_pfns(cpu_rcache->loaded, iovad);\n\t\tiova_magazine_free_pfns(cpu_rcache->prev, iovad);\n\t\tspin_unlock_irqrestore(&cpu_rcache->lock, flags);\n\t}\n}\n\n \nstatic void free_global_cached_iovas(struct iova_domain *iovad)\n{\n\tstruct iova_rcache *rcache;\n\tunsigned long flags;\n\tint i, j;\n\n\tfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\n\t\trcache = &iovad->rcaches[i];\n\t\tspin_lock_irqsave(&rcache->lock, flags);\n\t\tfor (j = 0; j < rcache->depot_size; ++j) {\n\t\t\tiova_magazine_free_pfns(rcache->depot[j], iovad);\n\t\t\tiova_magazine_free(rcache->depot[j]);\n\t\t}\n\t\trcache->depot_size = 0;\n\t\tspin_unlock_irqrestore(&rcache->lock, flags);\n\t}\n}\nMODULE_AUTHOR(\"Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}