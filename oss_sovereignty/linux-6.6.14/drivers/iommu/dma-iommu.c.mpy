{
  "module_name": "dma-iommu.c",
  "hash_id": "635bb08cf4465bde68ef2c4051fcc88bc83e36340d2030c1a9ce5f87e9cd2b41",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/dma-iommu.c",
  "human_readable_source": "\n \n\n#include <linux/acpi_iort.h>\n#include <linux/atomic.h>\n#include <linux/crash_dump.h>\n#include <linux/device.h>\n#include <linux/dma-direct.h>\n#include <linux/dma-map-ops.h>\n#include <linux/gfp.h>\n#include <linux/huge_mm.h>\n#include <linux/iommu.h>\n#include <linux/iova.h>\n#include <linux/irq.h>\n#include <linux/list_sort.h>\n#include <linux/memremap.h>\n#include <linux/mm.h>\n#include <linux/mutex.h>\n#include <linux/of_iommu.h>\n#include <linux/pci.h>\n#include <linux/scatterlist.h>\n#include <linux/spinlock.h>\n#include <linux/swiotlb.h>\n#include <linux/vmalloc.h>\n#include <trace/events/swiotlb.h>\n\n#include \"dma-iommu.h\"\n\nstruct iommu_dma_msi_page {\n\tstruct list_head\tlist;\n\tdma_addr_t\t\tiova;\n\tphys_addr_t\t\tphys;\n};\n\nenum iommu_dma_cookie_type {\n\tIOMMU_DMA_IOVA_COOKIE,\n\tIOMMU_DMA_MSI_COOKIE,\n};\n\nstruct iommu_dma_cookie {\n\tenum iommu_dma_cookie_type\ttype;\n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tstruct iova_domain\tiovad;\n\n\t\t\tstruct iova_fq __percpu *fq;\t \n\t\t\t \n\t\t\tatomic64_t\t\tfq_flush_start_cnt;\n\t\t\t \n\t\t\tatomic64_t\t\tfq_flush_finish_cnt;\n\t\t\t \n\t\t\tstruct timer_list\tfq_timer;\n\t\t\t \n\t\t\tatomic_t\t\tfq_timer_on;\n\t\t};\n\t\t \n\t\tdma_addr_t\t\tmsi_iova;\n\t};\n\tstruct list_head\t\tmsi_page_list;\n\n\t \n\tstruct iommu_domain\t\t*fq_domain;\n\tstruct mutex\t\t\tmutex;\n};\n\nstatic DEFINE_STATIC_KEY_FALSE(iommu_deferred_attach_enabled);\nbool iommu_dma_forcedac __read_mostly;\n\nstatic int __init iommu_dma_forcedac_setup(char *str)\n{\n\tint ret = kstrtobool(str, &iommu_dma_forcedac);\n\n\tif (!ret && iommu_dma_forcedac)\n\t\tpr_info(\"Forcing DAC for PCI devices\\n\");\n\treturn ret;\n}\nearly_param(\"iommu.forcedac\", iommu_dma_forcedac_setup);\n\n \n#define IOVA_FQ_SIZE\t256\n\n \n#define IOVA_FQ_TIMEOUT\t10\n\n \nstruct iova_fq_entry {\n\tunsigned long iova_pfn;\n\tunsigned long pages;\n\tstruct list_head freelist;\n\tu64 counter;  \n};\n\n \nstruct iova_fq {\n\tstruct iova_fq_entry entries[IOVA_FQ_SIZE];\n\tunsigned int head, tail;\n\tspinlock_t lock;\n};\n\n#define fq_ring_for_each(i, fq) \\\n\tfor ((i) = (fq)->head; (i) != (fq)->tail; (i) = ((i) + 1) % IOVA_FQ_SIZE)\n\nstatic inline bool fq_full(struct iova_fq *fq)\n{\n\tassert_spin_locked(&fq->lock);\n\treturn (((fq->tail + 1) % IOVA_FQ_SIZE) == fq->head);\n}\n\nstatic inline unsigned int fq_ring_add(struct iova_fq *fq)\n{\n\tunsigned int idx = fq->tail;\n\n\tassert_spin_locked(&fq->lock);\n\n\tfq->tail = (idx + 1) % IOVA_FQ_SIZE;\n\n\treturn idx;\n}\n\nstatic void fq_ring_free(struct iommu_dma_cookie *cookie, struct iova_fq *fq)\n{\n\tu64 counter = atomic64_read(&cookie->fq_flush_finish_cnt);\n\tunsigned int idx;\n\n\tassert_spin_locked(&fq->lock);\n\n\tfq_ring_for_each(idx, fq) {\n\n\t\tif (fq->entries[idx].counter >= counter)\n\t\t\tbreak;\n\n\t\tput_pages_list(&fq->entries[idx].freelist);\n\t\tfree_iova_fast(&cookie->iovad,\n\t\t\t       fq->entries[idx].iova_pfn,\n\t\t\t       fq->entries[idx].pages);\n\n\t\tfq->head = (fq->head + 1) % IOVA_FQ_SIZE;\n\t}\n}\n\nstatic void fq_flush_iotlb(struct iommu_dma_cookie *cookie)\n{\n\tatomic64_inc(&cookie->fq_flush_start_cnt);\n\tcookie->fq_domain->ops->flush_iotlb_all(cookie->fq_domain);\n\tatomic64_inc(&cookie->fq_flush_finish_cnt);\n}\n\nstatic void fq_flush_timeout(struct timer_list *t)\n{\n\tstruct iommu_dma_cookie *cookie = from_timer(cookie, t, fq_timer);\n\tint cpu;\n\n\tatomic_set(&cookie->fq_timer_on, 0);\n\tfq_flush_iotlb(cookie);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tunsigned long flags;\n\t\tstruct iova_fq *fq;\n\n\t\tfq = per_cpu_ptr(cookie->fq, cpu);\n\t\tspin_lock_irqsave(&fq->lock, flags);\n\t\tfq_ring_free(cookie, fq);\n\t\tspin_unlock_irqrestore(&fq->lock, flags);\n\t}\n}\n\nstatic void queue_iova(struct iommu_dma_cookie *cookie,\n\t\tunsigned long pfn, unsigned long pages,\n\t\tstruct list_head *freelist)\n{\n\tstruct iova_fq *fq;\n\tunsigned long flags;\n\tunsigned int idx;\n\n\t \n\tsmp_mb();\n\n\tfq = raw_cpu_ptr(cookie->fq);\n\tspin_lock_irqsave(&fq->lock, flags);\n\n\t \n\tfq_ring_free(cookie, fq);\n\n\tif (fq_full(fq)) {\n\t\tfq_flush_iotlb(cookie);\n\t\tfq_ring_free(cookie, fq);\n\t}\n\n\tidx = fq_ring_add(fq);\n\n\tfq->entries[idx].iova_pfn = pfn;\n\tfq->entries[idx].pages    = pages;\n\tfq->entries[idx].counter  = atomic64_read(&cookie->fq_flush_start_cnt);\n\tlist_splice(freelist, &fq->entries[idx].freelist);\n\n\tspin_unlock_irqrestore(&fq->lock, flags);\n\n\t \n\tif (!atomic_read(&cookie->fq_timer_on) &&\n\t    !atomic_xchg(&cookie->fq_timer_on, 1))\n\t\tmod_timer(&cookie->fq_timer,\n\t\t\t  jiffies + msecs_to_jiffies(IOVA_FQ_TIMEOUT));\n}\n\nstatic void iommu_dma_free_fq(struct iommu_dma_cookie *cookie)\n{\n\tint cpu, idx;\n\n\tif (!cookie->fq)\n\t\treturn;\n\n\tdel_timer_sync(&cookie->fq_timer);\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct iova_fq *fq = per_cpu_ptr(cookie->fq, cpu);\n\n\t\tfq_ring_for_each(idx, fq)\n\t\t\tput_pages_list(&fq->entries[idx].freelist);\n\t}\n\n\tfree_percpu(cookie->fq);\n}\n\n \nint iommu_dma_init_fq(struct iommu_domain *domain)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_fq __percpu *queue;\n\tint i, cpu;\n\n\tif (cookie->fq_domain)\n\t\treturn 0;\n\n\tatomic64_set(&cookie->fq_flush_start_cnt,  0);\n\tatomic64_set(&cookie->fq_flush_finish_cnt, 0);\n\n\tqueue = alloc_percpu(struct iova_fq);\n\tif (!queue) {\n\t\tpr_warn(\"iova flush queue initialization failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct iova_fq *fq = per_cpu_ptr(queue, cpu);\n\n\t\tfq->head = 0;\n\t\tfq->tail = 0;\n\n\t\tspin_lock_init(&fq->lock);\n\n\t\tfor (i = 0; i < IOVA_FQ_SIZE; i++)\n\t\t\tINIT_LIST_HEAD(&fq->entries[i].freelist);\n\t}\n\n\tcookie->fq = queue;\n\n\ttimer_setup(&cookie->fq_timer, fq_flush_timeout, 0);\n\tatomic_set(&cookie->fq_timer_on, 0);\n\t \n\tsmp_wmb();\n\tWRITE_ONCE(cookie->fq_domain, domain);\n\treturn 0;\n}\n\nstatic inline size_t cookie_msi_granule(struct iommu_dma_cookie *cookie)\n{\n\tif (cookie->type == IOMMU_DMA_IOVA_COOKIE)\n\t\treturn cookie->iovad.granule;\n\treturn PAGE_SIZE;\n}\n\nstatic struct iommu_dma_cookie *cookie_alloc(enum iommu_dma_cookie_type type)\n{\n\tstruct iommu_dma_cookie *cookie;\n\n\tcookie = kzalloc(sizeof(*cookie), GFP_KERNEL);\n\tif (cookie) {\n\t\tINIT_LIST_HEAD(&cookie->msi_page_list);\n\t\tcookie->type = type;\n\t}\n\treturn cookie;\n}\n\n \nint iommu_get_dma_cookie(struct iommu_domain *domain)\n{\n\tif (domain->iova_cookie)\n\t\treturn -EEXIST;\n\n\tdomain->iova_cookie = cookie_alloc(IOMMU_DMA_IOVA_COOKIE);\n\tif (!domain->iova_cookie)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&domain->iova_cookie->mutex);\n\treturn 0;\n}\n\n \nint iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base)\n{\n\tstruct iommu_dma_cookie *cookie;\n\n\tif (domain->type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn -EINVAL;\n\n\tif (domain->iova_cookie)\n\t\treturn -EEXIST;\n\n\tcookie = cookie_alloc(IOMMU_DMA_MSI_COOKIE);\n\tif (!cookie)\n\t\treturn -ENOMEM;\n\n\tcookie->msi_iova = base;\n\tdomain->iova_cookie = cookie;\n\treturn 0;\n}\nEXPORT_SYMBOL(iommu_get_msi_cookie);\n\n \nvoid iommu_put_dma_cookie(struct iommu_domain *domain)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iommu_dma_msi_page *msi, *tmp;\n\n\tif (!cookie)\n\t\treturn;\n\n\tif (cookie->type == IOMMU_DMA_IOVA_COOKIE && cookie->iovad.granule) {\n\t\tiommu_dma_free_fq(cookie);\n\t\tput_iova_domain(&cookie->iovad);\n\t}\n\n\tlist_for_each_entry_safe(msi, tmp, &cookie->msi_page_list, list) {\n\t\tlist_del(&msi->list);\n\t\tkfree(msi);\n\t}\n\tkfree(cookie);\n\tdomain->iova_cookie = NULL;\n}\n\n \nvoid iommu_dma_get_resv_regions(struct device *dev, struct list_head *list)\n{\n\n\tif (!is_of_node(dev_iommu_fwspec_get(dev)->iommu_fwnode))\n\t\tiort_iommu_get_resv_regions(dev, list);\n\n\tif (dev->of_node)\n\t\tof_iommu_get_resv_regions(dev, list);\n}\nEXPORT_SYMBOL(iommu_dma_get_resv_regions);\n\nstatic int cookie_init_hw_msi_region(struct iommu_dma_cookie *cookie,\n\t\tphys_addr_t start, phys_addr_t end)\n{\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tstruct iommu_dma_msi_page *msi_page;\n\tint i, num_pages;\n\n\tstart -= iova_offset(iovad, start);\n\tnum_pages = iova_align(iovad, end - start) >> iova_shift(iovad);\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tmsi_page = kmalloc(sizeof(*msi_page), GFP_KERNEL);\n\t\tif (!msi_page)\n\t\t\treturn -ENOMEM;\n\n\t\tmsi_page->phys = start;\n\t\tmsi_page->iova = start;\n\t\tINIT_LIST_HEAD(&msi_page->list);\n\t\tlist_add(&msi_page->list, &cookie->msi_page_list);\n\t\tstart += iovad->granule;\n\t}\n\n\treturn 0;\n}\n\nstatic int iommu_dma_ranges_sort(void *priv, const struct list_head *a,\n\t\tconst struct list_head *b)\n{\n\tstruct resource_entry *res_a = list_entry(a, typeof(*res_a), node);\n\tstruct resource_entry *res_b = list_entry(b, typeof(*res_b), node);\n\n\treturn res_a->res->start > res_b->res->start;\n}\n\nstatic int iova_reserve_pci_windows(struct pci_dev *dev,\n\t\tstruct iova_domain *iovad)\n{\n\tstruct pci_host_bridge *bridge = pci_find_host_bridge(dev->bus);\n\tstruct resource_entry *window;\n\tunsigned long lo, hi;\n\tphys_addr_t start = 0, end;\n\n\tresource_list_for_each_entry(window, &bridge->windows) {\n\t\tif (resource_type(window->res) != IORESOURCE_MEM)\n\t\t\tcontinue;\n\n\t\tlo = iova_pfn(iovad, window->res->start - window->offset);\n\t\thi = iova_pfn(iovad, window->res->end - window->offset);\n\t\treserve_iova(iovad, lo, hi);\n\t}\n\n\t \n\tlist_sort(NULL, &bridge->dma_ranges, iommu_dma_ranges_sort);\n\tresource_list_for_each_entry(window, &bridge->dma_ranges) {\n\t\tend = window->res->start - window->offset;\nresv_iova:\n\t\tif (end > start) {\n\t\t\tlo = iova_pfn(iovad, start);\n\t\t\thi = iova_pfn(iovad, end);\n\t\t\treserve_iova(iovad, lo, hi);\n\t\t} else if (end < start) {\n\t\t\t \n\t\t\tdev_err(&dev->dev,\n\t\t\t\t\"Failed to reserve IOVA [%pa-%pa]\\n\",\n\t\t\t\t&start, &end);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tstart = window->res->end - window->offset + 1;\n\t\t \n\t\tif (window->node.next == &bridge->dma_ranges &&\n\t\t    end != ~(phys_addr_t)0) {\n\t\t\tend = ~(phys_addr_t)0;\n\t\t\tgoto resv_iova;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int iova_reserve_iommu_regions(struct device *dev,\n\t\tstruct iommu_domain *domain)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tstruct iommu_resv_region *region;\n\tLIST_HEAD(resv_regions);\n\tint ret = 0;\n\n\tif (dev_is_pci(dev)) {\n\t\tret = iova_reserve_pci_windows(to_pci_dev(dev), iovad);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tiommu_get_resv_regions(dev, &resv_regions);\n\tlist_for_each_entry(region, &resv_regions, list) {\n\t\tunsigned long lo, hi;\n\n\t\t \n\t\tif (region->type == IOMMU_RESV_SW_MSI)\n\t\t\tcontinue;\n\n\t\tlo = iova_pfn(iovad, region->start);\n\t\thi = iova_pfn(iovad, region->start + region->length - 1);\n\t\treserve_iova(iovad, lo, hi);\n\n\t\tif (region->type == IOMMU_RESV_MSI)\n\t\t\tret = cookie_init_hw_msi_region(cookie, region->start,\n\t\t\t\t\tregion->start + region->length);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tiommu_put_resv_regions(dev, &resv_regions);\n\n\treturn ret;\n}\n\nstatic bool dev_is_untrusted(struct device *dev)\n{\n\treturn dev_is_pci(dev) && to_pci_dev(dev)->untrusted;\n}\n\nstatic bool dev_use_swiotlb(struct device *dev, size_t size,\n\t\t\t    enum dma_data_direction dir)\n{\n\treturn IS_ENABLED(CONFIG_SWIOTLB) &&\n\t\t(dev_is_untrusted(dev) ||\n\t\t dma_kmalloc_needs_bounce(dev, size, dir));\n}\n\nstatic bool dev_use_sg_swiotlb(struct device *dev, struct scatterlist *sg,\n\t\t\t       int nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tif (!IS_ENABLED(CONFIG_SWIOTLB))\n\t\treturn false;\n\n\tif (dev_is_untrusted(dev))\n\t\treturn true;\n\n\t \n\tif (!dma_kmalloc_safe(dev, dir)) {\n\t\tfor_each_sg(sg, s, nents, i)\n\t\t\tif (!dma_kmalloc_size_aligned(s->length))\n\t\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int iommu_dma_init_domain(struct iommu_domain *domain, dma_addr_t base,\n\t\t\t\t dma_addr_t limit, struct device *dev)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tunsigned long order, base_pfn;\n\tstruct iova_domain *iovad;\n\tint ret;\n\n\tif (!cookie || cookie->type != IOMMU_DMA_IOVA_COOKIE)\n\t\treturn -EINVAL;\n\n\tiovad = &cookie->iovad;\n\n\t \n\torder = __ffs(domain->pgsize_bitmap);\n\tbase_pfn = max_t(unsigned long, 1, base >> order);\n\n\t \n\tif (domain->geometry.force_aperture) {\n\t\tif (base > domain->geometry.aperture_end ||\n\t\t    limit < domain->geometry.aperture_start) {\n\t\t\tpr_warn(\"specified DMA range outside IOMMU capability\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t \n\t\tbase_pfn = max_t(unsigned long, base_pfn,\n\t\t\t\tdomain->geometry.aperture_start >> order);\n\t}\n\n\t \n\tmutex_lock(&cookie->mutex);\n\tif (iovad->start_pfn) {\n\t\tif (1UL << order != iovad->granule ||\n\t\t    base_pfn != iovad->start_pfn) {\n\t\t\tpr_warn(\"Incompatible range for DMA domain\\n\");\n\t\t\tret = -EFAULT;\n\t\t\tgoto done_unlock;\n\t\t}\n\n\t\tret = 0;\n\t\tgoto done_unlock;\n\t}\n\n\tinit_iova_domain(iovad, 1UL << order, base_pfn);\n\tret = iova_domain_init_rcaches(iovad);\n\tif (ret)\n\t\tgoto done_unlock;\n\n\t \n\tif (domain->type == IOMMU_DOMAIN_DMA_FQ &&\n\t    (!device_iommu_capable(dev, IOMMU_CAP_DEFERRED_FLUSH) || iommu_dma_init_fq(domain)))\n\t\tdomain->type = IOMMU_DOMAIN_DMA;\n\n\tret = iova_reserve_iommu_regions(dev, domain);\n\ndone_unlock:\n\tmutex_unlock(&cookie->mutex);\n\treturn ret;\n}\n\n \nstatic int dma_info_to_prot(enum dma_data_direction dir, bool coherent,\n\t\t     unsigned long attrs)\n{\n\tint prot = coherent ? IOMMU_CACHE : 0;\n\n\tif (attrs & DMA_ATTR_PRIVILEGED)\n\t\tprot |= IOMMU_PRIV;\n\n\tswitch (dir) {\n\tcase DMA_BIDIRECTIONAL:\n\t\treturn prot | IOMMU_READ | IOMMU_WRITE;\n\tcase DMA_TO_DEVICE:\n\t\treturn prot | IOMMU_READ;\n\tcase DMA_FROM_DEVICE:\n\t\treturn prot | IOMMU_WRITE;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic dma_addr_t iommu_dma_alloc_iova(struct iommu_domain *domain,\n\t\tsize_t size, u64 dma_limit, struct device *dev)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tunsigned long shift, iova_len, iova;\n\n\tif (cookie->type == IOMMU_DMA_MSI_COOKIE) {\n\t\tcookie->msi_iova += size;\n\t\treturn cookie->msi_iova - size;\n\t}\n\n\tshift = iova_shift(iovad);\n\tiova_len = size >> shift;\n\n\tdma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);\n\n\tif (domain->geometry.force_aperture)\n\t\tdma_limit = min(dma_limit, (u64)domain->geometry.aperture_end);\n\n\t \n\tif (dma_limit > DMA_BIT_MASK(32) && dev->iommu->pci_32bit_workaround) {\n\t\tiova = alloc_iova_fast(iovad, iova_len,\n\t\t\t\t       DMA_BIT_MASK(32) >> shift, false);\n\t\tif (iova)\n\t\t\tgoto done;\n\n\t\tdev->iommu->pci_32bit_workaround = false;\n\t\tdev_notice(dev, \"Using %d-bit DMA addresses\\n\", bits_per(dma_limit));\n\t}\n\n\tiova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift, true);\ndone:\n\treturn (dma_addr_t)iova << shift;\n}\n\nstatic void iommu_dma_free_iova(struct iommu_dma_cookie *cookie,\n\t\tdma_addr_t iova, size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct iova_domain *iovad = &cookie->iovad;\n\n\t \n\tif (cookie->type == IOMMU_DMA_MSI_COOKIE)\n\t\tcookie->msi_iova -= size;\n\telse if (gather && gather->queued)\n\t\tqueue_iova(cookie, iova_pfn(iovad, iova),\n\t\t\t\tsize >> iova_shift(iovad),\n\t\t\t\t&gather->freelist);\n\telse\n\t\tfree_iova_fast(iovad, iova_pfn(iovad, iova),\n\t\t\t\tsize >> iova_shift(iovad));\n}\n\nstatic void __iommu_dma_unmap(struct device *dev, dma_addr_t dma_addr,\n\t\tsize_t size)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tsize_t iova_off = iova_offset(iovad, dma_addr);\n\tstruct iommu_iotlb_gather iotlb_gather;\n\tsize_t unmapped;\n\n\tdma_addr -= iova_off;\n\tsize = iova_align(iovad, size + iova_off);\n\tiommu_iotlb_gather_init(&iotlb_gather);\n\tiotlb_gather.queued = READ_ONCE(cookie->fq_domain);\n\n\tunmapped = iommu_unmap_fast(domain, dma_addr, size, &iotlb_gather);\n\tWARN_ON(unmapped != size);\n\n\tif (!iotlb_gather.queued)\n\t\tiommu_iotlb_sync(domain, &iotlb_gather);\n\tiommu_dma_free_iova(cookie, dma_addr, size, &iotlb_gather);\n}\n\nstatic dma_addr_t __iommu_dma_map(struct device *dev, phys_addr_t phys,\n\t\tsize_t size, int prot, u64 dma_mask)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tsize_t iova_off = iova_offset(iovad, phys);\n\tdma_addr_t iova;\n\n\tif (static_branch_unlikely(&iommu_deferred_attach_enabled) &&\n\t    iommu_deferred_attach(dev, domain))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tsize = iova_align(iovad, size + iova_off);\n\n\tiova = iommu_dma_alloc_iova(domain, size, dma_mask, dev);\n\tif (!iova)\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (iommu_map(domain, iova, phys - iova_off, size, prot, GFP_ATOMIC)) {\n\t\tiommu_dma_free_iova(cookie, iova, size, NULL);\n\t\treturn DMA_MAPPING_ERROR;\n\t}\n\treturn iova + iova_off;\n}\n\nstatic void __iommu_dma_free_pages(struct page **pages, int count)\n{\n\twhile (count--)\n\t\t__free_page(pages[count]);\n\tkvfree(pages);\n}\n\nstatic struct page **__iommu_dma_alloc_pages(struct device *dev,\n\t\tunsigned int count, unsigned long order_mask, gfp_t gfp)\n{\n\tstruct page **pages;\n\tunsigned int i = 0, nid = dev_to_node(dev);\n\n\torder_mask &= GENMASK(MAX_ORDER, 0);\n\tif (!order_mask)\n\t\treturn NULL;\n\n\tpages = kvcalloc(count, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn NULL;\n\n\t \n\tgfp |= __GFP_NOWARN | __GFP_HIGHMEM;\n\n\twhile (count) {\n\t\tstruct page *page = NULL;\n\t\tunsigned int order_size;\n\n\t\t \n\t\tfor (order_mask &= GENMASK(__fls(count), 0);\n\t\t     order_mask; order_mask &= ~order_size) {\n\t\t\tunsigned int order = __fls(order_mask);\n\t\t\tgfp_t alloc_flags = gfp;\n\n\t\t\torder_size = 1U << order;\n\t\t\tif (order_mask > order_size)\n\t\t\t\talloc_flags |= __GFP_NORETRY;\n\t\t\tpage = alloc_pages_node(nid, alloc_flags, order);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\t\t\tif (order)\n\t\t\t\tsplit_page(page, order);\n\t\t\tbreak;\n\t\t}\n\t\tif (!page) {\n\t\t\t__iommu_dma_free_pages(pages, i);\n\t\t\treturn NULL;\n\t\t}\n\t\tcount -= order_size;\n\t\twhile (order_size--)\n\t\t\tpages[i++] = page++;\n\t}\n\treturn pages;\n}\n\n \nstatic struct page **__iommu_dma_alloc_noncontiguous(struct device *dev,\n\t\tsize_t size, struct sg_table *sgt, gfp_t gfp, pgprot_t prot,\n\t\tunsigned long attrs)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tbool coherent = dev_is_dma_coherent(dev);\n\tint ioprot = dma_info_to_prot(DMA_BIDIRECTIONAL, coherent, attrs);\n\tunsigned int count, min_size, alloc_sizes = domain->pgsize_bitmap;\n\tstruct page **pages;\n\tdma_addr_t iova;\n\tssize_t ret;\n\n\tif (static_branch_unlikely(&iommu_deferred_attach_enabled) &&\n\t    iommu_deferred_attach(dev, domain))\n\t\treturn NULL;\n\n\tmin_size = alloc_sizes & -alloc_sizes;\n\tif (min_size < PAGE_SIZE) {\n\t\tmin_size = PAGE_SIZE;\n\t\talloc_sizes |= PAGE_SIZE;\n\t} else {\n\t\tsize = ALIGN(size, min_size);\n\t}\n\tif (attrs & DMA_ATTR_ALLOC_SINGLE_PAGES)\n\t\talloc_sizes = min_size;\n\n\tcount = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tpages = __iommu_dma_alloc_pages(dev, count, alloc_sizes >> PAGE_SHIFT,\n\t\t\t\t\tgfp);\n\tif (!pages)\n\t\treturn NULL;\n\n\tsize = iova_align(iovad, size);\n\tiova = iommu_dma_alloc_iova(domain, size, dev->coherent_dma_mask, dev);\n\tif (!iova)\n\t\tgoto out_free_pages;\n\n\t \n\tgfp &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM | __GFP_COMP);\n\n\tif (sg_alloc_table_from_pages(sgt, pages, count, 0, size, gfp))\n\t\tgoto out_free_iova;\n\n\tif (!(ioprot & IOMMU_CACHE)) {\n\t\tstruct scatterlist *sg;\n\t\tint i;\n\n\t\tfor_each_sg(sgt->sgl, sg, sgt->orig_nents, i)\n\t\t\tarch_dma_prep_coherent(sg_page(sg), sg->length);\n\t}\n\n\tret = iommu_map_sg(domain, iova, sgt->sgl, sgt->orig_nents, ioprot,\n\t\t\t   gfp);\n\tif (ret < 0 || ret < size)\n\t\tgoto out_free_sg;\n\n\tsgt->sgl->dma_address = iova;\n\tsgt->sgl->dma_length = size;\n\treturn pages;\n\nout_free_sg:\n\tsg_free_table(sgt);\nout_free_iova:\n\tiommu_dma_free_iova(cookie, iova, size, NULL);\nout_free_pages:\n\t__iommu_dma_free_pages(pages, count);\n\treturn NULL;\n}\n\nstatic void *iommu_dma_alloc_remap(struct device *dev, size_t size,\n\t\tdma_addr_t *dma_handle, gfp_t gfp, pgprot_t prot,\n\t\tunsigned long attrs)\n{\n\tstruct page **pages;\n\tstruct sg_table sgt;\n\tvoid *vaddr;\n\n\tpages = __iommu_dma_alloc_noncontiguous(dev, size, &sgt, gfp, prot,\n\t\t\t\t\t\tattrs);\n\tif (!pages)\n\t\treturn NULL;\n\t*dma_handle = sgt.sgl->dma_address;\n\tsg_free_table(&sgt);\n\tvaddr = dma_common_pages_remap(pages, size, prot,\n\t\t\t__builtin_return_address(0));\n\tif (!vaddr)\n\t\tgoto out_unmap;\n\treturn vaddr;\n\nout_unmap:\n\t__iommu_dma_unmap(dev, *dma_handle, size);\n\t__iommu_dma_free_pages(pages, PAGE_ALIGN(size) >> PAGE_SHIFT);\n\treturn NULL;\n}\n\nstatic struct sg_table *iommu_dma_alloc_noncontiguous(struct device *dev,\n\t\tsize_t size, enum dma_data_direction dir, gfp_t gfp,\n\t\tunsigned long attrs)\n{\n\tstruct dma_sgt_handle *sh;\n\n\tsh = kmalloc(sizeof(*sh), gfp);\n\tif (!sh)\n\t\treturn NULL;\n\n\tsh->pages = __iommu_dma_alloc_noncontiguous(dev, size, &sh->sgt, gfp,\n\t\t\t\t\t\t    PAGE_KERNEL, attrs);\n\tif (!sh->pages) {\n\t\tkfree(sh);\n\t\treturn NULL;\n\t}\n\treturn &sh->sgt;\n}\n\nstatic void iommu_dma_free_noncontiguous(struct device *dev, size_t size,\n\t\tstruct sg_table *sgt, enum dma_data_direction dir)\n{\n\tstruct dma_sgt_handle *sh = sgt_handle(sgt);\n\n\t__iommu_dma_unmap(dev, sgt->sgl->dma_address, size);\n\t__iommu_dma_free_pages(sh->pages, PAGE_ALIGN(size) >> PAGE_SHIFT);\n\tsg_free_table(&sh->sgt);\n\tkfree(sh);\n}\n\nstatic void iommu_dma_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t dma_handle, size_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t phys;\n\n\tif (dev_is_dma_coherent(dev) && !dev_use_swiotlb(dev, size, dir))\n\t\treturn;\n\n\tphys = iommu_iova_to_phys(iommu_get_dma_domain(dev), dma_handle);\n\tif (!dev_is_dma_coherent(dev))\n\t\tarch_sync_dma_for_cpu(phys, size, dir);\n\n\tif (is_swiotlb_buffer(dev, phys))\n\t\tswiotlb_sync_single_for_cpu(dev, phys, size, dir);\n}\n\nstatic void iommu_dma_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t dma_handle, size_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t phys;\n\n\tif (dev_is_dma_coherent(dev) && !dev_use_swiotlb(dev, size, dir))\n\t\treturn;\n\n\tphys = iommu_iova_to_phys(iommu_get_dma_domain(dev), dma_handle);\n\tif (is_swiotlb_buffer(dev, phys))\n\t\tswiotlb_sync_single_for_device(dev, phys, size, dir);\n\n\tif (!dev_is_dma_coherent(dev))\n\t\tarch_sync_dma_for_device(phys, size, dir);\n}\n\nstatic void iommu_dma_sync_sg_for_cpu(struct device *dev,\n\t\tstruct scatterlist *sgl, int nelems,\n\t\tenum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (sg_dma_is_swiotlb(sgl))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\tiommu_dma_sync_single_for_cpu(dev, sg_dma_address(sg),\n\t\t\t\t\t\t      sg->length, dir);\n\telse if (!dev_is_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\tarch_sync_dma_for_cpu(sg_phys(sg), sg->length, dir);\n}\n\nstatic void iommu_dma_sync_sg_for_device(struct device *dev,\n\t\tstruct scatterlist *sgl, int nelems,\n\t\tenum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (sg_dma_is_swiotlb(sgl))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\tiommu_dma_sync_single_for_device(dev,\n\t\t\t\t\t\t\t sg_dma_address(sg),\n\t\t\t\t\t\t\t sg->length, dir);\n\telse if (!dev_is_dma_coherent(dev))\n\t\tfor_each_sg(sgl, sg, nelems, i)\n\t\t\tarch_sync_dma_for_device(sg_phys(sg), sg->length, dir);\n}\n\nstatic dma_addr_t iommu_dma_map_page(struct device *dev, struct page *page,\n\t\tunsigned long offset, size_t size, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tphys_addr_t phys = page_to_phys(page) + offset;\n\tbool coherent = dev_is_dma_coherent(dev);\n\tint prot = dma_info_to_prot(dir, coherent, attrs);\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tdma_addr_t iova, dma_mask = dma_get_mask(dev);\n\n\t \n\tif (dev_use_swiotlb(dev, size, dir) &&\n\t    iova_offset(iovad, phys | size)) {\n\t\tvoid *padding_start;\n\t\tsize_t padding_size, aligned_size;\n\n\t\tif (!is_swiotlb_active(dev)) {\n\t\t\tdev_warn_once(dev, \"DMA bounce buffers are inactive, unable to map unaligned transaction.\\n\");\n\t\t\treturn DMA_MAPPING_ERROR;\n\t\t}\n\n\t\ttrace_swiotlb_bounced(dev, phys, size);\n\n\t\taligned_size = iova_align(iovad, size);\n\t\tphys = swiotlb_tbl_map_single(dev, phys, size, aligned_size,\n\t\t\t\t\t      iova_mask(iovad), dir, attrs);\n\n\t\tif (phys == DMA_MAPPING_ERROR)\n\t\t\treturn DMA_MAPPING_ERROR;\n\n\t\t \n\t\tpadding_start = phys_to_virt(phys);\n\t\tpadding_size = aligned_size;\n\n\t\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\n\t\t    (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)) {\n\t\t\tpadding_start += size;\n\t\t\tpadding_size -= size;\n\t\t}\n\n\t\tmemset(padding_start, 0, padding_size);\n\t}\n\n\tif (!coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\tarch_sync_dma_for_device(phys, size, dir);\n\n\tiova = __iommu_dma_map(dev, phys, size, prot, dma_mask);\n\tif (iova == DMA_MAPPING_ERROR && is_swiotlb_buffer(dev, phys))\n\t\tswiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);\n\treturn iova;\n}\n\nstatic void iommu_dma_unmap_page(struct device *dev, dma_addr_t dma_handle,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tphys_addr_t phys;\n\n\tphys = iommu_iova_to_phys(domain, dma_handle);\n\tif (WARN_ON(!phys))\n\t\treturn;\n\n\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) && !dev_is_dma_coherent(dev))\n\t\tarch_sync_dma_for_cpu(phys, size, dir);\n\n\t__iommu_dma_unmap(dev, dma_handle, size);\n\n\tif (unlikely(is_swiotlb_buffer(dev, phys)))\n\t\tswiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);\n}\n\n \nstatic int __finalise_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tdma_addr_t dma_addr)\n{\n\tstruct scatterlist *s, *cur = sg;\n\tunsigned long seg_mask = dma_get_seg_boundary(dev);\n\tunsigned int cur_len = 0, max_len = dma_get_max_seg_size(dev);\n\tint i, count = 0;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\t \n\t\tdma_addr_t s_dma_addr = sg_dma_address(s);\n\t\tunsigned int s_iova_off = sg_dma_address(s);\n\t\tunsigned int s_length = sg_dma_len(s);\n\t\tunsigned int s_iova_len = s->length;\n\n\t\tsg_dma_address(s) = DMA_MAPPING_ERROR;\n\t\tsg_dma_len(s) = 0;\n\n\t\tif (sg_dma_is_bus_address(s)) {\n\t\t\tif (i > 0)\n\t\t\t\tcur = sg_next(cur);\n\n\t\t\tsg_dma_unmark_bus_address(s);\n\t\t\tsg_dma_address(cur) = s_dma_addr;\n\t\t\tsg_dma_len(cur) = s_length;\n\t\t\tsg_dma_mark_bus_address(cur);\n\t\t\tcount++;\n\t\t\tcur_len = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\ts->offset += s_iova_off;\n\t\ts->length = s_length;\n\n\t\t \n\t\tif (cur_len && !s_iova_off && (dma_addr & seg_mask) &&\n\t\t    (max_len - cur_len >= s_length)) {\n\t\t\t \n\t\t\tcur_len += s_length;\n\t\t} else {\n\t\t\t \n\t\t\tif (i > 0)\n\t\t\t\tcur = sg_next(cur);\n\t\t\tcur_len = s_length;\n\t\t\tcount++;\n\n\t\t\tsg_dma_address(cur) = dma_addr + s_iova_off;\n\t\t}\n\n\t\tsg_dma_len(cur) = cur_len;\n\t\tdma_addr += s_iova_len;\n\n\t\tif (s_length + s_iova_off < s_iova_len)\n\t\t\tcur_len = 0;\n\t}\n\treturn count;\n}\n\n \nstatic void __invalidate_sg(struct scatterlist *sg, int nents)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (sg_dma_is_bus_address(s)) {\n\t\t\tsg_dma_unmark_bus_address(s);\n\t\t} else {\n\t\t\tif (sg_dma_address(s) != DMA_MAPPING_ERROR)\n\t\t\t\ts->offset += sg_dma_address(s);\n\t\t\tif (sg_dma_len(s))\n\t\t\t\ts->length = sg_dma_len(s);\n\t\t}\n\t\tsg_dma_address(s) = DMA_MAPPING_ERROR;\n\t\tsg_dma_len(s) = 0;\n\t}\n}\n\nstatic void iommu_dma_unmap_sg_swiotlb(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tiommu_dma_unmap_page(dev, sg_dma_address(s),\n\t\t\t\tsg_dma_len(s), dir, attrs);\n}\n\nstatic int iommu_dma_map_sg_swiotlb(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tsg_dma_mark_swiotlb(sg);\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tsg_dma_address(s) = iommu_dma_map_page(dev, sg_page(s),\n\t\t\t\ts->offset, s->length, dir, attrs);\n\t\tif (sg_dma_address(s) == DMA_MAPPING_ERROR)\n\t\t\tgoto out_unmap;\n\t\tsg_dma_len(s) = s->length;\n\t}\n\n\treturn nents;\n\nout_unmap:\n\tiommu_dma_unmap_sg_swiotlb(dev, sg, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);\n\treturn -EIO;\n}\n\n \nstatic int iommu_dma_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iova_domain *iovad = &cookie->iovad;\n\tstruct scatterlist *s, *prev = NULL;\n\tint prot = dma_info_to_prot(dir, dev_is_dma_coherent(dev), attrs);\n\tstruct pci_p2pdma_map_state p2pdma_state = {};\n\tenum pci_p2pdma_map_type map;\n\tdma_addr_t iova;\n\tsize_t iova_len = 0;\n\tunsigned long mask = dma_get_seg_boundary(dev);\n\tssize_t ret;\n\tint i;\n\n\tif (static_branch_unlikely(&iommu_deferred_attach_enabled)) {\n\t\tret = iommu_deferred_attach(dev, domain);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (dev_use_sg_swiotlb(dev, sg, nents, dir))\n\t\treturn iommu_dma_map_sg_swiotlb(dev, sg, nents, dir, attrs);\n\n\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\tiommu_dma_sync_sg_for_device(dev, sg, nents, dir);\n\n\t \n\tfor_each_sg(sg, s, nents, i) {\n\t\tsize_t s_iova_off = iova_offset(iovad, s->offset);\n\t\tsize_t s_length = s->length;\n\t\tsize_t pad_len = (mask - iova_len + 1) & mask;\n\n\t\tif (is_pci_p2pdma_page(sg_page(s))) {\n\t\t\tmap = pci_p2pdma_map_segment(&p2pdma_state, dev, s);\n\t\t\tswitch (map) {\n\t\t\tcase PCI_P2PDMA_MAP_BUS_ADDR:\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tcase PCI_P2PDMA_MAP_THRU_HOST_BRIDGE:\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tret = -EREMOTEIO;\n\t\t\t\tgoto out_restore_sg;\n\t\t\t}\n\t\t}\n\n\t\tsg_dma_address(s) = s_iova_off;\n\t\tsg_dma_len(s) = s_length;\n\t\ts->offset -= s_iova_off;\n\t\ts_length = iova_align(iovad, s_length + s_iova_off);\n\t\ts->length = s_length;\n\n\t\t \n\t\tif (pad_len && pad_len < s_length - 1) {\n\t\t\tprev->length += pad_len;\n\t\t\tiova_len += pad_len;\n\t\t}\n\n\t\tiova_len += s_length;\n\t\tprev = s;\n\t}\n\n\tif (!iova_len)\n\t\treturn __finalise_sg(dev, sg, nents, 0);\n\n\tiova = iommu_dma_alloc_iova(domain, iova_len, dma_get_mask(dev), dev);\n\tif (!iova) {\n\t\tret = -ENOMEM;\n\t\tgoto out_restore_sg;\n\t}\n\n\t \n\tret = iommu_map_sg(domain, iova, sg, nents, prot, GFP_ATOMIC);\n\tif (ret < 0 || ret < iova_len)\n\t\tgoto out_free_iova;\n\n\treturn __finalise_sg(dev, sg, nents, iova);\n\nout_free_iova:\n\tiommu_dma_free_iova(cookie, iova, iova_len, NULL);\nout_restore_sg:\n\t__invalidate_sg(sg, nents);\nout:\n\tif (ret != -ENOMEM && ret != -EREMOTEIO)\n\t\treturn -EINVAL;\n\treturn ret;\n}\n\nstatic void iommu_dma_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tdma_addr_t end = 0, start;\n\tstruct scatterlist *tmp;\n\tint i;\n\n\tif (sg_dma_is_swiotlb(sg)) {\n\t\tiommu_dma_unmap_sg_swiotlb(dev, sg, nents, dir, attrs);\n\t\treturn;\n\t}\n\n\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\tiommu_dma_sync_sg_for_cpu(dev, sg, nents, dir);\n\n\t \n\tfor_each_sg(sg, tmp, nents, i) {\n\t\tif (sg_dma_is_bus_address(tmp)) {\n\t\t\tsg_dma_unmark_bus_address(tmp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sg_dma_len(tmp) == 0)\n\t\t\tbreak;\n\n\t\tstart = sg_dma_address(tmp);\n\t\tbreak;\n\t}\n\n\tnents -= i;\n\tfor_each_sg(tmp, tmp, nents, i) {\n\t\tif (sg_dma_is_bus_address(tmp)) {\n\t\t\tsg_dma_unmark_bus_address(tmp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sg_dma_len(tmp) == 0)\n\t\t\tbreak;\n\n\t\tend = sg_dma_address(tmp) + sg_dma_len(tmp);\n\t}\n\n\tif (end)\n\t\t__iommu_dma_unmap(dev, start, end - start);\n}\n\nstatic dma_addr_t iommu_dma_map_resource(struct device *dev, phys_addr_t phys,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\treturn __iommu_dma_map(dev, phys, size,\n\t\t\tdma_info_to_prot(dir, false, attrs) | IOMMU_MMIO,\n\t\t\tdma_get_mask(dev));\n}\n\nstatic void iommu_dma_unmap_resource(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\t__iommu_dma_unmap(dev, handle, size);\n}\n\nstatic void __iommu_dma_free(struct device *dev, size_t size, void *cpu_addr)\n{\n\tsize_t alloc_size = PAGE_ALIGN(size);\n\tint count = alloc_size >> PAGE_SHIFT;\n\tstruct page *page = NULL, **pages = NULL;\n\n\t \n\tif (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&\n\t    dma_free_from_pool(dev, cpu_addr, alloc_size))\n\t\treturn;\n\n\tif (is_vmalloc_addr(cpu_addr)) {\n\t\t \n\t\tpages = dma_common_find_pages(cpu_addr);\n\t\tif (!pages)\n\t\t\tpage = vmalloc_to_page(cpu_addr);\n\t\tdma_common_free_remap(cpu_addr, alloc_size);\n\t} else {\n\t\t \n\t\tpage = virt_to_page(cpu_addr);\n\t}\n\n\tif (pages)\n\t\t__iommu_dma_free_pages(pages, count);\n\tif (page)\n\t\tdma_free_contiguous(dev, page, alloc_size);\n}\n\nstatic void iommu_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\tdma_addr_t handle, unsigned long attrs)\n{\n\t__iommu_dma_unmap(dev, handle, size);\n\t__iommu_dma_free(dev, size, cpu_addr);\n}\n\nstatic void *iommu_dma_alloc_pages(struct device *dev, size_t size,\n\t\tstruct page **pagep, gfp_t gfp, unsigned long attrs)\n{\n\tbool coherent = dev_is_dma_coherent(dev);\n\tsize_t alloc_size = PAGE_ALIGN(size);\n\tint node = dev_to_node(dev);\n\tstruct page *page = NULL;\n\tvoid *cpu_addr;\n\n\tpage = dma_alloc_contiguous(dev, alloc_size, gfp);\n\tif (!page)\n\t\tpage = alloc_pages_node(node, gfp, get_order(alloc_size));\n\tif (!page)\n\t\treturn NULL;\n\n\tif (!coherent || PageHighMem(page)) {\n\t\tpgprot_t prot = dma_pgprot(dev, PAGE_KERNEL, attrs);\n\n\t\tcpu_addr = dma_common_contiguous_remap(page, alloc_size,\n\t\t\t\tprot, __builtin_return_address(0));\n\t\tif (!cpu_addr)\n\t\t\tgoto out_free_pages;\n\n\t\tif (!coherent)\n\t\t\tarch_dma_prep_coherent(page, size);\n\t} else {\n\t\tcpu_addr = page_address(page);\n\t}\n\n\t*pagep = page;\n\tmemset(cpu_addr, 0, alloc_size);\n\treturn cpu_addr;\nout_free_pages:\n\tdma_free_contiguous(dev, page, alloc_size);\n\treturn NULL;\n}\n\nstatic void *iommu_dma_alloc(struct device *dev, size_t size,\n\t\tdma_addr_t *handle, gfp_t gfp, unsigned long attrs)\n{\n\tbool coherent = dev_is_dma_coherent(dev);\n\tint ioprot = dma_info_to_prot(DMA_BIDIRECTIONAL, coherent, attrs);\n\tstruct page *page = NULL;\n\tvoid *cpu_addr;\n\n\tgfp |= __GFP_ZERO;\n\n\tif (gfpflags_allow_blocking(gfp) &&\n\t    !(attrs & DMA_ATTR_FORCE_CONTIGUOUS)) {\n\t\treturn iommu_dma_alloc_remap(dev, size, handle, gfp,\n\t\t\t\tdma_pgprot(dev, PAGE_KERNEL, attrs), attrs);\n\t}\n\n\tif (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&\n\t    !gfpflags_allow_blocking(gfp) && !coherent)\n\t\tpage = dma_alloc_from_pool(dev, PAGE_ALIGN(size), &cpu_addr,\n\t\t\t\t\t       gfp, NULL);\n\telse\n\t\tcpu_addr = iommu_dma_alloc_pages(dev, size, &page, gfp, attrs);\n\tif (!cpu_addr)\n\t\treturn NULL;\n\n\t*handle = __iommu_dma_map(dev, page_to_phys(page), size, ioprot,\n\t\t\tdev->coherent_dma_mask);\n\tif (*handle == DMA_MAPPING_ERROR) {\n\t\t__iommu_dma_free(dev, size, cpu_addr);\n\t\treturn NULL;\n\t}\n\n\treturn cpu_addr;\n}\n\nstatic int iommu_dma_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\tunsigned long attrs)\n{\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn, off = vma->vm_pgoff;\n\tint ret;\n\n\tvma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);\n\n\tif (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off >= nr_pages || vma_pages(vma) > nr_pages - off)\n\t\treturn -ENXIO;\n\n\tif (is_vmalloc_addr(cpu_addr)) {\n\t\tstruct page **pages = dma_common_find_pages(cpu_addr);\n\n\t\tif (pages)\n\t\t\treturn vm_map_pages(vma, pages, nr_pages);\n\t\tpfn = vmalloc_to_pfn(cpu_addr);\n\t} else {\n\t\tpfn = page_to_pfn(virt_to_page(cpu_addr));\n\t}\n\n\treturn remap_pfn_range(vma, vma->vm_start, pfn + off,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot);\n}\n\nstatic int iommu_dma_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\tunsigned long attrs)\n{\n\tstruct page *page;\n\tint ret;\n\n\tif (is_vmalloc_addr(cpu_addr)) {\n\t\tstruct page **pages = dma_common_find_pages(cpu_addr);\n\n\t\tif (pages) {\n\t\t\treturn sg_alloc_table_from_pages(sgt, pages,\n\t\t\t\t\tPAGE_ALIGN(size) >> PAGE_SHIFT,\n\t\t\t\t\t0, size, GFP_KERNEL);\n\t\t}\n\n\t\tpage = vmalloc_to_page(cpu_addr);\n\t} else {\n\t\tpage = virt_to_page(cpu_addr);\n\t}\n\n\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);\n\tif (!ret)\n\t\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);\n\treturn ret;\n}\n\nstatic unsigned long iommu_dma_get_merge_boundary(struct device *dev)\n{\n\tstruct iommu_domain *domain = iommu_get_dma_domain(dev);\n\n\treturn (1UL << __ffs(domain->pgsize_bitmap)) - 1;\n}\n\nstatic size_t iommu_dma_opt_mapping_size(void)\n{\n\treturn iova_rcache_range();\n}\n\nstatic const struct dma_map_ops iommu_dma_ops = {\n\t.flags\t\t\t= DMA_F_PCI_P2PDMA_SUPPORTED,\n\t.alloc\t\t\t= iommu_dma_alloc,\n\t.free\t\t\t= iommu_dma_free,\n\t.alloc_pages\t\t= dma_common_alloc_pages,\n\t.free_pages\t\t= dma_common_free_pages,\n\t.alloc_noncontiguous\t= iommu_dma_alloc_noncontiguous,\n\t.free_noncontiguous\t= iommu_dma_free_noncontiguous,\n\t.mmap\t\t\t= iommu_dma_mmap,\n\t.get_sgtable\t\t= iommu_dma_get_sgtable,\n\t.map_page\t\t= iommu_dma_map_page,\n\t.unmap_page\t\t= iommu_dma_unmap_page,\n\t.map_sg\t\t\t= iommu_dma_map_sg,\n\t.unmap_sg\t\t= iommu_dma_unmap_sg,\n\t.sync_single_for_cpu\t= iommu_dma_sync_single_for_cpu,\n\t.sync_single_for_device\t= iommu_dma_sync_single_for_device,\n\t.sync_sg_for_cpu\t= iommu_dma_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= iommu_dma_sync_sg_for_device,\n\t.map_resource\t\t= iommu_dma_map_resource,\n\t.unmap_resource\t\t= iommu_dma_unmap_resource,\n\t.get_merge_boundary\t= iommu_dma_get_merge_boundary,\n\t.opt_mapping_size\t= iommu_dma_opt_mapping_size,\n};\n\n \nvoid iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 dma_limit)\n{\n\tstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\n\tif (!domain)\n\t\tgoto out_err;\n\n\t \n\tif (iommu_is_dma_domain(domain)) {\n\t\tif (iommu_dma_init_domain(domain, dma_base, dma_limit, dev))\n\t\t\tgoto out_err;\n\t\tdev->dma_ops = &iommu_dma_ops;\n\t}\n\n\treturn;\nout_err:\n\t pr_warn(\"Failed to set up IOMMU for device %s; retaining platform DMA ops\\n\",\n\t\t dev_name(dev));\n}\nEXPORT_SYMBOL_GPL(iommu_setup_dma_ops);\n\nstatic struct iommu_dma_msi_page *iommu_dma_get_msi_page(struct device *dev,\n\t\tphys_addr_t msi_addr, struct iommu_domain *domain)\n{\n\tstruct iommu_dma_cookie *cookie = domain->iova_cookie;\n\tstruct iommu_dma_msi_page *msi_page;\n\tdma_addr_t iova;\n\tint prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\tsize_t size = cookie_msi_granule(cookie);\n\n\tmsi_addr &= ~(phys_addr_t)(size - 1);\n\tlist_for_each_entry(msi_page, &cookie->msi_page_list, list)\n\t\tif (msi_page->phys == msi_addr)\n\t\t\treturn msi_page;\n\n\tmsi_page = kzalloc(sizeof(*msi_page), GFP_KERNEL);\n\tif (!msi_page)\n\t\treturn NULL;\n\n\tiova = iommu_dma_alloc_iova(domain, size, dma_get_mask(dev), dev);\n\tif (!iova)\n\t\tgoto out_free_page;\n\n\tif (iommu_map(domain, iova, msi_addr, size, prot, GFP_KERNEL))\n\t\tgoto out_free_iova;\n\n\tINIT_LIST_HEAD(&msi_page->list);\n\tmsi_page->phys = msi_addr;\n\tmsi_page->iova = iova;\n\tlist_add(&msi_page->list, &cookie->msi_page_list);\n\treturn msi_page;\n\nout_free_iova:\n\tiommu_dma_free_iova(cookie, iova, size, NULL);\nout_free_page:\n\tkfree(msi_page);\n\treturn NULL;\n}\n\n \nint iommu_dma_prepare_msi(struct msi_desc *desc, phys_addr_t msi_addr)\n{\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\tstruct iommu_dma_msi_page *msi_page;\n\tstatic DEFINE_MUTEX(msi_prepare_lock);  \n\n\tif (!domain || !domain->iova_cookie) {\n\t\tdesc->iommu_cookie = NULL;\n\t\treturn 0;\n\t}\n\n\t \n\tmutex_lock(&msi_prepare_lock);\n\tmsi_page = iommu_dma_get_msi_page(dev, msi_addr, domain);\n\tmutex_unlock(&msi_prepare_lock);\n\n\tmsi_desc_set_iommu_cookie(desc, msi_page);\n\n\tif (!msi_page)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n \nvoid iommu_dma_compose_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tconst struct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\tconst struct iommu_dma_msi_page *msi_page;\n\n\tmsi_page = msi_desc_get_iommu_cookie(desc);\n\n\tif (!domain || !domain->iova_cookie || WARN_ON(!msi_page))\n\t\treturn;\n\n\tmsg->address_hi = upper_32_bits(msi_page->iova);\n\tmsg->address_lo &= cookie_msi_granule(domain->iova_cookie) - 1;\n\tmsg->address_lo += lower_32_bits(msi_page->iova);\n}\n\nstatic int iommu_dma_init(void)\n{\n\tif (is_kdump_kernel())\n\t\tstatic_branch_enable(&iommu_deferred_attach_enabled);\n\n\treturn iova_cache_get();\n}\narch_initcall(iommu_dma_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}