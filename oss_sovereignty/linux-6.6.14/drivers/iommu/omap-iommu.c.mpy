{
  "module_name": "omap-iommu.c",
  "hash_id": "fb71113bc2dc0564934cc3a81ac82d3fb46248f376ea30bb54d6041dc882ba4e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/omap-iommu.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/ioport.h>\n#include <linux/platform_device.h>\n#include <linux/iommu.h>\n#include <linux/omap-iommu.h>\n#include <linux/mutex.h>\n#include <linux/spinlock.h>\n#include <linux/io.h>\n#include <linux/pm_runtime.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/of_platform.h>\n#include <linux/regmap.h>\n#include <linux/mfd/syscon.h>\n\n#include <linux/platform_data/iommu-omap.h>\n\n#include \"omap-iopgtable.h\"\n#include \"omap-iommu.h\"\n\nstatic const struct iommu_ops omap_iommu_ops;\n\n#define to_iommu(dev)\t((struct omap_iommu *)dev_get_drvdata(dev))\n\n \n#define OMAP_IOMMU_PGSIZES\t(SZ_4K | SZ_64K | SZ_1M | SZ_16M)\n\n#define MMU_LOCK_BASE_SHIFT\t10\n#define MMU_LOCK_BASE_MASK\t(0x1f << MMU_LOCK_BASE_SHIFT)\n#define MMU_LOCK_BASE(x)\t\\\n\t((x & MMU_LOCK_BASE_MASK) >> MMU_LOCK_BASE_SHIFT)\n\n#define MMU_LOCK_VICT_SHIFT\t4\n#define MMU_LOCK_VICT_MASK\t(0x1f << MMU_LOCK_VICT_SHIFT)\n#define MMU_LOCK_VICT(x)\t\\\n\t((x & MMU_LOCK_VICT_MASK) >> MMU_LOCK_VICT_SHIFT)\n\nstatic struct platform_driver omap_iommu_driver;\nstatic struct kmem_cache *iopte_cachep;\n\n \nstatic struct omap_iommu_domain *to_omap_domain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct omap_iommu_domain, domain);\n}\n\n \nvoid omap_iommu_save_ctx(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tstruct omap_iommu *obj;\n\tu32 *p;\n\tint i;\n\n\tif (!arch_data)\n\t\treturn;\n\n\twhile (arch_data->iommu_dev) {\n\t\tobj = arch_data->iommu_dev;\n\t\tp = obj->ctx;\n\t\tfor (i = 0; i < (MMU_REG_SIZE / sizeof(u32)); i++) {\n\t\t\tp[i] = iommu_read_reg(obj, i * sizeof(u32));\n\t\t\tdev_dbg(obj->dev, \"%s\\t[%02d] %08x\\n\", __func__, i,\n\t\t\t\tp[i]);\n\t\t}\n\t\tarch_data++;\n\t}\n}\nEXPORT_SYMBOL_GPL(omap_iommu_save_ctx);\n\n \nvoid omap_iommu_restore_ctx(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tstruct omap_iommu *obj;\n\tu32 *p;\n\tint i;\n\n\tif (!arch_data)\n\t\treturn;\n\n\twhile (arch_data->iommu_dev) {\n\t\tobj = arch_data->iommu_dev;\n\t\tp = obj->ctx;\n\t\tfor (i = 0; i < (MMU_REG_SIZE / sizeof(u32)); i++) {\n\t\t\tiommu_write_reg(obj, p[i], i * sizeof(u32));\n\t\t\tdev_dbg(obj->dev, \"%s\\t[%02d] %08x\\n\", __func__, i,\n\t\t\t\tp[i]);\n\t\t}\n\t\tarch_data++;\n\t}\n}\nEXPORT_SYMBOL_GPL(omap_iommu_restore_ctx);\n\nstatic void dra7_cfg_dspsys_mmu(struct omap_iommu *obj, bool enable)\n{\n\tu32 val, mask;\n\n\tif (!obj->syscfg)\n\t\treturn;\n\n\tmask = (1 << (obj->id * DSP_SYS_MMU_CONFIG_EN_SHIFT));\n\tval = enable ? mask : 0;\n\tregmap_update_bits(obj->syscfg, DSP_SYS_MMU_CONFIG, mask, val);\n}\n\nstatic void __iommu_set_twl(struct omap_iommu *obj, bool on)\n{\n\tu32 l = iommu_read_reg(obj, MMU_CNTL);\n\n\tif (on)\n\t\tiommu_write_reg(obj, MMU_IRQ_TWL_MASK, MMU_IRQENABLE);\n\telse\n\t\tiommu_write_reg(obj, MMU_IRQ_TLB_MISS_MASK, MMU_IRQENABLE);\n\n\tl &= ~MMU_CNTL_MASK;\n\tif (on)\n\t\tl |= (MMU_CNTL_MMU_EN | MMU_CNTL_TWL_EN);\n\telse\n\t\tl |= (MMU_CNTL_MMU_EN);\n\n\tiommu_write_reg(obj, l, MMU_CNTL);\n}\n\nstatic int omap2_iommu_enable(struct omap_iommu *obj)\n{\n\tu32 l, pa;\n\n\tif (!obj->iopgd || !IS_ALIGNED((unsigned long)obj->iopgd,  SZ_16K))\n\t\treturn -EINVAL;\n\n\tpa = virt_to_phys(obj->iopgd);\n\tif (!IS_ALIGNED(pa, SZ_16K))\n\t\treturn -EINVAL;\n\n\tl = iommu_read_reg(obj, MMU_REVISION);\n\tdev_info(obj->dev, \"%s: version %d.%d\\n\", obj->name,\n\t\t (l >> 4) & 0xf, l & 0xf);\n\n\tiommu_write_reg(obj, pa, MMU_TTB);\n\n\tdra7_cfg_dspsys_mmu(obj, true);\n\n\tif (obj->has_bus_err_back)\n\t\tiommu_write_reg(obj, MMU_GP_REG_BUS_ERR_BACK_EN, MMU_GP_REG);\n\n\t__iommu_set_twl(obj, true);\n\n\treturn 0;\n}\n\nstatic void omap2_iommu_disable(struct omap_iommu *obj)\n{\n\tu32 l = iommu_read_reg(obj, MMU_CNTL);\n\n\tl &= ~MMU_CNTL_MASK;\n\tiommu_write_reg(obj, l, MMU_CNTL);\n\tdra7_cfg_dspsys_mmu(obj, false);\n\n\tdev_dbg(obj->dev, \"%s is shutting down\\n\", obj->name);\n}\n\nstatic int iommu_enable(struct omap_iommu *obj)\n{\n\tint ret;\n\n\tret = pm_runtime_get_sync(obj->dev);\n\tif (ret < 0)\n\t\tpm_runtime_put_noidle(obj->dev);\n\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic void iommu_disable(struct omap_iommu *obj)\n{\n\tpm_runtime_put_sync(obj->dev);\n}\n\n \nstatic u32 iotlb_cr_to_virt(struct cr_regs *cr)\n{\n\tu32 page_size = cr->cam & MMU_CAM_PGSZ_MASK;\n\tu32 mask = get_cam_va_mask(cr->cam & page_size);\n\n\treturn cr->cam & mask;\n}\n\nstatic u32 get_iopte_attr(struct iotlb_entry *e)\n{\n\tu32 attr;\n\n\tattr = e->mixed << 5;\n\tattr |= e->endian;\n\tattr |= e->elsz >> 3;\n\tattr <<= (((e->pgsz == MMU_CAM_PGSZ_4K) ||\n\t\t\t(e->pgsz == MMU_CAM_PGSZ_64K)) ? 0 : 6);\n\treturn attr;\n}\n\nstatic u32 iommu_report_fault(struct omap_iommu *obj, u32 *da)\n{\n\tu32 status, fault_addr;\n\n\tstatus = iommu_read_reg(obj, MMU_IRQSTATUS);\n\tstatus &= MMU_IRQ_MASK;\n\tif (!status) {\n\t\t*da = 0;\n\t\treturn 0;\n\t}\n\n\tfault_addr = iommu_read_reg(obj, MMU_FAULT_AD);\n\t*da = fault_addr;\n\n\tiommu_write_reg(obj, status, MMU_IRQSTATUS);\n\n\treturn status;\n}\n\nvoid iotlb_lock_get(struct omap_iommu *obj, struct iotlb_lock *l)\n{\n\tu32 val;\n\n\tval = iommu_read_reg(obj, MMU_LOCK);\n\n\tl->base = MMU_LOCK_BASE(val);\n\tl->vict = MMU_LOCK_VICT(val);\n}\n\nvoid iotlb_lock_set(struct omap_iommu *obj, struct iotlb_lock *l)\n{\n\tu32 val;\n\n\tval = (l->base << MMU_LOCK_BASE_SHIFT);\n\tval |= (l->vict << MMU_LOCK_VICT_SHIFT);\n\n\tiommu_write_reg(obj, val, MMU_LOCK);\n}\n\nstatic void iotlb_read_cr(struct omap_iommu *obj, struct cr_regs *cr)\n{\n\tcr->cam = iommu_read_reg(obj, MMU_READ_CAM);\n\tcr->ram = iommu_read_reg(obj, MMU_READ_RAM);\n}\n\nstatic void iotlb_load_cr(struct omap_iommu *obj, struct cr_regs *cr)\n{\n\tiommu_write_reg(obj, cr->cam | MMU_CAM_V, MMU_CAM);\n\tiommu_write_reg(obj, cr->ram, MMU_RAM);\n\n\tiommu_write_reg(obj, 1, MMU_FLUSH_ENTRY);\n\tiommu_write_reg(obj, 1, MMU_LD_TLB);\n}\n\n \nstruct cr_regs __iotlb_read_cr(struct omap_iommu *obj, int n)\n{\n\tstruct cr_regs cr;\n\tstruct iotlb_lock l;\n\n\tiotlb_lock_get(obj, &l);\n\tl.vict = n;\n\tiotlb_lock_set(obj, &l);\n\tiotlb_read_cr(obj, &cr);\n\n\treturn cr;\n}\n\n#ifdef PREFETCH_IOTLB\nstatic struct cr_regs *iotlb_alloc_cr(struct omap_iommu *obj,\n\t\t\t\t      struct iotlb_entry *e)\n{\n\tstruct cr_regs *cr;\n\n\tif (!e)\n\t\treturn NULL;\n\n\tif (e->da & ~(get_cam_va_mask(e->pgsz))) {\n\t\tdev_err(obj->dev, \"%s:\\twrong alignment: %08x\\n\", __func__,\n\t\t\te->da);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tcr = kmalloc(sizeof(*cr), GFP_KERNEL);\n\tif (!cr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcr->cam = (e->da & MMU_CAM_VATAG_MASK) | e->prsvd | e->pgsz | e->valid;\n\tcr->ram = e->pa | e->endian | e->elsz | e->mixed;\n\n\treturn cr;\n}\n\n \nstatic int load_iotlb_entry(struct omap_iommu *obj, struct iotlb_entry *e)\n{\n\tint err = 0;\n\tstruct iotlb_lock l;\n\tstruct cr_regs *cr;\n\n\tif (!obj || !obj->nr_tlb_entries || !e)\n\t\treturn -EINVAL;\n\n\tpm_runtime_get_sync(obj->dev);\n\n\tiotlb_lock_get(obj, &l);\n\tif (l.base == obj->nr_tlb_entries) {\n\t\tdev_warn(obj->dev, \"%s: preserve entries full\\n\", __func__);\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\tif (!e->prsvd) {\n\t\tint i;\n\t\tstruct cr_regs tmp;\n\n\t\tfor_each_iotlb_cr(obj, obj->nr_tlb_entries, i, tmp)\n\t\t\tif (!iotlb_cr_valid(&tmp))\n\t\t\t\tbreak;\n\n\t\tif (i == obj->nr_tlb_entries) {\n\t\t\tdev_dbg(obj->dev, \"%s: full: no entry\\n\", __func__);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\tiotlb_lock_get(obj, &l);\n\t} else {\n\t\tl.vict = l.base;\n\t\tiotlb_lock_set(obj, &l);\n\t}\n\n\tcr = iotlb_alloc_cr(obj, e);\n\tif (IS_ERR(cr)) {\n\t\tpm_runtime_put_sync(obj->dev);\n\t\treturn PTR_ERR(cr);\n\t}\n\n\tiotlb_load_cr(obj, cr);\n\tkfree(cr);\n\n\tif (e->prsvd)\n\t\tl.base++;\n\t \n\tif (++l.vict == obj->nr_tlb_entries)\n\t\tl.vict = l.base;\n\tiotlb_lock_set(obj, &l);\nout:\n\tpm_runtime_put_sync(obj->dev);\n\treturn err;\n}\n\n#else  \n\nstatic int load_iotlb_entry(struct omap_iommu *obj, struct iotlb_entry *e)\n{\n\treturn 0;\n}\n\n#endif  \n\nstatic int prefetch_iotlb_entry(struct omap_iommu *obj, struct iotlb_entry *e)\n{\n\treturn load_iotlb_entry(obj, e);\n}\n\n \nstatic void flush_iotlb_page(struct omap_iommu *obj, u32 da)\n{\n\tint i;\n\tstruct cr_regs cr;\n\n\tpm_runtime_get_sync(obj->dev);\n\n\tfor_each_iotlb_cr(obj, obj->nr_tlb_entries, i, cr) {\n\t\tu32 start;\n\t\tsize_t bytes;\n\n\t\tif (!iotlb_cr_valid(&cr))\n\t\t\tcontinue;\n\n\t\tstart = iotlb_cr_to_virt(&cr);\n\t\tbytes = iopgsz_to_bytes(cr.cam & 3);\n\n\t\tif ((start <= da) && (da < start + bytes)) {\n\t\t\tdev_dbg(obj->dev, \"%s: %08x<=%08x(%zx)\\n\",\n\t\t\t\t__func__, start, da, bytes);\n\t\t\tiotlb_load_cr(obj, &cr);\n\t\t\tiommu_write_reg(obj, 1, MMU_FLUSH_ENTRY);\n\t\t\tbreak;\n\t\t}\n\t}\n\tpm_runtime_put_sync(obj->dev);\n\n\tif (i == obj->nr_tlb_entries)\n\t\tdev_dbg(obj->dev, \"%s: no page for %08x\\n\", __func__, da);\n}\n\n \nstatic void flush_iotlb_all(struct omap_iommu *obj)\n{\n\tstruct iotlb_lock l;\n\n\tpm_runtime_get_sync(obj->dev);\n\n\tl.base = 0;\n\tl.vict = 0;\n\tiotlb_lock_set(obj, &l);\n\n\tiommu_write_reg(obj, 1, MMU_GFLUSH);\n\n\tpm_runtime_put_sync(obj->dev);\n}\n\n \nstatic void flush_iopte_range(struct device *dev, dma_addr_t dma,\n\t\t\t      unsigned long offset, int num_entries)\n{\n\tsize_t size = num_entries * sizeof(u32);\n\n\tdma_sync_single_range_for_device(dev, dma, offset, size, DMA_TO_DEVICE);\n}\n\nstatic void iopte_free(struct omap_iommu *obj, u32 *iopte, bool dma_valid)\n{\n\tdma_addr_t pt_dma;\n\n\t \n\tif (iopte) {\n\t\tif (dma_valid) {\n\t\t\tpt_dma = virt_to_phys(iopte);\n\t\t\tdma_unmap_single(obj->dev, pt_dma, IOPTE_TABLE_SIZE,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t}\n\n\t\tkmem_cache_free(iopte_cachep, iopte);\n\t}\n}\n\nstatic u32 *iopte_alloc(struct omap_iommu *obj, u32 *iopgd,\n\t\t\tdma_addr_t *pt_dma, u32 da)\n{\n\tu32 *iopte;\n\tunsigned long offset = iopgd_index(da) * sizeof(da);\n\n\t \n\tif (*iopgd)\n\t\tgoto pte_ready;\n\n\t \n\tspin_unlock(&obj->page_table_lock);\n\tiopte = kmem_cache_zalloc(iopte_cachep, GFP_KERNEL);\n\tspin_lock(&obj->page_table_lock);\n\n\tif (!*iopgd) {\n\t\tif (!iopte)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\n\t\t*pt_dma = dma_map_single(obj->dev, iopte, IOPTE_TABLE_SIZE,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(obj->dev, *pt_dma)) {\n\t\t\tdev_err(obj->dev, \"DMA map error for L2 table\\n\");\n\t\t\tiopte_free(obj, iopte, false);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\t \n\t\tif (WARN_ON(*pt_dma != virt_to_phys(iopte))) {\n\t\t\tdev_err(obj->dev, \"DMA translation error for L2 table\\n\");\n\t\t\tdma_unmap_single(obj->dev, *pt_dma, IOPTE_TABLE_SIZE,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tiopte_free(obj, iopte, false);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\t*iopgd = virt_to_phys(iopte) | IOPGD_TABLE;\n\n\t\tflush_iopte_range(obj->dev, obj->pd_dma, offset, 1);\n\t\tdev_vdbg(obj->dev, \"%s: a new pte:%p\\n\", __func__, iopte);\n\t} else {\n\t\t \n\t\tiopte_free(obj, iopte, false);\n\t}\n\npte_ready:\n\tiopte = iopte_offset(iopgd, da);\n\t*pt_dma = iopgd_page_paddr(iopgd);\n\tdev_vdbg(obj->dev,\n\t\t \"%s: da:%08x pgd:%p *pgd:%08x pte:%p *pte:%08x\\n\",\n\t\t __func__, da, iopgd, *iopgd, iopte, *iopte);\n\n\treturn iopte;\n}\n\nstatic int iopgd_alloc_section(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)\n{\n\tu32 *iopgd = iopgd_offset(obj, da);\n\tunsigned long offset = iopgd_index(da) * sizeof(da);\n\n\tif ((da | pa) & ~IOSECTION_MASK) {\n\t\tdev_err(obj->dev, \"%s: %08x:%08x should aligned on %08lx\\n\",\n\t\t\t__func__, da, pa, IOSECTION_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\t*iopgd = (pa & IOSECTION_MASK) | prot | IOPGD_SECTION;\n\tflush_iopte_range(obj->dev, obj->pd_dma, offset, 1);\n\treturn 0;\n}\n\nstatic int iopgd_alloc_super(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)\n{\n\tu32 *iopgd = iopgd_offset(obj, da);\n\tunsigned long offset = iopgd_index(da) * sizeof(da);\n\tint i;\n\n\tif ((da | pa) & ~IOSUPER_MASK) {\n\t\tdev_err(obj->dev, \"%s: %08x:%08x should aligned on %08lx\\n\",\n\t\t\t__func__, da, pa, IOSUPER_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < 16; i++)\n\t\t*(iopgd + i) = (pa & IOSUPER_MASK) | prot | IOPGD_SUPER;\n\tflush_iopte_range(obj->dev, obj->pd_dma, offset, 16);\n\treturn 0;\n}\n\nstatic int iopte_alloc_page(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)\n{\n\tu32 *iopgd = iopgd_offset(obj, da);\n\tdma_addr_t pt_dma;\n\tu32 *iopte = iopte_alloc(obj, iopgd, &pt_dma, da);\n\tunsigned long offset = iopte_index(da) * sizeof(da);\n\n\tif (IS_ERR(iopte))\n\t\treturn PTR_ERR(iopte);\n\n\t*iopte = (pa & IOPAGE_MASK) | prot | IOPTE_SMALL;\n\tflush_iopte_range(obj->dev, pt_dma, offset, 1);\n\n\tdev_vdbg(obj->dev, \"%s: da:%08x pa:%08x pte:%p *pte:%08x\\n\",\n\t\t __func__, da, pa, iopte, *iopte);\n\n\treturn 0;\n}\n\nstatic int iopte_alloc_large(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)\n{\n\tu32 *iopgd = iopgd_offset(obj, da);\n\tdma_addr_t pt_dma;\n\tu32 *iopte = iopte_alloc(obj, iopgd, &pt_dma, da);\n\tunsigned long offset = iopte_index(da) * sizeof(da);\n\tint i;\n\n\tif ((da | pa) & ~IOLARGE_MASK) {\n\t\tdev_err(obj->dev, \"%s: %08x:%08x should aligned on %08lx\\n\",\n\t\t\t__func__, da, pa, IOLARGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_ERR(iopte))\n\t\treturn PTR_ERR(iopte);\n\n\tfor (i = 0; i < 16; i++)\n\t\t*(iopte + i) = (pa & IOLARGE_MASK) | prot | IOPTE_LARGE;\n\tflush_iopte_range(obj->dev, pt_dma, offset, 16);\n\treturn 0;\n}\n\nstatic int\niopgtable_store_entry_core(struct omap_iommu *obj, struct iotlb_entry *e)\n{\n\tint (*fn)(struct omap_iommu *, u32, u32, u32);\n\tu32 prot;\n\tint err;\n\n\tif (!obj || !e)\n\t\treturn -EINVAL;\n\n\tswitch (e->pgsz) {\n\tcase MMU_CAM_PGSZ_16M:\n\t\tfn = iopgd_alloc_super;\n\t\tbreak;\n\tcase MMU_CAM_PGSZ_1M:\n\t\tfn = iopgd_alloc_section;\n\t\tbreak;\n\tcase MMU_CAM_PGSZ_64K:\n\t\tfn = iopte_alloc_large;\n\t\tbreak;\n\tcase MMU_CAM_PGSZ_4K:\n\t\tfn = iopte_alloc_page;\n\t\tbreak;\n\tdefault:\n\t\tfn = NULL;\n\t\tbreak;\n\t}\n\n\tif (WARN_ON(!fn))\n\t\treturn -EINVAL;\n\n\tprot = get_iopte_attr(e);\n\n\tspin_lock(&obj->page_table_lock);\n\terr = fn(obj, e->da, e->pa, prot);\n\tspin_unlock(&obj->page_table_lock);\n\n\treturn err;\n}\n\n \nstatic int\nomap_iopgtable_store_entry(struct omap_iommu *obj, struct iotlb_entry *e)\n{\n\tint err;\n\n\tflush_iotlb_page(obj, e->da);\n\terr = iopgtable_store_entry_core(obj, e);\n\tif (!err)\n\t\tprefetch_iotlb_entry(obj, e);\n\treturn err;\n}\n\n \nstatic void\niopgtable_lookup_entry(struct omap_iommu *obj, u32 da, u32 **ppgd, u32 **ppte)\n{\n\tu32 *iopgd, *iopte = NULL;\n\n\tiopgd = iopgd_offset(obj, da);\n\tif (!*iopgd)\n\t\tgoto out;\n\n\tif (iopgd_is_table(*iopgd))\n\t\tiopte = iopte_offset(iopgd, da);\nout:\n\t*ppgd = iopgd;\n\t*ppte = iopte;\n}\n\nstatic size_t iopgtable_clear_entry_core(struct omap_iommu *obj, u32 da)\n{\n\tsize_t bytes;\n\tu32 *iopgd = iopgd_offset(obj, da);\n\tint nent = 1;\n\tdma_addr_t pt_dma;\n\tunsigned long pd_offset = iopgd_index(da) * sizeof(da);\n\tunsigned long pt_offset = iopte_index(da) * sizeof(da);\n\n\tif (!*iopgd)\n\t\treturn 0;\n\n\tif (iopgd_is_table(*iopgd)) {\n\t\tint i;\n\t\tu32 *iopte = iopte_offset(iopgd, da);\n\n\t\tbytes = IOPTE_SIZE;\n\t\tif (*iopte & IOPTE_LARGE) {\n\t\t\tnent *= 16;\n\t\t\t \n\t\t\tiopte = iopte_offset(iopgd, (da & IOLARGE_MASK));\n\t\t}\n\t\tbytes *= nent;\n\t\tmemset(iopte, 0, nent * sizeof(*iopte));\n\t\tpt_dma = iopgd_page_paddr(iopgd);\n\t\tflush_iopte_range(obj->dev, pt_dma, pt_offset, nent);\n\n\t\t \n\t\tiopte = iopte_offset(iopgd, 0);\n\t\tfor (i = 0; i < PTRS_PER_IOPTE; i++)\n\t\t\tif (iopte[i])\n\t\t\t\tgoto out;\n\n\t\tiopte_free(obj, iopte, true);\n\t\tnent = 1;  \n\t} else {\n\t\tbytes = IOPGD_SIZE;\n\t\tif ((*iopgd & IOPGD_SUPER) == IOPGD_SUPER) {\n\t\t\tnent *= 16;\n\t\t\t \n\t\t\tiopgd = iopgd_offset(obj, (da & IOSUPER_MASK));\n\t\t}\n\t\tbytes *= nent;\n\t}\n\tmemset(iopgd, 0, nent * sizeof(*iopgd));\n\tflush_iopte_range(obj->dev, obj->pd_dma, pd_offset, nent);\nout:\n\treturn bytes;\n}\n\n \nstatic size_t iopgtable_clear_entry(struct omap_iommu *obj, u32 da)\n{\n\tsize_t bytes;\n\n\tspin_lock(&obj->page_table_lock);\n\n\tbytes = iopgtable_clear_entry_core(obj, da);\n\tflush_iotlb_page(obj, da);\n\n\tspin_unlock(&obj->page_table_lock);\n\n\treturn bytes;\n}\n\nstatic void iopgtable_clear_entry_all(struct omap_iommu *obj)\n{\n\tunsigned long offset;\n\tint i;\n\n\tspin_lock(&obj->page_table_lock);\n\n\tfor (i = 0; i < PTRS_PER_IOPGD; i++) {\n\t\tu32 da;\n\t\tu32 *iopgd;\n\n\t\tda = i << IOPGD_SHIFT;\n\t\tiopgd = iopgd_offset(obj, da);\n\t\toffset = iopgd_index(da) * sizeof(da);\n\n\t\tif (!*iopgd)\n\t\t\tcontinue;\n\n\t\tif (iopgd_is_table(*iopgd))\n\t\t\tiopte_free(obj, iopte_offset(iopgd, 0), true);\n\n\t\t*iopgd = 0;\n\t\tflush_iopte_range(obj->dev, obj->pd_dma, offset, 1);\n\t}\n\n\tflush_iotlb_all(obj);\n\n\tspin_unlock(&obj->page_table_lock);\n}\n\n \nstatic irqreturn_t iommu_fault_handler(int irq, void *data)\n{\n\tu32 da, errs;\n\tu32 *iopgd, *iopte;\n\tstruct omap_iommu *obj = data;\n\tstruct iommu_domain *domain = obj->domain;\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\n\tif (!omap_domain->dev)\n\t\treturn IRQ_NONE;\n\n\terrs = iommu_report_fault(obj, &da);\n\tif (errs == 0)\n\t\treturn IRQ_HANDLED;\n\n\t \n\tif (!report_iommu_fault(domain, obj->dev, da, 0))\n\t\treturn IRQ_HANDLED;\n\n\tiommu_write_reg(obj, 0, MMU_IRQENABLE);\n\n\tiopgd = iopgd_offset(obj, da);\n\n\tif (!iopgd_is_table(*iopgd)) {\n\t\tdev_err(obj->dev, \"%s: errs:0x%08x da:0x%08x pgd:0x%p *pgd:px%08x\\n\",\n\t\t\tobj->name, errs, da, iopgd, *iopgd);\n\t\treturn IRQ_NONE;\n\t}\n\n\tiopte = iopte_offset(iopgd, da);\n\n\tdev_err(obj->dev, \"%s: errs:0x%08x da:0x%08x pgd:0x%p *pgd:0x%08x pte:0x%p *pte:0x%08x\\n\",\n\t\tobj->name, errs, da, iopgd, *iopgd, iopte, *iopte);\n\n\treturn IRQ_NONE;\n}\n\n \nstatic int omap_iommu_attach(struct omap_iommu *obj, u32 *iopgd)\n{\n\tint err;\n\n\tspin_lock(&obj->iommu_lock);\n\n\tobj->pd_dma = dma_map_single(obj->dev, iopgd, IOPGD_TABLE_SIZE,\n\t\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(obj->dev, obj->pd_dma)) {\n\t\tdev_err(obj->dev, \"DMA map error for L1 table\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\tobj->iopgd = iopgd;\n\terr = iommu_enable(obj);\n\tif (err)\n\t\tgoto out_err;\n\tflush_iotlb_all(obj);\n\n\tspin_unlock(&obj->iommu_lock);\n\n\tdev_dbg(obj->dev, \"%s: %s\\n\", __func__, obj->name);\n\n\treturn 0;\n\nout_err:\n\tspin_unlock(&obj->iommu_lock);\n\n\treturn err;\n}\n\n \nstatic void omap_iommu_detach(struct omap_iommu *obj)\n{\n\tif (!obj || IS_ERR(obj))\n\t\treturn;\n\n\tspin_lock(&obj->iommu_lock);\n\n\tdma_unmap_single(obj->dev, obj->pd_dma, IOPGD_TABLE_SIZE,\n\t\t\t DMA_TO_DEVICE);\n\tobj->pd_dma = 0;\n\tobj->iopgd = NULL;\n\tiommu_disable(obj);\n\n\tspin_unlock(&obj->iommu_lock);\n\n\tdev_dbg(obj->dev, \"%s: %s\\n\", __func__, obj->name);\n}\n\nstatic void omap_iommu_save_tlb_entries(struct omap_iommu *obj)\n{\n\tstruct iotlb_lock lock;\n\tstruct cr_regs cr;\n\tstruct cr_regs *tmp;\n\tint i;\n\n\t \n\tiotlb_lock_get(obj, &lock);\n\tobj->num_cr_ctx = lock.base;\n\tif (!obj->num_cr_ctx)\n\t\treturn;\n\n\ttmp = obj->cr_ctx;\n\tfor_each_iotlb_cr(obj, obj->num_cr_ctx, i, cr)\n\t\t* tmp++ = cr;\n}\n\nstatic void omap_iommu_restore_tlb_entries(struct omap_iommu *obj)\n{\n\tstruct iotlb_lock l;\n\tstruct cr_regs *tmp;\n\tint i;\n\n\t \n\tif (!obj->num_cr_ctx)\n\t\treturn;\n\n\tl.base = 0;\n\ttmp = obj->cr_ctx;\n\tfor (i = 0; i < obj->num_cr_ctx; i++, tmp++) {\n\t\tl.vict = i;\n\t\tiotlb_lock_set(obj, &l);\n\t\tiotlb_load_cr(obj, tmp);\n\t}\n\tl.base = obj->num_cr_ctx;\n\tl.vict = i;\n\tiotlb_lock_set(obj, &l);\n}\n\n \nint omap_iommu_domain_deactivate(struct iommu_domain *domain)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct omap_iommu_device *iommu;\n\tstruct omap_iommu *oiommu;\n\tint i;\n\n\tif (!omap_domain->dev)\n\t\treturn 0;\n\n\tiommu = omap_domain->iommus;\n\tiommu += (omap_domain->num_iommus - 1);\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu--) {\n\t\toiommu = iommu->iommu_dev;\n\t\tpm_runtime_put_sync(oiommu->dev);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(omap_iommu_domain_deactivate);\n\n \nint omap_iommu_domain_activate(struct iommu_domain *domain)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct omap_iommu_device *iommu;\n\tstruct omap_iommu *oiommu;\n\tint i;\n\n\tif (!omap_domain->dev)\n\t\treturn 0;\n\n\tiommu = omap_domain->iommus;\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu++) {\n\t\toiommu = iommu->iommu_dev;\n\t\tpm_runtime_get_sync(oiommu->dev);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(omap_iommu_domain_activate);\n\n \nstatic __maybe_unused int omap_iommu_runtime_suspend(struct device *dev)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tstruct iommu_platform_data *pdata = dev_get_platdata(dev);\n\tstruct omap_iommu *obj = to_iommu(dev);\n\tint ret;\n\n\t \n\tif (obj->domain && obj->iopgd)\n\t\tomap_iommu_save_tlb_entries(obj);\n\n\tomap2_iommu_disable(obj);\n\n\tif (pdata && pdata->device_idle)\n\t\tpdata->device_idle(pdev);\n\n\tif (pdata && pdata->assert_reset)\n\t\tpdata->assert_reset(pdev, pdata->reset_name);\n\n\tif (pdata && pdata->set_pwrdm_constraint) {\n\t\tret = pdata->set_pwrdm_constraint(pdev, false, &obj->pwrst);\n\t\tif (ret) {\n\t\t\tdev_warn(obj->dev, \"pwrdm_constraint failed to be reset, status = %d\\n\",\n\t\t\t\t ret);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic __maybe_unused int omap_iommu_runtime_resume(struct device *dev)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tstruct iommu_platform_data *pdata = dev_get_platdata(dev);\n\tstruct omap_iommu *obj = to_iommu(dev);\n\tint ret = 0;\n\n\tif (pdata && pdata->set_pwrdm_constraint) {\n\t\tret = pdata->set_pwrdm_constraint(pdev, true, &obj->pwrst);\n\t\tif (ret) {\n\t\t\tdev_warn(obj->dev, \"pwrdm_constraint failed to be set, status = %d\\n\",\n\t\t\t\t ret);\n\t\t}\n\t}\n\n\tif (pdata && pdata->deassert_reset) {\n\t\tret = pdata->deassert_reset(pdev, pdata->reset_name);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"deassert_reset failed: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (pdata && pdata->device_enable)\n\t\tpdata->device_enable(pdev);\n\n\t \n\tif (obj->domain)\n\t\tomap_iommu_restore_tlb_entries(obj);\n\n\tret = omap2_iommu_enable(obj);\n\n\treturn ret;\n}\n\n \nstatic int omap_iommu_prepare(struct device *dev)\n{\n\tif (pm_runtime_status_suspended(dev))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic bool omap_iommu_can_register(struct platform_device *pdev)\n{\n\tstruct device_node *np = pdev->dev.of_node;\n\n\tif (!of_device_is_compatible(np, \"ti,dra7-dsp-iommu\"))\n\t\treturn true;\n\n\t \n\tif ((!strcmp(dev_name(&pdev->dev), \"40d01000.mmu\")) ||\n\t    (!strcmp(dev_name(&pdev->dev), \"41501000.mmu\")))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int omap_iommu_dra7_get_dsp_system_cfg(struct platform_device *pdev,\n\t\t\t\t\t      struct omap_iommu *obj)\n{\n\tstruct device_node *np = pdev->dev.of_node;\n\tint ret;\n\n\tif (!of_device_is_compatible(np, \"ti,dra7-dsp-iommu\"))\n\t\treturn 0;\n\n\tif (!of_property_read_bool(np, \"ti,syscon-mmuconfig\")) {\n\t\tdev_err(&pdev->dev, \"ti,syscon-mmuconfig property is missing\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tobj->syscfg =\n\t\tsyscon_regmap_lookup_by_phandle(np, \"ti,syscon-mmuconfig\");\n\tif (IS_ERR(obj->syscfg)) {\n\t\t \n\t\tret = PTR_ERR(obj->syscfg);\n\t\treturn ret;\n\t}\n\n\tif (of_property_read_u32_index(np, \"ti,syscon-mmuconfig\", 1,\n\t\t\t\t       &obj->id)) {\n\t\tdev_err(&pdev->dev, \"couldn't get the IOMMU instance id within subsystem\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (obj->id != 0 && obj->id != 1) {\n\t\tdev_err(&pdev->dev, \"invalid IOMMU instance id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int omap_iommu_probe(struct platform_device *pdev)\n{\n\tint err = -ENODEV;\n\tint irq;\n\tstruct omap_iommu *obj;\n\tstruct resource *res;\n\tstruct device_node *of = pdev->dev.of_node;\n\n\tif (!of) {\n\t\tpr_err(\"%s: only DT-based devices are supported\\n\", __func__);\n\t\treturn -ENODEV;\n\t}\n\n\tobj = devm_kzalloc(&pdev->dev, sizeof(*obj) + MMU_REG_SIZE, GFP_KERNEL);\n\tif (!obj)\n\t\treturn -ENOMEM;\n\n\t \n\tif (pdev->dev.pm_domain) {\n\t\tdev_dbg(&pdev->dev, \"device pm_domain is being reset\\n\");\n\t\tpdev->dev.pm_domain = NULL;\n\t}\n\n\tobj->name = dev_name(&pdev->dev);\n\tobj->nr_tlb_entries = 32;\n\terr = of_property_read_u32(of, \"ti,#tlb-entries\", &obj->nr_tlb_entries);\n\tif (err && err != -EINVAL)\n\t\treturn err;\n\tif (obj->nr_tlb_entries != 32 && obj->nr_tlb_entries != 8)\n\t\treturn -EINVAL;\n\tif (of_property_read_bool(of, \"ti,iommu-bus-err-back\"))\n\t\tobj->has_bus_err_back = MMU_GP_REG_BUS_ERR_BACK_EN;\n\n\tobj->dev = &pdev->dev;\n\tobj->ctx = (void *)obj + sizeof(*obj);\n\tobj->cr_ctx = devm_kzalloc(&pdev->dev,\n\t\t\t\t   sizeof(*obj->cr_ctx) * obj->nr_tlb_entries,\n\t\t\t\t   GFP_KERNEL);\n\tif (!obj->cr_ctx)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&obj->iommu_lock);\n\tspin_lock_init(&obj->page_table_lock);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tobj->regbase = devm_ioremap_resource(obj->dev, res);\n\tif (IS_ERR(obj->regbase))\n\t\treturn PTR_ERR(obj->regbase);\n\n\terr = omap_iommu_dra7_get_dsp_system_cfg(pdev, obj);\n\tif (err)\n\t\treturn err;\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0)\n\t\treturn -ENODEV;\n\n\terr = devm_request_irq(obj->dev, irq, iommu_fault_handler, IRQF_SHARED,\n\t\t\t       dev_name(obj->dev), obj);\n\tif (err < 0)\n\t\treturn err;\n\tplatform_set_drvdata(pdev, obj);\n\n\tif (omap_iommu_can_register(pdev)) {\n\t\tobj->group = iommu_group_alloc();\n\t\tif (IS_ERR(obj->group))\n\t\t\treturn PTR_ERR(obj->group);\n\n\t\terr = iommu_device_sysfs_add(&obj->iommu, obj->dev, NULL,\n\t\t\t\t\t     obj->name);\n\t\tif (err)\n\t\t\tgoto out_group;\n\n\t\terr = iommu_device_register(&obj->iommu, &omap_iommu_ops, &pdev->dev);\n\t\tif (err)\n\t\t\tgoto out_sysfs;\n\t}\n\n\tpm_runtime_enable(obj->dev);\n\n\tomap_iommu_debugfs_add(obj);\n\n\tdev_info(&pdev->dev, \"%s registered\\n\", obj->name);\n\n\t \n\tbus_iommu_probe(&platform_bus_type);\n\n\treturn 0;\n\nout_sysfs:\n\tiommu_device_sysfs_remove(&obj->iommu);\nout_group:\n\tiommu_group_put(obj->group);\n\treturn err;\n}\n\nstatic void omap_iommu_remove(struct platform_device *pdev)\n{\n\tstruct omap_iommu *obj = platform_get_drvdata(pdev);\n\n\tif (obj->group) {\n\t\tiommu_group_put(obj->group);\n\t\tobj->group = NULL;\n\n\t\tiommu_device_sysfs_remove(&obj->iommu);\n\t\tiommu_device_unregister(&obj->iommu);\n\t}\n\n\tomap_iommu_debugfs_remove(obj);\n\n\tpm_runtime_disable(obj->dev);\n\n\tdev_info(&pdev->dev, \"%s removed\\n\", obj->name);\n}\n\nstatic const struct dev_pm_ops omap_iommu_pm_ops = {\n\t.prepare = omap_iommu_prepare,\n\tSET_LATE_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,\n\t\t\t\t     pm_runtime_force_resume)\n\tSET_RUNTIME_PM_OPS(omap_iommu_runtime_suspend,\n\t\t\t   omap_iommu_runtime_resume, NULL)\n};\n\nstatic const struct of_device_id omap_iommu_of_match[] = {\n\t{ .compatible = \"ti,omap2-iommu\" },\n\t{ .compatible = \"ti,omap4-iommu\" },\n\t{ .compatible = \"ti,dra7-iommu\"\t},\n\t{ .compatible = \"ti,dra7-dsp-iommu\" },\n\t{},\n};\n\nstatic struct platform_driver omap_iommu_driver = {\n\t.probe\t= omap_iommu_probe,\n\t.remove_new = omap_iommu_remove,\n\t.driver\t= {\n\t\t.name\t= \"omap-iommu\",\n\t\t.pm\t= &omap_iommu_pm_ops,\n\t\t.of_match_table = of_match_ptr(omap_iommu_of_match),\n\t},\n};\n\nstatic u32 iotlb_init_entry(struct iotlb_entry *e, u32 da, u32 pa, int pgsz)\n{\n\tmemset(e, 0, sizeof(*e));\n\n\te->da\t\t= da;\n\te->pa\t\t= pa;\n\te->valid\t= MMU_CAM_V;\n\te->pgsz\t\t= pgsz;\n\te->endian\t= MMU_RAM_ENDIAN_LITTLE;\n\te->elsz\t\t= MMU_RAM_ELSZ_8;\n\te->mixed\t= 0;\n\n\treturn iopgsz_to_bytes(e->pgsz);\n}\n\nstatic int omap_iommu_map(struct iommu_domain *domain, unsigned long da,\n\t\t\t  phys_addr_t pa, size_t bytes, int prot, gfp_t gfp)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct device *dev = omap_domain->dev;\n\tstruct omap_iommu_device *iommu;\n\tstruct omap_iommu *oiommu;\n\tstruct iotlb_entry e;\n\tint omap_pgsz;\n\tu32 ret = -EINVAL;\n\tint i;\n\n\tomap_pgsz = bytes_to_iopgsz(bytes);\n\tif (omap_pgsz < 0) {\n\t\tdev_err(dev, \"invalid size to map: %zu\\n\", bytes);\n\t\treturn -EINVAL;\n\t}\n\n\tdev_dbg(dev, \"mapping da 0x%lx to pa %pa size 0x%zx\\n\", da, &pa, bytes);\n\n\tiotlb_init_entry(&e, da, pa, omap_pgsz);\n\n\tiommu = omap_domain->iommus;\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu++) {\n\t\toiommu = iommu->iommu_dev;\n\t\tret = omap_iopgtable_store_entry(oiommu, &e);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"omap_iopgtable_store_entry failed: %d\\n\",\n\t\t\t\tret);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret) {\n\t\twhile (i--) {\n\t\t\tiommu--;\n\t\t\toiommu = iommu->iommu_dev;\n\t\t\tiopgtable_clear_entry(oiommu, da);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic size_t omap_iommu_unmap(struct iommu_domain *domain, unsigned long da,\n\t\t\t       size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct device *dev = omap_domain->dev;\n\tstruct omap_iommu_device *iommu;\n\tstruct omap_iommu *oiommu;\n\tbool error = false;\n\tsize_t bytes = 0;\n\tint i;\n\n\tdev_dbg(dev, \"unmapping da 0x%lx size %zu\\n\", da, size);\n\n\tiommu = omap_domain->iommus;\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu++) {\n\t\toiommu = iommu->iommu_dev;\n\t\tbytes = iopgtable_clear_entry(oiommu, da);\n\t\tif (!bytes)\n\t\t\terror = true;\n\t}\n\n\t \n\treturn error ? 0 : bytes;\n}\n\nstatic int omap_iommu_count(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tint count = 0;\n\n\twhile (arch_data->iommu_dev) {\n\t\tcount++;\n\t\tarch_data++;\n\t}\n\n\treturn count;\n}\n\n \nstatic int omap_iommu_attach_init(struct device *dev,\n\t\t\t\t  struct omap_iommu_domain *odomain)\n{\n\tstruct omap_iommu_device *iommu;\n\tint i;\n\n\todomain->num_iommus = omap_iommu_count(dev);\n\tif (!odomain->num_iommus)\n\t\treturn -ENODEV;\n\n\todomain->iommus = kcalloc(odomain->num_iommus, sizeof(*iommu),\n\t\t\t\t  GFP_ATOMIC);\n\tif (!odomain->iommus)\n\t\treturn -ENOMEM;\n\n\tiommu = odomain->iommus;\n\tfor (i = 0; i < odomain->num_iommus; i++, iommu++) {\n\t\tiommu->pgtable = kzalloc(IOPGD_TABLE_SIZE, GFP_ATOMIC);\n\t\tif (!iommu->pgtable)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\tif (WARN_ON(!IS_ALIGNED((long)iommu->pgtable,\n\t\t\t\t\tIOPGD_TABLE_SIZE)))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void omap_iommu_detach_fini(struct omap_iommu_domain *odomain)\n{\n\tint i;\n\tstruct omap_iommu_device *iommu = odomain->iommus;\n\n\tfor (i = 0; iommu && i < odomain->num_iommus; i++, iommu++)\n\t\tkfree(iommu->pgtable);\n\n\tkfree(odomain->iommus);\n\todomain->num_iommus = 0;\n\todomain->iommus = NULL;\n}\n\nstatic int\nomap_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct omap_iommu_device *iommu;\n\tstruct omap_iommu *oiommu;\n\tint ret = 0;\n\tint i;\n\n\tif (!arch_data || !arch_data->iommu_dev) {\n\t\tdev_err(dev, \"device doesn't have an associated iommu\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tspin_lock(&omap_domain->lock);\n\n\t \n\tif (omap_domain->dev) {\n\t\tdev_err(dev, \"iommu domain is already attached\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = omap_iommu_attach_init(dev, omap_domain);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to allocate required iommu data %d\\n\",\n\t\t\tret);\n\t\tgoto init_fail;\n\t}\n\n\tiommu = omap_domain->iommus;\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu++, arch_data++) {\n\t\t \n\t\toiommu = arch_data->iommu_dev;\n\t\tret = omap_iommu_attach(oiommu, iommu->pgtable);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"can't get omap iommu: %d\\n\", ret);\n\t\t\tgoto attach_fail;\n\t\t}\n\n\t\toiommu->domain = domain;\n\t\tiommu->iommu_dev = oiommu;\n\t}\n\n\tomap_domain->dev = dev;\n\n\tgoto out;\n\nattach_fail:\n\twhile (i--) {\n\t\tiommu--;\n\t\tarch_data--;\n\t\toiommu = iommu->iommu_dev;\n\t\tomap_iommu_detach(oiommu);\n\t\tiommu->iommu_dev = NULL;\n\t\toiommu->domain = NULL;\n\t}\ninit_fail:\n\tomap_iommu_detach_fini(omap_domain);\nout:\n\tspin_unlock(&omap_domain->lock);\n\treturn ret;\n}\n\nstatic void _omap_iommu_detach_dev(struct omap_iommu_domain *omap_domain,\n\t\t\t\t   struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tstruct omap_iommu_device *iommu = omap_domain->iommus;\n\tstruct omap_iommu *oiommu;\n\tint i;\n\n\tif (!omap_domain->dev) {\n\t\tdev_err(dev, \"domain has no attached device\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (omap_domain->dev != dev) {\n\t\tdev_err(dev, \"invalid attached device\\n\");\n\t\treturn;\n\t}\n\n\t \n\tiommu += (omap_domain->num_iommus - 1);\n\tarch_data += (omap_domain->num_iommus - 1);\n\tfor (i = 0; i < omap_domain->num_iommus; i++, iommu--, arch_data--) {\n\t\toiommu = iommu->iommu_dev;\n\t\tiopgtable_clear_entry_all(oiommu);\n\n\t\tomap_iommu_detach(oiommu);\n\t\tiommu->iommu_dev = NULL;\n\t\toiommu->domain = NULL;\n\t}\n\n\tomap_iommu_detach_fini(omap_domain);\n\n\tomap_domain->dev = NULL;\n}\n\nstatic void omap_iommu_set_platform_dma(struct device *dev)\n{\n\tstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\n\tspin_lock(&omap_domain->lock);\n\t_omap_iommu_detach_dev(omap_domain, dev);\n\tspin_unlock(&omap_domain->lock);\n}\n\nstatic struct iommu_domain *omap_iommu_domain_alloc(unsigned type)\n{\n\tstruct omap_iommu_domain *omap_domain;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn NULL;\n\n\tomap_domain = kzalloc(sizeof(*omap_domain), GFP_KERNEL);\n\tif (!omap_domain)\n\t\treturn NULL;\n\n\tspin_lock_init(&omap_domain->lock);\n\n\tomap_domain->domain.geometry.aperture_start = 0;\n\tomap_domain->domain.geometry.aperture_end   = (1ULL << 32) - 1;\n\tomap_domain->domain.geometry.force_aperture = true;\n\n\treturn &omap_domain->domain;\n}\n\nstatic void omap_iommu_domain_free(struct iommu_domain *domain)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\n\t \n\tif (omap_domain->dev)\n\t\t_omap_iommu_detach_dev(omap_domain, omap_domain->dev);\n\n\tkfree(omap_domain);\n}\n\nstatic phys_addr_t omap_iommu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\t   dma_addr_t da)\n{\n\tstruct omap_iommu_domain *omap_domain = to_omap_domain(domain);\n\tstruct omap_iommu_device *iommu = omap_domain->iommus;\n\tstruct omap_iommu *oiommu = iommu->iommu_dev;\n\tstruct device *dev = oiommu->dev;\n\tu32 *pgd, *pte;\n\tphys_addr_t ret = 0;\n\n\t \n\tiopgtable_lookup_entry(oiommu, da, &pgd, &pte);\n\n\tif (pte) {\n\t\tif (iopte_is_small(*pte))\n\t\t\tret = omap_iommu_translate(*pte, da, IOPTE_MASK);\n\t\telse if (iopte_is_large(*pte))\n\t\t\tret = omap_iommu_translate(*pte, da, IOLARGE_MASK);\n\t\telse\n\t\t\tdev_err(dev, \"bogus pte 0x%x, da 0x%llx\", *pte,\n\t\t\t\t(unsigned long long)da);\n\t} else {\n\t\tif (iopgd_is_section(*pgd))\n\t\t\tret = omap_iommu_translate(*pgd, da, IOSECTION_MASK);\n\t\telse if (iopgd_is_super(*pgd))\n\t\t\tret = omap_iommu_translate(*pgd, da, IOSUPER_MASK);\n\t\telse\n\t\t\tdev_err(dev, \"bogus pgd 0x%x, da 0x%llx\", *pgd,\n\t\t\t\t(unsigned long long)da);\n\t}\n\n\treturn ret;\n}\n\nstatic struct iommu_device *omap_iommu_probe_device(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data, *tmp;\n\tstruct platform_device *pdev;\n\tstruct omap_iommu *oiommu;\n\tstruct device_node *np;\n\tint num_iommus, i;\n\n\t \n\tif (!dev->of_node)\n\t\treturn ERR_PTR(-ENODEV);\n\n\t \n\tnum_iommus = of_property_count_elems_of_size(dev->of_node, \"iommus\",\n\t\t\t\t\t\t     sizeof(phandle));\n\tif (num_iommus < 0)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tarch_data = kcalloc(num_iommus + 1, sizeof(*arch_data), GFP_KERNEL);\n\tif (!arch_data)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = 0, tmp = arch_data; i < num_iommus; i++, tmp++) {\n\t\tnp = of_parse_phandle(dev->of_node, \"iommus\", i);\n\t\tif (!np) {\n\t\t\tkfree(arch_data);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tpdev = of_find_device_by_node(np);\n\t\tif (!pdev) {\n\t\t\tof_node_put(np);\n\t\t\tkfree(arch_data);\n\t\t\treturn ERR_PTR(-ENODEV);\n\t\t}\n\n\t\toiommu = platform_get_drvdata(pdev);\n\t\tif (!oiommu) {\n\t\t\tof_node_put(np);\n\t\t\tkfree(arch_data);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\ttmp->iommu_dev = oiommu;\n\t\ttmp->dev = &pdev->dev;\n\n\t\tof_node_put(np);\n\t}\n\n\tdev_iommu_priv_set(dev, arch_data);\n\n\t \n\toiommu = arch_data->iommu_dev;\n\n\treturn &oiommu->iommu;\n}\n\nstatic void omap_iommu_release_device(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\n\tif (!dev->of_node || !arch_data)\n\t\treturn;\n\n\tdev_iommu_priv_set(dev, NULL);\n\tkfree(arch_data);\n\n}\n\nstatic struct iommu_group *omap_iommu_device_group(struct device *dev)\n{\n\tstruct omap_iommu_arch_data *arch_data = dev_iommu_priv_get(dev);\n\tstruct iommu_group *group = ERR_PTR(-EINVAL);\n\n\tif (!arch_data)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (arch_data->iommu_dev)\n\t\tgroup = iommu_group_ref_get(arch_data->iommu_dev->group);\n\n\treturn group;\n}\n\nstatic const struct iommu_ops omap_iommu_ops = {\n\t.domain_alloc\t= omap_iommu_domain_alloc,\n\t.probe_device\t= omap_iommu_probe_device,\n\t.release_device\t= omap_iommu_release_device,\n\t.device_group\t= omap_iommu_device_group,\n\t.set_platform_dma_ops = omap_iommu_set_platform_dma,\n\t.pgsize_bitmap\t= OMAP_IOMMU_PGSIZES,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= omap_iommu_attach_dev,\n\t\t.map\t\t= omap_iommu_map,\n\t\t.unmap\t\t= omap_iommu_unmap,\n\t\t.iova_to_phys\t= omap_iommu_iova_to_phys,\n\t\t.free\t\t= omap_iommu_domain_free,\n\t}\n};\n\nstatic int __init omap_iommu_init(void)\n{\n\tstruct kmem_cache *p;\n\tconst slab_flags_t flags = SLAB_HWCACHE_ALIGN;\n\tsize_t align = 1 << 10;  \n\tstruct device_node *np;\n\tint ret;\n\n\tnp = of_find_matching_node(NULL, omap_iommu_of_match);\n\tif (!np)\n\t\treturn 0;\n\n\tof_node_put(np);\n\n\tp = kmem_cache_create(\"iopte_cache\", IOPTE_TABLE_SIZE, align, flags,\n\t\t\t      NULL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tiopte_cachep = p;\n\n\tomap_iommu_debugfs_init();\n\n\tret = platform_driver_register(&omap_iommu_driver);\n\tif (ret) {\n\t\tpr_err(\"%s: failed to register driver\\n\", __func__);\n\t\tgoto fail_driver;\n\t}\n\n\treturn 0;\n\nfail_driver:\n\tkmem_cache_destroy(iopte_cachep);\n\treturn ret;\n}\nsubsys_initcall(omap_iommu_init);\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}