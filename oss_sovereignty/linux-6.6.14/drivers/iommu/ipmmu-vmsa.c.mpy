{
  "module_name": "ipmmu-vmsa.c",
  "hash_id": "1e178082bbc92f95fd4e9363173507ecd13e8874251f927f8472ed9da8534a3f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/ipmmu-vmsa.c",
  "human_readable_source": "\n \n\n#include <linux/bitmap.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/io-pgtable.h>\n#include <linux/iommu.h>\n#include <linux/of.h>\n#include <linux/of_platform.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/sys_soc.h>\n\n#if defined(CONFIG_ARM) && !defined(CONFIG_IOMMU_DMA)\n#include <asm/dma-iommu.h>\n#else\n#define arm_iommu_create_mapping(...)\tNULL\n#define arm_iommu_attach_device(...)\t-ENODEV\n#define arm_iommu_release_mapping(...)\tdo {} while (0)\n#endif\n\n#define IPMMU_CTX_MAX\t\t16U\n#define IPMMU_CTX_INVALID\t-1\n\n#define IPMMU_UTLB_MAX\t\t64U\n\nstruct ipmmu_features {\n\tbool use_ns_alias_offset;\n\tbool has_cache_leaf_nodes;\n\tunsigned int number_of_contexts;\n\tunsigned int num_utlbs;\n\tbool setup_imbuscr;\n\tbool twobit_imttbcr_sl0;\n\tbool reserved_context;\n\tbool cache_snoop;\n\tunsigned int ctx_offset_base;\n\tunsigned int ctx_offset_stride;\n\tunsigned int utlb_offset_base;\n};\n\nstruct ipmmu_vmsa_device {\n\tstruct device *dev;\n\tvoid __iomem *base;\n\tstruct iommu_device iommu;\n\tstruct ipmmu_vmsa_device *root;\n\tconst struct ipmmu_features *features;\n\tunsigned int num_ctx;\n\tspinlock_t lock;\t\t\t \n\tDECLARE_BITMAP(ctx, IPMMU_CTX_MAX);\n\tstruct ipmmu_vmsa_domain *domains[IPMMU_CTX_MAX];\n\ts8 utlb_ctx[IPMMU_UTLB_MAX];\n\n\tstruct iommu_group *group;\n\tstruct dma_iommu_mapping *mapping;\n};\n\nstruct ipmmu_vmsa_domain {\n\tstruct ipmmu_vmsa_device *mmu;\n\tstruct iommu_domain io_domain;\n\n\tstruct io_pgtable_cfg cfg;\n\tstruct io_pgtable_ops *iop;\n\n\tunsigned int context_id;\n\tstruct mutex mutex;\t\t\t \n};\n\nstatic struct ipmmu_vmsa_domain *to_vmsa_domain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct ipmmu_vmsa_domain, io_domain);\n}\n\nstatic struct ipmmu_vmsa_device *to_ipmmu(struct device *dev)\n{\n\treturn dev_iommu_priv_get(dev);\n}\n\n#define TLB_LOOP_TIMEOUT\t\t100\t \n\n \n\n#define IM_NS_ALIAS_OFFSET\t\t0x800\n\n \n#define IMCTR\t\t\t\t0x0000\t\t \n#define IMCTR_INTEN\t\t\t(1 << 2)\t \n#define IMCTR_FLUSH\t\t\t(1 << 1)\t \n#define IMCTR_MMUEN\t\t\t(1 << 0)\t \n\n#define IMTTBCR\t\t\t\t0x0008\t\t \n#define IMTTBCR_EAE\t\t\t(1 << 31)\t \n#define IMTTBCR_SH0_INNER_SHAREABLE\t(3 << 12)\t \n#define IMTTBCR_ORGN0_WB_WA\t\t(1 << 10)\t \n#define IMTTBCR_IRGN0_WB_WA\t\t(1 << 8)\t \n#define IMTTBCR_SL0_TWOBIT_LVL_1\t(2 << 6)\t \n#define IMTTBCR_SL0_LVL_1\t\t(1 << 4)\t \n\n#define IMBUSCR\t\t\t\t0x000c\t\t \n#define IMBUSCR_DVM\t\t\t(1 << 2)\t \n#define IMBUSCR_BUSSEL_MASK\t\t(3 << 0)\t \n\n#define IMTTLBR0\t\t\t0x0010\t\t \n#define IMTTUBR0\t\t\t0x0014\t\t \n\n#define IMSTR\t\t\t\t0x0020\t\t \n#define IMSTR_MHIT\t\t\t(1 << 4)\t \n#define IMSTR_ABORT\t\t\t(1 << 2)\t \n#define IMSTR_PF\t\t\t(1 << 1)\t \n#define IMSTR_TF\t\t\t(1 << 0)\t \n\n#define IMMAIR0\t\t\t\t0x0028\t\t \n\n#define IMELAR\t\t\t\t0x0030\t\t \n#define IMEUAR\t\t\t\t0x0034\t\t \n\n \n#define IMUCTR(n)\t\t\t((n) < 32 ? IMUCTR0(n) : IMUCTR32(n))\n#define IMUCTR0(n)\t\t\t(0x0300 + ((n) * 16))\t\t \n#define IMUCTR32(n)\t\t\t(0x0600 + (((n) - 32) * 16))\t \n#define IMUCTR_TTSEL_MMU(n)\t\t((n) << 4)\t \n#define IMUCTR_FLUSH\t\t\t(1 << 1)\t \n#define IMUCTR_MMUEN\t\t\t(1 << 0)\t \n\n#define IMUASID(n)\t\t\t((n) < 32 ? IMUASID0(n) : IMUASID32(n))\n#define IMUASID0(n)\t\t\t(0x0308 + ((n) * 16))\t\t \n#define IMUASID32(n)\t\t\t(0x0608 + (((n) - 32) * 16))\t \n\n \n\nstatic struct platform_driver ipmmu_driver;\n\nstatic bool ipmmu_is_root(struct ipmmu_vmsa_device *mmu)\n{\n\treturn mmu->root == mmu;\n}\n\nstatic int __ipmmu_check_device(struct device *dev, void *data)\n{\n\tstruct ipmmu_vmsa_device *mmu = dev_get_drvdata(dev);\n\tstruct ipmmu_vmsa_device **rootp = data;\n\n\tif (ipmmu_is_root(mmu))\n\t\t*rootp = mmu;\n\n\treturn 0;\n}\n\nstatic struct ipmmu_vmsa_device *ipmmu_find_root(void)\n{\n\tstruct ipmmu_vmsa_device *root = NULL;\n\n\treturn driver_for_each_device(&ipmmu_driver.driver, NULL, &root,\n\t\t\t\t      __ipmmu_check_device) == 0 ? root : NULL;\n}\n\n \n\nstatic u32 ipmmu_read(struct ipmmu_vmsa_device *mmu, unsigned int offset)\n{\n\treturn ioread32(mmu->base + offset);\n}\n\nstatic void ipmmu_write(struct ipmmu_vmsa_device *mmu, unsigned int offset,\n\t\t\tu32 data)\n{\n\tiowrite32(data, mmu->base + offset);\n}\n\nstatic unsigned int ipmmu_ctx_reg(struct ipmmu_vmsa_device *mmu,\n\t\t\t\t  unsigned int context_id, unsigned int reg)\n{\n\tunsigned int base = mmu->features->ctx_offset_base;\n\n\tif (context_id > 7)\n\t\tbase += 0x800 - 8 * 0x40;\n\n\treturn base + context_id * mmu->features->ctx_offset_stride + reg;\n}\n\nstatic u32 ipmmu_ctx_read(struct ipmmu_vmsa_device *mmu,\n\t\t\t  unsigned int context_id, unsigned int reg)\n{\n\treturn ipmmu_read(mmu, ipmmu_ctx_reg(mmu, context_id, reg));\n}\n\nstatic void ipmmu_ctx_write(struct ipmmu_vmsa_device *mmu,\n\t\t\t    unsigned int context_id, unsigned int reg, u32 data)\n{\n\tipmmu_write(mmu, ipmmu_ctx_reg(mmu, context_id, reg), data);\n}\n\nstatic u32 ipmmu_ctx_read_root(struct ipmmu_vmsa_domain *domain,\n\t\t\t       unsigned int reg)\n{\n\treturn ipmmu_ctx_read(domain->mmu->root, domain->context_id, reg);\n}\n\nstatic void ipmmu_ctx_write_root(struct ipmmu_vmsa_domain *domain,\n\t\t\t\t unsigned int reg, u32 data)\n{\n\tipmmu_ctx_write(domain->mmu->root, domain->context_id, reg, data);\n}\n\nstatic void ipmmu_ctx_write_all(struct ipmmu_vmsa_domain *domain,\n\t\t\t\tunsigned int reg, u32 data)\n{\n\tif (domain->mmu != domain->mmu->root)\n\t\tipmmu_ctx_write(domain->mmu, domain->context_id, reg, data);\n\n\tipmmu_ctx_write(domain->mmu->root, domain->context_id, reg, data);\n}\n\nstatic u32 ipmmu_utlb_reg(struct ipmmu_vmsa_device *mmu, unsigned int reg)\n{\n\treturn mmu->features->utlb_offset_base + reg;\n}\n\nstatic void ipmmu_imuasid_write(struct ipmmu_vmsa_device *mmu,\n\t\t\t\tunsigned int utlb, u32 data)\n{\n\tipmmu_write(mmu, ipmmu_utlb_reg(mmu, IMUASID(utlb)), data);\n}\n\nstatic void ipmmu_imuctr_write(struct ipmmu_vmsa_device *mmu,\n\t\t\t       unsigned int utlb, u32 data)\n{\n\tipmmu_write(mmu, ipmmu_utlb_reg(mmu, IMUCTR(utlb)), data);\n}\n\n \n\n \nstatic void ipmmu_tlb_sync(struct ipmmu_vmsa_domain *domain)\n{\n\tu32 val;\n\n\tif (read_poll_timeout_atomic(ipmmu_ctx_read_root, val,\n\t\t\t\t     !(val & IMCTR_FLUSH), 1, TLB_LOOP_TIMEOUT,\n\t\t\t\t     false, domain, IMCTR))\n\t\tdev_err_ratelimited(domain->mmu->dev,\n\t\t\t\"TLB sync timed out -- MMU may be deadlocked\\n\");\n}\n\nstatic void ipmmu_tlb_invalidate(struct ipmmu_vmsa_domain *domain)\n{\n\tu32 reg;\n\n\treg = ipmmu_ctx_read_root(domain, IMCTR);\n\treg |= IMCTR_FLUSH;\n\tipmmu_ctx_write_all(domain, IMCTR, reg);\n\n\tipmmu_tlb_sync(domain);\n}\n\n \nstatic void ipmmu_utlb_enable(struct ipmmu_vmsa_domain *domain,\n\t\t\t      unsigned int utlb)\n{\n\tstruct ipmmu_vmsa_device *mmu = domain->mmu;\n\n\t \n\n\t \n\tipmmu_imuasid_write(mmu, utlb, 0);\n\t \n\tipmmu_imuctr_write(mmu, utlb, IMUCTR_TTSEL_MMU(domain->context_id) |\n\t\t\t\t      IMUCTR_FLUSH | IMUCTR_MMUEN);\n\tmmu->utlb_ctx[utlb] = domain->context_id;\n}\n\nstatic void ipmmu_tlb_flush_all(void *cookie)\n{\n\tstruct ipmmu_vmsa_domain *domain = cookie;\n\n\tipmmu_tlb_invalidate(domain);\n}\n\nstatic void ipmmu_tlb_flush(unsigned long iova, size_t size,\n\t\t\t\tsize_t granule, void *cookie)\n{\n\tipmmu_tlb_flush_all(cookie);\n}\n\nstatic const struct iommu_flush_ops ipmmu_flush_ops = {\n\t.tlb_flush_all = ipmmu_tlb_flush_all,\n\t.tlb_flush_walk = ipmmu_tlb_flush,\n};\n\n \n\nstatic int ipmmu_domain_allocate_context(struct ipmmu_vmsa_device *mmu,\n\t\t\t\t\t struct ipmmu_vmsa_domain *domain)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tret = find_first_zero_bit(mmu->ctx, mmu->num_ctx);\n\tif (ret != mmu->num_ctx) {\n\t\tmmu->domains[ret] = domain;\n\t\tset_bit(ret, mmu->ctx);\n\t} else\n\t\tret = -EBUSY;\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\n\treturn ret;\n}\n\nstatic void ipmmu_domain_free_context(struct ipmmu_vmsa_device *mmu,\n\t\t\t\t      unsigned int context_id)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\tclear_bit(context_id, mmu->ctx);\n\tmmu->domains[context_id] = NULL;\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n}\n\nstatic void ipmmu_domain_setup_context(struct ipmmu_vmsa_domain *domain)\n{\n\tu64 ttbr;\n\tu32 tmp;\n\n\t \n\tttbr = domain->cfg.arm_lpae_s1_cfg.ttbr;\n\tipmmu_ctx_write_root(domain, IMTTLBR0, ttbr);\n\tipmmu_ctx_write_root(domain, IMTTUBR0, ttbr >> 32);\n\n\t \n\tif (domain->mmu->features->twobit_imttbcr_sl0)\n\t\ttmp = IMTTBCR_SL0_TWOBIT_LVL_1;\n\telse\n\t\ttmp = IMTTBCR_SL0_LVL_1;\n\n\tif (domain->mmu->features->cache_snoop)\n\t\ttmp |= IMTTBCR_SH0_INNER_SHAREABLE | IMTTBCR_ORGN0_WB_WA |\n\t\t       IMTTBCR_IRGN0_WB_WA;\n\n\tipmmu_ctx_write_root(domain, IMTTBCR, IMTTBCR_EAE | tmp);\n\n\t \n\tipmmu_ctx_write_root(domain, IMMAIR0,\n\t\t\t     domain->cfg.arm_lpae_s1_cfg.mair);\n\n\t \n\tif (domain->mmu->features->setup_imbuscr)\n\t\tipmmu_ctx_write_root(domain, IMBUSCR,\n\t\t\t\t     ipmmu_ctx_read_root(domain, IMBUSCR) &\n\t\t\t\t     ~(IMBUSCR_DVM | IMBUSCR_BUSSEL_MASK));\n\n\t \n\tipmmu_ctx_write_root(domain, IMSTR, ipmmu_ctx_read_root(domain, IMSTR));\n\n\t \n\tipmmu_ctx_write_all(domain, IMCTR,\n\t\t\t    IMCTR_INTEN | IMCTR_FLUSH | IMCTR_MMUEN);\n}\n\nstatic int ipmmu_domain_init_context(struct ipmmu_vmsa_domain *domain)\n{\n\tint ret;\n\n\t \n\tdomain->cfg.quirks = IO_PGTABLE_QUIRK_ARM_NS;\n\tdomain->cfg.pgsize_bitmap = SZ_1G | SZ_2M | SZ_4K;\n\tdomain->cfg.ias = 32;\n\tdomain->cfg.oas = 40;\n\tdomain->cfg.tlb = &ipmmu_flush_ops;\n\tdomain->io_domain.geometry.aperture_end = DMA_BIT_MASK(32);\n\tdomain->io_domain.geometry.force_aperture = true;\n\t \n\tdomain->cfg.coherent_walk = false;\n\tdomain->cfg.iommu_dev = domain->mmu->root->dev;\n\n\t \n\tret = ipmmu_domain_allocate_context(domain->mmu->root, domain);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdomain->context_id = ret;\n\n\tdomain->iop = alloc_io_pgtable_ops(ARM_32_LPAE_S1, &domain->cfg,\n\t\t\t\t\t   domain);\n\tif (!domain->iop) {\n\t\tipmmu_domain_free_context(domain->mmu->root,\n\t\t\t\t\t  domain->context_id);\n\t\treturn -EINVAL;\n\t}\n\n\tipmmu_domain_setup_context(domain);\n\treturn 0;\n}\n\nstatic void ipmmu_domain_destroy_context(struct ipmmu_vmsa_domain *domain)\n{\n\tif (!domain->mmu)\n\t\treturn;\n\n\t \n\tipmmu_ctx_write_all(domain, IMCTR, IMCTR_FLUSH);\n\tipmmu_tlb_sync(domain);\n\tipmmu_domain_free_context(domain->mmu->root, domain->context_id);\n}\n\n \n\nstatic irqreturn_t ipmmu_domain_irq(struct ipmmu_vmsa_domain *domain)\n{\n\tconst u32 err_mask = IMSTR_MHIT | IMSTR_ABORT | IMSTR_PF | IMSTR_TF;\n\tstruct ipmmu_vmsa_device *mmu = domain->mmu;\n\tunsigned long iova;\n\tu32 status;\n\n\tstatus = ipmmu_ctx_read_root(domain, IMSTR);\n\tif (!(status & err_mask))\n\t\treturn IRQ_NONE;\n\n\tiova = ipmmu_ctx_read_root(domain, IMELAR);\n\tif (IS_ENABLED(CONFIG_64BIT))\n\t\tiova |= (u64)ipmmu_ctx_read_root(domain, IMEUAR) << 32;\n\n\t \n\tipmmu_ctx_write_root(domain, IMSTR, 0);\n\n\t \n\tif (status & IMSTR_MHIT)\n\t\tdev_err_ratelimited(mmu->dev, \"Multiple TLB hits @0x%lx\\n\",\n\t\t\t\t    iova);\n\tif (status & IMSTR_ABORT)\n\t\tdev_err_ratelimited(mmu->dev, \"Page Table Walk Abort @0x%lx\\n\",\n\t\t\t\t    iova);\n\n\tif (!(status & (IMSTR_PF | IMSTR_TF)))\n\t\treturn IRQ_NONE;\n\n\t \n\tif (!report_iommu_fault(&domain->io_domain, mmu->dev, iova, 0))\n\t\treturn IRQ_HANDLED;\n\n\tdev_err_ratelimited(mmu->dev,\n\t\t\t    \"Unhandled fault: status 0x%08x iova 0x%lx\\n\",\n\t\t\t    status, iova);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t ipmmu_irq(int irq, void *dev)\n{\n\tstruct ipmmu_vmsa_device *mmu = dev;\n\tirqreturn_t status = IRQ_NONE;\n\tunsigned int i;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mmu->lock, flags);\n\n\t \n\tfor (i = 0; i < mmu->num_ctx; i++) {\n\t\tif (!mmu->domains[i])\n\t\t\tcontinue;\n\t\tif (ipmmu_domain_irq(mmu->domains[i]) == IRQ_HANDLED)\n\t\t\tstatus = IRQ_HANDLED;\n\t}\n\n\tspin_unlock_irqrestore(&mmu->lock, flags);\n\n\treturn status;\n}\n\n \n\nstatic struct iommu_domain *ipmmu_domain_alloc(unsigned type)\n{\n\tstruct ipmmu_vmsa_domain *domain;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED && type != IOMMU_DOMAIN_DMA)\n\t\treturn NULL;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\n\tmutex_init(&domain->mutex);\n\n\treturn &domain->io_domain;\n}\n\nstatic void ipmmu_domain_free(struct iommu_domain *io_domain)\n{\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\n\t \n\tipmmu_domain_destroy_context(domain);\n\tfree_io_pgtable_ops(domain->iop);\n\tkfree(domain);\n}\n\nstatic int ipmmu_attach_device(struct iommu_domain *io_domain,\n\t\t\t       struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct ipmmu_vmsa_device *mmu = to_ipmmu(dev);\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\tunsigned int i;\n\tint ret = 0;\n\n\tif (!mmu) {\n\t\tdev_err(dev, \"Cannot attach to IPMMU\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tmutex_lock(&domain->mutex);\n\n\tif (!domain->mmu) {\n\t\t \n\t\tdomain->mmu = mmu;\n\t\tret = ipmmu_domain_init_context(domain);\n\t\tif (ret < 0) {\n\t\t\tdev_err(dev, \"Unable to initialize IPMMU context\\n\");\n\t\t\tdomain->mmu = NULL;\n\t\t} else {\n\t\t\tdev_info(dev, \"Using IPMMU context %u\\n\",\n\t\t\t\t domain->context_id);\n\t\t}\n\t} else if (domain->mmu != mmu) {\n\t\t \n\t\tret = -EINVAL;\n\t} else\n\t\tdev_info(dev, \"Reusing IPMMU context %u\\n\", domain->context_id);\n\n\tmutex_unlock(&domain->mutex);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i < fwspec->num_ids; ++i)\n\t\tipmmu_utlb_enable(domain, fwspec->ids[i]);\n\n\treturn 0;\n}\n\nstatic int ipmmu_map(struct iommu_domain *io_domain, unsigned long iova,\n\t\t     phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t     int prot, gfp_t gfp, size_t *mapped)\n{\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\n\treturn domain->iop->map_pages(domain->iop, iova, paddr, pgsize, pgcount,\n\t\t\t\t      prot, gfp, mapped);\n}\n\nstatic size_t ipmmu_unmap(struct iommu_domain *io_domain, unsigned long iova,\n\t\t\t  size_t pgsize, size_t pgcount,\n\t\t\t  struct iommu_iotlb_gather *gather)\n{\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\n\treturn domain->iop->unmap_pages(domain->iop, iova, pgsize, pgcount, gather);\n}\n\nstatic void ipmmu_flush_iotlb_all(struct iommu_domain *io_domain)\n{\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\n\tif (domain->mmu)\n\t\tipmmu_tlb_flush_all(domain);\n}\n\nstatic void ipmmu_iotlb_sync(struct iommu_domain *io_domain,\n\t\t\t     struct iommu_iotlb_gather *gather)\n{\n\tipmmu_flush_iotlb_all(io_domain);\n}\n\nstatic phys_addr_t ipmmu_iova_to_phys(struct iommu_domain *io_domain,\n\t\t\t\t      dma_addr_t iova)\n{\n\tstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\n\n\t \n\n\treturn domain->iop->iova_to_phys(domain->iop, iova);\n}\n\nstatic int ipmmu_init_platform_device(struct device *dev,\n\t\t\t\t      struct of_phandle_args *args)\n{\n\tstruct platform_device *ipmmu_pdev;\n\n\tipmmu_pdev = of_find_device_by_node(args->np);\n\tif (!ipmmu_pdev)\n\t\treturn -ENODEV;\n\n\tdev_iommu_priv_set(dev, platform_get_drvdata(ipmmu_pdev));\n\n\treturn 0;\n}\n\nstatic const struct soc_device_attribute soc_needs_opt_in[] = {\n\t{ .family = \"R-Car Gen3\", },\n\t{ .family = \"R-Car Gen4\", },\n\t{ .family = \"RZ/G2\", },\n\t{   }\n};\n\nstatic const struct soc_device_attribute soc_denylist[] = {\n\t{ .soc_id = \"r8a774a1\", },\n\t{ .soc_id = \"r8a7795\", .revision = \"ES2.*\" },\n\t{ .soc_id = \"r8a7796\", },\n\t{   }\n};\n\nstatic const char * const devices_allowlist[] = {\n\t\"ee100000.mmc\",\n\t\"ee120000.mmc\",\n\t\"ee140000.mmc\",\n\t\"ee160000.mmc\"\n};\n\nstatic bool ipmmu_device_is_allowed(struct device *dev)\n{\n\tunsigned int i;\n\n\t \n\tif (!soc_device_match(soc_needs_opt_in))\n\t\treturn true;\n\n\t \n\tif (soc_device_match(soc_denylist))\n\t\treturn false;\n\n\t \n\tif (dev_is_pci(dev))\n\t\treturn true;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(devices_allowlist); i++) {\n\t\tif (!strcmp(dev_name(dev), devices_allowlist[i]))\n\t\t\treturn true;\n\t}\n\n\t \n\treturn false;\n}\n\nstatic int ipmmu_of_xlate(struct device *dev,\n\t\t\t  struct of_phandle_args *spec)\n{\n\tif (!ipmmu_device_is_allowed(dev))\n\t\treturn -ENODEV;\n\n\tiommu_fwspec_add_ids(dev, spec->args, 1);\n\n\t \n\tif (to_ipmmu(dev))\n\t\treturn 0;\n\n\treturn ipmmu_init_platform_device(dev, spec);\n}\n\nstatic int ipmmu_init_arm_mapping(struct device *dev)\n{\n\tstruct ipmmu_vmsa_device *mmu = to_ipmmu(dev);\n\tint ret;\n\n\t \n\tif (!mmu->mapping) {\n\t\tstruct dma_iommu_mapping *mapping;\n\n\t\tmapping = arm_iommu_create_mapping(&platform_bus_type,\n\t\t\t\t\t\t   SZ_1G, SZ_2G);\n\t\tif (IS_ERR(mapping)) {\n\t\t\tdev_err(mmu->dev, \"failed to create ARM IOMMU mapping\\n\");\n\t\t\tret = PTR_ERR(mapping);\n\t\t\tgoto error;\n\t\t}\n\n\t\tmmu->mapping = mapping;\n\t}\n\n\t \n\tret = arm_iommu_attach_device(dev, mmu->mapping);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"Failed to attach device to VA mapping\\n\");\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\tif (mmu->mapping)\n\t\tarm_iommu_release_mapping(mmu->mapping);\n\n\treturn ret;\n}\n\nstatic struct iommu_device *ipmmu_probe_device(struct device *dev)\n{\n\tstruct ipmmu_vmsa_device *mmu = to_ipmmu(dev);\n\n\t \n\tif (!mmu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\treturn &mmu->iommu;\n}\n\nstatic void ipmmu_probe_finalize(struct device *dev)\n{\n\tint ret = 0;\n\n\tif (IS_ENABLED(CONFIG_ARM) && !IS_ENABLED(CONFIG_IOMMU_DMA))\n\t\tret = ipmmu_init_arm_mapping(dev);\n\n\tif (ret)\n\t\tdev_err(dev, \"Can't create IOMMU mapping - DMA-OPS will not work\\n\");\n}\n\nstatic void ipmmu_release_device(struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct ipmmu_vmsa_device *mmu = to_ipmmu(dev);\n\tunsigned int i;\n\n\tfor (i = 0; i < fwspec->num_ids; ++i) {\n\t\tunsigned int utlb = fwspec->ids[i];\n\n\t\tipmmu_imuctr_write(mmu, utlb, 0);\n\t\tmmu->utlb_ctx[utlb] = IPMMU_CTX_INVALID;\n\t}\n\n\tarm_iommu_release_mapping(mmu->mapping);\n}\n\nstatic struct iommu_group *ipmmu_find_group(struct device *dev)\n{\n\tstruct ipmmu_vmsa_device *mmu = to_ipmmu(dev);\n\tstruct iommu_group *group;\n\n\tif (mmu->group)\n\t\treturn iommu_group_ref_get(mmu->group);\n\n\tgroup = iommu_group_alloc();\n\tif (!IS_ERR(group))\n\t\tmmu->group = group;\n\n\treturn group;\n}\n\nstatic const struct iommu_ops ipmmu_ops = {\n\t.domain_alloc = ipmmu_domain_alloc,\n\t.probe_device = ipmmu_probe_device,\n\t.release_device = ipmmu_release_device,\n\t.probe_finalize = ipmmu_probe_finalize,\n\t.device_group = IS_ENABLED(CONFIG_ARM) && !IS_ENABLED(CONFIG_IOMMU_DMA)\n\t\t\t? generic_device_group : ipmmu_find_group,\n\t.pgsize_bitmap = SZ_1G | SZ_2M | SZ_4K,\n\t.of_xlate = ipmmu_of_xlate,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= ipmmu_attach_device,\n\t\t.map_pages\t= ipmmu_map,\n\t\t.unmap_pages\t= ipmmu_unmap,\n\t\t.flush_iotlb_all = ipmmu_flush_iotlb_all,\n\t\t.iotlb_sync\t= ipmmu_iotlb_sync,\n\t\t.iova_to_phys\t= ipmmu_iova_to_phys,\n\t\t.free\t\t= ipmmu_domain_free,\n\t}\n};\n\n \n\nstatic void ipmmu_device_reset(struct ipmmu_vmsa_device *mmu)\n{\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < mmu->num_ctx; ++i)\n\t\tipmmu_ctx_write(mmu, i, IMCTR, 0);\n}\n\nstatic const struct ipmmu_features ipmmu_features_default = {\n\t.use_ns_alias_offset = true,\n\t.has_cache_leaf_nodes = false,\n\t.number_of_contexts = 1,  \n\t.num_utlbs = 32,\n\t.setup_imbuscr = true,\n\t.twobit_imttbcr_sl0 = false,\n\t.reserved_context = false,\n\t.cache_snoop = true,\n\t.ctx_offset_base = 0,\n\t.ctx_offset_stride = 0x40,\n\t.utlb_offset_base = 0,\n};\n\nstatic const struct ipmmu_features ipmmu_features_rcar_gen3 = {\n\t.use_ns_alias_offset = false,\n\t.has_cache_leaf_nodes = true,\n\t.number_of_contexts = 8,\n\t.num_utlbs = 48,\n\t.setup_imbuscr = false,\n\t.twobit_imttbcr_sl0 = true,\n\t.reserved_context = true,\n\t.cache_snoop = false,\n\t.ctx_offset_base = 0,\n\t.ctx_offset_stride = 0x40,\n\t.utlb_offset_base = 0,\n};\n\nstatic const struct ipmmu_features ipmmu_features_rcar_gen4 = {\n\t.use_ns_alias_offset = false,\n\t.has_cache_leaf_nodes = true,\n\t.number_of_contexts = 16,\n\t.num_utlbs = 64,\n\t.setup_imbuscr = false,\n\t.twobit_imttbcr_sl0 = true,\n\t.reserved_context = true,\n\t.cache_snoop = false,\n\t.ctx_offset_base = 0x10000,\n\t.ctx_offset_stride = 0x1040,\n\t.utlb_offset_base = 0x3000,\n};\n\nstatic const struct of_device_id ipmmu_of_ids[] = {\n\t{\n\t\t.compatible = \"renesas,ipmmu-vmsa\",\n\t\t.data = &ipmmu_features_default,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a774a1\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a774b1\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a774c0\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a774e1\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a7795\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a7796\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77961\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77965\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77970\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77980\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77990\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a77995\",\n\t\t.data = &ipmmu_features_rcar_gen3,\n\t}, {\n\t\t.compatible = \"renesas,ipmmu-r8a779a0\",\n\t\t.data = &ipmmu_features_rcar_gen4,\n\t}, {\n\t\t.compatible = \"renesas,rcar-gen4-ipmmu-vmsa\",\n\t\t.data = &ipmmu_features_rcar_gen4,\n\t}, {\n\t\t \n\t},\n};\n\nstatic int ipmmu_probe(struct platform_device *pdev)\n{\n\tstruct ipmmu_vmsa_device *mmu;\n\tstruct resource *res;\n\tint irq;\n\tint ret;\n\n\tmmu = devm_kzalloc(&pdev->dev, sizeof(*mmu), GFP_KERNEL);\n\tif (!mmu) {\n\t\tdev_err(&pdev->dev, \"cannot allocate device data\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmmu->dev = &pdev->dev;\n\tspin_lock_init(&mmu->lock);\n\tbitmap_zero(mmu->ctx, IPMMU_CTX_MAX);\n\tmmu->features = of_device_get_match_data(&pdev->dev);\n\tmemset(mmu->utlb_ctx, IPMMU_CTX_INVALID, mmu->features->num_utlbs);\n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(40));\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tmmu->base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(mmu->base))\n\t\treturn PTR_ERR(mmu->base);\n\n\t \n\tif (mmu->features->use_ns_alias_offset)\n\t\tmmu->base += IM_NS_ALIAS_OFFSET;\n\n\tmmu->num_ctx = min(IPMMU_CTX_MAX, mmu->features->number_of_contexts);\n\n\t \n\tif (!mmu->features->has_cache_leaf_nodes ||\n\t    !of_property_present(pdev->dev.of_node, \"renesas,ipmmu-main\"))\n\t\tmmu->root = mmu;\n\telse\n\t\tmmu->root = ipmmu_find_root();\n\n\t \n\tif (!mmu->root)\n\t\treturn -EPROBE_DEFER;\n\n\t \n\tif (ipmmu_is_root(mmu)) {\n\t\tirq = platform_get_irq(pdev, 0);\n\t\tif (irq < 0)\n\t\t\treturn irq;\n\n\t\tret = devm_request_irq(&pdev->dev, irq, ipmmu_irq, 0,\n\t\t\t\t       dev_name(&pdev->dev), mmu);\n\t\tif (ret < 0) {\n\t\t\tdev_err(&pdev->dev, \"failed to request IRQ %d\\n\", irq);\n\t\t\treturn ret;\n\t\t}\n\n\t\tipmmu_device_reset(mmu);\n\n\t\tif (mmu->features->reserved_context) {\n\t\t\tdev_info(&pdev->dev, \"IPMMU context 0 is reserved\\n\");\n\t\t\tset_bit(0, mmu->ctx);\n\t\t}\n\t}\n\n\t \n\tif (!mmu->features->has_cache_leaf_nodes || !ipmmu_is_root(mmu)) {\n\t\tret = iommu_device_sysfs_add(&mmu->iommu, &pdev->dev, NULL,\n\t\t\t\t\t     dev_name(&pdev->dev));\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = iommu_device_register(&mmu->iommu, &ipmmu_ops, &pdev->dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\n\tplatform_set_drvdata(pdev, mmu);\n\n\treturn 0;\n}\n\nstatic void ipmmu_remove(struct platform_device *pdev)\n{\n\tstruct ipmmu_vmsa_device *mmu = platform_get_drvdata(pdev);\n\n\tiommu_device_sysfs_remove(&mmu->iommu);\n\tiommu_device_unregister(&mmu->iommu);\n\n\tarm_iommu_release_mapping(mmu->mapping);\n\n\tipmmu_device_reset(mmu);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int ipmmu_resume_noirq(struct device *dev)\n{\n\tstruct ipmmu_vmsa_device *mmu = dev_get_drvdata(dev);\n\tunsigned int i;\n\n\t \n\tif (ipmmu_is_root(mmu)) {\n\t\tipmmu_device_reset(mmu);\n\n\t\tfor (i = 0; i < mmu->num_ctx; i++) {\n\t\t\tif (!mmu->domains[i])\n\t\t\t\tcontinue;\n\n\t\t\tipmmu_domain_setup_context(mmu->domains[i]);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < mmu->features->num_utlbs; i++) {\n\t\tif (mmu->utlb_ctx[i] == IPMMU_CTX_INVALID)\n\t\t\tcontinue;\n\n\t\tipmmu_utlb_enable(mmu->root->domains[mmu->utlb_ctx[i]], i);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops ipmmu_pm  = {\n\tSET_NOIRQ_SYSTEM_SLEEP_PM_OPS(NULL, ipmmu_resume_noirq)\n};\n#define DEV_PM_OPS\t&ipmmu_pm\n#else\n#define DEV_PM_OPS\tNULL\n#endif  \n\nstatic struct platform_driver ipmmu_driver = {\n\t.driver = {\n\t\t.name = \"ipmmu-vmsa\",\n\t\t.of_match_table = of_match_ptr(ipmmu_of_ids),\n\t\t.pm = DEV_PM_OPS,\n\t},\n\t.probe = ipmmu_probe,\n\t.remove_new = ipmmu_remove,\n};\nbuiltin_platform_driver(ipmmu_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}