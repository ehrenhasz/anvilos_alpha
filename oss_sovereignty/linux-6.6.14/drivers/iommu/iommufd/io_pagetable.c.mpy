{
  "module_name": "io_pagetable.c",
  "hash_id": "e93c51c4f3c4f4dafa28c4e51ed52fbbdb36216ab8b92f5bfb29de7d76624b5d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/iommufd/io_pagetable.c",
  "human_readable_source": "\n \n#include <linux/iommufd.h>\n#include <linux/lockdep.h>\n#include <linux/iommu.h>\n#include <linux/sched/mm.h>\n#include <linux/err.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n\n#include \"io_pagetable.h\"\n#include \"double_span.h\"\n\nstruct iopt_pages_list {\n\tstruct iopt_pages *pages;\n\tstruct iopt_area *area;\n\tstruct list_head next;\n\tunsigned long start_byte;\n\tunsigned long length;\n};\n\nstruct iopt_area *iopt_area_contig_init(struct iopt_area_contig_iter *iter,\n\t\t\t\t\tstruct io_pagetable *iopt,\n\t\t\t\t\tunsigned long iova,\n\t\t\t\t\tunsigned long last_iova)\n{\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\n\titer->cur_iova = iova;\n\titer->last_iova = last_iova;\n\titer->area = iopt_area_iter_first(iopt, iova, iova);\n\tif (!iter->area)\n\t\treturn NULL;\n\tif (!iter->area->pages) {\n\t\titer->area = NULL;\n\t\treturn NULL;\n\t}\n\treturn iter->area;\n}\n\nstruct iopt_area *iopt_area_contig_next(struct iopt_area_contig_iter *iter)\n{\n\tunsigned long last_iova;\n\n\tif (!iter->area)\n\t\treturn NULL;\n\tlast_iova = iopt_area_last_iova(iter->area);\n\tif (iter->last_iova <= last_iova)\n\t\treturn NULL;\n\n\titer->cur_iova = last_iova + 1;\n\titer->area = iopt_area_iter_next(iter->area, iter->cur_iova,\n\t\t\t\t\t iter->last_iova);\n\tif (!iter->area)\n\t\treturn NULL;\n\tif (iter->cur_iova != iopt_area_iova(iter->area) ||\n\t    !iter->area->pages) {\n\t\titer->area = NULL;\n\t\treturn NULL;\n\t}\n\treturn iter->area;\n}\n\nstatic bool __alloc_iova_check_hole(struct interval_tree_double_span_iter *span,\n\t\t\t\t    unsigned long length,\n\t\t\t\t    unsigned long iova_alignment,\n\t\t\t\t    unsigned long page_offset)\n{\n\tif (span->is_used || span->last_hole - span->start_hole < length - 1)\n\t\treturn false;\n\n\tspan->start_hole = ALIGN(span->start_hole, iova_alignment) |\n\t\t\t   page_offset;\n\tif (span->start_hole > span->last_hole ||\n\t    span->last_hole - span->start_hole < length - 1)\n\t\treturn false;\n\treturn true;\n}\n\nstatic bool __alloc_iova_check_used(struct interval_tree_span_iter *span,\n\t\t\t\t    unsigned long length,\n\t\t\t\t    unsigned long iova_alignment,\n\t\t\t\t    unsigned long page_offset)\n{\n\tif (span->is_hole || span->last_used - span->start_used < length - 1)\n\t\treturn false;\n\n\tspan->start_used = ALIGN(span->start_used, iova_alignment) |\n\t\t\t   page_offset;\n\tif (span->start_used > span->last_used ||\n\t    span->last_used - span->start_used < length - 1)\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic int iopt_alloc_iova(struct io_pagetable *iopt, unsigned long *iova,\n\t\t\t   unsigned long uptr, unsigned long length)\n{\n\tunsigned long page_offset = uptr % PAGE_SIZE;\n\tstruct interval_tree_double_span_iter used_span;\n\tstruct interval_tree_span_iter allowed_span;\n\tunsigned long iova_alignment;\n\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\n\t \n\tif (length == 0 || length >= ULONG_MAX / 2)\n\t\treturn -EOVERFLOW;\n\n\t \n\tif (!uptr)\n\t\tiova_alignment = roundup_pow_of_two(length);\n\telse\n\t\tiova_alignment = min_t(unsigned long,\n\t\t\t\t       roundup_pow_of_two(length),\n\t\t\t\t       1UL << __ffs64(uptr));\n\n\tif (iova_alignment < iopt->iova_alignment)\n\t\treturn -EINVAL;\n\n\tinterval_tree_for_each_span(&allowed_span, &iopt->allowed_itree,\n\t\t\t\t    PAGE_SIZE, ULONG_MAX - PAGE_SIZE) {\n\t\tif (RB_EMPTY_ROOT(&iopt->allowed_itree.rb_root)) {\n\t\t\tallowed_span.start_used = PAGE_SIZE;\n\t\t\tallowed_span.last_used = ULONG_MAX - PAGE_SIZE;\n\t\t\tallowed_span.is_hole = false;\n\t\t}\n\n\t\tif (!__alloc_iova_check_used(&allowed_span, length,\n\t\t\t\t\t     iova_alignment, page_offset))\n\t\t\tcontinue;\n\n\t\tinterval_tree_for_each_double_span(\n\t\t\t&used_span, &iopt->reserved_itree, &iopt->area_itree,\n\t\t\tallowed_span.start_used, allowed_span.last_used) {\n\t\t\tif (!__alloc_iova_check_hole(&used_span, length,\n\t\t\t\t\t\t     iova_alignment,\n\t\t\t\t\t\t     page_offset))\n\t\t\t\tcontinue;\n\n\t\t\t*iova = used_span.start_hole;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOSPC;\n}\n\nstatic int iopt_check_iova(struct io_pagetable *iopt, unsigned long iova,\n\t\t\t   unsigned long length)\n{\n\tunsigned long last;\n\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\n\tif ((iova & (iopt->iova_alignment - 1)))\n\t\treturn -EINVAL;\n\n\tif (check_add_overflow(iova, length - 1, &last))\n\t\treturn -EOVERFLOW;\n\n\t \n\tif (iopt_reserved_iter_first(iopt, iova, last))\n\t\treturn -EINVAL;\n\n\t \n\tif (iopt_area_iter_first(iopt, iova, last))\n\t\treturn -EEXIST;\n\treturn 0;\n}\n\n \nstatic int iopt_insert_area(struct io_pagetable *iopt, struct iopt_area *area,\n\t\t\t    struct iopt_pages *pages, unsigned long iova,\n\t\t\t    unsigned long start_byte, unsigned long length,\n\t\t\t    int iommu_prot)\n{\n\tlockdep_assert_held_write(&iopt->iova_rwsem);\n\n\tif ((iommu_prot & IOMMU_WRITE) && !pages->writable)\n\t\treturn -EPERM;\n\n\tarea->iommu_prot = iommu_prot;\n\tarea->page_offset = start_byte % PAGE_SIZE;\n\tif (area->page_offset & (iopt->iova_alignment - 1))\n\t\treturn -EINVAL;\n\n\tarea->node.start = iova;\n\tif (check_add_overflow(iova, length - 1, &area->node.last))\n\t\treturn -EOVERFLOW;\n\n\tarea->pages_node.start = start_byte / PAGE_SIZE;\n\tif (check_add_overflow(start_byte, length - 1, &area->pages_node.last))\n\t\treturn -EOVERFLOW;\n\tarea->pages_node.last = area->pages_node.last / PAGE_SIZE;\n\tif (WARN_ON(area->pages_node.last >= pages->npages))\n\t\treturn -EOVERFLOW;\n\n\t \n\tarea->iopt = iopt;\n\tinterval_tree_insert(&area->node, &iopt->area_itree);\n\treturn 0;\n}\n\nstatic struct iopt_area *iopt_area_alloc(void)\n{\n\tstruct iopt_area *area;\n\n\tarea = kzalloc(sizeof(*area), GFP_KERNEL_ACCOUNT);\n\tif (!area)\n\t\treturn NULL;\n\tRB_CLEAR_NODE(&area->node.rb);\n\tRB_CLEAR_NODE(&area->pages_node.rb);\n\treturn area;\n}\n\nstatic int iopt_alloc_area_pages(struct io_pagetable *iopt,\n\t\t\t\t struct list_head *pages_list,\n\t\t\t\t unsigned long length, unsigned long *dst_iova,\n\t\t\t\t int iommu_prot, unsigned int flags)\n{\n\tstruct iopt_pages_list *elm;\n\tunsigned long iova;\n\tint rc = 0;\n\n\tlist_for_each_entry(elm, pages_list, next) {\n\t\telm->area = iopt_area_alloc();\n\t\tif (!elm->area)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tdown_write(&iopt->iova_rwsem);\n\tif ((length & (iopt->iova_alignment - 1)) || !length) {\n\t\trc = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (flags & IOPT_ALLOC_IOVA) {\n\t\t \n\t\telm = list_first_entry(pages_list, struct iopt_pages_list,\n\t\t\t\t       next);\n\t\trc = iopt_alloc_iova(\n\t\t\tiopt, dst_iova,\n\t\t\t(uintptr_t)elm->pages->uptr + elm->start_byte, length);\n\t\tif (rc)\n\t\t\tgoto out_unlock;\n\t\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t\t    WARN_ON(iopt_check_iova(iopt, *dst_iova, length))) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\trc = iopt_check_iova(iopt, *dst_iova, length);\n\t\tif (rc)\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\tiova = *dst_iova;\n\tlist_for_each_entry(elm, pages_list, next) {\n\t\trc = iopt_insert_area(iopt, elm->area, elm->pages, iova,\n\t\t\t\t      elm->start_byte, elm->length, iommu_prot);\n\t\tif (rc)\n\t\t\tgoto out_unlock;\n\t\tiova += elm->length;\n\t}\n\nout_unlock:\n\tup_write(&iopt->iova_rwsem);\n\treturn rc;\n}\n\nstatic void iopt_abort_area(struct iopt_area *area)\n{\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(area->pages);\n\tif (area->iopt) {\n\t\tdown_write(&area->iopt->iova_rwsem);\n\t\tinterval_tree_remove(&area->node, &area->iopt->area_itree);\n\t\tup_write(&area->iopt->iova_rwsem);\n\t}\n\tkfree(area);\n}\n\nvoid iopt_free_pages_list(struct list_head *pages_list)\n{\n\tstruct iopt_pages_list *elm;\n\n\twhile ((elm = list_first_entry_or_null(pages_list,\n\t\t\t\t\t       struct iopt_pages_list, next))) {\n\t\tif (elm->area)\n\t\t\tiopt_abort_area(elm->area);\n\t\tif (elm->pages)\n\t\t\tiopt_put_pages(elm->pages);\n\t\tlist_del(&elm->next);\n\t\tkfree(elm);\n\t}\n}\n\nstatic int iopt_fill_domains_pages(struct list_head *pages_list)\n{\n\tstruct iopt_pages_list *undo_elm;\n\tstruct iopt_pages_list *elm;\n\tint rc;\n\n\tlist_for_each_entry(elm, pages_list, next) {\n\t\trc = iopt_area_fill_domains(elm->area, elm->pages);\n\t\tif (rc)\n\t\t\tgoto err_undo;\n\t}\n\treturn 0;\n\nerr_undo:\n\tlist_for_each_entry(undo_elm, pages_list, next) {\n\t\tif (undo_elm == elm)\n\t\t\tbreak;\n\t\tiopt_area_unfill_domains(undo_elm->area, undo_elm->pages);\n\t}\n\treturn rc;\n}\n\nint iopt_map_pages(struct io_pagetable *iopt, struct list_head *pages_list,\n\t\t   unsigned long length, unsigned long *dst_iova,\n\t\t   int iommu_prot, unsigned int flags)\n{\n\tstruct iopt_pages_list *elm;\n\tint rc;\n\n\trc = iopt_alloc_area_pages(iopt, pages_list, length, dst_iova,\n\t\t\t\t   iommu_prot, flags);\n\tif (rc)\n\t\treturn rc;\n\n\tdown_read(&iopt->domains_rwsem);\n\trc = iopt_fill_domains_pages(pages_list);\n\tif (rc)\n\t\tgoto out_unlock_domains;\n\n\tdown_write(&iopt->iova_rwsem);\n\tlist_for_each_entry(elm, pages_list, next) {\n\t\t \n\t\telm->area->pages = elm->pages;\n\t\telm->pages = NULL;\n\t\telm->area = NULL;\n\t}\n\tup_write(&iopt->iova_rwsem);\nout_unlock_domains:\n\tup_read(&iopt->domains_rwsem);\n\treturn rc;\n}\n\n \nint iopt_map_user_pages(struct iommufd_ctx *ictx, struct io_pagetable *iopt,\n\t\t\tunsigned long *iova, void __user *uptr,\n\t\t\tunsigned long length, int iommu_prot,\n\t\t\tunsigned int flags)\n{\n\tstruct iopt_pages_list elm = {};\n\tLIST_HEAD(pages_list);\n\tint rc;\n\n\telm.pages = iopt_alloc_pages(uptr, length, iommu_prot & IOMMU_WRITE);\n\tif (IS_ERR(elm.pages))\n\t\treturn PTR_ERR(elm.pages);\n\tif (ictx->account_mode == IOPT_PAGES_ACCOUNT_MM &&\n\t    elm.pages->account_mode == IOPT_PAGES_ACCOUNT_USER)\n\t\telm.pages->account_mode = IOPT_PAGES_ACCOUNT_MM;\n\telm.start_byte = uptr - elm.pages->uptr;\n\telm.length = length;\n\tlist_add(&elm.next, &pages_list);\n\n\trc = iopt_map_pages(iopt, &pages_list, length, iova, iommu_prot, flags);\n\tif (rc) {\n\t\tif (elm.area)\n\t\t\tiopt_abort_area(elm.area);\n\t\tif (elm.pages)\n\t\t\tiopt_put_pages(elm.pages);\n\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nint iopt_get_pages(struct io_pagetable *iopt, unsigned long iova,\n\t\t   unsigned long length, struct list_head *pages_list)\n{\n\tstruct iopt_area_contig_iter iter;\n\tunsigned long last_iova;\n\tstruct iopt_area *area;\n\tint rc;\n\n\tif (!length)\n\t\treturn -EINVAL;\n\tif (check_add_overflow(iova, length - 1, &last_iova))\n\t\treturn -EOVERFLOW;\n\n\tdown_read(&iopt->iova_rwsem);\n\tiopt_for_each_contig_area(&iter, area, iopt, iova, last_iova) {\n\t\tstruct iopt_pages_list *elm;\n\t\tunsigned long last = min(last_iova, iopt_area_last_iova(area));\n\n\t\telm = kzalloc(sizeof(*elm), GFP_KERNEL_ACCOUNT);\n\t\tif (!elm) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\t\telm->start_byte = iopt_area_start_byte(area, iter.cur_iova);\n\t\telm->pages = area->pages;\n\t\telm->length = (last - iter.cur_iova) + 1;\n\t\tkref_get(&elm->pages->kref);\n\t\tlist_add_tail(&elm->next, pages_list);\n\t}\n\tif (!iopt_area_contig_done(&iter)) {\n\t\trc = -ENOENT;\n\t\tgoto err_free;\n\t}\n\tup_read(&iopt->iova_rwsem);\n\treturn 0;\nerr_free:\n\tup_read(&iopt->iova_rwsem);\n\tiopt_free_pages_list(pages_list);\n\treturn rc;\n}\n\nstatic int iopt_unmap_iova_range(struct io_pagetable *iopt, unsigned long start,\n\t\t\t\t unsigned long last, unsigned long *unmapped)\n{\n\tstruct iopt_area *area;\n\tunsigned long unmapped_bytes = 0;\n\tunsigned int tries = 0;\n\tint rc = -ENOENT;\n\n\t \nagain:\n\tdown_read(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\twhile ((area = iopt_area_iter_first(iopt, start, last))) {\n\t\tunsigned long area_last = iopt_area_last_iova(area);\n\t\tunsigned long area_first = iopt_area_iova(area);\n\t\tstruct iopt_pages *pages;\n\n\t\t \n\t\tif (!area->pages) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out_unlock_iova;\n\t\t}\n\n\t\tif (area_first < start || area_last > last) {\n\t\t\trc = -ENOENT;\n\t\t\tgoto out_unlock_iova;\n\t\t}\n\n\t\tif (area_first != start)\n\t\t\ttries = 0;\n\n\t\t \n\t\tif (area->num_accesses) {\n\t\t\tsize_t length = iopt_area_length(area);\n\n\t\t\tstart = area_first;\n\t\t\tarea->prevent_access = true;\n\t\t\tup_write(&iopt->iova_rwsem);\n\t\t\tup_read(&iopt->domains_rwsem);\n\n\t\t\tiommufd_access_notify_unmap(iopt, area_first, length);\n\t\t\t \n\t\t\ttries++;\n\t\t\tif (WARN_ON(tries > 100))\n\t\t\t\treturn -EDEADLOCK;\n\t\t\tgoto again;\n\t\t}\n\n\t\tpages = area->pages;\n\t\tarea->pages = NULL;\n\t\tup_write(&iopt->iova_rwsem);\n\n\t\tiopt_area_unfill_domains(area, pages);\n\t\tiopt_abort_area(area);\n\t\tiopt_put_pages(pages);\n\n\t\tunmapped_bytes += area_last - area_first + 1;\n\n\t\tdown_write(&iopt->iova_rwsem);\n\t}\n\tif (unmapped_bytes)\n\t\trc = 0;\n\nout_unlock_iova:\n\tup_write(&iopt->iova_rwsem);\n\tup_read(&iopt->domains_rwsem);\n\tif (unmapped)\n\t\t*unmapped = unmapped_bytes;\n\treturn rc;\n}\n\n \nint iopt_unmap_iova(struct io_pagetable *iopt, unsigned long iova,\n\t\t    unsigned long length, unsigned long *unmapped)\n{\n\tunsigned long iova_last;\n\n\tif (!length)\n\t\treturn -EINVAL;\n\n\tif (check_add_overflow(iova, length - 1, &iova_last))\n\t\treturn -EOVERFLOW;\n\n\treturn iopt_unmap_iova_range(iopt, iova, iova_last, unmapped);\n}\n\nint iopt_unmap_all(struct io_pagetable *iopt, unsigned long *unmapped)\n{\n\tint rc;\n\n\trc = iopt_unmap_iova_range(iopt, 0, ULONG_MAX, unmapped);\n\t \n\tif (rc == -ENOENT)\n\t\treturn 0;\n\treturn rc;\n}\n\n \nint iopt_set_allow_iova(struct io_pagetable *iopt,\n\t\t\tstruct rb_root_cached *allowed_iova)\n{\n\tstruct iopt_allowed *allowed;\n\n\tdown_write(&iopt->iova_rwsem);\n\tswap(*allowed_iova, iopt->allowed_itree);\n\n\tfor (allowed = iopt_allowed_iter_first(iopt, 0, ULONG_MAX); allowed;\n\t     allowed = iopt_allowed_iter_next(allowed, 0, ULONG_MAX)) {\n\t\tif (iopt_reserved_iter_first(iopt, allowed->node.start,\n\t\t\t\t\t     allowed->node.last)) {\n\t\t\tswap(*allowed_iova, iopt->allowed_itree);\n\t\t\tup_write(&iopt->iova_rwsem);\n\t\t\treturn -EADDRINUSE;\n\t\t}\n\t}\n\tup_write(&iopt->iova_rwsem);\n\treturn 0;\n}\n\nint iopt_reserve_iova(struct io_pagetable *iopt, unsigned long start,\n\t\t      unsigned long last, void *owner)\n{\n\tstruct iopt_reserved *reserved;\n\n\tlockdep_assert_held_write(&iopt->iova_rwsem);\n\n\tif (iopt_area_iter_first(iopt, start, last) ||\n\t    iopt_allowed_iter_first(iopt, start, last))\n\t\treturn -EADDRINUSE;\n\n\treserved = kzalloc(sizeof(*reserved), GFP_KERNEL_ACCOUNT);\n\tif (!reserved)\n\t\treturn -ENOMEM;\n\treserved->node.start = start;\n\treserved->node.last = last;\n\treserved->owner = owner;\n\tinterval_tree_insert(&reserved->node, &iopt->reserved_itree);\n\treturn 0;\n}\n\nstatic void __iopt_remove_reserved_iova(struct io_pagetable *iopt, void *owner)\n{\n\tstruct iopt_reserved *reserved, *next;\n\n\tlockdep_assert_held_write(&iopt->iova_rwsem);\n\n\tfor (reserved = iopt_reserved_iter_first(iopt, 0, ULONG_MAX); reserved;\n\t     reserved = next) {\n\t\tnext = iopt_reserved_iter_next(reserved, 0, ULONG_MAX);\n\n\t\tif (reserved->owner == owner) {\n\t\t\tinterval_tree_remove(&reserved->node,\n\t\t\t\t\t     &iopt->reserved_itree);\n\t\t\tkfree(reserved);\n\t\t}\n\t}\n}\n\nvoid iopt_remove_reserved_iova(struct io_pagetable *iopt, void *owner)\n{\n\tdown_write(&iopt->iova_rwsem);\n\t__iopt_remove_reserved_iova(iopt, owner);\n\tup_write(&iopt->iova_rwsem);\n}\n\nvoid iopt_init_table(struct io_pagetable *iopt)\n{\n\tinit_rwsem(&iopt->iova_rwsem);\n\tinit_rwsem(&iopt->domains_rwsem);\n\tiopt->area_itree = RB_ROOT_CACHED;\n\tiopt->allowed_itree = RB_ROOT_CACHED;\n\tiopt->reserved_itree = RB_ROOT_CACHED;\n\txa_init_flags(&iopt->domains, XA_FLAGS_ACCOUNT);\n\txa_init_flags(&iopt->access_list, XA_FLAGS_ALLOC);\n\n\t \n\tiopt->iova_alignment = 1;\n}\n\nvoid iopt_destroy_table(struct io_pagetable *iopt)\n{\n\tstruct interval_tree_node *node;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tiopt_remove_reserved_iova(iopt, NULL);\n\n\twhile ((node = interval_tree_iter_first(&iopt->allowed_itree, 0,\n\t\t\t\t\t\tULONG_MAX))) {\n\t\tinterval_tree_remove(node, &iopt->allowed_itree);\n\t\tkfree(container_of(node, struct iopt_allowed, node));\n\t}\n\n\tWARN_ON(!RB_EMPTY_ROOT(&iopt->reserved_itree.rb_root));\n\tWARN_ON(!xa_empty(&iopt->domains));\n\tWARN_ON(!xa_empty(&iopt->access_list));\n\tWARN_ON(!RB_EMPTY_ROOT(&iopt->area_itree.rb_root));\n}\n\n \nstatic void iopt_unfill_domain(struct io_pagetable *iopt,\n\t\t\t       struct iommu_domain *domain)\n{\n\tstruct iopt_area *area;\n\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\tlockdep_assert_held_write(&iopt->domains_rwsem);\n\n\t \n\tif (iopt->next_domain_id != 0) {\n\t\t \n\t\tstruct iommu_domain *storage_domain =\n\t\t\txa_load(&iopt->domains, 0);\n\n\t\tfor (area = iopt_area_iter_first(iopt, 0, ULONG_MAX); area;\n\t\t     area = iopt_area_iter_next(area, 0, ULONG_MAX)) {\n\t\t\tstruct iopt_pages *pages = area->pages;\n\n\t\t\tif (!pages)\n\t\t\t\tcontinue;\n\n\t\t\tmutex_lock(&pages->mutex);\n\t\t\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\t\t\tWARN_ON(!area->storage_domain);\n\t\t\tif (area->storage_domain == domain)\n\t\t\t\tarea->storage_domain = storage_domain;\n\t\t\tmutex_unlock(&pages->mutex);\n\n\t\t\tiopt_area_unmap_domain(area, domain);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (area = iopt_area_iter_first(iopt, 0, ULONG_MAX); area;\n\t     area = iopt_area_iter_next(area, 0, ULONG_MAX)) {\n\t\tstruct iopt_pages *pages = area->pages;\n\n\t\tif (!pages)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&pages->mutex);\n\t\tinterval_tree_remove(&area->pages_node, &pages->domains_itree);\n\t\tWARN_ON(area->storage_domain != domain);\n\t\tarea->storage_domain = NULL;\n\t\tiopt_area_unfill_domain(area, pages, domain);\n\t\tmutex_unlock(&pages->mutex);\n\t}\n}\n\n \nstatic int iopt_fill_domain(struct io_pagetable *iopt,\n\t\t\t    struct iommu_domain *domain)\n{\n\tstruct iopt_area *end_area;\n\tstruct iopt_area *area;\n\tint rc;\n\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\tlockdep_assert_held_write(&iopt->domains_rwsem);\n\n\tfor (area = iopt_area_iter_first(iopt, 0, ULONG_MAX); area;\n\t     area = iopt_area_iter_next(area, 0, ULONG_MAX)) {\n\t\tstruct iopt_pages *pages = area->pages;\n\n\t\tif (!pages)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&pages->mutex);\n\t\trc = iopt_area_fill_domain(area, domain);\n\t\tif (rc) {\n\t\t\tmutex_unlock(&pages->mutex);\n\t\t\tgoto out_unfill;\n\t\t}\n\t\tif (!area->storage_domain) {\n\t\t\tWARN_ON(iopt->next_domain_id != 0);\n\t\t\tarea->storage_domain = domain;\n\t\t\tinterval_tree_insert(&area->pages_node,\n\t\t\t\t\t     &pages->domains_itree);\n\t\t}\n\t\tmutex_unlock(&pages->mutex);\n\t}\n\treturn 0;\n\nout_unfill:\n\tend_area = area;\n\tfor (area = iopt_area_iter_first(iopt, 0, ULONG_MAX); area;\n\t     area = iopt_area_iter_next(area, 0, ULONG_MAX)) {\n\t\tstruct iopt_pages *pages = area->pages;\n\n\t\tif (area == end_area)\n\t\t\tbreak;\n\t\tif (!pages)\n\t\t\tcontinue;\n\t\tmutex_lock(&pages->mutex);\n\t\tif (iopt->next_domain_id == 0) {\n\t\t\tinterval_tree_remove(&area->pages_node,\n\t\t\t\t\t     &pages->domains_itree);\n\t\t\tarea->storage_domain = NULL;\n\t\t}\n\t\tiopt_area_unfill_domain(area, pages, domain);\n\t\tmutex_unlock(&pages->mutex);\n\t}\n\treturn rc;\n}\n\n \nstatic int iopt_check_iova_alignment(struct io_pagetable *iopt,\n\t\t\t\t     unsigned long new_iova_alignment)\n{\n\tunsigned long align_mask = new_iova_alignment - 1;\n\tstruct iopt_area *area;\n\n\tlockdep_assert_held(&iopt->iova_rwsem);\n\tlockdep_assert_held(&iopt->domains_rwsem);\n\n\tfor (area = iopt_area_iter_first(iopt, 0, ULONG_MAX); area;\n\t     area = iopt_area_iter_next(area, 0, ULONG_MAX))\n\t\tif ((iopt_area_iova(area) & align_mask) ||\n\t\t    (iopt_area_length(area) & align_mask) ||\n\t\t    (area->page_offset & align_mask))\n\t\t\treturn -EADDRINUSE;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST)) {\n\t\tstruct iommufd_access *access;\n\t\tunsigned long index;\n\n\t\txa_for_each(&iopt->access_list, index, access)\n\t\t\tif (WARN_ON(access->iova_alignment >\n\t\t\t\t    new_iova_alignment))\n\t\t\t\treturn -EADDRINUSE;\n\t}\n\treturn 0;\n}\n\nint iopt_table_add_domain(struct io_pagetable *iopt,\n\t\t\t  struct iommu_domain *domain)\n{\n\tconst struct iommu_domain_geometry *geometry = &domain->geometry;\n\tstruct iommu_domain *iter_domain;\n\tunsigned int new_iova_alignment;\n\tunsigned long index;\n\tint rc;\n\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\n\txa_for_each(&iopt->domains, index, iter_domain) {\n\t\tif (WARN_ON(iter_domain == domain)) {\n\t\t\trc = -EEXIST;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t \n\tnew_iova_alignment = max_t(unsigned long,\n\t\t\t\t   1UL << __ffs(domain->pgsize_bitmap),\n\t\t\t\t   iopt->iova_alignment);\n\tif (new_iova_alignment > PAGE_SIZE) {\n\t\trc = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tif (new_iova_alignment != iopt->iova_alignment) {\n\t\trc = iopt_check_iova_alignment(iopt, new_iova_alignment);\n\t\tif (rc)\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (geometry->aperture_start != 0) {\n\t\trc = iopt_reserve_iova(iopt, 0, geometry->aperture_start - 1,\n\t\t\t\t       domain);\n\t\tif (rc)\n\t\t\tgoto out_reserved;\n\t}\n\tif (geometry->aperture_end != ULONG_MAX) {\n\t\trc = iopt_reserve_iova(iopt, geometry->aperture_end + 1,\n\t\t\t\t       ULONG_MAX, domain);\n\t\tif (rc)\n\t\t\tgoto out_reserved;\n\t}\n\n\trc = xa_reserve(&iopt->domains, iopt->next_domain_id, GFP_KERNEL);\n\tif (rc)\n\t\tgoto out_reserved;\n\n\trc = iopt_fill_domain(iopt, domain);\n\tif (rc)\n\t\tgoto out_release;\n\n\tiopt->iova_alignment = new_iova_alignment;\n\txa_store(&iopt->domains, iopt->next_domain_id, domain, GFP_KERNEL);\n\tiopt->next_domain_id++;\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n\treturn 0;\nout_release:\n\txa_release(&iopt->domains, iopt->next_domain_id);\nout_reserved:\n\t__iopt_remove_reserved_iova(iopt, domain);\nout_unlock:\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n\treturn rc;\n}\n\nstatic int iopt_calculate_iova_alignment(struct io_pagetable *iopt)\n{\n\tunsigned long new_iova_alignment;\n\tstruct iommufd_access *access;\n\tstruct iommu_domain *domain;\n\tunsigned long index;\n\n\tlockdep_assert_held_write(&iopt->iova_rwsem);\n\tlockdep_assert_held(&iopt->domains_rwsem);\n\n\t \n\tif (iopt->disable_large_pages)\n\t\tnew_iova_alignment = PAGE_SIZE;\n\telse\n\t\tnew_iova_alignment = 1;\n\n\txa_for_each(&iopt->domains, index, domain)\n\t\tnew_iova_alignment = max_t(unsigned long,\n\t\t\t\t\t   1UL << __ffs(domain->pgsize_bitmap),\n\t\t\t\t\t   new_iova_alignment);\n\txa_for_each(&iopt->access_list, index, access)\n\t\tnew_iova_alignment = max_t(unsigned long,\n\t\t\t\t\t   access->iova_alignment,\n\t\t\t\t\t   new_iova_alignment);\n\n\tif (new_iova_alignment > iopt->iova_alignment) {\n\t\tint rc;\n\n\t\trc = iopt_check_iova_alignment(iopt, new_iova_alignment);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\tiopt->iova_alignment = new_iova_alignment;\n\treturn 0;\n}\n\nvoid iopt_table_remove_domain(struct io_pagetable *iopt,\n\t\t\t      struct iommu_domain *domain)\n{\n\tstruct iommu_domain *iter_domain = NULL;\n\tunsigned long index;\n\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\n\txa_for_each(&iopt->domains, index, iter_domain)\n\t\tif (iter_domain == domain)\n\t\t\tbreak;\n\tif (WARN_ON(iter_domain != domain) || index >= iopt->next_domain_id)\n\t\tgoto out_unlock;\n\n\t \n\tiopt->next_domain_id--;\n\titer_domain = xa_erase(&iopt->domains, iopt->next_domain_id);\n\tif (index != iopt->next_domain_id)\n\t\txa_store(&iopt->domains, index, iter_domain, GFP_KERNEL);\n\n\tiopt_unfill_domain(iopt, domain);\n\t__iopt_remove_reserved_iova(iopt, domain);\n\n\tWARN_ON(iopt_calculate_iova_alignment(iopt));\nout_unlock:\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n}\n\n \nstatic int iopt_area_split(struct iopt_area *area, unsigned long iova)\n{\n\tunsigned long alignment = area->iopt->iova_alignment;\n\tunsigned long last_iova = iopt_area_last_iova(area);\n\tunsigned long start_iova = iopt_area_iova(area);\n\tunsigned long new_start = iova + 1;\n\tstruct io_pagetable *iopt = area->iopt;\n\tstruct iopt_pages *pages = area->pages;\n\tstruct iopt_area *lhs;\n\tstruct iopt_area *rhs;\n\tint rc;\n\n\tlockdep_assert_held_write(&iopt->iova_rwsem);\n\n\tif (iova == start_iova || iova == last_iova)\n\t\treturn 0;\n\n\tif (!pages || area->prevent_access)\n\t\treturn -EBUSY;\n\n\tif (new_start & (alignment - 1) ||\n\t    iopt_area_start_byte(area, new_start) & (alignment - 1))\n\t\treturn -EINVAL;\n\n\tlhs = iopt_area_alloc();\n\tif (!lhs)\n\t\treturn -ENOMEM;\n\n\trhs = iopt_area_alloc();\n\tif (!rhs) {\n\t\trc = -ENOMEM;\n\t\tgoto err_free_lhs;\n\t}\n\n\tmutex_lock(&pages->mutex);\n\t \n\tif (area->num_accesses) {\n\t\trc = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\t \n\tif (area->storage_domain && !iopt->disable_large_pages) {\n\t\trc = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tinterval_tree_remove(&area->node, &iopt->area_itree);\n\trc = iopt_insert_area(iopt, lhs, area->pages, start_iova,\n\t\t\t      iopt_area_start_byte(area, start_iova),\n\t\t\t      (new_start - 1) - start_iova + 1,\n\t\t\t      area->iommu_prot);\n\tif (WARN_ON(rc))\n\t\tgoto err_insert;\n\n\trc = iopt_insert_area(iopt, rhs, area->pages, new_start,\n\t\t\t      iopt_area_start_byte(area, new_start),\n\t\t\t      last_iova - new_start + 1, area->iommu_prot);\n\tif (WARN_ON(rc))\n\t\tgoto err_remove_lhs;\n\n\t \n\tif (area->storage_domain) {\n\t\tinterval_tree_remove(&area->pages_node, &pages->domains_itree);\n\t\tinterval_tree_insert(&lhs->pages_node, &pages->domains_itree);\n\t\tinterval_tree_insert(&rhs->pages_node, &pages->domains_itree);\n\t}\n\n\tlhs->storage_domain = area->storage_domain;\n\tlhs->pages = area->pages;\n\trhs->storage_domain = area->storage_domain;\n\trhs->pages = area->pages;\n\tkref_get(&rhs->pages->kref);\n\tkfree(area);\n\tmutex_unlock(&pages->mutex);\n\n\t \n\treturn 0;\n\nerr_remove_lhs:\n\tinterval_tree_remove(&lhs->node, &iopt->area_itree);\nerr_insert:\n\tinterval_tree_insert(&area->node, &iopt->area_itree);\nerr_unlock:\n\tmutex_unlock(&pages->mutex);\n\tkfree(rhs);\nerr_free_lhs:\n\tkfree(lhs);\n\treturn rc;\n}\n\nint iopt_cut_iova(struct io_pagetable *iopt, unsigned long *iovas,\n\t\t  size_t num_iovas)\n{\n\tint rc = 0;\n\tint i;\n\n\tdown_write(&iopt->iova_rwsem);\n\tfor (i = 0; i < num_iovas; i++) {\n\t\tstruct iopt_area *area;\n\n\t\tarea = iopt_area_iter_first(iopt, iovas[i], iovas[i]);\n\t\tif (!area)\n\t\t\tcontinue;\n\t\trc = iopt_area_split(area, iovas[i]);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\tup_write(&iopt->iova_rwsem);\n\treturn rc;\n}\n\nvoid iopt_enable_large_pages(struct io_pagetable *iopt)\n{\n\tint rc;\n\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\tWRITE_ONCE(iopt->disable_large_pages, false);\n\trc = iopt_calculate_iova_alignment(iopt);\n\tWARN_ON(rc);\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n}\n\nint iopt_disable_large_pages(struct io_pagetable *iopt)\n{\n\tint rc = 0;\n\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\tif (iopt->disable_large_pages)\n\t\tgoto out_unlock;\n\n\t \n\tif (!xa_empty(&iopt->domains) &&\n\t    !RB_EMPTY_ROOT(&iopt->area_itree.rb_root)) {\n\t\trc = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tWRITE_ONCE(iopt->disable_large_pages, true);\n\trc = iopt_calculate_iova_alignment(iopt);\n\tif (rc)\n\t\tWRITE_ONCE(iopt->disable_large_pages, false);\nout_unlock:\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n\treturn rc;\n}\n\nint iopt_add_access(struct io_pagetable *iopt, struct iommufd_access *access)\n{\n\tint rc;\n\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\trc = xa_alloc(&iopt->access_list, &access->iopt_access_list_id, access,\n\t\t      xa_limit_16b, GFP_KERNEL_ACCOUNT);\n\tif (rc)\n\t\tgoto out_unlock;\n\n\trc = iopt_calculate_iova_alignment(iopt);\n\tif (rc) {\n\t\txa_erase(&iopt->access_list, access->iopt_access_list_id);\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n\treturn rc;\n}\n\nvoid iopt_remove_access(struct io_pagetable *iopt,\n\t\t\tstruct iommufd_access *access,\n\t\t\tu32 iopt_access_list_id)\n{\n\tdown_write(&iopt->domains_rwsem);\n\tdown_write(&iopt->iova_rwsem);\n\tWARN_ON(xa_erase(&iopt->access_list, iopt_access_list_id) != access);\n\tWARN_ON(iopt_calculate_iova_alignment(iopt));\n\tup_write(&iopt->iova_rwsem);\n\tup_write(&iopt->domains_rwsem);\n}\n\n \nint iopt_table_enforce_dev_resv_regions(struct io_pagetable *iopt,\n\t\t\t\t\tstruct device *dev,\n\t\t\t\t\tphys_addr_t *sw_msi_start)\n{\n\tstruct iommu_resv_region *resv;\n\tLIST_HEAD(resv_regions);\n\tunsigned int num_hw_msi = 0;\n\tunsigned int num_sw_msi = 0;\n\tint rc;\n\n\tif (iommufd_should_fail())\n\t\treturn -EINVAL;\n\n\tdown_write(&iopt->iova_rwsem);\n\t \n\tiommu_get_resv_regions(dev, &resv_regions);\n\n\tlist_for_each_entry(resv, &resv_regions, list) {\n\t\tif (resv->type == IOMMU_RESV_DIRECT_RELAXABLE)\n\t\t\tcontinue;\n\n\t\tif (sw_msi_start && resv->type == IOMMU_RESV_MSI)\n\t\t\tnum_hw_msi++;\n\t\tif (sw_msi_start && resv->type == IOMMU_RESV_SW_MSI) {\n\t\t\t*sw_msi_start = resv->start;\n\t\t\tnum_sw_msi++;\n\t\t}\n\n\t\trc = iopt_reserve_iova(iopt, resv->start,\n\t\t\t\t       resv->length - 1 + resv->start, dev);\n\t\tif (rc)\n\t\t\tgoto out_reserved;\n\t}\n\n\t \n\tif (WARN_ON(num_sw_msi && num_hw_msi) || WARN_ON(num_sw_msi > 1)) {\n\t\trc = -EINVAL;\n\t\tgoto out_reserved;\n\t}\n\n\trc = 0;\n\tgoto out_free_resv;\n\nout_reserved:\n\t__iopt_remove_reserved_iova(iopt, dev);\nout_free_resv:\n\tiommu_put_resv_regions(dev, &resv_regions);\n\tup_write(&iopt->iova_rwsem);\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}