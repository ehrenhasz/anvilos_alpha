{
  "module_name": "pages.c",
  "hash_id": "5b69ff174bcc5e95dee5db5efd8098a641bdc36741862957b0abbbad376a6590",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/iommufd/pages.c",
  "human_readable_source": "\n \n#include <linux/overflow.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/sched/mm.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/iommufd.h>\n\n#include \"io_pagetable.h\"\n#include \"double_span.h\"\n\n#ifndef CONFIG_IOMMUFD_TEST\n#define TEMP_MEMORY_LIMIT 65536\n#else\n#define TEMP_MEMORY_LIMIT iommufd_test_memory_limit\n#endif\n#define BATCH_BACKUP_SIZE 32\n\n \nstatic void *temp_kmalloc(size_t *size, void *backup, size_t backup_len)\n{\n\tvoid *res;\n\n\tif (WARN_ON(*size == 0))\n\t\treturn NULL;\n\n\tif (*size < backup_len)\n\t\treturn backup;\n\n\tif (!backup && iommufd_should_fail())\n\t\treturn NULL;\n\n\t*size = min_t(size_t, *size, TEMP_MEMORY_LIMIT);\n\tres = kmalloc(*size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (res)\n\t\treturn res;\n\t*size = PAGE_SIZE;\n\tif (backup_len) {\n\t\tres = kmalloc(*size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\t\tif (res)\n\t\t\treturn res;\n\t\t*size = backup_len;\n\t\treturn backup;\n\t}\n\treturn kmalloc(*size, GFP_KERNEL);\n}\n\nvoid interval_tree_double_span_iter_update(\n\tstruct interval_tree_double_span_iter *iter)\n{\n\tunsigned long last_hole = ULONG_MAX;\n\tunsigned int i;\n\n\tfor (i = 0; i != ARRAY_SIZE(iter->spans); i++) {\n\t\tif (interval_tree_span_iter_done(&iter->spans[i])) {\n\t\t\titer->is_used = -1;\n\t\t\treturn;\n\t\t}\n\n\t\tif (iter->spans[i].is_hole) {\n\t\t\tlast_hole = min(last_hole, iter->spans[i].last_hole);\n\t\t\tcontinue;\n\t\t}\n\n\t\titer->is_used = i + 1;\n\t\titer->start_used = iter->spans[i].start_used;\n\t\titer->last_used = min(iter->spans[i].last_used, last_hole);\n\t\treturn;\n\t}\n\n\titer->is_used = 0;\n\titer->start_hole = iter->spans[0].start_hole;\n\titer->last_hole =\n\t\tmin(iter->spans[0].last_hole, iter->spans[1].last_hole);\n}\n\nvoid interval_tree_double_span_iter_first(\n\tstruct interval_tree_double_span_iter *iter,\n\tstruct rb_root_cached *itree1, struct rb_root_cached *itree2,\n\tunsigned long first_index, unsigned long last_index)\n{\n\tunsigned int i;\n\n\titer->itrees[0] = itree1;\n\titer->itrees[1] = itree2;\n\tfor (i = 0; i != ARRAY_SIZE(iter->spans); i++)\n\t\tinterval_tree_span_iter_first(&iter->spans[i], iter->itrees[i],\n\t\t\t\t\t      first_index, last_index);\n\tinterval_tree_double_span_iter_update(iter);\n}\n\nvoid interval_tree_double_span_iter_next(\n\tstruct interval_tree_double_span_iter *iter)\n{\n\tunsigned int i;\n\n\tif (iter->is_used == -1 ||\n\t    iter->last_hole == iter->spans[0].last_index) {\n\t\titer->is_used = -1;\n\t\treturn;\n\t}\n\n\tfor (i = 0; i != ARRAY_SIZE(iter->spans); i++)\n\t\tinterval_tree_span_iter_advance(\n\t\t\t&iter->spans[i], iter->itrees[i], iter->last_hole + 1);\n\tinterval_tree_double_span_iter_update(iter);\n}\n\nstatic void iopt_pages_add_npinned(struct iopt_pages *pages, size_t npages)\n{\n\tint rc;\n\n\trc = check_add_overflow(pages->npinned, npages, &pages->npinned);\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(rc || pages->npinned > pages->npages);\n}\n\nstatic void iopt_pages_sub_npinned(struct iopt_pages *pages, size_t npages)\n{\n\tint rc;\n\n\trc = check_sub_overflow(pages->npinned, npages, &pages->npinned);\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(rc || pages->npinned > pages->npages);\n}\n\nstatic void iopt_pages_err_unpin(struct iopt_pages *pages,\n\t\t\t\t unsigned long start_index,\n\t\t\t\t unsigned long last_index,\n\t\t\t\t struct page **page_list)\n{\n\tunsigned long npages = last_index - start_index + 1;\n\n\tunpin_user_pages(page_list, npages);\n\tiopt_pages_sub_npinned(pages, npages);\n}\n\n \nstatic unsigned long iopt_area_index_to_iova(struct iopt_area *area,\n\t\t\t\t\t     unsigned long index)\n{\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(index < iopt_area_index(area) ||\n\t\t\tindex > iopt_area_last_index(area));\n\tindex -= iopt_area_index(area);\n\tif (index == 0)\n\t\treturn iopt_area_iova(area);\n\treturn iopt_area_iova(area) - area->page_offset + index * PAGE_SIZE;\n}\n\nstatic unsigned long iopt_area_index_to_iova_last(struct iopt_area *area,\n\t\t\t\t\t\t  unsigned long index)\n{\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(index < iopt_area_index(area) ||\n\t\t\tindex > iopt_area_last_index(area));\n\tif (index == iopt_area_last_index(area))\n\t\treturn iopt_area_last_iova(area);\n\treturn iopt_area_iova(area) - area->page_offset +\n\t       (index - iopt_area_index(area) + 1) * PAGE_SIZE - 1;\n}\n\nstatic void iommu_unmap_nofail(struct iommu_domain *domain, unsigned long iova,\n\t\t\t       size_t size)\n{\n\tsize_t ret;\n\n\tret = iommu_unmap(domain, iova, size);\n\t \n\tWARN_ON(ret != size);\n}\n\nstatic void iopt_area_unmap_domain_range(struct iopt_area *area,\n\t\t\t\t\t struct iommu_domain *domain,\n\t\t\t\t\t unsigned long start_index,\n\t\t\t\t\t unsigned long last_index)\n{\n\tunsigned long start_iova = iopt_area_index_to_iova(area, start_index);\n\n\tiommu_unmap_nofail(domain, start_iova,\n\t\t\t   iopt_area_index_to_iova_last(area, last_index) -\n\t\t\t\t   start_iova + 1);\n}\n\nstatic struct iopt_area *iopt_pages_find_domain_area(struct iopt_pages *pages,\n\t\t\t\t\t\t     unsigned long index)\n{\n\tstruct interval_tree_node *node;\n\n\tnode = interval_tree_iter_first(&pages->domains_itree, index, index);\n\tif (!node)\n\t\treturn NULL;\n\treturn container_of(node, struct iopt_area, pages_node);\n}\n\n \nstruct pfn_batch {\n\tunsigned long *pfns;\n\tu32 *npfns;\n\tunsigned int array_size;\n\tunsigned int end;\n\tunsigned int total_pfns;\n};\n\nstatic void batch_clear(struct pfn_batch *batch)\n{\n\tbatch->total_pfns = 0;\n\tbatch->end = 0;\n\tbatch->pfns[0] = 0;\n\tbatch->npfns[0] = 0;\n}\n\n \nstatic void batch_clear_carry(struct pfn_batch *batch, unsigned int keep_pfns)\n{\n\tif (!keep_pfns)\n\t\treturn batch_clear(batch);\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(!batch->end ||\n\t\t\tbatch->npfns[batch->end - 1] < keep_pfns);\n\n\tbatch->total_pfns = keep_pfns;\n\tbatch->pfns[0] = batch->pfns[batch->end - 1] +\n\t\t\t (batch->npfns[batch->end - 1] - keep_pfns);\n\tbatch->npfns[0] = keep_pfns;\n\tbatch->end = 1;\n}\n\nstatic void batch_skip_carry(struct pfn_batch *batch, unsigned int skip_pfns)\n{\n\tif (!batch->total_pfns)\n\t\treturn;\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(batch->total_pfns != batch->npfns[0]);\n\tskip_pfns = min(batch->total_pfns, skip_pfns);\n\tbatch->pfns[0] += skip_pfns;\n\tbatch->npfns[0] -= skip_pfns;\n\tbatch->total_pfns -= skip_pfns;\n}\n\nstatic int __batch_init(struct pfn_batch *batch, size_t max_pages, void *backup,\n\t\t\tsize_t backup_len)\n{\n\tconst size_t elmsz = sizeof(*batch->pfns) + sizeof(*batch->npfns);\n\tsize_t size = max_pages * elmsz;\n\n\tbatch->pfns = temp_kmalloc(&size, backup, backup_len);\n\tif (!batch->pfns)\n\t\treturn -ENOMEM;\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) && WARN_ON(size < elmsz))\n\t\treturn -EINVAL;\n\tbatch->array_size = size / elmsz;\n\tbatch->npfns = (u32 *)(batch->pfns + batch->array_size);\n\tbatch_clear(batch);\n\treturn 0;\n}\n\nstatic int batch_init(struct pfn_batch *batch, size_t max_pages)\n{\n\treturn __batch_init(batch, max_pages, NULL, 0);\n}\n\nstatic void batch_init_backup(struct pfn_batch *batch, size_t max_pages,\n\t\t\t      void *backup, size_t backup_len)\n{\n\t__batch_init(batch, max_pages, backup, backup_len);\n}\n\nstatic void batch_destroy(struct pfn_batch *batch, void *backup)\n{\n\tif (batch->pfns != backup)\n\t\tkfree(batch->pfns);\n}\n\n \nstatic bool batch_add_pfn(struct pfn_batch *batch, unsigned long pfn)\n{\n\tconst unsigned int MAX_NPFNS = type_max(typeof(*batch->npfns));\n\n\tif (batch->end &&\n\t    pfn == batch->pfns[batch->end - 1] + batch->npfns[batch->end - 1] &&\n\t    batch->npfns[batch->end - 1] != MAX_NPFNS) {\n\t\tbatch->npfns[batch->end - 1]++;\n\t\tbatch->total_pfns++;\n\t\treturn true;\n\t}\n\tif (batch->end == batch->array_size)\n\t\treturn false;\n\tbatch->total_pfns++;\n\tbatch->pfns[batch->end] = pfn;\n\tbatch->npfns[batch->end] = 1;\n\tbatch->end++;\n\treturn true;\n}\n\n \nstatic void batch_from_domain(struct pfn_batch *batch,\n\t\t\t      struct iommu_domain *domain,\n\t\t\t      struct iopt_area *area, unsigned long start_index,\n\t\t\t      unsigned long last_index)\n{\n\tunsigned int page_offset = 0;\n\tunsigned long iova;\n\tphys_addr_t phys;\n\n\tiova = iopt_area_index_to_iova(area, start_index);\n\tif (start_index == iopt_area_index(area))\n\t\tpage_offset = area->page_offset;\n\twhile (start_index <= last_index) {\n\t\t \n\t\tphys = iommu_iova_to_phys(domain, iova) - page_offset;\n\t\tif (!batch_add_pfn(batch, PHYS_PFN(phys)))\n\t\t\treturn;\n\t\tiova += PAGE_SIZE - page_offset;\n\t\tpage_offset = 0;\n\t\tstart_index++;\n\t}\n}\n\nstatic struct page **raw_pages_from_domain(struct iommu_domain *domain,\n\t\t\t\t\t   struct iopt_area *area,\n\t\t\t\t\t   unsigned long start_index,\n\t\t\t\t\t   unsigned long last_index,\n\t\t\t\t\t   struct page **out_pages)\n{\n\tunsigned int page_offset = 0;\n\tunsigned long iova;\n\tphys_addr_t phys;\n\n\tiova = iopt_area_index_to_iova(area, start_index);\n\tif (start_index == iopt_area_index(area))\n\t\tpage_offset = area->page_offset;\n\twhile (start_index <= last_index) {\n\t\tphys = iommu_iova_to_phys(domain, iova) - page_offset;\n\t\t*(out_pages++) = pfn_to_page(PHYS_PFN(phys));\n\t\tiova += PAGE_SIZE - page_offset;\n\t\tpage_offset = 0;\n\t\tstart_index++;\n\t}\n\treturn out_pages;\n}\n\n \nstatic void batch_from_domain_continue(struct pfn_batch *batch,\n\t\t\t\t       struct iommu_domain *domain,\n\t\t\t\t       struct iopt_area *area,\n\t\t\t\t       unsigned long start_index,\n\t\t\t\t       unsigned long last_index)\n{\n\tunsigned int array_size = batch->array_size;\n\n\tbatch->array_size = batch->end;\n\tbatch_from_domain(batch, domain, area, start_index, last_index);\n\tbatch->array_size = array_size;\n}\n\n \nstatic int batch_iommu_map_small(struct iommu_domain *domain,\n\t\t\t\t unsigned long iova, phys_addr_t paddr,\n\t\t\t\t size_t size, int prot)\n{\n\tunsigned long start_iova = iova;\n\tint rc;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(paddr % PAGE_SIZE || iova % PAGE_SIZE ||\n\t\t\tsize % PAGE_SIZE);\n\n\twhile (size) {\n\t\trc = iommu_map(domain, iova, paddr, PAGE_SIZE, prot,\n\t\t\t       GFP_KERNEL_ACCOUNT);\n\t\tif (rc)\n\t\t\tgoto err_unmap;\n\t\tiova += PAGE_SIZE;\n\t\tpaddr += PAGE_SIZE;\n\t\tsize -= PAGE_SIZE;\n\t}\n\treturn 0;\n\nerr_unmap:\n\tif (start_iova != iova)\n\t\tiommu_unmap_nofail(domain, start_iova, iova - start_iova);\n\treturn rc;\n}\n\nstatic int batch_to_domain(struct pfn_batch *batch, struct iommu_domain *domain,\n\t\t\t   struct iopt_area *area, unsigned long start_index)\n{\n\tbool disable_large_pages = area->iopt->disable_large_pages;\n\tunsigned long last_iova = iopt_area_last_iova(area);\n\tunsigned int page_offset = 0;\n\tunsigned long start_iova;\n\tunsigned long next_iova;\n\tunsigned int cur = 0;\n\tunsigned long iova;\n\tint rc;\n\n\t \n\tif (start_index == iopt_area_index(area))\n\t\tpage_offset = area->page_offset;\n\tnext_iova = iova = start_iova =\n\t\tiopt_area_index_to_iova(area, start_index);\n\twhile (cur < batch->end) {\n\t\tnext_iova = min(last_iova + 1,\n\t\t\t\tnext_iova + batch->npfns[cur] * PAGE_SIZE -\n\t\t\t\t\tpage_offset);\n\t\tif (disable_large_pages)\n\t\t\trc = batch_iommu_map_small(\n\t\t\t\tdomain, iova,\n\t\t\t\tPFN_PHYS(batch->pfns[cur]) + page_offset,\n\t\t\t\tnext_iova - iova, area->iommu_prot);\n\t\telse\n\t\t\trc = iommu_map(domain, iova,\n\t\t\t\t       PFN_PHYS(batch->pfns[cur]) + page_offset,\n\t\t\t\t       next_iova - iova, area->iommu_prot,\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\t\tif (rc)\n\t\t\tgoto err_unmap;\n\t\tiova = next_iova;\n\t\tpage_offset = 0;\n\t\tcur++;\n\t}\n\treturn 0;\nerr_unmap:\n\tif (start_iova != iova)\n\t\tiommu_unmap_nofail(domain, start_iova, iova - start_iova);\n\treturn rc;\n}\n\nstatic void batch_from_xarray(struct pfn_batch *batch, struct xarray *xa,\n\t\t\t      unsigned long start_index,\n\t\t\t      unsigned long last_index)\n{\n\tXA_STATE(xas, xa, start_index);\n\tvoid *entry;\n\n\trcu_read_lock();\n\twhile (true) {\n\t\tentry = xas_next(&xas);\n\t\tif (xas_retry(&xas, entry))\n\t\t\tcontinue;\n\t\tWARN_ON(!xa_is_value(entry));\n\t\tif (!batch_add_pfn(batch, xa_to_value(entry)) ||\n\t\t    start_index == last_index)\n\t\t\tbreak;\n\t\tstart_index++;\n\t}\n\trcu_read_unlock();\n}\n\nstatic void batch_from_xarray_clear(struct pfn_batch *batch, struct xarray *xa,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    unsigned long last_index)\n{\n\tXA_STATE(xas, xa, start_index);\n\tvoid *entry;\n\n\txas_lock(&xas);\n\twhile (true) {\n\t\tentry = xas_next(&xas);\n\t\tif (xas_retry(&xas, entry))\n\t\t\tcontinue;\n\t\tWARN_ON(!xa_is_value(entry));\n\t\tif (!batch_add_pfn(batch, xa_to_value(entry)))\n\t\t\tbreak;\n\t\txas_store(&xas, NULL);\n\t\tif (start_index == last_index)\n\t\t\tbreak;\n\t\tstart_index++;\n\t}\n\txas_unlock(&xas);\n}\n\nstatic void clear_xarray(struct xarray *xa, unsigned long start_index,\n\t\t\t unsigned long last_index)\n{\n\tXA_STATE(xas, xa, start_index);\n\tvoid *entry;\n\n\txas_lock(&xas);\n\txas_for_each(&xas, entry, last_index)\n\t\txas_store(&xas, NULL);\n\txas_unlock(&xas);\n}\n\nstatic int pages_to_xarray(struct xarray *xa, unsigned long start_index,\n\t\t\t   unsigned long last_index, struct page **pages)\n{\n\tstruct page **end_pages = pages + (last_index - start_index) + 1;\n\tstruct page **half_pages = pages + (end_pages - pages) / 2;\n\tXA_STATE(xas, xa, start_index);\n\n\tdo {\n\t\tvoid *old;\n\n\t\txas_lock(&xas);\n\t\twhile (pages != end_pages) {\n\t\t\t \n\t\t\tif (pages == half_pages && iommufd_should_fail()) {\n\t\t\t\txas_set_err(&xas, -EINVAL);\n\t\t\t\txas_unlock(&xas);\n\t\t\t\t \n\t\t\t\txas_nomem(&xas, GFP_KERNEL);\n\t\t\t\tgoto err_clear;\n\t\t\t}\n\n\t\t\told = xas_store(&xas, xa_mk_value(page_to_pfn(*pages)));\n\t\t\tif (xas_error(&xas))\n\t\t\t\tbreak;\n\t\t\tWARN_ON(old);\n\t\t\tpages++;\n\t\t\txas_next(&xas);\n\t\t}\n\t\txas_unlock(&xas);\n\t} while (xas_nomem(&xas, GFP_KERNEL));\n\nerr_clear:\n\tif (xas_error(&xas)) {\n\t\tif (xas.xa_index != start_index)\n\t\t\tclear_xarray(xa, start_index, xas.xa_index - 1);\n\t\treturn xas_error(&xas);\n\t}\n\treturn 0;\n}\n\nstatic void batch_from_pages(struct pfn_batch *batch, struct page **pages,\n\t\t\t     size_t npages)\n{\n\tstruct page **end = pages + npages;\n\n\tfor (; pages != end; pages++)\n\t\tif (!batch_add_pfn(batch, page_to_pfn(*pages)))\n\t\t\tbreak;\n}\n\nstatic void batch_unpin(struct pfn_batch *batch, struct iopt_pages *pages,\n\t\t\tunsigned int first_page_off, size_t npages)\n{\n\tunsigned int cur = 0;\n\n\twhile (first_page_off) {\n\t\tif (batch->npfns[cur] > first_page_off)\n\t\t\tbreak;\n\t\tfirst_page_off -= batch->npfns[cur];\n\t\tcur++;\n\t}\n\n\twhile (npages) {\n\t\tsize_t to_unpin = min_t(size_t, npages,\n\t\t\t\t\tbatch->npfns[cur] - first_page_off);\n\n\t\tunpin_user_page_range_dirty_lock(\n\t\t\tpfn_to_page(batch->pfns[cur] + first_page_off),\n\t\t\tto_unpin, pages->writable);\n\t\tiopt_pages_sub_npinned(pages, to_unpin);\n\t\tcur++;\n\t\tfirst_page_off = 0;\n\t\tnpages -= to_unpin;\n\t}\n}\n\nstatic void copy_data_page(struct page *page, void *data, unsigned long offset,\n\t\t\t   size_t length, unsigned int flags)\n{\n\tvoid *mem;\n\n\tmem = kmap_local_page(page);\n\tif (flags & IOMMUFD_ACCESS_RW_WRITE) {\n\t\tmemcpy(mem + offset, data, length);\n\t\tset_page_dirty_lock(page);\n\t} else {\n\t\tmemcpy(data, mem + offset, length);\n\t}\n\tkunmap_local(mem);\n}\n\nstatic unsigned long batch_rw(struct pfn_batch *batch, void *data,\n\t\t\t      unsigned long offset, unsigned long length,\n\t\t\t      unsigned int flags)\n{\n\tunsigned long copied = 0;\n\tunsigned int npage = 0;\n\tunsigned int cur = 0;\n\n\twhile (cur < batch->end) {\n\t\tunsigned long bytes = min(length, PAGE_SIZE - offset);\n\n\t\tcopy_data_page(pfn_to_page(batch->pfns[cur] + npage), data,\n\t\t\t       offset, bytes, flags);\n\t\toffset = 0;\n\t\tlength -= bytes;\n\t\tdata += bytes;\n\t\tcopied += bytes;\n\t\tnpage++;\n\t\tif (npage == batch->npfns[cur]) {\n\t\t\tnpage = 0;\n\t\t\tcur++;\n\t\t}\n\t\tif (!length)\n\t\t\tbreak;\n\t}\n\treturn copied;\n}\n\n \nstruct pfn_reader_user {\n\tstruct page **upages;\n\tsize_t upages_len;\n\tunsigned long upages_start;\n\tunsigned long upages_end;\n\tunsigned int gup_flags;\n\t \n\tint locked;\n};\n\nstatic void pfn_reader_user_init(struct pfn_reader_user *user,\n\t\t\t\t struct iopt_pages *pages)\n{\n\tuser->upages = NULL;\n\tuser->upages_start = 0;\n\tuser->upages_end = 0;\n\tuser->locked = -1;\n\n\tuser->gup_flags = FOLL_LONGTERM;\n\tif (pages->writable)\n\t\tuser->gup_flags |= FOLL_WRITE;\n}\n\nstatic void pfn_reader_user_destroy(struct pfn_reader_user *user,\n\t\t\t\t    struct iopt_pages *pages)\n{\n\tif (user->locked != -1) {\n\t\tif (user->locked)\n\t\t\tmmap_read_unlock(pages->source_mm);\n\t\tif (pages->source_mm != current->mm)\n\t\t\tmmput(pages->source_mm);\n\t\tuser->locked = -1;\n\t}\n\n\tkfree(user->upages);\n\tuser->upages = NULL;\n}\n\nstatic int pfn_reader_user_pin(struct pfn_reader_user *user,\n\t\t\t       struct iopt_pages *pages,\n\t\t\t       unsigned long start_index,\n\t\t\t       unsigned long last_index)\n{\n\tbool remote_mm = pages->source_mm != current->mm;\n\tunsigned long npages;\n\tuintptr_t uptr;\n\tlong rc;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t    WARN_ON(last_index < start_index))\n\t\treturn -EINVAL;\n\n\tif (!user->upages) {\n\t\t \n\t\tuser->upages_len =\n\t\t\t(last_index - start_index + 1) * sizeof(*user->upages);\n\t\tuser->upages = temp_kmalloc(&user->upages_len, NULL, 0);\n\t\tif (!user->upages)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (user->locked == -1) {\n\t\t \n\t\tif (remote_mm) {\n\t\t\tif (!mmget_not_zero(pages->source_mm))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\tuser->locked = 0;\n\t}\n\n\tnpages = min_t(unsigned long, last_index - start_index + 1,\n\t\t       user->upages_len / sizeof(*user->upages));\n\n\n\tif (iommufd_should_fail())\n\t\treturn -EFAULT;\n\n\tuptr = (uintptr_t)(pages->uptr + start_index * PAGE_SIZE);\n\tif (!remote_mm)\n\t\trc = pin_user_pages_fast(uptr, npages, user->gup_flags,\n\t\t\t\t\t user->upages);\n\telse {\n\t\tif (!user->locked) {\n\t\t\tmmap_read_lock(pages->source_mm);\n\t\t\tuser->locked = 1;\n\t\t}\n\t\trc = pin_user_pages_remote(pages->source_mm, uptr, npages,\n\t\t\t\t\t   user->gup_flags, user->upages,\n\t\t\t\t\t   &user->locked);\n\t}\n\tif (rc <= 0) {\n\t\tif (WARN_ON(!rc))\n\t\t\treturn -EFAULT;\n\t\treturn rc;\n\t}\n\tiopt_pages_add_npinned(pages, rc);\n\tuser->upages_start = start_index;\n\tuser->upages_end = start_index + rc;\n\treturn 0;\n}\n\n \nstatic int incr_user_locked_vm(struct iopt_pages *pages, unsigned long npages)\n{\n\tunsigned long lock_limit;\n\tunsigned long cur_pages;\n\tunsigned long new_pages;\n\n\tlock_limit = task_rlimit(pages->source_task, RLIMIT_MEMLOCK) >>\n\t\t     PAGE_SHIFT;\n\tdo {\n\t\tcur_pages = atomic_long_read(&pages->source_user->locked_vm);\n\t\tnew_pages = cur_pages + npages;\n\t\tif (new_pages > lock_limit)\n\t\t\treturn -ENOMEM;\n\t} while (atomic_long_cmpxchg(&pages->source_user->locked_vm, cur_pages,\n\t\t\t\t     new_pages) != cur_pages);\n\treturn 0;\n}\n\nstatic void decr_user_locked_vm(struct iopt_pages *pages, unsigned long npages)\n{\n\tif (WARN_ON(atomic_long_read(&pages->source_user->locked_vm) < npages))\n\t\treturn;\n\tatomic_long_sub(npages, &pages->source_user->locked_vm);\n}\n\n \nstatic int update_mm_locked_vm(struct iopt_pages *pages, unsigned long npages,\n\t\t\t       bool inc, struct pfn_reader_user *user)\n{\n\tbool do_put = false;\n\tint rc;\n\n\tif (user && user->locked) {\n\t\tmmap_read_unlock(pages->source_mm);\n\t\tuser->locked = 0;\n\t\t \n\t} else if ((!user || !user->upages) &&\n\t\t   pages->source_mm != current->mm) {\n\t\tif (!mmget_not_zero(pages->source_mm))\n\t\t\treturn -EINVAL;\n\t\tdo_put = true;\n\t}\n\n\tmmap_write_lock(pages->source_mm);\n\trc = __account_locked_vm(pages->source_mm, npages, inc,\n\t\t\t\t pages->source_task, false);\n\tmmap_write_unlock(pages->source_mm);\n\n\tif (do_put)\n\t\tmmput(pages->source_mm);\n\treturn rc;\n}\n\nstatic int do_update_pinned(struct iopt_pages *pages, unsigned long npages,\n\t\t\t    bool inc, struct pfn_reader_user *user)\n{\n\tint rc = 0;\n\n\tswitch (pages->account_mode) {\n\tcase IOPT_PAGES_ACCOUNT_NONE:\n\t\tbreak;\n\tcase IOPT_PAGES_ACCOUNT_USER:\n\t\tif (inc)\n\t\t\trc = incr_user_locked_vm(pages, npages);\n\t\telse\n\t\t\tdecr_user_locked_vm(pages, npages);\n\t\tbreak;\n\tcase IOPT_PAGES_ACCOUNT_MM:\n\t\trc = update_mm_locked_vm(pages, npages, inc, user);\n\t\tbreak;\n\t}\n\tif (rc)\n\t\treturn rc;\n\n\tpages->last_npinned = pages->npinned;\n\tif (inc)\n\t\tatomic64_add(npages, &pages->source_mm->pinned_vm);\n\telse\n\t\tatomic64_sub(npages, &pages->source_mm->pinned_vm);\n\treturn 0;\n}\n\nstatic void update_unpinned(struct iopt_pages *pages)\n{\n\tif (WARN_ON(pages->npinned > pages->last_npinned))\n\t\treturn;\n\tif (pages->npinned == pages->last_npinned)\n\t\treturn;\n\tdo_update_pinned(pages, pages->last_npinned - pages->npinned, false,\n\t\t\t NULL);\n}\n\n \nstatic int pfn_reader_user_update_pinned(struct pfn_reader_user *user,\n\t\t\t\t\t struct iopt_pages *pages)\n{\n\tunsigned long npages;\n\tbool inc;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\tif (pages->npinned == pages->last_npinned)\n\t\treturn 0;\n\n\tif (pages->npinned < pages->last_npinned) {\n\t\tnpages = pages->last_npinned - pages->npinned;\n\t\tinc = false;\n\t} else {\n\t\tif (iommufd_should_fail())\n\t\t\treturn -ENOMEM;\n\t\tnpages = pages->npinned - pages->last_npinned;\n\t\tinc = true;\n\t}\n\treturn do_update_pinned(pages, npages, inc, user);\n}\n\n \nstruct pfn_reader {\n\tstruct iopt_pages *pages;\n\tstruct interval_tree_double_span_iter span;\n\tstruct pfn_batch batch;\n\tunsigned long batch_start_index;\n\tunsigned long batch_end_index;\n\tunsigned long last_index;\n\n\tstruct pfn_reader_user user;\n};\n\nstatic int pfn_reader_update_pinned(struct pfn_reader *pfns)\n{\n\treturn pfn_reader_user_update_pinned(&pfns->user, pfns->pages);\n}\n\n \nstatic void pfn_reader_unpin(struct pfn_reader *pfns)\n{\n\tunsigned long last = pfns->batch_end_index - 1;\n\tunsigned long start = pfns->batch_start_index;\n\tstruct interval_tree_double_span_iter span;\n\tstruct iopt_pages *pages = pfns->pages;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\tinterval_tree_for_each_double_span(&span, &pages->access_itree,\n\t\t\t\t\t   &pages->domains_itree, start, last) {\n\t\tif (span.is_used)\n\t\t\tcontinue;\n\n\t\tbatch_unpin(&pfns->batch, pages, span.start_hole - start,\n\t\t\t    span.last_hole - span.start_hole + 1);\n\t}\n}\n\n \nstatic int pfn_reader_fill_span(struct pfn_reader *pfns)\n{\n\tstruct interval_tree_double_span_iter *span = &pfns->span;\n\tunsigned long start_index = pfns->batch_end_index;\n\tstruct iopt_area *area;\n\tint rc;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t    WARN_ON(span->last_used < start_index))\n\t\treturn -EINVAL;\n\n\tif (span->is_used == 1) {\n\t\tbatch_from_xarray(&pfns->batch, &pfns->pages->pinned_pfns,\n\t\t\t\t  start_index, span->last_used);\n\t\treturn 0;\n\t}\n\n\tif (span->is_used == 2) {\n\t\t \n\t\tarea = iopt_pages_find_domain_area(pfns->pages, start_index);\n\t\tif (WARN_ON(!area))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tbatch_from_domain(\n\t\t\t&pfns->batch, area->storage_domain, area, start_index,\n\t\t\tmin(iopt_area_last_index(area), span->last_used));\n\t\treturn 0;\n\t}\n\n\tif (start_index >= pfns->user.upages_end) {\n\t\trc = pfn_reader_user_pin(&pfns->user, pfns->pages, start_index,\n\t\t\t\t\t span->last_hole);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tbatch_from_pages(&pfns->batch,\n\t\t\t pfns->user.upages +\n\t\t\t\t (start_index - pfns->user.upages_start),\n\t\t\t pfns->user.upages_end - start_index);\n\treturn 0;\n}\n\nstatic bool pfn_reader_done(struct pfn_reader *pfns)\n{\n\treturn pfns->batch_start_index == pfns->last_index + 1;\n}\n\nstatic int pfn_reader_next(struct pfn_reader *pfns)\n{\n\tint rc;\n\n\tbatch_clear(&pfns->batch);\n\tpfns->batch_start_index = pfns->batch_end_index;\n\n\twhile (pfns->batch_end_index != pfns->last_index + 1) {\n\t\tunsigned int npfns = pfns->batch.total_pfns;\n\n\t\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t\t    WARN_ON(interval_tree_double_span_iter_done(&pfns->span)))\n\t\t\treturn -EINVAL;\n\n\t\trc = pfn_reader_fill_span(pfns);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (WARN_ON(!pfns->batch.total_pfns))\n\t\t\treturn -EINVAL;\n\n\t\tpfns->batch_end_index =\n\t\t\tpfns->batch_start_index + pfns->batch.total_pfns;\n\t\tif (pfns->batch_end_index == pfns->span.last_used + 1)\n\t\t\tinterval_tree_double_span_iter_next(&pfns->span);\n\n\t\t \n\t\tif (npfns == pfns->batch.total_pfns)\n\t\t\treturn 0;\n\t}\n\treturn 0;\n}\n\nstatic int pfn_reader_init(struct pfn_reader *pfns, struct iopt_pages *pages,\n\t\t\t   unsigned long start_index, unsigned long last_index)\n{\n\tint rc;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\tpfns->pages = pages;\n\tpfns->batch_start_index = start_index;\n\tpfns->batch_end_index = start_index;\n\tpfns->last_index = last_index;\n\tpfn_reader_user_init(&pfns->user, pages);\n\trc = batch_init(&pfns->batch, last_index - start_index + 1);\n\tif (rc)\n\t\treturn rc;\n\tinterval_tree_double_span_iter_first(&pfns->span, &pages->access_itree,\n\t\t\t\t\t     &pages->domains_itree, start_index,\n\t\t\t\t\t     last_index);\n\treturn 0;\n}\n\n \nstatic void pfn_reader_release_pins(struct pfn_reader *pfns)\n{\n\tstruct iopt_pages *pages = pfns->pages;\n\n\tif (pfns->user.upages_end > pfns->batch_end_index) {\n\t\tsize_t npages = pfns->user.upages_end - pfns->batch_end_index;\n\n\t\t \n\t\tunpin_user_pages(pfns->user.upages + (pfns->batch_end_index -\n\t\t\t\t\t\t      pfns->user.upages_start),\n\t\t\t\t npages);\n\t\tiopt_pages_sub_npinned(pages, npages);\n\t\tpfns->user.upages_end = pfns->batch_end_index;\n\t}\n\tif (pfns->batch_start_index != pfns->batch_end_index) {\n\t\tpfn_reader_unpin(pfns);\n\t\tpfns->batch_start_index = pfns->batch_end_index;\n\t}\n}\n\nstatic void pfn_reader_destroy(struct pfn_reader *pfns)\n{\n\tstruct iopt_pages *pages = pfns->pages;\n\n\tpfn_reader_release_pins(pfns);\n\tpfn_reader_user_destroy(&pfns->user, pfns->pages);\n\tbatch_destroy(&pfns->batch, NULL);\n\tWARN_ON(pages->last_npinned != pages->npinned);\n}\n\nstatic int pfn_reader_first(struct pfn_reader *pfns, struct iopt_pages *pages,\n\t\t\t    unsigned long start_index, unsigned long last_index)\n{\n\tint rc;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t    WARN_ON(last_index < start_index))\n\t\treturn -EINVAL;\n\n\trc = pfn_reader_init(pfns, pages, start_index, last_index);\n\tif (rc)\n\t\treturn rc;\n\trc = pfn_reader_next(pfns);\n\tif (rc) {\n\t\tpfn_reader_destroy(pfns);\n\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nstruct iopt_pages *iopt_alloc_pages(void __user *uptr, unsigned long length,\n\t\t\t\t    bool writable)\n{\n\tstruct iopt_pages *pages;\n\tunsigned long end;\n\n\t \n\tif (length > SIZE_MAX - PAGE_SIZE || length == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (check_add_overflow((unsigned long)uptr, length, &end))\n\t\treturn ERR_PTR(-EOVERFLOW);\n\n\tpages = kzalloc(sizeof(*pages), GFP_KERNEL_ACCOUNT);\n\tif (!pages)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tkref_init(&pages->kref);\n\txa_init_flags(&pages->pinned_pfns, XA_FLAGS_ACCOUNT);\n\tmutex_init(&pages->mutex);\n\tpages->source_mm = current->mm;\n\tmmgrab(pages->source_mm);\n\tpages->uptr = (void __user *)ALIGN_DOWN((uintptr_t)uptr, PAGE_SIZE);\n\tpages->npages = DIV_ROUND_UP(length + (uptr - pages->uptr), PAGE_SIZE);\n\tpages->access_itree = RB_ROOT_CACHED;\n\tpages->domains_itree = RB_ROOT_CACHED;\n\tpages->writable = writable;\n\tif (capable(CAP_IPC_LOCK))\n\t\tpages->account_mode = IOPT_PAGES_ACCOUNT_NONE;\n\telse\n\t\tpages->account_mode = IOPT_PAGES_ACCOUNT_USER;\n\tpages->source_task = current->group_leader;\n\tget_task_struct(current->group_leader);\n\tpages->source_user = get_uid(current_user());\n\treturn pages;\n}\n\nvoid iopt_release_pages(struct kref *kref)\n{\n\tstruct iopt_pages *pages = container_of(kref, struct iopt_pages, kref);\n\n\tWARN_ON(!RB_EMPTY_ROOT(&pages->access_itree.rb_root));\n\tWARN_ON(!RB_EMPTY_ROOT(&pages->domains_itree.rb_root));\n\tWARN_ON(pages->npinned);\n\tWARN_ON(!xa_empty(&pages->pinned_pfns));\n\tmmdrop(pages->source_mm);\n\tmutex_destroy(&pages->mutex);\n\tput_task_struct(pages->source_task);\n\tfree_uid(pages->source_user);\n\tkfree(pages);\n}\n\nstatic void\niopt_area_unpin_domain(struct pfn_batch *batch, struct iopt_area *area,\n\t\t       struct iopt_pages *pages, struct iommu_domain *domain,\n\t\t       unsigned long start_index, unsigned long last_index,\n\t\t       unsigned long *unmapped_end_index,\n\t\t       unsigned long real_last_index)\n{\n\twhile (start_index <= last_index) {\n\t\tunsigned long batch_last_index;\n\n\t\tif (*unmapped_end_index <= last_index) {\n\t\t\tunsigned long start =\n\t\t\t\tmax(start_index, *unmapped_end_index);\n\n\t\t\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t\t\t    batch->total_pfns)\n\t\t\t\tWARN_ON(*unmapped_end_index -\n\t\t\t\t\t\tbatch->total_pfns !=\n\t\t\t\t\tstart_index);\n\t\t\tbatch_from_domain(batch, domain, area, start,\n\t\t\t\t\t  last_index);\n\t\t\tbatch_last_index = start_index + batch->total_pfns - 1;\n\t\t} else {\n\t\t\tbatch_last_index = last_index;\n\t\t}\n\n\t\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\t\tWARN_ON(batch_last_index > real_last_index);\n\n\t\t \n\t\tif (batch_last_index == last_index &&\n\t\t    last_index != real_last_index)\n\t\t\tbatch_from_domain_continue(batch, domain, area,\n\t\t\t\t\t\t   last_index + 1,\n\t\t\t\t\t\t   real_last_index);\n\n\t\tif (*unmapped_end_index <= batch_last_index) {\n\t\t\tiopt_area_unmap_domain_range(\n\t\t\t\tarea, domain, *unmapped_end_index,\n\t\t\t\tstart_index + batch->total_pfns - 1);\n\t\t\t*unmapped_end_index = start_index + batch->total_pfns;\n\t\t}\n\n\t\t \n\t\tbatch_unpin(batch, pages, 0,\n\t\t\t    batch_last_index - start_index + 1);\n\t\tstart_index = batch_last_index + 1;\n\n\t\tbatch_clear_carry(batch,\n\t\t\t\t  *unmapped_end_index - batch_last_index - 1);\n\t}\n}\n\nstatic void __iopt_area_unfill_domain(struct iopt_area *area,\n\t\t\t\t      struct iopt_pages *pages,\n\t\t\t\t      struct iommu_domain *domain,\n\t\t\t\t      unsigned long last_index)\n{\n\tstruct interval_tree_double_span_iter span;\n\tunsigned long start_index = iopt_area_index(area);\n\tunsigned long unmapped_end_index = start_index;\n\tu64 backup[BATCH_BACKUP_SIZE];\n\tstruct pfn_batch batch;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\t \n\tbatch_init_backup(&batch, last_index + 1, backup, sizeof(backup));\n\tinterval_tree_for_each_double_span(&span, &pages->domains_itree,\n\t\t\t\t\t   &pages->access_itree, start_index,\n\t\t\t\t\t   last_index) {\n\t\tif (span.is_used) {\n\t\t\tbatch_skip_carry(&batch,\n\t\t\t\t\t span.last_used - span.start_used + 1);\n\t\t\tcontinue;\n\t\t}\n\t\tiopt_area_unpin_domain(&batch, area, pages, domain,\n\t\t\t\t       span.start_hole, span.last_hole,\n\t\t\t\t       &unmapped_end_index, last_index);\n\t}\n\t \n\tif (unmapped_end_index != last_index + 1)\n\t\tiopt_area_unmap_domain_range(area, domain, unmapped_end_index,\n\t\t\t\t\t     last_index);\n\tWARN_ON(batch.total_pfns);\n\tbatch_destroy(&batch, backup);\n\tupdate_unpinned(pages);\n}\n\nstatic void iopt_area_unfill_partial_domain(struct iopt_area *area,\n\t\t\t\t\t    struct iopt_pages *pages,\n\t\t\t\t\t    struct iommu_domain *domain,\n\t\t\t\t\t    unsigned long end_index)\n{\n\tif (end_index != iopt_area_index(area))\n\t\t__iopt_area_unfill_domain(area, pages, domain, end_index - 1);\n}\n\n \nvoid iopt_area_unmap_domain(struct iopt_area *area, struct iommu_domain *domain)\n{\n\tiommu_unmap_nofail(domain, iopt_area_iova(area),\n\t\t\t   iopt_area_length(area));\n}\n\n \nvoid iopt_area_unfill_domain(struct iopt_area *area, struct iopt_pages *pages,\n\t\t\t     struct iommu_domain *domain)\n{\n\t__iopt_area_unfill_domain(area, pages, domain,\n\t\t\t\t  iopt_area_last_index(area));\n}\n\n \nint iopt_area_fill_domain(struct iopt_area *area, struct iommu_domain *domain)\n{\n\tunsigned long done_end_index;\n\tstruct pfn_reader pfns;\n\tint rc;\n\n\tlockdep_assert_held(&area->pages->mutex);\n\n\trc = pfn_reader_first(&pfns, area->pages, iopt_area_index(area),\n\t\t\t      iopt_area_last_index(area));\n\tif (rc)\n\t\treturn rc;\n\n\twhile (!pfn_reader_done(&pfns)) {\n\t\tdone_end_index = pfns.batch_start_index;\n\t\trc = batch_to_domain(&pfns.batch, domain, area,\n\t\t\t\t     pfns.batch_start_index);\n\t\tif (rc)\n\t\t\tgoto out_unmap;\n\t\tdone_end_index = pfns.batch_end_index;\n\n\t\trc = pfn_reader_next(&pfns);\n\t\tif (rc)\n\t\t\tgoto out_unmap;\n\t}\n\n\trc = pfn_reader_update_pinned(&pfns);\n\tif (rc)\n\t\tgoto out_unmap;\n\tgoto out_destroy;\n\nout_unmap:\n\tpfn_reader_release_pins(&pfns);\n\tiopt_area_unfill_partial_domain(area, area->pages, domain,\n\t\t\t\t\tdone_end_index);\nout_destroy:\n\tpfn_reader_destroy(&pfns);\n\treturn rc;\n}\n\n \nint iopt_area_fill_domains(struct iopt_area *area, struct iopt_pages *pages)\n{\n\tunsigned long done_first_end_index;\n\tunsigned long done_all_end_index;\n\tstruct iommu_domain *domain;\n\tunsigned long unmap_index;\n\tstruct pfn_reader pfns;\n\tunsigned long index;\n\tint rc;\n\n\tlockdep_assert_held(&area->iopt->domains_rwsem);\n\n\tif (xa_empty(&area->iopt->domains))\n\t\treturn 0;\n\n\tmutex_lock(&pages->mutex);\n\trc = pfn_reader_first(&pfns, pages, iopt_area_index(area),\n\t\t\t      iopt_area_last_index(area));\n\tif (rc)\n\t\tgoto out_unlock;\n\n\twhile (!pfn_reader_done(&pfns)) {\n\t\tdone_first_end_index = pfns.batch_end_index;\n\t\tdone_all_end_index = pfns.batch_start_index;\n\t\txa_for_each(&area->iopt->domains, index, domain) {\n\t\t\trc = batch_to_domain(&pfns.batch, domain, area,\n\t\t\t\t\t     pfns.batch_start_index);\n\t\t\tif (rc)\n\t\t\t\tgoto out_unmap;\n\t\t}\n\t\tdone_all_end_index = done_first_end_index;\n\n\t\trc = pfn_reader_next(&pfns);\n\t\tif (rc)\n\t\t\tgoto out_unmap;\n\t}\n\trc = pfn_reader_update_pinned(&pfns);\n\tif (rc)\n\t\tgoto out_unmap;\n\n\tarea->storage_domain = xa_load(&area->iopt->domains, 0);\n\tinterval_tree_insert(&area->pages_node, &pages->domains_itree);\n\tgoto out_destroy;\n\nout_unmap:\n\tpfn_reader_release_pins(&pfns);\n\txa_for_each(&area->iopt->domains, unmap_index, domain) {\n\t\tunsigned long end_index;\n\n\t\tif (unmap_index < index)\n\t\t\tend_index = done_first_end_index;\n\t\telse\n\t\t\tend_index = done_all_end_index;\n\n\t\t \n\t\tif (unmap_index != area->iopt->next_domain_id - 1) {\n\t\t\tif (end_index != iopt_area_index(area))\n\t\t\t\tiopt_area_unmap_domain_range(\n\t\t\t\t\tarea, domain, iopt_area_index(area),\n\t\t\t\t\tend_index - 1);\n\t\t} else {\n\t\t\tiopt_area_unfill_partial_domain(area, pages, domain,\n\t\t\t\t\t\t\tend_index);\n\t\t}\n\t}\nout_destroy:\n\tpfn_reader_destroy(&pfns);\nout_unlock:\n\tmutex_unlock(&pages->mutex);\n\treturn rc;\n}\n\n \nvoid iopt_area_unfill_domains(struct iopt_area *area, struct iopt_pages *pages)\n{\n\tstruct io_pagetable *iopt = area->iopt;\n\tstruct iommu_domain *domain;\n\tunsigned long index;\n\n\tlockdep_assert_held(&iopt->domains_rwsem);\n\n\tmutex_lock(&pages->mutex);\n\tif (!area->storage_domain)\n\t\tgoto out_unlock;\n\n\txa_for_each(&iopt->domains, index, domain)\n\t\tif (domain != area->storage_domain)\n\t\t\tiopt_area_unmap_domain_range(\n\t\t\t\tarea, domain, iopt_area_index(area),\n\t\t\t\tiopt_area_last_index(area));\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST))\n\t\tWARN_ON(RB_EMPTY_NODE(&area->pages_node.rb));\n\tinterval_tree_remove(&area->pages_node, &pages->domains_itree);\n\tiopt_area_unfill_domain(area, pages, area->storage_domain);\n\tarea->storage_domain = NULL;\nout_unlock:\n\tmutex_unlock(&pages->mutex);\n}\n\nstatic void iopt_pages_unpin_xarray(struct pfn_batch *batch,\n\t\t\t\t    struct iopt_pages *pages,\n\t\t\t\t    unsigned long start_index,\n\t\t\t\t    unsigned long end_index)\n{\n\twhile (start_index <= end_index) {\n\t\tbatch_from_xarray_clear(batch, &pages->pinned_pfns, start_index,\n\t\t\t\t\tend_index);\n\t\tbatch_unpin(batch, pages, 0, batch->total_pfns);\n\t\tstart_index += batch->total_pfns;\n\t\tbatch_clear(batch);\n\t}\n}\n\n \nvoid iopt_pages_unfill_xarray(struct iopt_pages *pages,\n\t\t\t      unsigned long start_index,\n\t\t\t      unsigned long last_index)\n{\n\tstruct interval_tree_double_span_iter span;\n\tu64 backup[BATCH_BACKUP_SIZE];\n\tstruct pfn_batch batch;\n\tbool batch_inited = false;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\tinterval_tree_for_each_double_span(&span, &pages->access_itree,\n\t\t\t\t\t   &pages->domains_itree, start_index,\n\t\t\t\t\t   last_index) {\n\t\tif (!span.is_used) {\n\t\t\tif (!batch_inited) {\n\t\t\t\tbatch_init_backup(&batch,\n\t\t\t\t\t\t  last_index - start_index + 1,\n\t\t\t\t\t\t  backup, sizeof(backup));\n\t\t\t\tbatch_inited = true;\n\t\t\t}\n\t\t\tiopt_pages_unpin_xarray(&batch, pages, span.start_hole,\n\t\t\t\t\t\tspan.last_hole);\n\t\t} else if (span.is_used == 2) {\n\t\t\t \n\t\t\tclear_xarray(&pages->pinned_pfns, span.start_used,\n\t\t\t\t     span.last_used);\n\t\t}\n\t\t \n\t}\n\tif (batch_inited)\n\t\tbatch_destroy(&batch, backup);\n\tupdate_unpinned(pages);\n}\n\n \nvoid iopt_pages_fill_from_xarray(struct iopt_pages *pages,\n\t\t\t\t unsigned long start_index,\n\t\t\t\t unsigned long last_index,\n\t\t\t\t struct page **out_pages)\n{\n\tXA_STATE(xas, &pages->pinned_pfns, start_index);\n\tvoid *entry;\n\n\trcu_read_lock();\n\twhile (start_index <= last_index) {\n\t\tentry = xas_next(&xas);\n\t\tif (xas_retry(&xas, entry))\n\t\t\tcontinue;\n\t\tWARN_ON(!xa_is_value(entry));\n\t\t*(out_pages++) = pfn_to_page(xa_to_value(entry));\n\t\tstart_index++;\n\t}\n\trcu_read_unlock();\n}\n\nstatic int iopt_pages_fill_from_domain(struct iopt_pages *pages,\n\t\t\t\t       unsigned long start_index,\n\t\t\t\t       unsigned long last_index,\n\t\t\t\t       struct page **out_pages)\n{\n\twhile (start_index != last_index + 1) {\n\t\tunsigned long domain_last;\n\t\tstruct iopt_area *area;\n\n\t\tarea = iopt_pages_find_domain_area(pages, start_index);\n\t\tif (WARN_ON(!area))\n\t\t\treturn -EINVAL;\n\n\t\tdomain_last = min(iopt_area_last_index(area), last_index);\n\t\tout_pages = raw_pages_from_domain(area->storage_domain, area,\n\t\t\t\t\t\t  start_index, domain_last,\n\t\t\t\t\t\t  out_pages);\n\t\tstart_index = domain_last + 1;\n\t}\n\treturn 0;\n}\n\nstatic int iopt_pages_fill_from_mm(struct iopt_pages *pages,\n\t\t\t\t   struct pfn_reader_user *user,\n\t\t\t\t   unsigned long start_index,\n\t\t\t\t   unsigned long last_index,\n\t\t\t\t   struct page **out_pages)\n{\n\tunsigned long cur_index = start_index;\n\tint rc;\n\n\twhile (cur_index != last_index + 1) {\n\t\tuser->upages = out_pages + (cur_index - start_index);\n\t\trc = pfn_reader_user_pin(user, pages, cur_index, last_index);\n\t\tif (rc)\n\t\t\tgoto out_unpin;\n\t\tcur_index = user->upages_end;\n\t}\n\treturn 0;\n\nout_unpin:\n\tif (start_index != cur_index)\n\t\tiopt_pages_err_unpin(pages, start_index, cur_index - 1,\n\t\t\t\t     out_pages);\n\treturn rc;\n}\n\n \nint iopt_pages_fill_xarray(struct iopt_pages *pages, unsigned long start_index,\n\t\t\t   unsigned long last_index, struct page **out_pages)\n{\n\tstruct interval_tree_double_span_iter span;\n\tunsigned long xa_end = start_index;\n\tstruct pfn_reader_user user;\n\tint rc;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\tpfn_reader_user_init(&user, pages);\n\tuser.upages_len = (last_index - start_index + 1) * sizeof(*out_pages);\n\tinterval_tree_for_each_double_span(&span, &pages->access_itree,\n\t\t\t\t\t   &pages->domains_itree, start_index,\n\t\t\t\t\t   last_index) {\n\t\tstruct page **cur_pages;\n\n\t\tif (span.is_used == 1) {\n\t\t\tcur_pages = out_pages + (span.start_used - start_index);\n\t\t\tiopt_pages_fill_from_xarray(pages, span.start_used,\n\t\t\t\t\t\t    span.last_used, cur_pages);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (span.is_used == 2) {\n\t\t\tcur_pages = out_pages + (span.start_used - start_index);\n\t\t\tiopt_pages_fill_from_domain(pages, span.start_used,\n\t\t\t\t\t\t    span.last_used, cur_pages);\n\t\t\trc = pages_to_xarray(&pages->pinned_pfns,\n\t\t\t\t\t     span.start_used, span.last_used,\n\t\t\t\t\t     cur_pages);\n\t\t\tif (rc)\n\t\t\t\tgoto out_clean_xa;\n\t\t\txa_end = span.last_used + 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tcur_pages = out_pages + (span.start_hole - start_index);\n\t\trc = iopt_pages_fill_from_mm(pages, &user, span.start_hole,\n\t\t\t\t\t     span.last_hole, cur_pages);\n\t\tif (rc)\n\t\t\tgoto out_clean_xa;\n\t\trc = pages_to_xarray(&pages->pinned_pfns, span.start_hole,\n\t\t\t\t     span.last_hole, cur_pages);\n\t\tif (rc) {\n\t\t\tiopt_pages_err_unpin(pages, span.start_hole,\n\t\t\t\t\t     span.last_hole, cur_pages);\n\t\t\tgoto out_clean_xa;\n\t\t}\n\t\txa_end = span.last_hole + 1;\n\t}\n\trc = pfn_reader_user_update_pinned(&user, pages);\n\tif (rc)\n\t\tgoto out_clean_xa;\n\tuser.upages = NULL;\n\tpfn_reader_user_destroy(&user, pages);\n\treturn 0;\n\nout_clean_xa:\n\tif (start_index != xa_end)\n\t\tiopt_pages_unfill_xarray(pages, start_index, xa_end - 1);\n\tuser.upages = NULL;\n\tpfn_reader_user_destroy(&user, pages);\n\treturn rc;\n}\n\n \nstatic int iopt_pages_rw_slow(struct iopt_pages *pages,\n\t\t\t      unsigned long start_index,\n\t\t\t      unsigned long last_index, unsigned long offset,\n\t\t\t      void *data, unsigned long length,\n\t\t\t      unsigned int flags)\n{\n\tstruct pfn_reader pfns;\n\tint rc;\n\n\tmutex_lock(&pages->mutex);\n\n\trc = pfn_reader_first(&pfns, pages, start_index, last_index);\n\tif (rc)\n\t\tgoto out_unlock;\n\n\twhile (!pfn_reader_done(&pfns)) {\n\t\tunsigned long done;\n\n\t\tdone = batch_rw(&pfns.batch, data, offset, length, flags);\n\t\tdata += done;\n\t\tlength -= done;\n\t\toffset = 0;\n\t\tpfn_reader_unpin(&pfns);\n\n\t\trc = pfn_reader_next(&pfns);\n\t\tif (rc)\n\t\t\tgoto out_destroy;\n\t}\n\tif (WARN_ON(length != 0))\n\t\trc = -EINVAL;\nout_destroy:\n\tpfn_reader_destroy(&pfns);\nout_unlock:\n\tmutex_unlock(&pages->mutex);\n\treturn rc;\n}\n\n \nstatic int iopt_pages_rw_page(struct iopt_pages *pages, unsigned long index,\n\t\t\t      unsigned long offset, void *data,\n\t\t\t      unsigned long length, unsigned int flags)\n{\n\tstruct page *page = NULL;\n\tint rc;\n\n\tif (!mmget_not_zero(pages->source_mm))\n\t\treturn iopt_pages_rw_slow(pages, index, index, offset, data,\n\t\t\t\t\t  length, flags);\n\n\tif (iommufd_should_fail()) {\n\t\trc = -EINVAL;\n\t\tgoto out_mmput;\n\t}\n\n\tmmap_read_lock(pages->source_mm);\n\trc = pin_user_pages_remote(\n\t\tpages->source_mm, (uintptr_t)(pages->uptr + index * PAGE_SIZE),\n\t\t1, (flags & IOMMUFD_ACCESS_RW_WRITE) ? FOLL_WRITE : 0, &page,\n\t\tNULL);\n\tmmap_read_unlock(pages->source_mm);\n\tif (rc != 1) {\n\t\tif (WARN_ON(rc >= 0))\n\t\t\trc = -EINVAL;\n\t\tgoto out_mmput;\n\t}\n\tcopy_data_page(page, data, offset, length, flags);\n\tunpin_user_page(page);\n\trc = 0;\n\nout_mmput:\n\tmmput(pages->source_mm);\n\treturn rc;\n}\n\n \nint iopt_pages_rw_access(struct iopt_pages *pages, unsigned long start_byte,\n\t\t\t void *data, unsigned long length, unsigned int flags)\n{\n\tunsigned long start_index = start_byte / PAGE_SIZE;\n\tunsigned long last_index = (start_byte + length - 1) / PAGE_SIZE;\n\tbool change_mm = current->mm != pages->source_mm;\n\tint rc = 0;\n\n\tif (IS_ENABLED(CONFIG_IOMMUFD_TEST) &&\n\t    (flags & __IOMMUFD_ACCESS_RW_SLOW_PATH))\n\t\tchange_mm = true;\n\n\tif ((flags & IOMMUFD_ACCESS_RW_WRITE) && !pages->writable)\n\t\treturn -EPERM;\n\n\tif (!(flags & IOMMUFD_ACCESS_RW_KTHREAD) && change_mm) {\n\t\tif (start_index == last_index)\n\t\t\treturn iopt_pages_rw_page(pages, start_index,\n\t\t\t\t\t\t  start_byte % PAGE_SIZE, data,\n\t\t\t\t\t\t  length, flags);\n\t\treturn iopt_pages_rw_slow(pages, start_index, last_index,\n\t\t\t\t\t  start_byte % PAGE_SIZE, data, length,\n\t\t\t\t\t  flags);\n\t}\n\n\t \n\tif (change_mm) {\n\t\tif (!mmget_not_zero(pages->source_mm))\n\t\t\treturn iopt_pages_rw_slow(pages, start_index,\n\t\t\t\t\t\t  last_index,\n\t\t\t\t\t\t  start_byte % PAGE_SIZE, data,\n\t\t\t\t\t\t  length, flags);\n\t\tkthread_use_mm(pages->source_mm);\n\t}\n\n\tif (flags & IOMMUFD_ACCESS_RW_WRITE) {\n\t\tif (copy_to_user(pages->uptr + start_byte, data, length))\n\t\t\trc = -EFAULT;\n\t} else {\n\t\tif (copy_from_user(data, pages->uptr + start_byte, length))\n\t\t\trc = -EFAULT;\n\t}\n\n\tif (change_mm) {\n\t\tkthread_unuse_mm(pages->source_mm);\n\t\tmmput(pages->source_mm);\n\t}\n\n\treturn rc;\n}\n\nstatic struct iopt_pages_access *\niopt_pages_get_exact_access(struct iopt_pages *pages, unsigned long index,\n\t\t\t    unsigned long last)\n{\n\tstruct interval_tree_node *node;\n\n\tlockdep_assert_held(&pages->mutex);\n\n\t \n\tfor (node = interval_tree_iter_first(&pages->access_itree, index, last);\n\t     node; node = interval_tree_iter_next(node, index, last))\n\t\tif (node->start == index && node->last == last)\n\t\t\treturn container_of(node, struct iopt_pages_access,\n\t\t\t\t\t    node);\n\treturn NULL;\n}\n\n \nint iopt_area_add_access(struct iopt_area *area, unsigned long start_index,\n\t\t\t  unsigned long last_index, struct page **out_pages,\n\t\t\t  unsigned int flags)\n{\n\tstruct iopt_pages *pages = area->pages;\n\tstruct iopt_pages_access *access;\n\tint rc;\n\n\tif ((flags & IOMMUFD_ACCESS_RW_WRITE) && !pages->writable)\n\t\treturn -EPERM;\n\n\tmutex_lock(&pages->mutex);\n\taccess = iopt_pages_get_exact_access(pages, start_index, last_index);\n\tif (access) {\n\t\tarea->num_accesses++;\n\t\taccess->users++;\n\t\tiopt_pages_fill_from_xarray(pages, start_index, last_index,\n\t\t\t\t\t    out_pages);\n\t\tmutex_unlock(&pages->mutex);\n\t\treturn 0;\n\t}\n\n\taccess = kzalloc(sizeof(*access), GFP_KERNEL_ACCOUNT);\n\tif (!access) {\n\t\trc = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\trc = iopt_pages_fill_xarray(pages, start_index, last_index, out_pages);\n\tif (rc)\n\t\tgoto err_free;\n\n\taccess->node.start = start_index;\n\taccess->node.last = last_index;\n\taccess->users = 1;\n\tarea->num_accesses++;\n\tinterval_tree_insert(&access->node, &pages->access_itree);\n\tmutex_unlock(&pages->mutex);\n\treturn 0;\n\nerr_free:\n\tkfree(access);\nerr_unlock:\n\tmutex_unlock(&pages->mutex);\n\treturn rc;\n}\n\n \nvoid iopt_area_remove_access(struct iopt_area *area, unsigned long start_index,\n\t\t\t     unsigned long last_index)\n{\n\tstruct iopt_pages *pages = area->pages;\n\tstruct iopt_pages_access *access;\n\n\tmutex_lock(&pages->mutex);\n\taccess = iopt_pages_get_exact_access(pages, start_index, last_index);\n\tif (WARN_ON(!access))\n\t\tgoto out_unlock;\n\n\tWARN_ON(area->num_accesses == 0 || access->users == 0);\n\tarea->num_accesses--;\n\taccess->users--;\n\tif (access->users)\n\t\tgoto out_unlock;\n\n\tinterval_tree_remove(&access->node, &pages->access_itree);\n\tiopt_pages_unfill_xarray(pages, start_index, last_index);\n\tkfree(access);\nout_unlock:\n\tmutex_unlock(&pages->mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}