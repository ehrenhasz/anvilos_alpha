{
  "module_name": "sun50i-iommu.c",
  "hash_id": "17660d931bc117fccd80b0f6f9eff3ace0c2e640894ad65b791737dda16d2947",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/sun50i-iommu.c",
  "human_readable_source": "\n\n\n\n#include <linux/bitfield.h>\n#include <linux/bug.h>\n#include <linux/clk.h>\n#include <linux/device.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/iommu.h>\n#include <linux/iopoll.h>\n#include <linux/ioport.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/of_platform.h>\n#include <linux/platform_device.h>\n#include <linux/pm.h>\n#include <linux/pm_runtime.h>\n#include <linux/reset.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/types.h>\n\n#define IOMMU_RESET_REG\t\t\t0x010\n#define IOMMU_RESET_RELEASE_ALL\t\t\t0xffffffff\n#define IOMMU_ENABLE_REG\t\t0x020\n#define IOMMU_ENABLE_ENABLE\t\t\tBIT(0)\n\n#define IOMMU_BYPASS_REG\t\t0x030\n#define IOMMU_AUTO_GATING_REG\t\t0x040\n#define IOMMU_AUTO_GATING_ENABLE\t\tBIT(0)\n\n#define IOMMU_WBUF_CTRL_REG\t\t0x044\n#define IOMMU_OOO_CTRL_REG\t\t0x048\n#define IOMMU_4KB_BDY_PRT_CTRL_REG\t0x04c\n#define IOMMU_TTB_REG\t\t\t0x050\n#define IOMMU_TLB_ENABLE_REG\t\t0x060\n#define IOMMU_TLB_PREFETCH_REG\t\t0x070\n#define IOMMU_TLB_PREFETCH_MASTER_ENABLE(m)\tBIT(m)\n\n#define IOMMU_TLB_FLUSH_REG\t\t0x080\n#define IOMMU_TLB_FLUSH_PTW_CACHE\t\tBIT(17)\n#define IOMMU_TLB_FLUSH_MACRO_TLB\t\tBIT(16)\n#define IOMMU_TLB_FLUSH_MICRO_TLB(i)\t\t(BIT(i) & GENMASK(5, 0))\n\n#define IOMMU_TLB_IVLD_ADDR_REG\t\t0x090\n#define IOMMU_TLB_IVLD_ADDR_MASK_REG\t0x094\n#define IOMMU_TLB_IVLD_ENABLE_REG\t0x098\n#define IOMMU_TLB_IVLD_ENABLE_ENABLE\t\tBIT(0)\n\n#define IOMMU_PC_IVLD_ADDR_REG\t\t0x0a0\n#define IOMMU_PC_IVLD_ENABLE_REG\t0x0a8\n#define IOMMU_PC_IVLD_ENABLE_ENABLE\t\tBIT(0)\n\n#define IOMMU_DM_AUT_CTRL_REG(d)\t(0x0b0 + ((d) / 2) * 4)\n#define IOMMU_DM_AUT_CTRL_RD_UNAVAIL(d, m)\t(1 << (((d & 1) * 16) + ((m) * 2)))\n#define IOMMU_DM_AUT_CTRL_WR_UNAVAIL(d, m)\t(1 << (((d & 1) * 16) + ((m) * 2) + 1))\n\n#define IOMMU_DM_AUT_OVWT_REG\t\t0x0d0\n#define IOMMU_INT_ENABLE_REG\t\t0x100\n#define IOMMU_INT_CLR_REG\t\t0x104\n#define IOMMU_INT_STA_REG\t\t0x108\n#define IOMMU_INT_ERR_ADDR_REG(i)\t(0x110 + (i) * 4)\n#define IOMMU_INT_ERR_ADDR_L1_REG\t0x130\n#define IOMMU_INT_ERR_ADDR_L2_REG\t0x134\n#define IOMMU_INT_ERR_DATA_REG(i)\t(0x150 + (i) * 4)\n#define IOMMU_L1PG_INT_REG\t\t0x0180\n#define IOMMU_L2PG_INT_REG\t\t0x0184\n\n#define IOMMU_INT_INVALID_L2PG\t\t\tBIT(17)\n#define IOMMU_INT_INVALID_L1PG\t\t\tBIT(16)\n#define IOMMU_INT_MASTER_PERMISSION(m)\t\tBIT(m)\n#define IOMMU_INT_MASTER_MASK\t\t\t(IOMMU_INT_MASTER_PERMISSION(0) | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_PERMISSION(1) | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_PERMISSION(2) | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_PERMISSION(3) | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_PERMISSION(4) | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_PERMISSION(5))\n#define IOMMU_INT_MASK\t\t\t\t(IOMMU_INT_INVALID_L1PG | \\\n\t\t\t\t\t\t IOMMU_INT_INVALID_L2PG | \\\n\t\t\t\t\t\t IOMMU_INT_MASTER_MASK)\n\n#define PT_ENTRY_SIZE\t\t\tsizeof(u32)\n\n#define NUM_DT_ENTRIES\t\t\t4096\n#define DT_SIZE\t\t\t\t(NUM_DT_ENTRIES * PT_ENTRY_SIZE)\n\n#define NUM_PT_ENTRIES\t\t\t256\n#define PT_SIZE\t\t\t\t(NUM_PT_ENTRIES * PT_ENTRY_SIZE)\n\n#define SPAGE_SIZE\t\t\t4096\n\nstruct sun50i_iommu {\n\tstruct iommu_device iommu;\n\n\t \n\tspinlock_t iommu_lock;\n\n\tstruct device *dev;\n\tvoid __iomem *base;\n\tstruct reset_control *reset;\n\tstruct clk *clk;\n\n\tstruct iommu_domain *domain;\n\tstruct iommu_group *group;\n\tstruct kmem_cache *pt_pool;\n};\n\nstruct sun50i_iommu_domain {\n\tstruct iommu_domain domain;\n\n\t \n\trefcount_t refcnt;\n\n\t \n\tu32 *dt;\n\tdma_addr_t dt_dma;\n\n\tstruct sun50i_iommu *iommu;\n};\n\nstatic struct sun50i_iommu_domain *to_sun50i_domain(struct iommu_domain *domain)\n{\n\treturn container_of(domain, struct sun50i_iommu_domain, domain);\n}\n\nstatic struct sun50i_iommu *sun50i_iommu_from_dev(struct device *dev)\n{\n\treturn dev_iommu_priv_get(dev);\n}\n\nstatic u32 iommu_read(struct sun50i_iommu *iommu, u32 offset)\n{\n\treturn readl(iommu->base + offset);\n}\n\nstatic void iommu_write(struct sun50i_iommu *iommu, u32 offset, u32 value)\n{\n\twritel(value, iommu->base + offset);\n}\n\n \n\n#define SUN50I_IOVA_DTE_MASK\tGENMASK(31, 20)\n#define SUN50I_IOVA_PTE_MASK\tGENMASK(19, 12)\n#define SUN50I_IOVA_PAGE_MASK\tGENMASK(11, 0)\n\nstatic u32 sun50i_iova_get_dte_index(dma_addr_t iova)\n{\n\treturn FIELD_GET(SUN50I_IOVA_DTE_MASK, iova);\n}\n\nstatic u32 sun50i_iova_get_pte_index(dma_addr_t iova)\n{\n\treturn FIELD_GET(SUN50I_IOVA_PTE_MASK, iova);\n}\n\nstatic u32 sun50i_iova_get_page_offset(dma_addr_t iova)\n{\n\treturn FIELD_GET(SUN50I_IOVA_PAGE_MASK, iova);\n}\n\n \n\n#define SUN50I_DTE_PT_ADDRESS_MASK\tGENMASK(31, 10)\n#define SUN50I_DTE_PT_ATTRS\t\tGENMASK(1, 0)\n#define SUN50I_DTE_PT_VALID\t\t1\n\nstatic phys_addr_t sun50i_dte_get_pt_address(u32 dte)\n{\n\treturn (phys_addr_t)dte & SUN50I_DTE_PT_ADDRESS_MASK;\n}\n\nstatic bool sun50i_dte_is_pt_valid(u32 dte)\n{\n\treturn (dte & SUN50I_DTE_PT_ATTRS) == SUN50I_DTE_PT_VALID;\n}\n\nstatic u32 sun50i_mk_dte(dma_addr_t pt_dma)\n{\n\treturn (pt_dma & SUN50I_DTE_PT_ADDRESS_MASK) | SUN50I_DTE_PT_VALID;\n}\n\n \n\nenum sun50i_iommu_aci {\n\tSUN50I_IOMMU_ACI_DO_NOT_USE = 0,\n\tSUN50I_IOMMU_ACI_NONE,\n\tSUN50I_IOMMU_ACI_RD,\n\tSUN50I_IOMMU_ACI_WR,\n\tSUN50I_IOMMU_ACI_RD_WR,\n};\n\n#define SUN50I_PTE_PAGE_ADDRESS_MASK\tGENMASK(31, 12)\n#define SUN50I_PTE_ACI_MASK\t\tGENMASK(7, 4)\n#define SUN50I_PTE_PAGE_VALID\t\tBIT(1)\n\nstatic phys_addr_t sun50i_pte_get_page_address(u32 pte)\n{\n\treturn (phys_addr_t)pte & SUN50I_PTE_PAGE_ADDRESS_MASK;\n}\n\nstatic enum sun50i_iommu_aci sun50i_get_pte_aci(u32 pte)\n{\n\treturn FIELD_GET(SUN50I_PTE_ACI_MASK, pte);\n}\n\nstatic bool sun50i_pte_is_page_valid(u32 pte)\n{\n\treturn pte & SUN50I_PTE_PAGE_VALID;\n}\n\nstatic u32 sun50i_mk_pte(phys_addr_t page, int prot)\n{\n\tenum sun50i_iommu_aci aci;\n\tu32 flags = 0;\n\n\tif ((prot & (IOMMU_READ | IOMMU_WRITE)) == (IOMMU_READ | IOMMU_WRITE))\n\t\taci = SUN50I_IOMMU_ACI_RD_WR;\n\telse if (prot & IOMMU_READ)\n\t\taci = SUN50I_IOMMU_ACI_RD;\n\telse if (prot & IOMMU_WRITE)\n\t\taci = SUN50I_IOMMU_ACI_WR;\n\telse\n\t\taci = SUN50I_IOMMU_ACI_NONE;\n\n\tflags |= FIELD_PREP(SUN50I_PTE_ACI_MASK, aci);\n\tpage &= SUN50I_PTE_PAGE_ADDRESS_MASK;\n\treturn page | flags | SUN50I_PTE_PAGE_VALID;\n}\n\nstatic void sun50i_table_flush(struct sun50i_iommu_domain *sun50i_domain,\n\t\t\t       void *vaddr, unsigned int count)\n{\n\tstruct sun50i_iommu *iommu = sun50i_domain->iommu;\n\tdma_addr_t dma = virt_to_phys(vaddr);\n\tsize_t size = count * PT_ENTRY_SIZE;\n\n\tdma_sync_single_for_device(iommu->dev, dma, size, DMA_TO_DEVICE);\n}\n\nstatic void sun50i_iommu_zap_iova(struct sun50i_iommu *iommu,\n\t\t\t\t  unsigned long iova)\n{\n\tu32 reg;\n\tint ret;\n\n\tiommu_write(iommu, IOMMU_TLB_IVLD_ADDR_REG, iova);\n\tiommu_write(iommu, IOMMU_TLB_IVLD_ADDR_MASK_REG, GENMASK(31, 12));\n\tiommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG,\n\t\t    IOMMU_TLB_IVLD_ENABLE_ENABLE);\n\n\tret = readl_poll_timeout_atomic(iommu->base + IOMMU_TLB_IVLD_ENABLE_REG,\n\t\t\t\t\treg, !reg, 1, 2000);\n\tif (ret)\n\t\tdev_warn(iommu->dev, \"TLB invalidation timed out!\\n\");\n}\n\nstatic void sun50i_iommu_zap_ptw_cache(struct sun50i_iommu *iommu,\n\t\t\t\t       unsigned long iova)\n{\n\tu32 reg;\n\tint ret;\n\n\tiommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG, iova);\n\tiommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG,\n\t\t    IOMMU_PC_IVLD_ENABLE_ENABLE);\n\n\tret = readl_poll_timeout_atomic(iommu->base + IOMMU_PC_IVLD_ENABLE_REG,\n\t\t\t\t\treg, !reg, 1, 2000);\n\tif (ret)\n\t\tdev_warn(iommu->dev, \"PTW cache invalidation timed out!\\n\");\n}\n\nstatic void sun50i_iommu_zap_range(struct sun50i_iommu *iommu,\n\t\t\t\t   unsigned long iova, size_t size)\n{\n\tassert_spin_locked(&iommu->iommu_lock);\n\n\tiommu_write(iommu, IOMMU_AUTO_GATING_REG, 0);\n\n\tsun50i_iommu_zap_iova(iommu, iova);\n\tsun50i_iommu_zap_iova(iommu, iova + SPAGE_SIZE);\n\tif (size > SPAGE_SIZE) {\n\t\tsun50i_iommu_zap_iova(iommu, iova + size);\n\t\tsun50i_iommu_zap_iova(iommu, iova + size + SPAGE_SIZE);\n\t}\n\tsun50i_iommu_zap_ptw_cache(iommu, iova);\n\tsun50i_iommu_zap_ptw_cache(iommu, iova + SZ_1M);\n\tif (size > SZ_1M) {\n\t\tsun50i_iommu_zap_ptw_cache(iommu, iova + size);\n\t\tsun50i_iommu_zap_ptw_cache(iommu, iova + size + SZ_1M);\n\t}\n\n\tiommu_write(iommu, IOMMU_AUTO_GATING_REG, IOMMU_AUTO_GATING_ENABLE);\n}\n\nstatic int sun50i_iommu_flush_all_tlb(struct sun50i_iommu *iommu)\n{\n\tu32 reg;\n\tint ret;\n\n\tassert_spin_locked(&iommu->iommu_lock);\n\n\tiommu_write(iommu,\n\t\t    IOMMU_TLB_FLUSH_REG,\n\t\t    IOMMU_TLB_FLUSH_PTW_CACHE |\n\t\t    IOMMU_TLB_FLUSH_MACRO_TLB |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(5) |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(4) |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(3) |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(2) |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(1) |\n\t\t    IOMMU_TLB_FLUSH_MICRO_TLB(0));\n\n\tret = readl_poll_timeout_atomic(iommu->base + IOMMU_TLB_FLUSH_REG,\n\t\t\t\t\treg, !reg,\n\t\t\t\t\t1, 2000);\n\tif (ret)\n\t\tdev_warn(iommu->dev, \"TLB Flush timed out!\\n\");\n\n\treturn ret;\n}\n\nstatic void sun50i_iommu_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tstruct sun50i_iommu *iommu = sun50i_domain->iommu;\n\tunsigned long flags;\n\n\t \n\tif (!iommu)\n\t\treturn;\n\n\tspin_lock_irqsave(&iommu->iommu_lock, flags);\n\tsun50i_iommu_flush_all_tlb(iommu);\n\tspin_unlock_irqrestore(&iommu->iommu_lock, flags);\n}\n\nstatic void sun50i_iommu_iotlb_sync_map(struct iommu_domain *domain,\n\t\t\t\t\tunsigned long iova, size_t size)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tstruct sun50i_iommu *iommu = sun50i_domain->iommu;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&iommu->iommu_lock, flags);\n\tsun50i_iommu_zap_range(iommu, iova, size);\n\tspin_unlock_irqrestore(&iommu->iommu_lock, flags);\n}\n\nstatic void sun50i_iommu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\t    struct iommu_iotlb_gather *gather)\n{\n\tsun50i_iommu_flush_iotlb_all(domain);\n}\n\nstatic int sun50i_iommu_enable(struct sun50i_iommu *iommu)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain;\n\tunsigned long flags;\n\tint ret;\n\n\tif (!iommu->domain)\n\t\treturn 0;\n\n\tsun50i_domain = to_sun50i_domain(iommu->domain);\n\n\tret = reset_control_deassert(iommu->reset);\n\tif (ret)\n\t\treturn ret;\n\n\tret = clk_prepare_enable(iommu->clk);\n\tif (ret)\n\t\tgoto err_reset_assert;\n\n\tspin_lock_irqsave(&iommu->iommu_lock, flags);\n\n\tiommu_write(iommu, IOMMU_TTB_REG, sun50i_domain->dt_dma);\n\tiommu_write(iommu, IOMMU_TLB_PREFETCH_REG,\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(0) |\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(1) |\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(2) |\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(3) |\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(4) |\n\t\t    IOMMU_TLB_PREFETCH_MASTER_ENABLE(5));\n\tiommu_write(iommu, IOMMU_INT_ENABLE_REG, IOMMU_INT_MASK);\n\tiommu_write(iommu, IOMMU_DM_AUT_CTRL_REG(SUN50I_IOMMU_ACI_NONE),\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 0) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 0) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 1) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 1) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 2) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 2) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 3) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 3) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 4) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 4) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 5) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_NONE, 5));\n\n\tiommu_write(iommu, IOMMU_DM_AUT_CTRL_REG(SUN50I_IOMMU_ACI_RD),\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 0) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 1) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 2) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 3) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 4) |\n\t\t    IOMMU_DM_AUT_CTRL_WR_UNAVAIL(SUN50I_IOMMU_ACI_RD, 5));\n\n\tiommu_write(iommu, IOMMU_DM_AUT_CTRL_REG(SUN50I_IOMMU_ACI_WR),\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 0) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 1) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 2) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 3) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 4) |\n\t\t    IOMMU_DM_AUT_CTRL_RD_UNAVAIL(SUN50I_IOMMU_ACI_WR, 5));\n\n\tret = sun50i_iommu_flush_all_tlb(iommu);\n\tif (ret) {\n\t\tspin_unlock_irqrestore(&iommu->iommu_lock, flags);\n\t\tgoto err_clk_disable;\n\t}\n\n\tiommu_write(iommu, IOMMU_AUTO_GATING_REG, IOMMU_AUTO_GATING_ENABLE);\n\tiommu_write(iommu, IOMMU_ENABLE_REG, IOMMU_ENABLE_ENABLE);\n\n\tspin_unlock_irqrestore(&iommu->iommu_lock, flags);\n\n\treturn 0;\n\nerr_clk_disable:\n\tclk_disable_unprepare(iommu->clk);\n\nerr_reset_assert:\n\treset_control_assert(iommu->reset);\n\n\treturn ret;\n}\n\nstatic void sun50i_iommu_disable(struct sun50i_iommu *iommu)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&iommu->iommu_lock, flags);\n\n\tiommu_write(iommu, IOMMU_ENABLE_REG, 0);\n\tiommu_write(iommu, IOMMU_TTB_REG, 0);\n\n\tspin_unlock_irqrestore(&iommu->iommu_lock, flags);\n\n\tclk_disable_unprepare(iommu->clk);\n\treset_control_assert(iommu->reset);\n}\n\nstatic void *sun50i_iommu_alloc_page_table(struct sun50i_iommu *iommu,\n\t\t\t\t\t   gfp_t gfp)\n{\n\tdma_addr_t pt_dma;\n\tu32 *page_table;\n\n\tpage_table = kmem_cache_zalloc(iommu->pt_pool, gfp);\n\tif (!page_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpt_dma = dma_map_single(iommu->dev, page_table, PT_SIZE, DMA_TO_DEVICE);\n\tif (dma_mapping_error(iommu->dev, pt_dma)) {\n\t\tdev_err(iommu->dev, \"Couldn't map L2 Page Table\\n\");\n\t\tkmem_cache_free(iommu->pt_pool, page_table);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\tWARN_ON(pt_dma != virt_to_phys(page_table));\n\n\treturn page_table;\n}\n\nstatic void sun50i_iommu_free_page_table(struct sun50i_iommu *iommu,\n\t\t\t\t\t u32 *page_table)\n{\n\tphys_addr_t pt_phys = virt_to_phys(page_table);\n\n\tdma_unmap_single(iommu->dev, pt_phys, PT_SIZE, DMA_TO_DEVICE);\n\tkmem_cache_free(iommu->pt_pool, page_table);\n}\n\nstatic u32 *sun50i_dte_get_page_table(struct sun50i_iommu_domain *sun50i_domain,\n\t\t\t\t      dma_addr_t iova, gfp_t gfp)\n{\n\tstruct sun50i_iommu *iommu = sun50i_domain->iommu;\n\tu32 *page_table;\n\tu32 *dte_addr;\n\tu32 old_dte;\n\tu32 dte;\n\n\tdte_addr = &sun50i_domain->dt[sun50i_iova_get_dte_index(iova)];\n\tdte = *dte_addr;\n\tif (sun50i_dte_is_pt_valid(dte)) {\n\t\tphys_addr_t pt_phys = sun50i_dte_get_pt_address(dte);\n\t\treturn (u32 *)phys_to_virt(pt_phys);\n\t}\n\n\tpage_table = sun50i_iommu_alloc_page_table(iommu, gfp);\n\tif (IS_ERR(page_table))\n\t\treturn page_table;\n\n\tdte = sun50i_mk_dte(virt_to_phys(page_table));\n\told_dte = cmpxchg(dte_addr, 0, dte);\n\tif (old_dte) {\n\t\tphys_addr_t installed_pt_phys =\n\t\t\tsun50i_dte_get_pt_address(old_dte);\n\t\tu32 *installed_pt = phys_to_virt(installed_pt_phys);\n\t\tu32 *drop_pt = page_table;\n\n\t\tpage_table = installed_pt;\n\t\tdte = old_dte;\n\t\tsun50i_iommu_free_page_table(iommu, drop_pt);\n\t}\n\n\tsun50i_table_flush(sun50i_domain, page_table, NUM_PT_ENTRIES);\n\tsun50i_table_flush(sun50i_domain, dte_addr, 1);\n\n\treturn page_table;\n}\n\nstatic int sun50i_iommu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t\t    phys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tstruct sun50i_iommu *iommu = sun50i_domain->iommu;\n\tu32 pte_index;\n\tu32 *page_table, *pte_addr;\n\tint ret = 0;\n\n\tpage_table = sun50i_dte_get_page_table(sun50i_domain, iova, gfp);\n\tif (IS_ERR(page_table)) {\n\t\tret = PTR_ERR(page_table);\n\t\tgoto out;\n\t}\n\n\tpte_index = sun50i_iova_get_pte_index(iova);\n\tpte_addr = &page_table[pte_index];\n\tif (unlikely(sun50i_pte_is_page_valid(*pte_addr))) {\n\t\tphys_addr_t page_phys = sun50i_pte_get_page_address(*pte_addr);\n\t\tdev_err(iommu->dev,\n\t\t\t\"iova %pad already mapped to %pa cannot remap to %pa prot: %#x\\n\",\n\t\t\t&iova, &page_phys, &paddr, prot);\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t*pte_addr = sun50i_mk_pte(paddr, prot);\n\tsun50i_table_flush(sun50i_domain, pte_addr, 1);\n\nout:\n\treturn ret;\n}\n\nstatic size_t sun50i_iommu_unmap(struct iommu_domain *domain, unsigned long iova,\n\t\t\t\t size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tphys_addr_t pt_phys;\n\tu32 *pte_addr;\n\tu32 dte;\n\n\tdte = sun50i_domain->dt[sun50i_iova_get_dte_index(iova)];\n\tif (!sun50i_dte_is_pt_valid(dte))\n\t\treturn 0;\n\n\tpt_phys = sun50i_dte_get_pt_address(dte);\n\tpte_addr = (u32 *)phys_to_virt(pt_phys) + sun50i_iova_get_pte_index(iova);\n\n\tif (!sun50i_pte_is_page_valid(*pte_addr))\n\t\treturn 0;\n\n\tmemset(pte_addr, 0, sizeof(*pte_addr));\n\tsun50i_table_flush(sun50i_domain, pte_addr, 1);\n\n\treturn SZ_4K;\n}\n\nstatic phys_addr_t sun50i_iommu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\t     dma_addr_t iova)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tphys_addr_t pt_phys;\n\tu32 *page_table;\n\tu32 dte, pte;\n\n\tdte = sun50i_domain->dt[sun50i_iova_get_dte_index(iova)];\n\tif (!sun50i_dte_is_pt_valid(dte))\n\t\treturn 0;\n\n\tpt_phys = sun50i_dte_get_pt_address(dte);\n\tpage_table = (u32 *)phys_to_virt(pt_phys);\n\tpte = page_table[sun50i_iova_get_pte_index(iova)];\n\tif (!sun50i_pte_is_page_valid(pte))\n\t\treturn 0;\n\n\treturn sun50i_pte_get_page_address(pte) +\n\t\tsun50i_iova_get_page_offset(iova);\n}\n\nstatic struct iommu_domain *sun50i_iommu_domain_alloc(unsigned type)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain;\n\n\tif (type != IOMMU_DOMAIN_DMA &&\n\t    type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn NULL;\n\n\tsun50i_domain = kzalloc(sizeof(*sun50i_domain), GFP_KERNEL);\n\tif (!sun50i_domain)\n\t\treturn NULL;\n\n\tsun50i_domain->dt = (u32 *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t\t    get_order(DT_SIZE));\n\tif (!sun50i_domain->dt)\n\t\tgoto err_free_domain;\n\n\trefcount_set(&sun50i_domain->refcnt, 1);\n\n\tsun50i_domain->domain.geometry.aperture_start = 0;\n\tsun50i_domain->domain.geometry.aperture_end = DMA_BIT_MASK(32);\n\tsun50i_domain->domain.geometry.force_aperture = true;\n\n\treturn &sun50i_domain->domain;\n\nerr_free_domain:\n\tkfree(sun50i_domain);\n\n\treturn NULL;\n}\n\nstatic void sun50i_iommu_domain_free(struct iommu_domain *domain)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\n\tfree_pages((unsigned long)sun50i_domain->dt, get_order(DT_SIZE));\n\tsun50i_domain->dt = NULL;\n\n\tkfree(sun50i_domain);\n}\n\nstatic int sun50i_iommu_attach_domain(struct sun50i_iommu *iommu,\n\t\t\t\t      struct sun50i_iommu_domain *sun50i_domain)\n{\n\tiommu->domain = &sun50i_domain->domain;\n\tsun50i_domain->iommu = iommu;\n\n\tsun50i_domain->dt_dma = dma_map_single(iommu->dev, sun50i_domain->dt,\n\t\t\t\t\t       DT_SIZE, DMA_TO_DEVICE);\n\tif (dma_mapping_error(iommu->dev, sun50i_domain->dt_dma)) {\n\t\tdev_err(iommu->dev, \"Couldn't map L1 Page Table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn sun50i_iommu_enable(iommu);\n}\n\nstatic void sun50i_iommu_detach_domain(struct sun50i_iommu *iommu,\n\t\t\t\t       struct sun50i_iommu_domain *sun50i_domain)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < NUM_DT_ENTRIES; i++) {\n\t\tphys_addr_t pt_phys;\n\t\tu32 *page_table;\n\t\tu32 *dte_addr;\n\t\tu32 dte;\n\n\t\tdte_addr = &sun50i_domain->dt[i];\n\t\tdte = *dte_addr;\n\t\tif (!sun50i_dte_is_pt_valid(dte))\n\t\t\tcontinue;\n\n\t\tmemset(dte_addr, 0, sizeof(*dte_addr));\n\t\tsun50i_table_flush(sun50i_domain, dte_addr, 1);\n\n\t\tpt_phys = sun50i_dte_get_pt_address(dte);\n\t\tpage_table = phys_to_virt(pt_phys);\n\t\tsun50i_iommu_free_page_table(iommu, page_table);\n\t}\n\n\n\tsun50i_iommu_disable(iommu);\n\n\tdma_unmap_single(iommu->dev, virt_to_phys(sun50i_domain->dt),\n\t\t\t DT_SIZE, DMA_TO_DEVICE);\n\n\tiommu->domain = NULL;\n}\n\nstatic void sun50i_iommu_detach_device(struct iommu_domain *domain,\n\t\t\t\t       struct device *dev)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tstruct sun50i_iommu *iommu = dev_iommu_priv_get(dev);\n\n\tdev_dbg(dev, \"Detaching from IOMMU domain\\n\");\n\n\tif (iommu->domain != domain)\n\t\treturn;\n\n\tif (refcount_dec_and_test(&sun50i_domain->refcnt))\n\t\tsun50i_iommu_detach_domain(iommu, sun50i_domain);\n}\n\nstatic int sun50i_iommu_attach_device(struct iommu_domain *domain,\n\t\t\t\t      struct device *dev)\n{\n\tstruct sun50i_iommu_domain *sun50i_domain = to_sun50i_domain(domain);\n\tstruct sun50i_iommu *iommu;\n\n\tiommu = sun50i_iommu_from_dev(dev);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\tdev_dbg(dev, \"Attaching to IOMMU domain\\n\");\n\n\trefcount_inc(&sun50i_domain->refcnt);\n\n\tif (iommu->domain == domain)\n\t\treturn 0;\n\n\tif (iommu->domain)\n\t\tsun50i_iommu_detach_device(iommu->domain, dev);\n\n\tsun50i_iommu_attach_domain(iommu, sun50i_domain);\n\n\treturn 0;\n}\n\nstatic struct iommu_device *sun50i_iommu_probe_device(struct device *dev)\n{\n\tstruct sun50i_iommu *iommu;\n\n\tiommu = sun50i_iommu_from_dev(dev);\n\tif (!iommu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\treturn &iommu->iommu;\n}\n\nstatic struct iommu_group *sun50i_iommu_device_group(struct device *dev)\n{\n\tstruct sun50i_iommu *iommu = sun50i_iommu_from_dev(dev);\n\n\treturn iommu_group_ref_get(iommu->group);\n}\n\nstatic int sun50i_iommu_of_xlate(struct device *dev,\n\t\t\t\t struct of_phandle_args *args)\n{\n\tstruct platform_device *iommu_pdev = of_find_device_by_node(args->np);\n\tunsigned id = args->args[0];\n\n\tdev_iommu_priv_set(dev, platform_get_drvdata(iommu_pdev));\n\n\treturn iommu_fwspec_add_ids(dev, &id, 1);\n}\n\nstatic const struct iommu_ops sun50i_iommu_ops = {\n\t.pgsize_bitmap\t= SZ_4K,\n\t.device_group\t= sun50i_iommu_device_group,\n\t.domain_alloc\t= sun50i_iommu_domain_alloc,\n\t.of_xlate\t= sun50i_iommu_of_xlate,\n\t.probe_device\t= sun50i_iommu_probe_device,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= sun50i_iommu_attach_device,\n\t\t.flush_iotlb_all = sun50i_iommu_flush_iotlb_all,\n\t\t.iotlb_sync_map = sun50i_iommu_iotlb_sync_map,\n\t\t.iotlb_sync\t= sun50i_iommu_iotlb_sync,\n\t\t.iova_to_phys\t= sun50i_iommu_iova_to_phys,\n\t\t.map\t\t= sun50i_iommu_map,\n\t\t.unmap\t\t= sun50i_iommu_unmap,\n\t\t.free\t\t= sun50i_iommu_domain_free,\n\t}\n};\n\nstatic void sun50i_iommu_report_fault(struct sun50i_iommu *iommu,\n\t\t\t\t      unsigned master, phys_addr_t iova,\n\t\t\t\t      unsigned prot)\n{\n\tdev_err(iommu->dev, \"Page fault for %pad (master %d, dir %s)\\n\",\n\t\t&iova, master, (prot == IOMMU_FAULT_WRITE) ? \"wr\" : \"rd\");\n\n\tif (iommu->domain)\n\t\treport_iommu_fault(iommu->domain, iommu->dev, iova, prot);\n\telse\n\t\tdev_err(iommu->dev, \"Page fault while iommu not attached to any domain?\\n\");\n\n\tsun50i_iommu_zap_range(iommu, iova, SPAGE_SIZE);\n}\n\nstatic phys_addr_t sun50i_iommu_handle_pt_irq(struct sun50i_iommu *iommu,\n\t\t\t\t\t      unsigned addr_reg,\n\t\t\t\t\t      unsigned blame_reg)\n{\n\tphys_addr_t iova;\n\tunsigned master;\n\tu32 blame;\n\n\tassert_spin_locked(&iommu->iommu_lock);\n\n\tiova = iommu_read(iommu, addr_reg);\n\tblame = iommu_read(iommu, blame_reg);\n\tmaster = ilog2(blame & IOMMU_INT_MASTER_MASK);\n\n\t \n\tsun50i_iommu_report_fault(iommu, master, iova, IOMMU_FAULT_READ);\n\n\treturn iova;\n}\n\nstatic phys_addr_t sun50i_iommu_handle_perm_irq(struct sun50i_iommu *iommu)\n{\n\tenum sun50i_iommu_aci aci;\n\tphys_addr_t iova;\n\tunsigned master;\n\tunsigned dir;\n\tu32 blame;\n\n\tassert_spin_locked(&iommu->iommu_lock);\n\n\tblame = iommu_read(iommu, IOMMU_INT_STA_REG);\n\tmaster = ilog2(blame & IOMMU_INT_MASTER_MASK);\n\tiova = iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG(master));\n\taci = sun50i_get_pte_aci(iommu_read(iommu,\n\t\t\t\t\t    IOMMU_INT_ERR_DATA_REG(master)));\n\n\tswitch (aci) {\n\t\t \n\tcase SUN50I_IOMMU_ACI_RD:\n\t\tdir = IOMMU_FAULT_WRITE;\n\t\tbreak;\n\n\t\t \n\tcase SUN50I_IOMMU_ACI_WR:\n\n\t\t \n\tcase SUN50I_IOMMU_ACI_NONE:\n\n\t\t \n\tcase SUN50I_IOMMU_ACI_RD_WR:\n\tdefault:\n\t\tdir = IOMMU_FAULT_READ;\n\t\tbreak;\n\t}\n\n\t \n\tsun50i_iommu_report_fault(iommu, master, iova, dir);\n\n\treturn iova;\n}\n\nstatic irqreturn_t sun50i_iommu_irq(int irq, void *dev_id)\n{\n\tu32 status, l1_status, l2_status, resets;\n\tstruct sun50i_iommu *iommu = dev_id;\n\n\tspin_lock(&iommu->iommu_lock);\n\n\tstatus = iommu_read(iommu, IOMMU_INT_STA_REG);\n\tif (!(status & IOMMU_INT_MASK)) {\n\t\tspin_unlock(&iommu->iommu_lock);\n\t\treturn IRQ_NONE;\n\t}\n\n\tl1_status = iommu_read(iommu, IOMMU_L1PG_INT_REG);\n\tl2_status = iommu_read(iommu, IOMMU_L2PG_INT_REG);\n\n\tif (status & IOMMU_INT_INVALID_L2PG)\n\t\tsun50i_iommu_handle_pt_irq(iommu,\n\t\t\t\t\t    IOMMU_INT_ERR_ADDR_L2_REG,\n\t\t\t\t\t    IOMMU_L2PG_INT_REG);\n\telse if (status & IOMMU_INT_INVALID_L1PG)\n\t\tsun50i_iommu_handle_pt_irq(iommu,\n\t\t\t\t\t   IOMMU_INT_ERR_ADDR_L1_REG,\n\t\t\t\t\t   IOMMU_L1PG_INT_REG);\n\telse\n\t\tsun50i_iommu_handle_perm_irq(iommu);\n\n\tiommu_write(iommu, IOMMU_INT_CLR_REG, status);\n\n\tresets = (status | l1_status | l2_status) & IOMMU_INT_MASTER_MASK;\n\tiommu_write(iommu, IOMMU_RESET_REG, ~resets);\n\tiommu_write(iommu, IOMMU_RESET_REG, IOMMU_RESET_RELEASE_ALL);\n\n\tspin_unlock(&iommu->iommu_lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int sun50i_iommu_probe(struct platform_device *pdev)\n{\n\tstruct sun50i_iommu *iommu;\n\tint ret, irq;\n\n\tiommu = devm_kzalloc(&pdev->dev, sizeof(*iommu), GFP_KERNEL);\n\tif (!iommu)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&iommu->iommu_lock);\n\tplatform_set_drvdata(pdev, iommu);\n\tiommu->dev = &pdev->dev;\n\n\tiommu->pt_pool = kmem_cache_create(dev_name(&pdev->dev),\n\t\t\t\t\t   PT_SIZE, PT_SIZE,\n\t\t\t\t\t   SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t   NULL);\n\tif (!iommu->pt_pool)\n\t\treturn -ENOMEM;\n\n\tiommu->group = iommu_group_alloc();\n\tif (IS_ERR(iommu->group)) {\n\t\tret = PTR_ERR(iommu->group);\n\t\tgoto err_free_cache;\n\t}\n\n\tiommu->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(iommu->base)) {\n\t\tret = PTR_ERR(iommu->base);\n\t\tgoto err_free_group;\n\t}\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0) {\n\t\tret = irq;\n\t\tgoto err_free_group;\n\t}\n\n\tiommu->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(iommu->clk)) {\n\t\tdev_err(&pdev->dev, \"Couldn't get our clock.\\n\");\n\t\tret = PTR_ERR(iommu->clk);\n\t\tgoto err_free_group;\n\t}\n\n\tiommu->reset = devm_reset_control_get(&pdev->dev, NULL);\n\tif (IS_ERR(iommu->reset)) {\n\t\tdev_err(&pdev->dev, \"Couldn't get our reset line.\\n\");\n\t\tret = PTR_ERR(iommu->reset);\n\t\tgoto err_free_group;\n\t}\n\n\tret = iommu_device_sysfs_add(&iommu->iommu, &pdev->dev,\n\t\t\t\t     NULL, dev_name(&pdev->dev));\n\tif (ret)\n\t\tgoto err_free_group;\n\n\tret = iommu_device_register(&iommu->iommu, &sun50i_iommu_ops, &pdev->dev);\n\tif (ret)\n\t\tgoto err_remove_sysfs;\n\n\tret = devm_request_irq(&pdev->dev, irq, sun50i_iommu_irq, 0,\n\t\t\t       dev_name(&pdev->dev), iommu);\n\tif (ret < 0)\n\t\tgoto err_unregister;\n\n\treturn 0;\n\nerr_unregister:\n\tiommu_device_unregister(&iommu->iommu);\n\nerr_remove_sysfs:\n\tiommu_device_sysfs_remove(&iommu->iommu);\n\nerr_free_group:\n\tiommu_group_put(iommu->group);\n\nerr_free_cache:\n\tkmem_cache_destroy(iommu->pt_pool);\n\n\treturn ret;\n}\n\nstatic const struct of_device_id sun50i_iommu_dt[] = {\n\t{ .compatible = \"allwinner,sun50i-h6-iommu\", },\n\t{   },\n};\nMODULE_DEVICE_TABLE(of, sun50i_iommu_dt);\n\nstatic struct platform_driver sun50i_iommu_driver = {\n\t.driver\t\t= {\n\t\t.name\t\t\t= \"sun50i-iommu\",\n\t\t.of_match_table \t= sun50i_iommu_dt,\n\t\t.suppress_bind_attrs\t= true,\n\t}\n};\nbuiltin_platform_driver_probe(sun50i_iommu_driver, sun50i_iommu_probe);\n\nMODULE_DESCRIPTION(\"Allwinner H6 IOMMU driver\");\nMODULE_AUTHOR(\"Maxime Ripard <maxime@cerno.tech>\");\nMODULE_AUTHOR(\"zhuxianbin <zhuxianbin@allwinnertech.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}