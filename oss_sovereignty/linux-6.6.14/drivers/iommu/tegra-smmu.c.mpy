{
  "module_name": "tegra-smmu.c",
  "hash_id": "52bd793e5916d81de3e9c0957d3ebea734842494066f97665fa0bdebdc9750bd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/tegra-smmu.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/debugfs.h>\n#include <linux/err.h>\n#include <linux/iommu.h>\n#include <linux/kernel.h>\n#include <linux/of.h>\n#include <linux/of_platform.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/dma-mapping.h>\n\n#include <soc/tegra/ahb.h>\n#include <soc/tegra/mc.h>\n\nstruct tegra_smmu_group {\n\tstruct list_head list;\n\tstruct tegra_smmu *smmu;\n\tconst struct tegra_smmu_group_soc *soc;\n\tstruct iommu_group *group;\n\tunsigned int swgroup;\n};\n\nstruct tegra_smmu {\n\tvoid __iomem *regs;\n\tstruct device *dev;\n\n\tstruct tegra_mc *mc;\n\tconst struct tegra_smmu_soc *soc;\n\n\tstruct list_head groups;\n\n\tunsigned long pfn_mask;\n\tunsigned long tlb_mask;\n\n\tunsigned long *asids;\n\tstruct mutex lock;\n\n\tstruct list_head list;\n\n\tstruct dentry *debugfs;\n\n\tstruct iommu_device iommu;\t \n};\n\nstruct tegra_smmu_as {\n\tstruct iommu_domain domain;\n\tstruct tegra_smmu *smmu;\n\tunsigned int use_count;\n\tspinlock_t lock;\n\tu32 *count;\n\tstruct page **pts;\n\tstruct page *pd;\n\tdma_addr_t pd_dma;\n\tunsigned id;\n\tu32 attr;\n};\n\nstatic struct tegra_smmu_as *to_smmu_as(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct tegra_smmu_as, domain);\n}\n\nstatic inline void smmu_writel(struct tegra_smmu *smmu, u32 value,\n\t\t\t       unsigned long offset)\n{\n\twritel(value, smmu->regs + offset);\n}\n\nstatic inline u32 smmu_readl(struct tegra_smmu *smmu, unsigned long offset)\n{\n\treturn readl(smmu->regs + offset);\n}\n\n#define SMMU_CONFIG 0x010\n#define  SMMU_CONFIG_ENABLE (1 << 0)\n\n#define SMMU_TLB_CONFIG 0x14\n#define  SMMU_TLB_CONFIG_HIT_UNDER_MISS (1 << 29)\n#define  SMMU_TLB_CONFIG_ROUND_ROBIN_ARBITRATION (1 << 28)\n#define  SMMU_TLB_CONFIG_ACTIVE_LINES(smmu) \\\n\t((smmu)->soc->num_tlb_lines & (smmu)->tlb_mask)\n\n#define SMMU_PTC_CONFIG 0x18\n#define  SMMU_PTC_CONFIG_ENABLE (1 << 29)\n#define  SMMU_PTC_CONFIG_REQ_LIMIT(x) (((x) & 0x0f) << 24)\n#define  SMMU_PTC_CONFIG_INDEX_MAP(x) ((x) & 0x3f)\n\n#define SMMU_PTB_ASID 0x01c\n#define  SMMU_PTB_ASID_VALUE(x) ((x) & 0x7f)\n\n#define SMMU_PTB_DATA 0x020\n#define  SMMU_PTB_DATA_VALUE(dma, attr) ((dma) >> 12 | (attr))\n\n#define SMMU_MK_PDE(dma, attr) ((dma) >> SMMU_PTE_SHIFT | (attr))\n\n#define SMMU_TLB_FLUSH 0x030\n#define  SMMU_TLB_FLUSH_VA_MATCH_ALL     (0 << 0)\n#define  SMMU_TLB_FLUSH_VA_MATCH_SECTION (2 << 0)\n#define  SMMU_TLB_FLUSH_VA_MATCH_GROUP   (3 << 0)\n#define  SMMU_TLB_FLUSH_VA_SECTION(addr) ((((addr) & 0xffc00000) >> 12) | \\\n\t\t\t\t\t  SMMU_TLB_FLUSH_VA_MATCH_SECTION)\n#define  SMMU_TLB_FLUSH_VA_GROUP(addr)   ((((addr) & 0xffffc000) >> 12) | \\\n\t\t\t\t\t  SMMU_TLB_FLUSH_VA_MATCH_GROUP)\n#define  SMMU_TLB_FLUSH_ASID_MATCH       (1 << 31)\n\n#define SMMU_PTC_FLUSH 0x034\n#define  SMMU_PTC_FLUSH_TYPE_ALL (0 << 0)\n#define  SMMU_PTC_FLUSH_TYPE_ADR (1 << 0)\n\n#define SMMU_PTC_FLUSH_HI 0x9b8\n#define  SMMU_PTC_FLUSH_HI_MASK 0x3\n\n \n#define SMMU_ASID_ENABLE (1 << 31)\n#define SMMU_ASID_MASK 0x7f\n#define SMMU_ASID_VALUE(x) ((x) & SMMU_ASID_MASK)\n\n \n#define SMMU_NUM_PDE 1024\n#define SMMU_NUM_PTE 1024\n\n#define SMMU_SIZE_PD (SMMU_NUM_PDE * 4)\n#define SMMU_SIZE_PT (SMMU_NUM_PTE * 4)\n\n#define SMMU_PDE_SHIFT 22\n#define SMMU_PTE_SHIFT 12\n\n#define SMMU_PAGE_MASK\t\t(~(SMMU_SIZE_PT-1))\n#define SMMU_OFFSET_IN_PAGE(x)\t((unsigned long)(x) & ~SMMU_PAGE_MASK)\n#define SMMU_PFN_PHYS(x)\t((phys_addr_t)(x) << SMMU_PTE_SHIFT)\n#define SMMU_PHYS_PFN(x)\t((unsigned long)((x) >> SMMU_PTE_SHIFT))\n\n#define SMMU_PD_READABLE\t(1 << 31)\n#define SMMU_PD_WRITABLE\t(1 << 30)\n#define SMMU_PD_NONSECURE\t(1 << 29)\n\n#define SMMU_PDE_READABLE\t(1 << 31)\n#define SMMU_PDE_WRITABLE\t(1 << 30)\n#define SMMU_PDE_NONSECURE\t(1 << 29)\n#define SMMU_PDE_NEXT\t\t(1 << 28)\n\n#define SMMU_PTE_READABLE\t(1 << 31)\n#define SMMU_PTE_WRITABLE\t(1 << 30)\n#define SMMU_PTE_NONSECURE\t(1 << 29)\n\n#define SMMU_PDE_ATTR\t\t(SMMU_PDE_READABLE | SMMU_PDE_WRITABLE | \\\n\t\t\t\t SMMU_PDE_NONSECURE)\n\nstatic unsigned int iova_pd_index(unsigned long iova)\n{\n\treturn (iova >> SMMU_PDE_SHIFT) & (SMMU_NUM_PDE - 1);\n}\n\nstatic unsigned int iova_pt_index(unsigned long iova)\n{\n\treturn (iova >> SMMU_PTE_SHIFT) & (SMMU_NUM_PTE - 1);\n}\n\nstatic bool smmu_dma_addr_valid(struct tegra_smmu *smmu, dma_addr_t addr)\n{\n\taddr >>= 12;\n\treturn (addr & smmu->pfn_mask) == addr;\n}\n\nstatic dma_addr_t smmu_pde_to_dma(struct tegra_smmu *smmu, u32 pde)\n{\n\treturn (dma_addr_t)(pde & smmu->pfn_mask) << 12;\n}\n\nstatic void smmu_flush_ptc_all(struct tegra_smmu *smmu)\n{\n\tsmmu_writel(smmu, SMMU_PTC_FLUSH_TYPE_ALL, SMMU_PTC_FLUSH);\n}\n\nstatic inline void smmu_flush_ptc(struct tegra_smmu *smmu, dma_addr_t dma,\n\t\t\t\t  unsigned long offset)\n{\n\tu32 value;\n\n\toffset &= ~(smmu->mc->soc->atom_size - 1);\n\n\tif (smmu->mc->soc->num_address_bits > 32) {\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\tvalue = (dma >> 32) & SMMU_PTC_FLUSH_HI_MASK;\n#else\n\t\tvalue = 0;\n#endif\n\t\tsmmu_writel(smmu, value, SMMU_PTC_FLUSH_HI);\n\t}\n\n\tvalue = (dma + offset) | SMMU_PTC_FLUSH_TYPE_ADR;\n\tsmmu_writel(smmu, value, SMMU_PTC_FLUSH);\n}\n\nstatic inline void smmu_flush_tlb(struct tegra_smmu *smmu)\n{\n\tsmmu_writel(smmu, SMMU_TLB_FLUSH_VA_MATCH_ALL, SMMU_TLB_FLUSH);\n}\n\nstatic inline void smmu_flush_tlb_asid(struct tegra_smmu *smmu,\n\t\t\t\t       unsigned long asid)\n{\n\tu32 value;\n\n\tif (smmu->soc->num_asids == 4)\n\t\tvalue = (asid & 0x3) << 29;\n\telse\n\t\tvalue = (asid & 0x7f) << 24;\n\n\tvalue |= SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_VA_MATCH_ALL;\n\tsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\n}\n\nstatic inline void smmu_flush_tlb_section(struct tegra_smmu *smmu,\n\t\t\t\t\t  unsigned long asid,\n\t\t\t\t\t  unsigned long iova)\n{\n\tu32 value;\n\n\tif (smmu->soc->num_asids == 4)\n\t\tvalue = (asid & 0x3) << 29;\n\telse\n\t\tvalue = (asid & 0x7f) << 24;\n\n\tvalue |= SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_VA_SECTION(iova);\n\tsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\n}\n\nstatic inline void smmu_flush_tlb_group(struct tegra_smmu *smmu,\n\t\t\t\t\tunsigned long asid,\n\t\t\t\t\tunsigned long iova)\n{\n\tu32 value;\n\n\tif (smmu->soc->num_asids == 4)\n\t\tvalue = (asid & 0x3) << 29;\n\telse\n\t\tvalue = (asid & 0x7f) << 24;\n\n\tvalue |= SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_VA_GROUP(iova);\n\tsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\n}\n\nstatic inline void smmu_flush(struct tegra_smmu *smmu)\n{\n\tsmmu_readl(smmu, SMMU_PTB_ASID);\n}\n\nstatic int tegra_smmu_alloc_asid(struct tegra_smmu *smmu, unsigned int *idp)\n{\n\tunsigned long id;\n\n\tid = find_first_zero_bit(smmu->asids, smmu->soc->num_asids);\n\tif (id >= smmu->soc->num_asids)\n\t\treturn -ENOSPC;\n\n\tset_bit(id, smmu->asids);\n\t*idp = id;\n\n\treturn 0;\n}\n\nstatic void tegra_smmu_free_asid(struct tegra_smmu *smmu, unsigned int id)\n{\n\tclear_bit(id, smmu->asids);\n}\n\nstatic struct iommu_domain *tegra_smmu_domain_alloc(unsigned type)\n{\n\tstruct tegra_smmu_as *as;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn NULL;\n\n\tas = kzalloc(sizeof(*as), GFP_KERNEL);\n\tif (!as)\n\t\treturn NULL;\n\n\tas->attr = SMMU_PD_READABLE | SMMU_PD_WRITABLE | SMMU_PD_NONSECURE;\n\n\tas->pd = alloc_page(GFP_KERNEL | __GFP_DMA | __GFP_ZERO);\n\tif (!as->pd) {\n\t\tkfree(as);\n\t\treturn NULL;\n\t}\n\n\tas->count = kcalloc(SMMU_NUM_PDE, sizeof(u32), GFP_KERNEL);\n\tif (!as->count) {\n\t\t__free_page(as->pd);\n\t\tkfree(as);\n\t\treturn NULL;\n\t}\n\n\tas->pts = kcalloc(SMMU_NUM_PDE, sizeof(*as->pts), GFP_KERNEL);\n\tif (!as->pts) {\n\t\tkfree(as->count);\n\t\t__free_page(as->pd);\n\t\tkfree(as);\n\t\treturn NULL;\n\t}\n\n\tspin_lock_init(&as->lock);\n\n\t \n\tas->domain.geometry.aperture_start = 0;\n\tas->domain.geometry.aperture_end = 0xffffffff;\n\tas->domain.geometry.force_aperture = true;\n\n\treturn &as->domain;\n}\n\nstatic void tegra_smmu_domain_free(struct iommu_domain *domain)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\n\t \n\n\tWARN_ON_ONCE(as->use_count);\n\tkfree(as->count);\n\tkfree(as->pts);\n\tkfree(as);\n}\n\nstatic const struct tegra_smmu_swgroup *\ntegra_smmu_find_swgroup(struct tegra_smmu *smmu, unsigned int swgroup)\n{\n\tconst struct tegra_smmu_swgroup *group = NULL;\n\tunsigned int i;\n\n\tfor (i = 0; i < smmu->soc->num_swgroups; i++) {\n\t\tif (smmu->soc->swgroups[i].swgroup == swgroup) {\n\t\t\tgroup = &smmu->soc->swgroups[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn group;\n}\n\nstatic void tegra_smmu_enable(struct tegra_smmu *smmu, unsigned int swgroup,\n\t\t\t      unsigned int asid)\n{\n\tconst struct tegra_smmu_swgroup *group;\n\tunsigned int i;\n\tu32 value;\n\n\tgroup = tegra_smmu_find_swgroup(smmu, swgroup);\n\tif (group) {\n\t\tvalue = smmu_readl(smmu, group->reg);\n\t\tvalue &= ~SMMU_ASID_MASK;\n\t\tvalue |= SMMU_ASID_VALUE(asid);\n\t\tvalue |= SMMU_ASID_ENABLE;\n\t\tsmmu_writel(smmu, value, group->reg);\n\t} else {\n\t\tpr_warn(\"%s group from swgroup %u not found\\n\", __func__,\n\t\t\t\tswgroup);\n\t\t \n\t\treturn;\n\t}\n\n\tfor (i = 0; i < smmu->soc->num_clients; i++) {\n\t\tconst struct tegra_mc_client *client = &smmu->soc->clients[i];\n\n\t\tif (client->swgroup != swgroup)\n\t\t\tcontinue;\n\n\t\tvalue = smmu_readl(smmu, client->regs.smmu.reg);\n\t\tvalue |= BIT(client->regs.smmu.bit);\n\t\tsmmu_writel(smmu, value, client->regs.smmu.reg);\n\t}\n}\n\nstatic void tegra_smmu_disable(struct tegra_smmu *smmu, unsigned int swgroup,\n\t\t\t       unsigned int asid)\n{\n\tconst struct tegra_smmu_swgroup *group;\n\tunsigned int i;\n\tu32 value;\n\n\tgroup = tegra_smmu_find_swgroup(smmu, swgroup);\n\tif (group) {\n\t\tvalue = smmu_readl(smmu, group->reg);\n\t\tvalue &= ~SMMU_ASID_MASK;\n\t\tvalue |= SMMU_ASID_VALUE(asid);\n\t\tvalue &= ~SMMU_ASID_ENABLE;\n\t\tsmmu_writel(smmu, value, group->reg);\n\t}\n\n\tfor (i = 0; i < smmu->soc->num_clients; i++) {\n\t\tconst struct tegra_mc_client *client = &smmu->soc->clients[i];\n\n\t\tif (client->swgroup != swgroup)\n\t\t\tcontinue;\n\n\t\tvalue = smmu_readl(smmu, client->regs.smmu.reg);\n\t\tvalue &= ~BIT(client->regs.smmu.bit);\n\t\tsmmu_writel(smmu, value, client->regs.smmu.reg);\n\t}\n}\n\nstatic int tegra_smmu_as_prepare(struct tegra_smmu *smmu,\n\t\t\t\t struct tegra_smmu_as *as)\n{\n\tu32 value;\n\tint err = 0;\n\n\tmutex_lock(&smmu->lock);\n\n\tif (as->use_count > 0) {\n\t\tas->use_count++;\n\t\tgoto unlock;\n\t}\n\n\tas->pd_dma = dma_map_page(smmu->dev, as->pd, 0, SMMU_SIZE_PD,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(smmu->dev, as->pd_dma)) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (!smmu_dma_addr_valid(smmu, as->pd_dma)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_unmap;\n\t}\n\n\terr = tegra_smmu_alloc_asid(smmu, &as->id);\n\tif (err < 0)\n\t\tgoto err_unmap;\n\n\tsmmu_flush_ptc(smmu, as->pd_dma, 0);\n\tsmmu_flush_tlb_asid(smmu, as->id);\n\n\tsmmu_writel(smmu, as->id & 0x7f, SMMU_PTB_ASID);\n\tvalue = SMMU_PTB_DATA_VALUE(as->pd_dma, as->attr);\n\tsmmu_writel(smmu, value, SMMU_PTB_DATA);\n\tsmmu_flush(smmu);\n\n\tas->smmu = smmu;\n\tas->use_count++;\n\n\tmutex_unlock(&smmu->lock);\n\n\treturn 0;\n\nerr_unmap:\n\tdma_unmap_page(smmu->dev, as->pd_dma, SMMU_SIZE_PD, DMA_TO_DEVICE);\nunlock:\n\tmutex_unlock(&smmu->lock);\n\n\treturn err;\n}\n\nstatic void tegra_smmu_as_unprepare(struct tegra_smmu *smmu,\n\t\t\t\t    struct tegra_smmu_as *as)\n{\n\tmutex_lock(&smmu->lock);\n\n\tif (--as->use_count > 0) {\n\t\tmutex_unlock(&smmu->lock);\n\t\treturn;\n\t}\n\n\ttegra_smmu_free_asid(smmu, as->id);\n\n\tdma_unmap_page(smmu->dev, as->pd_dma, SMMU_SIZE_PD, DMA_TO_DEVICE);\n\n\tas->smmu = NULL;\n\n\tmutex_unlock(&smmu->lock);\n}\n\nstatic int tegra_smmu_attach_dev(struct iommu_domain *domain,\n\t\t\t\t struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct tegra_smmu *smmu = dev_iommu_priv_get(dev);\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tunsigned int index;\n\tint err;\n\n\tif (!fwspec)\n\t\treturn -ENOENT;\n\n\tfor (index = 0; index < fwspec->num_ids; index++) {\n\t\terr = tegra_smmu_as_prepare(smmu, as);\n\t\tif (err)\n\t\t\tgoto disable;\n\n\t\ttegra_smmu_enable(smmu, fwspec->ids[index], as->id);\n\t}\n\n\tif (index == 0)\n\t\treturn -ENODEV;\n\n\treturn 0;\n\ndisable:\n\twhile (index--) {\n\t\ttegra_smmu_disable(smmu, fwspec->ids[index], as->id);\n\t\ttegra_smmu_as_unprepare(smmu, as);\n\t}\n\n\treturn err;\n}\n\nstatic void tegra_smmu_set_platform_dma(struct device *dev)\n{\n\tstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tstruct tegra_smmu *smmu = as->smmu;\n\tunsigned int index;\n\n\tif (!fwspec)\n\t\treturn;\n\n\tfor (index = 0; index < fwspec->num_ids; index++) {\n\t\ttegra_smmu_disable(smmu, fwspec->ids[index], as->id);\n\t\ttegra_smmu_as_unprepare(smmu, as);\n\t}\n}\n\nstatic void tegra_smmu_set_pde(struct tegra_smmu_as *as, unsigned long iova,\n\t\t\t       u32 value)\n{\n\tunsigned int pd_index = iova_pd_index(iova);\n\tstruct tegra_smmu *smmu = as->smmu;\n\tu32 *pd = page_address(as->pd);\n\tunsigned long offset = pd_index * sizeof(*pd);\n\n\t \n\tpd[pd_index] = value;\n\n\t \n\tdma_sync_single_range_for_device(smmu->dev, as->pd_dma, offset,\n\t\t\t\t\t sizeof(*pd), DMA_TO_DEVICE);\n\n\t \n\tsmmu_flush_ptc(smmu, as->pd_dma, offset);\n\tsmmu_flush_tlb_section(smmu, as->id, iova);\n\tsmmu_flush(smmu);\n}\n\nstatic u32 *tegra_smmu_pte_offset(struct page *pt_page, unsigned long iova)\n{\n\tu32 *pt = page_address(pt_page);\n\n\treturn pt + iova_pt_index(iova);\n}\n\nstatic u32 *tegra_smmu_pte_lookup(struct tegra_smmu_as *as, unsigned long iova,\n\t\t\t\t  dma_addr_t *dmap)\n{\n\tunsigned int pd_index = iova_pd_index(iova);\n\tstruct tegra_smmu *smmu = as->smmu;\n\tstruct page *pt_page;\n\tu32 *pd;\n\n\tpt_page = as->pts[pd_index];\n\tif (!pt_page)\n\t\treturn NULL;\n\n\tpd = page_address(as->pd);\n\t*dmap = smmu_pde_to_dma(smmu, pd[pd_index]);\n\n\treturn tegra_smmu_pte_offset(pt_page, iova);\n}\n\nstatic u32 *as_get_pte(struct tegra_smmu_as *as, dma_addr_t iova,\n\t\t       dma_addr_t *dmap, struct page *page)\n{\n\tunsigned int pde = iova_pd_index(iova);\n\tstruct tegra_smmu *smmu = as->smmu;\n\n\tif (!as->pts[pde]) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_page(smmu->dev, page, 0, SMMU_SIZE_PT,\n\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(smmu->dev, dma)) {\n\t\t\t__free_page(page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (!smmu_dma_addr_valid(smmu, dma)) {\n\t\t\tdma_unmap_page(smmu->dev, dma, SMMU_SIZE_PT,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t__free_page(page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tas->pts[pde] = page;\n\n\t\ttegra_smmu_set_pde(as, iova, SMMU_MK_PDE(dma, SMMU_PDE_ATTR |\n\t\t\t\t\t\t\t      SMMU_PDE_NEXT));\n\n\t\t*dmap = dma;\n\t} else {\n\t\tu32 *pd = page_address(as->pd);\n\n\t\t*dmap = smmu_pde_to_dma(smmu, pd[pde]);\n\t}\n\n\treturn tegra_smmu_pte_offset(as->pts[pde], iova);\n}\n\nstatic void tegra_smmu_pte_get_use(struct tegra_smmu_as *as, unsigned long iova)\n{\n\tunsigned int pd_index = iova_pd_index(iova);\n\n\tas->count[pd_index]++;\n}\n\nstatic void tegra_smmu_pte_put_use(struct tegra_smmu_as *as, unsigned long iova)\n{\n\tunsigned int pde = iova_pd_index(iova);\n\tstruct page *page = as->pts[pde];\n\n\t \n\tif (--as->count[pde] == 0) {\n\t\tstruct tegra_smmu *smmu = as->smmu;\n\t\tu32 *pd = page_address(as->pd);\n\t\tdma_addr_t pte_dma = smmu_pde_to_dma(smmu, pd[pde]);\n\n\t\ttegra_smmu_set_pde(as, iova, 0);\n\n\t\tdma_unmap_page(smmu->dev, pte_dma, SMMU_SIZE_PT, DMA_TO_DEVICE);\n\t\t__free_page(page);\n\t\tas->pts[pde] = NULL;\n\t}\n}\n\nstatic void tegra_smmu_set_pte(struct tegra_smmu_as *as, unsigned long iova,\n\t\t\t       u32 *pte, dma_addr_t pte_dma, u32 val)\n{\n\tstruct tegra_smmu *smmu = as->smmu;\n\tunsigned long offset = SMMU_OFFSET_IN_PAGE(pte);\n\n\t*pte = val;\n\n\tdma_sync_single_range_for_device(smmu->dev, pte_dma, offset,\n\t\t\t\t\t 4, DMA_TO_DEVICE);\n\tsmmu_flush_ptc(smmu, pte_dma, offset);\n\tsmmu_flush_tlb_group(smmu, as->id, iova);\n\tsmmu_flush(smmu);\n}\n\nstatic struct page *as_get_pde_page(struct tegra_smmu_as *as,\n\t\t\t\t    unsigned long iova, gfp_t gfp,\n\t\t\t\t    unsigned long *flags)\n{\n\tunsigned int pde = iova_pd_index(iova);\n\tstruct page *page = as->pts[pde];\n\n\t \n\tif (page)\n\t\treturn page;\n\n\t \n\tif (gfpflags_allow_blocking(gfp))\n\t\tspin_unlock_irqrestore(&as->lock, *flags);\n\n\tpage = alloc_page(gfp | __GFP_DMA | __GFP_ZERO);\n\n\tif (gfpflags_allow_blocking(gfp))\n\t\tspin_lock_irqsave(&as->lock, *flags);\n\n\t \n\tif (as->pts[pde]) {\n\t\tif (page)\n\t\t\t__free_page(page);\n\n\t\tpage = as->pts[pde];\n\t}\n\n\treturn page;\n}\n\nstatic int\n__tegra_smmu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t phys_addr_t paddr, size_t size, int prot, gfp_t gfp,\n\t\t unsigned long *flags)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tdma_addr_t pte_dma;\n\tstruct page *page;\n\tu32 pte_attrs;\n\tu32 *pte;\n\n\tpage = as_get_pde_page(as, iova, gfp, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tpte = as_get_pte(as, iova, &pte_dma, page);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\n\t \n\tif (*pte == 0)\n\t\ttegra_smmu_pte_get_use(as, iova);\n\n\tpte_attrs = SMMU_PTE_NONSECURE;\n\n\tif (prot & IOMMU_READ)\n\t\tpte_attrs |= SMMU_PTE_READABLE;\n\n\tif (prot & IOMMU_WRITE)\n\t\tpte_attrs |= SMMU_PTE_WRITABLE;\n\n\ttegra_smmu_set_pte(as, iova, pte, pte_dma,\n\t\t\t   SMMU_PHYS_PFN(paddr) | pte_attrs);\n\n\treturn 0;\n}\n\nstatic size_t\n__tegra_smmu_unmap(struct iommu_domain *domain, unsigned long iova,\n\t\t   size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tdma_addr_t pte_dma;\n\tu32 *pte;\n\n\tpte = tegra_smmu_pte_lookup(as, iova, &pte_dma);\n\tif (!pte || !*pte)\n\t\treturn 0;\n\n\ttegra_smmu_set_pte(as, iova, pte, pte_dma, 0);\n\ttegra_smmu_pte_put_use(as, iova);\n\n\treturn size;\n}\n\nstatic int tegra_smmu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t\t  phys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&as->lock, flags);\n\tret = __tegra_smmu_map(domain, iova, paddr, size, prot, gfp, &flags);\n\tspin_unlock_irqrestore(&as->lock, flags);\n\n\treturn ret;\n}\n\nstatic size_t tegra_smmu_unmap(struct iommu_domain *domain, unsigned long iova,\n\t\t\t       size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&as->lock, flags);\n\tsize = __tegra_smmu_unmap(domain, iova, size, gather);\n\tspin_unlock_irqrestore(&as->lock, flags);\n\n\treturn size;\n}\n\nstatic phys_addr_t tegra_smmu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\t   dma_addr_t iova)\n{\n\tstruct tegra_smmu_as *as = to_smmu_as(domain);\n\tunsigned long pfn;\n\tdma_addr_t pte_dma;\n\tu32 *pte;\n\n\tpte = tegra_smmu_pte_lookup(as, iova, &pte_dma);\n\tif (!pte || !*pte)\n\t\treturn 0;\n\n\tpfn = *pte & as->smmu->pfn_mask;\n\n\treturn SMMU_PFN_PHYS(pfn) + SMMU_OFFSET_IN_PAGE(iova);\n}\n\nstatic struct tegra_smmu *tegra_smmu_find(struct device_node *np)\n{\n\tstruct platform_device *pdev;\n\tstruct tegra_mc *mc;\n\n\tpdev = of_find_device_by_node(np);\n\tif (!pdev)\n\t\treturn NULL;\n\n\tmc = platform_get_drvdata(pdev);\n\tif (!mc) {\n\t\tput_device(&pdev->dev);\n\t\treturn NULL;\n\t}\n\n\treturn mc->smmu;\n}\n\nstatic int tegra_smmu_configure(struct tegra_smmu *smmu, struct device *dev,\n\t\t\t\tstruct of_phandle_args *args)\n{\n\tconst struct iommu_ops *ops = smmu->iommu.ops;\n\tint err;\n\n\terr = iommu_fwspec_init(dev, &dev->of_node->fwnode, ops);\n\tif (err < 0) {\n\t\tdev_err(dev, \"failed to initialize fwspec: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = ops->of_xlate(dev, args);\n\tif (err < 0) {\n\t\tdev_err(dev, \"failed to parse SW group ID: %d\\n\", err);\n\t\tiommu_fwspec_free(dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic struct iommu_device *tegra_smmu_probe_device(struct device *dev)\n{\n\tstruct device_node *np = dev->of_node;\n\tstruct tegra_smmu *smmu = NULL;\n\tstruct of_phandle_args args;\n\tunsigned int index = 0;\n\tint err;\n\n\twhile (of_parse_phandle_with_args(np, \"iommus\", \"#iommu-cells\", index,\n\t\t\t\t\t  &args) == 0) {\n\t\tsmmu = tegra_smmu_find(args.np);\n\t\tif (smmu) {\n\t\t\terr = tegra_smmu_configure(smmu, dev, &args);\n\n\t\t\tif (err < 0) {\n\t\t\t\tof_node_put(args.np);\n\t\t\t\treturn ERR_PTR(err);\n\t\t\t}\n\t\t}\n\n\t\tof_node_put(args.np);\n\t\tindex++;\n\t}\n\n\tsmmu = dev_iommu_priv_get(dev);\n\tif (!smmu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\treturn &smmu->iommu;\n}\n\nstatic const struct tegra_smmu_group_soc *\ntegra_smmu_find_group(struct tegra_smmu *smmu, unsigned int swgroup)\n{\n\tunsigned int i, j;\n\n\tfor (i = 0; i < smmu->soc->num_groups; i++)\n\t\tfor (j = 0; j < smmu->soc->groups[i].num_swgroups; j++)\n\t\t\tif (smmu->soc->groups[i].swgroups[j] == swgroup)\n\t\t\t\treturn &smmu->soc->groups[i];\n\n\treturn NULL;\n}\n\nstatic void tegra_smmu_group_release(void *iommu_data)\n{\n\tstruct tegra_smmu_group *group = iommu_data;\n\tstruct tegra_smmu *smmu = group->smmu;\n\n\tmutex_lock(&smmu->lock);\n\tlist_del(&group->list);\n\tmutex_unlock(&smmu->lock);\n}\n\nstatic struct iommu_group *tegra_smmu_device_group(struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct tegra_smmu *smmu = dev_iommu_priv_get(dev);\n\tconst struct tegra_smmu_group_soc *soc;\n\tunsigned int swgroup = fwspec->ids[0];\n\tstruct tegra_smmu_group *group;\n\tstruct iommu_group *grp;\n\n\t \n\tsoc = tegra_smmu_find_group(smmu, swgroup);\n\n\tmutex_lock(&smmu->lock);\n\n\t \n\tlist_for_each_entry(group, &smmu->groups, list)\n\t\tif ((group->swgroup == swgroup) || (soc && group->soc == soc)) {\n\t\t\tgrp = iommu_group_ref_get(group->group);\n\t\t\tmutex_unlock(&smmu->lock);\n\t\t\treturn grp;\n\t\t}\n\n\tgroup = devm_kzalloc(smmu->dev, sizeof(*group), GFP_KERNEL);\n\tif (!group) {\n\t\tmutex_unlock(&smmu->lock);\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&group->list);\n\tgroup->swgroup = swgroup;\n\tgroup->smmu = smmu;\n\tgroup->soc = soc;\n\n\tif (dev_is_pci(dev))\n\t\tgroup->group = pci_device_group(dev);\n\telse\n\t\tgroup->group = generic_device_group(dev);\n\n\tif (IS_ERR(group->group)) {\n\t\tdevm_kfree(smmu->dev, group);\n\t\tmutex_unlock(&smmu->lock);\n\t\treturn NULL;\n\t}\n\n\tiommu_group_set_iommudata(group->group, group, tegra_smmu_group_release);\n\tif (soc)\n\t\tiommu_group_set_name(group->group, soc->name);\n\tlist_add_tail(&group->list, &smmu->groups);\n\tmutex_unlock(&smmu->lock);\n\n\treturn group->group;\n}\n\nstatic int tegra_smmu_of_xlate(struct device *dev,\n\t\t\t       struct of_phandle_args *args)\n{\n\tstruct platform_device *iommu_pdev = of_find_device_by_node(args->np);\n\tstruct tegra_mc *mc = platform_get_drvdata(iommu_pdev);\n\tu32 id = args->args[0];\n\n\t \n\tput_device(&iommu_pdev->dev);\n\n\tdev_iommu_priv_set(dev, mc->smmu);\n\n\treturn iommu_fwspec_add_ids(dev, &id, 1);\n}\n\nstatic const struct iommu_ops tegra_smmu_ops = {\n\t.domain_alloc = tegra_smmu_domain_alloc,\n\t.probe_device = tegra_smmu_probe_device,\n\t.device_group = tegra_smmu_device_group,\n\t.set_platform_dma_ops = tegra_smmu_set_platform_dma,\n\t.of_xlate = tegra_smmu_of_xlate,\n\t.pgsize_bitmap = SZ_4K,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= tegra_smmu_attach_dev,\n\t\t.map\t\t= tegra_smmu_map,\n\t\t.unmap\t\t= tegra_smmu_unmap,\n\t\t.iova_to_phys\t= tegra_smmu_iova_to_phys,\n\t\t.free\t\t= tegra_smmu_domain_free,\n\t}\n};\n\nstatic void tegra_smmu_ahb_enable(void)\n{\n\tstatic const struct of_device_id ahb_match[] = {\n\t\t{ .compatible = \"nvidia,tegra30-ahb\", },\n\t\t{ }\n\t};\n\tstruct device_node *ahb;\n\n\tahb = of_find_matching_node(NULL, ahb_match);\n\tif (ahb) {\n\t\ttegra_ahb_enable_smmu(ahb);\n\t\tof_node_put(ahb);\n\t}\n}\n\nstatic int tegra_smmu_swgroups_show(struct seq_file *s, void *data)\n{\n\tstruct tegra_smmu *smmu = s->private;\n\tunsigned int i;\n\tu32 value;\n\n\tseq_printf(s, \"swgroup    enabled  ASID\\n\");\n\tseq_printf(s, \"------------------------\\n\");\n\n\tfor (i = 0; i < smmu->soc->num_swgroups; i++) {\n\t\tconst struct tegra_smmu_swgroup *group = &smmu->soc->swgroups[i];\n\t\tconst char *status;\n\t\tunsigned int asid;\n\n\t\tvalue = smmu_readl(smmu, group->reg);\n\n\t\tif (value & SMMU_ASID_ENABLE)\n\t\t\tstatus = \"yes\";\n\t\telse\n\t\t\tstatus = \"no\";\n\n\t\tasid = value & SMMU_ASID_MASK;\n\n\t\tseq_printf(s, \"%-9s  %-7s  %#04x\\n\", group->name, status,\n\t\t\t   asid);\n\t}\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(tegra_smmu_swgroups);\n\nstatic int tegra_smmu_clients_show(struct seq_file *s, void *data)\n{\n\tstruct tegra_smmu *smmu = s->private;\n\tunsigned int i;\n\tu32 value;\n\n\tseq_printf(s, \"client       enabled\\n\");\n\tseq_printf(s, \"--------------------\\n\");\n\n\tfor (i = 0; i < smmu->soc->num_clients; i++) {\n\t\tconst struct tegra_mc_client *client = &smmu->soc->clients[i];\n\t\tconst char *status;\n\n\t\tvalue = smmu_readl(smmu, client->regs.smmu.reg);\n\n\t\tif (value & BIT(client->regs.smmu.bit))\n\t\t\tstatus = \"yes\";\n\t\telse\n\t\t\tstatus = \"no\";\n\n\t\tseq_printf(s, \"%-12s %s\\n\", client->name, status);\n\t}\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(tegra_smmu_clients);\n\nstatic void tegra_smmu_debugfs_init(struct tegra_smmu *smmu)\n{\n\tsmmu->debugfs = debugfs_create_dir(\"smmu\", NULL);\n\tif (!smmu->debugfs)\n\t\treturn;\n\n\tdebugfs_create_file(\"swgroups\", S_IRUGO, smmu->debugfs, smmu,\n\t\t\t    &tegra_smmu_swgroups_fops);\n\tdebugfs_create_file(\"clients\", S_IRUGO, smmu->debugfs, smmu,\n\t\t\t    &tegra_smmu_clients_fops);\n}\n\nstatic void tegra_smmu_debugfs_exit(struct tegra_smmu *smmu)\n{\n\tdebugfs_remove_recursive(smmu->debugfs);\n}\n\nstruct tegra_smmu *tegra_smmu_probe(struct device *dev,\n\t\t\t\t    const struct tegra_smmu_soc *soc,\n\t\t\t\t    struct tegra_mc *mc)\n{\n\tstruct tegra_smmu *smmu;\n\tu32 value;\n\tint err;\n\n\tsmmu = devm_kzalloc(dev, sizeof(*smmu), GFP_KERNEL);\n\tif (!smmu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tmc->smmu = smmu;\n\n\tsmmu->asids = devm_bitmap_zalloc(dev, soc->num_asids, GFP_KERNEL);\n\tif (!smmu->asids)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&smmu->groups);\n\tmutex_init(&smmu->lock);\n\n\tsmmu->regs = mc->regs;\n\tsmmu->soc = soc;\n\tsmmu->dev = dev;\n\tsmmu->mc = mc;\n\n\tsmmu->pfn_mask =\n\t\tBIT_MASK(mc->soc->num_address_bits - SMMU_PTE_SHIFT) - 1;\n\tdev_dbg(dev, \"address bits: %u, PFN mask: %#lx\\n\",\n\t\tmc->soc->num_address_bits, smmu->pfn_mask);\n\tsmmu->tlb_mask = (1 << fls(smmu->soc->num_tlb_lines)) - 1;\n\tdev_dbg(dev, \"TLB lines: %u, mask: %#lx\\n\", smmu->soc->num_tlb_lines,\n\t\tsmmu->tlb_mask);\n\n\tvalue = SMMU_PTC_CONFIG_ENABLE | SMMU_PTC_CONFIG_INDEX_MAP(0x3f);\n\n\tif (soc->supports_request_limit)\n\t\tvalue |= SMMU_PTC_CONFIG_REQ_LIMIT(8);\n\n\tsmmu_writel(smmu, value, SMMU_PTC_CONFIG);\n\n\tvalue = SMMU_TLB_CONFIG_HIT_UNDER_MISS |\n\t\tSMMU_TLB_CONFIG_ACTIVE_LINES(smmu);\n\n\tif (soc->supports_round_robin_arbitration)\n\t\tvalue |= SMMU_TLB_CONFIG_ROUND_ROBIN_ARBITRATION;\n\n\tsmmu_writel(smmu, value, SMMU_TLB_CONFIG);\n\n\tsmmu_flush_ptc_all(smmu);\n\tsmmu_flush_tlb(smmu);\n\tsmmu_writel(smmu, SMMU_CONFIG_ENABLE, SMMU_CONFIG);\n\tsmmu_flush(smmu);\n\n\ttegra_smmu_ahb_enable();\n\n\terr = iommu_device_sysfs_add(&smmu->iommu, dev, NULL, dev_name(dev));\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\terr = iommu_device_register(&smmu->iommu, &tegra_smmu_ops, dev);\n\tif (err) {\n\t\tiommu_device_sysfs_remove(&smmu->iommu);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tif (IS_ENABLED(CONFIG_DEBUG_FS))\n\t\ttegra_smmu_debugfs_init(smmu);\n\n\treturn smmu;\n}\n\nvoid tegra_smmu_remove(struct tegra_smmu *smmu)\n{\n\tiommu_device_unregister(&smmu->iommu);\n\tiommu_device_sysfs_remove(&smmu->iommu);\n\n\tif (IS_ENABLED(CONFIG_DEBUG_FS))\n\t\ttegra_smmu_debugfs_exit(smmu);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}