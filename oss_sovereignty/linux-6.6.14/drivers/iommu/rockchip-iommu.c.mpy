{
  "module_name": "rockchip-iommu.c",
  "hash_id": "a42ef006d6d9f4c453523a6118d374bb7a23c3d7f928bd5251275c9a7ed8985f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/rockchip-iommu.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/compiler.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/iommu.h>\n#include <linux/iopoll.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/of.h>\n#include <linux/of_platform.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n \n#define RK_MMU_DTE_ADDR\t\t0x00\t \n#define RK_MMU_STATUS\t\t0x04\n#define RK_MMU_COMMAND\t\t0x08\n#define RK_MMU_PAGE_FAULT_ADDR\t0x0C\t \n#define RK_MMU_ZAP_ONE_LINE\t0x10\t \n#define RK_MMU_INT_RAWSTAT\t0x14\t \n#define RK_MMU_INT_CLEAR\t0x18\t \n#define RK_MMU_INT_MASK\t\t0x1C\t \n#define RK_MMU_INT_STATUS\t0x20\t \n#define RK_MMU_AUTO_GATING\t0x24\n\n#define DTE_ADDR_DUMMY\t\t0xCAFEBABE\n\n#define RK_MMU_POLL_PERIOD_US\t\t100\n#define RK_MMU_FORCE_RESET_TIMEOUT_US\t100000\n#define RK_MMU_POLL_TIMEOUT_US\t\t1000\n\n \n#define RK_MMU_STATUS_PAGING_ENABLED       BIT(0)\n#define RK_MMU_STATUS_PAGE_FAULT_ACTIVE    BIT(1)\n#define RK_MMU_STATUS_STALL_ACTIVE         BIT(2)\n#define RK_MMU_STATUS_IDLE                 BIT(3)\n#define RK_MMU_STATUS_REPLAY_BUFFER_EMPTY  BIT(4)\n#define RK_MMU_STATUS_PAGE_FAULT_IS_WRITE  BIT(5)\n#define RK_MMU_STATUS_STALL_NOT_ACTIVE     BIT(31)\n\n \n#define RK_MMU_CMD_ENABLE_PAGING    0   \n#define RK_MMU_CMD_DISABLE_PAGING   1   \n#define RK_MMU_CMD_ENABLE_STALL     2   \n#define RK_MMU_CMD_DISABLE_STALL    3   \n#define RK_MMU_CMD_ZAP_CACHE        4   \n#define RK_MMU_CMD_PAGE_FAULT_DONE  5   \n#define RK_MMU_CMD_FORCE_RESET      6   \n\n \n#define RK_MMU_IRQ_PAGE_FAULT    0x01   \n#define RK_MMU_IRQ_BUS_ERROR     0x02   \n#define RK_MMU_IRQ_MASK          (RK_MMU_IRQ_PAGE_FAULT | RK_MMU_IRQ_BUS_ERROR)\n\n#define NUM_DT_ENTRIES 1024\n#define NUM_PT_ENTRIES 1024\n\n#define SPAGE_ORDER 12\n#define SPAGE_SIZE (1 << SPAGE_ORDER)\n\n  \n#define RK_IOMMU_PGSIZE_BITMAP 0x007ff000\n\nstruct rk_iommu_domain {\n\tstruct list_head iommus;\n\tu32 *dt;  \n\tdma_addr_t dt_dma;\n\tspinlock_t iommus_lock;  \n\tspinlock_t dt_lock;  \n\n\tstruct iommu_domain domain;\n};\n\n \nstatic const char * const rk_iommu_clocks[] = {\n\t\"aclk\", \"iface\",\n};\n\nstruct rk_iommu_ops {\n\tphys_addr_t (*pt_address)(u32 dte);\n\tu32 (*mk_dtentries)(dma_addr_t pt_dma);\n\tu32 (*mk_ptentries)(phys_addr_t page, int prot);\n\tu64 dma_bit_mask;\n\tgfp_t gfp_flags;\n};\n\nstruct rk_iommu {\n\tstruct device *dev;\n\tvoid __iomem **bases;\n\tint num_mmu;\n\tint num_irq;\n\tstruct clk_bulk_data *clocks;\n\tint num_clocks;\n\tbool reset_disabled;\n\tstruct iommu_device iommu;\n\tstruct list_head node;  \n\tstruct iommu_domain *domain;  \n\tstruct iommu_group *group;\n};\n\nstruct rk_iommudata {\n\tstruct device_link *link;  \n\tstruct rk_iommu *iommu;\n};\n\nstatic struct device *dma_dev;\nstatic const struct rk_iommu_ops *rk_ops;\nstatic struct iommu_domain rk_identity_domain;\n\nstatic inline void rk_table_flush(struct rk_iommu_domain *dom, dma_addr_t dma,\n\t\t\t\t  unsigned int count)\n{\n\tsize_t size = count * sizeof(u32);  \n\n\tdma_sync_single_for_device(dma_dev, dma, size, DMA_TO_DEVICE);\n}\n\nstatic struct rk_iommu_domain *to_rk_domain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct rk_iommu_domain, domain);\n}\n\n \n\n \n#define RK_DTE_PT_ADDRESS_MASK    0xfffff000\n#define RK_DTE_PT_VALID           BIT(0)\n\nstatic inline phys_addr_t rk_dte_pt_address(u32 dte)\n{\n\treturn (phys_addr_t)dte & RK_DTE_PT_ADDRESS_MASK;\n}\n\n \n#define RK_DTE_PT_ADDRESS_MASK_V2 GENMASK_ULL(31, 4)\n#define DTE_HI_MASK1\tGENMASK(11, 8)\n#define DTE_HI_MASK2\tGENMASK(7, 4)\n#define DTE_HI_SHIFT1\t24  \n#define DTE_HI_SHIFT2\t32  \n#define PAGE_DESC_HI_MASK1\tGENMASK_ULL(35, 32)\n#define PAGE_DESC_HI_MASK2\tGENMASK_ULL(39, 36)\n\nstatic inline phys_addr_t rk_dte_pt_address_v2(u32 dte)\n{\n\tu64 dte_v2 = dte;\n\n\tdte_v2 = ((dte_v2 & DTE_HI_MASK2) << DTE_HI_SHIFT2) |\n\t\t ((dte_v2 & DTE_HI_MASK1) << DTE_HI_SHIFT1) |\n\t\t (dte_v2 & RK_DTE_PT_ADDRESS_MASK);\n\n\treturn (phys_addr_t)dte_v2;\n}\n\nstatic inline bool rk_dte_is_pt_valid(u32 dte)\n{\n\treturn dte & RK_DTE_PT_VALID;\n}\n\nstatic inline u32 rk_mk_dte(dma_addr_t pt_dma)\n{\n\treturn (pt_dma & RK_DTE_PT_ADDRESS_MASK) | RK_DTE_PT_VALID;\n}\n\nstatic inline u32 rk_mk_dte_v2(dma_addr_t pt_dma)\n{\n\tpt_dma = (pt_dma & RK_DTE_PT_ADDRESS_MASK) |\n\t\t ((pt_dma & PAGE_DESC_HI_MASK1) >> DTE_HI_SHIFT1) |\n\t\t (pt_dma & PAGE_DESC_HI_MASK2) >> DTE_HI_SHIFT2;\n\n\treturn (pt_dma & RK_DTE_PT_ADDRESS_MASK_V2) | RK_DTE_PT_VALID;\n}\n\n \n#define RK_PTE_PAGE_ADDRESS_MASK  0xfffff000\n#define RK_PTE_PAGE_FLAGS_MASK    0x000001fe\n#define RK_PTE_PAGE_WRITABLE      BIT(2)\n#define RK_PTE_PAGE_READABLE      BIT(1)\n#define RK_PTE_PAGE_VALID         BIT(0)\n\nstatic inline bool rk_pte_is_page_valid(u32 pte)\n{\n\treturn pte & RK_PTE_PAGE_VALID;\n}\n\n \nstatic u32 rk_mk_pte(phys_addr_t page, int prot)\n{\n\tu32 flags = 0;\n\tflags |= (prot & IOMMU_READ) ? RK_PTE_PAGE_READABLE : 0;\n\tflags |= (prot & IOMMU_WRITE) ? RK_PTE_PAGE_WRITABLE : 0;\n\tpage &= RK_PTE_PAGE_ADDRESS_MASK;\n\treturn page | flags | RK_PTE_PAGE_VALID;\n}\n\n \n\nstatic u32 rk_mk_pte_v2(phys_addr_t page, int prot)\n{\n\tu32 flags = 0;\n\n\tflags |= (prot & IOMMU_READ) ? RK_PTE_PAGE_READABLE : 0;\n\tflags |= (prot & IOMMU_WRITE) ? RK_PTE_PAGE_WRITABLE : 0;\n\n\treturn rk_mk_dte_v2(page) | flags;\n}\n\nstatic u32 rk_mk_pte_invalid(u32 pte)\n{\n\treturn pte & ~RK_PTE_PAGE_VALID;\n}\n\n \n#define RK_IOVA_DTE_MASK    0xffc00000\n#define RK_IOVA_DTE_SHIFT   22\n#define RK_IOVA_PTE_MASK    0x003ff000\n#define RK_IOVA_PTE_SHIFT   12\n#define RK_IOVA_PAGE_MASK   0x00000fff\n#define RK_IOVA_PAGE_SHIFT  0\n\nstatic u32 rk_iova_dte_index(dma_addr_t iova)\n{\n\treturn (u32)(iova & RK_IOVA_DTE_MASK) >> RK_IOVA_DTE_SHIFT;\n}\n\nstatic u32 rk_iova_pte_index(dma_addr_t iova)\n{\n\treturn (u32)(iova & RK_IOVA_PTE_MASK) >> RK_IOVA_PTE_SHIFT;\n}\n\nstatic u32 rk_iova_page_offset(dma_addr_t iova)\n{\n\treturn (u32)(iova & RK_IOVA_PAGE_MASK) >> RK_IOVA_PAGE_SHIFT;\n}\n\nstatic u32 rk_iommu_read(void __iomem *base, u32 offset)\n{\n\treturn readl(base + offset);\n}\n\nstatic void rk_iommu_write(void __iomem *base, u32 offset, u32 value)\n{\n\twritel(value, base + offset);\n}\n\nstatic void rk_iommu_command(struct rk_iommu *iommu, u32 command)\n{\n\tint i;\n\n\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\twritel(command, iommu->bases[i] + RK_MMU_COMMAND);\n}\n\nstatic void rk_iommu_base_command(void __iomem *base, u32 command)\n{\n\twritel(command, base + RK_MMU_COMMAND);\n}\nstatic void rk_iommu_zap_lines(struct rk_iommu *iommu, dma_addr_t iova_start,\n\t\t\t       size_t size)\n{\n\tint i;\n\tdma_addr_t iova_end = iova_start + size;\n\t \n\tfor (i = 0; i < iommu->num_mmu; i++) {\n\t\tdma_addr_t iova;\n\n\t\tfor (iova = iova_start; iova < iova_end; iova += SPAGE_SIZE)\n\t\t\trk_iommu_write(iommu->bases[i], RK_MMU_ZAP_ONE_LINE, iova);\n\t}\n}\n\nstatic bool rk_iommu_is_stall_active(struct rk_iommu *iommu)\n{\n\tbool active = true;\n\tint i;\n\n\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\tactive &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &\n\t\t\t\t\t   RK_MMU_STATUS_STALL_ACTIVE);\n\n\treturn active;\n}\n\nstatic bool rk_iommu_is_paging_enabled(struct rk_iommu *iommu)\n{\n\tbool enable = true;\n\tint i;\n\n\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\tenable &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &\n\t\t\t\t\t   RK_MMU_STATUS_PAGING_ENABLED);\n\n\treturn enable;\n}\n\nstatic bool rk_iommu_is_reset_done(struct rk_iommu *iommu)\n{\n\tbool done = true;\n\tint i;\n\n\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\tdone &= rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR) == 0;\n\n\treturn done;\n}\n\nstatic int rk_iommu_enable_stall(struct rk_iommu *iommu)\n{\n\tint ret, i;\n\tbool val;\n\n\tif (rk_iommu_is_stall_active(iommu))\n\t\treturn 0;\n\n\t \n\tif (!rk_iommu_is_paging_enabled(iommu))\n\t\treturn 0;\n\n\trk_iommu_command(iommu, RK_MMU_CMD_ENABLE_STALL);\n\n\tret = readx_poll_timeout(rk_iommu_is_stall_active, iommu, val,\n\t\t\t\t val, RK_MMU_POLL_PERIOD_US,\n\t\t\t\t RK_MMU_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\t\tdev_err(iommu->dev, \"Enable stall request timed out, status: %#08x\\n\",\n\t\t\t\trk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\n\n\treturn ret;\n}\n\nstatic int rk_iommu_disable_stall(struct rk_iommu *iommu)\n{\n\tint ret, i;\n\tbool val;\n\n\tif (!rk_iommu_is_stall_active(iommu))\n\t\treturn 0;\n\n\trk_iommu_command(iommu, RK_MMU_CMD_DISABLE_STALL);\n\n\tret = readx_poll_timeout(rk_iommu_is_stall_active, iommu, val,\n\t\t\t\t !val, RK_MMU_POLL_PERIOD_US,\n\t\t\t\t RK_MMU_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\t\tdev_err(iommu->dev, \"Disable stall request timed out, status: %#08x\\n\",\n\t\t\t\trk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\n\n\treturn ret;\n}\n\nstatic int rk_iommu_enable_paging(struct rk_iommu *iommu)\n{\n\tint ret, i;\n\tbool val;\n\n\tif (rk_iommu_is_paging_enabled(iommu))\n\t\treturn 0;\n\n\trk_iommu_command(iommu, RK_MMU_CMD_ENABLE_PAGING);\n\n\tret = readx_poll_timeout(rk_iommu_is_paging_enabled, iommu, val,\n\t\t\t\t val, RK_MMU_POLL_PERIOD_US,\n\t\t\t\t RK_MMU_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\t\tdev_err(iommu->dev, \"Enable paging request timed out, status: %#08x\\n\",\n\t\t\t\trk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\n\n\treturn ret;\n}\n\nstatic int rk_iommu_disable_paging(struct rk_iommu *iommu)\n{\n\tint ret, i;\n\tbool val;\n\n\tif (!rk_iommu_is_paging_enabled(iommu))\n\t\treturn 0;\n\n\trk_iommu_command(iommu, RK_MMU_CMD_DISABLE_PAGING);\n\n\tret = readx_poll_timeout(rk_iommu_is_paging_enabled, iommu, val,\n\t\t\t\t !val, RK_MMU_POLL_PERIOD_US,\n\t\t\t\t RK_MMU_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tfor (i = 0; i < iommu->num_mmu; i++)\n\t\t\tdev_err(iommu->dev, \"Disable paging request timed out, status: %#08x\\n\",\n\t\t\t\trk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\n\n\treturn ret;\n}\n\nstatic int rk_iommu_force_reset(struct rk_iommu *iommu)\n{\n\tint ret, i;\n\tu32 dte_addr;\n\tbool val;\n\n\tif (iommu->reset_disabled)\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < iommu->num_mmu; i++) {\n\t\tdte_addr = rk_ops->pt_address(DTE_ADDR_DUMMY);\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, dte_addr);\n\n\t\tif (dte_addr != rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR)) {\n\t\t\tdev_err(iommu->dev, \"Error during raw reset. MMU_DTE_ADDR is not functioning\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\trk_iommu_command(iommu, RK_MMU_CMD_FORCE_RESET);\n\n\tret = readx_poll_timeout(rk_iommu_is_reset_done, iommu, val,\n\t\t\t\t val, RK_MMU_FORCE_RESET_TIMEOUT_US,\n\t\t\t\t RK_MMU_POLL_TIMEOUT_US);\n\tif (ret) {\n\t\tdev_err(iommu->dev, \"FORCE_RESET command timed out\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void log_iova(struct rk_iommu *iommu, int index, dma_addr_t iova)\n{\n\tvoid __iomem *base = iommu->bases[index];\n\tu32 dte_index, pte_index, page_offset;\n\tu32 mmu_dte_addr;\n\tphys_addr_t mmu_dte_addr_phys, dte_addr_phys;\n\tu32 *dte_addr;\n\tu32 dte;\n\tphys_addr_t pte_addr_phys = 0;\n\tu32 *pte_addr = NULL;\n\tu32 pte = 0;\n\tphys_addr_t page_addr_phys = 0;\n\tu32 page_flags = 0;\n\n\tdte_index = rk_iova_dte_index(iova);\n\tpte_index = rk_iova_pte_index(iova);\n\tpage_offset = rk_iova_page_offset(iova);\n\n\tmmu_dte_addr = rk_iommu_read(base, RK_MMU_DTE_ADDR);\n\tmmu_dte_addr_phys = rk_ops->pt_address(mmu_dte_addr);\n\n\tdte_addr_phys = mmu_dte_addr_phys + (4 * dte_index);\n\tdte_addr = phys_to_virt(dte_addr_phys);\n\tdte = *dte_addr;\n\n\tif (!rk_dte_is_pt_valid(dte))\n\t\tgoto print_it;\n\n\tpte_addr_phys = rk_ops->pt_address(dte) + (pte_index * 4);\n\tpte_addr = phys_to_virt(pte_addr_phys);\n\tpte = *pte_addr;\n\n\tif (!rk_pte_is_page_valid(pte))\n\t\tgoto print_it;\n\n\tpage_addr_phys = rk_ops->pt_address(pte) + page_offset;\n\tpage_flags = pte & RK_PTE_PAGE_FLAGS_MASK;\n\nprint_it:\n\tdev_err(iommu->dev, \"iova = %pad: dte_index: %#03x pte_index: %#03x page_offset: %#03x\\n\",\n\t\t&iova, dte_index, pte_index, page_offset);\n\tdev_err(iommu->dev, \"mmu_dte_addr: %pa dte@%pa: %#08x valid: %u pte@%pa: %#08x valid: %u page@%pa flags: %#03x\\n\",\n\t\t&mmu_dte_addr_phys, &dte_addr_phys, dte,\n\t\trk_dte_is_pt_valid(dte), &pte_addr_phys, pte,\n\t\trk_pte_is_page_valid(pte), &page_addr_phys, page_flags);\n}\n\nstatic irqreturn_t rk_iommu_irq(int irq, void *dev_id)\n{\n\tstruct rk_iommu *iommu = dev_id;\n\tu32 status;\n\tu32 int_status;\n\tdma_addr_t iova;\n\tirqreturn_t ret = IRQ_NONE;\n\tint i, err;\n\n\terr = pm_runtime_get_if_in_use(iommu->dev);\n\tif (!err || WARN_ON_ONCE(err < 0))\n\t\treturn ret;\n\n\tif (WARN_ON(clk_bulk_enable(iommu->num_clocks, iommu->clocks)))\n\t\tgoto out;\n\n\tfor (i = 0; i < iommu->num_mmu; i++) {\n\t\tint_status = rk_iommu_read(iommu->bases[i], RK_MMU_INT_STATUS);\n\t\tif (int_status == 0)\n\t\t\tcontinue;\n\n\t\tret = IRQ_HANDLED;\n\t\tiova = rk_iommu_read(iommu->bases[i], RK_MMU_PAGE_FAULT_ADDR);\n\n\t\tif (int_status & RK_MMU_IRQ_PAGE_FAULT) {\n\t\t\tint flags;\n\n\t\t\tstatus = rk_iommu_read(iommu->bases[i], RK_MMU_STATUS);\n\t\t\tflags = (status & RK_MMU_STATUS_PAGE_FAULT_IS_WRITE) ?\n\t\t\t\t\tIOMMU_FAULT_WRITE : IOMMU_FAULT_READ;\n\n\t\t\tdev_err(iommu->dev, \"Page fault at %pad of type %s\\n\",\n\t\t\t\t&iova,\n\t\t\t\t(flags == IOMMU_FAULT_WRITE) ? \"write\" : \"read\");\n\n\t\t\tlog_iova(iommu, i, iova);\n\n\t\t\t \n\t\t\tif (iommu->domain != &rk_identity_domain)\n\t\t\t\treport_iommu_fault(iommu->domain, iommu->dev, iova,\n\t\t\t\t\t\t   flags);\n\t\t\telse\n\t\t\t\tdev_err(iommu->dev, \"Page fault while iommu not attached to domain?\\n\");\n\n\t\t\trk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);\n\t\t\trk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_PAGE_FAULT_DONE);\n\t\t}\n\n\t\tif (int_status & RK_MMU_IRQ_BUS_ERROR)\n\t\t\tdev_err(iommu->dev, \"BUS_ERROR occurred at %pad\\n\", &iova);\n\n\t\tif (int_status & ~RK_MMU_IRQ_MASK)\n\t\t\tdev_err(iommu->dev, \"unexpected int_status: %#08x\\n\",\n\t\t\t\tint_status);\n\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_INT_CLEAR, int_status);\n\t}\n\n\tclk_bulk_disable(iommu->num_clocks, iommu->clocks);\n\nout:\n\tpm_runtime_put(iommu->dev);\n\treturn ret;\n}\n\nstatic phys_addr_t rk_iommu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t\t dma_addr_t iova)\n{\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tunsigned long flags;\n\tphys_addr_t pt_phys, phys = 0;\n\tu32 dte, pte;\n\tu32 *page_table;\n\n\tspin_lock_irqsave(&rk_domain->dt_lock, flags);\n\n\tdte = rk_domain->dt[rk_iova_dte_index(iova)];\n\tif (!rk_dte_is_pt_valid(dte))\n\t\tgoto out;\n\n\tpt_phys = rk_ops->pt_address(dte);\n\tpage_table = (u32 *)phys_to_virt(pt_phys);\n\tpte = page_table[rk_iova_pte_index(iova)];\n\tif (!rk_pte_is_page_valid(pte))\n\t\tgoto out;\n\n\tphys = rk_ops->pt_address(pte) + rk_iova_page_offset(iova);\nout:\n\tspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\n\n\treturn phys;\n}\n\nstatic void rk_iommu_zap_iova(struct rk_iommu_domain *rk_domain,\n\t\t\t      dma_addr_t iova, size_t size)\n{\n\tstruct list_head *pos;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&rk_domain->iommus_lock, flags);\n\tlist_for_each(pos, &rk_domain->iommus) {\n\t\tstruct rk_iommu *iommu;\n\t\tint ret;\n\n\t\tiommu = list_entry(pos, struct rk_iommu, node);\n\n\t\t \n\t\tret = pm_runtime_get_if_in_use(iommu->dev);\n\t\tif (WARN_ON_ONCE(ret < 0))\n\t\t\tcontinue;\n\t\tif (ret) {\n\t\t\tWARN_ON(clk_bulk_enable(iommu->num_clocks,\n\t\t\t\t\t\tiommu->clocks));\n\t\t\trk_iommu_zap_lines(iommu, iova, size);\n\t\t\tclk_bulk_disable(iommu->num_clocks, iommu->clocks);\n\t\t\tpm_runtime_put(iommu->dev);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\n}\n\nstatic void rk_iommu_zap_iova_first_last(struct rk_iommu_domain *rk_domain,\n\t\t\t\t\t dma_addr_t iova, size_t size)\n{\n\trk_iommu_zap_iova(rk_domain, iova, SPAGE_SIZE);\n\tif (size > SPAGE_SIZE)\n\t\trk_iommu_zap_iova(rk_domain, iova + size - SPAGE_SIZE,\n\t\t\t\t\tSPAGE_SIZE);\n}\n\nstatic u32 *rk_dte_get_page_table(struct rk_iommu_domain *rk_domain,\n\t\t\t\t  dma_addr_t iova)\n{\n\tu32 *page_table, *dte_addr;\n\tu32 dte_index, dte;\n\tphys_addr_t pt_phys;\n\tdma_addr_t pt_dma;\n\n\tassert_spin_locked(&rk_domain->dt_lock);\n\n\tdte_index = rk_iova_dte_index(iova);\n\tdte_addr = &rk_domain->dt[dte_index];\n\tdte = *dte_addr;\n\tif (rk_dte_is_pt_valid(dte))\n\t\tgoto done;\n\n\tpage_table = (u32 *)get_zeroed_page(GFP_ATOMIC | rk_ops->gfp_flags);\n\tif (!page_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpt_dma = dma_map_single(dma_dev, page_table, SPAGE_SIZE, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dma_dev, pt_dma)) {\n\t\tdev_err(dma_dev, \"DMA mapping error while allocating page table\\n\");\n\t\tfree_page((unsigned long)page_table);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tdte = rk_ops->mk_dtentries(pt_dma);\n\t*dte_addr = dte;\n\n\trk_table_flush(rk_domain,\n\t\t       rk_domain->dt_dma + dte_index * sizeof(u32), 1);\ndone:\n\tpt_phys = rk_ops->pt_address(dte);\n\treturn (u32 *)phys_to_virt(pt_phys);\n}\n\nstatic size_t rk_iommu_unmap_iova(struct rk_iommu_domain *rk_domain,\n\t\t\t\t  u32 *pte_addr, dma_addr_t pte_dma,\n\t\t\t\t  size_t size)\n{\n\tunsigned int pte_count;\n\tunsigned int pte_total = size / SPAGE_SIZE;\n\n\tassert_spin_locked(&rk_domain->dt_lock);\n\n\tfor (pte_count = 0; pte_count < pte_total; pte_count++) {\n\t\tu32 pte = pte_addr[pte_count];\n\t\tif (!rk_pte_is_page_valid(pte))\n\t\t\tbreak;\n\n\t\tpte_addr[pte_count] = rk_mk_pte_invalid(pte);\n\t}\n\n\trk_table_flush(rk_domain, pte_dma, pte_count);\n\n\treturn pte_count * SPAGE_SIZE;\n}\n\nstatic int rk_iommu_map_iova(struct rk_iommu_domain *rk_domain, u32 *pte_addr,\n\t\t\t     dma_addr_t pte_dma, dma_addr_t iova,\n\t\t\t     phys_addr_t paddr, size_t size, int prot)\n{\n\tunsigned int pte_count;\n\tunsigned int pte_total = size / SPAGE_SIZE;\n\tphys_addr_t page_phys;\n\n\tassert_spin_locked(&rk_domain->dt_lock);\n\n\tfor (pte_count = 0; pte_count < pte_total; pte_count++) {\n\t\tu32 pte = pte_addr[pte_count];\n\n\t\tif (rk_pte_is_page_valid(pte))\n\t\t\tgoto unwind;\n\n\t\tpte_addr[pte_count] = rk_ops->mk_ptentries(paddr, prot);\n\n\t\tpaddr += SPAGE_SIZE;\n\t}\n\n\trk_table_flush(rk_domain, pte_dma, pte_total);\n\n\t \n\trk_iommu_zap_iova_first_last(rk_domain, iova, size);\n\n\treturn 0;\nunwind:\n\t \n\trk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma,\n\t\t\t    pte_count * SPAGE_SIZE);\n\n\tiova += pte_count * SPAGE_SIZE;\n\tpage_phys = rk_ops->pt_address(pte_addr[pte_count]);\n\tpr_err(\"iova: %pad already mapped to %pa cannot remap to phys: %pa prot: %#x\\n\",\n\t       &iova, &page_phys, &paddr, prot);\n\n\treturn -EADDRINUSE;\n}\n\nstatic int rk_iommu_map(struct iommu_domain *domain, unsigned long _iova,\n\t\t\tphys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tunsigned long flags;\n\tdma_addr_t pte_dma, iova = (dma_addr_t)_iova;\n\tu32 *page_table, *pte_addr;\n\tu32 dte_index, pte_index;\n\tint ret;\n\n\tspin_lock_irqsave(&rk_domain->dt_lock, flags);\n\n\t \n\tpage_table = rk_dte_get_page_table(rk_domain, iova);\n\tif (IS_ERR(page_table)) {\n\t\tspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\n\t\treturn PTR_ERR(page_table);\n\t}\n\n\tdte_index = rk_domain->dt[rk_iova_dte_index(iova)];\n\tpte_index = rk_iova_pte_index(iova);\n\tpte_addr = &page_table[pte_index];\n\n\tpte_dma = rk_ops->pt_address(dte_index) + pte_index * sizeof(u32);\n\tret = rk_iommu_map_iova(rk_domain, pte_addr, pte_dma, iova,\n\t\t\t\tpaddr, size, prot);\n\n\tspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\n\n\treturn ret;\n}\n\nstatic size_t rk_iommu_unmap(struct iommu_domain *domain, unsigned long _iova,\n\t\t\t     size_t size, struct iommu_iotlb_gather *gather)\n{\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tunsigned long flags;\n\tdma_addr_t pte_dma, iova = (dma_addr_t)_iova;\n\tphys_addr_t pt_phys;\n\tu32 dte;\n\tu32 *pte_addr;\n\tsize_t unmap_size;\n\n\tspin_lock_irqsave(&rk_domain->dt_lock, flags);\n\n\t \n\tdte = rk_domain->dt[rk_iova_dte_index(iova)];\n\t \n\tif (!rk_dte_is_pt_valid(dte)) {\n\t\tspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\n\t\treturn 0;\n\t}\n\n\tpt_phys = rk_ops->pt_address(dte);\n\tpte_addr = (u32 *)phys_to_virt(pt_phys) + rk_iova_pte_index(iova);\n\tpte_dma = pt_phys + rk_iova_pte_index(iova) * sizeof(u32);\n\tunmap_size = rk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma, size);\n\n\tspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\n\n\t \n\trk_iommu_zap_iova(rk_domain, iova, unmap_size);\n\n\treturn unmap_size;\n}\n\nstatic struct rk_iommu *rk_iommu_from_dev(struct device *dev)\n{\n\tstruct rk_iommudata *data = dev_iommu_priv_get(dev);\n\n\treturn data ? data->iommu : NULL;\n}\n\n \nstatic void rk_iommu_disable(struct rk_iommu *iommu)\n{\n\tint i;\n\n\t \n\tWARN_ON(clk_bulk_enable(iommu->num_clocks, iommu->clocks));\n\trk_iommu_enable_stall(iommu);\n\trk_iommu_disable_paging(iommu);\n\tfor (i = 0; i < iommu->num_mmu; i++) {\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, 0);\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, 0);\n\t}\n\trk_iommu_disable_stall(iommu);\n\tclk_bulk_disable(iommu->num_clocks, iommu->clocks);\n}\n\n \nstatic int rk_iommu_enable(struct rk_iommu *iommu)\n{\n\tstruct iommu_domain *domain = iommu->domain;\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tint ret, i;\n\n\tret = clk_bulk_enable(iommu->num_clocks, iommu->clocks);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rk_iommu_enable_stall(iommu);\n\tif (ret)\n\t\tgoto out_disable_clocks;\n\n\tret = rk_iommu_force_reset(iommu);\n\tif (ret)\n\t\tgoto out_disable_stall;\n\n\tfor (i = 0; i < iommu->num_mmu; i++) {\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR,\n\t\t\t       rk_ops->mk_dtentries(rk_domain->dt_dma));\n\t\trk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);\n\t\trk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, RK_MMU_IRQ_MASK);\n\t}\n\n\tret = rk_iommu_enable_paging(iommu);\n\nout_disable_stall:\n\trk_iommu_disable_stall(iommu);\nout_disable_clocks:\n\tclk_bulk_disable(iommu->num_clocks, iommu->clocks);\n\treturn ret;\n}\n\nstatic int rk_iommu_identity_attach(struct iommu_domain *identity_domain,\n\t\t\t\t    struct device *dev)\n{\n\tstruct rk_iommu *iommu;\n\tstruct rk_iommu_domain *rk_domain;\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tiommu = rk_iommu_from_dev(dev);\n\tif (!iommu)\n\t\treturn -ENODEV;\n\n\trk_domain = to_rk_domain(iommu->domain);\n\n\tdev_dbg(dev, \"Detaching from iommu domain\\n\");\n\n\tif (iommu->domain == identity_domain)\n\t\treturn 0;\n\n\tiommu->domain = identity_domain;\n\n\tspin_lock_irqsave(&rk_domain->iommus_lock, flags);\n\tlist_del_init(&iommu->node);\n\tspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\n\n\tret = pm_runtime_get_if_in_use(iommu->dev);\n\tWARN_ON_ONCE(ret < 0);\n\tif (ret > 0) {\n\t\trk_iommu_disable(iommu);\n\t\tpm_runtime_put(iommu->dev);\n\t}\n\n\treturn 0;\n}\n\nstatic void rk_iommu_identity_free(struct iommu_domain *domain)\n{\n}\n\nstatic struct iommu_domain_ops rk_identity_ops = {\n\t.attach_dev = rk_iommu_identity_attach,\n\t.free = rk_iommu_identity_free,\n};\n\nstatic struct iommu_domain rk_identity_domain = {\n\t.type = IOMMU_DOMAIN_IDENTITY,\n\t.ops = &rk_identity_ops,\n};\n\n#ifdef CONFIG_ARM\nstatic void rk_iommu_set_platform_dma(struct device *dev)\n{\n\tWARN_ON(rk_iommu_identity_attach(&rk_identity_domain, dev));\n}\n#endif\n\nstatic int rk_iommu_attach_device(struct iommu_domain *domain,\n\t\tstruct device *dev)\n{\n\tstruct rk_iommu *iommu;\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tiommu = rk_iommu_from_dev(dev);\n\tif (!iommu)\n\t\treturn 0;\n\n\tdev_dbg(dev, \"Attaching to iommu domain\\n\");\n\n\t \n\tif (iommu->domain == domain)\n\t\treturn 0;\n\n\tret = rk_iommu_identity_attach(&rk_identity_domain, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tiommu->domain = domain;\n\n\tspin_lock_irqsave(&rk_domain->iommus_lock, flags);\n\tlist_add_tail(&iommu->node, &rk_domain->iommus);\n\tspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\n\n\tret = pm_runtime_get_if_in_use(iommu->dev);\n\tif (!ret || WARN_ON_ONCE(ret < 0))\n\t\treturn 0;\n\n\tret = rk_iommu_enable(iommu);\n\tif (ret)\n\t\tWARN_ON(rk_iommu_identity_attach(&rk_identity_domain, dev));\n\n\tpm_runtime_put(iommu->dev);\n\n\treturn ret;\n}\n\nstatic struct iommu_domain *rk_iommu_domain_alloc(unsigned type)\n{\n\tstruct rk_iommu_domain *rk_domain;\n\n\tif (type == IOMMU_DOMAIN_IDENTITY)\n\t\treturn &rk_identity_domain;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED && type != IOMMU_DOMAIN_DMA)\n\t\treturn NULL;\n\n\tif (!dma_dev)\n\t\treturn NULL;\n\n\trk_domain = kzalloc(sizeof(*rk_domain), GFP_KERNEL);\n\tif (!rk_domain)\n\t\treturn NULL;\n\n\t \n\trk_domain->dt = (u32 *)get_zeroed_page(GFP_KERNEL | rk_ops->gfp_flags);\n\tif (!rk_domain->dt)\n\t\tgoto err_free_domain;\n\n\trk_domain->dt_dma = dma_map_single(dma_dev, rk_domain->dt,\n\t\t\t\t\t   SPAGE_SIZE, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dma_dev, rk_domain->dt_dma)) {\n\t\tdev_err(dma_dev, \"DMA map error for DT\\n\");\n\t\tgoto err_free_dt;\n\t}\n\n\tspin_lock_init(&rk_domain->iommus_lock);\n\tspin_lock_init(&rk_domain->dt_lock);\n\tINIT_LIST_HEAD(&rk_domain->iommus);\n\n\trk_domain->domain.geometry.aperture_start = 0;\n\trk_domain->domain.geometry.aperture_end   = DMA_BIT_MASK(32);\n\trk_domain->domain.geometry.force_aperture = true;\n\n\treturn &rk_domain->domain;\n\nerr_free_dt:\n\tfree_page((unsigned long)rk_domain->dt);\nerr_free_domain:\n\tkfree(rk_domain);\n\n\treturn NULL;\n}\n\nstatic void rk_iommu_domain_free(struct iommu_domain *domain)\n{\n\tstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\n\tint i;\n\n\tWARN_ON(!list_empty(&rk_domain->iommus));\n\n\tfor (i = 0; i < NUM_DT_ENTRIES; i++) {\n\t\tu32 dte = rk_domain->dt[i];\n\t\tif (rk_dte_is_pt_valid(dte)) {\n\t\t\tphys_addr_t pt_phys = rk_ops->pt_address(dte);\n\t\t\tu32 *page_table = phys_to_virt(pt_phys);\n\t\t\tdma_unmap_single(dma_dev, pt_phys,\n\t\t\t\t\t SPAGE_SIZE, DMA_TO_DEVICE);\n\t\t\tfree_page((unsigned long)page_table);\n\t\t}\n\t}\n\n\tdma_unmap_single(dma_dev, rk_domain->dt_dma,\n\t\t\t SPAGE_SIZE, DMA_TO_DEVICE);\n\tfree_page((unsigned long)rk_domain->dt);\n\n\tkfree(rk_domain);\n}\n\nstatic struct iommu_device *rk_iommu_probe_device(struct device *dev)\n{\n\tstruct rk_iommudata *data;\n\tstruct rk_iommu *iommu;\n\n\tdata = dev_iommu_priv_get(dev);\n\tif (!data)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tiommu = rk_iommu_from_dev(dev);\n\n\tdata->link = device_link_add(dev, iommu->dev,\n\t\t\t\t     DL_FLAG_STATELESS | DL_FLAG_PM_RUNTIME);\n\n\treturn &iommu->iommu;\n}\n\nstatic void rk_iommu_release_device(struct device *dev)\n{\n\tstruct rk_iommudata *data = dev_iommu_priv_get(dev);\n\n\tdevice_link_del(data->link);\n}\n\nstatic struct iommu_group *rk_iommu_device_group(struct device *dev)\n{\n\tstruct rk_iommu *iommu;\n\n\tiommu = rk_iommu_from_dev(dev);\n\n\treturn iommu_group_ref_get(iommu->group);\n}\n\nstatic int rk_iommu_of_xlate(struct device *dev,\n\t\t\t     struct of_phandle_args *args)\n{\n\tstruct platform_device *iommu_dev;\n\tstruct rk_iommudata *data;\n\n\tdata = devm_kzalloc(dma_dev, sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tiommu_dev = of_find_device_by_node(args->np);\n\n\tdata->iommu = platform_get_drvdata(iommu_dev);\n\tdata->iommu->domain = &rk_identity_domain;\n\tdev_iommu_priv_set(dev, data);\n\n\tplatform_device_put(iommu_dev);\n\n\treturn 0;\n}\n\nstatic const struct iommu_ops rk_iommu_ops = {\n\t.domain_alloc = rk_iommu_domain_alloc,\n\t.probe_device = rk_iommu_probe_device,\n\t.release_device = rk_iommu_release_device,\n\t.device_group = rk_iommu_device_group,\n#ifdef CONFIG_ARM\n\t.set_platform_dma_ops = rk_iommu_set_platform_dma,\n#endif\n\t.pgsize_bitmap = RK_IOMMU_PGSIZE_BITMAP,\n\t.of_xlate = rk_iommu_of_xlate,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= rk_iommu_attach_device,\n\t\t.map\t\t= rk_iommu_map,\n\t\t.unmap\t\t= rk_iommu_unmap,\n\t\t.iova_to_phys\t= rk_iommu_iova_to_phys,\n\t\t.free\t\t= rk_iommu_domain_free,\n\t}\n};\n\nstatic int rk_iommu_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct rk_iommu *iommu;\n\tstruct resource *res;\n\tconst struct rk_iommu_ops *ops;\n\tint num_res = pdev->num_resources;\n\tint err, i;\n\n\tiommu = devm_kzalloc(dev, sizeof(*iommu), GFP_KERNEL);\n\tif (!iommu)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, iommu);\n\tiommu->dev = dev;\n\tiommu->num_mmu = 0;\n\n\tops = of_device_get_match_data(dev);\n\tif (!rk_ops)\n\t\trk_ops = ops;\n\n\t \n\tif (WARN_ON(rk_ops != ops))\n\t\treturn -EINVAL;\n\n\tiommu->bases = devm_kcalloc(dev, num_res, sizeof(*iommu->bases),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iommu->bases)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_res; i++) {\n\t\tres = platform_get_resource(pdev, IORESOURCE_MEM, i);\n\t\tif (!res)\n\t\t\tcontinue;\n\t\tiommu->bases[i] = devm_ioremap_resource(&pdev->dev, res);\n\t\tif (IS_ERR(iommu->bases[i]))\n\t\t\tcontinue;\n\t\tiommu->num_mmu++;\n\t}\n\tif (iommu->num_mmu == 0)\n\t\treturn PTR_ERR(iommu->bases[0]);\n\n\tiommu->num_irq = platform_irq_count(pdev);\n\tif (iommu->num_irq < 0)\n\t\treturn iommu->num_irq;\n\n\tiommu->reset_disabled = device_property_read_bool(dev,\n\t\t\t\t\t\"rockchip,disable-mmu-reset\");\n\n\tiommu->num_clocks = ARRAY_SIZE(rk_iommu_clocks);\n\tiommu->clocks = devm_kcalloc(iommu->dev, iommu->num_clocks,\n\t\t\t\t     sizeof(*iommu->clocks), GFP_KERNEL);\n\tif (!iommu->clocks)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < iommu->num_clocks; ++i)\n\t\tiommu->clocks[i].id = rk_iommu_clocks[i];\n\n\t \n\terr = devm_clk_bulk_get(iommu->dev, iommu->num_clocks, iommu->clocks);\n\tif (err == -ENOENT)\n\t\tiommu->num_clocks = 0;\n\telse if (err)\n\t\treturn err;\n\n\terr = clk_bulk_prepare(iommu->num_clocks, iommu->clocks);\n\tif (err)\n\t\treturn err;\n\n\tiommu->group = iommu_group_alloc();\n\tif (IS_ERR(iommu->group)) {\n\t\terr = PTR_ERR(iommu->group);\n\t\tgoto err_unprepare_clocks;\n\t}\n\n\terr = iommu_device_sysfs_add(&iommu->iommu, dev, NULL, dev_name(dev));\n\tif (err)\n\t\tgoto err_put_group;\n\n\terr = iommu_device_register(&iommu->iommu, &rk_iommu_ops, dev);\n\tif (err)\n\t\tgoto err_remove_sysfs;\n\n\t \n\tif (!dma_dev)\n\t\tdma_dev = &pdev->dev;\n\n\tpm_runtime_enable(dev);\n\n\tfor (i = 0; i < iommu->num_irq; i++) {\n\t\tint irq = platform_get_irq(pdev, i);\n\n\t\tif (irq < 0) {\n\t\t\terr = irq;\n\t\t\tgoto err_pm_disable;\n\t\t}\n\n\t\terr = devm_request_irq(iommu->dev, irq, rk_iommu_irq,\n\t\t\t\t       IRQF_SHARED, dev_name(dev), iommu);\n\t\tif (err)\n\t\t\tgoto err_pm_disable;\n\t}\n\n\tdma_set_mask_and_coherent(dev, rk_ops->dma_bit_mask);\n\n\treturn 0;\nerr_pm_disable:\n\tpm_runtime_disable(dev);\nerr_remove_sysfs:\n\tiommu_device_sysfs_remove(&iommu->iommu);\nerr_put_group:\n\tiommu_group_put(iommu->group);\nerr_unprepare_clocks:\n\tclk_bulk_unprepare(iommu->num_clocks, iommu->clocks);\n\treturn err;\n}\n\nstatic void rk_iommu_shutdown(struct platform_device *pdev)\n{\n\tstruct rk_iommu *iommu = platform_get_drvdata(pdev);\n\tint i;\n\n\tfor (i = 0; i < iommu->num_irq; i++) {\n\t\tint irq = platform_get_irq(pdev, i);\n\n\t\tdevm_free_irq(iommu->dev, irq, iommu);\n\t}\n\n\tpm_runtime_force_suspend(&pdev->dev);\n}\n\nstatic int __maybe_unused rk_iommu_suspend(struct device *dev)\n{\n\tstruct rk_iommu *iommu = dev_get_drvdata(dev);\n\n\tif (iommu->domain == &rk_identity_domain)\n\t\treturn 0;\n\n\trk_iommu_disable(iommu);\n\treturn 0;\n}\n\nstatic int __maybe_unused rk_iommu_resume(struct device *dev)\n{\n\tstruct rk_iommu *iommu = dev_get_drvdata(dev);\n\n\tif (iommu->domain == &rk_identity_domain)\n\t\treturn 0;\n\n\treturn rk_iommu_enable(iommu);\n}\n\nstatic const struct dev_pm_ops rk_iommu_pm_ops = {\n\tSET_RUNTIME_PM_OPS(rk_iommu_suspend, rk_iommu_resume, NULL)\n\tSET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,\n\t\t\t\tpm_runtime_force_resume)\n};\n\nstatic struct rk_iommu_ops iommu_data_ops_v1 = {\n\t.pt_address = &rk_dte_pt_address,\n\t.mk_dtentries = &rk_mk_dte,\n\t.mk_ptentries = &rk_mk_pte,\n\t.dma_bit_mask = DMA_BIT_MASK(32),\n\t.gfp_flags = GFP_DMA32,\n};\n\nstatic struct rk_iommu_ops iommu_data_ops_v2 = {\n\t.pt_address = &rk_dte_pt_address_v2,\n\t.mk_dtentries = &rk_mk_dte_v2,\n\t.mk_ptentries = &rk_mk_pte_v2,\n\t.dma_bit_mask = DMA_BIT_MASK(40),\n\t.gfp_flags = 0,\n};\n\nstatic const struct of_device_id rk_iommu_dt_ids[] = {\n\t{\t.compatible = \"rockchip,iommu\",\n\t\t.data = &iommu_data_ops_v1,\n\t},\n\t{\t.compatible = \"rockchip,rk3568-iommu\",\n\t\t.data = &iommu_data_ops_v2,\n\t},\n\t{   }\n};\n\nstatic struct platform_driver rk_iommu_driver = {\n\t.probe = rk_iommu_probe,\n\t.shutdown = rk_iommu_shutdown,\n\t.driver = {\n\t\t   .name = \"rk_iommu\",\n\t\t   .of_match_table = rk_iommu_dt_ids,\n\t\t   .pm = &rk_iommu_pm_ops,\n\t\t   .suppress_bind_attrs = true,\n\t},\n};\nbuiltin_platform_driver(rk_iommu_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}