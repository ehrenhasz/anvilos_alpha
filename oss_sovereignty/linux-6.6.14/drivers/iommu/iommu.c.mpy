{
  "module_name": "iommu.c",
  "hash_id": "addf9a940dae62eac1cb8b3ed1794dda0e34e9cc13814506d11a5b993f8733e3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/iommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)    \"iommu: \" fmt\n\n#include <linux/amba/bus.h>\n#include <linux/device.h>\n#include <linux/kernel.h>\n#include <linux/bits.h>\n#include <linux/bug.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/host1x_context_bus.h>\n#include <linux/iommu.h>\n#include <linux/idr.h>\n#include <linux/err.h>\n#include <linux/pci.h>\n#include <linux/pci-ats.h>\n#include <linux/bitops.h>\n#include <linux/platform_device.h>\n#include <linux/property.h>\n#include <linux/fsl/mc.h>\n#include <linux/module.h>\n#include <linux/cc_platform.h>\n#include <linux/cdx/cdx_bus.h>\n#include <trace/events/iommu.h>\n#include <linux/sched/mm.h>\n#include <linux/msi.h>\n\n#include \"dma-iommu.h\"\n#include \"iommu-priv.h\"\n\n#include \"iommu-sva.h\"\n#include \"iommu-priv.h\"\n\nstatic struct kset *iommu_group_kset;\nstatic DEFINE_IDA(iommu_group_ida);\nstatic DEFINE_IDA(iommu_global_pasid_ida);\n\nstatic unsigned int iommu_def_domain_type __read_mostly;\nstatic bool iommu_dma_strict __read_mostly = IS_ENABLED(CONFIG_IOMMU_DEFAULT_DMA_STRICT);\nstatic u32 iommu_cmd_line __read_mostly;\n\nstruct iommu_group {\n\tstruct kobject kobj;\n\tstruct kobject *devices_kobj;\n\tstruct list_head devices;\n\tstruct xarray pasid_array;\n\tstruct mutex mutex;\n\tvoid *iommu_data;\n\tvoid (*iommu_data_release)(void *iommu_data);\n\tchar *name;\n\tint id;\n\tstruct iommu_domain *default_domain;\n\tstruct iommu_domain *blocking_domain;\n\tstruct iommu_domain *domain;\n\tstruct list_head entry;\n\tunsigned int owner_cnt;\n\tvoid *owner;\n};\n\nstruct group_device {\n\tstruct list_head list;\n\tstruct device *dev;\n\tchar *name;\n};\n\n \n#define for_each_group_device(group, pos) \\\n\tlist_for_each_entry(pos, &(group)->devices, list)\n\nstruct iommu_group_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct iommu_group *group, char *buf);\n\tssize_t (*store)(struct iommu_group *group,\n\t\t\t const char *buf, size_t count);\n};\n\nstatic const char * const iommu_group_resv_type_string[] = {\n\t[IOMMU_RESV_DIRECT]\t\t\t= \"direct\",\n\t[IOMMU_RESV_DIRECT_RELAXABLE]\t\t= \"direct-relaxable\",\n\t[IOMMU_RESV_RESERVED]\t\t\t= \"reserved\",\n\t[IOMMU_RESV_MSI]\t\t\t= \"msi\",\n\t[IOMMU_RESV_SW_MSI]\t\t\t= \"msi\",\n};\n\n#define IOMMU_CMD_LINE_DMA_API\t\tBIT(0)\n#define IOMMU_CMD_LINE_STRICT\t\tBIT(1)\n\nstatic int iommu_bus_notifier(struct notifier_block *nb,\n\t\t\t      unsigned long action, void *data);\nstatic void iommu_release_device(struct device *dev);\nstatic struct iommu_domain *__iommu_domain_alloc(const struct bus_type *bus,\n\t\t\t\t\t\t unsigned type);\nstatic int __iommu_attach_device(struct iommu_domain *domain,\n\t\t\t\t struct device *dev);\nstatic int __iommu_attach_group(struct iommu_domain *domain,\n\t\t\t\tstruct iommu_group *group);\n\nenum {\n\tIOMMU_SET_DOMAIN_MUST_SUCCEED = 1 << 0,\n};\n\nstatic int __iommu_device_set_domain(struct iommu_group *group,\n\t\t\t\t     struct device *dev,\n\t\t\t\t     struct iommu_domain *new_domain,\n\t\t\t\t     unsigned int flags);\nstatic int __iommu_group_set_domain_internal(struct iommu_group *group,\n\t\t\t\t\t     struct iommu_domain *new_domain,\n\t\t\t\t\t     unsigned int flags);\nstatic int __iommu_group_set_domain(struct iommu_group *group,\n\t\t\t\t    struct iommu_domain *new_domain)\n{\n\treturn __iommu_group_set_domain_internal(group, new_domain, 0);\n}\nstatic void __iommu_group_set_domain_nofail(struct iommu_group *group,\n\t\t\t\t\t    struct iommu_domain *new_domain)\n{\n\tWARN_ON(__iommu_group_set_domain_internal(\n\t\tgroup, new_domain, IOMMU_SET_DOMAIN_MUST_SUCCEED));\n}\n\nstatic int iommu_setup_default_domain(struct iommu_group *group,\n\t\t\t\t      int target_type);\nstatic int iommu_create_device_direct_mappings(struct iommu_domain *domain,\n\t\t\t\t\t       struct device *dev);\nstatic ssize_t iommu_group_store_type(struct iommu_group *group,\n\t\t\t\t      const char *buf, size_t count);\nstatic struct group_device *iommu_group_alloc_device(struct iommu_group *group,\n\t\t\t\t\t\t     struct device *dev);\nstatic void __iommu_group_free_device(struct iommu_group *group,\n\t\t\t\t      struct group_device *grp_dev);\n\n#define IOMMU_GROUP_ATTR(_name, _mode, _show, _store)\t\t\\\nstruct iommu_group_attribute iommu_group_attr_##_name =\t\t\\\n\t__ATTR(_name, _mode, _show, _store)\n\n#define to_iommu_group_attr(_attr)\t\\\n\tcontainer_of(_attr, struct iommu_group_attribute, attr)\n#define to_iommu_group(_kobj)\t\t\\\n\tcontainer_of(_kobj, struct iommu_group, kobj)\n\nstatic LIST_HEAD(iommu_device_list);\nstatic DEFINE_SPINLOCK(iommu_device_lock);\n\nstatic struct bus_type * const iommu_buses[] = {\n\t&platform_bus_type,\n#ifdef CONFIG_PCI\n\t&pci_bus_type,\n#endif\n#ifdef CONFIG_ARM_AMBA\n\t&amba_bustype,\n#endif\n#ifdef CONFIG_FSL_MC_BUS\n\t&fsl_mc_bus_type,\n#endif\n#ifdef CONFIG_TEGRA_HOST1X_CONTEXT_BUS\n\t&host1x_context_device_bus_type,\n#endif\n#ifdef CONFIG_CDX_BUS\n\t&cdx_bus_type,\n#endif\n};\n\n \nstatic const char *iommu_domain_type_str(unsigned int t)\n{\n\tswitch (t) {\n\tcase IOMMU_DOMAIN_BLOCKED:\n\t\treturn \"Blocked\";\n\tcase IOMMU_DOMAIN_IDENTITY:\n\t\treturn \"Passthrough\";\n\tcase IOMMU_DOMAIN_UNMANAGED:\n\t\treturn \"Unmanaged\";\n\tcase IOMMU_DOMAIN_DMA:\n\tcase IOMMU_DOMAIN_DMA_FQ:\n\t\treturn \"Translated\";\n\tdefault:\n\t\treturn \"Unknown\";\n\t}\n}\n\nstatic int __init iommu_subsys_init(void)\n{\n\tstruct notifier_block *nb;\n\n\tif (!(iommu_cmd_line & IOMMU_CMD_LINE_DMA_API)) {\n\t\tif (IS_ENABLED(CONFIG_IOMMU_DEFAULT_PASSTHROUGH))\n\t\t\tiommu_set_default_passthrough(false);\n\t\telse\n\t\t\tiommu_set_default_translated(false);\n\n\t\tif (iommu_default_passthrough() && cc_platform_has(CC_ATTR_MEM_ENCRYPT)) {\n\t\t\tpr_info(\"Memory encryption detected - Disabling default IOMMU Passthrough\\n\");\n\t\t\tiommu_set_default_translated(false);\n\t\t}\n\t}\n\n\tif (!iommu_default_passthrough() && !iommu_dma_strict)\n\t\tiommu_def_domain_type = IOMMU_DOMAIN_DMA_FQ;\n\n\tpr_info(\"Default domain type: %s%s\\n\",\n\t\tiommu_domain_type_str(iommu_def_domain_type),\n\t\t(iommu_cmd_line & IOMMU_CMD_LINE_DMA_API) ?\n\t\t\t\" (set via kernel command line)\" : \"\");\n\n\tif (!iommu_default_passthrough())\n\t\tpr_info(\"DMA domain TLB invalidation policy: %s mode%s\\n\",\n\t\t\tiommu_dma_strict ? \"strict\" : \"lazy\",\n\t\t\t(iommu_cmd_line & IOMMU_CMD_LINE_STRICT) ?\n\t\t\t\t\" (set via kernel command line)\" : \"\");\n\n\tnb = kcalloc(ARRAY_SIZE(iommu_buses), sizeof(*nb), GFP_KERNEL);\n\tif (!nb)\n\t\treturn -ENOMEM;\n\n\tfor (int i = 0; i < ARRAY_SIZE(iommu_buses); i++) {\n\t\tnb[i].notifier_call = iommu_bus_notifier;\n\t\tbus_register_notifier(iommu_buses[i], &nb[i]);\n\t}\n\n\treturn 0;\n}\nsubsys_initcall(iommu_subsys_init);\n\nstatic int remove_iommu_group(struct device *dev, void *data)\n{\n\tif (dev->iommu && dev->iommu->iommu_dev == data)\n\t\tiommu_release_device(dev);\n\n\treturn 0;\n}\n\n \nint iommu_device_register(struct iommu_device *iommu,\n\t\t\t  const struct iommu_ops *ops, struct device *hwdev)\n{\n\tint err = 0;\n\n\t \n\tif (WARN_ON(is_module_address((unsigned long)ops) && !ops->owner))\n\t\treturn -EINVAL;\n\t \n\tif (iommu_buses[0]->iommu_ops && iommu_buses[0]->iommu_ops != ops)\n\t\treturn -EBUSY;\n\n\tiommu->ops = ops;\n\tif (hwdev)\n\t\tiommu->fwnode = dev_fwnode(hwdev);\n\n\tspin_lock(&iommu_device_lock);\n\tlist_add_tail(&iommu->list, &iommu_device_list);\n\tspin_unlock(&iommu_device_lock);\n\n\tfor (int i = 0; i < ARRAY_SIZE(iommu_buses) && !err; i++) {\n\t\tiommu_buses[i]->iommu_ops = ops;\n\t\terr = bus_iommu_probe(iommu_buses[i]);\n\t}\n\tif (err)\n\t\tiommu_device_unregister(iommu);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(iommu_device_register);\n\nvoid iommu_device_unregister(struct iommu_device *iommu)\n{\n\tfor (int i = 0; i < ARRAY_SIZE(iommu_buses); i++)\n\t\tbus_for_each_dev(iommu_buses[i], NULL, iommu, remove_iommu_group);\n\n\tspin_lock(&iommu_device_lock);\n\tlist_del(&iommu->list);\n\tspin_unlock(&iommu_device_lock);\n}\nEXPORT_SYMBOL_GPL(iommu_device_unregister);\n\n#if IS_ENABLED(CONFIG_IOMMUFD_TEST)\nvoid iommu_device_unregister_bus(struct iommu_device *iommu,\n\t\t\t\t struct bus_type *bus,\n\t\t\t\t struct notifier_block *nb)\n{\n\tbus_unregister_notifier(bus, nb);\n\tiommu_device_unregister(iommu);\n}\nEXPORT_SYMBOL_GPL(iommu_device_unregister_bus);\n\n \nint iommu_device_register_bus(struct iommu_device *iommu,\n\t\t\t      const struct iommu_ops *ops, struct bus_type *bus,\n\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\tiommu->ops = ops;\n\tnb->notifier_call = iommu_bus_notifier;\n\terr = bus_register_notifier(bus, nb);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&iommu_device_lock);\n\tlist_add_tail(&iommu->list, &iommu_device_list);\n\tspin_unlock(&iommu_device_lock);\n\n\tbus->iommu_ops = ops;\n\terr = bus_iommu_probe(bus);\n\tif (err) {\n\t\tiommu_device_unregister_bus(iommu, bus, nb);\n\t\treturn err;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iommu_device_register_bus);\n#endif\n\nstatic struct dev_iommu *dev_iommu_get(struct device *dev)\n{\n\tstruct dev_iommu *param = dev->iommu;\n\n\tif (param)\n\t\treturn param;\n\n\tparam = kzalloc(sizeof(*param), GFP_KERNEL);\n\tif (!param)\n\t\treturn NULL;\n\n\tmutex_init(&param->lock);\n\tdev->iommu = param;\n\treturn param;\n}\n\nstatic void dev_iommu_free(struct device *dev)\n{\n\tstruct dev_iommu *param = dev->iommu;\n\n\tdev->iommu = NULL;\n\tif (param->fwspec) {\n\t\tfwnode_handle_put(param->fwspec->iommu_fwnode);\n\t\tkfree(param->fwspec);\n\t}\n\tkfree(param);\n}\n\nstatic u32 dev_iommu_get_max_pasids(struct device *dev)\n{\n\tu32 max_pasids = 0, bits = 0;\n\tint ret;\n\n\tif (dev_is_pci(dev)) {\n\t\tret = pci_max_pasids(to_pci_dev(dev));\n\t\tif (ret > 0)\n\t\t\tmax_pasids = ret;\n\t} else {\n\t\tret = device_property_read_u32(dev, \"pasid-num-bits\", &bits);\n\t\tif (!ret)\n\t\t\tmax_pasids = 1UL << bits;\n\t}\n\n\treturn min_t(u32, max_pasids, dev->iommu->iommu_dev->max_pasids);\n}\n\n \nstatic int iommu_init_device(struct device *dev, const struct iommu_ops *ops)\n{\n\tstruct iommu_device *iommu_dev;\n\tstruct iommu_group *group;\n\tint ret;\n\n\tif (!dev_iommu_get(dev))\n\t\treturn -ENOMEM;\n\n\tif (!try_module_get(ops->owner)) {\n\t\tret = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tiommu_dev = ops->probe_device(dev);\n\tif (IS_ERR(iommu_dev)) {\n\t\tret = PTR_ERR(iommu_dev);\n\t\tgoto err_module_put;\n\t}\n\n\tret = iommu_device_link(iommu_dev, dev);\n\tif (ret)\n\t\tgoto err_release;\n\n\tgroup = ops->device_group(dev);\n\tif (WARN_ON_ONCE(group == NULL))\n\t\tgroup = ERR_PTR(-EINVAL);\n\tif (IS_ERR(group)) {\n\t\tret = PTR_ERR(group);\n\t\tgoto err_unlink;\n\t}\n\tdev->iommu_group = group;\n\n\tdev->iommu->iommu_dev = iommu_dev;\n\tdev->iommu->max_pasids = dev_iommu_get_max_pasids(dev);\n\tif (ops->is_attach_deferred)\n\t\tdev->iommu->attach_deferred = ops->is_attach_deferred(dev);\n\treturn 0;\n\nerr_unlink:\n\tiommu_device_unlink(iommu_dev, dev);\nerr_release:\n\tif (ops->release_device)\n\t\tops->release_device(dev);\nerr_module_put:\n\tmodule_put(ops->owner);\nerr_free:\n\tdev_iommu_free(dev);\n\treturn ret;\n}\n\nstatic void iommu_deinit_device(struct device *dev)\n{\n\tstruct iommu_group *group = dev->iommu_group;\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\n\tlockdep_assert_held(&group->mutex);\n\n\tiommu_device_unlink(dev->iommu->iommu_dev, dev);\n\n\t \n\tif (ops->release_device)\n\t\tops->release_device(dev);\n\n\t \n\tif (list_empty(&group->devices)) {\n\t\tif (group->default_domain) {\n\t\t\tiommu_domain_free(group->default_domain);\n\t\t\tgroup->default_domain = NULL;\n\t\t}\n\t\tif (group->blocking_domain) {\n\t\t\tiommu_domain_free(group->blocking_domain);\n\t\t\tgroup->blocking_domain = NULL;\n\t\t}\n\t\tgroup->domain = NULL;\n\t}\n\n\t \n\tdev->iommu_group = NULL;\n\tmodule_put(ops->owner);\n\tdev_iommu_free(dev);\n}\n\nDEFINE_MUTEX(iommu_probe_device_lock);\n\nstatic int __iommu_probe_device(struct device *dev, struct list_head *group_list)\n{\n\tconst struct iommu_ops *ops = dev->bus->iommu_ops;\n\tstruct iommu_group *group;\n\tstruct group_device *gdev;\n\tint ret;\n\n\tif (!ops)\n\t\treturn -ENODEV;\n\t \n\tlockdep_assert_held(&iommu_probe_device_lock);\n\n\t \n\tif (dev->iommu_group)\n\t\treturn 0;\n\n\tret = iommu_init_device(dev, ops);\n\tif (ret)\n\t\treturn ret;\n\n\tgroup = dev->iommu_group;\n\tgdev = iommu_group_alloc_device(group, dev);\n\tmutex_lock(&group->mutex);\n\tif (IS_ERR(gdev)) {\n\t\tret = PTR_ERR(gdev);\n\t\tgoto err_put_group;\n\t}\n\n\t \n\tlist_add_tail(&gdev->list, &group->devices);\n\tWARN_ON(group->default_domain && !group->domain);\n\tif (group->default_domain)\n\t\tiommu_create_device_direct_mappings(group->default_domain, dev);\n\tif (group->domain) {\n\t\tret = __iommu_device_set_domain(group, dev, group->domain, 0);\n\t\tif (ret)\n\t\t\tgoto err_remove_gdev;\n\t} else if (!group->default_domain && !group_list) {\n\t\tret = iommu_setup_default_domain(group, 0);\n\t\tif (ret)\n\t\t\tgoto err_remove_gdev;\n\t} else if (!group->default_domain) {\n\t\t \n\t\tif (list_empty(&group->entry))\n\t\t\tlist_add_tail(&group->entry, group_list);\n\t}\n\tmutex_unlock(&group->mutex);\n\n\tif (dev_is_pci(dev))\n\t\tiommu_dma_set_pci_32bit_workaround(dev);\n\n\treturn 0;\n\nerr_remove_gdev:\n\tlist_del(&gdev->list);\n\t__iommu_group_free_device(group, gdev);\nerr_put_group:\n\tiommu_deinit_device(dev);\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n\n\treturn ret;\n}\n\nint iommu_probe_device(struct device *dev)\n{\n\tconst struct iommu_ops *ops;\n\tint ret;\n\n\tmutex_lock(&iommu_probe_device_lock);\n\tret = __iommu_probe_device(dev, NULL);\n\tmutex_unlock(&iommu_probe_device_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tops = dev_iommu_ops(dev);\n\tif (ops->probe_finalize)\n\t\tops->probe_finalize(dev);\n\n\treturn 0;\n}\n\nstatic void __iommu_group_free_device(struct iommu_group *group,\n\t\t\t\t      struct group_device *grp_dev)\n{\n\tstruct device *dev = grp_dev->dev;\n\n\tsysfs_remove_link(group->devices_kobj, grp_dev->name);\n\tsysfs_remove_link(&dev->kobj, \"iommu_group\");\n\n\ttrace_remove_device_from_group(group->id, dev);\n\n\t \n\tif (list_empty(&group->devices))\n\t\tWARN_ON(group->owner_cnt ||\n\t\t\tgroup->domain != group->default_domain);\n\n\tkfree(grp_dev->name);\n\tkfree(grp_dev);\n}\n\n \nstatic void __iommu_group_remove_device(struct device *dev)\n{\n\tstruct iommu_group *group = dev->iommu_group;\n\tstruct group_device *device;\n\n\tmutex_lock(&group->mutex);\n\tfor_each_group_device(group, device) {\n\t\tif (device->dev != dev)\n\t\t\tcontinue;\n\n\t\tlist_del(&device->list);\n\t\t__iommu_group_free_device(group, device);\n\t\tif (dev->iommu && dev->iommu->iommu_dev)\n\t\t\tiommu_deinit_device(dev);\n\t\telse\n\t\t\tdev->iommu_group = NULL;\n\t\tbreak;\n\t}\n\tmutex_unlock(&group->mutex);\n\n\t \n\tiommu_group_put(group);\n}\n\nstatic void iommu_release_device(struct device *dev)\n{\n\tstruct iommu_group *group = dev->iommu_group;\n\n\tif (group)\n\t\t__iommu_group_remove_device(dev);\n\n\t \n\tif (dev->iommu)\n\t\tdev_iommu_free(dev);\n}\n\nstatic int __init iommu_set_def_domain_type(char *str)\n{\n\tbool pt;\n\tint ret;\n\n\tret = kstrtobool(str, &pt);\n\tif (ret)\n\t\treturn ret;\n\n\tif (pt)\n\t\tiommu_set_default_passthrough(true);\n\telse\n\t\tiommu_set_default_translated(true);\n\n\treturn 0;\n}\nearly_param(\"iommu.passthrough\", iommu_set_def_domain_type);\n\nstatic int __init iommu_dma_setup(char *str)\n{\n\tint ret = kstrtobool(str, &iommu_dma_strict);\n\n\tif (!ret)\n\t\tiommu_cmd_line |= IOMMU_CMD_LINE_STRICT;\n\treturn ret;\n}\nearly_param(\"iommu.strict\", iommu_dma_setup);\n\nvoid iommu_set_dma_strict(void)\n{\n\tiommu_dma_strict = true;\n\tif (iommu_def_domain_type == IOMMU_DOMAIN_DMA_FQ)\n\t\tiommu_def_domain_type = IOMMU_DOMAIN_DMA;\n}\n\nstatic ssize_t iommu_group_attr_show(struct kobject *kobj,\n\t\t\t\t     struct attribute *__attr, char *buf)\n{\n\tstruct iommu_group_attribute *attr = to_iommu_group_attr(__attr);\n\tstruct iommu_group *group = to_iommu_group(kobj);\n\tssize_t ret = -EIO;\n\n\tif (attr->show)\n\t\tret = attr->show(group, buf);\n\treturn ret;\n}\n\nstatic ssize_t iommu_group_attr_store(struct kobject *kobj,\n\t\t\t\t      struct attribute *__attr,\n\t\t\t\t      const char *buf, size_t count)\n{\n\tstruct iommu_group_attribute *attr = to_iommu_group_attr(__attr);\n\tstruct iommu_group *group = to_iommu_group(kobj);\n\tssize_t ret = -EIO;\n\n\tif (attr->store)\n\t\tret = attr->store(group, buf, count);\n\treturn ret;\n}\n\nstatic const struct sysfs_ops iommu_group_sysfs_ops = {\n\t.show = iommu_group_attr_show,\n\t.store = iommu_group_attr_store,\n};\n\nstatic int iommu_group_create_file(struct iommu_group *group,\n\t\t\t\t   struct iommu_group_attribute *attr)\n{\n\treturn sysfs_create_file(&group->kobj, &attr->attr);\n}\n\nstatic void iommu_group_remove_file(struct iommu_group *group,\n\t\t\t\t    struct iommu_group_attribute *attr)\n{\n\tsysfs_remove_file(&group->kobj, &attr->attr);\n}\n\nstatic ssize_t iommu_group_show_name(struct iommu_group *group, char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\", group->name);\n}\n\n \nstatic int iommu_insert_resv_region(struct iommu_resv_region *new,\n\t\t\t\t    struct list_head *regions)\n{\n\tstruct iommu_resv_region *iter, *tmp, *nr, *top;\n\tLIST_HEAD(stack);\n\n\tnr = iommu_alloc_resv_region(new->start, new->length,\n\t\t\t\t     new->prot, new->type, GFP_KERNEL);\n\tif (!nr)\n\t\treturn -ENOMEM;\n\n\t \n\tlist_for_each_entry(iter, regions, list) {\n\t\tif (nr->start < iter->start ||\n\t\t    (nr->start == iter->start && nr->type <= iter->type))\n\t\t\tbreak;\n\t}\n\tlist_add_tail(&nr->list, &iter->list);\n\n\t \n\tlist_for_each_entry_safe(iter, tmp, regions, list) {\n\t\tphys_addr_t top_end, iter_end = iter->start + iter->length - 1;\n\n\t\t \n\t\tif (iter->type != new->type) {\n\t\t\tlist_move_tail(&iter->list, &stack);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry_reverse(top, &stack, list)\n\t\t\tif (top->type == iter->type)\n\t\t\t\tgoto check_overlap;\n\n\t\tlist_move_tail(&iter->list, &stack);\n\t\tcontinue;\n\ncheck_overlap:\n\t\ttop_end = top->start + top->length - 1;\n\n\t\tif (iter->start > top_end + 1) {\n\t\t\tlist_move_tail(&iter->list, &stack);\n\t\t} else {\n\t\t\ttop->length = max(top_end, iter_end) - top->start + 1;\n\t\t\tlist_del(&iter->list);\n\t\t\tkfree(iter);\n\t\t}\n\t}\n\tlist_splice(&stack, regions);\n\treturn 0;\n}\n\nstatic int\niommu_insert_device_resv_regions(struct list_head *dev_resv_regions,\n\t\t\t\t struct list_head *group_resv_regions)\n{\n\tstruct iommu_resv_region *entry;\n\tint ret = 0;\n\n\tlist_for_each_entry(entry, dev_resv_regions, list) {\n\t\tret = iommu_insert_resv_region(entry, group_resv_regions);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nint iommu_get_group_resv_regions(struct iommu_group *group,\n\t\t\t\t struct list_head *head)\n{\n\tstruct group_device *device;\n\tint ret = 0;\n\n\tmutex_lock(&group->mutex);\n\tfor_each_group_device(group, device) {\n\t\tstruct list_head dev_resv_regions;\n\n\t\t \n\t\tif (!device->dev->iommu)\n\t\t\tbreak;\n\n\t\tINIT_LIST_HEAD(&dev_resv_regions);\n\t\tiommu_get_resv_regions(device->dev, &dev_resv_regions);\n\t\tret = iommu_insert_device_resv_regions(&dev_resv_regions, head);\n\t\tiommu_put_resv_regions(device->dev, &dev_resv_regions);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&group->mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_get_group_resv_regions);\n\nstatic ssize_t iommu_group_show_resv_regions(struct iommu_group *group,\n\t\t\t\t\t     char *buf)\n{\n\tstruct iommu_resv_region *region, *next;\n\tstruct list_head group_resv_regions;\n\tint offset = 0;\n\n\tINIT_LIST_HEAD(&group_resv_regions);\n\tiommu_get_group_resv_regions(group, &group_resv_regions);\n\n\tlist_for_each_entry_safe(region, next, &group_resv_regions, list) {\n\t\toffset += sysfs_emit_at(buf, offset, \"0x%016llx 0x%016llx %s\\n\",\n\t\t\t\t\t(long long)region->start,\n\t\t\t\t\t(long long)(region->start +\n\t\t\t\t\t\t    region->length - 1),\n\t\t\t\t\tiommu_group_resv_type_string[region->type]);\n\t\tkfree(region);\n\t}\n\n\treturn offset;\n}\n\nstatic ssize_t iommu_group_show_type(struct iommu_group *group,\n\t\t\t\t     char *buf)\n{\n\tchar *type = \"unknown\";\n\n\tmutex_lock(&group->mutex);\n\tif (group->default_domain) {\n\t\tswitch (group->default_domain->type) {\n\t\tcase IOMMU_DOMAIN_BLOCKED:\n\t\t\ttype = \"blocked\";\n\t\t\tbreak;\n\t\tcase IOMMU_DOMAIN_IDENTITY:\n\t\t\ttype = \"identity\";\n\t\t\tbreak;\n\t\tcase IOMMU_DOMAIN_UNMANAGED:\n\t\t\ttype = \"unmanaged\";\n\t\t\tbreak;\n\t\tcase IOMMU_DOMAIN_DMA:\n\t\t\ttype = \"DMA\";\n\t\t\tbreak;\n\t\tcase IOMMU_DOMAIN_DMA_FQ:\n\t\t\ttype = \"DMA-FQ\";\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&group->mutex);\n\n\treturn sysfs_emit(buf, \"%s\\n\", type);\n}\n\nstatic IOMMU_GROUP_ATTR(name, S_IRUGO, iommu_group_show_name, NULL);\n\nstatic IOMMU_GROUP_ATTR(reserved_regions, 0444,\n\t\t\tiommu_group_show_resv_regions, NULL);\n\nstatic IOMMU_GROUP_ATTR(type, 0644, iommu_group_show_type,\n\t\t\tiommu_group_store_type);\n\nstatic void iommu_group_release(struct kobject *kobj)\n{\n\tstruct iommu_group *group = to_iommu_group(kobj);\n\n\tpr_debug(\"Releasing group %d\\n\", group->id);\n\n\tif (group->iommu_data_release)\n\t\tgroup->iommu_data_release(group->iommu_data);\n\n\tida_free(&iommu_group_ida, group->id);\n\n\t \n\tWARN_ON(group->default_domain);\n\tWARN_ON(group->blocking_domain);\n\n\tkfree(group->name);\n\tkfree(group);\n}\n\nstatic const struct kobj_type iommu_group_ktype = {\n\t.sysfs_ops = &iommu_group_sysfs_ops,\n\t.release = iommu_group_release,\n};\n\n \nstruct iommu_group *iommu_group_alloc(void)\n{\n\tstruct iommu_group *group;\n\tint ret;\n\n\tgroup = kzalloc(sizeof(*group), GFP_KERNEL);\n\tif (!group)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tgroup->kobj.kset = iommu_group_kset;\n\tmutex_init(&group->mutex);\n\tINIT_LIST_HEAD(&group->devices);\n\tINIT_LIST_HEAD(&group->entry);\n\txa_init(&group->pasid_array);\n\n\tret = ida_alloc(&iommu_group_ida, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tkfree(group);\n\t\treturn ERR_PTR(ret);\n\t}\n\tgroup->id = ret;\n\n\tret = kobject_init_and_add(&group->kobj, &iommu_group_ktype,\n\t\t\t\t   NULL, \"%d\", group->id);\n\tif (ret) {\n\t\tkobject_put(&group->kobj);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tgroup->devices_kobj = kobject_create_and_add(\"devices\", &group->kobj);\n\tif (!group->devices_kobj) {\n\t\tkobject_put(&group->kobj);  \n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\tkobject_put(&group->kobj);\n\n\tret = iommu_group_create_file(group,\n\t\t\t\t      &iommu_group_attr_reserved_regions);\n\tif (ret) {\n\t\tkobject_put(group->devices_kobj);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = iommu_group_create_file(group, &iommu_group_attr_type);\n\tif (ret) {\n\t\tkobject_put(group->devices_kobj);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tpr_debug(\"Allocated group %d\\n\", group->id);\n\n\treturn group;\n}\nEXPORT_SYMBOL_GPL(iommu_group_alloc);\n\n \nvoid *iommu_group_get_iommudata(struct iommu_group *group)\n{\n\treturn group->iommu_data;\n}\nEXPORT_SYMBOL_GPL(iommu_group_get_iommudata);\n\n \nvoid iommu_group_set_iommudata(struct iommu_group *group, void *iommu_data,\n\t\t\t       void (*release)(void *iommu_data))\n{\n\tgroup->iommu_data = iommu_data;\n\tgroup->iommu_data_release = release;\n}\nEXPORT_SYMBOL_GPL(iommu_group_set_iommudata);\n\n \nint iommu_group_set_name(struct iommu_group *group, const char *name)\n{\n\tint ret;\n\n\tif (group->name) {\n\t\tiommu_group_remove_file(group, &iommu_group_attr_name);\n\t\tkfree(group->name);\n\t\tgroup->name = NULL;\n\t\tif (!name)\n\t\t\treturn 0;\n\t}\n\n\tgroup->name = kstrdup(name, GFP_KERNEL);\n\tif (!group->name)\n\t\treturn -ENOMEM;\n\n\tret = iommu_group_create_file(group, &iommu_group_attr_name);\n\tif (ret) {\n\t\tkfree(group->name);\n\t\tgroup->name = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iommu_group_set_name);\n\nstatic int iommu_create_device_direct_mappings(struct iommu_domain *domain,\n\t\t\t\t\t       struct device *dev)\n{\n\tstruct iommu_resv_region *entry;\n\tstruct list_head mappings;\n\tunsigned long pg_size;\n\tint ret = 0;\n\n\tpg_size = domain->pgsize_bitmap ? 1UL << __ffs(domain->pgsize_bitmap) : 0;\n\tINIT_LIST_HEAD(&mappings);\n\n\tif (WARN_ON_ONCE(iommu_is_dma_domain(domain) && !pg_size))\n\t\treturn -EINVAL;\n\n\tiommu_get_resv_regions(dev, &mappings);\n\n\t \n\tlist_for_each_entry(entry, &mappings, list) {\n\t\tdma_addr_t start, end, addr;\n\t\tsize_t map_size = 0;\n\n\t\tif (entry->type == IOMMU_RESV_DIRECT)\n\t\t\tdev->iommu->require_direct = 1;\n\n\t\tif ((entry->type != IOMMU_RESV_DIRECT &&\n\t\t     entry->type != IOMMU_RESV_DIRECT_RELAXABLE) ||\n\t\t    !iommu_is_dma_domain(domain))\n\t\t\tcontinue;\n\n\t\tstart = ALIGN(entry->start, pg_size);\n\t\tend   = ALIGN(entry->start + entry->length, pg_size);\n\n\t\tfor (addr = start; addr <= end; addr += pg_size) {\n\t\t\tphys_addr_t phys_addr;\n\n\t\t\tif (addr == end)\n\t\t\t\tgoto map_end;\n\n\t\t\tphys_addr = iommu_iova_to_phys(domain, addr);\n\t\t\tif (!phys_addr) {\n\t\t\t\tmap_size += pg_size;\n\t\t\t\tcontinue;\n\t\t\t}\n\nmap_end:\n\t\t\tif (map_size) {\n\t\t\t\tret = iommu_map(domain, addr - map_size,\n\t\t\t\t\t\taddr - map_size, map_size,\n\t\t\t\t\t\tentry->prot, GFP_KERNEL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tmap_size = 0;\n\t\t\t}\n\t\t}\n\n\t}\n\n\tif (!list_empty(&mappings) && iommu_is_dma_domain(domain))\n\t\tiommu_flush_iotlb_all(domain);\n\nout:\n\tiommu_put_resv_regions(dev, &mappings);\n\n\treturn ret;\n}\n\n \nstatic struct group_device *iommu_group_alloc_device(struct iommu_group *group,\n\t\t\t\t\t\t     struct device *dev)\n{\n\tint ret, i = 0;\n\tstruct group_device *device;\n\n\tdevice = kzalloc(sizeof(*device), GFP_KERNEL);\n\tif (!device)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdevice->dev = dev;\n\n\tret = sysfs_create_link(&dev->kobj, &group->kobj, \"iommu_group\");\n\tif (ret)\n\t\tgoto err_free_device;\n\n\tdevice->name = kasprintf(GFP_KERNEL, \"%s\", kobject_name(&dev->kobj));\nrename:\n\tif (!device->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_remove_link;\n\t}\n\n\tret = sysfs_create_link_nowarn(group->devices_kobj,\n\t\t\t\t       &dev->kobj, device->name);\n\tif (ret) {\n\t\tif (ret == -EEXIST && i >= 0) {\n\t\t\t \n\t\t\tkfree(device->name);\n\t\t\tdevice->name = kasprintf(GFP_KERNEL, \"%s.%d\",\n\t\t\t\t\t\t kobject_name(&dev->kobj), i++);\n\t\t\tgoto rename;\n\t\t}\n\t\tgoto err_free_name;\n\t}\n\n\ttrace_add_device_to_group(group->id, dev);\n\n\tdev_info(dev, \"Adding to iommu group %d\\n\", group->id);\n\n\treturn device;\n\nerr_free_name:\n\tkfree(device->name);\nerr_remove_link:\n\tsysfs_remove_link(&dev->kobj, \"iommu_group\");\nerr_free_device:\n\tkfree(device);\n\tdev_err(dev, \"Failed to add to iommu group %d: %d\\n\", group->id, ret);\n\treturn ERR_PTR(ret);\n}\n\n \nint iommu_group_add_device(struct iommu_group *group, struct device *dev)\n{\n\tstruct group_device *gdev;\n\n\tgdev = iommu_group_alloc_device(group, dev);\n\tif (IS_ERR(gdev))\n\t\treturn PTR_ERR(gdev);\n\n\tiommu_group_ref_get(group);\n\tdev->iommu_group = group;\n\n\tmutex_lock(&group->mutex);\n\tlist_add_tail(&gdev->list, &group->devices);\n\tmutex_unlock(&group->mutex);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iommu_group_add_device);\n\n \nvoid iommu_group_remove_device(struct device *dev)\n{\n\tstruct iommu_group *group = dev->iommu_group;\n\n\tif (!group)\n\t\treturn;\n\n\tdev_info(dev, \"Removing from iommu group %d\\n\", group->id);\n\n\t__iommu_group_remove_device(dev);\n}\nEXPORT_SYMBOL_GPL(iommu_group_remove_device);\n\n \nint iommu_group_for_each_dev(struct iommu_group *group, void *data,\n\t\t\t     int (*fn)(struct device *, void *))\n{\n\tstruct group_device *device;\n\tint ret = 0;\n\n\tmutex_lock(&group->mutex);\n\tfor_each_group_device(group, device) {\n\t\tret = fn(device->dev, data);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&group->mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_group_for_each_dev);\n\n \nstruct iommu_group *iommu_group_get(struct device *dev)\n{\n\tstruct iommu_group *group = dev->iommu_group;\n\n\tif (group)\n\t\tkobject_get(group->devices_kobj);\n\n\treturn group;\n}\nEXPORT_SYMBOL_GPL(iommu_group_get);\n\n \nstruct iommu_group *iommu_group_ref_get(struct iommu_group *group)\n{\n\tkobject_get(group->devices_kobj);\n\treturn group;\n}\nEXPORT_SYMBOL_GPL(iommu_group_ref_get);\n\n \nvoid iommu_group_put(struct iommu_group *group)\n{\n\tif (group)\n\t\tkobject_put(group->devices_kobj);\n}\nEXPORT_SYMBOL_GPL(iommu_group_put);\n\n \nint iommu_register_device_fault_handler(struct device *dev,\n\t\t\t\t\tiommu_dev_fault_handler_t handler,\n\t\t\t\t\tvoid *data)\n{\n\tstruct dev_iommu *param = dev->iommu;\n\tint ret = 0;\n\n\tif (!param)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&param->lock);\n\t \n\tif (param->fault_param) {\n\t\tret = -EBUSY;\n\t\tgoto done_unlock;\n\t}\n\n\tget_device(dev);\n\tparam->fault_param = kzalloc(sizeof(*param->fault_param), GFP_KERNEL);\n\tif (!param->fault_param) {\n\t\tput_device(dev);\n\t\tret = -ENOMEM;\n\t\tgoto done_unlock;\n\t}\n\tparam->fault_param->handler = handler;\n\tparam->fault_param->data = data;\n\tmutex_init(&param->fault_param->lock);\n\tINIT_LIST_HEAD(&param->fault_param->faults);\n\ndone_unlock:\n\tmutex_unlock(&param->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_register_device_fault_handler);\n\n \nint iommu_unregister_device_fault_handler(struct device *dev)\n{\n\tstruct dev_iommu *param = dev->iommu;\n\tint ret = 0;\n\n\tif (!param)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&param->lock);\n\n\tif (!param->fault_param)\n\t\tgoto unlock;\n\n\t \n\tif (!list_empty(&param->fault_param->faults)) {\n\t\tret = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\tkfree(param->fault_param);\n\tparam->fault_param = NULL;\n\tput_device(dev);\nunlock:\n\tmutex_unlock(&param->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_unregister_device_fault_handler);\n\n \nint iommu_report_device_fault(struct device *dev, struct iommu_fault_event *evt)\n{\n\tstruct dev_iommu *param = dev->iommu;\n\tstruct iommu_fault_event *evt_pending = NULL;\n\tstruct iommu_fault_param *fparam;\n\tint ret = 0;\n\n\tif (!param || !evt)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&param->lock);\n\tfparam = param->fault_param;\n\tif (!fparam || !fparam->handler) {\n\t\tret = -EINVAL;\n\t\tgoto done_unlock;\n\t}\n\n\tif (evt->fault.type == IOMMU_FAULT_PAGE_REQ &&\n\t    (evt->fault.prm.flags & IOMMU_FAULT_PAGE_REQUEST_LAST_PAGE)) {\n\t\tevt_pending = kmemdup(evt, sizeof(struct iommu_fault_event),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!evt_pending) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto done_unlock;\n\t\t}\n\t\tmutex_lock(&fparam->lock);\n\t\tlist_add_tail(&evt_pending->list, &fparam->faults);\n\t\tmutex_unlock(&fparam->lock);\n\t}\n\n\tret = fparam->handler(&evt->fault, fparam->data);\n\tif (ret && evt_pending) {\n\t\tmutex_lock(&fparam->lock);\n\t\tlist_del(&evt_pending->list);\n\t\tmutex_unlock(&fparam->lock);\n\t\tkfree(evt_pending);\n\t}\ndone_unlock:\n\tmutex_unlock(&param->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_report_device_fault);\n\nint iommu_page_response(struct device *dev,\n\t\t\tstruct iommu_page_response *msg)\n{\n\tbool needs_pasid;\n\tint ret = -EINVAL;\n\tstruct iommu_fault_event *evt;\n\tstruct iommu_fault_page_request *prm;\n\tstruct dev_iommu *param = dev->iommu;\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\tbool has_pasid = msg->flags & IOMMU_PAGE_RESP_PASID_VALID;\n\n\tif (!ops->page_response)\n\t\treturn -ENODEV;\n\n\tif (!param || !param->fault_param)\n\t\treturn -EINVAL;\n\n\tif (msg->version != IOMMU_PAGE_RESP_VERSION_1 ||\n\t    msg->flags & ~IOMMU_PAGE_RESP_PASID_VALID)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&param->fault_param->lock);\n\tif (list_empty(&param->fault_param->faults)) {\n\t\tdev_warn_ratelimited(dev, \"no pending PRQ, drop response\\n\");\n\t\tgoto done_unlock;\n\t}\n\t \n\tlist_for_each_entry(evt, &param->fault_param->faults, list) {\n\t\tprm = &evt->fault.prm;\n\t\tif (prm->grpid != msg->grpid)\n\t\t\tcontinue;\n\n\t\t \n\t\tneeds_pasid = prm->flags & IOMMU_FAULT_PAGE_RESPONSE_NEEDS_PASID;\n\t\tif (needs_pasid && (!has_pasid || msg->pasid != prm->pasid))\n\t\t\tcontinue;\n\n\t\tif (!needs_pasid && has_pasid) {\n\t\t\t \n\t\t\tmsg->flags &= ~IOMMU_PAGE_RESP_PASID_VALID;\n\t\t\tmsg->pasid = 0;\n\t\t}\n\n\t\tret = ops->page_response(dev, evt, msg);\n\t\tlist_del(&evt->list);\n\t\tkfree(evt);\n\t\tbreak;\n\t}\n\ndone_unlock:\n\tmutex_unlock(&param->fault_param->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_page_response);\n\n \nint iommu_group_id(struct iommu_group *group)\n{\n\treturn group->id;\n}\nEXPORT_SYMBOL_GPL(iommu_group_id);\n\nstatic struct iommu_group *get_pci_alias_group(struct pci_dev *pdev,\n\t\t\t\t\t       unsigned long *devfns);\n\n \n#define REQ_ACS_FLAGS   (PCI_ACS_SV | PCI_ACS_RR | PCI_ACS_CR | PCI_ACS_UF)\n\n \nstatic struct iommu_group *get_pci_function_alias_group(struct pci_dev *pdev,\n\t\t\t\t\t\t\tunsigned long *devfns)\n{\n\tstruct pci_dev *tmp = NULL;\n\tstruct iommu_group *group;\n\n\tif (!pdev->multifunction || pci_acs_enabled(pdev, REQ_ACS_FLAGS))\n\t\treturn NULL;\n\n\tfor_each_pci_dev(tmp) {\n\t\tif (tmp == pdev || tmp->bus != pdev->bus ||\n\t\t    PCI_SLOT(tmp->devfn) != PCI_SLOT(pdev->devfn) ||\n\t\t    pci_acs_enabled(tmp, REQ_ACS_FLAGS))\n\t\t\tcontinue;\n\n\t\tgroup = get_pci_alias_group(tmp, devfns);\n\t\tif (group) {\n\t\t\tpci_dev_put(tmp);\n\t\t\treturn group;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct iommu_group *get_pci_alias_group(struct pci_dev *pdev,\n\t\t\t\t\t       unsigned long *devfns)\n{\n\tstruct pci_dev *tmp = NULL;\n\tstruct iommu_group *group;\n\n\tif (test_and_set_bit(pdev->devfn & 0xff, devfns))\n\t\treturn NULL;\n\n\tgroup = iommu_group_get(&pdev->dev);\n\tif (group)\n\t\treturn group;\n\n\tfor_each_pci_dev(tmp) {\n\t\tif (tmp == pdev || tmp->bus != pdev->bus)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (pci_devs_are_dma_aliases(pdev, tmp)) {\n\t\t\tgroup = get_pci_alias_group(tmp, devfns);\n\t\t\tif (group) {\n\t\t\t\tpci_dev_put(tmp);\n\t\t\t\treturn group;\n\t\t\t}\n\n\t\t\tgroup = get_pci_function_alias_group(tmp, devfns);\n\t\t\tif (group) {\n\t\t\t\tpci_dev_put(tmp);\n\t\t\t\treturn group;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstruct group_for_pci_data {\n\tstruct pci_dev *pdev;\n\tstruct iommu_group *group;\n};\n\n \nstatic int get_pci_alias_or_group(struct pci_dev *pdev, u16 alias, void *opaque)\n{\n\tstruct group_for_pci_data *data = opaque;\n\n\tdata->pdev = pdev;\n\tdata->group = iommu_group_get(&pdev->dev);\n\n\treturn data->group != NULL;\n}\n\n \nstruct iommu_group *generic_device_group(struct device *dev)\n{\n\treturn iommu_group_alloc();\n}\nEXPORT_SYMBOL_GPL(generic_device_group);\n\n \nstruct iommu_group *pci_device_group(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct group_for_pci_data data;\n\tstruct pci_bus *bus;\n\tstruct iommu_group *group = NULL;\n\tu64 devfns[4] = { 0 };\n\n\tif (WARN_ON(!dev_is_pci(dev)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (pci_for_each_dma_alias(pdev, get_pci_alias_or_group, &data))\n\t\treturn data.group;\n\n\tpdev = data.pdev;\n\n\t \n\tfor (bus = pdev->bus; !pci_is_root_bus(bus); bus = bus->parent) {\n\t\tif (!bus->self)\n\t\t\tcontinue;\n\n\t\tif (pci_acs_path_enabled(bus->self, NULL, REQ_ACS_FLAGS))\n\t\t\tbreak;\n\n\t\tpdev = bus->self;\n\n\t\tgroup = iommu_group_get(&pdev->dev);\n\t\tif (group)\n\t\t\treturn group;\n\t}\n\n\t \n\tgroup = get_pci_alias_group(pdev, (unsigned long *)devfns);\n\tif (group)\n\t\treturn group;\n\n\t \n\tgroup = get_pci_function_alias_group(pdev, (unsigned long *)devfns);\n\tif (group)\n\t\treturn group;\n\n\t \n\treturn iommu_group_alloc();\n}\nEXPORT_SYMBOL_GPL(pci_device_group);\n\n \nstruct iommu_group *fsl_mc_device_group(struct device *dev)\n{\n\tstruct device *cont_dev = fsl_mc_cont_dev(dev);\n\tstruct iommu_group *group;\n\n\tgroup = iommu_group_get(cont_dev);\n\tif (!group)\n\t\tgroup = iommu_group_alloc();\n\treturn group;\n}\nEXPORT_SYMBOL_GPL(fsl_mc_device_group);\n\nstatic int iommu_get_def_domain_type(struct device *dev)\n{\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\n\tif (dev_is_pci(dev) && to_pci_dev(dev)->untrusted)\n\t\treturn IOMMU_DOMAIN_DMA;\n\n\tif (ops->def_domain_type)\n\t\treturn ops->def_domain_type(dev);\n\n\treturn 0;\n}\n\nstatic struct iommu_domain *\n__iommu_group_alloc_default_domain(const struct bus_type *bus,\n\t\t\t\t   struct iommu_group *group, int req_type)\n{\n\tif (group->default_domain && group->default_domain->type == req_type)\n\t\treturn group->default_domain;\n\treturn __iommu_domain_alloc(bus, req_type);\n}\n\n \nstatic struct iommu_domain *\niommu_group_alloc_default_domain(struct iommu_group *group, int req_type)\n{\n\tconst struct bus_type *bus =\n\t\tlist_first_entry(&group->devices, struct group_device, list)\n\t\t\t->dev->bus;\n\tstruct iommu_domain *dom;\n\n\tlockdep_assert_held(&group->mutex);\n\n\tif (req_type)\n\t\treturn __iommu_group_alloc_default_domain(bus, group, req_type);\n\n\t \n\tdom = __iommu_group_alloc_default_domain(bus, group, iommu_def_domain_type);\n\tif (dom)\n\t\treturn dom;\n\n\t \n\tif (iommu_def_domain_type == IOMMU_DOMAIN_DMA)\n\t\treturn NULL;\n\tdom = __iommu_group_alloc_default_domain(bus, group, IOMMU_DOMAIN_DMA);\n\tif (!dom)\n\t\treturn NULL;\n\n\tpr_warn(\"Failed to allocate default IOMMU domain of type %u for group %s - Falling back to IOMMU_DOMAIN_DMA\",\n\t\tiommu_def_domain_type, group->name);\n\treturn dom;\n}\n\nstruct iommu_domain *iommu_group_default_domain(struct iommu_group *group)\n{\n\treturn group->default_domain;\n}\n\nstatic int probe_iommu_group(struct device *dev, void *data)\n{\n\tstruct list_head *group_list = data;\n\tint ret;\n\n\tmutex_lock(&iommu_probe_device_lock);\n\tret = __iommu_probe_device(dev, group_list);\n\tmutex_unlock(&iommu_probe_device_lock);\n\tif (ret == -ENODEV)\n\t\tret = 0;\n\n\treturn ret;\n}\n\nstatic int iommu_bus_notifier(struct notifier_block *nb,\n\t\t\t      unsigned long action, void *data)\n{\n\tstruct device *dev = data;\n\n\tif (action == BUS_NOTIFY_ADD_DEVICE) {\n\t\tint ret;\n\n\t\tret = iommu_probe_device(dev);\n\t\treturn (ret) ? NOTIFY_DONE : NOTIFY_OK;\n\t} else if (action == BUS_NOTIFY_REMOVED_DEVICE) {\n\t\tiommu_release_device(dev);\n\t\treturn NOTIFY_OK;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int iommu_get_default_domain_type(struct iommu_group *group,\n\t\t\t\t\t int target_type)\n{\n\tint best_type = target_type;\n\tstruct group_device *gdev;\n\tstruct device *last_dev;\n\n\tlockdep_assert_held(&group->mutex);\n\n\tfor_each_group_device(group, gdev) {\n\t\tunsigned int type = iommu_get_def_domain_type(gdev->dev);\n\n\t\tif (best_type && type && best_type != type) {\n\t\t\tif (target_type) {\n\t\t\t\tdev_err_ratelimited(\n\t\t\t\t\tgdev->dev,\n\t\t\t\t\t\"Device cannot be in %s domain\\n\",\n\t\t\t\t\tiommu_domain_type_str(target_type));\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tdev_warn(\n\t\t\t\tgdev->dev,\n\t\t\t\t\"Device needs domain type %s, but device %s in the same iommu group requires type %s - using default\\n\",\n\t\t\t\tiommu_domain_type_str(type), dev_name(last_dev),\n\t\t\t\tiommu_domain_type_str(best_type));\n\t\t\treturn 0;\n\t\t}\n\t\tif (!best_type)\n\t\t\tbest_type = type;\n\t\tlast_dev = gdev->dev;\n\t}\n\treturn best_type;\n}\n\nstatic void iommu_group_do_probe_finalize(struct device *dev)\n{\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\n\tif (ops->probe_finalize)\n\t\tops->probe_finalize(dev);\n}\n\nint bus_iommu_probe(const struct bus_type *bus)\n{\n\tstruct iommu_group *group, *next;\n\tLIST_HEAD(group_list);\n\tint ret;\n\n\tret = bus_for_each_dev(bus, NULL, &group_list, probe_iommu_group);\n\tif (ret)\n\t\treturn ret;\n\n\tlist_for_each_entry_safe(group, next, &group_list, entry) {\n\t\tstruct group_device *gdev;\n\n\t\tmutex_lock(&group->mutex);\n\n\t\t \n\t\tlist_del_init(&group->entry);\n\n\t\t \n\t\tret = iommu_setup_default_domain(group, 0);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&group->mutex);\n\t\t\treturn ret;\n\t\t}\n\t\tmutex_unlock(&group->mutex);\n\n\t\t \n\t\tfor_each_group_device(group, gdev)\n\t\t\tiommu_group_do_probe_finalize(gdev->dev);\n\t}\n\n\treturn 0;\n}\n\nbool iommu_present(const struct bus_type *bus)\n{\n\treturn bus->iommu_ops != NULL;\n}\nEXPORT_SYMBOL_GPL(iommu_present);\n\n \nbool device_iommu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tconst struct iommu_ops *ops;\n\n\tif (!dev->iommu || !dev->iommu->iommu_dev)\n\t\treturn false;\n\n\tops = dev_iommu_ops(dev);\n\tif (!ops->capable)\n\t\treturn false;\n\n\treturn ops->capable(dev, cap);\n}\nEXPORT_SYMBOL_GPL(device_iommu_capable);\n\n \nbool iommu_group_has_isolated_msi(struct iommu_group *group)\n{\n\tstruct group_device *group_dev;\n\tbool ret = true;\n\n\tmutex_lock(&group->mutex);\n\tfor_each_group_device(group, group_dev)\n\t\tret &= msi_device_has_isolated_msi(group_dev->dev);\n\tmutex_unlock(&group->mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_group_has_isolated_msi);\n\n \nvoid iommu_set_fault_handler(struct iommu_domain *domain,\n\t\t\t\t\tiommu_fault_handler_t handler,\n\t\t\t\t\tvoid *token)\n{\n\tBUG_ON(!domain);\n\n\tdomain->handler = handler;\n\tdomain->handler_token = token;\n}\nEXPORT_SYMBOL_GPL(iommu_set_fault_handler);\n\nstatic struct iommu_domain *__iommu_domain_alloc(const struct bus_type *bus,\n\t\t\t\t\t\t unsigned type)\n{\n\tstruct iommu_domain *domain;\n\tunsigned int alloc_type = type & IOMMU_DOMAIN_ALLOC_FLAGS;\n\n\tif (bus == NULL || bus->iommu_ops == NULL)\n\t\treturn NULL;\n\n\tdomain = bus->iommu_ops->domain_alloc(alloc_type);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->type = type;\n\t \n\tif (!domain->pgsize_bitmap)\n\t\tdomain->pgsize_bitmap = bus->iommu_ops->pgsize_bitmap;\n\n\tif (!domain->ops)\n\t\tdomain->ops = bus->iommu_ops->default_domain_ops;\n\n\tif (iommu_is_dma_domain(domain) && iommu_get_dma_cookie(domain)) {\n\t\tiommu_domain_free(domain);\n\t\tdomain = NULL;\n\t}\n\treturn domain;\n}\n\nstruct iommu_domain *iommu_domain_alloc(const struct bus_type *bus)\n{\n\treturn __iommu_domain_alloc(bus, IOMMU_DOMAIN_UNMANAGED);\n}\nEXPORT_SYMBOL_GPL(iommu_domain_alloc);\n\nvoid iommu_domain_free(struct iommu_domain *domain)\n{\n\tif (domain->type == IOMMU_DOMAIN_SVA)\n\t\tmmdrop(domain->mm);\n\tiommu_put_dma_cookie(domain);\n\tdomain->ops->free(domain);\n}\nEXPORT_SYMBOL_GPL(iommu_domain_free);\n\n \nstatic void __iommu_group_set_core_domain(struct iommu_group *group)\n{\n\tstruct iommu_domain *new_domain;\n\n\tif (group->owner)\n\t\tnew_domain = group->blocking_domain;\n\telse\n\t\tnew_domain = group->default_domain;\n\n\t__iommu_group_set_domain_nofail(group, new_domain);\n}\n\nstatic int __iommu_attach_device(struct iommu_domain *domain,\n\t\t\t\t struct device *dev)\n{\n\tint ret;\n\n\tif (unlikely(domain->ops->attach_dev == NULL))\n\t\treturn -ENODEV;\n\n\tret = domain->ops->attach_dev(domain, dev);\n\tif (ret)\n\t\treturn ret;\n\tdev->iommu->attach_deferred = 0;\n\ttrace_attach_device_to_domain(dev);\n\treturn 0;\n}\n\n \nint iommu_attach_device(struct iommu_domain *domain, struct device *dev)\n{\n\tstruct iommu_group *group;\n\tint ret;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn -ENODEV;\n\n\t \n\tmutex_lock(&group->mutex);\n\tret = -EINVAL;\n\tif (list_count_nodes(&group->devices) != 1)\n\t\tgoto out_unlock;\n\n\tret = __iommu_attach_group(domain, group);\n\nout_unlock:\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_attach_device);\n\nint iommu_deferred_attach(struct device *dev, struct iommu_domain *domain)\n{\n\tif (dev->iommu && dev->iommu->attach_deferred)\n\t\treturn __iommu_attach_device(domain, dev);\n\n\treturn 0;\n}\n\nvoid iommu_detach_device(struct iommu_domain *domain, struct device *dev)\n{\n\tstruct iommu_group *group;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn;\n\n\tmutex_lock(&group->mutex);\n\tif (WARN_ON(domain != group->domain) ||\n\t    WARN_ON(list_count_nodes(&group->devices) != 1))\n\t\tgoto out_unlock;\n\t__iommu_group_set_core_domain(group);\n\nout_unlock:\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n}\nEXPORT_SYMBOL_GPL(iommu_detach_device);\n\nstruct iommu_domain *iommu_get_domain_for_dev(struct device *dev)\n{\n\tstruct iommu_domain *domain;\n\tstruct iommu_group *group;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn NULL;\n\n\tdomain = group->domain;\n\n\tiommu_group_put(group);\n\n\treturn domain;\n}\nEXPORT_SYMBOL_GPL(iommu_get_domain_for_dev);\n\n \nstruct iommu_domain *iommu_get_dma_domain(struct device *dev)\n{\n\treturn dev->iommu_group->default_domain;\n}\n\nstatic int __iommu_attach_group(struct iommu_domain *domain,\n\t\t\t\tstruct iommu_group *group)\n{\n\tif (group->domain && group->domain != group->default_domain &&\n\t    group->domain != group->blocking_domain)\n\t\treturn -EBUSY;\n\n\treturn __iommu_group_set_domain(group, domain);\n}\n\n \nint iommu_attach_group(struct iommu_domain *domain, struct iommu_group *group)\n{\n\tint ret;\n\n\tmutex_lock(&group->mutex);\n\tret = __iommu_attach_group(domain, group);\n\tmutex_unlock(&group->mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_attach_group);\n\n \nint iommu_group_replace_domain(struct iommu_group *group,\n\t\t\t       struct iommu_domain *new_domain)\n{\n\tint ret;\n\n\tif (!new_domain)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&group->mutex);\n\tret = __iommu_group_set_domain(group, new_domain);\n\tmutex_unlock(&group->mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_NS_GPL(iommu_group_replace_domain, IOMMUFD_INTERNAL);\n\nstatic int __iommu_device_set_domain(struct iommu_group *group,\n\t\t\t\t     struct device *dev,\n\t\t\t\t     struct iommu_domain *new_domain,\n\t\t\t\t     unsigned int flags)\n{\n\tint ret;\n\n\t \n\tif (dev->iommu->require_direct &&\n\t    (new_domain->type == IOMMU_DOMAIN_BLOCKED ||\n\t     new_domain == group->blocking_domain)) {\n\t\tdev_warn(dev,\n\t\t\t \"Firmware has requested this device have a 1:1 IOMMU mapping, rejecting configuring the device without a 1:1 mapping. Contact your platform vendor.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->iommu->attach_deferred) {\n\t\tif (new_domain == group->default_domain)\n\t\t\treturn 0;\n\t\tdev->iommu->attach_deferred = 0;\n\t}\n\n\tret = __iommu_attach_device(new_domain, dev);\n\tif (ret) {\n\t\t \n\t\tif ((flags & IOMMU_SET_DOMAIN_MUST_SUCCEED) &&\n\t\t    group->blocking_domain &&\n\t\t    group->blocking_domain != new_domain)\n\t\t\t__iommu_attach_device(group->blocking_domain, dev);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nstatic int __iommu_group_set_domain_internal(struct iommu_group *group,\n\t\t\t\t\t     struct iommu_domain *new_domain,\n\t\t\t\t\t     unsigned int flags)\n{\n\tstruct group_device *last_gdev;\n\tstruct group_device *gdev;\n\tint result;\n\tint ret;\n\n\tlockdep_assert_held(&group->mutex);\n\n\tif (group->domain == new_domain)\n\t\treturn 0;\n\n\t \n\tif (!new_domain) {\n\t\tfor_each_group_device(group, gdev) {\n\t\t\tconst struct iommu_ops *ops = dev_iommu_ops(gdev->dev);\n\n\t\t\tif (!WARN_ON(!ops->set_platform_dma_ops))\n\t\t\t\tops->set_platform_dma_ops(gdev->dev);\n\t\t}\n\t\tgroup->domain = NULL;\n\t\treturn 0;\n\t}\n\n\t \n\tresult = 0;\n\tfor_each_group_device(group, gdev) {\n\t\tret = __iommu_device_set_domain(group, gdev->dev, new_domain,\n\t\t\t\t\t\tflags);\n\t\tif (ret) {\n\t\t\tresult = ret;\n\t\t\t \n\t\t\tif (flags & IOMMU_SET_DOMAIN_MUST_SUCCEED)\n\t\t\t\tcontinue;\n\t\t\tgoto err_revert;\n\t\t}\n\t}\n\tgroup->domain = new_domain;\n\treturn result;\n\nerr_revert:\n\t \n\tlast_gdev = gdev;\n\tfor_each_group_device(group, gdev) {\n\t\tconst struct iommu_ops *ops = dev_iommu_ops(gdev->dev);\n\n\t\t \n\t\tif (group->domain)\n\t\t\tWARN_ON(__iommu_device_set_domain(\n\t\t\t\tgroup, gdev->dev, group->domain,\n\t\t\t\tIOMMU_SET_DOMAIN_MUST_SUCCEED));\n\t\telse if (ops->set_platform_dma_ops)\n\t\t\tops->set_platform_dma_ops(gdev->dev);\n\t\tif (gdev == last_gdev)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nvoid iommu_detach_group(struct iommu_domain *domain, struct iommu_group *group)\n{\n\tmutex_lock(&group->mutex);\n\t__iommu_group_set_core_domain(group);\n\tmutex_unlock(&group->mutex);\n}\nEXPORT_SYMBOL_GPL(iommu_detach_group);\n\nphys_addr_t iommu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova)\n{\n\tif (domain->type == IOMMU_DOMAIN_IDENTITY)\n\t\treturn iova;\n\n\tif (domain->type == IOMMU_DOMAIN_BLOCKED)\n\t\treturn 0;\n\n\treturn domain->ops->iova_to_phys(domain, iova);\n}\nEXPORT_SYMBOL_GPL(iommu_iova_to_phys);\n\nstatic size_t iommu_pgsize(struct iommu_domain *domain, unsigned long iova,\n\t\t\t   phys_addr_t paddr, size_t size, size_t *count)\n{\n\tunsigned int pgsize_idx, pgsize_idx_next;\n\tunsigned long pgsizes;\n\tsize_t offset, pgsize, pgsize_next;\n\tunsigned long addr_merge = paddr | iova;\n\n\t \n\tpgsizes = domain->pgsize_bitmap & GENMASK(__fls(size), 0);\n\n\t \n\tif (likely(addr_merge))\n\t\tpgsizes &= GENMASK(__ffs(addr_merge), 0);\n\n\t \n\tBUG_ON(!pgsizes);\n\n\t \n\tpgsize_idx = __fls(pgsizes);\n\tpgsize = BIT(pgsize_idx);\n\tif (!count)\n\t\treturn pgsize;\n\n\t \n\tpgsizes = domain->pgsize_bitmap & ~GENMASK(pgsize_idx, 0);\n\tif (!pgsizes)\n\t\tgoto out_set_count;\n\n\tpgsize_idx_next = __ffs(pgsizes);\n\tpgsize_next = BIT(pgsize_idx_next);\n\n\t \n\tif ((iova ^ paddr) & (pgsize_next - 1))\n\t\tgoto out_set_count;\n\n\t \n\toffset = pgsize_next - (addr_merge & (pgsize_next - 1));\n\n\t \n\tif (offset + pgsize_next <= size)\n\t\tsize = offset;\n\nout_set_count:\n\t*count = size >> pgsize_idx;\n\treturn pgsize;\n}\n\nstatic int __iommu_map_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t     phys_addr_t paddr, size_t size, int prot,\n\t\t\t     gfp_t gfp, size_t *mapped)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tsize_t pgsize, count;\n\tint ret;\n\n\tpgsize = iommu_pgsize(domain, iova, paddr, size, &count);\n\n\tpr_debug(\"mapping: iova 0x%lx pa %pa pgsize 0x%zx count %zu\\n\",\n\t\t iova, &paddr, pgsize, count);\n\n\tif (ops->map_pages) {\n\t\tret = ops->map_pages(domain, iova, paddr, pgsize, count, prot,\n\t\t\t\t     gfp, mapped);\n\t} else {\n\t\tret = ops->map(domain, iova, paddr, pgsize, prot, gfp);\n\t\t*mapped = ret ? 0 : pgsize;\n\t}\n\n\treturn ret;\n}\n\nstatic int __iommu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t       phys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tunsigned long orig_iova = iova;\n\tunsigned int min_pagesz;\n\tsize_t orig_size = size;\n\tphys_addr_t orig_paddr = paddr;\n\tint ret = 0;\n\n\tif (unlikely(!(ops->map || ops->map_pages) ||\n\t\t     domain->pgsize_bitmap == 0UL))\n\t\treturn -ENODEV;\n\n\tif (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))\n\t\treturn -EINVAL;\n\n\t \n\tmin_pagesz = 1 << __ffs(domain->pgsize_bitmap);\n\n\t \n\tif (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {\n\t\tpr_err(\"unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\\n\",\n\t\t       iova, &paddr, size, min_pagesz);\n\t\treturn -EINVAL;\n\t}\n\n\tpr_debug(\"map: iova 0x%lx pa %pa size 0x%zx\\n\", iova, &paddr, size);\n\n\twhile (size) {\n\t\tsize_t mapped = 0;\n\n\t\tret = __iommu_map_pages(domain, iova, paddr, size, prot, gfp,\n\t\t\t\t\t&mapped);\n\t\t \n\t\tsize -= mapped;\n\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tiova += mapped;\n\t\tpaddr += mapped;\n\t}\n\n\t \n\tif (ret)\n\t\tiommu_unmap(domain, orig_iova, orig_size - size);\n\telse\n\t\ttrace_map(orig_iova, orig_paddr, orig_size);\n\n\treturn ret;\n}\n\nint iommu_map(struct iommu_domain *domain, unsigned long iova,\n\t      phys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tint ret;\n\n\tmight_sleep_if(gfpflags_allow_blocking(gfp));\n\n\t \n\tif (WARN_ON_ONCE(gfp & (__GFP_COMP | __GFP_DMA | __GFP_DMA32 |\n\t\t\t\t__GFP_HIGHMEM)))\n\t\treturn -EINVAL;\n\n\tret = __iommu_map(domain, iova, paddr, size, prot, gfp);\n\tif (ret == 0 && ops->iotlb_sync_map)\n\t\tops->iotlb_sync_map(domain, iova, size);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_map);\n\nstatic size_t __iommu_unmap_pages(struct iommu_domain *domain,\n\t\t\t\t  unsigned long iova, size_t size,\n\t\t\t\t  struct iommu_iotlb_gather *iotlb_gather)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tsize_t pgsize, count;\n\n\tpgsize = iommu_pgsize(domain, iova, iova, size, &count);\n\treturn ops->unmap_pages ?\n\t       ops->unmap_pages(domain, iova, pgsize, count, iotlb_gather) :\n\t       ops->unmap(domain, iova, pgsize, iotlb_gather);\n}\n\nstatic size_t __iommu_unmap(struct iommu_domain *domain,\n\t\t\t    unsigned long iova, size_t size,\n\t\t\t    struct iommu_iotlb_gather *iotlb_gather)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tsize_t unmapped_page, unmapped = 0;\n\tunsigned long orig_iova = iova;\n\tunsigned int min_pagesz;\n\n\tif (unlikely(!(ops->unmap || ops->unmap_pages) ||\n\t\t     domain->pgsize_bitmap == 0UL))\n\t\treturn 0;\n\n\tif (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))\n\t\treturn 0;\n\n\t \n\tmin_pagesz = 1 << __ffs(domain->pgsize_bitmap);\n\n\t \n\tif (!IS_ALIGNED(iova | size, min_pagesz)) {\n\t\tpr_err(\"unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\\n\",\n\t\t       iova, size, min_pagesz);\n\t\treturn 0;\n\t}\n\n\tpr_debug(\"unmap this: iova 0x%lx size 0x%zx\\n\", iova, size);\n\n\t \n\twhile (unmapped < size) {\n\t\tunmapped_page = __iommu_unmap_pages(domain, iova,\n\t\t\t\t\t\t    size - unmapped,\n\t\t\t\t\t\t    iotlb_gather);\n\t\tif (!unmapped_page)\n\t\t\tbreak;\n\n\t\tpr_debug(\"unmapped: iova 0x%lx size 0x%zx\\n\",\n\t\t\t iova, unmapped_page);\n\n\t\tiova += unmapped_page;\n\t\tunmapped += unmapped_page;\n\t}\n\n\ttrace_unmap(orig_iova, size, unmapped);\n\treturn unmapped;\n}\n\nsize_t iommu_unmap(struct iommu_domain *domain,\n\t\t   unsigned long iova, size_t size)\n{\n\tstruct iommu_iotlb_gather iotlb_gather;\n\tsize_t ret;\n\n\tiommu_iotlb_gather_init(&iotlb_gather);\n\tret = __iommu_unmap(domain, iova, size, &iotlb_gather);\n\tiommu_iotlb_sync(domain, &iotlb_gather);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_unmap);\n\nsize_t iommu_unmap_fast(struct iommu_domain *domain,\n\t\t\tunsigned long iova, size_t size,\n\t\t\tstruct iommu_iotlb_gather *iotlb_gather)\n{\n\treturn __iommu_unmap(domain, iova, size, iotlb_gather);\n}\nEXPORT_SYMBOL_GPL(iommu_unmap_fast);\n\nssize_t iommu_map_sg(struct iommu_domain *domain, unsigned long iova,\n\t\t     struct scatterlist *sg, unsigned int nents, int prot,\n\t\t     gfp_t gfp)\n{\n\tconst struct iommu_domain_ops *ops = domain->ops;\n\tsize_t len = 0, mapped = 0;\n\tphys_addr_t start;\n\tunsigned int i = 0;\n\tint ret;\n\n\tmight_sleep_if(gfpflags_allow_blocking(gfp));\n\n\t \n\tif (WARN_ON_ONCE(gfp & (__GFP_COMP | __GFP_DMA | __GFP_DMA32 |\n\t\t\t\t__GFP_HIGHMEM)))\n\t\treturn -EINVAL;\n\n\twhile (i <= nents) {\n\t\tphys_addr_t s_phys = sg_phys(sg);\n\n\t\tif (len && s_phys != start + len) {\n\t\t\tret = __iommu_map(domain, iova + mapped, start,\n\t\t\t\t\tlen, prot, gfp);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out_err;\n\n\t\t\tmapped += len;\n\t\t\tlen = 0;\n\t\t}\n\n\t\tif (sg_dma_is_bus_address(sg))\n\t\t\tgoto next;\n\n\t\tif (len) {\n\t\t\tlen += sg->length;\n\t\t} else {\n\t\t\tlen = sg->length;\n\t\t\tstart = s_phys;\n\t\t}\n\nnext:\n\t\tif (++i < nents)\n\t\t\tsg = sg_next(sg);\n\t}\n\n\tif (ops->iotlb_sync_map)\n\t\tops->iotlb_sync_map(domain, iova, mapped);\n\treturn mapped;\n\nout_err:\n\t \n\tiommu_unmap(domain, iova, mapped);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_map_sg);\n\n \nint report_iommu_fault(struct iommu_domain *domain, struct device *dev,\n\t\t       unsigned long iova, int flags)\n{\n\tint ret = -ENOSYS;\n\n\t \n\tif (domain->handler)\n\t\tret = domain->handler(domain, dev, iova, flags,\n\t\t\t\t\t\tdomain->handler_token);\n\n\ttrace_io_page_fault(dev, iova, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(report_iommu_fault);\n\nstatic int __init iommu_init(void)\n{\n\tiommu_group_kset = kset_create_and_add(\"iommu_groups\",\n\t\t\t\t\t       NULL, kernel_kobj);\n\tBUG_ON(!iommu_group_kset);\n\n\tiommu_debugfs_setup();\n\n\treturn 0;\n}\ncore_initcall(iommu_init);\n\nint iommu_enable_nesting(struct iommu_domain *domain)\n{\n\tif (domain->type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn -EINVAL;\n\tif (!domain->ops->enable_nesting)\n\t\treturn -EINVAL;\n\treturn domain->ops->enable_nesting(domain);\n}\nEXPORT_SYMBOL_GPL(iommu_enable_nesting);\n\nint iommu_set_pgtable_quirks(struct iommu_domain *domain,\n\t\tunsigned long quirk)\n{\n\tif (domain->type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn -EINVAL;\n\tif (!domain->ops->set_pgtable_quirks)\n\t\treturn -EINVAL;\n\treturn domain->ops->set_pgtable_quirks(domain, quirk);\n}\nEXPORT_SYMBOL_GPL(iommu_set_pgtable_quirks);\n\n \nvoid iommu_get_resv_regions(struct device *dev, struct list_head *list)\n{\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\n\tif (ops->get_resv_regions)\n\t\tops->get_resv_regions(dev, list);\n}\nEXPORT_SYMBOL_GPL(iommu_get_resv_regions);\n\n \nvoid iommu_put_resv_regions(struct device *dev, struct list_head *list)\n{\n\tstruct iommu_resv_region *entry, *next;\n\n\tlist_for_each_entry_safe(entry, next, list, list) {\n\t\tif (entry->free)\n\t\t\tentry->free(dev, entry);\n\t\telse\n\t\t\tkfree(entry);\n\t}\n}\nEXPORT_SYMBOL(iommu_put_resv_regions);\n\nstruct iommu_resv_region *iommu_alloc_resv_region(phys_addr_t start,\n\t\t\t\t\t\t  size_t length, int prot,\n\t\t\t\t\t\t  enum iommu_resv_type type,\n\t\t\t\t\t\t  gfp_t gfp)\n{\n\tstruct iommu_resv_region *region;\n\n\tregion = kzalloc(sizeof(*region), gfp);\n\tif (!region)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&region->list);\n\tregion->start = start;\n\tregion->length = length;\n\tregion->prot = prot;\n\tregion->type = type;\n\treturn region;\n}\nEXPORT_SYMBOL_GPL(iommu_alloc_resv_region);\n\nvoid iommu_set_default_passthrough(bool cmd_line)\n{\n\tif (cmd_line)\n\t\tiommu_cmd_line |= IOMMU_CMD_LINE_DMA_API;\n\tiommu_def_domain_type = IOMMU_DOMAIN_IDENTITY;\n}\n\nvoid iommu_set_default_translated(bool cmd_line)\n{\n\tif (cmd_line)\n\t\tiommu_cmd_line |= IOMMU_CMD_LINE_DMA_API;\n\tiommu_def_domain_type = IOMMU_DOMAIN_DMA;\n}\n\nbool iommu_default_passthrough(void)\n{\n\treturn iommu_def_domain_type == IOMMU_DOMAIN_IDENTITY;\n}\nEXPORT_SYMBOL_GPL(iommu_default_passthrough);\n\nconst struct iommu_ops *iommu_ops_from_fwnode(struct fwnode_handle *fwnode)\n{\n\tconst struct iommu_ops *ops = NULL;\n\tstruct iommu_device *iommu;\n\n\tspin_lock(&iommu_device_lock);\n\tlist_for_each_entry(iommu, &iommu_device_list, list)\n\t\tif (iommu->fwnode == fwnode) {\n\t\t\tops = iommu->ops;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&iommu_device_lock);\n\treturn ops;\n}\n\nint iommu_fwspec_init(struct device *dev, struct fwnode_handle *iommu_fwnode,\n\t\t      const struct iommu_ops *ops)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (fwspec)\n\t\treturn ops == fwspec->ops ? 0 : -EINVAL;\n\n\tif (!dev_iommu_get(dev))\n\t\treturn -ENOMEM;\n\n\t \n\tfwspec = kzalloc(struct_size(fwspec, ids, 1), GFP_KERNEL);\n\tif (!fwspec)\n\t\treturn -ENOMEM;\n\n\tof_node_get(to_of_node(iommu_fwnode));\n\tfwspec->iommu_fwnode = iommu_fwnode;\n\tfwspec->ops = ops;\n\tdev_iommu_fwspec_set(dev, fwspec);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iommu_fwspec_init);\n\nvoid iommu_fwspec_free(struct device *dev)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (fwspec) {\n\t\tfwnode_handle_put(fwspec->iommu_fwnode);\n\t\tkfree(fwspec);\n\t\tdev_iommu_fwspec_set(dev, NULL);\n\t}\n}\nEXPORT_SYMBOL_GPL(iommu_fwspec_free);\n\nint iommu_fwspec_add_ids(struct device *dev, u32 *ids, int num_ids)\n{\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tint i, new_num;\n\n\tif (!fwspec)\n\t\treturn -EINVAL;\n\n\tnew_num = fwspec->num_ids + num_ids;\n\tif (new_num > 1) {\n\t\tfwspec = krealloc(fwspec, struct_size(fwspec, ids, new_num),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!fwspec)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_iommu_fwspec_set(dev, fwspec);\n\t}\n\n\tfor (i = 0; i < num_ids; i++)\n\t\tfwspec->ids[fwspec->num_ids + i] = ids[i];\n\n\tfwspec->num_ids = new_num;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iommu_fwspec_add_ids);\n\n \nint iommu_dev_enable_feature(struct device *dev, enum iommu_dev_features feat)\n{\n\tif (dev->iommu && dev->iommu->iommu_dev) {\n\t\tconst struct iommu_ops *ops = dev->iommu->iommu_dev->ops;\n\n\t\tif (ops->dev_enable_feat)\n\t\t\treturn ops->dev_enable_feat(dev, feat);\n\t}\n\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(iommu_dev_enable_feature);\n\n \nint iommu_dev_disable_feature(struct device *dev, enum iommu_dev_features feat)\n{\n\tif (dev->iommu && dev->iommu->iommu_dev) {\n\t\tconst struct iommu_ops *ops = dev->iommu->iommu_dev->ops;\n\n\t\tif (ops->dev_disable_feat)\n\t\t\treturn ops->dev_disable_feat(dev, feat);\n\t}\n\n\treturn -EBUSY;\n}\nEXPORT_SYMBOL_GPL(iommu_dev_disable_feature);\n\n \nstatic int iommu_setup_default_domain(struct iommu_group *group,\n\t\t\t\t      int target_type)\n{\n\tstruct iommu_domain *old_dom = group->default_domain;\n\tstruct group_device *gdev;\n\tstruct iommu_domain *dom;\n\tbool direct_failed;\n\tint req_type;\n\tint ret;\n\n\tlockdep_assert_held(&group->mutex);\n\n\treq_type = iommu_get_default_domain_type(group, target_type);\n\tif (req_type < 0)\n\t\treturn -EINVAL;\n\n\t \n\tdom = iommu_group_alloc_default_domain(group, req_type);\n\tif (!dom) {\n\t\t \n\t\tif (group->default_domain)\n\t\t\treturn -ENODEV;\n\t\tgroup->default_domain = NULL;\n\t\treturn 0;\n\t}\n\n\tif (group->default_domain == dom)\n\t\treturn 0;\n\n\t \n\tdirect_failed = false;\n\tfor_each_group_device(group, gdev) {\n\t\tif (iommu_create_device_direct_mappings(dom, gdev->dev)) {\n\t\t\tdirect_failed = true;\n\t\t\tdev_warn_once(\n\t\t\t\tgdev->dev->iommu->iommu_dev->dev,\n\t\t\t\t\"IOMMU driver was not able to establish FW requested direct mapping.\");\n\t\t}\n\t}\n\n\t \n\tgroup->default_domain = dom;\n\tif (!group->domain) {\n\t\t \n\t\tret = __iommu_group_set_domain_internal(\n\t\t\tgroup, dom, IOMMU_SET_DOMAIN_MUST_SUCCEED);\n\t\tif (WARN_ON(ret))\n\t\t\tgoto out_free_old;\n\t} else {\n\t\tret = __iommu_group_set_domain(group, dom);\n\t\tif (ret)\n\t\t\tgoto err_restore_def_domain;\n\t}\n\n\t \n\tif (direct_failed) {\n\t\tfor_each_group_device(group, gdev) {\n\t\t\tret = iommu_create_device_direct_mappings(dom, gdev->dev);\n\t\t\tif (ret)\n\t\t\t\tgoto err_restore_domain;\n\t\t}\n\t}\n\nout_free_old:\n\tif (old_dom)\n\t\tiommu_domain_free(old_dom);\n\treturn ret;\n\nerr_restore_domain:\n\tif (old_dom)\n\t\t__iommu_group_set_domain_internal(\n\t\t\tgroup, old_dom, IOMMU_SET_DOMAIN_MUST_SUCCEED);\nerr_restore_def_domain:\n\tif (old_dom) {\n\t\tiommu_domain_free(dom);\n\t\tgroup->default_domain = old_dom;\n\t}\n\treturn ret;\n}\n\n \nstatic ssize_t iommu_group_store_type(struct iommu_group *group,\n\t\t\t\t      const char *buf, size_t count)\n{\n\tstruct group_device *gdev;\n\tint ret, req_type;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\n\tif (WARN_ON(!group) || !group->default_domain)\n\t\treturn -EINVAL;\n\n\tif (sysfs_streq(buf, \"identity\"))\n\t\treq_type = IOMMU_DOMAIN_IDENTITY;\n\telse if (sysfs_streq(buf, \"DMA\"))\n\t\treq_type = IOMMU_DOMAIN_DMA;\n\telse if (sysfs_streq(buf, \"DMA-FQ\"))\n\t\treq_type = IOMMU_DOMAIN_DMA_FQ;\n\telse if (sysfs_streq(buf, \"auto\"))\n\t\treq_type = 0;\n\telse\n\t\treturn -EINVAL;\n\n\tmutex_lock(&group->mutex);\n\t \n\tif (req_type == IOMMU_DOMAIN_DMA_FQ &&\n\t    group->default_domain->type == IOMMU_DOMAIN_DMA) {\n\t\tret = iommu_dma_init_fq(group->default_domain);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\n\t\tgroup->default_domain->type = IOMMU_DOMAIN_DMA_FQ;\n\t\tret = count;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (list_empty(&group->devices) || group->owner_cnt) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tret = iommu_setup_default_domain(group, req_type);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tmutex_unlock(&group->mutex);\n\n\t \n\tfor_each_group_device(group, gdev)\n\t\tiommu_group_do_probe_finalize(gdev->dev);\n\treturn count;\n\nout_unlock:\n\tmutex_unlock(&group->mutex);\n\treturn ret ?: count;\n}\n\nstatic bool iommu_is_default_domain(struct iommu_group *group)\n{\n\tif (group->domain == group->default_domain)\n\t\treturn true;\n\n\t \n\tif (group->default_domain &&\n\t    group->default_domain->type == IOMMU_DOMAIN_IDENTITY &&\n\t    group->domain && group->domain->type == IOMMU_DOMAIN_IDENTITY)\n\t\treturn true;\n\treturn false;\n}\n\n \nint iommu_device_use_default_domain(struct device *dev)\n{\n\tstruct iommu_group *group = iommu_group_get(dev);\n\tint ret = 0;\n\n\tif (!group)\n\t\treturn 0;\n\n\tmutex_lock(&group->mutex);\n\tif (group->owner_cnt) {\n\t\tif (group->owner || !iommu_is_default_domain(group) ||\n\t\t    !xa_empty(&group->pasid_array)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto unlock_out;\n\t\t}\n\t}\n\n\tgroup->owner_cnt++;\n\nunlock_out:\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n\n\treturn ret;\n}\n\n \nvoid iommu_device_unuse_default_domain(struct device *dev)\n{\n\tstruct iommu_group *group = iommu_group_get(dev);\n\n\tif (!group)\n\t\treturn;\n\n\tmutex_lock(&group->mutex);\n\tif (!WARN_ON(!group->owner_cnt || !xa_empty(&group->pasid_array)))\n\t\tgroup->owner_cnt--;\n\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n}\n\nstatic int __iommu_group_alloc_blocking_domain(struct iommu_group *group)\n{\n\tstruct group_device *dev =\n\t\tlist_first_entry(&group->devices, struct group_device, list);\n\n\tif (group->blocking_domain)\n\t\treturn 0;\n\n\tgroup->blocking_domain =\n\t\t__iommu_domain_alloc(dev->dev->bus, IOMMU_DOMAIN_BLOCKED);\n\tif (!group->blocking_domain) {\n\t\t \n\t\tgroup->blocking_domain = __iommu_domain_alloc(\n\t\t\tdev->dev->bus, IOMMU_DOMAIN_UNMANAGED);\n\t\tif (!group->blocking_domain)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int __iommu_take_dma_ownership(struct iommu_group *group, void *owner)\n{\n\tint ret;\n\n\tif ((group->domain && group->domain != group->default_domain) ||\n\t    !xa_empty(&group->pasid_array))\n\t\treturn -EBUSY;\n\n\tret = __iommu_group_alloc_blocking_domain(group);\n\tif (ret)\n\t\treturn ret;\n\tret = __iommu_group_set_domain(group, group->blocking_domain);\n\tif (ret)\n\t\treturn ret;\n\n\tgroup->owner = owner;\n\tgroup->owner_cnt++;\n\treturn 0;\n}\n\n \nint iommu_group_claim_dma_owner(struct iommu_group *group, void *owner)\n{\n\tint ret = 0;\n\n\tif (WARN_ON(!owner))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&group->mutex);\n\tif (group->owner_cnt) {\n\t\tret = -EPERM;\n\t\tgoto unlock_out;\n\t}\n\n\tret = __iommu_take_dma_ownership(group, owner);\nunlock_out:\n\tmutex_unlock(&group->mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_group_claim_dma_owner);\n\n \nint iommu_device_claim_dma_owner(struct device *dev, void *owner)\n{\n\tstruct iommu_group *group;\n\tint ret = 0;\n\n\tif (WARN_ON(!owner))\n\t\treturn -EINVAL;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&group->mutex);\n\tif (group->owner_cnt) {\n\t\tif (group->owner != owner) {\n\t\t\tret = -EPERM;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\tgroup->owner_cnt++;\n\t\tgoto unlock_out;\n\t}\n\n\tret = __iommu_take_dma_ownership(group, owner);\nunlock_out:\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_device_claim_dma_owner);\n\nstatic void __iommu_release_dma_ownership(struct iommu_group *group)\n{\n\tif (WARN_ON(!group->owner_cnt || !group->owner ||\n\t\t    !xa_empty(&group->pasid_array)))\n\t\treturn;\n\n\tgroup->owner_cnt = 0;\n\tgroup->owner = NULL;\n\t__iommu_group_set_domain_nofail(group, group->default_domain);\n}\n\n \nvoid iommu_group_release_dma_owner(struct iommu_group *group)\n{\n\tmutex_lock(&group->mutex);\n\t__iommu_release_dma_ownership(group);\n\tmutex_unlock(&group->mutex);\n}\nEXPORT_SYMBOL_GPL(iommu_group_release_dma_owner);\n\n \nvoid iommu_device_release_dma_owner(struct device *dev)\n{\n\tstruct iommu_group *group = iommu_group_get(dev);\n\n\tmutex_lock(&group->mutex);\n\tif (group->owner_cnt > 1)\n\t\tgroup->owner_cnt--;\n\telse\n\t\t__iommu_release_dma_ownership(group);\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n}\nEXPORT_SYMBOL_GPL(iommu_device_release_dma_owner);\n\n \nbool iommu_group_dma_owner_claimed(struct iommu_group *group)\n{\n\tunsigned int user;\n\n\tmutex_lock(&group->mutex);\n\tuser = group->owner_cnt;\n\tmutex_unlock(&group->mutex);\n\n\treturn user;\n}\nEXPORT_SYMBOL_GPL(iommu_group_dma_owner_claimed);\n\nstatic int __iommu_set_group_pasid(struct iommu_domain *domain,\n\t\t\t\t   struct iommu_group *group, ioasid_t pasid)\n{\n\tstruct group_device *device;\n\tint ret = 0;\n\n\tfor_each_group_device(group, device) {\n\t\tret = domain->ops->set_dev_pasid(domain, device->dev, pasid);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void __iommu_remove_group_pasid(struct iommu_group *group,\n\t\t\t\t       ioasid_t pasid)\n{\n\tstruct group_device *device;\n\tconst struct iommu_ops *ops;\n\n\tfor_each_group_device(group, device) {\n\t\tops = dev_iommu_ops(device->dev);\n\t\tops->remove_dev_pasid(device->dev, pasid);\n\t}\n}\n\n \nint iommu_attach_device_pasid(struct iommu_domain *domain,\n\t\t\t      struct device *dev, ioasid_t pasid)\n{\n\tstruct iommu_group *group;\n\tvoid *curr;\n\tint ret;\n\n\tif (!domain->ops->set_dev_pasid)\n\t\treturn -EOPNOTSUPP;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&group->mutex);\n\tcurr = xa_cmpxchg(&group->pasid_array, pasid, NULL, domain, GFP_KERNEL);\n\tif (curr) {\n\t\tret = xa_err(curr) ? : -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tret = __iommu_set_group_pasid(domain, group, pasid);\n\tif (ret) {\n\t\t__iommu_remove_group_pasid(group, pasid);\n\t\txa_erase(&group->pasid_array, pasid);\n\t}\nout_unlock:\n\tmutex_unlock(&group->mutex);\n\tiommu_group_put(group);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iommu_attach_device_pasid);\n\n \nvoid iommu_detach_device_pasid(struct iommu_domain *domain, struct device *dev,\n\t\t\t       ioasid_t pasid)\n{\n\tstruct iommu_group *group = iommu_group_get(dev);\n\n\tmutex_lock(&group->mutex);\n\t__iommu_remove_group_pasid(group, pasid);\n\tWARN_ON(xa_erase(&group->pasid_array, pasid) != domain);\n\tmutex_unlock(&group->mutex);\n\n\tiommu_group_put(group);\n}\nEXPORT_SYMBOL_GPL(iommu_detach_device_pasid);\n\n \nstruct iommu_domain *iommu_get_domain_for_dev_pasid(struct device *dev,\n\t\t\t\t\t\t    ioasid_t pasid,\n\t\t\t\t\t\t    unsigned int type)\n{\n\tstruct iommu_domain *domain;\n\tstruct iommu_group *group;\n\n\tgroup = iommu_group_get(dev);\n\tif (!group)\n\t\treturn NULL;\n\n\txa_lock(&group->pasid_array);\n\tdomain = xa_load(&group->pasid_array, pasid);\n\tif (type && domain && domain->type != type)\n\t\tdomain = ERR_PTR(-EBUSY);\n\txa_unlock(&group->pasid_array);\n\tiommu_group_put(group);\n\n\treturn domain;\n}\nEXPORT_SYMBOL_GPL(iommu_get_domain_for_dev_pasid);\n\nstruct iommu_domain *iommu_sva_domain_alloc(struct device *dev,\n\t\t\t\t\t    struct mm_struct *mm)\n{\n\tconst struct iommu_ops *ops = dev_iommu_ops(dev);\n\tstruct iommu_domain *domain;\n\n\tdomain = ops->domain_alloc(IOMMU_DOMAIN_SVA);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->type = IOMMU_DOMAIN_SVA;\n\tmmgrab(mm);\n\tdomain->mm = mm;\n\tdomain->iopf_handler = iommu_sva_handle_iopf;\n\tdomain->fault_data = mm;\n\n\treturn domain;\n}\n\nioasid_t iommu_alloc_global_pasid(struct device *dev)\n{\n\tint ret;\n\n\t \n\tif (!dev->iommu->max_pasids)\n\t\treturn IOMMU_PASID_INVALID;\n\n\t \n\tret = ida_alloc_range(&iommu_global_pasid_ida, IOMMU_FIRST_GLOBAL_PASID,\n\t\t\t      dev->iommu->max_pasids - 1, GFP_KERNEL);\n\treturn ret < 0 ? IOMMU_PASID_INVALID : ret;\n}\nEXPORT_SYMBOL_GPL(iommu_alloc_global_pasid);\n\nvoid iommu_free_global_pasid(ioasid_t pasid)\n{\n\tif (WARN_ON(pasid == IOMMU_PASID_INVALID))\n\t\treturn;\n\n\tida_free(&iommu_global_pasid_ida, pasid);\n}\nEXPORT_SYMBOL_GPL(iommu_free_global_pasid);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}