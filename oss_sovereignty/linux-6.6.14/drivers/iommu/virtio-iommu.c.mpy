{
  "module_name": "virtio-iommu.c",
  "hash_id": "9deee8cf1c7742fe44a7e60c15ba0821a27dceae6591eb58a2f752242cb72e9e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/virtio-iommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/delay.h>\n#include <linux/dma-map-ops.h>\n#include <linux/freezer.h>\n#include <linux/interval_tree.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/pci.h>\n#include <linux/virtio.h>\n#include <linux/virtio_config.h>\n#include <linux/virtio_ids.h>\n#include <linux/wait.h>\n\n#include <uapi/linux/virtio_iommu.h>\n\n#include \"dma-iommu.h\"\n\n#define MSI_IOVA_BASE\t\t\t0x8000000\n#define MSI_IOVA_LENGTH\t\t\t0x100000\n\n#define VIOMMU_REQUEST_VQ\t\t0\n#define VIOMMU_EVENT_VQ\t\t\t1\n#define VIOMMU_NR_VQS\t\t\t2\n\nstruct viommu_dev {\n\tstruct iommu_device\t\tiommu;\n\tstruct device\t\t\t*dev;\n\tstruct virtio_device\t\t*vdev;\n\n\tstruct ida\t\t\tdomain_ids;\n\n\tstruct virtqueue\t\t*vqs[VIOMMU_NR_VQS];\n\tspinlock_t\t\t\trequest_lock;\n\tstruct list_head\t\trequests;\n\tvoid\t\t\t\t*evts;\n\n\t \n\tstruct iommu_domain_geometry\tgeometry;\n\tu64\t\t\t\tpgsize_bitmap;\n\tu32\t\t\t\tfirst_domain;\n\tu32\t\t\t\tlast_domain;\n\t \n\tu32\t\t\t\tmap_flags;\n\tu32\t\t\t\tprobe_size;\n};\n\nstruct viommu_mapping {\n\tphys_addr_t\t\t\tpaddr;\n\tstruct interval_tree_node\tiova;\n\tu32\t\t\t\tflags;\n};\n\nstruct viommu_domain {\n\tstruct iommu_domain\t\tdomain;\n\tstruct viommu_dev\t\t*viommu;\n\tstruct mutex\t\t\tmutex;  \n\tunsigned int\t\t\tid;\n\tu32\t\t\t\tmap_flags;\n\n\tspinlock_t\t\t\tmappings_lock;\n\tstruct rb_root_cached\t\tmappings;\n\n\tunsigned long\t\t\tnr_endpoints;\n\tbool\t\t\t\tbypass;\n};\n\nstruct viommu_endpoint {\n\tstruct device\t\t\t*dev;\n\tstruct viommu_dev\t\t*viommu;\n\tstruct viommu_domain\t\t*vdomain;\n\tstruct list_head\t\tresv_regions;\n};\n\nstruct viommu_request {\n\tstruct list_head\t\tlist;\n\tvoid\t\t\t\t*writeback;\n\tunsigned int\t\t\twrite_offset;\n\tunsigned int\t\t\tlen;\n\tchar\t\t\t\tbuf[];\n};\n\n#define VIOMMU_FAULT_RESV_MASK\t\t0xffffff00\n\nstruct viommu_event {\n\tunion {\n\t\tu32\t\t\thead;\n\t\tstruct virtio_iommu_fault fault;\n\t};\n};\n\n#define to_viommu_domain(domain)\t\\\n\tcontainer_of(domain, struct viommu_domain, domain)\n\nstatic int viommu_get_req_errno(void *buf, size_t len)\n{\n\tstruct virtio_iommu_req_tail *tail = buf + len - sizeof(*tail);\n\n\tswitch (tail->status) {\n\tcase VIRTIO_IOMMU_S_OK:\n\t\treturn 0;\n\tcase VIRTIO_IOMMU_S_UNSUPP:\n\t\treturn -ENOSYS;\n\tcase VIRTIO_IOMMU_S_INVAL:\n\t\treturn -EINVAL;\n\tcase VIRTIO_IOMMU_S_RANGE:\n\t\treturn -ERANGE;\n\tcase VIRTIO_IOMMU_S_NOENT:\n\t\treturn -ENOENT;\n\tcase VIRTIO_IOMMU_S_FAULT:\n\t\treturn -EFAULT;\n\tcase VIRTIO_IOMMU_S_NOMEM:\n\t\treturn -ENOMEM;\n\tcase VIRTIO_IOMMU_S_IOERR:\n\tcase VIRTIO_IOMMU_S_DEVERR:\n\tdefault:\n\t\treturn -EIO;\n\t}\n}\n\nstatic void viommu_set_req_status(void *buf, size_t len, int status)\n{\n\tstruct virtio_iommu_req_tail *tail = buf + len - sizeof(*tail);\n\n\ttail->status = status;\n}\n\nstatic off_t viommu_get_write_desc_offset(struct viommu_dev *viommu,\n\t\t\t\t\t  struct virtio_iommu_req_head *req,\n\t\t\t\t\t  size_t len)\n{\n\tsize_t tail_size = sizeof(struct virtio_iommu_req_tail);\n\n\tif (req->type == VIRTIO_IOMMU_T_PROBE)\n\t\treturn len - viommu->probe_size - tail_size;\n\n\treturn len - tail_size;\n}\n\n \nstatic int __viommu_sync_req(struct viommu_dev *viommu)\n{\n\tunsigned int len;\n\tsize_t write_len;\n\tstruct viommu_request *req;\n\tstruct virtqueue *vq = viommu->vqs[VIOMMU_REQUEST_VQ];\n\n\tassert_spin_locked(&viommu->request_lock);\n\n\tvirtqueue_kick(vq);\n\n\twhile (!list_empty(&viommu->requests)) {\n\t\tlen = 0;\n\t\treq = virtqueue_get_buf(vq, &len);\n\t\tif (!req)\n\t\t\tcontinue;\n\n\t\tif (!len)\n\t\t\tviommu_set_req_status(req->buf, req->len,\n\t\t\t\t\t      VIRTIO_IOMMU_S_IOERR);\n\n\t\twrite_len = req->len - req->write_offset;\n\t\tif (req->writeback && len == write_len)\n\t\t\tmemcpy(req->writeback, req->buf + req->write_offset,\n\t\t\t       write_len);\n\n\t\tlist_del(&req->list);\n\t\tkfree(req);\n\t}\n\n\treturn 0;\n}\n\nstatic int viommu_sync_req(struct viommu_dev *viommu)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&viommu->request_lock, flags);\n\tret = __viommu_sync_req(viommu);\n\tif (ret)\n\t\tdev_dbg(viommu->dev, \"could not sync requests (%d)\\n\", ret);\n\tspin_unlock_irqrestore(&viommu->request_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int __viommu_add_req(struct viommu_dev *viommu, void *buf, size_t len,\n\t\t\t    bool writeback)\n{\n\tint ret;\n\toff_t write_offset;\n\tstruct viommu_request *req;\n\tstruct scatterlist top_sg, bottom_sg;\n\tstruct scatterlist *sg[2] = { &top_sg, &bottom_sg };\n\tstruct virtqueue *vq = viommu->vqs[VIOMMU_REQUEST_VQ];\n\n\tassert_spin_locked(&viommu->request_lock);\n\n\twrite_offset = viommu_get_write_desc_offset(viommu, buf, len);\n\tif (write_offset <= 0)\n\t\treturn -EINVAL;\n\n\treq = kzalloc(sizeof(*req) + len, GFP_ATOMIC);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->len = len;\n\tif (writeback) {\n\t\treq->writeback = buf + write_offset;\n\t\treq->write_offset = write_offset;\n\t}\n\tmemcpy(&req->buf, buf, write_offset);\n\n\tsg_init_one(&top_sg, req->buf, write_offset);\n\tsg_init_one(&bottom_sg, req->buf + write_offset, len - write_offset);\n\n\tret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);\n\tif (ret == -ENOSPC) {\n\t\t \n\t\tif (!__viommu_sync_req(viommu))\n\t\t\tret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);\n\t}\n\tif (ret)\n\t\tgoto err_free;\n\n\tlist_add_tail(&req->list, &viommu->requests);\n\treturn 0;\n\nerr_free:\n\tkfree(req);\n\treturn ret;\n}\n\nstatic int viommu_add_req(struct viommu_dev *viommu, void *buf, size_t len)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&viommu->request_lock, flags);\n\tret = __viommu_add_req(viommu, buf, len, false);\n\tif (ret)\n\t\tdev_dbg(viommu->dev, \"could not add request: %d\\n\", ret);\n\tspin_unlock_irqrestore(&viommu->request_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int viommu_send_req_sync(struct viommu_dev *viommu, void *buf,\n\t\t\t\tsize_t len)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&viommu->request_lock, flags);\n\n\tret = __viommu_add_req(viommu, buf, len, true);\n\tif (ret) {\n\t\tdev_dbg(viommu->dev, \"could not add request (%d)\\n\", ret);\n\t\tgoto out_unlock;\n\t}\n\n\tret = __viommu_sync_req(viommu);\n\tif (ret) {\n\t\tdev_dbg(viommu->dev, \"could not sync requests (%d)\\n\", ret);\n\t\t \n\t}\n\n\tret = viommu_get_req_errno(buf, len);\nout_unlock:\n\tspin_unlock_irqrestore(&viommu->request_lock, flags);\n\treturn ret;\n}\n\n \nstatic int viommu_add_mapping(struct viommu_domain *vdomain, u64 iova, u64 end,\n\t\t\t      phys_addr_t paddr, u32 flags)\n{\n\tunsigned long irqflags;\n\tstruct viommu_mapping *mapping;\n\n\tmapping = kzalloc(sizeof(*mapping), GFP_ATOMIC);\n\tif (!mapping)\n\t\treturn -ENOMEM;\n\n\tmapping->paddr\t\t= paddr;\n\tmapping->iova.start\t= iova;\n\tmapping->iova.last\t= end;\n\tmapping->flags\t\t= flags;\n\n\tspin_lock_irqsave(&vdomain->mappings_lock, irqflags);\n\tinterval_tree_insert(&mapping->iova, &vdomain->mappings);\n\tspin_unlock_irqrestore(&vdomain->mappings_lock, irqflags);\n\n\treturn 0;\n}\n\n \nstatic size_t viommu_del_mappings(struct viommu_domain *vdomain,\n\t\t\t\t  u64 iova, u64 end)\n{\n\tsize_t unmapped = 0;\n\tunsigned long flags;\n\tstruct viommu_mapping *mapping = NULL;\n\tstruct interval_tree_node *node, *next;\n\n\tspin_lock_irqsave(&vdomain->mappings_lock, flags);\n\tnext = interval_tree_iter_first(&vdomain->mappings, iova, end);\n\twhile (next) {\n\t\tnode = next;\n\t\tmapping = container_of(node, struct viommu_mapping, iova);\n\t\tnext = interval_tree_iter_next(node, iova, end);\n\n\t\t \n\t\tif (mapping->iova.start < iova)\n\t\t\tbreak;\n\n\t\t \n\t\tunmapped += mapping->iova.last - mapping->iova.start + 1;\n\n\t\tinterval_tree_remove(node, &vdomain->mappings);\n\t\tkfree(mapping);\n\t}\n\tspin_unlock_irqrestore(&vdomain->mappings_lock, flags);\n\n\treturn unmapped;\n}\n\n \nstatic int viommu_domain_map_identity(struct viommu_endpoint *vdev,\n\t\t\t\t      struct viommu_domain *vdomain)\n{\n\tint ret;\n\tstruct iommu_resv_region *resv;\n\tu64 iova = vdomain->domain.geometry.aperture_start;\n\tu64 limit = vdomain->domain.geometry.aperture_end;\n\tu32 flags = VIRTIO_IOMMU_MAP_F_READ | VIRTIO_IOMMU_MAP_F_WRITE;\n\tunsigned long granule = 1UL << __ffs(vdomain->domain.pgsize_bitmap);\n\n\tiova = ALIGN(iova, granule);\n\tlimit = ALIGN_DOWN(limit + 1, granule) - 1;\n\n\tlist_for_each_entry(resv, &vdev->resv_regions, list) {\n\t\tu64 resv_start = ALIGN_DOWN(resv->start, granule);\n\t\tu64 resv_end = ALIGN(resv->start + resv->length, granule) - 1;\n\n\t\tif (resv_end < iova || resv_start > limit)\n\t\t\t \n\t\t\tcontinue;\n\n\t\tif (resv_start > iova) {\n\t\t\tret = viommu_add_mapping(vdomain, iova, resv_start - 1,\n\t\t\t\t\t\t (phys_addr_t)iova, flags);\n\t\t\tif (ret)\n\t\t\t\tgoto err_unmap;\n\t\t}\n\n\t\tif (resv_end >= limit)\n\t\t\treturn 0;\n\n\t\tiova = resv_end + 1;\n\t}\n\n\tret = viommu_add_mapping(vdomain, iova, limit, (phys_addr_t)iova,\n\t\t\t\t flags);\n\tif (ret)\n\t\tgoto err_unmap;\n\treturn 0;\n\nerr_unmap:\n\tviommu_del_mappings(vdomain, 0, iova);\n\treturn ret;\n}\n\n \nstatic int viommu_replay_mappings(struct viommu_domain *vdomain)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct viommu_mapping *mapping;\n\tstruct interval_tree_node *node;\n\tstruct virtio_iommu_req_map map;\n\n\tspin_lock_irqsave(&vdomain->mappings_lock, flags);\n\tnode = interval_tree_iter_first(&vdomain->mappings, 0, -1UL);\n\twhile (node) {\n\t\tmapping = container_of(node, struct viommu_mapping, iova);\n\t\tmap = (struct virtio_iommu_req_map) {\n\t\t\t.head.type\t= VIRTIO_IOMMU_T_MAP,\n\t\t\t.domain\t\t= cpu_to_le32(vdomain->id),\n\t\t\t.virt_start\t= cpu_to_le64(mapping->iova.start),\n\t\t\t.virt_end\t= cpu_to_le64(mapping->iova.last),\n\t\t\t.phys_start\t= cpu_to_le64(mapping->paddr),\n\t\t\t.flags\t\t= cpu_to_le32(mapping->flags),\n\t\t};\n\n\t\tret = viommu_send_req_sync(vdomain->viommu, &map, sizeof(map));\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tnode = interval_tree_iter_next(node, 0, -1UL);\n\t}\n\tspin_unlock_irqrestore(&vdomain->mappings_lock, flags);\n\n\treturn ret;\n}\n\nstatic int viommu_add_resv_mem(struct viommu_endpoint *vdev,\n\t\t\t       struct virtio_iommu_probe_resv_mem *mem,\n\t\t\t       size_t len)\n{\n\tsize_t size;\n\tu64 start64, end64;\n\tphys_addr_t start, end;\n\tstruct iommu_resv_region *region = NULL, *next;\n\tunsigned long prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\n\tstart = start64 = le64_to_cpu(mem->start);\n\tend = end64 = le64_to_cpu(mem->end);\n\tsize = end64 - start64 + 1;\n\n\t \n\tif (start != start64 || end != end64 || size < end64 - start64)\n\t\treturn -EOVERFLOW;\n\n\tif (len < sizeof(*mem))\n\t\treturn -EINVAL;\n\n\tswitch (mem->subtype) {\n\tdefault:\n\t\tdev_warn(vdev->dev, \"unknown resv mem subtype 0x%x\\n\",\n\t\t\t mem->subtype);\n\t\tfallthrough;\n\tcase VIRTIO_IOMMU_RESV_MEM_T_RESERVED:\n\t\tregion = iommu_alloc_resv_region(start, size, 0,\n\t\t\t\t\t\t IOMMU_RESV_RESERVED,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tbreak;\n\tcase VIRTIO_IOMMU_RESV_MEM_T_MSI:\n\t\tregion = iommu_alloc_resv_region(start, size, prot,\n\t\t\t\t\t\t IOMMU_RESV_MSI,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tbreak;\n\t}\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\t \n\tlist_for_each_entry(next, &vdev->resv_regions, list) {\n\t\tif (next->start > region->start)\n\t\t\tbreak;\n\t}\n\tlist_add_tail(&region->list, &next->list);\n\treturn 0;\n}\n\nstatic int viommu_probe_endpoint(struct viommu_dev *viommu, struct device *dev)\n{\n\tint ret;\n\tu16 type, len;\n\tsize_t cur = 0;\n\tsize_t probe_len;\n\tstruct virtio_iommu_req_probe *probe;\n\tstruct virtio_iommu_probe_property *prop;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct viommu_endpoint *vdev = dev_iommu_priv_get(dev);\n\n\tif (!fwspec->num_ids)\n\t\treturn -EINVAL;\n\n\tprobe_len = sizeof(*probe) + viommu->probe_size +\n\t\t    sizeof(struct virtio_iommu_req_tail);\n\tprobe = kzalloc(probe_len, GFP_KERNEL);\n\tif (!probe)\n\t\treturn -ENOMEM;\n\n\tprobe->head.type = VIRTIO_IOMMU_T_PROBE;\n\t \n\tprobe->endpoint = cpu_to_le32(fwspec->ids[0]);\n\n\tret = viommu_send_req_sync(viommu, probe, probe_len);\n\tif (ret)\n\t\tgoto out_free;\n\n\tprop = (void *)probe->properties;\n\ttype = le16_to_cpu(prop->type) & VIRTIO_IOMMU_PROBE_T_MASK;\n\n\twhile (type != VIRTIO_IOMMU_PROBE_T_NONE &&\n\t       cur < viommu->probe_size) {\n\t\tlen = le16_to_cpu(prop->length) + sizeof(*prop);\n\n\t\tswitch (type) {\n\t\tcase VIRTIO_IOMMU_PROBE_T_RESV_MEM:\n\t\t\tret = viommu_add_resv_mem(vdev, (void *)prop, len);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"unknown viommu prop 0x%x\\n\", type);\n\t\t}\n\n\t\tif (ret)\n\t\t\tdev_err(dev, \"failed to parse viommu prop 0x%x\\n\", type);\n\n\t\tcur += len;\n\t\tif (cur >= viommu->probe_size)\n\t\t\tbreak;\n\n\t\tprop = (void *)probe->properties + cur;\n\t\ttype = le16_to_cpu(prop->type) & VIRTIO_IOMMU_PROBE_T_MASK;\n\t}\n\nout_free:\n\tkfree(probe);\n\treturn ret;\n}\n\nstatic int viommu_fault_handler(struct viommu_dev *viommu,\n\t\t\t\tstruct virtio_iommu_fault *fault)\n{\n\tchar *reason_str;\n\n\tu8 reason\t= fault->reason;\n\tu32 flags\t= le32_to_cpu(fault->flags);\n\tu32 endpoint\t= le32_to_cpu(fault->endpoint);\n\tu64 address\t= le64_to_cpu(fault->address);\n\n\tswitch (reason) {\n\tcase VIRTIO_IOMMU_FAULT_R_DOMAIN:\n\t\treason_str = \"domain\";\n\t\tbreak;\n\tcase VIRTIO_IOMMU_FAULT_R_MAPPING:\n\t\treason_str = \"page\";\n\t\tbreak;\n\tcase VIRTIO_IOMMU_FAULT_R_UNKNOWN:\n\tdefault:\n\t\treason_str = \"unknown\";\n\t\tbreak;\n\t}\n\n\t \n\tif (flags & VIRTIO_IOMMU_FAULT_F_ADDRESS)\n\t\tdev_err_ratelimited(viommu->dev, \"%s fault from EP %u at %#llx [%s%s%s]\\n\",\n\t\t\t\t    reason_str, endpoint, address,\n\t\t\t\t    flags & VIRTIO_IOMMU_FAULT_F_READ ? \"R\" : \"\",\n\t\t\t\t    flags & VIRTIO_IOMMU_FAULT_F_WRITE ? \"W\" : \"\",\n\t\t\t\t    flags & VIRTIO_IOMMU_FAULT_F_EXEC ? \"X\" : \"\");\n\telse\n\t\tdev_err_ratelimited(viommu->dev, \"%s fault from EP %u\\n\",\n\t\t\t\t    reason_str, endpoint);\n\treturn 0;\n}\n\nstatic void viommu_event_handler(struct virtqueue *vq)\n{\n\tint ret;\n\tunsigned int len;\n\tstruct scatterlist sg[1];\n\tstruct viommu_event *evt;\n\tstruct viommu_dev *viommu = vq->vdev->priv;\n\n\twhile ((evt = virtqueue_get_buf(vq, &len)) != NULL) {\n\t\tif (len > sizeof(*evt)) {\n\t\t\tdev_err(viommu->dev,\n\t\t\t\t\"invalid event buffer (len %u != %zu)\\n\",\n\t\t\t\tlen, sizeof(*evt));\n\t\t} else if (!(evt->head & VIOMMU_FAULT_RESV_MASK)) {\n\t\t\tviommu_fault_handler(viommu, &evt->fault);\n\t\t}\n\n\t\tsg_init_one(sg, evt, sizeof(*evt));\n\t\tret = virtqueue_add_inbuf(vq, sg, 1, evt, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tdev_err(viommu->dev, \"could not add event buffer\\n\");\n\t}\n\n\tvirtqueue_kick(vq);\n}\n\n \n\nstatic struct iommu_domain *viommu_domain_alloc(unsigned type)\n{\n\tstruct viommu_domain *vdomain;\n\n\tif (type != IOMMU_DOMAIN_UNMANAGED &&\n\t    type != IOMMU_DOMAIN_DMA &&\n\t    type != IOMMU_DOMAIN_IDENTITY)\n\t\treturn NULL;\n\n\tvdomain = kzalloc(sizeof(*vdomain), GFP_KERNEL);\n\tif (!vdomain)\n\t\treturn NULL;\n\n\tmutex_init(&vdomain->mutex);\n\tspin_lock_init(&vdomain->mappings_lock);\n\tvdomain->mappings = RB_ROOT_CACHED;\n\n\treturn &vdomain->domain;\n}\n\nstatic int viommu_domain_finalise(struct viommu_endpoint *vdev,\n\t\t\t\t  struct iommu_domain *domain)\n{\n\tint ret;\n\tunsigned long viommu_page_size;\n\tstruct viommu_dev *viommu = vdev->viommu;\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\tviommu_page_size = 1UL << __ffs(viommu->pgsize_bitmap);\n\tif (viommu_page_size > PAGE_SIZE) {\n\t\tdev_err(vdev->dev,\n\t\t\t\"granule 0x%lx larger than system page size 0x%lx\\n\",\n\t\t\tviommu_page_size, PAGE_SIZE);\n\t\treturn -ENODEV;\n\t}\n\n\tret = ida_alloc_range(&viommu->domain_ids, viommu->first_domain,\n\t\t\t      viommu->last_domain, GFP_KERNEL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tvdomain->id\t\t= (unsigned int)ret;\n\n\tdomain->pgsize_bitmap\t= viommu->pgsize_bitmap;\n\tdomain->geometry\t= viommu->geometry;\n\n\tvdomain->map_flags\t= viommu->map_flags;\n\tvdomain->viommu\t\t= viommu;\n\n\tif (domain->type == IOMMU_DOMAIN_IDENTITY) {\n\t\tif (virtio_has_feature(viommu->vdev,\n\t\t\t\t       VIRTIO_IOMMU_F_BYPASS_CONFIG)) {\n\t\t\tvdomain->bypass = true;\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = viommu_domain_map_identity(vdev, vdomain);\n\t\tif (ret) {\n\t\t\tida_free(&viommu->domain_ids, vdomain->id);\n\t\t\tvdomain->viommu = NULL;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void viommu_domain_free(struct iommu_domain *domain)\n{\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\t \n\tviommu_del_mappings(vdomain, 0, ULLONG_MAX);\n\n\tif (vdomain->viommu)\n\t\tida_free(&vdomain->viommu->domain_ids, vdomain->id);\n\n\tkfree(vdomain);\n}\n\nstatic int viommu_attach_dev(struct iommu_domain *domain, struct device *dev)\n{\n\tint i;\n\tint ret = 0;\n\tstruct virtio_iommu_req_attach req;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\tstruct viommu_endpoint *vdev = dev_iommu_priv_get(dev);\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\tmutex_lock(&vdomain->mutex);\n\tif (!vdomain->viommu) {\n\t\t \n\t\tret = viommu_domain_finalise(vdev, domain);\n\t} else if (vdomain->viommu != vdev->viommu) {\n\t\tret = -EINVAL;\n\t}\n\tmutex_unlock(&vdomain->mutex);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (vdev->vdomain)\n\t\tvdev->vdomain->nr_endpoints--;\n\n\treq = (struct virtio_iommu_req_attach) {\n\t\t.head.type\t= VIRTIO_IOMMU_T_ATTACH,\n\t\t.domain\t\t= cpu_to_le32(vdomain->id),\n\t};\n\n\tif (vdomain->bypass)\n\t\treq.flags |= cpu_to_le32(VIRTIO_IOMMU_ATTACH_F_BYPASS);\n\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\treq.endpoint = cpu_to_le32(fwspec->ids[i]);\n\n\t\tret = viommu_send_req_sync(vdomain->viommu, &req, sizeof(req));\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!vdomain->nr_endpoints) {\n\t\t \n\t\tret = viommu_replay_mappings(vdomain);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tvdomain->nr_endpoints++;\n\tvdev->vdomain = vdomain;\n\n\treturn 0;\n}\n\nstatic void viommu_detach_dev(struct viommu_endpoint *vdev)\n{\n\tint i;\n\tstruct virtio_iommu_req_detach req;\n\tstruct viommu_domain *vdomain = vdev->vdomain;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(vdev->dev);\n\n\tif (!vdomain)\n\t\treturn;\n\n\treq = (struct virtio_iommu_req_detach) {\n\t\t.head.type\t= VIRTIO_IOMMU_T_DETACH,\n\t\t.domain\t\t= cpu_to_le32(vdomain->id),\n\t};\n\n\tfor (i = 0; i < fwspec->num_ids; i++) {\n\t\treq.endpoint = cpu_to_le32(fwspec->ids[i]);\n\t\tWARN_ON(viommu_send_req_sync(vdev->viommu, &req, sizeof(req)));\n\t}\n\tvdomain->nr_endpoints--;\n\tvdev->vdomain = NULL;\n}\n\nstatic int viommu_map_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t    phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t    int prot, gfp_t gfp, size_t *mapped)\n{\n\tint ret;\n\tu32 flags;\n\tsize_t size = pgsize * pgcount;\n\tu64 end = iova + size - 1;\n\tstruct virtio_iommu_req_map map;\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\tflags = (prot & IOMMU_READ ? VIRTIO_IOMMU_MAP_F_READ : 0) |\n\t\t(prot & IOMMU_WRITE ? VIRTIO_IOMMU_MAP_F_WRITE : 0) |\n\t\t(prot & IOMMU_MMIO ? VIRTIO_IOMMU_MAP_F_MMIO : 0);\n\n\tif (flags & ~vdomain->map_flags)\n\t\treturn -EINVAL;\n\n\tret = viommu_add_mapping(vdomain, iova, end, paddr, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (vdomain->nr_endpoints) {\n\t\tmap = (struct virtio_iommu_req_map) {\n\t\t\t.head.type\t= VIRTIO_IOMMU_T_MAP,\n\t\t\t.domain\t\t= cpu_to_le32(vdomain->id),\n\t\t\t.virt_start\t= cpu_to_le64(iova),\n\t\t\t.phys_start\t= cpu_to_le64(paddr),\n\t\t\t.virt_end\t= cpu_to_le64(end),\n\t\t\t.flags\t\t= cpu_to_le32(flags),\n\t\t};\n\n\t\tret = viommu_send_req_sync(vdomain->viommu, &map, sizeof(map));\n\t\tif (ret) {\n\t\t\tviommu_del_mappings(vdomain, iova, end);\n\t\t\treturn ret;\n\t\t}\n\t}\n\tif (mapped)\n\t\t*mapped = size;\n\n\treturn 0;\n}\n\nstatic size_t viommu_unmap_pages(struct iommu_domain *domain, unsigned long iova,\n\t\t\t\t size_t pgsize, size_t pgcount,\n\t\t\t\t struct iommu_iotlb_gather *gather)\n{\n\tint ret = 0;\n\tsize_t unmapped;\n\tstruct virtio_iommu_req_unmap unmap;\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\tsize_t size = pgsize * pgcount;\n\n\tunmapped = viommu_del_mappings(vdomain, iova, iova + size - 1);\n\tif (unmapped < size)\n\t\treturn 0;\n\n\t \n\tif (!vdomain->nr_endpoints)\n\t\treturn unmapped;\n\n\tunmap = (struct virtio_iommu_req_unmap) {\n\t\t.head.type\t= VIRTIO_IOMMU_T_UNMAP,\n\t\t.domain\t\t= cpu_to_le32(vdomain->id),\n\t\t.virt_start\t= cpu_to_le64(iova),\n\t\t.virt_end\t= cpu_to_le64(iova + unmapped - 1),\n\t};\n\n\tret = viommu_add_req(vdomain->viommu, &unmap, sizeof(unmap));\n\treturn ret ? 0 : unmapped;\n}\n\nstatic phys_addr_t viommu_iova_to_phys(struct iommu_domain *domain,\n\t\t\t\t       dma_addr_t iova)\n{\n\tu64 paddr = 0;\n\tunsigned long flags;\n\tstruct viommu_mapping *mapping;\n\tstruct interval_tree_node *node;\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\tspin_lock_irqsave(&vdomain->mappings_lock, flags);\n\tnode = interval_tree_iter_first(&vdomain->mappings, iova, iova);\n\tif (node) {\n\t\tmapping = container_of(node, struct viommu_mapping, iova);\n\t\tpaddr = mapping->paddr + (iova - mapping->iova.start);\n\t}\n\tspin_unlock_irqrestore(&vdomain->mappings_lock, flags);\n\n\treturn paddr;\n}\n\nstatic void viommu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t      struct iommu_iotlb_gather *gather)\n{\n\tstruct viommu_domain *vdomain = to_viommu_domain(domain);\n\n\tviommu_sync_req(vdomain->viommu);\n}\n\nstatic void viommu_get_resv_regions(struct device *dev, struct list_head *head)\n{\n\tstruct iommu_resv_region *entry, *new_entry, *msi = NULL;\n\tstruct viommu_endpoint *vdev = dev_iommu_priv_get(dev);\n\tint prot = IOMMU_WRITE | IOMMU_NOEXEC | IOMMU_MMIO;\n\n\tlist_for_each_entry(entry, &vdev->resv_regions, list) {\n\t\tif (entry->type == IOMMU_RESV_MSI)\n\t\t\tmsi = entry;\n\n\t\tnew_entry = kmemdup(entry, sizeof(*entry), GFP_KERNEL);\n\t\tif (!new_entry)\n\t\t\treturn;\n\t\tlist_add_tail(&new_entry->list, head);\n\t}\n\n\t \n\tif (!msi) {\n\t\tmsi = iommu_alloc_resv_region(MSI_IOVA_BASE, MSI_IOVA_LENGTH,\n\t\t\t\t\t      prot, IOMMU_RESV_SW_MSI,\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!msi)\n\t\t\treturn;\n\n\t\tlist_add_tail(&msi->list, head);\n\t}\n\n\tiommu_dma_get_resv_regions(dev, head);\n}\n\nstatic struct iommu_ops viommu_ops;\nstatic struct virtio_driver virtio_iommu_drv;\n\nstatic int viommu_match_node(struct device *dev, const void *data)\n{\n\treturn device_match_fwnode(dev->parent, data);\n}\n\nstatic struct viommu_dev *viommu_get_by_fwnode(struct fwnode_handle *fwnode)\n{\n\tstruct device *dev = driver_find_device(&virtio_iommu_drv.driver, NULL,\n\t\t\t\t\t\tfwnode, viommu_match_node);\n\tput_device(dev);\n\n\treturn dev ? dev_to_virtio(dev)->priv : NULL;\n}\n\nstatic struct iommu_device *viommu_probe_device(struct device *dev)\n{\n\tint ret;\n\tstruct viommu_endpoint *vdev;\n\tstruct viommu_dev *viommu = NULL;\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (!fwspec || fwspec->ops != &viommu_ops)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tviommu = viommu_get_by_fwnode(fwspec->iommu_fwnode);\n\tif (!viommu)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tvdev = kzalloc(sizeof(*vdev), GFP_KERNEL);\n\tif (!vdev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvdev->dev = dev;\n\tvdev->viommu = viommu;\n\tINIT_LIST_HEAD(&vdev->resv_regions);\n\tdev_iommu_priv_set(dev, vdev);\n\n\tif (viommu->probe_size) {\n\t\t \n\t\tret = viommu_probe_endpoint(viommu, dev);\n\t\tif (ret)\n\t\t\tgoto err_free_dev;\n\t}\n\n\treturn &viommu->iommu;\n\nerr_free_dev:\n\tiommu_put_resv_regions(dev, &vdev->resv_regions);\n\tkfree(vdev);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic void viommu_probe_finalize(struct device *dev)\n{\n#ifndef CONFIG_ARCH_HAS_SETUP_DMA_OPS\n\t \n\tset_dma_ops(dev, NULL);\n\tiommu_setup_dma_ops(dev, 0, U64_MAX);\n#endif\n}\n\nstatic void viommu_release_device(struct device *dev)\n{\n\tstruct viommu_endpoint *vdev = dev_iommu_priv_get(dev);\n\n\tviommu_detach_dev(vdev);\n\tiommu_put_resv_regions(dev, &vdev->resv_regions);\n\tkfree(vdev);\n}\n\nstatic struct iommu_group *viommu_device_group(struct device *dev)\n{\n\tif (dev_is_pci(dev))\n\t\treturn pci_device_group(dev);\n\telse\n\t\treturn generic_device_group(dev);\n}\n\nstatic int viommu_of_xlate(struct device *dev, struct of_phandle_args *args)\n{\n\treturn iommu_fwspec_add_ids(dev, args->args, 1);\n}\n\nstatic bool viommu_capable(struct device *dev, enum iommu_cap cap)\n{\n\tswitch (cap) {\n\tcase IOMMU_CAP_CACHE_COHERENCY:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct iommu_ops viommu_ops = {\n\t.capable\t\t= viommu_capable,\n\t.domain_alloc\t\t= viommu_domain_alloc,\n\t.probe_device\t\t= viommu_probe_device,\n\t.probe_finalize\t\t= viommu_probe_finalize,\n\t.release_device\t\t= viommu_release_device,\n\t.device_group\t\t= viommu_device_group,\n\t.get_resv_regions\t= viommu_get_resv_regions,\n\t.of_xlate\t\t= viommu_of_xlate,\n\t.owner\t\t\t= THIS_MODULE,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t\t= viommu_attach_dev,\n\t\t.map_pages\t\t= viommu_map_pages,\n\t\t.unmap_pages\t\t= viommu_unmap_pages,\n\t\t.iova_to_phys\t\t= viommu_iova_to_phys,\n\t\t.iotlb_sync\t\t= viommu_iotlb_sync,\n\t\t.free\t\t\t= viommu_domain_free,\n\t}\n};\n\nstatic int viommu_init_vqs(struct viommu_dev *viommu)\n{\n\tstruct virtio_device *vdev = dev_to_virtio(viommu->dev);\n\tconst char *names[] = { \"request\", \"event\" };\n\tvq_callback_t *callbacks[] = {\n\t\tNULL,  \n\t\tviommu_event_handler,\n\t};\n\n\treturn virtio_find_vqs(vdev, VIOMMU_NR_VQS, viommu->vqs, callbacks,\n\t\t\t       names, NULL);\n}\n\nstatic int viommu_fill_evtq(struct viommu_dev *viommu)\n{\n\tint i, ret;\n\tstruct scatterlist sg[1];\n\tstruct viommu_event *evts;\n\tstruct virtqueue *vq = viommu->vqs[VIOMMU_EVENT_VQ];\n\tsize_t nr_evts = vq->num_free;\n\n\tviommu->evts = evts = devm_kmalloc_array(viommu->dev, nr_evts,\n\t\t\t\t\t\t sizeof(*evts), GFP_KERNEL);\n\tif (!evts)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nr_evts; i++) {\n\t\tsg_init_one(sg, &evts[i], sizeof(*evts));\n\t\tret = virtqueue_add_inbuf(vq, sg, 1, &evts[i], GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int viommu_probe(struct virtio_device *vdev)\n{\n\tstruct device *parent_dev = vdev->dev.parent;\n\tstruct viommu_dev *viommu = NULL;\n\tstruct device *dev = &vdev->dev;\n\tu64 input_start = 0;\n\tu64 input_end = -1UL;\n\tint ret;\n\n\tif (!virtio_has_feature(vdev, VIRTIO_F_VERSION_1) ||\n\t    !virtio_has_feature(vdev, VIRTIO_IOMMU_F_MAP_UNMAP))\n\t\treturn -ENODEV;\n\n\tviommu = devm_kzalloc(dev, sizeof(*viommu), GFP_KERNEL);\n\tif (!viommu)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&viommu->request_lock);\n\tida_init(&viommu->domain_ids);\n\tviommu->dev = dev;\n\tviommu->vdev = vdev;\n\tINIT_LIST_HEAD(&viommu->requests);\n\n\tret = viommu_init_vqs(viommu);\n\tif (ret)\n\t\treturn ret;\n\n\tvirtio_cread_le(vdev, struct virtio_iommu_config, page_size_mask,\n\t\t\t&viommu->pgsize_bitmap);\n\n\tif (!viommu->pgsize_bitmap) {\n\t\tret = -EINVAL;\n\t\tgoto err_free_vqs;\n\t}\n\n\tviommu->map_flags = VIRTIO_IOMMU_MAP_F_READ | VIRTIO_IOMMU_MAP_F_WRITE;\n\tviommu->last_domain = ~0U;\n\n\t \n\tvirtio_cread_le_feature(vdev, VIRTIO_IOMMU_F_INPUT_RANGE,\n\t\t\t\tstruct virtio_iommu_config, input_range.start,\n\t\t\t\t&input_start);\n\n\tvirtio_cread_le_feature(vdev, VIRTIO_IOMMU_F_INPUT_RANGE,\n\t\t\t\tstruct virtio_iommu_config, input_range.end,\n\t\t\t\t&input_end);\n\n\tvirtio_cread_le_feature(vdev, VIRTIO_IOMMU_F_DOMAIN_RANGE,\n\t\t\t\tstruct virtio_iommu_config, domain_range.start,\n\t\t\t\t&viommu->first_domain);\n\n\tvirtio_cread_le_feature(vdev, VIRTIO_IOMMU_F_DOMAIN_RANGE,\n\t\t\t\tstruct virtio_iommu_config, domain_range.end,\n\t\t\t\t&viommu->last_domain);\n\n\tvirtio_cread_le_feature(vdev, VIRTIO_IOMMU_F_PROBE,\n\t\t\t\tstruct virtio_iommu_config, probe_size,\n\t\t\t\t&viommu->probe_size);\n\n\tviommu->geometry = (struct iommu_domain_geometry) {\n\t\t.aperture_start\t= input_start,\n\t\t.aperture_end\t= input_end,\n\t\t.force_aperture\t= true,\n\t};\n\n\tif (virtio_has_feature(vdev, VIRTIO_IOMMU_F_MMIO))\n\t\tviommu->map_flags |= VIRTIO_IOMMU_MAP_F_MMIO;\n\n\tviommu_ops.pgsize_bitmap = viommu->pgsize_bitmap;\n\n\tvirtio_device_ready(vdev);\n\n\t \n\tret = viommu_fill_evtq(viommu);\n\tif (ret)\n\t\tgoto err_free_vqs;\n\n\tret = iommu_device_sysfs_add(&viommu->iommu, dev, NULL, \"%s\",\n\t\t\t\t     virtio_bus_name(vdev));\n\tif (ret)\n\t\tgoto err_free_vqs;\n\n\tiommu_device_register(&viommu->iommu, &viommu_ops, parent_dev);\n\n\tvdev->priv = viommu;\n\n\tdev_info(dev, \"input address: %u bits\\n\",\n\t\t order_base_2(viommu->geometry.aperture_end));\n\tdev_info(dev, \"page mask: %#llx\\n\", viommu->pgsize_bitmap);\n\n\treturn 0;\n\nerr_free_vqs:\n\tvdev->config->del_vqs(vdev);\n\n\treturn ret;\n}\n\nstatic void viommu_remove(struct virtio_device *vdev)\n{\n\tstruct viommu_dev *viommu = vdev->priv;\n\n\tiommu_device_sysfs_remove(&viommu->iommu);\n\tiommu_device_unregister(&viommu->iommu);\n\n\t \n\tvirtio_reset_device(vdev);\n\tvdev->config->del_vqs(vdev);\n\n\tdev_info(&vdev->dev, \"device removed\\n\");\n}\n\nstatic void viommu_config_changed(struct virtio_device *vdev)\n{\n\tdev_warn(&vdev->dev, \"config changed\\n\");\n}\n\nstatic unsigned int features[] = {\n\tVIRTIO_IOMMU_F_MAP_UNMAP,\n\tVIRTIO_IOMMU_F_INPUT_RANGE,\n\tVIRTIO_IOMMU_F_DOMAIN_RANGE,\n\tVIRTIO_IOMMU_F_PROBE,\n\tVIRTIO_IOMMU_F_MMIO,\n\tVIRTIO_IOMMU_F_BYPASS_CONFIG,\n};\n\nstatic struct virtio_device_id id_table[] = {\n\t{ VIRTIO_ID_IOMMU, VIRTIO_DEV_ANY_ID },\n\t{ 0 },\n};\nMODULE_DEVICE_TABLE(virtio, id_table);\n\nstatic struct virtio_driver virtio_iommu_drv = {\n\t.driver.name\t\t= KBUILD_MODNAME,\n\t.driver.owner\t\t= THIS_MODULE,\n\t.id_table\t\t= id_table,\n\t.feature_table\t\t= features,\n\t.feature_table_size\t= ARRAY_SIZE(features),\n\t.probe\t\t\t= viommu_probe,\n\t.remove\t\t\t= viommu_remove,\n\t.config_changed\t\t= viommu_config_changed,\n};\n\nmodule_virtio_driver(virtio_iommu_drv);\n\nMODULE_DESCRIPTION(\"Virtio IOMMU driver\");\nMODULE_AUTHOR(\"Jean-Philippe Brucker <jean-philippe.brucker@arm.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}