{
  "module_name": "exynos-iommu.c",
  "hash_id": "f5565d3b234a5d3bac0b0dd7a237c22f227620e59ba5dae75cc7149554f2c308",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iommu/exynos-iommu.c",
  "human_readable_source": "\n \n\n#ifdef CONFIG_EXYNOS_IOMMU_DEBUG\n#define DEBUG\n#endif\n\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/io.h>\n#include <linux/iommu.h>\n#include <linux/interrupt.h>\n#include <linux/kmemleak.h>\n#include <linux/list.h>\n#include <linux/of.h>\n#include <linux/of_platform.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n\ntypedef u32 sysmmu_iova_t;\ntypedef u32 sysmmu_pte_t;\n\n \n#define SECT_ORDER 20\n#define LPAGE_ORDER 16\n#define SPAGE_ORDER 12\n\n#define SECT_SIZE (1 << SECT_ORDER)\n#define LPAGE_SIZE (1 << LPAGE_ORDER)\n#define SPAGE_SIZE (1 << SPAGE_ORDER)\n\n#define SECT_MASK (~(SECT_SIZE - 1))\n#define LPAGE_MASK (~(LPAGE_SIZE - 1))\n#define SPAGE_MASK (~(SPAGE_SIZE - 1))\n\n#define lv1ent_fault(sent) ((*(sent) == ZERO_LV2LINK) || \\\n\t\t\t   ((*(sent) & 3) == 0) || ((*(sent) & 3) == 3))\n#define lv1ent_zero(sent) (*(sent) == ZERO_LV2LINK)\n#define lv1ent_page_zero(sent) ((*(sent) & 3) == 1)\n#define lv1ent_page(sent) ((*(sent) != ZERO_LV2LINK) && \\\n\t\t\t  ((*(sent) & 3) == 1))\n#define lv1ent_section(sent) ((*(sent) & 3) == 2)\n\n#define lv2ent_fault(pent) ((*(pent) & 3) == 0)\n#define lv2ent_small(pent) ((*(pent) & 2) == 2)\n#define lv2ent_large(pent) ((*(pent) & 3) == 1)\n\n \nstatic short PG_ENT_SHIFT = -1;\n#define SYSMMU_PG_ENT_SHIFT 0\n#define SYSMMU_V5_PG_ENT_SHIFT 4\n\nstatic const sysmmu_pte_t *LV1_PROT;\nstatic const sysmmu_pte_t SYSMMU_LV1_PROT[] = {\n\t((0 << 15) | (0 << 10)),  \n\t((1 << 15) | (1 << 10)),  \n\t((0 << 15) | (1 << 10)),  \n\t((0 << 15) | (1 << 10)),  \n};\nstatic const sysmmu_pte_t SYSMMU_V5_LV1_PROT[] = {\n\t(0 << 4),  \n\t(1 << 4),  \n\t(2 << 4),  \n\t(3 << 4),  \n};\n\nstatic const sysmmu_pte_t *LV2_PROT;\nstatic const sysmmu_pte_t SYSMMU_LV2_PROT[] = {\n\t((0 << 9) | (0 << 4)),  \n\t((1 << 9) | (1 << 4)),  \n\t((0 << 9) | (1 << 4)),  \n\t((0 << 9) | (1 << 4)),  \n};\nstatic const sysmmu_pte_t SYSMMU_V5_LV2_PROT[] = {\n\t(0 << 2),  \n\t(1 << 2),  \n\t(2 << 2),  \n\t(3 << 2),  \n};\n\n#define SYSMMU_SUPPORTED_PROT_BITS (IOMMU_READ | IOMMU_WRITE)\n\n#define sect_to_phys(ent) (((phys_addr_t) ent) << PG_ENT_SHIFT)\n#define section_phys(sent) (sect_to_phys(*(sent)) & SECT_MASK)\n#define section_offs(iova) (iova & (SECT_SIZE - 1))\n#define lpage_phys(pent) (sect_to_phys(*(pent)) & LPAGE_MASK)\n#define lpage_offs(iova) (iova & (LPAGE_SIZE - 1))\n#define spage_phys(pent) (sect_to_phys(*(pent)) & SPAGE_MASK)\n#define spage_offs(iova) (iova & (SPAGE_SIZE - 1))\n\n#define NUM_LV1ENTRIES 4096\n#define NUM_LV2ENTRIES (SECT_SIZE / SPAGE_SIZE)\n\nstatic u32 lv1ent_offset(sysmmu_iova_t iova)\n{\n\treturn iova >> SECT_ORDER;\n}\n\nstatic u32 lv2ent_offset(sysmmu_iova_t iova)\n{\n\treturn (iova >> SPAGE_ORDER) & (NUM_LV2ENTRIES - 1);\n}\n\n#define LV1TABLE_SIZE (NUM_LV1ENTRIES * sizeof(sysmmu_pte_t))\n#define LV2TABLE_SIZE (NUM_LV2ENTRIES * sizeof(sysmmu_pte_t))\n\n#define SPAGES_PER_LPAGE (LPAGE_SIZE / SPAGE_SIZE)\n#define lv2table_base(sent) (sect_to_phys(*(sent) & 0xFFFFFFC0))\n\n#define mk_lv1ent_sect(pa, prot) ((pa >> PG_ENT_SHIFT) | LV1_PROT[prot] | 2)\n#define mk_lv1ent_page(pa) ((pa >> PG_ENT_SHIFT) | 1)\n#define mk_lv2ent_lpage(pa, prot) ((pa >> PG_ENT_SHIFT) | LV2_PROT[prot] | 1)\n#define mk_lv2ent_spage(pa, prot) ((pa >> PG_ENT_SHIFT) | LV2_PROT[prot] | 2)\n\n#define CTRL_ENABLE\t0x5\n#define CTRL_BLOCK\t0x7\n#define CTRL_DISABLE\t0x0\n\n#define CFG_LRU\t\t0x1\n#define CFG_EAP\t\t(1 << 2)\n#define CFG_QOS(n)\t((n & 0xF) << 7)\n#define CFG_ACGEN\t(1 << 24)  \n#define CFG_SYSSEL\t(1 << 22)  \n#define CFG_FLPDCACHE\t(1 << 20)  \n\n#define CTRL_VM_ENABLE\t\t\tBIT(0)\n#define CTRL_VM_FAULT_MODE_STALL\tBIT(3)\n#define CAPA0_CAPA1_EXIST\t\tBIT(11)\n#define CAPA1_VCR_ENABLED\t\tBIT(14)\n\n \n#define REG_MMU_CTRL\t\t0x000\n#define REG_MMU_CFG\t\t0x004\n#define REG_MMU_STATUS\t\t0x008\n#define REG_MMU_VERSION\t\t0x034\n\n#define MMU_MAJ_VER(val)\t((val) >> 7)\n#define MMU_MIN_VER(val)\t((val) & 0x7F)\n#define MMU_RAW_VER(reg)\t(((reg) >> 21) & ((1 << 11) - 1))  \n\n#define MAKE_MMU_VER(maj, min)\t((((maj) & 0xF) << 7) | ((min) & 0x7F))\n\n \n#define REG_PAGE_FAULT_ADDR\t0x024\n#define REG_AW_FAULT_ADDR\t0x028\n#define REG_AR_FAULT_ADDR\t0x02C\n#define REG_DEFAULT_SLAVE_ADDR\t0x030\n\n \n#define REG_V5_FAULT_AR_VA\t0x070\n#define REG_V5_FAULT_AW_VA\t0x080\n\n \n#define REG_V7_CAPA0\t\t0x870\n#define REG_V7_CAPA1\t\t0x874\n#define REG_V7_CTRL_VM\t\t0x8000\n\n#define has_sysmmu(dev)\t\t(dev_iommu_priv_get(dev) != NULL)\n\nstatic struct device *dma_dev;\nstatic struct kmem_cache *lv2table_kmem_cache;\nstatic sysmmu_pte_t *zero_lv2_table;\n#define ZERO_LV2LINK mk_lv1ent_page(virt_to_phys(zero_lv2_table))\n\nstatic sysmmu_pte_t *section_entry(sysmmu_pte_t *pgtable, sysmmu_iova_t iova)\n{\n\treturn pgtable + lv1ent_offset(iova);\n}\n\nstatic sysmmu_pte_t *page_entry(sysmmu_pte_t *sent, sysmmu_iova_t iova)\n{\n\treturn (sysmmu_pte_t *)phys_to_virt(\n\t\t\t\tlv2table_base(sent)) + lv2ent_offset(iova);\n}\n\nstruct sysmmu_fault {\n\tsysmmu_iova_t addr;\t \n\tconst char *name;\t \n\tunsigned int type;\t \n};\n\nstruct sysmmu_v1_fault_info {\n\tunsigned short addr_reg;  \n\tconst char *name;\t \n\tunsigned int type;\t \n};\n\nstatic const struct sysmmu_v1_fault_info sysmmu_v1_faults[] = {\n\t{ REG_PAGE_FAULT_ADDR, \"PAGE\", IOMMU_FAULT_READ },\n\t{ REG_AR_FAULT_ADDR, \"MULTI-HIT\", IOMMU_FAULT_READ },\n\t{ REG_AW_FAULT_ADDR, \"MULTI-HIT\", IOMMU_FAULT_WRITE },\n\t{ REG_DEFAULT_SLAVE_ADDR, \"BUS ERROR\", IOMMU_FAULT_READ },\n\t{ REG_AR_FAULT_ADDR, \"SECURITY PROTECTION\", IOMMU_FAULT_READ },\n\t{ REG_AR_FAULT_ADDR, \"ACCESS PROTECTION\", IOMMU_FAULT_READ },\n\t{ REG_AW_FAULT_ADDR, \"SECURITY PROTECTION\", IOMMU_FAULT_WRITE },\n\t{ REG_AW_FAULT_ADDR, \"ACCESS PROTECTION\", IOMMU_FAULT_WRITE },\n};\n\n \nstatic const char * const sysmmu_v5_fault_names[] = {\n\t\"PTW\",\n\t\"PAGE\",\n\t\"MULTI-HIT\",\n\t\"ACCESS PROTECTION\",\n\t\"SECURITY PROTECTION\"\n};\n\nstatic const char * const sysmmu_v7_fault_names[] = {\n\t\"PTW\",\n\t\"PAGE\",\n\t\"ACCESS PROTECTION\",\n\t\"RESERVED\"\n};\n\n \nstruct exynos_iommu_owner {\n\tstruct list_head controllers;\t \n\tstruct iommu_domain *domain;\t \n\tstruct mutex rpm_lock;\t\t \n};\n\n \nstruct exynos_iommu_domain {\n\tstruct list_head clients;  \n\tsysmmu_pte_t *pgtable;\t \n\tshort *lv2entcnt;\t \n\tspinlock_t lock;\t \n\tspinlock_t pgtablelock;\t \n\tstruct iommu_domain domain;  \n};\n\nstruct sysmmu_drvdata;\n\n \nstruct sysmmu_variant {\n\tu32 pt_base;\t\t \n\tu32 flush_all;\t\t \n\tu32 flush_entry;\t \n\tu32 flush_range;\t \n\tu32 flush_start;\t \n\tu32 flush_end;\t\t \n\tu32 int_status;\t\t \n\tu32 int_clear;\t\t \n\tu32 fault_va;\t\t \n\tu32 fault_info;\t\t \n\n\tint (*get_fault_info)(struct sysmmu_drvdata *data, unsigned int itype,\n\t\t\t      struct sysmmu_fault *fault);\n};\n\n \nstruct sysmmu_drvdata {\n\tstruct device *sysmmu;\t\t \n\tstruct device *master;\t\t \n\tstruct device_link *link;\t \n\tvoid __iomem *sfrbase;\t\t \n\tstruct clk *clk;\t\t \n\tstruct clk *aclk;\t\t \n\tstruct clk *pclk;\t\t \n\tstruct clk *clk_master;\t\t \n\tspinlock_t lock;\t\t \n\tbool active;\t\t\t \n\tstruct exynos_iommu_domain *domain;  \n\tstruct list_head domain_node;\t \n\tstruct list_head owner_node;\t \n\tphys_addr_t pgtable;\t\t \n\tunsigned int version;\t\t \n\n\tstruct iommu_device iommu;\t \n\tconst struct sysmmu_variant *variant;  \n\n\t \n\tbool has_vcr;\t\t\t \n};\n\n#define SYSMMU_REG(data, reg) ((data)->sfrbase + (data)->variant->reg)\n\nstatic int exynos_sysmmu_v1_get_fault_info(struct sysmmu_drvdata *data,\n\t\t\t\t\t   unsigned int itype,\n\t\t\t\t\t   struct sysmmu_fault *fault)\n{\n\tconst struct sysmmu_v1_fault_info *finfo;\n\n\tif (itype >= ARRAY_SIZE(sysmmu_v1_faults))\n\t\treturn -ENXIO;\n\n\tfinfo = &sysmmu_v1_faults[itype];\n\tfault->addr = readl(data->sfrbase + finfo->addr_reg);\n\tfault->name = finfo->name;\n\tfault->type = finfo->type;\n\n\treturn 0;\n}\n\nstatic int exynos_sysmmu_v5_get_fault_info(struct sysmmu_drvdata *data,\n\t\t\t\t\t   unsigned int itype,\n\t\t\t\t\t   struct sysmmu_fault *fault)\n{\n\tunsigned int addr_reg;\n\n\tif (itype < ARRAY_SIZE(sysmmu_v5_fault_names)) {\n\t\tfault->type = IOMMU_FAULT_READ;\n\t\taddr_reg = REG_V5_FAULT_AR_VA;\n\t} else if (itype >= 16 && itype <= 20) {\n\t\tfault->type = IOMMU_FAULT_WRITE;\n\t\taddr_reg = REG_V5_FAULT_AW_VA;\n\t\titype -= 16;\n\t} else {\n\t\treturn -ENXIO;\n\t}\n\n\tfault->name = sysmmu_v5_fault_names[itype];\n\tfault->addr = readl(data->sfrbase + addr_reg);\n\n\treturn 0;\n}\n\nstatic int exynos_sysmmu_v7_get_fault_info(struct sysmmu_drvdata *data,\n\t\t\t\t\t   unsigned int itype,\n\t\t\t\t\t   struct sysmmu_fault *fault)\n{\n\tu32 info = readl(SYSMMU_REG(data, fault_info));\n\n\tfault->addr = readl(SYSMMU_REG(data, fault_va));\n\tfault->name = sysmmu_v7_fault_names[itype % 4];\n\tfault->type = (info & BIT(20)) ? IOMMU_FAULT_WRITE : IOMMU_FAULT_READ;\n\n\treturn 0;\n}\n\n \nstatic const struct sysmmu_variant sysmmu_v1_variant = {\n\t.flush_all\t= 0x0c,\n\t.flush_entry\t= 0x10,\n\t.pt_base\t= 0x14,\n\t.int_status\t= 0x18,\n\t.int_clear\t= 0x1c,\n\n\t.get_fault_info\t= exynos_sysmmu_v1_get_fault_info,\n};\n\n \nstatic const struct sysmmu_variant sysmmu_v5_variant = {\n\t.pt_base\t= 0x0c,\n\t.flush_all\t= 0x10,\n\t.flush_entry\t= 0x14,\n\t.flush_range\t= 0x18,\n\t.flush_start\t= 0x20,\n\t.flush_end\t= 0x24,\n\t.int_status\t= 0x60,\n\t.int_clear\t= 0x64,\n\n\t.get_fault_info\t= exynos_sysmmu_v5_get_fault_info,\n};\n\n \nstatic const struct sysmmu_variant sysmmu_v7_variant = {\n\t.pt_base\t= 0x0c,\n\t.flush_all\t= 0x10,\n\t.flush_entry\t= 0x14,\n\t.flush_range\t= 0x18,\n\t.flush_start\t= 0x20,\n\t.flush_end\t= 0x24,\n\t.int_status\t= 0x60,\n\t.int_clear\t= 0x64,\n\t.fault_va\t= 0x70,\n\t.fault_info\t= 0x78,\n\n\t.get_fault_info\t= exynos_sysmmu_v7_get_fault_info,\n};\n\n \nstatic const struct sysmmu_variant sysmmu_v7_vm_variant = {\n\t.pt_base\t= 0x800c,\n\t.flush_all\t= 0x8010,\n\t.flush_entry\t= 0x8014,\n\t.flush_range\t= 0x8018,\n\t.flush_start\t= 0x8020,\n\t.flush_end\t= 0x8024,\n\t.int_status\t= 0x60,\n\t.int_clear\t= 0x64,\n\t.fault_va\t= 0x1000,\n\t.fault_info\t= 0x1004,\n\n\t.get_fault_info\t= exynos_sysmmu_v7_get_fault_info,\n};\n\nstatic struct exynos_iommu_domain *to_exynos_domain(struct iommu_domain *dom)\n{\n\treturn container_of(dom, struct exynos_iommu_domain, domain);\n}\n\nstatic void sysmmu_unblock(struct sysmmu_drvdata *data)\n{\n\twritel(CTRL_ENABLE, data->sfrbase + REG_MMU_CTRL);\n}\n\nstatic bool sysmmu_block(struct sysmmu_drvdata *data)\n{\n\tint i = 120;\n\n\twritel(CTRL_BLOCK, data->sfrbase + REG_MMU_CTRL);\n\twhile ((i > 0) && !(readl(data->sfrbase + REG_MMU_STATUS) & 1))\n\t\t--i;\n\n\tif (!(readl(data->sfrbase + REG_MMU_STATUS) & 1)) {\n\t\tsysmmu_unblock(data);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __sysmmu_tlb_invalidate(struct sysmmu_drvdata *data)\n{\n\twritel(0x1, SYSMMU_REG(data, flush_all));\n}\n\nstatic void __sysmmu_tlb_invalidate_entry(struct sysmmu_drvdata *data,\n\t\t\t\tsysmmu_iova_t iova, unsigned int num_inv)\n{\n\tunsigned int i;\n\n\tif (MMU_MAJ_VER(data->version) < 5 || num_inv == 1) {\n\t\tfor (i = 0; i < num_inv; i++) {\n\t\t\twritel((iova & SPAGE_MASK) | 1,\n\t\t\t       SYSMMU_REG(data, flush_entry));\n\t\t\tiova += SPAGE_SIZE;\n\t\t}\n\t} else {\n\t\twritel(iova & SPAGE_MASK, SYSMMU_REG(data, flush_start));\n\t\twritel((iova & SPAGE_MASK) + (num_inv - 1) * SPAGE_SIZE,\n\t\t       SYSMMU_REG(data, flush_end));\n\t\twritel(0x1, SYSMMU_REG(data, flush_range));\n\t}\n}\n\nstatic void __sysmmu_set_ptbase(struct sysmmu_drvdata *data, phys_addr_t pgd)\n{\n\tu32 pt_base;\n\n\tif (MMU_MAJ_VER(data->version) < 5)\n\t\tpt_base = pgd;\n\telse\n\t\tpt_base = pgd >> SPAGE_ORDER;\n\n\twritel(pt_base, SYSMMU_REG(data, pt_base));\n\t__sysmmu_tlb_invalidate(data);\n}\n\nstatic void __sysmmu_enable_clocks(struct sysmmu_drvdata *data)\n{\n\tBUG_ON(clk_prepare_enable(data->clk_master));\n\tBUG_ON(clk_prepare_enable(data->clk));\n\tBUG_ON(clk_prepare_enable(data->pclk));\n\tBUG_ON(clk_prepare_enable(data->aclk));\n}\n\nstatic void __sysmmu_disable_clocks(struct sysmmu_drvdata *data)\n{\n\tclk_disable_unprepare(data->aclk);\n\tclk_disable_unprepare(data->pclk);\n\tclk_disable_unprepare(data->clk);\n\tclk_disable_unprepare(data->clk_master);\n}\n\nstatic bool __sysmmu_has_capa1(struct sysmmu_drvdata *data)\n{\n\tu32 capa0 = readl(data->sfrbase + REG_V7_CAPA0);\n\n\treturn capa0 & CAPA0_CAPA1_EXIST;\n}\n\nstatic void __sysmmu_get_vcr(struct sysmmu_drvdata *data)\n{\n\tu32 capa1 = readl(data->sfrbase + REG_V7_CAPA1);\n\n\tdata->has_vcr = capa1 & CAPA1_VCR_ENABLED;\n}\n\nstatic void __sysmmu_get_version(struct sysmmu_drvdata *data)\n{\n\tu32 ver;\n\n\t__sysmmu_enable_clocks(data);\n\n\tver = readl(data->sfrbase + REG_MMU_VERSION);\n\n\t \n\tif (ver == 0x80000001u)\n\t\tdata->version = MAKE_MMU_VER(1, 0);\n\telse\n\t\tdata->version = MMU_RAW_VER(ver);\n\n\tdev_dbg(data->sysmmu, \"hardware version: %d.%d\\n\",\n\t\tMMU_MAJ_VER(data->version), MMU_MIN_VER(data->version));\n\n\tif (MMU_MAJ_VER(data->version) < 5) {\n\t\tdata->variant = &sysmmu_v1_variant;\n\t} else if (MMU_MAJ_VER(data->version) < 7) {\n\t\tdata->variant = &sysmmu_v5_variant;\n\t} else {\n\t\tif (__sysmmu_has_capa1(data))\n\t\t\t__sysmmu_get_vcr(data);\n\t\tif (data->has_vcr)\n\t\t\tdata->variant = &sysmmu_v7_vm_variant;\n\t\telse\n\t\t\tdata->variant = &sysmmu_v7_variant;\n\t}\n\n\t__sysmmu_disable_clocks(data);\n}\n\nstatic void show_fault_information(struct sysmmu_drvdata *data,\n\t\t\t\t   const struct sysmmu_fault *fault)\n{\n\tsysmmu_pte_t *ent;\n\n\tdev_err(data->sysmmu, \"%s: [%s] %s FAULT occurred at %#x\\n\",\n\t\tdev_name(data->master),\n\t\tfault->type == IOMMU_FAULT_READ ? \"READ\" : \"WRITE\",\n\t\tfault->name, fault->addr);\n\tdev_dbg(data->sysmmu, \"Page table base: %pa\\n\", &data->pgtable);\n\tent = section_entry(phys_to_virt(data->pgtable), fault->addr);\n\tdev_dbg(data->sysmmu, \"\\tLv1 entry: %#x\\n\", *ent);\n\tif (lv1ent_page(ent)) {\n\t\tent = page_entry(ent, fault->addr);\n\t\tdev_dbg(data->sysmmu, \"\\t Lv2 entry: %#x\\n\", *ent);\n\t}\n}\n\nstatic irqreturn_t exynos_sysmmu_irq(int irq, void *dev_id)\n{\n\tstruct sysmmu_drvdata *data = dev_id;\n\tunsigned int itype;\n\tstruct sysmmu_fault fault;\n\tint ret = -ENOSYS;\n\n\tWARN_ON(!data->active);\n\n\tspin_lock(&data->lock);\n\tclk_enable(data->clk_master);\n\n\titype = __ffs(readl(SYSMMU_REG(data, int_status)));\n\tret = data->variant->get_fault_info(data, itype, &fault);\n\tif (ret) {\n\t\tdev_err(data->sysmmu, \"Unhandled interrupt bit %u\\n\", itype);\n\t\tgoto out;\n\t}\n\tshow_fault_information(data, &fault);\n\n\tif (data->domain) {\n\t\tret = report_iommu_fault(&data->domain->domain, data->master,\n\t\t\t\t\t fault.addr, fault.type);\n\t}\n\tif (ret)\n\t\tpanic(\"Unrecoverable System MMU Fault!\");\n\nout:\n\twritel(1 << itype, SYSMMU_REG(data, int_clear));\n\n\t \n\tsysmmu_unblock(data);\n\tclk_disable(data->clk_master);\n\tspin_unlock(&data->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void __sysmmu_disable(struct sysmmu_drvdata *data)\n{\n\tunsigned long flags;\n\n\tclk_enable(data->clk_master);\n\n\tspin_lock_irqsave(&data->lock, flags);\n\twritel(CTRL_DISABLE, data->sfrbase + REG_MMU_CTRL);\n\twritel(0, data->sfrbase + REG_MMU_CFG);\n\tdata->active = false;\n\tspin_unlock_irqrestore(&data->lock, flags);\n\n\t__sysmmu_disable_clocks(data);\n}\n\nstatic void __sysmmu_init_config(struct sysmmu_drvdata *data)\n{\n\tunsigned int cfg;\n\n\tif (data->version <= MAKE_MMU_VER(3, 1))\n\t\tcfg = CFG_LRU | CFG_QOS(15);\n\telse if (data->version <= MAKE_MMU_VER(3, 2))\n\t\tcfg = CFG_LRU | CFG_QOS(15) | CFG_FLPDCACHE | CFG_SYSSEL;\n\telse\n\t\tcfg = CFG_QOS(15) | CFG_FLPDCACHE | CFG_ACGEN;\n\n\tcfg |= CFG_EAP;  \n\n\twritel(cfg, data->sfrbase + REG_MMU_CFG);\n}\n\nstatic void __sysmmu_enable_vid(struct sysmmu_drvdata *data)\n{\n\tu32 ctrl;\n\n\tif (MMU_MAJ_VER(data->version) < 7 || !data->has_vcr)\n\t\treturn;\n\n\tctrl = readl(data->sfrbase + REG_V7_CTRL_VM);\n\tctrl |= CTRL_VM_ENABLE | CTRL_VM_FAULT_MODE_STALL;\n\twritel(ctrl, data->sfrbase + REG_V7_CTRL_VM);\n}\n\nstatic void __sysmmu_enable(struct sysmmu_drvdata *data)\n{\n\tunsigned long flags;\n\n\t__sysmmu_enable_clocks(data);\n\n\tspin_lock_irqsave(&data->lock, flags);\n\twritel(CTRL_BLOCK, data->sfrbase + REG_MMU_CTRL);\n\t__sysmmu_init_config(data);\n\t__sysmmu_set_ptbase(data, data->pgtable);\n\t__sysmmu_enable_vid(data);\n\twritel(CTRL_ENABLE, data->sfrbase + REG_MMU_CTRL);\n\tdata->active = true;\n\tspin_unlock_irqrestore(&data->lock, flags);\n\n\t \n\tclk_disable(data->clk_master);\n}\n\nstatic void sysmmu_tlb_invalidate_flpdcache(struct sysmmu_drvdata *data,\n\t\t\t\t\t    sysmmu_iova_t iova)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&data->lock, flags);\n\tif (data->active && data->version >= MAKE_MMU_VER(3, 3)) {\n\t\tclk_enable(data->clk_master);\n\t\tif (sysmmu_block(data)) {\n\t\t\tif (data->version >= MAKE_MMU_VER(5, 0))\n\t\t\t\t__sysmmu_tlb_invalidate(data);\n\t\t\telse\n\t\t\t\t__sysmmu_tlb_invalidate_entry(data, iova, 1);\n\t\t\tsysmmu_unblock(data);\n\t\t}\n\t\tclk_disable(data->clk_master);\n\t}\n\tspin_unlock_irqrestore(&data->lock, flags);\n}\n\nstatic void sysmmu_tlb_invalidate_entry(struct sysmmu_drvdata *data,\n\t\t\t\t\tsysmmu_iova_t iova, size_t size)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&data->lock, flags);\n\tif (data->active) {\n\t\tunsigned int num_inv = 1;\n\n\t\tclk_enable(data->clk_master);\n\n\t\t \n\t\tif (MMU_MAJ_VER(data->version) == 2)\n\t\t\tnum_inv = min_t(unsigned int, size / SPAGE_SIZE, 64);\n\n\t\tif (sysmmu_block(data)) {\n\t\t\t__sysmmu_tlb_invalidate_entry(data, iova, num_inv);\n\t\t\tsysmmu_unblock(data);\n\t\t}\n\t\tclk_disable(data->clk_master);\n\t}\n\tspin_unlock_irqrestore(&data->lock, flags);\n}\n\nstatic const struct iommu_ops exynos_iommu_ops;\n\nstatic int exynos_sysmmu_probe(struct platform_device *pdev)\n{\n\tint irq, ret;\n\tstruct device *dev = &pdev->dev;\n\tstruct sysmmu_drvdata *data;\n\tstruct resource *res;\n\n\tdata = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tdata->sfrbase = devm_ioremap_resource(dev, res);\n\tif (IS_ERR(data->sfrbase))\n\t\treturn PTR_ERR(data->sfrbase);\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq <= 0)\n\t\treturn irq;\n\n\tret = devm_request_irq(dev, irq, exynos_sysmmu_irq, 0,\n\t\t\t\tdev_name(dev), data);\n\tif (ret) {\n\t\tdev_err(dev, \"Unabled to register handler of irq %d\\n\", irq);\n\t\treturn ret;\n\t}\n\n\tdata->clk = devm_clk_get_optional(dev, \"sysmmu\");\n\tif (IS_ERR(data->clk))\n\t\treturn PTR_ERR(data->clk);\n\n\tdata->aclk = devm_clk_get_optional(dev, \"aclk\");\n\tif (IS_ERR(data->aclk))\n\t\treturn PTR_ERR(data->aclk);\n\n\tdata->pclk = devm_clk_get_optional(dev, \"pclk\");\n\tif (IS_ERR(data->pclk))\n\t\treturn PTR_ERR(data->pclk);\n\n\tif (!data->clk && (!data->aclk || !data->pclk)) {\n\t\tdev_err(dev, \"Failed to get device clock(s)!\\n\");\n\t\treturn -ENOSYS;\n\t}\n\n\tdata->clk_master = devm_clk_get_optional(dev, \"master\");\n\tif (IS_ERR(data->clk_master))\n\t\treturn PTR_ERR(data->clk_master);\n\n\tdata->sysmmu = dev;\n\tspin_lock_init(&data->lock);\n\n\t__sysmmu_get_version(data);\n\n\tret = iommu_device_sysfs_add(&data->iommu, &pdev->dev, NULL,\n\t\t\t\t     dev_name(data->sysmmu));\n\tif (ret)\n\t\treturn ret;\n\n\tplatform_set_drvdata(pdev, data);\n\n\tif (PG_ENT_SHIFT < 0) {\n\t\tif (MMU_MAJ_VER(data->version) < 5) {\n\t\t\tPG_ENT_SHIFT = SYSMMU_PG_ENT_SHIFT;\n\t\t\tLV1_PROT = SYSMMU_LV1_PROT;\n\t\t\tLV2_PROT = SYSMMU_LV2_PROT;\n\t\t} else {\n\t\t\tPG_ENT_SHIFT = SYSMMU_V5_PG_ENT_SHIFT;\n\t\t\tLV1_PROT = SYSMMU_V5_LV1_PROT;\n\t\t\tLV2_PROT = SYSMMU_V5_LV2_PROT;\n\t\t}\n\t}\n\n\tif (MMU_MAJ_VER(data->version) >= 5) {\n\t\tret = dma_set_mask(dev, DMA_BIT_MASK(36));\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Unable to set DMA mask: %d\\n\", ret);\n\t\t\tgoto err_dma_set_mask;\n\t\t}\n\t}\n\n\t \n\tif (!dma_dev)\n\t\tdma_dev = &pdev->dev;\n\n\tpm_runtime_enable(dev);\n\n\tret = iommu_device_register(&data->iommu, &exynos_iommu_ops, dev);\n\tif (ret)\n\t\tgoto err_dma_set_mask;\n\n\treturn 0;\n\nerr_dma_set_mask:\n\tiommu_device_sysfs_remove(&data->iommu);\n\treturn ret;\n}\n\nstatic int __maybe_unused exynos_sysmmu_suspend(struct device *dev)\n{\n\tstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\n\tstruct device *master = data->master;\n\n\tif (master) {\n\t\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(master);\n\n\t\tmutex_lock(&owner->rpm_lock);\n\t\tif (data->domain) {\n\t\t\tdev_dbg(data->sysmmu, \"saving state\\n\");\n\t\t\t__sysmmu_disable(data);\n\t\t}\n\t\tmutex_unlock(&owner->rpm_lock);\n\t}\n\treturn 0;\n}\n\nstatic int __maybe_unused exynos_sysmmu_resume(struct device *dev)\n{\n\tstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\n\tstruct device *master = data->master;\n\n\tif (master) {\n\t\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(master);\n\n\t\tmutex_lock(&owner->rpm_lock);\n\t\tif (data->domain) {\n\t\t\tdev_dbg(data->sysmmu, \"restoring state\\n\");\n\t\t\t__sysmmu_enable(data);\n\t\t}\n\t\tmutex_unlock(&owner->rpm_lock);\n\t}\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops sysmmu_pm_ops = {\n\tSET_RUNTIME_PM_OPS(exynos_sysmmu_suspend, exynos_sysmmu_resume, NULL)\n\tSET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,\n\t\t\t\tpm_runtime_force_resume)\n};\n\nstatic const struct of_device_id sysmmu_of_match[] = {\n\t{ .compatible\t= \"samsung,exynos-sysmmu\", },\n\t{ },\n};\n\nstatic struct platform_driver exynos_sysmmu_driver __refdata = {\n\t.probe\t= exynos_sysmmu_probe,\n\t.driver\t= {\n\t\t.name\t\t= \"exynos-sysmmu\",\n\t\t.of_match_table\t= sysmmu_of_match,\n\t\t.pm\t\t= &sysmmu_pm_ops,\n\t\t.suppress_bind_attrs = true,\n\t}\n};\n\nstatic inline void exynos_iommu_set_pte(sysmmu_pte_t *ent, sysmmu_pte_t val)\n{\n\tdma_sync_single_for_cpu(dma_dev, virt_to_phys(ent), sizeof(*ent),\n\t\t\t\tDMA_TO_DEVICE);\n\t*ent = cpu_to_le32(val);\n\tdma_sync_single_for_device(dma_dev, virt_to_phys(ent), sizeof(*ent),\n\t\t\t\t   DMA_TO_DEVICE);\n}\n\nstatic struct iommu_domain *exynos_iommu_domain_alloc(unsigned type)\n{\n\tstruct exynos_iommu_domain *domain;\n\tdma_addr_t handle;\n\tint i;\n\n\t \n\tBUG_ON(PG_ENT_SHIFT < 0 || !dma_dev);\n\n\tif (type != IOMMU_DOMAIN_DMA && type != IOMMU_DOMAIN_UNMANAGED)\n\t\treturn NULL;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->pgtable = (sysmmu_pte_t *)__get_free_pages(GFP_KERNEL, 2);\n\tif (!domain->pgtable)\n\t\tgoto err_pgtable;\n\n\tdomain->lv2entcnt = (short *)__get_free_pages(GFP_KERNEL | __GFP_ZERO, 1);\n\tif (!domain->lv2entcnt)\n\t\tgoto err_counter;\n\n\t \n\tfor (i = 0; i < NUM_LV1ENTRIES; i++)\n\t\tdomain->pgtable[i] = ZERO_LV2LINK;\n\n\thandle = dma_map_single(dma_dev, domain->pgtable, LV1TABLE_SIZE,\n\t\t\t\tDMA_TO_DEVICE);\n\t \n\tBUG_ON(handle != virt_to_phys(domain->pgtable));\n\tif (dma_mapping_error(dma_dev, handle))\n\t\tgoto err_lv2ent;\n\n\tspin_lock_init(&domain->lock);\n\tspin_lock_init(&domain->pgtablelock);\n\tINIT_LIST_HEAD(&domain->clients);\n\n\tdomain->domain.geometry.aperture_start = 0;\n\tdomain->domain.geometry.aperture_end   = ~0UL;\n\tdomain->domain.geometry.force_aperture = true;\n\n\treturn &domain->domain;\n\nerr_lv2ent:\n\tfree_pages((unsigned long)domain->lv2entcnt, 1);\nerr_counter:\n\tfree_pages((unsigned long)domain->pgtable, 2);\nerr_pgtable:\n\tkfree(domain);\n\treturn NULL;\n}\n\nstatic void exynos_iommu_domain_free(struct iommu_domain *iommu_domain)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tstruct sysmmu_drvdata *data, *next;\n\tunsigned long flags;\n\tint i;\n\n\tWARN_ON(!list_empty(&domain->clients));\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\n\t\tspin_lock(&data->lock);\n\t\t__sysmmu_disable(data);\n\t\tdata->pgtable = 0;\n\t\tdata->domain = NULL;\n\t\tlist_del_init(&data->domain_node);\n\t\tspin_unlock(&data->lock);\n\t}\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\tdma_unmap_single(dma_dev, virt_to_phys(domain->pgtable), LV1TABLE_SIZE,\n\t\t\t DMA_TO_DEVICE);\n\n\tfor (i = 0; i < NUM_LV1ENTRIES; i++)\n\t\tif (lv1ent_page(domain->pgtable + i)) {\n\t\t\tphys_addr_t base = lv2table_base(domain->pgtable + i);\n\n\t\t\tdma_unmap_single(dma_dev, base, LV2TABLE_SIZE,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tkmem_cache_free(lv2table_kmem_cache,\n\t\t\t\t\tphys_to_virt(base));\n\t\t}\n\n\tfree_pages((unsigned long)domain->pgtable, 2);\n\tfree_pages((unsigned long)domain->lv2entcnt, 1);\n\tkfree(domain);\n}\n\nstatic void exynos_iommu_detach_device(struct iommu_domain *iommu_domain,\n\t\t\t\t    struct device *dev)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\tphys_addr_t pagetable = virt_to_phys(domain->pgtable);\n\tstruct sysmmu_drvdata *data, *next;\n\tunsigned long flags;\n\n\tif (!has_sysmmu(dev) || owner->domain != iommu_domain)\n\t\treturn;\n\n\tmutex_lock(&owner->rpm_lock);\n\n\tlist_for_each_entry(data, &owner->controllers, owner_node) {\n\t\tpm_runtime_get_noresume(data->sysmmu);\n\t\tif (pm_runtime_active(data->sysmmu))\n\t\t\t__sysmmu_disable(data);\n\t\tpm_runtime_put(data->sysmmu);\n\t}\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\n\t\tspin_lock(&data->lock);\n\t\tdata->pgtable = 0;\n\t\tdata->domain = NULL;\n\t\tlist_del_init(&data->domain_node);\n\t\tspin_unlock(&data->lock);\n\t}\n\towner->domain = NULL;\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\tmutex_unlock(&owner->rpm_lock);\n\n\tdev_dbg(dev, \"%s: Detached IOMMU with pgtable %pa\\n\", __func__,\n\t\t&pagetable);\n}\n\nstatic int exynos_iommu_attach_device(struct iommu_domain *iommu_domain,\n\t\t\t\t   struct device *dev)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\tstruct sysmmu_drvdata *data;\n\tphys_addr_t pagetable = virt_to_phys(domain->pgtable);\n\tunsigned long flags;\n\n\tif (!has_sysmmu(dev))\n\t\treturn -ENODEV;\n\n\tif (owner->domain)\n\t\texynos_iommu_detach_device(owner->domain, dev);\n\n\tmutex_lock(&owner->rpm_lock);\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\tlist_for_each_entry(data, &owner->controllers, owner_node) {\n\t\tspin_lock(&data->lock);\n\t\tdata->pgtable = pagetable;\n\t\tdata->domain = domain;\n\t\tlist_add_tail(&data->domain_node, &domain->clients);\n\t\tspin_unlock(&data->lock);\n\t}\n\towner->domain = iommu_domain;\n\tspin_unlock_irqrestore(&domain->lock, flags);\n\n\tlist_for_each_entry(data, &owner->controllers, owner_node) {\n\t\tpm_runtime_get_noresume(data->sysmmu);\n\t\tif (pm_runtime_active(data->sysmmu))\n\t\t\t__sysmmu_enable(data);\n\t\tpm_runtime_put(data->sysmmu);\n\t}\n\n\tmutex_unlock(&owner->rpm_lock);\n\n\tdev_dbg(dev, \"%s: Attached IOMMU with pgtable %pa\\n\", __func__,\n\t\t&pagetable);\n\n\treturn 0;\n}\n\nstatic sysmmu_pte_t *alloc_lv2entry(struct exynos_iommu_domain *domain,\n\t\tsysmmu_pte_t *sent, sysmmu_iova_t iova, short *pgcounter)\n{\n\tif (lv1ent_section(sent)) {\n\t\tWARN(1, \"Trying mapping on %#08x mapped with 1MiB page\", iova);\n\t\treturn ERR_PTR(-EADDRINUSE);\n\t}\n\n\tif (lv1ent_fault(sent)) {\n\t\tdma_addr_t handle;\n\t\tsysmmu_pte_t *pent;\n\t\tbool need_flush_flpd_cache = lv1ent_zero(sent);\n\n\t\tpent = kmem_cache_zalloc(lv2table_kmem_cache, GFP_ATOMIC);\n\t\tBUG_ON((uintptr_t)pent & (LV2TABLE_SIZE - 1));\n\t\tif (!pent)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\n\t\texynos_iommu_set_pte(sent, mk_lv1ent_page(virt_to_phys(pent)));\n\t\tkmemleak_ignore(pent);\n\t\t*pgcounter = NUM_LV2ENTRIES;\n\t\thandle = dma_map_single(dma_dev, pent, LV2TABLE_SIZE,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dma_dev, handle)) {\n\t\t\tkmem_cache_free(lv2table_kmem_cache, pent);\n\t\t\treturn ERR_PTR(-EADDRINUSE);\n\t\t}\n\n\t\t \n\t\tif (need_flush_flpd_cache) {\n\t\t\tstruct sysmmu_drvdata *data;\n\n\t\t\tspin_lock(&domain->lock);\n\t\t\tlist_for_each_entry(data, &domain->clients, domain_node)\n\t\t\t\tsysmmu_tlb_invalidate_flpdcache(data, iova);\n\t\t\tspin_unlock(&domain->lock);\n\t\t}\n\t}\n\n\treturn page_entry(sent, iova);\n}\n\nstatic int lv1set_section(struct exynos_iommu_domain *domain,\n\t\t\t  sysmmu_pte_t *sent, sysmmu_iova_t iova,\n\t\t\t  phys_addr_t paddr, int prot, short *pgcnt)\n{\n\tif (lv1ent_section(sent)) {\n\t\tWARN(1, \"Trying mapping on 1MiB@%#08x that is mapped\",\n\t\t\tiova);\n\t\treturn -EADDRINUSE;\n\t}\n\n\tif (lv1ent_page(sent)) {\n\t\tif (*pgcnt != NUM_LV2ENTRIES) {\n\t\t\tWARN(1, \"Trying mapping on 1MiB@%#08x that is mapped\",\n\t\t\t\tiova);\n\t\t\treturn -EADDRINUSE;\n\t\t}\n\n\t\tkmem_cache_free(lv2table_kmem_cache, page_entry(sent, 0));\n\t\t*pgcnt = 0;\n\t}\n\n\texynos_iommu_set_pte(sent, mk_lv1ent_sect(paddr, prot));\n\n\tspin_lock(&domain->lock);\n\tif (lv1ent_page_zero(sent)) {\n\t\tstruct sysmmu_drvdata *data;\n\t\t \n\t\tlist_for_each_entry(data, &domain->clients, domain_node)\n\t\t\tsysmmu_tlb_invalidate_flpdcache(data, iova);\n\t}\n\tspin_unlock(&domain->lock);\n\n\treturn 0;\n}\n\nstatic int lv2set_page(sysmmu_pte_t *pent, phys_addr_t paddr, size_t size,\n\t\t       int prot, short *pgcnt)\n{\n\tif (size == SPAGE_SIZE) {\n\t\tif (WARN_ON(!lv2ent_fault(pent)))\n\t\t\treturn -EADDRINUSE;\n\n\t\texynos_iommu_set_pte(pent, mk_lv2ent_spage(paddr, prot));\n\t\t*pgcnt -= 1;\n\t} else {  \n\t\tint i;\n\t\tdma_addr_t pent_base = virt_to_phys(pent);\n\n\t\tdma_sync_single_for_cpu(dma_dev, pent_base,\n\t\t\t\t\tsizeof(*pent) * SPAGES_PER_LPAGE,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\tfor (i = 0; i < SPAGES_PER_LPAGE; i++, pent++) {\n\t\t\tif (WARN_ON(!lv2ent_fault(pent))) {\n\t\t\t\tif (i > 0)\n\t\t\t\t\tmemset(pent - i, 0, sizeof(*pent) * i);\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\n\t\t\t*pent = mk_lv2ent_lpage(paddr, prot);\n\t\t}\n\t\tdma_sync_single_for_device(dma_dev, pent_base,\n\t\t\t\t\t   sizeof(*pent) * SPAGES_PER_LPAGE,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\t*pgcnt -= SPAGES_PER_LPAGE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int exynos_iommu_map(struct iommu_domain *iommu_domain,\n\t\t\t    unsigned long l_iova, phys_addr_t paddr, size_t size,\n\t\t\t    int prot, gfp_t gfp)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tsysmmu_pte_t *entry;\n\tsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\n\tunsigned long flags;\n\tint ret = -ENOMEM;\n\n\tBUG_ON(domain->pgtable == NULL);\n\tprot &= SYSMMU_SUPPORTED_PROT_BITS;\n\n\tspin_lock_irqsave(&domain->pgtablelock, flags);\n\n\tentry = section_entry(domain->pgtable, iova);\n\n\tif (size == SECT_SIZE) {\n\t\tret = lv1set_section(domain, entry, iova, paddr, prot,\n\t\t\t\t     &domain->lv2entcnt[lv1ent_offset(iova)]);\n\t} else {\n\t\tsysmmu_pte_t *pent;\n\n\t\tpent = alloc_lv2entry(domain, entry, iova,\n\t\t\t\t      &domain->lv2entcnt[lv1ent_offset(iova)]);\n\n\t\tif (IS_ERR(pent))\n\t\t\tret = PTR_ERR(pent);\n\t\telse\n\t\t\tret = lv2set_page(pent, paddr, size, prot,\n\t\t\t\t       &domain->lv2entcnt[lv1ent_offset(iova)]);\n\t}\n\n\tif (ret)\n\t\tpr_err(\"%s: Failed(%d) to map %#zx bytes @ %#x\\n\",\n\t\t\t__func__, ret, size, iova);\n\n\tspin_unlock_irqrestore(&domain->pgtablelock, flags);\n\n\treturn ret;\n}\n\nstatic void exynos_iommu_tlb_invalidate_entry(struct exynos_iommu_domain *domain,\n\t\t\t\t\t      sysmmu_iova_t iova, size_t size)\n{\n\tstruct sysmmu_drvdata *data;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&domain->lock, flags);\n\n\tlist_for_each_entry(data, &domain->clients, domain_node)\n\t\tsysmmu_tlb_invalidate_entry(data, iova, size);\n\n\tspin_unlock_irqrestore(&domain->lock, flags);\n}\n\nstatic size_t exynos_iommu_unmap(struct iommu_domain *iommu_domain,\n\t\t\t\t unsigned long l_iova, size_t size,\n\t\t\t\t struct iommu_iotlb_gather *gather)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\n\tsysmmu_pte_t *ent;\n\tsize_t err_pgsize;\n\tunsigned long flags;\n\n\tBUG_ON(domain->pgtable == NULL);\n\n\tspin_lock_irqsave(&domain->pgtablelock, flags);\n\n\tent = section_entry(domain->pgtable, iova);\n\n\tif (lv1ent_section(ent)) {\n\t\tif (WARN_ON(size < SECT_SIZE)) {\n\t\t\terr_pgsize = SECT_SIZE;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\texynos_iommu_set_pte(ent, ZERO_LV2LINK);\n\t\tsize = SECT_SIZE;\n\t\tgoto done;\n\t}\n\n\tif (unlikely(lv1ent_fault(ent))) {\n\t\tif (size > SECT_SIZE)\n\t\t\tsize = SECT_SIZE;\n\t\tgoto done;\n\t}\n\n\t \n\n\tent = page_entry(ent, iova);\n\n\tif (unlikely(lv2ent_fault(ent))) {\n\t\tsize = SPAGE_SIZE;\n\t\tgoto done;\n\t}\n\n\tif (lv2ent_small(ent)) {\n\t\texynos_iommu_set_pte(ent, 0);\n\t\tsize = SPAGE_SIZE;\n\t\tdomain->lv2entcnt[lv1ent_offset(iova)] += 1;\n\t\tgoto done;\n\t}\n\n\t \n\tif (WARN_ON(size < LPAGE_SIZE)) {\n\t\terr_pgsize = LPAGE_SIZE;\n\t\tgoto err;\n\t}\n\n\tdma_sync_single_for_cpu(dma_dev, virt_to_phys(ent),\n\t\t\t\tsizeof(*ent) * SPAGES_PER_LPAGE,\n\t\t\t\tDMA_TO_DEVICE);\n\tmemset(ent, 0, sizeof(*ent) * SPAGES_PER_LPAGE);\n\tdma_sync_single_for_device(dma_dev, virt_to_phys(ent),\n\t\t\t\t   sizeof(*ent) * SPAGES_PER_LPAGE,\n\t\t\t\t   DMA_TO_DEVICE);\n\tsize = LPAGE_SIZE;\n\tdomain->lv2entcnt[lv1ent_offset(iova)] += SPAGES_PER_LPAGE;\ndone:\n\tspin_unlock_irqrestore(&domain->pgtablelock, flags);\n\n\texynos_iommu_tlb_invalidate_entry(domain, iova, size);\n\n\treturn size;\nerr:\n\tspin_unlock_irqrestore(&domain->pgtablelock, flags);\n\n\tpr_err(\"%s: Failed: size(%#zx) @ %#x is smaller than page size %#zx\\n\",\n\t\t__func__, size, iova, err_pgsize);\n\n\treturn 0;\n}\n\nstatic phys_addr_t exynos_iommu_iova_to_phys(struct iommu_domain *iommu_domain,\n\t\t\t\t\t  dma_addr_t iova)\n{\n\tstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\n\tsysmmu_pte_t *entry;\n\tunsigned long flags;\n\tphys_addr_t phys = 0;\n\n\tspin_lock_irqsave(&domain->pgtablelock, flags);\n\n\tentry = section_entry(domain->pgtable, iova);\n\n\tif (lv1ent_section(entry)) {\n\t\tphys = section_phys(entry) + section_offs(iova);\n\t} else if (lv1ent_page(entry)) {\n\t\tentry = page_entry(entry, iova);\n\n\t\tif (lv2ent_large(entry))\n\t\t\tphys = lpage_phys(entry) + lpage_offs(iova);\n\t\telse if (lv2ent_small(entry))\n\t\t\tphys = spage_phys(entry) + spage_offs(iova);\n\t}\n\n\tspin_unlock_irqrestore(&domain->pgtablelock, flags);\n\n\treturn phys;\n}\n\nstatic struct iommu_device *exynos_iommu_probe_device(struct device *dev)\n{\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\tstruct sysmmu_drvdata *data;\n\n\tif (!has_sysmmu(dev))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tlist_for_each_entry(data, &owner->controllers, owner_node) {\n\t\t \n\t\tdata->link = device_link_add(dev, data->sysmmu,\n\t\t\t\t\t     DL_FLAG_STATELESS |\n\t\t\t\t\t     DL_FLAG_PM_RUNTIME);\n\t}\n\n\t \n\tdata = list_first_entry(&owner->controllers,\n\t\t\t\tstruct sysmmu_drvdata, owner_node);\n\n\treturn &data->iommu;\n}\n\nstatic void exynos_iommu_set_platform_dma(struct device *dev)\n{\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\n\tif (owner->domain) {\n\t\tstruct iommu_group *group = iommu_group_get(dev);\n\n\t\tif (group) {\n\t\t\texynos_iommu_detach_device(owner->domain, dev);\n\t\t\tiommu_group_put(group);\n\t\t}\n\t}\n}\n\nstatic void exynos_iommu_release_device(struct device *dev)\n{\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\tstruct sysmmu_drvdata *data;\n\n\texynos_iommu_set_platform_dma(dev);\n\n\tlist_for_each_entry(data, &owner->controllers, owner_node)\n\t\tdevice_link_del(data->link);\n}\n\nstatic int exynos_iommu_of_xlate(struct device *dev,\n\t\t\t\t struct of_phandle_args *spec)\n{\n\tstruct platform_device *sysmmu = of_find_device_by_node(spec->np);\n\tstruct exynos_iommu_owner *owner = dev_iommu_priv_get(dev);\n\tstruct sysmmu_drvdata *data, *entry;\n\n\tif (!sysmmu)\n\t\treturn -ENODEV;\n\n\tdata = platform_get_drvdata(sysmmu);\n\tif (!data) {\n\t\tput_device(&sysmmu->dev);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!owner) {\n\t\towner = kzalloc(sizeof(*owner), GFP_KERNEL);\n\t\tif (!owner) {\n\t\t\tput_device(&sysmmu->dev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&owner->controllers);\n\t\tmutex_init(&owner->rpm_lock);\n\t\tdev_iommu_priv_set(dev, owner);\n\t}\n\n\tlist_for_each_entry(entry, &owner->controllers, owner_node)\n\t\tif (entry == data)\n\t\t\treturn 0;\n\n\tlist_add_tail(&data->owner_node, &owner->controllers);\n\tdata->master = dev;\n\n\treturn 0;\n}\n\nstatic const struct iommu_ops exynos_iommu_ops = {\n\t.domain_alloc = exynos_iommu_domain_alloc,\n\t.device_group = generic_device_group,\n#ifdef CONFIG_ARM\n\t.set_platform_dma_ops = exynos_iommu_set_platform_dma,\n#endif\n\t.probe_device = exynos_iommu_probe_device,\n\t.release_device = exynos_iommu_release_device,\n\t.pgsize_bitmap = SECT_SIZE | LPAGE_SIZE | SPAGE_SIZE,\n\t.of_xlate = exynos_iommu_of_xlate,\n\t.default_domain_ops = &(const struct iommu_domain_ops) {\n\t\t.attach_dev\t= exynos_iommu_attach_device,\n\t\t.map\t\t= exynos_iommu_map,\n\t\t.unmap\t\t= exynos_iommu_unmap,\n\t\t.iova_to_phys\t= exynos_iommu_iova_to_phys,\n\t\t.free\t\t= exynos_iommu_domain_free,\n\t}\n};\n\nstatic int __init exynos_iommu_init(void)\n{\n\tstruct device_node *np;\n\tint ret;\n\n\tnp = of_find_matching_node(NULL, sysmmu_of_match);\n\tif (!np)\n\t\treturn 0;\n\n\tof_node_put(np);\n\n\tlv2table_kmem_cache = kmem_cache_create(\"exynos-iommu-lv2table\",\n\t\t\t\tLV2TABLE_SIZE, LV2TABLE_SIZE, 0, NULL);\n\tif (!lv2table_kmem_cache) {\n\t\tpr_err(\"%s: Failed to create kmem cache\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\n\tzero_lv2_table = kmem_cache_zalloc(lv2table_kmem_cache, GFP_KERNEL);\n\tif (zero_lv2_table == NULL) {\n\t\tpr_err(\"%s: Failed to allocate zero level2 page table\\n\",\n\t\t\t__func__);\n\t\tret = -ENOMEM;\n\t\tgoto err_zero_lv2;\n\t}\n\n\tret = platform_driver_register(&exynos_sysmmu_driver);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register driver\\n\", __func__);\n\t\tgoto err_reg_driver;\n\t}\n\n\treturn 0;\nerr_reg_driver:\n\tkmem_cache_free(lv2table_kmem_cache, zero_lv2_table);\nerr_zero_lv2:\n\tkmem_cache_destroy(lv2table_kmem_cache);\n\treturn ret;\n}\ncore_initcall(exynos_iommu_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}