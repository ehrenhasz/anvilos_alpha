{
  "module_name": "hid_bpf_jmp_table.c",
  "hash_id": "921b08dee5cfac2495b391b3e75f6c56966815db2052e2628152d64484fcd8e8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/hid/bpf/hid_bpf_jmp_table.c",
  "human_readable_source": "\n\n \n\n#include <linux/bitops.h>\n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n#include <linux/circ_buf.h>\n#include <linux/filter.h>\n#include <linux/hid.h>\n#include <linux/hid_bpf.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/workqueue.h>\n#include \"hid_bpf_dispatch.h\"\n#include \"entrypoints/entrypoints.lskel.h\"\n\n#define HID_BPF_MAX_PROGS 1024  \n\n#define NEXT(idx) (((idx) + 1) & (HID_BPF_MAX_PROGS - 1))\n#define PREV(idx) (((idx) - 1) & (HID_BPF_MAX_PROGS - 1))\n\n \nstruct hid_bpf_prog_entry {\n\tstruct bpf_prog *prog;\n\tstruct hid_device *hdev;\n\tenum hid_bpf_prog_type type;\n\tu16 idx;\n};\n\nstruct hid_bpf_jmp_table {\n\tstruct bpf_map *map;\n\tstruct hid_bpf_prog_entry entries[HID_BPF_MAX_PROGS];  \n\tint tail, head;\n\tstruct bpf_prog *progs[HID_BPF_MAX_PROGS];  \n\tunsigned long enabled[BITS_TO_LONGS(HID_BPF_MAX_PROGS)];\n};\n\n#define FOR_ENTRIES(__i, __start, __end) \\\n\tfor (__i = __start; CIRC_CNT(__end, __i, HID_BPF_MAX_PROGS); __i = NEXT(__i))\n\nstatic struct hid_bpf_jmp_table jmp_table;\n\nstatic DEFINE_MUTEX(hid_bpf_attach_lock);\t\t \n\nstatic void hid_bpf_release_progs(struct work_struct *work);\n\nstatic DECLARE_WORK(release_work, hid_bpf_release_progs);\n\nBTF_ID_LIST(hid_bpf_btf_ids)\nBTF_ID(func, hid_bpf_device_event)\t\t\t \nBTF_ID(func, hid_bpf_rdesc_fixup)\t\t\t \n\nstatic int hid_bpf_max_programs(enum hid_bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase HID_BPF_PROG_TYPE_DEVICE_EVENT:\n\t\treturn HID_BPF_MAX_PROGS_PER_DEV;\n\tcase HID_BPF_PROG_TYPE_RDESC_FIXUP:\n\t\treturn 1;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int hid_bpf_program_count(struct hid_device *hdev,\n\t\t\t\t struct bpf_prog *prog,\n\t\t\t\t enum hid_bpf_prog_type type)\n{\n\tint i, n = 0;\n\n\tif (type >= HID_BPF_PROG_TYPE_MAX)\n\t\treturn -EINVAL;\n\n\tFOR_ENTRIES(i, jmp_table.tail, jmp_table.head) {\n\t\tstruct hid_bpf_prog_entry *entry = &jmp_table.entries[i];\n\n\t\tif (type != HID_BPF_PROG_TYPE_UNDEF && entry->type != type)\n\t\t\tcontinue;\n\n\t\tif (hdev && entry->hdev != hdev)\n\t\t\tcontinue;\n\n\t\tif (prog && entry->prog != prog)\n\t\t\tcontinue;\n\n\t\tn++;\n\t}\n\n\treturn n;\n}\n\n__weak noinline int __hid_bpf_tail_call(struct hid_bpf_ctx *ctx)\n{\n\treturn 0;\n}\n\nint hid_bpf_prog_run(struct hid_device *hdev, enum hid_bpf_prog_type type,\n\t\t     struct hid_bpf_ctx_kern *ctx_kern)\n{\n\tstruct hid_bpf_prog_list *prog_list;\n\tint i, idx, err = 0;\n\n\trcu_read_lock();\n\tprog_list = rcu_dereference(hdev->bpf.progs[type]);\n\n\tif (!prog_list)\n\t\tgoto out_unlock;\n\n\tfor (i = 0; i < prog_list->prog_cnt; i++) {\n\t\tidx = prog_list->prog_idx[i];\n\n\t\tif (!test_bit(idx, jmp_table.enabled))\n\t\t\tcontinue;\n\n\t\tctx_kern->ctx.index = idx;\n\t\terr = __hid_bpf_tail_call(&ctx_kern->ctx);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t\tif (err)\n\t\t\tctx_kern->ctx.retval = err;\n\t}\n\n out_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\n \nstatic void __hid_bpf_set_hdev_progs(struct hid_device *hdev, struct hid_bpf_prog_list *new_list,\n\t\t\t\t     enum hid_bpf_prog_type type)\n{\n\tstruct hid_bpf_prog_list *old_list;\n\n\tspin_lock(&hdev->bpf.progs_lock);\n\told_list = rcu_dereference_protected(hdev->bpf.progs[type],\n\t\t\t\t\t     lockdep_is_held(&hdev->bpf.progs_lock));\n\trcu_assign_pointer(hdev->bpf.progs[type], new_list);\n\tspin_unlock(&hdev->bpf.progs_lock);\n\tsynchronize_rcu();\n\n\tkfree(old_list);\n}\n\n \nstatic int hid_bpf_populate_hdev(struct hid_device *hdev, enum hid_bpf_prog_type type)\n{\n\tstruct hid_bpf_prog_list *new_list;\n\tint i;\n\n\tif (type >= HID_BPF_PROG_TYPE_MAX || !hdev)\n\t\treturn -EINVAL;\n\n\tif (hdev->bpf.destroyed)\n\t\treturn 0;\n\n\tnew_list = kzalloc(sizeof(*new_list), GFP_KERNEL);\n\tif (!new_list)\n\t\treturn -ENOMEM;\n\n\tFOR_ENTRIES(i, jmp_table.tail, jmp_table.head) {\n\t\tstruct hid_bpf_prog_entry *entry = &jmp_table.entries[i];\n\n\t\tif (entry->type == type && entry->hdev == hdev &&\n\t\t    test_bit(entry->idx, jmp_table.enabled))\n\t\t\tnew_list->prog_idx[new_list->prog_cnt++] = entry->idx;\n\t}\n\n\t__hid_bpf_set_hdev_progs(hdev, new_list, type);\n\n\treturn 0;\n}\n\nstatic void __hid_bpf_do_release_prog(int map_fd, unsigned int idx)\n{\n\tskel_map_delete_elem(map_fd, &idx);\n\tjmp_table.progs[idx] = NULL;\n}\n\nstatic void hid_bpf_release_progs(struct work_struct *work)\n{\n\tint i, j, n, map_fd = -1;\n\n\tif (!jmp_table.map)\n\t\treturn;\n\n\t \n\tmap_fd = skel_map_get_fd_by_id(jmp_table.map->id);\n\tif (map_fd < 0)\n\t\treturn;\n\n\tmutex_lock(&hid_bpf_attach_lock);  \n\n\t \n\tFOR_ENTRIES(i, jmp_table.tail, jmp_table.head) {\n\t\tstruct hid_bpf_prog_entry *entry = &jmp_table.entries[i];\n\t\tenum hid_bpf_prog_type type;\n\t\tstruct hid_device *hdev;\n\n\t\tif (test_bit(entry->idx, jmp_table.enabled))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (entry->hdev) {\n\t\t\thdev = entry->hdev;\n\t\t\ttype = entry->type;\n\n\t\t\thid_bpf_populate_hdev(hdev, type);\n\n\t\t\t \n\t\t\tFOR_ENTRIES(j, i, jmp_table.head) {\n\t\t\t\tstruct hid_bpf_prog_entry *next;\n\n\t\t\t\tnext = &jmp_table.entries[j];\n\n\t\t\t\tif (test_bit(next->idx, jmp_table.enabled))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (next->hdev == hdev && next->type == type)\n\t\t\t\t\tnext->hdev = NULL;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (type == HID_BPF_PROG_TYPE_RDESC_FIXUP)\n\t\t\t\thid_bpf_reconnect(hdev);\n\t\t}\n\t}\n\n\t \n\tFOR_ENTRIES(i, jmp_table.tail, jmp_table.head) {\n\t\tstruct hid_bpf_prog_entry *entry = &jmp_table.entries[i];\n\n\t\tif (test_bit(entry->idx, jmp_table.enabled))\n\t\t\tcontinue;\n\n\t\tif (entry->prog)\n\t\t\t__hid_bpf_do_release_prog(map_fd, entry->idx);\n\t}\n\n\t \n\tn = jmp_table.tail;\n\tFOR_ENTRIES(i, jmp_table.tail, jmp_table.head) {\n\t\tstruct hid_bpf_prog_entry *entry = &jmp_table.entries[i];\n\n\t\tif (!test_bit(entry->idx, jmp_table.enabled))\n\t\t\tcontinue;\n\n\t\tjmp_table.entries[n] = jmp_table.entries[i];\n\t\tn = NEXT(n);\n\t}\n\n\tjmp_table.head = n;\n\n\tmutex_unlock(&hid_bpf_attach_lock);\n\n\tif (map_fd >= 0)\n\t\tclose_fd(map_fd);\n}\n\nstatic void hid_bpf_release_prog_at(int idx)\n{\n\tint map_fd = -1;\n\n\t \n\tmap_fd = skel_map_get_fd_by_id(jmp_table.map->id);\n\tif (map_fd < 0)\n\t\treturn;\n\n\t__hid_bpf_do_release_prog(map_fd, idx);\n\n\tclose(map_fd);\n}\n\n \nstatic int hid_bpf_insert_prog(int prog_fd, struct bpf_prog *prog)\n{\n\tint i, index = -1, map_fd = -1, err = -EINVAL;\n\n\t \n\tmap_fd = skel_map_get_fd_by_id(jmp_table.map->id);\n\n\tif (map_fd < 0) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tfor (i = 0; i < HID_BPF_MAX_PROGS; i++) {\n\t\tif (!jmp_table.progs[i] && index < 0) {\n\t\t\t \n\t\t\tjmp_table.progs[i] = prog;\n\t\t\tindex = i;\n\t\t\t__set_bit(i, jmp_table.enabled);\n\t\t}\n\t}\n\tif (index < 0) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\terr = skel_map_update_elem(map_fd, &index, &prog_fd, 0);\n\tif (err)\n\t\tgoto out;\n\n\t \n\terr = index;\n\n out:\n\tif (err < 0)\n\t\t__hid_bpf_do_release_prog(map_fd, index);\n\tif (map_fd >= 0)\n\t\tclose_fd(map_fd);\n\treturn err;\n}\n\nint hid_bpf_get_prog_attach_type(int prog_fd)\n{\n\tstruct bpf_prog *prog = NULL;\n\tint i;\n\tint prog_type = HID_BPF_PROG_TYPE_UNDEF;\n\n\tprog = bpf_prog_get(prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tfor (i = 0; i < HID_BPF_PROG_TYPE_MAX; i++) {\n\t\tif (hid_bpf_btf_ids[i] == prog->aux->attach_btf_id) {\n\t\t\tprog_type = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tbpf_prog_put(prog);\n\n\treturn prog_type;\n}\n\nstatic void hid_bpf_link_release(struct bpf_link *link)\n{\n\tstruct hid_bpf_link *hid_link =\n\t\tcontainer_of(link, struct hid_bpf_link, link);\n\n\t__clear_bit(hid_link->hid_table_index, jmp_table.enabled);\n\tschedule_work(&release_work);\n}\n\nstatic void hid_bpf_link_dealloc(struct bpf_link *link)\n{\n\tstruct hid_bpf_link *hid_link =\n\t\tcontainer_of(link, struct hid_bpf_link, link);\n\n\tkfree(hid_link);\n}\n\nstatic void hid_bpf_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t\t struct seq_file *seq)\n{\n\tseq_printf(seq,\n\t\t   \"attach_type:\\tHID-BPF\\n\");\n}\n\nstatic const struct bpf_link_ops hid_bpf_link_lops = {\n\t.release = hid_bpf_link_release,\n\t.dealloc = hid_bpf_link_dealloc,\n\t.show_fdinfo = hid_bpf_link_show_fdinfo,\n};\n\n \nnoinline int\n__hid_bpf_attach_prog(struct hid_device *hdev, enum hid_bpf_prog_type prog_type,\n\t\t      int prog_fd, __u32 flags)\n{\n\tstruct bpf_link_primer link_primer;\n\tstruct hid_bpf_link *link;\n\tstruct bpf_prog *prog = NULL;\n\tstruct hid_bpf_prog_entry *prog_entry;\n\tint cnt, err = -EINVAL, prog_table_idx = -1;\n\n\t \n\tprog = bpf_prog_get(prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tmutex_lock(&hid_bpf_attach_lock);\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_UNSPEC,\n\t\t      &hid_bpf_link_lops, prog);\n\n\t \n\tcnt = hid_bpf_program_count(hdev, NULL, prog_type);\n\tif (cnt < 0) {\n\t\terr = cnt;\n\t\tgoto err_unlock;\n\t}\n\n\tif (cnt >= hid_bpf_max_programs(prog_type)) {\n\t\terr = -E2BIG;\n\t\tgoto err_unlock;\n\t}\n\n\tprog_table_idx = hid_bpf_insert_prog(prog_fd, prog);\n\t \n\tif (prog_table_idx < 0) {\n\t\terr = prog_table_idx;\n\t\tgoto err_unlock;\n\t}\n\n\tif (flags & HID_BPF_FLAG_INSERT_HEAD) {\n\t\t \n\t\tjmp_table.tail = PREV(jmp_table.tail);\n\t\tprog_entry = &jmp_table.entries[jmp_table.tail];\n\t} else {\n\t\t \n\t\tprog_entry = &jmp_table.entries[jmp_table.head];\n\t\tjmp_table.head = NEXT(jmp_table.head);\n\t}\n\n\t \n\tprog_entry->prog = prog;\n\tprog_entry->idx = prog_table_idx;\n\tprog_entry->hdev = hdev;\n\tprog_entry->type = prog_type;\n\n\t \n\terr = hid_bpf_populate_hdev(hdev, prog_type);\n\tif (err) {\n\t\thid_bpf_release_prog_at(prog_table_idx);\n\t\tgoto err_unlock;\n\t}\n\n\tlink->hid_table_index = prog_table_idx;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tmutex_unlock(&hid_bpf_attach_lock);\n\n\treturn bpf_link_settle(&link_primer);\n\n err_unlock:\n\tmutex_unlock(&hid_bpf_attach_lock);\n\n\tbpf_prog_put(prog);\n\tkfree(link);\n\n\treturn err;\n}\n\nvoid __hid_bpf_destroy_device(struct hid_device *hdev)\n{\n\tint type, i;\n\tstruct hid_bpf_prog_list *prog_list;\n\n\trcu_read_lock();\n\n\tfor (type = 0; type < HID_BPF_PROG_TYPE_MAX; type++) {\n\t\tprog_list = rcu_dereference(hdev->bpf.progs[type]);\n\n\t\tif (!prog_list)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < prog_list->prog_cnt; i++)\n\t\t\t__clear_bit(prog_list->prog_idx[i], jmp_table.enabled);\n\t}\n\n\trcu_read_unlock();\n\n\tfor (type = 0; type < HID_BPF_PROG_TYPE_MAX; type++)\n\t\t__hid_bpf_set_hdev_progs(hdev, NULL, type);\n\n\t \n\tschedule_work(&release_work);\n}\n\n#define HID_BPF_PROGS_COUNT 1\n\nstatic struct bpf_link *links[HID_BPF_PROGS_COUNT];\nstatic struct entrypoints_bpf *skel;\n\nvoid hid_bpf_free_links_and_skel(void)\n{\n\tint i;\n\n\t \n\tif (jmp_table.map)\n\t\tbpf_map_put_with_uref(jmp_table.map);\n\n\tfor (i = 0; i < ARRAY_SIZE(links); i++) {\n\t\tif (!IS_ERR_OR_NULL(links[i]))\n\t\t\tbpf_link_put(links[i]);\n\t}\n\tentrypoints_bpf__destroy(skel);\n}\n\n#define ATTACH_AND_STORE_LINK(__name) do {\t\t\t\t\t\\\n\terr = entrypoints_bpf__##__name##__attach(skel);\t\t\t\\\n\tif (err)\t\t\t\t\t\t\t\t\\\n\t\tgoto out;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tlinks[idx] = bpf_link_get_from_fd(skel->links.__name##_fd);\t\t\\\n\tif (IS_ERR(links[idx])) {\t\t\t\t\t\t\\\n\t\terr = PTR_ERR(links[idx]);\t\t\t\t\t\\\n\t\tgoto out;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t \t\t\t\t\t\t\t\t\t\\\n\tclose_fd(skel->links.__name##_fd);\t\t\t\t\t\\\n\tskel->links.__name##_fd = 0;\t\t\t\t\t\t\\\n\tidx++;\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\nint hid_bpf_preload_skel(void)\n{\n\tint err, idx = 0;\n\n\tskel = entrypoints_bpf__open();\n\tif (!skel)\n\t\treturn -ENOMEM;\n\n\terr = entrypoints_bpf__load(skel);\n\tif (err)\n\t\tgoto out;\n\n\tjmp_table.map = bpf_map_get_with_uref(skel->maps.hid_jmp_table.map_fd);\n\tif (IS_ERR(jmp_table.map)) {\n\t\terr = PTR_ERR(jmp_table.map);\n\t\tgoto out;\n\t}\n\n\tATTACH_AND_STORE_LINK(hid_tail_call);\n\n\treturn 0;\nout:\n\thid_bpf_free_links_and_skel();\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}