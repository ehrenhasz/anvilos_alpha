{
  "module_name": "spi-dw-dma.c",
  "hash_id": "4e68e1cb02e001527dd7a0679d641a5b91f8b2284e3b0d57f2a2a124bb56c216",
  "original_prompt": "Ingested from linux-6.6.14/drivers/spi/spi-dw-dma.c",
  "human_readable_source": "\n \n\n#include <linux/completion.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/irqreturn.h>\n#include <linux/jiffies.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/platform_data/dma-dw.h>\n#include <linux/spi/spi.h>\n#include <linux/types.h>\n\n#include \"spi-dw.h\"\n\n#define DW_SPI_RX_BUSY\t\t0\n#define DW_SPI_RX_BURST_LEVEL\t16\n#define DW_SPI_TX_BUSY\t\t1\n#define DW_SPI_TX_BURST_LEVEL\t16\n\nstatic bool dw_spi_dma_chan_filter(struct dma_chan *chan, void *param)\n{\n\tstruct dw_dma_slave *s = param;\n\n\tif (s->dma_dev != chan->device->dev)\n\t\treturn false;\n\n\tchan->private = s;\n\treturn true;\n}\n\nstatic void dw_spi_dma_maxburst_init(struct dw_spi *dws)\n{\n\tstruct dma_slave_caps caps;\n\tu32 max_burst, def_burst;\n\tint ret;\n\n\tdef_burst = dws->fifo_len / 2;\n\n\tret = dma_get_slave_caps(dws->rxchan, &caps);\n\tif (!ret && caps.max_burst)\n\t\tmax_burst = caps.max_burst;\n\telse\n\t\tmax_burst = DW_SPI_RX_BURST_LEVEL;\n\n\tdws->rxburst = min(max_burst, def_burst);\n\tdw_writel(dws, DW_SPI_DMARDLR, dws->rxburst - 1);\n\n\tret = dma_get_slave_caps(dws->txchan, &caps);\n\tif (!ret && caps.max_burst)\n\t\tmax_burst = caps.max_burst;\n\telse\n\t\tmax_burst = DW_SPI_TX_BURST_LEVEL;\n\n\t \n\tdws->txburst = min(max_burst, def_burst);\n\tdw_writel(dws, DW_SPI_DMATDLR, dws->txburst);\n}\n\nstatic int dw_spi_dma_caps_init(struct dw_spi *dws)\n{\n\tstruct dma_slave_caps tx, rx;\n\tint ret;\n\n\tret = dma_get_slave_caps(dws->txchan, &tx);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dma_get_slave_caps(dws->rxchan, &rx);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(tx.directions & BIT(DMA_MEM_TO_DEV) &&\n\t      rx.directions & BIT(DMA_DEV_TO_MEM)))\n\t\treturn -ENXIO;\n\n\tif (tx.max_sg_burst > 0 && rx.max_sg_burst > 0)\n\t\tdws->dma_sg_burst = min(tx.max_sg_burst, rx.max_sg_burst);\n\telse if (tx.max_sg_burst > 0)\n\t\tdws->dma_sg_burst = tx.max_sg_burst;\n\telse if (rx.max_sg_burst > 0)\n\t\tdws->dma_sg_burst = rx.max_sg_burst;\n\telse\n\t\tdws->dma_sg_burst = 0;\n\n\t \n\tdws->dma_addr_widths = tx.dst_addr_widths & rx.src_addr_widths;\n\n\treturn 0;\n}\n\nstatic int dw_spi_dma_init_mfld(struct device *dev, struct dw_spi *dws)\n{\n\tstruct dw_dma_slave dma_tx = { .dst_id = 1 }, *tx = &dma_tx;\n\tstruct dw_dma_slave dma_rx = { .src_id = 0 }, *rx = &dma_rx;\n\tstruct pci_dev *dma_dev;\n\tdma_cap_mask_t mask;\n\tint ret = -EBUSY;\n\n\t \n\tdma_dev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x0827, NULL);\n\tif (!dma_dev)\n\t\treturn -ENODEV;\n\n\tdma_cap_zero(mask);\n\tdma_cap_set(DMA_SLAVE, mask);\n\n\t \n\trx->dma_dev = &dma_dev->dev;\n\tdws->rxchan = dma_request_channel(mask, dw_spi_dma_chan_filter, rx);\n\tif (!dws->rxchan)\n\t\tgoto err_exit;\n\n\t \n\ttx->dma_dev = &dma_dev->dev;\n\tdws->txchan = dma_request_channel(mask, dw_spi_dma_chan_filter, tx);\n\tif (!dws->txchan)\n\t\tgoto free_rxchan;\n\n\tdws->host->dma_rx = dws->rxchan;\n\tdws->host->dma_tx = dws->txchan;\n\n\tinit_completion(&dws->dma_completion);\n\n\tret = dw_spi_dma_caps_init(dws);\n\tif (ret)\n\t\tgoto free_txchan;\n\n\tdw_spi_dma_maxburst_init(dws);\n\n\tpci_dev_put(dma_dev);\n\n\treturn 0;\n\nfree_txchan:\n\tdma_release_channel(dws->txchan);\n\tdws->txchan = NULL;\nfree_rxchan:\n\tdma_release_channel(dws->rxchan);\n\tdws->rxchan = NULL;\nerr_exit:\n\tpci_dev_put(dma_dev);\n\treturn ret;\n}\n\nstatic int dw_spi_dma_init_generic(struct device *dev, struct dw_spi *dws)\n{\n\tint ret;\n\n\tdws->rxchan = dma_request_chan(dev, \"rx\");\n\tif (IS_ERR(dws->rxchan)) {\n\t\tret = PTR_ERR(dws->rxchan);\n\t\tdws->rxchan = NULL;\n\t\tgoto err_exit;\n\t}\n\n\tdws->txchan = dma_request_chan(dev, \"tx\");\n\tif (IS_ERR(dws->txchan)) {\n\t\tret = PTR_ERR(dws->txchan);\n\t\tdws->txchan = NULL;\n\t\tgoto free_rxchan;\n\t}\n\n\tdws->host->dma_rx = dws->rxchan;\n\tdws->host->dma_tx = dws->txchan;\n\n\tinit_completion(&dws->dma_completion);\n\n\tret = dw_spi_dma_caps_init(dws);\n\tif (ret)\n\t\tgoto free_txchan;\n\n\tdw_spi_dma_maxburst_init(dws);\n\n\treturn 0;\n\nfree_txchan:\n\tdma_release_channel(dws->txchan);\n\tdws->txchan = NULL;\nfree_rxchan:\n\tdma_release_channel(dws->rxchan);\n\tdws->rxchan = NULL;\nerr_exit:\n\treturn ret;\n}\n\nstatic void dw_spi_dma_exit(struct dw_spi *dws)\n{\n\tif (dws->txchan) {\n\t\tdmaengine_terminate_sync(dws->txchan);\n\t\tdma_release_channel(dws->txchan);\n\t}\n\n\tif (dws->rxchan) {\n\t\tdmaengine_terminate_sync(dws->rxchan);\n\t\tdma_release_channel(dws->rxchan);\n\t}\n}\n\nstatic irqreturn_t dw_spi_dma_transfer_handler(struct dw_spi *dws)\n{\n\tdw_spi_check_status(dws, false);\n\n\tcomplete(&dws->dma_completion);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic enum dma_slave_buswidth dw_spi_dma_convert_width(u8 n_bytes)\n{\n\tswitch (n_bytes) {\n\tcase 1:\n\t\treturn DMA_SLAVE_BUSWIDTH_1_BYTE;\n\tcase 2:\n\t\treturn DMA_SLAVE_BUSWIDTH_2_BYTES;\n\tcase 4:\n\t\treturn DMA_SLAVE_BUSWIDTH_4_BYTES;\n\tdefault:\n\t\treturn DMA_SLAVE_BUSWIDTH_UNDEFINED;\n\t}\n}\n\nstatic bool dw_spi_can_dma(struct spi_controller *host,\n\t\t\t   struct spi_device *spi, struct spi_transfer *xfer)\n{\n\tstruct dw_spi *dws = spi_controller_get_devdata(host);\n\tenum dma_slave_buswidth dma_bus_width;\n\n\tif (xfer->len <= dws->fifo_len)\n\t\treturn false;\n\n\tdma_bus_width = dw_spi_dma_convert_width(dws->n_bytes);\n\n\treturn dws->dma_addr_widths & BIT(dma_bus_width);\n}\n\nstatic int dw_spi_dma_wait(struct dw_spi *dws, unsigned int len, u32 speed)\n{\n\tunsigned long long ms;\n\n\tms = len * MSEC_PER_SEC * BITS_PER_BYTE;\n\tdo_div(ms, speed);\n\tms += ms + 200;\n\n\tif (ms > UINT_MAX)\n\t\tms = UINT_MAX;\n\n\tms = wait_for_completion_timeout(&dws->dma_completion,\n\t\t\t\t\t msecs_to_jiffies(ms));\n\n\tif (ms == 0) {\n\t\tdev_err(&dws->host->cur_msg->spi->dev,\n\t\t\t\"DMA transaction timed out\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool dw_spi_dma_tx_busy(struct dw_spi *dws)\n{\n\treturn !(dw_readl(dws, DW_SPI_SR) & DW_SPI_SR_TF_EMPT);\n}\n\nstatic int dw_spi_dma_wait_tx_done(struct dw_spi *dws,\n\t\t\t\t   struct spi_transfer *xfer)\n{\n\tint retry = DW_SPI_WAIT_RETRIES;\n\tstruct spi_delay delay;\n\tu32 nents;\n\n\tnents = dw_readl(dws, DW_SPI_TXFLR);\n\tdelay.unit = SPI_DELAY_UNIT_SCK;\n\tdelay.value = nents * dws->n_bytes * BITS_PER_BYTE;\n\n\twhile (dw_spi_dma_tx_busy(dws) && retry--)\n\t\tspi_delay_exec(&delay, xfer);\n\n\tif (retry < 0) {\n\t\tdev_err(&dws->host->dev, \"Tx hanged up\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void dw_spi_dma_tx_done(void *arg)\n{\n\tstruct dw_spi *dws = arg;\n\n\tclear_bit(DW_SPI_TX_BUSY, &dws->dma_chan_busy);\n\tif (test_bit(DW_SPI_RX_BUSY, &dws->dma_chan_busy))\n\t\treturn;\n\n\tcomplete(&dws->dma_completion);\n}\n\nstatic int dw_spi_dma_config_tx(struct dw_spi *dws)\n{\n\tstruct dma_slave_config txconf;\n\n\tmemset(&txconf, 0, sizeof(txconf));\n\ttxconf.direction = DMA_MEM_TO_DEV;\n\ttxconf.dst_addr = dws->dma_addr;\n\ttxconf.dst_maxburst = dws->txburst;\n\ttxconf.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\ttxconf.dst_addr_width = dw_spi_dma_convert_width(dws->n_bytes);\n\ttxconf.device_fc = false;\n\n\treturn dmaengine_slave_config(dws->txchan, &txconf);\n}\n\nstatic int dw_spi_dma_submit_tx(struct dw_spi *dws, struct scatterlist *sgl,\n\t\t\t\tunsigned int nents)\n{\n\tstruct dma_async_tx_descriptor *txdesc;\n\tdma_cookie_t cookie;\n\tint ret;\n\n\ttxdesc = dmaengine_prep_slave_sg(dws->txchan, sgl, nents,\n\t\t\t\t\t DMA_MEM_TO_DEV,\n\t\t\t\t\t DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\tif (!txdesc)\n\t\treturn -ENOMEM;\n\n\ttxdesc->callback = dw_spi_dma_tx_done;\n\ttxdesc->callback_param = dws;\n\n\tcookie = dmaengine_submit(txdesc);\n\tret = dma_submit_error(cookie);\n\tif (ret) {\n\t\tdmaengine_terminate_sync(dws->txchan);\n\t\treturn ret;\n\t}\n\n\tset_bit(DW_SPI_TX_BUSY, &dws->dma_chan_busy);\n\n\treturn 0;\n}\n\nstatic inline bool dw_spi_dma_rx_busy(struct dw_spi *dws)\n{\n\treturn !!(dw_readl(dws, DW_SPI_SR) & DW_SPI_SR_RF_NOT_EMPT);\n}\n\nstatic int dw_spi_dma_wait_rx_done(struct dw_spi *dws)\n{\n\tint retry = DW_SPI_WAIT_RETRIES;\n\tstruct spi_delay delay;\n\tunsigned long ns, us;\n\tu32 nents;\n\n\t \n\tnents = dw_readl(dws, DW_SPI_RXFLR);\n\tns = 4U * NSEC_PER_SEC / dws->max_freq * nents;\n\tif (ns <= NSEC_PER_USEC) {\n\t\tdelay.unit = SPI_DELAY_UNIT_NSECS;\n\t\tdelay.value = ns;\n\t} else {\n\t\tus = DIV_ROUND_UP(ns, NSEC_PER_USEC);\n\t\tdelay.unit = SPI_DELAY_UNIT_USECS;\n\t\tdelay.value = clamp_val(us, 0, USHRT_MAX);\n\t}\n\n\twhile (dw_spi_dma_rx_busy(dws) && retry--)\n\t\tspi_delay_exec(&delay, NULL);\n\n\tif (retry < 0) {\n\t\tdev_err(&dws->host->dev, \"Rx hanged up\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void dw_spi_dma_rx_done(void *arg)\n{\n\tstruct dw_spi *dws = arg;\n\n\tclear_bit(DW_SPI_RX_BUSY, &dws->dma_chan_busy);\n\tif (test_bit(DW_SPI_TX_BUSY, &dws->dma_chan_busy))\n\t\treturn;\n\n\tcomplete(&dws->dma_completion);\n}\n\nstatic int dw_spi_dma_config_rx(struct dw_spi *dws)\n{\n\tstruct dma_slave_config rxconf;\n\n\tmemset(&rxconf, 0, sizeof(rxconf));\n\trxconf.direction = DMA_DEV_TO_MEM;\n\trxconf.src_addr = dws->dma_addr;\n\trxconf.src_maxburst = dws->rxburst;\n\trxconf.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\trxconf.src_addr_width = dw_spi_dma_convert_width(dws->n_bytes);\n\trxconf.device_fc = false;\n\n\treturn dmaengine_slave_config(dws->rxchan, &rxconf);\n}\n\nstatic int dw_spi_dma_submit_rx(struct dw_spi *dws, struct scatterlist *sgl,\n\t\t\t\tunsigned int nents)\n{\n\tstruct dma_async_tx_descriptor *rxdesc;\n\tdma_cookie_t cookie;\n\tint ret;\n\n\trxdesc = dmaengine_prep_slave_sg(dws->rxchan, sgl, nents,\n\t\t\t\t\t DMA_DEV_TO_MEM,\n\t\t\t\t\t DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\tif (!rxdesc)\n\t\treturn -ENOMEM;\n\n\trxdesc->callback = dw_spi_dma_rx_done;\n\trxdesc->callback_param = dws;\n\n\tcookie = dmaengine_submit(rxdesc);\n\tret = dma_submit_error(cookie);\n\tif (ret) {\n\t\tdmaengine_terminate_sync(dws->rxchan);\n\t\treturn ret;\n\t}\n\n\tset_bit(DW_SPI_RX_BUSY, &dws->dma_chan_busy);\n\n\treturn 0;\n}\n\nstatic int dw_spi_dma_setup(struct dw_spi *dws, struct spi_transfer *xfer)\n{\n\tu16 imr, dma_ctrl;\n\tint ret;\n\n\tif (!xfer->tx_buf)\n\t\treturn -EINVAL;\n\n\t \n\tret = dw_spi_dma_config_tx(dws);\n\tif (ret)\n\t\treturn ret;\n\n\tif (xfer->rx_buf) {\n\t\tret = dw_spi_dma_config_rx(dws);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tdma_ctrl = DW_SPI_DMACR_TDMAE;\n\tif (xfer->rx_buf)\n\t\tdma_ctrl |= DW_SPI_DMACR_RDMAE;\n\tdw_writel(dws, DW_SPI_DMACR, dma_ctrl);\n\n\t \n\timr = DW_SPI_INT_TXOI;\n\tif (xfer->rx_buf)\n\t\timr |= DW_SPI_INT_RXUI | DW_SPI_INT_RXOI;\n\tdw_spi_umask_intr(dws, imr);\n\n\treinit_completion(&dws->dma_completion);\n\n\tdws->transfer_handler = dw_spi_dma_transfer_handler;\n\n\treturn 0;\n}\n\nstatic int dw_spi_dma_transfer_all(struct dw_spi *dws,\n\t\t\t\t   struct spi_transfer *xfer)\n{\n\tint ret;\n\n\t \n\tret = dw_spi_dma_submit_tx(dws, xfer->tx_sg.sgl, xfer->tx_sg.nents);\n\tif (ret)\n\t\tgoto err_clear_dmac;\n\n\t \n\tif (xfer->rx_buf) {\n\t\tret = dw_spi_dma_submit_rx(dws, xfer->rx_sg.sgl,\n\t\t\t\t\t   xfer->rx_sg.nents);\n\t\tif (ret)\n\t\t\tgoto err_clear_dmac;\n\n\t\t \n\t\tdma_async_issue_pending(dws->rxchan);\n\t}\n\n\tdma_async_issue_pending(dws->txchan);\n\n\tret = dw_spi_dma_wait(dws, xfer->len, xfer->effective_speed_hz);\n\nerr_clear_dmac:\n\tdw_writel(dws, DW_SPI_DMACR, 0);\n\n\treturn ret;\n}\n\n \n\nstatic int dw_spi_dma_transfer_one(struct dw_spi *dws,\n\t\t\t\t   struct spi_transfer *xfer)\n{\n\tstruct scatterlist *tx_sg = NULL, *rx_sg = NULL, tx_tmp, rx_tmp;\n\tunsigned int tx_len = 0, rx_len = 0;\n\tunsigned int base, len;\n\tint ret;\n\n\tsg_init_table(&tx_tmp, 1);\n\tsg_init_table(&rx_tmp, 1);\n\n\tfor (base = 0, len = 0; base < xfer->len; base += len) {\n\t\t \n\t\tif (!tx_len) {\n\t\t\ttx_sg = !tx_sg ? &xfer->tx_sg.sgl[0] : sg_next(tx_sg);\n\t\t\tsg_dma_address(&tx_tmp) = sg_dma_address(tx_sg);\n\t\t\ttx_len = sg_dma_len(tx_sg);\n\t\t}\n\n\t\t \n\t\tif (!rx_len) {\n\t\t\trx_sg = !rx_sg ? &xfer->rx_sg.sgl[0] : sg_next(rx_sg);\n\t\t\tsg_dma_address(&rx_tmp) = sg_dma_address(rx_sg);\n\t\t\trx_len = sg_dma_len(rx_sg);\n\t\t}\n\n\t\tlen = min(tx_len, rx_len);\n\n\t\tsg_dma_len(&tx_tmp) = len;\n\t\tsg_dma_len(&rx_tmp) = len;\n\n\t\t \n\t\tret = dw_spi_dma_submit_tx(dws, &tx_tmp, 1);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tret = dw_spi_dma_submit_rx(dws, &rx_tmp, 1);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_async_issue_pending(dws->rxchan);\n\n\t\tdma_async_issue_pending(dws->txchan);\n\n\t\t \n\t\tret = dw_spi_dma_wait(dws, len, xfer->effective_speed_hz);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\treinit_completion(&dws->dma_completion);\n\n\t\tsg_dma_address(&tx_tmp) += len;\n\t\tsg_dma_address(&rx_tmp) += len;\n\t\ttx_len -= len;\n\t\trx_len -= len;\n\t}\n\n\tdw_writel(dws, DW_SPI_DMACR, 0);\n\n\treturn ret;\n}\n\nstatic int dw_spi_dma_transfer(struct dw_spi *dws, struct spi_transfer *xfer)\n{\n\tunsigned int nents;\n\tint ret;\n\n\tnents = max(xfer->tx_sg.nents, xfer->rx_sg.nents);\n\n\t \n\tif (!dws->dma_sg_burst || !xfer->rx_buf || nents <= dws->dma_sg_burst)\n\t\tret = dw_spi_dma_transfer_all(dws, xfer);\n\telse\n\t\tret = dw_spi_dma_transfer_one(dws, xfer);\n\tif (ret)\n\t\treturn ret;\n\n\tif (dws->host->cur_msg->status == -EINPROGRESS) {\n\t\tret = dw_spi_dma_wait_tx_done(dws, xfer);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (xfer->rx_buf && dws->host->cur_msg->status == -EINPROGRESS)\n\t\tret = dw_spi_dma_wait_rx_done(dws);\n\n\treturn ret;\n}\n\nstatic void dw_spi_dma_stop(struct dw_spi *dws)\n{\n\tif (test_bit(DW_SPI_TX_BUSY, &dws->dma_chan_busy)) {\n\t\tdmaengine_terminate_sync(dws->txchan);\n\t\tclear_bit(DW_SPI_TX_BUSY, &dws->dma_chan_busy);\n\t}\n\tif (test_bit(DW_SPI_RX_BUSY, &dws->dma_chan_busy)) {\n\t\tdmaengine_terminate_sync(dws->rxchan);\n\t\tclear_bit(DW_SPI_RX_BUSY, &dws->dma_chan_busy);\n\t}\n}\n\nstatic const struct dw_spi_dma_ops dw_spi_dma_mfld_ops = {\n\t.dma_init\t= dw_spi_dma_init_mfld,\n\t.dma_exit\t= dw_spi_dma_exit,\n\t.dma_setup\t= dw_spi_dma_setup,\n\t.can_dma\t= dw_spi_can_dma,\n\t.dma_transfer\t= dw_spi_dma_transfer,\n\t.dma_stop\t= dw_spi_dma_stop,\n};\n\nvoid dw_spi_dma_setup_mfld(struct dw_spi *dws)\n{\n\tdws->dma_ops = &dw_spi_dma_mfld_ops;\n}\nEXPORT_SYMBOL_NS_GPL(dw_spi_dma_setup_mfld, SPI_DW_CORE);\n\nstatic const struct dw_spi_dma_ops dw_spi_dma_generic_ops = {\n\t.dma_init\t= dw_spi_dma_init_generic,\n\t.dma_exit\t= dw_spi_dma_exit,\n\t.dma_setup\t= dw_spi_dma_setup,\n\t.can_dma\t= dw_spi_can_dma,\n\t.dma_transfer\t= dw_spi_dma_transfer,\n\t.dma_stop\t= dw_spi_dma_stop,\n};\n\nvoid dw_spi_dma_setup_generic(struct dw_spi *dws)\n{\n\tdws->dma_ops = &dw_spi_dma_generic_ops;\n}\nEXPORT_SYMBOL_NS_GPL(dw_spi_dma_setup_generic, SPI_DW_CORE);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}