{
  "module_name": "cmt_speech.c",
  "hash_id": "5f72d97b90c31fd9f5a7704c1b73882b1efb862eae2824722d80835893d79439",
  "original_prompt": "Ingested from linux-6.6.14/drivers/hsi/clients/cmt_speech.c",
  "human_readable_source": "\n \n\n#include <linux/errno.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/miscdevice.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/fs.h>\n#include <linux/poll.h>\n#include <linux/sched/signal.h>\n#include <linux/ioctl.h>\n#include <linux/uaccess.h>\n#include <linux/pm_qos.h>\n#include <linux/hsi/hsi.h>\n#include <linux/hsi/ssi_protocol.h>\n#include <linux/hsi/cs-protocol.h>\n\n#define CS_MMAP_SIZE\tPAGE_SIZE\n\nstruct char_queue {\n\tstruct list_head\tlist;\n\tu32\t\t\tmsg;\n};\n\nstruct cs_char {\n\tunsigned int\t\topened;\n\tstruct hsi_client\t*cl;\n\tstruct cs_hsi_iface\t*hi;\n\tstruct list_head\tchardev_queue;\n\tstruct list_head\tdataind_queue;\n\tint\t\t\tdataind_pending;\n\t \n\tunsigned long\t\tmmap_base;\n\tunsigned long\t\tmmap_size;\n\tspinlock_t\t\tlock;\n\tstruct fasync_struct\t*async_queue;\n\twait_queue_head_t\twait;\n\t \n\tint                     channel_id_cmd;\n\tint                     channel_id_data;\n};\n\n#define SSI_CHANNEL_STATE_READING\t1\n#define SSI_CHANNEL_STATE_WRITING\t(1 << 1)\n#define SSI_CHANNEL_STATE_POLL\t\t(1 << 2)\n#define SSI_CHANNEL_STATE_ERROR\t\t(1 << 3)\n\n#define TARGET_MASK\t\t\t0xf000000\n#define TARGET_REMOTE\t\t\t(1 << CS_DOMAIN_SHIFT)\n#define TARGET_LOCAL\t\t\t0\n\n \n#define CS_MAX_CMDS\t\t        4\n\n \n#define CS_QOS_LATENCY_FOR_DATA_USEC\t20000\n\n \n#define CS_HSI_TRANSFER_TIMEOUT_MS      500\n\n\n#define RX_PTR_BOUNDARY_SHIFT\t\t8\n#define RX_PTR_MAX_SHIFT\t\t(RX_PTR_BOUNDARY_SHIFT + \\\n\t\t\t\t\t\tCS_MAX_BUFFERS_SHIFT)\nstruct cs_hsi_iface {\n\tstruct hsi_client\t\t*cl;\n\tstruct hsi_client\t\t*master;\n\n\tunsigned int\t\t\tiface_state;\n\tunsigned int\t\t\twakeline_state;\n\tunsigned int\t\t\tcontrol_state;\n\tunsigned int\t\t\tdata_state;\n\n\t \n\tstruct cs_mmap_config_block\t*mmap_cfg;\n\n\tunsigned long\t\t\tmmap_base;\n\tunsigned long\t\t\tmmap_size;\n\n\tunsigned int\t\t\trx_slot;\n\tunsigned int\t\t\ttx_slot;\n\n\t \n\tunsigned int\t\t\tbuf_size;\n\tunsigned int\t\t\trx_bufs;\n\tunsigned int\t\t\ttx_bufs;\n\tunsigned int\t\t\trx_ptr_boundary;\n\tunsigned int\t\t\trx_offsets[CS_MAX_BUFFERS];\n\tunsigned int\t\t\ttx_offsets[CS_MAX_BUFFERS];\n\n\t \n\tunsigned int\t\t\tslot_size;\n\tunsigned int\t\t\tflags;\n\n\tstruct list_head\t\tcmdqueue;\n\n\tstruct hsi_msg\t\t\t*data_rx_msg;\n\tstruct hsi_msg\t\t\t*data_tx_msg;\n\twait_queue_head_t\t\tdatawait;\n\n\tstruct pm_qos_request           pm_qos_req;\n\n\tspinlock_t\t\t\tlock;\n};\n\nstatic struct cs_char cs_char_data;\n\nstatic void cs_hsi_read_on_control(struct cs_hsi_iface *hi);\nstatic void cs_hsi_read_on_data(struct cs_hsi_iface *hi);\n\nstatic inline void rx_ptr_shift_too_big(void)\n{\n\tBUILD_BUG_ON((1LLU << RX_PTR_MAX_SHIFT) > UINT_MAX);\n}\n\nstatic void cs_notify(u32 message, struct list_head *head)\n{\n\tstruct char_queue *entry;\n\n\tspin_lock(&cs_char_data.lock);\n\n\tif (!cs_char_data.opened) {\n\t\tspin_unlock(&cs_char_data.lock);\n\t\tgoto out;\n\t}\n\n\tentry = kmalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tdev_err(&cs_char_data.cl->device,\n\t\t\t\"Can't allocate new entry for the queue.\\n\");\n\t\tspin_unlock(&cs_char_data.lock);\n\t\tgoto out;\n\t}\n\n\tentry->msg = message;\n\tlist_add_tail(&entry->list, head);\n\n\tspin_unlock(&cs_char_data.lock);\n\n\twake_up_interruptible(&cs_char_data.wait);\n\tkill_fasync(&cs_char_data.async_queue, SIGIO, POLL_IN);\n\nout:\n\treturn;\n}\n\nstatic u32 cs_pop_entry(struct list_head *head)\n{\n\tstruct char_queue *entry;\n\tu32 data;\n\n\tentry = list_entry(head->next, struct char_queue, list);\n\tdata = entry->msg;\n\tlist_del(&entry->list);\n\tkfree(entry);\n\n\treturn data;\n}\n\nstatic void cs_notify_control(u32 message)\n{\n\tcs_notify(message, &cs_char_data.chardev_queue);\n}\n\nstatic void cs_notify_data(u32 message, int maxlength)\n{\n\tcs_notify(message, &cs_char_data.dataind_queue);\n\n\tspin_lock(&cs_char_data.lock);\n\tcs_char_data.dataind_pending++;\n\twhile (cs_char_data.dataind_pending > maxlength &&\n\t\t\t\t!list_empty(&cs_char_data.dataind_queue)) {\n\t\tdev_dbg(&cs_char_data.cl->device, \"data notification \"\n\t\t\"queue overrun (%u entries)\\n\", cs_char_data.dataind_pending);\n\n\t\tcs_pop_entry(&cs_char_data.dataind_queue);\n\t\tcs_char_data.dataind_pending--;\n\t}\n\tspin_unlock(&cs_char_data.lock);\n}\n\nstatic inline void cs_set_cmd(struct hsi_msg *msg, u32 cmd)\n{\n\tu32 *data = sg_virt(msg->sgt.sgl);\n\t*data = cmd;\n}\n\nstatic inline u32 cs_get_cmd(struct hsi_msg *msg)\n{\n\tu32 *data = sg_virt(msg->sgt.sgl);\n\treturn *data;\n}\n\nstatic void cs_release_cmd(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\n\tlist_add_tail(&msg->link, &hi->cmdqueue);\n}\n\nstatic void cs_cmd_destructor(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\n\tspin_lock(&hi->lock);\n\n\tdev_dbg(&cs_char_data.cl->device, \"control cmd destructor\\n\");\n\n\tif (hi->iface_state != CS_STATE_CLOSED)\n\t\tdev_err(&hi->cl->device, \"Cmd flushed while driver active\\n\");\n\n\tif (msg->ttype == HSI_MSG_READ)\n\t\thi->control_state &=\n\t\t\t~(SSI_CHANNEL_STATE_POLL | SSI_CHANNEL_STATE_READING);\n\telse if (msg->ttype == HSI_MSG_WRITE &&\n\t\t\thi->control_state & SSI_CHANNEL_STATE_WRITING)\n\t\thi->control_state &= ~SSI_CHANNEL_STATE_WRITING;\n\n\tcs_release_cmd(msg);\n\n\tspin_unlock(&hi->lock);\n}\n\nstatic struct hsi_msg *cs_claim_cmd(struct cs_hsi_iface* ssi)\n{\n\tstruct hsi_msg *msg;\n\n\tBUG_ON(list_empty(&ssi->cmdqueue));\n\n\tmsg = list_first_entry(&ssi->cmdqueue, struct hsi_msg, link);\n\tlist_del(&msg->link);\n\tmsg->destructor = cs_cmd_destructor;\n\n\treturn msg;\n}\n\nstatic void cs_free_cmds(struct cs_hsi_iface *ssi)\n{\n\tstruct hsi_msg *msg, *tmp;\n\n\tlist_for_each_entry_safe(msg, tmp, &ssi->cmdqueue, link) {\n\t\tlist_del(&msg->link);\n\t\tmsg->destructor = NULL;\n\t\tkfree(sg_virt(msg->sgt.sgl));\n\t\thsi_free_msg(msg);\n\t}\n}\n\nstatic int cs_alloc_cmds(struct cs_hsi_iface *hi)\n{\n\tstruct hsi_msg *msg;\n\tu32 *buf;\n\tunsigned int i;\n\n\tINIT_LIST_HEAD(&hi->cmdqueue);\n\n\tfor (i = 0; i < CS_MAX_CMDS; i++) {\n\t\tmsg = hsi_alloc_msg(1, GFP_KERNEL);\n\t\tif (!msg)\n\t\t\tgoto out;\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf) {\n\t\t\thsi_free_msg(msg);\n\t\t\tgoto out;\n\t\t}\n\t\tsg_init_one(msg->sgt.sgl, buf, sizeof(*buf));\n\t\tmsg->channel = cs_char_data.channel_id_cmd;\n\t\tmsg->context = hi;\n\t\tlist_add_tail(&msg->link, &hi->cmdqueue);\n\t}\n\n\treturn 0;\n\nout:\n\tcs_free_cmds(hi);\n\treturn -ENOMEM;\n}\n\nstatic void cs_hsi_data_destructor(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\tconst char *dir = (msg->ttype == HSI_MSG_READ) ? \"TX\" : \"RX\";\n\n\tdev_dbg(&cs_char_data.cl->device, \"Freeing data %s message\\n\", dir);\n\n\tspin_lock(&hi->lock);\n\tif (hi->iface_state != CS_STATE_CLOSED)\n\t\tdev_err(&cs_char_data.cl->device,\n\t\t\t\t\"Data %s flush while device active\\n\", dir);\n\tif (msg->ttype == HSI_MSG_READ)\n\t\thi->data_state &=\n\t\t\t~(SSI_CHANNEL_STATE_POLL | SSI_CHANNEL_STATE_READING);\n\telse\n\t\thi->data_state &= ~SSI_CHANNEL_STATE_WRITING;\n\n\tmsg->status = HSI_STATUS_COMPLETED;\n\tif (unlikely(waitqueue_active(&hi->datawait)))\n\t\twake_up_interruptible(&hi->datawait);\n\n\tspin_unlock(&hi->lock);\n}\n\nstatic int cs_hsi_alloc_data(struct cs_hsi_iface *hi)\n{\n\tstruct hsi_msg *txmsg, *rxmsg;\n\tint res = 0;\n\n\trxmsg = hsi_alloc_msg(1, GFP_KERNEL);\n\tif (!rxmsg) {\n\t\tres = -ENOMEM;\n\t\tgoto out1;\n\t}\n\trxmsg->channel = cs_char_data.channel_id_data;\n\trxmsg->destructor = cs_hsi_data_destructor;\n\trxmsg->context = hi;\n\n\ttxmsg = hsi_alloc_msg(1, GFP_KERNEL);\n\tif (!txmsg) {\n\t\tres = -ENOMEM;\n\t\tgoto out2;\n\t}\n\ttxmsg->channel = cs_char_data.channel_id_data;\n\ttxmsg->destructor = cs_hsi_data_destructor;\n\ttxmsg->context = hi;\n\n\thi->data_rx_msg = rxmsg;\n\thi->data_tx_msg = txmsg;\n\n\treturn 0;\n\nout2:\n\thsi_free_msg(rxmsg);\nout1:\n\treturn res;\n}\n\nstatic void cs_hsi_free_data_msg(struct hsi_msg *msg)\n{\n\tWARN_ON(msg->status != HSI_STATUS_COMPLETED &&\n\t\t\t\t\tmsg->status != HSI_STATUS_ERROR);\n\thsi_free_msg(msg);\n}\n\nstatic void cs_hsi_free_data(struct cs_hsi_iface *hi)\n{\n\tcs_hsi_free_data_msg(hi->data_rx_msg);\n\tcs_hsi_free_data_msg(hi->data_tx_msg);\n}\n\nstatic inline void __cs_hsi_error_pre(struct cs_hsi_iface *hi,\n\t\t\t\t\tstruct hsi_msg *msg, const char *info,\n\t\t\t\t\tunsigned int *state)\n{\n\tspin_lock(&hi->lock);\n\tdev_err(&hi->cl->device, \"HSI %s error, msg %d, state %u\\n\",\n\t\tinfo, msg->status, *state);\n}\n\nstatic inline void __cs_hsi_error_post(struct cs_hsi_iface *hi)\n{\n\tspin_unlock(&hi->lock);\n}\n\nstatic inline void __cs_hsi_error_read_bits(unsigned int *state)\n{\n\t*state |= SSI_CHANNEL_STATE_ERROR;\n\t*state &= ~(SSI_CHANNEL_STATE_READING | SSI_CHANNEL_STATE_POLL);\n}\n\nstatic inline void __cs_hsi_error_write_bits(unsigned int *state)\n{\n\t*state |= SSI_CHANNEL_STATE_ERROR;\n\t*state &= ~SSI_CHANNEL_STATE_WRITING;\n}\n\nstatic void cs_hsi_control_read_error(struct cs_hsi_iface *hi,\n\t\t\t\t\t\t\tstruct hsi_msg *msg)\n{\n\t__cs_hsi_error_pre(hi, msg, \"control read\", &hi->control_state);\n\tcs_release_cmd(msg);\n\t__cs_hsi_error_read_bits(&hi->control_state);\n\t__cs_hsi_error_post(hi);\n}\n\nstatic void cs_hsi_control_write_error(struct cs_hsi_iface *hi,\n\t\t\t\t\t\t\tstruct hsi_msg *msg)\n{\n\t__cs_hsi_error_pre(hi, msg, \"control write\", &hi->control_state);\n\tcs_release_cmd(msg);\n\t__cs_hsi_error_write_bits(&hi->control_state);\n\t__cs_hsi_error_post(hi);\n\n}\n\nstatic void cs_hsi_data_read_error(struct cs_hsi_iface *hi, struct hsi_msg *msg)\n{\n\t__cs_hsi_error_pre(hi, msg, \"data read\", &hi->data_state);\n\t__cs_hsi_error_read_bits(&hi->data_state);\n\t__cs_hsi_error_post(hi);\n}\n\nstatic void cs_hsi_data_write_error(struct cs_hsi_iface *hi,\n\t\t\t\t\t\t\tstruct hsi_msg *msg)\n{\n\t__cs_hsi_error_pre(hi, msg, \"data write\", &hi->data_state);\n\t__cs_hsi_error_write_bits(&hi->data_state);\n\t__cs_hsi_error_post(hi);\n}\n\nstatic void cs_hsi_read_on_control_complete(struct hsi_msg *msg)\n{\n\tu32 cmd = cs_get_cmd(msg);\n\tstruct cs_hsi_iface *hi = msg->context;\n\n\tspin_lock(&hi->lock);\n\thi->control_state &= ~SSI_CHANNEL_STATE_READING;\n\tif (msg->status == HSI_STATUS_ERROR) {\n\t\tdev_err(&hi->cl->device, \"Control RX error detected\\n\");\n\t\tspin_unlock(&hi->lock);\n\t\tcs_hsi_control_read_error(hi, msg);\n\t\tgoto out;\n\t}\n\tdev_dbg(&hi->cl->device, \"Read on control: %08X\\n\", cmd);\n\tcs_release_cmd(msg);\n\tif (hi->flags & CS_FEAT_TSTAMP_RX_CTRL) {\n\t\tstruct timespec64 tspec;\n\t\tstruct cs_timestamp *tstamp =\n\t\t\t&hi->mmap_cfg->tstamp_rx_ctrl;\n\n\t\tktime_get_ts64(&tspec);\n\n\t\ttstamp->tv_sec = (__u32) tspec.tv_sec;\n\t\ttstamp->tv_nsec = (__u32) tspec.tv_nsec;\n\t}\n\tspin_unlock(&hi->lock);\n\n\tcs_notify_control(cmd);\n\nout:\n\tcs_hsi_read_on_control(hi);\n}\n\nstatic void cs_hsi_peek_on_control_complete(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\tint ret;\n\n\tif (msg->status == HSI_STATUS_ERROR) {\n\t\tdev_err(&hi->cl->device, \"Control peek RX error detected\\n\");\n\t\tcs_hsi_control_read_error(hi, msg);\n\t\treturn;\n\t}\n\n\tWARN_ON(!(hi->control_state & SSI_CHANNEL_STATE_READING));\n\n\tdev_dbg(&hi->cl->device, \"Peek on control complete, reading\\n\");\n\tmsg->sgt.nents = 1;\n\tmsg->complete = cs_hsi_read_on_control_complete;\n\tret = hsi_async_read(hi->cl, msg);\n\tif (ret)\n\t\tcs_hsi_control_read_error(hi, msg);\n}\n\nstatic void cs_hsi_read_on_control(struct cs_hsi_iface *hi)\n{\n\tstruct hsi_msg *msg;\n\tint ret;\n\n\tspin_lock(&hi->lock);\n\tif (hi->control_state & SSI_CHANNEL_STATE_READING) {\n\t\tdev_err(&hi->cl->device, \"Control read already pending (%d)\\n\",\n\t\t\thi->control_state);\n\t\tspin_unlock(&hi->lock);\n\t\treturn;\n\t}\n\tif (hi->control_state & SSI_CHANNEL_STATE_ERROR) {\n\t\tdev_err(&hi->cl->device, \"Control read error (%d)\\n\",\n\t\t\thi->control_state);\n\t\tspin_unlock(&hi->lock);\n\t\treturn;\n\t}\n\thi->control_state |= SSI_CHANNEL_STATE_READING;\n\tdev_dbg(&hi->cl->device, \"Issuing RX on control\\n\");\n\tmsg = cs_claim_cmd(hi);\n\tspin_unlock(&hi->lock);\n\n\tmsg->sgt.nents = 0;\n\tmsg->complete = cs_hsi_peek_on_control_complete;\n\tret = hsi_async_read(hi->cl, msg);\n\tif (ret)\n\t\tcs_hsi_control_read_error(hi, msg);\n}\n\nstatic void cs_hsi_write_on_control_complete(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\tif (msg->status == HSI_STATUS_COMPLETED) {\n\t\tspin_lock(&hi->lock);\n\t\thi->control_state &= ~SSI_CHANNEL_STATE_WRITING;\n\t\tcs_release_cmd(msg);\n\t\tspin_unlock(&hi->lock);\n\t} else if (msg->status == HSI_STATUS_ERROR) {\n\t\tcs_hsi_control_write_error(hi, msg);\n\t} else {\n\t\tdev_err(&hi->cl->device,\n\t\t\t\"unexpected status in control write callback %d\\n\",\n\t\t\tmsg->status);\n\t}\n}\n\nstatic int cs_hsi_write_on_control(struct cs_hsi_iface *hi, u32 message)\n{\n\tstruct hsi_msg *msg;\n\tint ret;\n\n\tspin_lock(&hi->lock);\n\tif (hi->control_state & SSI_CHANNEL_STATE_ERROR) {\n\t\tspin_unlock(&hi->lock);\n\t\treturn -EIO;\n\t}\n\tif (hi->control_state & SSI_CHANNEL_STATE_WRITING) {\n\t\tdev_err(&hi->cl->device,\n\t\t\t\"Write still pending on control channel.\\n\");\n\t\tspin_unlock(&hi->lock);\n\t\treturn -EBUSY;\n\t}\n\thi->control_state |= SSI_CHANNEL_STATE_WRITING;\n\tmsg = cs_claim_cmd(hi);\n\tspin_unlock(&hi->lock);\n\n\tcs_set_cmd(msg, message);\n\tmsg->sgt.nents = 1;\n\tmsg->complete = cs_hsi_write_on_control_complete;\n\tdev_dbg(&hi->cl->device,\n\t\t\"Sending control message %08X\\n\", message);\n\tret = hsi_async_write(hi->cl, msg);\n\tif (ret) {\n\t\tdev_err(&hi->cl->device,\n\t\t\t\"async_write failed with %d\\n\", ret);\n\t\tcs_hsi_control_write_error(hi, msg);\n\t}\n\n\t \n\tif (!(hi->control_state & SSI_CHANNEL_STATE_READING)) {\n\t\tdev_err(&hi->cl->device, \"Restarting control reads\\n\");\n\t\tcs_hsi_read_on_control(hi);\n\t}\n\n\treturn 0;\n}\n\nstatic void cs_hsi_read_on_data_complete(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\tu32 payload;\n\n\tif (unlikely(msg->status == HSI_STATUS_ERROR)) {\n\t\tcs_hsi_data_read_error(hi, msg);\n\t\treturn;\n\t}\n\n\tspin_lock(&hi->lock);\n\tWARN_ON(!(hi->data_state & SSI_CHANNEL_STATE_READING));\n\thi->data_state &= ~SSI_CHANNEL_STATE_READING;\n\tpayload = CS_RX_DATA_RECEIVED;\n\tpayload |= hi->rx_slot;\n\thi->rx_slot++;\n\thi->rx_slot %= hi->rx_ptr_boundary;\n\t \n\thi->mmap_cfg->rx_ptr = hi->rx_slot;\n\tif (unlikely(waitqueue_active(&hi->datawait)))\n\t\twake_up_interruptible(&hi->datawait);\n\tspin_unlock(&hi->lock);\n\n\tcs_notify_data(payload, hi->rx_bufs);\n\tcs_hsi_read_on_data(hi);\n}\n\nstatic void cs_hsi_peek_on_data_complete(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\tu32 *address;\n\tint ret;\n\n\tif (unlikely(msg->status == HSI_STATUS_ERROR)) {\n\t\tcs_hsi_data_read_error(hi, msg);\n\t\treturn;\n\t}\n\tif (unlikely(hi->iface_state != CS_STATE_CONFIGURED)) {\n\t\tdev_err(&hi->cl->device, \"Data received in invalid state\\n\");\n\t\tcs_hsi_data_read_error(hi, msg);\n\t\treturn;\n\t}\n\n\tspin_lock(&hi->lock);\n\tWARN_ON(!(hi->data_state & SSI_CHANNEL_STATE_POLL));\n\thi->data_state &= ~SSI_CHANNEL_STATE_POLL;\n\thi->data_state |= SSI_CHANNEL_STATE_READING;\n\tspin_unlock(&hi->lock);\n\n\taddress = (u32 *)(hi->mmap_base +\n\t\t\t\thi->rx_offsets[hi->rx_slot % hi->rx_bufs]);\n\tsg_init_one(msg->sgt.sgl, address, hi->buf_size);\n\tmsg->sgt.nents = 1;\n\tmsg->complete = cs_hsi_read_on_data_complete;\n\tret = hsi_async_read(hi->cl, msg);\n\tif (ret)\n\t\tcs_hsi_data_read_error(hi, msg);\n}\n\n \nstatic inline int cs_state_xfer_active(unsigned int state)\n{\n\treturn (state & SSI_CHANNEL_STATE_WRITING) ||\n\t\t(state & SSI_CHANNEL_STATE_READING);\n}\n\n \nstatic inline int cs_state_idle(unsigned int state)\n{\n\treturn !(state & ~SSI_CHANNEL_STATE_ERROR);\n}\n\nstatic void cs_hsi_read_on_data(struct cs_hsi_iface *hi)\n{\n\tstruct hsi_msg *rxmsg;\n\tint ret;\n\n\tspin_lock(&hi->lock);\n\tif (hi->data_state &\n\t\t(SSI_CHANNEL_STATE_READING | SSI_CHANNEL_STATE_POLL)) {\n\t\tdev_dbg(&hi->cl->device, \"Data read already pending (%u)\\n\",\n\t\t\thi->data_state);\n\t\tspin_unlock(&hi->lock);\n\t\treturn;\n\t}\n\thi->data_state |= SSI_CHANNEL_STATE_POLL;\n\tspin_unlock(&hi->lock);\n\n\trxmsg = hi->data_rx_msg;\n\tsg_init_one(rxmsg->sgt.sgl, (void *)hi->mmap_base, 0);\n\trxmsg->sgt.nents = 0;\n\trxmsg->complete = cs_hsi_peek_on_data_complete;\n\n\tret = hsi_async_read(hi->cl, rxmsg);\n\tif (ret)\n\t\tcs_hsi_data_read_error(hi, rxmsg);\n}\n\nstatic void cs_hsi_write_on_data_complete(struct hsi_msg *msg)\n{\n\tstruct cs_hsi_iface *hi = msg->context;\n\n\tif (msg->status == HSI_STATUS_COMPLETED) {\n\t\tspin_lock(&hi->lock);\n\t\thi->data_state &= ~SSI_CHANNEL_STATE_WRITING;\n\t\tif (unlikely(waitqueue_active(&hi->datawait)))\n\t\t\twake_up_interruptible(&hi->datawait);\n\t\tspin_unlock(&hi->lock);\n\t} else {\n\t\tcs_hsi_data_write_error(hi, msg);\n\t}\n}\n\nstatic int cs_hsi_write_on_data(struct cs_hsi_iface *hi, unsigned int slot)\n{\n\tu32 *address;\n\tstruct hsi_msg *txmsg;\n\tint ret;\n\n\tspin_lock(&hi->lock);\n\tif (hi->iface_state != CS_STATE_CONFIGURED) {\n\t\tdev_err(&hi->cl->device, \"Not configured, aborting\\n\");\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tif (hi->data_state & SSI_CHANNEL_STATE_ERROR) {\n\t\tdev_err(&hi->cl->device, \"HSI error, aborting\\n\");\n\t\tret = -EIO;\n\t\tgoto error;\n\t}\n\tif (hi->data_state & SSI_CHANNEL_STATE_WRITING) {\n\t\tdev_err(&hi->cl->device, \"Write pending on data channel.\\n\");\n\t\tret = -EBUSY;\n\t\tgoto error;\n\t}\n\thi->data_state |= SSI_CHANNEL_STATE_WRITING;\n\tspin_unlock(&hi->lock);\n\n\thi->tx_slot = slot;\n\taddress = (u32 *)(hi->mmap_base + hi->tx_offsets[hi->tx_slot]);\n\ttxmsg = hi->data_tx_msg;\n\tsg_init_one(txmsg->sgt.sgl, address, hi->buf_size);\n\ttxmsg->complete = cs_hsi_write_on_data_complete;\n\tret = hsi_async_write(hi->cl, txmsg);\n\tif (ret)\n\t\tcs_hsi_data_write_error(hi, txmsg);\n\n\treturn ret;\n\nerror:\n\tspin_unlock(&hi->lock);\n\tif (ret == -EIO)\n\t\tcs_hsi_data_write_error(hi, hi->data_tx_msg);\n\n\treturn ret;\n}\n\nstatic unsigned int cs_hsi_get_state(struct cs_hsi_iface *hi)\n{\n\treturn hi->iface_state;\n}\n\nstatic int cs_hsi_command(struct cs_hsi_iface *hi, u32 cmd)\n{\n\tint ret = 0;\n\n\tlocal_bh_disable();\n\tswitch (cmd & TARGET_MASK) {\n\tcase TARGET_REMOTE:\n\t\tret = cs_hsi_write_on_control(hi, cmd);\n\t\tbreak;\n\tcase TARGET_LOCAL:\n\t\tif ((cmd & CS_CMD_MASK) == CS_TX_DATA_READY)\n\t\t\tret = cs_hsi_write_on_data(hi, cmd & CS_PARAM_MASK);\n\t\telse\n\t\t\tret = -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\tlocal_bh_enable();\n\n\treturn ret;\n}\n\nstatic void cs_hsi_set_wakeline(struct cs_hsi_iface *hi, bool new_state)\n{\n\tint change = 0;\n\n\tspin_lock_bh(&hi->lock);\n\tif (hi->wakeline_state != new_state) {\n\t\thi->wakeline_state = new_state;\n\t\tchange = 1;\n\t\tdev_dbg(&hi->cl->device, \"setting wake line to %d (%p)\\n\",\n\t\t\tnew_state, hi->cl);\n\t}\n\tspin_unlock_bh(&hi->lock);\n\n\tif (change) {\n\t\tif (new_state)\n\t\t\tssip_slave_start_tx(hi->master);\n\t\telse\n\t\t\tssip_slave_stop_tx(hi->master);\n\t}\n\n\tdev_dbg(&hi->cl->device, \"wake line set to %d (%p)\\n\",\n\t\tnew_state, hi->cl);\n}\n\nstatic void set_buffer_sizes(struct cs_hsi_iface *hi, int rx_bufs, int tx_bufs)\n{\n\thi->rx_bufs = rx_bufs;\n\thi->tx_bufs = tx_bufs;\n\thi->mmap_cfg->rx_bufs = rx_bufs;\n\thi->mmap_cfg->tx_bufs = tx_bufs;\n\n\tif (hi->flags & CS_FEAT_ROLLING_RX_COUNTER) {\n\t\t \n\t\thi->rx_ptr_boundary = (rx_bufs << RX_PTR_BOUNDARY_SHIFT);\n\t\thi->mmap_cfg->rx_ptr_boundary = hi->rx_ptr_boundary;\n\t} else {\n\t\thi->rx_ptr_boundary = hi->rx_bufs;\n\t}\n}\n\nstatic int check_buf_params(struct cs_hsi_iface *hi,\n\t\t\t\t\tconst struct cs_buffer_config *buf_cfg)\n{\n\tsize_t buf_size_aligned = L1_CACHE_ALIGN(buf_cfg->buf_size) *\n\t\t\t\t\t(buf_cfg->rx_bufs + buf_cfg->tx_bufs);\n\tsize_t ctrl_size_aligned = L1_CACHE_ALIGN(sizeof(*hi->mmap_cfg));\n\tint r = 0;\n\n\tif (buf_cfg->rx_bufs > CS_MAX_BUFFERS ||\n\t\t\t\t\tbuf_cfg->tx_bufs > CS_MAX_BUFFERS) {\n\t\tr = -EINVAL;\n\t} else if ((buf_size_aligned + ctrl_size_aligned) >= hi->mmap_size) {\n\t\tdev_err(&hi->cl->device, \"No space for the requested buffer \"\n\t\t\t\"configuration\\n\");\n\t\tr = -ENOBUFS;\n\t}\n\n\treturn r;\n}\n\n \nstatic int cs_hsi_data_sync(struct cs_hsi_iface *hi)\n{\n\tint r = 0;\n\n\tspin_lock_bh(&hi->lock);\n\n\tif (!cs_state_xfer_active(hi->data_state)) {\n\t\tdev_dbg(&hi->cl->device, \"hsi_data_sync break, idle\\n\");\n\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tint s;\n\t\tDEFINE_WAIT(wait);\n\t\tif (!cs_state_xfer_active(hi->data_state))\n\t\t\tgoto out;\n\t\tif (signal_pending(current)) {\n\t\t\tr = -ERESTARTSYS;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tprepare_to_wait(&hi->datawait, &wait, TASK_INTERRUPTIBLE);\n\t\tspin_unlock_bh(&hi->lock);\n\t\ts = schedule_timeout(\n\t\t\tmsecs_to_jiffies(CS_HSI_TRANSFER_TIMEOUT_MS));\n\t\tspin_lock_bh(&hi->lock);\n\t\tfinish_wait(&hi->datawait, &wait);\n\t\tif (!s) {\n\t\t\tdev_dbg(&hi->cl->device,\n\t\t\t\t\"hsi_data_sync timeout after %d ms\\n\",\n\t\t\t\tCS_HSI_TRANSFER_TIMEOUT_MS);\n\t\t\tr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tspin_unlock_bh(&hi->lock);\n\tdev_dbg(&hi->cl->device, \"hsi_data_sync done with res %d\\n\", r);\n\n\treturn r;\n}\n\nstatic void cs_hsi_data_enable(struct cs_hsi_iface *hi,\n\t\t\t\t\tstruct cs_buffer_config *buf_cfg)\n{\n\tunsigned int data_start, i;\n\n\tBUG_ON(hi->buf_size == 0);\n\n\tset_buffer_sizes(hi, buf_cfg->rx_bufs, buf_cfg->tx_bufs);\n\n\thi->slot_size = L1_CACHE_ALIGN(hi->buf_size);\n\tdev_dbg(&hi->cl->device,\n\t\t\t\"setting slot size to %u, buf size %u, align %u\\n\",\n\t\t\thi->slot_size, hi->buf_size, L1_CACHE_BYTES);\n\n\tdata_start = L1_CACHE_ALIGN(sizeof(*hi->mmap_cfg));\n\tdev_dbg(&hi->cl->device,\n\t\t\t\"setting data start at %u, cfg block %u, align %u\\n\",\n\t\t\tdata_start, sizeof(*hi->mmap_cfg), L1_CACHE_BYTES);\n\n\tfor (i = 0; i < hi->mmap_cfg->rx_bufs; i++) {\n\t\thi->rx_offsets[i] = data_start + i * hi->slot_size;\n\t\thi->mmap_cfg->rx_offsets[i] = hi->rx_offsets[i];\n\t\tdev_dbg(&hi->cl->device, \"DL buf #%u at %u\\n\",\n\t\t\t\t\ti, hi->rx_offsets[i]);\n\t}\n\tfor (i = 0; i < hi->mmap_cfg->tx_bufs; i++) {\n\t\thi->tx_offsets[i] = data_start +\n\t\t\t(i + hi->mmap_cfg->rx_bufs) * hi->slot_size;\n\t\thi->mmap_cfg->tx_offsets[i] = hi->tx_offsets[i];\n\t\tdev_dbg(&hi->cl->device, \"UL buf #%u at %u\\n\",\n\t\t\t\t\ti, hi->rx_offsets[i]);\n\t}\n\n\thi->iface_state = CS_STATE_CONFIGURED;\n}\n\nstatic void cs_hsi_data_disable(struct cs_hsi_iface *hi, int old_state)\n{\n\tif (old_state == CS_STATE_CONFIGURED) {\n\t\tdev_dbg(&hi->cl->device,\n\t\t\t\"closing data channel with slot size 0\\n\");\n\t\thi->iface_state = CS_STATE_OPENED;\n\t}\n}\n\nstatic int cs_hsi_buf_config(struct cs_hsi_iface *hi,\n\t\t\t\t\tstruct cs_buffer_config *buf_cfg)\n{\n\tint r = 0;\n\tunsigned int old_state = hi->iface_state;\n\n\tspin_lock_bh(&hi->lock);\n\t \n\tif (old_state == CS_STATE_CONFIGURED)\n\t\thi->iface_state = CS_STATE_OPENED;\n\tspin_unlock_bh(&hi->lock);\n\n\t \n\tr = cs_hsi_data_sync(hi);\n\tif (r < 0)\n\t\treturn r;\n\n\tWARN_ON(cs_state_xfer_active(hi->data_state));\n\n\tspin_lock_bh(&hi->lock);\n\tr = check_buf_params(hi, buf_cfg);\n\tif (r < 0)\n\t\tgoto error;\n\n\thi->buf_size = buf_cfg->buf_size;\n\thi->mmap_cfg->buf_size = hi->buf_size;\n\thi->flags = buf_cfg->flags;\n\n\thi->rx_slot = 0;\n\thi->tx_slot = 0;\n\thi->slot_size = 0;\n\n\tif (hi->buf_size)\n\t\tcs_hsi_data_enable(hi, buf_cfg);\n\telse\n\t\tcs_hsi_data_disable(hi, old_state);\n\n\tspin_unlock_bh(&hi->lock);\n\n\tif (old_state != hi->iface_state) {\n\t\tif (hi->iface_state == CS_STATE_CONFIGURED) {\n\t\t\tcpu_latency_qos_add_request(&hi->pm_qos_req,\n\t\t\t\tCS_QOS_LATENCY_FOR_DATA_USEC);\n\t\t\tlocal_bh_disable();\n\t\t\tcs_hsi_read_on_data(hi);\n\t\t\tlocal_bh_enable();\n\t\t} else if (old_state == CS_STATE_CONFIGURED) {\n\t\t\tcpu_latency_qos_remove_request(&hi->pm_qos_req);\n\t\t}\n\t}\n\treturn r;\n\nerror:\n\tspin_unlock_bh(&hi->lock);\n\treturn r;\n}\n\nstatic int cs_hsi_start(struct cs_hsi_iface **hi, struct hsi_client *cl,\n\t\t\tunsigned long mmap_base, unsigned long mmap_size)\n{\n\tint err = 0;\n\tstruct cs_hsi_iface *hsi_if = kzalloc(sizeof(*hsi_if), GFP_KERNEL);\n\n\tdev_dbg(&cl->device, \"cs_hsi_start\\n\");\n\n\tif (!hsi_if) {\n\t\terr = -ENOMEM;\n\t\tgoto leave0;\n\t}\n\tspin_lock_init(&hsi_if->lock);\n\thsi_if->cl = cl;\n\thsi_if->iface_state = CS_STATE_CLOSED;\n\thsi_if->mmap_cfg = (struct cs_mmap_config_block *)mmap_base;\n\thsi_if->mmap_base = mmap_base;\n\thsi_if->mmap_size = mmap_size;\n\tmemset(hsi_if->mmap_cfg, 0, sizeof(*hsi_if->mmap_cfg));\n\tinit_waitqueue_head(&hsi_if->datawait);\n\terr = cs_alloc_cmds(hsi_if);\n\tif (err < 0) {\n\t\tdev_err(&cl->device, \"Unable to alloc HSI messages\\n\");\n\t\tgoto leave1;\n\t}\n\terr = cs_hsi_alloc_data(hsi_if);\n\tif (err < 0) {\n\t\tdev_err(&cl->device, \"Unable to alloc HSI messages for data\\n\");\n\t\tgoto leave2;\n\t}\n\terr = hsi_claim_port(cl, 1);\n\tif (err < 0) {\n\t\tdev_err(&cl->device,\n\t\t\t\t\"Could not open, HSI port already claimed\\n\");\n\t\tgoto leave3;\n\t}\n\thsi_if->master = ssip_slave_get_master(cl);\n\tif (IS_ERR(hsi_if->master)) {\n\t\terr = PTR_ERR(hsi_if->master);\n\t\tdev_err(&cl->device, \"Could not get HSI master client\\n\");\n\t\tgoto leave4;\n\t}\n\tif (!ssip_slave_running(hsi_if->master)) {\n\t\terr = -ENODEV;\n\t\tdev_err(&cl->device,\n\t\t\t\t\"HSI port not initialized\\n\");\n\t\tgoto leave4;\n\t}\n\n\thsi_if->iface_state = CS_STATE_OPENED;\n\tlocal_bh_disable();\n\tcs_hsi_read_on_control(hsi_if);\n\tlocal_bh_enable();\n\n\tdev_dbg(&cl->device, \"cs_hsi_start...done\\n\");\n\n\tBUG_ON(!hi);\n\t*hi = hsi_if;\n\n\treturn 0;\n\nleave4:\n\thsi_release_port(cl);\nleave3:\n\tcs_hsi_free_data(hsi_if);\nleave2:\n\tcs_free_cmds(hsi_if);\nleave1:\n\tkfree(hsi_if);\nleave0:\n\tdev_dbg(&cl->device, \"cs_hsi_start...done/error\\n\\n\");\n\n\treturn err;\n}\n\nstatic void cs_hsi_stop(struct cs_hsi_iface *hi)\n{\n\tdev_dbg(&hi->cl->device, \"cs_hsi_stop\\n\");\n\tcs_hsi_set_wakeline(hi, 0);\n\tssip_slave_put_master(hi->master);\n\n\t \n\thi->iface_state = CS_STATE_CLOSED;\n\thsi_release_port(hi->cl);\n\n\t \n\tWARN_ON(!cs_state_idle(hi->control_state));\n\tWARN_ON(!cs_state_idle(hi->data_state));\n\n\tif (cpu_latency_qos_request_active(&hi->pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&hi->pm_qos_req);\n\n\tspin_lock_bh(&hi->lock);\n\tcs_hsi_free_data(hi);\n\tcs_free_cmds(hi);\n\tspin_unlock_bh(&hi->lock);\n\tkfree(hi);\n}\n\nstatic vm_fault_t cs_char_vma_fault(struct vm_fault *vmf)\n{\n\tstruct cs_char *csdata = vmf->vma->vm_private_data;\n\tstruct page *page;\n\n\tpage = virt_to_page((void *)csdata->mmap_base);\n\tget_page(page);\n\tvmf->page = page;\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct cs_char_vm_ops = {\n\t.fault\t= cs_char_vma_fault,\n};\n\nstatic int cs_char_fasync(int fd, struct file *file, int on)\n{\n\tstruct cs_char *csdata = file->private_data;\n\n\tif (fasync_helper(fd, file, on, &csdata->async_queue) < 0)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic __poll_t cs_char_poll(struct file *file, poll_table *wait)\n{\n\tstruct cs_char *csdata = file->private_data;\n\t__poll_t ret = 0;\n\n\tpoll_wait(file, &cs_char_data.wait, wait);\n\tspin_lock_bh(&csdata->lock);\n\tif (!list_empty(&csdata->chardev_queue))\n\t\tret = EPOLLIN | EPOLLRDNORM;\n\telse if (!list_empty(&csdata->dataind_queue))\n\t\tret = EPOLLIN | EPOLLRDNORM;\n\tspin_unlock_bh(&csdata->lock);\n\n\treturn ret;\n}\n\nstatic ssize_t cs_char_read(struct file *file, char __user *buf, size_t count,\n\t\t\t\t\t\t\t\tloff_t *unused)\n{\n\tstruct cs_char *csdata = file->private_data;\n\tu32 data;\n\tssize_t retval;\n\n\tif (count < sizeof(data))\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tspin_lock_bh(&csdata->lock);\n\t\tif (!list_empty(&csdata->chardev_queue)) {\n\t\t\tdata = cs_pop_entry(&csdata->chardev_queue);\n\t\t} else if (!list_empty(&csdata->dataind_queue)) {\n\t\t\tdata = cs_pop_entry(&csdata->dataind_queue);\n\t\t\tcsdata->dataind_pending--;\n\t\t} else {\n\t\t\tdata = 0;\n\t\t}\n\t\tspin_unlock_bh(&csdata->lock);\n\n\t\tif (data)\n\t\t\tbreak;\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tgoto out;\n\t\t} else if (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tgoto out;\n\t\t}\n\t\tprepare_to_wait_exclusive(&csdata->wait, &wait,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\tschedule();\n\t\tfinish_wait(&csdata->wait, &wait);\n\t}\n\n\tretval = put_user(data, (u32 __user *)buf);\n\tif (!retval)\n\t\tretval = sizeof(data);\n\nout:\n\treturn retval;\n}\n\nstatic ssize_t cs_char_write(struct file *file, const char __user *buf,\n\t\t\t\t\t\tsize_t count, loff_t *unused)\n{\n\tstruct cs_char *csdata = file->private_data;\n\tu32 data;\n\tint err;\n\tssize_t\tretval;\n\n\tif (count < sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (get_user(data, (u32 __user *)buf))\n\t\tretval = -EFAULT;\n\telse\n\t\tretval = count;\n\n\terr = cs_hsi_command(csdata->hi, data);\n\tif (err < 0)\n\t\tretval = err;\n\n\treturn retval;\n}\n\nstatic long cs_char_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\tunsigned long arg)\n{\n\tstruct cs_char *csdata = file->private_data;\n\tint r = 0;\n\n\tswitch (cmd) {\n\tcase CS_GET_STATE: {\n\t\tunsigned int state;\n\n\t\tstate = cs_hsi_get_state(csdata->hi);\n\t\tif (copy_to_user((void __user *)arg, &state, sizeof(state)))\n\t\t\tr = -EFAULT;\n\n\t\tbreak;\n\t}\n\tcase CS_SET_WAKELINE: {\n\t\tunsigned int state;\n\n\t\tif (copy_from_user(&state, (void __user *)arg, sizeof(state))) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (state > 1) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tcs_hsi_set_wakeline(csdata->hi, !!state);\n\n\t\tbreak;\n\t}\n\tcase CS_GET_IF_VERSION: {\n\t\tunsigned int ifver = CS_IF_VERSION;\n\n\t\tif (copy_to_user((void __user *)arg, &ifver, sizeof(ifver)))\n\t\t\tr = -EFAULT;\n\n\t\tbreak;\n\t}\n\tcase CS_CONFIG_BUFS: {\n\t\tstruct cs_buffer_config buf_cfg;\n\n\t\tif (copy_from_user(&buf_cfg, (void __user *)arg,\n\t\t\t\t\t\t\tsizeof(buf_cfg)))\n\t\t\tr = -EFAULT;\n\t\telse\n\t\t\tr = cs_hsi_buf_config(csdata->hi, &buf_cfg);\n\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -ENOTTY;\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic int cs_char_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\n\tif (vma_pages(vma) != 1)\n\t\treturn -EINVAL;\n\n\tvm_flags_set(vma, VM_IO | VM_DONTDUMP | VM_DONTEXPAND);\n\tvma->vm_ops = &cs_char_vm_ops;\n\tvma->vm_private_data = file->private_data;\n\n\treturn 0;\n}\n\nstatic int cs_char_open(struct inode *unused, struct file *file)\n{\n\tint ret = 0;\n\tunsigned long p;\n\n\tspin_lock_bh(&cs_char_data.lock);\n\tif (cs_char_data.opened) {\n\t\tret = -EBUSY;\n\t\tspin_unlock_bh(&cs_char_data.lock);\n\t\tgoto out1;\n\t}\n\tcs_char_data.opened = 1;\n\tcs_char_data.dataind_pending = 0;\n\tspin_unlock_bh(&cs_char_data.lock);\n\n\tp = get_zeroed_page(GFP_KERNEL);\n\tif (!p) {\n\t\tret = -ENOMEM;\n\t\tgoto out2;\n\t}\n\n\tret = cs_hsi_start(&cs_char_data.hi, cs_char_data.cl, p, CS_MMAP_SIZE);\n\tif (ret) {\n\t\tdev_err(&cs_char_data.cl->device, \"Unable to initialize HSI\\n\");\n\t\tgoto out3;\n\t}\n\n\t \n\tcs_char_data.mmap_base = p;\n\tcs_char_data.mmap_size = CS_MMAP_SIZE;\n\n\tfile->private_data = &cs_char_data;\n\n\treturn 0;\n\nout3:\n\tfree_page(p);\nout2:\n\tspin_lock_bh(&cs_char_data.lock);\n\tcs_char_data.opened = 0;\n\tspin_unlock_bh(&cs_char_data.lock);\nout1:\n\treturn ret;\n}\n\nstatic void cs_free_char_queue(struct list_head *head)\n{\n\tstruct char_queue *entry;\n\tstruct list_head *cursor, *next;\n\n\tif (!list_empty(head)) {\n\t\tlist_for_each_safe(cursor, next, head) {\n\t\t\tentry = list_entry(cursor, struct char_queue, list);\n\t\t\tlist_del(&entry->list);\n\t\t\tkfree(entry);\n\t\t}\n\t}\n\n}\n\nstatic int cs_char_release(struct inode *unused, struct file *file)\n{\n\tstruct cs_char *csdata = file->private_data;\n\n\tcs_hsi_stop(csdata->hi);\n\tspin_lock_bh(&csdata->lock);\n\tcsdata->hi = NULL;\n\tfree_page(csdata->mmap_base);\n\tcs_free_char_queue(&csdata->chardev_queue);\n\tcs_free_char_queue(&csdata->dataind_queue);\n\tcsdata->opened = 0;\n\tspin_unlock_bh(&csdata->lock);\n\n\treturn 0;\n}\n\nstatic const struct file_operations cs_char_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.read\t\t= cs_char_read,\n\t.write\t\t= cs_char_write,\n\t.poll\t\t= cs_char_poll,\n\t.unlocked_ioctl\t= cs_char_ioctl,\n\t.mmap\t\t= cs_char_mmap,\n\t.open\t\t= cs_char_open,\n\t.release\t= cs_char_release,\n\t.fasync\t\t= cs_char_fasync,\n};\n\nstatic struct miscdevice cs_char_miscdev = {\n\t.minor\t= MISC_DYNAMIC_MINOR,\n\t.name\t= \"cmt_speech\",\n\t.fops\t= &cs_char_fops\n};\n\nstatic int cs_hsi_client_probe(struct device *dev)\n{\n\tint err = 0;\n\tstruct hsi_client *cl = to_hsi_client(dev);\n\n\tdev_dbg(dev, \"hsi_client_probe\\n\");\n\tinit_waitqueue_head(&cs_char_data.wait);\n\tspin_lock_init(&cs_char_data.lock);\n\tcs_char_data.opened = 0;\n\tcs_char_data.cl = cl;\n\tcs_char_data.hi = NULL;\n\tINIT_LIST_HEAD(&cs_char_data.chardev_queue);\n\tINIT_LIST_HEAD(&cs_char_data.dataind_queue);\n\n\tcs_char_data.channel_id_cmd = hsi_get_channel_id_by_name(cl,\n\t\t\"speech-control\");\n\tif (cs_char_data.channel_id_cmd < 0) {\n\t\terr = cs_char_data.channel_id_cmd;\n\t\tdev_err(dev, \"Could not get cmd channel (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\tcs_char_data.channel_id_data = hsi_get_channel_id_by_name(cl,\n\t\t\"speech-data\");\n\tif (cs_char_data.channel_id_data < 0) {\n\t\terr = cs_char_data.channel_id_data;\n\t\tdev_err(dev, \"Could not get data channel (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = misc_register(&cs_char_miscdev);\n\tif (err)\n\t\tdev_err(dev, \"Failed to register: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int cs_hsi_client_remove(struct device *dev)\n{\n\tstruct cs_hsi_iface *hi;\n\n\tdev_dbg(dev, \"hsi_client_remove\\n\");\n\tmisc_deregister(&cs_char_miscdev);\n\tspin_lock_bh(&cs_char_data.lock);\n\thi = cs_char_data.hi;\n\tcs_char_data.hi = NULL;\n\tspin_unlock_bh(&cs_char_data.lock);\n\tif (hi)\n\t\tcs_hsi_stop(hi);\n\n\treturn 0;\n}\n\nstatic struct hsi_client_driver cs_hsi_driver = {\n\t.driver = {\n\t\t.name\t= \"cmt-speech\",\n\t\t.owner\t= THIS_MODULE,\n\t\t.probe\t= cs_hsi_client_probe,\n\t\t.remove\t= cs_hsi_client_remove,\n\t},\n};\n\nstatic int __init cs_char_init(void)\n{\n\tpr_info(\"CMT speech driver added\\n\");\n\treturn hsi_register_client_driver(&cs_hsi_driver);\n}\nmodule_init(cs_char_init);\n\nstatic void __exit cs_char_exit(void)\n{\n\thsi_unregister_client_driver(&cs_hsi_driver);\n\tpr_info(\"CMT speech driver removed\\n\");\n}\nmodule_exit(cs_char_exit);\n\nMODULE_ALIAS(\"hsi:cmt-speech\");\nMODULE_AUTHOR(\"Kai Vehmanen <kai.vehmanen@nokia.com>\");\nMODULE_AUTHOR(\"Peter Ujfalusi <peter.ujfalusi@nokia.com>\");\nMODULE_DESCRIPTION(\"CMT speech driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}