{
  "module_name": "main.c",
  "hash_id": "3a3099a4bf9458e829d114cb0f208f4c59266747ec2a5a2aa04cd4907e76a844",
  "original_prompt": "Ingested from linux-6.6.14/drivers/bus/mhi/host/main.c",
  "human_readable_source": "\n \n\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/mhi.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include \"internal.h\"\n\nint __must_check mhi_read_reg(struct mhi_controller *mhi_cntrl,\n\t\t\t      void __iomem *base, u32 offset, u32 *out)\n{\n\treturn mhi_cntrl->read_reg(mhi_cntrl, base + offset, out);\n}\n\nint __must_check mhi_read_reg_field(struct mhi_controller *mhi_cntrl,\n\t\t\t\t    void __iomem *base, u32 offset,\n\t\t\t\t    u32 mask, u32 *out)\n{\n\tu32 tmp;\n\tint ret;\n\n\tret = mhi_read_reg(mhi_cntrl, base, offset, &tmp);\n\tif (ret)\n\t\treturn ret;\n\n\t*out = (tmp & mask) >> __ffs(mask);\n\n\treturn 0;\n}\n\nint __must_check mhi_poll_reg_field(struct mhi_controller *mhi_cntrl,\n\t\t\t\t    void __iomem *base, u32 offset,\n\t\t\t\t    u32 mask, u32 val, u32 delayus)\n{\n\tint ret;\n\tu32 out, retry = (mhi_cntrl->timeout_ms * 1000) / delayus;\n\n\twhile (retry--) {\n\t\tret = mhi_read_reg_field(mhi_cntrl, base, offset, mask, &out);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (out == val)\n\t\t\treturn 0;\n\n\t\tfsleep(delayus);\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nvoid mhi_write_reg(struct mhi_controller *mhi_cntrl, void __iomem *base,\n\t\t   u32 offset, u32 val)\n{\n\tmhi_cntrl->write_reg(mhi_cntrl, base + offset, val);\n}\n\nint __must_check mhi_write_reg_field(struct mhi_controller *mhi_cntrl,\n\t\t\t\t     void __iomem *base, u32 offset, u32 mask,\n\t\t\t\t     u32 val)\n{\n\tint ret;\n\tu32 tmp;\n\n\tret = mhi_read_reg(mhi_cntrl, base, offset, &tmp);\n\tif (ret)\n\t\treturn ret;\n\n\ttmp &= ~mask;\n\ttmp |= (val << __ffs(mask));\n\tmhi_write_reg(mhi_cntrl, base, offset, tmp);\n\n\treturn 0;\n}\n\nvoid mhi_write_db(struct mhi_controller *mhi_cntrl, void __iomem *db_addr,\n\t\t  dma_addr_t db_val)\n{\n\tmhi_write_reg(mhi_cntrl, db_addr, 4, upper_32_bits(db_val));\n\tmhi_write_reg(mhi_cntrl, db_addr, 0, lower_32_bits(db_val));\n}\n\nvoid mhi_db_brstmode(struct mhi_controller *mhi_cntrl,\n\t\t     struct db_cfg *db_cfg,\n\t\t     void __iomem *db_addr,\n\t\t     dma_addr_t db_val)\n{\n\tif (db_cfg->db_mode) {\n\t\tdb_cfg->db_val = db_val;\n\t\tmhi_write_db(mhi_cntrl, db_addr, db_val);\n\t\tdb_cfg->db_mode = 0;\n\t}\n}\n\nvoid mhi_db_brstmode_disable(struct mhi_controller *mhi_cntrl,\n\t\t\t     struct db_cfg *db_cfg,\n\t\t\t     void __iomem *db_addr,\n\t\t\t     dma_addr_t db_val)\n{\n\tdb_cfg->db_val = db_val;\n\tmhi_write_db(mhi_cntrl, db_addr, db_val);\n}\n\nvoid mhi_ring_er_db(struct mhi_event *mhi_event)\n{\n\tstruct mhi_ring *ring = &mhi_event->ring;\n\n\tmhi_event->db_cfg.process_db(mhi_event->mhi_cntrl, &mhi_event->db_cfg,\n\t\t\t\t     ring->db_addr, le64_to_cpu(*ring->ctxt_wp));\n}\n\nvoid mhi_ring_cmd_db(struct mhi_controller *mhi_cntrl, struct mhi_cmd *mhi_cmd)\n{\n\tdma_addr_t db;\n\tstruct mhi_ring *ring = &mhi_cmd->ring;\n\n\tdb = ring->iommu_base + (ring->wp - ring->base);\n\t*ring->ctxt_wp = cpu_to_le64(db);\n\tmhi_write_db(mhi_cntrl, ring->db_addr, db);\n}\n\nvoid mhi_ring_chan_db(struct mhi_controller *mhi_cntrl,\n\t\t      struct mhi_chan *mhi_chan)\n{\n\tstruct mhi_ring *ring = &mhi_chan->tre_ring;\n\tdma_addr_t db;\n\n\tdb = ring->iommu_base + (ring->wp - ring->base);\n\n\t \n\tdma_wmb();\n\t*ring->ctxt_wp = cpu_to_le64(db);\n\n\tmhi_chan->db_cfg.process_db(mhi_cntrl, &mhi_chan->db_cfg,\n\t\t\t\t    ring->db_addr, db);\n}\n\nenum mhi_ee_type mhi_get_exec_env(struct mhi_controller *mhi_cntrl)\n{\n\tu32 exec;\n\tint ret = mhi_read_reg(mhi_cntrl, mhi_cntrl->bhi, BHI_EXECENV, &exec);\n\n\treturn (ret) ? MHI_EE_MAX : exec;\n}\nEXPORT_SYMBOL_GPL(mhi_get_exec_env);\n\nenum mhi_state mhi_get_mhi_state(struct mhi_controller *mhi_cntrl)\n{\n\tu32 state;\n\tint ret = mhi_read_reg_field(mhi_cntrl, mhi_cntrl->regs, MHISTATUS,\n\t\t\t\t     MHISTATUS_MHISTATE_MASK, &state);\n\treturn ret ? MHI_STATE_MAX : state;\n}\nEXPORT_SYMBOL_GPL(mhi_get_mhi_state);\n\nvoid mhi_soc_reset(struct mhi_controller *mhi_cntrl)\n{\n\tif (mhi_cntrl->reset) {\n\t\tmhi_cntrl->reset(mhi_cntrl);\n\t\treturn;\n\t}\n\n\t \n\tmhi_write_reg(mhi_cntrl, mhi_cntrl->regs, MHI_SOC_RESET_REQ_OFFSET,\n\t\t      MHI_SOC_RESET_REQ);\n}\nEXPORT_SYMBOL_GPL(mhi_soc_reset);\n\nint mhi_map_single_no_bb(struct mhi_controller *mhi_cntrl,\n\t\t\t struct mhi_buf_info *buf_info)\n{\n\tbuf_info->p_addr = dma_map_single(mhi_cntrl->cntrl_dev,\n\t\t\t\t\t  buf_info->v_addr, buf_info->len,\n\t\t\t\t\t  buf_info->dir);\n\tif (dma_mapping_error(mhi_cntrl->cntrl_dev, buf_info->p_addr))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint mhi_map_single_use_bb(struct mhi_controller *mhi_cntrl,\n\t\t\t  struct mhi_buf_info *buf_info)\n{\n\tvoid *buf = dma_alloc_coherent(mhi_cntrl->cntrl_dev, buf_info->len,\n\t\t\t\t       &buf_info->p_addr, GFP_ATOMIC);\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (buf_info->dir == DMA_TO_DEVICE)\n\t\tmemcpy(buf, buf_info->v_addr, buf_info->len);\n\n\tbuf_info->bb_addr = buf;\n\n\treturn 0;\n}\n\nvoid mhi_unmap_single_no_bb(struct mhi_controller *mhi_cntrl,\n\t\t\t    struct mhi_buf_info *buf_info)\n{\n\tdma_unmap_single(mhi_cntrl->cntrl_dev, buf_info->p_addr, buf_info->len,\n\t\t\t buf_info->dir);\n}\n\nvoid mhi_unmap_single_use_bb(struct mhi_controller *mhi_cntrl,\n\t\t\t     struct mhi_buf_info *buf_info)\n{\n\tif (buf_info->dir == DMA_FROM_DEVICE)\n\t\tmemcpy(buf_info->v_addr, buf_info->bb_addr, buf_info->len);\n\n\tdma_free_coherent(mhi_cntrl->cntrl_dev, buf_info->len,\n\t\t\t  buf_info->bb_addr, buf_info->p_addr);\n}\n\nstatic int get_nr_avail_ring_elements(struct mhi_controller *mhi_cntrl,\n\t\t\t\t      struct mhi_ring *ring)\n{\n\tint nr_el;\n\n\tif (ring->wp < ring->rp) {\n\t\tnr_el = ((ring->rp - ring->wp) / ring->el_size) - 1;\n\t} else {\n\t\tnr_el = (ring->rp - ring->base) / ring->el_size;\n\t\tnr_el += ((ring->base + ring->len - ring->wp) /\n\t\t\t  ring->el_size) - 1;\n\t}\n\n\treturn nr_el;\n}\n\nstatic void *mhi_to_virtual(struct mhi_ring *ring, dma_addr_t addr)\n{\n\treturn (addr - ring->iommu_base) + ring->base;\n}\n\nstatic void mhi_add_ring_element(struct mhi_controller *mhi_cntrl,\n\t\t\t\t struct mhi_ring *ring)\n{\n\tring->wp += ring->el_size;\n\tif (ring->wp >= (ring->base + ring->len))\n\t\tring->wp = ring->base;\n\t \n\tsmp_wmb();\n}\n\nstatic void mhi_del_ring_element(struct mhi_controller *mhi_cntrl,\n\t\t\t\t struct mhi_ring *ring)\n{\n\tring->rp += ring->el_size;\n\tif (ring->rp >= (ring->base + ring->len))\n\t\tring->rp = ring->base;\n\t \n\tsmp_wmb();\n}\n\nstatic bool is_valid_ring_ptr(struct mhi_ring *ring, dma_addr_t addr)\n{\n\treturn addr >= ring->iommu_base && addr < ring->iommu_base + ring->len;\n}\n\nint mhi_destroy_device(struct device *dev, void *data)\n{\n\tstruct mhi_chan *ul_chan, *dl_chan;\n\tstruct mhi_device *mhi_dev;\n\tstruct mhi_controller *mhi_cntrl;\n\tenum mhi_ee_type ee = MHI_EE_MAX;\n\n\tif (dev->bus != &mhi_bus_type)\n\t\treturn 0;\n\n\tmhi_dev = to_mhi_device(dev);\n\tmhi_cntrl = mhi_dev->mhi_cntrl;\n\n\t \n\tif (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)\n\t\treturn 0;\n\n\tul_chan = mhi_dev->ul_chan;\n\tdl_chan = mhi_dev->dl_chan;\n\n\t \n\tif (data)\n\t\tee = *(enum mhi_ee_type *)data;\n\n\t \n\tif (ul_chan) {\n\t\tif (ee != MHI_EE_MAX && !(ul_chan->ee_mask & BIT(ee)))\n\t\t\treturn 0;\n\n\t\tput_device(&ul_chan->mhi_dev->dev);\n\t}\n\n\tif (dl_chan) {\n\t\tif (ee != MHI_EE_MAX && !(dl_chan->ee_mask & BIT(ee)))\n\t\t\treturn 0;\n\n\t\tput_device(&dl_chan->mhi_dev->dev);\n\t}\n\n\tdev_dbg(&mhi_cntrl->mhi_dev->dev, \"destroy device for chan:%s\\n\",\n\t\t mhi_dev->name);\n\n\t \n\tdevice_del(dev);\n\tput_device(dev);\n\n\treturn 0;\n}\n\nint mhi_get_free_desc_count(struct mhi_device *mhi_dev,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ?\n\t\tmhi_dev->ul_chan : mhi_dev->dl_chan;\n\tstruct mhi_ring *tre_ring = &mhi_chan->tre_ring;\n\n\treturn get_nr_avail_ring_elements(mhi_cntrl, tre_ring);\n}\nEXPORT_SYMBOL_GPL(mhi_get_free_desc_count);\n\nvoid mhi_notify(struct mhi_device *mhi_dev, enum mhi_callback cb_reason)\n{\n\tstruct mhi_driver *mhi_drv;\n\n\tif (!mhi_dev->dev.driver)\n\t\treturn;\n\n\tmhi_drv = to_mhi_driver(mhi_dev->dev.driver);\n\n\tif (mhi_drv->status_cb)\n\t\tmhi_drv->status_cb(mhi_dev, cb_reason);\n}\nEXPORT_SYMBOL_GPL(mhi_notify);\n\n \nvoid mhi_create_devices(struct mhi_controller *mhi_cntrl)\n{\n\tstruct mhi_chan *mhi_chan;\n\tstruct mhi_device *mhi_dev;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tint i, ret;\n\n\tmhi_chan = mhi_cntrl->mhi_chan;\n\tfor (i = 0; i < mhi_cntrl->max_chan; i++, mhi_chan++) {\n\t\tif (!mhi_chan->configured || mhi_chan->mhi_dev ||\n\t\t    !(mhi_chan->ee_mask & BIT(mhi_cntrl->ee)))\n\t\t\tcontinue;\n\t\tmhi_dev = mhi_alloc_device(mhi_cntrl);\n\t\tif (IS_ERR(mhi_dev))\n\t\t\treturn;\n\n\t\tmhi_dev->dev_type = MHI_DEVICE_XFER;\n\t\tswitch (mhi_chan->dir) {\n\t\tcase DMA_TO_DEVICE:\n\t\t\tmhi_dev->ul_chan = mhi_chan;\n\t\t\tmhi_dev->ul_chan_id = mhi_chan->chan;\n\t\t\tbreak;\n\t\tcase DMA_FROM_DEVICE:\n\t\t\t \n\t\t\tmhi_dev->dl_chan = mhi_chan;\n\t\t\tmhi_dev->dl_chan_id = mhi_chan->chan;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"Direction not supported\\n\");\n\t\t\tput_device(&mhi_dev->dev);\n\t\t\treturn;\n\t\t}\n\n\t\tget_device(&mhi_dev->dev);\n\t\tmhi_chan->mhi_dev = mhi_dev;\n\n\t\t \n\t\tif ((i + 1) < mhi_cntrl->max_chan && mhi_chan[1].configured) {\n\t\t\tif (!strcmp(mhi_chan[1].name, mhi_chan->name)) {\n\t\t\t\ti++;\n\t\t\t\tmhi_chan++;\n\t\t\t\tif (mhi_chan->dir == DMA_TO_DEVICE) {\n\t\t\t\t\tmhi_dev->ul_chan = mhi_chan;\n\t\t\t\t\tmhi_dev->ul_chan_id = mhi_chan->chan;\n\t\t\t\t} else {\n\t\t\t\t\tmhi_dev->dl_chan = mhi_chan;\n\t\t\t\t\tmhi_dev->dl_chan_id = mhi_chan->chan;\n\t\t\t\t}\n\t\t\t\tget_device(&mhi_dev->dev);\n\t\t\t\tmhi_chan->mhi_dev = mhi_dev;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tmhi_dev->name = mhi_chan->name;\n\t\tdev_set_name(&mhi_dev->dev, \"%s_%s\",\n\t\t\t     dev_name(&mhi_cntrl->mhi_dev->dev),\n\t\t\t     mhi_dev->name);\n\n\t\t \n\t\tif (mhi_dev->dl_chan && mhi_dev->dl_chan->wake_capable)\n\t\t\tdevice_init_wakeup(&mhi_dev->dev, true);\n\n\t\tret = device_add(&mhi_dev->dev);\n\t\tif (ret)\n\t\t\tput_device(&mhi_dev->dev);\n\t}\n}\n\nirqreturn_t mhi_irq_handler(int irq_number, void *dev)\n{\n\tstruct mhi_event *mhi_event = dev;\n\tstruct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;\n\tstruct mhi_event_ctxt *er_ctxt;\n\tstruct mhi_ring *ev_ring = &mhi_event->ring;\n\tdma_addr_t ptr;\n\tvoid *dev_rp;\n\n\t \n\tif (!mhi_cntrl->mhi_ctxt) {\n\t\tdev_dbg(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"mhi_ctxt has been freed\\n\");\n\t\treturn IRQ_HANDLED;\n\t}\n\n\ter_ctxt = &mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];\n\tptr = le64_to_cpu(er_ctxt->rp);\n\n\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\n\t \n\tif (ev_ring->rp == dev_rp)\n\t\treturn IRQ_HANDLED;\n\n\t \n\tif (mhi_event->cl_manage) {\n\t\tstruct mhi_chan *mhi_chan = mhi_event->mhi_chan;\n\t\tstruct mhi_device *mhi_dev = mhi_chan->mhi_dev;\n\n\t\tif (mhi_dev)\n\t\t\tmhi_notify(mhi_dev, MHI_CB_PENDING_DATA);\n\t} else {\n\t\ttasklet_schedule(&mhi_event->task);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t mhi_intvec_threaded_handler(int irq_number, void *priv)\n{\n\tstruct mhi_controller *mhi_cntrl = priv;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tenum mhi_state state;\n\tenum mhi_pm_state pm_state = 0;\n\tenum mhi_ee_type ee;\n\n\twrite_lock_irq(&mhi_cntrl->pm_lock);\n\tif (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {\n\t\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\t\tgoto exit_intvec;\n\t}\n\n\tstate = mhi_get_mhi_state(mhi_cntrl);\n\tee = mhi_get_exec_env(mhi_cntrl);\n\tdev_dbg(dev, \"local ee: %s state: %s device ee: %s state: %s\\n\",\n\t\tTO_MHI_EXEC_STR(mhi_cntrl->ee),\n\t\tmhi_state_str(mhi_cntrl->dev_state),\n\t\tTO_MHI_EXEC_STR(ee), mhi_state_str(state));\n\n\tif (state == MHI_STATE_SYS_ERR) {\n\t\tdev_dbg(dev, \"System error detected\\n\");\n\t\tpm_state = mhi_tryset_pm_state(mhi_cntrl,\n\t\t\t\t\t       MHI_PM_SYS_ERR_DETECT);\n\t}\n\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\n\tif (pm_state != MHI_PM_SYS_ERR_DETECT)\n\t\tgoto exit_intvec;\n\n\tswitch (ee) {\n\tcase MHI_EE_RDDM:\n\t\t \n\t\tif (mhi_cntrl->rddm_image && mhi_is_active(mhi_cntrl)) {\n\t\t\tmhi_cntrl->status_cb(mhi_cntrl, MHI_CB_EE_RDDM);\n\t\t\tmhi_cntrl->ee = ee;\n\t\t\twake_up_all(&mhi_cntrl->state_event);\n\t\t}\n\t\tbreak;\n\tcase MHI_EE_PBL:\n\tcase MHI_EE_EDL:\n\tcase MHI_EE_PTHRU:\n\t\tmhi_cntrl->status_cb(mhi_cntrl, MHI_CB_FATAL_ERROR);\n\t\tmhi_cntrl->ee = ee;\n\t\twake_up_all(&mhi_cntrl->state_event);\n\t\tmhi_pm_sys_err_handler(mhi_cntrl);\n\t\tbreak;\n\tdefault:\n\t\twake_up_all(&mhi_cntrl->state_event);\n\t\tmhi_pm_sys_err_handler(mhi_cntrl);\n\t\tbreak;\n\t}\n\nexit_intvec:\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t mhi_intvec_handler(int irq_number, void *dev)\n{\n\tstruct mhi_controller *mhi_cntrl = dev;\n\n\t \n\twake_up_all(&mhi_cntrl->state_event);\n\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic void mhi_recycle_ev_ring_element(struct mhi_controller *mhi_cntrl,\n\t\t\t\t\tstruct mhi_ring *ring)\n{\n\t \n\tring->wp += ring->el_size;\n\n\tif (ring->wp >= (ring->base + ring->len))\n\t\tring->wp = ring->base;\n\n\t*ring->ctxt_wp = cpu_to_le64(ring->iommu_base + (ring->wp - ring->base));\n\n\t \n\tring->rp += ring->el_size;\n\tif (ring->rp >= (ring->base + ring->len))\n\t\tring->rp = ring->base;\n\n\t \n\tsmp_wmb();\n}\n\nstatic int parse_xfer_event(struct mhi_controller *mhi_cntrl,\n\t\t\t    struct mhi_ring_element *event,\n\t\t\t    struct mhi_chan *mhi_chan)\n{\n\tstruct mhi_ring *buf_ring, *tre_ring;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tstruct mhi_result result;\n\tunsigned long flags = 0;\n\tu32 ev_code;\n\n\tev_code = MHI_TRE_GET_EV_CODE(event);\n\tbuf_ring = &mhi_chan->buf_ring;\n\ttre_ring = &mhi_chan->tre_ring;\n\n\tresult.transaction_status = (ev_code == MHI_EV_CC_OVERFLOW) ?\n\t\t-EOVERFLOW : 0;\n\n\t \n\tif (ev_code >= MHI_EV_CC_OOB)\n\t\twrite_lock_irqsave(&mhi_chan->lock, flags);\n\telse\n\t\tread_lock_bh(&mhi_chan->lock);\n\n\tif (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)\n\t\tgoto end_process_tx_event;\n\n\tswitch (ev_code) {\n\tcase MHI_EV_CC_OVERFLOW:\n\tcase MHI_EV_CC_EOB:\n\tcase MHI_EV_CC_EOT:\n\t{\n\t\tdma_addr_t ptr = MHI_TRE_GET_EV_PTR(event);\n\t\tstruct mhi_ring_element *local_rp, *ev_tre;\n\t\tvoid *dev_rp;\n\t\tstruct mhi_buf_info *buf_info;\n\t\tu16 xfer_len;\n\n\t\tif (!is_valid_ring_ptr(tre_ring, ptr)) {\n\t\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\t\"Event element points outside of the tre ring\\n\");\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tev_tre = mhi_to_virtual(tre_ring, ptr);\n\n\t\tdev_rp = ev_tre + 1;\n\t\tif (dev_rp >= (tre_ring->base + tre_ring->len))\n\t\t\tdev_rp = tre_ring->base;\n\n\t\tresult.dir = mhi_chan->dir;\n\n\t\tlocal_rp = tre_ring->rp;\n\t\twhile (local_rp != dev_rp) {\n\t\t\tbuf_info = buf_ring->rp;\n\t\t\t \n\t\t\tif (local_rp == ev_tre)\n\t\t\t\txfer_len = MHI_TRE_GET_EV_LEN(event);\n\t\t\telse\n\t\t\t\txfer_len = buf_info->len;\n\n\t\t\t \n\t\t\tif (likely(!buf_info->pre_mapped))\n\t\t\t\tmhi_cntrl->unmap_single(mhi_cntrl, buf_info);\n\n\t\t\tresult.buf_addr = buf_info->cb_buf;\n\n\t\t\t \n\t\t\tresult.bytes_xferd =\n\t\t\t\tmin_t(u16, xfer_len, buf_info->len);\n\t\t\tmhi_del_ring_element(mhi_cntrl, buf_ring);\n\t\t\tmhi_del_ring_element(mhi_cntrl, tre_ring);\n\t\t\tlocal_rp = tre_ring->rp;\n\n\t\t\t \n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\n\t\t\tif (mhi_chan->dir == DMA_TO_DEVICE) {\n\t\t\t\tatomic_dec(&mhi_cntrl->pending_pkts);\n\t\t\t\t \n\t\t\t\tmhi_cntrl->runtime_put(mhi_cntrl);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (mhi_chan->pre_alloc) {\n\t\t\t\tif (mhi_queue_buf(mhi_chan->mhi_dev,\n\t\t\t\t\t\t  mhi_chan->dir,\n\t\t\t\t\t\t  buf_info->cb_buf,\n\t\t\t\t\t\t  buf_info->len, MHI_EOT)) {\n\t\t\t\t\tdev_err(dev,\n\t\t\t\t\t\t\"Error recycling buffer for chan:%d\\n\",\n\t\t\t\t\t\tmhi_chan->chan);\n\t\t\t\t\tkfree(buf_info->cb_buf);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}  \n\tcase MHI_EV_CC_OOB:\n\tcase MHI_EV_CC_DB_MODE:\n\t{\n\t\tunsigned long pm_lock_flags;\n\n\t\tmhi_chan->db_cfg.db_mode = 1;\n\t\tread_lock_irqsave(&mhi_cntrl->pm_lock, pm_lock_flags);\n\t\tif (tre_ring->wp != tre_ring->rp &&\n\t\t    MHI_DB_ACCESS_VALID(mhi_cntrl)) {\n\t\t\tmhi_ring_chan_db(mhi_cntrl, mhi_chan);\n\t\t}\n\t\tread_unlock_irqrestore(&mhi_cntrl->pm_lock, pm_lock_flags);\n\t\tbreak;\n\t}\n\tcase MHI_EV_CC_BAD_TRE:\n\tdefault:\n\t\tdev_err(dev, \"Unknown event 0x%x\\n\", ev_code);\n\t\tbreak;\n\t}  \n\nend_process_tx_event:\n\tif (ev_code >= MHI_EV_CC_OOB)\n\t\twrite_unlock_irqrestore(&mhi_chan->lock, flags);\n\telse\n\t\tread_unlock_bh(&mhi_chan->lock);\n\n\treturn 0;\n}\n\nstatic int parse_rsc_event(struct mhi_controller *mhi_cntrl,\n\t\t\t   struct mhi_ring_element *event,\n\t\t\t   struct mhi_chan *mhi_chan)\n{\n\tstruct mhi_ring *buf_ring, *tre_ring;\n\tstruct mhi_buf_info *buf_info;\n\tstruct mhi_result result;\n\tint ev_code;\n\tu32 cookie;  \n\tu16 xfer_len;\n\n\tbuf_ring = &mhi_chan->buf_ring;\n\ttre_ring = &mhi_chan->tre_ring;\n\n\tev_code = MHI_TRE_GET_EV_CODE(event);\n\tcookie = MHI_TRE_GET_EV_COOKIE(event);\n\txfer_len = MHI_TRE_GET_EV_LEN(event);\n\n\t \n\tWARN_ON(cookie >= buf_ring->len);\n\n\tbuf_info = buf_ring->base + cookie;\n\n\tresult.transaction_status = (ev_code == MHI_EV_CC_OVERFLOW) ?\n\t\t-EOVERFLOW : 0;\n\n\t \n\tresult.bytes_xferd = min_t(u16, xfer_len, buf_info->len);\n\tresult.buf_addr = buf_info->cb_buf;\n\tresult.dir = mhi_chan->dir;\n\n\tread_lock_bh(&mhi_chan->lock);\n\n\tif (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)\n\t\tgoto end_process_rsc_event;\n\n\tWARN_ON(!buf_info->used);\n\n\t \n\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\n\t \n\tmhi_del_ring_element(mhi_cntrl, tre_ring);\n\tbuf_info->used = false;\n\nend_process_rsc_event:\n\tread_unlock_bh(&mhi_chan->lock);\n\n\treturn 0;\n}\n\nstatic void mhi_process_cmd_completion(struct mhi_controller *mhi_cntrl,\n\t\t\t\t       struct mhi_ring_element *tre)\n{\n\tdma_addr_t ptr = MHI_TRE_GET_EV_PTR(tre);\n\tstruct mhi_cmd *cmd_ring = &mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING];\n\tstruct mhi_ring *mhi_ring = &cmd_ring->ring;\n\tstruct mhi_ring_element *cmd_pkt;\n\tstruct mhi_chan *mhi_chan;\n\tu32 chan;\n\n\tif (!is_valid_ring_ptr(mhi_ring, ptr)) {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Event element points outside of the cmd ring\\n\");\n\t\treturn;\n\t}\n\n\tcmd_pkt = mhi_to_virtual(mhi_ring, ptr);\n\n\tchan = MHI_TRE_GET_CMD_CHID(cmd_pkt);\n\n\tif (chan < mhi_cntrl->max_chan &&\n\t    mhi_cntrl->mhi_chan[chan].configured) {\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[chan];\n\t\twrite_lock_bh(&mhi_chan->lock);\n\t\tmhi_chan->ccs = MHI_TRE_GET_EV_CODE(tre);\n\t\tcomplete(&mhi_chan->completion);\n\t\twrite_unlock_bh(&mhi_chan->lock);\n\t} else {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Completion packet for invalid channel ID: %d\\n\", chan);\n\t}\n\n\tmhi_del_ring_element(mhi_cntrl, mhi_ring);\n}\n\nint mhi_process_ctrl_ev_ring(struct mhi_controller *mhi_cntrl,\n\t\t\t     struct mhi_event *mhi_event,\n\t\t\t     u32 event_quota)\n{\n\tstruct mhi_ring_element *dev_rp, *local_rp;\n\tstruct mhi_ring *ev_ring = &mhi_event->ring;\n\tstruct mhi_event_ctxt *er_ctxt =\n\t\t&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];\n\tstruct mhi_chan *mhi_chan;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tu32 chan;\n\tint count = 0;\n\tdma_addr_t ptr = le64_to_cpu(er_ctxt->rp);\n\n\t \n\tif (unlikely(MHI_EVENT_ACCESS_INVALID(mhi_cntrl->pm_state)))\n\t\treturn -EIO;\n\n\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\treturn -EIO;\n\t}\n\n\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\tlocal_rp = ev_ring->rp;\n\n\twhile (dev_rp != local_rp) {\n\t\tenum mhi_pkt_type type = MHI_TRE_GET_EV_TYPE(local_rp);\n\n\t\tswitch (type) {\n\t\tcase MHI_PKT_TYPE_BW_REQ_EVENT:\n\t\t{\n\t\t\tstruct mhi_link_info *link_info;\n\n\t\t\tlink_info = &mhi_cntrl->mhi_link_info;\n\t\t\twrite_lock_irq(&mhi_cntrl->pm_lock);\n\t\t\tlink_info->target_link_speed =\n\t\t\t\tMHI_TRE_GET_EV_LINKSPEED(local_rp);\n\t\t\tlink_info->target_link_width =\n\t\t\t\tMHI_TRE_GET_EV_LINKWIDTH(local_rp);\n\t\t\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\t\t\tdev_dbg(dev, \"Received BW_REQ event\\n\");\n\t\t\tmhi_cntrl->status_cb(mhi_cntrl, MHI_CB_BW_REQ);\n\t\t\tbreak;\n\t\t}\n\t\tcase MHI_PKT_TYPE_STATE_CHANGE_EVENT:\n\t\t{\n\t\t\tenum mhi_state new_state;\n\n\t\t\tnew_state = MHI_TRE_GET_EV_STATE(local_rp);\n\n\t\t\tdev_dbg(dev, \"State change event to state: %s\\n\",\n\t\t\t\tmhi_state_str(new_state));\n\n\t\t\tswitch (new_state) {\n\t\t\tcase MHI_STATE_M0:\n\t\t\t\tmhi_pm_m0_transition(mhi_cntrl);\n\t\t\t\tbreak;\n\t\t\tcase MHI_STATE_M1:\n\t\t\t\tmhi_pm_m1_transition(mhi_cntrl);\n\t\t\t\tbreak;\n\t\t\tcase MHI_STATE_M3:\n\t\t\t\tmhi_pm_m3_transition(mhi_cntrl);\n\t\t\t\tbreak;\n\t\t\tcase MHI_STATE_SYS_ERR:\n\t\t\t{\n\t\t\t\tenum mhi_pm_state pm_state;\n\n\t\t\t\tdev_dbg(dev, \"System error detected\\n\");\n\t\t\t\twrite_lock_irq(&mhi_cntrl->pm_lock);\n\t\t\t\tpm_state = mhi_tryset_pm_state(mhi_cntrl,\n\t\t\t\t\t\t\tMHI_PM_SYS_ERR_DETECT);\n\t\t\t\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\t\t\t\tif (pm_state == MHI_PM_SYS_ERR_DETECT)\n\t\t\t\t\tmhi_pm_sys_err_handler(mhi_cntrl);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:\n\t\t\t\tdev_err(dev, \"Invalid state: %s\\n\",\n\t\t\t\t\tmhi_state_str(new_state));\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t\tcase MHI_PKT_TYPE_CMD_COMPLETION_EVENT:\n\t\t\tmhi_process_cmd_completion(mhi_cntrl, local_rp);\n\t\t\tbreak;\n\t\tcase MHI_PKT_TYPE_EE_EVENT:\n\t\t{\n\t\t\tenum dev_st_transition st = DEV_ST_TRANSITION_MAX;\n\t\t\tenum mhi_ee_type event = MHI_TRE_GET_EV_EXECENV(local_rp);\n\n\t\t\tdev_dbg(dev, \"Received EE event: %s\\n\",\n\t\t\t\tTO_MHI_EXEC_STR(event));\n\t\t\tswitch (event) {\n\t\t\tcase MHI_EE_SBL:\n\t\t\t\tst = DEV_ST_TRANSITION_SBL;\n\t\t\t\tbreak;\n\t\t\tcase MHI_EE_WFW:\n\t\t\tcase MHI_EE_AMSS:\n\t\t\t\tst = DEV_ST_TRANSITION_MISSION_MODE;\n\t\t\t\tbreak;\n\t\t\tcase MHI_EE_FP:\n\t\t\t\tst = DEV_ST_TRANSITION_FP;\n\t\t\t\tbreak;\n\t\t\tcase MHI_EE_RDDM:\n\t\t\t\tmhi_cntrl->status_cb(mhi_cntrl, MHI_CB_EE_RDDM);\n\t\t\t\twrite_lock_irq(&mhi_cntrl->pm_lock);\n\t\t\t\tmhi_cntrl->ee = event;\n\t\t\t\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\t\t\t\twake_up_all(&mhi_cntrl->state_event);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"Unhandled EE event: 0x%x\\n\", type);\n\t\t\t}\n\t\t\tif (st != DEV_ST_TRANSITION_MAX)\n\t\t\t\tmhi_queue_state_transition(mhi_cntrl, st);\n\n\t\t\tbreak;\n\t\t}\n\t\tcase MHI_PKT_TYPE_TX_EVENT:\n\t\t\tchan = MHI_TRE_GET_EV_CHID(local_rp);\n\n\t\t\tWARN_ON(chan >= mhi_cntrl->max_chan);\n\n\t\t\t \n\t\t\tif (chan < mhi_cntrl->max_chan) {\n\t\t\t\tmhi_chan = &mhi_cntrl->mhi_chan[chan];\n\t\t\t\tif (!mhi_chan->configured)\n\t\t\t\t\tbreak;\n\t\t\t\tparse_xfer_event(mhi_cntrl, local_rp, mhi_chan);\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"Unhandled event type: %d\\n\", type);\n\t\t\tbreak;\n\t\t}\n\n\t\tmhi_recycle_ev_ring_element(mhi_cntrl, ev_ring);\n\t\tlocal_rp = ev_ring->rp;\n\n\t\tptr = le64_to_cpu(er_ctxt->rp);\n\t\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\t\tcount++;\n\t}\n\n\tread_lock_bh(&mhi_cntrl->pm_lock);\n\n\t \n\tif (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)) && count)\n\t\tmhi_ring_er_db(mhi_event);\n\tread_unlock_bh(&mhi_cntrl->pm_lock);\n\n\treturn count;\n}\n\nint mhi_process_data_event_ring(struct mhi_controller *mhi_cntrl,\n\t\t\t\tstruct mhi_event *mhi_event,\n\t\t\t\tu32 event_quota)\n{\n\tstruct mhi_ring_element *dev_rp, *local_rp;\n\tstruct mhi_ring *ev_ring = &mhi_event->ring;\n\tstruct mhi_event_ctxt *er_ctxt =\n\t\t&mhi_cntrl->mhi_ctxt->er_ctxt[mhi_event->er_index];\n\tint count = 0;\n\tu32 chan;\n\tstruct mhi_chan *mhi_chan;\n\tdma_addr_t ptr = le64_to_cpu(er_ctxt->rp);\n\n\tif (unlikely(MHI_EVENT_ACCESS_INVALID(mhi_cntrl->pm_state)))\n\t\treturn -EIO;\n\n\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\treturn -EIO;\n\t}\n\n\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\tlocal_rp = ev_ring->rp;\n\n\twhile (dev_rp != local_rp && event_quota > 0) {\n\t\tenum mhi_pkt_type type = MHI_TRE_GET_EV_TYPE(local_rp);\n\n\t\tchan = MHI_TRE_GET_EV_CHID(local_rp);\n\n\t\tWARN_ON(chan >= mhi_cntrl->max_chan);\n\n\t\t \n\t\tif (chan < mhi_cntrl->max_chan &&\n\t\t    mhi_cntrl->mhi_chan[chan].configured) {\n\t\t\tmhi_chan = &mhi_cntrl->mhi_chan[chan];\n\n\t\t\tif (likely(type == MHI_PKT_TYPE_TX_EVENT)) {\n\t\t\t\tparse_xfer_event(mhi_cntrl, local_rp, mhi_chan);\n\t\t\t\tevent_quota--;\n\t\t\t} else if (type == MHI_PKT_TYPE_RSC_TX_EVENT) {\n\t\t\t\tparse_rsc_event(mhi_cntrl, local_rp, mhi_chan);\n\t\t\t\tevent_quota--;\n\t\t\t}\n\t\t}\n\n\t\tmhi_recycle_ev_ring_element(mhi_cntrl, ev_ring);\n\t\tlocal_rp = ev_ring->rp;\n\n\t\tptr = le64_to_cpu(er_ctxt->rp);\n\t\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\t\tcount++;\n\t}\n\tread_lock_bh(&mhi_cntrl->pm_lock);\n\n\t \n\tif (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)) && count)\n\t\tmhi_ring_er_db(mhi_event);\n\tread_unlock_bh(&mhi_cntrl->pm_lock);\n\n\treturn count;\n}\n\nvoid mhi_ev_task(unsigned long data)\n{\n\tstruct mhi_event *mhi_event = (struct mhi_event *)data;\n\tstruct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;\n\n\t \n\tspin_lock_bh(&mhi_event->lock);\n\tmhi_event->process_event(mhi_cntrl, mhi_event, U32_MAX);\n\tspin_unlock_bh(&mhi_event->lock);\n}\n\nvoid mhi_ctrl_ev_task(unsigned long data)\n{\n\tstruct mhi_event *mhi_event = (struct mhi_event *)data;\n\tstruct mhi_controller *mhi_cntrl = mhi_event->mhi_cntrl;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tenum mhi_state state;\n\tenum mhi_pm_state pm_state = 0;\n\tint ret;\n\n\t \n\tif (!MHI_REG_ACCESS_VALID(mhi_cntrl->pm_state)) {\n\t\t \n\t\tmhi_trigger_resume(mhi_cntrl);\n\n\t\treturn;\n\t}\n\n\t \n\tret = mhi_event->process_event(mhi_cntrl, mhi_event, U32_MAX);\n\n\t \n\tif (!ret) {\n\t\twrite_lock_irq(&mhi_cntrl->pm_lock);\n\t\tstate = mhi_get_mhi_state(mhi_cntrl);\n\t\tif (state == MHI_STATE_SYS_ERR) {\n\t\t\tdev_dbg(dev, \"System error detected\\n\");\n\t\t\tpm_state = mhi_tryset_pm_state(mhi_cntrl,\n\t\t\t\t\t\t       MHI_PM_SYS_ERR_DETECT);\n\t\t}\n\t\twrite_unlock_irq(&mhi_cntrl->pm_lock);\n\t\tif (pm_state == MHI_PM_SYS_ERR_DETECT)\n\t\t\tmhi_pm_sys_err_handler(mhi_cntrl);\n\t}\n}\n\nstatic bool mhi_is_ring_full(struct mhi_controller *mhi_cntrl,\n\t\t\t     struct mhi_ring *ring)\n{\n\tvoid *tmp = ring->wp + ring->el_size;\n\n\tif (tmp >= (ring->base + ring->len))\n\t\ttmp = ring->base;\n\n\treturn (tmp == ring->rp);\n}\n\nstatic int mhi_queue(struct mhi_device *mhi_dev, struct mhi_buf_info *buf_info,\n\t\t     enum dma_data_direction dir, enum mhi_flags mflags)\n{\n\tstruct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :\n\t\t\t\t\t\t\t     mhi_dev->dl_chan;\n\tstruct mhi_ring *tre_ring = &mhi_chan->tre_ring;\n\tunsigned long flags;\n\tint ret;\n\n\tif (unlikely(MHI_PM_IN_ERROR_STATE(mhi_cntrl->pm_state)))\n\t\treturn -EIO;\n\n\tread_lock_irqsave(&mhi_cntrl->pm_lock, flags);\n\n\tret = mhi_is_ring_full(mhi_cntrl, tre_ring);\n\tif (unlikely(ret)) {\n\t\tret = -EAGAIN;\n\t\tgoto exit_unlock;\n\t}\n\n\tret = mhi_gen_tre(mhi_cntrl, mhi_chan, buf_info, mflags);\n\tif (unlikely(ret))\n\t\tgoto exit_unlock;\n\n\t \n\tmhi_cntrl->runtime_get(mhi_cntrl);\n\n\t \n\tmhi_cntrl->wake_toggle(mhi_cntrl);\n\n\tif (mhi_chan->dir == DMA_TO_DEVICE)\n\t\tatomic_inc(&mhi_cntrl->pending_pkts);\n\n\tif (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))\n\t\tmhi_ring_chan_db(mhi_cntrl, mhi_chan);\n\n\tif (dir == DMA_FROM_DEVICE)\n\t\tmhi_cntrl->runtime_put(mhi_cntrl);\n\nexit_unlock:\n\tread_unlock_irqrestore(&mhi_cntrl->pm_lock, flags);\n\n\treturn ret;\n}\n\nint mhi_queue_skb(struct mhi_device *mhi_dev, enum dma_data_direction dir,\n\t\t  struct sk_buff *skb, size_t len, enum mhi_flags mflags)\n{\n\tstruct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :\n\t\t\t\t\t\t\t     mhi_dev->dl_chan;\n\tstruct mhi_buf_info buf_info = { };\n\n\tbuf_info.v_addr = skb->data;\n\tbuf_info.cb_buf = skb;\n\tbuf_info.len = len;\n\n\tif (unlikely(mhi_chan->pre_alloc))\n\t\treturn -EINVAL;\n\n\treturn mhi_queue(mhi_dev, &buf_info, dir, mflags);\n}\nEXPORT_SYMBOL_GPL(mhi_queue_skb);\n\nint mhi_queue_dma(struct mhi_device *mhi_dev, enum dma_data_direction dir,\n\t\t  struct mhi_buf *mhi_buf, size_t len, enum mhi_flags mflags)\n{\n\tstruct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ? mhi_dev->ul_chan :\n\t\t\t\t\t\t\t     mhi_dev->dl_chan;\n\tstruct mhi_buf_info buf_info = { };\n\n\tbuf_info.p_addr = mhi_buf->dma_addr;\n\tbuf_info.cb_buf = mhi_buf;\n\tbuf_info.pre_mapped = true;\n\tbuf_info.len = len;\n\n\tif (unlikely(mhi_chan->pre_alloc))\n\t\treturn -EINVAL;\n\n\treturn mhi_queue(mhi_dev, &buf_info, dir, mflags);\n}\nEXPORT_SYMBOL_GPL(mhi_queue_dma);\n\nint mhi_gen_tre(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan,\n\t\t\tstruct mhi_buf_info *info, enum mhi_flags flags)\n{\n\tstruct mhi_ring *buf_ring, *tre_ring;\n\tstruct mhi_ring_element *mhi_tre;\n\tstruct mhi_buf_info *buf_info;\n\tint eot, eob, chain, bei;\n\tint ret;\n\n\tbuf_ring = &mhi_chan->buf_ring;\n\ttre_ring = &mhi_chan->tre_ring;\n\n\tbuf_info = buf_ring->wp;\n\tWARN_ON(buf_info->used);\n\tbuf_info->pre_mapped = info->pre_mapped;\n\tif (info->pre_mapped)\n\t\tbuf_info->p_addr = info->p_addr;\n\telse\n\t\tbuf_info->v_addr = info->v_addr;\n\tbuf_info->cb_buf = info->cb_buf;\n\tbuf_info->wp = tre_ring->wp;\n\tbuf_info->dir = mhi_chan->dir;\n\tbuf_info->len = info->len;\n\n\tif (!info->pre_mapped) {\n\t\tret = mhi_cntrl->map_single(mhi_cntrl, buf_info);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\teob = !!(flags & MHI_EOB);\n\teot = !!(flags & MHI_EOT);\n\tchain = !!(flags & MHI_CHAIN);\n\tbei = !!(mhi_chan->intmod);\n\n\tmhi_tre = tre_ring->wp;\n\tmhi_tre->ptr = MHI_TRE_DATA_PTR(buf_info->p_addr);\n\tmhi_tre->dword[0] = MHI_TRE_DATA_DWORD0(info->len);\n\tmhi_tre->dword[1] = MHI_TRE_DATA_DWORD1(bei, eot, eob, chain);\n\n\t \n\tmhi_add_ring_element(mhi_cntrl, tre_ring);\n\tmhi_add_ring_element(mhi_cntrl, buf_ring);\n\n\treturn 0;\n}\n\nint mhi_queue_buf(struct mhi_device *mhi_dev, enum dma_data_direction dir,\n\t\t  void *buf, size_t len, enum mhi_flags mflags)\n{\n\tstruct mhi_buf_info buf_info = { };\n\n\tbuf_info.v_addr = buf;\n\tbuf_info.cb_buf = buf;\n\tbuf_info.len = len;\n\n\treturn mhi_queue(mhi_dev, &buf_info, dir, mflags);\n}\nEXPORT_SYMBOL_GPL(mhi_queue_buf);\n\nbool mhi_queue_is_full(struct mhi_device *mhi_dev, enum dma_data_direction dir)\n{\n\tstruct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_chan *mhi_chan = (dir == DMA_TO_DEVICE) ?\n\t\t\t\t\tmhi_dev->ul_chan : mhi_dev->dl_chan;\n\tstruct mhi_ring *tre_ring = &mhi_chan->tre_ring;\n\n\treturn mhi_is_ring_full(mhi_cntrl, tre_ring);\n}\nEXPORT_SYMBOL_GPL(mhi_queue_is_full);\n\nint mhi_send_cmd(struct mhi_controller *mhi_cntrl,\n\t\t struct mhi_chan *mhi_chan,\n\t\t enum mhi_cmd_type cmd)\n{\n\tstruct mhi_ring_element *cmd_tre = NULL;\n\tstruct mhi_cmd *mhi_cmd = &mhi_cntrl->mhi_cmd[PRIMARY_CMD_RING];\n\tstruct mhi_ring *ring = &mhi_cmd->ring;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tint chan = 0;\n\n\tif (mhi_chan)\n\t\tchan = mhi_chan->chan;\n\n\tspin_lock_bh(&mhi_cmd->lock);\n\tif (!get_nr_avail_ring_elements(mhi_cntrl, ring)) {\n\t\tspin_unlock_bh(&mhi_cmd->lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcmd_tre = ring->wp;\n\tswitch (cmd) {\n\tcase MHI_CMD_RESET_CHAN:\n\t\tcmd_tre->ptr = MHI_TRE_CMD_RESET_PTR;\n\t\tcmd_tre->dword[0] = MHI_TRE_CMD_RESET_DWORD0;\n\t\tcmd_tre->dword[1] = MHI_TRE_CMD_RESET_DWORD1(chan);\n\t\tbreak;\n\tcase MHI_CMD_STOP_CHAN:\n\t\tcmd_tre->ptr = MHI_TRE_CMD_STOP_PTR;\n\t\tcmd_tre->dword[0] = MHI_TRE_CMD_STOP_DWORD0;\n\t\tcmd_tre->dword[1] = MHI_TRE_CMD_STOP_DWORD1(chan);\n\t\tbreak;\n\tcase MHI_CMD_START_CHAN:\n\t\tcmd_tre->ptr = MHI_TRE_CMD_START_PTR;\n\t\tcmd_tre->dword[0] = MHI_TRE_CMD_START_DWORD0;\n\t\tcmd_tre->dword[1] = MHI_TRE_CMD_START_DWORD1(chan);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Command not supported\\n\");\n\t\tbreak;\n\t}\n\n\t \n\tmhi_add_ring_element(mhi_cntrl, ring);\n\tread_lock_bh(&mhi_cntrl->pm_lock);\n\tif (likely(MHI_DB_ACCESS_VALID(mhi_cntrl)))\n\t\tmhi_ring_cmd_db(mhi_cntrl, mhi_cmd);\n\tread_unlock_bh(&mhi_cntrl->pm_lock);\n\tspin_unlock_bh(&mhi_cmd->lock);\n\n\treturn 0;\n}\n\nstatic int mhi_update_channel_state(struct mhi_controller *mhi_cntrl,\n\t\t\t\t    struct mhi_chan *mhi_chan,\n\t\t\t\t    enum mhi_ch_state_type to_state)\n{\n\tstruct device *dev = &mhi_chan->mhi_dev->dev;\n\tenum mhi_cmd_type cmd = MHI_CMD_NOP;\n\tint ret;\n\n\tdev_dbg(dev, \"%d: Updating channel state to: %s\\n\", mhi_chan->chan,\n\t\tTO_CH_STATE_TYPE_STR(to_state));\n\n\tswitch (to_state) {\n\tcase MHI_CH_STATE_TYPE_RESET:\n\t\twrite_lock_irq(&mhi_chan->lock);\n\t\tif (mhi_chan->ch_state != MHI_CH_STATE_STOP &&\n\t\t    mhi_chan->ch_state != MHI_CH_STATE_ENABLED &&\n\t\t    mhi_chan->ch_state != MHI_CH_STATE_SUSPENDED) {\n\t\t\twrite_unlock_irq(&mhi_chan->lock);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmhi_chan->ch_state = MHI_CH_STATE_DISABLED;\n\t\twrite_unlock_irq(&mhi_chan->lock);\n\n\t\tcmd = MHI_CMD_RESET_CHAN;\n\t\tbreak;\n\tcase MHI_CH_STATE_TYPE_STOP:\n\t\tif (mhi_chan->ch_state != MHI_CH_STATE_ENABLED)\n\t\t\treturn -EINVAL;\n\n\t\tcmd = MHI_CMD_STOP_CHAN;\n\t\tbreak;\n\tcase MHI_CH_STATE_TYPE_START:\n\t\tif (mhi_chan->ch_state != MHI_CH_STATE_STOP &&\n\t\t    mhi_chan->ch_state != MHI_CH_STATE_DISABLED)\n\t\t\treturn -EINVAL;\n\n\t\tcmd = MHI_CMD_START_CHAN;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"%d: Channel state update to %s not allowed\\n\",\n\t\t\tmhi_chan->chan, TO_CH_STATE_TYPE_STR(to_state));\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = mhi_device_get_sync(mhi_cntrl->mhi_dev);\n\tif (ret)\n\t\treturn ret;\n\tmhi_cntrl->runtime_get(mhi_cntrl);\n\n\treinit_completion(&mhi_chan->completion);\n\tret = mhi_send_cmd(mhi_cntrl, mhi_chan, cmd);\n\tif (ret) {\n\t\tdev_err(dev, \"%d: Failed to send %s channel command\\n\",\n\t\t\tmhi_chan->chan, TO_CH_STATE_TYPE_STR(to_state));\n\t\tgoto exit_channel_update;\n\t}\n\n\tret = wait_for_completion_timeout(&mhi_chan->completion,\n\t\t\t\t       msecs_to_jiffies(mhi_cntrl->timeout_ms));\n\tif (!ret || mhi_chan->ccs != MHI_EV_CC_SUCCESS) {\n\t\tdev_err(dev,\n\t\t\t\"%d: Failed to receive %s channel command completion\\n\",\n\t\t\tmhi_chan->chan, TO_CH_STATE_TYPE_STR(to_state));\n\t\tret = -EIO;\n\t\tgoto exit_channel_update;\n\t}\n\n\tret = 0;\n\n\tif (to_state != MHI_CH_STATE_TYPE_RESET) {\n\t\twrite_lock_irq(&mhi_chan->lock);\n\t\tmhi_chan->ch_state = (to_state == MHI_CH_STATE_TYPE_START) ?\n\t\t\t\t      MHI_CH_STATE_ENABLED : MHI_CH_STATE_STOP;\n\t\twrite_unlock_irq(&mhi_chan->lock);\n\t}\n\n\tdev_dbg(dev, \"%d: Channel state change to %s successful\\n\",\n\t\tmhi_chan->chan, TO_CH_STATE_TYPE_STR(to_state));\n\nexit_channel_update:\n\tmhi_cntrl->runtime_put(mhi_cntrl);\n\tmhi_device_put(mhi_cntrl->mhi_dev);\n\n\treturn ret;\n}\n\nstatic void mhi_unprepare_channel(struct mhi_controller *mhi_cntrl,\n\t\t\t\t  struct mhi_chan *mhi_chan)\n{\n\tint ret;\n\tstruct device *dev = &mhi_chan->mhi_dev->dev;\n\n\tmutex_lock(&mhi_chan->mutex);\n\n\tif (!(BIT(mhi_cntrl->ee) & mhi_chan->ee_mask)) {\n\t\tdev_dbg(dev, \"Current EE: %s Required EE Mask: 0x%x\\n\",\n\t\t\tTO_MHI_EXEC_STR(mhi_cntrl->ee), mhi_chan->ee_mask);\n\t\tgoto exit_unprepare_channel;\n\t}\n\n\t \n\tret = mhi_update_channel_state(mhi_cntrl, mhi_chan,\n\t\t\t\t       MHI_CH_STATE_TYPE_RESET);\n\tif (ret)\n\t\tdev_err(dev, \"%d: Failed to reset channel, still resetting\\n\",\n\t\t\tmhi_chan->chan);\n\nexit_unprepare_channel:\n\twrite_lock_irq(&mhi_chan->lock);\n\tmhi_chan->ch_state = MHI_CH_STATE_DISABLED;\n\twrite_unlock_irq(&mhi_chan->lock);\n\n\tif (!mhi_chan->offload_ch) {\n\t\tmhi_reset_chan(mhi_cntrl, mhi_chan);\n\t\tmhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);\n\t}\n\tdev_dbg(dev, \"%d: successfully reset\\n\", mhi_chan->chan);\n\n\tmutex_unlock(&mhi_chan->mutex);\n}\n\nint mhi_prepare_channel(struct mhi_controller *mhi_cntrl,\n\t\t\tstruct mhi_chan *mhi_chan, unsigned int flags)\n{\n\tint ret = 0;\n\tstruct device *dev = &mhi_chan->mhi_dev->dev;\n\n\tif (!(BIT(mhi_cntrl->ee) & mhi_chan->ee_mask)) {\n\t\tdev_err(dev, \"Current EE: %s Required EE Mask: 0x%x\\n\",\n\t\t\tTO_MHI_EXEC_STR(mhi_cntrl->ee), mhi_chan->ee_mask);\n\t\treturn -ENOTCONN;\n\t}\n\n\tmutex_lock(&mhi_chan->mutex);\n\n\t \n\tif (!mhi_chan->offload_ch) {\n\t\tret = mhi_init_chan_ctxt(mhi_cntrl, mhi_chan);\n\t\tif (ret)\n\t\t\tgoto error_init_chan;\n\t}\n\n\tret = mhi_update_channel_state(mhi_cntrl, mhi_chan,\n\t\t\t\t       MHI_CH_STATE_TYPE_START);\n\tif (ret)\n\t\tgoto error_pm_state;\n\n\tif (mhi_chan->dir == DMA_FROM_DEVICE)\n\t\tmhi_chan->pre_alloc = !!(flags & MHI_CH_INBOUND_ALLOC_BUFS);\n\n\t \n\tif (mhi_chan->pre_alloc) {\n\t\tint nr_el = get_nr_avail_ring_elements(mhi_cntrl,\n\t\t\t\t\t\t       &mhi_chan->tre_ring);\n\t\tsize_t len = mhi_cntrl->buffer_len;\n\n\t\twhile (nr_el--) {\n\t\t\tvoid *buf;\n\t\t\tstruct mhi_buf_info info = { };\n\n\t\t\tbuf = kmalloc(len, GFP_KERNEL);\n\t\t\tif (!buf) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error_pre_alloc;\n\t\t\t}\n\n\t\t\t \n\t\t\tinfo.v_addr = buf;\n\t\t\tinfo.cb_buf = buf;\n\t\t\tinfo.len = len;\n\t\t\tret = mhi_gen_tre(mhi_cntrl, mhi_chan, &info, MHI_EOT);\n\t\t\tif (ret) {\n\t\t\t\tkfree(buf);\n\t\t\t\tgoto error_pre_alloc;\n\t\t\t}\n\t\t}\n\n\t\tread_lock_bh(&mhi_cntrl->pm_lock);\n\t\tif (MHI_DB_ACCESS_VALID(mhi_cntrl)) {\n\t\t\tread_lock_irq(&mhi_chan->lock);\n\t\t\tmhi_ring_chan_db(mhi_cntrl, mhi_chan);\n\t\t\tread_unlock_irq(&mhi_chan->lock);\n\t\t}\n\t\tread_unlock_bh(&mhi_cntrl->pm_lock);\n\t}\n\n\tmutex_unlock(&mhi_chan->mutex);\n\n\treturn 0;\n\nerror_pm_state:\n\tif (!mhi_chan->offload_ch)\n\t\tmhi_deinit_chan_ctxt(mhi_cntrl, mhi_chan);\n\nerror_init_chan:\n\tmutex_unlock(&mhi_chan->mutex);\n\n\treturn ret;\n\nerror_pre_alloc:\n\tmutex_unlock(&mhi_chan->mutex);\n\tmhi_unprepare_channel(mhi_cntrl, mhi_chan);\n\n\treturn ret;\n}\n\nstatic void mhi_mark_stale_events(struct mhi_controller *mhi_cntrl,\n\t\t\t\t  struct mhi_event *mhi_event,\n\t\t\t\t  struct mhi_event_ctxt *er_ctxt,\n\t\t\t\t  int chan)\n\n{\n\tstruct mhi_ring_element *dev_rp, *local_rp;\n\tstruct mhi_ring *ev_ring;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tunsigned long flags;\n\tdma_addr_t ptr;\n\n\tdev_dbg(dev, \"Marking all events for chan: %d as stale\\n\", chan);\n\n\tev_ring = &mhi_event->ring;\n\n\t \n\tspin_lock_irqsave(&mhi_event->lock, flags);\n\n\tptr = le64_to_cpu(er_ctxt->rp);\n\tif (!is_valid_ring_ptr(ev_ring, ptr)) {\n\t\tdev_err(&mhi_cntrl->mhi_dev->dev,\n\t\t\t\"Event ring rp points outside of the event ring\\n\");\n\t\tdev_rp = ev_ring->rp;\n\t} else {\n\t\tdev_rp = mhi_to_virtual(ev_ring, ptr);\n\t}\n\n\tlocal_rp = ev_ring->rp;\n\twhile (dev_rp != local_rp) {\n\t\tif (MHI_TRE_GET_EV_TYPE(local_rp) == MHI_PKT_TYPE_TX_EVENT &&\n\t\t    chan == MHI_TRE_GET_EV_CHID(local_rp))\n\t\t\tlocal_rp->dword[1] = MHI_TRE_EV_DWORD1(chan,\n\t\t\t\t\tMHI_PKT_TYPE_STALE_EVENT);\n\t\tlocal_rp++;\n\t\tif (local_rp == (ev_ring->base + ev_ring->len))\n\t\t\tlocal_rp = ev_ring->base;\n\t}\n\n\tdev_dbg(dev, \"Finished marking events as stale events\\n\");\n\tspin_unlock_irqrestore(&mhi_event->lock, flags);\n}\n\nstatic void mhi_reset_data_chan(struct mhi_controller *mhi_cntrl,\n\t\t\t\tstruct mhi_chan *mhi_chan)\n{\n\tstruct mhi_ring *buf_ring, *tre_ring;\n\tstruct mhi_result result;\n\n\t \n\tbuf_ring = &mhi_chan->buf_ring;\n\ttre_ring = &mhi_chan->tre_ring;\n\tresult.transaction_status = -ENOTCONN;\n\tresult.bytes_xferd = 0;\n\twhile (tre_ring->rp != tre_ring->wp) {\n\t\tstruct mhi_buf_info *buf_info = buf_ring->rp;\n\n\t\tif (mhi_chan->dir == DMA_TO_DEVICE) {\n\t\t\tatomic_dec(&mhi_cntrl->pending_pkts);\n\t\t\t \n\t\t\tmhi_cntrl->runtime_put(mhi_cntrl);\n\t\t}\n\n\t\tif (!buf_info->pre_mapped)\n\t\t\tmhi_cntrl->unmap_single(mhi_cntrl, buf_info);\n\n\t\tmhi_del_ring_element(mhi_cntrl, buf_ring);\n\t\tmhi_del_ring_element(mhi_cntrl, tre_ring);\n\n\t\tif (mhi_chan->pre_alloc) {\n\t\t\tkfree(buf_info->cb_buf);\n\t\t} else {\n\t\t\tresult.buf_addr = buf_info->cb_buf;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t}\n\t}\n}\n\nvoid mhi_reset_chan(struct mhi_controller *mhi_cntrl, struct mhi_chan *mhi_chan)\n{\n\tstruct mhi_event *mhi_event;\n\tstruct mhi_event_ctxt *er_ctxt;\n\tint chan = mhi_chan->chan;\n\n\t \n\tif (mhi_chan->offload_ch)\n\t\treturn;\n\n\tread_lock_bh(&mhi_cntrl->pm_lock);\n\tmhi_event = &mhi_cntrl->mhi_event[mhi_chan->er_index];\n\ter_ctxt = &mhi_cntrl->mhi_ctxt->er_ctxt[mhi_chan->er_index];\n\n\tmhi_mark_stale_events(mhi_cntrl, mhi_event, er_ctxt, chan);\n\n\tmhi_reset_data_chan(mhi_cntrl, mhi_chan);\n\n\tread_unlock_bh(&mhi_cntrl->pm_lock);\n}\n\nstatic int __mhi_prepare_for_transfer(struct mhi_device *mhi_dev, unsigned int flags)\n{\n\tint ret, dir;\n\tstruct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_chan *mhi_chan;\n\n\tfor (dir = 0; dir < 2; dir++) {\n\t\tmhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;\n\t\tif (!mhi_chan)\n\t\t\tcontinue;\n\n\t\tret = mhi_prepare_channel(mhi_cntrl, mhi_chan, flags);\n\t\tif (ret)\n\t\t\tgoto error_open_chan;\n\t}\n\n\treturn 0;\n\nerror_open_chan:\n\tfor (--dir; dir >= 0; dir--) {\n\t\tmhi_chan = dir ? mhi_dev->dl_chan : mhi_dev->ul_chan;\n\t\tif (!mhi_chan)\n\t\t\tcontinue;\n\n\t\tmhi_unprepare_channel(mhi_cntrl, mhi_chan);\n\t}\n\n\treturn ret;\n}\n\nint mhi_prepare_for_transfer(struct mhi_device *mhi_dev)\n{\n\treturn __mhi_prepare_for_transfer(mhi_dev, 0);\n}\nEXPORT_SYMBOL_GPL(mhi_prepare_for_transfer);\n\nint mhi_prepare_for_transfer_autoqueue(struct mhi_device *mhi_dev)\n{\n\treturn __mhi_prepare_for_transfer(mhi_dev, MHI_CH_INBOUND_ALLOC_BUFS);\n}\nEXPORT_SYMBOL_GPL(mhi_prepare_for_transfer_autoqueue);\n\nvoid mhi_unprepare_from_transfer(struct mhi_device *mhi_dev)\n{\n\tstruct mhi_controller *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_chan *mhi_chan;\n\tint dir;\n\n\tfor (dir = 0; dir < 2; dir++) {\n\t\tmhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;\n\t\tif (!mhi_chan)\n\t\t\tcontinue;\n\n\t\tmhi_unprepare_channel(mhi_cntrl, mhi_chan);\n\t}\n}\nEXPORT_SYMBOL_GPL(mhi_unprepare_from_transfer);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}