{
  "module_name": "main.c",
  "hash_id": "c2c89acd1f3224866372587d535121db36f8350d7aef6f250fd40d37edce9703",
  "original_prompt": "Ingested from linux-6.6.14/drivers/bus/mhi/ep/main.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/dma-direction.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/irq.h>\n#include <linux/mhi_ep.h>\n#include <linux/mod_devicetable.h>\n#include <linux/module.h>\n#include \"internal.h\"\n\n#define M0_WAIT_DELAY_MS\t100\n#define M0_WAIT_COUNT\t\t100\n\nstatic DEFINE_IDA(mhi_ep_cntrl_ida);\n\nstatic int mhi_ep_create_device(struct mhi_ep_cntrl *mhi_cntrl, u32 ch_id);\nstatic int mhi_ep_destroy_device(struct device *dev, void *data);\n\nstatic int mhi_ep_send_event(struct mhi_ep_cntrl *mhi_cntrl, u32 ring_idx,\n\t\t\t     struct mhi_ring_element *el, bool bei)\n{\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tunion mhi_ep_ring_ctx *ctx;\n\tstruct mhi_ep_ring *ring;\n\tint ret;\n\n\tmutex_lock(&mhi_cntrl->event_lock);\n\tring = &mhi_cntrl->mhi_event[ring_idx].ring;\n\tctx = (union mhi_ep_ring_ctx *)&mhi_cntrl->ev_ctx_cache[ring_idx];\n\tif (!ring->started) {\n\t\tret = mhi_ep_ring_start(mhi_cntrl, ring, ctx);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error starting event ring (%u)\\n\", ring_idx);\n\t\t\tgoto err_unlock;\n\t\t}\n\t}\n\n\t \n\tret = mhi_ep_ring_add_element(ring, el);\n\tif (ret) {\n\t\tdev_err(dev, \"Error adding element to event ring (%u)\\n\", ring_idx);\n\t\tgoto err_unlock;\n\t}\n\n\tmutex_unlock(&mhi_cntrl->event_lock);\n\n\t \n\tif (!bei)\n\t\tmhi_cntrl->raise_irq(mhi_cntrl, ring->irq_vector);\n\n\treturn 0;\n\nerr_unlock:\n\tmutex_unlock(&mhi_cntrl->event_lock);\n\n\treturn ret;\n}\n\nstatic int mhi_ep_send_completion_event(struct mhi_ep_cntrl *mhi_cntrl, struct mhi_ep_ring *ring,\n\t\t\t\t\tstruct mhi_ring_element *tre, u32 len, enum mhi_ev_ccs code)\n{\n\tstruct mhi_ring_element *event;\n\tint ret;\n\n\tevent = kmem_cache_zalloc(mhi_cntrl->ev_ring_el_cache, GFP_KERNEL | GFP_DMA);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tevent->ptr = cpu_to_le64(ring->rbase + ring->rd_offset * sizeof(*tre));\n\tevent->dword[0] = MHI_TRE_EV_DWORD0(code, len);\n\tevent->dword[1] = MHI_TRE_EV_DWORD1(ring->ch_id, MHI_PKT_TYPE_TX_EVENT);\n\n\tret = mhi_ep_send_event(mhi_cntrl, ring->er_index, event, MHI_TRE_DATA_GET_BEI(tre));\n\tkmem_cache_free(mhi_cntrl->ev_ring_el_cache, event);\n\n\treturn ret;\n}\n\nint mhi_ep_send_state_change_event(struct mhi_ep_cntrl *mhi_cntrl, enum mhi_state state)\n{\n\tstruct mhi_ring_element *event;\n\tint ret;\n\n\tevent = kmem_cache_zalloc(mhi_cntrl->ev_ring_el_cache, GFP_KERNEL | GFP_DMA);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tevent->dword[0] = MHI_SC_EV_DWORD0(state);\n\tevent->dword[1] = MHI_SC_EV_DWORD1(MHI_PKT_TYPE_STATE_CHANGE_EVENT);\n\n\tret = mhi_ep_send_event(mhi_cntrl, 0, event, 0);\n\tkmem_cache_free(mhi_cntrl->ev_ring_el_cache, event);\n\n\treturn ret;\n}\n\nint mhi_ep_send_ee_event(struct mhi_ep_cntrl *mhi_cntrl, enum mhi_ee_type exec_env)\n{\n\tstruct mhi_ring_element *event;\n\tint ret;\n\n\tevent = kmem_cache_zalloc(mhi_cntrl->ev_ring_el_cache, GFP_KERNEL | GFP_DMA);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tevent->dword[0] = MHI_EE_EV_DWORD0(exec_env);\n\tevent->dword[1] = MHI_SC_EV_DWORD1(MHI_PKT_TYPE_EE_EVENT);\n\n\tret = mhi_ep_send_event(mhi_cntrl, 0, event, 0);\n\tkmem_cache_free(mhi_cntrl->ev_ring_el_cache, event);\n\n\treturn ret;\n}\n\nstatic int mhi_ep_send_cmd_comp_event(struct mhi_ep_cntrl *mhi_cntrl, enum mhi_ev_ccs code)\n{\n\tstruct mhi_ep_ring *ring = &mhi_cntrl->mhi_cmd->ring;\n\tstruct mhi_ring_element *event;\n\tint ret;\n\n\tevent = kmem_cache_zalloc(mhi_cntrl->ev_ring_el_cache, GFP_KERNEL | GFP_DMA);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tevent->ptr = cpu_to_le64(ring->rbase + ring->rd_offset * sizeof(struct mhi_ring_element));\n\tevent->dword[0] = MHI_CC_EV_DWORD0(code);\n\tevent->dword[1] = MHI_CC_EV_DWORD1(MHI_PKT_TYPE_CMD_COMPLETION_EVENT);\n\n\tret = mhi_ep_send_event(mhi_cntrl, 0, event, 0);\n\tkmem_cache_free(mhi_cntrl->ev_ring_el_cache, event);\n\n\treturn ret;\n}\n\nstatic int mhi_ep_process_cmd_ring(struct mhi_ep_ring *ring, struct mhi_ring_element *el)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = ring->mhi_cntrl;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tstruct mhi_result result = {};\n\tstruct mhi_ep_chan *mhi_chan;\n\tstruct mhi_ep_ring *ch_ring;\n\tu32 tmp, ch_id;\n\tint ret;\n\n\tch_id = MHI_TRE_GET_CMD_CHID(el);\n\n\t \n\tif ((ch_id >= mhi_cntrl->max_chan) || !mhi_cntrl->mhi_chan[ch_id].name) {\n\t\tdev_dbg(dev, \"Channel (%u) not supported!\\n\", ch_id);\n\t\treturn -ENODEV;\n\t}\n\n\tmhi_chan = &mhi_cntrl->mhi_chan[ch_id];\n\tch_ring = &mhi_cntrl->mhi_chan[ch_id].ring;\n\n\tswitch (MHI_TRE_GET_CMD_TYPE(el)) {\n\tcase MHI_PKT_TYPE_START_CHAN_CMD:\n\t\tdev_dbg(dev, \"Received START command for channel (%u)\\n\", ch_id);\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\tif (!ch_ring->started) {\n\t\t\tret = mhi_ep_ring_start(mhi_cntrl, ch_ring,\n\t\t\t\t(union mhi_ep_ring_ctx *)&mhi_cntrl->ch_ctx_cache[ch_id]);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(dev, \"Failed to start ring for channel (%u)\\n\", ch_id);\n\t\t\t\tret = mhi_ep_send_cmd_comp_event(mhi_cntrl,\n\t\t\t\t\t\t\tMHI_EV_CC_UNDEFINED_ERR);\n\t\t\t\tif (ret)\n\t\t\t\t\tdev_err(dev, \"Error sending completion event: %d\\n\", ret);\n\n\t\t\t\tgoto err_unlock;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tmhi_chan->state = MHI_CH_STATE_RUNNING;\n\t\ttmp = le32_to_cpu(mhi_cntrl->ch_ctx_cache[ch_id].chcfg);\n\t\ttmp &= ~CHAN_CTX_CHSTATE_MASK;\n\t\ttmp |= FIELD_PREP(CHAN_CTX_CHSTATE_MASK, MHI_CH_STATE_RUNNING);\n\t\tmhi_cntrl->ch_ctx_cache[ch_id].chcfg = cpu_to_le32(tmp);\n\n\t\tret = mhi_ep_send_cmd_comp_event(mhi_cntrl, MHI_EV_CC_SUCCESS);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error sending command completion event (%u)\\n\",\n\t\t\t\tMHI_EV_CC_SUCCESS);\n\t\t\tgoto err_unlock;\n\t\t}\n\n\t\tmutex_unlock(&mhi_chan->lock);\n\n\t\t \n\t\tif (!(ch_id % 2) && !mhi_chan->mhi_dev) {\n\t\t\tret = mhi_ep_create_device(mhi_cntrl, ch_id);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(dev, \"Error creating device for channel (%u)\\n\", ch_id);\n\t\t\t\tmhi_ep_handle_syserr(mhi_cntrl);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tmhi_ep_mmio_enable_chdb(mhi_cntrl, ch_id);\n\n\t\tbreak;\n\tcase MHI_PKT_TYPE_STOP_CHAN_CMD:\n\t\tdev_dbg(dev, \"Received STOP command for channel (%u)\\n\", ch_id);\n\t\tif (!ch_ring->started) {\n\t\t\tdev_err(dev, \"Channel (%u) not opened\\n\", ch_id);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\tmhi_ep_mmio_disable_chdb(mhi_cntrl, ch_id);\n\n\t\t \n\t\tif (mhi_chan->xfer_cb) {\n\t\t\tresult.transaction_status = -ENOTCONN;\n\t\t\tresult.bytes_xferd = 0;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t}\n\n\t\t \n\t\tmhi_chan->state = MHI_CH_STATE_STOP;\n\t\ttmp = le32_to_cpu(mhi_cntrl->ch_ctx_cache[ch_id].chcfg);\n\t\ttmp &= ~CHAN_CTX_CHSTATE_MASK;\n\t\ttmp |= FIELD_PREP(CHAN_CTX_CHSTATE_MASK, MHI_CH_STATE_STOP);\n\t\tmhi_cntrl->ch_ctx_cache[ch_id].chcfg = cpu_to_le32(tmp);\n\n\t\tret = mhi_ep_send_cmd_comp_event(mhi_cntrl, MHI_EV_CC_SUCCESS);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error sending command completion event (%u)\\n\",\n\t\t\t\tMHI_EV_CC_SUCCESS);\n\t\t\tgoto err_unlock;\n\t\t}\n\n\t\tmutex_unlock(&mhi_chan->lock);\n\t\tbreak;\n\tcase MHI_PKT_TYPE_RESET_CHAN_CMD:\n\t\tdev_dbg(dev, \"Received RESET command for channel (%u)\\n\", ch_id);\n\t\tif (!ch_ring->started) {\n\t\t\tdev_err(dev, \"Channel (%u) not opened\\n\", ch_id);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\tmhi_ep_ring_reset(mhi_cntrl, ch_ring);\n\n\t\t \n\t\tif (mhi_chan->xfer_cb) {\n\t\t\tresult.transaction_status = -ENOTCONN;\n\t\t\tresult.bytes_xferd = 0;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t}\n\n\t\t \n\t\tmhi_chan->state = MHI_CH_STATE_DISABLED;\n\t\ttmp = le32_to_cpu(mhi_cntrl->ch_ctx_cache[ch_id].chcfg);\n\t\ttmp &= ~CHAN_CTX_CHSTATE_MASK;\n\t\ttmp |= FIELD_PREP(CHAN_CTX_CHSTATE_MASK, MHI_CH_STATE_DISABLED);\n\t\tmhi_cntrl->ch_ctx_cache[ch_id].chcfg = cpu_to_le32(tmp);\n\n\t\tret = mhi_ep_send_cmd_comp_event(mhi_cntrl, MHI_EV_CC_SUCCESS);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error sending command completion event (%u)\\n\",\n\t\t\t\tMHI_EV_CC_SUCCESS);\n\t\t\tgoto err_unlock;\n\t\t}\n\n\t\tmutex_unlock(&mhi_chan->lock);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Invalid command received: %lu for channel (%u)\\n\",\n\t\t\tMHI_TRE_GET_CMD_TYPE(el), ch_id);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n\nerr_unlock:\n\tmutex_unlock(&mhi_chan->lock);\n\n\treturn ret;\n}\n\nbool mhi_ep_queue_is_empty(struct mhi_ep_device *mhi_dev, enum dma_data_direction dir)\n{\n\tstruct mhi_ep_chan *mhi_chan = (dir == DMA_FROM_DEVICE) ? mhi_dev->dl_chan :\n\t\t\t\t\t\t\t\tmhi_dev->ul_chan;\n\tstruct mhi_ep_cntrl *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_ep_ring *ring = &mhi_cntrl->mhi_chan[mhi_chan->chan].ring;\n\n\treturn !!(ring->rd_offset == ring->wr_offset);\n}\nEXPORT_SYMBOL_GPL(mhi_ep_queue_is_empty);\n\nstatic int mhi_ep_read_channel(struct mhi_ep_cntrl *mhi_cntrl,\n\t\t\t\tstruct mhi_ep_ring *ring,\n\t\t\t\tstruct mhi_result *result,\n\t\t\t\tu32 len)\n{\n\tstruct mhi_ep_chan *mhi_chan = &mhi_cntrl->mhi_chan[ring->ch_id];\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tsize_t tr_len, read_offset, write_offset;\n\tstruct mhi_ep_buf_info buf_info = {};\n\tstruct mhi_ring_element *el;\n\tbool tr_done = false;\n\tu32 buf_left;\n\tint ret;\n\n\tbuf_left = len;\n\n\tdo {\n\t\t \n\t\tif (mhi_chan->state != MHI_CH_STATE_RUNNING) {\n\t\t\tdev_err(dev, \"Channel not available\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tel = &ring->ring_cache[ring->rd_offset];\n\n\t\t \n\t\tif (mhi_chan->tre_bytes_left) {\n\t\t\tdev_dbg(dev, \"TRE bytes remaining: %u\\n\", mhi_chan->tre_bytes_left);\n\t\t\ttr_len = min(buf_left, mhi_chan->tre_bytes_left);\n\t\t} else {\n\t\t\tmhi_chan->tre_loc = MHI_TRE_DATA_GET_PTR(el);\n\t\t\tmhi_chan->tre_size = MHI_TRE_DATA_GET_LEN(el);\n\t\t\tmhi_chan->tre_bytes_left = mhi_chan->tre_size;\n\n\t\t\ttr_len = min(buf_left, mhi_chan->tre_size);\n\t\t}\n\n\t\tread_offset = mhi_chan->tre_size - mhi_chan->tre_bytes_left;\n\t\twrite_offset = len - buf_left;\n\n\t\tbuf_info.host_addr = mhi_chan->tre_loc + read_offset;\n\t\tbuf_info.dev_addr = result->buf_addr + write_offset;\n\t\tbuf_info.size = tr_len;\n\n\t\tdev_dbg(dev, \"Reading %zd bytes from channel (%u)\\n\", tr_len, ring->ch_id);\n\t\tret = mhi_cntrl->read_from_host(mhi_cntrl, &buf_info);\n\t\tif (ret < 0) {\n\t\t\tdev_err(&mhi_chan->mhi_dev->dev, \"Error reading from channel\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tbuf_left -= tr_len;\n\t\tmhi_chan->tre_bytes_left -= tr_len;\n\n\t\t \n\t\tif (!mhi_chan->tre_bytes_left) {\n\t\t\t \n\t\t\tif (MHI_TRE_DATA_GET_CHAIN(el)) {\n\t\t\t\t \n\t\t\t\tif (MHI_TRE_DATA_GET_IEOB(el)) {\n\t\t\t\t\tret = mhi_ep_send_completion_event(mhi_cntrl, ring, el,\n\t\t\t\t\t\t\t\t     MHI_TRE_DATA_GET_LEN(el),\n\t\t\t\t\t\t\t\t     MHI_EV_CC_EOB);\n\t\t\t\t\tif (ret < 0) {\n\t\t\t\t\t\tdev_err(&mhi_chan->mhi_dev->dev,\n\t\t\t\t\t\t\t\"Error sending transfer compl. event\\n\");\n\t\t\t\t\t\treturn ret;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (MHI_TRE_DATA_GET_IEOT(el)) {\n\t\t\t\t\tret = mhi_ep_send_completion_event(mhi_cntrl, ring, el,\n\t\t\t\t\t\t\t\t     MHI_TRE_DATA_GET_LEN(el),\n\t\t\t\t\t\t\t\t     MHI_EV_CC_EOT);\n\t\t\t\t\tif (ret < 0) {\n\t\t\t\t\t\tdev_err(&mhi_chan->mhi_dev->dev,\n\t\t\t\t\t\t\t\"Error sending transfer compl. event\\n\");\n\t\t\t\t\t\treturn ret;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttr_done = true;\n\t\t\t}\n\n\t\t\tmhi_ep_ring_inc_index(ring);\n\t\t}\n\n\t\tresult->bytes_xferd += tr_len;\n\t} while (buf_left && !tr_done);\n\n\treturn 0;\n}\n\nstatic int mhi_ep_process_ch_ring(struct mhi_ep_ring *ring, struct mhi_ring_element *el)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = ring->mhi_cntrl;\n\tstruct mhi_result result = {};\n\tu32 len = MHI_EP_DEFAULT_MTU;\n\tstruct mhi_ep_chan *mhi_chan;\n\tint ret;\n\n\tmhi_chan = &mhi_cntrl->mhi_chan[ring->ch_id];\n\n\t \n\tif (!mhi_chan->xfer_cb) {\n\t\tdev_err(&mhi_chan->mhi_dev->dev, \"Client driver not available\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (ring->ch_id % 2) {\n\t\t \n\t\tresult.dir = mhi_chan->dir;\n\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t} else {\n\t\t \n\t\tresult.buf_addr = kmem_cache_zalloc(mhi_cntrl->tre_buf_cache, GFP_KERNEL | GFP_DMA);\n\t\tif (!result.buf_addr)\n\t\t\treturn -ENOMEM;\n\n\t\tdo {\n\t\t\tret = mhi_ep_read_channel(mhi_cntrl, ring, &result, len);\n\t\t\tif (ret < 0) {\n\t\t\t\tdev_err(&mhi_chan->mhi_dev->dev, \"Failed to read channel\\n\");\n\t\t\t\tkmem_cache_free(mhi_cntrl->tre_buf_cache, result.buf_addr);\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tresult.dir = mhi_chan->dir;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t\tresult.bytes_xferd = 0;\n\t\t\tmemset(result.buf_addr, 0, len);\n\n\t\t\t \n\t\t} while (!mhi_ep_queue_is_empty(mhi_chan->mhi_dev, DMA_TO_DEVICE));\n\n\t\tkmem_cache_free(mhi_cntrl->tre_buf_cache, result.buf_addr);\n\t}\n\n\treturn 0;\n}\n\n \nint mhi_ep_queue_skb(struct mhi_ep_device *mhi_dev, struct sk_buff *skb)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = mhi_dev->mhi_cntrl;\n\tstruct mhi_ep_chan *mhi_chan = mhi_dev->dl_chan;\n\tstruct device *dev = &mhi_chan->mhi_dev->dev;\n\tstruct mhi_ep_buf_info buf_info = {};\n\tstruct mhi_ring_element *el;\n\tu32 buf_left, read_offset;\n\tstruct mhi_ep_ring *ring;\n\tenum mhi_ev_ccs code;\n\tsize_t tr_len;\n\tu32 tre_len;\n\tint ret;\n\n\tbuf_left = skb->len;\n\tring = &mhi_cntrl->mhi_chan[mhi_chan->chan].ring;\n\n\tmutex_lock(&mhi_chan->lock);\n\n\tdo {\n\t\t \n\t\tif (mhi_chan->state != MHI_CH_STATE_RUNNING) {\n\t\t\tdev_err(dev, \"Channel not available\\n\");\n\t\t\tret = -ENODEV;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\tif (mhi_ep_queue_is_empty(mhi_dev, DMA_FROM_DEVICE)) {\n\t\t\tdev_err(dev, \"TRE not available!\\n\");\n\t\t\tret = -ENOSPC;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\tel = &ring->ring_cache[ring->rd_offset];\n\t\ttre_len = MHI_TRE_DATA_GET_LEN(el);\n\n\t\ttr_len = min(buf_left, tre_len);\n\t\tread_offset = skb->len - buf_left;\n\n\t\tbuf_info.dev_addr = skb->data + read_offset;\n\t\tbuf_info.host_addr = MHI_TRE_DATA_GET_PTR(el);\n\t\tbuf_info.size = tr_len;\n\n\t\tdev_dbg(dev, \"Writing %zd bytes to channel (%u)\\n\", tr_len, ring->ch_id);\n\t\tret = mhi_cntrl->write_to_host(mhi_cntrl, &buf_info);\n\t\tif (ret < 0) {\n\t\t\tdev_err(dev, \"Error writing to the channel\\n\");\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\tbuf_left -= tr_len;\n\t\t \n\t\tif (buf_left)\n\t\t\tcode = MHI_EV_CC_OVERFLOW;\n\t\telse\n\t\t\tcode = MHI_EV_CC_EOT;\n\n\t\tret = mhi_ep_send_completion_event(mhi_cntrl, ring, el, tr_len, code);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error sending transfer completion event\\n\");\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\tmhi_ep_ring_inc_index(ring);\n\t} while (buf_left);\n\n\tmutex_unlock(&mhi_chan->lock);\n\n\treturn 0;\n\nerr_exit:\n\tmutex_unlock(&mhi_chan->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mhi_ep_queue_skb);\n\nstatic int mhi_ep_cache_host_cfg(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tsize_t cmd_ctx_host_size, ch_ctx_host_size, ev_ctx_host_size;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tint ret;\n\n\t \n\tmhi_ep_mmio_update_ner(mhi_cntrl);\n\n\tdev_dbg(dev, \"Number of Event rings: %u, HW Event rings: %u\\n\",\n\t\t mhi_cntrl->event_rings, mhi_cntrl->hw_event_rings);\n\n\tch_ctx_host_size = sizeof(struct mhi_chan_ctxt) * mhi_cntrl->max_chan;\n\tev_ctx_host_size = sizeof(struct mhi_event_ctxt) * mhi_cntrl->event_rings;\n\tcmd_ctx_host_size = sizeof(struct mhi_cmd_ctxt) * NR_OF_CMD_RINGS;\n\n\t \n\tmhi_ep_mmio_get_chc_base(mhi_cntrl);\n\n\t \n\tret = mhi_cntrl->alloc_map(mhi_cntrl, mhi_cntrl->ch_ctx_host_pa,\n\t\t\t\t   &mhi_cntrl->ch_ctx_cache_phys,\n\t\t\t\t   (void __iomem **) &mhi_cntrl->ch_ctx_cache,\n\t\t\t\t   ch_ctx_host_size);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to allocate and map ch_ctx_cache\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tmhi_ep_mmio_get_erc_base(mhi_cntrl);\n\n\t \n\tret = mhi_cntrl->alloc_map(mhi_cntrl, mhi_cntrl->ev_ctx_host_pa,\n\t\t\t\t   &mhi_cntrl->ev_ctx_cache_phys,\n\t\t\t\t   (void __iomem **) &mhi_cntrl->ev_ctx_cache,\n\t\t\t\t   ev_ctx_host_size);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to allocate and map ev_ctx_cache\\n\");\n\t\tgoto err_ch_ctx;\n\t}\n\n\t \n\tmhi_ep_mmio_get_crc_base(mhi_cntrl);\n\n\t \n\tret = mhi_cntrl->alloc_map(mhi_cntrl, mhi_cntrl->cmd_ctx_host_pa,\n\t\t\t\t   &mhi_cntrl->cmd_ctx_cache_phys,\n\t\t\t\t   (void __iomem **) &mhi_cntrl->cmd_ctx_cache,\n\t\t\t\t   cmd_ctx_host_size);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to allocate and map cmd_ctx_cache\\n\");\n\t\tgoto err_ev_ctx;\n\t}\n\n\t \n\tret = mhi_ep_ring_start(mhi_cntrl, &mhi_cntrl->mhi_cmd->ring,\n\t\t\t\t(union mhi_ep_ring_ctx *)mhi_cntrl->cmd_ctx_cache);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to start the command ring\\n\");\n\t\tgoto err_cmd_ctx;\n\t}\n\n\treturn ret;\n\nerr_cmd_ctx:\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->cmd_ctx_host_pa, mhi_cntrl->cmd_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->cmd_ctx_cache, cmd_ctx_host_size);\n\nerr_ev_ctx:\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->ev_ctx_host_pa, mhi_cntrl->ev_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->ev_ctx_cache, ev_ctx_host_size);\n\nerr_ch_ctx:\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->ch_ctx_host_pa, mhi_cntrl->ch_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->ch_ctx_cache, ch_ctx_host_size);\n\n\treturn ret;\n}\n\nstatic void mhi_ep_free_host_cfg(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tsize_t cmd_ctx_host_size, ch_ctx_host_size, ev_ctx_host_size;\n\n\tch_ctx_host_size = sizeof(struct mhi_chan_ctxt) * mhi_cntrl->max_chan;\n\tev_ctx_host_size = sizeof(struct mhi_event_ctxt) * mhi_cntrl->event_rings;\n\tcmd_ctx_host_size = sizeof(struct mhi_cmd_ctxt) * NR_OF_CMD_RINGS;\n\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->cmd_ctx_host_pa, mhi_cntrl->cmd_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->cmd_ctx_cache, cmd_ctx_host_size);\n\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->ev_ctx_host_pa, mhi_cntrl->ev_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->ev_ctx_cache, ev_ctx_host_size);\n\n\tmhi_cntrl->unmap_free(mhi_cntrl, mhi_cntrl->ch_ctx_host_pa, mhi_cntrl->ch_ctx_cache_phys,\n\t\t\t      (void __iomem *) mhi_cntrl->ch_ctx_cache, ch_ctx_host_size);\n}\n\nstatic void mhi_ep_enable_int(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\t \n\tmhi_ep_mmio_enable_ctrl_interrupt(mhi_cntrl);\n\tmhi_ep_mmio_enable_cmdb_interrupt(mhi_cntrl);\n}\n\nstatic int mhi_ep_enable(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tenum mhi_state state;\n\tbool mhi_reset;\n\tu32 count = 0;\n\tint ret;\n\n\t \n\tdo {\n\t\tmsleep(M0_WAIT_DELAY_MS);\n\t\tmhi_ep_mmio_get_mhi_state(mhi_cntrl, &state, &mhi_reset);\n\t\tif (mhi_reset) {\n\t\t\t \n\t\t\tmhi_ep_mmio_clear_reset(mhi_cntrl);\n\t\t\tdev_info(dev, \"Detected Host reset while waiting for M0\\n\");\n\t\t}\n\t\tcount++;\n\t} while (state != MHI_STATE_M0 && count < M0_WAIT_COUNT);\n\n\tif (state != MHI_STATE_M0) {\n\t\tdev_err(dev, \"Host failed to enter M0\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tret = mhi_ep_cache_host_cfg(mhi_cntrl);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cache host config\\n\");\n\t\treturn ret;\n\t}\n\n\tmhi_ep_mmio_set_env(mhi_cntrl, MHI_EE_AMSS);\n\n\t \n\tmhi_ep_enable_int(mhi_cntrl);\n\n\treturn 0;\n}\n\nstatic void mhi_ep_cmd_ring_worker(struct work_struct *work)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = container_of(work, struct mhi_ep_cntrl, cmd_ring_work);\n\tstruct mhi_ep_ring *ring = &mhi_cntrl->mhi_cmd->ring;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tstruct mhi_ring_element *el;\n\tint ret;\n\n\t \n\tret = mhi_ep_update_wr_offset(ring);\n\tif (ret) {\n\t\tdev_err(dev, \"Error updating write offset for ring\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (ring->rd_offset == ring->wr_offset)\n\t\treturn;\n\n\t \n\twhile (ring->rd_offset != ring->wr_offset) {\n\t\tel = &ring->ring_cache[ring->rd_offset];\n\n\t\tret = mhi_ep_process_cmd_ring(ring, el);\n\t\tif (ret && ret != -ENODEV)\n\t\t\tdev_err(dev, \"Error processing cmd ring element: %zu\\n\", ring->rd_offset);\n\n\t\tmhi_ep_ring_inc_index(ring);\n\t}\n}\n\nstatic void mhi_ep_ch_ring_worker(struct work_struct *work)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = container_of(work, struct mhi_ep_cntrl, ch_ring_work);\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tstruct mhi_ep_ring_item *itr, *tmp;\n\tstruct mhi_ring_element *el;\n\tstruct mhi_ep_ring *ring;\n\tstruct mhi_ep_chan *chan;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\tint ret;\n\n\tspin_lock_irqsave(&mhi_cntrl->list_lock, flags);\n\tlist_splice_tail_init(&mhi_cntrl->ch_db_list, &head);\n\tspin_unlock_irqrestore(&mhi_cntrl->list_lock, flags);\n\n\t \n\tlist_for_each_entry_safe(itr, tmp, &head, node) {\n\t\tlist_del(&itr->node);\n\t\tring = itr->ring;\n\n\t\tchan = &mhi_cntrl->mhi_chan[ring->ch_id];\n\t\tmutex_lock(&chan->lock);\n\n\t\t \n\t\tif (!ring->started) {\n\t\t\tmutex_unlock(&chan->lock);\n\t\t\tkfree(itr);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tret = mhi_ep_update_wr_offset(ring);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error updating write offset for ring\\n\");\n\t\t\tmutex_unlock(&chan->lock);\n\t\t\tkmem_cache_free(mhi_cntrl->ring_item_cache, itr);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (ring->rd_offset == ring->wr_offset) {\n\t\t\tmutex_unlock(&chan->lock);\n\t\t\tkmem_cache_free(mhi_cntrl->ring_item_cache, itr);\n\t\t\tcontinue;\n\t\t}\n\n\t\tel = &ring->ring_cache[ring->rd_offset];\n\n\t\tdev_dbg(dev, \"Processing the ring for channel (%u)\\n\", ring->ch_id);\n\t\tret = mhi_ep_process_ch_ring(ring, el);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Error processing ring for channel (%u): %d\\n\",\n\t\t\t\tring->ch_id, ret);\n\t\t\tmutex_unlock(&chan->lock);\n\t\t\tkmem_cache_free(mhi_cntrl->ring_item_cache, itr);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmutex_unlock(&chan->lock);\n\t\tkmem_cache_free(mhi_cntrl->ring_item_cache, itr);\n\t}\n}\n\nstatic void mhi_ep_state_worker(struct work_struct *work)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = container_of(work, struct mhi_ep_cntrl, state_work);\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tstruct mhi_ep_state_transition *itr, *tmp;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\tint ret;\n\n\tspin_lock_irqsave(&mhi_cntrl->list_lock, flags);\n\tlist_splice_tail_init(&mhi_cntrl->st_transition_list, &head);\n\tspin_unlock_irqrestore(&mhi_cntrl->list_lock, flags);\n\n\tlist_for_each_entry_safe(itr, tmp, &head, node) {\n\t\tlist_del(&itr->node);\n\t\tdev_dbg(dev, \"Handling MHI state transition to %s\\n\",\n\t\t\t mhi_state_str(itr->state));\n\n\t\tswitch (itr->state) {\n\t\tcase MHI_STATE_M0:\n\t\t\tret = mhi_ep_set_m0_state(mhi_cntrl);\n\t\t\tif (ret)\n\t\t\t\tdev_err(dev, \"Failed to transition to M0 state\\n\");\n\t\t\tbreak;\n\t\tcase MHI_STATE_M3:\n\t\t\tret = mhi_ep_set_m3_state(mhi_cntrl);\n\t\t\tif (ret)\n\t\t\t\tdev_err(dev, \"Failed to transition to M3 state\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"Invalid MHI state transition: %d\\n\", itr->state);\n\t\t\tbreak;\n\t\t}\n\t\tkfree(itr);\n\t}\n}\n\nstatic void mhi_ep_queue_channel_db(struct mhi_ep_cntrl *mhi_cntrl, unsigned long ch_int,\n\t\t\t\t    u32 ch_idx)\n{\n\tstruct mhi_ep_ring_item *item;\n\tstruct mhi_ep_ring *ring;\n\tbool work = !!ch_int;\n\tLIST_HEAD(head);\n\tu32 i;\n\n\t \n\tfor_each_set_bit(i, &ch_int, 32) {\n\t\t \n\t\tu32 ch_id = ch_idx + i;\n\n\t\tring = &mhi_cntrl->mhi_chan[ch_id].ring;\n\t\titem = kmem_cache_zalloc(mhi_cntrl->ring_item_cache, GFP_ATOMIC);\n\t\tif (!item)\n\t\t\treturn;\n\n\t\titem->ring = ring;\n\t\tlist_add_tail(&item->node, &head);\n\t}\n\n\t \n\tif (work) {\n\t\tspin_lock(&mhi_cntrl->list_lock);\n\t\tlist_splice_tail_init(&head, &mhi_cntrl->ch_db_list);\n\t\tspin_unlock(&mhi_cntrl->list_lock);\n\n\t\tqueue_work(mhi_cntrl->wq, &mhi_cntrl->ch_ring_work);\n\t}\n}\n\n \nstatic void mhi_ep_check_channel_interrupt(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tu32 ch_int, ch_idx, i;\n\n\t \n\tif (!mhi_ep_mmio_read_chdb_status_interrupts(mhi_cntrl))\n\t\treturn;\n\n\tfor (i = 0; i < MHI_MASK_ROWS_CH_DB; i++) {\n\t\tch_idx = i * MHI_MASK_CH_LEN;\n\n\t\t \n\t\tch_int = mhi_cntrl->chdb[i].status & mhi_cntrl->chdb[i].mask;\n\t\tif (ch_int) {\n\t\t\tmhi_ep_queue_channel_db(mhi_cntrl, ch_int, ch_idx);\n\t\t\tmhi_ep_mmio_write(mhi_cntrl, MHI_CHDB_INT_CLEAR_n(i),\n\t\t\t\t\t\t\tmhi_cntrl->chdb[i].status);\n\t\t}\n\t}\n}\n\nstatic void mhi_ep_process_ctrl_interrupt(struct mhi_ep_cntrl *mhi_cntrl,\n\t\t\t\t\t enum mhi_state state)\n{\n\tstruct mhi_ep_state_transition *item;\n\n\titem = kzalloc(sizeof(*item), GFP_ATOMIC);\n\tif (!item)\n\t\treturn;\n\n\titem->state = state;\n\tspin_lock(&mhi_cntrl->list_lock);\n\tlist_add_tail(&item->node, &mhi_cntrl->st_transition_list);\n\tspin_unlock(&mhi_cntrl->list_lock);\n\n\tqueue_work(mhi_cntrl->wq, &mhi_cntrl->state_work);\n}\n\n \nstatic irqreturn_t mhi_ep_irq(int irq, void *data)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = data;\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tenum mhi_state state;\n\tu32 int_value;\n\tbool mhi_reset;\n\n\t \n\tint_value = mhi_ep_mmio_read(mhi_cntrl, MHI_CTRL_INT_STATUS);\n\tmhi_ep_mmio_write(mhi_cntrl, MHI_CTRL_INT_CLEAR, int_value);\n\n\t \n\tif (FIELD_GET(MHI_CTRL_INT_STATUS_MSK, int_value)) {\n\t\tdev_dbg(dev, \"Processing ctrl interrupt\\n\");\n\t\tmhi_ep_mmio_get_mhi_state(mhi_cntrl, &state, &mhi_reset);\n\t\tif (mhi_reset) {\n\t\t\tdev_info(dev, \"Host triggered MHI reset!\\n\");\n\t\t\tdisable_irq_nosync(mhi_cntrl->irq);\n\t\t\tschedule_work(&mhi_cntrl->reset_work);\n\t\t\treturn IRQ_HANDLED;\n\t\t}\n\n\t\tmhi_ep_process_ctrl_interrupt(mhi_cntrl, state);\n\t}\n\n\t \n\tif (FIELD_GET(MHI_CTRL_INT_STATUS_CRDB_MSK, int_value)) {\n\t\tdev_dbg(dev, \"Processing command doorbell interrupt\\n\");\n\t\tqueue_work(mhi_cntrl->wq, &mhi_cntrl->cmd_ring_work);\n\t}\n\n\t \n\tmhi_ep_check_channel_interrupt(mhi_cntrl);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mhi_ep_abort_transfer(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct mhi_ep_ring *ch_ring, *ev_ring;\n\tstruct mhi_result result = {};\n\tstruct mhi_ep_chan *mhi_chan;\n\tint i;\n\n\t \n\tfor (i = 0; i < mhi_cntrl->max_chan; i++) {\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[i];\n\t\tif (!mhi_chan->ring.started)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\tif (mhi_chan->xfer_cb) {\n\t\t\tresult.transaction_status = -ENOTCONN;\n\t\t\tresult.bytes_xferd = 0;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t}\n\n\t\tmhi_chan->state = MHI_CH_STATE_DISABLED;\n\t\tmutex_unlock(&mhi_chan->lock);\n\t}\n\n\tflush_workqueue(mhi_cntrl->wq);\n\n\t \n\tdevice_for_each_child(&mhi_cntrl->mhi_dev->dev, NULL, mhi_ep_destroy_device);\n\n\t \n\tfor (i = 0; i < mhi_cntrl->max_chan; i++) {\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[i];\n\t\tif (!mhi_chan->ring.started)\n\t\t\tcontinue;\n\n\t\tch_ring = &mhi_cntrl->mhi_chan[i].ring;\n\t\tmutex_lock(&mhi_chan->lock);\n\t\tmhi_ep_ring_reset(mhi_cntrl, ch_ring);\n\t\tmutex_unlock(&mhi_chan->lock);\n\t}\n\n\t \n\tfor (i = 0; i < mhi_cntrl->event_rings; i++) {\n\t\tev_ring = &mhi_cntrl->mhi_event[i].ring;\n\t\tif (!ev_ring->started)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&mhi_cntrl->event_lock);\n\t\tmhi_ep_ring_reset(mhi_cntrl, ev_ring);\n\t\tmutex_unlock(&mhi_cntrl->event_lock);\n\t}\n\n\t \n\tmhi_ep_ring_reset(mhi_cntrl, &mhi_cntrl->mhi_cmd->ring);\n\n\tmhi_ep_free_host_cfg(mhi_cntrl);\n\tmhi_ep_mmio_mask_interrupts(mhi_cntrl);\n\n\tmhi_cntrl->enabled = false;\n}\n\nstatic void mhi_ep_reset_worker(struct work_struct *work)\n{\n\tstruct mhi_ep_cntrl *mhi_cntrl = container_of(work, struct mhi_ep_cntrl, reset_work);\n\tenum mhi_state cur_state;\n\n\tmhi_ep_power_down(mhi_cntrl);\n\n\tmutex_lock(&mhi_cntrl->state_lock);\n\n\t \n\tmhi_ep_mmio_reset(mhi_cntrl);\n\tcur_state = mhi_cntrl->mhi_state;\n\n\t \n\tif (cur_state == MHI_STATE_SYS_ERR)\n\t\tmhi_ep_power_up(mhi_cntrl);\n\n\tmutex_unlock(&mhi_cntrl->state_lock);\n}\n\n \nvoid mhi_ep_handle_syserr(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tint ret;\n\n\tret = mhi_ep_set_mhi_state(mhi_cntrl, MHI_STATE_SYS_ERR);\n\tif (ret)\n\t\treturn;\n\n\t \n\tret = mhi_ep_send_state_change_event(mhi_cntrl, MHI_STATE_SYS_ERR);\n\tif (ret)\n\t\tdev_err(dev, \"Failed sending SYS_ERR state change event: %d\\n\", ret);\n}\n\nint mhi_ep_power_up(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct device *dev = &mhi_cntrl->mhi_dev->dev;\n\tint ret, i;\n\n\t \n\tmhi_ep_mmio_mask_interrupts(mhi_cntrl);\n\tmhi_ep_mmio_init(mhi_cntrl);\n\n\tmhi_cntrl->mhi_event = kzalloc(mhi_cntrl->event_rings * (sizeof(*mhi_cntrl->mhi_event)),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!mhi_cntrl->mhi_event)\n\t\treturn -ENOMEM;\n\n\t \n\tmhi_ep_ring_init(&mhi_cntrl->mhi_cmd->ring, RING_TYPE_CMD, 0);\n\tfor (i = 0; i < mhi_cntrl->max_chan; i++)\n\t\tmhi_ep_ring_init(&mhi_cntrl->mhi_chan[i].ring, RING_TYPE_CH, i);\n\tfor (i = 0; i < mhi_cntrl->event_rings; i++)\n\t\tmhi_ep_ring_init(&mhi_cntrl->mhi_event[i].ring, RING_TYPE_ER, i);\n\n\tmhi_cntrl->mhi_state = MHI_STATE_RESET;\n\n\t \n\tmhi_ep_mmio_set_env(mhi_cntrl, MHI_EE_AMSS);\n\n\t \n\tret = mhi_ep_set_ready_state(mhi_cntrl);\n\tif (ret)\n\t\tgoto err_free_event;\n\n\tdev_dbg(dev, \"READY state notification sent to the host\\n\");\n\n\tret = mhi_ep_enable(mhi_cntrl);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to enable MHI endpoint\\n\");\n\t\tgoto err_free_event;\n\t}\n\n\tenable_irq(mhi_cntrl->irq);\n\tmhi_cntrl->enabled = true;\n\n\treturn 0;\n\nerr_free_event:\n\tkfree(mhi_cntrl->mhi_event);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mhi_ep_power_up);\n\nvoid mhi_ep_power_down(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tif (mhi_cntrl->enabled) {\n\t\tmhi_ep_abort_transfer(mhi_cntrl);\n\t\tkfree(mhi_cntrl->mhi_event);\n\t\tdisable_irq(mhi_cntrl->irq);\n\t}\n}\nEXPORT_SYMBOL_GPL(mhi_ep_power_down);\n\nvoid mhi_ep_suspend_channels(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct mhi_ep_chan *mhi_chan;\n\tu32 tmp;\n\tint i;\n\n\tfor (i = 0; i < mhi_cntrl->max_chan; i++) {\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[i];\n\n\t\tif (!mhi_chan->mhi_dev)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\ttmp = le32_to_cpu(mhi_cntrl->ch_ctx_cache[i].chcfg);\n\t\tif (FIELD_GET(CHAN_CTX_CHSTATE_MASK, tmp) != MHI_CH_STATE_RUNNING) {\n\t\t\tmutex_unlock(&mhi_chan->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_dbg(&mhi_chan->mhi_dev->dev, \"Suspending channel\\n\");\n\t\t \n\t\tmhi_chan->state = MHI_CH_STATE_SUSPENDED;\n\t\ttmp &= ~CHAN_CTX_CHSTATE_MASK;\n\t\ttmp |= FIELD_PREP(CHAN_CTX_CHSTATE_MASK, MHI_CH_STATE_SUSPENDED);\n\t\tmhi_cntrl->ch_ctx_cache[i].chcfg = cpu_to_le32(tmp);\n\t\tmutex_unlock(&mhi_chan->lock);\n\t}\n}\n\nvoid mhi_ep_resume_channels(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct mhi_ep_chan *mhi_chan;\n\tu32 tmp;\n\tint i;\n\n\tfor (i = 0; i < mhi_cntrl->max_chan; i++) {\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[i];\n\n\t\tif (!mhi_chan->mhi_dev)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\ttmp = le32_to_cpu(mhi_cntrl->ch_ctx_cache[i].chcfg);\n\t\tif (FIELD_GET(CHAN_CTX_CHSTATE_MASK, tmp) != MHI_CH_STATE_SUSPENDED) {\n\t\t\tmutex_unlock(&mhi_chan->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_dbg(&mhi_chan->mhi_dev->dev, \"Resuming channel\\n\");\n\t\t \n\t\tmhi_chan->state = MHI_CH_STATE_RUNNING;\n\t\ttmp &= ~CHAN_CTX_CHSTATE_MASK;\n\t\ttmp |= FIELD_PREP(CHAN_CTX_CHSTATE_MASK, MHI_CH_STATE_RUNNING);\n\t\tmhi_cntrl->ch_ctx_cache[i].chcfg = cpu_to_le32(tmp);\n\t\tmutex_unlock(&mhi_chan->lock);\n\t}\n}\n\nstatic void mhi_ep_release_device(struct device *dev)\n{\n\tstruct mhi_ep_device *mhi_dev = to_mhi_ep_device(dev);\n\n\tif (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)\n\t\tmhi_dev->mhi_cntrl->mhi_dev = NULL;\n\n\t \n\tif (mhi_dev->ul_chan)\n\t\tmhi_dev->ul_chan->mhi_dev = NULL;\n\n\tif (mhi_dev->dl_chan)\n\t\tmhi_dev->dl_chan->mhi_dev = NULL;\n\n\tkfree(mhi_dev);\n}\n\nstatic struct mhi_ep_device *mhi_ep_alloc_device(struct mhi_ep_cntrl *mhi_cntrl,\n\t\t\t\t\t\t enum mhi_device_type dev_type)\n{\n\tstruct mhi_ep_device *mhi_dev;\n\tstruct device *dev;\n\n\tmhi_dev = kzalloc(sizeof(*mhi_dev), GFP_KERNEL);\n\tif (!mhi_dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdev = &mhi_dev->dev;\n\tdevice_initialize(dev);\n\tdev->bus = &mhi_ep_bus_type;\n\tdev->release = mhi_ep_release_device;\n\n\t \n\tif (dev_type == MHI_DEVICE_CONTROLLER)\n\t\t \n\t\tdev->parent = mhi_cntrl->cntrl_dev;\n\telse\n\t\t \n\t\tdev->parent = &mhi_cntrl->mhi_dev->dev;\n\n\tmhi_dev->mhi_cntrl = mhi_cntrl;\n\tmhi_dev->dev_type = dev_type;\n\n\treturn mhi_dev;\n}\n\n \nstatic int mhi_ep_create_device(struct mhi_ep_cntrl *mhi_cntrl, u32 ch_id)\n{\n\tstruct mhi_ep_chan *mhi_chan = &mhi_cntrl->mhi_chan[ch_id];\n\tstruct device *dev = mhi_cntrl->cntrl_dev;\n\tstruct mhi_ep_device *mhi_dev;\n\tint ret;\n\n\t \n\tif (strcmp(mhi_chan->name, mhi_chan[1].name)) {\n\t\tdev_err(dev, \"UL and DL channel names are not same: (%s) != (%s)\\n\",\n\t\t\tmhi_chan->name, mhi_chan[1].name);\n\t\treturn -EINVAL;\n\t}\n\n\tmhi_dev = mhi_ep_alloc_device(mhi_cntrl, MHI_DEVICE_XFER);\n\tif (IS_ERR(mhi_dev))\n\t\treturn PTR_ERR(mhi_dev);\n\n\t \n\tmhi_dev->ul_chan = mhi_chan;\n\tget_device(&mhi_dev->dev);\n\tmhi_chan->mhi_dev = mhi_dev;\n\n\t \n\tmhi_chan++;\n\tmhi_dev->dl_chan = mhi_chan;\n\tget_device(&mhi_dev->dev);\n\tmhi_chan->mhi_dev = mhi_dev;\n\n\t \n\tmhi_dev->name = mhi_chan->name;\n\tret = dev_set_name(&mhi_dev->dev, \"%s_%s\",\n\t\t     dev_name(&mhi_cntrl->mhi_dev->dev),\n\t\t     mhi_dev->name);\n\tif (ret) {\n\t\tput_device(&mhi_dev->dev);\n\t\treturn ret;\n\t}\n\n\tret = device_add(&mhi_dev->dev);\n\tif (ret)\n\t\tput_device(&mhi_dev->dev);\n\n\treturn ret;\n}\n\nstatic int mhi_ep_destroy_device(struct device *dev, void *data)\n{\n\tstruct mhi_ep_device *mhi_dev;\n\tstruct mhi_ep_cntrl *mhi_cntrl;\n\tstruct mhi_ep_chan *ul_chan, *dl_chan;\n\n\tif (dev->bus != &mhi_ep_bus_type)\n\t\treturn 0;\n\n\tmhi_dev = to_mhi_ep_device(dev);\n\tmhi_cntrl = mhi_dev->mhi_cntrl;\n\n\t \n\tif (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)\n\t\treturn 0;\n\n\tul_chan = mhi_dev->ul_chan;\n\tdl_chan = mhi_dev->dl_chan;\n\n\tif (ul_chan)\n\t\tput_device(&ul_chan->mhi_dev->dev);\n\n\tif (dl_chan)\n\t\tput_device(&dl_chan->mhi_dev->dev);\n\n\tdev_dbg(&mhi_cntrl->mhi_dev->dev, \"Destroying device for chan:%s\\n\",\n\t\t mhi_dev->name);\n\n\t \n\tdevice_del(dev);\n\tput_device(dev);\n\n\treturn 0;\n}\n\nstatic int mhi_ep_chan_init(struct mhi_ep_cntrl *mhi_cntrl,\n\t\t\t    const struct mhi_ep_cntrl_config *config)\n{\n\tconst struct mhi_ep_channel_config *ch_cfg;\n\tstruct device *dev = mhi_cntrl->cntrl_dev;\n\tu32 chan, i;\n\tint ret = -EINVAL;\n\n\tmhi_cntrl->max_chan = config->max_channels;\n\n\t \n\tmhi_cntrl->mhi_chan = kcalloc(mhi_cntrl->max_chan, sizeof(*mhi_cntrl->mhi_chan),\n\t\t\t\t      GFP_KERNEL);\n\tif (!mhi_cntrl->mhi_chan)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < config->num_channels; i++) {\n\t\tstruct mhi_ep_chan *mhi_chan;\n\n\t\tch_cfg = &config->ch_cfg[i];\n\n\t\tchan = ch_cfg->num;\n\t\tif (chan >= mhi_cntrl->max_chan) {\n\t\t\tdev_err(dev, \"Channel (%u) exceeds maximum available channels (%u)\\n\",\n\t\t\t\tchan, mhi_cntrl->max_chan);\n\t\t\tgoto error_chan_cfg;\n\t\t}\n\n\t\t \n\t\tif (ch_cfg->dir == DMA_BIDIRECTIONAL || ch_cfg->dir == DMA_NONE) {\n\t\t\tdev_err(dev, \"Invalid direction (%u) for channel (%u)\\n\",\n\t\t\t\tch_cfg->dir, chan);\n\t\t\tgoto error_chan_cfg;\n\t\t}\n\n\t\tmhi_chan = &mhi_cntrl->mhi_chan[chan];\n\t\tmhi_chan->name = ch_cfg->name;\n\t\tmhi_chan->chan = chan;\n\t\tmhi_chan->dir = ch_cfg->dir;\n\t\tmutex_init(&mhi_chan->lock);\n\t}\n\n\treturn 0;\n\nerror_chan_cfg:\n\tkfree(mhi_cntrl->mhi_chan);\n\n\treturn ret;\n}\n\n \nint mhi_ep_register_controller(struct mhi_ep_cntrl *mhi_cntrl,\n\t\t\t\tconst struct mhi_ep_cntrl_config *config)\n{\n\tstruct mhi_ep_device *mhi_dev;\n\tint ret;\n\n\tif (!mhi_cntrl || !mhi_cntrl->cntrl_dev || !mhi_cntrl->mmio || !mhi_cntrl->irq)\n\t\treturn -EINVAL;\n\n\tret = mhi_ep_chan_init(mhi_cntrl, config);\n\tif (ret)\n\t\treturn ret;\n\n\tmhi_cntrl->mhi_cmd = kcalloc(NR_OF_CMD_RINGS, sizeof(*mhi_cntrl->mhi_cmd), GFP_KERNEL);\n\tif (!mhi_cntrl->mhi_cmd) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_ch;\n\t}\n\n\tmhi_cntrl->ev_ring_el_cache = kmem_cache_create(\"mhi_ep_event_ring_el\",\n\t\t\t\t\t\t\tsizeof(struct mhi_ring_element), 0,\n\t\t\t\t\t\t\tSLAB_CACHE_DMA, NULL);\n\tif (!mhi_cntrl->ev_ring_el_cache) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_cmd;\n\t}\n\n\tmhi_cntrl->tre_buf_cache = kmem_cache_create(\"mhi_ep_tre_buf\", MHI_EP_DEFAULT_MTU, 0,\n\t\t\t\t\t\t      SLAB_CACHE_DMA, NULL);\n\tif (!mhi_cntrl->tre_buf_cache) {\n\t\tret = -ENOMEM;\n\t\tgoto err_destroy_ev_ring_el_cache;\n\t}\n\n\tmhi_cntrl->ring_item_cache = kmem_cache_create(\"mhi_ep_ring_item\",\n\t\t\t\t\t\t\tsizeof(struct mhi_ep_ring_item), 0,\n\t\t\t\t\t\t\t0, NULL);\n\tif (!mhi_cntrl->ev_ring_el_cache) {\n\t\tret = -ENOMEM;\n\t\tgoto err_destroy_tre_buf_cache;\n\t}\n\tINIT_WORK(&mhi_cntrl->state_work, mhi_ep_state_worker);\n\tINIT_WORK(&mhi_cntrl->reset_work, mhi_ep_reset_worker);\n\tINIT_WORK(&mhi_cntrl->cmd_ring_work, mhi_ep_cmd_ring_worker);\n\tINIT_WORK(&mhi_cntrl->ch_ring_work, mhi_ep_ch_ring_worker);\n\n\tmhi_cntrl->wq = alloc_workqueue(\"mhi_ep_wq\", 0, 0);\n\tif (!mhi_cntrl->wq) {\n\t\tret = -ENOMEM;\n\t\tgoto err_destroy_ring_item_cache;\n\t}\n\n\tINIT_LIST_HEAD(&mhi_cntrl->st_transition_list);\n\tINIT_LIST_HEAD(&mhi_cntrl->ch_db_list);\n\tspin_lock_init(&mhi_cntrl->list_lock);\n\tmutex_init(&mhi_cntrl->state_lock);\n\tmutex_init(&mhi_cntrl->event_lock);\n\n\t \n\tmhi_ep_mmio_write(mhi_cntrl, EP_MHIVER, config->mhi_version);\n\tmhi_ep_mmio_set_env(mhi_cntrl, MHI_EE_AMSS);\n\n\t \n\tret = ida_alloc(&mhi_ep_cntrl_ida, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto err_destroy_wq;\n\n\tmhi_cntrl->index = ret;\n\n\tirq_set_status_flags(mhi_cntrl->irq, IRQ_NOAUTOEN);\n\tret = request_irq(mhi_cntrl->irq, mhi_ep_irq, IRQF_TRIGGER_HIGH,\n\t\t\t  \"doorbell_irq\", mhi_cntrl);\n\tif (ret) {\n\t\tdev_err(mhi_cntrl->cntrl_dev, \"Failed to request Doorbell IRQ\\n\");\n\t\tgoto err_ida_free;\n\t}\n\n\t \n\tmhi_dev = mhi_ep_alloc_device(mhi_cntrl, MHI_DEVICE_CONTROLLER);\n\tif (IS_ERR(mhi_dev)) {\n\t\tdev_err(mhi_cntrl->cntrl_dev, \"Failed to allocate controller device\\n\");\n\t\tret = PTR_ERR(mhi_dev);\n\t\tgoto err_free_irq;\n\t}\n\n\tret = dev_set_name(&mhi_dev->dev, \"mhi_ep%u\", mhi_cntrl->index);\n\tif (ret)\n\t\tgoto err_put_dev;\n\n\tmhi_dev->name = dev_name(&mhi_dev->dev);\n\tmhi_cntrl->mhi_dev = mhi_dev;\n\n\tret = device_add(&mhi_dev->dev);\n\tif (ret)\n\t\tgoto err_put_dev;\n\n\tdev_dbg(&mhi_dev->dev, \"MHI EP Controller registered\\n\");\n\n\treturn 0;\n\nerr_put_dev:\n\tput_device(&mhi_dev->dev);\nerr_free_irq:\n\tfree_irq(mhi_cntrl->irq, mhi_cntrl);\nerr_ida_free:\n\tida_free(&mhi_ep_cntrl_ida, mhi_cntrl->index);\nerr_destroy_wq:\n\tdestroy_workqueue(mhi_cntrl->wq);\nerr_destroy_ring_item_cache:\n\tkmem_cache_destroy(mhi_cntrl->ring_item_cache);\nerr_destroy_ev_ring_el_cache:\n\tkmem_cache_destroy(mhi_cntrl->ev_ring_el_cache);\nerr_destroy_tre_buf_cache:\n\tkmem_cache_destroy(mhi_cntrl->tre_buf_cache);\nerr_free_cmd:\n\tkfree(mhi_cntrl->mhi_cmd);\nerr_free_ch:\n\tkfree(mhi_cntrl->mhi_chan);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mhi_ep_register_controller);\n\n \nvoid mhi_ep_unregister_controller(struct mhi_ep_cntrl *mhi_cntrl)\n{\n\tstruct mhi_ep_device *mhi_dev = mhi_cntrl->mhi_dev;\n\n\tdestroy_workqueue(mhi_cntrl->wq);\n\n\tfree_irq(mhi_cntrl->irq, mhi_cntrl);\n\n\tkmem_cache_destroy(mhi_cntrl->tre_buf_cache);\n\tkmem_cache_destroy(mhi_cntrl->ev_ring_el_cache);\n\tkmem_cache_destroy(mhi_cntrl->ring_item_cache);\n\tkfree(mhi_cntrl->mhi_cmd);\n\tkfree(mhi_cntrl->mhi_chan);\n\n\tdevice_del(&mhi_dev->dev);\n\tput_device(&mhi_dev->dev);\n\n\tida_free(&mhi_ep_cntrl_ida, mhi_cntrl->index);\n}\nEXPORT_SYMBOL_GPL(mhi_ep_unregister_controller);\n\nstatic int mhi_ep_driver_probe(struct device *dev)\n{\n\tstruct mhi_ep_device *mhi_dev = to_mhi_ep_device(dev);\n\tstruct mhi_ep_driver *mhi_drv = to_mhi_ep_driver(dev->driver);\n\tstruct mhi_ep_chan *ul_chan = mhi_dev->ul_chan;\n\tstruct mhi_ep_chan *dl_chan = mhi_dev->dl_chan;\n\n\tul_chan->xfer_cb = mhi_drv->ul_xfer_cb;\n\tdl_chan->xfer_cb = mhi_drv->dl_xfer_cb;\n\n\treturn mhi_drv->probe(mhi_dev, mhi_dev->id);\n}\n\nstatic int mhi_ep_driver_remove(struct device *dev)\n{\n\tstruct mhi_ep_device *mhi_dev = to_mhi_ep_device(dev);\n\tstruct mhi_ep_driver *mhi_drv = to_mhi_ep_driver(dev->driver);\n\tstruct mhi_result result = {};\n\tstruct mhi_ep_chan *mhi_chan;\n\tint dir;\n\n\t \n\tif (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)\n\t\treturn 0;\n\n\t \n\tfor (dir = 0; dir < 2; dir++) {\n\t\tmhi_chan = dir ? mhi_dev->ul_chan : mhi_dev->dl_chan;\n\n\t\tif (!mhi_chan)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&mhi_chan->lock);\n\t\t \n\t\tif (mhi_chan->xfer_cb) {\n\t\t\tresult.transaction_status = -ENOTCONN;\n\t\t\tresult.bytes_xferd = 0;\n\t\t\tmhi_chan->xfer_cb(mhi_chan->mhi_dev, &result);\n\t\t}\n\n\t\tmhi_chan->state = MHI_CH_STATE_DISABLED;\n\t\tmhi_chan->xfer_cb = NULL;\n\t\tmutex_unlock(&mhi_chan->lock);\n\t}\n\n\t \n\tmhi_drv->remove(mhi_dev);\n\n\treturn 0;\n}\n\nint __mhi_ep_driver_register(struct mhi_ep_driver *mhi_drv, struct module *owner)\n{\n\tstruct device_driver *driver = &mhi_drv->driver;\n\n\tif (!mhi_drv->probe || !mhi_drv->remove)\n\t\treturn -EINVAL;\n\n\t \n\tif (!mhi_drv->ul_xfer_cb || !mhi_drv->dl_xfer_cb)\n\t\treturn -EINVAL;\n\n\tdriver->bus = &mhi_ep_bus_type;\n\tdriver->owner = owner;\n\tdriver->probe = mhi_ep_driver_probe;\n\tdriver->remove = mhi_ep_driver_remove;\n\n\treturn driver_register(driver);\n}\nEXPORT_SYMBOL_GPL(__mhi_ep_driver_register);\n\nvoid mhi_ep_driver_unregister(struct mhi_ep_driver *mhi_drv)\n{\n\tdriver_unregister(&mhi_drv->driver);\n}\nEXPORT_SYMBOL_GPL(mhi_ep_driver_unregister);\n\nstatic int mhi_ep_uevent(const struct device *dev, struct kobj_uevent_env *env)\n{\n\tconst struct mhi_ep_device *mhi_dev = to_mhi_ep_device(dev);\n\n\treturn add_uevent_var(env, \"MODALIAS=\" MHI_EP_DEVICE_MODALIAS_FMT,\n\t\t\t\t\tmhi_dev->name);\n}\n\nstatic int mhi_ep_match(struct device *dev, struct device_driver *drv)\n{\n\tstruct mhi_ep_device *mhi_dev = to_mhi_ep_device(dev);\n\tstruct mhi_ep_driver *mhi_drv = to_mhi_ep_driver(drv);\n\tconst struct mhi_device_id *id;\n\n\t \n\tif (mhi_dev->dev_type == MHI_DEVICE_CONTROLLER)\n\t\treturn 0;\n\n\tfor (id = mhi_drv->id_table; id->chan[0]; id++)\n\t\tif (!strcmp(mhi_dev->name, id->chan)) {\n\t\t\tmhi_dev->id = id;\n\t\t\treturn 1;\n\t\t}\n\n\treturn 0;\n};\n\nstruct bus_type mhi_ep_bus_type = {\n\t.name = \"mhi_ep\",\n\t.dev_name = \"mhi_ep\",\n\t.match = mhi_ep_match,\n\t.uevent = mhi_ep_uevent,\n};\n\nstatic int __init mhi_ep_init(void)\n{\n\treturn bus_register(&mhi_ep_bus_type);\n}\n\nstatic void __exit mhi_ep_exit(void)\n{\n\tbus_unregister(&mhi_ep_bus_type);\n}\n\npostcore_initcall(mhi_ep_init);\nmodule_exit(mhi_ep_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"MHI Bus Endpoint stack\");\nMODULE_AUTHOR(\"Manivannan Sadhasivam <manivannan.sadhasivam@linaro.org>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}