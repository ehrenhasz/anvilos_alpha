{
  "module_name": "system_heap.c",
  "hash_id": "29314f6b093caa0f616abc71205efd3b00f1f64bca7bf928cdc281e878ace3d2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma-buf/heaps/system_heap.c",
  "human_readable_source": "\n \n\n#include <linux/dma-buf.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-heap.h>\n#include <linux/err.h>\n#include <linux/highmem.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\nstatic struct dma_heap *sys_heap;\n\nstruct system_heap_buffer {\n\tstruct dma_heap *heap;\n\tstruct list_head attachments;\n\tstruct mutex lock;\n\tunsigned long len;\n\tstruct sg_table sg_table;\n\tint vmap_cnt;\n\tvoid *vaddr;\n};\n\nstruct dma_heap_attachment {\n\tstruct device *dev;\n\tstruct sg_table *table;\n\tstruct list_head list;\n\tbool mapped;\n};\n\n#define LOW_ORDER_GFP (GFP_HIGHUSER | __GFP_ZERO)\n#define HIGH_ORDER_GFP  (((GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN \\\n\t\t\t\t| __GFP_NORETRY) & ~__GFP_RECLAIM) \\\n\t\t\t\t| __GFP_COMP)\nstatic gfp_t order_flags[] = {HIGH_ORDER_GFP, HIGH_ORDER_GFP, LOW_ORDER_GFP};\n \nstatic const unsigned int orders[] = {8, 4, 0};\n#define NUM_ORDERS ARRAY_SIZE(orders)\n\nstatic struct sg_table *dup_sg_table(struct sg_table *table)\n{\n\tstruct sg_table *new_table;\n\tint ret, i;\n\tstruct scatterlist *sg, *new_sg;\n\n\tnew_table = kzalloc(sizeof(*new_table), GFP_KERNEL);\n\tif (!new_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = sg_alloc_table(new_table, table->orig_nents, GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(new_table);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tnew_sg = new_table->sgl;\n\tfor_each_sgtable_sg(table, sg, i) {\n\t\tsg_set_page(new_sg, sg_page(sg), sg->length, sg->offset);\n\t\tnew_sg = sg_next(new_sg);\n\t}\n\n\treturn new_table;\n}\n\nstatic int system_heap_attach(struct dma_buf *dmabuf,\n\t\t\t      struct dma_buf_attachment *attachment)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\tstruct sg_table *table;\n\n\ta = kzalloc(sizeof(*a), GFP_KERNEL);\n\tif (!a)\n\t\treturn -ENOMEM;\n\n\ttable = dup_sg_table(&buffer->sg_table);\n\tif (IS_ERR(table)) {\n\t\tkfree(a);\n\t\treturn -ENOMEM;\n\t}\n\n\ta->table = table;\n\ta->dev = attachment->dev;\n\tINIT_LIST_HEAD(&a->list);\n\ta->mapped = false;\n\n\tattachment->priv = a;\n\n\tmutex_lock(&buffer->lock);\n\tlist_add(&a->list, &buffer->attachments);\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic void system_heap_detach(struct dma_buf *dmabuf,\n\t\t\t       struct dma_buf_attachment *attachment)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a = attachment->priv;\n\n\tmutex_lock(&buffer->lock);\n\tlist_del(&a->list);\n\tmutex_unlock(&buffer->lock);\n\n\tsg_free_table(a->table);\n\tkfree(a->table);\n\tkfree(a);\n}\n\nstatic struct sg_table *system_heap_map_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct dma_heap_attachment *a = attachment->priv;\n\tstruct sg_table *table = a->table;\n\tint ret;\n\n\tret = dma_map_sgtable(attachment->dev, table, direction, 0);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\ta->mapped = true;\n\treturn table;\n}\n\nstatic void system_heap_unmap_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t      struct sg_table *table,\n\t\t\t\t      enum dma_data_direction direction)\n{\n\tstruct dma_heap_attachment *a = attachment->priv;\n\n\ta->mapped = false;\n\tdma_unmap_sgtable(attachment->dev, table, direction, 0);\n}\n\nstatic int system_heap_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,\n\t\t\t\t\t\tenum dma_data_direction direction)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\n\tmutex_lock(&buffer->lock);\n\n\tif (buffer->vmap_cnt)\n\t\tinvalidate_kernel_vmap_range(buffer->vaddr, buffer->len);\n\n\tlist_for_each_entry(a, &buffer->attachments, list) {\n\t\tif (!a->mapped)\n\t\t\tcontinue;\n\t\tdma_sync_sgtable_for_cpu(a->dev, a->table, direction);\n\t}\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic int system_heap_dma_buf_end_cpu_access(struct dma_buf *dmabuf,\n\t\t\t\t\t      enum dma_data_direction direction)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\n\tmutex_lock(&buffer->lock);\n\n\tif (buffer->vmap_cnt)\n\t\tflush_kernel_vmap_range(buffer->vaddr, buffer->len);\n\n\tlist_for_each_entry(a, &buffer->attachments, list) {\n\t\tif (!a->mapped)\n\t\t\tcontinue;\n\t\tdma_sync_sgtable_for_device(a->dev, a->table, direction);\n\t}\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic int system_heap_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct sg_table *table = &buffer->sg_table;\n\tunsigned long addr = vma->vm_start;\n\tstruct sg_page_iter piter;\n\tint ret;\n\n\tfor_each_sgtable_page(table, &piter, vma->vm_pgoff) {\n\t\tstruct page *page = sg_page_iter_page(&piter);\n\n\t\tret = remap_pfn_range(vma, addr, page_to_pfn(page), PAGE_SIZE,\n\t\t\t\t      vma->vm_page_prot);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\taddr += PAGE_SIZE;\n\t\tif (addr >= vma->vm_end)\n\t\t\treturn 0;\n\t}\n\treturn 0;\n}\n\nstatic void *system_heap_do_vmap(struct system_heap_buffer *buffer)\n{\n\tstruct sg_table *table = &buffer->sg_table;\n\tint npages = PAGE_ALIGN(buffer->len) / PAGE_SIZE;\n\tstruct page **pages = vmalloc(sizeof(struct page *) * npages);\n\tstruct page **tmp = pages;\n\tstruct sg_page_iter piter;\n\tvoid *vaddr;\n\n\tif (!pages)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor_each_sgtable_page(table, &piter, 0) {\n\t\tWARN_ON(tmp - pages >= npages);\n\t\t*tmp++ = sg_page_iter_page(&piter);\n\t}\n\n\tvaddr = vmap(pages, npages, VM_MAP, PAGE_KERNEL);\n\tvfree(pages);\n\n\tif (!vaddr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn vaddr;\n}\n\nstatic int system_heap_vmap(struct dma_buf *dmabuf, struct iosys_map *map)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tvoid *vaddr;\n\tint ret = 0;\n\n\tmutex_lock(&buffer->lock);\n\tif (buffer->vmap_cnt) {\n\t\tbuffer->vmap_cnt++;\n\t\tiosys_map_set_vaddr(map, buffer->vaddr);\n\t\tgoto out;\n\t}\n\n\tvaddr = system_heap_do_vmap(buffer);\n\tif (IS_ERR(vaddr)) {\n\t\tret = PTR_ERR(vaddr);\n\t\tgoto out;\n\t}\n\n\tbuffer->vaddr = vaddr;\n\tbuffer->vmap_cnt++;\n\tiosys_map_set_vaddr(map, buffer->vaddr);\nout:\n\tmutex_unlock(&buffer->lock);\n\n\treturn ret;\n}\n\nstatic void system_heap_vunmap(struct dma_buf *dmabuf, struct iosys_map *map)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\n\tmutex_lock(&buffer->lock);\n\tif (!--buffer->vmap_cnt) {\n\t\tvunmap(buffer->vaddr);\n\t\tbuffer->vaddr = NULL;\n\t}\n\tmutex_unlock(&buffer->lock);\n\tiosys_map_clear(map);\n}\n\nstatic void system_heap_dma_buf_release(struct dma_buf *dmabuf)\n{\n\tstruct system_heap_buffer *buffer = dmabuf->priv;\n\tstruct sg_table *table;\n\tstruct scatterlist *sg;\n\tint i;\n\n\ttable = &buffer->sg_table;\n\tfor_each_sgtable_sg(table, sg, i) {\n\t\tstruct page *page = sg_page(sg);\n\n\t\t__free_pages(page, compound_order(page));\n\t}\n\tsg_free_table(table);\n\tkfree(buffer);\n}\n\nstatic const struct dma_buf_ops system_heap_buf_ops = {\n\t.attach = system_heap_attach,\n\t.detach = system_heap_detach,\n\t.map_dma_buf = system_heap_map_dma_buf,\n\t.unmap_dma_buf = system_heap_unmap_dma_buf,\n\t.begin_cpu_access = system_heap_dma_buf_begin_cpu_access,\n\t.end_cpu_access = system_heap_dma_buf_end_cpu_access,\n\t.mmap = system_heap_mmap,\n\t.vmap = system_heap_vmap,\n\t.vunmap = system_heap_vunmap,\n\t.release = system_heap_dma_buf_release,\n};\n\nstatic struct page *alloc_largest_available(unsigned long size,\n\t\t\t\t\t    unsigned int max_order)\n{\n\tstruct page *page;\n\tint i;\n\n\tfor (i = 0; i < NUM_ORDERS; i++) {\n\t\tif (size <  (PAGE_SIZE << orders[i]))\n\t\t\tcontinue;\n\t\tif (max_order < orders[i])\n\t\t\tcontinue;\n\n\t\tpage = alloc_pages(order_flags[i], orders[i]);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\treturn page;\n\t}\n\treturn NULL;\n}\n\nstatic struct dma_buf *system_heap_allocate(struct dma_heap *heap,\n\t\t\t\t\t    unsigned long len,\n\t\t\t\t\t    unsigned long fd_flags,\n\t\t\t\t\t    unsigned long heap_flags)\n{\n\tstruct system_heap_buffer *buffer;\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\tunsigned long size_remaining = len;\n\tunsigned int max_order = orders[0];\n\tstruct dma_buf *dmabuf;\n\tstruct sg_table *table;\n\tstruct scatterlist *sg;\n\tstruct list_head pages;\n\tstruct page *page, *tmp_page;\n\tint i, ret = -ENOMEM;\n\n\tbuffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&buffer->attachments);\n\tmutex_init(&buffer->lock);\n\tbuffer->heap = heap;\n\tbuffer->len = len;\n\n\tINIT_LIST_HEAD(&pages);\n\ti = 0;\n\twhile (size_remaining > 0) {\n\t\t \n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto free_buffer;\n\t\t}\n\n\t\tpage = alloc_largest_available(size_remaining, max_order);\n\t\tif (!page)\n\t\t\tgoto free_buffer;\n\n\t\tlist_add_tail(&page->lru, &pages);\n\t\tsize_remaining -= page_size(page);\n\t\tmax_order = compound_order(page);\n\t\ti++;\n\t}\n\n\ttable = &buffer->sg_table;\n\tif (sg_alloc_table(table, i, GFP_KERNEL))\n\t\tgoto free_buffer;\n\n\tsg = table->sgl;\n\tlist_for_each_entry_safe(page, tmp_page, &pages, lru) {\n\t\tsg_set_page(sg, page, page_size(page), 0);\n\t\tsg = sg_next(sg);\n\t\tlist_del(&page->lru);\n\t}\n\n\t \n\texp_info.exp_name = dma_heap_get_name(heap);\n\texp_info.ops = &system_heap_buf_ops;\n\texp_info.size = buffer->len;\n\texp_info.flags = fd_flags;\n\texp_info.priv = buffer;\n\tdmabuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(dmabuf)) {\n\t\tret = PTR_ERR(dmabuf);\n\t\tgoto free_pages;\n\t}\n\treturn dmabuf;\n\nfree_pages:\n\tfor_each_sgtable_sg(table, sg, i) {\n\t\tstruct page *p = sg_page(sg);\n\n\t\t__free_pages(p, compound_order(p));\n\t}\n\tsg_free_table(table);\nfree_buffer:\n\tlist_for_each_entry_safe(page, tmp_page, &pages, lru)\n\t\t__free_pages(page, compound_order(page));\n\tkfree(buffer);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic const struct dma_heap_ops system_heap_ops = {\n\t.allocate = system_heap_allocate,\n};\n\nstatic int system_heap_create(void)\n{\n\tstruct dma_heap_export_info exp_info;\n\n\texp_info.name = \"system\";\n\texp_info.ops = &system_heap_ops;\n\texp_info.priv = NULL;\n\n\tsys_heap = dma_heap_add(&exp_info);\n\tif (IS_ERR(sys_heap))\n\t\treturn PTR_ERR(sys_heap);\n\n\treturn 0;\n}\nmodule_init(system_heap_create);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}