{
  "module_name": "cma_heap.c",
  "hash_id": "9c8af62c6bc7f9904c263a535a6ef0fef13e24fea1af45ac5001aeef3abdf590",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma-buf/heaps/cma_heap.c",
  "human_readable_source": "\n \n#include <linux/cma.h>\n#include <linux/dma-buf.h>\n#include <linux/dma-heap.h>\n#include <linux/dma-map-ops.h>\n#include <linux/err.h>\n#include <linux/highmem.h>\n#include <linux/io.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n\nstruct cma_heap {\n\tstruct dma_heap *heap;\n\tstruct cma *cma;\n};\n\nstruct cma_heap_buffer {\n\tstruct cma_heap *heap;\n\tstruct list_head attachments;\n\tstruct mutex lock;\n\tunsigned long len;\n\tstruct page *cma_pages;\n\tstruct page **pages;\n\tpgoff_t pagecount;\n\tint vmap_cnt;\n\tvoid *vaddr;\n};\n\nstruct dma_heap_attachment {\n\tstruct device *dev;\n\tstruct sg_table table;\n\tstruct list_head list;\n\tbool mapped;\n};\n\nstatic int cma_heap_attach(struct dma_buf *dmabuf,\n\t\t\t   struct dma_buf_attachment *attachment)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\tint ret;\n\n\ta = kzalloc(sizeof(*a), GFP_KERNEL);\n\tif (!a)\n\t\treturn -ENOMEM;\n\n\tret = sg_alloc_table_from_pages(&a->table, buffer->pages,\n\t\t\t\t\tbuffer->pagecount, 0,\n\t\t\t\t\tbuffer->pagecount << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (ret) {\n\t\tkfree(a);\n\t\treturn ret;\n\t}\n\n\ta->dev = attachment->dev;\n\tINIT_LIST_HEAD(&a->list);\n\ta->mapped = false;\n\n\tattachment->priv = a;\n\n\tmutex_lock(&buffer->lock);\n\tlist_add(&a->list, &buffer->attachments);\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic void cma_heap_detach(struct dma_buf *dmabuf,\n\t\t\t    struct dma_buf_attachment *attachment)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a = attachment->priv;\n\n\tmutex_lock(&buffer->lock);\n\tlist_del(&a->list);\n\tmutex_unlock(&buffer->lock);\n\n\tsg_free_table(&a->table);\n\tkfree(a);\n}\n\nstatic struct sg_table *cma_heap_map_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t\t     enum dma_data_direction direction)\n{\n\tstruct dma_heap_attachment *a = attachment->priv;\n\tstruct sg_table *table = &a->table;\n\tint ret;\n\n\tret = dma_map_sgtable(attachment->dev, table, direction, 0);\n\tif (ret)\n\t\treturn ERR_PTR(-ENOMEM);\n\ta->mapped = true;\n\treturn table;\n}\n\nstatic void cma_heap_unmap_dma_buf(struct dma_buf_attachment *attachment,\n\t\t\t\t   struct sg_table *table,\n\t\t\t\t   enum dma_data_direction direction)\n{\n\tstruct dma_heap_attachment *a = attachment->priv;\n\n\ta->mapped = false;\n\tdma_unmap_sgtable(attachment->dev, table, direction, 0);\n}\n\nstatic int cma_heap_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,\n\t\t\t\t\t     enum dma_data_direction direction)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\n\tmutex_lock(&buffer->lock);\n\n\tif (buffer->vmap_cnt)\n\t\tinvalidate_kernel_vmap_range(buffer->vaddr, buffer->len);\n\n\tlist_for_each_entry(a, &buffer->attachments, list) {\n\t\tif (!a->mapped)\n\t\t\tcontinue;\n\t\tdma_sync_sgtable_for_cpu(a->dev, &a->table, direction);\n\t}\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic int cma_heap_dma_buf_end_cpu_access(struct dma_buf *dmabuf,\n\t\t\t\t\t   enum dma_data_direction direction)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tstruct dma_heap_attachment *a;\n\n\tmutex_lock(&buffer->lock);\n\n\tif (buffer->vmap_cnt)\n\t\tflush_kernel_vmap_range(buffer->vaddr, buffer->len);\n\n\tlist_for_each_entry(a, &buffer->attachments, list) {\n\t\tif (!a->mapped)\n\t\t\tcontinue;\n\t\tdma_sync_sgtable_for_device(a->dev, &a->table, direction);\n\t}\n\tmutex_unlock(&buffer->lock);\n\n\treturn 0;\n}\n\nstatic vm_fault_t cma_heap_vm_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct cma_heap_buffer *buffer = vma->vm_private_data;\n\n\tif (vmf->pgoff > buffer->pagecount)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = buffer->pages[vmf->pgoff];\n\tget_page(vmf->page);\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct dma_heap_vm_ops = {\n\t.fault = cma_heap_vm_fault,\n};\n\nstatic int cma_heap_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\n\tif ((vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) == 0)\n\t\treturn -EINVAL;\n\n\tvma->vm_ops = &dma_heap_vm_ops;\n\tvma->vm_private_data = buffer;\n\n\treturn 0;\n}\n\nstatic void *cma_heap_do_vmap(struct cma_heap_buffer *buffer)\n{\n\tvoid *vaddr;\n\n\tvaddr = vmap(buffer->pages, buffer->pagecount, VM_MAP, PAGE_KERNEL);\n\tif (!vaddr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn vaddr;\n}\n\nstatic int cma_heap_vmap(struct dma_buf *dmabuf, struct iosys_map *map)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tvoid *vaddr;\n\tint ret = 0;\n\n\tmutex_lock(&buffer->lock);\n\tif (buffer->vmap_cnt) {\n\t\tbuffer->vmap_cnt++;\n\t\tiosys_map_set_vaddr(map, buffer->vaddr);\n\t\tgoto out;\n\t}\n\n\tvaddr = cma_heap_do_vmap(buffer);\n\tif (IS_ERR(vaddr)) {\n\t\tret = PTR_ERR(vaddr);\n\t\tgoto out;\n\t}\n\tbuffer->vaddr = vaddr;\n\tbuffer->vmap_cnt++;\n\tiosys_map_set_vaddr(map, buffer->vaddr);\nout:\n\tmutex_unlock(&buffer->lock);\n\n\treturn ret;\n}\n\nstatic void cma_heap_vunmap(struct dma_buf *dmabuf, struct iosys_map *map)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\n\tmutex_lock(&buffer->lock);\n\tif (!--buffer->vmap_cnt) {\n\t\tvunmap(buffer->vaddr);\n\t\tbuffer->vaddr = NULL;\n\t}\n\tmutex_unlock(&buffer->lock);\n\tiosys_map_clear(map);\n}\n\nstatic void cma_heap_dma_buf_release(struct dma_buf *dmabuf)\n{\n\tstruct cma_heap_buffer *buffer = dmabuf->priv;\n\tstruct cma_heap *cma_heap = buffer->heap;\n\n\tif (buffer->vmap_cnt > 0) {\n\t\tWARN(1, \"%s: buffer still mapped in the kernel\\n\", __func__);\n\t\tvunmap(buffer->vaddr);\n\t\tbuffer->vaddr = NULL;\n\t}\n\n\t \n\tkfree(buffer->pages);\n\t \n\tcma_release(cma_heap->cma, buffer->cma_pages, buffer->pagecount);\n\tkfree(buffer);\n}\n\nstatic const struct dma_buf_ops cma_heap_buf_ops = {\n\t.attach = cma_heap_attach,\n\t.detach = cma_heap_detach,\n\t.map_dma_buf = cma_heap_map_dma_buf,\n\t.unmap_dma_buf = cma_heap_unmap_dma_buf,\n\t.begin_cpu_access = cma_heap_dma_buf_begin_cpu_access,\n\t.end_cpu_access = cma_heap_dma_buf_end_cpu_access,\n\t.mmap = cma_heap_mmap,\n\t.vmap = cma_heap_vmap,\n\t.vunmap = cma_heap_vunmap,\n\t.release = cma_heap_dma_buf_release,\n};\n\nstatic struct dma_buf *cma_heap_allocate(struct dma_heap *heap,\n\t\t\t\t\t unsigned long len,\n\t\t\t\t\t unsigned long fd_flags,\n\t\t\t\t\t unsigned long heap_flags)\n{\n\tstruct cma_heap *cma_heap = dma_heap_get_drvdata(heap);\n\tstruct cma_heap_buffer *buffer;\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\tsize_t size = PAGE_ALIGN(len);\n\tpgoff_t pagecount = size >> PAGE_SHIFT;\n\tunsigned long align = get_order(size);\n\tstruct page *cma_pages;\n\tstruct dma_buf *dmabuf;\n\tint ret = -ENOMEM;\n\tpgoff_t pg;\n\n\tbuffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&buffer->attachments);\n\tmutex_init(&buffer->lock);\n\tbuffer->len = size;\n\n\tif (align > CONFIG_CMA_ALIGNMENT)\n\t\talign = CONFIG_CMA_ALIGNMENT;\n\n\tcma_pages = cma_alloc(cma_heap->cma, pagecount, align, false);\n\tif (!cma_pages)\n\t\tgoto free_buffer;\n\n\t \n\tif (PageHighMem(cma_pages)) {\n\t\tunsigned long nr_clear_pages = pagecount;\n\t\tstruct page *page = cma_pages;\n\n\t\twhile (nr_clear_pages > 0) {\n\t\t\tvoid *vaddr = kmap_atomic(page);\n\n\t\t\tmemset(vaddr, 0, PAGE_SIZE);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\t \n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\tgoto free_cma;\n\t\t\tpage++;\n\t\t\tnr_clear_pages--;\n\t\t}\n\t} else {\n\t\tmemset(page_address(cma_pages), 0, size);\n\t}\n\n\tbuffer->pages = kmalloc_array(pagecount, sizeof(*buffer->pages), GFP_KERNEL);\n\tif (!buffer->pages) {\n\t\tret = -ENOMEM;\n\t\tgoto free_cma;\n\t}\n\n\tfor (pg = 0; pg < pagecount; pg++)\n\t\tbuffer->pages[pg] = &cma_pages[pg];\n\n\tbuffer->cma_pages = cma_pages;\n\tbuffer->heap = cma_heap;\n\tbuffer->pagecount = pagecount;\n\n\t \n\texp_info.exp_name = dma_heap_get_name(heap);\n\texp_info.ops = &cma_heap_buf_ops;\n\texp_info.size = buffer->len;\n\texp_info.flags = fd_flags;\n\texp_info.priv = buffer;\n\tdmabuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(dmabuf)) {\n\t\tret = PTR_ERR(dmabuf);\n\t\tgoto free_pages;\n\t}\n\treturn dmabuf;\n\nfree_pages:\n\tkfree(buffer->pages);\nfree_cma:\n\tcma_release(cma_heap->cma, cma_pages, pagecount);\nfree_buffer:\n\tkfree(buffer);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic const struct dma_heap_ops cma_heap_ops = {\n\t.allocate = cma_heap_allocate,\n};\n\nstatic int __add_cma_heap(struct cma *cma, void *data)\n{\n\tstruct cma_heap *cma_heap;\n\tstruct dma_heap_export_info exp_info;\n\n\tcma_heap = kzalloc(sizeof(*cma_heap), GFP_KERNEL);\n\tif (!cma_heap)\n\t\treturn -ENOMEM;\n\tcma_heap->cma = cma;\n\n\texp_info.name = cma_get_name(cma);\n\texp_info.ops = &cma_heap_ops;\n\texp_info.priv = cma_heap;\n\n\tcma_heap->heap = dma_heap_add(&exp_info);\n\tif (IS_ERR(cma_heap->heap)) {\n\t\tint ret = PTR_ERR(cma_heap->heap);\n\n\t\tkfree(cma_heap);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int add_default_cma_heap(void)\n{\n\tstruct cma *default_cma = dev_get_cma_area(NULL);\n\tint ret = 0;\n\n\tif (default_cma)\n\t\tret = __add_cma_heap(default_cma, NULL);\n\n\treturn ret;\n}\nmodule_init(add_default_cma_heap);\nMODULE_DESCRIPTION(\"DMA-BUF CMA Heap\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}