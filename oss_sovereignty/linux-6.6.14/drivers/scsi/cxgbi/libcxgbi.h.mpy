{
  "module_name": "libcxgbi.h",
  "hash_id": "c15afcd4f804a7b84d095912063e5eec1357859ee5465bc27bcea42080a011d7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/cxgbi/libcxgbi.h",
  "human_readable_source": " \n\n#ifndef\t__LIBCXGBI_H__\n#define\t__LIBCXGBI_H__\n\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/debugfs.h>\n#include <linux/list.h>\n#include <linux/netdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/scatterlist.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <scsi/scsi_device.h>\n#include <scsi/libiscsi_tcp.h>\n\n#include <libcxgb_ppm.h>\n\nenum cxgbi_dbg_flag {\n\tCXGBI_DBG_ISCSI,\n\tCXGBI_DBG_DDP,\n\tCXGBI_DBG_TOE,\n\tCXGBI_DBG_SOCK,\n\n\tCXGBI_DBG_PDU_TX,\n\tCXGBI_DBG_PDU_RX,\n\tCXGBI_DBG_DEV,\n};\n\n#define log_debug(level, fmt, ...)\t\\\n\tdo {\t\\\n\t\tif (dbg_level & (level)) \\\n\t\t\tpr_info(fmt, ##__VA_ARGS__); \\\n\t} while (0)\n\n#define pr_info_ipaddr(fmt_trail,\t\t\t\t\t\\\n\t\t\taddr1, addr2, args_trail...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (!((1 << CXGBI_DBG_SOCK) & dbg_level))\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tpr_info(\"%pISpc - %pISpc, \" fmt_trail,\t\t\t\t\\\n\t\taddr1, addr2, args_trail);\t\t\t\t\\\n} while (0)\n\n \n#define CXGBI_MAX_CONN\t\t16384\n\n \n#define SKB_TX_ISCSI_PDU_HEADER_MAX\t\\\n\t(sizeof(struct iscsi_hdr) + ISCSI_MAX_AHS_SIZE)\n\n#define\tISCSI_PDU_NONPAYLOAD_LEN\t312  \n\n \n#define cxgbi_align_pdu_size(n) do { n = (n) & (~511); } while (0)\n\n#define ULP2_MODE_ISCSI\t\t2\n\n#define ULP2_MAX_PKT_SIZE\t16224\n#define ULP2_MAX_PDU_PAYLOAD\t\\\n\t(ULP2_MAX_PKT_SIZE - ISCSI_PDU_NONPAYLOAD_LEN)\n\n#define CXGBI_ULP2_MAX_ISO_PAYLOAD\t65535\n\n#define CXGBI_MAX_ISO_DATA_IN_SKB\t\\\n\tmin_t(u32, MAX_SKB_FRAGS << PAGE_SHIFT, CXGBI_ULP2_MAX_ISO_PAYLOAD)\n\n#define cxgbi_is_iso_config(csk)\t((csk)->cdev->skb_iso_txhdr)\n#define cxgbi_is_iso_disabled(csk)\t((csk)->disable_iso)\n\n \nstatic const unsigned int ulp2_extra_len[] = { 0, 4, 4, 8 };\nstatic inline unsigned int cxgbi_ulp_extra_len(int submode)\n{\n\treturn ulp2_extra_len[submode & 3];\n}\n\n#define CPL_RX_DDP_STATUS_DDP_SHIFT\t16  \n#define CPL_RX_DDP_STATUS_PAD_SHIFT\t19  \n#define CPL_RX_DDP_STATUS_HCRC_SHIFT\t20  \n#define CPL_RX_DDP_STATUS_DCRC_SHIFT\t21  \n\n \nstruct sge_opaque_hdr {\n\tvoid *dev;\n\tdma_addr_t addr[MAX_SKB_FRAGS + 1];\n};\n\nstruct cxgbi_sock {\n\tstruct cxgbi_device *cdev;\n\n\tint tid;\n\tint atid;\n\tunsigned long flags;\n\tunsigned int mtu;\n\tunsigned short rss_qid;\n\tunsigned short txq_idx;\n\tunsigned short advmss;\n\tunsigned int tx_chan;\n\tunsigned int rx_chan;\n\tunsigned int mss_idx;\n\tunsigned int smac_idx;\n\tunsigned char port_id;\n\tint wr_max_cred;\n\tint wr_cred;\n\tint wr_una_cred;\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tu8 dcb_priority;\n#endif\n\tunsigned char hcrc_len;\n\tunsigned char dcrc_len;\n\n\tvoid *l2t;\n\tstruct sk_buff *wr_pending_head;\n\tstruct sk_buff *wr_pending_tail;\n\tstruct sk_buff *cpl_close;\n\tstruct sk_buff *cpl_abort_req;\n\tstruct sk_buff *cpl_abort_rpl;\n\tstruct sk_buff *skb_ulp_lhdr;\n\tspinlock_t lock;\n\tstruct kref refcnt;\n\tunsigned int state;\n\tunsigned int csk_family;\n\tunion {\n\t\tstruct sockaddr_in saddr;\n\t\tstruct sockaddr_in6 saddr6;\n\t};\n\tunion {\n\t\tstruct sockaddr_in daddr;\n\t\tstruct sockaddr_in6 daddr6;\n\t};\n\tstruct dst_entry *dst;\n\tstruct sk_buff_head receive_queue;\n\tstruct sk_buff_head write_queue;\n\tstruct timer_list retry_timer;\n\tstruct completion cmpl;\n\tint err;\n\trwlock_t callback_lock;\n\tvoid *user_data;\n\n\tu32 rcv_nxt;\n\tu32 copied_seq;\n\tu32 rcv_wup;\n\tu32 snd_nxt;\n\tu32 snd_una;\n\tu32 write_seq;\n\tu32 snd_win;\n\tu32 rcv_win;\n\n\tbool disable_iso;\n\tu32 no_tx_credits;\n\tunsigned long prev_iso_ts;\n};\n\n \nenum cxgbi_sock_states{\n\tCTP_CLOSED,\n\tCTP_CONNECTING,\n\tCTP_ACTIVE_OPEN,\n\tCTP_ESTABLISHED,\n\tCTP_ACTIVE_CLOSE,\n\tCTP_PASSIVE_CLOSE,\n\tCTP_CLOSE_WAIT_1,\n\tCTP_CLOSE_WAIT_2,\n\tCTP_ABORTING,\n};\n\n \nenum cxgbi_sock_flags {\n\tCTPF_ABORT_RPL_RCVD,\t \n\tCTPF_ABORT_REQ_RCVD,\t \n\tCTPF_ABORT_RPL_PENDING,\t \n\tCTPF_TX_DATA_SENT,\t \n\tCTPF_ACTIVE_CLOSE_NEEDED, \n\tCTPF_HAS_ATID,\t\t \n\tCTPF_HAS_TID,\t\t \n\tCTPF_OFFLOAD_DOWN,\t \n\tCTPF_LOGOUT_RSP_RCVD,    \n};\n\nstruct cxgbi_skb_rx_cb {\n\t__u32 ddigest;\n\t__u32 pdulen;\n};\n\nstruct cxgbi_skb_tx_cb {\n\tvoid *handle;\n\tvoid *arp_err_handler;\n\tstruct sk_buff *wr_next;\n\tu16 iscsi_hdr_len;\n\tu8 ulp_mode;\n};\n\nenum cxgbi_skcb_flags {\n\tSKCBF_TX_NEED_HDR,\t \n\tSKCBF_TX_MEM_WRITE,      \n\tSKCBF_TX_FLAG_COMPL,     \n\tSKCBF_RX_COALESCED,\t \n\tSKCBF_RX_HDR,\t\t \n\tSKCBF_RX_DATA,\t\t \n\tSKCBF_RX_STATUS,\t \n\tSKCBF_RX_ISCSI_COMPL,    \n\tSKCBF_RX_DATA_DDPD,\t \n\tSKCBF_RX_HCRC_ERR,\t \n\tSKCBF_RX_DCRC_ERR,\t \n\tSKCBF_RX_PAD_ERR,\t \n\tSKCBF_TX_ISO,\t\t \n};\n\nstruct cxgbi_skb_cb {\n\tunion {\n\t\tstruct cxgbi_skb_rx_cb rx;\n\t\tstruct cxgbi_skb_tx_cb tx;\n\t};\n\tunsigned long flags;\n\tunsigned int seq;\n};\n\n#define CXGBI_SKB_CB(skb)\t((struct cxgbi_skb_cb *)&((skb)->cb[0]))\n#define cxgbi_skcb_flags(skb)\t\t(CXGBI_SKB_CB(skb)->flags)\n#define cxgbi_skcb_tcp_seq(skb)\t\t(CXGBI_SKB_CB(skb)->seq)\n#define cxgbi_skcb_rx_ddigest(skb)\t(CXGBI_SKB_CB(skb)->rx.ddigest)\n#define cxgbi_skcb_rx_pdulen(skb)\t(CXGBI_SKB_CB(skb)->rx.pdulen)\n#define cxgbi_skcb_tx_wr_next(skb)\t(CXGBI_SKB_CB(skb)->tx.wr_next)\n#define cxgbi_skcb_tx_iscsi_hdrlen(skb)\t(CXGBI_SKB_CB(skb)->tx.iscsi_hdr_len)\n#define cxgbi_skcb_tx_ulp_mode(skb)\t(CXGBI_SKB_CB(skb)->tx.ulp_mode)\n\nstatic inline void cxgbi_skcb_set_flag(struct sk_buff *skb,\n\t\t\t\t\tenum cxgbi_skcb_flags flag)\n{\n\t__set_bit(flag, &(cxgbi_skcb_flags(skb)));\n}\n\nstatic inline void cxgbi_skcb_clear_flag(struct sk_buff *skb,\n\t\t\t\t\tenum cxgbi_skcb_flags flag)\n{\n\t__clear_bit(flag, &(cxgbi_skcb_flags(skb)));\n}\n\nstatic inline int cxgbi_skcb_test_flag(const struct sk_buff *skb,\n\t\t\t\t       enum cxgbi_skcb_flags flag)\n{\n\treturn test_bit(flag, &(cxgbi_skcb_flags(skb)));\n}\n\nstatic inline void cxgbi_sock_set_flag(struct cxgbi_sock *csk,\n\t\t\t\t\tenum cxgbi_sock_flags flag)\n{\n\t__set_bit(flag, &csk->flags);\n\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\"csk 0x%p,%u,0x%lx, bit %d.\\n\",\n\t\tcsk, csk->state, csk->flags, flag);\n}\n\nstatic inline void cxgbi_sock_clear_flag(struct cxgbi_sock *csk,\n\t\t\t\t\tenum cxgbi_sock_flags flag)\n{\n\t__clear_bit(flag, &csk->flags);\n\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\"csk 0x%p,%u,0x%lx, bit %d.\\n\",\n\t\tcsk, csk->state, csk->flags, flag);\n}\n\nstatic inline int cxgbi_sock_flag(struct cxgbi_sock *csk,\n\t\t\t\tenum cxgbi_sock_flags flag)\n{\n\tif (csk == NULL)\n\t\treturn 0;\n\treturn test_bit(flag, &csk->flags);\n}\n\nstatic inline void cxgbi_sock_set_state(struct cxgbi_sock *csk, int state)\n{\n\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\"csk 0x%p,%u,0x%lx, state -> %u.\\n\",\n\t\tcsk, csk->state, csk->flags, state);\n\tcsk->state = state;\n}\n\nstatic inline void cxgbi_sock_free(struct kref *kref)\n{\n\tstruct cxgbi_sock *csk = container_of(kref,\n\t\t\t\t\t\tstruct cxgbi_sock,\n\t\t\t\t\t\trefcnt);\n\tif (csk) {\n\t\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\t\"free csk 0x%p, state %u, flags 0x%lx\\n\",\n\t\t\tcsk, csk->state, csk->flags);\n\t\tkfree(csk);\n\t}\n}\n\nstatic inline void __cxgbi_sock_put(const char *fn, struct cxgbi_sock *csk)\n{\n\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\"%s, put csk 0x%p, ref %u-1.\\n\",\n\t\tfn, csk, kref_read(&csk->refcnt));\n\tkref_put(&csk->refcnt, cxgbi_sock_free);\n}\n#define cxgbi_sock_put(csk)\t__cxgbi_sock_put(__func__, csk)\n\nstatic inline void __cxgbi_sock_get(const char *fn, struct cxgbi_sock *csk)\n{\n\tlog_debug(1 << CXGBI_DBG_SOCK,\n\t\t\"%s, get csk 0x%p, ref %u+1.\\n\",\n\t\tfn, csk, kref_read(&csk->refcnt));\n\tkref_get(&csk->refcnt);\n}\n#define cxgbi_sock_get(csk)\t__cxgbi_sock_get(__func__, csk)\n\nstatic inline int cxgbi_sock_is_closing(struct cxgbi_sock *csk)\n{\n\treturn csk->state >= CTP_ACTIVE_CLOSE;\n}\n\nstatic inline int cxgbi_sock_is_established(struct cxgbi_sock *csk)\n{\n\treturn csk->state == CTP_ESTABLISHED;\n}\n\nstatic inline void cxgbi_sock_purge_write_queue(struct cxgbi_sock *csk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&csk->write_queue)))\n\t\t__kfree_skb(skb);\n}\n\nstatic inline unsigned int cxgbi_sock_compute_wscale(unsigned int win)\n{\n\tunsigned int wscale = 0;\n\n\twhile (wscale < 14 && (65535 << wscale) < win)\n\t\twscale++;\n\treturn wscale;\n}\n\nstatic inline struct sk_buff *alloc_wr(int wrlen, int dlen, gfp_t gfp)\n{\n\tstruct sk_buff *skb = alloc_skb(wrlen + dlen, gfp);\n\n\tif (skb) {\n\t\t__skb_put(skb, wrlen);\n\t\tmemset(skb->head, 0, wrlen + dlen);\n\t} else\n\t\tpr_info(\"alloc cpl wr skb %u+%u, OOM.\\n\", wrlen, dlen);\n\treturn skb;\n}\n\n\n \n#define SKB_WR_LIST_SIZE\t (MAX_SKB_FRAGS + 2)\n\nstatic inline void cxgbi_sock_reset_wr_list(struct cxgbi_sock *csk)\n{\n\tcsk->wr_pending_head = csk->wr_pending_tail = NULL;\n}\n\nstatic inline void cxgbi_sock_enqueue_wr(struct cxgbi_sock *csk,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\tcxgbi_skcb_tx_wr_next(skb) = NULL;\n\t \n\tskb_get(skb);\n\n\tif (!csk->wr_pending_head)\n\t\tcsk->wr_pending_head = skb;\n\telse\n\t\tcxgbi_skcb_tx_wr_next(csk->wr_pending_tail) = skb;\n\tcsk->wr_pending_tail = skb;\n}\n\nstatic inline int cxgbi_sock_count_pending_wrs(const struct cxgbi_sock *csk)\n{\n\tint n = 0;\n\tconst struct sk_buff *skb = csk->wr_pending_head;\n\n\twhile (skb) {\n\t\tn += skb->csum;\n\t\tskb = cxgbi_skcb_tx_wr_next(skb);\n\t}\n\treturn n;\n}\n\nstatic inline struct sk_buff *cxgbi_sock_peek_wr(const struct cxgbi_sock *csk)\n{\n\treturn csk->wr_pending_head;\n}\n\nstatic inline struct sk_buff *cxgbi_sock_dequeue_wr(struct cxgbi_sock *csk)\n{\n\tstruct sk_buff *skb = csk->wr_pending_head;\n\n\tif (likely(skb)) {\n\t\tcsk->wr_pending_head = cxgbi_skcb_tx_wr_next(skb);\n\t\tcxgbi_skcb_tx_wr_next(skb) = NULL;\n\t}\n\treturn skb;\n}\n\nvoid cxgbi_sock_check_wr_invariants(const struct cxgbi_sock *);\nvoid cxgbi_sock_purge_wr_queue(struct cxgbi_sock *);\nvoid cxgbi_sock_skb_entail(struct cxgbi_sock *, struct sk_buff *);\nvoid cxgbi_sock_fail_act_open(struct cxgbi_sock *, int);\nvoid cxgbi_sock_act_open_req_arp_failure(void *, struct sk_buff *);\nvoid cxgbi_sock_closed(struct cxgbi_sock *);\nvoid cxgbi_sock_established(struct cxgbi_sock *, unsigned int, unsigned int);\nvoid cxgbi_sock_rcv_abort_rpl(struct cxgbi_sock *);\nvoid cxgbi_sock_rcv_peer_close(struct cxgbi_sock *);\nvoid cxgbi_sock_rcv_close_conn_rpl(struct cxgbi_sock *, u32);\nvoid cxgbi_sock_rcv_wr_ack(struct cxgbi_sock *, unsigned int, unsigned int,\n\t\t\t\tint);\nunsigned int cxgbi_sock_select_mss(struct cxgbi_sock *, unsigned int);\nvoid cxgbi_sock_free_cpl_skbs(struct cxgbi_sock *);\n\nstruct cxgbi_hba {\n\tstruct net_device *ndev;\n\tstruct net_device *vdev;\t \n\tstruct Scsi_Host *shost;\n\tstruct cxgbi_device *cdev;\n\t__be32 ipv4addr;\n\tunsigned char port_id;\n};\n\nstruct cxgbi_ports_map {\n\tunsigned int max_connect;\n\tunsigned int used;\n\tunsigned short sport_base;\n\tspinlock_t lock;\n\tunsigned int next;\n\tstruct cxgbi_sock **port_csk;\n};\n\n#define CXGBI_FLAG_DEV_T3\t\t0x1\n#define CXGBI_FLAG_DEV_T4\t\t0x2\n#define CXGBI_FLAG_ADAPTER_RESET\t0x4\n#define CXGBI_FLAG_IPV4_SET\t\t0x10\n#define CXGBI_FLAG_USE_PPOD_OFLDQ       0x40\n#define CXGBI_FLAG_DDP_OFF\t\t0x100\n#define CXGBI_FLAG_DEV_ISO_OFF\t\t0x400\n\nstruct cxgbi_device {\n\tstruct list_head list_head;\n\tstruct list_head rcu_node;\n\tunsigned int flags;\n\tstruct net_device **ports;\n\tvoid *lldev;\n\tstruct cxgbi_hba **hbas;\n\tconst unsigned short *mtus;\n\tunsigned char nmtus;\n\tunsigned char nports;\n\tstruct pci_dev *pdev;\n\tstruct dentry *debugfs_root;\n\tstruct iscsi_transport *itp;\n\tstruct module *owner;\n\n\tunsigned int pfvf;\n\tunsigned int rx_credit_thres;\n\tunsigned int skb_tx_rsvd;\n\tu32 skb_iso_txhdr;\n\tunsigned int skb_rx_extra;\t \n\tunsigned int tx_max_size;\n\tunsigned int rx_max_size;\n\tunsigned int rxq_idx_cntr;\n\tstruct cxgbi_ports_map pmap;\n\n\tvoid (*dev_ddp_cleanup)(struct cxgbi_device *);\n\tstruct cxgbi_ppm* (*cdev2ppm)(struct cxgbi_device *);\n\tint (*csk_ddp_set_map)(struct cxgbi_ppm *, struct cxgbi_sock *,\n\t\t\t       struct cxgbi_task_tag_info *);\n\tvoid (*csk_ddp_clear_map)(struct cxgbi_device *cdev,\n\t\t\t\t  struct cxgbi_ppm *,\n\t\t\t\t  struct cxgbi_task_tag_info *);\n\tint (*csk_ddp_setup_digest)(struct cxgbi_sock *,\n\t\t\t\t    unsigned int, int, int);\n\tint (*csk_ddp_setup_pgidx)(struct cxgbi_sock *,\n\t\t\t\t   unsigned int, int);\n\n\tvoid (*csk_release_offload_resources)(struct cxgbi_sock *);\n\tint (*csk_rx_pdu_ready)(struct cxgbi_sock *, struct sk_buff *);\n\tu32 (*csk_send_rx_credits)(struct cxgbi_sock *, u32);\n\tint (*csk_push_tx_frames)(struct cxgbi_sock *, int);\n\tvoid (*csk_send_abort_req)(struct cxgbi_sock *);\n\tvoid (*csk_send_close_req)(struct cxgbi_sock *);\n\tint (*csk_alloc_cpls)(struct cxgbi_sock *);\n\tint (*csk_init_act_open)(struct cxgbi_sock *);\n\n\tvoid *dd_data;\n};\n#define cxgbi_cdev_priv(cdev)\t((cdev)->dd_data)\n\nstruct cxgbi_conn {\n\tstruct cxgbi_endpoint *cep;\n\tstruct iscsi_conn *iconn;\n\tstruct cxgbi_hba *chba;\n\tu32 task_idx_bits;\n\tunsigned int ddp_full;\n\tunsigned int ddp_tag_full;\n};\n\nstruct cxgbi_endpoint {\n\tstruct cxgbi_conn *cconn;\n\tstruct cxgbi_hba *chba;\n\tstruct cxgbi_sock *csk;\n};\n\nstruct cxgbi_task_data {\n#define CXGBI_TASK_SGL_CHECKED\t0x1\n#define CXGBI_TASK_SGL_COPY\t0x2\n\tu8 flags;\n\tunsigned short nr_frags;\n\tstruct page_frag frags[MAX_SKB_FRAGS];\n\tstruct sk_buff *skb;\n\tunsigned int dlen;\n\tunsigned int offset;\n\tunsigned int count;\n\tunsigned int sgoffset;\n\tu32 total_count;\n\tu32 total_offset;\n\tu32 max_xmit_dlength;\n\tstruct cxgbi_task_tag_info ttinfo;\n};\n#define iscsi_task_cxgbi_data(task) \\\n\t((task)->dd_data + sizeof(struct iscsi_tcp_task))\n\nstruct cxgbi_iso_info {\n#define CXGBI_ISO_INFO_FSLICE\t\t0x1\n#define CXGBI_ISO_INFO_LSLICE\t\t0x2\n#define CXGBI_ISO_INFO_IMM_ENABLE\t0x4\n\tu8 flags;\n\tu8 op;\n\tu8 ahs;\n\tu8 num_pdu;\n\tu32 mpdu;\n\tu32 burst_size;\n\tu32 len;\n\tu32 segment_offset;\n\tu32 datasn_offset;\n\tu32 buffer_offset;\n};\n\nstatic inline void cxgbi_set_iscsi_ipv4(struct cxgbi_hba *chba, __be32 ipaddr)\n{\n\tif (chba->cdev->flags & CXGBI_FLAG_IPV4_SET)\n\t\tchba->ipv4addr = ipaddr;\n\telse\n\t\tpr_info(\"set iscsi ipv4 NOT supported, using %s ipv4.\\n\",\n\t\t\tchba->ndev->name);\n}\n\nstruct cxgbi_device *cxgbi_device_register(unsigned int, unsigned int);\nvoid cxgbi_device_unregister(struct cxgbi_device *);\nvoid cxgbi_device_unregister_all(unsigned int flag);\nstruct cxgbi_device *cxgbi_device_find_by_lldev(void *);\nstruct cxgbi_device *cxgbi_device_find_by_netdev(struct net_device *, int *);\nstruct cxgbi_device *cxgbi_device_find_by_netdev_rcu(struct net_device *,\n\t\t\t\t\t\t     int *);\nint cxgbi_hbas_add(struct cxgbi_device *, u64, unsigned int,\n\t\t\tconst struct scsi_host_template *,\n\t\t\tstruct scsi_transport_template *);\nvoid cxgbi_hbas_remove(struct cxgbi_device *);\n\nint cxgbi_device_portmap_create(struct cxgbi_device *cdev, unsigned int base,\n\t\t\tunsigned int max_conn);\nvoid cxgbi_device_portmap_cleanup(struct cxgbi_device *cdev);\n\nvoid cxgbi_conn_tx_open(struct cxgbi_sock *);\nvoid cxgbi_conn_pdu_ready(struct cxgbi_sock *);\nint cxgbi_conn_alloc_pdu(struct iscsi_task *, u8);\nint cxgbi_conn_init_pdu(struct iscsi_task *, unsigned int , unsigned int);\nint cxgbi_conn_xmit_pdu(struct iscsi_task *);\n\nvoid cxgbi_cleanup_task(struct iscsi_task *task);\n\numode_t cxgbi_attr_is_visible(int param_type, int param);\nvoid cxgbi_get_conn_stats(struct iscsi_cls_conn *, struct iscsi_stats *);\nint cxgbi_set_conn_param(struct iscsi_cls_conn *,\n\t\t\tenum iscsi_param, char *, int);\nint cxgbi_get_ep_param(struct iscsi_endpoint *ep, enum iscsi_param, char *);\nstruct iscsi_cls_conn *cxgbi_create_conn(struct iscsi_cls_session *, u32);\nint cxgbi_bind_conn(struct iscsi_cls_session *,\n\t\t\tstruct iscsi_cls_conn *, u64, int);\nvoid cxgbi_destroy_session(struct iscsi_cls_session *);\nstruct iscsi_cls_session *cxgbi_create_session(struct iscsi_endpoint *,\n\t\t\tu16, u16, u32);\nint cxgbi_set_host_param(struct Scsi_Host *,\n\t\t\tenum iscsi_host_param, char *, int);\nint cxgbi_get_host_param(struct Scsi_Host *, enum iscsi_host_param, char *);\nstruct iscsi_endpoint *cxgbi_ep_connect(struct Scsi_Host *,\n\t\t\tstruct sockaddr *, int);\nint cxgbi_ep_poll(struct iscsi_endpoint *, int);\nvoid cxgbi_ep_disconnect(struct iscsi_endpoint *);\n\nint cxgbi_iscsi_init(struct iscsi_transport *,\n\t\t\tstruct scsi_transport_template **);\nvoid cxgbi_iscsi_cleanup(struct iscsi_transport *,\n\t\t\tstruct scsi_transport_template **);\nvoid cxgbi_parse_pdu_itt(struct iscsi_conn *, itt_t, int *, int *);\nint cxgbi_ddp_init(struct cxgbi_device *, unsigned int, unsigned int,\n\t\t\tunsigned int, unsigned int);\nint cxgbi_ddp_cleanup(struct cxgbi_device *);\nvoid cxgbi_ddp_page_size_factor(int *);\nvoid cxgbi_ddp_set_one_ppod(struct cxgbi_pagepod *,\n\t\t\t    struct cxgbi_task_tag_info *,\n\t\t\t    struct scatterlist **sg_pp, unsigned int *sg_off);\nint cxgbi_ddp_ppm_setup(void **ppm_pp, struct cxgbi_device *cdev,\n\t\t\tstruct cxgbi_tag_format *tformat,\n\t\t\tunsigned int iscsi_size, unsigned int llimit,\n\t\t\tunsigned int start, unsigned int rsvd_factor,\n\t\t\tunsigned int edram_start, unsigned int edram_size);\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}