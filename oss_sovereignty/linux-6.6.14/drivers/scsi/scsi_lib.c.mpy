{
  "module_name": "scsi_lib.c",
  "hash_id": "aa9717a737ed37f4680b7f7deee3f125294d4b85a0b8e451cfc8e124c95163d7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/scsi_lib.c",
  "human_readable_source": "\n \n\n#include <linux/bio.h>\n#include <linux/bitops.h>\n#include <linux/blkdev.h>\n#include <linux/completion.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/scatterlist.h>\n#include <linux/blk-mq.h>\n#include <linux/blk-integrity.h>\n#include <linux/ratelimit.h>\n#include <asm/unaligned.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_transport.h>  \n#include <scsi/scsi_dh.h>\n\n#include <trace/events/scsi.h>\n\n#include \"scsi_debugfs.h\"\n#include \"scsi_priv.h\"\n#include \"scsi_logging.h\"\n\n \n#ifdef CONFIG_ARCH_NO_SG_CHAIN\n#define  SCSI_INLINE_PROT_SG_CNT  0\n#define  SCSI_INLINE_SG_CNT  0\n#else\n#define  SCSI_INLINE_PROT_SG_CNT  1\n#define  SCSI_INLINE_SG_CNT  2\n#endif\n\nstatic struct kmem_cache *scsi_sense_cache;\nstatic DEFINE_MUTEX(scsi_sense_cache_mutex);\n\nstatic void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd);\n\nint scsi_init_sense_cache(struct Scsi_Host *shost)\n{\n\tint ret = 0;\n\n\tmutex_lock(&scsi_sense_cache_mutex);\n\tif (!scsi_sense_cache) {\n\t\tscsi_sense_cache =\n\t\t\tkmem_cache_create_usercopy(\"scsi_sense_cache\",\n\t\t\t\tSCSI_SENSE_BUFFERSIZE, 0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t0, SCSI_SENSE_BUFFERSIZE, NULL);\n\t\tif (!scsi_sense_cache)\n\t\t\tret = -ENOMEM;\n\t}\n\tmutex_unlock(&scsi_sense_cache_mutex);\n\treturn ret;\n}\n\nstatic void\nscsi_set_blocked(struct scsi_cmnd *cmd, int reason)\n{\n\tstruct Scsi_Host *host = cmd->device->host;\n\tstruct scsi_device *device = cmd->device;\n\tstruct scsi_target *starget = scsi_target(device);\n\n\t \n\tswitch (reason) {\n\tcase SCSI_MLQUEUE_HOST_BUSY:\n\t\tatomic_set(&host->host_blocked, host->max_host_blocked);\n\t\tbreak;\n\tcase SCSI_MLQUEUE_DEVICE_BUSY:\n\tcase SCSI_MLQUEUE_EH_RETRY:\n\t\tatomic_set(&device->device_blocked,\n\t\t\t   device->max_device_blocked);\n\t\tbreak;\n\tcase SCSI_MLQUEUE_TARGET_BUSY:\n\t\tatomic_set(&starget->target_blocked,\n\t\t\t   starget->max_target_blocked);\n\t\tbreak;\n\t}\n}\n\nstatic void scsi_mq_requeue_cmd(struct scsi_cmnd *cmd, unsigned long msecs)\n{\n\tstruct request *rq = scsi_cmd_to_rq(cmd);\n\n\tif (rq->rq_flags & RQF_DONTPREP) {\n\t\trq->rq_flags &= ~RQF_DONTPREP;\n\t\tscsi_mq_uninit_cmd(cmd);\n\t} else {\n\t\tWARN_ON_ONCE(true);\n\t}\n\n\tblk_mq_requeue_request(rq, false);\n\tif (!scsi_host_in_recovery(cmd->device->host))\n\t\tblk_mq_delay_kick_requeue_list(rq->q, msecs);\n}\n\n \nstatic void __scsi_queue_insert(struct scsi_cmnd *cmd, int reason, bool unbusy)\n{\n\tstruct scsi_device *device = cmd->device;\n\n\tSCSI_LOG_MLQUEUE(1, scmd_printk(KERN_INFO, cmd,\n\t\t\"Inserting command %p into mlqueue\\n\", cmd));\n\n\tscsi_set_blocked(cmd, reason);\n\n\t \n\tif (unbusy)\n\t\tscsi_device_unbusy(device, cmd);\n\n\t \n\tcmd->result = 0;\n\n\tblk_mq_requeue_request(scsi_cmd_to_rq(cmd),\n\t\t\t       !scsi_host_in_recovery(cmd->device->host));\n}\n\n \nvoid scsi_queue_insert(struct scsi_cmnd *cmd, int reason)\n{\n\t__scsi_queue_insert(cmd, reason, true);\n}\n\n \nint scsi_execute_cmd(struct scsi_device *sdev, const unsigned char *cmd,\n\t\t     blk_opf_t opf, void *buffer, unsigned int bufflen,\n\t\t     int timeout, int retries,\n\t\t     const struct scsi_exec_args *args)\n{\n\tstatic const struct scsi_exec_args default_args;\n\tstruct request *req;\n\tstruct scsi_cmnd *scmd;\n\tint ret;\n\n\tif (!args)\n\t\targs = &default_args;\n\telse if (WARN_ON_ONCE(args->sense &&\n\t\t\t      args->sense_len != SCSI_SENSE_BUFFERSIZE))\n\t\treturn -EINVAL;\n\n\treq = scsi_alloc_request(sdev->request_queue, opf, args->req_flags);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\tif (bufflen) {\n\t\tret = blk_rq_map_kern(sdev->request_queue, req,\n\t\t\t\t      buffer, bufflen, GFP_NOIO);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tscmd = blk_mq_rq_to_pdu(req);\n\tscmd->cmd_len = COMMAND_SIZE(cmd[0]);\n\tmemcpy(scmd->cmnd, cmd, scmd->cmd_len);\n\tscmd->allowed = retries;\n\tscmd->flags |= args->scmd_flags;\n\treq->timeout = timeout;\n\treq->rq_flags |= RQF_QUIET;\n\n\t \n\tblk_execute_rq(req, true);\n\n\t \n\tif (unlikely(scmd->resid_len > 0 && scmd->resid_len <= bufflen))\n\t\tmemset(buffer + bufflen - scmd->resid_len, 0, scmd->resid_len);\n\n\tif (args->resid)\n\t\t*args->resid = scmd->resid_len;\n\tif (args->sense)\n\t\tmemcpy(args->sense, scmd->sense_buffer, SCSI_SENSE_BUFFERSIZE);\n\tif (args->sshdr)\n\t\tscsi_normalize_sense(scmd->sense_buffer, scmd->sense_len,\n\t\t\t\t     args->sshdr);\n\n\tret = scmd->result;\n out:\n\tblk_mq_free_request(req);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(scsi_execute_cmd);\n\n \nstatic void scsi_dec_host_busy(struct Scsi_Host *shost, struct scsi_cmnd *cmd)\n{\n\tunsigned long flags;\n\n\trcu_read_lock();\n\t__clear_bit(SCMD_STATE_INFLIGHT, &cmd->state);\n\tif (unlikely(scsi_host_in_recovery(shost))) {\n\t\tspin_lock_irqsave(shost->host_lock, flags);\n\t\tif (shost->host_failed || shost->host_eh_scheduled)\n\t\t\tscsi_eh_wakeup(shost);\n\t\tspin_unlock_irqrestore(shost->host_lock, flags);\n\t}\n\trcu_read_unlock();\n}\n\nvoid scsi_device_unbusy(struct scsi_device *sdev, struct scsi_cmnd *cmd)\n{\n\tstruct Scsi_Host *shost = sdev->host;\n\tstruct scsi_target *starget = scsi_target(sdev);\n\n\tscsi_dec_host_busy(shost, cmd);\n\n\tif (starget->can_queue > 0)\n\t\tatomic_dec(&starget->target_busy);\n\n\tsbitmap_put(&sdev->budget_map, cmd->budget_token);\n\tcmd->budget_token = -1;\n}\n\n \nstatic void scsi_kick_sdev_queue(struct scsi_device *sdev, void *data)\n{\n\tstruct scsi_device *current_sdev = data;\n\n\tif (sdev != current_sdev)\n\t\tblk_mq_run_hw_queues(sdev->request_queue, true);\n}\n\n \nstatic void scsi_single_lun_run(struct scsi_device *current_sdev)\n{\n\tstruct Scsi_Host *shost = current_sdev->host;\n\tstruct scsi_target *starget = scsi_target(current_sdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(shost->host_lock, flags);\n\tstarget->starget_sdev_user = NULL;\n\tspin_unlock_irqrestore(shost->host_lock, flags);\n\n\t \n\tblk_mq_run_hw_queues(current_sdev->request_queue,\n\t\t\t     shost->queuecommand_may_block);\n\n\tspin_lock_irqsave(shost->host_lock, flags);\n\tif (!starget->starget_sdev_user)\n\t\t__starget_for_each_device(starget, current_sdev,\n\t\t\t\t\t  scsi_kick_sdev_queue);\n\tspin_unlock_irqrestore(shost->host_lock, flags);\n}\n\nstatic inline bool scsi_device_is_busy(struct scsi_device *sdev)\n{\n\tif (scsi_device_busy(sdev) >= sdev->queue_depth)\n\t\treturn true;\n\tif (atomic_read(&sdev->device_blocked) > 0)\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool scsi_target_is_busy(struct scsi_target *starget)\n{\n\tif (starget->can_queue > 0) {\n\t\tif (atomic_read(&starget->target_busy) >= starget->can_queue)\n\t\t\treturn true;\n\t\tif (atomic_read(&starget->target_blocked) > 0)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool scsi_host_is_busy(struct Scsi_Host *shost)\n{\n\tif (atomic_read(&shost->host_blocked) > 0)\n\t\treturn true;\n\tif (shost->host_self_blocked)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void scsi_starved_list_run(struct Scsi_Host *shost)\n{\n\tLIST_HEAD(starved_list);\n\tstruct scsi_device *sdev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(shost->host_lock, flags);\n\tlist_splice_init(&shost->starved_list, &starved_list);\n\n\twhile (!list_empty(&starved_list)) {\n\t\tstruct request_queue *slq;\n\n\t\t \n\t\tif (scsi_host_is_busy(shost))\n\t\t\tbreak;\n\n\t\tsdev = list_entry(starved_list.next,\n\t\t\t\t  struct scsi_device, starved_entry);\n\t\tlist_del_init(&sdev->starved_entry);\n\t\tif (scsi_target_is_busy(scsi_target(sdev))) {\n\t\t\tlist_move_tail(&sdev->starved_entry,\n\t\t\t\t       &shost->starved_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tslq = sdev->request_queue;\n\t\tif (!blk_get_queue(slq))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(shost->host_lock, flags);\n\n\t\tblk_mq_run_hw_queues(slq, false);\n\t\tblk_put_queue(slq);\n\n\t\tspin_lock_irqsave(shost->host_lock, flags);\n\t}\n\t \n\tlist_splice(&starved_list, &shost->starved_list);\n\tspin_unlock_irqrestore(shost->host_lock, flags);\n}\n\n \nstatic void scsi_run_queue(struct request_queue *q)\n{\n\tstruct scsi_device *sdev = q->queuedata;\n\n\tif (scsi_target(sdev)->single_lun)\n\t\tscsi_single_lun_run(sdev);\n\tif (!list_empty(&sdev->host->starved_list))\n\t\tscsi_starved_list_run(sdev->host);\n\n\t \n\tblk_mq_kick_requeue_list(q);\n}\n\nvoid scsi_requeue_run_queue(struct work_struct *work)\n{\n\tstruct scsi_device *sdev;\n\tstruct request_queue *q;\n\n\tsdev = container_of(work, struct scsi_device, requeue_work);\n\tq = sdev->request_queue;\n\tscsi_run_queue(q);\n}\n\nvoid scsi_run_host_queues(struct Scsi_Host *shost)\n{\n\tstruct scsi_device *sdev;\n\n\tshost_for_each_device(sdev, shost)\n\t\tscsi_run_queue(sdev->request_queue);\n}\n\nstatic void scsi_uninit_cmd(struct scsi_cmnd *cmd)\n{\n\tif (!blk_rq_is_passthrough(scsi_cmd_to_rq(cmd))) {\n\t\tstruct scsi_driver *drv = scsi_cmd_to_driver(cmd);\n\n\t\tif (drv->uninit_command)\n\t\t\tdrv->uninit_command(cmd);\n\t}\n}\n\nvoid scsi_free_sgtables(struct scsi_cmnd *cmd)\n{\n\tif (cmd->sdb.table.nents)\n\t\tsg_free_table_chained(&cmd->sdb.table,\n\t\t\t\tSCSI_INLINE_SG_CNT);\n\tif (scsi_prot_sg_count(cmd))\n\t\tsg_free_table_chained(&cmd->prot_sdb->table,\n\t\t\t\tSCSI_INLINE_PROT_SG_CNT);\n}\nEXPORT_SYMBOL_GPL(scsi_free_sgtables);\n\nstatic void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd)\n{\n\tscsi_free_sgtables(cmd);\n\tscsi_uninit_cmd(cmd);\n}\n\nstatic void scsi_run_queue_async(struct scsi_device *sdev)\n{\n\tif (scsi_host_in_recovery(sdev->host))\n\t\treturn;\n\n\tif (scsi_target(sdev)->single_lun ||\n\t    !list_empty(&sdev->host->starved_list)) {\n\t\tkblockd_schedule_work(&sdev->requeue_work);\n\t} else {\n\t\t \n\t\tint old = atomic_read(&sdev->restarts);\n\n\t\t \n\t\tif (old && atomic_cmpxchg(&sdev->restarts, old, 0) == old)\n\t\t\tblk_mq_run_hw_queues(sdev->request_queue, true);\n\t}\n}\n\n \nstatic bool scsi_end_request(struct request *req, blk_status_t error,\n\t\tunsigned int bytes)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\tstruct scsi_device *sdev = cmd->device;\n\tstruct request_queue *q = sdev->request_queue;\n\n\tif (blk_update_request(req, error, bytes))\n\t\treturn true;\n\n\t \n\tif (blk_queue_add_random(q))\n\t\tadd_disk_randomness(req->q->disk);\n\n\tif (!blk_rq_is_passthrough(req)) {\n\t\tWARN_ON_ONCE(!(cmd->flags & SCMD_INITIALIZED));\n\t\tcmd->flags &= ~SCMD_INITIALIZED;\n\t}\n\n\t \n\tdestroy_rcu_head(&cmd->rcu);\n\n\t \n\tscsi_mq_uninit_cmd(cmd);\n\n\t \n\tpercpu_ref_get(&q->q_usage_counter);\n\n\t__blk_mq_end_request(req, error);\n\n\tscsi_run_queue_async(sdev);\n\n\tpercpu_ref_put(&q->q_usage_counter);\n\treturn false;\n}\n\n \nstatic blk_status_t scsi_result_to_blk_status(int result)\n{\n\t \n\tswitch (scsi_ml_byte(result)) {\n\tcase SCSIML_STAT_OK:\n\t\tbreak;\n\tcase SCSIML_STAT_RESV_CONFLICT:\n\t\treturn BLK_STS_RESV_CONFLICT;\n\tcase SCSIML_STAT_NOSPC:\n\t\treturn BLK_STS_NOSPC;\n\tcase SCSIML_STAT_MED_ERROR:\n\t\treturn BLK_STS_MEDIUM;\n\tcase SCSIML_STAT_TGT_FAILURE:\n\t\treturn BLK_STS_TARGET;\n\tcase SCSIML_STAT_DL_TIMEOUT:\n\t\treturn BLK_STS_DURATION_LIMIT;\n\t}\n\n\tswitch (host_byte(result)) {\n\tcase DID_OK:\n\t\tif (scsi_status_is_good(result))\n\t\t\treturn BLK_STS_OK;\n\t\treturn BLK_STS_IOERR;\n\tcase DID_TRANSPORT_FAILFAST:\n\tcase DID_TRANSPORT_MARGINAL:\n\t\treturn BLK_STS_TRANSPORT;\n\tdefault:\n\t\treturn BLK_STS_IOERR;\n\t}\n}\n\n \nstatic unsigned int scsi_rq_err_bytes(const struct request *rq)\n{\n\tblk_opf_t ff = rq->cmd_flags & REQ_FAILFAST_MASK;\n\tunsigned int bytes = 0;\n\tstruct bio *bio;\n\n\tif (!(rq->rq_flags & RQF_MIXED_MERGE))\n\t\treturn blk_rq_bytes(rq);\n\n\t \n\tfor (bio = rq->bio; bio; bio = bio->bi_next) {\n\t\tif ((bio->bi_opf & ff) != ff)\n\t\t\tbreak;\n\t\tbytes += bio->bi_iter.bi_size;\n\t}\n\n\t \n\tBUG_ON(blk_rq_bytes(rq) && !bytes);\n\treturn bytes;\n}\n\nstatic bool scsi_cmd_runtime_exceeced(struct scsi_cmnd *cmd)\n{\n\tstruct request *req = scsi_cmd_to_rq(cmd);\n\tunsigned long wait_for;\n\n\tif (cmd->allowed == SCSI_CMD_RETRIES_NO_LIMIT)\n\t\treturn false;\n\n\twait_for = (cmd->allowed + 1) * req->timeout;\n\tif (time_before(cmd->jiffies_at_alloc + wait_for, jiffies)) {\n\t\tscmd_printk(KERN_ERR, cmd, \"timing out command, waited %lus\\n\",\n\t\t\t    wait_for/HZ);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \n#define ALUA_TRANSITION_REPREP_DELAY\t1000\n\n \nstatic void scsi_io_completion_action(struct scsi_cmnd *cmd, int result)\n{\n\tstruct request *req = scsi_cmd_to_rq(cmd);\n\tint level = 0;\n\tenum {ACTION_FAIL, ACTION_REPREP, ACTION_DELAYED_REPREP,\n\t      ACTION_RETRY, ACTION_DELAYED_RETRY} action;\n\tstruct scsi_sense_hdr sshdr;\n\tbool sense_valid;\n\tbool sense_current = true;       \n\tblk_status_t blk_stat;\n\n\tsense_valid = scsi_command_normalize_sense(cmd, &sshdr);\n\tif (sense_valid)\n\t\tsense_current = !scsi_sense_is_deferred(&sshdr);\n\n\tblk_stat = scsi_result_to_blk_status(result);\n\n\tif (host_byte(result) == DID_RESET) {\n\t\t \n\t\taction = ACTION_RETRY;\n\t} else if (sense_valid && sense_current) {\n\t\tswitch (sshdr.sense_key) {\n\t\tcase UNIT_ATTENTION:\n\t\t\tif (cmd->device->removable) {\n\t\t\t\t \n\t\t\t\tcmd->device->changed = 1;\n\t\t\t\taction = ACTION_FAIL;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\taction = ACTION_RETRY;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ILLEGAL_REQUEST:\n\t\t\t \n\t\t\tif ((cmd->device->use_10_for_rw &&\n\t\t\t    sshdr.asc == 0x20 && sshdr.ascq == 0x00) &&\n\t\t\t    (cmd->cmnd[0] == READ_10 ||\n\t\t\t     cmd->cmnd[0] == WRITE_10)) {\n\t\t\t\t \n\t\t\t\tcmd->device->use_10_for_rw = 0;\n\t\t\t\taction = ACTION_REPREP;\n\t\t\t} else if (sshdr.asc == 0x10)   {\n\t\t\t\taction = ACTION_FAIL;\n\t\t\t\tblk_stat = BLK_STS_PROTECTION;\n\t\t\t \n\t\t\t} else if (sshdr.asc == 0x20 || sshdr.asc == 0x24) {\n\t\t\t\taction = ACTION_FAIL;\n\t\t\t\tblk_stat = BLK_STS_TARGET;\n\t\t\t} else\n\t\t\t\taction = ACTION_FAIL;\n\t\t\tbreak;\n\t\tcase ABORTED_COMMAND:\n\t\t\taction = ACTION_FAIL;\n\t\t\tif (sshdr.asc == 0x10)  \n\t\t\t\tblk_stat = BLK_STS_PROTECTION;\n\t\t\tbreak;\n\t\tcase NOT_READY:\n\t\t\t \n\t\t\tif (sshdr.asc == 0x04) {\n\t\t\t\tswitch (sshdr.ascq) {\n\t\t\t\tcase 0x01:  \n\t\t\t\tcase 0x04:  \n\t\t\t\tcase 0x05:  \n\t\t\t\tcase 0x06:  \n\t\t\t\tcase 0x07:  \n\t\t\t\tcase 0x08:  \n\t\t\t\tcase 0x09:  \n\t\t\t\tcase 0x11:  \n\t\t\t\tcase 0x14:  \n\t\t\t\tcase 0x1a:  \n\t\t\t\tcase 0x1b:  \n\t\t\t\tcase 0x1d:  \n\t\t\t\tcase 0x24:  \n\t\t\t\t\taction = ACTION_DELAYED_RETRY;\n\t\t\t\t\tbreak;\n\t\t\t\tcase 0x0a:  \n\t\t\t\t\taction = ACTION_DELAYED_REPREP;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\taction = ACTION_FAIL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\taction = ACTION_FAIL;\n\t\t\tbreak;\n\t\tcase VOLUME_OVERFLOW:\n\t\t\t \n\t\t\taction = ACTION_FAIL;\n\t\t\tbreak;\n\t\tcase DATA_PROTECT:\n\t\t\taction = ACTION_FAIL;\n\t\t\tif ((sshdr.asc == 0x0C && sshdr.ascq == 0x12) ||\n\t\t\t    (sshdr.asc == 0x55 &&\n\t\t\t     (sshdr.ascq == 0x0E || sshdr.ascq == 0x0F))) {\n\t\t\t\t \n\t\t\t\tblk_stat = BLK_STS_ZONE_OPEN_RESOURCE;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase COMPLETED:\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\taction = ACTION_FAIL;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\taction = ACTION_FAIL;\n\n\tif (action != ACTION_FAIL && scsi_cmd_runtime_exceeced(cmd))\n\t\taction = ACTION_FAIL;\n\n\tswitch (action) {\n\tcase ACTION_FAIL:\n\t\t \n\t\tif (!(req->rq_flags & RQF_QUIET)) {\n\t\t\tstatic DEFINE_RATELIMIT_STATE(_rs,\n\t\t\t\t\tDEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t\tDEFAULT_RATELIMIT_BURST);\n\n\t\t\tif (unlikely(scsi_logging_level))\n\t\t\t\tlevel =\n\t\t\t\t     SCSI_LOG_LEVEL(SCSI_LOG_MLCOMPLETE_SHIFT,\n\t\t\t\t\t\t    SCSI_LOG_MLCOMPLETE_BITS);\n\n\t\t\t \n\t\t\tif (!level && __ratelimit(&_rs)) {\n\t\t\t\tscsi_print_result(cmd, NULL, FAILED);\n\t\t\t\tif (sense_valid)\n\t\t\t\t\tscsi_print_sense(cmd);\n\t\t\t\tscsi_print_command(cmd);\n\t\t\t}\n\t\t}\n\t\tif (!scsi_end_request(req, blk_stat, scsi_rq_err_bytes(req)))\n\t\t\treturn;\n\t\tfallthrough;\n\tcase ACTION_REPREP:\n\t\tscsi_mq_requeue_cmd(cmd, 0);\n\t\tbreak;\n\tcase ACTION_DELAYED_REPREP:\n\t\tscsi_mq_requeue_cmd(cmd, ALUA_TRANSITION_REPREP_DELAY);\n\t\tbreak;\n\tcase ACTION_RETRY:\n\t\t \n\t\t__scsi_queue_insert(cmd, SCSI_MLQUEUE_EH_RETRY, false);\n\t\tbreak;\n\tcase ACTION_DELAYED_RETRY:\n\t\t \n\t\t__scsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY, false);\n\t\tbreak;\n\t}\n}\n\n \nstatic int scsi_io_completion_nz_result(struct scsi_cmnd *cmd, int result,\n\t\t\t\t\tblk_status_t *blk_statp)\n{\n\tbool sense_valid;\n\tbool sense_current = true;\t \n\tstruct request *req = scsi_cmd_to_rq(cmd);\n\tstruct scsi_sense_hdr sshdr;\n\n\tsense_valid = scsi_command_normalize_sense(cmd, &sshdr);\n\tif (sense_valid)\n\t\tsense_current = !scsi_sense_is_deferred(&sshdr);\n\n\tif (blk_rq_is_passthrough(req)) {\n\t\tif (sense_valid) {\n\t\t\t \n\t\t\tcmd->sense_len = min(8 + cmd->sense_buffer[7],\n\t\t\t\t\t     SCSI_SENSE_BUFFERSIZE);\n\t\t}\n\t\tif (sense_current)\n\t\t\t*blk_statp = scsi_result_to_blk_status(result);\n\t} else if (blk_rq_bytes(req) == 0 && sense_current) {\n\t\t \n\t\t*blk_statp = scsi_result_to_blk_status(result);\n\t}\n\t \n\tif (sense_valid && (sshdr.sense_key == RECOVERED_ERROR)) {\n\t\tbool do_print = true;\n\t\t \n\t\tif ((sshdr.asc == 0x0) && (sshdr.ascq == 0x1d))\n\t\t\tdo_print = false;\n\t\telse if (req->rq_flags & RQF_QUIET)\n\t\t\tdo_print = false;\n\t\tif (do_print)\n\t\t\tscsi_print_sense(cmd);\n\t\tresult = 0;\n\t\t \n\t\t*blk_statp = BLK_STS_OK;\n\t}\n\t \n\tif ((result & 0xff) && scsi_status_is_good(result)) {\n\t\tresult = 0;\n\t\t*blk_statp = BLK_STS_OK;\n\t}\n\treturn result;\n}\n\n \nvoid scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)\n{\n\tint result = cmd->result;\n\tstruct request *req = scsi_cmd_to_rq(cmd);\n\tblk_status_t blk_stat = BLK_STS_OK;\n\n\tif (unlikely(result))\t \n\t\tresult = scsi_io_completion_nz_result(cmd, result, &blk_stat);\n\n\t \n\tSCSI_LOG_HLCOMPLETE(1, scmd_printk(KERN_INFO, cmd,\n\t\t\"%u sectors total, %d bytes done.\\n\",\n\t\tblk_rq_sectors(req), good_bytes));\n\n\t \n\tif (likely(blk_rq_bytes(req) > 0 || blk_stat == BLK_STS_OK)) {\n\t\tif (likely(!scsi_end_request(req, blk_stat, good_bytes)))\n\t\t\treturn;  \n\t}\n\n\t \n\tif (unlikely(blk_stat && scsi_noretry_cmd(cmd))) {\n\t\tif (scsi_end_request(req, blk_stat, blk_rq_bytes(req)))\n\t\t\tWARN_ONCE(true,\n\t\t\t    \"Bytes remaining after failed, no-retry command\");\n\t\treturn;\n\t}\n\n\t \n\tif (likely(result == 0))\n\t\tscsi_mq_requeue_cmd(cmd, 0);\n\telse\n\t\tscsi_io_completion_action(cmd, result);\n}\n\nstatic inline bool scsi_cmd_needs_dma_drain(struct scsi_device *sdev,\n\t\tstruct request *rq)\n{\n\treturn sdev->dma_drain_len && blk_rq_is_passthrough(rq) &&\n\t       !op_is_write(req_op(rq)) &&\n\t       sdev->host->hostt->dma_need_drain(rq);\n}\n\n \nblk_status_t scsi_alloc_sgtables(struct scsi_cmnd *cmd)\n{\n\tstruct scsi_device *sdev = cmd->device;\n\tstruct request *rq = scsi_cmd_to_rq(cmd);\n\tunsigned short nr_segs = blk_rq_nr_phys_segments(rq);\n\tstruct scatterlist *last_sg = NULL;\n\tblk_status_t ret;\n\tbool need_drain = scsi_cmd_needs_dma_drain(sdev, rq);\n\tint count;\n\n\tif (WARN_ON_ONCE(!nr_segs))\n\t\treturn BLK_STS_IOERR;\n\n\t \n\tif (need_drain)\n\t\tnr_segs++;\n\n\t \n\tif (unlikely(sg_alloc_table_chained(&cmd->sdb.table, nr_segs,\n\t\t\tcmd->sdb.table.sgl, SCSI_INLINE_SG_CNT)))\n\t\treturn BLK_STS_RESOURCE;\n\n\t \n\tcount = __blk_rq_map_sg(rq->q, rq, cmd->sdb.table.sgl, &last_sg);\n\n\tif (blk_rq_bytes(rq) & rq->q->dma_pad_mask) {\n\t\tunsigned int pad_len =\n\t\t\t(rq->q->dma_pad_mask & ~blk_rq_bytes(rq)) + 1;\n\n\t\tlast_sg->length += pad_len;\n\t\tcmd->extra_len += pad_len;\n\t}\n\n\tif (need_drain) {\n\t\tsg_unmark_end(last_sg);\n\t\tlast_sg = sg_next(last_sg);\n\t\tsg_set_buf(last_sg, sdev->dma_drain_buf, sdev->dma_drain_len);\n\t\tsg_mark_end(last_sg);\n\n\t\tcmd->extra_len += sdev->dma_drain_len;\n\t\tcount++;\n\t}\n\n\tBUG_ON(count > cmd->sdb.table.nents);\n\tcmd->sdb.table.nents = count;\n\tcmd->sdb.length = blk_rq_payload_bytes(rq);\n\n\tif (blk_integrity_rq(rq)) {\n\t\tstruct scsi_data_buffer *prot_sdb = cmd->prot_sdb;\n\t\tint ivecs;\n\n\t\tif (WARN_ON_ONCE(!prot_sdb)) {\n\t\t\t \n\t\t\tret = BLK_STS_IOERR;\n\t\t\tgoto out_free_sgtables;\n\t\t}\n\n\t\tivecs = blk_rq_count_integrity_sg(rq->q, rq->bio);\n\n\t\tif (sg_alloc_table_chained(&prot_sdb->table, ivecs,\n\t\t\t\tprot_sdb->table.sgl,\n\t\t\t\tSCSI_INLINE_PROT_SG_CNT)) {\n\t\t\tret = BLK_STS_RESOURCE;\n\t\t\tgoto out_free_sgtables;\n\t\t}\n\n\t\tcount = blk_rq_map_integrity_sg(rq->q, rq->bio,\n\t\t\t\t\t\tprot_sdb->table.sgl);\n\t\tBUG_ON(count > ivecs);\n\t\tBUG_ON(count > queue_max_integrity_segments(rq->q));\n\n\t\tcmd->prot_sdb = prot_sdb;\n\t\tcmd->prot_sdb->table.nents = count;\n\t}\n\n\treturn BLK_STS_OK;\nout_free_sgtables:\n\tscsi_free_sgtables(cmd);\n\treturn ret;\n}\nEXPORT_SYMBOL(scsi_alloc_sgtables);\n\n \nstatic void scsi_initialize_rq(struct request *rq)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tmemset(cmd->cmnd, 0, sizeof(cmd->cmnd));\n\tcmd->cmd_len = MAX_COMMAND_SIZE;\n\tcmd->sense_len = 0;\n\tinit_rcu_head(&cmd->rcu);\n\tcmd->jiffies_at_alloc = jiffies;\n\tcmd->retries = 0;\n}\n\nstruct request *scsi_alloc_request(struct request_queue *q, blk_opf_t opf,\n\t\t\t\t   blk_mq_req_flags_t flags)\n{\n\tstruct request *rq;\n\n\trq = blk_mq_alloc_request(q, opf, flags);\n\tif (!IS_ERR(rq))\n\t\tscsi_initialize_rq(rq);\n\treturn rq;\n}\nEXPORT_SYMBOL_GPL(scsi_alloc_request);\n\n \nstatic void scsi_cleanup_rq(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_DONTPREP) {\n\t\tscsi_mq_uninit_cmd(blk_mq_rq_to_pdu(rq));\n\t\trq->rq_flags &= ~RQF_DONTPREP;\n\t}\n}\n\n \nvoid scsi_init_command(struct scsi_device *dev, struct scsi_cmnd *cmd)\n{\n\tstruct request *rq = scsi_cmd_to_rq(cmd);\n\n\tif (!blk_rq_is_passthrough(rq) && !(cmd->flags & SCMD_INITIALIZED)) {\n\t\tcmd->flags |= SCMD_INITIALIZED;\n\t\tscsi_initialize_rq(rq);\n\t}\n\n\tcmd->device = dev;\n\tINIT_LIST_HEAD(&cmd->eh_entry);\n\tINIT_DELAYED_WORK(&cmd->abort_work, scmd_eh_abort_handler);\n}\n\nstatic blk_status_t scsi_setup_scsi_cmnd(struct scsi_device *sdev,\n\t\tstruct request *req)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\n\t \n\tif (req->bio) {\n\t\tblk_status_t ret = scsi_alloc_sgtables(cmd);\n\t\tif (unlikely(ret != BLK_STS_OK))\n\t\t\treturn ret;\n\t} else {\n\t\tBUG_ON(blk_rq_bytes(req));\n\n\t\tmemset(&cmd->sdb, 0, sizeof(cmd->sdb));\n\t}\n\n\tcmd->transfersize = blk_rq_bytes(req);\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t\nscsi_device_state_check(struct scsi_device *sdev, struct request *req)\n{\n\tswitch (sdev->sdev_state) {\n\tcase SDEV_CREATED:\n\t\treturn BLK_STS_OK;\n\tcase SDEV_OFFLINE:\n\tcase SDEV_TRANSPORT_OFFLINE:\n\t\t \n\t\tif (!sdev->offline_already) {\n\t\t\tsdev->offline_already = true;\n\t\t\tsdev_printk(KERN_ERR, sdev,\n\t\t\t\t    \"rejecting I/O to offline device\\n\");\n\t\t}\n\t\treturn BLK_STS_IOERR;\n\tcase SDEV_DEL:\n\t\t \n\t\tsdev_printk(KERN_ERR, sdev,\n\t\t\t    \"rejecting I/O to dead device\\n\");\n\t\treturn BLK_STS_IOERR;\n\tcase SDEV_BLOCK:\n\tcase SDEV_CREATED_BLOCK:\n\t\treturn BLK_STS_RESOURCE;\n\tcase SDEV_QUIESCE:\n\t\t \n\t\tif (req && WARN_ON_ONCE(!(req->rq_flags & RQF_PM)))\n\t\t\treturn BLK_STS_RESOURCE;\n\t\treturn BLK_STS_OK;\n\tdefault:\n\t\t \n\t\tif (req && !(req->rq_flags & RQF_PM))\n\t\t\treturn BLK_STS_OFFLINE;\n\t\treturn BLK_STS_OK;\n\t}\n}\n\n \nstatic inline int scsi_dev_queue_ready(struct request_queue *q,\n\t\t\t\t  struct scsi_device *sdev)\n{\n\tint token;\n\n\ttoken = sbitmap_get(&sdev->budget_map);\n\tif (atomic_read(&sdev->device_blocked)) {\n\t\tif (token < 0)\n\t\t\tgoto out;\n\n\t\tif (scsi_device_busy(sdev) > 1)\n\t\t\tgoto out_dec;\n\n\t\t \n\t\tif (atomic_dec_return(&sdev->device_blocked) > 0)\n\t\t\tgoto out_dec;\n\t\tSCSI_LOG_MLQUEUE(3, sdev_printk(KERN_INFO, sdev,\n\t\t\t\t   \"unblocking device at zero depth\\n\"));\n\t}\n\n\treturn token;\nout_dec:\n\tif (token >= 0)\n\t\tsbitmap_put(&sdev->budget_map, token);\nout:\n\treturn -1;\n}\n\n \nstatic inline int scsi_target_queue_ready(struct Scsi_Host *shost,\n\t\t\t\t\t   struct scsi_device *sdev)\n{\n\tstruct scsi_target *starget = scsi_target(sdev);\n\tunsigned int busy;\n\n\tif (starget->single_lun) {\n\t\tspin_lock_irq(shost->host_lock);\n\t\tif (starget->starget_sdev_user &&\n\t\t    starget->starget_sdev_user != sdev) {\n\t\t\tspin_unlock_irq(shost->host_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tstarget->starget_sdev_user = sdev;\n\t\tspin_unlock_irq(shost->host_lock);\n\t}\n\n\tif (starget->can_queue <= 0)\n\t\treturn 1;\n\n\tbusy = atomic_inc_return(&starget->target_busy) - 1;\n\tif (atomic_read(&starget->target_blocked) > 0) {\n\t\tif (busy)\n\t\t\tgoto starved;\n\n\t\t \n\t\tif (atomic_dec_return(&starget->target_blocked) > 0)\n\t\t\tgoto out_dec;\n\n\t\tSCSI_LOG_MLQUEUE(3, starget_printk(KERN_INFO, starget,\n\t\t\t\t \"unblocking target at zero depth\\n\"));\n\t}\n\n\tif (busy >= starget->can_queue)\n\t\tgoto starved;\n\n\treturn 1;\n\nstarved:\n\tspin_lock_irq(shost->host_lock);\n\tlist_move_tail(&sdev->starved_entry, &shost->starved_list);\n\tspin_unlock_irq(shost->host_lock);\nout_dec:\n\tif (starget->can_queue > 0)\n\t\tatomic_dec(&starget->target_busy);\n\treturn 0;\n}\n\n \nstatic inline int scsi_host_queue_ready(struct request_queue *q,\n\t\t\t\t   struct Scsi_Host *shost,\n\t\t\t\t   struct scsi_device *sdev,\n\t\t\t\t   struct scsi_cmnd *cmd)\n{\n\tif (atomic_read(&shost->host_blocked) > 0) {\n\t\tif (scsi_host_busy(shost) > 0)\n\t\t\tgoto starved;\n\n\t\t \n\t\tif (atomic_dec_return(&shost->host_blocked) > 0)\n\t\t\tgoto out_dec;\n\n\t\tSCSI_LOG_MLQUEUE(3,\n\t\t\tshost_printk(KERN_INFO, shost,\n\t\t\t\t     \"unblocking host at zero depth\\n\"));\n\t}\n\n\tif (shost->host_self_blocked)\n\t\tgoto starved;\n\n\t \n\tif (!list_empty(&sdev->starved_entry)) {\n\t\tspin_lock_irq(shost->host_lock);\n\t\tif (!list_empty(&sdev->starved_entry))\n\t\t\tlist_del_init(&sdev->starved_entry);\n\t\tspin_unlock_irq(shost->host_lock);\n\t}\n\n\t__set_bit(SCMD_STATE_INFLIGHT, &cmd->state);\n\n\treturn 1;\n\nstarved:\n\tspin_lock_irq(shost->host_lock);\n\tif (list_empty(&sdev->starved_entry))\n\t\tlist_add_tail(&sdev->starved_entry, &shost->starved_list);\n\tspin_unlock_irq(shost->host_lock);\nout_dec:\n\tscsi_dec_host_busy(shost, cmd);\n\treturn 0;\n}\n\n \nstatic bool scsi_mq_lld_busy(struct request_queue *q)\n{\n\tstruct scsi_device *sdev = q->queuedata;\n\tstruct Scsi_Host *shost;\n\n\tif (blk_queue_dying(q))\n\t\treturn false;\n\n\tshost = sdev->host;\n\n\t \n\tif (scsi_host_in_recovery(shost) || scsi_device_is_busy(sdev))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void scsi_complete(struct request *rq)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\n\tenum scsi_disposition disposition;\n\n\tINIT_LIST_HEAD(&cmd->eh_entry);\n\n\tatomic_inc(&cmd->device->iodone_cnt);\n\tif (cmd->result)\n\t\tatomic_inc(&cmd->device->ioerr_cnt);\n\n\tdisposition = scsi_decide_disposition(cmd);\n\tif (disposition != SUCCESS && scsi_cmd_runtime_exceeced(cmd))\n\t\tdisposition = SUCCESS;\n\n\tscsi_log_completion(cmd, disposition);\n\n\tswitch (disposition) {\n\tcase SUCCESS:\n\t\tscsi_finish_command(cmd);\n\t\tbreak;\n\tcase NEEDS_RETRY:\n\t\tscsi_queue_insert(cmd, SCSI_MLQUEUE_EH_RETRY);\n\t\tbreak;\n\tcase ADD_TO_MLQUEUE:\n\t\tscsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY);\n\t\tbreak;\n\tdefault:\n\t\tscsi_eh_scmd_add(cmd);\n\t\tbreak;\n\t}\n}\n\n \nstatic int scsi_dispatch_cmd(struct scsi_cmnd *cmd)\n{\n\tstruct Scsi_Host *host = cmd->device->host;\n\tint rtn = 0;\n\n\tatomic_inc(&cmd->device->iorequest_cnt);\n\n\t \n\tif (unlikely(cmd->device->sdev_state == SDEV_DEL)) {\n\t\t \n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\tgoto done;\n\t}\n\n\t \n\tif (unlikely(scsi_device_blocked(cmd->device))) {\n\t\t \n\t\tSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\n\t\t\t\"queuecommand : device blocked\\n\"));\n\t\tatomic_dec(&cmd->device->iorequest_cnt);\n\t\treturn SCSI_MLQUEUE_DEVICE_BUSY;\n\t}\n\n\t \n\tif (cmd->device->lun_in_cdb)\n\t\tcmd->cmnd[1] = (cmd->cmnd[1] & 0x1f) |\n\t\t\t       (cmd->device->lun << 5 & 0xe0);\n\n\tscsi_log_send(cmd);\n\n\t \n\tif (cmd->cmd_len > cmd->device->host->max_cmd_len) {\n\t\tSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\n\t\t\t       \"queuecommand : command too long. \"\n\t\t\t       \"cdb_size=%d host->max_cmd_len=%d\\n\",\n\t\t\t       cmd->cmd_len, cmd->device->host->max_cmd_len));\n\t\tcmd->result = (DID_ABORT << 16);\n\t\tgoto done;\n\t}\n\n\tif (unlikely(host->shost_state == SHOST_DEL)) {\n\t\tcmd->result = (DID_NO_CONNECT << 16);\n\t\tgoto done;\n\n\t}\n\n\ttrace_scsi_dispatch_cmd_start(cmd);\n\trtn = host->hostt->queuecommand(host, cmd);\n\tif (rtn) {\n\t\tatomic_dec(&cmd->device->iorequest_cnt);\n\t\ttrace_scsi_dispatch_cmd_error(cmd, rtn);\n\t\tif (rtn != SCSI_MLQUEUE_DEVICE_BUSY &&\n\t\t    rtn != SCSI_MLQUEUE_TARGET_BUSY)\n\t\t\trtn = SCSI_MLQUEUE_HOST_BUSY;\n\n\t\tSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\n\t\t\t\"queuecommand : request rejected\\n\"));\n\t}\n\n\treturn rtn;\n done:\n\tscsi_done(cmd);\n\treturn 0;\n}\n\n \nstatic unsigned int scsi_mq_inline_sgl_size(struct Scsi_Host *shost)\n{\n\treturn min_t(unsigned int, shost->sg_tablesize, SCSI_INLINE_SG_CNT) *\n\t\tsizeof(struct scatterlist);\n}\n\nstatic blk_status_t scsi_prepare_cmd(struct request *req)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\tstruct scsi_device *sdev = req->q->queuedata;\n\tstruct Scsi_Host *shost = sdev->host;\n\tbool in_flight = test_bit(SCMD_STATE_INFLIGHT, &cmd->state);\n\tstruct scatterlist *sg;\n\n\tscsi_init_command(sdev, cmd);\n\n\tcmd->eh_eflags = 0;\n\tcmd->prot_type = 0;\n\tcmd->prot_flags = 0;\n\tcmd->submitter = 0;\n\tmemset(&cmd->sdb, 0, sizeof(cmd->sdb));\n\tcmd->underflow = 0;\n\tcmd->transfersize = 0;\n\tcmd->host_scribble = NULL;\n\tcmd->result = 0;\n\tcmd->extra_len = 0;\n\tcmd->state = 0;\n\tif (in_flight)\n\t\t__set_bit(SCMD_STATE_INFLIGHT, &cmd->state);\n\n\t \n\tif (!shost->hostt->init_cmd_priv)\n\t\tmemset(cmd + 1, 0, shost->hostt->cmd_size);\n\n\tcmd->prot_op = SCSI_PROT_NORMAL;\n\tif (blk_rq_bytes(req))\n\t\tcmd->sc_data_direction = rq_dma_dir(req);\n\telse\n\t\tcmd->sc_data_direction = DMA_NONE;\n\n\tsg = (void *)cmd + sizeof(struct scsi_cmnd) + shost->hostt->cmd_size;\n\tcmd->sdb.table.sgl = sg;\n\n\tif (scsi_host_get_prot(shost)) {\n\t\tmemset(cmd->prot_sdb, 0, sizeof(struct scsi_data_buffer));\n\n\t\tcmd->prot_sdb->table.sgl =\n\t\t\t(struct scatterlist *)(cmd->prot_sdb + 1);\n\t}\n\n\t \n\tif (blk_rq_is_passthrough(req))\n\t\treturn scsi_setup_scsi_cmnd(sdev, req);\n\n\tif (sdev->handler && sdev->handler->prep_fn) {\n\t\tblk_status_t ret = sdev->handler->prep_fn(sdev, req);\n\n\t\tif (ret != BLK_STS_OK)\n\t\t\treturn ret;\n\t}\n\n\t \n\tcmd->allowed = 0;\n\tmemset(cmd->cmnd, 0, sizeof(cmd->cmnd));\n\treturn scsi_cmd_to_driver(cmd)->init_command(cmd);\n}\n\nstatic void scsi_done_internal(struct scsi_cmnd *cmd, bool complete_directly)\n{\n\tstruct request *req = scsi_cmd_to_rq(cmd);\n\n\tswitch (cmd->submitter) {\n\tcase SUBMITTED_BY_BLOCK_LAYER:\n\t\tbreak;\n\tcase SUBMITTED_BY_SCSI_ERROR_HANDLER:\n\t\treturn scsi_eh_done(cmd);\n\tcase SUBMITTED_BY_SCSI_RESET_IOCTL:\n\t\treturn;\n\t}\n\n\tif (unlikely(blk_should_fake_timeout(scsi_cmd_to_rq(cmd)->q)))\n\t\treturn;\n\tif (unlikely(test_and_set_bit(SCMD_STATE_COMPLETE, &cmd->state)))\n\t\treturn;\n\ttrace_scsi_dispatch_cmd_done(cmd);\n\n\tif (complete_directly)\n\t\tblk_mq_complete_request_direct(req, scsi_complete);\n\telse\n\t\tblk_mq_complete_request(req);\n}\n\nvoid scsi_done(struct scsi_cmnd *cmd)\n{\n\tscsi_done_internal(cmd, false);\n}\nEXPORT_SYMBOL(scsi_done);\n\nvoid scsi_done_direct(struct scsi_cmnd *cmd)\n{\n\tscsi_done_internal(cmd, true);\n}\nEXPORT_SYMBOL(scsi_done_direct);\n\nstatic void scsi_mq_put_budget(struct request_queue *q, int budget_token)\n{\n\tstruct scsi_device *sdev = q->queuedata;\n\n\tsbitmap_put(&sdev->budget_map, budget_token);\n}\n\n \n#define SCSI_QUEUE_DELAY 3\n\nstatic int scsi_mq_get_budget(struct request_queue *q)\n{\n\tstruct scsi_device *sdev = q->queuedata;\n\tint token = scsi_dev_queue_ready(q, sdev);\n\n\tif (token >= 0)\n\t\treturn token;\n\n\tatomic_inc(&sdev->restarts);\n\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\tif (unlikely(scsi_device_busy(sdev) == 0 &&\n\t\t\t\t!scsi_device_blocked(sdev)))\n\t\tblk_mq_delay_run_hw_queues(sdev->request_queue, SCSI_QUEUE_DELAY);\n\treturn -1;\n}\n\nstatic void scsi_mq_set_rq_budget_token(struct request *req, int token)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\n\tcmd->budget_token = token;\n}\n\nstatic int scsi_mq_get_rq_budget_token(struct request *req)\n{\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\n\treturn cmd->budget_token;\n}\n\nstatic blk_status_t scsi_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t const struct blk_mq_queue_data *bd)\n{\n\tstruct request *req = bd->rq;\n\tstruct request_queue *q = req->q;\n\tstruct scsi_device *sdev = q->queuedata;\n\tstruct Scsi_Host *shost = sdev->host;\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\n\tblk_status_t ret;\n\tint reason;\n\n\tWARN_ON_ONCE(cmd->budget_token < 0);\n\n\t \n\tif (unlikely(sdev->sdev_state != SDEV_RUNNING)) {\n\t\tret = scsi_device_state_check(sdev, req);\n\t\tif (ret != BLK_STS_OK)\n\t\t\tgoto out_put_budget;\n\t}\n\n\tret = BLK_STS_RESOURCE;\n\tif (!scsi_target_queue_ready(shost, sdev))\n\t\tgoto out_put_budget;\n\tif (unlikely(scsi_host_in_recovery(shost))) {\n\t\tif (cmd->flags & SCMD_FAIL_IF_RECOVERING)\n\t\t\tret = BLK_STS_OFFLINE;\n\t\tgoto out_dec_target_busy;\n\t}\n\tif (!scsi_host_queue_ready(q, shost, sdev, cmd))\n\t\tgoto out_dec_target_busy;\n\n\tif (!(req->rq_flags & RQF_DONTPREP)) {\n\t\tret = scsi_prepare_cmd(req);\n\t\tif (ret != BLK_STS_OK)\n\t\t\tgoto out_dec_host_busy;\n\t\treq->rq_flags |= RQF_DONTPREP;\n\t} else {\n\t\tclear_bit(SCMD_STATE_COMPLETE, &cmd->state);\n\t}\n\n\tcmd->flags &= SCMD_PRESERVED_FLAGS;\n\tif (sdev->simple_tags)\n\t\tcmd->flags |= SCMD_TAGGED;\n\tif (bd->last)\n\t\tcmd->flags |= SCMD_LAST;\n\n\tscsi_set_resid(cmd, 0);\n\tmemset(cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\n\tcmd->submitter = SUBMITTED_BY_BLOCK_LAYER;\n\n\tblk_mq_start_request(req);\n\treason = scsi_dispatch_cmd(cmd);\n\tif (reason) {\n\t\tscsi_set_blocked(cmd, reason);\n\t\tret = BLK_STS_RESOURCE;\n\t\tgoto out_dec_host_busy;\n\t}\n\n\treturn BLK_STS_OK;\n\nout_dec_host_busy:\n\tscsi_dec_host_busy(shost, cmd);\nout_dec_target_busy:\n\tif (scsi_target(sdev)->can_queue > 0)\n\t\tatomic_dec(&scsi_target(sdev)->target_busy);\nout_put_budget:\n\tscsi_mq_put_budget(q, cmd->budget_token);\n\tcmd->budget_token = -1;\n\tswitch (ret) {\n\tcase BLK_STS_OK:\n\t\tbreak;\n\tcase BLK_STS_RESOURCE:\n\tcase BLK_STS_ZONE_RESOURCE:\n\t\tif (scsi_device_blocked(sdev))\n\t\t\tret = BLK_STS_DEV_RESOURCE;\n\t\tbreak;\n\tcase BLK_STS_AGAIN:\n\t\tcmd->result = DID_BUS_BUSY << 16;\n\t\tif (req->rq_flags & RQF_DONTPREP)\n\t\t\tscsi_mq_uninit_cmd(cmd);\n\t\tbreak;\n\tdefault:\n\t\tif (unlikely(!scsi_device_online(sdev)))\n\t\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\telse\n\t\t\tcmd->result = DID_ERROR << 16;\n\t\t \n\t\tif (req->rq_flags & RQF_DONTPREP)\n\t\t\tscsi_mq_uninit_cmd(cmd);\n\t\tscsi_run_queue_async(sdev);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int scsi_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t\tunsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct Scsi_Host *shost = set->driver_data;\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\n\tstruct scatterlist *sg;\n\tint ret = 0;\n\n\tcmd->sense_buffer =\n\t\tkmem_cache_alloc_node(scsi_sense_cache, GFP_KERNEL, numa_node);\n\tif (!cmd->sense_buffer)\n\t\treturn -ENOMEM;\n\n\tif (scsi_host_get_prot(shost)) {\n\t\tsg = (void *)cmd + sizeof(struct scsi_cmnd) +\n\t\t\tshost->hostt->cmd_size;\n\t\tcmd->prot_sdb = (void *)sg + scsi_mq_inline_sgl_size(shost);\n\t}\n\n\tif (shost->hostt->init_cmd_priv) {\n\t\tret = shost->hostt->init_cmd_priv(shost, cmd);\n\t\tif (ret < 0)\n\t\t\tkmem_cache_free(scsi_sense_cache, cmd->sense_buffer);\n\t}\n\n\treturn ret;\n}\n\nstatic void scsi_mq_exit_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t\t unsigned int hctx_idx)\n{\n\tstruct Scsi_Host *shost = set->driver_data;\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tif (shost->hostt->exit_cmd_priv)\n\t\tshost->hostt->exit_cmd_priv(shost, cmd);\n\tkmem_cache_free(scsi_sense_cache, cmd->sense_buffer);\n}\n\n\nstatic int scsi_mq_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct Scsi_Host *shost = hctx->driver_data;\n\n\tif (shost->hostt->mq_poll)\n\t\treturn shost->hostt->mq_poll(shost, hctx->queue_num);\n\n\treturn 0;\n}\n\nstatic int scsi_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\t\t  unsigned int hctx_idx)\n{\n\tstruct Scsi_Host *shost = data;\n\n\thctx->driver_data = shost;\n\treturn 0;\n}\n\nstatic void scsi_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct Scsi_Host *shost = container_of(set, struct Scsi_Host, tag_set);\n\n\tif (shost->hostt->map_queues)\n\t\treturn shost->hostt->map_queues(shost);\n\tblk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);\n}\n\nvoid __scsi_init_queue(struct Scsi_Host *shost, struct request_queue *q)\n{\n\tstruct device *dev = shost->dma_dev;\n\n\t \n\tblk_queue_max_segments(q, min_t(unsigned short, shost->sg_tablesize,\n\t\t\t\t\tSG_MAX_SEGMENTS));\n\n\tif (scsi_host_prot_dma(shost)) {\n\t\tshost->sg_prot_tablesize =\n\t\t\tmin_not_zero(shost->sg_prot_tablesize,\n\t\t\t\t     (unsigned short)SCSI_MAX_PROT_SG_SEGMENTS);\n\t\tBUG_ON(shost->sg_prot_tablesize < shost->sg_tablesize);\n\t\tblk_queue_max_integrity_segments(q, shost->sg_prot_tablesize);\n\t}\n\n\tblk_queue_max_hw_sectors(q, shost->max_sectors);\n\tblk_queue_segment_boundary(q, shost->dma_boundary);\n\tdma_set_seg_boundary(dev, shost->dma_boundary);\n\n\tblk_queue_max_segment_size(q, shost->max_segment_size);\n\tblk_queue_virt_boundary(q, shost->virt_boundary_mask);\n\tdma_set_max_seg_size(dev, queue_max_segment_size(q));\n\n\t \n\tblk_queue_dma_alignment(q, max(4, dma_get_cache_alignment()) - 1);\n}\nEXPORT_SYMBOL_GPL(__scsi_init_queue);\n\nstatic const struct blk_mq_ops scsi_mq_ops_no_commit = {\n\t.get_budget\t= scsi_mq_get_budget,\n\t.put_budget\t= scsi_mq_put_budget,\n\t.queue_rq\t= scsi_queue_rq,\n\t.complete\t= scsi_complete,\n\t.timeout\t= scsi_timeout,\n#ifdef CONFIG_BLK_DEBUG_FS\n\t.show_rq\t= scsi_show_rq,\n#endif\n\t.init_request\t= scsi_mq_init_request,\n\t.exit_request\t= scsi_mq_exit_request,\n\t.cleanup_rq\t= scsi_cleanup_rq,\n\t.busy\t\t= scsi_mq_lld_busy,\n\t.map_queues\t= scsi_map_queues,\n\t.init_hctx\t= scsi_init_hctx,\n\t.poll\t\t= scsi_mq_poll,\n\t.set_rq_budget_token = scsi_mq_set_rq_budget_token,\n\t.get_rq_budget_token = scsi_mq_get_rq_budget_token,\n};\n\n\nstatic void scsi_commit_rqs(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct Scsi_Host *shost = hctx->driver_data;\n\n\tshost->hostt->commit_rqs(shost, hctx->queue_num);\n}\n\nstatic const struct blk_mq_ops scsi_mq_ops = {\n\t.get_budget\t= scsi_mq_get_budget,\n\t.put_budget\t= scsi_mq_put_budget,\n\t.queue_rq\t= scsi_queue_rq,\n\t.commit_rqs\t= scsi_commit_rqs,\n\t.complete\t= scsi_complete,\n\t.timeout\t= scsi_timeout,\n#ifdef CONFIG_BLK_DEBUG_FS\n\t.show_rq\t= scsi_show_rq,\n#endif\n\t.init_request\t= scsi_mq_init_request,\n\t.exit_request\t= scsi_mq_exit_request,\n\t.cleanup_rq\t= scsi_cleanup_rq,\n\t.busy\t\t= scsi_mq_lld_busy,\n\t.map_queues\t= scsi_map_queues,\n\t.init_hctx\t= scsi_init_hctx,\n\t.poll\t\t= scsi_mq_poll,\n\t.set_rq_budget_token = scsi_mq_set_rq_budget_token,\n\t.get_rq_budget_token = scsi_mq_get_rq_budget_token,\n};\n\nint scsi_mq_setup_tags(struct Scsi_Host *shost)\n{\n\tunsigned int cmd_size, sgl_size;\n\tstruct blk_mq_tag_set *tag_set = &shost->tag_set;\n\n\tsgl_size = max_t(unsigned int, sizeof(struct scatterlist),\n\t\t\t\tscsi_mq_inline_sgl_size(shost));\n\tcmd_size = sizeof(struct scsi_cmnd) + shost->hostt->cmd_size + sgl_size;\n\tif (scsi_host_get_prot(shost))\n\t\tcmd_size += sizeof(struct scsi_data_buffer) +\n\t\t\tsizeof(struct scatterlist) * SCSI_INLINE_PROT_SG_CNT;\n\n\tmemset(tag_set, 0, sizeof(*tag_set));\n\tif (shost->hostt->commit_rqs)\n\t\ttag_set->ops = &scsi_mq_ops;\n\telse\n\t\ttag_set->ops = &scsi_mq_ops_no_commit;\n\ttag_set->nr_hw_queues = shost->nr_hw_queues ? : 1;\n\ttag_set->nr_maps = shost->nr_maps ? : 1;\n\ttag_set->queue_depth = shost->can_queue;\n\ttag_set->cmd_size = cmd_size;\n\ttag_set->numa_node = dev_to_node(shost->dma_dev);\n\ttag_set->flags = BLK_MQ_F_SHOULD_MERGE;\n\ttag_set->flags |=\n\t\tBLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);\n\tif (shost->queuecommand_may_block)\n\t\ttag_set->flags |= BLK_MQ_F_BLOCKING;\n\ttag_set->driver_data = shost;\n\tif (shost->host_tagset)\n\t\ttag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;\n\n\treturn blk_mq_alloc_tag_set(tag_set);\n}\n\nvoid scsi_mq_free_tags(struct kref *kref)\n{\n\tstruct Scsi_Host *shost = container_of(kref, typeof(*shost),\n\t\t\t\t\t       tagset_refcnt);\n\n\tblk_mq_free_tag_set(&shost->tag_set);\n\tcomplete(&shost->tagset_freed);\n}\n\n \nstruct scsi_device *scsi_device_from_queue(struct request_queue *q)\n{\n\tstruct scsi_device *sdev = NULL;\n\n\tif (q->mq_ops == &scsi_mq_ops_no_commit ||\n\t    q->mq_ops == &scsi_mq_ops)\n\t\tsdev = q->queuedata;\n\tif (!sdev || !get_device(&sdev->sdev_gendev))\n\t\tsdev = NULL;\n\n\treturn sdev;\n}\n \n#ifdef CONFIG_CDROM_PKTCDVD_MODULE\nEXPORT_SYMBOL_GPL(scsi_device_from_queue);\n#endif\n\n \nvoid scsi_block_requests(struct Scsi_Host *shost)\n{\n\tshost->host_self_blocked = 1;\n}\nEXPORT_SYMBOL(scsi_block_requests);\n\n \nvoid scsi_unblock_requests(struct Scsi_Host *shost)\n{\n\tshost->host_self_blocked = 0;\n\tscsi_run_host_queues(shost);\n}\nEXPORT_SYMBOL(scsi_unblock_requests);\n\nvoid scsi_exit_queue(void)\n{\n\tkmem_cache_destroy(scsi_sense_cache);\n}\n\n \nint scsi_mode_select(struct scsi_device *sdev, int pf, int sp,\n\t\t     unsigned char *buffer, int len, int timeout, int retries,\n\t\t     struct scsi_mode_data *data, struct scsi_sense_hdr *sshdr)\n{\n\tunsigned char cmd[10];\n\tunsigned char *real_buffer;\n\tconst struct scsi_exec_args exec_args = {\n\t\t.sshdr = sshdr,\n\t};\n\tint ret;\n\n\tmemset(cmd, 0, sizeof(cmd));\n\tcmd[1] = (pf ? 0x10 : 0) | (sp ? 0x01 : 0);\n\n\t \n\tif (sdev->use_10_for_ms ||\n\t    len + 4 > 255 ||\n\t    data->block_descriptor_length > 255) {\n\t\tif (len > 65535 - 8)\n\t\t\treturn -EINVAL;\n\t\treal_buffer = kmalloc(8 + len, GFP_KERNEL);\n\t\tif (!real_buffer)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(real_buffer + 8, buffer, len);\n\t\tlen += 8;\n\t\treal_buffer[0] = 0;\n\t\treal_buffer[1] = 0;\n\t\treal_buffer[2] = data->medium_type;\n\t\treal_buffer[3] = data->device_specific;\n\t\treal_buffer[4] = data->longlba ? 0x01 : 0;\n\t\treal_buffer[5] = 0;\n\t\tput_unaligned_be16(data->block_descriptor_length,\n\t\t\t\t   &real_buffer[6]);\n\n\t\tcmd[0] = MODE_SELECT_10;\n\t\tput_unaligned_be16(len, &cmd[7]);\n\t} else {\n\t\tif (data->longlba)\n\t\t\treturn -EINVAL;\n\n\t\treal_buffer = kmalloc(4 + len, GFP_KERNEL);\n\t\tif (!real_buffer)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(real_buffer + 4, buffer, len);\n\t\tlen += 4;\n\t\treal_buffer[0] = 0;\n\t\treal_buffer[1] = data->medium_type;\n\t\treal_buffer[2] = data->device_specific;\n\t\treal_buffer[3] = data->block_descriptor_length;\n\n\t\tcmd[0] = MODE_SELECT;\n\t\tcmd[4] = len;\n\t}\n\n\tret = scsi_execute_cmd(sdev, cmd, REQ_OP_DRV_OUT, real_buffer, len,\n\t\t\t       timeout, retries, &exec_args);\n\tkfree(real_buffer);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(scsi_mode_select);\n\n \nint\nscsi_mode_sense(struct scsi_device *sdev, int dbd, int modepage, int subpage,\n\t\t  unsigned char *buffer, int len, int timeout, int retries,\n\t\t  struct scsi_mode_data *data, struct scsi_sense_hdr *sshdr)\n{\n\tunsigned char cmd[12];\n\tint use_10_for_ms;\n\tint header_length;\n\tint result, retry_count = retries;\n\tstruct scsi_sense_hdr my_sshdr;\n\tconst struct scsi_exec_args exec_args = {\n\t\t \n\t\t.sshdr = sshdr ? : &my_sshdr,\n\t};\n\n\tmemset(data, 0, sizeof(*data));\n\tmemset(&cmd[0], 0, 12);\n\n\tdbd = sdev->set_dbd_for_ms ? 8 : dbd;\n\tcmd[1] = dbd & 0x18;\t \n\tcmd[2] = modepage;\n\tcmd[3] = subpage;\n\n\tsshdr = exec_args.sshdr;\n\n retry:\n\tuse_10_for_ms = sdev->use_10_for_ms || len > 255;\n\n\tif (use_10_for_ms) {\n\t\tif (len < 8 || len > 65535)\n\t\t\treturn -EINVAL;\n\n\t\tcmd[0] = MODE_SENSE_10;\n\t\tput_unaligned_be16(len, &cmd[7]);\n\t\theader_length = 8;\n\t} else {\n\t\tif (len < 4)\n\t\t\treturn -EINVAL;\n\n\t\tcmd[0] = MODE_SENSE;\n\t\tcmd[4] = len;\n\t\theader_length = 4;\n\t}\n\n\tmemset(buffer, 0, len);\n\n\tresult = scsi_execute_cmd(sdev, cmd, REQ_OP_DRV_IN, buffer, len,\n\t\t\t\t  timeout, retries, &exec_args);\n\tif (result < 0)\n\t\treturn result;\n\n\t \n\n\tif (!scsi_status_is_good(result)) {\n\t\tif (scsi_sense_valid(sshdr)) {\n\t\t\tif ((sshdr->sense_key == ILLEGAL_REQUEST) &&\n\t\t\t    (sshdr->asc == 0x20) && (sshdr->ascq == 0)) {\n\t\t\t\t \n\t\t\t\tif (use_10_for_ms) {\n\t\t\t\t\tif (len > 255)\n\t\t\t\t\t\treturn -EIO;\n\t\t\t\t\tsdev->use_10_for_ms = 0;\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (scsi_status_is_check_condition(result) &&\n\t\t\t    sshdr->sense_key == UNIT_ATTENTION &&\n\t\t\t    retry_count) {\n\t\t\t\tretry_count--;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\t\treturn -EIO;\n\t}\n\tif (unlikely(buffer[0] == 0x86 && buffer[1] == 0x0b &&\n\t\t     (modepage == 6 || modepage == 8))) {\n\t\t \n\t\theader_length = 0;\n\t\tdata->length = 13;\n\t\tdata->medium_type = 0;\n\t\tdata->device_specific = 0;\n\t\tdata->longlba = 0;\n\t\tdata->block_descriptor_length = 0;\n\t} else if (use_10_for_ms) {\n\t\tdata->length = get_unaligned_be16(&buffer[0]) + 2;\n\t\tdata->medium_type = buffer[2];\n\t\tdata->device_specific = buffer[3];\n\t\tdata->longlba = buffer[4] & 0x01;\n\t\tdata->block_descriptor_length = get_unaligned_be16(&buffer[6]);\n\t} else {\n\t\tdata->length = buffer[0] + 1;\n\t\tdata->medium_type = buffer[1];\n\t\tdata->device_specific = buffer[2];\n\t\tdata->block_descriptor_length = buffer[3];\n\t}\n\tdata->header_length = header_length;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(scsi_mode_sense);\n\n \nint\nscsi_test_unit_ready(struct scsi_device *sdev, int timeout, int retries,\n\t\t     struct scsi_sense_hdr *sshdr)\n{\n\tchar cmd[] = {\n\t\tTEST_UNIT_READY, 0, 0, 0, 0, 0,\n\t};\n\tconst struct scsi_exec_args exec_args = {\n\t\t.sshdr = sshdr,\n\t};\n\tint result;\n\n\t \n\tdo {\n\t\tresult = scsi_execute_cmd(sdev, cmd, REQ_OP_DRV_IN, NULL, 0,\n\t\t\t\t\t  timeout, 1, &exec_args);\n\t\tif (sdev->removable && scsi_sense_valid(sshdr) &&\n\t\t    sshdr->sense_key == UNIT_ATTENTION)\n\t\t\tsdev->changed = 1;\n\t} while (scsi_sense_valid(sshdr) &&\n\t\t sshdr->sense_key == UNIT_ATTENTION && --retries);\n\n\treturn result;\n}\nEXPORT_SYMBOL(scsi_test_unit_ready);\n\n \nint\nscsi_device_set_state(struct scsi_device *sdev, enum scsi_device_state state)\n{\n\tenum scsi_device_state oldstate = sdev->sdev_state;\n\n\tif (state == oldstate)\n\t\treturn 0;\n\n\tswitch (state) {\n\tcase SDEV_CREATED:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED_BLOCK:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_RUNNING:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED:\n\t\tcase SDEV_OFFLINE:\n\t\tcase SDEV_TRANSPORT_OFFLINE:\n\t\tcase SDEV_QUIESCE:\n\t\tcase SDEV_BLOCK:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_QUIESCE:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_RUNNING:\n\t\tcase SDEV_OFFLINE:\n\t\tcase SDEV_TRANSPORT_OFFLINE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_OFFLINE:\n\tcase SDEV_TRANSPORT_OFFLINE:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED:\n\t\tcase SDEV_RUNNING:\n\t\tcase SDEV_QUIESCE:\n\t\tcase SDEV_BLOCK:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_BLOCK:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_RUNNING:\n\t\tcase SDEV_CREATED_BLOCK:\n\t\tcase SDEV_QUIESCE:\n\t\tcase SDEV_OFFLINE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_CREATED_BLOCK:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_CANCEL:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED:\n\t\tcase SDEV_RUNNING:\n\t\tcase SDEV_QUIESCE:\n\t\tcase SDEV_OFFLINE:\n\t\tcase SDEV_TRANSPORT_OFFLINE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\tcase SDEV_DEL:\n\t\tswitch (oldstate) {\n\t\tcase SDEV_CREATED:\n\t\tcase SDEV_RUNNING:\n\t\tcase SDEV_OFFLINE:\n\t\tcase SDEV_TRANSPORT_OFFLINE:\n\t\tcase SDEV_CANCEL:\n\t\tcase SDEV_BLOCK:\n\t\tcase SDEV_CREATED_BLOCK:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto illegal;\n\t\t}\n\t\tbreak;\n\n\t}\n\tsdev->offline_already = false;\n\tsdev->sdev_state = state;\n\treturn 0;\n\n illegal:\n\tSCSI_LOG_ERROR_RECOVERY(1,\n\t\t\t\tsdev_printk(KERN_ERR, sdev,\n\t\t\t\t\t    \"Illegal state transition %s->%s\",\n\t\t\t\t\t    scsi_device_state_name(oldstate),\n\t\t\t\t\t    scsi_device_state_name(state))\n\t\t\t\t);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(scsi_device_set_state);\n\n \nstatic void scsi_evt_emit(struct scsi_device *sdev, struct scsi_event *evt)\n{\n\tint idx = 0;\n\tchar *envp[3];\n\n\tswitch (evt->evt_type) {\n\tcase SDEV_EVT_MEDIA_CHANGE:\n\t\tenvp[idx++] = \"SDEV_MEDIA_CHANGE=1\";\n\t\tbreak;\n\tcase SDEV_EVT_INQUIRY_CHANGE_REPORTED:\n\t\tscsi_rescan_device(sdev);\n\t\tenvp[idx++] = \"SDEV_UA=INQUIRY_DATA_HAS_CHANGED\";\n\t\tbreak;\n\tcase SDEV_EVT_CAPACITY_CHANGE_REPORTED:\n\t\tenvp[idx++] = \"SDEV_UA=CAPACITY_DATA_HAS_CHANGED\";\n\t\tbreak;\n\tcase SDEV_EVT_SOFT_THRESHOLD_REACHED_REPORTED:\n\t       envp[idx++] = \"SDEV_UA=THIN_PROVISIONING_SOFT_THRESHOLD_REACHED\";\n\t\tbreak;\n\tcase SDEV_EVT_MODE_PARAMETER_CHANGE_REPORTED:\n\t\tenvp[idx++] = \"SDEV_UA=MODE_PARAMETERS_CHANGED\";\n\t\tbreak;\n\tcase SDEV_EVT_LUN_CHANGE_REPORTED:\n\t\tenvp[idx++] = \"SDEV_UA=REPORTED_LUNS_DATA_HAS_CHANGED\";\n\t\tbreak;\n\tcase SDEV_EVT_ALUA_STATE_CHANGE_REPORTED:\n\t\tenvp[idx++] = \"SDEV_UA=ASYMMETRIC_ACCESS_STATE_CHANGED\";\n\t\tbreak;\n\tcase SDEV_EVT_POWER_ON_RESET_OCCURRED:\n\t\tenvp[idx++] = \"SDEV_UA=POWER_ON_RESET_OCCURRED\";\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\tenvp[idx++] = NULL;\n\n\tkobject_uevent_env(&sdev->sdev_gendev.kobj, KOBJ_CHANGE, envp);\n}\n\n \nvoid scsi_evt_thread(struct work_struct *work)\n{\n\tstruct scsi_device *sdev;\n\tenum scsi_device_event evt_type;\n\tLIST_HEAD(event_list);\n\n\tsdev = container_of(work, struct scsi_device, event_work);\n\n\tfor (evt_type = SDEV_EVT_FIRST; evt_type <= SDEV_EVT_LAST; evt_type++)\n\t\tif (test_and_clear_bit(evt_type, sdev->pending_events))\n\t\t\tsdev_evt_send_simple(sdev, evt_type, GFP_KERNEL);\n\n\twhile (1) {\n\t\tstruct scsi_event *evt;\n\t\tstruct list_head *this, *tmp;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&sdev->list_lock, flags);\n\t\tlist_splice_init(&sdev->event_list, &event_list);\n\t\tspin_unlock_irqrestore(&sdev->list_lock, flags);\n\n\t\tif (list_empty(&event_list))\n\t\t\tbreak;\n\n\t\tlist_for_each_safe(this, tmp, &event_list) {\n\t\t\tevt = list_entry(this, struct scsi_event, node);\n\t\t\tlist_del(&evt->node);\n\t\t\tscsi_evt_emit(sdev, evt);\n\t\t\tkfree(evt);\n\t\t}\n\t}\n}\n\n \nvoid sdev_evt_send(struct scsi_device *sdev, struct scsi_event *evt)\n{\n\tunsigned long flags;\n\n#if 0\n\t \n\tif (!test_bit(evt->evt_type, sdev->supported_events)) {\n\t\tkfree(evt);\n\t\treturn;\n\t}\n#endif\n\n\tspin_lock_irqsave(&sdev->list_lock, flags);\n\tlist_add_tail(&evt->node, &sdev->event_list);\n\tschedule_work(&sdev->event_work);\n\tspin_unlock_irqrestore(&sdev->list_lock, flags);\n}\nEXPORT_SYMBOL_GPL(sdev_evt_send);\n\n \nstruct scsi_event *sdev_evt_alloc(enum scsi_device_event evt_type,\n\t\t\t\t  gfp_t gfpflags)\n{\n\tstruct scsi_event *evt = kzalloc(sizeof(struct scsi_event), gfpflags);\n\tif (!evt)\n\t\treturn NULL;\n\n\tevt->evt_type = evt_type;\n\tINIT_LIST_HEAD(&evt->node);\n\n\t \n\tswitch (evt_type) {\n\tcase SDEV_EVT_MEDIA_CHANGE:\n\tcase SDEV_EVT_INQUIRY_CHANGE_REPORTED:\n\tcase SDEV_EVT_CAPACITY_CHANGE_REPORTED:\n\tcase SDEV_EVT_SOFT_THRESHOLD_REACHED_REPORTED:\n\tcase SDEV_EVT_MODE_PARAMETER_CHANGE_REPORTED:\n\tcase SDEV_EVT_LUN_CHANGE_REPORTED:\n\tcase SDEV_EVT_ALUA_STATE_CHANGE_REPORTED:\n\tcase SDEV_EVT_POWER_ON_RESET_OCCURRED:\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\treturn evt;\n}\nEXPORT_SYMBOL_GPL(sdev_evt_alloc);\n\n \nvoid sdev_evt_send_simple(struct scsi_device *sdev,\n\t\t\t  enum scsi_device_event evt_type, gfp_t gfpflags)\n{\n\tstruct scsi_event *evt = sdev_evt_alloc(evt_type, gfpflags);\n\tif (!evt) {\n\t\tsdev_printk(KERN_ERR, sdev, \"event %d eaten due to OOM\\n\",\n\t\t\t    evt_type);\n\t\treturn;\n\t}\n\n\tsdev_evt_send(sdev, evt);\n}\nEXPORT_SYMBOL_GPL(sdev_evt_send_simple);\n\n \nint\nscsi_device_quiesce(struct scsi_device *sdev)\n{\n\tstruct request_queue *q = sdev->request_queue;\n\tint err;\n\n\t \n\tWARN_ON_ONCE(sdev->quiesced_by && sdev->quiesced_by != current);\n\n\tif (sdev->quiesced_by == current)\n\t\treturn 0;\n\n\tblk_set_pm_only(q);\n\n\tblk_mq_freeze_queue(q);\n\t \n\tsynchronize_rcu();\n\tblk_mq_unfreeze_queue(q);\n\n\tmutex_lock(&sdev->state_mutex);\n\terr = scsi_device_set_state(sdev, SDEV_QUIESCE);\n\tif (err == 0)\n\t\tsdev->quiesced_by = current;\n\telse\n\t\tblk_clear_pm_only(q);\n\tmutex_unlock(&sdev->state_mutex);\n\n\treturn err;\n}\nEXPORT_SYMBOL(scsi_device_quiesce);\n\n \nvoid scsi_device_resume(struct scsi_device *sdev)\n{\n\t \n\tmutex_lock(&sdev->state_mutex);\n\tif (sdev->sdev_state == SDEV_QUIESCE)\n\t\tscsi_device_set_state(sdev, SDEV_RUNNING);\n\tif (sdev->quiesced_by) {\n\t\tsdev->quiesced_by = NULL;\n\t\tblk_clear_pm_only(sdev->request_queue);\n\t}\n\tmutex_unlock(&sdev->state_mutex);\n}\nEXPORT_SYMBOL(scsi_device_resume);\n\nstatic void\ndevice_quiesce_fn(struct scsi_device *sdev, void *data)\n{\n\tscsi_device_quiesce(sdev);\n}\n\nvoid\nscsi_target_quiesce(struct scsi_target *starget)\n{\n\tstarget_for_each_device(starget, NULL, device_quiesce_fn);\n}\nEXPORT_SYMBOL(scsi_target_quiesce);\n\nstatic void\ndevice_resume_fn(struct scsi_device *sdev, void *data)\n{\n\tscsi_device_resume(sdev);\n}\n\nvoid\nscsi_target_resume(struct scsi_target *starget)\n{\n\tstarget_for_each_device(starget, NULL, device_resume_fn);\n}\nEXPORT_SYMBOL(scsi_target_resume);\n\nstatic int __scsi_internal_device_block_nowait(struct scsi_device *sdev)\n{\n\tif (scsi_device_set_state(sdev, SDEV_BLOCK))\n\t\treturn scsi_device_set_state(sdev, SDEV_CREATED_BLOCK);\n\n\treturn 0;\n}\n\nvoid scsi_start_queue(struct scsi_device *sdev)\n{\n\tif (cmpxchg(&sdev->queue_stopped, 1, 0))\n\t\tblk_mq_unquiesce_queue(sdev->request_queue);\n}\n\nstatic void scsi_stop_queue(struct scsi_device *sdev)\n{\n\t \n\tif (!cmpxchg(&sdev->queue_stopped, 0, 1))\n\t\tblk_mq_quiesce_queue_nowait(sdev->request_queue);\n}\n\n \nint scsi_internal_device_block_nowait(struct scsi_device *sdev)\n{\n\tint ret = __scsi_internal_device_block_nowait(sdev);\n\n\t \n\tif (!ret)\n\t\tscsi_stop_queue(sdev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(scsi_internal_device_block_nowait);\n\n \nstatic void scsi_device_block(struct scsi_device *sdev, void *data)\n{\n\tint err;\n\tenum scsi_device_state state;\n\n\tmutex_lock(&sdev->state_mutex);\n\terr = __scsi_internal_device_block_nowait(sdev);\n\tstate = sdev->sdev_state;\n\tif (err == 0)\n\t\t \n\t\tscsi_stop_queue(sdev);\n\n\tmutex_unlock(&sdev->state_mutex);\n\n\tWARN_ONCE(err, \"%s: failed to block %s in state %d\\n\",\n\t\t  __func__, dev_name(&sdev->sdev_gendev), state);\n}\n\n \nint scsi_internal_device_unblock_nowait(struct scsi_device *sdev,\n\t\t\t\t\tenum scsi_device_state new_state)\n{\n\tswitch (new_state) {\n\tcase SDEV_RUNNING:\n\tcase SDEV_TRANSPORT_OFFLINE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tswitch (sdev->sdev_state) {\n\tcase SDEV_BLOCK:\n\tcase SDEV_TRANSPORT_OFFLINE:\n\t\tsdev->sdev_state = new_state;\n\t\tbreak;\n\tcase SDEV_CREATED_BLOCK:\n\t\tif (new_state == SDEV_TRANSPORT_OFFLINE ||\n\t\t    new_state == SDEV_OFFLINE)\n\t\t\tsdev->sdev_state = new_state;\n\t\telse\n\t\t\tsdev->sdev_state = SDEV_CREATED;\n\t\tbreak;\n\tcase SDEV_CANCEL:\n\tcase SDEV_OFFLINE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tscsi_start_queue(sdev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(scsi_internal_device_unblock_nowait);\n\n \nstatic int scsi_internal_device_unblock(struct scsi_device *sdev,\n\t\t\t\t\tenum scsi_device_state new_state)\n{\n\tint ret;\n\n\tmutex_lock(&sdev->state_mutex);\n\tret = scsi_internal_device_unblock_nowait(sdev, new_state);\n\tmutex_unlock(&sdev->state_mutex);\n\n\treturn ret;\n}\n\nstatic int\ntarget_block(struct device *dev, void *data)\n{\n\tif (scsi_is_target_device(dev))\n\t\tstarget_for_each_device(to_scsi_target(dev), NULL,\n\t\t\t\t\tscsi_device_block);\n\treturn 0;\n}\n\n \nvoid\nscsi_block_targets(struct Scsi_Host *shost, struct device *dev)\n{\n\tWARN_ON_ONCE(scsi_is_target_device(dev));\n\tdevice_for_each_child(dev, NULL, target_block);\n\tblk_mq_wait_quiesce_done(&shost->tag_set);\n}\nEXPORT_SYMBOL_GPL(scsi_block_targets);\n\nstatic void\ndevice_unblock(struct scsi_device *sdev, void *data)\n{\n\tscsi_internal_device_unblock(sdev, *(enum scsi_device_state *)data);\n}\n\nstatic int\ntarget_unblock(struct device *dev, void *data)\n{\n\tif (scsi_is_target_device(dev))\n\t\tstarget_for_each_device(to_scsi_target(dev), data,\n\t\t\t\t\tdevice_unblock);\n\treturn 0;\n}\n\nvoid\nscsi_target_unblock(struct device *dev, enum scsi_device_state new_state)\n{\n\tif (scsi_is_target_device(dev))\n\t\tstarget_for_each_device(to_scsi_target(dev), &new_state,\n\t\t\t\t\tdevice_unblock);\n\telse\n\t\tdevice_for_each_child(dev, &new_state, target_unblock);\n}\nEXPORT_SYMBOL_GPL(scsi_target_unblock);\n\n \nint\nscsi_host_block(struct Scsi_Host *shost)\n{\n\tstruct scsi_device *sdev;\n\tint ret;\n\n\t \n\tshost_for_each_device(sdev, shost) {\n\t\tmutex_lock(&sdev->state_mutex);\n\t\tret = scsi_internal_device_block_nowait(sdev);\n\t\tmutex_unlock(&sdev->state_mutex);\n\t\tif (ret) {\n\t\t\tscsi_device_put(sdev);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tblk_mq_wait_quiesce_done(&shost->tag_set);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(scsi_host_block);\n\nint\nscsi_host_unblock(struct Scsi_Host *shost, int new_state)\n{\n\tstruct scsi_device *sdev;\n\tint ret = 0;\n\n\tshost_for_each_device(sdev, shost) {\n\t\tret = scsi_internal_device_unblock(sdev, new_state);\n\t\tif (ret) {\n\t\t\tscsi_device_put(sdev);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(scsi_host_unblock);\n\n \nvoid *scsi_kmap_atomic_sg(struct scatterlist *sgl, int sg_count,\n\t\t\t  size_t *offset, size_t *len)\n{\n\tint i;\n\tsize_t sg_len = 0, len_complete = 0;\n\tstruct scatterlist *sg;\n\tstruct page *page;\n\n\tWARN_ON(!irqs_disabled());\n\n\tfor_each_sg(sgl, sg, sg_count, i) {\n\t\tlen_complete = sg_len;  \n\t\tsg_len += sg->length;\n\t\tif (sg_len > *offset)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(i == sg_count)) {\n\t\tprintk(KERN_ERR \"%s: Bytes in sg: %zu, requested offset %zu, \"\n\t\t\t\"elements %d\\n\",\n\t\t       __func__, sg_len, *offset, sg_count);\n\t\tWARN_ON(1);\n\t\treturn NULL;\n\t}\n\n\t \n\t*offset = *offset - len_complete + sg->offset;\n\n\t \n\tpage = nth_page(sg_page(sg), (*offset >> PAGE_SHIFT));\n\t*offset &= ~PAGE_MASK;\n\n\t \n\tsg_len = PAGE_SIZE - *offset;\n\tif (*len > sg_len)\n\t\t*len = sg_len;\n\n\treturn kmap_atomic(page);\n}\nEXPORT_SYMBOL(scsi_kmap_atomic_sg);\n\n \nvoid scsi_kunmap_atomic_sg(void *virt)\n{\n\tkunmap_atomic(virt);\n}\nEXPORT_SYMBOL(scsi_kunmap_atomic_sg);\n\nvoid sdev_disable_disk_events(struct scsi_device *sdev)\n{\n\tatomic_inc(&sdev->disk_events_disable_depth);\n}\nEXPORT_SYMBOL(sdev_disable_disk_events);\n\nvoid sdev_enable_disk_events(struct scsi_device *sdev)\n{\n\tif (WARN_ON_ONCE(atomic_read(&sdev->disk_events_disable_depth) <= 0))\n\t\treturn;\n\tatomic_dec(&sdev->disk_events_disable_depth);\n}\nEXPORT_SYMBOL(sdev_enable_disk_events);\n\nstatic unsigned char designator_prio(const unsigned char *d)\n{\n\tif (d[1] & 0x30)\n\t\t \n\t\treturn 0;\n\n\tif (d[3] == 0)\n\t\t \n\t\treturn 0;\n\n\t \n\n\tswitch (d[1] & 0xf) {\n\tcase 8:\n\t\t \n\t\treturn 9;\n\tcase 3:\n\t\tswitch (d[4] >> 4) {\n\t\tcase 6:\n\t\t\t \n\t\t\treturn 8;\n\t\tcase 5:\n\t\t\t \n\t\t\treturn 5;\n\t\tcase 4:\n\t\t\t \n\t\t\treturn 4;\n\t\tcase 3:\n\t\t\t \n\t\t\treturn 1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (d[3]) {\n\t\tcase 16:\n\t\t\t \n\t\t\treturn 7;\n\t\tcase 12:\n\t\t\t \n\t\t\treturn 6;\n\t\tcase 8:\n\t\t\t \n\t\t\treturn 3;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 1:\n\t\t \n\t\treturn 1;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n \nint scsi_vpd_lun_id(struct scsi_device *sdev, char *id, size_t id_len)\n{\n\tu8 cur_id_prio = 0;\n\tu8 cur_id_size = 0;\n\tconst unsigned char *d, *cur_id_str;\n\tconst struct scsi_vpd *vpd_pg83;\n\tint id_size = -EINVAL;\n\n\trcu_read_lock();\n\tvpd_pg83 = rcu_dereference(sdev->vpd_pg83);\n\tif (!vpd_pg83) {\n\t\trcu_read_unlock();\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tif (id_len < 21) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(id, 0, id_len);\n\tfor (d = vpd_pg83->data + 4;\n\t     d < vpd_pg83->data + vpd_pg83->len;\n\t     d += d[3] + 4) {\n\t\tu8 prio = designator_prio(d);\n\n\t\tif (prio == 0 || cur_id_prio > prio)\n\t\t\tcontinue;\n\n\t\tswitch (d[1] & 0xf) {\n\t\tcase 0x1:\n\t\t\t \n\t\t\tif (cur_id_size > d[3])\n\t\t\t\tbreak;\n\t\t\tcur_id_prio = prio;\n\t\t\tcur_id_size = d[3];\n\t\t\tif (cur_id_size + 4 > id_len)\n\t\t\t\tcur_id_size = id_len - 4;\n\t\t\tcur_id_str = d + 4;\n\t\t\tid_size = snprintf(id, id_len, \"t10.%*pE\",\n\t\t\t\t\t   cur_id_size, cur_id_str);\n\t\t\tbreak;\n\t\tcase 0x2:\n\t\t\t \n\t\t\tcur_id_prio = prio;\n\t\t\tcur_id_size = d[3];\n\t\t\tcur_id_str = d + 4;\n\t\t\tswitch (cur_id_size) {\n\t\t\tcase 8:\n\t\t\t\tid_size = snprintf(id, id_len,\n\t\t\t\t\t\t   \"eui.%8phN\",\n\t\t\t\t\t\t   cur_id_str);\n\t\t\t\tbreak;\n\t\t\tcase 12:\n\t\t\t\tid_size = snprintf(id, id_len,\n\t\t\t\t\t\t   \"eui.%12phN\",\n\t\t\t\t\t\t   cur_id_str);\n\t\t\t\tbreak;\n\t\t\tcase 16:\n\t\t\t\tid_size = snprintf(id, id_len,\n\t\t\t\t\t\t   \"eui.%16phN\",\n\t\t\t\t\t\t   cur_id_str);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x3:\n\t\t\t \n\t\t\tcur_id_prio = prio;\n\t\t\tcur_id_size = d[3];\n\t\t\tcur_id_str = d + 4;\n\t\t\tswitch (cur_id_size) {\n\t\t\tcase 8:\n\t\t\t\tid_size = snprintf(id, id_len,\n\t\t\t\t\t\t   \"naa.%8phN\",\n\t\t\t\t\t\t   cur_id_str);\n\t\t\t\tbreak;\n\t\t\tcase 16:\n\t\t\t\tid_size = snprintf(id, id_len,\n\t\t\t\t\t\t   \"naa.%16phN\",\n\t\t\t\t\t\t   cur_id_str);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x8:\n\t\t\t \n\t\t\tif (cur_id_size > d[3])\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (d[3] > id_len) {\n\t\t\t\tprio = 2;\n\t\t\t\tif (cur_id_prio > prio)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcur_id_prio = prio;\n\t\t\tcur_id_size = id_size = d[3];\n\t\t\tcur_id_str = d + 4;\n\t\t\tif (cur_id_size >= id_len)\n\t\t\t\tcur_id_size = id_len - 1;\n\t\t\tmemcpy(id, cur_id_str, cur_id_size);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn id_size;\n}\nEXPORT_SYMBOL(scsi_vpd_lun_id);\n\n \nint scsi_vpd_tpg_id(struct scsi_device *sdev, int *rel_id)\n{\n\tconst unsigned char *d;\n\tconst struct scsi_vpd *vpd_pg83;\n\tint group_id = -EAGAIN, rel_port = -1;\n\n\trcu_read_lock();\n\tvpd_pg83 = rcu_dereference(sdev->vpd_pg83);\n\tif (!vpd_pg83) {\n\t\trcu_read_unlock();\n\t\treturn -ENXIO;\n\t}\n\n\td = vpd_pg83->data + 4;\n\twhile (d < vpd_pg83->data + vpd_pg83->len) {\n\t\tswitch (d[1] & 0xf) {\n\t\tcase 0x4:\n\t\t\t \n\t\t\trel_port = get_unaligned_be16(&d[6]);\n\t\t\tbreak;\n\t\tcase 0x5:\n\t\t\t \n\t\t\tgroup_id = get_unaligned_be16(&d[6]);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\td += d[3] + 4;\n\t}\n\trcu_read_unlock();\n\n\tif (group_id >= 0 && rel_id && rel_port != -1)\n\t\t*rel_id = rel_port;\n\n\treturn group_id;\n}\nEXPORT_SYMBOL(scsi_vpd_tpg_id);\n\n \nvoid scsi_build_sense(struct scsi_cmnd *scmd, int desc, u8 key, u8 asc, u8 ascq)\n{\n\tscsi_build_sense_buffer(desc, scmd->sense_buffer, key, asc, ascq);\n\tscmd->result = SAM_STAT_CHECK_CONDITION;\n}\nEXPORT_SYMBOL_GPL(scsi_build_sense);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}