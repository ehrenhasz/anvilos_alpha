{
  "module_name": "qedf_io.c",
  "hash_id": "fe851b3dd08aeaa327ba9ccc652f8e54870d45082e413c1abd73834a4523a022",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/qedf/qedf_io.c",
  "human_readable_source": "\n \n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include \"qedf.h\"\n#include <scsi/scsi_tcq.h>\n\nvoid qedf_cmd_timer_set(struct qedf_ctx *qedf, struct qedf_ioreq *io_req,\n\tunsigned int timer_msec)\n{\n\tqueue_delayed_work(qedf->timer_work_queue, &io_req->timeout_work,\n\t    msecs_to_jiffies(timer_msec));\n}\n\nstatic void qedf_cmd_timeout(struct work_struct *work)\n{\n\n\tstruct qedf_ioreq *io_req =\n\t    container_of(work, struct qedf_ioreq, timeout_work.work);\n\tstruct qedf_ctx *qedf;\n\tstruct qedf_rport *fcport;\n\n\tfcport = io_req->fcport;\n\tif (io_req->fcport == NULL) {\n\t\tQEDF_INFO(NULL, QEDF_LOG_IO,  \"fcport is NULL.\\n\");\n\t\treturn;\n\t}\n\n\tqedf = fcport->qedf;\n\n\tswitch (io_req->cmd_type) {\n\tcase QEDF_ABTS:\n\t\tif (qedf == NULL) {\n\t\t\tQEDF_INFO(NULL, QEDF_LOG_IO,\n\t\t\t\t  \"qedf is NULL for ABTS xid=0x%x.\\n\",\n\t\t\t\t  io_req->xid);\n\t\t\treturn;\n\t\t}\n\n\t\tQEDF_ERR((&qedf->dbg_ctx), \"ABTS timeout, xid=0x%x.\\n\",\n\t\t    io_req->xid);\n\t\t \n\t\tqedf_initiate_cleanup(io_req, true);\n\t\tcomplete(&io_req->abts_done);\n\n\t\t \n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\n\t\t \n\t\tclear_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\n\n\t\t \n\t\tqedf_restart_rport(fcport);\n\t\tbreak;\n\tcase QEDF_ELS:\n\t\tif (!qedf) {\n\t\t\tQEDF_INFO(NULL, QEDF_LOG_IO,\n\t\t\t\t  \"qedf is NULL for ELS xid=0x%x.\\n\",\n\t\t\t\t  io_req->xid);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\n\t\tkref_get(&io_req->refcount);\n\t\t \n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"ELS timeout, xid=0x%x.\\n\",\n\t\t\t  io_req->xid);\n\t\tqedf_initiate_cleanup(io_req, true);\n\t\tio_req->event = QEDF_IOREQ_EV_ELS_TMO;\n\t\t \n\t\tif (io_req->cb_func && io_req->cb_arg) {\n\t\t\tio_req->cb_func(io_req->cb_arg);\n\t\t\tio_req->cb_arg = NULL;\n\t\t}\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\tbreak;\n\tcase QEDF_SEQ_CLEANUP:\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Sequence cleanup timeout, \"\n\t\t    \"xid=0x%x.\\n\", io_req->xid);\n\t\tqedf_initiate_cleanup(io_req, true);\n\t\tio_req->event = QEDF_IOREQ_EV_ELS_TMO;\n\t\tqedf_process_seq_cleanup_compl(qedf, NULL, io_req);\n\t\tbreak;\n\tdefault:\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"Hit default case, xid=0x%x.\\n\", io_req->xid);\n\t\tbreak;\n\t}\n}\n\nvoid qedf_cmd_mgr_free(struct qedf_cmd_mgr *cmgr)\n{\n\tstruct io_bdt *bdt_info;\n\tstruct qedf_ctx *qedf = cmgr->qedf;\n\tsize_t bd_tbl_sz;\n\tu16 min_xid = 0;\n\tu16 max_xid = (FCOE_PARAMS_NUM_TASKS - 1);\n\tint num_ios;\n\tint i;\n\tstruct qedf_ioreq *io_req;\n\n\tnum_ios = max_xid - min_xid + 1;\n\n\t \n\tif (!cmgr->io_bdt_pool) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"io_bdt_pool is NULL.\\n\");\n\t\tgoto free_cmd_pool;\n\t}\n\n\tbd_tbl_sz = QEDF_MAX_BDS_PER_CMD * sizeof(struct scsi_sge);\n\tfor (i = 0; i < num_ios; i++) {\n\t\tbdt_info = cmgr->io_bdt_pool[i];\n\t\tif (bdt_info->bd_tbl) {\n\t\t\tdma_free_coherent(&qedf->pdev->dev, bd_tbl_sz,\n\t\t\t    bdt_info->bd_tbl, bdt_info->bd_tbl_dma);\n\t\t\tbdt_info->bd_tbl = NULL;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < num_ios; i++) {\n\t\tkfree(cmgr->io_bdt_pool[i]);\n\t\tcmgr->io_bdt_pool[i] = NULL;\n\t}\n\n\tkfree(cmgr->io_bdt_pool);\n\tcmgr->io_bdt_pool = NULL;\n\nfree_cmd_pool:\n\n\tfor (i = 0; i < num_ios; i++) {\n\t\tio_req = &cmgr->cmds[i];\n\t\tkfree(io_req->sgl_task_params);\n\t\tkfree(io_req->task_params);\n\t\t \n\t\tif (io_req->sense_buffer)\n\t\t\tdma_free_coherent(&qedf->pdev->dev,\n\t\t\t    QEDF_SCSI_SENSE_BUFFERSIZE, io_req->sense_buffer,\n\t\t\t    io_req->sense_buffer_dma);\n\t\tcancel_delayed_work_sync(&io_req->rrq_work);\n\t}\n\n\t \n\tvfree(cmgr);\n}\n\nstatic void qedf_handle_rrq(struct work_struct *work)\n{\n\tstruct qedf_ioreq *io_req =\n\t    container_of(work, struct qedf_ioreq, rrq_work.work);\n\n\tatomic_set(&io_req->state, QEDFC_CMD_ST_RRQ_ACTIVE);\n\tqedf_send_rrq(io_req);\n\n}\n\nstruct qedf_cmd_mgr *qedf_cmd_mgr_alloc(struct qedf_ctx *qedf)\n{\n\tstruct qedf_cmd_mgr *cmgr;\n\tstruct io_bdt *bdt_info;\n\tstruct qedf_ioreq *io_req;\n\tu16 xid;\n\tint i;\n\tint num_ios;\n\tu16 min_xid = 0;\n\tu16 max_xid = (FCOE_PARAMS_NUM_TASKS - 1);\n\n\t \n\tif (!qedf->num_queues) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"num_queues is not set.\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (max_xid <= min_xid || max_xid == FC_XID_UNKNOWN) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"Invalid min_xid 0x%x and \"\n\t\t\t   \"max_xid 0x%x.\\n\", min_xid, max_xid);\n\t\treturn NULL;\n\t}\n\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_DISC, \"min xid 0x%x, max xid \"\n\t\t   \"0x%x.\\n\", min_xid, max_xid);\n\n\tnum_ios = max_xid - min_xid + 1;\n\n\tcmgr = vzalloc(sizeof(struct qedf_cmd_mgr));\n\tif (!cmgr) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"Failed to alloc cmd mgr.\\n\");\n\t\treturn NULL;\n\t}\n\n\tcmgr->qedf = qedf;\n\tspin_lock_init(&cmgr->lock);\n\n\t \n\txid = 0;\n\n\tfor (i = 0; i < num_ios; i++) {\n\t\tio_req = &cmgr->cmds[i];\n\t\tINIT_DELAYED_WORK(&io_req->timeout_work, qedf_cmd_timeout);\n\n\t\tio_req->xid = xid++;\n\n\t\tINIT_DELAYED_WORK(&io_req->rrq_work, qedf_handle_rrq);\n\n\t\t \n\t\tio_req->sense_buffer = dma_alloc_coherent(&qedf->pdev->dev,\n\t\t    QEDF_SCSI_SENSE_BUFFERSIZE, &io_req->sense_buffer_dma,\n\t\t    GFP_KERNEL);\n\t\tif (!io_req->sense_buffer) {\n\t\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\t \"Failed to alloc sense buffer.\\n\");\n\t\t\tgoto mem_err;\n\t\t}\n\n\t\t \n\t\tio_req->task_params = kzalloc(sizeof(*io_req->task_params),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!io_req->task_params) {\n\t\t\tQEDF_ERR(&(qedf->dbg_ctx),\n\t\t\t\t \"Failed to allocate task_params for xid=0x%x\\n\",\n\t\t\t\t i);\n\t\t\tgoto mem_err;\n\t\t}\n\n\t\t \n\t\tio_req->sgl_task_params = kzalloc(\n\t\t    sizeof(struct scsi_sgl_task_params), GFP_KERNEL);\n\t\tif (!io_req->sgl_task_params) {\n\t\t\tQEDF_ERR(&(qedf->dbg_ctx),\n\t\t\t\t \"Failed to allocate sgl_task_params for xid=0x%x\\n\",\n\t\t\t\t i);\n\t\t\tgoto mem_err;\n\t\t}\n\t}\n\n\t \n\tcmgr->io_bdt_pool = kmalloc_array(num_ios, sizeof(struct io_bdt *),\n\t    GFP_KERNEL);\n\n\tif (!cmgr->io_bdt_pool) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"Failed to alloc io_bdt_pool.\\n\");\n\t\tgoto mem_err;\n\t}\n\n\tfor (i = 0; i < num_ios; i++) {\n\t\tcmgr->io_bdt_pool[i] = kmalloc(sizeof(struct io_bdt),\n\t\t    GFP_KERNEL);\n\t\tif (!cmgr->io_bdt_pool[i]) {\n\t\t\tQEDF_WARN(&(qedf->dbg_ctx),\n\t\t\t\t  \"Failed to alloc io_bdt_pool[%d].\\n\", i);\n\t\t\tgoto mem_err;\n\t\t}\n\t}\n\n\tfor (i = 0; i < num_ios; i++) {\n\t\tbdt_info = cmgr->io_bdt_pool[i];\n\t\tbdt_info->bd_tbl = dma_alloc_coherent(&qedf->pdev->dev,\n\t\t    QEDF_MAX_BDS_PER_CMD * sizeof(struct scsi_sge),\n\t\t    &bdt_info->bd_tbl_dma, GFP_KERNEL);\n\t\tif (!bdt_info->bd_tbl) {\n\t\t\tQEDF_WARN(&(qedf->dbg_ctx),\n\t\t\t\t  \"Failed to alloc bdt_tbl[%d].\\n\", i);\n\t\t\tgoto mem_err;\n\t\t}\n\t}\n\tatomic_set(&cmgr->free_list_cnt, num_ios);\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t    \"cmgr->free_list_cnt=%d.\\n\",\n\t    atomic_read(&cmgr->free_list_cnt));\n\n\treturn cmgr;\n\nmem_err:\n\tqedf_cmd_mgr_free(cmgr);\n\treturn NULL;\n}\n\nstruct qedf_ioreq *qedf_alloc_cmd(struct qedf_rport *fcport, u8 cmd_type)\n{\n\tstruct qedf_ctx *qedf = fcport->qedf;\n\tstruct qedf_cmd_mgr *cmd_mgr = qedf->cmd_mgr;\n\tstruct qedf_ioreq *io_req = NULL;\n\tstruct io_bdt *bd_tbl;\n\tu16 xid;\n\tuint32_t free_sqes;\n\tint i;\n\tunsigned long flags;\n\n\tfree_sqes = atomic_read(&fcport->free_sqes);\n\n\tif (!free_sqes) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Returning NULL, free_sqes=%d.\\n \",\n\t\t    free_sqes);\n\t\tgoto out_failed;\n\t}\n\n\t \n\tif ((atomic_read(&fcport->num_active_ios) >=\n\t    NUM_RW_TASKS_PER_CONNECTION)) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Returning NULL, num_active_ios=%d.\\n\",\n\t\t    atomic_read(&fcport->num_active_ios));\n\t\tgoto out_failed;\n\t}\n\n\t \n\tif (atomic_read(&cmd_mgr->free_list_cnt) <= GBL_RSVD_TASKS) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Returning NULL, free_list_cnt=%d.\\n\",\n\t\t    atomic_read(&cmd_mgr->free_list_cnt));\n\t\tgoto out_failed;\n\t}\n\n\tspin_lock_irqsave(&cmd_mgr->lock, flags);\n\tfor (i = 0; i < FCOE_PARAMS_NUM_TASKS; i++) {\n\t\tio_req = &cmd_mgr->cmds[cmd_mgr->idx];\n\t\tcmd_mgr->idx++;\n\t\tif (cmd_mgr->idx == FCOE_PARAMS_NUM_TASKS)\n\t\t\tcmd_mgr->idx = 0;\n\n\t\t \n\t\tif (!io_req->alloc)\n\t\t\tbreak;\n\t}\n\n\tif (i == FCOE_PARAMS_NUM_TASKS) {\n\t\tspin_unlock_irqrestore(&cmd_mgr->lock, flags);\n\t\tgoto out_failed;\n\t}\n\n\tif (test_bit(QEDF_CMD_DIRTY, &io_req->flags))\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"io_req found to be dirty ox_id = 0x%x.\\n\",\n\t\t\t io_req->xid);\n\n\t \n\tio_req->flags = 0;\n\tio_req->alloc = 1;\n\tspin_unlock_irqrestore(&cmd_mgr->lock, flags);\n\n\tatomic_inc(&fcport->num_active_ios);\n\tatomic_dec(&fcport->free_sqes);\n\txid = io_req->xid;\n\tatomic_dec(&cmd_mgr->free_list_cnt);\n\n\tio_req->cmd_mgr = cmd_mgr;\n\tio_req->fcport = fcport;\n\n\t \n\tio_req->sc_cmd = NULL;\n\tio_req->lun = -1;\n\n\t \n\tkref_init(&io_req->refcount);\t \n\tatomic_set(&io_req->state, QEDFC_CMD_ST_IO_ACTIVE);\n\n\t \n\t \n\tbd_tbl = io_req->bd_tbl = cmd_mgr->io_bdt_pool[xid];\n\tif (bd_tbl == NULL) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"bd_tbl is NULL, xid=%x.\\n\", xid);\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\tgoto out_failed;\n\t}\n\tbd_tbl->io_req = io_req;\n\tio_req->cmd_type = cmd_type;\n\tio_req->tm_flags = 0;\n\n\t \n\tio_req->rx_buf_off = 0;\n\tio_req->tx_buf_off = 0;\n\tio_req->rx_id = 0xffff;  \n\n\treturn io_req;\n\nout_failed:\n\t \n\tqedf->alloc_failures++;\n\treturn NULL;\n}\n\nstatic void qedf_free_mp_resc(struct qedf_ioreq *io_req)\n{\n\tstruct qedf_mp_req *mp_req = &(io_req->mp_req);\n\tstruct qedf_ctx *qedf = io_req->fcport->qedf;\n\tuint64_t sz = sizeof(struct scsi_sge);\n\n\t \n\tif (mp_req->mp_req_bd) {\n\t\tdma_free_coherent(&qedf->pdev->dev, sz,\n\t\t    mp_req->mp_req_bd, mp_req->mp_req_bd_dma);\n\t\tmp_req->mp_req_bd = NULL;\n\t}\n\tif (mp_req->mp_resp_bd) {\n\t\tdma_free_coherent(&qedf->pdev->dev, sz,\n\t\t    mp_req->mp_resp_bd, mp_req->mp_resp_bd_dma);\n\t\tmp_req->mp_resp_bd = NULL;\n\t}\n\tif (mp_req->req_buf) {\n\t\tdma_free_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\n\t\t    mp_req->req_buf, mp_req->req_buf_dma);\n\t\tmp_req->req_buf = NULL;\n\t}\n\tif (mp_req->resp_buf) {\n\t\tdma_free_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\n\t\t    mp_req->resp_buf, mp_req->resp_buf_dma);\n\t\tmp_req->resp_buf = NULL;\n\t}\n}\n\nvoid qedf_release_cmd(struct kref *ref)\n{\n\tstruct qedf_ioreq *io_req =\n\t    container_of(ref, struct qedf_ioreq, refcount);\n\tstruct qedf_cmd_mgr *cmd_mgr = io_req->cmd_mgr;\n\tstruct qedf_rport *fcport = io_req->fcport;\n\tunsigned long flags;\n\n\tif (io_req->cmd_type == QEDF_SCSI_CMD) {\n\t\tQEDF_WARN(&fcport->qedf->dbg_ctx,\n\t\t\t  \"Cmd released called without scsi_done called, io_req %p xid=0x%x.\\n\",\n\t\t\t  io_req, io_req->xid);\n\t\tWARN_ON(io_req->sc_cmd);\n\t}\n\n\tif (io_req->cmd_type == QEDF_ELS ||\n\t    io_req->cmd_type == QEDF_TASK_MGMT_CMD)\n\t\tqedf_free_mp_resc(io_req);\n\n\tatomic_inc(&cmd_mgr->free_list_cnt);\n\tatomic_dec(&fcport->num_active_ios);\n\tatomic_set(&io_req->state, QEDF_CMD_ST_INACTIVE);\n\tif (atomic_read(&fcport->num_active_ios) < 0) {\n\t\tQEDF_WARN(&(fcport->qedf->dbg_ctx), \"active_ios < 0.\\n\");\n\t\tWARN_ON(1);\n\t}\n\n\t \n\tio_req->task_retry_identifier++;\n\tio_req->fcport = NULL;\n\n\tclear_bit(QEDF_CMD_DIRTY, &io_req->flags);\n\tio_req->cpu = 0;\n\tspin_lock_irqsave(&cmd_mgr->lock, flags);\n\tio_req->fcport = NULL;\n\tio_req->alloc = 0;\n\tspin_unlock_irqrestore(&cmd_mgr->lock, flags);\n}\n\nstatic int qedf_map_sg(struct qedf_ioreq *io_req)\n{\n\tstruct scsi_cmnd *sc = io_req->sc_cmd;\n\tstruct Scsi_Host *host = sc->device->host;\n\tstruct fc_lport *lport = shost_priv(host);\n\tstruct qedf_ctx *qedf = lport_priv(lport);\n\tstruct scsi_sge *bd = io_req->bd_tbl->bd_tbl;\n\tstruct scatterlist *sg;\n\tint byte_count = 0;\n\tint sg_count = 0;\n\tint bd_count = 0;\n\tu32 sg_len;\n\tu64 addr;\n\tint i = 0;\n\n\tsg_count = dma_map_sg(&qedf->pdev->dev, scsi_sglist(sc),\n\t    scsi_sg_count(sc), sc->sc_data_direction);\n\tsg = scsi_sglist(sc);\n\n\tio_req->sge_type = QEDF_IOREQ_UNKNOWN_SGE;\n\n\tif (sg_count <= 8 || io_req->io_req_flags == QEDF_READ)\n\t\tio_req->sge_type = QEDF_IOREQ_FAST_SGE;\n\n\tscsi_for_each_sg(sc, sg, sg_count, i) {\n\t\tsg_len = (u32)sg_dma_len(sg);\n\t\taddr = (u64)sg_dma_address(sg);\n\n\t\t \n\t\tif (io_req->sge_type == QEDF_IOREQ_UNKNOWN_SGE && (i) &&\n\t\t    (i != (sg_count - 1)) && sg_len < QEDF_PAGE_SIZE)\n\t\t\tio_req->sge_type = QEDF_IOREQ_SLOW_SGE;\n\n\t\tbd[bd_count].sge_addr.lo = cpu_to_le32(U64_LO(addr));\n\t\tbd[bd_count].sge_addr.hi  = cpu_to_le32(U64_HI(addr));\n\t\tbd[bd_count].sge_len = cpu_to_le32(sg_len);\n\n\t\tbd_count++;\n\t\tbyte_count += sg_len;\n\t}\n\n\t \n\tif (io_req->sge_type == QEDF_IOREQ_UNKNOWN_SGE)\n\t\tio_req->sge_type = QEDF_IOREQ_FAST_SGE;\n\n\tif (byte_count != scsi_bufflen(sc))\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"byte_count = %d != \"\n\t\t\t  \"scsi_bufflen = %d, task_id = 0x%x.\\n\", byte_count,\n\t\t\t   scsi_bufflen(sc), io_req->xid);\n\n\treturn bd_count;\n}\n\nstatic int qedf_build_bd_list_from_sg(struct qedf_ioreq *io_req)\n{\n\tstruct scsi_cmnd *sc = io_req->sc_cmd;\n\tstruct scsi_sge *bd = io_req->bd_tbl->bd_tbl;\n\tint bd_count;\n\n\tif (scsi_sg_count(sc)) {\n\t\tbd_count = qedf_map_sg(io_req);\n\t\tif (bd_count == 0)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tbd_count = 0;\n\t\tbd[0].sge_addr.lo = bd[0].sge_addr.hi = 0;\n\t\tbd[0].sge_len = 0;\n\t}\n\tio_req->bd_tbl->bd_valid = bd_count;\n\n\treturn 0;\n}\n\nstatic void qedf_build_fcp_cmnd(struct qedf_ioreq *io_req,\n\t\t\t\t  struct fcp_cmnd *fcp_cmnd)\n{\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\n\t \n\tmemset(fcp_cmnd, 0, FCP_CMND_LEN);\n\n\t \n\tint_to_scsilun(sc_cmd->device->lun,\n\t\t\t(struct scsi_lun *)&fcp_cmnd->fc_lun);\n\n\t \n\tfcp_cmnd->fc_pri_ta = 0;\n\tfcp_cmnd->fc_tm_flags = io_req->tm_flags;\n\tfcp_cmnd->fc_flags = io_req->io_req_flags;\n\tfcp_cmnd->fc_cmdref = 0;\n\n\t \n\tif (io_req->cmd_type == QEDF_TASK_MGMT_CMD) {\n\t\tfcp_cmnd->fc_flags |= FCP_CFL_RDDATA;\n\t} else {\n\t\tif (sc_cmd->sc_data_direction == DMA_TO_DEVICE)\n\t\t\tfcp_cmnd->fc_flags |= FCP_CFL_WRDATA;\n\t\telse if (sc_cmd->sc_data_direction == DMA_FROM_DEVICE)\n\t\t\tfcp_cmnd->fc_flags |= FCP_CFL_RDDATA;\n\t}\n\n\tfcp_cmnd->fc_pri_ta = FCP_PTA_SIMPLE;\n\n\t \n\tif (io_req->cmd_type != QEDF_TASK_MGMT_CMD)\n\t\tmemcpy(fcp_cmnd->fc_cdb, sc_cmd->cmnd, sc_cmd->cmd_len);\n\n\t \n\tfcp_cmnd->fc_dl = htonl(io_req->data_xfer_len);\n}\n\nstatic void  qedf_init_task(struct qedf_rport *fcport, struct fc_lport *lport,\n\tstruct qedf_ioreq *io_req, struct fcoe_task_context *task_ctx,\n\tstruct fcoe_wqe *sqe)\n{\n\tenum fcoe_task_type task_type;\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\tstruct io_bdt *bd_tbl = io_req->bd_tbl;\n\tu8 fcp_cmnd[32];\n\tu32 tmp_fcp_cmnd[8];\n\tint bd_count = 0;\n\tstruct qedf_ctx *qedf = fcport->qedf;\n\tuint16_t cq_idx = smp_processor_id() % qedf->num_queues;\n\tstruct regpair sense_data_buffer_phys_addr;\n\tu32 tx_io_size = 0;\n\tu32 rx_io_size = 0;\n\tint i, cnt;\n\n\t \n\tio_req->task = task_ctx;\n\tmemset(task_ctx, 0, sizeof(struct fcoe_task_context));\n\tmemset(io_req->task_params, 0, sizeof(struct fcoe_task_params));\n\tmemset(io_req->sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\n\n\t \n\tif (io_req->cmd_type == QEDF_TASK_MGMT_CMD) {\n\t\ttask_type = FCOE_TASK_TYPE_READ_INITIATOR;\n\t} else {\n\t\tif (sc_cmd->sc_data_direction == DMA_TO_DEVICE) {\n\t\t\ttask_type = FCOE_TASK_TYPE_WRITE_INITIATOR;\n\t\t\ttx_io_size = io_req->data_xfer_len;\n\t\t} else {\n\t\t\ttask_type = FCOE_TASK_TYPE_READ_INITIATOR;\n\t\t\trx_io_size = io_req->data_xfer_len;\n\t\t}\n\t}\n\n\t \n\tio_req->task_params->context = task_ctx;\n\tio_req->task_params->sqe = sqe;\n\tio_req->task_params->task_type = task_type;\n\tio_req->task_params->tx_io_size = tx_io_size;\n\tio_req->task_params->rx_io_size = rx_io_size;\n\tio_req->task_params->conn_cid = fcport->fw_cid;\n\tio_req->task_params->itid = io_req->xid;\n\tio_req->task_params->cq_rss_number = cq_idx;\n\tio_req->task_params->is_tape_device = fcport->dev_type;\n\n\t \n\tif (io_req->cmd_type != QEDF_TASK_MGMT_CMD) {\n\t\tbd_count = bd_tbl->bd_valid;\n\t\tio_req->sgl_task_params->sgl = bd_tbl->bd_tbl;\n\t\tio_req->sgl_task_params->sgl_phys_addr.lo =\n\t\t\tU64_LO(bd_tbl->bd_tbl_dma);\n\t\tio_req->sgl_task_params->sgl_phys_addr.hi =\n\t\t\tU64_HI(bd_tbl->bd_tbl_dma);\n\t\tio_req->sgl_task_params->num_sges = bd_count;\n\t\tio_req->sgl_task_params->total_buffer_size =\n\t\t    scsi_bufflen(io_req->sc_cmd);\n\t\tif (io_req->sge_type == QEDF_IOREQ_SLOW_SGE)\n\t\t\tio_req->sgl_task_params->small_mid_sge = 1;\n\t\telse\n\t\t\tio_req->sgl_task_params->small_mid_sge = 0;\n\t}\n\n\t \n\tsense_data_buffer_phys_addr.lo = U64_LO(io_req->sense_buffer_dma);\n\tsense_data_buffer_phys_addr.hi = U64_HI(io_req->sense_buffer_dma);\n\n\t \n\tqedf_build_fcp_cmnd(io_req, (struct fcp_cmnd *)tmp_fcp_cmnd);\n\n\t \n\tcnt = sizeof(struct fcp_cmnd) / sizeof(u32);\n\tfor (i = 0; i < cnt; i++) {\n\t\ttmp_fcp_cmnd[i] = cpu_to_be32(tmp_fcp_cmnd[i]);\n\t}\n\tmemcpy(fcp_cmnd, tmp_fcp_cmnd, sizeof(struct fcp_cmnd));\n\n\tinit_initiator_rw_fcoe_task(io_req->task_params,\n\t\t\t\t    io_req->sgl_task_params,\n\t\t\t\t    sense_data_buffer_phys_addr,\n\t\t\t\t    io_req->task_retry_identifier, fcp_cmnd);\n\n\t \n\tif (io_req->sge_type == QEDF_IOREQ_SLOW_SGE)\n\t\tqedf->slow_sge_ios++;\n\telse\n\t\tqedf->fast_sge_ios++;\n}\n\nvoid qedf_init_mp_task(struct qedf_ioreq *io_req,\n\tstruct fcoe_task_context *task_ctx, struct fcoe_wqe *sqe)\n{\n\tstruct qedf_mp_req *mp_req = &(io_req->mp_req);\n\tstruct qedf_rport *fcport = io_req->fcport;\n\tstruct qedf_ctx *qedf = io_req->fcport->qedf;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct fcoe_tx_mid_path_params task_fc_hdr;\n\tstruct scsi_sgl_task_params tx_sgl_task_params;\n\tstruct scsi_sgl_task_params rx_sgl_task_params;\n\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_DISC,\n\t\t  \"Initializing MP task for cmd_type=%d\\n\",\n\t\t  io_req->cmd_type);\n\n\tqedf->control_requests++;\n\n\tmemset(&tx_sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\n\tmemset(&rx_sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\n\tmemset(task_ctx, 0, sizeof(struct fcoe_task_context));\n\tmemset(&task_fc_hdr, 0, sizeof(struct fcoe_tx_mid_path_params));\n\n\t \n\tio_req->task = task_ctx;\n\n\t \n\tio_req->task_params->context = task_ctx;\n\tio_req->task_params->sqe = sqe;\n\tio_req->task_params->task_type = FCOE_TASK_TYPE_MIDPATH;\n\tio_req->task_params->tx_io_size = io_req->data_xfer_len;\n\t \n\tio_req->task_params->rx_io_size = PAGE_SIZE;\n\tio_req->task_params->conn_cid = fcport->fw_cid;\n\tio_req->task_params->itid = io_req->xid;\n\t \n\tio_req->task_params->cq_rss_number = 0;\n\tio_req->task_params->is_tape_device = fcport->dev_type;\n\n\tfc_hdr = &(mp_req->req_fc_hdr);\n\t \n\tfc_hdr->fh_ox_id = io_req->xid;\n\tfc_hdr->fh_rx_id = htons(0xffff);\n\n\t \n\ttask_fc_hdr.parameter = fc_hdr->fh_parm_offset;\n\ttask_fc_hdr.r_ctl = fc_hdr->fh_r_ctl;\n\ttask_fc_hdr.type = fc_hdr->fh_type;\n\ttask_fc_hdr.cs_ctl = fc_hdr->fh_cs_ctl;\n\ttask_fc_hdr.df_ctl = fc_hdr->fh_df_ctl;\n\ttask_fc_hdr.rx_id = fc_hdr->fh_rx_id;\n\ttask_fc_hdr.ox_id = fc_hdr->fh_ox_id;\n\n\t \n\ttx_sgl_task_params.sgl = mp_req->mp_req_bd;\n\ttx_sgl_task_params.sgl_phys_addr.lo = U64_LO(mp_req->mp_req_bd_dma);\n\ttx_sgl_task_params.sgl_phys_addr.hi = U64_HI(mp_req->mp_req_bd_dma);\n\ttx_sgl_task_params.num_sges = 1;\n\t \n\ttx_sgl_task_params.total_buffer_size = io_req->data_xfer_len;\n\ttx_sgl_task_params.small_mid_sge = 0;\n\n\t \n\trx_sgl_task_params.sgl = mp_req->mp_resp_bd;\n\trx_sgl_task_params.sgl_phys_addr.lo = U64_LO(mp_req->mp_resp_bd_dma);\n\trx_sgl_task_params.sgl_phys_addr.hi = U64_HI(mp_req->mp_resp_bd_dma);\n\trx_sgl_task_params.num_sges = 1;\n\t \n\trx_sgl_task_params.total_buffer_size = PAGE_SIZE;\n\trx_sgl_task_params.small_mid_sge = 0;\n\n\n\t \n\tinit_initiator_midpath_unsolicited_fcoe_task(io_req->task_params,\n\t\t\t\t\t\t     &task_fc_hdr,\n\t\t\t\t\t\t     &tx_sgl_task_params,\n\t\t\t\t\t\t     &rx_sgl_task_params, 0);\n}\n\n \nu16 qedf_get_sqe_idx(struct qedf_rport *fcport)\n{\n\tuint16_t total_sqe = (fcport->sq_mem_size)/(sizeof(struct fcoe_wqe));\n\tu16 rval;\n\n\trval = fcport->sq_prod_idx;\n\n\t \n\tfcport->sq_prod_idx++;\n\tfcport->fw_sq_prod_idx++;\n\tif (fcport->sq_prod_idx == total_sqe)\n\t\tfcport->sq_prod_idx = 0;\n\n\treturn rval;\n}\n\nvoid qedf_ring_doorbell(struct qedf_rport *fcport)\n{\n\tstruct fcoe_db_data dbell = { 0 };\n\n\tdbell.agg_flags = 0;\n\n\tdbell.params |= DB_DEST_XCM << FCOE_DB_DATA_DEST_SHIFT;\n\tdbell.params |= DB_AGG_CMD_SET << FCOE_DB_DATA_AGG_CMD_SHIFT;\n\tdbell.params |= DQ_XCM_FCOE_SQ_PROD_CMD <<\n\t    FCOE_DB_DATA_AGG_VAL_SEL_SHIFT;\n\n\tdbell.sq_prod = fcport->fw_sq_prod_idx;\n\t \n\twmb();\n\tbarrier();\n\twritel(*(u32 *)&dbell, fcport->p_doorbell);\n\t \n\twmb();\n}\n\nstatic void qedf_trace_io(struct qedf_rport *fcport, struct qedf_ioreq *io_req,\n\t\t\t  int8_t direction)\n{\n\tstruct qedf_ctx *qedf = fcport->qedf;\n\tstruct qedf_io_log *io_log;\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qedf->io_trace_lock, flags);\n\n\tio_log = &qedf->io_trace_buf[qedf->io_trace_idx];\n\tio_log->direction = direction;\n\tio_log->task_id = io_req->xid;\n\tio_log->port_id = fcport->rdata->ids.port_id;\n\tio_log->lun = sc_cmd->device->lun;\n\tio_log->op = sc_cmd->cmnd[0];\n\tio_log->lba[0] = sc_cmd->cmnd[2];\n\tio_log->lba[1] = sc_cmd->cmnd[3];\n\tio_log->lba[2] = sc_cmd->cmnd[4];\n\tio_log->lba[3] = sc_cmd->cmnd[5];\n\tio_log->bufflen = scsi_bufflen(sc_cmd);\n\tio_log->sg_count = scsi_sg_count(sc_cmd);\n\tio_log->result = sc_cmd->result;\n\tio_log->jiffies = jiffies;\n\tio_log->refcount = kref_read(&io_req->refcount);\n\n\tif (direction == QEDF_IO_TRACE_REQ) {\n\t\t \n\t\tio_log->req_cpu = io_req->cpu;\n\t\tio_log->int_cpu = 0;\n\t\tio_log->rsp_cpu = 0;\n\t} else if (direction == QEDF_IO_TRACE_RSP) {\n\t\tio_log->req_cpu = io_req->cpu;\n\t\tio_log->int_cpu = io_req->int_cpu;\n\t\tio_log->rsp_cpu = smp_processor_id();\n\t}\n\n\tio_log->sge_type = io_req->sge_type;\n\n\tqedf->io_trace_idx++;\n\tif (qedf->io_trace_idx == QEDF_IO_TRACE_SIZE)\n\t\tqedf->io_trace_idx = 0;\n\n\tspin_unlock_irqrestore(&qedf->io_trace_lock, flags);\n}\n\nint qedf_post_io_req(struct qedf_rport *fcport, struct qedf_ioreq *io_req)\n{\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\tstruct Scsi_Host *host = sc_cmd->device->host;\n\tstruct fc_lport *lport = shost_priv(host);\n\tstruct qedf_ctx *qedf = lport_priv(lport);\n\tstruct fcoe_task_context *task_ctx;\n\tu16 xid;\n\tstruct fcoe_wqe *sqe;\n\tu16 sqe_idx;\n\n\t \n\tio_req->data_xfer_len = scsi_bufflen(sc_cmd);\n\tqedf_priv(sc_cmd)->io_req = io_req;\n\tio_req->sge_type = QEDF_IOREQ_FAST_SGE;  \n\n\t \n\tio_req->cpu = smp_processor_id();\n\n\tif (sc_cmd->sc_data_direction == DMA_FROM_DEVICE) {\n\t\tio_req->io_req_flags = QEDF_READ;\n\t\tqedf->input_requests++;\n\t} else if (sc_cmd->sc_data_direction == DMA_TO_DEVICE) {\n\t\tio_req->io_req_flags = QEDF_WRITE;\n\t\tqedf->output_requests++;\n\t} else {\n\t\tio_req->io_req_flags = 0;\n\t\tqedf->control_requests++;\n\t}\n\n\txid = io_req->xid;\n\n\t \n\tif (qedf_build_bd_list_from_sg(io_req)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"BD list creation failed.\\n\");\n\t\t \n\t\tio_req->sc_cmd = NULL;\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags) ||\n\t    test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Session not offloaded yet.\\n\");\n\t\t \n\t\tio_req->sc_cmd = NULL;\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tio_req->lun = (int)sc_cmd->device->lun;\n\n\t \n\tsqe_idx = qedf_get_sqe_idx(fcport);\n\tsqe = &fcport->sq[sqe_idx];\n\tmemset(sqe, 0, sizeof(struct fcoe_wqe));\n\n\t \n\ttask_ctx = qedf_get_task_mem(&qedf->tasks, xid);\n\tif (!task_ctx) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"task_ctx is NULL, xid=%d.\\n\",\n\t\t\t   xid);\n\t\t \n\t\tio_req->sc_cmd = NULL;\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\treturn -EINVAL;\n\t}\n\n\tqedf_init_task(fcport, lport, io_req, task_ctx, sqe);\n\n\t \n\tqedf_ring_doorbell(fcport);\n\n\t \n\tset_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\n\tif (qedf_io_tracing && io_req->sc_cmd)\n\t\tqedf_trace_io(fcport, io_req, QEDF_IO_TRACE_REQ);\n\n\treturn false;\n}\n\nint\nqedf_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *sc_cmd)\n{\n\tstruct fc_lport *lport = shost_priv(host);\n\tstruct qedf_ctx *qedf = lport_priv(lport);\n\tstruct fc_rport *rport = starget_to_rport(scsi_target(sc_cmd->device));\n\tstruct fc_rport_libfc_priv *rp = rport->dd_data;\n\tstruct qedf_rport *fcport;\n\tstruct qedf_ioreq *io_req;\n\tint rc = 0;\n\tint rval;\n\tunsigned long flags = 0;\n\tint num_sgs = 0;\n\n\tnum_sgs = scsi_sg_count(sc_cmd);\n\tif (scsi_sg_count(sc_cmd) > QEDF_MAX_BDS_PER_CMD) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"Number of SG elements %d exceeds what hardware limitation of %d.\\n\",\n\t\t\t num_sgs, QEDF_MAX_BDS_PER_CMD);\n\t\tsc_cmd->result = DID_ERROR;\n\t\tscsi_done(sc_cmd);\n\t\treturn 0;\n\t}\n\n\tif (test_bit(QEDF_UNLOADING, &qedf->flags) ||\n\t    test_bit(QEDF_DBG_STOP_IO, &qedf->flags)) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"Returning DNC as unloading or stop io, flags 0x%lx.\\n\",\n\t\t\t  qedf->flags);\n\t\tsc_cmd->result = DID_NO_CONNECT << 16;\n\t\tscsi_done(sc_cmd);\n\t\treturn 0;\n\t}\n\n\tif (!qedf->pdev->msix_enabled) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Completing sc_cmd=%p DID_NO_CONNECT as MSI-X is not enabled.\\n\",\n\t\t    sc_cmd);\n\t\tsc_cmd->result = DID_NO_CONNECT << 16;\n\t\tscsi_done(sc_cmd);\n\t\treturn 0;\n\t}\n\n\trval = fc_remote_port_chkready(rport);\n\tif (rval) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"fc_remote_port_chkready failed=0x%x for port_id=0x%06x.\\n\",\n\t\t\t  rval, rport->port_id);\n\t\tsc_cmd->result = rval;\n\t\tscsi_done(sc_cmd);\n\t\treturn 0;\n\t}\n\n\t \n\tif (test_bit(QEDF_DRAIN_ACTIVE, &qedf->flags)) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO, \"Drain active.\\n\");\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto exit_qcmd;\n\t}\n\n\tif (lport->state != LPORT_ST_READY ||\n\t    atomic_read(&qedf->link_state) != QEDF_LINK_UP) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO, \"Link down.\\n\");\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto exit_qcmd;\n\t}\n\n\t \n\tfcport = (struct qedf_rport *)&rp[1];\n\n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags) ||\n\t    test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\t \n\t\trc = SCSI_MLQUEUE_TARGET_BUSY;\n\t\tgoto exit_qcmd;\n\t}\n\n\tatomic_inc(&fcport->ios_to_queue);\n\n\tif (fcport->retry_delay_timestamp) {\n\t\t \n\t\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\t\tif (time_after(jiffies, fcport->retry_delay_timestamp)) {\n\t\t\tfcport->retry_delay_timestamp = 0;\n\t\t} else {\n\t\t\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\t\t\t \n\t\t\trc = SCSI_MLQUEUE_TARGET_BUSY;\n\t\t\tatomic_dec(&fcport->ios_to_queue);\n\t\t\tgoto exit_qcmd;\n\t\t}\n\t\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\t}\n\n\tio_req = qedf_alloc_cmd(fcport, QEDF_SCSI_CMD);\n\tif (!io_req) {\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tatomic_dec(&fcport->ios_to_queue);\n\t\tgoto exit_qcmd;\n\t}\n\n\tio_req->sc_cmd = sc_cmd;\n\n\t \n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\tif (qedf_post_io_req(fcport, io_req)) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"Unable to post io_req\\n\");\n\t\t \n\t\tatomic_inc(&fcport->free_sqes);\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\tatomic_dec(&fcport->ios_to_queue);\n\nexit_qcmd:\n\treturn rc;\n}\n\nstatic void qedf_parse_fcp_rsp(struct qedf_ioreq *io_req,\n\t\t\t\t struct fcoe_cqe_rsp_info *fcp_rsp)\n{\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\tstruct qedf_ctx *qedf = io_req->fcport->qedf;\n\tu8 rsp_flags = fcp_rsp->rsp_flags.flags;\n\tint fcp_sns_len = 0;\n\tint fcp_rsp_len = 0;\n\tuint8_t *rsp_info, *sense_data;\n\n\tio_req->fcp_status = FC_GOOD;\n\tio_req->fcp_resid = 0;\n\tif (rsp_flags & (FCOE_FCP_RSP_FLAGS_FCP_RESID_OVER |\n\t    FCOE_FCP_RSP_FLAGS_FCP_RESID_UNDER))\n\t\tio_req->fcp_resid = fcp_rsp->fcp_resid;\n\n\tio_req->scsi_comp_flags = rsp_flags;\n\tio_req->cdb_status = fcp_rsp->scsi_status_code;\n\n\tif (rsp_flags &\n\t    FCOE_FCP_RSP_FLAGS_FCP_RSP_LEN_VALID)\n\t\tfcp_rsp_len = fcp_rsp->fcp_rsp_len;\n\n\tif (rsp_flags &\n\t    FCOE_FCP_RSP_FLAGS_FCP_SNS_LEN_VALID)\n\t\tfcp_sns_len = fcp_rsp->fcp_sns_len;\n\n\tio_req->fcp_rsp_len = fcp_rsp_len;\n\tio_req->fcp_sns_len = fcp_sns_len;\n\trsp_info = sense_data = io_req->sense_buffer;\n\n\t \n\tif ((fcp_rsp_len == 4) || (fcp_rsp_len == 8)) {\n\t\t \n\t\tio_req->fcp_rsp_code = rsp_info[3];\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"fcp_rsp_code = %d\\n\", io_req->fcp_rsp_code);\n\t\t \n\t\tsense_data += fcp_rsp_len;\n\t}\n\n\tif (fcp_sns_len > SCSI_SENSE_BUFFERSIZE) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Truncating sense buffer\\n\");\n\t\tfcp_sns_len = SCSI_SENSE_BUFFERSIZE;\n\t}\n\n\t \n\tif (sc_cmd->sense_buffer) {\n\t\tmemset(sc_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\n\t\tif (fcp_sns_len)\n\t\t\tmemcpy(sc_cmd->sense_buffer, sense_data,\n\t\t\t    fcp_sns_len);\n\t}\n}\n\nstatic void qedf_unmap_sg_list(struct qedf_ctx *qedf, struct qedf_ioreq *io_req)\n{\n\tstruct scsi_cmnd *sc = io_req->sc_cmd;\n\n\tif (io_req->bd_tbl->bd_valid && sc && scsi_sg_count(sc)) {\n\t\tdma_unmap_sg(&qedf->pdev->dev, scsi_sglist(sc),\n\t\t    scsi_sg_count(sc), sc->sc_data_direction);\n\t\tio_req->bd_tbl->bd_valid = 0;\n\t}\n}\n\nvoid qedf_scsi_completion(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tstruct scsi_cmnd *sc_cmd;\n\tstruct fcoe_cqe_rsp_info *fcp_rsp;\n\tstruct qedf_rport *fcport;\n\tint refcount;\n\tu16 scope, qualifier = 0;\n\tu8 fw_residual_flag = 0;\n\tunsigned long flags = 0;\n\tu16 chk_scope = 0;\n\n\tif (!io_req)\n\t\treturn;\n\tif (!cqe)\n\t\treturn;\n\n\tif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags) ||\n\t    test_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags) ||\n\t    test_bit(QEDF_CMD_IN_ABORT, &io_req->flags)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"io_req xid=0x%x already in cleanup or abort processing or already completed.\\n\",\n\t\t\t io_req->xid);\n\t\treturn;\n\t}\n\n\tsc_cmd = io_req->sc_cmd;\n\tfcp_rsp = &cqe->cqe_info.rsp_info;\n\n\tif (!sc_cmd) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"sc_cmd is NULL!\\n\");\n\t\treturn;\n\t}\n\n\tif (!qedf_priv(sc_cmd)->io_req) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx),\n\t\t\t  \"io_req is NULL, returned in another context.\\n\");\n\t\treturn;\n\t}\n\n\tif (!sc_cmd->device) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"Device for sc_cmd %p is NULL.\\n\", sc_cmd);\n\t\treturn;\n\t}\n\n\tif (!scsi_cmd_to_rq(sc_cmd)->q) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"request->q is NULL so request \"\n\t\t   \"is not valid, sc_cmd=%p.\\n\", sc_cmd);\n\t\treturn;\n\t}\n\n\tfcport = io_req->fcport;\n\n\t \n\tif (test_bit(QEDF_RPORT_IN_TARGET_RESET, &fcport->flags) ||\n\t    (test_bit(QEDF_RPORT_IN_LUN_RESET, &fcport->flags) &&\n\t     sc_cmd->device->lun == (u64)fcport->lun_reset_lun)) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"Dropping good completion xid=0x%x as fcport is flushing\",\n\t\t\t  io_req->xid);\n\t\treturn;\n\t}\n\n\tqedf_parse_fcp_rsp(io_req, fcp_rsp);\n\n\tqedf_unmap_sg_list(qedf, io_req);\n\n\t \n\tif (io_req->fcp_rsp_len > 3 && io_req->fcp_rsp_code) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx),\n\t\t    \"FCP I/O protocol failure xid=0x%x fcp_rsp_len=%d \"\n\t\t    \"fcp_rsp_code=%d.\\n\", io_req->xid, io_req->fcp_rsp_len,\n\t\t    io_req->fcp_rsp_code);\n\t\tsc_cmd->result = DID_BUS_BUSY << 16;\n\t\tgoto out;\n\t}\n\n\tfw_residual_flag = GET_FIELD(cqe->cqe_info.rsp_info.fw_error_flags,\n\t    FCOE_CQE_RSP_INFO_FW_UNDERRUN);\n\tif (fw_residual_flag) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"Firmware detected underrun: xid=0x%x fcp_rsp.flags=0x%02x fcp_resid=%d fw_residual=0x%x lba=%02x%02x%02x%02x.\\n\",\n\t\t\t io_req->xid, fcp_rsp->rsp_flags.flags,\n\t\t\t io_req->fcp_resid,\n\t\t\t cqe->cqe_info.rsp_info.fw_residual, sc_cmd->cmnd[2],\n\t\t\t sc_cmd->cmnd[3], sc_cmd->cmnd[4], sc_cmd->cmnd[5]);\n\n\t\tif (io_req->cdb_status == 0)\n\t\t\tsc_cmd->result = (DID_ERROR << 16) | io_req->cdb_status;\n\t\telse\n\t\t\tsc_cmd->result = (DID_OK << 16) | io_req->cdb_status;\n\n\t\t \n\t\tscsi_set_resid(sc_cmd, scsi_bufflen(sc_cmd));\n\t\tgoto out;\n\t}\n\n\tswitch (io_req->fcp_status) {\n\tcase FC_GOOD:\n\t\tif (io_req->cdb_status == 0) {\n\t\t\t \n\t\t\tsc_cmd->result = DID_OK << 16;\n\t\t} else {\n\t\t\trefcount = kref_read(&io_req->refcount);\n\t\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t\t    \"%d:0:%d:%lld xid=0x%0x op=0x%02x \"\n\t\t\t    \"lba=%02x%02x%02x%02x cdb_status=%d \"\n\t\t\t    \"fcp_resid=0x%x refcount=%d.\\n\",\n\t\t\t    qedf->lport->host->host_no, sc_cmd->device->id,\n\t\t\t    sc_cmd->device->lun, io_req->xid,\n\t\t\t    sc_cmd->cmnd[0], sc_cmd->cmnd[2], sc_cmd->cmnd[3],\n\t\t\t    sc_cmd->cmnd[4], sc_cmd->cmnd[5],\n\t\t\t    io_req->cdb_status, io_req->fcp_resid,\n\t\t\t    refcount);\n\t\t\tsc_cmd->result = (DID_OK << 16) | io_req->cdb_status;\n\n\t\t\tif (io_req->cdb_status == SAM_STAT_TASK_SET_FULL ||\n\t\t\t    io_req->cdb_status == SAM_STAT_BUSY) {\n\t\t\t\t \n\n\t\t\t\t \n\t\t\t\tscope = fcp_rsp->retry_delay_timer & 0xC000;\n\t\t\t\t \n\t\t\t\tqualifier = fcp_rsp->retry_delay_timer & 0x3FFF;\n\n\t\t\t\tif (qedf_retry_delay)\n\t\t\t\t\tchk_scope = 1;\n\t\t\t\t \n\t\t\t\tif (io_req->cdb_status ==\n\t\t\t\t    SAM_STAT_TASK_SET_FULL)\n\t\t\t\t\tqedf->task_set_fulls++;\n\t\t\t\telse\n\t\t\t\t\tqedf->busy++;\n\t\t\t}\n\t\t}\n\t\tif (io_req->fcp_resid)\n\t\t\tscsi_set_resid(sc_cmd, io_req->fcp_resid);\n\n\t\tif (chk_scope == 1) {\n\t\t\tif ((scope == 1 || scope == 2) &&\n\t\t\t    (qualifier > 0 && qualifier <= 0x3FEF)) {\n\t\t\t\t \n\t\t\t\tif (qualifier > QEDF_RETRY_DELAY_MAX) {\n\t\t\t\t\tqualifier = QEDF_RETRY_DELAY_MAX;\n\t\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t\t  \"qualifier = %d\\n\",\n\t\t\t\t\t\t  (fcp_rsp->retry_delay_timer &\n\t\t\t\t\t\t  0x3FFF));\n\t\t\t\t}\n\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t  \"Scope = %d and qualifier = %d\",\n\t\t\t\t\t  scope, qualifier);\n\t\t\t\t \n\t\t\t\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\t\t\t\tfcport->retry_delay_timestamp =\n\t\t\t\t\tjiffies + (qualifier * HZ / 10);\n\t\t\t\tspin_unlock_irqrestore(&fcport->rport_lock,\n\t\t\t\t\t\t       flags);\n\n\t\t\t} else {\n\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t  \"combination of scope = %d and qualifier = %d is not handled in qedf.\\n\",\n\t\t\t\t\t  scope, qualifier);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, \"fcp_status=%d.\\n\",\n\t\t\t   io_req->fcp_status);\n\t\tbreak;\n\t}\n\nout:\n\tif (qedf_io_tracing)\n\t\tqedf_trace_io(fcport, io_req, QEDF_IO_TRACE_RSP);\n\n\t \n\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\n\tio_req->sc_cmd = NULL;\n\tqedf_priv(sc_cmd)->io_req =  NULL;\n\tscsi_done(sc_cmd);\n\tkref_put(&io_req->refcount, qedf_release_cmd);\n}\n\n \nvoid qedf_scsi_done(struct qedf_ctx *qedf, struct qedf_ioreq *io_req,\n\tint result)\n{\n\tstruct scsi_cmnd *sc_cmd;\n\tint refcount;\n\n\tif (!io_req) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO, \"io_req is NULL\\n\");\n\t\treturn;\n\t}\n\n\tif (test_and_set_bit(QEDF_CMD_ERR_SCSI_DONE, &io_req->flags)) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"io_req:%p scsi_done handling already done\\n\",\n\t\t\t  io_req);\n\t\treturn;\n\t}\n\n\t \n\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\n\tsc_cmd = io_req->sc_cmd;\n\n\tif (!sc_cmd) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"sc_cmd is NULL!\\n\");\n\t\treturn;\n\t}\n\n\tif (!virt_addr_valid(sc_cmd)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"sc_cmd=%p is not valid.\", sc_cmd);\n\t\tgoto bad_scsi_ptr;\n\t}\n\n\tif (!qedf_priv(sc_cmd)->io_req) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx),\n\t\t\t  \"io_req is NULL, returned in another context.\\n\");\n\t\treturn;\n\t}\n\n\tif (!sc_cmd->device) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"Device for sc_cmd %p is NULL.\\n\",\n\t\t\t sc_cmd);\n\t\tgoto bad_scsi_ptr;\n\t}\n\n\tif (!virt_addr_valid(sc_cmd->device)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"Device pointer for sc_cmd %p is bad.\\n\", sc_cmd);\n\t\tgoto bad_scsi_ptr;\n\t}\n\n\tif (!sc_cmd->sense_buffer) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"sc_cmd->sense_buffer for sc_cmd %p is NULL.\\n\",\n\t\t\t sc_cmd);\n\t\tgoto bad_scsi_ptr;\n\t}\n\n\tif (!virt_addr_valid(sc_cmd->sense_buffer)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"sc_cmd->sense_buffer for sc_cmd %p is bad.\\n\",\n\t\t\t sc_cmd);\n\t\tgoto bad_scsi_ptr;\n\t}\n\n\tqedf_unmap_sg_list(qedf, io_req);\n\n\tsc_cmd->result = result << 16;\n\trefcount = kref_read(&io_req->refcount);\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, \"%d:0:%d:%lld: Completing \"\n\t    \"sc_cmd=%p result=0x%08x op=0x%02x lba=0x%02x%02x%02x%02x, \"\n\t    \"allowed=%d retries=%d refcount=%d.\\n\",\n\t    qedf->lport->host->host_no, sc_cmd->device->id,\n\t    sc_cmd->device->lun, sc_cmd, sc_cmd->result, sc_cmd->cmnd[0],\n\t    sc_cmd->cmnd[2], sc_cmd->cmnd[3], sc_cmd->cmnd[4],\n\t    sc_cmd->cmnd[5], sc_cmd->allowed, sc_cmd->retries,\n\t    refcount);\n\n\t \n\tscsi_set_resid(sc_cmd, scsi_bufflen(sc_cmd));\n\n\tif (qedf_io_tracing)\n\t\tqedf_trace_io(io_req->fcport, io_req, QEDF_IO_TRACE_RSP);\n\n\tio_req->sc_cmd = NULL;\n\tqedf_priv(sc_cmd)->io_req = NULL;\n\tscsi_done(sc_cmd);\n\tkref_put(&io_req->refcount, qedf_release_cmd);\n\treturn;\n\nbad_scsi_ptr:\n\t \n\tio_req->sc_cmd = NULL;\n\tkref_put(&io_req->refcount, qedf_release_cmd);   \n}\n\n \nvoid qedf_process_warning_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tint rval, i;\n\tstruct qedf_rport *fcport = io_req->fcport;\n\tu64 err_warn_bit_map;\n\tu8 err_warn = 0xff;\n\n\tif (!cqe) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"cqe is NULL for io_req %p xid=0x%x\\n\",\n\t\t\t  io_req, io_req->xid);\n\t\treturn;\n\t}\n\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), \"Warning CQE, \"\n\t\t  \"xid=0x%x\\n\", io_req->xid);\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx),\n\t\t  \"err_warn_bitmap=%08x:%08x\\n\",\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_hi),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_lo));\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), \"tx_buff_off=%08x, \"\n\t\t  \"rx_buff_off=%08x, rx_id=%04x\\n\",\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.tx_buf_off),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.rx_buf_off),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.rx_id));\n\n\t \n\terr_warn_bit_map = (u64)\n\t    ((u64)cqe->cqe_info.err_info.err_warn_bitmap_hi << 32) |\n\t    (u64)cqe->cqe_info.err_info.err_warn_bitmap_lo;\n\tfor (i = 0; i < 64; i++) {\n\t\tif (err_warn_bit_map & (u64)((u64)1 << i)) {\n\t\t\terr_warn = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (fcport->dev_type == QEDF_RPORT_TYPE_TAPE) {\n\t\tif (err_warn ==\n\t\t    FCOE_WARNING_CODE_REC_TOV_TIMER_EXPIRATION) {\n\t\t\tQEDF_ERR(&(qedf->dbg_ctx), \"REC timer expired.\\n\");\n\t\t\tif (!test_bit(QEDF_CMD_SRR_SENT, &io_req->flags)) {\n\t\t\t\tio_req->rx_buf_off =\n\t\t\t\t    cqe->cqe_info.err_info.rx_buf_off;\n\t\t\t\tio_req->tx_buf_off =\n\t\t\t\t    cqe->cqe_info.err_info.tx_buf_off;\n\t\t\t\tio_req->rx_id = cqe->cqe_info.err_info.rx_id;\n\t\t\t\trval = qedf_send_rec(io_req);\n\t\t\t\t \n\t\t\t\tif (rval)\n\t\t\t\t\tgoto send_abort;\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\nsend_abort:\n\tinit_completion(&io_req->abts_done);\n\trval = qedf_initiate_abts(io_req, true);\n\tif (rval)\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Failed to queue ABTS.\\n\");\n}\n\n \nvoid qedf_process_error_detect(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tint rval;\n\n\tif (io_req == NULL) {\n\t\tQEDF_INFO(NULL, QEDF_LOG_IO, \"io_req is NULL.\\n\");\n\t\treturn;\n\t}\n\n\tif (io_req->fcport == NULL) {\n\t\tQEDF_INFO(NULL, QEDF_LOG_IO, \"fcport is NULL.\\n\");\n\t\treturn;\n\t}\n\n\tif (!cqe) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\"cqe is NULL for io_req %p\\n\", io_req);\n\t\treturn;\n\t}\n\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), \"Error detection CQE, \"\n\t\t  \"xid=0x%x\\n\", io_req->xid);\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx),\n\t\t  \"err_warn_bitmap=%08x:%08x\\n\",\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_hi),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_lo));\n\tQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), \"tx_buff_off=%08x, \"\n\t\t  \"rx_buff_off=%08x, rx_id=%04x\\n\",\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.tx_buf_off),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.rx_buf_off),\n\t\t  le32_to_cpu(cqe->cqe_info.err_info.rx_id));\n\n\t \n\tif (test_bit(QEDF_RPORT_IN_TARGET_RESET, &io_req->fcport->flags) ||\n\t\t(test_bit(QEDF_RPORT_IN_LUN_RESET, &io_req->fcport->flags) &&\n\t\t io_req->sc_cmd->device->lun == (u64)io_req->fcport->lun_reset_lun)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\"Dropping EQE for xid=0x%x as fcport is flushing\",\n\t\t\tio_req->xid);\n\t\treturn;\n\t}\n\n\tif (qedf->stop_io_on_error) {\n\t\tqedf_stop_all_io(qedf);\n\t\treturn;\n\t}\n\n\tinit_completion(&io_req->abts_done);\n\trval = qedf_initiate_abts(io_req, true);\n\tif (rval)\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Failed to queue ABTS.\\n\");\n}\n\nstatic void qedf_flush_els_req(struct qedf_ctx *qedf,\n\tstruct qedf_ioreq *els_req)\n{\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t    \"Flushing ELS request xid=0x%x refcount=%d.\\n\", els_req->xid,\n\t    kref_read(&els_req->refcount));\n\n\t \n\tels_req->event = QEDF_IOREQ_EV_ELS_FLUSH;\n\n\tclear_bit(QEDF_CMD_OUTSTANDING, &els_req->flags);\n\n\t \n\tcancel_delayed_work_sync(&els_req->timeout_work);\n\n\t \n\tif (els_req->cb_func && els_req->cb_arg) {\n\t\tels_req->cb_func(els_req->cb_arg);\n\t\tels_req->cb_arg = NULL;\n\t}\n\n\t \n\tkref_put(&els_req->refcount, qedf_release_cmd);\n}\n\n \nvoid qedf_flush_active_ios(struct qedf_rport *fcport, int lun)\n{\n\tstruct qedf_ioreq *io_req;\n\tstruct qedf_ctx *qedf;\n\tstruct qedf_cmd_mgr *cmd_mgr;\n\tint i, rc;\n\tunsigned long flags;\n\tint flush_cnt = 0;\n\tint wait_cnt = 100;\n\tint refcount = 0;\n\n\tif (!fcport) {\n\t\tQEDF_ERR(NULL, \"fcport is NULL\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\n\t\tQEDF_ERR(NULL, \"fcport is no longer offloaded.\\n\");\n\t\treturn;\n\t}\n\n\tqedf = fcport->qedf;\n\n\tif (!qedf) {\n\t\tQEDF_ERR(NULL, \"qedf is NULL.\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags) &&\n\t    (lun == -1)) {\n\t\twhile (atomic_read(&fcport->ios_to_queue)) {\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t  \"Waiting for %d I/Os to be queued\\n\",\n\t\t\t\t  atomic_read(&fcport->ios_to_queue));\n\t\t\tif (wait_cnt == 0) {\n\t\t\t\tQEDF_ERR(NULL,\n\t\t\t\t\t \"%d IOs request could not be queued\\n\",\n\t\t\t\t\t atomic_read(&fcport->ios_to_queue));\n\t\t\t}\n\t\t\tmsleep(20);\n\t\t\twait_cnt--;\n\t\t}\n\t}\n\n\tcmd_mgr = qedf->cmd_mgr;\n\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t  \"Flush active i/o's num=0x%x fcport=0x%p port_id=0x%06x scsi_id=%d.\\n\",\n\t\t  atomic_read(&fcport->num_active_ios), fcport,\n\t\t  fcport->rdata->ids.port_id, fcport->rport->scsi_target_id);\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO, \"Locking flush mutex.\\n\");\n\n\tmutex_lock(&qedf->flush_mutex);\n\tif (lun == -1) {\n\t\tset_bit(QEDF_RPORT_IN_TARGET_RESET, &fcport->flags);\n\t} else {\n\t\tset_bit(QEDF_RPORT_IN_LUN_RESET, &fcport->flags);\n\t\tfcport->lun_reset_lun = lun;\n\t}\n\n\tfor (i = 0; i < FCOE_PARAMS_NUM_TASKS; i++) {\n\t\tio_req = &cmd_mgr->cmds[i];\n\n\t\tif (!io_req)\n\t\t\tcontinue;\n\t\tif (!io_req->fcport)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&cmd_mgr->lock, flags);\n\n\t\tif (io_req->alloc) {\n\t\t\tif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags)) {\n\t\t\t\tif (io_req->cmd_type == QEDF_SCSI_CMD)\n\t\t\t\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\t\t\t \"Allocated but not queued, xid=0x%x\\n\",\n\t\t\t\t\t\t io_req->xid);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&cmd_mgr->lock, flags);\n\t\t} else {\n\t\t\tspin_unlock_irqrestore(&cmd_mgr->lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (io_req->fcport != fcport)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags)) {\n\t\t\trefcount = kref_read(&io_req->refcount);\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t  \"Not outstanding, xid=0x%x, cmd_type=%d refcount=%d.\\n\",\n\t\t\t\t  io_req->xid, io_req->cmd_type, refcount);\n\t\t\t \n\t\t\tif (atomic_read(&io_req->state) ==\n\t\t\t    QEDFC_CMD_ST_RRQ_WAIT) {\n\t\t\t\tif (cancel_delayed_work_sync\n\t\t\t\t    (&io_req->rrq_work)) {\n\t\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t\t  \"Putting reference for pending RRQ work xid=0x%x.\\n\",\n\t\t\t\t\t\t  io_req->xid);\n\t\t\t\t\t \n\t\t\t\t\tkref_put(&io_req->refcount,\n\t\t\t\t\t\t qedf_release_cmd);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (io_req->cmd_type == QEDF_ELS &&\n\t\t    lun == -1) {\n\t\t\trc = kref_get_unless_zero(&io_req->refcount);\n\t\t\tif (!rc) {\n\t\t\t\tQEDF_ERR(&(qedf->dbg_ctx),\n\t\t\t\t    \"Could not get kref for ELS io_req=0x%p xid=0x%x.\\n\",\n\t\t\t\t    io_req, io_req->xid);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tqedf_initiate_cleanup(io_req, false);\n\t\t\tflush_cnt++;\n\t\t\tqedf_flush_els_req(qedf, io_req);\n\n\t\t\t \n\t\t\tgoto free_cmd;\n\t\t}\n\n\t\tif (io_req->cmd_type == QEDF_ABTS) {\n\t\t\t \n\t\t\trc = kref_get_unless_zero(&io_req->refcount);\n\t\t\tif (!rc) {\n\t\t\t\tQEDF_ERR(&(qedf->dbg_ctx),\n\t\t\t\t    \"Could not get kref for abort io_req=0x%p xid=0x%x.\\n\",\n\t\t\t\t    io_req, io_req->xid);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (lun != -1 && io_req->lun != lun)\n\t\t\t\tgoto free_cmd;\n\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t    \"Flushing abort xid=0x%x.\\n\", io_req->xid);\n\n\t\t\tif (cancel_delayed_work_sync(&io_req->rrq_work)) {\n\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t  \"Putting ref for cancelled RRQ work xid=0x%x.\\n\",\n\t\t\t\t\t  io_req->xid);\n\t\t\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\t\t}\n\n\t\t\tif (cancel_delayed_work_sync(&io_req->timeout_work)) {\n\t\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t\t  \"Putting ref for cancelled tmo work xid=0x%x.\\n\",\n\t\t\t\t\t  io_req->xid);\n\t\t\t\tqedf_initiate_cleanup(io_req, true);\n\t\t\t\t \n\t\t\t\tcomplete(&io_req->abts_done);\n\t\t\t\tclear_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\n\t\t\t\t \n\t\t\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\t\t}\n\t\t\tflush_cnt++;\n\t\t\tgoto free_cmd;\n\t\t}\n\n\t\tif (!io_req->sc_cmd)\n\t\t\tcontinue;\n\t\tif (!io_req->sc_cmd->device) {\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t  \"Device backpointer NULL for sc_cmd=%p.\\n\",\n\t\t\t\t  io_req->sc_cmd);\n\t\t\t \n\t\t\tio_req->sc_cmd = NULL;\n\t\t\tqedf_initiate_cleanup(io_req, false);\n\t\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\t\tcontinue;\n\t\t}\n\t\tif (lun > -1) {\n\t\t\tif (io_req->lun != lun)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\trc = kref_get_unless_zero(&io_req->refcount);\n\t\tif (!rc) {\n\t\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Could not get kref for \"\n\t\t\t    \"io_req=0x%p xid=0x%x\\n\", io_req, io_req->xid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\n\t\t    \"Cleanup xid=0x%x.\\n\", io_req->xid);\n\t\tflush_cnt++;\n\n\t\t \n\t\tqedf_initiate_cleanup(io_req, true);\n\nfree_cmd:\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\t \n\t}\n\n\twait_cnt = 60;\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t  \"Flushed 0x%x I/Os, active=0x%x.\\n\",\n\t\t  flush_cnt, atomic_read(&fcport->num_active_ios));\n\t \n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags) &&\n\t    (lun == -1)) {\n\t\twhile (atomic_read(&fcport->num_active_ios)) {\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t\t  \"Flushed 0x%x I/Os, active=0x%x cnt=%d.\\n\",\n\t\t\t\t  flush_cnt,\n\t\t\t\t  atomic_read(&fcport->num_active_ios),\n\t\t\t\t  wait_cnt);\n\t\t\tif (wait_cnt == 0) {\n\t\t\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\t\t \"Flushed %d I/Os, active=%d.\\n\",\n\t\t\t\t\t flush_cnt,\n\t\t\t\t\t atomic_read(&fcport->num_active_ios));\n\t\t\t\tfor (i = 0; i < FCOE_PARAMS_NUM_TASKS; i++) {\n\t\t\t\t\tio_req = &cmd_mgr->cmds[i];\n\t\t\t\t\tif (io_req->fcport &&\n\t\t\t\t\t    io_req->fcport == fcport) {\n\t\t\t\t\t\trefcount =\n\t\t\t\t\t\tkref_read(&io_req->refcount);\n\t\t\t\t\t\tset_bit(QEDF_CMD_DIRTY,\n\t\t\t\t\t\t\t&io_req->flags);\n\t\t\t\t\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\t\t\t\t \"Outstanding io_req =%p xid=0x%x flags=0x%lx, sc_cmd=%p refcount=%d cmd_type=%d.\\n\",\n\t\t\t\t\t\t\t io_req, io_req->xid,\n\t\t\t\t\t\t\t io_req->flags,\n\t\t\t\t\t\t\t io_req->sc_cmd,\n\t\t\t\t\t\t\t refcount,\n\t\t\t\t\t\t\t io_req->cmd_type);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tWARN_ON(1);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(500);\n\t\t\twait_cnt--;\n\t\t}\n\t}\n\n\tclear_bit(QEDF_RPORT_IN_LUN_RESET, &fcport->flags);\n\tclear_bit(QEDF_RPORT_IN_TARGET_RESET, &fcport->flags);\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO, \"Unlocking flush mutex.\\n\");\n\tmutex_unlock(&qedf->flush_mutex);\n}\n\n \nint qedf_initiate_abts(struct qedf_ioreq *io_req, bool return_scsi_cmd_on_abts)\n{\n\tstruct fc_lport *lport;\n\tstruct qedf_rport *fcport = io_req->fcport;\n\tstruct fc_rport_priv *rdata;\n\tstruct qedf_ctx *qedf;\n\tu16 xid;\n\tint rc = 0;\n\tunsigned long flags;\n\tstruct fcoe_wqe *sqe;\n\tu16 sqe_idx;\n\tint refcount = 0;\n\n\t \n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\n\t\tQEDF_ERR(NULL, \"tgt not offloaded\\n\");\n\t\trc = 1;\n\t\tgoto out;\n\t}\n\n\tqedf = fcport->qedf;\n\trdata = fcport->rdata;\n\n\tif (!rdata || !kref_get_unless_zero(&rdata->kref)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"stale rport\\n\");\n\t\trc = 1;\n\t\tgoto out;\n\t}\n\n\tlport = qedf->lport;\n\n\tif (lport->state != LPORT_ST_READY || !(lport->link_up)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"link is not ready\\n\");\n\t\trc = 1;\n\t\tgoto drop_rdata_kref;\n\t}\n\n\tif (atomic_read(&qedf->link_down_tmo_valid) > 0) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"link_down_tmo active.\\n\");\n\t\trc = 1;\n\t\tgoto drop_rdata_kref;\n\t}\n\n\t \n\tif (!atomic_read(&fcport->free_sqes)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"No SQ entries available\\n\");\n\t\trc = 1;\n\t\tgoto drop_rdata_kref;\n\t}\n\n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"fcport is uploading.\\n\");\n\t\trc = 1;\n\t\tgoto drop_rdata_kref;\n\t}\n\n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\tif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags) ||\n\t    test_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags) ||\n\t    test_bit(QEDF_CMD_IN_ABORT, &io_req->flags)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"io_req xid=0x%x sc_cmd=%p already in cleanup or abort processing or already completed.\\n\",\n\t\t\t io_req->xid, io_req->sc_cmd);\n\t\trc = 1;\n\t\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\t\tgoto drop_rdata_kref;\n\t}\n\n\t \n\tio_req->cmd_type = QEDF_ABTS;\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\n\tkref_get(&io_req->refcount);\n\n\txid = io_req->xid;\n\tqedf->control_requests++;\n\tqedf->packet_aborts++;\n\n\tio_req->return_scsi_cmd_on_abts = return_scsi_cmd_on_abts;\n\n\tset_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\n\trefcount = kref_read(&io_req->refcount);\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_SCSI_TM,\n\t\t  \"ABTS io_req xid = 0x%x refcount=%d\\n\",\n\t\t  xid, refcount);\n\n\tqedf_cmd_timer_set(qedf, io_req, QEDF_ABORT_TIMEOUT);\n\n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\n\tsqe_idx = qedf_get_sqe_idx(fcport);\n\tsqe = &fcport->sq[sqe_idx];\n\tmemset(sqe, 0, sizeof(struct fcoe_wqe));\n\tio_req->task_params->sqe = sqe;\n\n\tinit_initiator_abort_fcoe_task(io_req->task_params);\n\tqedf_ring_doorbell(fcport);\n\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\ndrop_rdata_kref:\n\tkref_put(&rdata->kref, fc_rport_destroy);\nout:\n\treturn rc;\n}\n\nvoid qedf_process_abts_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tuint32_t r_ctl;\n\tint rc;\n\tstruct qedf_rport *fcport = io_req->fcport;\n\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, \"Entered with xid = \"\n\t\t   \"0x%x cmd_type = %d\\n\", io_req->xid, io_req->cmd_type);\n\n\tr_ctl = cqe->cqe_info.abts_info.r_ctl;\n\n\t \n\tif (!fcport) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"Dropping ABTS completion xid=0x%x as fcport is NULL\",\n\t\t\t  io_req->xid);\n\t\treturn;\n\t}\n\n\t \n\tif (test_bit(QEDF_RPORT_IN_TARGET_RESET, &fcport->flags) ||\n\t    test_bit(QEDF_RPORT_IN_LUN_RESET, &fcport->flags)) {\n\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t\t  \"Dropping ABTS completion xid=0x%x as fcport is flushing\",\n\t\t\t  io_req->xid);\n\t\treturn;\n\t}\n\n\tif (!cancel_delayed_work(&io_req->timeout_work)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"Wasn't able to cancel abts timeout work.\\n\");\n\t}\n\n\tswitch (r_ctl) {\n\tcase FC_RCTL_BA_ACC:\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM,\n\t\t    \"ABTS response - ACC Send RRQ after R_A_TOV\\n\");\n\t\tio_req->event = QEDF_IOREQ_EV_ABORT_SUCCESS;\n\t\trc = kref_get_unless_zero(&io_req->refcount);\t \n\t\tif (!rc) {\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_SCSI_TM,\n\t\t\t\t  \"kref is already zero so ABTS was already completed or flushed xid=0x%x.\\n\",\n\t\t\t\t  io_req->xid);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tqueue_delayed_work(qedf->dpc_wq, &io_req->rrq_work,\n\t\t    msecs_to_jiffies(qedf->lport->r_a_tov));\n\t\tatomic_set(&io_req->state, QEDFC_CMD_ST_RRQ_WAIT);\n\t\tbreak;\n\t \n\tcase FC_RCTL_BA_RJT:\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM,\n\t\t   \"ABTS response - RJT\\n\");\n\t\tio_req->event = QEDF_IOREQ_EV_ABORT_FAILED;\n\t\tbreak;\n\tdefault:\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Unknown ABTS response\\n\");\n\t\tbreak;\n\t}\n\n\tclear_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\n\n\tif (io_req->sc_cmd) {\n\t\tif (!io_req->return_scsi_cmd_on_abts)\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_SCSI_TM,\n\t\t\t\t  \"Not call scsi_done for xid=0x%x.\\n\",\n\t\t\t\t  io_req->xid);\n\t\tif (io_req->return_scsi_cmd_on_abts)\n\t\t\tqedf_scsi_done(qedf, io_req, DID_ERROR);\n\t}\n\n\t \n\tcomplete(&io_req->abts_done);\n\n\tkref_put(&io_req->refcount, qedf_release_cmd);\n}\n\nint qedf_init_mp_req(struct qedf_ioreq *io_req)\n{\n\tstruct qedf_mp_req *mp_req;\n\tstruct scsi_sge *mp_req_bd;\n\tstruct scsi_sge *mp_resp_bd;\n\tstruct qedf_ctx *qedf = io_req->fcport->qedf;\n\tdma_addr_t addr;\n\tuint64_t sz;\n\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_MP_REQ, \"Entered.\\n\");\n\n\tmp_req = (struct qedf_mp_req *)&(io_req->mp_req);\n\tmemset(mp_req, 0, sizeof(struct qedf_mp_req));\n\n\tif (io_req->cmd_type != QEDF_ELS) {\n\t\tmp_req->req_len = sizeof(struct fcp_cmnd);\n\t\tio_req->data_xfer_len = mp_req->req_len;\n\t} else\n\t\tmp_req->req_len = io_req->data_xfer_len;\n\n\tmp_req->req_buf = dma_alloc_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\n\t    &mp_req->req_buf_dma, GFP_KERNEL);\n\tif (!mp_req->req_buf) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Unable to alloc MP req buffer\\n\");\n\t\tqedf_free_mp_resc(io_req);\n\t\treturn -ENOMEM;\n\t}\n\n\tmp_req->resp_buf = dma_alloc_coherent(&qedf->pdev->dev,\n\t    QEDF_PAGE_SIZE, &mp_req->resp_buf_dma, GFP_KERNEL);\n\tif (!mp_req->resp_buf) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Unable to alloc TM resp \"\n\t\t\t  \"buffer\\n\");\n\t\tqedf_free_mp_resc(io_req);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tsz = sizeof(struct scsi_sge);\n\tmp_req->mp_req_bd = dma_alloc_coherent(&qedf->pdev->dev, sz,\n\t    &mp_req->mp_req_bd_dma, GFP_KERNEL);\n\tif (!mp_req->mp_req_bd) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Unable to alloc MP req bd\\n\");\n\t\tqedf_free_mp_resc(io_req);\n\t\treturn -ENOMEM;\n\t}\n\n\tmp_req->mp_resp_bd = dma_alloc_coherent(&qedf->pdev->dev, sz,\n\t    &mp_req->mp_resp_bd_dma, GFP_KERNEL);\n\tif (!mp_req->mp_resp_bd) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Unable to alloc MP resp bd\\n\");\n\t\tqedf_free_mp_resc(io_req);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\taddr = mp_req->req_buf_dma;\n\tmp_req_bd = mp_req->mp_req_bd;\n\tmp_req_bd->sge_addr.lo = U64_LO(addr);\n\tmp_req_bd->sge_addr.hi = U64_HI(addr);\n\tmp_req_bd->sge_len = QEDF_PAGE_SIZE;\n\n\t \n\tmp_resp_bd = mp_req->mp_resp_bd;\n\taddr = mp_req->resp_buf_dma;\n\tmp_resp_bd->sge_addr.lo = U64_LO(addr);\n\tmp_resp_bd->sge_addr.hi = U64_HI(addr);\n\tmp_resp_bd->sge_len = QEDF_PAGE_SIZE;\n\n\treturn 0;\n}\n\n \nstatic void qedf_drain_request(struct qedf_ctx *qedf)\n{\n\tif (test_bit(QEDF_DRAIN_ACTIVE, &qedf->flags)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"MCP drain already active.\\n\");\n\t\treturn;\n\t}\n\n\t \n\tset_bit(QEDF_DRAIN_ACTIVE, &qedf->flags);\n\n\t \n\tqed_ops->common->drain(qedf->cdev);\n\n\t \n\tmsleep(100);\n\n\t \n\tclear_bit(QEDF_DRAIN_ACTIVE, &qedf->flags);\n}\n\n \nint qedf_initiate_cleanup(struct qedf_ioreq *io_req,\n\tbool return_scsi_cmd_on_abts)\n{\n\tstruct qedf_rport *fcport;\n\tstruct qedf_ctx *qedf;\n\tint tmo = 0;\n\tint rc = SUCCESS;\n\tunsigned long flags;\n\tstruct fcoe_wqe *sqe;\n\tu16 sqe_idx;\n\tint refcount = 0;\n\n\tfcport = io_req->fcport;\n\tif (!fcport) {\n\t\tQEDF_ERR(NULL, \"fcport is NULL.\\n\");\n\t\treturn SUCCESS;\n\t}\n\n\t \n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\n\t\tQEDF_ERR(NULL, \"tgt not offloaded\\n\");\n\t\treturn SUCCESS;\n\t}\n\n\tqedf = fcport->qedf;\n\tif (!qedf) {\n\t\tQEDF_ERR(NULL, \"qedf is NULL.\\n\");\n\t\treturn SUCCESS;\n\t}\n\n\tif (io_req->cmd_type == QEDF_ELS) {\n\t\tgoto process_els;\n\t}\n\n\tif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags) ||\n\t    test_and_set_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"io_req xid=0x%x already in \"\n\t\t\t  \"cleanup processing or already completed.\\n\",\n\t\t\t  io_req->xid);\n\t\treturn SUCCESS;\n\t}\n\tset_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\n\nprocess_els:\n\t \n\tif (!atomic_read(&fcport->free_sqes)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"No SQ entries available\\n\");\n\t\t \n\t\tclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\n\t\treturn FAILED;\n\t}\n\n\tif (io_req->cmd_type == QEDF_CLEANUP) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"io_req=0x%x is already a cleanup command cmd_type=%d.\\n\",\n\t\t\t io_req->xid, io_req->cmd_type);\n\t\tclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\n\t\treturn SUCCESS;\n\t}\n\n\trefcount = kref_read(&io_req->refcount);\n\n\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_IO,\n\t\t  \"Entered xid=0x%x sc_cmd=%p cmd_type=%d flags=0x%lx refcount=%d fcport=%p port_id=0x%06x\\n\",\n\t\t  io_req->xid, io_req->sc_cmd, io_req->cmd_type, io_req->flags,\n\t\t  refcount, fcport, fcport->rdata->ids.port_id);\n\n\t \n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\tio_req->cmd_type = QEDF_CLEANUP;\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\tio_req->return_scsi_cmd_on_abts = return_scsi_cmd_on_abts;\n\n\tinit_completion(&io_req->cleanup_done);\n\n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\n\tsqe_idx = qedf_get_sqe_idx(fcport);\n\tsqe = &fcport->sq[sqe_idx];\n\tmemset(sqe, 0, sizeof(struct fcoe_wqe));\n\tio_req->task_params->sqe = sqe;\n\n\tinit_initiator_cleanup_fcoe_task(io_req->task_params);\n\tqedf_ring_doorbell(fcport);\n\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\n\ttmo = wait_for_completion_timeout(&io_req->cleanup_done,\n\t\t\t\t\t  QEDF_CLEANUP_TIMEOUT * HZ);\n\n\tif (!tmo) {\n\t\trc = FAILED;\n\t\t \n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Cleanup command timeout, \"\n\t\t\t  \"xid=%x.\\n\", io_req->xid);\n\t\tclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\n\t\t \n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Issuing MCP drain request.\\n\");\n\t\tqedf_drain_request(qedf);\n\t}\n\n\t \n\tif (io_req->tm_flags  == FCP_TMF_LUN_RESET ||\n\t    io_req->tm_flags == FCP_TMF_TGT_RESET) {\n\t\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\t\tio_req->sc_cmd = NULL;\n\t\tkref_put(&io_req->refcount, qedf_release_cmd);\n\t\tcomplete(&io_req->tm_done);\n\t}\n\n\tif (io_req->sc_cmd) {\n\t\tif (!io_req->return_scsi_cmd_on_abts)\n\t\t\tQEDF_INFO(&qedf->dbg_ctx, QEDF_LOG_SCSI_TM,\n\t\t\t\t  \"Not call scsi_done for xid=0x%x.\\n\",\n\t\t\t\t  io_req->xid);\n\t\tif (io_req->return_scsi_cmd_on_abts)\n\t\t\tqedf_scsi_done(qedf, io_req, DID_ERROR);\n\t}\n\n\tif (rc == SUCCESS)\n\t\tio_req->event = QEDF_IOREQ_EV_CLEANUP_SUCCESS;\n\telse\n\t\tio_req->event = QEDF_IOREQ_EV_CLEANUP_FAILED;\n\n\treturn rc;\n}\n\nvoid qedf_process_cleanup_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, \"Entered xid = 0x%x\\n\",\n\t\t   io_req->xid);\n\n\tclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\n\n\t \n\tcomplete(&io_req->cleanup_done);\n}\n\nstatic int qedf_execute_tmf(struct qedf_rport *fcport, struct scsi_cmnd *sc_cmd,\n\tuint8_t tm_flags)\n{\n\tstruct qedf_ioreq *io_req;\n\tstruct fcoe_task_context *task;\n\tstruct qedf_ctx *qedf = fcport->qedf;\n\tstruct fc_lport *lport = qedf->lport;\n\tint rc = 0;\n\tuint16_t xid;\n\tint tmo = 0;\n\tint lun = 0;\n\tunsigned long flags;\n\tstruct fcoe_wqe *sqe;\n\tu16 sqe_idx;\n\n\tif (!sc_cmd) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"sc_cmd is NULL\\n\");\n\t\treturn FAILED;\n\t}\n\n\tlun = (int)sc_cmd->device->lun;\n\tif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"fcport not offloaded\\n\");\n\t\trc = FAILED;\n\t\tgoto no_flush;\n\t}\n\n\tio_req = qedf_alloc_cmd(fcport, QEDF_TASK_MGMT_CMD);\n\tif (!io_req) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Failed TMF\");\n\t\trc = -EAGAIN;\n\t\tgoto no_flush;\n\t}\n\n\tif (tm_flags == FCP_TMF_LUN_RESET)\n\t\tqedf->lun_resets++;\n\telse if (tm_flags == FCP_TMF_TGT_RESET)\n\t\tqedf->target_resets++;\n\n\t \n\tio_req->sc_cmd = sc_cmd;\n\tio_req->fcport = fcport;\n\tio_req->cmd_type = QEDF_TASK_MGMT_CMD;\n\n\t \n\tio_req->cpu = smp_processor_id();\n\n\t \n\tio_req->io_req_flags = QEDF_READ;\n\tio_req->data_xfer_len = 0;\n\tio_req->tm_flags = tm_flags;\n\n\t \n\tio_req->return_scsi_cmd_on_abts = false;\n\n\t \n\txid = io_req->xid;\n\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, \"TMF io_req xid = \"\n\t\t   \"0x%x\\n\", xid);\n\n\t \n\ttask = qedf_get_task_mem(&qedf->tasks, xid);\n\n\tinit_completion(&io_req->tm_done);\n\n\tspin_lock_irqsave(&fcport->rport_lock, flags);\n\n\tsqe_idx = qedf_get_sqe_idx(fcport);\n\tsqe = &fcport->sq[sqe_idx];\n\tmemset(sqe, 0, sizeof(struct fcoe_wqe));\n\n\tqedf_init_task(fcport, lport, io_req, task, sqe);\n\tqedf_ring_doorbell(fcport);\n\n\tspin_unlock_irqrestore(&fcport->rport_lock, flags);\n\n\tset_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\ttmo = wait_for_completion_timeout(&io_req->tm_done,\n\t    QEDF_TM_TIMEOUT * HZ);\n\n\tif (!tmo) {\n\t\trc = FAILED;\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"wait for tm_cmpl timeout!\\n\");\n\t\t \n\t\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\t\tio_req->sc_cmd = NULL;\n\t} else {\n\t\t \n\t\tif (io_req->fcp_rsp_code == 0)\n\t\t\trc = SUCCESS;\n\t\telse\n\t\t\trc = FAILED;\n\t}\n\t \n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t \"fcport is uploading, not executing flush.\\n\");\n\t\tgoto no_flush;\n\t}\n\t \n\tkref_put(&io_req->refcount, qedf_release_cmd);\n\n\n\tif (tm_flags == FCP_TMF_LUN_RESET)\n\t\tqedf_flush_active_ios(fcport, lun);\n\telse\n\t\tqedf_flush_active_ios(fcport, -1);\n\nno_flush:\n\tif (rc != SUCCESS) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"task mgmt command failed...\\n\");\n\t\trc = FAILED;\n\t} else {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"task mgmt command success...\\n\");\n\t\trc = SUCCESS;\n\t}\n\treturn rc;\n}\n\nint qedf_initiate_tmf(struct scsi_cmnd *sc_cmd, u8 tm_flags)\n{\n\tstruct fc_rport *rport = starget_to_rport(scsi_target(sc_cmd->device));\n\tstruct fc_rport_libfc_priv *rp = rport->dd_data;\n\tstruct qedf_rport *fcport = (struct qedf_rport *)&rp[1];\n\tstruct qedf_ctx *qedf;\n\tstruct fc_lport *lport = shost_priv(sc_cmd->device->host);\n\tint rc = SUCCESS;\n\tint rval;\n\tstruct qedf_ioreq *io_req = NULL;\n\tint ref_cnt = 0;\n\tstruct fc_rport_priv *rdata = fcport->rdata;\n\n\tQEDF_ERR(NULL,\n\t\t \"tm_flags 0x%x sc_cmd %p op = 0x%02x target_id = 0x%x lun=%d\\n\",\n\t\t tm_flags, sc_cmd, sc_cmd->cmd_len ? sc_cmd->cmnd[0] : 0xff,\n\t\t rport->scsi_target_id, (int)sc_cmd->device->lun);\n\n\tif (!rdata || !kref_get_unless_zero(&rdata->kref)) {\n\t\tQEDF_ERR(NULL, \"stale rport\\n\");\n\t\treturn FAILED;\n\t}\n\n\tQEDF_ERR(NULL, \"portid=%06x tm_flags =%s\\n\", rdata->ids.port_id,\n\t\t (tm_flags == FCP_TMF_TGT_RESET) ? \"TARGET RESET\" :\n\t\t \"LUN RESET\");\n\n\tif (qedf_priv(sc_cmd)->io_req) {\n\t\tio_req = qedf_priv(sc_cmd)->io_req;\n\t\tref_cnt = kref_read(&io_req->refcount);\n\t\tQEDF_ERR(NULL,\n\t\t\t \"orig io_req = %p xid = 0x%x ref_cnt = %d.\\n\",\n\t\t\t io_req, io_req->xid, ref_cnt);\n\t}\n\n\trval = fc_remote_port_chkready(rport);\n\tif (rval) {\n\t\tQEDF_ERR(NULL, \"device_reset rport not ready\\n\");\n\t\trc = FAILED;\n\t\tgoto tmf_err;\n\t}\n\n\trc = fc_block_scsi_eh(sc_cmd);\n\tif (rc)\n\t\tgoto tmf_err;\n\n\tif (!fcport) {\n\t\tQEDF_ERR(NULL, \"device_reset: rport is NULL\\n\");\n\t\trc = FAILED;\n\t\tgoto tmf_err;\n\t}\n\n\tqedf = fcport->qedf;\n\n\tif (!qedf) {\n\t\tQEDF_ERR(NULL, \"qedf is NULL.\\n\");\n\t\trc = FAILED;\n\t\tgoto tmf_err;\n\t}\n\n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\tQEDF_ERR(&qedf->dbg_ctx, \"Connection is getting uploaded.\\n\");\n\t\trc = SUCCESS;\n\t\tgoto tmf_err;\n\t}\n\n\tif (test_bit(QEDF_UNLOADING, &qedf->flags) ||\n\t    test_bit(QEDF_DBG_STOP_IO, &qedf->flags)) {\n\t\trc = SUCCESS;\n\t\tgoto tmf_err;\n\t}\n\n\tif (lport->state != LPORT_ST_READY || !(lport->link_up)) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"link is not ready\\n\");\n\t\trc = FAILED;\n\t\tgoto tmf_err;\n\t}\n\n\tif (test_bit(QEDF_RPORT_UPLOADING_CONNECTION, &fcport->flags)) {\n\t\tif (!fcport->rdata)\n\t\t\tQEDF_ERR(&qedf->dbg_ctx, \"fcport %p is uploading.\\n\",\n\t\t\t\t fcport);\n\t\telse\n\t\t\tQEDF_ERR(&qedf->dbg_ctx,\n\t\t\t\t \"fcport %p port_id=%06x is uploading.\\n\",\n\t\t\t\t fcport, fcport->rdata->ids.port_id);\n\t\trc = FAILED;\n\t\tgoto tmf_err;\n\t}\n\n\trc = qedf_execute_tmf(fcport, sc_cmd, tm_flags);\n\ntmf_err:\n\tkref_put(&rdata->kref, fc_rport_destroy);\n\treturn rc;\n}\n\nvoid qedf_process_tmf_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\n\tstruct qedf_ioreq *io_req)\n{\n\tstruct fcoe_cqe_rsp_info *fcp_rsp;\n\n\tclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\n\n\tfcp_rsp = &cqe->cqe_info.rsp_info;\n\tqedf_parse_fcp_rsp(io_req, fcp_rsp);\n\n\tio_req->sc_cmd = NULL;\n\tcomplete(&io_req->tm_done);\n}\n\nvoid qedf_process_unsol_compl(struct qedf_ctx *qedf, uint16_t que_idx,\n\tstruct fcoe_cqe *cqe)\n{\n\tunsigned long flags;\n\tuint16_t pktlen = cqe->cqe_info.unsolic_info.pkt_len;\n\tu32 payload_len, crc;\n\tstruct fc_frame_header *fh;\n\tstruct fc_frame *fp;\n\tstruct qedf_io_work *io_work;\n\tu32 bdq_idx;\n\tvoid *bdq_addr;\n\tstruct scsi_bd *p_bd_info;\n\n\tp_bd_info = &cqe->cqe_info.unsolic_info.bd_info;\n\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_UNSOL,\n\t\t  \"address.hi=%x, address.lo=%x, opaque_data.hi=%x, opaque_data.lo=%x, bdq_prod_idx=%u, len=%u\\n\",\n\t\t  le32_to_cpu(p_bd_info->address.hi),\n\t\t  le32_to_cpu(p_bd_info->address.lo),\n\t\t  le32_to_cpu(p_bd_info->opaque.fcoe_opaque.hi),\n\t\t  le32_to_cpu(p_bd_info->opaque.fcoe_opaque.lo),\n\t\t  qedf->bdq_prod_idx, pktlen);\n\n\tbdq_idx = le32_to_cpu(p_bd_info->opaque.fcoe_opaque.lo);\n\tif (bdq_idx >= QEDF_BDQ_SIZE) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"bdq_idx is out of range %d.\\n\",\n\t\t    bdq_idx);\n\t\tgoto increment_prod;\n\t}\n\n\tbdq_addr = qedf->bdq[bdq_idx].buf_addr;\n\tif (!bdq_addr) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"bdq_addr is NULL, dropping \"\n\t\t    \"unsolicited packet.\\n\");\n\t\tgoto increment_prod;\n\t}\n\n\tif (qedf_dump_frames) {\n\t\tQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_UNSOL,\n\t\t    \"BDQ frame is at addr=%p.\\n\", bdq_addr);\n\t\tprint_hex_dump(KERN_WARNING, \"bdq \", DUMP_PREFIX_OFFSET, 16, 1,\n\t\t    (void *)bdq_addr, pktlen, false);\n\t}\n\n\t \n\tpayload_len = pktlen - sizeof(struct fc_frame_header);\n\tfp = fc_frame_alloc(qedf->lport, payload_len);\n\tif (!fp) {\n\t\tQEDF_ERR(&(qedf->dbg_ctx), \"Could not allocate fp.\\n\");\n\t\tgoto increment_prod;\n\t}\n\n\t \n\tfh = (struct fc_frame_header *)fc_frame_header_get(fp);\n\tmemcpy(fh, (void *)bdq_addr, pktlen);\n\n\tQEDF_WARN(&qedf->dbg_ctx,\n\t\t  \"Processing Unsolicated frame, src=%06x dest=%06x r_ctl=0x%x type=0x%x cmd=%02x\\n\",\n\t\t  ntoh24(fh->fh_s_id), ntoh24(fh->fh_d_id), fh->fh_r_ctl,\n\t\t  fh->fh_type, fc_frame_payload_op(fp));\n\n\t \n\tcrc = fcoe_fc_crc(fp);\n\tfc_frame_init(fp);\n\tfr_dev(fp) = qedf->lport;\n\tfr_sof(fp) = FC_SOF_I3;\n\tfr_eof(fp) = FC_EOF_T;\n\tfr_crc(fp) = cpu_to_le32(~crc);\n\n\t \n\tio_work = mempool_alloc(qedf->io_mempool, GFP_ATOMIC);\n\tif (!io_work) {\n\t\tQEDF_WARN(&(qedf->dbg_ctx), \"Could not allocate \"\n\t\t\t   \"work for I/O completion.\\n\");\n\t\tfc_frame_free(fp);\n\t\tgoto increment_prod;\n\t}\n\tmemset(io_work, 0, sizeof(struct qedf_io_work));\n\n\tINIT_WORK(&io_work->work, qedf_fp_io_handler);\n\n\t \n\tmemcpy(&io_work->cqe, cqe, sizeof(struct fcoe_cqe));\n\n\tio_work->qedf = qedf;\n\tio_work->fp = fp;\n\n\tqueue_work_on(smp_processor_id(), qedf_io_wq, &io_work->work);\nincrement_prod:\n\tspin_lock_irqsave(&qedf->hba_lock, flags);\n\n\t \n\tqedf->bdq_prod_idx++;\n\n\t \n\tif (qedf->bdq_prod_idx == 0xffff)\n\t\tqedf->bdq_prod_idx = 0;\n\n\twritew(qedf->bdq_prod_idx, qedf->bdq_primary_prod);\n\treadw(qedf->bdq_primary_prod);\n\twritew(qedf->bdq_prod_idx, qedf->bdq_secondary_prod);\n\treadw(qedf->bdq_secondary_prod);\n\n\tspin_unlock_irqrestore(&qedf->hba_lock, flags);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}