{
  "module_name": "hpsa.c",
  "hash_id": "7fb237ed3ea6a0348b6828da9efe8aba542e195f849ba08975417056be66b02f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/hpsa.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/fs.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <linux/spinlock.h>\n#include <linux/compat.h>\n#include <linux/blktrace_api.h>\n#include <linux/uaccess.h>\n#include <linux/io.h>\n#include <linux/dma-mapping.h>\n#include <linux/completion.h>\n#include <linux/moduleparam.h>\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_tcq.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_transport_sas.h>\n#include <scsi/scsi_dbg.h>\n#include <linux/cciss_ioctl.h>\n#include <linux/string.h>\n#include <linux/bitmap.h>\n#include <linux/atomic.h>\n#include <linux/jiffies.h>\n#include <linux/percpu-defs.h>\n#include <linux/percpu.h>\n#include <asm/unaligned.h>\n#include <asm/div64.h>\n#include \"hpsa_cmd.h\"\n#include \"hpsa.h\"\n\n \n#define HPSA_DRIVER_VERSION \"3.4.20-200\"\n#define DRIVER_NAME \"HP HPSA Driver (v \" HPSA_DRIVER_VERSION \")\"\n#define HPSA \"hpsa\"\n\n \n#define CLEAR_EVENT_WAIT_INTERVAL 20\t \n#define MODE_CHANGE_WAIT_INTERVAL 10\t \n#define MAX_CLEAR_EVENT_WAIT 30000\t \n#define MAX_MODE_CHANGE_WAIT 2000\t \n#define MAX_IOCTL_CONFIG_WAIT 1000\n\n \n#define MAX_CMD_RETRIES 3\n \n#define HPSA_EH_PTRAID_TIMEOUT (240 * HZ)\n\n \nMODULE_AUTHOR(\"Hewlett-Packard Company\");\nMODULE_DESCRIPTION(\"Driver for HP Smart Array Controller version \" \\\n\tHPSA_DRIVER_VERSION);\nMODULE_VERSION(HPSA_DRIVER_VERSION);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"cciss\");\n\nstatic int hpsa_simple_mode;\nmodule_param(hpsa_simple_mode, int, S_IRUGO|S_IWUSR);\nMODULE_PARM_DESC(hpsa_simple_mode,\n\t\"Use 'simple mode' rather than 'performant mode'\");\n\n \nstatic const struct pci_device_id hpsa_pci_device_id[] = {\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3241},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3243},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3245},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3247},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3249},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324A},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324B},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3233},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3350},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3351},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3352},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3353},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3354},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3355},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3356},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103c, 0x1920},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1921},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1922},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1923},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1924},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103c, 0x1925},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1926},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1928},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1929},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BD},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BE},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BF},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C0},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C1},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C2},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C3},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C4},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C5},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C6},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C7},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C8},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C9},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CA},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CB},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CC},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CD},\n\t{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CE},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0580},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0581},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0582},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0583},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0584},\n\t{PCI_VENDOR_ID_ADAPTEC2, 0x0290, 0x9005, 0x0585},\n\t{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0076},\n\t{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0087},\n\t{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x007D},\n\t{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0088},\n\t{PCI_VENDOR_ID_HP, 0x333f, 0x103c, 0x333f},\n\t{PCI_VENDOR_ID_HP,     PCI_ANY_ID,\tPCI_ANY_ID, PCI_ANY_ID,\n\t\tPCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},\n\t{PCI_VENDOR_ID_COMPAQ,     PCI_ANY_ID,\tPCI_ANY_ID, PCI_ANY_ID,\n\t\tPCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},\n\t{0,}\n};\n\nMODULE_DEVICE_TABLE(pci, hpsa_pci_device_id);\n\n \nstatic struct board_type products[] = {\n\t{0x40700E11, \"Smart Array 5300\", &SA5A_access},\n\t{0x40800E11, \"Smart Array 5i\", &SA5B_access},\n\t{0x40820E11, \"Smart Array 532\", &SA5B_access},\n\t{0x40830E11, \"Smart Array 5312\", &SA5B_access},\n\t{0x409A0E11, \"Smart Array 641\", &SA5A_access},\n\t{0x409B0E11, \"Smart Array 642\", &SA5A_access},\n\t{0x409C0E11, \"Smart Array 6400\", &SA5A_access},\n\t{0x409D0E11, \"Smart Array 6400 EM\", &SA5A_access},\n\t{0x40910E11, \"Smart Array 6i\", &SA5A_access},\n\t{0x3225103C, \"Smart Array P600\", &SA5A_access},\n\t{0x3223103C, \"Smart Array P800\", &SA5A_access},\n\t{0x3234103C, \"Smart Array P400\", &SA5A_access},\n\t{0x3235103C, \"Smart Array P400i\", &SA5A_access},\n\t{0x3211103C, \"Smart Array E200i\", &SA5A_access},\n\t{0x3212103C, \"Smart Array E200\", &SA5A_access},\n\t{0x3213103C, \"Smart Array E200i\", &SA5A_access},\n\t{0x3214103C, \"Smart Array E200i\", &SA5A_access},\n\t{0x3215103C, \"Smart Array E200i\", &SA5A_access},\n\t{0x3237103C, \"Smart Array E500\", &SA5A_access},\n\t{0x323D103C, \"Smart Array P700m\", &SA5A_access},\n\t{0x3241103C, \"Smart Array P212\", &SA5_access},\n\t{0x3243103C, \"Smart Array P410\", &SA5_access},\n\t{0x3245103C, \"Smart Array P410i\", &SA5_access},\n\t{0x3247103C, \"Smart Array P411\", &SA5_access},\n\t{0x3249103C, \"Smart Array P812\", &SA5_access},\n\t{0x324A103C, \"Smart Array P712m\", &SA5_access},\n\t{0x324B103C, \"Smart Array P711m\", &SA5_access},\n\t{0x3233103C, \"HP StorageWorks 1210m\", &SA5_access},  \n\t{0x3350103C, \"Smart Array P222\", &SA5_access},\n\t{0x3351103C, \"Smart Array P420\", &SA5_access},\n\t{0x3352103C, \"Smart Array P421\", &SA5_access},\n\t{0x3353103C, \"Smart Array P822\", &SA5_access},\n\t{0x3354103C, \"Smart Array P420i\", &SA5_access},\n\t{0x3355103C, \"Smart Array P220i\", &SA5_access},\n\t{0x3356103C, \"Smart Array P721m\", &SA5_access},\n\t{0x1920103C, \"Smart Array P430i\", &SA5_access},\n\t{0x1921103C, \"Smart Array P830i\", &SA5_access},\n\t{0x1922103C, \"Smart Array P430\", &SA5_access},\n\t{0x1923103C, \"Smart Array P431\", &SA5_access},\n\t{0x1924103C, \"Smart Array P830\", &SA5_access},\n\t{0x1925103C, \"Smart Array P831\", &SA5_access},\n\t{0x1926103C, \"Smart Array P731m\", &SA5_access},\n\t{0x1928103C, \"Smart Array P230i\", &SA5_access},\n\t{0x1929103C, \"Smart Array P530\", &SA5_access},\n\t{0x21BD103C, \"Smart Array P244br\", &SA5_access},\n\t{0x21BE103C, \"Smart Array P741m\", &SA5_access},\n\t{0x21BF103C, \"Smart HBA H240ar\", &SA5_access},\n\t{0x21C0103C, \"Smart Array P440ar\", &SA5_access},\n\t{0x21C1103C, \"Smart Array P840ar\", &SA5_access},\n\t{0x21C2103C, \"Smart Array P440\", &SA5_access},\n\t{0x21C3103C, \"Smart Array P441\", &SA5_access},\n\t{0x21C4103C, \"Smart Array\", &SA5_access},\n\t{0x21C5103C, \"Smart Array P841\", &SA5_access},\n\t{0x21C6103C, \"Smart HBA H244br\", &SA5_access},\n\t{0x21C7103C, \"Smart HBA H240\", &SA5_access},\n\t{0x21C8103C, \"Smart HBA H241\", &SA5_access},\n\t{0x21C9103C, \"Smart Array\", &SA5_access},\n\t{0x21CA103C, \"Smart Array P246br\", &SA5_access},\n\t{0x21CB103C, \"Smart Array P840\", &SA5_access},\n\t{0x21CC103C, \"Smart Array\", &SA5_access},\n\t{0x21CD103C, \"Smart Array\", &SA5_access},\n\t{0x21CE103C, \"Smart HBA\", &SA5_access},\n\t{0x05809005, \"SmartHBA-SA\", &SA5_access},\n\t{0x05819005, \"SmartHBA-SA 8i\", &SA5_access},\n\t{0x05829005, \"SmartHBA-SA 8i8e\", &SA5_access},\n\t{0x05839005, \"SmartHBA-SA 8e\", &SA5_access},\n\t{0x05849005, \"SmartHBA-SA 16i\", &SA5_access},\n\t{0x05859005, \"SmartHBA-SA 4i4e\", &SA5_access},\n\t{0x00761590, \"HP Storage P1224 Array Controller\", &SA5_access},\n\t{0x00871590, \"HP Storage P1224e Array Controller\", &SA5_access},\n\t{0x007D1590, \"HP Storage P1228 Array Controller\", &SA5_access},\n\t{0x00881590, \"HP Storage P1228e Array Controller\", &SA5_access},\n\t{0x333f103c, \"HP StorageWorks 1210m Array Controller\", &SA5_access},\n\t{0xFFFF103C, \"Unknown Smart Array\", &SA5_access},\n};\n\nstatic struct scsi_transport_template *hpsa_sas_transport_template;\nstatic int hpsa_add_sas_host(struct ctlr_info *h);\nstatic void hpsa_delete_sas_host(struct ctlr_info *h);\nstatic int hpsa_add_sas_device(struct hpsa_sas_node *hpsa_sas_node,\n\t\t\tstruct hpsa_scsi_dev_t *device);\nstatic void hpsa_remove_sas_device(struct hpsa_scsi_dev_t *device);\nstatic struct hpsa_scsi_dev_t\n\t*hpsa_find_device_by_sas_rphy(struct ctlr_info *h,\n\t\tstruct sas_rphy *rphy);\n\n#define SCSI_CMD_BUSY ((struct scsi_cmnd *)&hpsa_cmd_busy)\nstatic const struct scsi_cmnd hpsa_cmd_busy;\n#define SCSI_CMD_IDLE ((struct scsi_cmnd *)&hpsa_cmd_idle)\nstatic const struct scsi_cmnd hpsa_cmd_idle;\nstatic int number_of_controllers;\n\nstatic irqreturn_t do_hpsa_intr_intx(int irq, void *dev_id);\nstatic irqreturn_t do_hpsa_intr_msi(int irq, void *dev_id);\nstatic int hpsa_ioctl(struct scsi_device *dev, unsigned int cmd,\n\t\t      void __user *arg);\nstatic int hpsa_passthru_ioctl(struct ctlr_info *h,\n\t\t\t       IOCTL_Command_struct *iocommand);\nstatic int hpsa_big_passthru_ioctl(struct ctlr_info *h,\n\t\t\t\t   BIG_IOCTL_Command_struct *ioc);\n\n#ifdef CONFIG_COMPAT\nstatic int hpsa_compat_ioctl(struct scsi_device *dev, unsigned int cmd,\n\tvoid __user *arg);\n#endif\n\nstatic void cmd_free(struct ctlr_info *h, struct CommandList *c);\nstatic struct CommandList *cmd_alloc(struct ctlr_info *h);\nstatic void cmd_tagged_free(struct ctlr_info *h, struct CommandList *c);\nstatic struct CommandList *cmd_tagged_alloc(struct ctlr_info *h,\n\t\t\t\t\t    struct scsi_cmnd *scmd);\nstatic int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,\n\tvoid *buff, size_t size, u16 page_code, unsigned char *scsi3addr,\n\tint cmd_type);\nstatic void hpsa_free_cmd_pool(struct ctlr_info *h);\n#define VPD_PAGE (1 << 8)\n#define HPSA_SIMPLE_ERROR_BITS 0x03\n\nstatic int hpsa_scsi_queue_command(struct Scsi_Host *h, struct scsi_cmnd *cmd);\nstatic void hpsa_scan_start(struct Scsi_Host *);\nstatic int hpsa_scan_finished(struct Scsi_Host *sh,\n\tunsigned long elapsed_time);\nstatic int hpsa_change_queue_depth(struct scsi_device *sdev, int qdepth);\n\nstatic int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd);\nstatic int hpsa_slave_alloc(struct scsi_device *sdev);\nstatic int hpsa_slave_configure(struct scsi_device *sdev);\nstatic void hpsa_slave_destroy(struct scsi_device *sdev);\n\nstatic void hpsa_update_scsi_devices(struct ctlr_info *h);\nstatic int check_for_unit_attention(struct ctlr_info *h,\n\tstruct CommandList *c);\nstatic void check_ioctl_unit_attention(struct ctlr_info *h,\n\tstruct CommandList *c);\n \nstatic void calc_bucket_map(int *bucket, int num_buckets,\n\tint nsgs, int min_blocks, u32 *bucket_map);\nstatic void hpsa_free_performant_mode(struct ctlr_info *h);\nstatic int hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h);\nstatic inline u32 next_command(struct ctlr_info *h, u8 q);\nstatic int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,\n\t\t\t       u32 *cfg_base_addr, u64 *cfg_base_addr_index,\n\t\t\t       u64 *cfg_offset);\nstatic int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,\n\t\t\t\t    unsigned long *memory_bar);\nstatic int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id,\n\t\t\t\tbool *legacy_board);\nstatic int wait_for_device_to_become_ready(struct ctlr_info *h,\n\t\t\t\t\t   unsigned char lunaddr[],\n\t\t\t\t\t   int reply_queue);\nstatic int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,\n\t\t\t\t     int wait_for_ready);\nstatic inline void finish_cmd(struct CommandList *c);\nstatic int hpsa_wait_for_mode_change_ack(struct ctlr_info *h);\n#define BOARD_NOT_READY 0\n#define BOARD_READY 1\nstatic void hpsa_drain_accel_commands(struct ctlr_info *h);\nstatic void hpsa_flush_cache(struct ctlr_info *h);\nstatic int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,\n\tstruct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,\n\tu8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk);\nstatic void hpsa_command_resubmit_worker(struct work_struct *work);\nstatic u32 lockup_detected(struct ctlr_info *h);\nstatic int detect_controller_lockup(struct ctlr_info *h);\nstatic void hpsa_disable_rld_caching(struct ctlr_info *h);\nstatic inline int hpsa_scsi_do_report_phys_luns(struct ctlr_info *h,\n\tstruct ReportExtendedLUNdata *buf, int bufsize);\nstatic bool hpsa_vpd_page_supported(struct ctlr_info *h,\n\tunsigned char scsi3addr[], u8 page);\nstatic int hpsa_luns_changed(struct ctlr_info *h);\nstatic bool hpsa_cmd_dev_match(struct ctlr_info *h, struct CommandList *c,\n\t\t\t       struct hpsa_scsi_dev_t *dev,\n\t\t\t       unsigned char *scsi3addr);\n\nstatic inline struct ctlr_info *sdev_to_hba(struct scsi_device *sdev)\n{\n\tunsigned long *priv = shost_priv(sdev->host);\n\treturn (struct ctlr_info *) *priv;\n}\n\nstatic inline struct ctlr_info *shost_to_hba(struct Scsi_Host *sh)\n{\n\tunsigned long *priv = shost_priv(sh);\n\treturn (struct ctlr_info *) *priv;\n}\n\nstatic inline bool hpsa_is_cmd_idle(struct CommandList *c)\n{\n\treturn c->scsi_cmd == SCSI_CMD_IDLE;\n}\n\n \nstatic void decode_sense_data(const u8 *sense_data, int sense_data_len,\n\t\t\tu8 *sense_key, u8 *asc, u8 *ascq)\n{\n\tstruct scsi_sense_hdr sshdr;\n\tbool rc;\n\n\t*sense_key = -1;\n\t*asc = -1;\n\t*ascq = -1;\n\n\tif (sense_data_len < 1)\n\t\treturn;\n\n\trc = scsi_normalize_sense(sense_data, sense_data_len, &sshdr);\n\tif (rc) {\n\t\t*sense_key = sshdr.sense_key;\n\t\t*asc = sshdr.asc;\n\t\t*ascq = sshdr.ascq;\n\t}\n}\n\nstatic int check_for_unit_attention(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tu8 sense_key, asc, ascq;\n\tint sense_len;\n\n\tif (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))\n\t\tsense_len = sizeof(c->err_info->SenseInfo);\n\telse\n\t\tsense_len = c->err_info->SenseLen;\n\n\tdecode_sense_data(c->err_info->SenseInfo, sense_len,\n\t\t\t\t&sense_key, &asc, &ascq);\n\tif (sense_key != UNIT_ATTENTION || asc == 0xff)\n\t\treturn 0;\n\n\tswitch (asc) {\n\tcase STATE_CHANGED:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: a state change detected, command retried\\n\",\n\t\t\th->devname);\n\t\tbreak;\n\tcase LUN_FAILED:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: LUN failure detected\\n\", h->devname);\n\t\tbreak;\n\tcase REPORT_LUNS_CHANGED:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: report LUN data changed\\n\", h->devname);\n\t \n\t\tbreak;\n\tcase POWER_OR_RESET:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: a power on or device reset detected\\n\",\n\t\t\th->devname);\n\t\tbreak;\n\tcase UNIT_ATTENTION_CLEARED:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: unit attention cleared by another initiator\\n\",\n\t\t\th->devname);\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: unknown unit attention detected\\n\",\n\t\t\th->devname);\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\nstatic int check_for_busy(struct ctlr_info *h, struct CommandList *c)\n{\n\tif (c->err_info->CommandStatus != CMD_TARGET_STATUS ||\n\t\t(c->err_info->ScsiStatus != SAM_STAT_BUSY &&\n\t\t c->err_info->ScsiStatus != SAM_STAT_TASK_SET_FULL))\n\t\treturn 0;\n\tdev_warn(&h->pdev->dev, HPSA \"device busy\");\n\treturn 1;\n}\n\nstatic u32 lockup_detected(struct ctlr_info *h);\nstatic ssize_t host_show_lockup_detected(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tint ld;\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\tld = lockup_detected(h);\n\n\treturn sprintf(buf, \"ld=%d\\n\", ld);\n}\n\nstatic ssize_t host_store_hp_ssd_smart_path_status(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t const char *buf, size_t count)\n{\n\tint status, len;\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tchar tmpbuf[10];\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\tlen = count > sizeof(tmpbuf) - 1 ? sizeof(tmpbuf) - 1 : count;\n\tstrncpy(tmpbuf, buf, len);\n\ttmpbuf[len] = '\\0';\n\tif (sscanf(tmpbuf, \"%d\", &status) != 1)\n\t\treturn -EINVAL;\n\th = shost_to_hba(shost);\n\th->acciopath_status = !!status;\n\tdev_warn(&h->pdev->dev,\n\t\t\"hpsa: HP SSD Smart Path %s via sysfs update.\\n\",\n\t\th->acciopath_status ? \"enabled\" : \"disabled\");\n\treturn count;\n}\n\nstatic ssize_t host_store_raid_offload_debug(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t const char *buf, size_t count)\n{\n\tint debug_level, len;\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tchar tmpbuf[10];\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\tlen = count > sizeof(tmpbuf) - 1 ? sizeof(tmpbuf) - 1 : count;\n\tstrncpy(tmpbuf, buf, len);\n\ttmpbuf[len] = '\\0';\n\tif (sscanf(tmpbuf, \"%d\", &debug_level) != 1)\n\t\treturn -EINVAL;\n\tif (debug_level < 0)\n\t\tdebug_level = 0;\n\th = shost_to_hba(shost);\n\th->raid_offload_debug = debug_level;\n\tdev_warn(&h->pdev->dev, \"hpsa: Set raid_offload_debug level = %d\\n\",\n\t\th->raid_offload_debug);\n\treturn count;\n}\n\nstatic ssize_t host_store_rescan(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\th = shost_to_hba(shost);\n\thpsa_scan_start(h->scsi_host);\n\treturn count;\n}\n\nstatic void hpsa_turn_off_ioaccel_for_device(struct hpsa_scsi_dev_t *device)\n{\n\tdevice->offload_enabled = 0;\n\tdevice->offload_to_be_enabled = 0;\n}\n\nstatic ssize_t host_show_firmware_revision(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tunsigned char *fwrev;\n\n\th = shost_to_hba(shost);\n\tif (!h->hba_inquiry_data)\n\t\treturn 0;\n\tfwrev = &h->hba_inquiry_data[32];\n\treturn snprintf(buf, 20, \"%c%c%c%c\\n\",\n\t\tfwrev[0], fwrev[1], fwrev[2], fwrev[3]);\n}\n\nstatic ssize_t host_show_commands_outstanding(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ctlr_info *h = shost_to_hba(shost);\n\n\treturn snprintf(buf, 20, \"%d\\n\",\n\t\t\tatomic_read(&h->commands_outstanding));\n}\n\nstatic ssize_t host_show_transport_mode(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\treturn snprintf(buf, 20, \"%s\\n\",\n\t\th->transMethod & CFGTBL_Trans_Performant ?\n\t\t\t\"performant\" : \"simple\");\n}\n\nstatic ssize_t host_show_hp_ssd_smart_path_status(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\treturn snprintf(buf, 30, \"HP SSD Smart Path %s\\n\",\n\t\t(h->acciopath_status == 1) ?  \"enabled\" : \"disabled\");\n}\n\n \nstatic u32 unresettable_controller[] = {\n\t0x324a103C,  \n\t0x324b103C,  \n\t0x3223103C,  \n\t0x3234103C,  \n\t0x3235103C,  \n\t0x3211103C,  \n\t0x3212103C,  \n\t0x3213103C,  \n\t0x3214103C,  \n\t0x3215103C,  \n\t0x3237103C,  \n\t0x323D103C,  \n\t0x40800E11,  \n\t0x409C0E11,  \n\t0x409D0E11,  \n\t0x40700E11,  \n\t0x40820E11,  \n\t0x40830E11,  \n\t0x409A0E11,  \n\t0x409B0E11,  \n\t0x40910E11,  \n};\n\n \nstatic u32 soft_unresettable_controller[] = {\n\t0x40800E11,  \n\t0x40700E11,  \n\t0x40820E11,  \n\t0x40830E11,  \n\t0x409A0E11,  \n\t0x409B0E11,  \n\t0x40910E11,  \n\t \n\t0x409C0E11,  \n\t0x409D0E11,  \n};\n\nstatic int board_id_in_array(u32 a[], int nelems, u32 board_id)\n{\n\tint i;\n\n\tfor (i = 0; i < nelems; i++)\n\t\tif (a[i] == board_id)\n\t\t\treturn 1;\n\treturn 0;\n}\n\nstatic int ctlr_is_hard_resettable(u32 board_id)\n{\n\treturn !board_id_in_array(unresettable_controller,\n\t\t\tARRAY_SIZE(unresettable_controller), board_id);\n}\n\nstatic int ctlr_is_soft_resettable(u32 board_id)\n{\n\treturn !board_id_in_array(soft_unresettable_controller,\n\t\t\tARRAY_SIZE(soft_unresettable_controller), board_id);\n}\n\nstatic int ctlr_is_resettable(u32 board_id)\n{\n\treturn ctlr_is_hard_resettable(board_id) ||\n\t\tctlr_is_soft_resettable(board_id);\n}\n\nstatic ssize_t host_show_resettable(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\treturn snprintf(buf, 20, \"%d\\n\", ctlr_is_resettable(h->board_id));\n}\n\nstatic inline int is_logical_dev_addr_mode(unsigned char scsi3addr[])\n{\n\treturn (scsi3addr[3] & 0xC0) == 0x40;\n}\n\nstatic const char * const raid_label[] = { \"0\", \"4\", \"1(+0)\", \"5\", \"5+1\", \"6\",\n\t\"1(+0)ADM\", \"UNKNOWN\", \"PHYS DRV\"\n};\n#define HPSA_RAID_0\t0\n#define HPSA_RAID_4\t1\n#define HPSA_RAID_1\t2\t \n#define HPSA_RAID_5\t3\t \n#define HPSA_RAID_51\t4\n#define HPSA_RAID_6\t5\t \n#define HPSA_RAID_ADM\t6\t \n#define RAID_UNKNOWN (ARRAY_SIZE(raid_label) - 2)\n#define PHYSICAL_DRIVE (ARRAY_SIZE(raid_label) - 1)\n\nstatic inline bool is_logical_device(struct hpsa_scsi_dev_t *device)\n{\n\treturn !device->physical_device;\n}\n\nstatic ssize_t raid_level_show(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tssize_t l = 0;\n\tunsigned char rlevel;\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->lock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tif (!is_logical_device(hdev)) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\tl = snprintf(buf, PAGE_SIZE, \"N/A\\n\");\n\t\treturn l;\n\t}\n\n\trlevel = hdev->raid_level;\n\tspin_unlock_irqrestore(&h->lock, flags);\n\tif (rlevel > RAID_UNKNOWN)\n\t\trlevel = RAID_UNKNOWN;\n\tl = snprintf(buf, PAGE_SIZE, \"RAID %s\\n\", raid_label[rlevel]);\n\treturn l;\n}\n\nstatic ssize_t lunid_show(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\tunsigned char lunid[8];\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->lock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\tmemcpy(lunid, hdev->scsi3addr, sizeof(lunid));\n\tspin_unlock_irqrestore(&h->lock, flags);\n\treturn snprintf(buf, 20, \"0x%8phN\\n\", lunid);\n}\n\nstatic ssize_t unique_id_show(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\tunsigned char sn[16];\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->lock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\tmemcpy(sn, hdev->device_id, sizeof(sn));\n\tspin_unlock_irqrestore(&h->lock, flags);\n\treturn snprintf(buf, 16 * 2 + 2,\n\t\t\t\"%02X%02X%02X%02X%02X%02X%02X%02X\"\n\t\t\t\"%02X%02X%02X%02X%02X%02X%02X%02X\\n\",\n\t\t\tsn[0], sn[1], sn[2], sn[3],\n\t\t\tsn[4], sn[5], sn[6], sn[7],\n\t\t\tsn[8], sn[9], sn[10], sn[11],\n\t\t\tsn[12], sn[13], sn[14], sn[15]);\n}\n\nstatic ssize_t sas_address_show(struct device *dev,\n\t      struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\tu64 sas_address;\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->lock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev || is_logical_device(hdev) || !hdev->expose_device) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\tsas_address = hdev->sas_address;\n\tspin_unlock_irqrestore(&h->lock, flags);\n\n\treturn snprintf(buf, PAGE_SIZE, \"0x%016llx\\n\", sas_address);\n}\n\nstatic ssize_t host_show_hp_ssd_smart_path_enabled(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\tint offload_enabled;\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->lock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\toffload_enabled = hdev->offload_enabled;\n\tspin_unlock_irqrestore(&h->lock, flags);\n\n\tif (hdev->devtype == TYPE_DISK || hdev->devtype == TYPE_ZBC)\n\t\treturn snprintf(buf, 20, \"%d\\n\", offload_enabled);\n\telse\n\t\treturn snprintf(buf, 40, \"%s\\n\",\n\t\t\t\t\"Not applicable for a controller\");\n}\n\n#define MAX_PATHS 8\nstatic ssize_t path_info_show(struct device *dev,\n\t     struct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct scsi_device *sdev;\n\tstruct hpsa_scsi_dev_t *hdev;\n\tunsigned long flags;\n\tint i;\n\tint output_len = 0;\n\tu8 box;\n\tu8 bay;\n\tu8 path_map_index = 0;\n\tchar *active;\n\tunsigned char phys_connector[2];\n\n\tsdev = to_scsi_device(dev);\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->devlock, flags);\n\thdev = sdev->hostdata;\n\tif (!hdev) {\n\t\tspin_unlock_irqrestore(&h->devlock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tbay = hdev->bay;\n\tfor (i = 0; i < MAX_PATHS; i++) {\n\t\tpath_map_index = 1<<i;\n\t\tif (i == hdev->active_path_index)\n\t\t\tactive = \"Active\";\n\t\telse if (hdev->path_map & path_map_index)\n\t\t\tactive = \"Inactive\";\n\t\telse\n\t\t\tcontinue;\n\n\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\"[%d:%d:%d:%d] %20.20s \",\n\t\t\t\th->scsi_host->host_no,\n\t\t\t\thdev->bus, hdev->target, hdev->lun,\n\t\t\t\tscsi_device_type(hdev->devtype));\n\n\t\tif (hdev->devtype == TYPE_RAID || is_logical_device(hdev)) {\n\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\t\"%s\\n\", active);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbox = hdev->box[i];\n\t\tmemcpy(&phys_connector, &hdev->phys_connector[i],\n\t\t\tsizeof(phys_connector));\n\t\tif (phys_connector[0] < '0')\n\t\t\tphys_connector[0] = '0';\n\t\tif (phys_connector[1] < '0')\n\t\t\tphys_connector[1] = '0';\n\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\"PORT: %.2s \",\n\t\t\t\tphys_connector);\n\t\tif ((hdev->devtype == TYPE_DISK || hdev->devtype == TYPE_ZBC) &&\n\t\t\thdev->expose_device) {\n\t\t\tif (box == 0 || box == 0xFF) {\n\t\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\"BAY: %hhu %s\\n\",\n\t\t\t\t\tbay, active);\n\t\t\t} else {\n\t\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\"BOX: %hhu BAY: %hhu %s\\n\",\n\t\t\t\t\tbox, bay, active);\n\t\t\t}\n\t\t} else if (box != 0 && box != 0xFF) {\n\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\tPAGE_SIZE - output_len, \"BOX: %hhu %s\\n\",\n\t\t\t\tbox, active);\n\t\t} else\n\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\tPAGE_SIZE - output_len, \"%s\\n\", active);\n\t}\n\n\tspin_unlock_irqrestore(&h->devlock, flags);\n\treturn output_len;\n}\n\nstatic ssize_t host_show_ctlr_num(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\treturn snprintf(buf, 20, \"%d\\n\", h->ctlr);\n}\n\nstatic ssize_t host_show_legacy_board(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ctlr_info *h;\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\th = shost_to_hba(shost);\n\treturn snprintf(buf, 20, \"%d\\n\", h->legacy_board ? 1 : 0);\n}\n\nstatic DEVICE_ATTR_RO(raid_level);\nstatic DEVICE_ATTR_RO(lunid);\nstatic DEVICE_ATTR_RO(unique_id);\nstatic DEVICE_ATTR(rescan, S_IWUSR, NULL, host_store_rescan);\nstatic DEVICE_ATTR_RO(sas_address);\nstatic DEVICE_ATTR(hp_ssd_smart_path_enabled, S_IRUGO,\n\t\t\thost_show_hp_ssd_smart_path_enabled, NULL);\nstatic DEVICE_ATTR_RO(path_info);\nstatic DEVICE_ATTR(hp_ssd_smart_path_status, S_IWUSR|S_IRUGO|S_IROTH,\n\t\thost_show_hp_ssd_smart_path_status,\n\t\thost_store_hp_ssd_smart_path_status);\nstatic DEVICE_ATTR(raid_offload_debug, S_IWUSR, NULL,\n\t\t\thost_store_raid_offload_debug);\nstatic DEVICE_ATTR(firmware_revision, S_IRUGO,\n\thost_show_firmware_revision, NULL);\nstatic DEVICE_ATTR(commands_outstanding, S_IRUGO,\n\thost_show_commands_outstanding, NULL);\nstatic DEVICE_ATTR(transport_mode, S_IRUGO,\n\thost_show_transport_mode, NULL);\nstatic DEVICE_ATTR(resettable, S_IRUGO,\n\thost_show_resettable, NULL);\nstatic DEVICE_ATTR(lockup_detected, S_IRUGO,\n\thost_show_lockup_detected, NULL);\nstatic DEVICE_ATTR(ctlr_num, S_IRUGO,\n\thost_show_ctlr_num, NULL);\nstatic DEVICE_ATTR(legacy_board, S_IRUGO,\n\thost_show_legacy_board, NULL);\n\nstatic struct attribute *hpsa_sdev_attrs[] = {\n\t&dev_attr_raid_level.attr,\n\t&dev_attr_lunid.attr,\n\t&dev_attr_unique_id.attr,\n\t&dev_attr_hp_ssd_smart_path_enabled.attr,\n\t&dev_attr_path_info.attr,\n\t&dev_attr_sas_address.attr,\n\tNULL,\n};\n\nATTRIBUTE_GROUPS(hpsa_sdev);\n\nstatic struct attribute *hpsa_shost_attrs[] = {\n\t&dev_attr_rescan.attr,\n\t&dev_attr_firmware_revision.attr,\n\t&dev_attr_commands_outstanding.attr,\n\t&dev_attr_transport_mode.attr,\n\t&dev_attr_resettable.attr,\n\t&dev_attr_hp_ssd_smart_path_status.attr,\n\t&dev_attr_raid_offload_debug.attr,\n\t&dev_attr_lockup_detected.attr,\n\t&dev_attr_ctlr_num.attr,\n\t&dev_attr_legacy_board.attr,\n\tNULL,\n};\n\nATTRIBUTE_GROUPS(hpsa_shost);\n\n#define HPSA_NRESERVED_CMDS\t(HPSA_CMDS_RESERVED_FOR_DRIVER +\\\n\t\t\t\t HPSA_MAX_CONCURRENT_PASSTHRUS)\n\nstatic const struct scsi_host_template hpsa_driver_template = {\n\t.module\t\t\t= THIS_MODULE,\n\t.name\t\t\t= HPSA,\n\t.proc_name\t\t= HPSA,\n\t.queuecommand\t\t= hpsa_scsi_queue_command,\n\t.scan_start\t\t= hpsa_scan_start,\n\t.scan_finished\t\t= hpsa_scan_finished,\n\t.change_queue_depth\t= hpsa_change_queue_depth,\n\t.this_id\t\t= -1,\n\t.eh_device_reset_handler = hpsa_eh_device_reset_handler,\n\t.ioctl\t\t\t= hpsa_ioctl,\n\t.slave_alloc\t\t= hpsa_slave_alloc,\n\t.slave_configure\t= hpsa_slave_configure,\n\t.slave_destroy\t\t= hpsa_slave_destroy,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t\t= hpsa_compat_ioctl,\n#endif\n\t.sdev_groups = hpsa_sdev_groups,\n\t.shost_groups = hpsa_shost_groups,\n\t.max_sectors = 2048,\n\t.no_write_same = 1,\n};\n\nstatic inline u32 next_command(struct ctlr_info *h, u8 q)\n{\n\tu32 a;\n\tstruct reply_queue_buffer *rq = &h->reply_queue[q];\n\n\tif (h->transMethod & CFGTBL_Trans_io_accel1)\n\t\treturn h->access.command_completed(h, q);\n\n\tif (unlikely(!(h->transMethod & CFGTBL_Trans_Performant)))\n\t\treturn h->access.command_completed(h, q);\n\n\tif ((rq->head[rq->current_entry] & 1) == rq->wraparound) {\n\t\ta = rq->head[rq->current_entry];\n\t\trq->current_entry++;\n\t\tatomic_dec(&h->commands_outstanding);\n\t} else {\n\t\ta = FIFO_EMPTY;\n\t}\n\t \n\tif (rq->current_entry == h->max_commands) {\n\t\trq->current_entry = 0;\n\t\trq->wraparound ^= 1;\n\t}\n\treturn a;\n}\n\n \n\n \n#define DEFAULT_REPLY_QUEUE (-1)\nstatic void set_performant_mode(struct ctlr_info *h, struct CommandList *c,\n\t\t\t\t\tint reply_queue)\n{\n\tif (likely(h->transMethod & CFGTBL_Trans_Performant)) {\n\t\tc->busaddr |= 1 | (h->blockFetchTable[c->Header.SGList] << 1);\n\t\tif (unlikely(!h->msix_vectors))\n\t\t\treturn;\n\t\tc->Header.ReplyQueue = reply_queue;\n\t}\n}\n\nstatic void set_ioaccel1_performant_mode(struct ctlr_info *h,\n\t\t\t\t\t\tstruct CommandList *c,\n\t\t\t\t\t\tint reply_queue)\n{\n\tstruct io_accel1_cmd *cp = &h->ioaccel_cmd_pool[c->cmdindex];\n\n\t \n\tcp->ReplyQueue = reply_queue;\n\t \n\tc->busaddr |= 1 | (h->ioaccel1_blockFetchTable[c->Header.SGList] << 1) |\n\t\t\t\t\tIOACCEL1_BUSADDR_CMDTYPE;\n}\n\nstatic void set_ioaccel2_tmf_performant_mode(struct ctlr_info *h,\n\t\t\t\t\t\tstruct CommandList *c,\n\t\t\t\t\t\tint reply_queue)\n{\n\tstruct hpsa_tmf_struct *cp = (struct hpsa_tmf_struct *)\n\t\t&h->ioaccel2_cmd_pool[c->cmdindex];\n\n\t \n\tcp->reply_queue = reply_queue;\n\t \n\tc->busaddr |= h->ioaccel2_blockFetchTable[0];\n}\n\nstatic void set_ioaccel2_performant_mode(struct ctlr_info *h,\n\t\t\t\t\t\tstruct CommandList *c,\n\t\t\t\t\t\tint reply_queue)\n{\n\tstruct io_accel2_cmd *cp = &h->ioaccel2_cmd_pool[c->cmdindex];\n\n\t \n\tcp->reply_queue = reply_queue;\n\t \n\tc->busaddr |= (h->ioaccel2_blockFetchTable[cp->sg_count]);\n}\n\nstatic int is_firmware_flash_cmd(u8 *cdb)\n{\n\treturn cdb[0] == BMIC_WRITE && cdb[6] == BMIC_FLASH_FIRMWARE;\n}\n\n \n#define HEARTBEAT_SAMPLE_INTERVAL_DURING_FLASH (240 * HZ)\n#define HEARTBEAT_SAMPLE_INTERVAL (30 * HZ)\n#define HPSA_EVENT_MONITOR_INTERVAL (15 * HZ)\nstatic void dial_down_lockup_detection_during_fw_flash(struct ctlr_info *h,\n\t\tstruct CommandList *c)\n{\n\tif (!is_firmware_flash_cmd(c->Request.CDB))\n\t\treturn;\n\tatomic_inc(&h->firmware_flash_in_progress);\n\th->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL_DURING_FLASH;\n}\n\nstatic void dial_up_lockup_detection_on_fw_flash_complete(struct ctlr_info *h,\n\t\tstruct CommandList *c)\n{\n\tif (is_firmware_flash_cmd(c->Request.CDB) &&\n\t\tatomic_dec_and_test(&h->firmware_flash_in_progress))\n\t\th->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;\n}\n\nstatic void __enqueue_cmd_and_start_io(struct ctlr_info *h,\n\tstruct CommandList *c, int reply_queue)\n{\n\tdial_down_lockup_detection_during_fw_flash(h, c);\n\tatomic_inc(&h->commands_outstanding);\n\t \n\tif (c->device && !c->retry_pending)\n\t\tatomic_inc(&c->device->commands_outstanding);\n\n\treply_queue = h->reply_map[raw_smp_processor_id()];\n\tswitch (c->cmd_type) {\n\tcase CMD_IOACCEL1:\n\t\tset_ioaccel1_performant_mode(h, c, reply_queue);\n\t\twritel(c->busaddr, h->vaddr + SA5_REQUEST_PORT_OFFSET);\n\t\tbreak;\n\tcase CMD_IOACCEL2:\n\t\tset_ioaccel2_performant_mode(h, c, reply_queue);\n\t\twritel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);\n\t\tbreak;\n\tcase IOACCEL2_TMF:\n\t\tset_ioaccel2_tmf_performant_mode(h, c, reply_queue);\n\t\twritel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);\n\t\tbreak;\n\tdefault:\n\t\tset_performant_mode(h, c, reply_queue);\n\t\th->access.submit_command(h, c);\n\t}\n}\n\nstatic void enqueue_cmd_and_start_io(struct ctlr_info *h, struct CommandList *c)\n{\n\t__enqueue_cmd_and_start_io(h, c, DEFAULT_REPLY_QUEUE);\n}\n\nstatic inline int is_hba_lunid(unsigned char scsi3addr[])\n{\n\treturn memcmp(scsi3addr, RAID_CTLR_LUNID, 8) == 0;\n}\n\nstatic inline int is_scsi_rev_5(struct ctlr_info *h)\n{\n\tif (!h->hba_inquiry_data)\n\t\treturn 0;\n\tif ((h->hba_inquiry_data[2] & 0x07) == 5)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int hpsa_find_target_lun(struct ctlr_info *h,\n\tunsigned char scsi3addr[], int bus, int *target, int *lun)\n{\n\t \n\tint i, found = 0;\n\tDECLARE_BITMAP(lun_taken, HPSA_MAX_DEVICES);\n\n\tbitmap_zero(lun_taken, HPSA_MAX_DEVICES);\n\n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tif (h->dev[i]->bus == bus && h->dev[i]->target != -1)\n\t\t\t__set_bit(h->dev[i]->target, lun_taken);\n\t}\n\n\ti = find_first_zero_bit(lun_taken, HPSA_MAX_DEVICES);\n\tif (i < HPSA_MAX_DEVICES) {\n\t\t \n\t\t*target = i;\n\t\t*lun = 0;\n\t\tfound = 1;\n\t}\n\treturn !found;\n}\n\nstatic void hpsa_show_dev_msg(const char *level, struct ctlr_info *h,\n\tstruct hpsa_scsi_dev_t *dev, char *description)\n{\n#define LABEL_SIZE 25\n\tchar label[LABEL_SIZE];\n\n\tif (h == NULL || h->pdev == NULL || h->scsi_host == NULL)\n\t\treturn;\n\n\tswitch (dev->devtype) {\n\tcase TYPE_RAID:\n\t\tsnprintf(label, LABEL_SIZE, \"controller\");\n\t\tbreak;\n\tcase TYPE_ENCLOSURE:\n\t\tsnprintf(label, LABEL_SIZE, \"enclosure\");\n\t\tbreak;\n\tcase TYPE_DISK:\n\tcase TYPE_ZBC:\n\t\tif (dev->external)\n\t\t\tsnprintf(label, LABEL_SIZE, \"external\");\n\t\telse if (!is_logical_dev_addr_mode(dev->scsi3addr))\n\t\t\tsnprintf(label, LABEL_SIZE, \"%s\",\n\t\t\t\traid_label[PHYSICAL_DRIVE]);\n\t\telse\n\t\t\tsnprintf(label, LABEL_SIZE, \"RAID-%s\",\n\t\t\t\tdev->raid_level > RAID_UNKNOWN ? \"?\" :\n\t\t\t\traid_label[dev->raid_level]);\n\t\tbreak;\n\tcase TYPE_ROM:\n\t\tsnprintf(label, LABEL_SIZE, \"rom\");\n\t\tbreak;\n\tcase TYPE_TAPE:\n\t\tsnprintf(label, LABEL_SIZE, \"tape\");\n\t\tbreak;\n\tcase TYPE_MEDIUM_CHANGER:\n\t\tsnprintf(label, LABEL_SIZE, \"changer\");\n\t\tbreak;\n\tdefault:\n\t\tsnprintf(label, LABEL_SIZE, \"UNKNOWN\");\n\t\tbreak;\n\t}\n\n\tdev_printk(level, &h->pdev->dev,\n\t\t\t\"scsi %d:%d:%d:%d: %s %s %.8s %.16s %s SSDSmartPathCap%c En%c Exp=%d\\n\",\n\t\t\th->scsi_host->host_no, dev->bus, dev->target, dev->lun,\n\t\t\tdescription,\n\t\t\tscsi_device_type(dev->devtype),\n\t\t\tdev->vendor,\n\t\t\tdev->model,\n\t\t\tlabel,\n\t\t\tdev->offload_config ? '+' : '-',\n\t\t\tdev->offload_to_be_enabled ? '+' : '-',\n\t\t\tdev->expose_device);\n}\n\n \nstatic int hpsa_scsi_add_entry(struct ctlr_info *h,\n\t\tstruct hpsa_scsi_dev_t *device,\n\t\tstruct hpsa_scsi_dev_t *added[], int *nadded)\n{\n\t \n\tint n = h->ndevices;\n\tint i;\n\tunsigned char addr1[8], addr2[8];\n\tstruct hpsa_scsi_dev_t *sd;\n\n\tif (n >= HPSA_MAX_DEVICES) {\n\t\tdev_err(&h->pdev->dev, \"too many devices, some will be \"\n\t\t\t\"inaccessible.\\n\");\n\t\treturn -1;\n\t}\n\n\t \n\tif (device->lun != -1)\n\t\t \n\t\tgoto lun_assigned;\n\n\t \n\tif (device->scsi3addr[4] == 0) {\n\t\t \n\t\tif (hpsa_find_target_lun(h, device->scsi3addr,\n\t\t\tdevice->bus, &device->target, &device->lun) != 0)\n\t\t\treturn -1;\n\t\tgoto lun_assigned;\n\t}\n\n\t \n\tmemcpy(addr1, device->scsi3addr, 8);\n\taddr1[4] = 0;\n\taddr1[5] = 0;\n\tfor (i = 0; i < n; i++) {\n\t\tsd = h->dev[i];\n\t\tmemcpy(addr2, sd->scsi3addr, 8);\n\t\taddr2[4] = 0;\n\t\taddr2[5] = 0;\n\t\t \n\t\tif (memcmp(addr1, addr2, 8) == 0) {\n\t\t\tdevice->bus = sd->bus;\n\t\t\tdevice->target = sd->target;\n\t\t\tdevice->lun = device->scsi3addr[4];\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (device->lun == -1) {\n\t\tdev_warn(&h->pdev->dev, \"physical device with no LUN=0,\"\n\t\t\t\" suspect firmware bug or unsupported hardware \"\n\t\t\t\"configuration.\\n\");\n\t\treturn -1;\n\t}\n\nlun_assigned:\n\n\th->dev[n] = device;\n\th->ndevices++;\n\tadded[*nadded] = device;\n\t(*nadded)++;\n\thpsa_show_dev_msg(KERN_INFO, h, device,\n\t\tdevice->expose_device ? \"added\" : \"masked\");\n\treturn 0;\n}\n\n \nstatic void hpsa_scsi_update_entry(struct ctlr_info *h,\n\tint entry, struct hpsa_scsi_dev_t *new_entry)\n{\n\t \n\tBUG_ON(entry < 0 || entry >= HPSA_MAX_DEVICES);\n\n\t \n\th->dev[entry]->raid_level = new_entry->raid_level;\n\n\t \n\th->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;\n\n\t \n\tif (new_entry->offload_config && new_entry->offload_to_be_enabled) {\n\t\t \n\t\th->dev[entry]->raid_map = new_entry->raid_map;\n\t\th->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;\n\t}\n\tif (new_entry->offload_to_be_enabled) {\n\t\th->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;\n\t\twmb();  \n\t}\n\th->dev[entry]->hba_ioaccel_enabled = new_entry->hba_ioaccel_enabled;\n\th->dev[entry]->offload_config = new_entry->offload_config;\n\th->dev[entry]->offload_to_mirror = new_entry->offload_to_mirror;\n\th->dev[entry]->queue_depth = new_entry->queue_depth;\n\n\t \n\th->dev[entry]->offload_to_be_enabled = new_entry->offload_to_be_enabled;\n\n\t \n\tif (!new_entry->offload_to_be_enabled)\n\t\th->dev[entry]->offload_enabled = 0;\n\n\thpsa_show_dev_msg(KERN_INFO, h, h->dev[entry], \"updated\");\n}\n\n \nstatic void hpsa_scsi_replace_entry(struct ctlr_info *h,\n\tint entry, struct hpsa_scsi_dev_t *new_entry,\n\tstruct hpsa_scsi_dev_t *added[], int *nadded,\n\tstruct hpsa_scsi_dev_t *removed[], int *nremoved)\n{\n\t \n\tBUG_ON(entry < 0 || entry >= HPSA_MAX_DEVICES);\n\tremoved[*nremoved] = h->dev[entry];\n\t(*nremoved)++;\n\n\t \n\tif (new_entry->target == -1) {\n\t\tnew_entry->target = h->dev[entry]->target;\n\t\tnew_entry->lun = h->dev[entry]->lun;\n\t}\n\n\th->dev[entry] = new_entry;\n\tadded[*nadded] = new_entry;\n\t(*nadded)++;\n\n\thpsa_show_dev_msg(KERN_INFO, h, new_entry, \"replaced\");\n}\n\n \nstatic void hpsa_scsi_remove_entry(struct ctlr_info *h, int entry,\n\tstruct hpsa_scsi_dev_t *removed[], int *nremoved)\n{\n\t \n\tint i;\n\tstruct hpsa_scsi_dev_t *sd;\n\n\tBUG_ON(entry < 0 || entry >= HPSA_MAX_DEVICES);\n\n\tsd = h->dev[entry];\n\tremoved[*nremoved] = h->dev[entry];\n\t(*nremoved)++;\n\n\tfor (i = entry; i < h->ndevices-1; i++)\n\t\th->dev[i] = h->dev[i+1];\n\th->ndevices--;\n\thpsa_show_dev_msg(KERN_INFO, h, sd, \"removed\");\n}\n\n#define SCSI3ADDR_EQ(a, b) ( \\\n\t(a)[7] == (b)[7] && \\\n\t(a)[6] == (b)[6] && \\\n\t(a)[5] == (b)[5] && \\\n\t(a)[4] == (b)[4] && \\\n\t(a)[3] == (b)[3] && \\\n\t(a)[2] == (b)[2] && \\\n\t(a)[1] == (b)[1] && \\\n\t(a)[0] == (b)[0])\n\nstatic void fixup_botched_add(struct ctlr_info *h,\n\tstruct hpsa_scsi_dev_t *added)\n{\n\t \n\tunsigned long flags;\n\tint i, j;\n\n\tspin_lock_irqsave(&h->lock, flags);\n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tif (h->dev[i] == added) {\n\t\t\tfor (j = i; j < h->ndevices-1; j++)\n\t\t\t\th->dev[j] = h->dev[j+1];\n\t\t\th->ndevices--;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&h->lock, flags);\n\tkfree(added);\n}\n\nstatic inline int device_is_the_same(struct hpsa_scsi_dev_t *dev1,\n\tstruct hpsa_scsi_dev_t *dev2)\n{\n\t \n\tif (memcmp(dev1->scsi3addr, dev2->scsi3addr,\n\t\tsizeof(dev1->scsi3addr)) != 0)\n\t\treturn 0;\n\tif (memcmp(dev1->device_id, dev2->device_id,\n\t\tsizeof(dev1->device_id)) != 0)\n\t\treturn 0;\n\tif (memcmp(dev1->model, dev2->model, sizeof(dev1->model)) != 0)\n\t\treturn 0;\n\tif (memcmp(dev1->vendor, dev2->vendor, sizeof(dev1->vendor)) != 0)\n\t\treturn 0;\n\tif (dev1->devtype != dev2->devtype)\n\t\treturn 0;\n\tif (dev1->bus != dev2->bus)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline int device_updated(struct hpsa_scsi_dev_t *dev1,\n\tstruct hpsa_scsi_dev_t *dev2)\n{\n\t \n\tif (dev1->raid_level != dev2->raid_level)\n\t\treturn 1;\n\tif (dev1->offload_config != dev2->offload_config)\n\t\treturn 1;\n\tif (dev1->offload_to_be_enabled != dev2->offload_to_be_enabled)\n\t\treturn 1;\n\tif (!is_logical_dev_addr_mode(dev1->scsi3addr))\n\t\tif (dev1->queue_depth != dev2->queue_depth)\n\t\t\treturn 1;\n\t \n\tif (dev1->ioaccel_handle != dev2->ioaccel_handle)\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic int hpsa_scsi_find_entry(struct hpsa_scsi_dev_t *needle,\n\tstruct hpsa_scsi_dev_t *haystack[], int haystack_size,\n\tint *index)\n{\n\tint i;\n#define DEVICE_NOT_FOUND 0\n#define DEVICE_CHANGED 1\n#define DEVICE_SAME 2\n#define DEVICE_UPDATED 3\n\tif (needle == NULL)\n\t\treturn DEVICE_NOT_FOUND;\n\n\tfor (i = 0; i < haystack_size; i++) {\n\t\tif (haystack[i] == NULL)  \n\t\t\tcontinue;\n\t\tif (SCSI3ADDR_EQ(needle->scsi3addr, haystack[i]->scsi3addr)) {\n\t\t\t*index = i;\n\t\t\tif (device_is_the_same(needle, haystack[i])) {\n\t\t\t\tif (device_updated(needle, haystack[i]))\n\t\t\t\t\treturn DEVICE_UPDATED;\n\t\t\t\treturn DEVICE_SAME;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (needle->volume_offline)\n\t\t\t\t\treturn DEVICE_NOT_FOUND;\n\t\t\t\treturn DEVICE_CHANGED;\n\t\t\t}\n\t\t}\n\t}\n\t*index = -1;\n\treturn DEVICE_NOT_FOUND;\n}\n\nstatic void hpsa_monitor_offline_device(struct ctlr_info *h,\n\t\t\t\t\tunsigned char scsi3addr[])\n{\n\tstruct offline_device_entry *device;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&h->offline_device_lock, flags);\n\tlist_for_each_entry(device, &h->offline_device_list, offline_list) {\n\t\tif (memcmp(device->scsi3addr, scsi3addr,\n\t\t\tsizeof(device->scsi3addr)) == 0) {\n\t\t\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n\t\t\treturn;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n\n\t \n\tdevice = kmalloc(sizeof(*device), GFP_KERNEL);\n\tif (!device)\n\t\treturn;\n\n\tmemcpy(device->scsi3addr, scsi3addr, sizeof(device->scsi3addr));\n\tspin_lock_irqsave(&h->offline_device_lock, flags);\n\tlist_add_tail(&device->offline_list, &h->offline_device_list);\n\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n}\n\n \nstatic void hpsa_show_volume_status(struct ctlr_info *h,\n\tstruct hpsa_scsi_dev_t *sd)\n{\n\tif (sd->volume_offline == HPSA_VPD_LV_STATUS_UNSUPPORTED)\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume status is not available through vital product data pages.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\tswitch (sd->volume_offline) {\n\tcase HPSA_LV_OK:\n\t\tbreak;\n\tcase HPSA_LV_UNDERGOING_ERASE:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is undergoing background erase process.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_NOT_AVAILABLE:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is waiting for transforming volume.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_UNDERGOING_RPI:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is undergoing rapid parity init.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_PENDING_RPI:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is queued for rapid parity initialization process.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_ENCRYPTED_NO_KEY:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because key is not present.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is not encrypted and cannot be accessed because controller is in encryption-only mode.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_UNDERGOING_ENCRYPTION:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is undergoing encryption process.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is undergoing encryption re-keying process.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because controller does not have encryption enabled.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_PENDING_ENCRYPTION:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is pending migration to encrypted state, but process has not started.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\tcase HPSA_LV_PENDING_ENCRYPTION_REKEYING:\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"C%d:B%d:T%d:L%d Volume is encrypted and is pending encryption rekeying.\\n\",\n\t\t\th->scsi_host->host_no,\n\t\t\tsd->bus, sd->target, sd->lun);\n\t\tbreak;\n\t}\n}\n\n \nstatic void hpsa_figure_phys_disk_ptrs(struct ctlr_info *h,\n\t\t\t\tstruct hpsa_scsi_dev_t *dev[], int ndevices,\n\t\t\t\tstruct hpsa_scsi_dev_t *logical_drive)\n{\n\tstruct raid_map_data *map = &logical_drive->raid_map;\n\tstruct raid_map_disk_data *dd = &map->data[0];\n\tint i, j;\n\tint total_disks_per_row = le16_to_cpu(map->data_disks_per_row) +\n\t\t\t\tle16_to_cpu(map->metadata_disks_per_row);\n\tint nraid_map_entries = le16_to_cpu(map->row_cnt) *\n\t\t\t\tle16_to_cpu(map->layout_map_count) *\n\t\t\t\ttotal_disks_per_row;\n\tint nphys_disk = le16_to_cpu(map->layout_map_count) *\n\t\t\t\ttotal_disks_per_row;\n\tint qdepth;\n\n\tif (nraid_map_entries > RAID_MAP_MAX_ENTRIES)\n\t\tnraid_map_entries = RAID_MAP_MAX_ENTRIES;\n\n\tlogical_drive->nphysical_disks = nraid_map_entries;\n\n\tqdepth = 0;\n\tfor (i = 0; i < nraid_map_entries; i++) {\n\t\tlogical_drive->phys_disk[i] = NULL;\n\t\tif (!logical_drive->offload_config)\n\t\t\tcontinue;\n\t\tfor (j = 0; j < ndevices; j++) {\n\t\t\tif (dev[j] == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (dev[j]->devtype != TYPE_DISK &&\n\t\t\t    dev[j]->devtype != TYPE_ZBC)\n\t\t\t\tcontinue;\n\t\t\tif (is_logical_device(dev[j]))\n\t\t\t\tcontinue;\n\t\t\tif (dev[j]->ioaccel_handle != dd[i].ioaccel_handle)\n\t\t\t\tcontinue;\n\n\t\t\tlogical_drive->phys_disk[i] = dev[j];\n\t\t\tif (i < nphys_disk)\n\t\t\t\tqdepth = min(h->nr_cmds, qdepth +\n\t\t\t\t    logical_drive->phys_disk[i]->queue_depth);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!logical_drive->phys_disk[i]) {\n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"%s: [%d:%d:%d:%d] A phys disk component of LV is missing, turning off offload_enabled for LV.\\n\",\n\t\t\t\t__func__,\n\t\t\t\th->scsi_host->host_no, logical_drive->bus,\n\t\t\t\tlogical_drive->target, logical_drive->lun);\n\t\t\thpsa_turn_off_ioaccel_for_device(logical_drive);\n\t\t\tlogical_drive->queue_depth = 8;\n\t\t}\n\t}\n\tif (nraid_map_entries)\n\t\t \n\t\tlogical_drive->queue_depth = qdepth;\n\telse {\n\t\tif (logical_drive->external)\n\t\t\tlogical_drive->queue_depth = EXTERNAL_QD;\n\t\telse\n\t\t\tlogical_drive->queue_depth = h->nr_cmds;\n\t}\n}\n\nstatic void hpsa_update_log_drive_phys_drive_ptrs(struct ctlr_info *h,\n\t\t\t\tstruct hpsa_scsi_dev_t *dev[], int ndevices)\n{\n\tint i;\n\n\tfor (i = 0; i < ndevices; i++) {\n\t\tif (dev[i] == NULL)\n\t\t\tcontinue;\n\t\tif (dev[i]->devtype != TYPE_DISK &&\n\t\t    dev[i]->devtype != TYPE_ZBC)\n\t\t\tcontinue;\n\t\tif (!is_logical_device(dev[i]))\n\t\t\tcontinue;\n\n\t\t \n\n\t\tif (!dev[i]->offload_enabled && dev[i]->offload_to_be_enabled)\n\t\t\thpsa_figure_phys_disk_ptrs(h, dev, ndevices, dev[i]);\n\t}\n}\n\nstatic int hpsa_add_device(struct ctlr_info *h, struct hpsa_scsi_dev_t *device)\n{\n\tint rc = 0;\n\n\tif (!h->scsi_host)\n\t\treturn 1;\n\n\tif (is_logical_device(device))  \n\t\trc = scsi_add_device(h->scsi_host, device->bus,\n\t\t\t\t\tdevice->target, device->lun);\n\telse  \n\t\trc = hpsa_add_sas_device(h->sas_host, device);\n\n\treturn rc;\n}\n\nstatic int hpsa_find_outstanding_commands_for_dev(struct ctlr_info *h,\n\t\t\t\t\t\tstruct hpsa_scsi_dev_t *dev)\n{\n\tint i;\n\tint count = 0;\n\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\tstruct CommandList *c = h->cmd_pool + i;\n\t\tint refcount = atomic_inc_return(&c->refcount);\n\n\t\tif (refcount > 1 && hpsa_cmd_dev_match(h, c, dev,\n\t\t\t\tdev->scsi3addr)) {\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&h->lock, flags);\t \n\t\t\tif (!hpsa_is_cmd_idle(c))\n\t\t\t\t++count;\n\t\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\t}\n\n\t\tcmd_free(h, c);\n\t}\n\n\treturn count;\n}\n\n#define NUM_WAIT 20\nstatic void hpsa_wait_for_outstanding_commands_for_dev(struct ctlr_info *h,\n\t\t\t\t\t\tstruct hpsa_scsi_dev_t *device)\n{\n\tint cmds = 0;\n\tint waits = 0;\n\tint num_wait = NUM_WAIT;\n\n\tif (device->external)\n\t\tnum_wait = HPSA_EH_PTRAID_TIMEOUT;\n\n\twhile (1) {\n\t\tcmds = hpsa_find_outstanding_commands_for_dev(h, device);\n\t\tif (cmds == 0)\n\t\t\tbreak;\n\t\tif (++waits > num_wait)\n\t\t\tbreak;\n\t\tmsleep(1000);\n\t}\n\n\tif (waits > num_wait) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: removing device [%d:%d:%d:%d] with %d outstanding commands!\\n\",\n\t\t\t__func__,\n\t\t\th->scsi_host->host_no,\n\t\t\tdevice->bus, device->target, device->lun, cmds);\n\t}\n}\n\nstatic void hpsa_remove_device(struct ctlr_info *h,\n\t\t\tstruct hpsa_scsi_dev_t *device)\n{\n\tstruct scsi_device *sdev = NULL;\n\n\tif (!h->scsi_host)\n\t\treturn;\n\n\t \n\tdevice->removed = 1;\n\thpsa_wait_for_outstanding_commands_for_dev(h, device);\n\n\tif (is_logical_device(device)) {  \n\t\tsdev = scsi_device_lookup(h->scsi_host, device->bus,\n\t\t\t\t\t\tdevice->target, device->lun);\n\t\tif (sdev) {\n\t\t\tscsi_remove_device(sdev);\n\t\t\tscsi_device_put(sdev);\n\t\t} else {\n\t\t\t \n\t\t\thpsa_show_dev_msg(KERN_WARNING, h, device,\n\t\t\t\t\t\"didn't find device for removal.\");\n\t\t}\n\t} else {  \n\n\t\thpsa_remove_sas_device(device);\n\t}\n}\n\nstatic void adjust_hpsa_scsi_table(struct ctlr_info *h,\n\tstruct hpsa_scsi_dev_t *sd[], int nsds)\n{\n\t \n\tint i, entry, device_change, changes = 0;\n\tstruct hpsa_scsi_dev_t *csd;\n\tunsigned long flags;\n\tstruct hpsa_scsi_dev_t **added, **removed;\n\tint nadded, nremoved;\n\n\t \n\tspin_lock_irqsave(&h->reset_lock, flags);\n\tif (h->reset_in_progress) {\n\t\th->drv_req_rescan = 1;\n\t\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\n\tadded = kcalloc(HPSA_MAX_DEVICES, sizeof(*added), GFP_KERNEL);\n\tremoved = kcalloc(HPSA_MAX_DEVICES, sizeof(*removed), GFP_KERNEL);\n\n\tif (!added || !removed) {\n\t\tdev_warn(&h->pdev->dev, \"out of memory in \"\n\t\t\t\"adjust_hpsa_scsi_table\\n\");\n\t\tgoto free_and_out;\n\t}\n\n\tspin_lock_irqsave(&h->devlock, flags);\n\n\t \n\ti = 0;\n\tnremoved = 0;\n\tnadded = 0;\n\twhile (i < h->ndevices) {\n\t\tcsd = h->dev[i];\n\t\tdevice_change = hpsa_scsi_find_entry(csd, sd, nsds, &entry);\n\t\tif (device_change == DEVICE_NOT_FOUND) {\n\t\t\tchanges++;\n\t\t\thpsa_scsi_remove_entry(h, i, removed, &nremoved);\n\t\t\tcontinue;  \n\t\t} else if (device_change == DEVICE_CHANGED) {\n\t\t\tchanges++;\n\t\t\thpsa_scsi_replace_entry(h, i, sd[entry],\n\t\t\t\tadded, &nadded, removed, &nremoved);\n\t\t\t \n\t\t\tsd[entry] = NULL;\n\t\t} else if (device_change == DEVICE_UPDATED) {\n\t\t\thpsa_scsi_update_entry(h, i, sd[entry]);\n\t\t}\n\t\ti++;\n\t}\n\n\t \n\n\tfor (i = 0; i < nsds; i++) {\n\t\tif (!sd[i])  \n\t\t\tcontinue;\n\n\t\t \n\t\tif (sd[i]->volume_offline) {\n\t\t\thpsa_show_volume_status(h, sd[i]);\n\t\t\thpsa_show_dev_msg(KERN_INFO, h, sd[i], \"offline\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tdevice_change = hpsa_scsi_find_entry(sd[i], h->dev,\n\t\t\t\t\th->ndevices, &entry);\n\t\tif (device_change == DEVICE_NOT_FOUND) {\n\t\t\tchanges++;\n\t\t\tif (hpsa_scsi_add_entry(h, sd[i], added, &nadded) != 0)\n\t\t\t\tbreak;\n\t\t\tsd[i] = NULL;  \n\t\t} else if (device_change == DEVICE_CHANGED) {\n\t\t\t \n\t\t\tchanges++;\n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"device unexpectedly changed.\\n\");\n\t\t\t \n\t\t}\n\t}\n\thpsa_update_log_drive_phys_drive_ptrs(h, h->dev, h->ndevices);\n\n\t \n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tif (h->dev[i] == NULL)\n\t\t\tcontinue;\n\t\th->dev[i]->offload_enabled = h->dev[i]->offload_to_be_enabled;\n\t}\n\n\tspin_unlock_irqrestore(&h->devlock, flags);\n\n\t \n\tfor (i = 0; i < nsds; i++) {\n\t\tif (!sd[i])  \n\t\t\tcontinue;\n\t\tif (sd[i]->volume_offline)\n\t\t\thpsa_monitor_offline_device(h, sd[i]->scsi3addr);\n\t}\n\n\t \n\tif (!changes)\n\t\tgoto free_and_out;\n\n\t \n\tfor (i = 0; i < nremoved; i++) {\n\t\tif (removed[i] == NULL)\n\t\t\tcontinue;\n\t\tif (removed[i]->expose_device)\n\t\t\thpsa_remove_device(h, removed[i]);\n\t\tkfree(removed[i]);\n\t\tremoved[i] = NULL;\n\t}\n\n\t \n\tfor (i = 0; i < nadded; i++) {\n\t\tint rc = 0;\n\n\t\tif (added[i] == NULL)\n\t\t\tcontinue;\n\t\tif (!(added[i]->expose_device))\n\t\t\tcontinue;\n\t\trc = hpsa_add_device(h, added[i]);\n\t\tif (!rc)\n\t\t\tcontinue;\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"addition failed %d, device not added.\", rc);\n\t\t \n\t\tfixup_botched_add(h, added[i]);\n\t\th->drv_req_rescan = 1;\n\t}\n\nfree_and_out:\n\tkfree(added);\n\tkfree(removed);\n}\n\n \nstatic struct hpsa_scsi_dev_t *lookup_hpsa_scsi_dev(struct ctlr_info *h,\n\tint bus, int target, int lun)\n{\n\tint i;\n\tstruct hpsa_scsi_dev_t *sd;\n\n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tsd = h->dev[i];\n\t\tif (sd->bus == bus && sd->target == target && sd->lun == lun)\n\t\t\treturn sd;\n\t}\n\treturn NULL;\n}\n\nstatic int hpsa_slave_alloc(struct scsi_device *sdev)\n{\n\tstruct hpsa_scsi_dev_t *sd = NULL;\n\tunsigned long flags;\n\tstruct ctlr_info *h;\n\n\th = sdev_to_hba(sdev);\n\tspin_lock_irqsave(&h->devlock, flags);\n\tif (sdev_channel(sdev) == HPSA_PHYSICAL_DEVICE_BUS) {\n\t\tstruct scsi_target *starget;\n\t\tstruct sas_rphy *rphy;\n\n\t\tstarget = scsi_target(sdev);\n\t\trphy = target_to_rphy(starget);\n\t\tsd = hpsa_find_device_by_sas_rphy(h, rphy);\n\t\tif (sd) {\n\t\t\tsd->target = sdev_id(sdev);\n\t\t\tsd->lun = sdev->lun;\n\t\t}\n\t}\n\tif (!sd)\n\t\tsd = lookup_hpsa_scsi_dev(h, sdev_channel(sdev),\n\t\t\t\t\tsdev_id(sdev), sdev->lun);\n\n\tif (sd && sd->expose_device) {\n\t\tatomic_set(&sd->ioaccel_cmds_out, 0);\n\t\tsdev->hostdata = sd;\n\t} else\n\t\tsdev->hostdata = NULL;\n\tspin_unlock_irqrestore(&h->devlock, flags);\n\treturn 0;\n}\n\n \n#define CTLR_TIMEOUT (120 * HZ)\nstatic int hpsa_slave_configure(struct scsi_device *sdev)\n{\n\tstruct hpsa_scsi_dev_t *sd;\n\tint queue_depth;\n\n\tsd = sdev->hostdata;\n\tsdev->no_uld_attach = !sd || !sd->expose_device;\n\n\tif (sd) {\n\t\tsd->was_removed = 0;\n\t\tqueue_depth = sd->queue_depth != 0 ?\n\t\t\t\tsd->queue_depth : sdev->host->can_queue;\n\t\tif (sd->external) {\n\t\t\tqueue_depth = EXTERNAL_QD;\n\t\t\tsdev->eh_timeout = HPSA_EH_PTRAID_TIMEOUT;\n\t\t\tblk_queue_rq_timeout(sdev->request_queue,\n\t\t\t\t\t\tHPSA_EH_PTRAID_TIMEOUT);\n\t\t}\n\t\tif (is_hba_lunid(sd->scsi3addr)) {\n\t\t\tsdev->eh_timeout = CTLR_TIMEOUT;\n\t\t\tblk_queue_rq_timeout(sdev->request_queue, CTLR_TIMEOUT);\n\t\t}\n\t} else {\n\t\tqueue_depth = sdev->host->can_queue;\n\t}\n\n\tscsi_change_queue_depth(sdev, queue_depth);\n\n\treturn 0;\n}\n\nstatic void hpsa_slave_destroy(struct scsi_device *sdev)\n{\n\tstruct hpsa_scsi_dev_t *hdev = NULL;\n\n\thdev = sdev->hostdata;\n\n\tif (hdev)\n\t\thdev->was_removed = 1;\n}\n\nstatic void hpsa_free_ioaccel2_sg_chain_blocks(struct ctlr_info *h)\n{\n\tint i;\n\n\tif (!h->ioaccel2_cmd_sg_list)\n\t\treturn;\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\tkfree(h->ioaccel2_cmd_sg_list[i]);\n\t\th->ioaccel2_cmd_sg_list[i] = NULL;\n\t}\n\tkfree(h->ioaccel2_cmd_sg_list);\n\th->ioaccel2_cmd_sg_list = NULL;\n}\n\nstatic int hpsa_allocate_ioaccel2_sg_chain_blocks(struct ctlr_info *h)\n{\n\tint i;\n\n\tif (h->chainsize <= 0)\n\t\treturn 0;\n\n\th->ioaccel2_cmd_sg_list =\n\t\tkcalloc(h->nr_cmds, sizeof(*h->ioaccel2_cmd_sg_list),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!h->ioaccel2_cmd_sg_list)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\th->ioaccel2_cmd_sg_list[i] =\n\t\t\tkmalloc_array(h->maxsgentries,\n\t\t\t\t      sizeof(*h->ioaccel2_cmd_sg_list[i]),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!h->ioaccel2_cmd_sg_list[i])\n\t\t\tgoto clean;\n\t}\n\treturn 0;\n\nclean:\n\thpsa_free_ioaccel2_sg_chain_blocks(h);\n\treturn -ENOMEM;\n}\n\nstatic void hpsa_free_sg_chain_blocks(struct ctlr_info *h)\n{\n\tint i;\n\n\tif (!h->cmd_sg_list)\n\t\treturn;\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\tkfree(h->cmd_sg_list[i]);\n\t\th->cmd_sg_list[i] = NULL;\n\t}\n\tkfree(h->cmd_sg_list);\n\th->cmd_sg_list = NULL;\n}\n\nstatic int hpsa_alloc_sg_chain_blocks(struct ctlr_info *h)\n{\n\tint i;\n\n\tif (h->chainsize <= 0)\n\t\treturn 0;\n\n\th->cmd_sg_list = kcalloc(h->nr_cmds, sizeof(*h->cmd_sg_list),\n\t\t\t\t GFP_KERNEL);\n\tif (!h->cmd_sg_list)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\th->cmd_sg_list[i] = kmalloc_array(h->chainsize,\n\t\t\t\t\t\t  sizeof(*h->cmd_sg_list[i]),\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!h->cmd_sg_list[i])\n\t\t\tgoto clean;\n\n\t}\n\treturn 0;\n\nclean:\n\thpsa_free_sg_chain_blocks(h);\n\treturn -ENOMEM;\n}\n\nstatic int hpsa_map_ioaccel2_sg_chain_block(struct ctlr_info *h,\n\tstruct io_accel2_cmd *cp, struct CommandList *c)\n{\n\tstruct ioaccel2_sg_element *chain_block;\n\tu64 temp64;\n\tu32 chain_size;\n\n\tchain_block = h->ioaccel2_cmd_sg_list[c->cmdindex];\n\tchain_size = le32_to_cpu(cp->sg[0].length);\n\ttemp64 = dma_map_single(&h->pdev->dev, chain_block, chain_size,\n\t\t\t\tDMA_TO_DEVICE);\n\tif (dma_mapping_error(&h->pdev->dev, temp64)) {\n\t\t \n\t\tcp->sg->address = 0;\n\t\treturn -1;\n\t}\n\tcp->sg->address = cpu_to_le64(temp64);\n\treturn 0;\n}\n\nstatic void hpsa_unmap_ioaccel2_sg_chain_block(struct ctlr_info *h,\n\tstruct io_accel2_cmd *cp)\n{\n\tstruct ioaccel2_sg_element *chain_sg;\n\tu64 temp64;\n\tu32 chain_size;\n\n\tchain_sg = cp->sg;\n\ttemp64 = le64_to_cpu(chain_sg->address);\n\tchain_size = le32_to_cpu(cp->sg[0].length);\n\tdma_unmap_single(&h->pdev->dev, temp64, chain_size, DMA_TO_DEVICE);\n}\n\nstatic int hpsa_map_sg_chain_block(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tstruct SGDescriptor *chain_sg, *chain_block;\n\tu64 temp64;\n\tu32 chain_len;\n\n\tchain_sg = &c->SG[h->max_cmd_sg_entries - 1];\n\tchain_block = h->cmd_sg_list[c->cmdindex];\n\tchain_sg->Ext = cpu_to_le32(HPSA_SG_CHAIN);\n\tchain_len = sizeof(*chain_sg) *\n\t\t(le16_to_cpu(c->Header.SGTotal) - h->max_cmd_sg_entries);\n\tchain_sg->Len = cpu_to_le32(chain_len);\n\ttemp64 = dma_map_single(&h->pdev->dev, chain_block, chain_len,\n\t\t\t\tDMA_TO_DEVICE);\n\tif (dma_mapping_error(&h->pdev->dev, temp64)) {\n\t\t \n\t\tchain_sg->Addr = cpu_to_le64(0);\n\t\treturn -1;\n\t}\n\tchain_sg->Addr = cpu_to_le64(temp64);\n\treturn 0;\n}\n\nstatic void hpsa_unmap_sg_chain_block(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tstruct SGDescriptor *chain_sg;\n\n\tif (le16_to_cpu(c->Header.SGTotal) <= h->max_cmd_sg_entries)\n\t\treturn;\n\n\tchain_sg = &c->SG[h->max_cmd_sg_entries - 1];\n\tdma_unmap_single(&h->pdev->dev, le64_to_cpu(chain_sg->Addr),\n\t\t\tle32_to_cpu(chain_sg->Len), DMA_TO_DEVICE);\n}\n\n\n \nstatic int handle_ioaccel_mode2_error(struct ctlr_info *h,\n\t\t\t\t\tstruct CommandList *c,\n\t\t\t\t\tstruct scsi_cmnd *cmd,\n\t\t\t\t\tstruct io_accel2_cmd *c2,\n\t\t\t\t\tstruct hpsa_scsi_dev_t *dev)\n{\n\tint data_len;\n\tint retry = 0;\n\tu32 ioaccel2_resid = 0;\n\n\tswitch (c2->error_data.serv_response) {\n\tcase IOACCEL2_SERV_RESPONSE_COMPLETE:\n\t\tswitch (c2->error_data.status) {\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_GOOD:\n\t\t\tif (cmd)\n\t\t\t\tcmd->result = 0;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_CHK_COND:\n\t\t\tcmd->result |= SAM_STAT_CHECK_CONDITION;\n\t\t\tif (c2->error_data.data_present !=\n\t\t\t\t\tIOACCEL2_SENSE_DATA_PRESENT) {\n\t\t\t\tmemset(cmd->sense_buffer, 0,\n\t\t\t\t\tSCSI_SENSE_BUFFERSIZE);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\tdata_len = c2->error_data.sense_data_len;\n\t\t\tif (data_len > SCSI_SENSE_BUFFERSIZE)\n\t\t\t\tdata_len = SCSI_SENSE_BUFFERSIZE;\n\t\t\tif (data_len > sizeof(c2->error_data.sense_data_buff))\n\t\t\t\tdata_len =\n\t\t\t\t\tsizeof(c2->error_data.sense_data_buff);\n\t\t\tmemcpy(cmd->sense_buffer,\n\t\t\t\tc2->error_data.sense_data_buff, data_len);\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_BUSY:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_RES_CON:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_TASK_COMP_ABORTED:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase IOACCEL2_SERV_RESPONSE_FAILURE:\n\t\tswitch (c2->error_data.status) {\n\t\tcase IOACCEL2_STATUS_SR_IO_ERROR:\n\t\tcase IOACCEL2_STATUS_SR_IO_ABORTED:\n\t\tcase IOACCEL2_STATUS_SR_OVERRUN:\n\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_UNDERRUN:\n\t\t\tcmd->result = (DID_OK << 16);\t\t \n\t\t\tioaccel2_resid = get_unaligned_le32(\n\t\t\t\t\t\t&c2->error_data.resid_cnt[0]);\n\t\t\tscsi_set_resid(cmd, ioaccel2_resid);\n\t\t\tbreak;\n\t\tcase IOACCEL2_STATUS_SR_NO_PATH_TO_DEVICE:\n\t\tcase IOACCEL2_STATUS_SR_INVALID_DEVICE:\n\t\tcase IOACCEL2_STATUS_SR_IOACCEL_DISABLED:\n\t\t\t \n\t\t\tif (dev->physical_device && dev->expose_device) {\n\t\t\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\t\t\tdev->removed = 1;\n\t\t\t\th->drv_req_rescan = 1;\n\t\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\t\"%s: device is gone!\\n\", __func__);\n\t\t\t} else\n\t\t\t\t \n\t\t\t\tretry = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tretry = 1;\n\t\t}\n\t\tbreak;\n\tcase IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:\n\t\tbreak;\n\tcase IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:\n\t\tbreak;\n\tcase IOACCEL2_SERV_RESPONSE_TMF_REJECTED:\n\t\tretry = 1;\n\t\tbreak;\n\tcase IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:\n\t\tbreak;\n\tdefault:\n\t\tretry = 1;\n\t\tbreak;\n\t}\n\n\tif (dev->in_reset)\n\t\tretry = 0;\n\n\treturn retry;\t \n}\n\nstatic void hpsa_cmd_resolve_events(struct ctlr_info *h,\n\t\tstruct CommandList *c)\n{\n\tstruct hpsa_scsi_dev_t *dev = c->device;\n\n\t \n\tc->scsi_cmd = SCSI_CMD_IDLE;\n\tmb();\t \n\tif (dev) {\n\t\tatomic_dec(&dev->commands_outstanding);\n\t\tif (dev->in_reset &&\n\t\t\tatomic_read(&dev->commands_outstanding) <= 0)\n\t\t\twake_up_all(&h->event_sync_wait_queue);\n\t}\n}\n\nstatic void hpsa_cmd_resolve_and_free(struct ctlr_info *h,\n\t\t\t\t      struct CommandList *c)\n{\n\thpsa_cmd_resolve_events(h, c);\n\tcmd_tagged_free(h, c);\n}\n\nstatic void hpsa_cmd_free_and_done(struct ctlr_info *h,\n\t\tstruct CommandList *c, struct scsi_cmnd *cmd)\n{\n\thpsa_cmd_resolve_and_free(h, c);\n\tif (cmd)\n\t\tscsi_done(cmd);\n}\n\nstatic void hpsa_retry_cmd(struct ctlr_info *h, struct CommandList *c)\n{\n\tINIT_WORK(&c->work, hpsa_command_resubmit_worker);\n\tqueue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);\n}\n\nstatic void process_ioaccel2_completion(struct ctlr_info *h,\n\t\tstruct CommandList *c, struct scsi_cmnd *cmd,\n\t\tstruct hpsa_scsi_dev_t *dev)\n{\n\tstruct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];\n\n\t \n\tif (likely(c2->error_data.serv_response == 0 &&\n\t\t\tc2->error_data.status == 0)) {\n\t\tcmd->result = 0;\n\t\treturn hpsa_cmd_free_and_done(h, c, cmd);\n\t}\n\n\t \n\tif (is_logical_device(dev) &&\n\t\tc2->error_data.serv_response ==\n\t\t\tIOACCEL2_SERV_RESPONSE_FAILURE) {\n\t\tif (c2->error_data.status ==\n\t\t\tIOACCEL2_STATUS_SR_IOACCEL_DISABLED) {\n\t\t\thpsa_turn_off_ioaccel_for_device(dev);\n\t\t}\n\n\t\tif (dev->in_reset) {\n\t\t\tcmd->result = DID_RESET << 16;\n\t\t\treturn hpsa_cmd_free_and_done(h, c, cmd);\n\t\t}\n\n\t\treturn hpsa_retry_cmd(h, c);\n\t}\n\n\tif (handle_ioaccel_mode2_error(h, c, cmd, c2, dev))\n\t\treturn hpsa_retry_cmd(h, c);\n\n\treturn hpsa_cmd_free_and_done(h, c, cmd);\n}\n\n \nstatic int hpsa_evaluate_tmf_status(struct ctlr_info *h,\n\t\t\t\t\tstruct CommandList *cp)\n{\n\tu8 tmf_status = cp->err_info->ScsiStatus;\n\n\tswitch (tmf_status) {\n\tcase CISS_TMF_COMPLETE:\n\t\t \n\tcase CISS_TMF_SUCCESS:\n\t\treturn 0;\n\tcase CISS_TMF_INVALID_FRAME:\n\tcase CISS_TMF_NOT_SUPPORTED:\n\tcase CISS_TMF_FAILED:\n\tcase CISS_TMF_WRONG_LUN:\n\tcase CISS_TMF_OVERLAPPED_TAG:\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(&h->pdev->dev, \"Unknown TMF status: 0x%02x\\n\",\n\t\t\t\ttmf_status);\n\t\tbreak;\n\t}\n\treturn -tmf_status;\n}\n\nstatic void complete_scsi_command(struct CommandList *cp)\n{\n\tstruct scsi_cmnd *cmd;\n\tstruct ctlr_info *h;\n\tstruct ErrorInfo *ei;\n\tstruct hpsa_scsi_dev_t *dev;\n\tstruct io_accel2_cmd *c2;\n\n\tu8 sense_key;\n\tu8 asc;       \n\tu8 ascq;      \n\tunsigned long sense_data_size;\n\n\tei = cp->err_info;\n\tcmd = cp->scsi_cmd;\n\th = cp->h;\n\n\tif (!cmd->device) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n\t}\n\n\tdev = cmd->device->hostdata;\n\tif (!dev) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n\t}\n\tc2 = &h->ioaccel2_cmd_pool[cp->cmdindex];\n\n\tscsi_dma_unmap(cmd);  \n\tif ((cp->cmd_type == CMD_SCSI) &&\n\t\t(le16_to_cpu(cp->Header.SGTotal) > h->max_cmd_sg_entries))\n\t\thpsa_unmap_sg_chain_block(h, cp);\n\n\tif ((cp->cmd_type == CMD_IOACCEL2) &&\n\t\t(c2->sg[0].chain_indicator == IOACCEL2_CHAIN))\n\t\thpsa_unmap_ioaccel2_sg_chain_block(h, c2);\n\n\tcmd->result = (DID_OK << 16);\t\t \n\n\t \n\tif (dev->was_removed) {\n\t\thpsa_cmd_resolve_and_free(h, cp);\n\t\treturn;\n\t}\n\n\tif (cp->cmd_type == CMD_IOACCEL2 || cp->cmd_type == CMD_IOACCEL1) {\n\t\tif (dev->physical_device && dev->expose_device &&\n\t\t\tdev->removed) {\n\t\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\t\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n\t\t}\n\t\tif (likely(cp->phys_disk != NULL))\n\t\t\tatomic_dec(&cp->phys_disk->ioaccel_cmds_out);\n\t}\n\n\t \n\tif (unlikely(ei->CommandStatus == CMD_CTLR_LOCKUP)) {\n\t\t \n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n\t}\n\n\tif (cp->cmd_type == CMD_IOACCEL2)\n\t\treturn process_ioaccel2_completion(h, cp, cmd, dev);\n\n\tscsi_set_resid(cmd, ei->ResidualCnt);\n\tif (ei->CommandStatus == 0)\n\t\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n\n\t \n\tif (cp->cmd_type == CMD_IOACCEL1) {\n\t\tstruct io_accel1_cmd *c = &h->ioaccel_cmd_pool[cp->cmdindex];\n\t\tcp->Header.SGList = scsi_sg_count(cmd);\n\t\tcp->Header.SGTotal = cpu_to_le16(cp->Header.SGList);\n\t\tcp->Request.CDBLen = le16_to_cpu(c->io_flags) &\n\t\t\tIOACCEL1_IOFLAGS_CDBLEN_MASK;\n\t\tcp->Header.tag = c->tag;\n\t\tmemcpy(cp->Header.LUN.LunAddrBytes, c->CISS_LUN, 8);\n\t\tmemcpy(cp->Request.CDB, c->CDB, cp->Request.CDBLen);\n\n\t\t \n\t\tif (is_logical_device(dev)) {\n\t\t\tif (ei->CommandStatus == CMD_IOACCEL_DISABLED)\n\t\t\t\tdev->offload_enabled = 0;\n\t\t\treturn hpsa_retry_cmd(h, cp);\n\t\t}\n\t}\n\n\t \n\tswitch (ei->CommandStatus) {\n\n\tcase CMD_TARGET_STATUS:\n\t\tcmd->result |= ei->ScsiStatus;\n\t\t \n\t\tif (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))\n\t\t\tsense_data_size = SCSI_SENSE_BUFFERSIZE;\n\t\telse\n\t\t\tsense_data_size = sizeof(ei->SenseInfo);\n\t\tif (ei->SenseLen < sense_data_size)\n\t\t\tsense_data_size = ei->SenseLen;\n\t\tmemcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);\n\t\tif (ei->ScsiStatus)\n\t\t\tdecode_sense_data(ei->SenseInfo, sense_data_size,\n\t\t\t\t&sense_key, &asc, &ascq);\n\t\tif (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION) {\n\t\t\tswitch (sense_key) {\n\t\t\tcase ABORTED_COMMAND:\n\t\t\t\tcmd->result |= DID_SOFT_ERROR << 16;\n\t\t\t\tbreak;\n\t\t\tcase UNIT_ATTENTION:\n\t\t\t\tif (asc == 0x3F && ascq == 0x0E)\n\t\t\t\t\th->drv_req_rescan = 1;\n\t\t\t\tbreak;\n\t\t\tcase ILLEGAL_REQUEST:\n\t\t\t\tif (asc == 0x25 && ascq == 0x00) {\n\t\t\t\t\tdev->removed = 1;\n\t\t\t\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (ei->ScsiStatus) {\n\t\t\tdev_warn(&h->pdev->dev, \"cp %p has status 0x%x \"\n\t\t\t\t\"Sense: 0x%x, ASC: 0x%x, ASCQ: 0x%x, \"\n\t\t\t\t\"Returning result: 0x%x\\n\",\n\t\t\t\tcp, ei->ScsiStatus,\n\t\t\t\tsense_key, asc, ascq,\n\t\t\t\tcmd->result);\n\t\t} else {   \n\t\t\tdev_warn(&h->pdev->dev, \"cp %p SCSI status was 0. \"\n\t\t\t\t\"Returning no connection.\\n\", cp),\n\n\t\t\t \n\n\t\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\t}\n\t\tbreak;\n\n\tcase CMD_DATA_UNDERRUN:  \n\t\tbreak;\n\tcase CMD_DATA_OVERRUN:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"CDB %16phN data overrun\\n\", cp->Request.CDB);\n\t\tbreak;\n\tcase CMD_INVALID: {\n\t\t \n\t\t \n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t}\n\t\tbreak;\n\tcase CMD_PROTOCOL_ERR:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN : protocol error\\n\",\n\t\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_HARDWARE_ERR:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN : hardware error\\n\",\n\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_CONNECTION_LOST:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN : connection lost\\n\",\n\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_ABORTED:\n\t\tcmd->result = DID_ABORT << 16;\n\t\tbreak;\n\tcase CMD_ABORT_FAILED:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN : abort failed\\n\",\n\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_UNSOLICITED_ABORT:\n\t\tcmd->result = DID_SOFT_ERROR << 16;  \n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN : unsolicited abort\\n\",\n\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_TIMEOUT:\n\t\tcmd->result = DID_TIME_OUT << 16;\n\t\tdev_warn(&h->pdev->dev, \"CDB %16phN timed out\\n\",\n\t\t\tcp->Request.CDB);\n\t\tbreak;\n\tcase CMD_UNABORTABLE:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"Command unabortable\\n\");\n\t\tbreak;\n\tcase CMD_TMF_STATUS:\n\t\tif (hpsa_evaluate_tmf_status(h, cp))  \n\t\t\tcmd->result = DID_ERROR << 16;\n\t\tbreak;\n\tcase CMD_IOACCEL_DISABLED:\n\t\t \n\t\tcmd->result = DID_SOFT_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"cp %p had HP SSD Smart Path error\\n\", cp);\n\t\tbreak;\n\tdefault:\n\t\tcmd->result = DID_ERROR << 16;\n\t\tdev_warn(&h->pdev->dev, \"cp %p returned unknown status %x\\n\",\n\t\t\t\tcp, ei->CommandStatus);\n\t}\n\n\treturn hpsa_cmd_free_and_done(h, cp, cmd);\n}\n\nstatic void hpsa_pci_unmap(struct pci_dev *pdev, struct CommandList *c,\n\t\tint sg_used, enum dma_data_direction data_direction)\n{\n\tint i;\n\n\tfor (i = 0; i < sg_used; i++)\n\t\tdma_unmap_single(&pdev->dev, le64_to_cpu(c->SG[i].Addr),\n\t\t\t\tle32_to_cpu(c->SG[i].Len),\n\t\t\t\tdata_direction);\n}\n\nstatic int hpsa_map_one(struct pci_dev *pdev,\n\t\tstruct CommandList *cp,\n\t\tunsigned char *buf,\n\t\tsize_t buflen,\n\t\tenum dma_data_direction data_direction)\n{\n\tu64 addr64;\n\n\tif (buflen == 0 || data_direction == DMA_NONE) {\n\t\tcp->Header.SGList = 0;\n\t\tcp->Header.SGTotal = cpu_to_le16(0);\n\t\treturn 0;\n\t}\n\n\taddr64 = dma_map_single(&pdev->dev, buf, buflen, data_direction);\n\tif (dma_mapping_error(&pdev->dev, addr64)) {\n\t\t \n\t\tcp->Header.SGList = 0;\n\t\tcp->Header.SGTotal = cpu_to_le16(0);\n\t\treturn -1;\n\t}\n\tcp->SG[0].Addr = cpu_to_le64(addr64);\n\tcp->SG[0].Len = cpu_to_le32(buflen);\n\tcp->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST);  \n\tcp->Header.SGList = 1;    \n\tcp->Header.SGTotal = cpu_to_le16(1);  \n\treturn 0;\n}\n\n#define NO_TIMEOUT ((unsigned long) -1)\n#define DEFAULT_TIMEOUT 30000  \nstatic int hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,\n\tstruct CommandList *c, int reply_queue, unsigned long timeout_msecs)\n{\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\n\tc->waiting = &wait;\n\t__enqueue_cmd_and_start_io(h, c, reply_queue);\n\tif (timeout_msecs == NO_TIMEOUT) {\n\t\t \n\t\twait_for_completion_io(&wait);\n\t\treturn IO_OK;\n\t}\n\tif (!wait_for_completion_io_timeout(&wait,\n\t\t\t\t\tmsecs_to_jiffies(timeout_msecs))) {\n\t\tdev_warn(&h->pdev->dev, \"Command timed out.\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\treturn IO_OK;\n}\n\nstatic int hpsa_scsi_do_simple_cmd(struct ctlr_info *h, struct CommandList *c,\n\t\t\t\t   int reply_queue, unsigned long timeout_msecs)\n{\n\tif (unlikely(lockup_detected(h))) {\n\t\tc->err_info->CommandStatus = CMD_CTLR_LOCKUP;\n\t\treturn IO_OK;\n\t}\n\treturn hpsa_scsi_do_simple_cmd_core(h, c, reply_queue, timeout_msecs);\n}\n\nstatic u32 lockup_detected(struct ctlr_info *h)\n{\n\tint cpu;\n\tu32 rc, *lockup_detected;\n\n\tcpu = get_cpu();\n\tlockup_detected = per_cpu_ptr(h->lockup_detected, cpu);\n\trc = *lockup_detected;\n\tput_cpu();\n\treturn rc;\n}\n\n#define MAX_DRIVER_CMD_RETRIES 25\nstatic int hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,\n\t\tstruct CommandList *c, enum dma_data_direction data_direction,\n\t\tunsigned long timeout_msecs)\n{\n\tint backoff_time = 10, retry_count = 0;\n\tint rc;\n\n\tdo {\n\t\tmemset(c->err_info, 0, sizeof(*c->err_info));\n\t\trc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE,\n\t\t\t\t\t\t  timeout_msecs);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tretry_count++;\n\t\tif (retry_count > 3) {\n\t\t\tmsleep(backoff_time);\n\t\t\tif (backoff_time < 1000)\n\t\t\t\tbackoff_time *= 2;\n\t\t}\n\t} while ((check_for_unit_attention(h, c) ||\n\t\t\tcheck_for_busy(h, c)) &&\n\t\t\tretry_count <= MAX_DRIVER_CMD_RETRIES);\n\thpsa_pci_unmap(h->pdev, c, 1, data_direction);\n\tif (retry_count > MAX_DRIVER_CMD_RETRIES)\n\t\trc = -EIO;\n\treturn rc;\n}\n\nstatic void hpsa_print_cmd(struct ctlr_info *h, char *txt,\n\t\t\t\tstruct CommandList *c)\n{\n\tconst u8 *cdb = c->Request.CDB;\n\tconst u8 *lun = c->Header.LUN.LunAddrBytes;\n\n\tdev_warn(&h->pdev->dev, \"%s: LUN:%8phN CDB:%16phN\\n\",\n\t\t txt, lun, cdb);\n}\n\nstatic void hpsa_scsi_interpret_error(struct ctlr_info *h,\n\t\t\tstruct CommandList *cp)\n{\n\tconst struct ErrorInfo *ei = cp->err_info;\n\tstruct device *d = &cp->h->pdev->dev;\n\tu8 sense_key, asc, ascq;\n\tint sense_len;\n\n\tswitch (ei->CommandStatus) {\n\tcase CMD_TARGET_STATUS:\n\t\tif (ei->SenseLen > sizeof(ei->SenseInfo))\n\t\t\tsense_len = sizeof(ei->SenseInfo);\n\t\telse\n\t\t\tsense_len = ei->SenseLen;\n\t\tdecode_sense_data(ei->SenseInfo, sense_len,\n\t\t\t\t\t&sense_key, &asc, &ascq);\n\t\thpsa_print_cmd(h, \"SCSI status\", cp);\n\t\tif (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION)\n\t\t\tdev_warn(d, \"SCSI Status = 02, Sense key = 0x%02x, ASC = 0x%02x, ASCQ = 0x%02x\\n\",\n\t\t\t\tsense_key, asc, ascq);\n\t\telse\n\t\t\tdev_warn(d, \"SCSI Status = 0x%02x\\n\", ei->ScsiStatus);\n\t\tif (ei->ScsiStatus == 0)\n\t\t\tdev_warn(d, \"SCSI status is abnormally zero.  \"\n\t\t\t\"(probably indicates selection timeout \"\n\t\t\t\"reported incorrectly due to a known \"\n\t\t\t\"firmware bug, circa July, 2001.)\\n\");\n\t\tbreak;\n\tcase CMD_DATA_UNDERRUN:  \n\t\tbreak;\n\tcase CMD_DATA_OVERRUN:\n\t\thpsa_print_cmd(h, \"overrun condition\", cp);\n\t\tbreak;\n\tcase CMD_INVALID: {\n\t\t \n\t\thpsa_print_cmd(h, \"invalid command\", cp);\n\t\tdev_warn(d, \"probably means device no longer present\\n\");\n\t\t}\n\t\tbreak;\n\tcase CMD_PROTOCOL_ERR:\n\t\thpsa_print_cmd(h, \"protocol error\", cp);\n\t\tbreak;\n\tcase CMD_HARDWARE_ERR:\n\t\thpsa_print_cmd(h, \"hardware error\", cp);\n\t\tbreak;\n\tcase CMD_CONNECTION_LOST:\n\t\thpsa_print_cmd(h, \"connection lost\", cp);\n\t\tbreak;\n\tcase CMD_ABORTED:\n\t\thpsa_print_cmd(h, \"aborted\", cp);\n\t\tbreak;\n\tcase CMD_ABORT_FAILED:\n\t\thpsa_print_cmd(h, \"abort failed\", cp);\n\t\tbreak;\n\tcase CMD_UNSOLICITED_ABORT:\n\t\thpsa_print_cmd(h, \"unsolicited abort\", cp);\n\t\tbreak;\n\tcase CMD_TIMEOUT:\n\t\thpsa_print_cmd(h, \"timed out\", cp);\n\t\tbreak;\n\tcase CMD_UNABORTABLE:\n\t\thpsa_print_cmd(h, \"unabortable\", cp);\n\t\tbreak;\n\tcase CMD_CTLR_LOCKUP:\n\t\thpsa_print_cmd(h, \"controller lockup detected\", cp);\n\t\tbreak;\n\tdefault:\n\t\thpsa_print_cmd(h, \"unknown status\", cp);\n\t\tdev_warn(d, \"Unknown command status %x\\n\",\n\t\t\t\tei->CommandStatus);\n\t}\n}\n\nstatic int hpsa_do_receive_diagnostic(struct ctlr_info *h, u8 *scsi3addr,\n\t\t\t\t\tu8 page, u8 *buf, size_t bufsize)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\tif (fill_cmd(c, RECEIVE_DIAGNOSTIC, h, buf, bufsize,\n\t\t\tpage, scsi3addr, TYPE_CMD)) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic u64 hpsa_get_enclosure_logical_identifier(struct ctlr_info *h,\n\t\t\t\t\t\tu8 *scsi3addr)\n{\n\tu8 *buf;\n\tu64 sa = 0;\n\tint rc = 0;\n\n\tbuf = kzalloc(1024, GFP_KERNEL);\n\tif (!buf)\n\t\treturn 0;\n\n\trc = hpsa_do_receive_diagnostic(h, scsi3addr, RECEIVE_DIAGNOSTIC,\n\t\t\t\t\tbuf, 1024);\n\n\tif (rc)\n\t\tgoto out;\n\n\tsa = get_unaligned_be64(buf+12);\n\nout:\n\tkfree(buf);\n\treturn sa;\n}\n\nstatic int hpsa_scsi_do_inquiry(struct ctlr_info *h, unsigned char *scsi3addr,\n\t\t\tu16 page, unsigned char *buf,\n\t\t\tunsigned char bufsize)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\n\tif (fill_cmd(c, HPSA_INQUIRY, h, buf, bufsize,\n\t\t\tpage, scsi3addr, TYPE_CMD)) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic int hpsa_send_reset(struct ctlr_info *h, struct hpsa_scsi_dev_t *dev,\n\tu8 reset_type, int reply_queue)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\tc->device = dev;\n\n\t \n\t(void) fill_cmd(c, reset_type, h, NULL, 0, 0, dev->scsi3addr, TYPE_MSG);\n\trc = hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);\n\tif (rc) {\n\t\tdev_warn(&h->pdev->dev, \"Failed to send reset command\\n\");\n\t\tgoto out;\n\t}\n\t \n\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic bool hpsa_cmd_dev_match(struct ctlr_info *h, struct CommandList *c,\n\t\t\t       struct hpsa_scsi_dev_t *dev,\n\t\t\t       unsigned char *scsi3addr)\n{\n\tint i;\n\tbool match = false;\n\tstruct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];\n\tstruct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;\n\n\tif (hpsa_is_cmd_idle(c))\n\t\treturn false;\n\n\tswitch (c->cmd_type) {\n\tcase CMD_SCSI:\n\tcase CMD_IOCTL_PEND:\n\t\tmatch = !memcmp(scsi3addr, &c->Header.LUN.LunAddrBytes,\n\t\t\t\tsizeof(c->Header.LUN.LunAddrBytes));\n\t\tbreak;\n\n\tcase CMD_IOACCEL1:\n\tcase CMD_IOACCEL2:\n\t\tif (c->phys_disk == dev) {\n\t\t\t \n\t\t\tmatch = true;\n\t\t} else {\n\t\t\t \n\t\t\t \n\t\t\tfor (i = 0; i < dev->nphysical_disks && !match; i++) {\n\t\t\t\t \n\t\t\t\tmatch = dev->phys_disk[i] == c->phys_disk;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase IOACCEL2_TMF:\n\t\tfor (i = 0; i < dev->nphysical_disks && !match; i++) {\n\t\t\tmatch = dev->phys_disk[i]->ioaccel_handle ==\n\t\t\t\t\tle32_to_cpu(ac->it_nexus);\n\t\t}\n\t\tbreak;\n\n\tcase 0:\t\t \n\t\tmatch = false;\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&h->pdev->dev, \"unexpected cmd_type: %d\\n\",\n\t\t\tc->cmd_type);\n\t\tBUG();\n\t}\n\n\treturn match;\n}\n\nstatic int hpsa_do_reset(struct ctlr_info *h, struct hpsa_scsi_dev_t *dev,\n\tu8 reset_type, int reply_queue)\n{\n\tint rc = 0;\n\n\t \n\tif (mutex_lock_interruptible(&h->reset_mutex) == -EINTR) {\n\t\tdev_warn(&h->pdev->dev, \"concurrent reset wait interrupted.\\n\");\n\t\treturn -EINTR;\n\t}\n\n\trc = hpsa_send_reset(h, dev, reset_type, reply_queue);\n\tif (!rc) {\n\t\t \n\t\tatomic_dec(&dev->commands_outstanding);\n\t\twait_event(h->event_sync_wait_queue,\n\t\t\tatomic_read(&dev->commands_outstanding) <= 0 ||\n\t\t\tlockup_detected(h));\n\t}\n\n\tif (unlikely(lockup_detected(h))) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t \"Controller lockup detected during reset wait\\n\");\n\t\trc = -ENODEV;\n\t}\n\n\tif (!rc)\n\t\trc = wait_for_device_to_become_ready(h, dev->scsi3addr, 0);\n\n\tmutex_unlock(&h->reset_mutex);\n\treturn rc;\n}\n\nstatic void hpsa_get_raid_level(struct ctlr_info *h,\n\tunsigned char *scsi3addr, unsigned char *raid_level)\n{\n\tint rc;\n\tunsigned char *buf;\n\n\t*raid_level = RAID_UNKNOWN;\n\tbuf = kzalloc(64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tif (!hpsa_vpd_page_supported(h, scsi3addr,\n\t\tHPSA_VPD_LV_DEVICE_GEOMETRY))\n\t\tgoto exit;\n\n\trc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE |\n\t\tHPSA_VPD_LV_DEVICE_GEOMETRY, buf, 64);\n\n\tif (rc == 0)\n\t\t*raid_level = buf[8];\n\tif (*raid_level > RAID_UNKNOWN)\n\t\t*raid_level = RAID_UNKNOWN;\nexit:\n\tkfree(buf);\n\treturn;\n}\n\n#define HPSA_MAP_DEBUG\n#ifdef HPSA_MAP_DEBUG\nstatic void hpsa_debug_map_buff(struct ctlr_info *h, int rc,\n\t\t\t\tstruct raid_map_data *map_buff)\n{\n\tstruct raid_map_disk_data *dd = &map_buff->data[0];\n\tint map, row, col;\n\tu16 map_cnt, row_cnt, disks_per_row;\n\n\tif (rc != 0)\n\t\treturn;\n\n\t \n\tif (h->raid_offload_debug < 2)\n\t\treturn;\n\n\tdev_info(&h->pdev->dev, \"structure_size = %u\\n\",\n\t\t\t\tle32_to_cpu(map_buff->structure_size));\n\tdev_info(&h->pdev->dev, \"volume_blk_size = %u\\n\",\n\t\t\tle32_to_cpu(map_buff->volume_blk_size));\n\tdev_info(&h->pdev->dev, \"volume_blk_cnt = 0x%llx\\n\",\n\t\t\tle64_to_cpu(map_buff->volume_blk_cnt));\n\tdev_info(&h->pdev->dev, \"physicalBlockShift = %u\\n\",\n\t\t\tmap_buff->phys_blk_shift);\n\tdev_info(&h->pdev->dev, \"parity_rotation_shift = %u\\n\",\n\t\t\tmap_buff->parity_rotation_shift);\n\tdev_info(&h->pdev->dev, \"strip_size = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->strip_size));\n\tdev_info(&h->pdev->dev, \"disk_starting_blk = 0x%llx\\n\",\n\t\t\tle64_to_cpu(map_buff->disk_starting_blk));\n\tdev_info(&h->pdev->dev, \"disk_blk_cnt = 0x%llx\\n\",\n\t\t\tle64_to_cpu(map_buff->disk_blk_cnt));\n\tdev_info(&h->pdev->dev, \"data_disks_per_row = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->data_disks_per_row));\n\tdev_info(&h->pdev->dev, \"metadata_disks_per_row = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->metadata_disks_per_row));\n\tdev_info(&h->pdev->dev, \"row_cnt = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->row_cnt));\n\tdev_info(&h->pdev->dev, \"layout_map_count = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->layout_map_count));\n\tdev_info(&h->pdev->dev, \"flags = 0x%x\\n\",\n\t\t\tle16_to_cpu(map_buff->flags));\n\tdev_info(&h->pdev->dev, \"encryption = %s\\n\",\n\t\t\tle16_to_cpu(map_buff->flags) &\n\t\t\tRAID_MAP_FLAG_ENCRYPT_ON ?  \"ON\" : \"OFF\");\n\tdev_info(&h->pdev->dev, \"dekindex = %u\\n\",\n\t\t\tle16_to_cpu(map_buff->dekindex));\n\tmap_cnt = le16_to_cpu(map_buff->layout_map_count);\n\tfor (map = 0; map < map_cnt; map++) {\n\t\tdev_info(&h->pdev->dev, \"Map%u:\\n\", map);\n\t\trow_cnt = le16_to_cpu(map_buff->row_cnt);\n\t\tfor (row = 0; row < row_cnt; row++) {\n\t\t\tdev_info(&h->pdev->dev, \"  Row%u:\\n\", row);\n\t\t\tdisks_per_row =\n\t\t\t\tle16_to_cpu(map_buff->data_disks_per_row);\n\t\t\tfor (col = 0; col < disks_per_row; col++, dd++)\n\t\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t\t\"    D%02u: h=0x%04x xor=%u,%u\\n\",\n\t\t\t\t\tcol, dd->ioaccel_handle,\n\t\t\t\t\tdd->xor_mult[0], dd->xor_mult[1]);\n\t\t\tdisks_per_row =\n\t\t\t\tle16_to_cpu(map_buff->metadata_disks_per_row);\n\t\t\tfor (col = 0; col < disks_per_row; col++, dd++)\n\t\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t\t\"    M%02u: h=0x%04x xor=%u,%u\\n\",\n\t\t\t\t\tcol, dd->ioaccel_handle,\n\t\t\t\t\tdd->xor_mult[0], dd->xor_mult[1]);\n\t\t}\n\t}\n}\n#else\nstatic void hpsa_debug_map_buff(__attribute__((unused)) struct ctlr_info *h,\n\t\t\t__attribute__((unused)) int rc,\n\t\t\t__attribute__((unused)) struct raid_map_data *map_buff)\n{\n}\n#endif\n\nstatic int hpsa_get_raid_map(struct ctlr_info *h,\n\tunsigned char *scsi3addr, struct hpsa_scsi_dev_t *this_device)\n{\n\tint rc = 0;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\n\tif (fill_cmd(c, HPSA_GET_RAID_MAP, h, &this_device->raid_map,\n\t\t\tsizeof(this_device->raid_map), 0,\n\t\t\tscsi3addr, TYPE_CMD)) {\n\t\tdev_warn(&h->pdev->dev, \"hpsa_get_raid_map fill_cmd failed\\n\");\n\t\tcmd_free(h, c);\n\t\treturn -1;\n\t}\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\tcmd_free(h, c);\n\n\t \n\tif (le32_to_cpu(this_device->raid_map.structure_size) >\n\t\t\t\tsizeof(this_device->raid_map)) {\n\t\tdev_warn(&h->pdev->dev, \"RAID map size is too large!\\n\");\n\t\trc = -1;\n\t}\n\thpsa_debug_map_buff(h, rc, &this_device->raid_map);\n\treturn rc;\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic int hpsa_bmic_sense_subsystem_information(struct ctlr_info *h,\n\t\tunsigned char scsi3addr[], u16 bmic_device_index,\n\t\tstruct bmic_sense_subsystem_info *buf, size_t bufsize)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\n\trc = fill_cmd(c, BMIC_SENSE_SUBSYSTEM_INFORMATION, h, buf, bufsize,\n\t\t0, RAID_CTLR_LUNID, TYPE_CMD);\n\tif (rc)\n\t\tgoto out;\n\n\tc->Request.CDB[2] = bmic_device_index & 0xff;\n\tc->Request.CDB[9] = (bmic_device_index >> 8) & 0xff;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic int hpsa_bmic_id_controller(struct ctlr_info *h,\n\tstruct bmic_identify_controller *buf, size_t bufsize)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\n\trc = fill_cmd(c, BMIC_IDENTIFY_CONTROLLER, h, buf, bufsize,\n\t\t0, RAID_CTLR_LUNID, TYPE_CMD);\n\tif (rc)\n\t\tgoto out;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic int hpsa_bmic_id_physical_device(struct ctlr_info *h,\n\t\tunsigned char scsi3addr[], u16 bmic_device_index,\n\t\tstruct bmic_identify_physical_device *buf, size_t bufsize)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\trc = fill_cmd(c, BMIC_IDENTIFY_PHYSICAL_DEVICE, h, buf, bufsize,\n\t\t0, RAID_CTLR_LUNID, TYPE_CMD);\n\tif (rc)\n\t\tgoto out;\n\n\tc->Request.CDB[2] = bmic_device_index & 0xff;\n\tc->Request.CDB[9] = (bmic_device_index >> 8) & 0xff;\n\n\thpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\t\t\t\tNO_TIMEOUT);\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -1;\n\t}\nout:\n\tcmd_free(h, c);\n\n\treturn rc;\n}\n\n \nstatic void hpsa_get_enclosure_info(struct ctlr_info *h,\n\t\t\tunsigned char *scsi3addr,\n\t\t\tstruct ReportExtendedLUNdata *rlep, int rle_index,\n\t\t\tstruct hpsa_scsi_dev_t *encl_dev)\n{\n\tint rc = -1;\n\tstruct CommandList *c = NULL;\n\tstruct ErrorInfo *ei = NULL;\n\tstruct bmic_sense_storage_box_params *bssbp = NULL;\n\tstruct bmic_identify_physical_device *id_phys = NULL;\n\tstruct ext_report_lun_entry *rle;\n\tu16 bmic_device_index = 0;\n\n\tif (rle_index < 0 || rle_index >= HPSA_MAX_PHYS_LUN)\n\t\treturn;\n\n\trle = &rlep->LUN[rle_index];\n\n\tencl_dev->eli =\n\t\thpsa_get_enclosure_logical_identifier(h, scsi3addr);\n\n\tbmic_device_index = GET_BMIC_DRIVE_NUMBER(&rle->lunid[0]);\n\n\tif (encl_dev->target == -1 || encl_dev->lun == -1) {\n\t\trc = IO_OK;\n\t\tgoto out;\n\t}\n\n\tif (bmic_device_index == 0xFF00 || MASKED_DEVICE(&rle->lunid[0])) {\n\t\trc = IO_OK;\n\t\tgoto out;\n\t}\n\n\tbssbp = kzalloc(sizeof(*bssbp), GFP_KERNEL);\n\tif (!bssbp)\n\t\tgoto out;\n\n\tid_phys = kzalloc(sizeof(*id_phys), GFP_KERNEL);\n\tif (!id_phys)\n\t\tgoto out;\n\n\trc = hpsa_bmic_id_physical_device(h, scsi3addr, bmic_device_index,\n\t\t\t\t\t\tid_phys, sizeof(*id_phys));\n\tif (rc) {\n\t\tdev_warn(&h->pdev->dev, \"%s: id_phys failed %d bdi[0x%x]\\n\",\n\t\t\t__func__, encl_dev->external, bmic_device_index);\n\t\tgoto out;\n\t}\n\n\tc = cmd_alloc(h);\n\n\trc = fill_cmd(c, BMIC_SENSE_STORAGE_BOX_PARAMS, h, bssbp,\n\t\t\tsizeof(*bssbp), 0, RAID_CTLR_LUNID, TYPE_CMD);\n\n\tif (rc)\n\t\tgoto out;\n\n\tif (id_phys->phys_connector[1] == 'E')\n\t\tc->Request.CDB[5] = id_phys->box_index;\n\telse\n\t\tc->Request.CDB[5] = 0;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\t\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\tencl_dev->box[id_phys->active_path_number] = bssbp->phys_box_on_port;\n\tmemcpy(&encl_dev->phys_connector[id_phys->active_path_number],\n\t\tbssbp->phys_connector, sizeof(bssbp->phys_connector));\n\n\trc = IO_OK;\nout:\n\tkfree(bssbp);\n\tkfree(id_phys);\n\n\tif (c)\n\t\tcmd_free(h, c);\n\n\tif (rc != IO_OK)\n\t\thpsa_show_dev_msg(KERN_INFO, h, encl_dev,\n\t\t\t\"Error, could not get enclosure information\");\n}\n\nstatic u64 hpsa_get_sas_address_from_report_physical(struct ctlr_info *h,\n\t\t\t\t\t\tunsigned char *scsi3addr)\n{\n\tstruct ReportExtendedLUNdata *physdev;\n\tu32 nphysicals;\n\tu64 sa = 0;\n\tint i;\n\n\tphysdev = kzalloc(sizeof(*physdev), GFP_KERNEL);\n\tif (!physdev)\n\t\treturn 0;\n\n\tif (hpsa_scsi_do_report_phys_luns(h, physdev, sizeof(*physdev))) {\n\t\tdev_err(&h->pdev->dev, \"report physical LUNs failed.\\n\");\n\t\tkfree(physdev);\n\t\treturn 0;\n\t}\n\tnphysicals = get_unaligned_be32(physdev->LUNListLength) / 24;\n\n\tfor (i = 0; i < nphysicals; i++)\n\t\tif (!memcmp(&physdev->LUN[i].lunid[0], scsi3addr, 8)) {\n\t\t\tsa = get_unaligned_be64(&physdev->LUN[i].wwid[0]);\n\t\t\tbreak;\n\t\t}\n\n\tkfree(physdev);\n\n\treturn sa;\n}\n\nstatic void hpsa_get_sas_address(struct ctlr_info *h, unsigned char *scsi3addr,\n\t\t\t\t\tstruct hpsa_scsi_dev_t *dev)\n{\n\tint rc;\n\tu64 sa = 0;\n\n\tif (is_hba_lunid(scsi3addr)) {\n\t\tstruct bmic_sense_subsystem_info *ssi;\n\n\t\tssi = kzalloc(sizeof(*ssi), GFP_KERNEL);\n\t\tif (!ssi)\n\t\t\treturn;\n\n\t\trc = hpsa_bmic_sense_subsystem_information(h,\n\t\t\t\t\tscsi3addr, 0, ssi, sizeof(*ssi));\n\t\tif (rc == 0) {\n\t\t\tsa = get_unaligned_be64(ssi->primary_world_wide_id);\n\t\t\th->sas_address = sa;\n\t\t}\n\n\t\tkfree(ssi);\n\t} else\n\t\tsa = hpsa_get_sas_address_from_report_physical(h, scsi3addr);\n\n\tdev->sas_address = sa;\n}\n\nstatic void hpsa_ext_ctrl_present(struct ctlr_info *h,\n\tstruct ReportExtendedLUNdata *physdev)\n{\n\tu32 nphysicals;\n\tint i;\n\n\tif (h->discovery_polling)\n\t\treturn;\n\n\tnphysicals = (get_unaligned_be32(physdev->LUNListLength) / 24) + 1;\n\n\tfor (i = 0; i < nphysicals; i++) {\n\t\tif (physdev->LUN[i].device_type ==\n\t\t\tBMIC_DEVICE_TYPE_CONTROLLER\n\t\t\t&& !is_hba_lunid(physdev->LUN[i].lunid)) {\n\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t\"External controller present, activate discovery polling and disable rld caching\\n\");\n\t\t\thpsa_disable_rld_caching(h);\n\t\t\th->discovery_polling = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic bool hpsa_vpd_page_supported(struct ctlr_info *h,\n\tunsigned char scsi3addr[], u8 page)\n{\n\tint rc;\n\tint i;\n\tint pages;\n\tunsigned char *buf, bufsize;\n\n\tbuf = kzalloc(256, GFP_KERNEL);\n\tif (!buf)\n\t\treturn false;\n\n\t \n\trc = hpsa_scsi_do_inquiry(h, scsi3addr,\n\t\t\t\tVPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,\n\t\t\t\tbuf, HPSA_VPD_HEADER_SZ);\n\tif (rc != 0)\n\t\tgoto exit_unsupported;\n\tpages = buf[3];\n\tif ((pages + HPSA_VPD_HEADER_SZ) <= 255)\n\t\tbufsize = pages + HPSA_VPD_HEADER_SZ;\n\telse\n\t\tbufsize = 255;\n\n\t \n\trc = hpsa_scsi_do_inquiry(h, scsi3addr,\n\t\t\t\tVPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,\n\t\t\t\tbuf, bufsize);\n\tif (rc != 0)\n\t\tgoto exit_unsupported;\n\n\tpages = buf[3];\n\tfor (i = 1; i <= pages; i++)\n\t\tif (buf[3 + i] == page)\n\t\t\tgoto exit_supported;\nexit_unsupported:\n\tkfree(buf);\n\treturn false;\nexit_supported:\n\tkfree(buf);\n\treturn true;\n}\n\n \nstatic void hpsa_get_ioaccel_status(struct ctlr_info *h,\n\tunsigned char *scsi3addr, struct hpsa_scsi_dev_t *this_device)\n{\n\tint rc;\n\tunsigned char *buf;\n\tu8 ioaccel_status;\n\n\tthis_device->offload_config = 0;\n\tthis_device->offload_enabled = 0;\n\tthis_device->offload_to_be_enabled = 0;\n\n\tbuf = kzalloc(64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\tif (!hpsa_vpd_page_supported(h, scsi3addr, HPSA_VPD_LV_IOACCEL_STATUS))\n\t\tgoto out;\n\trc = hpsa_scsi_do_inquiry(h, scsi3addr,\n\t\t\tVPD_PAGE | HPSA_VPD_LV_IOACCEL_STATUS, buf, 64);\n\tif (rc != 0)\n\t\tgoto out;\n\n#define IOACCEL_STATUS_BYTE 4\n#define OFFLOAD_CONFIGURED_BIT 0x01\n#define OFFLOAD_ENABLED_BIT 0x02\n\tioaccel_status = buf[IOACCEL_STATUS_BYTE];\n\tthis_device->offload_config =\n\t\t!!(ioaccel_status & OFFLOAD_CONFIGURED_BIT);\n\tif (this_device->offload_config) {\n\t\tbool offload_enabled =\n\t\t\t!!(ioaccel_status & OFFLOAD_ENABLED_BIT);\n\t\t \n\t\tif (offload_enabled) {\n\t\t\trc = hpsa_get_raid_map(h, scsi3addr, this_device);\n\t\t\tif (rc)  \n\t\t\t\tgoto out;\n\t\t\tthis_device->offload_to_be_enabled = 1;\n\t\t}\n\t}\n\nout:\n\tkfree(buf);\n\treturn;\n}\n\n \nstatic int hpsa_get_device_id(struct ctlr_info *h, unsigned char *scsi3addr,\n\tunsigned char *device_id, int index, int buflen)\n{\n\tint rc;\n\tunsigned char *buf;\n\n\t \n\tif (!hpsa_vpd_page_supported(h, scsi3addr, HPSA_VPD_LV_DEVICE_ID))\n\t\treturn 1;  \n\n\tbuf = kzalloc(64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\trc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE |\n\t\t\t\t\tHPSA_VPD_LV_DEVICE_ID, buf, 64);\n\tif (rc == 0) {\n\t\tif (buflen > 16)\n\t\t\tbuflen = 16;\n\t\tmemcpy(device_id, &buf[8], buflen);\n\t}\n\n\tkfree(buf);\n\n\treturn rc;  \n}\n\nstatic int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,\n\t\tvoid *buf, int bufsize,\n\t\tint extended_response)\n{\n\tint rc = IO_OK;\n\tstruct CommandList *c;\n\tunsigned char scsi3addr[8];\n\tstruct ErrorInfo *ei;\n\n\tc = cmd_alloc(h);\n\n\t \n\tmemset(scsi3addr, 0, sizeof(scsi3addr));\n\tif (fill_cmd(c, logical ? HPSA_REPORT_LOG : HPSA_REPORT_PHYS, h,\n\t\tbuf, bufsize, 0, scsi3addr, TYPE_CMD)) {\n\t\trc = -EAGAIN;\n\t\tgoto out;\n\t}\n\tif (extended_response)\n\t\tc->Request.CDB[1] = extended_response;\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tei = c->err_info;\n\tif (ei->CommandStatus != 0 &&\n\t    ei->CommandStatus != CMD_DATA_UNDERRUN) {\n\t\thpsa_scsi_interpret_error(h, c);\n\t\trc = -EIO;\n\t} else {\n\t\tstruct ReportLUNdata *rld = buf;\n\n\t\tif (rld->extended_response_flag != extended_response) {\n\t\t\tif (!h->legacy_board) {\n\t\t\t\tdev_err(&h->pdev->dev,\n\t\t\t\t\t\"report luns requested format %u, got %u\\n\",\n\t\t\t\t\textended_response,\n\t\t\t\t\trld->extended_response_flag);\n\t\t\t\trc = -EINVAL;\n\t\t\t} else\n\t\t\t\trc = -EOPNOTSUPP;\n\t\t}\n\t}\nout:\n\tcmd_free(h, c);\n\treturn rc;\n}\n\nstatic inline int hpsa_scsi_do_report_phys_luns(struct ctlr_info *h,\n\t\tstruct ReportExtendedLUNdata *buf, int bufsize)\n{\n\tint rc;\n\tstruct ReportLUNdata *lbuf;\n\n\trc = hpsa_scsi_do_report_luns(h, 0, buf, bufsize,\n\t\t\t\t      HPSA_REPORT_PHYS_EXTENDED);\n\tif (!rc || rc != -EOPNOTSUPP)\n\t\treturn rc;\n\n\t \n\tlbuf = kzalloc(sizeof(*lbuf), GFP_KERNEL);\n\tif (!lbuf)\n\t\treturn -ENOMEM;\n\n\trc = hpsa_scsi_do_report_luns(h, 0, lbuf, sizeof(*lbuf), 0);\n\tif (!rc) {\n\t\tint i;\n\t\tu32 nphys;\n\n\t\t \n\t\tmemcpy(buf, lbuf, 8);\n\t\tnphys = be32_to_cpu(*((__be32 *)lbuf->LUNListLength)) / 8;\n\t\tfor (i = 0; i < nphys; i++)\n\t\t\tmemcpy(buf->LUN[i].lunid, lbuf->LUN[i], 8);\n\t}\n\tkfree(lbuf);\n\treturn rc;\n}\n\nstatic inline int hpsa_scsi_do_report_log_luns(struct ctlr_info *h,\n\t\tstruct ReportLUNdata *buf, int bufsize)\n{\n\treturn hpsa_scsi_do_report_luns(h, 1, buf, bufsize, 0);\n}\n\nstatic inline void hpsa_set_bus_target_lun(struct hpsa_scsi_dev_t *device,\n\tint bus, int target, int lun)\n{\n\tdevice->bus = bus;\n\tdevice->target = target;\n\tdevice->lun = lun;\n}\n\n \nstatic int hpsa_get_volume_status(struct ctlr_info *h,\n\t\t\t\t\tunsigned char scsi3addr[])\n{\n\tint rc;\n\tint status;\n\tint size;\n\tunsigned char *buf;\n\n\tbuf = kzalloc(64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn HPSA_VPD_LV_STATUS_UNSUPPORTED;\n\n\t \n\tif (!hpsa_vpd_page_supported(h, scsi3addr, HPSA_VPD_LV_STATUS))\n\t\tgoto exit_failed;\n\n\t \n\trc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,\n\t\t\t\t\tbuf, HPSA_VPD_HEADER_SZ);\n\tif (rc != 0)\n\t\tgoto exit_failed;\n\tsize = buf[3];\n\n\t \n\trc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,\n\t\t\t\t\tbuf, size + HPSA_VPD_HEADER_SZ);\n\tif (rc != 0)\n\t\tgoto exit_failed;\n\tstatus = buf[4];  \n\n\tkfree(buf);\n\treturn status;\nexit_failed:\n\tkfree(buf);\n\treturn HPSA_VPD_LV_STATUS_UNSUPPORTED;\n}\n\n \nstatic unsigned char hpsa_volume_offline(struct ctlr_info *h,\n\t\t\t\t\tunsigned char scsi3addr[])\n{\n\tstruct CommandList *c;\n\tunsigned char *sense;\n\tu8 sense_key, asc, ascq;\n\tint sense_len;\n\tint rc, ldstat = 0;\n#define ASC_LUN_NOT_READY 0x04\n#define ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS 0x04\n#define ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ 0x02\n\n\tc = cmd_alloc(h);\n\n\t(void) fill_cmd(c, TEST_UNIT_READY, h, NULL, 0, 0, scsi3addr, TYPE_CMD);\n\trc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE,\n\t\t\t\t\tNO_TIMEOUT);\n\tif (rc) {\n\t\tcmd_free(h, c);\n\t\treturn HPSA_VPD_LV_STATUS_UNSUPPORTED;\n\t}\n\tsense = c->err_info->SenseInfo;\n\tif (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))\n\t\tsense_len = sizeof(c->err_info->SenseInfo);\n\telse\n\t\tsense_len = c->err_info->SenseLen;\n\tdecode_sense_data(sense, sense_len, &sense_key, &asc, &ascq);\n\tcmd_free(h, c);\n\n\t \n\tldstat = hpsa_get_volume_status(h, scsi3addr);\n\n\t \n\tswitch (ldstat) {\n\tcase HPSA_LV_FAILED:\n\tcase HPSA_LV_UNDERGOING_ERASE:\n\tcase HPSA_LV_NOT_AVAILABLE:\n\tcase HPSA_LV_UNDERGOING_RPI:\n\tcase HPSA_LV_PENDING_RPI:\n\tcase HPSA_LV_ENCRYPTED_NO_KEY:\n\tcase HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:\n\tcase HPSA_LV_UNDERGOING_ENCRYPTION:\n\tcase HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:\n\tcase HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:\n\t\treturn ldstat;\n\tcase HPSA_VPD_LV_STATUS_UNSUPPORTED:\n\t\t \n\t\tif ((ascq == ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS) ||\n\t\t\t(ascq == ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ))\n\t\t\treturn ldstat;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn HPSA_LV_OK;\n}\n\nstatic int hpsa_update_device_info(struct ctlr_info *h,\n\tunsigned char scsi3addr[], struct hpsa_scsi_dev_t *this_device,\n\tunsigned char *is_OBDR_device)\n{\n\n#define OBDR_SIG_OFFSET 43\n#define OBDR_TAPE_SIG \"$DR-10\"\n#define OBDR_SIG_LEN (sizeof(OBDR_TAPE_SIG) - 1)\n#define OBDR_TAPE_INQ_SIZE (OBDR_SIG_OFFSET + OBDR_SIG_LEN)\n\n\tunsigned char *inq_buff;\n\tunsigned char *obdr_sig;\n\tint rc = 0;\n\n\tinq_buff = kzalloc(OBDR_TAPE_INQ_SIZE, GFP_KERNEL);\n\tif (!inq_buff) {\n\t\trc = -ENOMEM;\n\t\tgoto bail_out;\n\t}\n\n\t \n\tif (hpsa_scsi_do_inquiry(h, scsi3addr, 0, inq_buff,\n\t\t(unsigned char) OBDR_TAPE_INQ_SIZE) != 0) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"%s: inquiry failed, device will be skipped.\\n\",\n\t\t\t__func__);\n\t\trc = HPSA_INQUIRY_FAILED;\n\t\tgoto bail_out;\n\t}\n\n\tscsi_sanitize_inquiry_string(&inq_buff[8], 8);\n\tscsi_sanitize_inquiry_string(&inq_buff[16], 16);\n\n\tthis_device->devtype = (inq_buff[0] & 0x1f);\n\tmemcpy(this_device->scsi3addr, scsi3addr, 8);\n\tmemcpy(this_device->vendor, &inq_buff[8],\n\t\tsizeof(this_device->vendor));\n\tmemcpy(this_device->model, &inq_buff[16],\n\t\tsizeof(this_device->model));\n\tthis_device->rev = inq_buff[2];\n\tmemset(this_device->device_id, 0,\n\t\tsizeof(this_device->device_id));\n\tif (hpsa_get_device_id(h, scsi3addr, this_device->device_id, 8,\n\t\tsizeof(this_device->device_id)) < 0) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"hpsa%d: %s: can't get device id for [%d:%d:%d:%d]\\t%s\\t%.16s\\n\",\n\t\t\th->ctlr, __func__,\n\t\t\th->scsi_host->host_no,\n\t\t\tthis_device->bus, this_device->target,\n\t\t\tthis_device->lun,\n\t\t\tscsi_device_type(this_device->devtype),\n\t\t\tthis_device->model);\n\t\trc = HPSA_LV_FAILED;\n\t\tgoto bail_out;\n\t}\n\n\tif ((this_device->devtype == TYPE_DISK ||\n\t\tthis_device->devtype == TYPE_ZBC) &&\n\t\tis_logical_dev_addr_mode(scsi3addr)) {\n\t\tunsigned char volume_offline;\n\n\t\thpsa_get_raid_level(h, scsi3addr, &this_device->raid_level);\n\t\tif (h->fw_support & MISC_FW_RAID_OFFLOAD_BASIC)\n\t\t\thpsa_get_ioaccel_status(h, scsi3addr, this_device);\n\t\tvolume_offline = hpsa_volume_offline(h, scsi3addr);\n\t\tif (volume_offline == HPSA_VPD_LV_STATUS_UNSUPPORTED &&\n\t\t    h->legacy_board) {\n\t\t\t \n\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t \"C0:T%d:L%d Volume status not available, assuming online.\\n\",\n\t\t\t\t this_device->target, this_device->lun);\n\t\t\tvolume_offline = 0;\n\t\t}\n\t\tthis_device->volume_offline = volume_offline;\n\t\tif (volume_offline == HPSA_LV_FAILED) {\n\t\t\trc = HPSA_LV_FAILED;\n\t\t\tdev_err(&h->pdev->dev,\n\t\t\t\t\"%s: LV failed, device will be skipped.\\n\",\n\t\t\t\t__func__);\n\t\t\tgoto bail_out;\n\t\t}\n\t} else {\n\t\tthis_device->raid_level = RAID_UNKNOWN;\n\t\tthis_device->offload_config = 0;\n\t\thpsa_turn_off_ioaccel_for_device(this_device);\n\t\tthis_device->hba_ioaccel_enabled = 0;\n\t\tthis_device->volume_offline = 0;\n\t\tthis_device->queue_depth = h->nr_cmds;\n\t}\n\n\tif (this_device->external)\n\t\tthis_device->queue_depth = EXTERNAL_QD;\n\n\tif (is_OBDR_device) {\n\t\t \n\t\tobdr_sig = &inq_buff[OBDR_SIG_OFFSET];\n\t\t*is_OBDR_device = (this_device->devtype == TYPE_ROM &&\n\t\t\t\t\tstrncmp(obdr_sig, OBDR_TAPE_SIG,\n\t\t\t\t\t\tOBDR_SIG_LEN) == 0);\n\t}\n\tkfree(inq_buff);\n\treturn 0;\n\nbail_out:\n\tkfree(inq_buff);\n\treturn rc;\n}\n\n \nstatic void figure_bus_target_lun(struct ctlr_info *h,\n\tu8 *lunaddrbytes, struct hpsa_scsi_dev_t *device)\n{\n\tu32 lunid = get_unaligned_le32(lunaddrbytes);\n\n\tif (!is_logical_dev_addr_mode(lunaddrbytes)) {\n\t\t \n\t\tif (is_hba_lunid(lunaddrbytes)) {\n\t\t\tint bus = HPSA_HBA_BUS;\n\n\t\t\tif (!device->rev)\n\t\t\t\tbus = HPSA_LEGACY_HBA_BUS;\n\t\t\thpsa_set_bus_target_lun(device,\n\t\t\t\t\tbus, 0, lunid & 0x3fff);\n\t\t} else\n\t\t\t \n\t\t\thpsa_set_bus_target_lun(device,\n\t\t\t\t\tHPSA_PHYSICAL_DEVICE_BUS, -1, -1);\n\t\treturn;\n\t}\n\t \n\tif (device->external) {\n\t\thpsa_set_bus_target_lun(device,\n\t\t\tHPSA_EXTERNAL_RAID_VOLUME_BUS, (lunid >> 16) & 0x3fff,\n\t\t\tlunid & 0x00ff);\n\t\treturn;\n\t}\n\thpsa_set_bus_target_lun(device, HPSA_RAID_VOLUME_BUS,\n\t\t\t\t0, lunid & 0x3fff);\n}\n\nstatic int  figure_external_status(struct ctlr_info *h, int raid_ctlr_position,\n\tint i, int nphysicals, int nlocal_logicals)\n{\n\t \n\tint logicals_start = nphysicals + (raid_ctlr_position == 0);\n\n\tif (i == raid_ctlr_position)\n\t\treturn 0;\n\n\tif (i < logicals_start)\n\t\treturn 0;\n\n\t \n\tif ((i - nphysicals - (raid_ctlr_position == 0)) < nlocal_logicals)\n\t\treturn 0;\n\n\treturn 1;  \n}\n\n \nstatic int hpsa_gather_lun_info(struct ctlr_info *h,\n\tstruct ReportExtendedLUNdata *physdev, u32 *nphysicals,\n\tstruct ReportLUNdata *logdev, u32 *nlogicals)\n{\n\tif (hpsa_scsi_do_report_phys_luns(h, physdev, sizeof(*physdev))) {\n\t\tdev_err(&h->pdev->dev, \"report physical LUNs failed.\\n\");\n\t\treturn -1;\n\t}\n\t*nphysicals = be32_to_cpu(*((__be32 *)physdev->LUNListLength)) / 24;\n\tif (*nphysicals > HPSA_MAX_PHYS_LUN) {\n\t\tdev_warn(&h->pdev->dev, \"maximum physical LUNs (%d) exceeded. %d LUNs ignored.\\n\",\n\t\t\tHPSA_MAX_PHYS_LUN, *nphysicals - HPSA_MAX_PHYS_LUN);\n\t\t*nphysicals = HPSA_MAX_PHYS_LUN;\n\t}\n\tif (hpsa_scsi_do_report_log_luns(h, logdev, sizeof(*logdev))) {\n\t\tdev_err(&h->pdev->dev, \"report logical LUNs failed.\\n\");\n\t\treturn -1;\n\t}\n\t*nlogicals = be32_to_cpu(*((__be32 *) logdev->LUNListLength)) / 8;\n\t \n\tif (*nlogicals > HPSA_MAX_LUN) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"maximum logical LUNs (%d) exceeded.  \"\n\t\t\t\"%d LUNs ignored.\\n\", HPSA_MAX_LUN,\n\t\t\t*nlogicals - HPSA_MAX_LUN);\n\t\t*nlogicals = HPSA_MAX_LUN;\n\t}\n\tif (*nlogicals + *nphysicals > HPSA_MAX_PHYS_LUN) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"maximum logical + physical LUNs (%d) exceeded. \"\n\t\t\t\"%d LUNs ignored.\\n\", HPSA_MAX_PHYS_LUN,\n\t\t\t*nphysicals + *nlogicals - HPSA_MAX_PHYS_LUN);\n\t\t*nlogicals = HPSA_MAX_PHYS_LUN - *nphysicals;\n\t}\n\treturn 0;\n}\n\nstatic u8 *figure_lunaddrbytes(struct ctlr_info *h, int raid_ctlr_position,\n\tint i, int nphysicals, int nlogicals,\n\tstruct ReportExtendedLUNdata *physdev_list,\n\tstruct ReportLUNdata *logdev_list)\n{\n\t \n\n\tint logicals_start = nphysicals + (raid_ctlr_position == 0);\n\tint last_device = nphysicals + nlogicals + (raid_ctlr_position == 0);\n\n\tif (i == raid_ctlr_position)\n\t\treturn RAID_CTLR_LUNID;\n\n\tif (i < logicals_start)\n\t\treturn &physdev_list->LUN[i -\n\t\t\t\t(raid_ctlr_position == 0)].lunid[0];\n\n\tif (i < last_device)\n\t\treturn &logdev_list->LUN[i - nphysicals -\n\t\t\t(raid_ctlr_position == 0)][0];\n\tBUG();\n\treturn NULL;\n}\n\n \nstatic void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,\n\t\tstruct hpsa_scsi_dev_t *dev,\n\t\tstruct ReportExtendedLUNdata *rlep, int rle_index,\n\t\tstruct bmic_identify_physical_device *id_phys)\n{\n\tint rc;\n\tstruct ext_report_lun_entry *rle;\n\n\tif (rle_index < 0 || rle_index >= HPSA_MAX_PHYS_LUN)\n\t\treturn;\n\n\trle = &rlep->LUN[rle_index];\n\n\tdev->ioaccel_handle = rle->ioaccel_handle;\n\tif ((rle->device_flags & 0x08) && dev->ioaccel_handle)\n\t\tdev->hba_ioaccel_enabled = 1;\n\tmemset(id_phys, 0, sizeof(*id_phys));\n\trc = hpsa_bmic_id_physical_device(h, &rle->lunid[0],\n\t\t\tGET_BMIC_DRIVE_NUMBER(&rle->lunid[0]), id_phys,\n\t\t\tsizeof(*id_phys));\n\tif (!rc)\n\t\t \n#define DRIVE_CMDS_RESERVED_FOR_FW 2\n#define DRIVE_QUEUE_DEPTH 7\n\t\tdev->queue_depth =\n\t\t\tle16_to_cpu(id_phys->current_queue_depth_limit) -\n\t\t\t\tDRIVE_CMDS_RESERVED_FOR_FW;\n\telse\n\t\tdev->queue_depth = DRIVE_QUEUE_DEPTH;  \n}\n\nstatic void hpsa_get_path_info(struct hpsa_scsi_dev_t *this_device,\n\tstruct ReportExtendedLUNdata *rlep, int rle_index,\n\tstruct bmic_identify_physical_device *id_phys)\n{\n\tstruct ext_report_lun_entry *rle;\n\n\tif (rle_index < 0 || rle_index >= HPSA_MAX_PHYS_LUN)\n\t\treturn;\n\n\trle = &rlep->LUN[rle_index];\n\n\tif ((rle->device_flags & 0x08) && this_device->ioaccel_handle)\n\t\tthis_device->hba_ioaccel_enabled = 1;\n\n\tmemcpy(&this_device->active_path_index,\n\t\t&id_phys->active_path_number,\n\t\tsizeof(this_device->active_path_index));\n\tmemcpy(&this_device->path_map,\n\t\t&id_phys->redundant_path_present_map,\n\t\tsizeof(this_device->path_map));\n\tmemcpy(&this_device->box,\n\t\t&id_phys->alternate_paths_phys_box_on_port,\n\t\tsizeof(this_device->box));\n\tmemcpy(&this_device->phys_connector,\n\t\t&id_phys->alternate_paths_phys_connector,\n\t\tsizeof(this_device->phys_connector));\n\tmemcpy(&this_device->bay,\n\t\t&id_phys->phys_bay_in_box,\n\t\tsizeof(this_device->bay));\n}\n\n \nstatic int hpsa_set_local_logical_count(struct ctlr_info *h,\n\tstruct bmic_identify_controller *id_ctlr,\n\tu32 *nlocals)\n{\n\tint rc;\n\n\tif (!id_ctlr) {\n\t\tdev_warn(&h->pdev->dev, \"%s: id_ctlr buffer is NULL.\\n\",\n\t\t\t__func__);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(id_ctlr, 0, sizeof(*id_ctlr));\n\trc = hpsa_bmic_id_controller(h, id_ctlr, sizeof(*id_ctlr));\n\tif (!rc)\n\t\tif (id_ctlr->configured_logical_drive_count < 255)\n\t\t\t*nlocals = id_ctlr->configured_logical_drive_count;\n\t\telse\n\t\t\t*nlocals = le16_to_cpu(\n\t\t\t\t\tid_ctlr->extended_logical_unit_count);\n\telse\n\t\t*nlocals = -1;\n\treturn rc;\n}\n\nstatic bool hpsa_is_disk_spare(struct ctlr_info *h, u8 *lunaddrbytes)\n{\n\tstruct bmic_identify_physical_device *id_phys;\n\tbool is_spare = false;\n\tint rc;\n\n\tid_phys = kzalloc(sizeof(*id_phys), GFP_KERNEL);\n\tif (!id_phys)\n\t\treturn false;\n\n\trc = hpsa_bmic_id_physical_device(h,\n\t\t\t\t\tlunaddrbytes,\n\t\t\t\t\tGET_BMIC_DRIVE_NUMBER(lunaddrbytes),\n\t\t\t\t\tid_phys, sizeof(*id_phys));\n\tif (rc == 0)\n\t\tis_spare = (id_phys->more_flags >> 6) & 0x01;\n\n\tkfree(id_phys);\n\treturn is_spare;\n}\n\n#define RPL_DEV_FLAG_NON_DISK                           0x1\n#define RPL_DEV_FLAG_UNCONFIG_DISK_REPORTING_SUPPORTED  0x2\n#define RPL_DEV_FLAG_UNCONFIG_DISK                      0x4\n\n#define BMIC_DEVICE_TYPE_ENCLOSURE  6\n\nstatic bool hpsa_skip_device(struct ctlr_info *h, u8 *lunaddrbytes,\n\t\t\t\tstruct ext_report_lun_entry *rle)\n{\n\tu8 device_flags;\n\tu8 device_type;\n\n\tif (!MASKED_DEVICE(lunaddrbytes))\n\t\treturn false;\n\n\tdevice_flags = rle->device_flags;\n\tdevice_type = rle->device_type;\n\n\tif (device_flags & RPL_DEV_FLAG_NON_DISK) {\n\t\tif (device_type == BMIC_DEVICE_TYPE_ENCLOSURE)\n\t\t\treturn false;\n\t\treturn true;\n\t}\n\n\tif (!(device_flags & RPL_DEV_FLAG_UNCONFIG_DISK_REPORTING_SUPPORTED))\n\t\treturn false;\n\n\tif (device_flags & RPL_DEV_FLAG_UNCONFIG_DISK)\n\t\treturn false;\n\n\t \n\tif (hpsa_is_disk_spare(h, lunaddrbytes))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void hpsa_update_scsi_devices(struct ctlr_info *h)\n{\n\t \n\tstruct ReportExtendedLUNdata *physdev_list = NULL;\n\tstruct ReportLUNdata *logdev_list = NULL;\n\tstruct bmic_identify_physical_device *id_phys = NULL;\n\tstruct bmic_identify_controller *id_ctlr = NULL;\n\tu32 nphysicals = 0;\n\tu32 nlogicals = 0;\n\tu32 nlocal_logicals = 0;\n\tu32 ndev_allocated = 0;\n\tstruct hpsa_scsi_dev_t **currentsd, *this_device, *tmpdevice;\n\tint ncurrent = 0;\n\tint i, ndevs_to_allocate;\n\tint raid_ctlr_position;\n\tbool physical_device;\n\n\tcurrentsd = kcalloc(HPSA_MAX_DEVICES, sizeof(*currentsd), GFP_KERNEL);\n\tphysdev_list = kzalloc(sizeof(*physdev_list), GFP_KERNEL);\n\tlogdev_list = kzalloc(sizeof(*logdev_list), GFP_KERNEL);\n\ttmpdevice = kzalloc(sizeof(*tmpdevice), GFP_KERNEL);\n\tid_phys = kzalloc(sizeof(*id_phys), GFP_KERNEL);\n\tid_ctlr = kzalloc(sizeof(*id_ctlr), GFP_KERNEL);\n\n\tif (!currentsd || !physdev_list || !logdev_list ||\n\t\t!tmpdevice || !id_phys || !id_ctlr) {\n\t\tdev_err(&h->pdev->dev, \"out of memory\\n\");\n\t\tgoto out;\n\t}\n\n\th->drv_req_rescan = 0;  \n\n\tif (hpsa_gather_lun_info(h, physdev_list, &nphysicals,\n\t\t\tlogdev_list, &nlogicals)) {\n\t\th->drv_req_rescan = 1;\n\t\tgoto out;\n\t}\n\n\t \n\tif (hpsa_set_local_logical_count(h, id_ctlr, &nlocal_logicals)) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"%s: Can't determine number of local logical devices.\\n\",\n\t\t\t__func__);\n\t}\n\n\t \n\tndevs_to_allocate = nphysicals + nlogicals + MAX_EXT_TARGETS + 1;\n\n\thpsa_ext_ctrl_present(h, physdev_list);\n\n\t \n\tfor (i = 0; i < ndevs_to_allocate; i++) {\n\t\tif (i >= HPSA_MAX_DEVICES) {\n\t\t\tdev_warn(&h->pdev->dev, \"maximum devices (%d) exceeded.\"\n\t\t\t\t\"  %d devices ignored.\\n\", HPSA_MAX_DEVICES,\n\t\t\t\tndevs_to_allocate - HPSA_MAX_DEVICES);\n\t\t\tbreak;\n\t\t}\n\n\t\tcurrentsd[i] = kzalloc(sizeof(*currentsd[i]), GFP_KERNEL);\n\t\tif (!currentsd[i]) {\n\t\t\th->drv_req_rescan = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tndev_allocated++;\n\t}\n\n\tif (is_scsi_rev_5(h))\n\t\traid_ctlr_position = 0;\n\telse\n\t\traid_ctlr_position = nphysicals + nlogicals;\n\n\t \n\tfor (i = 0; i < nphysicals + nlogicals + 1; i++) {\n\t\tu8 *lunaddrbytes, is_OBDR = 0;\n\t\tint rc = 0;\n\t\tint phys_dev_index = i - (raid_ctlr_position == 0);\n\t\tbool skip_device = false;\n\n\t\tmemset(tmpdevice, 0, sizeof(*tmpdevice));\n\n\t\tphysical_device = i < nphysicals + (raid_ctlr_position == 0);\n\n\t\t \n\t\tlunaddrbytes = figure_lunaddrbytes(h, raid_ctlr_position,\n\t\t\ti, nphysicals, nlogicals, physdev_list, logdev_list);\n\n\t\t \n\t\ttmpdevice->external =\n\t\t\tfigure_external_status(h, raid_ctlr_position, i,\n\t\t\t\t\t\tnphysicals, nlocal_logicals);\n\n\t\t \n\t\tif (phys_dev_index >= 0 && !tmpdevice->external &&\n\t\t\tphysical_device) {\n\t\t\tskip_device = hpsa_skip_device(h, lunaddrbytes,\n\t\t\t\t\t&physdev_list->LUN[phys_dev_index]);\n\t\t\tif (skip_device)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\trc = hpsa_update_device_info(h, lunaddrbytes, tmpdevice,\n\t\t\t\t\t\t\t&is_OBDR);\n\t\tif (rc == -ENOMEM) {\n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"Out of memory, rescan deferred.\\n\");\n\t\t\th->drv_req_rescan = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tif (rc) {\n\t\t\th->drv_req_rescan = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfigure_bus_target_lun(h, lunaddrbytes, tmpdevice);\n\t\tthis_device = currentsd[ncurrent];\n\n\t\t*this_device = *tmpdevice;\n\t\tthis_device->physical_device = physical_device;\n\n\t\t \n\t\tif (MASKED_DEVICE(lunaddrbytes) && this_device->physical_device)\n\t\t\tthis_device->expose_device = 0;\n\t\telse\n\t\t\tthis_device->expose_device = 1;\n\n\n\t\t \n\t\tif (this_device->physical_device && this_device->expose_device)\n\t\t\thpsa_get_sas_address(h, lunaddrbytes, this_device);\n\n\t\tswitch (this_device->devtype) {\n\t\tcase TYPE_ROM:\n\t\t\t \n\t\t\tif (is_OBDR)\n\t\t\t\tncurrent++;\n\t\t\tbreak;\n\t\tcase TYPE_DISK:\n\t\tcase TYPE_ZBC:\n\t\t\tif (this_device->physical_device) {\n\t\t\t\t \n\t\t\t\t \n\t\t\t\tthis_device->offload_enabled = 0;\n\t\t\t\thpsa_get_ioaccel_drive_info(h, this_device,\n\t\t\t\t\tphysdev_list, phys_dev_index, id_phys);\n\t\t\t\thpsa_get_path_info(this_device,\n\t\t\t\t\tphysdev_list, phys_dev_index, id_phys);\n\t\t\t}\n\t\t\tncurrent++;\n\t\t\tbreak;\n\t\tcase TYPE_TAPE:\n\t\tcase TYPE_MEDIUM_CHANGER:\n\t\t\tncurrent++;\n\t\t\tbreak;\n\t\tcase TYPE_ENCLOSURE:\n\t\t\tif (!this_device->external)\n\t\t\t\thpsa_get_enclosure_info(h, lunaddrbytes,\n\t\t\t\t\t\tphysdev_list, phys_dev_index,\n\t\t\t\t\t\tthis_device);\n\t\t\tncurrent++;\n\t\t\tbreak;\n\t\tcase TYPE_RAID:\n\t\t\t \n\t\t\tif (!is_hba_lunid(lunaddrbytes))\n\t\t\t\tbreak;\n\t\t\tncurrent++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tif (ncurrent >= HPSA_MAX_DEVICES)\n\t\t\tbreak;\n\t}\n\n\tif (h->sas_host == NULL) {\n\t\tint rc = 0;\n\n\t\trc = hpsa_add_sas_host(h);\n\t\tif (rc) {\n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"Could not add sas host %d\\n\", rc);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tadjust_hpsa_scsi_table(h, currentsd, ncurrent);\nout:\n\tkfree(tmpdevice);\n\tfor (i = 0; i < ndev_allocated; i++)\n\t\tkfree(currentsd[i]);\n\tkfree(currentsd);\n\tkfree(physdev_list);\n\tkfree(logdev_list);\n\tkfree(id_ctlr);\n\tkfree(id_phys);\n}\n\nstatic void hpsa_set_sg_descriptor(struct SGDescriptor *desc,\n\t\t\t\t   struct scatterlist *sg)\n{\n\tu64 addr64 = (u64) sg_dma_address(sg);\n\tunsigned int len = sg_dma_len(sg);\n\n\tdesc->Addr = cpu_to_le64(addr64);\n\tdesc->Len = cpu_to_le32(len);\n\tdesc->Ext = 0;\n}\n\n \nstatic int hpsa_scatter_gather(struct ctlr_info *h,\n\t\tstruct CommandList *cp,\n\t\tstruct scsi_cmnd *cmd)\n{\n\tstruct scatterlist *sg;\n\tint use_sg, i, sg_limit, chained;\n\tstruct SGDescriptor *curr_sg;\n\n\tBUG_ON(scsi_sg_count(cmd) > h->maxsgentries);\n\n\tuse_sg = scsi_dma_map(cmd);\n\tif (use_sg < 0)\n\t\treturn use_sg;\n\n\tif (!use_sg)\n\t\tgoto sglist_finished;\n\n\t \n\tcurr_sg = cp->SG;\n\tchained = use_sg > h->max_cmd_sg_entries;\n\tsg_limit = chained ? h->max_cmd_sg_entries - 1 : use_sg;\n\tscsi_for_each_sg(cmd, sg, sg_limit, i) {\n\t\thpsa_set_sg_descriptor(curr_sg, sg);\n\t\tcurr_sg++;\n\t}\n\n\tif (chained) {\n\t\t \n\t\tcurr_sg = h->cmd_sg_list[cp->cmdindex];\n\t\tsg_limit = use_sg - sg_limit;\n\t\tfor_each_sg(sg, sg, sg_limit, i) {\n\t\t\thpsa_set_sg_descriptor(curr_sg, sg);\n\t\t\tcurr_sg++;\n\t\t}\n\t}\n\n\t \n\t(curr_sg - 1)->Ext = cpu_to_le32(HPSA_SG_LAST);\n\n\tif (use_sg + chained > h->maxSG)\n\t\th->maxSG = use_sg + chained;\n\n\tif (chained) {\n\t\tcp->Header.SGList = h->max_cmd_sg_entries;\n\t\tcp->Header.SGTotal = cpu_to_le16(use_sg + 1);\n\t\tif (hpsa_map_sg_chain_block(h, cp)) {\n\t\t\tscsi_dma_unmap(cmd);\n\t\t\treturn -1;\n\t\t}\n\t\treturn 0;\n\t}\n\nsglist_finished:\n\n\tcp->Header.SGList = (u8) use_sg;    \n\tcp->Header.SGTotal = cpu_to_le16(use_sg);  \n\treturn 0;\n}\n\nstatic inline void warn_zero_length_transfer(struct ctlr_info *h,\n\t\t\t\t\t\tu8 *cdb, int cdb_len,\n\t\t\t\t\t\tconst char *func)\n{\n\tdev_warn(&h->pdev->dev,\n\t\t \"%s: Blocking zero-length request: CDB:%*phN\\n\",\n\t\t func, cdb_len, cdb);\n}\n\n#define IO_ACCEL_INELIGIBLE 1\n \nstatic bool is_zero_length_transfer(u8 *cdb)\n{\n\tu32 block_cnt;\n\n\t \n\tswitch (cdb[0]) {\n\tcase READ_10:\n\tcase WRITE_10:\n\tcase VERIFY:\t\t \n\tcase WRITE_VERIFY:\t \n\t\tblock_cnt = get_unaligned_be16(&cdb[7]);\n\t\tbreak;\n\tcase READ_12:\n\tcase WRITE_12:\n\tcase VERIFY_12:  \n\tcase WRITE_VERIFY_12:\t \n\t\tblock_cnt = get_unaligned_be32(&cdb[6]);\n\t\tbreak;\n\tcase READ_16:\n\tcase WRITE_16:\n\tcase VERIFY_16:\t\t \n\t\tblock_cnt = get_unaligned_be32(&cdb[10]);\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn block_cnt == 0;\n}\n\nstatic int fixup_ioaccel_cdb(u8 *cdb, int *cdb_len)\n{\n\tint is_write = 0;\n\tu32 block;\n\tu32 block_cnt;\n\n\t \n\tswitch (cdb[0]) {\n\tcase WRITE_6:\n\tcase WRITE_12:\n\t\tis_write = 1;\n\t\tfallthrough;\n\tcase READ_6:\n\tcase READ_12:\n\t\tif (*cdb_len == 6) {\n\t\t\tblock = (((cdb[1] & 0x1F) << 16) |\n\t\t\t\t(cdb[2] << 8) |\n\t\t\t\tcdb[3]);\n\t\t\tblock_cnt = cdb[4];\n\t\t\tif (block_cnt == 0)\n\t\t\t\tblock_cnt = 256;\n\t\t} else {\n\t\t\tBUG_ON(*cdb_len != 12);\n\t\t\tblock = get_unaligned_be32(&cdb[2]);\n\t\t\tblock_cnt = get_unaligned_be32(&cdb[6]);\n\t\t}\n\t\tif (block_cnt > 0xffff)\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t\tcdb[0] = is_write ? WRITE_10 : READ_10;\n\t\tcdb[1] = 0;\n\t\tcdb[2] = (u8) (block >> 24);\n\t\tcdb[3] = (u8) (block >> 16);\n\t\tcdb[4] = (u8) (block >> 8);\n\t\tcdb[5] = (u8) (block);\n\t\tcdb[6] = 0;\n\t\tcdb[7] = (u8) (block_cnt >> 8);\n\t\tcdb[8] = (u8) (block_cnt);\n\t\tcdb[9] = 0;\n\t\t*cdb_len = 10;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int hpsa_scsi_ioaccel1_queue_command(struct ctlr_info *h,\n\tstruct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,\n\tu8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk)\n{\n\tstruct scsi_cmnd *cmd = c->scsi_cmd;\n\tstruct io_accel1_cmd *cp = &h->ioaccel_cmd_pool[c->cmdindex];\n\tunsigned int len;\n\tunsigned int total_len = 0;\n\tstruct scatterlist *sg;\n\tu64 addr64;\n\tint use_sg, i;\n\tstruct SGDescriptor *curr_sg;\n\tu32 control = IOACCEL1_CONTROL_SIMPLEQUEUE;\n\n\t \n\tif (scsi_sg_count(cmd) > h->ioaccel_maxsg) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tBUG_ON(cmd->cmd_len > IOACCEL1_IOFLAGS_CDBLEN_MAX);\n\n\tif (is_zero_length_transfer(cdb)) {\n\t\twarn_zero_length_transfer(h, cdb, cdb_len, __func__);\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tif (fixup_ioaccel_cdb(cdb, &cdb_len)) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tc->cmd_type = CMD_IOACCEL1;\n\n\t \n\tc->busaddr = (u32) h->ioaccel_cmd_pool_dhandle +\n\t\t\t\t(c->cmdindex * sizeof(*cp));\n\tBUG_ON(c->busaddr & 0x0000007F);\n\n\tuse_sg = scsi_dma_map(cmd);\n\tif (use_sg < 0) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn use_sg;\n\t}\n\n\tif (use_sg) {\n\t\tcurr_sg = cp->SG;\n\t\tscsi_for_each_sg(cmd, sg, use_sg, i) {\n\t\t\taddr64 = (u64) sg_dma_address(sg);\n\t\t\tlen  = sg_dma_len(sg);\n\t\t\ttotal_len += len;\n\t\t\tcurr_sg->Addr = cpu_to_le64(addr64);\n\t\t\tcurr_sg->Len = cpu_to_le32(len);\n\t\t\tcurr_sg->Ext = cpu_to_le32(0);\n\t\t\tcurr_sg++;\n\t\t}\n\t\t(--curr_sg)->Ext = cpu_to_le32(HPSA_SG_LAST);\n\n\t\tswitch (cmd->sc_data_direction) {\n\t\tcase DMA_TO_DEVICE:\n\t\t\tcontrol |= IOACCEL1_CONTROL_DATA_OUT;\n\t\t\tbreak;\n\t\tcase DMA_FROM_DEVICE:\n\t\t\tcontrol |= IOACCEL1_CONTROL_DATA_IN;\n\t\t\tbreak;\n\t\tcase DMA_NONE:\n\t\t\tcontrol |= IOACCEL1_CONTROL_NODATAXFER;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(&h->pdev->dev, \"unknown data direction: %d\\n\",\n\t\t\tcmd->sc_data_direction);\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tcontrol |= IOACCEL1_CONTROL_NODATAXFER;\n\t}\n\n\tc->Header.SGList = use_sg;\n\t \n\tcp->dev_handle = cpu_to_le16(ioaccel_handle & 0xFFFF);\n\tcp->transfer_len = cpu_to_le32(total_len);\n\tcp->io_flags = cpu_to_le16(IOACCEL1_IOFLAGS_IO_REQ |\n\t\t\t(cdb_len & IOACCEL1_IOFLAGS_CDBLEN_MASK));\n\tcp->control = cpu_to_le32(control);\n\tmemcpy(cp->CDB, cdb, cdb_len);\n\tmemcpy(cp->CISS_LUN, scsi3addr, 8);\n\t \n\tenqueue_cmd_and_start_io(h, c);\n\treturn 0;\n}\n\n \nstatic int hpsa_scsi_ioaccel_direct_map(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tstruct scsi_cmnd *cmd = c->scsi_cmd;\n\tstruct hpsa_scsi_dev_t *dev = cmd->device->hostdata;\n\n\tif (!dev)\n\t\treturn -1;\n\n\tc->phys_disk = dev;\n\n\tif (dev->in_reset)\n\t\treturn -1;\n\n\treturn hpsa_scsi_ioaccel_queue_command(h, c, dev->ioaccel_handle,\n\t\tcmd->cmnd, cmd->cmd_len, dev->scsi3addr, dev);\n}\n\n \nstatic void set_encrypt_ioaccel2(struct ctlr_info *h,\n\tstruct CommandList *c, struct io_accel2_cmd *cp)\n{\n\tstruct scsi_cmnd *cmd = c->scsi_cmd;\n\tstruct hpsa_scsi_dev_t *dev = cmd->device->hostdata;\n\tstruct raid_map_data *map = &dev->raid_map;\n\tu64 first_block;\n\n\t \n\tif (!(le16_to_cpu(map->flags) & RAID_MAP_FLAG_ENCRYPT_ON))\n\t\treturn;\n\t \n\tcp->dekindex = map->dekindex;\n\n\t \n\tcp->direction |= IOACCEL2_DIRECTION_ENCRYPT_MASK;\n\n\t \n\tswitch (cmd->cmnd[0]) {\n\t \n\tcase READ_6:\n\tcase WRITE_6:\n\t\tfirst_block = (((cmd->cmnd[1] & 0x1F) << 16) |\n\t\t\t\t(cmd->cmnd[2] << 8) |\n\t\t\t\tcmd->cmnd[3]);\n\t\tbreak;\n\tcase WRITE_10:\n\tcase READ_10:\n\t \n\tcase WRITE_12:\n\tcase READ_12:\n\t\tfirst_block = get_unaligned_be32(&cmd->cmnd[2]);\n\t\tbreak;\n\tcase WRITE_16:\n\tcase READ_16:\n\t\tfirst_block = get_unaligned_be64(&cmd->cmnd[2]);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"ERROR: %s: size (0x%x) not supported for encryption\\n\",\n\t\t\t__func__, cmd->cmnd[0]);\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (le32_to_cpu(map->volume_blk_size) != 512)\n\t\tfirst_block = first_block *\n\t\t\t\tle32_to_cpu(map->volume_blk_size)/512;\n\n\tcp->tweak_lower = cpu_to_le32(first_block);\n\tcp->tweak_upper = cpu_to_le32(first_block >> 32);\n}\n\nstatic int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,\n\tstruct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,\n\tu8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk)\n{\n\tstruct scsi_cmnd *cmd = c->scsi_cmd;\n\tstruct io_accel2_cmd *cp = &h->ioaccel2_cmd_pool[c->cmdindex];\n\tstruct ioaccel2_sg_element *curr_sg;\n\tint use_sg, i;\n\tstruct scatterlist *sg;\n\tu64 addr64;\n\tu32 len;\n\tu32 total_len = 0;\n\n\tif (!cmd->device)\n\t\treturn -1;\n\n\tif (!cmd->device->hostdata)\n\t\treturn -1;\n\n\tBUG_ON(scsi_sg_count(cmd) > h->maxsgentries);\n\n\tif (is_zero_length_transfer(cdb)) {\n\t\twarn_zero_length_transfer(h, cdb, cdb_len, __func__);\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tif (fixup_ioaccel_cdb(cdb, &cdb_len)) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tc->cmd_type = CMD_IOACCEL2;\n\t \n\tc->busaddr = (u32) h->ioaccel2_cmd_pool_dhandle +\n\t\t\t\t(c->cmdindex * sizeof(*cp));\n\tBUG_ON(c->busaddr & 0x0000007F);\n\n\tmemset(cp, 0, sizeof(*cp));\n\tcp->IU_type = IOACCEL2_IU_TYPE;\n\n\tuse_sg = scsi_dma_map(cmd);\n\tif (use_sg < 0) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn use_sg;\n\t}\n\n\tif (use_sg) {\n\t\tcurr_sg = cp->sg;\n\t\tif (use_sg > h->ioaccel_maxsg) {\n\t\t\taddr64 = le64_to_cpu(\n\t\t\t\th->ioaccel2_cmd_sg_list[c->cmdindex]->address);\n\t\t\tcurr_sg->address = cpu_to_le64(addr64);\n\t\t\tcurr_sg->length = 0;\n\t\t\tcurr_sg->reserved[0] = 0;\n\t\t\tcurr_sg->reserved[1] = 0;\n\t\t\tcurr_sg->reserved[2] = 0;\n\t\t\tcurr_sg->chain_indicator = IOACCEL2_CHAIN;\n\n\t\t\tcurr_sg = h->ioaccel2_cmd_sg_list[c->cmdindex];\n\t\t}\n\t\tscsi_for_each_sg(cmd, sg, use_sg, i) {\n\t\t\taddr64 = (u64) sg_dma_address(sg);\n\t\t\tlen  = sg_dma_len(sg);\n\t\t\ttotal_len += len;\n\t\t\tcurr_sg->address = cpu_to_le64(addr64);\n\t\t\tcurr_sg->length = cpu_to_le32(len);\n\t\t\tcurr_sg->reserved[0] = 0;\n\t\t\tcurr_sg->reserved[1] = 0;\n\t\t\tcurr_sg->reserved[2] = 0;\n\t\t\tcurr_sg->chain_indicator = 0;\n\t\t\tcurr_sg++;\n\t\t}\n\n\t\t \n\t\t(curr_sg - 1)->chain_indicator = IOACCEL2_LAST_SG;\n\n\t\tswitch (cmd->sc_data_direction) {\n\t\tcase DMA_TO_DEVICE:\n\t\t\tcp->direction &= ~IOACCEL2_DIRECTION_MASK;\n\t\t\tcp->direction |= IOACCEL2_DIR_DATA_OUT;\n\t\t\tbreak;\n\t\tcase DMA_FROM_DEVICE:\n\t\t\tcp->direction &= ~IOACCEL2_DIRECTION_MASK;\n\t\t\tcp->direction |= IOACCEL2_DIR_DATA_IN;\n\t\t\tbreak;\n\t\tcase DMA_NONE:\n\t\t\tcp->direction &= ~IOACCEL2_DIRECTION_MASK;\n\t\t\tcp->direction |= IOACCEL2_DIR_NO_DATA;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(&h->pdev->dev, \"unknown data direction: %d\\n\",\n\t\t\t\tcmd->sc_data_direction);\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tcp->direction &= ~IOACCEL2_DIRECTION_MASK;\n\t\tcp->direction |= IOACCEL2_DIR_NO_DATA;\n\t}\n\n\t \n\tset_encrypt_ioaccel2(h, c, cp);\n\n\tcp->scsi_nexus = cpu_to_le32(ioaccel_handle);\n\tcp->Tag = cpu_to_le32(c->cmdindex << DIRECT_LOOKUP_SHIFT);\n\tmemcpy(cp->cdb, cdb, sizeof(cp->cdb));\n\n\tcp->data_len = cpu_to_le32(total_len);\n\tcp->err_ptr = cpu_to_le64(c->busaddr +\n\t\t\toffsetof(struct io_accel2_cmd, error_data));\n\tcp->err_len = cpu_to_le32(sizeof(cp->error_data));\n\n\t \n\tif (use_sg > h->ioaccel_maxsg) {\n\t\tcp->sg_count = 1;\n\t\tcp->sg[0].length = cpu_to_le32(use_sg * sizeof(cp->sg[0]));\n\t\tif (hpsa_map_ioaccel2_sg_chain_block(h, cp, c)) {\n\t\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\t\tscsi_dma_unmap(cmd);\n\t\t\treturn -1;\n\t\t}\n\t} else\n\t\tcp->sg_count = (u8) use_sg;\n\n\tif (phys_disk->in_reset) {\n\t\tcmd->result = DID_RESET << 16;\n\t\treturn -1;\n\t}\n\n\tenqueue_cmd_and_start_io(h, c);\n\treturn 0;\n}\n\n \nstatic int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,\n\tstruct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,\n\tu8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk)\n{\n\tif (!c->scsi_cmd->device)\n\t\treturn -1;\n\n\tif (!c->scsi_cmd->device->hostdata)\n\t\treturn -1;\n\n\tif (phys_disk->in_reset)\n\t\treturn -1;\n\n\t \n\tif (atomic_inc_return(&phys_disk->ioaccel_cmds_out) >\n\t\t\t\t\tphys_disk->queue_depth) {\n\t\tatomic_dec(&phys_disk->ioaccel_cmds_out);\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\tif (h->transMethod & CFGTBL_Trans_io_accel1)\n\t\treturn hpsa_scsi_ioaccel1_queue_command(h, c, ioaccel_handle,\n\t\t\t\t\t\tcdb, cdb_len, scsi3addr,\n\t\t\t\t\t\tphys_disk);\n\telse\n\t\treturn hpsa_scsi_ioaccel2_queue_command(h, c, ioaccel_handle,\n\t\t\t\t\t\tcdb, cdb_len, scsi3addr,\n\t\t\t\t\t\tphys_disk);\n}\n\nstatic void raid_map_helper(struct raid_map_data *map,\n\t\tint offload_to_mirror, u32 *map_index, u32 *current_group)\n{\n\tif (offload_to_mirror == 0)  {\n\t\t \n\t\t*map_index %= le16_to_cpu(map->data_disks_per_row);\n\t\treturn;\n\t}\n\tdo {\n\t\t \n\t\t*current_group = *map_index /\n\t\t\tle16_to_cpu(map->data_disks_per_row);\n\t\tif (offload_to_mirror == *current_group)\n\t\t\tcontinue;\n\t\tif (*current_group < le16_to_cpu(map->layout_map_count) - 1) {\n\t\t\t \n\t\t\t*map_index += le16_to_cpu(map->data_disks_per_row);\n\t\t\t(*current_group)++;\n\t\t} else {\n\t\t\t \n\t\t\t*map_index %= le16_to_cpu(map->data_disks_per_row);\n\t\t\t*current_group = 0;\n\t\t}\n\t} while (offload_to_mirror != *current_group);\n}\n\n \nstatic int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tstruct scsi_cmnd *cmd = c->scsi_cmd;\n\tstruct hpsa_scsi_dev_t *dev = cmd->device->hostdata;\n\tstruct raid_map_data *map = &dev->raid_map;\n\tstruct raid_map_disk_data *dd = &map->data[0];\n\tint is_write = 0;\n\tu32 map_index;\n\tu64 first_block, last_block;\n\tu32 block_cnt;\n\tu32 blocks_per_row;\n\tu64 first_row, last_row;\n\tu32 first_row_offset, last_row_offset;\n\tu32 first_column, last_column;\n\tu64 r0_first_row, r0_last_row;\n\tu32 r5or6_blocks_per_row;\n\tu64 r5or6_first_row, r5or6_last_row;\n\tu32 r5or6_first_row_offset, r5or6_last_row_offset;\n\tu32 r5or6_first_column, r5or6_last_column;\n\tu32 total_disks_per_row;\n\tu32 stripesize;\n\tu32 first_group, last_group, current_group;\n\tu32 map_row;\n\tu32 disk_handle;\n\tu64 disk_block;\n\tu32 disk_block_cnt;\n\tu8 cdb[16];\n\tu8 cdb_len;\n\tu16 strip_size;\n#if BITS_PER_LONG == 32\n\tu64 tmpdiv;\n#endif\n\tint offload_to_mirror;\n\n\tif (!dev)\n\t\treturn -1;\n\n\tif (dev->in_reset)\n\t\treturn -1;\n\n\t \n\tswitch (cmd->cmnd[0]) {\n\tcase WRITE_6:\n\t\tis_write = 1;\n\t\tfallthrough;\n\tcase READ_6:\n\t\tfirst_block = (((cmd->cmnd[1] & 0x1F) << 16) |\n\t\t\t\t(cmd->cmnd[2] << 8) |\n\t\t\t\tcmd->cmnd[3]);\n\t\tblock_cnt = cmd->cmnd[4];\n\t\tif (block_cnt == 0)\n\t\t\tblock_cnt = 256;\n\t\tbreak;\n\tcase WRITE_10:\n\t\tis_write = 1;\n\t\tfallthrough;\n\tcase READ_10:\n\t\tfirst_block =\n\t\t\t(((u64) cmd->cmnd[2]) << 24) |\n\t\t\t(((u64) cmd->cmnd[3]) << 16) |\n\t\t\t(((u64) cmd->cmnd[4]) << 8) |\n\t\t\tcmd->cmnd[5];\n\t\tblock_cnt =\n\t\t\t(((u32) cmd->cmnd[7]) << 8) |\n\t\t\tcmd->cmnd[8];\n\t\tbreak;\n\tcase WRITE_12:\n\t\tis_write = 1;\n\t\tfallthrough;\n\tcase READ_12:\n\t\tfirst_block =\n\t\t\t(((u64) cmd->cmnd[2]) << 24) |\n\t\t\t(((u64) cmd->cmnd[3]) << 16) |\n\t\t\t(((u64) cmd->cmnd[4]) << 8) |\n\t\t\tcmd->cmnd[5];\n\t\tblock_cnt =\n\t\t\t(((u32) cmd->cmnd[6]) << 24) |\n\t\t\t(((u32) cmd->cmnd[7]) << 16) |\n\t\t\t(((u32) cmd->cmnd[8]) << 8) |\n\t\tcmd->cmnd[9];\n\t\tbreak;\n\tcase WRITE_16:\n\t\tis_write = 1;\n\t\tfallthrough;\n\tcase READ_16:\n\t\tfirst_block =\n\t\t\t(((u64) cmd->cmnd[2]) << 56) |\n\t\t\t(((u64) cmd->cmnd[3]) << 48) |\n\t\t\t(((u64) cmd->cmnd[4]) << 40) |\n\t\t\t(((u64) cmd->cmnd[5]) << 32) |\n\t\t\t(((u64) cmd->cmnd[6]) << 24) |\n\t\t\t(((u64) cmd->cmnd[7]) << 16) |\n\t\t\t(((u64) cmd->cmnd[8]) << 8) |\n\t\t\tcmd->cmnd[9];\n\t\tblock_cnt =\n\t\t\t(((u32) cmd->cmnd[10]) << 24) |\n\t\t\t(((u32) cmd->cmnd[11]) << 16) |\n\t\t\t(((u32) cmd->cmnd[12]) << 8) |\n\t\t\tcmd->cmnd[13];\n\t\tbreak;\n\tdefault:\n\t\treturn IO_ACCEL_INELIGIBLE;  \n\t}\n\tlast_block = first_block + block_cnt - 1;\n\n\t \n\tif (is_write && dev->raid_level != 0)\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t \n\tif (last_block >= le64_to_cpu(map->volume_blk_cnt) ||\n\t\tlast_block < first_block)\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t \n\tblocks_per_row = le16_to_cpu(map->data_disks_per_row) *\n\t\t\t\tle16_to_cpu(map->strip_size);\n\tstrip_size = le16_to_cpu(map->strip_size);\n#if BITS_PER_LONG == 32\n\ttmpdiv = first_block;\n\t(void) do_div(tmpdiv, blocks_per_row);\n\tfirst_row = tmpdiv;\n\ttmpdiv = last_block;\n\t(void) do_div(tmpdiv, blocks_per_row);\n\tlast_row = tmpdiv;\n\tfirst_row_offset = (u32) (first_block - (first_row * blocks_per_row));\n\tlast_row_offset = (u32) (last_block - (last_row * blocks_per_row));\n\ttmpdiv = first_row_offset;\n\t(void) do_div(tmpdiv, strip_size);\n\tfirst_column = tmpdiv;\n\ttmpdiv = last_row_offset;\n\t(void) do_div(tmpdiv, strip_size);\n\tlast_column = tmpdiv;\n#else\n\tfirst_row = first_block / blocks_per_row;\n\tlast_row = last_block / blocks_per_row;\n\tfirst_row_offset = (u32) (first_block - (first_row * blocks_per_row));\n\tlast_row_offset = (u32) (last_block - (last_row * blocks_per_row));\n\tfirst_column = first_row_offset / strip_size;\n\tlast_column = last_row_offset / strip_size;\n#endif\n\n\t \n\tif ((first_row != last_row) || (first_column != last_column))\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t \n\ttotal_disks_per_row = le16_to_cpu(map->data_disks_per_row) +\n\t\t\t\tle16_to_cpu(map->metadata_disks_per_row);\n\tmap_row = ((u32)(first_row >> map->parity_rotation_shift)) %\n\t\t\t\tle16_to_cpu(map->row_cnt);\n\tmap_index = (map_row * total_disks_per_row) + first_column;\n\n\tswitch (dev->raid_level) {\n\tcase HPSA_RAID_0:\n\t\tbreak;  \n\tcase HPSA_RAID_1:\n\t\t \n\t\tif (le16_to_cpu(map->layout_map_count) != 2) {\n\t\t\thpsa_turn_off_ioaccel_for_device(dev);\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\t\t}\n\t\tif (dev->offload_to_mirror)\n\t\t\tmap_index += le16_to_cpu(map->data_disks_per_row);\n\t\tdev->offload_to_mirror = !dev->offload_to_mirror;\n\t\tbreak;\n\tcase HPSA_RAID_ADM:\n\t\t \n\t\tif (le16_to_cpu(map->layout_map_count) != 3) {\n\t\t\thpsa_turn_off_ioaccel_for_device(dev);\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\t\t}\n\n\t\toffload_to_mirror = dev->offload_to_mirror;\n\t\traid_map_helper(map, offload_to_mirror,\n\t\t\t\t&map_index, &current_group);\n\t\t \n\t\toffload_to_mirror =\n\t\t\t(offload_to_mirror >=\n\t\t\tle16_to_cpu(map->layout_map_count) - 1)\n\t\t\t? 0 : offload_to_mirror + 1;\n\t\tdev->offload_to_mirror = offload_to_mirror;\n\t\t \n\t\tbreak;\n\tcase HPSA_RAID_5:\n\tcase HPSA_RAID_6:\n\t\tif (le16_to_cpu(map->layout_map_count) <= 1)\n\t\t\tbreak;\n\n\t\t \n\t\tr5or6_blocks_per_row =\n\t\t\tle16_to_cpu(map->strip_size) *\n\t\t\tle16_to_cpu(map->data_disks_per_row);\n\t\tif (r5or6_blocks_per_row == 0) {\n\t\t\thpsa_turn_off_ioaccel_for_device(dev);\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\t\t}\n\t\tstripesize = r5or6_blocks_per_row *\n\t\t\tle16_to_cpu(map->layout_map_count);\n#if BITS_PER_LONG == 32\n\t\ttmpdiv = first_block;\n\t\tfirst_group = do_div(tmpdiv, stripesize);\n\t\ttmpdiv = first_group;\n\t\t(void) do_div(tmpdiv, r5or6_blocks_per_row);\n\t\tfirst_group = tmpdiv;\n\t\ttmpdiv = last_block;\n\t\tlast_group = do_div(tmpdiv, stripesize);\n\t\ttmpdiv = last_group;\n\t\t(void) do_div(tmpdiv, r5or6_blocks_per_row);\n\t\tlast_group = tmpdiv;\n#else\n\t\tfirst_group = (first_block % stripesize) / r5or6_blocks_per_row;\n\t\tlast_group = (last_block % stripesize) / r5or6_blocks_per_row;\n#endif\n\t\tif (first_group != last_group)\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t\t \n#if BITS_PER_LONG == 32\n\t\ttmpdiv = first_block;\n\t\t(void) do_div(tmpdiv, stripesize);\n\t\tfirst_row = r5or6_first_row = r0_first_row = tmpdiv;\n\t\ttmpdiv = last_block;\n\t\t(void) do_div(tmpdiv, stripesize);\n\t\tr5or6_last_row = r0_last_row = tmpdiv;\n#else\n\t\tfirst_row = r5or6_first_row = r0_first_row =\n\t\t\t\t\t\tfirst_block / stripesize;\n\t\tr5or6_last_row = r0_last_row = last_block / stripesize;\n#endif\n\t\tif (r5or6_first_row != r5or6_last_row)\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\n\n\t\t \n#if BITS_PER_LONG == 32\n\t\ttmpdiv = first_block;\n\t\tfirst_row_offset = do_div(tmpdiv, stripesize);\n\t\ttmpdiv = first_row_offset;\n\t\tfirst_row_offset = (u32) do_div(tmpdiv, r5or6_blocks_per_row);\n\t\tr5or6_first_row_offset = first_row_offset;\n\t\ttmpdiv = last_block;\n\t\tr5or6_last_row_offset = do_div(tmpdiv, stripesize);\n\t\ttmpdiv = r5or6_last_row_offset;\n\t\tr5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);\n\t\ttmpdiv = r5or6_first_row_offset;\n\t\t(void) do_div(tmpdiv, map->strip_size);\n\t\tfirst_column = r5or6_first_column = tmpdiv;\n\t\ttmpdiv = r5or6_last_row_offset;\n\t\t(void) do_div(tmpdiv, map->strip_size);\n\t\tr5or6_last_column = tmpdiv;\n#else\n\t\tfirst_row_offset = r5or6_first_row_offset =\n\t\t\t(u32)((first_block % stripesize) %\n\t\t\t\t\t\tr5or6_blocks_per_row);\n\n\t\tr5or6_last_row_offset =\n\t\t\t(u32)((last_block % stripesize) %\n\t\t\t\t\t\tr5or6_blocks_per_row);\n\n\t\tfirst_column = r5or6_first_column =\n\t\t\tr5or6_first_row_offset / le16_to_cpu(map->strip_size);\n\t\tr5or6_last_column =\n\t\t\tr5or6_last_row_offset / le16_to_cpu(map->strip_size);\n#endif\n\t\tif (r5or6_first_column != r5or6_last_column)\n\t\t\treturn IO_ACCEL_INELIGIBLE;\n\n\t\t \n\t\tmap_row = ((u32)(first_row >> map->parity_rotation_shift)) %\n\t\t\tle16_to_cpu(map->row_cnt);\n\n\t\tmap_index = (first_group *\n\t\t\t(le16_to_cpu(map->row_cnt) * total_disks_per_row)) +\n\t\t\t(map_row * total_disks_per_row) + first_column;\n\t\tbreak;\n\tdefault:\n\t\treturn IO_ACCEL_INELIGIBLE;\n\t}\n\n\tif (unlikely(map_index >= RAID_MAP_MAX_ENTRIES))\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\tc->phys_disk = dev->phys_disk[map_index];\n\tif (!c->phys_disk)\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\tdisk_handle = dd[map_index].ioaccel_handle;\n\tdisk_block = le64_to_cpu(map->disk_starting_blk) +\n\t\t\tfirst_row * le16_to_cpu(map->strip_size) +\n\t\t\t(first_row_offset - first_column *\n\t\t\tle16_to_cpu(map->strip_size));\n\tdisk_block_cnt = block_cnt;\n\n\t \n\tif (map->phys_blk_shift) {\n\t\tdisk_block <<= map->phys_blk_shift;\n\t\tdisk_block_cnt <<= map->phys_blk_shift;\n\t}\n\tBUG_ON(disk_block_cnt > 0xffff);\n\n\t \n\tif (disk_block > 0xffffffff) {\n\t\tcdb[0] = is_write ? WRITE_16 : READ_16;\n\t\tcdb[1] = 0;\n\t\tcdb[2] = (u8) (disk_block >> 56);\n\t\tcdb[3] = (u8) (disk_block >> 48);\n\t\tcdb[4] = (u8) (disk_block >> 40);\n\t\tcdb[5] = (u8) (disk_block >> 32);\n\t\tcdb[6] = (u8) (disk_block >> 24);\n\t\tcdb[7] = (u8) (disk_block >> 16);\n\t\tcdb[8] = (u8) (disk_block >> 8);\n\t\tcdb[9] = (u8) (disk_block);\n\t\tcdb[10] = (u8) (disk_block_cnt >> 24);\n\t\tcdb[11] = (u8) (disk_block_cnt >> 16);\n\t\tcdb[12] = (u8) (disk_block_cnt >> 8);\n\t\tcdb[13] = (u8) (disk_block_cnt);\n\t\tcdb[14] = 0;\n\t\tcdb[15] = 0;\n\t\tcdb_len = 16;\n\t} else {\n\t\tcdb[0] = is_write ? WRITE_10 : READ_10;\n\t\tcdb[1] = 0;\n\t\tcdb[2] = (u8) (disk_block >> 24);\n\t\tcdb[3] = (u8) (disk_block >> 16);\n\t\tcdb[4] = (u8) (disk_block >> 8);\n\t\tcdb[5] = (u8) (disk_block);\n\t\tcdb[6] = 0;\n\t\tcdb[7] = (u8) (disk_block_cnt >> 8);\n\t\tcdb[8] = (u8) (disk_block_cnt);\n\t\tcdb[9] = 0;\n\t\tcdb_len = 10;\n\t}\n\treturn hpsa_scsi_ioaccel_queue_command(h, c, disk_handle, cdb, cdb_len,\n\t\t\t\t\t\tdev->scsi3addr,\n\t\t\t\t\t\tdev->phys_disk[map_index]);\n}\n\n \nstatic int hpsa_ciss_submit(struct ctlr_info *h,\n\tstruct CommandList *c, struct scsi_cmnd *cmd,\n\tstruct hpsa_scsi_dev_t *dev)\n{\n\tcmd->host_scribble = (unsigned char *) c;\n\tc->cmd_type = CMD_SCSI;\n\tc->scsi_cmd = cmd;\n\tc->Header.ReplyQueue = 0;   \n\tmemcpy(&c->Header.LUN.LunAddrBytes[0], &dev->scsi3addr[0], 8);\n\tc->Header.tag = cpu_to_le64((c->cmdindex << DIRECT_LOOKUP_SHIFT));\n\n\t \n\n\tc->Request.Timeout = 0;\n\tBUG_ON(cmd->cmd_len > sizeof(c->Request.CDB));\n\tc->Request.CDBLen = cmd->cmd_len;\n\tmemcpy(c->Request.CDB, cmd->cmnd, cmd->cmd_len);\n\tswitch (cmd->sc_data_direction) {\n\tcase DMA_TO_DEVICE:\n\t\tc->Request.type_attr_dir =\n\t\t\tTYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_WRITE);\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\tc->Request.type_attr_dir =\n\t\t\tTYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_READ);\n\t\tbreak;\n\tcase DMA_NONE:\n\t\tc->Request.type_attr_dir =\n\t\t\tTYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_NONE);\n\t\tbreak;\n\tcase DMA_BIDIRECTIONAL:\n\t\t \n\n\t\tc->Request.type_attr_dir =\n\t\t\tTYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_RSVD);\n\t\t \n\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&h->pdev->dev, \"unknown data direction: %d\\n\",\n\t\t\tcmd->sc_data_direction);\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (hpsa_scatter_gather(h, c, cmd) < 0) {  \n\t\thpsa_cmd_resolve_and_free(h, c);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tif (dev->in_reset) {\n\t\thpsa_cmd_resolve_and_free(h, c);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tc->device = dev;\n\n\tenqueue_cmd_and_start_io(h, c);\n\t \n\treturn 0;\n}\n\nstatic void hpsa_cmd_init(struct ctlr_info *h, int index,\n\t\t\t\tstruct CommandList *c)\n{\n\tdma_addr_t cmd_dma_handle, err_dma_handle;\n\n\t \n\tmemset(c, 0, offsetof(struct CommandList, refcount));\n\tc->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));\n\tcmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);\n\tc->err_info = h->errinfo_pool + index;\n\tmemset(c->err_info, 0, sizeof(*c->err_info));\n\terr_dma_handle = h->errinfo_pool_dhandle\n\t    + index * sizeof(*c->err_info);\n\tc->cmdindex = index;\n\tc->busaddr = (u32) cmd_dma_handle;\n\tc->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);\n\tc->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));\n\tc->h = h;\n\tc->scsi_cmd = SCSI_CMD_IDLE;\n}\n\nstatic void hpsa_preinitialize_commands(struct ctlr_info *h)\n{\n\tint i;\n\n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\tstruct CommandList *c = h->cmd_pool + i;\n\n\t\thpsa_cmd_init(h, i, c);\n\t\tatomic_set(&c->refcount, 0);\n\t}\n}\n\nstatic inline void hpsa_cmd_partial_init(struct ctlr_info *h, int index,\n\t\t\t\tstruct CommandList *c)\n{\n\tdma_addr_t cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);\n\n\tBUG_ON(c->cmdindex != index);\n\n\tmemset(c->Request.CDB, 0, sizeof(c->Request.CDB));\n\tmemset(c->err_info, 0, sizeof(*c->err_info));\n\tc->busaddr = (u32) cmd_dma_handle;\n}\n\nstatic int hpsa_ioaccel_submit(struct ctlr_info *h,\n\t\tstruct CommandList *c, struct scsi_cmnd *cmd,\n\t\tbool retry)\n{\n\tstruct hpsa_scsi_dev_t *dev = cmd->device->hostdata;\n\tint rc = IO_ACCEL_INELIGIBLE;\n\n\tif (!dev)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\tif (dev->in_reset)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\tif (hpsa_simple_mode)\n\t\treturn IO_ACCEL_INELIGIBLE;\n\n\tcmd->host_scribble = (unsigned char *) c;\n\n\tif (dev->offload_enabled) {\n\t\thpsa_cmd_init(h, c->cmdindex, c);  \n\t\tc->cmd_type = CMD_SCSI;\n\t\tc->scsi_cmd = cmd;\n\t\tc->device = dev;\n\t\tif (retry)  \n\t\t\tc->retry_pending = true;\n\t\trc = hpsa_scsi_ioaccel_raid_map(h, c);\n\t\tif (rc < 0)      \n\t\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t} else if (dev->hba_ioaccel_enabled) {\n\t\thpsa_cmd_init(h, c->cmdindex, c);  \n\t\tc->cmd_type = CMD_SCSI;\n\t\tc->scsi_cmd = cmd;\n\t\tc->device = dev;\n\t\tif (retry)  \n\t\t\tc->retry_pending = true;\n\t\trc = hpsa_scsi_ioaccel_direct_map(h, c);\n\t\tif (rc < 0)      \n\t\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\treturn rc;\n}\n\nstatic void hpsa_command_resubmit_worker(struct work_struct *work)\n{\n\tstruct scsi_cmnd *cmd;\n\tstruct hpsa_scsi_dev_t *dev;\n\tstruct CommandList *c = container_of(work, struct CommandList, work);\n\n\tcmd = c->scsi_cmd;\n\tdev = cmd->device->hostdata;\n\tif (!dev) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\treturn hpsa_cmd_free_and_done(c->h, c, cmd);\n\t}\n\n\tif (dev->in_reset) {\n\t\tcmd->result = DID_RESET << 16;\n\t\treturn hpsa_cmd_free_and_done(c->h, c, cmd);\n\t}\n\n\tif (c->cmd_type == CMD_IOACCEL2) {\n\t\tstruct ctlr_info *h = c->h;\n\t\tstruct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];\n\t\tint rc;\n\n\t\tif (c2->error_data.serv_response ==\n\t\t\t\tIOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {\n\t\t\t \n\t\t\trc = hpsa_ioaccel_submit(h, c, cmd, true);\n\t\t\tif (rc == 0)\n\t\t\t\treturn;\n\t\t\tif (rc == SCSI_MLQUEUE_HOST_BUSY) {\n\t\t\t\t \n\t\t\t\tcmd->result = DID_IMM_RETRY << 16;\n\t\t\t\treturn hpsa_cmd_free_and_done(h, c, cmd);\n\t\t\t}\n\t\t\t \n\t\t}\n\t}\n\thpsa_cmd_partial_init(c->h, c->cmdindex, c);\n\t \n\tc->retry_pending = true;\n\tif (hpsa_ciss_submit(c->h, c, cmd, dev)) {\n\t\t \n\t\tcmd->result = DID_IMM_RETRY << 16;\n\t\tscsi_done(cmd);\n\t}\n}\n\n \nstatic int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)\n{\n\tstruct ctlr_info *h;\n\tstruct hpsa_scsi_dev_t *dev;\n\tstruct CommandList *c;\n\tint rc = 0;\n\n\t \n\th = sdev_to_hba(cmd->device);\n\n\tBUG_ON(scsi_cmd_to_rq(cmd)->tag < 0);\n\n\tdev = cmd->device->hostdata;\n\tif (!dev) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\tscsi_done(cmd);\n\t\treturn 0;\n\t}\n\n\tif (dev->removed) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\tscsi_done(cmd);\n\t\treturn 0;\n\t}\n\n\tif (unlikely(lockup_detected(h))) {\n\t\tcmd->result = DID_NO_CONNECT << 16;\n\t\tscsi_done(cmd);\n\t\treturn 0;\n\t}\n\n\tif (dev->in_reset)\n\t\treturn SCSI_MLQUEUE_DEVICE_BUSY;\n\n\tc = cmd_tagged_alloc(h, cmd);\n\tif (c == NULL)\n\t\treturn SCSI_MLQUEUE_DEVICE_BUSY;\n\n\t \n\tcmd->result = 0;\n\n\t \n\tif (likely(cmd->retries == 0 &&\n\t\t\t!blk_rq_is_passthrough(scsi_cmd_to_rq(cmd)) &&\n\t\t\th->acciopath_status)) {\n\t\t \n\t\trc = hpsa_ioaccel_submit(h, c, cmd, false);\n\t\tif (rc == 0)\n\t\t\treturn 0;\n\t\tif (rc == SCSI_MLQUEUE_HOST_BUSY) {\n\t\t\thpsa_cmd_resolve_and_free(h, c);\n\t\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t\t}\n\t}\n\treturn hpsa_ciss_submit(h, c, cmd, dev);\n}\n\nstatic void hpsa_scan_complete(struct ctlr_info *h)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&h->scan_lock, flags);\n\th->scan_finished = 1;\n\twake_up(&h->scan_wait_queue);\n\tspin_unlock_irqrestore(&h->scan_lock, flags);\n}\n\nstatic void hpsa_scan_start(struct Scsi_Host *sh)\n{\n\tstruct ctlr_info *h = shost_to_hba(sh);\n\tunsigned long flags;\n\n\t \n\tif (unlikely(lockup_detected(h)))\n\t\treturn hpsa_scan_complete(h);\n\n\t \n\tspin_lock_irqsave(&h->scan_lock, flags);\n\tif (h->scan_waiting) {\n\t\tspin_unlock_irqrestore(&h->scan_lock, flags);\n\t\treturn;\n\t}\n\n\tspin_unlock_irqrestore(&h->scan_lock, flags);\n\n\t \n\twhile (1) {\n\t\tspin_lock_irqsave(&h->scan_lock, flags);\n\t\tif (h->scan_finished)\n\t\t\tbreak;\n\t\th->scan_waiting = 1;\n\t\tspin_unlock_irqrestore(&h->scan_lock, flags);\n\t\twait_event(h->scan_wait_queue, h->scan_finished);\n\t\t \n\t}\n\th->scan_finished = 0;  \n\th->scan_waiting = 0;\n\tspin_unlock_irqrestore(&h->scan_lock, flags);\n\n\tif (unlikely(lockup_detected(h)))\n\t\treturn hpsa_scan_complete(h);\n\n\t \n\tspin_lock_irqsave(&h->reset_lock, flags);\n\tif (h->reset_in_progress) {\n\t\th->drv_req_rescan = 1;\n\t\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\t\thpsa_scan_complete(h);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\n\thpsa_update_scsi_devices(h);\n\n\thpsa_scan_complete(h);\n}\n\nstatic int hpsa_change_queue_depth(struct scsi_device *sdev, int qdepth)\n{\n\tstruct hpsa_scsi_dev_t *logical_drive = sdev->hostdata;\n\n\tif (!logical_drive)\n\t\treturn -ENODEV;\n\n\tif (qdepth < 1)\n\t\tqdepth = 1;\n\telse if (qdepth > logical_drive->queue_depth)\n\t\tqdepth = logical_drive->queue_depth;\n\n\treturn scsi_change_queue_depth(sdev, qdepth);\n}\n\nstatic int hpsa_scan_finished(struct Scsi_Host *sh,\n\tunsigned long elapsed_time)\n{\n\tstruct ctlr_info *h = shost_to_hba(sh);\n\tunsigned long flags;\n\tint finished;\n\n\tspin_lock_irqsave(&h->scan_lock, flags);\n\tfinished = h->scan_finished;\n\tspin_unlock_irqrestore(&h->scan_lock, flags);\n\treturn finished;\n}\n\nstatic int hpsa_scsi_host_alloc(struct ctlr_info *h)\n{\n\tstruct Scsi_Host *sh;\n\n\tsh = scsi_host_alloc(&hpsa_driver_template, sizeof(struct ctlr_info));\n\tif (sh == NULL) {\n\t\tdev_err(&h->pdev->dev, \"scsi_host_alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tsh->io_port = 0;\n\tsh->n_io_port = 0;\n\tsh->this_id = -1;\n\tsh->max_channel = 3;\n\tsh->max_cmd_len = MAX_COMMAND_SIZE;\n\tsh->max_lun = HPSA_MAX_LUN;\n\tsh->max_id = HPSA_MAX_LUN;\n\tsh->can_queue = h->nr_cmds - HPSA_NRESERVED_CMDS;\n\tsh->cmd_per_lun = sh->can_queue;\n\tsh->sg_tablesize = h->maxsgentries;\n\tsh->transportt = hpsa_sas_transport_template;\n\tsh->hostdata[0] = (unsigned long) h;\n\tsh->irq = pci_irq_vector(h->pdev, 0);\n\tsh->unique_id = sh->irq;\n\n\th->scsi_host = sh;\n\treturn 0;\n}\n\nstatic int hpsa_scsi_add_host(struct ctlr_info *h)\n{\n\tint rv;\n\n\trv = scsi_add_host(h->scsi_host, &h->pdev->dev);\n\tif (rv) {\n\t\tdev_err(&h->pdev->dev, \"scsi_add_host failed\\n\");\n\t\treturn rv;\n\t}\n\tscsi_scan_host(h->scsi_host);\n\treturn 0;\n}\n\n \nstatic int hpsa_get_cmd_index(struct scsi_cmnd *scmd)\n{\n\tint idx = scsi_cmd_to_rq(scmd)->tag;\n\n\tif (idx < 0)\n\t\treturn idx;\n\n\t \n\treturn idx += HPSA_NRESERVED_CMDS;\n}\n\n \nstatic int hpsa_send_test_unit_ready(struct ctlr_info *h,\n\t\t\t\tstruct CommandList *c, unsigned char lunaddr[],\n\t\t\t\tint reply_queue)\n{\n\tint rc;\n\n\t \n\t(void) fill_cmd(c, TEST_UNIT_READY, h,\n\t\t\tNULL, 0, 0, lunaddr, TYPE_CMD);\n\trc = hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);\n\tif (rc)\n\t\treturn rc;\n\t \n\n\t \n\tif (c->err_info->CommandStatus == CMD_SUCCESS)\n\t\treturn 0;\n\n\t \n\tif (c->err_info->CommandStatus == CMD_TARGET_STATUS &&\n\t\tc->err_info->ScsiStatus == SAM_STAT_CHECK_CONDITION &&\n\t\t\t(c->err_info->SenseInfo[2] == NO_SENSE ||\n\t\t\t c->err_info->SenseInfo[2] == UNIT_ATTENTION))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nstatic int hpsa_wait_for_test_unit_ready(struct ctlr_info *h,\n\t\t\t\tstruct CommandList *c,\n\t\t\t\tunsigned char lunaddr[], int reply_queue)\n{\n\tint rc;\n\tint count = 0;\n\tint waittime = 1;  \n\n\t \n\tfor (count = 0; count < HPSA_TUR_RETRY_LIMIT; count++) {\n\n\t\t \n\t\tmsleep(1000 * waittime);\n\n\t\trc = hpsa_send_test_unit_ready(h, c, lunaddr, reply_queue);\n\t\tif (!rc)\n\t\t\tbreak;\n\n\t\t \n\t\tif (waittime < HPSA_MAX_WAIT_INTERVAL_SECS)\n\t\t\twaittime *= 2;\n\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t \"waiting %d secs for device to become ready.\\n\",\n\t\t\t waittime);\n\t}\n\n\treturn rc;\n}\n\nstatic int wait_for_device_to_become_ready(struct ctlr_info *h,\n\t\t\t\t\t   unsigned char lunaddr[],\n\t\t\t\t\t   int reply_queue)\n{\n\tint first_queue;\n\tint last_queue;\n\tint rq;\n\tint rc = 0;\n\tstruct CommandList *c;\n\n\tc = cmd_alloc(h);\n\n\t \n\tif (reply_queue == DEFAULT_REPLY_QUEUE) {\n\t\tfirst_queue = 0;\n\t\tlast_queue = h->nreply_queues - 1;\n\t} else {\n\t\tfirst_queue = reply_queue;\n\t\tlast_queue = reply_queue;\n\t}\n\n\tfor (rq = first_queue; rq <= last_queue; rq++) {\n\t\trc = hpsa_wait_for_test_unit_ready(h, c, lunaddr, rq);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\tif (rc)\n\t\tdev_warn(&h->pdev->dev, \"giving up on device.\\n\");\n\telse\n\t\tdev_warn(&h->pdev->dev, \"device is ready.\\n\");\n\n\tcmd_free(h, c);\n\treturn rc;\n}\n\n \nstatic int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)\n{\n\tint rc = SUCCESS;\n\tint i;\n\tstruct ctlr_info *h;\n\tstruct hpsa_scsi_dev_t *dev = NULL;\n\tu8 reset_type;\n\tchar msg[48];\n\tunsigned long flags;\n\n\t \n\th = sdev_to_hba(scsicmd->device);\n\tif (h == NULL)  \n\t\treturn FAILED;\n\n\tspin_lock_irqsave(&h->reset_lock, flags);\n\th->reset_in_progress = 1;\n\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\n\tif (lockup_detected(h)) {\n\t\trc = FAILED;\n\t\tgoto return_reset_status;\n\t}\n\n\tdev = scsicmd->device->hostdata;\n\tif (!dev) {\n\t\tdev_err(&h->pdev->dev, \"%s: device lookup failed\\n\", __func__);\n\t\trc = FAILED;\n\t\tgoto return_reset_status;\n\t}\n\n\tif (dev->devtype == TYPE_ENCLOSURE) {\n\t\trc = SUCCESS;\n\t\tgoto return_reset_status;\n\t}\n\n\t \n\tif (lockup_detected(h)) {\n\t\tsnprintf(msg, sizeof(msg),\n\t\t\t \"cmd %d RESET FAILED, lockup detected\",\n\t\t\t hpsa_get_cmd_index(scsicmd));\n\t\thpsa_show_dev_msg(KERN_WARNING, h, dev, msg);\n\t\trc = FAILED;\n\t\tgoto return_reset_status;\n\t}\n\n\t \n\tif (detect_controller_lockup(h)) {\n\t\tsnprintf(msg, sizeof(msg),\n\t\t\t \"cmd %d RESET FAILED, new lockup detected\",\n\t\t\t hpsa_get_cmd_index(scsicmd));\n\t\thpsa_show_dev_msg(KERN_WARNING, h, dev, msg);\n\t\trc = FAILED;\n\t\tgoto return_reset_status;\n\t}\n\n\t \n\tif (is_hba_lunid(dev->scsi3addr)) {\n\t\trc = SUCCESS;\n\t\tgoto return_reset_status;\n\t}\n\n\tif (is_logical_dev_addr_mode(dev->scsi3addr))\n\t\treset_type = HPSA_DEVICE_RESET_MSG;\n\telse\n\t\treset_type = HPSA_PHYS_TARGET_RESET;\n\n\tsprintf(msg, \"resetting %s\",\n\t\treset_type == HPSA_DEVICE_RESET_MSG ? \"logical \" : \"physical \");\n\thpsa_show_dev_msg(KERN_WARNING, h, dev, msg);\n\n\t \n\tdev->in_reset = true;  \n\tfor (i = 0; i < 10; i++) {\n\t\tif (atomic_read(&dev->commands_outstanding) > 0)\n\t\t\tmsleep(1000);\n\t\telse\n\t\t\tbreak;\n\t}\n\n\t \n\trc = hpsa_do_reset(h, dev, reset_type, DEFAULT_REPLY_QUEUE);\n\tif (rc == 0)\n\t\trc = SUCCESS;\n\telse\n\t\trc = FAILED;\n\n\tsprintf(msg, \"reset %s %s\",\n\t\treset_type == HPSA_DEVICE_RESET_MSG ? \"logical \" : \"physical \",\n\t\trc == SUCCESS ? \"completed successfully\" : \"failed\");\n\thpsa_show_dev_msg(KERN_WARNING, h, dev, msg);\n\nreturn_reset_status:\n\tspin_lock_irqsave(&h->reset_lock, flags);\n\th->reset_in_progress = 0;\n\tif (dev)\n\t\tdev->in_reset = false;\n\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\treturn rc;\n}\n\n \nstatic struct CommandList *cmd_tagged_alloc(struct ctlr_info *h,\n\t\t\t\t\t    struct scsi_cmnd *scmd)\n{\n\tint idx = hpsa_get_cmd_index(scmd);\n\tstruct CommandList *c = h->cmd_pool + idx;\n\n\tif (idx < HPSA_NRESERVED_CMDS || idx >= h->nr_cmds) {\n\t\tdev_err(&h->pdev->dev, \"Bad block tag: %d not in [%d..%d]\\n\",\n\t\t\tidx, HPSA_NRESERVED_CMDS, h->nr_cmds - 1);\n\t\t \n\t\tBUG();\n\t}\n\n\tif (unlikely(!hpsa_is_cmd_idle(c))) {\n\t\t \n\t\tif (idx != h->last_collision_tag) {  \n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"%s: tag collision (tag=%d)\\n\", __func__, idx);\n\t\t\tif (scmd)\n\t\t\t\tscsi_print_command(scmd);\n\t\t\th->last_collision_tag = idx;\n\t\t}\n\t\treturn NULL;\n\t}\n\n\tatomic_inc(&c->refcount);\n\thpsa_cmd_partial_init(h, idx, c);\n\n\t \n\tc->retry_pending = false;\n\n\treturn c;\n}\n\nstatic void cmd_tagged_free(struct ctlr_info *h, struct CommandList *c)\n{\n\t \n\t(void)atomic_dec(&c->refcount);\n}\n\n \n\nstatic struct CommandList *cmd_alloc(struct ctlr_info *h)\n{\n\tstruct CommandList *c;\n\tint refcount, i;\n\tint offset = 0;\n\n\t \n\n\tfor (;;) {\n\t\ti = find_next_zero_bit(h->cmd_pool_bits,\n\t\t\t\t\tHPSA_NRESERVED_CMDS,\n\t\t\t\t\toffset);\n\t\tif (unlikely(i >= HPSA_NRESERVED_CMDS)) {\n\t\t\toffset = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tc = h->cmd_pool + i;\n\t\trefcount = atomic_inc_return(&c->refcount);\n\t\tif (unlikely(refcount > 1)) {\n\t\t\tcmd_free(h, c);  \n\t\t\toffset = (i + 1) % HPSA_NRESERVED_CMDS;\n\t\t\tcontinue;\n\t\t}\n\t\tset_bit(i, h->cmd_pool_bits);\n\t\tbreak;  \n\t}\n\thpsa_cmd_partial_init(h, i, c);\n\tc->device = NULL;\n\n\t \n\tc->retry_pending = false;\n\n\treturn c;\n}\n\n \nstatic void cmd_free(struct ctlr_info *h, struct CommandList *c)\n{\n\tif (atomic_dec_and_test(&c->refcount)) {\n\t\tint i;\n\n\t\ti = c - h->cmd_pool;\n\t\tclear_bit(i, h->cmd_pool_bits);\n\t}\n}\n\n#ifdef CONFIG_COMPAT\n\nstatic int hpsa_ioctl32_passthru(struct scsi_device *dev, unsigned int cmd,\n\tvoid __user *arg)\n{\n\tstruct ctlr_info *h = sdev_to_hba(dev);\n\tIOCTL32_Command_struct __user *arg32 = arg;\n\tIOCTL_Command_struct arg64;\n\tint err;\n\tu32 cp;\n\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\tmemset(&arg64, 0, sizeof(arg64));\n\tif (copy_from_user(&arg64, arg32, offsetof(IOCTL_Command_struct, buf)))\n\t\treturn -EFAULT;\n\tif (get_user(cp, &arg32->buf))\n\t\treturn -EFAULT;\n\targ64.buf = compat_ptr(cp);\n\n\tif (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)\n\t\treturn -EAGAIN;\n\terr = hpsa_passthru_ioctl(h, &arg64);\n\tatomic_inc(&h->passthru_cmds_avail);\n\tif (err)\n\t\treturn err;\n\tif (copy_to_user(&arg32->error_info, &arg64.error_info,\n\t\t\t sizeof(arg32->error_info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int hpsa_ioctl32_big_passthru(struct scsi_device *dev,\n\tunsigned int cmd, void __user *arg)\n{\n\tstruct ctlr_info *h = sdev_to_hba(dev);\n\tBIG_IOCTL32_Command_struct __user *arg32 = arg;\n\tBIG_IOCTL_Command_struct arg64;\n\tint err;\n\tu32 cp;\n\n\tif (!arg)\n\t\treturn -EINVAL;\n\tmemset(&arg64, 0, sizeof(arg64));\n\tif (copy_from_user(&arg64, arg32,\n\t\t\t   offsetof(BIG_IOCTL32_Command_struct, buf)))\n\t\treturn -EFAULT;\n\tif (get_user(cp, &arg32->buf))\n\t\treturn -EFAULT;\n\targ64.buf = compat_ptr(cp);\n\n\tif (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)\n\t\treturn -EAGAIN;\n\terr = hpsa_big_passthru_ioctl(h, &arg64);\n\tatomic_inc(&h->passthru_cmds_avail);\n\tif (err)\n\t\treturn err;\n\tif (copy_to_user(&arg32->error_info, &arg64.error_info,\n\t\t\t sizeof(arg32->error_info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int hpsa_compat_ioctl(struct scsi_device *dev, unsigned int cmd,\n\t\t\t     void __user *arg)\n{\n\tswitch (cmd) {\n\tcase CCISS_GETPCIINFO:\n\tcase CCISS_GETINTINFO:\n\tcase CCISS_SETINTINFO:\n\tcase CCISS_GETNODENAME:\n\tcase CCISS_SETNODENAME:\n\tcase CCISS_GETHEARTBEAT:\n\tcase CCISS_GETBUSTYPES:\n\tcase CCISS_GETFIRMVER:\n\tcase CCISS_GETDRIVVER:\n\tcase CCISS_REVALIDVOLS:\n\tcase CCISS_DEREGDISK:\n\tcase CCISS_REGNEWDISK:\n\tcase CCISS_REGNEWD:\n\tcase CCISS_RESCANDISK:\n\tcase CCISS_GETLUNINFO:\n\t\treturn hpsa_ioctl(dev, cmd, arg);\n\n\tcase CCISS_PASSTHRU32:\n\t\treturn hpsa_ioctl32_passthru(dev, cmd, arg);\n\tcase CCISS_BIG_PASSTHRU32:\n\t\treturn hpsa_ioctl32_big_passthru(dev, cmd, arg);\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\n#endif\n\nstatic int hpsa_getpciinfo_ioctl(struct ctlr_info *h, void __user *argp)\n{\n\tstruct hpsa_pci_info pciinfo;\n\n\tif (!argp)\n\t\treturn -EINVAL;\n\tpciinfo.domain = pci_domain_nr(h->pdev->bus);\n\tpciinfo.bus = h->pdev->bus->number;\n\tpciinfo.dev_fn = h->pdev->devfn;\n\tpciinfo.board_id = h->board_id;\n\tif (copy_to_user(argp, &pciinfo, sizeof(pciinfo)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int hpsa_getdrivver_ioctl(struct ctlr_info *h, void __user *argp)\n{\n\tDriverVer_type DriverVer;\n\tunsigned char vmaj, vmin, vsubmin;\n\tint rc;\n\n\trc = sscanf(HPSA_DRIVER_VERSION, \"%hhu.%hhu.%hhu\",\n\t\t&vmaj, &vmin, &vsubmin);\n\tif (rc != 3) {\n\t\tdev_info(&h->pdev->dev, \"driver version string '%s' \"\n\t\t\t\"unrecognized.\", HPSA_DRIVER_VERSION);\n\t\tvmaj = 0;\n\t\tvmin = 0;\n\t\tvsubmin = 0;\n\t}\n\tDriverVer = (vmaj << 16) | (vmin << 8) | vsubmin;\n\tif (!argp)\n\t\treturn -EINVAL;\n\tif (copy_to_user(argp, &DriverVer, sizeof(DriverVer_type)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int hpsa_passthru_ioctl(struct ctlr_info *h,\n\t\t\t       IOCTL_Command_struct *iocommand)\n{\n\tstruct CommandList *c;\n\tchar *buff = NULL;\n\tu64 temp64;\n\tint rc = 0;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\tif ((iocommand->buf_size < 1) &&\n\t    (iocommand->Request.Type.Direction != XFER_NONE)) {\n\t\treturn -EINVAL;\n\t}\n\tif (iocommand->buf_size > 0) {\n\t\tbuff = kmalloc(iocommand->buf_size, GFP_KERNEL);\n\t\tif (buff == NULL)\n\t\t\treturn -ENOMEM;\n\t\tif (iocommand->Request.Type.Direction & XFER_WRITE) {\n\t\t\t \n\t\t\tif (copy_from_user(buff, iocommand->buf,\n\t\t\t\tiocommand->buf_size)) {\n\t\t\t\trc = -EFAULT;\n\t\t\t\tgoto out_kfree;\n\t\t\t}\n\t\t} else {\n\t\t\tmemset(buff, 0, iocommand->buf_size);\n\t\t}\n\t}\n\tc = cmd_alloc(h);\n\n\t \n\tc->cmd_type = CMD_IOCTL_PEND;\n\tc->scsi_cmd = SCSI_CMD_BUSY;\n\t \n\tc->Header.ReplyQueue = 0;  \n\tif (iocommand->buf_size > 0) {\t \n\t\tc->Header.SGList = 1;\n\t\tc->Header.SGTotal = cpu_to_le16(1);\n\t} else\t{  \n\t\tc->Header.SGList = 0;\n\t\tc->Header.SGTotal = cpu_to_le16(0);\n\t}\n\tmemcpy(&c->Header.LUN, &iocommand->LUN_info, sizeof(c->Header.LUN));\n\n\t \n\tmemcpy(&c->Request, &iocommand->Request,\n\t\tsizeof(c->Request));\n\n\t \n\tif (iocommand->buf_size > 0) {\n\t\ttemp64 = dma_map_single(&h->pdev->dev, buff,\n\t\t\tiocommand->buf_size, DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(&h->pdev->dev, (dma_addr_t) temp64)) {\n\t\t\tc->SG[0].Addr = cpu_to_le64(0);\n\t\t\tc->SG[0].Len = cpu_to_le32(0);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tc->SG[0].Addr = cpu_to_le64(temp64);\n\t\tc->SG[0].Len = cpu_to_le32(iocommand->buf_size);\n\t\tc->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST);  \n\t}\n\trc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE,\n\t\t\t\t\tNO_TIMEOUT);\n\tif (iocommand->buf_size > 0)\n\t\thpsa_pci_unmap(h->pdev, c, 1, DMA_BIDIRECTIONAL);\n\tcheck_ioctl_unit_attention(h, c);\n\tif (rc) {\n\t\trc = -EIO;\n\t\tgoto out;\n\t}\n\n\t \n\tmemcpy(&iocommand->error_info, c->err_info,\n\t\tsizeof(iocommand->error_info));\n\tif ((iocommand->Request.Type.Direction & XFER_READ) &&\n\t\tiocommand->buf_size > 0) {\n\t\t \n\t\tif (copy_to_user(iocommand->buf, buff, iocommand->buf_size)) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tcmd_free(h, c);\nout_kfree:\n\tkfree(buff);\n\treturn rc;\n}\n\nstatic int hpsa_big_passthru_ioctl(struct ctlr_info *h,\n\t\t\t\t   BIG_IOCTL_Command_struct *ioc)\n{\n\tstruct CommandList *c;\n\tunsigned char **buff = NULL;\n\tint *buff_size = NULL;\n\tu64 temp64;\n\tBYTE sg_used = 0;\n\tint status = 0;\n\tu32 left;\n\tu32 sz;\n\tBYTE __user *data_ptr;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\tif ((ioc->buf_size < 1) &&\n\t    (ioc->Request.Type.Direction != XFER_NONE))\n\t\treturn -EINVAL;\n\t \n\tif (ioc->malloc_size > MAX_KMALLOC_SIZE)\n\t\treturn -EINVAL;\n\tif (ioc->buf_size > ioc->malloc_size * SG_ENTRIES_IN_CMD)\n\t\treturn -EINVAL;\n\tbuff = kcalloc(SG_ENTRIES_IN_CMD, sizeof(char *), GFP_KERNEL);\n\tif (!buff) {\n\t\tstatus = -ENOMEM;\n\t\tgoto cleanup1;\n\t}\n\tbuff_size = kmalloc_array(SG_ENTRIES_IN_CMD, sizeof(int), GFP_KERNEL);\n\tif (!buff_size) {\n\t\tstatus = -ENOMEM;\n\t\tgoto cleanup1;\n\t}\n\tleft = ioc->buf_size;\n\tdata_ptr = ioc->buf;\n\twhile (left) {\n\t\tsz = (left > ioc->malloc_size) ? ioc->malloc_size : left;\n\t\tbuff_size[sg_used] = sz;\n\t\tbuff[sg_used] = kmalloc(sz, GFP_KERNEL);\n\t\tif (buff[sg_used] == NULL) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto cleanup1;\n\t\t}\n\t\tif (ioc->Request.Type.Direction & XFER_WRITE) {\n\t\t\tif (copy_from_user(buff[sg_used], data_ptr, sz)) {\n\t\t\t\tstatus = -EFAULT;\n\t\t\t\tgoto cleanup1;\n\t\t\t}\n\t\t} else\n\t\t\tmemset(buff[sg_used], 0, sz);\n\t\tleft -= sz;\n\t\tdata_ptr += sz;\n\t\tsg_used++;\n\t}\n\tc = cmd_alloc(h);\n\n\tc->cmd_type = CMD_IOCTL_PEND;\n\tc->scsi_cmd = SCSI_CMD_BUSY;\n\tc->Header.ReplyQueue = 0;\n\tc->Header.SGList = (u8) sg_used;\n\tc->Header.SGTotal = cpu_to_le16(sg_used);\n\tmemcpy(&c->Header.LUN, &ioc->LUN_info, sizeof(c->Header.LUN));\n\tmemcpy(&c->Request, &ioc->Request, sizeof(c->Request));\n\tif (ioc->buf_size > 0) {\n\t\tint i;\n\t\tfor (i = 0; i < sg_used; i++) {\n\t\t\ttemp64 = dma_map_single(&h->pdev->dev, buff[i],\n\t\t\t\t    buff_size[i], DMA_BIDIRECTIONAL);\n\t\t\tif (dma_mapping_error(&h->pdev->dev,\n\t\t\t\t\t\t\t(dma_addr_t) temp64)) {\n\t\t\t\tc->SG[i].Addr = cpu_to_le64(0);\n\t\t\t\tc->SG[i].Len = cpu_to_le32(0);\n\t\t\t\thpsa_pci_unmap(h->pdev, c, i,\n\t\t\t\t\tDMA_BIDIRECTIONAL);\n\t\t\t\tstatus = -ENOMEM;\n\t\t\t\tgoto cleanup0;\n\t\t\t}\n\t\t\tc->SG[i].Addr = cpu_to_le64(temp64);\n\t\t\tc->SG[i].Len = cpu_to_le32(buff_size[i]);\n\t\t\tc->SG[i].Ext = cpu_to_le32(0);\n\t\t}\n\t\tc->SG[--i].Ext = cpu_to_le32(HPSA_SG_LAST);\n\t}\n\tstatus = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE,\n\t\t\t\t\t\tNO_TIMEOUT);\n\tif (sg_used)\n\t\thpsa_pci_unmap(h->pdev, c, sg_used, DMA_BIDIRECTIONAL);\n\tcheck_ioctl_unit_attention(h, c);\n\tif (status) {\n\t\tstatus = -EIO;\n\t\tgoto cleanup0;\n\t}\n\n\t \n\tmemcpy(&ioc->error_info, c->err_info, sizeof(ioc->error_info));\n\tif ((ioc->Request.Type.Direction & XFER_READ) && ioc->buf_size > 0) {\n\t\tint i;\n\n\t\t \n\t\tBYTE __user *ptr = ioc->buf;\n\t\tfor (i = 0; i < sg_used; i++) {\n\t\t\tif (copy_to_user(ptr, buff[i], buff_size[i])) {\n\t\t\t\tstatus = -EFAULT;\n\t\t\t\tgoto cleanup0;\n\t\t\t}\n\t\t\tptr += buff_size[i];\n\t\t}\n\t}\n\tstatus = 0;\ncleanup0:\n\tcmd_free(h, c);\ncleanup1:\n\tif (buff) {\n\t\tint i;\n\n\t\tfor (i = 0; i < sg_used; i++)\n\t\t\tkfree(buff[i]);\n\t\tkfree(buff);\n\t}\n\tkfree(buff_size);\n\treturn status;\n}\n\nstatic void check_ioctl_unit_attention(struct ctlr_info *h,\n\tstruct CommandList *c)\n{\n\tif (c->err_info->CommandStatus == CMD_TARGET_STATUS &&\n\t\t\tc->err_info->ScsiStatus != SAM_STAT_CHECK_CONDITION)\n\t\t(void) check_for_unit_attention(h, c);\n}\n\n \nstatic int hpsa_ioctl(struct scsi_device *dev, unsigned int cmd,\n\t\t      void __user *argp)\n{\n\tstruct ctlr_info *h = sdev_to_hba(dev);\n\tint rc;\n\n\tswitch (cmd) {\n\tcase CCISS_DEREGDISK:\n\tcase CCISS_REGNEWDISK:\n\tcase CCISS_REGNEWD:\n\t\thpsa_scan_start(h->scsi_host);\n\t\treturn 0;\n\tcase CCISS_GETPCIINFO:\n\t\treturn hpsa_getpciinfo_ioctl(h, argp);\n\tcase CCISS_GETDRIVVER:\n\t\treturn hpsa_getdrivver_ioctl(h, argp);\n\tcase CCISS_PASSTHRU: {\n\t\tIOCTL_Command_struct iocommand;\n\n\t\tif (!argp)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&iocommand, argp, sizeof(iocommand)))\n\t\t\treturn -EFAULT;\n\t\tif (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)\n\t\t\treturn -EAGAIN;\n\t\trc = hpsa_passthru_ioctl(h, &iocommand);\n\t\tatomic_inc(&h->passthru_cmds_avail);\n\t\tif (!rc && copy_to_user(argp, &iocommand, sizeof(iocommand)))\n\t\t\trc = -EFAULT;\n\t\treturn rc;\n\t}\n\tcase CCISS_BIG_PASSTHRU: {\n\t\tBIG_IOCTL_Command_struct ioc;\n\t\tif (!argp)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&ioc, argp, sizeof(ioc)))\n\t\t\treturn -EFAULT;\n\t\tif (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)\n\t\t\treturn -EAGAIN;\n\t\trc = hpsa_big_passthru_ioctl(h, &ioc);\n\t\tatomic_inc(&h->passthru_cmds_avail);\n\t\tif (!rc && copy_to_user(argp, &ioc, sizeof(ioc)))\n\t\t\trc = -EFAULT;\n\t\treturn rc;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}\n\nstatic void hpsa_send_host_reset(struct ctlr_info *h, u8 reset_type)\n{\n\tstruct CommandList *c;\n\n\tc = cmd_alloc(h);\n\n\t \n\t(void) fill_cmd(c, HPSA_DEVICE_RESET_MSG, h, NULL, 0, 0,\n\t\tRAID_CTLR_LUNID, TYPE_MSG);\n\tc->Request.CDB[1] = reset_type;  \n\tc->waiting = NULL;\n\tenqueue_cmd_and_start_io(h, c);\n\t \n\treturn;\n}\n\nstatic int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,\n\tvoid *buff, size_t size, u16 page_code, unsigned char *scsi3addr,\n\tint cmd_type)\n{\n\tenum dma_data_direction dir = DMA_NONE;\n\n\tc->cmd_type = CMD_IOCTL_PEND;\n\tc->scsi_cmd = SCSI_CMD_BUSY;\n\tc->Header.ReplyQueue = 0;\n\tif (buff != NULL && size > 0) {\n\t\tc->Header.SGList = 1;\n\t\tc->Header.SGTotal = cpu_to_le16(1);\n\t} else {\n\t\tc->Header.SGList = 0;\n\t\tc->Header.SGTotal = cpu_to_le16(0);\n\t}\n\tmemcpy(c->Header.LUN.LunAddrBytes, scsi3addr, 8);\n\n\tif (cmd_type == TYPE_CMD) {\n\t\tswitch (cmd) {\n\t\tcase HPSA_INQUIRY:\n\t\t\t \n\t\t\tif (page_code & VPD_PAGE) {\n\t\t\t\tc->Request.CDB[1] = 0x01;\n\t\t\t\tc->Request.CDB[2] = (page_code & 0xff);\n\t\t\t}\n\t\t\tc->Request.CDBLen = 6;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = HPSA_INQUIRY;\n\t\t\tc->Request.CDB[4] = size & 0xFF;\n\t\t\tbreak;\n\t\tcase RECEIVE_DIAGNOSTIC:\n\t\t\tc->Request.CDBLen = 6;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = cmd;\n\t\t\tc->Request.CDB[1] = 1;\n\t\t\tc->Request.CDB[2] = 1;\n\t\t\tc->Request.CDB[3] = (size >> 8) & 0xFF;\n\t\t\tc->Request.CDB[4] = size & 0xFF;\n\t\t\tbreak;\n\t\tcase HPSA_REPORT_LOG:\n\t\tcase HPSA_REPORT_PHYS:\n\t\t\t \n\t\t\tc->Request.CDBLen = 12;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = cmd;\n\t\t\tc->Request.CDB[6] = (size >> 24) & 0xFF;  \n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0xFF;\n\t\t\tc->Request.CDB[9] = size & 0xFF;\n\t\t\tbreak;\n\t\tcase BMIC_SENSE_DIAG_OPTIONS:\n\t\t\tc->Request.CDBLen = 16;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\t \n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[6] = BMIC_SENSE_DIAG_OPTIONS;\n\t\t\tbreak;\n\t\tcase BMIC_SET_DIAG_OPTIONS:\n\t\t\tc->Request.CDBLen = 16;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\t\tTYPE_ATTR_DIR(cmd_type,\n\t\t\t\t\t\tATTR_SIMPLE, XFER_WRITE);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_WRITE;\n\t\t\tc->Request.CDB[6] = BMIC_SET_DIAG_OPTIONS;\n\t\t\tbreak;\n\t\tcase HPSA_CACHE_FLUSH:\n\t\t\tc->Request.CDBLen = 12;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\t\tTYPE_ATTR_DIR(cmd_type,\n\t\t\t\t\t\tATTR_SIMPLE, XFER_WRITE);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_WRITE;\n\t\t\tc->Request.CDB[6] = BMIC_CACHE_FLUSH;\n\t\t\tc->Request.CDB[7] = (size >> 8) & 0xFF;\n\t\t\tc->Request.CDB[8] = size & 0xFF;\n\t\t\tbreak;\n\t\tcase TEST_UNIT_READY:\n\t\t\tc->Request.CDBLen = 6;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tbreak;\n\t\tcase HPSA_GET_RAID_MAP:\n\t\t\tc->Request.CDBLen = 12;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = HPSA_CISS_READ;\n\t\t\tc->Request.CDB[1] = cmd;\n\t\t\tc->Request.CDB[6] = (size >> 24) & 0xFF;  \n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0xFF;\n\t\t\tc->Request.CDB[9] = size & 0xFF;\n\t\t\tbreak;\n\t\tcase BMIC_SENSE_CONTROLLER_PARAMETERS:\n\t\t\tc->Request.CDBLen = 10;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[6] = BMIC_SENSE_CONTROLLER_PARAMETERS;\n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0xFF;\n\t\t\tbreak;\n\t\tcase BMIC_IDENTIFY_PHYSICAL_DEVICE:\n\t\t\tc->Request.CDBLen = 10;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[6] = BMIC_IDENTIFY_PHYSICAL_DEVICE;\n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0XFF;\n\t\t\tbreak;\n\t\tcase BMIC_SENSE_SUBSYSTEM_INFORMATION:\n\t\t\tc->Request.CDBLen = 10;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[6] = BMIC_SENSE_SUBSYSTEM_INFORMATION;\n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0XFF;\n\t\t\tbreak;\n\t\tcase BMIC_SENSE_STORAGE_BOX_PARAMS:\n\t\t\tc->Request.CDBLen = 10;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[6] = BMIC_SENSE_STORAGE_BOX_PARAMS;\n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0XFF;\n\t\t\tbreak;\n\t\tcase BMIC_IDENTIFY_CONTROLLER:\n\t\t\tc->Request.CDBLen = 10;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);\n\t\t\tc->Request.Timeout = 0;\n\t\t\tc->Request.CDB[0] = BMIC_READ;\n\t\t\tc->Request.CDB[1] = 0;\n\t\t\tc->Request.CDB[2] = 0;\n\t\t\tc->Request.CDB[3] = 0;\n\t\t\tc->Request.CDB[4] = 0;\n\t\t\tc->Request.CDB[5] = 0;\n\t\t\tc->Request.CDB[6] = BMIC_IDENTIFY_CONTROLLER;\n\t\t\tc->Request.CDB[7] = (size >> 16) & 0xFF;\n\t\t\tc->Request.CDB[8] = (size >> 8) & 0XFF;\n\t\t\tc->Request.CDB[9] = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&h->pdev->dev, \"unknown command 0x%c\\n\", cmd);\n\t\t\tBUG();\n\t\t}\n\t} else if (cmd_type == TYPE_MSG) {\n\t\tswitch (cmd) {\n\n\t\tcase  HPSA_PHYS_TARGET_RESET:\n\t\t\tc->Request.CDBLen = 16;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);\n\t\t\tc->Request.Timeout = 0;  \n\t\t\tmemset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));\n\t\t\tc->Request.CDB[0] = HPSA_RESET;\n\t\t\tc->Request.CDB[1] = HPSA_TARGET_RESET_TYPE;\n\t\t\t \n\t\t\tc->Request.CDB[4] = 0x00;\n\t\t\tc->Request.CDB[5] = 0x00;\n\t\t\tc->Request.CDB[6] = 0x00;\n\t\t\tc->Request.CDB[7] = 0x00;\n\t\t\tbreak;\n\t\tcase  HPSA_DEVICE_RESET_MSG:\n\t\t\tc->Request.CDBLen = 16;\n\t\t\tc->Request.type_attr_dir =\n\t\t\t\tTYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);\n\t\t\tc->Request.Timeout = 0;  \n\t\t\tmemset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));\n\t\t\tc->Request.CDB[0] =  cmd;\n\t\t\tc->Request.CDB[1] = HPSA_RESET_TYPE_LUN;\n\t\t\t \n\t\t\t \n\t\t\tc->Request.CDB[4] = 0x00;\n\t\t\tc->Request.CDB[5] = 0x00;\n\t\t\tc->Request.CDB[6] = 0x00;\n\t\t\tc->Request.CDB[7] = 0x00;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&h->pdev->dev, \"unknown message type %d\\n\",\n\t\t\t\tcmd);\n\t\t\tBUG();\n\t\t}\n\t} else {\n\t\tdev_warn(&h->pdev->dev, \"unknown command type %d\\n\", cmd_type);\n\t\tBUG();\n\t}\n\n\tswitch (GET_DIR(c->Request.type_attr_dir)) {\n\tcase XFER_READ:\n\t\tdir = DMA_FROM_DEVICE;\n\t\tbreak;\n\tcase XFER_WRITE:\n\t\tdir = DMA_TO_DEVICE;\n\t\tbreak;\n\tcase XFER_NONE:\n\t\tdir = DMA_NONE;\n\t\tbreak;\n\tdefault:\n\t\tdir = DMA_BIDIRECTIONAL;\n\t}\n\tif (hpsa_map_one(h->pdev, c, buff, size, dir))\n\t\treturn -1;\n\treturn 0;\n}\n\n \nstatic void __iomem *remap_pci_mem(ulong base, ulong size)\n{\n\tulong page_base = ((ulong) base) & PAGE_MASK;\n\tulong page_offs = ((ulong) base) - page_base;\n\tvoid __iomem *page_remapped = ioremap(page_base,\n\t\tpage_offs + size);\n\n\treturn page_remapped ? (page_remapped + page_offs) : NULL;\n}\n\nstatic inline unsigned long get_next_completion(struct ctlr_info *h, u8 q)\n{\n\treturn h->access.command_completed(h, q);\n}\n\nstatic inline bool interrupt_pending(struct ctlr_info *h)\n{\n\treturn h->access.intr_pending(h);\n}\n\nstatic inline long interrupt_not_for_us(struct ctlr_info *h)\n{\n\treturn (h->access.intr_pending(h) == 0) ||\n\t\t(h->interrupts_enabled == 0);\n}\n\nstatic inline int bad_tag(struct ctlr_info *h, u32 tag_index,\n\tu32 raw_tag)\n{\n\tif (unlikely(tag_index >= h->nr_cmds)) {\n\t\tdev_warn(&h->pdev->dev, \"bad tag 0x%08x ignored.\\n\", raw_tag);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline void finish_cmd(struct CommandList *c)\n{\n\tdial_up_lockup_detection_on_fw_flash_complete(c->h, c);\n\tif (likely(c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_SCSI\n\t\t\t|| c->cmd_type == CMD_IOACCEL2))\n\t\tcomplete_scsi_command(c);\n\telse if (c->cmd_type == CMD_IOCTL_PEND || c->cmd_type == IOACCEL2_TMF)\n\t\tcomplete(c->waiting);\n}\n\n \nstatic inline void process_indexed_cmd(struct ctlr_info *h,\n\tu32 raw_tag)\n{\n\tu32 tag_index;\n\tstruct CommandList *c;\n\n\ttag_index = raw_tag >> DIRECT_LOOKUP_SHIFT;\n\tif (!bad_tag(h, tag_index, raw_tag)) {\n\t\tc = h->cmd_pool + tag_index;\n\t\tfinish_cmd(c);\n\t}\n}\n\n \nstatic int ignore_bogus_interrupt(struct ctlr_info *h)\n{\n\tif (likely(!reset_devices))\n\t\treturn 0;\n\n\tif (likely(h->interrupts_enabled))\n\t\treturn 0;\n\n\tdev_info(&h->pdev->dev, \"Received interrupt while interrupts disabled \"\n\t\t\"(known firmware bug.)  Ignoring.\\n\");\n\n\treturn 1;\n}\n\n \nstatic struct ctlr_info *queue_to_hba(u8 *queue)\n{\n\treturn container_of((queue - *queue), struct ctlr_info, q[0]);\n}\n\nstatic irqreturn_t hpsa_intx_discard_completions(int irq, void *queue)\n{\n\tstruct ctlr_info *h = queue_to_hba(queue);\n\tu8 q = *(u8 *) queue;\n\tu32 raw_tag;\n\n\tif (ignore_bogus_interrupt(h))\n\t\treturn IRQ_NONE;\n\n\tif (interrupt_not_for_us(h))\n\t\treturn IRQ_NONE;\n\th->last_intr_timestamp = get_jiffies_64();\n\twhile (interrupt_pending(h)) {\n\t\traw_tag = get_next_completion(h, q);\n\t\twhile (raw_tag != FIFO_EMPTY)\n\t\t\traw_tag = next_command(h, q);\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t hpsa_msix_discard_completions(int irq, void *queue)\n{\n\tstruct ctlr_info *h = queue_to_hba(queue);\n\tu32 raw_tag;\n\tu8 q = *(u8 *) queue;\n\n\tif (ignore_bogus_interrupt(h))\n\t\treturn IRQ_NONE;\n\n\th->last_intr_timestamp = get_jiffies_64();\n\traw_tag = get_next_completion(h, q);\n\twhile (raw_tag != FIFO_EMPTY)\n\t\traw_tag = next_command(h, q);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t do_hpsa_intr_intx(int irq, void *queue)\n{\n\tstruct ctlr_info *h = queue_to_hba((u8 *) queue);\n\tu32 raw_tag;\n\tu8 q = *(u8 *) queue;\n\n\tif (interrupt_not_for_us(h))\n\t\treturn IRQ_NONE;\n\th->last_intr_timestamp = get_jiffies_64();\n\twhile (interrupt_pending(h)) {\n\t\traw_tag = get_next_completion(h, q);\n\t\twhile (raw_tag != FIFO_EMPTY) {\n\t\t\tprocess_indexed_cmd(h, raw_tag);\n\t\t\traw_tag = next_command(h, q);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t do_hpsa_intr_msi(int irq, void *queue)\n{\n\tstruct ctlr_info *h = queue_to_hba(queue);\n\tu32 raw_tag;\n\tu8 q = *(u8 *) queue;\n\n\th->last_intr_timestamp = get_jiffies_64();\n\traw_tag = get_next_completion(h, q);\n\twhile (raw_tag != FIFO_EMPTY) {\n\t\tprocess_indexed_cmd(h, raw_tag);\n\t\traw_tag = next_command(h, q);\n\t}\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int hpsa_message(struct pci_dev *pdev, unsigned char opcode,\n\t\t\tunsigned char type)\n{\n\tstruct Command {\n\t\tstruct CommandListHeader CommandHeader;\n\t\tstruct RequestBlock Request;\n\t\tstruct ErrDescriptor ErrorDescriptor;\n\t};\n\tstruct Command *cmd;\n\tstatic const size_t cmd_sz = sizeof(*cmd) +\n\t\t\t\t\tsizeof(cmd->ErrorDescriptor);\n\tdma_addr_t paddr64;\n\t__le32 paddr32;\n\tu32 tag;\n\tvoid __iomem *vaddr;\n\tint i, err;\n\n\tvaddr = pci_ioremap_bar(pdev, 0);\n\tif (vaddr == NULL)\n\t\treturn -ENOMEM;\n\n\t \n\terr = dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));\n\tif (err) {\n\t\tiounmap(vaddr);\n\t\treturn err;\n\t}\n\n\tcmd = dma_alloc_coherent(&pdev->dev, cmd_sz, &paddr64, GFP_KERNEL);\n\tif (cmd == NULL) {\n\t\tiounmap(vaddr);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tpaddr32 = cpu_to_le32(paddr64);\n\n\tcmd->CommandHeader.ReplyQueue = 0;\n\tcmd->CommandHeader.SGList = 0;\n\tcmd->CommandHeader.SGTotal = cpu_to_le16(0);\n\tcmd->CommandHeader.tag = cpu_to_le64(paddr64);\n\tmemset(&cmd->CommandHeader.LUN.LunAddrBytes, 0, 8);\n\n\tcmd->Request.CDBLen = 16;\n\tcmd->Request.type_attr_dir =\n\t\t\tTYPE_ATTR_DIR(TYPE_MSG, ATTR_HEADOFQUEUE, XFER_NONE);\n\tcmd->Request.Timeout = 0;  \n\tcmd->Request.CDB[0] = opcode;\n\tcmd->Request.CDB[1] = type;\n\tmemset(&cmd->Request.CDB[2], 0, 14);  \n\tcmd->ErrorDescriptor.Addr =\n\t\t\tcpu_to_le64((le32_to_cpu(paddr32) + sizeof(*cmd)));\n\tcmd->ErrorDescriptor.Len = cpu_to_le32(sizeof(struct ErrorInfo));\n\n\twritel(le32_to_cpu(paddr32), vaddr + SA5_REQUEST_PORT_OFFSET);\n\n\tfor (i = 0; i < HPSA_MSG_SEND_RETRY_LIMIT; i++) {\n\t\ttag = readl(vaddr + SA5_REPLY_PORT_OFFSET);\n\t\tif ((tag & ~HPSA_SIMPLE_ERROR_BITS) == paddr64)\n\t\t\tbreak;\n\t\tmsleep(HPSA_MSG_SEND_RETRY_INTERVAL_MSECS);\n\t}\n\n\tiounmap(vaddr);\n\n\t \n\tif (i == HPSA_MSG_SEND_RETRY_LIMIT) {\n\t\tdev_err(&pdev->dev, \"controller message %02x:%02x timed out\\n\",\n\t\t\topcode, type);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tdma_free_coherent(&pdev->dev, cmd_sz, cmd, paddr64);\n\n\tif (tag & HPSA_ERROR_BIT) {\n\t\tdev_err(&pdev->dev, \"controller message %02x:%02x failed\\n\",\n\t\t\topcode, type);\n\t\treturn -EIO;\n\t}\n\n\tdev_info(&pdev->dev, \"controller message %02x:%02x succeeded\\n\",\n\t\topcode, type);\n\treturn 0;\n}\n\n#define hpsa_noop(p) hpsa_message(p, 3, 0)\n\nstatic int hpsa_controller_hard_reset(struct pci_dev *pdev,\n\tvoid __iomem *vaddr, u32 use_doorbell)\n{\n\n\tif (use_doorbell) {\n\t\t \n\t\tdev_info(&pdev->dev, \"using doorbell to reset controller\\n\");\n\t\twritel(use_doorbell, vaddr + SA5_DOORBELL);\n\n\t\t \n\t\tmsleep(10000);\n\t} else {  \n\n\t\t \n\n\t\tint rc = 0;\n\n\t\tdev_info(&pdev->dev, \"using PCI PM to reset controller\\n\");\n\n\t\t \n\t\trc = pci_set_power_state(pdev, PCI_D3hot);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tmsleep(500);\n\n\t\t \n\t\trc = pci_set_power_state(pdev, PCI_D0);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\tmsleep(500);\n\t}\n\treturn 0;\n}\n\nstatic void init_driver_version(char *driver_version, int len)\n{\n\tmemset(driver_version, 0, len);\n\tstrncpy(driver_version, HPSA \" \" HPSA_DRIVER_VERSION, len - 1);\n}\n\nstatic int write_driver_ver_to_cfgtable(struct CfgTable __iomem *cfgtable)\n{\n\tchar *driver_version;\n\tint i, size = sizeof(cfgtable->driver_version);\n\n\tdriver_version = kmalloc(size, GFP_KERNEL);\n\tif (!driver_version)\n\t\treturn -ENOMEM;\n\n\tinit_driver_version(driver_version, size);\n\tfor (i = 0; i < size; i++)\n\t\twriteb(driver_version[i], &cfgtable->driver_version[i]);\n\tkfree(driver_version);\n\treturn 0;\n}\n\nstatic void read_driver_ver_from_cfgtable(struct CfgTable __iomem *cfgtable,\n\t\t\t\t\t  unsigned char *driver_ver)\n{\n\tint i;\n\n\tfor (i = 0; i < sizeof(cfgtable->driver_version); i++)\n\t\tdriver_ver[i] = readb(&cfgtable->driver_version[i]);\n}\n\nstatic int controller_reset_failed(struct CfgTable __iomem *cfgtable)\n{\n\n\tchar *driver_ver, *old_driver_ver;\n\tint rc, size = sizeof(cfgtable->driver_version);\n\n\told_driver_ver = kmalloc_array(2, size, GFP_KERNEL);\n\tif (!old_driver_ver)\n\t\treturn -ENOMEM;\n\tdriver_ver = old_driver_ver + size;\n\n\t \n\tinit_driver_version(old_driver_ver, size);\n\tread_driver_ver_from_cfgtable(cfgtable, driver_ver);\n\trc = !memcmp(driver_ver, old_driver_ver, size);\n\tkfree(old_driver_ver);\n\treturn rc;\n}\n \nstatic int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev, u32 board_id)\n{\n\tu64 cfg_offset;\n\tu32 cfg_base_addr;\n\tu64 cfg_base_addr_index;\n\tvoid __iomem *vaddr;\n\tunsigned long paddr;\n\tu32 misc_fw_support;\n\tint rc;\n\tstruct CfgTable __iomem *cfgtable;\n\tu32 use_doorbell;\n\tu16 command_register;\n\n\t \n\n\tif (!ctlr_is_resettable(board_id)) {\n\t\tdev_warn(&pdev->dev, \"Controller not resettable\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tif (!ctlr_is_hard_resettable(board_id))\n\t\treturn -ENOTSUPP;  \n\n\t \n\tpci_read_config_word(pdev, 4, &command_register);\n\tpci_save_state(pdev);\n\n\t \n\trc = hpsa_pci_find_memory_BAR(pdev, &paddr);\n\tif (rc)\n\t\treturn rc;\n\tvaddr = remap_pci_mem(paddr, 0x250);\n\tif (!vaddr)\n\t\treturn -ENOMEM;\n\n\t \n\trc = hpsa_find_cfg_addrs(pdev, vaddr, &cfg_base_addr,\n\t\t\t\t\t&cfg_base_addr_index, &cfg_offset);\n\tif (rc)\n\t\tgoto unmap_vaddr;\n\tcfgtable = remap_pci_mem(pci_resource_start(pdev,\n\t\t       cfg_base_addr_index) + cfg_offset, sizeof(*cfgtable));\n\tif (!cfgtable) {\n\t\trc = -ENOMEM;\n\t\tgoto unmap_vaddr;\n\t}\n\trc = write_driver_ver_to_cfgtable(cfgtable);\n\tif (rc)\n\t\tgoto unmap_cfgtable;\n\n\t \n\tmisc_fw_support = readl(&cfgtable->misc_fw_support);\n\tuse_doorbell = misc_fw_support & MISC_FW_DOORBELL_RESET2;\n\tif (use_doorbell) {\n\t\tuse_doorbell = DOORBELL_CTLR_RESET2;\n\t} else {\n\t\tuse_doorbell = misc_fw_support & MISC_FW_DOORBELL_RESET;\n\t\tif (use_doorbell) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t\"Soft reset not supported. Firmware update is required.\\n\");\n\t\t\trc = -ENOTSUPP;  \n\t\t\tgoto unmap_cfgtable;\n\t\t}\n\t}\n\n\trc = hpsa_controller_hard_reset(pdev, vaddr, use_doorbell);\n\tif (rc)\n\t\tgoto unmap_cfgtable;\n\n\tpci_restore_state(pdev);\n\tpci_write_config_word(pdev, 4, command_register);\n\n\t \n\tmsleep(HPSA_POST_RESET_PAUSE_MSECS);\n\n\trc = hpsa_wait_for_board_state(pdev, vaddr, BOARD_READY);\n\tif (rc) {\n\t\tdev_warn(&pdev->dev,\n\t\t\t\"Failed waiting for board to become ready after hard reset\\n\");\n\t\tgoto unmap_cfgtable;\n\t}\n\n\trc = controller_reset_failed(vaddr);\n\tif (rc < 0)\n\t\tgoto unmap_cfgtable;\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"Unable to successfully reset \"\n\t\t\t\"controller. Will try soft reset.\\n\");\n\t\trc = -ENOTSUPP;\n\t} else {\n\t\tdev_info(&pdev->dev, \"board ready after hard reset.\\n\");\n\t}\n\nunmap_cfgtable:\n\tiounmap(cfgtable);\n\nunmap_vaddr:\n\tiounmap(vaddr);\n\treturn rc;\n}\n\n \nstatic void print_cfg_table(struct device *dev, struct CfgTable __iomem *tb)\n{\n#ifdef HPSA_DEBUG\n\tint i;\n\tchar temp_name[17];\n\n\tdev_info(dev, \"Controller Configuration information\\n\");\n\tdev_info(dev, \"------------------------------------\\n\");\n\tfor (i = 0; i < 4; i++)\n\t\ttemp_name[i] = readb(&(tb->Signature[i]));\n\ttemp_name[4] = '\\0';\n\tdev_info(dev, \"   Signature = %s\\n\", temp_name);\n\tdev_info(dev, \"   Spec Number = %d\\n\", readl(&(tb->SpecValence)));\n\tdev_info(dev, \"   Transport methods supported = 0x%x\\n\",\n\t       readl(&(tb->TransportSupport)));\n\tdev_info(dev, \"   Transport methods active = 0x%x\\n\",\n\t       readl(&(tb->TransportActive)));\n\tdev_info(dev, \"   Requested transport Method = 0x%x\\n\",\n\t       readl(&(tb->HostWrite.TransportRequest)));\n\tdev_info(dev, \"   Coalesce Interrupt Delay = 0x%x\\n\",\n\t       readl(&(tb->HostWrite.CoalIntDelay)));\n\tdev_info(dev, \"   Coalesce Interrupt Count = 0x%x\\n\",\n\t       readl(&(tb->HostWrite.CoalIntCount)));\n\tdev_info(dev, \"   Max outstanding commands = %d\\n\",\n\t       readl(&(tb->CmdsOutMax)));\n\tdev_info(dev, \"   Bus Types = 0x%x\\n\", readl(&(tb->BusTypes)));\n\tfor (i = 0; i < 16; i++)\n\t\ttemp_name[i] = readb(&(tb->ServerName[i]));\n\ttemp_name[16] = '\\0';\n\tdev_info(dev, \"   Server Name = %s\\n\", temp_name);\n\tdev_info(dev, \"   Heartbeat Counter = 0x%x\\n\\n\\n\",\n\t\treadl(&(tb->HeartBeat)));\n#endif\t\t\t\t \n}\n\nstatic int find_PCI_BAR_index(struct pci_dev *pdev, unsigned long pci_bar_addr)\n{\n\tint i, offset, mem_type, bar_type;\n\n\tif (pci_bar_addr == PCI_BASE_ADDRESS_0)\t \n\t\treturn 0;\n\toffset = 0;\n\tfor (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {\n\t\tbar_type = pci_resource_flags(pdev, i) & PCI_BASE_ADDRESS_SPACE;\n\t\tif (bar_type == PCI_BASE_ADDRESS_SPACE_IO)\n\t\t\toffset += 4;\n\t\telse {\n\t\t\tmem_type = pci_resource_flags(pdev, i) &\n\t\t\t    PCI_BASE_ADDRESS_MEM_TYPE_MASK;\n\t\t\tswitch (mem_type) {\n\t\t\tcase PCI_BASE_ADDRESS_MEM_TYPE_32:\n\t\t\tcase PCI_BASE_ADDRESS_MEM_TYPE_1M:\n\t\t\t\toffset += 4;\t \n\t\t\t\tbreak;\n\t\t\tcase PCI_BASE_ADDRESS_MEM_TYPE_64:\n\t\t\t\toffset += 8;\n\t\t\t\tbreak;\n\t\t\tdefault:\t \n\t\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t       \"base address is invalid\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t\tif (offset == pci_bar_addr - PCI_BASE_ADDRESS_0)\n\t\t\treturn i + 1;\n\t}\n\treturn -1;\n}\n\nstatic void hpsa_disable_interrupt_mode(struct ctlr_info *h)\n{\n\tpci_free_irq_vectors(h->pdev);\n\th->msix_vectors = 0;\n}\n\nstatic void hpsa_setup_reply_map(struct ctlr_info *h)\n{\n\tconst struct cpumask *mask;\n\tunsigned int queue, cpu;\n\n\tfor (queue = 0; queue < h->msix_vectors; queue++) {\n\t\tmask = pci_irq_get_affinity(h->pdev, queue);\n\t\tif (!mask)\n\t\t\tgoto fallback;\n\n\t\tfor_each_cpu(cpu, mask)\n\t\t\th->reply_map[cpu] = queue;\n\t}\n\treturn;\n\nfallback:\n\tfor_each_possible_cpu(cpu)\n\t\th->reply_map[cpu] = 0;\n}\n\n \nstatic int hpsa_interrupt_mode(struct ctlr_info *h)\n{\n\tunsigned int flags = PCI_IRQ_LEGACY;\n\tint ret;\n\n\t \n\tswitch (h->board_id) {\n\tcase 0x40700E11:\n\tcase 0x40800E11:\n\tcase 0x40820E11:\n\tcase 0x40830E11:\n\t\tbreak;\n\tdefault:\n\t\tret = pci_alloc_irq_vectors(h->pdev, 1, MAX_REPLY_QUEUES,\n\t\t\t\tPCI_IRQ_MSIX | PCI_IRQ_AFFINITY);\n\t\tif (ret > 0) {\n\t\t\th->msix_vectors = ret;\n\t\t\treturn 0;\n\t\t}\n\n\t\tflags |= PCI_IRQ_MSI;\n\t\tbreak;\n\t}\n\n\tret = pci_alloc_irq_vectors(h->pdev, 1, 1, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id,\n\t\t\t\tbool *legacy_board)\n{\n\tint i;\n\tu32 subsystem_vendor_id, subsystem_device_id;\n\n\tsubsystem_vendor_id = pdev->subsystem_vendor;\n\tsubsystem_device_id = pdev->subsystem_device;\n\t*board_id = ((subsystem_device_id << 16) & 0xffff0000) |\n\t\t    subsystem_vendor_id;\n\n\tif (legacy_board)\n\t\t*legacy_board = false;\n\tfor (i = 0; i < ARRAY_SIZE(products); i++)\n\t\tif (*board_id == products[i].board_id) {\n\t\t\tif (products[i].access != &SA5A_access &&\n\t\t\t    products[i].access != &SA5B_access)\n\t\t\t\treturn i;\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"legacy board ID: 0x%08x\\n\",\n\t\t\t\t *board_id);\n\t\t\tif (legacy_board)\n\t\t\t    *legacy_board = true;\n\t\t\treturn i;\n\t\t}\n\n\tdev_warn(&pdev->dev, \"unrecognized board ID: 0x%08x\\n\", *board_id);\n\tif (legacy_board)\n\t\t*legacy_board = true;\n\treturn ARRAY_SIZE(products) - 1;  \n}\n\nstatic int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,\n\t\t\t\t    unsigned long *memory_bar)\n{\n\tint i;\n\n\tfor (i = 0; i < DEVICE_COUNT_RESOURCE; i++)\n\t\tif (pci_resource_flags(pdev, i) & IORESOURCE_MEM) {\n\t\t\t \n\t\t\t*memory_bar = pci_resource_start(pdev, i);\n\t\t\tdev_dbg(&pdev->dev, \"memory BAR = %lx\\n\",\n\t\t\t\t*memory_bar);\n\t\t\treturn 0;\n\t\t}\n\tdev_warn(&pdev->dev, \"no memory BAR found\\n\");\n\treturn -ENODEV;\n}\n\nstatic int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,\n\t\t\t\t     int wait_for_ready)\n{\n\tint i, iterations;\n\tu32 scratchpad;\n\tif (wait_for_ready)\n\t\titerations = HPSA_BOARD_READY_ITERATIONS;\n\telse\n\t\titerations = HPSA_BOARD_NOT_READY_ITERATIONS;\n\n\tfor (i = 0; i < iterations; i++) {\n\t\tscratchpad = readl(vaddr + SA5_SCRATCHPAD_OFFSET);\n\t\tif (wait_for_ready) {\n\t\t\tif (scratchpad == HPSA_FIRMWARE_READY)\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (scratchpad != HPSA_FIRMWARE_READY)\n\t\t\t\treturn 0;\n\t\t}\n\t\tmsleep(HPSA_BOARD_READY_POLL_INTERVAL_MSECS);\n\t}\n\tdev_warn(&pdev->dev, \"board not ready, timed out.\\n\");\n\treturn -ENODEV;\n}\n\nstatic int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,\n\t\t\t       u32 *cfg_base_addr, u64 *cfg_base_addr_index,\n\t\t\t       u64 *cfg_offset)\n{\n\t*cfg_base_addr = readl(vaddr + SA5_CTCFG_OFFSET);\n\t*cfg_offset = readl(vaddr + SA5_CTMEM_OFFSET);\n\t*cfg_base_addr &= (u32) 0x0000ffff;\n\t*cfg_base_addr_index = find_PCI_BAR_index(pdev, *cfg_base_addr);\n\tif (*cfg_base_addr_index == -1) {\n\t\tdev_warn(&pdev->dev, \"cannot find cfg_base_addr_index\\n\");\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic void hpsa_free_cfgtables(struct ctlr_info *h)\n{\n\tif (h->transtable) {\n\t\tiounmap(h->transtable);\n\t\th->transtable = NULL;\n\t}\n\tif (h->cfgtable) {\n\t\tiounmap(h->cfgtable);\n\t\th->cfgtable = NULL;\n\t}\n}\n\n \nstatic int hpsa_find_cfgtables(struct ctlr_info *h)\n{\n\tu64 cfg_offset;\n\tu32 cfg_base_addr;\n\tu64 cfg_base_addr_index;\n\tu32 trans_offset;\n\tint rc;\n\n\trc = hpsa_find_cfg_addrs(h->pdev, h->vaddr, &cfg_base_addr,\n\t\t&cfg_base_addr_index, &cfg_offset);\n\tif (rc)\n\t\treturn rc;\n\th->cfgtable = remap_pci_mem(pci_resource_start(h->pdev,\n\t\t       cfg_base_addr_index) + cfg_offset, sizeof(*h->cfgtable));\n\tif (!h->cfgtable) {\n\t\tdev_err(&h->pdev->dev, \"Failed mapping cfgtable\\n\");\n\t\treturn -ENOMEM;\n\t}\n\trc = write_driver_ver_to_cfgtable(h->cfgtable);\n\tif (rc)\n\t\treturn rc;\n\t \n\ttrans_offset = readl(&h->cfgtable->TransMethodOffset);\n\th->transtable = remap_pci_mem(pci_resource_start(h->pdev,\n\t\t\t\tcfg_base_addr_index)+cfg_offset+trans_offset,\n\t\t\t\tsizeof(*h->transtable));\n\tif (!h->transtable) {\n\t\tdev_err(&h->pdev->dev, \"Failed mapping transfer table\\n\");\n\t\thpsa_free_cfgtables(h);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)\n{\n#define MIN_MAX_COMMANDS 16\n\tBUILD_BUG_ON(MIN_MAX_COMMANDS <= HPSA_NRESERVED_CMDS);\n\n\th->max_commands = readl(&h->cfgtable->MaxPerformantModeCommands);\n\n\t \n\tif (reset_devices && h->max_commands > 32)\n\t\th->max_commands = 32;\n\n\tif (h->max_commands < MIN_MAX_COMMANDS) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"Controller reports max supported commands of %d Using %d instead. Ensure that firmware is up to date.\\n\",\n\t\t\th->max_commands,\n\t\t\tMIN_MAX_COMMANDS);\n\t\th->max_commands = MIN_MAX_COMMANDS;\n\t}\n}\n\n \nstatic int hpsa_supports_chained_sg_blocks(struct ctlr_info *h)\n{\n\treturn h->maxsgentries > 512;\n}\n\n \nstatic void hpsa_find_board_params(struct ctlr_info *h)\n{\n\thpsa_get_max_perf_mode_cmds(h);\n\th->nr_cmds = h->max_commands;\n\th->maxsgentries = readl(&(h->cfgtable->MaxScatterGatherElements));\n\th->fw_support = readl(&(h->cfgtable->misc_fw_support));\n\tif (hpsa_supports_chained_sg_blocks(h)) {\n\t\t \n\t\th->max_cmd_sg_entries = 32;\n\t\th->chainsize = h->maxsgentries - h->max_cmd_sg_entries;\n\t\th->maxsgentries--;  \n\t} else {\n\t\t \n\t\th->max_cmd_sg_entries = 31;\n\t\th->maxsgentries = 31;  \n\t\th->chainsize = 0;\n\t}\n\n\t \n\th->TMFSupportFlags = readl(&(h->cfgtable->TMFSupportFlags));\n\tif (!(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags))\n\t\tdev_warn(&h->pdev->dev, \"Physical aborts not supported\\n\");\n\tif (!(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags))\n\t\tdev_warn(&h->pdev->dev, \"Logical aborts not supported\\n\");\n\tif (!(HPSATMF_IOACCEL_ENABLED & h->TMFSupportFlags))\n\t\tdev_warn(&h->pdev->dev, \"HP SSD Smart Path aborts not supported\\n\");\n}\n\nstatic inline bool hpsa_CISS_signature_present(struct ctlr_info *h)\n{\n\tif (!check_signature(h->cfgtable->Signature, \"CISS\", 4)) {\n\t\tdev_err(&h->pdev->dev, \"not a valid CISS config table\\n\");\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic inline void hpsa_set_driver_support_bits(struct ctlr_info *h)\n{\n\tu32 driver_support;\n\n\tdriver_support = readl(&(h->cfgtable->driver_support));\n\t \n#ifdef CONFIG_X86\n\tdriver_support |= ENABLE_SCSI_PREFETCH;\n#endif\n\tdriver_support |= ENABLE_UNIT_ATTN;\n\twritel(driver_support, &(h->cfgtable->driver_support));\n}\n\n \nstatic inline void hpsa_p600_dma_prefetch_quirk(struct ctlr_info *h)\n{\n\tu32 dma_prefetch;\n\n\tif (h->board_id != 0x3225103C)\n\t\treturn;\n\tdma_prefetch = readl(h->vaddr + I2O_DMA1_CFG);\n\tdma_prefetch |= 0x8000;\n\twritel(dma_prefetch, h->vaddr + I2O_DMA1_CFG);\n}\n\nstatic int hpsa_wait_for_clear_event_notify_ack(struct ctlr_info *h)\n{\n\tint i;\n\tu32 doorbell_value;\n\tunsigned long flags;\n\t \n\tfor (i = 0; i < MAX_CLEAR_EVENT_WAIT; i++) {\n\t\tspin_lock_irqsave(&h->lock, flags);\n\t\tdoorbell_value = readl(h->vaddr + SA5_DOORBELL);\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\tif (!(doorbell_value & DOORBELL_CLEAR_EVENTS))\n\t\t\tgoto done;\n\t\t \n\t\tmsleep(CLEAR_EVENT_WAIT_INTERVAL);\n\t}\n\treturn -ENODEV;\ndone:\n\treturn 0;\n}\n\nstatic int hpsa_wait_for_mode_change_ack(struct ctlr_info *h)\n{\n\tint i;\n\tu32 doorbell_value;\n\tunsigned long flags;\n\n\t \n\tfor (i = 0; i < MAX_MODE_CHANGE_WAIT; i++) {\n\t\tif (h->remove_in_progress)\n\t\t\tgoto done;\n\t\tspin_lock_irqsave(&h->lock, flags);\n\t\tdoorbell_value = readl(h->vaddr + SA5_DOORBELL);\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\tif (!(doorbell_value & CFGTBL_ChangeReq))\n\t\t\tgoto done;\n\t\t \n\t\tmsleep(MODE_CHANGE_WAIT_INTERVAL);\n\t}\n\treturn -ENODEV;\ndone:\n\treturn 0;\n}\n\n \nstatic int hpsa_enter_simple_mode(struct ctlr_info *h)\n{\n\tu32 trans_support;\n\n\ttrans_support = readl(&(h->cfgtable->TransportSupport));\n\tif (!(trans_support & SIMPLE_MODE))\n\t\treturn -ENOTSUPP;\n\n\th->max_commands = readl(&(h->cfgtable->CmdsOutMax));\n\n\t \n\twritel(CFGTBL_Trans_Simple, &(h->cfgtable->HostWrite.TransportRequest));\n\twritel(0, &h->cfgtable->HostWrite.command_pool_addr_hi);\n\twritel(CFGTBL_ChangeReq, h->vaddr + SA5_DOORBELL);\n\tif (hpsa_wait_for_mode_change_ack(h))\n\t\tgoto error;\n\tprint_cfg_table(&h->pdev->dev, h->cfgtable);\n\tif (!(readl(&(h->cfgtable->TransportActive)) & CFGTBL_Trans_Simple))\n\t\tgoto error;\n\th->transMethod = CFGTBL_Trans_Simple;\n\treturn 0;\nerror:\n\tdev_err(&h->pdev->dev, \"failed to enter simple mode\\n\");\n\treturn -ENODEV;\n}\n\n \nstatic void hpsa_free_pci_init(struct ctlr_info *h)\n{\n\thpsa_free_cfgtables(h);\t\t\t \n\tiounmap(h->vaddr);\t\t\t \n\th->vaddr = NULL;\n\thpsa_disable_interrupt_mode(h);\t\t \n\t \n\tpci_disable_device(h->pdev);\t\t \n\tpci_release_regions(h->pdev);\t\t \n}\n\n \nstatic int hpsa_pci_init(struct ctlr_info *h)\n{\n\tint prod_index, err;\n\tbool legacy_board;\n\n\tprod_index = hpsa_lookup_board_id(h->pdev, &h->board_id, &legacy_board);\n\tif (prod_index < 0)\n\t\treturn prod_index;\n\th->product_name = products[prod_index].product_name;\n\th->access = *(products[prod_index].access);\n\th->legacy_board = legacy_board;\n\tpci_disable_link_state(h->pdev, PCIE_LINK_STATE_L0S |\n\t\t\t       PCIE_LINK_STATE_L1 | PCIE_LINK_STATE_CLKPM);\n\n\terr = pci_enable_device(h->pdev);\n\tif (err) {\n\t\tdev_err(&h->pdev->dev, \"failed to enable PCI device\\n\");\n\t\tpci_disable_device(h->pdev);\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(h->pdev, HPSA);\n\tif (err) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"failed to obtain PCI resources\\n\");\n\t\tpci_disable_device(h->pdev);\n\t\treturn err;\n\t}\n\n\tpci_set_master(h->pdev);\n\n\terr = hpsa_interrupt_mode(h);\n\tif (err)\n\t\tgoto clean1;\n\n\t \n\thpsa_setup_reply_map(h);\n\n\terr = hpsa_pci_find_memory_BAR(h->pdev, &h->paddr);\n\tif (err)\n\t\tgoto clean2;\t \n\th->vaddr = remap_pci_mem(h->paddr, 0x250);\n\tif (!h->vaddr) {\n\t\tdev_err(&h->pdev->dev, \"failed to remap PCI mem\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto clean2;\t \n\t}\n\terr = hpsa_wait_for_board_state(h->pdev, h->vaddr, BOARD_READY);\n\tif (err)\n\t\tgoto clean3;\t \n\terr = hpsa_find_cfgtables(h);\n\tif (err)\n\t\tgoto clean3;\t \n\thpsa_find_board_params(h);\n\n\tif (!hpsa_CISS_signature_present(h)) {\n\t\terr = -ENODEV;\n\t\tgoto clean4;\t \n\t}\n\thpsa_set_driver_support_bits(h);\n\thpsa_p600_dma_prefetch_quirk(h);\n\terr = hpsa_enter_simple_mode(h);\n\tif (err)\n\t\tgoto clean4;\t \n\treturn 0;\n\nclean4:\t \n\thpsa_free_cfgtables(h);\nclean3:\t \n\tiounmap(h->vaddr);\n\th->vaddr = NULL;\nclean2:\t \n\thpsa_disable_interrupt_mode(h);\nclean1:\n\t \n\tpci_disable_device(h->pdev);\n\tpci_release_regions(h->pdev);\n\treturn err;\n}\n\nstatic void hpsa_hba_inquiry(struct ctlr_info *h)\n{\n\tint rc;\n\n#define HBA_INQUIRY_BYTE_COUNT 64\n\th->hba_inquiry_data = kmalloc(HBA_INQUIRY_BYTE_COUNT, GFP_KERNEL);\n\tif (!h->hba_inquiry_data)\n\t\treturn;\n\trc = hpsa_scsi_do_inquiry(h, RAID_CTLR_LUNID, 0,\n\t\th->hba_inquiry_data, HBA_INQUIRY_BYTE_COUNT);\n\tif (rc != 0) {\n\t\tkfree(h->hba_inquiry_data);\n\t\th->hba_inquiry_data = NULL;\n\t}\n}\n\nstatic int hpsa_init_reset_devices(struct pci_dev *pdev, u32 board_id)\n{\n\tint rc, i;\n\tvoid __iomem *vaddr;\n\n\tif (!reset_devices)\n\t\treturn 0;\n\n\t \n\trc = pci_enable_device(pdev);\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"Failed to enable PCI device\\n\");\n\t\treturn -ENODEV;\n\t}\n\tpci_disable_device(pdev);\n\tmsleep(260);\t\t\t \n\trc = pci_enable_device(pdev);\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"failed to enable device.\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tpci_set_master(pdev);\n\n\tvaddr = pci_ioremap_bar(pdev, 0);\n\tif (vaddr == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto out_disable;\n\t}\n\twritel(SA5_INTR_OFF, vaddr + SA5_REPLY_INTR_MASK_OFFSET);\n\tiounmap(vaddr);\n\n\t \n\trc = hpsa_kdump_hard_reset_controller(pdev, board_id);\n\n\t \n\tif (rc)\n\t\tgoto out_disable;\n\n\t \n\tdev_info(&pdev->dev, \"Waiting for controller to respond to no-op\\n\");\n\tfor (i = 0; i < HPSA_POST_RESET_NOOP_RETRIES; i++) {\n\t\tif (hpsa_noop(pdev) == 0)\n\t\t\tbreak;\n\t\telse\n\t\t\tdev_warn(&pdev->dev, \"no-op failed%s\\n\",\n\t\t\t\t\t(i < 11 ? \"; re-trying\" : \"\"));\n\t}\n\nout_disable:\n\n\tpci_disable_device(pdev);\n\treturn rc;\n}\n\nstatic void hpsa_free_cmd_pool(struct ctlr_info *h)\n{\n\tbitmap_free(h->cmd_pool_bits);\n\th->cmd_pool_bits = NULL;\n\tif (h->cmd_pool) {\n\t\tdma_free_coherent(&h->pdev->dev,\n\t\t\t\th->nr_cmds * sizeof(struct CommandList),\n\t\t\t\th->cmd_pool,\n\t\t\t\th->cmd_pool_dhandle);\n\t\th->cmd_pool = NULL;\n\t\th->cmd_pool_dhandle = 0;\n\t}\n\tif (h->errinfo_pool) {\n\t\tdma_free_coherent(&h->pdev->dev,\n\t\t\t\th->nr_cmds * sizeof(struct ErrorInfo),\n\t\t\t\th->errinfo_pool,\n\t\t\t\th->errinfo_pool_dhandle);\n\t\th->errinfo_pool = NULL;\n\t\th->errinfo_pool_dhandle = 0;\n\t}\n}\n\nstatic int hpsa_alloc_cmd_pool(struct ctlr_info *h)\n{\n\th->cmd_pool_bits = bitmap_zalloc(h->nr_cmds, GFP_KERNEL);\n\th->cmd_pool = dma_alloc_coherent(&h->pdev->dev,\n\t\t    h->nr_cmds * sizeof(*h->cmd_pool),\n\t\t    &h->cmd_pool_dhandle, GFP_KERNEL);\n\th->errinfo_pool = dma_alloc_coherent(&h->pdev->dev,\n\t\t    h->nr_cmds * sizeof(*h->errinfo_pool),\n\t\t    &h->errinfo_pool_dhandle, GFP_KERNEL);\n\tif ((h->cmd_pool_bits == NULL)\n\t    || (h->cmd_pool == NULL)\n\t    || (h->errinfo_pool == NULL)) {\n\t\tdev_err(&h->pdev->dev, \"out of memory in %s\", __func__);\n\t\tgoto clean_up;\n\t}\n\thpsa_preinitialize_commands(h);\n\treturn 0;\nclean_up:\n\thpsa_free_cmd_pool(h);\n\treturn -ENOMEM;\n}\n\n \nstatic void hpsa_free_irqs(struct ctlr_info *h)\n{\n\tint i;\n\tint irq_vector = 0;\n\n\tif (hpsa_simple_mode)\n\t\tirq_vector = h->intr_mode;\n\n\tif (!h->msix_vectors || h->intr_mode != PERF_MODE_INT) {\n\t\t \n\t\tfree_irq(pci_irq_vector(h->pdev, irq_vector),\n\t\t\t\t&h->q[h->intr_mode]);\n\t\th->q[h->intr_mode] = 0;\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < h->msix_vectors; i++) {\n\t\tfree_irq(pci_irq_vector(h->pdev, i), &h->q[i]);\n\t\th->q[i] = 0;\n\t}\n\tfor (; i < MAX_REPLY_QUEUES; i++)\n\t\th->q[i] = 0;\n}\n\n \nstatic int hpsa_request_irqs(struct ctlr_info *h,\n\tirqreturn_t (*msixhandler)(int, void *),\n\tirqreturn_t (*intxhandler)(int, void *))\n{\n\tint rc, i;\n\tint irq_vector = 0;\n\n\tif (hpsa_simple_mode)\n\t\tirq_vector = h->intr_mode;\n\n\t \n\tfor (i = 0; i < MAX_REPLY_QUEUES; i++)\n\t\th->q[i] = (u8) i;\n\n\tif (h->intr_mode == PERF_MODE_INT && h->msix_vectors > 0) {\n\t\t \n\t\tfor (i = 0; i < h->msix_vectors; i++) {\n\t\t\tsprintf(h->intrname[i], \"%s-msix%d\", h->devname, i);\n\t\t\trc = request_irq(pci_irq_vector(h->pdev, i), msixhandler,\n\t\t\t\t\t0, h->intrname[i],\n\t\t\t\t\t&h->q[i]);\n\t\t\tif (rc) {\n\t\t\t\tint j;\n\n\t\t\t\tdev_err(&h->pdev->dev,\n\t\t\t\t\t\"failed to get irq %d for %s\\n\",\n\t\t\t\t       pci_irq_vector(h->pdev, i), h->devname);\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tfree_irq(pci_irq_vector(h->pdev, j), &h->q[j]);\n\t\t\t\t\th->q[j] = 0;\n\t\t\t\t}\n\t\t\t\tfor (; j < MAX_REPLY_QUEUES; j++)\n\t\t\t\t\th->q[j] = 0;\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tif (h->msix_vectors > 0 || h->pdev->msi_enabled) {\n\t\t\tsprintf(h->intrname[0], \"%s-msi%s\", h->devname,\n\t\t\t\th->msix_vectors ? \"x\" : \"\");\n\t\t\trc = request_irq(pci_irq_vector(h->pdev, irq_vector),\n\t\t\t\tmsixhandler, 0,\n\t\t\t\th->intrname[0],\n\t\t\t\t&h->q[h->intr_mode]);\n\t\t} else {\n\t\t\tsprintf(h->intrname[h->intr_mode],\n\t\t\t\t\"%s-intx\", h->devname);\n\t\t\trc = request_irq(pci_irq_vector(h->pdev, irq_vector),\n\t\t\t\tintxhandler, IRQF_SHARED,\n\t\t\t\th->intrname[0],\n\t\t\t\t&h->q[h->intr_mode]);\n\t\t}\n\t}\n\tif (rc) {\n\t\tdev_err(&h->pdev->dev, \"failed to get irq %d for %s\\n\",\n\t\t       pci_irq_vector(h->pdev, irq_vector), h->devname);\n\t\thpsa_free_irqs(h);\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic int hpsa_kdump_soft_reset(struct ctlr_info *h)\n{\n\tint rc;\n\thpsa_send_host_reset(h, HPSA_RESET_TYPE_CONTROLLER);\n\n\tdev_info(&h->pdev->dev, \"Waiting for board to soft reset.\\n\");\n\trc = hpsa_wait_for_board_state(h->pdev, h->vaddr, BOARD_NOT_READY);\n\tif (rc) {\n\t\tdev_warn(&h->pdev->dev, \"Soft reset had no effect.\\n\");\n\t\treturn rc;\n\t}\n\n\tdev_info(&h->pdev->dev, \"Board reset, awaiting READY status.\\n\");\n\trc = hpsa_wait_for_board_state(h->pdev, h->vaddr, BOARD_READY);\n\tif (rc) {\n\t\tdev_warn(&h->pdev->dev, \"Board failed to become ready \"\n\t\t\t\"after soft reset.\\n\");\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void hpsa_free_reply_queues(struct ctlr_info *h)\n{\n\tint i;\n\n\tfor (i = 0; i < h->nreply_queues; i++) {\n\t\tif (!h->reply_queue[i].head)\n\t\t\tcontinue;\n\t\tdma_free_coherent(&h->pdev->dev,\n\t\t\t\t\th->reply_queue_size,\n\t\t\t\t\th->reply_queue[i].head,\n\t\t\t\t\th->reply_queue[i].busaddr);\n\t\th->reply_queue[i].head = NULL;\n\t\th->reply_queue[i].busaddr = 0;\n\t}\n\th->reply_queue_size = 0;\n}\n\nstatic void hpsa_undo_allocations_after_kdump_soft_reset(struct ctlr_info *h)\n{\n\thpsa_free_performant_mode(h);\t\t \n\thpsa_free_sg_chain_blocks(h);\t\t \n\thpsa_free_cmd_pool(h);\t\t\t \n\thpsa_free_irqs(h);\t\t\t \n\tscsi_host_put(h->scsi_host);\t\t \n\th->scsi_host = NULL;\t\t\t \n\thpsa_free_pci_init(h);\t\t\t \n\tfree_percpu(h->lockup_detected);\t \n\th->lockup_detected = NULL;\t\t \n\tif (h->resubmit_wq) {\n\t\tdestroy_workqueue(h->resubmit_wq);\t \n\t\th->resubmit_wq = NULL;\n\t}\n\tif (h->rescan_ctlr_wq) {\n\t\tdestroy_workqueue(h->rescan_ctlr_wq);\n\t\th->rescan_ctlr_wq = NULL;\n\t}\n\tif (h->monitor_ctlr_wq) {\n\t\tdestroy_workqueue(h->monitor_ctlr_wq);\n\t\th->monitor_ctlr_wq = NULL;\n\t}\n\n\tkfree(h);\t\t\t\t \n}\n\n \nstatic void fail_all_outstanding_cmds(struct ctlr_info *h)\n{\n\tint i, refcount;\n\tstruct CommandList *c;\n\tint failcount = 0;\n\n\tflush_workqueue(h->resubmit_wq);  \n\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\tc = h->cmd_pool + i;\n\t\trefcount = atomic_inc_return(&c->refcount);\n\t\tif (refcount > 1) {\n\t\t\tc->err_info->CommandStatus = CMD_CTLR_LOCKUP;\n\t\t\tfinish_cmd(c);\n\t\t\tatomic_dec(&h->commands_outstanding);\n\t\t\tfailcount++;\n\t\t}\n\t\tcmd_free(h, c);\n\t}\n\tdev_warn(&h->pdev->dev,\n\t\t\"failed %d commands in fail_all\\n\", failcount);\n}\n\nstatic void set_lockup_detected_for_all_cpus(struct ctlr_info *h, u32 value)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu) {\n\t\tu32 *lockup_detected;\n\t\tlockup_detected = per_cpu_ptr(h->lockup_detected, cpu);\n\t\t*lockup_detected = value;\n\t}\n\twmb();  \n}\n\nstatic void controller_lockup_detected(struct ctlr_info *h)\n{\n\tunsigned long flags;\n\tu32 lockup_detected;\n\n\th->access.set_intr_mask(h, HPSA_INTR_OFF);\n\tspin_lock_irqsave(&h->lock, flags);\n\tlockup_detected = readl(h->vaddr + SA5_SCRATCHPAD_OFFSET);\n\tif (!lockup_detected) {\n\t\t \n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"lockup detected after %d but scratchpad register is zero\\n\",\n\t\t\th->heartbeat_sample_interval / HZ);\n\t\tlockup_detected = 0xffffffff;\n\t}\n\tset_lockup_detected_for_all_cpus(h, lockup_detected);\n\tspin_unlock_irqrestore(&h->lock, flags);\n\tdev_warn(&h->pdev->dev, \"Controller lockup detected: 0x%08x after %d\\n\",\n\t\t\tlockup_detected, h->heartbeat_sample_interval / HZ);\n\tif (lockup_detected == 0xffff0000) {\n\t\tdev_warn(&h->pdev->dev, \"Telling controller to do a CHKPT\\n\");\n\t\twritel(DOORBELL_GENERATE_CHKPT, h->vaddr + SA5_DOORBELL);\n\t}\n\tpci_disable_device(h->pdev);\n\tfail_all_outstanding_cmds(h);\n}\n\nstatic int detect_controller_lockup(struct ctlr_info *h)\n{\n\tu64 now;\n\tu32 heartbeat;\n\tunsigned long flags;\n\n\tnow = get_jiffies_64();\n\t \n\tif (time_after64(h->last_intr_timestamp +\n\t\t\t\t(h->heartbeat_sample_interval), now))\n\t\treturn false;\n\n\t \n\tif (time_after64(h->last_heartbeat_timestamp +\n\t\t\t\t(h->heartbeat_sample_interval), now))\n\t\treturn false;\n\n\t \n\tspin_lock_irqsave(&h->lock, flags);\n\theartbeat = readl(&h->cfgtable->HeartBeat);\n\tspin_unlock_irqrestore(&h->lock, flags);\n\tif (h->last_heartbeat == heartbeat) {\n\t\tcontroller_lockup_detected(h);\n\t\treturn true;\n\t}\n\n\t \n\th->last_heartbeat = heartbeat;\n\th->last_heartbeat_timestamp = now;\n\treturn false;\n}\n\n \nstatic void hpsa_set_ioaccel_status(struct ctlr_info *h)\n{\n\tint rc;\n\tint i;\n\tu8 ioaccel_status;\n\tunsigned char *buf;\n\tstruct hpsa_scsi_dev_t *device;\n\n\tif (!h)\n\t\treturn;\n\n\tbuf = kmalloc(64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tint offload_to_be_enabled = 0;\n\t\tint offload_config = 0;\n\n\t\tdevice = h->dev[i];\n\n\t\tif (!device)\n\t\t\tcontinue;\n\t\tif (!hpsa_vpd_page_supported(h, device->scsi3addr,\n\t\t\t\t\t\tHPSA_VPD_LV_IOACCEL_STATUS))\n\t\t\tcontinue;\n\n\t\tmemset(buf, 0, 64);\n\n\t\trc = hpsa_scsi_do_inquiry(h, device->scsi3addr,\n\t\t\t\t\tVPD_PAGE | HPSA_VPD_LV_IOACCEL_STATUS,\n\t\t\t\t\tbuf, 64);\n\t\tif (rc != 0)\n\t\t\tcontinue;\n\n\t\tioaccel_status = buf[IOACCEL_STATUS_BYTE];\n\n\t\t \n\t\toffload_config =\n\t\t\t\t!!(ioaccel_status & OFFLOAD_CONFIGURED_BIT);\n\t\t \n\t\tif (offload_config)\n\t\t\toffload_to_be_enabled =\n\t\t\t\t!!(ioaccel_status & OFFLOAD_ENABLED_BIT);\n\n\t\t \n\t\tif (offload_to_be_enabled)\n\t\t\tcontinue;\n\n\t\t \n\t\thpsa_turn_off_ioaccel_for_device(device);\n\t}\n\n\tkfree(buf);\n}\n\nstatic void hpsa_ack_ctlr_events(struct ctlr_info *h)\n{\n\tchar *event_type;\n\n\tif (!(h->fw_support & MISC_FW_EVENT_NOTIFY))\n\t\treturn;\n\n\t \n\tif ((h->transMethod & (CFGTBL_Trans_io_accel1\n\t\t\t| CFGTBL_Trans_io_accel2)) &&\n\t\t(h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE ||\n\t\t h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)) {\n\n\t\tif (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE)\n\t\t\tevent_type = \"state change\";\n\t\tif (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)\n\t\t\tevent_type = \"configuration change\";\n\t\t \n\t\tscsi_block_requests(h->scsi_host);\n\t\thpsa_set_ioaccel_status(h);\n\t\thpsa_drain_accel_commands(h);\n\t\t \n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"Acknowledging event: 0x%08x (HP SSD Smart Path %s)\\n\",\n\t\t\th->events, event_type);\n\t\twritel(h->events, &(h->cfgtable->clear_event_notify));\n\t\t \n\t\twritel(DOORBELL_CLEAR_EVENTS, h->vaddr + SA5_DOORBELL);\n\t\t \n\t\thpsa_wait_for_clear_event_notify_ack(h);\n\t\tscsi_unblock_requests(h->scsi_host);\n\t} else {\n\t\t \n\t\twritel(h->events, &(h->cfgtable->clear_event_notify));\n\t\twritel(DOORBELL_CLEAR_EVENTS, h->vaddr + SA5_DOORBELL);\n\t\thpsa_wait_for_clear_event_notify_ack(h);\n\t}\n\treturn;\n}\n\n \nstatic int hpsa_ctlr_needs_rescan(struct ctlr_info *h)\n{\n\tif (h->drv_req_rescan) {\n\t\th->drv_req_rescan = 0;\n\t\treturn 1;\n\t}\n\n\tif (!(h->fw_support & MISC_FW_EVENT_NOTIFY))\n\t\treturn 0;\n\n\th->events = readl(&(h->cfgtable->event_notify));\n\treturn h->events & RESCAN_REQUIRED_EVENT_BITS;\n}\n\n \nstatic int hpsa_offline_devices_ready(struct ctlr_info *h)\n{\n\tunsigned long flags;\n\tstruct offline_device_entry *d;\n\tstruct list_head *this, *tmp;\n\n\tspin_lock_irqsave(&h->offline_device_lock, flags);\n\tlist_for_each_safe(this, tmp, &h->offline_device_list) {\n\t\td = list_entry(this, struct offline_device_entry,\n\t\t\t\toffline_list);\n\t\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n\t\tif (!hpsa_volume_offline(h, d->scsi3addr)) {\n\t\t\tspin_lock_irqsave(&h->offline_device_lock, flags);\n\t\t\tlist_del(&d->offline_list);\n\t\t\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n\t\t\treturn 1;\n\t\t}\n\t\tspin_lock_irqsave(&h->offline_device_lock, flags);\n\t}\n\tspin_unlock_irqrestore(&h->offline_device_lock, flags);\n\treturn 0;\n}\n\nstatic int hpsa_luns_changed(struct ctlr_info *h)\n{\n\tint rc = 1;  \n\tstruct ReportLUNdata *logdev = NULL;\n\n\t \n\n\tif (!h->lastlogicals)\n\t\treturn rc;\n\n\tlogdev = kzalloc(sizeof(*logdev), GFP_KERNEL);\n\tif (!logdev)\n\t\treturn rc;\n\n\tif (hpsa_scsi_do_report_luns(h, 1, logdev, sizeof(*logdev), 0)) {\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"report luns failed, can't track lun changes.\\n\");\n\t\tgoto out;\n\t}\n\tif (memcmp(logdev, h->lastlogicals, sizeof(*logdev))) {\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"Lun changes detected.\\n\");\n\t\tmemcpy(h->lastlogicals, logdev, sizeof(*logdev));\n\t\tgoto out;\n\t} else\n\t\trc = 0;  \nout:\n\tkfree(logdev);\n\treturn rc;\n}\n\nstatic void hpsa_perform_rescan(struct ctlr_info *h)\n{\n\tstruct Scsi_Host *sh = NULL;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&h->reset_lock, flags);\n\tif (h->reset_in_progress) {\n\t\th->drv_req_rescan = 1;\n\t\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&h->reset_lock, flags);\n\n\tsh = scsi_host_get(h->scsi_host);\n\tif (sh != NULL) {\n\t\thpsa_scan_start(sh);\n\t\tscsi_host_put(sh);\n\t\th->drv_req_rescan = 0;\n\t}\n}\n\n \nstatic void hpsa_event_monitor_worker(struct work_struct *work)\n{\n\tstruct ctlr_info *h = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct ctlr_info, event_monitor_work);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&h->lock, flags);\n\tif (h->remove_in_progress) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&h->lock, flags);\n\n\tif (hpsa_ctlr_needs_rescan(h)) {\n\t\thpsa_ack_ctlr_events(h);\n\t\thpsa_perform_rescan(h);\n\t}\n\n\tspin_lock_irqsave(&h->lock, flags);\n\tif (!h->remove_in_progress)\n\t\tqueue_delayed_work(h->monitor_ctlr_wq, &h->event_monitor_work,\n\t\t\t\tHPSA_EVENT_MONITOR_INTERVAL);\n\tspin_unlock_irqrestore(&h->lock, flags);\n}\n\nstatic void hpsa_rescan_ctlr_worker(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct ctlr_info *h = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct ctlr_info, rescan_ctlr_work);\n\n\tspin_lock_irqsave(&h->lock, flags);\n\tif (h->remove_in_progress) {\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&h->lock, flags);\n\n\tif (h->drv_req_rescan || hpsa_offline_devices_ready(h)) {\n\t\thpsa_perform_rescan(h);\n\t} else if (h->discovery_polling) {\n\t\tif (hpsa_luns_changed(h)) {\n\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t\"driver discovery polling rescan.\\n\");\n\t\t\thpsa_perform_rescan(h);\n\t\t}\n\t}\n\tspin_lock_irqsave(&h->lock, flags);\n\tif (!h->remove_in_progress)\n\t\tqueue_delayed_work(h->rescan_ctlr_wq, &h->rescan_ctlr_work,\n\t\t\t\th->heartbeat_sample_interval);\n\tspin_unlock_irqrestore(&h->lock, flags);\n}\n\nstatic void hpsa_monitor_ctlr_worker(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct ctlr_info *h = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct ctlr_info, monitor_ctlr_work);\n\n\tdetect_controller_lockup(h);\n\tif (lockup_detected(h))\n\t\treturn;\n\n\tspin_lock_irqsave(&h->lock, flags);\n\tif (!h->remove_in_progress)\n\t\tqueue_delayed_work(h->monitor_ctlr_wq, &h->monitor_ctlr_work,\n\t\t\t\th->heartbeat_sample_interval);\n\tspin_unlock_irqrestore(&h->lock, flags);\n}\n\nstatic struct workqueue_struct *hpsa_create_controller_wq(struct ctlr_info *h,\n\t\t\t\t\t\tchar *name)\n{\n\tstruct workqueue_struct *wq = NULL;\n\n\twq = alloc_ordered_workqueue(\"%s_%d_hpsa\", 0, name, h->ctlr);\n\tif (!wq)\n\t\tdev_err(&h->pdev->dev, \"failed to create %s workqueue\\n\", name);\n\n\treturn wq;\n}\n\nstatic void hpda_free_ctlr_info(struct ctlr_info *h)\n{\n\tkfree(h->reply_map);\n\tkfree(h);\n}\n\nstatic struct ctlr_info *hpda_alloc_ctlr_info(void)\n{\n\tstruct ctlr_info *h;\n\n\th = kzalloc(sizeof(*h), GFP_KERNEL);\n\tif (!h)\n\t\treturn NULL;\n\n\th->reply_map = kcalloc(nr_cpu_ids, sizeof(*h->reply_map), GFP_KERNEL);\n\tif (!h->reply_map) {\n\t\tkfree(h);\n\t\treturn NULL;\n\t}\n\treturn h;\n}\n\nstatic int hpsa_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tint rc;\n\tstruct ctlr_info *h;\n\tint try_soft_reset = 0;\n\tunsigned long flags;\n\tu32 board_id;\n\n\tif (number_of_controllers == 0)\n\t\tprintk(KERN_INFO DRIVER_NAME \"\\n\");\n\n\trc = hpsa_lookup_board_id(pdev, &board_id, NULL);\n\tif (rc < 0) {\n\t\tdev_warn(&pdev->dev, \"Board ID not found\\n\");\n\t\treturn rc;\n\t}\n\n\trc = hpsa_init_reset_devices(pdev, board_id);\n\tif (rc) {\n\t\tif (rc != -ENOTSUPP)\n\t\t\treturn rc;\n\t\t \n\t\ttry_soft_reset = 1;\n\t\trc = 0;\n\t}\n\nreinit_after_soft_reset:\n\n\t \n\tBUILD_BUG_ON(sizeof(struct CommandList) % COMMANDLIST_ALIGNMENT);\n\th = hpda_alloc_ctlr_info();\n\tif (!h) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate controller head\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\th->pdev = pdev;\n\n\th->intr_mode = hpsa_simple_mode ? SIMPLE_MODE_INT : PERF_MODE_INT;\n\tINIT_LIST_HEAD(&h->offline_device_list);\n\tspin_lock_init(&h->lock);\n\tspin_lock_init(&h->offline_device_lock);\n\tspin_lock_init(&h->scan_lock);\n\tspin_lock_init(&h->reset_lock);\n\tatomic_set(&h->passthru_cmds_avail, HPSA_MAX_CONCURRENT_PASSTHRUS);\n\n\t \n\th->lockup_detected = alloc_percpu(u32);\n\tif (!h->lockup_detected) {\n\t\tdev_err(&h->pdev->dev, \"Failed to allocate lockup detector\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto clean1;\t \n\t}\n\tset_lockup_detected_for_all_cpus(h, 0);\n\n\trc = hpsa_pci_init(h);\n\tif (rc)\n\t\tgoto clean2;\t \n\n\t \n\trc = hpsa_scsi_host_alloc(h);\n\tif (rc)\n\t\tgoto clean2_5;\t \n\n\tsprintf(h->devname, HPSA \"%d\", h->scsi_host->host_no);\n\th->ctlr = number_of_controllers;\n\tnumber_of_controllers++;\n\n\t \n\trc = dma_set_mask(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc != 0) {\n\t\trc = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (rc != 0) {\n\t\t\tdev_err(&pdev->dev, \"no suitable DMA available\\n\");\n\t\t\tgoto clean3;\t \n\t\t}\n\t}\n\n\t \n\th->access.set_intr_mask(h, HPSA_INTR_OFF);\n\n\trc = hpsa_request_irqs(h, do_hpsa_intr_msi, do_hpsa_intr_intx);\n\tif (rc)\n\t\tgoto clean3;\t \n\trc = hpsa_alloc_cmd_pool(h);\n\tif (rc)\n\t\tgoto clean4;\t \n\trc = hpsa_alloc_sg_chain_blocks(h);\n\tif (rc)\n\t\tgoto clean5;\t \n\tinit_waitqueue_head(&h->scan_wait_queue);\n\tinit_waitqueue_head(&h->event_sync_wait_queue);\n\tmutex_init(&h->reset_mutex);\n\th->scan_finished = 1;  \n\th->scan_waiting = 0;\n\n\tpci_set_drvdata(pdev, h);\n\th->ndevices = 0;\n\n\tspin_lock_init(&h->devlock);\n\trc = hpsa_put_ctlr_into_performant_mode(h);\n\tif (rc)\n\t\tgoto clean6;  \n\n\t \n\th->rescan_ctlr_wq = hpsa_create_controller_wq(h, \"rescan\");\n\tif (!h->rescan_ctlr_wq) {\n\t\trc = -ENOMEM;\n\t\tgoto clean7;\n\t}\n\n\th->resubmit_wq = hpsa_create_controller_wq(h, \"resubmit\");\n\tif (!h->resubmit_wq) {\n\t\trc = -ENOMEM;\n\t\tgoto clean7;\t \n\t}\n\n\th->monitor_ctlr_wq = hpsa_create_controller_wq(h, \"monitor\");\n\tif (!h->monitor_ctlr_wq) {\n\t\trc = -ENOMEM;\n\t\tgoto clean7;\n\t}\n\n\t \n\tif (try_soft_reset) {\n\n\t\t \n\t\tspin_lock_irqsave(&h->lock, flags);\n\t\th->access.set_intr_mask(h, HPSA_INTR_OFF);\n\t\tspin_unlock_irqrestore(&h->lock, flags);\n\t\thpsa_free_irqs(h);\n\t\trc = hpsa_request_irqs(h, hpsa_msix_discard_completions,\n\t\t\t\t\thpsa_intx_discard_completions);\n\t\tif (rc) {\n\t\t\tdev_warn(&h->pdev->dev,\n\t\t\t\t\"Failed to request_irq after soft reset.\\n\");\n\t\t\t \n\t\t\thpsa_free_performant_mode(h);\t \n\t\t\thpsa_free_sg_chain_blocks(h);\t \n\t\t\thpsa_free_cmd_pool(h);\t\t \n\t\t\t \n\t\t\tgoto clean3;\n\t\t}\n\n\t\trc = hpsa_kdump_soft_reset(h);\n\t\tif (rc)\n\t\t\t \n\t\t\tgoto clean7;\n\n\t\tdev_info(&h->pdev->dev, \"Board READY.\\n\");\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"Waiting for stale completions to drain.\\n\");\n\t\th->access.set_intr_mask(h, HPSA_INTR_ON);\n\t\tmsleep(10000);\n\t\th->access.set_intr_mask(h, HPSA_INTR_OFF);\n\n\t\trc = controller_reset_failed(h->cfgtable);\n\t\tif (rc)\n\t\t\tdev_info(&h->pdev->dev,\n\t\t\t\t\"Soft reset appears to have failed.\\n\");\n\n\t\t \n\t\thpsa_undo_allocations_after_kdump_soft_reset(h);\n\t\ttry_soft_reset = 0;\n\t\tif (rc)\n\t\t\t \n\t\t\treturn -ENODEV;\n\n\t\tgoto reinit_after_soft_reset;\n\t}\n\n\t \n\th->acciopath_status = 1;\n\t \n\th->discovery_polling = 0;\n\n\n\t \n\th->access.set_intr_mask(h, HPSA_INTR_ON);\n\n\thpsa_hba_inquiry(h);\n\n\th->lastlogicals = kzalloc(sizeof(*(h->lastlogicals)), GFP_KERNEL);\n\tif (!h->lastlogicals)\n\t\tdev_info(&h->pdev->dev,\n\t\t\t\"Can't track change to report lun data\\n\");\n\n\t \n\trc = hpsa_scsi_add_host(h);\n\tif (rc)\n\t\tgoto clean8;  \n\n\t \n\th->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;\n\tINIT_DELAYED_WORK(&h->monitor_ctlr_work, hpsa_monitor_ctlr_worker);\n\tschedule_delayed_work(&h->monitor_ctlr_work,\n\t\t\t\th->heartbeat_sample_interval);\n\tINIT_DELAYED_WORK(&h->rescan_ctlr_work, hpsa_rescan_ctlr_worker);\n\tqueue_delayed_work(h->rescan_ctlr_wq, &h->rescan_ctlr_work,\n\t\t\t\th->heartbeat_sample_interval);\n\tINIT_DELAYED_WORK(&h->event_monitor_work, hpsa_event_monitor_worker);\n\tschedule_delayed_work(&h->event_monitor_work,\n\t\t\t\tHPSA_EVENT_MONITOR_INTERVAL);\n\treturn 0;\n\nclean8:  \n\tkfree(h->lastlogicals);\nclean7:  \n\thpsa_free_performant_mode(h);\n\th->access.set_intr_mask(h, HPSA_INTR_OFF);\nclean6:  \n\thpsa_free_sg_chain_blocks(h);\nclean5:  \n\thpsa_free_cmd_pool(h);\nclean4:  \n\thpsa_free_irqs(h);\nclean3:  \n\tscsi_host_put(h->scsi_host);\n\th->scsi_host = NULL;\nclean2_5:  \n\thpsa_free_pci_init(h);\nclean2:  \n\tif (h->lockup_detected) {\n\t\tfree_percpu(h->lockup_detected);\n\t\th->lockup_detected = NULL;\n\t}\nclean1:\t \n\tif (h->resubmit_wq) {\n\t\tdestroy_workqueue(h->resubmit_wq);\n\t\th->resubmit_wq = NULL;\n\t}\n\tif (h->rescan_ctlr_wq) {\n\t\tdestroy_workqueue(h->rescan_ctlr_wq);\n\t\th->rescan_ctlr_wq = NULL;\n\t}\n\tif (h->monitor_ctlr_wq) {\n\t\tdestroy_workqueue(h->monitor_ctlr_wq);\n\t\th->monitor_ctlr_wq = NULL;\n\t}\n\thpda_free_ctlr_info(h);\n\treturn rc;\n}\n\nstatic void hpsa_flush_cache(struct ctlr_info *h)\n{\n\tchar *flush_buf;\n\tstruct CommandList *c;\n\tint rc;\n\n\tif (unlikely(lockup_detected(h)))\n\t\treturn;\n\tflush_buf = kzalloc(4, GFP_KERNEL);\n\tif (!flush_buf)\n\t\treturn;\n\n\tc = cmd_alloc(h);\n\n\tif (fill_cmd(c, HPSA_CACHE_FLUSH, h, flush_buf, 4, 0,\n\t\tRAID_CTLR_LUNID, TYPE_CMD)) {\n\t\tgoto out;\n\t}\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_TO_DEVICE,\n\t\t\tDEFAULT_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\tif (c->err_info->CommandStatus != 0)\nout:\n\t\tdev_warn(&h->pdev->dev,\n\t\t\t\"error flushing cache on controller\\n\");\n\tcmd_free(h, c);\n\tkfree(flush_buf);\n}\n\n \nstatic void hpsa_disable_rld_caching(struct ctlr_info *h)\n{\n\tu32 *options;\n\tstruct CommandList *c;\n\tint rc;\n\n\t \n\tif (unlikely(h->lockup_detected))\n\t\treturn;\n\n\toptions = kzalloc(sizeof(*options), GFP_KERNEL);\n\tif (!options)\n\t\treturn;\n\n\tc = cmd_alloc(h);\n\n\t \n\tif (fill_cmd(c, BMIC_SENSE_DIAG_OPTIONS, h, options, 4, 0,\n\t\tRAID_CTLR_LUNID, TYPE_CMD))\n\t\tgoto errout;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif ((rc != 0) || (c->err_info->CommandStatus != 0))\n\t\tgoto errout;\n\n\t \n\t*options |= HPSA_DIAG_OPTS_DISABLE_RLD_CACHING;\n\n\tif (fill_cmd(c, BMIC_SET_DIAG_OPTIONS, h, options, 4, 0,\n\t\tRAID_CTLR_LUNID, TYPE_CMD))\n\t\tgoto errout;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_TO_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif ((rc != 0)  || (c->err_info->CommandStatus != 0))\n\t\tgoto errout;\n\n\t \n\tif (fill_cmd(c, BMIC_SENSE_DIAG_OPTIONS, h, options, 4, 0,\n\t\tRAID_CTLR_LUNID, TYPE_CMD))\n\t\tgoto errout;\n\n\trc = hpsa_scsi_do_simple_cmd_with_retry(h, c, DMA_FROM_DEVICE,\n\t\t\tNO_TIMEOUT);\n\tif ((rc != 0)  || (c->err_info->CommandStatus != 0))\n\t\tgoto errout;\n\n\tif (*options & HPSA_DIAG_OPTS_DISABLE_RLD_CACHING)\n\t\tgoto out;\n\nerrout:\n\tdev_err(&h->pdev->dev,\n\t\t\t\"Error: failed to disable report lun data caching.\\n\");\nout:\n\tcmd_free(h, c);\n\tkfree(options);\n}\n\nstatic void __hpsa_shutdown(struct pci_dev *pdev)\n{\n\tstruct ctlr_info *h;\n\n\th = pci_get_drvdata(pdev);\n\t \n\thpsa_flush_cache(h);\n\th->access.set_intr_mask(h, HPSA_INTR_OFF);\n\thpsa_free_irqs(h);\t\t\t \n\thpsa_disable_interrupt_mode(h);\t\t \n}\n\nstatic void hpsa_shutdown(struct pci_dev *pdev)\n{\n\t__hpsa_shutdown(pdev);\n\tpci_disable_device(pdev);\n}\n\nstatic void hpsa_free_device_info(struct ctlr_info *h)\n{\n\tint i;\n\n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tkfree(h->dev[i]);\n\t\th->dev[i] = NULL;\n\t}\n}\n\nstatic void hpsa_remove_one(struct pci_dev *pdev)\n{\n\tstruct ctlr_info *h;\n\tunsigned long flags;\n\n\tif (pci_get_drvdata(pdev) == NULL) {\n\t\tdev_err(&pdev->dev, \"unable to remove device\\n\");\n\t\treturn;\n\t}\n\th = pci_get_drvdata(pdev);\n\n\t \n\tspin_lock_irqsave(&h->lock, flags);\n\th->remove_in_progress = 1;\n\tspin_unlock_irqrestore(&h->lock, flags);\n\tcancel_delayed_work_sync(&h->monitor_ctlr_work);\n\tcancel_delayed_work_sync(&h->rescan_ctlr_work);\n\tcancel_delayed_work_sync(&h->event_monitor_work);\n\tdestroy_workqueue(h->rescan_ctlr_wq);\n\tdestroy_workqueue(h->resubmit_wq);\n\tdestroy_workqueue(h->monitor_ctlr_wq);\n\n\thpsa_delete_sas_host(h);\n\n\t \n\tif (h->scsi_host)\n\t\tscsi_remove_host(h->scsi_host);\t\t \n\t \n\t \n\t__hpsa_shutdown(pdev);\n\n\thpsa_free_device_info(h);\t\t \n\n\tkfree(h->hba_inquiry_data);\t\t\t \n\th->hba_inquiry_data = NULL;\t\t\t \n\thpsa_free_ioaccel2_sg_chain_blocks(h);\n\thpsa_free_performant_mode(h);\t\t\t \n\thpsa_free_sg_chain_blocks(h);\t\t\t \n\thpsa_free_cmd_pool(h);\t\t\t\t \n\tkfree(h->lastlogicals);\n\n\t \n\n\tscsi_host_put(h->scsi_host);\t\t\t \n\th->scsi_host = NULL;\t\t\t\t \n\n\t \n\thpsa_free_pci_init(h);\t\t\t\t \n\n\tfree_percpu(h->lockup_detected);\t\t \n\th->lockup_detected = NULL;\t\t\t \n\n\thpda_free_ctlr_info(h);\t\t\t\t \n}\n\nstatic int __maybe_unused hpsa_suspend(\n\t__attribute__((unused)) struct device *dev)\n{\n\treturn -ENOSYS;\n}\n\nstatic int __maybe_unused hpsa_resume\n\t(__attribute__((unused)) struct device *dev)\n{\n\treturn -ENOSYS;\n}\n\nstatic SIMPLE_DEV_PM_OPS(hpsa_pm_ops, hpsa_suspend, hpsa_resume);\n\nstatic struct pci_driver hpsa_pci_driver = {\n\t.name = HPSA,\n\t.probe = hpsa_init_one,\n\t.remove = hpsa_remove_one,\n\t.id_table = hpsa_pci_device_id,\t \n\t.shutdown = hpsa_shutdown,\n\t.driver.pm = &hpsa_pm_ops,\n};\n\n \nstatic void  calc_bucket_map(int bucket[], int num_buckets,\n\tint nsgs, int min_blocks, u32 *bucket_map)\n{\n\tint i, j, b, size;\n\n\t \n\tfor (i = 0; i <= nsgs; i++) {\n\t\t \n\t\tsize = i + min_blocks;\n\t\tb = num_buckets;  \n\t\t \n\t\tfor (j = 0; j < num_buckets; j++) {\n\t\t\tif (bucket[j] >= size) {\n\t\t\t\tb = j;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t \n\t\tbucket_map[i] = b;\n\t}\n}\n\n \nstatic int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)\n{\n\tint i;\n\tunsigned long register_value;\n\tunsigned long transMethod = CFGTBL_Trans_Performant |\n\t\t\t(trans_support & CFGTBL_Trans_use_short_tags) |\n\t\t\t\tCFGTBL_Trans_enable_directed_msix |\n\t\t\t(trans_support & (CFGTBL_Trans_io_accel1 |\n\t\t\t\tCFGTBL_Trans_io_accel2));\n\tstruct access_method access = SA5_performant_access;\n\n\t \n\tint bft[8] = {5, 6, 8, 10, 12, 20, 28, SG_ENTRIES_IN_CMD + 4};\n#define MIN_IOACCEL2_BFT_ENTRY 5\n#define HPSA_IOACCEL2_HEADER_SZ 4\n\tint bft2[16] = {MIN_IOACCEL2_BFT_ENTRY, 6, 7, 8, 9, 10, 11, 12,\n\t\t\t13, 14, 15, 16, 17, 18, 19,\n\t\t\tHPSA_IOACCEL2_HEADER_SZ + IOACCEL2_MAXSGENTRIES};\n\tBUILD_BUG_ON(ARRAY_SIZE(bft2) != 16);\n\tBUILD_BUG_ON(ARRAY_SIZE(bft) != 8);\n\tBUILD_BUG_ON(offsetof(struct io_accel2_cmd, sg) >\n\t\t\t\t 16 * MIN_IOACCEL2_BFT_ENTRY);\n\tBUILD_BUG_ON(sizeof(struct ioaccel2_sg_element) != 16);\n\tBUILD_BUG_ON(28 > SG_ENTRIES_IN_CMD + 4);\n\t \n\n\t \n\tif (trans_support & (CFGTBL_Trans_io_accel1 | CFGTBL_Trans_io_accel2))\n\t\taccess = SA5_performant_access_no_read;\n\n\t \n\tfor (i = 0; i < h->nreply_queues; i++)\n\t\tmemset(h->reply_queue[i].head, 0, h->reply_queue_size);\n\n\tbft[7] = SG_ENTRIES_IN_CMD + 4;\n\tcalc_bucket_map(bft, ARRAY_SIZE(bft),\n\t\t\t\tSG_ENTRIES_IN_CMD, 4, h->blockFetchTable);\n\tfor (i = 0; i < 8; i++)\n\t\twritel(bft[i], &h->transtable->BlockFetch[i]);\n\n\t \n\twritel(h->max_commands, &h->transtable->RepQSize);\n\twritel(h->nreply_queues, &h->transtable->RepQCount);\n\twritel(0, &h->transtable->RepQCtrAddrLow32);\n\twritel(0, &h->transtable->RepQCtrAddrHigh32);\n\n\tfor (i = 0; i < h->nreply_queues; i++) {\n\t\twritel(0, &h->transtable->RepQAddr[i].upper);\n\t\twritel(h->reply_queue[i].busaddr,\n\t\t\t&h->transtable->RepQAddr[i].lower);\n\t}\n\n\twritel(0, &h->cfgtable->HostWrite.command_pool_addr_hi);\n\twritel(transMethod, &(h->cfgtable->HostWrite.TransportRequest));\n\t \n\tif (trans_support & CFGTBL_Trans_io_accel1) {\n\t\taccess = SA5_ioaccel_mode1_access;\n\t\twritel(10, &h->cfgtable->HostWrite.CoalIntDelay);\n\t\twritel(4, &h->cfgtable->HostWrite.CoalIntCount);\n\t} else\n\t\tif (trans_support & CFGTBL_Trans_io_accel2)\n\t\t\taccess = SA5_ioaccel_mode2_access;\n\twritel(CFGTBL_ChangeReq, h->vaddr + SA5_DOORBELL);\n\tif (hpsa_wait_for_mode_change_ack(h)) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"performant mode problem - doorbell timeout\\n\");\n\t\treturn -ENODEV;\n\t}\n\tregister_value = readl(&(h->cfgtable->TransportActive));\n\tif (!(register_value & CFGTBL_Trans_Performant)) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"performant mode problem - transport not active\\n\");\n\t\treturn -ENODEV;\n\t}\n\t \n\th->access = access;\n\th->transMethod = transMethod;\n\n\tif (!((trans_support & CFGTBL_Trans_io_accel1) ||\n\t\t(trans_support & CFGTBL_Trans_io_accel2)))\n\t\treturn 0;\n\n\tif (trans_support & CFGTBL_Trans_io_accel1) {\n\t\t \n\t\tfor (i = 0; i < h->nreply_queues; i++) {\n\t\t\twritel(i, h->vaddr + IOACCEL_MODE1_REPLY_QUEUE_INDEX);\n\t\t\th->reply_queue[i].current_entry =\n\t\t\t\treadl(h->vaddr + IOACCEL_MODE1_PRODUCER_INDEX);\n\t\t}\n\t\tbft[7] = h->ioaccel_maxsg + 8;\n\t\tcalc_bucket_map(bft, ARRAY_SIZE(bft), h->ioaccel_maxsg, 8,\n\t\t\t\th->ioaccel1_blockFetchTable);\n\n\t\t \n\t\tfor (i = 0; i < h->nreply_queues; i++)\n\t\t\tmemset(h->reply_queue[i].head,\n\t\t\t\t(u8) IOACCEL_MODE1_REPLY_UNUSED,\n\t\t\t\th->reply_queue_size);\n\n\t\t \n\t\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\t\tstruct io_accel1_cmd *cp = &h->ioaccel_cmd_pool[i];\n\n\t\t\tcp->function = IOACCEL1_FUNCTION_SCSIIO;\n\t\t\tcp->err_info = (u32) (h->errinfo_pool_dhandle +\n\t\t\t\t\t(i * sizeof(struct ErrorInfo)));\n\t\t\tcp->err_info_len = sizeof(struct ErrorInfo);\n\t\t\tcp->sgl_offset = IOACCEL1_SGLOFFSET;\n\t\t\tcp->host_context_flags =\n\t\t\t\tcpu_to_le16(IOACCEL1_HCFLAGS_CISS_FORMAT);\n\t\t\tcp->timeout_sec = 0;\n\t\t\tcp->ReplyQueue = 0;\n\t\t\tcp->tag =\n\t\t\t\tcpu_to_le64((i << DIRECT_LOOKUP_SHIFT));\n\t\t\tcp->host_addr =\n\t\t\t\tcpu_to_le64(h->ioaccel_cmd_pool_dhandle +\n\t\t\t\t\t(i * sizeof(struct io_accel1_cmd)));\n\t\t}\n\t} else if (trans_support & CFGTBL_Trans_io_accel2) {\n\t\tu64 cfg_offset, cfg_base_addr_index;\n\t\tu32 bft2_offset, cfg_base_addr;\n\n\t\thpsa_find_cfg_addrs(h->pdev, h->vaddr, &cfg_base_addr,\n\t\t\t\t    &cfg_base_addr_index, &cfg_offset);\n\t\tBUILD_BUG_ON(offsetof(struct io_accel2_cmd, sg) != 64);\n\t\tbft2[15] = h->ioaccel_maxsg + HPSA_IOACCEL2_HEADER_SZ;\n\t\tcalc_bucket_map(bft2, ARRAY_SIZE(bft2), h->ioaccel_maxsg,\n\t\t\t\t4, h->ioaccel2_blockFetchTable);\n\t\tbft2_offset = readl(&h->cfgtable->io_accel_request_size_offset);\n\t\tBUILD_BUG_ON(offsetof(struct CfgTable,\n\t\t\t\tio_accel_request_size_offset) != 0xb8);\n\t\th->ioaccel2_bft2_regs =\n\t\t\tremap_pci_mem(pci_resource_start(h->pdev,\n\t\t\t\t\tcfg_base_addr_index) +\n\t\t\t\t\tcfg_offset + bft2_offset,\n\t\t\t\t\tARRAY_SIZE(bft2) *\n\t\t\t\t\tsizeof(*h->ioaccel2_bft2_regs));\n\t\tfor (i = 0; i < ARRAY_SIZE(bft2); i++)\n\t\t\twritel(bft2[i], &h->ioaccel2_bft2_regs[i]);\n\t}\n\twritel(CFGTBL_ChangeReq, h->vaddr + SA5_DOORBELL);\n\tif (hpsa_wait_for_mode_change_ack(h)) {\n\t\tdev_err(&h->pdev->dev,\n\t\t\t\"performant mode problem - enabling ioaccel mode\\n\");\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\n \nstatic void hpsa_free_ioaccel1_cmd_and_bft(struct ctlr_info *h)\n{\n\tif (h->ioaccel_cmd_pool) {\n\t\tdma_free_coherent(&h->pdev->dev,\n\t\t\t\t  h->nr_cmds * sizeof(*h->ioaccel_cmd_pool),\n\t\t\t\t  h->ioaccel_cmd_pool,\n\t\t\t\t  h->ioaccel_cmd_pool_dhandle);\n\t\th->ioaccel_cmd_pool = NULL;\n\t\th->ioaccel_cmd_pool_dhandle = 0;\n\t}\n\tkfree(h->ioaccel1_blockFetchTable);\n\th->ioaccel1_blockFetchTable = NULL;\n}\n\n \nstatic int hpsa_alloc_ioaccel1_cmd_and_bft(struct ctlr_info *h)\n{\n\th->ioaccel_maxsg =\n\t\treadl(&(h->cfgtable->io_accel_max_embedded_sg_count));\n\tif (h->ioaccel_maxsg > IOACCEL1_MAXSGENTRIES)\n\t\th->ioaccel_maxsg = IOACCEL1_MAXSGENTRIES;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct io_accel1_cmd) %\n\t\t\tIOACCEL1_COMMANDLIST_ALIGNMENT);\n\th->ioaccel_cmd_pool =\n\t\tdma_alloc_coherent(&h->pdev->dev,\n\t\t\th->nr_cmds * sizeof(*h->ioaccel_cmd_pool),\n\t\t\t&h->ioaccel_cmd_pool_dhandle, GFP_KERNEL);\n\n\th->ioaccel1_blockFetchTable =\n\t\tkmalloc(((h->ioaccel_maxsg + 1) *\n\t\t\t\tsizeof(u32)), GFP_KERNEL);\n\n\tif ((h->ioaccel_cmd_pool == NULL) ||\n\t\t(h->ioaccel1_blockFetchTable == NULL))\n\t\tgoto clean_up;\n\n\tmemset(h->ioaccel_cmd_pool, 0,\n\t\th->nr_cmds * sizeof(*h->ioaccel_cmd_pool));\n\treturn 0;\n\nclean_up:\n\thpsa_free_ioaccel1_cmd_and_bft(h);\n\treturn -ENOMEM;\n}\n\n \nstatic void hpsa_free_ioaccel2_cmd_and_bft(struct ctlr_info *h)\n{\n\thpsa_free_ioaccel2_sg_chain_blocks(h);\n\n\tif (h->ioaccel2_cmd_pool) {\n\t\tdma_free_coherent(&h->pdev->dev,\n\t\t\t\t  h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool),\n\t\t\t\t  h->ioaccel2_cmd_pool,\n\t\t\t\t  h->ioaccel2_cmd_pool_dhandle);\n\t\th->ioaccel2_cmd_pool = NULL;\n\t\th->ioaccel2_cmd_pool_dhandle = 0;\n\t}\n\tkfree(h->ioaccel2_blockFetchTable);\n\th->ioaccel2_blockFetchTable = NULL;\n}\n\n \nstatic int hpsa_alloc_ioaccel2_cmd_and_bft(struct ctlr_info *h)\n{\n\tint rc;\n\n\t \n\n\th->ioaccel_maxsg =\n\t\treadl(&(h->cfgtable->io_accel_max_embedded_sg_count));\n\tif (h->ioaccel_maxsg > IOACCEL2_MAXSGENTRIES)\n\t\th->ioaccel_maxsg = IOACCEL2_MAXSGENTRIES;\n\n\tBUILD_BUG_ON(sizeof(struct io_accel2_cmd) %\n\t\t\tIOACCEL2_COMMANDLIST_ALIGNMENT);\n\th->ioaccel2_cmd_pool =\n\t\tdma_alloc_coherent(&h->pdev->dev,\n\t\t\th->nr_cmds * sizeof(*h->ioaccel2_cmd_pool),\n\t\t\t&h->ioaccel2_cmd_pool_dhandle, GFP_KERNEL);\n\n\th->ioaccel2_blockFetchTable =\n\t\tkmalloc(((h->ioaccel_maxsg + 1) *\n\t\t\t\tsizeof(u32)), GFP_KERNEL);\n\n\tif ((h->ioaccel2_cmd_pool == NULL) ||\n\t\t(h->ioaccel2_blockFetchTable == NULL)) {\n\t\trc = -ENOMEM;\n\t\tgoto clean_up;\n\t}\n\n\trc = hpsa_allocate_ioaccel2_sg_chain_blocks(h);\n\tif (rc)\n\t\tgoto clean_up;\n\n\tmemset(h->ioaccel2_cmd_pool, 0,\n\t\th->nr_cmds * sizeof(*h->ioaccel2_cmd_pool));\n\treturn 0;\n\nclean_up:\n\thpsa_free_ioaccel2_cmd_and_bft(h);\n\treturn rc;\n}\n\n \nstatic void hpsa_free_performant_mode(struct ctlr_info *h)\n{\n\tkfree(h->blockFetchTable);\n\th->blockFetchTable = NULL;\n\thpsa_free_reply_queues(h);\n\thpsa_free_ioaccel1_cmd_and_bft(h);\n\thpsa_free_ioaccel2_cmd_and_bft(h);\n}\n\n \nstatic int hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)\n{\n\tu32 trans_support;\n\tint i, rc;\n\n\tif (hpsa_simple_mode)\n\t\treturn 0;\n\n\ttrans_support = readl(&(h->cfgtable->TransportSupport));\n\tif (!(trans_support & PERFORMANT_MODE))\n\t\treturn 0;\n\n\t \n\tif (trans_support & CFGTBL_Trans_io_accel1) {\n\t\trc = hpsa_alloc_ioaccel1_cmd_and_bft(h);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else if (trans_support & CFGTBL_Trans_io_accel2) {\n\t\trc = hpsa_alloc_ioaccel2_cmd_and_bft(h);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\th->nreply_queues = h->msix_vectors > 0 ? h->msix_vectors : 1;\n\thpsa_get_max_perf_mode_cmds(h);\n\t \n\th->reply_queue_size = h->max_commands * sizeof(u64);\n\n\tfor (i = 0; i < h->nreply_queues; i++) {\n\t\th->reply_queue[i].head = dma_alloc_coherent(&h->pdev->dev,\n\t\t\t\t\t\th->reply_queue_size,\n\t\t\t\t\t\t&h->reply_queue[i].busaddr,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!h->reply_queue[i].head) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto clean1;\t \n\t\t}\n\t\th->reply_queue[i].size = h->max_commands;\n\t\th->reply_queue[i].wraparound = 1;   \n\t\th->reply_queue[i].current_entry = 0;\n\t}\n\n\t \n\th->blockFetchTable = kmalloc(((SG_ENTRIES_IN_CMD + 1) *\n\t\t\t\tsizeof(u32)), GFP_KERNEL);\n\tif (!h->blockFetchTable) {\n\t\trc = -ENOMEM;\n\t\tgoto clean1;\t \n\t}\n\n\trc = hpsa_enter_performant_mode(h, trans_support);\n\tif (rc)\n\t\tgoto clean2;\t \n\treturn 0;\n\nclean2:\t \n\tkfree(h->blockFetchTable);\n\th->blockFetchTable = NULL;\nclean1:\t \n\thpsa_free_reply_queues(h);\n\thpsa_free_ioaccel1_cmd_and_bft(h);\n\thpsa_free_ioaccel2_cmd_and_bft(h);\n\treturn rc;\n}\n\nstatic int is_accelerated_cmd(struct CommandList *c)\n{\n\treturn c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_IOACCEL2;\n}\n\nstatic void hpsa_drain_accel_commands(struct ctlr_info *h)\n{\n\tstruct CommandList *c = NULL;\n\tint i, accel_cmds_out;\n\tint refcount;\n\n\tdo {  \n\t\taccel_cmds_out = 0;\n\t\tfor (i = 0; i < h->nr_cmds; i++) {\n\t\t\tc = h->cmd_pool + i;\n\t\t\trefcount = atomic_inc_return(&c->refcount);\n\t\t\tif (refcount > 1)  \n\t\t\t\taccel_cmds_out += is_accelerated_cmd(c);\n\t\t\tcmd_free(h, c);\n\t\t}\n\t\tif (accel_cmds_out <= 0)\n\t\t\tbreak;\n\t\tmsleep(100);\n\t} while (1);\n}\n\nstatic struct hpsa_sas_phy *hpsa_alloc_sas_phy(\n\t\t\t\tstruct hpsa_sas_port *hpsa_sas_port)\n{\n\tstruct hpsa_sas_phy *hpsa_sas_phy;\n\tstruct sas_phy *phy;\n\n\thpsa_sas_phy = kzalloc(sizeof(*hpsa_sas_phy), GFP_KERNEL);\n\tif (!hpsa_sas_phy)\n\t\treturn NULL;\n\n\tphy = sas_phy_alloc(hpsa_sas_port->parent_node->parent_dev,\n\t\thpsa_sas_port->next_phy_index);\n\tif (!phy) {\n\t\tkfree(hpsa_sas_phy);\n\t\treturn NULL;\n\t}\n\n\thpsa_sas_port->next_phy_index++;\n\thpsa_sas_phy->phy = phy;\n\thpsa_sas_phy->parent_port = hpsa_sas_port;\n\n\treturn hpsa_sas_phy;\n}\n\nstatic void hpsa_free_sas_phy(struct hpsa_sas_phy *hpsa_sas_phy)\n{\n\tstruct sas_phy *phy = hpsa_sas_phy->phy;\n\n\tsas_port_delete_phy(hpsa_sas_phy->parent_port->port, phy);\n\tif (hpsa_sas_phy->added_to_port)\n\t\tlist_del(&hpsa_sas_phy->phy_list_entry);\n\tsas_phy_delete(phy);\n\tkfree(hpsa_sas_phy);\n}\n\nstatic int hpsa_sas_port_add_phy(struct hpsa_sas_phy *hpsa_sas_phy)\n{\n\tint rc;\n\tstruct hpsa_sas_port *hpsa_sas_port;\n\tstruct sas_phy *phy;\n\tstruct sas_identify *identify;\n\n\thpsa_sas_port = hpsa_sas_phy->parent_port;\n\tphy = hpsa_sas_phy->phy;\n\n\tidentify = &phy->identify;\n\tmemset(identify, 0, sizeof(*identify));\n\tidentify->sas_address = hpsa_sas_port->sas_address;\n\tidentify->device_type = SAS_END_DEVICE;\n\tidentify->initiator_port_protocols = SAS_PROTOCOL_STP;\n\tidentify->target_port_protocols = SAS_PROTOCOL_STP;\n\tphy->minimum_linkrate_hw = SAS_LINK_RATE_UNKNOWN;\n\tphy->maximum_linkrate_hw = SAS_LINK_RATE_UNKNOWN;\n\tphy->minimum_linkrate = SAS_LINK_RATE_UNKNOWN;\n\tphy->maximum_linkrate = SAS_LINK_RATE_UNKNOWN;\n\tphy->negotiated_linkrate = SAS_LINK_RATE_UNKNOWN;\n\n\trc = sas_phy_add(hpsa_sas_phy->phy);\n\tif (rc)\n\t\treturn rc;\n\n\tsas_port_add_phy(hpsa_sas_port->port, hpsa_sas_phy->phy);\n\tlist_add_tail(&hpsa_sas_phy->phy_list_entry,\n\t\t\t&hpsa_sas_port->phy_list_head);\n\thpsa_sas_phy->added_to_port = true;\n\n\treturn 0;\n}\n\nstatic int\n\thpsa_sas_port_add_rphy(struct hpsa_sas_port *hpsa_sas_port,\n\t\t\t\tstruct sas_rphy *rphy)\n{\n\tstruct sas_identify *identify;\n\n\tidentify = &rphy->identify;\n\tidentify->sas_address = hpsa_sas_port->sas_address;\n\tidentify->initiator_port_protocols = SAS_PROTOCOL_STP;\n\tidentify->target_port_protocols = SAS_PROTOCOL_STP;\n\n\treturn sas_rphy_add(rphy);\n}\n\nstatic struct hpsa_sas_port\n\t*hpsa_alloc_sas_port(struct hpsa_sas_node *hpsa_sas_node,\n\t\t\t\tu64 sas_address)\n{\n\tint rc;\n\tstruct hpsa_sas_port *hpsa_sas_port;\n\tstruct sas_port *port;\n\n\thpsa_sas_port = kzalloc(sizeof(*hpsa_sas_port), GFP_KERNEL);\n\tif (!hpsa_sas_port)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&hpsa_sas_port->phy_list_head);\n\thpsa_sas_port->parent_node = hpsa_sas_node;\n\n\tport = sas_port_alloc_num(hpsa_sas_node->parent_dev);\n\tif (!port)\n\t\tgoto free_hpsa_port;\n\n\trc = sas_port_add(port);\n\tif (rc)\n\t\tgoto free_sas_port;\n\n\thpsa_sas_port->port = port;\n\thpsa_sas_port->sas_address = sas_address;\n\tlist_add_tail(&hpsa_sas_port->port_list_entry,\n\t\t\t&hpsa_sas_node->port_list_head);\n\n\treturn hpsa_sas_port;\n\nfree_sas_port:\n\tsas_port_free(port);\nfree_hpsa_port:\n\tkfree(hpsa_sas_port);\n\n\treturn NULL;\n}\n\nstatic void hpsa_free_sas_port(struct hpsa_sas_port *hpsa_sas_port)\n{\n\tstruct hpsa_sas_phy *hpsa_sas_phy;\n\tstruct hpsa_sas_phy *next;\n\n\tlist_for_each_entry_safe(hpsa_sas_phy, next,\n\t\t\t&hpsa_sas_port->phy_list_head, phy_list_entry)\n\t\thpsa_free_sas_phy(hpsa_sas_phy);\n\n\tsas_port_delete(hpsa_sas_port->port);\n\tlist_del(&hpsa_sas_port->port_list_entry);\n\tkfree(hpsa_sas_port);\n}\n\nstatic struct hpsa_sas_node *hpsa_alloc_sas_node(struct device *parent_dev)\n{\n\tstruct hpsa_sas_node *hpsa_sas_node;\n\n\thpsa_sas_node = kzalloc(sizeof(*hpsa_sas_node), GFP_KERNEL);\n\tif (hpsa_sas_node) {\n\t\thpsa_sas_node->parent_dev = parent_dev;\n\t\tINIT_LIST_HEAD(&hpsa_sas_node->port_list_head);\n\t}\n\n\treturn hpsa_sas_node;\n}\n\nstatic void hpsa_free_sas_node(struct hpsa_sas_node *hpsa_sas_node)\n{\n\tstruct hpsa_sas_port *hpsa_sas_port;\n\tstruct hpsa_sas_port *next;\n\n\tif (!hpsa_sas_node)\n\t\treturn;\n\n\tlist_for_each_entry_safe(hpsa_sas_port, next,\n\t\t\t&hpsa_sas_node->port_list_head, port_list_entry)\n\t\thpsa_free_sas_port(hpsa_sas_port);\n\n\tkfree(hpsa_sas_node);\n}\n\nstatic struct hpsa_scsi_dev_t\n\t*hpsa_find_device_by_sas_rphy(struct ctlr_info *h,\n\t\t\t\t\tstruct sas_rphy *rphy)\n{\n\tint i;\n\tstruct hpsa_scsi_dev_t *device;\n\n\tfor (i = 0; i < h->ndevices; i++) {\n\t\tdevice = h->dev[i];\n\t\tif (!device->sas_port)\n\t\t\tcontinue;\n\t\tif (device->sas_port->rphy == rphy)\n\t\t\treturn device;\n\t}\n\n\treturn NULL;\n}\n\nstatic int hpsa_add_sas_host(struct ctlr_info *h)\n{\n\tint rc;\n\tstruct device *parent_dev;\n\tstruct hpsa_sas_node *hpsa_sas_node;\n\tstruct hpsa_sas_port *hpsa_sas_port;\n\tstruct hpsa_sas_phy *hpsa_sas_phy;\n\n\tparent_dev = &h->scsi_host->shost_dev;\n\n\thpsa_sas_node = hpsa_alloc_sas_node(parent_dev);\n\tif (!hpsa_sas_node)\n\t\treturn -ENOMEM;\n\n\thpsa_sas_port = hpsa_alloc_sas_port(hpsa_sas_node, h->sas_address);\n\tif (!hpsa_sas_port) {\n\t\trc = -ENODEV;\n\t\tgoto free_sas_node;\n\t}\n\n\thpsa_sas_phy = hpsa_alloc_sas_phy(hpsa_sas_port);\n\tif (!hpsa_sas_phy) {\n\t\trc = -ENODEV;\n\t\tgoto free_sas_port;\n\t}\n\n\trc = hpsa_sas_port_add_phy(hpsa_sas_phy);\n\tif (rc)\n\t\tgoto free_sas_phy;\n\n\th->sas_host = hpsa_sas_node;\n\n\treturn 0;\n\nfree_sas_phy:\n\tsas_phy_free(hpsa_sas_phy->phy);\n\tkfree(hpsa_sas_phy);\nfree_sas_port:\n\thpsa_free_sas_port(hpsa_sas_port);\nfree_sas_node:\n\thpsa_free_sas_node(hpsa_sas_node);\n\n\treturn rc;\n}\n\nstatic void hpsa_delete_sas_host(struct ctlr_info *h)\n{\n\thpsa_free_sas_node(h->sas_host);\n}\n\nstatic int hpsa_add_sas_device(struct hpsa_sas_node *hpsa_sas_node,\n\t\t\t\tstruct hpsa_scsi_dev_t *device)\n{\n\tint rc;\n\tstruct hpsa_sas_port *hpsa_sas_port;\n\tstruct sas_rphy *rphy;\n\n\thpsa_sas_port = hpsa_alloc_sas_port(hpsa_sas_node, device->sas_address);\n\tif (!hpsa_sas_port)\n\t\treturn -ENOMEM;\n\n\trphy = sas_end_device_alloc(hpsa_sas_port->port);\n\tif (!rphy) {\n\t\trc = -ENODEV;\n\t\tgoto free_sas_port;\n\t}\n\n\thpsa_sas_port->rphy = rphy;\n\tdevice->sas_port = hpsa_sas_port;\n\n\trc = hpsa_sas_port_add_rphy(hpsa_sas_port, rphy);\n\tif (rc)\n\t\tgoto free_sas_rphy;\n\n\treturn 0;\n\nfree_sas_rphy:\n\tsas_rphy_free(rphy);\nfree_sas_port:\n\thpsa_free_sas_port(hpsa_sas_port);\n\tdevice->sas_port = NULL;\n\n\treturn rc;\n}\n\nstatic void hpsa_remove_sas_device(struct hpsa_scsi_dev_t *device)\n{\n\tif (device->sas_port) {\n\t\thpsa_free_sas_port(device->sas_port);\n\t\tdevice->sas_port = NULL;\n\t}\n}\n\nstatic int\nhpsa_sas_get_linkerrors(struct sas_phy *phy)\n{\n\treturn 0;\n}\n\nstatic int\nhpsa_sas_get_enclosure_identifier(struct sas_rphy *rphy, u64 *identifier)\n{\n\tstruct Scsi_Host *shost = phy_to_shost(rphy);\n\tstruct ctlr_info *h;\n\tstruct hpsa_scsi_dev_t *sd;\n\n\tif (!shost)\n\t\treturn -ENXIO;\n\n\th = shost_to_hba(shost);\n\n\tif (!h)\n\t\treturn -ENXIO;\n\n\tsd = hpsa_find_device_by_sas_rphy(h, rphy);\n\tif (!sd)\n\t\treturn -ENXIO;\n\n\t*identifier = sd->eli;\n\n\treturn 0;\n}\n\nstatic int\nhpsa_sas_get_bay_identifier(struct sas_rphy *rphy)\n{\n\treturn -ENXIO;\n}\n\nstatic int\nhpsa_sas_phy_reset(struct sas_phy *phy, int hard_reset)\n{\n\treturn 0;\n}\n\nstatic int\nhpsa_sas_phy_enable(struct sas_phy *phy, int enable)\n{\n\treturn 0;\n}\n\nstatic int\nhpsa_sas_phy_setup(struct sas_phy *phy)\n{\n\treturn 0;\n}\n\nstatic void\nhpsa_sas_phy_release(struct sas_phy *phy)\n{\n}\n\nstatic int\nhpsa_sas_phy_speed(struct sas_phy *phy, struct sas_phy_linkrates *rates)\n{\n\treturn -EINVAL;\n}\n\nstatic struct sas_function_template hpsa_sas_transport_functions = {\n\t.get_linkerrors = hpsa_sas_get_linkerrors,\n\t.get_enclosure_identifier = hpsa_sas_get_enclosure_identifier,\n\t.get_bay_identifier = hpsa_sas_get_bay_identifier,\n\t.phy_reset = hpsa_sas_phy_reset,\n\t.phy_enable = hpsa_sas_phy_enable,\n\t.phy_setup = hpsa_sas_phy_setup,\n\t.phy_release = hpsa_sas_phy_release,\n\t.set_phy_speed = hpsa_sas_phy_speed,\n};\n\n \nstatic int __init hpsa_init(void)\n{\n\tint rc;\n\n\thpsa_sas_transport_template =\n\t\tsas_attach_transport(&hpsa_sas_transport_functions);\n\tif (!hpsa_sas_transport_template)\n\t\treturn -ENODEV;\n\n\trc = pci_register_driver(&hpsa_pci_driver);\n\n\tif (rc)\n\t\tsas_release_transport(hpsa_sas_transport_template);\n\n\treturn rc;\n}\n\nstatic void __exit hpsa_cleanup(void)\n{\n\tpci_unregister_driver(&hpsa_pci_driver);\n\tsas_release_transport(hpsa_sas_transport_template);\n}\n\nstatic void __attribute__((unused)) verify_offsets(void)\n{\n#define VERIFY_OFFSET(member, offset) \\\n\tBUILD_BUG_ON(offsetof(struct raid_map_data, member) != offset)\n\n\tVERIFY_OFFSET(structure_size, 0);\n\tVERIFY_OFFSET(volume_blk_size, 4);\n\tVERIFY_OFFSET(volume_blk_cnt, 8);\n\tVERIFY_OFFSET(phys_blk_shift, 16);\n\tVERIFY_OFFSET(parity_rotation_shift, 17);\n\tVERIFY_OFFSET(strip_size, 18);\n\tVERIFY_OFFSET(disk_starting_blk, 20);\n\tVERIFY_OFFSET(disk_blk_cnt, 28);\n\tVERIFY_OFFSET(data_disks_per_row, 36);\n\tVERIFY_OFFSET(metadata_disks_per_row, 38);\n\tVERIFY_OFFSET(row_cnt, 40);\n\tVERIFY_OFFSET(layout_map_count, 42);\n\tVERIFY_OFFSET(flags, 44);\n\tVERIFY_OFFSET(dekindex, 46);\n\t \n\tVERIFY_OFFSET(data, 64);\n\n#undef VERIFY_OFFSET\n\n#define VERIFY_OFFSET(member, offset) \\\n\tBUILD_BUG_ON(offsetof(struct io_accel2_cmd, member) != offset)\n\n\tVERIFY_OFFSET(IU_type, 0);\n\tVERIFY_OFFSET(direction, 1);\n\tVERIFY_OFFSET(reply_queue, 2);\n\t \n\tVERIFY_OFFSET(scsi_nexus, 4);\n\tVERIFY_OFFSET(Tag, 8);\n\tVERIFY_OFFSET(cdb, 16);\n\tVERIFY_OFFSET(cciss_lun, 32);\n\tVERIFY_OFFSET(data_len, 40);\n\tVERIFY_OFFSET(cmd_priority_task_attr, 44);\n\tVERIFY_OFFSET(sg_count, 45);\n\t \n\tVERIFY_OFFSET(err_ptr, 48);\n\tVERIFY_OFFSET(err_len, 56);\n\t \n\tVERIFY_OFFSET(sg, 64);\n\n#undef VERIFY_OFFSET\n\n#define VERIFY_OFFSET(member, offset) \\\n\tBUILD_BUG_ON(offsetof(struct io_accel1_cmd, member) != offset)\n\n\tVERIFY_OFFSET(dev_handle, 0x00);\n\tVERIFY_OFFSET(reserved1, 0x02);\n\tVERIFY_OFFSET(function, 0x03);\n\tVERIFY_OFFSET(reserved2, 0x04);\n\tVERIFY_OFFSET(err_info, 0x0C);\n\tVERIFY_OFFSET(reserved3, 0x10);\n\tVERIFY_OFFSET(err_info_len, 0x12);\n\tVERIFY_OFFSET(reserved4, 0x13);\n\tVERIFY_OFFSET(sgl_offset, 0x14);\n\tVERIFY_OFFSET(reserved5, 0x15);\n\tVERIFY_OFFSET(transfer_len, 0x1C);\n\tVERIFY_OFFSET(reserved6, 0x20);\n\tVERIFY_OFFSET(io_flags, 0x24);\n\tVERIFY_OFFSET(reserved7, 0x26);\n\tVERIFY_OFFSET(LUN, 0x34);\n\tVERIFY_OFFSET(control, 0x3C);\n\tVERIFY_OFFSET(CDB, 0x40);\n\tVERIFY_OFFSET(reserved8, 0x50);\n\tVERIFY_OFFSET(host_context_flags, 0x60);\n\tVERIFY_OFFSET(timeout_sec, 0x62);\n\tVERIFY_OFFSET(ReplyQueue, 0x64);\n\tVERIFY_OFFSET(reserved9, 0x65);\n\tVERIFY_OFFSET(tag, 0x68);\n\tVERIFY_OFFSET(host_addr, 0x70);\n\tVERIFY_OFFSET(CISS_LUN, 0x78);\n\tVERIFY_OFFSET(SG, 0x78 + 8);\n#undef VERIFY_OFFSET\n}\n\nmodule_init(hpsa_init);\nmodule_exit(hpsa_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}