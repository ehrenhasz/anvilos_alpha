{
  "module_name": "ipr.c",
  "hash_id": "41eac5adf8c033ce17398abf355afd1446ca57f534d997d1a4c6d15d501592a7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/ipr.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/ioport.h>\n#include <linux/delay.h>\n#include <linux/pci.h>\n#include <linux/wait.h>\n#include <linux/spinlock.h>\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/blkdev.h>\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/hdreg.h>\n#include <linux/reboot.h>\n#include <linux/stringify.h>\n#include <asm/io.h>\n#include <asm/irq.h>\n#include <asm/processor.h>\n#include <scsi/scsi.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_tcq.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_cmnd.h>\n#include \"ipr.h\"\n\n \nstatic LIST_HEAD(ipr_ioa_head);\nstatic unsigned int ipr_log_level = IPR_DEFAULT_LOG_LEVEL;\nstatic unsigned int ipr_max_speed = 1;\nstatic int ipr_testmode = 0;\nstatic unsigned int ipr_fastfail = 0;\nstatic unsigned int ipr_transop_timeout = 0;\nstatic unsigned int ipr_debug = 0;\nstatic unsigned int ipr_max_devs = IPR_DEFAULT_SIS64_DEVS;\nstatic unsigned int ipr_dual_ioa_raid = 1;\nstatic unsigned int ipr_number_of_msix = 16;\nstatic unsigned int ipr_fast_reboot;\nstatic DEFINE_SPINLOCK(ipr_driver_lock);\n\n \nstatic const struct ipr_chip_cfg_t ipr_chip_cfg[] = {\n\t{  \n\t\t.mailbox = 0x0042C,\n\t\t.max_cmds = 100,\n\t\t.cache_line_size = 0x20,\n\t\t.clear_isr = 1,\n\t\t.iopoll_weight = 0,\n\t\t{\n\t\t\t.set_interrupt_mask_reg = 0x0022C,\n\t\t\t.clr_interrupt_mask_reg = 0x00230,\n\t\t\t.clr_interrupt_mask_reg32 = 0x00230,\n\t\t\t.sense_interrupt_mask_reg = 0x0022C,\n\t\t\t.sense_interrupt_mask_reg32 = 0x0022C,\n\t\t\t.clr_interrupt_reg = 0x00228,\n\t\t\t.clr_interrupt_reg32 = 0x00228,\n\t\t\t.sense_interrupt_reg = 0x00224,\n\t\t\t.sense_interrupt_reg32 = 0x00224,\n\t\t\t.ioarrin_reg = 0x00404,\n\t\t\t.sense_uproc_interrupt_reg = 0x00214,\n\t\t\t.sense_uproc_interrupt_reg32 = 0x00214,\n\t\t\t.set_uproc_interrupt_reg = 0x00214,\n\t\t\t.set_uproc_interrupt_reg32 = 0x00214,\n\t\t\t.clr_uproc_interrupt_reg = 0x00218,\n\t\t\t.clr_uproc_interrupt_reg32 = 0x00218\n\t\t}\n\t},\n\t{  \n\t\t.mailbox = 0x0052C,\n\t\t.max_cmds = 100,\n\t\t.cache_line_size = 0x20,\n\t\t.clear_isr = 1,\n\t\t.iopoll_weight = 0,\n\t\t{\n\t\t\t.set_interrupt_mask_reg = 0x00288,\n\t\t\t.clr_interrupt_mask_reg = 0x0028C,\n\t\t\t.clr_interrupt_mask_reg32 = 0x0028C,\n\t\t\t.sense_interrupt_mask_reg = 0x00288,\n\t\t\t.sense_interrupt_mask_reg32 = 0x00288,\n\t\t\t.clr_interrupt_reg = 0x00284,\n\t\t\t.clr_interrupt_reg32 = 0x00284,\n\t\t\t.sense_interrupt_reg = 0x00280,\n\t\t\t.sense_interrupt_reg32 = 0x00280,\n\t\t\t.ioarrin_reg = 0x00504,\n\t\t\t.sense_uproc_interrupt_reg = 0x00290,\n\t\t\t.sense_uproc_interrupt_reg32 = 0x00290,\n\t\t\t.set_uproc_interrupt_reg = 0x00290,\n\t\t\t.set_uproc_interrupt_reg32 = 0x00290,\n\t\t\t.clr_uproc_interrupt_reg = 0x00294,\n\t\t\t.clr_uproc_interrupt_reg32 = 0x00294\n\t\t}\n\t},\n\t{  \n\t\t.mailbox = 0x00044,\n\t\t.max_cmds = 1000,\n\t\t.cache_line_size = 0x20,\n\t\t.clear_isr = 0,\n\t\t.iopoll_weight = 64,\n\t\t{\n\t\t\t.set_interrupt_mask_reg = 0x00010,\n\t\t\t.clr_interrupt_mask_reg = 0x00018,\n\t\t\t.clr_interrupt_mask_reg32 = 0x0001C,\n\t\t\t.sense_interrupt_mask_reg = 0x00010,\n\t\t\t.sense_interrupt_mask_reg32 = 0x00014,\n\t\t\t.clr_interrupt_reg = 0x00008,\n\t\t\t.clr_interrupt_reg32 = 0x0000C,\n\t\t\t.sense_interrupt_reg = 0x00000,\n\t\t\t.sense_interrupt_reg32 = 0x00004,\n\t\t\t.ioarrin_reg = 0x00070,\n\t\t\t.sense_uproc_interrupt_reg = 0x00020,\n\t\t\t.sense_uproc_interrupt_reg32 = 0x00024,\n\t\t\t.set_uproc_interrupt_reg = 0x00020,\n\t\t\t.set_uproc_interrupt_reg32 = 0x00024,\n\t\t\t.clr_uproc_interrupt_reg = 0x00028,\n\t\t\t.clr_uproc_interrupt_reg32 = 0x0002C,\n\t\t\t.init_feedback_reg = 0x0005C,\n\t\t\t.dump_addr_reg = 0x00064,\n\t\t\t.dump_data_reg = 0x00068,\n\t\t\t.endian_swap_reg = 0x00084\n\t\t}\n\t},\n};\n\nstatic const struct ipr_chip_t ipr_chip[] = {\n\t{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[0] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CITRINE, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[0] },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_OBSIDIAN, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[0] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[0] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN_E, true, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[0] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_SNIPE, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[1] },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_SCAMP, false, IPR_SIS32, IPR_PCI_CFG, &ipr_chip_cfg[1] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2, true, IPR_SIS64, IPR_MMIO, &ipr_chip_cfg[2] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE, true, IPR_SIS64, IPR_MMIO, &ipr_chip_cfg[2] },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_RATTLESNAKE, true, IPR_SIS64, IPR_MMIO, &ipr_chip_cfg[2] }\n};\n\nstatic int ipr_max_bus_speeds[] = {\n\tIPR_80MBs_SCSI_RATE, IPR_U160_SCSI_RATE, IPR_U320_SCSI_RATE\n};\n\nMODULE_AUTHOR(\"Brian King <brking@us.ibm.com>\");\nMODULE_DESCRIPTION(\"IBM Power RAID SCSI Adapter Driver\");\nmodule_param_named(max_speed, ipr_max_speed, uint, 0);\nMODULE_PARM_DESC(max_speed, \"Maximum bus speed (0-2). Default: 1=U160. Speeds: 0=80 MB/s, 1=U160, 2=U320\");\nmodule_param_named(log_level, ipr_log_level, uint, 0);\nMODULE_PARM_DESC(log_level, \"Set to 0 - 4 for increasing verbosity of device driver\");\nmodule_param_named(testmode, ipr_testmode, int, 0);\nMODULE_PARM_DESC(testmode, \"DANGEROUS!!! Allows unsupported configurations\");\nmodule_param_named(fastfail, ipr_fastfail, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(fastfail, \"Reduce timeouts and retries\");\nmodule_param_named(transop_timeout, ipr_transop_timeout, int, 0);\nMODULE_PARM_DESC(transop_timeout, \"Time in seconds to wait for adapter to come operational (default: 300)\");\nmodule_param_named(debug, ipr_debug, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(debug, \"Enable device driver debugging logging. Set to 1 to enable. (default: 0)\");\nmodule_param_named(dual_ioa_raid, ipr_dual_ioa_raid, int, 0);\nMODULE_PARM_DESC(dual_ioa_raid, \"Enable dual adapter RAID support. Set to 1 to enable. (default: 1)\");\nmodule_param_named(max_devs, ipr_max_devs, int, 0);\nMODULE_PARM_DESC(max_devs, \"Specify the maximum number of physical devices. \"\n\t\t \"[Default=\" __stringify(IPR_DEFAULT_SIS64_DEVS) \"]\");\nmodule_param_named(number_of_msix, ipr_number_of_msix, int, 0);\nMODULE_PARM_DESC(number_of_msix, \"Specify the number of MSIX interrupts to use on capable adapters (1 - 16).  (default:16)\");\nmodule_param_named(fast_reboot, ipr_fast_reboot, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(fast_reboot, \"Skip adapter shutdown during reboot. Set to 1 to enable. (default: 0)\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(IPR_DRIVER_VERSION);\n\n \nstatic const\nstruct ipr_error_table_t ipr_error_table[] = {\n\t{0x00000000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"8155: An unknown error was received\"},\n\t{0x00330000, 0, 0,\n\t\"Soft underlength error\"},\n\t{0x005A0000, 0, 0,\n\t\"Command to be cancelled not found\"},\n\t{0x00808000, 0, 0,\n\t\"Qualified success\"},\n\t{0x01080000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFE: Soft device bus error recovered by the IOA\"},\n\t{0x01088100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4101: Soft device bus fabric error\"},\n\t{0x01100100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFC: Logical block guard error recovered by the device\"},\n\t{0x01100300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFC: Logical block reference tag error recovered by the device\"},\n\t{0x01108300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4171: Recovered scatter list tag / sequence number error\"},\n\t{0x01109000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FF3D: Recovered logical block CRC error on IOA to Host transfer\"},\n\t{0x01109200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4171: Recovered logical block sequence number error on IOA to Host transfer\"},\n\t{0x0110A000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFD: Recovered logical block reference tag error detected by the IOA\"},\n\t{0x0110A100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFD: Logical block guard error recovered by the IOA\"},\n\t{0x01170600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF9: Device sector reassign successful\"},\n\t{0x01170900, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF7: Media error recovered by device rewrite procedures\"},\n\t{0x01180200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"7001: IOA sector reassignment successful\"},\n\t{0x01180500, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF9: Soft media error. Sector reassignment recommended\"},\n\t{0x01180600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF7: Media error recovered by IOA rewrite procedures\"},\n\t{0x01418000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FF3D: Soft PCI bus error recovered by the IOA\"},\n\t{0x01440000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF6: Device hardware error recovered by the IOA\"},\n\t{0x01448100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF6: Device hardware error recovered by the device\"},\n\t{0x01448200, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FF3D: Soft IOA error recovered by the IOA\"},\n\t{0x01448300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFA: Undefined device response recovered by the IOA\"},\n\t{0x014A0000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF6: Device bus error, message or command phase\"},\n\t{0x014A8000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFE: Task Management Function failed\"},\n\t{0x015D0000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF6: Failure prediction threshold exceeded\"},\n\t{0x015D9200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"8009: Impending cache battery pack failure\"},\n\t{0x02040100, 0, 0,\n\t\"Logical Unit in process of becoming ready\"},\n\t{0x02040200, 0, 0,\n\t\"Initializing command required\"},\n\t{0x02040400, 0, 0,\n\t\"34FF: Disk device format in progress\"},\n\t{0x02040C00, 0, 0,\n\t\"Logical unit not accessible, target port in unavailable state\"},\n\t{0x02048000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9070: IOA requested reset\"},\n\t{0x023F0000, 0, 0,\n\t\"Synchronization required\"},\n\t{0x02408500, 0, 0,\n\t\"IOA microcode download required\"},\n\t{0x02408600, 0, 0,\n\t\"Device bus connection is prohibited by host\"},\n\t{0x024E0000, 0, 0,\n\t\"No ready, IOA shutdown\"},\n\t{0x025A0000, 0, 0,\n\t\"Not ready, IOA has been shutdown\"},\n\t{0x02670100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3020: Storage subsystem configuration error\"},\n\t{0x03110B00, 0, 0,\n\t\"FFF5: Medium error, data unreadable, recommend reassign\"},\n\t{0x03110C00, 0, 0,\n\t\"7000: Medium error, data unreadable, do not reassign\"},\n\t{0x03310000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF3: Disk media format bad\"},\n\t{0x04050000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3002: Addressed device failed to respond to selection\"},\n\t{0x04080000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"3100: Device bus error\"},\n\t{0x04080100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3109: IOA timed out a device command\"},\n\t{0x04088000, 0, 0,\n\t\"3120: SCSI bus is not operational\"},\n\t{0x04088100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4100: Hard device bus fabric error\"},\n\t{0x04100100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"310C: Logical block guard error detected by the device\"},\n\t{0x04100300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"310C: Logical block reference tag error detected by the device\"},\n\t{0x04108300, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"4170: Scatter list tag / sequence number error\"},\n\t{0x04109000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"8150: Logical block CRC error on IOA to Host transfer\"},\n\t{0x04109200, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"4170: Logical block sequence number error on IOA to Host transfer\"},\n\t{0x0410A000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"310D: Logical block reference tag error detected by the IOA\"},\n\t{0x0410A100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"310D: Logical block guard error detected by the IOA\"},\n\t{0x04118000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9000: IOA reserved area data check\"},\n\t{0x04118100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9001: IOA reserved area invalid data pattern\"},\n\t{0x04118200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9002: IOA reserved area LRC error\"},\n\t{0x04118300, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"Hardware Error, IOA metadata access error\"},\n\t{0x04320000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"102E: Out of alternate sectors for disk storage\"},\n\t{0x04330000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF4: Data transfer underlength error\"},\n\t{0x04338000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF4: Data transfer overlength error\"},\n\t{0x043E0100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3400: Logical unit failure\"},\n\t{0x04408500, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF4: Device microcode is corrupt\"},\n\t{0x04418000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"8150: PCI bus error\"},\n\t{0x04430000, 1, 0,\n\t\"Unsupported device bus message received\"},\n\t{0x04440000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF4: Disk device problem\"},\n\t{0x04448200, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"8150: Permanent IOA failure\"},\n\t{0x04448300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3010: Disk device returned wrong response to IOA\"},\n\t{0x04448400, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"8151: IOA microcode error\"},\n\t{0x04448500, 0, 0,\n\t\"Device bus status error\"},\n\t{0x04448600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"8157: IOA error requiring IOA reset to recover\"},\n\t{0x04448700, 0, 0,\n\t\"ATA device status error\"},\n\t{0x04490000, 0, 0,\n\t\"Message reject received from the device\"},\n\t{0x04449200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"8008: A permanent cache battery pack failure occurred\"},\n\t{0x0444A000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9090: Disk unit has been modified after the last known status\"},\n\t{0x0444A200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9081: IOA detected device error\"},\n\t{0x0444A300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9082: IOA detected device error\"},\n\t{0x044A0000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"3110: Device bus error, message or command phase\"},\n\t{0x044A8000, 1, IPR_DEFAULT_LOG_LEVEL,\n\t\"3110: SAS Command / Task Management Function failed\"},\n\t{0x04670400, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9091: Incorrect hardware configuration change has been detected\"},\n\t{0x04678000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9073: Invalid multi-adapter configuration\"},\n\t{0x04678100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4010: Incorrect connection between cascaded expanders\"},\n\t{0x04678200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4020: Connections exceed IOA design limits\"},\n\t{0x04678300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4030: Incorrect multipath connection\"},\n\t{0x04679000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4110: Unsupported enclosure function\"},\n\t{0x04679800, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4120: SAS cable VPD cannot be read\"},\n\t{0x046E0000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFF4: Command to logical unit failed\"},\n\t{0x05240000, 1, 0,\n\t\"Illegal request, invalid request type or request packet\"},\n\t{0x05250000, 0, 0,\n\t\"Illegal request, invalid resource handle\"},\n\t{0x05258000, 0, 0,\n\t\"Illegal request, commands not allowed to this device\"},\n\t{0x05258100, 0, 0,\n\t\"Illegal request, command not allowed to a secondary adapter\"},\n\t{0x05258200, 0, 0,\n\t\"Illegal request, command not allowed to a non-optimized resource\"},\n\t{0x05260000, 0, 0,\n\t\"Illegal request, invalid field in parameter list\"},\n\t{0x05260100, 0, 0,\n\t\"Illegal request, parameter not supported\"},\n\t{0x05260200, 0, 0,\n\t\"Illegal request, parameter value invalid\"},\n\t{0x052C0000, 0, 0,\n\t\"Illegal request, command sequence error\"},\n\t{0x052C8000, 1, 0,\n\t\"Illegal request, dual adapter support not enabled\"},\n\t{0x052C8100, 1, 0,\n\t\"Illegal request, another cable connector was physically disabled\"},\n\t{0x054E8000, 1, 0,\n\t\"Illegal request, inconsistent group id/group count\"},\n\t{0x06040500, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9031: Array protection temporarily suspended, protection resuming\"},\n\t{0x06040600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9040: Array protection temporarily suspended, protection resuming\"},\n\t{0x060B0100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4080: IOA exceeded maximum operating temperature\"},\n\t{0x060B8000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4085: Service required\"},\n\t{0x060B8100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4086: SAS Adapter Hardware Configuration Error\"},\n\t{0x06288000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3140: Device bus not ready to ready transition\"},\n\t{0x06290000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFB: SCSI bus was reset\"},\n\t{0x06290500, 0, 0,\n\t\"FFFE: SCSI bus transition to single ended\"},\n\t{0x06290600, 0, 0,\n\t\"FFFE: SCSI bus transition to LVD\"},\n\t{0x06298000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"FFFB: SCSI bus was reset by another initiator\"},\n\t{0x063F0300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3029: A device replacement has occurred\"},\n\t{0x063F8300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4102: Device bus fabric performance degradation\"},\n\t{0x064C8000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9051: IOA cache data exists for a missing or failed device\"},\n\t{0x064C8100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9055: Auxiliary cache IOA contains cache data needed by the primary IOA\"},\n\t{0x06670100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9025: Disk unit is not supported at its physical location\"},\n\t{0x06670600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3020: IOA detected a SCSI bus configuration error\"},\n\t{0x06678000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"3150: SCSI bus configuration error\"},\n\t{0x06678100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9074: Asymmetric advanced function disk configuration\"},\n\t{0x06678300, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4040: Incomplete multipath connection between IOA and enclosure\"},\n\t{0x06678400, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4041: Incomplete multipath connection between enclosure and device\"},\n\t{0x06678500, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9075: Incomplete multipath connection between IOA and remote IOA\"},\n\t{0x06678600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9076: Configuration error, missing remote IOA\"},\n\t{0x06679100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4050: Enclosure does not support a required multipath function\"},\n\t{0x06679800, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4121: Configuration error, required cable is missing\"},\n\t{0x06679900, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4122: Cable is not plugged into the correct location on remote IOA\"},\n\t{0x06679A00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4123: Configuration error, invalid cable vital product data\"},\n\t{0x06679B00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4124: Configuration error, both cable ends are plugged into the same IOA\"},\n\t{0x06690000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4070: Logically bad block written on device\"},\n\t{0x06690200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9041: Array protection temporarily suspended\"},\n\t{0x06698200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9042: Corrupt array parity detected on specified device\"},\n\t{0x066B0200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9030: Array no longer protected due to missing or failed disk unit\"},\n\t{0x066B8000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9071: Link operational transition\"},\n\t{0x066B8100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9072: Link not operational transition\"},\n\t{0x066B8200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9032: Array exposed but still protected\"},\n\t{0x066B8300, 0, IPR_DEBUG_LOG_LEVEL,\n\t\"70DD: Device forced failed by disrupt device command\"},\n\t{0x066B9100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4061: Multipath redundancy level got better\"},\n\t{0x066B9200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"4060: Multipath redundancy level got worse\"},\n\t{0x06808100, 0, IPR_DEBUG_LOG_LEVEL,\n\t\"9083: Device raw mode enabled\"},\n\t{0x06808200, 0, IPR_DEBUG_LOG_LEVEL,\n\t\"9084: Device raw mode disabled\"},\n\t{0x07270000, 0, 0,\n\t\"Failure due to other device\"},\n\t{0x07278000, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9008: IOA does not support functions expected by devices\"},\n\t{0x07278100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9010: Cache data associated with attached devices cannot be found\"},\n\t{0x07278200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9011: Cache data belongs to devices other than those attached\"},\n\t{0x07278400, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9020: Array missing 2 or more devices with only 1 device present\"},\n\t{0x07278500, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9021: Array missing 2 or more devices with 2 or more devices present\"},\n\t{0x07278600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9022: Exposed array is missing a required device\"},\n\t{0x07278700, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9023: Array member(s) not at required physical locations\"},\n\t{0x07278800, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9024: Array not functional due to present hardware configuration\"},\n\t{0x07278900, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9026: Array not functional due to present hardware configuration\"},\n\t{0x07278A00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9027: Array is missing a device and parity is out of sync\"},\n\t{0x07278B00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9028: Maximum number of arrays already exist\"},\n\t{0x07278C00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9050: Required cache data cannot be located for a disk unit\"},\n\t{0x07278D00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9052: Cache data exists for a device that has been modified\"},\n\t{0x07278F00, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9054: IOA resources not available due to previous problems\"},\n\t{0x07279100, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9092: Disk unit requires initialization before use\"},\n\t{0x07279200, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9029: Incorrect hardware configuration change has been detected\"},\n\t{0x07279600, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9060: One or more disk pairs are missing from an array\"},\n\t{0x07279700, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9061: One or more disks are missing from an array\"},\n\t{0x07279800, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9062: One or more disks are missing from an array\"},\n\t{0x07279900, 0, IPR_DEFAULT_LOG_LEVEL,\n\t\"9063: Maximum number of functional arrays has been exceeded\"},\n\t{0x07279A00, 0, 0,\n\t\"Data protect, other volume set problem\"},\n\t{0x0B260000, 0, 0,\n\t\"Aborted command, invalid descriptor\"},\n\t{0x0B3F9000, 0, 0,\n\t\"Target operating conditions have changed, dual adapter takeover\"},\n\t{0x0B530200, 0, 0,\n\t\"Aborted command, medium removal prevented\"},\n\t{0x0B5A0000, 0, 0,\n\t\"Command terminated by host\"},\n\t{0x0B5B8000, 0, 0,\n\t\"Aborted command, command terminated by host\"}\n};\n\nstatic const struct ipr_ses_table_entry ipr_ses_table[] = {\n\t{ \"2104-DL1        \", \"XXXXXXXXXXXXXXXX\", 80 },\n\t{ \"2104-TL1        \", \"XXXXXXXXXXXXXXXX\", 80 },\n\t{ \"HSBP07M P U2SCSI\", \"XXXXXXXXXXXXXXXX\", 80 },  \n\t{ \"HSBP05M P U2SCSI\", \"XXXXXXXXXXXXXXXX\", 80 },  \n\t{ \"HSBP05M S U2SCSI\", \"XXXXXXXXXXXXXXXX\", 80 },  \n\t{ \"HSBP06E ASU2SCSI\", \"XXXXXXXXXXXXXXXX\", 80 },  \n\t{ \"2104-DU3        \", \"XXXXXXXXXXXXXXXX\", 160 },\n\t{ \"2104-TU3        \", \"XXXXXXXXXXXXXXXX\", 160 },\n\t{ \"HSBP04C RSU2SCSI\", \"XXXXXXX*XXXXXXXX\", 160 },\n\t{ \"HSBP06E RSU2SCSI\", \"XXXXXXX*XXXXXXXX\", 160 },\n\t{ \"St  V1S2        \", \"XXXXXXXXXXXXXXXX\", 160 },\n\t{ \"HSBPD4M  PU3SCSI\", \"XXXXXXX*XXXXXXXX\", 160 },\n\t{ \"VSBPD1H   U3SCSI\", \"XXXXXXX*XXXXXXXX\", 160 }\n};\n\n \nstatic int ipr_reset_alert(struct ipr_cmnd *);\nstatic void ipr_process_ccn(struct ipr_cmnd *);\nstatic void ipr_process_error(struct ipr_cmnd *);\nstatic void ipr_reset_ioa_job(struct ipr_cmnd *);\nstatic void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *,\n\t\t\t\t   enum ipr_shutdown_type);\n\n#ifdef CONFIG_SCSI_IPR_TRACE\n \nstatic void ipr_trc_hook(struct ipr_cmnd *ipr_cmd,\n\t\t\t u8 type, u32 add_data)\n{\n\tstruct ipr_trace_entry *trace_entry;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tunsigned int trace_index;\n\n\ttrace_index = atomic_add_return(1, &ioa_cfg->trace_index) & IPR_TRACE_INDEX_MASK;\n\ttrace_entry = &ioa_cfg->trace[trace_index];\n\ttrace_entry->time = jiffies;\n\ttrace_entry->op_code = ipr_cmd->ioarcb.cmd_pkt.cdb[0];\n\ttrace_entry->type = type;\n\ttrace_entry->cmd_index = ipr_cmd->cmd_index & 0xff;\n\ttrace_entry->res_handle = ipr_cmd->ioarcb.res_handle;\n\ttrace_entry->u.add_data = add_data;\n\twmb();\n}\n#else\n#define ipr_trc_hook(ipr_cmd, type, add_data) do { } while (0)\n#endif\n\n \nstatic void ipr_lock_and_done(struct ipr_cmnd *ipr_cmd)\n{\n\tunsigned long lock_flags;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tipr_cmd->done(ipr_cmd);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n}\n\n \nstatic void ipr_reinit_ipr_cmnd(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\n\tdma_addr_t dma_addr = ipr_cmd->dma_addr;\n\tint hrrq_id;\n\n\thrrq_id = ioarcb->cmd_pkt.hrrq_id;\n\tmemset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));\n\tioarcb->cmd_pkt.hrrq_id = hrrq_id;\n\tioarcb->data_transfer_length = 0;\n\tioarcb->read_data_transfer_length = 0;\n\tioarcb->ioadl_len = 0;\n\tioarcb->read_ioadl_len = 0;\n\n\tif (ipr_cmd->ioa_cfg->sis64) {\n\t\tioarcb->u.sis64_addr_data.data_ioadl_addr =\n\t\t\tcpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\n\t} else {\n\t\tioarcb->write_ioadl_addr =\n\t\t\tcpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\n\t\tioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\n\t}\n\n\tioasa->hdr.ioasc = 0;\n\tioasa->hdr.residual_data_len = 0;\n\tipr_cmd->scsi_cmd = NULL;\n\tipr_cmd->sense_buffer[0] = 0;\n\tipr_cmd->dma_use_sg = 0;\n}\n\n \nstatic void ipr_init_ipr_cmnd(struct ipr_cmnd *ipr_cmd,\n\t\t\t      void (*fast_done) (struct ipr_cmnd *))\n{\n\tipr_reinit_ipr_cmnd(ipr_cmd);\n\tipr_cmd->u.scratch = 0;\n\tipr_cmd->sibling = NULL;\n\tipr_cmd->eh_comp = NULL;\n\tipr_cmd->fast_done = fast_done;\n\ttimer_setup(&ipr_cmd->timer, NULL, 0);\n}\n\n \nstatic\nstruct ipr_cmnd *__ipr_get_free_ipr_cmnd(struct ipr_hrr_queue *hrrq)\n{\n\tstruct ipr_cmnd *ipr_cmd = NULL;\n\n\tif (likely(!list_empty(&hrrq->hrrq_free_q))) {\n\t\tipr_cmd = list_entry(hrrq->hrrq_free_q.next,\n\t\t\tstruct ipr_cmnd, queue);\n\t\tlist_del(&ipr_cmd->queue);\n\t}\n\n\n\treturn ipr_cmd;\n}\n\n \nstatic\nstruct ipr_cmnd *ipr_get_free_ipr_cmnd(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_cmnd *ipr_cmd =\n\t\t__ipr_get_free_ipr_cmnd(&ioa_cfg->hrrq[IPR_INIT_HRRQ]);\n\tipr_init_ipr_cmnd(ipr_cmd, ipr_lock_and_done);\n\treturn ipr_cmd;\n}\n\n \nstatic void ipr_mask_and_clear_interrupts(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t  u32 clr_ints)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].allow_interrupts = 0;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\n\t \n\tif (ioa_cfg->sis64)\n\t\twriteq(~0, ioa_cfg->regs.set_interrupt_mask_reg);\n\telse\n\t\twritel(~0, ioa_cfg->regs.set_interrupt_mask_reg);\n\n\t \n\tif (ioa_cfg->sis64)\n\t\twritel(~0, ioa_cfg->regs.clr_interrupt_reg);\n\twritel(clr_ints, ioa_cfg->regs.clr_interrupt_reg32);\n\treadl(ioa_cfg->regs.sense_interrupt_reg);\n}\n\n \nstatic int ipr_save_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);\n\n\tif (pcix_cmd_reg == 0)\n\t\treturn 0;\n\n\tif (pci_read_config_word(ioa_cfg->pdev, pcix_cmd_reg + PCI_X_CMD,\n\t\t\t\t &ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {\n\t\tdev_err(&ioa_cfg->pdev->dev, \"Failed to save PCI-X command register\\n\");\n\t\treturn -EIO;\n\t}\n\n\tioa_cfg->saved_pcix_cmd_reg |= PCI_X_CMD_DPERR_E | PCI_X_CMD_ERO;\n\treturn 0;\n}\n\n \nstatic int ipr_set_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);\n\n\tif (pcix_cmd_reg) {\n\t\tif (pci_write_config_word(ioa_cfg->pdev, pcix_cmd_reg + PCI_X_CMD,\n\t\t\t\t\t  ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {\n\t\t\tdev_err(&ioa_cfg->pdev->dev, \"Failed to setup PCI-X command register\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n\n \nstatic void __ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\n\tscsi_cmd->result |= (DID_ERROR << 16);\n\n\tscsi_dma_unmap(ipr_cmd->scsi_cmd);\n\tscsi_done(scsi_cmd);\n\tif (ipr_cmd->eh_comp)\n\t\tcomplete(ipr_cmd->eh_comp);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n}\n\n \nstatic void ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)\n{\n\tunsigned long hrrq_flags;\n\tstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\n\n\tspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\n\t__ipr_scsi_eh_done(ipr_cmd);\n\tspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\n}\n\n \nstatic void ipr_fail_all_ops(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_cmnd *ipr_cmd, *temp;\n\tstruct ipr_hrr_queue *hrrq;\n\n\tENTER;\n\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\tspin_lock(&hrrq->_lock);\n\t\tlist_for_each_entry_safe(ipr_cmd,\n\t\t\t\t\ttemp, &hrrq->hrrq_pending_q, queue) {\n\t\t\tlist_del(&ipr_cmd->queue);\n\n\t\t\tipr_cmd->s.ioasa.hdr.ioasc =\n\t\t\t\tcpu_to_be32(IPR_IOASC_IOA_WAS_RESET);\n\t\t\tipr_cmd->s.ioasa.hdr.ilid =\n\t\t\t\tcpu_to_be32(IPR_DRIVER_ILID);\n\n\t\t\tif (ipr_cmd->scsi_cmd)\n\t\t\t\tipr_cmd->done = __ipr_scsi_eh_done;\n\n\t\t\tipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH,\n\t\t\t\t     IPR_IOASC_IOA_WAS_RESET);\n\t\t\tdel_timer(&ipr_cmd->timer);\n\t\t\tipr_cmd->done(ipr_cmd);\n\t\t}\n\t\tspin_unlock(&hrrq->_lock);\n\t}\n\tLEAVE;\n}\n\n \nstatic void ipr_send_command(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tdma_addr_t send_dma_addr = ipr_cmd->dma_addr;\n\n\tif (ioa_cfg->sis64) {\n\t\t \n\t\tsend_dma_addr |= 0x1;\n\n\t\t \n\t\tif (ipr_cmd->dma_use_sg * sizeof(struct ipr_ioadl64_desc) > 128 )\n\t\t\tsend_dma_addr |= 0x4;\n\t\twriteq(send_dma_addr, ioa_cfg->regs.ioarrin_reg);\n\t} else\n\t\twritel(send_dma_addr, ioa_cfg->regs.ioarrin_reg);\n}\n\n \nstatic void ipr_do_req(struct ipr_cmnd *ipr_cmd,\n\t\t       void (*done) (struct ipr_cmnd *),\n\t\t       void (*timeout_func) (struct timer_list *), u32 timeout)\n{\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\n\tipr_cmd->done = done;\n\n\tipr_cmd->timer.expires = jiffies + timeout;\n\tipr_cmd->timer.function = timeout_func;\n\n\tadd_timer(&ipr_cmd->timer);\n\n\tipr_trc_hook(ipr_cmd, IPR_TRACE_START, 0);\n\n\tipr_send_command(ipr_cmd);\n}\n\n \nstatic void ipr_internal_cmd_done(struct ipr_cmnd *ipr_cmd)\n{\n\tif (ipr_cmd->sibling)\n\t\tipr_cmd->sibling = NULL;\n\telse\n\t\tcomplete(&ipr_cmd->completion);\n}\n\n \nstatic void ipr_init_ioadl(struct ipr_cmnd *ipr_cmd, dma_addr_t dma_addr,\n\t\t\t   u32 len, int flags)\n{\n\tstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\n\tstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\n\n\tipr_cmd->dma_use_sg = 1;\n\n\tif (ipr_cmd->ioa_cfg->sis64) {\n\t\tioadl64->flags = cpu_to_be32(flags);\n\t\tioadl64->data_len = cpu_to_be32(len);\n\t\tioadl64->address = cpu_to_be64(dma_addr);\n\n\t\tipr_cmd->ioarcb.ioadl_len =\n\t\t       \tcpu_to_be32(sizeof(struct ipr_ioadl64_desc));\n\t\tipr_cmd->ioarcb.data_transfer_length = cpu_to_be32(len);\n\t} else {\n\t\tioadl->flags_and_data_len = cpu_to_be32(flags | len);\n\t\tioadl->address = cpu_to_be32(dma_addr);\n\n\t\tif (flags == IPR_IOADL_FLAGS_READ_LAST) {\n\t\t\tipr_cmd->ioarcb.read_ioadl_len =\n\t\t\t\tcpu_to_be32(sizeof(struct ipr_ioadl_desc));\n\t\t\tipr_cmd->ioarcb.read_data_transfer_length = cpu_to_be32(len);\n\t\t} else {\n\t\t\tipr_cmd->ioarcb.ioadl_len =\n\t\t\t       \tcpu_to_be32(sizeof(struct ipr_ioadl_desc));\n\t\t\tipr_cmd->ioarcb.data_transfer_length = cpu_to_be32(len);\n\t\t}\n\t}\n}\n\n \nstatic void ipr_send_blocking_cmd(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t  void (*timeout_func) (struct timer_list *),\n\t\t\t\t  u32 timeout)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tinit_completion(&ipr_cmd->completion);\n\tipr_do_req(ipr_cmd, ipr_internal_cmd_done, timeout_func, timeout);\n\n\tspin_unlock_irq(ioa_cfg->host->host_lock);\n\twait_for_completion(&ipr_cmd->completion);\n\tspin_lock_irq(ioa_cfg->host->host_lock);\n}\n\nstatic int ipr_get_hrrq_index(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tunsigned int hrrq;\n\n\tif (ioa_cfg->hrrq_num == 1)\n\t\thrrq = 0;\n\telse {\n\t\thrrq = atomic_add_return(1, &ioa_cfg->hrrq_index);\n\t\thrrq = (hrrq % (ioa_cfg->hrrq_num - 1)) + 1;\n\t}\n\treturn hrrq;\n}\n\n \nstatic void ipr_send_hcam(struct ipr_ioa_cfg *ioa_cfg, u8 type,\n\t\t\t  struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioarcb *ioarcb;\n\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds) {\n\t\tipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\t\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\t\tlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_pending_q);\n\n\t\tipr_cmd->u.hostrcb = hostrcb;\n\t\tioarcb = &ipr_cmd->ioarcb;\n\n\t\tioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\t\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_HCAM;\n\t\tioarcb->cmd_pkt.cdb[0] = IPR_HOST_CONTROLLED_ASYNC;\n\t\tioarcb->cmd_pkt.cdb[1] = type;\n\t\tioarcb->cmd_pkt.cdb[7] = (sizeof(hostrcb->hcam) >> 8) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[8] = sizeof(hostrcb->hcam) & 0xff;\n\n\t\tipr_init_ioadl(ipr_cmd, hostrcb->hostrcb_dma,\n\t\t\t       sizeof(hostrcb->hcam), IPR_IOADL_FLAGS_READ_LAST);\n\n\t\tif (type == IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE)\n\t\t\tipr_cmd->done = ipr_process_ccn;\n\t\telse\n\t\t\tipr_cmd->done = ipr_process_error;\n\n\t\tipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_IOA_RES_ADDR);\n\n\t\tipr_send_command(ipr_cmd);\n\t} else {\n\t\tlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\n\t}\n}\n\n \nstatic void ipr_init_res_entry(struct ipr_resource_entry *res,\n\t\t\t       struct ipr_config_table_entry_wrapper *cfgtew)\n{\n\tint found = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = res->ioa_cfg;\n\tstruct ipr_resource_entry *gscsi_res = NULL;\n\n\tres->needs_sync_complete = 0;\n\tres->in_erp = 0;\n\tres->add_to_ml = 0;\n\tres->del_from_ml = 0;\n\tres->resetting_device = 0;\n\tres->reset_occurred = 0;\n\tres->sdev = NULL;\n\n\tif (ioa_cfg->sis64) {\n\t\tres->flags = be16_to_cpu(cfgtew->u.cfgte64->flags);\n\t\tres->res_flags = be16_to_cpu(cfgtew->u.cfgte64->res_flags);\n\t\tres->qmodel = IPR_QUEUEING_MODEL64(res);\n\t\tres->type = cfgtew->u.cfgte64->res_type;\n\n\t\tmemcpy(res->res_path, &cfgtew->u.cfgte64->res_path,\n\t\t\tsizeof(res->res_path));\n\n\t\tres->bus = 0;\n\t\tmemcpy(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\n\t\t\tsizeof(res->dev_lun.scsi_lun));\n\t\tres->lun = scsilun_to_int(&res->dev_lun);\n\n\t\tif (res->type == IPR_RES_TYPE_GENERIC_SCSI) {\n\t\t\tlist_for_each_entry(gscsi_res, &ioa_cfg->used_res_q, queue) {\n\t\t\t\tif (gscsi_res->dev_id == cfgtew->u.cfgte64->dev_id) {\n\t\t\t\t\tfound = 1;\n\t\t\t\t\tres->target = gscsi_res->target;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!found) {\n\t\t\t\tres->target = find_first_zero_bit(ioa_cfg->target_ids,\n\t\t\t\t\t\t\t\t  ioa_cfg->max_devs_supported);\n\t\t\t\tset_bit(res->target, ioa_cfg->target_ids);\n\t\t\t}\n\t\t} else if (res->type == IPR_RES_TYPE_IOAFP) {\n\t\t\tres->bus = IPR_IOAFP_VIRTUAL_BUS;\n\t\t\tres->target = 0;\n\t\t} else if (res->type == IPR_RES_TYPE_ARRAY) {\n\t\t\tres->bus = IPR_ARRAY_VIRTUAL_BUS;\n\t\t\tres->target = find_first_zero_bit(ioa_cfg->array_ids,\n\t\t\t\t\t\t\t  ioa_cfg->max_devs_supported);\n\t\t\tset_bit(res->target, ioa_cfg->array_ids);\n\t\t} else if (res->type == IPR_RES_TYPE_VOLUME_SET) {\n\t\t\tres->bus = IPR_VSET_VIRTUAL_BUS;\n\t\t\tres->target = find_first_zero_bit(ioa_cfg->vset_ids,\n\t\t\t\t\t\t\t  ioa_cfg->max_devs_supported);\n\t\t\tset_bit(res->target, ioa_cfg->vset_ids);\n\t\t} else {\n\t\t\tres->target = find_first_zero_bit(ioa_cfg->target_ids,\n\t\t\t\t\t\t\t  ioa_cfg->max_devs_supported);\n\t\t\tset_bit(res->target, ioa_cfg->target_ids);\n\t\t}\n\t} else {\n\t\tres->qmodel = IPR_QUEUEING_MODEL(res);\n\t\tres->flags = cfgtew->u.cfgte->flags;\n\t\tif (res->flags & IPR_IS_IOA_RESOURCE)\n\t\t\tres->type = IPR_RES_TYPE_IOAFP;\n\t\telse\n\t\t\tres->type = cfgtew->u.cfgte->rsvd_subtype & 0x0f;\n\n\t\tres->bus = cfgtew->u.cfgte->res_addr.bus;\n\t\tres->target = cfgtew->u.cfgte->res_addr.target;\n\t\tres->lun = cfgtew->u.cfgte->res_addr.lun;\n\t\tres->lun_wwn = get_unaligned_be64(cfgtew->u.cfgte->lun_wwn);\n\t}\n}\n\n \nstatic int ipr_is_same_device(struct ipr_resource_entry *res,\n\t\t\t      struct ipr_config_table_entry_wrapper *cfgtew)\n{\n\tif (res->ioa_cfg->sis64) {\n\t\tif (!memcmp(&res->dev_id, &cfgtew->u.cfgte64->dev_id,\n\t\t\t\t\tsizeof(cfgtew->u.cfgte64->dev_id)) &&\n\t\t\t!memcmp(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\n\t\t\t\t\tsizeof(cfgtew->u.cfgte64->lun))) {\n\t\t\treturn 1;\n\t\t}\n\t} else {\n\t\tif (res->bus == cfgtew->u.cfgte->res_addr.bus &&\n\t\t    res->target == cfgtew->u.cfgte->res_addr.target &&\n\t\t    res->lun == cfgtew->u.cfgte->res_addr.lun)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic char *__ipr_format_res_path(u8 *res_path, char *buffer, int len)\n{\n\tint i;\n\tchar *p = buffer;\n\n\t*p = '\\0';\n\tp += scnprintf(p, buffer + len - p, \"%02X\", res_path[0]);\n\tfor (i = 1; res_path[i] != 0xff && i < IPR_RES_PATH_BYTES; i++)\n\t\tp += scnprintf(p, buffer + len - p, \"-%02X\", res_path[i]);\n\n\treturn buffer;\n}\n\n \nstatic char *ipr_format_res_path(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t u8 *res_path, char *buffer, int len)\n{\n\tchar *p = buffer;\n\n\t*p = '\\0';\n\tp += scnprintf(p, buffer + len - p, \"%d/\", ioa_cfg->host->host_no);\n\t__ipr_format_res_path(res_path, p, len - (p - buffer));\n\treturn buffer;\n}\n\n \nstatic void ipr_update_res_entry(struct ipr_resource_entry *res,\n\t\t\t\t struct ipr_config_table_entry_wrapper *cfgtew)\n{\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\tint new_path = 0;\n\n\tif (res->ioa_cfg->sis64) {\n\t\tres->flags = be16_to_cpu(cfgtew->u.cfgte64->flags);\n\t\tres->res_flags = be16_to_cpu(cfgtew->u.cfgte64->res_flags);\n\t\tres->type = cfgtew->u.cfgte64->res_type;\n\n\t\tmemcpy(&res->std_inq_data, &cfgtew->u.cfgte64->std_inq_data,\n\t\t\tsizeof(struct ipr_std_inq_data));\n\n\t\tres->qmodel = IPR_QUEUEING_MODEL64(res);\n\t\tres->res_handle = cfgtew->u.cfgte64->res_handle;\n\t\tres->dev_id = cfgtew->u.cfgte64->dev_id;\n\n\t\tmemcpy(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\n\t\t\tsizeof(res->dev_lun.scsi_lun));\n\n\t\tif (memcmp(res->res_path, &cfgtew->u.cfgte64->res_path,\n\t\t\t\t\tsizeof(res->res_path))) {\n\t\t\tmemcpy(res->res_path, &cfgtew->u.cfgte64->res_path,\n\t\t\t\tsizeof(res->res_path));\n\t\t\tnew_path = 1;\n\t\t}\n\n\t\tif (res->sdev && new_path)\n\t\t\tsdev_printk(KERN_INFO, res->sdev, \"Resource path: %s\\n\",\n\t\t\t\t    ipr_format_res_path(res->ioa_cfg,\n\t\t\t\t\tres->res_path, buffer, sizeof(buffer)));\n\t} else {\n\t\tres->flags = cfgtew->u.cfgte->flags;\n\t\tif (res->flags & IPR_IS_IOA_RESOURCE)\n\t\t\tres->type = IPR_RES_TYPE_IOAFP;\n\t\telse\n\t\t\tres->type = cfgtew->u.cfgte->rsvd_subtype & 0x0f;\n\n\t\tmemcpy(&res->std_inq_data, &cfgtew->u.cfgte->std_inq_data,\n\t\t\tsizeof(struct ipr_std_inq_data));\n\n\t\tres->qmodel = IPR_QUEUEING_MODEL(res);\n\t\tres->res_handle = cfgtew->u.cfgte->res_handle;\n\t}\n}\n\n \nstatic void ipr_clear_res_target(struct ipr_resource_entry *res)\n{\n\tstruct ipr_resource_entry *gscsi_res = NULL;\n\tstruct ipr_ioa_cfg *ioa_cfg = res->ioa_cfg;\n\n\tif (!ioa_cfg->sis64)\n\t\treturn;\n\n\tif (res->bus == IPR_ARRAY_VIRTUAL_BUS)\n\t\tclear_bit(res->target, ioa_cfg->array_ids);\n\telse if (res->bus == IPR_VSET_VIRTUAL_BUS)\n\t\tclear_bit(res->target, ioa_cfg->vset_ids);\n\telse if (res->bus == 0 && res->type == IPR_RES_TYPE_GENERIC_SCSI) {\n\t\tlist_for_each_entry(gscsi_res, &ioa_cfg->used_res_q, queue)\n\t\t\tif (gscsi_res->dev_id == res->dev_id && gscsi_res != res)\n\t\t\t\treturn;\n\t\tclear_bit(res->target, ioa_cfg->target_ids);\n\n\t} else if (res->bus == 0)\n\t\tclear_bit(res->target, ioa_cfg->target_ids);\n}\n\n \nstatic void ipr_handle_config_change(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t     struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_resource_entry *res = NULL;\n\tstruct ipr_config_table_entry_wrapper cfgtew;\n\t__be32 cc_res_handle;\n\n\tu32 is_ndn = 1;\n\n\tif (ioa_cfg->sis64) {\n\t\tcfgtew.u.cfgte64 = &hostrcb->hcam.u.ccn.u.cfgte64;\n\t\tcc_res_handle = cfgtew.u.cfgte64->res_handle;\n\t} else {\n\t\tcfgtew.u.cfgte = &hostrcb->hcam.u.ccn.u.cfgte;\n\t\tcc_res_handle = cfgtew.u.cfgte->res_handle;\n\t}\n\n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif (res->res_handle == cc_res_handle) {\n\t\t\tis_ndn = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (is_ndn) {\n\t\tif (list_empty(&ioa_cfg->free_res_q)) {\n\t\t\tipr_send_hcam(ioa_cfg,\n\t\t\t\t      IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,\n\t\t\t\t      hostrcb);\n\t\t\treturn;\n\t\t}\n\n\t\tres = list_entry(ioa_cfg->free_res_q.next,\n\t\t\t\t struct ipr_resource_entry, queue);\n\n\t\tlist_del(&res->queue);\n\t\tipr_init_res_entry(res, &cfgtew);\n\t\tlist_add_tail(&res->queue, &ioa_cfg->used_res_q);\n\t}\n\n\tipr_update_res_entry(res, &cfgtew);\n\n\tif (hostrcb->hcam.notify_type == IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY) {\n\t\tif (res->sdev) {\n\t\t\tres->del_from_ml = 1;\n\t\t\tres->res_handle = IPR_INVALID_RES_HANDLE;\n\t\t\tschedule_work(&ioa_cfg->work_q);\n\t\t} else {\n\t\t\tipr_clear_res_target(res);\n\t\t\tlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\n\t\t}\n\t} else if (!res->sdev || res->del_from_ml) {\n\t\tres->add_to_ml = 1;\n\t\tschedule_work(&ioa_cfg->work_q);\n\t}\n\n\tipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);\n}\n\n \nstatic void ipr_process_ccn(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_hostrcb *hostrcb = ipr_cmd->u.hostrcb;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tlist_del_init(&hostrcb->queue);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\n\tif (ioasc) {\n\t\tif (ioasc != IPR_IOASC_IOA_WAS_RESET &&\n\t\t    ioasc != IPR_IOASC_ABORTED_CMD_TERM_BY_HOST)\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Host RCB failed with IOASC: 0x%08X\\n\", ioasc);\n\n\t\tipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);\n\t} else {\n\t\tipr_handle_config_change(ioa_cfg, hostrcb);\n\t}\n}\n\n \nstatic void strip_whitespace(int i, char *buf)\n{\n\tif (i < 1)\n\t\treturn;\n\ti--;\n\twhile (i && buf[i] == ' ')\n\t\ti--;\n\tbuf[i+1] = '\\0';\n}\n\n \nstatic void ipr_log_vpd_compact(char *prefix, struct ipr_hostrcb *hostrcb,\n\t\t\t\tstruct ipr_vpd *vpd)\n{\n\tchar vendor_id[IPR_VENDOR_ID_LEN + 1];\n\tchar product_id[IPR_PROD_ID_LEN + 1];\n\tchar sn[IPR_SERIAL_NUM_LEN + 1];\n\n\tmemcpy(vendor_id, vpd->vpids.vendor_id, IPR_VENDOR_ID_LEN);\n\tstrip_whitespace(IPR_VENDOR_ID_LEN, vendor_id);\n\n\tmemcpy(product_id, vpd->vpids.product_id, IPR_PROD_ID_LEN);\n\tstrip_whitespace(IPR_PROD_ID_LEN, product_id);\n\n\tmemcpy(sn, vpd->sn, IPR_SERIAL_NUM_LEN);\n\tstrip_whitespace(IPR_SERIAL_NUM_LEN, sn);\n\n\tipr_hcam_err(hostrcb, \"%s VPID/SN: %s %s %s\\n\", prefix,\n\t\t     vendor_id, product_id, sn);\n}\n\n \nstatic void ipr_log_vpd(struct ipr_vpd *vpd)\n{\n\tchar buffer[IPR_VENDOR_ID_LEN + IPR_PROD_ID_LEN\n\t\t    + IPR_SERIAL_NUM_LEN];\n\n\tmemcpy(buffer, vpd->vpids.vendor_id, IPR_VENDOR_ID_LEN);\n\tmemcpy(buffer + IPR_VENDOR_ID_LEN, vpd->vpids.product_id,\n\t       IPR_PROD_ID_LEN);\n\tbuffer[IPR_VENDOR_ID_LEN + IPR_PROD_ID_LEN] = '\\0';\n\tipr_err(\"Vendor/Product ID: %s\\n\", buffer);\n\n\tmemcpy(buffer, vpd->sn, IPR_SERIAL_NUM_LEN);\n\tbuffer[IPR_SERIAL_NUM_LEN] = '\\0';\n\tipr_err(\"    Serial Number: %s\\n\", buffer);\n}\n\n \nstatic void ipr_log_ext_vpd_compact(char *prefix, struct ipr_hostrcb *hostrcb,\n\t\t\t\t    struct ipr_ext_vpd *vpd)\n{\n\tipr_log_vpd_compact(prefix, hostrcb, &vpd->vpd);\n\tipr_hcam_err(hostrcb, \"%s WWN: %08X%08X\\n\", prefix,\n\t\t     be32_to_cpu(vpd->wwid[0]), be32_to_cpu(vpd->wwid[1]));\n}\n\n \nstatic void ipr_log_ext_vpd(struct ipr_ext_vpd *vpd)\n{\n\tipr_log_vpd(&vpd->vpd);\n\tipr_err(\"    WWN: %08X%08X\\n\", be32_to_cpu(vpd->wwid[0]),\n\t\tbe32_to_cpu(vpd->wwid[1]));\n}\n\n \nstatic void ipr_log_enhanced_cache_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_12_error *error;\n\n\tif (ioa_cfg->sis64)\n\t\terror = &hostrcb->hcam.u.error64.u.type_12_error;\n\telse\n\t\terror = &hostrcb->hcam.u.error.u.type_12_error;\n\n\tipr_err(\"-----Current Configuration-----\\n\");\n\tipr_err(\"Cache Directory Card Information:\\n\");\n\tipr_log_ext_vpd(&error->ioa_vpd);\n\tipr_err(\"Adapter Card Information:\\n\");\n\tipr_log_ext_vpd(&error->cfc_vpd);\n\n\tipr_err(\"-----Expected Configuration-----\\n\");\n\tipr_err(\"Cache Directory Card Information:\\n\");\n\tipr_log_ext_vpd(&error->ioa_last_attached_to_cfc_vpd);\n\tipr_err(\"Adapter Card Information:\\n\");\n\tipr_log_ext_vpd(&error->cfc_last_attached_to_ioa_vpd);\n\n\tipr_err(\"Additional IOA Data: %08X %08X %08X\\n\",\n\t\t     be32_to_cpu(error->ioa_data[0]),\n\t\t     be32_to_cpu(error->ioa_data[1]),\n\t\t     be32_to_cpu(error->ioa_data[2]));\n}\n\n \nstatic void ipr_log_cache_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\tstruct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_02_error *error =\n\t\t&hostrcb->hcam.u.error.u.type_02_error;\n\n\tipr_err(\"-----Current Configuration-----\\n\");\n\tipr_err(\"Cache Directory Card Information:\\n\");\n\tipr_log_vpd(&error->ioa_vpd);\n\tipr_err(\"Adapter Card Information:\\n\");\n\tipr_log_vpd(&error->cfc_vpd);\n\n\tipr_err(\"-----Expected Configuration-----\\n\");\n\tipr_err(\"Cache Directory Card Information:\\n\");\n\tipr_log_vpd(&error->ioa_last_attached_to_cfc_vpd);\n\tipr_err(\"Adapter Card Information:\\n\");\n\tipr_log_vpd(&error->cfc_last_attached_to_ioa_vpd);\n\n\tipr_err(\"Additional IOA Data: %08X %08X %08X\\n\",\n\t\t     be32_to_cpu(error->ioa_data[0]),\n\t\t     be32_to_cpu(error->ioa_data[1]),\n\t\t     be32_to_cpu(error->ioa_data[2]));\n}\n\n \nstatic void ipr_log_enhanced_config_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t  struct ipr_hostrcb *hostrcb)\n{\n\tint errors_logged, i;\n\tstruct ipr_hostrcb_device_data_entry_enhanced *dev_entry;\n\tstruct ipr_hostrcb_type_13_error *error;\n\n\terror = &hostrcb->hcam.u.error.u.type_13_error;\n\terrors_logged = be32_to_cpu(error->errors_logged);\n\n\tipr_err(\"Device Errors Detected/Logged: %d/%d\\n\",\n\t\tbe32_to_cpu(error->errors_detected), errors_logged);\n\n\tdev_entry = error->dev;\n\n\tfor (i = 0; i < errors_logged; i++, dev_entry++) {\n\t\tipr_err_separator;\n\n\t\tipr_phys_res_err(ioa_cfg, dev_entry->dev_res_addr, \"Device %d\", i + 1);\n\t\tipr_log_ext_vpd(&dev_entry->vpd);\n\n\t\tipr_err(\"-----New Device Information-----\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->new_vpd);\n\n\t\tipr_err(\"Cache Directory Card Information:\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->ioa_last_with_dev_vpd);\n\n\t\tipr_err(\"Adapter Card Information:\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->cfc_last_with_dev_vpd);\n\t}\n}\n\n \nstatic void ipr_log_sis64_config_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t       struct ipr_hostrcb *hostrcb)\n{\n\tint errors_logged, i;\n\tstruct ipr_hostrcb64_device_data_entry_enhanced *dev_entry;\n\tstruct ipr_hostrcb_type_23_error *error;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\terror = &hostrcb->hcam.u.error64.u.type_23_error;\n\terrors_logged = be32_to_cpu(error->errors_logged);\n\n\tipr_err(\"Device Errors Detected/Logged: %d/%d\\n\",\n\t\tbe32_to_cpu(error->errors_detected), errors_logged);\n\n\tdev_entry = error->dev;\n\n\tfor (i = 0; i < errors_logged; i++, dev_entry++) {\n\t\tipr_err_separator;\n\n\t\tipr_err(\"Device %d : %s\", i + 1,\n\t\t\t__ipr_format_res_path(dev_entry->res_path,\n\t\t\t\t\t      buffer, sizeof(buffer)));\n\t\tipr_log_ext_vpd(&dev_entry->vpd);\n\n\t\tipr_err(\"-----New Device Information-----\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->new_vpd);\n\n\t\tipr_err(\"Cache Directory Card Information:\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->ioa_last_with_dev_vpd);\n\n\t\tipr_err(\"Adapter Card Information:\\n\");\n\t\tipr_log_ext_vpd(&dev_entry->cfc_last_with_dev_vpd);\n\t}\n}\n\n \nstatic void ipr_log_config_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t struct ipr_hostrcb *hostrcb)\n{\n\tint errors_logged, i;\n\tstruct ipr_hostrcb_device_data_entry *dev_entry;\n\tstruct ipr_hostrcb_type_03_error *error;\n\n\terror = &hostrcb->hcam.u.error.u.type_03_error;\n\terrors_logged = be32_to_cpu(error->errors_logged);\n\n\tipr_err(\"Device Errors Detected/Logged: %d/%d\\n\",\n\t\tbe32_to_cpu(error->errors_detected), errors_logged);\n\n\tdev_entry = error->dev;\n\n\tfor (i = 0; i < errors_logged; i++, dev_entry++) {\n\t\tipr_err_separator;\n\n\t\tipr_phys_res_err(ioa_cfg, dev_entry->dev_res_addr, \"Device %d\", i + 1);\n\t\tipr_log_vpd(&dev_entry->vpd);\n\n\t\tipr_err(\"-----New Device Information-----\\n\");\n\t\tipr_log_vpd(&dev_entry->new_vpd);\n\n\t\tipr_err(\"Cache Directory Card Information:\\n\");\n\t\tipr_log_vpd(&dev_entry->ioa_last_with_dev_vpd);\n\n\t\tipr_err(\"Adapter Card Information:\\n\");\n\t\tipr_log_vpd(&dev_entry->cfc_last_with_dev_vpd);\n\n\t\tipr_err(\"Additional IOA Data: %08X %08X %08X %08X %08X\\n\",\n\t\t\tbe32_to_cpu(dev_entry->ioa_data[0]),\n\t\t\tbe32_to_cpu(dev_entry->ioa_data[1]),\n\t\t\tbe32_to_cpu(dev_entry->ioa_data[2]),\n\t\t\tbe32_to_cpu(dev_entry->ioa_data[3]),\n\t\t\tbe32_to_cpu(dev_entry->ioa_data[4]));\n\t}\n}\n\n \nstatic void ipr_log_enhanced_array_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t struct ipr_hostrcb *hostrcb)\n{\n\tint i, num_entries;\n\tstruct ipr_hostrcb_type_14_error *error;\n\tstruct ipr_hostrcb_array_data_entry_enhanced *array_entry;\n\tconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\n\n\terror = &hostrcb->hcam.u.error.u.type_14_error;\n\n\tipr_err_separator;\n\n\tipr_err(\"RAID %s Array Configuration: %d:%d:%d:%d\\n\",\n\t\terror->protection_level,\n\t\tioa_cfg->host->host_no,\n\t\terror->last_func_vset_res_addr.bus,\n\t\terror->last_func_vset_res_addr.target,\n\t\terror->last_func_vset_res_addr.lun);\n\n\tipr_err_separator;\n\n\tarray_entry = error->array_member;\n\tnum_entries = min_t(u32, be32_to_cpu(error->num_entries),\n\t\t\t    ARRAY_SIZE(error->array_member));\n\n\tfor (i = 0; i < num_entries; i++, array_entry++) {\n\t\tif (!memcmp(array_entry->vpd.vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\n\t\t\tcontinue;\n\n\t\tif (be32_to_cpu(error->exposed_mode_adn) == i)\n\t\t\tipr_err(\"Exposed Array Member %d:\\n\", i);\n\t\telse\n\t\t\tipr_err(\"Array Member %d:\\n\", i);\n\n\t\tipr_log_ext_vpd(&array_entry->vpd);\n\t\tipr_phys_res_err(ioa_cfg, array_entry->dev_res_addr, \"Current Location\");\n\t\tipr_phys_res_err(ioa_cfg, array_entry->expected_dev_res_addr,\n\t\t\t\t \"Expected Location\");\n\n\t\tipr_err_separator;\n\t}\n}\n\n \nstatic void ipr_log_array_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\tstruct ipr_hostrcb *hostrcb)\n{\n\tint i;\n\tstruct ipr_hostrcb_type_04_error *error;\n\tstruct ipr_hostrcb_array_data_entry *array_entry;\n\tconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\n\n\terror = &hostrcb->hcam.u.error.u.type_04_error;\n\n\tipr_err_separator;\n\n\tipr_err(\"RAID %s Array Configuration: %d:%d:%d:%d\\n\",\n\t\terror->protection_level,\n\t\tioa_cfg->host->host_no,\n\t\terror->last_func_vset_res_addr.bus,\n\t\terror->last_func_vset_res_addr.target,\n\t\terror->last_func_vset_res_addr.lun);\n\n\tipr_err_separator;\n\n\tarray_entry = error->array_member;\n\n\tfor (i = 0; i < 18; i++) {\n\t\tif (!memcmp(array_entry->vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\n\t\t\tcontinue;\n\n\t\tif (be32_to_cpu(error->exposed_mode_adn) == i)\n\t\t\tipr_err(\"Exposed Array Member %d:\\n\", i);\n\t\telse\n\t\t\tipr_err(\"Array Member %d:\\n\", i);\n\n\t\tipr_log_vpd(&array_entry->vpd);\n\n\t\tipr_phys_res_err(ioa_cfg, array_entry->dev_res_addr, \"Current Location\");\n\t\tipr_phys_res_err(ioa_cfg, array_entry->expected_dev_res_addr,\n\t\t\t\t \"Expected Location\");\n\n\t\tipr_err_separator;\n\n\t\tif (i == 9)\n\t\t\tarray_entry = error->array_member2;\n\t\telse\n\t\t\tarray_entry++;\n\t}\n}\n\n \nstatic void ipr_log_hex_data(struct ipr_ioa_cfg *ioa_cfg, __be32 *data, int len)\n{\n\tint i;\n\n\tif (len == 0)\n\t\treturn;\n\n\tif (ioa_cfg->log_level <= IPR_DEFAULT_LOG_LEVEL)\n\t\tlen = min_t(int, len, IPR_DEFAULT_MAX_ERROR_DUMP);\n\n\tfor (i = 0; i < len / 4; i += 4) {\n\t\tipr_err(\"%08X: %08X %08X %08X %08X\\n\", i*4,\n\t\t\tbe32_to_cpu(data[i]),\n\t\t\tbe32_to_cpu(data[i+1]),\n\t\t\tbe32_to_cpu(data[i+2]),\n\t\t\tbe32_to_cpu(data[i+3]));\n\t}\n}\n\n \nstatic void ipr_log_enhanced_dual_ioa_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t    struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_17_error *error;\n\n\tif (ioa_cfg->sis64)\n\t\terror = &hostrcb->hcam.u.error64.u.type_17_error;\n\telse\n\t\terror = &hostrcb->hcam.u.error.u.type_17_error;\n\n\terror->failure_reason[sizeof(error->failure_reason) - 1] = '\\0';\n\tstrim(error->failure_reason);\n\n\tipr_hcam_err(hostrcb, \"%s [PRC: %08X]\\n\", error->failure_reason,\n\t\t     be32_to_cpu(hostrcb->hcam.u.error.prc));\n\tipr_log_ext_vpd_compact(\"Remote IOA\", hostrcb, &error->vpd);\n\tipr_log_hex_data(ioa_cfg, error->data,\n\t\t\t be32_to_cpu(hostrcb->hcam.length) -\n\t\t\t (offsetof(struct ipr_hostrcb_error, u) +\n\t\t\t  offsetof(struct ipr_hostrcb_type_17_error, data)));\n}\n\n \nstatic void ipr_log_dual_ioa_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t   struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_07_error *error;\n\n\terror = &hostrcb->hcam.u.error.u.type_07_error;\n\terror->failure_reason[sizeof(error->failure_reason) - 1] = '\\0';\n\tstrim(error->failure_reason);\n\n\tipr_hcam_err(hostrcb, \"%s [PRC: %08X]\\n\", error->failure_reason,\n\t\t     be32_to_cpu(hostrcb->hcam.u.error.prc));\n\tipr_log_vpd_compact(\"Remote IOA\", hostrcb, &error->vpd);\n\tipr_log_hex_data(ioa_cfg, error->data,\n\t\t\t be32_to_cpu(hostrcb->hcam.length) -\n\t\t\t (offsetof(struct ipr_hostrcb_error, u) +\n\t\t\t  offsetof(struct ipr_hostrcb_type_07_error, data)));\n}\n\nstatic const struct {\n\tu8 active;\n\tchar *desc;\n} path_active_desc[] = {\n\t{ IPR_PATH_NO_INFO, \"Path\" },\n\t{ IPR_PATH_ACTIVE, \"Active path\" },\n\t{ IPR_PATH_NOT_ACTIVE, \"Inactive path\" }\n};\n\nstatic const struct {\n\tu8 state;\n\tchar *desc;\n} path_state_desc[] = {\n\t{ IPR_PATH_STATE_NO_INFO, \"has no path state information available\" },\n\t{ IPR_PATH_HEALTHY, \"is healthy\" },\n\t{ IPR_PATH_DEGRADED, \"is degraded\" },\n\t{ IPR_PATH_FAILED, \"is failed\" }\n};\n\n \nstatic void ipr_log_fabric_path(struct ipr_hostrcb *hostrcb,\n\t\t\t\tstruct ipr_hostrcb_fabric_desc *fabric)\n{\n\tint i, j;\n\tu8 path_state = fabric->path_state;\n\tu8 active = path_state & IPR_PATH_ACTIVE_MASK;\n\tu8 state = path_state & IPR_PATH_STATE_MASK;\n\n\tfor (i = 0; i < ARRAY_SIZE(path_active_desc); i++) {\n\t\tif (path_active_desc[i].active != active)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(path_state_desc); j++) {\n\t\t\tif (path_state_desc[j].state != state)\n\t\t\t\tcontinue;\n\n\t\t\tif (fabric->cascaded_expander == 0xff && fabric->phy == 0xff) {\n\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: IOA Port=%d\\n\",\n\t\t\t\t\t     path_active_desc[i].desc, path_state_desc[j].desc,\n\t\t\t\t\t     fabric->ioa_port);\n\t\t\t} else if (fabric->cascaded_expander == 0xff) {\n\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: IOA Port=%d, Phy=%d\\n\",\n\t\t\t\t\t     path_active_desc[i].desc, path_state_desc[j].desc,\n\t\t\t\t\t     fabric->ioa_port, fabric->phy);\n\t\t\t} else if (fabric->phy == 0xff) {\n\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: IOA Port=%d, Cascade=%d\\n\",\n\t\t\t\t\t     path_active_desc[i].desc, path_state_desc[j].desc,\n\t\t\t\t\t     fabric->ioa_port, fabric->cascaded_expander);\n\t\t\t} else {\n\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: IOA Port=%d, Cascade=%d, Phy=%d\\n\",\n\t\t\t\t\t     path_active_desc[i].desc, path_state_desc[j].desc,\n\t\t\t\t\t     fabric->ioa_port, fabric->cascaded_expander, fabric->phy);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tipr_err(\"Path state=%02X IOA Port=%d Cascade=%d Phy=%d\\n\", path_state,\n\t\tfabric->ioa_port, fabric->cascaded_expander, fabric->phy);\n}\n\n \nstatic void ipr_log64_fabric_path(struct ipr_hostrcb *hostrcb,\n\t\t\t\t  struct ipr_hostrcb64_fabric_desc *fabric)\n{\n\tint i, j;\n\tu8 path_state = fabric->path_state;\n\tu8 active = path_state & IPR_PATH_ACTIVE_MASK;\n\tu8 state = path_state & IPR_PATH_STATE_MASK;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\tfor (i = 0; i < ARRAY_SIZE(path_active_desc); i++) {\n\t\tif (path_active_desc[i].active != active)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(path_state_desc); j++) {\n\t\t\tif (path_state_desc[j].state != state)\n\t\t\t\tcontinue;\n\n\t\t\tipr_hcam_err(hostrcb, \"%s %s: Resource Path=%s\\n\",\n\t\t\t\t     path_active_desc[i].desc, path_state_desc[j].desc,\n\t\t\t\t     ipr_format_res_path(hostrcb->ioa_cfg,\n\t\t\t\t\t\tfabric->res_path,\n\t\t\t\t\t\tbuffer, sizeof(buffer)));\n\t\t\treturn;\n\t\t}\n\t}\n\n\tipr_err(\"Path state=%02X Resource Path=%s\\n\", path_state,\n\t\tipr_format_res_path(hostrcb->ioa_cfg, fabric->res_path,\n\t\t\t\t    buffer, sizeof(buffer)));\n}\n\nstatic const struct {\n\tu8 type;\n\tchar *desc;\n} path_type_desc[] = {\n\t{ IPR_PATH_CFG_IOA_PORT, \"IOA port\" },\n\t{ IPR_PATH_CFG_EXP_PORT, \"Expander port\" },\n\t{ IPR_PATH_CFG_DEVICE_PORT, \"Device port\" },\n\t{ IPR_PATH_CFG_DEVICE_LUN, \"Device LUN\" }\n};\n\nstatic const struct {\n\tu8 status;\n\tchar *desc;\n} path_status_desc[] = {\n\t{ IPR_PATH_CFG_NO_PROB, \"Functional\" },\n\t{ IPR_PATH_CFG_DEGRADED, \"Degraded\" },\n\t{ IPR_PATH_CFG_FAILED, \"Failed\" },\n\t{ IPR_PATH_CFG_SUSPECT, \"Suspect\" },\n\t{ IPR_PATH_NOT_DETECTED, \"Missing\" },\n\t{ IPR_PATH_INCORRECT_CONN, \"Incorrectly connected\" }\n};\n\nstatic const char *link_rate[] = {\n\t\"unknown\",\n\t\"disabled\",\n\t\"phy reset problem\",\n\t\"spinup hold\",\n\t\"port selector\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"1.5Gbps\",\n\t\"3.0Gbps\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"unknown\",\n\t\"unknown\"\n};\n\n \nstatic void ipr_log_path_elem(struct ipr_hostrcb *hostrcb,\n\t\t\t      struct ipr_hostrcb_config_element *cfg)\n{\n\tint i, j;\n\tu8 type = cfg->type_status & IPR_PATH_CFG_TYPE_MASK;\n\tu8 status = cfg->type_status & IPR_PATH_CFG_STATUS_MASK;\n\n\tif (type == IPR_PATH_CFG_NOT_EXIST)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(path_type_desc); i++) {\n\t\tif (path_type_desc[i].type != type)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(path_status_desc); j++) {\n\t\t\tif (path_status_desc[j].status != status)\n\t\t\t\tcontinue;\n\n\t\t\tif (type == IPR_PATH_CFG_IOA_PORT) {\n\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: Phy=%d, Link rate=%s, WWN=%08X%08X\\n\",\n\t\t\t\t\t     path_status_desc[j].desc, path_type_desc[i].desc,\n\t\t\t\t\t     cfg->phy, link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n\t\t\t} else {\n\t\t\t\tif (cfg->cascaded_expander == 0xff && cfg->phy == 0xff) {\n\t\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: Link rate=%s, WWN=%08X%08X\\n\",\n\t\t\t\t\t\t     path_status_desc[j].desc, path_type_desc[i].desc,\n\t\t\t\t\t\t     link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n\t\t\t\t} else if (cfg->cascaded_expander == 0xff) {\n\t\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: Phy=%d, Link rate=%s, \"\n\t\t\t\t\t\t     \"WWN=%08X%08X\\n\", path_status_desc[j].desc,\n\t\t\t\t\t\t     path_type_desc[i].desc, cfg->phy,\n\t\t\t\t\t\t     link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n\t\t\t\t} else if (cfg->phy == 0xff) {\n\t\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: Cascade=%d, Link rate=%s, \"\n\t\t\t\t\t\t     \"WWN=%08X%08X\\n\", path_status_desc[j].desc,\n\t\t\t\t\t\t     path_type_desc[i].desc, cfg->cascaded_expander,\n\t\t\t\t\t\t     link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n\t\t\t\t} else {\n\t\t\t\t\tipr_hcam_err(hostrcb, \"%s %s: Cascade=%d, Phy=%d, Link rate=%s \"\n\t\t\t\t\t\t     \"WWN=%08X%08X\\n\", path_status_desc[j].desc,\n\t\t\t\t\t\t     path_type_desc[i].desc, cfg->cascaded_expander, cfg->phy,\n\t\t\t\t\t\t     link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tipr_hcam_err(hostrcb, \"Path element=%02X: Cascade=%d Phy=%d Link rate=%s \"\n\t\t     \"WWN=%08X%08X\\n\", cfg->type_status, cfg->cascaded_expander, cfg->phy,\n\t\t     link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t     be32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n}\n\n \nstatic void ipr_log64_path_elem(struct ipr_hostrcb *hostrcb,\n\t\t\t\tstruct ipr_hostrcb64_config_element *cfg)\n{\n\tint i, j;\n\tu8 desc_id = cfg->descriptor_id & IPR_DESCRIPTOR_MASK;\n\tu8 type = cfg->type_status & IPR_PATH_CFG_TYPE_MASK;\n\tu8 status = cfg->type_status & IPR_PATH_CFG_STATUS_MASK;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\tif (type == IPR_PATH_CFG_NOT_EXIST || desc_id != IPR_DESCRIPTOR_SIS64)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(path_type_desc); i++) {\n\t\tif (path_type_desc[i].type != type)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(path_status_desc); j++) {\n\t\t\tif (path_status_desc[j].status != status)\n\t\t\t\tcontinue;\n\n\t\t\tipr_hcam_err(hostrcb, \"%s %s: Resource Path=%s, Link rate=%s, WWN=%08X%08X\\n\",\n\t\t\t\t     path_status_desc[j].desc, path_type_desc[i].desc,\n\t\t\t\t     ipr_format_res_path(hostrcb->ioa_cfg,\n\t\t\t\t\tcfg->res_path, buffer, sizeof(buffer)),\n\t\t\t\t\tlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\t\t\tbe32_to_cpu(cfg->wwid[0]),\n\t\t\t\t\tbe32_to_cpu(cfg->wwid[1]));\n\t\t\treturn;\n\t\t}\n\t}\n\tipr_hcam_err(hostrcb, \"Path element=%02X: Resource Path=%s, Link rate=%s \"\n\t\t     \"WWN=%08X%08X\\n\", cfg->type_status,\n\t\t     ipr_format_res_path(hostrcb->ioa_cfg,\n\t\t\tcfg->res_path, buffer, sizeof(buffer)),\n\t\t\tlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\n\t\t\tbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\n}\n\n \nstatic void ipr_log_fabric_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_20_error *error;\n\tstruct ipr_hostrcb_fabric_desc *fabric;\n\tstruct ipr_hostrcb_config_element *cfg;\n\tint i, add_len;\n\n\terror = &hostrcb->hcam.u.error.u.type_20_error;\n\terror->failure_reason[sizeof(error->failure_reason) - 1] = '\\0';\n\tipr_hcam_err(hostrcb, \"%s\\n\", error->failure_reason);\n\n\tadd_len = be32_to_cpu(hostrcb->hcam.length) -\n\t\t(offsetof(struct ipr_hostrcb_error, u) +\n\t\t offsetof(struct ipr_hostrcb_type_20_error, desc));\n\n\tfor (i = 0, fabric = error->desc; i < error->num_entries; i++) {\n\t\tipr_log_fabric_path(hostrcb, fabric);\n\t\tfor_each_fabric_cfg(fabric, cfg)\n\t\t\tipr_log_path_elem(hostrcb, cfg);\n\n\t\tadd_len -= be16_to_cpu(fabric->length);\n\t\tfabric = (struct ipr_hostrcb_fabric_desc *)\n\t\t\t((unsigned long)fabric + be16_to_cpu(fabric->length));\n\t}\n\n\tipr_log_hex_data(ioa_cfg, (__be32 *)fabric, add_len);\n}\n\n \nstatic void ipr_log_sis64_array_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t      struct ipr_hostrcb *hostrcb)\n{\n\tint i, num_entries;\n\tstruct ipr_hostrcb_type_24_error *error;\n\tstruct ipr_hostrcb64_array_data_entry *array_entry;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\tconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\n\n\terror = &hostrcb->hcam.u.error64.u.type_24_error;\n\n\tipr_err_separator;\n\n\tipr_err(\"RAID %s Array Configuration: %s\\n\",\n\t\terror->protection_level,\n\t\tipr_format_res_path(ioa_cfg, error->last_res_path,\n\t\t\tbuffer, sizeof(buffer)));\n\n\tipr_err_separator;\n\n\tarray_entry = error->array_member;\n\tnum_entries = min_t(u32, error->num_entries,\n\t\t\t    ARRAY_SIZE(error->array_member));\n\n\tfor (i = 0; i < num_entries; i++, array_entry++) {\n\n\t\tif (!memcmp(array_entry->vpd.vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\n\t\t\tcontinue;\n\n\t\tif (error->exposed_mode_adn == i)\n\t\t\tipr_err(\"Exposed Array Member %d:\\n\", i);\n\t\telse\n\t\t\tipr_err(\"Array Member %d:\\n\", i);\n\n\t\tipr_err(\"Array Member %d:\\n\", i);\n\t\tipr_log_ext_vpd(&array_entry->vpd);\n\t\tipr_err(\"Current Location: %s\\n\",\n\t\t\t ipr_format_res_path(ioa_cfg, array_entry->res_path,\n\t\t\t\tbuffer, sizeof(buffer)));\n\t\tipr_err(\"Expected Location: %s\\n\",\n\t\t\t ipr_format_res_path(ioa_cfg,\n\t\t\t\tarray_entry->expected_res_path,\n\t\t\t\tbuffer, sizeof(buffer)));\n\n\t\tipr_err_separator;\n\t}\n}\n\n \nstatic void ipr_log_sis64_fabric_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t       struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_30_error *error;\n\tstruct ipr_hostrcb64_fabric_desc *fabric;\n\tstruct ipr_hostrcb64_config_element *cfg;\n\tint i, add_len;\n\n\terror = &hostrcb->hcam.u.error64.u.type_30_error;\n\n\terror->failure_reason[sizeof(error->failure_reason) - 1] = '\\0';\n\tipr_hcam_err(hostrcb, \"%s\\n\", error->failure_reason);\n\n\tadd_len = be32_to_cpu(hostrcb->hcam.length) -\n\t\t(offsetof(struct ipr_hostrcb64_error, u) +\n\t\t offsetof(struct ipr_hostrcb_type_30_error, desc));\n\n\tfor (i = 0, fabric = error->desc; i < error->num_entries; i++) {\n\t\tipr_log64_fabric_path(hostrcb, fabric);\n\t\tfor_each_fabric_cfg(fabric, cfg)\n\t\t\tipr_log64_path_elem(hostrcb, cfg);\n\n\t\tadd_len -= be16_to_cpu(fabric->length);\n\t\tfabric = (struct ipr_hostrcb64_fabric_desc *)\n\t\t\t((unsigned long)fabric + be16_to_cpu(fabric->length));\n\t}\n\n\tipr_log_hex_data(ioa_cfg, (__be32 *)fabric, add_len);\n}\n\n \nstatic void ipr_log_sis64_service_required_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t       struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_41_error *error;\n\n\terror = &hostrcb->hcam.u.error64.u.type_41_error;\n\n\terror->failure_reason[sizeof(error->failure_reason) - 1] = '\\0';\n\tipr_err(\"Primary Failure Reason: %s\\n\", error->failure_reason);\n\tipr_log_hex_data(ioa_cfg, error->data,\n\t\t\t be32_to_cpu(hostrcb->hcam.length) -\n\t\t\t (offsetof(struct ipr_hostrcb_error, u) +\n\t\t\t  offsetof(struct ipr_hostrcb_type_41_error, data)));\n}\n \nstatic void ipr_log_generic_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t  struct ipr_hostrcb *hostrcb)\n{\n\tipr_log_hex_data(ioa_cfg, hostrcb->hcam.u.raw.data,\n\t\t\t be32_to_cpu(hostrcb->hcam.length));\n}\n\n \nstatic void ipr_log_sis64_device_error(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t struct ipr_hostrcb *hostrcb)\n{\n\tstruct ipr_hostrcb_type_21_error *error;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\terror = &hostrcb->hcam.u.error64.u.type_21_error;\n\n\tipr_err(\"-----Failing Device Information-----\\n\");\n\tipr_err(\"World Wide Unique ID: %08X%08X%08X%08X\\n\",\n\t\tbe32_to_cpu(error->wwn[0]), be32_to_cpu(error->wwn[1]),\n\t\t be32_to_cpu(error->wwn[2]), be32_to_cpu(error->wwn[3]));\n\tipr_err(\"Device Resource Path: %s\\n\",\n\t\t__ipr_format_res_path(error->res_path,\n\t\t\t\t      buffer, sizeof(buffer)));\n\terror->primary_problem_desc[sizeof(error->primary_problem_desc) - 1] = '\\0';\n\terror->second_problem_desc[sizeof(error->second_problem_desc) - 1] = '\\0';\n\tipr_err(\"Primary Problem Description: %s\\n\", error->primary_problem_desc);\n\tipr_err(\"Secondary Problem Description:  %s\\n\", error->second_problem_desc);\n\tipr_err(\"SCSI Sense Data:\\n\");\n\tipr_log_hex_data(ioa_cfg, error->sense_data, sizeof(error->sense_data));\n\tipr_err(\"SCSI Command Descriptor Block: \\n\");\n\tipr_log_hex_data(ioa_cfg, error->cdb, sizeof(error->cdb));\n\n\tipr_err(\"Additional IOA Data:\\n\");\n\tipr_log_hex_data(ioa_cfg, error->ioa_data, be32_to_cpu(error->length_of_error));\n}\n\n \nstatic u32 ipr_get_error(u32 ioasc)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ipr_error_table); i++)\n\t\tif (ipr_error_table[i].ioasc == (ioasc & IPR_IOASC_IOASC_MASK))\n\t\t\treturn i;\n\n\treturn 0;\n}\n\n \nstatic void ipr_handle_log_data(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\tstruct ipr_hostrcb *hostrcb)\n{\n\tu32 ioasc;\n\tint error_index;\n\tstruct ipr_hostrcb_type_21_error *error;\n\n\tif (hostrcb->hcam.notify_type != IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY)\n\t\treturn;\n\n\tif (hostrcb->hcam.notifications_lost == IPR_HOST_RCB_NOTIFICATIONS_LOST)\n\t\tdev_err(&ioa_cfg->pdev->dev, \"Error notifications lost\\n\");\n\n\tif (ioa_cfg->sis64)\n\t\tioasc = be32_to_cpu(hostrcb->hcam.u.error64.fd_ioasc);\n\telse\n\t\tioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\n\n\tif (!ioa_cfg->sis64 && (ioasc == IPR_IOASC_BUS_WAS_RESET ||\n\t    ioasc == IPR_IOASC_BUS_WAS_RESET_BY_OTHER)) {\n\t\t \n\t\tscsi_report_bus_reset(ioa_cfg->host,\n\t\t\t\t      hostrcb->hcam.u.error.fd_res_addr.bus);\n\t}\n\n\terror_index = ipr_get_error(ioasc);\n\n\tif (!ipr_error_table[error_index].log_hcam)\n\t\treturn;\n\n\tif (ioasc == IPR_IOASC_HW_CMD_FAILED &&\n\t    hostrcb->hcam.overlay_id == IPR_HOST_RCB_OVERLAY_ID_21) {\n\t\terror = &hostrcb->hcam.u.error64.u.type_21_error;\n\n\t\tif (((be32_to_cpu(error->sense_data[0]) & 0x0000ff00) >> 8) == ILLEGAL_REQUEST &&\n\t\t\tioa_cfg->log_level <= IPR_DEFAULT_LOG_LEVEL)\n\t\t\t\treturn;\n\t}\n\n\tipr_hcam_err(hostrcb, \"%s\\n\", ipr_error_table[error_index].error);\n\n\t \n\tioa_cfg->errors_logged++;\n\n\tif (ioa_cfg->log_level < ipr_error_table[error_index].log_hcam)\n\t\treturn;\n\tif (be32_to_cpu(hostrcb->hcam.length) > sizeof(hostrcb->hcam.u.raw))\n\t\thostrcb->hcam.length = cpu_to_be32(sizeof(hostrcb->hcam.u.raw));\n\n\tswitch (hostrcb->hcam.overlay_id) {\n\tcase IPR_HOST_RCB_OVERLAY_ID_2:\n\t\tipr_log_cache_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_3:\n\t\tipr_log_config_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_4:\n\tcase IPR_HOST_RCB_OVERLAY_ID_6:\n\t\tipr_log_array_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_7:\n\t\tipr_log_dual_ioa_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_12:\n\t\tipr_log_enhanced_cache_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_13:\n\t\tipr_log_enhanced_config_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_14:\n\tcase IPR_HOST_RCB_OVERLAY_ID_16:\n\t\tipr_log_enhanced_array_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_17:\n\t\tipr_log_enhanced_dual_ioa_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_20:\n\t\tipr_log_fabric_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_21:\n\t\tipr_log_sis64_device_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_23:\n\t\tipr_log_sis64_config_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_24:\n\tcase IPR_HOST_RCB_OVERLAY_ID_26:\n\t\tipr_log_sis64_array_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_30:\n\t\tipr_log_sis64_fabric_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_41:\n\t\tipr_log_sis64_service_required_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\tcase IPR_HOST_RCB_OVERLAY_ID_1:\n\tcase IPR_HOST_RCB_OVERLAY_ID_DEFAULT:\n\tdefault:\n\t\tipr_log_generic_error(ioa_cfg, hostrcb);\n\t\tbreak;\n\t}\n}\n\nstatic struct ipr_hostrcb *ipr_get_free_hostrcb(struct ipr_ioa_cfg *ioa)\n{\n\tstruct ipr_hostrcb *hostrcb;\n\n\thostrcb = list_first_entry_or_null(&ioa->hostrcb_free_q,\n\t\t\t\t\tstruct ipr_hostrcb, queue);\n\n\tif (unlikely(!hostrcb)) {\n\t\tdev_info(&ioa->pdev->dev, \"Reclaiming async error buffers.\");\n\t\thostrcb = list_first_entry_or_null(&ioa->hostrcb_report_q,\n\t\t\t\t\t\tstruct ipr_hostrcb, queue);\n\t}\n\n\tlist_del_init(&hostrcb->queue);\n\treturn hostrcb;\n}\n\n \nstatic void ipr_process_error(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_hostrcb *hostrcb = ipr_cmd->u.hostrcb;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\tu32 fd_ioasc;\n\n\tif (ioa_cfg->sis64)\n\t\tfd_ioasc = be32_to_cpu(hostrcb->hcam.u.error64.fd_ioasc);\n\telse\n\t\tfd_ioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\n\n\tlist_del_init(&hostrcb->queue);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\n\tif (!ioasc) {\n\t\tipr_handle_log_data(ioa_cfg, hostrcb);\n\t\tif (fd_ioasc == IPR_IOASC_NR_IOA_RESET_REQUIRED)\n\t\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_ABBREV);\n\t} else if (ioasc != IPR_IOASC_IOA_WAS_RESET &&\n\t\t   ioasc != IPR_IOASC_ABORTED_CMD_TERM_BY_HOST) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Host RCB failed with IOASC: 0x%08X\\n\", ioasc);\n\t}\n\n\tlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_report_q);\n\tschedule_work(&ioa_cfg->work_q);\n\thostrcb = ipr_get_free_hostrcb(ioa_cfg);\n\n\tipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);\n}\n\n \nstatic void ipr_timeout(struct timer_list *t)\n{\n\tstruct ipr_cmnd *ipr_cmd = from_timer(ipr_cmd, t, timer);\n\tunsigned long lock_flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tioa_cfg->errors_logged++;\n\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\"Adapter being reset due to command timeout.\\n\");\n\n\tif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\n\t\tioa_cfg->sdt_state = GET_DUMP;\n\n\tif (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd)\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n}\n\n \nstatic void ipr_oper_timeout(struct timer_list *t)\n{\n\tstruct ipr_cmnd *ipr_cmd = from_timer(ipr_cmd, t, timer);\n\tunsigned long lock_flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tioa_cfg->errors_logged++;\n\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\"Adapter timed out transitioning to operational.\\n\");\n\n\tif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\n\t\tioa_cfg->sdt_state = GET_DUMP;\n\n\tif (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd) {\n\t\tif (ipr_fastfail)\n\t\t\tioa_cfg->reset_retries += IPR_NUM_RESET_RELOAD_RETRIES;\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t}\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n}\n\n \nstatic const struct ipr_ses_table_entry *\nipr_find_ses_entry(struct ipr_resource_entry *res)\n{\n\tint i, j, matches;\n\tstruct ipr_std_inq_vpids *vpids;\n\tconst struct ipr_ses_table_entry *ste = ipr_ses_table;\n\n\tfor (i = 0; i < ARRAY_SIZE(ipr_ses_table); i++, ste++) {\n\t\tfor (j = 0, matches = 0; j < IPR_PROD_ID_LEN; j++) {\n\t\t\tif (ste->compare_product_id_byte[j] == 'X') {\n\t\t\t\tvpids = &res->std_inq_data.vpids;\n\t\t\t\tif (vpids->product_id[j] == ste->product_id[j])\n\t\t\t\t\tmatches++;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tmatches++;\n\t\t}\n\n\t\tif (matches == IPR_PROD_ID_LEN)\n\t\t\treturn ste;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic u32 ipr_get_max_scsi_speed(struct ipr_ioa_cfg *ioa_cfg, u8 bus, u8 bus_width)\n{\n\tstruct ipr_resource_entry *res;\n\tconst struct ipr_ses_table_entry *ste;\n\tu32 max_xfer_rate = IPR_MAX_SCSI_RATE(bus_width);\n\n\t \n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif (!(IPR_IS_SES_DEVICE(res->std_inq_data)))\n\t\t\tcontinue;\n\n\t\tif (bus != res->bus)\n\t\t\tcontinue;\n\n\t\tif (!(ste = ipr_find_ses_entry(res)))\n\t\t\tcontinue;\n\n\t\tmax_xfer_rate = (ste->max_bus_speed_limit * 10) / (bus_width / 8);\n\t}\n\n\treturn max_xfer_rate;\n}\n\n \nstatic int ipr_wait_iodbg_ack(struct ipr_ioa_cfg *ioa_cfg, int max_delay)\n{\n\tvolatile u32 pcii_reg;\n\tint delay = 1;\n\n\t \n\twhile (delay < max_delay) {\n\t\tpcii_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\n\n\t\tif (pcii_reg & IPR_PCII_IO_DEBUG_ACKNOWLEDGE)\n\t\t\treturn 0;\n\n\t\t \n\t\tif ((delay / 1000) > MAX_UDELAY_MS)\n\t\t\tmdelay(delay / 1000);\n\t\telse\n\t\t\tudelay(delay);\n\n\t\tdelay += delay;\n\t}\n\treturn -EIO;\n}\n\n \nstatic int ipr_get_sis64_dump_data_section(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t   u32 start_addr,\n\t\t\t\t\t   __be32 *dest, u32 length_in_words)\n{\n\tint i;\n\n\tfor (i = 0; i < length_in_words; i++) {\n\t\twritel(start_addr+(i*4), ioa_cfg->regs.dump_addr_reg);\n\t\t*dest = cpu_to_be32(readl(ioa_cfg->regs.dump_data_reg));\n\t\tdest++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ipr_get_ldump_data_section(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t      u32 start_addr,\n\t\t\t\t      __be32 *dest, u32 length_in_words)\n{\n\tvolatile u32 temp_pcii_reg;\n\tint i, delay = 0;\n\n\tif (ioa_cfg->sis64)\n\t\treturn ipr_get_sis64_dump_data_section(ioa_cfg, start_addr,\n\t\t\t\t\t\t       dest, length_in_words);\n\n\t \n\twritel((IPR_UPROCI_RESET_ALERT | IPR_UPROCI_IO_DEBUG_ALERT),\n\t       ioa_cfg->regs.set_uproc_interrupt_reg32);\n\n\t \n\tif (ipr_wait_iodbg_ack(ioa_cfg,\n\t\t\t       IPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC)) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"IOA dump long data transfer timeout\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\twritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\n\t       ioa_cfg->regs.clr_interrupt_reg);\n\n\t \n\twritel(start_addr, ioa_cfg->ioa_mailbox);\n\n\t \n\twritel(IPR_UPROCI_RESET_ALERT,\n\t       ioa_cfg->regs.clr_uproc_interrupt_reg32);\n\n\tfor (i = 0; i < length_in_words; i++) {\n\t\t \n\t\tif (ipr_wait_iodbg_ack(ioa_cfg,\n\t\t\t\t       IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC)) {\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"IOA dump short data transfer timeout\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\t*dest = cpu_to_be32(readl(ioa_cfg->ioa_mailbox));\n\t\tdest++;\n\n\t\t \n\t\tif (i < (length_in_words - 1)) {\n\t\t\t \n\t\t\twritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\n\t\t\t       ioa_cfg->regs.clr_interrupt_reg);\n\t\t}\n\t}\n\n\t \n\twritel(IPR_UPROCI_RESET_ALERT,\n\t       ioa_cfg->regs.set_uproc_interrupt_reg32);\n\n\twritel(IPR_UPROCI_IO_DEBUG_ALERT,\n\t       ioa_cfg->regs.clr_uproc_interrupt_reg32);\n\n\t \n\twritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\n\t       ioa_cfg->regs.clr_interrupt_reg);\n\n\t \n\twhile (delay < IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC) {\n\t\ttemp_pcii_reg =\n\t\t    readl(ioa_cfg->regs.sense_uproc_interrupt_reg32);\n\n\t\tif (!(temp_pcii_reg & IPR_UPROCI_RESET_ALERT))\n\t\t\treturn 0;\n\n\t\tudelay(10);\n\t\tdelay += 10;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_SCSI_IPR_DUMP\n \nstatic int ipr_sdt_copy(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\tunsigned long pci_address, u32 length)\n{\n\tint bytes_copied = 0;\n\tint cur_len, rc, rem_len, rem_page_len, max_dump_size;\n\t__be32 *page;\n\tunsigned long lock_flags = 0;\n\tstruct ipr_ioa_dump *ioa_dump = &ioa_cfg->dump->ioa_dump;\n\n\tif (ioa_cfg->sis64)\n\t\tmax_dump_size = IPR_FMT3_MAX_IOA_DUMP_SIZE;\n\telse\n\t\tmax_dump_size = IPR_FMT2_MAX_IOA_DUMP_SIZE;\n\n\twhile (bytes_copied < length &&\n\t       (ioa_dump->hdr.len + bytes_copied) < max_dump_size) {\n\t\tif (ioa_dump->page_offset >= PAGE_SIZE ||\n\t\t    ioa_dump->page_offset == 0) {\n\t\t\tpage = (__be32 *)__get_free_page(GFP_ATOMIC);\n\n\t\t\tif (!page) {\n\t\t\t\tipr_trace;\n\t\t\t\treturn bytes_copied;\n\t\t\t}\n\n\t\t\tioa_dump->page_offset = 0;\n\t\t\tioa_dump->ioa_data[ioa_dump->next_page_index] = page;\n\t\t\tioa_dump->next_page_index++;\n\t\t} else\n\t\t\tpage = ioa_dump->ioa_data[ioa_dump->next_page_index - 1];\n\n\t\trem_len = length - bytes_copied;\n\t\trem_page_len = PAGE_SIZE - ioa_dump->page_offset;\n\t\tcur_len = min(rem_len, rem_page_len);\n\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\tif (ioa_cfg->sdt_state == ABORT_DUMP) {\n\t\t\trc = -EIO;\n\t\t} else {\n\t\t\trc = ipr_get_ldump_data_section(ioa_cfg,\n\t\t\t\t\t\t\tpci_address + bytes_copied,\n\t\t\t\t\t\t\t&page[ioa_dump->page_offset / 4],\n\t\t\t\t\t\t\t(cur_len / sizeof(u32)));\n\t\t}\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\t\tif (!rc) {\n\t\t\tioa_dump->page_offset += cur_len;\n\t\t\tbytes_copied += cur_len;\n\t\t} else {\n\t\t\tipr_trace;\n\t\t\tbreak;\n\t\t}\n\t\tschedule();\n\t}\n\n\treturn bytes_copied;\n}\n\n \nstatic void ipr_init_dump_entry_hdr(struct ipr_dump_entry_header *hdr)\n{\n\thdr->eye_catcher = IPR_DUMP_EYE_CATCHER;\n\thdr->num_elems = 1;\n\thdr->offset = sizeof(*hdr);\n\thdr->status = IPR_DUMP_STATUS_SUCCESS;\n}\n\n \nstatic void ipr_dump_ioa_type_data(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t   struct ipr_driver_dump *driver_dump)\n{\n\tstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\n\n\tipr_init_dump_entry_hdr(&driver_dump->ioa_type_entry.hdr);\n\tdriver_dump->ioa_type_entry.hdr.len =\n\t\tsizeof(struct ipr_dump_ioa_type_entry) -\n\t\tsizeof(struct ipr_dump_entry_header);\n\tdriver_dump->ioa_type_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\n\tdriver_dump->ioa_type_entry.hdr.id = IPR_DUMP_DRIVER_TYPE_ID;\n\tdriver_dump->ioa_type_entry.type = ioa_cfg->type;\n\tdriver_dump->ioa_type_entry.fw_version = (ucode_vpd->major_release << 24) |\n\t\t(ucode_vpd->card_type << 16) | (ucode_vpd->minor_release[0] << 8) |\n\t\tucode_vpd->minor_release[1];\n\tdriver_dump->hdr.num_entries++;\n}\n\n \nstatic void ipr_dump_version_data(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t  struct ipr_driver_dump *driver_dump)\n{\n\tipr_init_dump_entry_hdr(&driver_dump->version_entry.hdr);\n\tdriver_dump->version_entry.hdr.len =\n\t\tsizeof(struct ipr_dump_version_entry) -\n\t\tsizeof(struct ipr_dump_entry_header);\n\tdriver_dump->version_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;\n\tdriver_dump->version_entry.hdr.id = IPR_DUMP_DRIVER_VERSION_ID;\n\tstrcpy(driver_dump->version_entry.version, IPR_DRIVER_VERSION);\n\tdriver_dump->hdr.num_entries++;\n}\n\n \nstatic void ipr_dump_trace_data(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t   struct ipr_driver_dump *driver_dump)\n{\n\tipr_init_dump_entry_hdr(&driver_dump->trace_entry.hdr);\n\tdriver_dump->trace_entry.hdr.len =\n\t\tsizeof(struct ipr_dump_trace_entry) -\n\t\tsizeof(struct ipr_dump_entry_header);\n\tdriver_dump->trace_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\n\tdriver_dump->trace_entry.hdr.id = IPR_DUMP_TRACE_ID;\n\tmemcpy(driver_dump->trace_entry.trace, ioa_cfg->trace, IPR_TRACE_SIZE);\n\tdriver_dump->hdr.num_entries++;\n}\n\n \nstatic void ipr_dump_location_data(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t   struct ipr_driver_dump *driver_dump)\n{\n\tipr_init_dump_entry_hdr(&driver_dump->location_entry.hdr);\n\tdriver_dump->location_entry.hdr.len =\n\t\tsizeof(struct ipr_dump_location_entry) -\n\t\tsizeof(struct ipr_dump_entry_header);\n\tdriver_dump->location_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;\n\tdriver_dump->location_entry.hdr.id = IPR_DUMP_LOCATION_ID;\n\tstrcpy(driver_dump->location_entry.location, dev_name(&ioa_cfg->pdev->dev));\n\tdriver_dump->hdr.num_entries++;\n}\n\n \nstatic void ipr_get_ioa_dump(struct ipr_ioa_cfg *ioa_cfg, struct ipr_dump *dump)\n{\n\tunsigned long start_addr, sdt_word;\n\tunsigned long lock_flags = 0;\n\tstruct ipr_driver_dump *driver_dump = &dump->driver_dump;\n\tstruct ipr_ioa_dump *ioa_dump = &dump->ioa_dump;\n\tu32 num_entries, max_num_entries, start_off, end_off;\n\tu32 max_dump_size, bytes_to_copy, bytes_copied, rc;\n\tstruct ipr_sdt *sdt;\n\tint valid = 1;\n\tint i;\n\n\tENTER;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (ioa_cfg->sdt_state != READ_DUMP) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tif (ioa_cfg->sis64) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tssleep(IPR_DUMP_DELAY_SECONDS);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t}\n\n\tstart_addr = readl(ioa_cfg->ioa_mailbox);\n\n\tif (!ioa_cfg->sis64 && !ipr_sdt_is_fmt2(start_addr)) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Invalid dump table format: %lx\\n\", start_addr);\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tdev_err(&ioa_cfg->pdev->dev, \"Dump of IOA initiated\\n\");\n\n\tdriver_dump->hdr.eye_catcher = IPR_DUMP_EYE_CATCHER;\n\n\t \n\tdriver_dump->hdr.len = sizeof(struct ipr_driver_dump);\n\tdriver_dump->hdr.num_entries = 1;\n\tdriver_dump->hdr.first_entry_offset = sizeof(struct ipr_dump_header);\n\tdriver_dump->hdr.status = IPR_DUMP_STATUS_SUCCESS;\n\tdriver_dump->hdr.os = IPR_DUMP_OS_LINUX;\n\tdriver_dump->hdr.driver_name = IPR_DUMP_DRIVER_NAME;\n\n\tipr_dump_version_data(ioa_cfg, driver_dump);\n\tipr_dump_location_data(ioa_cfg, driver_dump);\n\tipr_dump_ioa_type_data(ioa_cfg, driver_dump);\n\tipr_dump_trace_data(ioa_cfg, driver_dump);\n\n\t \n\tdriver_dump->hdr.len += sizeof(struct ipr_dump_entry_header);\n\n\t \n\tipr_init_dump_entry_hdr(&ioa_dump->hdr);\n\tioa_dump->hdr.len = 0;\n\tioa_dump->hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\n\tioa_dump->hdr.id = IPR_DUMP_IOA_DUMP_ID;\n\n\t \n\tsdt = &ioa_dump->sdt;\n\n\tif (ioa_cfg->sis64) {\n\t\tmax_num_entries = IPR_FMT3_NUM_SDT_ENTRIES;\n\t\tmax_dump_size = IPR_FMT3_MAX_IOA_DUMP_SIZE;\n\t} else {\n\t\tmax_num_entries = IPR_FMT2_NUM_SDT_ENTRIES;\n\t\tmax_dump_size = IPR_FMT2_MAX_IOA_DUMP_SIZE;\n\t}\n\n\tbytes_to_copy = offsetof(struct ipr_sdt, entry) +\n\t\t\t(max_num_entries * sizeof(struct ipr_sdt_entry));\n\trc = ipr_get_ldump_data_section(ioa_cfg, start_addr, (__be32 *)sdt,\n\t\t\t\t\tbytes_to_copy / sizeof(__be32));\n\n\t \n\tif (rc || ((be32_to_cpu(sdt->hdr.state) != IPR_FMT3_SDT_READY_TO_USE) &&\n\t    (be32_to_cpu(sdt->hdr.state) != IPR_FMT2_SDT_READY_TO_USE))) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Dump of IOA failed. Dump table not valid: %d, %X.\\n\",\n\t\t\trc, be32_to_cpu(sdt->hdr.state));\n\t\tdriver_dump->hdr.status = IPR_DUMP_STATUS_FAILED;\n\t\tioa_cfg->sdt_state = DUMP_OBTAINED;\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tnum_entries = be32_to_cpu(sdt->hdr.num_entries_used);\n\n\tif (num_entries > max_num_entries)\n\t\tnum_entries = max_num_entries;\n\n\t \n\tdump->driver_dump.hdr.len += sizeof(struct ipr_sdt_header);\n\tif (ioa_cfg->sis64)\n\t\tdump->driver_dump.hdr.len += num_entries * sizeof(struct ipr_sdt_entry);\n\telse\n\t\tdump->driver_dump.hdr.len += max_num_entries * sizeof(struct ipr_sdt_entry);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tif (ioa_dump->hdr.len > max_dump_size) {\n\t\t\tdriver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sdt->entry[i].flags & IPR_SDT_VALID_ENTRY) {\n\t\t\tsdt_word = be32_to_cpu(sdt->entry[i].start_token);\n\t\t\tif (ioa_cfg->sis64)\n\t\t\t\tbytes_to_copy = be32_to_cpu(sdt->entry[i].end_token);\n\t\t\telse {\n\t\t\t\tstart_off = sdt_word & IPR_FMT2_MBX_ADDR_MASK;\n\t\t\t\tend_off = be32_to_cpu(sdt->entry[i].end_token);\n\n\t\t\t\tif (ipr_sdt_is_fmt2(sdt_word) && sdt_word)\n\t\t\t\t\tbytes_to_copy = end_off - start_off;\n\t\t\t\telse\n\t\t\t\t\tvalid = 0;\n\t\t\t}\n\t\t\tif (valid) {\n\t\t\t\tif (bytes_to_copy > max_dump_size) {\n\t\t\t\t\tsdt->entry[i].flags &= ~IPR_SDT_VALID_ENTRY;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tbytes_copied = ipr_sdt_copy(ioa_cfg, sdt_word,\n\t\t\t\t\t\t\t    bytes_to_copy);\n\n\t\t\t\tioa_dump->hdr.len += bytes_copied;\n\n\t\t\t\tif (bytes_copied != bytes_to_copy) {\n\t\t\t\t\tdriver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_err(&ioa_cfg->pdev->dev, \"Dump of IOA completed.\\n\");\n\n\t \n\tdriver_dump->hdr.len += ioa_dump->hdr.len;\n\twmb();\n\tioa_cfg->sdt_state = DUMP_OBTAINED;\n\tLEAVE;\n}\n\n#else\n#define ipr_get_ioa_dump(ioa_cfg, dump) do { } while (0)\n#endif\n\n \nstatic void ipr_release_dump(struct kref *kref)\n{\n\tstruct ipr_dump *dump = container_of(kref, struct ipr_dump, kref);\n\tstruct ipr_ioa_cfg *ioa_cfg = dump->ioa_cfg;\n\tunsigned long lock_flags = 0;\n\tint i;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tioa_cfg->dump = NULL;\n\tioa_cfg->sdt_state = INACTIVE;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tfor (i = 0; i < dump->ioa_dump.next_page_index; i++)\n\t\tfree_page((unsigned long) dump->ioa_dump.ioa_data[i]);\n\n\tvfree(dump->ioa_dump.ioa_data);\n\tkfree(dump);\n\tLEAVE;\n}\n\nstatic void ipr_add_remove_thread(struct work_struct *work)\n{\n\tunsigned long lock_flags;\n\tstruct ipr_resource_entry *res;\n\tstruct scsi_device *sdev;\n\tstruct ipr_ioa_cfg *ioa_cfg =\n\t\tcontainer_of(work, struct ipr_ioa_cfg, scsi_add_work_q);\n\tu8 bus, target, lun;\n\tint did_work;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\nrestart:\n\tdo {\n\t\tdid_work = 0;\n\t\tif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds) {\n\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\t\treturn;\n\t\t}\n\n\t\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\t\tif (res->del_from_ml && res->sdev) {\n\t\t\t\tdid_work = 1;\n\t\t\t\tsdev = res->sdev;\n\t\t\t\tif (!scsi_device_get(sdev)) {\n\t\t\t\t\tif (!res->add_to_ml)\n\t\t\t\t\t\tlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\n\t\t\t\t\telse\n\t\t\t\t\t\tres->del_from_ml = 0;\n\t\t\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\t\t\t\tscsi_remove_device(sdev);\n\t\t\t\t\tscsi_device_put(sdev);\n\t\t\t\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} while (did_work);\n\n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif (res->add_to_ml) {\n\t\t\tbus = res->bus;\n\t\t\ttarget = res->target;\n\t\t\tlun = res->lun;\n\t\t\tres->add_to_ml = 0;\n\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\t\tscsi_add_device(ioa_cfg->host, bus, target, lun);\n\t\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\t\tgoto restart;\n\t\t}\n\t}\n\n\tioa_cfg->scan_done = 1;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tkobject_uevent(&ioa_cfg->host->shost_dev.kobj, KOBJ_CHANGE);\n\tLEAVE;\n}\n\n \nstatic void ipr_worker_thread(struct work_struct *work)\n{\n\tunsigned long lock_flags;\n\tstruct ipr_dump *dump;\n\tstruct ipr_ioa_cfg *ioa_cfg =\n\t\tcontainer_of(work, struct ipr_ioa_cfg, work_q);\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (ioa_cfg->sdt_state == READ_DUMP) {\n\t\tdump = ioa_cfg->dump;\n\t\tif (!dump) {\n\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\t\treturn;\n\t\t}\n\t\tkref_get(&dump->kref);\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tipr_get_ioa_dump(ioa_cfg, dump);\n\t\tkref_put(&dump->kref, ipr_release_dump);\n\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\tif (ioa_cfg->sdt_state == DUMP_OBTAINED && !ioa_cfg->dump_timeout)\n\t\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tif (ioa_cfg->scsi_unblock) {\n\t\tioa_cfg->scsi_unblock = 0;\n\t\tioa_cfg->scsi_blocked = 0;\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tscsi_unblock_requests(ioa_cfg->host);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\tif (ioa_cfg->scsi_blocked)\n\t\t\tscsi_block_requests(ioa_cfg->host);\n\t}\n\n\tif (!ioa_cfg->scan_enabled) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tschedule_work(&ioa_cfg->scsi_add_work_q);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n}\n\n#ifdef CONFIG_SCSI_IPR_TRACE\n \nstatic ssize_t ipr_read_trace(struct file *filp, struct kobject *kobj,\n\t\t\t      struct bin_attribute *bin_attr,\n\t\t\t      char *buf, loff_t off, size_t count)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tssize_t ret;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tret = memory_read_from_buffer(buf, count, &off, ioa_cfg->trace,\n\t\t\t\tIPR_TRACE_SIZE);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\treturn ret;\n}\n\nstatic struct bin_attribute ipr_trace_attr = {\n\t.attr =\t{\n\t\t.name = \"trace\",\n\t\t.mode = S_IRUGO,\n\t},\n\t.size = 0,\n\t.read = ipr_read_trace,\n};\n#endif\n\n \nstatic ssize_t ipr_show_fw_version(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\n\tunsigned long lock_flags = 0;\n\tint len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tlen = snprintf(buf, PAGE_SIZE, \"%02X%02X%02X%02X\\n\",\n\t\t       ucode_vpd->major_release, ucode_vpd->card_type,\n\t\t       ucode_vpd->minor_release[0],\n\t\t       ucode_vpd->minor_release[1]);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_fw_version_attr = {\n\t.attr = {\n\t\t.name =\t\t\"fw_version\",\n\t\t.mode =\t\tS_IRUGO,\n\t},\n\t.show = ipr_show_fw_version,\n};\n\n \nstatic ssize_t ipr_show_log_level(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tint len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\", ioa_cfg->log_level);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\n \nstatic ssize_t ipr_store_log_level(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tioa_cfg->log_level = simple_strtoul(buf, NULL, 10);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn strlen(buf);\n}\n\nstatic struct device_attribute ipr_log_level_attr = {\n\t.attr = {\n\t\t.name =\t\t\"log_level\",\n\t\t.mode =\t\tS_IRUGO | S_IWUSR,\n\t},\n\t.show = ipr_show_log_level,\n\t.store = ipr_store_log_level\n};\n\n \nstatic ssize_t ipr_store_diagnostics(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tint rc = count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\twhile (ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t}\n\n\tioa_cfg->errors_logged = 0;\n\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\n\n\tif (ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\n\t\t \n\t\tmsleep(1000);\n\t} else {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn -EIO;\n\t}\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ioa_cfg->in_reset_reload || ioa_cfg->errors_logged)\n\t\trc = -EIO;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\treturn rc;\n}\n\nstatic struct device_attribute ipr_diagnostics_attr = {\n\t.attr = {\n\t\t.name =\t\t\"run_diagnostics\",\n\t\t.mode =\t\tS_IWUSR,\n\t},\n\t.store = ipr_store_diagnostics\n};\n\n \nstatic ssize_t ipr_show_adapter_state(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tint len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"offline\\n\");\n\telse\n\t\tlen = snprintf(buf, PAGE_SIZE, \"online\\n\");\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\n \nstatic ssize_t ipr_store_adapter_state(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags;\n\tint result = count, i;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead &&\n\t    !strncmp(buf, \"online\", 6)) {\n\t\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\t\tioa_cfg->hrrq[i].ioa_is_dead = 0;\n\t\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t\t}\n\t\twmb();\n\t\tioa_cfg->reset_retries = 0;\n\t\tioa_cfg->in_ioa_bringdown = 0;\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t}\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\n\treturn result;\n}\n\nstatic struct device_attribute ipr_ioa_state_attr = {\n\t.attr = {\n\t\t.name =\t\t\"online_state\",\n\t\t.mode =\t\tS_IRUGO | S_IWUSR,\n\t},\n\t.show = ipr_show_adapter_state,\n\t.store = ipr_store_adapter_state\n};\n\n \nstatic ssize_t ipr_store_reset_adapter(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags;\n\tint result = count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (!ioa_cfg->in_reset_reload)\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\n\treturn result;\n}\n\nstatic struct device_attribute ipr_ioa_reset_attr = {\n\t.attr = {\n\t\t.name =\t\t\"reset_host\",\n\t\t.mode =\t\tS_IWUSR,\n\t},\n\t.store = ipr_store_reset_adapter\n};\n\nstatic int ipr_iopoll(struct irq_poll *iop, int budget);\n  \nstatic ssize_t ipr_show_iopoll_weight(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tint len;\n\n\tspin_lock_irqsave(shost->host_lock, lock_flags);\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\", ioa_cfg->iopoll_weight);\n\tspin_unlock_irqrestore(shost->host_lock, lock_flags);\n\n\treturn len;\n}\n\n \nstatic ssize_t ipr_store_iopoll_weight(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long user_iopoll_weight;\n\tunsigned long lock_flags = 0;\n\tint i;\n\n\tif (!ioa_cfg->sis64) {\n\t\tdev_info(&ioa_cfg->pdev->dev, \"irq_poll not supported on this adapter\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (kstrtoul(buf, 10, &user_iopoll_weight))\n\t\treturn -EINVAL;\n\n\tif (user_iopoll_weight > 256) {\n\t\tdev_info(&ioa_cfg->pdev->dev, \"Invalid irq_poll weight. It must be less than 256\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (user_iopoll_weight == ioa_cfg->iopoll_weight) {\n\t\tdev_info(&ioa_cfg->pdev->dev, \"Current irq_poll weight has the same weight\\n\");\n\t\treturn strlen(buf);\n\t}\n\n\tif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\n\t\tfor (i = 1; i < ioa_cfg->hrrq_num; i++)\n\t\t\tirq_poll_disable(&ioa_cfg->hrrq[i].iopoll);\n\t}\n\n\tspin_lock_irqsave(shost->host_lock, lock_flags);\n\tioa_cfg->iopoll_weight = user_iopoll_weight;\n\tif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\n\t\tfor (i = 1; i < ioa_cfg->hrrq_num; i++) {\n\t\t\tirq_poll_init(&ioa_cfg->hrrq[i].iopoll,\n\t\t\t\t\tioa_cfg->iopoll_weight, ipr_iopoll);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(shost->host_lock, lock_flags);\n\n\treturn strlen(buf);\n}\n\nstatic struct device_attribute ipr_iopoll_weight_attr = {\n\t.attr = {\n\t\t.name =\t\t\"iopoll_weight\",\n\t\t.mode =\t\tS_IRUGO | S_IWUSR,\n\t},\n\t.show = ipr_show_iopoll_weight,\n\t.store = ipr_store_iopoll_weight\n};\n\n \nstatic struct ipr_sglist *ipr_alloc_ucode_buffer(int buf_len)\n{\n\tint sg_size, order;\n\tstruct ipr_sglist *sglist;\n\n\t \n\tsg_size = buf_len / (IPR_MAX_SGLIST - 1);\n\n\t \n\torder = get_order(sg_size);\n\n\t \n\tsglist = kzalloc(sizeof(struct ipr_sglist), GFP_KERNEL);\n\tif (sglist == NULL) {\n\t\tipr_trace;\n\t\treturn NULL;\n\t}\n\tsglist->order = order;\n\tsglist->scatterlist = sgl_alloc_order(buf_len, order, false, GFP_KERNEL,\n\t\t\t\t\t      &sglist->num_sg);\n\tif (!sglist->scatterlist) {\n\t\tkfree(sglist);\n\t\treturn NULL;\n\t}\n\n\treturn sglist;\n}\n\n \nstatic void ipr_free_ucode_buffer(struct ipr_sglist *sglist)\n{\n\tsgl_free_order(sglist->scatterlist, sglist->order);\n\tkfree(sglist);\n}\n\n \nstatic int ipr_copy_ucode_buffer(struct ipr_sglist *sglist,\n\t\t\t\t u8 *buffer, u32 len)\n{\n\tint bsize_elem, i, result = 0;\n\tstruct scatterlist *sg;\n\n\t \n\tbsize_elem = PAGE_SIZE * (1 << sglist->order);\n\n\tsg = sglist->scatterlist;\n\n\tfor (i = 0; i < (len / bsize_elem); i++, sg = sg_next(sg),\n\t\t\tbuffer += bsize_elem) {\n\t\tstruct page *page = sg_page(sg);\n\n\t\tmemcpy_to_page(page, 0, buffer, bsize_elem);\n\n\t\tsg->length = bsize_elem;\n\n\t\tif (result != 0) {\n\t\t\tipr_trace;\n\t\t\treturn result;\n\t\t}\n\t}\n\n\tif (len % bsize_elem) {\n\t\tstruct page *page = sg_page(sg);\n\n\t\tmemcpy_to_page(page, 0, buffer, len % bsize_elem);\n\n\t\tsg->length = len % bsize_elem;\n\t}\n\n\tsglist->buffer_len = len;\n\treturn result;\n}\n\n \nstatic void ipr_build_ucode_ioadl64(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t    struct ipr_sglist *sglist)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\n\tstruct scatterlist *scatterlist = sglist->scatterlist;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tipr_cmd->dma_use_sg = sglist->num_dma_sg;\n\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\tioarcb->data_transfer_length = cpu_to_be32(sglist->buffer_len);\n\n\tioarcb->ioadl_len =\n\t\tcpu_to_be32(sizeof(struct ipr_ioadl64_desc) * ipr_cmd->dma_use_sg);\n\tfor_each_sg(scatterlist, sg, ipr_cmd->dma_use_sg, i) {\n\t\tioadl64[i].flags = cpu_to_be32(IPR_IOADL_FLAGS_WRITE);\n\t\tioadl64[i].data_len = cpu_to_be32(sg_dma_len(sg));\n\t\tioadl64[i].address = cpu_to_be64(sg_dma_address(sg));\n\t}\n\n\tioadl64[i-1].flags |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\n}\n\n \nstatic void ipr_build_ucode_ioadl(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t  struct ipr_sglist *sglist)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\n\tstruct scatterlist *scatterlist = sglist->scatterlist;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tipr_cmd->dma_use_sg = sglist->num_dma_sg;\n\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\tioarcb->data_transfer_length = cpu_to_be32(sglist->buffer_len);\n\n\tioarcb->ioadl_len =\n\t\tcpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\n\n\tfor_each_sg(scatterlist, sg, ipr_cmd->dma_use_sg, i) {\n\t\tioadl[i].flags_and_data_len =\n\t\t\tcpu_to_be32(IPR_IOADL_FLAGS_WRITE | sg_dma_len(sg));\n\t\tioadl[i].address =\n\t\t\tcpu_to_be32(sg_dma_address(sg));\n\t}\n\n\tioadl[i-1].flags_and_data_len |=\n\t\tcpu_to_be32(IPR_IOADL_FLAGS_LAST);\n}\n\n \nstatic int ipr_update_ioa_ucode(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\tstruct ipr_sglist *sglist)\n{\n\tunsigned long lock_flags;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\twhile (ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t}\n\n\tif (ioa_cfg->ucode_sglist) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Microcode download already in progress\\n\");\n\t\treturn -EIO;\n\t}\n\n\tsglist->num_dma_sg = dma_map_sg(&ioa_cfg->pdev->dev,\n\t\t\t\t\tsglist->scatterlist, sglist->num_sg,\n\t\t\t\t\tDMA_TO_DEVICE);\n\n\tif (!sglist->num_dma_sg) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Failed to map microcode download buffer!\\n\");\n\t\treturn -EIO;\n\t}\n\n\tioa_cfg->ucode_sglist = sglist;\n\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tioa_cfg->ucode_sglist = NULL;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn 0;\n}\n\n \nstatic ssize_t ipr_store_update_fw(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tstruct ipr_ucode_image_header *image_hdr;\n\tconst struct firmware *fw_entry;\n\tstruct ipr_sglist *sglist;\n\tchar fname[100];\n\tchar *src;\n\tchar *endline;\n\tint result, dnld_size;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tsnprintf(fname, sizeof(fname), \"%s\", buf);\n\n\tendline = strchr(fname, '\\n');\n\tif (endline)\n\t\t*endline = '\\0';\n\n\tif (request_firmware(&fw_entry, fname, &ioa_cfg->pdev->dev)) {\n\t\tdev_err(&ioa_cfg->pdev->dev, \"Firmware file %s not found\\n\", fname);\n\t\treturn -EIO;\n\t}\n\n\timage_hdr = (struct ipr_ucode_image_header *)fw_entry->data;\n\n\tsrc = (u8 *)image_hdr + be32_to_cpu(image_hdr->header_length);\n\tdnld_size = fw_entry->size - be32_to_cpu(image_hdr->header_length);\n\tsglist = ipr_alloc_ucode_buffer(dnld_size);\n\n\tif (!sglist) {\n\t\tdev_err(&ioa_cfg->pdev->dev, \"Microcode buffer allocation failed\\n\");\n\t\trelease_firmware(fw_entry);\n\t\treturn -ENOMEM;\n\t}\n\n\tresult = ipr_copy_ucode_buffer(sglist, src, dnld_size);\n\n\tif (result) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Microcode buffer copy to DMA buffer failed\\n\");\n\t\tgoto out;\n\t}\n\n\tipr_info(\"Updating microcode, please be patient.  This may take up to 30 minutes.\\n\");\n\n\tresult = ipr_update_ioa_ucode(ioa_cfg, sglist);\n\n\tif (!result)\n\t\tresult = count;\nout:\n\tipr_free_ucode_buffer(sglist);\n\trelease_firmware(fw_entry);\n\treturn result;\n}\n\nstatic struct device_attribute ipr_update_fw_attr = {\n\t.attr = {\n\t\t.name =\t\t\"update_fw\",\n\t\t.mode =\t\tS_IWUSR,\n\t},\n\t.store = ipr_store_update_fw\n};\n\n \nstatic ssize_t ipr_show_fw_type(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tunsigned long lock_flags = 0;\n\tint len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\", ioa_cfg->sis64);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_ioa_fw_type_attr = {\n\t.attr = {\n\t\t.name =\t\t\"fw_type\",\n\t\t.mode =\t\tS_IRUGO,\n\t},\n\t.show = ipr_show_fw_type\n};\n\nstatic ssize_t ipr_read_async_err_log(struct file *filep, struct kobject *kobj,\n\t\t\t\tstruct bin_attribute *bin_attr, char *buf,\n\t\t\t\tloff_t off, size_t count)\n{\n\tstruct device *cdev = kobj_to_dev(kobj);\n\tstruct Scsi_Host *shost = class_to_shost(cdev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tstruct ipr_hostrcb *hostrcb;\n\tunsigned long lock_flags = 0;\n\tint ret;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\thostrcb = list_first_entry_or_null(&ioa_cfg->hostrcb_report_q,\n\t\t\t\t\tstruct ipr_hostrcb, queue);\n\tif (!hostrcb) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn 0;\n\t}\n\tret = memory_read_from_buffer(buf, count, &off, &hostrcb->hcam,\n\t\t\t\tsizeof(hostrcb->hcam));\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn ret;\n}\n\nstatic ssize_t ipr_next_async_err_log(struct file *filep, struct kobject *kobj,\n\t\t\t\tstruct bin_attribute *bin_attr, char *buf,\n\t\t\t\tloff_t off, size_t count)\n{\n\tstruct device *cdev = kobj_to_dev(kobj);\n\tstruct Scsi_Host *shost = class_to_shost(cdev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tstruct ipr_hostrcb *hostrcb;\n\tunsigned long lock_flags = 0;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\thostrcb = list_first_entry_or_null(&ioa_cfg->hostrcb_report_q,\n\t\t\t\t\tstruct ipr_hostrcb, queue);\n\tif (!hostrcb) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn count;\n\t}\n\n\t \n\tlist_move_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn count;\n}\n\nstatic struct bin_attribute ipr_ioa_async_err_log = {\n\t.attr = {\n\t\t.name =\t\t\"async_err_log\",\n\t\t.mode =\t\tS_IRUGO | S_IWUSR,\n\t},\n\t.size = 0,\n\t.read = ipr_read_async_err_log,\n\t.write = ipr_next_async_err_log\n};\n\nstatic struct attribute *ipr_ioa_attrs[] = {\n\t&ipr_fw_version_attr.attr,\n\t&ipr_log_level_attr.attr,\n\t&ipr_diagnostics_attr.attr,\n\t&ipr_ioa_state_attr.attr,\n\t&ipr_ioa_reset_attr.attr,\n\t&ipr_update_fw_attr.attr,\n\t&ipr_ioa_fw_type_attr.attr,\n\t&ipr_iopoll_weight_attr.attr,\n\tNULL,\n};\n\nATTRIBUTE_GROUPS(ipr_ioa);\n\n#ifdef CONFIG_SCSI_IPR_DUMP\n \nstatic ssize_t ipr_read_dump(struct file *filp, struct kobject *kobj,\n\t\t\t     struct bin_attribute *bin_attr,\n\t\t\t     char *buf, loff_t off, size_t count)\n{\n\tstruct device *cdev = kobj_to_dev(kobj);\n\tstruct Scsi_Host *shost = class_to_shost(cdev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tstruct ipr_dump *dump;\n\tunsigned long lock_flags = 0;\n\tchar *src;\n\tint len, sdt_end;\n\tsize_t rc = count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tdump = ioa_cfg->dump;\n\n\tif (ioa_cfg->sdt_state != DUMP_OBTAINED || !dump) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn 0;\n\t}\n\tkref_get(&dump->kref);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (off > dump->driver_dump.hdr.len) {\n\t\tkref_put(&dump->kref, ipr_release_dump);\n\t\treturn 0;\n\t}\n\n\tif (off + count > dump->driver_dump.hdr.len) {\n\t\tcount = dump->driver_dump.hdr.len - off;\n\t\trc = count;\n\t}\n\n\tif (count && off < sizeof(dump->driver_dump)) {\n\t\tif (off + count > sizeof(dump->driver_dump))\n\t\t\tlen = sizeof(dump->driver_dump) - off;\n\t\telse\n\t\t\tlen = count;\n\t\tsrc = (u8 *)&dump->driver_dump + off;\n\t\tmemcpy(buf, src, len);\n\t\tbuf += len;\n\t\toff += len;\n\t\tcount -= len;\n\t}\n\n\toff -= sizeof(dump->driver_dump);\n\n\tif (ioa_cfg->sis64)\n\t\tsdt_end = offsetof(struct ipr_ioa_dump, sdt.entry) +\n\t\t\t  (be32_to_cpu(dump->ioa_dump.sdt.hdr.num_entries_used) *\n\t\t\t   sizeof(struct ipr_sdt_entry));\n\telse\n\t\tsdt_end = offsetof(struct ipr_ioa_dump, sdt.entry) +\n\t\t\t  (IPR_FMT2_NUM_SDT_ENTRIES * sizeof(struct ipr_sdt_entry));\n\n\tif (count && off < sdt_end) {\n\t\tif (off + count > sdt_end)\n\t\t\tlen = sdt_end - off;\n\t\telse\n\t\t\tlen = count;\n\t\tsrc = (u8 *)&dump->ioa_dump + off;\n\t\tmemcpy(buf, src, len);\n\t\tbuf += len;\n\t\toff += len;\n\t\tcount -= len;\n\t}\n\n\toff -= sdt_end;\n\n\twhile (count) {\n\t\tif ((off & PAGE_MASK) != ((off + count) & PAGE_MASK))\n\t\t\tlen = PAGE_ALIGN(off) - off;\n\t\telse\n\t\t\tlen = count;\n\t\tsrc = (u8 *)dump->ioa_dump.ioa_data[(off & PAGE_MASK) >> PAGE_SHIFT];\n\t\tsrc += off & ~PAGE_MASK;\n\t\tmemcpy(buf, src, len);\n\t\tbuf += len;\n\t\toff += len;\n\t\tcount -= len;\n\t}\n\n\tkref_put(&dump->kref, ipr_release_dump);\n\treturn rc;\n}\n\n \nstatic int ipr_alloc_dump(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_dump *dump;\n\t__be32 **ioa_data;\n\tunsigned long lock_flags = 0;\n\n\tdump = kzalloc(sizeof(struct ipr_dump), GFP_KERNEL);\n\n\tif (!dump) {\n\t\tipr_err(\"Dump memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (ioa_cfg->sis64)\n\t\tioa_data = vmalloc(array_size(IPR_FMT3_MAX_NUM_DUMP_PAGES,\n\t\t\t\t\t      sizeof(__be32 *)));\n\telse\n\t\tioa_data = vmalloc(array_size(IPR_FMT2_MAX_NUM_DUMP_PAGES,\n\t\t\t\t\t      sizeof(__be32 *)));\n\n\tif (!ioa_data) {\n\t\tipr_err(\"Dump memory allocation failed\\n\");\n\t\tkfree(dump);\n\t\treturn -ENOMEM;\n\t}\n\n\tdump->ioa_dump.ioa_data = ioa_data;\n\n\tkref_init(&dump->kref);\n\tdump->ioa_cfg = ioa_cfg;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (INACTIVE != ioa_cfg->sdt_state) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\tvfree(dump->ioa_dump.ioa_data);\n\t\tkfree(dump);\n\t\treturn 0;\n\t}\n\n\tioa_cfg->dump = dump;\n\tioa_cfg->sdt_state = WAIT_FOR_DUMP;\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead && !ioa_cfg->dump_taken) {\n\t\tioa_cfg->dump_taken = 1;\n\t\tschedule_work(&ioa_cfg->work_q);\n\t}\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\treturn 0;\n}\n\n \nstatic int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_dump *dump;\n\tunsigned long lock_flags = 0;\n\n\tENTER;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tdump = ioa_cfg->dump;\n\tif (!dump) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn 0;\n\t}\n\n\tioa_cfg->dump = NULL;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tkref_put(&dump->kref, ipr_release_dump);\n\n\tLEAVE;\n\treturn 0;\n}\n\n \nstatic ssize_t ipr_write_dump(struct file *filp, struct kobject *kobj,\n\t\t\t      struct bin_attribute *bin_attr,\n\t\t\t      char *buf, loff_t off, size_t count)\n{\n\tstruct device *cdev = kobj_to_dev(kobj);\n\tstruct Scsi_Host *shost = class_to_shost(cdev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\tint rc;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (buf[0] == '1')\n\t\trc = ipr_alloc_dump(ioa_cfg);\n\telse if (buf[0] == '0')\n\t\trc = ipr_free_dump(ioa_cfg);\n\telse\n\t\treturn -EINVAL;\n\n\tif (rc)\n\t\treturn rc;\n\telse\n\t\treturn count;\n}\n\nstatic struct bin_attribute ipr_dump_attr = {\n\t.attr =\t{\n\t\t.name = \"dump\",\n\t\t.mode = S_IRUSR | S_IWUSR,\n\t},\n\t.size = 0,\n\t.read = ipr_read_dump,\n\t.write = ipr_write_dump\n};\n#else\nstatic int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg) { return 0; };\n#endif\n\n \nstatic int ipr_change_queue_depth(struct scsi_device *sdev, int qdepth)\n{\n\tscsi_change_queue_depth(sdev, qdepth);\n\treturn sdev->queue_depth;\n}\n\n \nstatic ssize_t ipr_show_adapter_handle(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len = -ENXIO;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\tif (res)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"%08X\\n\", res->res_handle);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_adapter_handle_attr = {\n\t.attr = {\n\t\t.name = \t\"adapter_handle\",\n\t\t.mode =\t\tS_IRUSR,\n\t},\n\t.show = ipr_show_adapter_handle\n};\n\n \nstatic ssize_t ipr_show_resource_path(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len = -ENXIO;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\tif (res && ioa_cfg->sis64)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t       __ipr_format_res_path(res->res_path, buffer,\n\t\t\t\t\t\t     sizeof(buffer)));\n\telse if (res)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"%d:%d:%d:%d\\n\", ioa_cfg->host->host_no,\n\t\t\t       res->bus, res->target, res->lun);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_resource_path_attr = {\n\t.attr = {\n\t\t.name = \t\"resource_path\",\n\t\t.mode =\t\tS_IRUGO,\n\t},\n\t.show = ipr_show_resource_path\n};\n\n \nstatic ssize_t ipr_show_device_id(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len = -ENXIO;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\tif (res && ioa_cfg->sis64)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"0x%llx\\n\", be64_to_cpu(res->dev_id));\n\telse if (res)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"0x%llx\\n\", res->lun_wwn);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_device_id_attr = {\n\t.attr = {\n\t\t.name =\t\t\"device_id\",\n\t\t.mode =\t\tS_IRUGO,\n\t},\n\t.show = ipr_show_device_id\n};\n\n \nstatic ssize_t ipr_show_resource_type(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len = -ENXIO;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\n\tif (res)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"%x\\n\", res->type);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_resource_type_attr = {\n\t.attr = {\n\t\t.name =\t\t\"resource_type\",\n\t\t.mode =\t\tS_IRUGO,\n\t},\n\t.show = ipr_show_resource_type\n};\n\n \nstatic ssize_t ipr_show_raw_mode(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\tif (res)\n\t\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\", res->raw_mode);\n\telse\n\t\tlen = -ENXIO;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\n \nstatic ssize_t ipr_store_raw_mode(struct device *dev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tssize_t len;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *)sdev->hostdata;\n\tif (res) {\n\t\tif (ipr_is_af_dasd_device(res)) {\n\t\t\tres->raw_mode = simple_strtoul(buf, NULL, 10);\n\t\t\tlen = strlen(buf);\n\t\t\tif (res->sdev)\n\t\t\t\tsdev_printk(KERN_INFO, res->sdev, \"raw mode is %s\\n\",\n\t\t\t\t\tres->raw_mode ? \"enabled\" : \"disabled\");\n\t\t} else\n\t\t\tlen = -EINVAL;\n\t} else\n\t\tlen = -ENXIO;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn len;\n}\n\nstatic struct device_attribute ipr_raw_mode_attr = {\n\t.attr = {\n\t\t.name =\t\t\"raw_mode\",\n\t\t.mode =\t\tS_IRUGO | S_IWUSR,\n\t},\n\t.show = ipr_show_raw_mode,\n\t.store = ipr_store_raw_mode\n};\n\nstatic struct attribute *ipr_dev_attrs[] = {\n\t&ipr_adapter_handle_attr.attr,\n\t&ipr_resource_path_attr.attr,\n\t&ipr_device_id_attr.attr,\n\t&ipr_resource_type_attr.attr,\n\t&ipr_raw_mode_attr.attr,\n\tNULL,\n};\n\nATTRIBUTE_GROUPS(ipr_dev);\n\n \nstatic int ipr_biosparam(struct scsi_device *sdev,\n\t\t\t struct block_device *block_device,\n\t\t\t sector_t capacity, int *parm)\n{\n\tint heads, sectors;\n\tsector_t cylinders;\n\n\theads = 128;\n\tsectors = 32;\n\n\tcylinders = capacity;\n\tsector_div(cylinders, (128 * 32));\n\n\t \n\tparm[0] = heads;\n\tparm[1] = sectors;\n\tparm[2] = cylinders;\n\n\treturn 0;\n}\n\n \nstatic struct ipr_resource_entry *ipr_find_starget(struct scsi_target *starget)\n{\n\tstruct Scsi_Host *shost = dev_to_shost(&starget->dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\n\tstruct ipr_resource_entry *res;\n\n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif ((res->bus == starget->channel) &&\n\t\t    (res->target == starget->id)) {\n\t\t\treturn res;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void ipr_target_destroy(struct scsi_target *starget)\n{\n\tstruct Scsi_Host *shost = dev_to_shost(&starget->dev);\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\n\n\tif (ioa_cfg->sis64) {\n\t\tif (!ipr_find_starget(starget)) {\n\t\t\tif (starget->channel == IPR_ARRAY_VIRTUAL_BUS)\n\t\t\t\tclear_bit(starget->id, ioa_cfg->array_ids);\n\t\t\telse if (starget->channel == IPR_VSET_VIRTUAL_BUS)\n\t\t\t\tclear_bit(starget->id, ioa_cfg->vset_ids);\n\t\t\telse if (starget->channel == 0)\n\t\t\t\tclear_bit(starget->id, ioa_cfg->target_ids);\n\t\t}\n\t}\n}\n\n \nstatic struct ipr_resource_entry *ipr_find_sdev(struct scsi_device *sdev)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif ((res->bus == sdev->channel) &&\n\t\t    (res->target == sdev->id) &&\n\t\t    (res->lun == sdev->lun))\n\t\t\treturn res;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void ipr_slave_destroy(struct scsi_device *sdev)\n{\n\tstruct ipr_resource_entry *res;\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tunsigned long lock_flags = 0;\n\n\tioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = (struct ipr_resource_entry *) sdev->hostdata;\n\tif (res) {\n\t\tsdev->hostdata = NULL;\n\t\tres->sdev = NULL;\n\t}\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n}\n\n \nstatic int ipr_slave_configure(struct scsi_device *sdev)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags = 0;\n\tchar buffer[IPR_MAX_RES_PATH_LENGTH];\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tres = sdev->hostdata;\n\tif (res) {\n\t\tif (ipr_is_af_dasd_device(res))\n\t\t\tsdev->type = TYPE_RAID;\n\t\tif (ipr_is_af_dasd_device(res) || ipr_is_ioa_resource(res)) {\n\t\t\tsdev->scsi_level = 4;\n\t\t\tsdev->no_uld_attach = 1;\n\t\t}\n\t\tif (ipr_is_vset_device(res)) {\n\t\t\tsdev->scsi_level = SCSI_SPC_3;\n\t\t\tsdev->no_report_opcodes = 1;\n\t\t\tblk_queue_rq_timeout(sdev->request_queue,\n\t\t\t\t\t     IPR_VSET_RW_TIMEOUT);\n\t\t\tblk_queue_max_hw_sectors(sdev->request_queue, IPR_VSET_MAX_SECTORS);\n\t\t}\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\t\tif (ioa_cfg->sis64)\n\t\t\tsdev_printk(KERN_INFO, sdev, \"Resource path: %s\\n\",\n\t\t\t\t    ipr_format_res_path(ioa_cfg,\n\t\t\t\tres->res_path, buffer, sizeof(buffer)));\n\t\treturn 0;\n\t}\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn 0;\n}\n\n \nstatic int ipr_slave_alloc(struct scsi_device *sdev)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\n\tstruct ipr_resource_entry *res;\n\tunsigned long lock_flags;\n\tint rc = -ENXIO;\n\n\tsdev->hostdata = NULL;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tres = ipr_find_sdev(sdev);\n\tif (res) {\n\t\tres->sdev = sdev;\n\t\tres->add_to_ml = 0;\n\t\tres->in_erp = 0;\n\t\tsdev->hostdata = res;\n\t\tif (!ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\trc = 0;\n\t\tif (ipr_is_gata(res)) {\n\t\t\tsdev_printk(KERN_ERR, sdev, \"SATA devices are no longer \"\n\t\t\t\t\"supported by this driver. Skipping device.\\n\");\n\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\t\treturn -ENXIO;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\treturn rc;\n}\n\n \nstatic int ipr_match_lun(struct ipr_cmnd *ipr_cmd, void *device)\n{\n\tif (ipr_cmd->scsi_cmd && ipr_cmd->scsi_cmd->device == device)\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic bool ipr_cmnd_is_free(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_cmnd *loop_cmd;\n\n\tlist_for_each_entry(loop_cmd, &ipr_cmd->hrrq->hrrq_free_q, queue) {\n\t\tif (loop_cmd == ipr_cmd)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int ipr_wait_for_ops(struct ipr_ioa_cfg *ioa_cfg, void *device,\n\t\t\t    int (*match)(struct ipr_cmnd *, void *))\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tint wait, i;\n\tunsigned long flags;\n\tstruct ipr_hrr_queue *hrrq;\n\tsigned long timeout = IPR_ABORT_TASK_TIMEOUT;\n\tDECLARE_COMPLETION_ONSTACK(comp);\n\n\tENTER;\n\tdo {\n\t\twait = 0;\n\n\t\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\t\tspin_lock_irqsave(hrrq->lock, flags);\n\t\t\tfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\n\t\t\t\tipr_cmd = ioa_cfg->ipr_cmnd_list[i];\n\t\t\t\tif (!ipr_cmnd_is_free(ipr_cmd)) {\n\t\t\t\t\tif (match(ipr_cmd, device)) {\n\t\t\t\t\t\tipr_cmd->eh_comp = &comp;\n\t\t\t\t\t\twait++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(hrrq->lock, flags);\n\t\t}\n\n\t\tif (wait) {\n\t\t\ttimeout = wait_for_completion_timeout(&comp, timeout);\n\n\t\t\tif (!timeout) {\n\t\t\t\twait = 0;\n\n\t\t\t\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\t\t\t\tspin_lock_irqsave(hrrq->lock, flags);\n\t\t\t\t\tfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\n\t\t\t\t\t\tipr_cmd = ioa_cfg->ipr_cmnd_list[i];\n\t\t\t\t\t\tif (!ipr_cmnd_is_free(ipr_cmd)) {\n\t\t\t\t\t\t\tif (match(ipr_cmd, device)) {\n\t\t\t\t\t\t\t\tipr_cmd->eh_comp = NULL;\n\t\t\t\t\t\t\t\twait++;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tspin_unlock_irqrestore(hrrq->lock, flags);\n\t\t\t\t}\n\n\t\t\t\tif (wait)\n\t\t\t\t\tdev_err(&ioa_cfg->pdev->dev, \"Timed out waiting for aborted commands\\n\");\n\t\t\t\tLEAVE;\n\t\t\t\treturn wait ? FAILED : SUCCESS;\n\t\t\t}\n\t\t}\n\t} while (wait);\n\n\tLEAVE;\n\treturn SUCCESS;\n}\n\nstatic int ipr_eh_host_reset(struct scsi_cmnd *cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tunsigned long lock_flags = 0;\n\tint rc = SUCCESS;\n\n\tENTER;\n\tioa_cfg = (struct ipr_ioa_cfg *) cmd->device->host->hostdata;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (!ioa_cfg->in_reset_reload && !ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_ABBREV);\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Adapter being reset as a result of error recovery.\\n\");\n\n\t\tif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\n\t\t\tioa_cfg->sdt_state = GET_DUMP;\n\t}\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\t \n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\n\t\tipr_trace;\n\t\trc = FAILED;\n\t}\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic int ipr_device_reset(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t    struct ipr_resource_entry *res)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioarcb *ioarcb;\n\tstruct ipr_cmd_pkt *cmd_pkt;\n\tu32 ioasc;\n\n\tENTER;\n\tipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\tioarcb = &ipr_cmd->ioarcb;\n\tcmd_pkt = &ioarcb->cmd_pkt;\n\n\tif (ipr_cmd->ioa_cfg->sis64)\n\t\tioarcb->add_cmd_parms_offset = cpu_to_be16(sizeof(*ioarcb));\n\n\tioarcb->res_handle = res->res_handle;\n\tcmd_pkt->request_type = IPR_RQTYPE_IOACMD;\n\tcmd_pkt->cdb[0] = IPR_RESET_DEVICE;\n\n\tipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\n\tioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\n\tLEAVE;\n\treturn IPR_IOASC_SENSE_KEY(ioasc) ? -EIO : 0;\n}\n\n \nstatic int __ipr_eh_dev_reset(struct scsi_cmnd *scsi_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\tint rc = 0;\n\n\tENTER;\n\tioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;\n\tres = scsi_cmd->device->hostdata;\n\n\t \n\tif (ioa_cfg->in_reset_reload)\n\t\treturn FAILED;\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\n\t\treturn FAILED;\n\n\tres->resetting_device = 1;\n\tscmd_printk(KERN_ERR, scsi_cmd, \"Resetting device\\n\");\n\n\trc = ipr_device_reset(ioa_cfg, res);\n\tres->resetting_device = 0;\n\tres->reset_occurred = 1;\n\n\tLEAVE;\n\treturn rc ? FAILED : SUCCESS;\n}\n\nstatic int ipr_eh_dev_reset(struct scsi_cmnd *cmd)\n{\n\tint rc;\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\n\tioa_cfg = (struct ipr_ioa_cfg *) cmd->device->host->hostdata;\n\tres = cmd->device->hostdata;\n\n\tif (!res)\n\t\treturn FAILED;\n\n\tspin_lock_irq(cmd->device->host->host_lock);\n\trc = __ipr_eh_dev_reset(cmd);\n\tspin_unlock_irq(cmd->device->host->host_lock);\n\n\tif (rc == SUCCESS)\n\t\trc = ipr_wait_for_ops(ioa_cfg, cmd->device, ipr_match_lun);\n\n\treturn rc;\n}\n\n \nstatic void ipr_bus_reset_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\n\tENTER;\n\tif (!ioa_cfg->sis64)\n\t\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\t\tif (res->res_handle == ipr_cmd->ioarcb.res_handle) {\n\t\t\t\tscsi_report_bus_reset(ioa_cfg->host, res->bus);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t \n\tif (ipr_cmd->sibling->sibling)\n\t\tipr_cmd->sibling->sibling = NULL;\n\telse\n\t\tipr_cmd->sibling->done(ipr_cmd->sibling);\n\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\tLEAVE;\n}\n\n \nstatic void ipr_abort_timeout(struct timer_list *t)\n{\n\tstruct ipr_cmnd *ipr_cmd = from_timer(ipr_cmd, t, timer);\n\tstruct ipr_cmnd *reset_cmd;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_cmd_pkt *cmd_pkt;\n\tunsigned long lock_flags = 0;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ipr_cmd->completion.done || ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\treturn;\n\t}\n\n\tsdev_printk(KERN_ERR, ipr_cmd->u.sdev, \"Abort timed out. Resetting bus.\\n\");\n\treset_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\tipr_cmd->sibling = reset_cmd;\n\treset_cmd->sibling = ipr_cmd;\n\treset_cmd->ioarcb.res_handle = ipr_cmd->ioarcb.res_handle;\n\tcmd_pkt = &reset_cmd->ioarcb.cmd_pkt;\n\tcmd_pkt->request_type = IPR_RQTYPE_IOACMD;\n\tcmd_pkt->cdb[0] = IPR_RESET_DEVICE;\n\tcmd_pkt->cdb[2] = IPR_RESET_TYPE_SELECT | IPR_BUS_RESET;\n\n\tipr_do_req(reset_cmd, ipr_bus_reset_done, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n}\n\n \nstatic int ipr_cancel_op(struct scsi_cmnd *scsi_cmd)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\tstruct ipr_cmd_pkt *cmd_pkt;\n\tu32 ioasc;\n\tint i, op_found = 0;\n\tstruct ipr_hrr_queue *hrrq;\n\n\tENTER;\n\tioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;\n\tres = scsi_cmd->device->hostdata;\n\n\t \n\tif (ioa_cfg->in_reset_reload ||\n\t    ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\n\t\treturn FAILED;\n\tif (!res)\n\t\treturn FAILED;\n\n\t \n\treadl(ioa_cfg->regs.sense_interrupt_reg);\n\n\tif (!ipr_is_gscsi(res))\n\t\treturn FAILED;\n\n\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\tspin_lock(&hrrq->_lock);\n\t\tfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\n\t\t\tif (ioa_cfg->ipr_cmnd_list[i]->scsi_cmd == scsi_cmd) {\n\t\t\t\tif (!ipr_cmnd_is_free(ioa_cfg->ipr_cmnd_list[i])) {\n\t\t\t\t\top_found = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&hrrq->_lock);\n\t}\n\n\tif (!op_found)\n\t\treturn SUCCESS;\n\n\tipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\tipr_cmd->ioarcb.res_handle = res->res_handle;\n\tcmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\n\tcmd_pkt->request_type = IPR_RQTYPE_IOACMD;\n\tcmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;\n\tipr_cmd->u.sdev = scsi_cmd->device;\n\n\tscmd_printk(KERN_ERR, scsi_cmd, \"Aborting command: %02X\\n\",\n\t\t    scsi_cmd->cmnd[0]);\n\tipr_send_blocking_cmd(ipr_cmd, ipr_abort_timeout, IPR_CANCEL_ALL_TIMEOUT);\n\tioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\t \n\tif (ioasc == IPR_IOASC_BUS_WAS_RESET || ioasc == IPR_IOASC_SYNC_REQUIRED) {\n\t\tioasc = 0;\n\t\tipr_trace;\n\t}\n\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\tif (!ipr_is_naca_model(res))\n\t\tres->needs_sync_complete = 1;\n\n\tLEAVE;\n\treturn IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS;\n}\n\n \nstatic int ipr_scan_finished(struct Scsi_Host *shost, unsigned long elapsed_time)\n{\n\tunsigned long lock_flags;\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\n\tint rc = 0;\n\n\tspin_lock_irqsave(shost->host_lock, lock_flags);\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead || ioa_cfg->scan_done)\n\t\trc = 1;\n\tif ((elapsed_time/HZ) > (ioa_cfg->transop_timeout * 2))\n\t\trc = 1;\n\tspin_unlock_irqrestore(shost->host_lock, lock_flags);\n\treturn rc;\n}\n\n \nstatic int ipr_eh_abort(struct scsi_cmnd *scsi_cmd)\n{\n\tunsigned long flags;\n\tint rc;\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\n\tENTER;\n\n\tioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;\n\n\tspin_lock_irqsave(scsi_cmd->device->host->host_lock, flags);\n\trc = ipr_cancel_op(scsi_cmd);\n\tspin_unlock_irqrestore(scsi_cmd->device->host->host_lock, flags);\n\n\tif (rc == SUCCESS)\n\t\trc = ipr_wait_for_ops(ioa_cfg, scsi_cmd->device, ipr_match_lun);\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic irqreturn_t ipr_handle_other_interrupt(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t      u32 int_reg)\n{\n\tirqreturn_t rc = IRQ_HANDLED;\n\tu32 int_mask_reg;\n\n\tint_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg32);\n\tint_reg &= ~int_mask_reg;\n\n\t \n\tif ((int_reg & IPR_PCII_OPER_INTERRUPTS) == 0) {\n\t\tif (ioa_cfg->sis64) {\n\t\t\tint_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;\n\t\t\tif (int_reg & IPR_PCII_IPL_STAGE_CHANGE) {\n\n\t\t\t\t \n\t\t\t\twritel(IPR_PCII_IPL_STAGE_CHANGE, ioa_cfg->regs.clr_interrupt_reg);\n\t\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;\n\t\t\t\tlist_del(&ioa_cfg->reset_cmd->queue);\n\t\t\t\tdel_timer(&ioa_cfg->reset_cmd->timer);\n\t\t\t\tipr_reset_ioa_job(ioa_cfg->reset_cmd);\n\t\t\t\treturn IRQ_HANDLED;\n\t\t\t}\n\t\t}\n\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\n\t\t \n\t\twritel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.set_interrupt_mask_reg);\n\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\n\n\t\tlist_del(&ioa_cfg->reset_cmd->queue);\n\t\tdel_timer(&ioa_cfg->reset_cmd->timer);\n\t\tipr_reset_ioa_job(ioa_cfg->reset_cmd);\n\t} else if ((int_reg & IPR_PCII_HRRQ_UPDATED) == int_reg) {\n\t\tif (ioa_cfg->clear_isr) {\n\t\t\tif (ipr_debug && printk_ratelimit())\n\t\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\t\"Spurious interrupt detected. 0x%08X\\n\", int_reg);\n\t\t\twritel(IPR_PCII_HRRQ_UPDATED, ioa_cfg->regs.clr_interrupt_reg32);\n\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\t\t\treturn IRQ_NONE;\n\t\t}\n\t} else {\n\t\tif (int_reg & IPR_PCII_IOA_UNIT_CHECKED)\n\t\t\tioa_cfg->ioa_unit_checked = 1;\n\t\telse if (int_reg & IPR_PCII_NO_HOST_RRQ)\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"No Host RRQ. 0x%08X\\n\", int_reg);\n\t\telse\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Permanent IOA failure. 0x%08X\\n\", int_reg);\n\n\t\tif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\n\t\t\tioa_cfg->sdt_state = GET_DUMP;\n\n\t\tipr_mask_and_clear_interrupts(ioa_cfg, ~0);\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t}\n\n\treturn rc;\n}\n\n \nstatic void ipr_isr_eh(struct ipr_ioa_cfg *ioa_cfg, char *msg, u16 number)\n{\n\tioa_cfg->errors_logged++;\n\tdev_err(&ioa_cfg->pdev->dev, \"%s %d\\n\", msg, number);\n\n\tif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\n\t\tioa_cfg->sdt_state = GET_DUMP;\n\n\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n}\n\nstatic int ipr_process_hrrq(struct ipr_hrr_queue *hrr_queue, int budget,\n\t\t\t\t\t\tstruct list_head *doneq)\n{\n\tu32 ioasc;\n\tu16 cmd_index;\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioa_cfg *ioa_cfg = hrr_queue->ioa_cfg;\n\tint num_hrrq = 0;\n\n\t \n\tif (!hrr_queue->allow_interrupts)\n\t\treturn 0;\n\n\twhile ((be32_to_cpu(*hrr_queue->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\n\t       hrr_queue->toggle_bit) {\n\n\t\tcmd_index = (be32_to_cpu(*hrr_queue->hrrq_curr) &\n\t\t\t     IPR_HRRQ_REQ_RESP_HANDLE_MASK) >>\n\t\t\t     IPR_HRRQ_REQ_RESP_HANDLE_SHIFT;\n\n\t\tif (unlikely(cmd_index > hrr_queue->max_cmd_id ||\n\t\t\t     cmd_index < hrr_queue->min_cmd_id)) {\n\t\t\tipr_isr_eh(ioa_cfg,\n\t\t\t\t\"Invalid response handle from IOA: \",\n\t\t\t\tcmd_index);\n\t\t\tbreak;\n\t\t}\n\n\t\tipr_cmd = ioa_cfg->ipr_cmnd_list[cmd_index];\n\t\tioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\t\tipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, ioasc);\n\n\t\tlist_move_tail(&ipr_cmd->queue, doneq);\n\n\t\tif (hrr_queue->hrrq_curr < hrr_queue->hrrq_end) {\n\t\t\thrr_queue->hrrq_curr++;\n\t\t} else {\n\t\t\thrr_queue->hrrq_curr = hrr_queue->hrrq_start;\n\t\t\thrr_queue->toggle_bit ^= 1u;\n\t\t}\n\t\tnum_hrrq++;\n\t\tif (budget > 0 && num_hrrq >= budget)\n\t\t\tbreak;\n\t}\n\n\treturn num_hrrq;\n}\n\nstatic int ipr_iopoll(struct irq_poll *iop, int budget)\n{\n\tstruct ipr_hrr_queue *hrrq;\n\tstruct ipr_cmnd *ipr_cmd, *temp;\n\tunsigned long hrrq_flags;\n\tint completed_ops;\n\tLIST_HEAD(doneq);\n\n\thrrq = container_of(iop, struct ipr_hrr_queue, iopoll);\n\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\tcompleted_ops = ipr_process_hrrq(hrrq, budget, &doneq);\n\n\tif (completed_ops < budget)\n\t\tirq_poll_complete(iop);\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\n\tlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\n\t\tlist_del(&ipr_cmd->queue);\n\t\tdel_timer(&ipr_cmd->timer);\n\t\tipr_cmd->fast_done(ipr_cmd);\n\t}\n\n\treturn completed_ops;\n}\n\n \nstatic irqreturn_t ipr_isr(int irq, void *devp)\n{\n\tstruct ipr_hrr_queue *hrrq = (struct ipr_hrr_queue *)devp;\n\tstruct ipr_ioa_cfg *ioa_cfg = hrrq->ioa_cfg;\n\tunsigned long hrrq_flags = 0;\n\tu32 int_reg = 0;\n\tint num_hrrq = 0;\n\tint irq_none = 0;\n\tstruct ipr_cmnd *ipr_cmd, *temp;\n\tirqreturn_t rc = IRQ_NONE;\n\tLIST_HEAD(doneq);\n\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\t \n\tif (!hrrq->allow_interrupts) {\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\treturn IRQ_NONE;\n\t}\n\n\twhile (1) {\n\t\tif (ipr_process_hrrq(hrrq, -1, &doneq)) {\n\t\t\trc =  IRQ_HANDLED;\n\n\t\t\tif (!ioa_cfg->clear_isr)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tnum_hrrq = 0;\n\t\t\tdo {\n\t\t\t\twritel(IPR_PCII_HRRQ_UPDATED,\n\t\t\t\t     ioa_cfg->regs.clr_interrupt_reg32);\n\t\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\t\t\t} while (int_reg & IPR_PCII_HRRQ_UPDATED &&\n\t\t\t\tnum_hrrq++ < IPR_MAX_HRRQ_RETRIES);\n\n\t\t} else if (rc == IRQ_NONE && irq_none == 0) {\n\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\t\t\tirq_none++;\n\t\t} else if (num_hrrq == IPR_MAX_HRRQ_RETRIES &&\n\t\t\t   int_reg & IPR_PCII_HRRQ_UPDATED) {\n\t\t\tipr_isr_eh(ioa_cfg,\n\t\t\t\t\"Error clearing HRRQ: \", num_hrrq);\n\t\t\trc = IRQ_HANDLED;\n\t\t\tbreak;\n\t\t} else\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(rc == IRQ_NONE))\n\t\trc = ipr_handle_other_interrupt(ioa_cfg, int_reg);\n\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\tlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\n\t\tlist_del(&ipr_cmd->queue);\n\t\tdel_timer(&ipr_cmd->timer);\n\t\tipr_cmd->fast_done(ipr_cmd);\n\t}\n\treturn rc;\n}\n\n \nstatic irqreturn_t ipr_isr_mhrrq(int irq, void *devp)\n{\n\tstruct ipr_hrr_queue *hrrq = (struct ipr_hrr_queue *)devp;\n\tstruct ipr_ioa_cfg *ioa_cfg = hrrq->ioa_cfg;\n\tunsigned long hrrq_flags = 0;\n\tstruct ipr_cmnd *ipr_cmd, *temp;\n\tirqreturn_t rc = IRQ_NONE;\n\tLIST_HEAD(doneq);\n\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\n\t \n\tif (!hrrq->allow_interrupts) {\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\n\t\tif ((be32_to_cpu(*hrrq->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\n\t\t       hrrq->toggle_bit) {\n\t\t\tirq_poll_sched(&hrrq->iopoll);\n\t\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\t\treturn IRQ_HANDLED;\n\t\t}\n\t} else {\n\t\tif ((be32_to_cpu(*hrrq->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\n\t\t\thrrq->toggle_bit)\n\n\t\t\tif (ipr_process_hrrq(hrrq, -1, &doneq))\n\t\t\t\trc =  IRQ_HANDLED;\n\t}\n\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\n\tlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\n\t\tlist_del(&ipr_cmd->queue);\n\t\tdel_timer(&ipr_cmd->timer);\n\t\tipr_cmd->fast_done(ipr_cmd);\n\t}\n\treturn rc;\n}\n\n \nstatic int ipr_build_ioadl64(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t     struct ipr_cmnd *ipr_cmd)\n{\n\tint i, nseg;\n\tstruct scatterlist *sg;\n\tu32 length;\n\tu32 ioadl_flags = 0;\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\n\n\tlength = scsi_bufflen(scsi_cmd);\n\tif (!length)\n\t\treturn 0;\n\n\tnseg = scsi_dma_map(scsi_cmd);\n\tif (nseg < 0) {\n\t\tif (printk_ratelimit())\n\t\t\tdev_err(&ioa_cfg->pdev->dev, \"scsi_dma_map failed!\\n\");\n\t\treturn -1;\n\t}\n\n\tipr_cmd->dma_use_sg = nseg;\n\n\tioarcb->data_transfer_length = cpu_to_be32(length);\n\tioarcb->ioadl_len =\n\t\tcpu_to_be32(sizeof(struct ipr_ioadl64_desc) * ipr_cmd->dma_use_sg);\n\n\tif (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {\n\t\tioadl_flags = IPR_IOADL_FLAGS_WRITE;\n\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\t} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE)\n\t\tioadl_flags = IPR_IOADL_FLAGS_READ;\n\n\tscsi_for_each_sg(scsi_cmd, sg, ipr_cmd->dma_use_sg, i) {\n\t\tioadl64[i].flags = cpu_to_be32(ioadl_flags);\n\t\tioadl64[i].data_len = cpu_to_be32(sg_dma_len(sg));\n\t\tioadl64[i].address = cpu_to_be64(sg_dma_address(sg));\n\t}\n\n\tioadl64[i-1].flags |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\n\treturn 0;\n}\n\n \nstatic int ipr_build_ioadl(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t   struct ipr_cmnd *ipr_cmd)\n{\n\tint i, nseg;\n\tstruct scatterlist *sg;\n\tu32 length;\n\tu32 ioadl_flags = 0;\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\n\n\tlength = scsi_bufflen(scsi_cmd);\n\tif (!length)\n\t\treturn 0;\n\n\tnseg = scsi_dma_map(scsi_cmd);\n\tif (nseg < 0) {\n\t\tdev_err(&ioa_cfg->pdev->dev, \"scsi_dma_map failed!\\n\");\n\t\treturn -1;\n\t}\n\n\tipr_cmd->dma_use_sg = nseg;\n\n\tif (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {\n\t\tioadl_flags = IPR_IOADL_FLAGS_WRITE;\n\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\t\tioarcb->data_transfer_length = cpu_to_be32(length);\n\t\tioarcb->ioadl_len =\n\t\t\tcpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\n\t} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {\n\t\tioadl_flags = IPR_IOADL_FLAGS_READ;\n\t\tioarcb->read_data_transfer_length = cpu_to_be32(length);\n\t\tioarcb->read_ioadl_len =\n\t\t\tcpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\n\t}\n\n\tif (ipr_cmd->dma_use_sg <= ARRAY_SIZE(ioarcb->u.add_data.u.ioadl)) {\n\t\tioadl = ioarcb->u.add_data.u.ioadl;\n\t\tioarcb->write_ioadl_addr = cpu_to_be32((ipr_cmd->dma_addr) +\n\t\t\t\t    offsetof(struct ipr_ioarcb, u.add_data));\n\t\tioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\n\t}\n\n\tscsi_for_each_sg(scsi_cmd, sg, ipr_cmd->dma_use_sg, i) {\n\t\tioadl[i].flags_and_data_len =\n\t\t\tcpu_to_be32(ioadl_flags | sg_dma_len(sg));\n\t\tioadl[i].address = cpu_to_be32(sg_dma_address(sg));\n\t}\n\n\tioadl[i-1].flags_and_data_len |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\n\treturn 0;\n}\n\n \nstatic void __ipr_erp_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tif (IPR_IOASC_SENSE_KEY(ioasc) > 0) {\n\t\tscsi_cmd->result |= (DID_ERROR << 16);\n\t\tscmd_printk(KERN_ERR, scsi_cmd,\n\t\t\t    \"Request Sense failed with IOASC: 0x%08X\\n\", ioasc);\n\t} else {\n\t\tmemcpy(scsi_cmd->sense_buffer, ipr_cmd->sense_buffer,\n\t\t       SCSI_SENSE_BUFFERSIZE);\n\t}\n\n\tif (res) {\n\t\tif (!ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\tres->in_erp = 0;\n\t}\n\tscsi_dma_unmap(ipr_cmd->scsi_cmd);\n\tscsi_done(scsi_cmd);\n\tif (ipr_cmd->eh_comp)\n\t\tcomplete(ipr_cmd->eh_comp);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n}\n\n \nstatic void ipr_erp_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\n\tunsigned long hrrq_flags;\n\n\tspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\n\t__ipr_erp_done(ipr_cmd);\n\tspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\n}\n\n \nstatic void ipr_reinit_ipr_cmnd_for_erp(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\n\tdma_addr_t dma_addr = ipr_cmd->dma_addr;\n\n\tmemset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));\n\tioarcb->data_transfer_length = 0;\n\tioarcb->read_data_transfer_length = 0;\n\tioarcb->ioadl_len = 0;\n\tioarcb->read_ioadl_len = 0;\n\tioasa->hdr.ioasc = 0;\n\tioasa->hdr.residual_data_len = 0;\n\n\tif (ipr_cmd->ioa_cfg->sis64)\n\t\tioarcb->u.sis64_addr_data.data_ioadl_addr =\n\t\t\tcpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\n\telse {\n\t\tioarcb->write_ioadl_addr =\n\t\t\tcpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\n\t\tioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\n\t}\n}\n\n \nstatic void __ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_cmd_pkt *cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tif (IPR_IOASC_SENSE_KEY(ioasc) > 0) {\n\t\t__ipr_erp_done(ipr_cmd);\n\t\treturn;\n\t}\n\n\tipr_reinit_ipr_cmnd_for_erp(ipr_cmd);\n\n\tcmd_pkt->request_type = IPR_RQTYPE_SCSICDB;\n\tcmd_pkt->cdb[0] = REQUEST_SENSE;\n\tcmd_pkt->cdb[4] = SCSI_SENSE_BUFFERSIZE;\n\tcmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_OVERRIDE;\n\tcmd_pkt->flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\n\tcmd_pkt->timeout = cpu_to_be16(IPR_REQUEST_SENSE_TIMEOUT / HZ);\n\n\tipr_init_ioadl(ipr_cmd, ipr_cmd->sense_buffer_dma,\n\t\t       SCSI_SENSE_BUFFERSIZE, IPR_IOADL_FLAGS_READ_LAST);\n\n\tipr_do_req(ipr_cmd, ipr_erp_done, ipr_timeout,\n\t\t   IPR_REQUEST_SENSE_TIMEOUT * 2);\n}\n\n \nstatic void ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\n\tunsigned long hrrq_flags;\n\n\tspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\n\t__ipr_erp_request_sense(ipr_cmd);\n\tspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\n}\n\n \nstatic void ipr_erp_cancel_all(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\n\tstruct ipr_cmd_pkt *cmd_pkt;\n\n\tres->in_erp = 1;\n\n\tipr_reinit_ipr_cmnd_for_erp(ipr_cmd);\n\n\tif (!scsi_cmd->device->simple_tags) {\n\t\t__ipr_erp_request_sense(ipr_cmd);\n\t\treturn;\n\t}\n\n\tcmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\n\tcmd_pkt->request_type = IPR_RQTYPE_IOACMD;\n\tcmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;\n\n\tipr_do_req(ipr_cmd, ipr_erp_request_sense, ipr_timeout,\n\t\t   IPR_CANCEL_ALL_TIMEOUT);\n}\n\n \nstatic void ipr_dump_ioasa(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t   struct ipr_cmnd *ipr_cmd, struct ipr_resource_entry *res)\n{\n\tint i;\n\tu16 data_len;\n\tu32 ioasc, fd_ioasc;\n\tstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\n\t__be32 *ioasa_data = (__be32 *)ioasa;\n\tint error_index;\n\n\tioasc = be32_to_cpu(ioasa->hdr.ioasc) & IPR_IOASC_IOASC_MASK;\n\tfd_ioasc = be32_to_cpu(ioasa->hdr.fd_ioasc) & IPR_IOASC_IOASC_MASK;\n\n\tif (0 == ioasc)\n\t\treturn;\n\n\tif (ioa_cfg->log_level < IPR_DEFAULT_LOG_LEVEL)\n\t\treturn;\n\n\tif (ioasc == IPR_IOASC_BUS_WAS_RESET && fd_ioasc)\n\t\terror_index = ipr_get_error(fd_ioasc);\n\telse\n\t\terror_index = ipr_get_error(ioasc);\n\n\tif (ioa_cfg->log_level < IPR_MAX_LOG_LEVEL) {\n\t\t \n\t\tif (ioasa->hdr.ilid != 0)\n\t\t\treturn;\n\n\t\tif (!ipr_is_gscsi(res))\n\t\t\treturn;\n\n\t\tif (ipr_error_table[error_index].log_ioasa == 0)\n\t\t\treturn;\n\t}\n\n\tipr_res_err(ioa_cfg, res, \"%s\\n\", ipr_error_table[error_index].error);\n\n\tdata_len = be16_to_cpu(ioasa->hdr.ret_stat_len);\n\tif (ioa_cfg->sis64 && sizeof(struct ipr_ioasa64) < data_len)\n\t\tdata_len = sizeof(struct ipr_ioasa64);\n\telse if (!ioa_cfg->sis64 && sizeof(struct ipr_ioasa) < data_len)\n\t\tdata_len = sizeof(struct ipr_ioasa);\n\n\tipr_err(\"IOASA Dump:\\n\");\n\n\tfor (i = 0; i < data_len / 4; i += 4) {\n\t\tipr_err(\"%08X: %08X %08X %08X %08X\\n\", i*4,\n\t\t\tbe32_to_cpu(ioasa_data[i]),\n\t\t\tbe32_to_cpu(ioasa_data[i+1]),\n\t\t\tbe32_to_cpu(ioasa_data[i+2]),\n\t\t\tbe32_to_cpu(ioasa_data[i+3]));\n\t}\n}\n\n \nstatic void ipr_gen_sense(struct ipr_cmnd *ipr_cmd)\n{\n\tu32 failing_lba;\n\tu8 *sense_buf = ipr_cmd->scsi_cmd->sense_buffer;\n\tstruct ipr_resource_entry *res = ipr_cmd->scsi_cmd->device->hostdata;\n\tstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\n\tu32 ioasc = be32_to_cpu(ioasa->hdr.ioasc);\n\n\tmemset(sense_buf, 0, SCSI_SENSE_BUFFERSIZE);\n\n\tif (ioasc >= IPR_FIRST_DRIVER_IOASC)\n\t\treturn;\n\n\tipr_cmd->scsi_cmd->result = SAM_STAT_CHECK_CONDITION;\n\n\tif (ipr_is_vset_device(res) &&\n\t    ioasc == IPR_IOASC_MED_DO_NOT_REALLOC &&\n\t    ioasa->u.vset.failing_lba_hi != 0) {\n\t\tsense_buf[0] = 0x72;\n\t\tsense_buf[1] = IPR_IOASC_SENSE_KEY(ioasc);\n\t\tsense_buf[2] = IPR_IOASC_SENSE_CODE(ioasc);\n\t\tsense_buf[3] = IPR_IOASC_SENSE_QUAL(ioasc);\n\n\t\tsense_buf[7] = 12;\n\t\tsense_buf[8] = 0;\n\t\tsense_buf[9] = 0x0A;\n\t\tsense_buf[10] = 0x80;\n\n\t\tfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_hi);\n\n\t\tsense_buf[12] = (failing_lba & 0xff000000) >> 24;\n\t\tsense_buf[13] = (failing_lba & 0x00ff0000) >> 16;\n\t\tsense_buf[14] = (failing_lba & 0x0000ff00) >> 8;\n\t\tsense_buf[15] = failing_lba & 0x000000ff;\n\n\t\tfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_lo);\n\n\t\tsense_buf[16] = (failing_lba & 0xff000000) >> 24;\n\t\tsense_buf[17] = (failing_lba & 0x00ff0000) >> 16;\n\t\tsense_buf[18] = (failing_lba & 0x0000ff00) >> 8;\n\t\tsense_buf[19] = failing_lba & 0x000000ff;\n\t} else {\n\t\tsense_buf[0] = 0x70;\n\t\tsense_buf[2] = IPR_IOASC_SENSE_KEY(ioasc);\n\t\tsense_buf[12] = IPR_IOASC_SENSE_CODE(ioasc);\n\t\tsense_buf[13] = IPR_IOASC_SENSE_QUAL(ioasc);\n\n\t\t \n\t\tif ((IPR_IOASC_SENSE_KEY(ioasc) == 0x05) &&\n\t\t    (be32_to_cpu(ioasa->hdr.ioasc_specific) & IPR_FIELD_POINTER_VALID)) {\n\t\t\tsense_buf[7] = 10;\t \n\n\t\t\t \n\t\t\tif (IPR_IOASC_SENSE_CODE(ioasc) == 0x24)\n\t\t\t\tsense_buf[15] = 0xC0;\n\t\t\telse\t \n\t\t\t\tsense_buf[15] = 0x80;\n\n\t\t\tsense_buf[16] =\n\t\t\t    ((IPR_FIELD_POINTER_MASK &\n\t\t\t      be32_to_cpu(ioasa->hdr.ioasc_specific)) >> 8) & 0xff;\n\t\t\tsense_buf[17] =\n\t\t\t    (IPR_FIELD_POINTER_MASK &\n\t\t\t     be32_to_cpu(ioasa->hdr.ioasc_specific)) & 0xff;\n\t\t} else {\n\t\t\tif (ioasc == IPR_IOASC_MED_DO_NOT_REALLOC) {\n\t\t\t\tif (ipr_is_vset_device(res))\n\t\t\t\t\tfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_lo);\n\t\t\t\telse\n\t\t\t\t\tfailing_lba = be32_to_cpu(ioasa->u.dasd.failing_lba);\n\n\t\t\t\tsense_buf[0] |= 0x80;\t \n\t\t\t\tsense_buf[3] = (failing_lba & 0xff000000) >> 24;\n\t\t\t\tsense_buf[4] = (failing_lba & 0x00ff0000) >> 16;\n\t\t\t\tsense_buf[5] = (failing_lba & 0x0000ff00) >> 8;\n\t\t\t\tsense_buf[6] = failing_lba & 0x000000ff;\n\t\t\t}\n\n\t\t\tsense_buf[7] = 6;\t \n\t\t}\n\t}\n}\n\n \nstatic int ipr_get_autosense(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\n\tstruct ipr_ioasa64 *ioasa64 = &ipr_cmd->s.ioasa64;\n\n\tif ((be32_to_cpu(ioasa->hdr.ioasc_specific) & IPR_AUTOSENSE_VALID) == 0)\n\t\treturn 0;\n\n\tif (ipr_cmd->ioa_cfg->sis64)\n\t\tmemcpy(ipr_cmd->scsi_cmd->sense_buffer, ioasa64->auto_sense.data,\n\t\t       min_t(u16, be16_to_cpu(ioasa64->auto_sense.auto_sense_len),\n\t\t\t   SCSI_SENSE_BUFFERSIZE));\n\telse\n\t\tmemcpy(ipr_cmd->scsi_cmd->sense_buffer, ioasa->auto_sense.data,\n\t\t       min_t(u16, be16_to_cpu(ioasa->auto_sense.auto_sense_len),\n\t\t\t   SCSI_SENSE_BUFFERSIZE));\n\treturn 1;\n}\n\n \nstatic void ipr_erp_start(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t      struct ipr_cmnd *ipr_cmd)\n{\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\tu32 masked_ioasc = ioasc & IPR_IOASC_IOASC_MASK;\n\n\tif (!res) {\n\t\t__ipr_scsi_eh_done(ipr_cmd);\n\t\treturn;\n\t}\n\n\tif (!ipr_is_gscsi(res) && masked_ioasc != IPR_IOASC_HW_DEV_BUS_STATUS)\n\t\tipr_gen_sense(ipr_cmd);\n\n\tipr_dump_ioasa(ioa_cfg, ipr_cmd, res);\n\n\tswitch (masked_ioasc) {\n\tcase IPR_IOASC_ABORTED_CMD_TERM_BY_HOST:\n\t\tif (ipr_is_naca_model(res))\n\t\t\tscsi_cmd->result |= (DID_ABORT << 16);\n\t\telse\n\t\t\tscsi_cmd->result |= (DID_IMM_RETRY << 16);\n\t\tbreak;\n\tcase IPR_IOASC_IR_RESOURCE_HANDLE:\n\tcase IPR_IOASC_IR_NO_CMDS_TO_2ND_IOA:\n\t\tscsi_cmd->result |= (DID_NO_CONNECT << 16);\n\t\tbreak;\n\tcase IPR_IOASC_HW_SEL_TIMEOUT:\n\t\tscsi_cmd->result |= (DID_NO_CONNECT << 16);\n\t\tif (!ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\tbreak;\n\tcase IPR_IOASC_SYNC_REQUIRED:\n\t\tif (!res->in_erp)\n\t\t\tres->needs_sync_complete = 1;\n\t\tscsi_cmd->result |= (DID_IMM_RETRY << 16);\n\t\tbreak;\n\tcase IPR_IOASC_MED_DO_NOT_REALLOC:  \n\tcase IPR_IOASA_IR_DUAL_IOA_DISABLED:\n\t\t \n\t\tif (scsi_cmd->result != SAM_STAT_CHECK_CONDITION)\n\t\t\tscsi_cmd->result |= (DID_PASSTHROUGH << 16);\n\t\tbreak;\n\tcase IPR_IOASC_BUS_WAS_RESET:\n\tcase IPR_IOASC_BUS_WAS_RESET_BY_OTHER:\n\t\t \n\t\tif (!res->resetting_device)\n\t\t\tscsi_report_bus_reset(ioa_cfg->host, scsi_cmd->device->channel);\n\t\tscsi_cmd->result |= (DID_ERROR << 16);\n\t\tif (!ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\tbreak;\n\tcase IPR_IOASC_HW_DEV_BUS_STATUS:\n\t\tscsi_cmd->result |= IPR_IOASC_SENSE_STATUS(ioasc);\n\t\tif (IPR_IOASC_SENSE_STATUS(ioasc) == SAM_STAT_CHECK_CONDITION) {\n\t\t\tif (!ipr_get_autosense(ipr_cmd)) {\n\t\t\t\tif (!ipr_is_naca_model(res)) {\n\t\t\t\t\tipr_erp_cancel_all(ipr_cmd);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (!ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\tbreak;\n\tcase IPR_IOASC_NR_INIT_CMD_REQUIRED:\n\t\tbreak;\n\tcase IPR_IOASC_IR_NON_OPTIMIZED:\n\t\tif (res->raw_mode) {\n\t\t\tres->raw_mode = 0;\n\t\t\tscsi_cmd->result |= (DID_IMM_RETRY << 16);\n\t\t} else\n\t\t\tscsi_cmd->result |= (DID_ERROR << 16);\n\t\tbreak;\n\tdefault:\n\t\tif (IPR_IOASC_SENSE_KEY(ioasc) > RECOVERED_ERROR)\n\t\t\tscsi_cmd->result |= (DID_ERROR << 16);\n\t\tif (!ipr_is_vset_device(res) && !ipr_is_naca_model(res))\n\t\t\tres->needs_sync_complete = 1;\n\t\tbreak;\n\t}\n\n\tscsi_dma_unmap(ipr_cmd->scsi_cmd);\n\tscsi_done(scsi_cmd);\n\tif (ipr_cmd->eh_comp)\n\t\tcomplete(ipr_cmd->eh_comp);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n}\n\n \nstatic void ipr_scsi_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\tunsigned long lock_flags;\n\n\tscsi_set_resid(scsi_cmd, be32_to_cpu(ipr_cmd->s.ioasa.hdr.residual_data_len));\n\n\tif (likely(IPR_IOASC_SENSE_KEY(ioasc) == 0)) {\n\t\tscsi_dma_unmap(scsi_cmd);\n\n\t\tspin_lock_irqsave(ipr_cmd->hrrq->lock, lock_flags);\n\t\tscsi_done(scsi_cmd);\n\t\tif (ipr_cmd->eh_comp)\n\t\t\tcomplete(ipr_cmd->eh_comp);\n\t\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\t\tspin_unlock_irqrestore(ipr_cmd->hrrq->lock, lock_flags);\n\t} else {\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t\tspin_lock(&ipr_cmd->hrrq->_lock);\n\t\tipr_erp_start(ioa_cfg, ipr_cmd);\n\t\tspin_unlock(&ipr_cmd->hrrq->_lock);\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t}\n}\n\n \nstatic int ipr_queuecommand(struct Scsi_Host *shost,\n\t\t\t    struct scsi_cmnd *scsi_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\tstruct ipr_ioarcb *ioarcb;\n\tstruct ipr_cmnd *ipr_cmd;\n\tunsigned long hrrq_flags;\n\tint rc;\n\tstruct ipr_hrr_queue *hrrq;\n\tint hrrq_id;\n\n\tioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\n\n\tscsi_cmd->result = (DID_OK << 16);\n\tres = scsi_cmd->device->hostdata;\n\n\thrrq_id = ipr_get_hrrq_index(ioa_cfg);\n\thrrq = &ioa_cfg->hrrq[hrrq_id];\n\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\t \n\tif (unlikely(!hrrq->allow_cmds && !hrrq->ioa_is_dead && !hrrq->removing_ioa)) {\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\t \n\tif (unlikely(hrrq->ioa_is_dead || hrrq->removing_ioa || !res)) {\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\tgoto err_nodev;\n\t}\n\n\tipr_cmd = __ipr_get_free_ipr_cmnd(hrrq);\n\tif (ipr_cmd == NULL) {\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\n\tipr_init_ipr_cmnd(ipr_cmd, ipr_scsi_done);\n\tioarcb = &ipr_cmd->ioarcb;\n\n\tmemcpy(ioarcb->cmd_pkt.cdb, scsi_cmd->cmnd, scsi_cmd->cmd_len);\n\tipr_cmd->scsi_cmd = scsi_cmd;\n\tipr_cmd->done = ipr_scsi_eh_done;\n\n\tif (ipr_is_gscsi(res)) {\n\t\tif (scsi_cmd->underflow == 0)\n\t\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\n\n\t\tif (res->reset_occurred) {\n\t\t\tres->reset_occurred = 0;\n\t\t\tioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_DELAY_AFTER_RST;\n\t\t}\n\t}\n\n\tif (ipr_is_gscsi(res) || ipr_is_vset_device(res)) {\n\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_LINK_DESC;\n\n\t\tioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_ALIGNED_BFR;\n\t\tif (scsi_cmd->flags & SCMD_TAGGED)\n\t\t\tioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_SIMPLE_TASK;\n\t\telse\n\t\t\tioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_UNTAGGED_TASK;\n\t}\n\n\tif (scsi_cmd->cmnd[0] >= 0xC0 &&\n\t    (!ipr_is_gscsi(res) || scsi_cmd->cmnd[0] == IPR_QUERY_RSRC_STATE)) {\n\t\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\t}\n\tif (res->raw_mode && ipr_is_af_dasd_device(res)) {\n\t\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_PIPE;\n\n\t\tif (scsi_cmd->underflow == 0)\n\t\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\n\t}\n\n\tif (ioa_cfg->sis64)\n\t\trc = ipr_build_ioadl64(ioa_cfg, ipr_cmd);\n\telse\n\t\trc = ipr_build_ioadl(ioa_cfg, ipr_cmd);\n\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\tif (unlikely(rc || (!hrrq->allow_cmds && !hrrq->ioa_is_dead))) {\n\t\tlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_free_q);\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\tif (!rc)\n\t\t\tscsi_dma_unmap(scsi_cmd);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tif (unlikely(hrrq->ioa_is_dead)) {\n\t\tlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_free_q);\n\t\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\t\tscsi_dma_unmap(scsi_cmd);\n\t\tgoto err_nodev;\n\t}\n\n\tioarcb->res_handle = res->res_handle;\n\tif (res->needs_sync_complete) {\n\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;\n\t\tres->needs_sync_complete = 0;\n\t}\n\tlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_pending_q);\n\tipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_GET_RES_PHYS_LOC(res));\n\tipr_send_command(ipr_cmd);\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\treturn 0;\n\nerr_nodev:\n\tspin_lock_irqsave(hrrq->lock, hrrq_flags);\n\tmemset(scsi_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\n\tscsi_cmd->result = (DID_NO_CONNECT << 16);\n\tscsi_done(scsi_cmd);\n\tspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\n\treturn 0;\n}\n\n \nstatic const char *ipr_ioa_info(struct Scsi_Host *host)\n{\n\tstatic char buffer[512];\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tunsigned long lock_flags = 0;\n\n\tioa_cfg = (struct ipr_ioa_cfg *) host->hostdata;\n\n\tspin_lock_irqsave(host->host_lock, lock_flags);\n\tsprintf(buffer, \"IBM %X Storage Adapter\", ioa_cfg->type);\n\tspin_unlock_irqrestore(host->host_lock, lock_flags);\n\n\treturn buffer;\n}\n\nstatic const struct scsi_host_template driver_template = {\n\t.module = THIS_MODULE,\n\t.name = \"IPR\",\n\t.info = ipr_ioa_info,\n\t.queuecommand = ipr_queuecommand,\n\t.eh_abort_handler = ipr_eh_abort,\n\t.eh_device_reset_handler = ipr_eh_dev_reset,\n\t.eh_host_reset_handler = ipr_eh_host_reset,\n\t.slave_alloc = ipr_slave_alloc,\n\t.slave_configure = ipr_slave_configure,\n\t.slave_destroy = ipr_slave_destroy,\n\t.scan_finished = ipr_scan_finished,\n\t.target_destroy = ipr_target_destroy,\n\t.change_queue_depth = ipr_change_queue_depth,\n\t.bios_param = ipr_biosparam,\n\t.can_queue = IPR_MAX_COMMANDS,\n\t.this_id = -1,\n\t.sg_tablesize = IPR_MAX_SGLIST,\n\t.max_sectors = IPR_IOA_MAX_SECTORS,\n\t.cmd_per_lun = IPR_MAX_CMD_PER_LUN,\n\t.shost_groups = ipr_ioa_groups,\n\t.sdev_groups = ipr_dev_groups,\n\t.proc_name = IPR_NAME,\n};\n\n#ifdef CONFIG_PPC_PSERIES\nstatic const u16 ipr_blocked_processors[] = {\n\tPVR_NORTHSTAR,\n\tPVR_PULSAR,\n\tPVR_POWER4,\n\tPVR_ICESTAR,\n\tPVR_SSTAR,\n\tPVR_POWER4p,\n\tPVR_630,\n\tPVR_630p\n};\n\n \nstatic int ipr_invalid_adapter(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint i;\n\n\tif ((ioa_cfg->type == 0x5702) && (ioa_cfg->pdev->revision < 4)) {\n\t\tfor (i = 0; i < ARRAY_SIZE(ipr_blocked_processors); i++) {\n\t\t\tif (pvr_version_is(ipr_blocked_processors[i]))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n#else\n#define ipr_invalid_adapter(ioa_cfg) 0\n#endif\n\n \nstatic int ipr_ioa_bringdown_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint i;\n\n\tENTER;\n\tif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\n\t\tipr_trace;\n\t\tioa_cfg->scsi_unblock = 1;\n\t\tschedule_work(&ioa_cfg->work_q);\n\t}\n\n\tioa_cfg->in_reset_reload = 0;\n\tioa_cfg->reset_retries = 0;\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].ioa_is_dead = 1;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\twmb();\n\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\twake_up_all(&ioa_cfg->reset_wait_q);\n\tLEAVE;\n\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_ioa_reset_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_resource_entry *res;\n\tint j;\n\n\tENTER;\n\tioa_cfg->in_reset_reload = 0;\n\tfor (j = 0; j < ioa_cfg->hrrq_num; j++) {\n\t\tspin_lock(&ioa_cfg->hrrq[j]._lock);\n\t\tioa_cfg->hrrq[j].allow_cmds = 1;\n\t\tspin_unlock(&ioa_cfg->hrrq[j]._lock);\n\t}\n\twmb();\n\tioa_cfg->reset_cmd = NULL;\n\tioa_cfg->doorbell |= IPR_RUNTIME_RESET;\n\n\tlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\n\t\tif (res->add_to_ml || res->del_from_ml) {\n\t\t\tipr_trace;\n\t\t\tbreak;\n\t\t}\n\t}\n\tschedule_work(&ioa_cfg->work_q);\n\n\tfor (j = 0; j < IPR_NUM_HCAMS; j++) {\n\t\tlist_del_init(&ioa_cfg->hostrcb[j]->queue);\n\t\tif (j < IPR_NUM_LOG_HCAMS)\n\t\t\tipr_send_hcam(ioa_cfg,\n\t\t\t\tIPR_HCAM_CDB_OP_CODE_LOG_DATA,\n\t\t\t\tioa_cfg->hostrcb[j]);\n\t\telse\n\t\t\tipr_send_hcam(ioa_cfg,\n\t\t\t\tIPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,\n\t\t\t\tioa_cfg->hostrcb[j]);\n\t}\n\n\tscsi_report_bus_reset(ioa_cfg->host, IPR_VSET_BUS);\n\tdev_info(&ioa_cfg->pdev->dev, \"IOA initialized.\\n\");\n\n\tioa_cfg->reset_retries = 0;\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\twake_up_all(&ioa_cfg->reset_wait_q);\n\n\tioa_cfg->scsi_unblock = 1;\n\tschedule_work(&ioa_cfg->work_q);\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic void ipr_set_sup_dev_dflt(struct ipr_supported_device *supported_dev,\n\t\t\t\t struct ipr_std_inq_vpids *vpids)\n{\n\tmemset(supported_dev, 0, sizeof(struct ipr_supported_device));\n\tmemcpy(&supported_dev->vpids, vpids, sizeof(struct ipr_std_inq_vpids));\n\tsupported_dev->num_records = 1;\n\tsupported_dev->data_length =\n\t\tcpu_to_be16(sizeof(struct ipr_supported_device));\n\tsupported_dev->reserved = 0;\n}\n\n \nstatic int ipr_set_supported_devs(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_supported_device *supp_dev = &ioa_cfg->vpd_cbs->supp_dev;\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_resource_entry *res = ipr_cmd->u.res;\n\n\tipr_cmd->job_step = ipr_ioa_reset_done;\n\n\tlist_for_each_entry_continue(res, &ioa_cfg->used_res_q, queue) {\n\t\tif (!ipr_is_scsi_disk(res))\n\t\t\tcontinue;\n\n\t\tipr_cmd->u.res = res;\n\t\tipr_set_sup_dev_dflt(supp_dev, &res->std_inq_data.vpids);\n\n\t\tioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\t\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\t\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\n\t\tioarcb->cmd_pkt.cdb[0] = IPR_SET_SUPPORTED_DEVICES;\n\t\tioarcb->cmd_pkt.cdb[1] = IPR_SET_ALL_SUPPORTED_DEVICES;\n\t\tioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_supported_device) >> 8) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_supported_device) & 0xff;\n\n\t\tipr_init_ioadl(ipr_cmd,\n\t\t\t       ioa_cfg->vpd_cbs_dma +\n\t\t\t\t offsetof(struct ipr_misc_cbs, supp_dev),\n\t\t\t       sizeof(struct ipr_supported_device),\n\t\t\t       IPR_IOADL_FLAGS_WRITE_LAST);\n\n\t\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\n\t\t\t   IPR_SET_SUP_DEVICE_TIMEOUT);\n\n\t\tif (!ioa_cfg->sis64)\n\t\t\tipr_cmd->job_step = ipr_set_supported_devs;\n\t\tLEAVE;\n\t\treturn IPR_RC_JOB_RETURN;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic void *ipr_get_mode_page(struct ipr_mode_pages *mode_pages,\n\t\t\t       u32 page_code, u32 len)\n{\n\tstruct ipr_mode_page_hdr *mode_hdr;\n\tu32 page_length;\n\tu32 length;\n\n\tif (!mode_pages || (mode_pages->hdr.length == 0))\n\t\treturn NULL;\n\n\tlength = (mode_pages->hdr.length + 1) - 4 - mode_pages->hdr.block_desc_len;\n\tmode_hdr = (struct ipr_mode_page_hdr *)\n\t\t(mode_pages->data + mode_pages->hdr.block_desc_len);\n\n\twhile (length) {\n\t\tif (IPR_GET_MODE_PAGE_CODE(mode_hdr) == page_code) {\n\t\t\tif (mode_hdr->page_length >= (len - sizeof(struct ipr_mode_page_hdr)))\n\t\t\t\treturn mode_hdr;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tpage_length = (sizeof(struct ipr_mode_page_hdr) +\n\t\t\t\t       mode_hdr->page_length);\n\t\t\tlength -= page_length;\n\t\t\tmode_hdr = (struct ipr_mode_page_hdr *)\n\t\t\t\t((unsigned long)mode_hdr + page_length);\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstatic void ipr_check_term_power(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t struct ipr_mode_pages *mode_pages)\n{\n\tint i;\n\tint entry_length;\n\tstruct ipr_dev_bus_entry *bus;\n\tstruct ipr_mode_page28 *mode_page;\n\n\tmode_page = ipr_get_mode_page(mode_pages, 0x28,\n\t\t\t\t      sizeof(struct ipr_mode_page28));\n\n\tentry_length = mode_page->entry_length;\n\n\tbus = mode_page->bus;\n\n\tfor (i = 0; i < mode_page->num_entries; i++) {\n\t\tif (bus->flags & IPR_SCSI_ATTR_NO_TERM_PWR) {\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Term power is absent on scsi bus %d\\n\",\n\t\t\t\tbus->res_addr.bus);\n\t\t}\n\n\t\tbus = (struct ipr_dev_bus_entry *)((char *)bus + entry_length);\n\t}\n}\n\n \nstatic void ipr_scsi_bus_speed_limit(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tu32 max_xfer_rate;\n\tint i;\n\n\tfor (i = 0; i < IPR_MAX_NUM_BUSES; i++) {\n\t\tmax_xfer_rate = ipr_get_max_scsi_speed(ioa_cfg, i,\n\t\t\t\t\t\t       ioa_cfg->bus_attr[i].bus_width);\n\n\t\tif (max_xfer_rate < ioa_cfg->bus_attr[i].max_xfer_rate)\n\t\t\tioa_cfg->bus_attr[i].max_xfer_rate = max_xfer_rate;\n\t}\n}\n\n \nstatic void ipr_modify_ioafp_mode_page_28(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t\t  struct ipr_mode_pages *mode_pages)\n{\n\tint i, entry_length;\n\tstruct ipr_dev_bus_entry *bus;\n\tstruct ipr_bus_attributes *bus_attr;\n\tstruct ipr_mode_page28 *mode_page;\n\n\tmode_page = ipr_get_mode_page(mode_pages, 0x28,\n\t\t\t\t      sizeof(struct ipr_mode_page28));\n\n\tentry_length = mode_page->entry_length;\n\n\t \n\tfor (i = 0, bus = mode_page->bus;\n\t     i < mode_page->num_entries;\n\t     i++, bus = (struct ipr_dev_bus_entry *)((u8 *)bus + entry_length)) {\n\t\tif (bus->res_addr.bus > IPR_MAX_NUM_BUSES) {\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Invalid resource address reported: 0x%08X\\n\",\n\t\t\t\tIPR_GET_PHYS_LOC(bus->res_addr));\n\t\t\tcontinue;\n\t\t}\n\n\t\tbus_attr = &ioa_cfg->bus_attr[i];\n\t\tbus->extended_reset_delay = IPR_EXTENDED_RESET_DELAY;\n\t\tbus->bus_width = bus_attr->bus_width;\n\t\tbus->max_xfer_rate = cpu_to_be32(bus_attr->max_xfer_rate);\n\t\tbus->flags &= ~IPR_SCSI_ATTR_QAS_MASK;\n\t\tif (bus_attr->qas_enabled)\n\t\t\tbus->flags |= IPR_SCSI_ATTR_ENABLE_QAS;\n\t\telse\n\t\t\tbus->flags |= IPR_SCSI_ATTR_DISABLE_QAS;\n\t}\n}\n\n \nstatic void ipr_build_mode_select(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t  __be32 res_handle, u8 parm,\n\t\t\t\t  dma_addr_t dma_addr, u8 xfer_len)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\n\tioarcb->res_handle = res_handle;\n\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\n\tioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\n\tioarcb->cmd_pkt.cdb[0] = MODE_SELECT;\n\tioarcb->cmd_pkt.cdb[1] = parm;\n\tioarcb->cmd_pkt.cdb[4] = xfer_len;\n\n\tipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_WRITE_LAST);\n}\n\n \nstatic int ipr_ioafp_mode_select_page28(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;\n\tint length;\n\n\tENTER;\n\tipr_scsi_bus_speed_limit(ioa_cfg);\n\tipr_check_term_power(ioa_cfg, mode_pages);\n\tipr_modify_ioafp_mode_page_28(ioa_cfg, mode_pages);\n\tlength = mode_pages->hdr.length + 1;\n\tmode_pages->hdr.length = 0;\n\n\tipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,\n\t\t\t      ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),\n\t\t\t      length);\n\n\tipr_cmd->job_step = ipr_set_supported_devs;\n\tipr_cmd->u.res = list_entry(ioa_cfg->used_res_q.next,\n\t\t\t\t    struct ipr_resource_entry, queue);\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic void ipr_build_mode_sense(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t __be32 res_handle,\n\t\t\t\t u8 parm, dma_addr_t dma_addr, u8 xfer_len)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\n\tioarcb->res_handle = res_handle;\n\tioarcb->cmd_pkt.cdb[0] = MODE_SENSE;\n\tioarcb->cmd_pkt.cdb[2] = parm;\n\tioarcb->cmd_pkt.cdb[4] = xfer_len;\n\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\n\n\tipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_READ_LAST);\n}\n\n \nstatic int ipr_reset_cmd_failed(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\"0x%02X failed with IOASC: 0x%08X\\n\",\n\t\tipr_cmd->ioarcb.cmd_pkt.cdb[0], ioasc);\n\n\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_mode_sense_failed(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT) {\n\t\tipr_cmd->job_step = ipr_set_supported_devs;\n\t\tipr_cmd->u.res = list_entry(ioa_cfg->used_res_q.next,\n\t\t\t\t\t    struct ipr_resource_entry, queue);\n\t\treturn IPR_RC_JOB_CONTINUE;\n\t}\n\n\treturn ipr_reset_cmd_failed(ipr_cmd);\n}\n\n \nstatic int ipr_ioafp_mode_sense_page28(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),\n\t\t\t     0x28, ioa_cfg->vpd_cbs_dma +\n\t\t\t     offsetof(struct ipr_misc_cbs, mode_pages),\n\t\t\t     sizeof(struct ipr_mode_pages));\n\n\tipr_cmd->job_step = ipr_ioafp_mode_select_page28;\n\tipr_cmd->job_step_failed = ipr_reset_mode_sense_failed;\n\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_ioafp_mode_select_page24(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;\n\tstruct ipr_mode_page24 *mode_page;\n\tint length;\n\n\tENTER;\n\tmode_page = ipr_get_mode_page(mode_pages, 0x24,\n\t\t\t\t      sizeof(struct ipr_mode_page24));\n\n\tif (mode_page)\n\t\tmode_page->flags |= IPR_ENABLE_DUAL_IOA_AF;\n\n\tlength = mode_pages->hdr.length + 1;\n\tmode_pages->hdr.length = 0;\n\n\tipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,\n\t\t\t      ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),\n\t\t\t      length);\n\n\tipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_mode_sense_page24_failed(struct ipr_cmnd *ipr_cmd)\n{\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT) {\n\t\tipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\n\t\treturn IPR_RC_JOB_CONTINUE;\n\t}\n\n\treturn ipr_reset_cmd_failed(ipr_cmd);\n}\n\n \nstatic int ipr_ioafp_mode_sense_page24(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),\n\t\t\t     0x24, ioa_cfg->vpd_cbs_dma +\n\t\t\t     offsetof(struct ipr_misc_cbs, mode_pages),\n\t\t\t     sizeof(struct ipr_mode_pages));\n\n\tipr_cmd->job_step = ipr_ioafp_mode_select_page24;\n\tipr_cmd->job_step_failed = ipr_reset_mode_sense_page24_failed;\n\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_init_res_table(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_resource_entry *res, *temp;\n\tstruct ipr_config_table_entry_wrapper cfgtew;\n\tint entries, found, flag, i;\n\tLIST_HEAD(old_res);\n\n\tENTER;\n\tif (ioa_cfg->sis64)\n\t\tflag = ioa_cfg->u.cfg_table64->hdr64.flags;\n\telse\n\t\tflag = ioa_cfg->u.cfg_table->hdr.flags;\n\n\tif (flag & IPR_UCODE_DOWNLOAD_REQ)\n\t\tdev_err(&ioa_cfg->pdev->dev, \"Microcode download required\\n\");\n\n\tlist_for_each_entry_safe(res, temp, &ioa_cfg->used_res_q, queue)\n\t\tlist_move_tail(&res->queue, &old_res);\n\n\tif (ioa_cfg->sis64)\n\t\tentries = be16_to_cpu(ioa_cfg->u.cfg_table64->hdr64.num_entries);\n\telse\n\t\tentries = ioa_cfg->u.cfg_table->hdr.num_entries;\n\n\tfor (i = 0; i < entries; i++) {\n\t\tif (ioa_cfg->sis64)\n\t\t\tcfgtew.u.cfgte64 = &ioa_cfg->u.cfg_table64->dev[i];\n\t\telse\n\t\t\tcfgtew.u.cfgte = &ioa_cfg->u.cfg_table->dev[i];\n\t\tfound = 0;\n\n\t\tlist_for_each_entry_safe(res, temp, &old_res, queue) {\n\t\t\tif (ipr_is_same_device(res, &cfgtew)) {\n\t\t\t\tlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\tif (list_empty(&ioa_cfg->free_res_q)) {\n\t\t\t\tdev_err(&ioa_cfg->pdev->dev, \"Too many devices attached\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfound = 1;\n\t\t\tres = list_entry(ioa_cfg->free_res_q.next,\n\t\t\t\t\t struct ipr_resource_entry, queue);\n\t\t\tlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\n\t\t\tipr_init_res_entry(res, &cfgtew);\n\t\t\tres->add_to_ml = 1;\n\t\t} else if (res->sdev && (ipr_is_vset_device(res) || ipr_is_scsi_disk(res)))\n\t\t\tres->sdev->allow_restart = 1;\n\n\t\tif (found)\n\t\t\tipr_update_res_entry(res, &cfgtew);\n\t}\n\n\tlist_for_each_entry_safe(res, temp, &old_res, queue) {\n\t\tif (res->sdev) {\n\t\t\tres->del_from_ml = 1;\n\t\t\tres->res_handle = IPR_INVALID_RES_HANDLE;\n\t\t\tlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\n\t\t}\n\t}\n\n\tlist_for_each_entry_safe(res, temp, &old_res, queue) {\n\t\tipr_clear_res_target(res);\n\t\tlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\n\t}\n\n\tif (ioa_cfg->dual_raid && ipr_dual_ioa_raid)\n\t\tipr_cmd->job_step = ipr_ioafp_mode_sense_page24;\n\telse\n\t\tipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_ioafp_query_ioa_cfg(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\n\tstruct ipr_inquiry_cap *cap = &ioa_cfg->vpd_cbs->cap;\n\n\tENTER;\n\tif (cap->cap & IPR_CAP_DUAL_IOA_RAID)\n\t\tioa_cfg->dual_raid = 1;\n\tdev_info(&ioa_cfg->pdev->dev, \"Adapter firmware version: %02X%02X%02X%02X\\n\",\n\t\t ucode_vpd->major_release, ucode_vpd->card_type,\n\t\t ucode_vpd->minor_release[0], ucode_vpd->minor_release[1]);\n\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\tioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\n\tioarcb->cmd_pkt.cdb[0] = IPR_QUERY_IOA_CONFIG;\n\tioarcb->cmd_pkt.cdb[6] = (ioa_cfg->cfg_table_size >> 16) & 0xff;\n\tioarcb->cmd_pkt.cdb[7] = (ioa_cfg->cfg_table_size >> 8) & 0xff;\n\tioarcb->cmd_pkt.cdb[8] = ioa_cfg->cfg_table_size & 0xff;\n\n\tipr_init_ioadl(ipr_cmd, ioa_cfg->cfg_table_dma, ioa_cfg->cfg_table_size,\n\t\t       IPR_IOADL_FLAGS_READ_LAST);\n\n\tipr_cmd->job_step = ipr_init_res_table;\n\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\nstatic int ipr_ioa_service_action_failed(struct ipr_cmnd *ipr_cmd)\n{\n\tu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\tif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT)\n\t\treturn IPR_RC_JOB_CONTINUE;\n\n\treturn ipr_reset_cmd_failed(ipr_cmd);\n}\n\nstatic void ipr_build_ioa_service_action(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t\t __be32 res_handle, u8 sa_code)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\n\tioarcb->res_handle = res_handle;\n\tioarcb->cmd_pkt.cdb[0] = IPR_IOA_SERVICE_ACTION;\n\tioarcb->cmd_pkt.cdb[1] = sa_code;\n\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n}\n\n \nstatic int ipr_ioafp_set_caching_parameters(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_inquiry_pageC4 *pageC4 = &ioa_cfg->vpd_cbs->pageC4_data;\n\n\tENTER;\n\n\tipr_cmd->job_step = ipr_ioafp_query_ioa_cfg;\n\n\tif (pageC4->cache_cap[0] & IPR_CAP_SYNC_CACHE) {\n\t\tipr_build_ioa_service_action(ipr_cmd,\n\t\t\t\t\t     cpu_to_be32(IPR_IOA_RES_HANDLE),\n\t\t\t\t\t     IPR_IOA_SA_CHANGE_CACHE_PARAMS);\n\n\t\tioarcb->cmd_pkt.cdb[2] = 0x40;\n\n\t\tipr_cmd->job_step_failed = ipr_ioa_service_action_failed;\n\t\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\n\t\t\t   IPR_SET_SUP_DEVICE_TIMEOUT);\n\n\t\tLEAVE;\n\t\treturn IPR_RC_JOB_RETURN;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic void ipr_ioafp_inquiry(struct ipr_cmnd *ipr_cmd, u8 flags, u8 page,\n\t\t\t      dma_addr_t dma_addr, u8 xfer_len)\n{\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\n\tENTER;\n\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\n\tioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\n\tioarcb->cmd_pkt.cdb[0] = INQUIRY;\n\tioarcb->cmd_pkt.cdb[1] = flags;\n\tioarcb->cmd_pkt.cdb[2] = page;\n\tioarcb->cmd_pkt.cdb[4] = xfer_len;\n\n\tipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_READ_LAST);\n\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\n\tLEAVE;\n}\n\n \nstatic int ipr_inquiry_page_supported(struct ipr_inquiry_page0 *page0, u8 page)\n{\n\tint i;\n\n\tfor (i = 0; i < min_t(u8, page0->len, IPR_INQUIRY_PAGE0_ENTRIES); i++)\n\t\tif (page0->page[i] == page)\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int ipr_ioafp_pageC4_inquiry(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_inquiry_page0 *page0 = &ioa_cfg->vpd_cbs->page0_data;\n\tstruct ipr_inquiry_pageC4 *pageC4 = &ioa_cfg->vpd_cbs->pageC4_data;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioafp_set_caching_parameters;\n\tmemset(pageC4, 0, sizeof(*pageC4));\n\n\tif (ipr_inquiry_page_supported(page0, 0xC4)) {\n\t\tipr_ioafp_inquiry(ipr_cmd, 1, 0xC4,\n\t\t\t\t  (ioa_cfg->vpd_cbs_dma\n\t\t\t\t   + offsetof(struct ipr_misc_cbs,\n\t\t\t\t\t      pageC4_data)),\n\t\t\t\t  sizeof(struct ipr_inquiry_pageC4));\n\t\treturn IPR_RC_JOB_RETURN;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_ioafp_cap_inquiry(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_inquiry_page0 *page0 = &ioa_cfg->vpd_cbs->page0_data;\n\tstruct ipr_inquiry_cap *cap = &ioa_cfg->vpd_cbs->cap;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioafp_pageC4_inquiry;\n\tmemset(cap, 0, sizeof(*cap));\n\n\tif (ipr_inquiry_page_supported(page0, 0xD0)) {\n\t\tipr_ioafp_inquiry(ipr_cmd, 1, 0xD0,\n\t\t\t\t  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, cap),\n\t\t\t\t  sizeof(struct ipr_inquiry_cap));\n\t\treturn IPR_RC_JOB_RETURN;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_ioafp_page3_inquiry(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\n\tipr_cmd->job_step = ipr_ioafp_cap_inquiry;\n\n\tipr_ioafp_inquiry(ipr_cmd, 1, 3,\n\t\t\t  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page3_data),\n\t\t\t  sizeof(struct ipr_inquiry_page3));\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_ioafp_page0_inquiry(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tchar type[5];\n\n\tENTER;\n\n\t \n\tmemcpy(type, ioa_cfg->vpd_cbs->ioa_vpd.std_inq_data.vpids.product_id, 4);\n\ttype[4] = '\\0';\n\tioa_cfg->type = simple_strtoul((char *)type, NULL, 16);\n\n\tif (ipr_invalid_adapter(ioa_cfg)) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"Adapter not supported in this hardware configuration.\\n\");\n\n\t\tif (!ipr_testmode) {\n\t\t\tioa_cfg->reset_retries += IPR_NUM_RESET_RELOAD_RETRIES;\n\t\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t\t\tlist_add_tail(&ipr_cmd->queue,\n\t\t\t\t\t&ioa_cfg->hrrq->hrrq_free_q);\n\t\t\treturn IPR_RC_JOB_RETURN;\n\t\t}\n\t}\n\n\tipr_cmd->job_step = ipr_ioafp_page3_inquiry;\n\n\tipr_ioafp_inquiry(ipr_cmd, 1, 0,\n\t\t\t  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page0_data),\n\t\t\t  sizeof(struct ipr_inquiry_page0));\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_ioafp_std_inquiry(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioafp_page0_inquiry;\n\n\tipr_ioafp_inquiry(ipr_cmd, 0, 0,\n\t\t\t  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, ioa_vpd),\n\t\t\t  sizeof(struct ipr_ioa_vpd));\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_ioafp_identify_hrrq(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\n\tstruct ipr_hrr_queue *hrrq;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioafp_std_inquiry;\n\tif (ioa_cfg->identify_hrrq_index == 0)\n\t\tdev_info(&ioa_cfg->pdev->dev, \"Starting IOA initialization sequence.\\n\");\n\n\tif (ioa_cfg->identify_hrrq_index < ioa_cfg->hrrq_num) {\n\t\thrrq = &ioa_cfg->hrrq[ioa_cfg->identify_hrrq_index];\n\n\t\tioarcb->cmd_pkt.cdb[0] = IPR_ID_HOST_RR_Q;\n\t\tioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\n\t\tioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\t\tif (ioa_cfg->sis64)\n\t\t\tioarcb->cmd_pkt.cdb[1] = 0x1;\n\n\t\tif (ioa_cfg->nvectors == 1)\n\t\t\tioarcb->cmd_pkt.cdb[1] &= ~IPR_ID_HRRQ_SELE_ENABLE;\n\t\telse\n\t\t\tioarcb->cmd_pkt.cdb[1] |= IPR_ID_HRRQ_SELE_ENABLE;\n\n\t\tioarcb->cmd_pkt.cdb[2] =\n\t\t\t((u64) hrrq->host_rrq_dma >> 24) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[3] =\n\t\t\t((u64) hrrq->host_rrq_dma >> 16) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[4] =\n\t\t\t((u64) hrrq->host_rrq_dma >> 8) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[5] =\n\t\t\t((u64) hrrq->host_rrq_dma) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[7] =\n\t\t\t((sizeof(u32) * hrrq->size) >> 8) & 0xff;\n\t\tioarcb->cmd_pkt.cdb[8] =\n\t\t\t(sizeof(u32) * hrrq->size) & 0xff;\n\n\t\tif (ioarcb->cmd_pkt.cdb[1] & IPR_ID_HRRQ_SELE_ENABLE)\n\t\t\tioarcb->cmd_pkt.cdb[9] =\n\t\t\t\t\tioa_cfg->identify_hrrq_index;\n\n\t\tif (ioa_cfg->sis64) {\n\t\t\tioarcb->cmd_pkt.cdb[10] =\n\t\t\t\t((u64) hrrq->host_rrq_dma >> 56) & 0xff;\n\t\t\tioarcb->cmd_pkt.cdb[11] =\n\t\t\t\t((u64) hrrq->host_rrq_dma >> 48) & 0xff;\n\t\t\tioarcb->cmd_pkt.cdb[12] =\n\t\t\t\t((u64) hrrq->host_rrq_dma >> 40) & 0xff;\n\t\t\tioarcb->cmd_pkt.cdb[13] =\n\t\t\t\t((u64) hrrq->host_rrq_dma >> 32) & 0xff;\n\t\t}\n\n\t\tif (ioarcb->cmd_pkt.cdb[1] & IPR_ID_HRRQ_SELE_ENABLE)\n\t\t\tioarcb->cmd_pkt.cdb[14] =\n\t\t\t\t\tioa_cfg->identify_hrrq_index;\n\n\t\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\n\t\t\t   IPR_INTERNAL_TIMEOUT);\n\n\t\tif (++ioa_cfg->identify_hrrq_index < ioa_cfg->hrrq_num)\n\t\t\tipr_cmd->job_step = ipr_ioafp_identify_hrrq;\n\n\t\tLEAVE;\n\t\treturn IPR_RC_JOB_RETURN;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic void ipr_reset_timer_done(struct timer_list *t)\n{\n\tstruct ipr_cmnd *ipr_cmd = from_timer(ipr_cmd, t, timer);\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tunsigned long lock_flags = 0;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (ioa_cfg->reset_cmd == ipr_cmd) {\n\t\tlist_del(&ipr_cmd->queue);\n\t\tipr_cmd->done(ipr_cmd);\n\t}\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n}\n\n \nstatic void ipr_reset_start_timer(struct ipr_cmnd *ipr_cmd,\n\t\t\t\t  unsigned long timeout)\n{\n\n\tENTER;\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\tipr_cmd->done = ipr_reset_ioa_job;\n\n\tipr_cmd->timer.expires = jiffies + timeout;\n\tipr_cmd->timer.function = ipr_reset_timer_done;\n\tadd_timer(&ipr_cmd->timer);\n}\n\n \nstatic void ipr_init_ioa_mem(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_hrr_queue *hrrq;\n\n\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\tspin_lock(&hrrq->_lock);\n\t\tmemset(hrrq->host_rrq, 0, sizeof(u32) * hrrq->size);\n\n\t\t \n\t\thrrq->hrrq_start = hrrq->host_rrq;\n\t\thrrq->hrrq_end = &hrrq->host_rrq[hrrq->size - 1];\n\t\thrrq->hrrq_curr = hrrq->hrrq_start;\n\t\thrrq->toggle_bit = 1;\n\t\tspin_unlock(&hrrq->_lock);\n\t}\n\twmb();\n\n\tioa_cfg->identify_hrrq_index = 0;\n\tif (ioa_cfg->hrrq_num == 1)\n\t\tatomic_set(&ioa_cfg->hrrq_index, 0);\n\telse\n\t\tatomic_set(&ioa_cfg->hrrq_index, 1);\n\n\t \n\tmemset(ioa_cfg->u.cfg_table, 0, ioa_cfg->cfg_table_size);\n}\n\n \nstatic int ipr_reset_next_stage(struct ipr_cmnd *ipr_cmd)\n{\n\tunsigned long stage, stage_time;\n\tu32 feedback;\n\tvolatile u32 int_reg;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tu64 maskval = 0;\n\n\tfeedback = readl(ioa_cfg->regs.init_feedback_reg);\n\tstage = feedback & IPR_IPL_INIT_STAGE_MASK;\n\tstage_time = feedback & IPR_IPL_INIT_STAGE_TIME_MASK;\n\n\tipr_dbg(\"IPL stage = 0x%lx, IPL stage time = %ld\\n\", stage, stage_time);\n\n\t \n\tif (stage_time == 0)\n\t\tstage_time = IPR_IPL_INIT_DEFAULT_STAGE_TIME;\n\telse if (stage_time < IPR_IPL_INIT_MIN_STAGE_TIME)\n\t\tstage_time = IPR_IPL_INIT_MIN_STAGE_TIME;\n\telse if (stage_time > IPR_LONG_OPERATIONAL_TIMEOUT)\n\t\tstage_time = IPR_LONG_OPERATIONAL_TIMEOUT;\n\n\tif (stage == IPR_IPL_INIT_STAGE_UNKNOWN) {\n\t\twritel(IPR_PCII_IPL_STAGE_CHANGE, ioa_cfg->regs.set_interrupt_mask_reg);\n\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\t\tstage_time = ioa_cfg->transop_timeout;\n\t\tipr_cmd->job_step = ipr_ioafp_identify_hrrq;\n\t} else if (stage == IPR_IPL_INIT_STAGE_TRANSOP) {\n\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\t\tif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\n\t\t\tipr_cmd->job_step = ipr_ioafp_identify_hrrq;\n\t\t\tmaskval = IPR_PCII_IPL_STAGE_CHANGE;\n\t\t\tmaskval = (maskval << 32) | IPR_PCII_IOA_TRANS_TO_OPER;\n\t\t\twriteq(maskval, ioa_cfg->regs.set_interrupt_mask_reg);\n\t\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\t\t\treturn IPR_RC_JOB_CONTINUE;\n\t\t}\n\t}\n\n\tipr_cmd->timer.expires = jiffies + stage_time * HZ;\n\tipr_cmd->timer.function = ipr_oper_timeout;\n\tipr_cmd->done = ipr_reset_ioa_job;\n\tadd_timer(&ipr_cmd->timer);\n\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_enable_ioa(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tvolatile u32 int_reg;\n\tvolatile u64 maskval;\n\tint i;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioafp_identify_hrrq;\n\tipr_init_ioa_mem(ioa_cfg);\n\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].allow_interrupts = 1;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\tif (ioa_cfg->sis64) {\n\t\t \n\t\twritel(IPR_ENDIAN_SWAP_KEY, ioa_cfg->regs.endian_swap_reg);\n\t\tint_reg = readl(ioa_cfg->regs.endian_swap_reg);\n\t}\n\n\tint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\n\tif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\n\t\twritel((IPR_PCII_ERROR_INTERRUPTS | IPR_PCII_HRRQ_UPDATED),\n\t\t       ioa_cfg->regs.clr_interrupt_mask_reg32);\n\t\tint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\t\treturn IPR_RC_JOB_CONTINUE;\n\t}\n\n\t \n\twritel(ioa_cfg->doorbell, ioa_cfg->regs.set_uproc_interrupt_reg32);\n\n\tif (ioa_cfg->sis64) {\n\t\tmaskval = IPR_PCII_IPL_STAGE_CHANGE;\n\t\tmaskval = (maskval << 32) | IPR_PCII_OPER_INTERRUPTS;\n\t\twriteq(maskval, ioa_cfg->regs.clr_interrupt_mask_reg);\n\t} else\n\t\twritel(IPR_PCII_OPER_INTERRUPTS, ioa_cfg->regs.clr_interrupt_mask_reg32);\n\n\tint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\n\tdev_info(&ioa_cfg->pdev->dev, \"Initializing IOA.\\n\");\n\n\tif (ioa_cfg->sis64) {\n\t\tipr_cmd->job_step = ipr_reset_next_stage;\n\t\treturn IPR_RC_JOB_CONTINUE;\n\t}\n\n\tipr_cmd->timer.expires = jiffies + (ioa_cfg->transop_timeout * HZ);\n\tipr_cmd->timer.function = ipr_oper_timeout;\n\tipr_cmd->done = ipr_reset_ioa_job;\n\tadd_timer(&ipr_cmd->timer);\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_wait_for_dump(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tif (ioa_cfg->sdt_state == GET_DUMP)\n\t\tioa_cfg->sdt_state = WAIT_FOR_DUMP;\n\telse if (ioa_cfg->sdt_state == READ_DUMP)\n\t\tioa_cfg->sdt_state = ABORT_DUMP;\n\n\tioa_cfg->dump_timeout = 1;\n\tipr_cmd->job_step = ipr_reset_alert;\n\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic void ipr_unit_check_no_data(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tioa_cfg->errors_logged++;\n\tdev_err(&ioa_cfg->pdev->dev, \"IOA unit check with no data\\n\");\n}\n\n \nstatic void ipr_get_unit_check_buffer(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tunsigned long mailbox;\n\tstruct ipr_hostrcb *hostrcb;\n\tstruct ipr_uc_sdt sdt;\n\tint rc, length;\n\tu32 ioasc;\n\n\tmailbox = readl(ioa_cfg->ioa_mailbox);\n\n\tif (!ioa_cfg->sis64 && !ipr_sdt_is_fmt2(mailbox)) {\n\t\tipr_unit_check_no_data(ioa_cfg);\n\t\treturn;\n\t}\n\n\tmemset(&sdt, 0, sizeof(struct ipr_uc_sdt));\n\trc = ipr_get_ldump_data_section(ioa_cfg, mailbox, (__be32 *) &sdt,\n\t\t\t\t\t(sizeof(struct ipr_uc_sdt)) / sizeof(__be32));\n\n\tif (rc || !(sdt.entry[0].flags & IPR_SDT_VALID_ENTRY) ||\n\t    ((be32_to_cpu(sdt.hdr.state) != IPR_FMT3_SDT_READY_TO_USE) &&\n\t    (be32_to_cpu(sdt.hdr.state) != IPR_FMT2_SDT_READY_TO_USE))) {\n\t\tipr_unit_check_no_data(ioa_cfg);\n\t\treturn;\n\t}\n\n\t \n\tif (be32_to_cpu(sdt.hdr.state) == IPR_FMT3_SDT_READY_TO_USE)\n\t\tlength = be32_to_cpu(sdt.entry[0].end_token);\n\telse\n\t\tlength = (be32_to_cpu(sdt.entry[0].end_token) -\n\t\t\t  be32_to_cpu(sdt.entry[0].start_token)) &\n\t\t\t  IPR_FMT2_MBX_ADDR_MASK;\n\n\thostrcb = list_entry(ioa_cfg->hostrcb_free_q.next,\n\t\t\t     struct ipr_hostrcb, queue);\n\tlist_del_init(&hostrcb->queue);\n\tmemset(&hostrcb->hcam, 0, sizeof(hostrcb->hcam));\n\n\trc = ipr_get_ldump_data_section(ioa_cfg,\n\t\t\t\t\tbe32_to_cpu(sdt.entry[0].start_token),\n\t\t\t\t\t(__be32 *)&hostrcb->hcam,\n\t\t\t\t\tmin(length, (int)sizeof(hostrcb->hcam)) / sizeof(__be32));\n\n\tif (!rc) {\n\t\tipr_handle_log_data(ioa_cfg, hostrcb);\n\t\tioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\n\t\tif (ioasc == IPR_IOASC_NR_IOA_RESET_REQUIRED &&\n\t\t    ioa_cfg->sdt_state == GET_DUMP)\n\t\t\tioa_cfg->sdt_state = WAIT_FOR_DUMP;\n\t} else\n\t\tipr_unit_check_no_data(ioa_cfg);\n\n\tlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\n}\n\n \nstatic int ipr_reset_get_unit_check_job(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tioa_cfg->ioa_unit_checked = 0;\n\tipr_get_unit_check_buffer(ioa_cfg);\n\tipr_cmd->job_step = ipr_reset_alert;\n\tipr_reset_start_timer(ipr_cmd, 0);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\nstatic int ipr_dump_mailbox_wait(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\n\tif (ioa_cfg->sdt_state != GET_DUMP)\n\t\treturn IPR_RC_JOB_RETURN;\n\n\tif (!ioa_cfg->sis64 || !ipr_cmd->u.time_left ||\n\t    (readl(ioa_cfg->regs.sense_interrupt_reg) &\n\t     IPR_PCII_MAILBOX_STABLE)) {\n\n\t\tif (!ipr_cmd->u.time_left)\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Timed out waiting for Mailbox register.\\n\");\n\n\t\tioa_cfg->sdt_state = READ_DUMP;\n\t\tioa_cfg->dump_timeout = 0;\n\t\tif (ioa_cfg->sis64)\n\t\t\tipr_reset_start_timer(ipr_cmd, IPR_SIS64_DUMP_TIMEOUT);\n\t\telse\n\t\t\tipr_reset_start_timer(ipr_cmd, IPR_SIS32_DUMP_TIMEOUT);\n\t\tipr_cmd->job_step = ipr_reset_wait_for_dump;\n\t\tschedule_work(&ioa_cfg->work_q);\n\n\t} else {\n\t\tipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\n\t\tipr_reset_start_timer(ipr_cmd,\n\t\t\t\t      IPR_CHECK_FOR_RESET_TIMEOUT);\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_restore_cfg_space(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tioa_cfg->pdev->state_saved = true;\n\tpci_restore_state(ioa_cfg->pdev);\n\n\tif (ipr_set_pcix_cmd_reg(ioa_cfg)) {\n\t\tipr_cmd->s.ioasa.hdr.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);\n\t\treturn IPR_RC_JOB_CONTINUE;\n\t}\n\n\tipr_fail_all_ops(ioa_cfg);\n\n\tif (ioa_cfg->sis64) {\n\t\t \n\t\twritel(IPR_ENDIAN_SWAP_KEY, ioa_cfg->regs.endian_swap_reg);\n\t\treadl(ioa_cfg->regs.endian_swap_reg);\n\t}\n\n\tif (ioa_cfg->ioa_unit_checked) {\n\t\tif (ioa_cfg->sis64) {\n\t\t\tipr_cmd->job_step = ipr_reset_get_unit_check_job;\n\t\t\tipr_reset_start_timer(ipr_cmd, IPR_DUMP_DELAY_TIMEOUT);\n\t\t\treturn IPR_RC_JOB_RETURN;\n\t\t} else {\n\t\t\tioa_cfg->ioa_unit_checked = 0;\n\t\t\tipr_get_unit_check_buffer(ioa_cfg);\n\t\t\tipr_cmd->job_step = ipr_reset_alert;\n\t\t\tipr_reset_start_timer(ipr_cmd, 0);\n\t\t\treturn IPR_RC_JOB_RETURN;\n\t\t}\n\t}\n\n\tif (ioa_cfg->in_ioa_bringdown) {\n\t\tipr_cmd->job_step = ipr_ioa_bringdown_done;\n\t} else if (ioa_cfg->sdt_state == GET_DUMP) {\n\t\tipr_cmd->job_step = ipr_dump_mailbox_wait;\n\t\tipr_cmd->u.time_left = IPR_WAIT_FOR_MAILBOX;\n\t} else {\n\t\tipr_cmd->job_step = ipr_reset_enable_ioa;\n\t}\n\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_reset_bist_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tif (ioa_cfg->cfg_locked)\n\t\tpci_cfg_access_unlock(ioa_cfg->pdev);\n\tioa_cfg->cfg_locked = 0;\n\tipr_cmd->job_step = ipr_reset_restore_cfg_space;\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_reset_start_bist(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint rc = PCIBIOS_SUCCESSFUL;\n\n\tENTER;\n\tif (ioa_cfg->ipr_chip->bist_method == IPR_MMIO)\n\t\twritel(IPR_UPROCI_SIS64_START_BIST,\n\t\t       ioa_cfg->regs.set_uproc_interrupt_reg32);\n\telse\n\t\trc = pci_write_config_byte(ioa_cfg->pdev, PCI_BIST, PCI_BIST_START);\n\n\tif (rc == PCIBIOS_SUCCESSFUL) {\n\t\tipr_cmd->job_step = ipr_reset_bist_done;\n\t\tipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);\n\t\trc = IPR_RC_JOB_RETURN;\n\t} else {\n\t\tif (ioa_cfg->cfg_locked)\n\t\t\tpci_cfg_access_unlock(ipr_cmd->ioa_cfg->pdev);\n\t\tioa_cfg->cfg_locked = 0;\n\t\tipr_cmd->s.ioasa.hdr.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);\n\t\trc = IPR_RC_JOB_CONTINUE;\n\t}\n\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic int ipr_reset_slot_reset_done(struct ipr_cmnd *ipr_cmd)\n{\n\tENTER;\n\tipr_cmd->job_step = ipr_reset_bist_done;\n\tipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic void ipr_reset_reset_work(struct work_struct *work)\n{\n\tstruct ipr_cmnd *ipr_cmd = container_of(work, struct ipr_cmnd, work);\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct pci_dev *pdev = ioa_cfg->pdev;\n\tunsigned long lock_flags = 0;\n\n\tENTER;\n\tpci_set_pcie_reset_state(pdev, pcie_warm_reset);\n\tmsleep(jiffies_to_msecs(IPR_PCI_RESET_TIMEOUT));\n\tpci_set_pcie_reset_state(pdev, pcie_deassert_reset);\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ioa_cfg->reset_cmd == ipr_cmd)\n\t\tipr_reset_ioa_job(ipr_cmd);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\tLEAVE;\n}\n\n \nstatic int ipr_reset_slot_reset(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tINIT_WORK(&ipr_cmd->work, ipr_reset_reset_work);\n\tqueue_work(ioa_cfg->reset_work_q, &ipr_cmd->work);\n\tipr_cmd->job_step = ipr_reset_slot_reset_done;\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_block_config_access_wait(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint rc = IPR_RC_JOB_CONTINUE;\n\n\tif (pci_cfg_access_trylock(ioa_cfg->pdev)) {\n\t\tioa_cfg->cfg_locked = 1;\n\t\tipr_cmd->job_step = ioa_cfg->reset;\n\t} else {\n\t\tif (ipr_cmd->u.time_left) {\n\t\t\trc = IPR_RC_JOB_RETURN;\n\t\t\tipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\n\t\t\tipr_reset_start_timer(ipr_cmd,\n\t\t\t\t\t      IPR_CHECK_FOR_RESET_TIMEOUT);\n\t\t} else {\n\t\t\tipr_cmd->job_step = ioa_cfg->reset;\n\t\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\t\"Timed out waiting to lock config access. Resetting anyway.\\n\");\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n \nstatic int ipr_reset_block_config_access(struct ipr_cmnd *ipr_cmd)\n{\n\tipr_cmd->ioa_cfg->cfg_locked = 0;\n\tipr_cmd->job_step = ipr_reset_block_config_access_wait;\n\tipr_cmd->u.time_left = IPR_WAIT_FOR_RESET_TIMEOUT;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_reset_allowed(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tvolatile u32 temp_reg;\n\n\ttemp_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\n\treturn ((temp_reg & IPR_PCII_CRITICAL_OPERATION) == 0);\n}\n\n \nstatic int ipr_reset_wait_to_start_bist(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint rc = IPR_RC_JOB_RETURN;\n\n\tif (!ipr_reset_allowed(ioa_cfg) && ipr_cmd->u.time_left) {\n\t\tipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\n\t\tipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);\n\t} else {\n\t\tipr_cmd->job_step = ipr_reset_block_config_access;\n\t\trc = IPR_RC_JOB_CONTINUE;\n\t}\n\n\treturn rc;\n}\n\n \nstatic int ipr_reset_alert(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tu16 cmd_reg;\n\tint rc;\n\n\tENTER;\n\trc = pci_read_config_word(ioa_cfg->pdev, PCI_COMMAND, &cmd_reg);\n\n\tif ((rc == PCIBIOS_SUCCESSFUL) && (cmd_reg & PCI_COMMAND_MEMORY)) {\n\t\tipr_mask_and_clear_interrupts(ioa_cfg, ~0);\n\t\twritel(IPR_UPROCI_RESET_ALERT, ioa_cfg->regs.set_uproc_interrupt_reg32);\n\t\tipr_cmd->job_step = ipr_reset_wait_to_start_bist;\n\t} else {\n\t\tipr_cmd->job_step = ipr_reset_block_config_access;\n\t}\n\n\tipr_cmd->u.time_left = IPR_WAIT_FOR_RESET_TIMEOUT;\n\tipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_quiesce_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_ioa_bringdown_done;\n\tipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\n\tLEAVE;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_reset_cancel_hcam_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_cmnd *loop_cmd;\n\tstruct ipr_hrr_queue *hrrq;\n\tint rc = IPR_RC_JOB_CONTINUE;\n\tint count = 0;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_reset_quiesce_done;\n\n\tfor_each_hrrq(hrrq, ioa_cfg) {\n\t\tspin_lock(&hrrq->_lock);\n\t\tlist_for_each_entry(loop_cmd, &hrrq->hrrq_pending_q, queue) {\n\t\t\tcount++;\n\t\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t\t\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\t\t\trc = IPR_RC_JOB_RETURN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&hrrq->_lock);\n\n\t\tif (count)\n\t\t\tbreak;\n\t}\n\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic int ipr_reset_cancel_hcam(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint rc = IPR_RC_JOB_CONTINUE;\n\tstruct ipr_cmd_pkt *cmd_pkt;\n\tstruct ipr_cmnd *hcam_cmd;\n\tstruct ipr_hrr_queue *hrrq = &ioa_cfg->hrrq[IPR_INIT_HRRQ];\n\n\tENTER;\n\tipr_cmd->job_step = ipr_reset_cancel_hcam_done;\n\n\tif (!hrrq->ioa_is_dead) {\n\t\tif (!list_empty(&ioa_cfg->hostrcb_pending_q)) {\n\t\t\tlist_for_each_entry(hcam_cmd, &hrrq->hrrq_pending_q, queue) {\n\t\t\t\tif (hcam_cmd->ioarcb.cmd_pkt.cdb[0] != IPR_HOST_CONTROLLED_ASYNC)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\t\t\t\tipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\t\t\t\tcmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\n\t\t\t\tcmd_pkt->request_type = IPR_RQTYPE_IOACMD;\n\t\t\t\tcmd_pkt->cdb[0] = IPR_CANCEL_REQUEST;\n\t\t\t\tcmd_pkt->cdb[1] = IPR_CANCEL_64BIT_IOARCB;\n\t\t\t\tcmd_pkt->cdb[10] = ((u64) hcam_cmd->dma_addr >> 56) & 0xff;\n\t\t\t\tcmd_pkt->cdb[11] = ((u64) hcam_cmd->dma_addr >> 48) & 0xff;\n\t\t\t\tcmd_pkt->cdb[12] = ((u64) hcam_cmd->dma_addr >> 40) & 0xff;\n\t\t\t\tcmd_pkt->cdb[13] = ((u64) hcam_cmd->dma_addr >> 32) & 0xff;\n\t\t\t\tcmd_pkt->cdb[2] = ((u64) hcam_cmd->dma_addr >> 24) & 0xff;\n\t\t\t\tcmd_pkt->cdb[3] = ((u64) hcam_cmd->dma_addr >> 16) & 0xff;\n\t\t\t\tcmd_pkt->cdb[4] = ((u64) hcam_cmd->dma_addr >> 8) & 0xff;\n\t\t\t\tcmd_pkt->cdb[5] = ((u64) hcam_cmd->dma_addr) & 0xff;\n\n\t\t\t\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\n\t\t\t\t\t   IPR_CANCEL_TIMEOUT);\n\n\t\t\t\trc = IPR_RC_JOB_RETURN;\n\t\t\t\tipr_cmd->job_step = ipr_reset_cancel_hcam;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} else\n\t\tipr_cmd->job_step = ipr_reset_alert;\n\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic int ipr_reset_ucode_download_done(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_sglist *sglist = ioa_cfg->ucode_sglist;\n\n\tdma_unmap_sg(&ioa_cfg->pdev->dev, sglist->scatterlist,\n\t\t     sglist->num_sg, DMA_TO_DEVICE);\n\n\tipr_cmd->job_step = ipr_reset_alert;\n\treturn IPR_RC_JOB_CONTINUE;\n}\n\n \nstatic int ipr_reset_ucode_download(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tstruct ipr_sglist *sglist = ioa_cfg->ucode_sglist;\n\n\tENTER;\n\tipr_cmd->job_step = ipr_reset_alert;\n\n\tif (!sglist)\n\t\treturn IPR_RC_JOB_CONTINUE;\n\n\tipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\tipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\n\tipr_cmd->ioarcb.cmd_pkt.cdb[0] = WRITE_BUFFER;\n\tipr_cmd->ioarcb.cmd_pkt.cdb[1] = IPR_WR_BUF_DOWNLOAD_AND_SAVE;\n\tipr_cmd->ioarcb.cmd_pkt.cdb[6] = (sglist->buffer_len & 0xff0000) >> 16;\n\tipr_cmd->ioarcb.cmd_pkt.cdb[7] = (sglist->buffer_len & 0x00ff00) >> 8;\n\tipr_cmd->ioarcb.cmd_pkt.cdb[8] = sglist->buffer_len & 0x0000ff;\n\n\tif (ioa_cfg->sis64)\n\t\tipr_build_ucode_ioadl64(ipr_cmd, sglist);\n\telse\n\t\tipr_build_ucode_ioadl(ipr_cmd, sglist);\n\tipr_cmd->job_step = ipr_reset_ucode_download_done;\n\n\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\n\t\t   IPR_WRITE_BUFFER_TIMEOUT);\n\n\tLEAVE;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic int ipr_reset_shutdown_ioa(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tenum ipr_shutdown_type shutdown_type = ipr_cmd->u.shutdown_type;\n\tunsigned long timeout;\n\tint rc = IPR_RC_JOB_CONTINUE;\n\n\tENTER;\n\tif (shutdown_type == IPR_SHUTDOWN_QUIESCE)\n\t\tipr_cmd->job_step = ipr_reset_cancel_hcam;\n\telse if (shutdown_type != IPR_SHUTDOWN_NONE &&\n\t\t\t!ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\n\t\tipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\t\tipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\t\tipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;\n\t\tipr_cmd->ioarcb.cmd_pkt.cdb[1] = shutdown_type;\n\n\t\tif (shutdown_type == IPR_SHUTDOWN_NORMAL)\n\t\t\ttimeout = IPR_SHUTDOWN_TIMEOUT;\n\t\telse if (shutdown_type == IPR_SHUTDOWN_PREPARE_FOR_NORMAL)\n\t\t\ttimeout = IPR_INTERNAL_TIMEOUT;\n\t\telse if (ioa_cfg->dual_raid && ipr_dual_ioa_raid)\n\t\t\ttimeout = IPR_DUAL_IOA_ABBR_SHUTDOWN_TO;\n\t\telse\n\t\t\ttimeout = IPR_ABBREV_SHUTDOWN_TIMEOUT;\n\n\t\tipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, timeout);\n\n\t\trc = IPR_RC_JOB_RETURN;\n\t\tipr_cmd->job_step = ipr_reset_ucode_download;\n\t} else\n\t\tipr_cmd->job_step = ipr_reset_alert;\n\n\tLEAVE;\n\treturn rc;\n}\n\n \nstatic void ipr_reset_ioa_job(struct ipr_cmnd *ipr_cmd)\n{\n\tu32 rc, ioasc;\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\n\tdo {\n\t\tioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\n\n\t\tif (ioa_cfg->reset_cmd != ipr_cmd) {\n\t\t\t \n\t\t\tlist_add_tail(&ipr_cmd->queue,\n\t\t\t\t\t&ipr_cmd->hrrq->hrrq_free_q);\n\t\t\treturn;\n\t\t}\n\n\t\tif (IPR_IOASC_SENSE_KEY(ioasc)) {\n\t\t\trc = ipr_cmd->job_step_failed(ipr_cmd);\n\t\t\tif (rc == IPR_RC_JOB_RETURN)\n\t\t\t\treturn;\n\t\t}\n\n\t\tipr_reinit_ipr_cmnd(ipr_cmd);\n\t\tipr_cmd->job_step_failed = ipr_reset_cmd_failed;\n\t\trc = ipr_cmd->job_step(ipr_cmd);\n\t} while (rc == IPR_RC_JOB_CONTINUE);\n}\n\n \nstatic void _ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t    int (*job_step) (struct ipr_cmnd *),\n\t\t\t\t    enum ipr_shutdown_type shutdown_type)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tint i;\n\n\tioa_cfg->in_reset_reload = 1;\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].allow_cmds = 0;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\twmb();\n\tif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\n\t\tioa_cfg->scsi_unblock = 0;\n\t\tioa_cfg->scsi_blocked = 1;\n\t\tscsi_block_requests(ioa_cfg->host);\n\t}\n\n\tipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\tioa_cfg->reset_cmd = ipr_cmd;\n\tipr_cmd->job_step = job_step;\n\tipr_cmd->u.shutdown_type = shutdown_type;\n\n\tipr_reset_ioa_job(ipr_cmd);\n}\n\n \nstatic void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t   enum ipr_shutdown_type shutdown_type)\n{\n\tint i;\n\n\tif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\n\t\treturn;\n\n\tif (ioa_cfg->in_reset_reload) {\n\t\tif (ioa_cfg->sdt_state == GET_DUMP)\n\t\t\tioa_cfg->sdt_state = WAIT_FOR_DUMP;\n\t\telse if (ioa_cfg->sdt_state == READ_DUMP)\n\t\t\tioa_cfg->sdt_state = ABORT_DUMP;\n\t}\n\n\tif (ioa_cfg->reset_retries++ >= IPR_NUM_RESET_RELOAD_RETRIES) {\n\t\tdev_err(&ioa_cfg->pdev->dev,\n\t\t\t\"IOA taken offline - error recovery failed\\n\");\n\n\t\tioa_cfg->reset_retries = 0;\n\t\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\t\tioa_cfg->hrrq[i].ioa_is_dead = 1;\n\t\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t\t}\n\t\twmb();\n\n\t\tif (ioa_cfg->in_ioa_bringdown) {\n\t\t\tioa_cfg->reset_cmd = NULL;\n\t\t\tioa_cfg->in_reset_reload = 0;\n\t\t\tipr_fail_all_ops(ioa_cfg);\n\t\t\twake_up_all(&ioa_cfg->reset_wait_q);\n\n\t\t\tif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\n\t\t\t\tioa_cfg->scsi_unblock = 1;\n\t\t\t\tschedule_work(&ioa_cfg->work_q);\n\t\t\t}\n\t\t\treturn;\n\t\t} else {\n\t\t\tioa_cfg->in_ioa_bringdown = 1;\n\t\t\tshutdown_type = IPR_SHUTDOWN_NONE;\n\t\t}\n\t}\n\n\t_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_shutdown_ioa,\n\t\t\t\tshutdown_type);\n}\n\n \nstatic int ipr_reset_freeze(struct ipr_cmnd *ipr_cmd)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\n\tint i;\n\n\t \n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].allow_interrupts = 0;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\twmb();\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\n\tipr_cmd->done = ipr_reset_ioa_job;\n\treturn IPR_RC_JOB_RETURN;\n}\n\n \nstatic pci_ers_result_t ipr_pci_mmio_enabled(struct pci_dev *pdev)\n{\n\tunsigned long flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\tif (!ioa_cfg->probe_done)\n\t\tpci_save_state(pdev);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic void ipr_pci_frozen(struct pci_dev *pdev)\n{\n\tunsigned long flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\tif (ioa_cfg->probe_done)\n\t\t_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_freeze, IPR_SHUTDOWN_NONE);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n}\n\n \nstatic pci_ers_result_t ipr_pci_slot_reset(struct pci_dev *pdev)\n{\n\tunsigned long flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\tif (ioa_cfg->probe_done) {\n\t\tif (ioa_cfg->needs_warm_reset)\n\t\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t\telse\n\t\t\t_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_restore_cfg_space,\n\t\t\t\t\t\tIPR_SHUTDOWN_NONE);\n\t} else\n\t\twake_up_all(&ioa_cfg->eeh_wait_q);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\n \nstatic void ipr_pci_perm_failure(struct pci_dev *pdev)\n{\n\tunsigned long flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\tint i;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\tif (ioa_cfg->probe_done) {\n\t\tif (ioa_cfg->sdt_state == WAIT_FOR_DUMP)\n\t\t\tioa_cfg->sdt_state = ABORT_DUMP;\n\t\tioa_cfg->reset_retries = IPR_NUM_RESET_RELOAD_RETRIES - 1;\n\t\tioa_cfg->in_ioa_bringdown = 1;\n\t\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\t\tioa_cfg->hrrq[i].allow_cmds = 0;\n\t\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t\t}\n\t\twmb();\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t} else\n\t\twake_up_all(&ioa_cfg->eeh_wait_q);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n}\n\n \nstatic pci_ers_result_t ipr_pci_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t       pci_channel_state_t state)\n{\n\tswitch (state) {\n\tcase pci_channel_io_frozen:\n\t\tipr_pci_frozen(pdev);\n\t\treturn PCI_ERS_RESULT_CAN_RECOVER;\n\tcase pci_channel_io_perm_failure:\n\t\tipr_pci_perm_failure(pdev);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic void ipr_probe_ioa_part2(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tunsigned long host_lock_flags = 0;\n\n\tENTER;\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\n\tdev_dbg(&ioa_cfg->pdev->dev, \"ioa_cfg adx: 0x%p\\n\", ioa_cfg);\n\tioa_cfg->probe_done = 1;\n\tif (ioa_cfg->needs_hard_reset) {\n\t\tioa_cfg->needs_hard_reset = 0;\n\t\tipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\n\t} else\n\t\t_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_enable_ioa,\n\t\t\t\t\tIPR_SHUTDOWN_NONE);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\n\n\tLEAVE;\n}\n\n \nstatic void ipr_free_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint i;\n\n\tif (ioa_cfg->ipr_cmnd_list) {\n\t\tfor (i = 0; i < IPR_NUM_CMD_BLKS; i++) {\n\t\t\tif (ioa_cfg->ipr_cmnd_list[i])\n\t\t\t\tdma_pool_free(ioa_cfg->ipr_cmd_pool,\n\t\t\t\t\t      ioa_cfg->ipr_cmnd_list[i],\n\t\t\t\t\t      ioa_cfg->ipr_cmnd_list_dma[i]);\n\n\t\t\tioa_cfg->ipr_cmnd_list[i] = NULL;\n\t\t}\n\t}\n\n\tdma_pool_destroy(ioa_cfg->ipr_cmd_pool);\n\n\tkfree(ioa_cfg->ipr_cmnd_list);\n\tkfree(ioa_cfg->ipr_cmnd_list_dma);\n\tioa_cfg->ipr_cmnd_list = NULL;\n\tioa_cfg->ipr_cmnd_list_dma = NULL;\n\tioa_cfg->ipr_cmd_pool = NULL;\n}\n\n \nstatic void ipr_free_mem(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint i;\n\n\tkfree(ioa_cfg->res_entries);\n\tdma_free_coherent(&ioa_cfg->pdev->dev, sizeof(struct ipr_misc_cbs),\n\t\t\t  ioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);\n\tipr_free_cmd_blks(ioa_cfg);\n\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++)\n\t\tdma_free_coherent(&ioa_cfg->pdev->dev,\n\t\t\t\t  sizeof(u32) * ioa_cfg->hrrq[i].size,\n\t\t\t\t  ioa_cfg->hrrq[i].host_rrq,\n\t\t\t\t  ioa_cfg->hrrq[i].host_rrq_dma);\n\n\tdma_free_coherent(&ioa_cfg->pdev->dev, ioa_cfg->cfg_table_size,\n\t\t\t  ioa_cfg->u.cfg_table, ioa_cfg->cfg_table_dma);\n\n\tfor (i = 0; i < IPR_MAX_HCAMS; i++) {\n\t\tdma_free_coherent(&ioa_cfg->pdev->dev,\n\t\t\t\t  sizeof(struct ipr_hostrcb),\n\t\t\t\t  ioa_cfg->hostrcb[i],\n\t\t\t\t  ioa_cfg->hostrcb_dma[i]);\n\t}\n\n\tipr_free_dump(ioa_cfg);\n\tkfree(ioa_cfg->trace);\n}\n\n \nstatic void ipr_free_irqs(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct pci_dev *pdev = ioa_cfg->pdev;\n\tint i;\n\n\tfor (i = 0; i < ioa_cfg->nvectors; i++)\n\t\tfree_irq(pci_irq_vector(pdev, i), &ioa_cfg->hrrq[i]);\n\tpci_free_irq_vectors(pdev);\n}\n\n \nstatic void ipr_free_all_resources(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct pci_dev *pdev = ioa_cfg->pdev;\n\n\tENTER;\n\tipr_free_irqs(ioa_cfg);\n\tif (ioa_cfg->reset_work_q)\n\t\tdestroy_workqueue(ioa_cfg->reset_work_q);\n\tiounmap(ioa_cfg->hdw_dma_regs);\n\tpci_release_regions(pdev);\n\tipr_free_mem(ioa_cfg);\n\tscsi_host_put(ioa_cfg->host);\n\tpci_disable_device(pdev);\n\tLEAVE;\n}\n\n \nstatic int ipr_alloc_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioarcb *ioarcb;\n\tdma_addr_t dma_addr;\n\tint i, entries_each_hrrq, hrrq_id = 0;\n\n\tioa_cfg->ipr_cmd_pool = dma_pool_create(IPR_NAME, &ioa_cfg->pdev->dev,\n\t\t\t\t\t\tsizeof(struct ipr_cmnd), 512, 0);\n\n\tif (!ioa_cfg->ipr_cmd_pool)\n\t\treturn -ENOMEM;\n\n\tioa_cfg->ipr_cmnd_list = kcalloc(IPR_NUM_CMD_BLKS, sizeof(struct ipr_cmnd *), GFP_KERNEL);\n\tioa_cfg->ipr_cmnd_list_dma = kcalloc(IPR_NUM_CMD_BLKS, sizeof(dma_addr_t), GFP_KERNEL);\n\n\tif (!ioa_cfg->ipr_cmnd_list || !ioa_cfg->ipr_cmnd_list_dma) {\n\t\tipr_free_cmd_blks(ioa_cfg);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tif (ioa_cfg->hrrq_num > 1) {\n\t\t\tif (i == 0) {\n\t\t\t\tentries_each_hrrq = IPR_NUM_INTERNAL_CMD_BLKS;\n\t\t\t\tioa_cfg->hrrq[i].min_cmd_id = 0;\n\t\t\t\tioa_cfg->hrrq[i].max_cmd_id =\n\t\t\t\t\t(entries_each_hrrq - 1);\n\t\t\t} else {\n\t\t\t\tentries_each_hrrq =\n\t\t\t\t\tIPR_NUM_BASE_CMD_BLKS/\n\t\t\t\t\t(ioa_cfg->hrrq_num - 1);\n\t\t\t\tioa_cfg->hrrq[i].min_cmd_id =\n\t\t\t\t\tIPR_NUM_INTERNAL_CMD_BLKS +\n\t\t\t\t\t(i - 1) * entries_each_hrrq;\n\t\t\t\tioa_cfg->hrrq[i].max_cmd_id =\n\t\t\t\t\t(IPR_NUM_INTERNAL_CMD_BLKS +\n\t\t\t\t\ti * entries_each_hrrq - 1);\n\t\t\t}\n\t\t} else {\n\t\t\tentries_each_hrrq = IPR_NUM_CMD_BLKS;\n\t\t\tioa_cfg->hrrq[i].min_cmd_id = 0;\n\t\t\tioa_cfg->hrrq[i].max_cmd_id = (entries_each_hrrq - 1);\n\t\t}\n\t\tioa_cfg->hrrq[i].size = entries_each_hrrq;\n\t}\n\n\tBUG_ON(ioa_cfg->hrrq_num == 0);\n\n\ti = IPR_NUM_CMD_BLKS -\n\t\tioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id - 1;\n\tif (i > 0) {\n\t\tioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].size += i;\n\t\tioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id += i;\n\t}\n\n\tfor (i = 0; i < IPR_NUM_CMD_BLKS; i++) {\n\t\tipr_cmd = dma_pool_zalloc(ioa_cfg->ipr_cmd_pool,\n\t\t\t\tGFP_KERNEL, &dma_addr);\n\n\t\tif (!ipr_cmd) {\n\t\t\tipr_free_cmd_blks(ioa_cfg);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tioa_cfg->ipr_cmnd_list[i] = ipr_cmd;\n\t\tioa_cfg->ipr_cmnd_list_dma[i] = dma_addr;\n\n\t\tioarcb = &ipr_cmd->ioarcb;\n\t\tipr_cmd->dma_addr = dma_addr;\n\t\tif (ioa_cfg->sis64)\n\t\t\tioarcb->a.ioarcb_host_pci_addr64 = cpu_to_be64(dma_addr);\n\t\telse\n\t\t\tioarcb->a.ioarcb_host_pci_addr = cpu_to_be32(dma_addr);\n\n\t\tioarcb->host_response_handle = cpu_to_be32(i << 2);\n\t\tif (ioa_cfg->sis64) {\n\t\t\tioarcb->u.sis64_addr_data.data_ioadl_addr =\n\t\t\t\tcpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\n\t\t\tioarcb->u.sis64_addr_data.ioasa_host_pci_addr =\n\t\t\t\tcpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, s.ioasa64));\n\t\t} else {\n\t\t\tioarcb->write_ioadl_addr =\n\t\t\t\tcpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\n\t\t\tioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\n\t\t\tioarcb->ioasa_host_pci_addr =\n\t\t\t\tcpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, s.ioasa));\n\t\t}\n\t\tioarcb->ioasa_len = cpu_to_be16(sizeof(struct ipr_ioasa));\n\t\tipr_cmd->cmd_index = i;\n\t\tipr_cmd->ioa_cfg = ioa_cfg;\n\t\tipr_cmd->sense_buffer_dma = dma_addr +\n\t\t\toffsetof(struct ipr_cmnd, sense_buffer);\n\n\t\tipr_cmd->ioarcb.cmd_pkt.hrrq_id = hrrq_id;\n\t\tipr_cmd->hrrq = &ioa_cfg->hrrq[hrrq_id];\n\t\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n\t\tif (i >= ioa_cfg->hrrq[hrrq_id].max_cmd_id)\n\t\t\thrrq_id++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ipr_alloc_mem(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct pci_dev *pdev = ioa_cfg->pdev;\n\tint i, rc = -ENOMEM;\n\n\tENTER;\n\tioa_cfg->res_entries = kcalloc(ioa_cfg->max_devs_supported,\n\t\t\t\t       sizeof(struct ipr_resource_entry),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ioa_cfg->res_entries)\n\t\tgoto out;\n\n\tfor (i = 0; i < ioa_cfg->max_devs_supported; i++) {\n\t\tlist_add_tail(&ioa_cfg->res_entries[i].queue, &ioa_cfg->free_res_q);\n\t\tioa_cfg->res_entries[i].ioa_cfg = ioa_cfg;\n\t}\n\n\tioa_cfg->vpd_cbs = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t      sizeof(struct ipr_misc_cbs),\n\t\t\t\t\t      &ioa_cfg->vpd_cbs_dma,\n\t\t\t\t\t      GFP_KERNEL);\n\n\tif (!ioa_cfg->vpd_cbs)\n\t\tgoto out_free_res_entries;\n\n\tif (ipr_alloc_cmd_blks(ioa_cfg))\n\t\tgoto out_free_vpd_cbs;\n\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tioa_cfg->hrrq[i].host_rrq = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\tsizeof(u32) * ioa_cfg->hrrq[i].size,\n\t\t\t\t\t&ioa_cfg->hrrq[i].host_rrq_dma,\n\t\t\t\t\tGFP_KERNEL);\n\n\t\tif (!ioa_cfg->hrrq[i].host_rrq)  {\n\t\t\twhile (--i >= 0)\n\t\t\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t\tsizeof(u32) * ioa_cfg->hrrq[i].size,\n\t\t\t\t\tioa_cfg->hrrq[i].host_rrq,\n\t\t\t\t\tioa_cfg->hrrq[i].host_rrq_dma);\n\t\t\tgoto out_ipr_free_cmd_blocks;\n\t\t}\n\t\tioa_cfg->hrrq[i].ioa_cfg = ioa_cfg;\n\t}\n\n\tioa_cfg->u.cfg_table = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\t  ioa_cfg->cfg_table_size,\n\t\t\t\t\t\t  &ioa_cfg->cfg_table_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\n\tif (!ioa_cfg->u.cfg_table)\n\t\tgoto out_free_host_rrq;\n\n\tfor (i = 0; i < IPR_MAX_HCAMS; i++) {\n\t\tioa_cfg->hostrcb[i] = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\t\t sizeof(struct ipr_hostrcb),\n\t\t\t\t\t\t\t &ioa_cfg->hostrcb_dma[i],\n\t\t\t\t\t\t\t GFP_KERNEL);\n\n\t\tif (!ioa_cfg->hostrcb[i])\n\t\t\tgoto out_free_hostrcb_dma;\n\n\t\tioa_cfg->hostrcb[i]->hostrcb_dma =\n\t\t\tioa_cfg->hostrcb_dma[i] + offsetof(struct ipr_hostrcb, hcam);\n\t\tioa_cfg->hostrcb[i]->ioa_cfg = ioa_cfg;\n\t\tlist_add_tail(&ioa_cfg->hostrcb[i]->queue, &ioa_cfg->hostrcb_free_q);\n\t}\n\n\tioa_cfg->trace = kcalloc(IPR_NUM_TRACE_ENTRIES,\n\t\t\t\t sizeof(struct ipr_trace_entry),\n\t\t\t\t GFP_KERNEL);\n\n\tif (!ioa_cfg->trace)\n\t\tgoto out_free_hostrcb_dma;\n\n\trc = 0;\nout:\n\tLEAVE;\n\treturn rc;\n\nout_free_hostrcb_dma:\n\twhile (i-- > 0) {\n\t\tdma_free_coherent(&pdev->dev, sizeof(struct ipr_hostrcb),\n\t\t\t\t  ioa_cfg->hostrcb[i],\n\t\t\t\t  ioa_cfg->hostrcb_dma[i]);\n\t}\n\tdma_free_coherent(&pdev->dev, ioa_cfg->cfg_table_size,\n\t\t\t  ioa_cfg->u.cfg_table, ioa_cfg->cfg_table_dma);\nout_free_host_rrq:\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t  sizeof(u32) * ioa_cfg->hrrq[i].size,\n\t\t\t\t  ioa_cfg->hrrq[i].host_rrq,\n\t\t\t\t  ioa_cfg->hrrq[i].host_rrq_dma);\n\t}\nout_ipr_free_cmd_blocks:\n\tipr_free_cmd_blks(ioa_cfg);\nout_free_vpd_cbs:\n\tdma_free_coherent(&pdev->dev, sizeof(struct ipr_misc_cbs),\n\t\t\t  ioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);\nout_free_res_entries:\n\tkfree(ioa_cfg->res_entries);\n\tgoto out;\n}\n\n \nstatic void ipr_initialize_bus_attr(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint i;\n\n\tfor (i = 0; i < IPR_MAX_NUM_BUSES; i++) {\n\t\tioa_cfg->bus_attr[i].bus = i;\n\t\tioa_cfg->bus_attr[i].qas_enabled = 0;\n\t\tioa_cfg->bus_attr[i].bus_width = IPR_DEFAULT_BUS_WIDTH;\n\t\tif (ipr_max_speed < ARRAY_SIZE(ipr_max_bus_speeds))\n\t\t\tioa_cfg->bus_attr[i].max_xfer_rate = ipr_max_bus_speeds[ipr_max_speed];\n\t\telse\n\t\t\tioa_cfg->bus_attr[i].max_xfer_rate = IPR_U160_SCSI_RATE;\n\t}\n}\n\n \nstatic void ipr_init_regs(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tconst struct ipr_interrupt_offsets *p;\n\tstruct ipr_interrupts *t;\n\tvoid __iomem *base;\n\n\tp = &ioa_cfg->chip_cfg->regs;\n\tt = &ioa_cfg->regs;\n\tbase = ioa_cfg->hdw_dma_regs;\n\n\tt->set_interrupt_mask_reg = base + p->set_interrupt_mask_reg;\n\tt->clr_interrupt_mask_reg = base + p->clr_interrupt_mask_reg;\n\tt->clr_interrupt_mask_reg32 = base + p->clr_interrupt_mask_reg32;\n\tt->sense_interrupt_mask_reg = base + p->sense_interrupt_mask_reg;\n\tt->sense_interrupt_mask_reg32 = base + p->sense_interrupt_mask_reg32;\n\tt->clr_interrupt_reg = base + p->clr_interrupt_reg;\n\tt->clr_interrupt_reg32 = base + p->clr_interrupt_reg32;\n\tt->sense_interrupt_reg = base + p->sense_interrupt_reg;\n\tt->sense_interrupt_reg32 = base + p->sense_interrupt_reg32;\n\tt->ioarrin_reg = base + p->ioarrin_reg;\n\tt->sense_uproc_interrupt_reg = base + p->sense_uproc_interrupt_reg;\n\tt->sense_uproc_interrupt_reg32 = base + p->sense_uproc_interrupt_reg32;\n\tt->set_uproc_interrupt_reg = base + p->set_uproc_interrupt_reg;\n\tt->set_uproc_interrupt_reg32 = base + p->set_uproc_interrupt_reg32;\n\tt->clr_uproc_interrupt_reg = base + p->clr_uproc_interrupt_reg;\n\tt->clr_uproc_interrupt_reg32 = base + p->clr_uproc_interrupt_reg32;\n\n\tif (ioa_cfg->sis64) {\n\t\tt->init_feedback_reg = base + p->init_feedback_reg;\n\t\tt->dump_addr_reg = base + p->dump_addr_reg;\n\t\tt->dump_data_reg = base + p->dump_data_reg;\n\t\tt->endian_swap_reg = base + p->endian_swap_reg;\n\t}\n}\n\n \nstatic void ipr_init_ioa_cfg(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t     struct Scsi_Host *host, struct pci_dev *pdev)\n{\n\tint i;\n\n\tioa_cfg->host = host;\n\tioa_cfg->pdev = pdev;\n\tioa_cfg->log_level = ipr_log_level;\n\tioa_cfg->doorbell = IPR_DOORBELL;\n\tsprintf(ioa_cfg->eye_catcher, IPR_EYECATCHER);\n\tsprintf(ioa_cfg->trace_start, IPR_TRACE_START_LABEL);\n\tsprintf(ioa_cfg->cfg_table_start, IPR_CFG_TBL_START);\n\tsprintf(ioa_cfg->resource_table_label, IPR_RES_TABLE_LABEL);\n\tsprintf(ioa_cfg->ipr_hcam_label, IPR_HCAM_LABEL);\n\tsprintf(ioa_cfg->ipr_cmd_label, IPR_CMD_LABEL);\n\n\tINIT_LIST_HEAD(&ioa_cfg->hostrcb_free_q);\n\tINIT_LIST_HEAD(&ioa_cfg->hostrcb_pending_q);\n\tINIT_LIST_HEAD(&ioa_cfg->hostrcb_report_q);\n\tINIT_LIST_HEAD(&ioa_cfg->free_res_q);\n\tINIT_LIST_HEAD(&ioa_cfg->used_res_q);\n\tINIT_WORK(&ioa_cfg->work_q, ipr_worker_thread);\n\tINIT_WORK(&ioa_cfg->scsi_add_work_q, ipr_add_remove_thread);\n\tinit_waitqueue_head(&ioa_cfg->reset_wait_q);\n\tinit_waitqueue_head(&ioa_cfg->msi_wait_q);\n\tinit_waitqueue_head(&ioa_cfg->eeh_wait_q);\n\tioa_cfg->sdt_state = INACTIVE;\n\n\tipr_initialize_bus_attr(ioa_cfg);\n\tioa_cfg->max_devs_supported = ipr_max_devs;\n\n\tif (ioa_cfg->sis64) {\n\t\thost->max_channel = IPR_MAX_SIS64_BUSES;\n\t\thost->max_id = IPR_MAX_SIS64_TARGETS_PER_BUS;\n\t\thost->max_lun = IPR_MAX_SIS64_LUNS_PER_TARGET;\n\t\tif (ipr_max_devs > IPR_MAX_SIS64_DEVS)\n\t\t\tioa_cfg->max_devs_supported = IPR_MAX_SIS64_DEVS;\n\t\tioa_cfg->cfg_table_size = (sizeof(struct ipr_config_table_hdr64)\n\t\t\t\t\t   + ((sizeof(struct ipr_config_table_entry64)\n\t\t\t\t\t       * ioa_cfg->max_devs_supported)));\n\t} else {\n\t\thost->max_channel = IPR_VSET_BUS;\n\t\thost->max_id = IPR_MAX_NUM_TARGETS_PER_BUS;\n\t\thost->max_lun = IPR_MAX_NUM_LUNS_PER_TARGET;\n\t\tif (ipr_max_devs > IPR_MAX_PHYSICAL_DEVS)\n\t\t\tioa_cfg->max_devs_supported = IPR_MAX_PHYSICAL_DEVS;\n\t\tioa_cfg->cfg_table_size = (sizeof(struct ipr_config_table_hdr)\n\t\t\t\t\t   + ((sizeof(struct ipr_config_table_entry)\n\t\t\t\t\t       * ioa_cfg->max_devs_supported)));\n\t}\n\n\thost->unique_id = host->host_no;\n\thost->max_cmd_len = IPR_MAX_CDB_LEN;\n\thost->can_queue = ioa_cfg->max_cmds;\n\tpci_set_drvdata(pdev, ioa_cfg);\n\n\tfor (i = 0; i < ARRAY_SIZE(ioa_cfg->hrrq); i++) {\n\t\tINIT_LIST_HEAD(&ioa_cfg->hrrq[i].hrrq_free_q);\n\t\tINIT_LIST_HEAD(&ioa_cfg->hrrq[i].hrrq_pending_q);\n\t\tspin_lock_init(&ioa_cfg->hrrq[i]._lock);\n\t\tif (i == 0)\n\t\t\tioa_cfg->hrrq[i].lock = ioa_cfg->host->host_lock;\n\t\telse\n\t\t\tioa_cfg->hrrq[i].lock = &ioa_cfg->hrrq[i]._lock;\n\t}\n}\n\n \nstatic const struct ipr_chip_t *\nipr_get_chip_info(const struct pci_device_id *dev_id)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ipr_chip); i++)\n\t\tif (ipr_chip[i].vendor == dev_id->vendor &&\n\t\t    ipr_chip[i].device == dev_id->device)\n\t\t\treturn &ipr_chip[i];\n\treturn NULL;\n}\n\n \nstatic void ipr_wait_for_pci_err_recovery(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tstruct pci_dev *pdev = ioa_cfg->pdev;\n\n\tif (pci_channel_offline(pdev)) {\n\t\twait_event_timeout(ioa_cfg->eeh_wait_q,\n\t\t\t\t   !pci_channel_offline(pdev),\n\t\t\t\t   IPR_PCI_ERROR_RECOVERY_TIMEOUT);\n\t\tpci_restore_state(pdev);\n\t}\n}\n\nstatic void name_msi_vectors(struct ipr_ioa_cfg *ioa_cfg)\n{\n\tint vec_idx, n = sizeof(ioa_cfg->vectors_info[0].desc) - 1;\n\n\tfor (vec_idx = 0; vec_idx < ioa_cfg->nvectors; vec_idx++) {\n\t\tsnprintf(ioa_cfg->vectors_info[vec_idx].desc, n,\n\t\t\t \"host%d-%d\", ioa_cfg->host->host_no, vec_idx);\n\t\tioa_cfg->vectors_info[vec_idx].\n\t\t\tdesc[strlen(ioa_cfg->vectors_info[vec_idx].desc)] = 0;\n\t}\n}\n\nstatic int ipr_request_other_msi_irqs(struct ipr_ioa_cfg *ioa_cfg,\n\t\tstruct pci_dev *pdev)\n{\n\tint i, rc;\n\n\tfor (i = 1; i < ioa_cfg->nvectors; i++) {\n\t\trc = request_irq(pci_irq_vector(pdev, i),\n\t\t\tipr_isr_mhrrq,\n\t\t\t0,\n\t\t\tioa_cfg->vectors_info[i].desc,\n\t\t\t&ioa_cfg->hrrq[i]);\n\t\tif (rc) {\n\t\t\twhile (--i > 0)\n\t\t\t\tfree_irq(pci_irq_vector(pdev, i),\n\t\t\t\t\t&ioa_cfg->hrrq[i]);\n\t\t\treturn rc;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic irqreturn_t ipr_test_intr(int irq, void *devp)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)devp;\n\tunsigned long lock_flags = 0;\n\n\tdev_info(&ioa_cfg->pdev->dev, \"Received IRQ : %d\\n\", irq);\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\n\tioa_cfg->msi_received = 1;\n\twake_up(&ioa_cfg->msi_wait_q);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int ipr_test_msi(struct ipr_ioa_cfg *ioa_cfg, struct pci_dev *pdev)\n{\n\tint rc;\n\tunsigned long lock_flags = 0;\n\tint irq = pci_irq_vector(pdev, 0);\n\n\tENTER;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tinit_waitqueue_head(&ioa_cfg->msi_wait_q);\n\tioa_cfg->msi_received = 0;\n\tipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\n\twritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE, ioa_cfg->regs.clr_interrupt_mask_reg32);\n\treadl(ioa_cfg->regs.sense_interrupt_mask_reg);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\trc = request_irq(irq, ipr_test_intr, 0, IPR_NAME, ioa_cfg);\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"Can not assign irq %d\\n\", irq);\n\t\treturn rc;\n\t} else if (ipr_debug)\n\t\tdev_info(&pdev->dev, \"IRQ assigned: %d\\n\", irq);\n\n\twritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE, ioa_cfg->regs.sense_interrupt_reg32);\n\treadl(ioa_cfg->regs.sense_interrupt_reg);\n\twait_event_timeout(ioa_cfg->msi_wait_q, ioa_cfg->msi_received, HZ);\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\n\n\tif (!ioa_cfg->msi_received) {\n\t\t \n\t\tdev_info(&pdev->dev, \"MSI test failed.  Falling back to LSI.\\n\");\n\t\trc = -EOPNOTSUPP;\n\t} else if (ipr_debug)\n\t\tdev_info(&pdev->dev, \"MSI test succeeded.\\n\");\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tfree_irq(irq, ioa_cfg);\n\n\tLEAVE;\n\n\treturn rc;\n}\n\n  \nstatic int ipr_probe_ioa(struct pci_dev *pdev,\n\t\t\t const struct pci_device_id *dev_id)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tstruct Scsi_Host *host;\n\tunsigned long ipr_regs_pci;\n\tvoid __iomem *ipr_regs;\n\tint rc = PCIBIOS_SUCCESSFUL;\n\tvolatile u32 mask, uproc, interrupts;\n\tunsigned long lock_flags, driver_lock_flags;\n\tunsigned int irq_flag;\n\n\tENTER;\n\n\tdev_info(&pdev->dev, \"Found IOA with IRQ: %d\\n\", pdev->irq);\n\thost = scsi_host_alloc(&driver_template, sizeof(*ioa_cfg));\n\n\tif (!host) {\n\t\tdev_err(&pdev->dev, \"call to scsi_host_alloc failed!\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tioa_cfg = (struct ipr_ioa_cfg *)host->hostdata;\n\tmemset(ioa_cfg, 0, sizeof(struct ipr_ioa_cfg));\n\n\tioa_cfg->ipr_chip = ipr_get_chip_info(dev_id);\n\n\tif (!ioa_cfg->ipr_chip) {\n\t\tdev_err(&pdev->dev, \"Unknown adapter chipset 0x%04X 0x%04X\\n\",\n\t\t\tdev_id->vendor, dev_id->device);\n\t\tgoto out_scsi_host_put;\n\t}\n\n\t \n\tioa_cfg->sis64 = ioa_cfg->ipr_chip->sis_type == IPR_SIS64 ? 1 : 0;\n\tioa_cfg->chip_cfg = ioa_cfg->ipr_chip->cfg;\n\tioa_cfg->clear_isr = ioa_cfg->chip_cfg->clear_isr;\n\tioa_cfg->max_cmds = ioa_cfg->chip_cfg->max_cmds;\n\n\tif (ipr_transop_timeout)\n\t\tioa_cfg->transop_timeout = ipr_transop_timeout;\n\telse if (dev_id->driver_data & IPR_USE_LONG_TRANSOP_TIMEOUT)\n\t\tioa_cfg->transop_timeout = IPR_LONG_OPERATIONAL_TIMEOUT;\n\telse\n\t\tioa_cfg->transop_timeout = IPR_OPERATIONAL_TIMEOUT;\n\n\tioa_cfg->revid = pdev->revision;\n\n\tipr_init_ioa_cfg(ioa_cfg, host, pdev);\n\n\tipr_regs_pci = pci_resource_start(pdev, 0);\n\n\trc = pci_request_regions(pdev, IPR_NAME);\n\tif (rc < 0) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Couldn't register memory range of registers\\n\");\n\t\tgoto out_scsi_host_put;\n\t}\n\n\trc = pci_enable_device(pdev);\n\n\tif (rc || pci_channel_offline(pdev)) {\n\t\tif (pci_channel_offline(pdev)) {\n\t\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\t\trc = pci_enable_device(pdev);\n\t\t}\n\n\t\tif (rc) {\n\t\t\tdev_err(&pdev->dev, \"Cannot enable adapter\\n\");\n\t\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\t\tgoto out_release_regions;\n\t\t}\n\t}\n\n\tipr_regs = pci_ioremap_bar(pdev, 0);\n\n\tif (!ipr_regs) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Couldn't map memory range of registers\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto out_disable;\n\t}\n\n\tioa_cfg->hdw_dma_regs = ipr_regs;\n\tioa_cfg->hdw_dma_regs_pci = ipr_regs_pci;\n\tioa_cfg->ioa_mailbox = ioa_cfg->chip_cfg->mailbox + ipr_regs;\n\n\tipr_init_regs(ioa_cfg);\n\n\tif (ioa_cfg->sis64) {\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\t\tif (rc < 0) {\n\t\t\tdev_dbg(&pdev->dev, \"Failed to set 64 bit DMA mask\\n\");\n\t\t\trc = dma_set_mask_and_coherent(&pdev->dev,\n\t\t\t\t\t\t       DMA_BIT_MASK(32));\n\t\t}\n\t} else\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\n\tif (rc < 0) {\n\t\tdev_err(&pdev->dev, \"Failed to set DMA mask\\n\");\n\t\tgoto cleanup_nomem;\n\t}\n\n\trc = pci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE,\n\t\t\t\t   ioa_cfg->chip_cfg->cache_line_size);\n\n\tif (rc != PCIBIOS_SUCCESSFUL) {\n\t\tdev_err(&pdev->dev, \"Write of cache line size failed\\n\");\n\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\trc = -EIO;\n\t\tgoto cleanup_nomem;\n\t}\n\n\t \n\tinterrupts = readl(ioa_cfg->regs.sense_interrupt_reg);\n\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\n\tif (ipr_number_of_msix > IPR_MAX_MSIX_VECTORS) {\n\t\tdev_err(&pdev->dev, \"The max number of MSIX is %d\\n\",\n\t\t\tIPR_MAX_MSIX_VECTORS);\n\t\tipr_number_of_msix = IPR_MAX_MSIX_VECTORS;\n\t}\n\n\tirq_flag = PCI_IRQ_LEGACY;\n\tif (ioa_cfg->ipr_chip->has_msi)\n\t\tirq_flag |= PCI_IRQ_MSI | PCI_IRQ_MSIX;\n\trc = pci_alloc_irq_vectors(pdev, 1, ipr_number_of_msix, irq_flag);\n\tif (rc < 0) {\n\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\tgoto cleanup_nomem;\n\t}\n\tioa_cfg->nvectors = rc;\n\n\tif (!pdev->msi_enabled && !pdev->msix_enabled)\n\t\tioa_cfg->clear_isr = 1;\n\n\tpci_set_master(pdev);\n\n\tif (pci_channel_offline(pdev)) {\n\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\tpci_set_master(pdev);\n\t\tif (pci_channel_offline(pdev)) {\n\t\t\trc = -EIO;\n\t\t\tgoto out_msi_disable;\n\t\t}\n\t}\n\n\tif (pdev->msi_enabled || pdev->msix_enabled) {\n\t\trc = ipr_test_msi(ioa_cfg, pdev);\n\t\tswitch (rc) {\n\t\tcase 0:\n\t\t\tdev_info(&pdev->dev,\n\t\t\t\t\"Request for %d MSI%ss succeeded.\", ioa_cfg->nvectors,\n\t\t\t\tpdev->msix_enabled ? \"-X\" : \"\");\n\t\t\tbreak;\n\t\tcase -EOPNOTSUPP:\n\t\t\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\t\t\tpci_free_irq_vectors(pdev);\n\n\t\t\tioa_cfg->nvectors = 1;\n\t\t\tioa_cfg->clear_isr = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out_msi_disable;\n\t\t}\n\t}\n\n\tioa_cfg->hrrq_num = min3(ioa_cfg->nvectors,\n\t\t\t\t(unsigned int)num_online_cpus(),\n\t\t\t\t(unsigned int)IPR_MAX_HRRQ_NUM);\n\n\tif ((rc = ipr_save_pcix_cmd_reg(ioa_cfg)))\n\t\tgoto out_msi_disable;\n\n\tif ((rc = ipr_set_pcix_cmd_reg(ioa_cfg)))\n\t\tgoto out_msi_disable;\n\n\trc = ipr_alloc_mem(ioa_cfg);\n\tif (rc < 0) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Couldn't allocate enough memory for device driver!\\n\");\n\t\tgoto out_msi_disable;\n\t}\n\n\t \n\trc = pci_save_state(pdev);\n\n\tif (rc != PCIBIOS_SUCCESSFUL) {\n\t\tdev_err(&pdev->dev, \"Failed to save PCI config space\\n\");\n\t\trc = -EIO;\n\t\tgoto cleanup_nolog;\n\t}\n\n\t \n\tmask = readl(ioa_cfg->regs.sense_interrupt_mask_reg32);\n\tinterrupts = readl(ioa_cfg->regs.sense_interrupt_reg32);\n\tuproc = readl(ioa_cfg->regs.sense_uproc_interrupt_reg32);\n\tif ((mask & IPR_PCII_HRRQ_UPDATED) == 0 || (uproc & IPR_UPROCI_RESET_ALERT))\n\t\tioa_cfg->needs_hard_reset = 1;\n\tif ((interrupts & IPR_PCII_ERROR_INTERRUPTS) || reset_devices)\n\t\tioa_cfg->needs_hard_reset = 1;\n\tif (interrupts & IPR_PCII_IOA_UNIT_CHECKED)\n\t\tioa_cfg->ioa_unit_checked = 1;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\n\tif (pdev->msi_enabled || pdev->msix_enabled) {\n\t\tname_msi_vectors(ioa_cfg);\n\t\trc = request_irq(pci_irq_vector(pdev, 0), ipr_isr, 0,\n\t\t\tioa_cfg->vectors_info[0].desc,\n\t\t\t&ioa_cfg->hrrq[0]);\n\t\tif (!rc)\n\t\t\trc = ipr_request_other_msi_irqs(ioa_cfg, pdev);\n\t} else {\n\t\trc = request_irq(pdev->irq, ipr_isr,\n\t\t\t IRQF_SHARED,\n\t\t\t IPR_NAME, &ioa_cfg->hrrq[0]);\n\t}\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"Couldn't register IRQ %d! rc=%d\\n\",\n\t\t\tpdev->irq, rc);\n\t\tgoto cleanup_nolog;\n\t}\n\n\tif ((dev_id->driver_data & IPR_USE_PCI_WARM_RESET) ||\n\t    (dev_id->device == PCI_DEVICE_ID_IBM_OBSIDIAN_E && !ioa_cfg->revid)) {\n\t\tioa_cfg->needs_warm_reset = 1;\n\t\tioa_cfg->reset = ipr_reset_slot_reset;\n\n\t\tioa_cfg->reset_work_q = alloc_ordered_workqueue(\"ipr_reset_%d\",\n\t\t\t\t\t\t\t\tWQ_MEM_RECLAIM, host->host_no);\n\n\t\tif (!ioa_cfg->reset_work_q) {\n\t\t\tdev_err(&pdev->dev, \"Couldn't register reset workqueue\\n\");\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out_free_irq;\n\t\t}\n\t} else\n\t\tioa_cfg->reset = ipr_reset_start_bist;\n\n\tspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\n\tlist_add_tail(&ioa_cfg->queue, &ipr_ioa_head);\n\tspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\n\n\tLEAVE;\nout:\n\treturn rc;\n\nout_free_irq:\n\tipr_free_irqs(ioa_cfg);\ncleanup_nolog:\n\tipr_free_mem(ioa_cfg);\nout_msi_disable:\n\tipr_wait_for_pci_err_recovery(ioa_cfg);\n\tpci_free_irq_vectors(pdev);\ncleanup_nomem:\n\tiounmap(ipr_regs);\nout_disable:\n\tpci_disable_device(pdev);\nout_release_regions:\n\tpci_release_regions(pdev);\nout_scsi_host_put:\n\tscsi_host_put(host);\n\tgoto out;\n}\n\n \nstatic void ipr_initiate_ioa_bringdown(struct ipr_ioa_cfg *ioa_cfg,\n\t\t\t\t       enum ipr_shutdown_type shutdown_type)\n{\n\tENTER;\n\tif (ioa_cfg->sdt_state == WAIT_FOR_DUMP)\n\t\tioa_cfg->sdt_state = ABORT_DUMP;\n\tioa_cfg->reset_retries = 0;\n\tioa_cfg->in_ioa_bringdown = 1;\n\tipr_initiate_ioa_reset(ioa_cfg, shutdown_type);\n\tLEAVE;\n}\n\n \nstatic void __ipr_remove(struct pci_dev *pdev)\n{\n\tunsigned long host_lock_flags = 0;\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\tint i;\n\tunsigned long driver_lock_flags;\n\tENTER;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\n\twhile (ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\n\t\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\n\t}\n\n\tfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\n\t\tspin_lock(&ioa_cfg->hrrq[i]._lock);\n\t\tioa_cfg->hrrq[i].removing_ioa = 1;\n\t\tspin_unlock(&ioa_cfg->hrrq[i]._lock);\n\t}\n\twmb();\n\tipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);\n\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\tflush_work(&ioa_cfg->work_q);\n\tif (ioa_cfg->reset_work_q)\n\t\tflush_workqueue(ioa_cfg->reset_work_q);\n\tINIT_LIST_HEAD(&ioa_cfg->used_res_q);\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\n\n\tspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\n\tlist_del(&ioa_cfg->queue);\n\tspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\n\n\tif (ioa_cfg->sdt_state == ABORT_DUMP)\n\t\tioa_cfg->sdt_state = WAIT_FOR_DUMP;\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\n\n\tipr_free_all_resources(ioa_cfg);\n\n\tLEAVE;\n}\n\n \nstatic void ipr_remove(struct pci_dev *pdev)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\n\tENTER;\n\n\tipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t      &ipr_trace_attr);\n\tipr_remove_dump_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t     &ipr_dump_attr);\n\tsysfs_remove_bin_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t&ipr_ioa_async_err_log);\n\tscsi_remove_host(ioa_cfg->host);\n\n\t__ipr_remove(pdev);\n\n\tLEAVE;\n}\n\n \nstatic int ipr_probe(struct pci_dev *pdev, const struct pci_device_id *dev_id)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tunsigned long flags;\n\tint rc, i;\n\n\trc = ipr_probe_ioa(pdev, dev_id);\n\n\tif (rc)\n\t\treturn rc;\n\n\tioa_cfg = pci_get_drvdata(pdev);\n\tipr_probe_ioa_part2(ioa_cfg);\n\n\trc = scsi_add_host(ioa_cfg->host, &pdev->dev);\n\n\tif (rc) {\n\t\t__ipr_remove(pdev);\n\t\treturn rc;\n\t}\n\n\trc = ipr_create_trace_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t   &ipr_trace_attr);\n\n\tif (rc) {\n\t\tscsi_remove_host(ioa_cfg->host);\n\t\t__ipr_remove(pdev);\n\t\treturn rc;\n\t}\n\n\trc = sysfs_create_bin_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t&ipr_ioa_async_err_log);\n\n\tif (rc) {\n\t\tipr_remove_dump_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t&ipr_dump_attr);\n\t\tipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t&ipr_trace_attr);\n\t\tscsi_remove_host(ioa_cfg->host);\n\t\t__ipr_remove(pdev);\n\t\treturn rc;\n\t}\n\n\trc = ipr_create_dump_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t   &ipr_dump_attr);\n\n\tif (rc) {\n\t\tsysfs_remove_bin_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t      &ipr_ioa_async_err_log);\n\t\tipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\n\t\t\t\t      &ipr_trace_attr);\n\t\tscsi_remove_host(ioa_cfg->host);\n\t\t__ipr_remove(pdev);\n\t\treturn rc;\n\t}\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\tioa_cfg->scan_enabled = 1;\n\tschedule_work(&ioa_cfg->work_q);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n\n\tioa_cfg->iopoll_weight = ioa_cfg->chip_cfg->iopoll_weight;\n\n\tif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\n\t\tfor (i = 1; i < ioa_cfg->hrrq_num; i++) {\n\t\t\tirq_poll_init(&ioa_cfg->hrrq[i].iopoll,\n\t\t\t\t\tioa_cfg->iopoll_weight, ipr_iopoll);\n\t\t}\n\t}\n\n\tscsi_scan_host(ioa_cfg->host);\n\n\treturn 0;\n}\n\n \nstatic void ipr_shutdown(struct pci_dev *pdev)\n{\n\tstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\n\tunsigned long lock_flags = 0;\n\tenum ipr_shutdown_type shutdown_type = IPR_SHUTDOWN_NORMAL;\n\tint i;\n\n\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\tif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\n\t\tioa_cfg->iopoll_weight = 0;\n\t\tfor (i = 1; i < ioa_cfg->hrrq_num; i++)\n\t\t\tirq_poll_disable(&ioa_cfg->hrrq[i].iopoll);\n\t}\n\n\twhile (ioa_cfg->in_reset_reload) {\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\t\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\n\t}\n\n\tif (ipr_fast_reboot && system_state == SYSTEM_RESTART && ioa_cfg->sis64)\n\t\tshutdown_type = IPR_SHUTDOWN_QUIESCE;\n\n\tipr_initiate_ioa_bringdown(ioa_cfg, shutdown_type);\n\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\n\twait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\n\tif (ipr_fast_reboot && system_state == SYSTEM_RESTART && ioa_cfg->sis64) {\n\t\tipr_free_irqs(ioa_cfg);\n\t\tpci_disable_device(ioa_cfg->pdev);\n\t}\n}\n\nstatic struct pci_device_id ipr_pci_table[] = {\n\t{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5702, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5703, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_573D, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_573E, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CITRINE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_571B, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CITRINE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572E, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CITRINE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_571A, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CITRINE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_575B, 0, 0,\n\t\tIPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572A, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572B, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_575C, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572A, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572B, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT},\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_575C, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN_E,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_574E, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN_E,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B3, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN_E,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57CC, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_OBSIDIAN_E,\n\t      PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B7, 0, 0,\n\t      IPR_USE_LONG_TRANSOP_TIMEOUT | IPR_USE_PCI_WARM_RESET },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_SNIPE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2780, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_SCAMP,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_571E, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_SCAMP,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_571F, 0, 0,\n\t\tIPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_ADAPTEC2, PCI_DEVICE_ID_ADAPTEC2_SCAMP,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_572F, 0, 0,\n\t\tIPR_USE_LONG_TRANSOP_TIMEOUT },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B5, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_574D, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B2, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57C0, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57C3, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROC_FPGA_E2,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57C4, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B4, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57B1, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57C6, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57C8, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57CE, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57D5, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57D6, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57D7, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57D8, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57D9, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57DA, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57EB, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57EC, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57ED, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57EE, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57EF, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_57F0, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2CCA, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2CD2, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CROCODILE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2CCD, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_RATTLESNAKE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_580A, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_RATTLESNAKE,\n\t\tPCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_580B, 0, 0, 0 },\n\t{ }\n};\nMODULE_DEVICE_TABLE(pci, ipr_pci_table);\n\nstatic const struct pci_error_handlers ipr_err_handler = {\n\t.error_detected = ipr_pci_error_detected,\n\t.mmio_enabled = ipr_pci_mmio_enabled,\n\t.slot_reset = ipr_pci_slot_reset,\n};\n\nstatic struct pci_driver ipr_driver = {\n\t.name = IPR_NAME,\n\t.id_table = ipr_pci_table,\n\t.probe = ipr_probe,\n\t.remove = ipr_remove,\n\t.shutdown = ipr_shutdown,\n\t.err_handler = &ipr_err_handler,\n};\n\n \nstatic void ipr_halt_done(struct ipr_cmnd *ipr_cmd)\n{\n\tlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\n}\n\n \nstatic int ipr_halt(struct notifier_block *nb, ulong event, void *buf)\n{\n\tstruct ipr_cmnd *ipr_cmd;\n\tstruct ipr_ioa_cfg *ioa_cfg;\n\tunsigned long flags = 0, driver_lock_flags;\n\n\tif (event != SYS_RESTART && event != SYS_HALT && event != SYS_POWER_OFF)\n\t\treturn NOTIFY_DONE;\n\n\tspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\n\n\tlist_for_each_entry(ioa_cfg, &ipr_ioa_head, queue) {\n\t\tspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\n\t\tif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds ||\n\t\t    (ipr_fast_reboot && event == SYS_RESTART && ioa_cfg->sis64)) {\n\t\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\n\t\tipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\n\t\tipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\n\t\tipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;\n\t\tipr_cmd->ioarcb.cmd_pkt.cdb[1] = IPR_SHUTDOWN_PREPARE_FOR_NORMAL;\n\n\t\tipr_do_req(ipr_cmd, ipr_halt_done, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\n\t\tspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\n\t}\n\tspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block ipr_notifier = {\n\tipr_halt, NULL, 0\n};\n\n \nstatic int __init ipr_init(void)\n{\n\tint rc;\n\n\tipr_info(\"IBM Power RAID SCSI Device Driver version: %s %s\\n\",\n\t\t IPR_DRIVER_VERSION, IPR_DRIVER_DATE);\n\n\tregister_reboot_notifier(&ipr_notifier);\n\trc = pci_register_driver(&ipr_driver);\n\tif (rc) {\n\t\tunregister_reboot_notifier(&ipr_notifier);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __exit ipr_exit(void)\n{\n\tunregister_reboot_notifier(&ipr_notifier);\n\tpci_unregister_driver(&ipr_driver);\n}\n\nmodule_init(ipr_init);\nmodule_exit(ipr_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}