{
  "module_name": "mvumi.c",
  "hash_id": "d140df7f9bc7a6dac70c57d81f8798a82dcb684ca38f27a496d46669b2207a50",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/mvumi.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/pci.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/ktime.h>\n#include <linux/blkdev.h>\n#include <linux/io.h>\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_transport.h>\n#include <scsi/scsi_eh.h>\n#include <linux/uaccess.h>\n#include <linux/kthread.h>\n\n#include \"mvumi.h\"\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"jyli@marvell.com\");\nMODULE_DESCRIPTION(\"Marvell UMI Driver\");\n\nstatic const struct pci_device_id mvumi_pci_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_MARVELL_EXT, PCI_DEVICE_ID_MARVELL_MV9143) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_MARVELL_EXT, PCI_DEVICE_ID_MARVELL_MV9580) },\n\t{ 0 }\n};\n\nMODULE_DEVICE_TABLE(pci, mvumi_pci_table);\n\nstatic void tag_init(struct mvumi_tag *st, unsigned short size)\n{\n\tunsigned short i;\n\tBUG_ON(size != st->size);\n\tst->top = size;\n\tfor (i = 0; i < size; i++)\n\t\tst->stack[i] = size - 1 - i;\n}\n\nstatic unsigned short tag_get_one(struct mvumi_hba *mhba, struct mvumi_tag *st)\n{\n\tBUG_ON(st->top <= 0);\n\treturn st->stack[--st->top];\n}\n\nstatic void tag_release_one(struct mvumi_hba *mhba, struct mvumi_tag *st,\n\t\t\t\t\t\t\tunsigned short tag)\n{\n\tBUG_ON(st->top >= st->size);\n\tst->stack[st->top++] = tag;\n}\n\nstatic bool tag_is_empty(struct mvumi_tag *st)\n{\n\tif (st->top == 0)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void mvumi_unmap_pci_addr(struct pci_dev *dev, void **addr_array)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_BASE_ADDRESS; i++)\n\t\tif ((pci_resource_flags(dev, i) & IORESOURCE_MEM) &&\n\t\t\t\t\t\t\t\taddr_array[i])\n\t\t\tpci_iounmap(dev, addr_array[i]);\n}\n\nstatic int mvumi_map_pci_addr(struct pci_dev *dev, void **addr_array)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_BASE_ADDRESS; i++) {\n\t\tif (pci_resource_flags(dev, i) & IORESOURCE_MEM) {\n\t\t\taddr_array[i] = pci_iomap(dev, i, 0);\n\t\t\tif (!addr_array[i]) {\n\t\t\t\tdev_err(&dev->dev, \"failed to map Bar[%d]\\n\",\n\t\t\t\t\t\t\t\t\ti);\n\t\t\t\tmvumi_unmap_pci_addr(dev, addr_array);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else\n\t\t\taddr_array[i] = NULL;\n\n\t\tdev_dbg(&dev->dev, \"Bar %d : %p.\\n\", i, addr_array[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic struct mvumi_res *mvumi_alloc_mem_resource(struct mvumi_hba *mhba,\n\t\t\t\tenum resource_type type, unsigned int size)\n{\n\tstruct mvumi_res *res = kzalloc(sizeof(*res), GFP_ATOMIC);\n\n\tif (!res) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"Failed to allocate memory for resource manager.\\n\");\n\t\treturn NULL;\n\t}\n\n\tswitch (type) {\n\tcase RESOURCE_CACHED_MEMORY:\n\t\tres->virt_addr = kzalloc(size, GFP_ATOMIC);\n\t\tif (!res->virt_addr) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"unable to allocate memory,size = %d.\\n\", size);\n\t\t\tkfree(res);\n\t\t\treturn NULL;\n\t\t}\n\t\tbreak;\n\n\tcase RESOURCE_UNCACHED_MEMORY:\n\t\tsize = round_up(size, 8);\n\t\tres->virt_addr = dma_alloc_coherent(&mhba->pdev->dev, size,\n\t\t\t\t\t\t    &res->bus_addr,\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!res->virt_addr) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\t\"unable to allocate consistent mem,\"\n\t\t\t\t\t\t\t\"size = %d.\\n\", size);\n\t\t\tkfree(res);\n\t\t\treturn NULL;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&mhba->pdev->dev, \"unknown resource type %d.\\n\", type);\n\t\tkfree(res);\n\t\treturn NULL;\n\t}\n\n\tres->type = type;\n\tres->size = size;\n\tINIT_LIST_HEAD(&res->entry);\n\tlist_add_tail(&res->entry, &mhba->res_list);\n\n\treturn res;\n}\n\nstatic void mvumi_release_mem_resource(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, &mhba->res_list, entry) {\n\t\tswitch (res->type) {\n\t\tcase RESOURCE_UNCACHED_MEMORY:\n\t\t\tdma_free_coherent(&mhba->pdev->dev, res->size,\n\t\t\t\t\t\tres->virt_addr, res->bus_addr);\n\t\t\tbreak;\n\t\tcase RESOURCE_CACHED_MEMORY:\n\t\t\tkfree(res->virt_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"unknown resource type %d\\n\", res->type);\n\t\t\tbreak;\n\t\t}\n\t\tlist_del(&res->entry);\n\t\tkfree(res);\n\t}\n\tmhba->fw_flag &= ~MVUMI_FW_ALLOC;\n}\n\n \nstatic int mvumi_make_sgl(struct mvumi_hba *mhba, struct scsi_cmnd *scmd,\n\t\t\t\t\tvoid *sgl_p, unsigned char *sg_count)\n{\n\tstruct scatterlist *sg;\n\tstruct mvumi_sgl *m_sg = (struct mvumi_sgl *) sgl_p;\n\tunsigned int i;\n\tunsigned int sgnum = scsi_sg_count(scmd);\n\tdma_addr_t busaddr;\n\n\t*sg_count = dma_map_sg(&mhba->pdev->dev, scsi_sglist(scmd), sgnum,\n\t\t\t       scmd->sc_data_direction);\n\tif (*sg_count > mhba->max_sge) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"sg count[0x%x] is bigger than max sg[0x%x].\\n\",\n\t\t\t*sg_count, mhba->max_sge);\n\t\tdma_unmap_sg(&mhba->pdev->dev, scsi_sglist(scmd), sgnum,\n\t\t\t     scmd->sc_data_direction);\n\t\treturn -1;\n\t}\n\tscsi_for_each_sg(scmd, sg, *sg_count, i) {\n\t\tbusaddr = sg_dma_address(sg);\n\t\tm_sg->baseaddr_l = cpu_to_le32(lower_32_bits(busaddr));\n\t\tm_sg->baseaddr_h = cpu_to_le32(upper_32_bits(busaddr));\n\t\tm_sg->flags = 0;\n\t\tsgd_setsz(mhba, m_sg, cpu_to_le32(sg_dma_len(sg)));\n\t\tif ((i + 1) == *sg_count)\n\t\t\tm_sg->flags |= 1U << mhba->eot_flag;\n\n\t\tsgd_inc(mhba, m_sg);\n\t}\n\n\treturn 0;\n}\n\nstatic int mvumi_internal_cmd_sgl(struct mvumi_hba *mhba, struct mvumi_cmd *cmd,\n\t\t\t\t\t\t\tunsigned int size)\n{\n\tstruct mvumi_sgl *m_sg;\n\tvoid *virt_addr;\n\tdma_addr_t phy_addr;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tvirt_addr = dma_alloc_coherent(&mhba->pdev->dev, size, &phy_addr,\n\t\t\t\t       GFP_KERNEL);\n\tif (!virt_addr)\n\t\treturn -1;\n\n\tm_sg = (struct mvumi_sgl *) &cmd->frame->payload[0];\n\tcmd->frame->sg_counts = 1;\n\tcmd->data_buf = virt_addr;\n\n\tm_sg->baseaddr_l = cpu_to_le32(lower_32_bits(phy_addr));\n\tm_sg->baseaddr_h = cpu_to_le32(upper_32_bits(phy_addr));\n\tm_sg->flags = 1U << mhba->eot_flag;\n\tsgd_setsz(mhba, m_sg, cpu_to_le32(size));\n\n\treturn 0;\n}\n\nstatic struct mvumi_cmd *mvumi_create_internal_cmd(struct mvumi_hba *mhba,\n\t\t\t\tunsigned int buf_size)\n{\n\tstruct mvumi_cmd *cmd;\n\n\tcmd = kzalloc(sizeof(*cmd), GFP_KERNEL);\n\tif (!cmd) {\n\t\tdev_err(&mhba->pdev->dev, \"failed to create a internal cmd\\n\");\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&cmd->queue_pointer);\n\n\tcmd->frame = dma_alloc_coherent(&mhba->pdev->dev, mhba->ib_max_size,\n\t\t\t&cmd->frame_phys, GFP_KERNEL);\n\tif (!cmd->frame) {\n\t\tdev_err(&mhba->pdev->dev, \"failed to allocate memory for FW\"\n\t\t\t\" frame,size = %d.\\n\", mhba->ib_max_size);\n\t\tkfree(cmd);\n\t\treturn NULL;\n\t}\n\n\tif (buf_size) {\n\t\tif (mvumi_internal_cmd_sgl(mhba, cmd, buf_size)) {\n\t\t\tdev_err(&mhba->pdev->dev, \"failed to allocate memory\"\n\t\t\t\t\t\t\" for internal frame\\n\");\n\t\t\tdma_free_coherent(&mhba->pdev->dev, mhba->ib_max_size,\n\t\t\t\t\tcmd->frame, cmd->frame_phys);\n\t\t\tkfree(cmd);\n\t\t\treturn NULL;\n\t\t}\n\t} else\n\t\tcmd->frame->sg_counts = 0;\n\n\treturn cmd;\n}\n\nstatic void mvumi_delete_internal_cmd(struct mvumi_hba *mhba,\n\t\t\t\t\t\tstruct mvumi_cmd *cmd)\n{\n\tstruct mvumi_sgl *m_sg;\n\tunsigned int size;\n\tdma_addr_t phy_addr;\n\n\tif (cmd && cmd->frame) {\n\t\tif (cmd->frame->sg_counts) {\n\t\t\tm_sg = (struct mvumi_sgl *) &cmd->frame->payload[0];\n\t\t\tsgd_getsz(mhba, m_sg, size);\n\n\t\t\tphy_addr = (dma_addr_t) m_sg->baseaddr_l |\n\t\t\t\t(dma_addr_t) ((m_sg->baseaddr_h << 16) << 16);\n\n\t\t\tdma_free_coherent(&mhba->pdev->dev, size, cmd->data_buf,\n\t\t\t\t\t\t\t\tphy_addr);\n\t\t}\n\t\tdma_free_coherent(&mhba->pdev->dev, mhba->ib_max_size,\n\t\t\t\tcmd->frame, cmd->frame_phys);\n\t\tkfree(cmd);\n\t}\n}\n\n \nstatic struct mvumi_cmd *mvumi_get_cmd(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_cmd *cmd = NULL;\n\n\tif (likely(!list_empty(&mhba->cmd_pool))) {\n\t\tcmd = list_entry((&mhba->cmd_pool)->next,\n\t\t\t\tstruct mvumi_cmd, queue_pointer);\n\t\tlist_del_init(&cmd->queue_pointer);\n\t} else\n\t\tdev_warn(&mhba->pdev->dev, \"command pool is empty!\\n\");\n\n\treturn cmd;\n}\n\n \nstatic inline void mvumi_return_cmd(struct mvumi_hba *mhba,\n\t\t\t\t\t\tstruct mvumi_cmd *cmd)\n{\n\tcmd->scmd = NULL;\n\tlist_add_tail(&cmd->queue_pointer, &mhba->cmd_pool);\n}\n\n \nstatic void mvumi_free_cmds(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_cmd *cmd;\n\n\twhile (!list_empty(&mhba->cmd_pool)) {\n\t\tcmd = list_first_entry(&mhba->cmd_pool, struct mvumi_cmd,\n\t\t\t\t\t\t\tqueue_pointer);\n\t\tlist_del(&cmd->queue_pointer);\n\t\tif (!(mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC))\n\t\t\tkfree(cmd->frame);\n\t\tkfree(cmd);\n\t}\n}\n\n \nstatic int mvumi_alloc_cmds(struct mvumi_hba *mhba)\n{\n\tint i;\n\tstruct mvumi_cmd *cmd;\n\n\tfor (i = 0; i < mhba->max_io; i++) {\n\t\tcmd = kzalloc(sizeof(*cmd), GFP_KERNEL);\n\t\tif (!cmd)\n\t\t\tgoto err_exit;\n\n\t\tINIT_LIST_HEAD(&cmd->queue_pointer);\n\t\tlist_add_tail(&cmd->queue_pointer, &mhba->cmd_pool);\n\t\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC) {\n\t\t\tcmd->frame = mhba->ib_frame + i * mhba->ib_max_size;\n\t\t\tcmd->frame_phys = mhba->ib_frame_phys\n\t\t\t\t\t\t+ i * mhba->ib_max_size;\n\t\t} else\n\t\t\tcmd->frame = kzalloc(mhba->ib_max_size, GFP_KERNEL);\n\t\tif (!cmd->frame)\n\t\t\tgoto err_exit;\n\t}\n\treturn 0;\n\nerr_exit:\n\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for cmd[0x%x].\\n\", i);\n\twhile (!list_empty(&mhba->cmd_pool)) {\n\t\tcmd = list_first_entry(&mhba->cmd_pool, struct mvumi_cmd,\n\t\t\t\t\t\tqueue_pointer);\n\t\tlist_del(&cmd->queue_pointer);\n\t\tif (!(mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC))\n\t\t\tkfree(cmd->frame);\n\t\tkfree(cmd);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic unsigned int mvumi_check_ib_list_9143(struct mvumi_hba *mhba)\n{\n\tunsigned int ib_rp_reg;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tib_rp_reg = ioread32(mhba->regs->inb_read_pointer);\n\n\tif (unlikely(((ib_rp_reg & regs->cl_slot_num_mask) ==\n\t\t\t(mhba->ib_cur_slot & regs->cl_slot_num_mask)) &&\n\t\t\t((ib_rp_reg & regs->cl_pointer_toggle)\n\t\t\t != (mhba->ib_cur_slot & regs->cl_pointer_toggle)))) {\n\t\tdev_warn(&mhba->pdev->dev, \"no free slot to use.\\n\");\n\t\treturn 0;\n\t}\n\tif (atomic_read(&mhba->fw_outstanding) >= mhba->max_io) {\n\t\tdev_warn(&mhba->pdev->dev, \"firmware io overflow.\\n\");\n\t\treturn 0;\n\t} else {\n\t\treturn mhba->max_io - atomic_read(&mhba->fw_outstanding);\n\t}\n}\n\nstatic unsigned int mvumi_check_ib_list_9580(struct mvumi_hba *mhba)\n{\n\tunsigned int count;\n\tif (atomic_read(&mhba->fw_outstanding) >= (mhba->max_io - 1))\n\t\treturn 0;\n\tcount = ioread32(mhba->ib_shadow);\n\tif (count == 0xffff)\n\t\treturn 0;\n\treturn count;\n}\n\nstatic void mvumi_get_ib_list_entry(struct mvumi_hba *mhba, void **ib_entry)\n{\n\tunsigned int cur_ib_entry;\n\n\tcur_ib_entry = mhba->ib_cur_slot & mhba->regs->cl_slot_num_mask;\n\tcur_ib_entry++;\n\tif (cur_ib_entry >= mhba->list_num_io) {\n\t\tcur_ib_entry -= mhba->list_num_io;\n\t\tmhba->ib_cur_slot ^= mhba->regs->cl_pointer_toggle;\n\t}\n\tmhba->ib_cur_slot &= ~mhba->regs->cl_slot_num_mask;\n\tmhba->ib_cur_slot |= (cur_ib_entry & mhba->regs->cl_slot_num_mask);\n\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC) {\n\t\t*ib_entry = mhba->ib_list + cur_ib_entry *\n\t\t\t\tsizeof(struct mvumi_dyn_list_entry);\n\t} else {\n\t\t*ib_entry = mhba->ib_list + cur_ib_entry * mhba->ib_max_size;\n\t}\n\tatomic_inc(&mhba->fw_outstanding);\n}\n\nstatic void mvumi_send_ib_list_entry(struct mvumi_hba *mhba)\n{\n\tiowrite32(0xffff, mhba->ib_shadow);\n\tiowrite32(mhba->ib_cur_slot, mhba->regs->inb_write_pointer);\n}\n\nstatic char mvumi_check_ob_frame(struct mvumi_hba *mhba,\n\t\tunsigned int cur_obf, struct mvumi_rsp_frame *p_outb_frame)\n{\n\tunsigned short tag, request_id;\n\n\tudelay(1);\n\tp_outb_frame = mhba->ob_list + cur_obf * mhba->ob_max_size;\n\trequest_id = p_outb_frame->request_id;\n\ttag = p_outb_frame->tag;\n\tif (tag > mhba->tag_pool.size) {\n\t\tdev_err(&mhba->pdev->dev, \"ob frame data error\\n\");\n\t\treturn -1;\n\t}\n\tif (mhba->tag_cmd[tag] == NULL) {\n\t\tdev_err(&mhba->pdev->dev, \"tag[0x%x] with NO command\\n\", tag);\n\t\treturn -1;\n\t} else if (mhba->tag_cmd[tag]->request_id != request_id &&\n\t\t\t\t\t\tmhba->request_id_enabled) {\n\t\t\tdev_err(&mhba->pdev->dev, \"request ID from FW:0x%x,\"\n\t\t\t\t\t\"cmd request ID:0x%x\\n\", request_id,\n\t\t\t\t\tmhba->tag_cmd[tag]->request_id);\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int mvumi_check_ob_list_9143(struct mvumi_hba *mhba,\n\t\t\tunsigned int *cur_obf, unsigned int *assign_obf_end)\n{\n\tunsigned int ob_write, ob_write_shadow;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tdo {\n\t\tob_write = ioread32(regs->outb_copy_pointer);\n\t\tob_write_shadow = ioread32(mhba->ob_shadow);\n\t} while ((ob_write & regs->cl_slot_num_mask) != ob_write_shadow);\n\n\t*cur_obf = mhba->ob_cur_slot & mhba->regs->cl_slot_num_mask;\n\t*assign_obf_end = ob_write & mhba->regs->cl_slot_num_mask;\n\n\tif ((ob_write & regs->cl_pointer_toggle) !=\n\t\t\t(mhba->ob_cur_slot & regs->cl_pointer_toggle)) {\n\t\t*assign_obf_end += mhba->list_num_io;\n\t}\n\treturn 0;\n}\n\nstatic int mvumi_check_ob_list_9580(struct mvumi_hba *mhba,\n\t\t\tunsigned int *cur_obf, unsigned int *assign_obf_end)\n{\n\tunsigned int ob_write;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tob_write = ioread32(regs->outb_read_pointer);\n\tob_write = ioread32(regs->outb_copy_pointer);\n\t*cur_obf = mhba->ob_cur_slot & mhba->regs->cl_slot_num_mask;\n\t*assign_obf_end = ob_write & mhba->regs->cl_slot_num_mask;\n\tif (*assign_obf_end < *cur_obf)\n\t\t*assign_obf_end += mhba->list_num_io;\n\telse if (*assign_obf_end == *cur_obf)\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic void mvumi_receive_ob_list_entry(struct mvumi_hba *mhba)\n{\n\tunsigned int cur_obf, assign_obf_end, i;\n\tstruct mvumi_ob_data *ob_data;\n\tstruct mvumi_rsp_frame *p_outb_frame;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tif (mhba->instancet->check_ob_list(mhba, &cur_obf, &assign_obf_end))\n\t\treturn;\n\n\tfor (i = (assign_obf_end - cur_obf); i != 0; i--) {\n\t\tcur_obf++;\n\t\tif (cur_obf >= mhba->list_num_io) {\n\t\t\tcur_obf -= mhba->list_num_io;\n\t\t\tmhba->ob_cur_slot ^= regs->cl_pointer_toggle;\n\t\t}\n\n\t\tp_outb_frame = mhba->ob_list + cur_obf * mhba->ob_max_size;\n\n\t\t \n\t\tif (unlikely(p_outb_frame->tag > mhba->tag_pool.size ||\n\t\t\tmhba->tag_cmd[p_outb_frame->tag] == NULL ||\n\t\t\tp_outb_frame->request_id !=\n\t\t\t\tmhba->tag_cmd[p_outb_frame->tag]->request_id))\n\t\t\tif (mvumi_check_ob_frame(mhba, cur_obf, p_outb_frame))\n\t\t\t\tcontinue;\n\n\t\tif (!list_empty(&mhba->ob_data_list)) {\n\t\t\tob_data = (struct mvumi_ob_data *)\n\t\t\t\tlist_first_entry(&mhba->ob_data_list,\n\t\t\t\t\tstruct mvumi_ob_data, list);\n\t\t\tlist_del_init(&ob_data->list);\n\t\t} else {\n\t\t\tob_data = NULL;\n\t\t\tif (cur_obf == 0) {\n\t\t\t\tcur_obf = mhba->list_num_io - 1;\n\t\t\t\tmhba->ob_cur_slot ^= regs->cl_pointer_toggle;\n\t\t\t} else\n\t\t\t\tcur_obf -= 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(ob_data->data, p_outb_frame, mhba->ob_max_size);\n\t\tp_outb_frame->tag = 0xff;\n\n\t\tlist_add_tail(&ob_data->list, &mhba->free_ob_list);\n\t}\n\tmhba->ob_cur_slot &= ~regs->cl_slot_num_mask;\n\tmhba->ob_cur_slot |= (cur_obf & regs->cl_slot_num_mask);\n\tiowrite32(mhba->ob_cur_slot, regs->outb_read_pointer);\n}\n\nstatic void mvumi_reset(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tiowrite32(0, regs->enpointa_mask_reg);\n\tif (ioread32(regs->arm_to_pciea_msg1) != HANDSHAKE_DONESTATE)\n\t\treturn;\n\n\tiowrite32(DRBL_SOFT_RESET, regs->pciea_to_arm_drbl_reg);\n}\n\nstatic unsigned char mvumi_start(struct mvumi_hba *mhba);\n\nstatic int mvumi_wait_for_outstanding(struct mvumi_hba *mhba)\n{\n\tmhba->fw_state = FW_STATE_ABORT;\n\tmvumi_reset(mhba);\n\n\tif (mvumi_start(mhba))\n\t\treturn FAILED;\n\telse\n\t\treturn SUCCESS;\n}\n\nstatic int mvumi_wait_for_fw(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\tu32 tmp;\n\tunsigned long before;\n\tbefore = jiffies;\n\n\tiowrite32(0, regs->enpointa_mask_reg);\n\ttmp = ioread32(regs->arm_to_pciea_msg1);\n\twhile (tmp != HANDSHAKE_READYSTATE) {\n\t\tiowrite32(DRBL_MU_RESET, regs->pciea_to_arm_drbl_reg);\n\t\tif (time_after(jiffies, before + FW_MAX_DELAY * HZ)) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"FW reset failed [0x%x].\\n\", tmp);\n\t\t\treturn FAILED;\n\t\t}\n\n\t\tmsleep(500);\n\t\trmb();\n\t\ttmp = ioread32(regs->arm_to_pciea_msg1);\n\t}\n\n\treturn SUCCESS;\n}\n\nstatic void mvumi_backup_bar_addr(struct mvumi_hba *mhba)\n{\n\tunsigned char i;\n\n\tfor (i = 0; i < MAX_BASE_ADDRESS; i++) {\n\t\tpci_read_config_dword(mhba->pdev, 0x10 + i * 4,\n\t\t\t\t\t\t&mhba->pci_base[i]);\n\t}\n}\n\nstatic void mvumi_restore_bar_addr(struct mvumi_hba *mhba)\n{\n\tunsigned char i;\n\n\tfor (i = 0; i < MAX_BASE_ADDRESS; i++) {\n\t\tif (mhba->pci_base[i])\n\t\t\tpci_write_config_dword(mhba->pdev, 0x10 + i * 4,\n\t\t\t\t\t\tmhba->pci_base[i]);\n\t}\n}\n\nstatic int mvumi_pci_set_master(struct pci_dev *pdev)\n{\n\tint ret = 0;\n\n\tpci_set_master(pdev);\n\n\tif (IS_DMA64) {\n\t\tif (dma_set_mask(&pdev->dev, DMA_BIT_MASK(64)))\n\t\t\tret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\t} else\n\t\tret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\n\treturn ret;\n}\n\nstatic int mvumi_reset_host_9580(struct mvumi_hba *mhba)\n{\n\tmhba->fw_state = FW_STATE_ABORT;\n\n\tiowrite32(0, mhba->regs->reset_enable);\n\tiowrite32(0xf, mhba->regs->reset_request);\n\n\tiowrite32(0x10, mhba->regs->reset_enable);\n\tiowrite32(0x10, mhba->regs->reset_request);\n\tmsleep(100);\n\tpci_disable_device(mhba->pdev);\n\n\tif (pci_enable_device(mhba->pdev)) {\n\t\tdev_err(&mhba->pdev->dev, \"enable device failed\\n\");\n\t\treturn FAILED;\n\t}\n\tif (mvumi_pci_set_master(mhba->pdev)) {\n\t\tdev_err(&mhba->pdev->dev, \"set master failed\\n\");\n\t\treturn FAILED;\n\t}\n\tmvumi_restore_bar_addr(mhba);\n\tif (mvumi_wait_for_fw(mhba) == FAILED)\n\t\treturn FAILED;\n\n\treturn mvumi_wait_for_outstanding(mhba);\n}\n\nstatic int mvumi_reset_host_9143(struct mvumi_hba *mhba)\n{\n\treturn mvumi_wait_for_outstanding(mhba);\n}\n\nstatic int mvumi_host_reset(struct scsi_cmnd *scmd)\n{\n\tstruct mvumi_hba *mhba;\n\n\tmhba = (struct mvumi_hba *) scmd->device->host->hostdata;\n\n\tscmd_printk(KERN_NOTICE, scmd, \"RESET -%u cmd=%x retries=%x\\n\",\n\t\t\tscsi_cmd_to_rq(scmd)->tag, scmd->cmnd[0], scmd->retries);\n\n\treturn mhba->instancet->reset_host(mhba);\n}\n\nstatic int mvumi_issue_blocked_cmd(struct mvumi_hba *mhba,\n\t\t\t\t\t\tstruct mvumi_cmd *cmd)\n{\n\tunsigned long flags;\n\n\tcmd->cmd_status = REQ_STATUS_PENDING;\n\n\tif (atomic_read(&cmd->sync_cmd)) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"last blocked cmd not finished, sync_cmd = %d\\n\",\n\t\t\t\t\t\tatomic_read(&cmd->sync_cmd));\n\t\tBUG_ON(1);\n\t\treturn -1;\n\t}\n\tatomic_inc(&cmd->sync_cmd);\n\tspin_lock_irqsave(mhba->shost->host_lock, flags);\n\tmhba->instancet->fire_cmd(mhba, cmd);\n\tspin_unlock_irqrestore(mhba->shost->host_lock, flags);\n\n\twait_event_timeout(mhba->int_cmd_wait_q,\n\t\t(cmd->cmd_status != REQ_STATUS_PENDING),\n\t\tMVUMI_INTERNAL_CMD_WAIT_TIME * HZ);\n\n\t \n\tif (atomic_read(&cmd->sync_cmd)) {\n\t\tspin_lock_irqsave(mhba->shost->host_lock, flags);\n\t\tatomic_dec(&cmd->sync_cmd);\n\t\tif (mhba->tag_cmd[cmd->frame->tag]) {\n\t\t\tmhba->tag_cmd[cmd->frame->tag] = NULL;\n\t\t\tdev_warn(&mhba->pdev->dev, \"TIMEOUT:release tag [%d]\\n\",\n\t\t\t\t\t\t\tcmd->frame->tag);\n\t\t\ttag_release_one(mhba, &mhba->tag_pool, cmd->frame->tag);\n\t\t}\n\t\tif (!list_empty(&cmd->queue_pointer)) {\n\t\t\tdev_warn(&mhba->pdev->dev,\n\t\t\t\t\"TIMEOUT:A internal command doesn't send!\\n\");\n\t\t\tlist_del_init(&cmd->queue_pointer);\n\t\t} else\n\t\t\tatomic_dec(&mhba->fw_outstanding);\n\n\t\tspin_unlock_irqrestore(mhba->shost->host_lock, flags);\n\t}\n\treturn 0;\n}\n\nstatic void mvumi_release_fw(struct mvumi_hba *mhba)\n{\n\tmvumi_free_cmds(mhba);\n\tmvumi_release_mem_resource(mhba);\n\tmvumi_unmap_pci_addr(mhba->pdev, mhba->base_addr);\n\tdma_free_coherent(&mhba->pdev->dev, HSP_MAX_SIZE,\n\t\tmhba->handshake_page, mhba->handshake_page_phys);\n\tkfree(mhba->regs);\n\tpci_release_regions(mhba->pdev);\n}\n\nstatic unsigned char mvumi_flush_cache(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_cmd *cmd;\n\tstruct mvumi_msg_frame *frame;\n\tunsigned char device_id, retry = 0;\n\tunsigned char bitcount = sizeof(unsigned char) * 8;\n\n\tfor (device_id = 0; device_id < mhba->max_target_id; device_id++) {\n\t\tif (!(mhba->target_map[device_id / bitcount] &\n\t\t\t\t(1 << (device_id % bitcount))))\n\t\t\tcontinue;\nget_cmd:\tcmd = mvumi_create_internal_cmd(mhba, 0);\n\t\tif (!cmd) {\n\t\t\tif (retry++ >= 5) {\n\t\t\t\tdev_err(&mhba->pdev->dev, \"failed to get memory\"\n\t\t\t\t\t\" for internal flush cache cmd for \"\n\t\t\t\t\t\"device %d\", device_id);\n\t\t\t\tretry = 0;\n\t\t\t\tcontinue;\n\t\t\t} else\n\t\t\t\tgoto get_cmd;\n\t\t}\n\t\tcmd->scmd = NULL;\n\t\tcmd->cmd_status = REQ_STATUS_PENDING;\n\t\tatomic_set(&cmd->sync_cmd, 0);\n\t\tframe = cmd->frame;\n\t\tframe->req_function = CL_FUN_SCSI_CMD;\n\t\tframe->device_id = device_id;\n\t\tframe->cmd_flag = CMD_FLAG_NON_DATA;\n\t\tframe->data_transfer_length = 0;\n\t\tframe->cdb_length = MAX_COMMAND_SIZE;\n\t\tmemset(frame->cdb, 0, MAX_COMMAND_SIZE);\n\t\tframe->cdb[0] = SCSI_CMD_MARVELL_SPECIFIC;\n\t\tframe->cdb[1] = CDB_CORE_MODULE;\n\t\tframe->cdb[2] = CDB_CORE_SHUTDOWN;\n\n\t\tmvumi_issue_blocked_cmd(mhba, cmd);\n\t\tif (cmd->cmd_status != SAM_STAT_GOOD) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"device %d flush cache failed, status=0x%x.\\n\",\n\t\t\t\tdevice_id, cmd->cmd_status);\n\t\t}\n\n\t\tmvumi_delete_internal_cmd(mhba, cmd);\n\t}\n\treturn 0;\n}\n\nstatic unsigned char\nmvumi_calculate_checksum(struct mvumi_hs_header *p_header,\n\t\t\t\t\t\t\tunsigned short len)\n{\n\tunsigned char *ptr;\n\tunsigned char ret = 0, i;\n\n\tptr = (unsigned char *) p_header->frame_content;\n\tfor (i = 0; i < len; i++) {\n\t\tret ^= *ptr;\n\t\tptr++;\n\t}\n\n\treturn ret;\n}\n\nstatic void mvumi_hs_build_page(struct mvumi_hba *mhba,\n\t\t\t\tstruct mvumi_hs_header *hs_header)\n{\n\tstruct mvumi_hs_page2 *hs_page2;\n\tstruct mvumi_hs_page4 *hs_page4;\n\tstruct mvumi_hs_page3 *hs_page3;\n\tu64 time;\n\tu64 local_time;\n\n\tswitch (hs_header->page_code) {\n\tcase HS_PAGE_HOST_INFO:\n\t\ths_page2 = (struct mvumi_hs_page2 *) hs_header;\n\t\ths_header->frame_length = sizeof(*hs_page2) - 4;\n\t\tmemset(hs_header->frame_content, 0, hs_header->frame_length);\n\t\ths_page2->host_type = 3;  \n\t\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC)\n\t\t\ths_page2->host_cap = 0x08; \n\t\ths_page2->host_ver.ver_major = VER_MAJOR;\n\t\ths_page2->host_ver.ver_minor = VER_MINOR;\n\t\ths_page2->host_ver.ver_oem = VER_OEM;\n\t\ths_page2->host_ver.ver_build = VER_BUILD;\n\t\ths_page2->system_io_bus = 0;\n\t\ths_page2->slot_number = 0;\n\t\ths_page2->intr_level = 0;\n\t\ths_page2->intr_vector = 0;\n\t\ttime = ktime_get_real_seconds();\n\t\tlocal_time = (time - (sys_tz.tz_minuteswest * 60));\n\t\ths_page2->seconds_since1970 = local_time;\n\t\ths_header->checksum = mvumi_calculate_checksum(hs_header,\n\t\t\t\t\t\ths_header->frame_length);\n\t\tbreak;\n\n\tcase HS_PAGE_FIRM_CTL:\n\t\ths_page3 = (struct mvumi_hs_page3 *) hs_header;\n\t\ths_header->frame_length = sizeof(*hs_page3) - 4;\n\t\tmemset(hs_header->frame_content, 0, hs_header->frame_length);\n\t\ths_header->checksum = mvumi_calculate_checksum(hs_header,\n\t\t\t\t\t\ths_header->frame_length);\n\t\tbreak;\n\n\tcase HS_PAGE_CL_INFO:\n\t\ths_page4 = (struct mvumi_hs_page4 *) hs_header;\n\t\ths_header->frame_length = sizeof(*hs_page4) - 4;\n\t\tmemset(hs_header->frame_content, 0, hs_header->frame_length);\n\t\ths_page4->ib_baseaddr_l = lower_32_bits(mhba->ib_list_phys);\n\t\ths_page4->ib_baseaddr_h = upper_32_bits(mhba->ib_list_phys);\n\n\t\ths_page4->ob_baseaddr_l = lower_32_bits(mhba->ob_list_phys);\n\t\ths_page4->ob_baseaddr_h = upper_32_bits(mhba->ob_list_phys);\n\t\ths_page4->ib_entry_size = mhba->ib_max_size_setting;\n\t\ths_page4->ob_entry_size = mhba->ob_max_size_setting;\n\t\tif (mhba->hba_capability\n\t\t\t& HS_CAPABILITY_NEW_PAGE_IO_DEPTH_DEF) {\n\t\t\ths_page4->ob_depth = find_first_bit((unsigned long *)\n\t\t\t\t\t\t\t    &mhba->list_num_io,\n\t\t\t\t\t\t\t    BITS_PER_LONG);\n\t\t\ths_page4->ib_depth = find_first_bit((unsigned long *)\n\t\t\t\t\t\t\t    &mhba->list_num_io,\n\t\t\t\t\t\t\t    BITS_PER_LONG);\n\t\t} else {\n\t\t\ths_page4->ob_depth = (u8) mhba->list_num_io;\n\t\t\ths_page4->ib_depth = (u8) mhba->list_num_io;\n\t\t}\n\t\ths_header->checksum = mvumi_calculate_checksum(hs_header,\n\t\t\t\t\t\ths_header->frame_length);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&mhba->pdev->dev, \"cannot build page, code[0x%x]\\n\",\n\t\t\ths_header->page_code);\n\t\tbreak;\n\t}\n}\n\n \nstatic int mvumi_init_data(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_ob_data *ob_pool;\n\tstruct mvumi_res *res_mgnt;\n\tunsigned int tmp_size, offset, i;\n\tvoid *virmem, *v;\n\tdma_addr_t p;\n\n\tif (mhba->fw_flag & MVUMI_FW_ALLOC)\n\t\treturn 0;\n\n\ttmp_size = mhba->ib_max_size * mhba->max_io;\n\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC)\n\t\ttmp_size += sizeof(struct mvumi_dyn_list_entry) * mhba->max_io;\n\n\ttmp_size += 128 + mhba->ob_max_size * mhba->max_io;\n\ttmp_size += 8 + sizeof(u32)*2 + 16;\n\n\tres_mgnt = mvumi_alloc_mem_resource(mhba,\n\t\t\t\t\tRESOURCE_UNCACHED_MEMORY, tmp_size);\n\tif (!res_mgnt) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for inbound list\\n\");\n\t\tgoto fail_alloc_dma_buf;\n\t}\n\n\tp = res_mgnt->bus_addr;\n\tv = res_mgnt->virt_addr;\n\t \n\toffset = round_up(p, 128) - p;\n\tp += offset;\n\tv += offset;\n\tmhba->ib_list = v;\n\tmhba->ib_list_phys = p;\n\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC) {\n\t\tv += sizeof(struct mvumi_dyn_list_entry) * mhba->max_io;\n\t\tp += sizeof(struct mvumi_dyn_list_entry) * mhba->max_io;\n\t\tmhba->ib_frame = v;\n\t\tmhba->ib_frame_phys = p;\n\t}\n\tv += mhba->ib_max_size * mhba->max_io;\n\tp += mhba->ib_max_size * mhba->max_io;\n\n\t \n\toffset = round_up(p, 8) - p;\n\tp += offset;\n\tv += offset;\n\tmhba->ib_shadow = v;\n\tmhba->ib_shadow_phys = p;\n\tp += sizeof(u32)*2;\n\tv += sizeof(u32)*2;\n\t \n\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9580) {\n\t\toffset = round_up(p, 8) - p;\n\t\tp += offset;\n\t\tv += offset;\n\t\tmhba->ob_shadow = v;\n\t\tmhba->ob_shadow_phys = p;\n\t\tp += 8;\n\t\tv += 8;\n\t} else {\n\t\toffset = round_up(p, 4) - p;\n\t\tp += offset;\n\t\tv += offset;\n\t\tmhba->ob_shadow = v;\n\t\tmhba->ob_shadow_phys = p;\n\t\tp += 4;\n\t\tv += 4;\n\t}\n\n\t \n\toffset = round_up(p, 128) - p;\n\tp += offset;\n\tv += offset;\n\n\tmhba->ob_list = v;\n\tmhba->ob_list_phys = p;\n\n\t \n\ttmp_size = mhba->max_io * (mhba->ob_max_size + sizeof(*ob_pool));\n\ttmp_size = round_up(tmp_size, 8);\n\n\tres_mgnt = mvumi_alloc_mem_resource(mhba,\n\t\t\t\tRESOURCE_CACHED_MEMORY, tmp_size);\n\tif (!res_mgnt) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for outbound data buffer\\n\");\n\t\tgoto fail_alloc_dma_buf;\n\t}\n\tvirmem = res_mgnt->virt_addr;\n\n\tfor (i = mhba->max_io; i != 0; i--) {\n\t\tob_pool = (struct mvumi_ob_data *) virmem;\n\t\tlist_add_tail(&ob_pool->list, &mhba->ob_data_list);\n\t\tvirmem += mhba->ob_max_size + sizeof(*ob_pool);\n\t}\n\n\ttmp_size = sizeof(unsigned short) * mhba->max_io +\n\t\t\t\tsizeof(struct mvumi_cmd *) * mhba->max_io;\n\ttmp_size += round_up(mhba->max_target_id, sizeof(unsigned char) * 8) /\n\t\t\t\t\t\t(sizeof(unsigned char) * 8);\n\n\tres_mgnt = mvumi_alloc_mem_resource(mhba,\n\t\t\t\tRESOURCE_CACHED_MEMORY, tmp_size);\n\tif (!res_mgnt) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for tag and target map\\n\");\n\t\tgoto fail_alloc_dma_buf;\n\t}\n\n\tvirmem = res_mgnt->virt_addr;\n\tmhba->tag_pool.stack = virmem;\n\tmhba->tag_pool.size = mhba->max_io;\n\ttag_init(&mhba->tag_pool, mhba->max_io);\n\tvirmem += sizeof(unsigned short) * mhba->max_io;\n\n\tmhba->tag_cmd = virmem;\n\tvirmem += sizeof(struct mvumi_cmd *) * mhba->max_io;\n\n\tmhba->target_map = virmem;\n\n\tmhba->fw_flag |= MVUMI_FW_ALLOC;\n\treturn 0;\n\nfail_alloc_dma_buf:\n\tmvumi_release_mem_resource(mhba);\n\treturn -1;\n}\n\nstatic int mvumi_hs_process_page(struct mvumi_hba *mhba,\n\t\t\t\tstruct mvumi_hs_header *hs_header)\n{\n\tstruct mvumi_hs_page1 *hs_page1;\n\tunsigned char page_checksum;\n\n\tpage_checksum = mvumi_calculate_checksum(hs_header,\n\t\t\t\t\t\ths_header->frame_length);\n\tif (page_checksum != hs_header->checksum) {\n\t\tdev_err(&mhba->pdev->dev, \"checksum error\\n\");\n\t\treturn -1;\n\t}\n\n\tswitch (hs_header->page_code) {\n\tcase HS_PAGE_FIRM_CAP:\n\t\ths_page1 = (struct mvumi_hs_page1 *) hs_header;\n\n\t\tmhba->max_io = hs_page1->max_io_support;\n\t\tmhba->list_num_io = hs_page1->cl_inout_list_depth;\n\t\tmhba->max_transfer_size = hs_page1->max_transfer_size;\n\t\tmhba->max_target_id = hs_page1->max_devices_support;\n\t\tmhba->hba_capability = hs_page1->capability;\n\t\tmhba->ib_max_size_setting = hs_page1->cl_in_max_entry_size;\n\t\tmhba->ib_max_size = (1 << hs_page1->cl_in_max_entry_size) << 2;\n\n\t\tmhba->ob_max_size_setting = hs_page1->cl_out_max_entry_size;\n\t\tmhba->ob_max_size = (1 << hs_page1->cl_out_max_entry_size) << 2;\n\n\t\tdev_dbg(&mhba->pdev->dev, \"FW version:%d\\n\",\n\t\t\t\t\t\ths_page1->fw_ver.ver_build);\n\n\t\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_COMPACT_SG)\n\t\t\tmhba->eot_flag = 22;\n\t\telse\n\t\t\tmhba->eot_flag = 27;\n\t\tif (mhba->hba_capability & HS_CAPABILITY_NEW_PAGE_IO_DEPTH_DEF)\n\t\t\tmhba->list_num_io = 1 << hs_page1->cl_inout_list_depth;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&mhba->pdev->dev, \"handshake: page code error\\n\");\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n \nstatic int mvumi_handshake(struct mvumi_hba *mhba)\n{\n\tunsigned int hs_state, tmp, hs_fun;\n\tstruct mvumi_hs_header *hs_header;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tif (mhba->fw_state == FW_STATE_STARTING)\n\t\ths_state = HS_S_START;\n\telse {\n\t\ttmp = ioread32(regs->arm_to_pciea_msg0);\n\t\ths_state = HS_GET_STATE(tmp);\n\t\tdev_dbg(&mhba->pdev->dev, \"handshake state[0x%x].\\n\", hs_state);\n\t\tif (HS_GET_STATUS(tmp) != HS_STATUS_OK) {\n\t\t\tmhba->fw_state = FW_STATE_STARTING;\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\ths_fun = 0;\n\tswitch (hs_state) {\n\tcase HS_S_START:\n\t\tmhba->fw_state = FW_STATE_HANDSHAKING;\n\t\tHS_SET_STATUS(hs_fun, HS_STATUS_OK);\n\t\tHS_SET_STATE(hs_fun, HS_S_RESET);\n\t\tiowrite32(HANDSHAKE_SIGNATURE, regs->pciea_to_arm_msg1);\n\t\tiowrite32(hs_fun, regs->pciea_to_arm_msg0);\n\t\tiowrite32(DRBL_HANDSHAKE, regs->pciea_to_arm_drbl_reg);\n\t\tbreak;\n\n\tcase HS_S_RESET:\n\t\tiowrite32(lower_32_bits(mhba->handshake_page_phys),\n\t\t\t\t\tregs->pciea_to_arm_msg1);\n\t\tiowrite32(upper_32_bits(mhba->handshake_page_phys),\n\t\t\t\t\tregs->arm_to_pciea_msg1);\n\t\tHS_SET_STATUS(hs_fun, HS_STATUS_OK);\n\t\tHS_SET_STATE(hs_fun, HS_S_PAGE_ADDR);\n\t\tiowrite32(hs_fun, regs->pciea_to_arm_msg0);\n\t\tiowrite32(DRBL_HANDSHAKE, regs->pciea_to_arm_drbl_reg);\n\t\tbreak;\n\n\tcase HS_S_PAGE_ADDR:\n\tcase HS_S_QUERY_PAGE:\n\tcase HS_S_SEND_PAGE:\n\t\ths_header = (struct mvumi_hs_header *) mhba->handshake_page;\n\t\tif (hs_header->page_code == HS_PAGE_FIRM_CAP) {\n\t\t\tmhba->hba_total_pages =\n\t\t\t((struct mvumi_hs_page1 *) hs_header)->total_pages;\n\n\t\t\tif (mhba->hba_total_pages == 0)\n\t\t\t\tmhba->hba_total_pages = HS_PAGE_TOTAL-1;\n\t\t}\n\n\t\tif (hs_state == HS_S_QUERY_PAGE) {\n\t\t\tif (mvumi_hs_process_page(mhba, hs_header)) {\n\t\t\t\tHS_SET_STATE(hs_fun, HS_S_ABORT);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tif (mvumi_init_data(mhba)) {\n\t\t\t\tHS_SET_STATE(hs_fun, HS_S_ABORT);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else if (hs_state == HS_S_PAGE_ADDR) {\n\t\t\ths_header->page_code = 0;\n\t\t\tmhba->hba_total_pages = HS_PAGE_TOTAL-1;\n\t\t}\n\n\t\tif ((hs_header->page_code + 1) <= mhba->hba_total_pages) {\n\t\t\ths_header->page_code++;\n\t\t\tif (hs_header->page_code != HS_PAGE_FIRM_CAP) {\n\t\t\t\tmvumi_hs_build_page(mhba, hs_header);\n\t\t\t\tHS_SET_STATE(hs_fun, HS_S_SEND_PAGE);\n\t\t\t} else\n\t\t\t\tHS_SET_STATE(hs_fun, HS_S_QUERY_PAGE);\n\t\t} else\n\t\t\tHS_SET_STATE(hs_fun, HS_S_END);\n\n\t\tHS_SET_STATUS(hs_fun, HS_STATUS_OK);\n\t\tiowrite32(hs_fun, regs->pciea_to_arm_msg0);\n\t\tiowrite32(DRBL_HANDSHAKE, regs->pciea_to_arm_drbl_reg);\n\t\tbreak;\n\n\tcase HS_S_END:\n\t\t \n\t\ttmp = ioread32(regs->enpointa_mask_reg);\n\t\ttmp |= regs->int_comaout | regs->int_comaerr;\n\t\tiowrite32(tmp, regs->enpointa_mask_reg);\n\t\tiowrite32(mhba->list_num_io, mhba->ib_shadow);\n\t\t \n\t\tiowrite32(lower_32_bits(mhba->ib_shadow_phys),\n\t\t\t\t\tregs->inb_aval_count_basel);\n\t\tiowrite32(upper_32_bits(mhba->ib_shadow_phys),\n\t\t\t\t\tregs->inb_aval_count_baseh);\n\n\t\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9143) {\n\t\t\t \n\t\t\tiowrite32((mhba->list_num_io-1) |\n\t\t\t\t\t\t\tregs->cl_pointer_toggle,\n\t\t\t\t\t\t\tmhba->ob_shadow);\n\t\t\tiowrite32(lower_32_bits(mhba->ob_shadow_phys),\n\t\t\t\t\t\t\tregs->outb_copy_basel);\n\t\t\tiowrite32(upper_32_bits(mhba->ob_shadow_phys),\n\t\t\t\t\t\t\tregs->outb_copy_baseh);\n\t\t}\n\n\t\tmhba->ib_cur_slot = (mhba->list_num_io - 1) |\n\t\t\t\t\t\t\tregs->cl_pointer_toggle;\n\t\tmhba->ob_cur_slot = (mhba->list_num_io - 1) |\n\t\t\t\t\t\t\tregs->cl_pointer_toggle;\n\t\tmhba->fw_state = FW_STATE_STARTED;\n\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&mhba->pdev->dev, \"unknown handshake state [0x%x].\\n\",\n\t\t\t\t\t\t\t\ths_state);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic unsigned char mvumi_handshake_event(struct mvumi_hba *mhba)\n{\n\tunsigned int isr_status;\n\tunsigned long before;\n\n\tbefore = jiffies;\n\tmvumi_handshake(mhba);\n\tdo {\n\t\tisr_status = mhba->instancet->read_fw_status_reg(mhba);\n\n\t\tif (mhba->fw_state == FW_STATE_STARTED)\n\t\t\treturn 0;\n\t\tif (time_after(jiffies, before + FW_MAX_DELAY * HZ)) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"no handshake response at state 0x%x.\\n\",\n\t\t\t\t  mhba->fw_state);\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"isr : global=0x%x,status=0x%x.\\n\",\n\t\t\t\t\tmhba->global_isr, isr_status);\n\t\t\treturn -1;\n\t\t}\n\t\trmb();\n\t\tusleep_range(1000, 2000);\n\t} while (!(isr_status & DRBL_HANDSHAKE_ISR));\n\n\treturn 0;\n}\n\nstatic unsigned char mvumi_check_handshake(struct mvumi_hba *mhba)\n{\n\tunsigned int tmp;\n\tunsigned long before;\n\n\tbefore = jiffies;\n\ttmp = ioread32(mhba->regs->arm_to_pciea_msg1);\n\twhile ((tmp != HANDSHAKE_READYSTATE) && (tmp != HANDSHAKE_DONESTATE)) {\n\t\tif (tmp != HANDSHAKE_READYSTATE)\n\t\t\tiowrite32(DRBL_MU_RESET,\n\t\t\t\t\tmhba->regs->pciea_to_arm_drbl_reg);\n\t\tif (time_after(jiffies, before + FW_MAX_DELAY * HZ)) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\"invalid signature [0x%x].\\n\", tmp);\n\t\t\treturn -1;\n\t\t}\n\t\tusleep_range(1000, 2000);\n\t\trmb();\n\t\ttmp = ioread32(mhba->regs->arm_to_pciea_msg1);\n\t}\n\n\tmhba->fw_state = FW_STATE_STARTING;\n\tdev_dbg(&mhba->pdev->dev, \"start firmware handshake...\\n\");\n\tdo {\n\t\tif (mvumi_handshake_event(mhba)) {\n\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\t\"handshake failed at state 0x%x.\\n\",\n\t\t\t\t\t\tmhba->fw_state);\n\t\t\treturn -1;\n\t\t}\n\t} while (mhba->fw_state != FW_STATE_STARTED);\n\n\tdev_dbg(&mhba->pdev->dev, \"firmware handshake done\\n\");\n\n\treturn 0;\n}\n\nstatic unsigned char mvumi_start(struct mvumi_hba *mhba)\n{\n\tunsigned int tmp;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\t \n\ttmp = ioread32(regs->arm_to_pciea_drbl_reg);\n\tiowrite32(tmp, regs->arm_to_pciea_drbl_reg);\n\n\tiowrite32(regs->int_drbl_int_mask, regs->arm_to_pciea_mask_reg);\n\ttmp = ioread32(regs->enpointa_mask_reg) | regs->int_dl_cpu2pciea;\n\tiowrite32(tmp, regs->enpointa_mask_reg);\n\tmsleep(100);\n\tif (mvumi_check_handshake(mhba))\n\t\treturn -1;\n\n\treturn 0;\n}\n\n \nstatic void mvumi_complete_cmd(struct mvumi_hba *mhba, struct mvumi_cmd *cmd,\n\t\t\t\t\tstruct mvumi_rsp_frame *ob_frame)\n{\n\tstruct scsi_cmnd *scmd = cmd->scmd;\n\n\tmvumi_priv(cmd->scmd)->cmd_priv = NULL;\n\tscmd->result = ob_frame->req_status;\n\n\tswitch (ob_frame->req_status) {\n\tcase SAM_STAT_GOOD:\n\t\tscmd->result |= DID_OK << 16;\n\t\tbreak;\n\tcase SAM_STAT_BUSY:\n\t\tscmd->result |= DID_BUS_BUSY << 16;\n\t\tbreak;\n\tcase SAM_STAT_CHECK_CONDITION:\n\t\tscmd->result |= (DID_OK << 16);\n\t\tif (ob_frame->rsp_flag & CL_RSP_FLAG_SENSEDATA) {\n\t\t\tmemcpy(cmd->scmd->sense_buffer, ob_frame->payload,\n\t\t\t\tsizeof(struct mvumi_sense_data));\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tscmd->result |= (DID_ABORT << 16);\n\t\tbreak;\n\t}\n\n\tif (scsi_bufflen(scmd))\n\t\tdma_unmap_sg(&mhba->pdev->dev, scsi_sglist(scmd),\n\t\t\t     scsi_sg_count(scmd),\n\t\t\t     scmd->sc_data_direction);\n\tscsi_done(scmd);\n\tmvumi_return_cmd(mhba, cmd);\n}\n\nstatic void mvumi_complete_internal_cmd(struct mvumi_hba *mhba,\n\t\t\t\t\t\tstruct mvumi_cmd *cmd,\n\t\t\t\t\tstruct mvumi_rsp_frame *ob_frame)\n{\n\tif (atomic_read(&cmd->sync_cmd)) {\n\t\tcmd->cmd_status = ob_frame->req_status;\n\n\t\tif ((ob_frame->req_status == SAM_STAT_CHECK_CONDITION) &&\n\t\t\t\t(ob_frame->rsp_flag & CL_RSP_FLAG_SENSEDATA) &&\n\t\t\t\tcmd->data_buf) {\n\t\t\tmemcpy(cmd->data_buf, ob_frame->payload,\n\t\t\t\t\tsizeof(struct mvumi_sense_data));\n\t\t}\n\t\tatomic_dec(&cmd->sync_cmd);\n\t\twake_up(&mhba->int_cmd_wait_q);\n\t}\n}\n\nstatic void mvumi_show_event(struct mvumi_hba *mhba,\n\t\t\tstruct mvumi_driver_event *ptr)\n{\n\tunsigned int i;\n\n\tdev_warn(&mhba->pdev->dev,\n\t\t\"Event[0x%x] id[0x%x] severity[0x%x] device id[0x%x]\\n\",\n\t\tptr->sequence_no, ptr->event_id, ptr->severity, ptr->device_id);\n\tif (ptr->param_count) {\n\t\tprintk(KERN_WARNING \"Event param(len 0x%x): \",\n\t\t\t\t\t\tptr->param_count);\n\t\tfor (i = 0; i < ptr->param_count; i++)\n\t\t\tprintk(KERN_WARNING \"0x%x \", ptr->params[i]);\n\n\t\tprintk(KERN_WARNING \"\\n\");\n\t}\n\n\tif (ptr->sense_data_length) {\n\t\tprintk(KERN_WARNING \"Event sense data(len 0x%x): \",\n\t\t\t\t\t\tptr->sense_data_length);\n\t\tfor (i = 0; i < ptr->sense_data_length; i++)\n\t\t\tprintk(KERN_WARNING \"0x%x \", ptr->sense_data[i]);\n\t\tprintk(KERN_WARNING \"\\n\");\n\t}\n}\n\nstatic int mvumi_handle_hotplug(struct mvumi_hba *mhba, u16 devid, int status)\n{\n\tstruct scsi_device *sdev;\n\tint ret = -1;\n\n\tif (status == DEVICE_OFFLINE) {\n\t\tsdev = scsi_device_lookup(mhba->shost, 0, devid, 0);\n\t\tif (sdev) {\n\t\t\tdev_dbg(&mhba->pdev->dev, \"remove disk %d-%d-%d.\\n\", 0,\n\t\t\t\t\t\t\t\tsdev->id, 0);\n\t\t\tscsi_remove_device(sdev);\n\t\t\tscsi_device_put(sdev);\n\t\t\tret = 0;\n\t\t} else\n\t\t\tdev_err(&mhba->pdev->dev, \" no disk[%d] to remove\\n\",\n\t\t\t\t\t\t\t\t\tdevid);\n\t} else if (status == DEVICE_ONLINE) {\n\t\tsdev = scsi_device_lookup(mhba->shost, 0, devid, 0);\n\t\tif (!sdev) {\n\t\t\tscsi_add_device(mhba->shost, 0, devid, 0);\n\t\t\tdev_dbg(&mhba->pdev->dev, \" add disk %d-%d-%d.\\n\", 0,\n\t\t\t\t\t\t\t\tdevid, 0);\n\t\t\tret = 0;\n\t\t} else {\n\t\t\tdev_err(&mhba->pdev->dev, \" don't add disk %d-%d-%d.\\n\",\n\t\t\t\t\t\t\t\t0, devid, 0);\n\t\t\tscsi_device_put(sdev);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic u64 mvumi_inquiry(struct mvumi_hba *mhba,\n\tunsigned int id, struct mvumi_cmd *cmd)\n{\n\tstruct mvumi_msg_frame *frame;\n\tu64 wwid = 0;\n\tint cmd_alloc = 0;\n\tint data_buf_len = 64;\n\n\tif (!cmd) {\n\t\tcmd = mvumi_create_internal_cmd(mhba, data_buf_len);\n\t\tif (cmd)\n\t\t\tcmd_alloc = 1;\n\t\telse\n\t\t\treturn 0;\n\t} else {\n\t\tmemset(cmd->data_buf, 0, data_buf_len);\n\t}\n\tcmd->scmd = NULL;\n\tcmd->cmd_status = REQ_STATUS_PENDING;\n\tatomic_set(&cmd->sync_cmd, 0);\n\tframe = cmd->frame;\n\tframe->device_id = (u16) id;\n\tframe->cmd_flag = CMD_FLAG_DATA_IN;\n\tframe->req_function = CL_FUN_SCSI_CMD;\n\tframe->cdb_length = 6;\n\tframe->data_transfer_length = MVUMI_INQUIRY_LENGTH;\n\tmemset(frame->cdb, 0, frame->cdb_length);\n\tframe->cdb[0] = INQUIRY;\n\tframe->cdb[4] = frame->data_transfer_length;\n\n\tmvumi_issue_blocked_cmd(mhba, cmd);\n\n\tif (cmd->cmd_status == SAM_STAT_GOOD) {\n\t\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9143)\n\t\t\twwid = id + 1;\n\t\telse\n\t\t\tmemcpy((void *)&wwid,\n\t\t\t       (cmd->data_buf + MVUMI_INQUIRY_UUID_OFF),\n\t\t\t       MVUMI_INQUIRY_UUID_LEN);\n\t\tdev_dbg(&mhba->pdev->dev,\n\t\t\t\"inquiry device(0:%d:0) wwid(%llx)\\n\", id, wwid);\n\t} else {\n\t\twwid = 0;\n\t}\n\tif (cmd_alloc)\n\t\tmvumi_delete_internal_cmd(mhba, cmd);\n\n\treturn wwid;\n}\n\nstatic void mvumi_detach_devices(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_device *mv_dev = NULL , *dev_next;\n\tstruct scsi_device *sdev = NULL;\n\n\tmutex_lock(&mhba->device_lock);\n\n\t \n\tlist_for_each_entry_safe(mv_dev, dev_next,\n\t\t&mhba->shost_dev_list, list) {\n\t\tmvumi_handle_hotplug(mhba, mv_dev->id, DEVICE_OFFLINE);\n\t\tlist_del_init(&mv_dev->list);\n\t\tdev_dbg(&mhba->pdev->dev, \"release device(0:%d:0) wwid(%llx)\\n\",\n\t\t\tmv_dev->id, mv_dev->wwid);\n\t\tkfree(mv_dev);\n\t}\n\tlist_for_each_entry_safe(mv_dev, dev_next, &mhba->mhba_dev_list, list) {\n\t\tlist_del_init(&mv_dev->list);\n\t\tdev_dbg(&mhba->pdev->dev, \"release device(0:%d:0) wwid(%llx)\\n\",\n\t\t\tmv_dev->id, mv_dev->wwid);\n\t\tkfree(mv_dev);\n\t}\n\n\t \n\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9580)\n\t\tsdev = scsi_device_lookup(mhba->shost, 0,\n\t\t\t\t\t\tmhba->max_target_id - 1, 0);\n\n\tif (sdev) {\n\t\tscsi_remove_device(sdev);\n\t\tscsi_device_put(sdev);\n\t}\n\n\tmutex_unlock(&mhba->device_lock);\n}\n\nstatic void mvumi_rescan_devices(struct mvumi_hba *mhba, int id)\n{\n\tstruct scsi_device *sdev;\n\n\tsdev = scsi_device_lookup(mhba->shost, 0, id, 0);\n\tif (sdev) {\n\t\tscsi_rescan_device(sdev);\n\t\tscsi_device_put(sdev);\n\t}\n}\n\nstatic int mvumi_match_devices(struct mvumi_hba *mhba, int id, u64 wwid)\n{\n\tstruct mvumi_device *mv_dev = NULL;\n\n\tlist_for_each_entry(mv_dev, &mhba->shost_dev_list, list) {\n\t\tif (mv_dev->wwid == wwid) {\n\t\t\tif (mv_dev->id != id) {\n\t\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\t\"%s has same wwid[%llx] ,\"\n\t\t\t\t\t\" but different id[%d %d]\\n\",\n\t\t\t\t\t__func__, mv_dev->wwid, mv_dev->id, id);\n\t\t\t\treturn -1;\n\t\t\t} else {\n\t\t\t\tif (mhba->pdev->device ==\n\t\t\t\t\t\tPCI_DEVICE_ID_MARVELL_MV9143)\n\t\t\t\t\tmvumi_rescan_devices(mhba, id);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void mvumi_remove_devices(struct mvumi_hba *mhba, int id)\n{\n\tstruct mvumi_device *mv_dev = NULL, *dev_next;\n\n\tlist_for_each_entry_safe(mv_dev, dev_next,\n\t\t\t\t&mhba->shost_dev_list, list) {\n\t\tif (mv_dev->id == id) {\n\t\t\tdev_dbg(&mhba->pdev->dev,\n\t\t\t\t\"detach device(0:%d:0) wwid(%llx) from HOST\\n\",\n\t\t\t\tmv_dev->id, mv_dev->wwid);\n\t\t\tmvumi_handle_hotplug(mhba, mv_dev->id, DEVICE_OFFLINE);\n\t\t\tlist_del_init(&mv_dev->list);\n\t\t\tkfree(mv_dev);\n\t\t}\n\t}\n}\n\nstatic int mvumi_probe_devices(struct mvumi_hba *mhba)\n{\n\tint id, maxid;\n\tu64 wwid = 0;\n\tstruct mvumi_device *mv_dev = NULL;\n\tstruct mvumi_cmd *cmd = NULL;\n\tint found = 0;\n\n\tcmd = mvumi_create_internal_cmd(mhba, 64);\n\tif (!cmd)\n\t\treturn -1;\n\n\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9143)\n\t\tmaxid = mhba->max_target_id;\n\telse\n\t\tmaxid = mhba->max_target_id - 1;\n\n\tfor (id = 0; id < maxid; id++) {\n\t\twwid = mvumi_inquiry(mhba, id, cmd);\n\t\tif (!wwid) {\n\t\t\t \n\t\t\tmvumi_remove_devices(mhba, id);\n\t\t} else {\n\t\t\t \n\t\t\tfound = mvumi_match_devices(mhba, id, wwid);\n\t\t\tif (!found) {\n\t\t\t\tmvumi_remove_devices(mhba, id);\n\t\t\t\tmv_dev = kzalloc(sizeof(struct mvumi_device),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tif (!mv_dev) {\n\t\t\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\t\t\"%s alloc mv_dev failed\\n\",\n\t\t\t\t\t\t__func__);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tmv_dev->id = id;\n\t\t\t\tmv_dev->wwid = wwid;\n\t\t\t\tmv_dev->sdev = NULL;\n\t\t\t\tINIT_LIST_HEAD(&mv_dev->list);\n\t\t\t\tlist_add_tail(&mv_dev->list,\n\t\t\t\t\t      &mhba->mhba_dev_list);\n\t\t\t\tdev_dbg(&mhba->pdev->dev,\n\t\t\t\t\t\"probe a new device(0:%d:0)\"\n\t\t\t\t\t\" wwid(%llx)\\n\", id, mv_dev->wwid);\n\t\t\t} else if (found == -1)\n\t\t\t\treturn -1;\n\t\t\telse\n\t\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (cmd)\n\t\tmvumi_delete_internal_cmd(mhba, cmd);\n\n\treturn 0;\n}\n\nstatic int mvumi_rescan_bus(void *data)\n{\n\tint ret = 0;\n\tstruct mvumi_hba *mhba = (struct mvumi_hba *) data;\n\tstruct mvumi_device *mv_dev = NULL , *dev_next;\n\n\twhile (!kthread_should_stop()) {\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (!atomic_read(&mhba->pnp_count))\n\t\t\tschedule();\n\t\tmsleep(1000);\n\t\tatomic_set(&mhba->pnp_count, 0);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tmutex_lock(&mhba->device_lock);\n\t\tret = mvumi_probe_devices(mhba);\n\t\tif (!ret) {\n\t\t\tlist_for_each_entry_safe(mv_dev, dev_next,\n\t\t\t\t\t\t &mhba->mhba_dev_list, list) {\n\t\t\t\tif (mvumi_handle_hotplug(mhba, mv_dev->id,\n\t\t\t\t\t\t\t DEVICE_ONLINE)) {\n\t\t\t\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\t\t\t\"%s add device(0:%d:0) failed\"\n\t\t\t\t\t\t\"wwid(%llx) has exist\\n\",\n\t\t\t\t\t\t__func__,\n\t\t\t\t\t\tmv_dev->id, mv_dev->wwid);\n\t\t\t\t\tlist_del_init(&mv_dev->list);\n\t\t\t\t\tkfree(mv_dev);\n\t\t\t\t} else {\n\t\t\t\t\tlist_move_tail(&mv_dev->list,\n\t\t\t\t\t\t       &mhba->shost_dev_list);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&mhba->device_lock);\n\t}\n\treturn 0;\n}\n\nstatic void mvumi_proc_msg(struct mvumi_hba *mhba,\n\t\t\t\t\tstruct mvumi_hotplug_event *param)\n{\n\tu16 size = param->size;\n\tconst unsigned long *ar_bitmap;\n\tconst unsigned long *re_bitmap;\n\tint index;\n\n\tif (mhba->fw_flag & MVUMI_FW_ATTACH) {\n\t\tindex = -1;\n\t\tar_bitmap = (const unsigned long *) param->bitmap;\n\t\tre_bitmap = (const unsigned long *) &param->bitmap[size >> 3];\n\n\t\tmutex_lock(&mhba->sas_discovery_mutex);\n\t\tdo {\n\t\t\tindex = find_next_zero_bit(ar_bitmap, size, index + 1);\n\t\t\tif (index >= size)\n\t\t\t\tbreak;\n\t\t\tmvumi_handle_hotplug(mhba, index, DEVICE_ONLINE);\n\t\t} while (1);\n\n\t\tindex = -1;\n\t\tdo {\n\t\t\tindex = find_next_zero_bit(re_bitmap, size, index + 1);\n\t\t\tif (index >= size)\n\t\t\t\tbreak;\n\t\t\tmvumi_handle_hotplug(mhba, index, DEVICE_OFFLINE);\n\t\t} while (1);\n\t\tmutex_unlock(&mhba->sas_discovery_mutex);\n\t}\n}\n\nstatic void mvumi_notification(struct mvumi_hba *mhba, u8 msg, void *buffer)\n{\n\tif (msg == APICDB1_EVENT_GETEVENT) {\n\t\tint i, count;\n\t\tstruct mvumi_driver_event *param = NULL;\n\t\tstruct mvumi_event_req *er = buffer;\n\t\tcount = er->count;\n\t\tif (count > MAX_EVENTS_RETURNED) {\n\t\t\tdev_err(&mhba->pdev->dev, \"event count[0x%x] is bigger\"\n\t\t\t\t\t\" than max event count[0x%x].\\n\",\n\t\t\t\t\tcount, MAX_EVENTS_RETURNED);\n\t\t\treturn;\n\t\t}\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tparam = &er->events[i];\n\t\t\tmvumi_show_event(mhba, param);\n\t\t}\n\t} else if (msg == APICDB1_HOST_GETEVENT) {\n\t\tmvumi_proc_msg(mhba, buffer);\n\t}\n}\n\nstatic int mvumi_get_event(struct mvumi_hba *mhba, unsigned char msg)\n{\n\tstruct mvumi_cmd *cmd;\n\tstruct mvumi_msg_frame *frame;\n\n\tcmd = mvumi_create_internal_cmd(mhba, 512);\n\tif (!cmd)\n\t\treturn -1;\n\tcmd->scmd = NULL;\n\tcmd->cmd_status = REQ_STATUS_PENDING;\n\tatomic_set(&cmd->sync_cmd, 0);\n\tframe = cmd->frame;\n\tframe->device_id = 0;\n\tframe->cmd_flag = CMD_FLAG_DATA_IN;\n\tframe->req_function = CL_FUN_SCSI_CMD;\n\tframe->cdb_length = MAX_COMMAND_SIZE;\n\tframe->data_transfer_length = sizeof(struct mvumi_event_req);\n\tmemset(frame->cdb, 0, MAX_COMMAND_SIZE);\n\tframe->cdb[0] = APICDB0_EVENT;\n\tframe->cdb[1] = msg;\n\tmvumi_issue_blocked_cmd(mhba, cmd);\n\n\tif (cmd->cmd_status != SAM_STAT_GOOD)\n\t\tdev_err(&mhba->pdev->dev, \"get event failed, status=0x%x.\\n\",\n\t\t\t\t\t\t\tcmd->cmd_status);\n\telse\n\t\tmvumi_notification(mhba, cmd->frame->cdb[1], cmd->data_buf);\n\n\tmvumi_delete_internal_cmd(mhba, cmd);\n\treturn 0;\n}\n\nstatic void mvumi_scan_events(struct work_struct *work)\n{\n\tstruct mvumi_events_wq *mu_ev =\n\t\tcontainer_of(work, struct mvumi_events_wq, work_q);\n\n\tmvumi_get_event(mu_ev->mhba, mu_ev->event);\n\tkfree(mu_ev);\n}\n\nstatic void mvumi_launch_events(struct mvumi_hba *mhba, u32 isr_status)\n{\n\tstruct mvumi_events_wq *mu_ev;\n\n\twhile (isr_status & (DRBL_BUS_CHANGE | DRBL_EVENT_NOTIFY)) {\n\t\tif (isr_status & DRBL_BUS_CHANGE) {\n\t\t\tatomic_inc(&mhba->pnp_count);\n\t\t\twake_up_process(mhba->dm_thread);\n\t\t\tisr_status &= ~(DRBL_BUS_CHANGE);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmu_ev = kzalloc(sizeof(*mu_ev), GFP_ATOMIC);\n\t\tif (mu_ev) {\n\t\t\tINIT_WORK(&mu_ev->work_q, mvumi_scan_events);\n\t\t\tmu_ev->mhba = mhba;\n\t\t\tmu_ev->event = APICDB1_EVENT_GETEVENT;\n\t\t\tisr_status &= ~(DRBL_EVENT_NOTIFY);\n\t\t\tmu_ev->param = NULL;\n\t\t\tschedule_work(&mu_ev->work_q);\n\t\t}\n\t}\n}\n\nstatic void mvumi_handle_clob(struct mvumi_hba *mhba)\n{\n\tstruct mvumi_rsp_frame *ob_frame;\n\tstruct mvumi_cmd *cmd;\n\tstruct mvumi_ob_data *pool;\n\n\twhile (!list_empty(&mhba->free_ob_list)) {\n\t\tpool = list_first_entry(&mhba->free_ob_list,\n\t\t\t\t\t\tstruct mvumi_ob_data, list);\n\t\tlist_del_init(&pool->list);\n\t\tlist_add_tail(&pool->list, &mhba->ob_data_list);\n\n\t\tob_frame = (struct mvumi_rsp_frame *) &pool->data[0];\n\t\tcmd = mhba->tag_cmd[ob_frame->tag];\n\n\t\tatomic_dec(&mhba->fw_outstanding);\n\t\tmhba->tag_cmd[ob_frame->tag] = NULL;\n\t\ttag_release_one(mhba, &mhba->tag_pool, ob_frame->tag);\n\t\tif (cmd->scmd)\n\t\t\tmvumi_complete_cmd(mhba, cmd, ob_frame);\n\t\telse\n\t\t\tmvumi_complete_internal_cmd(mhba, cmd, ob_frame);\n\t}\n\tmhba->instancet->fire_cmd(mhba, NULL);\n}\n\nstatic irqreturn_t mvumi_isr_handler(int irq, void *devp)\n{\n\tstruct mvumi_hba *mhba = (struct mvumi_hba *) devp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(mhba->shost->host_lock, flags);\n\tif (unlikely(mhba->instancet->clear_intr(mhba) || !mhba->global_isr)) {\n\t\tspin_unlock_irqrestore(mhba->shost->host_lock, flags);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (mhba->global_isr & mhba->regs->int_dl_cpu2pciea) {\n\t\tif (mhba->isr_status & (DRBL_BUS_CHANGE | DRBL_EVENT_NOTIFY))\n\t\t\tmvumi_launch_events(mhba, mhba->isr_status);\n\t\tif (mhba->isr_status & DRBL_HANDSHAKE_ISR) {\n\t\t\tdev_warn(&mhba->pdev->dev, \"enter handshake again!\\n\");\n\t\t\tmvumi_handshake(mhba);\n\t\t}\n\n\t}\n\n\tif (mhba->global_isr & mhba->regs->int_comaout)\n\t\tmvumi_receive_ob_list_entry(mhba);\n\n\tmhba->global_isr = 0;\n\tmhba->isr_status = 0;\n\tif (mhba->fw_state == FW_STATE_STARTED)\n\t\tmvumi_handle_clob(mhba);\n\tspin_unlock_irqrestore(mhba->shost->host_lock, flags);\n\treturn IRQ_HANDLED;\n}\n\nstatic enum mvumi_qc_result mvumi_send_command(struct mvumi_hba *mhba,\n\t\t\t\t\t\tstruct mvumi_cmd *cmd)\n{\n\tvoid *ib_entry;\n\tstruct mvumi_msg_frame *ib_frame;\n\tunsigned int frame_len;\n\n\tib_frame = cmd->frame;\n\tif (unlikely(mhba->fw_state != FW_STATE_STARTED)) {\n\t\tdev_dbg(&mhba->pdev->dev, \"firmware not ready.\\n\");\n\t\treturn MV_QUEUE_COMMAND_RESULT_NO_RESOURCE;\n\t}\n\tif (tag_is_empty(&mhba->tag_pool)) {\n\t\tdev_dbg(&mhba->pdev->dev, \"no free tag.\\n\");\n\t\treturn MV_QUEUE_COMMAND_RESULT_NO_RESOURCE;\n\t}\n\tmvumi_get_ib_list_entry(mhba, &ib_entry);\n\n\tcmd->frame->tag = tag_get_one(mhba, &mhba->tag_pool);\n\tcmd->frame->request_id = mhba->io_seq++;\n\tcmd->request_id = cmd->frame->request_id;\n\tmhba->tag_cmd[cmd->frame->tag] = cmd;\n\tframe_len = sizeof(*ib_frame) +\n\t\t\t\tib_frame->sg_counts * sizeof(struct mvumi_sgl);\n\tif (mhba->hba_capability & HS_CAPABILITY_SUPPORT_DYN_SRC) {\n\t\tstruct mvumi_dyn_list_entry *dle;\n\t\tdle = ib_entry;\n\t\tdle->src_low_addr =\n\t\t\tcpu_to_le32(lower_32_bits(cmd->frame_phys));\n\t\tdle->src_high_addr =\n\t\t\tcpu_to_le32(upper_32_bits(cmd->frame_phys));\n\t\tdle->if_length = (frame_len >> 2) & 0xFFF;\n\t} else {\n\t\tmemcpy(ib_entry, ib_frame, frame_len);\n\t}\n\treturn MV_QUEUE_COMMAND_RESULT_SENT;\n}\n\nstatic void mvumi_fire_cmd(struct mvumi_hba *mhba, struct mvumi_cmd *cmd)\n{\n\tunsigned short num_of_cl_sent = 0;\n\tunsigned int count;\n\tenum mvumi_qc_result result;\n\n\tif (cmd)\n\t\tlist_add_tail(&cmd->queue_pointer, &mhba->waiting_req_list);\n\tcount = mhba->instancet->check_ib_list(mhba);\n\tif (list_empty(&mhba->waiting_req_list) || !count)\n\t\treturn;\n\n\tdo {\n\t\tcmd = list_first_entry(&mhba->waiting_req_list,\n\t\t\t\t       struct mvumi_cmd, queue_pointer);\n\t\tlist_del_init(&cmd->queue_pointer);\n\t\tresult = mvumi_send_command(mhba, cmd);\n\t\tswitch (result) {\n\t\tcase MV_QUEUE_COMMAND_RESULT_SENT:\n\t\t\tnum_of_cl_sent++;\n\t\t\tbreak;\n\t\tcase MV_QUEUE_COMMAND_RESULT_NO_RESOURCE:\n\t\t\tlist_add(&cmd->queue_pointer, &mhba->waiting_req_list);\n\t\t\tif (num_of_cl_sent > 0)\n\t\t\t\tmvumi_send_ib_list_entry(mhba);\n\n\t\t\treturn;\n\t\t}\n\t} while (!list_empty(&mhba->waiting_req_list) && count--);\n\n\tif (num_of_cl_sent > 0)\n\t\tmvumi_send_ib_list_entry(mhba);\n}\n\n \nstatic void mvumi_enable_intr(struct mvumi_hba *mhba)\n{\n\tunsigned int mask;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tiowrite32(regs->int_drbl_int_mask, regs->arm_to_pciea_mask_reg);\n\tmask = ioread32(regs->enpointa_mask_reg);\n\tmask |= regs->int_dl_cpu2pciea | regs->int_comaout | regs->int_comaerr;\n\tiowrite32(mask, regs->enpointa_mask_reg);\n}\n\n \nstatic void mvumi_disable_intr(struct mvumi_hba *mhba)\n{\n\tunsigned int mask;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tiowrite32(0, regs->arm_to_pciea_mask_reg);\n\tmask = ioread32(regs->enpointa_mask_reg);\n\tmask &= ~(regs->int_dl_cpu2pciea | regs->int_comaout |\n\t\t\t\t\t\t\tregs->int_comaerr);\n\tiowrite32(mask, regs->enpointa_mask_reg);\n}\n\nstatic int mvumi_clear_intr(void *extend)\n{\n\tstruct mvumi_hba *mhba = (struct mvumi_hba *) extend;\n\tunsigned int status, isr_status = 0, tmp = 0;\n\tstruct mvumi_hw_regs *regs = mhba->regs;\n\n\tstatus = ioread32(regs->main_int_cause_reg);\n\tif (!(status & regs->int_mu) || status == 0xFFFFFFFF)\n\t\treturn 1;\n\tif (unlikely(status & regs->int_comaerr)) {\n\t\ttmp = ioread32(regs->outb_isr_cause);\n\t\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9580) {\n\t\t\tif (tmp & regs->clic_out_err) {\n\t\t\t\tiowrite32(tmp & regs->clic_out_err,\n\t\t\t\t\t\t\tregs->outb_isr_cause);\n\t\t\t}\n\t\t} else {\n\t\t\tif (tmp & (regs->clic_in_err | regs->clic_out_err))\n\t\t\t\tiowrite32(tmp & (regs->clic_in_err |\n\t\t\t\t\t\tregs->clic_out_err),\n\t\t\t\t\t\tregs->outb_isr_cause);\n\t\t}\n\t\tstatus ^= mhba->regs->int_comaerr;\n\t\t \n\t}\n\tif (status & regs->int_comaout) {\n\t\ttmp = ioread32(regs->outb_isr_cause);\n\t\tif (tmp & regs->clic_irq)\n\t\t\tiowrite32(tmp & regs->clic_irq, regs->outb_isr_cause);\n\t}\n\tif (status & regs->int_dl_cpu2pciea) {\n\t\tisr_status = ioread32(regs->arm_to_pciea_drbl_reg);\n\t\tif (isr_status)\n\t\t\tiowrite32(isr_status, regs->arm_to_pciea_drbl_reg);\n\t}\n\n\tmhba->global_isr = status;\n\tmhba->isr_status = isr_status;\n\n\treturn 0;\n}\n\n \nstatic unsigned int mvumi_read_fw_status_reg(struct mvumi_hba *mhba)\n{\n\tunsigned int status;\n\n\tstatus = ioread32(mhba->regs->arm_to_pciea_drbl_reg);\n\tif (status)\n\t\tiowrite32(status, mhba->regs->arm_to_pciea_drbl_reg);\n\treturn status;\n}\n\nstatic struct mvumi_instance_template mvumi_instance_9143 = {\n\t.fire_cmd = mvumi_fire_cmd,\n\t.enable_intr = mvumi_enable_intr,\n\t.disable_intr = mvumi_disable_intr,\n\t.clear_intr = mvumi_clear_intr,\n\t.read_fw_status_reg = mvumi_read_fw_status_reg,\n\t.check_ib_list = mvumi_check_ib_list_9143,\n\t.check_ob_list = mvumi_check_ob_list_9143,\n\t.reset_host = mvumi_reset_host_9143,\n};\n\nstatic struct mvumi_instance_template mvumi_instance_9580 = {\n\t.fire_cmd = mvumi_fire_cmd,\n\t.enable_intr = mvumi_enable_intr,\n\t.disable_intr = mvumi_disable_intr,\n\t.clear_intr = mvumi_clear_intr,\n\t.read_fw_status_reg = mvumi_read_fw_status_reg,\n\t.check_ib_list = mvumi_check_ib_list_9580,\n\t.check_ob_list = mvumi_check_ob_list_9580,\n\t.reset_host = mvumi_reset_host_9580,\n};\n\nstatic int mvumi_slave_configure(struct scsi_device *sdev)\n{\n\tstruct mvumi_hba *mhba;\n\tunsigned char bitcount = sizeof(unsigned char) * 8;\n\n\tmhba = (struct mvumi_hba *) sdev->host->hostdata;\n\tif (sdev->id >= mhba->max_target_id)\n\t\treturn -EINVAL;\n\n\tmhba->target_map[sdev->id / bitcount] |= (1 << (sdev->id % bitcount));\n\treturn 0;\n}\n\n \nstatic unsigned char mvumi_build_frame(struct mvumi_hba *mhba,\n\t\t\t\tstruct scsi_cmnd *scmd, struct mvumi_cmd *cmd)\n{\n\tstruct mvumi_msg_frame *pframe;\n\n\tcmd->scmd = scmd;\n\tcmd->cmd_status = REQ_STATUS_PENDING;\n\tpframe = cmd->frame;\n\tpframe->device_id = ((unsigned short) scmd->device->id) |\n\t\t\t\t(((unsigned short) scmd->device->lun) << 8);\n\tpframe->cmd_flag = 0;\n\n\tswitch (scmd->sc_data_direction) {\n\tcase DMA_NONE:\n\t\tpframe->cmd_flag |= CMD_FLAG_NON_DATA;\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\tpframe->cmd_flag |= CMD_FLAG_DATA_IN;\n\t\tbreak;\n\tcase DMA_TO_DEVICE:\n\t\tpframe->cmd_flag |= CMD_FLAG_DATA_OUT;\n\t\tbreak;\n\tcase DMA_BIDIRECTIONAL:\n\tdefault:\n\t\tdev_warn(&mhba->pdev->dev, \"unexpected data direction[%d] \"\n\t\t\t\"cmd[0x%x]\\n\", scmd->sc_data_direction, scmd->cmnd[0]);\n\t\tgoto error;\n\t}\n\n\tpframe->cdb_length = scmd->cmd_len;\n\tmemcpy(pframe->cdb, scmd->cmnd, pframe->cdb_length);\n\tpframe->req_function = CL_FUN_SCSI_CMD;\n\tif (scsi_bufflen(scmd)) {\n\t\tif (mvumi_make_sgl(mhba, scmd, &pframe->payload[0],\n\t\t\t&pframe->sg_counts))\n\t\t\tgoto error;\n\n\t\tpframe->data_transfer_length = scsi_bufflen(scmd);\n\t} else {\n\t\tpframe->sg_counts = 0;\n\t\tpframe->data_transfer_length = 0;\n\t}\n\treturn 0;\n\nerror:\n\tscsi_build_sense(scmd, 0, ILLEGAL_REQUEST, 0x24, 0);\n\treturn -1;\n}\n\n \nstatic int mvumi_queue_command(struct Scsi_Host *shost,\n\t\t\t\t\tstruct scsi_cmnd *scmd)\n{\n\tstruct mvumi_cmd *cmd;\n\tstruct mvumi_hba *mhba;\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(shost->host_lock, irq_flags);\n\n\tmhba = (struct mvumi_hba *) shost->hostdata;\n\tscmd->result = 0;\n\tcmd = mvumi_get_cmd(mhba);\n\tif (unlikely(!cmd)) {\n\t\tspin_unlock_irqrestore(shost->host_lock, irq_flags);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tif (unlikely(mvumi_build_frame(mhba, scmd, cmd)))\n\t\tgoto out_return_cmd;\n\n\tcmd->scmd = scmd;\n\tmvumi_priv(scmd)->cmd_priv = cmd;\n\tmhba->instancet->fire_cmd(mhba, cmd);\n\tspin_unlock_irqrestore(shost->host_lock, irq_flags);\n\treturn 0;\n\nout_return_cmd:\n\tmvumi_return_cmd(mhba, cmd);\n\tscsi_done(scmd);\n\tspin_unlock_irqrestore(shost->host_lock, irq_flags);\n\treturn 0;\n}\n\nstatic enum scsi_timeout_action mvumi_timed_out(struct scsi_cmnd *scmd)\n{\n\tstruct mvumi_cmd *cmd = mvumi_priv(scmd)->cmd_priv;\n\tstruct Scsi_Host *host = scmd->device->host;\n\tstruct mvumi_hba *mhba = shost_priv(host);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(mhba->shost->host_lock, flags);\n\n\tif (mhba->tag_cmd[cmd->frame->tag]) {\n\t\tmhba->tag_cmd[cmd->frame->tag] = NULL;\n\t\ttag_release_one(mhba, &mhba->tag_pool, cmd->frame->tag);\n\t}\n\tif (!list_empty(&cmd->queue_pointer))\n\t\tlist_del_init(&cmd->queue_pointer);\n\telse\n\t\tatomic_dec(&mhba->fw_outstanding);\n\n\tscmd->result = (DID_ABORT << 16);\n\tmvumi_priv(scmd)->cmd_priv = NULL;\n\tif (scsi_bufflen(scmd)) {\n\t\tdma_unmap_sg(&mhba->pdev->dev, scsi_sglist(scmd),\n\t\t\t     scsi_sg_count(scmd),\n\t\t\t     scmd->sc_data_direction);\n\t}\n\tmvumi_return_cmd(mhba, cmd);\n\tspin_unlock_irqrestore(mhba->shost->host_lock, flags);\n\n\treturn SCSI_EH_NOT_HANDLED;\n}\n\nstatic int\nmvumi_bios_param(struct scsi_device *sdev, struct block_device *bdev,\n\t\t\tsector_t capacity, int geom[])\n{\n\tint heads, sectors;\n\tsector_t cylinders;\n\tunsigned long tmp;\n\n\theads = 64;\n\tsectors = 32;\n\ttmp = heads * sectors;\n\tcylinders = capacity;\n\tsector_div(cylinders, tmp);\n\n\tif (capacity >= 0x200000) {\n\t\theads = 255;\n\t\tsectors = 63;\n\t\ttmp = heads * sectors;\n\t\tcylinders = capacity;\n\t\tsector_div(cylinders, tmp);\n\t}\n\tgeom[0] = heads;\n\tgeom[1] = sectors;\n\tgeom[2] = cylinders;\n\n\treturn 0;\n}\n\nstatic const struct scsi_host_template mvumi_template = {\n\n\t.module = THIS_MODULE,\n\t.name = \"Marvell Storage Controller\",\n\t.slave_configure = mvumi_slave_configure,\n\t.queuecommand = mvumi_queue_command,\n\t.eh_timed_out = mvumi_timed_out,\n\t.eh_host_reset_handler = mvumi_host_reset,\n\t.bios_param = mvumi_bios_param,\n\t.dma_boundary = PAGE_SIZE - 1,\n\t.this_id = -1,\n\t.cmd_size = sizeof(struct mvumi_cmd_priv),\n};\n\nstatic int mvumi_cfg_hw_reg(struct mvumi_hba *mhba)\n{\n\tvoid *base = NULL;\n\tstruct mvumi_hw_regs *regs;\n\n\tswitch (mhba->pdev->device) {\n\tcase PCI_DEVICE_ID_MARVELL_MV9143:\n\t\tmhba->mmio = mhba->base_addr[0];\n\t\tbase = mhba->mmio;\n\t\tif (!mhba->regs) {\n\t\t\tmhba->regs = kzalloc(sizeof(*regs), GFP_KERNEL);\n\t\t\tif (mhba->regs == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tregs = mhba->regs;\n\n\t\t \n\t\tregs->ctrl_sts_reg          = base + 0x20104;\n\t\tregs->rstoutn_mask_reg      = base + 0x20108;\n\t\tregs->sys_soft_rst_reg      = base + 0x2010C;\n\t\tregs->main_int_cause_reg    = base + 0x20200;\n\t\tregs->enpointa_mask_reg     = base + 0x2020C;\n\t\tregs->rstoutn_en_reg        = base + 0xF1400;\n\t\t \n\t\tregs->pciea_to_arm_drbl_reg = base + 0x20400;\n\t\tregs->arm_to_pciea_drbl_reg = base + 0x20408;\n\t\tregs->arm_to_pciea_mask_reg = base + 0x2040C;\n\t\tregs->pciea_to_arm_msg0     = base + 0x20430;\n\t\tregs->pciea_to_arm_msg1     = base + 0x20434;\n\t\tregs->arm_to_pciea_msg0     = base + 0x20438;\n\t\tregs->arm_to_pciea_msg1     = base + 0x2043C;\n\n\t\t \n\n\t\tregs->inb_aval_count_basel  = base + 0x508;\n\t\tregs->inb_aval_count_baseh  = base + 0x50C;\n\t\tregs->inb_write_pointer     = base + 0x518;\n\t\tregs->inb_read_pointer      = base + 0x51C;\n\t\tregs->outb_coal_cfg         = base + 0x568;\n\t\tregs->outb_copy_basel       = base + 0x5B0;\n\t\tregs->outb_copy_baseh       = base + 0x5B4;\n\t\tregs->outb_copy_pointer     = base + 0x544;\n\t\tregs->outb_read_pointer     = base + 0x548;\n\t\tregs->outb_isr_cause        = base + 0x560;\n\t\tregs->outb_coal_cfg         = base + 0x568;\n\t\t \n\t\tregs->int_comaout           = 1 << 8;\n\t\tregs->int_comaerr           = 1 << 6;\n\t\tregs->int_dl_cpu2pciea      = 1 << 1;\n\t\tregs->cl_pointer_toggle     = 1 << 12;\n\t\tregs->clic_irq              = 1 << 1;\n\t\tregs->clic_in_err           = 1 << 8;\n\t\tregs->clic_out_err          = 1 << 12;\n\t\tregs->cl_slot_num_mask      = 0xFFF;\n\t\tregs->int_drbl_int_mask     = 0x3FFFFFFF;\n\t\tregs->int_mu = regs->int_dl_cpu2pciea | regs->int_comaout |\n\t\t\t\t\t\t\tregs->int_comaerr;\n\t\tbreak;\n\tcase PCI_DEVICE_ID_MARVELL_MV9580:\n\t\tmhba->mmio = mhba->base_addr[2];\n\t\tbase = mhba->mmio;\n\t\tif (!mhba->regs) {\n\t\t\tmhba->regs = kzalloc(sizeof(*regs), GFP_KERNEL);\n\t\t\tif (mhba->regs == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tregs = mhba->regs;\n\t\t \n\t\tregs->ctrl_sts_reg          = base + 0x20104;\n\t\tregs->rstoutn_mask_reg      = base + 0x1010C;\n\t\tregs->sys_soft_rst_reg      = base + 0x10108;\n\t\tregs->main_int_cause_reg    = base + 0x10200;\n\t\tregs->enpointa_mask_reg     = base + 0x1020C;\n\t\tregs->rstoutn_en_reg        = base + 0xF1400;\n\n\t\t \n\t\tregs->pciea_to_arm_drbl_reg = base + 0x10460;\n\t\tregs->arm_to_pciea_drbl_reg = base + 0x10480;\n\t\tregs->arm_to_pciea_mask_reg = base + 0x10484;\n\t\tregs->pciea_to_arm_msg0     = base + 0x10400;\n\t\tregs->pciea_to_arm_msg1     = base + 0x10404;\n\t\tregs->arm_to_pciea_msg0     = base + 0x10420;\n\t\tregs->arm_to_pciea_msg1     = base + 0x10424;\n\n\t\t \n\t\tregs->reset_request         = base + 0x10108;\n\t\tregs->reset_enable          = base + 0x1010c;\n\n\t\t \n\t\tregs->inb_aval_count_basel  = base + 0x4008;\n\t\tregs->inb_aval_count_baseh  = base + 0x400C;\n\t\tregs->inb_write_pointer     = base + 0x4018;\n\t\tregs->inb_read_pointer      = base + 0x401C;\n\t\tregs->outb_copy_basel       = base + 0x4058;\n\t\tregs->outb_copy_baseh       = base + 0x405C;\n\t\tregs->outb_copy_pointer     = base + 0x406C;\n\t\tregs->outb_read_pointer     = base + 0x4070;\n\t\tregs->outb_coal_cfg         = base + 0x4080;\n\t\tregs->outb_isr_cause        = base + 0x4088;\n\t\t \n\t\tregs->int_comaout           = 1 << 4;\n\t\tregs->int_dl_cpu2pciea      = 1 << 12;\n\t\tregs->int_comaerr           = 1 << 29;\n\t\tregs->cl_pointer_toggle     = 1 << 14;\n\t\tregs->cl_slot_num_mask      = 0x3FFF;\n\t\tregs->clic_irq              = 1 << 0;\n\t\tregs->clic_out_err          = 1 << 1;\n\t\tregs->int_drbl_int_mask     = 0x3FFFFFFF;\n\t\tregs->int_mu = regs->int_dl_cpu2pciea | regs->int_comaout;\n\t\tbreak;\n\tdefault:\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mvumi_init_fw(struct mvumi_hba *mhba)\n{\n\tint ret = 0;\n\n\tif (pci_request_regions(mhba->pdev, MV_DRIVER_NAME)) {\n\t\tdev_err(&mhba->pdev->dev, \"IO memory region busy!\\n\");\n\t\treturn -EBUSY;\n\t}\n\tret = mvumi_map_pci_addr(mhba->pdev, mhba->base_addr);\n\tif (ret)\n\t\tgoto fail_ioremap;\n\n\tswitch (mhba->pdev->device) {\n\tcase PCI_DEVICE_ID_MARVELL_MV9143:\n\t\tmhba->instancet = &mvumi_instance_9143;\n\t\tmhba->io_seq = 0;\n\t\tmhba->max_sge = MVUMI_MAX_SG_ENTRY;\n\t\tmhba->request_id_enabled = 1;\n\t\tbreak;\n\tcase PCI_DEVICE_ID_MARVELL_MV9580:\n\t\tmhba->instancet = &mvumi_instance_9580;\n\t\tmhba->io_seq = 0;\n\t\tmhba->max_sge = MVUMI_MAX_SG_ENTRY;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&mhba->pdev->dev, \"device 0x%x not supported!\\n\",\n\t\t\t\t\t\t\tmhba->pdev->device);\n\t\tmhba->instancet = NULL;\n\t\tret = -EINVAL;\n\t\tgoto fail_alloc_mem;\n\t}\n\tdev_dbg(&mhba->pdev->dev, \"device id : %04X is found.\\n\",\n\t\t\t\t\t\t\tmhba->pdev->device);\n\tret = mvumi_cfg_hw_reg(mhba);\n\tif (ret) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for reg\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail_alloc_mem;\n\t}\n\tmhba->handshake_page = dma_alloc_coherent(&mhba->pdev->dev,\n\t\t\tHSP_MAX_SIZE, &mhba->handshake_page_phys, GFP_KERNEL);\n\tif (!mhba->handshake_page) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to allocate memory for handshake\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail_alloc_page;\n\t}\n\n\tif (mvumi_start(mhba)) {\n\t\tret = -EINVAL;\n\t\tgoto fail_ready_state;\n\t}\n\tret = mvumi_alloc_cmds(mhba);\n\tif (ret)\n\t\tgoto fail_ready_state;\n\n\treturn 0;\n\nfail_ready_state:\n\tmvumi_release_mem_resource(mhba);\n\tdma_free_coherent(&mhba->pdev->dev, HSP_MAX_SIZE,\n\t\tmhba->handshake_page, mhba->handshake_page_phys);\nfail_alloc_page:\n\tkfree(mhba->regs);\nfail_alloc_mem:\n\tmvumi_unmap_pci_addr(mhba->pdev, mhba->base_addr);\nfail_ioremap:\n\tpci_release_regions(mhba->pdev);\n\n\treturn ret;\n}\n\n \nstatic int mvumi_io_attach(struct mvumi_hba *mhba)\n{\n\tstruct Scsi_Host *host = mhba->shost;\n\tstruct scsi_device *sdev = NULL;\n\tint ret;\n\tunsigned int max_sg = (mhba->ib_max_size -\n\t\tsizeof(struct mvumi_msg_frame)) / sizeof(struct mvumi_sgl);\n\n\thost->irq = mhba->pdev->irq;\n\thost->unique_id = mhba->unique_id;\n\thost->can_queue = (mhba->max_io - 1) ? (mhba->max_io - 1) : 1;\n\thost->sg_tablesize = mhba->max_sge > max_sg ? max_sg : mhba->max_sge;\n\thost->max_sectors = mhba->max_transfer_size / 512;\n\thost->cmd_per_lun = (mhba->max_io - 1) ? (mhba->max_io - 1) : 1;\n\thost->max_id = mhba->max_target_id;\n\thost->max_cmd_len = MAX_COMMAND_SIZE;\n\n\tret = scsi_add_host(host, &mhba->pdev->dev);\n\tif (ret) {\n\t\tdev_err(&mhba->pdev->dev, \"scsi_add_host failed\\n\");\n\t\treturn ret;\n\t}\n\tmhba->fw_flag |= MVUMI_FW_ATTACH;\n\n\tmutex_lock(&mhba->sas_discovery_mutex);\n\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9580)\n\t\tret = scsi_add_device(host, 0, mhba->max_target_id - 1, 0);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tdev_err(&mhba->pdev->dev, \"add virtual device failed\\n\");\n\t\tmutex_unlock(&mhba->sas_discovery_mutex);\n\t\tgoto fail_add_device;\n\t}\n\n\tmhba->dm_thread = kthread_create(mvumi_rescan_bus,\n\t\t\t\t\t\tmhba, \"mvumi_scanthread\");\n\tif (IS_ERR(mhba->dm_thread)) {\n\t\tdev_err(&mhba->pdev->dev,\n\t\t\t\"failed to create device scan thread\\n\");\n\t\tret = PTR_ERR(mhba->dm_thread);\n\t\tmutex_unlock(&mhba->sas_discovery_mutex);\n\t\tgoto fail_create_thread;\n\t}\n\tatomic_set(&mhba->pnp_count, 1);\n\twake_up_process(mhba->dm_thread);\n\n\tmutex_unlock(&mhba->sas_discovery_mutex);\n\treturn 0;\n\nfail_create_thread:\n\tif (mhba->pdev->device == PCI_DEVICE_ID_MARVELL_MV9580)\n\t\tsdev = scsi_device_lookup(mhba->shost, 0,\n\t\t\t\t\t\tmhba->max_target_id - 1, 0);\n\tif (sdev) {\n\t\tscsi_remove_device(sdev);\n\t\tscsi_device_put(sdev);\n\t}\nfail_add_device:\n\tscsi_remove_host(mhba->shost);\n\treturn ret;\n}\n\n \nstatic int mvumi_probe_one(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct Scsi_Host *host;\n\tstruct mvumi_hba *mhba;\n\tint ret;\n\n\tdev_dbg(&pdev->dev, \" %#4.04x:%#4.04x:%#4.04x:%#4.04x: \",\n\t\t\tpdev->vendor, pdev->device, pdev->subsystem_vendor,\n\t\t\tpdev->subsystem_device);\n\n\tret = pci_enable_device(pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mvumi_pci_set_master(pdev);\n\tif (ret)\n\t\tgoto fail_set_dma_mask;\n\n\thost = scsi_host_alloc(&mvumi_template, sizeof(*mhba));\n\tif (!host) {\n\t\tdev_err(&pdev->dev, \"scsi_host_alloc failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail_alloc_instance;\n\t}\n\tmhba = shost_priv(host);\n\n\tINIT_LIST_HEAD(&mhba->cmd_pool);\n\tINIT_LIST_HEAD(&mhba->ob_data_list);\n\tINIT_LIST_HEAD(&mhba->free_ob_list);\n\tINIT_LIST_HEAD(&mhba->res_list);\n\tINIT_LIST_HEAD(&mhba->waiting_req_list);\n\tmutex_init(&mhba->device_lock);\n\tINIT_LIST_HEAD(&mhba->mhba_dev_list);\n\tINIT_LIST_HEAD(&mhba->shost_dev_list);\n\tatomic_set(&mhba->fw_outstanding, 0);\n\tinit_waitqueue_head(&mhba->int_cmd_wait_q);\n\tmutex_init(&mhba->sas_discovery_mutex);\n\n\tmhba->pdev = pdev;\n\tmhba->shost = host;\n\tmhba->unique_id = pci_dev_id(pdev);\n\n\tret = mvumi_init_fw(mhba);\n\tif (ret)\n\t\tgoto fail_init_fw;\n\n\tret = request_irq(mhba->pdev->irq, mvumi_isr_handler, IRQF_SHARED,\n\t\t\t\t\"mvumi\", mhba);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register IRQ\\n\");\n\t\tgoto fail_init_irq;\n\t}\n\n\tmhba->instancet->enable_intr(mhba);\n\tpci_set_drvdata(pdev, mhba);\n\n\tret = mvumi_io_attach(mhba);\n\tif (ret)\n\t\tgoto fail_io_attach;\n\n\tmvumi_backup_bar_addr(mhba);\n\tdev_dbg(&pdev->dev, \"probe mvumi driver successfully.\\n\");\n\n\treturn 0;\n\nfail_io_attach:\n\tmhba->instancet->disable_intr(mhba);\n\tfree_irq(mhba->pdev->irq, mhba);\nfail_init_irq:\n\tmvumi_release_fw(mhba);\nfail_init_fw:\n\tscsi_host_put(host);\n\nfail_alloc_instance:\nfail_set_dma_mask:\n\tpci_disable_device(pdev);\n\n\treturn ret;\n}\n\nstatic void mvumi_detach_one(struct pci_dev *pdev)\n{\n\tstruct Scsi_Host *host;\n\tstruct mvumi_hba *mhba;\n\n\tmhba = pci_get_drvdata(pdev);\n\tif (mhba->dm_thread) {\n\t\tkthread_stop(mhba->dm_thread);\n\t\tmhba->dm_thread = NULL;\n\t}\n\n\tmvumi_detach_devices(mhba);\n\thost = mhba->shost;\n\tscsi_remove_host(mhba->shost);\n\tmvumi_flush_cache(mhba);\n\n\tmhba->instancet->disable_intr(mhba);\n\tfree_irq(mhba->pdev->irq, mhba);\n\tmvumi_release_fw(mhba);\n\tscsi_host_put(host);\n\tpci_disable_device(pdev);\n\tdev_dbg(&pdev->dev, \"driver is removed!\\n\");\n}\n\n \nstatic void mvumi_shutdown(struct pci_dev *pdev)\n{\n\tstruct mvumi_hba *mhba = pci_get_drvdata(pdev);\n\n\tmvumi_flush_cache(mhba);\n}\n\nstatic int __maybe_unused mvumi_suspend(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct mvumi_hba *mhba = pci_get_drvdata(pdev);\n\n\tmvumi_flush_cache(mhba);\n\n\tmhba->instancet->disable_intr(mhba);\n\tmvumi_unmap_pci_addr(pdev, mhba->base_addr);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused mvumi_resume(struct device *dev)\n{\n\tint ret;\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct mvumi_hba *mhba = pci_get_drvdata(pdev);\n\n\tret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\tif (ret)\n\t\tgoto fail;\n\tret = mvumi_map_pci_addr(mhba->pdev, mhba->base_addr);\n\tif (ret)\n\t\tgoto release_regions;\n\n\tif (mvumi_cfg_hw_reg(mhba)) {\n\t\tret = -EINVAL;\n\t\tgoto unmap_pci_addr;\n\t}\n\n\tmhba->mmio = mhba->base_addr[0];\n\tmvumi_reset(mhba);\n\n\tif (mvumi_start(mhba)) {\n\t\tret = -EINVAL;\n\t\tgoto unmap_pci_addr;\n\t}\n\n\tmhba->instancet->enable_intr(mhba);\n\n\treturn 0;\n\nunmap_pci_addr:\n\tmvumi_unmap_pci_addr(pdev, mhba->base_addr);\nrelease_regions:\n\tpci_release_regions(pdev);\nfail:\n\n\treturn ret;\n}\n\nstatic SIMPLE_DEV_PM_OPS(mvumi_pm_ops, mvumi_suspend, mvumi_resume);\n\nstatic struct pci_driver mvumi_pci_driver = {\n\n\t.name = MV_DRIVER_NAME,\n\t.id_table = mvumi_pci_table,\n\t.probe = mvumi_probe_one,\n\t.remove = mvumi_detach_one,\n\t.shutdown = mvumi_shutdown,\n\t.driver.pm = &mvumi_pm_ops,\n};\n\nmodule_pci_driver(mvumi_pci_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}