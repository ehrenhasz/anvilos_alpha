{
  "module_name": "lpfc_nvmet.c",
  "hash_id": "8fb4d50b9b8e96a2cadb1a9542f50367192310678cd979f67c785bbc8b0a8178",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/lpfc/lpfc_nvmet.c",
  "human_readable_source": " \n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <asm/unaligned.h>\n#include <linux/crc-t10dif.h>\n#include <net/checksum.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_tcq.h>\n#include <scsi/scsi_transport_fc.h>\n#include <scsi/fc/fc_fs.h>\n\n#include \"lpfc_version.h\"\n#include \"lpfc_hw4.h\"\n#include \"lpfc_hw.h\"\n#include \"lpfc_sli.h\"\n#include \"lpfc_sli4.h\"\n#include \"lpfc_nl.h\"\n#include \"lpfc_disc.h\"\n#include \"lpfc.h\"\n#include \"lpfc_scsi.h\"\n#include \"lpfc_nvme.h\"\n#include \"lpfc_logmsg.h\"\n#include \"lpfc_crtn.h\"\n#include \"lpfc_vport.h\"\n#include \"lpfc_debugfs.h\"\n\nstatic struct lpfc_iocbq *lpfc_nvmet_prep_ls_wqe(struct lpfc_hba *,\n\t\t\t\t\t\t struct lpfc_async_xchg_ctx *,\n\t\t\t\t\t\t dma_addr_t rspbuf,\n\t\t\t\t\t\t uint16_t rspsize);\nstatic struct lpfc_iocbq *lpfc_nvmet_prep_fcp_wqe(struct lpfc_hba *,\n\t\t\t\t\t\t  struct lpfc_async_xchg_ctx *);\nstatic int lpfc_nvmet_sol_fcp_issue_abort(struct lpfc_hba *,\n\t\t\t\t\t  struct lpfc_async_xchg_ctx *,\n\t\t\t\t\t  uint32_t, uint16_t);\nstatic int lpfc_nvmet_unsol_fcp_issue_abort(struct lpfc_hba *,\n\t\t\t\t\t    struct lpfc_async_xchg_ctx *,\n\t\t\t\t\t    uint32_t, uint16_t);\nstatic void lpfc_nvmet_wqfull_flush(struct lpfc_hba *, struct lpfc_queue *,\n\t\t\t\t    struct lpfc_async_xchg_ctx *);\nstatic void lpfc_nvmet_fcp_rqst_defer_work(struct work_struct *);\n\nstatic void lpfc_nvmet_process_rcv_fcp_req(struct lpfc_nvmet_ctxbuf *ctx_buf);\n\nstatic union lpfc_wqe128 lpfc_tsend_cmd_template;\nstatic union lpfc_wqe128 lpfc_treceive_cmd_template;\nstatic union lpfc_wqe128 lpfc_trsp_cmd_template;\n\n \nvoid\nlpfc_nvmet_cmd_template(void)\n{\n\tunion lpfc_wqe128 *wqe;\n\n\t \n\twqe = &lpfc_tsend_cmd_template;\n\tmemset(wqe, 0, sizeof(union lpfc_wqe128));\n\n\t \n\n\t \n\n\t \n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_cmnd, &wqe->fcp_tsend.wqe_com, CMD_FCP_TSEND64_WQE);\n\tbf_set(wqe_pu, &wqe->fcp_tsend.wqe_com, PARM_REL_OFF);\n\tbf_set(wqe_class, &wqe->fcp_tsend.wqe_com, CLASS3);\n\tbf_set(wqe_ct, &wqe->fcp_tsend.wqe_com, SLI4_CT_RPI);\n\tbf_set(wqe_ar, &wqe->fcp_tsend.wqe_com, 1);\n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_xchg, &wqe->fcp_tsend.wqe_com, LPFC_NVME_XCHG);\n\tbf_set(wqe_dbde, &wqe->fcp_tsend.wqe_com, 1);\n\tbf_set(wqe_wqes, &wqe->fcp_tsend.wqe_com, 0);\n\tbf_set(wqe_xc, &wqe->fcp_tsend.wqe_com, 1);\n\tbf_set(wqe_iod, &wqe->fcp_tsend.wqe_com, LPFC_WQE_IOD_WRITE);\n\tbf_set(wqe_lenloc, &wqe->fcp_tsend.wqe_com, LPFC_WQE_LENLOC_WORD12);\n\n\t \n\tbf_set(wqe_cmd_type, &wqe->fcp_tsend.wqe_com, FCP_COMMAND_TSEND);\n\tbf_set(wqe_cqid, &wqe->fcp_tsend.wqe_com, LPFC_WQE_CQ_ID_DEFAULT);\n\tbf_set(wqe_sup, &wqe->fcp_tsend.wqe_com, 0);\n\tbf_set(wqe_irsp, &wqe->fcp_tsend.wqe_com, 0);\n\tbf_set(wqe_irsplen, &wqe->fcp_tsend.wqe_com, 0);\n\tbf_set(wqe_pbde, &wqe->fcp_tsend.wqe_com, 0);\n\n\t \n\n\t \n\n\t \n\twqe = &lpfc_treceive_cmd_template;\n\tmemset(wqe, 0, sizeof(union lpfc_wqe128));\n\n\t \n\n\t \n\twqe->fcp_treceive.payload_offset_len = TXRDY_PAYLOAD_LEN;\n\n\t \n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_cmnd, &wqe->fcp_treceive.wqe_com, CMD_FCP_TRECEIVE64_WQE);\n\tbf_set(wqe_pu, &wqe->fcp_treceive.wqe_com, PARM_REL_OFF);\n\tbf_set(wqe_class, &wqe->fcp_treceive.wqe_com, CLASS3);\n\tbf_set(wqe_ct, &wqe->fcp_treceive.wqe_com, SLI4_CT_RPI);\n\tbf_set(wqe_ar, &wqe->fcp_treceive.wqe_com, 0);\n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_dbde, &wqe->fcp_treceive.wqe_com, 1);\n\tbf_set(wqe_wqes, &wqe->fcp_treceive.wqe_com, 0);\n\tbf_set(wqe_xchg, &wqe->fcp_treceive.wqe_com, LPFC_NVME_XCHG);\n\tbf_set(wqe_iod, &wqe->fcp_treceive.wqe_com, LPFC_WQE_IOD_READ);\n\tbf_set(wqe_lenloc, &wqe->fcp_treceive.wqe_com, LPFC_WQE_LENLOC_WORD12);\n\tbf_set(wqe_xc, &wqe->fcp_tsend.wqe_com, 1);\n\n\t \n\tbf_set(wqe_cmd_type, &wqe->fcp_treceive.wqe_com, FCP_COMMAND_TRECEIVE);\n\tbf_set(wqe_cqid, &wqe->fcp_treceive.wqe_com, LPFC_WQE_CQ_ID_DEFAULT);\n\tbf_set(wqe_sup, &wqe->fcp_treceive.wqe_com, 0);\n\tbf_set(wqe_irsp, &wqe->fcp_treceive.wqe_com, 0);\n\tbf_set(wqe_irsplen, &wqe->fcp_treceive.wqe_com, 0);\n\tbf_set(wqe_pbde, &wqe->fcp_treceive.wqe_com, 1);\n\n\t \n\n\t \n\n\t \n\twqe = &lpfc_trsp_cmd_template;\n\tmemset(wqe, 0, sizeof(union lpfc_wqe128));\n\n\t \n\n\t \n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_cmnd, &wqe->fcp_trsp.wqe_com, CMD_FCP_TRSP64_WQE);\n\tbf_set(wqe_pu, &wqe->fcp_trsp.wqe_com, PARM_UNUSED);\n\tbf_set(wqe_class, &wqe->fcp_trsp.wqe_com, CLASS3);\n\tbf_set(wqe_ct, &wqe->fcp_trsp.wqe_com, SLI4_CT_RPI);\n\tbf_set(wqe_ag, &wqe->fcp_trsp.wqe_com, 1);  \n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_dbde, &wqe->fcp_trsp.wqe_com, 1);\n\tbf_set(wqe_xchg, &wqe->fcp_trsp.wqe_com, LPFC_NVME_XCHG);\n\tbf_set(wqe_wqes, &wqe->fcp_trsp.wqe_com, 0);\n\tbf_set(wqe_xc, &wqe->fcp_trsp.wqe_com, 0);\n\tbf_set(wqe_iod, &wqe->fcp_trsp.wqe_com, LPFC_WQE_IOD_NONE);\n\tbf_set(wqe_lenloc, &wqe->fcp_trsp.wqe_com, LPFC_WQE_LENLOC_WORD3);\n\n\t \n\tbf_set(wqe_cmd_type, &wqe->fcp_trsp.wqe_com, FCP_COMMAND_TRSP);\n\tbf_set(wqe_cqid, &wqe->fcp_trsp.wqe_com, LPFC_WQE_CQ_ID_DEFAULT);\n\tbf_set(wqe_sup, &wqe->fcp_trsp.wqe_com, 0);\n\tbf_set(wqe_irsp, &wqe->fcp_trsp.wqe_com, 0);\n\tbf_set(wqe_irsplen, &wqe->fcp_trsp.wqe_com, 0);\n\tbf_set(wqe_pbde, &wqe->fcp_trsp.wqe_com, 0);\n\n\t \n}\n\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\nstatic struct lpfc_async_xchg_ctx *\nlpfc_nvmet_get_ctx_for_xri(struct lpfc_hba *phba, u16 xri)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tunsigned long iflag;\n\tbool found = false;\n\n\tspin_lock_irqsave(&phba->sli4_hba.t_active_list_lock, iflag);\n\tlist_for_each_entry(ctxp, &phba->sli4_hba.t_active_ctx_list, list) {\n\t\tif (ctxp->ctxbuf->sglq->sli4_xritag != xri)\n\t\t\tcontinue;\n\n\t\tfound = true;\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&phba->sli4_hba.t_active_list_lock, iflag);\n\tif (found)\n\t\treturn ctxp;\n\n\treturn NULL;\n}\n\nstatic struct lpfc_async_xchg_ctx *\nlpfc_nvmet_get_ctx_for_oxid(struct lpfc_hba *phba, u16 oxid, u32 sid)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tunsigned long iflag;\n\tbool found = false;\n\n\tspin_lock_irqsave(&phba->sli4_hba.t_active_list_lock, iflag);\n\tlist_for_each_entry(ctxp, &phba->sli4_hba.t_active_ctx_list, list) {\n\t\tif (ctxp->oxid != oxid || ctxp->sid != sid)\n\t\t\tcontinue;\n\n\t\tfound = true;\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&phba->sli4_hba.t_active_list_lock, iflag);\n\tif (found)\n\t\treturn ctxp;\n\n\treturn NULL;\n}\n#endif\n\nstatic void\nlpfc_nvmet_defer_release(struct lpfc_hba *phba,\n\t\t\tstruct lpfc_async_xchg_ctx *ctxp)\n{\n\tlockdep_assert_held(&ctxp->ctxlock);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6313 NVMET Defer ctx release oxid x%x flg x%x\\n\",\n\t\t\tctxp->oxid, ctxp->flag);\n\n\tif (ctxp->flag & LPFC_NVME_CTX_RLS)\n\t\treturn;\n\n\tctxp->flag |= LPFC_NVME_CTX_RLS;\n\tspin_lock(&phba->sli4_hba.t_active_list_lock);\n\tlist_del(&ctxp->list);\n\tspin_unlock(&phba->sli4_hba.t_active_list_lock);\n\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\tlist_add_tail(&ctxp->list, &phba->sli4_hba.lpfc_abts_nvmet_ctx_list);\n\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n}\n\n \nvoid\n__lpfc_nvme_xmt_ls_rsp_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t   struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_async_xchg_ctx *axchg = cmdwqe->context_un.axchg;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\tstruct nvmefc_ls_rsp *ls_rsp = &axchg->ls_rsp;\n\tuint32_t status, result;\n\n\tstatus = bf_get(lpfc_wcqe_c_status, wcqe);\n\tresult = wcqe->parameter;\n\n\tif (axchg->state != LPFC_NVME_STE_LS_RSP || axchg->entry_cnt != 2) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6410 NVMEx LS cmpl state mismatch IO x%x: \"\n\t\t\t\t\"%d %d\\n\",\n\t\t\t\taxchg->oxid, axchg->state, axchg->entry_cnt);\n\t}\n\n\tlpfc_nvmeio_data(phba, \"NVMEx LS  CMPL: xri x%x stat x%x result x%x\\n\",\n\t\t\t axchg->oxid, status, result);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\"6038 NVMEx LS rsp cmpl: %d %d oxid x%x\\n\",\n\t\t\tstatus, result, axchg->oxid);\n\n\tlpfc_nlp_put(cmdwqe->ndlp);\n\tcmdwqe->context_un.axchg = NULL;\n\tcmdwqe->bpl_dmabuf = NULL;\n\tlpfc_sli_release_iocbq(phba, cmdwqe);\n\tls_rsp->done(ls_rsp);\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\"6200 NVMEx LS rsp cmpl done status %d oxid x%x\\n\",\n\t\t\tstatus, axchg->oxid);\n\tkfree(axchg);\n}\n\n \nstatic void\nlpfc_nvmet_xmt_ls_rsp_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t  struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tuint32_t status, result;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\n\tif (!phba->targetport)\n\t\tgoto finish;\n\n\tstatus = bf_get(lpfc_wcqe_c_status, wcqe);\n\tresult = wcqe->parameter;\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tif (tgtp) {\n\t\tif (status) {\n\t\t\tatomic_inc(&tgtp->xmt_ls_rsp_error);\n\t\t\tif (result == IOERR_ABORT_REQUESTED)\n\t\t\t\tatomic_inc(&tgtp->xmt_ls_rsp_aborted);\n\t\t\tif (bf_get(lpfc_wcqe_c_xb, wcqe))\n\t\t\t\tatomic_inc(&tgtp->xmt_ls_rsp_xb_set);\n\t\t} else {\n\t\t\tatomic_inc(&tgtp->xmt_ls_rsp_cmpl);\n\t\t}\n\t}\n\nfinish:\n\t__lpfc_nvme_xmt_ls_rsp_cmp(phba, cmdwqe, rspwqe);\n}\n\n \nvoid\nlpfc_nvmet_ctxbuf_post(struct lpfc_hba *phba, struct lpfc_nvmet_ctxbuf *ctx_buf)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_async_xchg_ctx *ctxp = ctx_buf->context;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct rqb_dmabuf *nvmebuf;\n\tstruct lpfc_nvmet_ctx_info *infop;\n\tuint32_t size, oxid, sid;\n\tint cpu;\n\tunsigned long iflag;\n\n\tif (ctxp->state == LPFC_NVME_STE_FREE) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6411 NVMET free, already free IO x%x: %d %d\\n\",\n\t\t\t\tctxp->oxid, ctxp->state, ctxp->entry_cnt);\n\t}\n\n\tif (ctxp->rqb_buffer) {\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tnvmebuf = ctxp->rqb_buffer;\n\t\t \n\t\tif (nvmebuf) {\n\t\t\tctxp->rqb_buffer = NULL;\n\t\t\tif (ctxp->flag & LPFC_NVME_CTX_REUSE_WQ) {\n\t\t\t\tctxp->flag &= ~LPFC_NVME_CTX_REUSE_WQ;\n\t\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\t\t\tnvmebuf->hrq->rqbp->rqb_free_buffer(phba,\n\t\t\t\t\t\t\t\t    nvmebuf);\n\t\t\t} else {\n\t\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\t\t\t \n\t\t\t\tlpfc_rq_buf_free(phba, &nvmebuf->hbuf);\n\t\t\t}\n\t\t} else {\n\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\t}\n\t}\n\tctxp->state = LPFC_NVME_STE_FREE;\n\n\tspin_lock_irqsave(&phba->sli4_hba.nvmet_io_wait_lock, iflag);\n\tif (phba->sli4_hba.nvmet_io_wait_cnt) {\n\t\tlist_remove_head(&phba->sli4_hba.lpfc_nvmet_io_wait_list,\n\t\t\t\t nvmebuf, struct rqb_dmabuf,\n\t\t\t\t hbuf.list);\n\t\tphba->sli4_hba.nvmet_io_wait_cnt--;\n\t\tspin_unlock_irqrestore(&phba->sli4_hba.nvmet_io_wait_lock,\n\t\t\t\t       iflag);\n\n\t\tfc_hdr = (struct fc_frame_header *)(nvmebuf->hbuf.virt);\n\t\toxid = be16_to_cpu(fc_hdr->fh_ox_id);\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\t\tsize = nvmebuf->bytes_recv;\n\t\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\n\t\tctxp = (struct lpfc_async_xchg_ctx *)ctx_buf->context;\n\t\tctxp->wqeq = NULL;\n\t\tctxp->offset = 0;\n\t\tctxp->phba = phba;\n\t\tctxp->size = size;\n\t\tctxp->oxid = oxid;\n\t\tctxp->sid = sid;\n\t\tctxp->state = LPFC_NVME_STE_RCV;\n\t\tctxp->entry_cnt = 1;\n\t\tctxp->flag = 0;\n\t\tctxp->ctxbuf = ctx_buf;\n\t\tctxp->rqb_buffer = (void *)nvmebuf;\n\t\tspin_lock_init(&ctxp->ctxlock);\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\t \n\t\tif (ctxp->ts_isr_cmd) {\n\t\t\tctxp->ts_cmd_nvme = 0;\n\t\t\tctxp->ts_nvme_data = 0;\n\t\t\tctxp->ts_data_wqput = 0;\n\t\t\tctxp->ts_isr_data = 0;\n\t\t\tctxp->ts_data_nvme = 0;\n\t\t\tctxp->ts_nvme_status = 0;\n\t\t\tctxp->ts_status_wqput = 0;\n\t\t\tctxp->ts_isr_status = 0;\n\t\t\tctxp->ts_status_nvme = 0;\n\t\t}\n#endif\n\t\tatomic_inc(&tgtp->rcv_fcp_cmd_in);\n\n\t\t \n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tctxp->flag |= LPFC_NVME_CTX_REUSE_WQ;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\n\t\tif (!queue_work(phba->wq, &ctx_buf->defer_work)) {\n\t\t\tatomic_inc(&tgtp->rcv_fcp_cmd_drop);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6181 Unable to queue deferred work \"\n\t\t\t\t\t\"for oxid x%x. \"\n\t\t\t\t\t\"FCP Drop IO [x%x x%x x%x]\\n\",\n\t\t\t\t\tctxp->oxid,\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_in),\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_out),\n\t\t\t\t\tatomic_read(&tgtp->xmt_fcp_release));\n\n\t\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\t\tlpfc_nvmet_defer_release(phba, ctxp);\n\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\t\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, sid, oxid);\n\t\t}\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&phba->sli4_hba.nvmet_io_wait_lock, iflag);\n\n\t \n\tspin_lock_irqsave(&phba->sli4_hba.t_active_list_lock, iflag);\n\tlist_del_init(&ctxp->list);\n\tspin_unlock_irqrestore(&phba->sli4_hba.t_active_list_lock, iflag);\n\tcpu = raw_smp_processor_id();\n\tinfop = lpfc_get_ctx_list(phba, cpu, ctxp->idx);\n\tspin_lock_irqsave(&infop->nvmet_ctx_list_lock, iflag);\n\tlist_add_tail(&ctx_buf->list, &infop->nvmet_ctx_list);\n\tinfop->nvmet_ctx_list_cnt++;\n\tspin_unlock_irqrestore(&infop->nvmet_ctx_list_lock, iflag);\n#endif\n}\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\nstatic void\nlpfc_nvmet_ktime(struct lpfc_hba *phba,\n\t\t struct lpfc_async_xchg_ctx *ctxp)\n{\n\tuint64_t seg1, seg2, seg3, seg4, seg5;\n\tuint64_t seg6, seg7, seg8, seg9, seg10;\n\tuint64_t segsum;\n\n\tif (!ctxp->ts_isr_cmd || !ctxp->ts_cmd_nvme ||\n\t    !ctxp->ts_nvme_data || !ctxp->ts_data_wqput ||\n\t    !ctxp->ts_isr_data || !ctxp->ts_data_nvme ||\n\t    !ctxp->ts_nvme_status || !ctxp->ts_status_wqput ||\n\t    !ctxp->ts_isr_status || !ctxp->ts_status_nvme)\n\t\treturn;\n\n\tif (ctxp->ts_status_nvme < ctxp->ts_isr_cmd)\n\t\treturn;\n\tif (ctxp->ts_isr_cmd  > ctxp->ts_cmd_nvme)\n\t\treturn;\n\tif (ctxp->ts_cmd_nvme > ctxp->ts_nvme_data)\n\t\treturn;\n\tif (ctxp->ts_nvme_data > ctxp->ts_data_wqput)\n\t\treturn;\n\tif (ctxp->ts_data_wqput > ctxp->ts_isr_data)\n\t\treturn;\n\tif (ctxp->ts_isr_data > ctxp->ts_data_nvme)\n\t\treturn;\n\tif (ctxp->ts_data_nvme > ctxp->ts_nvme_status)\n\t\treturn;\n\tif (ctxp->ts_nvme_status > ctxp->ts_status_wqput)\n\t\treturn;\n\tif (ctxp->ts_status_wqput > ctxp->ts_isr_status)\n\t\treturn;\n\tif (ctxp->ts_isr_status > ctxp->ts_status_nvme)\n\t\treturn;\n\t \n\tseg1 = ctxp->ts_cmd_nvme - ctxp->ts_isr_cmd;\n\tsegsum = seg1;\n\n\tseg2 = ctxp->ts_nvme_data - ctxp->ts_isr_cmd;\n\tif (segsum > seg2)\n\t\treturn;\n\tseg2 -= segsum;\n\tsegsum += seg2;\n\n\tseg3 = ctxp->ts_data_wqput - ctxp->ts_isr_cmd;\n\tif (segsum > seg3)\n\t\treturn;\n\tseg3 -= segsum;\n\tsegsum += seg3;\n\n\tseg4 = ctxp->ts_isr_data - ctxp->ts_isr_cmd;\n\tif (segsum > seg4)\n\t\treturn;\n\tseg4 -= segsum;\n\tsegsum += seg4;\n\n\tseg5 = ctxp->ts_data_nvme - ctxp->ts_isr_cmd;\n\tif (segsum > seg5)\n\t\treturn;\n\tseg5 -= segsum;\n\tsegsum += seg5;\n\n\n\t \n\tif (ctxp->ts_nvme_status > ctxp->ts_data_nvme) {\n\t\tseg6 = ctxp->ts_nvme_status - ctxp->ts_isr_cmd;\n\t\tif (segsum > seg6)\n\t\t\treturn;\n\t\tseg6 -= segsum;\n\t\tsegsum += seg6;\n\n\t\tseg7 = ctxp->ts_status_wqput - ctxp->ts_isr_cmd;\n\t\tif (segsum > seg7)\n\t\t\treturn;\n\t\tseg7 -= segsum;\n\t\tsegsum += seg7;\n\n\t\tseg8 = ctxp->ts_isr_status - ctxp->ts_isr_cmd;\n\t\tif (segsum > seg8)\n\t\t\treturn;\n\t\tseg8 -= segsum;\n\t\tsegsum += seg8;\n\n\t\tseg9 = ctxp->ts_status_nvme - ctxp->ts_isr_cmd;\n\t\tif (segsum > seg9)\n\t\t\treturn;\n\t\tseg9 -= segsum;\n\t\tsegsum += seg9;\n\n\t\tif (ctxp->ts_isr_status < ctxp->ts_isr_cmd)\n\t\t\treturn;\n\t\tseg10 = (ctxp->ts_isr_status -\n\t\t\tctxp->ts_isr_cmd);\n\t} else {\n\t\tif (ctxp->ts_isr_data < ctxp->ts_isr_cmd)\n\t\t\treturn;\n\t\tseg6 =  0;\n\t\tseg7 =  0;\n\t\tseg8 =  0;\n\t\tseg9 =  0;\n\t\tseg10 = (ctxp->ts_isr_data - ctxp->ts_isr_cmd);\n\t}\n\n\tphba->ktime_seg1_total += seg1;\n\tif (seg1 < phba->ktime_seg1_min)\n\t\tphba->ktime_seg1_min = seg1;\n\telse if (seg1 > phba->ktime_seg1_max)\n\t\tphba->ktime_seg1_max = seg1;\n\n\tphba->ktime_seg2_total += seg2;\n\tif (seg2 < phba->ktime_seg2_min)\n\t\tphba->ktime_seg2_min = seg2;\n\telse if (seg2 > phba->ktime_seg2_max)\n\t\tphba->ktime_seg2_max = seg2;\n\n\tphba->ktime_seg3_total += seg3;\n\tif (seg3 < phba->ktime_seg3_min)\n\t\tphba->ktime_seg3_min = seg3;\n\telse if (seg3 > phba->ktime_seg3_max)\n\t\tphba->ktime_seg3_max = seg3;\n\n\tphba->ktime_seg4_total += seg4;\n\tif (seg4 < phba->ktime_seg4_min)\n\t\tphba->ktime_seg4_min = seg4;\n\telse if (seg4 > phba->ktime_seg4_max)\n\t\tphba->ktime_seg4_max = seg4;\n\n\tphba->ktime_seg5_total += seg5;\n\tif (seg5 < phba->ktime_seg5_min)\n\t\tphba->ktime_seg5_min = seg5;\n\telse if (seg5 > phba->ktime_seg5_max)\n\t\tphba->ktime_seg5_max = seg5;\n\n\tphba->ktime_data_samples++;\n\tif (!seg6)\n\t\tgoto out;\n\n\tphba->ktime_seg6_total += seg6;\n\tif (seg6 < phba->ktime_seg6_min)\n\t\tphba->ktime_seg6_min = seg6;\n\telse if (seg6 > phba->ktime_seg6_max)\n\t\tphba->ktime_seg6_max = seg6;\n\n\tphba->ktime_seg7_total += seg7;\n\tif (seg7 < phba->ktime_seg7_min)\n\t\tphba->ktime_seg7_min = seg7;\n\telse if (seg7 > phba->ktime_seg7_max)\n\t\tphba->ktime_seg7_max = seg7;\n\n\tphba->ktime_seg8_total += seg8;\n\tif (seg8 < phba->ktime_seg8_min)\n\t\tphba->ktime_seg8_min = seg8;\n\telse if (seg8 > phba->ktime_seg8_max)\n\t\tphba->ktime_seg8_max = seg8;\n\n\tphba->ktime_seg9_total += seg9;\n\tif (seg9 < phba->ktime_seg9_min)\n\t\tphba->ktime_seg9_min = seg9;\n\telse if (seg9 > phba->ktime_seg9_max)\n\t\tphba->ktime_seg9_max = seg9;\nout:\n\tphba->ktime_seg10_total += seg10;\n\tif (seg10 < phba->ktime_seg10_min)\n\t\tphba->ktime_seg10_min = seg10;\n\telse if (seg10 > phba->ktime_seg10_max)\n\t\tphba->ktime_seg10_max = seg10;\n\tphba->ktime_status_samples++;\n}\n#endif\n\n \nstatic void\nlpfc_nvmet_xmt_fcp_op_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t  struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct nvmefc_tgt_fcp_req *rsp;\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tuint32_t status, result, op, logerr;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tint id;\n#endif\n\n\tctxp = cmdwqe->context_un.axchg;\n\tctxp->flag &= ~LPFC_NVME_IO_INP;\n\n\trsp = &ctxp->hdlrctx.fcp_req;\n\top = rsp->op;\n\n\tstatus = bf_get(lpfc_wcqe_c_status, wcqe);\n\tresult = wcqe->parameter;\n\n\tif (phba->targetport)\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\telse\n\t\ttgtp = NULL;\n\n\tlpfc_nvmeio_data(phba, \"NVMET FCP CMPL: xri x%x op x%x status x%x\\n\",\n\t\t\t ctxp->oxid, op, status);\n\n\tif (status) {\n\t\trsp->fcp_error = NVME_SC_DATA_XFER_ERROR;\n\t\trsp->transferred_length = 0;\n\t\tif (tgtp) {\n\t\t\tatomic_inc(&tgtp->xmt_fcp_rsp_error);\n\t\t\tif (result == IOERR_ABORT_REQUESTED)\n\t\t\t\tatomic_inc(&tgtp->xmt_fcp_rsp_aborted);\n\t\t}\n\n\t\tlogerr = LOG_NVME_IOERR;\n\n\t\t \n\t\tif (bf_get(lpfc_wcqe_c_xb, wcqe)) {\n\t\t\tctxp->flag |= LPFC_NVME_XBUSY;\n\t\t\tlogerr |= LOG_NVME_ABTS;\n\t\t\tif (tgtp)\n\t\t\t\tatomic_inc(&tgtp->xmt_fcp_rsp_xb_set);\n\n\t\t} else {\n\t\t\tctxp->flag &= ~LPFC_NVME_XBUSY;\n\t\t}\n\n\t\tlpfc_printf_log(phba, KERN_INFO, logerr,\n\t\t\t\t\"6315 IO Error Cmpl oxid: x%x xri: x%x %x/%x \"\n\t\t\t\t\"XBUSY:x%x\\n\",\n\t\t\t\tctxp->oxid, ctxp->ctxbuf->sglq->sli4_xritag,\n\t\t\t\tstatus, result, ctxp->flag);\n\n\t} else {\n\t\trsp->fcp_error = NVME_SC_SUCCESS;\n\t\tif (op == NVMET_FCOP_RSP)\n\t\t\trsp->transferred_length = rsp->rsplen;\n\t\telse\n\t\t\trsp->transferred_length = rsp->transfer_length;\n\t\tif (tgtp)\n\t\t\tatomic_inc(&tgtp->xmt_fcp_rsp_cmpl);\n\t}\n\n\tif ((op == NVMET_FCOP_READDATA_RSP) ||\n\t    (op == NVMET_FCOP_RSP)) {\n\t\t \n\t\tctxp->state = LPFC_NVME_STE_DONE;\n\t\tctxp->entry_cnt++;\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\tif (ctxp->ts_cmd_nvme) {\n\t\t\tif (rsp->op == NVMET_FCOP_READDATA_RSP) {\n\t\t\t\tctxp->ts_isr_data =\n\t\t\t\t\tcmdwqe->isr_timestamp;\n\t\t\t\tctxp->ts_data_nvme =\n\t\t\t\t\tktime_get_ns();\n\t\t\t\tctxp->ts_nvme_status =\n\t\t\t\t\tctxp->ts_data_nvme;\n\t\t\t\tctxp->ts_status_wqput =\n\t\t\t\t\tctxp->ts_data_nvme;\n\t\t\t\tctxp->ts_isr_status =\n\t\t\t\t\tctxp->ts_data_nvme;\n\t\t\t\tctxp->ts_status_nvme =\n\t\t\t\t\tctxp->ts_data_nvme;\n\t\t\t} else {\n\t\t\t\tctxp->ts_isr_status =\n\t\t\t\t\tcmdwqe->isr_timestamp;\n\t\t\t\tctxp->ts_status_nvme =\n\t\t\t\t\tktime_get_ns();\n\t\t\t}\n\t\t}\n#endif\n\t\trsp->done(rsp);\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\tif (ctxp->ts_cmd_nvme)\n\t\t\tlpfc_nvmet_ktime(phba, ctxp);\n#endif\n\t\t \n\t} else {\n\t\tctxp->entry_cnt++;\n\t\tmemset_startat(cmdwqe, 0, cmd_flag);\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\tif (ctxp->ts_cmd_nvme) {\n\t\t\tctxp->ts_isr_data = cmdwqe->isr_timestamp;\n\t\t\tctxp->ts_data_nvme = ktime_get_ns();\n\t\t}\n#endif\n\t\trsp->done(rsp);\n\t}\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tif (phba->hdwqstat_on & LPFC_CHECK_NVMET_IO) {\n\t\tid = raw_smp_processor_id();\n\t\tthis_cpu_inc(phba->sli4_hba.c_stat->cmpl_io);\n\t\tif (ctxp->cpu != id)\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\t\"6704 CPU Check cmdcmpl: \"\n\t\t\t\t\t\"cpu %d expect %d\\n\",\n\t\t\t\t\tid, ctxp->cpu);\n\t}\n#endif\n}\n\n \nint\n__lpfc_nvme_xmt_ls_rsp(struct lpfc_async_xchg_ctx *axchg,\n\t\t\tstruct nvmefc_ls_rsp *ls_rsp,\n\t\t\tvoid (*xmt_ls_rsp_cmp)(struct lpfc_hba *phba,\n\t\t\t\tstruct lpfc_iocbq *cmdwqe,\n\t\t\t\tstruct lpfc_iocbq *rspwqe))\n{\n\tstruct lpfc_hba *phba = axchg->phba;\n\tstruct hbq_dmabuf *nvmebuf = (struct hbq_dmabuf *)axchg->rqb_buffer;\n\tstruct lpfc_iocbq *nvmewqeq;\n\tstruct lpfc_dmabuf dmabuf;\n\tstruct ulp_bde64 bpl;\n\tint rc;\n\n\tif (phba->pport->load_flag & FC_UNLOADING)\n\t\treturn -ENODEV;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\"6023 NVMEx LS rsp oxid x%x\\n\", axchg->oxid);\n\n\tif (axchg->state != LPFC_NVME_STE_LS_RCV || axchg->entry_cnt != 1) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6412 NVMEx LS rsp state mismatch \"\n\t\t\t\t\"oxid x%x: %d %d\\n\",\n\t\t\t\taxchg->oxid, axchg->state, axchg->entry_cnt);\n\t\treturn -EALREADY;\n\t}\n\taxchg->state = LPFC_NVME_STE_LS_RSP;\n\taxchg->entry_cnt++;\n\n\tnvmewqeq = lpfc_nvmet_prep_ls_wqe(phba, axchg, ls_rsp->rspdma,\n\t\t\t\t\t ls_rsp->rsplen);\n\tif (nvmewqeq == NULL) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6150 NVMEx LS Drop Rsp x%x: Prep\\n\",\n\t\t\t\taxchg->oxid);\n\t\trc = -ENOMEM;\n\t\tgoto out_free_buf;\n\t}\n\n\t \n\tnvmewqeq->num_bdes = 1;\n\tnvmewqeq->hba_wqidx = 0;\n\tnvmewqeq->bpl_dmabuf = &dmabuf;\n\tdmabuf.virt = &bpl;\n\tbpl.addrLow = nvmewqeq->wqe.xmit_sequence.bde.addrLow;\n\tbpl.addrHigh = nvmewqeq->wqe.xmit_sequence.bde.addrHigh;\n\tbpl.tus.f.bdeSize = ls_rsp->rsplen;\n\tbpl.tus.f.bdeFlags = 0;\n\tbpl.tus.w = le32_to_cpu(bpl.tus.w);\n\t \n\n\tnvmewqeq->cmd_cmpl = xmt_ls_rsp_cmp;\n\tnvmewqeq->context_un.axchg = axchg;\n\n\tlpfc_nvmeio_data(phba, \"NVMEx LS RSP: xri x%x wqidx x%x len x%x\\n\",\n\t\t\t axchg->oxid, nvmewqeq->hba_wqidx, ls_rsp->rsplen);\n\n\trc = lpfc_sli4_issue_wqe(phba, axchg->hdwq, nvmewqeq);\n\n\t \n\tnvmewqeq->bpl_dmabuf = NULL;\n\n\tif (rc == WQE_SUCCESS) {\n\t\t \n\t\tlpfc_in_buf_free(phba, &nvmebuf->dbuf);\n\t\treturn 0;\n\t}\n\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6151 NVMEx LS RSP x%x: failed to transmit %d\\n\",\n\t\t\taxchg->oxid, rc);\n\n\trc = -ENXIO;\n\n\tlpfc_nlp_put(nvmewqeq->ndlp);\n\nout_free_buf:\n\t \n\tlpfc_in_buf_free(phba, &nvmebuf->dbuf);\n\n\t \n\tlpfc_nvme_unsol_ls_issue_abort(phba, axchg, axchg->sid, axchg->oxid);\n\treturn rc;\n}\n\n \nstatic int\nlpfc_nvmet_xmt_ls_rsp(struct nvmet_fc_target_port *tgtport,\n\t\t      struct nvmefc_ls_rsp *ls_rsp)\n{\n\tstruct lpfc_async_xchg_ctx *axchg =\n\t\tcontainer_of(ls_rsp, struct lpfc_async_xchg_ctx, ls_rsp);\n\tstruct lpfc_nvmet_tgtport *nvmep = tgtport->private;\n\tint rc;\n\n\tif (axchg->phba->pport->load_flag & FC_UNLOADING)\n\t\treturn -ENODEV;\n\n\trc = __lpfc_nvme_xmt_ls_rsp(axchg, ls_rsp, lpfc_nvmet_xmt_ls_rsp_cmp);\n\n\tif (rc) {\n\t\tatomic_inc(&nvmep->xmt_ls_drop);\n\t\t \n\t\tif (rc != -EALREADY)\n\t\t\tatomic_inc(&nvmep->xmt_ls_abort);\n\t\treturn rc;\n\t}\n\n\tatomic_inc(&nvmep->xmt_ls_rsp);\n\treturn 0;\n}\n\nstatic int\nlpfc_nvmet_xmt_fcp_op(struct nvmet_fc_target_port *tgtport,\n\t\t      struct nvmefc_tgt_fcp_req *rsp)\n{\n\tstruct lpfc_nvmet_tgtport *lpfc_nvmep = tgtport->private;\n\tstruct lpfc_async_xchg_ctx *ctxp =\n\t\tcontainer_of(rsp, struct lpfc_async_xchg_ctx, hdlrctx.fcp_req);\n\tstruct lpfc_hba *phba = ctxp->phba;\n\tstruct lpfc_queue *wq;\n\tstruct lpfc_iocbq *nvmewqeq;\n\tstruct lpfc_sli_ring *pring;\n\tunsigned long iflags;\n\tint rc;\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tint id;\n#endif\n\n\tif (phba->pport->load_flag & FC_UNLOADING) {\n\t\trc = -ENODEV;\n\t\tgoto aerr;\n\t}\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tif (ctxp->ts_cmd_nvme) {\n\t\tif (rsp->op == NVMET_FCOP_RSP)\n\t\t\tctxp->ts_nvme_status = ktime_get_ns();\n\t\telse\n\t\t\tctxp->ts_nvme_data = ktime_get_ns();\n\t}\n\n\t \n\tif (!ctxp->hdwq)\n\t\tctxp->hdwq = &phba->sli4_hba.hdwq[rsp->hwqid];\n\n\tif (phba->hdwqstat_on & LPFC_CHECK_NVMET_IO) {\n\t\tid = raw_smp_processor_id();\n\t\tthis_cpu_inc(phba->sli4_hba.c_stat->xmt_io);\n\t\tif (rsp->hwqid != id)\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\t\"6705 CPU Check OP: \"\n\t\t\t\t\t\"cpu %d expect %d\\n\",\n\t\t\t\t\tid, rsp->hwqid);\n\t\tctxp->cpu = id;  \n\t}\n#endif\n\n\t \n\tif ((ctxp->flag & LPFC_NVME_ABTS_RCV) ||\n\t    (ctxp->state == LPFC_NVME_STE_ABORT)) {\n\t\tatomic_inc(&lpfc_nvmep->xmt_fcp_drop);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6102 IO oxid x%x aborted\\n\",\n\t\t\t\tctxp->oxid);\n\t\trc = -ENXIO;\n\t\tgoto aerr;\n\t}\n\n\tnvmewqeq = lpfc_nvmet_prep_fcp_wqe(phba, ctxp);\n\tif (nvmewqeq == NULL) {\n\t\tatomic_inc(&lpfc_nvmep->xmt_fcp_drop);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6152 FCP Drop IO x%x: Prep\\n\",\n\t\t\t\tctxp->oxid);\n\t\trc = -ENXIO;\n\t\tgoto aerr;\n\t}\n\n\tnvmewqeq->cmd_cmpl = lpfc_nvmet_xmt_fcp_op_cmp;\n\tnvmewqeq->context_un.axchg = ctxp;\n\tnvmewqeq->cmd_flag |=  LPFC_IO_NVMET;\n\tctxp->wqeq->hba_wqidx = rsp->hwqid;\n\n\tlpfc_nvmeio_data(phba, \"NVMET FCP CMND: xri x%x op x%x len x%x\\n\",\n\t\t\t ctxp->oxid, rsp->op, rsp->rsplen);\n\n\tctxp->flag |= LPFC_NVME_IO_INP;\n\trc = lpfc_sli4_issue_wqe(phba, ctxp->hdwq, nvmewqeq);\n\tif (rc == WQE_SUCCESS) {\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\tif (!ctxp->ts_cmd_nvme)\n\t\t\treturn 0;\n\t\tif (rsp->op == NVMET_FCOP_RSP)\n\t\t\tctxp->ts_status_wqput = ktime_get_ns();\n\t\telse\n\t\t\tctxp->ts_data_wqput = ktime_get_ns();\n#endif\n\t\treturn 0;\n\t}\n\n\tif (rc == -EBUSY) {\n\t\t \n\t\tctxp->flag |= LPFC_NVME_DEFER_WQFULL;\n\t\twq = ctxp->hdwq->io_wq;\n\t\tpring = wq->pring;\n\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t\tlist_add_tail(&nvmewqeq->list, &wq->wqfull_list);\n\t\twq->q_flag |= HBA_NVMET_WQFULL;\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\tatomic_inc(&lpfc_nvmep->defer_wqfull);\n\t\treturn 0;\n\t}\n\n\t \n\tatomic_inc(&lpfc_nvmep->xmt_fcp_drop);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6153 FCP Drop IO x%x: Issue: %d\\n\",\n\t\t\tctxp->oxid, rc);\n\n\tctxp->wqeq->hba_wqidx = 0;\n\tnvmewqeq->context_un.axchg = NULL;\n\tnvmewqeq->bpl_dmabuf = NULL;\n\trc = -EBUSY;\naerr:\n\treturn rc;\n}\n\nstatic void\nlpfc_nvmet_targetport_delete(struct nvmet_fc_target_port *targetport)\n{\n\tstruct lpfc_nvmet_tgtport *tport = targetport->private;\n\n\t \n\tif (tport->phba->targetport)\n\t\tcomplete(tport->tport_unreg_cmp);\n}\n\nstatic void\nlpfc_nvmet_xmt_fcp_abort(struct nvmet_fc_target_port *tgtport,\n\t\t\t struct nvmefc_tgt_fcp_req *req)\n{\n\tstruct lpfc_nvmet_tgtport *lpfc_nvmep = tgtport->private;\n\tstruct lpfc_async_xchg_ctx *ctxp =\n\t\tcontainer_of(req, struct lpfc_async_xchg_ctx, hdlrctx.fcp_req);\n\tstruct lpfc_hba *phba = ctxp->phba;\n\tstruct lpfc_queue *wq;\n\tunsigned long flags;\n\n\tif (phba->pport->load_flag & FC_UNLOADING)\n\t\treturn;\n\n\tif (!ctxp->hdwq)\n\t\tctxp->hdwq = &phba->sli4_hba.hdwq[0];\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6103 NVMET Abort op: oxid x%x flg x%x ste %d\\n\",\n\t\t\tctxp->oxid, ctxp->flag, ctxp->state);\n\n\tlpfc_nvmeio_data(phba, \"NVMET FCP ABRT: xri x%x flg x%x ste x%x\\n\",\n\t\t\t ctxp->oxid, ctxp->flag, ctxp->state);\n\n\tatomic_inc(&lpfc_nvmep->xmt_fcp_abort);\n\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\n\t \n\tif (ctxp->flag & (LPFC_NVME_XBUSY | LPFC_NVME_ABORT_OP)) {\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\treturn;\n\t}\n\tctxp->flag |= LPFC_NVME_ABORT_OP;\n\n\tif (ctxp->flag & LPFC_NVME_DEFER_WQFULL) {\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, ctxp->sid,\n\t\t\t\t\t\t ctxp->oxid);\n\t\twq = ctxp->hdwq->io_wq;\n\t\tlpfc_nvmet_wqfull_flush(phba, wq, ctxp);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\n\t \n\tif (ctxp->state == LPFC_NVME_STE_RCV)\n\t\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, ctxp->sid,\n\t\t\t\t\t\t ctxp->oxid);\n\telse\n\t\tlpfc_nvmet_sol_fcp_issue_abort(phba, ctxp, ctxp->sid,\n\t\t\t\t\t       ctxp->oxid);\n}\n\nstatic void\nlpfc_nvmet_xmt_fcp_release(struct nvmet_fc_target_port *tgtport,\n\t\t\t   struct nvmefc_tgt_fcp_req *rsp)\n{\n\tstruct lpfc_nvmet_tgtport *lpfc_nvmep = tgtport->private;\n\tstruct lpfc_async_xchg_ctx *ctxp =\n\t\tcontainer_of(rsp, struct lpfc_async_xchg_ctx, hdlrctx.fcp_req);\n\tstruct lpfc_hba *phba = ctxp->phba;\n\tunsigned long flags;\n\tbool aborting = false;\n\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tif (ctxp->flag & LPFC_NVME_XBUSY)\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\"6027 NVMET release with XBUSY flag x%x\"\n\t\t\t\t\" oxid x%x\\n\",\n\t\t\t\tctxp->flag, ctxp->oxid);\n\telse if (ctxp->state != LPFC_NVME_STE_DONE &&\n\t\t ctxp->state != LPFC_NVME_STE_ABORT)\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6413 NVMET release bad state %d %d oxid x%x\\n\",\n\t\t\t\tctxp->state, ctxp->entry_cnt, ctxp->oxid);\n\n\tif ((ctxp->flag & LPFC_NVME_ABORT_OP) ||\n\t    (ctxp->flag & LPFC_NVME_XBUSY)) {\n\t\taborting = true;\n\t\t \n\t\tlpfc_nvmet_defer_release(phba, ctxp);\n\t}\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\n\tlpfc_nvmeio_data(phba, \"NVMET FCP FREE: xri x%x ste %d abt %d\\n\", ctxp->oxid,\n\t\t\t ctxp->state, aborting);\n\n\tatomic_inc(&lpfc_nvmep->xmt_fcp_release);\n\tctxp->flag &= ~LPFC_NVME_TNOTIFY;\n\n\tif (aborting)\n\t\treturn;\n\n\tlpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);\n}\n\nstatic void\nlpfc_nvmet_defer_rcv(struct nvmet_fc_target_port *tgtport,\n\t\t     struct nvmefc_tgt_fcp_req *rsp)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct lpfc_async_xchg_ctx *ctxp =\n\t\tcontainer_of(rsp, struct lpfc_async_xchg_ctx, hdlrctx.fcp_req);\n\tstruct rqb_dmabuf *nvmebuf = ctxp->rqb_buffer;\n\tstruct lpfc_hba *phba = ctxp->phba;\n\tunsigned long iflag;\n\n\n\tlpfc_nvmeio_data(phba, \"NVMET DEFERRCV: xri x%x sz %d CPU %02x\\n\",\n\t\t\t ctxp->oxid, ctxp->size, raw_smp_processor_id());\n\n\tif (!nvmebuf) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\"6425 Defer rcv: no buffer oxid x%x: \"\n\t\t\t\t\"flg %x ste %x\\n\",\n\t\t\t\tctxp->oxid, ctxp->flag, ctxp->state);\n\t\treturn;\n\t}\n\n\ttgtp = phba->targetport->private;\n\tif (tgtp)\n\t\tatomic_inc(&tgtp->rcv_fcp_cmd_defer);\n\n\t \n\tnvmebuf->hrq->rqbp->rqb_free_buffer(phba, nvmebuf);\n\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\tctxp->rqb_buffer = NULL;\n\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n}\n\n \nstatic void\nlpfc_nvmet_ls_req_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t      struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\t__lpfc_nvme_ls_req_cmp(phba, cmdwqe->vport, cmdwqe, wcqe);\n}\n\n \nstatic int\nlpfc_nvmet_ls_req(struct nvmet_fc_target_port *targetport,\n\t\t  void *hosthandle,\n\t\t  struct nvmefc_ls_req *pnvme_lsreq)\n{\n\tstruct lpfc_nvmet_tgtport *lpfc_nvmet = targetport->private;\n\tstruct lpfc_hba *phba;\n\tstruct lpfc_nodelist *ndlp;\n\tint ret;\n\tu32 hstate;\n\n\tif (!lpfc_nvmet)\n\t\treturn -EINVAL;\n\n\tphba = lpfc_nvmet->phba;\n\tif (phba->pport->load_flag & FC_UNLOADING)\n\t\treturn -EINVAL;\n\n\thstate = atomic_read(&lpfc_nvmet->state);\n\tif (hstate == LPFC_NVMET_INV_HOST_ACTIVE)\n\t\treturn -EACCES;\n\n\tndlp = (struct lpfc_nodelist *)hosthandle;\n\n\tret = __lpfc_nvme_ls_req(phba->pport, ndlp, pnvme_lsreq,\n\t\t\t\t lpfc_nvmet_ls_req_cmp);\n\n\treturn ret;\n}\n\n \nstatic void\nlpfc_nvmet_ls_abort(struct nvmet_fc_target_port *targetport,\n\t\t    void *hosthandle,\n\t\t    struct nvmefc_ls_req *pnvme_lsreq)\n{\n\tstruct lpfc_nvmet_tgtport *lpfc_nvmet = targetport->private;\n\tstruct lpfc_hba *phba;\n\tstruct lpfc_nodelist *ndlp;\n\tint ret;\n\n\tphba = lpfc_nvmet->phba;\n\tif (phba->pport->load_flag & FC_UNLOADING)\n\t\treturn;\n\n\tndlp = (struct lpfc_nodelist *)hosthandle;\n\n\tret = __lpfc_nvme_ls_abort(phba->pport, ndlp, pnvme_lsreq);\n\tif (!ret)\n\t\tatomic_inc(&lpfc_nvmet->xmt_ls_abort);\n}\n\nstatic void\nlpfc_nvmet_host_release(void *hosthandle)\n{\n\tstruct lpfc_nodelist *ndlp = hosthandle;\n\tstruct lpfc_hba *phba = ndlp->phba;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\n\tif (!phba->targetport || !phba->targetport->private)\n\t\treturn;\n\n\tlpfc_printf_log(phba, KERN_ERR, LOG_NVME,\n\t\t\t\"6202 NVMET XPT releasing hosthandle x%px \"\n\t\t\t\"DID x%x xflags x%x refcnt %d\\n\",\n\t\t\thosthandle, ndlp->nlp_DID, ndlp->fc4_xpt_flags,\n\t\t\tkref_read(&ndlp->kref));\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tspin_lock_irq(&ndlp->lock);\n\tndlp->fc4_xpt_flags &= ~NLP_XPT_HAS_HH;\n\tspin_unlock_irq(&ndlp->lock);\n\tlpfc_nlp_put(ndlp);\n\tatomic_set(&tgtp->state, 0);\n}\n\nstatic void\nlpfc_nvmet_discovery_event(struct nvmet_fc_target_port *tgtport)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct lpfc_hba *phba;\n\tuint32_t rc;\n\n\ttgtp = tgtport->private;\n\tphba = tgtp->phba;\n\n\trc = lpfc_issue_els_rscn(phba->pport, 0);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6420 NVMET subsystem change: Notification %s\\n\",\n\t\t\t(rc) ? \"Failed\" : \"Sent\");\n}\n\nstatic struct nvmet_fc_target_template lpfc_tgttemplate = {\n\t.targetport_delete = lpfc_nvmet_targetport_delete,\n\t.xmt_ls_rsp     = lpfc_nvmet_xmt_ls_rsp,\n\t.fcp_op         = lpfc_nvmet_xmt_fcp_op,\n\t.fcp_abort      = lpfc_nvmet_xmt_fcp_abort,\n\t.fcp_req_release = lpfc_nvmet_xmt_fcp_release,\n\t.defer_rcv\t= lpfc_nvmet_defer_rcv,\n\t.discovery_event = lpfc_nvmet_discovery_event,\n\t.ls_req         = lpfc_nvmet_ls_req,\n\t.ls_abort       = lpfc_nvmet_ls_abort,\n\t.host_release   = lpfc_nvmet_host_release,\n\n\t.max_hw_queues  = 1,\n\t.max_sgl_segments = LPFC_NVMET_DEFAULT_SEGS,\n\t.max_dif_sgl_segments = LPFC_NVMET_DEFAULT_SEGS,\n\t.dma_boundary = 0xFFFFFFFF,\n\n\t \n\t.target_features = 0,\n\t \n\t.target_priv_sz = sizeof(struct lpfc_nvmet_tgtport),\n\t.lsrqst_priv_sz = 0,\n};\n\nstatic void\n__lpfc_nvmet_clean_io_for_cpu(struct lpfc_hba *phba,\n\t\tstruct lpfc_nvmet_ctx_info *infop)\n{\n\tstruct lpfc_nvmet_ctxbuf *ctx_buf, *next_ctx_buf;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&infop->nvmet_ctx_list_lock, flags);\n\tlist_for_each_entry_safe(ctx_buf, next_ctx_buf,\n\t\t\t\t&infop->nvmet_ctx_list, list) {\n\t\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\tlist_del_init(&ctx_buf->list);\n\t\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\n\t\tspin_lock(&phba->hbalock);\n\t\t__lpfc_clear_active_sglq(phba, ctx_buf->sglq->sli4_lxritag);\n\t\tspin_unlock(&phba->hbalock);\n\n\t\tctx_buf->sglq->state = SGL_FREED;\n\t\tctx_buf->sglq->ndlp = NULL;\n\n\t\tspin_lock(&phba->sli4_hba.sgl_list_lock);\n\t\tlist_add_tail(&ctx_buf->sglq->list,\n\t\t\t\t&phba->sli4_hba.lpfc_nvmet_sgl_list);\n\t\tspin_unlock(&phba->sli4_hba.sgl_list_lock);\n\n\t\tlpfc_sli_release_iocbq(phba, ctx_buf->iocbq);\n\t\tkfree(ctx_buf->context);\n\t}\n\tspin_unlock_irqrestore(&infop->nvmet_ctx_list_lock, flags);\n}\n\nstatic void\nlpfc_nvmet_cleanup_io_context(struct lpfc_hba *phba)\n{\n\tstruct lpfc_nvmet_ctx_info *infop;\n\tint i, j;\n\n\t \n\tinfop = phba->sli4_hba.nvmet_ctx_info;\n\tif (!infop)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < phba->cfg_nvmet_mrq; i++) {\n\t\tfor_each_present_cpu(j) {\n\t\t\tinfop = lpfc_get_ctx_list(phba, j, i);\n\t\t\t__lpfc_nvmet_clean_io_for_cpu(phba, infop);\n\t\t}\n\t}\n\tkfree(phba->sli4_hba.nvmet_ctx_info);\n\tphba->sli4_hba.nvmet_ctx_info = NULL;\n}\n\nstatic int\nlpfc_nvmet_setup_io_context(struct lpfc_hba *phba)\n{\n\tstruct lpfc_nvmet_ctxbuf *ctx_buf;\n\tstruct lpfc_iocbq *nvmewqe;\n\tunion lpfc_wqe128 *wqe;\n\tstruct lpfc_nvmet_ctx_info *last_infop;\n\tstruct lpfc_nvmet_ctx_info *infop;\n\tint i, j, idx, cpu;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME,\n\t\t\t\"6403 Allocate NVMET resources for %d XRIs\\n\",\n\t\t\tphba->sli4_hba.nvmet_xri_cnt);\n\n\tphba->sli4_hba.nvmet_ctx_info = kcalloc(\n\t\tphba->sli4_hba.num_possible_cpu * phba->cfg_nvmet_mrq,\n\t\tsizeof(struct lpfc_nvmet_ctx_info), GFP_KERNEL);\n\tif (!phba->sli4_hba.nvmet_ctx_info) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6419 Failed allocate memory for \"\n\t\t\t\t\"nvmet context lists\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor_each_possible_cpu(i) {\n\t\tfor (j = 0; j < phba->cfg_nvmet_mrq; j++) {\n\t\t\tinfop = lpfc_get_ctx_list(phba, i, j);\n\t\t\tINIT_LIST_HEAD(&infop->nvmet_ctx_list);\n\t\t\tspin_lock_init(&infop->nvmet_ctx_list_lock);\n\t\t\tinfop->nvmet_ctx_list_cnt = 0;\n\t\t}\n\t}\n\n\t \n\tfor (j = 0; j < phba->cfg_nvmet_mrq; j++) {\n\t\tlast_infop = lpfc_get_ctx_list(phba,\n\t\t\t\t\t       cpumask_first(cpu_present_mask),\n\t\t\t\t\t       j);\n\t\tfor (i = phba->sli4_hba.num_possible_cpu - 1;  i >= 0; i--) {\n\t\t\tinfop = lpfc_get_ctx_list(phba, i, j);\n\t\t\tinfop->nvmet_ctx_next_cpu = last_infop;\n\t\t\tlast_infop = infop;\n\t\t}\n\t}\n\n\t \n\tidx = 0;\n\tcpu = cpumask_first(cpu_present_mask);\n\tfor (i = 0; i < phba->sli4_hba.nvmet_xri_cnt; i++) {\n\t\tctx_buf = kzalloc(sizeof(*ctx_buf), GFP_KERNEL);\n\t\tif (!ctx_buf) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6404 Ran out of memory for NVMET\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tctx_buf->context = kzalloc(sizeof(*ctx_buf->context),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!ctx_buf->context) {\n\t\t\tkfree(ctx_buf);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6405 Ran out of NVMET \"\n\t\t\t\t\t\"context memory\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tctx_buf->context->ctxbuf = ctx_buf;\n\t\tctx_buf->context->state = LPFC_NVME_STE_FREE;\n\n\t\tctx_buf->iocbq = lpfc_sli_get_iocbq(phba);\n\t\tif (!ctx_buf->iocbq) {\n\t\t\tkfree(ctx_buf->context);\n\t\t\tkfree(ctx_buf);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6406 Ran out of NVMET iocb/WQEs\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tctx_buf->iocbq->cmd_flag = LPFC_IO_NVMET;\n\t\tnvmewqe = ctx_buf->iocbq;\n\t\twqe = &nvmewqe->wqe;\n\n\t\t \n\t\tmemset(wqe, 0, sizeof(union lpfc_wqe));\n\n\t\tctx_buf->iocbq->cmd_dmabuf = NULL;\n\t\tspin_lock(&phba->sli4_hba.sgl_list_lock);\n\t\tctx_buf->sglq = __lpfc_sli_get_nvmet_sglq(phba, ctx_buf->iocbq);\n\t\tspin_unlock(&phba->sli4_hba.sgl_list_lock);\n\t\tif (!ctx_buf->sglq) {\n\t\t\tlpfc_sli_release_iocbq(phba, ctx_buf->iocbq);\n\t\t\tkfree(ctx_buf->context);\n\t\t\tkfree(ctx_buf);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6407 Ran out of NVMET XRIs\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tINIT_WORK(&ctx_buf->defer_work, lpfc_nvmet_fcp_rqst_defer_work);\n\n\t\t \n\t\tinfop = lpfc_get_ctx_list(phba, cpu, idx);\n\t\tspin_lock(&infop->nvmet_ctx_list_lock);\n\t\tlist_add_tail(&ctx_buf->list, &infop->nvmet_ctx_list);\n\t\tinfop->nvmet_ctx_list_cnt++;\n\t\tspin_unlock(&infop->nvmet_ctx_list_lock);\n\n\t\t \n\t\tidx++;\n\t\tif (idx >= phba->cfg_nvmet_mrq) {\n\t\t\tidx = 0;\n\t\t\tcpu = cpumask_first(cpu_present_mask);\n\t\t\tcontinue;\n\t\t}\n\t\tcpu = lpfc_next_present_cpu(cpu);\n\t}\n\n\tfor_each_present_cpu(i) {\n\t\tfor (j = 0; j < phba->cfg_nvmet_mrq; j++) {\n\t\t\tinfop = lpfc_get_ctx_list(phba, i, j);\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME | LOG_INIT,\n\t\t\t\t\t\"6408 TOTAL NVMET ctx for CPU %d \"\n\t\t\t\t\t\"MRQ %d: cnt %d nextcpu x%px\\n\",\n\t\t\t\t\ti, j, infop->nvmet_ctx_list_cnt,\n\t\t\t\t\tinfop->nvmet_ctx_next_cpu);\n\t\t}\n\t}\n\treturn 0;\n}\n\nint\nlpfc_nvmet_create_targetport(struct lpfc_hba *phba)\n{\n\tstruct lpfc_vport  *vport = phba->pport;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct nvmet_fc_port_info pinfo;\n\tint error;\n\n\tif (phba->targetport)\n\t\treturn 0;\n\n\terror = lpfc_nvmet_setup_io_context(phba);\n\tif (error)\n\t\treturn error;\n\n\tmemset(&pinfo, 0, sizeof(struct nvmet_fc_port_info));\n\tpinfo.node_name = wwn_to_u64(vport->fc_nodename.u.wwn);\n\tpinfo.port_name = wwn_to_u64(vport->fc_portname.u.wwn);\n\tpinfo.port_id = vport->fc_myDID;\n\n\t \n\tlpfc_tgttemplate.max_sgl_segments = phba->cfg_nvme_seg_cnt + 1;\n\tlpfc_tgttemplate.max_hw_queues = phba->cfg_hdw_queue;\n\tlpfc_tgttemplate.target_features = NVMET_FCTGTFEAT_READDATA_RSP;\n\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\terror = nvmet_fc_register_targetport(&pinfo, &lpfc_tgttemplate,\n\t\t\t\t\t     &phba->pcidev->dev,\n\t\t\t\t\t     &phba->targetport);\n#else\n\terror = -ENOENT;\n#endif\n\tif (error) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6025 Cannot register NVME targetport x%x: \"\n\t\t\t\t\"portnm %llx nodenm %llx segs %d qs %d\\n\",\n\t\t\t\terror,\n\t\t\t\tpinfo.port_name, pinfo.node_name,\n\t\t\t\tlpfc_tgttemplate.max_sgl_segments,\n\t\t\t\tlpfc_tgttemplate.max_hw_queues);\n\t\tphba->targetport = NULL;\n\t\tphba->nvmet_support = 0;\n\n\t\tlpfc_nvmet_cleanup_io_context(phba);\n\n\t} else {\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)\n\t\t\tphba->targetport->private;\n\t\ttgtp->phba = phba;\n\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\t\"6026 Registered NVME \"\n\t\t\t\t\"targetport: x%px, private x%px \"\n\t\t\t\t\"portnm %llx nodenm %llx segs %d qs %d\\n\",\n\t\t\t\tphba->targetport, tgtp,\n\t\t\t\tpinfo.port_name, pinfo.node_name,\n\t\t\t\tlpfc_tgttemplate.max_sgl_segments,\n\t\t\t\tlpfc_tgttemplate.max_hw_queues);\n\n\t\tatomic_set(&tgtp->rcv_ls_req_in, 0);\n\t\tatomic_set(&tgtp->rcv_ls_req_out, 0);\n\t\tatomic_set(&tgtp->rcv_ls_req_drop, 0);\n\t\tatomic_set(&tgtp->xmt_ls_abort, 0);\n\t\tatomic_set(&tgtp->xmt_ls_abort_cmpl, 0);\n\t\tatomic_set(&tgtp->xmt_ls_rsp, 0);\n\t\tatomic_set(&tgtp->xmt_ls_drop, 0);\n\t\tatomic_set(&tgtp->xmt_ls_rsp_error, 0);\n\t\tatomic_set(&tgtp->xmt_ls_rsp_xb_set, 0);\n\t\tatomic_set(&tgtp->xmt_ls_rsp_aborted, 0);\n\t\tatomic_set(&tgtp->xmt_ls_rsp_cmpl, 0);\n\t\tatomic_set(&tgtp->rcv_fcp_cmd_in, 0);\n\t\tatomic_set(&tgtp->rcv_fcp_cmd_out, 0);\n\t\tatomic_set(&tgtp->rcv_fcp_cmd_drop, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_drop, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_read_rsp, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_read, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_write, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_release, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp_cmpl, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp_error, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp_xb_set, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp_aborted, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_rsp_drop, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_xri_abort_cqe, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_abort, 0);\n\t\tatomic_set(&tgtp->xmt_fcp_abort_cmpl, 0);\n\t\tatomic_set(&tgtp->xmt_abort_unsol, 0);\n\t\tatomic_set(&tgtp->xmt_abort_sol, 0);\n\t\tatomic_set(&tgtp->xmt_abort_rsp, 0);\n\t\tatomic_set(&tgtp->xmt_abort_rsp_error, 0);\n\t\tatomic_set(&tgtp->defer_ctx, 0);\n\t\tatomic_set(&tgtp->defer_fod, 0);\n\t\tatomic_set(&tgtp->defer_wqfull, 0);\n\t}\n\treturn error;\n}\n\nint\nlpfc_nvmet_update_targetport(struct lpfc_hba *phba)\n{\n\tstruct lpfc_vport  *vport = phba->pport;\n\n\tif (!phba->targetport)\n\t\treturn 0;\n\n\tlpfc_printf_vlog(vport, KERN_INFO, LOG_NVME,\n\t\t\t \"6007 Update NVMET port x%px did x%x\\n\",\n\t\t\t phba->targetport, vport->fc_myDID);\n\n\tphba->targetport->port_id = vport->fc_myDID;\n\treturn 0;\n}\n\n \nvoid\nlpfc_sli4_nvmet_xri_aborted(struct lpfc_hba *phba,\n\t\t\t    struct sli4_wcqe_xri_aborted *axri)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tuint16_t xri = bf_get(lpfc_wcqe_xa_xri, axri);\n\tuint16_t rxid = bf_get(lpfc_wcqe_xa_remote_xid, axri);\n\tstruct lpfc_async_xchg_ctx *ctxp, *next_ctxp;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct nvmefc_tgt_fcp_req *req = NULL;\n\tstruct lpfc_nodelist *ndlp;\n\tunsigned long iflag = 0;\n\tint rrq_empty = 0;\n\tbool released = false;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6317 XB aborted xri x%x rxid x%x\\n\", xri, rxid);\n\n\tif (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))\n\t\treturn;\n\n\tif (phba->targetport) {\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\t\tatomic_inc(&tgtp->xmt_fcp_xri_abort_cqe);\n\t}\n\n\tspin_lock_irqsave(&phba->sli4_hba.abts_nvmet_buf_list_lock, iflag);\n\tlist_for_each_entry_safe(ctxp, next_ctxp,\n\t\t\t\t &phba->sli4_hba.lpfc_abts_nvmet_ctx_list,\n\t\t\t\t list) {\n\t\tif (ctxp->ctxbuf->sglq->sli4_xritag != xri)\n\t\t\tcontinue;\n\n\t\tspin_unlock_irqrestore(&phba->sli4_hba.abts_nvmet_buf_list_lock,\n\t\t\t\t       iflag);\n\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\t \n\t\tif (ctxp->flag & LPFC_NVME_CTX_RLS &&\n\t\t    !(ctxp->flag & LPFC_NVME_ABORT_OP)) {\n\t\t\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\t\tlist_del_init(&ctxp->list);\n\t\t\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\t\treleased = true;\n\t\t}\n\t\tctxp->flag &= ~LPFC_NVME_XBUSY;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\n\t\trrq_empty = list_empty(&phba->active_rrq_list);\n\t\tndlp = lpfc_findnode_did(phba->pport, ctxp->sid);\n\t\tif (ndlp &&\n\t\t    (ndlp->nlp_state == NLP_STE_UNMAPPED_NODE ||\n\t\t     ndlp->nlp_state == NLP_STE_MAPPED_NODE)) {\n\t\t\tlpfc_set_rrq_active(phba, ndlp,\n\t\t\t\tctxp->ctxbuf->sglq->sli4_lxritag,\n\t\t\t\trxid, 1);\n\t\t\tlpfc_sli4_abts_err_handler(phba, ndlp, axri);\n\t\t}\n\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\"6318 XB aborted oxid x%x flg x%x (%x)\\n\",\n\t\t\t\tctxp->oxid, ctxp->flag, released);\n\t\tif (released)\n\t\t\tlpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);\n\n\t\tif (rrq_empty)\n\t\t\tlpfc_worker_wake_up(phba);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&phba->sli4_hba.abts_nvmet_buf_list_lock, iflag);\n\tctxp = lpfc_nvmet_get_ctx_for_xri(phba, xri);\n\tif (ctxp) {\n\t\t \n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\"6323 NVMET Rcv ABTS xri x%x ctxp state x%x \"\n\t\t\t\t\"flag x%x oxid x%x rxid x%x\\n\",\n\t\t\t\txri, ctxp->state, ctxp->flag, ctxp->oxid,\n\t\t\t\trxid);\n\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tctxp->flag |= LPFC_NVME_ABTS_RCV;\n\t\tctxp->state = LPFC_NVME_STE_ABORT;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\n\t\tlpfc_nvmeio_data(phba,\n\t\t\t\t \"NVMET ABTS RCV: xri x%x CPU %02x rjt %d\\n\",\n\t\t\t\t xri, raw_smp_processor_id(), 0);\n\n\t\treq = &ctxp->hdlrctx.fcp_req;\n\t\tif (req)\n\t\t\tnvmet_fc_rcv_fcp_abort(phba->targetport, req);\n\t}\n#endif\n}\n\nint\nlpfc_nvmet_rcv_unsol_abort(struct lpfc_vport *vport,\n\t\t\t   struct fc_frame_header *fc_hdr)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_async_xchg_ctx *ctxp, *next_ctxp;\n\tstruct nvmefc_tgt_fcp_req *rsp;\n\tuint32_t sid;\n\tuint16_t oxid, xri;\n\tunsigned long iflag = 0;\n\n\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\toxid = be16_to_cpu(fc_hdr->fh_ox_id);\n\n\tspin_lock_irqsave(&phba->sli4_hba.abts_nvmet_buf_list_lock, iflag);\n\tlist_for_each_entry_safe(ctxp, next_ctxp,\n\t\t\t\t &phba->sli4_hba.lpfc_abts_nvmet_ctx_list,\n\t\t\t\t list) {\n\t\tif (ctxp->oxid != oxid || ctxp->sid != sid)\n\t\t\tcontinue;\n\n\t\txri = ctxp->ctxbuf->sglq->sli4_xritag;\n\n\t\tspin_unlock_irqrestore(&phba->sli4_hba.abts_nvmet_buf_list_lock,\n\t\t\t\t       iflag);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tctxp->flag |= LPFC_NVME_ABTS_RCV;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\n\t\tlpfc_nvmeio_data(phba,\n\t\t\t\"NVMET ABTS RCV: xri x%x CPU %02x rjt %d\\n\",\n\t\t\txri, raw_smp_processor_id(), 0);\n\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\"6319 NVMET Rcv ABTS:acc xri x%x\\n\", xri);\n\n\t\trsp = &ctxp->hdlrctx.fcp_req;\n\t\tnvmet_fc_rcv_fcp_abort(phba->targetport, rsp);\n\n\t\t \n\t\tlpfc_sli4_seq_abort_rsp(vport, fc_hdr, 1);\n\t\treturn 0;\n\t}\n\tspin_unlock_irqrestore(&phba->sli4_hba.abts_nvmet_buf_list_lock, iflag);\n\t \n\tif (phba->sli4_hba.nvmet_io_wait_cnt) {\n\t\tstruct rqb_dmabuf *nvmebuf;\n\t\tstruct fc_frame_header *fc_hdr_tmp;\n\t\tu32 sid_tmp;\n\t\tu16 oxid_tmp;\n\t\tbool found = false;\n\n\t\tspin_lock_irqsave(&phba->sli4_hba.nvmet_io_wait_lock, iflag);\n\n\t\t \n\t\tlist_for_each_entry(nvmebuf,\n\t\t\t\t    &phba->sli4_hba.lpfc_nvmet_io_wait_list,\n\t\t\t\t    hbuf.list) {\n\t\t\tfc_hdr_tmp = (struct fc_frame_header *)\n\t\t\t\t\t(nvmebuf->hbuf.virt);\n\t\t\toxid_tmp = be16_to_cpu(fc_hdr_tmp->fh_ox_id);\n\t\t\tsid_tmp = sli4_sid_from_fc_hdr(fc_hdr_tmp);\n\t\t\tif (oxid_tmp != oxid || sid_tmp != sid)\n\t\t\t\tcontinue;\n\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\t\"6321 NVMET Rcv ABTS oxid x%x from x%x \"\n\t\t\t\t\t\"is waiting for a ctxp\\n\",\n\t\t\t\t\toxid, sid);\n\n\t\t\tlist_del_init(&nvmebuf->hbuf.list);\n\t\t\tphba->sli4_hba.nvmet_io_wait_cnt--;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irqrestore(&phba->sli4_hba.nvmet_io_wait_lock,\n\t\t\t\t       iflag);\n\n\t\t \n\t\tif (found) {\n\t\t\tnvmebuf->hrq->rqbp->rqb_free_buffer(phba, nvmebuf);\n\t\t\t \n\t\t\tlpfc_sli4_seq_abort_rsp(vport, fc_hdr, 1);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tctxp = lpfc_nvmet_get_ctx_for_oxid(phba, oxid, sid);\n\tif (ctxp) {\n\t\txri = ctxp->ctxbuf->sglq->sli4_xritag;\n\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tctxp->flag |= (LPFC_NVME_ABTS_RCV | LPFC_NVME_ABORT_OP);\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\n\t\tlpfc_nvmeio_data(phba,\n\t\t\t\t \"NVMET ABTS RCV: xri x%x CPU %02x rjt %d\\n\",\n\t\t\t\t xri, raw_smp_processor_id(), 0);\n\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\"6322 NVMET Rcv ABTS:acc oxid x%x xri x%x \"\n\t\t\t\t\"flag x%x state x%x\\n\",\n\t\t\t\tctxp->oxid, xri, ctxp->flag, ctxp->state);\n\n\t\tif (ctxp->flag & LPFC_NVME_TNOTIFY) {\n\t\t\t \n\t\t\tnvmet_fc_rcv_fcp_abort(phba->targetport,\n\t\t\t\t\t       &ctxp->hdlrctx.fcp_req);\n\t\t} else {\n\t\t\tcancel_work_sync(&ctxp->ctxbuf->defer_work);\n\t\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\t\tlpfc_nvmet_defer_release(phba, ctxp);\n\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\t}\n\t\tlpfc_nvmet_sol_fcp_issue_abort(phba, ctxp, ctxp->sid,\n\t\t\t\t\t       ctxp->oxid);\n\n\t\tlpfc_sli4_seq_abort_rsp(vport, fc_hdr, 1);\n\t\treturn 0;\n\t}\n\n\tlpfc_nvmeio_data(phba, \"NVMET ABTS RCV: oxid x%x CPU %02x rjt %d\\n\",\n\t\t\t oxid, raw_smp_processor_id(), 1);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6320 NVMET Rcv ABTS:rjt oxid x%x\\n\", oxid);\n\n\t \n\tlpfc_sli4_seq_abort_rsp(vport, fc_hdr, 0);\n#endif\n\treturn 0;\n}\n\nstatic void\nlpfc_nvmet_wqfull_flush(struct lpfc_hba *phba, struct lpfc_queue *wq,\n\t\t\tstruct lpfc_async_xchg_ctx *ctxp)\n{\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_iocbq *nvmewqeq;\n\tstruct lpfc_iocbq *next_nvmewqeq;\n\tunsigned long iflags;\n\tstruct lpfc_wcqe_complete wcqe;\n\tstruct lpfc_wcqe_complete *wcqep;\n\n\tpring = wq->pring;\n\twcqep = &wcqe;\n\n\t \n\tmemset(wcqep, 0, sizeof(struct lpfc_wcqe_complete));\n\tbf_set(lpfc_wcqe_c_status, wcqep, IOSTAT_LOCAL_REJECT);\n\twcqep->parameter = IOERR_ABORT_REQUESTED;\n\n\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\tlist_for_each_entry_safe(nvmewqeq, next_nvmewqeq,\n\t\t\t\t &wq->wqfull_list, list) {\n\t\tif (ctxp) {\n\t\t\t \n\t\t\tif (nvmewqeq->context_un.axchg == ctxp) {\n\t\t\t\tlist_del(&nvmewqeq->list);\n\t\t\t\tspin_unlock_irqrestore(&pring->ring_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\tmemcpy(&nvmewqeq->wcqe_cmpl, wcqep,\n\t\t\t\t       sizeof(*wcqep));\n\t\t\t\tlpfc_nvmet_xmt_fcp_op_cmp(phba, nvmewqeq,\n\t\t\t\t\t\t\t  nvmewqeq);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\t \n\t\t\tlist_del(&nvmewqeq->list);\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\tmemcpy(&nvmewqeq->wcqe_cmpl, wcqep, sizeof(*wcqep));\n\t\t\tlpfc_nvmet_xmt_fcp_op_cmp(phba, nvmewqeq, nvmewqeq);\n\t\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t\t}\n\t}\n\tif (!ctxp)\n\t\twq->q_flag &= ~HBA_NVMET_WQFULL;\n\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n}\n\nvoid\nlpfc_nvmet_wqfull_process(struct lpfc_hba *phba,\n\t\t\t  struct lpfc_queue *wq)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_iocbq *nvmewqeq;\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tunsigned long iflags;\n\tint rc;\n\n\t \n\tpring = wq->pring;\n\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\twhile (!list_empty(&wq->wqfull_list)) {\n\t\tlist_remove_head(&wq->wqfull_list, nvmewqeq, struct lpfc_iocbq,\n\t\t\t\t list);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\tctxp = nvmewqeq->context_un.axchg;\n\t\trc = lpfc_sli4_issue_wqe(phba, ctxp->hdwq, nvmewqeq);\n\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t\tif (rc == -EBUSY) {\n\t\t\t \n\t\t\tlist_add(&nvmewqeq->list, &wq->wqfull_list);\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn;\n\t\t}\n\t\tif (rc == WQE_SUCCESS) {\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\t\t\tif (ctxp->ts_cmd_nvme) {\n\t\t\t\tif (ctxp->hdlrctx.fcp_req.op == NVMET_FCOP_RSP)\n\t\t\t\t\tctxp->ts_status_wqput = ktime_get_ns();\n\t\t\t\telse\n\t\t\t\t\tctxp->ts_data_wqput = ktime_get_ns();\n\t\t\t}\n#endif\n\t\t} else {\n\t\t\tWARN_ON(rc);\n\t\t}\n\t}\n\twq->q_flag &= ~HBA_NVMET_WQFULL;\n\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n#endif\n}\n\nvoid\nlpfc_nvmet_destroy_targetport(struct lpfc_hba *phba)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct lpfc_queue *wq;\n\tuint32_t qidx;\n\tDECLARE_COMPLETION_ONSTACK(tport_unreg_cmp);\n\n\tif (phba->nvmet_support == 0)\n\t\treturn;\n\tif (phba->targetport) {\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\t\tfor (qidx = 0; qidx < phba->cfg_hdw_queue; qidx++) {\n\t\t\twq = phba->sli4_hba.hdwq[qidx].io_wq;\n\t\t\tlpfc_nvmet_wqfull_flush(phba, wq, NULL);\n\t\t}\n\t\ttgtp->tport_unreg_cmp = &tport_unreg_cmp;\n\t\tnvmet_fc_unregister_targetport(phba->targetport);\n\t\tif (!wait_for_completion_timeout(&tport_unreg_cmp,\n\t\t\t\t\tmsecs_to_jiffies(LPFC_NVMET_WAIT_TMO)))\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6179 Unreg targetport x%px timeout \"\n\t\t\t\t\t\"reached.\\n\", phba->targetport);\n\t\tlpfc_nvmet_cleanup_io_context(phba);\n\t}\n\tphba->targetport = NULL;\n#endif\n}\n\n \nint\nlpfc_nvmet_handle_lsreq(struct lpfc_hba *phba,\n\t\t\tstruct lpfc_async_xchg_ctx *axchg)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_nvmet_tgtport *tgtp = phba->targetport->private;\n\tuint32_t *payload = axchg->payload;\n\tint rc;\n\n\tatomic_inc(&tgtp->rcv_ls_req_in);\n\n\t \n\trc = nvmet_fc_rcv_ls_req(phba->targetport, axchg->ndlp, &axchg->ls_rsp,\n\t\t\t\t axchg->payload, axchg->size);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\"6037 NVMET Unsol rcv: sz %d rc %d: %08x %08x %08x \"\n\t\t\t\"%08x %08x %08x\\n\", axchg->size, rc,\n\t\t\t*payload, *(payload+1), *(payload+2),\n\t\t\t*(payload+3), *(payload+4), *(payload+5));\n\n\tif (!rc) {\n\t\tatomic_inc(&tgtp->rcv_ls_req_out);\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&tgtp->rcv_ls_req_drop);\n#endif\n\treturn 1;\n}\n\nstatic void\nlpfc_nvmet_process_rcv_fcp_req(struct lpfc_nvmet_ctxbuf *ctx_buf)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_async_xchg_ctx *ctxp = ctx_buf->context;\n\tstruct lpfc_hba *phba = ctxp->phba;\n\tstruct rqb_dmabuf *nvmebuf = ctxp->rqb_buffer;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tuint32_t *payload, qno;\n\tuint32_t rc;\n\tunsigned long iflags;\n\n\tif (!nvmebuf) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6159 process_rcv_fcp_req, nvmebuf is NULL, \"\n\t\t\t\"oxid: x%x flg: x%x state: x%x\\n\",\n\t\t\tctxp->oxid, ctxp->flag, ctxp->state);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflags);\n\t\tlpfc_nvmet_defer_release(phba, ctxp);\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\t\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, ctxp->sid,\n\t\t\t\t\t\t ctxp->oxid);\n\t\treturn;\n\t}\n\n\tif (ctxp->flag & LPFC_NVME_ABTS_RCV) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6324 IO oxid x%x aborted\\n\",\n\t\t\t\tctxp->oxid);\n\t\treturn;\n\t}\n\n\tpayload = (uint32_t *)(nvmebuf->dbuf.virt);\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tctxp->flag |= LPFC_NVME_TNOTIFY;\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tif (ctxp->ts_isr_cmd)\n\t\tctxp->ts_cmd_nvme = ktime_get_ns();\n#endif\n\t \n\trc = nvmet_fc_rcv_fcp_req(phba->targetport, &ctxp->hdlrctx.fcp_req,\n\t\t\t\t  payload, ctxp->size);\n\t \n\tif (rc == 0) {\n\t\tatomic_inc(&tgtp->rcv_fcp_cmd_out);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflags);\n\t\tif ((ctxp->flag & LPFC_NVME_CTX_REUSE_WQ) ||\n\t\t    (nvmebuf != ctxp->rqb_buffer)) {\n\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\t\t\treturn;\n\t\t}\n\t\tctxp->rqb_buffer = NULL;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\t\tlpfc_rq_buf_free(phba, &nvmebuf->hbuf);  \n\t\treturn;\n\t}\n\n\t \n\tif (rc == -EOVERFLOW) {\n\t\tlpfc_nvmeio_data(phba, \"NVMET RCV BUSY: xri x%x sz %d \"\n\t\t\t\t \"from %06x\\n\",\n\t\t\t\t ctxp->oxid, ctxp->size, ctxp->sid);\n\t\tatomic_inc(&tgtp->rcv_fcp_cmd_out);\n\t\tatomic_inc(&tgtp->defer_fod);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflags);\n\t\tif (ctxp->flag & LPFC_NVME_CTX_REUSE_WQ) {\n\t\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\t\t \n\t\tqno = nvmebuf->idx;\n\t\tlpfc_post_rq_buffer(\n\t\t\tphba, phba->sli4_hba.nvmet_mrq_hdr[qno],\n\t\t\tphba->sli4_hba.nvmet_mrq_data[qno], 1, qno);\n\t\treturn;\n\t}\n\tctxp->flag &= ~LPFC_NVME_TNOTIFY;\n\tatomic_inc(&tgtp->rcv_fcp_cmd_drop);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2582 FCP Drop IO x%x: err x%x: x%x x%x x%x\\n\",\n\t\t\tctxp->oxid, rc,\n\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_in),\n\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_out),\n\t\t\tatomic_read(&tgtp->xmt_fcp_release));\n\tlpfc_nvmeio_data(phba, \"NVMET FCP DROP: xri x%x sz %d from %06x\\n\",\n\t\t\t ctxp->oxid, ctxp->size, ctxp->sid);\n\tspin_lock_irqsave(&ctxp->ctxlock, iflags);\n\tlpfc_nvmet_defer_release(phba, ctxp);\n\tspin_unlock_irqrestore(&ctxp->ctxlock, iflags);\n\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, ctxp->sid, ctxp->oxid);\n#endif\n}\n\nstatic void\nlpfc_nvmet_fcp_rqst_defer_work(struct work_struct *work)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_nvmet_ctxbuf *ctx_buf =\n\t\tcontainer_of(work, struct lpfc_nvmet_ctxbuf, defer_work);\n\n\tlpfc_nvmet_process_rcv_fcp_req(ctx_buf);\n#endif\n}\n\nstatic struct lpfc_nvmet_ctxbuf *\nlpfc_nvmet_replenish_context(struct lpfc_hba *phba,\n\t\t\t     struct lpfc_nvmet_ctx_info *current_infop)\n{\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\tstruct lpfc_nvmet_ctxbuf *ctx_buf = NULL;\n\tstruct lpfc_nvmet_ctx_info *get_infop;\n\tint i;\n\n\t \n\tif (current_infop->nvmet_ctx_start_cpu)\n\t\tget_infop = current_infop->nvmet_ctx_start_cpu;\n\telse\n\t\tget_infop = current_infop->nvmet_ctx_next_cpu;\n\n\tfor (i = 0; i < phba->sli4_hba.num_possible_cpu; i++) {\n\t\tif (get_infop == current_infop) {\n\t\t\tget_infop = get_infop->nvmet_ctx_next_cpu;\n\t\t\tcontinue;\n\t\t}\n\t\tspin_lock(&get_infop->nvmet_ctx_list_lock);\n\n\t\t \n\t\tif (get_infop->nvmet_ctx_list_cnt) {\n\t\t\tlist_splice_init(&get_infop->nvmet_ctx_list,\n\t\t\t\t    &current_infop->nvmet_ctx_list);\n\t\t\tcurrent_infop->nvmet_ctx_list_cnt =\n\t\t\t\tget_infop->nvmet_ctx_list_cnt - 1;\n\t\t\tget_infop->nvmet_ctx_list_cnt = 0;\n\t\t\tspin_unlock(&get_infop->nvmet_ctx_list_lock);\n\n\t\t\tcurrent_infop->nvmet_ctx_start_cpu = get_infop;\n\t\t\tlist_remove_head(&current_infop->nvmet_ctx_list,\n\t\t\t\t\t ctx_buf, struct lpfc_nvmet_ctxbuf,\n\t\t\t\t\t list);\n\t\t\treturn ctx_buf;\n\t\t}\n\n\t\t \n\t\tspin_unlock(&get_infop->nvmet_ctx_list_lock);\n\t\tget_infop = get_infop->nvmet_ctx_next_cpu;\n\t}\n\n#endif\n\t \n\treturn NULL;\n}\n\n \nstatic void\nlpfc_nvmet_unsol_fcp_buffer(struct lpfc_hba *phba,\n\t\t\t    uint32_t idx,\n\t\t\t    struct rqb_dmabuf *nvmebuf,\n\t\t\t    uint64_t isr_timestamp,\n\t\t\t    uint8_t cqflag)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_nvmet_ctxbuf *ctx_buf;\n\tstruct lpfc_nvmet_ctx_info *current_infop;\n\tuint32_t size, oxid, sid, qno;\n\tunsigned long iflag;\n\tint current_cpu;\n\n\tif (!IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\t\treturn;\n\n\tctx_buf = NULL;\n\tif (!nvmebuf || !phba->targetport) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6157 NVMET FCP Drop IO\\n\");\n\t\tif (nvmebuf)\n\t\t\tlpfc_rq_buf_free(phba, &nvmebuf->hbuf);\n\t\treturn;\n\t}\n\n\t \n\tcurrent_cpu = raw_smp_processor_id();\n\tcurrent_infop = lpfc_get_ctx_list(phba, current_cpu, idx);\n\tspin_lock_irqsave(&current_infop->nvmet_ctx_list_lock, iflag);\n\tif (current_infop->nvmet_ctx_list_cnt) {\n\t\tlist_remove_head(&current_infop->nvmet_ctx_list,\n\t\t\t\t ctx_buf, struct lpfc_nvmet_ctxbuf, list);\n\t\tcurrent_infop->nvmet_ctx_list_cnt--;\n\t} else {\n\t\tctx_buf = lpfc_nvmet_replenish_context(phba, current_infop);\n\t}\n\tspin_unlock_irqrestore(&current_infop->nvmet_ctx_list_lock, iflag);\n\n\tfc_hdr = (struct fc_frame_header *)(nvmebuf->hbuf.virt);\n\toxid = be16_to_cpu(fc_hdr->fh_ox_id);\n\tsize = nvmebuf->bytes_recv;\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tif (phba->hdwqstat_on & LPFC_CHECK_NVMET_IO) {\n\t\tthis_cpu_inc(phba->sli4_hba.c_stat->rcv_io);\n\t\tif (idx != current_cpu)\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\t\"6703 CPU Check rcv: \"\n\t\t\t\t\t\"cpu %d expect %d\\n\",\n\t\t\t\t\tcurrent_cpu, idx);\n\t}\n#endif\n\n\tlpfc_nvmeio_data(phba, \"NVMET FCP  RCV: xri x%x sz %d CPU %02x\\n\",\n\t\t\t oxid, size, raw_smp_processor_id());\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\n\tif (!ctx_buf) {\n\t\t \n\t\tspin_lock_irqsave(&phba->sli4_hba.nvmet_io_wait_lock, iflag);\n\t\tlist_add_tail(&nvmebuf->hbuf.list,\n\t\t\t      &phba->sli4_hba.lpfc_nvmet_io_wait_list);\n\t\tphba->sli4_hba.nvmet_io_wait_cnt++;\n\t\tphba->sli4_hba.nvmet_io_wait_total++;\n\t\tspin_unlock_irqrestore(&phba->sli4_hba.nvmet_io_wait_lock,\n\t\t\t\t       iflag);\n\n\t\t \n\t\tqno = nvmebuf->idx;\n\t\tlpfc_post_rq_buffer(\n\t\t\tphba, phba->sli4_hba.nvmet_mrq_hdr[qno],\n\t\t\tphba->sli4_hba.nvmet_mrq_data[qno], 1, qno);\n\n\t\tatomic_inc(&tgtp->defer_ctx);\n\t\treturn;\n\t}\n\n\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\n\tctxp = (struct lpfc_async_xchg_ctx *)ctx_buf->context;\n\tspin_lock_irqsave(&phba->sli4_hba.t_active_list_lock, iflag);\n\tlist_add_tail(&ctxp->list, &phba->sli4_hba.t_active_ctx_list);\n\tspin_unlock_irqrestore(&phba->sli4_hba.t_active_list_lock, iflag);\n\tif (ctxp->state != LPFC_NVME_STE_FREE) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6414 NVMET Context corrupt %d %d oxid x%x\\n\",\n\t\t\t\tctxp->state, ctxp->entry_cnt, ctxp->oxid);\n\t}\n\tctxp->wqeq = NULL;\n\tctxp->offset = 0;\n\tctxp->phba = phba;\n\tctxp->size = size;\n\tctxp->oxid = oxid;\n\tctxp->sid = sid;\n\tctxp->idx = idx;\n\tctxp->state = LPFC_NVME_STE_RCV;\n\tctxp->entry_cnt = 1;\n\tctxp->flag = 0;\n\tctxp->ctxbuf = ctx_buf;\n\tctxp->rqb_buffer = (void *)nvmebuf;\n\tctxp->hdwq = NULL;\n\tspin_lock_init(&ctxp->ctxlock);\n\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tif (isr_timestamp)\n\t\tctxp->ts_isr_cmd = isr_timestamp;\n\tctxp->ts_cmd_nvme = 0;\n\tctxp->ts_nvme_data = 0;\n\tctxp->ts_data_wqput = 0;\n\tctxp->ts_isr_data = 0;\n\tctxp->ts_data_nvme = 0;\n\tctxp->ts_nvme_status = 0;\n\tctxp->ts_status_wqput = 0;\n\tctxp->ts_isr_status = 0;\n\tctxp->ts_status_nvme = 0;\n#endif\n\n\tatomic_inc(&tgtp->rcv_fcp_cmd_in);\n\t \n\tif (!cqflag) {\n\t\tlpfc_nvmet_process_rcv_fcp_req(ctx_buf);\n\t\treturn;\n\t}\n\n\tif (!queue_work(phba->wq, &ctx_buf->defer_work)) {\n\t\tatomic_inc(&tgtp->rcv_fcp_cmd_drop);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6325 Unable to queue work for oxid x%x. \"\n\t\t\t\t\"FCP Drop IO [x%x x%x x%x]\\n\",\n\t\t\t\tctxp->oxid,\n\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_in),\n\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_out),\n\t\t\t\tatomic_read(&tgtp->xmt_fcp_release));\n\n\t\tspin_lock_irqsave(&ctxp->ctxlock, iflag);\n\t\tlpfc_nvmet_defer_release(phba, ctxp);\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, iflag);\n\t\tlpfc_nvmet_unsol_fcp_issue_abort(phba, ctxp, sid, oxid);\n\t}\n}\n\n \nvoid\nlpfc_nvmet_unsol_fcp_event(struct lpfc_hba *phba,\n\t\t\t   uint32_t idx,\n\t\t\t   struct rqb_dmabuf *nvmebuf,\n\t\t\t   uint64_t isr_timestamp,\n\t\t\t   uint8_t cqflag)\n{\n\tif (!nvmebuf) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3167 NVMET FCP Drop IO\\n\");\n\t\treturn;\n\t}\n\tif (phba->nvmet_support == 0) {\n\t\tlpfc_rq_buf_free(phba, &nvmebuf->hbuf);\n\t\treturn;\n\t}\n\tlpfc_nvmet_unsol_fcp_buffer(phba, idx, nvmebuf, isr_timestamp, cqflag);\n}\n\n \nstatic struct lpfc_iocbq *\nlpfc_nvmet_prep_ls_wqe(struct lpfc_hba *phba,\n\t\t       struct lpfc_async_xchg_ctx *ctxp,\n\t\t       dma_addr_t rspbuf, uint16_t rspsize)\n{\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_iocbq *nvmewqe;\n\tunion lpfc_wqe128 *wqe;\n\n\tif (!lpfc_is_link_up(phba)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6104 NVMET prep LS wqe: link err: \"\n\t\t\t\t\"NPORT x%x oxid:x%x ste %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\treturn NULL;\n\t}\n\n\t \n\tnvmewqe = lpfc_sli_get_iocbq(phba);\n\tif (nvmewqe == NULL) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6105 NVMET prep LS wqe: No WQE: \"\n\t\t\t\t\"NPORT x%x oxid x%x ste %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\treturn NULL;\n\t}\n\n\tndlp = lpfc_findnode_did(phba->pport, ctxp->sid);\n\tif (!ndlp ||\n\t    ((ndlp->nlp_state != NLP_STE_UNMAPPED_NODE) &&\n\t    (ndlp->nlp_state != NLP_STE_MAPPED_NODE))) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6106 NVMET prep LS wqe: No ndlp: \"\n\t\t\t\t\"NPORT x%x oxid x%x ste %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\tgoto nvme_wqe_free_wqeq_exit;\n\t}\n\tctxp->wqeq = nvmewqe;\n\n\t \n\tnvmewqe->ndlp = lpfc_nlp_get(ndlp);\n\tif (!nvmewqe->ndlp)\n\t\tgoto nvme_wqe_free_wqeq_exit;\n\tnvmewqe->context_un.axchg = ctxp;\n\n\twqe = &nvmewqe->wqe;\n\tmemset(wqe, 0, sizeof(union lpfc_wqe));\n\n\t \n\twqe->xmit_sequence.bde.tus.f.bdeFlags = BUFF_TYPE_BDE_64;\n\twqe->xmit_sequence.bde.tus.f.bdeSize = rspsize;\n\twqe->xmit_sequence.bde.addrLow = le32_to_cpu(putPaddrLow(rspbuf));\n\twqe->xmit_sequence.bde.addrHigh = le32_to_cpu(putPaddrHigh(rspbuf));\n\n\t \n\n\t \n\n\t \n\tbf_set(wqe_dfctl, &wqe->xmit_sequence.wge_ctl, 0);\n\tbf_set(wqe_ls, &wqe->xmit_sequence.wge_ctl, 1);\n\tbf_set(wqe_la, &wqe->xmit_sequence.wge_ctl, 0);\n\tbf_set(wqe_rctl, &wqe->xmit_sequence.wge_ctl, FC_RCTL_ELS4_REP);\n\tbf_set(wqe_type, &wqe->xmit_sequence.wge_ctl, FC_TYPE_NVME);\n\n\t \n\tbf_set(wqe_ctxt_tag, &wqe->xmit_sequence.wqe_com,\n\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\tbf_set(wqe_xri_tag, &wqe->xmit_sequence.wqe_com, nvmewqe->sli4_xritag);\n\n\t \n\tbf_set(wqe_cmnd, &wqe->xmit_sequence.wqe_com,\n\t       CMD_XMIT_SEQUENCE64_WQE);\n\tbf_set(wqe_ct, &wqe->xmit_sequence.wqe_com, SLI4_CT_RPI);\n\tbf_set(wqe_class, &wqe->xmit_sequence.wqe_com, CLASS3);\n\tbf_set(wqe_pu, &wqe->xmit_sequence.wqe_com, 0);\n\n\t \n\twqe->xmit_sequence.wqe_com.abort_tag = nvmewqe->iotag;\n\n\t \n\tbf_set(wqe_reqtag, &wqe->xmit_sequence.wqe_com, nvmewqe->iotag);\n\t \n\tbf_set(wqe_rcvoxid, &wqe->xmit_sequence.wqe_com, ctxp->oxid);\n\n\t \n\tbf_set(wqe_dbde, &wqe->xmit_sequence.wqe_com, 1);\n\tbf_set(wqe_iod, &wqe->xmit_sequence.wqe_com, LPFC_WQE_IOD_WRITE);\n\tbf_set(wqe_lenloc, &wqe->xmit_sequence.wqe_com,\n\t       LPFC_WQE_LENLOC_WORD12);\n\tbf_set(wqe_ebde_cnt, &wqe->xmit_sequence.wqe_com, 0);\n\n\t \n\tbf_set(wqe_cqid, &wqe->xmit_sequence.wqe_com,\n\t       LPFC_WQE_CQ_ID_DEFAULT);\n\tbf_set(wqe_cmd_type, &wqe->xmit_sequence.wqe_com,\n\t       OTHER_COMMAND);\n\n\t \n\twqe->xmit_sequence.xmit_len = rspsize;\n\n\tnvmewqe->retry = 1;\n\tnvmewqe->vport = phba->pport;\n\tnvmewqe->drvrTimeout = (phba->fc_ratov * 3) + LPFC_DRVR_TIMEOUT;\n\tnvmewqe->cmd_flag |= LPFC_IO_NVME_LS;\n\n\t \n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,\n\t\t\t\"6039 Xmit NVMET LS response to remote \"\n\t\t\t\"NPORT x%x iotag:x%x oxid:x%x size:x%x\\n\",\n\t\t\tndlp->nlp_DID, nvmewqe->iotag, ctxp->oxid,\n\t\t\trspsize);\n\treturn nvmewqe;\n\nnvme_wqe_free_wqeq_exit:\n\tnvmewqe->context_un.axchg = NULL;\n\tnvmewqe->ndlp = NULL;\n\tnvmewqe->bpl_dmabuf = NULL;\n\tlpfc_sli_release_iocbq(phba, nvmewqe);\n\treturn NULL;\n}\n\n\nstatic struct lpfc_iocbq *\nlpfc_nvmet_prep_fcp_wqe(struct lpfc_hba *phba,\n\t\t\tstruct lpfc_async_xchg_ctx *ctxp)\n{\n\tstruct nvmefc_tgt_fcp_req *rsp = &ctxp->hdlrctx.fcp_req;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct sli4_sge *sgl;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_iocbq *nvmewqe;\n\tstruct scatterlist *sgel;\n\tunion lpfc_wqe128 *wqe;\n\tstruct ulp_bde64 *bde;\n\tdma_addr_t physaddr;\n\tint i, cnt, nsegs;\n\tbool use_pbde = false;\n\tint xc = 1;\n\n\tif (!lpfc_is_link_up(phba)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6107 NVMET prep FCP wqe: link err:\"\n\t\t\t\t\"NPORT x%x oxid x%x ste %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\treturn NULL;\n\t}\n\n\tndlp = lpfc_findnode_did(phba->pport, ctxp->sid);\n\tif (!ndlp ||\n\t    ((ndlp->nlp_state != NLP_STE_UNMAPPED_NODE) &&\n\t     (ndlp->nlp_state != NLP_STE_MAPPED_NODE))) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6108 NVMET prep FCP wqe: no ndlp: \"\n\t\t\t\t\"NPORT x%x oxid x%x ste %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\treturn NULL;\n\t}\n\n\tif (rsp->sg_cnt > lpfc_tgttemplate.max_sgl_segments) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6109 NVMET prep FCP wqe: seg cnt err: \"\n\t\t\t\t\"NPORT x%x oxid x%x ste %d cnt %d\\n\",\n\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state,\n\t\t\t\tphba->cfg_nvme_seg_cnt);\n\t\treturn NULL;\n\t}\n\tnsegs = rsp->sg_cnt;\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tnvmewqe = ctxp->wqeq;\n\tif (nvmewqe == NULL) {\n\t\t \n\t\tnvmewqe = ctxp->ctxbuf->iocbq;\n\t\tif (nvmewqe == NULL) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6110 NVMET prep FCP wqe: No \"\n\t\t\t\t\t\"WQE: NPORT x%x oxid x%x ste %d\\n\",\n\t\t\t\t\tctxp->sid, ctxp->oxid, ctxp->state);\n\t\t\treturn NULL;\n\t\t}\n\t\tctxp->wqeq = nvmewqe;\n\t\txc = 0;  \n\t\tnvmewqe->sli4_lxritag = NO_XRI;\n\t\tnvmewqe->sli4_xritag = NO_XRI;\n\t}\n\n\t \n\tif (((ctxp->state == LPFC_NVME_STE_RCV) &&\n\t    (ctxp->entry_cnt == 1)) ||\n\t    (ctxp->state == LPFC_NVME_STE_DATA)) {\n\t\twqe = &nvmewqe->wqe;\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6111 Wrong state NVMET FCP: %d  cnt %d\\n\",\n\t\t\t\tctxp->state, ctxp->entry_cnt);\n\t\treturn NULL;\n\t}\n\n\tsgl  = (struct sli4_sge *)ctxp->ctxbuf->sglq->sgl;\n\tswitch (rsp->op) {\n\tcase NVMET_FCOP_READDATA:\n\tcase NVMET_FCOP_READDATA_RSP:\n\t\t \n\t\tmemcpy(&wqe->words[7],\n\t\t       &lpfc_tsend_cmd_template.words[7],\n\t\t       sizeof(uint32_t) * 5);\n\n\t\t \n\t\tsgel = &rsp->sg[0];\n\t\tphysaddr = sg_dma_address(sgel);\n\t\twqe->fcp_tsend.bde.tus.f.bdeFlags = BUFF_TYPE_BDE_64;\n\t\twqe->fcp_tsend.bde.tus.f.bdeSize = sg_dma_len(sgel);\n\t\twqe->fcp_tsend.bde.addrLow = cpu_to_le32(putPaddrLow(physaddr));\n\t\twqe->fcp_tsend.bde.addrHigh =\n\t\t\tcpu_to_le32(putPaddrHigh(physaddr));\n\n\t\t \n\t\twqe->fcp_tsend.payload_offset_len = 0;\n\n\t\t \n\t\twqe->fcp_tsend.relative_offset = ctxp->offset;\n\n\t\t \n\t\twqe->fcp_tsend.reserved = 0;\n\n\t\t \n\t\tbf_set(wqe_ctxt_tag, &wqe->fcp_tsend.wqe_com,\n\t\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\tbf_set(wqe_xri_tag, &wqe->fcp_tsend.wqe_com,\n\t\t       nvmewqe->sli4_xritag);\n\n\t\t \n\n\t\t \n\t\twqe->fcp_tsend.wqe_com.abort_tag = nvmewqe->iotag;\n\n\t\t \n\t\tbf_set(wqe_reqtag, &wqe->fcp_tsend.wqe_com, nvmewqe->iotag);\n\t\tbf_set(wqe_rcvoxid, &wqe->fcp_tsend.wqe_com, ctxp->oxid);\n\n\t\t \n\t\tif (!xc)\n\t\t\tbf_set(wqe_xc, &wqe->fcp_tsend.wqe_com, 0);\n\n\t\t \n\t\twqe->fcp_tsend.fcp_data_len = rsp->transfer_length;\n\n\t\t \n\t\tsgl->addr_hi = 0;\n\t\tsgl->addr_lo = 0;\n\t\tsgl->word2 = 0;\n\t\tbf_set(lpfc_sli4_sge_type, sgl, LPFC_SGE_TYPE_SKIP);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = 0;\n\t\tsgl++;\n\t\tsgl->addr_hi = 0;\n\t\tsgl->addr_lo = 0;\n\t\tsgl->word2 = 0;\n\t\tbf_set(lpfc_sli4_sge_type, sgl, LPFC_SGE_TYPE_SKIP);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = 0;\n\t\tsgl++;\n\t\tif (rsp->op == NVMET_FCOP_READDATA_RSP) {\n\t\t\tatomic_inc(&tgtp->xmt_fcp_read_rsp);\n\n\t\t\t \n\n\t\t\tif (rsp->rsplen == LPFC_NVMET_SUCCESS_LEN) {\n\t\t\t\tif (ndlp->nlp_flag & NLP_SUPPRESS_RSP)\n\t\t\t\t\tbf_set(wqe_sup,\n\t\t\t\t\t       &wqe->fcp_tsend.wqe_com, 1);\n\t\t\t} else {\n\t\t\t\tbf_set(wqe_wqes, &wqe->fcp_tsend.wqe_com, 1);\n\t\t\t\tbf_set(wqe_irsp, &wqe->fcp_tsend.wqe_com, 1);\n\t\t\t\tbf_set(wqe_irsplen, &wqe->fcp_tsend.wqe_com,\n\t\t\t\t       ((rsp->rsplen >> 2) - 1));\n\t\t\t\tmemcpy(&wqe->words[16], rsp->rspaddr,\n\t\t\t\t       rsp->rsplen);\n\t\t\t}\n\t\t} else {\n\t\t\tatomic_inc(&tgtp->xmt_fcp_read);\n\n\t\t\t \n\t\t\tbf_set(wqe_ar, &wqe->fcp_tsend.wqe_com, 0);\n\t\t}\n\t\tbreak;\n\n\tcase NVMET_FCOP_WRITEDATA:\n\t\t \n\t\tmemcpy(&wqe->words[3],\n\t\t       &lpfc_treceive_cmd_template.words[3],\n\t\t       sizeof(uint32_t) * 9);\n\n\t\t \n\t\twqe->fcp_treceive.bde.tus.f.bdeFlags = LPFC_SGE_TYPE_SKIP;\n\t\twqe->fcp_treceive.bde.tus.f.bdeSize = 0;\n\t\twqe->fcp_treceive.bde.addrLow = 0;\n\t\twqe->fcp_treceive.bde.addrHigh = 0;\n\n\t\t \n\t\twqe->fcp_treceive.relative_offset = ctxp->offset;\n\n\t\t \n\t\tbf_set(wqe_ctxt_tag, &wqe->fcp_treceive.wqe_com,\n\t\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\tbf_set(wqe_xri_tag, &wqe->fcp_treceive.wqe_com,\n\t\t       nvmewqe->sli4_xritag);\n\n\t\t \n\n\t\t \n\t\twqe->fcp_treceive.wqe_com.abort_tag = nvmewqe->iotag;\n\n\t\t \n\t\tbf_set(wqe_reqtag, &wqe->fcp_treceive.wqe_com, nvmewqe->iotag);\n\t\tbf_set(wqe_rcvoxid, &wqe->fcp_treceive.wqe_com, ctxp->oxid);\n\n\t\t \n\t\tif (!xc)\n\t\t\tbf_set(wqe_xc, &wqe->fcp_treceive.wqe_com, 0);\n\n\t\t \n\t\tif (nsegs == 1 && phba->cfg_enable_pbde) {\n\t\t\tuse_pbde = true;\n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tbf_set(wqe_pbde, &wqe->fcp_treceive.wqe_com, 0);\n\t\t}\n\n\t\t \n\t\twqe->fcp_tsend.fcp_data_len = rsp->transfer_length;\n\n\t\t \n\t\tsgl->addr_hi = 0;\n\t\tsgl->addr_lo = 0;\n\t\tsgl->word2 = 0;\n\t\tbf_set(lpfc_sli4_sge_type, sgl, LPFC_SGE_TYPE_SKIP);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = 0;\n\t\tsgl++;\n\t\tsgl->addr_hi = 0;\n\t\tsgl->addr_lo = 0;\n\t\tsgl->word2 = 0;\n\t\tbf_set(lpfc_sli4_sge_type, sgl, LPFC_SGE_TYPE_SKIP);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = 0;\n\t\tsgl++;\n\t\tatomic_inc(&tgtp->xmt_fcp_write);\n\t\tbreak;\n\n\tcase NVMET_FCOP_RSP:\n\t\t \n\t\tmemcpy(&wqe->words[4],\n\t\t       &lpfc_trsp_cmd_template.words[4],\n\t\t       sizeof(uint32_t) * 8);\n\n\t\t \n\t\tphysaddr = rsp->rspdma;\n\t\twqe->fcp_trsp.bde.tus.f.bdeFlags = BUFF_TYPE_BDE_64;\n\t\twqe->fcp_trsp.bde.tus.f.bdeSize = rsp->rsplen;\n\t\twqe->fcp_trsp.bde.addrLow =\n\t\t\tcpu_to_le32(putPaddrLow(physaddr));\n\t\twqe->fcp_trsp.bde.addrHigh =\n\t\t\tcpu_to_le32(putPaddrHigh(physaddr));\n\n\t\t \n\t\twqe->fcp_trsp.response_len = rsp->rsplen;\n\n\t\t \n\t\tbf_set(wqe_ctxt_tag, &wqe->fcp_trsp.wqe_com,\n\t\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\tbf_set(wqe_xri_tag, &wqe->fcp_trsp.wqe_com,\n\t\t       nvmewqe->sli4_xritag);\n\n\t\t \n\n\t\t \n\t\twqe->fcp_trsp.wqe_com.abort_tag = nvmewqe->iotag;\n\n\t\t \n\t\tbf_set(wqe_reqtag, &wqe->fcp_trsp.wqe_com, nvmewqe->iotag);\n\t\tbf_set(wqe_rcvoxid, &wqe->fcp_trsp.wqe_com, ctxp->oxid);\n\n\t\t \n\t\tif (xc)\n\t\t\tbf_set(wqe_xc, &wqe->fcp_trsp.wqe_com, 1);\n\n\t\t \n\t\t \n\t\tif (rsp->rsplen != LPFC_NVMET_SUCCESS_LEN) {\n\t\t\t \n\t\t\tbf_set(wqe_wqes, &wqe->fcp_trsp.wqe_com, 1);\n\t\t\tbf_set(wqe_irsp, &wqe->fcp_trsp.wqe_com, 1);\n\t\t\tbf_set(wqe_irsplen, &wqe->fcp_trsp.wqe_com,\n\t\t\t       ((rsp->rsplen >> 2) - 1));\n\t\t\tmemcpy(&wqe->words[16], rsp->rspaddr, rsp->rsplen);\n\t\t}\n\n\t\t \n\t\twqe->fcp_trsp.rsvd_12_15[0] = 0;\n\n\t\t \n\t\tnsegs = 0;\n\t\tsgl->word2 = 0;\n\t\tatomic_inc(&tgtp->xmt_fcp_rsp);\n\t\tbreak;\n\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_IOERR,\n\t\t\t\t\"6064 Unknown Rsp Op %d\\n\",\n\t\t\t\trsp->op);\n\t\treturn NULL;\n\t}\n\n\tnvmewqe->retry = 1;\n\tnvmewqe->vport = phba->pport;\n\tnvmewqe->drvrTimeout = (phba->fc_ratov * 3) + LPFC_DRVR_TIMEOUT;\n\tnvmewqe->ndlp = ndlp;\n\n\tfor_each_sg(rsp->sg, sgel, nsegs, i) {\n\t\tphysaddr = sg_dma_address(sgel);\n\t\tcnt = sg_dma_len(sgel);\n\t\tsgl->addr_hi = putPaddrHigh(physaddr);\n\t\tsgl->addr_lo = putPaddrLow(physaddr);\n\t\tsgl->word2 = 0;\n\t\tbf_set(lpfc_sli4_sge_type, sgl, LPFC_SGE_TYPE_DATA);\n\t\tbf_set(lpfc_sli4_sge_offset, sgl, ctxp->offset);\n\t\tif ((i+1) == rsp->sg_cnt)\n\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 1);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = cpu_to_le32(cnt);\n\t\tsgl++;\n\t\tctxp->offset += cnt;\n\t}\n\n\tbde = (struct ulp_bde64 *)&wqe->words[13];\n\tif (use_pbde) {\n\t\t \n\t\tsgl--;\n\n\t\t \n\t\tbde->addrLow = sgl->addr_lo;\n\t\tbde->addrHigh = sgl->addr_hi;\n\t\tbde->tus.f.bdeSize = le32_to_cpu(sgl->sge_len);\n\t\tbde->tus.f.bdeFlags = BUFF_TYPE_BDE_64;\n\t\tbde->tus.w = cpu_to_le32(bde->tus.w);\n\t} else {\n\t\tmemset(bde, 0, sizeof(struct ulp_bde64));\n\t}\n\tctxp->state = LPFC_NVME_STE_DATA;\n\tctxp->entry_cnt++;\n\treturn nvmewqe;\n}\n\n \nstatic void\nlpfc_nvmet_sol_fcp_abort_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t     struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tuint32_t result;\n\tunsigned long flags;\n\tbool released = false;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\n\tctxp = cmdwqe->context_un.axchg;\n\tresult = wcqe->parameter;\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tif (ctxp->flag & LPFC_NVME_ABORT_OP)\n\t\tatomic_inc(&tgtp->xmt_fcp_abort_cmpl);\n\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tctxp->state = LPFC_NVME_STE_DONE;\n\n\t \n\tif ((ctxp->flag & LPFC_NVME_CTX_RLS) &&\n\t    !(ctxp->flag & LPFC_NVME_XBUSY)) {\n\t\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\tlist_del_init(&ctxp->list);\n\t\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\treleased = true;\n\t}\n\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\tatomic_inc(&tgtp->xmt_abort_rsp);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6165 ABORT cmpl: oxid x%x flg x%x (%d) \"\n\t\t\t\"WCQE: %08x %08x %08x %08x\\n\",\n\t\t\tctxp->oxid, ctxp->flag, released,\n\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\tresult, wcqe->word3);\n\n\tcmdwqe->rsp_dmabuf = NULL;\n\tcmdwqe->bpl_dmabuf = NULL;\n\t \n\tif (released)\n\t\tlpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);\n\n\t \n\tlpfc_sli_release_iocbq(phba, cmdwqe);\n\n\t \n}\n\n \nstatic void\nlpfc_nvmet_unsol_fcp_abort_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t       struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tunsigned long flags;\n\tuint32_t result;\n\tbool released = false;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\n\tctxp = cmdwqe->context_un.axchg;\n\tresult = wcqe->parameter;\n\n\tif (!ctxp) {\n\t\t \n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\t\"6070 ABTS cmpl: WCQE: %08x %08x %08x %08x\\n\",\n\t\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\t\tresult, wcqe->word3);\n\t\treturn;\n\t}\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tif (ctxp->flag & LPFC_NVME_ABORT_OP)\n\t\tatomic_inc(&tgtp->xmt_fcp_abort_cmpl);\n\n\t \n\tif (ctxp->state != LPFC_NVME_STE_ABORT) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6112 ABTS Wrong state:%d oxid x%x\\n\",\n\t\t\t\tctxp->state, ctxp->oxid);\n\t}\n\n\t \n\tctxp->state = LPFC_NVME_STE_DONE;\n\tif ((ctxp->flag & LPFC_NVME_CTX_RLS) &&\n\t    !(ctxp->flag & LPFC_NVME_XBUSY)) {\n\t\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\tlist_del_init(&ctxp->list);\n\t\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\treleased = true;\n\t}\n\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\tatomic_inc(&tgtp->xmt_abort_rsp);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6316 ABTS cmpl oxid x%x flg x%x (%x) \"\n\t\t\t\"WCQE: %08x %08x %08x %08x\\n\",\n\t\t\tctxp->oxid, ctxp->flag, released,\n\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\tresult, wcqe->word3);\n\n\tcmdwqe->rsp_dmabuf = NULL;\n\tcmdwqe->bpl_dmabuf = NULL;\n\t \n\tif (released)\n\t\tlpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);\n\n\t \n}\n\n \nstatic void\nlpfc_nvmet_xmt_ls_abort_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,\n\t\t\t    struct lpfc_iocbq *rspwqe)\n{\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tuint32_t result;\n\tstruct lpfc_wcqe_complete *wcqe = &rspwqe->wcqe_cmpl;\n\n\tctxp = cmdwqe->context_un.axchg;\n\tresult = wcqe->parameter;\n\n\tif (phba->nvmet_support) {\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\t\tatomic_inc(&tgtp->xmt_ls_abort_cmpl);\n\t}\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6083 Abort cmpl: ctx x%px WCQE:%08x %08x %08x %08x\\n\",\n\t\t\tctxp, wcqe->word0, wcqe->total_data_placed,\n\t\t\tresult, wcqe->word3);\n\n\tif (!ctxp) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6415 NVMET LS Abort No ctx: WCQE: \"\n\t\t\t\t \"%08x %08x %08x %08x\\n\",\n\t\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\t\tresult, wcqe->word3);\n\n\t\tlpfc_sli_release_iocbq(phba, cmdwqe);\n\t\treturn;\n\t}\n\n\tif (ctxp->state != LPFC_NVME_STE_LS_ABORT) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6416 NVMET LS abort cmpl state mismatch: \"\n\t\t\t\t\"oxid x%x: %d %d\\n\",\n\t\t\t\tctxp->oxid, ctxp->state, ctxp->entry_cnt);\n\t}\n\n\tcmdwqe->rsp_dmabuf = NULL;\n\tcmdwqe->bpl_dmabuf = NULL;\n\tlpfc_sli_release_iocbq(phba, cmdwqe);\n\tkfree(ctxp);\n}\n\nstatic int\nlpfc_nvmet_unsol_issue_abort(struct lpfc_hba *phba,\n\t\t\t     struct lpfc_async_xchg_ctx *ctxp,\n\t\t\t     uint32_t sid, uint16_t xri)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp = NULL;\n\tstruct lpfc_iocbq *abts_wqeq;\n\tunion lpfc_wqe128 *wqe_abts;\n\tstruct lpfc_nodelist *ndlp;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6067 ABTS: sid %x xri x%x/x%x\\n\",\n\t\t\tsid, xri, ctxp->wqeq->sli4_xritag);\n\n\tif (phba->nvmet_support && phba->targetport)\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\n\tndlp = lpfc_findnode_did(phba->pport, sid);\n\tif (!ndlp ||\n\t    ((ndlp->nlp_state != NLP_STE_UNMAPPED_NODE) &&\n\t    (ndlp->nlp_state != NLP_STE_MAPPED_NODE))) {\n\t\tif (tgtp)\n\t\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6134 Drop ABTS - wrong NDLP state x%x.\\n\",\n\t\t\t\t(ndlp) ? ndlp->nlp_state : NLP_STE_MAX_STATE);\n\n\t\t \n\t\treturn 0;\n\t}\n\n\tabts_wqeq = ctxp->wqeq;\n\twqe_abts = &abts_wqeq->wqe;\n\n\t \n\tmemset(wqe_abts, 0, sizeof(union lpfc_wqe));\n\n\t \n\tbf_set(wqe_dfctl, &wqe_abts->xmit_sequence.wge_ctl, 0);\n\tbf_set(wqe_ls, &wqe_abts->xmit_sequence.wge_ctl, 1);\n\tbf_set(wqe_la, &wqe_abts->xmit_sequence.wge_ctl, 0);\n\tbf_set(wqe_rctl, &wqe_abts->xmit_sequence.wge_ctl, FC_RCTL_BA_ABTS);\n\tbf_set(wqe_type, &wqe_abts->xmit_sequence.wge_ctl, FC_TYPE_BLS);\n\n\t \n\tbf_set(wqe_ctxt_tag, &wqe_abts->xmit_sequence.wqe_com,\n\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\tbf_set(wqe_xri_tag, &wqe_abts->xmit_sequence.wqe_com,\n\t       abts_wqeq->sli4_xritag);\n\n\t \n\tbf_set(wqe_cmnd, &wqe_abts->xmit_sequence.wqe_com,\n\t       CMD_XMIT_SEQUENCE64_WQE);\n\tbf_set(wqe_ct, &wqe_abts->xmit_sequence.wqe_com, SLI4_CT_RPI);\n\tbf_set(wqe_class, &wqe_abts->xmit_sequence.wqe_com, CLASS3);\n\tbf_set(wqe_pu, &wqe_abts->xmit_sequence.wqe_com, 0);\n\n\t \n\twqe_abts->xmit_sequence.wqe_com.abort_tag = abts_wqeq->iotag;\n\n\t \n\tbf_set(wqe_reqtag, &wqe_abts->xmit_sequence.wqe_com, abts_wqeq->iotag);\n\t \n\tbf_set(wqe_rcvoxid, &wqe_abts->xmit_sequence.wqe_com, xri);\n\n\t \n\tbf_set(wqe_iod, &wqe_abts->xmit_sequence.wqe_com, LPFC_WQE_IOD_WRITE);\n\tbf_set(wqe_lenloc, &wqe_abts->xmit_sequence.wqe_com,\n\t       LPFC_WQE_LENLOC_WORD12);\n\tbf_set(wqe_ebde_cnt, &wqe_abts->xmit_sequence.wqe_com, 0);\n\tbf_set(wqe_qosd, &wqe_abts->xmit_sequence.wqe_com, 0);\n\n\t \n\tbf_set(wqe_cqid, &wqe_abts->xmit_sequence.wqe_com,\n\t       LPFC_WQE_CQ_ID_DEFAULT);\n\tbf_set(wqe_cmd_type, &wqe_abts->xmit_sequence.wqe_com,\n\t       OTHER_COMMAND);\n\n\tabts_wqeq->vport = phba->pport;\n\tabts_wqeq->ndlp = ndlp;\n\tabts_wqeq->context_un.axchg = ctxp;\n\tabts_wqeq->bpl_dmabuf = NULL;\n\tabts_wqeq->num_bdes = 0;\n\t \n\tabts_wqeq->iocb.ulpCommand = CMD_XMIT_SEQUENCE64_CR;\n\tabts_wqeq->iocb.ulpLe = 1;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6069 Issue ABTS to xri x%x reqtag x%x\\n\",\n\t\t\txri, abts_wqeq->iotag);\n\treturn 1;\n}\n\nstatic int\nlpfc_nvmet_sol_fcp_issue_abort(struct lpfc_hba *phba,\n\t\t\t       struct lpfc_async_xchg_ctx *ctxp,\n\t\t\t       uint32_t sid, uint16_t xri)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct lpfc_iocbq *abts_wqeq;\n\tstruct lpfc_nodelist *ndlp;\n\tunsigned long flags;\n\tbool ia;\n\tint rc;\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tif (!ctxp->wqeq) {\n\t\tctxp->wqeq = ctxp->ctxbuf->iocbq;\n\t\tctxp->wqeq->hba_wqidx = 0;\n\t}\n\n\tndlp = lpfc_findnode_did(phba->pport, sid);\n\tif (!ndlp ||\n\t    ((ndlp->nlp_state != NLP_STE_UNMAPPED_NODE) &&\n\t    (ndlp->nlp_state != NLP_STE_MAPPED_NODE))) {\n\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6160 Drop ABORT - wrong NDLP state x%x.\\n\",\n\t\t\t\t(ndlp) ? ndlp->nlp_state : NLP_STE_MAX_STATE);\n\n\t\t \n\t\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\t\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\treturn 0;\n\t}\n\n\t \n\tctxp->abort_wqeq = lpfc_sli_get_iocbq(phba);\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tif (!ctxp->abort_wqeq) {\n\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6161 ABORT failed: No wqeqs: \"\n\t\t\t\t\"xri: x%x\\n\", ctxp->oxid);\n\t\t \n\t\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\treturn 0;\n\t}\n\tabts_wqeq = ctxp->abort_wqeq;\n\tctxp->state = LPFC_NVME_STE_ABORT;\n\tia = (ctxp->flag & LPFC_NVME_ABTS_RCV) ? true : false;\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\n\t \n\tlpfc_printf_log(phba, KERN_INFO, LOG_NVME_ABTS,\n\t\t\t\"6162 ABORT Request to rport DID x%06x \"\n\t\t\t\"for xri x%x x%x\\n\",\n\t\t\tctxp->sid, ctxp->oxid, ctxp->wqeq->sli4_xritag);\n\n\t \n\tspin_lock_irqsave(&phba->hbalock, flags);\n\t \n\tif (phba->hba_flag & HBA_IOQ_FLUSH) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6163 Driver in reset cleanup - flushing \"\n\t\t\t\t\"NVME Req now. hba_flag x%x oxid x%x\\n\",\n\t\t\t\tphba->hba_flag, ctxp->oxid);\n\t\tlpfc_sli_release_iocbq(phba, abts_wqeq);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\t\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\treturn 0;\n\t}\n\n\t \n\tif (abts_wqeq->cmd_flag & LPFC_DRIVER_ABORTED) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6164 Outstanding NVME I/O Abort Request \"\n\t\t\t\t\"still pending on oxid x%x\\n\",\n\t\t\t\tctxp->oxid);\n\t\tlpfc_sli_release_iocbq(phba, abts_wqeq);\n\t\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\t\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\t\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\t\treturn 0;\n\t}\n\n\t \n\tabts_wqeq->cmd_flag |= LPFC_DRIVER_ABORTED;\n\n\tlpfc_sli_prep_abort_xri(phba, abts_wqeq, ctxp->wqeq->sli4_xritag,\n\t\t\t\tabts_wqeq->iotag, CLASS3,\n\t\t\t\tLPFC_WQE_CQ_ID_DEFAULT, ia, true);\n\n\t \n\tabts_wqeq->hba_wqidx = ctxp->wqeq->hba_wqidx;\n\tabts_wqeq->cmd_cmpl = lpfc_nvmet_sol_fcp_abort_cmp;\n\tabts_wqeq->cmd_flag |= LPFC_IO_NVME;\n\tabts_wqeq->context_un.axchg = ctxp;\n\tabts_wqeq->vport = phba->pport;\n\tif (!ctxp->hdwq)\n\t\tctxp->hdwq = &phba->sli4_hba.hdwq[abts_wqeq->hba_wqidx];\n\n\trc = lpfc_sli4_issue_wqe(phba, ctxp->hdwq, abts_wqeq);\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\tif (rc == WQE_SUCCESS) {\n\t\tatomic_inc(&tgtp->xmt_abort_sol);\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tctxp->flag &= ~LPFC_NVME_ABORT_OP;\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\tlpfc_sli_release_iocbq(phba, abts_wqeq);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6166 Failed ABORT issue_wqe with status x%x \"\n\t\t\t\"for oxid x%x.\\n\",\n\t\t\trc, ctxp->oxid);\n\treturn 1;\n}\n\nstatic int\nlpfc_nvmet_unsol_fcp_issue_abort(struct lpfc_hba *phba,\n\t\t\t\t struct lpfc_async_xchg_ctx *ctxp,\n\t\t\t\t uint32_t sid, uint16_t xri)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct lpfc_iocbq *abts_wqeq;\n\tunsigned long flags;\n\tbool released = false;\n\tint rc;\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tif (!ctxp->wqeq) {\n\t\tctxp->wqeq = ctxp->ctxbuf->iocbq;\n\t\tctxp->wqeq->hba_wqidx = 0;\n\t}\n\n\tif (ctxp->state == LPFC_NVME_STE_FREE) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6417 NVMET ABORT ctx freed %d %d oxid x%x\\n\",\n\t\t\t\tctxp->state, ctxp->entry_cnt, ctxp->oxid);\n\t\trc = WQE_BUSY;\n\t\tgoto aerr;\n\t}\n\tctxp->state = LPFC_NVME_STE_ABORT;\n\tctxp->entry_cnt++;\n\trc = lpfc_nvmet_unsol_issue_abort(phba, ctxp, sid, xri);\n\tif (rc == 0)\n\t\tgoto aerr;\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\tabts_wqeq = ctxp->wqeq;\n\tabts_wqeq->cmd_cmpl = lpfc_nvmet_unsol_fcp_abort_cmp;\n\tabts_wqeq->cmd_flag |= LPFC_IO_NVMET;\n\tif (!ctxp->hdwq)\n\t\tctxp->hdwq = &phba->sli4_hba.hdwq[abts_wqeq->hba_wqidx];\n\n\trc = lpfc_sli4_issue_wqe(phba, ctxp->hdwq, abts_wqeq);\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\tif (rc == WQE_SUCCESS) {\n\t\treturn 0;\n\t}\n\naerr:\n\tspin_lock_irqsave(&ctxp->ctxlock, flags);\n\tif (ctxp->flag & LPFC_NVME_CTX_RLS) {\n\t\tspin_lock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\tlist_del_init(&ctxp->list);\n\t\tspin_unlock(&phba->sli4_hba.abts_nvmet_buf_list_lock);\n\t\treleased = true;\n\t}\n\tctxp->flag &= ~(LPFC_NVME_ABORT_OP | LPFC_NVME_CTX_RLS);\n\tspin_unlock_irqrestore(&ctxp->ctxlock, flags);\n\n\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6135 Failed to Issue ABTS for oxid x%x. Status x%x \"\n\t\t\t\"(%x)\\n\",\n\t\t\tctxp->oxid, rc, released);\n\tif (released)\n\t\tlpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);\n\treturn 1;\n}\n\n \nint\nlpfc_nvme_unsol_ls_issue_abort(struct lpfc_hba *phba,\n\t\t\t\tstruct lpfc_async_xchg_ctx *ctxp,\n\t\t\t\tuint32_t sid, uint16_t xri)\n{\n\tstruct lpfc_nvmet_tgtport *tgtp = NULL;\n\tstruct lpfc_iocbq *abts_wqeq;\n\tunsigned long flags;\n\tint rc;\n\n\tif ((ctxp->state == LPFC_NVME_STE_LS_RCV && ctxp->entry_cnt == 1) ||\n\t    (ctxp->state == LPFC_NVME_STE_LS_RSP && ctxp->entry_cnt == 2)) {\n\t\tctxp->state = LPFC_NVME_STE_LS_ABORT;\n\t\tctxp->entry_cnt++;\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6418 NVMET LS abort state mismatch \"\n\t\t\t\t\"IO x%x: %d %d\\n\",\n\t\t\t\tctxp->oxid, ctxp->state, ctxp->entry_cnt);\n\t\tctxp->state = LPFC_NVME_STE_LS_ABORT;\n\t}\n\n\tif (phba->nvmet_support && phba->targetport)\n\t\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\n\tif (!ctxp->wqeq) {\n\t\t \n\t\tctxp->wqeq = lpfc_sli_get_iocbq(phba);\n\t\tif (!ctxp->wqeq) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6068 Abort failed: No wqeqs: \"\n\t\t\t\t\t\"xri: x%x\\n\", xri);\n\t\t\t \n\t\t\tkfree(ctxp);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tabts_wqeq = ctxp->wqeq;\n\n\tif (lpfc_nvmet_unsol_issue_abort(phba, ctxp, sid, xri) == 0) {\n\t\trc = WQE_BUSY;\n\t\tgoto out;\n\t}\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\tabts_wqeq->cmd_cmpl = lpfc_nvmet_xmt_ls_abort_cmp;\n\tabts_wqeq->cmd_flag |=  LPFC_IO_NVME_LS;\n\trc = lpfc_sli4_issue_wqe(phba, ctxp->hdwq, abts_wqeq);\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\tif (rc == WQE_SUCCESS) {\n\t\tif (tgtp)\n\t\t\tatomic_inc(&tgtp->xmt_abort_unsol);\n\t\treturn 0;\n\t}\nout:\n\tif (tgtp)\n\t\tatomic_inc(&tgtp->xmt_abort_rsp_error);\n\tabts_wqeq->rsp_dmabuf = NULL;\n\tabts_wqeq->bpl_dmabuf = NULL;\n\tlpfc_sli_release_iocbq(phba, abts_wqeq);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6056 Failed to Issue ABTS. Status x%x\\n\", rc);\n\treturn 1;\n}\n\n \nvoid\nlpfc_nvmet_invalidate_host(struct lpfc_hba *phba, struct lpfc_nodelist *ndlp)\n{\n\tu32 ndlp_has_hh;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\n\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\tLOG_NVME | LOG_NVME_ABTS | LOG_NVME_DISC,\n\t\t\t\"6203 Invalidating hosthandle x%px\\n\",\n\t\t\tndlp);\n\n\ttgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;\n\tatomic_set(&tgtp->state, LPFC_NVMET_INV_HOST_ACTIVE);\n\n\tspin_lock_irq(&ndlp->lock);\n\tndlp_has_hh = ndlp->fc4_xpt_flags & NLP_XPT_HAS_HH;\n\tspin_unlock_irq(&ndlp->lock);\n\n\t \n\tif (!ndlp_has_hh) {\n\t\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\t\tLOG_NVME | LOG_NVME_ABTS | LOG_NVME_DISC,\n\t\t\t\t\"6204 Skip invalidate on node x%px DID x%x\\n\",\n\t\t\t\tndlp, ndlp->nlp_DID);\n\t\treturn;\n\t}\n\n#if (IS_ENABLED(CONFIG_NVME_TARGET_FC))\n\t \n\tnvmet_fc_invalidate_host(phba->targetport, ndlp);\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}