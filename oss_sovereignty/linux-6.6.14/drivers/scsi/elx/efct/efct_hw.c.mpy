{
  "module_name": "efct_hw.c",
  "hash_id": "3725b1f63fc57b669a8d946946875bb45bbe919de9a57dd4a0aca7b4f07a85ac",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/elx/efct/efct_hw.c",
  "human_readable_source": "\n \n\n#include \"efct_driver.h\"\n#include \"efct_hw.h\"\n#include \"efct_unsol.h\"\n\nstruct efct_hw_link_stat_cb_arg {\n\tvoid (*cb)(int status, u32 num_counters,\n\t\t   struct efct_hw_link_stat_counts *counters, void *arg);\n\tvoid *arg;\n};\n\nstruct efct_hw_host_stat_cb_arg {\n\tvoid (*cb)(int status, u32 num_counters,\n\t\t   struct efct_hw_host_stat_counts *counters, void *arg);\n\tvoid *arg;\n};\n\nstruct efct_hw_fw_wr_cb_arg {\n\tvoid (*cb)(int status, u32 bytes_written, u32 change_status, void *arg);\n\tvoid *arg;\n};\n\nstruct efct_mbox_rqst_ctx {\n\tint (*callback)(struct efc *efc, int status, u8 *mqe, void *arg);\n\tvoid *arg;\n};\n\nstatic int\nefct_hw_link_event_init(struct efct_hw *hw)\n{\n\thw->link.status = SLI4_LINK_STATUS_MAX;\n\thw->link.topology = SLI4_LINK_TOPO_NONE;\n\thw->link.medium = SLI4_LINK_MEDIUM_MAX;\n\thw->link.speed = 0;\n\thw->link.loop_map = NULL;\n\thw->link.fc_id = U32_MAX;\n\n\treturn 0;\n}\n\nstatic int\nefct_hw_read_max_dump_size(struct efct_hw *hw)\n{\n\tu8 buf[SLI4_BMBX_SIZE];\n\tstruct efct *efct = hw->os;\n\tint rc = 0;\n\tstruct sli4_rsp_cmn_set_dump_location *rsp;\n\n\t \n\tif (PCI_FUNC(efct->pci->devfn) != 0)\n\t\treturn rc;\n\n\tif (sli_cmd_common_set_dump_location(&hw->sli, buf, 1, 0, NULL, 0))\n\t\treturn -EIO;\n\n\trsp = (struct sli4_rsp_cmn_set_dump_location *)\n\t      (buf + offsetof(struct sli4_cmd_sli_config, payload.embed));\n\n\trc = efct_hw_command(hw, buf, EFCT_CMD_POLL, NULL, NULL);\n\tif (rc != 0) {\n\t\tefc_log_debug(hw->os, \"set dump location cmd failed\\n\");\n\t\treturn rc;\n\t}\n\n\thw->dump_size =\n\t  le32_to_cpu(rsp->buffer_length_dword) & SLI4_CMN_SET_DUMP_BUFFER_LEN;\n\n\tefc_log_debug(hw->os, \"Dump size %x\\n\",\thw->dump_size);\n\n\treturn rc;\n}\n\nstatic int\n__efct_read_topology_cb(struct efct_hw *hw, int status, u8 *mqe, void *arg)\n{\n\tstruct sli4_cmd_read_topology *read_topo =\n\t\t\t\t(struct sli4_cmd_read_topology *)mqe;\n\tu8 speed;\n\tstruct efc_domain_record drec = {0};\n\tstruct efct *efct = hw->os;\n\n\tif (status || le16_to_cpu(read_topo->hdr.status)) {\n\t\tefc_log_debug(hw->os, \"bad status cqe=%#x mqe=%#x\\n\", status,\n\t\t\t      le16_to_cpu(read_topo->hdr.status));\n\t\treturn -EIO;\n\t}\n\n\tswitch (le32_to_cpu(read_topo->dw2_attentype) &\n\t\tSLI4_READTOPO_ATTEN_TYPE) {\n\tcase SLI4_READ_TOPOLOGY_LINK_UP:\n\t\thw->link.status = SLI4_LINK_STATUS_UP;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_LINK_DOWN:\n\t\thw->link.status = SLI4_LINK_STATUS_DOWN;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_LINK_NO_ALPA:\n\t\thw->link.status = SLI4_LINK_STATUS_NO_ALPA;\n\t\tbreak;\n\tdefault:\n\t\thw->link.status = SLI4_LINK_STATUS_MAX;\n\t\tbreak;\n\t}\n\n\tswitch (read_topo->topology) {\n\tcase SLI4_READ_TOPO_NON_FC_AL:\n\t\thw->link.topology = SLI4_LINK_TOPO_NON_FC_AL;\n\t\tbreak;\n\tcase SLI4_READ_TOPO_FC_AL:\n\t\thw->link.topology = SLI4_LINK_TOPO_FC_AL;\n\t\tif (hw->link.status == SLI4_LINK_STATUS_UP)\n\t\t\thw->link.loop_map = hw->loop_map.virt;\n\t\thw->link.fc_id = read_topo->acquired_al_pa;\n\t\tbreak;\n\tdefault:\n\t\thw->link.topology = SLI4_LINK_TOPO_MAX;\n\t\tbreak;\n\t}\n\n\thw->link.medium = SLI4_LINK_MEDIUM_FC;\n\n\tspeed = (le32_to_cpu(read_topo->currlink_state) &\n\t\t SLI4_READTOPO_LINKSTATE_SPEED) >> 8;\n\tswitch (speed) {\n\tcase SLI4_READ_TOPOLOGY_SPEED_1G:\n\t\thw->link.speed =  1 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_2G:\n\t\thw->link.speed =  2 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_4G:\n\t\thw->link.speed =  4 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_8G:\n\t\thw->link.speed =  8 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_16G:\n\t\thw->link.speed = 16 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_32G:\n\t\thw->link.speed = 32 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_64G:\n\t\thw->link.speed = 64 * 1000;\n\t\tbreak;\n\tcase SLI4_READ_TOPOLOGY_SPEED_128G:\n\t\thw->link.speed = 128 * 1000;\n\t\tbreak;\n\t}\n\n\tdrec.speed = hw->link.speed;\n\tdrec.fc_id = hw->link.fc_id;\n\tdrec.is_nport = true;\n\tefc_domain_cb(efct->efcport, EFC_HW_DOMAIN_FOUND, &drec);\n\n\treturn 0;\n}\n\nstatic int\nefct_hw_cb_link(void *ctx, void *e)\n{\n\tstruct efct_hw *hw = ctx;\n\tstruct sli4_link_event *event = e;\n\tstruct efc_domain *d = NULL;\n\tint rc = 0;\n\tstruct efct *efct = hw->os;\n\n\tefct_hw_link_event_init(hw);\n\n\tswitch (event->status) {\n\tcase SLI4_LINK_STATUS_UP:\n\n\t\thw->link = *event;\n\t\tefct->efcport->link_status = EFC_LINK_STATUS_UP;\n\n\t\tif (event->topology == SLI4_LINK_TOPO_NON_FC_AL) {\n\t\t\tstruct efc_domain_record drec = {0};\n\n\t\t\tefc_log_info(hw->os, \"Link Up, NPORT, speed is %d\\n\",\n\t\t\t\t     event->speed);\n\t\t\tdrec.speed = event->speed;\n\t\t\tdrec.fc_id = event->fc_id;\n\t\t\tdrec.is_nport = true;\n\t\t\tefc_domain_cb(efct->efcport, EFC_HW_DOMAIN_FOUND,\n\t\t\t\t      &drec);\n\t\t} else if (event->topology == SLI4_LINK_TOPO_FC_AL) {\n\t\t\tu8 buf[SLI4_BMBX_SIZE];\n\n\t\t\tefc_log_info(hw->os, \"Link Up, LOOP, speed is %d\\n\",\n\t\t\t\t     event->speed);\n\n\t\t\tif (!sli_cmd_read_topology(&hw->sli, buf,\n\t\t\t\t\t\t   &hw->loop_map)) {\n\t\t\t\trc = efct_hw_command(hw, buf, EFCT_CMD_NOWAIT,\n\t\t\t\t\t\t__efct_read_topology_cb, NULL);\n\t\t\t}\n\n\t\t\tif (rc)\n\t\t\t\tefc_log_debug(hw->os, \"READ_TOPOLOGY failed\\n\");\n\t\t} else {\n\t\t\tefc_log_info(hw->os, \"%s(%#x), speed is %d\\n\",\n\t\t\t\t     \"Link Up, unsupported topology \",\n\t\t\t\t     event->topology, event->speed);\n\t\t}\n\t\tbreak;\n\tcase SLI4_LINK_STATUS_DOWN:\n\t\tefc_log_info(hw->os, \"Link down\\n\");\n\n\t\thw->link.status = event->status;\n\t\tefct->efcport->link_status = EFC_LINK_STATUS_DOWN;\n\n\t\td = efct->efcport->domain;\n\t\tif (d)\n\t\t\tefc_domain_cb(efct->efcport, EFC_HW_DOMAIN_LOST, d);\n\t\tbreak;\n\tdefault:\n\t\tefc_log_debug(hw->os, \"unhandled link status %#x\\n\",\n\t\t\t      event->status);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint\nefct_hw_setup(struct efct_hw *hw, void *os, struct pci_dev *pdev)\n{\n\tu32 i, max_sgl, cpus;\n\n\tif (hw->hw_setup_called)\n\t\treturn 0;\n\n\t \n\tmemset(hw, 0, sizeof(struct efct_hw));\n\n\thw->hw_setup_called = true;\n\n\thw->os = os;\n\n\tmutex_init(&hw->bmbx_lock);\n\tspin_lock_init(&hw->cmd_lock);\n\tINIT_LIST_HEAD(&hw->cmd_head);\n\tINIT_LIST_HEAD(&hw->cmd_pending);\n\thw->cmd_head_count = 0;\n\n\t \n\thw->cmd_ctx_pool = mempool_create_kmalloc_pool(EFCT_CMD_CTX_POOL_SZ,\n\t\t\t\t\tsizeof(struct efct_command_ctx));\n\tif (!hw->cmd_ctx_pool) {\n\t\tefc_log_err(hw->os, \"failed to allocate mailbox buffer pool\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\thw->mbox_rqst_pool = mempool_create_kmalloc_pool(EFCT_CMD_CTX_POOL_SZ,\n\t\t\t\t\tsizeof(struct efct_mbox_rqst_ctx));\n\tif (!hw->mbox_rqst_pool) {\n\t\tefc_log_err(hw->os, \"failed to allocate mbox request pool\\n\");\n\t\treturn -EIO;\n\t}\n\n\tspin_lock_init(&hw->io_lock);\n\tINIT_LIST_HEAD(&hw->io_inuse);\n\tINIT_LIST_HEAD(&hw->io_free);\n\tINIT_LIST_HEAD(&hw->io_wait_free);\n\n\tatomic_set(&hw->io_alloc_failed_count, 0);\n\n\thw->config.speed = SLI4_LINK_SPEED_AUTO_16_8_4;\n\tif (sli_setup(&hw->sli, hw->os, pdev, ((struct efct *)os)->reg)) {\n\t\tefc_log_err(hw->os, \"SLI setup failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\tefct_hw_link_event_init(hw);\n\n\tsli_callback(&hw->sli, SLI4_CB_LINK, efct_hw_cb_link, hw);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(hw->num_qentries); i++)\n\t\thw->num_qentries[i] = hw->sli.qinfo.max_qentries[i];\n\t \n\thw->num_qentries[SLI4_QTYPE_WQ] = hw->num_qentries[SLI4_QTYPE_CQ] / 2;\n\n\t \n\n\thw->config.rq_default_buffer_size = EFCT_HW_RQ_SIZE_PAYLOAD;\n\thw->config.n_io = hw->sli.ext[SLI4_RSRC_XRI].size;\n\n\tcpus = num_possible_cpus();\n\thw->config.n_eq = cpus > EFCT_HW_MAX_NUM_EQ ? EFCT_HW_MAX_NUM_EQ : cpus;\n\n\tmax_sgl = sli_get_max_sgl(&hw->sli) - SLI4_SGE_MAX_RESERVED;\n\tmax_sgl = (max_sgl > EFCT_FC_MAX_SGL) ? EFCT_FC_MAX_SGL : max_sgl;\n\thw->config.n_sgl = max_sgl;\n\n\t(void)efct_hw_read_max_dump_size(hw);\n\n\treturn 0;\n}\n\nstatic void\nefct_logfcfi(struct efct_hw *hw, u32 j, u32 i, u32 id)\n{\n\tefc_log_info(hw->os,\n\t\t     \"REG_FCFI: filter[%d] %08X -> RQ[%d] id=%d\\n\",\n\t\t     j, hw->config.filter_def[j], i, id);\n}\n\nstatic inline void\nefct_hw_init_free_io(struct efct_hw_io *io)\n{\n\t \n\tio->done = NULL;\n\tio->abort_done = NULL;\n\tio->status_saved = false;\n\tio->abort_in_progress = false;\n\tio->type = 0xFFFF;\n\tio->wq = NULL;\n}\n\nstatic bool efct_hw_iotype_is_originator(u16 io_type)\n{\n\tswitch (io_type) {\n\tcase EFCT_HW_FC_CT:\n\tcase EFCT_HW_ELS_REQ:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void\nefct_hw_io_restore_sgl(struct efct_hw *hw, struct efct_hw_io *io)\n{\n\t \n\tio->sgl = &io->def_sgl;\n\tio->sgl_count = io->def_sgl_count;\n}\n\nstatic void\nefct_hw_wq_process_io(void *arg, u8 *cqe, int status)\n{\n\tstruct efct_hw_io *io = arg;\n\tstruct efct_hw *hw = io->hw;\n\tstruct sli4_fc_wcqe *wcqe = (void *)cqe;\n\tu32\tlen = 0;\n\tu32 ext = 0;\n\n\t \n\tif (io->xbusy && (wcqe->flags & SLI4_WCQE_XB) == 0)\n\t\tio->xbusy = false;\n\n\t \n\tswitch (io->type) {\n\tcase EFCT_HW_BLS_ACC:\n\tcase EFCT_HW_BLS_RJT:\n\t\tbreak;\n\tcase EFCT_HW_ELS_REQ:\n\t\tsli_fc_els_did(&hw->sli, cqe, &ext);\n\t\tlen = sli_fc_response_length(&hw->sli, cqe);\n\t\tbreak;\n\tcase EFCT_HW_ELS_RSP:\n\tcase EFCT_HW_FC_CT_RSP:\n\t\tbreak;\n\tcase EFCT_HW_FC_CT:\n\t\tlen = sli_fc_response_length(&hw->sli, cqe);\n\t\tbreak;\n\tcase EFCT_HW_IO_TARGET_WRITE:\n\t\tlen = sli_fc_io_length(&hw->sli, cqe);\n\t\tbreak;\n\tcase EFCT_HW_IO_TARGET_READ:\n\t\tlen = sli_fc_io_length(&hw->sli, cqe);\n\t\tbreak;\n\tcase EFCT_HW_IO_TARGET_RSP:\n\t\tbreak;\n\tcase EFCT_HW_IO_DNRX_REQUEUE:\n\t\t \n\t\t \n\t\tbreak;\n\tdefault:\n\t\tefc_log_err(hw->os, \"unhandled io type %#x for XRI 0x%x\\n\",\n\t\t\t    io->type, io->indicator);\n\t\tbreak;\n\t}\n\tif (status) {\n\t\text = sli_fc_ext_status(&hw->sli, cqe);\n\t\t \n\t\tif (efct_hw_iotype_is_originator(io->type) &&\n\t\t    wcqe->flags & SLI4_WCQE_XB) {\n\t\t\tint rc;\n\n\t\t\tefc_log_debug(hw->os, \"aborting xri=%#x tag=%#x\\n\",\n\t\t\t\t      io->indicator, io->reqtag);\n\n\t\t\t \n\t\t\trc = efct_hw_io_abort(hw, io, false, NULL, NULL);\n\t\t\tif (rc == 0) {\n\t\t\t\t \n\t\t\t\tio->status_saved = true;\n\t\t\t\tio->saved_status = status;\n\t\t\t\tio->saved_ext = ext;\n\t\t\t\tio->saved_len = len;\n\t\t\t\tgoto exit_efct_hw_wq_process_io;\n\t\t\t} else if (rc == -EINPROGRESS) {\n\t\t\t\t \n\t\t\t\tefc_log_debug(hw->os, \"%s%#x tag=%#x\\n\",\n\t\t\t\t\t      \"abort in progress xri=\",\n\t\t\t\t\t      io->indicator, io->reqtag);\n\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tefc_log_debug(hw->os, \"%s%#x tag=%#x rc=%d\\n\",\n\t\t\t\t\t      \"Failed to abort xri=\",\n\t\t\t\t\t      io->indicator, io->reqtag, rc);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (io->done) {\n\t\tefct_hw_done_t done = io->done;\n\n\t\tio->done = NULL;\n\n\t\tif (io->status_saved) {\n\t\t\t \n\t\t\tstatus = io->saved_status;\n\t\t\tlen = io->saved_len;\n\t\t\text = io->saved_ext;\n\t\t\tio->status_saved = false;\n\t\t}\n\n\t\t \n\t\tefct_hw_io_restore_sgl(hw, io);\n\t\tdone(io, len, status, ext, io->arg);\n\t}\n\nexit_efct_hw_wq_process_io:\n\treturn;\n}\n\nstatic int\nefct_hw_setup_io(struct efct_hw *hw)\n{\n\tu32\ti = 0;\n\tstruct efct_hw_io\t*io = NULL;\n\tuintptr_t\txfer_virt = 0;\n\tuintptr_t\txfer_phys = 0;\n\tu32\tindex;\n\tbool new_alloc = true;\n\tstruct efc_dma *dma;\n\tstruct efct *efct = hw->os;\n\n\tif (!hw->io) {\n\t\thw->io = kmalloc_array(hw->config.n_io, sizeof(io), GFP_KERNEL);\n\t\tif (!hw->io)\n\t\t\treturn -ENOMEM;\n\n\t\tmemset(hw->io, 0, hw->config.n_io * sizeof(io));\n\n\t\tfor (i = 0; i < hw->config.n_io; i++) {\n\t\t\thw->io[i] = kzalloc(sizeof(*io), GFP_KERNEL);\n\t\t\tif (!hw->io[i])\n\t\t\t\tgoto error;\n\t\t}\n\n\t\t \n\t\thw->wqe_buffs = kzalloc((hw->config.n_io * hw->sli.wqe_size),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!hw->wqe_buffs) {\n\t\t\tkfree(hw->io);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t} else {\n\t\t \n\t\tnew_alloc = false;\n\t}\n\n\tif (new_alloc) {\n\t\tdma = &hw->xfer_rdy;\n\t\tdma->size = sizeof(struct fcp_txrdy) * hw->config.n_io;\n\t\tdma->virt = dma_alloc_coherent(&efct->pci->dev,\n\t\t\t\t\t       dma->size, &dma->phys, GFP_KERNEL);\n\t\tif (!dma->virt)\n\t\t\treturn -ENOMEM;\n\t}\n\txfer_virt = (uintptr_t)hw->xfer_rdy.virt;\n\txfer_phys = hw->xfer_rdy.phys;\n\n\t \n\tfor (i = 0; i < hw->config.n_io; i++) {\n\t\tstruct hw_wq_callback *wqcb;\n\n\t\tio = hw->io[i];\n\n\t\t \n\t\tio->hw = hw;\n\n\t\t \n\t\tio->wqe.wqebuf = &hw->wqe_buffs[i * hw->sli.wqe_size];\n\n\t\t \n\t\twqcb = efct_hw_reqtag_alloc(hw, efct_hw_wq_process_io, io);\n\t\tif (!wqcb) {\n\t\t\tefc_log_err(hw->os, \"can't allocate request tag\\n\");\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\tio->reqtag = wqcb->instance_index;\n\n\t\t \n\t\tefct_hw_init_free_io(io);\n\n\t\t \n\t\tio->xbusy = 0;\n\n\t\tif (sli_resource_alloc(&hw->sli, SLI4_RSRC_XRI,\n\t\t\t\t       &io->indicator, &index)) {\n\t\t\tefc_log_err(hw->os,\n\t\t\t\t    \"sli_resource_alloc failed @ %d\\n\", i);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (new_alloc) {\n\t\t\tdma = &io->def_sgl;\n\t\t\tdma->size = hw->config.n_sgl *\n\t\t\t\t\tsizeof(struct sli4_sge);\n\t\t\tdma->virt = dma_alloc_coherent(&efct->pci->dev,\n\t\t\t\t\t\t       dma->size, &dma->phys,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\tif (!dma->virt) {\n\t\t\t\tefc_log_err(hw->os, \"dma_alloc fail %d\\n\", i);\n\t\t\t\tmemset(&io->def_sgl, 0,\n\t\t\t\t       sizeof(struct efc_dma));\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\t\tio->def_sgl_count = hw->config.n_sgl;\n\t\tio->sgl = &io->def_sgl;\n\t\tio->sgl_count = io->def_sgl_count;\n\n\t\tif (hw->xfer_rdy.size) {\n\t\t\tio->xfer_rdy.virt = (void *)xfer_virt;\n\t\t\tio->xfer_rdy.phys = xfer_phys;\n\t\t\tio->xfer_rdy.size = sizeof(struct fcp_txrdy);\n\n\t\t\txfer_virt += sizeof(struct fcp_txrdy);\n\t\t\txfer_phys += sizeof(struct fcp_txrdy);\n\t\t}\n\t}\n\n\treturn 0;\nerror:\n\tfor (i = 0; i < hw->config.n_io && hw->io[i]; i++) {\n\t\tkfree(hw->io[i]);\n\t\thw->io[i] = NULL;\n\t}\n\n\tkfree(hw->io);\n\thw->io = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic int\nefct_hw_init_prereg_io(struct efct_hw *hw)\n{\n\tu32 i, idx = 0;\n\tstruct efct_hw_io *io = NULL;\n\tu8 cmd[SLI4_BMBX_SIZE];\n\tint rc = 0;\n\tu32 n_rem;\n\tu32 n = 0;\n\tu32 sgls_per_request = 256;\n\tstruct efc_dma **sgls = NULL;\n\tstruct efc_dma req;\n\tstruct efct *efct = hw->os;\n\n\tsgls = kmalloc_array(sgls_per_request, sizeof(*sgls), GFP_KERNEL);\n\tif (!sgls)\n\t\treturn -ENOMEM;\n\n\tmemset(&req, 0, sizeof(struct efc_dma));\n\treq.size = 32 + sgls_per_request * 16;\n\treq.virt = dma_alloc_coherent(&efct->pci->dev, req.size, &req.phys,\n\t\t\t\t      GFP_KERNEL);\n\tif (!req.virt) {\n\t\tkfree(sgls);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (n_rem = hw->config.n_io; n_rem; n_rem -= n) {\n\t\t \n\t\tu32 min = (sgls_per_request < n_rem) ? sgls_per_request : n_rem;\n\n\t\tfor (n = 0; n < min; n++) {\n\t\t\t \n\t\t\tif (n > 0) {\n\t\t\t\tif (hw->io[idx + n]->indicator !=\n\t\t\t\t    hw->io[idx + n - 1]->indicator + 1)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tsgls[n] = hw->io[idx + n]->sgl;\n\t\t}\n\n\t\tif (sli_cmd_post_sgl_pages(&hw->sli, cmd,\n\t\t\t\thw->io[idx]->indicator,\tn, sgls, NULL, &req)) {\n\t\t\trc = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\trc = efct_hw_command(hw, cmd, EFCT_CMD_POLL, NULL, NULL);\n\t\tif (rc) {\n\t\t\tefc_log_err(hw->os, \"SGL post failed, rc=%d\\n\", rc);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < n; i++, idx++) {\n\t\t\tio = hw->io[idx];\n\t\t\tio->state = EFCT_HW_IO_STATE_FREE;\n\t\t\tINIT_LIST_HEAD(&io->list_entry);\n\t\t\tlist_add_tail(&io->list_entry, &hw->io_free);\n\t\t}\n\t}\n\n\tdma_free_coherent(&efct->pci->dev, req.size, req.virt, req.phys);\n\tmemset(&req, 0, sizeof(struct efc_dma));\n\tkfree(sgls);\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_init_io(struct efct_hw *hw)\n{\n\tu32 i, idx = 0;\n\tbool prereg = false;\n\tstruct efct_hw_io *io = NULL;\n\tint rc = 0;\n\n\tprereg = hw->sli.params.sgl_pre_registered;\n\n\tif (prereg)\n\t\treturn efct_hw_init_prereg_io(hw);\n\n\tfor (i = 0; i < hw->config.n_io; i++, idx++) {\n\t\tio = hw->io[idx];\n\t\tio->state = EFCT_HW_IO_STATE_FREE;\n\t\tINIT_LIST_HEAD(&io->list_entry);\n\t\tlist_add_tail(&io->list_entry, &hw->io_free);\n\t}\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_config_set_fdt_xfer_hint(struct efct_hw *hw, u32 fdt_xfer_hint)\n{\n\tint rc = 0;\n\tu8 buf[SLI4_BMBX_SIZE];\n\tstruct sli4_rqst_cmn_set_features_set_fdt_xfer_hint param;\n\n\tmemset(&param, 0, sizeof(param));\n\tparam.fdt_xfer_hint = cpu_to_le32(fdt_xfer_hint);\n\t \n\tsli_cmd_common_set_features(&hw->sli, buf,\n\t\tSLI4_SET_FEATURES_SET_FTD_XFER_HINT, sizeof(param), &param);\n\n\trc = efct_hw_command(hw, buf, EFCT_CMD_POLL, NULL, NULL);\n\tif (rc)\n\t\tefc_log_warn(hw->os, \"set FDT hint %d failed: %d\\n\",\n\t\t\t     fdt_xfer_hint, rc);\n\telse\n\t\tefc_log_info(hw->os, \"Set FTD transfer hint to %d\\n\",\n\t\t\t     le32_to_cpu(param.fdt_xfer_hint));\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_config_rq(struct efct_hw *hw)\n{\n\tu32 min_rq_count, i, rc;\n\tstruct sli4_cmd_rq_cfg rq_cfg[SLI4_CMD_REG_FCFI_NUM_RQ_CFG];\n\tu8 buf[SLI4_BMBX_SIZE];\n\n\tefc_log_info(hw->os, \"using REG_FCFI standard\\n\");\n\n\t \n\tfor (i = 0; i < SLI4_CMD_REG_FCFI_NUM_RQ_CFG; i++) {\n\t\trq_cfg[i].rq_id = cpu_to_le16(0xffff);\n\t\trq_cfg[i].r_ctl_mask = (u8)hw->config.filter_def[i];\n\t\trq_cfg[i].r_ctl_match = (u8)(hw->config.filter_def[i] >> 8);\n\t\trq_cfg[i].type_mask = (u8)(hw->config.filter_def[i] >> 16);\n\t\trq_cfg[i].type_match = (u8)(hw->config.filter_def[i] >> 24);\n\t}\n\n\t \n\tmin_rq_count = (hw->hw_rq_count < SLI4_CMD_REG_FCFI_NUM_RQ_CFG)\t?\n\t\t\thw->hw_rq_count : SLI4_CMD_REG_FCFI_NUM_RQ_CFG;\n\tfor (i = 0; i < min_rq_count; i++) {\n\t\tstruct hw_rq *rq = hw->hw_rq[i];\n\t\tu32 j;\n\n\t\tfor (j = 0; j < SLI4_CMD_REG_FCFI_NUM_RQ_CFG; j++) {\n\t\t\tu32 mask = (rq->filter_mask != 0) ?\n\t\t\t\trq->filter_mask : 1;\n\n\t\t\tif (!(mask & (1U << j)))\n\t\t\t\tcontinue;\n\n\t\t\trq_cfg[i].rq_id = cpu_to_le16(rq->hdr->id);\n\t\t\tefct_logfcfi(hw, j, i, rq->hdr->id);\n\t\t}\n\t}\n\n\trc = -EIO;\n\tif (!sli_cmd_reg_fcfi(&hw->sli, buf, 0,\trq_cfg))\n\t\trc = efct_hw_command(hw, buf, EFCT_CMD_POLL, NULL, NULL);\n\n\tif (rc != 0) {\n\t\tefc_log_err(hw->os, \"FCFI registration failed\\n\");\n\t\treturn rc;\n\t}\n\thw->fcf_indicator =\n\t\tle16_to_cpu(((struct sli4_cmd_reg_fcfi *)buf)->fcfi);\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_config_mrq(struct efct_hw *hw, u8 mode, u16 fcf_index)\n{\n\tu8 buf[SLI4_BMBX_SIZE], mrq_bitmask = 0;\n\tstruct hw_rq *rq;\n\tstruct sli4_cmd_reg_fcfi_mrq *rsp = NULL;\n\tstruct sli4_cmd_rq_cfg rq_filter[SLI4_CMD_REG_FCFI_MRQ_NUM_RQ_CFG];\n\tu32 rc, i;\n\n\tif (mode == SLI4_CMD_REG_FCFI_SET_FCFI_MODE)\n\t\tgoto issue_cmd;\n\n\t \n\tfor (i = 0; i < SLI4_CMD_REG_FCFI_NUM_RQ_CFG; i++) {\n\t\trq_filter[i].rq_id = cpu_to_le16(0xffff);\n\t\trq_filter[i].type_mask = (u8)hw->config.filter_def[i];\n\t\trq_filter[i].type_match = (u8)(hw->config.filter_def[i] >> 8);\n\t\trq_filter[i].r_ctl_mask = (u8)(hw->config.filter_def[i] >> 16);\n\t\trq_filter[i].r_ctl_match = (u8)(hw->config.filter_def[i] >> 24);\n\t}\n\n\trq = hw->hw_rq[0];\n\trq_filter[0].rq_id = cpu_to_le16(rq->hdr->id);\n\trq_filter[1].rq_id = cpu_to_le16(rq->hdr->id);\n\n\tmrq_bitmask = 0x2;\nissue_cmd:\n\tefc_log_debug(hw->os, \"Issue reg_fcfi_mrq count:%d policy:%d mode:%d\\n\",\n\t\t      hw->hw_rq_count, hw->config.rq_selection_policy, mode);\n\t \n\trc = sli_cmd_reg_fcfi_mrq(&hw->sli, buf, mode, fcf_index,\n\t\t\t\t  hw->config.rq_selection_policy, mrq_bitmask,\n\t\t\t\t  hw->hw_mrq_count, rq_filter);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"sli_cmd_reg_fcfi_mrq() failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = efct_hw_command(hw, buf, EFCT_CMD_POLL, NULL, NULL);\n\n\trsp = (struct sli4_cmd_reg_fcfi_mrq *)buf;\n\n\tif ((rc) || (le16_to_cpu(rsp->hdr.status))) {\n\t\tefc_log_err(hw->os, \"FCFI MRQ reg failed. cmd=%x status=%x\\n\",\n\t\t\t    rsp->hdr.command, le16_to_cpu(rsp->hdr.status));\n\t\treturn -EIO;\n\t}\n\n\tif (mode == SLI4_CMD_REG_FCFI_SET_FCFI_MODE)\n\t\thw->fcf_indicator = le16_to_cpu(rsp->fcfi);\n\n\treturn 0;\n}\n\nstatic void\nefct_hw_queue_hash_add(struct efct_queue_hash *hash,\n\t\t       u16 id, u16 index)\n{\n\tu32 hash_index = id & (EFCT_HW_Q_HASH_SIZE - 1);\n\n\t \n\twhile (hash[hash_index].in_use)\n\t\thash_index = (hash_index + 1) & (EFCT_HW_Q_HASH_SIZE - 1);\n\n\t \n\thash[hash_index].id = id;\n\thash[hash_index].in_use = true;\n\thash[hash_index].index = index;\n}\n\nstatic int\nefct_hw_config_sli_port_health_check(struct efct_hw *hw, u8 query, u8 enable)\n{\n\tint rc = 0;\n\tu8 buf[SLI4_BMBX_SIZE];\n\tstruct sli4_rqst_cmn_set_features_health_check param;\n\tu32 health_check_flag = 0;\n\n\tmemset(&param, 0, sizeof(param));\n\n\tif (enable)\n\t\thealth_check_flag |= SLI4_RQ_HEALTH_CHECK_ENABLE;\n\n\tif (query)\n\t\thealth_check_flag |= SLI4_RQ_HEALTH_CHECK_QUERY;\n\n\tparam.health_check_dword = cpu_to_le32(health_check_flag);\n\n\t \n\tsli_cmd_common_set_features(&hw->sli, buf,\n\t\tSLI4_SET_FEATURES_SLI_PORT_HEALTH_CHECK, sizeof(param), &param);\n\n\trc = efct_hw_command(hw, buf, EFCT_CMD_POLL, NULL, NULL);\n\tif (rc)\n\t\tefc_log_err(hw->os, \"efct_hw_command returns %d\\n\", rc);\n\telse\n\t\tefc_log_debug(hw->os, \"SLI Port Health Check is enabled\\n\");\n\n\treturn rc;\n}\n\nint\nefct_hw_init(struct efct_hw *hw)\n{\n\tint rc;\n\tu32 i = 0;\n\tint rem_count;\n\tunsigned long flags = 0;\n\tstruct efct_hw_io *temp;\n\tstruct efc_dma *dma;\n\n\t \n\tspin_lock_irqsave(&hw->cmd_lock, flags);\n\tif (!list_empty(&hw->cmd_head)) {\n\t\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\t\tefc_log_err(hw->os, \"command found on cmd list\\n\");\n\t\treturn -EIO;\n\t}\n\tif (!list_empty(&hw->cmd_pending)) {\n\t\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\t\tefc_log_err(hw->os, \"command found on pending list\\n\");\n\t\treturn -EIO;\n\t}\n\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\n\t \n\tefct_hw_rx_free(hw);\n\n\t \n\n\t \n\trem_count = 0;\n\twhile ((!list_empty(&hw->io_wait_free))) {\n\t\trem_count++;\n\t\ttemp = list_first_entry(&hw->io_wait_free, struct efct_hw_io,\n\t\t\t\t\tlist_entry);\n\t\tlist_del_init(&temp->list_entry);\n\t}\n\tif (rem_count > 0)\n\t\tefc_log_debug(hw->os, \"rmvd %d items from io_wait_free list\\n\",\n\t\t\t      rem_count);\n\n\trem_count = 0;\n\twhile ((!list_empty(&hw->io_inuse))) {\n\t\trem_count++;\n\t\ttemp = list_first_entry(&hw->io_inuse, struct efct_hw_io,\n\t\t\t\t\tlist_entry);\n\t\tlist_del_init(&temp->list_entry);\n\t}\n\tif (rem_count > 0)\n\t\tefc_log_debug(hw->os, \"rmvd %d items from io_inuse list\\n\",\n\t\t\t      rem_count);\n\n\trem_count = 0;\n\twhile ((!list_empty(&hw->io_free))) {\n\t\trem_count++;\n\t\ttemp = list_first_entry(&hw->io_free, struct efct_hw_io,\n\t\t\t\t\tlist_entry);\n\t\tlist_del_init(&temp->list_entry);\n\t}\n\tif (rem_count > 0)\n\t\tefc_log_debug(hw->os, \"rmvd %d items from io_free list\\n\",\n\t\t\t      rem_count);\n\n\t \n\tif (hw->config.n_rq == 1)\n\t\thw->sli.features &= (~SLI4_REQFEAT_MRQP);\n\n\tif (sli_init(&hw->sli)) {\n\t\tefc_log_err(hw->os, \"SLI failed to initialize\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (hw->sliport_healthcheck) {\n\t\trc = efct_hw_config_sli_port_health_check(hw, 0, 1);\n\t\tif (rc != 0) {\n\t\t\tefc_log_err(hw->os, \"Enable port Health check fail\\n\");\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\tif (hw->sli.if_type == SLI4_INTF_IF_TYPE_2) {\n\t\t \n\t\tefct_hw_config_set_fdt_xfer_hint(hw, EFCT_HW_FDT_XFER_HINT);\n\t}\n\n\t \n\tmemset(hw->cq_hash, 0, sizeof(hw->cq_hash));\n\tefc_log_debug(hw->os, \"Max CQs %d, hash size = %d\\n\",\n\t\t      EFCT_HW_MAX_NUM_CQ, EFCT_HW_Q_HASH_SIZE);\n\n\tmemset(hw->rq_hash, 0, sizeof(hw->rq_hash));\n\tefc_log_debug(hw->os, \"Max RQs %d, hash size = %d\\n\",\n\t\t      EFCT_HW_MAX_NUM_RQ, EFCT_HW_Q_HASH_SIZE);\n\n\tmemset(hw->wq_hash, 0, sizeof(hw->wq_hash));\n\tefc_log_debug(hw->os, \"Max WQs %d, hash size = %d\\n\",\n\t\t      EFCT_HW_MAX_NUM_WQ, EFCT_HW_Q_HASH_SIZE);\n\n\trc = efct_hw_init_queues(hw);\n\tif (rc)\n\t\treturn rc;\n\n\trc = efct_hw_map_wq_cpu(hw);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = efct_hw_rx_allocate(hw);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"rx_allocate failed\\n\");\n\t\treturn rc;\n\t}\n\n\trc = efct_hw_rx_post(hw);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"WARNING - error posting RQ buffers\\n\");\n\t\treturn rc;\n\t}\n\n\tif (hw->config.n_eq == 1) {\n\t\trc = efct_hw_config_rq(hw);\n\t\tif (rc) {\n\t\t\tefc_log_err(hw->os, \"config rq failed %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\trc = efct_hw_config_mrq(hw, SLI4_CMD_REG_FCFI_SET_FCFI_MODE, 0);\n\t\tif (rc != 0) {\n\t\t\tefc_log_err(hw->os, \"REG_FCFI_MRQ FCFI reg failed\\n\");\n\t\t\treturn rc;\n\t\t}\n\n\t\trc = efct_hw_config_mrq(hw, SLI4_CMD_REG_FCFI_SET_MRQ_MODE, 0);\n\t\tif (rc != 0) {\n\t\t\tefc_log_err(hw->os, \"REG_FCFI_MRQ MRQ reg failed\\n\");\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\thw->wq_reqtag_pool = efct_hw_reqtag_pool_alloc(hw);\n\tif (!hw->wq_reqtag_pool) {\n\t\tefc_log_err(hw->os, \"efct_hw_reqtag_pool_alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trc = efct_hw_setup_io(hw);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"IO allocation failure\\n\");\n\t\treturn rc;\n\t}\n\n\trc = efct_hw_init_io(hw);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"IO initialization failure\\n\");\n\t\treturn rc;\n\t}\n\n\tdma = &hw->loop_map;\n\tdma->size = SLI4_MIN_LOOP_MAP_BYTES;\n\tdma->virt = dma_alloc_coherent(&hw->os->pci->dev, dma->size, &dma->phys,\n\t\t\t\t       GFP_KERNEL);\n\tif (!dma->virt)\n\t\treturn -EIO;\n\n\t \n\tfor (i = 0; i < hw->eq_count; i++)\n\t\tsli_queue_arm(&hw->sli, &hw->eq[i], true);\n\n\t \n\tfor (i = 0; i < hw->rq_count; i++)\n\t\tefct_hw_queue_hash_add(hw->rq_hash, hw->rq[i].id, i);\n\n\t \n\tfor (i = 0; i < hw->wq_count; i++)\n\t\tefct_hw_queue_hash_add(hw->wq_hash, hw->wq[i].id, i);\n\n\t \n\tfor (i = 0; i < hw->cq_count; i++) {\n\t\tefct_hw_queue_hash_add(hw->cq_hash, hw->cq[i].id, i);\n\t\tsli_queue_arm(&hw->sli, &hw->cq[i], true);\n\t}\n\n\t \n\tfor (i = 0; i < hw->hw_rq_count; i++) {\n\t\tstruct hw_rq *rq = hw->hw_rq[i];\n\n\t\thw->cq[rq->cq->instance].proc_limit = hw->config.n_io / 2;\n\t}\n\n\t \n\thw->state = EFCT_HW_STATE_ACTIVE;\n\t \n\thw->hw_wq[0]->send_frame_io = efct_hw_io_alloc(hw);\n\tif (!hw->hw_wq[0]->send_frame_io)\n\t\tefc_log_err(hw->os, \"alloc for send_frame_io failed\\n\");\n\n\t \n\tatomic_set(&hw->send_frame_seq_id, 0);\n\n\treturn 0;\n}\n\nint\nefct_hw_parse_filter(struct efct_hw *hw, void *value)\n{\n\tint rc = 0;\n\tchar *p = NULL;\n\tchar *token;\n\tu32 idx = 0;\n\n\tfor (idx = 0; idx < ARRAY_SIZE(hw->config.filter_def); idx++)\n\t\thw->config.filter_def[idx] = 0;\n\n\tp = kstrdup(value, GFP_KERNEL);\n\tif (!p || !*p) {\n\t\tefc_log_err(hw->os, \"p is NULL\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tidx = 0;\n\twhile ((token = strsep(&p, \",\")) && *token) {\n\t\tif (kstrtou32(token, 0, &hw->config.filter_def[idx++]))\n\t\t\tefc_log_err(hw->os, \"kstrtoint failed\\n\");\n\n\t\tif (!p || !*p)\n\t\t\tbreak;\n\n\t\tif (idx == ARRAY_SIZE(hw->config.filter_def))\n\t\t\tbreak;\n\t}\n\tkfree(p);\n\n\treturn rc;\n}\n\nu64\nefct_get_wwnn(struct efct_hw *hw)\n{\n\tstruct sli4 *sli = &hw->sli;\n\tu8 p[8];\n\n\tmemcpy(p, sli->wwnn, sizeof(p));\n\treturn get_unaligned_be64(p);\n}\n\nu64\nefct_get_wwpn(struct efct_hw *hw)\n{\n\tstruct sli4 *sli = &hw->sli;\n\tu8 p[8];\n\n\tmemcpy(p, sli->wwpn, sizeof(p));\n\treturn get_unaligned_be64(p);\n}\n\nstatic struct efc_hw_rq_buffer *\nefct_hw_rx_buffer_alloc(struct efct_hw *hw, u32 rqindex, u32 count,\n\t\t\tu32 size)\n{\n\tstruct efct *efct = hw->os;\n\tstruct efc_hw_rq_buffer *rq_buf = NULL;\n\tstruct efc_hw_rq_buffer *prq;\n\tu32 i;\n\n\tif (!count)\n\t\treturn NULL;\n\n\trq_buf = kmalloc_array(count, sizeof(*rq_buf), GFP_KERNEL);\n\tif (!rq_buf)\n\t\treturn NULL;\n\tmemset(rq_buf, 0, sizeof(*rq_buf) * count);\n\n\tfor (i = 0, prq = rq_buf; i < count; i ++, prq++) {\n\t\tprq->rqindex = rqindex;\n\t\tprq->dma.size = size;\n\t\tprq->dma.virt = dma_alloc_coherent(&efct->pci->dev,\n\t\t\t\t\t\t   prq->dma.size,\n\t\t\t\t\t\t   &prq->dma.phys,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!prq->dma.virt) {\n\t\t\tefc_log_err(hw->os, \"DMA allocation failed\\n\");\n\t\t\tkfree(rq_buf);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn rq_buf;\n}\n\nstatic void\nefct_hw_rx_buffer_free(struct efct_hw *hw,\n\t\t       struct efc_hw_rq_buffer *rq_buf,\n\t\t\tu32 count)\n{\n\tstruct efct *efct = hw->os;\n\tu32 i;\n\tstruct efc_hw_rq_buffer *prq;\n\n\tif (rq_buf) {\n\t\tfor (i = 0, prq = rq_buf; i < count; i++, prq++) {\n\t\t\tdma_free_coherent(&efct->pci->dev,\n\t\t\t\t\t  prq->dma.size, prq->dma.virt,\n\t\t\t\t\t  prq->dma.phys);\n\t\t\tmemset(&prq->dma, 0, sizeof(struct efc_dma));\n\t\t}\n\n\t\tkfree(rq_buf);\n\t}\n}\n\nint\nefct_hw_rx_allocate(struct efct_hw *hw)\n{\n\tstruct efct *efct = hw->os;\n\tu32 i;\n\tint rc = 0;\n\tu32 rqindex = 0;\n\tu32 hdr_size = EFCT_HW_RQ_SIZE_HDR;\n\tu32 payload_size = hw->config.rq_default_buffer_size;\n\n\trqindex = 0;\n\n\tfor (i = 0; i < hw->hw_rq_count; i++) {\n\t\tstruct hw_rq *rq = hw->hw_rq[i];\n\n\t\t \n\t\trq->hdr_buf = efct_hw_rx_buffer_alloc(hw, rqindex,\n\t\t\t\t\t\t      rq->entry_count,\n\t\t\t\t\t\t      hdr_size);\n\t\tif (!rq->hdr_buf) {\n\t\t\tefc_log_err(efct, \"rx_buffer_alloc hdr_buf failed\\n\");\n\t\t\trc = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tefc_log_debug(hw->os,\n\t\t\t      \"rq[%2d] rq_id %02d header  %4d by %4d bytes\\n\",\n\t\t\t      i, rq->hdr->id, rq->entry_count, hdr_size);\n\n\t\trqindex++;\n\n\t\t \n\t\trq->payload_buf = efct_hw_rx_buffer_alloc(hw, rqindex,\n\t\t\t\t\t\t\t  rq->entry_count,\n\t\t\t\t\t\t\t  payload_size);\n\t\tif (!rq->payload_buf) {\n\t\t\tefc_log_err(efct, \"rx_buffer_alloc fb_buf failed\\n\");\n\t\t\trc = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tefc_log_debug(hw->os,\n\t\t\t      \"rq[%2d] rq_id %02d default %4d by %4d bytes\\n\",\n\t\t\t      i, rq->data->id, rq->entry_count, payload_size);\n\t\trqindex++;\n\t}\n\n\treturn rc ? -EIO : 0;\n}\n\nint\nefct_hw_rx_post(struct efct_hw *hw)\n{\n\tu32 i;\n\tu32 idx;\n\tu32 rq_idx;\n\tint rc = 0;\n\n\tif (!hw->seq_pool) {\n\t\tu32 count = 0;\n\n\t\tfor (i = 0; i < hw->hw_rq_count; i++)\n\t\t\tcount += hw->hw_rq[i]->entry_count;\n\n\t\thw->seq_pool = kmalloc_array(count,\n\t\t\t\tsizeof(struct efc_hw_sequence),\tGFP_KERNEL);\n\t\tif (!hw->seq_pool)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (rq_idx = 0, idx = 0; rq_idx < hw->hw_rq_count; rq_idx++) {\n\t\tstruct hw_rq *rq = hw->hw_rq[rq_idx];\n\n\t\tfor (i = 0; i < rq->entry_count - 1; i++) {\n\t\t\tstruct efc_hw_sequence *seq;\n\n\t\t\tseq = hw->seq_pool + idx;\n\t\t\tidx++;\n\t\t\tseq->header = &rq->hdr_buf[i];\n\t\t\tseq->payload = &rq->payload_buf[i];\n\t\t\trc = efct_hw_sequence_free(hw, seq);\n\t\t\tif (rc)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\tif (rc && hw->seq_pool)\n\t\tkfree(hw->seq_pool);\n\n\treturn rc;\n}\n\nvoid\nefct_hw_rx_free(struct efct_hw *hw)\n{\n\tu32 i;\n\n\t \n\tfor (i = 0; i < hw->hw_rq_count; i++) {\n\t\tstruct hw_rq *rq = hw->hw_rq[i];\n\n\t\tif (rq) {\n\t\t\tefct_hw_rx_buffer_free(hw, rq->hdr_buf,\n\t\t\t\t\t       rq->entry_count);\n\t\t\trq->hdr_buf = NULL;\n\t\t\tefct_hw_rx_buffer_free(hw, rq->payload_buf,\n\t\t\t\t\t       rq->entry_count);\n\t\t\trq->payload_buf = NULL;\n\t\t}\n\t}\n}\n\nstatic int\nefct_hw_cmd_submit_pending(struct efct_hw *hw)\n{\n\tint rc = 0;\n\n\t \n\n\t \n\twhile (hw->cmd_head_count < (EFCT_HW_MQ_DEPTH - 1) &&\n\t       !list_empty(&hw->cmd_pending)) {\n\t\tstruct efct_command_ctx *ctx;\n\n\t\tctx = list_first_entry(&hw->cmd_pending,\n\t\t\t\t       struct efct_command_ctx, list_entry);\n\t\tif (!ctx)\n\t\t\tbreak;\n\n\t\tlist_del_init(&ctx->list_entry);\n\n\t\tlist_add_tail(&ctx->list_entry, &hw->cmd_head);\n\t\thw->cmd_head_count++;\n\t\tif (sli_mq_write(&hw->sli, hw->mq, ctx->buf) < 0) {\n\t\t\tefc_log_debug(hw->os,\n\t\t\t\t      \"sli_queue_write failed: %d\\n\", rc);\n\t\t\trc = -EIO;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn rc;\n}\n\nint\nefct_hw_command(struct efct_hw *hw, u8 *cmd, u32 opts, void *cb, void *arg)\n{\n\tint rc = -EIO;\n\tunsigned long flags = 0;\n\tvoid *bmbx = NULL;\n\n\t \n\tif (sli_fw_error_status(&hw->sli) > 0) {\n\t\tefc_log_crit(hw->os, \"Chip in an error state - reset needed\\n\");\n\t\tefc_log_crit(hw->os, \"status=%#x error1=%#x error2=%#x\\n\",\n\t\t\t     sli_reg_read_status(&hw->sli),\n\t\t\t     sli_reg_read_err1(&hw->sli),\n\t\t\t     sli_reg_read_err2(&hw->sli));\n\n\t\treturn -EIO;\n\t}\n\n\t \n\n\tif (opts == EFCT_CMD_POLL) {\n\t\tmutex_lock(&hw->bmbx_lock);\n\t\tbmbx = hw->sli.bmbx.virt;\n\n\t\tmemcpy(bmbx, cmd, SLI4_BMBX_SIZE);\n\n\t\tif (sli_bmbx_command(&hw->sli) == 0) {\n\t\t\trc = 0;\n\t\t\tmemcpy(cmd, bmbx, SLI4_BMBX_SIZE);\n\t\t}\n\t\tmutex_unlock(&hw->bmbx_lock);\n\t} else if (opts == EFCT_CMD_NOWAIT) {\n\t\tstruct efct_command_ctx\t*ctx = NULL;\n\n\t\tif (hw->state != EFCT_HW_STATE_ACTIVE) {\n\t\t\tefc_log_err(hw->os, \"Can't send command, HW state=%d\\n\",\n\t\t\t\t    hw->state);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tctx = mempool_alloc(hw->cmd_ctx_pool, GFP_ATOMIC);\n\t\tif (!ctx)\n\t\t\treturn -ENOSPC;\n\n\t\tmemset(ctx, 0, sizeof(struct efct_command_ctx));\n\n\t\tif (cb) {\n\t\t\tctx->cb = cb;\n\t\t\tctx->arg = arg;\n\t\t}\n\n\t\tmemcpy(ctx->buf, cmd, SLI4_BMBX_SIZE);\n\t\tctx->ctx = hw;\n\n\t\tspin_lock_irqsave(&hw->cmd_lock, flags);\n\n\t\t \n\t\tINIT_LIST_HEAD(&ctx->list_entry);\n\t\tlist_add_tail(&ctx->list_entry, &hw->cmd_pending);\n\n\t\t \n\t\trc = efct_hw_cmd_submit_pending(hw);\n\n\t\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\t}\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_command_process(struct efct_hw *hw, int status, u8 *mqe,\n\t\t\tsize_t size)\n{\n\tstruct efct_command_ctx *ctx = NULL;\n\tunsigned long flags = 0;\n\n\tspin_lock_irqsave(&hw->cmd_lock, flags);\n\tif (!list_empty(&hw->cmd_head)) {\n\t\tctx = list_first_entry(&hw->cmd_head,\n\t\t\t\t       struct efct_command_ctx, list_entry);\n\t\tlist_del_init(&ctx->list_entry);\n\t}\n\tif (!ctx) {\n\t\tefc_log_err(hw->os, \"no command context\\n\");\n\t\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\t\treturn -EIO;\n\t}\n\n\thw->cmd_head_count--;\n\n\t \n\tefct_hw_cmd_submit_pending(hw);\n\n\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\n\tif (ctx->cb) {\n\t\tmemcpy(ctx->buf, mqe, size);\n\t\tctx->cb(hw, status, ctx->buf, ctx->arg);\n\t}\n\n\tmempool_free(ctx, hw->cmd_ctx_pool);\n\n\treturn 0;\n}\n\nstatic int\nefct_hw_mq_process(struct efct_hw *hw,\n\t\t   int status, struct sli4_queue *mq)\n{\n\tu8 mqe[SLI4_BMBX_SIZE];\n\tint rc;\n\n\trc = sli_mq_read(&hw->sli, mq, mqe);\n\tif (!rc)\n\t\trc = efct_hw_command_process(hw, status, mqe, mq->size);\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_command_cancel(struct efct_hw *hw)\n{\n\tunsigned long flags = 0;\n\tint rc = 0;\n\n\tspin_lock_irqsave(&hw->cmd_lock, flags);\n\n\t \n\twhile (!list_empty(&hw->cmd_head)) {\n\t\tu8\t\tmqe[SLI4_BMBX_SIZE] = { 0 };\n\t\tstruct efct_command_ctx *ctx;\n\n\t\tctx = list_first_entry(&hw->cmd_head,\n\t\t\t\t       struct efct_command_ctx, list_entry);\n\n\t\tefc_log_debug(hw->os, \"hung command %08x\\n\",\n\t\t\t      !ctx ? U32_MAX : *((u32 *)ctx->buf));\n\t\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\t\trc = efct_hw_command_process(hw, -1, mqe, SLI4_BMBX_SIZE);\n\t\tspin_lock_irqsave(&hw->cmd_lock, flags);\n\t}\n\n\tspin_unlock_irqrestore(&hw->cmd_lock, flags);\n\n\treturn rc;\n}\n\nstatic void\nefct_mbox_rsp_cb(struct efct_hw *hw, int status, u8 *mqe, void *arg)\n{\n\tstruct efct_mbox_rqst_ctx *ctx = arg;\n\n\tif (ctx) {\n\t\tif (ctx->callback)\n\t\t\t(*ctx->callback)(hw->os->efcport, status, mqe,\n\t\t\t\t\t ctx->arg);\n\n\t\tmempool_free(ctx, hw->mbox_rqst_pool);\n\t}\n}\n\nint\nefct_issue_mbox_rqst(void *base, void *cmd, void *cb, void *arg)\n{\n\tstruct efct_mbox_rqst_ctx *ctx;\n\tstruct efct *efct = base;\n\tstruct efct_hw *hw = &efct->hw;\n\tint rc;\n\n\t \n\tctx = mempool_alloc(hw->mbox_rqst_pool, GFP_ATOMIC);\n\tif (!ctx)\n\t\treturn -EIO;\n\n\tctx->callback = cb;\n\tctx->arg = arg;\n\n\trc = efct_hw_command(hw, cmd, EFCT_CMD_NOWAIT, efct_mbox_rsp_cb, ctx);\n\tif (rc) {\n\t\tefc_log_err(efct, \"issue mbox rqst failure rc:%d\\n\", rc);\n\t\tmempool_free(ctx, hw->mbox_rqst_pool);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic inline struct efct_hw_io *\n_efct_hw_io_alloc(struct efct_hw *hw)\n{\n\tstruct efct_hw_io *io = NULL;\n\n\tif (!list_empty(&hw->io_free)) {\n\t\tio = list_first_entry(&hw->io_free, struct efct_hw_io,\n\t\t\t\t      list_entry);\n\t\tlist_del(&io->list_entry);\n\t}\n\tif (io) {\n\t\tINIT_LIST_HEAD(&io->list_entry);\n\t\tlist_add_tail(&io->list_entry, &hw->io_inuse);\n\t\tio->state = EFCT_HW_IO_STATE_INUSE;\n\t\tio->abort_reqtag = U32_MAX;\n\t\tio->wq = hw->wq_cpu_array[raw_smp_processor_id()];\n\t\tif (!io->wq) {\n\t\t\tefc_log_err(hw->os, \"WQ not assigned for cpu:%d\\n\",\n\t\t\t\t    raw_smp_processor_id());\n\t\t\tio->wq = hw->hw_wq[0];\n\t\t}\n\t\tkref_init(&io->ref);\n\t\tio->release = efct_hw_io_free_internal;\n\t} else {\n\t\tatomic_add(1, &hw->io_alloc_failed_count);\n\t}\n\n\treturn io;\n}\n\nstruct efct_hw_io *\nefct_hw_io_alloc(struct efct_hw *hw)\n{\n\tstruct efct_hw_io *io = NULL;\n\tunsigned long flags = 0;\n\n\tspin_lock_irqsave(&hw->io_lock, flags);\n\tio = _efct_hw_io_alloc(hw);\n\tspin_unlock_irqrestore(&hw->io_lock, flags);\n\n\treturn io;\n}\n\nstatic void\nefct_hw_io_free_move_correct_list(struct efct_hw *hw,\n\t\t\t\t  struct efct_hw_io *io)\n{\n\t \n\tif (io->xbusy) {\n\t\t \n\t\tINIT_LIST_HEAD(&io->list_entry);\n\t\tlist_add_tail(&io->list_entry, &hw->io_wait_free);\n\t\tio->state = EFCT_HW_IO_STATE_WAIT_FREE;\n\t} else {\n\t\t \n\t\tINIT_LIST_HEAD(&io->list_entry);\n\t\tlist_add_tail(&io->list_entry, &hw->io_free);\n\t\tio->state = EFCT_HW_IO_STATE_FREE;\n\t}\n}\n\nstatic inline void\nefct_hw_io_free_common(struct efct_hw *hw, struct efct_hw_io *io)\n{\n\t \n\tefct_hw_init_free_io(io);\n\n\t \n\tefct_hw_io_restore_sgl(hw, io);\n}\n\nvoid\nefct_hw_io_free_internal(struct kref *arg)\n{\n\tunsigned long flags = 0;\n\tstruct efct_hw_io *io =\tcontainer_of(arg, struct efct_hw_io, ref);\n\tstruct efct_hw *hw = io->hw;\n\n\t \n\tefct_hw_io_free_common(hw, io);\n\n\tspin_lock_irqsave(&hw->io_lock, flags);\n\t \n\tif (!list_empty(&io->list_entry) && !list_empty(&hw->io_inuse)) {\n\t\tlist_del_init(&io->list_entry);\n\t\tefct_hw_io_free_move_correct_list(hw, io);\n\t}\n\tspin_unlock_irqrestore(&hw->io_lock, flags);\n}\n\nint\nefct_hw_io_free(struct efct_hw *hw, struct efct_hw_io *io)\n{\n\treturn kref_put(&io->ref, io->release);\n}\n\nstruct efct_hw_io *\nefct_hw_io_lookup(struct efct_hw *hw, u32 xri)\n{\n\tu32 ioindex;\n\n\tioindex = xri - hw->sli.ext[SLI4_RSRC_XRI].base[0];\n\treturn hw->io[ioindex];\n}\n\nint\nefct_hw_io_init_sges(struct efct_hw *hw, struct efct_hw_io *io,\n\t\t     enum efct_hw_io_type type)\n{\n\tstruct sli4_sge\t*data = NULL;\n\tu32 i = 0;\n\tu32 skips = 0;\n\tu32 sge_flags = 0;\n\n\tif (!io) {\n\t\tefc_log_err(hw->os, \"bad parameter hw=%p io=%p\\n\", hw, io);\n\t\treturn -EIO;\n\t}\n\n\t \n\tio->sgl = &io->def_sgl;\n\tio->sgl_count = io->def_sgl_count;\n\tio->first_data_sge = 0;\n\n\tmemset(io->sgl->virt, 0, 2 * sizeof(struct sli4_sge));\n\tio->n_sge = 0;\n\tio->sge_offset = 0;\n\n\tio->type = type;\n\n\tdata = io->sgl->virt;\n\n\t \n\tswitch (type) {\n\tcase EFCT_HW_IO_TARGET_WRITE:\n\n\t\t \n\t\tsge_flags = le32_to_cpu(data->dw2_flags);\n\t\tsge_flags &= (~SLI4_SGE_TYPE_MASK);\n\t\tsge_flags |= (SLI4_SGE_TYPE_DATA << SLI4_SGE_TYPE_SHIFT);\n\t\tdata->buffer_address_high =\n\t\t\tcpu_to_le32(upper_32_bits(io->xfer_rdy.phys));\n\t\tdata->buffer_address_low  =\n\t\t\tcpu_to_le32(lower_32_bits(io->xfer_rdy.phys));\n\t\tdata->buffer_length = cpu_to_le32(io->xfer_rdy.size);\n\t\tdata->dw2_flags = cpu_to_le32(sge_flags);\n\t\tdata++;\n\n\t\tskips = EFCT_TARGET_WRITE_SKIPS;\n\n\t\tio->n_sge = 1;\n\t\tbreak;\n\tcase EFCT_HW_IO_TARGET_READ:\n\t\t \n\t\tskips = EFCT_TARGET_READ_SKIPS;\n\t\tbreak;\n\tcase EFCT_HW_IO_TARGET_RSP:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tefc_log_err(hw->os, \"unsupported IO type %#x\\n\", type);\n\t\treturn -EIO;\n\t}\n\n\t \n\tfor (i = 0; i < skips; i++) {\n\t\tsge_flags = le32_to_cpu(data->dw2_flags);\n\t\tsge_flags &= (~SLI4_SGE_TYPE_MASK);\n\t\tsge_flags |= (SLI4_SGE_TYPE_SKIP << SLI4_SGE_TYPE_SHIFT);\n\t\tdata->dw2_flags = cpu_to_le32(sge_flags);\n\t\tdata++;\n\t}\n\n\tio->n_sge += skips;\n\n\t \n\tsge_flags = le32_to_cpu(data->dw2_flags);\n\tsge_flags |= SLI4_SGE_LAST;\n\tdata->dw2_flags = cpu_to_le32(sge_flags);\n\n\treturn 0;\n}\n\nint\nefct_hw_io_add_sge(struct efct_hw *hw, struct efct_hw_io *io,\n\t\t   uintptr_t addr, u32 length)\n{\n\tstruct sli4_sge\t*data = NULL;\n\tu32 sge_flags = 0;\n\n\tif (!io || !addr || !length) {\n\t\tefc_log_err(hw->os,\n\t\t\t    \"bad parameter hw=%p io=%p addr=%lx length=%u\\n\",\n\t\t\t    hw, io, addr, length);\n\t\treturn -EIO;\n\t}\n\n\tif (length > hw->sli.sge_supported_length) {\n\t\tefc_log_err(hw->os,\n\t\t\t    \"length of SGE %d bigger than allowed %d\\n\",\n\t\t\t    length, hw->sli.sge_supported_length);\n\t\treturn -EIO;\n\t}\n\n\tdata = io->sgl->virt;\n\tdata += io->n_sge;\n\n\tsge_flags = le32_to_cpu(data->dw2_flags);\n\tsge_flags &= ~SLI4_SGE_TYPE_MASK;\n\tsge_flags |= SLI4_SGE_TYPE_DATA << SLI4_SGE_TYPE_SHIFT;\n\tsge_flags &= ~SLI4_SGE_DATA_OFFSET_MASK;\n\tsge_flags |= SLI4_SGE_DATA_OFFSET_MASK & io->sge_offset;\n\n\tdata->buffer_address_high = cpu_to_le32(upper_32_bits(addr));\n\tdata->buffer_address_low  = cpu_to_le32(lower_32_bits(addr));\n\tdata->buffer_length = cpu_to_le32(length);\n\n\t \n\tsge_flags |= SLI4_SGE_LAST;\n\tdata->dw2_flags = cpu_to_le32(sge_flags);\n\n\tif (io->n_sge) {\n\t\tsge_flags = le32_to_cpu(data[-1].dw2_flags);\n\t\tsge_flags &= ~SLI4_SGE_LAST;\n\t\tdata[-1].dw2_flags = cpu_to_le32(sge_flags);\n\t}\n\n\t \n\tif (io->first_data_sge == 0)\n\t\tio->first_data_sge = io->n_sge;\n\n\tio->sge_offset += length;\n\tio->n_sge++;\n\n\treturn 0;\n}\n\nvoid\nefct_hw_io_abort_all(struct efct_hw *hw)\n{\n\tstruct efct_hw_io *io_to_abort\t= NULL;\n\tstruct efct_hw_io *next_io = NULL;\n\n\tlist_for_each_entry_safe(io_to_abort, next_io,\n\t\t\t\t &hw->io_inuse, list_entry) {\n\t\tefct_hw_io_abort(hw, io_to_abort, true, NULL, NULL);\n\t}\n}\n\nstatic void\nefct_hw_wq_process_abort(void *arg, u8 *cqe, int status)\n{\n\tstruct efct_hw_io *io = arg;\n\tstruct efct_hw *hw = io->hw;\n\tu32 ext = 0;\n\tu32 len = 0;\n\tstruct hw_wq_callback *wqcb;\n\n\t \n\text = sli_fc_ext_status(&hw->sli, cqe);\n\tif (status == SLI4_FC_WCQE_STATUS_LOCAL_REJECT &&\n\t    ext == SLI4_FC_LOCAL_REJECT_NO_XRI && io->done) {\n\t\tefct_hw_done_t done = io->done;\n\n\t\tio->done = NULL;\n\n\t\t \n\t\tstatus = io->saved_status;\n\t\tlen = io->saved_len;\n\t\text = io->saved_ext;\n\t\tio->status_saved = false;\n\t\tdone(io, len, status, ext, io->arg);\n\t}\n\n\tif (io->abort_done) {\n\t\tefct_hw_done_t done = io->abort_done;\n\n\t\tio->abort_done = NULL;\n\t\tdone(io, len, status, ext, io->abort_arg);\n\t}\n\n\t \n\tio->abort_in_progress = false;\n\n\t \n\tif (io->abort_reqtag == U32_MAX) {\n\t\tefc_log_err(hw->os, \"HW IO already freed\\n\");\n\t\treturn;\n\t}\n\n\twqcb = efct_hw_reqtag_get_instance(hw, io->abort_reqtag);\n\tefct_hw_reqtag_free(hw, wqcb);\n\n\t \n\t(void)efct_hw_io_free(hw, io);\n}\n\nstatic void\nefct_hw_fill_abort_wqe(struct efct_hw *hw, struct efct_hw_wqe *wqe)\n{\n\tstruct sli4_abort_wqe *abort = (void *)wqe->wqebuf;\n\n\tmemset(abort, 0, hw->sli.wqe_size);\n\n\tabort->criteria = SLI4_ABORT_CRITERIA_XRI_TAG;\n\tabort->ia_ir_byte |= wqe->send_abts ? 0 : 1;\n\n\t \n\tabort->ia_ir_byte |= SLI4_ABRT_WQE_IR;\n\n\tabort->t_tag  = cpu_to_le32(wqe->id);\n\tabort->command = SLI4_WQE_ABORT;\n\tabort->request_tag = cpu_to_le16(wqe->abort_reqtag);\n\n\tabort->dw10w0_flags = cpu_to_le16(SLI4_ABRT_WQE_QOSD);\n\n\tabort->cq_id = cpu_to_le16(SLI4_CQ_DEFAULT);\n}\n\nint\nefct_hw_io_abort(struct efct_hw *hw, struct efct_hw_io *io_to_abort,\n\t\t bool send_abts, void *cb, void *arg)\n{\n\tstruct hw_wq_callback *wqcb;\n\tunsigned long flags = 0;\n\n\tif (!io_to_abort) {\n\t\tefc_log_err(hw->os, \"bad parameter hw=%p io=%p\\n\",\n\t\t\t    hw, io_to_abort);\n\t\treturn -EIO;\n\t}\n\n\tif (hw->state != EFCT_HW_STATE_ACTIVE) {\n\t\tefc_log_err(hw->os, \"cannot send IO abort, HW state=%d\\n\",\n\t\t\t    hw->state);\n\t\treturn -EIO;\n\t}\n\n\t \n\tif (kref_get_unless_zero(&io_to_abort->ref) == 0) {\n\t\t \n\t\tefc_log_debug(hw->os,\n\t\t\t      \"io not active xri=0x%x tag=0x%x\\n\",\n\t\t\t      io_to_abort->indicator, io_to_abort->reqtag);\n\t\treturn -ENOENT;\n\t}\n\n\t \n\tif (!io_to_abort->wq) {\n\t\tefc_log_debug(hw->os, \"io_to_abort xri=0x%x not active on WQ\\n\",\n\t\t\t      io_to_abort->indicator);\n\t\t \n\t\tkref_put(&io_to_abort->ref, io_to_abort->release);\n\t\treturn -ENOENT;\n\t}\n\n\t \n\tif (cmpxchg(&io_to_abort->abort_in_progress, false, true)) {\n\t\t \n\t\tkref_put(&io_to_abort->ref, io_to_abort->release);\n\t\tefc_log_debug(hw->os,\n\t\t\t      \"io already being aborted xri=0x%x tag=0x%x\\n\",\n\t\t\t      io_to_abort->indicator, io_to_abort->reqtag);\n\t\treturn -EINPROGRESS;\n\t}\n\n\t \n\tio_to_abort->abort_done = cb;\n\tio_to_abort->abort_arg  = arg;\n\n\t \n\twqcb = efct_hw_reqtag_alloc(hw, efct_hw_wq_process_abort, io_to_abort);\n\tif (!wqcb) {\n\t\tefc_log_err(hw->os, \"can't allocate request tag\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tio_to_abort->abort_reqtag = wqcb->instance_index;\n\tio_to_abort->wqe.send_abts = send_abts;\n\tio_to_abort->wqe.id = io_to_abort->indicator;\n\tio_to_abort->wqe.abort_reqtag = io_to_abort->abort_reqtag;\n\n\t \n\tif (io_to_abort->wq) {\n\t\tspin_lock_irqsave(&io_to_abort->wq->queue->lock, flags);\n\t\tif (io_to_abort->wqe.list_entry.next) {\n\t\t\tio_to_abort->wqe.abort_wqe_submit_needed = true;\n\t\t\tspin_unlock_irqrestore(&io_to_abort->wq->queue->lock,\n\t\t\t\t\t       flags);\n\t\t\treturn 0;\n\t\t}\n\t\tspin_unlock_irqrestore(&io_to_abort->wq->queue->lock, flags);\n\t}\n\n\tefct_hw_fill_abort_wqe(hw, &io_to_abort->wqe);\n\n\t \n\tif (efct_hw_wq_write(io_to_abort->wq, &io_to_abort->wqe)) {\n\t\tio_to_abort->abort_in_progress = false;\n\t\t \n\t\tkref_put(&io_to_abort->ref, io_to_abort->release);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nvoid\nefct_hw_reqtag_pool_free(struct efct_hw *hw)\n{\n\tu32 i;\n\tstruct reqtag_pool *reqtag_pool = hw->wq_reqtag_pool;\n\tstruct hw_wq_callback *wqcb = NULL;\n\n\tif (reqtag_pool) {\n\t\tfor (i = 0; i < U16_MAX; i++) {\n\t\t\twqcb = reqtag_pool->tags[i];\n\t\t\tif (!wqcb)\n\t\t\t\tcontinue;\n\n\t\t\tkfree(wqcb);\n\t\t}\n\t\tkfree(reqtag_pool);\n\t\thw->wq_reqtag_pool = NULL;\n\t}\n}\n\nstruct reqtag_pool *\nefct_hw_reqtag_pool_alloc(struct efct_hw *hw)\n{\n\tu32 i = 0;\n\tstruct reqtag_pool *reqtag_pool;\n\tstruct hw_wq_callback *wqcb;\n\n\treqtag_pool = kzalloc(sizeof(*reqtag_pool), GFP_KERNEL);\n\tif (!reqtag_pool)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&reqtag_pool->freelist);\n\t \n\tspin_lock_init(&reqtag_pool->lock);\n\tfor (i = 0; i < U16_MAX; i++) {\n\t\twqcb = kmalloc(sizeof(*wqcb), GFP_KERNEL);\n\t\tif (!wqcb)\n\t\t\tbreak;\n\n\t\treqtag_pool->tags[i] = wqcb;\n\t\twqcb->instance_index = i;\n\t\twqcb->callback = NULL;\n\t\twqcb->arg = NULL;\n\t\tINIT_LIST_HEAD(&wqcb->list_entry);\n\t\tlist_add_tail(&wqcb->list_entry, &reqtag_pool->freelist);\n\t}\n\n\treturn reqtag_pool;\n}\n\nstruct hw_wq_callback *\nefct_hw_reqtag_alloc(struct efct_hw *hw,\n\t\t     void (*callback)(void *arg, u8 *cqe, int status),\n\t\t     void *arg)\n{\n\tstruct hw_wq_callback *wqcb = NULL;\n\tstruct reqtag_pool *reqtag_pool = hw->wq_reqtag_pool;\n\tunsigned long flags = 0;\n\n\tif (!callback)\n\t\treturn wqcb;\n\n\tspin_lock_irqsave(&reqtag_pool->lock, flags);\n\n\tif (!list_empty(&reqtag_pool->freelist)) {\n\t\twqcb = list_first_entry(&reqtag_pool->freelist,\n\t\t\t\t\tstruct hw_wq_callback, list_entry);\n\t}\n\n\tif (wqcb) {\n\t\tlist_del_init(&wqcb->list_entry);\n\t\tspin_unlock_irqrestore(&reqtag_pool->lock, flags);\n\t\twqcb->callback = callback;\n\t\twqcb->arg = arg;\n\t} else {\n\t\tspin_unlock_irqrestore(&reqtag_pool->lock, flags);\n\t}\n\n\treturn wqcb;\n}\n\nvoid\nefct_hw_reqtag_free(struct efct_hw *hw, struct hw_wq_callback *wqcb)\n{\n\tunsigned long flags = 0;\n\tstruct reqtag_pool *reqtag_pool = hw->wq_reqtag_pool;\n\n\tif (!wqcb->callback)\n\t\tefc_log_err(hw->os, \"WQCB is already freed\\n\");\n\n\tspin_lock_irqsave(&reqtag_pool->lock, flags);\n\twqcb->callback = NULL;\n\twqcb->arg = NULL;\n\tINIT_LIST_HEAD(&wqcb->list_entry);\n\tlist_add(&wqcb->list_entry, &hw->wq_reqtag_pool->freelist);\n\tspin_unlock_irqrestore(&reqtag_pool->lock, flags);\n}\n\nstruct hw_wq_callback *\nefct_hw_reqtag_get_instance(struct efct_hw *hw, u32 instance_index)\n{\n\tstruct hw_wq_callback *wqcb;\n\n\twqcb = hw->wq_reqtag_pool->tags[instance_index];\n\tif (!wqcb)\n\t\tefc_log_err(hw->os, \"wqcb for instance %d is null\\n\",\n\t\t\t    instance_index);\n\n\treturn wqcb;\n}\n\nint\nefct_hw_queue_hash_find(struct efct_queue_hash *hash, u16 id)\n{\n\tint index = -1;\n\tint i = id & (EFCT_HW_Q_HASH_SIZE - 1);\n\n\t \n\tdo {\n\t\tif (hash[i].in_use && hash[i].id == id)\n\t\t\tindex = hash[i].index;\n\t\telse\n\t\t\ti = (i + 1) & (EFCT_HW_Q_HASH_SIZE - 1);\n\t} while (index == -1 && hash[i].in_use);\n\n\treturn index;\n}\n\nint\nefct_hw_process(struct efct_hw *hw, u32 vector,\n\t\tu32 max_isr_time_msec)\n{\n\tstruct hw_eq *eq;\n\n\t \n\tif (hw->state == EFCT_HW_STATE_UNINITIALIZED)\n\t\treturn 0;\n\n\t \n\teq = hw->hw_eq[vector];\n\tif (!eq)\n\t\treturn 0;\n\n\teq->use_count++;\n\n\treturn efct_hw_eq_process(hw, eq, max_isr_time_msec);\n}\n\nint\nefct_hw_eq_process(struct efct_hw *hw, struct hw_eq *eq,\n\t\t   u32 max_isr_time_msec)\n{\n\tu8 eqe[sizeof(struct sli4_eqe)] = { 0 };\n\tu32 tcheck_count;\n\tu64 tstart;\n\tu64 telapsed;\n\tbool done = false;\n\n\ttcheck_count = EFCT_HW_TIMECHECK_ITERATIONS;\n\ttstart = jiffies_to_msecs(jiffies);\n\n\twhile (!done && !sli_eq_read(&hw->sli, eq->queue, eqe)) {\n\t\tu16 cq_id = 0;\n\t\tint rc;\n\n\t\trc = sli_eq_parse(&hw->sli, eqe, &cq_id);\n\t\tif (unlikely(rc)) {\n\t\t\tif (rc == SLI4_EQE_STATUS_EQ_FULL) {\n\t\t\t\tu32 i;\n\n\t\t\t\t \n\t\t\t\tfor (i = 0; i < hw->cq_count; i++)\n\t\t\t\t\tefct_hw_cq_process(hw, hw->hw_cq[i]);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else {\n\t\t\tint index;\n\n\t\t\tindex  = efct_hw_queue_hash_find(hw->cq_hash, cq_id);\n\n\t\t\tif (likely(index >= 0))\n\t\t\t\tefct_hw_cq_process(hw, hw->hw_cq[index]);\n\t\t\telse\n\t\t\t\tefc_log_err(hw->os, \"bad CQ_ID %#06x\\n\", cq_id);\n\t\t}\n\n\t\tif (eq->queue->n_posted > eq->queue->posted_limit)\n\t\t\tsli_queue_arm(&hw->sli, eq->queue, false);\n\n\t\tif (tcheck_count && (--tcheck_count == 0)) {\n\t\t\ttcheck_count = EFCT_HW_TIMECHECK_ITERATIONS;\n\t\t\ttelapsed = jiffies_to_msecs(jiffies) - tstart;\n\t\t\tif (telapsed >= max_isr_time_msec)\n\t\t\t\tdone = true;\n\t\t}\n\t}\n\tsli_queue_eq_arm(&hw->sli, eq->queue, true);\n\n\treturn 0;\n}\n\nstatic int\n_efct_hw_wq_write(struct hw_wq *wq, struct efct_hw_wqe *wqe)\n{\n\tint queue_rc;\n\n\t \n\tif (wq->wqec_count)\n\t\twq->wqec_count--;\n\n\tif (wq->wqec_count == 0) {\n\t\tstruct sli4_generic_wqe *genwqe = (void *)wqe->wqebuf;\n\n\t\tgenwqe->cmdtype_wqec_byte |= SLI4_GEN_WQE_WQEC;\n\t\twq->wqec_count = wq->wqec_set_count;\n\t}\n\n\t \n\twq->free_count--;\n\n\tqueue_rc = sli_wq_write(&wq->hw->sli, wq->queue, wqe->wqebuf);\n\n\treturn (queue_rc < 0) ? -EIO : 0;\n}\n\nstatic void\nhw_wq_submit_pending(struct hw_wq *wq, u32 update_free_count)\n{\n\tstruct efct_hw_wqe *wqe;\n\tunsigned long flags = 0;\n\n\tspin_lock_irqsave(&wq->queue->lock, flags);\n\n\t \n\twq->free_count += update_free_count;\n\n\twhile ((wq->free_count > 0) && (!list_empty(&wq->pending_list))) {\n\t\twqe = list_first_entry(&wq->pending_list,\n\t\t\t\t       struct efct_hw_wqe, list_entry);\n\t\tlist_del_init(&wqe->list_entry);\n\t\t_efct_hw_wq_write(wq, wqe);\n\n\t\tif (wqe->abort_wqe_submit_needed) {\n\t\t\twqe->abort_wqe_submit_needed = false;\n\t\t\tefct_hw_fill_abort_wqe(wq->hw, wqe);\n\t\t\tINIT_LIST_HEAD(&wqe->list_entry);\n\t\t\tlist_add_tail(&wqe->list_entry, &wq->pending_list);\n\t\t\twq->wq_pending_count++;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&wq->queue->lock, flags);\n}\n\nvoid\nefct_hw_cq_process(struct efct_hw *hw, struct hw_cq *cq)\n{\n\tu8 cqe[sizeof(struct sli4_mcqe)];\n\tu16 rid = U16_MAX;\n\t \n\tenum sli4_qentry ctype;\n\tu32 n_processed = 0;\n\tu32 tstart, telapsed;\n\n\ttstart = jiffies_to_msecs(jiffies);\n\n\twhile (!sli_cq_read(&hw->sli, cq->queue, cqe)) {\n\t\tint status;\n\n\t\tstatus = sli_cq_parse(&hw->sli, cq->queue, cqe, &ctype, &rid);\n\t\t \n\t\tif (status < 0) {\n\t\t\tif (status == SLI4_MCQE_STATUS_NOT_COMPLETED)\n\t\t\t\t \n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (ctype) {\n\t\tcase SLI4_QENTRY_ASYNC:\n\t\t\tsli_cqe_async(&hw->sli, cqe);\n\t\t\tbreak;\n\t\tcase SLI4_QENTRY_MQ:\n\t\t\t \n\t\t\tefct_hw_mq_process(hw, status, hw->mq);\n\t\t\tbreak;\n\t\tcase SLI4_QENTRY_WQ:\n\t\t\tefct_hw_wq_process(hw, cq, cqe, status, rid);\n\t\t\tbreak;\n\t\tcase SLI4_QENTRY_WQ_RELEASE: {\n\t\t\tu32 wq_id = rid;\n\t\t\tint index;\n\t\t\tstruct hw_wq *wq = NULL;\n\n\t\t\tindex = efct_hw_queue_hash_find(hw->wq_hash, wq_id);\n\n\t\t\tif (likely(index >= 0)) {\n\t\t\t\twq = hw->hw_wq[index];\n\t\t\t} else {\n\t\t\t\tefc_log_err(hw->os, \"bad WQ_ID %#06x\\n\", wq_id);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\thw_wq_submit_pending(wq, wq->wqec_set_count);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tcase SLI4_QENTRY_RQ:\n\t\t\tefct_hw_rqpair_process_rq(hw, cq, cqe);\n\t\t\tbreak;\n\t\tcase SLI4_QENTRY_XABT: {\n\t\t\tefct_hw_xabt_process(hw, cq, cqe, rid);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tefc_log_debug(hw->os, \"unhandled ctype=%#x rid=%#x\\n\",\n\t\t\t\t      ctype, rid);\n\t\t\tbreak;\n\t\t}\n\n\t\tn_processed++;\n\t\tif (n_processed == cq->queue->proc_limit)\n\t\t\tbreak;\n\n\t\tif (cq->queue->n_posted >= cq->queue->posted_limit)\n\t\t\tsli_queue_arm(&hw->sli, cq->queue, false);\n\t}\n\n\tsli_queue_arm(&hw->sli, cq->queue, true);\n\n\tif (n_processed > cq->queue->max_num_processed)\n\t\tcq->queue->max_num_processed = n_processed;\n\ttelapsed = jiffies_to_msecs(jiffies) - tstart;\n\tif (telapsed > cq->queue->max_process_time)\n\t\tcq->queue->max_process_time = telapsed;\n}\n\nvoid\nefct_hw_wq_process(struct efct_hw *hw, struct hw_cq *cq,\n\t\t   u8 *cqe, int status, u16 rid)\n{\n\tstruct hw_wq_callback *wqcb;\n\n\tif (rid == EFCT_HW_REQUE_XRI_REGTAG) {\n\t\tif (status)\n\t\t\tefc_log_err(hw->os, \"reque xri failed, status = %d\\n\",\n\t\t\t\t    status);\n\t\treturn;\n\t}\n\n\twqcb = efct_hw_reqtag_get_instance(hw, rid);\n\tif (!wqcb) {\n\t\tefc_log_err(hw->os, \"invalid request tag: x%x\\n\", rid);\n\t\treturn;\n\t}\n\n\tif (!wqcb->callback) {\n\t\tefc_log_err(hw->os, \"wqcb callback is NULL\\n\");\n\t\treturn;\n\t}\n\n\t(*wqcb->callback)(wqcb->arg, cqe, status);\n}\n\nvoid\nefct_hw_xabt_process(struct efct_hw *hw, struct hw_cq *cq,\n\t\t     u8 *cqe, u16 rid)\n{\n\t \n\tstruct efct_hw_io *io = NULL;\n\tunsigned long flags = 0;\n\n\tio = efct_hw_io_lookup(hw, rid);\n\tif (!io) {\n\t\t \n\t\tefc_log_err(hw->os, \"xabt io lookup failed rid=%#x\\n\", rid);\n\t\treturn;\n\t}\n\n\tif (!io->xbusy)\n\t\tefc_log_debug(hw->os, \"xabt io not busy rid=%#x\\n\", rid);\n\telse\n\t\t \n\t\tio->xbusy = false;\n\n\t \n\tif (io->done) {\n\t\tefct_hw_done_t done = io->done;\n\t\tvoid\t\t*arg = io->arg;\n\n\t\t \n\t\tint status = io->saved_status;\n\t\tu32 len = io->saved_len;\n\t\tu32 ext = io->saved_ext;\n\n\t\tio->done = NULL;\n\t\tio->status_saved = false;\n\n\t\tdone(io, len, status, ext, arg);\n\t}\n\n\tspin_lock_irqsave(&hw->io_lock, flags);\n\tif (io->state == EFCT_HW_IO_STATE_INUSE ||\n\t    io->state == EFCT_HW_IO_STATE_WAIT_FREE) {\n\t\t \n\t\tif (io->state == EFCT_HW_IO_STATE_WAIT_FREE) {\n\t\t\tio->state = EFCT_HW_IO_STATE_FREE;\n\t\t\tlist_del_init(&io->list_entry);\n\t\t\tefct_hw_io_free_move_correct_list(hw, io);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&hw->io_lock, flags);\n}\n\nstatic int\nefct_hw_flush(struct efct_hw *hw)\n{\n\tu32 i = 0;\n\n\t \n\tfor (i = 0; i < hw->eq_count; i++)\n\t\tefct_hw_process(hw, i, ~0);\n\n\treturn 0;\n}\n\nint\nefct_hw_wq_write(struct hw_wq *wq, struct efct_hw_wqe *wqe)\n{\n\tint rc = 0;\n\tunsigned long flags = 0;\n\n\tspin_lock_irqsave(&wq->queue->lock, flags);\n\tif (list_empty(&wq->pending_list)) {\n\t\tif (wq->free_count > 0) {\n\t\t\trc = _efct_hw_wq_write(wq, wqe);\n\t\t} else {\n\t\t\tINIT_LIST_HEAD(&wqe->list_entry);\n\t\t\tlist_add_tail(&wqe->list_entry, &wq->pending_list);\n\t\t\twq->wq_pending_count++;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&wq->queue->lock, flags);\n\t\treturn rc;\n\t}\n\n\tINIT_LIST_HEAD(&wqe->list_entry);\n\tlist_add_tail(&wqe->list_entry, &wq->pending_list);\n\twq->wq_pending_count++;\n\twhile (wq->free_count > 0) {\n\t\twqe = list_first_entry(&wq->pending_list, struct efct_hw_wqe,\n\t\t\t\t       list_entry);\n\t\tif (!wqe)\n\t\t\tbreak;\n\n\t\tlist_del_init(&wqe->list_entry);\n\t\trc = _efct_hw_wq_write(wq, wqe);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tif (wqe->abort_wqe_submit_needed) {\n\t\t\twqe->abort_wqe_submit_needed = false;\n\t\t\tefct_hw_fill_abort_wqe(wq->hw, wqe);\n\n\t\t\tINIT_LIST_HEAD(&wqe->list_entry);\n\t\t\tlist_add_tail(&wqe->list_entry, &wq->pending_list);\n\t\t\twq->wq_pending_count++;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&wq->queue->lock, flags);\n\n\treturn rc;\n}\n\nint\nefct_efc_bls_send(struct efc *efc, u32 type, struct sli_bls_params *bls)\n{\n\tstruct efct *efct = efc->base;\n\n\treturn efct_hw_bls_send(efct, type, bls, NULL, NULL);\n}\n\nint\nefct_hw_bls_send(struct efct *efct, u32 type, struct sli_bls_params *bls_params,\n\t\t void *cb, void *arg)\n{\n\tstruct efct_hw *hw = &efct->hw;\n\tstruct efct_hw_io *hio;\n\tstruct sli_bls_payload bls;\n\tint rc;\n\n\tif (hw->state != EFCT_HW_STATE_ACTIVE) {\n\t\tefc_log_err(hw->os,\n\t\t\t    \"cannot send BLS, HW state=%d\\n\", hw->state);\n\t\treturn -EIO;\n\t}\n\n\thio = efct_hw_io_alloc(hw);\n\tif (!hio) {\n\t\tefc_log_err(hw->os, \"HIO allocation failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\thio->done = cb;\n\thio->arg  = arg;\n\n\tbls_params->xri = hio->indicator;\n\tbls_params->tag = hio->reqtag;\n\n\tif (type == FC_RCTL_BA_ACC) {\n\t\thio->type = EFCT_HW_BLS_ACC;\n\t\tbls.type = SLI4_SLI_BLS_ACC;\n\t\tmemcpy(&bls.u.acc, bls_params->payload, sizeof(bls.u.acc));\n\t} else {\n\t\thio->type = EFCT_HW_BLS_RJT;\n\t\tbls.type = SLI4_SLI_BLS_RJT;\n\t\tmemcpy(&bls.u.rjt, bls_params->payload, sizeof(bls.u.rjt));\n\t}\n\n\tbls.ox_id = cpu_to_le16(bls_params->ox_id);\n\tbls.rx_id = cpu_to_le16(bls_params->rx_id);\n\n\tif (sli_xmit_bls_rsp64_wqe(&hw->sli, hio->wqe.wqebuf,\n\t\t\t\t   &bls, bls_params)) {\n\t\tefc_log_err(hw->os, \"XMIT_BLS_RSP64 WQE error\\n\");\n\t\treturn -EIO;\n\t}\n\n\thio->xbusy = true;\n\n\t \n\thio->wq->use_count++;\n\trc = efct_hw_wq_write(hio->wq, &hio->wqe);\n\tif (rc >= 0) {\n\t\t \n\t\trc = 0;\n\t} else {\n\t\t \n\t\tefc_log_err(hw->os,\n\t\t\t    \"sli_queue_write failed: %d\\n\", rc);\n\t\thio->xbusy = false;\n\t}\n\n\treturn rc;\n}\n\nstatic int\nefct_els_ssrs_send_cb(struct efct_hw_io *hio, u32 length, int status,\n\t\t      u32 ext_status, void *arg)\n{\n\tstruct efc_disc_io *io = arg;\n\n\tefc_disc_io_complete(io, length, status, ext_status);\n\treturn 0;\n}\n\nstatic inline void\nefct_fill_els_params(struct efc_disc_io *io, struct sli_els_params *params)\n{\n\tu8 *cmd = io->req.virt;\n\n\tparams->cmd = *cmd;\n\tparams->s_id = io->s_id;\n\tparams->d_id = io->d_id;\n\tparams->ox_id = io->iparam.els.ox_id;\n\tparams->rpi = io->rpi;\n\tparams->vpi = io->vpi;\n\tparams->rpi_registered = io->rpi_registered;\n\tparams->xmit_len = io->xmit_len;\n\tparams->rsp_len = io->rsp_len;\n\tparams->timeout = io->iparam.els.timeout;\n}\n\nstatic inline void\nefct_fill_ct_params(struct efc_disc_io *io, struct sli_ct_params *params)\n{\n\tparams->r_ctl = io->iparam.ct.r_ctl;\n\tparams->type = io->iparam.ct.type;\n\tparams->df_ctl =  io->iparam.ct.df_ctl;\n\tparams->d_id = io->d_id;\n\tparams->ox_id = io->iparam.ct.ox_id;\n\tparams->rpi = io->rpi;\n\tparams->vpi = io->vpi;\n\tparams->rpi_registered = io->rpi_registered;\n\tparams->xmit_len = io->xmit_len;\n\tparams->rsp_len = io->rsp_len;\n\tparams->timeout = io->iparam.ct.timeout;\n}\n\n \nint\nefct_els_hw_srrs_send(struct efc *efc, struct efc_disc_io *io)\n{\n\tstruct efct *efct = efc->base;\n\tstruct efct_hw_io *hio;\n\tstruct efct_hw *hw = &efct->hw;\n\tstruct efc_dma *send = &io->req;\n\tstruct efc_dma *receive = &io->rsp;\n\tstruct sli4_sge\t*sge = NULL;\n\tint rc = 0;\n\tu32 len = io->xmit_len;\n\tu32 sge0_flags;\n\tu32 sge1_flags;\n\n\thio = efct_hw_io_alloc(hw);\n\tif (!hio) {\n\t\tpr_err(\"HIO alloc failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (hw->state != EFCT_HW_STATE_ACTIVE) {\n\t\tefc_log_debug(hw->os,\n\t\t\t      \"cannot send SRRS, HW state=%d\\n\", hw->state);\n\t\treturn -EIO;\n\t}\n\n\thio->done = efct_els_ssrs_send_cb;\n\thio->arg  = io;\n\n\tsge = hio->sgl->virt;\n\n\t \n\tmemset(hio->sgl->virt, 0, 2 * sizeof(struct sli4_sge));\n\n\tsge0_flags = le32_to_cpu(sge[0].dw2_flags);\n\tsge1_flags = le32_to_cpu(sge[1].dw2_flags);\n\tif (send->size) {\n\t\tsge[0].buffer_address_high =\n\t\t\tcpu_to_le32(upper_32_bits(send->phys));\n\t\tsge[0].buffer_address_low  =\n\t\t\tcpu_to_le32(lower_32_bits(send->phys));\n\n\t\tsge0_flags |= (SLI4_SGE_TYPE_DATA << SLI4_SGE_TYPE_SHIFT);\n\n\t\tsge[0].buffer_length = cpu_to_le32(len);\n\t}\n\n\tif (io->io_type == EFC_DISC_IO_ELS_REQ ||\n\t    io->io_type == EFC_DISC_IO_CT_REQ) {\n\t\tsge[1].buffer_address_high =\n\t\t\tcpu_to_le32(upper_32_bits(receive->phys));\n\t\tsge[1].buffer_address_low  =\n\t\t\tcpu_to_le32(lower_32_bits(receive->phys));\n\n\t\tsge1_flags |= (SLI4_SGE_TYPE_DATA << SLI4_SGE_TYPE_SHIFT);\n\t\tsge1_flags |= SLI4_SGE_LAST;\n\n\t\tsge[1].buffer_length = cpu_to_le32(receive->size);\n\t} else {\n\t\tsge0_flags |= SLI4_SGE_LAST;\n\t}\n\n\tsge[0].dw2_flags = cpu_to_le32(sge0_flags);\n\tsge[1].dw2_flags = cpu_to_le32(sge1_flags);\n\n\tswitch (io->io_type) {\n\tcase EFC_DISC_IO_ELS_REQ: {\n\t\tstruct sli_els_params els_params;\n\n\t\thio->type = EFCT_HW_ELS_REQ;\n\t\tefct_fill_els_params(io, &els_params);\n\t\tels_params.xri = hio->indicator;\n\t\tels_params.tag = hio->reqtag;\n\n\t\tif (sli_els_request64_wqe(&hw->sli, hio->wqe.wqebuf, hio->sgl,\n\t\t\t\t\t  &els_params)) {\n\t\t\tefc_log_err(hw->os, \"REQ WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tcase EFC_DISC_IO_ELS_RESP: {\n\t\tstruct sli_els_params els_params;\n\n\t\thio->type = EFCT_HW_ELS_RSP;\n\t\tefct_fill_els_params(io, &els_params);\n\t\tels_params.xri = hio->indicator;\n\t\tels_params.tag = hio->reqtag;\n\t\tif (sli_xmit_els_rsp64_wqe(&hw->sli, hio->wqe.wqebuf, send,\n\t\t\t\t\t   &els_params)){\n\t\t\tefc_log_err(hw->os, \"RSP WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tcase EFC_DISC_IO_CT_REQ: {\n\t\tstruct sli_ct_params ct_params;\n\n\t\thio->type = EFCT_HW_FC_CT;\n\t\tefct_fill_ct_params(io, &ct_params);\n\t\tct_params.xri = hio->indicator;\n\t\tct_params.tag = hio->reqtag;\n\t\tif (sli_gen_request64_wqe(&hw->sli, hio->wqe.wqebuf, hio->sgl,\n\t\t\t\t\t  &ct_params)){\n\t\t\tefc_log_err(hw->os, \"GEN WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tcase EFC_DISC_IO_CT_RESP: {\n\t\tstruct sli_ct_params ct_params;\n\n\t\thio->type = EFCT_HW_FC_CT_RSP;\n\t\tefct_fill_ct_params(io, &ct_params);\n\t\tct_params.xri = hio->indicator;\n\t\tct_params.tag = hio->reqtag;\n\t\tif (sli_xmit_sequence64_wqe(&hw->sli, hio->wqe.wqebuf, hio->sgl,\n\t\t\t\t\t    &ct_params)){\n\t\t\tefc_log_err(hw->os, \"XMIT SEQ WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tefc_log_err(hw->os, \"bad SRRS type %#x\\n\", io->io_type);\n\t\trc = -EIO;\n\t}\n\n\tif (rc == 0) {\n\t\thio->xbusy = true;\n\n\t\t \n\t\thio->wq->use_count++;\n\t\trc = efct_hw_wq_write(hio->wq, &hio->wqe);\n\t\tif (rc >= 0) {\n\t\t\t \n\t\t\trc = 0;\n\t\t} else {\n\t\t\t \n\t\t\tefc_log_err(hw->os,\n\t\t\t\t    \"sli_queue_write failed: %d\\n\", rc);\n\t\t\thio->xbusy = false;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nint\nefct_hw_io_send(struct efct_hw *hw, enum efct_hw_io_type type,\n\t\tstruct efct_hw_io *io, union efct_hw_io_param_u *iparam,\n\t\tvoid *cb, void *arg)\n{\n\tint rc = 0;\n\tbool send_wqe = true;\n\n\tif (!io) {\n\t\tpr_err(\"bad parm hw=%p io=%p\\n\", hw, io);\n\t\treturn -EIO;\n\t}\n\n\tif (hw->state != EFCT_HW_STATE_ACTIVE) {\n\t\tefc_log_err(hw->os, \"cannot send IO, HW state=%d\\n\", hw->state);\n\t\treturn -EIO;\n\t}\n\n\t \n\tio->type  = type;\n\tio->done  = cb;\n\tio->arg   = arg;\n\n\t \n\tswitch (type) {\n\tcase EFCT_HW_IO_TARGET_WRITE: {\n\t\tu16 *flags = &iparam->fcp_tgt.flags;\n\t\tstruct fcp_txrdy *xfer = io->xfer_rdy.virt;\n\n\t\t \n\t\txfer->ft_data_ro = cpu_to_be32(iparam->fcp_tgt.offset);\n\t\txfer->ft_burst_len = cpu_to_be32(iparam->fcp_tgt.xmit_len);\n\n\t\tif (io->xbusy)\n\t\t\t*flags |= SLI4_IO_CONTINUATION;\n\t\telse\n\t\t\t*flags &= ~SLI4_IO_CONTINUATION;\n\t\tiparam->fcp_tgt.xri = io->indicator;\n\t\tiparam->fcp_tgt.tag = io->reqtag;\n\n\t\tif (sli_fcp_treceive64_wqe(&hw->sli, io->wqe.wqebuf,\n\t\t\t\t\t   &io->def_sgl, io->first_data_sge,\n\t\t\t\t\t   SLI4_CQ_DEFAULT,\n\t\t\t\t\t   0, 0, &iparam->fcp_tgt)) {\n\t\t\tefc_log_err(hw->os, \"TRECEIVE WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tcase EFCT_HW_IO_TARGET_READ: {\n\t\tu16 *flags = &iparam->fcp_tgt.flags;\n\n\t\tif (io->xbusy)\n\t\t\t*flags |= SLI4_IO_CONTINUATION;\n\t\telse\n\t\t\t*flags &= ~SLI4_IO_CONTINUATION;\n\n\t\tiparam->fcp_tgt.xri = io->indicator;\n\t\tiparam->fcp_tgt.tag = io->reqtag;\n\n\t\tif (sli_fcp_tsend64_wqe(&hw->sli, io->wqe.wqebuf,\n\t\t\t\t\t&io->def_sgl, io->first_data_sge,\n\t\t\t\t\tSLI4_CQ_DEFAULT,\n\t\t\t\t\t0, 0, &iparam->fcp_tgt)) {\n\t\t\tefc_log_err(hw->os, \"TSEND WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\t}\n\tcase EFCT_HW_IO_TARGET_RSP: {\n\t\tu16 *flags = &iparam->fcp_tgt.flags;\n\n\t\tif (io->xbusy)\n\t\t\t*flags |= SLI4_IO_CONTINUATION;\n\t\telse\n\t\t\t*flags &= ~SLI4_IO_CONTINUATION;\n\n\t\tiparam->fcp_tgt.xri = io->indicator;\n\t\tiparam->fcp_tgt.tag = io->reqtag;\n\n\t\tif (sli_fcp_trsp64_wqe(&hw->sli, io->wqe.wqebuf,\n\t\t\t\t       &io->def_sgl, SLI4_CQ_DEFAULT,\n\t\t\t\t       0, &iparam->fcp_tgt)) {\n\t\t\tefc_log_err(hw->os, \"TRSP WQE error\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\n\t\tbreak;\n\t}\n\tdefault:\n\t\tefc_log_err(hw->os, \"unsupported IO type %#x\\n\", type);\n\t\trc = -EIO;\n\t}\n\n\tif (send_wqe && rc == 0) {\n\t\tio->xbusy = true;\n\n\t\t \n\t\thw->tcmd_wq_submit[io->wq->instance]++;\n\t\tio->wq->use_count++;\n\t\trc = efct_hw_wq_write(io->wq, &io->wqe);\n\t\tif (rc >= 0) {\n\t\t\t \n\t\t\trc = 0;\n\t\t} else {\n\t\t\t \n\t\t\tefc_log_err(hw->os,\n\t\t\t\t    \"sli_queue_write failed: %d\\n\", rc);\n\t\t\tio->xbusy = false;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nint\nefct_hw_send_frame(struct efct_hw *hw, struct fc_frame_header *hdr,\n\t\t   u8 sof, u8 eof, struct efc_dma *payload,\n\t\t   struct efct_hw_send_frame_context *ctx,\n\t\t   void (*callback)(void *arg, u8 *cqe, int status),\n\t\t   void *arg)\n{\n\tint rc;\n\tstruct efct_hw_wqe *wqe;\n\tu32 xri;\n\tstruct hw_wq *wq;\n\n\twqe = &ctx->wqe;\n\n\t \n\tctx->hw = hw;\n\n\t \n\tctx->wqcb = efct_hw_reqtag_alloc(hw, callback, arg);\n\tif (!ctx->wqcb) {\n\t\tefc_log_err(hw->os, \"can't allocate request tag\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\twq = hw->hw_wq[0];\n\n\t \n\txri = wq->send_frame_io->indicator;\n\n\t \n\trc = sli_send_frame_wqe(&hw->sli, wqe->wqebuf,\n\t\t\t\tsof, eof, (u32 *)hdr, payload, payload->len,\n\t\t\t\tEFCT_HW_SEND_FRAME_TIMEOUT, xri,\n\t\t\t\tctx->wqcb->instance_index);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"sli_send_frame_wqe failed: %d\\n\", rc);\n\t\treturn -EIO;\n\t}\n\n\t \n\trc = efct_hw_wq_write(wq, wqe);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"efct_hw_wq_write failed: %d\\n\", rc);\n\t\treturn -EIO;\n\t}\n\n\twq->use_count++;\n\n\treturn 0;\n}\n\nstatic int\nefct_hw_cb_link_stat(struct efct_hw *hw, int status,\n\t\t     u8 *mqe, void  *arg)\n{\n\tstruct sli4_cmd_read_link_stats *mbox_rsp;\n\tstruct efct_hw_link_stat_cb_arg *cb_arg = arg;\n\tstruct efct_hw_link_stat_counts counts[EFCT_HW_LINK_STAT_MAX];\n\tu32 num_counters, i;\n\tu32 mbox_rsp_flags = 0;\n\n\tmbox_rsp = (struct sli4_cmd_read_link_stats *)mqe;\n\tmbox_rsp_flags = le32_to_cpu(mbox_rsp->dw1_flags);\n\tnum_counters = (mbox_rsp_flags & SLI4_READ_LNKSTAT_GEC) ? 20 : 13;\n\tmemset(counts, 0, sizeof(struct efct_hw_link_stat_counts) *\n\t\t\t\t EFCT_HW_LINK_STAT_MAX);\n\n\t \n\tfor (i = 0; i < EFCT_HW_LINK_STAT_MAX; i++)\n\t\tcounts[i].overflow = (mbox_rsp_flags & (1 << (i + 2)));\n\n\tcounts[EFCT_HW_LINK_STAT_LINK_FAILURE_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->linkfail_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_LOSS_OF_SYNC_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->losssync_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_LOSS_OF_SIGNAL_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->losssignal_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_PRIMITIVE_SEQ_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->primseq_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_INVALID_XMIT_WORD_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->inval_txword_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_CRC_COUNT].counter =\n\t\tle32_to_cpu(mbox_rsp->crc_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_PRIMITIVE_SEQ_TIMEOUT_COUNT].counter =\n\t\tle32_to_cpu(mbox_rsp->primseq_eventtimeout_cnt);\n\tcounts[EFCT_HW_LINK_STAT_ELASTIC_BUFFER_OVERRUN_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->elastic_bufoverrun_errcnt);\n\tcounts[EFCT_HW_LINK_STAT_ARB_TIMEOUT_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->arbit_fc_al_timeout_cnt);\n\tcounts[EFCT_HW_LINK_STAT_ADVERTISED_RCV_B2B_CREDIT].counter =\n\t\t le32_to_cpu(mbox_rsp->adv_rx_buftor_to_buf_credit);\n\tcounts[EFCT_HW_LINK_STAT_CURR_RCV_B2B_CREDIT].counter =\n\t\t le32_to_cpu(mbox_rsp->curr_rx_buf_to_buf_credit);\n\tcounts[EFCT_HW_LINK_STAT_ADVERTISED_XMIT_B2B_CREDIT].counter =\n\t\t le32_to_cpu(mbox_rsp->adv_tx_buf_to_buf_credit);\n\tcounts[EFCT_HW_LINK_STAT_CURR_XMIT_B2B_CREDIT].counter =\n\t\t le32_to_cpu(mbox_rsp->curr_tx_buf_to_buf_credit);\n\tcounts[EFCT_HW_LINK_STAT_RCV_EOFA_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_eofa_cnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_EOFDTI_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_eofdti_cnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_EOFNI_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_eofni_cnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_SOFF_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_soff_cnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_DROPPED_NO_AER_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_dropped_no_aer_cnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_DROPPED_NO_RPI_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_dropped_no_avail_rpi_rescnt);\n\tcounts[EFCT_HW_LINK_STAT_RCV_DROPPED_NO_XRI_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->rx_dropped_no_avail_xri_rescnt);\n\n\tif (cb_arg) {\n\t\tif (cb_arg->cb) {\n\t\t\tif (status == 0 && le16_to_cpu(mbox_rsp->hdr.status))\n\t\t\t\tstatus = le16_to_cpu(mbox_rsp->hdr.status);\n\t\t\tcb_arg->cb(status, num_counters, counts, cb_arg->arg);\n\t\t}\n\n\t\tkfree(cb_arg);\n\t}\n\n\treturn 0;\n}\n\nint\nefct_hw_get_link_stats(struct efct_hw *hw, u8 req_ext_counters,\n\t\t       u8 clear_overflow_flags, u8 clear_all_counters,\n\t\t       void (*cb)(int status, u32 num_counters,\n\t\t\t\t  struct efct_hw_link_stat_counts *counters,\n\t\t\t\t  void *arg),\n\t\t       void *arg)\n{\n\tint rc = -EIO;\n\tstruct efct_hw_link_stat_cb_arg *cb_arg;\n\tu8 mbxdata[SLI4_BMBX_SIZE];\n\n\tcb_arg = kzalloc(sizeof(*cb_arg), GFP_ATOMIC);\n\tif (!cb_arg)\n\t\treturn -ENOMEM;\n\n\tcb_arg->cb = cb;\n\tcb_arg->arg = arg;\n\n\t \n\tif (!sli_cmd_read_link_stats(&hw->sli, mbxdata, req_ext_counters,\n\t\t\t\t    clear_overflow_flags, clear_all_counters))\n\t\trc = efct_hw_command(hw, mbxdata, EFCT_CMD_NOWAIT,\n\t\t\t\t     efct_hw_cb_link_stat, cb_arg);\n\n\tif (rc)\n\t\tkfree(cb_arg);\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_cb_host_stat(struct efct_hw *hw, int status, u8 *mqe, void  *arg)\n{\n\tstruct sli4_cmd_read_status *mbox_rsp =\n\t\t\t\t\t(struct sli4_cmd_read_status *)mqe;\n\tstruct efct_hw_host_stat_cb_arg *cb_arg = arg;\n\tstruct efct_hw_host_stat_counts counts[EFCT_HW_HOST_STAT_MAX];\n\tu32 num_counters = EFCT_HW_HOST_STAT_MAX;\n\n\tmemset(counts, 0, sizeof(struct efct_hw_host_stat_counts) *\n\t       EFCT_HW_HOST_STAT_MAX);\n\n\tcounts[EFCT_HW_HOST_STAT_TX_KBYTE_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->trans_kbyte_cnt);\n\tcounts[EFCT_HW_HOST_STAT_RX_KBYTE_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->recv_kbyte_cnt);\n\tcounts[EFCT_HW_HOST_STAT_TX_FRAME_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->trans_frame_cnt);\n\tcounts[EFCT_HW_HOST_STAT_RX_FRAME_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->recv_frame_cnt);\n\tcounts[EFCT_HW_HOST_STAT_TX_SEQ_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->trans_seq_cnt);\n\tcounts[EFCT_HW_HOST_STAT_RX_SEQ_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->recv_seq_cnt);\n\tcounts[EFCT_HW_HOST_STAT_TOTAL_EXCH_ORIG].counter =\n\t\t le32_to_cpu(mbox_rsp->tot_exchanges_orig);\n\tcounts[EFCT_HW_HOST_STAT_TOTAL_EXCH_RESP].counter =\n\t\t le32_to_cpu(mbox_rsp->tot_exchanges_resp);\n\tcounts[EFCT_HW_HOSY_STAT_RX_P_BSY_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->recv_p_bsy_cnt);\n\tcounts[EFCT_HW_HOST_STAT_RX_F_BSY_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->recv_f_bsy_cnt);\n\tcounts[EFCT_HW_HOST_STAT_DROP_FRM_DUE_TO_NO_RQ_BUF_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->no_rq_buf_dropped_frames_cnt);\n\tcounts[EFCT_HW_HOST_STAT_EMPTY_RQ_TIMEOUT_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->empty_rq_timeout_cnt);\n\tcounts[EFCT_HW_HOST_STAT_DROP_FRM_DUE_TO_NO_XRI_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->no_xri_dropped_frames_cnt);\n\tcounts[EFCT_HW_HOST_STAT_EMPTY_XRI_POOL_COUNT].counter =\n\t\t le32_to_cpu(mbox_rsp->empty_xri_pool_cnt);\n\n\tif (cb_arg) {\n\t\tif (cb_arg->cb) {\n\t\t\tif (status == 0 && le16_to_cpu(mbox_rsp->hdr.status))\n\t\t\t\tstatus = le16_to_cpu(mbox_rsp->hdr.status);\n\t\t\tcb_arg->cb(status, num_counters, counts, cb_arg->arg);\n\t\t}\n\n\t\tkfree(cb_arg);\n\t}\n\n\treturn 0;\n}\n\nint\nefct_hw_get_host_stats(struct efct_hw *hw, u8 cc,\n\t\t       void (*cb)(int status, u32 num_counters,\n\t\t\t\t  struct efct_hw_host_stat_counts *counters,\n\t\t\t\t  void *arg),\n\t\t       void *arg)\n{\n\tint rc = -EIO;\n\tstruct efct_hw_host_stat_cb_arg *cb_arg;\n\tu8 mbxdata[SLI4_BMBX_SIZE];\n\n\tcb_arg = kmalloc(sizeof(*cb_arg), GFP_ATOMIC);\n\tif (!cb_arg)\n\t\treturn -ENOMEM;\n\n\tcb_arg->cb = cb;\n\tcb_arg->arg = arg;\n\n\t  \n\tif (!sli_cmd_read_status(&hw->sli, mbxdata, cc))\n\t\trc = efct_hw_command(hw, mbxdata, EFCT_CMD_NOWAIT,\n\t\t\t\t     efct_hw_cb_host_stat, cb_arg);\n\n\tif (rc) {\n\t\tefc_log_debug(hw->os, \"READ_HOST_STATS failed\\n\");\n\t\tkfree(cb_arg);\n\t}\n\n\treturn rc;\n}\n\nstruct efct_hw_async_call_ctx {\n\tefct_hw_async_cb_t callback;\n\tvoid *arg;\n\tu8 cmd[SLI4_BMBX_SIZE];\n};\n\nstatic void\nefct_hw_async_cb(struct efct_hw *hw, int status, u8 *mqe, void *arg)\n{\n\tstruct efct_hw_async_call_ctx *ctx = arg;\n\n\tif (ctx) {\n\t\tif (ctx->callback)\n\t\t\t(*ctx->callback)(hw, status, mqe, ctx->arg);\n\n\t\tkfree(ctx);\n\t}\n}\n\nint\nefct_hw_async_call(struct efct_hw *hw, efct_hw_async_cb_t callback, void *arg)\n{\n\tstruct efct_hw_async_call_ctx *ctx;\n\tint rc;\n\n\t \n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->callback = callback;\n\tctx->arg = arg;\n\n\t \n\tif (sli_cmd_common_nop(&hw->sli, ctx->cmd, 0)) {\n\t\tefc_log_err(hw->os, \"COMMON_NOP format failure\\n\");\n\t\tkfree(ctx);\n\t\treturn -EIO;\n\t}\n\n\trc = efct_hw_command(hw, ctx->cmd, EFCT_CMD_NOWAIT, efct_hw_async_cb,\n\t\t\t     ctx);\n\tif (rc) {\n\t\tefc_log_err(hw->os, \"COMMON_NOP command failure, rc=%d\\n\", rc);\n\t\tkfree(ctx);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic int\nefct_hw_cb_fw_write(struct efct_hw *hw, int status, u8 *mqe, void  *arg)\n{\n\tstruct sli4_cmd_sli_config *mbox_rsp =\n\t\t\t\t\t(struct sli4_cmd_sli_config *)mqe;\n\tstruct sli4_rsp_cmn_write_object *wr_obj_rsp;\n\tstruct efct_hw_fw_wr_cb_arg *cb_arg = arg;\n\tu32 bytes_written;\n\tu16 mbox_status;\n\tu32 change_status;\n\n\twr_obj_rsp = (struct sli4_rsp_cmn_write_object *)\n\t\t      &mbox_rsp->payload.embed;\n\tbytes_written = le32_to_cpu(wr_obj_rsp->actual_write_length);\n\tmbox_status = le16_to_cpu(mbox_rsp->hdr.status);\n\tchange_status = (le32_to_cpu(wr_obj_rsp->change_status_dword) &\n\t\t\t RSP_CHANGE_STATUS);\n\n\tif (cb_arg) {\n\t\tif (cb_arg->cb) {\n\t\t\tif (!status && mbox_status)\n\t\t\t\tstatus = mbox_status;\n\t\t\tcb_arg->cb(status, bytes_written, change_status,\n\t\t\t\t   cb_arg->arg);\n\t\t}\n\n\t\tkfree(cb_arg);\n\t}\n\n\treturn 0;\n}\n\nint\nefct_hw_firmware_write(struct efct_hw *hw, struct efc_dma *dma, u32 size,\n\t\t       u32 offset, int last,\n\t\t       void (*cb)(int status, u32 bytes_written,\n\t\t\t\t   u32 change_status, void *arg),\n\t\t       void *arg)\n{\n\tint rc = -EIO;\n\tu8 mbxdata[SLI4_BMBX_SIZE];\n\tstruct efct_hw_fw_wr_cb_arg *cb_arg;\n\tint noc = 0;\n\n\tcb_arg = kzalloc(sizeof(*cb_arg), GFP_KERNEL);\n\tif (!cb_arg)\n\t\treturn -ENOMEM;\n\n\tcb_arg->cb = cb;\n\tcb_arg->arg = arg;\n\n\t \n\tif (!sli_cmd_common_write_object(&hw->sli, mbxdata,\n\t\t\t\t\t noc, last, size, offset, \"/prg/\",\n\t\t\t\t\t dma))\n\t\trc = efct_hw_command(hw, mbxdata, EFCT_CMD_NOWAIT,\n\t\t\t\t     efct_hw_cb_fw_write, cb_arg);\n\n\tif (rc != 0) {\n\t\tefc_log_debug(hw->os, \"COMMON_WRITE_OBJECT failed\\n\");\n\t\tkfree(cb_arg);\n\t}\n\n\treturn rc;\n}\n\nstatic int\nefct_hw_cb_port_control(struct efct_hw *hw, int status, u8 *mqe,\n\t\t\tvoid  *arg)\n{\n\treturn 0;\n}\n\nint\nefct_hw_port_control(struct efct_hw *hw, enum efct_hw_port ctrl,\n\t\t     uintptr_t value,\n\t\t     void (*cb)(int status, uintptr_t value, void *arg),\n\t\t     void *arg)\n{\n\tint rc = -EIO;\n\tu8 link[SLI4_BMBX_SIZE];\n\tu32 speed = 0;\n\tu8 reset_alpa = 0;\n\n\tswitch (ctrl) {\n\tcase EFCT_HW_PORT_INIT:\n\t\tif (!sli_cmd_config_link(&hw->sli, link))\n\t\t\trc = efct_hw_command(hw, link, EFCT_CMD_NOWAIT,\n\t\t\t\t\t     efct_hw_cb_port_control, NULL);\n\n\t\tif (rc != 0) {\n\t\t\tefc_log_err(hw->os, \"CONFIG_LINK failed\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tspeed = hw->config.speed;\n\t\treset_alpa = (u8)(value & 0xff);\n\n\t\trc = -EIO;\n\t\tif (!sli_cmd_init_link(&hw->sli, link, speed, reset_alpa))\n\t\t\trc = efct_hw_command(hw, link, EFCT_CMD_NOWAIT,\n\t\t\t\t\t     efct_hw_cb_port_control, NULL);\n\t\t \n\t\tif (rc)\n\t\t\tefc_log_err(hw->os, \"INIT_LINK failed\\n\");\n\t\tbreak;\n\n\tcase EFCT_HW_PORT_SHUTDOWN:\n\t\tif (!sli_cmd_down_link(&hw->sli, link))\n\t\t\trc = efct_hw_command(hw, link, EFCT_CMD_NOWAIT,\n\t\t\t\t\t     efct_hw_cb_port_control, NULL);\n\t\t \n\t\tif (rc)\n\t\t\tefc_log_err(hw->os, \"DOWN_LINK failed\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\tefc_log_debug(hw->os, \"unhandled control %#x\\n\", ctrl);\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nvoid\nefct_hw_teardown(struct efct_hw *hw)\n{\n\tu32 i = 0;\n\tu32 destroy_queues;\n\tu32 free_memory;\n\tstruct efc_dma *dma;\n\tstruct efct *efct = hw->os;\n\n\tdestroy_queues = (hw->state == EFCT_HW_STATE_ACTIVE);\n\tfree_memory = (hw->state != EFCT_HW_STATE_UNINITIALIZED);\n\n\t \n\tif (hw->sliport_healthcheck) {\n\t\thw->sliport_healthcheck = 0;\n\t\tefct_hw_config_sli_port_health_check(hw, 0, 0);\n\t}\n\n\tif (hw->state != EFCT_HW_STATE_QUEUES_ALLOCATED) {\n\t\thw->state = EFCT_HW_STATE_TEARDOWN_IN_PROGRESS;\n\n\t\tefct_hw_flush(hw);\n\n\t\tif (list_empty(&hw->cmd_head))\n\t\t\tefc_log_debug(hw->os,\n\t\t\t\t      \"All commands completed on MQ queue\\n\");\n\t\telse\n\t\t\tefc_log_debug(hw->os,\n\t\t\t\t      \"Some cmds still pending on MQ queue\\n\");\n\n\t\t \n\t\tefct_hw_command_cancel(hw);\n\t} else {\n\t\thw->state = EFCT_HW_STATE_TEARDOWN_IN_PROGRESS;\n\t}\n\n\tdma_free_coherent(&efct->pci->dev,\n\t\t\t  hw->rnode_mem.size, hw->rnode_mem.virt,\n\t\t\t  hw->rnode_mem.phys);\n\tmemset(&hw->rnode_mem, 0, sizeof(struct efc_dma));\n\n\tif (hw->io) {\n\t\tfor (i = 0; i < hw->config.n_io; i++) {\n\t\t\tif (hw->io[i] && hw->io[i]->sgl &&\n\t\t\t    hw->io[i]->sgl->virt) {\n\t\t\t\tdma_free_coherent(&efct->pci->dev,\n\t\t\t\t\t\t  hw->io[i]->sgl->size,\n\t\t\t\t\t\t  hw->io[i]->sgl->virt,\n\t\t\t\t\t\t  hw->io[i]->sgl->phys);\n\t\t\t}\n\t\t\tkfree(hw->io[i]);\n\t\t\thw->io[i] = NULL;\n\t\t}\n\t\tkfree(hw->io);\n\t\thw->io = NULL;\n\t\tkfree(hw->wqe_buffs);\n\t\thw->wqe_buffs = NULL;\n\t}\n\n\tdma = &hw->xfer_rdy;\n\tdma_free_coherent(&efct->pci->dev,\n\t\t\t  dma->size, dma->virt, dma->phys);\n\tmemset(dma, 0, sizeof(struct efc_dma));\n\n\tdma = &hw->loop_map;\n\tdma_free_coherent(&efct->pci->dev,\n\t\t\t  dma->size, dma->virt, dma->phys);\n\tmemset(dma, 0, sizeof(struct efc_dma));\n\n\tfor (i = 0; i < hw->wq_count; i++)\n\t\tsli_queue_free(&hw->sli, &hw->wq[i], destroy_queues,\n\t\t\t       free_memory);\n\n\tfor (i = 0; i < hw->rq_count; i++)\n\t\tsli_queue_free(&hw->sli, &hw->rq[i], destroy_queues,\n\t\t\t       free_memory);\n\n\tfor (i = 0; i < hw->mq_count; i++)\n\t\tsli_queue_free(&hw->sli, &hw->mq[i], destroy_queues,\n\t\t\t       free_memory);\n\n\tfor (i = 0; i < hw->cq_count; i++)\n\t\tsli_queue_free(&hw->sli, &hw->cq[i], destroy_queues,\n\t\t\t       free_memory);\n\n\tfor (i = 0; i < hw->eq_count; i++)\n\t\tsli_queue_free(&hw->sli, &hw->eq[i], destroy_queues,\n\t\t\t       free_memory);\n\n\t \n\tefct_hw_rx_free(hw);\n\n\tefct_hw_queue_teardown(hw);\n\n\tkfree(hw->wq_cpu_array);\n\n\tsli_teardown(&hw->sli);\n\n\t \n\thw->state = EFCT_HW_STATE_UNINITIALIZED;\n\n\t \n\tkfree(hw->seq_pool);\n\thw->seq_pool = NULL;\n\n\t \n\tefct_hw_reqtag_pool_free(hw);\n\n\tmempool_destroy(hw->cmd_ctx_pool);\n\tmempool_destroy(hw->mbox_rqst_pool);\n\n\t \n\thw->hw_setup_called = false;\n}\n\nstatic int\nefct_hw_sli_reset(struct efct_hw *hw, enum efct_hw_reset reset,\n\t\t  enum efct_hw_state prev_state)\n{\n\tint rc = 0;\n\n\tswitch (reset) {\n\tcase EFCT_HW_RESET_FUNCTION:\n\t\tefc_log_debug(hw->os, \"issuing function level reset\\n\");\n\t\tif (sli_reset(&hw->sli)) {\n\t\t\tefc_log_err(hw->os, \"sli_reset failed\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\tcase EFCT_HW_RESET_FIRMWARE:\n\t\tefc_log_debug(hw->os, \"issuing firmware reset\\n\");\n\t\tif (sli_fw_reset(&hw->sli)) {\n\t\t\tefc_log_err(hw->os, \"sli_soft_reset failed\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\t \n\t\tefc_log_debug(hw->os, \"issuing function level reset\\n\");\n\t\tif (sli_reset(&hw->sli)) {\n\t\t\tefc_log_err(hw->os, \"sli_reset failed\\n\");\n\t\t\trc = -EIO;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tefc_log_err(hw->os, \"unknown type - no reset performed\\n\");\n\t\thw->state = prev_state;\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nint\nefct_hw_reset(struct efct_hw *hw, enum efct_hw_reset reset)\n{\n\tint rc = 0;\n\tenum efct_hw_state prev_state = hw->state;\n\n\tif (hw->state != EFCT_HW_STATE_ACTIVE)\n\t\tefc_log_debug(hw->os,\n\t\t\t      \"HW state %d is not active\\n\", hw->state);\n\n\thw->state = EFCT_HW_STATE_RESET_IN_PROGRESS;\n\n\t \n\tif (prev_state == EFCT_HW_STATE_RESET_IN_PROGRESS ||\n\t    prev_state == EFCT_HW_STATE_TEARDOWN_IN_PROGRESS)\n\t\treturn efct_hw_sli_reset(hw, reset, prev_state);\n\n\tif (prev_state != EFCT_HW_STATE_UNINITIALIZED) {\n\t\tefct_hw_flush(hw);\n\n\t\tif (list_empty(&hw->cmd_head))\n\t\t\tefc_log_debug(hw->os,\n\t\t\t\t      \"All commands completed on MQ queue\\n\");\n\t\telse\n\t\t\tefc_log_err(hw->os,\n\t\t\t\t    \"Some commands still pending on MQ queue\\n\");\n\t}\n\n\t \n\trc = efct_hw_sli_reset(hw, reset, prev_state);\n\tif (rc == -EINVAL)\n\t\treturn -EIO;\n\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}