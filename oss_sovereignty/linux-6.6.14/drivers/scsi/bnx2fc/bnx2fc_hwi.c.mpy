{
  "module_name": "bnx2fc_hwi.c",
  "hash_id": "f24e9577312265694a00e39c0261cb18c0ccd629b75b14f776f15e3726be7c1f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/bnx2fc/bnx2fc_hwi.c",
  "human_readable_source": " \n\n#include \"bnx2fc.h\"\n\nDECLARE_PER_CPU(struct bnx2fc_percpu_s, bnx2fc_percpu);\n\nstatic void bnx2fc_fastpath_notification(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *new_cqe_kcqe);\nstatic void bnx2fc_process_ofld_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *ofld_kcqe);\nstatic void bnx2fc_process_enable_conn_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\t\tstruct fcoe_kcqe *ofld_kcqe);\nstatic void bnx2fc_init_failure(struct bnx2fc_hba *hba, u32 err_code);\nstatic void bnx2fc_process_conn_destroy_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *destroy_kcqe);\n\nint bnx2fc_send_stat_req(struct bnx2fc_hba *hba)\n{\n\tstruct fcoe_kwqe_stat stat_req;\n\tstruct kwqe *kwqe_arr[2];\n\tint num_kwqes = 1;\n\tint rc = 0;\n\n\tmemset(&stat_req, 0x00, sizeof(struct fcoe_kwqe_stat));\n\tstat_req.hdr.op_code = FCOE_KWQE_OPCODE_STAT;\n\tstat_req.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tstat_req.stat_params_addr_lo = (u32) hba->stats_buf_dma;\n\tstat_req.stat_params_addr_hi = (u32) ((u64)hba->stats_buf_dma >> 32);\n\n\tkwqe_arr[0] = (struct kwqe *) &stat_req;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\n\treturn rc;\n}\n\n \nint bnx2fc_send_fw_fcoe_init_msg(struct bnx2fc_hba *hba)\n{\n\tstruct fcoe_kwqe_init1 fcoe_init1;\n\tstruct fcoe_kwqe_init2 fcoe_init2;\n\tstruct fcoe_kwqe_init3 fcoe_init3;\n\tstruct kwqe *kwqe_arr[3];\n\tint num_kwqes = 3;\n\tint rc = 0;\n\n\tif (!hba->cnic) {\n\t\tprintk(KERN_ERR PFX \"hba->cnic NULL during fcoe fw init\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tmemset(&fcoe_init1, 0x00, sizeof(struct fcoe_kwqe_init1));\n\tfcoe_init1.hdr.op_code = FCOE_KWQE_OPCODE_INIT1;\n\tfcoe_init1.hdr.flags = (FCOE_KWQE_LAYER_CODE <<\n\t\t\t\t\tFCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tfcoe_init1.num_tasks = hba->max_tasks;\n\tfcoe_init1.sq_num_wqes = BNX2FC_SQ_WQES_MAX;\n\tfcoe_init1.rq_num_wqes = BNX2FC_RQ_WQES_MAX;\n\tfcoe_init1.rq_buffer_log_size = BNX2FC_RQ_BUF_LOG_SZ;\n\tfcoe_init1.cq_num_wqes = BNX2FC_CQ_WQES_MAX;\n\tfcoe_init1.dummy_buffer_addr_lo = (u32) hba->dummy_buf_dma;\n\tfcoe_init1.dummy_buffer_addr_hi = (u32) ((u64)hba->dummy_buf_dma >> 32);\n\tfcoe_init1.task_list_pbl_addr_lo = (u32) hba->task_ctx_bd_dma;\n\tfcoe_init1.task_list_pbl_addr_hi =\n\t\t\t\t(u32) ((u64) hba->task_ctx_bd_dma >> 32);\n\tfcoe_init1.mtu = BNX2FC_MINI_JUMBO_MTU;\n\n\tfcoe_init1.flags = (PAGE_SHIFT <<\n\t\t\t\tFCOE_KWQE_INIT1_LOG_PAGE_SIZE_SHIFT);\n\n\tfcoe_init1.num_sessions_log = BNX2FC_NUM_MAX_SESS_LOG;\n\n\t \n\tmemset(&fcoe_init2, 0x00, sizeof(struct fcoe_kwqe_init2));\n\tfcoe_init2.hdr.op_code = FCOE_KWQE_OPCODE_INIT2;\n\tfcoe_init2.hdr.flags = (FCOE_KWQE_LAYER_CODE <<\n\t\t\t\t\tFCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tfcoe_init2.hsi_major_version = FCOE_HSI_MAJOR_VERSION;\n\tfcoe_init2.hsi_minor_version = FCOE_HSI_MINOR_VERSION;\n\n\n\tfcoe_init2.hash_tbl_pbl_addr_lo = (u32) hba->hash_tbl_pbl_dma;\n\tfcoe_init2.hash_tbl_pbl_addr_hi = (u32)\n\t\t\t\t\t   ((u64) hba->hash_tbl_pbl_dma >> 32);\n\n\tfcoe_init2.t2_hash_tbl_addr_lo = (u32) hba->t2_hash_tbl_dma;\n\tfcoe_init2.t2_hash_tbl_addr_hi = (u32)\n\t\t\t\t\t  ((u64) hba->t2_hash_tbl_dma >> 32);\n\n\tfcoe_init2.t2_ptr_hash_tbl_addr_lo = (u32) hba->t2_hash_tbl_ptr_dma;\n\tfcoe_init2.t2_ptr_hash_tbl_addr_hi = (u32)\n\t\t\t\t\t((u64) hba->t2_hash_tbl_ptr_dma >> 32);\n\n\tfcoe_init2.free_list_count = BNX2FC_NUM_MAX_SESS;\n\n\t \n\tmemset(&fcoe_init3, 0x00, sizeof(struct fcoe_kwqe_init3));\n\tfcoe_init3.hdr.op_code = FCOE_KWQE_OPCODE_INIT3;\n\tfcoe_init3.hdr.flags = (FCOE_KWQE_LAYER_CODE <<\n\t\t\t\t\tFCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\tfcoe_init3.error_bit_map_lo = 0xffffffff;\n\tfcoe_init3.error_bit_map_hi = 0xffffffff;\n\n\t \n\tfcoe_init3.perf_config = 3;\n\n\tkwqe_arr[0] = (struct kwqe *) &fcoe_init1;\n\tkwqe_arr[1] = (struct kwqe *) &fcoe_init2;\n\tkwqe_arr[2] = (struct kwqe *) &fcoe_init3;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\n\treturn rc;\n}\nint bnx2fc_send_fw_fcoe_destroy_msg(struct bnx2fc_hba *hba)\n{\n\tstruct fcoe_kwqe_destroy fcoe_destroy;\n\tstruct kwqe *kwqe_arr[2];\n\tint num_kwqes = 1;\n\tint rc = -1;\n\n\t \n\tmemset(&fcoe_destroy, 0x00, sizeof(struct fcoe_kwqe_destroy));\n\tfcoe_destroy.hdr.op_code = FCOE_KWQE_OPCODE_DESTROY;\n\tfcoe_destroy.hdr.flags = (FCOE_KWQE_LAYER_CODE <<\n\t\t\t\t\tFCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\tkwqe_arr[0] = (struct kwqe *) &fcoe_destroy;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\treturn rc;\n}\n\n \nint bnx2fc_send_session_ofld_req(struct fcoe_port *port,\n\t\t\t\t\tstruct bnx2fc_rport *tgt)\n{\n\tstruct fc_lport *lport = port->lport;\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct fcoe_ctlr *ctlr = bnx2fc_to_ctlr(interface);\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tstruct kwqe *kwqe_arr[4];\n\tstruct fcoe_kwqe_conn_offload1 ofld_req1;\n\tstruct fcoe_kwqe_conn_offload2 ofld_req2;\n\tstruct fcoe_kwqe_conn_offload3 ofld_req3;\n\tstruct fcoe_kwqe_conn_offload4 ofld_req4;\n\tstruct fc_rport_priv *rdata = tgt->rdata;\n\tstruct fc_rport *rport = tgt->rport;\n\tint num_kwqes = 4;\n\tu32 port_id;\n\tint rc = 0;\n\tu16 conn_id;\n\n\t \n\tmemset(&ofld_req1, 0x00, sizeof(struct fcoe_kwqe_conn_offload1));\n\n\tofld_req1.hdr.op_code = FCOE_KWQE_OPCODE_OFFLOAD_CONN1;\n\tofld_req1.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\n\tconn_id = (u16)tgt->fcoe_conn_id;\n\tofld_req1.fcoe_conn_id = conn_id;\n\n\n\tofld_req1.sq_addr_lo = (u32) tgt->sq_dma;\n\tofld_req1.sq_addr_hi = (u32)((u64) tgt->sq_dma >> 32);\n\n\tofld_req1.rq_pbl_addr_lo = (u32) tgt->rq_pbl_dma;\n\tofld_req1.rq_pbl_addr_hi = (u32)((u64) tgt->rq_pbl_dma >> 32);\n\n\tofld_req1.rq_first_pbe_addr_lo = (u32) tgt->rq_dma;\n\tofld_req1.rq_first_pbe_addr_hi =\n\t\t\t\t(u32)((u64) tgt->rq_dma >> 32);\n\n\tofld_req1.rq_prod = 0x8000;\n\n\t \n\tmemset(&ofld_req2, 0x00, sizeof(struct fcoe_kwqe_conn_offload2));\n\n\tofld_req2.hdr.op_code = FCOE_KWQE_OPCODE_OFFLOAD_CONN2;\n\tofld_req2.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tofld_req2.tx_max_fc_pay_len = rdata->maxframe_size;\n\n\tofld_req2.cq_addr_lo = (u32) tgt->cq_dma;\n\tofld_req2.cq_addr_hi = (u32)((u64)tgt->cq_dma >> 32);\n\n\tofld_req2.xferq_addr_lo = (u32) tgt->xferq_dma;\n\tofld_req2.xferq_addr_hi = (u32)((u64)tgt->xferq_dma >> 32);\n\n\tofld_req2.conn_db_addr_lo = (u32)tgt->conn_db_dma;\n\tofld_req2.conn_db_addr_hi = (u32)((u64)tgt->conn_db_dma >> 32);\n\n\t \n\tmemset(&ofld_req3, 0x00, sizeof(struct fcoe_kwqe_conn_offload3));\n\n\tofld_req3.hdr.op_code = FCOE_KWQE_OPCODE_OFFLOAD_CONN3;\n\tofld_req3.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tofld_req3.vlan_tag = interface->vlan_id <<\n\t\t\t\tFCOE_KWQE_CONN_OFFLOAD3_VLAN_ID_SHIFT;\n\tofld_req3.vlan_tag |= 3 << FCOE_KWQE_CONN_OFFLOAD3_PRIORITY_SHIFT;\n\n\tport_id = fc_host_port_id(lport->host);\n\tif (port_id == 0) {\n\t\tBNX2FC_HBA_DBG(lport, \"ofld_req: port_id = 0, link down?\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\ttgt->sid = port_id;\n\tofld_req3.s_id[0] = (port_id & 0x000000FF);\n\tofld_req3.s_id[1] = (port_id & 0x0000FF00) >> 8;\n\tofld_req3.s_id[2] = (port_id & 0x00FF0000) >> 16;\n\n\tport_id = rport->port_id;\n\tofld_req3.d_id[0] = (port_id & 0x000000FF);\n\tofld_req3.d_id[1] = (port_id & 0x0000FF00) >> 8;\n\tofld_req3.d_id[2] = (port_id & 0x00FF0000) >> 16;\n\n\tofld_req3.tx_total_conc_seqs = rdata->max_seq;\n\n\tofld_req3.tx_max_conc_seqs_c3 = rdata->max_seq;\n\tofld_req3.rx_max_fc_pay_len  = lport->mfs;\n\n\tofld_req3.rx_total_conc_seqs = BNX2FC_MAX_SEQS;\n\tofld_req3.rx_max_conc_seqs_c3 = BNX2FC_MAX_SEQS;\n\tofld_req3.rx_open_seqs_exch_c3 = 1;\n\n\tofld_req3.confq_first_pbe_addr_lo = tgt->confq_dma;\n\tofld_req3.confq_first_pbe_addr_hi = (u32)((u64) tgt->confq_dma >> 32);\n\n\t \n\tofld_req3.flags = 0;\n\t \n\t \n\tofld_req3.flags |= (((rdata->sp_features & FC_SP_FT_EDTR) ? 1 : 0) <<\n\t\t\t     FCOE_KWQE_CONN_OFFLOAD3_B_E_D_TOV_RES_SHIFT);\n\n\tofld_req3.flags |= (((rdata->sp_features & FC_SP_FT_SEQC) ? 1 : 0) <<\n\t\t\t     FCOE_KWQE_CONN_OFFLOAD3_B_CONT_INCR_SEQ_CNT_SHIFT);\n\n\t \n\tif (tgt->dev_type == TYPE_TAPE) {\n\t\tofld_req3.flags |= 1 <<\n\t\t\t\t    FCOE_KWQE_CONN_OFFLOAD3_B_CONF_REQ_SHIFT;\n\t\tofld_req3.flags |= (((rdata->flags & FC_RP_FLAGS_REC_SUPPORTED)\n\t\t\t\t    ? 1 : 0) <<\n\t\t\t\t    FCOE_KWQE_CONN_OFFLOAD3_B_REC_VALID_SHIFT);\n\t}\n\n\t \n\tofld_req3.flags |= (interface->vlan_enabled <<\n\t\t\t    FCOE_KWQE_CONN_OFFLOAD3_B_VLAN_FLAG_SHIFT);\n\n\t \n\n\n\t \n\tmemset(&ofld_req4, 0x00, sizeof(struct fcoe_kwqe_conn_offload4));\n\tofld_req4.hdr.op_code = FCOE_KWQE_OPCODE_OFFLOAD_CONN4;\n\tofld_req4.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tofld_req4.e_d_tov_timer_val = lport->e_d_tov / 20;\n\n\n\tofld_req4.src_mac_addr_lo[0] =  port->data_src_addr[5];\n\t\t\t\t\t\t\t \n\tofld_req4.src_mac_addr_lo[1] =  port->data_src_addr[4];\n\tofld_req4.src_mac_addr_mid[0] =  port->data_src_addr[3];\n\tofld_req4.src_mac_addr_mid[1] =  port->data_src_addr[2];\n\tofld_req4.src_mac_addr_hi[0] =  port->data_src_addr[1];\n\tofld_req4.src_mac_addr_hi[1] =  port->data_src_addr[0];\n\tofld_req4.dst_mac_addr_lo[0] =  ctlr->dest_addr[5];\n\t\t\t\t\t\t\t \n\tofld_req4.dst_mac_addr_lo[1] = ctlr->dest_addr[4];\n\tofld_req4.dst_mac_addr_mid[0] = ctlr->dest_addr[3];\n\tofld_req4.dst_mac_addr_mid[1] = ctlr->dest_addr[2];\n\tofld_req4.dst_mac_addr_hi[0] = ctlr->dest_addr[1];\n\tofld_req4.dst_mac_addr_hi[1] = ctlr->dest_addr[0];\n\n\tofld_req4.lcq_addr_lo = (u32) tgt->lcq_dma;\n\tofld_req4.lcq_addr_hi = (u32)((u64) tgt->lcq_dma >> 32);\n\n\tofld_req4.confq_pbl_base_addr_lo = (u32) tgt->confq_pbl_dma;\n\tofld_req4.confq_pbl_base_addr_hi =\n\t\t\t\t\t(u32)((u64) tgt->confq_pbl_dma >> 32);\n\n\tkwqe_arr[0] = (struct kwqe *) &ofld_req1;\n\tkwqe_arr[1] = (struct kwqe *) &ofld_req2;\n\tkwqe_arr[2] = (struct kwqe *) &ofld_req3;\n\tkwqe_arr[3] = (struct kwqe *) &ofld_req4;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\n\treturn rc;\n}\n\n \nint bnx2fc_send_session_enable_req(struct fcoe_port *port,\n\t\t\t\t\tstruct bnx2fc_rport *tgt)\n{\n\tstruct kwqe *kwqe_arr[2];\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct fcoe_ctlr *ctlr = bnx2fc_to_ctlr(interface);\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tstruct fcoe_kwqe_conn_enable_disable enbl_req;\n\tstruct fc_lport *lport = port->lport;\n\tstruct fc_rport *rport = tgt->rport;\n\tint num_kwqes = 1;\n\tint rc = 0;\n\tu32 port_id;\n\n\tmemset(&enbl_req, 0x00,\n\t       sizeof(struct fcoe_kwqe_conn_enable_disable));\n\tenbl_req.hdr.op_code = FCOE_KWQE_OPCODE_ENABLE_CONN;\n\tenbl_req.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tenbl_req.src_mac_addr_lo[0] =  port->data_src_addr[5];\n\t\t\t\t\t\t\t \n\tenbl_req.src_mac_addr_lo[1] =  port->data_src_addr[4];\n\tenbl_req.src_mac_addr_mid[0] =  port->data_src_addr[3];\n\tenbl_req.src_mac_addr_mid[1] =  port->data_src_addr[2];\n\tenbl_req.src_mac_addr_hi[0] =  port->data_src_addr[1];\n\tenbl_req.src_mac_addr_hi[1] =  port->data_src_addr[0];\n\tmemcpy(tgt->src_addr, port->data_src_addr, ETH_ALEN);\n\n\tenbl_req.dst_mac_addr_lo[0] =  ctlr->dest_addr[5];\n\tenbl_req.dst_mac_addr_lo[1] =  ctlr->dest_addr[4];\n\tenbl_req.dst_mac_addr_mid[0] = ctlr->dest_addr[3];\n\tenbl_req.dst_mac_addr_mid[1] = ctlr->dest_addr[2];\n\tenbl_req.dst_mac_addr_hi[0] = ctlr->dest_addr[1];\n\tenbl_req.dst_mac_addr_hi[1] = ctlr->dest_addr[0];\n\n\tport_id = fc_host_port_id(lport->host);\n\tif (port_id != tgt->sid) {\n\t\tprintk(KERN_ERR PFX \"WARN: enable_req port_id = 0x%x,\"\n\t\t\t\t\"sid = 0x%x\\n\", port_id, tgt->sid);\n\t\tport_id = tgt->sid;\n\t}\n\tenbl_req.s_id[0] = (port_id & 0x000000FF);\n\tenbl_req.s_id[1] = (port_id & 0x0000FF00) >> 8;\n\tenbl_req.s_id[2] = (port_id & 0x00FF0000) >> 16;\n\n\tport_id = rport->port_id;\n\tenbl_req.d_id[0] = (port_id & 0x000000FF);\n\tenbl_req.d_id[1] = (port_id & 0x0000FF00) >> 8;\n\tenbl_req.d_id[2] = (port_id & 0x00FF0000) >> 16;\n\tenbl_req.vlan_tag = interface->vlan_id <<\n\t\t\t\tFCOE_KWQE_CONN_ENABLE_DISABLE_VLAN_ID_SHIFT;\n\tenbl_req.vlan_tag |= 3 << FCOE_KWQE_CONN_ENABLE_DISABLE_PRIORITY_SHIFT;\n\tenbl_req.vlan_flag = interface->vlan_enabled;\n\tenbl_req.context_id = tgt->context_id;\n\tenbl_req.conn_id = tgt->fcoe_conn_id;\n\n\tkwqe_arr[0] = (struct kwqe *) &enbl_req;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\treturn rc;\n}\n\n \nint bnx2fc_send_session_disable_req(struct fcoe_port *port,\n\t\t\t\t    struct bnx2fc_rport *tgt)\n{\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct fcoe_ctlr *ctlr = bnx2fc_to_ctlr(interface);\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tstruct fcoe_kwqe_conn_enable_disable disable_req;\n\tstruct kwqe *kwqe_arr[2];\n\tstruct fc_rport *rport = tgt->rport;\n\tint num_kwqes = 1;\n\tint rc = 0;\n\tu32 port_id;\n\n\tmemset(&disable_req, 0x00,\n\t       sizeof(struct fcoe_kwqe_conn_enable_disable));\n\tdisable_req.hdr.op_code = FCOE_KWQE_OPCODE_DISABLE_CONN;\n\tdisable_req.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tdisable_req.src_mac_addr_lo[0] =  tgt->src_addr[5];\n\tdisable_req.src_mac_addr_lo[1] =  tgt->src_addr[4];\n\tdisable_req.src_mac_addr_mid[0] =  tgt->src_addr[3];\n\tdisable_req.src_mac_addr_mid[1] =  tgt->src_addr[2];\n\tdisable_req.src_mac_addr_hi[0] =  tgt->src_addr[1];\n\tdisable_req.src_mac_addr_hi[1] =  tgt->src_addr[0];\n\n\tdisable_req.dst_mac_addr_lo[0] =  ctlr->dest_addr[5];\n\tdisable_req.dst_mac_addr_lo[1] =  ctlr->dest_addr[4];\n\tdisable_req.dst_mac_addr_mid[0] = ctlr->dest_addr[3];\n\tdisable_req.dst_mac_addr_mid[1] = ctlr->dest_addr[2];\n\tdisable_req.dst_mac_addr_hi[0] = ctlr->dest_addr[1];\n\tdisable_req.dst_mac_addr_hi[1] = ctlr->dest_addr[0];\n\n\tport_id = tgt->sid;\n\tdisable_req.s_id[0] = (port_id & 0x000000FF);\n\tdisable_req.s_id[1] = (port_id & 0x0000FF00) >> 8;\n\tdisable_req.s_id[2] = (port_id & 0x00FF0000) >> 16;\n\n\n\tport_id = rport->port_id;\n\tdisable_req.d_id[0] = (port_id & 0x000000FF);\n\tdisable_req.d_id[1] = (port_id & 0x0000FF00) >> 8;\n\tdisable_req.d_id[2] = (port_id & 0x00FF0000) >> 16;\n\tdisable_req.context_id = tgt->context_id;\n\tdisable_req.conn_id = tgt->fcoe_conn_id;\n\tdisable_req.vlan_tag = interface->vlan_id <<\n\t\t\t\tFCOE_KWQE_CONN_ENABLE_DISABLE_VLAN_ID_SHIFT;\n\tdisable_req.vlan_tag |=\n\t\t\t3 << FCOE_KWQE_CONN_ENABLE_DISABLE_PRIORITY_SHIFT;\n\tdisable_req.vlan_flag = interface->vlan_enabled;\n\n\tkwqe_arr[0] = (struct kwqe *) &disable_req;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\n\treturn rc;\n}\n\n \nint bnx2fc_send_session_destroy_req(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct bnx2fc_rport *tgt)\n{\n\tstruct fcoe_kwqe_conn_destroy destroy_req;\n\tstruct kwqe *kwqe_arr[2];\n\tint num_kwqes = 1;\n\tint rc = 0;\n\n\tmemset(&destroy_req, 0x00, sizeof(struct fcoe_kwqe_conn_destroy));\n\tdestroy_req.hdr.op_code = FCOE_KWQE_OPCODE_DESTROY_CONN;\n\tdestroy_req.hdr.flags =\n\t\t(FCOE_KWQE_LAYER_CODE << FCOE_KWQE_HEADER_LAYER_CODE_SHIFT);\n\n\tdestroy_req.context_id = tgt->context_id;\n\tdestroy_req.conn_id = tgt->fcoe_conn_id;\n\n\tkwqe_arr[0] = (struct kwqe *) &destroy_req;\n\n\tif (hba->cnic && hba->cnic->submit_kwqes)\n\t\trc = hba->cnic->submit_kwqes(hba->cnic, kwqe_arr, num_kwqes);\n\n\treturn rc;\n}\n\nstatic bool is_valid_lport(struct bnx2fc_hba *hba, struct fc_lport *lport)\n{\n\tstruct bnx2fc_lport *blport;\n\n\tspin_lock_bh(&hba->hba_lock);\n\tlist_for_each_entry(blport, &hba->vports, list) {\n\t\tif (blport->lport == lport) {\n\t\t\tspin_unlock_bh(&hba->hba_lock);\n\t\t\treturn true;\n\t\t}\n\t}\n\tspin_unlock_bh(&hba->hba_lock);\n\treturn false;\n\n}\n\n\nstatic void bnx2fc_unsol_els_work(struct work_struct *work)\n{\n\tstruct bnx2fc_unsol_els *unsol_els;\n\tstruct fc_lport *lport;\n\tstruct bnx2fc_hba *hba;\n\tstruct fc_frame *fp;\n\n\tunsol_els = container_of(work, struct bnx2fc_unsol_els, unsol_els_work);\n\tlport = unsol_els->lport;\n\tfp = unsol_els->fp;\n\thba = unsol_els->hba;\n\tif (is_valid_lport(hba, lport))\n\t\tfc_exch_recv(lport, fp);\n\tkfree(unsol_els);\n}\n\nvoid bnx2fc_process_l2_frame_compl(struct bnx2fc_rport *tgt,\n\t\t\t\t   unsigned char *buf,\n\t\t\t\t   u32 frame_len, u16 l2_oxid)\n{\n\tstruct fcoe_port *port = tgt->port;\n\tstruct fc_lport *lport = port->lport;\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct bnx2fc_unsol_els *unsol_els;\n\tstruct fc_frame_header *fh;\n\tstruct fc_frame *fp;\n\tstruct sk_buff *skb;\n\tu32 payload_len;\n\tu32 crc;\n\tu8 op;\n\n\n\tunsol_els = kzalloc(sizeof(*unsol_els), GFP_ATOMIC);\n\tif (!unsol_els) {\n\t\tBNX2FC_TGT_DBG(tgt, \"Unable to allocate unsol_work\\n\");\n\t\treturn;\n\t}\n\n\tBNX2FC_TGT_DBG(tgt, \"l2_frame_compl l2_oxid = 0x%x, frame_len = %d\\n\",\n\t\tl2_oxid, frame_len);\n\n\tpayload_len = frame_len - sizeof(struct fc_frame_header);\n\n\tfp = fc_frame_alloc(lport, payload_len);\n\tif (!fp) {\n\t\tprintk(KERN_ERR PFX \"fc_frame_alloc failure\\n\");\n\t\tkfree(unsol_els);\n\t\treturn;\n\t}\n\n\tfh = (struct fc_frame_header *) fc_frame_header_get(fp);\n\t \n\tmemcpy(fh, buf, frame_len);\n\n\tif (l2_oxid != FC_XID_UNKNOWN)\n\t\tfh->fh_ox_id = htons(l2_oxid);\n\n\tskb = fp_skb(fp);\n\n\tif ((fh->fh_r_ctl == FC_RCTL_ELS_REQ) ||\n\t    (fh->fh_r_ctl == FC_RCTL_ELS_REP)) {\n\n\t\tif (fh->fh_type == FC_TYPE_ELS) {\n\t\t\top = fc_frame_payload_op(fp);\n\t\t\tif ((op == ELS_TEST) ||\t(op == ELS_ESTC) ||\n\t\t\t    (op == ELS_FAN) || (op == ELS_CSU)) {\n\t\t\t\t \n\t\t\t\tprintk(KERN_ERR PFX \"dropping ELS 0x%x\\n\", op);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tkfree(unsol_els);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tcrc = fcoe_fc_crc(fp);\n\t\tfc_frame_init(fp);\n\t\tfr_dev(fp) = lport;\n\t\tfr_sof(fp) = FC_SOF_I3;\n\t\tfr_eof(fp) = FC_EOF_T;\n\t\tfr_crc(fp) = cpu_to_le32(~crc);\n\t\tunsol_els->lport = lport;\n\t\tunsol_els->hba = interface->hba;\n\t\tunsol_els->fp = fp;\n\t\tINIT_WORK(&unsol_els->unsol_els_work, bnx2fc_unsol_els_work);\n\t\tqueue_work(bnx2fc_wq, &unsol_els->unsol_els_work);\n\t} else {\n\t\tBNX2FC_HBA_DBG(lport, \"fh_r_ctl = 0x%x\\n\", fh->fh_r_ctl);\n\t\tkfree_skb(skb);\n\t\tkfree(unsol_els);\n\t}\n}\n\nstatic void bnx2fc_process_unsol_compl(struct bnx2fc_rport *tgt, u16 wqe)\n{\n\tu8 num_rq;\n\tstruct fcoe_err_report_entry *err_entry;\n\tunsigned char *rq_data;\n\tunsigned char *buf = NULL, *buf1;\n\tint i;\n\tu16 xid;\n\tu32 frame_len, len;\n\tstruct bnx2fc_cmd *io_req = NULL;\n\tstruct bnx2fc_interface *interface = tgt->port->priv;\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tint rc = 0;\n\tu64 err_warn_bit_map;\n\tu8 err_warn = 0xff;\n\n\n\tBNX2FC_TGT_DBG(tgt, \"Entered UNSOL COMPLETION wqe = 0x%x\\n\", wqe);\n\tswitch (wqe & FCOE_UNSOLICITED_CQE_SUBTYPE) {\n\tcase FCOE_UNSOLICITED_FRAME_CQE_TYPE:\n\t\tframe_len = (wqe & FCOE_UNSOLICITED_CQE_PKT_LEN) >>\n\t\t\t     FCOE_UNSOLICITED_CQE_PKT_LEN_SHIFT;\n\n\t\tnum_rq = (frame_len + BNX2FC_RQ_BUF_SZ - 1) / BNX2FC_RQ_BUF_SZ;\n\n\t\tspin_lock_bh(&tgt->tgt_lock);\n\t\trq_data = (unsigned char *)bnx2fc_get_next_rqe(tgt, num_rq);\n\t\tspin_unlock_bh(&tgt->tgt_lock);\n\n\t\tif (rq_data) {\n\t\t\tbuf = rq_data;\n\t\t} else {\n\t\t\tbuf1 = buf = kmalloc((num_rq * BNX2FC_RQ_BUF_SZ),\n\t\t\t\t\t      GFP_ATOMIC);\n\n\t\t\tif (!buf1) {\n\t\t\t\tBNX2FC_TGT_DBG(tgt, \"Memory alloc failure\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfor (i = 0; i < num_rq; i++) {\n\t\t\t\tspin_lock_bh(&tgt->tgt_lock);\n\t\t\t\trq_data = (unsigned char *)\n\t\t\t\t\t   bnx2fc_get_next_rqe(tgt, 1);\n\t\t\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\t\t\tlen = BNX2FC_RQ_BUF_SZ;\n\t\t\t\tmemcpy(buf1, rq_data, len);\n\t\t\t\tbuf1 += len;\n\t\t\t}\n\t\t}\n\t\tbnx2fc_process_l2_frame_compl(tgt, buf, frame_len,\n\t\t\t\t\t      FC_XID_UNKNOWN);\n\n\t\tif (buf != rq_data)\n\t\t\tkfree(buf);\n\t\tspin_lock_bh(&tgt->tgt_lock);\n\t\tbnx2fc_return_rqe(tgt, num_rq);\n\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\tbreak;\n\n\tcase FCOE_ERROR_DETECTION_CQE_TYPE:\n\t\t \n\t\tspin_lock_bh(&tgt->tgt_lock);\n\t\tnum_rq = 1;\n\t\terr_entry = (struct fcoe_err_report_entry *)\n\t\t\t     bnx2fc_get_next_rqe(tgt, 1);\n\t\txid = err_entry->fc_hdr.ox_id;\n\t\tBNX2FC_TGT_DBG(tgt, \"Unsol Error Frame OX_ID = 0x%x\\n\", xid);\n\t\tBNX2FC_TGT_DBG(tgt, \"err_warn_bitmap = %08x:%08x\\n\",\n\t\t\terr_entry->data.err_warn_bitmap_hi,\n\t\t\terr_entry->data.err_warn_bitmap_lo);\n\t\tBNX2FC_TGT_DBG(tgt, \"buf_offsets - tx = 0x%x, rx = 0x%x\\n\",\n\t\t\terr_entry->data.tx_buf_off, err_entry->data.rx_buf_off);\n\n\t\tif (xid > hba->max_xid) {\n\t\t\tBNX2FC_TGT_DBG(tgt, \"xid(0x%x) out of FW range\\n\",\n\t\t\t\t   xid);\n\t\t\tgoto ret_err_rqe;\n\t\t}\n\n\n\t\tio_req = (struct bnx2fc_cmd *)hba->cmd_mgr->cmds[xid];\n\t\tif (!io_req)\n\t\t\tgoto ret_err_rqe;\n\n\t\tif (io_req->cmd_type != BNX2FC_SCSI_CMD) {\n\t\t\tprintk(KERN_ERR PFX \"err_warn: Not a SCSI cmd\\n\");\n\t\t\tgoto ret_err_rqe;\n\t\t}\n\n\t\tif (test_and_clear_bit(BNX2FC_FLAG_IO_CLEANUP,\n\t\t\t\t       &io_req->req_flags)) {\n\t\t\tBNX2FC_IO_DBG(io_req, \"unsol_err: cleanup in \"\n\t\t\t\t\t    \"progress.. ignore unsol err\\n\");\n\t\t\tgoto ret_err_rqe;\n\t\t}\n\n\t\terr_warn_bit_map = (u64)\n\t\t\t((u64)err_entry->data.err_warn_bitmap_hi << 32) |\n\t\t\t(u64)err_entry->data.err_warn_bitmap_lo;\n\t\tfor (i = 0; i < BNX2FC_NUM_ERR_BITS; i++) {\n\t\t\tif (err_warn_bit_map & (u64)((u64)1 << i)) {\n\t\t\t\terr_warn = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (test_bit(BNX2FC_FLAG_ISSUE_ABTS, &io_req->req_flags)) {\n\t\t\tprintk(KERN_ERR PFX \"err_warn: io_req (0x%x) already \"\n\t\t\t\t\t    \"in ABTS processing\\n\", xid);\n\t\t\tgoto ret_err_rqe;\n\t\t}\n\t\tBNX2FC_TGT_DBG(tgt, \"err = 0x%x\\n\", err_warn);\n\t\tif (tgt->dev_type != TYPE_TAPE)\n\t\t\tgoto skip_rec;\n\t\tswitch (err_warn) {\n\t\tcase FCOE_ERROR_CODE_REC_TOV_TIMER_EXPIRATION:\n\t\tcase FCOE_ERROR_CODE_DATA_OOO_RO:\n\t\tcase FCOE_ERROR_CODE_COMMON_INCORRECT_SEQ_CNT:\n\t\tcase FCOE_ERROR_CODE_DATA_SOFI3_SEQ_ACTIVE_SET:\n\t\tcase FCOE_ERROR_CODE_FCP_RSP_OPENED_SEQ:\n\t\tcase FCOE_ERROR_CODE_DATA_SOFN_SEQ_ACTIVE_RESET:\n\t\t\tBNX2FC_TGT_DBG(tgt, \"REC TOV popped for xid - 0x%x\\n\",\n\t\t\t\t   xid);\n\t\t\tmemcpy(&io_req->err_entry, err_entry,\n\t\t\t       sizeof(struct fcoe_err_report_entry));\n\t\t\tif (!test_bit(BNX2FC_FLAG_SRR_SENT,\n\t\t\t\t      &io_req->req_flags)) {\n\t\t\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\t\t\trc = bnx2fc_send_rec(io_req);\n\t\t\t\tspin_lock_bh(&tgt->tgt_lock);\n\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto skip_rec;\n\t\t\t} else\n\t\t\t\tprintk(KERN_ERR PFX \"SRR in progress\\n\");\n\t\t\tgoto ret_err_rqe;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\nskip_rec:\n\t\tset_bit(BNX2FC_FLAG_ISSUE_ABTS, &io_req->req_flags);\n\t\t \n\t\tif (cancel_delayed_work(&io_req->timeout_work))\n\t\t\tkref_put(&io_req->refcount, bnx2fc_cmd_release);\n\n\t\trc = bnx2fc_initiate_abts(io_req);\n\t\tif (rc != SUCCESS) {\n\t\t\tprintk(KERN_ERR PFX \"err_warn: initiate_abts \"\n\t\t\t\t\"failed xid = 0x%x. issue cleanup\\n\",\n\t\t\t\tio_req->xid);\n\t\t\tbnx2fc_initiate_cleanup(io_req);\n\t\t}\nret_err_rqe:\n\t\tbnx2fc_return_rqe(tgt, 1);\n\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\tbreak;\n\n\tcase FCOE_WARNING_DETECTION_CQE_TYPE:\n\t\t \n\t\tspin_lock_bh(&tgt->tgt_lock);\n\t\tnum_rq = 1;\n\t\terr_entry = (struct fcoe_err_report_entry *)\n\t\t\t     bnx2fc_get_next_rqe(tgt, 1);\n\t\txid = cpu_to_be16(err_entry->fc_hdr.ox_id);\n\t\tBNX2FC_TGT_DBG(tgt, \"Unsol Warning Frame OX_ID = 0x%x\\n\", xid);\n\t\tBNX2FC_TGT_DBG(tgt, \"err_warn_bitmap = %08x:%08x\",\n\t\t\terr_entry->data.err_warn_bitmap_hi,\n\t\t\terr_entry->data.err_warn_bitmap_lo);\n\t\tBNX2FC_TGT_DBG(tgt, \"buf_offsets - tx = 0x%x, rx = 0x%x\",\n\t\t\terr_entry->data.tx_buf_off, err_entry->data.rx_buf_off);\n\n\t\tif (xid > hba->max_xid) {\n\t\t\tBNX2FC_TGT_DBG(tgt, \"xid(0x%x) out of FW range\\n\", xid);\n\t\t\tgoto ret_warn_rqe;\n\t\t}\n\n\t\terr_warn_bit_map = (u64)\n\t\t\t((u64)err_entry->data.err_warn_bitmap_hi << 32) |\n\t\t\t(u64)err_entry->data.err_warn_bitmap_lo;\n\t\tfor (i = 0; i < BNX2FC_NUM_ERR_BITS; i++) {\n\t\t\tif (err_warn_bit_map & ((u64)1 << i)) {\n\t\t\t\terr_warn = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tBNX2FC_TGT_DBG(tgt, \"warn = 0x%x\\n\", err_warn);\n\n\t\tio_req = (struct bnx2fc_cmd *)hba->cmd_mgr->cmds[xid];\n\t\tif (!io_req)\n\t\t\tgoto ret_warn_rqe;\n\n\t\tif (io_req->cmd_type != BNX2FC_SCSI_CMD) {\n\t\t\tprintk(KERN_ERR PFX \"err_warn: Not a SCSI cmd\\n\");\n\t\t\tgoto ret_warn_rqe;\n\t\t}\n\n\t\tmemcpy(&io_req->err_entry, err_entry,\n\t\t       sizeof(struct fcoe_err_report_entry));\n\n\t\tif (err_warn == FCOE_ERROR_CODE_REC_TOV_TIMER_EXPIRATION)\n\t\t\t \n\t\t\tBUG_ON(1);\n\t\telse\n\t\t\tBNX2FC_TGT_DBG(tgt, \"Unsolicited warning\\n\");\nret_warn_rqe:\n\t\tbnx2fc_return_rqe(tgt, 1);\n\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\tbreak;\n\n\tdefault:\n\t\tprintk(KERN_ERR PFX \"Unsol Compl: Invalid CQE Subtype\\n\");\n\t\tbreak;\n\t}\n}\n\nvoid bnx2fc_process_cq_compl(struct bnx2fc_rport *tgt, u16 wqe,\n\t\t\t     unsigned char *rq_data, u8 num_rq,\n\t\t\t     struct fcoe_task_ctx_entry *task)\n{\n\tstruct fcoe_port *port = tgt->port;\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tstruct bnx2fc_cmd *io_req;\n\n\tu16 xid;\n\tu8  cmd_type;\n\tu8 rx_state = 0;\n\n\tspin_lock_bh(&tgt->tgt_lock);\n\n\txid = wqe & FCOE_PEND_WQ_CQE_TASK_ID;\n\tio_req = (struct bnx2fc_cmd *)hba->cmd_mgr->cmds[xid];\n\n\tif (io_req == NULL) {\n\t\tprintk(KERN_ERR PFX \"ERROR? cq_compl - io_req is NULL\\n\");\n\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\treturn;\n\t}\n\n\t \n\tcmd_type = io_req->cmd_type;\n\n\trx_state = ((task->rxwr_txrd.var_ctx.rx_flags &\n\t\t    FCOE_TCE_RX_WR_TX_RD_VAR_RX_STATE) >>\n\t\t    FCOE_TCE_RX_WR_TX_RD_VAR_RX_STATE_SHIFT);\n\n\t \n\tswitch (cmd_type) {\n\tcase BNX2FC_SCSI_CMD:\n\t\tif (rx_state == FCOE_TASK_RX_STATE_COMPLETED) {\n\t\t\tbnx2fc_process_scsi_cmd_compl(io_req, task, num_rq,\n\t\t\t\t\t\t      rq_data);\n\t\t\tspin_unlock_bh(&tgt->tgt_lock);\n\t\t\treturn;\n\t\t}\n\n\t\tif (rx_state == FCOE_TASK_RX_STATE_ABTS_COMPLETED)\n\t\t\tbnx2fc_process_abts_compl(io_req, task, num_rq);\n\t\telse if (rx_state ==\n\t\t\t FCOE_TASK_RX_STATE_EXCHANGE_CLEANUP_COMPLETED)\n\t\t\tbnx2fc_process_cleanup_compl(io_req, task, num_rq);\n\t\telse\n\t\t\tprintk(KERN_ERR PFX \"Invalid rx state - %d\\n\",\n\t\t\t\trx_state);\n\t\tbreak;\n\n\tcase BNX2FC_TASK_MGMT_CMD:\n\t\tBNX2FC_IO_DBG(io_req, \"Processing TM complete\\n\");\n\t\tbnx2fc_process_tm_compl(io_req, task, num_rq, rq_data);\n\t\tbreak;\n\n\tcase BNX2FC_ABTS:\n\t\t \n\t\tBNX2FC_IO_DBG(io_req, \"cq_compl- ABTS sent out by fw\\n\");\n\t\tkref_put(&io_req->refcount, bnx2fc_cmd_release);\n\t\tbreak;\n\n\tcase BNX2FC_ELS:\n\t\tif (rx_state == FCOE_TASK_RX_STATE_COMPLETED)\n\t\t\tbnx2fc_process_els_compl(io_req, task, num_rq);\n\t\telse if (rx_state == FCOE_TASK_RX_STATE_ABTS_COMPLETED)\n\t\t\tbnx2fc_process_abts_compl(io_req, task, num_rq);\n\t\telse if (rx_state ==\n\t\t\t FCOE_TASK_RX_STATE_EXCHANGE_CLEANUP_COMPLETED)\n\t\t\tbnx2fc_process_cleanup_compl(io_req, task, num_rq);\n\t\telse\n\t\t\tprintk(KERN_ERR PFX \"Invalid rx state =  %d\\n\",\n\t\t\t\trx_state);\n\t\tbreak;\n\n\tcase BNX2FC_CLEANUP:\n\t\tBNX2FC_IO_DBG(io_req, \"cq_compl- cleanup resp rcvd\\n\");\n\t\tkref_put(&io_req->refcount, bnx2fc_cmd_release);\n\t\tbreak;\n\n\tcase BNX2FC_SEQ_CLEANUP:\n\t\tBNX2FC_IO_DBG(io_req, \"cq_compl(0x%x) - seq cleanup resp\\n\",\n\t\t\t      io_req->xid);\n\t\tbnx2fc_process_seq_cleanup_compl(io_req, task, rx_state);\n\t\tkref_put(&io_req->refcount, bnx2fc_cmd_release);\n\t\tbreak;\n\n\tdefault:\n\t\tprintk(KERN_ERR PFX \"Invalid cmd_type %d\\n\", cmd_type);\n\t\tbreak;\n\t}\n\tspin_unlock_bh(&tgt->tgt_lock);\n}\n\nvoid bnx2fc_arm_cq(struct bnx2fc_rport *tgt)\n{\n\tstruct b577xx_fcoe_rx_doorbell *rx_db = &tgt->rx_db;\n\tu32 msg;\n\n\twmb();\n\trx_db->doorbell_cq_cons = tgt->cq_cons_idx | (tgt->cq_curr_toggle_bit <<\n\t\t\tFCOE_CQE_TOGGLE_BIT_SHIFT);\n\tmsg = *((u32 *)rx_db);\n\twritel(cpu_to_le32(msg), tgt->ctx_base);\n\n}\n\nstatic struct bnx2fc_work *bnx2fc_alloc_work(struct bnx2fc_rport *tgt, u16 wqe,\n\t\t\t\t\t     unsigned char *rq_data, u8 num_rq,\n\t\t\t\t\t     struct fcoe_task_ctx_entry *task)\n{\n\tstruct bnx2fc_work *work;\n\twork = kzalloc(sizeof(struct bnx2fc_work), GFP_ATOMIC);\n\tif (!work)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&work->list);\n\twork->tgt = tgt;\n\twork->wqe = wqe;\n\twork->num_rq = num_rq;\n\twork->task = task;\n\tif (rq_data)\n\t\tmemcpy(work->rq_data, rq_data, BNX2FC_RQ_BUF_SZ);\n\n\treturn work;\n}\n\n \nstatic bool bnx2fc_pending_work(struct bnx2fc_rport *tgt, unsigned int wqe)\n{\n\tunsigned int cpu = wqe % num_possible_cpus();\n\tstruct bnx2fc_percpu_s *fps;\n\tstruct bnx2fc_work *work;\n\tstruct fcoe_task_ctx_entry *task;\n\tstruct fcoe_task_ctx_entry *task_page;\n\tstruct fcoe_port *port = tgt->port;\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct bnx2fc_hba *hba = interface->hba;\n\tunsigned char *rq_data = NULL;\n\tunsigned char rq_data_buff[BNX2FC_RQ_BUF_SZ];\n\tint task_idx, index;\n\tu16 xid;\n\tu8 num_rq;\n\tint i;\n\n\txid = wqe & FCOE_PEND_WQ_CQE_TASK_ID;\n\tif (xid >= hba->max_tasks) {\n\t\tpr_err(PFX \"ERROR:xid out of range\\n\");\n\t\treturn false;\n\t}\n\n\ttask_idx = xid / BNX2FC_TASKS_PER_PAGE;\n\tindex = xid % BNX2FC_TASKS_PER_PAGE;\n\ttask_page = (struct fcoe_task_ctx_entry *)hba->task_ctx[task_idx];\n\ttask = &task_page[index];\n\n\tnum_rq = ((task->rxwr_txrd.var_ctx.rx_flags &\n\t\t   FCOE_TCE_RX_WR_TX_RD_VAR_NUM_RQ_WQE) >>\n\t\t  FCOE_TCE_RX_WR_TX_RD_VAR_NUM_RQ_WQE_SHIFT);\n\n\tmemset(rq_data_buff, 0, BNX2FC_RQ_BUF_SZ);\n\n\tif (!num_rq)\n\t\tgoto num_rq_zero;\n\n\trq_data = bnx2fc_get_next_rqe(tgt, 1);\n\n\tif (num_rq > 1) {\n\t\t \n\t\tfor (i = 1; i < num_rq; i++)\n\t\t\tbnx2fc_get_next_rqe(tgt, 1);\n\t}\n\n\tif (rq_data)\n\t\tmemcpy(rq_data_buff, rq_data, BNX2FC_RQ_BUF_SZ);\n\n\t \n\tfor (i = 0; i < num_rq; i++)\n\t\tbnx2fc_return_rqe(tgt, 1);\n\nnum_rq_zero:\n\n\tfps = &per_cpu(bnx2fc_percpu, cpu);\n\tspin_lock_bh(&fps->fp_work_lock);\n\tif (fps->iothread) {\n\t\twork = bnx2fc_alloc_work(tgt, wqe, rq_data_buff,\n\t\t\t\t\t num_rq, task);\n\t\tif (work) {\n\t\t\tlist_add_tail(&work->list, &fps->work_list);\n\t\t\twake_up_process(fps->iothread);\n\t\t\tspin_unlock_bh(&fps->fp_work_lock);\n\t\t\treturn true;\n\t\t}\n\t}\n\tspin_unlock_bh(&fps->fp_work_lock);\n\tbnx2fc_process_cq_compl(tgt, wqe,\n\t\t\t\trq_data_buff, num_rq, task);\n\n\treturn true;\n}\n\nint bnx2fc_process_new_cqes(struct bnx2fc_rport *tgt)\n{\n\tstruct fcoe_cqe *cq;\n\tu32 cq_cons;\n\tstruct fcoe_cqe *cqe;\n\tu32 num_free_sqes = 0;\n\tu32 num_cqes = 0;\n\tu16 wqe;\n\n\t \n\tspin_lock_bh(&tgt->cq_lock);\n\n\tif (!tgt->cq) {\n\t\tprintk(KERN_ERR PFX \"process_new_cqes: cq is NULL\\n\");\n\t\tspin_unlock_bh(&tgt->cq_lock);\n\t\treturn 0;\n\t}\n\tcq = tgt->cq;\n\tcq_cons = tgt->cq_cons_idx;\n\tcqe = &cq[cq_cons];\n\n\twhile (((wqe = cqe->wqe) & FCOE_CQE_TOGGLE_BIT) ==\n\t       (tgt->cq_curr_toggle_bit <<\n\t       FCOE_CQE_TOGGLE_BIT_SHIFT)) {\n\n\t\t \n\t\tif (wqe & FCOE_CQE_CQE_TYPE) {\n\t\t\t \n\t\t\tbnx2fc_process_unsol_compl(tgt, wqe);\n\t\t} else {\n\t\t\tif (bnx2fc_pending_work(tgt, wqe))\n\t\t\t\tnum_free_sqes++;\n\t\t}\n\t\tcqe++;\n\t\ttgt->cq_cons_idx++;\n\t\tnum_cqes++;\n\n\t\tif (tgt->cq_cons_idx == BNX2FC_CQ_WQES_MAX) {\n\t\t\ttgt->cq_cons_idx = 0;\n\t\t\tcqe = cq;\n\t\t\ttgt->cq_curr_toggle_bit =\n\t\t\t\t1 - tgt->cq_curr_toggle_bit;\n\t\t}\n\t}\n\tif (num_cqes) {\n\t\t \n\t\tif (tgt->ctx_base)\n\t\t\tbnx2fc_arm_cq(tgt);\n\t\tatomic_add(num_free_sqes, &tgt->free_sqes);\n\t}\n\tspin_unlock_bh(&tgt->cq_lock);\n\treturn 0;\n}\n\n \nstatic void bnx2fc_fastpath_notification(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *new_cqe_kcqe)\n{\n\tu32 conn_id = new_cqe_kcqe->fcoe_conn_id;\n\tstruct bnx2fc_rport *tgt = hba->tgt_ofld_list[conn_id];\n\n\tif (!tgt) {\n\t\tprintk(KERN_ERR PFX \"conn_id 0x%x not valid\\n\", conn_id);\n\t\treturn;\n\t}\n\n\tbnx2fc_process_new_cqes(tgt);\n}\n\n \nstatic void bnx2fc_process_ofld_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *ofld_kcqe)\n{\n\tstruct bnx2fc_rport\t\t*tgt;\n\tstruct bnx2fc_interface\t\t*interface;\n\tu32\t\t\t\tconn_id;\n\tu32\t\t\t\tcontext_id;\n\n\tconn_id = ofld_kcqe->fcoe_conn_id;\n\tcontext_id = ofld_kcqe->fcoe_conn_context_id;\n\ttgt = hba->tgt_ofld_list[conn_id];\n\tif (!tgt) {\n\t\tprintk(KERN_ALERT PFX \"ERROR:ofld_cmpl: No pending ofld req\\n\");\n\t\treturn;\n\t}\n\tBNX2FC_TGT_DBG(tgt, \"Entered ofld compl - context_id = 0x%x\\n\",\n\t\tofld_kcqe->fcoe_conn_context_id);\n\tinterface = tgt->port->priv;\n\tif (hba != interface->hba) {\n\t\tprintk(KERN_ERR PFX \"ERROR:ofld_cmpl: HBA mismatch\\n\");\n\t\tgoto ofld_cmpl_err;\n\t}\n\t \n\ttgt->context_id = context_id;\n\tif (ofld_kcqe->completion_status) {\n\t\tif (ofld_kcqe->completion_status ==\n\t\t\t\tFCOE_KCQE_COMPLETION_STATUS_CTX_ALLOC_FAILURE) {\n\t\t\tprintk(KERN_ERR PFX \"unable to allocate FCoE context \"\n\t\t\t\t\"resources\\n\");\n\t\t\tset_bit(BNX2FC_FLAG_CTX_ALLOC_FAILURE, &tgt->flags);\n\t\t}\n\t} else {\n\t\t \n\t\tset_bit(BNX2FC_FLAG_OFFLOADED, &tgt->flags);\n\t}\nofld_cmpl_err:\n\tset_bit(BNX2FC_FLAG_OFLD_REQ_CMPL, &tgt->flags);\n\twake_up_interruptible(&tgt->ofld_wait);\n}\n\n \n\nstatic void bnx2fc_process_enable_conn_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\t\tstruct fcoe_kcqe *ofld_kcqe)\n{\n\tstruct bnx2fc_rport\t\t*tgt;\n\tstruct bnx2fc_interface\t\t*interface;\n\tu32\t\t\t\tconn_id;\n\tu32\t\t\t\tcontext_id;\n\n\tcontext_id = ofld_kcqe->fcoe_conn_context_id;\n\tconn_id = ofld_kcqe->fcoe_conn_id;\n\ttgt = hba->tgt_ofld_list[conn_id];\n\tif (!tgt) {\n\t\tprintk(KERN_ERR PFX \"ERROR:enbl_cmpl: No pending ofld req\\n\");\n\t\treturn;\n\t}\n\n\tBNX2FC_TGT_DBG(tgt, \"Enable compl - context_id = 0x%x\\n\",\n\t\tofld_kcqe->fcoe_conn_context_id);\n\n\t \n\tif (tgt->context_id != context_id) {\n\t\tprintk(KERN_ERR PFX \"context id mismatch\\n\");\n\t\treturn;\n\t}\n\tinterface = tgt->port->priv;\n\tif (hba != interface->hba) {\n\t\tprintk(KERN_ERR PFX \"bnx2fc-enbl_cmpl: HBA mismatch\\n\");\n\t\tgoto enbl_cmpl_err;\n\t}\n\tif (!ofld_kcqe->completion_status)\n\t\t \n\t\tset_bit(BNX2FC_FLAG_ENABLED, &tgt->flags);\n\nenbl_cmpl_err:\n\tset_bit(BNX2FC_FLAG_OFLD_REQ_CMPL, &tgt->flags);\n\twake_up_interruptible(&tgt->ofld_wait);\n}\n\nstatic void bnx2fc_process_conn_disable_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *disable_kcqe)\n{\n\n\tstruct bnx2fc_rport\t\t*tgt;\n\tu32\t\t\t\tconn_id;\n\n\tconn_id = disable_kcqe->fcoe_conn_id;\n\ttgt = hba->tgt_ofld_list[conn_id];\n\tif (!tgt) {\n\t\tprintk(KERN_ERR PFX \"ERROR: disable_cmpl: No disable req\\n\");\n\t\treturn;\n\t}\n\n\tBNX2FC_TGT_DBG(tgt, PFX \"disable_cmpl: conn_id %d\\n\", conn_id);\n\n\tif (disable_kcqe->completion_status) {\n\t\tprintk(KERN_ERR PFX \"Disable failed with cmpl status %d\\n\",\n\t\t\tdisable_kcqe->completion_status);\n\t\tset_bit(BNX2FC_FLAG_DISABLE_FAILED, &tgt->flags);\n\t\tset_bit(BNX2FC_FLAG_UPLD_REQ_COMPL, &tgt->flags);\n\t\twake_up_interruptible(&tgt->upld_wait);\n\t} else {\n\t\t \n\t\tBNX2FC_TGT_DBG(tgt, \"disable successful\\n\");\n\t\tclear_bit(BNX2FC_FLAG_OFFLOADED, &tgt->flags);\n\t\tclear_bit(BNX2FC_FLAG_ENABLED, &tgt->flags);\n\t\tset_bit(BNX2FC_FLAG_DISABLED, &tgt->flags);\n\t\tset_bit(BNX2FC_FLAG_UPLD_REQ_COMPL, &tgt->flags);\n\t\twake_up_interruptible(&tgt->upld_wait);\n\t}\n}\n\nstatic void bnx2fc_process_conn_destroy_cmpl(struct bnx2fc_hba *hba,\n\t\t\t\t\tstruct fcoe_kcqe *destroy_kcqe)\n{\n\tstruct bnx2fc_rport\t\t*tgt;\n\tu32\t\t\t\tconn_id;\n\n\tconn_id = destroy_kcqe->fcoe_conn_id;\n\ttgt = hba->tgt_ofld_list[conn_id];\n\tif (!tgt) {\n\t\tprintk(KERN_ERR PFX \"destroy_cmpl: No destroy req\\n\");\n\t\treturn;\n\t}\n\n\tBNX2FC_TGT_DBG(tgt, \"destroy_cmpl: conn_id %d\\n\", conn_id);\n\n\tif (destroy_kcqe->completion_status) {\n\t\tprintk(KERN_ERR PFX \"Destroy conn failed, cmpl status %d\\n\",\n\t\t\tdestroy_kcqe->completion_status);\n\t\treturn;\n\t} else {\n\t\t \n\t\tBNX2FC_TGT_DBG(tgt, \"upload successful\\n\");\n\t\tclear_bit(BNX2FC_FLAG_DISABLED, &tgt->flags);\n\t\tset_bit(BNX2FC_FLAG_DESTROYED, &tgt->flags);\n\t\tset_bit(BNX2FC_FLAG_UPLD_REQ_COMPL, &tgt->flags);\n\t\twake_up_interruptible(&tgt->upld_wait);\n\t}\n}\n\nstatic void bnx2fc_init_failure(struct bnx2fc_hba *hba, u32 err_code)\n{\n\tswitch (err_code) {\n\tcase FCOE_KCQE_COMPLETION_STATUS_INVALID_OPCODE:\n\t\tprintk(KERN_ERR PFX \"init_failure due to invalid opcode\\n\");\n\t\tbreak;\n\n\tcase FCOE_KCQE_COMPLETION_STATUS_CTX_ALLOC_FAILURE:\n\t\tprintk(KERN_ERR PFX \"init failed due to ctx alloc failure\\n\");\n\t\tbreak;\n\n\tcase FCOE_KCQE_COMPLETION_STATUS_NIC_ERROR:\n\t\tprintk(KERN_ERR PFX \"init_failure due to NIC error\\n\");\n\t\tbreak;\n\tcase FCOE_KCQE_COMPLETION_STATUS_ERROR:\n\t\tprintk(KERN_ERR PFX \"init failure due to compl status err\\n\");\n\t\tbreak;\n\tcase FCOE_KCQE_COMPLETION_STATUS_WRONG_HSI_VERSION:\n\t\tprintk(KERN_ERR PFX \"init failure due to HSI mismatch\\n\");\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR PFX \"Unknown Error code %d\\n\", err_code);\n\t}\n}\n\n \nvoid bnx2fc_indicate_kcqe(void *context, struct kcqe *kcq[],\n\t\t\t\t\tu32 num_cqe)\n{\n\tstruct bnx2fc_hba *hba = (struct bnx2fc_hba *)context;\n\tint i = 0;\n\tstruct fcoe_kcqe *kcqe = NULL;\n\n\twhile (i < num_cqe) {\n\t\tkcqe = (struct fcoe_kcqe *) kcq[i++];\n\n\t\tswitch (kcqe->op_code) {\n\t\tcase FCOE_KCQE_OPCODE_CQ_EVENT_NOTIFICATION:\n\t\t\tbnx2fc_fastpath_notification(hba, kcqe);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_OFFLOAD_CONN:\n\t\t\tbnx2fc_process_ofld_cmpl(hba, kcqe);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_ENABLE_CONN:\n\t\t\tbnx2fc_process_enable_conn_cmpl(hba, kcqe);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_INIT_FUNC:\n\t\t\tif (kcqe->completion_status !=\n\t\t\t\t\tFCOE_KCQE_COMPLETION_STATUS_SUCCESS) {\n\t\t\t\tbnx2fc_init_failure(hba,\n\t\t\t\t\t\tkcqe->completion_status);\n\t\t\t} else {\n\t\t\t\tset_bit(ADAPTER_STATE_UP, &hba->adapter_state);\n\t\t\t\tbnx2fc_get_link_state(hba);\n\t\t\t\tprintk(KERN_INFO PFX \"[%.2x]: FCOE_INIT passed\\n\",\n\t\t\t\t\t(u8)hba->pcidev->bus->number);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_DESTROY_FUNC:\n\t\t\tif (kcqe->completion_status !=\n\t\t\t\t\tFCOE_KCQE_COMPLETION_STATUS_SUCCESS) {\n\n\t\t\t\tprintk(KERN_ERR PFX \"DESTROY failed\\n\");\n\t\t\t} else {\n\t\t\t\tprintk(KERN_ERR PFX \"DESTROY success\\n\");\n\t\t\t}\n\t\t\tset_bit(BNX2FC_FLAG_DESTROY_CMPL, &hba->flags);\n\t\t\twake_up_interruptible(&hba->destroy_wait);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_DISABLE_CONN:\n\t\t\tbnx2fc_process_conn_disable_cmpl(hba, kcqe);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_DESTROY_CONN:\n\t\t\tbnx2fc_process_conn_destroy_cmpl(hba, kcqe);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_STAT_FUNC:\n\t\t\tif (kcqe->completion_status !=\n\t\t\t    FCOE_KCQE_COMPLETION_STATUS_SUCCESS)\n\t\t\t\tprintk(KERN_ERR PFX \"STAT failed\\n\");\n\t\t\tcomplete(&hba->stat_req_done);\n\t\t\tbreak;\n\n\t\tcase FCOE_KCQE_OPCODE_FCOE_ERROR:\n\t\tdefault:\n\t\t\tprintk(KERN_ERR PFX \"unknown opcode 0x%x\\n\",\n\t\t\t\t\t\t\t\tkcqe->op_code);\n\t\t}\n\t}\n}\n\nvoid bnx2fc_add_2_sq(struct bnx2fc_rport *tgt, u16 xid)\n{\n\tstruct fcoe_sqe *sqe;\n\n\tsqe = &tgt->sq[tgt->sq_prod_idx];\n\n\t \n\tsqe->wqe = xid << FCOE_SQE_TASK_ID_SHIFT;\n\tsqe->wqe |= tgt->sq_curr_toggle_bit << FCOE_SQE_TOGGLE_BIT_SHIFT;\n\n\t \n\tif (++tgt->sq_prod_idx == BNX2FC_SQ_WQES_MAX) {\n\t\ttgt->sq_prod_idx = 0;\n\t\ttgt->sq_curr_toggle_bit = 1 - tgt->sq_curr_toggle_bit;\n\t}\n}\n\nvoid bnx2fc_ring_doorbell(struct bnx2fc_rport *tgt)\n{\n\tstruct b577xx_doorbell_set_prod *sq_db = &tgt->sq_db;\n\tu32 msg;\n\n\twmb();\n\tsq_db->prod = tgt->sq_prod_idx |\n\t\t\t\t(tgt->sq_curr_toggle_bit << 15);\n\tmsg = *((u32 *)sq_db);\n\twritel(cpu_to_le32(msg), tgt->ctx_base);\n\n}\n\nint bnx2fc_map_doorbell(struct bnx2fc_rport *tgt)\n{\n\tu32 context_id = tgt->context_id;\n\tstruct fcoe_port *port = tgt->port;\n\tu32 reg_off;\n\tresource_size_t reg_base;\n\tstruct bnx2fc_interface *interface = port->priv;\n\tstruct bnx2fc_hba *hba = interface->hba;\n\n\treg_base = pci_resource_start(hba->pcidev,\n\t\t\t\t\tBNX2X_DOORBELL_PCI_BAR);\n\treg_off = (1 << BNX2X_DB_SHIFT) * (context_id & 0x1FFFF);\n\ttgt->ctx_base = ioremap(reg_base + reg_off, 4);\n\tif (!tgt->ctx_base)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nchar *bnx2fc_get_next_rqe(struct bnx2fc_rport *tgt, u8 num_items)\n{\n\tchar *buf = (char *)tgt->rq + (tgt->rq_cons_idx * BNX2FC_RQ_BUF_SZ);\n\n\tif (tgt->rq_cons_idx + num_items > BNX2FC_RQ_WQES_MAX)\n\t\treturn NULL;\n\n\ttgt->rq_cons_idx += num_items;\n\n\tif (tgt->rq_cons_idx >= BNX2FC_RQ_WQES_MAX)\n\t\ttgt->rq_cons_idx -= BNX2FC_RQ_WQES_MAX;\n\n\treturn buf;\n}\n\nvoid bnx2fc_return_rqe(struct bnx2fc_rport *tgt, u8 num_items)\n{\n\t \n\tu32 next_prod_idx = tgt->rq_prod_idx + num_items;\n\tif ((next_prod_idx & 0x7fff) == BNX2FC_RQ_WQES_MAX) {\n\t\t \n\t\tnext_prod_idx += 0x8000 - BNX2FC_RQ_WQES_MAX;\n\t}\n\ttgt->rq_prod_idx = next_prod_idx;\n\ttgt->conn_db->rq_prod = tgt->rq_prod_idx;\n}\n\nvoid bnx2fc_init_seq_cleanup_task(struct bnx2fc_cmd *seq_clnp_req,\n\t\t\t\t  struct fcoe_task_ctx_entry *task,\n\t\t\t\t  struct bnx2fc_cmd *orig_io_req,\n\t\t\t\t  u32 offset)\n{\n\tstruct scsi_cmnd *sc_cmd = orig_io_req->sc_cmd;\n\tstruct bnx2fc_rport *tgt = seq_clnp_req->tgt;\n\tstruct fcoe_bd_ctx *bd = orig_io_req->bd_tbl->bd_tbl;\n\tstruct fcoe_ext_mul_sges_ctx *sgl;\n\tu8 task_type = FCOE_TASK_TYPE_SEQUENCE_CLEANUP;\n\tu8 orig_task_type;\n\tu16 orig_xid = orig_io_req->xid;\n\tu32 context_id = tgt->context_id;\n\tu64 phys_addr = (u64)orig_io_req->bd_tbl->bd_tbl_dma;\n\tu32 orig_offset = offset;\n\tint bd_count;\n\tint i;\n\n\tmemset(task, 0, sizeof(struct fcoe_task_ctx_entry));\n\n\tif (sc_cmd->sc_data_direction == DMA_TO_DEVICE)\n\t\torig_task_type = FCOE_TASK_TYPE_WRITE;\n\telse\n\t\torig_task_type = FCOE_TASK_TYPE_READ;\n\n\t \n\ttask->txwr_rxrd.const_ctx.tx_flags =\n\t\t\t\tFCOE_TASK_TX_STATE_SEQUENCE_CLEANUP <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TX_STATE_SHIFT;\n\t \n\ttask->txwr_rxrd.const_ctx.init_flags = task_type <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TASK_TYPE_SHIFT;\n\ttask->txwr_rxrd.const_ctx.init_flags |= FCOE_TASK_CLASS_TYPE_3 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CLASS_TYPE_SHIFT;\n\ttask->rxwr_txrd.const_ctx.init_flags = context_id <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_CONST_CID_SHIFT;\n\ttask->rxwr_txrd.const_ctx.init_flags = context_id <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_CONST_CID_SHIFT;\n\n\ttask->txwr_rxrd.union_ctx.cleanup.ctx.cleaned_task_id = orig_xid;\n\n\ttask->txwr_rxrd.union_ctx.cleanup.ctx.rolled_tx_seq_cnt = 0;\n\ttask->txwr_rxrd.union_ctx.cleanup.ctx.rolled_tx_data_offset = offset;\n\n\tbd_count = orig_io_req->bd_tbl->bd_valid;\n\n\t \n\tfor (i = 0; i < bd_count; i++) {\n\t\tif (offset < bd[i].buf_len)\n\t\t\tbreak;\n\t\toffset -= bd[i].buf_len;\n\t}\n\tphys_addr += (i * sizeof(struct fcoe_bd_ctx));\n\n\tif (orig_task_type == FCOE_TASK_TYPE_WRITE) {\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.lo =\n\t\t\t\t(u32)phys_addr;\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.hi =\n\t\t\t\t(u32)((u64)phys_addr >> 32);\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.sgl_size =\n\t\t\t\tbd_count;\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_off =\n\t\t\t\toffset;  \n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_idx = i;\n\t} else {\n\n\t\t \n\t\tsgl = &task->rxwr_only.union_ctx.read_info.sgl_ctx.sgl;\n\t\tsgl->mul_sgl.cur_sge_addr.lo = (u32)phys_addr;\n\t\tsgl->mul_sgl.cur_sge_addr.hi = (u32)((u64)phys_addr >> 32);\n\t\tsgl->mul_sgl.sgl_size = bd_count;\n\t\tsgl->mul_sgl.cur_sge_off = offset;  \n\t\tsgl->mul_sgl.cur_sge_idx = i;\n\n\t\tmemset(&task->rxwr_only.rx_seq_ctx, 0,\n\t\t       sizeof(struct fcoe_rx_seq_ctx));\n\t\ttask->rxwr_only.rx_seq_ctx.low_exp_ro = orig_offset;\n\t\ttask->rxwr_only.rx_seq_ctx.high_exp_ro = orig_offset;\n\t}\n}\nvoid bnx2fc_init_cleanup_task(struct bnx2fc_cmd *io_req,\n\t\t\t      struct fcoe_task_ctx_entry *task,\n\t\t\t      u16 orig_xid)\n{\n\tu8 task_type = FCOE_TASK_TYPE_EXCHANGE_CLEANUP;\n\tstruct bnx2fc_rport *tgt = io_req->tgt;\n\tu32 context_id = tgt->context_id;\n\n\tmemset(task, 0, sizeof(struct fcoe_task_ctx_entry));\n\n\t \n\t \n\ttask->txwr_rxrd.const_ctx.init_flags = task_type <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TASK_TYPE_SHIFT;\n\ttask->txwr_rxrd.const_ctx.init_flags |= FCOE_TASK_CLASS_TYPE_3 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CLASS_TYPE_SHIFT;\n\tif (tgt->dev_type == TYPE_TAPE)\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_TAPE <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\telse\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_DISK <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\ttask->txwr_rxrd.union_ctx.cleanup.ctx.cleaned_task_id = orig_xid;\n\n\t \n\ttask->txwr_rxrd.const_ctx.tx_flags =\n\t\t\t\tFCOE_TASK_TX_STATE_EXCHANGE_CLEANUP <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TX_STATE_SHIFT;\n\n\t \n\ttask->rxwr_txrd.const_ctx.init_flags = context_id <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_CONST_CID_SHIFT;\n\ttask->rxwr_txrd.var_ctx.rx_flags |= 1 <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_VAR_EXP_FIRST_FRAME_SHIFT;\n}\n\nvoid bnx2fc_init_mp_task(struct bnx2fc_cmd *io_req,\n\t\t\t\tstruct fcoe_task_ctx_entry *task)\n{\n\tstruct bnx2fc_mp_req *mp_req = &(io_req->mp_req);\n\tstruct bnx2fc_rport *tgt = io_req->tgt;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct fcoe_ext_mul_sges_ctx *sgl;\n\tu8 task_type = 0;\n\tu64 *hdr;\n\tu64 temp_hdr[3];\n\tu32 context_id;\n\n\n\t \n\tif ((io_req->cmd_type == BNX2FC_TASK_MGMT_CMD) ||\n\t    (io_req->cmd_type == BNX2FC_ELS)) {\n\t\ttask_type = FCOE_TASK_TYPE_MIDPATH;\n\t} else if (io_req->cmd_type == BNX2FC_ABTS) {\n\t\ttask_type = FCOE_TASK_TYPE_ABTS;\n\t}\n\n\tmemset(task, 0, sizeof(struct fcoe_task_ctx_entry));\n\n\t \n\tio_req->task = task;\n\n\tBNX2FC_IO_DBG(io_req, \"Init MP task for cmd_type = %d task_type = %d\\n\",\n\t\tio_req->cmd_type, task_type);\n\n\t \n\tif ((task_type == FCOE_TASK_TYPE_MIDPATH) ||\n\t    (task_type == FCOE_TASK_TYPE_UNSOLICITED)) {\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.lo =\n\t\t\t\t(u32)mp_req->mp_req_bd_dma;\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.hi =\n\t\t\t\t(u32)((u64)mp_req->mp_req_bd_dma >> 32);\n\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.sgl_size = 1;\n\t}\n\n\t \n\t \n\ttask->txwr_rxrd.const_ctx.init_flags = task_type <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TASK_TYPE_SHIFT;\n\tif (tgt->dev_type == TYPE_TAPE)\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_TAPE <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\telse\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_DISK <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\ttask->txwr_rxrd.const_ctx.init_flags |= FCOE_TASK_CLASS_TYPE_3 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CLASS_TYPE_SHIFT;\n\n\t \n\ttask->txwr_rxrd.const_ctx.tx_flags = FCOE_TASK_TX_STATE_INIT <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TX_STATE_SHIFT;\n\n\t \n\ttask->rxwr_txrd.const_ctx.data_2_trns = io_req->data_xfer_len;\n\n\t \n\ttask->rxwr_txrd.var_ctx.rx_flags |= 1 <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_VAR_EXP_FIRST_FRAME_SHIFT;\n\n\tcontext_id = tgt->context_id;\n\ttask->rxwr_txrd.const_ctx.init_flags = context_id <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_CONST_CID_SHIFT;\n\n\tfc_hdr = &(mp_req->req_fc_hdr);\n\tif (task_type == FCOE_TASK_TYPE_MIDPATH) {\n\t\tfc_hdr->fh_ox_id = cpu_to_be16(io_req->xid);\n\t\tfc_hdr->fh_rx_id = htons(0xffff);\n\t\ttask->rxwr_txrd.var_ctx.rx_id = 0xffff;\n\t} else if (task_type == FCOE_TASK_TYPE_UNSOLICITED) {\n\t\tfc_hdr->fh_rx_id = cpu_to_be16(io_req->xid);\n\t}\n\n\t \n\thdr = (u64 *) &task->txwr_rxrd.union_ctx.tx_frame.fc_hdr;\n\tmemcpy(temp_hdr, fc_hdr, sizeof(temp_hdr));\n\thdr[0] = cpu_to_be64(temp_hdr[0]);\n\thdr[1] = cpu_to_be64(temp_hdr[1]);\n\thdr[2] = cpu_to_be64(temp_hdr[2]);\n\n\t \n\tif (task_type == FCOE_TASK_TYPE_MIDPATH) {\n\t\tsgl = &task->rxwr_only.union_ctx.read_info.sgl_ctx.sgl;\n\n\t\tsgl->mul_sgl.cur_sge_addr.lo = (u32)mp_req->mp_resp_bd_dma;\n\t\tsgl->mul_sgl.cur_sge_addr.hi =\n\t\t\t\t(u32)((u64)mp_req->mp_resp_bd_dma >> 32);\n\t\tsgl->mul_sgl.sgl_size = 1;\n\t}\n}\n\nvoid bnx2fc_init_task(struct bnx2fc_cmd *io_req,\n\t\t\t     struct fcoe_task_ctx_entry *task)\n{\n\tu8 task_type;\n\tstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\n\tstruct io_bdt *bd_tbl = io_req->bd_tbl;\n\tstruct bnx2fc_rport *tgt = io_req->tgt;\n\tstruct fcoe_cached_sge_ctx *cached_sge;\n\tstruct fcoe_ext_mul_sges_ctx *sgl;\n\tint dev_type = tgt->dev_type;\n\tu64 *fcp_cmnd;\n\tu64 tmp_fcp_cmnd[4];\n\tu32 context_id;\n\tint cnt, i;\n\tint bd_count;\n\n\tmemset(task, 0, sizeof(struct fcoe_task_ctx_entry));\n\n\t \n\tio_req->task = task;\n\n\tif (sc_cmd->sc_data_direction == DMA_TO_DEVICE)\n\t\ttask_type = FCOE_TASK_TYPE_WRITE;\n\telse\n\t\ttask_type = FCOE_TASK_TYPE_READ;\n\n\t \n\tbd_count = bd_tbl->bd_valid;\n\tcached_sge = &task->rxwr_only.union_ctx.read_info.sgl_ctx.cached_sge;\n\tif (task_type == FCOE_TASK_TYPE_WRITE) {\n\t\tif ((dev_type == TYPE_DISK) && (bd_count == 1)) {\n\t\t\tstruct fcoe_bd_ctx *fcoe_bd_tbl = bd_tbl->bd_tbl;\n\n\t\t\ttask->txwr_only.sgl_ctx.cached_sge.cur_buf_addr.lo =\n\t\t\tcached_sge->cur_buf_addr.lo =\n\t\t\t\t\tfcoe_bd_tbl->buf_addr_lo;\n\t\t\ttask->txwr_only.sgl_ctx.cached_sge.cur_buf_addr.hi =\n\t\t\tcached_sge->cur_buf_addr.hi =\n\t\t\t\t\tfcoe_bd_tbl->buf_addr_hi;\n\t\t\ttask->txwr_only.sgl_ctx.cached_sge.cur_buf_rem =\n\t\t\tcached_sge->cur_buf_rem =\n\t\t\t\t\tfcoe_bd_tbl->buf_len;\n\n\t\t\ttask->txwr_rxrd.const_ctx.init_flags |= 1 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CACHED_SGE_SHIFT;\n\t\t} else {\n\t\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.lo =\n\t\t\t\t\t(u32)bd_tbl->bd_tbl_dma;\n\t\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.cur_sge_addr.hi =\n\t\t\t\t\t(u32)((u64)bd_tbl->bd_tbl_dma >> 32);\n\t\t\ttask->txwr_only.sgl_ctx.sgl.mul_sgl.sgl_size =\n\t\t\t\t\tbd_tbl->bd_valid;\n\t\t}\n\t}\n\n\t \n\t \n\ttask->txwr_rxrd.const_ctx.init_flags |= task_type <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TASK_TYPE_SHIFT;\n\tif (dev_type == TYPE_TAPE) {\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_TAPE <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\t\tio_req->rec_retry = 0;\n\t\tio_req->rec_retry = 0;\n\t} else\n\t\ttask->txwr_rxrd.const_ctx.init_flags |=\n\t\t\t\tFCOE_TASK_DEV_TYPE_DISK <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_DEV_TYPE_SHIFT;\n\ttask->txwr_rxrd.const_ctx.init_flags |= FCOE_TASK_CLASS_TYPE_3 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CLASS_TYPE_SHIFT;\n\t \n\ttask->txwr_rxrd.const_ctx.tx_flags = FCOE_TASK_TX_STATE_NORMAL <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_TX_STATE_SHIFT;\n\n\t \n\ttask->txwr_rxrd.union_ctx.tx_seq.ctx.seq_cnt = 1;\n\n\t \n\tfcp_cmnd = (u64 *)\n\t\t    task->txwr_rxrd.union_ctx.fcp_cmd.opaque;\n\tbnx2fc_build_fcp_cmnd(io_req, (struct fcp_cmnd *)&tmp_fcp_cmnd);\n\n\t \n\tcnt = sizeof(struct fcp_cmnd) / sizeof(u64);\n\n\tfor (i = 0; i < cnt; i++) {\n\t\t*fcp_cmnd = cpu_to_be64(tmp_fcp_cmnd[i]);\n\t\tfcp_cmnd++;\n\t}\n\n\t \n\ttask->rxwr_txrd.const_ctx.data_2_trns = io_req->data_xfer_len;\n\n\tcontext_id = tgt->context_id;\n\ttask->rxwr_txrd.const_ctx.init_flags = context_id <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_CONST_CID_SHIFT;\n\n\t \n\t \n\ttask->rxwr_txrd.var_ctx.rx_flags |= 1 <<\n\t\t\t\tFCOE_TCE_RX_WR_TX_RD_VAR_EXP_FIRST_FRAME_SHIFT;\n\n\ttask->rxwr_txrd.var_ctx.rx_id = 0xffff;\n\n\t \n\tif (task_type != FCOE_TASK_TYPE_READ)\n\t\treturn;\n\n\tsgl = &task->rxwr_only.union_ctx.read_info.sgl_ctx.sgl;\n\tbd_count = bd_tbl->bd_valid;\n\n\tif (dev_type == TYPE_DISK) {\n\t\tif (bd_count == 1) {\n\n\t\t\tstruct fcoe_bd_ctx *fcoe_bd_tbl = bd_tbl->bd_tbl;\n\n\t\t\tcached_sge->cur_buf_addr.lo = fcoe_bd_tbl->buf_addr_lo;\n\t\t\tcached_sge->cur_buf_addr.hi = fcoe_bd_tbl->buf_addr_hi;\n\t\t\tcached_sge->cur_buf_rem = fcoe_bd_tbl->buf_len;\n\t\t\ttask->txwr_rxrd.const_ctx.init_flags |= 1 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CACHED_SGE_SHIFT;\n\t\t} else if (bd_count == 2) {\n\t\t\tstruct fcoe_bd_ctx *fcoe_bd_tbl = bd_tbl->bd_tbl;\n\n\t\t\tcached_sge->cur_buf_addr.lo = fcoe_bd_tbl->buf_addr_lo;\n\t\t\tcached_sge->cur_buf_addr.hi = fcoe_bd_tbl->buf_addr_hi;\n\t\t\tcached_sge->cur_buf_rem = fcoe_bd_tbl->buf_len;\n\n\t\t\tfcoe_bd_tbl++;\n\t\t\tcached_sge->second_buf_addr.lo =\n\t\t\t\t\t\t fcoe_bd_tbl->buf_addr_lo;\n\t\t\tcached_sge->second_buf_addr.hi =\n\t\t\t\t\t\tfcoe_bd_tbl->buf_addr_hi;\n\t\t\tcached_sge->second_buf_rem = fcoe_bd_tbl->buf_len;\n\t\t\ttask->txwr_rxrd.const_ctx.init_flags |= 1 <<\n\t\t\t\tFCOE_TCE_TX_WR_RX_RD_CONST_CACHED_SGE_SHIFT;\n\t\t} else {\n\n\t\t\tsgl->mul_sgl.cur_sge_addr.lo = (u32)bd_tbl->bd_tbl_dma;\n\t\t\tsgl->mul_sgl.cur_sge_addr.hi =\n\t\t\t\t\t(u32)((u64)bd_tbl->bd_tbl_dma >> 32);\n\t\t\tsgl->mul_sgl.sgl_size = bd_count;\n\t\t}\n\t} else {\n\t\tsgl->mul_sgl.cur_sge_addr.lo = (u32)bd_tbl->bd_tbl_dma;\n\t\tsgl->mul_sgl.cur_sge_addr.hi =\n\t\t\t\t(u32)((u64)bd_tbl->bd_tbl_dma >> 32);\n\t\tsgl->mul_sgl.sgl_size = bd_count;\n\t}\n}\n\n \nint bnx2fc_setup_task_ctx(struct bnx2fc_hba *hba)\n{\n\tint rc = 0;\n\tstruct regpair *task_ctx_bdt;\n\tdma_addr_t addr;\n\tint task_ctx_arr_sz;\n\tint i;\n\n\t \n\thba->task_ctx_bd_tbl = dma_alloc_coherent(&hba->pcidev->dev,\n\t\t\t\t\t\t  PAGE_SIZE,\n\t\t\t\t\t\t  &hba->task_ctx_bd_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!hba->task_ctx_bd_tbl) {\n\t\tprintk(KERN_ERR PFX \"unable to allocate task context BDT\\n\");\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\t \n\ttask_ctx_arr_sz = (hba->max_tasks / BNX2FC_TASKS_PER_PAGE);\n\thba->task_ctx = kzalloc((task_ctx_arr_sz * sizeof(void *)),\n\t\t\t\t GFP_KERNEL);\n\tif (!hba->task_ctx) {\n\t\tprintk(KERN_ERR PFX \"unable to allocate task context array\\n\");\n\t\trc = -1;\n\t\tgoto out1;\n\t}\n\n\t \n\thba->task_ctx_dma = kmalloc((task_ctx_arr_sz *\n\t\t\t\t\tsizeof(dma_addr_t)), GFP_KERNEL);\n\tif (!hba->task_ctx_dma) {\n\t\tprintk(KERN_ERR PFX \"unable to alloc context mapping array\\n\");\n\t\trc = -1;\n\t\tgoto out2;\n\t}\n\n\ttask_ctx_bdt = (struct regpair *)hba->task_ctx_bd_tbl;\n\tfor (i = 0; i < task_ctx_arr_sz; i++) {\n\n\t\thba->task_ctx[i] = dma_alloc_coherent(&hba->pcidev->dev,\n\t\t\t\t\t\t      PAGE_SIZE,\n\t\t\t\t\t\t      &hba->task_ctx_dma[i],\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!hba->task_ctx[i]) {\n\t\t\tprintk(KERN_ERR PFX \"unable to alloc task context\\n\");\n\t\t\trc = -1;\n\t\t\tgoto out3;\n\t\t}\n\t\taddr = (u64)hba->task_ctx_dma[i];\n\t\ttask_ctx_bdt->hi = cpu_to_le32((u64)addr >> 32);\n\t\ttask_ctx_bdt->lo = cpu_to_le32((u32)addr);\n\t\ttask_ctx_bdt++;\n\t}\n\treturn 0;\n\nout3:\n\tfor (i = 0; i < task_ctx_arr_sz; i++) {\n\t\tif (hba->task_ctx[i]) {\n\n\t\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\thba->task_ctx[i], hba->task_ctx_dma[i]);\n\t\t\thba->task_ctx[i] = NULL;\n\t\t}\n\t}\n\n\tkfree(hba->task_ctx_dma);\n\thba->task_ctx_dma = NULL;\nout2:\n\tkfree(hba->task_ctx);\n\thba->task_ctx = NULL;\nout1:\n\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\thba->task_ctx_bd_tbl, hba->task_ctx_bd_dma);\n\thba->task_ctx_bd_tbl = NULL;\nout:\n\treturn rc;\n}\n\nvoid bnx2fc_free_task_ctx(struct bnx2fc_hba *hba)\n{\n\tint task_ctx_arr_sz;\n\tint i;\n\n\tif (hba->task_ctx_bd_tbl) {\n\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t    hba->task_ctx_bd_tbl,\n\t\t\t\t    hba->task_ctx_bd_dma);\n\t\thba->task_ctx_bd_tbl = NULL;\n\t}\n\n\ttask_ctx_arr_sz = (hba->max_tasks / BNX2FC_TASKS_PER_PAGE);\n\tif (hba->task_ctx) {\n\t\tfor (i = 0; i < task_ctx_arr_sz; i++) {\n\t\t\tif (hba->task_ctx[i]) {\n\t\t\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t\t\t    hba->task_ctx[i],\n\t\t\t\t\t\t    hba->task_ctx_dma[i]);\n\t\t\t\thba->task_ctx[i] = NULL;\n\t\t\t}\n\t\t}\n\t\tkfree(hba->task_ctx);\n\t\thba->task_ctx = NULL;\n\t}\n\n\tkfree(hba->task_ctx_dma);\n\thba->task_ctx_dma = NULL;\n}\n\nstatic void bnx2fc_free_hash_table(struct bnx2fc_hba *hba)\n{\n\tint i;\n\tint segment_count;\n\tu32 *pbl;\n\n\tif (hba->hash_tbl_segments) {\n\n\t\tpbl = hba->hash_tbl_pbl;\n\t\tif (pbl) {\n\t\t\tsegment_count = hba->hash_tbl_segment_count;\n\t\t\tfor (i = 0; i < segment_count; ++i) {\n\t\t\t\tdma_addr_t dma_address;\n\n\t\t\t\tdma_address = le32_to_cpu(*pbl);\n\t\t\t\t++pbl;\n\t\t\t\tdma_address += ((u64)le32_to_cpu(*pbl)) << 32;\n\t\t\t\t++pbl;\n\t\t\t\tdma_free_coherent(&hba->pcidev->dev,\n\t\t\t\t\t\t  BNX2FC_HASH_TBL_CHUNK_SIZE,\n\t\t\t\t\t\t  hba->hash_tbl_segments[i],\n\t\t\t\t\t\t  dma_address);\n\t\t\t}\n\t\t}\n\n\t\tkfree(hba->hash_tbl_segments);\n\t\thba->hash_tbl_segments = NULL;\n\t}\n\n\tif (hba->hash_tbl_pbl) {\n\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t    hba->hash_tbl_pbl,\n\t\t\t\t    hba->hash_tbl_pbl_dma);\n\t\thba->hash_tbl_pbl = NULL;\n\t}\n}\n\nstatic int bnx2fc_allocate_hash_table(struct bnx2fc_hba *hba)\n{\n\tint i;\n\tint hash_table_size;\n\tint segment_count;\n\tint segment_array_size;\n\tint dma_segment_array_size;\n\tdma_addr_t *dma_segment_array;\n\tu32 *pbl;\n\n\thash_table_size = BNX2FC_NUM_MAX_SESS * BNX2FC_MAX_ROWS_IN_HASH_TBL *\n\t\tsizeof(struct fcoe_hash_table_entry);\n\n\tsegment_count = hash_table_size + BNX2FC_HASH_TBL_CHUNK_SIZE - 1;\n\tsegment_count /= BNX2FC_HASH_TBL_CHUNK_SIZE;\n\thba->hash_tbl_segment_count = segment_count;\n\n\tsegment_array_size = segment_count * sizeof(*hba->hash_tbl_segments);\n\thba->hash_tbl_segments = kzalloc(segment_array_size, GFP_KERNEL);\n\tif (!hba->hash_tbl_segments) {\n\t\tprintk(KERN_ERR PFX \"hash table pointers alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdma_segment_array_size = segment_count * sizeof(*dma_segment_array);\n\tdma_segment_array = kzalloc(dma_segment_array_size, GFP_KERNEL);\n\tif (!dma_segment_array) {\n\t\tprintk(KERN_ERR PFX \"hash table pointers (dma) alloc failed\\n\");\n\t\tgoto cleanup_ht;\n\t}\n\n\tfor (i = 0; i < segment_count; ++i) {\n\t\thba->hash_tbl_segments[i] = dma_alloc_coherent(&hba->pcidev->dev,\n\t\t\t\t\t\t\t       BNX2FC_HASH_TBL_CHUNK_SIZE,\n\t\t\t\t\t\t\t       &dma_segment_array[i],\n\t\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!hba->hash_tbl_segments[i]) {\n\t\t\tprintk(KERN_ERR PFX \"hash segment alloc failed\\n\");\n\t\t\tgoto cleanup_dma;\n\t\t}\n\t}\n\n\thba->hash_tbl_pbl = dma_alloc_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t\t       &hba->hash_tbl_pbl_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!hba->hash_tbl_pbl) {\n\t\tprintk(KERN_ERR PFX \"hash table pbl alloc failed\\n\");\n\t\tgoto cleanup_dma;\n\t}\n\n\tpbl = hba->hash_tbl_pbl;\n\tfor (i = 0; i < segment_count; ++i) {\n\t\tu64 paddr = dma_segment_array[i];\n\t\t*pbl = cpu_to_le32((u32) paddr);\n\t\t++pbl;\n\t\t*pbl = cpu_to_le32((u32) (paddr >> 32));\n\t\t++pbl;\n\t}\n\tpbl = hba->hash_tbl_pbl;\n\ti = 0;\n\twhile (*pbl && *(pbl + 1)) {\n\t\t++pbl;\n\t\t++pbl;\n\t\t++i;\n\t}\n\tkfree(dma_segment_array);\n\treturn 0;\n\ncleanup_dma:\n\tfor (i = 0; i < segment_count; ++i) {\n\t\tif (hba->hash_tbl_segments[i])\n\t\t\tdma_free_coherent(&hba->pcidev->dev,\n\t\t\t\t\t    BNX2FC_HASH_TBL_CHUNK_SIZE,\n\t\t\t\t\t    hba->hash_tbl_segments[i],\n\t\t\t\t\t    dma_segment_array[i]);\n\t}\n\n\tkfree(dma_segment_array);\n\ncleanup_ht:\n\tkfree(hba->hash_tbl_segments);\n\thba->hash_tbl_segments = NULL;\n\treturn -ENOMEM;\n}\n\n \nint bnx2fc_setup_fw_resc(struct bnx2fc_hba *hba)\n{\n\tu64 addr;\n\tu32 mem_size;\n\tint i;\n\n\tif (bnx2fc_allocate_hash_table(hba))\n\t\treturn -ENOMEM;\n\n\tmem_size = BNX2FC_NUM_MAX_SESS * sizeof(struct regpair);\n\thba->t2_hash_tbl_ptr = dma_alloc_coherent(&hba->pcidev->dev, mem_size,\n\t\t\t\t\t\t  &hba->t2_hash_tbl_ptr_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!hba->t2_hash_tbl_ptr) {\n\t\tprintk(KERN_ERR PFX \"unable to allocate t2 hash table ptr\\n\");\n\t\tbnx2fc_free_fw_resc(hba);\n\t\treturn -ENOMEM;\n\t}\n\n\tmem_size = BNX2FC_NUM_MAX_SESS *\n\t\t\t\tsizeof(struct fcoe_t2_hash_table_entry);\n\thba->t2_hash_tbl = dma_alloc_coherent(&hba->pcidev->dev, mem_size,\n\t\t\t\t\t      &hba->t2_hash_tbl_dma,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!hba->t2_hash_tbl) {\n\t\tprintk(KERN_ERR PFX \"unable to allocate t2 hash table\\n\");\n\t\tbnx2fc_free_fw_resc(hba);\n\t\treturn -ENOMEM;\n\t}\n\tfor (i = 0; i < BNX2FC_NUM_MAX_SESS; i++) {\n\t\taddr = (unsigned long) hba->t2_hash_tbl_dma +\n\t\t\t ((i+1) * sizeof(struct fcoe_t2_hash_table_entry));\n\t\thba->t2_hash_tbl[i].next.lo = addr & 0xffffffff;\n\t\thba->t2_hash_tbl[i].next.hi = addr >> 32;\n\t}\n\n\thba->dummy_buffer = dma_alloc_coherent(&hba->pcidev->dev,\n\t\t\t\t\t       PAGE_SIZE, &hba->dummy_buf_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!hba->dummy_buffer) {\n\t\tprintk(KERN_ERR PFX \"unable to alloc MP Dummy Buffer\\n\");\n\t\tbnx2fc_free_fw_resc(hba);\n\t\treturn -ENOMEM;\n\t}\n\n\thba->stats_buffer = dma_alloc_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t\t       &hba->stats_buf_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!hba->stats_buffer) {\n\t\tprintk(KERN_ERR PFX \"unable to alloc Stats Buffer\\n\");\n\t\tbnx2fc_free_fw_resc(hba);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid bnx2fc_free_fw_resc(struct bnx2fc_hba *hba)\n{\n\tu32 mem_size;\n\n\tif (hba->stats_buffer) {\n\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t  hba->stats_buffer, hba->stats_buf_dma);\n\t\thba->stats_buffer = NULL;\n\t}\n\n\tif (hba->dummy_buffer) {\n\t\tdma_free_coherent(&hba->pcidev->dev, PAGE_SIZE,\n\t\t\t\t  hba->dummy_buffer, hba->dummy_buf_dma);\n\t\thba->dummy_buffer = NULL;\n\t}\n\n\tif (hba->t2_hash_tbl_ptr) {\n\t\tmem_size = BNX2FC_NUM_MAX_SESS * sizeof(struct regpair);\n\t\tdma_free_coherent(&hba->pcidev->dev, mem_size,\n\t\t\t\t    hba->t2_hash_tbl_ptr,\n\t\t\t\t    hba->t2_hash_tbl_ptr_dma);\n\t\thba->t2_hash_tbl_ptr = NULL;\n\t}\n\n\tif (hba->t2_hash_tbl) {\n\t\tmem_size = BNX2FC_NUM_MAX_SESS *\n\t\t\t    sizeof(struct fcoe_t2_hash_table_entry);\n\t\tdma_free_coherent(&hba->pcidev->dev, mem_size,\n\t\t\t\t    hba->t2_hash_tbl, hba->t2_hash_tbl_dma);\n\t\thba->t2_hash_tbl = NULL;\n\t}\n\tbnx2fc_free_hash_table(hba);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}