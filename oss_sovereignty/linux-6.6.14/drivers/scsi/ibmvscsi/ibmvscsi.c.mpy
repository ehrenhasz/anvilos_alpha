{
  "module_name": "ibmvscsi.c",
  "hash_id": "c6bafb41c39895d0098efbcc40d95121a06bc9527b6abbb704be50ba80fd268f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/ibmvscsi/ibmvscsi.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/dma-mapping.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/of.h>\n#include <linux/pm.h>\n#include <linux/kthread.h>\n#include <asm/firmware.h>\n#include <asm/vio.h>\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_transport_srp.h>\n#include \"ibmvscsi.h\"\n\n \nstatic int max_id = 64;\nstatic int max_channel = 3;\nstatic int init_timeout = 300;\nstatic int login_timeout = 60;\nstatic int info_timeout = 30;\nstatic int abort_timeout = 60;\nstatic int reset_timeout = 60;\nstatic int max_requests = IBMVSCSI_MAX_REQUESTS_DEFAULT;\nstatic int max_events = IBMVSCSI_MAX_REQUESTS_DEFAULT + 2;\nstatic int fast_fail = 1;\nstatic int client_reserve = 1;\nstatic char partition_name[96] = \"UNKNOWN\";\nstatic unsigned int partition_number = -1;\nstatic LIST_HEAD(ibmvscsi_head);\nstatic DEFINE_SPINLOCK(ibmvscsi_driver_lock);\n\nstatic struct scsi_transport_template *ibmvscsi_transport_template;\n\n#define IBMVSCSI_VERSION \"1.5.9\"\n\nMODULE_DESCRIPTION(\"IBM Virtual SCSI\");\nMODULE_AUTHOR(\"Dave Boutcher\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(IBMVSCSI_VERSION);\n\nmodule_param_named(max_id, max_id, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(max_id, \"Largest ID value for each channel [Default=64]\");\nmodule_param_named(max_channel, max_channel, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(max_channel, \"Largest channel value [Default=3]\");\nmodule_param_named(init_timeout, init_timeout, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(init_timeout, \"Initialization timeout in seconds\");\nmodule_param_named(max_requests, max_requests, int, S_IRUGO);\nMODULE_PARM_DESC(max_requests, \"Maximum requests for this adapter\");\nmodule_param_named(fast_fail, fast_fail, int, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(fast_fail, \"Enable fast fail. [Default=1]\");\nmodule_param_named(client_reserve, client_reserve, int, S_IRUGO );\nMODULE_PARM_DESC(client_reserve, \"Attempt client managed reserve/release\");\n\nstatic void ibmvscsi_handle_crq(struct viosrp_crq *crq,\n\t\t\t\tstruct ibmvscsi_host_data *hostdata);\n\n \n \nstatic irqreturn_t ibmvscsi_handle_event(int irq, void *dev_instance)\n{\n\tstruct ibmvscsi_host_data *hostdata =\n\t    (struct ibmvscsi_host_data *)dev_instance;\n\tvio_disable_interrupts(to_vio_dev(hostdata->dev));\n\ttasklet_schedule(&hostdata->srp_task);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void ibmvscsi_release_crq_queue(struct crq_queue *queue,\n\t\t\t\t       struct ibmvscsi_host_data *hostdata,\n\t\t\t\t       int max_requests)\n{\n\tlong rc = 0;\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\tfree_irq(vdev->irq, (void *)hostdata);\n\ttasklet_kill(&hostdata->srp_task);\n\tdo {\n\t\tif (rc)\n\t\t\tmsleep(100);\n\t\trc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\n\t} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\n\tdma_unmap_single(hostdata->dev,\n\t\t\t queue->msg_token,\n\t\t\t queue->size * sizeof(*queue->msgs), DMA_BIDIRECTIONAL);\n\tfree_page((unsigned long)queue->msgs);\n}\n\n \nstatic struct viosrp_crq *crq_queue_next_crq(struct crq_queue *queue)\n{\n\tstruct viosrp_crq *crq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->lock, flags);\n\tcrq = &queue->msgs[queue->cur];\n\tif (crq->valid != VIOSRP_CRQ_FREE) {\n\t\tif (++queue->cur == queue->size)\n\t\t\tqueue->cur = 0;\n\n\t\t \n\t\trmb();\n\t} else\n\t\tcrq = NULL;\n\tspin_unlock_irqrestore(&queue->lock, flags);\n\n\treturn crq;\n}\n\n \nstatic int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata,\n\t\t\t     u64 word1, u64 word2)\n{\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\n\t \n\tmb();\n\treturn plpar_hcall_norets(H_SEND_CRQ, vdev->unit_address, word1, word2);\n}\n\n \nstatic void ibmvscsi_task(void *data)\n{\n\tstruct ibmvscsi_host_data *hostdata = (struct ibmvscsi_host_data *)data;\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\tstruct viosrp_crq *crq;\n\tint done = 0;\n\n\twhile (!done) {\n\t\t \n\t\twhile ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {\n\t\t\tibmvscsi_handle_crq(crq, hostdata);\n\t\t\tcrq->valid = VIOSRP_CRQ_FREE;\n\t\t\twmb();\n\t\t}\n\n\t\tvio_enable_interrupts(vdev);\n\t\tcrq = crq_queue_next_crq(&hostdata->queue);\n\t\tif (crq != NULL) {\n\t\t\tvio_disable_interrupts(vdev);\n\t\t\tibmvscsi_handle_crq(crq, hostdata);\n\t\t\tcrq->valid = VIOSRP_CRQ_FREE;\n\t\t\twmb();\n\t\t} else {\n\t\t\tdone = 1;\n\t\t}\n\t}\n}\n\nstatic void gather_partition_info(void)\n{\n\tconst char *ppartition_name;\n\tconst __be32 *p_number_ptr;\n\n\t \n\tif (!of_root)\n\t\treturn;\n\n\tof_node_get(of_root);\n\n\tppartition_name = of_get_property(of_root, \"ibm,partition-name\", NULL);\n\tif (ppartition_name)\n\t\tstrscpy(partition_name, ppartition_name,\n\t\t\t\tsizeof(partition_name));\n\tp_number_ptr = of_get_property(of_root, \"ibm,partition-no\", NULL);\n\tif (p_number_ptr)\n\t\tpartition_number = of_read_number(p_number_ptr, 1);\n\tof_node_put(of_root);\n}\n\nstatic void set_adapter_info(struct ibmvscsi_host_data *hostdata)\n{\n\tmemset(&hostdata->madapter_info, 0x00,\n\t\t\tsizeof(hostdata->madapter_info));\n\n\tdev_info(hostdata->dev, \"SRP_VERSION: %s\\n\", SRP_VERSION);\n\tstrcpy(hostdata->madapter_info.srp_version, SRP_VERSION);\n\n\tstrncpy(hostdata->madapter_info.partition_name, partition_name,\n\t\t\tsizeof(hostdata->madapter_info.partition_name));\n\n\thostdata->madapter_info.partition_number =\n\t\t\t\t\tcpu_to_be32(partition_number);\n\n\thostdata->madapter_info.mad_version = cpu_to_be32(SRP_MAD_VERSION_1);\n\thostdata->madapter_info.os_type = cpu_to_be32(SRP_MAD_OS_LINUX);\n}\n\n \nstatic int ibmvscsi_reset_crq_queue(struct crq_queue *queue,\n\t\t\t\t    struct ibmvscsi_host_data *hostdata)\n{\n\tint rc = 0;\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\n\t \n\tdo {\n\t\tif (rc)\n\t\t\tmsleep(100);\n\t\trc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\n\t} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\n\n\t \n\tmemset(queue->msgs, 0x00, PAGE_SIZE);\n\tqueue->cur = 0;\n\n\tset_adapter_info(hostdata);\n\n\t \n\trc = plpar_hcall_norets(H_REG_CRQ,\n\t\t\t\tvdev->unit_address,\n\t\t\t\tqueue->msg_token, PAGE_SIZE);\n\tif (rc == H_CLOSED) {\n\t\t \n\t\tdev_warn(hostdata->dev, \"Partner adapter not ready\\n\");\n\t} else if (rc != 0) {\n\t\tdev_warn(hostdata->dev, \"couldn't register crq--rc 0x%x\\n\", rc);\n\t}\n\treturn rc;\n}\n\n \nstatic int ibmvscsi_init_crq_queue(struct crq_queue *queue,\n\t\t\t\t   struct ibmvscsi_host_data *hostdata,\n\t\t\t\t   int max_requests)\n{\n\tint rc;\n\tint retrc;\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\n\tqueue->msgs = (struct viosrp_crq *)get_zeroed_page(GFP_KERNEL);\n\n\tif (!queue->msgs)\n\t\tgoto malloc_failed;\n\tqueue->size = PAGE_SIZE / sizeof(*queue->msgs);\n\n\tqueue->msg_token = dma_map_single(hostdata->dev, queue->msgs,\n\t\t\t\t\t  queue->size * sizeof(*queue->msgs),\n\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\n\tif (dma_mapping_error(hostdata->dev, queue->msg_token))\n\t\tgoto map_failed;\n\n\tgather_partition_info();\n\tset_adapter_info(hostdata);\n\n\tretrc = rc = plpar_hcall_norets(H_REG_CRQ,\n\t\t\t\tvdev->unit_address,\n\t\t\t\tqueue->msg_token, PAGE_SIZE);\n\tif (rc == H_RESOURCE)\n\t\t \n\t\trc = ibmvscsi_reset_crq_queue(queue,\n\t\t\t\t\t      hostdata);\n\n\tif (rc == H_CLOSED) {\n\t\t \n\t\tdev_warn(hostdata->dev, \"Partner adapter not ready\\n\");\n\t\tretrc = 0;\n\t} else if (rc != 0) {\n\t\tdev_warn(hostdata->dev, \"Error %d opening adapter\\n\", rc);\n\t\tgoto reg_crq_failed;\n\t}\n\n\tqueue->cur = 0;\n\tspin_lock_init(&queue->lock);\n\n\ttasklet_init(&hostdata->srp_task, (void *)ibmvscsi_task,\n\t\t     (unsigned long)hostdata);\n\n\tif (request_irq(vdev->irq,\n\t\t\tibmvscsi_handle_event,\n\t\t\t0, \"ibmvscsi\", (void *)hostdata) != 0) {\n\t\tdev_err(hostdata->dev, \"couldn't register irq 0x%x\\n\",\n\t\t\tvdev->irq);\n\t\tgoto req_irq_failed;\n\t}\n\n\trc = vio_enable_interrupts(vdev);\n\tif (rc != 0) {\n\t\tdev_err(hostdata->dev, \"Error %d enabling interrupts!!!\\n\", rc);\n\t\tgoto req_irq_failed;\n\t}\n\n\treturn retrc;\n\n      req_irq_failed:\n\ttasklet_kill(&hostdata->srp_task);\n\trc = 0;\n\tdo {\n\t\tif (rc)\n\t\t\tmsleep(100);\n\t\trc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\n\t} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\n      reg_crq_failed:\n\tdma_unmap_single(hostdata->dev,\n\t\t\t queue->msg_token,\n\t\t\t queue->size * sizeof(*queue->msgs), DMA_BIDIRECTIONAL);\n      map_failed:\n\tfree_page((unsigned long)queue->msgs);\n      malloc_failed:\n\treturn -1;\n}\n\n \nstatic int ibmvscsi_reenable_crq_queue(struct crq_queue *queue,\n\t\t\t\t       struct ibmvscsi_host_data *hostdata)\n{\n\tint rc = 0;\n\tstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\n\n\tset_adapter_info(hostdata);\n\n\t \n\tdo {\n\t\tif (rc)\n\t\t\tmsleep(100);\n\t\trc = plpar_hcall_norets(H_ENABLE_CRQ, vdev->unit_address);\n\t} while ((rc == H_IN_PROGRESS) || (rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\n\n\tif (rc)\n\t\tdev_err(hostdata->dev, \"Error %d enabling adapter\\n\", rc);\n\treturn rc;\n}\n\n \n \nstatic int initialize_event_pool(struct event_pool *pool,\n\t\t\t\t int size, struct ibmvscsi_host_data *hostdata)\n{\n\tint i;\n\n\tpool->size = size;\n\tpool->next = 0;\n\tpool->events = kcalloc(pool->size, sizeof(*pool->events), GFP_KERNEL);\n\tif (!pool->events)\n\t\treturn -ENOMEM;\n\n\tpool->iu_storage =\n\t    dma_alloc_coherent(hostdata->dev,\n\t\t\t       pool->size * sizeof(*pool->iu_storage),\n\t\t\t       &pool->iu_token, GFP_KERNEL);\n\tif (!pool->iu_storage) {\n\t\tkfree(pool->events);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < pool->size; ++i) {\n\t\tstruct srp_event_struct *evt = &pool->events[i];\n\t\tmemset(&evt->crq, 0x00, sizeof(evt->crq));\n\t\tatomic_set(&evt->free, 1);\n\t\tevt->crq.valid = VIOSRP_CRQ_CMD_RSP;\n\t\tevt->crq.IU_length = cpu_to_be16(sizeof(*evt->xfer_iu));\n\t\tevt->crq.IU_data_ptr = cpu_to_be64(pool->iu_token +\n\t\t\tsizeof(*evt->xfer_iu) * i);\n\t\tevt->xfer_iu = pool->iu_storage + i;\n\t\tevt->hostdata = hostdata;\n\t\tevt->ext_list = NULL;\n\t\tevt->ext_list_token = 0;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void release_event_pool(struct event_pool *pool,\n\t\t\t       struct ibmvscsi_host_data *hostdata)\n{\n\tint i, in_use = 0;\n\tfor (i = 0; i < pool->size; ++i) {\n\t\tif (atomic_read(&pool->events[i].free) != 1)\n\t\t\t++in_use;\n\t\tif (pool->events[i].ext_list) {\n\t\t\tdma_free_coherent(hostdata->dev,\n\t\t\t\t  SG_ALL * sizeof(struct srp_direct_buf),\n\t\t\t\t  pool->events[i].ext_list,\n\t\t\t\t  pool->events[i].ext_list_token);\n\t\t}\n\t}\n\tif (in_use)\n\t\tdev_warn(hostdata->dev, \"releasing event pool with %d \"\n\t\t\t \"events still in use?\\n\", in_use);\n\tkfree(pool->events);\n\tdma_free_coherent(hostdata->dev,\n\t\t\t  pool->size * sizeof(*pool->iu_storage),\n\t\t\t  pool->iu_storage, pool->iu_token);\n}\n\n \nstatic int valid_event_struct(struct event_pool *pool,\n\t\t\t\tstruct srp_event_struct *evt)\n{\n\tint index = evt - pool->events;\n\tif (index < 0 || index >= pool->size)\t \n\t\treturn 0;\n\tif (evt != pool->events + index)\t \n\t\treturn 0;\n\treturn 1;\n}\n\n \nstatic void free_event_struct(struct event_pool *pool,\n\t\t\t\t       struct srp_event_struct *evt)\n{\n\tif (!valid_event_struct(pool, evt)) {\n\t\tdev_err(evt->hostdata->dev, \"Freeing invalid event_struct %p \"\n\t\t\t\"(not in pool %p)\\n\", evt, pool->events);\n\t\treturn;\n\t}\n\tif (atomic_inc_return(&evt->free) != 1) {\n\t\tdev_err(evt->hostdata->dev, \"Freeing event_struct %p \"\n\t\t\t\"which is not in use!\\n\", evt);\n\t\treturn;\n\t}\n}\n\n \nstatic struct srp_event_struct *get_event_struct(struct event_pool *pool)\n{\n\tint i;\n\tint poolsize = pool->size;\n\tint offset = pool->next;\n\n\tfor (i = 0; i < poolsize; i++) {\n\t\toffset = (offset + 1) % poolsize;\n\t\tif (!atomic_dec_if_positive(&pool->events[offset].free)) {\n\t\t\tpool->next = offset;\n\t\t\treturn &pool->events[offset];\n\t\t}\n\t}\n\n\tprintk(KERN_ERR \"ibmvscsi: found no event struct in pool!\\n\");\n\treturn NULL;\n}\n\n \nstatic void init_event_struct(struct srp_event_struct *evt_struct,\n\t\t\t      void (*done) (struct srp_event_struct *),\n\t\t\t      u8 format,\n\t\t\t      int timeout)\n{\n\tevt_struct->cmnd = NULL;\n\tevt_struct->cmnd_done = NULL;\n\tevt_struct->sync_srp = NULL;\n\tevt_struct->crq.format = format;\n\tevt_struct->crq.timeout = cpu_to_be16(timeout);\n\tevt_struct->done = done;\n}\n\n \n\n \nstatic void set_srp_direction(struct scsi_cmnd *cmd,\n\t\t\t      struct srp_cmd *srp_cmd, \n\t\t\t      int numbuf)\n{\n\tu8 fmt;\n\n\tif (numbuf == 0)\n\t\treturn;\n\t\n\tif (numbuf == 1)\n\t\tfmt = SRP_DATA_DESC_DIRECT;\n\telse {\n\t\tfmt = SRP_DATA_DESC_INDIRECT;\n\t\tnumbuf = min(numbuf, MAX_INDIRECT_BUFS);\n\n\t\tif (cmd->sc_data_direction == DMA_TO_DEVICE)\n\t\t\tsrp_cmd->data_out_desc_cnt = numbuf;\n\t\telse\n\t\t\tsrp_cmd->data_in_desc_cnt = numbuf;\n\t}\n\n\tif (cmd->sc_data_direction == DMA_TO_DEVICE)\n\t\tsrp_cmd->buf_fmt = fmt << 4;\n\telse\n\t\tsrp_cmd->buf_fmt = fmt;\n}\n\n \nstatic void unmap_cmd_data(struct srp_cmd *cmd,\n\t\t\t   struct srp_event_struct *evt_struct,\n\t\t\t   struct device *dev)\n{\n\tu8 out_fmt, in_fmt;\n\n\tout_fmt = cmd->buf_fmt >> 4;\n\tin_fmt = cmd->buf_fmt & ((1U << 4) - 1);\n\n\tif (out_fmt == SRP_NO_DATA_DESC && in_fmt == SRP_NO_DATA_DESC)\n\t\treturn;\n\n\tif (evt_struct->cmnd)\n\t\tscsi_dma_unmap(evt_struct->cmnd);\n}\n\nstatic int map_sg_list(struct scsi_cmnd *cmd, int nseg,\n\t\t       struct srp_direct_buf *md)\n{\n\tint i;\n\tstruct scatterlist *sg;\n\tu64 total_length = 0;\n\n\tscsi_for_each_sg(cmd, sg, nseg, i) {\n\t\tstruct srp_direct_buf *descr = md + i;\n\t\tdescr->va = cpu_to_be64(sg_dma_address(sg));\n\t\tdescr->len = cpu_to_be32(sg_dma_len(sg));\n\t\tdescr->key = 0;\n\t\ttotal_length += sg_dma_len(sg);\n \t}\n\treturn total_length;\n}\n\n \nstatic int map_sg_data(struct scsi_cmnd *cmd,\n\t\t       struct srp_event_struct *evt_struct,\n\t\t       struct srp_cmd *srp_cmd, struct device *dev)\n{\n\n\tint sg_mapped;\n\tu64 total_length = 0;\n\tstruct srp_direct_buf *data =\n\t\t(struct srp_direct_buf *) srp_cmd->add_data;\n\tstruct srp_indirect_buf *indirect =\n\t\t(struct srp_indirect_buf *) data;\n\n\tsg_mapped = scsi_dma_map(cmd);\n\tif (!sg_mapped)\n\t\treturn 1;\n\telse if (sg_mapped < 0)\n\t\treturn 0;\n\n\tset_srp_direction(cmd, srp_cmd, sg_mapped);\n\n\t \n\tif (sg_mapped == 1) {\n\t\tmap_sg_list(cmd, sg_mapped, data);\n\t\treturn 1;\n\t}\n\n\tindirect->table_desc.va = 0;\n\tindirect->table_desc.len = cpu_to_be32(sg_mapped *\n\t\t\t\t\t       sizeof(struct srp_direct_buf));\n\tindirect->table_desc.key = 0;\n\n\tif (sg_mapped <= MAX_INDIRECT_BUFS) {\n\t\ttotal_length = map_sg_list(cmd, sg_mapped,\n\t\t\t\t\t   &indirect->desc_list[0]);\n\t\tindirect->len = cpu_to_be32(total_length);\n\t\treturn 1;\n\t}\n\n\t \n\tif (!evt_struct->ext_list) {\n\t\tevt_struct->ext_list = dma_alloc_coherent(dev,\n\t\t\t\t\t   SG_ALL * sizeof(struct srp_direct_buf),\n\t\t\t\t\t   &evt_struct->ext_list_token, 0);\n\t\tif (!evt_struct->ext_list) {\n\t\t\tif (!firmware_has_feature(FW_FEATURE_CMO))\n\t\t\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t\t            \"Can't allocate memory \"\n\t\t\t\t            \"for indirect table\\n\");\n\t\t\tscsi_dma_unmap(cmd);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\ttotal_length = map_sg_list(cmd, sg_mapped, evt_struct->ext_list);\n\n\tindirect->len = cpu_to_be32(total_length);\n\tindirect->table_desc.va = cpu_to_be64(evt_struct->ext_list_token);\n\tindirect->table_desc.len = cpu_to_be32(sg_mapped *\n\t\t\t\t\t       sizeof(indirect->desc_list[0]));\n\tmemcpy(indirect->desc_list, evt_struct->ext_list,\n\t       MAX_INDIRECT_BUFS * sizeof(struct srp_direct_buf));\n \treturn 1;\n}\n\n \nstatic int map_data_for_srp_cmd(struct scsi_cmnd *cmd,\n\t\t\t\tstruct srp_event_struct *evt_struct,\n\t\t\t\tstruct srp_cmd *srp_cmd, struct device *dev)\n{\n\tswitch (cmd->sc_data_direction) {\n\tcase DMA_FROM_DEVICE:\n\tcase DMA_TO_DEVICE:\n\t\tbreak;\n\tcase DMA_NONE:\n\t\treturn 1;\n\tcase DMA_BIDIRECTIONAL:\n\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t    \"Can't map DMA_BIDIRECTIONAL to read/write\\n\");\n\t\treturn 0;\n\tdefault:\n\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t    \"Unknown data direction 0x%02x; can't map!\\n\",\n\t\t\t    cmd->sc_data_direction);\n\t\treturn 0;\n\t}\n\n\treturn map_sg_data(cmd, evt_struct, srp_cmd, dev);\n}\n\n \nstatic void purge_requests(struct ibmvscsi_host_data *hostdata, int error_code)\n{\n\tstruct srp_event_struct *evt;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\twhile (!list_empty(&hostdata->sent)) {\n\t\tevt = list_first_entry(&hostdata->sent, struct srp_event_struct, list);\n\t\tlist_del(&evt->list);\n\t\tdel_timer(&evt->timer);\n\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\tif (evt->cmnd) {\n\t\t\tevt->cmnd->result = (error_code << 16);\n\t\t\tunmap_cmd_data(&evt->iu.srp.cmd, evt,\n\t\t\t\t       evt->hostdata->dev);\n\t\t\tif (evt->cmnd_done)\n\t\t\t\tevt->cmnd_done(evt->cmnd);\n\t\t} else if (evt->done && evt->crq.format != VIOSRP_MAD_FORMAT &&\n\t\t\t   evt->iu.srp.login_req.opcode != SRP_LOGIN_REQ)\n\t\t\tevt->done(evt);\n\t\tfree_event_struct(&evt->hostdata->pool, evt);\n\t\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\t}\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n}\n\n \nstatic void ibmvscsi_set_request_limit(struct ibmvscsi_host_data *hostdata, int limit)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tatomic_set(&hostdata->request_limit, limit);\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n}\n\n \nstatic void ibmvscsi_reset_host(struct ibmvscsi_host_data *hostdata)\n{\n\tscsi_block_requests(hostdata->host);\n\tibmvscsi_set_request_limit(hostdata, 0);\n\n\tpurge_requests(hostdata, DID_ERROR);\n\thostdata->action = IBMVSCSI_HOST_ACTION_RESET;\n\twake_up(&hostdata->work_wait_q);\n}\n\n \nstatic void ibmvscsi_timeout(struct timer_list *t)\n{\n\tstruct srp_event_struct *evt_struct = from_timer(evt_struct, t, timer);\n\tstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\n\n\tdev_err(hostdata->dev, \"Command timed out (%x). Resetting connection\\n\",\n\t\tevt_struct->iu.srp.cmd.opcode);\n\n\tibmvscsi_reset_host(hostdata);\n}\n\n\n \n \nstatic int ibmvscsi_send_srp_event(struct srp_event_struct *evt_struct,\n\t\t\t\t   struct ibmvscsi_host_data *hostdata,\n\t\t\t\t   unsigned long timeout)\n{\n\t__be64 *crq_as_u64 = (__be64 *)&evt_struct->crq;\n\tint request_status = 0;\n\tint rc;\n\tint srp_req = 0;\n\n\t \n\tif (evt_struct->crq.format == VIOSRP_SRP_FORMAT) {\n\t\tsrp_req = 1;\n\t\trequest_status =\n\t\t\tatomic_dec_if_positive(&hostdata->request_limit);\n\t\t \n\t\tif (request_status < -1)\n\t\t\tgoto send_error;\n\t\t \n\t\t \n\t\telse if (request_status == -1 &&\n\t\t         evt_struct->iu.srp.login_req.opcode != SRP_LOGIN_REQ)\n\t\t\tgoto send_busy;\n\t\t \n\t\telse if (request_status < 2 &&\n\t\t         evt_struct->iu.srp.cmd.opcode != SRP_TSK_MGMT) {\n\t\t\t \n\t\t\tint server_limit = request_status;\n\t\t\tstruct srp_event_struct *tmp_evt;\n\n\t\t\tlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\n\t\t\t\tserver_limit++;\n\t\t\t}\n\n\t\t\tif (server_limit > 2)\n\t\t\t\tgoto send_busy;\n\t\t}\n\t}\n\n\t \n\t*evt_struct->xfer_iu = evt_struct->iu;\n\tevt_struct->xfer_iu->srp.rsp.tag = (u64)evt_struct;\n\n\t \n\tlist_add_tail(&evt_struct->list, &hostdata->sent);\n\n\ttimer_setup(&evt_struct->timer, ibmvscsi_timeout, 0);\n\tif (timeout) {\n\t\tevt_struct->timer.expires = jiffies + (timeout * HZ);\n\t\tadd_timer(&evt_struct->timer);\n\t}\n\n\trc = ibmvscsi_send_crq(hostdata, be64_to_cpu(crq_as_u64[0]),\n\t\t\t       be64_to_cpu(crq_as_u64[1]));\n\tif (rc != 0) {\n\t\tlist_del(&evt_struct->list);\n\t\tdel_timer(&evt_struct->timer);\n\n\t\t \n\t\tif (rc == H_CLOSED) {\n\t\t\tdev_warn(hostdata->dev, \"send warning. \"\n\t\t\t         \"Receive queue closed, will retry.\\n\");\n\t\t\tgoto send_busy;\n\t\t}\n\t\tdev_err(hostdata->dev, \"send error %d\\n\", rc);\n\t\tif (srp_req)\n\t\t\tatomic_inc(&hostdata->request_limit);\n\t\tgoto send_error;\n\t}\n\n\treturn 0;\n\n send_busy:\n\tunmap_cmd_data(&evt_struct->iu.srp.cmd, evt_struct, hostdata->dev);\n\n\tfree_event_struct(&hostdata->pool, evt_struct);\n\tif (srp_req && request_status != -1)\n\t\tatomic_inc(&hostdata->request_limit);\n\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n send_error:\n\tunmap_cmd_data(&evt_struct->iu.srp.cmd, evt_struct, hostdata->dev);\n\n\tif (evt_struct->cmnd != NULL) {\n\t\tevt_struct->cmnd->result = DID_ERROR << 16;\n\t\tevt_struct->cmnd_done(evt_struct->cmnd);\n\t} else if (evt_struct->done)\n\t\tevt_struct->done(evt_struct);\n\n\tfree_event_struct(&hostdata->pool, evt_struct);\n\treturn 0;\n}\n\n \nstatic void handle_cmd_rsp(struct srp_event_struct *evt_struct)\n{\n\tstruct srp_rsp *rsp = &evt_struct->xfer_iu->srp.rsp;\n\tstruct scsi_cmnd *cmnd = evt_struct->cmnd;\n\n\tif (unlikely(rsp->opcode != SRP_RSP)) {\n\t\tif (printk_ratelimit())\n\t\t\tdev_warn(evt_struct->hostdata->dev,\n\t\t\t\t \"bad SRP RSP type %#02x\\n\", rsp->opcode);\n\t}\n\t\n\tif (cmnd) {\n\t\tcmnd->result |= rsp->status;\n\t\tif (scsi_status_is_check_condition(cmnd->result))\n\t\t\tmemcpy(cmnd->sense_buffer,\n\t\t\t       rsp->data,\n\t\t\t       be32_to_cpu(rsp->sense_data_len));\n\t\tunmap_cmd_data(&evt_struct->iu.srp.cmd, \n\t\t\t       evt_struct, \n\t\t\t       evt_struct->hostdata->dev);\n\n\t\tif (rsp->flags & SRP_RSP_FLAG_DOOVER)\n\t\t\tscsi_set_resid(cmnd,\n\t\t\t\t       be32_to_cpu(rsp->data_out_res_cnt));\n\t\telse if (rsp->flags & SRP_RSP_FLAG_DIOVER)\n\t\t\tscsi_set_resid(cmnd, be32_to_cpu(rsp->data_in_res_cnt));\n\t}\n\n\tif (evt_struct->cmnd_done)\n\t\tevt_struct->cmnd_done(cmnd);\n}\n\n \nstatic inline u16 lun_from_dev(struct scsi_device *dev)\n{\n\treturn (0x2 << 14) | (dev->id << 8) | (dev->channel << 5) | dev->lun;\n}\n\n \nstatic int ibmvscsi_queuecommand_lck(struct scsi_cmnd *cmnd)\n{\n\tvoid (*done)(struct scsi_cmnd *) = scsi_done;\n\tstruct srp_cmd *srp_cmd;\n\tstruct srp_event_struct *evt_struct;\n\tstruct srp_indirect_buf *indirect;\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(cmnd->device->host);\n\tu16 lun = lun_from_dev(cmnd->device);\n\tu8 out_fmt, in_fmt;\n\n\tcmnd->result = (DID_OK << 16);\n\tevt_struct = get_event_struct(&hostdata->pool);\n\tif (!evt_struct)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\t \n\tBUILD_BUG_ON(sizeof(evt_struct->iu.srp) != SRP_MAX_IU_LEN);\n\tmemset(&evt_struct->iu.srp, 0x00, sizeof(evt_struct->iu.srp));\n\tsrp_cmd = &evt_struct->iu.srp.cmd;\n\tsrp_cmd->opcode = SRP_CMD;\n\tmemcpy(srp_cmd->cdb, cmnd->cmnd, sizeof(srp_cmd->cdb));\n\tint_to_scsilun(lun, &srp_cmd->lun);\n\n\tif (!map_data_for_srp_cmd(cmnd, evt_struct, srp_cmd, hostdata->dev)) {\n\t\tif (!firmware_has_feature(FW_FEATURE_CMO))\n\t\t\tsdev_printk(KERN_ERR, cmnd->device,\n\t\t\t            \"couldn't convert cmd to srp_cmd\\n\");\n\t\tfree_event_struct(&hostdata->pool, evt_struct);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tinit_event_struct(evt_struct,\n\t\t\t  handle_cmd_rsp,\n\t\t\t  VIOSRP_SRP_FORMAT,\n\t\t\t  scsi_cmd_to_rq(cmnd)->timeout / HZ);\n\n\tevt_struct->cmnd = cmnd;\n\tevt_struct->cmnd_done = done;\n\n\t \n\tindirect = (struct srp_indirect_buf *) srp_cmd->add_data;\n\tout_fmt = srp_cmd->buf_fmt >> 4;\n\tin_fmt = srp_cmd->buf_fmt & ((1U << 4) - 1);\n\tif ((in_fmt == SRP_DATA_DESC_INDIRECT ||\n\t     out_fmt == SRP_DATA_DESC_INDIRECT) &&\n\t    indirect->table_desc.va == 0) {\n\t\tindirect->table_desc.va =\n\t\t\tcpu_to_be64(be64_to_cpu(evt_struct->crq.IU_data_ptr) +\n\t\t\toffsetof(struct srp_cmd, add_data) +\n\t\t\toffsetof(struct srp_indirect_buf, desc_list));\n\t}\n\n\treturn ibmvscsi_send_srp_event(evt_struct, hostdata, 0);\n}\n\nstatic DEF_SCSI_QCMD(ibmvscsi_queuecommand)\n\n \n\n \nstatic int map_persist_bufs(struct ibmvscsi_host_data *hostdata)\n{\n\n\thostdata->caps_addr = dma_map_single(hostdata->dev, &hostdata->caps,\n\t\t\t\t\t     sizeof(hostdata->caps), DMA_BIDIRECTIONAL);\n\n\tif (dma_mapping_error(hostdata->dev, hostdata->caps_addr)) {\n\t\tdev_err(hostdata->dev, \"Unable to map capabilities buffer!\\n\");\n\t\treturn 1;\n\t}\n\n\thostdata->adapter_info_addr = dma_map_single(hostdata->dev,\n\t\t\t\t\t\t     &hostdata->madapter_info,\n\t\t\t\t\t\t     sizeof(hostdata->madapter_info),\n\t\t\t\t\t\t     DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(hostdata->dev, hostdata->adapter_info_addr)) {\n\t\tdev_err(hostdata->dev, \"Unable to map adapter info buffer!\\n\");\n\t\tdma_unmap_single(hostdata->dev, hostdata->caps_addr,\n\t\t\t\t sizeof(hostdata->caps), DMA_BIDIRECTIONAL);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void unmap_persist_bufs(struct ibmvscsi_host_data *hostdata)\n{\n\tdma_unmap_single(hostdata->dev, hostdata->caps_addr,\n\t\t\t sizeof(hostdata->caps), DMA_BIDIRECTIONAL);\n\n\tdma_unmap_single(hostdata->dev, hostdata->adapter_info_addr,\n\t\t\t sizeof(hostdata->madapter_info), DMA_BIDIRECTIONAL);\n}\n\n \nstatic void login_rsp(struct srp_event_struct *evt_struct)\n{\n\tstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\n\tswitch (evt_struct->xfer_iu->srp.login_rsp.opcode) {\n\tcase SRP_LOGIN_RSP:\t \n\t\tbreak;\n\tcase SRP_LOGIN_REJ:\t \n\t\tdev_info(hostdata->dev, \"SRP_LOGIN_REJ reason %u\\n\",\n\t\t\t evt_struct->xfer_iu->srp.login_rej.reason);\n\t\t \n\t\tibmvscsi_set_request_limit(hostdata, -1);\n\t\treturn;\n\tdefault:\n\t\tdev_err(hostdata->dev, \"Invalid login response typecode 0x%02x!\\n\",\n\t\t\tevt_struct->xfer_iu->srp.login_rsp.opcode);\n\t\t \n\t\tibmvscsi_set_request_limit(hostdata, -1);\n\t\treturn;\n\t}\n\n\tdev_info(hostdata->dev, \"SRP_LOGIN succeeded\\n\");\n\thostdata->client_migrated = 0;\n\n\t \n\tibmvscsi_set_request_limit(hostdata,\n\t\t   be32_to_cpu(evt_struct->xfer_iu->srp.login_rsp.req_lim_delta));\n\n\t \n\thostdata->action = IBMVSCSI_HOST_ACTION_UNBLOCK;\n\twake_up(&hostdata->work_wait_q);\n}\n\n \nstatic int send_srp_login(struct ibmvscsi_host_data *hostdata)\n{\n\tint rc;\n\tunsigned long flags;\n\tstruct srp_login_req *login;\n\tstruct srp_event_struct *evt_struct = get_event_struct(&hostdata->pool);\n\n\tBUG_ON(!evt_struct);\n\tinit_event_struct(evt_struct, login_rsp,\n\t\t\t  VIOSRP_SRP_FORMAT, login_timeout);\n\n\tlogin = &evt_struct->iu.srp.login_req;\n\tmemset(login, 0, sizeof(*login));\n\tlogin->opcode = SRP_LOGIN_REQ;\n\tlogin->req_it_iu_len = cpu_to_be32(sizeof(union srp_iu));\n\tlogin->req_buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\n\t\t\t\t\t SRP_BUF_FORMAT_INDIRECT);\n\n\t \n\tibmvscsi_set_request_limit(hostdata, 0);\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\trc = ibmvscsi_send_srp_event(evt_struct, hostdata, login_timeout * 2);\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\tdev_info(hostdata->dev, \"sent SRP login\\n\");\n\treturn rc;\n};\n\n \nstatic void capabilities_rsp(struct srp_event_struct *evt_struct)\n{\n\tstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\n\n\tif (evt_struct->xfer_iu->mad.capabilities.common.status) {\n\t\tdev_err(hostdata->dev, \"error 0x%X getting capabilities info\\n\",\n\t\t\tevt_struct->xfer_iu->mad.capabilities.common.status);\n\t} else {\n\t\tif (hostdata->caps.migration.common.server_support !=\n\t\t    cpu_to_be16(SERVER_SUPPORTS_CAP))\n\t\t\tdev_info(hostdata->dev, \"Partition migration not supported\\n\");\n\n\t\tif (client_reserve) {\n\t\t\tif (hostdata->caps.reserve.common.server_support ==\n\t\t\t    cpu_to_be16(SERVER_SUPPORTS_CAP))\n\t\t\t\tdev_info(hostdata->dev, \"Client reserve enabled\\n\");\n\t\t\telse\n\t\t\t\tdev_info(hostdata->dev, \"Client reserve not supported\\n\");\n\t\t}\n\t}\n\n\tsend_srp_login(hostdata);\n}\n\n \nstatic void send_mad_capabilities(struct ibmvscsi_host_data *hostdata)\n{\n\tstruct viosrp_capabilities *req;\n\tstruct srp_event_struct *evt_struct;\n\tunsigned long flags;\n\tstruct device_node *of_node = hostdata->dev->of_node;\n\tconst char *location;\n\n\tevt_struct = get_event_struct(&hostdata->pool);\n\tBUG_ON(!evt_struct);\n\n\tinit_event_struct(evt_struct, capabilities_rsp,\n\t\t\t  VIOSRP_MAD_FORMAT, info_timeout);\n\n\treq = &evt_struct->iu.mad.capabilities;\n\tmemset(req, 0, sizeof(*req));\n\n\thostdata->caps.flags = cpu_to_be32(CAP_LIST_SUPPORTED);\n\tif (hostdata->client_migrated)\n\t\thostdata->caps.flags |= cpu_to_be32(CLIENT_MIGRATED);\n\n\tstrscpy(hostdata->caps.name, dev_name(&hostdata->host->shost_gendev),\n\t\tsizeof(hostdata->caps.name));\n\n\tlocation = of_get_property(of_node, \"ibm,loc-code\", NULL);\n\tlocation = location ? location : dev_name(hostdata->dev);\n\tstrscpy(hostdata->caps.loc, location, sizeof(hostdata->caps.loc));\n\n\treq->common.type = cpu_to_be32(VIOSRP_CAPABILITIES_TYPE);\n\treq->buffer = cpu_to_be64(hostdata->caps_addr);\n\n\thostdata->caps.migration.common.cap_type =\n\t\t\t\tcpu_to_be32(MIGRATION_CAPABILITIES);\n\thostdata->caps.migration.common.length =\n\t\t\t\tcpu_to_be16(sizeof(hostdata->caps.migration));\n\thostdata->caps.migration.common.server_support =\n\t\t\t\tcpu_to_be16(SERVER_SUPPORTS_CAP);\n\thostdata->caps.migration.ecl = cpu_to_be32(1);\n\n\tif (client_reserve) {\n\t\thostdata->caps.reserve.common.cap_type =\n\t\t\t\t\tcpu_to_be32(RESERVATION_CAPABILITIES);\n\t\thostdata->caps.reserve.common.length =\n\t\t\t\tcpu_to_be16(sizeof(hostdata->caps.reserve));\n\t\thostdata->caps.reserve.common.server_support =\n\t\t\t\tcpu_to_be16(SERVER_SUPPORTS_CAP);\n\t\thostdata->caps.reserve.type =\n\t\t\t\tcpu_to_be32(CLIENT_RESERVE_SCSI_2);\n\t\treq->common.length =\n\t\t\t\tcpu_to_be16(sizeof(hostdata->caps));\n\t} else\n\t\treq->common.length = cpu_to_be16(sizeof(hostdata->caps) -\n\t\t\t\t\t\tsizeof(hostdata->caps.reserve));\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tif (ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2))\n\t\tdev_err(hostdata->dev, \"couldn't send CAPABILITIES_REQ!\\n\");\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n};\n\n \nstatic void fast_fail_rsp(struct srp_event_struct *evt_struct)\n{\n\tstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\n\tu16 status = be16_to_cpu(evt_struct->xfer_iu->mad.fast_fail.common.status);\n\n\tif (status == VIOSRP_MAD_NOT_SUPPORTED)\n\t\tdev_err(hostdata->dev, \"fast_fail not supported in server\\n\");\n\telse if (status == VIOSRP_MAD_FAILED)\n\t\tdev_err(hostdata->dev, \"fast_fail request failed\\n\");\n\telse if (status != VIOSRP_MAD_SUCCESS)\n\t\tdev_err(hostdata->dev, \"error 0x%X enabling fast_fail\\n\", status);\n\n\tsend_mad_capabilities(hostdata);\n}\n\n \nstatic int enable_fast_fail(struct ibmvscsi_host_data *hostdata)\n{\n\tint rc;\n\tunsigned long flags;\n\tstruct viosrp_fast_fail *fast_fail_mad;\n\tstruct srp_event_struct *evt_struct;\n\n\tif (!fast_fail) {\n\t\tsend_mad_capabilities(hostdata);\n\t\treturn 0;\n\t}\n\n\tevt_struct = get_event_struct(&hostdata->pool);\n\tBUG_ON(!evt_struct);\n\n\tinit_event_struct(evt_struct, fast_fail_rsp, VIOSRP_MAD_FORMAT, info_timeout);\n\n\tfast_fail_mad = &evt_struct->iu.mad.fast_fail;\n\tmemset(fast_fail_mad, 0, sizeof(*fast_fail_mad));\n\tfast_fail_mad->common.type = cpu_to_be32(VIOSRP_ENABLE_FAST_FAIL);\n\tfast_fail_mad->common.length = cpu_to_be16(sizeof(*fast_fail_mad));\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\trc = ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2);\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\treturn rc;\n}\n\n \nstatic void adapter_info_rsp(struct srp_event_struct *evt_struct)\n{\n\tstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\n\n\tif (evt_struct->xfer_iu->mad.adapter_info.common.status) {\n\t\tdev_err(hostdata->dev, \"error %d getting adapter info\\n\",\n\t\t\tevt_struct->xfer_iu->mad.adapter_info.common.status);\n\t} else {\n\t\tdev_info(hostdata->dev, \"host srp version: %s, \"\n\t\t\t \"host partition %s (%d), OS %d, max io %u\\n\",\n\t\t\t hostdata->madapter_info.srp_version,\n\t\t\t hostdata->madapter_info.partition_name,\n\t\t\t be32_to_cpu(hostdata->madapter_info.partition_number),\n\t\t\t be32_to_cpu(hostdata->madapter_info.os_type),\n\t\t\t be32_to_cpu(hostdata->madapter_info.port_max_txu[0]));\n\t\t\n\t\tif (hostdata->madapter_info.port_max_txu[0]) \n\t\t\thostdata->host->max_sectors = \n\t\t\t\tbe32_to_cpu(hostdata->madapter_info.port_max_txu[0]) >> 9;\n\t\t\n\t\tif (be32_to_cpu(hostdata->madapter_info.os_type) == SRP_MAD_OS_AIX &&\n\t\t    strcmp(hostdata->madapter_info.srp_version, \"1.6a\") <= 0) {\n\t\t\tdev_err(hostdata->dev, \"host (Ver. %s) doesn't support large transfers\\n\",\n\t\t\t\thostdata->madapter_info.srp_version);\n\t\t\tdev_err(hostdata->dev, \"limiting scatterlists to %d\\n\",\n\t\t\t\tMAX_INDIRECT_BUFS);\n\t\t\thostdata->host->sg_tablesize = MAX_INDIRECT_BUFS;\n\t\t}\n\n\t\tif (be32_to_cpu(hostdata->madapter_info.os_type) == SRP_MAD_OS_AIX) {\n\t\t\tenable_fast_fail(hostdata);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tsend_srp_login(hostdata);\n}\n\n \nstatic void send_mad_adapter_info(struct ibmvscsi_host_data *hostdata)\n{\n\tstruct viosrp_adapter_info *req;\n\tstruct srp_event_struct *evt_struct;\n\tunsigned long flags;\n\n\tevt_struct = get_event_struct(&hostdata->pool);\n\tBUG_ON(!evt_struct);\n\n\tinit_event_struct(evt_struct,\n\t\t\t  adapter_info_rsp,\n\t\t\t  VIOSRP_MAD_FORMAT,\n\t\t\t  info_timeout);\n\t\n\treq = &evt_struct->iu.mad.adapter_info;\n\tmemset(req, 0x00, sizeof(*req));\n\t\n\treq->common.type = cpu_to_be32(VIOSRP_ADAPTER_INFO_TYPE);\n\treq->common.length = cpu_to_be16(sizeof(hostdata->madapter_info));\n\treq->buffer = cpu_to_be64(hostdata->adapter_info_addr);\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tif (ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2))\n\t\tdev_err(hostdata->dev, \"couldn't send ADAPTER_INFO_REQ!\\n\");\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n};\n\n \nstatic void init_adapter(struct ibmvscsi_host_data *hostdata)\n{\n\tsend_mad_adapter_info(hostdata);\n}\n\n \nstatic void sync_completion(struct srp_event_struct *evt_struct)\n{\n\t \n\tif (evt_struct->sync_srp)\n\t\t*evt_struct->sync_srp = *evt_struct->xfer_iu;\n\t\n\tcomplete(&evt_struct->comp);\n}\n\n \nstatic int ibmvscsi_eh_abort_handler(struct scsi_cmnd *cmd)\n{\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\n\tstruct srp_tsk_mgmt *tsk_mgmt;\n\tstruct srp_event_struct *evt;\n\tstruct srp_event_struct *tmp_evt, *found_evt;\n\tunion viosrp_iu srp_rsp;\n\tint rsp_rc;\n\tunsigned long flags;\n\tu16 lun = lun_from_dev(cmd->device);\n\tunsigned long wait_switch = 0;\n\n\t \n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\twait_switch = jiffies + (init_timeout * HZ);\n\tdo {\n\t\tfound_evt = NULL;\n\t\tlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\n\t\t\tif (tmp_evt->cmnd == cmd) {\n\t\t\t\tfound_evt = tmp_evt;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!found_evt) {\n\t\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\t\treturn SUCCESS;\n\t\t}\n\n\t\tevt = get_event_struct(&hostdata->pool);\n\t\tif (evt == NULL) {\n\t\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t\t\"failed to allocate abort event\\n\");\n\t\t\treturn FAILED;\n\t\t}\n\t\n\t\tinit_event_struct(evt,\n\t\t\t\t  sync_completion,\n\t\t\t\t  VIOSRP_SRP_FORMAT,\n\t\t\t\t  abort_timeout);\n\n\t\ttsk_mgmt = &evt->iu.srp.tsk_mgmt;\n\t\n\t\t \n\t\tmemset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));\n\t\ttsk_mgmt->opcode = SRP_TSK_MGMT;\n\t\tint_to_scsilun(lun, &tsk_mgmt->lun);\n\t\ttsk_mgmt->tsk_mgmt_func = SRP_TSK_ABORT_TASK;\n\t\ttsk_mgmt->task_tag = (u64) found_evt;\n\n\t\tevt->sync_srp = &srp_rsp;\n\n\t\tinit_completion(&evt->comp);\n\t\trsp_rc = ibmvscsi_send_srp_event(evt, hostdata, abort_timeout * 2);\n\n\t\tif (rsp_rc != SCSI_MLQUEUE_HOST_BUSY)\n\t\t\tbreak;\n\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\tmsleep(10);\n\t\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\t} while (time_before(jiffies, wait_switch));\n\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\n\tif (rsp_rc != 0) {\n\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t    \"failed to send abort() event. rc=%d\\n\", rsp_rc);\n\t\treturn FAILED;\n\t}\n\n\tsdev_printk(KERN_INFO, cmd->device,\n                    \"aborting command. lun 0x%llx, tag 0x%llx\\n\",\n\t\t    (((u64) lun) << 48), (u64) found_evt);\n\n\twait_for_completion(&evt->comp);\n\n\t \n\tif (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {\n\t\tif (printk_ratelimit())\n\t\t\tsdev_printk(KERN_WARNING, cmd->device, \"abort bad SRP RSP type %d\\n\",\n\t\t\t\t    srp_rsp.srp.rsp.opcode);\n\t\treturn FAILED;\n\t}\n\n\tif (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)\n\t\trsp_rc = *((int *)srp_rsp.srp.rsp.data);\n\telse\n\t\trsp_rc = srp_rsp.srp.rsp.status;\n\n\tif (rsp_rc) {\n\t\tif (printk_ratelimit())\n\t\t\tsdev_printk(KERN_WARNING, cmd->device,\n\t\t\t\t    \"abort code %d for task tag 0x%llx\\n\",\n\t\t\t\t    rsp_rc, tsk_mgmt->task_tag);\n\t\treturn FAILED;\n\t}\n\n\t \n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tfound_evt = NULL;\n\tlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\n\t\tif (tmp_evt->cmnd == cmd) {\n\t\t\tfound_evt = tmp_evt;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (found_evt == NULL) {\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\tsdev_printk(KERN_INFO, cmd->device, \"aborted task tag 0x%llx completed\\n\",\n\t\t\t    tsk_mgmt->task_tag);\n\t\treturn SUCCESS;\n\t}\n\n\tsdev_printk(KERN_INFO, cmd->device, \"successfully aborted task tag 0x%llx\\n\",\n\t\t    tsk_mgmt->task_tag);\n\n\tcmd->result = (DID_ABORT << 16);\n\tlist_del(&found_evt->list);\n\tunmap_cmd_data(&found_evt->iu.srp.cmd, found_evt,\n\t\t       found_evt->hostdata->dev);\n\tfree_event_struct(&found_evt->hostdata->pool, found_evt);\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\tatomic_inc(&hostdata->request_limit);\n\treturn SUCCESS;\n}\n\n \nstatic int ibmvscsi_eh_device_reset_handler(struct scsi_cmnd *cmd)\n{\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\n\tstruct srp_tsk_mgmt *tsk_mgmt;\n\tstruct srp_event_struct *evt;\n\tstruct srp_event_struct *tmp_evt, *pos;\n\tunion viosrp_iu srp_rsp;\n\tint rsp_rc;\n\tunsigned long flags;\n\tu16 lun = lun_from_dev(cmd->device);\n\tunsigned long wait_switch = 0;\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\twait_switch = jiffies + (init_timeout * HZ);\n\tdo {\n\t\tevt = get_event_struct(&hostdata->pool);\n\t\tif (evt == NULL) {\n\t\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t\t\"failed to allocate reset event\\n\");\n\t\t\treturn FAILED;\n\t\t}\n\t\n\t\tinit_event_struct(evt,\n\t\t\t\t  sync_completion,\n\t\t\t\t  VIOSRP_SRP_FORMAT,\n\t\t\t\t  reset_timeout);\n\n\t\ttsk_mgmt = &evt->iu.srp.tsk_mgmt;\n\n\t\t \n\t\tmemset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));\n\t\ttsk_mgmt->opcode = SRP_TSK_MGMT;\n\t\tint_to_scsilun(lun, &tsk_mgmt->lun);\n\t\ttsk_mgmt->tsk_mgmt_func = SRP_TSK_LUN_RESET;\n\n\t\tevt->sync_srp = &srp_rsp;\n\n\t\tinit_completion(&evt->comp);\n\t\trsp_rc = ibmvscsi_send_srp_event(evt, hostdata, reset_timeout * 2);\n\n\t\tif (rsp_rc != SCSI_MLQUEUE_HOST_BUSY)\n\t\t\tbreak;\n\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\tmsleep(10);\n\t\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\t} while (time_before(jiffies, wait_switch));\n\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\n\tif (rsp_rc != 0) {\n\t\tsdev_printk(KERN_ERR, cmd->device,\n\t\t\t    \"failed to send reset event. rc=%d\\n\", rsp_rc);\n\t\treturn FAILED;\n\t}\n\n\tsdev_printk(KERN_INFO, cmd->device, \"resetting device. lun 0x%llx\\n\",\n\t\t    (((u64) lun) << 48));\n\n\twait_for_completion(&evt->comp);\n\n\t \n\tif (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {\n\t\tif (printk_ratelimit())\n\t\t\tsdev_printk(KERN_WARNING, cmd->device, \"reset bad SRP RSP type %d\\n\",\n\t\t\t\t    srp_rsp.srp.rsp.opcode);\n\t\treturn FAILED;\n\t}\n\n\tif (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)\n\t\trsp_rc = *((int *)srp_rsp.srp.rsp.data);\n\telse\n\t\trsp_rc = srp_rsp.srp.rsp.status;\n\n\tif (rsp_rc) {\n\t\tif (printk_ratelimit())\n\t\t\tsdev_printk(KERN_WARNING, cmd->device,\n\t\t\t\t    \"reset code %d for task tag 0x%llx\\n\",\n\t\t\t\t    rsp_rc, tsk_mgmt->task_tag);\n\t\treturn FAILED;\n\t}\n\n\t \n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tlist_for_each_entry_safe(tmp_evt, pos, &hostdata->sent, list) {\n\t\tif ((tmp_evt->cmnd) && (tmp_evt->cmnd->device == cmd->device)) {\n\t\t\tif (tmp_evt->cmnd)\n\t\t\t\ttmp_evt->cmnd->result = (DID_RESET << 16);\n\t\t\tlist_del(&tmp_evt->list);\n\t\t\tunmap_cmd_data(&tmp_evt->iu.srp.cmd, tmp_evt,\n\t\t\t\t       tmp_evt->hostdata->dev);\n\t\t\tfree_event_struct(&tmp_evt->hostdata->pool,\n\t\t\t\t\t\t   tmp_evt);\n\t\t\tatomic_inc(&hostdata->request_limit);\n\t\t\tif (tmp_evt->cmnd_done)\n\t\t\t\ttmp_evt->cmnd_done(tmp_evt->cmnd);\n\t\t\telse if (tmp_evt->done)\n\t\t\t\ttmp_evt->done(tmp_evt);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\treturn SUCCESS;\n}\n\n \nstatic int ibmvscsi_eh_host_reset_handler(struct scsi_cmnd *cmd)\n{\n\tunsigned long wait_switch = 0;\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\n\n\tdev_err(hostdata->dev, \"Resetting connection due to error recovery\\n\");\n\n\tibmvscsi_reset_host(hostdata);\n\n\tfor (wait_switch = jiffies + (init_timeout * HZ);\n\t     time_before(jiffies, wait_switch) &&\n\t\t     atomic_read(&hostdata->request_limit) < 2;) {\n\n\t\tmsleep(10);\n\t}\n\n\tif (atomic_read(&hostdata->request_limit) <= 0)\n\t\treturn FAILED;\n\n\treturn SUCCESS;\n}\n\n \nstatic void ibmvscsi_handle_crq(struct viosrp_crq *crq,\n\t\t\t\tstruct ibmvscsi_host_data *hostdata)\n{\n\tlong rc;\n\tunsigned long flags;\n\t \n\tstruct srp_event_struct *evt_struct =\n\t\t\t(__force struct srp_event_struct *)crq->IU_data_ptr;\n\tswitch (crq->valid) {\n\tcase VIOSRP_CRQ_INIT_RSP:\t\t \n\t\tswitch (crq->format) {\n\t\tcase VIOSRP_CRQ_INIT:\t \n\t\t\tdev_info(hostdata->dev, \"partner initialized\\n\");\n\t\t\t \n\t\t\trc = ibmvscsi_send_crq(hostdata, 0xC002000000000000LL, 0);\n\t\t\tif (rc == 0) {\n\t\t\t\t \n\t\t\t\tinit_adapter(hostdata);\n\t\t\t} else {\n\t\t\t\tdev_err(hostdata->dev, \"Unable to send init rsp. rc=%ld\\n\", rc);\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase VIOSRP_CRQ_INIT_COMPLETE:\t \n\t\t\tdev_info(hostdata->dev, \"partner initialization complete\\n\");\n\n\t\t\t \n\t\t\tinit_adapter(hostdata);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hostdata->dev, \"unknown crq message type: %d\\n\", crq->format);\n\t\t}\n\t\treturn;\n\tcase VIOSRP_CRQ_XPORT_EVENT:\t \n\t\tscsi_block_requests(hostdata->host);\n\t\tibmvscsi_set_request_limit(hostdata, 0);\n\t\tif (crq->format == 0x06) {\n\t\t\t \n\t\t\tdev_info(hostdata->dev, \"Re-enabling adapter!\\n\");\n\t\t\thostdata->client_migrated = 1;\n\t\t\thostdata->action = IBMVSCSI_HOST_ACTION_REENABLE;\n\t\t\tpurge_requests(hostdata, DID_REQUEUE);\n\t\t\twake_up(&hostdata->work_wait_q);\n\t\t} else {\n\t\t\tdev_err(hostdata->dev, \"Virtual adapter failed rc %d!\\n\",\n\t\t\t\tcrq->format);\n\t\t\tibmvscsi_reset_host(hostdata);\n\t\t}\n\t\treturn;\n\tcase VIOSRP_CRQ_CMD_RSP:\t\t \n\t\tbreak;\n\tdefault:\n\t\tdev_err(hostdata->dev, \"got an invalid message type 0x%02x\\n\",\n\t\t\tcrq->valid);\n\t\treturn;\n\t}\n\n\t \n\tif (!valid_event_struct(&hostdata->pool, evt_struct)) {\n\t\tdev_err(hostdata->dev, \"returned correlation_token 0x%p is invalid!\\n\",\n\t\t       evt_struct);\n\t\treturn;\n\t}\n\n\tif (atomic_read(&evt_struct->free)) {\n\t\tdev_err(hostdata->dev, \"received duplicate correlation_token 0x%p!\\n\",\n\t\t\tevt_struct);\n\t\treturn;\n\t}\n\n\tif (crq->format == VIOSRP_SRP_FORMAT)\n\t\tatomic_add(be32_to_cpu(evt_struct->xfer_iu->srp.rsp.req_lim_delta),\n\t\t\t   &hostdata->request_limit);\n\n\tdel_timer(&evt_struct->timer);\n\n\tif ((crq->status != VIOSRP_OK && crq->status != VIOSRP_OK2) && evt_struct->cmnd)\n\t\tevt_struct->cmnd->result = DID_ERROR << 16;\n\tif (evt_struct->done)\n\t\tevt_struct->done(evt_struct);\n\telse\n\t\tdev_err(hostdata->dev, \"returned done() is NULL; not running it!\\n\");\n\n\t \n\tspin_lock_irqsave(evt_struct->hostdata->host->host_lock, flags);\n\tlist_del(&evt_struct->list);\n\tfree_event_struct(&evt_struct->hostdata->pool, evt_struct);\n\tspin_unlock_irqrestore(evt_struct->hostdata->host->host_lock, flags);\n}\n\n \nstatic int ibmvscsi_slave_configure(struct scsi_device *sdev)\n{\n\tstruct Scsi_Host *shost = sdev->host;\n\tunsigned long lock_flags = 0;\n\n\tspin_lock_irqsave(shost->host_lock, lock_flags);\n\tif (sdev->type == TYPE_DISK) {\n\t\tsdev->allow_restart = 1;\n\t\tblk_queue_rq_timeout(sdev->request_queue, 120 * HZ);\n\t}\n\tspin_unlock_irqrestore(shost->host_lock, lock_flags);\n\treturn 0;\n}\n\n \nstatic int ibmvscsi_change_queue_depth(struct scsi_device *sdev, int qdepth)\n{\n\tif (qdepth > IBMVSCSI_MAX_CMDS_PER_LUN)\n\t\tqdepth = IBMVSCSI_MAX_CMDS_PER_LUN;\n\treturn scsi_change_queue_depth(sdev, qdepth);\n}\n\n \nstatic ssize_t show_host_vhost_loc(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, sizeof(hostdata->caps.loc), \"%s\\n\",\n\t\t       hostdata->caps.loc);\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_vhost_loc = {\n\t.attr = {\n\t\t .name = \"vhost_loc\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_vhost_loc,\n};\n\nstatic ssize_t show_host_vhost_name(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, sizeof(hostdata->caps.name), \"%s\\n\",\n\t\t       hostdata->caps.name);\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_vhost_name = {\n\t.attr = {\n\t\t .name = \"vhost_name\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_vhost_name,\n};\n\nstatic ssize_t show_host_srp_version(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t       hostdata->madapter_info.srp_version);\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_srp_version = {\n\t.attr = {\n\t\t .name = \"srp_version\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_srp_version,\n};\n\nstatic ssize_t show_host_partition_name(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t       hostdata->madapter_info.partition_name);\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_partition_name = {\n\t.attr = {\n\t\t .name = \"partition_name\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_partition_name,\n};\n\nstatic ssize_t show_host_partition_number(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t       be32_to_cpu(hostdata->madapter_info.partition_number));\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_partition_number = {\n\t.attr = {\n\t\t .name = \"partition_number\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_partition_number,\n};\n\nstatic ssize_t show_host_mad_version(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t       be32_to_cpu(hostdata->madapter_info.mad_version));\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_mad_version = {\n\t.attr = {\n\t\t .name = \"mad_version\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_mad_version,\n};\n\nstatic ssize_t show_host_os_type(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\tint len;\n\n\tlen = snprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t       be32_to_cpu(hostdata->madapter_info.os_type));\n\treturn len;\n}\n\nstatic struct device_attribute ibmvscsi_host_os_type = {\n\t.attr = {\n\t\t .name = \"os_type\",\n\t\t .mode = S_IRUGO,\n\t\t },\n\t.show = show_host_os_type,\n};\n\nstatic ssize_t show_host_config(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\treturn 0;\n}\n\nstatic struct device_attribute ibmvscsi_host_config = {\n\t.attr = {\n\t\t.name = \"config\",\n\t\t.mode = S_IRUGO,\n\t\t},\n\t.show = show_host_config,\n};\n\nstatic int ibmvscsi_host_reset(struct Scsi_Host *shost, int reset_type)\n{\n\tstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\n\n\tdev_info(hostdata->dev, \"Initiating adapter reset!\\n\");\n\tibmvscsi_reset_host(hostdata);\n\n\treturn 0;\n}\n\nstatic struct attribute *ibmvscsi_host_attrs[] = {\n\t&ibmvscsi_host_vhost_loc.attr,\n\t&ibmvscsi_host_vhost_name.attr,\n\t&ibmvscsi_host_srp_version.attr,\n\t&ibmvscsi_host_partition_name.attr,\n\t&ibmvscsi_host_partition_number.attr,\n\t&ibmvscsi_host_mad_version.attr,\n\t&ibmvscsi_host_os_type.attr,\n\t&ibmvscsi_host_config.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(ibmvscsi_host);\n\n \nstatic struct scsi_host_template driver_template = {\n\t.module = THIS_MODULE,\n\t.name = \"IBM POWER Virtual SCSI Adapter \" IBMVSCSI_VERSION,\n\t.proc_name = \"ibmvscsi\",\n\t.queuecommand = ibmvscsi_queuecommand,\n\t.eh_timed_out = srp_timed_out,\n\t.eh_abort_handler = ibmvscsi_eh_abort_handler,\n\t.eh_device_reset_handler = ibmvscsi_eh_device_reset_handler,\n\t.eh_host_reset_handler = ibmvscsi_eh_host_reset_handler,\n\t.slave_configure = ibmvscsi_slave_configure,\n\t.change_queue_depth = ibmvscsi_change_queue_depth,\n\t.host_reset = ibmvscsi_host_reset,\n\t.cmd_per_lun = IBMVSCSI_CMDS_PER_LUN_DEFAULT,\n\t.can_queue = IBMVSCSI_MAX_REQUESTS_DEFAULT,\n\t.this_id = -1,\n\t.sg_tablesize = SG_ALL,\n\t.shost_groups = ibmvscsi_host_groups,\n};\n\n \nstatic unsigned long ibmvscsi_get_desired_dma(struct vio_dev *vdev)\n{\n\t \n\tunsigned long desired_io = max_events * sizeof(union viosrp_iu);\n\n\t \n\tdesired_io += (IBMVSCSI_MAX_SECTORS_DEFAULT * 512 *\n\t                     IBMVSCSI_CMDS_PER_LUN_DEFAULT);\n\n\treturn desired_io;\n}\n\nstatic void ibmvscsi_do_work(struct ibmvscsi_host_data *hostdata)\n{\n\tunsigned long flags;\n\tint rc;\n\tchar *action = \"reset\";\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\tswitch (hostdata->action) {\n\tcase IBMVSCSI_HOST_ACTION_UNBLOCK:\n\t\trc = 0;\n\t\tbreak;\n\tcase IBMVSCSI_HOST_ACTION_RESET:\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\trc = ibmvscsi_reset_crq_queue(&hostdata->queue, hostdata);\n\t\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\t\tif (!rc)\n\t\t\trc = ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0);\n\t\tvio_enable_interrupts(to_vio_dev(hostdata->dev));\n\t\tbreak;\n\tcase IBMVSCSI_HOST_ACTION_REENABLE:\n\t\taction = \"enable\";\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\trc = ibmvscsi_reenable_crq_queue(&hostdata->queue, hostdata);\n\t\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\t\tif (!rc)\n\t\t\trc = ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0);\n\t\tbreak;\n\tcase IBMVSCSI_HOST_ACTION_NONE:\n\tdefault:\n\t\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\t\treturn;\n\t}\n\n\thostdata->action = IBMVSCSI_HOST_ACTION_NONE;\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\n\tif (rc) {\n\t\tibmvscsi_set_request_limit(hostdata, -1);\n\t\tdev_err(hostdata->dev, \"error after %s\\n\", action);\n\t}\n\n\tscsi_unblock_requests(hostdata->host);\n}\n\nstatic int __ibmvscsi_work_to_do(struct ibmvscsi_host_data *hostdata)\n{\n\tif (kthread_should_stop())\n\t\treturn 1;\n\tswitch (hostdata->action) {\n\tcase IBMVSCSI_HOST_ACTION_NONE:\n\t\treturn 0;\n\tcase IBMVSCSI_HOST_ACTION_RESET:\n\tcase IBMVSCSI_HOST_ACTION_REENABLE:\n\tcase IBMVSCSI_HOST_ACTION_UNBLOCK:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 1;\n}\n\nstatic int ibmvscsi_work_to_do(struct ibmvscsi_host_data *hostdata)\n{\n\tunsigned long flags;\n\tint rc;\n\n\tspin_lock_irqsave(hostdata->host->host_lock, flags);\n\trc = __ibmvscsi_work_to_do(hostdata);\n\tspin_unlock_irqrestore(hostdata->host->host_lock, flags);\n\n\treturn rc;\n}\n\nstatic int ibmvscsi_work(void *data)\n{\n\tstruct ibmvscsi_host_data *hostdata = data;\n\tint rc;\n\n\tset_user_nice(current, MIN_NICE);\n\n\twhile (1) {\n\t\trc = wait_event_interruptible(hostdata->work_wait_q,\n\t\t\t\t\t      ibmvscsi_work_to_do(hostdata));\n\n\t\tBUG_ON(rc);\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tibmvscsi_do_work(hostdata);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ibmvscsi_probe(struct vio_dev *vdev, const struct vio_device_id *id)\n{\n\tstruct ibmvscsi_host_data *hostdata;\n\tstruct Scsi_Host *host;\n\tstruct device *dev = &vdev->dev;\n\tstruct srp_rport_identifiers ids;\n\tstruct srp_rport *rport;\n\tunsigned long wait_switch = 0;\n\tint rc;\n\n\tdev_set_drvdata(&vdev->dev, NULL);\n\n\thost = scsi_host_alloc(&driver_template, sizeof(*hostdata));\n\tif (!host) {\n\t\tdev_err(&vdev->dev, \"couldn't allocate host data\\n\");\n\t\tgoto scsi_host_alloc_failed;\n\t}\n\n\thost->transportt = ibmvscsi_transport_template;\n\thostdata = shost_priv(host);\n\tmemset(hostdata, 0x00, sizeof(*hostdata));\n\tINIT_LIST_HEAD(&hostdata->sent);\n\tinit_waitqueue_head(&hostdata->work_wait_q);\n\thostdata->host = host;\n\thostdata->dev = dev;\n\tibmvscsi_set_request_limit(hostdata, -1);\n\thostdata->host->max_sectors = IBMVSCSI_MAX_SECTORS_DEFAULT;\n\n\tif (map_persist_bufs(hostdata)) {\n\t\tdev_err(&vdev->dev, \"couldn't map persistent buffers\\n\");\n\t\tgoto persist_bufs_failed;\n\t}\n\n\thostdata->work_thread = kthread_run(ibmvscsi_work, hostdata, \"%s_%d\",\n\t\t\t\t\t    \"ibmvscsi\", host->host_no);\n\n\tif (IS_ERR(hostdata->work_thread)) {\n\t\tdev_err(&vdev->dev, \"couldn't initialize kthread. rc=%ld\\n\",\n\t\t\tPTR_ERR(hostdata->work_thread));\n\t\tgoto init_crq_failed;\n\t}\n\n\trc = ibmvscsi_init_crq_queue(&hostdata->queue, hostdata, max_events);\n\tif (rc != 0 && rc != H_RESOURCE) {\n\t\tdev_err(&vdev->dev, \"couldn't initialize crq. rc=%d\\n\", rc);\n\t\tgoto kill_kthread;\n\t}\n\tif (initialize_event_pool(&hostdata->pool, max_events, hostdata) != 0) {\n\t\tdev_err(&vdev->dev, \"couldn't initialize event pool\\n\");\n\t\tgoto init_pool_failed;\n\t}\n\n\thost->max_lun = IBMVSCSI_MAX_LUN;\n\thost->max_id = max_id;\n\thost->max_channel = max_channel;\n\thost->max_cmd_len = 16;\n\n\tdev_info(dev,\n\t\t \"Maximum ID: %d Maximum LUN: %llu Maximum Channel: %d\\n\",\n\t\t host->max_id, host->max_lun, host->max_channel);\n\n\tif (scsi_add_host(hostdata->host, hostdata->dev))\n\t\tgoto add_host_failed;\n\n\t \n\tmemcpy(ids.port_id, hostdata->madapter_info.partition_name,\n\t       sizeof(ids.port_id));\n\tids.roles = SRP_RPORT_ROLE_TARGET;\n\trport = srp_rport_add(host, &ids);\n\tif (IS_ERR(rport))\n\t\tgoto add_srp_port_failed;\n\n\t \n\tif (ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0) == 0\n\t    || rc == H_RESOURCE) {\n\t\t \n\t\tfor (wait_switch = jiffies + (init_timeout * HZ);\n\t\t     time_before(jiffies, wait_switch) &&\n\t\t     atomic_read(&hostdata->request_limit) < 2;) {\n\n\t\t\tmsleep(10);\n\t\t}\n\n\t\t \n\t\tif (atomic_read(&hostdata->request_limit) > 0)\n\t\t\tscsi_scan_host(host);\n\t}\n\n\tdev_set_drvdata(&vdev->dev, hostdata);\n\tspin_lock(&ibmvscsi_driver_lock);\n\tlist_add_tail(&hostdata->host_list, &ibmvscsi_head);\n\tspin_unlock(&ibmvscsi_driver_lock);\n\treturn 0;\n\n      add_srp_port_failed:\n\tscsi_remove_host(hostdata->host);\n      add_host_failed:\n\trelease_event_pool(&hostdata->pool, hostdata);\n      init_pool_failed:\n\tibmvscsi_release_crq_queue(&hostdata->queue, hostdata, max_events);\n      kill_kthread:\n      kthread_stop(hostdata->work_thread);\n      init_crq_failed:\n\tunmap_persist_bufs(hostdata);\n      persist_bufs_failed:\n\tscsi_host_put(host);\n      scsi_host_alloc_failed:\n\treturn -1;\n}\n\nstatic void ibmvscsi_remove(struct vio_dev *vdev)\n{\n\tstruct ibmvscsi_host_data *hostdata = dev_get_drvdata(&vdev->dev);\n\n\tsrp_remove_host(hostdata->host);\n\tscsi_remove_host(hostdata->host);\n\n\tpurge_requests(hostdata, DID_ERROR);\n\trelease_event_pool(&hostdata->pool, hostdata);\n\n\tibmvscsi_release_crq_queue(&hostdata->queue, hostdata,\n\t\t\t\t\tmax_events);\n\n\tkthread_stop(hostdata->work_thread);\n\tunmap_persist_bufs(hostdata);\n\n\tspin_lock(&ibmvscsi_driver_lock);\n\tlist_del(&hostdata->host_list);\n\tspin_unlock(&ibmvscsi_driver_lock);\n\n\tscsi_host_put(hostdata->host);\n}\n\n \nstatic int ibmvscsi_resume(struct device *dev)\n{\n\tstruct ibmvscsi_host_data *hostdata = dev_get_drvdata(dev);\n\tvio_disable_interrupts(to_vio_dev(hostdata->dev));\n\ttasklet_schedule(&hostdata->srp_task);\n\n\treturn 0;\n}\n\n \nstatic const struct vio_device_id ibmvscsi_device_table[] = {\n\t{\"vscsi\", \"IBM,v-scsi\"},\n\t{ \"\", \"\" }\n};\nMODULE_DEVICE_TABLE(vio, ibmvscsi_device_table);\n\nstatic const struct dev_pm_ops ibmvscsi_pm_ops = {\n\t.resume = ibmvscsi_resume\n};\n\nstatic struct vio_driver ibmvscsi_driver = {\n\t.id_table = ibmvscsi_device_table,\n\t.probe = ibmvscsi_probe,\n\t.remove = ibmvscsi_remove,\n\t.get_desired_dma = ibmvscsi_get_desired_dma,\n\t.name = \"ibmvscsi\",\n\t.pm = &ibmvscsi_pm_ops,\n};\n\nstatic struct srp_function_template ibmvscsi_transport_functions = {\n};\n\nstatic int __init ibmvscsi_module_init(void)\n{\n\tint ret;\n\n\t \n\tdriver_template.can_queue = max_requests;\n\tmax_events = max_requests + 2;\n\n\tif (!firmware_has_feature(FW_FEATURE_VIO))\n\t\treturn -ENODEV;\n\n\tibmvscsi_transport_template =\n\t\tsrp_attach_transport(&ibmvscsi_transport_functions);\n\tif (!ibmvscsi_transport_template)\n\t\treturn -ENOMEM;\n\n\tret = vio_register_driver(&ibmvscsi_driver);\n\tif (ret)\n\t\tsrp_release_transport(ibmvscsi_transport_template);\n\treturn ret;\n}\n\nstatic void __exit ibmvscsi_module_exit(void)\n{\n\tvio_unregister_driver(&ibmvscsi_driver);\n\tsrp_release_transport(ibmvscsi_transport_template);\n}\n\nmodule_init(ibmvscsi_module_init);\nmodule_exit(ibmvscsi_module_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}