{
  "module_name": "main.c",
  "hash_id": "fbc3ac48aaacbac92fccce1566e7b2f82f62d0c5ef2193278e21969c5bd20b27",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/cxlflash/main.c",
  "human_readable_source": "\n \n\n#include <linux/delay.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <asm/unaligned.h>\n\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_host.h>\n#include <uapi/scsi/cxlflash_ioctl.h>\n\n#include \"main.h\"\n#include \"sislite.h\"\n#include \"common.h\"\n\nMODULE_DESCRIPTION(CXLFLASH_ADAPTER_NAME);\nMODULE_AUTHOR(\"Manoj N. Kumar <manoj@linux.vnet.ibm.com>\");\nMODULE_AUTHOR(\"Matthew R. Ochs <mrochs@linux.vnet.ibm.com>\");\nMODULE_LICENSE(\"GPL\");\n\nstatic struct class *cxlflash_class;\nstatic u32 cxlflash_major;\nstatic DECLARE_BITMAP(cxlflash_minor, CXLFLASH_MAX_ADAPTERS);\n\n \nstatic void process_cmd_err(struct afu_cmd *cmd, struct scsi_cmnd *scp)\n{\n\tstruct afu *afu = cmd->parent;\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct sisl_ioasa *ioasa;\n\tu32 resid;\n\n\tioasa = &(cmd->sa);\n\n\tif (ioasa->rc.flags & SISL_RC_FLAGS_UNDERRUN) {\n\t\tresid = ioasa->resid;\n\t\tscsi_set_resid(scp, resid);\n\t\tdev_dbg(dev, \"%s: cmd underrun cmd = %p scp = %p, resid = %d\\n\",\n\t\t\t__func__, cmd, scp, resid);\n\t}\n\n\tif (ioasa->rc.flags & SISL_RC_FLAGS_OVERRUN) {\n\t\tdev_dbg(dev, \"%s: cmd underrun cmd = %p scp = %p\\n\",\n\t\t\t__func__, cmd, scp);\n\t\tscp->result = (DID_ERROR << 16);\n\t}\n\n\tdev_dbg(dev, \"%s: cmd failed afu_rc=%02x scsi_rc=%02x fc_rc=%02x \"\n\t\t\"afu_extra=%02x scsi_extra=%02x fc_extra=%02x\\n\", __func__,\n\t\tioasa->rc.afu_rc, ioasa->rc.scsi_rc, ioasa->rc.fc_rc,\n\t\tioasa->afu_extra, ioasa->scsi_extra, ioasa->fc_extra);\n\n\tif (ioasa->rc.scsi_rc) {\n\t\t \n\t\tif (ioasa->rc.flags & SISL_RC_FLAGS_SENSE_VALID) {\n\t\t\tmemcpy(scp->sense_buffer, ioasa->sense_data,\n\t\t\t       SISL_SENSE_DATA_LEN);\n\t\t\tscp->result = ioasa->rc.scsi_rc;\n\t\t} else\n\t\t\tscp->result = ioasa->rc.scsi_rc | (DID_ERROR << 16);\n\t}\n\n\t \n\tif (ioasa->rc.fc_rc) {\n\t\t \n\t\tswitch (ioasa->rc.fc_rc) {\n\t\tcase SISL_FC_RC_LINKDOWN:\n\t\t\tscp->result = (DID_REQUEUE << 16);\n\t\t\tbreak;\n\t\tcase SISL_FC_RC_RESID:\n\t\t\t \n\t\t\tif (!(ioasa->rc.flags & SISL_RC_FLAGS_OVERRUN)) {\n\t\t\t\t \n\t\t\t\tscp->result = (DID_ERROR << 16);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SISL_FC_RC_RESIDERR:\n\t\t\t \n\t\tcase SISL_FC_RC_TGTABORT:\n\t\tcase SISL_FC_RC_ABORTOK:\n\t\tcase SISL_FC_RC_ABORTFAIL:\n\t\tcase SISL_FC_RC_NOLOGI:\n\t\tcase SISL_FC_RC_ABORTPEND:\n\t\tcase SISL_FC_RC_WRABORTPEND:\n\t\tcase SISL_FC_RC_NOEXP:\n\t\tcase SISL_FC_RC_INUSE:\n\t\t\tscp->result = (DID_ERROR << 16);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ioasa->rc.afu_rc) {\n\t\t \n\t\tswitch (ioasa->rc.afu_rc) {\n\t\tcase SISL_AFU_RC_NO_CHANNELS:\n\t\t\tscp->result = (DID_NO_CONNECT << 16);\n\t\t\tbreak;\n\t\tcase SISL_AFU_RC_DATA_DMA_ERR:\n\t\t\tswitch (ioasa->afu_extra) {\n\t\t\tcase SISL_AFU_DMA_ERR_PAGE_IN:\n\t\t\t\t \n\t\t\t\tscp->result = (DID_IMM_RETRY << 16);\n\t\t\t\tbreak;\n\t\t\tcase SISL_AFU_DMA_ERR_INVALID_EA:\n\t\t\tdefault:\n\t\t\t\tscp->result = (DID_ERROR << 16);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SISL_AFU_RC_OUT_OF_DATA_BUFS:\n\t\t\t \n\t\t\tscp->result = (DID_ERROR << 16);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tscp->result = (DID_ERROR << 16);\n\t\t}\n\t}\n}\n\n \nstatic void cmd_complete(struct afu_cmd *cmd)\n{\n\tstruct scsi_cmnd *scp;\n\tulong lock_flags;\n\tstruct afu *afu = cmd->parent;\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq = get_hwq(afu, cmd->hwq_index);\n\n\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\tlist_del(&cmd->list);\n\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n\n\tif (cmd->scp) {\n\t\tscp = cmd->scp;\n\t\tif (unlikely(cmd->sa.ioasc))\n\t\t\tprocess_cmd_err(cmd, scp);\n\t\telse\n\t\t\tscp->result = (DID_OK << 16);\n\n\t\tdev_dbg_ratelimited(dev, \"%s:scp=%p result=%08x ioasc=%08x\\n\",\n\t\t\t\t    __func__, scp, scp->result, cmd->sa.ioasc);\n\t\tscsi_done(scp);\n\t} else if (cmd->cmd_tmf) {\n\t\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\t\tcfg->tmf_active = false;\n\t\twake_up_all_locked(&cfg->tmf_waitq);\n\t\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\t} else\n\t\tcomplete(&cmd->cevent);\n}\n\n \nstatic void flush_pending_cmds(struct hwq *hwq)\n{\n\tstruct cxlflash_cfg *cfg = hwq->afu->parent;\n\tstruct afu_cmd *cmd, *tmp;\n\tstruct scsi_cmnd *scp;\n\tulong lock_flags;\n\n\tlist_for_each_entry_safe(cmd, tmp, &hwq->pending_cmds, list) {\n\t\t \n\t\tif (!list_empty(&cmd->queue))\n\t\t\tcontinue;\n\n\t\tlist_del(&cmd->list);\n\n\t\tif (cmd->scp) {\n\t\t\tscp = cmd->scp;\n\t\t\tscp->result = (DID_IMM_RETRY << 16);\n\t\t\tscsi_done(scp);\n\t\t} else {\n\t\t\tcmd->cmd_aborted = true;\n\n\t\t\tif (cmd->cmd_tmf) {\n\t\t\t\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\t\t\t\tcfg->tmf_active = false;\n\t\t\t\twake_up_all_locked(&cfg->tmf_waitq);\n\t\t\t\tspin_unlock_irqrestore(&cfg->tmf_slock,\n\t\t\t\t\t\t       lock_flags);\n\t\t\t} else\n\t\t\t\tcomplete(&cmd->cevent);\n\t\t}\n\t}\n}\n\n \nstatic int context_reset(struct hwq *hwq, __be64 __iomem *reset_reg)\n{\n\tstruct cxlflash_cfg *cfg = hwq->afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = -ETIMEDOUT;\n\tint nretry = 0;\n\tu64 val = 0x1;\n\tulong lock_flags;\n\n\tdev_dbg(dev, \"%s: hwq=%p\\n\", __func__, hwq);\n\n\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\n\twriteq_be(val, reset_reg);\n\tdo {\n\t\tval = readq_be(reset_reg);\n\t\tif ((val & 0x1) == 0x0) {\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tudelay(1 << nretry);\n\t} while (nretry++ < MC_ROOM_RETRY_CNT);\n\n\tif (!rc)\n\t\tflush_pending_cmds(hwq);\n\n\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n\n\tdev_dbg(dev, \"%s: returning rc=%d, val=%016llx nretry=%d\\n\",\n\t\t__func__, rc, val, nretry);\n\treturn rc;\n}\n\n \nstatic int context_reset_ioarrin(struct hwq *hwq)\n{\n\treturn context_reset(hwq, &hwq->host_map->ioarrin);\n}\n\n \nstatic int context_reset_sq(struct hwq *hwq)\n{\n\treturn context_reset(hwq, &hwq->host_map->sq_ctx_reset);\n}\n\n \nstatic int send_cmd_ioarrin(struct afu *afu, struct afu_cmd *cmd)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq = get_hwq(afu, cmd->hwq_index);\n\tint rc = 0;\n\ts64 room;\n\tulong lock_flags;\n\n\t \n\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\tif (--hwq->room < 0) {\n\t\troom = readq_be(&hwq->host_map->cmd_room);\n\t\tif (room <= 0) {\n\t\t\tdev_dbg_ratelimited(dev, \"%s: no cmd_room to send \"\n\t\t\t\t\t    \"0x%02X, room=0x%016llX\\n\",\n\t\t\t\t\t    __func__, cmd->rcb.cdb[0], room);\n\t\t\thwq->room = 0;\n\t\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\t\tgoto out;\n\t\t}\n\t\thwq->room = room - 1;\n\t}\n\n\tlist_add(&cmd->list, &hwq->pending_cmds);\n\twriteq_be((u64)&cmd->rcb, &hwq->host_map->ioarrin);\nout:\n\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n\tdev_dbg_ratelimited(dev, \"%s: cmd=%p len=%u ea=%016llx rc=%d\\n\",\n\t\t__func__, cmd, cmd->rcb.data_len, cmd->rcb.data_ea, rc);\n\treturn rc;\n}\n\n \nstatic int send_cmd_sq(struct afu *afu, struct afu_cmd *cmd)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq = get_hwq(afu, cmd->hwq_index);\n\tint rc = 0;\n\tint newval;\n\tulong lock_flags;\n\n\tnewval = atomic_dec_if_positive(&hwq->hsq_credits);\n\tif (newval <= 0) {\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto out;\n\t}\n\n\tcmd->rcb.ioasa = &cmd->sa;\n\n\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\n\t*hwq->hsq_curr = cmd->rcb;\n\tif (hwq->hsq_curr < hwq->hsq_end)\n\t\thwq->hsq_curr++;\n\telse\n\t\thwq->hsq_curr = hwq->hsq_start;\n\n\tlist_add(&cmd->list, &hwq->pending_cmds);\n\twriteq_be((u64)hwq->hsq_curr, &hwq->host_map->sq_tail);\n\n\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\nout:\n\tdev_dbg(dev, \"%s: cmd=%p len=%u ea=%016llx ioasa=%p rc=%d curr=%p \"\n\t       \"head=%016llx tail=%016llx\\n\", __func__, cmd, cmd->rcb.data_len,\n\t       cmd->rcb.data_ea, cmd->rcb.ioasa, rc, hwq->hsq_curr,\n\t       readq_be(&hwq->host_map->sq_head),\n\t       readq_be(&hwq->host_map->sq_tail));\n\treturn rc;\n}\n\n \nstatic int wait_resp(struct afu *afu, struct afu_cmd *cmd)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = 0;\n\tulong timeout = msecs_to_jiffies(cmd->rcb.timeout * 2 * 1000);\n\n\ttimeout = wait_for_completion_timeout(&cmd->cevent, timeout);\n\tif (!timeout)\n\t\trc = -ETIMEDOUT;\n\n\tif (cmd->cmd_aborted)\n\t\trc = -EAGAIN;\n\n\tif (unlikely(cmd->sa.ioasc != 0)) {\n\t\tdev_err(dev, \"%s: cmd %02x failed, ioasc=%08x\\n\",\n\t\t\t__func__, cmd->rcb.cdb[0], cmd->sa.ioasc);\n\t\trc = -EIO;\n\t}\n\n\treturn rc;\n}\n\n \nstatic u32 cmd_to_target_hwq(struct Scsi_Host *host, struct scsi_cmnd *scp,\n\t\t\t     struct afu *afu)\n{\n\tu32 tag;\n\tu32 hwq = 0;\n\n\tif (afu->num_hwqs == 1)\n\t\treturn 0;\n\n\tswitch (afu->hwq_mode) {\n\tcase HWQ_MODE_RR:\n\t\thwq = afu->hwq_rr_count++ % afu->num_hwqs;\n\t\tbreak;\n\tcase HWQ_MODE_TAG:\n\t\ttag = blk_mq_unique_tag(scsi_cmd_to_rq(scp));\n\t\thwq = blk_mq_unique_tag_to_hwq(tag);\n\t\tbreak;\n\tcase HWQ_MODE_CPU:\n\t\thwq = smp_processor_id() % afu->num_hwqs;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\treturn hwq;\n}\n\n \nstatic int send_tmf(struct cxlflash_cfg *cfg, struct scsi_device *sdev,\n\t\t    u64 tmfcmd)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct afu_cmd *cmd = NULL;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);\n\tbool needs_deletion = false;\n\tchar *buf = NULL;\n\tulong lock_flags;\n\tint rc = 0;\n\tulong to;\n\n\tbuf = kzalloc(sizeof(*cmd) + __alignof__(*cmd) - 1, GFP_KERNEL);\n\tif (unlikely(!buf)) {\n\t\tdev_err(dev, \"%s: no memory for command\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcmd = (struct afu_cmd *)PTR_ALIGN(buf, __alignof__(*cmd));\n\tINIT_LIST_HEAD(&cmd->queue);\n\n\t \n\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\tif (cfg->tmf_active)\n\t\twait_event_interruptible_lock_irq(cfg->tmf_waitq,\n\t\t\t\t\t\t  !cfg->tmf_active,\n\t\t\t\t\t\t  cfg->tmf_slock);\n\tcfg->tmf_active = true;\n\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\n\tcmd->parent = afu;\n\tcmd->cmd_tmf = true;\n\tcmd->hwq_index = hwq->index;\n\n\tcmd->rcb.ctx_id = hwq->ctx_hndl;\n\tcmd->rcb.msi = SISL_MSI_RRQ_UPDATED;\n\tcmd->rcb.port_sel = CHAN2PORTMASK(sdev->channel);\n\tcmd->rcb.lun_id = lun_to_lunid(sdev->lun);\n\tcmd->rcb.req_flags = (SISL_REQ_FLAGS_PORT_LUN_ID |\n\t\t\t      SISL_REQ_FLAGS_SUP_UNDERRUN |\n\t\t\t      SISL_REQ_FLAGS_TMF_CMD);\n\tmemcpy(cmd->rcb.cdb, &tmfcmd, sizeof(tmfcmd));\n\n\trc = afu->send_cmd(afu, cmd);\n\tif (unlikely(rc)) {\n\t\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\t\tcfg->tmf_active = false;\n\t\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\t\tgoto out;\n\t}\n\n\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\tto = msecs_to_jiffies(5000);\n\tto = wait_event_interruptible_lock_irq_timeout(cfg->tmf_waitq,\n\t\t\t\t\t\t       !cfg->tmf_active,\n\t\t\t\t\t\t       cfg->tmf_slock,\n\t\t\t\t\t\t       to);\n\tif (!to) {\n\t\tdev_err(dev, \"%s: TMF timed out\\n\", __func__);\n\t\trc = -ETIMEDOUT;\n\t\tneeds_deletion = true;\n\t} else if (cmd->cmd_aborted) {\n\t\tdev_err(dev, \"%s: TMF aborted\\n\", __func__);\n\t\trc = -EAGAIN;\n\t} else if (cmd->sa.ioasc) {\n\t\tdev_err(dev, \"%s: TMF failed ioasc=%08x\\n\",\n\t\t\t__func__, cmd->sa.ioasc);\n\t\trc = -EIO;\n\t}\n\tcfg->tmf_active = false;\n\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\n\tif (needs_deletion) {\n\t\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\t\tlist_del(&cmd->list);\n\t\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n\t}\nout:\n\tkfree(buf);\n\treturn rc;\n}\n\n \nstatic const char *cxlflash_driver_info(struct Scsi_Host *host)\n{\n\treturn CXLFLASH_ADAPTER_NAME;\n}\n\n \nstatic int cxlflash_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *scp)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(host);\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct afu_cmd *cmd = sc_to_afuci(scp);\n\tstruct scatterlist *sg = scsi_sglist(scp);\n\tint hwq_index = cmd_to_target_hwq(host, scp, afu);\n\tstruct hwq *hwq = get_hwq(afu, hwq_index);\n\tu16 req_flags = SISL_REQ_FLAGS_SUP_UNDERRUN;\n\tulong lock_flags;\n\tint rc = 0;\n\n\tdev_dbg_ratelimited(dev, \"%s: (scp=%p) %d/%d/%d/%llu \"\n\t\t\t    \"cdb=(%08x-%08x-%08x-%08x)\\n\",\n\t\t\t    __func__, scp, host->host_no, scp->device->channel,\n\t\t\t    scp->device->id, scp->device->lun,\n\t\t\t    get_unaligned_be32(&((u32 *)scp->cmnd)[0]),\n\t\t\t    get_unaligned_be32(&((u32 *)scp->cmnd)[1]),\n\t\t\t    get_unaligned_be32(&((u32 *)scp->cmnd)[2]),\n\t\t\t    get_unaligned_be32(&((u32 *)scp->cmnd)[3]));\n\n\t \n\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\tif (cfg->tmf_active) {\n\t\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto out;\n\t}\n\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\n\tswitch (cfg->state) {\n\tcase STATE_PROBING:\n\tcase STATE_PROBED:\n\tcase STATE_RESET:\n\t\tdev_dbg_ratelimited(dev, \"%s: device is in reset\\n\", __func__);\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto out;\n\tcase STATE_FAILTERM:\n\t\tdev_dbg_ratelimited(dev, \"%s: device has failed\\n\", __func__);\n\t\tscp->result = (DID_NO_CONNECT << 16);\n\t\tscsi_done(scp);\n\t\trc = 0;\n\t\tgoto out;\n\tdefault:\n\t\tatomic_inc(&afu->cmds_active);\n\t\tbreak;\n\t}\n\n\tif (likely(sg)) {\n\t\tcmd->rcb.data_len = sg->length;\n\t\tcmd->rcb.data_ea = (uintptr_t)sg_virt(sg);\n\t}\n\n\tcmd->scp = scp;\n\tcmd->parent = afu;\n\tcmd->hwq_index = hwq_index;\n\n\tcmd->sa.ioasc = 0;\n\tcmd->rcb.ctx_id = hwq->ctx_hndl;\n\tcmd->rcb.msi = SISL_MSI_RRQ_UPDATED;\n\tcmd->rcb.port_sel = CHAN2PORTMASK(scp->device->channel);\n\tcmd->rcb.lun_id = lun_to_lunid(scp->device->lun);\n\n\tif (scp->sc_data_direction == DMA_TO_DEVICE)\n\t\treq_flags |= SISL_REQ_FLAGS_HOST_WRITE;\n\n\tcmd->rcb.req_flags = req_flags;\n\tmemcpy(cmd->rcb.cdb, scp->cmnd, sizeof(cmd->rcb.cdb));\n\n\trc = afu->send_cmd(afu, cmd);\n\tatomic_dec(&afu->cmds_active);\nout:\n\treturn rc;\n}\n\n \nstatic void cxlflash_wait_for_pci_err_recovery(struct cxlflash_cfg *cfg)\n{\n\tstruct pci_dev *pdev = cfg->dev;\n\n\tif (pci_channel_offline(pdev))\n\t\twait_event_timeout(cfg->reset_waitq,\n\t\t\t\t   !pci_channel_offline(pdev),\n\t\t\t\t   CXLFLASH_PCI_ERROR_RECOVERY_TIMEOUT);\n}\n\n \nstatic void free_mem(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\n\tif (cfg->afu) {\n\t\tfree_pages((ulong)afu, get_order(sizeof(struct afu)));\n\t\tcfg->afu = NULL;\n\t}\n}\n\n \nstatic void cxlflash_reset_sync(struct cxlflash_cfg *cfg)\n{\n\tif (cfg->async_reset_cookie == 0)\n\t\treturn;\n\n\t \n\tasync_synchronize_cookie(cfg->async_reset_cookie + 1);\n\tcfg->async_reset_cookie = 0;\n}\n\n \nstatic void stop_afu(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct hwq *hwq;\n\tint i;\n\n\tcancel_work_sync(&cfg->work_q);\n\tif (!current_is_async())\n\t\tcxlflash_reset_sync(cfg);\n\n\tif (likely(afu)) {\n\t\twhile (atomic_read(&afu->cmds_active))\n\t\t\tssleep(1);\n\n\t\tif (afu_is_irqpoll_enabled(afu)) {\n\t\t\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\t\t\thwq = get_hwq(afu, i);\n\n\t\t\t\tirq_poll_disable(&hwq->irqpoll);\n\t\t\t}\n\t\t}\n\n\t\tif (likely(afu->afu_map)) {\n\t\t\tcfg->ops->psa_unmap(afu->afu_map);\n\t\t\tafu->afu_map = NULL;\n\t\t}\n\t}\n}\n\n \nstatic void term_intr(struct cxlflash_cfg *cfg, enum undo_level level,\n\t\t      u32 index)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq;\n\n\tif (!afu) {\n\t\tdev_err(dev, \"%s: returning with NULL afu\\n\", __func__);\n\t\treturn;\n\t}\n\n\thwq = get_hwq(afu, index);\n\n\tif (!hwq->ctx_cookie) {\n\t\tdev_err(dev, \"%s: returning with NULL MC\\n\", __func__);\n\t\treturn;\n\t}\n\n\tswitch (level) {\n\tcase UNMAP_THREE:\n\t\t \n\t\tif (index == PRIMARY_HWQ)\n\t\t\tcfg->ops->unmap_afu_irq(hwq->ctx_cookie, 3, hwq);\n\t\tfallthrough;\n\tcase UNMAP_TWO:\n\t\tcfg->ops->unmap_afu_irq(hwq->ctx_cookie, 2, hwq);\n\t\tfallthrough;\n\tcase UNMAP_ONE:\n\t\tcfg->ops->unmap_afu_irq(hwq->ctx_cookie, 1, hwq);\n\t\tfallthrough;\n\tcase FREE_IRQ:\n\t\tcfg->ops->free_afu_irqs(hwq->ctx_cookie);\n\t\tfallthrough;\n\tcase UNDO_NOOP:\n\t\t \n\t\tbreak;\n\t}\n}\n\n \nstatic void term_mc(struct cxlflash_cfg *cfg, u32 index)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq;\n\tulong lock_flags;\n\n\tif (!afu) {\n\t\tdev_err(dev, \"%s: returning with NULL afu\\n\", __func__);\n\t\treturn;\n\t}\n\n\thwq = get_hwq(afu, index);\n\n\tif (!hwq->ctx_cookie) {\n\t\tdev_err(dev, \"%s: returning with NULL MC\\n\", __func__);\n\t\treturn;\n\t}\n\n\tWARN_ON(cfg->ops->stop_context(hwq->ctx_cookie));\n\tif (index != PRIMARY_HWQ)\n\t\tWARN_ON(cfg->ops->release_context(hwq->ctx_cookie));\n\thwq->ctx_cookie = NULL;\n\n\tspin_lock_irqsave(&hwq->hrrq_slock, lock_flags);\n\thwq->hrrq_online = false;\n\tspin_unlock_irqrestore(&hwq->hrrq_slock, lock_flags);\n\n\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\tflush_pending_cmds(hwq);\n\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n}\n\n \nstatic void term_afu(struct cxlflash_cfg *cfg)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tint k;\n\n\t \n\tfor (k = cfg->afu->num_hwqs - 1; k >= 0; k--)\n\t\tterm_intr(cfg, UNMAP_THREE, k);\n\n\tstop_afu(cfg);\n\n\tfor (k = cfg->afu->num_hwqs - 1; k >= 0; k--)\n\t\tterm_mc(cfg, k);\n\n\tdev_dbg(dev, \"%s: returning\\n\", __func__);\n}\n\n \nstatic void notify_shutdown(struct cxlflash_cfg *cfg, bool wait)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct dev_dependent_vals *ddv;\n\t__be64 __iomem *fc_port_regs;\n\tu64 reg, status;\n\tint i, retry_cnt = 0;\n\n\tddv = (struct dev_dependent_vals *)cfg->dev_id->driver_data;\n\tif (!(ddv->flags & CXLFLASH_NOTIFY_SHUTDOWN))\n\t\treturn;\n\n\tif (!afu || !afu->afu_map) {\n\t\tdev_dbg(dev, \"%s: Problem state area not mapped\\n\", __func__);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; i < cfg->num_fc_ports; i++) {\n\t\tfc_port_regs = get_fc_port_regs(cfg, i);\n\n\t\treg = readq_be(&fc_port_regs[FC_CONFIG2 / 8]);\n\t\treg |= SISL_FC_SHUTDOWN_NORMAL;\n\t\twriteq_be(reg, &fc_port_regs[FC_CONFIG2 / 8]);\n\t}\n\n\tif (!wait)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < cfg->num_fc_ports; i++) {\n\t\tfc_port_regs = get_fc_port_regs(cfg, i);\n\t\tretry_cnt = 0;\n\n\t\twhile (true) {\n\t\t\tstatus = readq_be(&fc_port_regs[FC_STATUS / 8]);\n\t\t\tif (status & SISL_STATUS_SHUTDOWN_COMPLETE)\n\t\t\t\tbreak;\n\t\t\tif (++retry_cnt >= MC_RETRY_CNT) {\n\t\t\t\tdev_dbg(dev, \"%s: port %d shutdown processing \"\n\t\t\t\t\t\"not yet completed\\n\", __func__, i);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(100 * retry_cnt);\n\t\t}\n\t}\n}\n\n \nstatic int cxlflash_get_minor(void)\n{\n\tint minor;\n\tlong bit;\n\n\tbit = find_first_zero_bit(cxlflash_minor, CXLFLASH_MAX_ADAPTERS);\n\tif (bit >= CXLFLASH_MAX_ADAPTERS)\n\t\treturn -1;\n\n\tminor = bit & MINORMASK;\n\tset_bit(minor, cxlflash_minor);\n\treturn minor;\n}\n\n \nstatic void cxlflash_put_minor(int minor)\n{\n\tclear_bit(minor, cxlflash_minor);\n}\n\n \nstatic void cxlflash_release_chrdev(struct cxlflash_cfg *cfg)\n{\n\tdevice_unregister(cfg->chardev);\n\tcfg->chardev = NULL;\n\tcdev_del(&cfg->cdev);\n\tcxlflash_put_minor(MINOR(cfg->cdev.dev));\n}\n\n \nstatic void cxlflash_remove(struct pci_dev *pdev)\n{\n\tstruct cxlflash_cfg *cfg = pci_get_drvdata(pdev);\n\tstruct device *dev = &pdev->dev;\n\tulong lock_flags;\n\n\tif (!pci_is_enabled(pdev)) {\n\t\tdev_dbg(dev, \"%s: Device is disabled\\n\", __func__);\n\t\treturn;\n\t}\n\n\t \n\twait_event(cfg->reset_waitq, cfg->state != STATE_RESET &&\n\t\t\t\t     cfg->state != STATE_PROBING);\n\tspin_lock_irqsave(&cfg->tmf_slock, lock_flags);\n\tif (cfg->tmf_active)\n\t\twait_event_interruptible_lock_irq(cfg->tmf_waitq,\n\t\t\t\t\t\t  !cfg->tmf_active,\n\t\t\t\t\t\t  cfg->tmf_slock);\n\tspin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);\n\n\t \n\tnotify_shutdown(cfg, true);\n\n\tcfg->state = STATE_FAILTERM;\n\tcxlflash_stop_term_user_contexts(cfg);\n\n\tswitch (cfg->init_state) {\n\tcase INIT_STATE_CDEV:\n\t\tcxlflash_release_chrdev(cfg);\n\t\tfallthrough;\n\tcase INIT_STATE_SCSI:\n\t\tcxlflash_term_local_luns(cfg);\n\t\tscsi_remove_host(cfg->host);\n\t\tfallthrough;\n\tcase INIT_STATE_AFU:\n\t\tterm_afu(cfg);\n\t\tfallthrough;\n\tcase INIT_STATE_PCI:\n\t\tcfg->ops->destroy_afu(cfg->afu_cookie);\n\t\tpci_disable_device(pdev);\n\t\tfallthrough;\n\tcase INIT_STATE_NONE:\n\t\tfree_mem(cfg);\n\t\tscsi_host_put(cfg->host);\n\t\tbreak;\n\t}\n\n\tdev_dbg(dev, \"%s: returning\\n\", __func__);\n}\n\n \nstatic int alloc_mem(struct cxlflash_cfg *cfg)\n{\n\tint rc = 0;\n\tstruct device *dev = &cfg->dev->dev;\n\n\t \n\tcfg->afu = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t    get_order(sizeof(struct afu)));\n\tif (unlikely(!cfg->afu)) {\n\t\tdev_err(dev, \"%s: cannot get %d free pages\\n\",\n\t\t\t__func__, get_order(sizeof(struct afu)));\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\tcfg->afu->parent = cfg;\n\tcfg->afu->desired_hwqs = CXLFLASH_DEF_HWQS;\n\tcfg->afu->afu_map = NULL;\nout:\n\treturn rc;\n}\n\n \nstatic int init_pci(struct cxlflash_cfg *cfg)\n{\n\tstruct pci_dev *pdev = cfg->dev;\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = 0;\n\n\trc = pci_enable_device(pdev);\n\tif (rc || pci_channel_offline(pdev)) {\n\t\tif (pci_channel_offline(pdev)) {\n\t\t\tcxlflash_wait_for_pci_err_recovery(cfg);\n\t\t\trc = pci_enable_device(pdev);\n\t\t}\n\n\t\tif (rc) {\n\t\t\tdev_err(dev, \"%s: Cannot enable adapter\\n\", __func__);\n\t\t\tcxlflash_wait_for_pci_err_recovery(cfg);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic int init_scsi(struct cxlflash_cfg *cfg)\n{\n\tstruct pci_dev *pdev = cfg->dev;\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = 0;\n\n\trc = scsi_add_host(cfg->host, &pdev->dev);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: scsi_add_host failed rc=%d\\n\", __func__, rc);\n\t\tgoto out;\n\t}\n\n\tscsi_scan_host(cfg->host);\n\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic void set_port_online(__be64 __iomem *fc_regs)\n{\n\tu64 cmdcfg;\n\n\tcmdcfg = readq_be(&fc_regs[FC_MTIP_CMDCONFIG / 8]);\n\tcmdcfg &= (~FC_MTIP_CMDCONFIG_OFFLINE);\t \n\tcmdcfg |= (FC_MTIP_CMDCONFIG_ONLINE);\t \n\twriteq_be(cmdcfg, &fc_regs[FC_MTIP_CMDCONFIG / 8]);\n}\n\n \nstatic void set_port_offline(__be64 __iomem *fc_regs)\n{\n\tu64 cmdcfg;\n\n\tcmdcfg = readq_be(&fc_regs[FC_MTIP_CMDCONFIG / 8]);\n\tcmdcfg &= (~FC_MTIP_CMDCONFIG_ONLINE);\t \n\tcmdcfg |= (FC_MTIP_CMDCONFIG_OFFLINE);\t \n\twriteq_be(cmdcfg, &fc_regs[FC_MTIP_CMDCONFIG / 8]);\n}\n\n \nstatic bool wait_port_online(__be64 __iomem *fc_regs, u32 delay_us, u32 nretry)\n{\n\tu64 status;\n\n\tWARN_ON(delay_us < 1000);\n\n\tdo {\n\t\tmsleep(delay_us / 1000);\n\t\tstatus = readq_be(&fc_regs[FC_MTIP_STATUS / 8]);\n\t\tif (status == U64_MAX)\n\t\t\tnretry /= 2;\n\t} while ((status & FC_MTIP_STATUS_MASK) != FC_MTIP_STATUS_ONLINE &&\n\t\t nretry--);\n\n\treturn ((status & FC_MTIP_STATUS_MASK) == FC_MTIP_STATUS_ONLINE);\n}\n\n \nstatic bool wait_port_offline(__be64 __iomem *fc_regs, u32 delay_us, u32 nretry)\n{\n\tu64 status;\n\n\tWARN_ON(delay_us < 1000);\n\n\tdo {\n\t\tmsleep(delay_us / 1000);\n\t\tstatus = readq_be(&fc_regs[FC_MTIP_STATUS / 8]);\n\t\tif (status == U64_MAX)\n\t\t\tnretry /= 2;\n\t} while ((status & FC_MTIP_STATUS_MASK) != FC_MTIP_STATUS_OFFLINE &&\n\t\t nretry--);\n\n\treturn ((status & FC_MTIP_STATUS_MASK) == FC_MTIP_STATUS_OFFLINE);\n}\n\n \nstatic void afu_set_wwpn(struct afu *afu, int port, __be64 __iomem *fc_regs,\n\t\t\t u64 wwpn)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\n\tset_port_offline(fc_regs);\n\tif (!wait_port_offline(fc_regs, FC_PORT_STATUS_RETRY_INTERVAL_US,\n\t\t\t       FC_PORT_STATUS_RETRY_CNT)) {\n\t\tdev_dbg(dev, \"%s: wait on port %d to go offline timed out\\n\",\n\t\t\t__func__, port);\n\t}\n\n\twriteq_be(wwpn, &fc_regs[FC_PNAME / 8]);\n\n\tset_port_online(fc_regs);\n\tif (!wait_port_online(fc_regs, FC_PORT_STATUS_RETRY_INTERVAL_US,\n\t\t\t      FC_PORT_STATUS_RETRY_CNT)) {\n\t\tdev_dbg(dev, \"%s: wait on port %d to go online timed out\\n\",\n\t\t\t__func__, port);\n\t}\n}\n\n \nstatic void afu_link_reset(struct afu *afu, int port, __be64 __iomem *fc_regs)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tu64 port_sel;\n\n\t \n\tport_sel = readq_be(&afu->afu_map->global.regs.afu_port_sel);\n\tport_sel &= ~(1ULL << port);\n\twriteq_be(port_sel, &afu->afu_map->global.regs.afu_port_sel);\n\tcxlflash_afu_sync(afu, 0, 0, AFU_GSYNC);\n\n\tset_port_offline(fc_regs);\n\tif (!wait_port_offline(fc_regs, FC_PORT_STATUS_RETRY_INTERVAL_US,\n\t\t\t       FC_PORT_STATUS_RETRY_CNT))\n\t\tdev_err(dev, \"%s: wait on port %d to go offline timed out\\n\",\n\t\t\t__func__, port);\n\n\tset_port_online(fc_regs);\n\tif (!wait_port_online(fc_regs, FC_PORT_STATUS_RETRY_INTERVAL_US,\n\t\t\t      FC_PORT_STATUS_RETRY_CNT))\n\t\tdev_err(dev, \"%s: wait on port %d to go online timed out\\n\",\n\t\t\t__func__, port);\n\n\t \n\tport_sel |= (1ULL << port);\n\twriteq_be(port_sel, &afu->afu_map->global.regs.afu_port_sel);\n\tcxlflash_afu_sync(afu, 0, 0, AFU_GSYNC);\n\n\tdev_dbg(dev, \"%s: returning port_sel=%016llx\\n\", __func__, port_sel);\n}\n\n \nstatic void afu_err_intr_init(struct afu *afu)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\t__be64 __iomem *fc_port_regs;\n\tint i;\n\tstruct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);\n\tu64 reg;\n\n\t \n\n\t \n\twriteq_be(-1ULL, &afu->afu_map->global.regs.aintr_mask);\n\t \n\treg = ((u64) (((hwq->ctx_hndl << 8) | SISL_MSI_ASYNC_ERROR)) << 40);\n\n\tif (afu->internal_lun)\n\t\treg |= 1;\t \n\twriteq_be(reg, &afu->afu_map->global.regs.afu_ctrl);\n\t \n\twriteq_be(-1ULL, &afu->afu_map->global.regs.aintr_clear);\n\t \n\t \n\twriteq_be(SISL_ASTATUS_MASK, &afu->afu_map->global.regs.aintr_mask);\n\t \n\t \n\twriteq_be(-1ULL, &afu->afu_map->global.regs.aintr_clear);\n\n\t \n\tfc_port_regs = get_fc_port_regs(cfg, 0);\n\treg = readq_be(&fc_port_regs[FC_CONFIG2 / 8]);\n\treg &= SISL_FC_INTERNAL_MASK;\n\tif (afu->internal_lun)\n\t\treg |= ((u64)(afu->internal_lun - 1) << SISL_FC_INTERNAL_SHIFT);\n\twriteq_be(reg, &fc_port_regs[FC_CONFIG2 / 8]);\n\n\t \n\tfor (i = 0; i < cfg->num_fc_ports; i++) {\n\t\tfc_port_regs = get_fc_port_regs(cfg, i);\n\n\t\twriteq_be(0xFFFFFFFFU, &fc_port_regs[FC_ERROR / 8]);\n\t\twriteq_be(0, &fc_port_regs[FC_ERRCAP / 8]);\n\t}\n\n\t \n\t \n\t \n\t \n\n\t \n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\n\t\treg = readq_be(&hwq->host_map->ctx_ctrl);\n\t\tWARN_ON((reg & SISL_CTX_CTRL_LISN_MASK) != 0);\n\t\treg |= SISL_MSI_SYNC_ERROR;\n\t\twriteq_be(reg, &hwq->host_map->ctx_ctrl);\n\t\twriteq_be(SISL_ISTATUS_MASK, &hwq->host_map->intr_mask);\n\t}\n}\n\n \nstatic irqreturn_t cxlflash_sync_err_irq(int irq, void *data)\n{\n\tstruct hwq *hwq = (struct hwq *)data;\n\tstruct cxlflash_cfg *cfg = hwq->afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tu64 reg;\n\tu64 reg_unmasked;\n\n\treg = readq_be(&hwq->host_map->intr_status);\n\treg_unmasked = (reg & SISL_ISTATUS_UNMASK);\n\n\tif (reg_unmasked == 0UL) {\n\t\tdev_err(dev, \"%s: spurious interrupt, intr_status=%016llx\\n\",\n\t\t\t__func__, reg);\n\t\tgoto cxlflash_sync_err_irq_exit;\n\t}\n\n\tdev_err(dev, \"%s: unexpected interrupt, intr_status=%016llx\\n\",\n\t\t__func__, reg);\n\n\twriteq_be(reg_unmasked, &hwq->host_map->intr_clear);\n\ncxlflash_sync_err_irq_exit:\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int process_hrrq(struct hwq *hwq, struct list_head *doneq, int budget)\n{\n\tstruct afu *afu = hwq->afu;\n\tstruct afu_cmd *cmd;\n\tstruct sisl_ioasa *ioasa;\n\tstruct sisl_ioarcb *ioarcb;\n\tbool toggle = hwq->toggle;\n\tint num_hrrq = 0;\n\tu64 entry,\n\t    *hrrq_start = hwq->hrrq_start,\n\t    *hrrq_end = hwq->hrrq_end,\n\t    *hrrq_curr = hwq->hrrq_curr;\n\n\t \n\twhile (true) {\n\t\tentry = *hrrq_curr;\n\n\t\tif ((entry & SISL_RESP_HANDLE_T_BIT) != toggle)\n\t\t\tbreak;\n\n\t\tentry &= ~SISL_RESP_HANDLE_T_BIT;\n\n\t\tif (afu_is_sq_cmd_mode(afu)) {\n\t\t\tioasa = (struct sisl_ioasa *)entry;\n\t\t\tcmd = container_of(ioasa, struct afu_cmd, sa);\n\t\t} else {\n\t\t\tioarcb = (struct sisl_ioarcb *)entry;\n\t\t\tcmd = container_of(ioarcb, struct afu_cmd, rcb);\n\t\t}\n\n\t\tlist_add_tail(&cmd->queue, doneq);\n\n\t\t \n\t\tif (hrrq_curr < hrrq_end)\n\t\t\thrrq_curr++;\n\t\telse {\n\t\t\thrrq_curr = hrrq_start;\n\t\t\ttoggle ^= SISL_RESP_HANDLE_T_BIT;\n\t\t}\n\n\t\tatomic_inc(&hwq->hsq_credits);\n\t\tnum_hrrq++;\n\n\t\tif (budget > 0 && num_hrrq >= budget)\n\t\t\tbreak;\n\t}\n\n\thwq->hrrq_curr = hrrq_curr;\n\thwq->toggle = toggle;\n\n\treturn num_hrrq;\n}\n\n \nstatic void process_cmd_doneq(struct list_head *doneq)\n{\n\tstruct afu_cmd *cmd, *tmp;\n\n\tWARN_ON(list_empty(doneq));\n\n\tlist_for_each_entry_safe(cmd, tmp, doneq, queue)\n\t\tcmd_complete(cmd);\n}\n\n \nstatic int cxlflash_irqpoll(struct irq_poll *irqpoll, int budget)\n{\n\tstruct hwq *hwq = container_of(irqpoll, struct hwq, irqpoll);\n\tunsigned long hrrq_flags;\n\tLIST_HEAD(doneq);\n\tint num_entries = 0;\n\n\tspin_lock_irqsave(&hwq->hrrq_slock, hrrq_flags);\n\n\tnum_entries = process_hrrq(hwq, &doneq, budget);\n\tif (num_entries < budget)\n\t\tirq_poll_complete(irqpoll);\n\n\tspin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);\n\n\tprocess_cmd_doneq(&doneq);\n\treturn num_entries;\n}\n\n \nstatic irqreturn_t cxlflash_rrq_irq(int irq, void *data)\n{\n\tstruct hwq *hwq = (struct hwq *)data;\n\tstruct afu *afu = hwq->afu;\n\tunsigned long hrrq_flags;\n\tLIST_HEAD(doneq);\n\tint num_entries = 0;\n\n\tspin_lock_irqsave(&hwq->hrrq_slock, hrrq_flags);\n\n\t \n\tif (!hwq->hrrq_online) {\n\t\tspin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (afu_is_irqpoll_enabled(afu)) {\n\t\tirq_poll_sched(&hwq->irqpoll);\n\t\tspin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tnum_entries = process_hrrq(hwq, &doneq, -1);\n\tspin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);\n\n\tif (num_entries == 0)\n\t\treturn IRQ_NONE;\n\n\tprocess_cmd_doneq(&doneq);\n\treturn IRQ_HANDLED;\n}\n\n \n#define ASTATUS_FC(_a, _b, _c, _d)\t\t\t\t\t \\\n\t{ SISL_ASTATUS_FC##_a##_##_b, _c, _a, (_d) }\n\n#define BUILD_SISL_ASTATUS_FC_PORT(_a)\t\t\t\t\t \\\n\tASTATUS_FC(_a, LINK_UP, \"link up\", 0),\t\t\t\t \\\n\tASTATUS_FC(_a, LINK_DN, \"link down\", 0),\t\t\t \\\n\tASTATUS_FC(_a, LOGI_S, \"login succeeded\", SCAN_HOST),\t\t \\\n\tASTATUS_FC(_a, LOGI_F, \"login failed\", CLR_FC_ERROR),\t\t \\\n\tASTATUS_FC(_a, LOGI_R, \"login timed out, retrying\", LINK_RESET), \\\n\tASTATUS_FC(_a, CRC_T, \"CRC threshold exceeded\", LINK_RESET),\t \\\n\tASTATUS_FC(_a, LOGO, \"target initiated LOGO\", 0),\t\t \\\n\tASTATUS_FC(_a, OTHER, \"other error\", CLR_FC_ERROR | LINK_RESET)\n\nstatic const struct asyc_intr_info ainfo[] = {\n\tBUILD_SISL_ASTATUS_FC_PORT(1),\n\tBUILD_SISL_ASTATUS_FC_PORT(0),\n\tBUILD_SISL_ASTATUS_FC_PORT(3),\n\tBUILD_SISL_ASTATUS_FC_PORT(2)\n};\n\n \nstatic irqreturn_t cxlflash_async_err_irq(int irq, void *data)\n{\n\tstruct hwq *hwq = (struct hwq *)data;\n\tstruct afu *afu = hwq->afu;\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tconst struct asyc_intr_info *info;\n\tstruct sisl_global_map __iomem *global = &afu->afu_map->global;\n\t__be64 __iomem *fc_port_regs;\n\tu64 reg_unmasked;\n\tu64 reg;\n\tu64 bit;\n\tu8 port;\n\n\treg = readq_be(&global->regs.aintr_status);\n\treg_unmasked = (reg & SISL_ASTATUS_UNMASK);\n\n\tif (unlikely(reg_unmasked == 0)) {\n\t\tdev_err(dev, \"%s: spurious interrupt, aintr_status=%016llx\\n\",\n\t\t\t__func__, reg);\n\t\tgoto out;\n\t}\n\n\t \n\twriteq_be(reg_unmasked, &global->regs.aintr_clear);\n\n\t \n\tfor_each_set_bit(bit, (ulong *)&reg_unmasked, BITS_PER_LONG) {\n\t\tif (unlikely(bit >= ARRAY_SIZE(ainfo))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinfo = &ainfo[bit];\n\t\tif (unlikely(info->status != 1ULL << bit)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tport = info->port;\n\t\tfc_port_regs = get_fc_port_regs(cfg, port);\n\n\t\tdev_err(dev, \"%s: FC Port %d -> %s, fc_status=%016llx\\n\",\n\t\t\t__func__, port, info->desc,\n\t\t       readq_be(&fc_port_regs[FC_STATUS / 8]));\n\n\t\t \n\t\tif (info->action & LINK_RESET) {\n\t\t\tdev_err(dev, \"%s: FC Port %d: resetting link\\n\",\n\t\t\t\t__func__, port);\n\t\t\tcfg->lr_state = LINK_RESET_REQUIRED;\n\t\t\tcfg->lr_port = port;\n\t\t\tschedule_work(&cfg->work_q);\n\t\t}\n\n\t\tif (info->action & CLR_FC_ERROR) {\n\t\t\treg = readq_be(&fc_port_regs[FC_ERROR / 8]);\n\n\t\t\t \n\n\t\t\tdev_err(dev, \"%s: fc %d: clearing fc_error=%016llx\\n\",\n\t\t\t\t__func__, port, reg);\n\n\t\t\twriteq_be(reg, &fc_port_regs[FC_ERROR / 8]);\n\t\t\twriteq_be(0, &fc_port_regs[FC_ERRCAP / 8]);\n\t\t}\n\n\t\tif (info->action & SCAN_HOST) {\n\t\t\tatomic_inc(&cfg->scan_host_needed);\n\t\t\tschedule_work(&cfg->work_q);\n\t\t}\n\t}\n\nout:\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int read_vpd(struct cxlflash_cfg *cfg, u64 wwpn[])\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct pci_dev *pdev = cfg->dev;\n\tint i, k, rc = 0;\n\tunsigned int kw_size;\n\tssize_t vpd_size;\n\tchar vpd_data[CXLFLASH_VPD_LEN];\n\tchar tmp_buf[WWPN_BUF_LEN] = { 0 };\n\tconst struct dev_dependent_vals *ddv = (struct dev_dependent_vals *)\n\t\t\t\t\t\tcfg->dev_id->driver_data;\n\tconst bool wwpn_vpd_required = ddv->flags & CXLFLASH_WWPN_VPD_REQUIRED;\n\tconst char *wwpn_vpd_tags[MAX_FC_PORTS] = { \"V5\", \"V6\", \"V7\", \"V8\" };\n\n\t \n\tvpd_size = cfg->ops->read_adapter_vpd(pdev, vpd_data, sizeof(vpd_data));\n\tif (unlikely(vpd_size <= 0)) {\n\t\tdev_err(dev, \"%s: Unable to read VPD (size = %ld)\\n\",\n\t\t\t__func__, vpd_size);\n\t\trc = -ENODEV;\n\t\tgoto out;\n\t}\n\n\t \n\tfor (k = 0; k < cfg->num_fc_ports; k++) {\n\t\ti = pci_vpd_find_ro_info_keyword(vpd_data, vpd_size,\n\t\t\t\t\t\t wwpn_vpd_tags[k], &kw_size);\n\t\tif (i == -ENOENT) {\n\t\t\tif (wwpn_vpd_required)\n\t\t\t\tdev_err(dev, \"%s: Port %d WWPN not found\\n\",\n\t\t\t\t\t__func__, k);\n\t\t\twwpn[k] = 0ULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (i < 0 || kw_size != WWPN_LEN) {\n\t\t\tdev_err(dev, \"%s: Port %d WWPN incomplete or bad VPD\\n\",\n\t\t\t\t__func__, k);\n\t\t\trc = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmemcpy(tmp_buf, &vpd_data[i], WWPN_LEN);\n\t\trc = kstrtoul(tmp_buf, WWPN_LEN, (ulong *)&wwpn[k]);\n\t\tif (unlikely(rc)) {\n\t\t\tdev_err(dev, \"%s: WWPN conversion failed for port %d\\n\",\n\t\t\t\t__func__, k);\n\t\t\trc = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev_dbg(dev, \"%s: wwpn%d=%016llx\\n\", __func__, k, wwpn[k]);\n\t}\n\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic void init_pcr(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct sisl_ctrl_map __iomem *ctrl_map;\n\tstruct hwq *hwq;\n\tvoid *cookie;\n\tint i;\n\n\tfor (i = 0; i < MAX_CONTEXT; i++) {\n\t\tctrl_map = &afu->afu_map->ctrls[i].ctrl;\n\t\t \n\t\t \n\t\twriteq_be(0, &ctrl_map->rht_start);\n\t\twriteq_be(0, &ctrl_map->rht_cnt_id);\n\t\twriteq_be(0, &ctrl_map->ctx_cap);\n\t}\n\n\t \n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\t\tcookie = hwq->ctx_cookie;\n\n\t\thwq->ctx_hndl = (u16) cfg->ops->process_element(cookie);\n\t\thwq->host_map = &afu->afu_map->hosts[hwq->ctx_hndl].host;\n\t\thwq->ctrl_map = &afu->afu_map->ctrls[hwq->ctx_hndl].ctrl;\n\n\t\t \n\t\twriteq_be(SISL_ENDIAN_CTRL, &hwq->host_map->endian_ctrl);\n\t}\n}\n\n \nstatic int init_global(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq;\n\tstruct sisl_host_map __iomem *hmap;\n\t__be64 __iomem *fc_port_regs;\n\tu64 wwpn[MAX_FC_PORTS];\t \n\tint i = 0, num_ports = 0;\n\tint rc = 0;\n\tint j;\n\tvoid *ctx;\n\tu64 reg;\n\n\trc = read_vpd(cfg, &wwpn[0]);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: could not read vpd rc=%d\\n\", __func__, rc);\n\t\tgoto out;\n\t}\n\n\t \n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\t\thmap = hwq->host_map;\n\n\t\twriteq_be((u64) hwq->hrrq_start, &hmap->rrq_start);\n\t\twriteq_be((u64) hwq->hrrq_end, &hmap->rrq_end);\n\t\thwq->hrrq_online = true;\n\n\t\tif (afu_is_sq_cmd_mode(afu)) {\n\t\t\twriteq_be((u64)hwq->hsq_start, &hmap->sq_start);\n\t\t\twriteq_be((u64)hwq->hsq_end, &hmap->sq_end);\n\t\t}\n\t}\n\n\t \n\treg = readq_be(&afu->afu_map->global.regs.afu_config);\n\treg |= SISL_AFUCONF_AR_ALL|SISL_AFUCONF_ENDIAN;\n\t \n\t \n\t \n\t \n\twriteq_be(reg, &afu->afu_map->global.regs.afu_config);\n\n\t \n\tif (afu->internal_lun) {\n\t\t \n\t\twriteq_be(PORT0, &afu->afu_map->global.regs.afu_port_sel);\n\t\tnum_ports = 0;\n\t} else {\n\t\twriteq_be(PORT_MASK(cfg->num_fc_ports),\n\t\t\t  &afu->afu_map->global.regs.afu_port_sel);\n\t\tnum_ports = cfg->num_fc_ports;\n\t}\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tfc_port_regs = get_fc_port_regs(cfg, i);\n\n\t\t \n\t\twriteq_be(0, &fc_port_regs[FC_ERRMSK / 8]);\n\t\t \n\t\t(void)readq_be(&fc_port_regs[FC_CNT_CRCERR / 8]);\n\t\twriteq_be(MC_CRC_THRESH, &fc_port_regs[FC_CRC_THRESH / 8]);\n\n\t\t \n\t\tif (wwpn[i] != 0)\n\t\t\tafu_set_wwpn(afu, i, &fc_port_regs[0], wwpn[i]);\n\t\t \n\t\tmsleep(100);\n\t}\n\n\tif (afu_is_ocxl_lisn(afu)) {\n\t\t \n\t\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\t\thwq = get_hwq(afu, i);\n\t\t\tctx = hwq->ctx_cookie;\n\n\t\t\tfor (j = 0; j < hwq->num_irqs; j++) {\n\t\t\t\treg = cfg->ops->get_irq_objhndl(ctx, j);\n\t\t\t\twriteq_be(reg, &hwq->ctrl_map->lisn_ea[j]);\n\t\t\t}\n\n\t\t\treg = hwq->ctx_hndl;\n\t\t\twriteq_be(SISL_LISN_PASID(reg, reg),\n\t\t\t\t  &hwq->ctrl_map->lisn_pasid[0]);\n\t\t\twriteq_be(SISL_LISN_PASID(0UL, reg),\n\t\t\t\t  &hwq->ctrl_map->lisn_pasid[1]);\n\t\t}\n\t}\n\n\t \n\t \n\t \n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\n\t\t(void)readq_be(&hwq->ctrl_map->mbox_r);\t \n\t\twriteq_be((SISL_CTX_CAP_REAL_MODE | SISL_CTX_CAP_HOST_XLATE |\n\t\t\tSISL_CTX_CAP_READ_CMD | SISL_CTX_CAP_WRITE_CMD |\n\t\t\tSISL_CTX_CAP_AFU_CMD | SISL_CTX_CAP_GSCSI_CMD),\n\t\t\t&hwq->ctrl_map->ctx_cap);\n\t}\n\n\t \n\thwq = get_hwq(afu, PRIMARY_HWQ);\n\treg = readq_be(&hwq->host_map->ctx_ctrl);\n\tif (reg & SISL_CTX_CTRL_UNMAP_SECTOR)\n\t\tcfg->ws_unmap = true;\n\n\t \n\tafu->hb = readq_be(&afu->afu_map->global.regs.afu_hb);\nout:\n\treturn rc;\n}\n\n \nstatic int start_afu(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq;\n\tint rc = 0;\n\tint i;\n\n\tinit_pcr(cfg);\n\n\t \n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\n\t\t \n\t\tmemset(&hwq->rrq_entry, 0, sizeof(hwq->rrq_entry));\n\n\t\t \n\t\thwq->hrrq_start = &hwq->rrq_entry[0];\n\t\thwq->hrrq_end = &hwq->rrq_entry[NUM_RRQ_ENTRY - 1];\n\t\thwq->hrrq_curr = hwq->hrrq_start;\n\t\thwq->toggle = 1;\n\n\t\t \n\t\tspin_lock_init(&hwq->hrrq_slock);\n\t\tspin_lock_init(&hwq->hsq_slock);\n\n\t\t \n\t\tif (afu_is_sq_cmd_mode(afu)) {\n\t\t\tmemset(&hwq->sq, 0, sizeof(hwq->sq));\n\t\t\thwq->hsq_start = &hwq->sq[0];\n\t\t\thwq->hsq_end = &hwq->sq[NUM_SQ_ENTRY - 1];\n\t\t\thwq->hsq_curr = hwq->hsq_start;\n\n\t\t\tatomic_set(&hwq->hsq_credits, NUM_SQ_ENTRY - 1);\n\t\t}\n\n\t\t \n\t\tif (afu_is_irqpoll_enabled(afu))\n\t\t\tirq_poll_init(&hwq->irqpoll, afu->irqpoll_weight,\n\t\t\t\t      cxlflash_irqpoll);\n\n\t}\n\n\trc = init_global(cfg);\n\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic enum undo_level init_intr(struct cxlflash_cfg *cfg,\n\t\t\t\t struct hwq *hwq)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tvoid *ctx = hwq->ctx_cookie;\n\tint rc = 0;\n\tenum undo_level level = UNDO_NOOP;\n\tbool is_primary_hwq = (hwq->index == PRIMARY_HWQ);\n\tint num_irqs = hwq->num_irqs;\n\n\trc = cfg->ops->allocate_afu_irqs(ctx, num_irqs);\n\tif (unlikely(rc)) {\n\t\tdev_err(dev, \"%s: allocate_afu_irqs failed rc=%d\\n\",\n\t\t\t__func__, rc);\n\t\tlevel = UNDO_NOOP;\n\t\tgoto out;\n\t}\n\n\trc = cfg->ops->map_afu_irq(ctx, 1, cxlflash_sync_err_irq, hwq,\n\t\t\t\t   \"SISL_MSI_SYNC_ERROR\");\n\tif (unlikely(rc <= 0)) {\n\t\tdev_err(dev, \"%s: SISL_MSI_SYNC_ERROR map failed\\n\", __func__);\n\t\tlevel = FREE_IRQ;\n\t\tgoto out;\n\t}\n\n\trc = cfg->ops->map_afu_irq(ctx, 2, cxlflash_rrq_irq, hwq,\n\t\t\t\t   \"SISL_MSI_RRQ_UPDATED\");\n\tif (unlikely(rc <= 0)) {\n\t\tdev_err(dev, \"%s: SISL_MSI_RRQ_UPDATED map failed\\n\", __func__);\n\t\tlevel = UNMAP_ONE;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!is_primary_hwq)\n\t\tgoto out;\n\n\trc = cfg->ops->map_afu_irq(ctx, 3, cxlflash_async_err_irq, hwq,\n\t\t\t\t   \"SISL_MSI_ASYNC_ERROR\");\n\tif (unlikely(rc <= 0)) {\n\t\tdev_err(dev, \"%s: SISL_MSI_ASYNC_ERROR map failed\\n\", __func__);\n\t\tlevel = UNMAP_TWO;\n\t\tgoto out;\n\t}\nout:\n\treturn level;\n}\n\n \nstatic int init_mc(struct cxlflash_cfg *cfg, u32 index)\n{\n\tvoid *ctx;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq = get_hwq(cfg->afu, index);\n\tint rc = 0;\n\tint num_irqs;\n\tenum undo_level level;\n\n\thwq->afu = cfg->afu;\n\thwq->index = index;\n\tINIT_LIST_HEAD(&hwq->pending_cmds);\n\n\tif (index == PRIMARY_HWQ) {\n\t\tctx = cfg->ops->get_context(cfg->dev, cfg->afu_cookie);\n\t\tnum_irqs = 3;\n\t} else {\n\t\tctx = cfg->ops->dev_context_init(cfg->dev, cfg->afu_cookie);\n\t\tnum_irqs = 2;\n\t}\n\tif (IS_ERR_OR_NULL(ctx)) {\n\t\trc = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tWARN_ON(hwq->ctx_cookie);\n\thwq->ctx_cookie = ctx;\n\thwq->num_irqs = num_irqs;\n\n\t \n\tcfg->ops->set_master(ctx);\n\n\t \n\tif (index == PRIMARY_HWQ) {\n\t\trc = cfg->ops->afu_reset(ctx);\n\t\tif (unlikely(rc)) {\n\t\t\tdev_err(dev, \"%s: AFU reset failed rc=%d\\n\",\n\t\t\t\t      __func__, rc);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tlevel = init_intr(cfg, hwq);\n\tif (unlikely(level)) {\n\t\tdev_err(dev, \"%s: interrupt init failed rc=%d\\n\", __func__, rc);\n\t\tgoto err2;\n\t}\n\n\t \n\trc = cfg->ops->start_context(hwq->ctx_cookie);\n\tif (unlikely(rc)) {\n\t\tdev_err(dev, \"%s: start context failed rc=%d\\n\", __func__, rc);\n\t\tlevel = UNMAP_THREE;\n\t\tgoto err2;\n\t}\n\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\nerr2:\n\tterm_intr(cfg, level, index);\n\tif (index != PRIMARY_HWQ)\n\t\tcfg->ops->release_context(ctx);\nerr1:\n\thwq->ctx_cookie = NULL;\n\tgoto out;\n}\n\n \nstatic void get_num_afu_ports(struct cxlflash_cfg *cfg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tu64 port_mask;\n\tint num_fc_ports = LEGACY_FC_PORTS;\n\n\tport_mask = readq_be(&afu->afu_map->global.regs.afu_port_sel);\n\tif (port_mask != 0ULL)\n\t\tnum_fc_ports = min(ilog2(port_mask) + 1, MAX_FC_PORTS);\n\n\tdev_dbg(dev, \"%s: port_mask=%016llx num_fc_ports=%d\\n\",\n\t\t__func__, port_mask, num_fc_ports);\n\n\tcfg->num_fc_ports = num_fc_ports;\n\tcfg->host->max_channel = PORTNUM2CHAN(num_fc_ports);\n}\n\n \nstatic int init_afu(struct cxlflash_cfg *cfg)\n{\n\tu64 reg;\n\tint rc = 0;\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct hwq *hwq;\n\tint i;\n\n\tcfg->ops->perst_reloads_same_image(cfg->afu_cookie, true);\n\n\tmutex_init(&afu->sync_active);\n\tafu->num_hwqs = afu->desired_hwqs;\n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\trc = init_mc(cfg, i);\n\t\tif (rc) {\n\t\t\tdev_err(dev, \"%s: init_mc failed rc=%d index=%d\\n\",\n\t\t\t\t__func__, rc, i);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\t \n\thwq = get_hwq(afu, PRIMARY_HWQ);\n\tafu->afu_map = cfg->ops->psa_map(hwq->ctx_cookie);\n\tif (!afu->afu_map) {\n\t\tdev_err(dev, \"%s: psa_map failed\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\t \n\treg = readq(&afu->afu_map->global.regs.afu_version);\n\tmemcpy(afu->version, &reg, sizeof(reg));\n\tafu->interface_version =\n\t    readq_be(&afu->afu_map->global.regs.interface_version);\n\tif ((afu->interface_version + 1) == 0) {\n\t\tdev_err(dev, \"Back level AFU, please upgrade. AFU version %s \"\n\t\t\t\"interface version %016llx\\n\", afu->version,\n\t\t       afu->interface_version);\n\t\trc = -EINVAL;\n\t\tgoto err1;\n\t}\n\n\tif (afu_is_sq_cmd_mode(afu)) {\n\t\tafu->send_cmd = send_cmd_sq;\n\t\tafu->context_reset = context_reset_sq;\n\t} else {\n\t\tafu->send_cmd = send_cmd_ioarrin;\n\t\tafu->context_reset = context_reset_ioarrin;\n\t}\n\n\tdev_dbg(dev, \"%s: afu_ver=%s interface_ver=%016llx\\n\", __func__,\n\t\tafu->version, afu->interface_version);\n\n\tget_num_afu_ports(cfg);\n\n\trc = start_afu(cfg);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: start_afu failed, rc=%d\\n\", __func__, rc);\n\t\tgoto err1;\n\t}\n\n\tafu_err_intr_init(cfg->afu);\n\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\thwq = get_hwq(afu, i);\n\n\t\thwq->room = readq_be(&hwq->host_map->cmd_room);\n\t}\n\n\t \n\tcxlflash_restore_luntable(cfg);\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n\nerr1:\n\tfor (i = afu->num_hwqs - 1; i >= 0; i--) {\n\t\tterm_intr(cfg, UNMAP_THREE, i);\n\t\tterm_mc(cfg, i);\n\t}\n\tgoto out;\n}\n\n \nstatic int afu_reset(struct cxlflash_cfg *cfg)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = 0;\n\n\t \n\tterm_afu(cfg);\n\n\trc = init_afu(cfg);\n\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic void drain_ioctls(struct cxlflash_cfg *cfg)\n{\n\tdown_write(&cfg->ioctl_rwsem);\n\tup_write(&cfg->ioctl_rwsem);\n}\n\n \nstatic void cxlflash_async_reset_host(void *data, async_cookie_t cookie)\n{\n\tstruct cxlflash_cfg *cfg = data;\n\tstruct device *dev = &cfg->dev->dev;\n\tint rc = 0;\n\n\tif (cfg->state != STATE_RESET) {\n\t\tdev_dbg(dev, \"%s: Not performing a reset, state=%d\\n\",\n\t\t\t__func__, cfg->state);\n\t\tgoto out;\n\t}\n\n\tdrain_ioctls(cfg);\n\tcxlflash_mark_contexts_error(cfg);\n\trc = afu_reset(cfg);\n\tif (rc)\n\t\tcfg->state = STATE_FAILTERM;\n\telse\n\t\tcfg->state = STATE_NORMAL;\n\twake_up_all(&cfg->reset_waitq);\n\nout:\n\tscsi_unblock_requests(cfg->host);\n}\n\n \nstatic void cxlflash_schedule_async_reset(struct cxlflash_cfg *cfg)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\n\tif (cfg->state != STATE_NORMAL) {\n\t\tdev_dbg(dev, \"%s: Not performing reset state=%d\\n\",\n\t\t\t__func__, cfg->state);\n\t\treturn;\n\t}\n\n\tcfg->state = STATE_RESET;\n\tscsi_block_requests(cfg->host);\n\tcfg->async_reset_cookie = async_schedule(cxlflash_async_reset_host,\n\t\t\t\t\t\t cfg);\n}\n\n \nstatic int send_afu_cmd(struct afu *afu, struct sisl_ioarcb *rcb)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct afu_cmd *cmd = NULL;\n\tstruct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);\n\tulong lock_flags;\n\tchar *buf = NULL;\n\tint rc = 0;\n\tint nretry = 0;\n\n\tif (cfg->state != STATE_NORMAL) {\n\t\tdev_dbg(dev, \"%s: Sync not required state=%u\\n\",\n\t\t\t__func__, cfg->state);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&afu->sync_active);\n\tatomic_inc(&afu->cmds_active);\n\tbuf = kmalloc(sizeof(*cmd) + __alignof__(*cmd) - 1, GFP_KERNEL);\n\tif (unlikely(!buf)) {\n\t\tdev_err(dev, \"%s: no memory for command\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcmd = (struct afu_cmd *)PTR_ALIGN(buf, __alignof__(*cmd));\n\nretry:\n\tmemset(cmd, 0, sizeof(*cmd));\n\tmemcpy(&cmd->rcb, rcb, sizeof(*rcb));\n\tINIT_LIST_HEAD(&cmd->queue);\n\tinit_completion(&cmd->cevent);\n\tcmd->parent = afu;\n\tcmd->hwq_index = hwq->index;\n\tcmd->rcb.ctx_id = hwq->ctx_hndl;\n\n\tdev_dbg(dev, \"%s: afu=%p cmd=%p type=%02x nretry=%d\\n\",\n\t\t__func__, afu, cmd, cmd->rcb.cdb[0], nretry);\n\n\trc = afu->send_cmd(afu, cmd);\n\tif (unlikely(rc)) {\n\t\trc = -ENOBUFS;\n\t\tgoto out;\n\t}\n\n\trc = wait_resp(afu, cmd);\n\tswitch (rc) {\n\tcase -ETIMEDOUT:\n\t\trc = afu->context_reset(hwq);\n\t\tif (rc) {\n\t\t\t \n\t\t\tspin_lock_irqsave(&hwq->hsq_slock, lock_flags);\n\t\t\tlist_del(&cmd->list);\n\t\t\tspin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);\n\n\t\t\tcxlflash_schedule_async_reset(cfg);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\t \n\tcase -EAGAIN:\n\t\tif (++nretry < 2)\n\t\t\tgoto retry;\n\t\tfallthrough;\t \n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (rcb->ioasa)\n\t\t*rcb->ioasa = cmd->sa;\nout:\n\tatomic_dec(&afu->cmds_active);\n\tmutex_unlock(&afu->sync_active);\n\tkfree(buf);\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nint cxlflash_afu_sync(struct afu *afu, ctx_hndl_t ctx, res_hndl_t res, u8 mode)\n{\n\tstruct cxlflash_cfg *cfg = afu->parent;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct sisl_ioarcb rcb = { 0 };\n\n\tdev_dbg(dev, \"%s: afu=%p ctx=%u res=%u mode=%u\\n\",\n\t\t__func__, afu, ctx, res, mode);\n\n\trcb.req_flags = SISL_REQ_FLAGS_AFU_CMD;\n\trcb.msi = SISL_MSI_RRQ_UPDATED;\n\trcb.timeout = MC_AFU_SYNC_TIMEOUT;\n\n\trcb.cdb[0] = SISL_AFU_CMD_SYNC;\n\trcb.cdb[1] = mode;\n\tput_unaligned_be16(ctx, &rcb.cdb[2]);\n\tput_unaligned_be32(res, &rcb.cdb[4]);\n\n\treturn send_afu_cmd(afu, &rcb);\n}\n\n \nstatic int cxlflash_eh_abort_handler(struct scsi_cmnd *scp)\n{\n\tint rc = FAILED;\n\tstruct Scsi_Host *host = scp->device->host;\n\tstruct cxlflash_cfg *cfg = shost_priv(host);\n\tstruct afu_cmd *cmd = sc_to_afuc(scp);\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct afu *afu = cfg->afu;\n\tstruct hwq *hwq = get_hwq(afu, cmd->hwq_index);\n\n\tdev_dbg(dev, \"%s: (scp=%p) %d/%d/%d/%llu \"\n\t\t\"cdb=(%08x-%08x-%08x-%08x)\\n\", __func__, scp, host->host_no,\n\t\tscp->device->channel, scp->device->id, scp->device->lun,\n\t\tget_unaligned_be32(&((u32 *)scp->cmnd)[0]),\n\t\tget_unaligned_be32(&((u32 *)scp->cmnd)[1]),\n\t\tget_unaligned_be32(&((u32 *)scp->cmnd)[2]),\n\t\tget_unaligned_be32(&((u32 *)scp->cmnd)[3]));\n\n\t \n\tif (cfg->state != STATE_NORMAL) {\n\t\tdev_dbg(dev, \"%s: Invalid state for abort, state=%d\\n\",\n\t\t\t__func__, cfg->state);\n\t\tgoto out;\n\t}\n\n\trc = afu->context_reset(hwq);\n\tif (unlikely(rc))\n\t\tgoto out;\n\n\trc = SUCCESS;\n\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic int cxlflash_eh_device_reset_handler(struct scsi_cmnd *scp)\n{\n\tint rc = SUCCESS;\n\tstruct scsi_device *sdev = scp->device;\n\tstruct Scsi_Host *host = sdev->host;\n\tstruct cxlflash_cfg *cfg = shost_priv(host);\n\tstruct device *dev = &cfg->dev->dev;\n\tint rcr = 0;\n\n\tdev_dbg(dev, \"%s: %d/%d/%d/%llu\\n\", __func__,\n\t\thost->host_no, sdev->channel, sdev->id, sdev->lun);\nretry:\n\tswitch (cfg->state) {\n\tcase STATE_NORMAL:\n\t\trcr = send_tmf(cfg, sdev, TMF_LUN_RESET);\n\t\tif (unlikely(rcr))\n\t\t\trc = FAILED;\n\t\tbreak;\n\tcase STATE_RESET:\n\t\twait_event(cfg->reset_waitq, cfg->state != STATE_RESET);\n\t\tgoto retry;\n\tdefault:\n\t\trc = FAILED;\n\t\tbreak;\n\t}\n\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic int cxlflash_eh_host_reset_handler(struct scsi_cmnd *scp)\n{\n\tint rc = SUCCESS;\n\tint rcr = 0;\n\tstruct Scsi_Host *host = scp->device->host;\n\tstruct cxlflash_cfg *cfg = shost_priv(host);\n\tstruct device *dev = &cfg->dev->dev;\n\n\tdev_dbg(dev, \"%s: %d\\n\", __func__, host->host_no);\n\n\tswitch (cfg->state) {\n\tcase STATE_NORMAL:\n\t\tcfg->state = STATE_RESET;\n\t\tdrain_ioctls(cfg);\n\t\tcxlflash_mark_contexts_error(cfg);\n\t\trcr = afu_reset(cfg);\n\t\tif (rcr) {\n\t\t\trc = FAILED;\n\t\t\tcfg->state = STATE_FAILTERM;\n\t\t} else\n\t\t\tcfg->state = STATE_NORMAL;\n\t\twake_up_all(&cfg->reset_waitq);\n\t\tssleep(1);\n\t\tfallthrough;\n\tcase STATE_RESET:\n\t\twait_event(cfg->reset_waitq, cfg->state != STATE_RESET);\n\t\tif (cfg->state == STATE_NORMAL)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\trc = FAILED;\n\t\tbreak;\n\t}\n\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic int cxlflash_change_queue_depth(struct scsi_device *sdev, int qdepth)\n{\n\n\tif (qdepth > CXLFLASH_MAX_CMDS_PER_LUN)\n\t\tqdepth = CXLFLASH_MAX_CMDS_PER_LUN;\n\n\tscsi_change_queue_depth(sdev, qdepth);\n\treturn sdev->queue_depth;\n}\n\n \nstatic ssize_t cxlflash_show_port_status(u32 port,\n\t\t\t\t\t struct cxlflash_cfg *cfg,\n\t\t\t\t\t char *buf)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tchar *disp_status;\n\tu64 status;\n\t__be64 __iomem *fc_port_regs;\n\n\tWARN_ON(port >= MAX_FC_PORTS);\n\n\tif (port >= cfg->num_fc_ports) {\n\t\tdev_info(dev, \"%s: Port %d not supported on this card.\\n\",\n\t\t\t__func__, port);\n\t\treturn -EINVAL;\n\t}\n\n\tfc_port_regs = get_fc_port_regs(cfg, port);\n\tstatus = readq_be(&fc_port_regs[FC_MTIP_STATUS / 8]);\n\tstatus &= FC_MTIP_STATUS_MASK;\n\n\tif (status == FC_MTIP_STATUS_ONLINE)\n\t\tdisp_status = \"online\";\n\telse if (status == FC_MTIP_STATUS_OFFLINE)\n\t\tdisp_status = \"offline\";\n\telse\n\t\tdisp_status = \"unknown\";\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%s\\n\", disp_status);\n}\n\n \nstatic ssize_t port0_show(struct device *dev,\n\t\t\t  struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_status(0, cfg, buf);\n}\n\n \nstatic ssize_t port1_show(struct device *dev,\n\t\t\t  struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_status(1, cfg, buf);\n}\n\n \nstatic ssize_t port2_show(struct device *dev,\n\t\t\t  struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_status(2, cfg, buf);\n}\n\n \nstatic ssize_t port3_show(struct device *dev,\n\t\t\t  struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_status(3, cfg, buf);\n}\n\n \nstatic ssize_t lun_mode_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct afu *afu = cfg->afu;\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%u\\n\", afu->internal_lun);\n}\n\n \nstatic ssize_t lun_mode_store(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct cxlflash_cfg *cfg = shost_priv(shost);\n\tstruct afu *afu = cfg->afu;\n\tint rc;\n\tu32 lun_mode;\n\n\trc = kstrtouint(buf, 10, &lun_mode);\n\tif (!rc && (lun_mode < 5) && (lun_mode != afu->internal_lun)) {\n\t\tafu->internal_lun = lun_mode;\n\n\t\t \n\t\tif (afu->internal_lun)\n\t\t\tshost->max_channel = 0;\n\t\telse\n\t\t\tshost->max_channel = PORTNUM2CHAN(cfg->num_fc_ports);\n\n\t\tafu_reset(cfg);\n\t\tscsi_scan_host(cfg->host);\n\t}\n\n\treturn count;\n}\n\n \nstatic ssize_t ioctl_version_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tssize_t bytes = 0;\n\n\tbytes = scnprintf(buf, PAGE_SIZE,\n\t\t\t  \"disk: %u\\n\", DK_CXLFLASH_VERSION_0);\n\tbytes += scnprintf(buf + bytes, PAGE_SIZE - bytes,\n\t\t\t   \"host: %u\\n\", HT_CXLFLASH_VERSION_0);\n\n\treturn bytes;\n}\n\n \nstatic ssize_t cxlflash_show_port_lun_table(u32 port,\n\t\t\t\t\t    struct cxlflash_cfg *cfg,\n\t\t\t\t\t    char *buf)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\t__be64 __iomem *fc_port_luns;\n\tint i;\n\tssize_t bytes = 0;\n\n\tWARN_ON(port >= MAX_FC_PORTS);\n\n\tif (port >= cfg->num_fc_ports) {\n\t\tdev_info(dev, \"%s: Port %d not supported on this card.\\n\",\n\t\t\t__func__, port);\n\t\treturn -EINVAL;\n\t}\n\n\tfc_port_luns = get_fc_port_luns(cfg, port);\n\n\tfor (i = 0; i < CXLFLASH_NUM_VLUNS; i++)\n\t\tbytes += scnprintf(buf + bytes, PAGE_SIZE - bytes,\n\t\t\t\t   \"%03d: %016llx\\n\",\n\t\t\t\t   i, readq_be(&fc_port_luns[i]));\n\treturn bytes;\n}\n\n \nstatic ssize_t port0_lun_table_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_lun_table(0, cfg, buf);\n}\n\n \nstatic ssize_t port1_lun_table_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_lun_table(1, cfg, buf);\n}\n\n \nstatic ssize_t port2_lun_table_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_lun_table(2, cfg, buf);\n}\n\n \nstatic ssize_t port3_lun_table_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\n\treturn cxlflash_show_port_lun_table(3, cfg, buf);\n}\n\n \nstatic ssize_t irqpoll_weight_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct afu *afu = cfg->afu;\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%u\\n\", afu->irqpoll_weight);\n}\n\n \nstatic ssize_t irqpoll_weight_store(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t count)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct device *cfgdev = &cfg->dev->dev;\n\tstruct afu *afu = cfg->afu;\n\tstruct hwq *hwq;\n\tu32 weight;\n\tint rc, i;\n\n\trc = kstrtouint(buf, 10, &weight);\n\tif (rc)\n\t\treturn -EINVAL;\n\n\tif (weight > 256) {\n\t\tdev_info(cfgdev,\n\t\t\t \"Invalid IRQ poll weight. It must be 256 or less.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (weight == afu->irqpoll_weight) {\n\t\tdev_info(cfgdev,\n\t\t\t \"Current IRQ poll weight has the same weight.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (afu_is_irqpoll_enabled(afu)) {\n\t\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\t\thwq = get_hwq(afu, i);\n\n\t\t\tirq_poll_disable(&hwq->irqpoll);\n\t\t}\n\t}\n\n\tafu->irqpoll_weight = weight;\n\n\tif (weight > 0) {\n\t\tfor (i = 0; i < afu->num_hwqs; i++) {\n\t\t\thwq = get_hwq(afu, i);\n\n\t\t\tirq_poll_init(&hwq->irqpoll, weight, cxlflash_irqpoll);\n\t\t}\n\t}\n\n\treturn count;\n}\n\n \nstatic ssize_t num_hwqs_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct afu *afu = cfg->afu;\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%u\\n\", afu->num_hwqs);\n}\n\n \nstatic ssize_t num_hwqs_store(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct afu *afu = cfg->afu;\n\tint rc;\n\tint nhwqs, num_hwqs;\n\n\trc = kstrtoint(buf, 10, &nhwqs);\n\tif (rc)\n\t\treturn -EINVAL;\n\n\tif (nhwqs >= 1)\n\t\tnum_hwqs = nhwqs;\n\telse if (nhwqs == 0)\n\t\tnum_hwqs = num_online_cpus();\n\telse\n\t\tnum_hwqs = num_online_cpus() / abs(nhwqs);\n\n\tafu->desired_hwqs = min(num_hwqs, CXLFLASH_MAX_HWQS);\n\tWARN_ON_ONCE(afu->desired_hwqs == 0);\n\nretry:\n\tswitch (cfg->state) {\n\tcase STATE_NORMAL:\n\t\tcfg->state = STATE_RESET;\n\t\tdrain_ioctls(cfg);\n\t\tcxlflash_mark_contexts_error(cfg);\n\t\trc = afu_reset(cfg);\n\t\tif (rc)\n\t\t\tcfg->state = STATE_FAILTERM;\n\t\telse\n\t\t\tcfg->state = STATE_NORMAL;\n\t\twake_up_all(&cfg->reset_waitq);\n\t\tbreak;\n\tcase STATE_RESET:\n\t\twait_event(cfg->reset_waitq, cfg->state != STATE_RESET);\n\t\tif (cfg->state == STATE_NORMAL)\n\t\t\tgoto retry;\n\t\tfallthrough;\n\tdefault:\n\t\t \n\t\tdev_err(dev, \"%s: Device is not ready, state=%d\\n\",\n\t\t\t__func__, cfg->state);\n\t\tbreak;\n\t}\n\n\treturn count;\n}\n\nstatic const char *hwq_mode_name[MAX_HWQ_MODE] = { \"rr\", \"tag\", \"cpu\" };\n\n \nstatic ssize_t hwq_mode_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));\n\tstruct afu *afu = cfg->afu;\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%s\\n\", hwq_mode_name[afu->hwq_mode]);\n}\n\n \nstatic ssize_t hwq_mode_store(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct cxlflash_cfg *cfg = shost_priv(shost);\n\tstruct device *cfgdev = &cfg->dev->dev;\n\tstruct afu *afu = cfg->afu;\n\tint i;\n\tu32 mode = MAX_HWQ_MODE;\n\n\tfor (i = 0; i < MAX_HWQ_MODE; i++) {\n\t\tif (!strncmp(hwq_mode_name[i], buf, strlen(hwq_mode_name[i]))) {\n\t\t\tmode = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (mode >= MAX_HWQ_MODE) {\n\t\tdev_info(cfgdev, \"Invalid HWQ steering mode.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tafu->hwq_mode = mode;\n\n\treturn count;\n}\n\n \nstatic ssize_t mode_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t sdev->hostdata ? \"superpipe\" : \"legacy\");\n}\n\n \nstatic DEVICE_ATTR_RO(port0);\nstatic DEVICE_ATTR_RO(port1);\nstatic DEVICE_ATTR_RO(port2);\nstatic DEVICE_ATTR_RO(port3);\nstatic DEVICE_ATTR_RW(lun_mode);\nstatic DEVICE_ATTR_RO(ioctl_version);\nstatic DEVICE_ATTR_RO(port0_lun_table);\nstatic DEVICE_ATTR_RO(port1_lun_table);\nstatic DEVICE_ATTR_RO(port2_lun_table);\nstatic DEVICE_ATTR_RO(port3_lun_table);\nstatic DEVICE_ATTR_RW(irqpoll_weight);\nstatic DEVICE_ATTR_RW(num_hwqs);\nstatic DEVICE_ATTR_RW(hwq_mode);\n\nstatic struct attribute *cxlflash_host_attrs[] = {\n\t&dev_attr_port0.attr,\n\t&dev_attr_port1.attr,\n\t&dev_attr_port2.attr,\n\t&dev_attr_port3.attr,\n\t&dev_attr_lun_mode.attr,\n\t&dev_attr_ioctl_version.attr,\n\t&dev_attr_port0_lun_table.attr,\n\t&dev_attr_port1_lun_table.attr,\n\t&dev_attr_port2_lun_table.attr,\n\t&dev_attr_port3_lun_table.attr,\n\t&dev_attr_irqpoll_weight.attr,\n\t&dev_attr_num_hwqs.attr,\n\t&dev_attr_hwq_mode.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(cxlflash_host);\n\n \nstatic DEVICE_ATTR_RO(mode);\n\nstatic struct attribute *cxlflash_dev_attrs[] = {\n\t&dev_attr_mode.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(cxlflash_dev);\n\n \nstatic struct scsi_host_template driver_template = {\n\t.module = THIS_MODULE,\n\t.name = CXLFLASH_ADAPTER_NAME,\n\t.info = cxlflash_driver_info,\n\t.ioctl = cxlflash_ioctl,\n\t.proc_name = CXLFLASH_NAME,\n\t.queuecommand = cxlflash_queuecommand,\n\t.eh_abort_handler = cxlflash_eh_abort_handler,\n\t.eh_device_reset_handler = cxlflash_eh_device_reset_handler,\n\t.eh_host_reset_handler = cxlflash_eh_host_reset_handler,\n\t.change_queue_depth = cxlflash_change_queue_depth,\n\t.cmd_per_lun = CXLFLASH_MAX_CMDS_PER_LUN,\n\t.can_queue = CXLFLASH_MAX_CMDS,\n\t.cmd_size = sizeof(struct afu_cmd) + __alignof__(struct afu_cmd) - 1,\n\t.this_id = -1,\n\t.sg_tablesize = 1,\t \n\t.max_sectors = CXLFLASH_MAX_SECTORS,\n\t.shost_groups = cxlflash_host_groups,\n\t.sdev_groups = cxlflash_dev_groups,\n};\n\n \nstatic struct dev_dependent_vals dev_corsa_vals = { CXLFLASH_MAX_SECTORS,\n\t\t\t\t\tCXLFLASH_WWPN_VPD_REQUIRED };\nstatic struct dev_dependent_vals dev_flash_gt_vals = { CXLFLASH_MAX_SECTORS,\n\t\t\t\t\tCXLFLASH_NOTIFY_SHUTDOWN };\nstatic struct dev_dependent_vals dev_briard_vals = { CXLFLASH_MAX_SECTORS,\n\t\t\t\t\t(CXLFLASH_NOTIFY_SHUTDOWN |\n\t\t\t\t\tCXLFLASH_OCXL_DEV) };\n\n \nstatic struct pci_device_id cxlflash_pci_table[] = {\n\t{PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_CORSA,\n\t PCI_ANY_ID, PCI_ANY_ID, 0, 0, (kernel_ulong_t)&dev_corsa_vals},\n\t{PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_FLASH_GT,\n\t PCI_ANY_ID, PCI_ANY_ID, 0, 0, (kernel_ulong_t)&dev_flash_gt_vals},\n\t{PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_BRIARD,\n\t PCI_ANY_ID, PCI_ANY_ID, 0, 0, (kernel_ulong_t)&dev_briard_vals},\n\t{}\n};\n\nMODULE_DEVICE_TABLE(pci, cxlflash_pci_table);\n\n \nstatic void cxlflash_worker_thread(struct work_struct *work)\n{\n\tstruct cxlflash_cfg *cfg = container_of(work, struct cxlflash_cfg,\n\t\t\t\t\t\twork_q);\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\t__be64 __iomem *fc_port_regs;\n\tint port;\n\tulong lock_flags;\n\n\t \n\n\tif (cfg->state != STATE_NORMAL)\n\t\treturn;\n\n\tspin_lock_irqsave(cfg->host->host_lock, lock_flags);\n\n\tif (cfg->lr_state == LINK_RESET_REQUIRED) {\n\t\tport = cfg->lr_port;\n\t\tif (port < 0)\n\t\t\tdev_err(dev, \"%s: invalid port index %d\\n\",\n\t\t\t\t__func__, port);\n\t\telse {\n\t\t\tspin_unlock_irqrestore(cfg->host->host_lock,\n\t\t\t\t\t       lock_flags);\n\n\t\t\t \n\t\t\tfc_port_regs = get_fc_port_regs(cfg, port);\n\t\t\tafu_link_reset(afu, port, fc_port_regs);\n\t\t\tspin_lock_irqsave(cfg->host->host_lock, lock_flags);\n\t\t}\n\n\t\tcfg->lr_state = LINK_RESET_COMPLETE;\n\t}\n\n\tspin_unlock_irqrestore(cfg->host->host_lock, lock_flags);\n\n\tif (atomic_dec_if_positive(&cfg->scan_host_needed) >= 0)\n\t\tscsi_scan_host(cfg->host);\n}\n\n \nstatic int cxlflash_chr_open(struct inode *inode, struct file *file)\n{\n\tstruct cxlflash_cfg *cfg;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tcfg = container_of(inode->i_cdev, struct cxlflash_cfg, cdev);\n\tfile->private_data = cfg;\n\n\treturn 0;\n}\n\n \nstatic char *decode_hioctl(unsigned int cmd)\n{\n\tswitch (cmd) {\n\tcase HT_CXLFLASH_LUN_PROVISION:\n\t\treturn __stringify_1(HT_CXLFLASH_LUN_PROVISION);\n\t}\n\n\treturn \"UNKNOWN\";\n}\n\n \nstatic int cxlflash_lun_provision(struct cxlflash_cfg *cfg,\n\t\t\t\t  struct ht_cxlflash_lun_provision *lunprov)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct sisl_ioarcb rcb;\n\tstruct sisl_ioasa asa;\n\t__be64 __iomem *fc_port_regs;\n\tu16 port = lunprov->port;\n\tu16 scmd = lunprov->hdr.subcmd;\n\tu16 type;\n\tu64 reg;\n\tu64 size;\n\tu64 lun_id;\n\tint rc = 0;\n\n\tif (!afu_is_lun_provision(afu)) {\n\t\trc = -ENOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (port >= cfg->num_fc_ports) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tswitch (scmd) {\n\tcase HT_CXLFLASH_LUN_PROVISION_SUBCMD_CREATE_LUN:\n\t\ttype = SISL_AFU_LUN_PROVISION_CREATE;\n\t\tsize = lunprov->size;\n\t\tlun_id = 0;\n\t\tbreak;\n\tcase HT_CXLFLASH_LUN_PROVISION_SUBCMD_DELETE_LUN:\n\t\ttype = SISL_AFU_LUN_PROVISION_DELETE;\n\t\tsize = 0;\n\t\tlun_id = lunprov->lun_id;\n\t\tbreak;\n\tcase HT_CXLFLASH_LUN_PROVISION_SUBCMD_QUERY_PORT:\n\t\tfc_port_regs = get_fc_port_regs(cfg, port);\n\n\t\treg = readq_be(&fc_port_regs[FC_MAX_NUM_LUNS / 8]);\n\t\tlunprov->max_num_luns = reg;\n\t\treg = readq_be(&fc_port_regs[FC_CUR_NUM_LUNS / 8]);\n\t\tlunprov->cur_num_luns = reg;\n\t\treg = readq_be(&fc_port_regs[FC_MAX_CAP_PORT / 8]);\n\t\tlunprov->max_cap_port = reg;\n\t\treg = readq_be(&fc_port_regs[FC_CUR_CAP_PORT / 8]);\n\t\tlunprov->cur_cap_port = reg;\n\n\t\tgoto out;\n\tdefault:\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&rcb, 0, sizeof(rcb));\n\tmemset(&asa, 0, sizeof(asa));\n\trcb.req_flags = SISL_REQ_FLAGS_AFU_CMD;\n\trcb.lun_id = lun_id;\n\trcb.msi = SISL_MSI_RRQ_UPDATED;\n\trcb.timeout = MC_LUN_PROV_TIMEOUT;\n\trcb.ioasa = &asa;\n\n\trcb.cdb[0] = SISL_AFU_CMD_LUN_PROVISION;\n\trcb.cdb[1] = type;\n\trcb.cdb[2] = port;\n\tput_unaligned_be64(size, &rcb.cdb[8]);\n\n\trc = send_afu_cmd(afu, &rcb);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: send_afu_cmd failed rc=%d asc=%08x afux=%x\\n\",\n\t\t\t__func__, rc, asa.ioasc, asa.afu_extra);\n\t\tgoto out;\n\t}\n\n\tif (scmd == HT_CXLFLASH_LUN_PROVISION_SUBCMD_CREATE_LUN) {\n\t\tlunprov->lun_id = (u64)asa.lunid_hi << 32 | asa.lunid_lo;\n\t\tmemcpy(lunprov->wwid, asa.wwid, sizeof(lunprov->wwid));\n\t}\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic int cxlflash_afu_debug(struct cxlflash_cfg *cfg,\n\t\t\t      struct ht_cxlflash_afu_debug *afu_dbg)\n{\n\tstruct afu *afu = cfg->afu;\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct sisl_ioarcb rcb;\n\tstruct sisl_ioasa asa;\n\tchar *buf = NULL;\n\tchar *kbuf = NULL;\n\tvoid __user *ubuf = (__force void __user *)afu_dbg->data_ea;\n\tu16 req_flags = SISL_REQ_FLAGS_AFU_CMD;\n\tu32 ulen = afu_dbg->data_len;\n\tbool is_write = afu_dbg->hdr.flags & HT_CXLFLASH_HOST_WRITE;\n\tint rc = 0;\n\n\tif (!afu_is_afu_debug(afu)) {\n\t\trc = -ENOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (ulen) {\n\t\treq_flags |= SISL_REQ_FLAGS_SUP_UNDERRUN;\n\n\t\tif (ulen > HT_CXLFLASH_AFU_DEBUG_MAX_DATA_LEN) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbuf = kmalloc(ulen + cache_line_size() - 1, GFP_KERNEL);\n\t\tif (unlikely(!buf)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkbuf = PTR_ALIGN(buf, cache_line_size());\n\n\t\tif (is_write) {\n\t\t\treq_flags |= SISL_REQ_FLAGS_HOST_WRITE;\n\n\t\t\tif (copy_from_user(kbuf, ubuf, ulen)) {\n\t\t\t\trc = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tmemset(&rcb, 0, sizeof(rcb));\n\tmemset(&asa, 0, sizeof(asa));\n\n\trcb.req_flags = req_flags;\n\trcb.msi = SISL_MSI_RRQ_UPDATED;\n\trcb.timeout = MC_AFU_DEBUG_TIMEOUT;\n\trcb.ioasa = &asa;\n\n\tif (ulen) {\n\t\trcb.data_len = ulen;\n\t\trcb.data_ea = (uintptr_t)kbuf;\n\t}\n\n\trcb.cdb[0] = SISL_AFU_CMD_DEBUG;\n\tmemcpy(&rcb.cdb[4], afu_dbg->afu_subcmd,\n\t       HT_CXLFLASH_AFU_DEBUG_SUBCMD_LEN);\n\n\trc = send_afu_cmd(afu, &rcb);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: send_afu_cmd failed rc=%d asc=%08x afux=%x\\n\",\n\t\t\t__func__, rc, asa.ioasc, asa.afu_extra);\n\t\tgoto out;\n\t}\n\n\tif (ulen && !is_write) {\n\t\tif (copy_to_user(ubuf, kbuf, ulen))\n\t\t\trc = -EFAULT;\n\t}\nout:\n\tkfree(buf);\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n}\n\n \nstatic long cxlflash_chr_ioctl(struct file *file, unsigned int cmd,\n\t\t\t       unsigned long arg)\n{\n\ttypedef int (*hioctl) (struct cxlflash_cfg *, void *);\n\n\tstruct cxlflash_cfg *cfg = file->private_data;\n\tstruct device *dev = &cfg->dev->dev;\n\tchar buf[sizeof(union cxlflash_ht_ioctls)];\n\tvoid __user *uarg = (void __user *)arg;\n\tstruct ht_cxlflash_hdr *hdr;\n\tsize_t size = 0;\n\tbool known_ioctl = false;\n\tint idx = 0;\n\tint rc = 0;\n\thioctl do_ioctl = NULL;\n\n\tstatic const struct {\n\t\tsize_t size;\n\t\thioctl ioctl;\n\t} ioctl_tbl[] = {\t \n\t{ sizeof(struct ht_cxlflash_lun_provision),\n\t\t(hioctl)cxlflash_lun_provision },\n\t{ sizeof(struct ht_cxlflash_afu_debug),\n\t\t(hioctl)cxlflash_afu_debug },\n\t};\n\n\t \n\tdown_read(&cfg->ioctl_rwsem);\n\n\tdev_dbg(dev, \"%s: cmd=%u idx=%d tbl_size=%lu\\n\",\n\t\t__func__, cmd, idx, sizeof(ioctl_tbl));\n\n\tswitch (cmd) {\n\tcase HT_CXLFLASH_LUN_PROVISION:\n\tcase HT_CXLFLASH_AFU_DEBUG:\n\t\tknown_ioctl = true;\n\t\tidx = _IOC_NR(HT_CXLFLASH_LUN_PROVISION) - _IOC_NR(cmd);\n\t\tsize = ioctl_tbl[idx].size;\n\t\tdo_ioctl = ioctl_tbl[idx].ioctl;\n\n\t\tif (likely(do_ioctl))\n\t\t\tbreak;\n\n\t\tfallthrough;\n\tdefault:\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(copy_from_user(&buf, uarg, size))) {\n\t\tdev_err(dev, \"%s: copy_from_user() fail \"\n\t\t\t\"size=%lu cmd=%d (%s) uarg=%p\\n\",\n\t\t\t__func__, size, cmd, decode_hioctl(cmd), uarg);\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\thdr = (struct ht_cxlflash_hdr *)&buf;\n\tif (hdr->version != HT_CXLFLASH_VERSION_0) {\n\t\tdev_dbg(dev, \"%s: Version %u not supported for %s\\n\",\n\t\t\t__func__, hdr->version, decode_hioctl(cmd));\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (hdr->rsvd[0] || hdr->rsvd[1] || hdr->return_flags) {\n\t\tdev_dbg(dev, \"%s: Reserved/rflags populated\\n\", __func__);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\trc = do_ioctl(cfg, (void *)&buf);\n\tif (likely(!rc))\n\t\tif (unlikely(copy_to_user(uarg, &buf, size))) {\n\t\t\tdev_err(dev, \"%s: copy_to_user() fail \"\n\t\t\t\t\"size=%lu cmd=%d (%s) uarg=%p\\n\",\n\t\t\t\t__func__, size, cmd, decode_hioctl(cmd), uarg);\n\t\t\trc = -EFAULT;\n\t\t}\n\n\t \n\nout:\n\tup_read(&cfg->ioctl_rwsem);\n\tif (unlikely(rc && known_ioctl))\n\t\tdev_err(dev, \"%s: ioctl %s (%08X) returned rc=%d\\n\",\n\t\t\t__func__, decode_hioctl(cmd), cmd, rc);\n\telse\n\t\tdev_dbg(dev, \"%s: ioctl %s (%08X) returned rc=%d\\n\",\n\t\t\t__func__, decode_hioctl(cmd), cmd, rc);\n\treturn rc;\n}\n\n \nstatic const struct file_operations cxlflash_chr_fops = {\n\t.owner          = THIS_MODULE,\n\t.open           = cxlflash_chr_open,\n\t.unlocked_ioctl\t= cxlflash_chr_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n};\n\n \nstatic int init_chrdev(struct cxlflash_cfg *cfg)\n{\n\tstruct device *dev = &cfg->dev->dev;\n\tstruct device *char_dev;\n\tdev_t devno;\n\tint minor;\n\tint rc = 0;\n\n\tminor = cxlflash_get_minor();\n\tif (unlikely(minor < 0)) {\n\t\tdev_err(dev, \"%s: Exhausted allowed adapters\\n\", __func__);\n\t\trc = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tdevno = MKDEV(cxlflash_major, minor);\n\tcdev_init(&cfg->cdev, &cxlflash_chr_fops);\n\n\trc = cdev_add(&cfg->cdev, devno, 1);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: cdev_add failed rc=%d\\n\", __func__, rc);\n\t\tgoto err1;\n\t}\n\n\tchar_dev = device_create(cxlflash_class, NULL, devno,\n\t\t\t\t NULL, \"cxlflash%d\", minor);\n\tif (IS_ERR(char_dev)) {\n\t\trc = PTR_ERR(char_dev);\n\t\tdev_err(dev, \"%s: device_create failed rc=%d\\n\",\n\t\t\t__func__, rc);\n\t\tgoto err2;\n\t}\n\n\tcfg->chardev = char_dev;\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\nerr2:\n\tcdev_del(&cfg->cdev);\nerr1:\n\tcxlflash_put_minor(minor);\n\tgoto out;\n}\n\n \nstatic int cxlflash_probe(struct pci_dev *pdev,\n\t\t\t  const struct pci_device_id *dev_id)\n{\n\tstruct Scsi_Host *host;\n\tstruct cxlflash_cfg *cfg = NULL;\n\tstruct device *dev = &pdev->dev;\n\tstruct dev_dependent_vals *ddv;\n\tint rc = 0;\n\tint k;\n\n\tdev_dbg(&pdev->dev, \"%s: Found CXLFLASH with IRQ: %d\\n\",\n\t\t__func__, pdev->irq);\n\n\tddv = (struct dev_dependent_vals *)dev_id->driver_data;\n\tdriver_template.max_sectors = ddv->max_sectors;\n\n\thost = scsi_host_alloc(&driver_template, sizeof(struct cxlflash_cfg));\n\tif (!host) {\n\t\tdev_err(dev, \"%s: scsi_host_alloc failed\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\thost->max_id = CXLFLASH_MAX_NUM_TARGETS_PER_BUS;\n\thost->max_lun = CXLFLASH_MAX_NUM_LUNS_PER_TARGET;\n\thost->unique_id = host->host_no;\n\thost->max_cmd_len = CXLFLASH_MAX_CDB_LEN;\n\n\tcfg = shost_priv(host);\n\tcfg->state = STATE_PROBING;\n\tcfg->host = host;\n\trc = alloc_mem(cfg);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: alloc_mem failed\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tscsi_host_put(cfg->host);\n\t\tgoto out;\n\t}\n\n\tcfg->init_state = INIT_STATE_NONE;\n\tcfg->dev = pdev;\n\tcfg->cxl_fops = cxlflash_cxl_fops;\n\tcfg->ops = cxlflash_assign_ops(ddv);\n\tWARN_ON_ONCE(!cfg->ops);\n\n\t \n\tcfg->promote_lun_index = 0;\n\n\tfor (k = 0; k < MAX_FC_PORTS; k++)\n\t\tcfg->last_lun_index[k] = CXLFLASH_NUM_VLUNS/2 - 1;\n\n\tcfg->dev_id = (struct pci_device_id *)dev_id;\n\n\tinit_waitqueue_head(&cfg->tmf_waitq);\n\tinit_waitqueue_head(&cfg->reset_waitq);\n\n\tINIT_WORK(&cfg->work_q, cxlflash_worker_thread);\n\tcfg->lr_state = LINK_RESET_INVALID;\n\tcfg->lr_port = -1;\n\tspin_lock_init(&cfg->tmf_slock);\n\tmutex_init(&cfg->ctx_tbl_list_mutex);\n\tmutex_init(&cfg->ctx_recovery_mutex);\n\tinit_rwsem(&cfg->ioctl_rwsem);\n\tINIT_LIST_HEAD(&cfg->ctx_err_recovery);\n\tINIT_LIST_HEAD(&cfg->lluns);\n\n\tpci_set_drvdata(pdev, cfg);\n\n\trc = init_pci(cfg);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: init_pci failed rc=%d\\n\", __func__, rc);\n\t\tgoto out_remove;\n\t}\n\tcfg->init_state = INIT_STATE_PCI;\n\n\tcfg->afu_cookie = cfg->ops->create_afu(pdev);\n\tif (unlikely(!cfg->afu_cookie)) {\n\t\tdev_err(dev, \"%s: create_afu failed\\n\", __func__);\n\t\trc = -ENOMEM;\n\t\tgoto out_remove;\n\t}\n\n\trc = init_afu(cfg);\n\tif (rc && !wq_has_sleeper(&cfg->reset_waitq)) {\n\t\tdev_err(dev, \"%s: init_afu failed rc=%d\\n\", __func__, rc);\n\t\tgoto out_remove;\n\t}\n\tcfg->init_state = INIT_STATE_AFU;\n\n\trc = init_scsi(cfg);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: init_scsi failed rc=%d\\n\", __func__, rc);\n\t\tgoto out_remove;\n\t}\n\tcfg->init_state = INIT_STATE_SCSI;\n\n\trc = init_chrdev(cfg);\n\tif (rc) {\n\t\tdev_err(dev, \"%s: init_chrdev failed rc=%d\\n\", __func__, rc);\n\t\tgoto out_remove;\n\t}\n\tcfg->init_state = INIT_STATE_CDEV;\n\n\tif (wq_has_sleeper(&cfg->reset_waitq)) {\n\t\tcfg->state = STATE_PROBED;\n\t\twake_up_all(&cfg->reset_waitq);\n\t} else\n\t\tcfg->state = STATE_NORMAL;\nout:\n\tdev_dbg(dev, \"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\n\nout_remove:\n\tcfg->state = STATE_PROBED;\n\tcxlflash_remove(pdev);\n\tgoto out;\n}\n\n \nstatic pci_ers_result_t cxlflash_pci_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t\t    pci_channel_state_t state)\n{\n\tint rc = 0;\n\tstruct cxlflash_cfg *cfg = pci_get_drvdata(pdev);\n\tstruct device *dev = &cfg->dev->dev;\n\n\tdev_dbg(dev, \"%s: pdev=%p state=%u\\n\", __func__, pdev, state);\n\n\tswitch (state) {\n\tcase pci_channel_io_frozen:\n\t\twait_event(cfg->reset_waitq, cfg->state != STATE_RESET &&\n\t\t\t\t\t     cfg->state != STATE_PROBING);\n\t\tif (cfg->state == STATE_FAILTERM)\n\t\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\t\tcfg->state = STATE_RESET;\n\t\tscsi_block_requests(cfg->host);\n\t\tdrain_ioctls(cfg);\n\t\trc = cxlflash_mark_contexts_error(cfg);\n\t\tif (unlikely(rc))\n\t\t\tdev_err(dev, \"%s: Failed to mark user contexts rc=%d\\n\",\n\t\t\t\t__func__, rc);\n\t\tterm_afu(cfg);\n\t\treturn PCI_ERS_RESULT_NEED_RESET;\n\tcase pci_channel_io_perm_failure:\n\t\tcfg->state = STATE_FAILTERM;\n\t\twake_up_all(&cfg->reset_waitq);\n\t\tscsi_unblock_requests(cfg->host);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t cxlflash_pci_slot_reset(struct pci_dev *pdev)\n{\n\tint rc = 0;\n\tstruct cxlflash_cfg *cfg = pci_get_drvdata(pdev);\n\tstruct device *dev = &cfg->dev->dev;\n\n\tdev_dbg(dev, \"%s: pdev=%p\\n\", __func__, pdev);\n\n\trc = init_afu(cfg);\n\tif (unlikely(rc)) {\n\t\tdev_err(dev, \"%s: EEH recovery failed rc=%d\\n\", __func__, rc);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\n \nstatic void cxlflash_pci_resume(struct pci_dev *pdev)\n{\n\tstruct cxlflash_cfg *cfg = pci_get_drvdata(pdev);\n\tstruct device *dev = &cfg->dev->dev;\n\n\tdev_dbg(dev, \"%s: pdev=%p\\n\", __func__, pdev);\n\n\tcfg->state = STATE_NORMAL;\n\twake_up_all(&cfg->reset_waitq);\n\tscsi_unblock_requests(cfg->host);\n}\n\n \nstatic char *cxlflash_devnode(const struct device *dev, umode_t *mode)\n{\n\treturn kasprintf(GFP_KERNEL, \"cxlflash/%s\", dev_name(dev));\n}\n\n \nstatic int cxlflash_class_init(void)\n{\n\tdev_t devno;\n\tint rc = 0;\n\n\trc = alloc_chrdev_region(&devno, 0, CXLFLASH_MAX_ADAPTERS, \"cxlflash\");\n\tif (unlikely(rc)) {\n\t\tpr_err(\"%s: alloc_chrdev_region failed rc=%d\\n\", __func__, rc);\n\t\tgoto out;\n\t}\n\n\tcxlflash_major = MAJOR(devno);\n\n\tcxlflash_class = class_create(\"cxlflash\");\n\tif (IS_ERR(cxlflash_class)) {\n\t\trc = PTR_ERR(cxlflash_class);\n\t\tpr_err(\"%s: class_create failed rc=%d\\n\", __func__, rc);\n\t\tgoto err;\n\t}\n\n\tcxlflash_class->devnode = cxlflash_devnode;\nout:\n\tpr_debug(\"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\nerr:\n\tunregister_chrdev_region(devno, CXLFLASH_MAX_ADAPTERS);\n\tgoto out;\n}\n\n \nstatic void cxlflash_class_exit(void)\n{\n\tdev_t devno = MKDEV(cxlflash_major, 0);\n\n\tclass_destroy(cxlflash_class);\n\tunregister_chrdev_region(devno, CXLFLASH_MAX_ADAPTERS);\n}\n\nstatic const struct pci_error_handlers cxlflash_err_handler = {\n\t.error_detected = cxlflash_pci_error_detected,\n\t.slot_reset = cxlflash_pci_slot_reset,\n\t.resume = cxlflash_pci_resume,\n};\n\n \nstatic struct pci_driver cxlflash_driver = {\n\t.name = CXLFLASH_NAME,\n\t.id_table = cxlflash_pci_table,\n\t.probe = cxlflash_probe,\n\t.remove = cxlflash_remove,\n\t.shutdown = cxlflash_remove,\n\t.err_handler = &cxlflash_err_handler,\n};\n\n \nstatic int __init init_cxlflash(void)\n{\n\tint rc;\n\n\tcheck_sizes();\n\tcxlflash_list_init();\n\trc = cxlflash_class_init();\n\tif (unlikely(rc))\n\t\tgoto out;\n\n\trc = pci_register_driver(&cxlflash_driver);\n\tif (unlikely(rc))\n\t\tgoto err;\nout:\n\tpr_debug(\"%s: returning rc=%d\\n\", __func__, rc);\n\treturn rc;\nerr:\n\tcxlflash_class_exit();\n\tgoto out;\n}\n\n \nstatic void __exit exit_cxlflash(void)\n{\n\tcxlflash_term_global_luns();\n\tcxlflash_free_errpage();\n\n\tpci_unregister_driver(&cxlflash_driver);\n\tcxlflash_class_exit();\n}\n\nmodule_init(init_cxlflash);\nmodule_exit(exit_cxlflash);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}