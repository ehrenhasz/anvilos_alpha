{
  "module_name": "smartpqi_init.c",
  "hash_id": "899eb4b6685fc11e7da13f4287a49fbb584b3e1d45ae495a5cc319e75fdeba69",
  "original_prompt": "Ingested from linux-6.6.14/drivers/scsi/smartpqi/smartpqi_init.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <linux/rtc.h>\n#include <linux/bcd.h>\n#include <linux/reboot.h>\n#include <linux/cciss_ioctl.h>\n#include <linux/blk-mq-pci.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_transport_sas.h>\n#include <asm/unaligned.h>\n#include \"smartpqi.h\"\n#include \"smartpqi_sis.h\"\n\n#if !defined(BUILD_TIMESTAMP)\n#define BUILD_TIMESTAMP\n#endif\n\n#define DRIVER_VERSION\t\t\"2.1.24-046\"\n#define DRIVER_MAJOR\t\t2\n#define DRIVER_MINOR\t\t1\n#define DRIVER_RELEASE\t\t24\n#define DRIVER_REVISION\t\t46\n\n#define DRIVER_NAME\t\t\"Microchip SmartPQI Driver (v\" \\\n\t\t\t\tDRIVER_VERSION BUILD_TIMESTAMP \")\"\n#define DRIVER_NAME_SHORT\t\"smartpqi\"\n\n#define PQI_EXTRA_SGL_MEMORY\t(12 * sizeof(struct pqi_sg_descriptor))\n\n#define PQI_POST_RESET_DELAY_SECS\t\t\t5\n#define PQI_POST_OFA_RESET_DELAY_UPON_TIMEOUT_SECS\t10\n\n#define PQI_NO_COMPLETION\t((void *)-1)\n\nMODULE_AUTHOR(\"Microchip\");\nMODULE_DESCRIPTION(\"Driver for Microchip Smart Family Controller version \"\n\tDRIVER_VERSION);\nMODULE_VERSION(DRIVER_VERSION);\nMODULE_LICENSE(\"GPL\");\n\nstruct pqi_cmd_priv {\n\tint this_residual;\n};\n\nstatic struct pqi_cmd_priv *pqi_cmd_priv(struct scsi_cmnd *cmd)\n{\n\treturn scsi_cmd_priv(cmd);\n}\n\nstatic void pqi_verify_structures(void);\nstatic void pqi_take_ctrl_offline(struct pqi_ctrl_info *ctrl_info,\n\tenum pqi_ctrl_shutdown_reason ctrl_shutdown_reason);\nstatic void pqi_ctrl_offline_worker(struct work_struct *work);\nstatic int pqi_scan_scsi_devices(struct pqi_ctrl_info *ctrl_info);\nstatic void pqi_scan_start(struct Scsi_Host *shost);\nstatic void pqi_start_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_queue_group *queue_group, enum pqi_io_path path,\n\tstruct pqi_io_request *io_request);\nstatic int pqi_submit_raid_request_synchronous(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_iu_header *request, unsigned int flags,\n\tstruct pqi_raid_error_info *error_info);\nstatic int pqi_aio_submit_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, u32 aio_handle, u8 *cdb,\n\tunsigned int cdb_length, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, bool raid_bypass, bool io_high_prio);\nstatic  int pqi_aio_submit_r1_write_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,\n\tstruct pqi_scsi_dev_raid_map_data *rmd);\nstatic int pqi_aio_submit_r56_write_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,\n\tstruct pqi_scsi_dev_raid_map_data *rmd);\nstatic void pqi_ofa_ctrl_quiesce(struct pqi_ctrl_info *ctrl_info);\nstatic void pqi_ofa_ctrl_unquiesce(struct pqi_ctrl_info *ctrl_info);\nstatic int pqi_ofa_ctrl_restart(struct pqi_ctrl_info *ctrl_info, unsigned int delay_secs);\nstatic void pqi_ofa_setup_host_buffer(struct pqi_ctrl_info *ctrl_info);\nstatic void pqi_ofa_free_host_buffer(struct pqi_ctrl_info *ctrl_info);\nstatic int pqi_ofa_host_memory_update(struct pqi_ctrl_info *ctrl_info);\nstatic int pqi_device_wait_for_pending_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, u8 lun, unsigned long timeout_msecs);\nstatic void pqi_fail_all_outstanding_requests(struct pqi_ctrl_info *ctrl_info);\nstatic void pqi_tmf_worker(struct work_struct *work);\n\n \n#define PQI_SYNC_FLAGS_INTERRUPTABLE\t0x1\n\nstatic struct scsi_transport_template *pqi_sas_transport_template;\n\nstatic atomic_t pqi_controller_count = ATOMIC_INIT(0);\n\nenum pqi_lockup_action {\n\tNONE,\n\tREBOOT,\n\tPANIC\n};\n\nstatic enum pqi_lockup_action pqi_lockup_action = NONE;\n\nstatic struct {\n\tenum pqi_lockup_action\taction;\n\tchar\t\t\t*name;\n} pqi_lockup_actions[] = {\n\t{\n\t\t.action = NONE,\n\t\t.name = \"none\",\n\t},\n\t{\n\t\t.action = REBOOT,\n\t\t.name = \"reboot\",\n\t},\n\t{\n\t\t.action = PANIC,\n\t\t.name = \"panic\",\n\t},\n};\n\nstatic unsigned int pqi_supported_event_types[] = {\n\tPQI_EVENT_TYPE_HOTPLUG,\n\tPQI_EVENT_TYPE_HARDWARE,\n\tPQI_EVENT_TYPE_PHYSICAL_DEVICE,\n\tPQI_EVENT_TYPE_LOGICAL_DEVICE,\n\tPQI_EVENT_TYPE_OFA,\n\tPQI_EVENT_TYPE_AIO_STATE_CHANGE,\n\tPQI_EVENT_TYPE_AIO_CONFIG_CHANGE,\n};\n\nstatic int pqi_disable_device_id_wildcards;\nmodule_param_named(disable_device_id_wildcards,\n\tpqi_disable_device_id_wildcards, int, 0644);\nMODULE_PARM_DESC(disable_device_id_wildcards,\n\t\"Disable device ID wildcards.\");\n\nstatic int pqi_disable_heartbeat;\nmodule_param_named(disable_heartbeat,\n\tpqi_disable_heartbeat, int, 0644);\nMODULE_PARM_DESC(disable_heartbeat,\n\t\"Disable heartbeat.\");\n\nstatic int pqi_disable_ctrl_shutdown;\nmodule_param_named(disable_ctrl_shutdown,\n\tpqi_disable_ctrl_shutdown, int, 0644);\nMODULE_PARM_DESC(disable_ctrl_shutdown,\n\t\"Disable controller shutdown when controller locked up.\");\n\nstatic char *pqi_lockup_action_param;\nmodule_param_named(lockup_action,\n\tpqi_lockup_action_param, charp, 0644);\nMODULE_PARM_DESC(lockup_action, \"Action to take when controller locked up.\\n\"\n\t\"\\t\\tSupported: none, reboot, panic\\n\"\n\t\"\\t\\tDefault: none\");\n\nstatic int pqi_expose_ld_first;\nmodule_param_named(expose_ld_first,\n\tpqi_expose_ld_first, int, 0644);\nMODULE_PARM_DESC(expose_ld_first, \"Expose logical drives before physical drives.\");\n\nstatic int pqi_hide_vsep;\nmodule_param_named(hide_vsep,\n\tpqi_hide_vsep, int, 0644);\nMODULE_PARM_DESC(hide_vsep, \"Hide the virtual SEP for direct attached drives.\");\n\nstatic int pqi_disable_managed_interrupts;\nmodule_param_named(disable_managed_interrupts,\n\tpqi_disable_managed_interrupts, int, 0644);\nMODULE_PARM_DESC(disable_managed_interrupts,\n\t\"Disable the kernel automatically assigning SMP affinity to IRQs.\");\n\nstatic unsigned int pqi_ctrl_ready_timeout_secs;\nmodule_param_named(ctrl_ready_timeout,\n\tpqi_ctrl_ready_timeout_secs, uint, 0644);\nMODULE_PARM_DESC(ctrl_ready_timeout,\n\t\"Timeout in seconds for driver to wait for controller ready.\");\n\nstatic char *raid_levels[] = {\n\t\"RAID-0\",\n\t\"RAID-4\",\n\t\"RAID-1(1+0)\",\n\t\"RAID-5\",\n\t\"RAID-5+1\",\n\t\"RAID-6\",\n\t\"RAID-1(Triple)\",\n};\n\nstatic char *pqi_raid_level_to_string(u8 raid_level)\n{\n\tif (raid_level < ARRAY_SIZE(raid_levels))\n\t\treturn raid_levels[raid_level];\n\n\treturn \"RAID UNKNOWN\";\n}\n\n#define SA_RAID_0\t\t0\n#define SA_RAID_4\t\t1\n#define SA_RAID_1\t\t2\t \n#define SA_RAID_5\t\t3\t \n#define SA_RAID_51\t\t4\n#define SA_RAID_6\t\t5\t \n#define SA_RAID_TRIPLE\t\t6\t \n#define SA_RAID_MAX\t\tSA_RAID_TRIPLE\n#define SA_RAID_UNKNOWN\t\t0xff\n\nstatic inline void pqi_scsi_done(struct scsi_cmnd *scmd)\n{\n\tpqi_prep_for_scsi_done(scmd);\n\tscsi_done(scmd);\n}\n\nstatic inline void pqi_disable_write_same(struct scsi_device *sdev)\n{\n\tsdev->no_write_same = 1;\n}\n\nstatic inline bool pqi_scsi3addr_equal(u8 *scsi3addr1, u8 *scsi3addr2)\n{\n\treturn memcmp(scsi3addr1, scsi3addr2, 8) == 0;\n}\n\nstatic inline bool pqi_is_logical_device(struct pqi_scsi_dev *device)\n{\n\treturn !device->is_physical_device;\n}\n\nstatic inline bool pqi_is_external_raid_addr(u8 *scsi3addr)\n{\n\treturn scsi3addr[2] != 0;\n}\n\nstatic inline bool pqi_ctrl_offline(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn !ctrl_info->controller_online;\n}\n\nstatic inline void pqi_check_ctrl_health(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (ctrl_info->controller_online)\n\t\tif (!sis_is_firmware_running(ctrl_info))\n\t\t\tpqi_take_ctrl_offline(ctrl_info, PQI_FIRMWARE_KERNEL_NOT_UP);\n}\n\nstatic inline bool pqi_is_hba_lunid(u8 *scsi3addr)\n{\n\treturn pqi_scsi3addr_equal(scsi3addr, RAID_CTLR_LUNID);\n}\n\n#define PQI_DRIVER_SCRATCH_PQI_MODE\t\t\t0x1\n#define PQI_DRIVER_SCRATCH_FW_TRIAGE_SUPPORTED\t\t0x2\n\nstatic inline enum pqi_ctrl_mode pqi_get_ctrl_mode(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn sis_read_driver_scratch(ctrl_info) & PQI_DRIVER_SCRATCH_PQI_MODE ? PQI_MODE : SIS_MODE;\n}\n\nstatic inline void pqi_save_ctrl_mode(struct pqi_ctrl_info *ctrl_info,\n\tenum pqi_ctrl_mode mode)\n{\n\tu32 driver_scratch;\n\n\tdriver_scratch = sis_read_driver_scratch(ctrl_info);\n\n\tif (mode == PQI_MODE)\n\t\tdriver_scratch |= PQI_DRIVER_SCRATCH_PQI_MODE;\n\telse\n\t\tdriver_scratch &= ~PQI_DRIVER_SCRATCH_PQI_MODE;\n\n\tsis_write_driver_scratch(ctrl_info, driver_scratch);\n}\n\nstatic inline bool pqi_is_fw_triage_supported(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn (sis_read_driver_scratch(ctrl_info) & PQI_DRIVER_SCRATCH_FW_TRIAGE_SUPPORTED) != 0;\n}\n\nstatic inline void pqi_save_fw_triage_setting(struct pqi_ctrl_info *ctrl_info, bool is_supported)\n{\n\tu32 driver_scratch;\n\n\tdriver_scratch = sis_read_driver_scratch(ctrl_info);\n\n\tif (is_supported)\n\t\tdriver_scratch |= PQI_DRIVER_SCRATCH_FW_TRIAGE_SUPPORTED;\n\telse\n\t\tdriver_scratch &= ~PQI_DRIVER_SCRATCH_FW_TRIAGE_SUPPORTED;\n\n\tsis_write_driver_scratch(ctrl_info, driver_scratch);\n}\n\nstatic inline void pqi_ctrl_block_scan(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->scan_blocked = true;\n\tmutex_lock(&ctrl_info->scan_mutex);\n}\n\nstatic inline void pqi_ctrl_unblock_scan(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->scan_blocked = false;\n\tmutex_unlock(&ctrl_info->scan_mutex);\n}\n\nstatic inline bool pqi_ctrl_scan_blocked(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn ctrl_info->scan_blocked;\n}\n\nstatic inline void pqi_ctrl_block_device_reset(struct pqi_ctrl_info *ctrl_info)\n{\n\tmutex_lock(&ctrl_info->lun_reset_mutex);\n}\n\nstatic inline void pqi_ctrl_unblock_device_reset(struct pqi_ctrl_info *ctrl_info)\n{\n\tmutex_unlock(&ctrl_info->lun_reset_mutex);\n}\n\nstatic inline void pqi_scsi_block_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct Scsi_Host *shost;\n\tunsigned int num_loops;\n\tint msecs_sleep;\n\n\tshost = ctrl_info->scsi_host;\n\n\tscsi_block_requests(shost);\n\n\tnum_loops = 0;\n\tmsecs_sleep = 20;\n\twhile (scsi_host_busy(shost)) {\n\t\tnum_loops++;\n\t\tif (num_loops == 10)\n\t\t\tmsecs_sleep = 500;\n\t\tmsleep(msecs_sleep);\n\t}\n}\n\nstatic inline void pqi_scsi_unblock_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tscsi_unblock_requests(ctrl_info->scsi_host);\n}\n\nstatic inline void pqi_ctrl_busy(struct pqi_ctrl_info *ctrl_info)\n{\n\tatomic_inc(&ctrl_info->num_busy_threads);\n}\n\nstatic inline void pqi_ctrl_unbusy(struct pqi_ctrl_info *ctrl_info)\n{\n\tatomic_dec(&ctrl_info->num_busy_threads);\n}\n\nstatic inline bool pqi_ctrl_blocked(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn ctrl_info->block_requests;\n}\n\nstatic inline void pqi_ctrl_block_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->block_requests = true;\n}\n\nstatic inline void pqi_ctrl_unblock_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->block_requests = false;\n\twake_up_all(&ctrl_info->block_requests_wait);\n}\n\nstatic void pqi_wait_if_ctrl_blocked(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (!pqi_ctrl_blocked(ctrl_info))\n\t\treturn;\n\n\tatomic_inc(&ctrl_info->num_blocked_threads);\n\twait_event(ctrl_info->block_requests_wait,\n\t\t!pqi_ctrl_blocked(ctrl_info));\n\tatomic_dec(&ctrl_info->num_blocked_threads);\n}\n\n#define PQI_QUIESCE_WARNING_TIMEOUT_SECS\t\t10\n\nstatic inline void pqi_ctrl_wait_until_quiesced(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned long start_jiffies;\n\tunsigned long warning_timeout;\n\tbool displayed_warning;\n\n\tdisplayed_warning = false;\n\tstart_jiffies = jiffies;\n\twarning_timeout = (PQI_QUIESCE_WARNING_TIMEOUT_SECS * HZ) + start_jiffies;\n\n\twhile (atomic_read(&ctrl_info->num_busy_threads) >\n\t\tatomic_read(&ctrl_info->num_blocked_threads)) {\n\t\tif (time_after(jiffies, warning_timeout)) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"waiting %u seconds for driver activity to quiesce\\n\",\n\t\t\t\tjiffies_to_msecs(jiffies - start_jiffies) / 1000);\n\t\t\tdisplayed_warning = true;\n\t\t\twarning_timeout = (PQI_QUIESCE_WARNING_TIMEOUT_SECS * HZ) + jiffies;\n\t\t}\n\t\tusleep_range(1000, 2000);\n\t}\n\n\tif (displayed_warning)\n\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\"driver activity quiesced after waiting for %u seconds\\n\",\n\t\t\tjiffies_to_msecs(jiffies - start_jiffies) / 1000);\n}\n\nstatic inline bool pqi_device_offline(struct pqi_scsi_dev *device)\n{\n\treturn device->device_offline;\n}\n\nstatic inline void pqi_ctrl_ofa_start(struct pqi_ctrl_info *ctrl_info)\n{\n\tmutex_lock(&ctrl_info->ofa_mutex);\n}\n\nstatic inline void pqi_ctrl_ofa_done(struct pqi_ctrl_info *ctrl_info)\n{\n\tmutex_unlock(&ctrl_info->ofa_mutex);\n}\n\nstatic inline void pqi_wait_until_ofa_finished(struct pqi_ctrl_info *ctrl_info)\n{\n\tmutex_lock(&ctrl_info->ofa_mutex);\n\tmutex_unlock(&ctrl_info->ofa_mutex);\n}\n\nstatic inline bool pqi_ofa_in_progress(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn mutex_is_locked(&ctrl_info->ofa_mutex);\n}\n\nstatic inline void pqi_device_remove_start(struct pqi_scsi_dev *device)\n{\n\tdevice->in_remove = true;\n}\n\nstatic inline bool pqi_device_in_remove(struct pqi_scsi_dev *device)\n{\n\treturn device->in_remove;\n}\n\nstatic inline void pqi_device_reset_start(struct pqi_scsi_dev *device, u8 lun)\n{\n\tdevice->in_reset[lun] = true;\n}\n\nstatic inline void pqi_device_reset_done(struct pqi_scsi_dev *device, u8 lun)\n{\n\tdevice->in_reset[lun] = false;\n}\n\nstatic inline bool pqi_device_in_reset(struct pqi_scsi_dev *device, u8 lun)\n{\n\treturn device->in_reset[lun];\n}\n\nstatic inline int pqi_event_type_to_event_index(unsigned int event_type)\n{\n\tint index;\n\n\tfor (index = 0; index < ARRAY_SIZE(pqi_supported_event_types); index++)\n\t\tif (event_type == pqi_supported_event_types[index])\n\t\t\treturn index;\n\n\treturn -1;\n}\n\nstatic inline bool pqi_is_supported_event(unsigned int event_type)\n{\n\treturn pqi_event_type_to_event_index(event_type) != -1;\n}\n\nstatic inline void pqi_schedule_rescan_worker_with_delay(struct pqi_ctrl_info *ctrl_info,\n\tunsigned long delay)\n{\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn;\n\n\tschedule_delayed_work(&ctrl_info->rescan_work, delay);\n}\n\nstatic inline void pqi_schedule_rescan_worker(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_schedule_rescan_worker_with_delay(ctrl_info, 0);\n}\n\n#define PQI_RESCAN_WORK_DELAY\t(10 * HZ)\n\nstatic inline void pqi_schedule_rescan_worker_delayed(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_schedule_rescan_worker_with_delay(ctrl_info, PQI_RESCAN_WORK_DELAY);\n}\n\nstatic inline void pqi_cancel_rescan_worker(struct pqi_ctrl_info *ctrl_info)\n{\n\tcancel_delayed_work_sync(&ctrl_info->rescan_work);\n}\n\nstatic inline u32 pqi_read_heartbeat_counter(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (!ctrl_info->heartbeat_counter)\n\t\treturn 0;\n\n\treturn readl(ctrl_info->heartbeat_counter);\n}\n\nstatic inline u8 pqi_read_soft_reset_status(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn readb(ctrl_info->soft_reset_status);\n}\n\nstatic inline void pqi_clear_soft_reset_status(struct pqi_ctrl_info *ctrl_info)\n{\n\tu8 status;\n\n\tstatus = pqi_read_soft_reset_status(ctrl_info);\n\tstatus &= ~PQI_SOFT_RESET_ABORT;\n\twriteb(status, ctrl_info->soft_reset_status);\n}\n\nstatic inline bool pqi_is_io_high_priority(struct pqi_scsi_dev *device, struct scsi_cmnd *scmd)\n{\n\tbool io_high_prio;\n\tint priority_class;\n\n\tio_high_prio = false;\n\n\tif (device->ncq_prio_enable) {\n\t\tpriority_class =\n\t\t\tIOPRIO_PRIO_CLASS(req_get_ioprio(scsi_cmd_to_rq(scmd)));\n\t\tif (priority_class == IOPRIO_CLASS_RT) {\n\t\t\t \n\t\t\tswitch (scmd->cmnd[0]) {\n\t\t\tcase WRITE_16:\n\t\t\tcase READ_16:\n\t\t\tcase WRITE_12:\n\t\t\tcase READ_12:\n\t\t\tcase WRITE_10:\n\t\t\tcase READ_10:\n\t\t\tcase WRITE_6:\n\t\t\tcase READ_6:\n\t\t\t\tio_high_prio = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn io_high_prio;\n}\n\nstatic int pqi_map_single(struct pci_dev *pci_dev,\n\tstruct pqi_sg_descriptor *sg_descriptor, void *buffer,\n\tsize_t buffer_length, enum dma_data_direction data_direction)\n{\n\tdma_addr_t bus_address;\n\n\tif (!buffer || buffer_length == 0 || data_direction == DMA_NONE)\n\t\treturn 0;\n\n\tbus_address = dma_map_single(&pci_dev->dev, buffer, buffer_length,\n\t\tdata_direction);\n\tif (dma_mapping_error(&pci_dev->dev, bus_address))\n\t\treturn -ENOMEM;\n\n\tput_unaligned_le64((u64)bus_address, &sg_descriptor->address);\n\tput_unaligned_le32(buffer_length, &sg_descriptor->length);\n\tput_unaligned_le32(CISS_SG_LAST, &sg_descriptor->flags);\n\n\treturn 0;\n}\n\nstatic void pqi_pci_unmap(struct pci_dev *pci_dev,\n\tstruct pqi_sg_descriptor *descriptors, int num_descriptors,\n\tenum dma_data_direction data_direction)\n{\n\tint i;\n\n\tif (data_direction == DMA_NONE)\n\t\treturn;\n\n\tfor (i = 0; i < num_descriptors; i++)\n\t\tdma_unmap_single(&pci_dev->dev,\n\t\t\t(dma_addr_t)get_unaligned_le64(&descriptors[i].address),\n\t\t\tget_unaligned_le32(&descriptors[i].length),\n\t\t\tdata_direction);\n}\n\nstatic int pqi_build_raid_path_request(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_raid_path_request *request, u8 cmd,\n\tu8 *scsi3addr, void *buffer, size_t buffer_length,\n\tu16 vpd_page, enum dma_data_direction *dir)\n{\n\tu8 *cdb;\n\tsize_t cdb_length = buffer_length;\n\n\tmemset(request, 0, sizeof(*request));\n\n\trequest->header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\n\tput_unaligned_le16(offsetof(struct pqi_raid_path_request,\n\t\tsg_descriptors[1]) - PQI_REQUEST_HEADER_LENGTH,\n\t\t&request->header.iu_length);\n\tput_unaligned_le32(buffer_length, &request->buffer_length);\n\tmemcpy(request->lun_number, scsi3addr, sizeof(request->lun_number));\n\trequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_0;\n\n\tcdb = request->cdb;\n\n\tswitch (cmd) {\n\tcase INQUIRY:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tcdb[0] = INQUIRY;\n\t\tif (vpd_page & VPD_PAGE) {\n\t\t\tcdb[1] = 0x1;\n\t\t\tcdb[2] = (u8)vpd_page;\n\t\t}\n\t\tcdb[4] = (u8)cdb_length;\n\t\tbreak;\n\tcase CISS_REPORT_LOG:\n\tcase CISS_REPORT_PHYS:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tcdb[0] = cmd;\n\t\tif (cmd == CISS_REPORT_PHYS) {\n\t\t\tif (ctrl_info->rpl_extended_format_4_5_supported)\n\t\t\t\tcdb[1] = CISS_REPORT_PHYS_FLAG_EXTENDED_FORMAT_4;\n\t\t\telse\n\t\t\t\tcdb[1] = CISS_REPORT_PHYS_FLAG_EXTENDED_FORMAT_2;\n\t\t} else {\n\t\t\tcdb[1] = ctrl_info->ciss_report_log_flags;\n\t\t}\n\t\tput_unaligned_be32(cdb_length, &cdb[6]);\n\t\tbreak;\n\tcase CISS_GET_RAID_MAP:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tcdb[0] = CISS_READ;\n\t\tcdb[1] = CISS_GET_RAID_MAP;\n\t\tput_unaligned_be32(cdb_length, &cdb[6]);\n\t\tbreak;\n\tcase SA_FLUSH_CACHE:\n\t\trequest->header.driver_flags = PQI_DRIVER_NONBLOCKABLE_REQUEST;\n\t\trequest->data_direction = SOP_WRITE_FLAG;\n\t\tcdb[0] = BMIC_WRITE;\n\t\tcdb[6] = BMIC_FLUSH_CACHE;\n\t\tput_unaligned_be16(cdb_length, &cdb[7]);\n\t\tbreak;\n\tcase BMIC_SENSE_DIAG_OPTIONS:\n\t\tcdb_length = 0;\n\t\tfallthrough;\n\tcase BMIC_IDENTIFY_CONTROLLER:\n\tcase BMIC_IDENTIFY_PHYSICAL_DEVICE:\n\tcase BMIC_SENSE_SUBSYSTEM_INFORMATION:\n\tcase BMIC_SENSE_FEATURE:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tcdb[0] = BMIC_READ;\n\t\tcdb[6] = cmd;\n\t\tput_unaligned_be16(cdb_length, &cdb[7]);\n\t\tbreak;\n\tcase BMIC_SET_DIAG_OPTIONS:\n\t\tcdb_length = 0;\n\t\tfallthrough;\n\tcase BMIC_WRITE_HOST_WELLNESS:\n\t\trequest->data_direction = SOP_WRITE_FLAG;\n\t\tcdb[0] = BMIC_WRITE;\n\t\tcdb[6] = cmd;\n\t\tput_unaligned_be16(cdb_length, &cdb[7]);\n\t\tbreak;\n\tcase BMIC_CSMI_PASSTHRU:\n\t\trequest->data_direction = SOP_BIDIRECTIONAL;\n\t\tcdb[0] = BMIC_WRITE;\n\t\tcdb[5] = CSMI_CC_SAS_SMP_PASSTHRU;\n\t\tcdb[6] = cmd;\n\t\tput_unaligned_be16(cdb_length, &cdb[7]);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ctrl_info->pci_dev->dev, \"unknown command 0x%c\\n\", cmd);\n\t\tbreak;\n\t}\n\n\tswitch (request->data_direction) {\n\tcase SOP_READ_FLAG:\n\t\t*dir = DMA_FROM_DEVICE;\n\t\tbreak;\n\tcase SOP_WRITE_FLAG:\n\t\t*dir = DMA_TO_DEVICE;\n\t\tbreak;\n\tcase SOP_NO_DIRECTION_FLAG:\n\t\t*dir = DMA_NONE;\n\t\tbreak;\n\tdefault:\n\t\t*dir = DMA_BIDIRECTIONAL;\n\t\tbreak;\n\t}\n\n\treturn pqi_map_single(ctrl_info->pci_dev, &request->sg_descriptors[0],\n\t\tbuffer, buffer_length, *dir);\n}\n\nstatic inline void pqi_reinit_io_request(struct pqi_io_request *io_request)\n{\n\tio_request->scmd = NULL;\n\tio_request->status = 0;\n\tio_request->error_info = NULL;\n\tio_request->raid_bypass = false;\n}\n\nstatic inline struct pqi_io_request *pqi_alloc_io_request(struct pqi_ctrl_info *ctrl_info, struct scsi_cmnd *scmd)\n{\n\tstruct pqi_io_request *io_request;\n\tu16 i;\n\n\tif (scmd) {  \n\t\tu32 blk_tag = blk_mq_unique_tag(scsi_cmd_to_rq(scmd));\n\n\t\ti = blk_mq_unique_tag_to_tag(blk_tag);\n\t\tio_request = &ctrl_info->io_request_pool[i];\n\t\tif (atomic_inc_return(&io_request->refcount) > 1) {\n\t\t\tatomic_dec(&io_request->refcount);\n\t\t\treturn NULL;\n\t\t}\n\t} else {  \n\t\t \n\t\ti = 0;\n\t\twhile (1) {\n\t\t\tio_request = &ctrl_info->io_request_pool[ctrl_info->scsi_ml_can_queue + i];\n\t\t\tif (atomic_inc_return(&io_request->refcount) == 1)\n\t\t\t\tbreak;\n\t\t\tatomic_dec(&io_request->refcount);\n\t\t\ti = (i + 1) % PQI_RESERVED_IO_SLOTS;\n\t\t}\n\t}\n\n\tif (io_request)\n\t\tpqi_reinit_io_request(io_request);\n\n\treturn io_request;\n}\n\nstatic void pqi_free_io_request(struct pqi_io_request *io_request)\n{\n\tatomic_dec(&io_request->refcount);\n}\n\nstatic int pqi_send_scsi_raid_request(struct pqi_ctrl_info *ctrl_info, u8 cmd,\n\tu8 *scsi3addr, void *buffer, size_t buffer_length, u16 vpd_page,\n\tstruct pqi_raid_error_info *error_info)\n{\n\tint rc;\n\tstruct pqi_raid_path_request request;\n\tenum dma_data_direction dir;\n\n\trc = pqi_build_raid_path_request(ctrl_info, &request, cmd, scsi3addr,\n\t\tbuffer, buffer_length, vpd_page, &dir);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, error_info);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1, dir);\n\n\treturn rc;\n}\n\n \n\nstatic inline int pqi_send_ctrl_raid_request(struct pqi_ctrl_info *ctrl_info,\n\tu8 cmd, void *buffer, size_t buffer_length)\n{\n\treturn pqi_send_scsi_raid_request(ctrl_info, cmd, RAID_CTLR_LUNID,\n\t\tbuffer, buffer_length, 0, NULL);\n}\n\nstatic inline int pqi_send_ctrl_raid_with_error(struct pqi_ctrl_info *ctrl_info,\n\tu8 cmd, void *buffer, size_t buffer_length,\n\tstruct pqi_raid_error_info *error_info)\n{\n\treturn pqi_send_scsi_raid_request(ctrl_info, cmd, RAID_CTLR_LUNID,\n\t\tbuffer, buffer_length, 0, error_info);\n}\n\nstatic inline int pqi_identify_controller(struct pqi_ctrl_info *ctrl_info,\n\tstruct bmic_identify_controller *buffer)\n{\n\treturn pqi_send_ctrl_raid_request(ctrl_info, BMIC_IDENTIFY_CONTROLLER,\n\t\tbuffer, sizeof(*buffer));\n}\n\nstatic inline int pqi_sense_subsystem_info(struct  pqi_ctrl_info *ctrl_info,\n\tstruct bmic_sense_subsystem_info *sense_info)\n{\n\treturn pqi_send_ctrl_raid_request(ctrl_info,\n\t\tBMIC_SENSE_SUBSYSTEM_INFORMATION, sense_info,\n\t\tsizeof(*sense_info));\n}\n\nstatic inline int pqi_scsi_inquiry(struct pqi_ctrl_info *ctrl_info,\n\tu8 *scsi3addr, u16 vpd_page, void *buffer, size_t buffer_length)\n{\n\treturn pqi_send_scsi_raid_request(ctrl_info, INQUIRY, scsi3addr,\n\t\tbuffer, buffer_length, vpd_page, NULL);\n}\n\nstatic int pqi_identify_physical_device(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device,\n\tstruct bmic_identify_physical_device *buffer, size_t buffer_length)\n{\n\tint rc;\n\tenum dma_data_direction dir;\n\tu16 bmic_device_index;\n\tstruct pqi_raid_path_request request;\n\n\trc = pqi_build_raid_path_request(ctrl_info, &request,\n\t\tBMIC_IDENTIFY_PHYSICAL_DEVICE, RAID_CTLR_LUNID, buffer,\n\t\tbuffer_length, 0, &dir);\n\tif (rc)\n\t\treturn rc;\n\n\tbmic_device_index = CISS_GET_DRIVE_NUMBER(device->scsi3addr);\n\trequest.cdb[2] = (u8)bmic_device_index;\n\trequest.cdb[9] = (u8)(bmic_device_index >> 8);\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1, dir);\n\n\treturn rc;\n}\n\nstatic inline u32 pqi_aio_limit_to_bytes(__le16 *limit)\n{\n\tu32 bytes;\n\n\tbytes = get_unaligned_le16(limit);\n\tif (bytes == 0)\n\t\tbytes = ~0;\n\telse\n\t\tbytes *= 1024;\n\n\treturn bytes;\n}\n\n#pragma pack(1)\n\nstruct bmic_sense_feature_buffer {\n\tstruct bmic_sense_feature_buffer_header header;\n\tstruct bmic_sense_feature_io_page_aio_subpage aio_subpage;\n};\n\n#pragma pack()\n\n#define MINIMUM_AIO_SUBPAGE_BUFFER_LENGTH\t\\\n\toffsetofend(struct bmic_sense_feature_buffer, \\\n\t\taio_subpage.max_write_raid_1_10_3drive)\n\n#define MINIMUM_AIO_SUBPAGE_LENGTH\t\\\n\t(offsetofend(struct bmic_sense_feature_io_page_aio_subpage, \\\n\t\tmax_write_raid_1_10_3drive) - \\\n\t\tsizeof_field(struct bmic_sense_feature_io_page_aio_subpage, header))\n\nstatic int pqi_get_advanced_raid_bypass_config(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tenum dma_data_direction dir;\n\tstruct pqi_raid_path_request request;\n\tstruct bmic_sense_feature_buffer *buffer;\n\n\tbuffer = kmalloc(sizeof(*buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\trc = pqi_build_raid_path_request(ctrl_info, &request, BMIC_SENSE_FEATURE, RAID_CTLR_LUNID,\n\t\tbuffer, sizeof(*buffer), 0, &dir);\n\tif (rc)\n\t\tgoto error;\n\n\trequest.cdb[2] = BMIC_SENSE_FEATURE_IO_PAGE;\n\trequest.cdb[3] = BMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE;\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1, dir);\n\n\tif (rc)\n\t\tgoto error;\n\n\tif (buffer->header.page_code != BMIC_SENSE_FEATURE_IO_PAGE ||\n\t\tbuffer->header.subpage_code !=\n\t\t\tBMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE ||\n\t\tget_unaligned_le16(&buffer->header.buffer_length) <\n\t\t\tMINIMUM_AIO_SUBPAGE_BUFFER_LENGTH ||\n\t\tbuffer->aio_subpage.header.page_code !=\n\t\t\tBMIC_SENSE_FEATURE_IO_PAGE ||\n\t\tbuffer->aio_subpage.header.subpage_code !=\n\t\t\tBMIC_SENSE_FEATURE_IO_PAGE_AIO_SUBPAGE ||\n\t\tget_unaligned_le16(&buffer->aio_subpage.header.page_length) <\n\t\t\tMINIMUM_AIO_SUBPAGE_LENGTH) {\n\t\tgoto error;\n\t}\n\n\tctrl_info->max_transfer_encrypted_sas_sata =\n\t\tpqi_aio_limit_to_bytes(\n\t\t\t&buffer->aio_subpage.max_transfer_encrypted_sas_sata);\n\n\tctrl_info->max_transfer_encrypted_nvme =\n\t\tpqi_aio_limit_to_bytes(\n\t\t\t&buffer->aio_subpage.max_transfer_encrypted_nvme);\n\n\tctrl_info->max_write_raid_5_6 =\n\t\tpqi_aio_limit_to_bytes(\n\t\t\t&buffer->aio_subpage.max_write_raid_5_6);\n\n\tctrl_info->max_write_raid_1_10_2drive =\n\t\tpqi_aio_limit_to_bytes(\n\t\t\t&buffer->aio_subpage.max_write_raid_1_10_2drive);\n\n\tctrl_info->max_write_raid_1_10_3drive =\n\t\tpqi_aio_limit_to_bytes(\n\t\t\t&buffer->aio_subpage.max_write_raid_1_10_3drive);\n\nerror:\n\tkfree(buffer);\n\n\treturn rc;\n}\n\nstatic int pqi_flush_cache(struct pqi_ctrl_info *ctrl_info,\n\tenum bmic_flush_cache_shutdown_event shutdown_event)\n{\n\tint rc;\n\tstruct bmic_flush_cache *flush_cache;\n\n\tflush_cache = kzalloc(sizeof(*flush_cache), GFP_KERNEL);\n\tif (!flush_cache)\n\t\treturn -ENOMEM;\n\n\tflush_cache->shutdown_event = shutdown_event;\n\n\trc = pqi_send_ctrl_raid_request(ctrl_info, SA_FLUSH_CACHE, flush_cache,\n\t\tsizeof(*flush_cache));\n\n\tkfree(flush_cache);\n\n\treturn rc;\n}\n\nint pqi_csmi_smp_passthru(struct pqi_ctrl_info *ctrl_info,\n\tstruct bmic_csmi_smp_passthru_buffer *buffer, size_t buffer_length,\n\tstruct pqi_raid_error_info *error_info)\n{\n\treturn pqi_send_ctrl_raid_with_error(ctrl_info, BMIC_CSMI_PASSTHRU,\n\t\tbuffer, buffer_length, error_info);\n}\n\n#define PQI_FETCH_PTRAID_DATA\t\t(1 << 31)\n\nstatic int pqi_set_diag_rescan(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct bmic_diag_options *diag;\n\n\tdiag = kzalloc(sizeof(*diag), GFP_KERNEL);\n\tif (!diag)\n\t\treturn -ENOMEM;\n\n\trc = pqi_send_ctrl_raid_request(ctrl_info, BMIC_SENSE_DIAG_OPTIONS,\n\t\tdiag, sizeof(*diag));\n\tif (rc)\n\t\tgoto out;\n\n\tdiag->options |= cpu_to_le32(PQI_FETCH_PTRAID_DATA);\n\n\trc = pqi_send_ctrl_raid_request(ctrl_info, BMIC_SET_DIAG_OPTIONS, diag,\n\t\tsizeof(*diag));\n\nout:\n\tkfree(diag);\n\n\treturn rc;\n}\n\nstatic inline int pqi_write_host_wellness(struct pqi_ctrl_info *ctrl_info,\n\tvoid *buffer, size_t buffer_length)\n{\n\treturn pqi_send_ctrl_raid_request(ctrl_info, BMIC_WRITE_HOST_WELLNESS,\n\t\tbuffer, buffer_length);\n}\n\n#pragma pack(1)\n\nstruct bmic_host_wellness_driver_version {\n\tu8\tstart_tag[4];\n\tu8\tdriver_version_tag[2];\n\t__le16\tdriver_version_length;\n\tchar\tdriver_version[32];\n\tu8\tdont_write_tag[2];\n\tu8\tend_tag[2];\n};\n\n#pragma pack()\n\nstatic int pqi_write_driver_version_to_host_wellness(\n\tstruct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct bmic_host_wellness_driver_version *buffer;\n\tsize_t buffer_length;\n\n\tbuffer_length = sizeof(*buffer);\n\n\tbuffer = kmalloc(buffer_length, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\tbuffer->start_tag[0] = '<';\n\tbuffer->start_tag[1] = 'H';\n\tbuffer->start_tag[2] = 'W';\n\tbuffer->start_tag[3] = '>';\n\tbuffer->driver_version_tag[0] = 'D';\n\tbuffer->driver_version_tag[1] = 'V';\n\tput_unaligned_le16(sizeof(buffer->driver_version),\n\t\t&buffer->driver_version_length);\n\tstrncpy(buffer->driver_version, \"Linux \" DRIVER_VERSION,\n\t\tsizeof(buffer->driver_version) - 1);\n\tbuffer->driver_version[sizeof(buffer->driver_version) - 1] = '\\0';\n\tbuffer->dont_write_tag[0] = 'D';\n\tbuffer->dont_write_tag[1] = 'W';\n\tbuffer->end_tag[0] = 'Z';\n\tbuffer->end_tag[1] = 'Z';\n\n\trc = pqi_write_host_wellness(ctrl_info, buffer, buffer_length);\n\n\tkfree(buffer);\n\n\treturn rc;\n}\n\n#pragma pack(1)\n\nstruct bmic_host_wellness_time {\n\tu8\tstart_tag[4];\n\tu8\ttime_tag[2];\n\t__le16\ttime_length;\n\tu8\ttime[8];\n\tu8\tdont_write_tag[2];\n\tu8\tend_tag[2];\n};\n\n#pragma pack()\n\nstatic int pqi_write_current_time_to_host_wellness(\n\tstruct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct bmic_host_wellness_time *buffer;\n\tsize_t buffer_length;\n\ttime64_t local_time;\n\tunsigned int year;\n\tstruct tm tm;\n\n\tbuffer_length = sizeof(*buffer);\n\n\tbuffer = kmalloc(buffer_length, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\tbuffer->start_tag[0] = '<';\n\tbuffer->start_tag[1] = 'H';\n\tbuffer->start_tag[2] = 'W';\n\tbuffer->start_tag[3] = '>';\n\tbuffer->time_tag[0] = 'T';\n\tbuffer->time_tag[1] = 'D';\n\tput_unaligned_le16(sizeof(buffer->time),\n\t\t&buffer->time_length);\n\n\tlocal_time = ktime_get_real_seconds();\n\ttime64_to_tm(local_time, -sys_tz.tz_minuteswest * 60, &tm);\n\tyear = tm.tm_year + 1900;\n\n\tbuffer->time[0] = bin2bcd(tm.tm_hour);\n\tbuffer->time[1] = bin2bcd(tm.tm_min);\n\tbuffer->time[2] = bin2bcd(tm.tm_sec);\n\tbuffer->time[3] = 0;\n\tbuffer->time[4] = bin2bcd(tm.tm_mon + 1);\n\tbuffer->time[5] = bin2bcd(tm.tm_mday);\n\tbuffer->time[6] = bin2bcd(year / 100);\n\tbuffer->time[7] = bin2bcd(year % 100);\n\n\tbuffer->dont_write_tag[0] = 'D';\n\tbuffer->dont_write_tag[1] = 'W';\n\tbuffer->end_tag[0] = 'Z';\n\tbuffer->end_tag[1] = 'Z';\n\n\trc = pqi_write_host_wellness(ctrl_info, buffer, buffer_length);\n\n\tkfree(buffer);\n\n\treturn rc;\n}\n\n#define PQI_UPDATE_TIME_WORK_INTERVAL\t(24UL * 60 * 60 * HZ)\n\nstatic void pqi_update_time_worker(struct work_struct *work)\n{\n\tint rc;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = container_of(to_delayed_work(work), struct pqi_ctrl_info,\n\t\tupdate_time_work);\n\n\trc = pqi_write_current_time_to_host_wellness(ctrl_info);\n\tif (rc)\n\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\"error updating time on controller\\n\");\n\n\tschedule_delayed_work(&ctrl_info->update_time_work,\n\t\tPQI_UPDATE_TIME_WORK_INTERVAL);\n}\n\nstatic inline void pqi_schedule_update_time_worker(struct pqi_ctrl_info *ctrl_info)\n{\n\tschedule_delayed_work(&ctrl_info->update_time_work, 0);\n}\n\nstatic inline void pqi_cancel_update_time_worker(struct pqi_ctrl_info *ctrl_info)\n{\n\tcancel_delayed_work_sync(&ctrl_info->update_time_work);\n}\n\nstatic inline int pqi_report_luns(struct pqi_ctrl_info *ctrl_info, u8 cmd, void *buffer,\n\tsize_t buffer_length)\n{\n\treturn pqi_send_ctrl_raid_request(ctrl_info, cmd, buffer, buffer_length);\n}\n\nstatic int pqi_report_phys_logical_luns(struct pqi_ctrl_info *ctrl_info, u8 cmd, void **buffer)\n{\n\tint rc;\n\tsize_t lun_list_length;\n\tsize_t lun_data_length;\n\tsize_t new_lun_list_length;\n\tvoid *lun_data = NULL;\n\tstruct report_lun_header *report_lun_header;\n\n\treport_lun_header = kmalloc(sizeof(*report_lun_header), GFP_KERNEL);\n\tif (!report_lun_header) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = pqi_report_luns(ctrl_info, cmd, report_lun_header, sizeof(*report_lun_header));\n\tif (rc)\n\t\tgoto out;\n\n\tlun_list_length = get_unaligned_be32(&report_lun_header->list_length);\n\nagain:\n\tlun_data_length = sizeof(struct report_lun_header) + lun_list_length;\n\n\tlun_data = kmalloc(lun_data_length, GFP_KERNEL);\n\tif (!lun_data) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (lun_list_length == 0) {\n\t\tmemcpy(lun_data, report_lun_header, sizeof(*report_lun_header));\n\t\tgoto out;\n\t}\n\n\trc = pqi_report_luns(ctrl_info, cmd, lun_data, lun_data_length);\n\tif (rc)\n\t\tgoto out;\n\n\tnew_lun_list_length =\n\t\tget_unaligned_be32(&((struct report_lun_header *)lun_data)->list_length);\n\n\tif (new_lun_list_length > lun_list_length) {\n\t\tlun_list_length = new_lun_list_length;\n\t\tkfree(lun_data);\n\t\tgoto again;\n\t}\n\nout:\n\tkfree(report_lun_header);\n\n\tif (rc) {\n\t\tkfree(lun_data);\n\t\tlun_data = NULL;\n\t}\n\n\t*buffer = lun_data;\n\n\treturn rc;\n}\n\nstatic inline int pqi_report_phys_luns(struct pqi_ctrl_info *ctrl_info, void **buffer)\n{\n\tint rc;\n\tunsigned int i;\n\tu8 rpl_response_format;\n\tu32 num_physicals;\n\tvoid *rpl_list;\n\tstruct report_lun_header *rpl_header;\n\tstruct report_phys_lun_8byte_wwid_list *rpl_8byte_wwid_list;\n\tstruct report_phys_lun_16byte_wwid_list *rpl_16byte_wwid_list;\n\n\trc = pqi_report_phys_logical_luns(ctrl_info, CISS_REPORT_PHYS, &rpl_list);\n\tif (rc)\n\t\treturn rc;\n\n\tif (ctrl_info->rpl_extended_format_4_5_supported) {\n\t\trpl_header = rpl_list;\n\t\trpl_response_format = rpl_header->flags & CISS_REPORT_PHYS_FLAG_EXTENDED_FORMAT_MASK;\n\t\tif (rpl_response_format == CISS_REPORT_PHYS_FLAG_EXTENDED_FORMAT_4) {\n\t\t\t*buffer = rpl_list;\n\t\t\treturn 0;\n\t\t} else if (rpl_response_format != CISS_REPORT_PHYS_FLAG_EXTENDED_FORMAT_2) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"RPL returned unsupported data format %u\\n\",\n\t\t\t\trpl_response_format);\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"RPL returned extended format 2 instead of 4\\n\");\n\t\t}\n\t}\n\n\trpl_8byte_wwid_list = rpl_list;\n\tnum_physicals = get_unaligned_be32(&rpl_8byte_wwid_list->header.list_length) / sizeof(rpl_8byte_wwid_list->lun_entries[0]);\n\n\trpl_16byte_wwid_list = kmalloc(struct_size(rpl_16byte_wwid_list, lun_entries,\n\t\t\t\t\t\t   num_physicals), GFP_KERNEL);\n\tif (!rpl_16byte_wwid_list)\n\t\treturn -ENOMEM;\n\n\tput_unaligned_be32(num_physicals * sizeof(struct report_phys_lun_16byte_wwid),\n\t\t&rpl_16byte_wwid_list->header.list_length);\n\trpl_16byte_wwid_list->header.flags = rpl_8byte_wwid_list->header.flags;\n\n\tfor (i = 0; i < num_physicals; i++) {\n\t\tmemcpy(&rpl_16byte_wwid_list->lun_entries[i].lunid, &rpl_8byte_wwid_list->lun_entries[i].lunid, sizeof(rpl_8byte_wwid_list->lun_entries[i].lunid));\n\t\tmemcpy(&rpl_16byte_wwid_list->lun_entries[i].wwid[0], &rpl_8byte_wwid_list->lun_entries[i].wwid, sizeof(rpl_8byte_wwid_list->lun_entries[i].wwid));\n\t\tmemset(&rpl_16byte_wwid_list->lun_entries[i].wwid[8], 0, 8);\n\t\trpl_16byte_wwid_list->lun_entries[i].device_type = rpl_8byte_wwid_list->lun_entries[i].device_type;\n\t\trpl_16byte_wwid_list->lun_entries[i].device_flags = rpl_8byte_wwid_list->lun_entries[i].device_flags;\n\t\trpl_16byte_wwid_list->lun_entries[i].lun_count = rpl_8byte_wwid_list->lun_entries[i].lun_count;\n\t\trpl_16byte_wwid_list->lun_entries[i].redundant_paths = rpl_8byte_wwid_list->lun_entries[i].redundant_paths;\n\t\trpl_16byte_wwid_list->lun_entries[i].aio_handle = rpl_8byte_wwid_list->lun_entries[i].aio_handle;\n\t}\n\n\tkfree(rpl_8byte_wwid_list);\n\t*buffer = rpl_16byte_wwid_list;\n\n\treturn 0;\n}\n\nstatic inline int pqi_report_logical_luns(struct pqi_ctrl_info *ctrl_info, void **buffer)\n{\n\treturn pqi_report_phys_logical_luns(ctrl_info, CISS_REPORT_LOG, buffer);\n}\n\nstatic int pqi_get_device_lists(struct pqi_ctrl_info *ctrl_info,\n\tstruct report_phys_lun_16byte_wwid_list **physdev_list,\n\tstruct report_log_lun_list **logdev_list)\n{\n\tint rc;\n\tsize_t logdev_list_length;\n\tsize_t logdev_data_length;\n\tstruct report_log_lun_list *internal_logdev_list;\n\tstruct report_log_lun_list *logdev_data;\n\tstruct report_lun_header report_lun_header;\n\n\trc = pqi_report_phys_luns(ctrl_info, (void **)physdev_list);\n\tif (rc)\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"report physical LUNs failed\\n\");\n\n\trc = pqi_report_logical_luns(ctrl_info, (void **)logdev_list);\n\tif (rc)\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"report logical LUNs failed\\n\");\n\n\t \n\n\tlogdev_data = *logdev_list;\n\n\tif (logdev_data) {\n\t\tlogdev_list_length =\n\t\t\tget_unaligned_be32(&logdev_data->header.list_length);\n\t} else {\n\t\tmemset(&report_lun_header, 0, sizeof(report_lun_header));\n\t\tlogdev_data =\n\t\t\t(struct report_log_lun_list *)&report_lun_header;\n\t\tlogdev_list_length = 0;\n\t}\n\n\tlogdev_data_length = sizeof(struct report_lun_header) +\n\t\tlogdev_list_length;\n\n\tinternal_logdev_list = kmalloc(logdev_data_length +\n\t\tsizeof(struct report_log_lun), GFP_KERNEL);\n\tif (!internal_logdev_list) {\n\t\tkfree(*logdev_list);\n\t\t*logdev_list = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(internal_logdev_list, logdev_data, logdev_data_length);\n\tmemset((u8 *)internal_logdev_list + logdev_data_length, 0,\n\t\tsizeof(struct report_log_lun));\n\tput_unaligned_be32(logdev_list_length +\n\t\tsizeof(struct report_log_lun),\n\t\t&internal_logdev_list->header.list_length);\n\n\tkfree(*logdev_list);\n\t*logdev_list = internal_logdev_list;\n\n\treturn 0;\n}\n\nstatic inline void pqi_set_bus_target_lun(struct pqi_scsi_dev *device,\n\tint bus, int target, int lun)\n{\n\tdevice->bus = bus;\n\tdevice->target = target;\n\tdevice->lun = lun;\n}\n\nstatic void pqi_assign_bus_target_lun(struct pqi_scsi_dev *device)\n{\n\tu8 *scsi3addr;\n\tu32 lunid;\n\tint bus;\n\tint target;\n\tint lun;\n\n\tscsi3addr = device->scsi3addr;\n\tlunid = get_unaligned_le32(scsi3addr);\n\n\tif (pqi_is_hba_lunid(scsi3addr)) {\n\t\t \n\t\tpqi_set_bus_target_lun(device, PQI_HBA_BUS, 0, lunid & 0x3fff);\n\t\tdevice->target_lun_valid = true;\n\t\treturn;\n\t}\n\n\tif (pqi_is_logical_device(device)) {\n\t\tif (device->is_external_raid_device) {\n\t\t\tbus = PQI_EXTERNAL_RAID_VOLUME_BUS;\n\t\t\ttarget = (lunid >> 16) & 0x3fff;\n\t\t\tlun = lunid & 0xff;\n\t\t} else {\n\t\t\tbus = PQI_RAID_VOLUME_BUS;\n\t\t\ttarget = 0;\n\t\t\tlun = lunid & 0x3fff;\n\t\t}\n\t\tpqi_set_bus_target_lun(device, bus, target, lun);\n\t\tdevice->target_lun_valid = true;\n\t\treturn;\n\t}\n\n\t \n\tpqi_set_bus_target_lun(device, PQI_PHYSICAL_DEVICE_BUS, 0, 0);\n}\n\nstatic void pqi_get_raid_level(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\tu8 raid_level;\n\tu8 *buffer;\n\n\traid_level = SA_RAID_UNKNOWN;\n\n\tbuffer = kmalloc(64, GFP_KERNEL);\n\tif (buffer) {\n\t\trc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\n\t\t\tVPD_PAGE | CISS_VPD_LV_DEVICE_GEOMETRY, buffer, 64);\n\t\tif (rc == 0) {\n\t\t\traid_level = buffer[8];\n\t\t\tif (raid_level > SA_RAID_MAX)\n\t\t\t\traid_level = SA_RAID_UNKNOWN;\n\t\t}\n\t\tkfree(buffer);\n\t}\n\n\tdevice->raid_level = raid_level;\n}\n\nstatic int pqi_validate_raid_map(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, struct raid_map *raid_map)\n{\n\tchar *err_msg;\n\tu32 raid_map_size;\n\tu32 r5or6_blocks_per_row;\n\n\traid_map_size = get_unaligned_le32(&raid_map->structure_size);\n\n\tif (raid_map_size < offsetof(struct raid_map, disk_data)) {\n\t\terr_msg = \"RAID map too small\";\n\t\tgoto bad_raid_map;\n\t}\n\n\tif (device->raid_level == SA_RAID_1) {\n\t\tif (get_unaligned_le16(&raid_map->layout_map_count) != 2) {\n\t\t\terr_msg = \"invalid RAID-1 map\";\n\t\t\tgoto bad_raid_map;\n\t\t}\n\t} else if (device->raid_level == SA_RAID_TRIPLE) {\n\t\tif (get_unaligned_le16(&raid_map->layout_map_count) != 3) {\n\t\t\terr_msg = \"invalid RAID-1(Triple) map\";\n\t\t\tgoto bad_raid_map;\n\t\t}\n\t} else if ((device->raid_level == SA_RAID_5 ||\n\t\tdevice->raid_level == SA_RAID_6) &&\n\t\tget_unaligned_le16(&raid_map->layout_map_count) > 1) {\n\t\t \n\t\tr5or6_blocks_per_row =\n\t\t\tget_unaligned_le16(&raid_map->strip_size) *\n\t\t\tget_unaligned_le16(&raid_map->data_disks_per_row);\n\t\tif (r5or6_blocks_per_row == 0) {\n\t\t\terr_msg = \"invalid RAID-5 or RAID-6 map\";\n\t\t\tgoto bad_raid_map;\n\t\t}\n\t}\n\n\treturn 0;\n\nbad_raid_map:\n\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\"logical device %08x%08x %s\\n\",\n\t\t*((u32 *)&device->scsi3addr),\n\t\t*((u32 *)&device->scsi3addr[4]), err_msg);\n\n\treturn -EINVAL;\n}\n\nstatic int pqi_get_raid_map(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\tu32 raid_map_size;\n\tstruct raid_map *raid_map;\n\n\traid_map = kmalloc(sizeof(*raid_map), GFP_KERNEL);\n\tif (!raid_map)\n\t\treturn -ENOMEM;\n\n\trc = pqi_send_scsi_raid_request(ctrl_info, CISS_GET_RAID_MAP,\n\t\tdevice->scsi3addr, raid_map, sizeof(*raid_map), 0, NULL);\n\tif (rc)\n\t\tgoto error;\n\n\traid_map_size = get_unaligned_le32(&raid_map->structure_size);\n\n\tif (raid_map_size > sizeof(*raid_map)) {\n\n\t\tkfree(raid_map);\n\n\t\traid_map = kmalloc(raid_map_size, GFP_KERNEL);\n\t\tif (!raid_map)\n\t\t\treturn -ENOMEM;\n\n\t\trc = pqi_send_scsi_raid_request(ctrl_info, CISS_GET_RAID_MAP,\n\t\t\tdevice->scsi3addr, raid_map, raid_map_size, 0, NULL);\n\t\tif (rc)\n\t\t\tgoto error;\n\n\t\tif (get_unaligned_le32(&raid_map->structure_size)\n\t\t\t!= raid_map_size) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"requested %u bytes, received %u bytes\\n\",\n\t\t\t\traid_map_size,\n\t\t\t\tget_unaligned_le32(&raid_map->structure_size));\n\t\t\trc = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\trc = pqi_validate_raid_map(ctrl_info, device, raid_map);\n\tif (rc)\n\t\tgoto error;\n\n\tdevice->raid_map = raid_map;\n\n\treturn 0;\n\nerror:\n\tkfree(raid_map);\n\n\treturn rc;\n}\n\nstatic void pqi_set_max_transfer_encrypted(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tif (!ctrl_info->lv_drive_type_mix_valid) {\n\t\tdevice->max_transfer_encrypted = ~0;\n\t\treturn;\n\t}\n\n\tswitch (LV_GET_DRIVE_TYPE_MIX(device->scsi3addr)) {\n\tcase LV_DRIVE_TYPE_MIX_SAS_HDD_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SATA_HDD_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SAS_OR_SATA_SSD_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SAS_SSD_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SATA_SSD_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SAS_ONLY:\n\tcase LV_DRIVE_TYPE_MIX_SATA_ONLY:\n\t\tdevice->max_transfer_encrypted =\n\t\t\tctrl_info->max_transfer_encrypted_sas_sata;\n\t\tbreak;\n\tcase LV_DRIVE_TYPE_MIX_NVME_ONLY:\n\t\tdevice->max_transfer_encrypted =\n\t\t\tctrl_info->max_transfer_encrypted_nvme;\n\t\tbreak;\n\tcase LV_DRIVE_TYPE_MIX_UNKNOWN:\n\tcase LV_DRIVE_TYPE_MIX_NO_RESTRICTION:\n\tdefault:\n\t\tdevice->max_transfer_encrypted =\n\t\t\tmin(ctrl_info->max_transfer_encrypted_sas_sata,\n\t\t\t\tctrl_info->max_transfer_encrypted_nvme);\n\t\tbreak;\n\t}\n}\n\nstatic void pqi_get_raid_bypass_status(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\tu8 *buffer;\n\tu8 bypass_status;\n\n\tbuffer = kmalloc(64, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn;\n\n\trc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\n\t\tVPD_PAGE | CISS_VPD_LV_BYPASS_STATUS, buffer, 64);\n\tif (rc)\n\t\tgoto out;\n\n#define RAID_BYPASS_STATUS\t\t4\n#define RAID_BYPASS_CONFIGURED\t\t0x1\n#define RAID_BYPASS_ENABLED\t\t0x2\n\n\tbypass_status = buffer[RAID_BYPASS_STATUS];\n\tdevice->raid_bypass_configured =\n\t\t(bypass_status & RAID_BYPASS_CONFIGURED) != 0;\n\tif (device->raid_bypass_configured &&\n\t\t(bypass_status & RAID_BYPASS_ENABLED) &&\n\t\tpqi_get_raid_map(ctrl_info, device) == 0) {\n\t\tdevice->raid_bypass_enabled = true;\n\t\tif (get_unaligned_le16(&device->raid_map->flags) &\n\t\t\tRAID_MAP_ENCRYPTION_ENABLED)\n\t\t\tpqi_set_max_transfer_encrypted(ctrl_info, device);\n\t}\n\nout:\n\tkfree(buffer);\n}\n\n \n\nstatic void pqi_get_volume_status(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\tsize_t page_length;\n\tu8 volume_status = CISS_LV_STATUS_UNAVAILABLE;\n\tbool volume_offline = true;\n\tu32 volume_flags;\n\tstruct ciss_vpd_logical_volume_status *vpd;\n\n\tvpd = kmalloc(sizeof(*vpd), GFP_KERNEL);\n\tif (!vpd)\n\t\tgoto no_buffer;\n\n\trc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\n\t\tVPD_PAGE | CISS_VPD_LV_STATUS, vpd, sizeof(*vpd));\n\tif (rc)\n\t\tgoto out;\n\n\tif (vpd->page_code != CISS_VPD_LV_STATUS)\n\t\tgoto out;\n\n\tpage_length = offsetof(struct ciss_vpd_logical_volume_status,\n\t\tvolume_status) + vpd->page_length;\n\tif (page_length < sizeof(*vpd))\n\t\tgoto out;\n\n\tvolume_status = vpd->volume_status;\n\tvolume_flags = get_unaligned_be32(&vpd->flags);\n\tvolume_offline = (volume_flags & CISS_LV_FLAGS_NO_HOST_IO) != 0;\n\nout:\n\tkfree(vpd);\nno_buffer:\n\tdevice->volume_status = volume_status;\n\tdevice->volume_offline = volume_offline;\n}\n\n#define PQI_DEVICE_NCQ_PRIO_SUPPORTED\t0x01\n#define PQI_DEVICE_PHY_MAP_SUPPORTED\t0x10\n#define PQI_DEVICE_ERASE_IN_PROGRESS\t0x10\n\nstatic int pqi_get_physical_device_info(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device,\n\tstruct bmic_identify_physical_device *id_phys)\n{\n\tint rc;\n\n\tmemset(id_phys, 0, sizeof(*id_phys));\n\n\trc = pqi_identify_physical_device(ctrl_info, device,\n\t\tid_phys, sizeof(*id_phys));\n\tif (rc) {\n\t\tdevice->queue_depth = PQI_PHYSICAL_DISK_DEFAULT_MAX_QUEUE_DEPTH;\n\t\treturn rc;\n\t}\n\n\tscsi_sanitize_inquiry_string(&id_phys->model[0], 8);\n\tscsi_sanitize_inquiry_string(&id_phys->model[8], 16);\n\n\tmemcpy(device->vendor, &id_phys->model[0], sizeof(device->vendor));\n\tmemcpy(device->model, &id_phys->model[8], sizeof(device->model));\n\n\tdevice->box_index = id_phys->box_index;\n\tdevice->phys_box_on_bus = id_phys->phys_box_on_bus;\n\tdevice->phy_connected_dev_type = id_phys->phy_connected_dev_type[0];\n\tdevice->queue_depth =\n\t\tget_unaligned_le16(&id_phys->current_queue_depth_limit);\n\tdevice->active_path_index = id_phys->active_path_number;\n\tdevice->path_map = id_phys->redundant_path_present_map;\n\tmemcpy(&device->box,\n\t\t&id_phys->alternate_paths_phys_box_on_port,\n\t\tsizeof(device->box));\n\tmemcpy(&device->phys_connector,\n\t\t&id_phys->alternate_paths_phys_connector,\n\t\tsizeof(device->phys_connector));\n\tdevice->bay = id_phys->phys_bay_in_box;\n\tdevice->lun_count = id_phys->multi_lun_device_lun_count;\n\tif ((id_phys->even_more_flags & PQI_DEVICE_PHY_MAP_SUPPORTED) &&\n\t\tid_phys->phy_count)\n\t\tdevice->phy_id =\n\t\t\tid_phys->phy_to_phy_map[device->active_path_index];\n\telse\n\t\tdevice->phy_id = 0xFF;\n\n\tdevice->ncq_prio_support =\n\t\t((get_unaligned_le32(&id_phys->misc_drive_flags) >> 16) &\n\t\tPQI_DEVICE_NCQ_PRIO_SUPPORTED);\n\n\tdevice->erase_in_progress = !!(get_unaligned_le16(&id_phys->extra_physical_drive_flags) & PQI_DEVICE_ERASE_IN_PROGRESS);\n\n\treturn 0;\n}\n\nstatic int pqi_get_logical_device_info(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\tu8 *buffer;\n\n\tbuffer = kmalloc(64, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\t \n\trc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr, 0, buffer, 64);\n\tif (rc)\n\t\tgoto out;\n\n\tscsi_sanitize_inquiry_string(&buffer[8], 8);\n\tscsi_sanitize_inquiry_string(&buffer[16], 16);\n\n\tdevice->devtype = buffer[0] & 0x1f;\n\tmemcpy(device->vendor, &buffer[8], sizeof(device->vendor));\n\tmemcpy(device->model, &buffer[16], sizeof(device->model));\n\n\tif (device->devtype == TYPE_DISK) {\n\t\tif (device->is_external_raid_device) {\n\t\t\tdevice->raid_level = SA_RAID_UNKNOWN;\n\t\t\tdevice->volume_status = CISS_LV_OK;\n\t\t\tdevice->volume_offline = false;\n\t\t} else {\n\t\t\tpqi_get_raid_level(ctrl_info, device);\n\t\t\tpqi_get_raid_bypass_status(ctrl_info, device);\n\t\t\tpqi_get_volume_status(ctrl_info, device);\n\t\t}\n\t}\n\nout:\n\tkfree(buffer);\n\n\treturn rc;\n}\n\n \nstatic inline bool pqi_keep_device_offline(struct pqi_scsi_dev *device)\n{\n\treturn device->erase_in_progress;\n}\n\nstatic int pqi_get_device_info_phys_logical(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device,\n\tstruct bmic_identify_physical_device *id_phys)\n{\n\tint rc;\n\n\tif (device->is_expander_smp_device)\n\t\treturn 0;\n\n\tif (pqi_is_logical_device(device))\n\t\trc = pqi_get_logical_device_info(ctrl_info, device);\n\telse\n\t\trc = pqi_get_physical_device_info(ctrl_info, device, id_phys);\n\n\treturn rc;\n}\n\nstatic int pqi_get_device_info(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device,\n\tstruct bmic_identify_physical_device *id_phys)\n{\n\tint rc;\n\n\trc = pqi_get_device_info_phys_logical(ctrl_info, device, id_phys);\n\n\tif (rc == 0 && device->lun_count == 0)\n\t\tdevice->lun_count = 1;\n\n\treturn rc;\n}\n\nstatic void pqi_show_volume_status(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tchar *status;\n\tstatic const char unknown_state_str[] =\n\t\t\"Volume is in an unknown state (%u)\";\n\tchar unknown_state_buffer[sizeof(unknown_state_str) + 10];\n\n\tswitch (device->volume_status) {\n\tcase CISS_LV_OK:\n\t\tstatus = \"Volume online\";\n\t\tbreak;\n\tcase CISS_LV_FAILED:\n\t\tstatus = \"Volume failed\";\n\t\tbreak;\n\tcase CISS_LV_NOT_CONFIGURED:\n\t\tstatus = \"Volume not configured\";\n\t\tbreak;\n\tcase CISS_LV_DEGRADED:\n\t\tstatus = \"Volume degraded\";\n\t\tbreak;\n\tcase CISS_LV_READY_FOR_RECOVERY:\n\t\tstatus = \"Volume ready for recovery operation\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_RECOVERY:\n\t\tstatus = \"Volume undergoing recovery\";\n\t\tbreak;\n\tcase CISS_LV_WRONG_PHYSICAL_DRIVE_REPLACED:\n\t\tstatus = \"Wrong physical drive was replaced\";\n\t\tbreak;\n\tcase CISS_LV_PHYSICAL_DRIVE_CONNECTION_PROBLEM:\n\t\tstatus = \"A physical drive not properly connected\";\n\t\tbreak;\n\tcase CISS_LV_HARDWARE_OVERHEATING:\n\t\tstatus = \"Hardware is overheating\";\n\t\tbreak;\n\tcase CISS_LV_HARDWARE_HAS_OVERHEATED:\n\t\tstatus = \"Hardware has overheated\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_EXPANSION:\n\t\tstatus = \"Volume undergoing expansion\";\n\t\tbreak;\n\tcase CISS_LV_NOT_AVAILABLE:\n\t\tstatus = \"Volume waiting for transforming volume\";\n\t\tbreak;\n\tcase CISS_LV_QUEUED_FOR_EXPANSION:\n\t\tstatus = \"Volume queued for expansion\";\n\t\tbreak;\n\tcase CISS_LV_DISABLED_SCSI_ID_CONFLICT:\n\t\tstatus = \"Volume disabled due to SCSI ID conflict\";\n\t\tbreak;\n\tcase CISS_LV_EJECTED:\n\t\tstatus = \"Volume has been ejected\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_ERASE:\n\t\tstatus = \"Volume undergoing background erase\";\n\t\tbreak;\n\tcase CISS_LV_READY_FOR_PREDICTIVE_SPARE_REBUILD:\n\t\tstatus = \"Volume ready for predictive spare rebuild\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_RPI:\n\t\tstatus = \"Volume undergoing rapid parity initialization\";\n\t\tbreak;\n\tcase CISS_LV_PENDING_RPI:\n\t\tstatus = \"Volume queued for rapid parity initialization\";\n\t\tbreak;\n\tcase CISS_LV_ENCRYPTED_NO_KEY:\n\t\tstatus = \"Encrypted volume inaccessible - key not present\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_ENCRYPTION:\n\t\tstatus = \"Volume undergoing encryption process\";\n\t\tbreak;\n\tcase CISS_LV_UNDERGOING_ENCRYPTION_REKEYING:\n\t\tstatus = \"Volume undergoing encryption re-keying process\";\n\t\tbreak;\n\tcase CISS_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:\n\t\tstatus = \"Volume encrypted but encryption is disabled\";\n\t\tbreak;\n\tcase CISS_LV_PENDING_ENCRYPTION:\n\t\tstatus = \"Volume pending migration to encrypted state\";\n\t\tbreak;\n\tcase CISS_LV_PENDING_ENCRYPTION_REKEYING:\n\t\tstatus = \"Volume pending encryption rekeying\";\n\t\tbreak;\n\tcase CISS_LV_NOT_SUPPORTED:\n\t\tstatus = \"Volume not supported on this controller\";\n\t\tbreak;\n\tcase CISS_LV_STATUS_UNAVAILABLE:\n\t\tstatus = \"Volume status not available\";\n\t\tbreak;\n\tdefault:\n\t\tsnprintf(unknown_state_buffer, sizeof(unknown_state_buffer),\n\t\t\tunknown_state_str, device->volume_status);\n\t\tstatus = unknown_state_buffer;\n\t\tbreak;\n\t}\n\n\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\"scsi %d:%d:%d:%d %s\\n\",\n\t\tctrl_info->scsi_host->host_no,\n\t\tdevice->bus, device->target, device->lun, status);\n}\n\nstatic void pqi_rescan_worker(struct work_struct *work)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = container_of(to_delayed_work(work), struct pqi_ctrl_info,\n\t\trescan_work);\n\n\tpqi_scan_scsi_devices(ctrl_info);\n}\n\nstatic int pqi_add_device(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tint rc;\n\n\tif (pqi_is_logical_device(device))\n\t\trc = scsi_add_device(ctrl_info->scsi_host, device->bus,\n\t\t\tdevice->target, device->lun);\n\telse\n\t\trc = pqi_add_sas_device(ctrl_info->sas_host, device);\n\n\treturn rc;\n}\n\n#define PQI_REMOVE_DEVICE_PENDING_IO_TIMEOUT_MSECS\t(20 * 1000)\n\nstatic inline void pqi_remove_device(struct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *device)\n{\n\tint rc;\n\tint lun;\n\n\tfor (lun = 0; lun < device->lun_count; lun++) {\n\t\trc = pqi_device_wait_for_pending_io(ctrl_info, device, lun,\n\t\t\tPQI_REMOVE_DEVICE_PENDING_IO_TIMEOUT_MSECS);\n\t\tif (rc)\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"scsi %d:%d:%d:%d removing device with %d outstanding command(s)\\n\",\n\t\t\t\tctrl_info->scsi_host->host_no, device->bus,\n\t\t\t\tdevice->target, lun,\n\t\t\t\tatomic_read(&device->scsi_cmds_outstanding[lun]));\n\t}\n\n\tif (pqi_is_logical_device(device))\n\t\tscsi_remove_device(device->sdev);\n\telse\n\t\tpqi_remove_sas_device(device);\n\n\tpqi_device_remove_start(device);\n}\n\n \n\nstatic struct pqi_scsi_dev *pqi_find_scsi_dev(struct pqi_ctrl_info *ctrl_info,\n\tint bus, int target, int lun)\n{\n\tstruct pqi_scsi_dev *device;\n\n\tlist_for_each_entry(device, &ctrl_info->scsi_device_list, scsi_device_list_entry)\n\t\tif (device->bus == bus && device->target == target && device->lun == lun)\n\t\t\treturn device;\n\n\treturn NULL;\n}\n\nstatic inline bool pqi_device_equal(struct pqi_scsi_dev *dev1, struct pqi_scsi_dev *dev2)\n{\n\tif (dev1->is_physical_device != dev2->is_physical_device)\n\t\treturn false;\n\n\tif (dev1->is_physical_device)\n\t\treturn memcmp(dev1->wwid, dev2->wwid, sizeof(dev1->wwid)) == 0;\n\n\treturn memcmp(dev1->volume_id, dev2->volume_id, sizeof(dev1->volume_id)) == 0;\n}\n\nenum pqi_find_result {\n\tDEVICE_NOT_FOUND,\n\tDEVICE_CHANGED,\n\tDEVICE_SAME,\n};\n\nstatic enum pqi_find_result pqi_scsi_find_entry(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device_to_find, struct pqi_scsi_dev **matching_device)\n{\n\tstruct pqi_scsi_dev *device;\n\n\tlist_for_each_entry(device, &ctrl_info->scsi_device_list, scsi_device_list_entry) {\n\t\tif (pqi_scsi3addr_equal(device_to_find->scsi3addr, device->scsi3addr)) {\n\t\t\t*matching_device = device;\n\t\t\tif (pqi_device_equal(device_to_find, device)) {\n\t\t\t\tif (device_to_find->volume_offline)\n\t\t\t\t\treturn DEVICE_CHANGED;\n\t\t\t\treturn DEVICE_SAME;\n\t\t\t}\n\t\t\treturn DEVICE_CHANGED;\n\t\t}\n\t}\n\n\treturn DEVICE_NOT_FOUND;\n}\n\nstatic inline const char *pqi_device_type(struct pqi_scsi_dev *device)\n{\n\tif (device->is_expander_smp_device)\n\t\treturn \"Enclosure SMP    \";\n\n\treturn scsi_device_type(device->devtype);\n}\n\n#define PQI_DEV_INFO_BUFFER_LENGTH\t128\n\nstatic void pqi_dev_info(struct pqi_ctrl_info *ctrl_info,\n\tchar *action, struct pqi_scsi_dev *device)\n{\n\tssize_t count;\n\tchar buffer[PQI_DEV_INFO_BUFFER_LENGTH];\n\n\tcount = scnprintf(buffer, PQI_DEV_INFO_BUFFER_LENGTH,\n\t\t\"%d:%d:\", ctrl_info->scsi_host->host_no, device->bus);\n\n\tif (device->target_lun_valid)\n\t\tcount += scnprintf(buffer + count,\n\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\"%d:%d\",\n\t\t\tdevice->target,\n\t\t\tdevice->lun);\n\telse\n\t\tcount += scnprintf(buffer + count,\n\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\"-:-\");\n\n\tif (pqi_is_logical_device(device))\n\t\tcount += scnprintf(buffer + count,\n\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\" %08x%08x\",\n\t\t\t*((u32 *)&device->scsi3addr),\n\t\t\t*((u32 *)&device->scsi3addr[4]));\n\telse\n\t\tcount += scnprintf(buffer + count,\n\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\" %016llx%016llx\",\n\t\t\tget_unaligned_be64(&device->wwid[0]),\n\t\t\tget_unaligned_be64(&device->wwid[8]));\n\n\tcount += scnprintf(buffer + count, PQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\" %s %.8s %.16s \",\n\t\tpqi_device_type(device),\n\t\tdevice->vendor,\n\t\tdevice->model);\n\n\tif (pqi_is_logical_device(device)) {\n\t\tif (device->devtype == TYPE_DISK)\n\t\t\tcount += scnprintf(buffer + count,\n\t\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\t\"SSDSmartPathCap%c En%c %-12s\",\n\t\t\t\tdevice->raid_bypass_configured ? '+' : '-',\n\t\t\t\tdevice->raid_bypass_enabled ? '+' : '-',\n\t\t\t\tpqi_raid_level_to_string(device->raid_level));\n\t} else {\n\t\tcount += scnprintf(buffer + count,\n\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\"AIO%c\", device->aio_enabled ? '+' : '-');\n\t\tif (device->devtype == TYPE_DISK ||\n\t\t\tdevice->devtype == TYPE_ZBC)\n\t\t\tcount += scnprintf(buffer + count,\n\t\t\t\tPQI_DEV_INFO_BUFFER_LENGTH - count,\n\t\t\t\t\" qd=%-6d\", device->queue_depth);\n\t}\n\n\tdev_info(&ctrl_info->pci_dev->dev, \"%s %s\\n\", action, buffer);\n}\n\nstatic bool pqi_raid_maps_equal(struct raid_map *raid_map1, struct raid_map *raid_map2)\n{\n\tu32 raid_map1_size;\n\tu32 raid_map2_size;\n\n\tif (raid_map1 == NULL || raid_map2 == NULL)\n\t\treturn raid_map1 == raid_map2;\n\n\traid_map1_size = get_unaligned_le32(&raid_map1->structure_size);\n\traid_map2_size = get_unaligned_le32(&raid_map2->structure_size);\n\n\tif (raid_map1_size != raid_map2_size)\n\t\treturn false;\n\n\treturn memcmp(raid_map1, raid_map2, raid_map1_size) == 0;\n}\n\n \n\nstatic void pqi_scsi_update_device(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *existing_device, struct pqi_scsi_dev *new_device)\n{\n\texisting_device->device_type = new_device->device_type;\n\texisting_device->bus = new_device->bus;\n\tif (new_device->target_lun_valid) {\n\t\texisting_device->target = new_device->target;\n\t\texisting_device->lun = new_device->lun;\n\t\texisting_device->target_lun_valid = true;\n\t}\n\n\t \n\n\texisting_device->is_physical_device = new_device->is_physical_device;\n\tmemcpy(existing_device->vendor, new_device->vendor, sizeof(existing_device->vendor));\n\tmemcpy(existing_device->model, new_device->model, sizeof(existing_device->model));\n\texisting_device->sas_address = new_device->sas_address;\n\texisting_device->queue_depth = new_device->queue_depth;\n\texisting_device->device_offline = false;\n\texisting_device->lun_count = new_device->lun_count;\n\n\tif (pqi_is_logical_device(existing_device)) {\n\t\texisting_device->is_external_raid_device = new_device->is_external_raid_device;\n\n\t\tif (existing_device->devtype == TYPE_DISK) {\n\t\t\texisting_device->raid_level = new_device->raid_level;\n\t\t\texisting_device->volume_status = new_device->volume_status;\n\t\t\tif (ctrl_info->logical_volume_rescan_needed)\n\t\t\t\texisting_device->rescan = true;\n\t\t\tmemset(existing_device->next_bypass_group, 0, sizeof(existing_device->next_bypass_group));\n\t\t\tif (!pqi_raid_maps_equal(existing_device->raid_map, new_device->raid_map)) {\n\t\t\t\tkfree(existing_device->raid_map);\n\t\t\t\texisting_device->raid_map = new_device->raid_map;\n\t\t\t\t \n\t\t\t\tnew_device->raid_map = NULL;\n\t\t\t}\n\t\t\texisting_device->raid_bypass_configured = new_device->raid_bypass_configured;\n\t\t\texisting_device->raid_bypass_enabled = new_device->raid_bypass_enabled;\n\t\t}\n\t} else {\n\t\texisting_device->aio_enabled = new_device->aio_enabled;\n\t\texisting_device->aio_handle = new_device->aio_handle;\n\t\texisting_device->is_expander_smp_device = new_device->is_expander_smp_device;\n\t\texisting_device->active_path_index = new_device->active_path_index;\n\t\texisting_device->phy_id = new_device->phy_id;\n\t\texisting_device->path_map = new_device->path_map;\n\t\texisting_device->bay = new_device->bay;\n\t\texisting_device->box_index = new_device->box_index;\n\t\texisting_device->phys_box_on_bus = new_device->phys_box_on_bus;\n\t\texisting_device->phy_connected_dev_type = new_device->phy_connected_dev_type;\n\t\tmemcpy(existing_device->box, new_device->box, sizeof(existing_device->box));\n\t\tmemcpy(existing_device->phys_connector, new_device->phys_connector, sizeof(existing_device->phys_connector));\n\t}\n}\n\nstatic inline void pqi_free_device(struct pqi_scsi_dev *device)\n{\n\tif (device) {\n\t\tkfree(device->raid_map);\n\t\tkfree(device);\n\t}\n}\n\n \n\nstatic inline void pqi_fixup_botched_add(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\tlist_del(&device->scsi_device_list_entry);\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\t \n\tdevice->keep_device = false;\n}\n\nstatic inline bool pqi_is_device_added(struct pqi_scsi_dev *device)\n{\n\tif (device->is_expander_smp_device)\n\t\treturn device->sas_port != NULL;\n\n\treturn device->sdev != NULL;\n}\n\nstatic inline void pqi_init_device_tmf_work(struct pqi_scsi_dev *device)\n{\n\tunsigned int lun;\n\tstruct pqi_tmf_work *tmf_work;\n\n\tfor (lun = 0, tmf_work = device->tmf_work; lun < PQI_MAX_LUNS_PER_DEVICE; lun++, tmf_work++)\n\t\tINIT_WORK(&tmf_work->work_struct, pqi_tmf_worker);\n}\n\nstatic void pqi_update_device_list(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *new_device_list[], unsigned int num_new_devices)\n{\n\tint rc;\n\tunsigned int i;\n\tunsigned long flags;\n\tenum pqi_find_result find_result;\n\tstruct pqi_scsi_dev *device;\n\tstruct pqi_scsi_dev *next;\n\tstruct pqi_scsi_dev *matching_device;\n\tLIST_HEAD(add_list);\n\tLIST_HEAD(delete_list);\n\n\t \n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\t \n\tlist_for_each_entry(device, &ctrl_info->scsi_device_list, scsi_device_list_entry)\n\t\tdevice->device_gone = true;\n\n\tfor (i = 0; i < num_new_devices; i++) {\n\t\tdevice = new_device_list[i];\n\n\t\tfind_result = pqi_scsi_find_entry(ctrl_info, device,\n\t\t\t&matching_device);\n\n\t\tswitch (find_result) {\n\t\tcase DEVICE_SAME:\n\t\t\t \n\t\t\tdevice->new_device = false;\n\t\t\tmatching_device->device_gone = false;\n\t\t\tpqi_scsi_update_device(ctrl_info, matching_device, device);\n\t\t\tbreak;\n\t\tcase DEVICE_NOT_FOUND:\n\t\t\t \n\t\t\tdevice->new_device = true;\n\t\t\tbreak;\n\t\tcase DEVICE_CHANGED:\n\t\t\t \n\t\t\tdevice->new_device = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tlist_for_each_entry_safe(device, next, &ctrl_info->scsi_device_list,\n\t\tscsi_device_list_entry) {\n\t\tif (device->device_gone) {\n\t\t\tlist_del(&device->scsi_device_list_entry);\n\t\t\tlist_add_tail(&device->delete_list_entry, &delete_list);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < num_new_devices; i++) {\n\t\tdevice = new_device_list[i];\n\t\tif (!device->new_device)\n\t\t\tcontinue;\n\t\tif (device->volume_offline)\n\t\t\tcontinue;\n\t\tlist_add_tail(&device->scsi_device_list_entry,\n\t\t\t&ctrl_info->scsi_device_list);\n\t\tlist_add_tail(&device->add_list_entry, &add_list);\n\t\t \n\t\tdevice->keep_device = true;\n\t\tpqi_init_device_tmf_work(device);\n\t}\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\t \n\tif (pqi_ofa_in_progress(ctrl_info)) {\n\t\tlist_for_each_entry_safe(device, next, &delete_list, delete_list_entry)\n\t\t\tif (pqi_is_device_added(device))\n\t\t\t\tpqi_device_remove_start(device);\n\t\tpqi_ctrl_unblock_device_reset(ctrl_info);\n\t\tpqi_scsi_unblock_requests(ctrl_info);\n\t}\n\n\t \n\tlist_for_each_entry_safe(device, next, &delete_list, delete_list_entry) {\n\t\tif (device->volume_offline) {\n\t\t\tpqi_dev_info(ctrl_info, \"offline\", device);\n\t\t\tpqi_show_volume_status(ctrl_info, device);\n\t\t} else {\n\t\t\tpqi_dev_info(ctrl_info, \"removed\", device);\n\t\t}\n\t\tif (pqi_is_device_added(device))\n\t\t\tpqi_remove_device(ctrl_info, device);\n\t\tlist_del(&device->delete_list_entry);\n\t\tpqi_free_device(device);\n\t}\n\n\t \n\tlist_for_each_entry(device, &ctrl_info->scsi_device_list, scsi_device_list_entry) {\n\t\tif (device->sdev && device->queue_depth != device->advertised_queue_depth) {\n\t\t\tdevice->advertised_queue_depth = device->queue_depth;\n\t\t\tscsi_change_queue_depth(device->sdev, device->advertised_queue_depth);\n\t\t\tif (device->rescan) {\n\t\t\t\tscsi_rescan_device(device->sdev);\n\t\t\t\tdevice->rescan = false;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tlist_for_each_entry_safe(device, next, &add_list, add_list_entry) {\n\t\tif (!pqi_is_device_added(device)) {\n\t\t\trc = pqi_add_device(ctrl_info, device);\n\t\t\tif (rc == 0) {\n\t\t\t\tpqi_dev_info(ctrl_info, \"added\", device);\n\t\t\t} else {\n\t\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\t\"scsi %d:%d:%d:%d addition failed, device not added\\n\",\n\t\t\t\t\tctrl_info->scsi_host->host_no,\n\t\t\t\t\tdevice->bus, device->target,\n\t\t\t\t\tdevice->lun);\n\t\t\t\tpqi_fixup_botched_add(ctrl_info, device);\n\t\t\t}\n\t\t}\n\t}\n\n\tctrl_info->logical_volume_rescan_needed = false;\n\n}\n\nstatic inline bool pqi_is_supported_device(struct pqi_scsi_dev *device)\n{\n\t \n\tif (device->device_type == SA_DEVICE_TYPE_CONTROLLER &&\n\t\t!pqi_is_hba_lunid(device->scsi3addr))\n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline bool pqi_skip_device(u8 *scsi3addr)\n{\n\t \n\tif (MASKED_DEVICE(scsi3addr))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline void pqi_mask_device(u8 *scsi3addr)\n{\n\tscsi3addr[3] |= 0xc0;\n}\n\nstatic inline bool pqi_is_multipath_device(struct pqi_scsi_dev *device)\n{\n\tif (pqi_is_logical_device(device))\n\t\treturn false;\n\n\treturn (device->path_map & (device->path_map - 1)) != 0;\n}\n\nstatic inline bool pqi_expose_device(struct pqi_scsi_dev *device)\n{\n\treturn !device->is_physical_device || !pqi_skip_device(device->scsi3addr);\n}\n\nstatic int pqi_update_scsi_devices(struct pqi_ctrl_info *ctrl_info)\n{\n\tint i;\n\tint rc;\n\tLIST_HEAD(new_device_list_head);\n\tstruct report_phys_lun_16byte_wwid_list *physdev_list = NULL;\n\tstruct report_log_lun_list *logdev_list = NULL;\n\tstruct report_phys_lun_16byte_wwid *phys_lun;\n\tstruct report_log_lun *log_lun;\n\tstruct bmic_identify_physical_device *id_phys = NULL;\n\tu32 num_physicals;\n\tu32 num_logicals;\n\tstruct pqi_scsi_dev **new_device_list = NULL;\n\tstruct pqi_scsi_dev *device;\n\tstruct pqi_scsi_dev *next;\n\tunsigned int num_new_devices;\n\tunsigned int num_valid_devices;\n\tbool is_physical_device;\n\tu8 *scsi3addr;\n\tunsigned int physical_index;\n\tunsigned int logical_index;\n\tstatic char *out_of_memory_msg =\n\t\t\"failed to allocate memory, device discovery stopped\";\n\n\trc = pqi_get_device_lists(ctrl_info, &physdev_list, &logdev_list);\n\tif (rc)\n\t\tgoto out;\n\n\tif (physdev_list)\n\t\tnum_physicals =\n\t\t\tget_unaligned_be32(&physdev_list->header.list_length)\n\t\t\t\t/ sizeof(physdev_list->lun_entries[0]);\n\telse\n\t\tnum_physicals = 0;\n\n\tif (logdev_list)\n\t\tnum_logicals =\n\t\t\tget_unaligned_be32(&logdev_list->header.list_length)\n\t\t\t\t/ sizeof(logdev_list->lun_entries[0]);\n\telse\n\t\tnum_logicals = 0;\n\n\tif (num_physicals) {\n\t\t \n\t\tid_phys = kmalloc(sizeof(*id_phys), GFP_KERNEL);\n\t\tif (!id_phys) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev, \"%s\\n\",\n\t\t\t\tout_of_memory_msg);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (pqi_hide_vsep) {\n\t\t\tfor (i = num_physicals - 1; i >= 0; i--) {\n\t\t\t\tphys_lun = &physdev_list->lun_entries[i];\n\t\t\t\tif (CISS_GET_DRIVE_NUMBER(phys_lun->lunid) == PQI_VSEP_CISS_BTL) {\n\t\t\t\t\tpqi_mask_device(phys_lun->lunid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (num_logicals &&\n\t\t(logdev_list->header.flags & CISS_REPORT_LOG_FLAG_DRIVE_TYPE_MIX))\n\t\tctrl_info->lv_drive_type_mix_valid = true;\n\n\tnum_new_devices = num_physicals + num_logicals;\n\n\tnew_device_list = kmalloc_array(num_new_devices,\n\t\t\t\t\tsizeof(*new_device_list),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!new_device_list) {\n\t\tdev_warn(&ctrl_info->pci_dev->dev, \"%s\\n\", out_of_memory_msg);\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_new_devices; i++) {\n\t\tdevice = kzalloc(sizeof(*device), GFP_KERNEL);\n\t\tif (!device) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev, \"%s\\n\",\n\t\t\t\tout_of_memory_msg);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlist_add_tail(&device->new_device_list_entry,\n\t\t\t&new_device_list_head);\n\t}\n\n\tdevice = NULL;\n\tnum_valid_devices = 0;\n\tphysical_index = 0;\n\tlogical_index = 0;\n\n\tfor (i = 0; i < num_new_devices; i++) {\n\n\t\tif ((!pqi_expose_ld_first && i < num_physicals) ||\n\t\t\t(pqi_expose_ld_first && i >= num_logicals)) {\n\t\t\tis_physical_device = true;\n\t\t\tphys_lun = &physdev_list->lun_entries[physical_index++];\n\t\t\tlog_lun = NULL;\n\t\t\tscsi3addr = phys_lun->lunid;\n\t\t} else {\n\t\t\tis_physical_device = false;\n\t\t\tphys_lun = NULL;\n\t\t\tlog_lun = &logdev_list->lun_entries[logical_index++];\n\t\t\tscsi3addr = log_lun->lunid;\n\t\t}\n\n\t\tif (is_physical_device && pqi_skip_device(scsi3addr))\n\t\t\tcontinue;\n\n\t\tif (device)\n\t\t\tdevice = list_next_entry(device, new_device_list_entry);\n\t\telse\n\t\t\tdevice = list_first_entry(&new_device_list_head,\n\t\t\t\tstruct pqi_scsi_dev, new_device_list_entry);\n\n\t\tmemcpy(device->scsi3addr, scsi3addr, sizeof(device->scsi3addr));\n\t\tdevice->is_physical_device = is_physical_device;\n\t\tif (is_physical_device) {\n\t\t\tdevice->device_type = phys_lun->device_type;\n\t\t\tif (device->device_type == SA_DEVICE_TYPE_EXPANDER_SMP)\n\t\t\t\tdevice->is_expander_smp_device = true;\n\t\t} else {\n\t\t\tdevice->is_external_raid_device =\n\t\t\t\tpqi_is_external_raid_addr(scsi3addr);\n\t\t}\n\n\t\tif (!pqi_is_supported_device(device))\n\t\t\tcontinue;\n\n\t\t \n\t\trc = pqi_get_device_info(ctrl_info, device, id_phys);\n\t\tif (rc == -ENOMEM) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev, \"%s\\n\",\n\t\t\t\tout_of_memory_msg);\n\t\t\tgoto out;\n\t\t}\n\t\tif (rc) {\n\t\t\tif (device->is_physical_device)\n\t\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\t\"obtaining device info failed, skipping physical device %016llx%016llx\\n\",\n\t\t\t\t\tget_unaligned_be64(&phys_lun->wwid[0]),\n\t\t\t\t\tget_unaligned_be64(&phys_lun->wwid[8]));\n\t\t\telse\n\t\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\t\"obtaining device info failed, skipping logical device %08x%08x\\n\",\n\t\t\t\t\t*((u32 *)&device->scsi3addr),\n\t\t\t\t\t*((u32 *)&device->scsi3addr[4]));\n\t\t\trc = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (pqi_keep_device_offline(device))\n\t\t\tcontinue;\n\n\t\tpqi_assign_bus_target_lun(device);\n\n\t\tif (device->is_physical_device) {\n\t\t\tmemcpy(device->wwid, phys_lun->wwid, sizeof(device->wwid));\n\t\t\tif ((phys_lun->device_flags &\n\t\t\t\tCISS_REPORT_PHYS_DEV_FLAG_AIO_ENABLED) &&\n\t\t\t\tphys_lun->aio_handle) {\n\t\t\t\t\tdevice->aio_enabled = true;\n\t\t\t\t\tdevice->aio_handle =\n\t\t\t\t\t\tphys_lun->aio_handle;\n\t\t\t}\n\t\t} else {\n\t\t\tmemcpy(device->volume_id, log_lun->volume_id,\n\t\t\t\tsizeof(device->volume_id));\n\t\t}\n\n\t\tdevice->sas_address = get_unaligned_be64(&device->wwid[0]);\n\n\t\tnew_device_list[num_valid_devices++] = device;\n\t}\n\n\tpqi_update_device_list(ctrl_info, new_device_list, num_valid_devices);\n\nout:\n\tlist_for_each_entry_safe(device, next, &new_device_list_head,\n\t\tnew_device_list_entry) {\n\t\tif (device->keep_device)\n\t\t\tcontinue;\n\t\tlist_del(&device->new_device_list_entry);\n\t\tpqi_free_device(device);\n\t}\n\n\tkfree(new_device_list);\n\tkfree(physdev_list);\n\tkfree(logdev_list);\n\tkfree(id_phys);\n\n\treturn rc;\n}\n\nstatic int pqi_scan_scsi_devices(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tint mutex_acquired;\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENXIO;\n\n\tmutex_acquired = mutex_trylock(&ctrl_info->scan_mutex);\n\n\tif (!mutex_acquired) {\n\t\tif (pqi_ctrl_scan_blocked(ctrl_info))\n\t\t\treturn -EBUSY;\n\t\tpqi_schedule_rescan_worker_delayed(ctrl_info);\n\t\treturn -EINPROGRESS;\n\t}\n\n\trc = pqi_update_scsi_devices(ctrl_info);\n\tif (rc && !pqi_ctrl_scan_blocked(ctrl_info))\n\t\tpqi_schedule_rescan_worker_delayed(ctrl_info);\n\n\tmutex_unlock(&ctrl_info->scan_mutex);\n\n\treturn rc;\n}\n\nstatic void pqi_scan_start(struct Scsi_Host *shost)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = shost_to_hba(shost);\n\n\tpqi_scan_scsi_devices(ctrl_info);\n}\n\n \n\nstatic int pqi_scan_finished(struct Scsi_Host *shost,\n\tunsigned long elapsed_time)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = shost_priv(shost);\n\n\treturn !mutex_is_locked(&ctrl_info->scan_mutex);\n}\n\nstatic inline void pqi_set_encryption_info(struct pqi_encryption_info *encryption_info,\n\tstruct raid_map *raid_map, u64 first_block)\n{\n\tu32 volume_blk_size;\n\n\t \n\tvolume_blk_size = get_unaligned_le32(&raid_map->volume_blk_size);\n\tif (volume_blk_size != 512)\n\t\tfirst_block = (first_block * volume_blk_size) / 512;\n\n\tencryption_info->data_encryption_key_index =\n\t\tget_unaligned_le16(&raid_map->data_encryption_key_index);\n\tencryption_info->encrypt_tweak_lower = lower_32_bits(first_block);\n\tencryption_info->encrypt_tweak_upper = upper_32_bits(first_block);\n}\n\n \n\nstatic bool pqi_aio_raid_level_supported(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev_raid_map_data *rmd)\n{\n\tbool is_supported = true;\n\n\tswitch (rmd->raid_level) {\n\tcase SA_RAID_0:\n\t\tbreak;\n\tcase SA_RAID_1:\n\t\tif (rmd->is_write && (!ctrl_info->enable_r1_writes ||\n\t\t\trmd->data_length > ctrl_info->max_write_raid_1_10_2drive))\n\t\t\tis_supported = false;\n\t\tbreak;\n\tcase SA_RAID_TRIPLE:\n\t\tif (rmd->is_write && (!ctrl_info->enable_r1_writes ||\n\t\t\trmd->data_length > ctrl_info->max_write_raid_1_10_3drive))\n\t\t\tis_supported = false;\n\t\tbreak;\n\tcase SA_RAID_5:\n\t\tif (rmd->is_write && (!ctrl_info->enable_r5_writes ||\n\t\t\trmd->data_length > ctrl_info->max_write_raid_5_6))\n\t\t\tis_supported = false;\n\t\tbreak;\n\tcase SA_RAID_6:\n\t\tif (rmd->is_write && (!ctrl_info->enable_r6_writes ||\n\t\t\trmd->data_length > ctrl_info->max_write_raid_5_6))\n\t\t\tis_supported = false;\n\t\tbreak;\n\tdefault:\n\t\tis_supported = false;\n\t\tbreak;\n\t}\n\n\treturn is_supported;\n}\n\n#define PQI_RAID_BYPASS_INELIGIBLE\t1\n\nstatic int pqi_get_aio_lba_and_block_count(struct scsi_cmnd *scmd,\n\tstruct pqi_scsi_dev_raid_map_data *rmd)\n{\n\t \n\tswitch (scmd->cmnd[0]) {\n\tcase WRITE_6:\n\t\trmd->is_write = true;\n\t\tfallthrough;\n\tcase READ_6:\n\t\trmd->first_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |\n\t\t\t(scmd->cmnd[2] << 8) | scmd->cmnd[3]);\n\t\trmd->block_cnt = (u32)scmd->cmnd[4];\n\t\tif (rmd->block_cnt == 0)\n\t\t\trmd->block_cnt = 256;\n\t\tbreak;\n\tcase WRITE_10:\n\t\trmd->is_write = true;\n\t\tfallthrough;\n\tcase READ_10:\n\t\trmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);\n\t\trmd->block_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);\n\t\tbreak;\n\tcase WRITE_12:\n\t\trmd->is_write = true;\n\t\tfallthrough;\n\tcase READ_12:\n\t\trmd->first_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);\n\t\trmd->block_cnt = get_unaligned_be32(&scmd->cmnd[6]);\n\t\tbreak;\n\tcase WRITE_16:\n\t\trmd->is_write = true;\n\t\tfallthrough;\n\tcase READ_16:\n\t\trmd->first_block = get_unaligned_be64(&scmd->cmnd[2]);\n\t\trmd->block_cnt = get_unaligned_be32(&scmd->cmnd[10]);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\t}\n\n\tput_unaligned_le32(scsi_bufflen(scmd), &rmd->data_length);\n\n\treturn 0;\n}\n\nstatic int pci_get_aio_common_raid_map_values(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev_raid_map_data *rmd, struct raid_map *raid_map)\n{\n#if BITS_PER_LONG == 32\n\tu64 tmpdiv;\n#endif\n\n\trmd->last_block = rmd->first_block + rmd->block_cnt - 1;\n\n\t \n\tif (rmd->last_block >=\n\t\tget_unaligned_le64(&raid_map->volume_blk_cnt) ||\n\t\trmd->last_block < rmd->first_block)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\trmd->data_disks_per_row =\n\t\tget_unaligned_le16(&raid_map->data_disks_per_row);\n\trmd->strip_size = get_unaligned_le16(&raid_map->strip_size);\n\trmd->layout_map_count = get_unaligned_le16(&raid_map->layout_map_count);\n\n\t \n\trmd->blocks_per_row = rmd->data_disks_per_row * rmd->strip_size;\n\tif (rmd->blocks_per_row == 0)  \n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n#if BITS_PER_LONG == 32\n\ttmpdiv = rmd->first_block;\n\tdo_div(tmpdiv, rmd->blocks_per_row);\n\trmd->first_row = tmpdiv;\n\ttmpdiv = rmd->last_block;\n\tdo_div(tmpdiv, rmd->blocks_per_row);\n\trmd->last_row = tmpdiv;\n\trmd->first_row_offset = (u32)(rmd->first_block - (rmd->first_row * rmd->blocks_per_row));\n\trmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row * rmd->blocks_per_row));\n\ttmpdiv = rmd->first_row_offset;\n\tdo_div(tmpdiv, rmd->strip_size);\n\trmd->first_column = tmpdiv;\n\ttmpdiv = rmd->last_row_offset;\n\tdo_div(tmpdiv, rmd->strip_size);\n\trmd->last_column = tmpdiv;\n#else\n\trmd->first_row = rmd->first_block / rmd->blocks_per_row;\n\trmd->last_row = rmd->last_block / rmd->blocks_per_row;\n\trmd->first_row_offset = (u32)(rmd->first_block -\n\t\t(rmd->first_row * rmd->blocks_per_row));\n\trmd->last_row_offset = (u32)(rmd->last_block - (rmd->last_row *\n\t\trmd->blocks_per_row));\n\trmd->first_column = rmd->first_row_offset / rmd->strip_size;\n\trmd->last_column = rmd->last_row_offset / rmd->strip_size;\n#endif\n\n\t \n\tif (rmd->first_row != rmd->last_row ||\n\t\trmd->first_column != rmd->last_column)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\t \n\trmd->total_disks_per_row = rmd->data_disks_per_row +\n\t\tget_unaligned_le16(&raid_map->metadata_disks_per_row);\n\trmd->map_row = ((u32)(rmd->first_row >>\n\t\traid_map->parity_rotation_shift)) %\n\t\tget_unaligned_le16(&raid_map->row_cnt);\n\trmd->map_index = (rmd->map_row * rmd->total_disks_per_row) +\n\t\trmd->first_column;\n\n\treturn 0;\n}\n\nstatic int pqi_calc_aio_r5_or_r6(struct pqi_scsi_dev_raid_map_data *rmd,\n\tstruct raid_map *raid_map)\n{\n#if BITS_PER_LONG == 32\n\tu64 tmpdiv;\n#endif\n\n\tif (rmd->blocks_per_row == 0)  \n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\t \n\t \n\trmd->stripesize = rmd->blocks_per_row * rmd->layout_map_count;\n#if BITS_PER_LONG == 32\n\ttmpdiv = rmd->first_block;\n\trmd->first_group = do_div(tmpdiv, rmd->stripesize);\n\ttmpdiv = rmd->first_group;\n\tdo_div(tmpdiv, rmd->blocks_per_row);\n\trmd->first_group = tmpdiv;\n\ttmpdiv = rmd->last_block;\n\trmd->last_group = do_div(tmpdiv, rmd->stripesize);\n\ttmpdiv = rmd->last_group;\n\tdo_div(tmpdiv, rmd->blocks_per_row);\n\trmd->last_group = tmpdiv;\n#else\n\trmd->first_group = (rmd->first_block % rmd->stripesize) / rmd->blocks_per_row;\n\trmd->last_group = (rmd->last_block % rmd->stripesize) / rmd->blocks_per_row;\n#endif\n\tif (rmd->first_group != rmd->last_group)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\t \n#if BITS_PER_LONG == 32\n\ttmpdiv = rmd->first_block;\n\tdo_div(tmpdiv, rmd->stripesize);\n\trmd->first_row = tmpdiv;\n\trmd->r5or6_first_row = tmpdiv;\n\ttmpdiv = rmd->last_block;\n\tdo_div(tmpdiv, rmd->stripesize);\n\trmd->r5or6_last_row = tmpdiv;\n#else\n\trmd->first_row = rmd->r5or6_first_row =\n\t\trmd->first_block / rmd->stripesize;\n\trmd->r5or6_last_row = rmd->last_block / rmd->stripesize;\n#endif\n\tif (rmd->r5or6_first_row != rmd->r5or6_last_row)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\t \n#if BITS_PER_LONG == 32\n\ttmpdiv = rmd->first_block;\n\trmd->first_row_offset = do_div(tmpdiv, rmd->stripesize);\n\ttmpdiv = rmd->first_row_offset;\n\trmd->first_row_offset = (u32)do_div(tmpdiv, rmd->blocks_per_row);\n\trmd->r5or6_first_row_offset = rmd->first_row_offset;\n\ttmpdiv = rmd->last_block;\n\trmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->stripesize);\n\ttmpdiv = rmd->r5or6_last_row_offset;\n\trmd->r5or6_last_row_offset = do_div(tmpdiv, rmd->blocks_per_row);\n\ttmpdiv = rmd->r5or6_first_row_offset;\n\tdo_div(tmpdiv, rmd->strip_size);\n\trmd->first_column = rmd->r5or6_first_column = tmpdiv;\n\ttmpdiv = rmd->r5or6_last_row_offset;\n\tdo_div(tmpdiv, rmd->strip_size);\n\trmd->r5or6_last_column = tmpdiv;\n#else\n\trmd->first_row_offset = rmd->r5or6_first_row_offset =\n\t\t(u32)((rmd->first_block % rmd->stripesize) %\n\t\trmd->blocks_per_row);\n\n\trmd->r5or6_last_row_offset =\n\t\t(u32)((rmd->last_block % rmd->stripesize) %\n\t\trmd->blocks_per_row);\n\n\trmd->first_column =\n\t\trmd->r5or6_first_row_offset / rmd->strip_size;\n\trmd->r5or6_first_column = rmd->first_column;\n\trmd->r5or6_last_column = rmd->r5or6_last_row_offset / rmd->strip_size;\n#endif\n\tif (rmd->r5or6_first_column != rmd->r5or6_last_column)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\t \n\trmd->map_row =\n\t\t((u32)(rmd->first_row >> raid_map->parity_rotation_shift)) %\n\t\tget_unaligned_le16(&raid_map->row_cnt);\n\n\trmd->map_index = (rmd->first_group *\n\t\t(get_unaligned_le16(&raid_map->row_cnt) *\n\t\trmd->total_disks_per_row)) +\n\t\t(rmd->map_row * rmd->total_disks_per_row) + rmd->first_column;\n\n\tif (rmd->is_write) {\n\t\tu32 index;\n\n\t\t \n\t\tindex = DIV_ROUND_UP(rmd->map_index + 1, rmd->total_disks_per_row);\n\t\tindex *= rmd->total_disks_per_row;\n\t\tindex -= get_unaligned_le16(&raid_map->metadata_disks_per_row);\n\n\t\trmd->p_parity_it_nexus = raid_map->disk_data[index].aio_handle;\n\t\tif (rmd->raid_level == SA_RAID_6) {\n\t\t\trmd->q_parity_it_nexus = raid_map->disk_data[index + 1].aio_handle;\n\t\t\trmd->xor_mult = raid_map->disk_data[rmd->map_index].xor_mult[1];\n\t\t}\n#if BITS_PER_LONG == 32\n\t\ttmpdiv = rmd->first_block;\n\t\tdo_div(tmpdiv, rmd->blocks_per_row);\n\t\trmd->row = tmpdiv;\n#else\n\t\trmd->row = rmd->first_block / rmd->blocks_per_row;\n#endif\n\t}\n\n\treturn 0;\n}\n\nstatic void pqi_set_aio_cdb(struct pqi_scsi_dev_raid_map_data *rmd)\n{\n\t \n\tif (rmd->disk_block > 0xffffffff) {\n\t\trmd->cdb[0] = rmd->is_write ? WRITE_16 : READ_16;\n\t\trmd->cdb[1] = 0;\n\t\tput_unaligned_be64(rmd->disk_block, &rmd->cdb[2]);\n\t\tput_unaligned_be32(rmd->disk_block_cnt, &rmd->cdb[10]);\n\t\trmd->cdb[14] = 0;\n\t\trmd->cdb[15] = 0;\n\t\trmd->cdb_length = 16;\n\t} else {\n\t\trmd->cdb[0] = rmd->is_write ? WRITE_10 : READ_10;\n\t\trmd->cdb[1] = 0;\n\t\tput_unaligned_be32((u32)rmd->disk_block, &rmd->cdb[2]);\n\t\trmd->cdb[6] = 0;\n\t\tput_unaligned_be16((u16)rmd->disk_block_cnt, &rmd->cdb[7]);\n\t\trmd->cdb[9] = 0;\n\t\trmd->cdb_length = 10;\n\t}\n}\n\nstatic void pqi_calc_aio_r1_nexus(struct raid_map *raid_map,\n\tstruct pqi_scsi_dev_raid_map_data *rmd)\n{\n\tu32 index;\n\tu32 group;\n\n\tgroup = rmd->map_index / rmd->data_disks_per_row;\n\n\tindex = rmd->map_index - (group * rmd->data_disks_per_row);\n\trmd->it_nexus[0] = raid_map->disk_data[index].aio_handle;\n\tindex += rmd->data_disks_per_row;\n\trmd->it_nexus[1] = raid_map->disk_data[index].aio_handle;\n\tif (rmd->layout_map_count > 2) {\n\t\tindex += rmd->data_disks_per_row;\n\t\trmd->it_nexus[2] = raid_map->disk_data[index].aio_handle;\n\t}\n\n\trmd->num_it_nexus_entries = rmd->layout_map_count;\n}\n\nstatic int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\n\tstruct pqi_queue_group *queue_group)\n{\n\tint rc;\n\tstruct raid_map *raid_map;\n\tu32 group;\n\tu32 next_bypass_group;\n\tstruct pqi_encryption_info *encryption_info_ptr;\n\tstruct pqi_encryption_info encryption_info;\n\tstruct pqi_scsi_dev_raid_map_data rmd = { 0 };\n\n\trc = pqi_get_aio_lba_and_block_count(scmd, &rmd);\n\tif (rc)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\trmd.raid_level = device->raid_level;\n\n\tif (!pqi_aio_raid_level_supported(ctrl_info, &rmd))\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\tif (unlikely(rmd.block_cnt == 0))\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\traid_map = device->raid_map;\n\n\trc = pci_get_aio_common_raid_map_values(ctrl_info, &rmd, raid_map);\n\tif (rc)\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\tif (device->raid_level == SA_RAID_1 ||\n\t\tdevice->raid_level == SA_RAID_TRIPLE) {\n\t\tif (rmd.is_write) {\n\t\t\tpqi_calc_aio_r1_nexus(raid_map, &rmd);\n\t\t} else {\n\t\t\tgroup = device->next_bypass_group[rmd.map_index];\n\t\t\tnext_bypass_group = group + 1;\n\t\t\tif (next_bypass_group >= rmd.layout_map_count)\n\t\t\t\tnext_bypass_group = 0;\n\t\t\tdevice->next_bypass_group[rmd.map_index] = next_bypass_group;\n\t\t\trmd.map_index += group * rmd.data_disks_per_row;\n\t\t}\n\t} else if ((device->raid_level == SA_RAID_5 ||\n\t\tdevice->raid_level == SA_RAID_6) &&\n\t\t(rmd.layout_map_count > 1 || rmd.is_write)) {\n\t\trc = pqi_calc_aio_r5_or_r6(&rmd, raid_map);\n\t\tif (rc)\n\t\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\t}\n\n\tif (unlikely(rmd.map_index >= RAID_MAP_MAX_ENTRIES))\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\trmd.aio_handle = raid_map->disk_data[rmd.map_index].aio_handle;\n\trmd.disk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +\n\t\trmd.first_row * rmd.strip_size +\n\t\t(rmd.first_row_offset - rmd.first_column * rmd.strip_size);\n\trmd.disk_block_cnt = rmd.block_cnt;\n\n\t \n\tif (raid_map->phys_blk_shift) {\n\t\trmd.disk_block <<= raid_map->phys_blk_shift;\n\t\trmd.disk_block_cnt <<= raid_map->phys_blk_shift;\n\t}\n\n\tif (unlikely(rmd.disk_block_cnt > 0xffff))\n\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\n\tpqi_set_aio_cdb(&rmd);\n\n\tif (get_unaligned_le16(&raid_map->flags) & RAID_MAP_ENCRYPTION_ENABLED) {\n\t\tif (rmd.data_length > device->max_transfer_encrypted)\n\t\t\treturn PQI_RAID_BYPASS_INELIGIBLE;\n\t\tpqi_set_encryption_info(&encryption_info, raid_map, rmd.first_block);\n\t\tencryption_info_ptr = &encryption_info;\n\t} else {\n\t\tencryption_info_ptr = NULL;\n\t}\n\n\tif (rmd.is_write) {\n\t\tswitch (device->raid_level) {\n\t\tcase SA_RAID_1:\n\t\tcase SA_RAID_TRIPLE:\n\t\t\treturn pqi_aio_submit_r1_write_io(ctrl_info, scmd, queue_group,\n\t\t\t\tencryption_info_ptr, device, &rmd);\n\t\tcase SA_RAID_5:\n\t\tcase SA_RAID_6:\n\t\t\treturn pqi_aio_submit_r56_write_io(ctrl_info, scmd, queue_group,\n\t\t\t\tencryption_info_ptr, device, &rmd);\n\t\t}\n\t}\n\n\treturn pqi_aio_submit_io(ctrl_info, scmd, rmd.aio_handle,\n\t\trmd.cdb, rmd.cdb_length, queue_group,\n\t\tencryption_info_ptr, true, false);\n}\n\n#define PQI_STATUS_IDLE\t\t0x0\n\n#define PQI_CREATE_ADMIN_QUEUE_PAIR\t1\n#define PQI_DELETE_ADMIN_QUEUE_PAIR\t2\n\n#define PQI_DEVICE_STATE_POWER_ON_AND_RESET\t\t0x0\n#define PQI_DEVICE_STATE_STATUS_AVAILABLE\t\t0x1\n#define PQI_DEVICE_STATE_ALL_REGISTERS_READY\t\t0x2\n#define PQI_DEVICE_STATE_ADMIN_QUEUE_PAIR_READY\t\t0x3\n#define PQI_DEVICE_STATE_ERROR\t\t\t\t0x4\n\n#define PQI_MODE_READY_TIMEOUT_SECS\t\t30\n#define PQI_MODE_READY_POLL_INTERVAL_MSECS\t1\n\nstatic int pqi_wait_for_pqi_mode_ready(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct pqi_device_registers __iomem *pqi_registers;\n\tunsigned long timeout;\n\tu64 signature;\n\tu8 status;\n\n\tpqi_registers = ctrl_info->pqi_registers;\n\ttimeout = (PQI_MODE_READY_TIMEOUT_SECS * HZ) + jiffies;\n\n\twhile (1) {\n\t\tsignature = readq(&pqi_registers->signature);\n\t\tif (memcmp(&signature, PQI_DEVICE_SIGNATURE,\n\t\t\tsizeof(signature)) == 0)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"timed out waiting for PQI signature\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\n\t}\n\n\twhile (1) {\n\t\tstatus = readb(&pqi_registers->function_and_status_code);\n\t\tif (status == PQI_STATUS_IDLE)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"timed out waiting for PQI IDLE\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\n\t}\n\n\twhile (1) {\n\t\tif (readl(&pqi_registers->device_status) ==\n\t\t\tPQI_DEVICE_STATE_ALL_REGISTERS_READY)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"timed out waiting for PQI all registers ready\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\n\t}\n\n\treturn 0;\n}\n\nstatic inline void pqi_aio_path_disabled(struct pqi_io_request *io_request)\n{\n\tstruct pqi_scsi_dev *device;\n\n\tdevice = io_request->scmd->device->hostdata;\n\tdevice->raid_bypass_enabled = false;\n\tdevice->aio_enabled = false;\n}\n\nstatic inline void pqi_take_device_offline(struct scsi_device *sdev, char *path)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_scsi_dev *device;\n\n\tdevice = sdev->hostdata;\n\tif (device->device_offline)\n\t\treturn;\n\n\tdevice->device_offline = true;\n\tctrl_info = shost_to_hba(sdev->host);\n\tpqi_schedule_rescan_worker(ctrl_info);\n\tdev_err(&ctrl_info->pci_dev->dev, \"re-scanning %s scsi %d:%d:%d:%d\\n\",\n\t\tpath, ctrl_info->scsi_host->host_no, device->bus,\n\t\tdevice->target, device->lun);\n}\n\nstatic void pqi_process_raid_io_error(struct pqi_io_request *io_request)\n{\n\tu8 scsi_status;\n\tu8 host_byte;\n\tstruct scsi_cmnd *scmd;\n\tstruct pqi_raid_error_info *error_info;\n\tsize_t sense_data_length;\n\tint residual_count;\n\tint xfer_count;\n\tstruct scsi_sense_hdr sshdr;\n\n\tscmd = io_request->scmd;\n\tif (!scmd)\n\t\treturn;\n\n\terror_info = io_request->error_info;\n\tscsi_status = error_info->status;\n\thost_byte = DID_OK;\n\n\tswitch (error_info->data_out_result) {\n\tcase PQI_DATA_IN_OUT_GOOD:\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_UNDERFLOW:\n\t\txfer_count =\n\t\t\tget_unaligned_le32(&error_info->data_out_transferred);\n\t\tresidual_count = scsi_bufflen(scmd) - xfer_count;\n\t\tscsi_set_resid(scmd, residual_count);\n\t\tif (xfer_count < scmd->underflow)\n\t\t\thost_byte = DID_SOFT_ERROR;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_UNSOLICITED_ABORT:\n\tcase PQI_DATA_IN_OUT_ABORTED:\n\t\thost_byte = DID_ABORT;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_TIMEOUT:\n\t\thost_byte = DID_TIME_OUT;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW:\n\tcase PQI_DATA_IN_OUT_PROTOCOL_ERROR:\n\tcase PQI_DATA_IN_OUT_BUFFER_ERROR:\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_DESCRIPTOR_AREA:\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_BRIDGE:\n\tcase PQI_DATA_IN_OUT_ERROR:\n\tcase PQI_DATA_IN_OUT_HARDWARE_ERROR:\n\tcase PQI_DATA_IN_OUT_PCIE_FABRIC_ERROR:\n\tcase PQI_DATA_IN_OUT_PCIE_COMPLETION_TIMEOUT:\n\tcase PQI_DATA_IN_OUT_PCIE_COMPLETER_ABORT_RECEIVED:\n\tcase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST_RECEIVED:\n\tcase PQI_DATA_IN_OUT_PCIE_ECRC_CHECK_FAILED:\n\tcase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST:\n\tcase PQI_DATA_IN_OUT_PCIE_ACS_VIOLATION:\n\tcase PQI_DATA_IN_OUT_PCIE_TLP_PREFIX_BLOCKED:\n\tcase PQI_DATA_IN_OUT_PCIE_POISONED_MEMORY_READ:\n\tdefault:\n\t\thost_byte = DID_ERROR;\n\t\tbreak;\n\t}\n\n\tsense_data_length = get_unaligned_le16(&error_info->sense_data_length);\n\tif (sense_data_length == 0)\n\t\tsense_data_length =\n\t\t\tget_unaligned_le16(&error_info->response_data_length);\n\tif (sense_data_length) {\n\t\tif (sense_data_length > sizeof(error_info->data))\n\t\t\tsense_data_length = sizeof(error_info->data);\n\n\t\tif (scsi_status == SAM_STAT_CHECK_CONDITION &&\n\t\t\tscsi_normalize_sense(error_info->data,\n\t\t\t\tsense_data_length, &sshdr) &&\n\t\t\t\tsshdr.sense_key == HARDWARE_ERROR &&\n\t\t\t\tsshdr.asc == 0x3e) {\n\t\t\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(scmd->device->host);\n\t\t\tstruct pqi_scsi_dev *device = scmd->device->hostdata;\n\n\t\t\tswitch (sshdr.ascq) {\n\t\t\tcase 0x1:  \n\t\t\t\tif (printk_ratelimit())\n\t\t\t\t\tscmd_printk(KERN_ERR, scmd, \"received 'logical unit failure' from controller for scsi %d:%d:%d:%d\\n\",\n\t\t\t\t\t\tctrl_info->scsi_host->host_no, device->bus, device->target, device->lun);\n\t\t\t\tpqi_take_device_offline(scmd->device, \"RAID\");\n\t\t\t\thost_byte = DID_NO_CONNECT;\n\t\t\t\tbreak;\n\n\t\t\tdefault:  \n\t\toq_ci = (oq_ci + 1) % ctrl_info->num_elements_per_oq;\n\t}\n\n\tif (num_responses) {\n\t\tqueue_group->oq_ci_copy = oq_ci;\n\t\twritel(oq_ci, queue_group->oq_ci);\n\t}\n\n\treturn num_responses;\n}\n\nstatic inline unsigned int pqi_num_elements_free(unsigned int pi,\n\tunsigned int ci, unsigned int elements_in_queue)\n{\n\tunsigned int num_elements_used;\n\n\tif (pi >= ci)\n\t\tnum_elements_used = pi - ci;\n\telse\n\t\tnum_elements_used = elements_in_queue - ci + pi;\n\n\treturn elements_in_queue - num_elements_used - 1;\n}\n\nstatic void pqi_send_event_ack(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_event_acknowledge_request *iu, size_t iu_length)\n{\n\tpqi_index_t iq_pi;\n\tpqi_index_t iq_ci;\n\tunsigned long flags;\n\tvoid *next_element;\n\tstruct pqi_queue_group *queue_group;\n\n\tqueue_group = &ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP];\n\tput_unaligned_le16(queue_group->oq_id, &iu->header.response_queue_id);\n\n\twhile (1) {\n\t\tspin_lock_irqsave(&queue_group->submit_lock[RAID_PATH], flags);\n\n\t\tiq_pi = queue_group->iq_pi_copy[RAID_PATH];\n\t\tiq_ci = readl(queue_group->iq_ci[RAID_PATH]);\n\n\t\tif (pqi_num_elements_free(iq_pi, iq_ci,\n\t\t\tctrl_info->num_elements_per_iq))\n\t\t\tbreak;\n\n\t\tspin_unlock_irqrestore(\n\t\t\t&queue_group->submit_lock[RAID_PATH], flags);\n\n\t\tif (pqi_ctrl_offline(ctrl_info))\n\t\t\treturn;\n\t}\n\n\tnext_element = queue_group->iq_element_array[RAID_PATH] +\n\t\t(iq_pi * PQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\tmemcpy(next_element, iu, iu_length);\n\n\tiq_pi = (iq_pi + 1) % ctrl_info->num_elements_per_iq;\n\tqueue_group->iq_pi_copy[RAID_PATH] = iq_pi;\n\n\t \n\twritel(iq_pi, queue_group->iq_pi[RAID_PATH]);\n\n\tspin_unlock_irqrestore(&queue_group->submit_lock[RAID_PATH], flags);\n}\n\nstatic void pqi_acknowledge_event(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_event *event)\n{\n\tstruct pqi_event_acknowledge_request request;\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_ACKNOWLEDGE_VENDOR_EVENT;\n\tput_unaligned_le16(sizeof(request) - PQI_REQUEST_HEADER_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.event_type = event->event_type;\n\tput_unaligned_le16(event->event_id, &request.event_id);\n\tput_unaligned_le32(event->additional_event_id, &request.additional_event_id);\n\n\tpqi_send_event_ack(ctrl_info, &request, sizeof(request));\n}\n\n#define PQI_SOFT_RESET_STATUS_TIMEOUT_SECS\t\t30\n#define PQI_SOFT_RESET_STATUS_POLL_INTERVAL_SECS\t1\n\nstatic enum pqi_soft_reset_status pqi_poll_for_soft_reset_status(\n\tstruct pqi_ctrl_info *ctrl_info)\n{\n\tu8 status;\n\tunsigned long timeout;\n\n\ttimeout = (PQI_SOFT_RESET_STATUS_TIMEOUT_SECS * HZ) + jiffies;\n\n\twhile (1) {\n\t\tstatus = pqi_read_soft_reset_status(ctrl_info);\n\t\tif (status & PQI_SOFT_RESET_INITIATE)\n\t\t\treturn RESET_INITIATE_DRIVER;\n\n\t\tif (status & PQI_SOFT_RESET_ABORT)\n\t\t\treturn RESET_ABORT;\n\n\t\tif (!sis_is_firmware_running(ctrl_info))\n\t\t\treturn RESET_NORESPONSE;\n\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"timed out waiting for soft reset status\\n\");\n\t\t\treturn RESET_TIMEDOUT;\n\t\t}\n\n\t\tssleep(PQI_SOFT_RESET_STATUS_POLL_INTERVAL_SECS);\n\t}\n}\n\nstatic void pqi_process_soft_reset(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tunsigned int delay_secs;\n\tenum pqi_soft_reset_status reset_status;\n\n\tif (ctrl_info->soft_reset_handshake_supported)\n\t\treset_status = pqi_poll_for_soft_reset_status(ctrl_info);\n\telse\n\t\treset_status = RESET_INITIATE_FIRMWARE;\n\n\tdelay_secs = PQI_POST_RESET_DELAY_SECS;\n\n\tswitch (reset_status) {\n\tcase RESET_TIMEDOUT:\n\t\tdelay_secs = PQI_POST_OFA_RESET_DELAY_UPON_TIMEOUT_SECS;\n\t\tfallthrough;\n\tcase RESET_INITIATE_DRIVER:\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"Online Firmware Activation: resetting controller\\n\");\n\t\tsis_soft_reset(ctrl_info);\n\t\tfallthrough;\n\tcase RESET_INITIATE_FIRMWARE:\n\t\tctrl_info->pqi_mode_enabled = false;\n\t\tpqi_save_ctrl_mode(ctrl_info, SIS_MODE);\n\t\trc = pqi_ofa_ctrl_restart(ctrl_info, delay_secs);\n\t\tpqi_ofa_free_host_buffer(ctrl_info);\n\t\tpqi_ctrl_ofa_done(ctrl_info);\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"Online Firmware Activation: %s\\n\",\n\t\t\t\trc == 0 ? \"SUCCESS\" : \"FAILED\");\n\t\tbreak;\n\tcase RESET_ABORT:\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"Online Firmware Activation ABORTED\\n\");\n\t\tif (ctrl_info->soft_reset_handshake_supported)\n\t\t\tpqi_clear_soft_reset_status(ctrl_info);\n\t\tpqi_ofa_free_host_buffer(ctrl_info);\n\t\tpqi_ctrl_ofa_done(ctrl_info);\n\t\tpqi_ofa_ctrl_unquiesce(ctrl_info);\n\t\tbreak;\n\tcase RESET_NORESPONSE:\n\t\tfallthrough;\n\tdefault:\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"unexpected Online Firmware Activation reset status: 0x%x\\n\",\n\t\t\treset_status);\n\t\tpqi_ofa_free_host_buffer(ctrl_info);\n\t\tpqi_ctrl_ofa_done(ctrl_info);\n\t\tpqi_ofa_ctrl_unquiesce(ctrl_info);\n\t\tpqi_take_ctrl_offline(ctrl_info, PQI_OFA_RESPONSE_TIMEOUT);\n\t\tbreak;\n\t}\n}\n\nstatic void pqi_ofa_memory_alloc_worker(struct work_struct *work)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = container_of(work, struct pqi_ctrl_info, ofa_memory_alloc_work);\n\n\tpqi_ctrl_ofa_start(ctrl_info);\n\tpqi_ofa_setup_host_buffer(ctrl_info);\n\tpqi_ofa_host_memory_update(ctrl_info);\n}\n\nstatic void pqi_ofa_quiesce_worker(struct work_struct *work)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_event *event;\n\n\tctrl_info = container_of(work, struct pqi_ctrl_info, ofa_quiesce_work);\n\n\tevent = &ctrl_info->events[pqi_event_type_to_event_index(PQI_EVENT_TYPE_OFA)];\n\n\tpqi_ofa_ctrl_quiesce(ctrl_info);\n\tpqi_acknowledge_event(ctrl_info, event);\n\tpqi_process_soft_reset(ctrl_info);\n}\n\nstatic bool pqi_ofa_process_event(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_event *event)\n{\n\tbool ack_event;\n\n\tack_event = true;\n\n\tswitch (event->event_id) {\n\tcase PQI_EVENT_OFA_MEMORY_ALLOCATION:\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\"received Online Firmware Activation memory allocation request\\n\");\n\t\tschedule_work(&ctrl_info->ofa_memory_alloc_work);\n\t\tbreak;\n\tcase PQI_EVENT_OFA_QUIESCE:\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\"received Online Firmware Activation quiesce request\\n\");\n\t\tschedule_work(&ctrl_info->ofa_quiesce_work);\n\t\tack_event = false;\n\t\tbreak;\n\tcase PQI_EVENT_OFA_CANCELED:\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\"received Online Firmware Activation cancel request: reason: %u\\n\",\n\t\t\tctrl_info->ofa_cancel_reason);\n\t\tpqi_ofa_free_host_buffer(ctrl_info);\n\t\tpqi_ctrl_ofa_done(ctrl_info);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"received unknown Online Firmware Activation request: event ID: %u\\n\",\n\t\t\tevent->event_id);\n\t\tbreak;\n\t}\n\n\treturn ack_event;\n}\n\nstatic void pqi_disable_raid_bypass(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned long flags;\n\tstruct pqi_scsi_dev *device;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tlist_for_each_entry(device, &ctrl_info->scsi_device_list, scsi_device_list_entry)\n\t\tif (device->raid_bypass_enabled)\n\t\t\tdevice->raid_bypass_enabled = false;\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n}\n\nstatic void pqi_event_worker(struct work_struct *work)\n{\n\tunsigned int i;\n\tbool rescan_needed;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_event *event;\n\tbool ack_event;\n\n\tctrl_info = container_of(work, struct pqi_ctrl_info, event_work);\n\n\tpqi_ctrl_busy(ctrl_info);\n\tpqi_wait_if_ctrl_blocked(ctrl_info);\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\tgoto out;\n\n\trescan_needed = false;\n\tevent = ctrl_info->events;\n\tfor (i = 0; i < PQI_NUM_SUPPORTED_EVENTS; i++) {\n\t\tif (event->pending) {\n\t\t\tevent->pending = false;\n\t\t\tif (event->event_type == PQI_EVENT_TYPE_OFA) {\n\t\t\t\tack_event = pqi_ofa_process_event(ctrl_info, event);\n\t\t\t} else {\n\t\t\t\tack_event = true;\n\t\t\t\trescan_needed = true;\n\t\t\t\tif (event->event_type == PQI_EVENT_TYPE_LOGICAL_DEVICE)\n\t\t\t\t\tctrl_info->logical_volume_rescan_needed = true;\n\t\t\t\telse if (event->event_type == PQI_EVENT_TYPE_AIO_STATE_CHANGE)\n\t\t\t\t\tpqi_disable_raid_bypass(ctrl_info);\n\t\t\t}\n\t\t\tif (ack_event)\n\t\t\t\tpqi_acknowledge_event(ctrl_info, event);\n\t\t}\n\t\tevent++;\n\t}\n\n#define PQI_RESCAN_WORK_FOR_EVENT_DELAY\t\t(5 * HZ)\n\n\tif (rescan_needed)\n\t\tpqi_schedule_rescan_worker_with_delay(ctrl_info,\n\t\t\tPQI_RESCAN_WORK_FOR_EVENT_DELAY);\n\nout:\n\tpqi_ctrl_unbusy(ctrl_info);\n}\n\n#define PQI_HEARTBEAT_TIMER_INTERVAL\t(10 * HZ)\n\nstatic void pqi_heartbeat_timer_handler(struct timer_list *t)\n{\n\tint num_interrupts;\n\tu32 heartbeat_count;\n\tstruct pqi_ctrl_info *ctrl_info = from_timer(ctrl_info, t, heartbeat_timer);\n\n\tpqi_check_ctrl_health(ctrl_info);\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn;\n\n\tnum_interrupts = atomic_read(&ctrl_info->num_interrupts);\n\theartbeat_count = pqi_read_heartbeat_counter(ctrl_info);\n\n\tif (num_interrupts == ctrl_info->previous_num_interrupts) {\n\t\tif (heartbeat_count == ctrl_info->previous_heartbeat_count) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"no heartbeat detected - last heartbeat count: %u\\n\",\n\t\t\t\theartbeat_count);\n\t\t\tpqi_take_ctrl_offline(ctrl_info, PQI_NO_HEARTBEAT);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tctrl_info->previous_num_interrupts = num_interrupts;\n\t}\n\n\tctrl_info->previous_heartbeat_count = heartbeat_count;\n\tmod_timer(&ctrl_info->heartbeat_timer,\n\t\tjiffies + PQI_HEARTBEAT_TIMER_INTERVAL);\n}\n\nstatic void pqi_start_heartbeat_timer(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (!ctrl_info->heartbeat_counter)\n\t\treturn;\n\n\tctrl_info->previous_num_interrupts =\n\t\tatomic_read(&ctrl_info->num_interrupts);\n\tctrl_info->previous_heartbeat_count =\n\t\tpqi_read_heartbeat_counter(ctrl_info);\n\n\tctrl_info->heartbeat_timer.expires =\n\t\tjiffies + PQI_HEARTBEAT_TIMER_INTERVAL;\n\tadd_timer(&ctrl_info->heartbeat_timer);\n}\n\nstatic inline void pqi_stop_heartbeat_timer(struct pqi_ctrl_info *ctrl_info)\n{\n\tdel_timer_sync(&ctrl_info->heartbeat_timer);\n}\n\nstatic void pqi_ofa_capture_event_payload(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_event *event, struct pqi_event_response *response)\n{\n\tswitch (event->event_id) {\n\tcase PQI_EVENT_OFA_MEMORY_ALLOCATION:\n\t\tctrl_info->ofa_bytes_requested =\n\t\t\tget_unaligned_le32(&response->data.ofa_memory_allocation.bytes_requested);\n\t\tbreak;\n\tcase PQI_EVENT_OFA_CANCELED:\n\t\tctrl_info->ofa_cancel_reason =\n\t\t\tget_unaligned_le16(&response->data.ofa_cancelled.reason);\n\t\tbreak;\n\t}\n}\n\nstatic int pqi_process_event_intr(struct pqi_ctrl_info *ctrl_info)\n{\n\tint num_events;\n\tpqi_index_t oq_pi;\n\tpqi_index_t oq_ci;\n\tstruct pqi_event_queue *event_queue;\n\tstruct pqi_event_response *response;\n\tstruct pqi_event *event;\n\tint event_index;\n\n\tevent_queue = &ctrl_info->event_queue;\n\tnum_events = 0;\n\toq_ci = event_queue->oq_ci_copy;\n\n\twhile (1) {\n\t\toq_pi = readl(event_queue->oq_pi);\n\t\tif (oq_pi >= PQI_NUM_EVENT_QUEUE_ELEMENTS) {\n\t\t\tpqi_invalid_response(ctrl_info, PQI_EVENT_PI_OUT_OF_RANGE);\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"event interrupt: producer index (%u) out of range (0-%u): consumer index: %u\\n\",\n\t\t\t\toq_pi, PQI_NUM_EVENT_QUEUE_ELEMENTS - 1, oq_ci);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (oq_pi == oq_ci)\n\t\t\tbreak;\n\n\t\tnum_events++;\n\t\tresponse = event_queue->oq_element_array + (oq_ci * PQI_EVENT_OQ_ELEMENT_LENGTH);\n\n\t\tevent_index = pqi_event_type_to_event_index(response->event_type);\n\n\t\tif (event_index >= 0 && response->request_acknowledge) {\n\t\t\tevent = &ctrl_info->events[event_index];\n\t\t\tevent->pending = true;\n\t\t\tevent->event_type = response->event_type;\n\t\t\tevent->event_id = get_unaligned_le16(&response->event_id);\n\t\t\tevent->additional_event_id =\n\t\t\t\tget_unaligned_le32(&response->additional_event_id);\n\t\t\tif (event->event_type == PQI_EVENT_TYPE_OFA)\n\t\t\t\tpqi_ofa_capture_event_payload(ctrl_info, event, response);\n\t\t}\n\n\t\toq_ci = (oq_ci + 1) % PQI_NUM_EVENT_QUEUE_ELEMENTS;\n\t}\n\n\tif (num_events) {\n\t\tevent_queue->oq_ci_copy = oq_ci;\n\t\twritel(oq_ci, event_queue->oq_ci);\n\t\tschedule_work(&ctrl_info->event_work);\n\t}\n\n\treturn num_events;\n}\n\n#define PQI_LEGACY_INTX_MASK\t0x1\n\nstatic inline void pqi_configure_legacy_intx(struct pqi_ctrl_info *ctrl_info, bool enable_intx)\n{\n\tu32 intx_mask;\n\tstruct pqi_device_registers __iomem *pqi_registers;\n\tvolatile void __iomem *register_addr;\n\n\tpqi_registers = ctrl_info->pqi_registers;\n\n\tif (enable_intx)\n\t\tregister_addr = &pqi_registers->legacy_intx_mask_clear;\n\telse\n\t\tregister_addr = &pqi_registers->legacy_intx_mask_set;\n\n\tintx_mask = readl(register_addr);\n\tintx_mask |= PQI_LEGACY_INTX_MASK;\n\twritel(intx_mask, register_addr);\n}\n\nstatic void pqi_change_irq_mode(struct pqi_ctrl_info *ctrl_info,\n\tenum pqi_irq_mode new_mode)\n{\n\tswitch (ctrl_info->irq_mode) {\n\tcase IRQ_MODE_MSIX:\n\t\tswitch (new_mode) {\n\t\tcase IRQ_MODE_MSIX:\n\t\t\tbreak;\n\t\tcase IRQ_MODE_INTX:\n\t\t\tpqi_configure_legacy_intx(ctrl_info, true);\n\t\t\tsis_enable_intx(ctrl_info);\n\t\t\tbreak;\n\t\tcase IRQ_MODE_NONE:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase IRQ_MODE_INTX:\n\t\tswitch (new_mode) {\n\t\tcase IRQ_MODE_MSIX:\n\t\t\tpqi_configure_legacy_intx(ctrl_info, false);\n\t\t\tsis_enable_msix(ctrl_info);\n\t\t\tbreak;\n\t\tcase IRQ_MODE_INTX:\n\t\t\tbreak;\n\t\tcase IRQ_MODE_NONE:\n\t\t\tpqi_configure_legacy_intx(ctrl_info, false);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase IRQ_MODE_NONE:\n\t\tswitch (new_mode) {\n\t\tcase IRQ_MODE_MSIX:\n\t\t\tsis_enable_msix(ctrl_info);\n\t\t\tbreak;\n\t\tcase IRQ_MODE_INTX:\n\t\t\tpqi_configure_legacy_intx(ctrl_info, true);\n\t\t\tsis_enable_intx(ctrl_info);\n\t\t\tbreak;\n\t\tcase IRQ_MODE_NONE:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tctrl_info->irq_mode = new_mode;\n}\n\n#define PQI_LEGACY_INTX_PENDING\t\t0x1\n\nstatic inline bool pqi_is_valid_irq(struct pqi_ctrl_info *ctrl_info)\n{\n\tbool valid_irq;\n\tu32 intx_status;\n\n\tswitch (ctrl_info->irq_mode) {\n\tcase IRQ_MODE_MSIX:\n\t\tvalid_irq = true;\n\t\tbreak;\n\tcase IRQ_MODE_INTX:\n\t\tintx_status = readl(&ctrl_info->pqi_registers->legacy_intx_status);\n\t\tif (intx_status & PQI_LEGACY_INTX_PENDING)\n\t\t\tvalid_irq = true;\n\t\telse\n\t\t\tvalid_irq = false;\n\t\tbreak;\n\tcase IRQ_MODE_NONE:\n\tdefault:\n\t\tvalid_irq = false;\n\t\tbreak;\n\t}\n\n\treturn valid_irq;\n}\n\nstatic irqreturn_t pqi_irq_handler(int irq, void *data)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_queue_group *queue_group;\n\tint num_io_responses_handled;\n\tint num_events_handled;\n\n\tqueue_group = data;\n\tctrl_info = queue_group->ctrl_info;\n\n\tif (!pqi_is_valid_irq(ctrl_info))\n\t\treturn IRQ_NONE;\n\n\tnum_io_responses_handled = pqi_process_io_intr(ctrl_info, queue_group);\n\tif (num_io_responses_handled < 0)\n\t\tgoto out;\n\n\tif (irq == ctrl_info->event_irq) {\n\t\tnum_events_handled = pqi_process_event_intr(ctrl_info);\n\t\tif (num_events_handled < 0)\n\t\t\tgoto out;\n\t} else {\n\t\tnum_events_handled = 0;\n\t}\n\n\tif (num_io_responses_handled + num_events_handled > 0)\n\t\tatomic_inc(&ctrl_info->num_interrupts);\n\n\tpqi_start_io(ctrl_info, queue_group, RAID_PATH, NULL);\n\tpqi_start_io(ctrl_info, queue_group, AIO_PATH, NULL);\n\nout:\n\treturn IRQ_HANDLED;\n}\n\nstatic int pqi_request_irqs(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct pci_dev *pci_dev = ctrl_info->pci_dev;\n\tint i;\n\tint rc;\n\n\tctrl_info->event_irq = pci_irq_vector(pci_dev, 0);\n\n\tfor (i = 0; i < ctrl_info->num_msix_vectors_enabled; i++) {\n\t\trc = request_irq(pci_irq_vector(pci_dev, i), pqi_irq_handler, 0,\n\t\t\tDRIVER_NAME_SHORT, &ctrl_info->queue_groups[i]);\n\t\tif (rc) {\n\t\t\tdev_err(&pci_dev->dev,\n\t\t\t\t\"irq %u init failed with error %d\\n\",\n\t\t\t\tpci_irq_vector(pci_dev, i), rc);\n\t\t\treturn rc;\n\t\t}\n\t\tctrl_info->num_msix_vectors_initialized++;\n\t}\n\n\treturn 0;\n}\n\nstatic void pqi_free_irqs(struct pqi_ctrl_info *ctrl_info)\n{\n\tint i;\n\n\tfor (i = 0; i < ctrl_info->num_msix_vectors_initialized; i++)\n\t\tfree_irq(pci_irq_vector(ctrl_info->pci_dev, i),\n\t\t\t&ctrl_info->queue_groups[i]);\n\n\tctrl_info->num_msix_vectors_initialized = 0;\n}\n\nstatic int pqi_enable_msix_interrupts(struct pqi_ctrl_info *ctrl_info)\n{\n\tint num_vectors_enabled;\n\tunsigned int flags = PCI_IRQ_MSIX;\n\n\tif (!pqi_disable_managed_interrupts)\n\t\tflags |= PCI_IRQ_AFFINITY;\n\n\tnum_vectors_enabled = pci_alloc_irq_vectors(ctrl_info->pci_dev,\n\t\t\tPQI_MIN_MSIX_VECTORS, ctrl_info->num_queue_groups,\n\t\t\tflags);\n\tif (num_vectors_enabled < 0) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"MSI-X init failed with error %d\\n\",\n\t\t\tnum_vectors_enabled);\n\t\treturn num_vectors_enabled;\n\t}\n\n\tctrl_info->num_msix_vectors_enabled = num_vectors_enabled;\n\tctrl_info->irq_mode = IRQ_MODE_MSIX;\n\treturn 0;\n}\n\nstatic void pqi_disable_msix_interrupts(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (ctrl_info->num_msix_vectors_enabled) {\n\t\tpci_free_irq_vectors(ctrl_info->pci_dev);\n\t\tctrl_info->num_msix_vectors_enabled = 0;\n\t}\n}\n\nstatic int pqi_alloc_operational_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tsize_t alloc_length;\n\tsize_t element_array_length_per_iq;\n\tsize_t element_array_length_per_oq;\n\tvoid *element_array;\n\tvoid __iomem *next_queue_index;\n\tvoid *aligned_pointer;\n\tunsigned int num_inbound_queues;\n\tunsigned int num_outbound_queues;\n\tunsigned int num_queue_indexes;\n\tstruct pqi_queue_group *queue_group;\n\n\telement_array_length_per_iq =\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH *\n\t\tctrl_info->num_elements_per_iq;\n\telement_array_length_per_oq =\n\t\tPQI_OPERATIONAL_OQ_ELEMENT_LENGTH *\n\t\tctrl_info->num_elements_per_oq;\n\tnum_inbound_queues = ctrl_info->num_queue_groups * 2;\n\tnum_outbound_queues = ctrl_info->num_queue_groups;\n\tnum_queue_indexes = (ctrl_info->num_queue_groups * 3) + 1;\n\n\taligned_pointer = NULL;\n\n\tfor (i = 0; i < num_inbound_queues; i++) {\n\t\taligned_pointer = PTR_ALIGN(aligned_pointer,\n\t\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\t\taligned_pointer += element_array_length_per_iq;\n\t}\n\n\tfor (i = 0; i < num_outbound_queues; i++) {\n\t\taligned_pointer = PTR_ALIGN(aligned_pointer,\n\t\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\t\taligned_pointer += element_array_length_per_oq;\n\t}\n\n\taligned_pointer = PTR_ALIGN(aligned_pointer,\n\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\taligned_pointer += PQI_NUM_EVENT_QUEUE_ELEMENTS *\n\t\tPQI_EVENT_OQ_ELEMENT_LENGTH;\n\n\tfor (i = 0; i < num_queue_indexes; i++) {\n\t\taligned_pointer = PTR_ALIGN(aligned_pointer,\n\t\t\tPQI_OPERATIONAL_INDEX_ALIGNMENT);\n\t\taligned_pointer += sizeof(pqi_index_t);\n\t}\n\n\talloc_length = (size_t)aligned_pointer +\n\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT;\n\n\talloc_length += PQI_EXTRA_SGL_MEMORY;\n\n\tctrl_info->queue_memory_base =\n\t\tdma_alloc_coherent(&ctrl_info->pci_dev->dev, alloc_length,\n\t\t\t\t   &ctrl_info->queue_memory_base_dma_handle,\n\t\t\t\t   GFP_KERNEL);\n\n\tif (!ctrl_info->queue_memory_base)\n\t\treturn -ENOMEM;\n\n\tctrl_info->queue_memory_length = alloc_length;\n\n\telement_array = PTR_ALIGN(ctrl_info->queue_memory_base,\n\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\t\tqueue_group->iq_element_array[RAID_PATH] = element_array;\n\t\tqueue_group->iq_element_array_bus_addr[RAID_PATH] =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t\t(element_array - ctrl_info->queue_memory_base);\n\t\telement_array += element_array_length_per_iq;\n\t\telement_array = PTR_ALIGN(element_array,\n\t\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\t\tqueue_group->iq_element_array[AIO_PATH] = element_array;\n\t\tqueue_group->iq_element_array_bus_addr[AIO_PATH] =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t(element_array - ctrl_info->queue_memory_base);\n\t\telement_array += element_array_length_per_iq;\n\t\telement_array = PTR_ALIGN(element_array,\n\t\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\t}\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\t\tqueue_group->oq_element_array = element_array;\n\t\tqueue_group->oq_element_array_bus_addr =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t(element_array - ctrl_info->queue_memory_base);\n\t\telement_array += element_array_length_per_oq;\n\t\telement_array = PTR_ALIGN(element_array,\n\t\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\t}\n\n\tctrl_info->event_queue.oq_element_array = element_array;\n\tctrl_info->event_queue.oq_element_array_bus_addr =\n\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t(element_array - ctrl_info->queue_memory_base);\n\telement_array += PQI_NUM_EVENT_QUEUE_ELEMENTS *\n\t\tPQI_EVENT_OQ_ELEMENT_LENGTH;\n\n\tnext_queue_index = (void __iomem *)PTR_ALIGN(element_array,\n\t\tPQI_OPERATIONAL_INDEX_ALIGNMENT);\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\t\tqueue_group->iq_ci[RAID_PATH] = next_queue_index;\n\t\tqueue_group->iq_ci_bus_addr[RAID_PATH] =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t(next_queue_index -\n\t\t\t(void __iomem *)ctrl_info->queue_memory_base);\n\t\tnext_queue_index += sizeof(pqi_index_t);\n\t\tnext_queue_index = PTR_ALIGN(next_queue_index,\n\t\t\tPQI_OPERATIONAL_INDEX_ALIGNMENT);\n\t\tqueue_group->iq_ci[AIO_PATH] = next_queue_index;\n\t\tqueue_group->iq_ci_bus_addr[AIO_PATH] =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t(next_queue_index -\n\t\t\t(void __iomem *)ctrl_info->queue_memory_base);\n\t\tnext_queue_index += sizeof(pqi_index_t);\n\t\tnext_queue_index = PTR_ALIGN(next_queue_index,\n\t\t\tPQI_OPERATIONAL_INDEX_ALIGNMENT);\n\t\tqueue_group->oq_pi = next_queue_index;\n\t\tqueue_group->oq_pi_bus_addr =\n\t\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t\t(next_queue_index -\n\t\t\t(void __iomem *)ctrl_info->queue_memory_base);\n\t\tnext_queue_index += sizeof(pqi_index_t);\n\t\tnext_queue_index = PTR_ALIGN(next_queue_index,\n\t\t\tPQI_OPERATIONAL_INDEX_ALIGNMENT);\n\t}\n\n\tctrl_info->event_queue.oq_pi = next_queue_index;\n\tctrl_info->event_queue.oq_pi_bus_addr =\n\t\tctrl_info->queue_memory_base_dma_handle +\n\t\t(next_queue_index -\n\t\t(void __iomem *)ctrl_info->queue_memory_base);\n\n\treturn 0;\n}\n\nstatic void pqi_init_operational_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tu16 next_iq_id = PQI_MIN_OPERATIONAL_QUEUE_ID;\n\tu16 next_oq_id = PQI_MIN_OPERATIONAL_QUEUE_ID;\n\n\t \n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++)\n\t\tctrl_info->queue_groups[i].ctrl_info = ctrl_info;\n\n\t \n\tctrl_info->event_queue.oq_id = next_oq_id++;\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tctrl_info->queue_groups[i].iq_id[RAID_PATH] = next_iq_id++;\n\t\tctrl_info->queue_groups[i].iq_id[AIO_PATH] = next_iq_id++;\n\t\tctrl_info->queue_groups[i].oq_id = next_oq_id++;\n\t}\n\n\t \n\tctrl_info->event_queue.int_msg_num = 0;\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++)\n\t\tctrl_info->queue_groups[i].int_msg_num = i;\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tspin_lock_init(&ctrl_info->queue_groups[i].submit_lock[0]);\n\t\tspin_lock_init(&ctrl_info->queue_groups[i].submit_lock[1]);\n\t\tINIT_LIST_HEAD(&ctrl_info->queue_groups[i].request_list[0]);\n\t\tINIT_LIST_HEAD(&ctrl_info->queue_groups[i].request_list[1]);\n\t}\n}\n\nstatic int pqi_alloc_admin_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tsize_t alloc_length;\n\tstruct pqi_admin_queues_aligned *admin_queues_aligned;\n\tstruct pqi_admin_queues *admin_queues;\n\n\talloc_length = sizeof(struct pqi_admin_queues_aligned) +\n\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT;\n\n\tctrl_info->admin_queue_memory_base =\n\t\tdma_alloc_coherent(&ctrl_info->pci_dev->dev, alloc_length,\n\t\t\t\t   &ctrl_info->admin_queue_memory_base_dma_handle,\n\t\t\t\t   GFP_KERNEL);\n\n\tif (!ctrl_info->admin_queue_memory_base)\n\t\treturn -ENOMEM;\n\n\tctrl_info->admin_queue_memory_length = alloc_length;\n\n\tadmin_queues = &ctrl_info->admin_queues;\n\tadmin_queues_aligned = PTR_ALIGN(ctrl_info->admin_queue_memory_base,\n\t\tPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\n\tadmin_queues->iq_element_array =\n\t\t&admin_queues_aligned->iq_element_array;\n\tadmin_queues->oq_element_array =\n\t\t&admin_queues_aligned->oq_element_array;\n\tadmin_queues->iq_ci =\n\t\t(pqi_index_t __iomem *)&admin_queues_aligned->iq_ci;\n\tadmin_queues->oq_pi =\n\t\t(pqi_index_t __iomem *)&admin_queues_aligned->oq_pi;\n\n\tadmin_queues->iq_element_array_bus_addr =\n\t\tctrl_info->admin_queue_memory_base_dma_handle +\n\t\t(admin_queues->iq_element_array -\n\t\tctrl_info->admin_queue_memory_base);\n\tadmin_queues->oq_element_array_bus_addr =\n\t\tctrl_info->admin_queue_memory_base_dma_handle +\n\t\t(admin_queues->oq_element_array -\n\t\tctrl_info->admin_queue_memory_base);\n\tadmin_queues->iq_ci_bus_addr =\n\t\tctrl_info->admin_queue_memory_base_dma_handle +\n\t\t((void __iomem *)admin_queues->iq_ci -\n\t\t(void __iomem *)ctrl_info->admin_queue_memory_base);\n\tadmin_queues->oq_pi_bus_addr =\n\t\tctrl_info->admin_queue_memory_base_dma_handle +\n\t\t((void __iomem *)admin_queues->oq_pi -\n\t\t(void __iomem *)ctrl_info->admin_queue_memory_base);\n\n\treturn 0;\n}\n\n#define PQI_ADMIN_QUEUE_CREATE_TIMEOUT_JIFFIES\t\tHZ\n#define PQI_ADMIN_QUEUE_CREATE_POLL_INTERVAL_MSECS\t1\n\nstatic int pqi_create_admin_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct pqi_device_registers __iomem *pqi_registers;\n\tstruct pqi_admin_queues *admin_queues;\n\tunsigned long timeout;\n\tu8 status;\n\tu32 reg;\n\n\tpqi_registers = ctrl_info->pqi_registers;\n\tadmin_queues = &ctrl_info->admin_queues;\n\n\twriteq((u64)admin_queues->iq_element_array_bus_addr,\n\t\t&pqi_registers->admin_iq_element_array_addr);\n\twriteq((u64)admin_queues->oq_element_array_bus_addr,\n\t\t&pqi_registers->admin_oq_element_array_addr);\n\twriteq((u64)admin_queues->iq_ci_bus_addr,\n\t\t&pqi_registers->admin_iq_ci_addr);\n\twriteq((u64)admin_queues->oq_pi_bus_addr,\n\t\t&pqi_registers->admin_oq_pi_addr);\n\n\treg = PQI_ADMIN_IQ_NUM_ELEMENTS |\n\t\t(PQI_ADMIN_OQ_NUM_ELEMENTS << 8) |\n\t\t(admin_queues->int_msg_num << 16);\n\twritel(reg, &pqi_registers->admin_iq_num_elements);\n\n\twritel(PQI_CREATE_ADMIN_QUEUE_PAIR,\n\t\t&pqi_registers->function_and_status_code);\n\n\ttimeout = PQI_ADMIN_QUEUE_CREATE_TIMEOUT_JIFFIES + jiffies;\n\twhile (1) {\n\t\tmsleep(PQI_ADMIN_QUEUE_CREATE_POLL_INTERVAL_MSECS);\n\t\tstatus = readb(&pqi_registers->function_and_status_code);\n\t\tif (status == PQI_STATUS_IDLE)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout))\n\t\t\treturn -ETIMEDOUT;\n\t}\n\n\t \n\tadmin_queues->iq_pi = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\treadq(&pqi_registers->admin_iq_pi_offset);\n\tadmin_queues->oq_ci = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\treadq(&pqi_registers->admin_oq_ci_offset);\n\n\treturn 0;\n}\n\nstatic void pqi_submit_admin_request(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_general_admin_request *request)\n{\n\tstruct pqi_admin_queues *admin_queues;\n\tvoid *next_element;\n\tpqi_index_t iq_pi;\n\n\tadmin_queues = &ctrl_info->admin_queues;\n\tiq_pi = admin_queues->iq_pi_copy;\n\n\tnext_element = admin_queues->iq_element_array +\n\t\t(iq_pi * PQI_ADMIN_IQ_ELEMENT_LENGTH);\n\n\tmemcpy(next_element, request, sizeof(*request));\n\n\tiq_pi = (iq_pi + 1) % PQI_ADMIN_IQ_NUM_ELEMENTS;\n\tadmin_queues->iq_pi_copy = iq_pi;\n\n\t \n\twritel(iq_pi, admin_queues->iq_pi);\n}\n\n#define PQI_ADMIN_REQUEST_TIMEOUT_SECS\t60\n\nstatic int pqi_poll_for_admin_response(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_general_admin_response *response)\n{\n\tstruct pqi_admin_queues *admin_queues;\n\tpqi_index_t oq_pi;\n\tpqi_index_t oq_ci;\n\tunsigned long timeout;\n\n\tadmin_queues = &ctrl_info->admin_queues;\n\toq_ci = admin_queues->oq_ci_copy;\n\n\ttimeout = (PQI_ADMIN_REQUEST_TIMEOUT_SECS * HZ) + jiffies;\n\n\twhile (1) {\n\t\toq_pi = readl(admin_queues->oq_pi);\n\t\tif (oq_pi != oq_ci)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"timed out waiting for admin response\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tif (!sis_is_firmware_running(ctrl_info))\n\t\t\treturn -ENXIO;\n\t\tusleep_range(1000, 2000);\n\t}\n\n\tmemcpy(response, admin_queues->oq_element_array +\n\t\t(oq_ci * PQI_ADMIN_OQ_ELEMENT_LENGTH), sizeof(*response));\n\n\toq_ci = (oq_ci + 1) % PQI_ADMIN_OQ_NUM_ELEMENTS;\n\tadmin_queues->oq_ci_copy = oq_ci;\n\twritel(oq_ci, admin_queues->oq_ci);\n\n\treturn 0;\n}\n\nstatic void pqi_start_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_queue_group *queue_group, enum pqi_io_path path,\n\tstruct pqi_io_request *io_request)\n{\n\tstruct pqi_io_request *next;\n\tvoid *next_element;\n\tpqi_index_t iq_pi;\n\tpqi_index_t iq_ci;\n\tsize_t iu_length;\n\tunsigned long flags;\n\tunsigned int num_elements_needed;\n\tunsigned int num_elements_to_end_of_queue;\n\tsize_t copy_count;\n\tstruct pqi_iu_header *request;\n\n\tspin_lock_irqsave(&queue_group->submit_lock[path], flags);\n\n\tif (io_request) {\n\t\tio_request->queue_group = queue_group;\n\t\tlist_add_tail(&io_request->request_list_entry,\n\t\t\t&queue_group->request_list[path]);\n\t}\n\n\tiq_pi = queue_group->iq_pi_copy[path];\n\n\tlist_for_each_entry_safe(io_request, next,\n\t\t&queue_group->request_list[path], request_list_entry) {\n\n\t\trequest = io_request->iu;\n\n\t\tiu_length = get_unaligned_le16(&request->iu_length) +\n\t\t\tPQI_REQUEST_HEADER_LENGTH;\n\t\tnum_elements_needed =\n\t\t\tDIV_ROUND_UP(iu_length,\n\t\t\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\t\tiq_ci = readl(queue_group->iq_ci[path]);\n\n\t\tif (num_elements_needed > pqi_num_elements_free(iq_pi, iq_ci,\n\t\t\tctrl_info->num_elements_per_iq))\n\t\t\tbreak;\n\n\t\tput_unaligned_le16(queue_group->oq_id,\n\t\t\t&request->response_queue_id);\n\n\t\tnext_element = queue_group->iq_element_array[path] +\n\t\t\t(iq_pi * PQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\t\tnum_elements_to_end_of_queue =\n\t\t\tctrl_info->num_elements_per_iq - iq_pi;\n\n\t\tif (num_elements_needed <= num_elements_to_end_of_queue) {\n\t\t\tmemcpy(next_element, request, iu_length);\n\t\t} else {\n\t\t\tcopy_count = num_elements_to_end_of_queue *\n\t\t\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH;\n\t\t\tmemcpy(next_element, request, copy_count);\n\t\t\tmemcpy(queue_group->iq_element_array[path],\n\t\t\t\t(u8 *)request + copy_count,\n\t\t\t\tiu_length - copy_count);\n\t\t}\n\n\t\tiq_pi = (iq_pi + num_elements_needed) %\n\t\t\tctrl_info->num_elements_per_iq;\n\n\t\tlist_del(&io_request->request_list_entry);\n\t}\n\n\tif (iq_pi != queue_group->iq_pi_copy[path]) {\n\t\tqueue_group->iq_pi_copy[path] = iq_pi;\n\t\t \n\t\twritel(iq_pi, queue_group->iq_pi[path]);\n\t}\n\n\tspin_unlock_irqrestore(&queue_group->submit_lock[path], flags);\n}\n\n#define PQI_WAIT_FOR_COMPLETION_IO_TIMEOUT_SECS\t\t10\n\nstatic int pqi_wait_for_completion_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct completion *wait)\n{\n\tint rc;\n\n\twhile (1) {\n\t\tif (wait_for_completion_io_timeout(wait,\n\t\t\tPQI_WAIT_FOR_COMPLETION_IO_TIMEOUT_SECS * HZ)) {\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tpqi_check_ctrl_health(ctrl_info);\n\t\tif (pqi_ctrl_offline(ctrl_info)) {\n\t\t\trc = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic void pqi_raid_synchronous_complete(struct pqi_io_request *io_request,\n\tvoid *context)\n{\n\tstruct completion *waiting = context;\n\n\tcomplete(waiting);\n}\n\nstatic int pqi_process_raid_io_error_synchronous(\n\tstruct pqi_raid_error_info *error_info)\n{\n\tint rc = -EIO;\n\n\tswitch (error_info->data_out_result) {\n\tcase PQI_DATA_IN_OUT_GOOD:\n\t\tif (error_info->status == SAM_STAT_GOOD)\n\t\t\trc = 0;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_UNDERFLOW:\n\t\tif (error_info->status == SAM_STAT_GOOD ||\n\t\t\terror_info->status == SAM_STAT_CHECK_CONDITION)\n\t\t\trc = 0;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_ABORTED:\n\t\trc = PQI_CMD_STATUS_ABORTED;\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nstatic inline bool pqi_is_blockable_request(struct pqi_iu_header *request)\n{\n\treturn (request->driver_flags & PQI_DRIVER_NONBLOCKABLE_REQUEST) == 0;\n}\n\nstatic int pqi_submit_raid_request_synchronous(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_iu_header *request, unsigned int flags,\n\tstruct pqi_raid_error_info *error_info)\n{\n\tint rc = 0;\n\tstruct pqi_io_request *io_request;\n\tsize_t iu_length;\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\n\tif (flags & PQI_SYNC_FLAGS_INTERRUPTABLE) {\n\t\tif (down_interruptible(&ctrl_info->sync_request_sem))\n\t\t\treturn -ERESTARTSYS;\n\t} else {\n\t\tdown(&ctrl_info->sync_request_sem);\n\t}\n\n\tpqi_ctrl_busy(ctrl_info);\n\t \n\tif (pqi_is_blockable_request(request))\n\t\tpqi_wait_if_ctrl_blocked(ctrl_info);\n\n\tif (pqi_ctrl_offline(ctrl_info)) {\n\t\trc = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tio_request = pqi_alloc_io_request(ctrl_info, NULL);\n\n\tput_unaligned_le16(io_request->index,\n\t\t&(((struct pqi_raid_path_request *)request)->request_id));\n\n\tif (request->iu_type == PQI_REQUEST_IU_RAID_PATH_IO)\n\t\t((struct pqi_raid_path_request *)request)->error_index =\n\t\t\t((struct pqi_raid_path_request *)request)->request_id;\n\n\tiu_length = get_unaligned_le16(&request->iu_length) +\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\tmemcpy(io_request->iu, request, iu_length);\n\n\tio_request->io_complete_callback = pqi_raid_synchronous_complete;\n\tio_request->context = &wait;\n\n\tpqi_start_io(ctrl_info, &ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP], RAID_PATH,\n\t\tio_request);\n\n\tpqi_wait_for_completion_io(ctrl_info, &wait);\n\n\tif (error_info) {\n\t\tif (io_request->error_info)\n\t\t\tmemcpy(error_info, io_request->error_info, sizeof(*error_info));\n\t\telse\n\t\t\tmemset(error_info, 0, sizeof(*error_info));\n\t} else if (rc == 0 && io_request->error_info) {\n\t\trc = pqi_process_raid_io_error_synchronous(io_request->error_info);\n\t}\n\n\tpqi_free_io_request(io_request);\n\nout:\n\tpqi_ctrl_unbusy(ctrl_info);\n\tup(&ctrl_info->sync_request_sem);\n\n\treturn rc;\n}\n\nstatic int pqi_validate_admin_response(\n\tstruct pqi_general_admin_response *response, u8 expected_function_code)\n{\n\tif (response->header.iu_type != PQI_RESPONSE_IU_GENERAL_ADMIN)\n\t\treturn -EINVAL;\n\n\tif (get_unaligned_le16(&response->header.iu_length) !=\n\t\tPQI_GENERAL_ADMIN_IU_LENGTH)\n\t\treturn -EINVAL;\n\n\tif (response->function_code != expected_function_code)\n\t\treturn -EINVAL;\n\n\tif (response->status != PQI_GENERAL_ADMIN_STATUS_SUCCESS)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int pqi_submit_admin_request_synchronous(\n\tstruct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_general_admin_request *request,\n\tstruct pqi_general_admin_response *response)\n{\n\tint rc;\n\n\tpqi_submit_admin_request(ctrl_info, request);\n\n\trc = pqi_poll_for_admin_response(ctrl_info, response);\n\n\tif (rc == 0)\n\t\trc = pqi_validate_admin_response(response, request->function_code);\n\n\treturn rc;\n}\n\nstatic int pqi_report_device_capability(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct pqi_general_admin_request request;\n\tstruct pqi_general_admin_response response;\n\tstruct pqi_device_capability *capability;\n\tstruct pqi_iu_layer_descriptor *sop_iu_layer_descriptor;\n\n\tcapability = kmalloc(sizeof(*capability), GFP_KERNEL);\n\tif (!capability)\n\t\treturn -ENOMEM;\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code =\n\t\tPQI_GENERAL_ADMIN_FUNCTION_REPORT_DEVICE_CAPABILITY;\n\tput_unaligned_le32(sizeof(*capability),\n\t\t&request.data.report_device_capability.buffer_length);\n\n\trc = pqi_map_single(ctrl_info->pci_dev,\n\t\t&request.data.report_device_capability.sg_descriptor,\n\t\tcapability, sizeof(*capability),\n\t\tDMA_FROM_DEVICE);\n\tif (rc)\n\t\tgoto out;\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request, &response);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev,\n\t\t&request.data.report_device_capability.sg_descriptor, 1,\n\t\tDMA_FROM_DEVICE);\n\n\tif (rc)\n\t\tgoto out;\n\n\tif (response.status != PQI_GENERAL_ADMIN_STATUS_SUCCESS) {\n\t\trc = -EIO;\n\t\tgoto out;\n\t}\n\n\tctrl_info->max_inbound_queues =\n\t\tget_unaligned_le16(&capability->max_inbound_queues);\n\tctrl_info->max_elements_per_iq =\n\t\tget_unaligned_le16(&capability->max_elements_per_iq);\n\tctrl_info->max_iq_element_length =\n\t\tget_unaligned_le16(&capability->max_iq_element_length)\n\t\t* 16;\n\tctrl_info->max_outbound_queues =\n\t\tget_unaligned_le16(&capability->max_outbound_queues);\n\tctrl_info->max_elements_per_oq =\n\t\tget_unaligned_le16(&capability->max_elements_per_oq);\n\tctrl_info->max_oq_element_length =\n\t\tget_unaligned_le16(&capability->max_oq_element_length)\n\t\t* 16;\n\n\tsop_iu_layer_descriptor =\n\t\t&capability->iu_layer_descriptors[PQI_PROTOCOL_SOP];\n\n\tctrl_info->max_inbound_iu_length_per_firmware =\n\t\tget_unaligned_le16(\n\t\t\t&sop_iu_layer_descriptor->max_inbound_iu_length);\n\tctrl_info->inbound_spanning_supported =\n\t\tsop_iu_layer_descriptor->inbound_spanning_supported;\n\tctrl_info->outbound_spanning_supported =\n\t\tsop_iu_layer_descriptor->outbound_spanning_supported;\n\nout:\n\tkfree(capability);\n\n\treturn rc;\n}\n\nstatic int pqi_validate_device_capability(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (ctrl_info->max_iq_element_length <\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"max. inbound queue element length of %d is less than the required length of %d\\n\",\n\t\t\tctrl_info->max_iq_element_length,\n\t\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctrl_info->max_oq_element_length <\n\t\tPQI_OPERATIONAL_OQ_ELEMENT_LENGTH) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"max. outbound queue element length of %d is less than the required length of %d\\n\",\n\t\t\tctrl_info->max_oq_element_length,\n\t\t\tPQI_OPERATIONAL_OQ_ELEMENT_LENGTH);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctrl_info->max_inbound_iu_length_per_firmware <\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"max. inbound IU length of %u is less than the min. required length of %d\\n\",\n\t\t\tctrl_info->max_inbound_iu_length_per_firmware,\n\t\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ctrl_info->inbound_spanning_supported) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"the controller does not support inbound spanning\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctrl_info->outbound_spanning_supported) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"the controller supports outbound spanning but this driver does not\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int pqi_create_event_queue(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct pqi_event_queue *event_queue;\n\tstruct pqi_general_admin_request request;\n\tstruct pqi_general_admin_response response;\n\n\tevent_queue = &ctrl_info->event_queue;\n\n\t \n\tmemset(&request, 0, sizeof(request));\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_OQ;\n\tput_unaligned_le16(event_queue->oq_id,\n\t\t&request.data.create_operational_oq.queue_id);\n\tput_unaligned_le64((u64)event_queue->oq_element_array_bus_addr,\n\t\t&request.data.create_operational_oq.element_array_addr);\n\tput_unaligned_le64((u64)event_queue->oq_pi_bus_addr,\n\t\t&request.data.create_operational_oq.pi_addr);\n\tput_unaligned_le16(PQI_NUM_EVENT_QUEUE_ELEMENTS,\n\t\t&request.data.create_operational_oq.num_elements);\n\tput_unaligned_le16(PQI_EVENT_OQ_ELEMENT_LENGTH / 16,\n\t\t&request.data.create_operational_oq.element_length);\n\trequest.data.create_operational_oq.queue_protocol = PQI_PROTOCOL_SOP;\n\tput_unaligned_le16(event_queue->int_msg_num,\n\t\t&request.data.create_operational_oq.int_msg_num);\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\n\t\t&response);\n\tif (rc)\n\t\treturn rc;\n\n\tevent_queue->oq_ci = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\tget_unaligned_le64(\n\t\t\t&response.data.create_operational_oq.oq_ci_offset);\n\n\treturn 0;\n}\n\nstatic int pqi_create_queue_group(struct pqi_ctrl_info *ctrl_info,\n\tunsigned int group_number)\n{\n\tint rc;\n\tstruct pqi_queue_group *queue_group;\n\tstruct pqi_general_admin_request request;\n\tstruct pqi_general_admin_response response;\n\n\tqueue_group = &ctrl_info->queue_groups[group_number];\n\n\t \n\tmemset(&request, 0, sizeof(request));\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_IQ;\n\tput_unaligned_le16(queue_group->iq_id[RAID_PATH],\n\t\t&request.data.create_operational_iq.queue_id);\n\tput_unaligned_le64(\n\t\t(u64)queue_group->iq_element_array_bus_addr[RAID_PATH],\n\t\t&request.data.create_operational_iq.element_array_addr);\n\tput_unaligned_le64((u64)queue_group->iq_ci_bus_addr[RAID_PATH],\n\t\t&request.data.create_operational_iq.ci_addr);\n\tput_unaligned_le16(ctrl_info->num_elements_per_iq,\n\t\t&request.data.create_operational_iq.num_elements);\n\tput_unaligned_le16(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH / 16,\n\t\t&request.data.create_operational_iq.element_length);\n\trequest.data.create_operational_iq.queue_protocol = PQI_PROTOCOL_SOP;\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\n\t\t&response);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating inbound RAID queue\\n\");\n\t\treturn rc;\n\t}\n\n\tqueue_group->iq_pi[RAID_PATH] = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\tget_unaligned_le64(\n\t\t\t&response.data.create_operational_iq.iq_pi_offset);\n\n\t \n\tmemset(&request, 0, sizeof(request));\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_IQ;\n\tput_unaligned_le16(queue_group->iq_id[AIO_PATH],\n\t\t&request.data.create_operational_iq.queue_id);\n\tput_unaligned_le64((u64)queue_group->\n\t\tiq_element_array_bus_addr[AIO_PATH],\n\t\t&request.data.create_operational_iq.element_array_addr);\n\tput_unaligned_le64((u64)queue_group->iq_ci_bus_addr[AIO_PATH],\n\t\t&request.data.create_operational_iq.ci_addr);\n\tput_unaligned_le16(ctrl_info->num_elements_per_iq,\n\t\t&request.data.create_operational_iq.num_elements);\n\tput_unaligned_le16(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH / 16,\n\t\t&request.data.create_operational_iq.element_length);\n\trequest.data.create_operational_iq.queue_protocol = PQI_PROTOCOL_SOP;\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\n\t\t&response);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating inbound AIO queue\\n\");\n\t\treturn rc;\n\t}\n\n\tqueue_group->iq_pi[AIO_PATH] = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\tget_unaligned_le64(\n\t\t\t&response.data.create_operational_iq.iq_pi_offset);\n\n\t \n\tmemset(&request, 0, sizeof(request));\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CHANGE_IQ_PROPERTY;\n\tput_unaligned_le16(queue_group->iq_id[AIO_PATH],\n\t\t&request.data.change_operational_iq_properties.queue_id);\n\tput_unaligned_le32(PQI_IQ_PROPERTY_IS_AIO_QUEUE,\n\t\t&request.data.change_operational_iq_properties.vendor_specific);\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\n\t\t&response);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error changing queue property\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&request, 0, sizeof(request));\n\trequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\n\tput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\n\t\t&request.header.iu_length);\n\trequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_OQ;\n\tput_unaligned_le16(queue_group->oq_id,\n\t\t&request.data.create_operational_oq.queue_id);\n\tput_unaligned_le64((u64)queue_group->oq_element_array_bus_addr,\n\t\t&request.data.create_operational_oq.element_array_addr);\n\tput_unaligned_le64((u64)queue_group->oq_pi_bus_addr,\n\t\t&request.data.create_operational_oq.pi_addr);\n\tput_unaligned_le16(ctrl_info->num_elements_per_oq,\n\t\t&request.data.create_operational_oq.num_elements);\n\tput_unaligned_le16(PQI_OPERATIONAL_OQ_ELEMENT_LENGTH / 16,\n\t\t&request.data.create_operational_oq.element_length);\n\trequest.data.create_operational_oq.queue_protocol = PQI_PROTOCOL_SOP;\n\tput_unaligned_le16(queue_group->int_msg_num,\n\t\t&request.data.create_operational_oq.int_msg_num);\n\n\trc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\n\t\t&response);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating outbound queue\\n\");\n\t\treturn rc;\n\t}\n\n\tqueue_group->oq_ci = ctrl_info->iomem_base +\n\t\tPQI_DEVICE_REGISTERS_OFFSET +\n\t\tget_unaligned_le64(\n\t\t\t&response.data.create_operational_oq.oq_ci_offset);\n\n\treturn 0;\n}\n\nstatic int pqi_create_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tunsigned int i;\n\n\trc = pqi_create_event_queue(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating event queue\\n\");\n\t\treturn rc;\n\t}\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\trc = pqi_create_queue_group(ctrl_info, i);\n\t\tif (rc) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"error creating queue group number %u/%u\\n\",\n\t\t\t\ti, ctrl_info->num_queue_groups);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#define PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH\t\\\n\tstruct_size_t(struct pqi_event_config,  descriptors, PQI_MAX_EVENT_DESCRIPTORS)\n\nstatic int pqi_configure_events(struct pqi_ctrl_info *ctrl_info,\n\tbool enable_events)\n{\n\tint rc;\n\tunsigned int i;\n\tstruct pqi_event_config *event_config;\n\tstruct pqi_event_descriptor *event_descriptor;\n\tstruct pqi_general_management_request request;\n\n\tevent_config = kmalloc(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\n\t\tGFP_KERNEL);\n\tif (!event_config)\n\t\treturn -ENOMEM;\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_REPORT_VENDOR_EVENT_CONFIG;\n\tput_unaligned_le16(offsetof(struct pqi_general_management_request,\n\t\tdata.report_event_configuration.sg_descriptors[1]) -\n\t\tPQI_REQUEST_HEADER_LENGTH, &request.header.iu_length);\n\tput_unaligned_le32(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\n\t\t&request.data.report_event_configuration.buffer_length);\n\n\trc = pqi_map_single(ctrl_info->pci_dev,\n\t\trequest.data.report_event_configuration.sg_descriptors,\n\t\tevent_config, PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\n\t\tDMA_FROM_DEVICE);\n\tif (rc)\n\t\tgoto out;\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev,\n\t\trequest.data.report_event_configuration.sg_descriptors, 1,\n\t\tDMA_FROM_DEVICE);\n\n\tif (rc)\n\t\tgoto out;\n\n\tfor (i = 0; i < event_config->num_event_descriptors; i++) {\n\t\tevent_descriptor = &event_config->descriptors[i];\n\t\tif (enable_events &&\n\t\t\tpqi_is_supported_event(event_descriptor->event_type))\n\t\t\t\tput_unaligned_le16(ctrl_info->event_queue.oq_id,\n\t\t\t\t\t&event_descriptor->oq_id);\n\t\telse\n\t\t\tput_unaligned_le16(0, &event_descriptor->oq_id);\n\t}\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_SET_VENDOR_EVENT_CONFIG;\n\tput_unaligned_le16(offsetof(struct pqi_general_management_request,\n\t\tdata.report_event_configuration.sg_descriptors[1]) -\n\t\tPQI_REQUEST_HEADER_LENGTH, &request.header.iu_length);\n\tput_unaligned_le32(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\n\t\t&request.data.report_event_configuration.buffer_length);\n\n\trc = pqi_map_single(ctrl_info->pci_dev,\n\t\trequest.data.report_event_configuration.sg_descriptors,\n\t\tevent_config, PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\n\t\tDMA_TO_DEVICE);\n\tif (rc)\n\t\tgoto out;\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n\n\tpqi_pci_unmap(ctrl_info->pci_dev,\n\t\trequest.data.report_event_configuration.sg_descriptors, 1,\n\t\tDMA_TO_DEVICE);\n\nout:\n\tkfree(event_config);\n\n\treturn rc;\n}\n\nstatic inline int pqi_enable_events(struct pqi_ctrl_info *ctrl_info)\n{\n\treturn pqi_configure_events(ctrl_info, true);\n}\n\nstatic void pqi_free_all_io_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tstruct device *dev;\n\tsize_t sg_chain_buffer_length;\n\tstruct pqi_io_request *io_request;\n\n\tif (!ctrl_info->io_request_pool)\n\t\treturn;\n\n\tdev = &ctrl_info->pci_dev->dev;\n\tsg_chain_buffer_length = ctrl_info->sg_chain_buffer_length;\n\tio_request = ctrl_info->io_request_pool;\n\n\tfor (i = 0; i < ctrl_info->max_io_slots; i++) {\n\t\tkfree(io_request->iu);\n\t\tif (!io_request->sg_chain_buffer)\n\t\t\tbreak;\n\t\tdma_free_coherent(dev, sg_chain_buffer_length,\n\t\t\tio_request->sg_chain_buffer,\n\t\t\tio_request->sg_chain_buffer_dma_handle);\n\t\tio_request++;\n\t}\n\n\tkfree(ctrl_info->io_request_pool);\n\tctrl_info->io_request_pool = NULL;\n}\n\nstatic inline int pqi_alloc_error_buffer(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->error_buffer = dma_alloc_coherent(&ctrl_info->pci_dev->dev,\n\t\t\t\t     ctrl_info->error_buffer_length,\n\t\t\t\t     &ctrl_info->error_buffer_dma_handle,\n\t\t\t\t     GFP_KERNEL);\n\tif (!ctrl_info->error_buffer)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int pqi_alloc_io_resources(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tvoid *sg_chain_buffer;\n\tsize_t sg_chain_buffer_length;\n\tdma_addr_t sg_chain_buffer_dma_handle;\n\tstruct device *dev;\n\tstruct pqi_io_request *io_request;\n\n\tctrl_info->io_request_pool = kcalloc(ctrl_info->max_io_slots,\n\t\tsizeof(ctrl_info->io_request_pool[0]), GFP_KERNEL);\n\n\tif (!ctrl_info->io_request_pool) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to allocate I/O request pool\\n\");\n\t\tgoto error;\n\t}\n\n\tdev = &ctrl_info->pci_dev->dev;\n\tsg_chain_buffer_length = ctrl_info->sg_chain_buffer_length;\n\tio_request = ctrl_info->io_request_pool;\n\n\tfor (i = 0; i < ctrl_info->max_io_slots; i++) {\n\t\tio_request->iu = kmalloc(ctrl_info->max_inbound_iu_length, GFP_KERNEL);\n\n\t\tif (!io_request->iu) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"failed to allocate IU buffers\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\tsg_chain_buffer = dma_alloc_coherent(dev,\n\t\t\tsg_chain_buffer_length, &sg_chain_buffer_dma_handle,\n\t\t\tGFP_KERNEL);\n\n\t\tif (!sg_chain_buffer) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"failed to allocate PQI scatter-gather chain buffers\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\tio_request->index = i;\n\t\tio_request->sg_chain_buffer = sg_chain_buffer;\n\t\tio_request->sg_chain_buffer_dma_handle = sg_chain_buffer_dma_handle;\n\t\tio_request++;\n\t}\n\n\treturn 0;\n\nerror:\n\tpqi_free_all_io_requests(ctrl_info);\n\n\treturn -ENOMEM;\n}\n\n \n\nstatic void pqi_calculate_io_resources(struct pqi_ctrl_info *ctrl_info)\n{\n\tu32 max_transfer_size;\n\tu32 max_sg_entries;\n\n\tctrl_info->scsi_ml_can_queue =\n\t\tctrl_info->max_outstanding_requests - PQI_RESERVED_IO_SLOTS;\n\tctrl_info->max_io_slots = ctrl_info->max_outstanding_requests;\n\n\tctrl_info->error_buffer_length =\n\t\tctrl_info->max_io_slots * PQI_ERROR_BUFFER_ELEMENT_LENGTH;\n\n\tif (reset_devices)\n\t\tmax_transfer_size = min(ctrl_info->max_transfer_size,\n\t\t\tPQI_MAX_TRANSFER_SIZE_KDUMP);\n\telse\n\t\tmax_transfer_size = min(ctrl_info->max_transfer_size,\n\t\t\tPQI_MAX_TRANSFER_SIZE);\n\n\tmax_sg_entries = max_transfer_size / PAGE_SIZE;\n\n\t \n\tmax_sg_entries++;\n\n\tmax_sg_entries = min(ctrl_info->max_sg_entries, max_sg_entries);\n\n\tmax_transfer_size = (max_sg_entries - 1) * PAGE_SIZE;\n\n\tctrl_info->sg_chain_buffer_length =\n\t\t(max_sg_entries * sizeof(struct pqi_sg_descriptor)) +\n\t\tPQI_EXTRA_SGL_MEMORY;\n\tctrl_info->sg_tablesize = max_sg_entries;\n\tctrl_info->max_sectors = max_transfer_size / 512;\n}\n\nstatic void pqi_calculate_queue_resources(struct pqi_ctrl_info *ctrl_info)\n{\n\tint num_queue_groups;\n\tu16 num_elements_per_iq;\n\tu16 num_elements_per_oq;\n\n\tif (reset_devices) {\n\t\tnum_queue_groups = 1;\n\t} else {\n\t\tint num_cpus;\n\t\tint max_queue_groups;\n\n\t\tmax_queue_groups = min(ctrl_info->max_inbound_queues / 2,\n\t\t\tctrl_info->max_outbound_queues - 1);\n\t\tmax_queue_groups = min(max_queue_groups, PQI_MAX_QUEUE_GROUPS);\n\n\t\tnum_cpus = num_online_cpus();\n\t\tnum_queue_groups = min(num_cpus, ctrl_info->max_msix_vectors);\n\t\tnum_queue_groups = min(num_queue_groups, max_queue_groups);\n\t}\n\n\tctrl_info->num_queue_groups = num_queue_groups;\n\n\t \n\tctrl_info->max_inbound_iu_length =\n\t\t(ctrl_info->max_inbound_iu_length_per_firmware /\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) *\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH;\n\n\tnum_elements_per_iq =\n\t\t(ctrl_info->max_inbound_iu_length /\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\t \n\tnum_elements_per_iq++;\n\n\tnum_elements_per_iq = min(num_elements_per_iq,\n\t\tctrl_info->max_elements_per_iq);\n\n\tnum_elements_per_oq = ((num_elements_per_iq - 1) * 2) + 1;\n\tnum_elements_per_oq = min(num_elements_per_oq,\n\t\tctrl_info->max_elements_per_oq);\n\n\tctrl_info->num_elements_per_iq = num_elements_per_iq;\n\tctrl_info->num_elements_per_oq = num_elements_per_oq;\n\n\tctrl_info->max_sg_per_iu =\n\t\t((ctrl_info->max_inbound_iu_length -\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) /\n\t\tsizeof(struct pqi_sg_descriptor)) +\n\t\tPQI_MAX_EMBEDDED_SG_DESCRIPTORS;\n\n\tctrl_info->max_sg_per_r56_iu =\n\t\t((ctrl_info->max_inbound_iu_length -\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) /\n\t\tsizeof(struct pqi_sg_descriptor)) +\n\t\tPQI_MAX_EMBEDDED_R56_SG_DESCRIPTORS;\n}\n\nstatic inline void pqi_set_sg_descriptor(struct pqi_sg_descriptor *sg_descriptor,\n\tstruct scatterlist *sg)\n{\n\tu64 address = (u64)sg_dma_address(sg);\n\tunsigned int length = sg_dma_len(sg);\n\n\tput_unaligned_le64(address, &sg_descriptor->address);\n\tput_unaligned_le32(length, &sg_descriptor->length);\n\tput_unaligned_le32(0, &sg_descriptor->flags);\n}\n\nstatic unsigned int pqi_build_sg_list(struct pqi_sg_descriptor *sg_descriptor,\n\tstruct scatterlist *sg, int sg_count, struct pqi_io_request *io_request,\n\tint max_sg_per_iu, bool *chained)\n{\n\tint i;\n\tunsigned int num_sg_in_iu;\n\n\t*chained = false;\n\ti = 0;\n\tnum_sg_in_iu = 0;\n\tmax_sg_per_iu--;\t \n\n\twhile (1) {\n\t\tpqi_set_sg_descriptor(sg_descriptor, sg);\n\t\tif (!*chained)\n\t\t\tnum_sg_in_iu++;\n\t\ti++;\n\t\tif (i == sg_count)\n\t\t\tbreak;\n\t\tsg_descriptor++;\n\t\tif (i == max_sg_per_iu) {\n\t\t\tput_unaligned_le64((u64)io_request->sg_chain_buffer_dma_handle,\n\t\t\t\t&sg_descriptor->address);\n\t\t\tput_unaligned_le32((sg_count - num_sg_in_iu) * sizeof(*sg_descriptor),\n\t\t\t\t&sg_descriptor->length);\n\t\t\tput_unaligned_le32(CISS_SG_CHAIN, &sg_descriptor->flags);\n\t\t\t*chained = true;\n\t\t\tnum_sg_in_iu++;\n\t\t\tsg_descriptor = io_request->sg_chain_buffer;\n\t\t}\n\t\tsg = sg_next(sg);\n\t}\n\n\tput_unaligned_le32(CISS_SG_LAST, &sg_descriptor->flags);\n\n\treturn num_sg_in_iu;\n}\n\nstatic int pqi_build_raid_sg_list(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_raid_path_request *request, struct scsi_cmnd *scmd,\n\tstruct pqi_io_request *io_request)\n{\n\tu16 iu_length;\n\tint sg_count;\n\tbool chained;\n\tunsigned int num_sg_in_iu;\n\tstruct scatterlist *sg;\n\tstruct pqi_sg_descriptor *sg_descriptor;\n\n\tsg_count = scsi_dma_map(scmd);\n\tif (sg_count < 0)\n\t\treturn sg_count;\n\n\tiu_length = offsetof(struct pqi_raid_path_request, sg_descriptors) -\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\n\tif (sg_count == 0)\n\t\tgoto out;\n\n\tsg = scsi_sglist(scmd);\n\tsg_descriptor = request->sg_descriptors;\n\n\tnum_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,\n\t\tctrl_info->max_sg_per_iu, &chained);\n\n\trequest->partial = chained;\n\tiu_length += num_sg_in_iu * sizeof(*sg_descriptor);\n\nout:\n\tput_unaligned_le16(iu_length, &request->header.iu_length);\n\n\treturn 0;\n}\n\nstatic int pqi_build_aio_r1_sg_list(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_aio_r1_path_request *request, struct scsi_cmnd *scmd,\n\tstruct pqi_io_request *io_request)\n{\n\tu16 iu_length;\n\tint sg_count;\n\tbool chained;\n\tunsigned int num_sg_in_iu;\n\tstruct scatterlist *sg;\n\tstruct pqi_sg_descriptor *sg_descriptor;\n\n\tsg_count = scsi_dma_map(scmd);\n\tif (sg_count < 0)\n\t\treturn sg_count;\n\n\tiu_length = offsetof(struct pqi_aio_r1_path_request, sg_descriptors) -\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\tnum_sg_in_iu = 0;\n\n\tif (sg_count == 0)\n\t\tgoto out;\n\n\tsg = scsi_sglist(scmd);\n\tsg_descriptor = request->sg_descriptors;\n\n\tnum_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,\n\t\tctrl_info->max_sg_per_iu, &chained);\n\n\trequest->partial = chained;\n\tiu_length += num_sg_in_iu * sizeof(*sg_descriptor);\n\nout:\n\tput_unaligned_le16(iu_length, &request->header.iu_length);\n\trequest->num_sg_descriptors = num_sg_in_iu;\n\n\treturn 0;\n}\n\nstatic int pqi_build_aio_r56_sg_list(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_aio_r56_path_request *request, struct scsi_cmnd *scmd,\n\tstruct pqi_io_request *io_request)\n{\n\tu16 iu_length;\n\tint sg_count;\n\tbool chained;\n\tunsigned int num_sg_in_iu;\n\tstruct scatterlist *sg;\n\tstruct pqi_sg_descriptor *sg_descriptor;\n\n\tsg_count = scsi_dma_map(scmd);\n\tif (sg_count < 0)\n\t\treturn sg_count;\n\n\tiu_length = offsetof(struct pqi_aio_r56_path_request, sg_descriptors) -\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\tnum_sg_in_iu = 0;\n\n\tif (sg_count != 0) {\n\t\tsg = scsi_sglist(scmd);\n\t\tsg_descriptor = request->sg_descriptors;\n\n\t\tnum_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,\n\t\t\tctrl_info->max_sg_per_r56_iu, &chained);\n\n\t\trequest->partial = chained;\n\t\tiu_length += num_sg_in_iu * sizeof(*sg_descriptor);\n\t}\n\n\tput_unaligned_le16(iu_length, &request->header.iu_length);\n\trequest->num_sg_descriptors = num_sg_in_iu;\n\n\treturn 0;\n}\n\nstatic int pqi_build_aio_sg_list(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_aio_path_request *request, struct scsi_cmnd *scmd,\n\tstruct pqi_io_request *io_request)\n{\n\tu16 iu_length;\n\tint sg_count;\n\tbool chained;\n\tunsigned int num_sg_in_iu;\n\tstruct scatterlist *sg;\n\tstruct pqi_sg_descriptor *sg_descriptor;\n\n\tsg_count = scsi_dma_map(scmd);\n\tif (sg_count < 0)\n\t\treturn sg_count;\n\n\tiu_length = offsetof(struct pqi_aio_path_request, sg_descriptors) -\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\tnum_sg_in_iu = 0;\n\n\tif (sg_count == 0)\n\t\tgoto out;\n\n\tsg = scsi_sglist(scmd);\n\tsg_descriptor = request->sg_descriptors;\n\n\tnum_sg_in_iu = pqi_build_sg_list(sg_descriptor, sg, sg_count, io_request,\n\t\tctrl_info->max_sg_per_iu, &chained);\n\n\trequest->partial = chained;\n\tiu_length += num_sg_in_iu * sizeof(*sg_descriptor);\n\nout:\n\tput_unaligned_le16(iu_length, &request->header.iu_length);\n\trequest->num_sg_descriptors = num_sg_in_iu;\n\n\treturn 0;\n}\n\nstatic void pqi_raid_io_complete(struct pqi_io_request *io_request,\n\tvoid *context)\n{\n\tstruct scsi_cmnd *scmd;\n\n\tscmd = io_request->scmd;\n\tpqi_free_io_request(io_request);\n\tscsi_dma_unmap(scmd);\n\tpqi_scsi_done(scmd);\n}\n\nstatic int pqi_raid_submit_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\n\tstruct pqi_queue_group *queue_group, bool io_high_prio)\n{\n\tint rc;\n\tsize_t cdb_length;\n\tstruct pqi_io_request *io_request;\n\tstruct pqi_raid_path_request *request;\n\n\tio_request = pqi_alloc_io_request(ctrl_info, scmd);\n\tif (!io_request)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\tio_request->io_complete_callback = pqi_raid_io_complete;\n\tio_request->scmd = scmd;\n\n\trequest = io_request->iu;\n\tmemset(request, 0, offsetof(struct pqi_raid_path_request, sg_descriptors));\n\n\trequest->header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\n\tput_unaligned_le32(scsi_bufflen(scmd), &request->buffer_length);\n\trequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\trequest->command_priority = io_high_prio;\n\tput_unaligned_le16(io_request->index, &request->request_id);\n\trequest->error_index = request->request_id;\n\tmemcpy(request->lun_number, device->scsi3addr, sizeof(request->lun_number));\n\trequest->ml_device_lun_number = (u8)scmd->device->lun;\n\n\tcdb_length = min_t(size_t, scmd->cmd_len, sizeof(request->cdb));\n\tmemcpy(request->cdb, scmd->cmnd, cdb_length);\n\n\tswitch (cdb_length) {\n\tcase 6:\n\tcase 10:\n\tcase 12:\n\tcase 16:\n\t\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_0;\n\t\tbreak;\n\tcase 20:\n\t\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_4;\n\t\tbreak;\n\tcase 24:\n\t\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_8;\n\t\tbreak;\n\tcase 28:\n\t\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_12;\n\t\tbreak;\n\tcase 32:\n\tdefault:\n\t\trequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_16;\n\t\tbreak;\n\t}\n\n\tswitch (scmd->sc_data_direction) {\n\tcase DMA_FROM_DEVICE:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tbreak;\n\tcase DMA_TO_DEVICE:\n\t\trequest->data_direction = SOP_WRITE_FLAG;\n\t\tbreak;\n\tcase DMA_NONE:\n\t\trequest->data_direction = SOP_NO_DIRECTION_FLAG;\n\t\tbreak;\n\tcase DMA_BIDIRECTIONAL:\n\t\trequest->data_direction = SOP_BIDIRECTIONAL;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"unknown data direction: %d\\n\",\n\t\t\tscmd->sc_data_direction);\n\t\tbreak;\n\t}\n\n\trc = pqi_build_raid_sg_list(ctrl_info, request, scmd, io_request);\n\tif (rc) {\n\t\tpqi_free_io_request(io_request);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tpqi_start_io(ctrl_info, queue_group, RAID_PATH, io_request);\n\n\treturn 0;\n}\n\nstatic inline int pqi_raid_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\n\tstruct pqi_queue_group *queue_group)\n{\n\tbool io_high_prio;\n\n\tio_high_prio = pqi_is_io_high_priority(device, scmd);\n\n\treturn pqi_raid_submit_io(ctrl_info, device, scmd, queue_group, io_high_prio);\n}\n\nstatic bool pqi_raid_bypass_retry_needed(struct pqi_io_request *io_request)\n{\n\tstruct scsi_cmnd *scmd;\n\tstruct pqi_scsi_dev *device;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tif (!io_request->raid_bypass)\n\t\treturn false;\n\n\tscmd = io_request->scmd;\n\tif ((scmd->result & 0xff) == SAM_STAT_GOOD)\n\t\treturn false;\n\tif (host_byte(scmd->result) == DID_NO_CONNECT)\n\t\treturn false;\n\n\tdevice = scmd->device->hostdata;\n\tif (pqi_device_offline(device) || pqi_device_in_remove(device))\n\t\treturn false;\n\n\tctrl_info = shost_to_hba(scmd->device->host);\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void pqi_aio_io_complete(struct pqi_io_request *io_request,\n\tvoid *context)\n{\n\tstruct scsi_cmnd *scmd;\n\n\tscmd = io_request->scmd;\n\tscsi_dma_unmap(scmd);\n\tif (io_request->status == -EAGAIN || pqi_raid_bypass_retry_needed(io_request)) {\n\t\tset_host_byte(scmd, DID_IMM_RETRY);\n\t\tpqi_cmd_priv(scmd)->this_residual++;\n\t}\n\n\tpqi_free_io_request(io_request);\n\tpqi_scsi_done(scmd);\n}\n\nstatic inline int pqi_aio_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\n\tstruct pqi_queue_group *queue_group)\n{\n\tbool io_high_prio;\n\n\tio_high_prio = pqi_is_io_high_priority(device, scmd);\n\n\treturn pqi_aio_submit_io(ctrl_info, scmd, device->aio_handle,\n\t\tscmd->cmnd, scmd->cmd_len, queue_group, NULL,\n\t\tfalse, io_high_prio);\n}\n\nstatic int pqi_aio_submit_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, u32 aio_handle, u8 *cdb,\n\tunsigned int cdb_length, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, bool raid_bypass,\n\tbool io_high_prio)\n{\n\tint rc;\n\tstruct pqi_io_request *io_request;\n\tstruct pqi_aio_path_request *request;\n\n\tio_request = pqi_alloc_io_request(ctrl_info, scmd);\n\tif (!io_request)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\tio_request->io_complete_callback = pqi_aio_io_complete;\n\tio_request->scmd = scmd;\n\tio_request->raid_bypass = raid_bypass;\n\n\trequest = io_request->iu;\n\tmemset(request, 0, offsetof(struct pqi_aio_path_request, sg_descriptors));\n\n\trequest->header.iu_type = PQI_REQUEST_IU_AIO_PATH_IO;\n\tput_unaligned_le32(aio_handle, &request->nexus_id);\n\tput_unaligned_le32(scsi_bufflen(scmd), &request->buffer_length);\n\trequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\trequest->command_priority = io_high_prio;\n\tput_unaligned_le16(io_request->index, &request->request_id);\n\trequest->error_index = request->request_id;\n\tif (!raid_bypass && ctrl_info->multi_lun_device_supported)\n\t\tput_unaligned_le64(scmd->device->lun << 8, &request->lun_number);\n\tif (cdb_length > sizeof(request->cdb))\n\t\tcdb_length = sizeof(request->cdb);\n\trequest->cdb_length = cdb_length;\n\tmemcpy(request->cdb, cdb, cdb_length);\n\n\tswitch (scmd->sc_data_direction) {\n\tcase DMA_TO_DEVICE:\n\t\trequest->data_direction = SOP_READ_FLAG;\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\trequest->data_direction = SOP_WRITE_FLAG;\n\t\tbreak;\n\tcase DMA_NONE:\n\t\trequest->data_direction = SOP_NO_DIRECTION_FLAG;\n\t\tbreak;\n\tcase DMA_BIDIRECTIONAL:\n\t\trequest->data_direction = SOP_BIDIRECTIONAL;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"unknown data direction: %d\\n\",\n\t\t\tscmd->sc_data_direction);\n\t\tbreak;\n\t}\n\n\tif (encryption_info) {\n\t\trequest->encryption_enable = true;\n\t\tput_unaligned_le16(encryption_info->data_encryption_key_index,\n\t\t\t&request->data_encryption_key_index);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_lower,\n\t\t\t&request->encrypt_tweak_lower);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_upper,\n\t\t\t&request->encrypt_tweak_upper);\n\t}\n\n\trc = pqi_build_aio_sg_list(ctrl_info, request, scmd, io_request);\n\tif (rc) {\n\t\tpqi_free_io_request(io_request);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tpqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);\n\n\treturn 0;\n}\n\nstatic  int pqi_aio_submit_r1_write_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,\n\tstruct pqi_scsi_dev_raid_map_data *rmd)\n{\n\tint rc;\n\tstruct pqi_io_request *io_request;\n\tstruct pqi_aio_r1_path_request *r1_request;\n\n\tio_request = pqi_alloc_io_request(ctrl_info, scmd);\n\tif (!io_request)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\n\tio_request->io_complete_callback = pqi_aio_io_complete;\n\tio_request->scmd = scmd;\n\tio_request->raid_bypass = true;\n\n\tr1_request = io_request->iu;\n\tmemset(r1_request, 0, offsetof(struct pqi_aio_r1_path_request, sg_descriptors));\n\n\tr1_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID1_IO;\n\tput_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r1_request->volume_id);\n\tr1_request->num_drives = rmd->num_it_nexus_entries;\n\tput_unaligned_le32(rmd->it_nexus[0], &r1_request->it_nexus_1);\n\tput_unaligned_le32(rmd->it_nexus[1], &r1_request->it_nexus_2);\n\tif (rmd->num_it_nexus_entries == 3)\n\t\tput_unaligned_le32(rmd->it_nexus[2], &r1_request->it_nexus_3);\n\n\tput_unaligned_le32(scsi_bufflen(scmd), &r1_request->data_length);\n\tr1_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\tput_unaligned_le16(io_request->index, &r1_request->request_id);\n\tr1_request->error_index = r1_request->request_id;\n\tif (rmd->cdb_length > sizeof(r1_request->cdb))\n\t\trmd->cdb_length = sizeof(r1_request->cdb);\n\tr1_request->cdb_length = rmd->cdb_length;\n\tmemcpy(r1_request->cdb, rmd->cdb, rmd->cdb_length);\n\n\t \n\tr1_request->data_direction = SOP_READ_FLAG;\n\n\tif (encryption_info) {\n\t\tr1_request->encryption_enable = true;\n\t\tput_unaligned_le16(encryption_info->data_encryption_key_index,\n\t\t\t\t&r1_request->data_encryption_key_index);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_lower,\n\t\t\t\t&r1_request->encrypt_tweak_lower);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_upper,\n\t\t\t\t&r1_request->encrypt_tweak_upper);\n\t}\n\n\trc = pqi_build_aio_r1_sg_list(ctrl_info, r1_request, scmd, io_request);\n\tif (rc) {\n\t\tpqi_free_io_request(io_request);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tpqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);\n\n\treturn 0;\n}\n\nstatic int pqi_aio_submit_r56_write_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd, struct pqi_queue_group *queue_group,\n\tstruct pqi_encryption_info *encryption_info, struct pqi_scsi_dev *device,\n\tstruct pqi_scsi_dev_raid_map_data *rmd)\n{\n\tint rc;\n\tstruct pqi_io_request *io_request;\n\tstruct pqi_aio_r56_path_request *r56_request;\n\n\tio_request = pqi_alloc_io_request(ctrl_info, scmd);\n\tif (!io_request)\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\tio_request->io_complete_callback = pqi_aio_io_complete;\n\tio_request->scmd = scmd;\n\tio_request->raid_bypass = true;\n\n\tr56_request = io_request->iu;\n\tmemset(r56_request, 0, offsetof(struct pqi_aio_r56_path_request, sg_descriptors));\n\n\tif (device->raid_level == SA_RAID_5 || device->raid_level == SA_RAID_51)\n\t\tr56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID5_IO;\n\telse\n\t\tr56_request->header.iu_type = PQI_REQUEST_IU_AIO_PATH_RAID6_IO;\n\n\tput_unaligned_le16(*(u16 *)device->scsi3addr & 0x3fff, &r56_request->volume_id);\n\tput_unaligned_le32(rmd->aio_handle, &r56_request->data_it_nexus);\n\tput_unaligned_le32(rmd->p_parity_it_nexus, &r56_request->p_parity_it_nexus);\n\tif (rmd->raid_level == SA_RAID_6) {\n\t\tput_unaligned_le32(rmd->q_parity_it_nexus, &r56_request->q_parity_it_nexus);\n\t\tr56_request->xor_multiplier = rmd->xor_mult;\n\t}\n\tput_unaligned_le32(scsi_bufflen(scmd), &r56_request->data_length);\n\tr56_request->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\tput_unaligned_le64(rmd->row, &r56_request->row);\n\n\tput_unaligned_le16(io_request->index, &r56_request->request_id);\n\tr56_request->error_index = r56_request->request_id;\n\n\tif (rmd->cdb_length > sizeof(r56_request->cdb))\n\t\trmd->cdb_length = sizeof(r56_request->cdb);\n\tr56_request->cdb_length = rmd->cdb_length;\n\tmemcpy(r56_request->cdb, rmd->cdb, rmd->cdb_length);\n\n\t \n\tr56_request->data_direction = SOP_READ_FLAG;\n\n\tif (encryption_info) {\n\t\tr56_request->encryption_enable = true;\n\t\tput_unaligned_le16(encryption_info->data_encryption_key_index,\n\t\t\t\t&r56_request->data_encryption_key_index);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_lower,\n\t\t\t\t&r56_request->encrypt_tweak_lower);\n\t\tput_unaligned_le32(encryption_info->encrypt_tweak_upper,\n\t\t\t\t&r56_request->encrypt_tweak_upper);\n\t}\n\n\trc = pqi_build_aio_r56_sg_list(ctrl_info, r56_request, scmd, io_request);\n\tif (rc) {\n\t\tpqi_free_io_request(io_request);\n\t\treturn SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\tpqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);\n\n\treturn 0;\n}\n\nstatic inline u16 pqi_get_hw_queue(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd)\n{\n\t \n\treturn blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scsi_cmd_to_rq(scmd)));\n}\n\nstatic inline bool pqi_is_bypass_eligible_request(struct scsi_cmnd *scmd)\n{\n\tif (blk_rq_is_passthrough(scsi_cmd_to_rq(scmd)))\n\t\treturn false;\n\n\treturn pqi_cmd_priv(scmd)->this_residual == 0;\n}\n\n \n\nvoid pqi_prep_for_scsi_done(struct scsi_cmnd *scmd)\n{\n\tstruct pqi_scsi_dev *device;\n\tstruct completion *wait;\n\n\tif (!scmd->device) {\n\t\tset_host_byte(scmd, DID_NO_CONNECT);\n\t\treturn;\n\t}\n\n\tdevice = scmd->device->hostdata;\n\tif (!device) {\n\t\tset_host_byte(scmd, DID_NO_CONNECT);\n\t\treturn;\n\t}\n\n\tatomic_dec(&device->scsi_cmds_outstanding[scmd->device->lun]);\n\n\twait = (struct completion *)xchg(&scmd->host_scribble, NULL);\n\tif (wait != PQI_NO_COMPLETION)\n\t\tcomplete(wait);\n}\n\nstatic bool pqi_is_parity_write_stream(struct pqi_ctrl_info *ctrl_info,\n\tstruct scsi_cmnd *scmd)\n{\n\tu32 oldest_jiffies;\n\tu8 lru_index;\n\tint i;\n\tint rc;\n\tstruct pqi_scsi_dev *device;\n\tstruct pqi_stream_data *pqi_stream_data;\n\tstruct pqi_scsi_dev_raid_map_data rmd;\n\n\tif (!ctrl_info->enable_stream_detection)\n\t\treturn false;\n\n\trc = pqi_get_aio_lba_and_block_count(scmd, &rmd);\n\tif (rc)\n\t\treturn false;\n\n\t \n\tif (!rmd.is_write)\n\t\treturn false;\n\n\tdevice = scmd->device->hostdata;\n\n\t \n\tif (device->raid_level != SA_RAID_5 && device->raid_level != SA_RAID_6)\n\t\treturn false;\n\n\t \n\tif ((device->raid_level == SA_RAID_5 && !ctrl_info->enable_r5_writes) ||\n\t\t(device->raid_level == SA_RAID_6 && !ctrl_info->enable_r6_writes))\n\t\treturn true;\n\n\tlru_index = 0;\n\toldest_jiffies = INT_MAX;\n\tfor (i = 0; i < NUM_STREAMS_PER_LUN; i++) {\n\t\tpqi_stream_data = &device->stream_data[i];\n\t\t \n\t\tif ((pqi_stream_data->next_lba &&\n\t\t\trmd.first_block >= pqi_stream_data->next_lba) &&\n\t\t\trmd.first_block <= pqi_stream_data->next_lba +\n\t\t\t\trmd.block_cnt) {\n\t\t\tpqi_stream_data->next_lba = rmd.first_block +\n\t\t\t\trmd.block_cnt;\n\t\t\tpqi_stream_data->last_accessed = jiffies;\n\t\t\treturn true;\n\t\t}\n\n\t\t \n\t\tif (pqi_stream_data->last_accessed == 0) {\n\t\t\tlru_index = i;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (pqi_stream_data->last_accessed <= oldest_jiffies) {\n\t\t\toldest_jiffies = pqi_stream_data->last_accessed;\n\t\t\tlru_index = i;\n\t\t}\n\t}\n\n\t \n\tpqi_stream_data = &device->stream_data[lru_index];\n\tpqi_stream_data->last_accessed = jiffies;\n\tpqi_stream_data->next_lba = rmd.first_block + rmd.block_cnt;\n\n\treturn false;\n}\n\nstatic int pqi_scsi_queue_command(struct Scsi_Host *shost, struct scsi_cmnd *scmd)\n{\n\tint rc;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_scsi_dev *device;\n\tu16 hw_queue;\n\tstruct pqi_queue_group *queue_group;\n\tbool raid_bypassed;\n\tu8 lun;\n\n\tscmd->host_scribble = PQI_NO_COMPLETION;\n\n\tdevice = scmd->device->hostdata;\n\n\tif (!device) {\n\t\tset_host_byte(scmd, DID_NO_CONNECT);\n\t\tpqi_scsi_done(scmd);\n\t\treturn 0;\n\t}\n\n\tlun = (u8)scmd->device->lun;\n\n\tatomic_inc(&device->scsi_cmds_outstanding[lun]);\n\n\tctrl_info = shost_to_hba(shost);\n\n\tif (pqi_ctrl_offline(ctrl_info) || pqi_device_in_remove(device)) {\n\t\tset_host_byte(scmd, DID_NO_CONNECT);\n\t\tpqi_scsi_done(scmd);\n\t\treturn 0;\n\t}\n\n\tif (pqi_ctrl_blocked(ctrl_info) || pqi_device_in_reset(device, lun)) {\n\t\trc = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tscmd->result = 0;\n\n\thw_queue = pqi_get_hw_queue(ctrl_info, scmd);\n\tqueue_group = &ctrl_info->queue_groups[hw_queue];\n\n\tif (pqi_is_logical_device(device)) {\n\t\traid_bypassed = false;\n\t\tif (device->raid_bypass_enabled &&\n\t\t\tpqi_is_bypass_eligible_request(scmd) &&\n\t\t\t!pqi_is_parity_write_stream(ctrl_info, scmd)) {\n\t\t\trc = pqi_raid_bypass_submit_scsi_cmd(ctrl_info, device, scmd, queue_group);\n\t\t\tif (rc == 0 || rc == SCSI_MLQUEUE_HOST_BUSY) {\n\t\t\t\traid_bypassed = true;\n\t\t\t\tdevice->raid_bypass_cnt++;\n\t\t\t}\n\t\t}\n\t\tif (!raid_bypassed)\n\t\t\trc = pqi_raid_submit_scsi_cmd(ctrl_info, device, scmd, queue_group);\n\t} else {\n\t\tif (device->aio_enabled)\n\t\t\trc = pqi_aio_submit_scsi_cmd(ctrl_info, device, scmd, queue_group);\n\t\telse\n\t\t\trc = pqi_raid_submit_scsi_cmd(ctrl_info, device, scmd, queue_group);\n\t}\n\nout:\n\tif (rc) {\n\t\tscmd->host_scribble = NULL;\n\t\tatomic_dec(&device->scsi_cmds_outstanding[lun]);\n\t}\n\n\treturn rc;\n}\n\nstatic unsigned int pqi_queued_io_count(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tunsigned int path;\n\tunsigned long flags;\n\tunsigned int queued_io_count;\n\tstruct pqi_queue_group *queue_group;\n\tstruct pqi_io_request *io_request;\n\n\tqueued_io_count = 0;\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\t\tfor (path = 0; path < 2; path++) {\n\t\t\tspin_lock_irqsave(&queue_group->submit_lock[path], flags);\n\t\t\tlist_for_each_entry(io_request, &queue_group->request_list[path], request_list_entry)\n\t\t\t\tqueued_io_count++;\n\t\t\tspin_unlock_irqrestore(&queue_group->submit_lock[path], flags);\n\t\t}\n\t}\n\n\treturn queued_io_count;\n}\n\nstatic unsigned int pqi_nonempty_inbound_queue_count(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tunsigned int path;\n\tunsigned int nonempty_inbound_queue_count;\n\tstruct pqi_queue_group *queue_group;\n\tpqi_index_t iq_pi;\n\tpqi_index_t iq_ci;\n\n\tnonempty_inbound_queue_count = 0;\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\t\tfor (path = 0; path < 2; path++) {\n\t\t\tiq_pi = queue_group->iq_pi_copy[path];\n\t\t\tiq_ci = readl(queue_group->iq_ci[path]);\n\t\t\tif (iq_ci != iq_pi)\n\t\t\t\tnonempty_inbound_queue_count++;\n\t\t}\n\t}\n\n\treturn nonempty_inbound_queue_count;\n}\n\n#define PQI_INBOUND_QUEUES_NONEMPTY_WARNING_TIMEOUT_SECS\t10\n\nstatic int pqi_wait_until_inbound_queues_empty(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned long start_jiffies;\n\tunsigned long warning_timeout;\n\tunsigned int queued_io_count;\n\tunsigned int nonempty_inbound_queue_count;\n\tbool displayed_warning;\n\n\tdisplayed_warning = false;\n\tstart_jiffies = jiffies;\n\twarning_timeout = (PQI_INBOUND_QUEUES_NONEMPTY_WARNING_TIMEOUT_SECS * HZ) + start_jiffies;\n\n\twhile (1) {\n\t\tqueued_io_count = pqi_queued_io_count(ctrl_info);\n\t\tnonempty_inbound_queue_count = pqi_nonempty_inbound_queue_count(ctrl_info);\n\t\tif (queued_io_count == 0 && nonempty_inbound_queue_count == 0)\n\t\t\tbreak;\n\t\tpqi_check_ctrl_health(ctrl_info);\n\t\tif (pqi_ctrl_offline(ctrl_info))\n\t\t\treturn -ENXIO;\n\t\tif (time_after(jiffies, warning_timeout)) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"waiting %u seconds for queued I/O to drain (queued I/O count: %u; non-empty inbound queue count: %u)\\n\",\n\t\t\t\tjiffies_to_msecs(jiffies - start_jiffies) / 1000, queued_io_count, nonempty_inbound_queue_count);\n\t\t\tdisplayed_warning = true;\n\t\t\twarning_timeout = (PQI_INBOUND_QUEUES_NONEMPTY_WARNING_TIMEOUT_SECS * HZ) + jiffies;\n\t\t}\n\t\tusleep_range(1000, 2000);\n\t}\n\n\tif (displayed_warning)\n\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\"queued I/O drained after waiting for %u seconds\\n\",\n\t\t\tjiffies_to_msecs(jiffies - start_jiffies) / 1000);\n\n\treturn 0;\n}\n\nstatic void pqi_fail_io_queued_for_device(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, u8 lun)\n{\n\tunsigned int i;\n\tunsigned int path;\n\tstruct pqi_queue_group *queue_group;\n\tunsigned long flags;\n\tstruct pqi_io_request *io_request;\n\tstruct pqi_io_request *next;\n\tstruct scsi_cmnd *scmd;\n\tstruct pqi_scsi_dev *scsi_device;\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tqueue_group = &ctrl_info->queue_groups[i];\n\n\t\tfor (path = 0; path < 2; path++) {\n\t\t\tspin_lock_irqsave(\n\t\t\t\t&queue_group->submit_lock[path], flags);\n\n\t\t\tlist_for_each_entry_safe(io_request, next,\n\t\t\t\t&queue_group->request_list[path],\n\t\t\t\trequest_list_entry) {\n\n\t\t\t\tscmd = io_request->scmd;\n\t\t\t\tif (!scmd)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tscsi_device = scmd->device->hostdata;\n\t\t\t\tif (scsi_device != device)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif ((u8)scmd->device->lun != lun)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tlist_del(&io_request->request_list_entry);\n\t\t\t\tset_host_byte(scmd, DID_RESET);\n\t\t\t\tpqi_free_io_request(io_request);\n\t\t\t\tscsi_dma_unmap(scmd);\n\t\t\t\tpqi_scsi_done(scmd);\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&queue_group->submit_lock[path], flags);\n\t\t}\n\t}\n}\n\n#define PQI_PENDING_IO_WARNING_TIMEOUT_SECS\t10\n\nstatic int pqi_device_wait_for_pending_io(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, u8 lun, unsigned long timeout_msecs)\n{\n\tint cmds_outstanding;\n\tunsigned long start_jiffies;\n\tunsigned long warning_timeout;\n\tunsigned long msecs_waiting;\n\n\tstart_jiffies = jiffies;\n\twarning_timeout = (PQI_PENDING_IO_WARNING_TIMEOUT_SECS * HZ) + start_jiffies;\n\n\twhile ((cmds_outstanding = atomic_read(&device->scsi_cmds_outstanding[lun])) > 0) {\n\t\tif (ctrl_info->ctrl_removal_state != PQI_CTRL_GRACEFUL_REMOVAL) {\n\t\t\tpqi_check_ctrl_health(ctrl_info);\n\t\t\tif (pqi_ctrl_offline(ctrl_info))\n\t\t\t\treturn -ENXIO;\n\t\t}\n\t\tmsecs_waiting = jiffies_to_msecs(jiffies - start_jiffies);\n\t\tif (msecs_waiting >= timeout_msecs) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"scsi %d:%d:%d:%d: timed out after %lu seconds waiting for %d outstanding command(s)\\n\",\n\t\t\t\tctrl_info->scsi_host->host_no, device->bus, device->target,\n\t\t\t\tlun, msecs_waiting / 1000, cmds_outstanding);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tif (time_after(jiffies, warning_timeout)) {\n\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"scsi %d:%d:%d:%d: waiting %lu seconds for %d outstanding command(s)\\n\",\n\t\t\t\tctrl_info->scsi_host->host_no, device->bus, device->target,\n\t\t\t\tlun, msecs_waiting / 1000, cmds_outstanding);\n\t\t\twarning_timeout = (PQI_PENDING_IO_WARNING_TIMEOUT_SECS * HZ) + jiffies;\n\t\t}\n\t\tusleep_range(1000, 2000);\n\t}\n\n\treturn 0;\n}\n\nstatic void pqi_lun_reset_complete(struct pqi_io_request *io_request,\n\tvoid *context)\n{\n\tstruct completion *waiting = context;\n\n\tcomplete(waiting);\n}\n\n#define PQI_LUN_RESET_POLL_COMPLETION_SECS\t10\n\nstatic int pqi_wait_for_lun_reset_completion(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_scsi_dev *device, u8 lun, struct completion *wait)\n{\n\tint rc;\n\tunsigned int wait_secs;\n\tint cmds_outstanding;\n\n\twait_secs = 0;\n\n\twhile (1) {\n\t\tif (wait_for_completion_io_timeout(wait,\n\t\t\tPQI_LUN_RESET_POLL_COMPLETION_SECS * HZ)) {\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tpqi_check_ctrl_health(ctrl_info);\n\t\tif (pqi_ctrl_offline(ctrl_info)) {\n\t\t\trc = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\n\t\twait_secs += PQI_LUN_RESET_POLL_COMPLETION_SECS;\n\t\tcmds_outstanding = atomic_read(&device->scsi_cmds_outstanding[lun]);\n\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\"scsi %d:%d:%d:%d: waiting %u seconds for LUN reset to complete (%d command(s) outstanding)\\n\",\n\t\t\tctrl_info->scsi_host->host_no, device->bus, device->target, lun, wait_secs, cmds_outstanding);\n\t}\n\n\treturn rc;\n}\n\n#define PQI_LUN_RESET_FIRMWARE_TIMEOUT_SECS\t30\n\nstatic int pqi_lun_reset(struct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *device, u8 lun)\n{\n\tint rc;\n\tstruct pqi_io_request *io_request;\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\tstruct pqi_task_management_request *request;\n\n\tio_request = pqi_alloc_io_request(ctrl_info, NULL);\n\tio_request->io_complete_callback = pqi_lun_reset_complete;\n\tio_request->context = &wait;\n\n\trequest = io_request->iu;\n\tmemset(request, 0, sizeof(*request));\n\n\trequest->header.iu_type = PQI_REQUEST_IU_TASK_MANAGEMENT;\n\tput_unaligned_le16(sizeof(*request) - PQI_REQUEST_HEADER_LENGTH,\n\t\t&request->header.iu_length);\n\tput_unaligned_le16(io_request->index, &request->request_id);\n\tmemcpy(request->lun_number, device->scsi3addr,\n\t\tsizeof(request->lun_number));\n\tif (!pqi_is_logical_device(device) && ctrl_info->multi_lun_device_supported)\n\t\trequest->ml_device_lun_number = lun;\n\trequest->task_management_function = SOP_TASK_MANAGEMENT_LUN_RESET;\n\tif (ctrl_info->tmf_iu_timeout_supported)\n\t\tput_unaligned_le16(PQI_LUN_RESET_FIRMWARE_TIMEOUT_SECS, &request->timeout);\n\n\tpqi_start_io(ctrl_info, &ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP], RAID_PATH,\n\t\tio_request);\n\n\trc = pqi_wait_for_lun_reset_completion(ctrl_info, device, lun, &wait);\n\tif (rc == 0)\n\t\trc = io_request->status;\n\n\tpqi_free_io_request(io_request);\n\n\treturn rc;\n}\n\n#define PQI_LUN_RESET_RETRIES\t\t\t\t3\n#define PQI_LUN_RESET_RETRY_INTERVAL_MSECS\t\t(10 * 1000)\n#define PQI_LUN_RESET_PENDING_IO_TIMEOUT_MSECS\t\t(10 * 60 * 1000)\n#define PQI_LUN_RESET_FAILED_PENDING_IO_TIMEOUT_MSECS\t(2 * 60 * 1000)\n\nstatic int pqi_lun_reset_with_retries(struct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *device, u8 lun)\n{\n\tint reset_rc;\n\tint wait_rc;\n\tunsigned int retries;\n\tunsigned long timeout_msecs;\n\n\tfor (retries = 0;;) {\n\t\treset_rc = pqi_lun_reset(ctrl_info, device, lun);\n\t\tif (reset_rc == 0 || reset_rc == -ENODEV || reset_rc == -ENXIO || ++retries > PQI_LUN_RESET_RETRIES)\n\t\t\tbreak;\n\t\tmsleep(PQI_LUN_RESET_RETRY_INTERVAL_MSECS);\n\t}\n\n\ttimeout_msecs = reset_rc ? PQI_LUN_RESET_FAILED_PENDING_IO_TIMEOUT_MSECS :\n\t\tPQI_LUN_RESET_PENDING_IO_TIMEOUT_MSECS;\n\n\twait_rc = pqi_device_wait_for_pending_io(ctrl_info, device, lun, timeout_msecs);\n\tif (wait_rc && reset_rc == 0)\n\t\treset_rc = wait_rc;\n\n\treturn reset_rc == 0 ? SUCCESS : FAILED;\n}\n\nstatic int pqi_device_reset(struct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *device, u8 lun)\n{\n\tint rc;\n\n\tpqi_ctrl_block_requests(ctrl_info);\n\tpqi_ctrl_wait_until_quiesced(ctrl_info);\n\tpqi_fail_io_queued_for_device(ctrl_info, device, lun);\n\trc = pqi_wait_until_inbound_queues_empty(ctrl_info);\n\tpqi_device_reset_start(device, lun);\n\tpqi_ctrl_unblock_requests(ctrl_info);\n\tif (rc)\n\t\trc = FAILED;\n\telse\n\t\trc = pqi_lun_reset_with_retries(ctrl_info, device, lun);\n\tpqi_device_reset_done(device, lun);\n\n\treturn rc;\n}\n\nstatic int pqi_device_reset_handler(struct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *device, u8 lun, struct scsi_cmnd *scmd, u8 scsi_opcode)\n{\n\tint rc;\n\n\tmutex_lock(&ctrl_info->lun_reset_mutex);\n\n\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\"resetting scsi %d:%d:%d:%u SCSI cmd at %p due to cmd opcode 0x%02x\\n\",\n\t\tctrl_info->scsi_host->host_no, device->bus, device->target, lun, scmd, scsi_opcode);\n\n\tpqi_check_ctrl_health(ctrl_info);\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\trc = FAILED;\n\telse\n\t\trc = pqi_device_reset(ctrl_info, device, lun);\n\n\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\"reset of scsi %d:%d:%d:%u: %s\\n\",\n\t\tctrl_info->scsi_host->host_no, device->bus, device->target, lun,\n\t\trc == SUCCESS ? \"SUCCESS\" : \"FAILED\");\n\n\tmutex_unlock(&ctrl_info->lun_reset_mutex);\n\n\treturn rc;\n}\n\nstatic int pqi_eh_device_reset_handler(struct scsi_cmnd *scmd)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_scsi_dev *device;\n\tu8 scsi_opcode;\n\n\tshost = scmd->device->host;\n\tctrl_info = shost_to_hba(shost);\n\tdevice = scmd->device->hostdata;\n\tscsi_opcode = scmd->cmd_len > 0 ? scmd->cmnd[0] : 0xff;\n\n\treturn pqi_device_reset_handler(ctrl_info, device, (u8)scmd->device->lun, scmd, scsi_opcode);\n}\n\nstatic void pqi_tmf_worker(struct work_struct *work)\n{\n\tstruct pqi_tmf_work *tmf_work;\n\tstruct scsi_cmnd *scmd;\n\n\ttmf_work = container_of(work, struct pqi_tmf_work, work_struct);\n\tscmd = (struct scsi_cmnd *)xchg(&tmf_work->scmd, NULL);\n\n\tpqi_device_reset_handler(tmf_work->ctrl_info, tmf_work->device, tmf_work->lun, scmd, tmf_work->scsi_opcode);\n}\n\nstatic int pqi_eh_abort_handler(struct scsi_cmnd *scmd)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_scsi_dev *device;\n\tstruct pqi_tmf_work *tmf_work;\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\n\tshost = scmd->device->host;\n\tctrl_info = shost_to_hba(shost);\n\tdevice = scmd->device->hostdata;\n\n\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\"attempting TASK ABORT on scsi %d:%d:%d:%d for SCSI cmd at %p\\n\",\n\t\tshost->host_no, device->bus, device->target, (int)scmd->device->lun, scmd);\n\n\tif (cmpxchg(&scmd->host_scribble, PQI_NO_COMPLETION, (void *)&wait) == NULL) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"scsi %d:%d:%d:%d for SCSI cmd at %p already completed\\n\",\n\t\t\tshost->host_no, device->bus, device->target, (int)scmd->device->lun, scmd);\n\t\tscmd->result = DID_RESET << 16;\n\t\tgoto out;\n\t}\n\n\ttmf_work = &device->tmf_work[scmd->device->lun];\n\n\tif (cmpxchg(&tmf_work->scmd, NULL, scmd) == NULL) {\n\t\ttmf_work->ctrl_info = ctrl_info;\n\t\ttmf_work->device = device;\n\t\ttmf_work->lun = (u8)scmd->device->lun;\n\t\ttmf_work->scsi_opcode = scmd->cmd_len > 0 ? scmd->cmnd[0] : 0xff;\n\t\tschedule_work(&tmf_work->work_struct);\n\t}\n\n\twait_for_completion(&wait);\n\n\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\"TASK ABORT on scsi %d:%d:%d:%d for SCSI cmd at %p: SUCCESS\\n\",\n\t\tshost->host_no, device->bus, device->target, (int)scmd->device->lun, scmd);\n\nout:\n\n\treturn SUCCESS;\n}\n\nstatic int pqi_slave_alloc(struct scsi_device *sdev)\n{\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_target *starget;\n\tstruct sas_rphy *rphy;\n\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tif (sdev_channel(sdev) == PQI_PHYSICAL_DEVICE_BUS) {\n\t\tstarget = scsi_target(sdev);\n\t\trphy = target_to_rphy(starget);\n\t\tdevice = pqi_find_device_by_sas_rphy(ctrl_info, rphy);\n\t\tif (device) {\n\t\t\tif (device->target_lun_valid) {\n\t\t\t\tdevice->ignore_device = true;\n\t\t\t} else {\n\t\t\t\tdevice->target = sdev_id(sdev);\n\t\t\t\tdevice->lun = sdev->lun;\n\t\t\t\tdevice->target_lun_valid = true;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdevice = pqi_find_scsi_dev(ctrl_info, sdev_channel(sdev),\n\t\t\tsdev_id(sdev), sdev->lun);\n\t}\n\n\tif (device) {\n\t\tsdev->hostdata = device;\n\t\tdevice->sdev = sdev;\n\t\tif (device->queue_depth) {\n\t\t\tdevice->advertised_queue_depth = device->queue_depth;\n\t\t\tscsi_change_queue_depth(sdev,\n\t\t\t\tdevice->advertised_queue_depth);\n\t\t}\n\t\tif (pqi_is_logical_device(device)) {\n\t\t\tpqi_disable_write_same(sdev);\n\t\t} else {\n\t\t\tsdev->allow_restart = 1;\n\t\t\tif (device->device_type == SA_DEVICE_TYPE_NVME)\n\t\t\t\tpqi_disable_write_same(sdev);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn 0;\n}\n\nstatic void pqi_map_queues(struct Scsi_Host *shost)\n{\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\n\tblk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],\n\t\t\t      ctrl_info->pci_dev, 0);\n}\n\nstatic inline bool pqi_is_tape_changer_device(struct pqi_scsi_dev *device)\n{\n\treturn device->devtype == TYPE_TAPE || device->devtype == TYPE_MEDIUM_CHANGER;\n}\n\nstatic int pqi_slave_configure(struct scsi_device *sdev)\n{\n\tint rc = 0;\n\tstruct pqi_scsi_dev *device;\n\n\tdevice = sdev->hostdata;\n\tdevice->devtype = sdev->type;\n\n\tif (pqi_is_tape_changer_device(device) && device->ignore_device) {\n\t\trc = -ENXIO;\n\t\tdevice->ignore_device = false;\n\t}\n\n\treturn rc;\n}\n\nstatic void pqi_slave_destroy(struct scsi_device *sdev)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_scsi_dev *device;\n\tint mutex_acquired;\n\tunsigned long flags;\n\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tmutex_acquired = mutex_trylock(&ctrl_info->scan_mutex);\n\tif (!mutex_acquired)\n\t\treturn;\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tmutex_unlock(&ctrl_info->scan_mutex);\n\t\treturn;\n\t}\n\n\tdevice->lun_count--;\n\tif (device->lun_count > 0) {\n\t\tmutex_unlock(&ctrl_info->scan_mutex);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\tlist_del(&device->scsi_device_list_entry);\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\tmutex_unlock(&ctrl_info->scan_mutex);\n\n\tpqi_dev_info(ctrl_info, \"removed\", device);\n\tpqi_free_device(device);\n}\n\nstatic int pqi_getpciinfo_ioctl(struct pqi_ctrl_info *ctrl_info, void __user *arg)\n{\n\tstruct pci_dev *pci_dev;\n\tu32 subsystem_vendor;\n\tu32 subsystem_device;\n\tcciss_pci_info_struct pci_info;\n\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\tpci_dev = ctrl_info->pci_dev;\n\n\tpci_info.domain = pci_domain_nr(pci_dev->bus);\n\tpci_info.bus = pci_dev->bus->number;\n\tpci_info.dev_fn = pci_dev->devfn;\n\tsubsystem_vendor = pci_dev->subsystem_vendor;\n\tsubsystem_device = pci_dev->subsystem_device;\n\tpci_info.board_id = ((subsystem_device << 16) & 0xffff0000) | subsystem_vendor;\n\n\tif (copy_to_user(arg, &pci_info, sizeof(pci_info)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int pqi_getdrivver_ioctl(void __user *arg)\n{\n\tu32 version;\n\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\tversion = (DRIVER_MAJOR << 28) | (DRIVER_MINOR << 24) |\n\t\t(DRIVER_RELEASE << 16) | DRIVER_REVISION;\n\n\tif (copy_to_user(arg, &version, sizeof(version)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstruct ciss_error_info {\n\tu8\tscsi_status;\n\tint\tcommand_status;\n\tsize_t\tsense_data_length;\n};\n\nstatic void pqi_error_info_to_ciss(struct pqi_raid_error_info *pqi_error_info,\n\tstruct ciss_error_info *ciss_error_info)\n{\n\tint ciss_cmd_status;\n\tsize_t sense_data_length;\n\n\tswitch (pqi_error_info->data_out_result) {\n\tcase PQI_DATA_IN_OUT_GOOD:\n\t\tciss_cmd_status = CISS_CMD_STATUS_SUCCESS;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_UNDERFLOW:\n\t\tciss_cmd_status = CISS_CMD_STATUS_DATA_UNDERRUN;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW:\n\t\tciss_cmd_status = CISS_CMD_STATUS_DATA_OVERRUN;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_PROTOCOL_ERROR:\n\tcase PQI_DATA_IN_OUT_BUFFER_ERROR:\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_DESCRIPTOR_AREA:\n\tcase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_BRIDGE:\n\tcase PQI_DATA_IN_OUT_ERROR:\n\t\tciss_cmd_status = CISS_CMD_STATUS_PROTOCOL_ERROR;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_HARDWARE_ERROR:\n\tcase PQI_DATA_IN_OUT_PCIE_FABRIC_ERROR:\n\tcase PQI_DATA_IN_OUT_PCIE_COMPLETION_TIMEOUT:\n\tcase PQI_DATA_IN_OUT_PCIE_COMPLETER_ABORT_RECEIVED:\n\tcase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST_RECEIVED:\n\tcase PQI_DATA_IN_OUT_PCIE_ECRC_CHECK_FAILED:\n\tcase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST:\n\tcase PQI_DATA_IN_OUT_PCIE_ACS_VIOLATION:\n\tcase PQI_DATA_IN_OUT_PCIE_TLP_PREFIX_BLOCKED:\n\tcase PQI_DATA_IN_OUT_PCIE_POISONED_MEMORY_READ:\n\t\tciss_cmd_status = CISS_CMD_STATUS_HARDWARE_ERROR;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_UNSOLICITED_ABORT:\n\t\tciss_cmd_status = CISS_CMD_STATUS_UNSOLICITED_ABORT;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_ABORTED:\n\t\tciss_cmd_status = CISS_CMD_STATUS_ABORTED;\n\t\tbreak;\n\tcase PQI_DATA_IN_OUT_TIMEOUT:\n\t\tciss_cmd_status = CISS_CMD_STATUS_TIMEOUT;\n\t\tbreak;\n\tdefault:\n\t\tciss_cmd_status = CISS_CMD_STATUS_TARGET_STATUS;\n\t\tbreak;\n\t}\n\n\tsense_data_length =\n\t\tget_unaligned_le16(&pqi_error_info->sense_data_length);\n\tif (sense_data_length == 0)\n\t\tsense_data_length =\n\t\tget_unaligned_le16(&pqi_error_info->response_data_length);\n\tif (sense_data_length)\n\t\tif (sense_data_length > sizeof(pqi_error_info->data))\n\t\t\tsense_data_length = sizeof(pqi_error_info->data);\n\n\tciss_error_info->scsi_status = pqi_error_info->status;\n\tciss_error_info->command_status = ciss_cmd_status;\n\tciss_error_info->sense_data_length = sense_data_length;\n}\n\nstatic int pqi_passthru_ioctl(struct pqi_ctrl_info *ctrl_info, void __user *arg)\n{\n\tint rc;\n\tchar *kernel_buffer = NULL;\n\tu16 iu_length;\n\tsize_t sense_data_length;\n\tIOCTL_Command_struct iocommand;\n\tstruct pqi_raid_path_request request;\n\tstruct pqi_raid_error_info pqi_error_info;\n\tstruct ciss_error_info ciss_error_info;\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENXIO;\n\tif (pqi_ofa_in_progress(ctrl_info) && pqi_ctrl_blocked(ctrl_info))\n\t\treturn -EBUSY;\n\tif (!arg)\n\t\treturn -EINVAL;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\tif (copy_from_user(&iocommand, arg, sizeof(iocommand)))\n\t\treturn -EFAULT;\n\tif (iocommand.buf_size < 1 &&\n\t\tiocommand.Request.Type.Direction != XFER_NONE)\n\t\treturn -EINVAL;\n\tif (iocommand.Request.CDBLen > sizeof(request.cdb))\n\t\treturn -EINVAL;\n\tif (iocommand.Request.Type.Type != TYPE_CMD)\n\t\treturn -EINVAL;\n\n\tswitch (iocommand.Request.Type.Direction) {\n\tcase XFER_NONE:\n\tcase XFER_WRITE:\n\tcase XFER_READ:\n\tcase XFER_READ | XFER_WRITE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (iocommand.buf_size > 0) {\n\t\tkernel_buffer = kmalloc(iocommand.buf_size, GFP_KERNEL);\n\t\tif (!kernel_buffer)\n\t\t\treturn -ENOMEM;\n\t\tif (iocommand.Request.Type.Direction & XFER_WRITE) {\n\t\t\tif (copy_from_user(kernel_buffer, iocommand.buf,\n\t\t\t\tiocommand.buf_size)) {\n\t\t\t\trc = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tmemset(kernel_buffer, 0, iocommand.buf_size);\n\t\t}\n\t}\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\n\tiu_length = offsetof(struct pqi_raid_path_request, sg_descriptors) -\n\t\tPQI_REQUEST_HEADER_LENGTH;\n\tmemcpy(request.lun_number, iocommand.LUN_info.LunAddrBytes,\n\t\tsizeof(request.lun_number));\n\tmemcpy(request.cdb, iocommand.Request.CDB, iocommand.Request.CDBLen);\n\trequest.additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_0;\n\n\tswitch (iocommand.Request.Type.Direction) {\n\tcase XFER_NONE:\n\t\trequest.data_direction = SOP_NO_DIRECTION_FLAG;\n\t\tbreak;\n\tcase XFER_WRITE:\n\t\trequest.data_direction = SOP_WRITE_FLAG;\n\t\tbreak;\n\tcase XFER_READ:\n\t\trequest.data_direction = SOP_READ_FLAG;\n\t\tbreak;\n\tcase XFER_READ | XFER_WRITE:\n\t\trequest.data_direction = SOP_BIDIRECTIONAL;\n\t\tbreak;\n\t}\n\n\trequest.task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\n\n\tif (iocommand.buf_size > 0) {\n\t\tput_unaligned_le32(iocommand.buf_size, &request.buffer_length);\n\n\t\trc = pqi_map_single(ctrl_info->pci_dev,\n\t\t\t&request.sg_descriptors[0], kernel_buffer,\n\t\t\tiocommand.buf_size, DMA_BIDIRECTIONAL);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\tiu_length += sizeof(request.sg_descriptors[0]);\n\t}\n\n\tput_unaligned_le16(iu_length, &request.header.iu_length);\n\n\tif (ctrl_info->raid_iu_timeout_supported)\n\t\tput_unaligned_le32(iocommand.Request.Timeout, &request.timeout);\n\n\trc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\n\t\tPQI_SYNC_FLAGS_INTERRUPTABLE, &pqi_error_info);\n\n\tif (iocommand.buf_size > 0)\n\t\tpqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\n\t\t\tDMA_BIDIRECTIONAL);\n\n\tmemset(&iocommand.error_info, 0, sizeof(iocommand.error_info));\n\n\tif (rc == 0) {\n\t\tpqi_error_info_to_ciss(&pqi_error_info, &ciss_error_info);\n\t\tiocommand.error_info.ScsiStatus = ciss_error_info.scsi_status;\n\t\tiocommand.error_info.CommandStatus =\n\t\t\tciss_error_info.command_status;\n\t\tsense_data_length = ciss_error_info.sense_data_length;\n\t\tif (sense_data_length) {\n\t\t\tif (sense_data_length >\n\t\t\t\tsizeof(iocommand.error_info.SenseInfo))\n\t\t\t\tsense_data_length =\n\t\t\t\t\tsizeof(iocommand.error_info.SenseInfo);\n\t\t\tmemcpy(iocommand.error_info.SenseInfo,\n\t\t\t\tpqi_error_info.data, sense_data_length);\n\t\t\tiocommand.error_info.SenseLen = sense_data_length;\n\t\t}\n\t}\n\n\tif (copy_to_user(arg, &iocommand, sizeof(iocommand))) {\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (rc == 0 && iocommand.buf_size > 0 &&\n\t\t(iocommand.Request.Type.Direction & XFER_READ)) {\n\t\tif (copy_to_user(iocommand.buf, kernel_buffer,\n\t\t\tiocommand.buf_size)) {\n\t\t\trc = -EFAULT;\n\t\t}\n\t}\n\nout:\n\tkfree(kernel_buffer);\n\n\treturn rc;\n}\n\nstatic int pqi_ioctl(struct scsi_device *sdev, unsigned int cmd,\n\t\t     void __user *arg)\n{\n\tint rc;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tswitch (cmd) {\n\tcase CCISS_DEREGDISK:\n\tcase CCISS_REGNEWDISK:\n\tcase CCISS_REGNEWD:\n\t\trc = pqi_scan_scsi_devices(ctrl_info);\n\t\tbreak;\n\tcase CCISS_GETPCIINFO:\n\t\trc = pqi_getpciinfo_ioctl(ctrl_info, arg);\n\t\tbreak;\n\tcase CCISS_GETDRIVVER:\n\t\trc = pqi_getdrivver_ioctl(arg);\n\t\tbreak;\n\tcase CCISS_PASSTHRU:\n\t\trc = pqi_passthru_ioctl(ctrl_info, arg);\n\t\tbreak;\n\tdefault:\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nstatic ssize_t pqi_firmware_version_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tshost = class_to_shost(dev);\n\tctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", ctrl_info->firmware_version);\n}\n\nstatic ssize_t pqi_driver_version_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", DRIVER_VERSION BUILD_TIMESTAMP);\n}\n\nstatic ssize_t pqi_serial_number_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tshost = class_to_shost(dev);\n\tctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", ctrl_info->serial_number);\n}\n\nstatic ssize_t pqi_model_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tshost = class_to_shost(dev);\n\tctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", ctrl_info->model);\n}\n\nstatic ssize_t pqi_vendor_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tshost = class_to_shost(dev);\n\tctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", ctrl_info->vendor);\n}\n\nstatic ssize_t pqi_host_rescan_store(struct device *dev,\n\tstruct device_attribute *attr, const char *buffer, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\n\tpqi_scan_start(shost);\n\n\treturn count;\n}\n\nstatic ssize_t pqi_lockup_action_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tint count = 0;\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_lockup_actions); i++) {\n\t\tif (pqi_lockup_actions[i].action == pqi_lockup_action)\n\t\t\tcount += scnprintf(buffer + count, PAGE_SIZE - count,\n\t\t\t\t\"[%s] \", pqi_lockup_actions[i].name);\n\t\telse\n\t\t\tcount += scnprintf(buffer + count, PAGE_SIZE - count,\n\t\t\t\t\"%s \", pqi_lockup_actions[i].name);\n\t}\n\n\tcount += scnprintf(buffer + count, PAGE_SIZE - count, \"\\n\");\n\n\treturn count;\n}\n\nstatic ssize_t pqi_lockup_action_store(struct device *dev,\n\tstruct device_attribute *attr, const char *buffer, size_t count)\n{\n\tunsigned int i;\n\tchar *action_name;\n\tchar action_name_buffer[32];\n\n\tstrscpy(action_name_buffer, buffer, sizeof(action_name_buffer));\n\taction_name = strstrip(action_name_buffer);\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_lockup_actions); i++) {\n\t\tif (strcmp(action_name, pqi_lockup_actions[i].name) == 0) {\n\t\t\tpqi_lockup_action = pqi_lockup_actions[i].action;\n\t\t\treturn count;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic ssize_t pqi_host_enable_stream_detection_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, 10, \"%x\\n\",\n\t\t\tctrl_info->enable_stream_detection);\n}\n\nstatic ssize_t pqi_host_enable_stream_detection_store(struct device *dev,\n\tstruct device_attribute *attr, const char *buffer, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\tu8 set_stream_detection = 0;\n\n\tif (kstrtou8(buffer, 0, &set_stream_detection))\n\t\treturn -EINVAL;\n\n\tif (set_stream_detection > 0)\n\t\tset_stream_detection = 1;\n\n\tctrl_info->enable_stream_detection = set_stream_detection;\n\n\treturn count;\n}\n\nstatic ssize_t pqi_host_enable_r5_writes_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, 10, \"%x\\n\", ctrl_info->enable_r5_writes);\n}\n\nstatic ssize_t pqi_host_enable_r5_writes_store(struct device *dev,\n\tstruct device_attribute *attr, const char *buffer, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\tu8 set_r5_writes = 0;\n\n\tif (kstrtou8(buffer, 0, &set_r5_writes))\n\t\treturn -EINVAL;\n\n\tif (set_r5_writes > 0)\n\t\tset_r5_writes = 1;\n\n\tctrl_info->enable_r5_writes = set_r5_writes;\n\n\treturn count;\n}\n\nstatic ssize_t pqi_host_enable_r6_writes_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\n\treturn scnprintf(buffer, 10, \"%x\\n\", ctrl_info->enable_r6_writes);\n}\n\nstatic ssize_t pqi_host_enable_r6_writes_store(struct device *dev,\n\tstruct device_attribute *attr, const char *buffer, size_t count)\n{\n\tstruct Scsi_Host *shost = class_to_shost(dev);\n\tstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\n\tu8 set_r6_writes = 0;\n\n\tif (kstrtou8(buffer, 0, &set_r6_writes))\n\t\treturn -EINVAL;\n\n\tif (set_r6_writes > 0)\n\t\tset_r6_writes = 1;\n\n\tctrl_info->enable_r6_writes = set_r6_writes;\n\n\treturn count;\n}\n\nstatic DEVICE_ATTR(driver_version, 0444, pqi_driver_version_show, NULL);\nstatic DEVICE_ATTR(firmware_version, 0444, pqi_firmware_version_show, NULL);\nstatic DEVICE_ATTR(model, 0444, pqi_model_show, NULL);\nstatic DEVICE_ATTR(serial_number, 0444, pqi_serial_number_show, NULL);\nstatic DEVICE_ATTR(vendor, 0444, pqi_vendor_show, NULL);\nstatic DEVICE_ATTR(rescan, 0200, NULL, pqi_host_rescan_store);\nstatic DEVICE_ATTR(lockup_action, 0644, pqi_lockup_action_show,\n\tpqi_lockup_action_store);\nstatic DEVICE_ATTR(enable_stream_detection, 0644,\n\tpqi_host_enable_stream_detection_show,\n\tpqi_host_enable_stream_detection_store);\nstatic DEVICE_ATTR(enable_r5_writes, 0644,\n\tpqi_host_enable_r5_writes_show, pqi_host_enable_r5_writes_store);\nstatic DEVICE_ATTR(enable_r6_writes, 0644,\n\tpqi_host_enable_r6_writes_show, pqi_host_enable_r6_writes_store);\n\nstatic struct attribute *pqi_shost_attrs[] = {\n\t&dev_attr_driver_version.attr,\n\t&dev_attr_firmware_version.attr,\n\t&dev_attr_model.attr,\n\t&dev_attr_serial_number.attr,\n\t&dev_attr_vendor.attr,\n\t&dev_attr_rescan.attr,\n\t&dev_attr_lockup_action.attr,\n\t&dev_attr_enable_stream_detection.attr,\n\t&dev_attr_enable_r5_writes.attr,\n\t&dev_attr_enable_r6_writes.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(pqi_shost);\n\nstatic ssize_t pqi_unique_id_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tu8 unique_id[16];\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tif (device->is_physical_device)\n\t\tmemcpy(unique_id, device->wwid, sizeof(device->wwid));\n\telse\n\t\tmemcpy(unique_id, device->volume_id, sizeof(device->volume_id));\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn scnprintf(buffer, PAGE_SIZE,\n\t\t\"%02X%02X%02X%02X%02X%02X%02X%02X\"\n\t\t\"%02X%02X%02X%02X%02X%02X%02X%02X\\n\",\n\t\tunique_id[0], unique_id[1], unique_id[2], unique_id[3],\n\t\tunique_id[4], unique_id[5], unique_id[6], unique_id[7],\n\t\tunique_id[8], unique_id[9], unique_id[10], unique_id[11],\n\t\tunique_id[12], unique_id[13], unique_id[14], unique_id[15]);\n}\n\nstatic ssize_t pqi_lunid_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tu8 lunid[8];\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tmemcpy(lunid, device->scsi3addr, sizeof(lunid));\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"0x%8phN\\n\", lunid);\n}\n\n#define MAX_PATHS\t8\n\nstatic ssize_t pqi_path_info_show(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tint i;\n\tint output_len = 0;\n\tu8 box;\n\tu8 bay;\n\tu8 path_map_index;\n\tchar *active;\n\tu8 phys_connector[2];\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tbay = device->bay;\n\tfor (i = 0; i < MAX_PATHS; i++) {\n\t\tpath_map_index = 1 << i;\n\t\tif (i == device->active_path_index)\n\t\t\tactive = \"Active\";\n\t\telse if (device->path_map & path_map_index)\n\t\t\tactive = \"Inactive\";\n\t\telse\n\t\t\tcontinue;\n\n\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\"[%d:%d:%d:%d] %20.20s \",\n\t\t\t\t\tctrl_info->scsi_host->host_no,\n\t\t\t\t\tdevice->bus, device->target,\n\t\t\t\t\tdevice->lun,\n\t\t\t\t\tscsi_device_type(device->devtype));\n\n\t\tif (device->devtype == TYPE_RAID ||\n\t\t\tpqi_is_logical_device(device))\n\t\t\tgoto end_buffer;\n\n\t\tmemcpy(&phys_connector, &device->phys_connector[i],\n\t\t\tsizeof(phys_connector));\n\t\tif (phys_connector[0] < '0')\n\t\t\tphys_connector[0] = '0';\n\t\tif (phys_connector[1] < '0')\n\t\t\tphys_connector[1] = '0';\n\n\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\"PORT: %.2s \", phys_connector);\n\n\t\tbox = device->box[i];\n\t\tif (box != 0 && box != 0xFF)\n\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\t\"BOX: %hhu \", box);\n\n\t\tif ((device->devtype == TYPE_DISK ||\n\t\t\tdevice->devtype == TYPE_ZBC) &&\n\t\t\tpqi_expose_device(device))\n\t\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\t\"BAY: %hhu \", bay);\n\nend_buffer:\n\t\toutput_len += scnprintf(buf + output_len,\n\t\t\t\t\tPAGE_SIZE - output_len,\n\t\t\t\t\t\"%s\\n\", active);\n\t}\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn output_len;\n}\n\nstatic ssize_t pqi_sas_address_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tu64 sas_address;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tsas_address = device->sas_address;\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"0x%016llx\\n\", sas_address);\n}\n\nstatic ssize_t pqi_ssd_smart_path_enabled_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tbuffer[0] = device->raid_bypass_enabled ? '1' : '0';\n\tbuffer[1] = '\\n';\n\tbuffer[2] = '\\0';\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn 2;\n}\n\nstatic ssize_t pqi_raid_level_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tchar *raid_level;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pqi_is_logical_device(device) && device->devtype == TYPE_DISK)\n\t\traid_level = pqi_raid_level_to_string(device->raid_level);\n\telse\n\t\traid_level = \"N/A\";\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", raid_level);\n}\n\nstatic ssize_t pqi_raid_bypass_cnt_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tunsigned int raid_bypass_cnt;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\traid_bypass_cnt = device->raid_bypass_cnt;\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"0x%x\\n\", raid_bypass_cnt);\n}\n\nstatic ssize_t pqi_sas_ncq_prio_enable_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tint output_len = 0;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tif (pqi_ctrl_offline(ctrl_info))\n\t\treturn -ENODEV;\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\toutput_len = snprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t\t\tdevice->ncq_prio_enable);\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn output_len;\n}\n\nstatic ssize_t pqi_sas_ncq_prio_enable_store(struct device *dev,\n\t\t\tstruct device_attribute *attr,\n\t\t\tconst char *buf, size_t count)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct scsi_device *sdev;\n\tstruct pqi_scsi_dev *device;\n\tunsigned long flags;\n\tu8 ncq_prio_enable = 0;\n\n\tif (kstrtou8(buf, 0, &ncq_prio_enable))\n\t\treturn -EINVAL;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\tspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\n\n\tdevice = sdev->hostdata;\n\n\tif (!device) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!device->ncq_prio_support) {\n\t\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n\tdevice->ncq_prio_enable = ncq_prio_enable;\n\n\tspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\n\n\treturn  strlen(buf);\n}\n\nstatic ssize_t pqi_numa_node_show(struct device *dev,\n\tstruct device_attribute *attr, char *buffer)\n{\n\tstruct scsi_device *sdev;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tsdev = to_scsi_device(dev);\n\tctrl_info = shost_to_hba(sdev->host);\n\n\treturn scnprintf(buffer, PAGE_SIZE, \"%d\\n\", ctrl_info->numa_node);\n}\n\nstatic DEVICE_ATTR(lunid, 0444, pqi_lunid_show, NULL);\nstatic DEVICE_ATTR(unique_id, 0444, pqi_unique_id_show, NULL);\nstatic DEVICE_ATTR(path_info, 0444, pqi_path_info_show, NULL);\nstatic DEVICE_ATTR(sas_address, 0444, pqi_sas_address_show, NULL);\nstatic DEVICE_ATTR(ssd_smart_path_enabled, 0444, pqi_ssd_smart_path_enabled_show, NULL);\nstatic DEVICE_ATTR(raid_level, 0444, pqi_raid_level_show, NULL);\nstatic DEVICE_ATTR(raid_bypass_cnt, 0444, pqi_raid_bypass_cnt_show, NULL);\nstatic DEVICE_ATTR(sas_ncq_prio_enable, 0644,\n\t\tpqi_sas_ncq_prio_enable_show, pqi_sas_ncq_prio_enable_store);\nstatic DEVICE_ATTR(numa_node, 0444, pqi_numa_node_show, NULL);\n\nstatic struct attribute *pqi_sdev_attrs[] = {\n\t&dev_attr_lunid.attr,\n\t&dev_attr_unique_id.attr,\n\t&dev_attr_path_info.attr,\n\t&dev_attr_sas_address.attr,\n\t&dev_attr_ssd_smart_path_enabled.attr,\n\t&dev_attr_raid_level.attr,\n\t&dev_attr_raid_bypass_cnt.attr,\n\t&dev_attr_sas_ncq_prio_enable.attr,\n\t&dev_attr_numa_node.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(pqi_sdev);\n\nstatic const struct scsi_host_template pqi_driver_template = {\n\t.module = THIS_MODULE,\n\t.name = DRIVER_NAME_SHORT,\n\t.proc_name = DRIVER_NAME_SHORT,\n\t.queuecommand = pqi_scsi_queue_command,\n\t.scan_start = pqi_scan_start,\n\t.scan_finished = pqi_scan_finished,\n\t.this_id = -1,\n\t.eh_device_reset_handler = pqi_eh_device_reset_handler,\n\t.eh_abort_handler = pqi_eh_abort_handler,\n\t.ioctl = pqi_ioctl,\n\t.slave_alloc = pqi_slave_alloc,\n\t.slave_configure = pqi_slave_configure,\n\t.slave_destroy = pqi_slave_destroy,\n\t.map_queues = pqi_map_queues,\n\t.sdev_groups = pqi_sdev_groups,\n\t.shost_groups = pqi_shost_groups,\n\t.cmd_size = sizeof(struct pqi_cmd_priv),\n};\n\nstatic int pqi_register_scsi(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct Scsi_Host *shost;\n\n\tshost = scsi_host_alloc(&pqi_driver_template, sizeof(ctrl_info));\n\tif (!shost) {\n\t\tdev_err(&ctrl_info->pci_dev->dev, \"scsi_host_alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tshost->io_port = 0;\n\tshost->n_io_port = 0;\n\tshost->this_id = -1;\n\tshost->max_channel = PQI_MAX_BUS;\n\tshost->max_cmd_len = MAX_COMMAND_SIZE;\n\tshost->max_lun = PQI_MAX_LUNS_PER_DEVICE;\n\tshost->max_id = ~0;\n\tshost->max_sectors = ctrl_info->max_sectors;\n\tshost->can_queue = ctrl_info->scsi_ml_can_queue;\n\tshost->cmd_per_lun = shost->can_queue;\n\tshost->sg_tablesize = ctrl_info->sg_tablesize;\n\tshost->transportt = pqi_sas_transport_template;\n\tshost->irq = pci_irq_vector(ctrl_info->pci_dev, 0);\n\tshost->unique_id = shost->irq;\n\tshost->nr_hw_queues = ctrl_info->num_queue_groups;\n\tshost->host_tagset = 1;\n\tshost->hostdata[0] = (unsigned long)ctrl_info;\n\n\trc = scsi_add_host(shost, &ctrl_info->pci_dev->dev);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev, \"scsi_add_host failed\\n\");\n\t\tgoto free_host;\n\t}\n\n\trc = pqi_add_sas_host(shost, ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev, \"add SAS host failed\\n\");\n\t\tgoto remove_host;\n\t}\n\n\tctrl_info->scsi_host = shost;\n\n\treturn 0;\n\nremove_host:\n\tscsi_remove_host(shost);\nfree_host:\n\tscsi_host_put(shost);\n\n\treturn rc;\n}\n\nstatic void pqi_unregister_scsi(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct Scsi_Host *shost;\n\n\tpqi_delete_sas_host(ctrl_info);\n\n\tshost = ctrl_info->scsi_host;\n\tif (!shost)\n\t\treturn;\n\n\tscsi_remove_host(shost);\n\tscsi_host_put(shost);\n}\n\nstatic int pqi_wait_for_pqi_reset_completion(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc = 0;\n\tstruct pqi_device_registers __iomem *pqi_registers;\n\tunsigned long timeout;\n\tunsigned int timeout_msecs;\n\tunion pqi_reset_register reset_reg;\n\n\tpqi_registers = ctrl_info->pqi_registers;\n\ttimeout_msecs = readw(&pqi_registers->max_reset_timeout) * 100;\n\ttimeout = msecs_to_jiffies(timeout_msecs) + jiffies;\n\n\twhile (1) {\n\t\tmsleep(PQI_RESET_POLL_INTERVAL_MSECS);\n\t\treset_reg.all_bits = readl(&pqi_registers->device_reset);\n\t\tif (reset_reg.bits.reset_action == PQI_RESET_ACTION_COMPLETED)\n\t\t\tbreak;\n\t\tif (!sis_is_firmware_running(ctrl_info)) {\n\t\t\trc = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\trc = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic int pqi_reset(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tunion pqi_reset_register reset_reg;\n\n\tif (ctrl_info->pqi_reset_quiesce_supported) {\n\t\trc = sis_pqi_reset_quiesce(ctrl_info);\n\t\tif (rc) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"PQI reset failed during quiesce with error %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treset_reg.all_bits = 0;\n\treset_reg.bits.reset_type = PQI_RESET_TYPE_HARD_RESET;\n\treset_reg.bits.reset_action = PQI_RESET_ACTION_RESET;\n\n\twritel(reset_reg.all_bits, &ctrl_info->pqi_registers->device_reset);\n\n\trc = pqi_wait_for_pqi_reset_completion(ctrl_info);\n\tif (rc)\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"PQI reset failed with error %d\\n\", rc);\n\n\treturn rc;\n}\n\nstatic int pqi_get_ctrl_serial_number(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct bmic_sense_subsystem_info *sense_info;\n\n\tsense_info = kzalloc(sizeof(*sense_info), GFP_KERNEL);\n\tif (!sense_info)\n\t\treturn -ENOMEM;\n\n\trc = pqi_sense_subsystem_info(ctrl_info, sense_info);\n\tif (rc)\n\t\tgoto out;\n\n\tmemcpy(ctrl_info->serial_number, sense_info->ctrl_serial_number,\n\t\tsizeof(sense_info->ctrl_serial_number));\n\tctrl_info->serial_number[sizeof(sense_info->ctrl_serial_number)] = '\\0';\n\nout:\n\tkfree(sense_info);\n\n\treturn rc;\n}\n\nstatic int pqi_get_ctrl_product_details(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tstruct bmic_identify_controller *identify;\n\n\tidentify = kmalloc(sizeof(*identify), GFP_KERNEL);\n\tif (!identify)\n\t\treturn -ENOMEM;\n\n\trc = pqi_identify_controller(ctrl_info, identify);\n\tif (rc)\n\t\tgoto out;\n\n\tif (get_unaligned_le32(&identify->extra_controller_flags) &\n\t\tBMIC_IDENTIFY_EXTRA_FLAGS_LONG_FW_VERSION_SUPPORTED) {\n\t\tmemcpy(ctrl_info->firmware_version,\n\t\t\tidentify->firmware_version_long,\n\t\t\tsizeof(identify->firmware_version_long));\n\t} else {\n\t\tmemcpy(ctrl_info->firmware_version,\n\t\t\tidentify->firmware_version_short,\n\t\t\tsizeof(identify->firmware_version_short));\n\t\tctrl_info->firmware_version\n\t\t\t[sizeof(identify->firmware_version_short)] = '\\0';\n\t\tsnprintf(ctrl_info->firmware_version +\n\t\t\tstrlen(ctrl_info->firmware_version),\n\t\t\tsizeof(ctrl_info->firmware_version) -\n\t\t\tsizeof(identify->firmware_version_short),\n\t\t\t\"-%u\",\n\t\t\tget_unaligned_le16(&identify->firmware_build_number));\n\t}\n\n\tmemcpy(ctrl_info->model, identify->product_id,\n\t\tsizeof(identify->product_id));\n\tctrl_info->model[sizeof(identify->product_id)] = '\\0';\n\n\tmemcpy(ctrl_info->vendor, identify->vendor_id,\n\t\tsizeof(identify->vendor_id));\n\tctrl_info->vendor[sizeof(identify->vendor_id)] = '\\0';\n\n\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\"Firmware version: %s\\n\", ctrl_info->firmware_version);\n\nout:\n\tkfree(identify);\n\n\treturn rc;\n}\n\nstruct pqi_config_table_section_info {\n\tstruct pqi_ctrl_info *ctrl_info;\n\tvoid\t\t*section;\n\tu32\t\tsection_offset;\n\tvoid __iomem\t*section_iomem_addr;\n};\n\nstatic inline bool pqi_is_firmware_feature_supported(\n\tstruct pqi_config_table_firmware_features *firmware_features,\n\tunsigned int bit_position)\n{\n\tunsigned int byte_index;\n\n\tbyte_index = bit_position / BITS_PER_BYTE;\n\n\tif (byte_index >= le16_to_cpu(firmware_features->num_elements))\n\t\treturn false;\n\n\treturn firmware_features->features_supported[byte_index] &\n\t\t(1 << (bit_position % BITS_PER_BYTE)) ? true : false;\n}\n\nstatic inline bool pqi_is_firmware_feature_enabled(\n\tstruct pqi_config_table_firmware_features *firmware_features,\n\tvoid __iomem *firmware_features_iomem_addr,\n\tunsigned int bit_position)\n{\n\tunsigned int byte_index;\n\tu8 __iomem *features_enabled_iomem_addr;\n\n\tbyte_index = (bit_position / BITS_PER_BYTE) +\n\t\t(le16_to_cpu(firmware_features->num_elements) * 2);\n\n\tfeatures_enabled_iomem_addr = firmware_features_iomem_addr +\n\t\toffsetof(struct pqi_config_table_firmware_features,\n\t\t\tfeatures_supported) + byte_index;\n\n\treturn *((__force u8 *)features_enabled_iomem_addr) &\n\t\t(1 << (bit_position % BITS_PER_BYTE)) ? true : false;\n}\n\nstatic inline void pqi_request_firmware_feature(\n\tstruct pqi_config_table_firmware_features *firmware_features,\n\tunsigned int bit_position)\n{\n\tunsigned int byte_index;\n\n\tbyte_index = (bit_position / BITS_PER_BYTE) +\n\t\tle16_to_cpu(firmware_features->num_elements);\n\n\tfirmware_features->features_supported[byte_index] |=\n\t\t(1 << (bit_position % BITS_PER_BYTE));\n}\n\nstatic int pqi_config_table_update(struct pqi_ctrl_info *ctrl_info,\n\tu16 first_section, u16 last_section)\n{\n\tstruct pqi_vendor_general_request request;\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_VENDOR_GENERAL;\n\tput_unaligned_le16(sizeof(request) - PQI_REQUEST_HEADER_LENGTH,\n\t\t&request.header.iu_length);\n\tput_unaligned_le16(PQI_VENDOR_GENERAL_CONFIG_TABLE_UPDATE,\n\t\t&request.function_code);\n\tput_unaligned_le16(first_section,\n\t\t&request.data.config_table_update.first_section);\n\tput_unaligned_le16(last_section,\n\t\t&request.data.config_table_update.last_section);\n\n\treturn pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n}\n\nstatic int pqi_enable_firmware_features(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_config_table_firmware_features *firmware_features,\n\tvoid __iomem *firmware_features_iomem_addr)\n{\n\tvoid *features_requested;\n\tvoid __iomem *features_requested_iomem_addr;\n\tvoid __iomem *host_max_known_feature_iomem_addr;\n\n\tfeatures_requested = firmware_features->features_supported +\n\t\tle16_to_cpu(firmware_features->num_elements);\n\n\tfeatures_requested_iomem_addr = firmware_features_iomem_addr +\n\t\t(features_requested - (void *)firmware_features);\n\n\tmemcpy_toio(features_requested_iomem_addr, features_requested,\n\t\tle16_to_cpu(firmware_features->num_elements));\n\n\tif (pqi_is_firmware_feature_supported(firmware_features,\n\t\tPQI_FIRMWARE_FEATURE_MAX_KNOWN_FEATURE)) {\n\t\thost_max_known_feature_iomem_addr =\n\t\t\tfeatures_requested_iomem_addr +\n\t\t\t(le16_to_cpu(firmware_features->num_elements) * 2) +\n\t\t\tsizeof(__le16);\n\t\twriteb(PQI_FIRMWARE_FEATURE_MAXIMUM & 0xFF, host_max_known_feature_iomem_addr);\n\t\twriteb((PQI_FIRMWARE_FEATURE_MAXIMUM & 0xFF00) >> 8, host_max_known_feature_iomem_addr + 1);\n\t}\n\n\treturn pqi_config_table_update(ctrl_info,\n\t\tPQI_CONFIG_TABLE_SECTION_FIRMWARE_FEATURES,\n\t\tPQI_CONFIG_TABLE_SECTION_FIRMWARE_FEATURES);\n}\n\nstruct pqi_firmware_feature {\n\tchar\t\t*feature_name;\n\tunsigned int\tfeature_bit;\n\tbool\t\tsupported;\n\tbool\t\tenabled;\n\tvoid (*feature_status)(struct pqi_ctrl_info *ctrl_info,\n\t\tstruct pqi_firmware_feature *firmware_feature);\n};\n\nstatic void pqi_firmware_feature_status(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_firmware_feature *firmware_feature)\n{\n\tif (!firmware_feature->supported) {\n\t\tdev_info(&ctrl_info->pci_dev->dev, \"%s not supported by controller\\n\",\n\t\t\tfirmware_feature->feature_name);\n\t\treturn;\n\t}\n\n\tif (firmware_feature->enabled) {\n\t\tdev_info(&ctrl_info->pci_dev->dev,\n\t\t\t\"%s enabled\\n\", firmware_feature->feature_name);\n\t\treturn;\n\t}\n\n\tdev_err(&ctrl_info->pci_dev->dev, \"failed to enable %s\\n\",\n\t\tfirmware_feature->feature_name);\n}\n\nstatic void pqi_ctrl_update_feature_flags(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_firmware_feature *firmware_feature)\n{\n\tswitch (firmware_feature->feature_bit) {\n\tcase PQI_FIRMWARE_FEATURE_RAID_1_WRITE_BYPASS:\n\t\tctrl_info->enable_r1_writes = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_RAID_5_WRITE_BYPASS:\n\t\tctrl_info->enable_r5_writes = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_RAID_6_WRITE_BYPASS:\n\t\tctrl_info->enable_r6_writes = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_SOFT_RESET_HANDSHAKE:\n\t\tctrl_info->soft_reset_handshake_supported =\n\t\t\tfirmware_feature->enabled &&\n\t\t\tpqi_read_soft_reset_status(ctrl_info);\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_RAID_IU_TIMEOUT:\n\t\tctrl_info->raid_iu_timeout_supported = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_TMF_IU_TIMEOUT:\n\t\tctrl_info->tmf_iu_timeout_supported = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_FW_TRIAGE:\n\t\tctrl_info->firmware_triage_supported = firmware_feature->enabled;\n\t\tpqi_save_fw_triage_setting(ctrl_info, firmware_feature->enabled);\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_RPL_EXTENDED_FORMAT_4_5:\n\t\tctrl_info->rpl_extended_format_4_5_supported = firmware_feature->enabled;\n\t\tbreak;\n\tcase PQI_FIRMWARE_FEATURE_MULTI_LUN_DEVICE_SUPPORT:\n\t\tctrl_info->multi_lun_device_supported = firmware_feature->enabled;\n\t\tbreak;\n\t}\n\n\tpqi_firmware_feature_status(ctrl_info, firmware_feature);\n}\n\nstatic inline void pqi_firmware_feature_update(struct pqi_ctrl_info *ctrl_info,\n\tstruct pqi_firmware_feature *firmware_feature)\n{\n\tif (firmware_feature->feature_status)\n\t\tfirmware_feature->feature_status(ctrl_info, firmware_feature);\n}\n\nstatic DEFINE_MUTEX(pqi_firmware_features_mutex);\n\nstatic struct pqi_firmware_feature pqi_firmware_features[] = {\n\t{\n\t\t.feature_name = \"Online Firmware Activation\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_OFA,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"Serial Management Protocol\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_SMP,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"Maximum Known Feature\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_MAX_KNOWN_FEATURE,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 0 Read Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_0_READ_BYPASS,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 1 Read Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_1_READ_BYPASS,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 5 Read Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_5_READ_BYPASS,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 6 Read Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_6_READ_BYPASS,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 0 Write Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_0_WRITE_BYPASS,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"RAID 1 Write Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_1_WRITE_BYPASS,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"RAID 5 Write Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_5_WRITE_BYPASS,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"RAID 6 Write Bypass\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_6_WRITE_BYPASS,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"New Soft Reset Handshake\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_SOFT_RESET_HANDSHAKE,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"RAID IU Timeout\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_IU_TIMEOUT,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"TMF IU Timeout\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_TMF_IU_TIMEOUT,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"RAID Bypass on encrypted logical volumes on NVMe\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RAID_BYPASS_ON_ENCRYPTED_NVME,\n\t\t.feature_status = pqi_firmware_feature_status,\n\t},\n\t{\n\t\t.feature_name = \"Firmware Triage\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_FW_TRIAGE,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"RPL Extended Formats 4 and 5\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_RPL_EXTENDED_FORMAT_4_5,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n\t{\n\t\t.feature_name = \"Multi-LUN Target\",\n\t\t.feature_bit = PQI_FIRMWARE_FEATURE_MULTI_LUN_DEVICE_SUPPORT,\n\t\t.feature_status = pqi_ctrl_update_feature_flags,\n\t},\n};\n\nstatic void pqi_process_firmware_features(\n\tstruct pqi_config_table_section_info *section_info)\n{\n\tint rc;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tstruct pqi_config_table_firmware_features *firmware_features;\n\tvoid __iomem *firmware_features_iomem_addr;\n\tunsigned int i;\n\tunsigned int num_features_supported;\n\n\tctrl_info = section_info->ctrl_info;\n\tfirmware_features = section_info->section;\n\tfirmware_features_iomem_addr = section_info->section_iomem_addr;\n\n\tfor (i = 0, num_features_supported = 0;\n\t\ti < ARRAY_SIZE(pqi_firmware_features); i++) {\n\t\tif (pqi_is_firmware_feature_supported(firmware_features,\n\t\t\tpqi_firmware_features[i].feature_bit)) {\n\t\t\tpqi_firmware_features[i].supported = true;\n\t\t\tnum_features_supported++;\n\t\t} else {\n\t\t\tpqi_firmware_feature_update(ctrl_info,\n\t\t\t\t&pqi_firmware_features[i]);\n\t\t}\n\t}\n\n\tif (num_features_supported == 0)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_firmware_features); i++) {\n\t\tif (!pqi_firmware_features[i].supported)\n\t\t\tcontinue;\n\t\tpqi_request_firmware_feature(firmware_features,\n\t\t\tpqi_firmware_features[i].feature_bit);\n\t}\n\n\trc = pqi_enable_firmware_features(ctrl_info, firmware_features,\n\t\tfirmware_features_iomem_addr);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to enable firmware features in PQI configuration table\\n\");\n\t\tfor (i = 0; i < ARRAY_SIZE(pqi_firmware_features); i++) {\n\t\t\tif (!pqi_firmware_features[i].supported)\n\t\t\t\tcontinue;\n\t\t\tpqi_firmware_feature_update(ctrl_info,\n\t\t\t\t&pqi_firmware_features[i]);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_firmware_features); i++) {\n\t\tif (!pqi_firmware_features[i].supported)\n\t\t\tcontinue;\n\t\tif (pqi_is_firmware_feature_enabled(firmware_features,\n\t\t\tfirmware_features_iomem_addr,\n\t\t\tpqi_firmware_features[i].feature_bit)) {\n\t\t\t\tpqi_firmware_features[i].enabled = true;\n\t\t}\n\t\tpqi_firmware_feature_update(ctrl_info,\n\t\t\t&pqi_firmware_features[i]);\n\t}\n}\n\nstatic void pqi_init_firmware_features(void)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_firmware_features); i++) {\n\t\tpqi_firmware_features[i].supported = false;\n\t\tpqi_firmware_features[i].enabled = false;\n\t}\n}\n\nstatic void pqi_process_firmware_features_section(\n\tstruct pqi_config_table_section_info *section_info)\n{\n\tmutex_lock(&pqi_firmware_features_mutex);\n\tpqi_init_firmware_features();\n\tpqi_process_firmware_features(section_info);\n\tmutex_unlock(&pqi_firmware_features_mutex);\n}\n\n \n\nstatic void pqi_ctrl_reset_config(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->heartbeat_counter = NULL;\n\tctrl_info->soft_reset_status = NULL;\n\tctrl_info->soft_reset_handshake_supported = false;\n\tctrl_info->enable_r1_writes = false;\n\tctrl_info->enable_r5_writes = false;\n\tctrl_info->enable_r6_writes = false;\n\tctrl_info->raid_iu_timeout_supported = false;\n\tctrl_info->tmf_iu_timeout_supported = false;\n\tctrl_info->firmware_triage_supported = false;\n\tctrl_info->rpl_extended_format_4_5_supported = false;\n\tctrl_info->multi_lun_device_supported = false;\n}\n\nstatic int pqi_process_config_table(struct pqi_ctrl_info *ctrl_info)\n{\n\tu32 table_length;\n\tu32 section_offset;\n\tbool firmware_feature_section_present;\n\tvoid __iomem *table_iomem_addr;\n\tstruct pqi_config_table *config_table;\n\tstruct pqi_config_table_section_header *section;\n\tstruct pqi_config_table_section_info section_info;\n\tstruct pqi_config_table_section_info feature_section_info = {0};\n\n\ttable_length = ctrl_info->config_table_length;\n\tif (table_length == 0)\n\t\treturn 0;\n\n\tconfig_table = kmalloc(table_length, GFP_KERNEL);\n\tif (!config_table) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to allocate memory for PQI configuration table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\ttable_iomem_addr = ctrl_info->iomem_base + ctrl_info->config_table_offset;\n\tmemcpy_fromio(config_table, table_iomem_addr, table_length);\n\n\tfirmware_feature_section_present = false;\n\tsection_info.ctrl_info = ctrl_info;\n\tsection_offset = get_unaligned_le32(&config_table->first_section_offset);\n\n\twhile (section_offset) {\n\t\tsection = (void *)config_table + section_offset;\n\n\t\tsection_info.section = section;\n\t\tsection_info.section_offset = section_offset;\n\t\tsection_info.section_iomem_addr = table_iomem_addr + section_offset;\n\n\t\tswitch (get_unaligned_le16(&section->section_id)) {\n\t\tcase PQI_CONFIG_TABLE_SECTION_FIRMWARE_FEATURES:\n\t\t\tfirmware_feature_section_present = true;\n\t\t\tfeature_section_info = section_info;\n\t\t\tbreak;\n\t\tcase PQI_CONFIG_TABLE_SECTION_HEARTBEAT:\n\t\t\tif (pqi_disable_heartbeat)\n\t\t\t\tdev_warn(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"heartbeat disabled by module parameter\\n\");\n\t\t\telse\n\t\t\t\tctrl_info->heartbeat_counter =\n\t\t\t\t\ttable_iomem_addr +\n\t\t\t\t\tsection_offset +\n\t\t\t\t\toffsetof(struct pqi_config_table_heartbeat,\n\t\t\t\t\t\theartbeat_counter);\n\t\t\tbreak;\n\t\tcase PQI_CONFIG_TABLE_SECTION_SOFT_RESET:\n\t\t\tctrl_info->soft_reset_status =\n\t\t\t\ttable_iomem_addr +\n\t\t\t\tsection_offset +\n\t\t\t\toffsetof(struct pqi_config_table_soft_reset,\n\t\t\t\t\tsoft_reset_status);\n\t\t\tbreak;\n\t\t}\n\n\t\tsection_offset = get_unaligned_le16(&section->next_section_offset);\n\t}\n\n\t \n\tif (firmware_feature_section_present)\n\t\tpqi_process_firmware_features_section(&feature_section_info);\n\n\tkfree(config_table);\n\n\treturn 0;\n}\n\n \n\nstatic int pqi_revert_to_sis_mode(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\n\tpqi_change_irq_mode(ctrl_info, IRQ_MODE_NONE);\n\trc = pqi_reset(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\trc = sis_reenable_sis_mode(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"re-enabling SIS mode failed with error %d\\n\", rc);\n\t\treturn rc;\n\t}\n\tpqi_save_ctrl_mode(ctrl_info, SIS_MODE);\n\n\treturn 0;\n}\n\n \n\nstatic int pqi_force_sis_mode(struct pqi_ctrl_info *ctrl_info)\n{\n\tif (!sis_is_firmware_running(ctrl_info))\n\t\treturn -ENXIO;\n\n\tif (pqi_get_ctrl_mode(ctrl_info) == SIS_MODE)\n\t\treturn 0;\n\n\tif (sis_is_kernel_up(ctrl_info)) {\n\t\tpqi_save_ctrl_mode(ctrl_info, SIS_MODE);\n\t\treturn 0;\n\t}\n\n\treturn pqi_revert_to_sis_mode(ctrl_info);\n}\n\nstatic void pqi_perform_lockup_action(void)\n{\n\tswitch (pqi_lockup_action) {\n\tcase PANIC:\n\t\tpanic(\"FATAL: Smart Family Controller lockup detected\");\n\t\tbreak;\n\tcase REBOOT:\n\t\temergency_restart();\n\t\tbreak;\n\tcase NONE:\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int pqi_ctrl_init(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tu32 product_id;\n\n\tif (reset_devices) {\n\t\tif (pqi_is_fw_triage_supported(ctrl_info)) {\n\t\t\trc = sis_wait_for_fw_triage_completion(ctrl_info);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t\tsis_soft_reset(ctrl_info);\n\t\tssleep(PQI_POST_RESET_DELAY_SECS);\n\t} else {\n\t\trc = pqi_force_sis_mode(ctrl_info);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\t \n\trc = sis_wait_for_ctrl_ready(ctrl_info);\n\tif (rc) {\n\t\tif (reset_devices) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"kdump init failed with error %d\\n\", rc);\n\t\t\tpqi_lockup_action = REBOOT;\n\t\t\tpqi_perform_lockup_action();\n\t\t}\n\t\treturn rc;\n\t}\n\n\t \n\trc = sis_get_ctrl_properties(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining controller properties\\n\");\n\t\treturn rc;\n\t}\n\n\trc = sis_get_pqi_capabilities(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining controller capabilities\\n\");\n\t\treturn rc;\n\t}\n\n\tproduct_id = sis_get_product_id(ctrl_info);\n\tctrl_info->product_id = (u8)product_id;\n\tctrl_info->product_revision = (u8)(product_id >> 8);\n\n\tif (reset_devices) {\n\t\tif (ctrl_info->max_outstanding_requests >\n\t\t\tPQI_MAX_OUTSTANDING_REQUESTS_KDUMP)\n\t\t\t\tctrl_info->max_outstanding_requests =\n\t\t\t\t\tPQI_MAX_OUTSTANDING_REQUESTS_KDUMP;\n\t} else {\n\t\tif (ctrl_info->max_outstanding_requests >\n\t\t\tPQI_MAX_OUTSTANDING_REQUESTS)\n\t\t\t\tctrl_info->max_outstanding_requests =\n\t\t\t\t\tPQI_MAX_OUTSTANDING_REQUESTS;\n\t}\n\n\tpqi_calculate_io_resources(ctrl_info);\n\n\trc = pqi_alloc_error_buffer(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to allocate PQI error buffer\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = sis_init_base_struct_addr(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error initializing PQI mode\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = pqi_wait_for_pqi_mode_ready(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"transition to PQI mode failed\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tctrl_info->pqi_mode_enabled = true;\n\tpqi_save_ctrl_mode(ctrl_info, PQI_MODE);\n\n\trc = pqi_alloc_admin_queues(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to allocate admin queues\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_create_admin_queues(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating admin queues\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_report_device_capability(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"obtaining device capability failed\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_validate_device_capability(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_calculate_queue_resources(ctrl_info);\n\n\trc = pqi_enable_msix_interrupts(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tif (ctrl_info->num_msix_vectors_enabled < ctrl_info->num_queue_groups) {\n\t\tctrl_info->max_msix_vectors =\n\t\t\tctrl_info->num_msix_vectors_enabled;\n\t\tpqi_calculate_queue_resources(ctrl_info);\n\t}\n\n\trc = pqi_alloc_io_resources(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pqi_alloc_operational_queues(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to allocate operational queues\\n\");\n\t\treturn rc;\n\t}\n\n\tpqi_init_operational_queues(ctrl_info);\n\n\trc = pqi_create_queues(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pqi_request_irqs(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_change_irq_mode(ctrl_info, IRQ_MODE_MSIX);\n\n\tctrl_info->controller_online = true;\n\n\trc = pqi_process_config_table(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_start_heartbeat_timer(ctrl_info);\n\n\tif (ctrl_info->enable_r5_writes || ctrl_info->enable_r6_writes) {\n\t\trc = pqi_get_advanced_raid_bypass_config(ctrl_info);\n\t\tif (rc) {  \n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"error obtaining advanced RAID bypass configuration\\n\");\n\t\t\treturn rc;\n\t\t}\n\t\tctrl_info->ciss_report_log_flags |=\n\t\t\tCISS_REPORT_LOG_FLAG_DRIVE_TYPE_MIX;\n\t}\n\n\trc = pqi_enable_events(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error enabling events\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = pqi_register_scsi(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pqi_get_ctrl_product_details(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining product details\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_get_ctrl_serial_number(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining ctrl serial number\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_set_diag_rescan(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error enabling multi-lun rescan\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_write_driver_version_to_host_wellness(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error updating host wellness\\n\");\n\t\treturn rc;\n\t}\n\n\tpqi_schedule_update_time_worker(ctrl_info);\n\n\tpqi_scan_scsi_devices(ctrl_info);\n\n\treturn 0;\n}\n\nstatic void pqi_reinit_queues(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tstruct pqi_admin_queues *admin_queues;\n\tstruct pqi_event_queue *event_queue;\n\n\tadmin_queues = &ctrl_info->admin_queues;\n\tadmin_queues->iq_pi_copy = 0;\n\tadmin_queues->oq_ci_copy = 0;\n\twritel(0, admin_queues->oq_pi);\n\n\tfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\n\t\tctrl_info->queue_groups[i].iq_pi_copy[RAID_PATH] = 0;\n\t\tctrl_info->queue_groups[i].iq_pi_copy[AIO_PATH] = 0;\n\t\tctrl_info->queue_groups[i].oq_ci_copy = 0;\n\n\t\twritel(0, ctrl_info->queue_groups[i].iq_ci[RAID_PATH]);\n\t\twritel(0, ctrl_info->queue_groups[i].iq_ci[AIO_PATH]);\n\t\twritel(0, ctrl_info->queue_groups[i].oq_pi);\n\t}\n\n\tevent_queue = &ctrl_info->event_queue;\n\twritel(0, event_queue->oq_pi);\n\tevent_queue->oq_ci_copy = 0;\n}\n\nstatic int pqi_ctrl_init_resume(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\n\trc = pqi_force_sis_mode(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = sis_wait_for_ctrl_ready_resume(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = sis_get_ctrl_properties(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining controller properties\\n\");\n\t\treturn rc;\n\t}\n\n\trc = sis_get_pqi_capabilities(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining controller capabilities\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = sis_init_base_struct_addr(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error initializing PQI mode\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = pqi_wait_for_pqi_mode_ready(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"transition to PQI mode failed\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tctrl_info->pqi_mode_enabled = true;\n\tpqi_save_ctrl_mode(ctrl_info, PQI_MODE);\n\n\tpqi_reinit_queues(ctrl_info);\n\n\trc = pqi_create_admin_queues(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error creating admin queues\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_create_queues(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_change_irq_mode(ctrl_info, IRQ_MODE_MSIX);\n\n\tctrl_info->controller_online = true;\n\tpqi_ctrl_unblock_requests(ctrl_info);\n\n\tpqi_ctrl_reset_config(ctrl_info);\n\n\trc = pqi_process_config_table(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_start_heartbeat_timer(ctrl_info);\n\n\tif (ctrl_info->enable_r5_writes || ctrl_info->enable_r6_writes) {\n\t\trc = pqi_get_advanced_raid_bypass_config(ctrl_info);\n\t\tif (rc) {\n\t\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\t\"error obtaining advanced RAID bypass configuration\\n\");\n\t\t\treturn rc;\n\t\t}\n\t\tctrl_info->ciss_report_log_flags |=\n\t\t\tCISS_REPORT_LOG_FLAG_DRIVE_TYPE_MIX;\n\t}\n\n\trc = pqi_enable_events(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error enabling events\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_get_ctrl_product_details(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error obtaining product details\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_set_diag_rescan(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error enabling multi-lun rescan\\n\");\n\t\treturn rc;\n\t}\n\n\trc = pqi_write_driver_version_to_host_wellness(ctrl_info);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"error updating host wellness\\n\");\n\t\treturn rc;\n\t}\n\n\tif (pqi_ofa_in_progress(ctrl_info))\n\t\tpqi_ctrl_unblock_scan(ctrl_info);\n\n\tpqi_scan_scsi_devices(ctrl_info);\n\n\treturn 0;\n}\n\nstatic inline int pqi_set_pcie_completion_timeout(struct pci_dev *pci_dev, u16 timeout)\n{\n\tint rc;\n\n\trc = pcie_capability_clear_and_set_word(pci_dev, PCI_EXP_DEVCTL2,\n\t\tPCI_EXP_DEVCTL2_COMP_TIMEOUT, timeout);\n\n\treturn pcibios_err_to_errno(rc);\n}\n\nstatic int pqi_pci_init(struct pqi_ctrl_info *ctrl_info)\n{\n\tint rc;\n\tu64 mask;\n\n\trc = pci_enable_device(ctrl_info->pci_dev);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to enable PCI device\\n\");\n\t\treturn rc;\n\t}\n\n\tif (sizeof(dma_addr_t) > 4)\n\t\tmask = DMA_BIT_MASK(64);\n\telse\n\t\tmask = DMA_BIT_MASK(32);\n\n\trc = dma_set_mask_and_coherent(&ctrl_info->pci_dev->dev, mask);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev, \"failed to set DMA mask\\n\");\n\t\tgoto disable_device;\n\t}\n\n\trc = pci_request_regions(ctrl_info->pci_dev, DRIVER_NAME_SHORT);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to obtain PCI resources\\n\");\n\t\tgoto disable_device;\n\t}\n\n\tctrl_info->iomem_base = ioremap(pci_resource_start(\n\t\tctrl_info->pci_dev, 0),\n\t\tpci_resource_len(ctrl_info->pci_dev, 0));\n\tif (!ctrl_info->iomem_base) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to map memory for controller registers\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto release_regions;\n\t}\n\n#define PCI_EXP_COMP_TIMEOUT_65_TO_210_MS\t\t0x6\n\n\t \n\trc = pqi_set_pcie_completion_timeout(ctrl_info->pci_dev,\n\t\tPCI_EXP_COMP_TIMEOUT_65_TO_210_MS);\n\tif (rc) {\n\t\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\t\"failed to set PCIe completion timeout\\n\");\n\t\tgoto release_regions;\n\t}\n\n\t \n\tpci_set_master(ctrl_info->pci_dev);\n\n\tctrl_info->registers = ctrl_info->iomem_base;\n\tctrl_info->pqi_registers = &ctrl_info->registers->pqi_registers;\n\n\tpci_set_drvdata(ctrl_info->pci_dev, ctrl_info);\n\n\treturn 0;\n\nrelease_regions:\n\tpci_release_regions(ctrl_info->pci_dev);\ndisable_device:\n\tpci_disable_device(ctrl_info->pci_dev);\n\n\treturn rc;\n}\n\nstatic void pqi_cleanup_pci_init(struct pqi_ctrl_info *ctrl_info)\n{\n\tiounmap(ctrl_info->iomem_base);\n\tpci_release_regions(ctrl_info->pci_dev);\n\tif (pci_is_enabled(ctrl_info->pci_dev))\n\t\tpci_disable_device(ctrl_info->pci_dev);\n\tpci_set_drvdata(ctrl_info->pci_dev, NULL);\n}\n\nstatic struct pqi_ctrl_info *pqi_alloc_ctrl_info(int numa_node)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = kzalloc_node(sizeof(struct pqi_ctrl_info),\n\t\t\tGFP_KERNEL, numa_node);\n\tif (!ctrl_info)\n\t\treturn NULL;\n\n\tmutex_init(&ctrl_info->scan_mutex);\n\tmutex_init(&ctrl_info->lun_reset_mutex);\n\tmutex_init(&ctrl_info->ofa_mutex);\n\n\tINIT_LIST_HEAD(&ctrl_info->scsi_device_list);\n\tspin_lock_init(&ctrl_info->scsi_device_list_lock);\n\n\tINIT_WORK(&ctrl_info->event_work, pqi_event_worker);\n\tatomic_set(&ctrl_info->num_interrupts, 0);\n\n\tINIT_DELAYED_WORK(&ctrl_info->rescan_work, pqi_rescan_worker);\n\tINIT_DELAYED_WORK(&ctrl_info->update_time_work, pqi_update_time_worker);\n\n\ttimer_setup(&ctrl_info->heartbeat_timer, pqi_heartbeat_timer_handler, 0);\n\tINIT_WORK(&ctrl_info->ctrl_offline_work, pqi_ctrl_offline_worker);\n\n\tINIT_WORK(&ctrl_info->ofa_memory_alloc_work, pqi_ofa_memory_alloc_worker);\n\tINIT_WORK(&ctrl_info->ofa_quiesce_work, pqi_ofa_quiesce_worker);\n\n\tsema_init(&ctrl_info->sync_request_sem,\n\t\tPQI_RESERVED_IO_SLOTS_SYNCHRONOUS_REQUESTS);\n\tinit_waitqueue_head(&ctrl_info->block_requests_wait);\n\n\tctrl_info->ctrl_id = atomic_inc_return(&pqi_controller_count) - 1;\n\tctrl_info->irq_mode = IRQ_MODE_NONE;\n\tctrl_info->max_msix_vectors = PQI_MAX_MSIX_VECTORS;\n\n\tctrl_info->ciss_report_log_flags = CISS_REPORT_LOG_FLAG_UNIQUE_LUN_ID;\n\tctrl_info->max_transfer_encrypted_sas_sata =\n\t\tPQI_DEFAULT_MAX_TRANSFER_ENCRYPTED_SAS_SATA;\n\tctrl_info->max_transfer_encrypted_nvme =\n\t\tPQI_DEFAULT_MAX_TRANSFER_ENCRYPTED_NVME;\n\tctrl_info->max_write_raid_5_6 = PQI_DEFAULT_MAX_WRITE_RAID_5_6;\n\tctrl_info->max_write_raid_1_10_2drive = ~0;\n\tctrl_info->max_write_raid_1_10_3drive = ~0;\n\tctrl_info->disable_managed_interrupts = pqi_disable_managed_interrupts;\n\n\treturn ctrl_info;\n}\n\nstatic inline void pqi_free_ctrl_info(struct pqi_ctrl_info *ctrl_info)\n{\n\tkfree(ctrl_info);\n}\n\nstatic void pqi_free_interrupts(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_free_irqs(ctrl_info);\n\tpqi_disable_msix_interrupts(ctrl_info);\n}\n\nstatic void pqi_free_ctrl_resources(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_free_interrupts(ctrl_info);\n\tif (ctrl_info->queue_memory_base)\n\t\tdma_free_coherent(&ctrl_info->pci_dev->dev,\n\t\t\tctrl_info->queue_memory_length,\n\t\t\tctrl_info->queue_memory_base,\n\t\t\tctrl_info->queue_memory_base_dma_handle);\n\tif (ctrl_info->admin_queue_memory_base)\n\t\tdma_free_coherent(&ctrl_info->pci_dev->dev,\n\t\t\tctrl_info->admin_queue_memory_length,\n\t\t\tctrl_info->admin_queue_memory_base,\n\t\t\tctrl_info->admin_queue_memory_base_dma_handle);\n\tpqi_free_all_io_requests(ctrl_info);\n\tif (ctrl_info->error_buffer)\n\t\tdma_free_coherent(&ctrl_info->pci_dev->dev,\n\t\t\tctrl_info->error_buffer_length,\n\t\t\tctrl_info->error_buffer,\n\t\t\tctrl_info->error_buffer_dma_handle);\n\tif (ctrl_info->iomem_base)\n\t\tpqi_cleanup_pci_init(ctrl_info);\n\tpqi_free_ctrl_info(ctrl_info);\n}\n\nstatic void pqi_remove_ctrl(struct pqi_ctrl_info *ctrl_info)\n{\n\tctrl_info->controller_online = false;\n\tpqi_stop_heartbeat_timer(ctrl_info);\n\tpqi_ctrl_block_requests(ctrl_info);\n\tpqi_cancel_rescan_worker(ctrl_info);\n\tpqi_cancel_update_time_worker(ctrl_info);\n\tif (ctrl_info->ctrl_removal_state == PQI_CTRL_SURPRISE_REMOVAL) {\n\t\tpqi_fail_all_outstanding_requests(ctrl_info);\n\t\tctrl_info->pqi_mode_enabled = false;\n\t}\n\tpqi_unregister_scsi(ctrl_info);\n\tif (ctrl_info->pqi_mode_enabled)\n\t\tpqi_revert_to_sis_mode(ctrl_info);\n\tpqi_free_ctrl_resources(ctrl_info);\n}\n\nstatic void pqi_ofa_ctrl_quiesce(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_ctrl_block_scan(ctrl_info);\n\tpqi_scsi_block_requests(ctrl_info);\n\tpqi_ctrl_block_device_reset(ctrl_info);\n\tpqi_ctrl_block_requests(ctrl_info);\n\tpqi_ctrl_wait_until_quiesced(ctrl_info);\n\tpqi_stop_heartbeat_timer(ctrl_info);\n}\n\nstatic void pqi_ofa_ctrl_unquiesce(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_start_heartbeat_timer(ctrl_info);\n\tpqi_ctrl_unblock_requests(ctrl_info);\n\tpqi_ctrl_unblock_device_reset(ctrl_info);\n\tpqi_scsi_unblock_requests(ctrl_info);\n\tpqi_ctrl_unblock_scan(ctrl_info);\n}\n\nstatic int pqi_ofa_alloc_mem(struct pqi_ctrl_info *ctrl_info, u32 total_size, u32 chunk_size)\n{\n\tint i;\n\tu32 sg_count;\n\tstruct device *dev;\n\tstruct pqi_ofa_memory *ofap;\n\tstruct pqi_sg_descriptor *mem_descriptor;\n\tdma_addr_t dma_handle;\n\n\tofap = ctrl_info->pqi_ofa_mem_virt_addr;\n\n\tsg_count = DIV_ROUND_UP(total_size, chunk_size);\n\tif (sg_count == 0 || sg_count > PQI_OFA_MAX_SG_DESCRIPTORS)\n\t\tgoto out;\n\n\tctrl_info->pqi_ofa_chunk_virt_addr = kmalloc_array(sg_count, sizeof(void *), GFP_KERNEL);\n\tif (!ctrl_info->pqi_ofa_chunk_virt_addr)\n\t\tgoto out;\n\n\tdev = &ctrl_info->pci_dev->dev;\n\n\tfor (i = 0; i < sg_count; i++) {\n\t\tctrl_info->pqi_ofa_chunk_virt_addr[i] =\n\t\t\tdma_alloc_coherent(dev, chunk_size, &dma_handle, GFP_KERNEL);\n\t\tif (!ctrl_info->pqi_ofa_chunk_virt_addr[i])\n\t\t\tgoto out_free_chunks;\n\t\tmem_descriptor = &ofap->sg_descriptor[i];\n\t\tput_unaligned_le64((u64)dma_handle, &mem_descriptor->address);\n\t\tput_unaligned_le32(chunk_size, &mem_descriptor->length);\n\t}\n\n\tput_unaligned_le32(CISS_SG_LAST, &mem_descriptor->flags);\n\tput_unaligned_le16(sg_count, &ofap->num_memory_descriptors);\n\tput_unaligned_le32(sg_count * chunk_size, &ofap->bytes_allocated);\n\n\treturn 0;\n\nout_free_chunks:\n\twhile (--i >= 0) {\n\t\tmem_descriptor = &ofap->sg_descriptor[i];\n\t\tdma_free_coherent(dev, chunk_size,\n\t\t\tctrl_info->pqi_ofa_chunk_virt_addr[i],\n\t\t\tget_unaligned_le64(&mem_descriptor->address));\n\t}\n\tkfree(ctrl_info->pqi_ofa_chunk_virt_addr);\n\nout:\n\treturn -ENOMEM;\n}\n\nstatic int pqi_ofa_alloc_host_buffer(struct pqi_ctrl_info *ctrl_info)\n{\n\tu32 total_size;\n\tu32 chunk_size;\n\tu32 min_chunk_size;\n\n\tif (ctrl_info->ofa_bytes_requested == 0)\n\t\treturn 0;\n\n\ttotal_size = PAGE_ALIGN(ctrl_info->ofa_bytes_requested);\n\tmin_chunk_size = DIV_ROUND_UP(total_size, PQI_OFA_MAX_SG_DESCRIPTORS);\n\tmin_chunk_size = PAGE_ALIGN(min_chunk_size);\n\n\tfor (chunk_size = total_size; chunk_size >= min_chunk_size;) {\n\t\tif (pqi_ofa_alloc_mem(ctrl_info, total_size, chunk_size) == 0)\n\t\t\treturn 0;\n\t\tchunk_size /= 2;\n\t\tchunk_size = PAGE_ALIGN(chunk_size);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic void pqi_ofa_setup_host_buffer(struct pqi_ctrl_info *ctrl_info)\n{\n\tstruct device *dev;\n\tstruct pqi_ofa_memory *ofap;\n\n\tdev = &ctrl_info->pci_dev->dev;\n\n\tofap = dma_alloc_coherent(dev, sizeof(*ofap),\n\t\t&ctrl_info->pqi_ofa_mem_dma_handle, GFP_KERNEL);\n\tif (!ofap)\n\t\treturn;\n\n\tctrl_info->pqi_ofa_mem_virt_addr = ofap;\n\n\tif (pqi_ofa_alloc_host_buffer(ctrl_info) < 0) {\n\t\tdev_err(dev,\n\t\t\t\"failed to allocate host buffer for Online Firmware Activation\\n\");\n\t\tdma_free_coherent(dev, sizeof(*ofap), ofap, ctrl_info->pqi_ofa_mem_dma_handle);\n\t\tctrl_info->pqi_ofa_mem_virt_addr = NULL;\n\t\treturn;\n\t}\n\n\tput_unaligned_le16(PQI_OFA_VERSION, &ofap->version);\n\tmemcpy(&ofap->signature, PQI_OFA_SIGNATURE, sizeof(ofap->signature));\n}\n\nstatic void pqi_ofa_free_host_buffer(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tstruct device *dev;\n\tstruct pqi_ofa_memory *ofap;\n\tstruct pqi_sg_descriptor *mem_descriptor;\n\tunsigned int num_memory_descriptors;\n\n\tofap = ctrl_info->pqi_ofa_mem_virt_addr;\n\tif (!ofap)\n\t\treturn;\n\n\tdev = &ctrl_info->pci_dev->dev;\n\n\tif (get_unaligned_le32(&ofap->bytes_allocated) == 0)\n\t\tgoto out;\n\n\tmem_descriptor = ofap->sg_descriptor;\n\tnum_memory_descriptors =\n\t\tget_unaligned_le16(&ofap->num_memory_descriptors);\n\n\tfor (i = 0; i < num_memory_descriptors; i++) {\n\t\tdma_free_coherent(dev,\n\t\t\tget_unaligned_le32(&mem_descriptor[i].length),\n\t\t\tctrl_info->pqi_ofa_chunk_virt_addr[i],\n\t\t\tget_unaligned_le64(&mem_descriptor[i].address));\n\t}\n\tkfree(ctrl_info->pqi_ofa_chunk_virt_addr);\n\nout:\n\tdma_free_coherent(dev, sizeof(*ofap), ofap,\n\t\tctrl_info->pqi_ofa_mem_dma_handle);\n\tctrl_info->pqi_ofa_mem_virt_addr = NULL;\n}\n\nstatic int pqi_ofa_host_memory_update(struct pqi_ctrl_info *ctrl_info)\n{\n\tu32 buffer_length;\n\tstruct pqi_vendor_general_request request;\n\tstruct pqi_ofa_memory *ofap;\n\n\tmemset(&request, 0, sizeof(request));\n\n\trequest.header.iu_type = PQI_REQUEST_IU_VENDOR_GENERAL;\n\tput_unaligned_le16(sizeof(request) - PQI_REQUEST_HEADER_LENGTH,\n\t\t&request.header.iu_length);\n\tput_unaligned_le16(PQI_VENDOR_GENERAL_HOST_MEMORY_UPDATE,\n\t\t&request.function_code);\n\n\tofap = ctrl_info->pqi_ofa_mem_virt_addr;\n\n\tif (ofap) {\n\t\tbuffer_length = offsetof(struct pqi_ofa_memory, sg_descriptor) +\n\t\t\tget_unaligned_le16(&ofap->num_memory_descriptors) *\n\t\t\tsizeof(struct pqi_sg_descriptor);\n\n\t\tput_unaligned_le64((u64)ctrl_info->pqi_ofa_mem_dma_handle,\n\t\t\t&request.data.ofa_memory_allocation.buffer_address);\n\t\tput_unaligned_le32(buffer_length,\n\t\t\t&request.data.ofa_memory_allocation.buffer_length);\n\t}\n\n\treturn pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0, NULL);\n}\n\nstatic int pqi_ofa_ctrl_restart(struct pqi_ctrl_info *ctrl_info, unsigned int delay_secs)\n{\n\tssleep(delay_secs);\n\n\treturn pqi_ctrl_init_resume(ctrl_info);\n}\n\nstatic struct pqi_raid_error_info pqi_ctrl_offline_raid_error_info = {\n\t.data_out_result = PQI_DATA_IN_OUT_HARDWARE_ERROR,\n\t.status = SAM_STAT_CHECK_CONDITION,\n};\n\nstatic void pqi_fail_all_outstanding_requests(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tstruct pqi_io_request *io_request;\n\tstruct scsi_cmnd *scmd;\n\tstruct scsi_device *sdev;\n\n\tfor (i = 0; i < ctrl_info->max_io_slots; i++) {\n\t\tio_request = &ctrl_info->io_request_pool[i];\n\t\tif (atomic_read(&io_request->refcount) == 0)\n\t\t\tcontinue;\n\n\t\tscmd = io_request->scmd;\n\t\tif (scmd) {\n\t\t\tsdev = scmd->device;\n\t\t\tif (!sdev || !scsi_device_online(sdev)) {\n\t\t\t\tpqi_free_io_request(io_request);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tset_host_byte(scmd, DID_NO_CONNECT);\n\t\t\t}\n\t\t} else {\n\t\t\tio_request->status = -ENXIO;\n\t\t\tio_request->error_info =\n\t\t\t\t&pqi_ctrl_offline_raid_error_info;\n\t\t}\n\n\t\tio_request->io_complete_callback(io_request,\n\t\t\tio_request->context);\n\t}\n}\n\nstatic void pqi_take_ctrl_offline_deferred(struct pqi_ctrl_info *ctrl_info)\n{\n\tpqi_perform_lockup_action();\n\tpqi_stop_heartbeat_timer(ctrl_info);\n\tpqi_free_interrupts(ctrl_info);\n\tpqi_cancel_rescan_worker(ctrl_info);\n\tpqi_cancel_update_time_worker(ctrl_info);\n\tpqi_ctrl_wait_until_quiesced(ctrl_info);\n\tpqi_fail_all_outstanding_requests(ctrl_info);\n\tpqi_ctrl_unblock_requests(ctrl_info);\n}\n\nstatic void pqi_ctrl_offline_worker(struct work_struct *work)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tctrl_info = container_of(work, struct pqi_ctrl_info, ctrl_offline_work);\n\tpqi_take_ctrl_offline_deferred(ctrl_info);\n}\n\nstatic char *pqi_ctrl_shutdown_reason_to_string(enum pqi_ctrl_shutdown_reason ctrl_shutdown_reason)\n{\n\tchar *string;\n\n\tswitch (ctrl_shutdown_reason) {\n\tcase PQI_IQ_NOT_DRAINED_TIMEOUT:\n\t\tstring = \"inbound queue not drained timeout\";\n\t\tbreak;\n\tcase PQI_LUN_RESET_TIMEOUT:\n\t\tstring = \"LUN reset timeout\";\n\t\tbreak;\n\tcase PQI_IO_PENDING_POST_LUN_RESET_TIMEOUT:\n\t\tstring = \"I/O pending timeout after LUN reset\";\n\t\tbreak;\n\tcase PQI_NO_HEARTBEAT:\n\t\tstring = \"no controller heartbeat detected\";\n\t\tbreak;\n\tcase PQI_FIRMWARE_KERNEL_NOT_UP:\n\t\tstring = \"firmware kernel not ready\";\n\t\tbreak;\n\tcase PQI_OFA_RESPONSE_TIMEOUT:\n\t\tstring = \"OFA response timeout\";\n\t\tbreak;\n\tcase PQI_INVALID_REQ_ID:\n\t\tstring = \"invalid request ID\";\n\t\tbreak;\n\tcase PQI_UNMATCHED_REQ_ID:\n\t\tstring = \"unmatched request ID\";\n\t\tbreak;\n\tcase PQI_IO_PI_OUT_OF_RANGE:\n\t\tstring = \"I/O queue producer index out of range\";\n\t\tbreak;\n\tcase PQI_EVENT_PI_OUT_OF_RANGE:\n\t\tstring = \"event queue producer index out of range\";\n\t\tbreak;\n\tcase PQI_UNEXPECTED_IU_TYPE:\n\t\tstring = \"unexpected IU type\";\n\t\tbreak;\n\tdefault:\n\t\tstring = \"unknown reason\";\n\t\tbreak;\n\t}\n\n\treturn string;\n}\n\nstatic void pqi_take_ctrl_offline(struct pqi_ctrl_info *ctrl_info,\n\tenum pqi_ctrl_shutdown_reason ctrl_shutdown_reason)\n{\n\tif (!ctrl_info->controller_online)\n\t\treturn;\n\n\tctrl_info->controller_online = false;\n\tctrl_info->pqi_mode_enabled = false;\n\tpqi_ctrl_block_requests(ctrl_info);\n\tif (!pqi_disable_ctrl_shutdown)\n\t\tsis_shutdown_ctrl(ctrl_info, ctrl_shutdown_reason);\n\tpci_disable_device(ctrl_info->pci_dev);\n\tdev_err(&ctrl_info->pci_dev->dev,\n\t\t\"controller offline: reason code 0x%x (%s)\\n\",\n\t\tctrl_shutdown_reason, pqi_ctrl_shutdown_reason_to_string(ctrl_shutdown_reason));\n\tschedule_work(&ctrl_info->ctrl_offline_work);\n}\n\nstatic void pqi_print_ctrl_info(struct pci_dev *pci_dev,\n\tconst struct pci_device_id *id)\n{\n\tchar *ctrl_description;\n\n\tif (id->driver_data)\n\t\tctrl_description = (char *)id->driver_data;\n\telse\n\t\tctrl_description = \"Microchip Smart Family Controller\";\n\n\tdev_info(&pci_dev->dev, \"%s found\\n\", ctrl_description);\n}\n\nstatic int pqi_pci_probe(struct pci_dev *pci_dev,\n\tconst struct pci_device_id *id)\n{\n\tint rc;\n\tint node;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tpqi_print_ctrl_info(pci_dev, id);\n\n\tif (pqi_disable_device_id_wildcards &&\n\t\tid->subvendor == PCI_ANY_ID &&\n\t\tid->subdevice == PCI_ANY_ID) {\n\t\tdev_warn(&pci_dev->dev,\n\t\t\t\"controller not probed because device ID wildcards are disabled\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (id->subvendor == PCI_ANY_ID || id->subdevice == PCI_ANY_ID)\n\t\tdev_warn(&pci_dev->dev,\n\t\t\t\"controller device ID matched using wildcards\\n\");\n\n\tnode = dev_to_node(&pci_dev->dev);\n\tif (node == NUMA_NO_NODE) {\n\t\tnode = cpu_to_node(0);\n\t\tif (node == NUMA_NO_NODE)\n\t\t\tnode = 0;\n\t\tset_dev_node(&pci_dev->dev, node);\n\t}\n\n\tctrl_info = pqi_alloc_ctrl_info(node);\n\tif (!ctrl_info) {\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"failed to allocate controller info block\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tctrl_info->numa_node = node;\n\n\tctrl_info->pci_dev = pci_dev;\n\n\trc = pqi_pci_init(ctrl_info);\n\tif (rc)\n\t\tgoto error;\n\n\trc = pqi_ctrl_init(ctrl_info);\n\tif (rc)\n\t\tgoto error;\n\n\treturn 0;\n\nerror:\n\tpqi_remove_ctrl(ctrl_info);\n\n\treturn rc;\n}\n\nstatic void pqi_pci_remove(struct pci_dev *pci_dev)\n{\n\tstruct pqi_ctrl_info *ctrl_info;\n\tu16 vendor_id;\n\tint rc;\n\n\tctrl_info = pci_get_drvdata(pci_dev);\n\tif (!ctrl_info)\n\t\treturn;\n\n\tpci_read_config_word(ctrl_info->pci_dev, PCI_SUBSYSTEM_VENDOR_ID, &vendor_id);\n\tif (vendor_id == 0xffff)\n\t\tctrl_info->ctrl_removal_state = PQI_CTRL_SURPRISE_REMOVAL;\n\telse\n\t\tctrl_info->ctrl_removal_state = PQI_CTRL_GRACEFUL_REMOVAL;\n\n\tif (ctrl_info->ctrl_removal_state == PQI_CTRL_GRACEFUL_REMOVAL) {\n\t\trc = pqi_flush_cache(ctrl_info, RESTART);\n\t\tif (rc)\n\t\t\tdev_err(&pci_dev->dev,\n\t\t\t\t\"unable to flush controller cache during remove\\n\");\n\t}\n\n\tpqi_remove_ctrl(ctrl_info);\n}\n\nstatic void pqi_crash_if_pending_command(struct pqi_ctrl_info *ctrl_info)\n{\n\tunsigned int i;\n\tstruct pqi_io_request *io_request;\n\tstruct scsi_cmnd *scmd;\n\n\tfor (i = 0; i < ctrl_info->max_io_slots; i++) {\n\t\tio_request = &ctrl_info->io_request_pool[i];\n\t\tif (atomic_read(&io_request->refcount) == 0)\n\t\t\tcontinue;\n\t\tscmd = io_request->scmd;\n\t\tWARN_ON(scmd != NULL);  \n\t\tWARN_ON(scmd == NULL);  \n\t}\n}\n\nstatic void pqi_shutdown(struct pci_dev *pci_dev)\n{\n\tint rc;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tenum bmic_flush_cache_shutdown_event shutdown_event;\n\n\tctrl_info = pci_get_drvdata(pci_dev);\n\tif (!ctrl_info) {\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"cache could not be flushed\\n\");\n\t\treturn;\n\t}\n\n\tpqi_wait_until_ofa_finished(ctrl_info);\n\n\tpqi_scsi_block_requests(ctrl_info);\n\tpqi_ctrl_block_device_reset(ctrl_info);\n\tpqi_ctrl_block_requests(ctrl_info);\n\tpqi_ctrl_wait_until_quiesced(ctrl_info);\n\n\tif (system_state == SYSTEM_RESTART)\n\t\tshutdown_event = RESTART;\n\telse\n\t\tshutdown_event = SHUTDOWN;\n\n\t \n\trc = pqi_flush_cache(ctrl_info, shutdown_event);\n\tif (rc)\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"unable to flush controller cache during shutdown\\n\");\n\n\tpqi_crash_if_pending_command(ctrl_info);\n\tpqi_reset(ctrl_info);\n}\n\nstatic void pqi_process_lockup_action_param(void)\n{\n\tunsigned int i;\n\n\tif (!pqi_lockup_action_param)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(pqi_lockup_actions); i++) {\n\t\tif (strcmp(pqi_lockup_action_param,\n\t\t\tpqi_lockup_actions[i].name) == 0) {\n\t\t\tpqi_lockup_action = pqi_lockup_actions[i].action;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tpr_warn(\"%s: invalid lockup action setting \\\"%s\\\" - supported settings: none, reboot, panic\\n\",\n\t\tDRIVER_NAME_SHORT, pqi_lockup_action_param);\n}\n\n#define PQI_CTRL_READY_TIMEOUT_PARAM_MIN_SECS\t\t30\n#define PQI_CTRL_READY_TIMEOUT_PARAM_MAX_SECS\t\t(30 * 60)\n\nstatic void pqi_process_ctrl_ready_timeout_param(void)\n{\n\tif (pqi_ctrl_ready_timeout_secs == 0)\n\t\treturn;\n\n\tif (pqi_ctrl_ready_timeout_secs < PQI_CTRL_READY_TIMEOUT_PARAM_MIN_SECS) {\n\t\tpr_warn(\"%s: ctrl_ready_timeout parm of %u second(s) is less than minimum timeout of %d seconds - setting timeout to %d seconds\\n\",\n\t\t\tDRIVER_NAME_SHORT, pqi_ctrl_ready_timeout_secs, PQI_CTRL_READY_TIMEOUT_PARAM_MIN_SECS, PQI_CTRL_READY_TIMEOUT_PARAM_MIN_SECS);\n\t\tpqi_ctrl_ready_timeout_secs = PQI_CTRL_READY_TIMEOUT_PARAM_MIN_SECS;\n\t} else if (pqi_ctrl_ready_timeout_secs > PQI_CTRL_READY_TIMEOUT_PARAM_MAX_SECS) {\n\t\tpr_warn(\"%s: ctrl_ready_timeout parm of %u seconds is greater than maximum timeout of %d seconds - setting timeout to %d seconds\\n\",\n\t\t\tDRIVER_NAME_SHORT, pqi_ctrl_ready_timeout_secs, PQI_CTRL_READY_TIMEOUT_PARAM_MAX_SECS, PQI_CTRL_READY_TIMEOUT_PARAM_MAX_SECS);\n\t\tpqi_ctrl_ready_timeout_secs = PQI_CTRL_READY_TIMEOUT_PARAM_MAX_SECS;\n\t}\n\n\tsis_ctrl_ready_timeout_secs = pqi_ctrl_ready_timeout_secs;\n}\n\nstatic void pqi_process_module_params(void)\n{\n\tpqi_process_lockup_action_param();\n\tpqi_process_ctrl_ready_timeout_param();\n}\n\n#if defined(CONFIG_PM)\n\nstatic inline enum bmic_flush_cache_shutdown_event pqi_get_flush_cache_shutdown_event(struct pci_dev *pci_dev)\n{\n\tif (pci_dev->subsystem_vendor == PCI_VENDOR_ID_ADAPTEC2 && pci_dev->subsystem_device == 0x1304)\n\t\treturn RESTART;\n\n\treturn SUSPEND;\n}\n\nstatic int pqi_suspend_or_freeze(struct device *dev, bool suspend)\n{\n\tstruct pci_dev *pci_dev;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tpci_dev = to_pci_dev(dev);\n\tctrl_info = pci_get_drvdata(pci_dev);\n\n\tpqi_wait_until_ofa_finished(ctrl_info);\n\n\tpqi_ctrl_block_scan(ctrl_info);\n\tpqi_scsi_block_requests(ctrl_info);\n\tpqi_ctrl_block_device_reset(ctrl_info);\n\tpqi_ctrl_block_requests(ctrl_info);\n\tpqi_ctrl_wait_until_quiesced(ctrl_info);\n\n\tif (suspend) {\n\t\tenum bmic_flush_cache_shutdown_event shutdown_event;\n\n\t\tshutdown_event = pqi_get_flush_cache_shutdown_event(pci_dev);\n\t\tpqi_flush_cache(ctrl_info, shutdown_event);\n\t}\n\n\tpqi_stop_heartbeat_timer(ctrl_info);\n\tpqi_crash_if_pending_command(ctrl_info);\n\tpqi_free_irqs(ctrl_info);\n\n\tctrl_info->controller_online = false;\n\tctrl_info->pqi_mode_enabled = false;\n\n\treturn 0;\n}\n\nstatic __maybe_unused int pqi_suspend(struct device *dev)\n{\n\treturn pqi_suspend_or_freeze(dev, true);\n}\n\nstatic int pqi_resume_or_restore(struct device *dev)\n{\n\tint rc;\n\tstruct pci_dev *pci_dev;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tpci_dev = to_pci_dev(dev);\n\tctrl_info = pci_get_drvdata(pci_dev);\n\n\trc = pqi_request_irqs(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tpqi_ctrl_unblock_device_reset(ctrl_info);\n\tpqi_ctrl_unblock_requests(ctrl_info);\n\tpqi_scsi_unblock_requests(ctrl_info);\n\tpqi_ctrl_unblock_scan(ctrl_info);\n\n\tssleep(PQI_POST_RESET_DELAY_SECS);\n\n\treturn pqi_ctrl_init_resume(ctrl_info);\n}\n\nstatic int pqi_freeze(struct device *dev)\n{\n\treturn pqi_suspend_or_freeze(dev, false);\n}\n\nstatic int pqi_thaw(struct device *dev)\n{\n\tint rc;\n\tstruct pci_dev *pci_dev;\n\tstruct pqi_ctrl_info *ctrl_info;\n\n\tpci_dev = to_pci_dev(dev);\n\tctrl_info = pci_get_drvdata(pci_dev);\n\n\trc = pqi_request_irqs(ctrl_info);\n\tif (rc)\n\t\treturn rc;\n\n\tctrl_info->controller_online = true;\n\tctrl_info->pqi_mode_enabled = true;\n\n\tpqi_ctrl_unblock_device_reset(ctrl_info);\n\tpqi_ctrl_unblock_requests(ctrl_info);\n\tpqi_scsi_unblock_requests(ctrl_info);\n\tpqi_ctrl_unblock_scan(ctrl_info);\n\n\treturn 0;\n}\n\nstatic int pqi_poweroff(struct device *dev)\n{\n\tstruct pci_dev *pci_dev;\n\tstruct pqi_ctrl_info *ctrl_info;\n\tenum bmic_flush_cache_shutdown_event shutdown_event;\n\n\tpci_dev = to_pci_dev(dev);\n\tctrl_info = pci_get_drvdata(pci_dev);\n\n\tshutdown_event = pqi_get_flush_cache_shutdown_event(pci_dev);\n\tpqi_flush_cache(ctrl_info, shutdown_event);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops pqi_pm_ops = {\n\t.suspend = pqi_suspend,\n\t.resume = pqi_resume_or_restore,\n\t.freeze = pqi_freeze,\n\t.thaw = pqi_thaw,\n\t.poweroff = pqi_poweroff,\n\t.restore = pqi_resume_or_restore,\n};\n\n#endif  \n\n \nstatic const struct pci_device_id pqi_pci_id_table[] = {\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x105b, 0x1211)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x105b, 0x1321)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x152d, 0x8a22)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x152d, 0x8a23)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x152d, 0x8a24)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x152d, 0x8a36)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x152d, 0x8a37)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1104)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1105)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1106)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1107)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1108)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x1109)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x110b)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x8460)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0x8461)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0xc460)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0xc461)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0xf460)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x193d, 0xf461)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0045)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0046)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0047)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0048)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x004a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x004b)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x004c)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x004f)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0051)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0052)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0053)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0054)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x006b)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x006c)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x006d)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x006f)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0070)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0071)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0072)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0086)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0087)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0088)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1bd4, 0x0089)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd227)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd228)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd229)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd22a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd22b)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x19e5, 0xd22c)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0110)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0608)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0659)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0800)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0801)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0802)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0803)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0804)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0805)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0806)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0807)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0808)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0809)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x080a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0900)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0901)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0902)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0903)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0904)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0905)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0906)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0907)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x0908)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x090a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1200)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1201)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1202)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1280)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1281)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1282)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1300)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1301)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1302)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1303)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1304)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1380)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1400)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1402)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1410)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1411)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1412)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1420)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1430)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1440)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1441)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1450)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1452)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1460)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1461)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1462)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1463)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1470)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1471)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1472)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1473)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1474)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1475)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1480)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1490)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x1491)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a1)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a2)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a4)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a5)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14a6)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14b0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14b1)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14c0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14c1)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14c2)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14c3)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14c4)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14d0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14e0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADAPTEC2, 0x14f0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_ADVANTECH, 0x8312)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_DELL, 0x1fe0)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0600)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0601)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0602)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0603)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0609)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0650)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0651)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0652)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0653)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0654)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0655)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0700)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x0701)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x1001)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x1002)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x1100)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_HP, 0x1101)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x0294)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x02db)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x02dc)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x032e)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x036f)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x0381)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x0382)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1590, 0x0383)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1d8d, 0x0800)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1d8d, 0x0908)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1d8d, 0x0806)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1d8d, 0x0916)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_GIGABYTE, 0x1000)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1dfc, 0x3161)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1f0c, 0x3161)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0804)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0805)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0806)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x5445)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x5446)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x5447)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x5449)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x544a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x544b)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x544d)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x544e)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x544f)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x54da)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x54db)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x54dc)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0b27)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0b29)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cf2, 0x0b45)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cc4, 0x0101)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       0x1cc4, 0x0201)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0220)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0221)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0520)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0522)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0620)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0621)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0622)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_VENDOR_ID_LENOVO, 0x0623)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1014, 0x0718)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1e93, 0x1000)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1e93, 0x1001)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1e93, 0x1002)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1e93, 0x1005)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1001)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1002)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1003)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1004)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1005)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1006)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1007)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1008)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x1009)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t\t0x1f51, 0x100a)\n\t},\n\t{\n\t\tPCI_DEVICE_SUB(PCI_VENDOR_ID_ADAPTEC2, 0x028f,\n\t\t\t       PCI_ANY_ID, PCI_ANY_ID)\n\t},\n\t{ 0 }\n};\n\nMODULE_DEVICE_TABLE(pci, pqi_pci_id_table);\n\nstatic struct pci_driver pqi_pci_driver = {\n\t.name = DRIVER_NAME_SHORT,\n\t.id_table = pqi_pci_id_table,\n\t.probe = pqi_pci_probe,\n\t.remove = pqi_pci_remove,\n\t.shutdown = pqi_shutdown,\n#if defined(CONFIG_PM)\n\t.driver = {\n\t\t.pm = &pqi_pm_ops\n\t},\n#endif\n};\n\nstatic int __init pqi_init(void)\n{\n\tint rc;\n\n\tpr_info(DRIVER_NAME \"\\n\");\n\tpqi_verify_structures();\n\tsis_verify_structures();\n\n\tpqi_sas_transport_template = sas_attach_transport(&pqi_sas_transport_functions);\n\tif (!pqi_sas_transport_template)\n\t\treturn -ENODEV;\n\n\tpqi_process_module_params();\n\n\trc = pci_register_driver(&pqi_pci_driver);\n\tif (rc)\n\t\tsas_release_transport(pqi_sas_transport_template);\n\n\treturn rc;\n}\n\nstatic void __exit pqi_cleanup(void)\n{\n\tpci_unregister_driver(&pqi_pci_driver);\n\tsas_release_transport(pqi_sas_transport_template);\n}\n\nmodule_init(pqi_init);\nmodule_exit(pqi_cleanup);\n\nstatic void pqi_verify_structures(void)\n{\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_host_to_ctrl_doorbell) != 0x20);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_interrupt_mask) != 0x34);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_ctrl_to_host_doorbell) != 0x9c);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_ctrl_to_host_doorbell_clear) != 0xa0);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_driver_scratch) != 0xb0);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_product_identifier) != 0xb4);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_firmware_status) != 0xbc);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_ctrl_shutdown_reason_code) != 0xcc);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tsis_mailbox) != 0x1000);\n\tBUILD_BUG_ON(offsetof(struct pqi_ctrl_registers,\n\t\tpqi_registers) != 0x4000);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_header,\n\t\tiu_type) != 0x0);\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_header,\n\t\tiu_length) != 0x2);\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_header,\n\t\tresponse_queue_id) != 0x4);\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_header,\n\t\tdriver_flags) != 0x6);\n\tBUILD_BUG_ON(sizeof(struct pqi_iu_header) != 0x8);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tstatus) != 0x0);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tservice_response) != 0x1);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tdata_present) != 0x2);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\treserved) != 0x3);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tresidual_count) != 0x4);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tdata_length) != 0x8);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\treserved1) != 0xa);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_error_info,\n\t\tdata) != 0xc);\n\tBUILD_BUG_ON(sizeof(struct pqi_aio_error_info) != 0x10c);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tdata_in_result) != 0x0);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tdata_out_result) != 0x1);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\treserved) != 0x2);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tstatus) != 0x5);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tstatus_qualifier) != 0x6);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tsense_data_length) != 0x8);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tresponse_data_length) != 0xa);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tdata_in_transferred) != 0xc);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tdata_out_transferred) != 0x10);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_error_info,\n\t\tdata) != 0x14);\n\tBUILD_BUG_ON(sizeof(struct pqi_raid_error_info) != 0x114);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tsignature) != 0x0);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tfunction_and_status_code) != 0x8);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tmax_admin_iq_elements) != 0x10);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tmax_admin_oq_elements) != 0x11);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_iq_element_length) != 0x12);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_oq_element_length) != 0x13);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tmax_reset_timeout) != 0x14);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tlegacy_intx_status) != 0x18);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tlegacy_intx_mask_set) != 0x1c);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tlegacy_intx_mask_clear) != 0x20);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tdevice_status) != 0x40);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_iq_pi_offset) != 0x48);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_oq_ci_offset) != 0x50);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_iq_element_array_addr) != 0x58);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_oq_element_array_addr) != 0x60);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_iq_ci_addr) != 0x68);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_oq_pi_addr) != 0x70);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_iq_num_elements) != 0x78);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_oq_num_elements) != 0x79);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tadmin_queue_int_msg_num) != 0x7a);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tdevice_error) != 0x80);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\terror_details) != 0x88);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tdevice_reset) != 0x90);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_registers,\n\t\tpower_action) != 0x94);\n\tBUILD_BUG_ON(sizeof(struct pqi_device_registers) != 0x100);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\theader.driver_flags) != 6);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tfunction_code) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.report_device_capability.buffer_length) != 44);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.report_device_capability.sg_descriptor) != 48);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.queue_id) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.element_array_addr) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.ci_addr) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.num_elements) != 32);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.element_length) != 34);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq.queue_protocol) != 36);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.queue_id) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.element_array_addr) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.pi_addr) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.num_elements) != 32);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.element_length) != 34);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.queue_protocol) != 36);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.int_msg_num) != 40);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.coalescing_count) != 42);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.min_coalescing_time) != 44);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq.max_coalescing_time) != 48);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_request,\n\t\tdata.delete_operational_queue.queue_id) != 12);\n\tBUILD_BUG_ON(sizeof(struct pqi_general_admin_request) != 64);\n\tBUILD_BUG_ON(sizeof_field(struct pqi_general_admin_request,\n\t\tdata.create_operational_iq) != 64 - 11);\n\tBUILD_BUG_ON(sizeof_field(struct pqi_general_admin_request,\n\t\tdata.create_operational_oq) != 64 - 11);\n\tBUILD_BUG_ON(sizeof_field(struct pqi_general_admin_request,\n\t\tdata.delete_operational_queue) != 64 - 11);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\theader.driver_flags) != 6);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tfunction_code) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tstatus) != 11);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tdata.create_operational_iq.status_descriptor) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tdata.create_operational_iq.iq_pi_offset) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tdata.create_operational_oq.status_descriptor) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_admin_response,\n\t\tdata.create_operational_oq.oq_ci_offset) != 16);\n\tBUILD_BUG_ON(sizeof(struct pqi_general_admin_response) != 64);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\theader.response_queue_id) != 4);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\theader.driver_flags) != 6);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tnexus_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tbuffer_length) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tlun_number) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tprotocol_specific) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\terror_index) != 27);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tcdb) != 32);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\ttimeout) != 60);\n\tBUILD_BUG_ON(offsetof(struct pqi_raid_path_request,\n\t\tsg_descriptors) != 64);\n\tBUILD_BUG_ON(sizeof(struct pqi_raid_path_request) !=\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\theader.response_queue_id) != 4);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\theader.driver_flags) != 6);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tnexus_id) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tbuffer_length) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tdata_encryption_key_index) != 22);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tencrypt_tweak_lower) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tencrypt_tweak_upper) != 28);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tcdb) != 32);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\terror_index) != 48);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tnum_sg_descriptors) != 50);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tcdb_length) != 51);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tlun_number) != 52);\n\tBUILD_BUG_ON(offsetof(struct pqi_aio_path_request,\n\t\tsg_descriptors) != 64);\n\tBUILD_BUG_ON(sizeof(struct pqi_aio_path_request) !=\n\t\tPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_io_response,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_io_response,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_io_response,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_io_response,\n\t\terror_index) != 10);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\theader.response_queue_id) != 4);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\tdata.report_event_configuration.buffer_length) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\tdata.report_event_configuration.sg_descriptors) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\tdata.set_event_configuration.global_event_oq_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\tdata.set_event_configuration.buffer_length) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_general_management_request,\n\t\tdata.set_event_configuration.sg_descriptors) != 16);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_layer_descriptor,\n\t\tmax_inbound_iu_length) != 6);\n\tBUILD_BUG_ON(offsetof(struct pqi_iu_layer_descriptor,\n\t\tmax_outbound_iu_length) != 14);\n\tBUILD_BUG_ON(sizeof(struct pqi_iu_layer_descriptor) != 16);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tdata_length) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tiq_arbitration_priority_support_bitmask) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmaximum_aw_a) != 9);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmaximum_aw_b) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmaximum_aw_c) != 11);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_inbound_queues) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_elements_per_iq) != 18);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_iq_element_length) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmin_iq_element_length) != 26);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_outbound_queues) != 30);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_elements_per_oq) != 32);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tintr_coalescing_time_granularity) != 34);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmax_oq_element_length) != 36);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tmin_oq_element_length) != 38);\n\tBUILD_BUG_ON(offsetof(struct pqi_device_capability,\n\t\tiu_layer_descriptors) != 64);\n\tBUILD_BUG_ON(sizeof(struct pqi_device_capability) != 576);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_event_descriptor,\n\t\tevent_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_descriptor,\n\t\toq_id) != 2);\n\tBUILD_BUG_ON(sizeof(struct pqi_event_descriptor) != 4);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_event_config,\n\t\tnum_event_descriptors) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_config,\n\t\tdescriptors) != 4);\n\n\tBUILD_BUG_ON(PQI_NUM_SUPPORTED_EVENTS !=\n\t\tARRAY_SIZE(pqi_supported_event_types));\n\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\tevent_type) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\tevent_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\tadditional_event_id) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_response,\n\t\tdata) != 16);\n\tBUILD_BUG_ON(sizeof(struct pqi_event_response) != 32);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_event_acknowledge_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_acknowledge_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_acknowledge_request,\n\t\tevent_type) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_acknowledge_request,\n\t\tevent_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_event_acknowledge_request,\n\t\tadditional_event_id) != 12);\n\tBUILD_BUG_ON(sizeof(struct pqi_event_acknowledge_request) != 16);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\tnexus_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\ttimeout) != 14);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\tlun_number) != 16);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\tprotocol_specific) != 24);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\toutbound_queue_id_to_manage) != 26);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\trequest_id_to_manage) != 28);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_request,\n\t\ttask_management_function) != 30);\n\tBUILD_BUG_ON(sizeof(struct pqi_task_management_request) != 32);\n\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\theader.iu_type) != 0);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\theader.iu_length) != 2);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\trequest_id) != 8);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\tnexus_id) != 10);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\tadditional_response_info) != 12);\n\tBUILD_BUG_ON(offsetof(struct pqi_task_management_response,\n\t\tresponse_code) != 15);\n\tBUILD_BUG_ON(sizeof(struct pqi_task_management_response) != 16);\n\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tconfigured_logical_drive_count) != 0);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tconfiguration_signature) != 1);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tfirmware_version_short) != 5);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\textended_logical_unit_count) != 154);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tfirmware_build_number) != 190);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tvendor_id) != 200);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tproduct_id) != 208);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\textra_controller_flags) != 286);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tcontroller_mode) != 292);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tspare_part_number) != 293);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_controller,\n\t\tfirmware_version_long) != 325);\n\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\tphys_bay_in_box) != 115);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\tdevice_type) != 120);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\tredundant_path_present_map) != 1736);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\tactive_path_number) != 1738);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\talternate_paths_phys_connector) != 1739);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\talternate_paths_phys_box_on_port) != 1755);\n\tBUILD_BUG_ON(offsetof(struct bmic_identify_physical_device,\n\t\tcurrent_queue_depth_limit) != 1796);\n\tBUILD_BUG_ON(sizeof(struct bmic_identify_physical_device) != 2560);\n\n\tBUILD_BUG_ON(sizeof(struct bmic_sense_feature_buffer_header) != 4);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_buffer_header,\n\t\tpage_code) != 0);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_buffer_header,\n\t\tsubpage_code) != 1);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_buffer_header,\n\t\tbuffer_length) != 2);\n\n\tBUILD_BUG_ON(sizeof(struct bmic_sense_feature_page_header) != 4);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_page_header,\n\t\tpage_code) != 0);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_page_header,\n\t\tsubpage_code) != 1);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_page_header,\n\t\tpage_length) != 2);\n\n\tBUILD_BUG_ON(sizeof(struct bmic_sense_feature_io_page_aio_subpage)\n\t\t!= 18);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\theader) != 0);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tfirmware_read_support) != 4);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tdriver_read_support) != 5);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tfirmware_write_support) != 6);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tdriver_write_support) != 7);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tmax_transfer_encrypted_sas_sata) != 8);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tmax_transfer_encrypted_nvme) != 10);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tmax_write_raid_5_6) != 12);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tmax_write_raid_1_10_2drive) != 14);\n\tBUILD_BUG_ON(offsetof(struct bmic_sense_feature_io_page_aio_subpage,\n\t\tmax_write_raid_1_10_3drive) != 16);\n\n\tBUILD_BUG_ON(PQI_ADMIN_IQ_NUM_ELEMENTS > 255);\n\tBUILD_BUG_ON(PQI_ADMIN_OQ_NUM_ELEMENTS > 255);\n\tBUILD_BUG_ON(PQI_ADMIN_IQ_ELEMENT_LENGTH %\n\t\tPQI_QUEUE_ELEMENT_LENGTH_ALIGNMENT != 0);\n\tBUILD_BUG_ON(PQI_ADMIN_OQ_ELEMENT_LENGTH %\n\t\tPQI_QUEUE_ELEMENT_LENGTH_ALIGNMENT != 0);\n\tBUILD_BUG_ON(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH > 1048560);\n\tBUILD_BUG_ON(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH %\n\t\tPQI_QUEUE_ELEMENT_LENGTH_ALIGNMENT != 0);\n\tBUILD_BUG_ON(PQI_OPERATIONAL_OQ_ELEMENT_LENGTH > 1048560);\n\tBUILD_BUG_ON(PQI_OPERATIONAL_OQ_ELEMENT_LENGTH %\n\t\tPQI_QUEUE_ELEMENT_LENGTH_ALIGNMENT != 0);\n\n\tBUILD_BUG_ON(PQI_RESERVED_IO_SLOTS >= PQI_MAX_OUTSTANDING_REQUESTS);\n\tBUILD_BUG_ON(PQI_RESERVED_IO_SLOTS >=\n\t\tPQI_MAX_OUTSTANDING_REQUESTS_KDUMP);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}