{
  "module_name": "caamhash.c",
  "hash_id": "af2a9d5cd17135bab92832853938aeba23ecafb11c303f74b557e0064697f94c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/caam/caamhash.c",
  "human_readable_source": "\n \n\n#include \"compat.h\"\n\n#include \"regs.h\"\n#include \"intern.h\"\n#include \"desc_constr.h\"\n#include \"jr.h\"\n#include \"error.h\"\n#include \"sg_sw_sec4.h\"\n#include \"key_gen.h\"\n#include \"caamhash_desc.h\"\n#include <crypto/internal/engine.h>\n#include <crypto/internal/hash.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n\n#define CAAM_CRA_PRIORITY\t\t3000\n\n \n#define CAAM_MAX_HASH_KEY_SIZE\t\t(SHA512_DIGEST_SIZE * 2)\n\n#define CAAM_MAX_HASH_BLOCK_SIZE\tSHA512_BLOCK_SIZE\n#define CAAM_MAX_HASH_DIGEST_SIZE\tSHA512_DIGEST_SIZE\n\n#define DESC_HASH_MAX_USED_BYTES\t(DESC_AHASH_FINAL_LEN + \\\n\t\t\t\t\t CAAM_MAX_HASH_KEY_SIZE)\n#define DESC_HASH_MAX_USED_LEN\t\t(DESC_HASH_MAX_USED_BYTES / CAAM_CMD_SZ)\n\n \n#define HASH_MSG_LEN\t\t\t8\n#define MAX_CTX_LEN\t\t\t(HASH_MSG_LEN + SHA512_DIGEST_SIZE)\n\nstatic struct list_head hash_list;\n\n \nstruct caam_hash_ctx {\n\tu32 sh_desc_update[DESC_HASH_MAX_USED_LEN] ____cacheline_aligned;\n\tu32 sh_desc_update_first[DESC_HASH_MAX_USED_LEN] ____cacheline_aligned;\n\tu32 sh_desc_fin[DESC_HASH_MAX_USED_LEN] ____cacheline_aligned;\n\tu32 sh_desc_digest[DESC_HASH_MAX_USED_LEN] ____cacheline_aligned;\n\tu8 key[CAAM_MAX_HASH_KEY_SIZE] ____cacheline_aligned;\n\tdma_addr_t sh_desc_update_dma ____cacheline_aligned;\n\tdma_addr_t sh_desc_update_first_dma;\n\tdma_addr_t sh_desc_fin_dma;\n\tdma_addr_t sh_desc_digest_dma;\n\tenum dma_data_direction dir;\n\tenum dma_data_direction key_dir;\n\tstruct device *jrdev;\n\tint ctx_len;\n\tstruct alginfo adata;\n};\n\n \nstruct caam_hash_state {\n\tdma_addr_t buf_dma;\n\tdma_addr_t ctx_dma;\n\tint ctx_dma_len;\n\tu8 buf[CAAM_MAX_HASH_BLOCK_SIZE] ____cacheline_aligned;\n\tint buflen;\n\tint next_buflen;\n\tu8 caam_ctx[MAX_CTX_LEN] ____cacheline_aligned;\n\tint (*update)(struct ahash_request *req) ____cacheline_aligned;\n\tint (*final)(struct ahash_request *req);\n\tint (*finup)(struct ahash_request *req);\n\tstruct ahash_edesc *edesc;\n\tvoid (*ahash_op_done)(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t      void *context);\n};\n\nstruct caam_export_state {\n\tu8 buf[CAAM_MAX_HASH_BLOCK_SIZE];\n\tu8 caam_ctx[MAX_CTX_LEN];\n\tint buflen;\n\tint (*update)(struct ahash_request *req);\n\tint (*final)(struct ahash_request *req);\n\tint (*finup)(struct ahash_request *req);\n};\n\nstatic inline bool is_cmac_aes(u32 algtype)\n{\n\treturn (algtype & (OP_ALG_ALGSEL_MASK | OP_ALG_AAI_MASK)) ==\n\t       (OP_ALG_ALGSEL_AES | OP_ALG_AAI_CMAC);\n}\n \n\n \nstatic inline int map_seq_out_ptr_ctx(u32 *desc, struct device *jrdev,\n\t\t\t\t      struct caam_hash_state *state,\n\t\t\t\t      int ctx_len)\n{\n\tstate->ctx_dma_len = ctx_len;\n\tstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx,\n\t\t\t\t\tctx_len, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(jrdev, state->ctx_dma)) {\n\t\tdev_err(jrdev, \"unable to map ctx\\n\");\n\t\tstate->ctx_dma = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\tappend_seq_out_ptr(desc, state->ctx_dma, ctx_len, 0);\n\n\treturn 0;\n}\n\n \nstatic inline int buf_map_to_sec4_sg(struct device *jrdev,\n\t\t\t\t     struct sec4_sg_entry *sec4_sg,\n\t\t\t\t     struct caam_hash_state *state)\n{\n\tint buflen = state->buflen;\n\n\tif (!buflen)\n\t\treturn 0;\n\n\tstate->buf_dma = dma_map_single(jrdev, state->buf, buflen,\n\t\t\t\t\tDMA_TO_DEVICE);\n\tif (dma_mapping_error(jrdev, state->buf_dma)) {\n\t\tdev_err(jrdev, \"unable to map buf\\n\");\n\t\tstate->buf_dma = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\tdma_to_sec4_sg_one(sec4_sg, state->buf_dma, buflen, 0);\n\n\treturn 0;\n}\n\n \nstatic inline int ctx_map_to_sec4_sg(struct device *jrdev,\n\t\t\t\t     struct caam_hash_state *state, int ctx_len,\n\t\t\t\t     struct sec4_sg_entry *sec4_sg, u32 flag)\n{\n\tstate->ctx_dma_len = ctx_len;\n\tstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx, ctx_len, flag);\n\tif (dma_mapping_error(jrdev, state->ctx_dma)) {\n\t\tdev_err(jrdev, \"unable to map ctx\\n\");\n\t\tstate->ctx_dma = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\tdma_to_sec4_sg_one(sec4_sg, state->ctx_dma, ctx_len, 0);\n\n\treturn 0;\n}\n\nstatic int ahash_set_sh_desc(struct crypto_ahash *ahash)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct device *jrdev = ctx->jrdev;\n\tstruct caam_drv_private *ctrlpriv = dev_get_drvdata(jrdev->parent);\n\tu32 *desc;\n\n\tctx->adata.key_virt = ctx->key;\n\n\t \n\tdesc = ctx->sh_desc_update;\n\tcnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_UPDATE, ctx->ctx_len,\n\t\t\t  ctx->ctx_len, true, ctrlpriv->era);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\n\tprint_hex_dump_debug(\"ahash update shdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\t \n\tdesc = ctx->sh_desc_update_first;\n\tcnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_INIT, ctx->ctx_len,\n\t\t\t  ctx->ctx_len, false, ctrlpriv->era);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_first_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"ahash update first shdesc@\"__stringify(__LINE__)\n\t\t\t     \": \", DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_fin;\n\tcnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_FINALIZE, digestsize,\n\t\t\t  ctx->ctx_len, true, ctrlpriv->era);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_fin_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\n\tprint_hex_dump_debug(\"ahash final shdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_digest;\n\tcnstr_shdsc_ahash(desc, &ctx->adata, OP_ALG_AS_INITFINAL, digestsize,\n\t\t\t  ctx->ctx_len, false, ctrlpriv->era);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_digest_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\n\tprint_hex_dump_debug(\"ahash digest shdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\treturn 0;\n}\n\nstatic int axcbc_set_sh_desc(struct crypto_ahash *ahash)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct device *jrdev = ctx->jrdev;\n\tu32 *desc;\n\n\t \n\tdesc = ctx->sh_desc_update;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_UPDATE,\n\t\t\t    ctx->ctx_len, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"axcbc update shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\t \n\tdesc = ctx->sh_desc_fin;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_FINALIZE,\n\t\t\t    digestsize, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_fin_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"axcbc finup shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\t \n\tctx->adata.key_virt = ctx->key;\n\n\t \n\tdesc = ctx->sh_desc_update_first;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_INIT, ctx->ctx_len,\n\t\t\t    ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_first_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"axcbc update first shdesc@\" __stringify(__LINE__)\n\t\t\t     \" : \", DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_digest;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_INITFINAL,\n\t\t\t    digestsize, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_digest_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"axcbc digest shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\treturn 0;\n}\n\nstatic int acmac_set_sh_desc(struct crypto_ahash *ahash)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct device *jrdev = ctx->jrdev;\n\tu32 *desc;\n\n\t \n\tdesc = ctx->sh_desc_update;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_UPDATE,\n\t\t\t    ctx->ctx_len, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"acmac update shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_fin;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_FINALIZE,\n\t\t\t    digestsize, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_fin_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"acmac finup shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_update_first;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_INIT, ctx->ctx_len,\n\t\t\t    ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_update_first_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"acmac update first shdesc@\" __stringify(__LINE__)\n\t\t\t     \" : \", DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\t \n\tdesc = ctx->sh_desc_digest;\n\tcnstr_shdsc_sk_hash(desc, &ctx->adata, OP_ALG_AS_INITFINAL,\n\t\t\t    digestsize, ctx->ctx_len);\n\tdma_sync_single_for_device(jrdev, ctx->sh_desc_digest_dma,\n\t\t\t\t   desc_bytes(desc), ctx->dir);\n\tprint_hex_dump_debug(\"acmac digest shdesc@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t     desc_bytes(desc), 1);\n\n\treturn 0;\n}\n\n \nstatic int hash_digest_key(struct caam_hash_ctx *ctx, u32 *keylen, u8 *key,\n\t\t\t   u32 digestsize)\n{\n\tstruct device *jrdev = ctx->jrdev;\n\tu32 *desc;\n\tstruct split_key_result result;\n\tdma_addr_t key_dma;\n\tint ret;\n\n\tdesc = kmalloc(CAAM_CMD_SZ * 8 + CAAM_PTR_SZ * 2, GFP_KERNEL);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\tinit_job_desc(desc, 0);\n\n\tkey_dma = dma_map_single(jrdev, key, *keylen, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(jrdev, key_dma)) {\n\t\tdev_err(jrdev, \"unable to map key memory\\n\");\n\t\tkfree(desc);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tappend_operation(desc, ctx->adata.algtype | OP_ALG_ENCRYPT |\n\t\t\t OP_ALG_AS_INITFINAL);\n\tappend_seq_in_ptr(desc, key_dma, *keylen, 0);\n\tappend_seq_fifo_load(desc, *keylen, FIFOLD_CLASS_CLASS2 |\n\t\t\t     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_MSG);\n\tappend_seq_out_ptr(desc, key_dma, digestsize, 0);\n\tappend_seq_store(desc, digestsize, LDST_CLASS_2_CCB |\n\t\t\t LDST_SRCDST_BYTE_CONTEXT);\n\n\tprint_hex_dump_debug(\"key_in@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, key, *keylen, 1);\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\tresult.err = 0;\n\tinit_completion(&result.completion);\n\n\tret = caam_jr_enqueue(jrdev, desc, split_key_done, &result);\n\tif (ret == -EINPROGRESS) {\n\t\t \n\t\twait_for_completion(&result.completion);\n\t\tret = result.err;\n\n\t\tprint_hex_dump_debug(\"digested key@\"__stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, key,\n\t\t\t\t     digestsize, 1);\n\t}\n\tdma_unmap_single(jrdev, key_dma, *keylen, DMA_BIDIRECTIONAL);\n\n\t*keylen = digestsize;\n\n\tkfree(desc);\n\n\treturn ret;\n}\n\nstatic int ahash_setkey(struct crypto_ahash *ahash,\n\t\t\tconst u8 *key, unsigned int keylen)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct device *jrdev = ctx->jrdev;\n\tint blocksize = crypto_tfm_alg_blocksize(&ahash->base);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct caam_drv_private *ctrlpriv = dev_get_drvdata(ctx->jrdev->parent);\n\tint ret;\n\tu8 *hashed_key = NULL;\n\n\tdev_dbg(jrdev, \"keylen %d\\n\", keylen);\n\n\tif (keylen > blocksize) {\n\t\tunsigned int aligned_len =\n\t\t\tALIGN(keylen, dma_get_cache_alignment());\n\n\t\tif (aligned_len < keylen)\n\t\t\treturn -EOVERFLOW;\n\n\t\thashed_key = kmemdup(key, keylen, GFP_KERNEL);\n\t\tif (!hashed_key)\n\t\t\treturn -ENOMEM;\n\t\tret = hash_digest_key(ctx, &keylen, hashed_key, digestsize);\n\t\tif (ret)\n\t\t\tgoto bad_free_key;\n\t\tkey = hashed_key;\n\t}\n\n\t \n\tif (ctrlpriv->era >= 6) {\n\t\tctx->adata.key_inline = true;\n\t\tctx->adata.keylen = keylen;\n\t\tctx->adata.keylen_pad = split_key_len(ctx->adata.algtype &\n\t\t\t\t\t\t      OP_ALG_ALGSEL_MASK);\n\n\t\tif (ctx->adata.keylen_pad > CAAM_MAX_HASH_KEY_SIZE)\n\t\t\tgoto bad_free_key;\n\n\t\tmemcpy(ctx->key, key, keylen);\n\n\t\t \n\t\tif (keylen > ctx->adata.keylen_pad)\n\t\t\tdma_sync_single_for_device(ctx->jrdev,\n\t\t\t\t\t\t   ctx->adata.key_dma,\n\t\t\t\t\t\t   ctx->adata.keylen_pad,\n\t\t\t\t\t\t   DMA_TO_DEVICE);\n\t} else {\n\t\tret = gen_split_key(ctx->jrdev, ctx->key, &ctx->adata, key,\n\t\t\t\t    keylen, CAAM_MAX_HASH_KEY_SIZE);\n\t\tif (ret)\n\t\t\tgoto bad_free_key;\n\t}\n\n\tkfree(hashed_key);\n\treturn ahash_set_sh_desc(ahash);\n bad_free_key:\n\tkfree(hashed_key);\n\treturn -EINVAL;\n}\n\nstatic int axcbc_setkey(struct crypto_ahash *ahash, const u8 *key,\n\t\t\tunsigned int keylen)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct device *jrdev = ctx->jrdev;\n\n\tif (keylen != AES_KEYSIZE_128)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->key, key, keylen);\n\tdma_sync_single_for_device(jrdev, ctx->adata.key_dma, keylen,\n\t\t\t\t   DMA_TO_DEVICE);\n\tctx->adata.keylen = keylen;\n\n\tprint_hex_dump_debug(\"axcbc ctx.key@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, ctx->key, keylen, 1);\n\n\treturn axcbc_set_sh_desc(ahash);\n}\n\nstatic int acmac_setkey(struct crypto_ahash *ahash, const u8 *key,\n\t\t\tunsigned int keylen)\n{\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tint err;\n\n\terr = aes_check_keylen(keylen);\n\tif (err)\n\t\treturn err;\n\n\t \n\tctx->adata.key_virt = key;\n\tctx->adata.keylen = keylen;\n\n\tprint_hex_dump_debug(\"acmac ctx.key@\" __stringify(__LINE__)\" : \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);\n\n\treturn acmac_set_sh_desc(ahash);\n}\n\n \nstruct ahash_edesc {\n\tdma_addr_t sec4_sg_dma;\n\tint src_nents;\n\tint sec4_sg_bytes;\n\tbool bklog;\n\tu32 hw_desc[DESC_JOB_IO_LEN_MAX / sizeof(u32)] ____cacheline_aligned;\n\tstruct sec4_sg_entry sec4_sg[];\n};\n\nstatic inline void ahash_unmap(struct device *dev,\n\t\t\tstruct ahash_edesc *edesc,\n\t\t\tstruct ahash_request *req, int dst_len)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\tif (edesc->src_nents)\n\t\tdma_unmap_sg(dev, req->src, edesc->src_nents, DMA_TO_DEVICE);\n\n\tif (edesc->sec4_sg_bytes)\n\t\tdma_unmap_single(dev, edesc->sec4_sg_dma,\n\t\t\t\t edesc->sec4_sg_bytes, DMA_TO_DEVICE);\n\n\tif (state->buf_dma) {\n\t\tdma_unmap_single(dev, state->buf_dma, state->buflen,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tstate->buf_dma = 0;\n\t}\n}\n\nstatic inline void ahash_unmap_ctx(struct device *dev,\n\t\t\tstruct ahash_edesc *edesc,\n\t\t\tstruct ahash_request *req, int dst_len, u32 flag)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\tif (state->ctx_dma) {\n\t\tdma_unmap_single(dev, state->ctx_dma, state->ctx_dma_len, flag);\n\t\tstate->ctx_dma = 0;\n\t}\n\tahash_unmap(dev, edesc, req, dst_len);\n}\n\nstatic inline void ahash_done_cpy(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t\t  void *context, enum dma_data_direction dir)\n{\n\tstruct ahash_request *req = context;\n\tstruct caam_drv_private_jr *jrp = dev_get_drvdata(jrdev);\n\tstruct ahash_edesc *edesc;\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tint ecode = 0;\n\tbool has_bklog;\n\n\tdev_dbg(jrdev, \"%s %d: err 0x%x\\n\", __func__, __LINE__, err);\n\n\tedesc = state->edesc;\n\thas_bklog = edesc->bklog;\n\n\tif (err)\n\t\tecode = caam_jr_strstatus(jrdev, err);\n\n\tahash_unmap_ctx(jrdev, edesc, req, digestsize, dir);\n\tmemcpy(req->result, state->caam_ctx, digestsize);\n\tkfree(edesc);\n\n\tprint_hex_dump_debug(\"ctx@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\n\t\t\t     ctx->ctx_len, 1);\n\n\t \n\tif (!has_bklog)\n\t\tahash_request_complete(req, ecode);\n\telse\n\t\tcrypto_finalize_hash_request(jrp->engine, req, ecode);\n}\n\nstatic void ahash_done(struct device *jrdev, u32 *desc, u32 err,\n\t\t       void *context)\n{\n\tahash_done_cpy(jrdev, desc, err, context, DMA_FROM_DEVICE);\n}\n\nstatic void ahash_done_ctx_src(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t       void *context)\n{\n\tahash_done_cpy(jrdev, desc, err, context, DMA_BIDIRECTIONAL);\n}\n\nstatic inline void ahash_done_switch(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t\t     void *context, enum dma_data_direction dir)\n{\n\tstruct ahash_request *req = context;\n\tstruct caam_drv_private_jr *jrp = dev_get_drvdata(jrdev);\n\tstruct ahash_edesc *edesc;\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tint ecode = 0;\n\tbool has_bklog;\n\n\tdev_dbg(jrdev, \"%s %d: err 0x%x\\n\", __func__, __LINE__, err);\n\n\tedesc = state->edesc;\n\thas_bklog = edesc->bklog;\n\tif (err)\n\t\tecode = caam_jr_strstatus(jrdev, err);\n\n\tahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, dir);\n\tkfree(edesc);\n\n\tscatterwalk_map_and_copy(state->buf, req->src,\n\t\t\t\t req->nbytes - state->next_buflen,\n\t\t\t\t state->next_buflen, 0);\n\tstate->buflen = state->next_buflen;\n\n\tprint_hex_dump_debug(\"buf@\" __stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, state->buf,\n\t\t\t     state->buflen, 1);\n\n\tprint_hex_dump_debug(\"ctx@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\n\t\t\t     ctx->ctx_len, 1);\n\tif (req->result)\n\t\tprint_hex_dump_debug(\"result@\"__stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, req->result,\n\t\t\t\t     digestsize, 1);\n\n\t \n\tif (!has_bklog)\n\t\tahash_request_complete(req, ecode);\n\telse\n\t\tcrypto_finalize_hash_request(jrp->engine, req, ecode);\n\n}\n\nstatic void ahash_done_bi(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t  void *context)\n{\n\tahash_done_switch(jrdev, desc, err, context, DMA_BIDIRECTIONAL);\n}\n\nstatic void ahash_done_ctx_dst(struct device *jrdev, u32 *desc, u32 err,\n\t\t\t       void *context)\n{\n\tahash_done_switch(jrdev, desc, err, context, DMA_FROM_DEVICE);\n}\n\n \nstatic struct ahash_edesc *ahash_edesc_alloc(struct ahash_request *req,\n\t\t\t\t\t     int sg_num, u32 *sh_desc,\n\t\t\t\t\t     dma_addr_t sh_desc_dma)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tgfp_t flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t       GFP_KERNEL : GFP_ATOMIC;\n\tstruct ahash_edesc *edesc;\n\n\tedesc = kzalloc(struct_size(edesc, sec4_sg, sg_num), flags);\n\tif (!edesc)\n\t\treturn NULL;\n\n\tstate->edesc = edesc;\n\n\tinit_job_desc_shared(edesc->hw_desc, sh_desc_dma, desc_len(sh_desc),\n\t\t\t     HDR_SHARE_DEFER | HDR_REVERSE);\n\n\treturn edesc;\n}\n\nstatic int ahash_edesc_add_src(struct caam_hash_ctx *ctx,\n\t\t\t       struct ahash_edesc *edesc,\n\t\t\t       struct ahash_request *req, int nents,\n\t\t\t       unsigned int first_sg,\n\t\t\t       unsigned int first_bytes, size_t to_hash)\n{\n\tdma_addr_t src_dma;\n\tu32 options;\n\n\tif (nents > 1 || first_sg) {\n\t\tstruct sec4_sg_entry *sg = edesc->sec4_sg;\n\t\tunsigned int sgsize = sizeof(*sg) *\n\t\t\t\t      pad_sg_nents(first_sg + nents);\n\n\t\tsg_to_sec4_sg_last(req->src, to_hash, sg + first_sg, 0);\n\n\t\tsrc_dma = dma_map_single(ctx->jrdev, sg, sgsize, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ctx->jrdev, src_dma)) {\n\t\t\tdev_err(ctx->jrdev, \"unable to map S/G table\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tedesc->sec4_sg_bytes = sgsize;\n\t\tedesc->sec4_sg_dma = src_dma;\n\t\toptions = LDST_SGF;\n\t} else {\n\t\tsrc_dma = sg_dma_address(req->src);\n\t\toptions = 0;\n\t}\n\n\tappend_seq_in_ptr(edesc->hw_desc, src_dma, first_bytes + to_hash,\n\t\t\t  options);\n\n\treturn 0;\n}\n\nstatic int ahash_do_one_req(struct crypto_engine *engine, void *areq)\n{\n\tstruct ahash_request *req = ahash_request_cast(areq);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(crypto_ahash_reqtfm(req));\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu32 *desc = state->edesc->hw_desc;\n\tint ret;\n\n\tstate->edesc->bklog = true;\n\n\tret = caam_jr_enqueue(jrdev, desc, state->ahash_op_done, req);\n\n\tif (ret == -ENOSPC && engine->retry_support)\n\t\treturn ret;\n\n\tif (ret != -EINPROGRESS) {\n\t\tahash_unmap(jrdev, state->edesc, req, 0);\n\t\tkfree(state->edesc);\n\t} else {\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int ahash_enqueue_req(struct device *jrdev,\n\t\t\t     void (*cbk)(struct device *jrdev, u32 *desc,\n\t\t\t\t\t u32 err, void *context),\n\t\t\t     struct ahash_request *req,\n\t\t\t     int dst_len, enum dma_data_direction dir)\n{\n\tstruct caam_drv_private_jr *jrpriv = dev_get_drvdata(jrdev);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct ahash_edesc *edesc = state->edesc;\n\tu32 *desc = edesc->hw_desc;\n\tint ret;\n\n\tstate->ahash_op_done = cbk;\n\n\t \n\tif (req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)\n\t\tret = crypto_transfer_hash_request_to_engine(jrpriv->engine,\n\t\t\t\t\t\t\t     req);\n\telse\n\t\tret = caam_jr_enqueue(jrdev, desc, cbk, req);\n\n\tif ((ret != -EINPROGRESS) && (ret != -EBUSY)) {\n\t\tahash_unmap_ctx(jrdev, edesc, req, dst_len, dir);\n\t\tkfree(edesc);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int ahash_update_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu8 *buf = state->buf;\n\tint *buflen = &state->buflen;\n\tint *next_buflen = &state->next_buflen;\n\tint blocksize = crypto_ahash_blocksize(ahash);\n\tint in_len = *buflen + req->nbytes, to_hash;\n\tu32 *desc;\n\tint src_nents, mapped_nents, sec4_sg_bytes, sec4_sg_src_index;\n\tstruct ahash_edesc *edesc;\n\tint ret = 0;\n\n\t*next_buflen = in_len & (blocksize - 1);\n\tto_hash = in_len - *next_buflen;\n\n\t \n\tif ((is_xcbc_aes(ctx->adata.algtype) ||\n\t     is_cmac_aes(ctx->adata.algtype)) && to_hash >= blocksize &&\n\t     (*next_buflen == 0)) {\n\t\t*next_buflen = blocksize;\n\t\tto_hash -= blocksize;\n\t}\n\n\tif (to_hash) {\n\t\tint pad_nents;\n\t\tint src_len = req->nbytes - *next_buflen;\n\n\t\tsrc_nents = sg_nents_for_len(req->src, src_len);\n\t\tif (src_nents < 0) {\n\t\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\t\treturn src_nents;\n\t\t}\n\n\t\tif (src_nents) {\n\t\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\tif (!mapped_nents) {\n\t\t\t\tdev_err(jrdev, \"unable to DMA map source\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\tmapped_nents = 0;\n\t\t}\n\n\t\tsec4_sg_src_index = 1 + (*buflen ? 1 : 0);\n\t\tpad_nents = pad_sg_nents(sec4_sg_src_index + mapped_nents);\n\t\tsec4_sg_bytes = pad_nents * sizeof(struct sec4_sg_entry);\n\n\t\t \n\t\tedesc = ahash_edesc_alloc(req, pad_nents, ctx->sh_desc_update,\n\t\t\t\t\t  ctx->sh_desc_update_dma);\n\t\tif (!edesc) {\n\t\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tedesc->src_nents = src_nents;\n\t\tedesc->sec4_sg_bytes = sec4_sg_bytes;\n\n\t\tret = ctx_map_to_sec4_sg(jrdev, state, ctx->ctx_len,\n\t\t\t\t\t edesc->sec4_sg, DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tif (mapped_nents)\n\t\t\tsg_to_sec4_sg_last(req->src, src_len,\n\t\t\t\t\t   edesc->sec4_sg + sec4_sg_src_index,\n\t\t\t\t\t   0);\n\t\telse\n\t\t\tsg_to_sec4_set_last(edesc->sec4_sg + sec4_sg_src_index -\n\t\t\t\t\t    1);\n\n\t\tdesc = edesc->hw_desc;\n\n\t\tedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\n\t\t\t\t\t\t     sec4_sg_bytes,\n\t\t\t\t\t\t     DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\n\t\t\tdev_err(jrdev, \"unable to map S/G table\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unmap_ctx;\n\t\t}\n\n\t\tappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len +\n\t\t\t\t       to_hash, LDST_SGF);\n\n\t\tappend_seq_out_ptr(desc, state->ctx_dma, ctx->ctx_len, 0);\n\n\t\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t\t     desc_bytes(desc), 1);\n\n\t\tret = ahash_enqueue_req(jrdev, ahash_done_bi, req,\n\t\t\t\t\tctx->ctx_len, DMA_BIDIRECTIONAL);\n\t} else if (*next_buflen) {\n\t\tscatterwalk_map_and_copy(buf + *buflen, req->src, 0,\n\t\t\t\t\t req->nbytes, 0);\n\t\t*buflen = *next_buflen;\n\n\t\tprint_hex_dump_debug(\"buf@\" __stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, buf,\n\t\t\t\t     *buflen, 1);\n\t}\n\n\treturn ret;\nunmap_ctx:\n\tahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_BIDIRECTIONAL);\n\tkfree(edesc);\n\treturn ret;\n}\n\nstatic int ahash_final_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tint buflen = state->buflen;\n\tu32 *desc;\n\tint sec4_sg_bytes;\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct ahash_edesc *edesc;\n\tint ret;\n\n\tsec4_sg_bytes = pad_sg_nents(1 + (buflen ? 1 : 0)) *\n\t\t\tsizeof(struct sec4_sg_entry);\n\n\t \n\tedesc = ahash_edesc_alloc(req, 4, ctx->sh_desc_fin,\n\t\t\t\t  ctx->sh_desc_fin_dma);\n\tif (!edesc)\n\t\treturn -ENOMEM;\n\n\tdesc = edesc->hw_desc;\n\n\tedesc->sec4_sg_bytes = sec4_sg_bytes;\n\n\tret = ctx_map_to_sec4_sg(jrdev, state, ctx->ctx_len,\n\t\t\t\t edesc->sec4_sg, DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto unmap_ctx;\n\n\tret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\n\tif (ret)\n\t\tgoto unmap_ctx;\n\n\tsg_to_sec4_set_last(edesc->sec4_sg + (buflen ? 1 : 0));\n\n\tedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\n\t\t\t\t\t    sec4_sg_bytes, DMA_TO_DEVICE);\n\tif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\n\t\tdev_err(jrdev, \"unable to map S/G table\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto unmap_ctx;\n\t}\n\n\tappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len + buflen,\n\t\t\t  LDST_SGF);\n\tappend_seq_out_ptr(desc, state->ctx_dma, digestsize, 0);\n\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\treturn ahash_enqueue_req(jrdev, ahash_done_ctx_src, req,\n\t\t\t\t digestsize, DMA_BIDIRECTIONAL);\n unmap_ctx:\n\tahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_BIDIRECTIONAL);\n\tkfree(edesc);\n\treturn ret;\n}\n\nstatic int ahash_finup_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tint buflen = state->buflen;\n\tu32 *desc;\n\tint sec4_sg_src_index;\n\tint src_nents, mapped_nents;\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct ahash_edesc *edesc;\n\tint ret;\n\n\tsrc_nents = sg_nents_for_len(req->src, req->nbytes);\n\tif (src_nents < 0) {\n\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\treturn src_nents;\n\t}\n\n\tif (src_nents) {\n\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (!mapped_nents) {\n\t\t\tdev_err(jrdev, \"unable to DMA map source\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else {\n\t\tmapped_nents = 0;\n\t}\n\n\tsec4_sg_src_index = 1 + (buflen ? 1 : 0);\n\n\t \n\tedesc = ahash_edesc_alloc(req, sec4_sg_src_index + mapped_nents,\n\t\t\t\t  ctx->sh_desc_fin, ctx->sh_desc_fin_dma);\n\tif (!edesc) {\n\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\treturn -ENOMEM;\n\t}\n\n\tdesc = edesc->hw_desc;\n\n\tedesc->src_nents = src_nents;\n\n\tret = ctx_map_to_sec4_sg(jrdev, state, ctx->ctx_len,\n\t\t\t\t edesc->sec4_sg, DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto unmap_ctx;\n\n\tret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\n\tif (ret)\n\t\tgoto unmap_ctx;\n\n\tret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents,\n\t\t\t\t  sec4_sg_src_index, ctx->ctx_len + buflen,\n\t\t\t\t  req->nbytes);\n\tif (ret)\n\t\tgoto unmap_ctx;\n\n\tappend_seq_out_ptr(desc, state->ctx_dma, digestsize, 0);\n\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\treturn ahash_enqueue_req(jrdev, ahash_done_ctx_src, req,\n\t\t\t\t digestsize, DMA_BIDIRECTIONAL);\n unmap_ctx:\n\tahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_BIDIRECTIONAL);\n\tkfree(edesc);\n\treturn ret;\n}\n\nstatic int ahash_digest(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu32 *desc;\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tint src_nents, mapped_nents;\n\tstruct ahash_edesc *edesc;\n\tint ret;\n\n\tstate->buf_dma = 0;\n\n\tsrc_nents = sg_nents_for_len(req->src, req->nbytes);\n\tif (src_nents < 0) {\n\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\treturn src_nents;\n\t}\n\n\tif (src_nents) {\n\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (!mapped_nents) {\n\t\t\tdev_err(jrdev, \"unable to map source for DMA\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else {\n\t\tmapped_nents = 0;\n\t}\n\n\t \n\tedesc = ahash_edesc_alloc(req, mapped_nents > 1 ? mapped_nents : 0,\n\t\t\t\t  ctx->sh_desc_digest, ctx->sh_desc_digest_dma);\n\tif (!edesc) {\n\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\treturn -ENOMEM;\n\t}\n\n\tedesc->src_nents = src_nents;\n\n\tret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 0, 0,\n\t\t\t\t  req->nbytes);\n\tif (ret) {\n\t\tahash_unmap(jrdev, edesc, req, digestsize);\n\t\tkfree(edesc);\n\t\treturn ret;\n\t}\n\n\tdesc = edesc->hw_desc;\n\n\tret = map_seq_out_ptr_ctx(desc, jrdev, state, digestsize);\n\tif (ret) {\n\t\tahash_unmap(jrdev, edesc, req, digestsize);\n\t\tkfree(edesc);\n\t\treturn -ENOMEM;\n\t}\n\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\treturn ahash_enqueue_req(jrdev, ahash_done, req, digestsize,\n\t\t\t\t DMA_FROM_DEVICE);\n}\n\n \nstatic int ahash_final_no_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu8 *buf = state->buf;\n\tint buflen = state->buflen;\n\tu32 *desc;\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct ahash_edesc *edesc;\n\tint ret;\n\n\t \n\tedesc = ahash_edesc_alloc(req, 0, ctx->sh_desc_digest,\n\t\t\t\t  ctx->sh_desc_digest_dma);\n\tif (!edesc)\n\t\treturn -ENOMEM;\n\n\tdesc = edesc->hw_desc;\n\n\tif (buflen) {\n\t\tstate->buf_dma = dma_map_single(jrdev, buf, buflen,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(jrdev, state->buf_dma)) {\n\t\t\tdev_err(jrdev, \"unable to map src\\n\");\n\t\t\tgoto unmap;\n\t\t}\n\n\t\tappend_seq_in_ptr(desc, state->buf_dma, buflen, 0);\n\t}\n\n\tret = map_seq_out_ptr_ctx(desc, jrdev, state, digestsize);\n\tif (ret)\n\t\tgoto unmap;\n\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\treturn ahash_enqueue_req(jrdev, ahash_done, req,\n\t\t\t\t digestsize, DMA_FROM_DEVICE);\n unmap:\n\tahash_unmap(jrdev, edesc, req, digestsize);\n\tkfree(edesc);\n\treturn -ENOMEM;\n}\n\n \nstatic int ahash_update_no_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu8 *buf = state->buf;\n\tint *buflen = &state->buflen;\n\tint *next_buflen = &state->next_buflen;\n\tint blocksize = crypto_ahash_blocksize(ahash);\n\tint in_len = *buflen + req->nbytes, to_hash;\n\tint sec4_sg_bytes, src_nents, mapped_nents;\n\tstruct ahash_edesc *edesc;\n\tu32 *desc;\n\tint ret = 0;\n\n\t*next_buflen = in_len & (blocksize - 1);\n\tto_hash = in_len - *next_buflen;\n\n\t \n\tif ((is_xcbc_aes(ctx->adata.algtype) ||\n\t     is_cmac_aes(ctx->adata.algtype)) && to_hash >= blocksize &&\n\t     (*next_buflen == 0)) {\n\t\t*next_buflen = blocksize;\n\t\tto_hash -= blocksize;\n\t}\n\n\tif (to_hash) {\n\t\tint pad_nents;\n\t\tint src_len = req->nbytes - *next_buflen;\n\n\t\tsrc_nents = sg_nents_for_len(req->src, src_len);\n\t\tif (src_nents < 0) {\n\t\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\t\treturn src_nents;\n\t\t}\n\n\t\tif (src_nents) {\n\t\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\tif (!mapped_nents) {\n\t\t\t\tdev_err(jrdev, \"unable to DMA map source\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\tmapped_nents = 0;\n\t\t}\n\n\t\tpad_nents = pad_sg_nents(1 + mapped_nents);\n\t\tsec4_sg_bytes = pad_nents * sizeof(struct sec4_sg_entry);\n\n\t\t \n\t\tedesc = ahash_edesc_alloc(req, pad_nents,\n\t\t\t\t\t  ctx->sh_desc_update_first,\n\t\t\t\t\t  ctx->sh_desc_update_first_dma);\n\t\tif (!edesc) {\n\t\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tedesc->src_nents = src_nents;\n\t\tedesc->sec4_sg_bytes = sec4_sg_bytes;\n\n\t\tret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, state);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tsg_to_sec4_sg_last(req->src, src_len, edesc->sec4_sg + 1, 0);\n\n\t\tdesc = edesc->hw_desc;\n\n\t\tedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\n\t\t\t\t\t\t    sec4_sg_bytes,\n\t\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\n\t\t\tdev_err(jrdev, \"unable to map S/G table\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unmap_ctx;\n\t\t}\n\n\t\tappend_seq_in_ptr(desc, edesc->sec4_sg_dma, to_hash, LDST_SGF);\n\n\t\tret = map_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t\t     desc_bytes(desc), 1);\n\n\t\tret = ahash_enqueue_req(jrdev, ahash_done_ctx_dst, req,\n\t\t\t\t\tctx->ctx_len, DMA_TO_DEVICE);\n\t\tif ((ret != -EINPROGRESS) && (ret != -EBUSY))\n\t\t\treturn ret;\n\t\tstate->update = ahash_update_ctx;\n\t\tstate->finup = ahash_finup_ctx;\n\t\tstate->final = ahash_final_ctx;\n\t} else if (*next_buflen) {\n\t\tscatterwalk_map_and_copy(buf + *buflen, req->src, 0,\n\t\t\t\t\t req->nbytes, 0);\n\t\t*buflen = *next_buflen;\n\n\t\tprint_hex_dump_debug(\"buf@\" __stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, buf,\n\t\t\t\t     *buflen, 1);\n\t}\n\n\treturn ret;\n unmap_ctx:\n\tahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_TO_DEVICE);\n\tkfree(edesc);\n\treturn ret;\n}\n\n \nstatic int ahash_finup_no_ctx(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tint buflen = state->buflen;\n\tu32 *desc;\n\tint sec4_sg_bytes, sec4_sg_src_index, src_nents, mapped_nents;\n\tint digestsize = crypto_ahash_digestsize(ahash);\n\tstruct ahash_edesc *edesc;\n\tint ret;\n\n\tsrc_nents = sg_nents_for_len(req->src, req->nbytes);\n\tif (src_nents < 0) {\n\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\treturn src_nents;\n\t}\n\n\tif (src_nents) {\n\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (!mapped_nents) {\n\t\t\tdev_err(jrdev, \"unable to DMA map source\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else {\n\t\tmapped_nents = 0;\n\t}\n\n\tsec4_sg_src_index = 2;\n\tsec4_sg_bytes = (sec4_sg_src_index + mapped_nents) *\n\t\t\t sizeof(struct sec4_sg_entry);\n\n\t \n\tedesc = ahash_edesc_alloc(req, sec4_sg_src_index + mapped_nents,\n\t\t\t\t  ctx->sh_desc_digest, ctx->sh_desc_digest_dma);\n\tif (!edesc) {\n\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\treturn -ENOMEM;\n\t}\n\n\tdesc = edesc->hw_desc;\n\n\tedesc->src_nents = src_nents;\n\tedesc->sec4_sg_bytes = sec4_sg_bytes;\n\n\tret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, state);\n\tif (ret)\n\t\tgoto unmap;\n\n\tret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 1, buflen,\n\t\t\t\t  req->nbytes);\n\tif (ret) {\n\t\tdev_err(jrdev, \"unable to map S/G table\\n\");\n\t\tgoto unmap;\n\t}\n\n\tret = map_seq_out_ptr_ctx(desc, jrdev, state, digestsize);\n\tif (ret)\n\t\tgoto unmap;\n\n\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc),\n\t\t\t     1);\n\n\treturn ahash_enqueue_req(jrdev, ahash_done, req,\n\t\t\t\t digestsize, DMA_FROM_DEVICE);\n unmap:\n\tahash_unmap(jrdev, edesc, req, digestsize);\n\tkfree(edesc);\n\treturn -ENOMEM;\n\n}\n\n \nstatic int ahash_update_first(struct ahash_request *req)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct device *jrdev = ctx->jrdev;\n\tu8 *buf = state->buf;\n\tint *buflen = &state->buflen;\n\tint *next_buflen = &state->next_buflen;\n\tint to_hash;\n\tint blocksize = crypto_ahash_blocksize(ahash);\n\tu32 *desc;\n\tint src_nents, mapped_nents;\n\tstruct ahash_edesc *edesc;\n\tint ret = 0;\n\n\t*next_buflen = req->nbytes & (blocksize - 1);\n\tto_hash = req->nbytes - *next_buflen;\n\n\t \n\tif ((is_xcbc_aes(ctx->adata.algtype) ||\n\t     is_cmac_aes(ctx->adata.algtype)) && to_hash >= blocksize &&\n\t     (*next_buflen == 0)) {\n\t\t*next_buflen = blocksize;\n\t\tto_hash -= blocksize;\n\t}\n\n\tif (to_hash) {\n\t\tsrc_nents = sg_nents_for_len(req->src,\n\t\t\t\t\t     req->nbytes - *next_buflen);\n\t\tif (src_nents < 0) {\n\t\t\tdev_err(jrdev, \"Invalid number of src SG.\\n\");\n\t\t\treturn src_nents;\n\t\t}\n\n\t\tif (src_nents) {\n\t\t\tmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\tif (!mapped_nents) {\n\t\t\t\tdev_err(jrdev, \"unable to map source for DMA\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\tmapped_nents = 0;\n\t\t}\n\n\t\t \n\t\tedesc = ahash_edesc_alloc(req, mapped_nents > 1 ?\n\t\t\t\t\t  mapped_nents : 0,\n\t\t\t\t\t  ctx->sh_desc_update_first,\n\t\t\t\t\t  ctx->sh_desc_update_first_dma);\n\t\tif (!edesc) {\n\t\t\tdma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tedesc->src_nents = src_nents;\n\n\t\tret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 0, 0,\n\t\t\t\t\t  to_hash);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tdesc = edesc->hw_desc;\n\n\t\tret = map_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\n\t\tif (ret)\n\t\t\tgoto unmap_ctx;\n\n\t\tprint_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, desc,\n\t\t\t\t     desc_bytes(desc), 1);\n\n\t\tret = ahash_enqueue_req(jrdev, ahash_done_ctx_dst, req,\n\t\t\t\t\tctx->ctx_len, DMA_TO_DEVICE);\n\t\tif ((ret != -EINPROGRESS) && (ret != -EBUSY))\n\t\t\treturn ret;\n\t\tstate->update = ahash_update_ctx;\n\t\tstate->finup = ahash_finup_ctx;\n\t\tstate->final = ahash_final_ctx;\n\t} else if (*next_buflen) {\n\t\tstate->update = ahash_update_no_ctx;\n\t\tstate->finup = ahash_finup_no_ctx;\n\t\tstate->final = ahash_final_no_ctx;\n\t\tscatterwalk_map_and_copy(buf, req->src, 0,\n\t\t\t\t\t req->nbytes, 0);\n\t\t*buflen = *next_buflen;\n\n\t\tprint_hex_dump_debug(\"buf@\" __stringify(__LINE__)\": \",\n\t\t\t\t     DUMP_PREFIX_ADDRESS, 16, 4, buf,\n\t\t\t\t     *buflen, 1);\n\t}\n\n\treturn ret;\n unmap_ctx:\n\tahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_TO_DEVICE);\n\tkfree(edesc);\n\treturn ret;\n}\n\nstatic int ahash_finup_first(struct ahash_request *req)\n{\n\treturn ahash_digest(req);\n}\n\nstatic int ahash_init(struct ahash_request *req)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\tstate->update = ahash_update_first;\n\tstate->finup = ahash_finup_first;\n\tstate->final = ahash_final_no_ctx;\n\n\tstate->ctx_dma = 0;\n\tstate->ctx_dma_len = 0;\n\tstate->buf_dma = 0;\n\tstate->buflen = 0;\n\tstate->next_buflen = 0;\n\n\treturn 0;\n}\n\nstatic int ahash_update(struct ahash_request *req)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\treturn state->update(req);\n}\n\nstatic int ahash_finup(struct ahash_request *req)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\treturn state->finup(req);\n}\n\nstatic int ahash_final(struct ahash_request *req)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\n\treturn state->final(req);\n}\n\nstatic int ahash_export(struct ahash_request *req, void *out)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tstruct caam_export_state *export = out;\n\tu8 *buf = state->buf;\n\tint len = state->buflen;\n\n\tmemcpy(export->buf, buf, len);\n\tmemcpy(export->caam_ctx, state->caam_ctx, sizeof(export->caam_ctx));\n\texport->buflen = len;\n\texport->update = state->update;\n\texport->final = state->final;\n\texport->finup = state->finup;\n\n\treturn 0;\n}\n\nstatic int ahash_import(struct ahash_request *req, const void *in)\n{\n\tstruct caam_hash_state *state = ahash_request_ctx_dma(req);\n\tconst struct caam_export_state *export = in;\n\n\tmemset(state, 0, sizeof(*state));\n\tmemcpy(state->buf, export->buf, export->buflen);\n\tmemcpy(state->caam_ctx, export->caam_ctx, sizeof(state->caam_ctx));\n\tstate->buflen = export->buflen;\n\tstate->update = export->update;\n\tstate->final = export->final;\n\tstate->finup = export->finup;\n\n\treturn 0;\n}\n\nstruct caam_hash_template {\n\tchar name[CRYPTO_MAX_ALG_NAME];\n\tchar driver_name[CRYPTO_MAX_ALG_NAME];\n\tchar hmac_name[CRYPTO_MAX_ALG_NAME];\n\tchar hmac_driver_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int blocksize;\n\tstruct ahash_alg template_ahash;\n\tu32 alg_type;\n};\n\n \nstatic struct caam_hash_template driver_hash[] = {\n\t{\n\t\t.name = \"sha1\",\n\t\t.driver_name = \"sha1-caam\",\n\t\t.hmac_name = \"hmac(sha1)\",\n\t\t.hmac_driver_name = \"hmac-sha1-caam\",\n\t\t.blocksize = SHA1_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_SHA1,\n\t}, {\n\t\t.name = \"sha224\",\n\t\t.driver_name = \"sha224-caam\",\n\t\t.hmac_name = \"hmac(sha224)\",\n\t\t.hmac_driver_name = \"hmac-sha224-caam\",\n\t\t.blocksize = SHA224_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_SHA224,\n\t}, {\n\t\t.name = \"sha256\",\n\t\t.driver_name = \"sha256-caam\",\n\t\t.hmac_name = \"hmac(sha256)\",\n\t\t.hmac_driver_name = \"hmac-sha256-caam\",\n\t\t.blocksize = SHA256_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_SHA256,\n\t}, {\n\t\t.name = \"sha384\",\n\t\t.driver_name = \"sha384-caam\",\n\t\t.hmac_name = \"hmac(sha384)\",\n\t\t.hmac_driver_name = \"hmac-sha384-caam\",\n\t\t.blocksize = SHA384_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_SHA384,\n\t}, {\n\t\t.name = \"sha512\",\n\t\t.driver_name = \"sha512-caam\",\n\t\t.hmac_name = \"hmac(sha512)\",\n\t\t.hmac_driver_name = \"hmac-sha512-caam\",\n\t\t.blocksize = SHA512_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_SHA512,\n\t}, {\n\t\t.name = \"md5\",\n\t\t.driver_name = \"md5-caam\",\n\t\t.hmac_name = \"hmac(md5)\",\n\t\t.hmac_driver_name = \"hmac-md5-caam\",\n\t\t.blocksize = MD5_BLOCK_WORDS * 4,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = ahash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = MD5_DIGEST_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t},\n\t\t.alg_type = OP_ALG_ALGSEL_MD5,\n\t}, {\n\t\t.hmac_name = \"xcbc(aes)\",\n\t\t.hmac_driver_name = \"xcbc-aes-caam\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = axcbc_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t },\n\t\t.alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_XCBC_MAC,\n\t}, {\n\t\t.hmac_name = \"cmac(aes)\",\n\t\t.hmac_driver_name = \"cmac-aes-caam\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_ahash = {\n\t\t\t.init = ahash_init,\n\t\t\t.update = ahash_update,\n\t\t\t.final = ahash_final,\n\t\t\t.finup = ahash_finup,\n\t\t\t.digest = ahash_digest,\n\t\t\t.export = ahash_export,\n\t\t\t.import = ahash_import,\n\t\t\t.setkey = acmac_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t\t.statesize = sizeof(struct caam_export_state),\n\t\t\t},\n\t\t },\n\t\t.alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CMAC,\n\t},\n};\n\nstruct caam_hash_alg {\n\tstruct list_head entry;\n\tint alg_type;\n\tstruct ahash_engine_alg ahash_alg;\n};\n\nstatic int caam_hash_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct crypto_alg *base = tfm->__crt_alg;\n\tstruct hash_alg_common *halg =\n\t\t container_of(base, struct hash_alg_common, base);\n\tstruct ahash_alg *alg =\n\t\t container_of(halg, struct ahash_alg, halg);\n\tstruct caam_hash_alg *caam_hash =\n\t\t container_of(alg, struct caam_hash_alg, ahash_alg.base);\n\tstruct caam_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\t \n\tstatic const u8 runninglen[] = { HASH_MSG_LEN + MD5_DIGEST_SIZE,\n\t\t\t\t\t HASH_MSG_LEN + SHA1_DIGEST_SIZE,\n\t\t\t\t\t HASH_MSG_LEN + 32,\n\t\t\t\t\t HASH_MSG_LEN + SHA256_DIGEST_SIZE,\n\t\t\t\t\t HASH_MSG_LEN + 64,\n\t\t\t\t\t HASH_MSG_LEN + SHA512_DIGEST_SIZE };\n\tconst size_t sh_desc_update_offset = offsetof(struct caam_hash_ctx,\n\t\t\t\t\t\t      sh_desc_update);\n\tdma_addr_t dma_addr;\n\tstruct caam_drv_private *priv;\n\n\t \n\tctx->jrdev = caam_jr_alloc();\n\tif (IS_ERR(ctx->jrdev)) {\n\t\tpr_err(\"Job Ring Device allocation for transform failed\\n\");\n\t\treturn PTR_ERR(ctx->jrdev);\n\t}\n\n\tpriv = dev_get_drvdata(ctx->jrdev->parent);\n\n\tif (is_xcbc_aes(caam_hash->alg_type)) {\n\t\tctx->dir = DMA_TO_DEVICE;\n\t\tctx->key_dir = DMA_BIDIRECTIONAL;\n\t\tctx->adata.algtype = OP_TYPE_CLASS1_ALG | caam_hash->alg_type;\n\t\tctx->ctx_len = 48;\n\t} else if (is_cmac_aes(caam_hash->alg_type)) {\n\t\tctx->dir = DMA_TO_DEVICE;\n\t\tctx->key_dir = DMA_NONE;\n\t\tctx->adata.algtype = OP_TYPE_CLASS1_ALG | caam_hash->alg_type;\n\t\tctx->ctx_len = 32;\n\t} else {\n\t\tif (priv->era >= 6) {\n\t\t\tctx->dir = DMA_BIDIRECTIONAL;\n\t\t\tctx->key_dir = alg->setkey ? DMA_TO_DEVICE : DMA_NONE;\n\t\t} else {\n\t\t\tctx->dir = DMA_TO_DEVICE;\n\t\t\tctx->key_dir = DMA_NONE;\n\t\t}\n\t\tctx->adata.algtype = OP_TYPE_CLASS2_ALG | caam_hash->alg_type;\n\t\tctx->ctx_len = runninglen[(ctx->adata.algtype &\n\t\t\t\t\t   OP_ALG_ALGSEL_SUBMASK) >>\n\t\t\t\t\t  OP_ALG_ALGSEL_SHIFT];\n\t}\n\n\tif (ctx->key_dir != DMA_NONE) {\n\t\tctx->adata.key_dma = dma_map_single_attrs(ctx->jrdev, ctx->key,\n\t\t\t\t\t\t\t  ARRAY_SIZE(ctx->key),\n\t\t\t\t\t\t\t  ctx->key_dir,\n\t\t\t\t\t\t\t  DMA_ATTR_SKIP_CPU_SYNC);\n\t\tif (dma_mapping_error(ctx->jrdev, ctx->adata.key_dma)) {\n\t\t\tdev_err(ctx->jrdev, \"unable to map key\\n\");\n\t\t\tcaam_jr_free(ctx->jrdev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tdma_addr = dma_map_single_attrs(ctx->jrdev, ctx->sh_desc_update,\n\t\t\t\t\toffsetof(struct caam_hash_ctx, key) -\n\t\t\t\t\tsh_desc_update_offset,\n\t\t\t\t\tctx->dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tif (dma_mapping_error(ctx->jrdev, dma_addr)) {\n\t\tdev_err(ctx->jrdev, \"unable to map shared descriptors\\n\");\n\n\t\tif (ctx->key_dir != DMA_NONE)\n\t\t\tdma_unmap_single_attrs(ctx->jrdev, ctx->adata.key_dma,\n\t\t\t\t\t       ARRAY_SIZE(ctx->key),\n\t\t\t\t\t       ctx->key_dir,\n\t\t\t\t\t       DMA_ATTR_SKIP_CPU_SYNC);\n\n\t\tcaam_jr_free(ctx->jrdev);\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->sh_desc_update_dma = dma_addr;\n\tctx->sh_desc_update_first_dma = dma_addr +\n\t\t\t\t\toffsetof(struct caam_hash_ctx,\n\t\t\t\t\t\t sh_desc_update_first) -\n\t\t\t\t\tsh_desc_update_offset;\n\tctx->sh_desc_fin_dma = dma_addr + offsetof(struct caam_hash_ctx,\n\t\t\t\t\t\t   sh_desc_fin) -\n\t\t\t\t\tsh_desc_update_offset;\n\tctx->sh_desc_digest_dma = dma_addr + offsetof(struct caam_hash_ctx,\n\t\t\t\t\t\t      sh_desc_digest) -\n\t\t\t\t\tsh_desc_update_offset;\n\n\tcrypto_ahash_set_reqsize_dma(ahash, sizeof(struct caam_hash_state));\n\n\t \n\treturn alg->setkey ? 0 : ahash_set_sh_desc(ahash);\n}\n\nstatic void caam_hash_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct caam_hash_ctx *ctx = crypto_tfm_ctx_dma(tfm);\n\n\tdma_unmap_single_attrs(ctx->jrdev, ctx->sh_desc_update_dma,\n\t\t\t       offsetof(struct caam_hash_ctx, key) -\n\t\t\t       offsetof(struct caam_hash_ctx, sh_desc_update),\n\t\t\t       ctx->dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tif (ctx->key_dir != DMA_NONE)\n\t\tdma_unmap_single_attrs(ctx->jrdev, ctx->adata.key_dma,\n\t\t\t\t       ARRAY_SIZE(ctx->key), ctx->key_dir,\n\t\t\t\t       DMA_ATTR_SKIP_CPU_SYNC);\n\tcaam_jr_free(ctx->jrdev);\n}\n\nvoid caam_algapi_hash_exit(void)\n{\n\tstruct caam_hash_alg *t_alg, *n;\n\n\tif (!hash_list.next)\n\t\treturn;\n\n\tlist_for_each_entry_safe(t_alg, n, &hash_list, entry) {\n\t\tcrypto_engine_unregister_ahash(&t_alg->ahash_alg);\n\t\tlist_del(&t_alg->entry);\n\t\tkfree(t_alg);\n\t}\n}\n\nstatic struct caam_hash_alg *\ncaam_hash_alloc(struct caam_hash_template *template,\n\t\tbool keyed)\n{\n\tstruct caam_hash_alg *t_alg;\n\tstruct ahash_alg *halg;\n\tstruct crypto_alg *alg;\n\n\tt_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);\n\tif (!t_alg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tt_alg->ahash_alg.base = template->template_ahash;\n\thalg = &t_alg->ahash_alg.base;\n\talg = &halg->halg.base;\n\n\tif (keyed) {\n\t\tsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->hmac_name);\n\t\tsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->hmac_driver_name);\n\t} else {\n\t\tsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->name);\n\t\tsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->driver_name);\n\t\thalg->setkey = NULL;\n\t}\n\talg->cra_module = THIS_MODULE;\n\talg->cra_init = caam_hash_cra_init;\n\talg->cra_exit = caam_hash_cra_exit;\n\talg->cra_ctxsize = sizeof(struct caam_hash_ctx) + crypto_dma_padding();\n\talg->cra_priority = CAAM_CRA_PRIORITY;\n\talg->cra_blocksize = template->blocksize;\n\talg->cra_alignmask = 0;\n\talg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY;\n\n\tt_alg->alg_type = template->alg_type;\n\tt_alg->ahash_alg.op.do_one_request = ahash_do_one_req;\n\n\treturn t_alg;\n}\n\nint caam_algapi_hash_init(struct device *ctrldev)\n{\n\tint i = 0, err = 0;\n\tstruct caam_drv_private *priv = dev_get_drvdata(ctrldev);\n\tunsigned int md_limit = SHA512_DIGEST_SIZE;\n\tu32 md_inst, md_vid;\n\n\t \n\tif (priv->era < 10) {\n\t\tstruct caam_perfmon __iomem *perfmon = &priv->jr[0]->perfmon;\n\n\t\tmd_vid = (rd_reg32(&perfmon->cha_id_ls) &\n\t\t\t  CHA_ID_LS_MD_MASK) >> CHA_ID_LS_MD_SHIFT;\n\t\tmd_inst = (rd_reg32(&perfmon->cha_num_ls) &\n\t\t\t   CHA_ID_LS_MD_MASK) >> CHA_ID_LS_MD_SHIFT;\n\t} else {\n\t\tu32 mdha = rd_reg32(&priv->jr[0]->vreg.mdha);\n\n\t\tmd_vid = (mdha & CHA_VER_VID_MASK) >> CHA_VER_VID_SHIFT;\n\t\tmd_inst = mdha & CHA_VER_NUM_MASK;\n\t}\n\n\t \n\tif (!md_inst)\n\t\treturn 0;\n\n\t \n\tif (md_vid == CHA_VER_VID_MD_LP256)\n\t\tmd_limit = SHA256_DIGEST_SIZE;\n\n\tINIT_LIST_HEAD(&hash_list);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(driver_hash); i++) {\n\t\tstruct caam_hash_alg *t_alg;\n\t\tstruct caam_hash_template *alg = driver_hash + i;\n\n\t\t \n\t\tif (is_mdha(alg->alg_type) &&\n\t\t    alg->template_ahash.halg.digestsize > md_limit)\n\t\t\tcontinue;\n\n\t\t \n\t\tt_alg = caam_hash_alloc(alg, true);\n\t\tif (IS_ERR(t_alg)) {\n\t\t\terr = PTR_ERR(t_alg);\n\t\t\tpr_warn(\"%s alg allocation failed\\n\",\n\t\t\t\talg->hmac_driver_name);\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = crypto_engine_register_ahash(&t_alg->ahash_alg);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s alg registration failed: %d\\n\",\n\t\t\t\tt_alg->ahash_alg.base.halg.base.cra_driver_name,\n\t\t\t\terr);\n\t\t\tkfree(t_alg);\n\t\t} else\n\t\t\tlist_add_tail(&t_alg->entry, &hash_list);\n\n\t\tif ((alg->alg_type & OP_ALG_ALGSEL_MASK) == OP_ALG_ALGSEL_AES)\n\t\t\tcontinue;\n\n\t\t \n\t\tt_alg = caam_hash_alloc(alg, false);\n\t\tif (IS_ERR(t_alg)) {\n\t\t\terr = PTR_ERR(t_alg);\n\t\t\tpr_warn(\"%s alg allocation failed\\n\", alg->driver_name);\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = crypto_engine_register_ahash(&t_alg->ahash_alg);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s alg registration failed: %d\\n\",\n\t\t\t\tt_alg->ahash_alg.base.halg.base.cra_driver_name,\n\t\t\t\terr);\n\t\t\tkfree(t_alg);\n\t\t} else\n\t\t\tlist_add_tail(&t_alg->entry, &hash_list);\n\t}\n\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}