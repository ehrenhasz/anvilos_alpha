{
  "module_name": "qi.c",
  "hash_id": "5e4354d829ad3b19a0a55c8ffdc6a89a8a5d9b68da650e17a9586a03b9e270db",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/caam/qi.c",
  "human_readable_source": "\n \n\n#include <linux/cpumask.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/netdevice.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <soc/fsl/qman.h>\n\n#include \"debugfs.h\"\n#include \"regs.h\"\n#include \"qi.h\"\n#include \"desc.h\"\n#include \"intern.h\"\n#include \"desc_constr.h\"\n\n#define PREHDR_RSLS_SHIFT\t31\n#define PREHDR_ABS\t\tBIT(25)\n\n \n#define MAX_RSP_FQ_BACKLOG_PER_CPU\t256\n\n#define CAAM_QI_ENQUEUE_RETRIES\t10000\n\n#define CAAM_NAPI_WEIGHT\t63\n\n \nstruct caam_napi {\n\tstruct napi_struct irqtask;\n\tstruct qman_portal *p;\n};\n\n \nstruct caam_qi_pcpu_priv {\n\tstruct caam_napi caam_napi;\n\tstruct net_device net_dev;\n\tstruct qman_fq *rsp_fq;\n} ____cacheline_aligned;\n\nstatic DEFINE_PER_CPU(struct caam_qi_pcpu_priv, pcpu_qipriv);\nstatic DEFINE_PER_CPU(int, last_cpu);\n\n \nstruct caam_qi_priv {\n\tstruct qman_cgr cgr;\n};\n\nstatic struct caam_qi_priv qipriv ____cacheline_aligned;\n\n \nbool caam_congested __read_mostly;\nEXPORT_SYMBOL(caam_congested);\n\n \nstatic struct kmem_cache *qi_cache;\n\nstatic void *caam_iova_to_virt(struct iommu_domain *domain,\n\t\t\t       dma_addr_t iova_addr)\n{\n\tphys_addr_t phys_addr;\n\n\tphys_addr = domain ? iommu_iova_to_phys(domain, iova_addr) : iova_addr;\n\n\treturn phys_to_virt(phys_addr);\n}\n\nint caam_qi_enqueue(struct device *qidev, struct caam_drv_req *req)\n{\n\tstruct qm_fd fd;\n\tdma_addr_t addr;\n\tint ret;\n\tint num_retries = 0;\n\n\tqm_fd_clear_fd(&fd);\n\tqm_fd_set_compound(&fd, qm_sg_entry_get_len(&req->fd_sgt[1]));\n\n\taddr = dma_map_single(qidev, req->fd_sgt, sizeof(req->fd_sgt),\n\t\t\t      DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(qidev, addr)) {\n\t\tdev_err(qidev, \"DMA mapping error for QI enqueue request\\n\");\n\t\treturn -EIO;\n\t}\n\tqm_fd_addr_set64(&fd, addr);\n\n\tdo {\n\t\tret = qman_enqueue(req->drv_ctx->req_fq, &fd);\n\t\tif (likely(!ret)) {\n\t\t\trefcount_inc(&req->drv_ctx->refcnt);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (ret != -EBUSY)\n\t\t\tbreak;\n\t\tnum_retries++;\n\t} while (num_retries < CAAM_QI_ENQUEUE_RETRIES);\n\n\tdev_err(qidev, \"qman_enqueue failed: %d\\n\", ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(caam_qi_enqueue);\n\nstatic void caam_fq_ern_cb(struct qman_portal *qm, struct qman_fq *fq,\n\t\t\t   const union qm_mr_entry *msg)\n{\n\tconst struct qm_fd *fd;\n\tstruct caam_drv_req *drv_req;\n\tstruct device *qidev = &(raw_cpu_ptr(&pcpu_qipriv)->net_dev.dev);\n\tstruct caam_drv_private *priv = dev_get_drvdata(qidev);\n\n\tfd = &msg->ern.fd;\n\n\tdrv_req = caam_iova_to_virt(priv->domain, qm_fd_addr_get64(fd));\n\tif (!drv_req) {\n\t\tdev_err(qidev,\n\t\t\t\"Can't find original request for CAAM response\\n\");\n\t\treturn;\n\t}\n\n\trefcount_dec(&drv_req->drv_ctx->refcnt);\n\n\tif (qm_fd_get_format(fd) != qm_fd_compound) {\n\t\tdev_err(qidev, \"Non-compound FD from CAAM\\n\");\n\t\treturn;\n\t}\n\n\tdma_unmap_single(drv_req->drv_ctx->qidev, qm_fd_addr(fd),\n\t\t\t sizeof(drv_req->fd_sgt), DMA_BIDIRECTIONAL);\n\n\tif (fd->status)\n\t\tdrv_req->cbk(drv_req, be32_to_cpu(fd->status));\n\telse\n\t\tdrv_req->cbk(drv_req, JRSTA_SSRC_QI);\n}\n\nstatic struct qman_fq *create_caam_req_fq(struct device *qidev,\n\t\t\t\t\t  struct qman_fq *rsp_fq,\n\t\t\t\t\t  dma_addr_t hwdesc,\n\t\t\t\t\t  int fq_sched_flag)\n{\n\tint ret;\n\tstruct qman_fq *req_fq;\n\tstruct qm_mcc_initfq opts;\n\n\treq_fq = kzalloc(sizeof(*req_fq), GFP_ATOMIC);\n\tif (!req_fq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treq_fq->cb.ern = caam_fq_ern_cb;\n\treq_fq->cb.fqs = NULL;\n\n\tret = qman_create_fq(0, QMAN_FQ_FLAG_DYNAMIC_FQID |\n\t\t\t\tQMAN_FQ_FLAG_TO_DCPORTAL, req_fq);\n\tif (ret) {\n\t\tdev_err(qidev, \"Failed to create session req FQ\\n\");\n\t\tgoto create_req_fq_fail;\n\t}\n\n\tmemset(&opts, 0, sizeof(opts));\n\topts.we_mask = cpu_to_be16(QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_DESTWQ |\n\t\t\t\t   QM_INITFQ_WE_CONTEXTB |\n\t\t\t\t   QM_INITFQ_WE_CONTEXTA | QM_INITFQ_WE_CGID);\n\topts.fqd.fq_ctrl = cpu_to_be16(QM_FQCTRL_CPCSTASH | QM_FQCTRL_CGE);\n\tqm_fqd_set_destwq(&opts.fqd, qm_channel_caam, 2);\n\topts.fqd.context_b = cpu_to_be32(qman_fq_fqid(rsp_fq));\n\tqm_fqd_context_a_set64(&opts.fqd, hwdesc);\n\topts.fqd.cgid = qipriv.cgr.cgrid;\n\n\tret = qman_init_fq(req_fq, fq_sched_flag, &opts);\n\tif (ret) {\n\t\tdev_err(qidev, \"Failed to init session req FQ\\n\");\n\t\tgoto init_req_fq_fail;\n\t}\n\n\tdev_dbg(qidev, \"Allocated request FQ %u for CPU %u\\n\", req_fq->fqid,\n\t\tsmp_processor_id());\n\treturn req_fq;\n\ninit_req_fq_fail:\n\tqman_destroy_fq(req_fq);\ncreate_req_fq_fail:\n\tkfree(req_fq);\n\treturn ERR_PTR(ret);\n}\n\nstatic int empty_retired_fq(struct device *qidev, struct qman_fq *fq)\n{\n\tint ret;\n\n\tret = qman_volatile_dequeue(fq, QMAN_VOLATILE_FLAG_WAIT_INT |\n\t\t\t\t    QMAN_VOLATILE_FLAG_FINISH,\n\t\t\t\t    QM_VDQCR_PRECEDENCE_VDQCR |\n\t\t\t\t    QM_VDQCR_NUMFRAMES_TILLEMPTY);\n\tif (ret) {\n\t\tdev_err(qidev, \"Volatile dequeue fail for FQ: %u\\n\", fq->fqid);\n\t\treturn ret;\n\t}\n\n\tdo {\n\t\tstruct qman_portal *p;\n\n\t\tp = qman_get_affine_portal(smp_processor_id());\n\t\tqman_p_poll_dqrr(p, 16);\n\t} while (fq->flags & QMAN_FQ_STATE_NE);\n\n\treturn 0;\n}\n\nstatic int kill_fq(struct device *qidev, struct qman_fq *fq)\n{\n\tu32 flags;\n\tint ret;\n\n\tret = qman_retire_fq(fq, &flags);\n\tif (ret < 0) {\n\t\tdev_err(qidev, \"qman_retire_fq failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (!ret)\n\t\tgoto empty_fq;\n\n\t \n\tif (ret == 1) {\n\t\t \n\t\tdo {\n\t\t\tmsleep(20);\n\t\t} while (fq->state != qman_fq_state_retired);\n\n\t\tWARN_ON(fq->flags & QMAN_FQ_STATE_BLOCKOOS);\n\t\tWARN_ON(fq->flags & QMAN_FQ_STATE_ORL);\n\t}\n\nempty_fq:\n\tif (fq->flags & QMAN_FQ_STATE_NE) {\n\t\tret = empty_retired_fq(qidev, fq);\n\t\tif (ret) {\n\t\t\tdev_err(qidev, \"empty_retired_fq fail for FQ: %u\\n\",\n\t\t\t\tfq->fqid);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = qman_oos_fq(fq);\n\tif (ret)\n\t\tdev_err(qidev, \"OOS of FQID: %u failed\\n\", fq->fqid);\n\n\tqman_destroy_fq(fq);\n\tkfree(fq);\n\n\treturn ret;\n}\n\nstatic int empty_caam_fq(struct qman_fq *fq, struct caam_drv_ctx *drv_ctx)\n{\n\tint ret;\n\tint retries = 10;\n\tstruct qm_mcr_queryfq_np np;\n\n\t \n\tdo {\n\t\tret = qman_query_fq_np(fq, &np);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!qm_mcr_np_get(&np, frm_cnt))\n\t\t\tbreak;\n\n\t\tmsleep(20);\n\t} while (1);\n\n\t \n\tdo {\n\t\tif (refcount_read(&drv_ctx->refcnt) == 1)\n\t\t\tbreak;\n\n\t\tmsleep(20);\n\t} while (--retries);\n\n\tif (!retries)\n\t\tdev_warn_once(drv_ctx->qidev, \"%d frames from FQID %u still pending in CAAM\\n\",\n\t\t\t      refcount_read(&drv_ctx->refcnt), fq->fqid);\n\n\treturn 0;\n}\n\nint caam_drv_ctx_update(struct caam_drv_ctx *drv_ctx, u32 *sh_desc)\n{\n\tint ret;\n\tu32 num_words;\n\tstruct qman_fq *new_fq, *old_fq;\n\tstruct device *qidev = drv_ctx->qidev;\n\n\tnum_words = desc_len(sh_desc);\n\tif (num_words > MAX_SDLEN) {\n\t\tdev_err(qidev, \"Invalid descriptor len: %d words\\n\", num_words);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\told_fq = drv_ctx->req_fq;\n\n\t \n\tnew_fq = create_caam_req_fq(drv_ctx->qidev, drv_ctx->rsp_fq,\n\t\t\t\t    drv_ctx->context_a, 0);\n\tif (IS_ERR(new_fq)) {\n\t\tdev_err(qidev, \"FQ allocation for shdesc update failed\\n\");\n\t\treturn PTR_ERR(new_fq);\n\t}\n\n\t \n\tdrv_ctx->req_fq = new_fq;\n\n\t \n\tret = empty_caam_fq(old_fq, drv_ctx);\n\tif (ret) {\n\t\tdev_err(qidev, \"Old CAAM FQ empty failed: %d\\n\", ret);\n\n\t\t \n\t\tdrv_ctx->req_fq = old_fq;\n\n\t\tif (kill_fq(qidev, new_fq))\n\t\t\tdev_warn(qidev, \"New CAAM FQ kill failed\\n\");\n\n\t\treturn ret;\n\t}\n\n\t \n\tdrv_ctx->prehdr[0] = cpu_to_caam32((1 << PREHDR_RSLS_SHIFT) |\n\t\t\t\t\t   num_words);\n\tdrv_ctx->prehdr[1] = cpu_to_caam32(PREHDR_ABS);\n\tmemcpy(drv_ctx->sh_desc, sh_desc, desc_bytes(sh_desc));\n\tdma_sync_single_for_device(qidev, drv_ctx->context_a,\n\t\t\t\t   sizeof(drv_ctx->sh_desc) +\n\t\t\t\t   sizeof(drv_ctx->prehdr),\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\n\t \n\tret = qman_schedule_fq(new_fq);\n\tif (ret) {\n\t\tdev_err(qidev, \"Fail to sched new CAAM FQ, ecode = %d\\n\", ret);\n\n\t\t \n\n\t\tdrv_ctx->req_fq = old_fq;\n\n\t\tif (kill_fq(qidev, new_fq))\n\t\t\tdev_warn(qidev, \"New CAAM FQ kill failed\\n\");\n\t} else if (kill_fq(qidev, old_fq)) {\n\t\tdev_warn(qidev, \"Old CAAM FQ kill failed\\n\");\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(caam_drv_ctx_update);\n\nstruct caam_drv_ctx *caam_drv_ctx_init(struct device *qidev,\n\t\t\t\t       int *cpu,\n\t\t\t\t       u32 *sh_desc)\n{\n\tsize_t size;\n\tu32 num_words;\n\tdma_addr_t hwdesc;\n\tstruct caam_drv_ctx *drv_ctx;\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\n\tnum_words = desc_len(sh_desc);\n\tif (num_words > MAX_SDLEN) {\n\t\tdev_err(qidev, \"Invalid descriptor len: %d words\\n\",\n\t\t\tnum_words);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tdrv_ctx = kzalloc(sizeof(*drv_ctx), GFP_ATOMIC);\n\tif (!drv_ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tdrv_ctx->prehdr[0] = cpu_to_caam32((1 << PREHDR_RSLS_SHIFT) |\n\t\t\t\t\t   num_words);\n\tdrv_ctx->prehdr[1] = cpu_to_caam32(PREHDR_ABS);\n\tmemcpy(drv_ctx->sh_desc, sh_desc, desc_bytes(sh_desc));\n\tsize = sizeof(drv_ctx->prehdr) + sizeof(drv_ctx->sh_desc);\n\thwdesc = dma_map_single(qidev, drv_ctx->prehdr, size,\n\t\t\t\tDMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(qidev, hwdesc)) {\n\t\tdev_err(qidev, \"DMA map error for preheader + shdesc\\n\");\n\t\tkfree(drv_ctx);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tdrv_ctx->context_a = hwdesc;\n\n\t \n\tif (!cpumask_test_cpu(*cpu, cpus)) {\n\t\tint *pcpu = &get_cpu_var(last_cpu);\n\n\t\t*pcpu = cpumask_next(*pcpu, cpus);\n\t\tif (*pcpu >= nr_cpu_ids)\n\t\t\t*pcpu = cpumask_first(cpus);\n\t\t*cpu = *pcpu;\n\n\t\tput_cpu_var(last_cpu);\n\t}\n\tdrv_ctx->cpu = *cpu;\n\n\t \n\tdrv_ctx->rsp_fq = per_cpu(pcpu_qipriv.rsp_fq, drv_ctx->cpu);\n\n\t \n\tdrv_ctx->req_fq = create_caam_req_fq(qidev, drv_ctx->rsp_fq, hwdesc,\n\t\t\t\t\t     QMAN_INITFQ_FLAG_SCHED);\n\tif (IS_ERR(drv_ctx->req_fq)) {\n\t\tdev_err(qidev, \"create_caam_req_fq failed\\n\");\n\t\tdma_unmap_single(qidev, hwdesc, size, DMA_BIDIRECTIONAL);\n\t\tkfree(drv_ctx);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\trefcount_set(&drv_ctx->refcnt, 1);\n\n\tdrv_ctx->qidev = qidev;\n\treturn drv_ctx;\n}\nEXPORT_SYMBOL(caam_drv_ctx_init);\n\nvoid *qi_cache_alloc(gfp_t flags)\n{\n\treturn kmem_cache_alloc(qi_cache, flags);\n}\nEXPORT_SYMBOL(qi_cache_alloc);\n\nvoid qi_cache_free(void *obj)\n{\n\tkmem_cache_free(qi_cache, obj);\n}\nEXPORT_SYMBOL(qi_cache_free);\n\nstatic int caam_qi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct caam_napi *np = container_of(napi, struct caam_napi, irqtask);\n\n\tint cleaned = qman_p_poll_dqrr(np->p, budget);\n\n\tif (cleaned < budget) {\n\t\tnapi_complete(napi);\n\t\tqman_p_irqsource_add(np->p, QM_PIRQ_DQRI);\n\t}\n\n\treturn cleaned;\n}\n\nvoid caam_drv_ctx_rel(struct caam_drv_ctx *drv_ctx)\n{\n\tif (IS_ERR_OR_NULL(drv_ctx))\n\t\treturn;\n\n\t \n\tif (kill_fq(drv_ctx->qidev, drv_ctx->req_fq))\n\t\tdev_err(drv_ctx->qidev, \"Crypto session req FQ kill failed\\n\");\n\n\tdma_unmap_single(drv_ctx->qidev, drv_ctx->context_a,\n\t\t\t sizeof(drv_ctx->sh_desc) + sizeof(drv_ctx->prehdr),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(drv_ctx);\n}\nEXPORT_SYMBOL(caam_drv_ctx_rel);\n\nstatic void caam_qi_shutdown(void *data)\n{\n\tint i;\n\tstruct device *qidev = data;\n\tstruct caam_qi_priv *priv = &qipriv;\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\n\tfor_each_cpu(i, cpus) {\n\t\tstruct napi_struct *irqtask;\n\n\t\tirqtask = &per_cpu_ptr(&pcpu_qipriv.caam_napi, i)->irqtask;\n\t\tnapi_disable(irqtask);\n\t\tnetif_napi_del(irqtask);\n\n\t\tif (kill_fq(qidev, per_cpu(pcpu_qipriv.rsp_fq, i)))\n\t\t\tdev_err(qidev, \"Rsp FQ kill failed, cpu: %d\\n\", i);\n\t}\n\n\tqman_delete_cgr_safe(&priv->cgr);\n\tqman_release_cgrid(priv->cgr.cgrid);\n\n\tkmem_cache_destroy(qi_cache);\n}\n\nstatic void cgr_cb(struct qman_portal *qm, struct qman_cgr *cgr, int congested)\n{\n\tcaam_congested = congested;\n\n\tif (congested) {\n\t\tcaam_debugfs_qi_congested();\n\n\t\tpr_debug_ratelimited(\"CAAM entered congestion\\n\");\n\n\t} else {\n\t\tpr_debug_ratelimited(\"CAAM exited congestion\\n\");\n\t}\n}\n\nstatic int caam_qi_napi_schedule(struct qman_portal *p, struct caam_napi *np,\n\t\t\t\t bool sched_napi)\n{\n\tif (sched_napi) {\n\t\t \n\t\tqman_p_irqsource_remove(p, QM_PIRQ_DQRI);\n\t\tnp->p = p;\n\t\tnapi_schedule(&np->irqtask);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic enum qman_cb_dqrr_result caam_rsp_fq_dqrr_cb(struct qman_portal *p,\n\t\t\t\t\t\t    struct qman_fq *rsp_fq,\n\t\t\t\t\t\t    const struct qm_dqrr_entry *dqrr,\n\t\t\t\t\t\t    bool sched_napi)\n{\n\tstruct caam_napi *caam_napi = raw_cpu_ptr(&pcpu_qipriv.caam_napi);\n\tstruct caam_drv_req *drv_req;\n\tconst struct qm_fd *fd;\n\tstruct device *qidev = &(raw_cpu_ptr(&pcpu_qipriv)->net_dev.dev);\n\tstruct caam_drv_private *priv = dev_get_drvdata(qidev);\n\tu32 status;\n\n\tif (caam_qi_napi_schedule(p, caam_napi, sched_napi))\n\t\treturn qman_cb_dqrr_stop;\n\n\tfd = &dqrr->fd;\n\n\tdrv_req = caam_iova_to_virt(priv->domain, qm_fd_addr_get64(fd));\n\tif (unlikely(!drv_req)) {\n\t\tdev_err(qidev,\n\t\t\t\"Can't find original request for caam response\\n\");\n\t\treturn qman_cb_dqrr_consume;\n\t}\n\n\trefcount_dec(&drv_req->drv_ctx->refcnt);\n\n\tstatus = be32_to_cpu(fd->status);\n\tif (unlikely(status)) {\n\t\tu32 ssrc = status & JRSTA_SSRC_MASK;\n\t\tu8 err_id = status & JRSTA_CCBERR_ERRID_MASK;\n\n\t\tif (ssrc != JRSTA_SSRC_CCB_ERROR ||\n\t\t    err_id != JRSTA_CCBERR_ERRID_ICVCHK)\n\t\t\tdev_err_ratelimited(qidev,\n\t\t\t\t\t    \"Error: %#x in CAAM response FD\\n\",\n\t\t\t\t\t    status);\n\t}\n\n\tif (unlikely(qm_fd_get_format(fd) != qm_fd_compound)) {\n\t\tdev_err(qidev, \"Non-compound FD from CAAM\\n\");\n\t\treturn qman_cb_dqrr_consume;\n\t}\n\n\tdma_unmap_single(drv_req->drv_ctx->qidev, qm_fd_addr(fd),\n\t\t\t sizeof(drv_req->fd_sgt), DMA_BIDIRECTIONAL);\n\n\tdrv_req->cbk(drv_req, status);\n\treturn qman_cb_dqrr_consume;\n}\n\nstatic int alloc_rsp_fq_cpu(struct device *qidev, unsigned int cpu)\n{\n\tstruct qm_mcc_initfq opts;\n\tstruct qman_fq *fq;\n\tint ret;\n\n\tfq = kzalloc(sizeof(*fq), GFP_KERNEL);\n\tif (!fq)\n\t\treturn -ENOMEM;\n\n\tfq->cb.dqrr = caam_rsp_fq_dqrr_cb;\n\n\tret = qman_create_fq(0, QMAN_FQ_FLAG_NO_ENQUEUE |\n\t\t\t     QMAN_FQ_FLAG_DYNAMIC_FQID, fq);\n\tif (ret) {\n\t\tdev_err(qidev, \"Rsp FQ create failed\\n\");\n\t\tkfree(fq);\n\t\treturn -ENODEV;\n\t}\n\n\tmemset(&opts, 0, sizeof(opts));\n\topts.we_mask = cpu_to_be16(QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_DESTWQ |\n\t\t\t\t   QM_INITFQ_WE_CONTEXTB |\n\t\t\t\t   QM_INITFQ_WE_CONTEXTA | QM_INITFQ_WE_CGID);\n\topts.fqd.fq_ctrl = cpu_to_be16(QM_FQCTRL_CTXASTASHING |\n\t\t\t\t       QM_FQCTRL_CPCSTASH | QM_FQCTRL_CGE);\n\tqm_fqd_set_destwq(&opts.fqd, qman_affine_channel(cpu), 3);\n\topts.fqd.cgid = qipriv.cgr.cgrid;\n\topts.fqd.context_a.stashing.exclusive =\tQM_STASHING_EXCL_CTX |\n\t\t\t\t\t\tQM_STASHING_EXCL_DATA;\n\tqm_fqd_set_stashing(&opts.fqd, 0, 1, 1);\n\n\tret = qman_init_fq(fq, QMAN_INITFQ_FLAG_SCHED, &opts);\n\tif (ret) {\n\t\tdev_err(qidev, \"Rsp FQ init failed\\n\");\n\t\tkfree(fq);\n\t\treturn -ENODEV;\n\t}\n\n\tper_cpu(pcpu_qipriv.rsp_fq, cpu) = fq;\n\n\tdev_dbg(qidev, \"Allocated response FQ %u for CPU %u\", fq->fqid, cpu);\n\treturn 0;\n}\n\nstatic int init_cgr(struct device *qidev)\n{\n\tint ret;\n\tstruct qm_mcc_initcgr opts;\n\tconst u64 val = (u64)cpumask_weight(qman_affine_cpus()) *\n\t\t\tMAX_RSP_FQ_BACKLOG_PER_CPU;\n\n\tret = qman_alloc_cgrid(&qipriv.cgr.cgrid);\n\tif (ret) {\n\t\tdev_err(qidev, \"CGR alloc failed for rsp FQs: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tqipriv.cgr.cb = cgr_cb;\n\tmemset(&opts, 0, sizeof(opts));\n\topts.we_mask = cpu_to_be16(QM_CGR_WE_CSCN_EN | QM_CGR_WE_CS_THRES |\n\t\t\t\t   QM_CGR_WE_MODE);\n\topts.cgr.cscn_en = QM_CGR_EN;\n\topts.cgr.mode = QMAN_CGR_MODE_FRAME;\n\tqm_cgr_cs_thres_set64(&opts.cgr.cs_thres, val, 1);\n\n\tret = qman_create_cgr(&qipriv.cgr, QMAN_CGR_FLAG_USE_INIT, &opts);\n\tif (ret) {\n\t\tdev_err(qidev, \"Error %d creating CAAM CGRID: %u\\n\", ret,\n\t\t\tqipriv.cgr.cgrid);\n\t\treturn ret;\n\t}\n\n\tdev_dbg(qidev, \"Congestion threshold set to %llu\\n\", val);\n\treturn 0;\n}\n\nstatic int alloc_rsp_fqs(struct device *qidev)\n{\n\tint ret, i;\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\n\t \n\tfor_each_cpu(i, cpus) {\n\t\tret = alloc_rsp_fq_cpu(qidev, i);\n\t\tif (ret) {\n\t\t\tdev_err(qidev, \"CAAM rsp FQ alloc failed, cpu: %u\", i);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_rsp_fqs(void)\n{\n\tint i;\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\n\tfor_each_cpu(i, cpus)\n\t\tkfree(per_cpu(pcpu_qipriv.rsp_fq, i));\n}\n\nint caam_qi_init(struct platform_device *caam_pdev)\n{\n\tint err, i;\n\tstruct device *ctrldev = &caam_pdev->dev, *qidev;\n\tstruct caam_drv_private *ctrlpriv;\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\n\tctrlpriv = dev_get_drvdata(ctrldev);\n\tqidev = ctrldev;\n\n\t \n\terr = init_cgr(qidev);\n\tif (err) {\n\t\tdev_err(qidev, \"CGR initialization failed: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\terr = alloc_rsp_fqs(qidev);\n\tif (err) {\n\t\tdev_err(qidev, \"Can't allocate CAAM response FQs: %d\\n\", err);\n\t\tfree_rsp_fqs();\n\t\treturn err;\n\t}\n\n\t \n\tfor_each_cpu(i, cpus) {\n\t\tstruct caam_qi_pcpu_priv *priv = per_cpu_ptr(&pcpu_qipriv, i);\n\t\tstruct caam_napi *caam_napi = &priv->caam_napi;\n\t\tstruct napi_struct *irqtask = &caam_napi->irqtask;\n\t\tstruct net_device *net_dev = &priv->net_dev;\n\n\t\tnet_dev->dev = *qidev;\n\t\tINIT_LIST_HEAD(&net_dev->napi_list);\n\n\t\tnetif_napi_add_tx_weight(net_dev, irqtask, caam_qi_poll,\n\t\t\t\t\t CAAM_NAPI_WEIGHT);\n\n\t\tnapi_enable(irqtask);\n\t}\n\n\tqi_cache = kmem_cache_create(\"caamqicache\", CAAM_QI_MEMCACHE_SIZE,\n\t\t\t\t     dma_get_cache_alignment(), 0, NULL);\n\tif (!qi_cache) {\n\t\tdev_err(qidev, \"Can't allocate CAAM cache\\n\");\n\t\tfree_rsp_fqs();\n\t\treturn -ENOMEM;\n\t}\n\n\tcaam_debugfs_qi_init(ctrlpriv);\n\n\terr = devm_add_action_or_reset(qidev, caam_qi_shutdown, ctrlpriv);\n\tif (err)\n\t\treturn err;\n\n\tdev_info(qidev, \"Linux CAAM Queue I/F driver initialised\\n\");\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}