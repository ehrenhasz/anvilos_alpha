{
  "module_name": "atmel-aes.c",
  "hash_id": "d7dd8e88f958247053f1db8fc716f27fcfbf569f0bbdaaf2c0a8611607fd639e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/atmel-aes.c",
  "human_readable_source": "\n \n\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/clk.h>\n#include <linux/io.h>\n#include <linux/hw_random.h>\n#include <linux/platform_device.h>\n\n#include <linux/device.h>\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-mapping.h>\n#include <linux/mod_devicetable.h>\n#include <linux/delay.h>\n#include <linux/crypto.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/algapi.h>\n#include <crypto/aes.h>\n#include <crypto/gcm.h>\n#include <crypto/xts.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include \"atmel-aes-regs.h\"\n#include \"atmel-authenc.h\"\n\n#define ATMEL_AES_PRIORITY\t300\n\n#define ATMEL_AES_BUFFER_ORDER\t2\n#define ATMEL_AES_BUFFER_SIZE\t(PAGE_SIZE << ATMEL_AES_BUFFER_ORDER)\n\n#define CFB8_BLOCK_SIZE\t\t1\n#define CFB16_BLOCK_SIZE\t2\n#define CFB32_BLOCK_SIZE\t4\n#define CFB64_BLOCK_SIZE\t8\n\n#define SIZE_IN_WORDS(x)\t((x) >> 2)\n\n \n \n#define AES_FLAGS_ENCRYPT\tAES_MR_CYPHER_ENC\n#define AES_FLAGS_GTAGEN\tAES_MR_GTAGEN\n#define AES_FLAGS_OPMODE_MASK\t(AES_MR_OPMOD_MASK | AES_MR_CFBS_MASK)\n#define AES_FLAGS_ECB\t\tAES_MR_OPMOD_ECB\n#define AES_FLAGS_CBC\t\tAES_MR_OPMOD_CBC\n#define AES_FLAGS_OFB\t\tAES_MR_OPMOD_OFB\n#define AES_FLAGS_CFB128\t(AES_MR_OPMOD_CFB | AES_MR_CFBS_128b)\n#define AES_FLAGS_CFB64\t\t(AES_MR_OPMOD_CFB | AES_MR_CFBS_64b)\n#define AES_FLAGS_CFB32\t\t(AES_MR_OPMOD_CFB | AES_MR_CFBS_32b)\n#define AES_FLAGS_CFB16\t\t(AES_MR_OPMOD_CFB | AES_MR_CFBS_16b)\n#define AES_FLAGS_CFB8\t\t(AES_MR_OPMOD_CFB | AES_MR_CFBS_8b)\n#define AES_FLAGS_CTR\t\tAES_MR_OPMOD_CTR\n#define AES_FLAGS_GCM\t\tAES_MR_OPMOD_GCM\n#define AES_FLAGS_XTS\t\tAES_MR_OPMOD_XTS\n\n#define AES_FLAGS_MODE_MASK\t(AES_FLAGS_OPMODE_MASK |\t\\\n\t\t\t\t AES_FLAGS_ENCRYPT |\t\t\\\n\t\t\t\t AES_FLAGS_GTAGEN)\n\n#define AES_FLAGS_BUSY\t\tBIT(3)\n#define AES_FLAGS_DUMP_REG\tBIT(4)\n#define AES_FLAGS_OWN_SHA\tBIT(5)\n\n#define AES_FLAGS_PERSISTENT\tAES_FLAGS_BUSY\n\n#define ATMEL_AES_QUEUE_LENGTH\t50\n\n#define ATMEL_AES_DMA_THRESHOLD\t\t256\n\n\nstruct atmel_aes_caps {\n\tbool\t\t\thas_dualbuff;\n\tbool\t\t\thas_cfb64;\n\tbool\t\t\thas_gcm;\n\tbool\t\t\thas_xts;\n\tbool\t\t\thas_authenc;\n\tu32\t\t\tmax_burst_size;\n};\n\nstruct atmel_aes_dev;\n\n\ntypedef int (*atmel_aes_fn_t)(struct atmel_aes_dev *);\n\n\nstruct atmel_aes_base_ctx {\n\tstruct atmel_aes_dev\t*dd;\n\tatmel_aes_fn_t\t\tstart;\n\tint\t\t\tkeylen;\n\tu32\t\t\tkey[AES_KEYSIZE_256 / sizeof(u32)];\n\tu16\t\t\tblock_size;\n\tbool\t\t\tis_aead;\n};\n\nstruct atmel_aes_ctx {\n\tstruct atmel_aes_base_ctx\tbase;\n};\n\nstruct atmel_aes_ctr_ctx {\n\tstruct atmel_aes_base_ctx\tbase;\n\n\t__be32\t\t\tiv[AES_BLOCK_SIZE / sizeof(u32)];\n\tsize_t\t\t\toffset;\n\tstruct scatterlist\tsrc[2];\n\tstruct scatterlist\tdst[2];\n\tu32\t\t\tblocks;\n};\n\nstruct atmel_aes_gcm_ctx {\n\tstruct atmel_aes_base_ctx\tbase;\n\n\tstruct scatterlist\tsrc[2];\n\tstruct scatterlist\tdst[2];\n\n\t__be32\t\t\tj0[AES_BLOCK_SIZE / sizeof(u32)];\n\tu32\t\t\ttag[AES_BLOCK_SIZE / sizeof(u32)];\n\t__be32\t\t\tghash[AES_BLOCK_SIZE / sizeof(u32)];\n\tsize_t\t\t\ttextlen;\n\n\tconst __be32\t\t*ghash_in;\n\t__be32\t\t\t*ghash_out;\n\tatmel_aes_fn_t\t\tghash_resume;\n};\n\nstruct atmel_aes_xts_ctx {\n\tstruct atmel_aes_base_ctx\tbase;\n\n\tu32\t\t\tkey2[AES_KEYSIZE_256 / sizeof(u32)];\n\tstruct crypto_skcipher *fallback_tfm;\n};\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\nstruct atmel_aes_authenc_ctx {\n\tstruct atmel_aes_base_ctx\tbase;\n\tstruct atmel_sha_authenc_ctx\t*auth;\n};\n#endif\n\nstruct atmel_aes_reqctx {\n\tunsigned long\t\tmode;\n\tu8\t\t\tlastc[AES_BLOCK_SIZE];\n\tstruct skcipher_request fallback_req;\n};\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\nstruct atmel_aes_authenc_reqctx {\n\tstruct atmel_aes_reqctx\tbase;\n\n\tstruct scatterlist\tsrc[2];\n\tstruct scatterlist\tdst[2];\n\tsize_t\t\t\ttextlen;\n\tu32\t\t\tdigest[SHA512_DIGEST_SIZE / sizeof(u32)];\n\n\t \n\tstruct ahash_request\tauth_req;\n};\n#endif\n\nstruct atmel_aes_dma {\n\tstruct dma_chan\t\t*chan;\n\tstruct scatterlist\t*sg;\n\tint\t\t\tnents;\n\tunsigned int\t\tremainder;\n\tunsigned int\t\tsg_len;\n};\n\nstruct atmel_aes_dev {\n\tstruct list_head\tlist;\n\tunsigned long\t\tphys_base;\n\tvoid __iomem\t\t*io_base;\n\n\tstruct crypto_async_request\t*areq;\n\tstruct atmel_aes_base_ctx\t*ctx;\n\n\tbool\t\t\tis_async;\n\tatmel_aes_fn_t\t\tresume;\n\tatmel_aes_fn_t\t\tcpu_transfer_complete;\n\n\tstruct device\t\t*dev;\n\tstruct clk\t\t*iclk;\n\tint\t\t\tirq;\n\n\tunsigned long\t\tflags;\n\n\tspinlock_t\t\tlock;\n\tstruct crypto_queue\tqueue;\n\n\tstruct tasklet_struct\tdone_task;\n\tstruct tasklet_struct\tqueue_task;\n\n\tsize_t\t\t\ttotal;\n\tsize_t\t\t\tdatalen;\n\tu32\t\t\t*data;\n\n\tstruct atmel_aes_dma\tsrc;\n\tstruct atmel_aes_dma\tdst;\n\n\tsize_t\t\t\tbuflen;\n\tvoid\t\t\t*buf;\n\tstruct scatterlist\taligned_sg;\n\tstruct scatterlist\t*real_dst;\n\n\tstruct atmel_aes_caps\tcaps;\n\n\tu32\t\t\thw_version;\n};\n\nstruct atmel_aes_drv {\n\tstruct list_head\tdev_list;\n\tspinlock_t\t\tlock;\n};\n\nstatic struct atmel_aes_drv atmel_aes = {\n\t.dev_list = LIST_HEAD_INIT(atmel_aes.dev_list),\n\t.lock = __SPIN_LOCK_UNLOCKED(atmel_aes.lock),\n};\n\n#ifdef VERBOSE_DEBUG\nstatic const char *atmel_aes_reg_name(u32 offset, char *tmp, size_t sz)\n{\n\tswitch (offset) {\n\tcase AES_CR:\n\t\treturn \"CR\";\n\n\tcase AES_MR:\n\t\treturn \"MR\";\n\n\tcase AES_ISR:\n\t\treturn \"ISR\";\n\n\tcase AES_IMR:\n\t\treturn \"IMR\";\n\n\tcase AES_IER:\n\t\treturn \"IER\";\n\n\tcase AES_IDR:\n\t\treturn \"IDR\";\n\n\tcase AES_KEYWR(0):\n\tcase AES_KEYWR(1):\n\tcase AES_KEYWR(2):\n\tcase AES_KEYWR(3):\n\tcase AES_KEYWR(4):\n\tcase AES_KEYWR(5):\n\tcase AES_KEYWR(6):\n\tcase AES_KEYWR(7):\n\t\tsnprintf(tmp, sz, \"KEYWR[%u]\", (offset - AES_KEYWR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_IDATAR(0):\n\tcase AES_IDATAR(1):\n\tcase AES_IDATAR(2):\n\tcase AES_IDATAR(3):\n\t\tsnprintf(tmp, sz, \"IDATAR[%u]\", (offset - AES_IDATAR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_ODATAR(0):\n\tcase AES_ODATAR(1):\n\tcase AES_ODATAR(2):\n\tcase AES_ODATAR(3):\n\t\tsnprintf(tmp, sz, \"ODATAR[%u]\", (offset - AES_ODATAR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_IVR(0):\n\tcase AES_IVR(1):\n\tcase AES_IVR(2):\n\tcase AES_IVR(3):\n\t\tsnprintf(tmp, sz, \"IVR[%u]\", (offset - AES_IVR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_AADLENR:\n\t\treturn \"AADLENR\";\n\n\tcase AES_CLENR:\n\t\treturn \"CLENR\";\n\n\tcase AES_GHASHR(0):\n\tcase AES_GHASHR(1):\n\tcase AES_GHASHR(2):\n\tcase AES_GHASHR(3):\n\t\tsnprintf(tmp, sz, \"GHASHR[%u]\", (offset - AES_GHASHR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_TAGR(0):\n\tcase AES_TAGR(1):\n\tcase AES_TAGR(2):\n\tcase AES_TAGR(3):\n\t\tsnprintf(tmp, sz, \"TAGR[%u]\", (offset - AES_TAGR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_CTRR:\n\t\treturn \"CTRR\";\n\n\tcase AES_GCMHR(0):\n\tcase AES_GCMHR(1):\n\tcase AES_GCMHR(2):\n\tcase AES_GCMHR(3):\n\t\tsnprintf(tmp, sz, \"GCMHR[%u]\", (offset - AES_GCMHR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_EMR:\n\t\treturn \"EMR\";\n\n\tcase AES_TWR(0):\n\tcase AES_TWR(1):\n\tcase AES_TWR(2):\n\tcase AES_TWR(3):\n\t\tsnprintf(tmp, sz, \"TWR[%u]\", (offset - AES_TWR(0)) >> 2);\n\t\tbreak;\n\n\tcase AES_ALPHAR(0):\n\tcase AES_ALPHAR(1):\n\tcase AES_ALPHAR(2):\n\tcase AES_ALPHAR(3):\n\t\tsnprintf(tmp, sz, \"ALPHAR[%u]\", (offset - AES_ALPHAR(0)) >> 2);\n\t\tbreak;\n\n\tdefault:\n\t\tsnprintf(tmp, sz, \"0x%02x\", offset);\n\t\tbreak;\n\t}\n\n\treturn tmp;\n}\n#endif  \n\n \n\nstatic inline u32 atmel_aes_read(struct atmel_aes_dev *dd, u32 offset)\n{\n\tu32 value = readl_relaxed(dd->io_base + offset);\n\n#ifdef VERBOSE_DEBUG\n\tif (dd->flags & AES_FLAGS_DUMP_REG) {\n\t\tchar tmp[16];\n\n\t\tdev_vdbg(dd->dev, \"read 0x%08x from %s\\n\", value,\n\t\t\t atmel_aes_reg_name(offset, tmp, sizeof(tmp)));\n\t}\n#endif  \n\n\treturn value;\n}\n\nstatic inline void atmel_aes_write(struct atmel_aes_dev *dd,\n\t\t\t\t\tu32 offset, u32 value)\n{\n#ifdef VERBOSE_DEBUG\n\tif (dd->flags & AES_FLAGS_DUMP_REG) {\n\t\tchar tmp[16];\n\n\t\tdev_vdbg(dd->dev, \"write 0x%08x into %s\\n\", value,\n\t\t\t atmel_aes_reg_name(offset, tmp, sizeof(tmp)));\n\t}\n#endif  \n\n\twritel_relaxed(value, dd->io_base + offset);\n}\n\nstatic void atmel_aes_read_n(struct atmel_aes_dev *dd, u32 offset,\n\t\t\t\t\tu32 *value, int count)\n{\n\tfor (; count--; value++, offset += 4)\n\t\t*value = atmel_aes_read(dd, offset);\n}\n\nstatic void atmel_aes_write_n(struct atmel_aes_dev *dd, u32 offset,\n\t\t\t      const u32 *value, int count)\n{\n\tfor (; count--; value++, offset += 4)\n\t\tatmel_aes_write(dd, offset, *value);\n}\n\nstatic inline void atmel_aes_read_block(struct atmel_aes_dev *dd, u32 offset,\n\t\t\t\t\tvoid *value)\n{\n\tatmel_aes_read_n(dd, offset, value, SIZE_IN_WORDS(AES_BLOCK_SIZE));\n}\n\nstatic inline void atmel_aes_write_block(struct atmel_aes_dev *dd, u32 offset,\n\t\t\t\t\t const void *value)\n{\n\tatmel_aes_write_n(dd, offset, value, SIZE_IN_WORDS(AES_BLOCK_SIZE));\n}\n\nstatic inline int atmel_aes_wait_for_data_ready(struct atmel_aes_dev *dd,\n\t\t\t\t\t\tatmel_aes_fn_t resume)\n{\n\tu32 isr = atmel_aes_read(dd, AES_ISR);\n\n\tif (unlikely(isr & AES_INT_DATARDY))\n\t\treturn resume(dd);\n\n\tdd->resume = resume;\n\tatmel_aes_write(dd, AES_IER, AES_INT_DATARDY);\n\treturn -EINPROGRESS;\n}\n\nstatic inline size_t atmel_aes_padlen(size_t len, size_t block_size)\n{\n\tlen &= block_size - 1;\n\treturn len ? block_size - len : 0;\n}\n\nstatic struct atmel_aes_dev *atmel_aes_dev_alloc(struct atmel_aes_base_ctx *ctx)\n{\n\tstruct atmel_aes_dev *aes_dd;\n\n\tspin_lock_bh(&atmel_aes.lock);\n\t \n\taes_dd = list_first_entry_or_null(&atmel_aes.dev_list,\n\t\t\t\t\t  struct atmel_aes_dev, list);\n\tspin_unlock_bh(&atmel_aes.lock);\n\treturn aes_dd;\n}\n\nstatic int atmel_aes_hw_init(struct atmel_aes_dev *dd)\n{\n\tint err;\n\n\terr = clk_enable(dd->iclk);\n\tif (err)\n\t\treturn err;\n\n\tatmel_aes_write(dd, AES_CR, AES_CR_SWRST);\n\tatmel_aes_write(dd, AES_MR, 0xE << AES_MR_CKEY_OFFSET);\n\n\treturn 0;\n}\n\nstatic inline unsigned int atmel_aes_get_version(struct atmel_aes_dev *dd)\n{\n\treturn atmel_aes_read(dd, AES_HW_VERSION) & 0x00000fff;\n}\n\nstatic int atmel_aes_hw_version_init(struct atmel_aes_dev *dd)\n{\n\tint err;\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn err;\n\n\tdd->hw_version = atmel_aes_get_version(dd);\n\n\tdev_info(dd->dev, \"version: 0x%x\\n\", dd->hw_version);\n\n\tclk_disable(dd->iclk);\n\treturn 0;\n}\n\nstatic inline void atmel_aes_set_mode(struct atmel_aes_dev *dd,\n\t\t\t\t      const struct atmel_aes_reqctx *rctx)\n{\n\t \n\tdd->flags = (dd->flags & AES_FLAGS_PERSISTENT) | rctx->mode;\n}\n\nstatic inline bool atmel_aes_is_encrypt(const struct atmel_aes_dev *dd)\n{\n\treturn (dd->flags & AES_FLAGS_ENCRYPT);\n}\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\nstatic void atmel_aes_authenc_complete(struct atmel_aes_dev *dd, int err);\n#endif\n\nstatic void atmel_aes_set_iv_as_last_ciphertext_block(struct atmel_aes_dev *dd)\n{\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(skcipher);\n\n\tif (req->cryptlen < ivsize)\n\t\treturn;\n\n\tif (rctx->mode & AES_FLAGS_ENCRYPT)\n\t\tscatterwalk_map_and_copy(req->iv, req->dst,\n\t\t\t\t\t req->cryptlen - ivsize, ivsize, 0);\n\telse\n\t\tmemcpy(req->iv, rctx->lastc, ivsize);\n}\n\nstatic inline struct atmel_aes_ctr_ctx *\natmel_aes_ctr_ctx_cast(struct atmel_aes_base_ctx *ctx)\n{\n\treturn container_of(ctx, struct atmel_aes_ctr_ctx, base);\n}\n\nstatic void atmel_aes_ctr_update_req_iv(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_ctr_ctx *ctx = atmel_aes_ctr_ctx_cast(dd->ctx);\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(skcipher);\n\tint i;\n\n\t \n\tfor (i = 0; i < ctx->blocks; i++)\n\t\tcrypto_inc((u8 *)ctx->iv, AES_BLOCK_SIZE);\n\n\tmemcpy(req->iv, ctx->iv, ivsize);\n}\n\nstatic inline int atmel_aes_complete(struct atmel_aes_dev *dd, int err)\n{\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n\tif (dd->ctx->is_aead)\n\t\tatmel_aes_authenc_complete(dd, err);\n#endif\n\n\tclk_disable(dd->iclk);\n\tdd->flags &= ~AES_FLAGS_BUSY;\n\n\tif (!err && !dd->ctx->is_aead &&\n\t    (rctx->mode & AES_FLAGS_OPMODE_MASK) != AES_FLAGS_ECB) {\n\t\tif ((rctx->mode & AES_FLAGS_OPMODE_MASK) != AES_FLAGS_CTR)\n\t\t\tatmel_aes_set_iv_as_last_ciphertext_block(dd);\n\t\telse\n\t\t\tatmel_aes_ctr_update_req_iv(dd);\n\t}\n\n\tif (dd->is_async)\n\t\tcrypto_request_complete(dd->areq, err);\n\n\ttasklet_schedule(&dd->queue_task);\n\n\treturn err;\n}\n\nstatic void atmel_aes_write_ctrl_key(struct atmel_aes_dev *dd, bool use_dma,\n\t\t\t\t     const __be32 *iv, const u32 *key, int keylen)\n{\n\tu32 valmr = 0;\n\n\t \n\tif (keylen == AES_KEYSIZE_128)\n\t\tvalmr |= AES_MR_KEYSIZE_128;\n\telse if (keylen == AES_KEYSIZE_192)\n\t\tvalmr |= AES_MR_KEYSIZE_192;\n\telse\n\t\tvalmr |= AES_MR_KEYSIZE_256;\n\n\tvalmr |= dd->flags & AES_FLAGS_MODE_MASK;\n\n\tif (use_dma) {\n\t\tvalmr |= AES_MR_SMOD_IDATAR0;\n\t\tif (dd->caps.has_dualbuff)\n\t\t\tvalmr |= AES_MR_DUALBUFF;\n\t} else {\n\t\tvalmr |= AES_MR_SMOD_AUTO;\n\t}\n\n\tatmel_aes_write(dd, AES_MR, valmr);\n\n\tatmel_aes_write_n(dd, AES_KEYWR(0), key, SIZE_IN_WORDS(keylen));\n\n\tif (iv && (valmr & AES_MR_OPMOD_MASK) != AES_MR_OPMOD_ECB)\n\t\tatmel_aes_write_block(dd, AES_IVR(0), iv);\n}\n\nstatic inline void atmel_aes_write_ctrl(struct atmel_aes_dev *dd, bool use_dma,\n\t\t\t\t\tconst __be32 *iv)\n\n{\n\tatmel_aes_write_ctrl_key(dd, use_dma, iv,\n\t\t\t\t dd->ctx->key, dd->ctx->keylen);\n}\n\n \n\nstatic int atmel_aes_cpu_transfer(struct atmel_aes_dev *dd)\n{\n\tint err = 0;\n\tu32 isr;\n\n\tfor (;;) {\n\t\tatmel_aes_read_block(dd, AES_ODATAR(0), dd->data);\n\t\tdd->data += 4;\n\t\tdd->datalen -= AES_BLOCK_SIZE;\n\n\t\tif (dd->datalen < AES_BLOCK_SIZE)\n\t\t\tbreak;\n\n\t\tatmel_aes_write_block(dd, AES_IDATAR(0), dd->data);\n\n\t\tisr = atmel_aes_read(dd, AES_ISR);\n\t\tif (!(isr & AES_INT_DATARDY)) {\n\t\t\tdd->resume = atmel_aes_cpu_transfer;\n\t\t\tatmel_aes_write(dd, AES_IER, AES_INT_DATARDY);\n\t\t\treturn -EINPROGRESS;\n\t\t}\n\t}\n\n\tif (!sg_copy_from_buffer(dd->real_dst, sg_nents(dd->real_dst),\n\t\t\t\t dd->buf, dd->total))\n\t\terr = -EINVAL;\n\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\treturn dd->cpu_transfer_complete(dd);\n}\n\nstatic int atmel_aes_cpu_start(struct atmel_aes_dev *dd,\n\t\t\t       struct scatterlist *src,\n\t\t\t       struct scatterlist *dst,\n\t\t\t       size_t len,\n\t\t\t       atmel_aes_fn_t resume)\n{\n\tsize_t padlen = atmel_aes_padlen(len, AES_BLOCK_SIZE);\n\n\tif (unlikely(len == 0))\n\t\treturn -EINVAL;\n\n\tsg_copy_to_buffer(src, sg_nents(src), dd->buf, len);\n\n\tdd->total = len;\n\tdd->real_dst = dst;\n\tdd->cpu_transfer_complete = resume;\n\tdd->datalen = len + padlen;\n\tdd->data = (u32 *)dd->buf;\n\tatmel_aes_write_block(dd, AES_IDATAR(0), dd->data);\n\treturn atmel_aes_wait_for_data_ready(dd, atmel_aes_cpu_transfer);\n}\n\n\n \n\nstatic void atmel_aes_dma_callback(void *data);\n\nstatic bool atmel_aes_check_aligned(struct atmel_aes_dev *dd,\n\t\t\t\t    struct scatterlist *sg,\n\t\t\t\t    size_t len,\n\t\t\t\t    struct atmel_aes_dma *dma)\n{\n\tint nents;\n\n\tif (!IS_ALIGNED(len, dd->ctx->block_size))\n\t\treturn false;\n\n\tfor (nents = 0; sg; sg = sg_next(sg), ++nents) {\n\t\tif (!IS_ALIGNED(sg->offset, sizeof(u32)))\n\t\t\treturn false;\n\n\t\tif (len <= sg->length) {\n\t\t\tif (!IS_ALIGNED(len, dd->ctx->block_size))\n\t\t\t\treturn false;\n\n\t\t\tdma->nents = nents+1;\n\t\t\tdma->remainder = sg->length - len;\n\t\t\tsg->length = len;\n\t\t\treturn true;\n\t\t}\n\n\t\tif (!IS_ALIGNED(sg->length, dd->ctx->block_size))\n\t\t\treturn false;\n\n\t\tlen -= sg->length;\n\t}\n\n\treturn false;\n}\n\nstatic inline void atmel_aes_restore_sg(const struct atmel_aes_dma *dma)\n{\n\tstruct scatterlist *sg = dma->sg;\n\tint nents = dma->nents;\n\n\tif (!dma->remainder)\n\t\treturn;\n\n\twhile (--nents > 0 && sg)\n\t\tsg = sg_next(sg);\n\n\tif (!sg)\n\t\treturn;\n\n\tsg->length += dma->remainder;\n}\n\nstatic int atmel_aes_map(struct atmel_aes_dev *dd,\n\t\t\t struct scatterlist *src,\n\t\t\t struct scatterlist *dst,\n\t\t\t size_t len)\n{\n\tbool src_aligned, dst_aligned;\n\tsize_t padlen;\n\n\tdd->total = len;\n\tdd->src.sg = src;\n\tdd->dst.sg = dst;\n\tdd->real_dst = dst;\n\n\tsrc_aligned = atmel_aes_check_aligned(dd, src, len, &dd->src);\n\tif (src == dst)\n\t\tdst_aligned = src_aligned;\n\telse\n\t\tdst_aligned = atmel_aes_check_aligned(dd, dst, len, &dd->dst);\n\tif (!src_aligned || !dst_aligned) {\n\t\tpadlen = atmel_aes_padlen(len, dd->ctx->block_size);\n\n\t\tif (dd->buflen < len + padlen)\n\t\t\treturn -ENOMEM;\n\n\t\tif (!src_aligned) {\n\t\t\tsg_copy_to_buffer(src, sg_nents(src), dd->buf, len);\n\t\t\tdd->src.sg = &dd->aligned_sg;\n\t\t\tdd->src.nents = 1;\n\t\t\tdd->src.remainder = 0;\n\t\t}\n\n\t\tif (!dst_aligned) {\n\t\t\tdd->dst.sg = &dd->aligned_sg;\n\t\t\tdd->dst.nents = 1;\n\t\t\tdd->dst.remainder = 0;\n\t\t}\n\n\t\tsg_init_table(&dd->aligned_sg, 1);\n\t\tsg_set_buf(&dd->aligned_sg, dd->buf, len + padlen);\n\t}\n\n\tif (dd->src.sg == dd->dst.sg) {\n\t\tdd->src.sg_len = dma_map_sg(dd->dev, dd->src.sg, dd->src.nents,\n\t\t\t\t\t    DMA_BIDIRECTIONAL);\n\t\tdd->dst.sg_len = dd->src.sg_len;\n\t\tif (!dd->src.sg_len)\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tdd->src.sg_len = dma_map_sg(dd->dev, dd->src.sg, dd->src.nents,\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (!dd->src.sg_len)\n\t\t\treturn -EFAULT;\n\n\t\tdd->dst.sg_len = dma_map_sg(dd->dev, dd->dst.sg, dd->dst.nents,\n\t\t\t\t\t    DMA_FROM_DEVICE);\n\t\tif (!dd->dst.sg_len) {\n\t\t\tdma_unmap_sg(dd->dev, dd->src.sg, dd->src.nents,\n\t\t\t\t     DMA_TO_DEVICE);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void atmel_aes_unmap(struct atmel_aes_dev *dd)\n{\n\tif (dd->src.sg == dd->dst.sg) {\n\t\tdma_unmap_sg(dd->dev, dd->src.sg, dd->src.nents,\n\t\t\t     DMA_BIDIRECTIONAL);\n\n\t\tif (dd->src.sg != &dd->aligned_sg)\n\t\t\tatmel_aes_restore_sg(&dd->src);\n\t} else {\n\t\tdma_unmap_sg(dd->dev, dd->dst.sg, dd->dst.nents,\n\t\t\t     DMA_FROM_DEVICE);\n\n\t\tif (dd->dst.sg != &dd->aligned_sg)\n\t\t\tatmel_aes_restore_sg(&dd->dst);\n\n\t\tdma_unmap_sg(dd->dev, dd->src.sg, dd->src.nents,\n\t\t\t     DMA_TO_DEVICE);\n\n\t\tif (dd->src.sg != &dd->aligned_sg)\n\t\t\tatmel_aes_restore_sg(&dd->src);\n\t}\n\n\tif (dd->dst.sg == &dd->aligned_sg)\n\t\tsg_copy_from_buffer(dd->real_dst, sg_nents(dd->real_dst),\n\t\t\t\t    dd->buf, dd->total);\n}\n\nstatic int atmel_aes_dma_transfer_start(struct atmel_aes_dev *dd,\n\t\t\t\t\tenum dma_slave_buswidth addr_width,\n\t\t\t\t\tenum dma_transfer_direction dir,\n\t\t\t\t\tu32 maxburst)\n{\n\tstruct dma_async_tx_descriptor *desc;\n\tstruct dma_slave_config config;\n\tdma_async_tx_callback callback;\n\tstruct atmel_aes_dma *dma;\n\tint err;\n\n\tmemset(&config, 0, sizeof(config));\n\tconfig.src_addr_width = addr_width;\n\tconfig.dst_addr_width = addr_width;\n\tconfig.src_maxburst = maxburst;\n\tconfig.dst_maxburst = maxburst;\n\n\tswitch (dir) {\n\tcase DMA_MEM_TO_DEV:\n\t\tdma = &dd->src;\n\t\tcallback = NULL;\n\t\tconfig.dst_addr = dd->phys_base + AES_IDATAR(0);\n\t\tbreak;\n\n\tcase DMA_DEV_TO_MEM:\n\t\tdma = &dd->dst;\n\t\tcallback = atmel_aes_dma_callback;\n\t\tconfig.src_addr = dd->phys_base + AES_ODATAR(0);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\terr = dmaengine_slave_config(dma->chan, &config);\n\tif (err)\n\t\treturn err;\n\n\tdesc = dmaengine_prep_slave_sg(dma->chan, dma->sg, dma->sg_len, dir,\n\t\t\t\t       DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\tdesc->callback = callback;\n\tdesc->callback_param = dd;\n\tdmaengine_submit(desc);\n\tdma_async_issue_pending(dma->chan);\n\n\treturn 0;\n}\n\nstatic int atmel_aes_dma_start(struct atmel_aes_dev *dd,\n\t\t\t       struct scatterlist *src,\n\t\t\t       struct scatterlist *dst,\n\t\t\t       size_t len,\n\t\t\t       atmel_aes_fn_t resume)\n{\n\tenum dma_slave_buswidth addr_width;\n\tu32 maxburst;\n\tint err;\n\n\tswitch (dd->ctx->block_size) {\n\tcase CFB8_BLOCK_SIZE:\n\t\taddr_width = DMA_SLAVE_BUSWIDTH_1_BYTE;\n\t\tmaxburst = 1;\n\t\tbreak;\n\n\tcase CFB16_BLOCK_SIZE:\n\t\taddr_width = DMA_SLAVE_BUSWIDTH_2_BYTES;\n\t\tmaxburst = 1;\n\t\tbreak;\n\n\tcase CFB32_BLOCK_SIZE:\n\tcase CFB64_BLOCK_SIZE:\n\t\taddr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\t\tmaxburst = 1;\n\t\tbreak;\n\n\tcase AES_BLOCK_SIZE:\n\t\taddr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\t\tmaxburst = dd->caps.max_burst_size;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\terr = atmel_aes_map(dd, src, dst, len);\n\tif (err)\n\t\tgoto exit;\n\n\tdd->resume = resume;\n\n\t \n\terr = atmel_aes_dma_transfer_start(dd, addr_width, DMA_DEV_TO_MEM,\n\t\t\t\t\t   maxburst);\n\tif (err)\n\t\tgoto unmap;\n\n\t \n\terr = atmel_aes_dma_transfer_start(dd, addr_width, DMA_MEM_TO_DEV,\n\t\t\t\t\t   maxburst);\n\tif (err)\n\t\tgoto output_transfer_stop;\n\n\treturn -EINPROGRESS;\n\noutput_transfer_stop:\n\tdmaengine_terminate_sync(dd->dst.chan);\nunmap:\n\tatmel_aes_unmap(dd);\nexit:\n\treturn atmel_aes_complete(dd, err);\n}\n\nstatic void atmel_aes_dma_callback(void *data)\n{\n\tstruct atmel_aes_dev *dd = data;\n\n\tatmel_aes_unmap(dd);\n\tdd->is_async = true;\n\t(void)dd->resume(dd);\n}\n\nstatic int atmel_aes_handle_queue(struct atmel_aes_dev *dd,\n\t\t\t\t  struct crypto_async_request *new_areq)\n{\n\tstruct crypto_async_request *areq, *backlog;\n\tstruct atmel_aes_base_ctx *ctx;\n\tunsigned long flags;\n\tbool start_async;\n\tint err, ret = 0;\n\n\tspin_lock_irqsave(&dd->lock, flags);\n\tif (new_areq)\n\t\tret = crypto_enqueue_request(&dd->queue, new_areq);\n\tif (dd->flags & AES_FLAGS_BUSY) {\n\t\tspin_unlock_irqrestore(&dd->lock, flags);\n\t\treturn ret;\n\t}\n\tbacklog = crypto_get_backlog(&dd->queue);\n\tareq = crypto_dequeue_request(&dd->queue);\n\tif (areq)\n\t\tdd->flags |= AES_FLAGS_BUSY;\n\tspin_unlock_irqrestore(&dd->lock, flags);\n\n\tif (!areq)\n\t\treturn ret;\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\tctx = crypto_tfm_ctx(areq->tfm);\n\n\tdd->areq = areq;\n\tdd->ctx = ctx;\n\tstart_async = (areq != new_areq);\n\tdd->is_async = start_async;\n\n\t \n\terr = ctx->start(dd);\n\treturn (start_async) ? ret : err;\n}\n\n\n \n\nstatic int atmel_aes_transfer_complete(struct atmel_aes_dev *dd)\n{\n\treturn atmel_aes_complete(dd, 0);\n}\n\nstatic int atmel_aes_start(struct atmel_aes_dev *dd)\n{\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tbool use_dma = (req->cryptlen >= ATMEL_AES_DMA_THRESHOLD ||\n\t\t\tdd->ctx->block_size != AES_BLOCK_SIZE);\n\tint err;\n\n\tatmel_aes_set_mode(dd, rctx);\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\tatmel_aes_write_ctrl(dd, use_dma, (void *)req->iv);\n\tif (use_dma)\n\t\treturn atmel_aes_dma_start(dd, req->src, req->dst,\n\t\t\t\t\t   req->cryptlen,\n\t\t\t\t\t   atmel_aes_transfer_complete);\n\n\treturn atmel_aes_cpu_start(dd, req->src, req->dst, req->cryptlen,\n\t\t\t\t   atmel_aes_transfer_complete);\n}\n\nstatic int atmel_aes_ctr_transfer(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_ctr_ctx *ctx = atmel_aes_ctr_ctx_cast(dd->ctx);\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct scatterlist *src, *dst;\n\tsize_t datalen;\n\tu32 ctr;\n\tu16 start, end;\n\tbool use_dma, fragmented = false;\n\n\t \n\tctx->offset += dd->total;\n\tif (ctx->offset >= req->cryptlen)\n\t\treturn atmel_aes_transfer_complete(dd);\n\n\t \n\tdatalen = req->cryptlen - ctx->offset;\n\tctx->blocks = DIV_ROUND_UP(datalen, AES_BLOCK_SIZE);\n\tctr = be32_to_cpu(ctx->iv[3]);\n\n\t \n\tstart = ctr & 0xffff;\n\tend = start + ctx->blocks - 1;\n\n\tif (ctx->blocks >> 16 || end < start) {\n\t\tctr |= 0xffff;\n\t\tdatalen = AES_BLOCK_SIZE * (0x10000 - start);\n\t\tfragmented = true;\n\t}\n\n\tuse_dma = (datalen >= ATMEL_AES_DMA_THRESHOLD);\n\n\t \n\tsrc = scatterwalk_ffwd(ctx->src, req->src, ctx->offset);\n\tdst = ((req->src == req->dst) ? src :\n\t       scatterwalk_ffwd(ctx->dst, req->dst, ctx->offset));\n\n\t \n\tatmel_aes_write_ctrl(dd, use_dma, ctx->iv);\n\tif (unlikely(fragmented)) {\n\t\t \n\t\tctx->iv[3] = cpu_to_be32(ctr);\n\t\tcrypto_inc((u8 *)ctx->iv, AES_BLOCK_SIZE);\n\t}\n\n\tif (use_dma)\n\t\treturn atmel_aes_dma_start(dd, src, dst, datalen,\n\t\t\t\t\t   atmel_aes_ctr_transfer);\n\n\treturn atmel_aes_cpu_start(dd, src, dst, datalen,\n\t\t\t\t   atmel_aes_ctr_transfer);\n}\n\nstatic int atmel_aes_ctr_start(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_ctr_ctx *ctx = atmel_aes_ctr_ctx_cast(dd->ctx);\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tint err;\n\n\tatmel_aes_set_mode(dd, rctx);\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\tmemcpy(ctx->iv, req->iv, AES_BLOCK_SIZE);\n\tctx->offset = 0;\n\tdd->total = 0;\n\treturn atmel_aes_ctr_transfer(dd);\n}\n\nstatic int atmel_aes_xts_fallback(struct skcipher_request *req, bool enc)\n{\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct atmel_aes_xts_ctx *ctx = crypto_skcipher_ctx(\n\t\t\tcrypto_skcipher_reqtfm(req));\n\n\tskcipher_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\tskcipher_request_set_callback(&rctx->fallback_req, req->base.flags,\n\t\t\t\t      req->base.complete, req->base.data);\n\tskcipher_request_set_crypt(&rctx->fallback_req, req->src, req->dst,\n\t\t\t\t   req->cryptlen, req->iv);\n\n\treturn enc ? crypto_skcipher_encrypt(&rctx->fallback_req) :\n\t\t     crypto_skcipher_decrypt(&rctx->fallback_req);\n}\n\nstatic int atmel_aes_crypt(struct skcipher_request *req, unsigned long mode)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct atmel_aes_base_ctx *ctx = crypto_skcipher_ctx(skcipher);\n\tstruct atmel_aes_reqctx *rctx;\n\tu32 opmode = mode & AES_FLAGS_OPMODE_MASK;\n\n\tif (opmode == AES_FLAGS_XTS) {\n\t\tif (req->cryptlen < XTS_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!IS_ALIGNED(req->cryptlen, XTS_BLOCK_SIZE))\n\t\t\treturn atmel_aes_xts_fallback(req,\n\t\t\t\t\t\t      mode & AES_FLAGS_ENCRYPT);\n\t}\n\n\t \n\tif (!req->cryptlen && opmode != AES_FLAGS_XTS)\n\t\treturn 0;\n\n\tif ((opmode == AES_FLAGS_ECB || opmode == AES_FLAGS_CBC) &&\n\t    !IS_ALIGNED(req->cryptlen, crypto_skcipher_blocksize(skcipher)))\n\t\treturn -EINVAL;\n\n\tswitch (mode & AES_FLAGS_OPMODE_MASK) {\n\tcase AES_FLAGS_CFB8:\n\t\tctx->block_size = CFB8_BLOCK_SIZE;\n\t\tbreak;\n\n\tcase AES_FLAGS_CFB16:\n\t\tctx->block_size = CFB16_BLOCK_SIZE;\n\t\tbreak;\n\n\tcase AES_FLAGS_CFB32:\n\t\tctx->block_size = CFB32_BLOCK_SIZE;\n\t\tbreak;\n\n\tcase AES_FLAGS_CFB64:\n\t\tctx->block_size = CFB64_BLOCK_SIZE;\n\t\tbreak;\n\n\tdefault:\n\t\tctx->block_size = AES_BLOCK_SIZE;\n\t\tbreak;\n\t}\n\tctx->is_aead = false;\n\n\trctx = skcipher_request_ctx(req);\n\trctx->mode = mode;\n\n\tif (opmode != AES_FLAGS_ECB &&\n\t    !(mode & AES_FLAGS_ENCRYPT)) {\n\t\tunsigned int ivsize = crypto_skcipher_ivsize(skcipher);\n\n\t\tif (req->cryptlen >= ivsize)\n\t\t\tscatterwalk_map_and_copy(rctx->lastc, req->src,\n\t\t\t\t\t\t req->cryptlen - ivsize,\n\t\t\t\t\t\t ivsize, 0);\n\t}\n\n\treturn atmel_aes_handle_queue(ctx->dd, &req->base);\n}\n\nstatic int atmel_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t   unsigned int keylen)\n{\n\tstruct atmel_aes_base_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (keylen != AES_KEYSIZE_128 &&\n\t    keylen != AES_KEYSIZE_192 &&\n\t    keylen != AES_KEYSIZE_256)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->key, key, keylen);\n\tctx->keylen = keylen;\n\n\treturn 0;\n}\n\nstatic int atmel_aes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_ECB | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_ECB);\n}\n\nstatic int atmel_aes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CBC | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CBC);\n}\n\nstatic int atmel_aes_ofb_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_OFB | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_ofb_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_OFB);\n}\n\nstatic int atmel_aes_cfb_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB128 | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cfb_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB128);\n}\n\nstatic int atmel_aes_cfb64_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB64 | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cfb64_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB64);\n}\n\nstatic int atmel_aes_cfb32_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB32 | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cfb32_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB32);\n}\n\nstatic int atmel_aes_cfb16_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB16 | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cfb16_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB16);\n}\n\nstatic int atmel_aes_cfb8_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB8 | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_cfb8_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CFB8);\n}\n\nstatic int atmel_aes_ctr_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CTR | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_ctr_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_CTR);\n}\n\nstatic int atmel_aes_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct atmel_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct atmel_aes_dev *dd;\n\n\tdd = atmel_aes_dev_alloc(&ctx->base);\n\tif (!dd)\n\t\treturn -ENODEV;\n\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct atmel_aes_reqctx));\n\tctx->base.dd = dd;\n\tctx->base.start = atmel_aes_start;\n\n\treturn 0;\n}\n\nstatic int atmel_aes_ctr_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct atmel_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct atmel_aes_dev *dd;\n\n\tdd = atmel_aes_dev_alloc(&ctx->base);\n\tif (!dd)\n\t\treturn -ENODEV;\n\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct atmel_aes_reqctx));\n\tctx->base.dd = dd;\n\tctx->base.start = atmel_aes_ctr_start;\n\n\treturn 0;\n}\n\nstatic struct skcipher_alg aes_algs[] = {\n{\n\t.base.cra_name\t\t= \"ecb(aes)\",\n\t.base.cra_driver_name\t= \"atmel-ecb-aes\",\n\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_ecb_encrypt,\n\t.decrypt\t\t= atmel_aes_ecb_decrypt,\n},\n{\n\t.base.cra_name\t\t= \"cbc(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cbc-aes\",\n\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cbc_encrypt,\n\t.decrypt\t\t= atmel_aes_cbc_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"ofb(aes)\",\n\t.base.cra_driver_name\t= \"atmel-ofb-aes\",\n\t.base.cra_blocksize\t= 1,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_ofb_encrypt,\n\t.decrypt\t\t= atmel_aes_ofb_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"cfb(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cfb-aes\",\n\t.base.cra_blocksize\t= 1,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cfb_encrypt,\n\t.decrypt\t\t= atmel_aes_cfb_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"cfb32(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cfb32-aes\",\n\t.base.cra_blocksize\t= CFB32_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cfb32_encrypt,\n\t.decrypt\t\t= atmel_aes_cfb32_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"cfb16(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cfb16-aes\",\n\t.base.cra_blocksize\t= CFB16_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cfb16_encrypt,\n\t.decrypt\t\t= atmel_aes_cfb16_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"cfb8(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cfb8-aes\",\n\t.base.cra_blocksize\t= CFB8_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cfb8_encrypt,\n\t.decrypt\t\t= atmel_aes_cfb8_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n{\n\t.base.cra_name\t\t= \"ctr(aes)\",\n\t.base.cra_driver_name\t= \"atmel-ctr-aes\",\n\t.base.cra_blocksize\t= 1,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctr_ctx),\n\n\t.init\t\t\t= atmel_aes_ctr_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_ctr_encrypt,\n\t.decrypt\t\t= atmel_aes_ctr_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n},\n};\n\nstatic struct skcipher_alg aes_cfb64_alg = {\n\t.base.cra_name\t\t= \"cfb64(aes)\",\n\t.base.cra_driver_name\t= \"atmel-cfb64-aes\",\n\t.base.cra_blocksize\t= CFB64_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_ctx),\n\n\t.init\t\t\t= atmel_aes_init_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= atmel_aes_setkey,\n\t.encrypt\t\t= atmel_aes_cfb64_encrypt,\n\t.decrypt\t\t= atmel_aes_cfb64_decrypt,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n};\n\n\n \n\nstatic int atmel_aes_gcm_ghash(struct atmel_aes_dev *dd,\n\t\t\t       const u32 *data, size_t datalen,\n\t\t\t       const __be32 *ghash_in, __be32 *ghash_out,\n\t\t\t       atmel_aes_fn_t resume);\nstatic int atmel_aes_gcm_ghash_init(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_ghash_finalize(struct atmel_aes_dev *dd);\n\nstatic int atmel_aes_gcm_start(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_process(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_length(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_data(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_tag_init(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_tag(struct atmel_aes_dev *dd);\nstatic int atmel_aes_gcm_finalize(struct atmel_aes_dev *dd);\n\nstatic inline struct atmel_aes_gcm_ctx *\natmel_aes_gcm_ctx_cast(struct atmel_aes_base_ctx *ctx)\n{\n\treturn container_of(ctx, struct atmel_aes_gcm_ctx, base);\n}\n\nstatic int atmel_aes_gcm_ghash(struct atmel_aes_dev *dd,\n\t\t\t       const u32 *data, size_t datalen,\n\t\t\t       const __be32 *ghash_in, __be32 *ghash_out,\n\t\t\t       atmel_aes_fn_t resume)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\n\tdd->data = (u32 *)data;\n\tdd->datalen = datalen;\n\tctx->ghash_in = ghash_in;\n\tctx->ghash_out = ghash_out;\n\tctx->ghash_resume = resume;\n\n\tatmel_aes_write_ctrl(dd, false, NULL);\n\treturn atmel_aes_wait_for_data_ready(dd, atmel_aes_gcm_ghash_init);\n}\n\nstatic int atmel_aes_gcm_ghash_init(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\n\t \n\tatmel_aes_write(dd, AES_AADLENR, dd->total);\n\tatmel_aes_write(dd, AES_CLENR, 0);\n\n\t \n\tif (ctx->ghash_in)\n\t\tatmel_aes_write_block(dd, AES_GHASHR(0), ctx->ghash_in);\n\n\treturn atmel_aes_gcm_ghash_finalize(dd);\n}\n\nstatic int atmel_aes_gcm_ghash_finalize(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tu32 isr;\n\n\t \n\twhile (dd->datalen > 0) {\n\t\tatmel_aes_write_block(dd, AES_IDATAR(0), dd->data);\n\t\tdd->data += 4;\n\t\tdd->datalen -= AES_BLOCK_SIZE;\n\n\t\tisr = atmel_aes_read(dd, AES_ISR);\n\t\tif (!(isr & AES_INT_DATARDY)) {\n\t\t\tdd->resume = atmel_aes_gcm_ghash_finalize;\n\t\t\tatmel_aes_write(dd, AES_IER, AES_INT_DATARDY);\n\t\t\treturn -EINPROGRESS;\n\t\t}\n\t}\n\n\t \n\tatmel_aes_read_block(dd, AES_GHASHR(0), ctx->ghash_out);\n\n\treturn ctx->ghash_resume(dd);\n}\n\n\nstatic int atmel_aes_gcm_start(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct atmel_aes_reqctx *rctx = aead_request_ctx(req);\n\tsize_t ivsize = crypto_aead_ivsize(tfm);\n\tsize_t datalen, padlen;\n\tconst void *iv = req->iv;\n\tu8 *data = dd->buf;\n\tint err;\n\n\tatmel_aes_set_mode(dd, rctx);\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\tif (likely(ivsize == GCM_AES_IV_SIZE)) {\n\t\tmemcpy(ctx->j0, iv, ivsize);\n\t\tctx->j0[3] = cpu_to_be32(1);\n\t\treturn atmel_aes_gcm_process(dd);\n\t}\n\n\tpadlen = atmel_aes_padlen(ivsize, AES_BLOCK_SIZE);\n\tdatalen = ivsize + padlen + AES_BLOCK_SIZE;\n\tif (datalen > dd->buflen)\n\t\treturn atmel_aes_complete(dd, -EINVAL);\n\n\tmemcpy(data, iv, ivsize);\n\tmemset(data + ivsize, 0, padlen + sizeof(u64));\n\t((__be64 *)(data + datalen))[-1] = cpu_to_be64(ivsize * 8);\n\n\treturn atmel_aes_gcm_ghash(dd, (const u32 *)data, datalen,\n\t\t\t\t   NULL, ctx->j0, atmel_aes_gcm_process);\n}\n\nstatic int atmel_aes_gcm_process(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tbool enc = atmel_aes_is_encrypt(dd);\n\tu32 authsize;\n\n\t \n\tauthsize = crypto_aead_authsize(tfm);\n\tctx->textlen = req->cryptlen - (enc ? 0 : authsize);\n\n\t \n\tif (likely(req->assoclen != 0 || ctx->textlen != 0))\n\t\tdd->flags |= AES_FLAGS_GTAGEN;\n\n\tatmel_aes_write_ctrl(dd, false, NULL);\n\treturn atmel_aes_wait_for_data_ready(dd, atmel_aes_gcm_length);\n}\n\nstatic int atmel_aes_gcm_length(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\t__be32 j0_lsw, *j0 = ctx->j0;\n\tsize_t padlen;\n\n\t \n\tj0_lsw = j0[3];\n\tbe32_add_cpu(&j0[3], 1);\n\tatmel_aes_write_block(dd, AES_IVR(0), j0);\n\tj0[3] = j0_lsw;\n\n\t \n\tatmel_aes_write(dd, AES_AADLENR, req->assoclen);\n\tatmel_aes_write(dd, AES_CLENR, ctx->textlen);\n\n\t \n\tif (unlikely(req->assoclen == 0)) {\n\t\tdd->datalen = 0;\n\t\treturn atmel_aes_gcm_data(dd);\n\t}\n\n\t \n\tpadlen = atmel_aes_padlen(req->assoclen, AES_BLOCK_SIZE);\n\tif (unlikely(req->assoclen + padlen > dd->buflen))\n\t\treturn atmel_aes_complete(dd, -EINVAL);\n\tsg_copy_to_buffer(req->src, sg_nents(req->src), dd->buf, req->assoclen);\n\n\t \n\tdd->data = (u32 *)dd->buf;\n\tdd->datalen = req->assoclen + padlen;\n\treturn atmel_aes_gcm_data(dd);\n}\n\nstatic int atmel_aes_gcm_data(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tbool use_dma = (ctx->textlen >= ATMEL_AES_DMA_THRESHOLD);\n\tstruct scatterlist *src, *dst;\n\tu32 isr, mr;\n\n\t \n\twhile (dd->datalen > 0) {\n\t\tatmel_aes_write_block(dd, AES_IDATAR(0), dd->data);\n\t\tdd->data += 4;\n\t\tdd->datalen -= AES_BLOCK_SIZE;\n\n\t\tisr = atmel_aes_read(dd, AES_ISR);\n\t\tif (!(isr & AES_INT_DATARDY)) {\n\t\t\tdd->resume = atmel_aes_gcm_data;\n\t\t\tatmel_aes_write(dd, AES_IER, AES_INT_DATARDY);\n\t\t\treturn -EINPROGRESS;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(ctx->textlen == 0))\n\t\treturn atmel_aes_gcm_tag_init(dd);\n\n\t \n\tsrc = scatterwalk_ffwd(ctx->src, req->src, req->assoclen);\n\tdst = ((req->src == req->dst) ? src :\n\t       scatterwalk_ffwd(ctx->dst, req->dst, req->assoclen));\n\n\tif (use_dma) {\n\t\t \n\t\tmr = atmel_aes_read(dd, AES_MR);\n\t\tmr &= ~(AES_MR_SMOD_MASK | AES_MR_DUALBUFF);\n\t\tmr |= AES_MR_SMOD_IDATAR0;\n\t\tif (dd->caps.has_dualbuff)\n\t\t\tmr |= AES_MR_DUALBUFF;\n\t\tatmel_aes_write(dd, AES_MR, mr);\n\n\t\treturn atmel_aes_dma_start(dd, src, dst, ctx->textlen,\n\t\t\t\t\t   atmel_aes_gcm_tag_init);\n\t}\n\n\treturn atmel_aes_cpu_start(dd, src, dst, ctx->textlen,\n\t\t\t\t   atmel_aes_gcm_tag_init);\n}\n\nstatic int atmel_aes_gcm_tag_init(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\t__be64 *data = dd->buf;\n\n\tif (likely(dd->flags & AES_FLAGS_GTAGEN)) {\n\t\tif (!(atmel_aes_read(dd, AES_ISR) & AES_INT_TAGRDY)) {\n\t\t\tdd->resume = atmel_aes_gcm_tag_init;\n\t\t\tatmel_aes_write(dd, AES_IER, AES_INT_TAGRDY);\n\t\t\treturn -EINPROGRESS;\n\t\t}\n\n\t\treturn atmel_aes_gcm_finalize(dd);\n\t}\n\n\t \n\tatmel_aes_read_block(dd, AES_GHASHR(0), ctx->ghash);\n\n\tdata[0] = cpu_to_be64(req->assoclen * 8);\n\tdata[1] = cpu_to_be64(ctx->textlen * 8);\n\n\treturn atmel_aes_gcm_ghash(dd, (const u32 *)data, AES_BLOCK_SIZE,\n\t\t\t\t   ctx->ghash, ctx->ghash, atmel_aes_gcm_tag);\n}\n\nstatic int atmel_aes_gcm_tag(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tunsigned long flags;\n\n\t \n\tflags = dd->flags;\n\tdd->flags &= ~(AES_FLAGS_OPMODE_MASK | AES_FLAGS_GTAGEN);\n\tdd->flags |= AES_FLAGS_CTR;\n\tatmel_aes_write_ctrl(dd, false, ctx->j0);\n\tdd->flags = flags;\n\n\tatmel_aes_write_block(dd, AES_IDATAR(0), ctx->ghash);\n\treturn atmel_aes_wait_for_data_ready(dd, atmel_aes_gcm_finalize);\n}\n\nstatic int atmel_aes_gcm_finalize(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = atmel_aes_gcm_ctx_cast(dd->ctx);\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tbool enc = atmel_aes_is_encrypt(dd);\n\tu32 offset, authsize, itag[4], *otag = ctx->tag;\n\tint err;\n\n\t \n\tif (likely(dd->flags & AES_FLAGS_GTAGEN))\n\t\tatmel_aes_read_block(dd, AES_TAGR(0), ctx->tag);\n\telse\n\t\tatmel_aes_read_block(dd, AES_ODATAR(0), ctx->tag);\n\n\toffset = req->assoclen + ctx->textlen;\n\tauthsize = crypto_aead_authsize(tfm);\n\tif (enc) {\n\t\tscatterwalk_map_and_copy(otag, req->dst, offset, authsize, 1);\n\t\terr = 0;\n\t} else {\n\t\tscatterwalk_map_and_copy(itag, req->src, offset, authsize, 0);\n\t\terr = crypto_memneq(itag, otag, authsize) ? -EBADMSG : 0;\n\t}\n\n\treturn atmel_aes_complete(dd, err);\n}\n\nstatic int atmel_aes_gcm_crypt(struct aead_request *req,\n\t\t\t       unsigned long mode)\n{\n\tstruct atmel_aes_base_ctx *ctx;\n\tstruct atmel_aes_reqctx *rctx;\n\n\tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));\n\tctx->block_size = AES_BLOCK_SIZE;\n\tctx->is_aead = true;\n\n\trctx = aead_request_ctx(req);\n\trctx->mode = AES_FLAGS_GCM | mode;\n\n\treturn atmel_aes_handle_queue(ctx->dd, &req->base);\n}\n\nstatic int atmel_aes_gcm_setkey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t\tunsigned int keylen)\n{\n\tstruct atmel_aes_base_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tif (keylen != AES_KEYSIZE_256 &&\n\t    keylen != AES_KEYSIZE_192 &&\n\t    keylen != AES_KEYSIZE_128)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->key, key, keylen);\n\tctx->keylen = keylen;\n\n\treturn 0;\n}\n\nstatic int atmel_aes_gcm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t     unsigned int authsize)\n{\n\treturn crypto_gcm_check_authsize(authsize);\n}\n\nstatic int atmel_aes_gcm_encrypt(struct aead_request *req)\n{\n\treturn atmel_aes_gcm_crypt(req, AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_gcm_decrypt(struct aead_request *req)\n{\n\treturn atmel_aes_gcm_crypt(req, 0);\n}\n\nstatic int atmel_aes_gcm_init(struct crypto_aead *tfm)\n{\n\tstruct atmel_aes_gcm_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct atmel_aes_dev *dd;\n\n\tdd = atmel_aes_dev_alloc(&ctx->base);\n\tif (!dd)\n\t\treturn -ENODEV;\n\n\tcrypto_aead_set_reqsize(tfm, sizeof(struct atmel_aes_reqctx));\n\tctx->base.dd = dd;\n\tctx->base.start = atmel_aes_gcm_start;\n\n\treturn 0;\n}\n\nstatic struct aead_alg aes_gcm_alg = {\n\t.setkey\t\t= atmel_aes_gcm_setkey,\n\t.setauthsize\t= atmel_aes_gcm_setauthsize,\n\t.encrypt\t= atmel_aes_gcm_encrypt,\n\t.decrypt\t= atmel_aes_gcm_decrypt,\n\t.init\t\t= atmel_aes_gcm_init,\n\t.ivsize\t\t= GCM_AES_IV_SIZE,\n\t.maxauthsize\t= AES_BLOCK_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"gcm(aes)\",\n\t\t.cra_driver_name\t= \"atmel-gcm-aes\",\n\t\t.cra_blocksize\t\t= 1,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_gcm_ctx),\n\t},\n};\n\n\n \n\nstatic inline struct atmel_aes_xts_ctx *\natmel_aes_xts_ctx_cast(struct atmel_aes_base_ctx *ctx)\n{\n\treturn container_of(ctx, struct atmel_aes_xts_ctx, base);\n}\n\nstatic int atmel_aes_xts_process_data(struct atmel_aes_dev *dd);\n\nstatic int atmel_aes_xts_start(struct atmel_aes_dev *dd)\n{\n\tstruct atmel_aes_xts_ctx *ctx = atmel_aes_xts_ctx_cast(dd->ctx);\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tstruct atmel_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tunsigned long flags;\n\tint err;\n\n\tatmel_aes_set_mode(dd, rctx);\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\t \n\tflags = dd->flags;\n\tdd->flags &= ~AES_FLAGS_MODE_MASK;\n\tdd->flags |= (AES_FLAGS_ECB | AES_FLAGS_ENCRYPT);\n\tatmel_aes_write_ctrl_key(dd, false, NULL,\n\t\t\t\t ctx->key2, ctx->base.keylen);\n\tdd->flags = flags;\n\n\tatmel_aes_write_block(dd, AES_IDATAR(0), req->iv);\n\treturn atmel_aes_wait_for_data_ready(dd, atmel_aes_xts_process_data);\n}\n\nstatic int atmel_aes_xts_process_data(struct atmel_aes_dev *dd)\n{\n\tstruct skcipher_request *req = skcipher_request_cast(dd->areq);\n\tbool use_dma = (req->cryptlen >= ATMEL_AES_DMA_THRESHOLD);\n\tu32 tweak[AES_BLOCK_SIZE / sizeof(u32)];\n\tstatic const __le32 one[AES_BLOCK_SIZE / sizeof(u32)] = {cpu_to_le32(1), };\n\tu8 *tweak_bytes = (u8 *)tweak;\n\tint i;\n\n\t \n\tatmel_aes_read_block(dd, AES_ODATAR(0), tweak);\n\t \n\tfor (i = 0; i < AES_BLOCK_SIZE/2; ++i)\n\t\tswap(tweak_bytes[i], tweak_bytes[AES_BLOCK_SIZE - 1 - i]);\n\n\t \n\tatmel_aes_write_ctrl(dd, use_dma, NULL);\n\tatmel_aes_write_block(dd, AES_TWR(0), tweak);\n\tatmel_aes_write_block(dd, AES_ALPHAR(0), one);\n\tif (use_dma)\n\t\treturn atmel_aes_dma_start(dd, req->src, req->dst,\n\t\t\t\t\t   req->cryptlen,\n\t\t\t\t\t   atmel_aes_transfer_complete);\n\n\treturn atmel_aes_cpu_start(dd, req->src, req->dst, req->cryptlen,\n\t\t\t\t   atmel_aes_transfer_complete);\n}\n\nstatic int atmel_aes_xts_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t\tunsigned int keylen)\n{\n\tstruct atmel_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint err;\n\n\terr = xts_verify_key(tfm, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_skcipher_clear_flags(ctx->fallback_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(ctx->fallback_tfm, tfm->base.crt_flags &\n\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\terr = crypto_skcipher_setkey(ctx->fallback_tfm, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tmemcpy(ctx->base.key, key, keylen/2);\n\tmemcpy(ctx->key2, key + keylen/2, keylen/2);\n\tctx->base.keylen = keylen/2;\n\n\treturn 0;\n}\n\nstatic int atmel_aes_xts_encrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_XTS | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_xts_decrypt(struct skcipher_request *req)\n{\n\treturn atmel_aes_crypt(req, AES_FLAGS_XTS);\n}\n\nstatic int atmel_aes_xts_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct atmel_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct atmel_aes_dev *dd;\n\tconst char *tfm_name = crypto_tfm_alg_name(&tfm->base);\n\n\tdd = atmel_aes_dev_alloc(&ctx->base);\n\tif (!dd)\n\t\treturn -ENODEV;\n\n\tctx->fallback_tfm = crypto_alloc_skcipher(tfm_name, 0,\n\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fallback_tfm))\n\t\treturn PTR_ERR(ctx->fallback_tfm);\n\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct atmel_aes_reqctx) +\n\t\t\t\t    crypto_skcipher_reqsize(ctx->fallback_tfm));\n\tctx->base.dd = dd;\n\tctx->base.start = atmel_aes_xts_start;\n\n\treturn 0;\n}\n\nstatic void atmel_aes_xts_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct atmel_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tcrypto_free_skcipher(ctx->fallback_tfm);\n}\n\nstatic struct skcipher_alg aes_xts_alg = {\n\t.base.cra_name\t\t= \"xts(aes)\",\n\t.base.cra_driver_name\t= \"atmel-xts-aes\",\n\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct atmel_aes_xts_ctx),\n\t.base.cra_flags\t\t= CRYPTO_ALG_NEED_FALLBACK,\n\n\t.min_keysize\t\t= 2 * AES_MIN_KEY_SIZE,\n\t.max_keysize\t\t= 2 * AES_MAX_KEY_SIZE,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t.setkey\t\t\t= atmel_aes_xts_setkey,\n\t.encrypt\t\t= atmel_aes_xts_encrypt,\n\t.decrypt\t\t= atmel_aes_xts_decrypt,\n\t.init\t\t\t= atmel_aes_xts_init_tfm,\n\t.exit\t\t\t= atmel_aes_xts_exit_tfm,\n};\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n \n\nstatic int atmel_aes_authenc_start(struct atmel_aes_dev *dd);\nstatic int atmel_aes_authenc_init(struct atmel_aes_dev *dd, int err,\n\t\t\t\t  bool is_async);\nstatic int atmel_aes_authenc_transfer(struct atmel_aes_dev *dd, int err,\n\t\t\t\t      bool is_async);\nstatic int atmel_aes_authenc_digest(struct atmel_aes_dev *dd);\nstatic int atmel_aes_authenc_final(struct atmel_aes_dev *dd, int err,\n\t\t\t\t   bool is_async);\n\nstatic void atmel_aes_authenc_complete(struct atmel_aes_dev *dd, int err)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\n\tif (err && (dd->flags & AES_FLAGS_OWN_SHA))\n\t\tatmel_sha_authenc_abort(&rctx->auth_req);\n\tdd->flags &= ~AES_FLAGS_OWN_SHA;\n}\n\nstatic int atmel_aes_authenc_start(struct atmel_aes_dev *dd)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct atmel_aes_authenc_ctx *ctx = crypto_aead_ctx(tfm);\n\tint err;\n\n\tatmel_aes_set_mode(dd, &rctx->base);\n\n\terr = atmel_aes_hw_init(dd);\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\treturn atmel_sha_authenc_schedule(&rctx->auth_req, ctx->auth,\n\t\t\t\t\t  atmel_aes_authenc_init, dd);\n}\n\nstatic int atmel_aes_authenc_init(struct atmel_aes_dev *dd, int err,\n\t\t\t\t  bool is_async)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\n\tif (is_async)\n\t\tdd->is_async = true;\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\t \n\tdd->flags |= AES_FLAGS_OWN_SHA;\n\n\t \n\treturn atmel_sha_authenc_init(&rctx->auth_req,\n\t\t\t\t      req->src, req->assoclen,\n\t\t\t\t      rctx->textlen,\n\t\t\t\t      atmel_aes_authenc_transfer, dd);\n}\n\nstatic int atmel_aes_authenc_transfer(struct atmel_aes_dev *dd, int err,\n\t\t\t\t      bool is_async)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\tbool enc = atmel_aes_is_encrypt(dd);\n\tstruct scatterlist *src, *dst;\n\t__be32 iv[AES_BLOCK_SIZE / sizeof(u32)];\n\tu32 emr;\n\n\tif (is_async)\n\t\tdd->is_async = true;\n\tif (err)\n\t\treturn atmel_aes_complete(dd, err);\n\n\t \n\tsrc = scatterwalk_ffwd(rctx->src, req->src, req->assoclen);\n\tdst = src;\n\n\tif (req->src != req->dst)\n\t\tdst = scatterwalk_ffwd(rctx->dst, req->dst, req->assoclen);\n\n\t \n\tmemcpy(iv, req->iv, sizeof(iv));\n\n\t \n\tatmel_aes_write_ctrl(dd, true, iv);\n\temr = AES_EMR_PLIPEN;\n\tif (!enc)\n\t\temr |= AES_EMR_PLIPD;\n\tatmel_aes_write(dd, AES_EMR, emr);\n\n\t \n\treturn atmel_aes_dma_start(dd, src, dst, rctx->textlen,\n\t\t\t\t   atmel_aes_authenc_digest);\n}\n\nstatic int atmel_aes_authenc_digest(struct atmel_aes_dev *dd)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\n\t \n\tdd->flags &= ~AES_FLAGS_OWN_SHA;\n\treturn atmel_sha_authenc_final(&rctx->auth_req,\n\t\t\t\t       rctx->digest, sizeof(rctx->digest),\n\t\t\t\t       atmel_aes_authenc_final, dd);\n}\n\nstatic int atmel_aes_authenc_final(struct atmel_aes_dev *dd, int err,\n\t\t\t\t   bool is_async)\n{\n\tstruct aead_request *req = aead_request_cast(dd->areq);\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tbool enc = atmel_aes_is_encrypt(dd);\n\tu32 idigest[SHA512_DIGEST_SIZE / sizeof(u32)], *odigest = rctx->digest;\n\tu32 offs, authsize;\n\n\tif (is_async)\n\t\tdd->is_async = true;\n\tif (err)\n\t\tgoto complete;\n\n\toffs = req->assoclen + rctx->textlen;\n\tauthsize = crypto_aead_authsize(tfm);\n\tif (enc) {\n\t\tscatterwalk_map_and_copy(odigest, req->dst, offs, authsize, 1);\n\t} else {\n\t\tscatterwalk_map_and_copy(idigest, req->src, offs, authsize, 0);\n\t\tif (crypto_memneq(idigest, odigest, authsize))\n\t\t\terr = -EBADMSG;\n\t}\n\ncomplete:\n\treturn atmel_aes_complete(dd, err);\n}\n\nstatic int atmel_aes_authenc_setkey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t\t    unsigned int keylen)\n{\n\tstruct atmel_aes_authenc_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct crypto_authenc_keys keys;\n\tint err;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto badkey;\n\n\tif (keys.enckeylen > sizeof(ctx->base.key))\n\t\tgoto badkey;\n\n\t \n\terr = atmel_sha_authenc_setkey(ctx->auth,\n\t\t\t\t       keys.authkey, keys.authkeylen,\n\t\t\t\t       crypto_aead_get_flags(tfm));\n\tif (err) {\n\t\tmemzero_explicit(&keys, sizeof(keys));\n\t\treturn err;\n\t}\n\n\t \n\tctx->base.keylen = keys.enckeylen;\n\tmemcpy(ctx->base.key, keys.enckey, keys.enckeylen);\n\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn 0;\n\nbadkey:\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn -EINVAL;\n}\n\nstatic int atmel_aes_authenc_init_tfm(struct crypto_aead *tfm,\n\t\t\t\t      unsigned long auth_mode)\n{\n\tstruct atmel_aes_authenc_ctx *ctx = crypto_aead_ctx(tfm);\n\tunsigned int auth_reqsize = atmel_sha_authenc_get_reqsize();\n\tstruct atmel_aes_dev *dd;\n\n\tdd = atmel_aes_dev_alloc(&ctx->base);\n\tif (!dd)\n\t\treturn -ENODEV;\n\n\tctx->auth = atmel_sha_authenc_spawn(auth_mode);\n\tif (IS_ERR(ctx->auth))\n\t\treturn PTR_ERR(ctx->auth);\n\n\tcrypto_aead_set_reqsize(tfm, (sizeof(struct atmel_aes_authenc_reqctx) +\n\t\t\t\t      auth_reqsize));\n\tctx->base.dd = dd;\n\tctx->base.start = atmel_aes_authenc_start;\n\n\treturn 0;\n}\n\nstatic int atmel_aes_authenc_hmac_sha1_init_tfm(struct crypto_aead *tfm)\n{\n\treturn atmel_aes_authenc_init_tfm(tfm, SHA_FLAGS_HMAC_SHA1);\n}\n\nstatic int atmel_aes_authenc_hmac_sha224_init_tfm(struct crypto_aead *tfm)\n{\n\treturn atmel_aes_authenc_init_tfm(tfm, SHA_FLAGS_HMAC_SHA224);\n}\n\nstatic int atmel_aes_authenc_hmac_sha256_init_tfm(struct crypto_aead *tfm)\n{\n\treturn atmel_aes_authenc_init_tfm(tfm, SHA_FLAGS_HMAC_SHA256);\n}\n\nstatic int atmel_aes_authenc_hmac_sha384_init_tfm(struct crypto_aead *tfm)\n{\n\treturn atmel_aes_authenc_init_tfm(tfm, SHA_FLAGS_HMAC_SHA384);\n}\n\nstatic int atmel_aes_authenc_hmac_sha512_init_tfm(struct crypto_aead *tfm)\n{\n\treturn atmel_aes_authenc_init_tfm(tfm, SHA_FLAGS_HMAC_SHA512);\n}\n\nstatic void atmel_aes_authenc_exit_tfm(struct crypto_aead *tfm)\n{\n\tstruct atmel_aes_authenc_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tatmel_sha_authenc_free(ctx->auth);\n}\n\nstatic int atmel_aes_authenc_crypt(struct aead_request *req,\n\t\t\t\t   unsigned long mode)\n{\n\tstruct atmel_aes_authenc_reqctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct atmel_aes_base_ctx *ctx = crypto_aead_ctx(tfm);\n\tu32 authsize = crypto_aead_authsize(tfm);\n\tbool enc = (mode & AES_FLAGS_ENCRYPT);\n\n\t \n\tif (!enc && req->cryptlen < authsize)\n\t\treturn -EINVAL;\n\trctx->textlen = req->cryptlen - (enc ? 0 : authsize);\n\n\t \n\tif (!rctx->textlen && !req->assoclen)\n\t\treturn -EINVAL;\n\n\trctx->base.mode = mode;\n\tctx->block_size = AES_BLOCK_SIZE;\n\tctx->is_aead = true;\n\n\treturn atmel_aes_handle_queue(ctx->dd, &req->base);\n}\n\nstatic int atmel_aes_authenc_cbc_aes_encrypt(struct aead_request *req)\n{\n\treturn atmel_aes_authenc_crypt(req, AES_FLAGS_CBC | AES_FLAGS_ENCRYPT);\n}\n\nstatic int atmel_aes_authenc_cbc_aes_decrypt(struct aead_request *req)\n{\n\treturn atmel_aes_authenc_crypt(req, AES_FLAGS_CBC);\n}\n\nstatic struct aead_alg aes_authenc_algs[] = {\n{\n\t.setkey\t\t= atmel_aes_authenc_setkey,\n\t.encrypt\t= atmel_aes_authenc_cbc_aes_encrypt,\n\t.decrypt\t= atmel_aes_authenc_cbc_aes_decrypt,\n\t.init\t\t= atmel_aes_authenc_hmac_sha1_init_tfm,\n\t.exit\t\t= atmel_aes_authenc_exit_tfm,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.maxauthsize\t= SHA1_DIGEST_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"authenc(hmac(sha1),cbc(aes))\",\n\t\t.cra_driver_name\t= \"atmel-authenc-hmac-sha1-cbc-aes\",\n\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_authenc_ctx),\n\t},\n},\n{\n\t.setkey\t\t= atmel_aes_authenc_setkey,\n\t.encrypt\t= atmel_aes_authenc_cbc_aes_encrypt,\n\t.decrypt\t= atmel_aes_authenc_cbc_aes_decrypt,\n\t.init\t\t= atmel_aes_authenc_hmac_sha224_init_tfm,\n\t.exit\t\t= atmel_aes_authenc_exit_tfm,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.maxauthsize\t= SHA224_DIGEST_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"authenc(hmac(sha224),cbc(aes))\",\n\t\t.cra_driver_name\t= \"atmel-authenc-hmac-sha224-cbc-aes\",\n\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_authenc_ctx),\n\t},\n},\n{\n\t.setkey\t\t= atmel_aes_authenc_setkey,\n\t.encrypt\t= atmel_aes_authenc_cbc_aes_encrypt,\n\t.decrypt\t= atmel_aes_authenc_cbc_aes_decrypt,\n\t.init\t\t= atmel_aes_authenc_hmac_sha256_init_tfm,\n\t.exit\t\t= atmel_aes_authenc_exit_tfm,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.maxauthsize\t= SHA256_DIGEST_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"authenc(hmac(sha256),cbc(aes))\",\n\t\t.cra_driver_name\t= \"atmel-authenc-hmac-sha256-cbc-aes\",\n\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_authenc_ctx),\n\t},\n},\n{\n\t.setkey\t\t= atmel_aes_authenc_setkey,\n\t.encrypt\t= atmel_aes_authenc_cbc_aes_encrypt,\n\t.decrypt\t= atmel_aes_authenc_cbc_aes_decrypt,\n\t.init\t\t= atmel_aes_authenc_hmac_sha384_init_tfm,\n\t.exit\t\t= atmel_aes_authenc_exit_tfm,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.maxauthsize\t= SHA384_DIGEST_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"authenc(hmac(sha384),cbc(aes))\",\n\t\t.cra_driver_name\t= \"atmel-authenc-hmac-sha384-cbc-aes\",\n\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_authenc_ctx),\n\t},\n},\n{\n\t.setkey\t\t= atmel_aes_authenc_setkey,\n\t.encrypt\t= atmel_aes_authenc_cbc_aes_encrypt,\n\t.decrypt\t= atmel_aes_authenc_cbc_aes_decrypt,\n\t.init\t\t= atmel_aes_authenc_hmac_sha512_init_tfm,\n\t.exit\t\t= atmel_aes_authenc_exit_tfm,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.maxauthsize\t= SHA512_DIGEST_SIZE,\n\n\t.base = {\n\t\t.cra_name\t\t= \"authenc(hmac(sha512),cbc(aes))\",\n\t\t.cra_driver_name\t= \"atmel-authenc-hmac-sha512-cbc-aes\",\n\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct atmel_aes_authenc_ctx),\n\t},\n},\n};\n#endif  \n\n \n\nstatic int atmel_aes_buff_init(struct atmel_aes_dev *dd)\n{\n\tdd->buf = (void *)__get_free_pages(GFP_KERNEL, ATMEL_AES_BUFFER_ORDER);\n\tdd->buflen = ATMEL_AES_BUFFER_SIZE;\n\tdd->buflen &= ~(AES_BLOCK_SIZE - 1);\n\n\tif (!dd->buf) {\n\t\tdev_err(dd->dev, \"unable to alloc pages.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void atmel_aes_buff_cleanup(struct atmel_aes_dev *dd)\n{\n\tfree_page((unsigned long)dd->buf);\n}\n\nstatic int atmel_aes_dma_init(struct atmel_aes_dev *dd)\n{\n\tint ret;\n\n\t \n\tdd->src.chan = dma_request_chan(dd->dev, \"tx\");\n\tif (IS_ERR(dd->src.chan)) {\n\t\tret = PTR_ERR(dd->src.chan);\n\t\tgoto err_dma_in;\n\t}\n\n\tdd->dst.chan = dma_request_chan(dd->dev, \"rx\");\n\tif (IS_ERR(dd->dst.chan)) {\n\t\tret = PTR_ERR(dd->dst.chan);\n\t\tgoto err_dma_out;\n\t}\n\n\treturn 0;\n\nerr_dma_out:\n\tdma_release_channel(dd->src.chan);\nerr_dma_in:\n\tdev_err(dd->dev, \"no DMA channel available\\n\");\n\treturn ret;\n}\n\nstatic void atmel_aes_dma_cleanup(struct atmel_aes_dev *dd)\n{\n\tdma_release_channel(dd->dst.chan);\n\tdma_release_channel(dd->src.chan);\n}\n\nstatic void atmel_aes_queue_task(unsigned long data)\n{\n\tstruct atmel_aes_dev *dd = (struct atmel_aes_dev *)data;\n\n\tatmel_aes_handle_queue(dd, NULL);\n}\n\nstatic void atmel_aes_done_task(unsigned long data)\n{\n\tstruct atmel_aes_dev *dd = (struct atmel_aes_dev *)data;\n\n\tdd->is_async = true;\n\t(void)dd->resume(dd);\n}\n\nstatic irqreturn_t atmel_aes_irq(int irq, void *dev_id)\n{\n\tstruct atmel_aes_dev *aes_dd = dev_id;\n\tu32 reg;\n\n\treg = atmel_aes_read(aes_dd, AES_ISR);\n\tif (reg & atmel_aes_read(aes_dd, AES_IMR)) {\n\t\tatmel_aes_write(aes_dd, AES_IDR, reg);\n\t\tif (AES_FLAGS_BUSY & aes_dd->flags)\n\t\t\ttasklet_schedule(&aes_dd->done_task);\n\t\telse\n\t\t\tdev_warn(aes_dd->dev, \"AES interrupt when no active requests.\\n\");\n\t\treturn IRQ_HANDLED;\n\t}\n\n\treturn IRQ_NONE;\n}\n\nstatic void atmel_aes_unregister_algs(struct atmel_aes_dev *dd)\n{\n\tint i;\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n\tif (dd->caps.has_authenc)\n\t\tfor (i = 0; i < ARRAY_SIZE(aes_authenc_algs); i++)\n\t\t\tcrypto_unregister_aead(&aes_authenc_algs[i]);\n#endif\n\n\tif (dd->caps.has_xts)\n\t\tcrypto_unregister_skcipher(&aes_xts_alg);\n\n\tif (dd->caps.has_gcm)\n\t\tcrypto_unregister_aead(&aes_gcm_alg);\n\n\tif (dd->caps.has_cfb64)\n\t\tcrypto_unregister_skcipher(&aes_cfb64_alg);\n\n\tfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\n\t\tcrypto_unregister_skcipher(&aes_algs[i]);\n}\n\nstatic void atmel_aes_crypto_alg_init(struct crypto_alg *alg)\n{\n\talg->cra_flags |= CRYPTO_ALG_ASYNC;\n\talg->cra_alignmask = 0xf;\n\talg->cra_priority = ATMEL_AES_PRIORITY;\n\talg->cra_module = THIS_MODULE;\n}\n\nstatic int atmel_aes_register_algs(struct atmel_aes_dev *dd)\n{\n\tint err, i, j;\n\n\tfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\n\t\tatmel_aes_crypto_alg_init(&aes_algs[i].base);\n\n\t\terr = crypto_register_skcipher(&aes_algs[i]);\n\t\tif (err)\n\t\t\tgoto err_aes_algs;\n\t}\n\n\tif (dd->caps.has_cfb64) {\n\t\tatmel_aes_crypto_alg_init(&aes_cfb64_alg.base);\n\n\t\terr = crypto_register_skcipher(&aes_cfb64_alg);\n\t\tif (err)\n\t\t\tgoto err_aes_cfb64_alg;\n\t}\n\n\tif (dd->caps.has_gcm) {\n\t\tatmel_aes_crypto_alg_init(&aes_gcm_alg.base);\n\n\t\terr = crypto_register_aead(&aes_gcm_alg);\n\t\tif (err)\n\t\t\tgoto err_aes_gcm_alg;\n\t}\n\n\tif (dd->caps.has_xts) {\n\t\tatmel_aes_crypto_alg_init(&aes_xts_alg.base);\n\n\t\terr = crypto_register_skcipher(&aes_xts_alg);\n\t\tif (err)\n\t\t\tgoto err_aes_xts_alg;\n\t}\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n\tif (dd->caps.has_authenc) {\n\t\tfor (i = 0; i < ARRAY_SIZE(aes_authenc_algs); i++) {\n\t\t\tatmel_aes_crypto_alg_init(&aes_authenc_algs[i].base);\n\n\t\t\terr = crypto_register_aead(&aes_authenc_algs[i]);\n\t\t\tif (err)\n\t\t\t\tgoto err_aes_authenc_alg;\n\t\t}\n\t}\n#endif\n\n\treturn 0;\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n\t \nerr_aes_authenc_alg:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_aead(&aes_authenc_algs[j]);\n\tcrypto_unregister_skcipher(&aes_xts_alg);\n#endif\nerr_aes_xts_alg:\n\tcrypto_unregister_aead(&aes_gcm_alg);\nerr_aes_gcm_alg:\n\tcrypto_unregister_skcipher(&aes_cfb64_alg);\nerr_aes_cfb64_alg:\n\ti = ARRAY_SIZE(aes_algs);\nerr_aes_algs:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_skcipher(&aes_algs[j]);\n\n\treturn err;\n}\n\nstatic void atmel_aes_get_cap(struct atmel_aes_dev *dd)\n{\n\tdd->caps.has_dualbuff = 0;\n\tdd->caps.has_cfb64 = 0;\n\tdd->caps.has_gcm = 0;\n\tdd->caps.has_xts = 0;\n\tdd->caps.has_authenc = 0;\n\tdd->caps.max_burst_size = 1;\n\n\t \n\tswitch (dd->hw_version & 0xff0) {\n\tcase 0x700:\n\tcase 0x600:\n\tcase 0x500:\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_cfb64 = 1;\n\t\tdd->caps.has_gcm = 1;\n\t\tdd->caps.has_xts = 1;\n\t\tdd->caps.has_authenc = 1;\n\t\tdd->caps.max_burst_size = 4;\n\t\tbreak;\n\tcase 0x200:\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_cfb64 = 1;\n\t\tdd->caps.has_gcm = 1;\n\t\tdd->caps.max_burst_size = 4;\n\t\tbreak;\n\tcase 0x130:\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_cfb64 = 1;\n\t\tdd->caps.max_burst_size = 4;\n\t\tbreak;\n\tcase 0x120:\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(dd->dev,\n\t\t\t\t\"Unmanaged aes version, set minimum capabilities\\n\");\n\t\tbreak;\n\t}\n}\n\nstatic const struct of_device_id atmel_aes_dt_ids[] = {\n\t{ .compatible = \"atmel,at91sam9g46-aes\" },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, atmel_aes_dt_ids);\n\nstatic int atmel_aes_probe(struct platform_device *pdev)\n{\n\tstruct atmel_aes_dev *aes_dd;\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *aes_res;\n\tint err;\n\n\taes_dd = devm_kzalloc(&pdev->dev, sizeof(*aes_dd), GFP_KERNEL);\n\tif (!aes_dd)\n\t\treturn -ENOMEM;\n\n\taes_dd->dev = dev;\n\n\tplatform_set_drvdata(pdev, aes_dd);\n\n\tINIT_LIST_HEAD(&aes_dd->list);\n\tspin_lock_init(&aes_dd->lock);\n\n\ttasklet_init(&aes_dd->done_task, atmel_aes_done_task,\n\t\t\t\t\t(unsigned long)aes_dd);\n\ttasklet_init(&aes_dd->queue_task, atmel_aes_queue_task,\n\t\t\t\t\t(unsigned long)aes_dd);\n\n\tcrypto_init_queue(&aes_dd->queue, ATMEL_AES_QUEUE_LENGTH);\n\n\taes_dd->io_base = devm_platform_get_and_ioremap_resource(pdev, 0, &aes_res);\n\tif (IS_ERR(aes_dd->io_base)) {\n\t\terr = PTR_ERR(aes_dd->io_base);\n\t\tgoto err_tasklet_kill;\n\t}\n\taes_dd->phys_base = aes_res->start;\n\n\t \n\taes_dd->irq = platform_get_irq(pdev,  0);\n\tif (aes_dd->irq < 0) {\n\t\terr = aes_dd->irq;\n\t\tgoto err_tasklet_kill;\n\t}\n\n\terr = devm_request_irq(&pdev->dev, aes_dd->irq, atmel_aes_irq,\n\t\t\t       IRQF_SHARED, \"atmel-aes\", aes_dd);\n\tif (err) {\n\t\tdev_err(dev, \"unable to request aes irq.\\n\");\n\t\tgoto err_tasklet_kill;\n\t}\n\n\t \n\taes_dd->iclk = devm_clk_get(&pdev->dev, \"aes_clk\");\n\tif (IS_ERR(aes_dd->iclk)) {\n\t\tdev_err(dev, \"clock initialization failed.\\n\");\n\t\terr = PTR_ERR(aes_dd->iclk);\n\t\tgoto err_tasklet_kill;\n\t}\n\n\terr = clk_prepare(aes_dd->iclk);\n\tif (err)\n\t\tgoto err_tasklet_kill;\n\n\terr = atmel_aes_hw_version_init(aes_dd);\n\tif (err)\n\t\tgoto err_iclk_unprepare;\n\n\tatmel_aes_get_cap(aes_dd);\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n\tif (aes_dd->caps.has_authenc && !atmel_sha_authenc_is_ready()) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto err_iclk_unprepare;\n\t}\n#endif\n\n\terr = atmel_aes_buff_init(aes_dd);\n\tif (err)\n\t\tgoto err_iclk_unprepare;\n\n\terr = atmel_aes_dma_init(aes_dd);\n\tif (err)\n\t\tgoto err_buff_cleanup;\n\n\tspin_lock(&atmel_aes.lock);\n\tlist_add_tail(&aes_dd->list, &atmel_aes.dev_list);\n\tspin_unlock(&atmel_aes.lock);\n\n\terr = atmel_aes_register_algs(aes_dd);\n\tif (err)\n\t\tgoto err_algs;\n\n\tdev_info(dev, \"Atmel AES - Using %s, %s for DMA transfers\\n\",\n\t\t\tdma_chan_name(aes_dd->src.chan),\n\t\t\tdma_chan_name(aes_dd->dst.chan));\n\n\treturn 0;\n\nerr_algs:\n\tspin_lock(&atmel_aes.lock);\n\tlist_del(&aes_dd->list);\n\tspin_unlock(&atmel_aes.lock);\n\tatmel_aes_dma_cleanup(aes_dd);\nerr_buff_cleanup:\n\tatmel_aes_buff_cleanup(aes_dd);\nerr_iclk_unprepare:\n\tclk_unprepare(aes_dd->iclk);\nerr_tasklet_kill:\n\ttasklet_kill(&aes_dd->done_task);\n\ttasklet_kill(&aes_dd->queue_task);\n\n\treturn err;\n}\n\nstatic int atmel_aes_remove(struct platform_device *pdev)\n{\n\tstruct atmel_aes_dev *aes_dd;\n\n\taes_dd = platform_get_drvdata(pdev);\n\n\tspin_lock(&atmel_aes.lock);\n\tlist_del(&aes_dd->list);\n\tspin_unlock(&atmel_aes.lock);\n\n\tatmel_aes_unregister_algs(aes_dd);\n\n\ttasklet_kill(&aes_dd->done_task);\n\ttasklet_kill(&aes_dd->queue_task);\n\n\tatmel_aes_dma_cleanup(aes_dd);\n\tatmel_aes_buff_cleanup(aes_dd);\n\n\tclk_unprepare(aes_dd->iclk);\n\n\treturn 0;\n}\n\nstatic struct platform_driver atmel_aes_driver = {\n\t.probe\t\t= atmel_aes_probe,\n\t.remove\t\t= atmel_aes_remove,\n\t.driver\t\t= {\n\t\t.name\t= \"atmel_aes\",\n\t\t.of_match_table = atmel_aes_dt_ids,\n\t},\n};\n\nmodule_platform_driver(atmel_aes_driver);\n\nMODULE_DESCRIPTION(\"Atmel AES hw acceleration support.\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Nicolas Royer - Eukr\u00e9a Electromatique\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}