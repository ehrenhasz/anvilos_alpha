{
  "module_name": "rk3288_crypto_ahash.c",
  "hash_id": "48a361c0e5f2d5533357094f20a83d78d50e6ac99e619996c1563a5dd4c03a1a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/rockchip/rk3288_crypto_ahash.c",
  "human_readable_source": "\n \n\n#include <asm/unaligned.h>\n#include <crypto/internal/hash.h>\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/iopoll.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/string.h>\n#include \"rk3288_crypto.h\"\n\n \n\nstatic bool rk_ahash_need_fallback(struct ahash_request *req)\n{\n\tstruct scatterlist *sg;\n\n\tsg = req->src;\n\twhile (sg) {\n\t\tif (!IS_ALIGNED(sg->offset, sizeof(u32))) {\n\t\t\treturn true;\n\t\t}\n\t\tif (sg->length % 4) {\n\t\t\treturn true;\n\t\t}\n\t\tsg = sg_next(sg);\n\t}\n\treturn false;\n}\n\nstatic int rk_ahash_digest_fb(struct ahash_request *areq)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(areq);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct rk_ahash_ctx *tfmctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_alg *alg = crypto_ahash_alg(tfm);\n\tstruct rk_crypto_tmp *algt = container_of(alg, struct rk_crypto_tmp, alg.hash.base);\n\n\talgt->stat_fb++;\n\n\tahash_request_set_tfm(&rctx->fallback_req, tfmctx->fallback_tfm);\n\trctx->fallback_req.base.flags = areq->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\n\trctx->fallback_req.nbytes = areq->nbytes;\n\trctx->fallback_req.src = areq->src;\n\trctx->fallback_req.result = areq->result;\n\n\treturn crypto_ahash_digest(&rctx->fallback_req);\n}\n\nstatic int zero_message_process(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tint rk_digest_size = crypto_ahash_digestsize(tfm);\n\n\tswitch (rk_digest_size) {\n\tcase SHA1_DIGEST_SIZE:\n\t\tmemcpy(req->result, sha1_zero_message_hash, rk_digest_size);\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tmemcpy(req->result, sha256_zero_message_hash, rk_digest_size);\n\t\tbreak;\n\tcase MD5_DIGEST_SIZE:\n\t\tmemcpy(req->result, md5_zero_message_hash, rk_digest_size);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void rk_ahash_reg_init(struct ahash_request *req,\n\t\t\t      struct rk_crypto_info *dev)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tint reg_status;\n\n\treg_status = CRYPTO_READ(dev, RK_CRYPTO_CTRL) |\n\t\t     RK_CRYPTO_HASH_FLUSH | _SBF(0xffff, 16);\n\tCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, reg_status);\n\n\treg_status = CRYPTO_READ(dev, RK_CRYPTO_CTRL);\n\treg_status &= (~RK_CRYPTO_HASH_FLUSH);\n\treg_status |= _SBF(0xffff, 16);\n\tCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, reg_status);\n\n\tmemset_io(dev->reg + RK_CRYPTO_HASH_DOUT_0, 0, 32);\n\n\tCRYPTO_WRITE(dev, RK_CRYPTO_INTENA, RK_CRYPTO_HRDMA_ERR_ENA |\n\t\t\t\t\t    RK_CRYPTO_HRDMA_DONE_ENA);\n\n\tCRYPTO_WRITE(dev, RK_CRYPTO_INTSTS, RK_CRYPTO_HRDMA_ERR_INT |\n\t\t\t\t\t    RK_CRYPTO_HRDMA_DONE_INT);\n\n\tCRYPTO_WRITE(dev, RK_CRYPTO_HASH_CTRL, rctx->mode |\n\t\t\t\t\t       RK_CRYPTO_HASH_SWAP_DO);\n\n\tCRYPTO_WRITE(dev, RK_CRYPTO_CONF, RK_CRYPTO_BYTESWAP_HRFIFO |\n\t\t\t\t\t  RK_CRYPTO_BYTESWAP_BRFIFO |\n\t\t\t\t\t  RK_CRYPTO_BYTESWAP_BTFIFO);\n\n\tCRYPTO_WRITE(dev, RK_CRYPTO_HASH_MSG_LEN, req->nbytes);\n}\n\nstatic int rk_ahash_init(struct ahash_request *req)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_init(&rctx->fallback_req);\n}\n\nstatic int rk_ahash_update(struct ahash_request *req)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\n\treturn crypto_ahash_update(&rctx->fallback_req);\n}\n\nstatic int rk_ahash_final(struct ahash_request *req)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_final(&rctx->fallback_req);\n}\n\nstatic int rk_ahash_finup(struct ahash_request *req)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_finup(&rctx->fallback_req);\n}\n\nstatic int rk_ahash_import(struct ahash_request *req, const void *in)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_import(&rctx->fallback_req, in);\n}\n\nstatic int rk_ahash_export(struct ahash_request *req, void *out)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags &\n\t\t\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_export(&rctx->fallback_req, out);\n}\n\nstatic int rk_ahash_digest(struct ahash_request *req)\n{\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\n\tstruct rk_crypto_info *dev;\n\tstruct crypto_engine *engine;\n\n\tif (rk_ahash_need_fallback(req))\n\t\treturn rk_ahash_digest_fb(req);\n\n\tif (!req->nbytes)\n\t\treturn zero_message_process(req);\n\n\tdev = get_rk_crypto();\n\n\trctx->dev = dev;\n\tengine = dev->engine;\n\n\treturn crypto_transfer_hash_request_to_engine(engine, req);\n}\n\nstatic void crypto_ahash_dma_start(struct rk_crypto_info *dev, struct scatterlist *sg)\n{\n\tCRYPTO_WRITE(dev, RK_CRYPTO_HRDMAS, sg_dma_address(sg));\n\tCRYPTO_WRITE(dev, RK_CRYPTO_HRDMAL, sg_dma_len(sg) / 4);\n\tCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, RK_CRYPTO_HASH_START |\n\t\t\t\t\t  (RK_CRYPTO_HASH_START << 16));\n}\n\nstatic int rk_hash_prepare(struct crypto_engine *engine, void *breq)\n{\n\tstruct ahash_request *areq = container_of(breq, struct ahash_request, base);\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(areq);\n\tstruct rk_crypto_info *rkc = rctx->dev;\n\tint ret;\n\n\tret = dma_map_sg(rkc->dev, areq->src, sg_nents(areq->src), DMA_TO_DEVICE);\n\tif (ret <= 0)\n\t\treturn -EINVAL;\n\n\trctx->nrsg = ret;\n\n\treturn 0;\n}\n\nstatic void rk_hash_unprepare(struct crypto_engine *engine, void *breq)\n{\n\tstruct ahash_request *areq = container_of(breq, struct ahash_request, base);\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(areq);\n\tstruct rk_crypto_info *rkc = rctx->dev;\n\n\tdma_unmap_sg(rkc->dev, areq->src, rctx->nrsg, DMA_TO_DEVICE);\n}\n\nstatic int rk_hash_run(struct crypto_engine *engine, void *breq)\n{\n\tstruct ahash_request *areq = container_of(breq, struct ahash_request, base);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct rk_ahash_rctx *rctx = ahash_request_ctx(areq);\n\tstruct ahash_alg *alg = crypto_ahash_alg(tfm);\n\tstruct rk_crypto_tmp *algt = container_of(alg, struct rk_crypto_tmp, alg.hash.base);\n\tstruct scatterlist *sg = areq->src;\n\tstruct rk_crypto_info *rkc = rctx->dev;\n\tint err;\n\tint i;\n\tu32 v;\n\n\terr = pm_runtime_resume_and_get(rkc->dev);\n\tif (err)\n\t\treturn err;\n\n\terr = rk_hash_prepare(engine, breq);\n\tif (err)\n\t\tgoto theend;\n\n\trctx->mode = 0;\n\n\talgt->stat_req++;\n\trkc->nreq++;\n\n\tswitch (crypto_ahash_digestsize(tfm)) {\n\tcase SHA1_DIGEST_SIZE:\n\t\trctx->mode = RK_CRYPTO_HASH_SHA1;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\trctx->mode = RK_CRYPTO_HASH_SHA256;\n\t\tbreak;\n\tcase MD5_DIGEST_SIZE:\n\t\trctx->mode = RK_CRYPTO_HASH_MD5;\n\t\tbreak;\n\tdefault:\n\t\terr =  -EINVAL;\n\t\tgoto theend;\n\t}\n\n\trk_ahash_reg_init(areq, rkc);\n\n\twhile (sg) {\n\t\treinit_completion(&rkc->complete);\n\t\trkc->status = 0;\n\t\tcrypto_ahash_dma_start(rkc, sg);\n\t\twait_for_completion_interruptible_timeout(&rkc->complete,\n\t\t\t\t\t\t\t  msecs_to_jiffies(2000));\n\t\tif (!rkc->status) {\n\t\t\tdev_err(rkc->dev, \"DMA timeout\\n\");\n\t\t\terr = -EFAULT;\n\t\t\tgoto theend;\n\t\t}\n\t\tsg = sg_next(sg);\n\t}\n\n\t \n\treadl_poll_timeout(rkc->reg + RK_CRYPTO_HASH_STS, v, v == 0, 10, 1000);\n\n\tfor (i = 0; i < crypto_ahash_digestsize(tfm) / 4; i++) {\n\t\tv = readl(rkc->reg + RK_CRYPTO_HASH_DOUT_0 + i * 4);\n\t\tput_unaligned_le32(v, areq->result + i * 4);\n\t}\n\ntheend:\n\tpm_runtime_put_autosuspend(rkc->dev);\n\n\tlocal_bh_disable();\n\tcrypto_finalize_hash_request(engine, breq, err);\n\tlocal_bh_enable();\n\n\trk_hash_unprepare(engine, breq);\n\n\treturn 0;\n}\n\nstatic int rk_hash_init_tfm(struct crypto_ahash *tfm)\n{\n\tstruct rk_ahash_ctx *tctx = crypto_ahash_ctx(tfm);\n\tconst char *alg_name = crypto_ahash_alg_name(tfm);\n\tstruct ahash_alg *alg = crypto_ahash_alg(tfm);\n\tstruct rk_crypto_tmp *algt = container_of(alg, struct rk_crypto_tmp, alg.hash.base);\n\n\t \n\ttctx->fallback_tfm = crypto_alloc_ahash(alg_name, 0,\n\t\t\t\t\t\tCRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(tctx->fallback_tfm)) {\n\t\tdev_err(algt->dev->dev, \"Could not load fallback driver.\\n\");\n\t\treturn PTR_ERR(tctx->fallback_tfm);\n\t}\n\n\tcrypto_ahash_set_reqsize(tfm,\n\t\t\t\t sizeof(struct rk_ahash_rctx) +\n\t\t\t\t crypto_ahash_reqsize(tctx->fallback_tfm));\n\n\treturn 0;\n}\n\nstatic void rk_hash_exit_tfm(struct crypto_ahash *tfm)\n{\n\tstruct rk_ahash_ctx *tctx = crypto_ahash_ctx(tfm);\n\n\tcrypto_free_ahash(tctx->fallback_tfm);\n}\n\nstruct rk_crypto_tmp rk_ahash_sha1 = {\n\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t.alg.hash.base = {\n\t\t.init = rk_ahash_init,\n\t\t.update = rk_ahash_update,\n\t\t.final = rk_ahash_final,\n\t\t.finup = rk_ahash_finup,\n\t\t.export = rk_ahash_export,\n\t\t.import = rk_ahash_import,\n\t\t.digest = rk_ahash_digest,\n\t\t.init_tfm = rk_hash_init_tfm,\n\t\t.exit_tfm = rk_hash_exit_tfm,\n\t\t.halg = {\n\t\t\t .digestsize = SHA1_DIGEST_SIZE,\n\t\t\t .statesize = sizeof(struct sha1_state),\n\t\t\t .base = {\n\t\t\t\t  .cra_name = \"sha1\",\n\t\t\t\t  .cra_driver_name = \"rk-sha1\",\n\t\t\t\t  .cra_priority = 300,\n\t\t\t\t  .cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t       CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t  .cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t\t  .cra_ctxsize = sizeof(struct rk_ahash_ctx),\n\t\t\t\t  .cra_alignmask = 3,\n\t\t\t\t  .cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t.alg.hash.op = {\n\t\t.do_one_request = rk_hash_run,\n\t},\n};\n\nstruct rk_crypto_tmp rk_ahash_sha256 = {\n\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t.alg.hash.base = {\n\t\t.init = rk_ahash_init,\n\t\t.update = rk_ahash_update,\n\t\t.final = rk_ahash_final,\n\t\t.finup = rk_ahash_finup,\n\t\t.export = rk_ahash_export,\n\t\t.import = rk_ahash_import,\n\t\t.digest = rk_ahash_digest,\n\t\t.init_tfm = rk_hash_init_tfm,\n\t\t.exit_tfm = rk_hash_exit_tfm,\n\t\t.halg = {\n\t\t\t .digestsize = SHA256_DIGEST_SIZE,\n\t\t\t .statesize = sizeof(struct sha256_state),\n\t\t\t .base = {\n\t\t\t\t  .cra_name = \"sha256\",\n\t\t\t\t  .cra_driver_name = \"rk-sha256\",\n\t\t\t\t  .cra_priority = 300,\n\t\t\t\t  .cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t       CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t  .cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t\t  .cra_ctxsize = sizeof(struct rk_ahash_ctx),\n\t\t\t\t  .cra_alignmask = 3,\n\t\t\t\t  .cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t.alg.hash.op = {\n\t\t.do_one_request = rk_hash_run,\n\t},\n};\n\nstruct rk_crypto_tmp rk_ahash_md5 = {\n\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t.alg.hash.base = {\n\t\t.init = rk_ahash_init,\n\t\t.update = rk_ahash_update,\n\t\t.final = rk_ahash_final,\n\t\t.finup = rk_ahash_finup,\n\t\t.export = rk_ahash_export,\n\t\t.import = rk_ahash_import,\n\t\t.digest = rk_ahash_digest,\n\t\t.init_tfm = rk_hash_init_tfm,\n\t\t.exit_tfm = rk_hash_exit_tfm,\n\t\t.halg = {\n\t\t\t .digestsize = MD5_DIGEST_SIZE,\n\t\t\t .statesize = sizeof(struct md5_state),\n\t\t\t .base = {\n\t\t\t\t  .cra_name = \"md5\",\n\t\t\t\t  .cra_driver_name = \"rk-md5\",\n\t\t\t\t  .cra_priority = 300,\n\t\t\t\t  .cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t       CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t  .cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t\t  .cra_ctxsize = sizeof(struct rk_ahash_ctx),\n\t\t\t\t  .cra_alignmask = 3,\n\t\t\t\t  .cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t.alg.hash.op = {\n\t\t.do_one_request = rk_hash_run,\n\t},\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}