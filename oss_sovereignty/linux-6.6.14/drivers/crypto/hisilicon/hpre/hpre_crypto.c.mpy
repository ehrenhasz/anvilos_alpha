{
  "module_name": "hpre_crypto.c",
  "hash_id": "a8e8799cc4f9d0098c81c4a82be74e0ed03831206c9a2c61812b7ddc17ed905f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/hisilicon/hpre/hpre_crypto.c",
  "human_readable_source": "\n \n#include <crypto/akcipher.h>\n#include <crypto/curve25519.h>\n#include <crypto/dh.h>\n#include <crypto/ecc_curve.h>\n#include <crypto/ecdh.h>\n#include <crypto/rng.h>\n#include <crypto/internal/akcipher.h>\n#include <crypto/internal/kpp.h>\n#include <crypto/internal/rsa.h>\n#include <crypto/kpp.h>\n#include <crypto/scatterwalk.h>\n#include <linux/dma-mapping.h>\n#include <linux/fips.h>\n#include <linux/module.h>\n#include <linux/time.h>\n#include \"hpre.h\"\n\nstruct hpre_ctx;\n\n#define HPRE_CRYPTO_ALG_PRI\t1000\n#define HPRE_ALIGN_SZ\t\t64\n#define HPRE_BITS_2_BYTES_SHIFT\t3\n#define HPRE_RSA_512BITS_KSZ\t64\n#define HPRE_RSA_1536BITS_KSZ\t192\n#define HPRE_CRT_PRMS\t\t5\n#define HPRE_CRT_Q\t\t2\n#define HPRE_CRT_P\t\t3\n#define HPRE_CRT_INV\t\t4\n#define HPRE_DH_G_FLAG\t\t0x02\n#define HPRE_TRY_SEND_TIMES\t100\n#define HPRE_INVLD_REQ_ID\t\t(-1)\n\n#define HPRE_SQE_ALG_BITS\t5\n#define HPRE_SQE_DONE_SHIFT\t30\n#define HPRE_DH_MAX_P_SZ\t512\n\n#define HPRE_DFX_SEC_TO_US\t1000000\n#define HPRE_DFX_US_TO_NS\t1000\n\n \n#define HPRE_ECC_MAX_KSZ\t66\n\n \n#define HPRE_ECC_NIST_P192_N_SIZE\t24\n#define HPRE_ECC_NIST_P256_N_SIZE\t32\n#define HPRE_ECC_NIST_P384_N_SIZE\t48\n\n \n#define HPRE_ECC_HW256_KSZ_B\t32\n#define HPRE_ECC_HW384_KSZ_B\t48\n\n \n#define HPRE_DRV_RSA_MASK_CAP\t\tBIT(0)\n#define HPRE_DRV_DH_MASK_CAP\t\tBIT(1)\n#define HPRE_DRV_ECDH_MASK_CAP\t\tBIT(2)\n#define HPRE_DRV_X25519_MASK_CAP\tBIT(5)\n\ntypedef void (*hpre_cb)(struct hpre_ctx *ctx, void *sqe);\n\nstruct hpre_rsa_ctx {\n\t \n\tchar *pubkey;\n\tdma_addr_t dma_pubkey;\n\n\t \n\tchar *prikey;\n\tdma_addr_t dma_prikey;\n\n\t \n\tchar *crt_prikey;\n\tdma_addr_t dma_crt_prikey;\n\n\tstruct crypto_akcipher *soft_tfm;\n};\n\nstruct hpre_dh_ctx {\n\t \n\tchar *xa_p;\n\tdma_addr_t dma_xa_p;\n\n\tchar *g;  \n\tdma_addr_t dma_g;\n};\n\nstruct hpre_ecdh_ctx {\n\t \n\tunsigned char *p;\n\tdma_addr_t dma_p;\n\n\t \n\tunsigned char *g;\n\tdma_addr_t dma_g;\n};\n\nstruct hpre_curve25519_ctx {\n\t \n\tunsigned char *p;\n\tdma_addr_t dma_p;\n\n\t \n\tunsigned char *g;\n\tdma_addr_t dma_g;\n};\n\nstruct hpre_ctx {\n\tstruct hisi_qp *qp;\n\tstruct device *dev;\n\tstruct hpre_asym_request **req_list;\n\tstruct hpre *hpre;\n\tspinlock_t req_lock;\n\tunsigned int key_sz;\n\tbool crt_g2_mode;\n\tstruct idr req_idr;\n\tunion {\n\t\tstruct hpre_rsa_ctx rsa;\n\t\tstruct hpre_dh_ctx dh;\n\t\tstruct hpre_ecdh_ctx ecdh;\n\t\tstruct hpre_curve25519_ctx curve25519;\n\t};\n\t \n\tunsigned int curve_id;\n};\n\nstruct hpre_asym_request {\n\tchar *src;\n\tchar *dst;\n\tstruct hpre_sqe req;\n\tstruct hpre_ctx *ctx;\n\tunion {\n\t\tstruct akcipher_request *rsa;\n\t\tstruct kpp_request *dh;\n\t\tstruct kpp_request *ecdh;\n\t\tstruct kpp_request *curve25519;\n\t} areq;\n\tint err;\n\tint req_id;\n\thpre_cb cb;\n\tstruct timespec64 req_time;\n};\n\nstatic inline unsigned int hpre_align_sz(void)\n{\n\treturn ((crypto_dma_align() - 1) | (HPRE_ALIGN_SZ - 1)) + 1;\n}\n\nstatic inline unsigned int hpre_align_pd(void)\n{\n\treturn (hpre_align_sz() - 1) & ~(crypto_tfm_ctx_alignment() - 1);\n}\n\nstatic int hpre_alloc_req_id(struct hpre_ctx *ctx)\n{\n\tunsigned long flags;\n\tint id;\n\n\tspin_lock_irqsave(&ctx->req_lock, flags);\n\tid = idr_alloc(&ctx->req_idr, NULL, 0, ctx->qp->sq_depth, GFP_ATOMIC);\n\tspin_unlock_irqrestore(&ctx->req_lock, flags);\n\n\treturn id;\n}\n\nstatic void hpre_free_req_id(struct hpre_ctx *ctx, int req_id)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->req_lock, flags);\n\tidr_remove(&ctx->req_idr, req_id);\n\tspin_unlock_irqrestore(&ctx->req_lock, flags);\n}\n\nstatic int hpre_add_req_to_ctx(struct hpre_asym_request *hpre_req)\n{\n\tstruct hpre_ctx *ctx;\n\tstruct hpre_dfx *dfx;\n\tint id;\n\n\tctx = hpre_req->ctx;\n\tid = hpre_alloc_req_id(ctx);\n\tif (unlikely(id < 0))\n\t\treturn -EINVAL;\n\n\tctx->req_list[id] = hpre_req;\n\thpre_req->req_id = id;\n\n\tdfx = ctx->hpre->debug.dfx;\n\tif (atomic64_read(&dfx[HPRE_OVERTIME_THRHLD].value))\n\t\tktime_get_ts64(&hpre_req->req_time);\n\n\treturn id;\n}\n\nstatic void hpre_rm_req_from_ctx(struct hpre_asym_request *hpre_req)\n{\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tint id = hpre_req->req_id;\n\n\tif (hpre_req->req_id >= 0) {\n\t\thpre_req->req_id = HPRE_INVLD_REQ_ID;\n\t\tctx->req_list[id] = NULL;\n\t\thpre_free_req_id(ctx, id);\n\t}\n}\n\nstatic struct hisi_qp *hpre_get_qp_and_start(u8 type)\n{\n\tstruct hisi_qp *qp;\n\tint ret;\n\n\tqp = hpre_create_qp(type);\n\tif (!qp) {\n\t\tpr_err(\"Can not create hpre qp!\\n\");\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tret = hisi_qm_start_qp(qp, 0);\n\tif (ret < 0) {\n\t\thisi_qm_free_qps(&qp, 1);\n\t\tpci_err(qp->qm->pdev, \"Can not start qp!\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn qp;\n}\n\nstatic int hpre_get_data_dma_addr(struct hpre_asym_request *hpre_req,\n\t\t\t\t  struct scatterlist *data, unsigned int len,\n\t\t\t\t  int is_src, dma_addr_t *tmp)\n{\n\tstruct device *dev = hpre_req->ctx->dev;\n\tenum dma_data_direction dma_dir;\n\n\tif (is_src) {\n\t\thpre_req->src = NULL;\n\t\tdma_dir = DMA_TO_DEVICE;\n\t} else {\n\t\thpre_req->dst = NULL;\n\t\tdma_dir = DMA_FROM_DEVICE;\n\t}\n\t*tmp = dma_map_single(dev, sg_virt(data), len, dma_dir);\n\tif (unlikely(dma_mapping_error(dev, *tmp))) {\n\t\tdev_err(dev, \"dma map data err!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int hpre_prepare_dma_buf(struct hpre_asym_request *hpre_req,\n\t\t\t\tstruct scatterlist *data, unsigned int len,\n\t\t\t\tint is_src, dma_addr_t *tmp)\n{\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tstruct device *dev = ctx->dev;\n\tvoid *ptr;\n\tint shift;\n\n\tshift = ctx->key_sz - len;\n\tif (unlikely(shift < 0))\n\t\treturn -EINVAL;\n\n\tptr = dma_alloc_coherent(dev, ctx->key_sz, tmp, GFP_ATOMIC);\n\tif (unlikely(!ptr))\n\t\treturn -ENOMEM;\n\n\tif (is_src) {\n\t\tscatterwalk_map_and_copy(ptr + shift, data, 0, len, 0);\n\t\thpre_req->src = ptr;\n\t} else {\n\t\thpre_req->dst = ptr;\n\t}\n\n\treturn 0;\n}\n\nstatic int hpre_hw_data_init(struct hpre_asym_request *hpre_req,\n\t\t\t     struct scatterlist *data, unsigned int len,\n\t\t\t     int is_src, int is_dh)\n{\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tdma_addr_t tmp = 0;\n\tint ret;\n\n\t \n\tif ((sg_is_last(data) && len == ctx->key_sz) &&\n\t    ((is_dh && !is_src) || !is_dh))\n\t\tret = hpre_get_data_dma_addr(hpre_req, data, len, is_src, &tmp);\n\telse\n\t\tret = hpre_prepare_dma_buf(hpre_req, data, len, is_src, &tmp);\n\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (is_src)\n\t\tmsg->in = cpu_to_le64(tmp);\n\telse\n\t\tmsg->out = cpu_to_le64(tmp);\n\n\treturn 0;\n}\n\nstatic void hpre_hw_data_clr_all(struct hpre_ctx *ctx,\n\t\t\t\t struct hpre_asym_request *req,\n\t\t\t\t struct scatterlist *dst,\n\t\t\t\t struct scatterlist *src)\n{\n\tstruct device *dev = ctx->dev;\n\tstruct hpre_sqe *sqe = &req->req;\n\tdma_addr_t tmp;\n\n\ttmp = le64_to_cpu(sqe->in);\n\tif (unlikely(dma_mapping_error(dev, tmp)))\n\t\treturn;\n\n\tif (src) {\n\t\tif (req->src)\n\t\t\tdma_free_coherent(dev, ctx->key_sz, req->src, tmp);\n\t\telse\n\t\t\tdma_unmap_single(dev, tmp, ctx->key_sz, DMA_TO_DEVICE);\n\t}\n\n\ttmp = le64_to_cpu(sqe->out);\n\tif (unlikely(dma_mapping_error(dev, tmp)))\n\t\treturn;\n\n\tif (req->dst) {\n\t\tif (dst)\n\t\t\tscatterwalk_map_and_copy(req->dst, dst, 0,\n\t\t\t\t\t\t ctx->key_sz, 1);\n\t\tdma_free_coherent(dev, ctx->key_sz, req->dst, tmp);\n\t} else {\n\t\tdma_unmap_single(dev, tmp, ctx->key_sz, DMA_FROM_DEVICE);\n\t}\n}\n\nstatic int hpre_alg_res_post_hf(struct hpre_ctx *ctx, struct hpre_sqe *sqe,\n\t\t\t\tvoid **kreq)\n{\n\tstruct hpre_asym_request *req;\n\tunsigned int err, done, alg;\n\tint id;\n\n#define HPRE_NO_HW_ERR\t\t0\n#define HPRE_HW_TASK_DONE\t3\n#define HREE_HW_ERR_MASK\tGENMASK(10, 0)\n#define HREE_SQE_DONE_MASK\tGENMASK(1, 0)\n#define HREE_ALG_TYPE_MASK\tGENMASK(4, 0)\n\tid = (int)le16_to_cpu(sqe->tag);\n\treq = ctx->req_list[id];\n\thpre_rm_req_from_ctx(req);\n\t*kreq = req;\n\n\terr = (le32_to_cpu(sqe->dw0) >> HPRE_SQE_ALG_BITS) &\n\t\tHREE_HW_ERR_MASK;\n\n\tdone = (le32_to_cpu(sqe->dw0) >> HPRE_SQE_DONE_SHIFT) &\n\t\tHREE_SQE_DONE_MASK;\n\n\tif (likely(err == HPRE_NO_HW_ERR && done == HPRE_HW_TASK_DONE))\n\t\treturn 0;\n\n\talg = le32_to_cpu(sqe->dw0) & HREE_ALG_TYPE_MASK;\n\tdev_err_ratelimited(ctx->dev, \"alg[0x%x] error: done[0x%x], etype[0x%x]\\n\",\n\t\talg, done, err);\n\n\treturn -EINVAL;\n}\n\nstatic int hpre_ctx_set(struct hpre_ctx *ctx, struct hisi_qp *qp, int qlen)\n{\n\tstruct hpre *hpre;\n\n\tif (!ctx || !qp || qlen < 0)\n\t\treturn -EINVAL;\n\n\tspin_lock_init(&ctx->req_lock);\n\tctx->qp = qp;\n\tctx->dev = &qp->qm->pdev->dev;\n\n\thpre = container_of(ctx->qp->qm, struct hpre, qm);\n\tctx->hpre = hpre;\n\tctx->req_list = kcalloc(qlen, sizeof(void *), GFP_KERNEL);\n\tif (!ctx->req_list)\n\t\treturn -ENOMEM;\n\tctx->key_sz = 0;\n\tctx->crt_g2_mode = false;\n\tidr_init(&ctx->req_idr);\n\n\treturn 0;\n}\n\nstatic void hpre_ctx_clear(struct hpre_ctx *ctx, bool is_clear_all)\n{\n\tif (is_clear_all) {\n\t\tidr_destroy(&ctx->req_idr);\n\t\tkfree(ctx->req_list);\n\t\thisi_qm_free_qps(&ctx->qp, 1);\n\t}\n\n\tctx->crt_g2_mode = false;\n\tctx->key_sz = 0;\n}\n\nstatic bool hpre_is_bd_timeout(struct hpre_asym_request *req,\n\t\t\t       u64 overtime_thrhld)\n{\n\tstruct timespec64 reply_time;\n\tu64 time_use_us;\n\n\tktime_get_ts64(&reply_time);\n\ttime_use_us = (reply_time.tv_sec - req->req_time.tv_sec) *\n\t\tHPRE_DFX_SEC_TO_US +\n\t\t(reply_time.tv_nsec - req->req_time.tv_nsec) /\n\t\tHPRE_DFX_US_TO_NS;\n\n\tif (time_use_us <= overtime_thrhld)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void hpre_dh_cb(struct hpre_ctx *ctx, void *resp)\n{\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tstruct hpre_asym_request *req;\n\tstruct kpp_request *areq;\n\tu64 overtime_thrhld;\n\tint ret;\n\n\tret = hpre_alg_res_post_hf(ctx, resp, (void **)&req);\n\tareq = req->areq.dh;\n\tareq->dst_len = ctx->key_sz;\n\n\tovertime_thrhld = atomic64_read(&dfx[HPRE_OVERTIME_THRHLD].value);\n\tif (overtime_thrhld && hpre_is_bd_timeout(req, overtime_thrhld))\n\t\tatomic64_inc(&dfx[HPRE_OVER_THRHLD_CNT].value);\n\n\thpre_hw_data_clr_all(ctx, req, areq->dst, areq->src);\n\tkpp_request_complete(areq, ret);\n\tatomic64_inc(&dfx[HPRE_RECV_CNT].value);\n}\n\nstatic void hpre_rsa_cb(struct hpre_ctx *ctx, void *resp)\n{\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tstruct hpre_asym_request *req;\n\tstruct akcipher_request *areq;\n\tu64 overtime_thrhld;\n\tint ret;\n\n\tret = hpre_alg_res_post_hf(ctx, resp, (void **)&req);\n\n\tovertime_thrhld = atomic64_read(&dfx[HPRE_OVERTIME_THRHLD].value);\n\tif (overtime_thrhld && hpre_is_bd_timeout(req, overtime_thrhld))\n\t\tatomic64_inc(&dfx[HPRE_OVER_THRHLD_CNT].value);\n\n\tareq = req->areq.rsa;\n\tareq->dst_len = ctx->key_sz;\n\thpre_hw_data_clr_all(ctx, req, areq->dst, areq->src);\n\takcipher_request_complete(areq, ret);\n\tatomic64_inc(&dfx[HPRE_RECV_CNT].value);\n}\n\nstatic void hpre_alg_cb(struct hisi_qp *qp, void *resp)\n{\n\tstruct hpre_ctx *ctx = qp->qp_ctx;\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tstruct hpre_sqe *sqe = resp;\n\tstruct hpre_asym_request *req = ctx->req_list[le16_to_cpu(sqe->tag)];\n\n\tif (unlikely(!req)) {\n\t\tatomic64_inc(&dfx[HPRE_INVALID_REQ_CNT].value);\n\t\treturn;\n\t}\n\n\treq->cb(ctx, resp);\n}\n\nstatic void hpre_stop_qp_and_put(struct hisi_qp *qp)\n{\n\thisi_qm_stop_qp(qp);\n\thisi_qm_free_qps(&qp, 1);\n}\n\nstatic int hpre_ctx_init(struct hpre_ctx *ctx, u8 type)\n{\n\tstruct hisi_qp *qp;\n\tint ret;\n\n\tqp = hpre_get_qp_and_start(type);\n\tif (IS_ERR(qp))\n\t\treturn PTR_ERR(qp);\n\n\tqp->qp_ctx = ctx;\n\tqp->req_cb = hpre_alg_cb;\n\n\tret = hpre_ctx_set(ctx, qp, qp->sq_depth);\n\tif (ret)\n\t\thpre_stop_qp_and_put(qp);\n\n\treturn ret;\n}\n\nstatic int hpre_msg_request_set(struct hpre_ctx *ctx, void *req, bool is_rsa)\n{\n\tstruct hpre_asym_request *h_req;\n\tstruct hpre_sqe *msg;\n\tint req_id;\n\tvoid *tmp;\n\n\tif (is_rsa) {\n\t\tstruct akcipher_request *akreq = req;\n\n\t\tif (akreq->dst_len < ctx->key_sz) {\n\t\t\takreq->dst_len = ctx->key_sz;\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\n\t\ttmp = akcipher_request_ctx(akreq);\n\t\th_req = PTR_ALIGN(tmp, hpre_align_sz());\n\t\th_req->cb = hpre_rsa_cb;\n\t\th_req->areq.rsa = akreq;\n\t\tmsg = &h_req->req;\n\t\tmemset(msg, 0, sizeof(*msg));\n\t} else {\n\t\tstruct kpp_request *kreq = req;\n\n\t\tif (kreq->dst_len < ctx->key_sz) {\n\t\t\tkreq->dst_len = ctx->key_sz;\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\n\t\ttmp = kpp_request_ctx(kreq);\n\t\th_req = PTR_ALIGN(tmp, hpre_align_sz());\n\t\th_req->cb = hpre_dh_cb;\n\t\th_req->areq.dh = kreq;\n\t\tmsg = &h_req->req;\n\t\tmemset(msg, 0, sizeof(*msg));\n\t\tmsg->key = cpu_to_le64(ctx->dh.dma_xa_p);\n\t}\n\n\tmsg->in = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->out = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->dw0 |= cpu_to_le32(0x1 << HPRE_SQE_DONE_SHIFT);\n\tmsg->task_len1 = (ctx->key_sz >> HPRE_BITS_2_BYTES_SHIFT) - 1;\n\th_req->ctx = ctx;\n\n\treq_id = hpre_add_req_to_ctx(h_req);\n\tif (req_id < 0)\n\t\treturn -EBUSY;\n\n\tmsg->tag = cpu_to_le16((u16)req_id);\n\n\treturn 0;\n}\n\nstatic int hpre_send(struct hpre_ctx *ctx, struct hpre_sqe *msg)\n{\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tint ctr = 0;\n\tint ret;\n\n\tdo {\n\t\tatomic64_inc(&dfx[HPRE_SEND_CNT].value);\n\t\tret = hisi_qp_send(ctx->qp, msg);\n\t\tif (ret != -EBUSY)\n\t\t\tbreak;\n\t\tatomic64_inc(&dfx[HPRE_SEND_BUSY_CNT].value);\n\t} while (ctr++ < HPRE_TRY_SEND_TIMES);\n\n\tif (likely(!ret))\n\t\treturn ret;\n\n\tif (ret != -EBUSY)\n\t\tatomic64_inc(&dfx[HPRE_SEND_FAIL_CNT].value);\n\n\treturn ret;\n}\n\nstatic int hpre_dh_compute_value(struct kpp_request *req)\n{\n\tstruct crypto_kpp *tfm = crypto_kpp_reqtfm(req);\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tvoid *tmp = kpp_request_ctx(req);\n\tstruct hpre_asym_request *hpre_req = PTR_ALIGN(tmp, hpre_align_sz());\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tint ret;\n\n\tret = hpre_msg_request_set(ctx, req, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (req->src) {\n\t\tret = hpre_hw_data_init(hpre_req, req->src, req->src_len, 1, 1);\n\t\tif (unlikely(ret))\n\t\t\tgoto clear_all;\n\t} else {\n\t\tmsg->in = cpu_to_le64(ctx->dh.dma_g);\n\t}\n\n\tret = hpre_hw_data_init(hpre_req, req->dst, req->dst_len, 0, 1);\n\tif (unlikely(ret))\n\t\tgoto clear_all;\n\n\tif (ctx->crt_g2_mode && !req->src)\n\t\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) | HPRE_ALG_DH_G2);\n\telse\n\t\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) | HPRE_ALG_DH);\n\n\t \n\tret = hpre_send(ctx, msg);\n\tif (likely(!ret))\n\t\treturn -EINPROGRESS;\n\nclear_all:\n\thpre_rm_req_from_ctx(hpre_req);\n\thpre_hw_data_clr_all(ctx, hpre_req, req->dst, req->src);\n\n\treturn ret;\n}\n\nstatic int hpre_is_dh_params_length_valid(unsigned int key_sz)\n{\n#define _HPRE_DH_GRP1\t\t768\n#define _HPRE_DH_GRP2\t\t1024\n#define _HPRE_DH_GRP5\t\t1536\n#define _HPRE_DH_GRP14\t\t2048\n#define _HPRE_DH_GRP15\t\t3072\n#define _HPRE_DH_GRP16\t\t4096\n\tswitch (key_sz) {\n\tcase _HPRE_DH_GRP1:\n\tcase _HPRE_DH_GRP2:\n\tcase _HPRE_DH_GRP5:\n\tcase _HPRE_DH_GRP14:\n\tcase _HPRE_DH_GRP15:\n\tcase _HPRE_DH_GRP16:\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int hpre_dh_set_params(struct hpre_ctx *ctx, struct dh *params)\n{\n\tstruct device *dev = ctx->dev;\n\tunsigned int sz;\n\n\tif (params->p_size > HPRE_DH_MAX_P_SZ)\n\t\treturn -EINVAL;\n\n\tif (hpre_is_dh_params_length_valid(params->p_size <<\n\t\t\t\t\t   HPRE_BITS_2_BYTES_SHIFT))\n\t\treturn -EINVAL;\n\n\tsz = ctx->key_sz = params->p_size;\n\tctx->dh.xa_p = dma_alloc_coherent(dev, sz << 1,\n\t\t\t\t\t  &ctx->dh.dma_xa_p, GFP_KERNEL);\n\tif (!ctx->dh.xa_p)\n\t\treturn -ENOMEM;\n\n\tmemcpy(ctx->dh.xa_p + sz, params->p, sz);\n\n\t \n\tif (params->g_size == 1 && *(char *)params->g == HPRE_DH_G_FLAG) {\n\t\tctx->crt_g2_mode = true;\n\t\treturn 0;\n\t}\n\n\tctx->dh.g = dma_alloc_coherent(dev, sz, &ctx->dh.dma_g, GFP_KERNEL);\n\tif (!ctx->dh.g) {\n\t\tdma_free_coherent(dev, sz << 1, ctx->dh.xa_p,\n\t\t\t\t  ctx->dh.dma_xa_p);\n\t\tctx->dh.xa_p = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(ctx->dh.g + (sz - params->g_size), params->g, params->g_size);\n\n\treturn 0;\n}\n\nstatic void hpre_dh_clear_ctx(struct hpre_ctx *ctx, bool is_clear_all)\n{\n\tstruct device *dev = ctx->dev;\n\tunsigned int sz = ctx->key_sz;\n\n\tif (is_clear_all)\n\t\thisi_qm_stop_qp(ctx->qp);\n\n\tif (ctx->dh.g) {\n\t\tdma_free_coherent(dev, sz, ctx->dh.g, ctx->dh.dma_g);\n\t\tctx->dh.g = NULL;\n\t}\n\n\tif (ctx->dh.xa_p) {\n\t\tmemzero_explicit(ctx->dh.xa_p, sz);\n\t\tdma_free_coherent(dev, sz << 1, ctx->dh.xa_p,\n\t\t\t\t  ctx->dh.dma_xa_p);\n\t\tctx->dh.xa_p = NULL;\n\t}\n\n\thpre_ctx_clear(ctx, is_clear_all);\n}\n\nstatic int hpre_dh_set_secret(struct crypto_kpp *tfm, const void *buf,\n\t\t\t      unsigned int len)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct dh params;\n\tint ret;\n\n\tif (crypto_dh_decode_key(buf, len, &params) < 0)\n\t\treturn -EINVAL;\n\n\t \n\thpre_dh_clear_ctx(ctx, false);\n\n\tret = hpre_dh_set_params(ctx, &params);\n\tif (ret < 0)\n\t\tgoto err_clear_ctx;\n\n\tmemcpy(ctx->dh.xa_p + (ctx->key_sz - params.key_size), params.key,\n\t       params.key_size);\n\n\treturn 0;\n\nerr_clear_ctx:\n\thpre_dh_clear_ctx(ctx, false);\n\treturn ret;\n}\n\nstatic unsigned int hpre_dh_max_size(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\treturn ctx->key_sz;\n}\n\nstatic int hpre_dh_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\tkpp_set_reqsize(tfm, sizeof(struct hpre_asym_request) + hpre_align_pd());\n\n\treturn hpre_ctx_init(ctx, HPRE_V2_ALG_TYPE);\n}\n\nstatic void hpre_dh_exit_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\thpre_dh_clear_ctx(ctx, true);\n}\n\nstatic void hpre_rsa_drop_leading_zeros(const char **ptr, size_t *len)\n{\n\twhile (!**ptr && *len) {\n\t\t(*ptr)++;\n\t\t(*len)--;\n\t}\n}\n\nstatic bool hpre_rsa_key_size_is_support(unsigned int len)\n{\n\tunsigned int bits = len << HPRE_BITS_2_BYTES_SHIFT;\n\n#define _RSA_1024BITS_KEY_WDTH\t\t1024\n#define _RSA_2048BITS_KEY_WDTH\t\t2048\n#define _RSA_3072BITS_KEY_WDTH\t\t3072\n#define _RSA_4096BITS_KEY_WDTH\t\t4096\n\n\tswitch (bits) {\n\tcase _RSA_1024BITS_KEY_WDTH:\n\tcase _RSA_2048BITS_KEY_WDTH:\n\tcase _RSA_3072BITS_KEY_WDTH:\n\tcase _RSA_4096BITS_KEY_WDTH:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int hpre_rsa_enc(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tvoid *tmp = akcipher_request_ctx(req);\n\tstruct hpre_asym_request *hpre_req = PTR_ALIGN(tmp, hpre_align_sz());\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tint ret;\n\n\t \n\tif (ctx->key_sz == HPRE_RSA_512BITS_KSZ ||\n\t    ctx->key_sz == HPRE_RSA_1536BITS_KSZ) {\n\t\takcipher_request_set_tfm(req, ctx->rsa.soft_tfm);\n\t\tret = crypto_akcipher_encrypt(req);\n\t\takcipher_request_set_tfm(req, tfm);\n\t\treturn ret;\n\t}\n\n\tif (unlikely(!ctx->rsa.pubkey))\n\t\treturn -EINVAL;\n\n\tret = hpre_msg_request_set(ctx, req, true);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg->dw0 |= cpu_to_le32(HPRE_ALG_NC_NCRT);\n\tmsg->key = cpu_to_le64(ctx->rsa.dma_pubkey);\n\n\tret = hpre_hw_data_init(hpre_req, req->src, req->src_len, 1, 0);\n\tif (unlikely(ret))\n\t\tgoto clear_all;\n\n\tret = hpre_hw_data_init(hpre_req, req->dst, req->dst_len, 0, 0);\n\tif (unlikely(ret))\n\t\tgoto clear_all;\n\n\t \n\tret = hpre_send(ctx, msg);\n\tif (likely(!ret))\n\t\treturn -EINPROGRESS;\n\nclear_all:\n\thpre_rm_req_from_ctx(hpre_req);\n\thpre_hw_data_clr_all(ctx, hpre_req, req->dst, req->src);\n\n\treturn ret;\n}\n\nstatic int hpre_rsa_dec(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tvoid *tmp = akcipher_request_ctx(req);\n\tstruct hpre_asym_request *hpre_req = PTR_ALIGN(tmp, hpre_align_sz());\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tint ret;\n\n\t \n\tif (ctx->key_sz == HPRE_RSA_512BITS_KSZ ||\n\t    ctx->key_sz == HPRE_RSA_1536BITS_KSZ) {\n\t\takcipher_request_set_tfm(req, ctx->rsa.soft_tfm);\n\t\tret = crypto_akcipher_decrypt(req);\n\t\takcipher_request_set_tfm(req, tfm);\n\t\treturn ret;\n\t}\n\n\tif (unlikely(!ctx->rsa.prikey))\n\t\treturn -EINVAL;\n\n\tret = hpre_msg_request_set(ctx, req, true);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (ctx->crt_g2_mode) {\n\t\tmsg->key = cpu_to_le64(ctx->rsa.dma_crt_prikey);\n\t\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) |\n\t\t\t\t       HPRE_ALG_NC_CRT);\n\t} else {\n\t\tmsg->key = cpu_to_le64(ctx->rsa.dma_prikey);\n\t\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) |\n\t\t\t\t       HPRE_ALG_NC_NCRT);\n\t}\n\n\tret = hpre_hw_data_init(hpre_req, req->src, req->src_len, 1, 0);\n\tif (unlikely(ret))\n\t\tgoto clear_all;\n\n\tret = hpre_hw_data_init(hpre_req, req->dst, req->dst_len, 0, 0);\n\tif (unlikely(ret))\n\t\tgoto clear_all;\n\n\t \n\tret = hpre_send(ctx, msg);\n\tif (likely(!ret))\n\t\treturn -EINPROGRESS;\n\nclear_all:\n\thpre_rm_req_from_ctx(hpre_req);\n\thpre_hw_data_clr_all(ctx, hpre_req, req->dst, req->src);\n\n\treturn ret;\n}\n\nstatic int hpre_rsa_set_n(struct hpre_ctx *ctx, const char *value,\n\t\t\t  size_t vlen, bool private)\n{\n\tconst char *ptr = value;\n\n\thpre_rsa_drop_leading_zeros(&ptr, &vlen);\n\n\tctx->key_sz = vlen;\n\n\t \n\tif (!hpre_rsa_key_size_is_support(ctx->key_sz))\n\t\treturn 0;\n\n\tctx->rsa.pubkey = dma_alloc_coherent(ctx->dev, vlen << 1,\n\t\t\t\t\t     &ctx->rsa.dma_pubkey,\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!ctx->rsa.pubkey)\n\t\treturn -ENOMEM;\n\n\tif (private) {\n\t\tctx->rsa.prikey = dma_alloc_coherent(ctx->dev, vlen << 1,\n\t\t\t\t\t\t     &ctx->rsa.dma_prikey,\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!ctx->rsa.prikey) {\n\t\t\tdma_free_coherent(ctx->dev, vlen << 1,\n\t\t\t\t\t  ctx->rsa.pubkey,\n\t\t\t\t\t  ctx->rsa.dma_pubkey);\n\t\t\tctx->rsa.pubkey = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmemcpy(ctx->rsa.prikey + vlen, ptr, vlen);\n\t}\n\tmemcpy(ctx->rsa.pubkey + vlen, ptr, vlen);\n\n\t \n\treturn 1;\n}\n\nstatic int hpre_rsa_set_e(struct hpre_ctx *ctx, const char *value,\n\t\t\t  size_t vlen)\n{\n\tconst char *ptr = value;\n\n\thpre_rsa_drop_leading_zeros(&ptr, &vlen);\n\n\tif (!ctx->key_sz || !vlen || vlen > ctx->key_sz)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->rsa.pubkey + ctx->key_sz - vlen, ptr, vlen);\n\n\treturn 0;\n}\n\nstatic int hpre_rsa_set_d(struct hpre_ctx *ctx, const char *value,\n\t\t\t  size_t vlen)\n{\n\tconst char *ptr = value;\n\n\thpre_rsa_drop_leading_zeros(&ptr, &vlen);\n\n\tif (!ctx->key_sz || !vlen || vlen > ctx->key_sz)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->rsa.prikey + ctx->key_sz - vlen, ptr, vlen);\n\n\treturn 0;\n}\n\nstatic int hpre_crt_para_get(char *para, size_t para_sz,\n\t\t\t     const char *raw, size_t raw_sz)\n{\n\tconst char *ptr = raw;\n\tsize_t len = raw_sz;\n\n\thpre_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len || len > para_sz)\n\t\treturn -EINVAL;\n\n\tmemcpy(para + para_sz - len, ptr, len);\n\n\treturn 0;\n}\n\nstatic int hpre_rsa_setkey_crt(struct hpre_ctx *ctx, struct rsa_key *rsa_key)\n{\n\tunsigned int hlf_ksz = ctx->key_sz >> 1;\n\tstruct device *dev = ctx->dev;\n\tu64 offset;\n\tint ret;\n\n\tctx->rsa.crt_prikey = dma_alloc_coherent(dev, hlf_ksz * HPRE_CRT_PRMS,\n\t\t\t\t\t&ctx->rsa.dma_crt_prikey,\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->rsa.crt_prikey)\n\t\treturn -ENOMEM;\n\n\tret = hpre_crt_para_get(ctx->rsa.crt_prikey, hlf_ksz,\n\t\t\t\trsa_key->dq, rsa_key->dq_sz);\n\tif (ret)\n\t\tgoto free_key;\n\n\toffset = hlf_ksz;\n\tret = hpre_crt_para_get(ctx->rsa.crt_prikey + offset, hlf_ksz,\n\t\t\t\trsa_key->dp, rsa_key->dp_sz);\n\tif (ret)\n\t\tgoto free_key;\n\n\toffset = hlf_ksz * HPRE_CRT_Q;\n\tret = hpre_crt_para_get(ctx->rsa.crt_prikey + offset, hlf_ksz,\n\t\t\t\trsa_key->q, rsa_key->q_sz);\n\tif (ret)\n\t\tgoto free_key;\n\n\toffset = hlf_ksz * HPRE_CRT_P;\n\tret = hpre_crt_para_get(ctx->rsa.crt_prikey + offset, hlf_ksz,\n\t\t\t\trsa_key->p, rsa_key->p_sz);\n\tif (ret)\n\t\tgoto free_key;\n\n\toffset = hlf_ksz * HPRE_CRT_INV;\n\tret = hpre_crt_para_get(ctx->rsa.crt_prikey + offset, hlf_ksz,\n\t\t\t\trsa_key->qinv, rsa_key->qinv_sz);\n\tif (ret)\n\t\tgoto free_key;\n\n\tctx->crt_g2_mode = true;\n\n\treturn 0;\n\nfree_key:\n\toffset = hlf_ksz * HPRE_CRT_PRMS;\n\tmemzero_explicit(ctx->rsa.crt_prikey, offset);\n\tdma_free_coherent(dev, hlf_ksz * HPRE_CRT_PRMS, ctx->rsa.crt_prikey,\n\t\t\t  ctx->rsa.dma_crt_prikey);\n\tctx->rsa.crt_prikey = NULL;\n\tctx->crt_g2_mode = false;\n\n\treturn ret;\n}\n\n \nstatic void hpre_rsa_clear_ctx(struct hpre_ctx *ctx, bool is_clear_all)\n{\n\tunsigned int half_key_sz = ctx->key_sz >> 1;\n\tstruct device *dev = ctx->dev;\n\n\tif (is_clear_all)\n\t\thisi_qm_stop_qp(ctx->qp);\n\n\tif (ctx->rsa.pubkey) {\n\t\tdma_free_coherent(dev, ctx->key_sz << 1,\n\t\t\t\t  ctx->rsa.pubkey, ctx->rsa.dma_pubkey);\n\t\tctx->rsa.pubkey = NULL;\n\t}\n\n\tif (ctx->rsa.crt_prikey) {\n\t\tmemzero_explicit(ctx->rsa.crt_prikey,\n\t\t\t\t half_key_sz * HPRE_CRT_PRMS);\n\t\tdma_free_coherent(dev, half_key_sz * HPRE_CRT_PRMS,\n\t\t\t\t  ctx->rsa.crt_prikey, ctx->rsa.dma_crt_prikey);\n\t\tctx->rsa.crt_prikey = NULL;\n\t}\n\n\tif (ctx->rsa.prikey) {\n\t\tmemzero_explicit(ctx->rsa.prikey, ctx->key_sz);\n\t\tdma_free_coherent(dev, ctx->key_sz << 1, ctx->rsa.prikey,\n\t\t\t\t  ctx->rsa.dma_prikey);\n\t\tctx->rsa.prikey = NULL;\n\t}\n\n\thpre_ctx_clear(ctx, is_clear_all);\n}\n\n \nstatic bool hpre_is_crt_key(struct rsa_key *key)\n{\n\tu16 len = key->p_sz + key->q_sz + key->dp_sz + key->dq_sz +\n\t\t  key->qinv_sz;\n\n#define LEN_OF_NCRT_PARA\t5\n\n\t \n\treturn len > LEN_OF_NCRT_PARA;\n}\n\nstatic int hpre_rsa_setkey(struct hpre_ctx *ctx, const void *key,\n\t\t\t   unsigned int keylen, bool private)\n{\n\tstruct rsa_key rsa_key;\n\tint ret;\n\n\thpre_rsa_clear_ctx(ctx, false);\n\n\tif (private)\n\t\tret = rsa_parse_priv_key(&rsa_key, key, keylen);\n\telse\n\t\tret = rsa_parse_pub_key(&rsa_key, key, keylen);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = hpre_rsa_set_n(ctx, rsa_key.n, rsa_key.n_sz, private);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tif (private) {\n\t\tret = hpre_rsa_set_d(ctx, rsa_key.d, rsa_key.d_sz);\n\t\tif (ret < 0)\n\t\t\tgoto free;\n\n\t\tif (hpre_is_crt_key(&rsa_key)) {\n\t\t\tret = hpre_rsa_setkey_crt(ctx, &rsa_key);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto free;\n\t\t}\n\t}\n\n\tret = hpre_rsa_set_e(ctx, rsa_key.e, rsa_key.e_sz);\n\tif (ret < 0)\n\t\tgoto free;\n\n\tif ((private && !ctx->rsa.prikey) || !ctx->rsa.pubkey) {\n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\n\treturn 0;\n\nfree:\n\thpre_rsa_clear_ctx(ctx, false);\n\treturn ret;\n}\n\nstatic int hpre_rsa_setpubkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t      unsigned int keylen)\n{\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tint ret;\n\n\tret = crypto_akcipher_set_pub_key(ctx->rsa.soft_tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn hpre_rsa_setkey(ctx, key, keylen, false);\n}\n\nstatic int hpre_rsa_setprivkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tint ret;\n\n\tret = crypto_akcipher_set_priv_key(ctx->rsa.soft_tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn hpre_rsa_setkey(ctx, key, keylen, true);\n}\n\nstatic unsigned int hpre_rsa_max_size(struct crypto_akcipher *tfm)\n{\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\n\t \n\tif (ctx->key_sz == HPRE_RSA_512BITS_KSZ ||\n\t    ctx->key_sz == HPRE_RSA_1536BITS_KSZ)\n\t\treturn crypto_akcipher_maxsize(ctx->rsa.soft_tfm);\n\n\treturn ctx->key_sz;\n}\n\nstatic int hpre_rsa_init_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tint ret;\n\n\tctx->rsa.soft_tfm = crypto_alloc_akcipher(\"rsa-generic\", 0, 0);\n\tif (IS_ERR(ctx->rsa.soft_tfm)) {\n\t\tpr_err(\"Can not alloc_akcipher!\\n\");\n\t\treturn PTR_ERR(ctx->rsa.soft_tfm);\n\t}\n\n\takcipher_set_reqsize(tfm, sizeof(struct hpre_asym_request) +\n\t\t\t\t  hpre_align_pd());\n\n\tret = hpre_ctx_init(ctx, HPRE_V2_ALG_TYPE);\n\tif (ret)\n\t\tcrypto_free_akcipher(ctx->rsa.soft_tfm);\n\n\treturn ret;\n}\n\nstatic void hpre_rsa_exit_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct hpre_ctx *ctx = akcipher_tfm_ctx(tfm);\n\n\thpre_rsa_clear_ctx(ctx, true);\n\tcrypto_free_akcipher(ctx->rsa.soft_tfm);\n}\n\nstatic void hpre_key_to_big_end(u8 *data, int len)\n{\n\tint i, j;\n\n\tfor (i = 0; i < len / 2; i++) {\n\t\tj = len - i - 1;\n\t\tswap(data[j], data[i]);\n\t}\n}\n\nstatic void hpre_ecc_clear_ctx(struct hpre_ctx *ctx, bool is_clear_all,\n\t\t\t       bool is_ecdh)\n{\n\tstruct device *dev = ctx->dev;\n\tunsigned int sz = ctx->key_sz;\n\tunsigned int shift = sz << 1;\n\n\tif (is_clear_all)\n\t\thisi_qm_stop_qp(ctx->qp);\n\n\tif (is_ecdh && ctx->ecdh.p) {\n\t\t \n\t\tmemzero_explicit(ctx->ecdh.p + shift, sz);\n\t\tdma_free_coherent(dev, sz << 3, ctx->ecdh.p, ctx->ecdh.dma_p);\n\t\tctx->ecdh.p = NULL;\n\t} else if (!is_ecdh && ctx->curve25519.p) {\n\t\t \n\t\tmemzero_explicit(ctx->curve25519.p + shift, sz);\n\t\tdma_free_coherent(dev, sz << 2, ctx->curve25519.p,\n\t\t\t\t  ctx->curve25519.dma_p);\n\t\tctx->curve25519.p = NULL;\n\t}\n\n\thpre_ctx_clear(ctx, is_clear_all);\n}\n\n \nstatic unsigned int hpre_ecdh_supported_curve(unsigned short id)\n{\n\tswitch (id) {\n\tcase ECC_CURVE_NIST_P192:\n\tcase ECC_CURVE_NIST_P256:\n\t\treturn HPRE_ECC_HW256_KSZ_B;\n\tcase ECC_CURVE_NIST_P384:\n\t\treturn HPRE_ECC_HW384_KSZ_B;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void fill_curve_param(void *addr, u64 *param, unsigned int cur_sz, u8 ndigits)\n{\n\tunsigned int sz = cur_sz - (ndigits - 1) * sizeof(u64);\n\tu8 i = 0;\n\n\twhile (i < ndigits - 1) {\n\t\tmemcpy(addr + sizeof(u64) * i, &param[i], sizeof(u64));\n\t\ti++;\n\t}\n\n\tmemcpy(addr + sizeof(u64) * i, &param[ndigits - 1], sz);\n\thpre_key_to_big_end((u8 *)addr, cur_sz);\n}\n\nstatic int hpre_ecdh_fill_curve(struct hpre_ctx *ctx, struct ecdh *params,\n\t\t\t\tunsigned int cur_sz)\n{\n\tunsigned int shifta = ctx->key_sz << 1;\n\tunsigned int shiftb = ctx->key_sz << 2;\n\tvoid *p = ctx->ecdh.p + ctx->key_sz - cur_sz;\n\tvoid *a = ctx->ecdh.p + shifta - cur_sz;\n\tvoid *b = ctx->ecdh.p + shiftb - cur_sz;\n\tvoid *x = ctx->ecdh.g + ctx->key_sz - cur_sz;\n\tvoid *y = ctx->ecdh.g + shifta - cur_sz;\n\tconst struct ecc_curve *curve = ecc_get_curve(ctx->curve_id);\n\tchar *n;\n\n\tif (unlikely(!curve))\n\t\treturn -EINVAL;\n\n\tn = kzalloc(ctx->key_sz, GFP_KERNEL);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\tfill_curve_param(p, curve->p, cur_sz, curve->g.ndigits);\n\tfill_curve_param(a, curve->a, cur_sz, curve->g.ndigits);\n\tfill_curve_param(b, curve->b, cur_sz, curve->g.ndigits);\n\tfill_curve_param(x, curve->g.x, cur_sz, curve->g.ndigits);\n\tfill_curve_param(y, curve->g.y, cur_sz, curve->g.ndigits);\n\tfill_curve_param(n, curve->n, cur_sz, curve->g.ndigits);\n\n\tif (params->key_size == cur_sz && memcmp(params->key, n, cur_sz) >= 0) {\n\t\tkfree(n);\n\t\treturn -EINVAL;\n\t}\n\n\tkfree(n);\n\treturn 0;\n}\n\nstatic unsigned int hpre_ecdh_get_curvesz(unsigned short id)\n{\n\tswitch (id) {\n\tcase ECC_CURVE_NIST_P192:\n\t\treturn HPRE_ECC_NIST_P192_N_SIZE;\n\tcase ECC_CURVE_NIST_P256:\n\t\treturn HPRE_ECC_NIST_P256_N_SIZE;\n\tcase ECC_CURVE_NIST_P384:\n\t\treturn HPRE_ECC_NIST_P384_N_SIZE;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int hpre_ecdh_set_param(struct hpre_ctx *ctx, struct ecdh *params)\n{\n\tstruct device *dev = ctx->dev;\n\tunsigned int sz, shift, curve_sz;\n\tint ret;\n\n\tctx->key_sz = hpre_ecdh_supported_curve(ctx->curve_id);\n\tif (!ctx->key_sz)\n\t\treturn -EINVAL;\n\n\tcurve_sz = hpre_ecdh_get_curvesz(ctx->curve_id);\n\tif (!curve_sz || params->key_size > curve_sz)\n\t\treturn -EINVAL;\n\n\tsz = ctx->key_sz;\n\n\tif (!ctx->ecdh.p) {\n\t\tctx->ecdh.p = dma_alloc_coherent(dev, sz << 3, &ctx->ecdh.dma_p,\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!ctx->ecdh.p)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tshift = sz << 2;\n\tctx->ecdh.g = ctx->ecdh.p + shift;\n\tctx->ecdh.dma_g = ctx->ecdh.dma_p + shift;\n\n\tret = hpre_ecdh_fill_curve(ctx, params, curve_sz);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to fill curve_param, ret = %d!\\n\", ret);\n\t\tdma_free_coherent(dev, sz << 3, ctx->ecdh.p, ctx->ecdh.dma_p);\n\t\tctx->ecdh.p = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic bool hpre_key_is_zero(char *key, unsigned short key_sz)\n{\n\tint i;\n\n\tfor (i = 0; i < key_sz; i++)\n\t\tif (key[i])\n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic int ecdh_gen_privkey(struct hpre_ctx *ctx, struct ecdh *params)\n{\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tret = crypto_get_default_rng();\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get default rng, ret = %d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = crypto_rng_get_bytes(crypto_default_rng, (u8 *)params->key,\n\t\t\t\t   params->key_size);\n\tcrypto_put_default_rng();\n\tif (ret)\n\t\tdev_err(dev, \"failed to get rng, ret = %d!\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int hpre_ecdh_set_secret(struct crypto_kpp *tfm, const void *buf,\n\t\t\t\tunsigned int len)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tunsigned int sz, sz_shift, curve_sz;\n\tstruct device *dev = ctx->dev;\n\tchar key[HPRE_ECC_MAX_KSZ];\n\tstruct ecdh params;\n\tint ret;\n\n\tif (crypto_ecdh_decode_key(buf, len, &params) < 0) {\n\t\tdev_err(dev, \"failed to decode ecdh key!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!params.key || !params.key_size) {\n\t\tparams.key = key;\n\t\tcurve_sz = hpre_ecdh_get_curvesz(ctx->curve_id);\n\t\tif (!curve_sz) {\n\t\t\tdev_err(dev, \"Invalid curve size!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tparams.key_size = curve_sz - 1;\n\t\tret = ecdh_gen_privkey(ctx, &params);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (hpre_key_is_zero(params.key, params.key_size)) {\n\t\tdev_err(dev, \"Invalid hpre key!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\thpre_ecc_clear_ctx(ctx, false, true);\n\n\tret = hpre_ecdh_set_param(ctx, &params);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"failed to set hpre param, ret = %d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tsz = ctx->key_sz;\n\tsz_shift = (sz << 1) + sz - params.key_size;\n\tmemcpy(ctx->ecdh.p + sz_shift, params.key, params.key_size);\n\n\treturn 0;\n}\n\nstatic void hpre_ecdh_hw_data_clr_all(struct hpre_ctx *ctx,\n\t\t\t\t      struct hpre_asym_request *req,\n\t\t\t\t      struct scatterlist *dst,\n\t\t\t\t      struct scatterlist *src)\n{\n\tstruct device *dev = ctx->dev;\n\tstruct hpre_sqe *sqe = &req->req;\n\tdma_addr_t dma;\n\n\tdma = le64_to_cpu(sqe->in);\n\tif (unlikely(dma_mapping_error(dev, dma)))\n\t\treturn;\n\n\tif (src && req->src)\n\t\tdma_free_coherent(dev, ctx->key_sz << 2, req->src, dma);\n\n\tdma = le64_to_cpu(sqe->out);\n\tif (unlikely(dma_mapping_error(dev, dma)))\n\t\treturn;\n\n\tif (req->dst)\n\t\tdma_free_coherent(dev, ctx->key_sz << 1, req->dst, dma);\n\tif (dst)\n\t\tdma_unmap_single(dev, dma, ctx->key_sz << 1, DMA_FROM_DEVICE);\n}\n\nstatic void hpre_ecdh_cb(struct hpre_ctx *ctx, void *resp)\n{\n\tunsigned int curve_sz = hpre_ecdh_get_curvesz(ctx->curve_id);\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tstruct hpre_asym_request *req = NULL;\n\tstruct kpp_request *areq;\n\tu64 overtime_thrhld;\n\tchar *p;\n\tint ret;\n\n\tret = hpre_alg_res_post_hf(ctx, resp, (void **)&req);\n\tareq = req->areq.ecdh;\n\tareq->dst_len = ctx->key_sz << 1;\n\n\tovertime_thrhld = atomic64_read(&dfx[HPRE_OVERTIME_THRHLD].value);\n\tif (overtime_thrhld && hpre_is_bd_timeout(req, overtime_thrhld))\n\t\tatomic64_inc(&dfx[HPRE_OVER_THRHLD_CNT].value);\n\n\tp = sg_virt(areq->dst);\n\tmemmove(p, p + ctx->key_sz - curve_sz, curve_sz);\n\tmemmove(p + curve_sz, p + areq->dst_len - curve_sz, curve_sz);\n\n\thpre_ecdh_hw_data_clr_all(ctx, req, areq->dst, areq->src);\n\tkpp_request_complete(areq, ret);\n\n\tatomic64_inc(&dfx[HPRE_RECV_CNT].value);\n}\n\nstatic int hpre_ecdh_msg_request_set(struct hpre_ctx *ctx,\n\t\t\t\t     struct kpp_request *req)\n{\n\tstruct hpre_asym_request *h_req;\n\tstruct hpre_sqe *msg;\n\tint req_id;\n\tvoid *tmp;\n\n\tif (req->dst_len < ctx->key_sz << 1) {\n\t\treq->dst_len = ctx->key_sz << 1;\n\t\treturn -EINVAL;\n\t}\n\n\ttmp = kpp_request_ctx(req);\n\th_req = PTR_ALIGN(tmp, hpre_align_sz());\n\th_req->cb = hpre_ecdh_cb;\n\th_req->areq.ecdh = req;\n\tmsg = &h_req->req;\n\tmemset(msg, 0, sizeof(*msg));\n\tmsg->in = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->out = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->key = cpu_to_le64(ctx->ecdh.dma_p);\n\n\tmsg->dw0 |= cpu_to_le32(0x1U << HPRE_SQE_DONE_SHIFT);\n\tmsg->task_len1 = (ctx->key_sz >> HPRE_BITS_2_BYTES_SHIFT) - 1;\n\th_req->ctx = ctx;\n\n\treq_id = hpre_add_req_to_ctx(h_req);\n\tif (req_id < 0)\n\t\treturn -EBUSY;\n\n\tmsg->tag = cpu_to_le16((u16)req_id);\n\treturn 0;\n}\n\nstatic int hpre_ecdh_src_data_init(struct hpre_asym_request *hpre_req,\n\t\t\t\t   struct scatterlist *data, unsigned int len)\n{\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tstruct device *dev = ctx->dev;\n\tunsigned int tmpshift;\n\tdma_addr_t dma = 0;\n\tvoid *ptr;\n\tint shift;\n\n\t \n\tshift = ctx->key_sz - (len >> 1);\n\tif (unlikely(shift < 0))\n\t\treturn -EINVAL;\n\n\tptr = dma_alloc_coherent(dev, ctx->key_sz << 2, &dma, GFP_KERNEL);\n\tif (unlikely(!ptr))\n\t\treturn -ENOMEM;\n\n\ttmpshift = ctx->key_sz << 1;\n\tscatterwalk_map_and_copy(ptr + tmpshift, data, 0, len, 0);\n\tmemcpy(ptr + shift, ptr + tmpshift, len >> 1);\n\tmemcpy(ptr + ctx->key_sz + shift, ptr + tmpshift + (len >> 1), len >> 1);\n\n\thpre_req->src = ptr;\n\tmsg->in = cpu_to_le64(dma);\n\treturn 0;\n}\n\nstatic int hpre_ecdh_dst_data_init(struct hpre_asym_request *hpre_req,\n\t\t\t\t   struct scatterlist *data, unsigned int len)\n{\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tstruct device *dev = ctx->dev;\n\tdma_addr_t dma;\n\n\tif (unlikely(!data || !sg_is_last(data) || len != ctx->key_sz << 1)) {\n\t\tdev_err(dev, \"data or data length is illegal!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\thpre_req->dst = NULL;\n\tdma = dma_map_single(dev, sg_virt(data), len, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, dma))) {\n\t\tdev_err(dev, \"dma map data err!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmsg->out = cpu_to_le64(dma);\n\treturn 0;\n}\n\nstatic int hpre_ecdh_compute_value(struct kpp_request *req)\n{\n\tstruct crypto_kpp *tfm = crypto_kpp_reqtfm(req);\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct device *dev = ctx->dev;\n\tvoid *tmp = kpp_request_ctx(req);\n\tstruct hpre_asym_request *hpre_req = PTR_ALIGN(tmp, hpre_align_sz());\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tint ret;\n\n\tret = hpre_ecdh_msg_request_set(ctx, req);\n\tif (unlikely(ret)) {\n\t\tdev_err(dev, \"failed to set ecdh request, ret = %d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (req->src) {\n\t\tret = hpre_ecdh_src_data_init(hpre_req, req->src, req->src_len);\n\t\tif (unlikely(ret)) {\n\t\t\tdev_err(dev, \"failed to init src data, ret = %d!\\n\", ret);\n\t\t\tgoto clear_all;\n\t\t}\n\t} else {\n\t\tmsg->in = cpu_to_le64(ctx->ecdh.dma_g);\n\t}\n\n\tret = hpre_ecdh_dst_data_init(hpre_req, req->dst, req->dst_len);\n\tif (unlikely(ret)) {\n\t\tdev_err(dev, \"failed to init dst data, ret = %d!\\n\", ret);\n\t\tgoto clear_all;\n\t}\n\n\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) | HPRE_ALG_ECC_MUL);\n\tret = hpre_send(ctx, msg);\n\tif (likely(!ret))\n\t\treturn -EINPROGRESS;\n\nclear_all:\n\thpre_rm_req_from_ctx(hpre_req);\n\thpre_ecdh_hw_data_clr_all(ctx, hpre_req, req->dst, req->src);\n\treturn ret;\n}\n\nstatic unsigned int hpre_ecdh_max_size(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\t \n\treturn ctx->key_sz << 1;\n}\n\nstatic int hpre_ecdh_nist_p192_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\tctx->curve_id = ECC_CURVE_NIST_P192;\n\n\tkpp_set_reqsize(tfm, sizeof(struct hpre_asym_request) + hpre_align_pd());\n\n\treturn hpre_ctx_init(ctx, HPRE_V3_ECC_ALG_TYPE);\n}\n\nstatic int hpre_ecdh_nist_p256_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\tctx->curve_id = ECC_CURVE_NIST_P256;\n\n\tkpp_set_reqsize(tfm, sizeof(struct hpre_asym_request) + hpre_align_pd());\n\n\treturn hpre_ctx_init(ctx, HPRE_V3_ECC_ALG_TYPE);\n}\n\nstatic int hpre_ecdh_nist_p384_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\tctx->curve_id = ECC_CURVE_NIST_P384;\n\n\tkpp_set_reqsize(tfm, sizeof(struct hpre_asym_request) + hpre_align_pd());\n\n\treturn hpre_ctx_init(ctx, HPRE_V3_ECC_ALG_TYPE);\n}\n\nstatic void hpre_ecdh_exit_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\thpre_ecc_clear_ctx(ctx, true, true);\n}\n\nstatic void hpre_curve25519_fill_curve(struct hpre_ctx *ctx, const void *buf,\n\t\t\t\t       unsigned int len)\n{\n\tu8 secret[CURVE25519_KEY_SIZE] = { 0 };\n\tunsigned int sz = ctx->key_sz;\n\tconst struct ecc_curve *curve;\n\tunsigned int shift = sz << 1;\n\tvoid *p;\n\n\t \n\tmemcpy(secret, buf, len);\n\tcurve25519_clamp_secret(secret);\n\thpre_key_to_big_end(secret, CURVE25519_KEY_SIZE);\n\n\tp = ctx->curve25519.p + sz - len;\n\n\tcurve = ecc_get_curve25519();\n\n\t \n\tfill_curve_param(p, curve->p, len, curve->g.ndigits);\n\tfill_curve_param(p + sz, curve->a, len, curve->g.ndigits);\n\tmemcpy(p + shift, secret, len);\n\tfill_curve_param(p + shift + sz, curve->g.x, len, curve->g.ndigits);\n\tmemzero_explicit(secret, CURVE25519_KEY_SIZE);\n}\n\nstatic int hpre_curve25519_set_param(struct hpre_ctx *ctx, const void *buf,\n\t\t\t\t     unsigned int len)\n{\n\tstruct device *dev = ctx->dev;\n\tunsigned int sz = ctx->key_sz;\n\tunsigned int shift = sz << 1;\n\n\t \n\tif (!ctx->curve25519.p) {\n\t\tctx->curve25519.p = dma_alloc_coherent(dev, sz << 2,\n\t\t\t\t\t\t       &ctx->curve25519.dma_p,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!ctx->curve25519.p)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tctx->curve25519.g = ctx->curve25519.p + shift + sz;\n\tctx->curve25519.dma_g = ctx->curve25519.dma_p + shift + sz;\n\n\thpre_curve25519_fill_curve(ctx, buf, len);\n\n\treturn 0;\n}\n\nstatic int hpre_curve25519_set_secret(struct crypto_kpp *tfm, const void *buf,\n\t\t\t\t      unsigned int len)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct device *dev = ctx->dev;\n\tint ret = -EINVAL;\n\n\tif (len != CURVE25519_KEY_SIZE ||\n\t    !crypto_memneq(buf, curve25519_null_point, CURVE25519_KEY_SIZE)) {\n\t\tdev_err(dev, \"key is null or key len is not 32bytes!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\thpre_ecc_clear_ctx(ctx, false, false);\n\n\tctx->key_sz = CURVE25519_KEY_SIZE;\n\tret = hpre_curve25519_set_param(ctx, buf, CURVE25519_KEY_SIZE);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to set curve25519 param, ret = %d!\\n\", ret);\n\t\thpre_ecc_clear_ctx(ctx, false, false);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void hpre_curve25519_hw_data_clr_all(struct hpre_ctx *ctx,\n\t\t\t\t\t    struct hpre_asym_request *req,\n\t\t\t\t\t    struct scatterlist *dst,\n\t\t\t\t\t    struct scatterlist *src)\n{\n\tstruct device *dev = ctx->dev;\n\tstruct hpre_sqe *sqe = &req->req;\n\tdma_addr_t dma;\n\n\tdma = le64_to_cpu(sqe->in);\n\tif (unlikely(dma_mapping_error(dev, dma)))\n\t\treturn;\n\n\tif (src && req->src)\n\t\tdma_free_coherent(dev, ctx->key_sz, req->src, dma);\n\n\tdma = le64_to_cpu(sqe->out);\n\tif (unlikely(dma_mapping_error(dev, dma)))\n\t\treturn;\n\n\tif (req->dst)\n\t\tdma_free_coherent(dev, ctx->key_sz, req->dst, dma);\n\tif (dst)\n\t\tdma_unmap_single(dev, dma, ctx->key_sz, DMA_FROM_DEVICE);\n}\n\nstatic void hpre_curve25519_cb(struct hpre_ctx *ctx, void *resp)\n{\n\tstruct hpre_dfx *dfx = ctx->hpre->debug.dfx;\n\tstruct hpre_asym_request *req = NULL;\n\tstruct kpp_request *areq;\n\tu64 overtime_thrhld;\n\tint ret;\n\n\tret = hpre_alg_res_post_hf(ctx, resp, (void **)&req);\n\tareq = req->areq.curve25519;\n\tareq->dst_len = ctx->key_sz;\n\n\tovertime_thrhld = atomic64_read(&dfx[HPRE_OVERTIME_THRHLD].value);\n\tif (overtime_thrhld && hpre_is_bd_timeout(req, overtime_thrhld))\n\t\tatomic64_inc(&dfx[HPRE_OVER_THRHLD_CNT].value);\n\n\thpre_key_to_big_end(sg_virt(areq->dst), CURVE25519_KEY_SIZE);\n\n\thpre_curve25519_hw_data_clr_all(ctx, req, areq->dst, areq->src);\n\tkpp_request_complete(areq, ret);\n\n\tatomic64_inc(&dfx[HPRE_RECV_CNT].value);\n}\n\nstatic int hpre_curve25519_msg_request_set(struct hpre_ctx *ctx,\n\t\t\t\t\t   struct kpp_request *req)\n{\n\tstruct hpre_asym_request *h_req;\n\tstruct hpre_sqe *msg;\n\tint req_id;\n\tvoid *tmp;\n\n\tif (unlikely(req->dst_len < ctx->key_sz)) {\n\t\treq->dst_len = ctx->key_sz;\n\t\treturn -EINVAL;\n\t}\n\n\ttmp = kpp_request_ctx(req);\n\th_req = PTR_ALIGN(tmp, hpre_align_sz());\n\th_req->cb = hpre_curve25519_cb;\n\th_req->areq.curve25519 = req;\n\tmsg = &h_req->req;\n\tmemset(msg, 0, sizeof(*msg));\n\tmsg->in = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->out = cpu_to_le64(DMA_MAPPING_ERROR);\n\tmsg->key = cpu_to_le64(ctx->curve25519.dma_p);\n\n\tmsg->dw0 |= cpu_to_le32(0x1U << HPRE_SQE_DONE_SHIFT);\n\tmsg->task_len1 = (ctx->key_sz >> HPRE_BITS_2_BYTES_SHIFT) - 1;\n\th_req->ctx = ctx;\n\n\treq_id = hpre_add_req_to_ctx(h_req);\n\tif (req_id < 0)\n\t\treturn -EBUSY;\n\n\tmsg->tag = cpu_to_le16((u16)req_id);\n\treturn 0;\n}\n\nstatic void hpre_curve25519_src_modulo_p(u8 *ptr)\n{\n\tint i;\n\n\tfor (i = 0; i < CURVE25519_KEY_SIZE - 1; i++)\n\t\tptr[i] = 0;\n\n\t \n\tptr[i] -= 0xed;\n}\n\nstatic int hpre_curve25519_src_init(struct hpre_asym_request *hpre_req,\n\t\t\t\t    struct scatterlist *data, unsigned int len)\n{\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tstruct device *dev = ctx->dev;\n\tu8 p[CURVE25519_KEY_SIZE] = { 0 };\n\tconst struct ecc_curve *curve;\n\tdma_addr_t dma = 0;\n\tu8 *ptr;\n\n\tif (len != CURVE25519_KEY_SIZE) {\n\t\tdev_err(dev, \"sourc_data len is not 32bytes, len = %u!\\n\", len);\n\t\treturn -EINVAL;\n\t}\n\n\tptr = dma_alloc_coherent(dev, ctx->key_sz, &dma, GFP_KERNEL);\n\tif (unlikely(!ptr))\n\t\treturn -ENOMEM;\n\n\tscatterwalk_map_and_copy(ptr, data, 0, len, 0);\n\n\tif (!crypto_memneq(ptr, curve25519_null_point, CURVE25519_KEY_SIZE)) {\n\t\tdev_err(dev, \"gx is null!\\n\");\n\t\tgoto err;\n\t}\n\n\t \n\tptr[31] &= 0x7f;\n\thpre_key_to_big_end(ptr, CURVE25519_KEY_SIZE);\n\n\tcurve = ecc_get_curve25519();\n\n\tfill_curve_param(p, curve->p, CURVE25519_KEY_SIZE, curve->g.ndigits);\n\n\t \n\tif (memcmp(ptr, p, ctx->key_sz) == 0) {\n\t\tdev_err(dev, \"gx is p!\\n\");\n\t\tgoto err;\n\t} else if (memcmp(ptr, p, ctx->key_sz) > 0) {\n\t\thpre_curve25519_src_modulo_p(ptr);\n\t}\n\n\thpre_req->src = ptr;\n\tmsg->in = cpu_to_le64(dma);\n\treturn 0;\n\nerr:\n\tdma_free_coherent(dev, ctx->key_sz, ptr, dma);\n\treturn -EINVAL;\n}\n\nstatic int hpre_curve25519_dst_init(struct hpre_asym_request *hpre_req,\n\t\t\t\t    struct scatterlist *data, unsigned int len)\n{\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tstruct hpre_ctx *ctx = hpre_req->ctx;\n\tstruct device *dev = ctx->dev;\n\tdma_addr_t dma;\n\n\tif (!data || !sg_is_last(data) || len != ctx->key_sz) {\n\t\tdev_err(dev, \"data or data length is illegal!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\thpre_req->dst = NULL;\n\tdma = dma_map_single(dev, sg_virt(data), len, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, dma))) {\n\t\tdev_err(dev, \"dma map data err!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmsg->out = cpu_to_le64(dma);\n\treturn 0;\n}\n\nstatic int hpre_curve25519_compute_value(struct kpp_request *req)\n{\n\tstruct crypto_kpp *tfm = crypto_kpp_reqtfm(req);\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct device *dev = ctx->dev;\n\tvoid *tmp = kpp_request_ctx(req);\n\tstruct hpre_asym_request *hpre_req = PTR_ALIGN(tmp, hpre_align_sz());\n\tstruct hpre_sqe *msg = &hpre_req->req;\n\tint ret;\n\n\tret = hpre_curve25519_msg_request_set(ctx, req);\n\tif (unlikely(ret)) {\n\t\tdev_err(dev, \"failed to set curve25519 request, ret = %d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (req->src) {\n\t\tret = hpre_curve25519_src_init(hpre_req, req->src, req->src_len);\n\t\tif (unlikely(ret)) {\n\t\t\tdev_err(dev, \"failed to init src data, ret = %d!\\n\",\n\t\t\t\tret);\n\t\t\tgoto clear_all;\n\t\t}\n\t} else {\n\t\tmsg->in = cpu_to_le64(ctx->curve25519.dma_g);\n\t}\n\n\tret = hpre_curve25519_dst_init(hpre_req, req->dst, req->dst_len);\n\tif (unlikely(ret)) {\n\t\tdev_err(dev, \"failed to init dst data, ret = %d!\\n\", ret);\n\t\tgoto clear_all;\n\t}\n\n\tmsg->dw0 = cpu_to_le32(le32_to_cpu(msg->dw0) | HPRE_ALG_CURVE25519_MUL);\n\tret = hpre_send(ctx, msg);\n\tif (likely(!ret))\n\t\treturn -EINPROGRESS;\n\nclear_all:\n\thpre_rm_req_from_ctx(hpre_req);\n\thpre_curve25519_hw_data_clr_all(ctx, hpre_req, req->dst, req->src);\n\treturn ret;\n}\n\nstatic unsigned int hpre_curve25519_max_size(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\treturn ctx->key_sz;\n}\n\nstatic int hpre_curve25519_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\tkpp_set_reqsize(tfm, sizeof(struct hpre_asym_request) + hpre_align_pd());\n\n\treturn hpre_ctx_init(ctx, HPRE_V3_ECC_ALG_TYPE);\n}\n\nstatic void hpre_curve25519_exit_tfm(struct crypto_kpp *tfm)\n{\n\tstruct hpre_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\thpre_ecc_clear_ctx(ctx, true, false);\n}\n\nstatic struct akcipher_alg rsa = {\n\t.sign = hpre_rsa_dec,\n\t.verify = hpre_rsa_enc,\n\t.encrypt = hpre_rsa_enc,\n\t.decrypt = hpre_rsa_dec,\n\t.set_pub_key = hpre_rsa_setpubkey,\n\t.set_priv_key = hpre_rsa_setprivkey,\n\t.max_size = hpre_rsa_max_size,\n\t.init = hpre_rsa_init_tfm,\n\t.exit = hpre_rsa_exit_tfm,\n\t.base = {\n\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t.cra_name = \"rsa\",\n\t\t.cra_driver_name = \"hpre-rsa\",\n\t\t.cra_module = THIS_MODULE,\n\t},\n};\n\nstatic struct kpp_alg dh = {\n\t.set_secret = hpre_dh_set_secret,\n\t.generate_public_key = hpre_dh_compute_value,\n\t.compute_shared_secret = hpre_dh_compute_value,\n\t.max_size = hpre_dh_max_size,\n\t.init = hpre_dh_init_tfm,\n\t.exit = hpre_dh_exit_tfm,\n\t.base = {\n\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t.cra_name = \"dh\",\n\t\t.cra_driver_name = \"hpre-dh\",\n\t\t.cra_module = THIS_MODULE,\n\t},\n};\n\nstatic struct kpp_alg ecdh_curves[] = {\n\t{\n\t\t.set_secret = hpre_ecdh_set_secret,\n\t\t.generate_public_key = hpre_ecdh_compute_value,\n\t\t.compute_shared_secret = hpre_ecdh_compute_value,\n\t\t.max_size = hpre_ecdh_max_size,\n\t\t.init = hpre_ecdh_nist_p192_init_tfm,\n\t\t.exit = hpre_ecdh_exit_tfm,\n\t\t.base = {\n\t\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t\t.cra_name = \"ecdh-nist-p192\",\n\t\t\t.cra_driver_name = \"hpre-ecdh-nist-p192\",\n\t\t\t.cra_module = THIS_MODULE,\n\t\t},\n\t}, {\n\t\t.set_secret = hpre_ecdh_set_secret,\n\t\t.generate_public_key = hpre_ecdh_compute_value,\n\t\t.compute_shared_secret = hpre_ecdh_compute_value,\n\t\t.max_size = hpre_ecdh_max_size,\n\t\t.init = hpre_ecdh_nist_p256_init_tfm,\n\t\t.exit = hpre_ecdh_exit_tfm,\n\t\t.base = {\n\t\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t\t.cra_name = \"ecdh-nist-p256\",\n\t\t\t.cra_driver_name = \"hpre-ecdh-nist-p256\",\n\t\t\t.cra_module = THIS_MODULE,\n\t\t},\n\t}, {\n\t\t.set_secret = hpre_ecdh_set_secret,\n\t\t.generate_public_key = hpre_ecdh_compute_value,\n\t\t.compute_shared_secret = hpre_ecdh_compute_value,\n\t\t.max_size = hpre_ecdh_max_size,\n\t\t.init = hpre_ecdh_nist_p384_init_tfm,\n\t\t.exit = hpre_ecdh_exit_tfm,\n\t\t.base = {\n\t\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t\t.cra_name = \"ecdh-nist-p384\",\n\t\t\t.cra_driver_name = \"hpre-ecdh-nist-p384\",\n\t\t\t.cra_module = THIS_MODULE,\n\t\t},\n\t}\n};\n\nstatic struct kpp_alg curve25519_alg = {\n\t.set_secret = hpre_curve25519_set_secret,\n\t.generate_public_key = hpre_curve25519_compute_value,\n\t.compute_shared_secret = hpre_curve25519_compute_value,\n\t.max_size = hpre_curve25519_max_size,\n\t.init = hpre_curve25519_init_tfm,\n\t.exit = hpre_curve25519_exit_tfm,\n\t.base = {\n\t\t.cra_ctxsize = sizeof(struct hpre_ctx),\n\t\t.cra_priority = HPRE_CRYPTO_ALG_PRI,\n\t\t.cra_name = \"curve25519\",\n\t\t.cra_driver_name = \"hpre-curve25519\",\n\t\t.cra_module = THIS_MODULE,\n\t},\n};\n\nstatic int hpre_register_rsa(struct hisi_qm *qm)\n{\n\tint ret;\n\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_RSA_MASK_CAP))\n\t\treturn 0;\n\n\trsa.base.cra_flags = 0;\n\tret = crypto_register_akcipher(&rsa);\n\tif (ret)\n\t\tdev_err(&qm->pdev->dev, \"failed to register rsa (%d)!\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void hpre_unregister_rsa(struct hisi_qm *qm)\n{\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_RSA_MASK_CAP))\n\t\treturn;\n\n\tcrypto_unregister_akcipher(&rsa);\n}\n\nstatic int hpre_register_dh(struct hisi_qm *qm)\n{\n\tint ret;\n\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_DH_MASK_CAP))\n\t\treturn 0;\n\n\tret = crypto_register_kpp(&dh);\n\tif (ret)\n\t\tdev_err(&qm->pdev->dev, \"failed to register dh (%d)!\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void hpre_unregister_dh(struct hisi_qm *qm)\n{\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_DH_MASK_CAP))\n\t\treturn;\n\n\tcrypto_unregister_kpp(&dh);\n}\n\nstatic int hpre_register_ecdh(struct hisi_qm *qm)\n{\n\tint ret, i;\n\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_ECDH_MASK_CAP))\n\t\treturn 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(ecdh_curves); i++) {\n\t\tret = crypto_register_kpp(&ecdh_curves[i]);\n\t\tif (ret) {\n\t\t\tdev_err(&qm->pdev->dev, \"failed to register %s (%d)!\\n\",\n\t\t\t\tecdh_curves[i].base.cra_name, ret);\n\t\t\tgoto unreg_kpp;\n\t\t}\n\t}\n\n\treturn 0;\n\nunreg_kpp:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_kpp(&ecdh_curves[i]);\n\n\treturn ret;\n}\n\nstatic void hpre_unregister_ecdh(struct hisi_qm *qm)\n{\n\tint i;\n\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_ECDH_MASK_CAP))\n\t\treturn;\n\n\tfor (i = ARRAY_SIZE(ecdh_curves) - 1; i >= 0; --i)\n\t\tcrypto_unregister_kpp(&ecdh_curves[i]);\n}\n\nstatic int hpre_register_x25519(struct hisi_qm *qm)\n{\n\tint ret;\n\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_X25519_MASK_CAP))\n\t\treturn 0;\n\n\tret = crypto_register_kpp(&curve25519_alg);\n\tif (ret)\n\t\tdev_err(&qm->pdev->dev, \"failed to register x25519 (%d)!\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void hpre_unregister_x25519(struct hisi_qm *qm)\n{\n\tif (!hpre_check_alg_support(qm, HPRE_DRV_X25519_MASK_CAP))\n\t\treturn;\n\n\tcrypto_unregister_kpp(&curve25519_alg);\n}\n\nint hpre_algs_register(struct hisi_qm *qm)\n{\n\tint ret;\n\n\tret = hpre_register_rsa(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tret = hpre_register_dh(qm);\n\tif (ret)\n\t\tgoto unreg_rsa;\n\n\tret = hpre_register_ecdh(qm);\n\tif (ret)\n\t\tgoto unreg_dh;\n\n\tret = hpre_register_x25519(qm);\n\tif (ret)\n\t\tgoto unreg_ecdh;\n\n\treturn ret;\n\nunreg_ecdh:\n\thpre_unregister_ecdh(qm);\nunreg_dh:\n\thpre_unregister_dh(qm);\nunreg_rsa:\n\thpre_unregister_rsa(qm);\n\treturn ret;\n}\n\nvoid hpre_algs_unregister(struct hisi_qm *qm)\n{\n\thpre_unregister_x25519(qm);\n\thpre_unregister_ecdh(qm);\n\thpre_unregister_dh(qm);\n\thpre_unregister_rsa(qm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}