{
  "module_name": "hpre_main.c",
  "hash_id": "229b9ca68fb2c30551577787dc3c1368c68f43d2e979a66560caefb0d375c767",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/hisilicon/hpre/hpre_main.c",
  "human_readable_source": "\n \n#include <linux/acpi.h>\n#include <linux/bitops.h>\n#include <linux/debugfs.h>\n#include <linux/init.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/topology.h>\n#include <linux/uacce.h>\n#include \"hpre.h\"\n\n#define HPRE_QM_ABNML_INT_MASK\t\t0x100004\n#define HPRE_CTRL_CNT_CLR_CE_BIT\tBIT(0)\n#define HPRE_COMM_CNT_CLR_CE\t\t0x0\n#define HPRE_CTRL_CNT_CLR_CE\t\t0x301000\n#define HPRE_FSM_MAX_CNT\t\t0x301008\n#define HPRE_VFG_AXQOS\t\t\t0x30100c\n#define HPRE_VFG_AXCACHE\t\t0x301010\n#define HPRE_RDCHN_INI_CFG\t\t0x301014\n#define HPRE_AWUSR_FP_CFG\t\t0x301018\n#define HPRE_BD_ENDIAN\t\t\t0x301020\n#define HPRE_ECC_BYPASS\t\t\t0x301024\n#define HPRE_RAS_WIDTH_CFG\t\t0x301028\n#define HPRE_POISON_BYPASS\t\t0x30102c\n#define HPRE_BD_ARUSR_CFG\t\t0x301030\n#define HPRE_BD_AWUSR_CFG\t\t0x301034\n#define HPRE_TYPES_ENB\t\t\t0x301038\n#define HPRE_RSA_ENB\t\t\tBIT(0)\n#define HPRE_ECC_ENB\t\t\tBIT(1)\n#define HPRE_DATA_RUSER_CFG\t\t0x30103c\n#define HPRE_DATA_WUSER_CFG\t\t0x301040\n#define HPRE_INT_MASK\t\t\t0x301400\n#define HPRE_INT_STATUS\t\t\t0x301800\n#define HPRE_HAC_INT_MSK\t\t0x301400\n#define HPRE_HAC_RAS_CE_ENB\t\t0x301410\n#define HPRE_HAC_RAS_NFE_ENB\t\t0x301414\n#define HPRE_HAC_RAS_FE_ENB\t\t0x301418\n#define HPRE_HAC_INT_SET\t\t0x301500\n#define HPRE_RNG_TIMEOUT_NUM\t\t0x301A34\n#define HPRE_CORE_INT_ENABLE\t\t0\n#define HPRE_CORE_INT_DISABLE\t\tGENMASK(21, 0)\n#define HPRE_RDCHN_INI_ST\t\t0x301a00\n#define HPRE_CLSTR_BASE\t\t\t0x302000\n#define HPRE_CORE_EN_OFFSET\t\t0x04\n#define HPRE_CORE_INI_CFG_OFFSET\t0x20\n#define HPRE_CORE_INI_STATUS_OFFSET\t0x80\n#define HPRE_CORE_HTBT_WARN_OFFSET\t0x8c\n#define HPRE_CORE_IS_SCHD_OFFSET\t0x90\n\n#define HPRE_RAS_CE_ENB\t\t\t0x301410\n#define HPRE_RAS_NFE_ENB\t\t0x301414\n#define HPRE_RAS_FE_ENB\t\t\t0x301418\n#define HPRE_OOO_SHUTDOWN_SEL\t\t0x301a3c\n#define HPRE_HAC_RAS_FE_ENABLE\t\t0\n\n#define HPRE_CORE_ENB\t\t(HPRE_CLSTR_BASE + HPRE_CORE_EN_OFFSET)\n#define HPRE_CORE_INI_CFG\t(HPRE_CLSTR_BASE + HPRE_CORE_INI_CFG_OFFSET)\n#define HPRE_CORE_INI_STATUS (HPRE_CLSTR_BASE + HPRE_CORE_INI_STATUS_OFFSET)\n#define HPRE_HAC_ECC1_CNT\t\t0x301a04\n#define HPRE_HAC_ECC2_CNT\t\t0x301a08\n#define HPRE_HAC_SOURCE_INT\t\t0x301600\n#define HPRE_CLSTR_ADDR_INTRVL\t\t0x1000\n#define HPRE_CLUSTER_INQURY\t\t0x100\n#define HPRE_CLSTR_ADDR_INQRY_RSLT\t0x104\n#define HPRE_TIMEOUT_ABNML_BIT\t\t6\n#define HPRE_PASID_EN_BIT\t\t9\n#define HPRE_REG_RD_INTVRL_US\t\t10\n#define HPRE_REG_RD_TMOUT_US\t\t1000\n#define HPRE_DBGFS_VAL_MAX_LEN\t\t20\n#define PCI_DEVICE_ID_HUAWEI_HPRE_PF\t0xa258\n#define HPRE_QM_USR_CFG_MASK\t\tGENMASK(31, 1)\n#define HPRE_QM_AXI_CFG_MASK\t\tGENMASK(15, 0)\n#define HPRE_QM_VFG_AX_MASK\t\tGENMASK(7, 0)\n#define HPRE_BD_USR_MASK\t\tGENMASK(1, 0)\n#define HPRE_PREFETCH_CFG\t\t0x301130\n#define HPRE_SVA_PREFTCH_DFX\t\t0x30115C\n#define HPRE_PREFETCH_ENABLE\t\t(~(BIT(0) | BIT(30)))\n#define HPRE_PREFETCH_DISABLE\t\tBIT(30)\n#define HPRE_SVA_DISABLE_READY\t\t(BIT(4) | BIT(8))\n\n \n#define HPRE_CLKGATE_CTL\t\t0x301a10\n#define HPRE_PEH_CFG_AUTO_GATE\t\t0x301a2c\n#define HPRE_CLUSTER_DYN_CTL\t\t0x302010\n#define HPRE_CORE_SHB_CFG\t\t0x302088\n#define HPRE_CLKGATE_CTL_EN\t\tBIT(0)\n#define HPRE_PEH_CFG_AUTO_GATE_EN\tBIT(0)\n#define HPRE_CLUSTER_DYN_CTL_EN\t\tBIT(0)\n#define HPRE_CORE_GATE_EN\t\t(BIT(30) | BIT(31))\n\n#define HPRE_AM_OOO_SHUTDOWN_ENB\t0x301044\n#define HPRE_AM_OOO_SHUTDOWN_ENABLE\tBIT(0)\n#define HPRE_WR_MSI_PORT\t\tBIT(2)\n\n#define HPRE_CORE_ECC_2BIT_ERR\t\tBIT(1)\n#define HPRE_OOO_ECC_2BIT_ERR\t\tBIT(5)\n\n#define HPRE_QM_BME_FLR\t\t\tBIT(7)\n#define HPRE_QM_PM_FLR\t\t\tBIT(11)\n#define HPRE_QM_SRIOV_FLR\t\tBIT(12)\n\n#define HPRE_SHAPER_TYPE_RATE\t\t640\n#define HPRE_VIA_MSI_DSM\t\t1\n#define HPRE_SQE_MASK_OFFSET\t\t8\n#define HPRE_SQE_MASK_LEN\t\t24\n\n#define HPRE_DFX_BASE\t\t0x301000\n#define HPRE_DFX_COMMON1\t\t0x301400\n#define HPRE_DFX_COMMON2\t\t0x301A00\n#define HPRE_DFX_CORE\t\t0x302000\n#define HPRE_DFX_BASE_LEN\t\t0x55\n#define HPRE_DFX_COMMON1_LEN\t\t0x41\n#define HPRE_DFX_COMMON2_LEN\t\t0xE\n#define HPRE_DFX_CORE_LEN\t\t0x43\n\nstatic const char hpre_name[] = \"hisi_hpre\";\nstatic struct dentry *hpre_debugfs_root;\nstatic const struct pci_device_id hpre_dev_ids[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_HPRE_PF) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_HPRE_VF) },\n\t{ 0, }\n};\n\nMODULE_DEVICE_TABLE(pci, hpre_dev_ids);\n\nstruct hpre_hw_error {\n\tu32 int_msk;\n\tconst char *msg;\n};\n\nstatic const struct qm_dev_alg hpre_dev_algs[] = {\n\t{\n\t\t.alg_msk = BIT(0),\n\t\t.alg = \"rsa\\n\"\n\t}, {\n\t\t.alg_msk = BIT(1),\n\t\t.alg = \"dh\\n\"\n\t}, {\n\t\t.alg_msk = BIT(2),\n\t\t.alg = \"ecdh\\n\"\n\t}, {\n\t\t.alg_msk = BIT(3),\n\t\t.alg = \"ecdsa\\n\"\n\t}, {\n\t\t.alg_msk = BIT(4),\n\t\t.alg = \"sm2\\n\"\n\t}, {\n\t\t.alg_msk = BIT(5),\n\t\t.alg = \"x25519\\n\"\n\t}, {\n\t\t.alg_msk = BIT(6),\n\t\t.alg = \"x448\\n\"\n\t}, {\n\t\t \n\t}\n};\n\nstatic struct hisi_qm_list hpre_devices = {\n\t.register_to_crypto\t= hpre_algs_register,\n\t.unregister_from_crypto\t= hpre_algs_unregister,\n};\n\nstatic const char * const hpre_debug_file_name[] = {\n\t[HPRE_CLEAR_ENABLE] = \"rdclr_en\",\n\t[HPRE_CLUSTER_CTRL] = \"cluster_ctrl\",\n};\n\nenum hpre_cap_type {\n\tHPRE_QM_NFE_MASK_CAP,\n\tHPRE_QM_RESET_MASK_CAP,\n\tHPRE_QM_OOO_SHUTDOWN_MASK_CAP,\n\tHPRE_QM_CE_MASK_CAP,\n\tHPRE_NFE_MASK_CAP,\n\tHPRE_RESET_MASK_CAP,\n\tHPRE_OOO_SHUTDOWN_MASK_CAP,\n\tHPRE_CE_MASK_CAP,\n\tHPRE_CLUSTER_NUM_CAP,\n\tHPRE_CORE_TYPE_NUM_CAP,\n\tHPRE_CORE_NUM_CAP,\n\tHPRE_CLUSTER_CORE_NUM_CAP,\n\tHPRE_CORE_ENABLE_BITMAP_CAP,\n\tHPRE_DRV_ALG_BITMAP_CAP,\n\tHPRE_DEV_ALG_BITMAP_CAP,\n\tHPRE_CORE1_ALG_BITMAP_CAP,\n\tHPRE_CORE2_ALG_BITMAP_CAP,\n\tHPRE_CORE3_ALG_BITMAP_CAP,\n\tHPRE_CORE4_ALG_BITMAP_CAP,\n\tHPRE_CORE5_ALG_BITMAP_CAP,\n\tHPRE_CORE6_ALG_BITMAP_CAP,\n\tHPRE_CORE7_ALG_BITMAP_CAP,\n\tHPRE_CORE8_ALG_BITMAP_CAP,\n\tHPRE_CORE9_ALG_BITMAP_CAP,\n\tHPRE_CORE10_ALG_BITMAP_CAP\n};\n\nstatic const struct hisi_qm_cap_info hpre_basic_info[] = {\n\t{HPRE_QM_NFE_MASK_CAP, 0x3124, 0, GENMASK(31, 0), 0x0, 0x1C37, 0x7C37},\n\t{HPRE_QM_RESET_MASK_CAP, 0x3128, 0, GENMASK(31, 0), 0x0, 0xC37, 0x6C37},\n\t{HPRE_QM_OOO_SHUTDOWN_MASK_CAP, 0x3128, 0, GENMASK(31, 0), 0x0, 0x4, 0x6C37},\n\t{HPRE_QM_CE_MASK_CAP, 0x312C, 0, GENMASK(31, 0), 0x0, 0x8, 0x8},\n\t{HPRE_NFE_MASK_CAP, 0x3130, 0, GENMASK(31, 0), 0x0, 0x3FFFFE, 0x1FFFFFE},\n\t{HPRE_RESET_MASK_CAP, 0x3134, 0, GENMASK(31, 0), 0x0, 0x3FFFFE, 0xBFFFFE},\n\t{HPRE_OOO_SHUTDOWN_MASK_CAP, 0x3134, 0, GENMASK(31, 0), 0x0, 0x22, 0xBFFFFE},\n\t{HPRE_CE_MASK_CAP, 0x3138, 0, GENMASK(31, 0), 0x0, 0x1, 0x1},\n\t{HPRE_CLUSTER_NUM_CAP, 0x313c, 20, GENMASK(3, 0), 0x0,  0x4, 0x1},\n\t{HPRE_CORE_TYPE_NUM_CAP, 0x313c, 16, GENMASK(3, 0), 0x0, 0x2, 0x2},\n\t{HPRE_CORE_NUM_CAP, 0x313c, 8, GENMASK(7, 0), 0x0, 0x8, 0xA},\n\t{HPRE_CLUSTER_CORE_NUM_CAP, 0x313c, 0, GENMASK(7, 0), 0x0, 0x2, 0xA},\n\t{HPRE_CORE_ENABLE_BITMAP_CAP, 0x3140, 0, GENMASK(31, 0), 0x0, 0xF, 0x3FF},\n\t{HPRE_DRV_ALG_BITMAP_CAP, 0x3144, 0, GENMASK(31, 0), 0x0, 0x03, 0x27},\n\t{HPRE_DEV_ALG_BITMAP_CAP, 0x3148, 0, GENMASK(31, 0), 0x0, 0x03, 0x7F},\n\t{HPRE_CORE1_ALG_BITMAP_CAP, 0x314c, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE2_ALG_BITMAP_CAP, 0x3150, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE3_ALG_BITMAP_CAP, 0x3154, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE4_ALG_BITMAP_CAP, 0x3158, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE5_ALG_BITMAP_CAP, 0x315c, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE6_ALG_BITMAP_CAP, 0x3160, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE7_ALG_BITMAP_CAP, 0x3164, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE8_ALG_BITMAP_CAP, 0x3168, 0, GENMASK(31, 0), 0x0, 0x7F, 0x7F},\n\t{HPRE_CORE9_ALG_BITMAP_CAP, 0x316c, 0, GENMASK(31, 0), 0x0, 0x10, 0x10},\n\t{HPRE_CORE10_ALG_BITMAP_CAP, 0x3170, 0, GENMASK(31, 0), 0x0, 0x10, 0x10}\n};\n\nenum hpre_pre_store_cap_idx {\n\tHPRE_CLUSTER_NUM_CAP_IDX = 0x0,\n\tHPRE_CORE_ENABLE_BITMAP_CAP_IDX,\n\tHPRE_DRV_ALG_BITMAP_CAP_IDX,\n\tHPRE_DEV_ALG_BITMAP_CAP_IDX,\n};\n\nstatic const u32 hpre_pre_store_caps[] = {\n\tHPRE_CLUSTER_NUM_CAP,\n\tHPRE_CORE_ENABLE_BITMAP_CAP,\n\tHPRE_DRV_ALG_BITMAP_CAP,\n\tHPRE_DEV_ALG_BITMAP_CAP,\n};\n\nstatic const struct hpre_hw_error hpre_hw_errors[] = {\n\t{\n\t\t.int_msk = BIT(0),\n\t\t.msg = \"core_ecc_1bit_err_int_set\"\n\t}, {\n\t\t.int_msk = BIT(1),\n\t\t.msg = \"core_ecc_2bit_err_int_set\"\n\t}, {\n\t\t.int_msk = BIT(2),\n\t\t.msg = \"dat_wb_poison_int_set\"\n\t}, {\n\t\t.int_msk = BIT(3),\n\t\t.msg = \"dat_rd_poison_int_set\"\n\t}, {\n\t\t.int_msk = BIT(4),\n\t\t.msg = \"bd_rd_poison_int_set\"\n\t}, {\n\t\t.int_msk = BIT(5),\n\t\t.msg = \"ooo_ecc_2bit_err_int_set\"\n\t}, {\n\t\t.int_msk = BIT(6),\n\t\t.msg = \"cluster1_shb_timeout_int_set\"\n\t}, {\n\t\t.int_msk = BIT(7),\n\t\t.msg = \"cluster2_shb_timeout_int_set\"\n\t}, {\n\t\t.int_msk = BIT(8),\n\t\t.msg = \"cluster3_shb_timeout_int_set\"\n\t}, {\n\t\t.int_msk = BIT(9),\n\t\t.msg = \"cluster4_shb_timeout_int_set\"\n\t}, {\n\t\t.int_msk = GENMASK(15, 10),\n\t\t.msg = \"ooo_rdrsp_err_int_set\"\n\t}, {\n\t\t.int_msk = GENMASK(21, 16),\n\t\t.msg = \"ooo_wrrsp_err_int_set\"\n\t}, {\n\t\t.int_msk = BIT(22),\n\t\t.msg = \"pt_rng_timeout_int_set\"\n\t}, {\n\t\t.int_msk = BIT(23),\n\t\t.msg = \"sva_fsm_timeout_int_set\"\n\t}, {\n\t\t.int_msk = BIT(24),\n\t\t.msg = \"sva_int_set\"\n\t}, {\n\t\t \n\t}\n};\n\nstatic const u64 hpre_cluster_offsets[] = {\n\t[HPRE_CLUSTER0] =\n\t\tHPRE_CLSTR_BASE + HPRE_CLUSTER0 * HPRE_CLSTR_ADDR_INTRVL,\n\t[HPRE_CLUSTER1] =\n\t\tHPRE_CLSTR_BASE + HPRE_CLUSTER1 * HPRE_CLSTR_ADDR_INTRVL,\n\t[HPRE_CLUSTER2] =\n\t\tHPRE_CLSTR_BASE + HPRE_CLUSTER2 * HPRE_CLSTR_ADDR_INTRVL,\n\t[HPRE_CLUSTER3] =\n\t\tHPRE_CLSTR_BASE + HPRE_CLUSTER3 * HPRE_CLSTR_ADDR_INTRVL,\n};\n\nstatic const struct debugfs_reg32 hpre_cluster_dfx_regs[] = {\n\t{\"CORES_EN_STATUS     \",  HPRE_CORE_EN_OFFSET},\n\t{\"CORES_INI_CFG       \",  HPRE_CORE_INI_CFG_OFFSET},\n\t{\"CORES_INI_STATUS    \",  HPRE_CORE_INI_STATUS_OFFSET},\n\t{\"CORES_HTBT_WARN     \",  HPRE_CORE_HTBT_WARN_OFFSET},\n\t{\"CORES_IS_SCHD       \",  HPRE_CORE_IS_SCHD_OFFSET},\n};\n\nstatic const struct debugfs_reg32 hpre_com_dfx_regs[] = {\n\t{\"READ_CLR_EN     \",  HPRE_CTRL_CNT_CLR_CE},\n\t{\"AXQOS           \",  HPRE_VFG_AXQOS},\n\t{\"AWUSR_CFG       \",  HPRE_AWUSR_FP_CFG},\n\t{\"BD_ENDIAN       \",  HPRE_BD_ENDIAN},\n\t{\"ECC_CHECK_CTRL  \",  HPRE_ECC_BYPASS},\n\t{\"RAS_INT_WIDTH   \",  HPRE_RAS_WIDTH_CFG},\n\t{\"POISON_BYPASS   \",  HPRE_POISON_BYPASS},\n\t{\"BD_ARUSER       \",  HPRE_BD_ARUSR_CFG},\n\t{\"BD_AWUSER       \",  HPRE_BD_AWUSR_CFG},\n\t{\"DATA_ARUSER     \",  HPRE_DATA_RUSER_CFG},\n\t{\"DATA_AWUSER     \",  HPRE_DATA_WUSER_CFG},\n\t{\"INT_STATUS      \",  HPRE_INT_STATUS},\n\t{\"INT_MASK        \",  HPRE_HAC_INT_MSK},\n\t{\"RAS_CE_ENB      \",  HPRE_HAC_RAS_CE_ENB},\n\t{\"RAS_NFE_ENB     \",  HPRE_HAC_RAS_NFE_ENB},\n\t{\"RAS_FE_ENB      \",  HPRE_HAC_RAS_FE_ENB},\n\t{\"INT_SET         \",  HPRE_HAC_INT_SET},\n\t{\"RNG_TIMEOUT_NUM \",  HPRE_RNG_TIMEOUT_NUM},\n};\n\nstatic const char *hpre_dfx_files[HPRE_DFX_FILE_NUM] = {\n\t\"send_cnt\",\n\t\"recv_cnt\",\n\t\"send_fail_cnt\",\n\t\"send_busy_cnt\",\n\t\"over_thrhld_cnt\",\n\t\"overtime_thrhld\",\n\t\"invalid_req_cnt\"\n};\n\n \nstatic struct dfx_diff_registers hpre_diff_regs[] = {\n\t{\n\t\t.reg_offset = HPRE_DFX_BASE,\n\t\t.reg_len = HPRE_DFX_BASE_LEN,\n\t}, {\n\t\t.reg_offset = HPRE_DFX_COMMON1,\n\t\t.reg_len = HPRE_DFX_COMMON1_LEN,\n\t}, {\n\t\t.reg_offset = HPRE_DFX_COMMON2,\n\t\t.reg_len = HPRE_DFX_COMMON2_LEN,\n\t}, {\n\t\t.reg_offset = HPRE_DFX_CORE,\n\t\t.reg_len = HPRE_DFX_CORE_LEN,\n\t},\n};\n\nbool hpre_check_alg_support(struct hisi_qm *qm, u32 alg)\n{\n\tu32 cap_val;\n\n\tcap_val = qm->cap_tables.dev_cap_table[HPRE_DRV_ALG_BITMAP_CAP_IDX].cap_val;\n\tif (alg & cap_val)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int hpre_diff_regs_show(struct seq_file *s, void *unused)\n{\n\tstruct hisi_qm *qm = s->private;\n\n\thisi_qm_acc_diff_regs_dump(qm, s, qm->debug.acc_diff_regs,\n\t\t\t\t\tARRAY_SIZE(hpre_diff_regs));\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(hpre_diff_regs);\n\nstatic int hpre_com_regs_show(struct seq_file *s, void *unused)\n{\n\thisi_qm_regs_dump(s, s->private);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(hpre_com_regs);\n\nstatic int hpre_cluster_regs_show(struct seq_file *s, void *unused)\n{\n\thisi_qm_regs_dump(s, s->private);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(hpre_cluster_regs);\n\nstatic const struct kernel_param_ops hpre_uacce_mode_ops = {\n\t.set = uacce_mode_set,\n\t.get = param_get_int,\n};\n\n \nstatic u32 uacce_mode = UACCE_MODE_NOUACCE;\nmodule_param_cb(uacce_mode, &hpre_uacce_mode_ops, &uacce_mode, 0444);\nMODULE_PARM_DESC(uacce_mode, UACCE_MODE_DESC);\n\nstatic bool pf_q_num_flag;\nstatic int pf_q_num_set(const char *val, const struct kernel_param *kp)\n{\n\tpf_q_num_flag = true;\n\n\treturn q_num_set(val, kp, PCI_DEVICE_ID_HUAWEI_HPRE_PF);\n}\n\nstatic const struct kernel_param_ops hpre_pf_q_num_ops = {\n\t.set = pf_q_num_set,\n\t.get = param_get_int,\n};\n\nstatic u32 pf_q_num = HPRE_PF_DEF_Q_NUM;\nmodule_param_cb(pf_q_num, &hpre_pf_q_num_ops, &pf_q_num, 0444);\nMODULE_PARM_DESC(pf_q_num, \"Number of queues in PF of CS(2-1024)\");\n\nstatic const struct kernel_param_ops vfs_num_ops = {\n\t.set = vfs_num_set,\n\t.get = param_get_int,\n};\n\nstatic u32 vfs_num;\nmodule_param_cb(vfs_num, &vfs_num_ops, &vfs_num, 0444);\nMODULE_PARM_DESC(vfs_num, \"Number of VFs to enable(1-63), 0(default)\");\n\nstruct hisi_qp *hpre_create_qp(u8 type)\n{\n\tint node = cpu_to_node(smp_processor_id());\n\tstruct hisi_qp *qp = NULL;\n\tint ret;\n\n\tif (type != HPRE_V2_ALG_TYPE && type != HPRE_V3_ECC_ALG_TYPE)\n\t\treturn NULL;\n\n\t \n\tret = hisi_qm_alloc_qps_node(&hpre_devices, 1, type, node, &qp);\n\tif (!ret)\n\t\treturn qp;\n\n\treturn NULL;\n}\n\nstatic void hpre_config_pasid(struct hisi_qm *qm)\n{\n\tu32 val1, val2;\n\n\tif (qm->ver >= QM_HW_V3)\n\t\treturn;\n\n\tval1 = readl_relaxed(qm->io_base + HPRE_DATA_RUSER_CFG);\n\tval2 = readl_relaxed(qm->io_base + HPRE_DATA_WUSER_CFG);\n\tif (qm->use_sva) {\n\t\tval1 |= BIT(HPRE_PASID_EN_BIT);\n\t\tval2 |= BIT(HPRE_PASID_EN_BIT);\n\t} else {\n\t\tval1 &= ~BIT(HPRE_PASID_EN_BIT);\n\t\tval2 &= ~BIT(HPRE_PASID_EN_BIT);\n\t}\n\twritel_relaxed(val1, qm->io_base + HPRE_DATA_RUSER_CFG);\n\twritel_relaxed(val2, qm->io_base + HPRE_DATA_WUSER_CFG);\n}\n\nstatic int hpre_cfg_by_dsm(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tunion acpi_object *obj;\n\tguid_t guid;\n\n\tif (guid_parse(\"b06b81ab-0134-4a45-9b0c-483447b95fa7\", &guid)) {\n\t\tdev_err(dev, \"Hpre GUID failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tobj = acpi_evaluate_dsm(ACPI_HANDLE(dev), &guid,\n\t\t\t\t0, HPRE_VIA_MSI_DSM, NULL);\n\tif (!obj) {\n\t\tdev_err(dev, \"ACPI handle failed!\\n\");\n\t\treturn -EIO;\n\t}\n\n\tACPI_FREE(obj);\n\n\treturn 0;\n}\n\nstatic int hpre_set_cluster(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tunsigned long offset;\n\tu32 cluster_core_mask;\n\tu8 clusters_num;\n\tu32 val = 0;\n\tint ret, i;\n\n\tcluster_core_mask = qm->cap_tables.dev_cap_table[HPRE_CORE_ENABLE_BITMAP_CAP_IDX].cap_val;\n\tclusters_num = qm->cap_tables.dev_cap_table[HPRE_CLUSTER_NUM_CAP_IDX].cap_val;\n\tfor (i = 0; i < clusters_num; i++) {\n\t\toffset = i * HPRE_CLSTR_ADDR_INTRVL;\n\n\t\t \n\t\twritel(cluster_core_mask,\n\t\t       qm->io_base + offset + HPRE_CORE_ENB);\n\t\twritel(0x1, qm->io_base + offset + HPRE_CORE_INI_CFG);\n\t\tret = readl_relaxed_poll_timeout(qm->io_base + offset +\n\t\t\t\t\tHPRE_CORE_INI_STATUS, val,\n\t\t\t\t\t((val & cluster_core_mask) ==\n\t\t\t\t\tcluster_core_mask),\n\t\t\t\t\tHPRE_REG_RD_INTVRL_US,\n\t\t\t\t\tHPRE_REG_RD_TMOUT_US);\n\t\tif (ret) {\n\t\t\tdev_err(dev,\n\t\t\t\t\"cluster %d int st status timeout!\\n\", i);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void disable_flr_of_bme(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\tval = readl(qm->io_base + QM_PEH_AXUSER_CFG);\n\tval &= ~(HPRE_QM_BME_FLR | HPRE_QM_SRIOV_FLR);\n\tval |= HPRE_QM_PM_FLR;\n\twritel(val, qm->io_base + QM_PEH_AXUSER_CFG);\n\twritel(PEH_AXUSER_CFG_ENABLE, qm->io_base + QM_PEH_AXUSER_CFG_ENABLE);\n}\n\nstatic void hpre_open_sva_prefetch(struct hisi_qm *qm)\n{\n\tu32 val;\n\tint ret;\n\n\tif (!test_bit(QM_SUPPORT_SVA_PREFETCH, &qm->caps))\n\t\treturn;\n\n\t \n\tval = readl_relaxed(qm->io_base + HPRE_PREFETCH_CFG);\n\tval &= HPRE_PREFETCH_ENABLE;\n\twritel(val, qm->io_base + HPRE_PREFETCH_CFG);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + HPRE_PREFETCH_CFG,\n\t\t\t\t\t val, !(val & HPRE_PREFETCH_DISABLE),\n\t\t\t\t\t HPRE_REG_RD_INTVRL_US,\n\t\t\t\t\t HPRE_REG_RD_TMOUT_US);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"failed to open sva prefetch\\n\");\n}\n\nstatic void hpre_close_sva_prefetch(struct hisi_qm *qm)\n{\n\tu32 val;\n\tint ret;\n\n\tif (!test_bit(QM_SUPPORT_SVA_PREFETCH, &qm->caps))\n\t\treturn;\n\n\tval = readl_relaxed(qm->io_base + HPRE_PREFETCH_CFG);\n\tval |= HPRE_PREFETCH_DISABLE;\n\twritel(val, qm->io_base + HPRE_PREFETCH_CFG);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + HPRE_SVA_PREFTCH_DFX,\n\t\t\t\t\t val, !(val & HPRE_SVA_DISABLE_READY),\n\t\t\t\t\t HPRE_REG_RD_INTVRL_US,\n\t\t\t\t\t HPRE_REG_RD_TMOUT_US);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"failed to close sva prefetch\\n\");\n}\n\nstatic void hpre_enable_clock_gate(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\tif (qm->ver < QM_HW_V3)\n\t\treturn;\n\n\tval = readl(qm->io_base + HPRE_CLKGATE_CTL);\n\tval |= HPRE_CLKGATE_CTL_EN;\n\twritel(val, qm->io_base + HPRE_CLKGATE_CTL);\n\n\tval = readl(qm->io_base + HPRE_PEH_CFG_AUTO_GATE);\n\tval |= HPRE_PEH_CFG_AUTO_GATE_EN;\n\twritel(val, qm->io_base + HPRE_PEH_CFG_AUTO_GATE);\n\n\tval = readl(qm->io_base + HPRE_CLUSTER_DYN_CTL);\n\tval |= HPRE_CLUSTER_DYN_CTL_EN;\n\twritel(val, qm->io_base + HPRE_CLUSTER_DYN_CTL);\n\n\tval = readl_relaxed(qm->io_base + HPRE_CORE_SHB_CFG);\n\tval |= HPRE_CORE_GATE_EN;\n\twritel(val, qm->io_base + HPRE_CORE_SHB_CFG);\n}\n\nstatic void hpre_disable_clock_gate(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\tif (qm->ver < QM_HW_V3)\n\t\treturn;\n\n\tval = readl(qm->io_base + HPRE_CLKGATE_CTL);\n\tval &= ~HPRE_CLKGATE_CTL_EN;\n\twritel(val, qm->io_base + HPRE_CLKGATE_CTL);\n\n\tval = readl(qm->io_base + HPRE_PEH_CFG_AUTO_GATE);\n\tval &= ~HPRE_PEH_CFG_AUTO_GATE_EN;\n\twritel(val, qm->io_base + HPRE_PEH_CFG_AUTO_GATE);\n\n\tval = readl(qm->io_base + HPRE_CLUSTER_DYN_CTL);\n\tval &= ~HPRE_CLUSTER_DYN_CTL_EN;\n\twritel(val, qm->io_base + HPRE_CLUSTER_DYN_CTL);\n\n\tval = readl_relaxed(qm->io_base + HPRE_CORE_SHB_CFG);\n\tval &= ~HPRE_CORE_GATE_EN;\n\twritel(val, qm->io_base + HPRE_CORE_SHB_CFG);\n}\n\nstatic int hpre_set_user_domain_and_cache(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tu32 val;\n\tint ret;\n\n\t \n\thpre_disable_clock_gate(qm);\n\n\twritel(HPRE_QM_USR_CFG_MASK, qm->io_base + QM_ARUSER_M_CFG_ENABLE);\n\twritel(HPRE_QM_USR_CFG_MASK, qm->io_base + QM_AWUSER_M_CFG_ENABLE);\n\twritel_relaxed(HPRE_QM_AXI_CFG_MASK, qm->io_base + QM_AXI_M_CFG);\n\n\t \n\tval = readl_relaxed(qm->io_base + HPRE_QM_ABNML_INT_MASK);\n\tval |= BIT(HPRE_TIMEOUT_ABNML_BIT);\n\twritel_relaxed(val, qm->io_base + HPRE_QM_ABNML_INT_MASK);\n\n\tif (qm->ver >= QM_HW_V3)\n\t\twritel(HPRE_RSA_ENB | HPRE_ECC_ENB,\n\t\t\tqm->io_base + HPRE_TYPES_ENB);\n\telse\n\t\twritel(HPRE_RSA_ENB, qm->io_base + HPRE_TYPES_ENB);\n\n\twritel(HPRE_QM_VFG_AX_MASK, qm->io_base + HPRE_VFG_AXCACHE);\n\twritel(0x0, qm->io_base + HPRE_BD_ENDIAN);\n\twritel(0x0, qm->io_base + HPRE_INT_MASK);\n\twritel(0x0, qm->io_base + HPRE_POISON_BYPASS);\n\twritel(0x0, qm->io_base + HPRE_COMM_CNT_CLR_CE);\n\twritel(0x0, qm->io_base + HPRE_ECC_BYPASS);\n\n\twritel(HPRE_BD_USR_MASK, qm->io_base + HPRE_BD_ARUSR_CFG);\n\twritel(HPRE_BD_USR_MASK, qm->io_base + HPRE_BD_AWUSR_CFG);\n\twritel(0x1, qm->io_base + HPRE_RDCHN_INI_CFG);\n\tret = readl_relaxed_poll_timeout(qm->io_base + HPRE_RDCHN_INI_ST, val,\n\t\t\tval & BIT(0),\n\t\t\tHPRE_REG_RD_INTVRL_US,\n\t\t\tHPRE_REG_RD_TMOUT_US);\n\tif (ret) {\n\t\tdev_err(dev, \"read rd channel timeout fail!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tret = hpre_set_cluster(qm);\n\tif (ret)\n\t\treturn -ETIMEDOUT;\n\n\t \n\tif (qm->ver == QM_HW_V2) {\n\t\tret = hpre_cfg_by_dsm(qm);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tdisable_flr_of_bme(qm);\n\t}\n\n\t \n\thpre_config_pasid(qm);\n\n\thpre_enable_clock_gate(qm);\n\n\treturn ret;\n}\n\nstatic void hpre_cnt_regs_clear(struct hisi_qm *qm)\n{\n\tunsigned long offset;\n\tu8 clusters_num;\n\tint i;\n\n\t \n\tclusters_num = qm->cap_tables.dev_cap_table[HPRE_CLUSTER_NUM_CAP_IDX].cap_val;\n\tfor (i = 0; i < clusters_num; i++) {\n\t\toffset = HPRE_CLSTR_BASE + i * HPRE_CLSTR_ADDR_INTRVL;\n\t\twritel(0x0, qm->io_base + offset + HPRE_CLUSTER_INQURY);\n\t}\n\n\t \n\twritel(0x0, qm->io_base + HPRE_CTRL_CNT_CLR_CE);\n\n\thisi_qm_debug_regs_clear(qm);\n}\n\nstatic void hpre_master_ooo_ctrl(struct hisi_qm *qm, bool enable)\n{\n\tu32 val1, val2;\n\n\tval1 = readl(qm->io_base + HPRE_AM_OOO_SHUTDOWN_ENB);\n\tif (enable) {\n\t\tval1 |= HPRE_AM_OOO_SHUTDOWN_ENABLE;\n\t\tval2 = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\t\t\t   HPRE_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\t} else {\n\t\tval1 &= ~HPRE_AM_OOO_SHUTDOWN_ENABLE;\n\t\tval2 = 0x0;\n\t}\n\n\tif (qm->ver > QM_HW_V2)\n\t\twritel(val2, qm->io_base + HPRE_OOO_SHUTDOWN_SEL);\n\n\twritel(val1, qm->io_base + HPRE_AM_OOO_SHUTDOWN_ENB);\n}\n\nstatic void hpre_hw_error_disable(struct hisi_qm *qm)\n{\n\tu32 ce, nfe;\n\n\tce = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_CE_MASK_CAP, qm->cap_ver);\n\tnfe = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_NFE_MASK_CAP, qm->cap_ver);\n\n\t \n\twritel(ce | nfe | HPRE_HAC_RAS_FE_ENABLE, qm->io_base + HPRE_INT_MASK);\n\t \n\thpre_master_ooo_ctrl(qm, false);\n}\n\nstatic void hpre_hw_error_enable(struct hisi_qm *qm)\n{\n\tu32 ce, nfe;\n\n\tce = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_CE_MASK_CAP, qm->cap_ver);\n\tnfe = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_NFE_MASK_CAP, qm->cap_ver);\n\n\t \n\twritel(ce | nfe | HPRE_HAC_RAS_FE_ENABLE, qm->io_base + HPRE_HAC_SOURCE_INT);\n\n\t \n\twritel(ce, qm->io_base + HPRE_RAS_CE_ENB);\n\twritel(nfe, qm->io_base + HPRE_RAS_NFE_ENB);\n\twritel(HPRE_HAC_RAS_FE_ENABLE, qm->io_base + HPRE_RAS_FE_ENB);\n\n\t \n\thpre_master_ooo_ctrl(qm, true);\n\n\t \n\twritel(HPRE_CORE_INT_ENABLE, qm->io_base + HPRE_INT_MASK);\n}\n\nstatic inline struct hisi_qm *hpre_file_to_qm(struct hpre_debugfs_file *file)\n{\n\tstruct hpre *hpre = container_of(file->debug, struct hpre, debug);\n\n\treturn &hpre->qm;\n}\n\nstatic u32 hpre_clear_enable_read(struct hpre_debugfs_file *file)\n{\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\n\treturn readl(qm->io_base + HPRE_CTRL_CNT_CLR_CE) &\n\t       HPRE_CTRL_CNT_CLR_CE_BIT;\n}\n\nstatic int hpre_clear_enable_write(struct hpre_debugfs_file *file, u32 val)\n{\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\tu32 tmp;\n\n\tif (val != 1 && val != 0)\n\t\treturn -EINVAL;\n\n\ttmp = (readl(qm->io_base + HPRE_CTRL_CNT_CLR_CE) &\n\t       ~HPRE_CTRL_CNT_CLR_CE_BIT) | val;\n\twritel(tmp, qm->io_base + HPRE_CTRL_CNT_CLR_CE);\n\n\treturn 0;\n}\n\nstatic u32 hpre_cluster_inqry_read(struct hpre_debugfs_file *file)\n{\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\tint cluster_index = file->index - HPRE_CLUSTER_CTRL;\n\tunsigned long offset = HPRE_CLSTR_BASE +\n\t\t\t       cluster_index * HPRE_CLSTR_ADDR_INTRVL;\n\n\treturn readl(qm->io_base + offset + HPRE_CLSTR_ADDR_INQRY_RSLT);\n}\n\nstatic void hpre_cluster_inqry_write(struct hpre_debugfs_file *file, u32 val)\n{\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\tint cluster_index = file->index - HPRE_CLUSTER_CTRL;\n\tunsigned long offset = HPRE_CLSTR_BASE + cluster_index *\n\t\t\t       HPRE_CLSTR_ADDR_INTRVL;\n\n\twritel(val, qm->io_base + offset + HPRE_CLUSTER_INQURY);\n}\n\nstatic ssize_t hpre_ctrl_debug_read(struct file *filp, char __user *buf,\n\t\t\t\t    size_t count, loff_t *pos)\n{\n\tstruct hpre_debugfs_file *file = filp->private_data;\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\tchar tbuf[HPRE_DBGFS_VAL_MAX_LEN];\n\tu32 val;\n\tint ret;\n\n\tret = hisi_qm_get_dfx_access(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irq(&file->lock);\n\tswitch (file->type) {\n\tcase HPRE_CLEAR_ENABLE:\n\t\tval = hpre_clear_enable_read(file);\n\t\tbreak;\n\tcase HPRE_CLUSTER_CTRL:\n\t\tval = hpre_cluster_inqry_read(file);\n\t\tbreak;\n\tdefault:\n\t\tgoto err_input;\n\t}\n\tspin_unlock_irq(&file->lock);\n\n\thisi_qm_put_dfx_access(qm);\n\tret = snprintf(tbuf, HPRE_DBGFS_VAL_MAX_LEN, \"%u\\n\", val);\n\treturn simple_read_from_buffer(buf, count, pos, tbuf, ret);\n\nerr_input:\n\tspin_unlock_irq(&file->lock);\n\thisi_qm_put_dfx_access(qm);\n\treturn -EINVAL;\n}\n\nstatic ssize_t hpre_ctrl_debug_write(struct file *filp, const char __user *buf,\n\t\t\t\t     size_t count, loff_t *pos)\n{\n\tstruct hpre_debugfs_file *file = filp->private_data;\n\tstruct hisi_qm *qm = hpre_file_to_qm(file);\n\tchar tbuf[HPRE_DBGFS_VAL_MAX_LEN];\n\tunsigned long val;\n\tint len, ret;\n\n\tif (*pos != 0)\n\t\treturn 0;\n\n\tif (count >= HPRE_DBGFS_VAL_MAX_LEN)\n\t\treturn -ENOSPC;\n\n\tlen = simple_write_to_buffer(tbuf, HPRE_DBGFS_VAL_MAX_LEN - 1,\n\t\t\t\t     pos, buf, count);\n\tif (len < 0)\n\t\treturn len;\n\n\ttbuf[len] = '\\0';\n\tif (kstrtoul(tbuf, 0, &val))\n\t\treturn -EFAULT;\n\n\tret = hisi_qm_get_dfx_access(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irq(&file->lock);\n\tswitch (file->type) {\n\tcase HPRE_CLEAR_ENABLE:\n\t\tret = hpre_clear_enable_write(file, val);\n\t\tif (ret)\n\t\t\tgoto err_input;\n\t\tbreak;\n\tcase HPRE_CLUSTER_CTRL:\n\t\thpre_cluster_inqry_write(file, val);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err_input;\n\t}\n\n\tret = count;\n\nerr_input:\n\tspin_unlock_irq(&file->lock);\n\thisi_qm_put_dfx_access(qm);\n\treturn ret;\n}\n\nstatic const struct file_operations hpre_ctrl_debug_fops = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = hpre_ctrl_debug_read,\n\t.write = hpre_ctrl_debug_write,\n};\n\nstatic int hpre_debugfs_atomic64_get(void *data, u64 *val)\n{\n\tstruct hpre_dfx *dfx_item = data;\n\n\t*val = atomic64_read(&dfx_item->value);\n\n\treturn 0;\n}\n\nstatic int hpre_debugfs_atomic64_set(void *data, u64 val)\n{\n\tstruct hpre_dfx *dfx_item = data;\n\tstruct hpre_dfx *hpre_dfx = NULL;\n\n\tif (dfx_item->type == HPRE_OVERTIME_THRHLD) {\n\t\thpre_dfx = dfx_item - HPRE_OVERTIME_THRHLD;\n\t\tatomic64_set(&hpre_dfx[HPRE_OVER_THRHLD_CNT].value, 0);\n\t} else if (val) {\n\t\treturn -EINVAL;\n\t}\n\n\tatomic64_set(&dfx_item->value, val);\n\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(hpre_atomic64_ops, hpre_debugfs_atomic64_get,\n\t\t\t hpre_debugfs_atomic64_set, \"%llu\\n\");\n\nstatic int hpre_create_debugfs_file(struct hisi_qm *qm, struct dentry *dir,\n\t\t\t\t    enum hpre_ctrl_dbgfs_file type, int indx)\n{\n\tstruct hpre *hpre = container_of(qm, struct hpre, qm);\n\tstruct hpre_debug *dbg = &hpre->debug;\n\tstruct dentry *file_dir;\n\n\tif (dir)\n\t\tfile_dir = dir;\n\telse\n\t\tfile_dir = qm->debug.debug_root;\n\n\tif (type >= HPRE_DEBUG_FILE_NUM)\n\t\treturn -EINVAL;\n\n\tspin_lock_init(&dbg->files[indx].lock);\n\tdbg->files[indx].debug = dbg;\n\tdbg->files[indx].type = type;\n\tdbg->files[indx].index = indx;\n\tdebugfs_create_file(hpre_debug_file_name[type], 0600, file_dir,\n\t\t\t    dbg->files + indx, &hpre_ctrl_debug_fops);\n\n\treturn 0;\n}\n\nstatic int hpre_pf_comm_regs_debugfs_init(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tstruct debugfs_regset32 *regset;\n\n\tregset = devm_kzalloc(dev, sizeof(*regset), GFP_KERNEL);\n\tif (!regset)\n\t\treturn -ENOMEM;\n\n\tregset->regs = hpre_com_dfx_regs;\n\tregset->nregs = ARRAY_SIZE(hpre_com_dfx_regs);\n\tregset->base = qm->io_base;\n\tregset->dev = dev;\n\n\tdebugfs_create_file(\"regs\", 0444, qm->debug.debug_root,\n\t\t\t    regset, &hpre_com_regs_fops);\n\n\treturn 0;\n}\n\nstatic int hpre_cluster_debugfs_init(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tchar buf[HPRE_DBGFS_VAL_MAX_LEN];\n\tstruct debugfs_regset32 *regset;\n\tstruct dentry *tmp_d;\n\tu8 clusters_num;\n\tint i, ret;\n\n\tclusters_num = qm->cap_tables.dev_cap_table[HPRE_CLUSTER_NUM_CAP_IDX].cap_val;\n\tfor (i = 0; i < clusters_num; i++) {\n\t\tret = snprintf(buf, HPRE_DBGFS_VAL_MAX_LEN, \"cluster%d\", i);\n\t\tif (ret >= HPRE_DBGFS_VAL_MAX_LEN)\n\t\t\treturn -EINVAL;\n\t\ttmp_d = debugfs_create_dir(buf, qm->debug.debug_root);\n\n\t\tregset = devm_kzalloc(dev, sizeof(*regset), GFP_KERNEL);\n\t\tif (!regset)\n\t\t\treturn -ENOMEM;\n\n\t\tregset->regs = hpre_cluster_dfx_regs;\n\t\tregset->nregs = ARRAY_SIZE(hpre_cluster_dfx_regs);\n\t\tregset->base = qm->io_base + hpre_cluster_offsets[i];\n\t\tregset->dev = dev;\n\n\t\tdebugfs_create_file(\"regs\", 0444, tmp_d, regset,\n\t\t\t\t    &hpre_cluster_regs_fops);\n\t\tret = hpre_create_debugfs_file(qm, tmp_d, HPRE_CLUSTER_CTRL,\n\t\t\t\t\t       i + HPRE_CLUSTER_CTRL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int hpre_ctrl_debug_init(struct hisi_qm *qm)\n{\n\tint ret;\n\n\tret = hpre_create_debugfs_file(qm, NULL, HPRE_CLEAR_ENABLE,\n\t\t\t\t       HPRE_CLEAR_ENABLE);\n\tif (ret)\n\t\treturn ret;\n\n\tret = hpre_pf_comm_regs_debugfs_init(qm);\n\tif (ret)\n\t\treturn ret;\n\n\treturn hpre_cluster_debugfs_init(qm);\n}\n\nstatic void hpre_dfx_debug_init(struct hisi_qm *qm)\n{\n\tstruct dfx_diff_registers *hpre_regs = qm->debug.acc_diff_regs;\n\tstruct hpre *hpre = container_of(qm, struct hpre, qm);\n\tstruct hpre_dfx *dfx = hpre->debug.dfx;\n\tstruct dentry *parent;\n\tint i;\n\n\tparent = debugfs_create_dir(\"hpre_dfx\", qm->debug.debug_root);\n\tfor (i = 0; i < HPRE_DFX_FILE_NUM; i++) {\n\t\tdfx[i].type = i;\n\t\tdebugfs_create_file(hpre_dfx_files[i], 0644, parent, &dfx[i],\n\t\t\t\t    &hpre_atomic64_ops);\n\t}\n\n\tif (qm->fun_type == QM_HW_PF && hpre_regs)\n\t\tdebugfs_create_file(\"diff_regs\", 0444, parent,\n\t\t\t\t      qm, &hpre_diff_regs_fops);\n}\n\nstatic int hpre_debugfs_init(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tint ret;\n\n\tqm->debug.debug_root = debugfs_create_dir(dev_name(dev),\n\t\t\t\t\t\t  hpre_debugfs_root);\n\n\tqm->debug.sqe_mask_offset = HPRE_SQE_MASK_OFFSET;\n\tqm->debug.sqe_mask_len = HPRE_SQE_MASK_LEN;\n\tret = hisi_qm_regs_debugfs_init(qm, hpre_diff_regs, ARRAY_SIZE(hpre_diff_regs));\n\tif (ret) {\n\t\tdev_warn(dev, \"Failed to init HPRE diff regs!\\n\");\n\t\tgoto debugfs_remove;\n\t}\n\n\thisi_qm_debug_init(qm);\n\n\tif (qm->pdev->device == PCI_DEVICE_ID_HUAWEI_HPRE_PF) {\n\t\tret = hpre_ctrl_debug_init(qm);\n\t\tif (ret)\n\t\t\tgoto failed_to_create;\n\t}\n\n\thpre_dfx_debug_init(qm);\n\n\treturn 0;\n\nfailed_to_create:\n\thisi_qm_regs_debugfs_uninit(qm, ARRAY_SIZE(hpre_diff_regs));\ndebugfs_remove:\n\tdebugfs_remove_recursive(qm->debug.debug_root);\n\treturn ret;\n}\n\nstatic void hpre_debugfs_exit(struct hisi_qm *qm)\n{\n\thisi_qm_regs_debugfs_uninit(qm, ARRAY_SIZE(hpre_diff_regs));\n\n\tdebugfs_remove_recursive(qm->debug.debug_root);\n}\n\nstatic int hpre_pre_store_cap_reg(struct hisi_qm *qm)\n{\n\tstruct hisi_qm_cap_record *hpre_cap;\n\tstruct device *dev = &qm->pdev->dev;\n\tsize_t i, size;\n\n\tsize = ARRAY_SIZE(hpre_pre_store_caps);\n\thpre_cap = devm_kzalloc(dev, sizeof(*hpre_cap) * size, GFP_KERNEL);\n\tif (!hpre_cap)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < size; i++) {\n\t\thpre_cap[i].type = hpre_pre_store_caps[i];\n\t\thpre_cap[i].cap_val = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\t\t      hpre_pre_store_caps[i], qm->cap_ver);\n\t}\n\n\tif (hpre_cap[HPRE_CLUSTER_NUM_CAP_IDX].cap_val > HPRE_CLUSTERS_NUM_MAX) {\n\t\tdev_err(dev, \"Device cluster num %u is out of range for driver supports %d!\\n\",\n\t\t\thpre_cap[HPRE_CLUSTER_NUM_CAP_IDX].cap_val, HPRE_CLUSTERS_NUM_MAX);\n\t\treturn -EINVAL;\n\t}\n\n\tqm->cap_tables.dev_cap_table = hpre_cap;\n\n\treturn 0;\n}\n\nstatic int hpre_qm_init(struct hisi_qm *qm, struct pci_dev *pdev)\n{\n\tu64 alg_msk;\n\tint ret;\n\n\tif (pdev->revision == QM_HW_V1) {\n\t\tpci_warn(pdev, \"HPRE version 1 is not supported!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tqm->mode = uacce_mode;\n\tqm->pdev = pdev;\n\tqm->ver = pdev->revision;\n\tqm->sqe_size = HPRE_SQE_SIZE;\n\tqm->dev_name = hpre_name;\n\n\tqm->fun_type = (pdev->device == PCI_DEVICE_ID_HUAWEI_HPRE_PF) ?\n\t\t\tQM_HW_PF : QM_HW_VF;\n\tif (qm->fun_type == QM_HW_PF) {\n\t\tqm->qp_base = HPRE_PF_DEF_Q_BASE;\n\t\tqm->qp_num = pf_q_num;\n\t\tqm->debug.curr_qm_qp_num = pf_q_num;\n\t\tqm->qm_list = &hpre_devices;\n\t\tif (pf_q_num_flag)\n\t\t\tset_bit(QM_MODULE_PARAM, &qm->misc_ctl);\n\t}\n\n\tret = hisi_qm_init(qm);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to init hpre qm configures!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = hpre_pre_store_cap_reg(qm);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to pre-store capability registers!\\n\");\n\t\thisi_qm_uninit(qm);\n\t\treturn ret;\n\t}\n\n\talg_msk = qm->cap_tables.dev_cap_table[HPRE_DEV_ALG_BITMAP_CAP_IDX].cap_val;\n\tret = hisi_qm_set_algs(qm, alg_msk, hpre_dev_algs, ARRAY_SIZE(hpre_dev_algs));\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to set hpre algs!\\n\");\n\t\thisi_qm_uninit(qm);\n\t}\n\n\treturn ret;\n}\n\nstatic int hpre_show_last_regs_init(struct hisi_qm *qm)\n{\n\tint cluster_dfx_regs_num =  ARRAY_SIZE(hpre_cluster_dfx_regs);\n\tint com_dfx_regs_num = ARRAY_SIZE(hpre_com_dfx_regs);\n\tstruct qm_debug *debug = &qm->debug;\n\tvoid __iomem *io_base;\n\tu8 clusters_num;\n\tint i, j, idx;\n\n\tclusters_num = qm->cap_tables.dev_cap_table[HPRE_CLUSTER_NUM_CAP_IDX].cap_val;\n\tdebug->last_words = kcalloc(cluster_dfx_regs_num * clusters_num +\n\t\t\tcom_dfx_regs_num, sizeof(unsigned int), GFP_KERNEL);\n\tif (!debug->last_words)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < com_dfx_regs_num; i++)\n\t\tdebug->last_words[i] = readl_relaxed(qm->io_base +\n\t\t\t\t\t\thpre_com_dfx_regs[i].offset);\n\n\tfor (i = 0; i < clusters_num; i++) {\n\t\tio_base = qm->io_base + hpre_cluster_offsets[i];\n\t\tfor (j = 0; j < cluster_dfx_regs_num; j++) {\n\t\t\tidx = com_dfx_regs_num + i * cluster_dfx_regs_num + j;\n\t\t\tdebug->last_words[idx] = readl_relaxed(\n\t\t\t\tio_base + hpre_cluster_dfx_regs[j].offset);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void hpre_show_last_regs_uninit(struct hisi_qm *qm)\n{\n\tstruct qm_debug *debug = &qm->debug;\n\n\tif (qm->fun_type == QM_HW_VF || !debug->last_words)\n\t\treturn;\n\n\tkfree(debug->last_words);\n\tdebug->last_words = NULL;\n}\n\nstatic void hpre_show_last_dfx_regs(struct hisi_qm *qm)\n{\n\tint cluster_dfx_regs_num =  ARRAY_SIZE(hpre_cluster_dfx_regs);\n\tint com_dfx_regs_num = ARRAY_SIZE(hpre_com_dfx_regs);\n\tstruct qm_debug *debug = &qm->debug;\n\tstruct pci_dev *pdev = qm->pdev;\n\tvoid __iomem *io_base;\n\tu8 clusters_num;\n\tint i, j, idx;\n\tu32 val;\n\n\tif (qm->fun_type == QM_HW_VF || !debug->last_words)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < com_dfx_regs_num; i++) {\n\t\tval = readl_relaxed(qm->io_base + hpre_com_dfx_regs[i].offset);\n\t\tif (debug->last_words[i] != val)\n\t\t\tpci_info(pdev, \"Common_core:%s \\t= 0x%08x => 0x%08x\\n\",\n\t\t\t  hpre_com_dfx_regs[i].name, debug->last_words[i], val);\n\t}\n\n\tclusters_num = qm->cap_tables.dev_cap_table[HPRE_CLUSTER_NUM_CAP_IDX].cap_val;\n\tfor (i = 0; i < clusters_num; i++) {\n\t\tio_base = qm->io_base + hpre_cluster_offsets[i];\n\t\tfor (j = 0; j <  cluster_dfx_regs_num; j++) {\n\t\t\tval = readl_relaxed(io_base +\n\t\t\t\t\t     hpre_cluster_dfx_regs[j].offset);\n\t\t\tidx = com_dfx_regs_num + i * cluster_dfx_regs_num + j;\n\t\t\tif (debug->last_words[idx] != val)\n\t\t\t\tpci_info(pdev, \"cluster-%d:%s \\t= 0x%08x => 0x%08x\\n\",\n\t\t\t\ti, hpre_cluster_dfx_regs[j].name, debug->last_words[idx], val);\n\t\t}\n\t}\n}\n\nstatic void hpre_log_hw_error(struct hisi_qm *qm, u32 err_sts)\n{\n\tconst struct hpre_hw_error *err = hpre_hw_errors;\n\tstruct device *dev = &qm->pdev->dev;\n\n\twhile (err->msg) {\n\t\tif (err->int_msk & err_sts)\n\t\t\tdev_warn(dev, \"%s [error status=0x%x] found\\n\",\n\t\t\t\t err->msg, err->int_msk);\n\t\terr++;\n\t}\n}\n\nstatic u32 hpre_get_hw_err_status(struct hisi_qm *qm)\n{\n\treturn readl(qm->io_base + HPRE_INT_STATUS);\n}\n\nstatic void hpre_clear_hw_err_status(struct hisi_qm *qm, u32 err_sts)\n{\n\tu32 nfe;\n\n\twritel(err_sts, qm->io_base + HPRE_HAC_SOURCE_INT);\n\tnfe = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_NFE_MASK_CAP, qm->cap_ver);\n\twritel(nfe, qm->io_base + HPRE_RAS_NFE_ENB);\n}\n\nstatic void hpre_open_axi_master_ooo(struct hisi_qm *qm)\n{\n\tu32 value;\n\n\tvalue = readl(qm->io_base + HPRE_AM_OOO_SHUTDOWN_ENB);\n\twritel(value & ~HPRE_AM_OOO_SHUTDOWN_ENABLE,\n\t       qm->io_base + HPRE_AM_OOO_SHUTDOWN_ENB);\n\twritel(value | HPRE_AM_OOO_SHUTDOWN_ENABLE,\n\t       qm->io_base + HPRE_AM_OOO_SHUTDOWN_ENB);\n}\n\nstatic void hpre_err_info_init(struct hisi_qm *qm)\n{\n\tstruct hisi_qm_err_info *err_info = &qm->err_info;\n\n\terr_info->fe = HPRE_HAC_RAS_FE_ENABLE;\n\terr_info->ce = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_QM_CE_MASK_CAP, qm->cap_ver);\n\terr_info->nfe = hisi_qm_get_hw_info(qm, hpre_basic_info, HPRE_QM_NFE_MASK_CAP, qm->cap_ver);\n\terr_info->ecc_2bits_mask = HPRE_CORE_ECC_2BIT_ERR | HPRE_OOO_ECC_2BIT_ERR;\n\terr_info->dev_shutdown_mask = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\tHPRE_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\terr_info->qm_shutdown_mask = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\tHPRE_QM_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\terr_info->qm_reset_mask = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\tHPRE_QM_RESET_MASK_CAP, qm->cap_ver);\n\terr_info->dev_reset_mask = hisi_qm_get_hw_info(qm, hpre_basic_info,\n\t\t\tHPRE_RESET_MASK_CAP, qm->cap_ver);\n\terr_info->msi_wr_port = HPRE_WR_MSI_PORT;\n\terr_info->acpi_rst = \"HRST\";\n}\n\nstatic const struct hisi_qm_err_ini hpre_err_ini = {\n\t.hw_init\t\t= hpre_set_user_domain_and_cache,\n\t.hw_err_enable\t\t= hpre_hw_error_enable,\n\t.hw_err_disable\t\t= hpre_hw_error_disable,\n\t.get_dev_hw_err_status\t= hpre_get_hw_err_status,\n\t.clear_dev_hw_err_status = hpre_clear_hw_err_status,\n\t.log_dev_hw_err\t\t= hpre_log_hw_error,\n\t.open_axi_master_ooo\t= hpre_open_axi_master_ooo,\n\t.open_sva_prefetch\t= hpre_open_sva_prefetch,\n\t.close_sva_prefetch\t= hpre_close_sva_prefetch,\n\t.show_last_dfx_regs\t= hpre_show_last_dfx_regs,\n\t.err_info_init\t\t= hpre_err_info_init,\n};\n\nstatic int hpre_pf_probe_init(struct hpre *hpre)\n{\n\tstruct hisi_qm *qm = &hpre->qm;\n\tint ret;\n\n\tret = hpre_set_user_domain_and_cache(qm);\n\tif (ret)\n\t\treturn ret;\n\n\thpre_open_sva_prefetch(qm);\n\n\tqm->err_ini = &hpre_err_ini;\n\tqm->err_ini->err_info_init(qm);\n\thisi_qm_dev_err_init(qm);\n\tret = hpre_show_last_regs_init(qm);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"Failed to init last word regs!\\n\");\n\n\treturn ret;\n}\n\nstatic int hpre_probe_init(struct hpre *hpre)\n{\n\tu32 type_rate = HPRE_SHAPER_TYPE_RATE;\n\tstruct hisi_qm *qm = &hpre->qm;\n\tint ret;\n\n\tif (qm->fun_type == QM_HW_PF) {\n\t\tret = hpre_pf_probe_init(hpre);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t \n\t\tif (qm->ver >= QM_HW_V3) {\n\t\t\ttype_rate |= QM_SHAPER_ENABLE;\n\t\t\tqm->type_rate = type_rate;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int hpre_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct hisi_qm *qm;\n\tstruct hpre *hpre;\n\tint ret;\n\n\thpre = devm_kzalloc(&pdev->dev, sizeof(*hpre), GFP_KERNEL);\n\tif (!hpre)\n\t\treturn -ENOMEM;\n\n\tqm = &hpre->qm;\n\tret = hpre_qm_init(qm, pdev);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to init HPRE QM (%d)!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hpre_probe_init(hpre);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to probe (%d)!\\n\", ret);\n\t\tgoto err_with_qm_init;\n\t}\n\n\tret = hisi_qm_start(qm);\n\tif (ret)\n\t\tgoto err_with_err_init;\n\n\tret = hpre_debugfs_init(qm);\n\tif (ret)\n\t\tdev_warn(&pdev->dev, \"init debugfs fail!\\n\");\n\n\tret = hisi_qm_alg_register(qm, &hpre_devices);\n\tif (ret < 0) {\n\t\tpci_err(pdev, \"fail to register algs to crypto!\\n\");\n\t\tgoto err_with_qm_start;\n\t}\n\n\tif (qm->uacce) {\n\t\tret = uacce_register(qm->uacce);\n\t\tif (ret) {\n\t\t\tpci_err(pdev, \"failed to register uacce (%d)!\\n\", ret);\n\t\t\tgoto err_with_alg_register;\n\t\t}\n\t}\n\n\tif (qm->fun_type == QM_HW_PF && vfs_num) {\n\t\tret = hisi_qm_sriov_enable(pdev, vfs_num);\n\t\tif (ret < 0)\n\t\t\tgoto err_with_alg_register;\n\t}\n\n\thisi_qm_pm_init(qm);\n\n\treturn 0;\n\nerr_with_alg_register:\n\thisi_qm_alg_unregister(qm, &hpre_devices);\n\nerr_with_qm_start:\n\thpre_debugfs_exit(qm);\n\thisi_qm_stop(qm, QM_NORMAL);\n\nerr_with_err_init:\n\thpre_show_last_regs_uninit(qm);\n\thisi_qm_dev_err_uninit(qm);\n\nerr_with_qm_init:\n\thisi_qm_uninit(qm);\n\n\treturn ret;\n}\n\nstatic void hpre_remove(struct pci_dev *pdev)\n{\n\tstruct hisi_qm *qm = pci_get_drvdata(pdev);\n\n\thisi_qm_pm_uninit(qm);\n\thisi_qm_wait_task_finish(qm, &hpre_devices);\n\thisi_qm_alg_unregister(qm, &hpre_devices);\n\tif (qm->fun_type == QM_HW_PF && qm->vfs_num)\n\t\thisi_qm_sriov_disable(pdev, true);\n\n\thpre_debugfs_exit(qm);\n\thisi_qm_stop(qm, QM_NORMAL);\n\n\tif (qm->fun_type == QM_HW_PF) {\n\t\thpre_cnt_regs_clear(qm);\n\t\tqm->debug.curr_qm_qp_num = 0;\n\t\thpre_show_last_regs_uninit(qm);\n\t\thisi_qm_dev_err_uninit(qm);\n\t}\n\n\thisi_qm_uninit(qm);\n}\n\nstatic const struct dev_pm_ops hpre_pm_ops = {\n\tSET_RUNTIME_PM_OPS(hisi_qm_suspend, hisi_qm_resume, NULL)\n};\n\nstatic const struct pci_error_handlers hpre_err_handler = {\n\t.error_detected\t\t= hisi_qm_dev_err_detected,\n\t.slot_reset\t\t= hisi_qm_dev_slot_reset,\n\t.reset_prepare\t\t= hisi_qm_reset_prepare,\n\t.reset_done\t\t= hisi_qm_reset_done,\n};\n\nstatic struct pci_driver hpre_pci_driver = {\n\t.name\t\t\t= hpre_name,\n\t.id_table\t\t= hpre_dev_ids,\n\t.probe\t\t\t= hpre_probe,\n\t.remove\t\t\t= hpre_remove,\n\t.sriov_configure\t= IS_ENABLED(CONFIG_PCI_IOV) ?\n\t\t\t\t  hisi_qm_sriov_configure : NULL,\n\t.err_handler\t\t= &hpre_err_handler,\n\t.shutdown\t\t= hisi_qm_dev_shutdown,\n\t.driver.pm\t\t= &hpre_pm_ops,\n};\n\nstruct pci_driver *hisi_hpre_get_pf_driver(void)\n{\n\treturn &hpre_pci_driver;\n}\nEXPORT_SYMBOL_GPL(hisi_hpre_get_pf_driver);\n\nstatic void hpre_register_debugfs(void)\n{\n\tif (!debugfs_initialized())\n\t\treturn;\n\n\thpre_debugfs_root = debugfs_create_dir(hpre_name, NULL);\n}\n\nstatic void hpre_unregister_debugfs(void)\n{\n\tdebugfs_remove_recursive(hpre_debugfs_root);\n}\n\nstatic int __init hpre_init(void)\n{\n\tint ret;\n\n\thisi_qm_init_list(&hpre_devices);\n\thpre_register_debugfs();\n\n\tret = pci_register_driver(&hpre_pci_driver);\n\tif (ret) {\n\t\thpre_unregister_debugfs();\n\t\tpr_err(\"hpre: can't register hisi hpre driver.\\n\");\n\t}\n\n\treturn ret;\n}\n\nstatic void __exit hpre_exit(void)\n{\n\tpci_unregister_driver(&hpre_pci_driver);\n\thpre_unregister_debugfs();\n}\n\nmodule_init(hpre_init);\nmodule_exit(hpre_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Zaibo Xu <xuzaibo@huawei.com>\");\nMODULE_AUTHOR(\"Meng Yu <yumeng18@huawei.com>\");\nMODULE_DESCRIPTION(\"Driver for HiSilicon HPRE accelerator\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}