{
  "module_name": "sec_main.c",
  "hash_id": "9ffa6174d899d3d391007a8c6e71b2890dea8e72ec51944411684c0ae52d1ca3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/hisilicon/sec2/sec_main.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/bitops.h>\n#include <linux/debugfs.h>\n#include <linux/init.h>\n#include <linux/io.h>\n#include <linux/iommu.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/seq_file.h>\n#include <linux/topology.h>\n#include <linux/uacce.h>\n\n#include \"sec.h\"\n\n#define SEC_VF_NUM\t\t\t63\n#define SEC_QUEUE_NUM_V1\t\t4096\n#define PCI_DEVICE_ID_HUAWEI_SEC_PF\t0xa255\n\n#define SEC_BD_ERR_CHK_EN0\t\t0xEFFFFFFF\n#define SEC_BD_ERR_CHK_EN1\t\t0x7ffff7fd\n#define SEC_BD_ERR_CHK_EN3\t\t0xffffbfff\n\n#define SEC_SQE_SIZE\t\t\t128\n#define SEC_PF_DEF_Q_NUM\t\t256\n#define SEC_PF_DEF_Q_BASE\t\t0\n#define SEC_CTX_Q_NUM_DEF\t\t2\n#define SEC_CTX_Q_NUM_MAX\t\t32\n\n#define SEC_CTRL_CNT_CLR_CE\t\t0x301120\n#define SEC_CTRL_CNT_CLR_CE_BIT\tBIT(0)\n#define SEC_CORE_INT_SOURCE\t\t0x301010\n#define SEC_CORE_INT_MASK\t\t0x301000\n#define SEC_CORE_INT_STATUS\t\t0x301008\n#define SEC_CORE_SRAM_ECC_ERR_INFO\t0x301C14\n#define SEC_ECC_NUM\t\t\t16\n#define SEC_ECC_MASH\t\t\t0xFF\n#define SEC_CORE_INT_DISABLE\t\t0x0\n\n#define SEC_RAS_CE_REG\t\t\t0x301050\n#define SEC_RAS_FE_REG\t\t\t0x301054\n#define SEC_RAS_NFE_REG\t\t\t0x301058\n#define SEC_RAS_FE_ENB_MSK\t\t0x0\n#define SEC_OOO_SHUTDOWN_SEL\t\t0x301014\n#define SEC_RAS_DISABLE\t\t0x0\n#define SEC_MEM_START_INIT_REG\t0x301100\n#define SEC_MEM_INIT_DONE_REG\t\t0x301104\n\n \n#define SEC_CONTROL_REG\t\t0x301200\n#define SEC_DYNAMIC_GATE_REG\t\t0x30121c\n#define SEC_CORE_AUTO_GATE\t\t0x30212c\n#define SEC_DYNAMIC_GATE_EN\t\t0x7fff\n#define SEC_CORE_AUTO_GATE_EN\t\tGENMASK(3, 0)\n#define SEC_CLK_GATE_ENABLE\t\tBIT(3)\n#define SEC_CLK_GATE_DISABLE\t\t(~BIT(3))\n\n#define SEC_TRNG_EN_SHIFT\t\t8\n#define SEC_AXI_SHUTDOWN_ENABLE\tBIT(12)\n#define SEC_AXI_SHUTDOWN_DISABLE\t0xFFFFEFFF\n\n#define SEC_INTERFACE_USER_CTRL0_REG\t0x301220\n#define SEC_INTERFACE_USER_CTRL1_REG\t0x301224\n#define SEC_SAA_EN_REG\t\t\t0x301270\n#define SEC_BD_ERR_CHK_EN_REG0\t\t0x301380\n#define SEC_BD_ERR_CHK_EN_REG1\t\t0x301384\n#define SEC_BD_ERR_CHK_EN_REG3\t\t0x30138c\n\n#define SEC_USER0_SMMU_NORMAL\t\t(BIT(23) | BIT(15))\n#define SEC_USER1_SMMU_NORMAL\t\t(BIT(31) | BIT(23) | BIT(15) | BIT(7))\n#define SEC_USER1_ENABLE_CONTEXT_SSV\tBIT(24)\n#define SEC_USER1_ENABLE_DATA_SSV\tBIT(16)\n#define SEC_USER1_WB_CONTEXT_SSV\tBIT(8)\n#define SEC_USER1_WB_DATA_SSV\t\tBIT(0)\n#define SEC_USER1_SVA_SET\t\t(SEC_USER1_ENABLE_CONTEXT_SSV | \\\n\t\t\t\t\tSEC_USER1_ENABLE_DATA_SSV | \\\n\t\t\t\t\tSEC_USER1_WB_CONTEXT_SSV |  \\\n\t\t\t\t\tSEC_USER1_WB_DATA_SSV)\n#define SEC_USER1_SMMU_SVA\t\t(SEC_USER1_SMMU_NORMAL | SEC_USER1_SVA_SET)\n#define SEC_USER1_SMMU_MASK\t\t(~SEC_USER1_SVA_SET)\n#define SEC_INTERFACE_USER_CTRL0_REG_V3\t0x302220\n#define SEC_INTERFACE_USER_CTRL1_REG_V3\t0x302224\n#define SEC_USER1_SMMU_NORMAL_V3\t(BIT(23) | BIT(17) | BIT(11) | BIT(5))\n#define SEC_USER1_SMMU_MASK_V3\t\t0xFF79E79E\n#define SEC_CORE_INT_STATUS_M_ECC\tBIT(2)\n\n#define SEC_PREFETCH_CFG\t\t0x301130\n#define SEC_SVA_TRANS\t\t\t0x301EC4\n#define SEC_PREFETCH_ENABLE\t\t(~(BIT(0) | BIT(1) | BIT(11)))\n#define SEC_PREFETCH_DISABLE\t\tBIT(1)\n#define SEC_SVA_DISABLE_READY\t\t(BIT(7) | BIT(11))\n\n#define SEC_DELAY_10_US\t\t\t10\n#define SEC_POLL_TIMEOUT_US\t\t1000\n#define SEC_DBGFS_VAL_MAX_LEN\t\t20\n#define SEC_SINGLE_PORT_MAX_TRANS\t0x2060\n\n#define SEC_SQE_MASK_OFFSET\t\t64\n#define SEC_SQE_MASK_LEN\t\t48\n#define SEC_SHAPER_TYPE_RATE\t\t400\n\n#define SEC_DFX_BASE\t\t0x301000\n#define SEC_DFX_CORE\t\t0x302100\n#define SEC_DFX_COMMON1\t\t0x301600\n#define SEC_DFX_COMMON2\t\t0x301C00\n#define SEC_DFX_BASE_LEN\t\t0x9D\n#define SEC_DFX_CORE_LEN\t\t0x32B\n#define SEC_DFX_COMMON1_LEN\t\t0x45\n#define SEC_DFX_COMMON2_LEN\t\t0xBA\n\n#define SEC_ALG_BITMAP_SHIFT\t\t32\n\n#define SEC_CIPHER_BITMAP\t\t(GENMASK_ULL(5, 0) | GENMASK_ULL(16, 12) | \\\n\t\t\t\t\tGENMASK(24, 21))\n#define SEC_DIGEST_BITMAP\t\t(GENMASK_ULL(11, 8) | GENMASK_ULL(20, 19) | \\\n\t\t\t\t\tGENMASK_ULL(42, 25))\n#define SEC_AEAD_BITMAP\t\t\t(GENMASK_ULL(7, 6) | GENMASK_ULL(18, 17) | \\\n\t\t\t\t\tGENMASK_ULL(45, 43))\n\nstruct sec_hw_error {\n\tu32 int_msk;\n\tconst char *msg;\n};\n\nstruct sec_dfx_item {\n\tconst char *name;\n\tu32 offset;\n};\n\nstatic const char sec_name[] = \"hisi_sec2\";\nstatic struct dentry *sec_debugfs_root;\n\nstatic struct hisi_qm_list sec_devices = {\n\t.register_to_crypto\t= sec_register_to_crypto,\n\t.unregister_from_crypto\t= sec_unregister_from_crypto,\n};\n\nstatic const struct hisi_qm_cap_info sec_basic_info[] = {\n\t{SEC_QM_NFE_MASK_CAP,   0x3124, 0, GENMASK(31, 0), 0x0, 0x1C77, 0x7C77},\n\t{SEC_QM_RESET_MASK_CAP, 0x3128, 0, GENMASK(31, 0), 0x0, 0xC77, 0x6C77},\n\t{SEC_QM_OOO_SHUTDOWN_MASK_CAP, 0x3128, 0, GENMASK(31, 0), 0x0, 0x4, 0x6C77},\n\t{SEC_QM_CE_MASK_CAP,    0x312C, 0, GENMASK(31, 0), 0x0, 0x8, 0x8},\n\t{SEC_NFE_MASK_CAP,      0x3130, 0, GENMASK(31, 0), 0x0, 0x177, 0x60177},\n\t{SEC_RESET_MASK_CAP,    0x3134, 0, GENMASK(31, 0), 0x0, 0x177, 0x177},\n\t{SEC_OOO_SHUTDOWN_MASK_CAP, 0x3134, 0, GENMASK(31, 0), 0x0, 0x4, 0x177},\n\t{SEC_CE_MASK_CAP,       0x3138, 0, GENMASK(31, 0), 0x0, 0x88, 0xC088},\n\t{SEC_CLUSTER_NUM_CAP, 0x313c, 20, GENMASK(3, 0), 0x1, 0x1, 0x1},\n\t{SEC_CORE_TYPE_NUM_CAP, 0x313c, 16, GENMASK(3, 0), 0x1, 0x1, 0x1},\n\t{SEC_CORE_NUM_CAP, 0x313c, 8, GENMASK(7, 0), 0x4, 0x4, 0x4},\n\t{SEC_CORES_PER_CLUSTER_NUM_CAP, 0x313c, 0, GENMASK(7, 0), 0x4, 0x4, 0x4},\n\t{SEC_CORE_ENABLE_BITMAP, 0x3140, 32, GENMASK(31, 0), 0x17F, 0x17F, 0xF},\n\t{SEC_DRV_ALG_BITMAP_LOW, 0x3144, 0, GENMASK(31, 0), 0x18050CB, 0x18050CB, 0x187F0FF},\n\t{SEC_DRV_ALG_BITMAP_HIGH, 0x3148, 0, GENMASK(31, 0), 0x395C, 0x395C, 0x395C},\n\t{SEC_DEV_ALG_BITMAP_LOW, 0x314c, 0, GENMASK(31, 0), 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF},\n\t{SEC_DEV_ALG_BITMAP_HIGH, 0x3150, 0, GENMASK(31, 0), 0x3FFF, 0x3FFF, 0x3FFF},\n\t{SEC_CORE1_ALG_BITMAP_LOW, 0x3154, 0, GENMASK(31, 0), 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF},\n\t{SEC_CORE1_ALG_BITMAP_HIGH, 0x3158, 0, GENMASK(31, 0), 0x3FFF, 0x3FFF, 0x3FFF},\n\t{SEC_CORE2_ALG_BITMAP_LOW, 0x315c, 0, GENMASK(31, 0), 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF},\n\t{SEC_CORE2_ALG_BITMAP_HIGH, 0x3160, 0, GENMASK(31, 0), 0x3FFF, 0x3FFF, 0x3FFF},\n\t{SEC_CORE3_ALG_BITMAP_LOW, 0x3164, 0, GENMASK(31, 0), 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF},\n\t{SEC_CORE3_ALG_BITMAP_HIGH, 0x3168, 0, GENMASK(31, 0), 0x3FFF, 0x3FFF, 0x3FFF},\n\t{SEC_CORE4_ALG_BITMAP_LOW, 0x316c, 0, GENMASK(31, 0), 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF},\n\t{SEC_CORE4_ALG_BITMAP_HIGH, 0x3170, 0, GENMASK(31, 0), 0x3FFF, 0x3FFF, 0x3FFF},\n};\n\nstatic const u32 sec_pre_store_caps[] = {\n\tSEC_DRV_ALG_BITMAP_LOW,\n\tSEC_DRV_ALG_BITMAP_HIGH,\n\tSEC_DEV_ALG_BITMAP_LOW,\n\tSEC_DEV_ALG_BITMAP_HIGH,\n};\n\nstatic const struct qm_dev_alg sec_dev_algs[] = { {\n\t\t.alg_msk = SEC_CIPHER_BITMAP,\n\t\t.alg = \"cipher\\n\",\n\t}, {\n\t\t.alg_msk = SEC_DIGEST_BITMAP,\n\t\t.alg = \"digest\\n\",\n\t}, {\n\t\t.alg_msk = SEC_AEAD_BITMAP,\n\t\t.alg = \"aead\\n\",\n\t},\n};\n\nstatic const struct sec_hw_error sec_hw_errors[] = {\n\t{\n\t\t.int_msk = BIT(0),\n\t\t.msg = \"sec_axi_rresp_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(1),\n\t\t.msg = \"sec_axi_bresp_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(2),\n\t\t.msg = \"sec_ecc_2bit_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(3),\n\t\t.msg = \"sec_ecc_1bit_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(4),\n\t\t.msg = \"sec_req_trng_timeout_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(5),\n\t\t.msg = \"sec_fsm_hbeat_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(6),\n\t\t.msg = \"sec_channel_req_rng_timeout_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(7),\n\t\t.msg = \"sec_bd_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(8),\n\t\t.msg = \"sec_chain_buff_err_rint\"\n\t},\n\t{\n\t\t.int_msk = BIT(14),\n\t\t.msg = \"sec_no_secure_access\"\n\t},\n\t{\n\t\t.int_msk = BIT(15),\n\t\t.msg = \"sec_wrapping_key_auth_err\"\n\t},\n\t{\n\t\t.int_msk = BIT(16),\n\t\t.msg = \"sec_km_key_crc_fail\"\n\t},\n\t{\n\t\t.int_msk = BIT(17),\n\t\t.msg = \"sec_axi_poison_err\"\n\t},\n\t{\n\t\t.int_msk = BIT(18),\n\t\t.msg = \"sec_sva_err\"\n\t},\n\t{}\n};\n\nstatic const char * const sec_dbg_file_name[] = {\n\t[SEC_CLEAR_ENABLE] = \"clear_enable\",\n};\n\nstatic struct sec_dfx_item sec_dfx_labels[] = {\n\t{\"send_cnt\", offsetof(struct sec_dfx, send_cnt)},\n\t{\"recv_cnt\", offsetof(struct sec_dfx, recv_cnt)},\n\t{\"send_busy_cnt\", offsetof(struct sec_dfx, send_busy_cnt)},\n\t{\"recv_busy_cnt\", offsetof(struct sec_dfx, recv_busy_cnt)},\n\t{\"err_bd_cnt\", offsetof(struct sec_dfx, err_bd_cnt)},\n\t{\"invalid_req_cnt\", offsetof(struct sec_dfx, invalid_req_cnt)},\n\t{\"done_flag_cnt\", offsetof(struct sec_dfx, done_flag_cnt)},\n};\n\nstatic const struct debugfs_reg32 sec_dfx_regs[] = {\n\t{\"SEC_PF_ABNORMAL_INT_SOURCE    \",  0x301010},\n\t{\"SEC_SAA_EN                    \",  0x301270},\n\t{\"SEC_BD_LATENCY_MIN            \",  0x301600},\n\t{\"SEC_BD_LATENCY_MAX            \",  0x301608},\n\t{\"SEC_BD_LATENCY_AVG            \",  0x30160C},\n\t{\"SEC_BD_NUM_IN_SAA0            \",  0x301670},\n\t{\"SEC_BD_NUM_IN_SAA1            \",  0x301674},\n\t{\"SEC_BD_NUM_IN_SEC             \",  0x301680},\n\t{\"SEC_ECC_1BIT_CNT              \",  0x301C00},\n\t{\"SEC_ECC_1BIT_INFO             \",  0x301C04},\n\t{\"SEC_ECC_2BIT_CNT              \",  0x301C10},\n\t{\"SEC_ECC_2BIT_INFO             \",  0x301C14},\n\t{\"SEC_BD_SAA0                   \",  0x301C20},\n\t{\"SEC_BD_SAA1                   \",  0x301C24},\n\t{\"SEC_BD_SAA2                   \",  0x301C28},\n\t{\"SEC_BD_SAA3                   \",  0x301C2C},\n\t{\"SEC_BD_SAA4                   \",  0x301C30},\n\t{\"SEC_BD_SAA5                   \",  0x301C34},\n\t{\"SEC_BD_SAA6                   \",  0x301C38},\n\t{\"SEC_BD_SAA7                   \",  0x301C3C},\n\t{\"SEC_BD_SAA8                   \",  0x301C40},\n};\n\n \nstatic struct dfx_diff_registers sec_diff_regs[] = {\n\t{\n\t\t.reg_offset = SEC_DFX_BASE,\n\t\t.reg_len = SEC_DFX_BASE_LEN,\n\t}, {\n\t\t.reg_offset = SEC_DFX_COMMON1,\n\t\t.reg_len = SEC_DFX_COMMON1_LEN,\n\t}, {\n\t\t.reg_offset = SEC_DFX_COMMON2,\n\t\t.reg_len = SEC_DFX_COMMON2_LEN,\n\t}, {\n\t\t.reg_offset = SEC_DFX_CORE,\n\t\t.reg_len = SEC_DFX_CORE_LEN,\n\t},\n};\n\nstatic int sec_diff_regs_show(struct seq_file *s, void *unused)\n{\n\tstruct hisi_qm *qm = s->private;\n\n\thisi_qm_acc_diff_regs_dump(qm, s, qm->debug.acc_diff_regs,\n\t\t\t\t\tARRAY_SIZE(sec_diff_regs));\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(sec_diff_regs);\n\nstatic bool pf_q_num_flag;\nstatic int sec_pf_q_num_set(const char *val, const struct kernel_param *kp)\n{\n\tpf_q_num_flag = true;\n\n\treturn q_num_set(val, kp, PCI_DEVICE_ID_HUAWEI_SEC_PF);\n}\n\nstatic const struct kernel_param_ops sec_pf_q_num_ops = {\n\t.set = sec_pf_q_num_set,\n\t.get = param_get_int,\n};\n\nstatic u32 pf_q_num = SEC_PF_DEF_Q_NUM;\nmodule_param_cb(pf_q_num, &sec_pf_q_num_ops, &pf_q_num, 0444);\nMODULE_PARM_DESC(pf_q_num, \"Number of queues in PF(v1 2-4096, v2 2-1024)\");\n\nstatic int sec_ctx_q_num_set(const char *val, const struct kernel_param *kp)\n{\n\tu32 ctx_q_num;\n\tint ret;\n\n\tif (!val)\n\t\treturn -EINVAL;\n\n\tret = kstrtou32(val, 10, &ctx_q_num);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\tif (!ctx_q_num || ctx_q_num > SEC_CTX_Q_NUM_MAX || ctx_q_num & 0x1) {\n\t\tpr_err(\"ctx queue num[%u] is invalid!\\n\", ctx_q_num);\n\t\treturn -EINVAL;\n\t}\n\n\treturn param_set_int(val, kp);\n}\n\nstatic const struct kernel_param_ops sec_ctx_q_num_ops = {\n\t.set = sec_ctx_q_num_set,\n\t.get = param_get_int,\n};\nstatic u32 ctx_q_num = SEC_CTX_Q_NUM_DEF;\nmodule_param_cb(ctx_q_num, &sec_ctx_q_num_ops, &ctx_q_num, 0444);\nMODULE_PARM_DESC(ctx_q_num, \"Queue num in ctx (2 default, 2, 4, ..., 32)\");\n\nstatic const struct kernel_param_ops vfs_num_ops = {\n\t.set = vfs_num_set,\n\t.get = param_get_int,\n};\n\nstatic u32 vfs_num;\nmodule_param_cb(vfs_num, &vfs_num_ops, &vfs_num, 0444);\nMODULE_PARM_DESC(vfs_num, \"Number of VFs to enable(1-63), 0(default)\");\n\nvoid sec_destroy_qps(struct hisi_qp **qps, int qp_num)\n{\n\thisi_qm_free_qps(qps, qp_num);\n\tkfree(qps);\n}\n\nstruct hisi_qp **sec_create_qps(void)\n{\n\tint node = cpu_to_node(smp_processor_id());\n\tu32 ctx_num = ctx_q_num;\n\tstruct hisi_qp **qps;\n\tint ret;\n\n\tqps = kcalloc(ctx_num, sizeof(struct hisi_qp *), GFP_KERNEL);\n\tif (!qps)\n\t\treturn NULL;\n\n\tret = hisi_qm_alloc_qps_node(&sec_devices, ctx_num, 0, node, qps);\n\tif (!ret)\n\t\treturn qps;\n\n\tkfree(qps);\n\treturn NULL;\n}\n\nu64 sec_get_alg_bitmap(struct hisi_qm *qm, u32 high, u32 low)\n{\n\tu32 cap_val_h, cap_val_l;\n\n\tcap_val_h = qm->cap_tables.dev_cap_table[high].cap_val;\n\tcap_val_l = qm->cap_tables.dev_cap_table[low].cap_val;\n\n\treturn ((u64)cap_val_h << SEC_ALG_BITMAP_SHIFT) | (u64)cap_val_l;\n}\n\nstatic const struct kernel_param_ops sec_uacce_mode_ops = {\n\t.set = uacce_mode_set,\n\t.get = param_get_int,\n};\n\n \nstatic u32 uacce_mode = UACCE_MODE_NOUACCE;\nmodule_param_cb(uacce_mode, &sec_uacce_mode_ops, &uacce_mode, 0444);\nMODULE_PARM_DESC(uacce_mode, UACCE_MODE_DESC);\n\nstatic const struct pci_device_id sec_dev_ids[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_SEC_PF) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_SEC_VF) },\n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, sec_dev_ids);\n\nstatic void sec_set_endian(struct hisi_qm *qm)\n{\n\tu32 reg;\n\n\treg = readl_relaxed(qm->io_base + SEC_CONTROL_REG);\n\treg &= ~(BIT(1) | BIT(0));\n\tif (!IS_ENABLED(CONFIG_64BIT))\n\t\treg |= BIT(1);\n\n\tif (!IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN))\n\t\treg |= BIT(0);\n\n\twritel_relaxed(reg, qm->io_base + SEC_CONTROL_REG);\n}\n\nstatic void sec_engine_sva_config(struct hisi_qm *qm)\n{\n\tu32 reg;\n\n\tif (qm->ver > QM_HW_V2) {\n\t\treg = readl_relaxed(qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL0_REG_V3);\n\t\treg |= SEC_USER0_SMMU_NORMAL;\n\t\twritel_relaxed(reg, qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL0_REG_V3);\n\n\t\treg = readl_relaxed(qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL1_REG_V3);\n\t\treg &= SEC_USER1_SMMU_MASK_V3;\n\t\treg |= SEC_USER1_SMMU_NORMAL_V3;\n\t\twritel_relaxed(reg, qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL1_REG_V3);\n\t} else {\n\t\treg = readl_relaxed(qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL0_REG);\n\t\treg |= SEC_USER0_SMMU_NORMAL;\n\t\twritel_relaxed(reg, qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL0_REG);\n\t\treg = readl_relaxed(qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL1_REG);\n\t\treg &= SEC_USER1_SMMU_MASK;\n\t\tif (qm->use_sva)\n\t\t\treg |= SEC_USER1_SMMU_SVA;\n\t\telse\n\t\t\treg |= SEC_USER1_SMMU_NORMAL;\n\t\twritel_relaxed(reg, qm->io_base +\n\t\t\t\tSEC_INTERFACE_USER_CTRL1_REG);\n\t}\n}\n\nstatic void sec_open_sva_prefetch(struct hisi_qm *qm)\n{\n\tu32 val;\n\tint ret;\n\n\tif (!test_bit(QM_SUPPORT_SVA_PREFETCH, &qm->caps))\n\t\treturn;\n\n\t \n\tval = readl_relaxed(qm->io_base + SEC_PREFETCH_CFG);\n\tval &= SEC_PREFETCH_ENABLE;\n\twritel(val, qm->io_base + SEC_PREFETCH_CFG);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + SEC_PREFETCH_CFG,\n\t\t\t\t\t val, !(val & SEC_PREFETCH_DISABLE),\n\t\t\t\t\t SEC_DELAY_10_US, SEC_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"failed to open sva prefetch\\n\");\n}\n\nstatic void sec_close_sva_prefetch(struct hisi_qm *qm)\n{\n\tu32 val;\n\tint ret;\n\n\tif (!test_bit(QM_SUPPORT_SVA_PREFETCH, &qm->caps))\n\t\treturn;\n\n\tval = readl_relaxed(qm->io_base + SEC_PREFETCH_CFG);\n\tval |= SEC_PREFETCH_DISABLE;\n\twritel(val, qm->io_base + SEC_PREFETCH_CFG);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + SEC_SVA_TRANS,\n\t\t\t\t\t val, !(val & SEC_SVA_DISABLE_READY),\n\t\t\t\t\t SEC_DELAY_10_US, SEC_POLL_TIMEOUT_US);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"failed to close sva prefetch\\n\");\n}\n\nstatic void sec_enable_clock_gate(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\tif (qm->ver < QM_HW_V3)\n\t\treturn;\n\n\tval = readl_relaxed(qm->io_base + SEC_CONTROL_REG);\n\tval |= SEC_CLK_GATE_ENABLE;\n\twritel_relaxed(val, qm->io_base + SEC_CONTROL_REG);\n\n\tval = readl(qm->io_base + SEC_DYNAMIC_GATE_REG);\n\tval |= SEC_DYNAMIC_GATE_EN;\n\twritel(val, qm->io_base + SEC_DYNAMIC_GATE_REG);\n\n\tval = readl(qm->io_base + SEC_CORE_AUTO_GATE);\n\tval |= SEC_CORE_AUTO_GATE_EN;\n\twritel(val, qm->io_base + SEC_CORE_AUTO_GATE);\n}\n\nstatic void sec_disable_clock_gate(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\t \n\tval = readl_relaxed(qm->io_base + SEC_CONTROL_REG);\n\tval &= SEC_CLK_GATE_DISABLE;\n\twritel_relaxed(val, qm->io_base + SEC_CONTROL_REG);\n}\n\nstatic int sec_engine_init(struct hisi_qm *qm)\n{\n\tint ret;\n\tu32 reg;\n\n\t \n\tsec_disable_clock_gate(qm);\n\n\twritel_relaxed(0x1, qm->io_base + SEC_MEM_START_INIT_REG);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + SEC_MEM_INIT_DONE_REG,\n\t\t\t\t\t reg, reg & 0x1, SEC_DELAY_10_US,\n\t\t\t\t\t SEC_POLL_TIMEOUT_US);\n\tif (ret) {\n\t\tpci_err(qm->pdev, \"fail to init sec mem\\n\");\n\t\treturn ret;\n\t}\n\n\treg = readl_relaxed(qm->io_base + SEC_CONTROL_REG);\n\treg |= (0x1 << SEC_TRNG_EN_SHIFT);\n\twritel_relaxed(reg, qm->io_base + SEC_CONTROL_REG);\n\n\tsec_engine_sva_config(qm);\n\n\twritel(SEC_SINGLE_PORT_MAX_TRANS,\n\t       qm->io_base + AM_CFG_SINGLE_PORT_MAX_TRANS);\n\n\treg = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_CORE_ENABLE_BITMAP, qm->cap_ver);\n\twritel(reg, qm->io_base + SEC_SAA_EN_REG);\n\n\tif (qm->ver < QM_HW_V3) {\n\t\t \n\t\twritel_relaxed(SEC_BD_ERR_CHK_EN0,\n\t\t\t       qm->io_base + SEC_BD_ERR_CHK_EN_REG0);\n\n\t\t \n\t\twritel_relaxed(SEC_BD_ERR_CHK_EN1,\n\t\t\t       qm->io_base + SEC_BD_ERR_CHK_EN_REG1);\n\t\twritel_relaxed(SEC_BD_ERR_CHK_EN3,\n\t\t\t       qm->io_base + SEC_BD_ERR_CHK_EN_REG3);\n\t}\n\n\t \n\tsec_set_endian(qm);\n\n\tsec_enable_clock_gate(qm);\n\n\treturn 0;\n}\n\nstatic int sec_set_user_domain_and_cache(struct hisi_qm *qm)\n{\n\t \n\twritel(AXUSER_BASE, qm->io_base + QM_ARUSER_M_CFG_1);\n\twritel(ARUSER_M_CFG_ENABLE, qm->io_base + QM_ARUSER_M_CFG_ENABLE);\n\twritel(AXUSER_BASE, qm->io_base + QM_AWUSER_M_CFG_1);\n\twritel(AWUSER_M_CFG_ENABLE, qm->io_base + QM_AWUSER_M_CFG_ENABLE);\n\twritel(WUSER_M_CFG_ENABLE, qm->io_base + QM_WUSER_M_CFG_ENABLE);\n\n\t \n\twritel(AXI_M_CFG, qm->io_base + QM_AXI_M_CFG);\n\twritel(AXI_M_CFG_ENABLE, qm->io_base + QM_AXI_M_CFG_ENABLE);\n\n\t \n\twritel(PEH_AXUSER_CFG, qm->io_base + QM_PEH_AXUSER_CFG);\n\twritel(PEH_AXUSER_CFG_ENABLE, qm->io_base + QM_PEH_AXUSER_CFG_ENABLE);\n\n\t \n\twritel(SQC_CACHE_ENABLE | CQC_CACHE_ENABLE | SQC_CACHE_WB_ENABLE |\n\t       CQC_CACHE_WB_ENABLE | FIELD_PREP(SQC_CACHE_WB_THRD, 1) |\n\t       FIELD_PREP(CQC_CACHE_WB_THRD, 1), qm->io_base + QM_CACHE_CTL);\n\n\treturn sec_engine_init(qm);\n}\n\n \nstatic void sec_debug_regs_clear(struct hisi_qm *qm)\n{\n\tint i;\n\n\t \n\twritel(0x1, qm->io_base + SEC_CTRL_CNT_CLR_CE);\n\tfor (i = 0; i < ARRAY_SIZE(sec_dfx_regs); i++)\n\t\treadl(qm->io_base + sec_dfx_regs[i].offset);\n\n\t \n\twritel(0x0, qm->io_base + SEC_CTRL_CNT_CLR_CE);\n\n\thisi_qm_debug_regs_clear(qm);\n}\n\nstatic void sec_master_ooo_ctrl(struct hisi_qm *qm, bool enable)\n{\n\tu32 val1, val2;\n\n\tval1 = readl(qm->io_base + SEC_CONTROL_REG);\n\tif (enable) {\n\t\tval1 |= SEC_AXI_SHUTDOWN_ENABLE;\n\t\tval2 = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\t\t\t   SEC_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\t} else {\n\t\tval1 &= SEC_AXI_SHUTDOWN_DISABLE;\n\t\tval2 = 0x0;\n\t}\n\n\tif (qm->ver > QM_HW_V2)\n\t\twritel(val2, qm->io_base + SEC_OOO_SHUTDOWN_SEL);\n\n\twritel(val1, qm->io_base + SEC_CONTROL_REG);\n}\n\nstatic void sec_hw_error_enable(struct hisi_qm *qm)\n{\n\tu32 ce, nfe;\n\n\tif (qm->ver == QM_HW_V1) {\n\t\twritel(SEC_CORE_INT_DISABLE, qm->io_base + SEC_CORE_INT_MASK);\n\t\tpci_info(qm->pdev, \"V1 not support hw error handle\\n\");\n\t\treturn;\n\t}\n\n\tce = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_CE_MASK_CAP, qm->cap_ver);\n\tnfe = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_NFE_MASK_CAP, qm->cap_ver);\n\n\t \n\twritel(ce | nfe | SEC_RAS_FE_ENB_MSK, qm->io_base + SEC_CORE_INT_SOURCE);\n\n\t \n\twritel(ce, qm->io_base + SEC_RAS_CE_REG);\n\twritel(SEC_RAS_FE_ENB_MSK, qm->io_base + SEC_RAS_FE_REG);\n\twritel(nfe, qm->io_base + SEC_RAS_NFE_REG);\n\n\t \n\tsec_master_ooo_ctrl(qm, true);\n\n\t \n\twritel(ce | nfe | SEC_RAS_FE_ENB_MSK, qm->io_base + SEC_CORE_INT_MASK);\n}\n\nstatic void sec_hw_error_disable(struct hisi_qm *qm)\n{\n\t \n\twritel(SEC_CORE_INT_DISABLE, qm->io_base + SEC_CORE_INT_MASK);\n\n\t \n\tsec_master_ooo_ctrl(qm, false);\n\n\t \n\twritel(SEC_RAS_DISABLE, qm->io_base + SEC_RAS_CE_REG);\n\twritel(SEC_RAS_DISABLE, qm->io_base + SEC_RAS_FE_REG);\n\twritel(SEC_RAS_DISABLE, qm->io_base + SEC_RAS_NFE_REG);\n}\n\nstatic u32 sec_clear_enable_read(struct hisi_qm *qm)\n{\n\treturn readl(qm->io_base + SEC_CTRL_CNT_CLR_CE) &\n\t\t\tSEC_CTRL_CNT_CLR_CE_BIT;\n}\n\nstatic int sec_clear_enable_write(struct hisi_qm *qm, u32 val)\n{\n\tu32 tmp;\n\n\tif (val != 1 && val)\n\t\treturn -EINVAL;\n\n\ttmp = (readl(qm->io_base + SEC_CTRL_CNT_CLR_CE) &\n\t       ~SEC_CTRL_CNT_CLR_CE_BIT) | val;\n\twritel(tmp, qm->io_base + SEC_CTRL_CNT_CLR_CE);\n\n\treturn 0;\n}\n\nstatic ssize_t sec_debug_read(struct file *filp, char __user *buf,\n\t\t\t       size_t count, loff_t *pos)\n{\n\tstruct sec_debug_file *file = filp->private_data;\n\tchar tbuf[SEC_DBGFS_VAL_MAX_LEN];\n\tstruct hisi_qm *qm = file->qm;\n\tu32 val;\n\tint ret;\n\n\tret = hisi_qm_get_dfx_access(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irq(&file->lock);\n\n\tswitch (file->index) {\n\tcase SEC_CLEAR_ENABLE:\n\t\tval = sec_clear_enable_read(qm);\n\t\tbreak;\n\tdefault:\n\t\tgoto err_input;\n\t}\n\n\tspin_unlock_irq(&file->lock);\n\n\thisi_qm_put_dfx_access(qm);\n\tret = snprintf(tbuf, SEC_DBGFS_VAL_MAX_LEN, \"%u\\n\", val);\n\treturn simple_read_from_buffer(buf, count, pos, tbuf, ret);\n\nerr_input:\n\tspin_unlock_irq(&file->lock);\n\thisi_qm_put_dfx_access(qm);\n\treturn -EINVAL;\n}\n\nstatic ssize_t sec_debug_write(struct file *filp, const char __user *buf,\n\t\t\t       size_t count, loff_t *pos)\n{\n\tstruct sec_debug_file *file = filp->private_data;\n\tchar tbuf[SEC_DBGFS_VAL_MAX_LEN];\n\tstruct hisi_qm *qm = file->qm;\n\tunsigned long val;\n\tint len, ret;\n\n\tif (*pos != 0)\n\t\treturn 0;\n\n\tif (count >= SEC_DBGFS_VAL_MAX_LEN)\n\t\treturn -ENOSPC;\n\n\tlen = simple_write_to_buffer(tbuf, SEC_DBGFS_VAL_MAX_LEN - 1,\n\t\t\t\t     pos, buf, count);\n\tif (len < 0)\n\t\treturn len;\n\n\ttbuf[len] = '\\0';\n\tif (kstrtoul(tbuf, 0, &val))\n\t\treturn -EFAULT;\n\n\tret = hisi_qm_get_dfx_access(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irq(&file->lock);\n\n\tswitch (file->index) {\n\tcase SEC_CLEAR_ENABLE:\n\t\tret = sec_clear_enable_write(qm, val);\n\t\tif (ret)\n\t\t\tgoto err_input;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err_input;\n\t}\n\n\tret = count;\n\n err_input:\n\tspin_unlock_irq(&file->lock);\n\thisi_qm_put_dfx_access(qm);\n\treturn ret;\n}\n\nstatic const struct file_operations sec_dbg_fops = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = sec_debug_read,\n\t.write = sec_debug_write,\n};\n\nstatic int sec_debugfs_atomic64_get(void *data, u64 *val)\n{\n\t*val = atomic64_read((atomic64_t *)data);\n\n\treturn 0;\n}\n\nstatic int sec_debugfs_atomic64_set(void *data, u64 val)\n{\n\tif (val)\n\t\treturn -EINVAL;\n\n\tatomic64_set((atomic64_t *)data, 0);\n\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(sec_atomic64_ops, sec_debugfs_atomic64_get,\n\t\t\t sec_debugfs_atomic64_set, \"%lld\\n\");\n\nstatic int sec_regs_show(struct seq_file *s, void *unused)\n{\n\thisi_qm_regs_dump(s, s->private);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(sec_regs);\n\nstatic int sec_core_debug_init(struct hisi_qm *qm)\n{\n\tstruct dfx_diff_registers *sec_regs = qm->debug.acc_diff_regs;\n\tstruct sec_dev *sec = container_of(qm, struct sec_dev, qm);\n\tstruct device *dev = &qm->pdev->dev;\n\tstruct sec_dfx *dfx = &sec->debug.dfx;\n\tstruct debugfs_regset32 *regset;\n\tstruct dentry *tmp_d;\n\tint i;\n\n\ttmp_d = debugfs_create_dir(\"sec_dfx\", qm->debug.debug_root);\n\n\tregset = devm_kzalloc(dev, sizeof(*regset), GFP_KERNEL);\n\tif (!regset)\n\t\treturn -ENOMEM;\n\n\tregset->regs = sec_dfx_regs;\n\tregset->nregs = ARRAY_SIZE(sec_dfx_regs);\n\tregset->base = qm->io_base;\n\tregset->dev = dev;\n\n\tif (qm->pdev->device == PCI_DEVICE_ID_HUAWEI_SEC_PF)\n\t\tdebugfs_create_file(\"regs\", 0444, tmp_d, regset, &sec_regs_fops);\n\tif (qm->fun_type == QM_HW_PF && sec_regs)\n\t\tdebugfs_create_file(\"diff_regs\", 0444, tmp_d,\n\t\t\t\t      qm, &sec_diff_regs_fops);\n\n\tfor (i = 0; i < ARRAY_SIZE(sec_dfx_labels); i++) {\n\t\tatomic64_t *data = (atomic64_t *)((uintptr_t)dfx +\n\t\t\t\t\tsec_dfx_labels[i].offset);\n\t\tdebugfs_create_file(sec_dfx_labels[i].name, 0644,\n\t\t\t\t   tmp_d, data, &sec_atomic64_ops);\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_debug_init(struct hisi_qm *qm)\n{\n\tstruct sec_dev *sec = container_of(qm, struct sec_dev, qm);\n\tint i;\n\n\tif (qm->pdev->device == PCI_DEVICE_ID_HUAWEI_SEC_PF) {\n\t\tfor (i = SEC_CLEAR_ENABLE; i < SEC_DEBUG_FILE_NUM; i++) {\n\t\t\tspin_lock_init(&sec->debug.files[i].lock);\n\t\t\tsec->debug.files[i].index = i;\n\t\t\tsec->debug.files[i].qm = qm;\n\n\t\t\tdebugfs_create_file(sec_dbg_file_name[i], 0600,\n\t\t\t\t\t\t  qm->debug.debug_root,\n\t\t\t\t\t\t  sec->debug.files + i,\n\t\t\t\t\t\t  &sec_dbg_fops);\n\t\t}\n\t}\n\n\treturn sec_core_debug_init(qm);\n}\n\nstatic int sec_debugfs_init(struct hisi_qm *qm)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tint ret;\n\n\tqm->debug.debug_root = debugfs_create_dir(dev_name(dev),\n\t\t\t\t\t\t  sec_debugfs_root);\n\tqm->debug.sqe_mask_offset = SEC_SQE_MASK_OFFSET;\n\tqm->debug.sqe_mask_len = SEC_SQE_MASK_LEN;\n\n\tret = hisi_qm_regs_debugfs_init(qm, sec_diff_regs, ARRAY_SIZE(sec_diff_regs));\n\tif (ret) {\n\t\tdev_warn(dev, \"Failed to init SEC diff regs!\\n\");\n\t\tgoto debugfs_remove;\n\t}\n\n\thisi_qm_debug_init(qm);\n\n\tret = sec_debug_init(qm);\n\tif (ret)\n\t\tgoto failed_to_create;\n\n\treturn 0;\n\nfailed_to_create:\n\thisi_qm_regs_debugfs_uninit(qm, ARRAY_SIZE(sec_diff_regs));\ndebugfs_remove:\n\tdebugfs_remove_recursive(sec_debugfs_root);\n\treturn ret;\n}\n\nstatic void sec_debugfs_exit(struct hisi_qm *qm)\n{\n\thisi_qm_regs_debugfs_uninit(qm, ARRAY_SIZE(sec_diff_regs));\n\n\tdebugfs_remove_recursive(qm->debug.debug_root);\n}\n\nstatic int sec_show_last_regs_init(struct hisi_qm *qm)\n{\n\tstruct qm_debug *debug = &qm->debug;\n\tint i;\n\n\tdebug->last_words = kcalloc(ARRAY_SIZE(sec_dfx_regs),\n\t\t\t\t\tsizeof(unsigned int), GFP_KERNEL);\n\tif (!debug->last_words)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ARRAY_SIZE(sec_dfx_regs); i++)\n\t\tdebug->last_words[i] = readl_relaxed(qm->io_base +\n\t\t\t\t\t\t\tsec_dfx_regs[i].offset);\n\n\treturn 0;\n}\n\nstatic void sec_show_last_regs_uninit(struct hisi_qm *qm)\n{\n\tstruct qm_debug *debug = &qm->debug;\n\n\tif (qm->fun_type == QM_HW_VF || !debug->last_words)\n\t\treturn;\n\n\tkfree(debug->last_words);\n\tdebug->last_words = NULL;\n}\n\nstatic void sec_show_last_dfx_regs(struct hisi_qm *qm)\n{\n\tstruct qm_debug *debug = &qm->debug;\n\tstruct pci_dev *pdev = qm->pdev;\n\tu32 val;\n\tint i;\n\n\tif (qm->fun_type == QM_HW_VF || !debug->last_words)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(sec_dfx_regs); i++) {\n\t\tval = readl_relaxed(qm->io_base + sec_dfx_regs[i].offset);\n\t\tif (val != debug->last_words[i])\n\t\t\tpci_info(pdev, \"%s \\t= 0x%08x => 0x%08x\\n\",\n\t\t\t\tsec_dfx_regs[i].name, debug->last_words[i], val);\n\t}\n}\n\nstatic void sec_log_hw_error(struct hisi_qm *qm, u32 err_sts)\n{\n\tconst struct sec_hw_error *errs = sec_hw_errors;\n\tstruct device *dev = &qm->pdev->dev;\n\tu32 err_val;\n\n\twhile (errs->msg) {\n\t\tif (errs->int_msk & err_sts) {\n\t\t\tdev_err(dev, \"%s [error status=0x%x] found\\n\",\n\t\t\t\t\terrs->msg, errs->int_msk);\n\n\t\t\tif (SEC_CORE_INT_STATUS_M_ECC & errs->int_msk) {\n\t\t\t\terr_val = readl(qm->io_base +\n\t\t\t\t\t\tSEC_CORE_SRAM_ECC_ERR_INFO);\n\t\t\t\tdev_err(dev, \"multi ecc sram num=0x%x\\n\",\n\t\t\t\t\t\t((err_val) >> SEC_ECC_NUM) &\n\t\t\t\t\t\tSEC_ECC_MASH);\n\t\t\t}\n\t\t}\n\t\terrs++;\n\t}\n}\n\nstatic u32 sec_get_hw_err_status(struct hisi_qm *qm)\n{\n\treturn readl(qm->io_base + SEC_CORE_INT_STATUS);\n}\n\nstatic void sec_clear_hw_err_status(struct hisi_qm *qm, u32 err_sts)\n{\n\tu32 nfe;\n\n\twritel(err_sts, qm->io_base + SEC_CORE_INT_SOURCE);\n\tnfe = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_NFE_MASK_CAP, qm->cap_ver);\n\twritel(nfe, qm->io_base + SEC_RAS_NFE_REG);\n}\n\nstatic void sec_open_axi_master_ooo(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\tval = readl(qm->io_base + SEC_CONTROL_REG);\n\twritel(val & SEC_AXI_SHUTDOWN_DISABLE, qm->io_base + SEC_CONTROL_REG);\n\twritel(val | SEC_AXI_SHUTDOWN_ENABLE, qm->io_base + SEC_CONTROL_REG);\n}\n\nstatic void sec_err_info_init(struct hisi_qm *qm)\n{\n\tstruct hisi_qm_err_info *err_info = &qm->err_info;\n\n\terr_info->fe = SEC_RAS_FE_ENB_MSK;\n\terr_info->ce = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_QM_CE_MASK_CAP, qm->cap_ver);\n\terr_info->nfe = hisi_qm_get_hw_info(qm, sec_basic_info, SEC_QM_NFE_MASK_CAP, qm->cap_ver);\n\terr_info->ecc_2bits_mask = SEC_CORE_INT_STATUS_M_ECC;\n\terr_info->qm_shutdown_mask = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\t\t     SEC_QM_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\terr_info->dev_shutdown_mask = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\tSEC_OOO_SHUTDOWN_MASK_CAP, qm->cap_ver);\n\terr_info->qm_reset_mask = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\tSEC_QM_RESET_MASK_CAP, qm->cap_ver);\n\terr_info->dev_reset_mask = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\tSEC_RESET_MASK_CAP, qm->cap_ver);\n\terr_info->msi_wr_port = BIT(0);\n\terr_info->acpi_rst = \"SRST\";\n}\n\nstatic const struct hisi_qm_err_ini sec_err_ini = {\n\t.hw_init\t\t= sec_set_user_domain_and_cache,\n\t.hw_err_enable\t\t= sec_hw_error_enable,\n\t.hw_err_disable\t\t= sec_hw_error_disable,\n\t.get_dev_hw_err_status\t= sec_get_hw_err_status,\n\t.clear_dev_hw_err_status = sec_clear_hw_err_status,\n\t.log_dev_hw_err\t\t= sec_log_hw_error,\n\t.open_axi_master_ooo\t= sec_open_axi_master_ooo,\n\t.open_sva_prefetch\t= sec_open_sva_prefetch,\n\t.close_sva_prefetch\t= sec_close_sva_prefetch,\n\t.show_last_dfx_regs\t= sec_show_last_dfx_regs,\n\t.err_info_init\t\t= sec_err_info_init,\n};\n\nstatic int sec_pf_probe_init(struct sec_dev *sec)\n{\n\tstruct hisi_qm *qm = &sec->qm;\n\tint ret;\n\n\tqm->err_ini = &sec_err_ini;\n\tqm->err_ini->err_info_init(qm);\n\n\tret = sec_set_user_domain_and_cache(qm);\n\tif (ret)\n\t\treturn ret;\n\n\tsec_open_sva_prefetch(qm);\n\thisi_qm_dev_err_init(qm);\n\tsec_debug_regs_clear(qm);\n\tret = sec_show_last_regs_init(qm);\n\tif (ret)\n\t\tpci_err(qm->pdev, \"Failed to init last word regs!\\n\");\n\n\treturn ret;\n}\n\nstatic int sec_pre_store_cap_reg(struct hisi_qm *qm)\n{\n\tstruct hisi_qm_cap_record *sec_cap;\n\tstruct pci_dev *pdev = qm->pdev;\n\tsize_t i, size;\n\n\tsize = ARRAY_SIZE(sec_pre_store_caps);\n\tsec_cap = devm_kzalloc(&pdev->dev, sizeof(*sec_cap) * size, GFP_KERNEL);\n\tif (!sec_cap)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < size; i++) {\n\t\tsec_cap[i].type = sec_pre_store_caps[i];\n\t\tsec_cap[i].cap_val = hisi_qm_get_hw_info(qm, sec_basic_info,\n\t\t\t\t     sec_pre_store_caps[i], qm->cap_ver);\n\t}\n\n\tqm->cap_tables.dev_cap_table = sec_cap;\n\n\treturn 0;\n}\n\nstatic int sec_qm_init(struct hisi_qm *qm, struct pci_dev *pdev)\n{\n\tu64 alg_msk;\n\tint ret;\n\n\tqm->pdev = pdev;\n\tqm->ver = pdev->revision;\n\tqm->mode = uacce_mode;\n\tqm->sqe_size = SEC_SQE_SIZE;\n\tqm->dev_name = sec_name;\n\n\tqm->fun_type = (pdev->device == PCI_DEVICE_ID_HUAWEI_SEC_PF) ?\n\t\t\tQM_HW_PF : QM_HW_VF;\n\tif (qm->fun_type == QM_HW_PF) {\n\t\tqm->qp_base = SEC_PF_DEF_Q_BASE;\n\t\tqm->qp_num = pf_q_num;\n\t\tqm->debug.curr_qm_qp_num = pf_q_num;\n\t\tqm->qm_list = &sec_devices;\n\t\tif (pf_q_num_flag)\n\t\t\tset_bit(QM_MODULE_PARAM, &qm->misc_ctl);\n\t} else if (qm->fun_type == QM_HW_VF && qm->ver == QM_HW_V1) {\n\t\t \n\t\tqm->qp_base = SEC_PF_DEF_Q_NUM;\n\t\tqm->qp_num = SEC_QUEUE_NUM_V1 - SEC_PF_DEF_Q_NUM;\n\t}\n\n\tret = hisi_qm_init(qm);\n\tif (ret) {\n\t\tpci_err(qm->pdev, \"Failed to init sec qm configures!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = sec_pre_store_cap_reg(qm);\n\tif (ret) {\n\t\tpci_err(qm->pdev, \"Failed to pre-store capability registers!\\n\");\n\t\thisi_qm_uninit(qm);\n\t\treturn ret;\n\t}\n\n\talg_msk = sec_get_alg_bitmap(qm, SEC_DEV_ALG_BITMAP_HIGH_IDX, SEC_DEV_ALG_BITMAP_LOW_IDX);\n\tret = hisi_qm_set_algs(qm, alg_msk, sec_dev_algs, ARRAY_SIZE(sec_dev_algs));\n\tif (ret) {\n\t\tpci_err(qm->pdev, \"Failed to set sec algs!\\n\");\n\t\thisi_qm_uninit(qm);\n\t}\n\n\treturn ret;\n}\n\nstatic void sec_qm_uninit(struct hisi_qm *qm)\n{\n\thisi_qm_uninit(qm);\n}\n\nstatic int sec_probe_init(struct sec_dev *sec)\n{\n\tu32 type_rate = SEC_SHAPER_TYPE_RATE;\n\tstruct hisi_qm *qm = &sec->qm;\n\tint ret;\n\n\tif (qm->fun_type == QM_HW_PF) {\n\t\tret = sec_pf_probe_init(sec);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t \n\t\tif (qm->ver >= QM_HW_V3) {\n\t\t\ttype_rate |= QM_SHAPER_ENABLE;\n\t\t\tqm->type_rate = type_rate;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_probe_uninit(struct hisi_qm *qm)\n{\n\thisi_qm_dev_err_uninit(qm);\n}\n\nstatic void sec_iommu_used_check(struct sec_dev *sec)\n{\n\tstruct iommu_domain *domain;\n\tstruct device *dev = &sec->qm.pdev->dev;\n\n\tdomain = iommu_get_domain_for_dev(dev);\n\n\t \n\tsec->iommu_used = false;\n\tif (domain) {\n\t\tif (domain->type & __IOMMU_DOMAIN_PAGING)\n\t\t\tsec->iommu_used = true;\n\t\tdev_info(dev, \"SMMU Opened, the iommu type = %u\\n\",\n\t\t\tdomain->type);\n\t}\n}\n\nstatic int sec_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct sec_dev *sec;\n\tstruct hisi_qm *qm;\n\tint ret;\n\n\tsec = devm_kzalloc(&pdev->dev, sizeof(*sec), GFP_KERNEL);\n\tif (!sec)\n\t\treturn -ENOMEM;\n\n\tqm = &sec->qm;\n\tret = sec_qm_init(qm, pdev);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to init SEC QM (%d)!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tsec->ctx_q_num = ctx_q_num;\n\tsec_iommu_used_check(sec);\n\n\tret = sec_probe_init(sec);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to probe!\\n\");\n\t\tgoto err_qm_uninit;\n\t}\n\n\tret = hisi_qm_start(qm);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to start sec qm!\\n\");\n\t\tgoto err_probe_uninit;\n\t}\n\n\tret = sec_debugfs_init(qm);\n\tif (ret)\n\t\tpci_warn(pdev, \"Failed to init debugfs!\\n\");\n\n\tif (qm->qp_num >= ctx_q_num) {\n\t\tret = hisi_qm_alg_register(qm, &sec_devices);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"Failed to register driver to crypto.\\n\");\n\t\t\tgoto err_qm_stop;\n\t\t}\n\t} else {\n\t\tpci_warn(qm->pdev,\n\t\t\t\"Failed to use kernel mode, qp not enough!\\n\");\n\t}\n\n\tif (qm->uacce) {\n\t\tret = uacce_register(qm->uacce);\n\t\tif (ret) {\n\t\t\tpci_err(pdev, \"failed to register uacce (%d)!\\n\", ret);\n\t\t\tgoto err_alg_unregister;\n\t\t}\n\t}\n\n\tif (qm->fun_type == QM_HW_PF && vfs_num) {\n\t\tret = hisi_qm_sriov_enable(pdev, vfs_num);\n\t\tif (ret < 0)\n\t\t\tgoto err_alg_unregister;\n\t}\n\n\thisi_qm_pm_init(qm);\n\n\treturn 0;\n\nerr_alg_unregister:\n\tif (qm->qp_num >= ctx_q_num)\n\t\thisi_qm_alg_unregister(qm, &sec_devices);\nerr_qm_stop:\n\tsec_debugfs_exit(qm);\n\thisi_qm_stop(qm, QM_NORMAL);\nerr_probe_uninit:\n\tsec_show_last_regs_uninit(qm);\n\tsec_probe_uninit(qm);\nerr_qm_uninit:\n\tsec_qm_uninit(qm);\n\treturn ret;\n}\n\nstatic void sec_remove(struct pci_dev *pdev)\n{\n\tstruct hisi_qm *qm = pci_get_drvdata(pdev);\n\n\thisi_qm_pm_uninit(qm);\n\thisi_qm_wait_task_finish(qm, &sec_devices);\n\tif (qm->qp_num >= ctx_q_num)\n\t\thisi_qm_alg_unregister(qm, &sec_devices);\n\n\tif (qm->fun_type == QM_HW_PF && qm->vfs_num)\n\t\thisi_qm_sriov_disable(pdev, true);\n\n\tsec_debugfs_exit(qm);\n\n\t(void)hisi_qm_stop(qm, QM_NORMAL);\n\n\tif (qm->fun_type == QM_HW_PF)\n\t\tsec_debug_regs_clear(qm);\n\tsec_show_last_regs_uninit(qm);\n\n\tsec_probe_uninit(qm);\n\n\tsec_qm_uninit(qm);\n}\n\nstatic const struct dev_pm_ops sec_pm_ops = {\n\tSET_RUNTIME_PM_OPS(hisi_qm_suspend, hisi_qm_resume, NULL)\n};\n\nstatic const struct pci_error_handlers sec_err_handler = {\n\t.error_detected = hisi_qm_dev_err_detected,\n\t.slot_reset\t= hisi_qm_dev_slot_reset,\n\t.reset_prepare\t= hisi_qm_reset_prepare,\n\t.reset_done\t= hisi_qm_reset_done,\n};\n\nstatic struct pci_driver sec_pci_driver = {\n\t.name = \"hisi_sec2\",\n\t.id_table = sec_dev_ids,\n\t.probe = sec_probe,\n\t.remove = sec_remove,\n\t.err_handler = &sec_err_handler,\n\t.sriov_configure = hisi_qm_sriov_configure,\n\t.shutdown = hisi_qm_dev_shutdown,\n\t.driver.pm = &sec_pm_ops,\n};\n\nstruct pci_driver *hisi_sec_get_pf_driver(void)\n{\n\treturn &sec_pci_driver;\n}\nEXPORT_SYMBOL_GPL(hisi_sec_get_pf_driver);\n\nstatic void sec_register_debugfs(void)\n{\n\tif (!debugfs_initialized())\n\t\treturn;\n\n\tsec_debugfs_root = debugfs_create_dir(\"hisi_sec2\", NULL);\n}\n\nstatic void sec_unregister_debugfs(void)\n{\n\tdebugfs_remove_recursive(sec_debugfs_root);\n}\n\nstatic int __init sec_init(void)\n{\n\tint ret;\n\n\thisi_qm_init_list(&sec_devices);\n\tsec_register_debugfs();\n\n\tret = pci_register_driver(&sec_pci_driver);\n\tif (ret < 0) {\n\t\tsec_unregister_debugfs();\n\t\tpr_err(\"Failed to register pci driver.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit sec_exit(void)\n{\n\tpci_unregister_driver(&sec_pci_driver);\n\tsec_unregister_debugfs();\n}\n\nmodule_init(sec_init);\nmodule_exit(sec_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Zaibo Xu <xuzaibo@huawei.com>\");\nMODULE_AUTHOR(\"Longfang Liu <liulongfang@huawei.com>\");\nMODULE_AUTHOR(\"Kai Ye <yekai13@huawei.com>\");\nMODULE_AUTHOR(\"Wei Zhang <zhangwei375@huawei.com>\");\nMODULE_DESCRIPTION(\"Driver for HiSilicon SEC accelerator\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}