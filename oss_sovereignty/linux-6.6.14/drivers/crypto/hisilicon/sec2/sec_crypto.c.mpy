{
  "module_name": "sec_crypto.c",
  "hash_id": "e265b4243f716cb8a58cabfa89683ca3a9eba2b71c3e6ea3604f938ed60198e7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/hisilicon/sec2/sec_crypto.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/aead.h>\n#include <crypto/algapi.h>\n#include <crypto/authenc.h>\n#include <crypto/des.h>\n#include <crypto/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/des.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/skcipher.h>\n#include <crypto/xts.h>\n#include <linux/crypto.h>\n#include <linux/dma-mapping.h>\n#include <linux/idr.h>\n\n#include \"sec.h\"\n#include \"sec_crypto.h\"\n\n#define SEC_PRIORITY\t\t4001\n#define SEC_XTS_MIN_KEY_SIZE\t(2 * AES_MIN_KEY_SIZE)\n#define SEC_XTS_MID_KEY_SIZE\t(3 * AES_MIN_KEY_SIZE)\n#define SEC_XTS_MAX_KEY_SIZE\t(2 * AES_MAX_KEY_SIZE)\n#define SEC_DES3_2KEY_SIZE\t(2 * DES_KEY_SIZE)\n#define SEC_DES3_3KEY_SIZE\t(3 * DES_KEY_SIZE)\n\n \n#define SEC_DE_OFFSET\t\t1\n#define SEC_CIPHER_OFFSET\t4\n#define SEC_SCENE_OFFSET\t3\n#define SEC_DST_SGL_OFFSET\t2\n#define SEC_SRC_SGL_OFFSET\t7\n#define SEC_CKEY_OFFSET\t\t9\n#define SEC_CMODE_OFFSET\t12\n#define SEC_AKEY_OFFSET         5\n#define SEC_AEAD_ALG_OFFSET     11\n#define SEC_AUTH_OFFSET\t\t6\n\n#define SEC_DE_OFFSET_V3\t\t9\n#define SEC_SCENE_OFFSET_V3\t5\n#define SEC_CKEY_OFFSET_V3\t13\n#define SEC_CTR_CNT_OFFSET\t25\n#define SEC_CTR_CNT_ROLLOVER\t2\n#define SEC_SRC_SGL_OFFSET_V3\t11\n#define SEC_DST_SGL_OFFSET_V3\t14\n#define SEC_CALG_OFFSET_V3\t4\n#define SEC_AKEY_OFFSET_V3\t9\n#define SEC_MAC_OFFSET_V3\t4\n#define SEC_AUTH_ALG_OFFSET_V3\t15\n#define SEC_CIPHER_AUTH_V3\t0xbf\n#define SEC_AUTH_CIPHER_V3\t0x40\n#define SEC_FLAG_OFFSET\t\t7\n#define SEC_FLAG_MASK\t\t0x0780\n#define SEC_TYPE_MASK\t\t0x0F\n#define SEC_DONE_MASK\t\t0x0001\n#define SEC_ICV_MASK\t\t0x000E\n#define SEC_SQE_LEN_RATE_MASK\t0x3\n\n#define SEC_TOTAL_IV_SZ(depth)\t(SEC_IV_SIZE * (depth))\n#define SEC_SGL_SGE_NR\t\t128\n#define SEC_CIPHER_AUTH\t\t0xfe\n#define SEC_AUTH_CIPHER\t\t0x1\n#define SEC_MAX_MAC_LEN\t\t64\n#define SEC_MAX_AAD_LEN\t\t65535\n#define SEC_MAX_CCM_AAD_LEN\t65279\n#define SEC_TOTAL_MAC_SZ(depth) (SEC_MAX_MAC_LEN * (depth))\n\n#define SEC_PBUF_SZ\t\t\t512\n#define SEC_PBUF_IV_OFFSET\t\tSEC_PBUF_SZ\n#define SEC_PBUF_MAC_OFFSET\t\t(SEC_PBUF_SZ + SEC_IV_SIZE)\n#define SEC_PBUF_PKG\t\t(SEC_PBUF_SZ + SEC_IV_SIZE +\t\\\n\t\t\tSEC_MAX_MAC_LEN * 2)\n#define SEC_PBUF_NUM\t\t(PAGE_SIZE / SEC_PBUF_PKG)\n#define SEC_PBUF_PAGE_NUM(depth)\t((depth) / SEC_PBUF_NUM)\n#define SEC_PBUF_LEFT_SZ(depth)\t\t(SEC_PBUF_PKG * ((depth) -\t\\\n\t\t\t\tSEC_PBUF_PAGE_NUM(depth) * SEC_PBUF_NUM))\n#define SEC_TOTAL_PBUF_SZ(depth)\t(PAGE_SIZE * SEC_PBUF_PAGE_NUM(depth) +\t\\\n\t\t\t\tSEC_PBUF_LEFT_SZ(depth))\n\n#define SEC_SQE_LEN_RATE\t4\n#define SEC_SQE_CFLAG\t\t2\n#define SEC_SQE_AEAD_FLAG\t3\n#define SEC_SQE_DONE\t\t0x1\n#define SEC_ICV_ERR\t\t0x2\n#define MIN_MAC_LEN\t\t4\n#define MAC_LEN_MASK\t\t0x1U\n#define MAX_INPUT_DATA_LEN\t0xFFFE00\n#define BITS_MASK\t\t0xFF\n#define BYTE_BITS\t\t0x8\n#define SEC_XTS_NAME_SZ\t\t0x3\n#define IV_CM_CAL_NUM\t\t2\n#define IV_CL_MASK\t\t0x7\n#define IV_CL_MIN\t\t2\n#define IV_CL_MID\t\t4\n#define IV_CL_MAX\t\t8\n#define IV_FLAGS_OFFSET\t0x6\n#define IV_CM_OFFSET\t\t0x3\n#define IV_LAST_BYTE1\t\t1\n#define IV_LAST_BYTE2\t\t2\n#define IV_LAST_BYTE_MASK\t0xFF\n#define IV_CTR_INIT\t\t0x1\n#define IV_BYTE_OFFSET\t\t0x8\n\nstruct sec_skcipher {\n\tu64 alg_msk;\n\tstruct skcipher_alg alg;\n};\n\nstruct sec_aead {\n\tu64 alg_msk;\n\tstruct aead_alg alg;\n};\n\n \nstatic inline int sec_alloc_queue_id(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tif (req->c_req.encrypt)\n\t\treturn (u32)atomic_inc_return(&ctx->enc_qcyclic) %\n\t\t\t\t ctx->hlf_q_num;\n\n\treturn (u32)atomic_inc_return(&ctx->dec_qcyclic) % ctx->hlf_q_num +\n\t\t\t\t ctx->hlf_q_num;\n}\n\nstatic inline void sec_free_queue_id(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tif (req->c_req.encrypt)\n\t\tatomic_dec(&ctx->enc_qcyclic);\n\telse\n\t\tatomic_dec(&ctx->dec_qcyclic);\n}\n\nstatic int sec_alloc_req_id(struct sec_req *req, struct sec_qp_ctx *qp_ctx)\n{\n\tint req_id;\n\n\tspin_lock_bh(&qp_ctx->req_lock);\n\treq_id = idr_alloc_cyclic(&qp_ctx->req_idr, NULL, 0, qp_ctx->qp->sq_depth, GFP_ATOMIC);\n\tspin_unlock_bh(&qp_ctx->req_lock);\n\tif (unlikely(req_id < 0)) {\n\t\tdev_err(req->ctx->dev, \"alloc req id fail!\\n\");\n\t\treturn req_id;\n\t}\n\n\treq->qp_ctx = qp_ctx;\n\tqp_ctx->req_list[req_id] = req;\n\n\treturn req_id;\n}\n\nstatic void sec_free_req_id(struct sec_req *req)\n{\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tint req_id = req->req_id;\n\n\tif (unlikely(req_id < 0 || req_id >= qp_ctx->qp->sq_depth)) {\n\t\tdev_err(req->ctx->dev, \"free request id invalid!\\n\");\n\t\treturn;\n\t}\n\n\tqp_ctx->req_list[req_id] = NULL;\n\treq->qp_ctx = NULL;\n\n\tspin_lock_bh(&qp_ctx->req_lock);\n\tidr_remove(&qp_ctx->req_idr, req_id);\n\tspin_unlock_bh(&qp_ctx->req_lock);\n}\n\nstatic u8 pre_parse_finished_bd(struct bd_status *status, void *resp)\n{\n\tstruct sec_sqe *bd = resp;\n\n\tstatus->done = le16_to_cpu(bd->type2.done_flag) & SEC_DONE_MASK;\n\tstatus->icv = (le16_to_cpu(bd->type2.done_flag) & SEC_ICV_MASK) >> 1;\n\tstatus->flag = (le16_to_cpu(bd->type2.done_flag) &\n\t\t\t\t\tSEC_FLAG_MASK) >> SEC_FLAG_OFFSET;\n\tstatus->tag = le16_to_cpu(bd->type2.tag);\n\tstatus->err_type = bd->type2.error_type;\n\n\treturn bd->type_cipher_auth & SEC_TYPE_MASK;\n}\n\nstatic u8 pre_parse_finished_bd3(struct bd_status *status, void *resp)\n{\n\tstruct sec_sqe3 *bd3 = resp;\n\n\tstatus->done = le16_to_cpu(bd3->done_flag) & SEC_DONE_MASK;\n\tstatus->icv = (le16_to_cpu(bd3->done_flag) & SEC_ICV_MASK) >> 1;\n\tstatus->flag = (le16_to_cpu(bd3->done_flag) &\n\t\t\t\t\tSEC_FLAG_MASK) >> SEC_FLAG_OFFSET;\n\tstatus->tag = le64_to_cpu(bd3->tag);\n\tstatus->err_type = bd3->error_type;\n\n\treturn le32_to_cpu(bd3->bd_param) & SEC_TYPE_MASK;\n}\n\nstatic int sec_cb_status_check(struct sec_req *req,\n\t\t\t       struct bd_status *status)\n{\n\tstruct sec_ctx *ctx = req->ctx;\n\n\tif (unlikely(req->err_type || status->done != SEC_SQE_DONE)) {\n\t\tdev_err_ratelimited(ctx->dev, \"err_type[%d], done[%u]\\n\",\n\t\t\t\t    req->err_type, status->done);\n\t\treturn -EIO;\n\t}\n\n\tif (unlikely(ctx->alg_type == SEC_SKCIPHER)) {\n\t\tif (unlikely(status->flag != SEC_SQE_CFLAG)) {\n\t\t\tdev_err_ratelimited(ctx->dev, \"flag[%u]\\n\",\n\t\t\t\t\t    status->flag);\n\t\t\treturn -EIO;\n\t\t}\n\t} else if (unlikely(ctx->alg_type == SEC_AEAD)) {\n\t\tif (unlikely(status->flag != SEC_SQE_AEAD_FLAG ||\n\t\t\t     status->icv == SEC_ICV_ERR)) {\n\t\t\tdev_err_ratelimited(ctx->dev,\n\t\t\t\t\t    \"flag[%u], icv[%u]\\n\",\n\t\t\t\t\t    status->flag, status->icv);\n\t\t\treturn -EBADMSG;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_req_cb(struct hisi_qp *qp, void *resp)\n{\n\tstruct sec_qp_ctx *qp_ctx = qp->qp_ctx;\n\tstruct sec_dfx *dfx = &qp_ctx->ctx->sec->debug.dfx;\n\tu8 type_supported = qp_ctx->ctx->type_supported;\n\tstruct bd_status status;\n\tstruct sec_ctx *ctx;\n\tstruct sec_req *req;\n\tint err;\n\tu8 type;\n\n\tif (type_supported == SEC_BD_TYPE2) {\n\t\ttype = pre_parse_finished_bd(&status, resp);\n\t\treq = qp_ctx->req_list[status.tag];\n\t} else {\n\t\ttype = pre_parse_finished_bd3(&status, resp);\n\t\treq = (void *)(uintptr_t)status.tag;\n\t}\n\n\tif (unlikely(type != type_supported)) {\n\t\tatomic64_inc(&dfx->err_bd_cnt);\n\t\tpr_err(\"err bd type [%u]\\n\", type);\n\t\treturn;\n\t}\n\n\tif (unlikely(!req)) {\n\t\tatomic64_inc(&dfx->invalid_req_cnt);\n\t\tatomic_inc(&qp->qp_status.used);\n\t\treturn;\n\t}\n\n\treq->err_type = status.err_type;\n\tctx = req->ctx;\n\terr = sec_cb_status_check(req, &status);\n\tif (err)\n\t\tatomic64_inc(&dfx->done_flag_cnt);\n\n\tatomic64_inc(&dfx->recv_cnt);\n\n\tctx->req_op->buf_unmap(ctx, req);\n\n\tctx->req_op->callback(ctx, req, err);\n}\n\nstatic int sec_bd_send(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tint ret;\n\n\tif (ctx->fake_req_limit <=\n\t    atomic_read(&qp_ctx->qp->qp_status.used) &&\n\t    !(req->flag & CRYPTO_TFM_REQ_MAY_BACKLOG))\n\t\treturn -EBUSY;\n\n\tspin_lock_bh(&qp_ctx->req_lock);\n\tret = hisi_qp_send(qp_ctx->qp, &req->sec_sqe);\n\tif (ctx->fake_req_limit <=\n\t    atomic_read(&qp_ctx->qp->qp_status.used) && !ret) {\n\t\tlist_add_tail(&req->backlog_head, &qp_ctx->backlog);\n\t\tatomic64_inc(&ctx->sec->debug.dfx.send_cnt);\n\t\tatomic64_inc(&ctx->sec->debug.dfx.send_busy_cnt);\n\t\tspin_unlock_bh(&qp_ctx->req_lock);\n\t\treturn -EBUSY;\n\t}\n\tspin_unlock_bh(&qp_ctx->req_lock);\n\n\tif (unlikely(ret == -EBUSY))\n\t\treturn -ENOBUFS;\n\n\tif (likely(!ret)) {\n\t\tret = -EINPROGRESS;\n\t\tatomic64_inc(&ctx->sec->debug.dfx.send_cnt);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int sec_alloc_civ_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tu16 q_depth = res->depth;\n\tint i;\n\n\tres->c_ivin = dma_alloc_coherent(dev, SEC_TOTAL_IV_SZ(q_depth),\n\t\t\t\t\t &res->c_ivin_dma, GFP_KERNEL);\n\tif (!res->c_ivin)\n\t\treturn -ENOMEM;\n\n\tfor (i = 1; i < q_depth; i++) {\n\t\tres[i].c_ivin_dma = res->c_ivin_dma + i * SEC_IV_SIZE;\n\t\tres[i].c_ivin = res->c_ivin + i * SEC_IV_SIZE;\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_free_civ_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tif (res->c_ivin)\n\t\tdma_free_coherent(dev, SEC_TOTAL_IV_SZ(res->depth),\n\t\t\t\t  res->c_ivin, res->c_ivin_dma);\n}\n\nstatic int sec_alloc_aiv_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tu16 q_depth = res->depth;\n\tint i;\n\n\tres->a_ivin = dma_alloc_coherent(dev, SEC_TOTAL_IV_SZ(q_depth),\n\t\t\t\t\t &res->a_ivin_dma, GFP_KERNEL);\n\tif (!res->a_ivin)\n\t\treturn -ENOMEM;\n\n\tfor (i = 1; i < q_depth; i++) {\n\t\tres[i].a_ivin_dma = res->a_ivin_dma + i * SEC_IV_SIZE;\n\t\tres[i].a_ivin = res->a_ivin + i * SEC_IV_SIZE;\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_free_aiv_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tif (res->a_ivin)\n\t\tdma_free_coherent(dev, SEC_TOTAL_IV_SZ(res->depth),\n\t\t\t\t  res->a_ivin, res->a_ivin_dma);\n}\n\nstatic int sec_alloc_mac_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tu16 q_depth = res->depth;\n\tint i;\n\n\tres->out_mac = dma_alloc_coherent(dev, SEC_TOTAL_MAC_SZ(q_depth) << 1,\n\t\t\t\t\t  &res->out_mac_dma, GFP_KERNEL);\n\tif (!res->out_mac)\n\t\treturn -ENOMEM;\n\n\tfor (i = 1; i < q_depth; i++) {\n\t\tres[i].out_mac_dma = res->out_mac_dma +\n\t\t\t\t     i * (SEC_MAX_MAC_LEN << 1);\n\t\tres[i].out_mac = res->out_mac + i * (SEC_MAX_MAC_LEN << 1);\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_free_mac_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tif (res->out_mac)\n\t\tdma_free_coherent(dev, SEC_TOTAL_MAC_SZ(res->depth) << 1,\n\t\t\t\t  res->out_mac, res->out_mac_dma);\n}\n\nstatic void sec_free_pbuf_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tif (res->pbuf)\n\t\tdma_free_coherent(dev, SEC_TOTAL_PBUF_SZ(res->depth),\n\t\t\t\t  res->pbuf, res->pbuf_dma);\n}\n\n \nstatic int sec_alloc_pbuf_resource(struct device *dev, struct sec_alg_res *res)\n{\n\tu16 q_depth = res->depth;\n\tint size = SEC_PBUF_PAGE_NUM(q_depth);\n\tint pbuf_page_offset;\n\tint i, j, k;\n\n\tres->pbuf = dma_alloc_coherent(dev, SEC_TOTAL_PBUF_SZ(q_depth),\n\t\t\t\t&res->pbuf_dma, GFP_KERNEL);\n\tif (!res->pbuf)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i <= size; i++) {\n\t\tpbuf_page_offset = PAGE_SIZE * i;\n\t\tfor (j = 0; j < SEC_PBUF_NUM; j++) {\n\t\t\tk = i * SEC_PBUF_NUM + j;\n\t\t\tif (k == q_depth)\n\t\t\t\tbreak;\n\t\t\tres[k].pbuf = res->pbuf +\n\t\t\t\tj * SEC_PBUF_PKG + pbuf_page_offset;\n\t\t\tres[k].pbuf_dma = res->pbuf_dma +\n\t\t\t\tj * SEC_PBUF_PKG + pbuf_page_offset;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_alg_resource_alloc(struct sec_ctx *ctx,\n\t\t\t\t  struct sec_qp_ctx *qp_ctx)\n{\n\tstruct sec_alg_res *res = qp_ctx->res;\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tret = sec_alloc_civ_resource(dev, res);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctx->alg_type == SEC_AEAD) {\n\t\tret = sec_alloc_aiv_resource(dev, res);\n\t\tif (ret)\n\t\t\tgoto alloc_aiv_fail;\n\n\t\tret = sec_alloc_mac_resource(dev, res);\n\t\tif (ret)\n\t\t\tgoto alloc_mac_fail;\n\t}\n\tif (ctx->pbuf_supported) {\n\t\tret = sec_alloc_pbuf_resource(dev, res);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"fail to alloc pbuf dma resource!\\n\");\n\t\t\tgoto alloc_pbuf_fail;\n\t\t}\n\t}\n\n\treturn 0;\n\nalloc_pbuf_fail:\n\tif (ctx->alg_type == SEC_AEAD)\n\t\tsec_free_mac_resource(dev, qp_ctx->res);\nalloc_mac_fail:\n\tif (ctx->alg_type == SEC_AEAD)\n\t\tsec_free_aiv_resource(dev, res);\nalloc_aiv_fail:\n\tsec_free_civ_resource(dev, res);\n\treturn ret;\n}\n\nstatic void sec_alg_resource_free(struct sec_ctx *ctx,\n\t\t\t\t  struct sec_qp_ctx *qp_ctx)\n{\n\tstruct device *dev = ctx->dev;\n\n\tsec_free_civ_resource(dev, qp_ctx->res);\n\n\tif (ctx->pbuf_supported)\n\t\tsec_free_pbuf_resource(dev, qp_ctx->res);\n\tif (ctx->alg_type == SEC_AEAD)\n\t\tsec_free_mac_resource(dev, qp_ctx->res);\n}\n\nstatic int sec_alloc_qp_ctx_resource(struct hisi_qm *qm, struct sec_ctx *ctx,\n\t\t\t\t     struct sec_qp_ctx *qp_ctx)\n{\n\tu16 q_depth = qp_ctx->qp->sq_depth;\n\tstruct device *dev = ctx->dev;\n\tint ret = -ENOMEM;\n\n\tqp_ctx->req_list = kcalloc(q_depth, sizeof(struct sec_req *), GFP_KERNEL);\n\tif (!qp_ctx->req_list)\n\t\treturn ret;\n\n\tqp_ctx->res = kcalloc(q_depth, sizeof(struct sec_alg_res), GFP_KERNEL);\n\tif (!qp_ctx->res)\n\t\tgoto err_free_req_list;\n\tqp_ctx->res->depth = q_depth;\n\n\tqp_ctx->c_in_pool = hisi_acc_create_sgl_pool(dev, q_depth, SEC_SGL_SGE_NR);\n\tif (IS_ERR(qp_ctx->c_in_pool)) {\n\t\tdev_err(dev, \"fail to create sgl pool for input!\\n\");\n\t\tgoto err_free_res;\n\t}\n\n\tqp_ctx->c_out_pool = hisi_acc_create_sgl_pool(dev, q_depth, SEC_SGL_SGE_NR);\n\tif (IS_ERR(qp_ctx->c_out_pool)) {\n\t\tdev_err(dev, \"fail to create sgl pool for output!\\n\");\n\t\tgoto err_free_c_in_pool;\n\t}\n\n\tret = sec_alg_resource_alloc(ctx, qp_ctx);\n\tif (ret)\n\t\tgoto err_free_c_out_pool;\n\n\treturn 0;\n\nerr_free_c_out_pool:\n\thisi_acc_free_sgl_pool(dev, qp_ctx->c_out_pool);\nerr_free_c_in_pool:\n\thisi_acc_free_sgl_pool(dev, qp_ctx->c_in_pool);\nerr_free_res:\n\tkfree(qp_ctx->res);\nerr_free_req_list:\n\tkfree(qp_ctx->req_list);\n\treturn ret;\n}\n\nstatic void sec_free_qp_ctx_resource(struct sec_ctx *ctx, struct sec_qp_ctx *qp_ctx)\n{\n\tstruct device *dev = ctx->dev;\n\n\tsec_alg_resource_free(ctx, qp_ctx);\n\thisi_acc_free_sgl_pool(dev, qp_ctx->c_out_pool);\n\thisi_acc_free_sgl_pool(dev, qp_ctx->c_in_pool);\n\tkfree(qp_ctx->res);\n\tkfree(qp_ctx->req_list);\n}\n\nstatic int sec_create_qp_ctx(struct hisi_qm *qm, struct sec_ctx *ctx,\n\t\t\t     int qp_ctx_id, int alg_type)\n{\n\tstruct sec_qp_ctx *qp_ctx;\n\tstruct hisi_qp *qp;\n\tint ret;\n\n\tqp_ctx = &ctx->qp_ctx[qp_ctx_id];\n\tqp = ctx->qps[qp_ctx_id];\n\tqp->req_type = 0;\n\tqp->qp_ctx = qp_ctx;\n\tqp_ctx->qp = qp;\n\tqp_ctx->ctx = ctx;\n\n\tqp->req_cb = sec_req_cb;\n\n\tspin_lock_init(&qp_ctx->req_lock);\n\tidr_init(&qp_ctx->req_idr);\n\tINIT_LIST_HEAD(&qp_ctx->backlog);\n\n\tret = sec_alloc_qp_ctx_resource(qm, ctx, qp_ctx);\n\tif (ret)\n\t\tgoto err_destroy_idr;\n\n\tret = hisi_qm_start_qp(qp, 0);\n\tif (ret < 0)\n\t\tgoto err_resource_free;\n\n\treturn 0;\n\nerr_resource_free:\n\tsec_free_qp_ctx_resource(ctx, qp_ctx);\nerr_destroy_idr:\n\tidr_destroy(&qp_ctx->req_idr);\n\treturn ret;\n}\n\nstatic void sec_release_qp_ctx(struct sec_ctx *ctx,\n\t\t\t       struct sec_qp_ctx *qp_ctx)\n{\n\thisi_qm_stop_qp(qp_ctx->qp);\n\tsec_free_qp_ctx_resource(ctx, qp_ctx);\n\tidr_destroy(&qp_ctx->req_idr);\n}\n\nstatic int sec_ctx_base_init(struct sec_ctx *ctx)\n{\n\tstruct sec_dev *sec;\n\tint i, ret;\n\n\tctx->qps = sec_create_qps();\n\tif (!ctx->qps) {\n\t\tpr_err(\"Can not create sec qps!\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tsec = container_of(ctx->qps[0]->qm, struct sec_dev, qm);\n\tctx->sec = sec;\n\tctx->dev = &sec->qm.pdev->dev;\n\tctx->hlf_q_num = sec->ctx_q_num >> 1;\n\n\tctx->pbuf_supported = ctx->sec->iommu_used;\n\n\t \n\tctx->fake_req_limit = ctx->qps[0]->sq_depth >> 1;\n\tctx->qp_ctx = kcalloc(sec->ctx_q_num, sizeof(struct sec_qp_ctx),\n\t\t\t      GFP_KERNEL);\n\tif (!ctx->qp_ctx) {\n\t\tret = -ENOMEM;\n\t\tgoto err_destroy_qps;\n\t}\n\n\tfor (i = 0; i < sec->ctx_q_num; i++) {\n\t\tret = sec_create_qp_ctx(&sec->qm, ctx, i, 0);\n\t\tif (ret)\n\t\t\tgoto err_sec_release_qp_ctx;\n\t}\n\n\treturn 0;\n\nerr_sec_release_qp_ctx:\n\tfor (i = i - 1; i >= 0; i--)\n\t\tsec_release_qp_ctx(ctx, &ctx->qp_ctx[i]);\n\tkfree(ctx->qp_ctx);\nerr_destroy_qps:\n\tsec_destroy_qps(ctx->qps, sec->ctx_q_num);\n\treturn ret;\n}\n\nstatic void sec_ctx_base_uninit(struct sec_ctx *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->sec->ctx_q_num; i++)\n\t\tsec_release_qp_ctx(ctx, &ctx->qp_ctx[i]);\n\n\tsec_destroy_qps(ctx->qps, ctx->sec->ctx_q_num);\n\tkfree(ctx->qp_ctx);\n}\n\nstatic int sec_cipher_init(struct sec_ctx *ctx)\n{\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\n\tc_ctx->c_key = dma_alloc_coherent(ctx->dev, SEC_MAX_KEY_SIZE,\n\t\t\t\t\t  &c_ctx->c_key_dma, GFP_KERNEL);\n\tif (!c_ctx->c_key)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void sec_cipher_uninit(struct sec_ctx *ctx)\n{\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\n\tmemzero_explicit(c_ctx->c_key, SEC_MAX_KEY_SIZE);\n\tdma_free_coherent(ctx->dev, SEC_MAX_KEY_SIZE,\n\t\t\t  c_ctx->c_key, c_ctx->c_key_dma);\n}\n\nstatic int sec_auth_init(struct sec_ctx *ctx)\n{\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\n\ta_ctx->a_key = dma_alloc_coherent(ctx->dev, SEC_MAX_AKEY_SIZE,\n\t\t\t\t\t  &a_ctx->a_key_dma, GFP_KERNEL);\n\tif (!a_ctx->a_key)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void sec_auth_uninit(struct sec_ctx *ctx)\n{\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\n\tmemzero_explicit(a_ctx->a_key, SEC_MAX_AKEY_SIZE);\n\tdma_free_coherent(ctx->dev, SEC_MAX_AKEY_SIZE,\n\t\t\t  a_ctx->a_key, a_ctx->a_key_dma);\n}\n\nstatic int sec_skcipher_fbtfm_init(struct crypto_skcipher *tfm)\n{\n\tconst char *alg = crypto_tfm_alg_name(&tfm->base);\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\n\tc_ctx->fallback = false;\n\n\t \n\tif (likely(strncmp(alg, \"xts\", SEC_XTS_NAME_SZ)))\n\t\treturn 0;\n\n\tc_ctx->fbtfm = crypto_alloc_sync_skcipher(alg, 0,\n\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(c_ctx->fbtfm)) {\n\t\tpr_err(\"failed to alloc xts mode fallback tfm!\\n\");\n\t\treturn PTR_ERR(c_ctx->fbtfm);\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_skcipher_init(struct crypto_skcipher *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint ret;\n\n\tctx->alg_type = SEC_SKCIPHER;\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct sec_req));\n\tctx->c_ctx.ivsize = crypto_skcipher_ivsize(tfm);\n\tif (ctx->c_ctx.ivsize > SEC_IV_SIZE) {\n\t\tpr_err(\"get error skcipher iv size!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = sec_ctx_base_init(ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tret = sec_cipher_init(ctx);\n\tif (ret)\n\t\tgoto err_cipher_init;\n\n\tret = sec_skcipher_fbtfm_init(tfm);\n\tif (ret)\n\t\tgoto err_fbtfm_init;\n\n\treturn 0;\n\nerr_fbtfm_init:\n\tsec_cipher_uninit(ctx);\nerr_cipher_init:\n\tsec_ctx_base_uninit(ctx);\n\treturn ret;\n}\n\nstatic void sec_skcipher_uninit(struct crypto_skcipher *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (ctx->c_ctx.fbtfm)\n\t\tcrypto_free_sync_skcipher(ctx->c_ctx.fbtfm);\n\n\tsec_cipher_uninit(ctx);\n\tsec_ctx_base_uninit(ctx);\n}\n\nstatic int sec_skcipher_3des_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t\t    const u32 keylen,\n\t\t\t\t    const enum sec_cmode c_mode)\n{\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tint ret;\n\n\tret = verify_skcipher_des3_key(tfm, key);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (keylen) {\n\tcase SEC_DES3_2KEY_SIZE:\n\t\tc_ctx->c_key_len = SEC_CKEY_3DES_2KEY;\n\t\tbreak;\n\tcase SEC_DES3_3KEY_SIZE:\n\t\tc_ctx->c_key_len = SEC_CKEY_3DES_3KEY;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_skcipher_aes_sm4_setkey(struct sec_cipher_ctx *c_ctx,\n\t\t\t\t       const u32 keylen,\n\t\t\t\t       const enum sec_cmode c_mode)\n{\n\tif (c_mode == SEC_CMODE_XTS) {\n\t\tswitch (keylen) {\n\t\tcase SEC_XTS_MIN_KEY_SIZE:\n\t\t\tc_ctx->c_key_len = SEC_CKEY_128BIT;\n\t\t\tbreak;\n\t\tcase SEC_XTS_MID_KEY_SIZE:\n\t\t\tc_ctx->fallback = true;\n\t\t\tbreak;\n\t\tcase SEC_XTS_MAX_KEY_SIZE:\n\t\t\tc_ctx->c_key_len = SEC_CKEY_256BIT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"hisi_sec2: xts mode key error!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (c_ctx->c_alg == SEC_CALG_SM4 &&\n\t\t    keylen != AES_KEYSIZE_128) {\n\t\t\tpr_err(\"hisi_sec2: sm4 key error!\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tswitch (keylen) {\n\t\t\tcase AES_KEYSIZE_128:\n\t\t\t\tc_ctx->c_key_len = SEC_CKEY_128BIT;\n\t\t\t\tbreak;\n\t\t\tcase AES_KEYSIZE_192:\n\t\t\t\tc_ctx->c_key_len = SEC_CKEY_192BIT;\n\t\t\t\tbreak;\n\t\t\tcase AES_KEYSIZE_256:\n\t\t\t\tc_ctx->c_key_len = SEC_CKEY_256BIT;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tpr_err(\"hisi_sec2: aes key error!\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t       const u32 keylen, const enum sec_calg c_alg,\n\t\t\t       const enum sec_cmode c_mode)\n{\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tif (c_mode == SEC_CMODE_XTS) {\n\t\tret = xts_verify_key(tfm, key, keylen);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"xts mode key err!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tc_ctx->c_alg  = c_alg;\n\tc_ctx->c_mode = c_mode;\n\n\tswitch (c_alg) {\n\tcase SEC_CALG_3DES:\n\t\tret = sec_skcipher_3des_setkey(tfm, key, keylen, c_mode);\n\t\tbreak;\n\tcase SEC_CALG_AES:\n\tcase SEC_CALG_SM4:\n\t\tret = sec_skcipher_aes_sm4_setkey(c_ctx, keylen, c_mode);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (ret) {\n\t\tdev_err(dev, \"set sec key err!\\n\");\n\t\treturn ret;\n\t}\n\n\tmemcpy(c_ctx->c_key, key, keylen);\n\tif (c_ctx->fallback && c_ctx->fbtfm) {\n\t\tret = crypto_sync_skcipher_setkey(c_ctx->fbtfm, key, keylen);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to set fallback skcipher key!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\n#define GEN_SEC_SETKEY_FUNC(name, c_alg, c_mode)\t\t\t\\\nstatic int sec_setkey_##name(struct crypto_skcipher *tfm, const u8 *key,\\\n\tu32 keylen)\t\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn sec_skcipher_setkey(tfm, key, keylen, c_alg, c_mode);\t\\\n}\n\nGEN_SEC_SETKEY_FUNC(aes_ecb, SEC_CALG_AES, SEC_CMODE_ECB)\nGEN_SEC_SETKEY_FUNC(aes_cbc, SEC_CALG_AES, SEC_CMODE_CBC)\nGEN_SEC_SETKEY_FUNC(aes_xts, SEC_CALG_AES, SEC_CMODE_XTS)\nGEN_SEC_SETKEY_FUNC(aes_ofb, SEC_CALG_AES, SEC_CMODE_OFB)\nGEN_SEC_SETKEY_FUNC(aes_cfb, SEC_CALG_AES, SEC_CMODE_CFB)\nGEN_SEC_SETKEY_FUNC(aes_ctr, SEC_CALG_AES, SEC_CMODE_CTR)\nGEN_SEC_SETKEY_FUNC(3des_ecb, SEC_CALG_3DES, SEC_CMODE_ECB)\nGEN_SEC_SETKEY_FUNC(3des_cbc, SEC_CALG_3DES, SEC_CMODE_CBC)\nGEN_SEC_SETKEY_FUNC(sm4_xts, SEC_CALG_SM4, SEC_CMODE_XTS)\nGEN_SEC_SETKEY_FUNC(sm4_cbc, SEC_CALG_SM4, SEC_CMODE_CBC)\nGEN_SEC_SETKEY_FUNC(sm4_ofb, SEC_CALG_SM4, SEC_CMODE_OFB)\nGEN_SEC_SETKEY_FUNC(sm4_cfb, SEC_CALG_SM4, SEC_CMODE_CFB)\nGEN_SEC_SETKEY_FUNC(sm4_ctr, SEC_CALG_SM4, SEC_CMODE_CTR)\n\nstatic int sec_cipher_pbuf_map(struct sec_ctx *ctx, struct sec_req *req,\n\t\t\tstruct scatterlist *src)\n{\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct aead_request *aead_req = a_req->aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tstruct device *dev = ctx->dev;\n\tint copy_size, pbuf_length;\n\tint req_id = req->req_id;\n\tstruct crypto_aead *tfm;\n\tsize_t authsize;\n\tu8 *mac_offset;\n\n\tif (ctx->alg_type == SEC_AEAD)\n\t\tcopy_size = aead_req->cryptlen + aead_req->assoclen;\n\telse\n\t\tcopy_size = c_req->c_len;\n\n\tpbuf_length = sg_copy_to_buffer(src, sg_nents(src),\n\t\t\tqp_ctx->res[req_id].pbuf, copy_size);\n\tif (unlikely(pbuf_length != copy_size)) {\n\t\tdev_err(dev, \"copy src data to pbuf error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (!c_req->encrypt && ctx->alg_type == SEC_AEAD) {\n\t\ttfm = crypto_aead_reqtfm(aead_req);\n\t\tauthsize = crypto_aead_authsize(tfm);\n\t\tmac_offset = qp_ctx->res[req_id].pbuf + copy_size - authsize;\n\t\tmemcpy(a_req->out_mac, mac_offset, authsize);\n\t}\n\n\treq->in_dma = qp_ctx->res[req_id].pbuf_dma;\n\tc_req->c_out_dma = req->in_dma;\n\n\treturn 0;\n}\n\nstatic void sec_cipher_pbuf_unmap(struct sec_ctx *ctx, struct sec_req *req,\n\t\t\tstruct scatterlist *dst)\n{\n\tstruct aead_request *aead_req = req->aead_req.aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tint copy_size, pbuf_length;\n\tint req_id = req->req_id;\n\n\tif (ctx->alg_type == SEC_AEAD)\n\t\tcopy_size = c_req->c_len + aead_req->assoclen;\n\telse\n\t\tcopy_size = c_req->c_len;\n\n\tpbuf_length = sg_copy_from_buffer(dst, sg_nents(dst),\n\t\t\tqp_ctx->res[req_id].pbuf, copy_size);\n\tif (unlikely(pbuf_length != copy_size))\n\t\tdev_err(ctx->dev, \"copy pbuf data to dst error!\\n\");\n}\n\nstatic int sec_aead_mac_init(struct sec_aead_req *req)\n{\n\tstruct aead_request *aead_req = req->aead_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(aead_req);\n\tsize_t authsize = crypto_aead_authsize(tfm);\n\tu8 *mac_out = req->out_mac;\n\tstruct scatterlist *sgl = aead_req->src;\n\tsize_t copy_size;\n\toff_t skip_size;\n\n\t \n\tskip_size = aead_req->assoclen + aead_req->cryptlen - authsize;\n\tcopy_size = sg_pcopy_to_buffer(sgl, sg_nents(sgl), mac_out,\n\t\t\t\t       authsize, skip_size);\n\tif (unlikely(copy_size != authsize))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int sec_cipher_map(struct sec_ctx *ctx, struct sec_req *req,\n\t\t\t  struct scatterlist *src, struct scatterlist *dst)\n{\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tstruct sec_alg_res *res = &qp_ctx->res[req->req_id];\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tif (req->use_pbuf) {\n\t\tc_req->c_ivin = res->pbuf + SEC_PBUF_IV_OFFSET;\n\t\tc_req->c_ivin_dma = res->pbuf_dma + SEC_PBUF_IV_OFFSET;\n\t\tif (ctx->alg_type == SEC_AEAD) {\n\t\t\ta_req->a_ivin = res->a_ivin;\n\t\t\ta_req->a_ivin_dma = res->a_ivin_dma;\n\t\t\ta_req->out_mac = res->pbuf + SEC_PBUF_MAC_OFFSET;\n\t\t\ta_req->out_mac_dma = res->pbuf_dma +\n\t\t\t\t\tSEC_PBUF_MAC_OFFSET;\n\t\t}\n\t\tret = sec_cipher_pbuf_map(ctx, req, src);\n\n\t\treturn ret;\n\t}\n\tc_req->c_ivin = res->c_ivin;\n\tc_req->c_ivin_dma = res->c_ivin_dma;\n\tif (ctx->alg_type == SEC_AEAD) {\n\t\ta_req->a_ivin = res->a_ivin;\n\t\ta_req->a_ivin_dma = res->a_ivin_dma;\n\t\ta_req->out_mac = res->out_mac;\n\t\ta_req->out_mac_dma = res->out_mac_dma;\n\t}\n\n\treq->in = hisi_acc_sg_buf_map_to_hw_sgl(dev, src,\n\t\t\t\t\t\tqp_ctx->c_in_pool,\n\t\t\t\t\t\treq->req_id,\n\t\t\t\t\t\t&req->in_dma);\n\tif (IS_ERR(req->in)) {\n\t\tdev_err(dev, \"fail to dma map input sgl buffers!\\n\");\n\t\treturn PTR_ERR(req->in);\n\t}\n\n\tif (!c_req->encrypt && ctx->alg_type == SEC_AEAD) {\n\t\tret = sec_aead_mac_init(a_req);\n\t\tif (unlikely(ret)) {\n\t\t\tdev_err(dev, \"fail to init mac data for ICV!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (dst == src) {\n\t\tc_req->c_out = req->in;\n\t\tc_req->c_out_dma = req->in_dma;\n\t} else {\n\t\tc_req->c_out = hisi_acc_sg_buf_map_to_hw_sgl(dev, dst,\n\t\t\t\t\t\t\t     qp_ctx->c_out_pool,\n\t\t\t\t\t\t\t     req->req_id,\n\t\t\t\t\t\t\t     &c_req->c_out_dma);\n\n\t\tif (IS_ERR(c_req->c_out)) {\n\t\t\tdev_err(dev, \"fail to dma map output sgl buffers!\\n\");\n\t\t\thisi_acc_sg_buf_unmap(dev, src, req->in);\n\t\t\treturn PTR_ERR(c_req->c_out);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_cipher_unmap(struct sec_ctx *ctx, struct sec_req *req,\n\t\t\t     struct scatterlist *src, struct scatterlist *dst)\n{\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct device *dev = ctx->dev;\n\n\tif (req->use_pbuf) {\n\t\tsec_cipher_pbuf_unmap(ctx, req, dst);\n\t} else {\n\t\tif (dst != src)\n\t\t\thisi_acc_sg_buf_unmap(dev, src, req->in);\n\n\t\thisi_acc_sg_buf_unmap(dev, dst, c_req->c_out);\n\t}\n}\n\nstatic int sec_skcipher_sgl_map(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct skcipher_request *sq = req->c_req.sk_req;\n\n\treturn sec_cipher_map(ctx, req, sq->src, sq->dst);\n}\n\nstatic void sec_skcipher_sgl_unmap(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct skcipher_request *sq = req->c_req.sk_req;\n\n\tsec_cipher_unmap(ctx, req, sq->src, sq->dst);\n}\n\nstatic int sec_aead_aes_set_key(struct sec_cipher_ctx *c_ctx,\n\t\t\t\tstruct crypto_authenc_keys *keys)\n{\n\tswitch (keys->enckeylen) {\n\tcase AES_KEYSIZE_128:\n\t\tc_ctx->c_key_len = SEC_CKEY_128BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tc_ctx->c_key_len = SEC_CKEY_192BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tc_ctx->c_key_len = SEC_CKEY_256BIT;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"hisi_sec2: aead aes key error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\tmemcpy(c_ctx->c_key, keys->enckey, keys->enckeylen);\n\n\treturn 0;\n}\n\nstatic int sec_aead_auth_set_key(struct sec_auth_ctx *ctx,\n\t\t\t\t struct crypto_authenc_keys *keys)\n{\n\tstruct crypto_shash *hash_tfm = ctx->hash_tfm;\n\tint blocksize, digestsize, ret;\n\n\tif (!keys->authkeylen) {\n\t\tpr_err(\"hisi_sec2: aead auth key error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tblocksize = crypto_shash_blocksize(hash_tfm);\n\tdigestsize = crypto_shash_digestsize(hash_tfm);\n\tif (keys->authkeylen > blocksize) {\n\t\tret = crypto_shash_tfm_digest(hash_tfm, keys->authkey,\n\t\t\t\t\t      keys->authkeylen, ctx->a_key);\n\t\tif (ret) {\n\t\t\tpr_err(\"hisi_sec2: aead auth digest error!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tctx->a_key_len = digestsize;\n\t} else {\n\t\tmemcpy(ctx->a_key, keys->authkey, keys->authkeylen);\n\t\tctx->a_key_len = keys->authkeylen;\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_aead_setauthsize(struct crypto_aead *aead, unsigned int authsize)\n{\n\tstruct crypto_tfm *tfm = crypto_aead_tfm(aead);\n\tstruct sec_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\n\tif (unlikely(a_ctx->fallback_aead_tfm))\n\t\treturn crypto_aead_setauthsize(a_ctx->fallback_aead_tfm, authsize);\n\n\treturn 0;\n}\n\nstatic int sec_aead_fallback_setkey(struct sec_auth_ctx *a_ctx,\n\t\t\t\t    struct crypto_aead *tfm, const u8 *key,\n\t\t\t\t    unsigned int keylen)\n{\n\tcrypto_aead_clear_flags(a_ctx->fallback_aead_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(a_ctx->fallback_aead_tfm,\n\t\t\t      crypto_aead_get_flags(tfm) & CRYPTO_TFM_REQ_MASK);\n\treturn crypto_aead_setkey(a_ctx->fallback_aead_tfm, key, keylen);\n}\n\nstatic int sec_aead_setkey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t   const u32 keylen, const enum sec_hash_alg a_alg,\n\t\t\t   const enum sec_calg c_alg,\n\t\t\t   const enum sec_mac_len mac_len,\n\t\t\t   const enum sec_cmode c_mode)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\tstruct device *dev = ctx->dev;\n\tstruct crypto_authenc_keys keys;\n\tint ret;\n\n\tctx->a_ctx.a_alg = a_alg;\n\tctx->c_ctx.c_alg = c_alg;\n\tctx->a_ctx.mac_len = mac_len;\n\tc_ctx->c_mode = c_mode;\n\n\tif (c_mode == SEC_CMODE_CCM || c_mode == SEC_CMODE_GCM) {\n\t\tret = sec_skcipher_aes_sm4_setkey(c_ctx, keylen, c_mode);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"set sec aes ccm cipher key err!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tmemcpy(c_ctx->c_key, key, keylen);\n\n\t\tif (unlikely(a_ctx->fallback_aead_tfm)) {\n\t\t\tret = sec_aead_fallback_setkey(a_ctx, tfm, key, keylen);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen))\n\t\tgoto bad_key;\n\n\tret = sec_aead_aes_set_key(c_ctx, &keys);\n\tif (ret) {\n\t\tdev_err(dev, \"set sec cipher key err!\\n\");\n\t\tgoto bad_key;\n\t}\n\n\tret = sec_aead_auth_set_key(&ctx->a_ctx, &keys);\n\tif (ret) {\n\t\tdev_err(dev, \"set sec auth key err!\\n\");\n\t\tgoto bad_key;\n\t}\n\n\tif ((ctx->a_ctx.mac_len & SEC_SQE_LEN_RATE_MASK)  ||\n\t    (ctx->a_ctx.a_key_len & SEC_SQE_LEN_RATE_MASK)) {\n\t\tdev_err(dev, \"MAC or AUTH key length error!\\n\");\n\t\tgoto bad_key;\n\t}\n\n\treturn 0;\n\nbad_key:\n\tmemzero_explicit(&keys, sizeof(struct crypto_authenc_keys));\n\treturn -EINVAL;\n}\n\n\n#define GEN_SEC_AEAD_SETKEY_FUNC(name, aalg, calg, maclen, cmode)\t\\\nstatic int sec_setkey_##name(struct crypto_aead *tfm, const u8 *key,\t\\\n\tu32 keylen)\t\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn sec_aead_setkey(tfm, key, keylen, aalg, calg, maclen, cmode);\\\n}\n\nGEN_SEC_AEAD_SETKEY_FUNC(aes_cbc_sha1, SEC_A_HMAC_SHA1,\n\t\t\t SEC_CALG_AES, SEC_HMAC_SHA1_MAC, SEC_CMODE_CBC)\nGEN_SEC_AEAD_SETKEY_FUNC(aes_cbc_sha256, SEC_A_HMAC_SHA256,\n\t\t\t SEC_CALG_AES, SEC_HMAC_SHA256_MAC, SEC_CMODE_CBC)\nGEN_SEC_AEAD_SETKEY_FUNC(aes_cbc_sha512, SEC_A_HMAC_SHA512,\n\t\t\t SEC_CALG_AES, SEC_HMAC_SHA512_MAC, SEC_CMODE_CBC)\nGEN_SEC_AEAD_SETKEY_FUNC(aes_ccm, 0, SEC_CALG_AES,\n\t\t\t SEC_HMAC_CCM_MAC, SEC_CMODE_CCM)\nGEN_SEC_AEAD_SETKEY_FUNC(aes_gcm, 0, SEC_CALG_AES,\n\t\t\t SEC_HMAC_GCM_MAC, SEC_CMODE_GCM)\nGEN_SEC_AEAD_SETKEY_FUNC(sm4_ccm, 0, SEC_CALG_SM4,\n\t\t\t SEC_HMAC_CCM_MAC, SEC_CMODE_CCM)\nGEN_SEC_AEAD_SETKEY_FUNC(sm4_gcm, 0, SEC_CALG_SM4,\n\t\t\t SEC_HMAC_GCM_MAC, SEC_CMODE_GCM)\n\nstatic int sec_aead_sgl_map(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct aead_request *aq = req->aead_req.aead_req;\n\n\treturn sec_cipher_map(ctx, req, aq->src, aq->dst);\n}\n\nstatic void sec_aead_sgl_unmap(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct aead_request *aq = req->aead_req.aead_req;\n\n\tsec_cipher_unmap(ctx, req, aq->src, aq->dst);\n}\n\nstatic int sec_request_transfer(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tint ret;\n\n\tret = ctx->req_op->buf_map(ctx, req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tctx->req_op->do_transfer(ctx, req);\n\n\tret = ctx->req_op->bd_fill(ctx, req);\n\tif (unlikely(ret))\n\t\tgoto unmap_req_buf;\n\n\treturn ret;\n\nunmap_req_buf:\n\tctx->req_op->buf_unmap(ctx, req);\n\treturn ret;\n}\n\nstatic void sec_request_untransfer(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tctx->req_op->buf_unmap(ctx, req);\n}\n\nstatic void sec_skcipher_copy_iv(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct skcipher_request *sk_req = req->c_req.sk_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\n\tmemcpy(c_req->c_ivin, sk_req->iv, ctx->c_ctx.ivsize);\n}\n\nstatic int sec_skcipher_bd_fill(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_sqe *sec_sqe = &req->sec_sqe;\n\tu8 scene, sa_type, da_type;\n\tu8 bd_type, cipher;\n\tu8 de = 0;\n\n\tmemset(sec_sqe, 0, sizeof(struct sec_sqe));\n\n\tsec_sqe->type2.c_key_addr = cpu_to_le64(c_ctx->c_key_dma);\n\tsec_sqe->type2.c_ivin_addr = cpu_to_le64(c_req->c_ivin_dma);\n\tsec_sqe->type2.data_src_addr = cpu_to_le64(req->in_dma);\n\tsec_sqe->type2.data_dst_addr = cpu_to_le64(c_req->c_out_dma);\n\n\tsec_sqe->type2.icvw_kmode |= cpu_to_le16(((u16)c_ctx->c_mode) <<\n\t\t\t\t\t\tSEC_CMODE_OFFSET);\n\tsec_sqe->type2.c_alg = c_ctx->c_alg;\n\tsec_sqe->type2.icvw_kmode |= cpu_to_le16(((u16)c_ctx->c_key_len) <<\n\t\t\t\t\t\tSEC_CKEY_OFFSET);\n\n\tbd_type = SEC_BD_TYPE2;\n\tif (c_req->encrypt)\n\t\tcipher = SEC_CIPHER_ENC << SEC_CIPHER_OFFSET;\n\telse\n\t\tcipher = SEC_CIPHER_DEC << SEC_CIPHER_OFFSET;\n\tsec_sqe->type_cipher_auth = bd_type | cipher;\n\n\t \n\tif (req->use_pbuf) {\n\t\tsa_type = SEC_PBUF << SEC_SRC_SGL_OFFSET;\n\t\tda_type = SEC_PBUF << SEC_DST_SGL_OFFSET;\n\t} else {\n\t\tsa_type = SEC_SGL << SEC_SRC_SGL_OFFSET;\n\t\tda_type = SEC_SGL << SEC_DST_SGL_OFFSET;\n\t}\n\n\tsec_sqe->sdm_addr_type |= da_type;\n\tscene = SEC_COMM_SCENE << SEC_SCENE_OFFSET;\n\tif (req->in_dma != c_req->c_out_dma)\n\t\tde = 0x1 << SEC_DE_OFFSET;\n\n\tsec_sqe->sds_sa_type = (de | scene | sa_type);\n\n\tsec_sqe->type2.clen_ivhlen |= cpu_to_le32(c_req->c_len);\n\tsec_sqe->type2.tag = cpu_to_le16((u16)req->req_id);\n\n\treturn 0;\n}\n\nstatic int sec_skcipher_bd_fill_v3(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_sqe3 *sec_sqe3 = &req->sec_sqe3;\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tu32 bd_param = 0;\n\tu16 cipher;\n\n\tmemset(sec_sqe3, 0, sizeof(struct sec_sqe3));\n\n\tsec_sqe3->c_key_addr = cpu_to_le64(c_ctx->c_key_dma);\n\tsec_sqe3->no_scene.c_ivin_addr = cpu_to_le64(c_req->c_ivin_dma);\n\tsec_sqe3->data_src_addr = cpu_to_le64(req->in_dma);\n\tsec_sqe3->data_dst_addr = cpu_to_le64(c_req->c_out_dma);\n\n\tsec_sqe3->c_mode_alg = ((u8)c_ctx->c_alg << SEC_CALG_OFFSET_V3) |\n\t\t\t\t\t\tc_ctx->c_mode;\n\tsec_sqe3->c_icv_key |= cpu_to_le16(((u16)c_ctx->c_key_len) <<\n\t\t\t\t\t\tSEC_CKEY_OFFSET_V3);\n\n\tif (c_req->encrypt)\n\t\tcipher = SEC_CIPHER_ENC;\n\telse\n\t\tcipher = SEC_CIPHER_DEC;\n\tsec_sqe3->c_icv_key |= cpu_to_le16(cipher);\n\n\t \n\tsec_sqe3->auth_mac_key = cpu_to_le32((u32)SEC_CTR_CNT_ROLLOVER <<\n\t\t\t\t\tSEC_CTR_CNT_OFFSET);\n\n\tif (req->use_pbuf) {\n\t\tbd_param |= SEC_PBUF << SEC_SRC_SGL_OFFSET_V3;\n\t\tbd_param |= SEC_PBUF << SEC_DST_SGL_OFFSET_V3;\n\t} else {\n\t\tbd_param |= SEC_SGL << SEC_SRC_SGL_OFFSET_V3;\n\t\tbd_param |= SEC_SGL << SEC_DST_SGL_OFFSET_V3;\n\t}\n\n\tbd_param |= SEC_COMM_SCENE << SEC_SCENE_OFFSET_V3;\n\tif (req->in_dma != c_req->c_out_dma)\n\t\tbd_param |= 0x1 << SEC_DE_OFFSET_V3;\n\n\tbd_param |= SEC_BD_TYPE3;\n\tsec_sqe3->bd_param = cpu_to_le32(bd_param);\n\n\tsec_sqe3->c_len_ivin |= cpu_to_le32(c_req->c_len);\n\tsec_sqe3->tag = cpu_to_le64(req);\n\n\treturn 0;\n}\n\n \nstatic void ctr_iv_inc(__u8 *counter, __u8 bits, __u32 nums)\n{\n\tdo {\n\t\t--bits;\n\t\tnums += counter[bits];\n\t\tcounter[bits] = nums & BITS_MASK;\n\t\tnums >>= BYTE_BITS;\n\t} while (bits && nums);\n}\n\nstatic void sec_update_iv(struct sec_req *req, enum sec_alg_type alg_type)\n{\n\tstruct aead_request *aead_req = req->aead_req.aead_req;\n\tstruct skcipher_request *sk_req = req->c_req.sk_req;\n\tu32 iv_size = req->ctx->c_ctx.ivsize;\n\tstruct scatterlist *sgl;\n\tunsigned int cryptlen;\n\tsize_t sz;\n\tu8 *iv;\n\n\tif (req->c_req.encrypt)\n\t\tsgl = alg_type == SEC_SKCIPHER ? sk_req->dst : aead_req->dst;\n\telse\n\t\tsgl = alg_type == SEC_SKCIPHER ? sk_req->src : aead_req->src;\n\n\tif (alg_type == SEC_SKCIPHER) {\n\t\tiv = sk_req->iv;\n\t\tcryptlen = sk_req->cryptlen;\n\t} else {\n\t\tiv = aead_req->iv;\n\t\tcryptlen = aead_req->cryptlen;\n\t}\n\n\tif (req->ctx->c_ctx.c_mode == SEC_CMODE_CBC) {\n\t\tsz = sg_pcopy_to_buffer(sgl, sg_nents(sgl), iv, iv_size,\n\t\t\t\t\tcryptlen - iv_size);\n\t\tif (unlikely(sz != iv_size))\n\t\t\tdev_err(req->ctx->dev, \"copy output iv error!\\n\");\n\t} else {\n\t\tsz = cryptlen / iv_size;\n\t\tif (cryptlen % iv_size)\n\t\t\tsz += 1;\n\t\tctr_iv_inc(iv, iv_size, sz);\n\t}\n}\n\nstatic struct sec_req *sec_back_req_clear(struct sec_ctx *ctx,\n\t\t\t\tstruct sec_qp_ctx *qp_ctx)\n{\n\tstruct sec_req *backlog_req = NULL;\n\n\tspin_lock_bh(&qp_ctx->req_lock);\n\tif (ctx->fake_req_limit >=\n\t    atomic_read(&qp_ctx->qp->qp_status.used) &&\n\t    !list_empty(&qp_ctx->backlog)) {\n\t\tbacklog_req = list_first_entry(&qp_ctx->backlog,\n\t\t\t\ttypeof(*backlog_req), backlog_head);\n\t\tlist_del(&backlog_req->backlog_head);\n\t}\n\tspin_unlock_bh(&qp_ctx->req_lock);\n\n\treturn backlog_req;\n}\n\nstatic void sec_skcipher_callback(struct sec_ctx *ctx, struct sec_req *req,\n\t\t\t\t  int err)\n{\n\tstruct skcipher_request *sk_req = req->c_req.sk_req;\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tstruct skcipher_request *backlog_sk_req;\n\tstruct sec_req *backlog_req;\n\n\tsec_free_req_id(req);\n\n\t \n\tif (!err && (ctx->c_ctx.c_mode == SEC_CMODE_CBC ||\n\t    ctx->c_ctx.c_mode == SEC_CMODE_CTR) && req->c_req.encrypt)\n\t\tsec_update_iv(req, SEC_SKCIPHER);\n\n\twhile (1) {\n\t\tbacklog_req = sec_back_req_clear(ctx, qp_ctx);\n\t\tif (!backlog_req)\n\t\t\tbreak;\n\n\t\tbacklog_sk_req = backlog_req->c_req.sk_req;\n\t\tskcipher_request_complete(backlog_sk_req, -EINPROGRESS);\n\t\tatomic64_inc(&ctx->sec->debug.dfx.recv_busy_cnt);\n\t}\n\n\tskcipher_request_complete(sk_req, err);\n}\n\nstatic void set_aead_auth_iv(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct aead_request *aead_req = req->aead_req.aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tsize_t authsize = ctx->a_ctx.mac_len;\n\tu32 data_size = aead_req->cryptlen;\n\tu8 flage = 0;\n\tu8 cm, cl;\n\n\t \n\tcl = c_req->c_ivin[0] + 1;\n\tc_req->c_ivin[ctx->c_ctx.ivsize - cl] = 0x00;\n\tmemset(&c_req->c_ivin[ctx->c_ctx.ivsize - cl], 0, cl);\n\tc_req->c_ivin[ctx->c_ctx.ivsize - IV_LAST_BYTE1] = IV_CTR_INIT;\n\n\t \n\tflage |= c_req->c_ivin[0] & IV_CL_MASK;\n\n\t \n\tcm = (authsize - IV_CM_CAL_NUM) / IV_CM_CAL_NUM;\n\tflage |= cm << IV_CM_OFFSET;\n\tif (aead_req->assoclen)\n\t\tflage |= 0x01 << IV_FLAGS_OFFSET;\n\n\tmemcpy(a_req->a_ivin, c_req->c_ivin, ctx->c_ctx.ivsize);\n\ta_req->a_ivin[0] = flage;\n\n\t \n\tif (!c_req->encrypt)\n\t\tdata_size = aead_req->cryptlen - authsize;\n\n\ta_req->a_ivin[ctx->c_ctx.ivsize - IV_LAST_BYTE1] =\n\t\t\tdata_size & IV_LAST_BYTE_MASK;\n\tdata_size >>= IV_BYTE_OFFSET;\n\ta_req->a_ivin[ctx->c_ctx.ivsize - IV_LAST_BYTE2] =\n\t\t\tdata_size & IV_LAST_BYTE_MASK;\n}\n\nstatic void sec_aead_set_iv(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct aead_request *aead_req = req->aead_req.aead_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(aead_req);\n\tsize_t authsize = crypto_aead_authsize(tfm);\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\n\tmemcpy(c_req->c_ivin, aead_req->iv, ctx->c_ctx.ivsize);\n\n\tif (ctx->c_ctx.c_mode == SEC_CMODE_CCM) {\n\t\t \n\t\tctx->a_ctx.mac_len = authsize;\n\t\t \n\t\tset_aead_auth_iv(ctx, req);\n\t}\n\n\t \n\tif (ctx->c_ctx.c_mode == SEC_CMODE_GCM) {\n\t\tctx->a_ctx.mac_len = authsize;\n\t\tmemcpy(a_req->a_ivin, c_req->c_ivin, SEC_AIV_SIZE);\n\t}\n}\n\nstatic void sec_auth_bd_fill_xcm(struct sec_auth_ctx *ctx, int dir,\n\t\t\t\t struct sec_req *req, struct sec_sqe *sec_sqe)\n{\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct aead_request *aq = a_req->aead_req;\n\n\t \n\tsec_sqe->type2.icvw_kmode |= cpu_to_le16((u16)ctx->mac_len);\n\n\t \n\tsec_sqe->type2.a_key_addr = sec_sqe->type2.c_key_addr;\n\tsec_sqe->type2.a_ivin_addr = cpu_to_le64(a_req->a_ivin_dma);\n\tsec_sqe->type_cipher_auth |= SEC_NO_AUTH << SEC_AUTH_OFFSET;\n\n\tif (dir)\n\t\tsec_sqe->sds_sa_type &= SEC_CIPHER_AUTH;\n\telse\n\t\tsec_sqe->sds_sa_type |= SEC_AUTH_CIPHER;\n\n\tsec_sqe->type2.alen_ivllen = cpu_to_le32(aq->assoclen);\n\tsec_sqe->type2.auth_src_offset = cpu_to_le16(0x0);\n\tsec_sqe->type2.cipher_src_offset = cpu_to_le16((u16)aq->assoclen);\n\n\tsec_sqe->type2.mac_addr = cpu_to_le64(a_req->out_mac_dma);\n}\n\nstatic void sec_auth_bd_fill_xcm_v3(struct sec_auth_ctx *ctx, int dir,\n\t\t\t\t    struct sec_req *req, struct sec_sqe3 *sqe3)\n{\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct aead_request *aq = a_req->aead_req;\n\n\t \n\tsqe3->c_icv_key |= cpu_to_le16((u16)ctx->mac_len << SEC_MAC_OFFSET_V3);\n\n\t \n\tsqe3->a_key_addr = sqe3->c_key_addr;\n\tsqe3->auth_ivin.a_ivin_addr = cpu_to_le64(a_req->a_ivin_dma);\n\tsqe3->auth_mac_key |= SEC_NO_AUTH;\n\n\tif (dir)\n\t\tsqe3->huk_iv_seq &= SEC_CIPHER_AUTH_V3;\n\telse\n\t\tsqe3->huk_iv_seq |= SEC_AUTH_CIPHER_V3;\n\n\tsqe3->a_len_key = cpu_to_le32(aq->assoclen);\n\tsqe3->auth_src_offset = cpu_to_le16(0x0);\n\tsqe3->cipher_src_offset = cpu_to_le16((u16)aq->assoclen);\n\tsqe3->mac_addr = cpu_to_le64(a_req->out_mac_dma);\n}\n\nstatic void sec_auth_bd_fill_ex(struct sec_auth_ctx *ctx, int dir,\n\t\t\t       struct sec_req *req, struct sec_sqe *sec_sqe)\n{\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct aead_request *aq = a_req->aead_req;\n\n\tsec_sqe->type2.a_key_addr = cpu_to_le64(ctx->a_key_dma);\n\n\tsec_sqe->type2.mac_key_alg =\n\t\t\tcpu_to_le32(ctx->mac_len / SEC_SQE_LEN_RATE);\n\n\tsec_sqe->type2.mac_key_alg |=\n\t\t\tcpu_to_le32((u32)((ctx->a_key_len) /\n\t\t\tSEC_SQE_LEN_RATE) << SEC_AKEY_OFFSET);\n\n\tsec_sqe->type2.mac_key_alg |=\n\t\t\tcpu_to_le32((u32)(ctx->a_alg) << SEC_AEAD_ALG_OFFSET);\n\n\tif (dir) {\n\t\tsec_sqe->type_cipher_auth |= SEC_AUTH_TYPE1 << SEC_AUTH_OFFSET;\n\t\tsec_sqe->sds_sa_type &= SEC_CIPHER_AUTH;\n\t} else {\n\t\tsec_sqe->type_cipher_auth |= SEC_AUTH_TYPE2 << SEC_AUTH_OFFSET;\n\t\tsec_sqe->sds_sa_type |= SEC_AUTH_CIPHER;\n\t}\n\tsec_sqe->type2.alen_ivllen = cpu_to_le32(c_req->c_len + aq->assoclen);\n\n\tsec_sqe->type2.cipher_src_offset = cpu_to_le16((u16)aq->assoclen);\n\n\tsec_sqe->type2.mac_addr = cpu_to_le64(a_req->out_mac_dma);\n}\n\nstatic int sec_aead_bd_fill(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_auth_ctx *auth_ctx = &ctx->a_ctx;\n\tstruct sec_sqe *sec_sqe = &req->sec_sqe;\n\tint ret;\n\n\tret = sec_skcipher_bd_fill(ctx, req);\n\tif (unlikely(ret)) {\n\t\tdev_err(ctx->dev, \"skcipher bd fill is error!\\n\");\n\t\treturn ret;\n\t}\n\n\tif (ctx->c_ctx.c_mode == SEC_CMODE_CCM ||\n\t    ctx->c_ctx.c_mode == SEC_CMODE_GCM)\n\t\tsec_auth_bd_fill_xcm(auth_ctx, req->c_req.encrypt, req, sec_sqe);\n\telse\n\t\tsec_auth_bd_fill_ex(auth_ctx, req->c_req.encrypt, req, sec_sqe);\n\n\treturn 0;\n}\n\nstatic void sec_auth_bd_fill_ex_v3(struct sec_auth_ctx *ctx, int dir,\n\t\t\t\t   struct sec_req *req, struct sec_sqe3 *sqe3)\n{\n\tstruct sec_aead_req *a_req = &req->aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tstruct aead_request *aq = a_req->aead_req;\n\n\tsqe3->a_key_addr = cpu_to_le64(ctx->a_key_dma);\n\n\tsqe3->auth_mac_key |=\n\t\t\tcpu_to_le32((u32)(ctx->mac_len /\n\t\t\tSEC_SQE_LEN_RATE) << SEC_MAC_OFFSET_V3);\n\n\tsqe3->auth_mac_key |=\n\t\t\tcpu_to_le32((u32)(ctx->a_key_len /\n\t\t\tSEC_SQE_LEN_RATE) << SEC_AKEY_OFFSET_V3);\n\n\tsqe3->auth_mac_key |=\n\t\t\tcpu_to_le32((u32)(ctx->a_alg) << SEC_AUTH_ALG_OFFSET_V3);\n\n\tif (dir) {\n\t\tsqe3->auth_mac_key |= cpu_to_le32((u32)SEC_AUTH_TYPE1);\n\t\tsqe3->huk_iv_seq &= SEC_CIPHER_AUTH_V3;\n\t} else {\n\t\tsqe3->auth_mac_key |= cpu_to_le32((u32)SEC_AUTH_TYPE2);\n\t\tsqe3->huk_iv_seq |= SEC_AUTH_CIPHER_V3;\n\t}\n\tsqe3->a_len_key = cpu_to_le32(c_req->c_len + aq->assoclen);\n\n\tsqe3->cipher_src_offset = cpu_to_le16((u16)aq->assoclen);\n\n\tsqe3->mac_addr = cpu_to_le64(a_req->out_mac_dma);\n}\n\nstatic int sec_aead_bd_fill_v3(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_auth_ctx *auth_ctx = &ctx->a_ctx;\n\tstruct sec_sqe3 *sec_sqe3 = &req->sec_sqe3;\n\tint ret;\n\n\tret = sec_skcipher_bd_fill_v3(ctx, req);\n\tif (unlikely(ret)) {\n\t\tdev_err(ctx->dev, \"skcipher bd3 fill is error!\\n\");\n\t\treturn ret;\n\t}\n\n\tif (ctx->c_ctx.c_mode == SEC_CMODE_CCM ||\n\t    ctx->c_ctx.c_mode == SEC_CMODE_GCM)\n\t\tsec_auth_bd_fill_xcm_v3(auth_ctx, req->c_req.encrypt,\n\t\t\t\t\treq, sec_sqe3);\n\telse\n\t\tsec_auth_bd_fill_ex_v3(auth_ctx, req->c_req.encrypt,\n\t\t\t\t       req, sec_sqe3);\n\n\treturn 0;\n}\n\nstatic void sec_aead_callback(struct sec_ctx *c, struct sec_req *req, int err)\n{\n\tstruct aead_request *a_req = req->aead_req.aead_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(a_req);\n\tstruct sec_aead_req *aead_req = &req->aead_req;\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tsize_t authsize = crypto_aead_authsize(tfm);\n\tstruct sec_qp_ctx *qp_ctx = req->qp_ctx;\n\tstruct aead_request *backlog_aead_req;\n\tstruct sec_req *backlog_req;\n\tsize_t sz;\n\n\tif (!err && c->c_ctx.c_mode == SEC_CMODE_CBC && c_req->encrypt)\n\t\tsec_update_iv(req, SEC_AEAD);\n\n\t \n\tif (!err && c_req->encrypt) {\n\t\tstruct scatterlist *sgl = a_req->dst;\n\n\t\tsz = sg_pcopy_from_buffer(sgl, sg_nents(sgl),\n\t\t\t\t\t  aead_req->out_mac,\n\t\t\t\t\t  authsize, a_req->cryptlen +\n\t\t\t\t\t  a_req->assoclen);\n\t\tif (unlikely(sz != authsize)) {\n\t\t\tdev_err(c->dev, \"copy out mac err!\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\n\tsec_free_req_id(req);\n\n\twhile (1) {\n\t\tbacklog_req = sec_back_req_clear(c, qp_ctx);\n\t\tif (!backlog_req)\n\t\t\tbreak;\n\n\t\tbacklog_aead_req = backlog_req->aead_req.aead_req;\n\t\taead_request_complete(backlog_aead_req, -EINPROGRESS);\n\t\tatomic64_inc(&c->sec->debug.dfx.recv_busy_cnt);\n\t}\n\n\taead_request_complete(a_req, err);\n}\n\nstatic void sec_request_uninit(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tsec_free_req_id(req);\n\tsec_free_queue_id(ctx, req);\n}\n\nstatic int sec_request_init(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_qp_ctx *qp_ctx;\n\tint queue_id;\n\n\t \n\tqueue_id = sec_alloc_queue_id(ctx, req);\n\tqp_ctx = &ctx->qp_ctx[queue_id];\n\n\treq->req_id = sec_alloc_req_id(req, qp_ctx);\n\tif (unlikely(req->req_id < 0)) {\n\t\tsec_free_queue_id(ctx, req);\n\t\treturn req->req_id;\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_process(struct sec_ctx *ctx, struct sec_req *req)\n{\n\tstruct sec_cipher_req *c_req = &req->c_req;\n\tint ret;\n\n\tret = sec_request_init(ctx, req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = sec_request_transfer(ctx, req);\n\tif (unlikely(ret))\n\t\tgoto err_uninit_req;\n\n\t \n\tif (!req->c_req.encrypt && (ctx->c_ctx.c_mode == SEC_CMODE_CBC ||\n\t    ctx->c_ctx.c_mode == SEC_CMODE_CTR))\n\t\tsec_update_iv(req, ctx->alg_type);\n\n\tret = ctx->req_op->bd_send(ctx, req);\n\tif (unlikely((ret != -EBUSY && ret != -EINPROGRESS) ||\n\t\t(ret == -EBUSY && !(req->flag & CRYPTO_TFM_REQ_MAY_BACKLOG)))) {\n\t\tdev_err_ratelimited(ctx->dev, \"send sec request failed!\\n\");\n\t\tgoto err_send_req;\n\t}\n\n\treturn ret;\n\nerr_send_req:\n\t \n\tif (ctx->c_ctx.c_mode == SEC_CMODE_CBC && !req->c_req.encrypt) {\n\t\tif (ctx->alg_type == SEC_SKCIPHER)\n\t\t\tmemcpy(req->c_req.sk_req->iv, c_req->c_ivin,\n\t\t\t       ctx->c_ctx.ivsize);\n\t\telse\n\t\t\tmemcpy(req->aead_req.aead_req->iv, c_req->c_ivin,\n\t\t\t       ctx->c_ctx.ivsize);\n\t}\n\n\tsec_request_untransfer(ctx, req);\nerr_uninit_req:\n\tsec_request_uninit(ctx, req);\n\treturn ret;\n}\n\nstatic const struct sec_req_op sec_skcipher_req_ops = {\n\t.buf_map\t= sec_skcipher_sgl_map,\n\t.buf_unmap\t= sec_skcipher_sgl_unmap,\n\t.do_transfer\t= sec_skcipher_copy_iv,\n\t.bd_fill\t= sec_skcipher_bd_fill,\n\t.bd_send\t= sec_bd_send,\n\t.callback\t= sec_skcipher_callback,\n\t.process\t= sec_process,\n};\n\nstatic const struct sec_req_op sec_aead_req_ops = {\n\t.buf_map\t= sec_aead_sgl_map,\n\t.buf_unmap\t= sec_aead_sgl_unmap,\n\t.do_transfer\t= sec_aead_set_iv,\n\t.bd_fill\t= sec_aead_bd_fill,\n\t.bd_send\t= sec_bd_send,\n\t.callback\t= sec_aead_callback,\n\t.process\t= sec_process,\n};\n\nstatic const struct sec_req_op sec_skcipher_req_ops_v3 = {\n\t.buf_map\t= sec_skcipher_sgl_map,\n\t.buf_unmap\t= sec_skcipher_sgl_unmap,\n\t.do_transfer\t= sec_skcipher_copy_iv,\n\t.bd_fill\t= sec_skcipher_bd_fill_v3,\n\t.bd_send\t= sec_bd_send,\n\t.callback\t= sec_skcipher_callback,\n\t.process\t= sec_process,\n};\n\nstatic const struct sec_req_op sec_aead_req_ops_v3 = {\n\t.buf_map\t= sec_aead_sgl_map,\n\t.buf_unmap\t= sec_aead_sgl_unmap,\n\t.do_transfer\t= sec_aead_set_iv,\n\t.bd_fill\t= sec_aead_bd_fill_v3,\n\t.bd_send\t= sec_bd_send,\n\t.callback\t= sec_aead_callback,\n\t.process\t= sec_process,\n};\n\nstatic int sec_skcipher_ctx_init(struct crypto_skcipher *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint ret;\n\n\tret = sec_skcipher_init(tfm);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctx->sec->qm.ver < QM_HW_V3) {\n\t\tctx->type_supported = SEC_BD_TYPE2;\n\t\tctx->req_op = &sec_skcipher_req_ops;\n\t} else {\n\t\tctx->type_supported = SEC_BD_TYPE3;\n\t\tctx->req_op = &sec_skcipher_req_ops_v3;\n\t}\n\n\treturn ret;\n}\n\nstatic void sec_skcipher_ctx_exit(struct crypto_skcipher *tfm)\n{\n\tsec_skcipher_uninit(tfm);\n}\n\nstatic int sec_aead_init(struct crypto_aead *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\tint ret;\n\n\tcrypto_aead_set_reqsize(tfm, sizeof(struct sec_req));\n\tctx->alg_type = SEC_AEAD;\n\tctx->c_ctx.ivsize = crypto_aead_ivsize(tfm);\n\tif (ctx->c_ctx.ivsize < SEC_AIV_SIZE ||\n\t    ctx->c_ctx.ivsize > SEC_IV_SIZE) {\n\t\tpr_err(\"get error aead iv size!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = sec_ctx_base_init(ctx);\n\tif (ret)\n\t\treturn ret;\n\tif (ctx->sec->qm.ver < QM_HW_V3) {\n\t\tctx->type_supported = SEC_BD_TYPE2;\n\t\tctx->req_op = &sec_aead_req_ops;\n\t} else {\n\t\tctx->type_supported = SEC_BD_TYPE3;\n\t\tctx->req_op = &sec_aead_req_ops_v3;\n\t}\n\n\tret = sec_auth_init(ctx);\n\tif (ret)\n\t\tgoto err_auth_init;\n\n\tret = sec_cipher_init(ctx);\n\tif (ret)\n\t\tgoto err_cipher_init;\n\n\treturn ret;\n\nerr_cipher_init:\n\tsec_auth_uninit(ctx);\nerr_auth_init:\n\tsec_ctx_base_uninit(ctx);\n\treturn ret;\n}\n\nstatic void sec_aead_exit(struct crypto_aead *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tsec_cipher_uninit(ctx);\n\tsec_auth_uninit(ctx);\n\tsec_ctx_base_uninit(ctx);\n}\n\nstatic int sec_aead_ctx_init(struct crypto_aead *tfm, const char *hash_name)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct sec_auth_ctx *auth_ctx = &ctx->a_ctx;\n\tint ret;\n\n\tret = sec_aead_init(tfm);\n\tif (ret) {\n\t\tpr_err(\"hisi_sec2: aead init error!\\n\");\n\t\treturn ret;\n\t}\n\n\tauth_ctx->hash_tfm = crypto_alloc_shash(hash_name, 0, 0);\n\tif (IS_ERR(auth_ctx->hash_tfm)) {\n\t\tdev_err(ctx->dev, \"aead alloc shash error!\\n\");\n\t\tsec_aead_exit(tfm);\n\t\treturn PTR_ERR(auth_ctx->hash_tfm);\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_aead_ctx_exit(struct crypto_aead *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tcrypto_free_shash(ctx->a_ctx.hash_tfm);\n\tsec_aead_exit(tfm);\n}\n\nstatic int sec_aead_xcm_ctx_init(struct crypto_aead *tfm)\n{\n\tstruct aead_alg *alg = crypto_aead_alg(tfm);\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\tconst char *aead_name = alg->base.cra_name;\n\tint ret;\n\n\tret = sec_aead_init(tfm);\n\tif (ret) {\n\t\tdev_err(ctx->dev, \"hisi_sec2: aead xcm init error!\\n\");\n\t\treturn ret;\n\t}\n\n\ta_ctx->fallback_aead_tfm = crypto_alloc_aead(aead_name, 0,\n\t\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK |\n\t\t\t\t\t\t     CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(a_ctx->fallback_aead_tfm)) {\n\t\tdev_err(ctx->dev, \"aead driver alloc fallback tfm error!\\n\");\n\t\tsec_aead_exit(tfm);\n\t\treturn PTR_ERR(a_ctx->fallback_aead_tfm);\n\t}\n\ta_ctx->fallback = false;\n\n\treturn 0;\n}\n\nstatic void sec_aead_xcm_ctx_exit(struct crypto_aead *tfm)\n{\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tcrypto_free_aead(ctx->a_ctx.fallback_aead_tfm);\n\tsec_aead_exit(tfm);\n}\n\nstatic int sec_aead_sha1_ctx_init(struct crypto_aead *tfm)\n{\n\treturn sec_aead_ctx_init(tfm, \"sha1\");\n}\n\nstatic int sec_aead_sha256_ctx_init(struct crypto_aead *tfm)\n{\n\treturn sec_aead_ctx_init(tfm, \"sha256\");\n}\n\nstatic int sec_aead_sha512_ctx_init(struct crypto_aead *tfm)\n{\n\treturn sec_aead_ctx_init(tfm, \"sha512\");\n}\n\nstatic int sec_skcipher_cryptlen_check(struct sec_ctx *ctx,\n\tstruct sec_req *sreq)\n{\n\tu32 cryptlen = sreq->c_req.sk_req->cryptlen;\n\tstruct device *dev = ctx->dev;\n\tu8 c_mode = ctx->c_ctx.c_mode;\n\tint ret = 0;\n\n\tswitch (c_mode) {\n\tcase SEC_CMODE_XTS:\n\t\tif (unlikely(cryptlen < AES_BLOCK_SIZE)) {\n\t\t\tdev_err(dev, \"skcipher XTS mode input length error!\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase SEC_CMODE_ECB:\n\tcase SEC_CMODE_CBC:\n\t\tif (unlikely(cryptlen & (AES_BLOCK_SIZE - 1))) {\n\t\t\tdev_err(dev, \"skcipher AES input length error!\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase SEC_CMODE_CFB:\n\tcase SEC_CMODE_OFB:\n\tcase SEC_CMODE_CTR:\n\t\tif (unlikely(ctx->sec->qm.ver < QM_HW_V3)) {\n\t\t\tdev_err(dev, \"skcipher HW version error!\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int sec_skcipher_param_check(struct sec_ctx *ctx, struct sec_req *sreq)\n{\n\tstruct skcipher_request *sk_req = sreq->c_req.sk_req;\n\tstruct device *dev = ctx->dev;\n\tu8 c_alg = ctx->c_ctx.c_alg;\n\n\tif (unlikely(!sk_req->src || !sk_req->dst ||\n\t\t     sk_req->cryptlen > MAX_INPUT_DATA_LEN)) {\n\t\tdev_err(dev, \"skcipher input param error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\tsreq->c_req.c_len = sk_req->cryptlen;\n\n\tif (ctx->pbuf_supported && sk_req->cryptlen <= SEC_PBUF_SZ)\n\t\tsreq->use_pbuf = true;\n\telse\n\t\tsreq->use_pbuf = false;\n\n\tif (c_alg == SEC_CALG_3DES) {\n\t\tif (unlikely(sk_req->cryptlen & (DES3_EDE_BLOCK_SIZE - 1))) {\n\t\t\tdev_err(dev, \"skcipher 3des input length error!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t} else if (c_alg == SEC_CALG_AES || c_alg == SEC_CALG_SM4) {\n\t\treturn sec_skcipher_cryptlen_check(ctx, sreq);\n\t}\n\n\tdev_err(dev, \"skcipher algorithm error!\\n\");\n\n\treturn -EINVAL;\n}\n\nstatic int sec_skcipher_soft_crypto(struct sec_ctx *ctx,\n\t\t\t\t    struct skcipher_request *sreq, bool encrypt)\n{\n\tstruct sec_cipher_ctx *c_ctx = &ctx->c_ctx;\n\tSYNC_SKCIPHER_REQUEST_ON_STACK(subreq, c_ctx->fbtfm);\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tif (!c_ctx->fbtfm) {\n\t\tdev_err_ratelimited(dev, \"the soft tfm isn't supported in the current system.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tskcipher_request_set_sync_tfm(subreq, c_ctx->fbtfm);\n\n\t \n\tskcipher_request_set_callback(subreq, sreq->base.flags,\n\t\t\t\t      NULL, NULL);\n\tskcipher_request_set_crypt(subreq, sreq->src, sreq->dst,\n\t\t\t\t   sreq->cryptlen, sreq->iv);\n\tif (encrypt)\n\t\tret = crypto_skcipher_encrypt(subreq);\n\telse\n\t\tret = crypto_skcipher_decrypt(subreq);\n\n\tskcipher_request_zero(subreq);\n\n\treturn ret;\n}\n\nstatic int sec_skcipher_crypto(struct skcipher_request *sk_req, bool encrypt)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(sk_req);\n\tstruct sec_req *req = skcipher_request_ctx(sk_req);\n\tstruct sec_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint ret;\n\n\tif (!sk_req->cryptlen) {\n\t\tif (ctx->c_ctx.c_mode == SEC_CMODE_XTS)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\treq->flag = sk_req->base.flags;\n\treq->c_req.sk_req = sk_req;\n\treq->c_req.encrypt = encrypt;\n\treq->ctx = ctx;\n\n\tret = sec_skcipher_param_check(ctx, req);\n\tif (unlikely(ret))\n\t\treturn -EINVAL;\n\n\tif (unlikely(ctx->c_ctx.fallback))\n\t\treturn sec_skcipher_soft_crypto(ctx, sk_req, encrypt);\n\n\treturn ctx->req_op->process(ctx, req);\n}\n\nstatic int sec_skcipher_encrypt(struct skcipher_request *sk_req)\n{\n\treturn sec_skcipher_crypto(sk_req, true);\n}\n\nstatic int sec_skcipher_decrypt(struct skcipher_request *sk_req)\n{\n\treturn sec_skcipher_crypto(sk_req, false);\n}\n\n#define SEC_SKCIPHER_GEN_ALG(sec_cra_name, sec_set_key, sec_min_key_size, \\\n\tsec_max_key_size, ctx_init, ctx_exit, blk_size, iv_size)\\\n{\\\n\t.base = {\\\n\t\t.cra_name = sec_cra_name,\\\n\t\t.cra_driver_name = \"hisi_sec_\"sec_cra_name,\\\n\t\t.cra_priority = SEC_PRIORITY,\\\n\t\t.cra_flags = CRYPTO_ALG_ASYNC |\\\n\t\t CRYPTO_ALG_NEED_FALLBACK,\\\n\t\t.cra_blocksize = blk_size,\\\n\t\t.cra_ctxsize = sizeof(struct sec_ctx),\\\n\t\t.cra_module = THIS_MODULE,\\\n\t},\\\n\t.init = ctx_init,\\\n\t.exit = ctx_exit,\\\n\t.setkey = sec_set_key,\\\n\t.decrypt = sec_skcipher_decrypt,\\\n\t.encrypt = sec_skcipher_encrypt,\\\n\t.min_keysize = sec_min_key_size,\\\n\t.max_keysize = sec_max_key_size,\\\n\t.ivsize = iv_size,\\\n}\n\n#define SEC_SKCIPHER_ALG(name, key_func, min_key_size, \\\n\tmax_key_size, blk_size, iv_size) \\\n\tSEC_SKCIPHER_GEN_ALG(name, key_func, min_key_size, max_key_size, \\\n\tsec_skcipher_ctx_init, sec_skcipher_ctx_exit, blk_size, iv_size)\n\nstatic struct sec_skcipher sec_skciphers[] = {\n\t{\n\t\t.alg_msk = BIT(0),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ecb(aes)\", sec_setkey_aes_ecb, AES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MAX_KEY_SIZE, AES_BLOCK_SIZE, 0),\n\t},\n\t{\n\t\t.alg_msk = BIT(1),\n\t\t.alg = SEC_SKCIPHER_ALG(\"cbc(aes)\", sec_setkey_aes_cbc, AES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MAX_KEY_SIZE, AES_BLOCK_SIZE, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(2),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ctr(aes)\", sec_setkey_aes_ctr,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MAX_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(3),\n\t\t.alg = SEC_SKCIPHER_ALG(\"xts(aes)\", sec_setkey_aes_xts,\tSEC_XTS_MIN_KEY_SIZE,\n\t\t\t\t\tSEC_XTS_MAX_KEY_SIZE, AES_BLOCK_SIZE, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(4),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ofb(aes)\", sec_setkey_aes_ofb,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MAX_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(5),\n\t\t.alg = SEC_SKCIPHER_ALG(\"cfb(aes)\", sec_setkey_aes_cfb,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MAX_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(12),\n\t\t.alg = SEC_SKCIPHER_ALG(\"cbc(sm4)\", sec_setkey_sm4_cbc,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MIN_KEY_SIZE, AES_BLOCK_SIZE, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(13),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ctr(sm4)\", sec_setkey_sm4_ctr, AES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MIN_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(14),\n\t\t.alg = SEC_SKCIPHER_ALG(\"xts(sm4)\", sec_setkey_sm4_xts,\tSEC_XTS_MIN_KEY_SIZE,\n\t\t\t\t\tSEC_XTS_MIN_KEY_SIZE, AES_BLOCK_SIZE, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(15),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ofb(sm4)\", sec_setkey_sm4_ofb,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MIN_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(16),\n\t\t.alg = SEC_SKCIPHER_ALG(\"cfb(sm4)\", sec_setkey_sm4_cfb,\tAES_MIN_KEY_SIZE,\n\t\t\t\t\tAES_MIN_KEY_SIZE, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(23),\n\t\t.alg = SEC_SKCIPHER_ALG(\"ecb(des3_ede)\", sec_setkey_3des_ecb, SEC_DES3_3KEY_SIZE,\n\t\t\t\t\tSEC_DES3_3KEY_SIZE, DES3_EDE_BLOCK_SIZE, 0),\n\t},\n\t{\n\t\t.alg_msk = BIT(24),\n\t\t.alg = SEC_SKCIPHER_ALG(\"cbc(des3_ede)\", sec_setkey_3des_cbc, SEC_DES3_3KEY_SIZE,\n\t\t\t\t\tSEC_DES3_3KEY_SIZE, DES3_EDE_BLOCK_SIZE,\n\t\t\t\t\tDES3_EDE_BLOCK_SIZE),\n\t},\n};\n\nstatic int aead_iv_demension_check(struct aead_request *aead_req)\n{\n\tu8 cl;\n\n\tcl = aead_req->iv[0] + 1;\n\tif (cl < IV_CL_MIN || cl > IV_CL_MAX)\n\t\treturn -EINVAL;\n\n\tif (cl < IV_CL_MID && aead_req->cryptlen >> (BYTE_BITS * cl))\n\t\treturn -EOVERFLOW;\n\n\treturn 0;\n}\n\nstatic int sec_aead_spec_check(struct sec_ctx *ctx, struct sec_req *sreq)\n{\n\tstruct aead_request *req = sreq->aead_req.aead_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tsize_t authsize = crypto_aead_authsize(tfm);\n\tu8 c_mode = ctx->c_ctx.c_mode;\n\tstruct device *dev = ctx->dev;\n\tint ret;\n\n\tif (unlikely(req->cryptlen + req->assoclen > MAX_INPUT_DATA_LEN ||\n\t    req->assoclen > SEC_MAX_AAD_LEN)) {\n\t\tdev_err(dev, \"aead input spec error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely((c_mode == SEC_CMODE_GCM && authsize < DES_BLOCK_SIZE) ||\n\t   (c_mode == SEC_CMODE_CCM && (authsize < MIN_MAC_LEN ||\n\t\tauthsize & MAC_LEN_MASK)))) {\n\t\tdev_err(dev, \"aead input mac length error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (c_mode == SEC_CMODE_CCM) {\n\t\tif (unlikely(req->assoclen > SEC_MAX_CCM_AAD_LEN)) {\n\t\t\tdev_err_ratelimited(dev, \"CCM input aad parameter is too long!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tret = aead_iv_demension_check(req);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"aead input iv param error!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (sreq->c_req.encrypt)\n\t\tsreq->c_req.c_len = req->cryptlen;\n\telse\n\t\tsreq->c_req.c_len = req->cryptlen - authsize;\n\tif (c_mode == SEC_CMODE_CBC) {\n\t\tif (unlikely(sreq->c_req.c_len & (AES_BLOCK_SIZE - 1))) {\n\t\t\tdev_err(dev, \"aead crypto length error!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_aead_param_check(struct sec_ctx *ctx, struct sec_req *sreq)\n{\n\tstruct aead_request *req = sreq->aead_req.aead_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tsize_t authsize = crypto_aead_authsize(tfm);\n\tstruct device *dev = ctx->dev;\n\tu8 c_alg = ctx->c_ctx.c_alg;\n\n\tif (unlikely(!req->src || !req->dst)) {\n\t\tdev_err(dev, \"aead input param error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx->sec->qm.ver == QM_HW_V2) {\n\t\tif (unlikely(!req->cryptlen || (!sreq->c_req.encrypt &&\n\t\t    req->cryptlen <= authsize))) {\n\t\t\tctx->a_ctx.fallback = true;\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(c_alg != SEC_CALG_AES && c_alg != SEC_CALG_SM4)) {\n\t\tdev_err(dev, \"aead crypto alg error!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(sec_aead_spec_check(ctx, sreq)))\n\t\treturn -EINVAL;\n\n\tif (ctx->pbuf_supported && (req->cryptlen + req->assoclen) <=\n\t\tSEC_PBUF_SZ)\n\t\tsreq->use_pbuf = true;\n\telse\n\t\tsreq->use_pbuf = false;\n\n\treturn 0;\n}\n\nstatic int sec_aead_soft_crypto(struct sec_ctx *ctx,\n\t\t\t\tstruct aead_request *aead_req,\n\t\t\t\tbool encrypt)\n{\n\tstruct sec_auth_ctx *a_ctx = &ctx->a_ctx;\n\tstruct device *dev = ctx->dev;\n\tstruct aead_request *subreq;\n\tint ret;\n\n\t \n\tif (!a_ctx->fallback_aead_tfm) {\n\t\tdev_err(dev, \"aead fallback tfm is NULL!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsubreq = aead_request_alloc(a_ctx->fallback_aead_tfm, GFP_KERNEL);\n\tif (!subreq)\n\t\treturn -ENOMEM;\n\n\taead_request_set_tfm(subreq, a_ctx->fallback_aead_tfm);\n\taead_request_set_callback(subreq, aead_req->base.flags,\n\t\t\t\t  aead_req->base.complete, aead_req->base.data);\n\taead_request_set_crypt(subreq, aead_req->src, aead_req->dst,\n\t\t\t       aead_req->cryptlen, aead_req->iv);\n\taead_request_set_ad(subreq, aead_req->assoclen);\n\n\tif (encrypt)\n\t\tret = crypto_aead_encrypt(subreq);\n\telse\n\t\tret = crypto_aead_decrypt(subreq);\n\taead_request_free(subreq);\n\n\treturn ret;\n}\n\nstatic int sec_aead_crypto(struct aead_request *a_req, bool encrypt)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(a_req);\n\tstruct sec_req *req = aead_request_ctx(a_req);\n\tstruct sec_ctx *ctx = crypto_aead_ctx(tfm);\n\tint ret;\n\n\treq->flag = a_req->base.flags;\n\treq->aead_req.aead_req = a_req;\n\treq->c_req.encrypt = encrypt;\n\treq->ctx = ctx;\n\n\tret = sec_aead_param_check(ctx, req);\n\tif (unlikely(ret)) {\n\t\tif (ctx->a_ctx.fallback)\n\t\t\treturn sec_aead_soft_crypto(ctx, a_req, encrypt);\n\t\treturn -EINVAL;\n\t}\n\n\treturn ctx->req_op->process(ctx, req);\n}\n\nstatic int sec_aead_encrypt(struct aead_request *a_req)\n{\n\treturn sec_aead_crypto(a_req, true);\n}\n\nstatic int sec_aead_decrypt(struct aead_request *a_req)\n{\n\treturn sec_aead_crypto(a_req, false);\n}\n\n#define SEC_AEAD_ALG(sec_cra_name, sec_set_key, ctx_init,\\\n\t\t\t ctx_exit, blk_size, iv_size, max_authsize)\\\n{\\\n\t.base = {\\\n\t\t.cra_name = sec_cra_name,\\\n\t\t.cra_driver_name = \"hisi_sec_\"sec_cra_name,\\\n\t\t.cra_priority = SEC_PRIORITY,\\\n\t\t.cra_flags = CRYPTO_ALG_ASYNC |\\\n\t\t CRYPTO_ALG_NEED_FALLBACK,\\\n\t\t.cra_blocksize = blk_size,\\\n\t\t.cra_ctxsize = sizeof(struct sec_ctx),\\\n\t\t.cra_module = THIS_MODULE,\\\n\t},\\\n\t.init = ctx_init,\\\n\t.exit = ctx_exit,\\\n\t.setkey = sec_set_key,\\\n\t.setauthsize = sec_aead_setauthsize,\\\n\t.decrypt = sec_aead_decrypt,\\\n\t.encrypt = sec_aead_encrypt,\\\n\t.ivsize = iv_size,\\\n\t.maxauthsize = max_authsize,\\\n}\n\nstatic struct sec_aead sec_aeads[] = {\n\t{\n\t\t.alg_msk = BIT(6),\n\t\t.alg = SEC_AEAD_ALG(\"ccm(aes)\", sec_setkey_aes_ccm, sec_aead_xcm_ctx_init,\n\t\t\t\t    sec_aead_xcm_ctx_exit, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(7),\n\t\t.alg = SEC_AEAD_ALG(\"gcm(aes)\", sec_setkey_aes_gcm, sec_aead_xcm_ctx_init,\n\t\t\t\t    sec_aead_xcm_ctx_exit, SEC_MIN_BLOCK_SZ, SEC_AIV_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(17),\n\t\t.alg = SEC_AEAD_ALG(\"ccm(sm4)\", sec_setkey_sm4_ccm, sec_aead_xcm_ctx_init,\n\t\t\t\t    sec_aead_xcm_ctx_exit, SEC_MIN_BLOCK_SZ, AES_BLOCK_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(18),\n\t\t.alg = SEC_AEAD_ALG(\"gcm(sm4)\", sec_setkey_sm4_gcm, sec_aead_xcm_ctx_init,\n\t\t\t\t    sec_aead_xcm_ctx_exit, SEC_MIN_BLOCK_SZ, SEC_AIV_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(43),\n\t\t.alg = SEC_AEAD_ALG(\"authenc(hmac(sha1),cbc(aes))\", sec_setkey_aes_cbc_sha1,\n\t\t\t\t    sec_aead_sha1_ctx_init, sec_aead_ctx_exit, AES_BLOCK_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE, SHA1_DIGEST_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(44),\n\t\t.alg = SEC_AEAD_ALG(\"authenc(hmac(sha256),cbc(aes))\", sec_setkey_aes_cbc_sha256,\n\t\t\t\t    sec_aead_sha256_ctx_init, sec_aead_ctx_exit, AES_BLOCK_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE, SHA256_DIGEST_SIZE),\n\t},\n\t{\n\t\t.alg_msk = BIT(45),\n\t\t.alg = SEC_AEAD_ALG(\"authenc(hmac(sha512),cbc(aes))\", sec_setkey_aes_cbc_sha512,\n\t\t\t\t    sec_aead_sha512_ctx_init, sec_aead_ctx_exit, AES_BLOCK_SIZE,\n\t\t\t\t    AES_BLOCK_SIZE, SHA512_DIGEST_SIZE),\n\t},\n};\n\nstatic void sec_unregister_skcipher(u64 alg_mask, int end)\n{\n\tint i;\n\n\tfor (i = 0; i < end; i++)\n\t\tif (sec_skciphers[i].alg_msk & alg_mask)\n\t\t\tcrypto_unregister_skcipher(&sec_skciphers[i].alg);\n}\n\nstatic int sec_register_skcipher(u64 alg_mask)\n{\n\tint i, ret, count;\n\n\tcount = ARRAY_SIZE(sec_skciphers);\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!(sec_skciphers[i].alg_msk & alg_mask))\n\t\t\tcontinue;\n\n\t\tret = crypto_register_skcipher(&sec_skciphers[i].alg);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tsec_unregister_skcipher(alg_mask, i);\n\n\treturn ret;\n}\n\nstatic void sec_unregister_aead(u64 alg_mask, int end)\n{\n\tint i;\n\n\tfor (i = 0; i < end; i++)\n\t\tif (sec_aeads[i].alg_msk & alg_mask)\n\t\t\tcrypto_unregister_aead(&sec_aeads[i].alg);\n}\n\nstatic int sec_register_aead(u64 alg_mask)\n{\n\tint i, ret, count;\n\n\tcount = ARRAY_SIZE(sec_aeads);\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!(sec_aeads[i].alg_msk & alg_mask))\n\t\t\tcontinue;\n\n\t\tret = crypto_register_aead(&sec_aeads[i].alg);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tsec_unregister_aead(alg_mask, i);\n\n\treturn ret;\n}\n\nint sec_register_to_crypto(struct hisi_qm *qm)\n{\n\tu64 alg_mask;\n\tint ret = 0;\n\n\talg_mask = sec_get_alg_bitmap(qm, SEC_DRV_ALG_BITMAP_HIGH_IDX,\n\t\t\t\t      SEC_DRV_ALG_BITMAP_LOW_IDX);\n\n\n\tret = sec_register_skcipher(alg_mask);\n\tif (ret)\n\t\treturn ret;\n\n\tret = sec_register_aead(alg_mask);\n\tif (ret)\n\t\tsec_unregister_skcipher(alg_mask, ARRAY_SIZE(sec_skciphers));\n\n\treturn ret;\n}\n\nvoid sec_unregister_from_crypto(struct hisi_qm *qm)\n{\n\tu64 alg_mask;\n\n\talg_mask = sec_get_alg_bitmap(qm, SEC_DRV_ALG_BITMAP_HIGH_IDX,\n\t\t\t\t      SEC_DRV_ALG_BITMAP_LOW_IDX);\n\n\tsec_unregister_aead(alg_mask, ARRAY_SIZE(sec_aeads));\n\tsec_unregister_skcipher(alg_mask, ARRAY_SIZE(sec_skciphers));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}