{
  "module_name": "sec_drv.c",
  "hash_id": "45a8412d50b45747d850cb732b208ba6c048470c670a690c3bd72179a84bdab9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/hisilicon/sec/sec_drv.c",
  "human_readable_source": "\n \n#include <linux/acpi.h>\n#include <linux/atomic.h>\n#include <linux/delay.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/io.h>\n#include <linux/iommu.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/irqreturn.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n\n#include \"sec_drv.h\"\n\n#define SEC_QUEUE_AR_FROCE_ALLOC\t\t\t0\n#define SEC_QUEUE_AR_FROCE_NOALLOC\t\t\t1\n#define SEC_QUEUE_AR_FROCE_DIS\t\t\t\t2\n\n#define SEC_QUEUE_AW_FROCE_ALLOC\t\t\t0\n#define SEC_QUEUE_AW_FROCE_NOALLOC\t\t\t1\n#define SEC_QUEUE_AW_FROCE_DIS\t\t\t\t2\n\n \n#define SEC_ALGSUB_CLK_EN_REG\t\t\t\t0x03b8\n#define SEC_ALGSUB_CLK_DIS_REG\t\t\t\t0x03bc\n#define SEC_ALGSUB_CLK_ST_REG\t\t\t\t0x535c\n#define SEC_ALGSUB_RST_REQ_REG\t\t\t\t0x0aa8\n#define SEC_ALGSUB_RST_DREQ_REG\t\t\t\t0x0aac\n#define SEC_ALGSUB_RST_ST_REG\t\t\t\t0x5a54\n#define   SEC_ALGSUB_RST_ST_IS_RST\t\t\tBIT(0)\n\n#define SEC_ALGSUB_BUILD_RST_REQ_REG\t\t\t0x0ab8\n#define SEC_ALGSUB_BUILD_RST_DREQ_REG\t\t\t0x0abc\n#define SEC_ALGSUB_BUILD_RST_ST_REG\t\t\t0x5a5c\n#define   SEC_ALGSUB_BUILD_RST_ST_IS_RST\t\tBIT(0)\n\n#define SEC_SAA_BASE\t\t\t\t\t0x00001000UL\n\n \n#define SEC_SAA_CTRL_REG(x)\t((x) * SEC_SAA_ADDR_SIZE)\n#define   SEC_SAA_CTRL_GET_QM_EN\t\t\tBIT(0)\n\n#define SEC_ST_INTMSK1_REG\t\t\t\t0x0200\n#define SEC_ST_RINT1_REG\t\t\t\t0x0400\n#define SEC_ST_INTSTS1_REG\t\t\t\t0x0600\n#define SEC_BD_MNG_STAT_REG\t\t\t\t0x0800\n#define SEC_PARSING_STAT_REG\t\t\t\t0x0804\n#define SEC_LOAD_TIME_OUT_CNT_REG\t\t\t0x0808\n#define SEC_CORE_WORK_TIME_OUT_CNT_REG\t\t\t0x080c\n#define SEC_BACK_TIME_OUT_CNT_REG\t\t\t0x0810\n#define SEC_BD1_PARSING_RD_TIME_OUT_CNT_REG\t\t0x0814\n#define SEC_BD1_PARSING_WR_TIME_OUT_CNT_REG\t\t0x0818\n#define SEC_BD2_PARSING_RD_TIME_OUT_CNT_REG\t\t0x081c\n#define SEC_BD2_PARSING_WR_TIME_OUT_CNT_REG\t\t0x0820\n#define SEC_SAA_ACC_REG\t\t\t\t\t0x083c\n#define SEC_BD_NUM_CNT_IN_SEC_REG\t\t\t0x0858\n#define SEC_LOAD_WORK_TIME_CNT_REG\t\t\t0x0860\n#define SEC_CORE_WORK_WORK_TIME_CNT_REG\t\t\t0x0864\n#define SEC_BACK_WORK_TIME_CNT_REG\t\t\t0x0868\n#define SEC_SAA_IDLE_TIME_CNT_REG\t\t\t0x086c\n#define SEC_SAA_CLK_CNT_REG\t\t\t\t0x0870\n\n \n#define SEC_CLK_EN_REG\t\t\t\t\t0x0000\n#define SEC_CTRL_REG\t\t\t\t\t0x0004\n\n#define SEC_COMMON_CNT_CLR_CE_REG\t\t\t0x0008\n#define   SEC_COMMON_CNT_CLR_CE_CLEAR\t\t\tBIT(0)\n#define   SEC_COMMON_CNT_CLR_CE_SNAP_EN\t\t\tBIT(1)\n\n#define SEC_SECURE_CTRL_REG\t\t\t\t0x000c\n#define SEC_AXI_CACHE_CFG_REG\t\t\t\t0x0010\n#define SEC_AXI_QOS_CFG_REG\t\t\t\t0x0014\n#define SEC_IPV4_MASK_TABLE_REG\t\t\t\t0x0020\n#define SEC_IPV6_MASK_TABLE_X_REG(x)\t(0x0024 + (x) * 4)\n#define SEC_FSM_MAX_CNT_REG\t\t\t\t0x0064\n\n#define SEC_CTRL2_REG\t\t\t\t\t0x0068\n#define   SEC_CTRL2_DATA_AXI_RD_OTSD_CFG_M\t\tGENMASK(3, 0)\n#define   SEC_CTRL2_DATA_AXI_RD_OTSD_CFG_S\t\t0\n#define   SEC_CTRL2_DATA_AXI_WR_OTSD_CFG_M\t\tGENMASK(6, 4)\n#define   SEC_CTRL2_DATA_AXI_WR_OTSD_CFG_S\t\t4\n#define   SEC_CTRL2_CLK_GATE_EN\t\t\t\tBIT(7)\n#define   SEC_CTRL2_ENDIAN_BD\t\t\t\tBIT(8)\n#define   SEC_CTRL2_ENDIAN_BD_TYPE\t\t\tBIT(9)\n\n#define SEC_CNT_PRECISION_CFG_REG\t\t\t0x006c\n#define SEC_DEBUG_BD_CFG_REG\t\t\t\t0x0070\n#define   SEC_DEBUG_BD_CFG_WB_NORMAL\t\t\tBIT(0)\n#define   SEC_DEBUG_BD_CFG_WB_EN\t\t\tBIT(1)\n\n#define SEC_Q_SIGHT_SEL\t\t\t\t\t0x0074\n#define SEC_Q_SIGHT_HIS_CLR\t\t\t\t0x0078\n#define SEC_Q_VMID_CFG_REG(q)\t\t(0x0100 + (q) * 4)\n#define SEC_Q_WEIGHT_CFG_REG(q)\t\t(0x200 + (q) * 4)\n#define SEC_STAT_CLR_REG\t\t\t\t0x0a00\n#define SEC_SAA_IDLE_CNT_CLR_REG\t\t\t0x0a04\n#define SEC_QM_CPL_Q_IDBUF_DFX_CFG_REG\t\t\t0x0b00\n#define SEC_QM_CPL_Q_IDBUF_DFX_RESULT_REG\t\t0x0b04\n#define SEC_QM_BD_DFX_CFG_REG\t\t\t\t0x0b08\n#define SEC_QM_BD_DFX_RESULT_REG\t\t\t0x0b0c\n#define SEC_QM_BDID_DFX_RESULT_REG\t\t\t0x0b10\n#define SEC_QM_BD_DFIFO_STATUS_REG\t\t\t0x0b14\n#define SEC_QM_BD_DFX_CFG2_REG\t\t\t\t0x0b1c\n#define SEC_QM_BD_DFX_RESULT2_REG\t\t\t0x0b20\n#define SEC_QM_BD_IDFIFO_STATUS_REG\t\t\t0x0b18\n#define SEC_QM_BD_DFIFO_STATUS2_REG\t\t\t0x0b28\n#define SEC_QM_BD_IDFIFO_STATUS2_REG\t\t\t0x0b2c\n\n#define SEC_HASH_IPV4_MASK\t\t\t\t0xfff00000\n#define SEC_MAX_SAA_NUM\t\t\t\t\t0xa\n#define SEC_SAA_ADDR_SIZE\t\t\t\t0x1000\n\n#define SEC_Q_INIT_REG\t\t\t\t\t0x0\n#define   SEC_Q_INIT_WO_STAT_CLEAR\t\t\t0x2\n#define   SEC_Q_INIT_AND_STAT_CLEAR\t\t\t0x3\n\n#define SEC_Q_CFG_REG\t\t\t\t\t0x8\n#define   SEC_Q_CFG_REORDER\t\t\t\tBIT(0)\n\n#define SEC_Q_PROC_NUM_CFG_REG\t\t\t\t0x10\n#define SEC_QUEUE_ENB_REG\t\t\t\t0x18\n\n#define SEC_Q_DEPTH_CFG_REG\t\t\t\t0x50\n#define   SEC_Q_DEPTH_CFG_DEPTH_M\t\t\tGENMASK(11, 0)\n#define   SEC_Q_DEPTH_CFG_DEPTH_S\t\t\t0\n\n#define SEC_Q_BASE_HADDR_REG\t\t\t\t0x54\n#define SEC_Q_BASE_LADDR_REG\t\t\t\t0x58\n#define SEC_Q_WR_PTR_REG\t\t\t\t0x5c\n#define SEC_Q_OUTORDER_BASE_HADDR_REG\t\t\t0x60\n#define SEC_Q_OUTORDER_BASE_LADDR_REG\t\t\t0x64\n#define SEC_Q_OUTORDER_RD_PTR_REG\t\t\t0x68\n#define SEC_Q_OT_TH_REG\t\t\t\t\t0x6c\n\n#define SEC_Q_ARUSER_CFG_REG\t\t\t\t0x70\n#define   SEC_Q_ARUSER_CFG_FA\t\t\t\tBIT(0)\n#define   SEC_Q_ARUSER_CFG_FNA\t\t\t\tBIT(1)\n#define   SEC_Q_ARUSER_CFG_RINVLD\t\t\tBIT(2)\n#define   SEC_Q_ARUSER_CFG_PKG\t\t\t\tBIT(3)\n\n#define SEC_Q_AWUSER_CFG_REG\t\t\t\t0x74\n#define   SEC_Q_AWUSER_CFG_FA\t\t\t\tBIT(0)\n#define   SEC_Q_AWUSER_CFG_FNA\t\t\t\tBIT(1)\n#define   SEC_Q_AWUSER_CFG_PKG\t\t\t\tBIT(2)\n\n#define SEC_Q_ERR_BASE_HADDR_REG\t\t\t0x7c\n#define SEC_Q_ERR_BASE_LADDR_REG\t\t\t0x80\n#define SEC_Q_CFG_VF_NUM_REG\t\t\t\t0x84\n#define SEC_Q_SOFT_PROC_PTR_REG\t\t\t\t0x88\n#define SEC_Q_FAIL_INT_MSK_REG\t\t\t\t0x300\n#define SEC_Q_FLOW_INT_MKS_REG\t\t\t\t0x304\n#define SEC_Q_FAIL_RINT_REG\t\t\t\t0x400\n#define SEC_Q_FLOW_RINT_REG\t\t\t\t0x404\n#define SEC_Q_FAIL_INT_STATUS_REG\t\t\t0x500\n#define SEC_Q_FLOW_INT_STATUS_REG\t\t\t0x504\n#define SEC_Q_STATUS_REG\t\t\t\t0x600\n#define SEC_Q_RD_PTR_REG\t\t\t\t0x604\n#define SEC_Q_PRO_PTR_REG\t\t\t\t0x608\n#define SEC_Q_OUTORDER_WR_PTR_REG\t\t\t0x60c\n#define SEC_Q_OT_CNT_STATUS_REG\t\t\t\t0x610\n#define SEC_Q_INORDER_BD_NUM_ST_REG\t\t\t0x650\n#define SEC_Q_INORDER_GET_FLAG_ST_REG\t\t\t0x654\n#define SEC_Q_INORDER_ADD_FLAG_ST_REG\t\t\t0x658\n#define SEC_Q_INORDER_TASK_INT_NUM_LEFT_ST_REG\t\t0x65c\n#define SEC_Q_RD_DONE_PTR_REG\t\t\t\t0x660\n#define SEC_Q_CPL_Q_BD_NUM_ST_REG\t\t\t0x700\n#define SEC_Q_CPL_Q_PTR_ST_REG\t\t\t\t0x704\n#define SEC_Q_CPL_Q_H_ADDR_ST_REG\t\t\t0x708\n#define SEC_Q_CPL_Q_L_ADDR_ST_REG\t\t\t0x70c\n#define SEC_Q_CPL_TASK_INT_NUM_LEFT_ST_REG\t\t0x710\n#define SEC_Q_WRR_ID_CHECK_REG\t\t\t\t0x714\n#define SEC_Q_CPLQ_FULL_CHECK_REG\t\t\t0x718\n#define SEC_Q_SUCCESS_BD_CNT_REG\t\t\t0x800\n#define SEC_Q_FAIL_BD_CNT_REG\t\t\t\t0x804\n#define SEC_Q_GET_BD_CNT_REG\t\t\t\t0x808\n#define SEC_Q_IVLD_CNT_REG\t\t\t\t0x80c\n#define SEC_Q_BD_PROC_GET_CNT_REG\t\t\t0x810\n#define SEC_Q_BD_PROC_DONE_CNT_REG\t\t\t0x814\n#define SEC_Q_LAT_CLR_REG\t\t\t\t0x850\n#define SEC_Q_PKT_LAT_MAX_REG\t\t\t\t0x854\n#define SEC_Q_PKT_LAT_AVG_REG\t\t\t\t0x858\n#define SEC_Q_PKT_LAT_MIN_REG\t\t\t\t0x85c\n#define SEC_Q_ID_CLR_CFG_REG\t\t\t\t0x900\n#define SEC_Q_1ST_BD_ERR_ID_REG\t\t\t\t0x904\n#define SEC_Q_1ST_AUTH_FAIL_ID_REG\t\t\t0x908\n#define SEC_Q_1ST_RD_ERR_ID_REG\t\t\t\t0x90c\n#define SEC_Q_1ST_ECC2_ERR_ID_REG\t\t\t0x910\n#define SEC_Q_1ST_IVLD_ID_REG\t\t\t\t0x914\n#define SEC_Q_1ST_BD_WR_ERR_ID_REG\t\t\t0x918\n#define SEC_Q_1ST_ERR_BD_WR_ERR_ID_REG\t\t\t0x91c\n#define SEC_Q_1ST_BD_MAC_WR_ERR_ID_REG\t\t\t0x920\n\nstruct sec_debug_bd_info {\n#define SEC_DEBUG_BD_INFO_SOFT_ERR_CHECK_M\tGENMASK(22, 0)\n\tu32 soft_err_check;\n#define SEC_DEBUG_BD_INFO_HARD_ERR_CHECK_M\tGENMASK(9, 0)\n\tu32 hard_err_check;\n\tu32 icv_mac1st_word;\n#define SEC_DEBUG_BD_INFO_GET_ID_M\t\tGENMASK(19, 0)\n\tu32 sec_get_id;\n\t \n\tu32 reserv_left[12];\n};\n\nstruct sec_out_bd_info\t{\n#define SEC_OUT_BD_INFO_Q_ID_M\t\t\tGENMASK(11, 0)\n#define SEC_OUT_BD_INFO_ECC_2BIT_ERR\t\tBIT(14)\n\tu16 data;\n};\n\n#define SEC_MAX_DEVICES\t\t\t\t8\nstatic struct sec_dev_info *sec_devices[SEC_MAX_DEVICES];\nstatic DEFINE_MUTEX(sec_id_lock);\n\nstatic int sec_queue_map_io(struct sec_queue *queue)\n{\n\tstruct device *dev = queue->dev_info->dev;\n\tstruct resource *res;\n\n\tres = platform_get_resource(to_platform_device(dev),\n\t\t\t\t    IORESOURCE_MEM,\n\t\t\t\t    2 + queue->queue_id);\n\tif (!res) {\n\t\tdev_err(dev, \"Failed to get queue %u memory resource\\n\",\n\t\t\tqueue->queue_id);\n\t\treturn -ENOMEM;\n\t}\n\tqueue->regs = ioremap(res->start, resource_size(res));\n\tif (!queue->regs)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void sec_queue_unmap_io(struct sec_queue *queue)\n{\n\t iounmap(queue->regs);\n}\n\nstatic int sec_queue_ar_pkgattr(struct sec_queue *queue, u32 ar_pkg)\n{\n\tvoid __iomem *addr = queue->regs +  SEC_Q_ARUSER_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (ar_pkg)\n\t\tregval |= SEC_Q_ARUSER_CFG_PKG;\n\telse\n\t\tregval &= ~SEC_Q_ARUSER_CFG_PKG;\n\twritel_relaxed(regval, addr);\n\n\treturn 0;\n}\n\nstatic int sec_queue_aw_pkgattr(struct sec_queue *queue, u32 aw_pkg)\n{\n\tvoid __iomem *addr = queue->regs + SEC_Q_AWUSER_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tregval |= SEC_Q_AWUSER_CFG_PKG;\n\twritel_relaxed(regval, addr);\n\n\treturn 0;\n}\n\nstatic int sec_clk_en(struct sec_dev_info *info)\n{\n\tvoid __iomem *base = info->regs[SEC_COMMON];\n\tu32 i = 0;\n\n\twritel_relaxed(0x7, base + SEC_ALGSUB_CLK_EN_REG);\n\tdo {\n\t\tusleep_range(1000, 10000);\n\t\tif ((readl_relaxed(base + SEC_ALGSUB_CLK_ST_REG) & 0x7) == 0x7)\n\t\t\treturn 0;\n\t\ti++;\n\t} while (i < 10);\n\tdev_err(info->dev, \"sec clock enable fail!\\n\");\n\n\treturn -EIO;\n}\n\nstatic int sec_clk_dis(struct sec_dev_info *info)\n{\n\tvoid __iomem *base = info->regs[SEC_COMMON];\n\tu32 i = 0;\n\n\twritel_relaxed(0x7, base + SEC_ALGSUB_CLK_DIS_REG);\n\tdo {\n\t\tusleep_range(1000, 10000);\n\t\tif ((readl_relaxed(base + SEC_ALGSUB_CLK_ST_REG) & 0x7) == 0)\n\t\t\treturn 0;\n\t\ti++;\n\t} while (i < 10);\n\tdev_err(info->dev, \"sec clock disable fail!\\n\");\n\n\treturn -EIO;\n}\n\nstatic int sec_reset_whole_module(struct sec_dev_info *info)\n{\n\tvoid __iomem *base = info->regs[SEC_COMMON];\n\tbool is_reset, b_is_reset;\n\tu32 i = 0;\n\n\twritel_relaxed(1, base + SEC_ALGSUB_RST_REQ_REG);\n\twritel_relaxed(1, base + SEC_ALGSUB_BUILD_RST_REQ_REG);\n\twhile (1) {\n\t\tusleep_range(1000, 10000);\n\t\tis_reset = readl_relaxed(base + SEC_ALGSUB_RST_ST_REG) &\n\t\t\tSEC_ALGSUB_RST_ST_IS_RST;\n\t\tb_is_reset = readl_relaxed(base + SEC_ALGSUB_BUILD_RST_ST_REG) &\n\t\t\tSEC_ALGSUB_BUILD_RST_ST_IS_RST;\n\t\tif (is_reset && b_is_reset)\n\t\t\tbreak;\n\t\ti++;\n\t\tif (i > 10) {\n\t\t\tdev_err(info->dev, \"Reset req failed\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\ti = 0;\n\twritel_relaxed(1, base + SEC_ALGSUB_RST_DREQ_REG);\n\twritel_relaxed(1, base + SEC_ALGSUB_BUILD_RST_DREQ_REG);\n\twhile (1) {\n\t\tusleep_range(1000, 10000);\n\t\tis_reset = readl_relaxed(base + SEC_ALGSUB_RST_ST_REG) &\n\t\t\tSEC_ALGSUB_RST_ST_IS_RST;\n\t\tb_is_reset = readl_relaxed(base + SEC_ALGSUB_BUILD_RST_ST_REG) &\n\t\t\tSEC_ALGSUB_BUILD_RST_ST_IS_RST;\n\t\tif (!is_reset && !b_is_reset)\n\t\t\tbreak;\n\n\t\ti++;\n\t\tif (i > 10) {\n\t\t\tdev_err(info->dev, \"Reset dreq failed\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_bd_endian_little(struct sec_dev_info *info)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_CTRL2_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tregval &= ~(SEC_CTRL2_ENDIAN_BD | SEC_CTRL2_ENDIAN_BD_TYPE);\n\twritel_relaxed(regval, addr);\n}\n\n \nstatic void sec_cache_config(struct sec_dev_info *info)\n{\n\tstruct iommu_domain *domain;\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_CTRL_REG;\n\n\tdomain = iommu_get_domain_for_dev(info->dev);\n\n\t \n\tif (domain && (domain->type & __IOMMU_DOMAIN_PAGING))\n\t\twritel_relaxed(0x44cf9e, addr);\n\telse\n\t\twritel_relaxed(0x4cfd9, addr);\n}\n\nstatic void sec_data_axiwr_otsd_cfg(struct sec_dev_info *info, u32 cfg)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_CTRL2_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tregval &= ~SEC_CTRL2_DATA_AXI_WR_OTSD_CFG_M;\n\tregval |= (cfg << SEC_CTRL2_DATA_AXI_WR_OTSD_CFG_S) &\n\t\tSEC_CTRL2_DATA_AXI_WR_OTSD_CFG_M;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_data_axird_otsd_cfg(struct sec_dev_info *info, u32 cfg)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_CTRL2_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tregval &= ~SEC_CTRL2_DATA_AXI_RD_OTSD_CFG_M;\n\tregval |= (cfg << SEC_CTRL2_DATA_AXI_RD_OTSD_CFG_S) &\n\t\tSEC_CTRL2_DATA_AXI_RD_OTSD_CFG_M;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_clk_gate_en(struct sec_dev_info *info, bool clkgate)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_CTRL2_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (clkgate)\n\t\tregval |= SEC_CTRL2_CLK_GATE_EN;\n\telse\n\t\tregval &= ~SEC_CTRL2_CLK_GATE_EN;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_comm_cnt_cfg(struct sec_dev_info *info, bool clr_ce)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_COMMON_CNT_CLR_CE_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (clr_ce)\n\t\tregval |= SEC_COMMON_CNT_CLR_CE_CLEAR;\n\telse\n\t\tregval &= ~SEC_COMMON_CNT_CLR_CE_CLEAR;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_commsnap_en(struct sec_dev_info *info, bool snap_en)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_COMMON_CNT_CLR_CE_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (snap_en)\n\t\tregval |= SEC_COMMON_CNT_CLR_CE_SNAP_EN;\n\telse\n\t\tregval &= ~SEC_COMMON_CNT_CLR_CE_SNAP_EN;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_ipv6_hashmask(struct sec_dev_info *info, u32 hash_mask[])\n{\n\tvoid __iomem *base = info->regs[SEC_SAA];\n\tint i;\n\n\tfor (i = 0; i < 10; i++)\n\t\twritel_relaxed(hash_mask[0],\n\t\t\t       base + SEC_IPV6_MASK_TABLE_X_REG(i));\n}\n\nstatic int sec_ipv4_hashmask(struct sec_dev_info *info, u32 hash_mask)\n{\n\tif (hash_mask & SEC_HASH_IPV4_MASK) {\n\t\tdev_err(info->dev, \"Sec Ipv4 Hash Mask Input Error!\\n \");\n\t\treturn -EINVAL;\n\t}\n\n\twritel_relaxed(hash_mask,\n\t\t       info->regs[SEC_SAA] + SEC_IPV4_MASK_TABLE_REG);\n\n\treturn 0;\n}\n\nstatic void sec_set_dbg_bd_cfg(struct sec_dev_info *info, u32 cfg)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_DEBUG_BD_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\t \n\tregval &= ~SEC_DEBUG_BD_CFG_WB_NORMAL;\n\n\tif (cfg)\n\t\tregval &= ~SEC_DEBUG_BD_CFG_WB_EN;\n\telse\n\t\tregval |= SEC_DEBUG_BD_CFG_WB_EN;\n\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_saa_getqm_en(struct sec_dev_info *info, u32 saa_indx, u32 en)\n{\n\tvoid __iomem *addr = info->regs[SEC_SAA] + SEC_SAA_BASE +\n\t\tSEC_SAA_CTRL_REG(saa_indx);\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (en)\n\t\tregval |= SEC_SAA_CTRL_GET_QM_EN;\n\telse\n\t\tregval &= ~SEC_SAA_CTRL_GET_QM_EN;\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_saa_int_mask(struct sec_dev_info *info, u32 saa_indx,\n\t\t\t     u32 saa_int_mask)\n{\n\twritel_relaxed(saa_int_mask,\n\t\t       info->regs[SEC_SAA] + SEC_SAA_BASE + SEC_ST_INTMSK1_REG +\n\t\t       saa_indx * SEC_SAA_ADDR_SIZE);\n}\n\nstatic void sec_streamid(struct sec_dev_info *info, int i)\n{\n\t#define SEC_SID 0x600\n\t#define SEC_VMID 0\n\n\twritel_relaxed((SEC_VMID | ((SEC_SID & 0xffff) << 8)),\n\t\t       info->regs[SEC_SAA] + SEC_Q_VMID_CFG_REG(i));\n}\n\nstatic void sec_queue_ar_alloc(struct sec_queue *queue, u32 alloc)\n{\n\tvoid __iomem *addr = queue->regs + SEC_Q_ARUSER_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (alloc == SEC_QUEUE_AR_FROCE_ALLOC) {\n\t\tregval |= SEC_Q_ARUSER_CFG_FA;\n\t\tregval &= ~SEC_Q_ARUSER_CFG_FNA;\n\t} else {\n\t\tregval &= ~SEC_Q_ARUSER_CFG_FA;\n\t\tregval |= SEC_Q_ARUSER_CFG_FNA;\n\t}\n\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_queue_aw_alloc(struct sec_queue *queue, u32 alloc)\n{\n\tvoid __iomem *addr = queue->regs + SEC_Q_AWUSER_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tif (alloc == SEC_QUEUE_AW_FROCE_ALLOC) {\n\t\tregval |= SEC_Q_AWUSER_CFG_FA;\n\t\tregval &= ~SEC_Q_AWUSER_CFG_FNA;\n\t} else {\n\t\tregval &= ~SEC_Q_AWUSER_CFG_FA;\n\t\tregval |= SEC_Q_AWUSER_CFG_FNA;\n\t}\n\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_queue_reorder(struct sec_queue *queue, bool reorder)\n{\n\tvoid __iomem *base = queue->regs;\n\tu32 regval;\n\n\tregval = readl_relaxed(base + SEC_Q_CFG_REG);\n\tif (reorder)\n\t\tregval |= SEC_Q_CFG_REORDER;\n\telse\n\t\tregval &= ~SEC_Q_CFG_REORDER;\n\twritel_relaxed(regval, base + SEC_Q_CFG_REG);\n}\n\nstatic void sec_queue_depth(struct sec_queue *queue, u32 depth)\n{\n\tvoid __iomem *addr = queue->regs + SEC_Q_DEPTH_CFG_REG;\n\tu32 regval;\n\n\tregval = readl_relaxed(addr);\n\tregval &= ~SEC_Q_DEPTH_CFG_DEPTH_M;\n\tregval |= (depth << SEC_Q_DEPTH_CFG_DEPTH_S) & SEC_Q_DEPTH_CFG_DEPTH_M;\n\n\twritel_relaxed(regval, addr);\n}\n\nstatic void sec_queue_cmdbase_addr(struct sec_queue *queue, u64 addr)\n{\n\twritel_relaxed(upper_32_bits(addr), queue->regs + SEC_Q_BASE_HADDR_REG);\n\twritel_relaxed(lower_32_bits(addr), queue->regs + SEC_Q_BASE_LADDR_REG);\n}\n\nstatic void sec_queue_outorder_addr(struct sec_queue *queue, u64 addr)\n{\n\twritel_relaxed(upper_32_bits(addr),\n\t\t       queue->regs + SEC_Q_OUTORDER_BASE_HADDR_REG);\n\twritel_relaxed(lower_32_bits(addr),\n\t\t       queue->regs + SEC_Q_OUTORDER_BASE_LADDR_REG);\n}\n\nstatic void sec_queue_errbase_addr(struct sec_queue *queue, u64 addr)\n{\n\twritel_relaxed(upper_32_bits(addr),\n\t\t       queue->regs + SEC_Q_ERR_BASE_HADDR_REG);\n\twritel_relaxed(lower_32_bits(addr),\n\t\t       queue->regs + SEC_Q_ERR_BASE_LADDR_REG);\n}\n\nstatic void sec_queue_irq_disable(struct sec_queue *queue)\n{\n\twritel_relaxed((u32)~0, queue->regs + SEC_Q_FLOW_INT_MKS_REG);\n}\n\nstatic void sec_queue_irq_enable(struct sec_queue *queue)\n{\n\twritel_relaxed(0, queue->regs + SEC_Q_FLOW_INT_MKS_REG);\n}\n\nstatic void sec_queue_abn_irq_disable(struct sec_queue *queue)\n{\n\twritel_relaxed((u32)~0, queue->regs + SEC_Q_FAIL_INT_MSK_REG);\n}\n\nstatic void sec_queue_stop(struct sec_queue *queue)\n{\n\tdisable_irq(queue->task_irq);\n\tsec_queue_irq_disable(queue);\n\twritel_relaxed(0x0, queue->regs + SEC_QUEUE_ENB_REG);\n}\n\nstatic void sec_queue_start(struct sec_queue *queue)\n{\n\tsec_queue_irq_enable(queue);\n\tenable_irq(queue->task_irq);\n\tqueue->expected = 0;\n\twritel_relaxed(SEC_Q_INIT_AND_STAT_CLEAR, queue->regs + SEC_Q_INIT_REG);\n\twritel_relaxed(0x1, queue->regs + SEC_QUEUE_ENB_REG);\n}\n\nstatic struct sec_queue *sec_alloc_queue(struct sec_dev_info *info)\n{\n\tint i;\n\n\tmutex_lock(&info->dev_lock);\n\n\t \n\tfor (i = 0; i < SEC_Q_NUM; i++)\n\t\tif (!info->queues[i].in_use) {\n\t\t\tinfo->queues[i].in_use = true;\n\t\t\tinfo->queues_in_use++;\n\t\t\tmutex_unlock(&info->dev_lock);\n\n\t\t\treturn &info->queues[i];\n\t\t}\n\tmutex_unlock(&info->dev_lock);\n\n\treturn ERR_PTR(-ENODEV);\n}\n\nstatic int sec_queue_free(struct sec_queue *queue)\n{\n\tstruct sec_dev_info *info = queue->dev_info;\n\n\tif (queue->queue_id >= SEC_Q_NUM) {\n\t\tdev_err(info->dev, \"No queue %u\\n\", queue->queue_id);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!queue->in_use) {\n\t\tdev_err(info->dev, \"Queue %u is idle\\n\", queue->queue_id);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&info->dev_lock);\n\tqueue->in_use = false;\n\tinfo->queues_in_use--;\n\tmutex_unlock(&info->dev_lock);\n\n\treturn 0;\n}\n\nstatic irqreturn_t sec_isr_handle_th(int irq, void *q)\n{\n\tsec_queue_irq_disable(q);\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic irqreturn_t sec_isr_handle(int irq, void *q)\n{\n\tstruct sec_queue *queue = q;\n\tstruct sec_queue_ring_cmd *msg_ring = &queue->ring_cmd;\n\tstruct sec_queue_ring_cq *cq_ring = &queue->ring_cq;\n\tstruct sec_out_bd_info *outorder_msg;\n\tstruct sec_bd_info *msg;\n\tu32 ooo_read, ooo_write;\n\tvoid __iomem *base = queue->regs;\n\tint q_id;\n\n\tooo_read = readl(base + SEC_Q_OUTORDER_RD_PTR_REG);\n\tooo_write = readl(base + SEC_Q_OUTORDER_WR_PTR_REG);\n\toutorder_msg = cq_ring->vaddr + ooo_read;\n\tq_id = outorder_msg->data & SEC_OUT_BD_INFO_Q_ID_M;\n\tmsg = msg_ring->vaddr + q_id;\n\n\twhile ((ooo_write != ooo_read) && msg->w0 & SEC_BD_W0_DONE) {\n\t\t \n\t\tset_bit(q_id, queue->unprocessed);\n\t\tif (q_id == queue->expected)\n\t\t\twhile (test_bit(queue->expected, queue->unprocessed)) {\n\t\t\t\tclear_bit(queue->expected, queue->unprocessed);\n\t\t\t\tmsg = msg_ring->vaddr + queue->expected;\n\t\t\t\tmsg->w0 &= ~SEC_BD_W0_DONE;\n\t\t\t\tmsg_ring->callback(msg,\n\t\t\t\t\t\tqueue->shadow[queue->expected]);\n\t\t\t\tqueue->shadow[queue->expected] = NULL;\n\t\t\t\tqueue->expected = (queue->expected + 1) %\n\t\t\t\t\tSEC_QUEUE_LEN;\n\t\t\t\tatomic_dec(&msg_ring->used);\n\t\t\t}\n\n\t\tooo_read = (ooo_read + 1) % SEC_QUEUE_LEN;\n\t\twritel(ooo_read, base + SEC_Q_OUTORDER_RD_PTR_REG);\n\t\tooo_write = readl(base + SEC_Q_OUTORDER_WR_PTR_REG);\n\t\toutorder_msg = cq_ring->vaddr + ooo_read;\n\t\tq_id = outorder_msg->data & SEC_OUT_BD_INFO_Q_ID_M;\n\t\tmsg = msg_ring->vaddr + q_id;\n\t}\n\n\tsec_queue_irq_enable(queue);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int sec_queue_irq_init(struct sec_queue *queue)\n{\n\tstruct sec_dev_info *info = queue->dev_info;\n\tint irq = queue->task_irq;\n\tint ret;\n\n\tret = request_threaded_irq(irq, sec_isr_handle_th, sec_isr_handle,\n\t\t\t\t   IRQF_TRIGGER_RISING, queue->name, queue);\n\tif (ret) {\n\t\tdev_err(info->dev, \"request irq(%d) failed %d\\n\", irq, ret);\n\t\treturn ret;\n\t}\n\tdisable_irq(irq);\n\n\treturn 0;\n}\n\nstatic int sec_queue_irq_uninit(struct sec_queue *queue)\n{\n\tfree_irq(queue->task_irq, queue);\n\n\treturn 0;\n}\n\nstatic struct sec_dev_info *sec_device_get(void)\n{\n\tstruct sec_dev_info *sec_dev = NULL;\n\tstruct sec_dev_info *this_sec_dev;\n\tint least_busy_n = SEC_Q_NUM + 1;\n\tint i;\n\n\t \n\tfor (i = 0; i < SEC_MAX_DEVICES; i++) {\n\t\tthis_sec_dev = sec_devices[i];\n\t\tif (this_sec_dev &&\n\t\t    this_sec_dev->queues_in_use < least_busy_n) {\n\t\t\tleast_busy_n = this_sec_dev->queues_in_use;\n\t\t\tsec_dev = this_sec_dev;\n\t\t}\n\t}\n\n\treturn sec_dev;\n}\n\nstatic struct sec_queue *sec_queue_alloc_start(struct sec_dev_info *info)\n{\n\tstruct sec_queue *queue;\n\n\tqueue = sec_alloc_queue(info);\n\tif (IS_ERR(queue)) {\n\t\tdev_err(info->dev, \"alloc sec queue failed! %ld\\n\",\n\t\t\tPTR_ERR(queue));\n\t\treturn queue;\n\t}\n\n\tsec_queue_start(queue);\n\n\treturn queue;\n}\n\n \nstruct sec_queue *sec_queue_alloc_start_safe(void)\n{\n\tstruct sec_dev_info *info;\n\tstruct sec_queue *queue = ERR_PTR(-ENODEV);\n\n\tmutex_lock(&sec_id_lock);\n\tinfo = sec_device_get();\n\tif (!info)\n\t\tgoto unlock;\n\n\tqueue = sec_queue_alloc_start(info);\n\nunlock:\n\tmutex_unlock(&sec_id_lock);\n\n\treturn queue;\n}\n\n \nint sec_queue_stop_release(struct sec_queue *queue)\n{\n\tstruct device *dev = queue->dev_info->dev;\n\tint ret;\n\n\tsec_queue_stop(queue);\n\n\tret = sec_queue_free(queue);\n\tif (ret)\n\t\tdev_err(dev, \"Releasing queue failed %d\\n\", ret);\n\n\treturn ret;\n}\n\n \nbool sec_queue_empty(struct sec_queue *queue)\n{\n\tstruct sec_queue_ring_cmd *msg_ring = &queue->ring_cmd;\n\n\treturn !atomic_read(&msg_ring->used);\n}\n\n \nint sec_queue_send(struct sec_queue *queue, struct sec_bd_info *msg, void *ctx)\n{\n\tstruct sec_queue_ring_cmd *msg_ring = &queue->ring_cmd;\n\tvoid __iomem *base = queue->regs;\n\tu32 write, read;\n\n\tmutex_lock(&msg_ring->lock);\n\tread = readl(base + SEC_Q_RD_PTR_REG);\n\twrite = readl(base + SEC_Q_WR_PTR_REG);\n\tif (write == read && atomic_read(&msg_ring->used) == SEC_QUEUE_LEN) {\n\t\tmutex_unlock(&msg_ring->lock);\n\t\treturn -EAGAIN;\n\t}\n\tmemcpy(msg_ring->vaddr + write, msg, sizeof(*msg));\n\tqueue->shadow[write] = ctx;\n\twrite = (write + 1) % SEC_QUEUE_LEN;\n\n\t \n\twmb();\n\twritel(write, base + SEC_Q_WR_PTR_REG);\n\n\tatomic_inc(&msg_ring->used);\n\tmutex_unlock(&msg_ring->lock);\n\n\treturn 0;\n}\n\nbool sec_queue_can_enqueue(struct sec_queue *queue, int num)\n{\n\tstruct sec_queue_ring_cmd *msg_ring = &queue->ring_cmd;\n\n\treturn SEC_QUEUE_LEN - atomic_read(&msg_ring->used) >= num;\n}\n\nstatic void sec_queue_hw_init(struct sec_queue *queue)\n{\n\tsec_queue_ar_alloc(queue, SEC_QUEUE_AR_FROCE_NOALLOC);\n\tsec_queue_aw_alloc(queue, SEC_QUEUE_AW_FROCE_NOALLOC);\n\tsec_queue_ar_pkgattr(queue, 1);\n\tsec_queue_aw_pkgattr(queue, 1);\n\n\t \n\tsec_queue_reorder(queue, true);\n\n\t \n\twritel_relaxed(1, queue->regs + SEC_Q_PROC_NUM_CFG_REG);\n\n\tsec_queue_depth(queue, SEC_QUEUE_LEN - 1);\n\n\tsec_queue_cmdbase_addr(queue, queue->ring_cmd.paddr);\n\n\tsec_queue_outorder_addr(queue, queue->ring_cq.paddr);\n\n\tsec_queue_errbase_addr(queue, queue->ring_db.paddr);\n\n\twritel_relaxed(0x100, queue->regs + SEC_Q_OT_TH_REG);\n\n\tsec_queue_abn_irq_disable(queue);\n\tsec_queue_irq_disable(queue);\n\twritel_relaxed(SEC_Q_INIT_AND_STAT_CLEAR, queue->regs + SEC_Q_INIT_REG);\n}\n\nstatic int sec_hw_init(struct sec_dev_info *info)\n{\n\tstruct iommu_domain *domain;\n\tu32 sec_ipv4_mask = 0;\n\tu32 sec_ipv6_mask[10] = {};\n\tu32 i, ret;\n\n\tdomain = iommu_get_domain_for_dev(info->dev);\n\n\t \n\tif (domain && (domain->type & __IOMMU_DOMAIN_PAGING))\n\t\tinfo->num_saas = 5;\n\n\telse\n\t\tinfo->num_saas = 10;\n\n\twritel_relaxed(GENMASK(info->num_saas - 1, 0),\n\t\t       info->regs[SEC_SAA] + SEC_CLK_EN_REG);\n\n\t \n\tsec_bd_endian_little(info);\n\n\tsec_cache_config(info);\n\n\t \n\tsec_data_axiwr_otsd_cfg(info, 0x7);\n\tsec_data_axird_otsd_cfg(info, 0x7);\n\n\t \n\tsec_clk_gate_en(info, true);\n\n\t \n\tsec_comm_cnt_cfg(info, false);\n\n\t \n\tsec_commsnap_en(info, false);\n\n\twritel_relaxed((u32)~0, info->regs[SEC_SAA] + SEC_FSM_MAX_CNT_REG);\n\n\tret = sec_ipv4_hashmask(info, sec_ipv4_mask);\n\tif (ret) {\n\t\tdev_err(info->dev, \"Failed to set ipv4 hashmask %d\\n\", ret);\n\t\treturn -EIO;\n\t}\n\n\tsec_ipv6_hashmask(info, sec_ipv6_mask);\n\n\t \n\tsec_set_dbg_bd_cfg(info, 0);\n\n\tif (domain && (domain->type & __IOMMU_DOMAIN_PAGING)) {\n\t\tfor (i = 0; i < SEC_Q_NUM; i++) {\n\t\t\tsec_streamid(info, i);\n\t\t\t \n\t\t\twritel_relaxed(0x3f,\n\t\t\t\t       info->regs[SEC_SAA] +\n\t\t\t\t       SEC_Q_WEIGHT_CFG_REG(i));\n\t\t}\n\t}\n\n\tfor (i = 0; i < info->num_saas; i++) {\n\t\tsec_saa_getqm_en(info, i, 1);\n\t\tsec_saa_int_mask(info, i, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic void sec_hw_exit(struct sec_dev_info *info)\n{\n\tint i;\n\n\tfor (i = 0; i < SEC_MAX_SAA_NUM; i++) {\n\t\tsec_saa_int_mask(info, i, (u32)~0);\n\t\tsec_saa_getqm_en(info, i, 0);\n\t}\n}\n\nstatic void sec_queue_base_init(struct sec_dev_info *info,\n\t\t\t\tstruct sec_queue *queue, int queue_id)\n{\n\tqueue->dev_info = info;\n\tqueue->queue_id = queue_id;\n\tsnprintf(queue->name, sizeof(queue->name),\n\t\t \"%s_%d\", dev_name(info->dev), queue->queue_id);\n}\n\nstatic int sec_map_io(struct sec_dev_info *info, struct platform_device *pdev)\n{\n\tstruct resource *res;\n\tint i;\n\n\tfor (i = 0; i < SEC_NUM_ADDR_REGIONS; i++) {\n\t\tres = platform_get_resource(pdev, IORESOURCE_MEM, i);\n\n\t\tif (!res) {\n\t\t\tdev_err(info->dev, \"Memory resource %d not found\\n\", i);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinfo->regs[i] = devm_ioremap(info->dev, res->start,\n\t\t\t\t\t     resource_size(res));\n\t\tif (!info->regs[i]) {\n\t\t\tdev_err(info->dev,\n\t\t\t\t\"Memory resource %d could not be remapped\\n\",\n\t\t\t\ti);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int sec_base_init(struct sec_dev_info *info,\n\t\t\t struct platform_device *pdev)\n{\n\tint ret;\n\n\tret = sec_map_io(info, pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = sec_clk_en(info);\n\tif (ret)\n\t\treturn ret;\n\n\tret = sec_reset_whole_module(info);\n\tif (ret)\n\t\tgoto sec_clk_disable;\n\n\tret = sec_hw_init(info);\n\tif (ret)\n\t\tgoto sec_clk_disable;\n\n\treturn 0;\n\nsec_clk_disable:\n\tsec_clk_dis(info);\n\n\treturn ret;\n}\n\nstatic void sec_base_exit(struct sec_dev_info *info)\n{\n\tsec_hw_exit(info);\n\tsec_clk_dis(info);\n}\n\n#define SEC_Q_CMD_SIZE \\\n\tround_up(SEC_QUEUE_LEN * sizeof(struct sec_bd_info), PAGE_SIZE)\n#define SEC_Q_CQ_SIZE \\\n\tround_up(SEC_QUEUE_LEN * sizeof(struct sec_out_bd_info), PAGE_SIZE)\n#define SEC_Q_DB_SIZE \\\n\tround_up(SEC_QUEUE_LEN * sizeof(struct sec_debug_bd_info), PAGE_SIZE)\n\nstatic int sec_queue_res_cfg(struct sec_queue *queue)\n{\n\tstruct device *dev = queue->dev_info->dev;\n\tstruct sec_queue_ring_cmd *ring_cmd = &queue->ring_cmd;\n\tstruct sec_queue_ring_cq *ring_cq = &queue->ring_cq;\n\tstruct sec_queue_ring_db *ring_db = &queue->ring_db;\n\tint ret;\n\n\tring_cmd->vaddr = dma_alloc_coherent(dev, SEC_Q_CMD_SIZE,\n\t\t\t\t\t     &ring_cmd->paddr, GFP_KERNEL);\n\tif (!ring_cmd->vaddr)\n\t\treturn -ENOMEM;\n\n\tatomic_set(&ring_cmd->used, 0);\n\tmutex_init(&ring_cmd->lock);\n\tring_cmd->callback = sec_alg_callback;\n\n\tring_cq->vaddr = dma_alloc_coherent(dev, SEC_Q_CQ_SIZE,\n\t\t\t\t\t    &ring_cq->paddr, GFP_KERNEL);\n\tif (!ring_cq->vaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_ring_cmd;\n\t}\n\n\tring_db->vaddr = dma_alloc_coherent(dev, SEC_Q_DB_SIZE,\n\t\t\t\t\t    &ring_db->paddr, GFP_KERNEL);\n\tif (!ring_db->vaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_ring_cq;\n\t}\n\tqueue->task_irq = platform_get_irq(to_platform_device(dev),\n\t\t\t\t\t   queue->queue_id * 2 + 1);\n\tif (queue->task_irq < 0) {\n\t\tret = queue->task_irq;\n\t\tgoto err_free_ring_db;\n\t}\n\n\treturn 0;\n\nerr_free_ring_db:\n\tdma_free_coherent(dev, SEC_Q_DB_SIZE, queue->ring_db.vaddr,\n\t\t\t  queue->ring_db.paddr);\nerr_free_ring_cq:\n\tdma_free_coherent(dev, SEC_Q_CQ_SIZE, queue->ring_cq.vaddr,\n\t\t\t  queue->ring_cq.paddr);\nerr_free_ring_cmd:\n\tdma_free_coherent(dev, SEC_Q_CMD_SIZE, queue->ring_cmd.vaddr,\n\t\t\t  queue->ring_cmd.paddr);\n\n\treturn ret;\n}\n\nstatic void sec_queue_free_ring_pages(struct sec_queue *queue)\n{\n\tstruct device *dev = queue->dev_info->dev;\n\n\tdma_free_coherent(dev, SEC_Q_DB_SIZE, queue->ring_db.vaddr,\n\t\t\t  queue->ring_db.paddr);\n\tdma_free_coherent(dev, SEC_Q_CQ_SIZE, queue->ring_cq.vaddr,\n\t\t\t  queue->ring_cq.paddr);\n\tdma_free_coherent(dev, SEC_Q_CMD_SIZE, queue->ring_cmd.vaddr,\n\t\t\t  queue->ring_cmd.paddr);\n}\n\nstatic int sec_queue_config(struct sec_dev_info *info, struct sec_queue *queue,\n\t\t\t    int queue_id)\n{\n\tint ret;\n\n\tsec_queue_base_init(info, queue, queue_id);\n\n\tret = sec_queue_res_cfg(queue);\n\tif (ret)\n\t\treturn ret;\n\n\tret = sec_queue_map_io(queue);\n\tif (ret) {\n\t\tdev_err(info->dev, \"Queue map failed %d\\n\", ret);\n\t\tsec_queue_free_ring_pages(queue);\n\t\treturn ret;\n\t}\n\n\tsec_queue_hw_init(queue);\n\n\treturn 0;\n}\n\nstatic void sec_queue_unconfig(struct sec_dev_info *info,\n\t\t\t       struct sec_queue *queue)\n{\n\tsec_queue_unmap_io(queue);\n\tsec_queue_free_ring_pages(queue);\n}\n\nstatic int sec_id_alloc(struct sec_dev_info *info)\n{\n\tint ret = 0;\n\tint i;\n\n\tmutex_lock(&sec_id_lock);\n\n\tfor (i = 0; i < SEC_MAX_DEVICES; i++)\n\t\tif (!sec_devices[i])\n\t\t\tbreak;\n\tif (i == SEC_MAX_DEVICES) {\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\tinfo->sec_id = i;\n\tsec_devices[info->sec_id] = info;\n\nunlock:\n\tmutex_unlock(&sec_id_lock);\n\n\treturn ret;\n}\n\nstatic void sec_id_free(struct sec_dev_info *info)\n{\n\tmutex_lock(&sec_id_lock);\n\tsec_devices[info->sec_id] = NULL;\n\tmutex_unlock(&sec_id_lock);\n}\n\nstatic int sec_probe(struct platform_device *pdev)\n{\n\tstruct sec_dev_info *info;\n\tstruct device *dev = &pdev->dev;\n\tint i, j;\n\tint ret;\n\n\tret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to set 64 bit dma mask %d\", ret);\n\t\treturn -ENODEV;\n\t}\n\n\tinfo = devm_kzalloc(dev, (sizeof(*info)), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->dev = dev;\n\tmutex_init(&info->dev_lock);\n\n\tinfo->hw_sgl_pool = dmam_pool_create(\"sgl\", dev,\n\t\t\t\t\t     sizeof(struct sec_hw_sgl), 64, 0);\n\tif (!info->hw_sgl_pool) {\n\t\tdev_err(dev, \"Failed to create sec sgl dma pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tret = sec_base_init(info, pdev);\n\tif (ret) {\n\t\tdev_err(dev, \"Base initialization fail! %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < SEC_Q_NUM; i++) {\n\t\tret = sec_queue_config(info, &info->queues[i], i);\n\t\tif (ret)\n\t\t\tgoto queues_unconfig;\n\n\t\tret = sec_queue_irq_init(&info->queues[i]);\n\t\tif (ret) {\n\t\t\tsec_queue_unconfig(info, &info->queues[i]);\n\t\t\tgoto queues_unconfig;\n\t\t}\n\t}\n\n\tret = sec_algs_register();\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to register algorithms with crypto %d\\n\",\n\t\t\tret);\n\t\tgoto queues_unconfig;\n\t}\n\n\tplatform_set_drvdata(pdev, info);\n\n\tret = sec_id_alloc(info);\n\tif (ret)\n\t\tgoto algs_unregister;\n\n\treturn 0;\n\nalgs_unregister:\n\tsec_algs_unregister();\nqueues_unconfig:\n\tfor (j = i - 1; j >= 0; j--) {\n\t\tsec_queue_irq_uninit(&info->queues[j]);\n\t\tsec_queue_unconfig(info, &info->queues[j]);\n\t}\n\tsec_base_exit(info);\n\n\treturn ret;\n}\n\nstatic int sec_remove(struct platform_device *pdev)\n{\n\tstruct sec_dev_info *info = platform_get_drvdata(pdev);\n\tint i;\n\n\t \n\tsec_id_free(info);\n\n\tsec_algs_unregister();\n\n\tfor (i = 0; i < SEC_Q_NUM; i++) {\n\t\tsec_queue_irq_uninit(&info->queues[i]);\n\t\tsec_queue_unconfig(info, &info->queues[i]);\n\t}\n\n\tsec_base_exit(info);\n\n\treturn 0;\n}\n\nstatic const __maybe_unused struct of_device_id sec_match[] = {\n\t{ .compatible = \"hisilicon,hip06-sec\" },\n\t{ .compatible = \"hisilicon,hip07-sec\" },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, sec_match);\n\nstatic const __maybe_unused struct acpi_device_id sec_acpi_match[] = {\n\t{ \"HISI02C1\", 0 },\n\t{ }\n};\nMODULE_DEVICE_TABLE(acpi, sec_acpi_match);\n\nstatic struct platform_driver sec_driver = {\n\t.probe = sec_probe,\n\t.remove = sec_remove,\n\t.driver = {\n\t\t.name = \"hisi_sec_platform_driver\",\n\t\t.of_match_table = sec_match,\n\t\t.acpi_match_table = ACPI_PTR(sec_acpi_match),\n\t},\n};\nmodule_platform_driver(sec_driver);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"HiSilicon Security Accelerators\");\nMODULE_AUTHOR(\"Zaibo Xu <xuzaibo@huawei.com\");\nMODULE_AUTHOR(\"Jonathan Cameron <jonathan.cameron@huawei.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}