{
  "module_name": "n2_core.c",
  "hash_id": "8b74478969d7f0a5d43e1adec7801b8b8e448773ab33053cafd948cf8119fa6b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/n2_core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/platform_device.h>\n#include <linux/cpumask.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/crypto.h>\n#include <crypto/md5.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/aes.h>\n#include <crypto/internal/des.h>\n#include <linux/mutex.h>\n#include <linux/delay.h>\n#include <linux/sched.h>\n\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/algapi.h>\n\n#include <asm/hypervisor.h>\n#include <asm/mdesc.h>\n\n#include \"n2_core.h\"\n\n#define DRV_MODULE_NAME\t\t\"n2_crypto\"\n#define DRV_MODULE_VERSION\t\"0.2\"\n#define DRV_MODULE_RELDATE\t\"July 28, 2011\"\n\nstatic const char version[] =\n\tDRV_MODULE_NAME \".c:v\" DRV_MODULE_VERSION \" (\" DRV_MODULE_RELDATE \")\\n\";\n\nMODULE_AUTHOR(\"David S. Miller (davem@davemloft.net)\");\nMODULE_DESCRIPTION(\"Niagara2 Crypto driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(DRV_MODULE_VERSION);\n\n#define N2_CRA_PRIORITY\t\t200\n\nstatic DEFINE_MUTEX(spu_lock);\n\nstruct spu_queue {\n\tcpumask_t\t\tsharing;\n\tunsigned long\t\tqhandle;\n\n\tspinlock_t\t\tlock;\n\tu8\t\t\tq_type;\n\tvoid\t\t\t*q;\n\tunsigned long\t\thead;\n\tunsigned long\t\ttail;\n\tstruct list_head\tjobs;\n\n\tunsigned long\t\tdevino;\n\n\tchar\t\t\tirq_name[32];\n\tunsigned int\t\tirq;\n\n\tstruct list_head\tlist;\n};\n\nstruct spu_qreg {\n\tstruct spu_queue\t*queue;\n\tunsigned long\t\ttype;\n};\n\nstatic struct spu_queue **cpu_to_cwq;\nstatic struct spu_queue **cpu_to_mau;\n\nstatic unsigned long spu_next_offset(struct spu_queue *q, unsigned long off)\n{\n\tif (q->q_type == HV_NCS_QTYPE_MAU) {\n\t\toff += MAU_ENTRY_SIZE;\n\t\tif (off == (MAU_ENTRY_SIZE * MAU_NUM_ENTRIES))\n\t\t\toff = 0;\n\t} else {\n\t\toff += CWQ_ENTRY_SIZE;\n\t\tif (off == (CWQ_ENTRY_SIZE * CWQ_NUM_ENTRIES))\n\t\t\toff = 0;\n\t}\n\treturn off;\n}\n\nstruct n2_request_common {\n\tstruct list_head\tentry;\n\tunsigned int\t\toffset;\n};\n#define OFFSET_NOT_RUNNING\t(~(unsigned int)0)\n\n \nstatic inline bool job_finished(struct spu_queue *q, unsigned int offset,\n\t\t\t\tunsigned long old_head, unsigned long new_head)\n{\n\tif (old_head <= new_head) {\n\t\tif (offset > old_head && offset <= new_head)\n\t\t\treturn true;\n\t} else {\n\t\tif (offset > old_head || offset <= new_head)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic irqreturn_t cwq_intr(int irq, void *dev_id)\n{\n\tunsigned long off, new_head, hv_ret;\n\tstruct spu_queue *q = dev_id;\n\n\tpr_err(\"CPU[%d]: Got CWQ interrupt for qhdl[%lx]\\n\",\n\t       smp_processor_id(), q->qhandle);\n\n\tspin_lock(&q->lock);\n\n\thv_ret = sun4v_ncs_gethead(q->qhandle, &new_head);\n\n\tpr_err(\"CPU[%d]: CWQ gethead[%lx] hv_ret[%lu]\\n\",\n\t       smp_processor_id(), new_head, hv_ret);\n\n\tfor (off = q->head; off != new_head; off = spu_next_offset(q, off)) {\n\t\t \n\t}\n\n\thv_ret = sun4v_ncs_sethead_marker(q->qhandle, new_head);\n\tif (hv_ret == HV_EOK)\n\t\tq->head = new_head;\n\n\tspin_unlock(&q->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t mau_intr(int irq, void *dev_id)\n{\n\tstruct spu_queue *q = dev_id;\n\tunsigned long head, hv_ret;\n\n\tspin_lock(&q->lock);\n\n\tpr_err(\"CPU[%d]: Got MAU interrupt for qhdl[%lx]\\n\",\n\t       smp_processor_id(), q->qhandle);\n\n\thv_ret = sun4v_ncs_gethead(q->qhandle, &head);\n\n\tpr_err(\"CPU[%d]: MAU gethead[%lx] hv_ret[%lu]\\n\",\n\t       smp_processor_id(), head, hv_ret);\n\n\tsun4v_ncs_sethead_marker(q->qhandle, head);\n\n\tspin_unlock(&q->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void *spu_queue_next(struct spu_queue *q, void *cur)\n{\n\treturn q->q + spu_next_offset(q, cur - q->q);\n}\n\nstatic int spu_queue_num_free(struct spu_queue *q)\n{\n\tunsigned long head = q->head;\n\tunsigned long tail = q->tail;\n\tunsigned long end = (CWQ_ENTRY_SIZE * CWQ_NUM_ENTRIES);\n\tunsigned long diff;\n\n\tif (head > tail)\n\t\tdiff = head - tail;\n\telse\n\t\tdiff = (end - tail) + head;\n\n\treturn (diff / CWQ_ENTRY_SIZE) - 1;\n}\n\nstatic void *spu_queue_alloc(struct spu_queue *q, int num_entries)\n{\n\tint avail = spu_queue_num_free(q);\n\n\tif (avail >= num_entries)\n\t\treturn q->q + q->tail;\n\n\treturn NULL;\n}\n\nstatic unsigned long spu_queue_submit(struct spu_queue *q, void *last)\n{\n\tunsigned long hv_ret, new_tail;\n\n\tnew_tail = spu_next_offset(q, last - q->q);\n\n\thv_ret = sun4v_ncs_settail(q->qhandle, new_tail);\n\tif (hv_ret == HV_EOK)\n\t\tq->tail = new_tail;\n\treturn hv_ret;\n}\n\nstatic u64 control_word_base(unsigned int len, unsigned int hmac_key_len,\n\t\t\t     int enc_type, int auth_type,\n\t\t\t     unsigned int hash_len,\n\t\t\t     bool sfas, bool sob, bool eob, bool encrypt,\n\t\t\t     int opcode)\n{\n\tu64 word = (len - 1) & CONTROL_LEN;\n\n\tword |= ((u64) opcode << CONTROL_OPCODE_SHIFT);\n\tword |= ((u64) enc_type << CONTROL_ENC_TYPE_SHIFT);\n\tword |= ((u64) auth_type << CONTROL_AUTH_TYPE_SHIFT);\n\tif (sfas)\n\t\tword |= CONTROL_STORE_FINAL_AUTH_STATE;\n\tif (sob)\n\t\tword |= CONTROL_START_OF_BLOCK;\n\tif (eob)\n\t\tword |= CONTROL_END_OF_BLOCK;\n\tif (encrypt)\n\t\tword |= CONTROL_ENCRYPT;\n\tif (hmac_key_len)\n\t\tword |= ((u64) (hmac_key_len - 1)) << CONTROL_HMAC_KEY_LEN_SHIFT;\n\tif (hash_len)\n\t\tword |= ((u64) (hash_len - 1)) << CONTROL_HASH_LEN_SHIFT;\n\n\treturn word;\n}\n\n#if 0\nstatic inline bool n2_should_run_async(struct spu_queue *qp, int this_len)\n{\n\tif (this_len >= 64 ||\n\t    qp->head != qp->tail)\n\t\treturn true;\n\treturn false;\n}\n#endif\n\nstruct n2_ahash_alg {\n\tstruct list_head\tentry;\n\tconst u8\t\t*hash_zero;\n\tconst u8\t\t*hash_init;\n\tu8\t\t\thw_op_hashsz;\n\tu8\t\t\tdigest_size;\n\tu8\t\t\tauth_type;\n\tu8\t\t\thmac_type;\n\tstruct ahash_alg\talg;\n};\n\nstatic inline struct n2_ahash_alg *n2_ahash_alg(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\tstruct ahash_alg *ahash_alg;\n\n\tahash_alg = container_of(alg, struct ahash_alg, halg.base);\n\n\treturn container_of(ahash_alg, struct n2_ahash_alg, alg);\n}\n\nstruct n2_hmac_alg {\n\tconst char\t\t*child_alg;\n\tstruct n2_ahash_alg\tderived;\n};\n\nstatic inline struct n2_hmac_alg *n2_hmac_alg(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\tstruct ahash_alg *ahash_alg;\n\n\tahash_alg = container_of(alg, struct ahash_alg, halg.base);\n\n\treturn container_of(ahash_alg, struct n2_hmac_alg, derived.alg);\n}\n\nstruct n2_hash_ctx {\n\tstruct crypto_ahash\t\t*fallback_tfm;\n};\n\n#define N2_HASH_KEY_MAX\t\t\t32  \n\nstruct n2_hmac_ctx {\n\tstruct n2_hash_ctx\t\tbase;\n\n\tstruct crypto_shash\t\t*child_shash;\n\n\tint\t\t\t\thash_key_len;\n\tunsigned char\t\t\thash_key[N2_HASH_KEY_MAX];\n};\n\nstruct n2_hash_req_ctx {\n\tunion {\n\t\tstruct md5_state\tmd5;\n\t\tstruct sha1_state\tsha1;\n\t\tstruct sha256_state\tsha256;\n\t} u;\n\n\tstruct ahash_request\t\tfallback_req;\n};\n\nstatic int n2_hash_async_init(struct ahash_request *req)\n{\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_init(&rctx->fallback_req);\n}\n\nstatic int n2_hash_async_update(struct ahash_request *req)\n{\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\n\treturn crypto_ahash_update(&rctx->fallback_req);\n}\n\nstatic int n2_hash_async_final(struct ahash_request *req)\n{\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_final(&rctx->fallback_req);\n}\n\nstatic int n2_hash_async_finup(struct ahash_request *req)\n{\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\trctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_finup(&rctx->fallback_req);\n}\n\nstatic int n2_hash_async_noimport(struct ahash_request *req, const void *in)\n{\n\treturn -ENOSYS;\n}\n\nstatic int n2_hash_async_noexport(struct ahash_request *req, void *out)\n{\n\treturn -ENOSYS;\n}\n\nstatic int n2_hash_cra_init(struct crypto_tfm *tfm)\n{\n\tconst char *fallback_driver_name = crypto_tfm_alg_name(tfm);\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(ahash);\n\tstruct crypto_ahash *fallback_tfm;\n\tint err;\n\n\tfallback_tfm = crypto_alloc_ahash(fallback_driver_name, 0,\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(fallback_tfm)) {\n\t\tpr_warn(\"Fallback driver '%s' could not be loaded!\\n\",\n\t\t\tfallback_driver_name);\n\t\terr = PTR_ERR(fallback_tfm);\n\t\tgoto out;\n\t}\n\n\tcrypto_ahash_set_reqsize(ahash, (sizeof(struct n2_hash_req_ctx) +\n\t\t\t\t\t crypto_ahash_reqsize(fallback_tfm)));\n\n\tctx->fallback_tfm = fallback_tfm;\n\treturn 0;\n\nout:\n\treturn err;\n}\n\nstatic void n2_hash_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(ahash);\n\n\tcrypto_free_ahash(ctx->fallback_tfm);\n}\n\nstatic int n2_hmac_cra_init(struct crypto_tfm *tfm)\n{\n\tconst char *fallback_driver_name = crypto_tfm_alg_name(tfm);\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(ahash);\n\tstruct n2_hmac_alg *n2alg = n2_hmac_alg(tfm);\n\tstruct crypto_ahash *fallback_tfm;\n\tstruct crypto_shash *child_shash;\n\tint err;\n\n\tfallback_tfm = crypto_alloc_ahash(fallback_driver_name, 0,\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(fallback_tfm)) {\n\t\tpr_warn(\"Fallback driver '%s' could not be loaded!\\n\",\n\t\t\tfallback_driver_name);\n\t\terr = PTR_ERR(fallback_tfm);\n\t\tgoto out;\n\t}\n\n\tchild_shash = crypto_alloc_shash(n2alg->child_alg, 0, 0);\n\tif (IS_ERR(child_shash)) {\n\t\tpr_warn(\"Child shash '%s' could not be loaded!\\n\",\n\t\t\tn2alg->child_alg);\n\t\terr = PTR_ERR(child_shash);\n\t\tgoto out_free_fallback;\n\t}\n\n\tcrypto_ahash_set_reqsize(ahash, (sizeof(struct n2_hash_req_ctx) +\n\t\t\t\t\t crypto_ahash_reqsize(fallback_tfm)));\n\n\tctx->child_shash = child_shash;\n\tctx->base.fallback_tfm = fallback_tfm;\n\treturn 0;\n\nout_free_fallback:\n\tcrypto_free_ahash(fallback_tfm);\n\nout:\n\treturn err;\n}\n\nstatic void n2_hmac_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(ahash);\n\n\tcrypto_free_ahash(ctx->base.fallback_tfm);\n\tcrypto_free_shash(ctx->child_shash);\n}\n\nstatic int n2_hmac_async_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\tunsigned int keylen)\n{\n\tstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct crypto_shash *child_shash = ctx->child_shash;\n\tstruct crypto_ahash *fallback_tfm;\n\tint err, bs, ds;\n\n\tfallback_tfm = ctx->base.fallback_tfm;\n\terr = crypto_ahash_setkey(fallback_tfm, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tbs = crypto_shash_blocksize(child_shash);\n\tds = crypto_shash_digestsize(child_shash);\n\tBUG_ON(ds > N2_HASH_KEY_MAX);\n\tif (keylen > bs) {\n\t\terr = crypto_shash_tfm_digest(child_shash, key, keylen,\n\t\t\t\t\t      ctx->hash_key);\n\t\tif (err)\n\t\t\treturn err;\n\t\tkeylen = ds;\n\t} else if (keylen <= N2_HASH_KEY_MAX)\n\t\tmemcpy(ctx->hash_key, key, keylen);\n\n\tctx->hash_key_len = keylen;\n\n\treturn err;\n}\n\nstatic unsigned long wait_for_tail(struct spu_queue *qp)\n{\n\tunsigned long head, hv_ret;\n\n\tdo {\n\t\thv_ret = sun4v_ncs_gethead(qp->qhandle, &head);\n\t\tif (hv_ret != HV_EOK) {\n\t\t\tpr_err(\"Hypervisor error on gethead\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tif (head == qp->tail) {\n\t\t\tqp->head = head;\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\treturn hv_ret;\n}\n\nstatic unsigned long submit_and_wait_for_tail(struct spu_queue *qp,\n\t\t\t\t\t      struct cwq_initial_entry *ent)\n{\n\tunsigned long hv_ret = spu_queue_submit(qp, ent);\n\n\tif (hv_ret == HV_EOK)\n\t\thv_ret = wait_for_tail(qp);\n\n\treturn hv_ret;\n}\n\nstatic int n2_do_async_digest(struct ahash_request *req,\n\t\t\t      unsigned int auth_type, unsigned int digest_size,\n\t\t\t      unsigned int result_size, void *hash_loc,\n\t\t\t      unsigned long auth_key, unsigned int auth_key_len)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cwq_initial_entry *ent;\n\tstruct crypto_hash_walk walk;\n\tstruct spu_queue *qp;\n\tunsigned long flags;\n\tint err = -ENODEV;\n\tint nbytes, cpu;\n\n\t \n\tif (unlikely(req->nbytes > (1 << 16))) {\n\t\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\t\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\t\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\t\trctx->fallback_req.base.flags =\n\t\t\treq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\t\trctx->fallback_req.nbytes = req->nbytes;\n\t\trctx->fallback_req.src = req->src;\n\t\trctx->fallback_req.result = req->result;\n\n\t\treturn crypto_ahash_digest(&rctx->fallback_req);\n\t}\n\n\tnbytes = crypto_hash_walk_first(req, &walk);\n\n\tcpu = get_cpu();\n\tqp = cpu_to_cwq[cpu];\n\tif (!qp)\n\t\tgoto out;\n\n\tspin_lock_irqsave(&qp->lock, flags);\n\n\t \n\tent = qp->q + qp->tail;\n\n\tent->control = control_word_base(nbytes, auth_key_len, 0,\n\t\t\t\t\t auth_type, digest_size,\n\t\t\t\t\t false, true, false, false,\n\t\t\t\t\t OPCODE_INPLACE_BIT |\n\t\t\t\t\t OPCODE_AUTH_MAC);\n\tent->src_addr = __pa(walk.data);\n\tent->auth_key_addr = auth_key;\n\tent->auth_iv_addr = __pa(hash_loc);\n\tent->final_auth_state_addr = 0UL;\n\tent->enc_key_addr = 0UL;\n\tent->enc_iv_addr = 0UL;\n\tent->dest_addr = __pa(hash_loc);\n\n\tnbytes = crypto_hash_walk_done(&walk, 0);\n\twhile (nbytes > 0) {\n\t\tent = spu_queue_next(qp, ent);\n\n\t\tent->control = (nbytes - 1);\n\t\tent->src_addr = __pa(walk.data);\n\t\tent->auth_key_addr = 0UL;\n\t\tent->auth_iv_addr = 0UL;\n\t\tent->final_auth_state_addr = 0UL;\n\t\tent->enc_key_addr = 0UL;\n\t\tent->enc_iv_addr = 0UL;\n\t\tent->dest_addr = 0UL;\n\n\t\tnbytes = crypto_hash_walk_done(&walk, 0);\n\t}\n\tent->control |= CONTROL_END_OF_BLOCK;\n\n\tif (submit_and_wait_for_tail(qp, ent) != HV_EOK)\n\t\terr = -EINVAL;\n\telse\n\t\terr = 0;\n\n\tspin_unlock_irqrestore(&qp->lock, flags);\n\n\tif (!err)\n\t\tmemcpy(req->result, hash_loc, result_size);\nout:\n\tput_cpu();\n\n\treturn err;\n}\n\nstatic int n2_hash_async_digest(struct ahash_request *req)\n{\n\tstruct n2_ahash_alg *n2alg = n2_ahash_alg(req->base.tfm);\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tint ds;\n\n\tds = n2alg->digest_size;\n\tif (unlikely(req->nbytes == 0)) {\n\t\tmemcpy(req->result, n2alg->hash_zero, ds);\n\t\treturn 0;\n\t}\n\tmemcpy(&rctx->u, n2alg->hash_init, n2alg->hw_op_hashsz);\n\n\treturn n2_do_async_digest(req, n2alg->auth_type,\n\t\t\t\t  n2alg->hw_op_hashsz, ds,\n\t\t\t\t  &rctx->u, 0UL, 0);\n}\n\nstatic int n2_hmac_async_digest(struct ahash_request *req)\n{\n\tstruct n2_hmac_alg *n2alg = n2_hmac_alg(req->base.tfm);\n\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(tfm);\n\tint ds;\n\n\tds = n2alg->derived.digest_size;\n\tif (unlikely(req->nbytes == 0) ||\n\t    unlikely(ctx->hash_key_len > N2_HASH_KEY_MAX)) {\n\t\tstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\n\t\tstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\t\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\t\trctx->fallback_req.base.flags =\n\t\t\treq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\t\trctx->fallback_req.nbytes = req->nbytes;\n\t\trctx->fallback_req.src = req->src;\n\t\trctx->fallback_req.result = req->result;\n\n\t\treturn crypto_ahash_digest(&rctx->fallback_req);\n\t}\n\tmemcpy(&rctx->u, n2alg->derived.hash_init,\n\t       n2alg->derived.hw_op_hashsz);\n\n\treturn n2_do_async_digest(req, n2alg->derived.hmac_type,\n\t\t\t\t  n2alg->derived.hw_op_hashsz, ds,\n\t\t\t\t  &rctx->u,\n\t\t\t\t  __pa(&ctx->hash_key),\n\t\t\t\t  ctx->hash_key_len);\n}\n\nstruct n2_skcipher_context {\n\tint\t\t\tkey_len;\n\tint\t\t\tenc_type;\n\tunion {\n\t\tu8\t\taes[AES_MAX_KEY_SIZE];\n\t\tu8\t\tdes[DES_KEY_SIZE];\n\t\tu8\t\tdes3[3 * DES_KEY_SIZE];\n\t} key;\n};\n\n#define N2_CHUNK_ARR_LEN\t16\n\nstruct n2_crypto_chunk {\n\tstruct list_head\tentry;\n\tunsigned long\t\tiv_paddr : 44;\n\tunsigned long\t\tarr_len : 20;\n\tunsigned long\t\tdest_paddr;\n\tunsigned long\t\tdest_final;\n\tstruct {\n\t\tunsigned long\tsrc_paddr : 44;\n\t\tunsigned long\tsrc_len : 20;\n\t} arr[N2_CHUNK_ARR_LEN];\n};\n\nstruct n2_request_context {\n\tstruct skcipher_walk\twalk;\n\tstruct list_head\tchunk_list;\n\tstruct n2_crypto_chunk\tchunk;\n\tu8\t\t\ttemp_iv[16];\n};\n\n \n\nstruct n2_skcipher_alg {\n\tstruct list_head\tentry;\n\tu8\t\t\tenc_type;\n\tstruct skcipher_alg\tskcipher;\n};\n\nstatic inline struct n2_skcipher_alg *n2_skcipher_alg(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\n\treturn container_of(alg, struct n2_skcipher_alg, skcipher);\n}\n\nstruct n2_skcipher_request_context {\n\tstruct skcipher_walk\twalk;\n};\n\nstatic int n2_aes_setkey(struct crypto_skcipher *skcipher, const u8 *key,\n\t\t\t unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct n2_skcipher_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct n2_skcipher_alg *n2alg = n2_skcipher_alg(skcipher);\n\n\tctx->enc_type = (n2alg->enc_type & ENC_TYPE_CHAINING_MASK);\n\n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128:\n\t\tctx->enc_type |= ENC_TYPE_ALG_AES128;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tctx->enc_type |= ENC_TYPE_ALG_AES192;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tctx->enc_type |= ENC_TYPE_ALG_AES256;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tctx->key_len = keylen;\n\tmemcpy(ctx->key.aes, key, keylen);\n\treturn 0;\n}\n\nstatic int n2_des_setkey(struct crypto_skcipher *skcipher, const u8 *key,\n\t\t\t unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct n2_skcipher_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct n2_skcipher_alg *n2alg = n2_skcipher_alg(skcipher);\n\tint err;\n\n\terr = verify_skcipher_des_key(skcipher, key);\n\tif (err)\n\t\treturn err;\n\n\tctx->enc_type = n2alg->enc_type;\n\n\tctx->key_len = keylen;\n\tmemcpy(ctx->key.des, key, keylen);\n\treturn 0;\n}\n\nstatic int n2_3des_setkey(struct crypto_skcipher *skcipher, const u8 *key,\n\t\t\t  unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct n2_skcipher_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct n2_skcipher_alg *n2alg = n2_skcipher_alg(skcipher);\n\tint err;\n\n\terr = verify_skcipher_des3_key(skcipher, key);\n\tif (err)\n\t\treturn err;\n\n\tctx->enc_type = n2alg->enc_type;\n\n\tctx->key_len = keylen;\n\tmemcpy(ctx->key.des3, key, keylen);\n\treturn 0;\n}\n\nstatic inline int skcipher_descriptor_len(int nbytes, unsigned int block_size)\n{\n\tint this_len = nbytes;\n\n\tthis_len -= (nbytes & (block_size - 1));\n\treturn this_len > (1 << 16) ? (1 << 16) : this_len;\n}\n\nstatic int __n2_crypt_chunk(struct crypto_skcipher *skcipher,\n\t\t\t    struct n2_crypto_chunk *cp,\n\t\t\t    struct spu_queue *qp, bool encrypt)\n{\n\tstruct n2_skcipher_context *ctx = crypto_skcipher_ctx(skcipher);\n\tstruct cwq_initial_entry *ent;\n\tbool in_place;\n\tint i;\n\n\tent = spu_queue_alloc(qp, cp->arr_len);\n\tif (!ent) {\n\t\tpr_info(\"queue_alloc() of %d fails\\n\",\n\t\t\tcp->arr_len);\n\t\treturn -EBUSY;\n\t}\n\n\tin_place = (cp->dest_paddr == cp->arr[0].src_paddr);\n\n\tent->control = control_word_base(cp->arr[0].src_len,\n\t\t\t\t\t 0, ctx->enc_type, 0, 0,\n\t\t\t\t\t false, true, false, encrypt,\n\t\t\t\t\t OPCODE_ENCRYPT |\n\t\t\t\t\t (in_place ? OPCODE_INPLACE_BIT : 0));\n\tent->src_addr = cp->arr[0].src_paddr;\n\tent->auth_key_addr = 0UL;\n\tent->auth_iv_addr = 0UL;\n\tent->final_auth_state_addr = 0UL;\n\tent->enc_key_addr = __pa(&ctx->key);\n\tent->enc_iv_addr = cp->iv_paddr;\n\tent->dest_addr = (in_place ? 0UL : cp->dest_paddr);\n\n\tfor (i = 1; i < cp->arr_len; i++) {\n\t\tent = spu_queue_next(qp, ent);\n\n\t\tent->control = cp->arr[i].src_len - 1;\n\t\tent->src_addr = cp->arr[i].src_paddr;\n\t\tent->auth_key_addr = 0UL;\n\t\tent->auth_iv_addr = 0UL;\n\t\tent->final_auth_state_addr = 0UL;\n\t\tent->enc_key_addr = 0UL;\n\t\tent->enc_iv_addr = 0UL;\n\t\tent->dest_addr = 0UL;\n\t}\n\tent->control |= CONTROL_END_OF_BLOCK;\n\n\treturn (spu_queue_submit(qp, ent) != HV_EOK) ? -EINVAL : 0;\n}\n\nstatic int n2_compute_chunks(struct skcipher_request *req)\n{\n\tstruct n2_request_context *rctx = skcipher_request_ctx(req);\n\tstruct skcipher_walk *walk = &rctx->walk;\n\tstruct n2_crypto_chunk *chunk;\n\tunsigned long dest_prev;\n\tunsigned int tot_len;\n\tbool prev_in_place;\n\tint err, nbytes;\n\n\terr = skcipher_walk_async(walk, req);\n\tif (err)\n\t\treturn err;\n\n\tINIT_LIST_HEAD(&rctx->chunk_list);\n\n\tchunk = &rctx->chunk;\n\tINIT_LIST_HEAD(&chunk->entry);\n\n\tchunk->iv_paddr = 0UL;\n\tchunk->arr_len = 0;\n\tchunk->dest_paddr = 0UL;\n\n\tprev_in_place = false;\n\tdest_prev = ~0UL;\n\ttot_len = 0;\n\n\twhile ((nbytes = walk->nbytes) != 0) {\n\t\tunsigned long dest_paddr, src_paddr;\n\t\tbool in_place;\n\t\tint this_len;\n\n\t\tsrc_paddr = (page_to_phys(walk->src.phys.page) +\n\t\t\t     walk->src.phys.offset);\n\t\tdest_paddr = (page_to_phys(walk->dst.phys.page) +\n\t\t\t      walk->dst.phys.offset);\n\t\tin_place = (src_paddr == dest_paddr);\n\t\tthis_len = skcipher_descriptor_len(nbytes, walk->blocksize);\n\n\t\tif (chunk->arr_len != 0) {\n\t\t\tif (in_place != prev_in_place ||\n\t\t\t    (!prev_in_place &&\n\t\t\t     dest_paddr != dest_prev) ||\n\t\t\t    chunk->arr_len == N2_CHUNK_ARR_LEN ||\n\t\t\t    tot_len + this_len > (1 << 16)) {\n\t\t\t\tchunk->dest_final = dest_prev;\n\t\t\t\tlist_add_tail(&chunk->entry,\n\t\t\t\t\t      &rctx->chunk_list);\n\t\t\t\tchunk = kzalloc(sizeof(*chunk), GFP_ATOMIC);\n\t\t\t\tif (!chunk) {\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tINIT_LIST_HEAD(&chunk->entry);\n\t\t\t}\n\t\t}\n\t\tif (chunk->arr_len == 0) {\n\t\t\tchunk->dest_paddr = dest_paddr;\n\t\t\ttot_len = 0;\n\t\t}\n\t\tchunk->arr[chunk->arr_len].src_paddr = src_paddr;\n\t\tchunk->arr[chunk->arr_len].src_len = this_len;\n\t\tchunk->arr_len++;\n\n\t\tdest_prev = dest_paddr + this_len;\n\t\tprev_in_place = in_place;\n\t\ttot_len += this_len;\n\n\t\terr = skcipher_walk_done(walk, nbytes - this_len);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tif (!err && chunk->arr_len != 0) {\n\t\tchunk->dest_final = dest_prev;\n\t\tlist_add_tail(&chunk->entry, &rctx->chunk_list);\n\t}\n\n\treturn err;\n}\n\nstatic void n2_chunk_complete(struct skcipher_request *req, void *final_iv)\n{\n\tstruct n2_request_context *rctx = skcipher_request_ctx(req);\n\tstruct n2_crypto_chunk *c, *tmp;\n\n\tif (final_iv)\n\t\tmemcpy(rctx->walk.iv, final_iv, rctx->walk.blocksize);\n\n\tlist_for_each_entry_safe(c, tmp, &rctx->chunk_list, entry) {\n\t\tlist_del(&c->entry);\n\t\tif (unlikely(c != &rctx->chunk))\n\t\t\tkfree(c);\n\t}\n\n}\n\nstatic int n2_do_ecb(struct skcipher_request *req, bool encrypt)\n{\n\tstruct n2_request_context *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tint err = n2_compute_chunks(req);\n\tstruct n2_crypto_chunk *c, *tmp;\n\tunsigned long flags, hv_ret;\n\tstruct spu_queue *qp;\n\n\tif (err)\n\t\treturn err;\n\n\tqp = cpu_to_cwq[get_cpu()];\n\terr = -ENODEV;\n\tif (!qp)\n\t\tgoto out;\n\n\tspin_lock_irqsave(&qp->lock, flags);\n\n\tlist_for_each_entry_safe(c, tmp, &rctx->chunk_list, entry) {\n\t\terr = __n2_crypt_chunk(tfm, c, qp, encrypt);\n\t\tif (err)\n\t\t\tbreak;\n\t\tlist_del(&c->entry);\n\t\tif (unlikely(c != &rctx->chunk))\n\t\t\tkfree(c);\n\t}\n\tif (!err) {\n\t\thv_ret = wait_for_tail(qp);\n\t\tif (hv_ret != HV_EOK)\n\t\t\terr = -EINVAL;\n\t}\n\n\tspin_unlock_irqrestore(&qp->lock, flags);\n\nout:\n\tput_cpu();\n\n\tn2_chunk_complete(req, NULL);\n\treturn err;\n}\n\nstatic int n2_encrypt_ecb(struct skcipher_request *req)\n{\n\treturn n2_do_ecb(req, true);\n}\n\nstatic int n2_decrypt_ecb(struct skcipher_request *req)\n{\n\treturn n2_do_ecb(req, false);\n}\n\nstatic int n2_do_chaining(struct skcipher_request *req, bool encrypt)\n{\n\tstruct n2_request_context *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tunsigned long flags, hv_ret, iv_paddr;\n\tint err = n2_compute_chunks(req);\n\tstruct n2_crypto_chunk *c, *tmp;\n\tstruct spu_queue *qp;\n\tvoid *final_iv_addr;\n\n\tfinal_iv_addr = NULL;\n\n\tif (err)\n\t\treturn err;\n\n\tqp = cpu_to_cwq[get_cpu()];\n\terr = -ENODEV;\n\tif (!qp)\n\t\tgoto out;\n\n\tspin_lock_irqsave(&qp->lock, flags);\n\n\tif (encrypt) {\n\t\tiv_paddr = __pa(rctx->walk.iv);\n\t\tlist_for_each_entry_safe(c, tmp, &rctx->chunk_list,\n\t\t\t\t\t entry) {\n\t\t\tc->iv_paddr = iv_paddr;\n\t\t\terr = __n2_crypt_chunk(tfm, c, qp, true);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tiv_paddr = c->dest_final - rctx->walk.blocksize;\n\t\t\tlist_del(&c->entry);\n\t\t\tif (unlikely(c != &rctx->chunk))\n\t\t\t\tkfree(c);\n\t\t}\n\t\tfinal_iv_addr = __va(iv_paddr);\n\t} else {\n\t\tlist_for_each_entry_safe_reverse(c, tmp, &rctx->chunk_list,\n\t\t\t\t\t\t entry) {\n\t\t\tif (c == &rctx->chunk) {\n\t\t\t\tiv_paddr = __pa(rctx->walk.iv);\n\t\t\t} else {\n\t\t\t\tiv_paddr = (tmp->arr[tmp->arr_len-1].src_paddr +\n\t\t\t\t\t    tmp->arr[tmp->arr_len-1].src_len -\n\t\t\t\t\t    rctx->walk.blocksize);\n\t\t\t}\n\t\t\tif (!final_iv_addr) {\n\t\t\t\tunsigned long pa;\n\n\t\t\t\tpa = (c->arr[c->arr_len-1].src_paddr +\n\t\t\t\t      c->arr[c->arr_len-1].src_len -\n\t\t\t\t      rctx->walk.blocksize);\n\t\t\t\tfinal_iv_addr = rctx->temp_iv;\n\t\t\t\tmemcpy(rctx->temp_iv, __va(pa),\n\t\t\t\t       rctx->walk.blocksize);\n\t\t\t}\n\t\t\tc->iv_paddr = iv_paddr;\n\t\t\terr = __n2_crypt_chunk(tfm, c, qp, false);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tlist_del(&c->entry);\n\t\t\tif (unlikely(c != &rctx->chunk))\n\t\t\t\tkfree(c);\n\t\t}\n\t}\n\tif (!err) {\n\t\thv_ret = wait_for_tail(qp);\n\t\tif (hv_ret != HV_EOK)\n\t\t\terr = -EINVAL;\n\t}\n\n\tspin_unlock_irqrestore(&qp->lock, flags);\n\nout:\n\tput_cpu();\n\n\tn2_chunk_complete(req, err ? NULL : final_iv_addr);\n\treturn err;\n}\n\nstatic int n2_encrypt_chaining(struct skcipher_request *req)\n{\n\treturn n2_do_chaining(req, true);\n}\n\nstatic int n2_decrypt_chaining(struct skcipher_request *req)\n{\n\treturn n2_do_chaining(req, false);\n}\n\nstruct n2_skcipher_tmpl {\n\tconst char\t\t*name;\n\tconst char\t\t*drv_name;\n\tu8\t\t\tblock_size;\n\tu8\t\t\tenc_type;\n\tstruct skcipher_alg\tskcipher;\n};\n\nstatic const struct n2_skcipher_tmpl skcipher_tmpls[] = {\n\t \n\t{\t.name\t\t= \"ecb(des)\",\n\t\t.drv_name\t= \"ecb-des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_DES |\n\t\t\t\t   ENC_TYPE_CHAINING_ECB),\n\t\t.skcipher\t= {\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_ecb,\n\t\t\t.decrypt\t= n2_decrypt_ecb,\n\t\t},\n\t},\n\t{\t.name\t\t= \"cbc(des)\",\n\t\t.drv_name\t= \"cbc-des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_DES |\n\t\t\t\t   ENC_TYPE_CHAINING_CBC),\n\t\t.skcipher\t= {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_decrypt_chaining,\n\t\t},\n\t},\n\t{\t.name\t\t= \"cfb(des)\",\n\t\t.drv_name\t= \"cfb-des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_DES |\n\t\t\t\t   ENC_TYPE_CHAINING_CFB),\n\t\t.skcipher\t= {\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_decrypt_chaining,\n\t\t},\n\t},\n\n\t \n\t{\t.name\t\t= \"ecb(des3_ede)\",\n\t\t.drv_name\t= \"ecb-3des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_3DES |\n\t\t\t\t   ENC_TYPE_CHAINING_ECB),\n\t\t.skcipher\t= {\n\t\t\t.min_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.max_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_3des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_ecb,\n\t\t\t.decrypt\t= n2_decrypt_ecb,\n\t\t},\n\t},\n\t{\t.name\t\t= \"cbc(des3_ede)\",\n\t\t.drv_name\t= \"cbc-3des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_3DES |\n\t\t\t\t   ENC_TYPE_CHAINING_CBC),\n\t\t.skcipher\t= {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.max_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_3des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_decrypt_chaining,\n\t\t},\n\t},\n\t{\t.name\t\t= \"cfb(des3_ede)\",\n\t\t.drv_name\t= \"cfb-3des\",\n\t\t.block_size\t= DES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_3DES |\n\t\t\t\t   ENC_TYPE_CHAINING_CFB),\n\t\t.skcipher\t= {\n\t\t\t.min_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.max_keysize\t= 3 * DES_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_3des_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_decrypt_chaining,\n\t\t},\n\t},\n\t \n\t{\t.name\t\t= \"ecb(aes)\",\n\t\t.drv_name\t= \"ecb-aes\",\n\t\t.block_size\t= AES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_AES128 |\n\t\t\t\t   ENC_TYPE_CHAINING_ECB),\n\t\t.skcipher\t= {\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_aes_setkey,\n\t\t\t.encrypt\t= n2_encrypt_ecb,\n\t\t\t.decrypt\t= n2_decrypt_ecb,\n\t\t},\n\t},\n\t{\t.name\t\t= \"cbc(aes)\",\n\t\t.drv_name\t= \"cbc-aes\",\n\t\t.block_size\t= AES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_AES128 |\n\t\t\t\t   ENC_TYPE_CHAINING_CBC),\n\t\t.skcipher\t= {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_aes_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_decrypt_chaining,\n\t\t},\n\t},\n\t{\t.name\t\t= \"ctr(aes)\",\n\t\t.drv_name\t= \"ctr-aes\",\n\t\t.block_size\t= AES_BLOCK_SIZE,\n\t\t.enc_type\t= (ENC_TYPE_ALG_AES128 |\n\t\t\t\t   ENC_TYPE_CHAINING_COUNTER),\n\t\t.skcipher\t= {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= n2_aes_setkey,\n\t\t\t.encrypt\t= n2_encrypt_chaining,\n\t\t\t.decrypt\t= n2_encrypt_chaining,\n\t\t},\n\t},\n\n};\n#define NUM_CIPHER_TMPLS ARRAY_SIZE(skcipher_tmpls)\n\nstatic LIST_HEAD(skcipher_algs);\n\nstruct n2_hash_tmpl {\n\tconst char\t*name;\n\tconst u8\t*hash_zero;\n\tconst u8\t*hash_init;\n\tu8\t\thw_op_hashsz;\n\tu8\t\tdigest_size;\n\tu8\t\tstatesize;\n\tu8\t\tblock_size;\n\tu8\t\tauth_type;\n\tu8\t\thmac_type;\n};\n\nstatic const __le32 n2_md5_init[MD5_HASH_WORDS] = {\n\tcpu_to_le32(MD5_H0),\n\tcpu_to_le32(MD5_H1),\n\tcpu_to_le32(MD5_H2),\n\tcpu_to_le32(MD5_H3),\n};\nstatic const u32 n2_sha1_init[SHA1_DIGEST_SIZE / 4] = {\n\tSHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3, SHA1_H4,\n};\nstatic const u32 n2_sha256_init[SHA256_DIGEST_SIZE / 4] = {\n\tSHA256_H0, SHA256_H1, SHA256_H2, SHA256_H3,\n\tSHA256_H4, SHA256_H5, SHA256_H6, SHA256_H7,\n};\nstatic const u32 n2_sha224_init[SHA256_DIGEST_SIZE / 4] = {\n\tSHA224_H0, SHA224_H1, SHA224_H2, SHA224_H3,\n\tSHA224_H4, SHA224_H5, SHA224_H6, SHA224_H7,\n};\n\nstatic const struct n2_hash_tmpl hash_tmpls[] = {\n\t{ .name\t\t= \"md5\",\n\t  .hash_zero\t= md5_zero_message_hash,\n\t  .hash_init\t= (u8 *)n2_md5_init,\n\t  .auth_type\t= AUTH_TYPE_MD5,\n\t  .hmac_type\t= AUTH_TYPE_HMAC_MD5,\n\t  .hw_op_hashsz\t= MD5_DIGEST_SIZE,\n\t  .digest_size\t= MD5_DIGEST_SIZE,\n\t  .statesize\t= sizeof(struct md5_state),\n\t  .block_size\t= MD5_HMAC_BLOCK_SIZE },\n\t{ .name\t\t= \"sha1\",\n\t  .hash_zero\t= sha1_zero_message_hash,\n\t  .hash_init\t= (u8 *)n2_sha1_init,\n\t  .auth_type\t= AUTH_TYPE_SHA1,\n\t  .hmac_type\t= AUTH_TYPE_HMAC_SHA1,\n\t  .hw_op_hashsz\t= SHA1_DIGEST_SIZE,\n\t  .digest_size\t= SHA1_DIGEST_SIZE,\n\t  .statesize\t= sizeof(struct sha1_state),\n\t  .block_size\t= SHA1_BLOCK_SIZE },\n\t{ .name\t\t= \"sha256\",\n\t  .hash_zero\t= sha256_zero_message_hash,\n\t  .hash_init\t= (u8 *)n2_sha256_init,\n\t  .auth_type\t= AUTH_TYPE_SHA256,\n\t  .hmac_type\t= AUTH_TYPE_HMAC_SHA256,\n\t  .hw_op_hashsz\t= SHA256_DIGEST_SIZE,\n\t  .digest_size\t= SHA256_DIGEST_SIZE,\n\t  .statesize\t= sizeof(struct sha256_state),\n\t  .block_size\t= SHA256_BLOCK_SIZE },\n\t{ .name\t\t= \"sha224\",\n\t  .hash_zero\t= sha224_zero_message_hash,\n\t  .hash_init\t= (u8 *)n2_sha224_init,\n\t  .auth_type\t= AUTH_TYPE_SHA256,\n\t  .hmac_type\t= AUTH_TYPE_RESERVED,\n\t  .hw_op_hashsz\t= SHA256_DIGEST_SIZE,\n\t  .digest_size\t= SHA224_DIGEST_SIZE,\n\t  .statesize\t= sizeof(struct sha256_state),\n\t  .block_size\t= SHA224_BLOCK_SIZE },\n};\n#define NUM_HASH_TMPLS ARRAY_SIZE(hash_tmpls)\n\nstatic LIST_HEAD(ahash_algs);\nstatic LIST_HEAD(hmac_algs);\n\nstatic int algs_registered;\n\nstatic void __n2_unregister_algs(void)\n{\n\tstruct n2_skcipher_alg *skcipher, *skcipher_tmp;\n\tstruct n2_ahash_alg *alg, *alg_tmp;\n\tstruct n2_hmac_alg *hmac, *hmac_tmp;\n\n\tlist_for_each_entry_safe(skcipher, skcipher_tmp, &skcipher_algs, entry) {\n\t\tcrypto_unregister_skcipher(&skcipher->skcipher);\n\t\tlist_del(&skcipher->entry);\n\t\tkfree(skcipher);\n\t}\n\tlist_for_each_entry_safe(hmac, hmac_tmp, &hmac_algs, derived.entry) {\n\t\tcrypto_unregister_ahash(&hmac->derived.alg);\n\t\tlist_del(&hmac->derived.entry);\n\t\tkfree(hmac);\n\t}\n\tlist_for_each_entry_safe(alg, alg_tmp, &ahash_algs, entry) {\n\t\tcrypto_unregister_ahash(&alg->alg);\n\t\tlist_del(&alg->entry);\n\t\tkfree(alg);\n\t}\n}\n\nstatic int n2_skcipher_init_tfm(struct crypto_skcipher *tfm)\n{\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct n2_request_context));\n\treturn 0;\n}\n\nstatic int __n2_register_one_skcipher(const struct n2_skcipher_tmpl *tmpl)\n{\n\tstruct n2_skcipher_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\n\tstruct skcipher_alg *alg;\n\tint err;\n\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\talg = &p->skcipher;\n\t*alg = tmpl->skcipher;\n\n\tsnprintf(alg->base.cra_name, CRYPTO_MAX_ALG_NAME, \"%s\", tmpl->name);\n\tsnprintf(alg->base.cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s-n2\", tmpl->drv_name);\n\talg->base.cra_priority = N2_CRA_PRIORITY;\n\talg->base.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t      CRYPTO_ALG_ALLOCATES_MEMORY;\n\talg->base.cra_blocksize = tmpl->block_size;\n\tp->enc_type = tmpl->enc_type;\n\talg->base.cra_ctxsize = sizeof(struct n2_skcipher_context);\n\talg->base.cra_module = THIS_MODULE;\n\talg->init = n2_skcipher_init_tfm;\n\n\tlist_add(&p->entry, &skcipher_algs);\n\terr = crypto_register_skcipher(alg);\n\tif (err) {\n\t\tpr_err(\"%s alg registration failed\\n\", alg->base.cra_name);\n\t\tlist_del(&p->entry);\n\t\tkfree(p);\n\t} else {\n\t\tpr_info(\"%s alg registered\\n\", alg->base.cra_name);\n\t}\n\treturn err;\n}\n\nstatic int __n2_register_one_hmac(struct n2_ahash_alg *n2ahash)\n{\n\tstruct n2_hmac_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\n\tstruct ahash_alg *ahash;\n\tstruct crypto_alg *base;\n\tint err;\n\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tp->child_alg = n2ahash->alg.halg.base.cra_name;\n\tmemcpy(&p->derived, n2ahash, sizeof(struct n2_ahash_alg));\n\tINIT_LIST_HEAD(&p->derived.entry);\n\n\tahash = &p->derived.alg;\n\tahash->digest = n2_hmac_async_digest;\n\tahash->setkey = n2_hmac_async_setkey;\n\n\tbase = &ahash->halg.base;\n\tsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, \"hmac(%s)\", p->child_alg);\n\tsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"hmac-%s-n2\", p->child_alg);\n\n\tbase->cra_ctxsize = sizeof(struct n2_hmac_ctx);\n\tbase->cra_init = n2_hmac_cra_init;\n\tbase->cra_exit = n2_hmac_cra_exit;\n\n\tlist_add(&p->derived.entry, &hmac_algs);\n\terr = crypto_register_ahash(ahash);\n\tif (err) {\n\t\tpr_err(\"%s alg registration failed\\n\", base->cra_name);\n\t\tlist_del(&p->derived.entry);\n\t\tkfree(p);\n\t} else {\n\t\tpr_info(\"%s alg registered\\n\", base->cra_name);\n\t}\n\treturn err;\n}\n\nstatic int __n2_register_one_ahash(const struct n2_hash_tmpl *tmpl)\n{\n\tstruct n2_ahash_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *base;\n\tstruct ahash_alg *ahash;\n\tint err;\n\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tp->hash_zero = tmpl->hash_zero;\n\tp->hash_init = tmpl->hash_init;\n\tp->auth_type = tmpl->auth_type;\n\tp->hmac_type = tmpl->hmac_type;\n\tp->hw_op_hashsz = tmpl->hw_op_hashsz;\n\tp->digest_size = tmpl->digest_size;\n\n\tahash = &p->alg;\n\tahash->init = n2_hash_async_init;\n\tahash->update = n2_hash_async_update;\n\tahash->final = n2_hash_async_final;\n\tahash->finup = n2_hash_async_finup;\n\tahash->digest = n2_hash_async_digest;\n\tahash->export = n2_hash_async_noexport;\n\tahash->import = n2_hash_async_noimport;\n\n\thalg = &ahash->halg;\n\thalg->digestsize = tmpl->digest_size;\n\thalg->statesize = tmpl->statesize;\n\n\tbase = &halg->base;\n\tsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, \"%s\", tmpl->name);\n\tsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s-n2\", tmpl->name);\n\tbase->cra_priority = N2_CRA_PRIORITY;\n\tbase->cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t  CRYPTO_ALG_NEED_FALLBACK;\n\tbase->cra_blocksize = tmpl->block_size;\n\tbase->cra_ctxsize = sizeof(struct n2_hash_ctx);\n\tbase->cra_module = THIS_MODULE;\n\tbase->cra_init = n2_hash_cra_init;\n\tbase->cra_exit = n2_hash_cra_exit;\n\n\tlist_add(&p->entry, &ahash_algs);\n\terr = crypto_register_ahash(ahash);\n\tif (err) {\n\t\tpr_err(\"%s alg registration failed\\n\", base->cra_name);\n\t\tlist_del(&p->entry);\n\t\tkfree(p);\n\t} else {\n\t\tpr_info(\"%s alg registered\\n\", base->cra_name);\n\t}\n\tif (!err && p->hmac_type != AUTH_TYPE_RESERVED)\n\t\terr = __n2_register_one_hmac(p);\n\treturn err;\n}\n\nstatic int n2_register_algs(void)\n{\n\tint i, err = 0;\n\n\tmutex_lock(&spu_lock);\n\tif (algs_registered++)\n\t\tgoto out;\n\n\tfor (i = 0; i < NUM_HASH_TMPLS; i++) {\n\t\terr = __n2_register_one_ahash(&hash_tmpls[i]);\n\t\tif (err) {\n\t\t\t__n2_unregister_algs();\n\t\t\tgoto out;\n\t\t}\n\t}\n\tfor (i = 0; i < NUM_CIPHER_TMPLS; i++) {\n\t\terr = __n2_register_one_skcipher(&skcipher_tmpls[i]);\n\t\tif (err) {\n\t\t\t__n2_unregister_algs();\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&spu_lock);\n\treturn err;\n}\n\nstatic void n2_unregister_algs(void)\n{\n\tmutex_lock(&spu_lock);\n\tif (!--algs_registered)\n\t\t__n2_unregister_algs();\n\tmutex_unlock(&spu_lock);\n}\n\n \nstatic int find_devino_index(struct platform_device *dev, struct spu_mdesc_info *ip,\n\t\t\t     unsigned long dev_ino)\n{\n\tconst unsigned int *dev_intrs;\n\tunsigned int intr;\n\tint i;\n\n\tfor (i = 0; i < ip->num_intrs; i++) {\n\t\tif (ip->ino_table[i].ino == dev_ino)\n\t\t\tbreak;\n\t}\n\tif (i == ip->num_intrs)\n\t\treturn -ENODEV;\n\n\tintr = ip->ino_table[i].intr;\n\n\tdev_intrs = of_get_property(dev->dev.of_node, \"interrupts\", NULL);\n\tif (!dev_intrs)\n\t\treturn -ENODEV;\n\n\tfor (i = 0; i < dev->archdata.num_irqs; i++) {\n\t\tif (dev_intrs[i] == intr)\n\t\t\treturn i;\n\t}\n\n\treturn -ENODEV;\n}\n\nstatic int spu_map_ino(struct platform_device *dev, struct spu_mdesc_info *ip,\n\t\t       const char *irq_name, struct spu_queue *p,\n\t\t       irq_handler_t handler)\n{\n\tunsigned long herr;\n\tint index;\n\n\therr = sun4v_ncs_qhandle_to_devino(p->qhandle, &p->devino);\n\tif (herr)\n\t\treturn -EINVAL;\n\n\tindex = find_devino_index(dev, ip, p->devino);\n\tif (index < 0)\n\t\treturn index;\n\n\tp->irq = dev->archdata.irqs[index];\n\n\tsprintf(p->irq_name, \"%s-%d\", irq_name, index);\n\n\treturn request_irq(p->irq, handler, 0, p->irq_name, p);\n}\n\nstatic struct kmem_cache *queue_cache[2];\n\nstatic void *new_queue(unsigned long q_type)\n{\n\treturn kmem_cache_zalloc(queue_cache[q_type - 1], GFP_KERNEL);\n}\n\nstatic void free_queue(void *p, unsigned long q_type)\n{\n\tkmem_cache_free(queue_cache[q_type - 1], p);\n}\n\nstatic int queue_cache_init(void)\n{\n\tif (!queue_cache[HV_NCS_QTYPE_MAU - 1])\n\t\tqueue_cache[HV_NCS_QTYPE_MAU - 1] =\n\t\t\tkmem_cache_create(\"mau_queue\",\n\t\t\t\t\t  (MAU_NUM_ENTRIES *\n\t\t\t\t\t   MAU_ENTRY_SIZE),\n\t\t\t\t\t  MAU_ENTRY_SIZE, 0, NULL);\n\tif (!queue_cache[HV_NCS_QTYPE_MAU - 1])\n\t\treturn -ENOMEM;\n\n\tif (!queue_cache[HV_NCS_QTYPE_CWQ - 1])\n\t\tqueue_cache[HV_NCS_QTYPE_CWQ - 1] =\n\t\t\tkmem_cache_create(\"cwq_queue\",\n\t\t\t\t\t  (CWQ_NUM_ENTRIES *\n\t\t\t\t\t   CWQ_ENTRY_SIZE),\n\t\t\t\t\t  CWQ_ENTRY_SIZE, 0, NULL);\n\tif (!queue_cache[HV_NCS_QTYPE_CWQ - 1]) {\n\t\tkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);\n\t\tqueue_cache[HV_NCS_QTYPE_MAU - 1] = NULL;\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void queue_cache_destroy(void)\n{\n\tkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);\n\tkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_CWQ - 1]);\n\tqueue_cache[HV_NCS_QTYPE_MAU - 1] = NULL;\n\tqueue_cache[HV_NCS_QTYPE_CWQ - 1] = NULL;\n}\n\nstatic long spu_queue_register_workfn(void *arg)\n{\n\tstruct spu_qreg *qr = arg;\n\tstruct spu_queue *p = qr->queue;\n\tunsigned long q_type = qr->type;\n\tunsigned long hv_ret;\n\n\thv_ret = sun4v_ncs_qconf(q_type, __pa(p->q),\n\t\t\t\t CWQ_NUM_ENTRIES, &p->qhandle);\n\tif (!hv_ret)\n\t\tsun4v_ncs_sethead_marker(p->qhandle, 0);\n\n\treturn hv_ret ? -EINVAL : 0;\n}\n\nstatic int spu_queue_register(struct spu_queue *p, unsigned long q_type)\n{\n\tint cpu = cpumask_any_and(&p->sharing, cpu_online_mask);\n\tstruct spu_qreg qr = { .queue = p, .type = q_type };\n\n\treturn work_on_cpu_safe(cpu, spu_queue_register_workfn, &qr);\n}\n\nstatic int spu_queue_setup(struct spu_queue *p)\n{\n\tint err;\n\n\tp->q = new_queue(p->q_type);\n\tif (!p->q)\n\t\treturn -ENOMEM;\n\n\terr = spu_queue_register(p, p->q_type);\n\tif (err) {\n\t\tfree_queue(p->q, p->q_type);\n\t\tp->q = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic void spu_queue_destroy(struct spu_queue *p)\n{\n\tunsigned long hv_ret;\n\n\tif (!p->q)\n\t\treturn;\n\n\thv_ret = sun4v_ncs_qconf(p->q_type, p->qhandle, 0, &p->qhandle);\n\n\tif (!hv_ret)\n\t\tfree_queue(p->q, p->q_type);\n}\n\nstatic void spu_list_destroy(struct list_head *list)\n{\n\tstruct spu_queue *p, *n;\n\n\tlist_for_each_entry_safe(p, n, list, list) {\n\t\tint i;\n\n\t\tfor (i = 0; i < NR_CPUS; i++) {\n\t\t\tif (cpu_to_cwq[i] == p)\n\t\t\t\tcpu_to_cwq[i] = NULL;\n\t\t}\n\n\t\tif (p->irq) {\n\t\t\tfree_irq(p->irq, p);\n\t\t\tp->irq = 0;\n\t\t}\n\t\tspu_queue_destroy(p);\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n}\n\n \nstatic int spu_mdesc_walk_arcs(struct mdesc_handle *mdesc,\n\t\t\t       struct platform_device *dev,\n\t\t\t       u64 node, struct spu_queue *p,\n\t\t\t       struct spu_queue **table)\n{\n\tu64 arc;\n\n\tmdesc_for_each_arc(arc, mdesc, node, MDESC_ARC_TYPE_BACK) {\n\t\tu64 tgt = mdesc_arc_target(mdesc, arc);\n\t\tconst char *name = mdesc_node_name(mdesc, tgt);\n\t\tconst u64 *id;\n\n\t\tif (strcmp(name, \"cpu\"))\n\t\t\tcontinue;\n\t\tid = mdesc_get_property(mdesc, tgt, \"id\", NULL);\n\t\tif (table[*id] != NULL) {\n\t\t\tdev_err(&dev->dev, \"%pOF: SPU cpu slot already set.\\n\",\n\t\t\t\tdev->dev.of_node);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcpumask_set_cpu(*id, &p->sharing);\n\t\ttable[*id] = p;\n\t}\n\treturn 0;\n}\n\n \nstatic int handle_exec_unit(struct spu_mdesc_info *ip, struct list_head *list,\n\t\t\t    struct platform_device *dev, struct mdesc_handle *mdesc,\n\t\t\t    u64 node, const char *iname, unsigned long q_type,\n\t\t\t    irq_handler_t handler, struct spu_queue **table)\n{\n\tstruct spu_queue *p;\n\tint err;\n\n\tp = kzalloc(sizeof(struct spu_queue), GFP_KERNEL);\n\tif (!p) {\n\t\tdev_err(&dev->dev, \"%pOF: Could not allocate SPU queue.\\n\",\n\t\t\tdev->dev.of_node);\n\t\treturn -ENOMEM;\n\t}\n\n\tcpumask_clear(&p->sharing);\n\tspin_lock_init(&p->lock);\n\tp->q_type = q_type;\n\tINIT_LIST_HEAD(&p->jobs);\n\tlist_add(&p->list, list);\n\n\terr = spu_mdesc_walk_arcs(mdesc, dev, node, p, table);\n\tif (err)\n\t\treturn err;\n\n\terr = spu_queue_setup(p);\n\tif (err)\n\t\treturn err;\n\n\treturn spu_map_ino(dev, ip, iname, p, handler);\n}\n\nstatic int spu_mdesc_scan(struct mdesc_handle *mdesc, struct platform_device *dev,\n\t\t\t  struct spu_mdesc_info *ip, struct list_head *list,\n\t\t\t  const char *exec_name, unsigned long q_type,\n\t\t\t  irq_handler_t handler, struct spu_queue **table)\n{\n\tint err = 0;\n\tu64 node;\n\n\tmdesc_for_each_node_by_name(mdesc, node, \"exec-unit\") {\n\t\tconst char *type;\n\n\t\ttype = mdesc_get_property(mdesc, node, \"type\", NULL);\n\t\tif (!type || strcmp(type, exec_name))\n\t\t\tcontinue;\n\n\t\terr = handle_exec_unit(ip, list, dev, mdesc, node,\n\t\t\t\t       exec_name, q_type, handler, table);\n\t\tif (err) {\n\t\t\tspu_list_destroy(list);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int get_irq_props(struct mdesc_handle *mdesc, u64 node,\n\t\t\t struct spu_mdesc_info *ip)\n{\n\tconst u64 *ino;\n\tint ino_len;\n\tint i;\n\n\tino = mdesc_get_property(mdesc, node, \"ino\", &ino_len);\n\tif (!ino) {\n\t\tprintk(\"NO 'ino'\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tip->num_intrs = ino_len / sizeof(u64);\n\tip->ino_table = kzalloc((sizeof(struct ino_blob) *\n\t\t\t\t ip->num_intrs),\n\t\t\t\tGFP_KERNEL);\n\tif (!ip->ino_table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ip->num_intrs; i++) {\n\t\tstruct ino_blob *b = &ip->ino_table[i];\n\t\tb->intr = i + 1;\n\t\tb->ino = ino[i];\n\t}\n\n\treturn 0;\n}\n\nstatic int grab_mdesc_irq_props(struct mdesc_handle *mdesc,\n\t\t\t\tstruct platform_device *dev,\n\t\t\t\tstruct spu_mdesc_info *ip,\n\t\t\t\tconst char *node_name)\n{\n\tu64 node, reg;\n\n\tif (of_property_read_reg(dev->dev.of_node, 0, &reg, NULL) < 0)\n\t\treturn -ENODEV;\n\n\tmdesc_for_each_node_by_name(mdesc, node, \"virtual-device\") {\n\t\tconst char *name;\n\t\tconst u64 *chdl;\n\n\t\tname = mdesc_get_property(mdesc, node, \"name\", NULL);\n\t\tif (!name || strcmp(name, node_name))\n\t\t\tcontinue;\n\t\tchdl = mdesc_get_property(mdesc, node, \"cfg-handle\", NULL);\n\t\tif (!chdl || (*chdl != reg))\n\t\t\tcontinue;\n\t\tip->cfg_handle = *chdl;\n\t\treturn get_irq_props(mdesc, node, ip);\n\t}\n\n\treturn -ENODEV;\n}\n\nstatic unsigned long n2_spu_hvapi_major;\nstatic unsigned long n2_spu_hvapi_minor;\n\nstatic int n2_spu_hvapi_register(void)\n{\n\tint err;\n\n\tn2_spu_hvapi_major = 2;\n\tn2_spu_hvapi_minor = 0;\n\n\terr = sun4v_hvapi_register(HV_GRP_NCS,\n\t\t\t\t   n2_spu_hvapi_major,\n\t\t\t\t   &n2_spu_hvapi_minor);\n\n\tif (!err)\n\t\tpr_info(\"Registered NCS HVAPI version %lu.%lu\\n\",\n\t\t\tn2_spu_hvapi_major,\n\t\t\tn2_spu_hvapi_minor);\n\n\treturn err;\n}\n\nstatic void n2_spu_hvapi_unregister(void)\n{\n\tsun4v_hvapi_unregister(HV_GRP_NCS);\n}\n\nstatic int global_ref;\n\nstatic int grab_global_resources(void)\n{\n\tint err = 0;\n\n\tmutex_lock(&spu_lock);\n\n\tif (global_ref++)\n\t\tgoto out;\n\n\terr = n2_spu_hvapi_register();\n\tif (err)\n\t\tgoto out;\n\n\terr = queue_cache_init();\n\tif (err)\n\t\tgoto out_hvapi_release;\n\n\terr = -ENOMEM;\n\tcpu_to_cwq = kcalloc(NR_CPUS, sizeof(struct spu_queue *),\n\t\t\t     GFP_KERNEL);\n\tif (!cpu_to_cwq)\n\t\tgoto out_queue_cache_destroy;\n\n\tcpu_to_mau = kcalloc(NR_CPUS, sizeof(struct spu_queue *),\n\t\t\t     GFP_KERNEL);\n\tif (!cpu_to_mau)\n\t\tgoto out_free_cwq_table;\n\n\terr = 0;\n\nout:\n\tif (err)\n\t\tglobal_ref--;\n\tmutex_unlock(&spu_lock);\n\treturn err;\n\nout_free_cwq_table:\n\tkfree(cpu_to_cwq);\n\tcpu_to_cwq = NULL;\n\nout_queue_cache_destroy:\n\tqueue_cache_destroy();\n\nout_hvapi_release:\n\tn2_spu_hvapi_unregister();\n\tgoto out;\n}\n\nstatic void release_global_resources(void)\n{\n\tmutex_lock(&spu_lock);\n\tif (!--global_ref) {\n\t\tkfree(cpu_to_cwq);\n\t\tcpu_to_cwq = NULL;\n\n\t\tkfree(cpu_to_mau);\n\t\tcpu_to_mau = NULL;\n\n\t\tqueue_cache_destroy();\n\t\tn2_spu_hvapi_unregister();\n\t}\n\tmutex_unlock(&spu_lock);\n}\n\nstatic struct n2_crypto *alloc_n2cp(void)\n{\n\tstruct n2_crypto *np = kzalloc(sizeof(struct n2_crypto), GFP_KERNEL);\n\n\tif (np)\n\t\tINIT_LIST_HEAD(&np->cwq_list);\n\n\treturn np;\n}\n\nstatic void free_n2cp(struct n2_crypto *np)\n{\n\tkfree(np->cwq_info.ino_table);\n\tnp->cwq_info.ino_table = NULL;\n\n\tkfree(np);\n}\n\nstatic void n2_spu_driver_version(void)\n{\n\tstatic int n2_spu_version_printed;\n\n\tif (n2_spu_version_printed++ == 0)\n\t\tpr_info(\"%s\", version);\n}\n\nstatic int n2_crypto_probe(struct platform_device *dev)\n{\n\tstruct mdesc_handle *mdesc;\n\tstruct n2_crypto *np;\n\tint err;\n\n\tn2_spu_driver_version();\n\n\tpr_info(\"Found N2CP at %pOF\\n\", dev->dev.of_node);\n\n\tnp = alloc_n2cp();\n\tif (!np) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to allocate n2cp.\\n\",\n\t\t\tdev->dev.of_node);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = grab_global_resources();\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab global resources.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tgoto out_free_n2cp;\n\t}\n\n\tmdesc = mdesc_grab();\n\n\tif (!mdesc) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab MDESC.\\n\",\n\t\t\tdev->dev.of_node);\n\t\terr = -ENODEV;\n\t\tgoto out_free_global;\n\t}\n\terr = grab_mdesc_irq_props(mdesc, dev, &np->cwq_info, \"n2cp\");\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab IRQ props.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tmdesc_release(mdesc);\n\t\tgoto out_free_global;\n\t}\n\n\terr = spu_mdesc_scan(mdesc, dev, &np->cwq_info, &np->cwq_list,\n\t\t\t     \"cwq\", HV_NCS_QTYPE_CWQ, cwq_intr,\n\t\t\t     cpu_to_cwq);\n\tmdesc_release(mdesc);\n\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: CWQ MDESC scan failed.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tgoto out_free_global;\n\t}\n\n\terr = n2_register_algs();\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to register algorithms.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tgoto out_free_spu_list;\n\t}\n\n\tdev_set_drvdata(&dev->dev, np);\n\n\treturn 0;\n\nout_free_spu_list:\n\tspu_list_destroy(&np->cwq_list);\n\nout_free_global:\n\trelease_global_resources();\n\nout_free_n2cp:\n\tfree_n2cp(np);\n\n\treturn err;\n}\n\nstatic int n2_crypto_remove(struct platform_device *dev)\n{\n\tstruct n2_crypto *np = dev_get_drvdata(&dev->dev);\n\n\tn2_unregister_algs();\n\n\tspu_list_destroy(&np->cwq_list);\n\n\trelease_global_resources();\n\n\tfree_n2cp(np);\n\n\treturn 0;\n}\n\nstatic struct n2_mau *alloc_ncp(void)\n{\n\tstruct n2_mau *mp = kzalloc(sizeof(struct n2_mau), GFP_KERNEL);\n\n\tif (mp)\n\t\tINIT_LIST_HEAD(&mp->mau_list);\n\n\treturn mp;\n}\n\nstatic void free_ncp(struct n2_mau *mp)\n{\n\tkfree(mp->mau_info.ino_table);\n\tmp->mau_info.ino_table = NULL;\n\n\tkfree(mp);\n}\n\nstatic int n2_mau_probe(struct platform_device *dev)\n{\n\tstruct mdesc_handle *mdesc;\n\tstruct n2_mau *mp;\n\tint err;\n\n\tn2_spu_driver_version();\n\n\tpr_info(\"Found NCP at %pOF\\n\", dev->dev.of_node);\n\n\tmp = alloc_ncp();\n\tif (!mp) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to allocate ncp.\\n\",\n\t\t\tdev->dev.of_node);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = grab_global_resources();\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab global resources.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tgoto out_free_ncp;\n\t}\n\n\tmdesc = mdesc_grab();\n\n\tif (!mdesc) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab MDESC.\\n\",\n\t\t\tdev->dev.of_node);\n\t\terr = -ENODEV;\n\t\tgoto out_free_global;\n\t}\n\n\terr = grab_mdesc_irq_props(mdesc, dev, &mp->mau_info, \"ncp\");\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: Unable to grab IRQ props.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tmdesc_release(mdesc);\n\t\tgoto out_free_global;\n\t}\n\n\terr = spu_mdesc_scan(mdesc, dev, &mp->mau_info, &mp->mau_list,\n\t\t\t     \"mau\", HV_NCS_QTYPE_MAU, mau_intr,\n\t\t\t     cpu_to_mau);\n\tmdesc_release(mdesc);\n\n\tif (err) {\n\t\tdev_err(&dev->dev, \"%pOF: MAU MDESC scan failed.\\n\",\n\t\t\tdev->dev.of_node);\n\t\tgoto out_free_global;\n\t}\n\n\tdev_set_drvdata(&dev->dev, mp);\n\n\treturn 0;\n\nout_free_global:\n\trelease_global_resources();\n\nout_free_ncp:\n\tfree_ncp(mp);\n\n\treturn err;\n}\n\nstatic int n2_mau_remove(struct platform_device *dev)\n{\n\tstruct n2_mau *mp = dev_get_drvdata(&dev->dev);\n\n\tspu_list_destroy(&mp->mau_list);\n\n\trelease_global_resources();\n\n\tfree_ncp(mp);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id n2_crypto_match[] = {\n\t{\n\t\t.name = \"n2cp\",\n\t\t.compatible = \"SUNW,n2-cwq\",\n\t},\n\t{\n\t\t.name = \"n2cp\",\n\t\t.compatible = \"SUNW,vf-cwq\",\n\t},\n\t{\n\t\t.name = \"n2cp\",\n\t\t.compatible = \"SUNW,kt-cwq\",\n\t},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, n2_crypto_match);\n\nstatic struct platform_driver n2_crypto_driver = {\n\t.driver = {\n\t\t.name\t\t=\t\"n2cp\",\n\t\t.of_match_table\t=\tn2_crypto_match,\n\t},\n\t.probe\t\t=\tn2_crypto_probe,\n\t.remove\t\t=\tn2_crypto_remove,\n};\n\nstatic const struct of_device_id n2_mau_match[] = {\n\t{\n\t\t.name = \"ncp\",\n\t\t.compatible = \"SUNW,n2-mau\",\n\t},\n\t{\n\t\t.name = \"ncp\",\n\t\t.compatible = \"SUNW,vf-mau\",\n\t},\n\t{\n\t\t.name = \"ncp\",\n\t\t.compatible = \"SUNW,kt-mau\",\n\t},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, n2_mau_match);\n\nstatic struct platform_driver n2_mau_driver = {\n\t.driver = {\n\t\t.name\t\t=\t\"ncp\",\n\t\t.of_match_table\t=\tn2_mau_match,\n\t},\n\t.probe\t\t=\tn2_mau_probe,\n\t.remove\t\t=\tn2_mau_remove,\n};\n\nstatic struct platform_driver * const drivers[] = {\n\t&n2_crypto_driver,\n\t&n2_mau_driver,\n};\n\nstatic int __init n2_init(void)\n{\n\treturn platform_register_drivers(drivers, ARRAY_SIZE(drivers));\n}\n\nstatic void __exit n2_exit(void)\n{\n\tplatform_unregister_drivers(drivers, ARRAY_SIZE(drivers));\n}\n\nmodule_init(n2_init);\nmodule_exit(n2_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}