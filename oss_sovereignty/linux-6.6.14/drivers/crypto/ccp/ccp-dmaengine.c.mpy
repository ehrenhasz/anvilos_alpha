{
  "module_name": "ccp-dmaengine.c",
  "hash_id": "ec4d4b764c1a7c83436e727dc2db3882172cdb67c50a9da137bea780e2ff53a7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccp/ccp-dmaengine.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/spinlock.h>\n#include <linux/mutex.h>\n#include <linux/ccp.h>\n\n#include \"ccp-dev.h\"\n#include \"../../dma/dmaengine.h\"\n\n#define CCP_DMA_WIDTH(_mask)\t\t\\\n({\t\t\t\t\t\\\n\tu64 mask = _mask + 1;\t\t\\\n\t(mask == 0) ? 64 : fls64(mask);\t\\\n})\n\n \nstatic unsigned int dma_chan_attr = CCP_DMA_DFLT;\nmodule_param(dma_chan_attr, uint, 0444);\nMODULE_PARM_DESC(dma_chan_attr, \"Set DMA channel visibility: 0 (default) = device defaults, 1 = make private, 2 = make public\");\n\nstatic unsigned int dmaengine = 1;\nmodule_param(dmaengine, uint, 0444);\nMODULE_PARM_DESC(dmaengine, \"Register services with the DMA subsystem (any non-zero value, default: 1)\");\n\nstatic unsigned int ccp_get_dma_chan_attr(struct ccp_device *ccp)\n{\n\tswitch (dma_chan_attr) {\n\tcase CCP_DMA_DFLT:\n\t\treturn ccp->vdata->dma_chan_attr;\n\n\tcase CCP_DMA_PRIV:\n\t\treturn DMA_PRIVATE;\n\n\tcase CCP_DMA_PUB:\n\t\treturn 0;\n\n\tdefault:\n\t\tdev_info_once(ccp->dev, \"Invalid value for dma_chan_attr: %d\\n\",\n\t\t\t      dma_chan_attr);\n\t\treturn ccp->vdata->dma_chan_attr;\n\t}\n}\n\nstatic void ccp_free_cmd_resources(struct ccp_device *ccp,\n\t\t\t\t   struct list_head *list)\n{\n\tstruct ccp_dma_cmd *cmd, *ctmp;\n\n\tlist_for_each_entry_safe(cmd, ctmp, list, entry) {\n\t\tlist_del(&cmd->entry);\n\t\tkmem_cache_free(ccp->dma_cmd_cache, cmd);\n\t}\n}\n\nstatic void ccp_free_desc_resources(struct ccp_device *ccp,\n\t\t\t\t    struct list_head *list)\n{\n\tstruct ccp_dma_desc *desc, *dtmp;\n\n\tlist_for_each_entry_safe(desc, dtmp, list, entry) {\n\t\tccp_free_cmd_resources(ccp, &desc->active);\n\t\tccp_free_cmd_resources(ccp, &desc->pending);\n\n\t\tlist_del(&desc->entry);\n\t\tkmem_cache_free(ccp->dma_desc_cache, desc);\n\t}\n}\n\nstatic void ccp_free_chan_resources(struct dma_chan *dma_chan)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tunsigned long flags;\n\n\tdev_dbg(chan->ccp->dev, \"%s - chan=%p\\n\", __func__, chan);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tccp_free_desc_resources(chan->ccp, &chan->complete);\n\tccp_free_desc_resources(chan->ccp, &chan->active);\n\tccp_free_desc_resources(chan->ccp, &chan->pending);\n\tccp_free_desc_resources(chan->ccp, &chan->created);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic void ccp_cleanup_desc_resources(struct ccp_device *ccp,\n\t\t\t\t       struct list_head *list)\n{\n\tstruct ccp_dma_desc *desc, *dtmp;\n\n\tlist_for_each_entry_safe_reverse(desc, dtmp, list, entry) {\n\t\tif (!async_tx_test_ack(&desc->tx_desc))\n\t\t\tcontinue;\n\n\t\tdev_dbg(ccp->dev, \"%s - desc=%p\\n\", __func__, desc);\n\n\t\tccp_free_cmd_resources(ccp, &desc->active);\n\t\tccp_free_cmd_resources(ccp, &desc->pending);\n\n\t\tlist_del(&desc->entry);\n\t\tkmem_cache_free(ccp->dma_desc_cache, desc);\n\t}\n}\n\nstatic void ccp_do_cleanup(unsigned long data)\n{\n\tstruct ccp_dma_chan *chan = (struct ccp_dma_chan *)data;\n\tunsigned long flags;\n\n\tdev_dbg(chan->ccp->dev, \"%s - chan=%s\\n\", __func__,\n\t\tdma_chan_name(&chan->dma_chan));\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tccp_cleanup_desc_resources(chan->ccp, &chan->complete);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic int ccp_issue_next_cmd(struct ccp_dma_desc *desc)\n{\n\tstruct ccp_dma_cmd *cmd;\n\tint ret;\n\n\tcmd = list_first_entry(&desc->pending, struct ccp_dma_cmd, entry);\n\tlist_move(&cmd->entry, &desc->active);\n\n\tdev_dbg(desc->ccp->dev, \"%s - tx %d, cmd=%p\\n\", __func__,\n\t\tdesc->tx_desc.cookie, cmd);\n\n\tret = ccp_enqueue_cmd(&cmd->ccp_cmd);\n\tif (!ret || (ret == -EINPROGRESS) || (ret == -EBUSY))\n\t\treturn 0;\n\n\tdev_dbg(desc->ccp->dev, \"%s - error: ret=%d, tx %d, cmd=%p\\n\", __func__,\n\t\tret, desc->tx_desc.cookie, cmd);\n\n\treturn ret;\n}\n\nstatic void ccp_free_active_cmd(struct ccp_dma_desc *desc)\n{\n\tstruct ccp_dma_cmd *cmd;\n\n\tcmd = list_first_entry_or_null(&desc->active, struct ccp_dma_cmd,\n\t\t\t\t       entry);\n\tif (!cmd)\n\t\treturn;\n\n\tdev_dbg(desc->ccp->dev, \"%s - freeing tx %d cmd=%p\\n\",\n\t\t__func__, desc->tx_desc.cookie, cmd);\n\n\tlist_del(&cmd->entry);\n\tkmem_cache_free(desc->ccp->dma_cmd_cache, cmd);\n}\n\nstatic struct ccp_dma_desc *__ccp_next_dma_desc(struct ccp_dma_chan *chan,\n\t\t\t\t\t\tstruct ccp_dma_desc *desc)\n{\n\t \n\tif (desc)\n\t\tlist_move(&desc->entry, &chan->complete);\n\n\t \n\tdesc = list_first_entry_or_null(&chan->active, struct ccp_dma_desc,\n\t\t\t\t\tentry);\n\n\treturn desc;\n}\n\nstatic struct ccp_dma_desc *ccp_handle_active_desc(struct ccp_dma_chan *chan,\n\t\t\t\t\t\t   struct ccp_dma_desc *desc)\n{\n\tstruct dma_async_tx_descriptor *tx_desc;\n\tunsigned long flags;\n\n\t \n\tdo {\n\t\tif (desc) {\n\t\t\t \n\t\t\tccp_free_active_cmd(desc);\n\n\t\t\tif (!list_empty(&desc->pending)) {\n\t\t\t\t \n\t\t\t\tif (desc->status != DMA_ERROR)\n\t\t\t\t\treturn desc;\n\n\t\t\t\t \n\t\t\t\tccp_free_cmd_resources(desc->ccp,\n\t\t\t\t\t\t       &desc->pending);\n\t\t\t}\n\n\t\t\ttx_desc = &desc->tx_desc;\n\t\t} else {\n\t\t\ttx_desc = NULL;\n\t\t}\n\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\n\t\tif (desc) {\n\t\t\tif (desc->status != DMA_ERROR)\n\t\t\t\tdesc->status = DMA_COMPLETE;\n\n\t\t\tdev_dbg(desc->ccp->dev,\n\t\t\t\t\"%s - tx %d complete, status=%u\\n\", __func__,\n\t\t\t\tdesc->tx_desc.cookie, desc->status);\n\n\t\t\tdma_cookie_complete(tx_desc);\n\t\t\tdma_descriptor_unmap(tx_desc);\n\t\t}\n\n\t\tdesc = __ccp_next_dma_desc(chan, desc);\n\n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\t\tif (tx_desc) {\n\t\t\tdmaengine_desc_get_callback_invoke(tx_desc, NULL);\n\n\t\t\tdma_run_dependencies(tx_desc);\n\t\t}\n\t} while (desc);\n\n\treturn NULL;\n}\n\nstatic struct ccp_dma_desc *__ccp_pending_to_active(struct ccp_dma_chan *chan)\n{\n\tstruct ccp_dma_desc *desc;\n\n\tif (list_empty(&chan->pending))\n\t\treturn NULL;\n\n\tdesc = list_empty(&chan->active)\n\t\t? list_first_entry(&chan->pending, struct ccp_dma_desc, entry)\n\t\t: NULL;\n\n\tlist_splice_tail_init(&chan->pending, &chan->active);\n\n\treturn desc;\n}\n\nstatic void ccp_cmd_callback(void *data, int err)\n{\n\tstruct ccp_dma_desc *desc = data;\n\tstruct ccp_dma_chan *chan;\n\tint ret;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tchan = container_of(desc->tx_desc.chan, struct ccp_dma_chan,\n\t\t\t    dma_chan);\n\n\tdev_dbg(chan->ccp->dev, \"%s - tx %d callback, err=%d\\n\",\n\t\t__func__, desc->tx_desc.cookie, err);\n\n\tif (err)\n\t\tdesc->status = DMA_ERROR;\n\n\twhile (true) {\n\t\t \n\t\tdesc = ccp_handle_active_desc(chan, desc);\n\n\t\t \n\t\tif (!desc || (chan->status == DMA_PAUSED))\n\t\t\tbreak;\n\n\t\tret = ccp_issue_next_cmd(desc);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tdesc->status = DMA_ERROR;\n\t}\n\n\ttasklet_schedule(&chan->cleanup_tasklet);\n}\n\nstatic dma_cookie_t ccp_tx_submit(struct dma_async_tx_descriptor *tx_desc)\n{\n\tstruct ccp_dma_desc *desc = container_of(tx_desc, struct ccp_dma_desc,\n\t\t\t\t\t\t tx_desc);\n\tstruct ccp_dma_chan *chan;\n\tdma_cookie_t cookie;\n\tunsigned long flags;\n\n\tchan = container_of(tx_desc->chan, struct ccp_dma_chan, dma_chan);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tcookie = dma_cookie_assign(tx_desc);\n\tlist_move_tail(&desc->entry, &chan->pending);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tdev_dbg(chan->ccp->dev, \"%s - added tx descriptor %d to pending list\\n\",\n\t\t__func__, cookie);\n\n\treturn cookie;\n}\n\nstatic struct ccp_dma_cmd *ccp_alloc_dma_cmd(struct ccp_dma_chan *chan)\n{\n\tstruct ccp_dma_cmd *cmd;\n\n\tcmd = kmem_cache_alloc(chan->ccp->dma_cmd_cache, GFP_NOWAIT);\n\tif (cmd)\n\t\tmemset(cmd, 0, sizeof(*cmd));\n\n\treturn cmd;\n}\n\nstatic struct ccp_dma_desc *ccp_alloc_dma_desc(struct ccp_dma_chan *chan,\n\t\t\t\t\t       unsigned long flags)\n{\n\tstruct ccp_dma_desc *desc;\n\n\tdesc = kmem_cache_zalloc(chan->ccp->dma_desc_cache, GFP_NOWAIT);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->tx_desc, &chan->dma_chan);\n\tdesc->tx_desc.flags = flags;\n\tdesc->tx_desc.tx_submit = ccp_tx_submit;\n\tdesc->ccp = chan->ccp;\n\tINIT_LIST_HEAD(&desc->entry);\n\tINIT_LIST_HEAD(&desc->pending);\n\tINIT_LIST_HEAD(&desc->active);\n\tdesc->status = DMA_IN_PROGRESS;\n\n\treturn desc;\n}\n\nstatic struct ccp_dma_desc *ccp_create_desc(struct dma_chan *dma_chan,\n\t\t\t\t\t    struct scatterlist *dst_sg,\n\t\t\t\t\t    unsigned int dst_nents,\n\t\t\t\t\t    struct scatterlist *src_sg,\n\t\t\t\t\t    unsigned int src_nents,\n\t\t\t\t\t    unsigned long flags)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_device *ccp = chan->ccp;\n\tstruct ccp_dma_desc *desc;\n\tstruct ccp_dma_cmd *cmd;\n\tstruct ccp_cmd *ccp_cmd;\n\tstruct ccp_passthru_nomap_engine *ccp_pt;\n\tunsigned int src_offset, src_len;\n\tunsigned int dst_offset, dst_len;\n\tunsigned int len;\n\tunsigned long sflags;\n\tsize_t total_len;\n\n\tif (!dst_sg || !src_sg)\n\t\treturn NULL;\n\n\tif (!dst_nents || !src_nents)\n\t\treturn NULL;\n\n\tdesc = ccp_alloc_dma_desc(chan, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\ttotal_len = 0;\n\n\tsrc_len = sg_dma_len(src_sg);\n\tsrc_offset = 0;\n\n\tdst_len = sg_dma_len(dst_sg);\n\tdst_offset = 0;\n\n\twhile (true) {\n\t\tif (!src_len) {\n\t\t\tsrc_nents--;\n\t\t\tif (!src_nents)\n\t\t\t\tbreak;\n\n\t\t\tsrc_sg = sg_next(src_sg);\n\t\t\tif (!src_sg)\n\t\t\t\tbreak;\n\n\t\t\tsrc_len = sg_dma_len(src_sg);\n\t\t\tsrc_offset = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!dst_len) {\n\t\t\tdst_nents--;\n\t\t\tif (!dst_nents)\n\t\t\t\tbreak;\n\n\t\t\tdst_sg = sg_next(dst_sg);\n\t\t\tif (!dst_sg)\n\t\t\t\tbreak;\n\n\t\t\tdst_len = sg_dma_len(dst_sg);\n\t\t\tdst_offset = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlen = min(dst_len, src_len);\n\n\t\tcmd = ccp_alloc_dma_cmd(chan);\n\t\tif (!cmd)\n\t\t\tgoto err;\n\n\t\tccp_cmd = &cmd->ccp_cmd;\n\t\tccp_cmd->ccp = chan->ccp;\n\t\tccp_pt = &ccp_cmd->u.passthru_nomap;\n\t\tccp_cmd->flags = CCP_CMD_MAY_BACKLOG;\n\t\tccp_cmd->flags |= CCP_CMD_PASSTHRU_NO_DMA_MAP;\n\t\tccp_cmd->engine = CCP_ENGINE_PASSTHRU;\n\t\tccp_pt->bit_mod = CCP_PASSTHRU_BITWISE_NOOP;\n\t\tccp_pt->byte_swap = CCP_PASSTHRU_BYTESWAP_NOOP;\n\t\tccp_pt->src_dma = sg_dma_address(src_sg) + src_offset;\n\t\tccp_pt->dst_dma = sg_dma_address(dst_sg) + dst_offset;\n\t\tccp_pt->src_len = len;\n\t\tccp_pt->final = 1;\n\t\tccp_cmd->callback = ccp_cmd_callback;\n\t\tccp_cmd->data = desc;\n\n\t\tlist_add_tail(&cmd->entry, &desc->pending);\n\n\t\tdev_dbg(ccp->dev,\n\t\t\t\"%s - cmd=%p, src=%pad, dst=%pad, len=%llu\\n\", __func__,\n\t\t\tcmd, &ccp_pt->src_dma,\n\t\t\t&ccp_pt->dst_dma, ccp_pt->src_len);\n\n\t\ttotal_len += len;\n\n\t\tsrc_len -= len;\n\t\tsrc_offset += len;\n\n\t\tdst_len -= len;\n\t\tdst_offset += len;\n\t}\n\n\tdesc->len = total_len;\n\n\tif (list_empty(&desc->pending))\n\t\tgoto err;\n\n\tdev_dbg(ccp->dev, \"%s - desc=%p\\n\", __func__, desc);\n\n\tspin_lock_irqsave(&chan->lock, sflags);\n\n\tlist_add_tail(&desc->entry, &chan->created);\n\n\tspin_unlock_irqrestore(&chan->lock, sflags);\n\n\treturn desc;\n\nerr:\n\tccp_free_cmd_resources(ccp, &desc->pending);\n\tkmem_cache_free(ccp->dma_desc_cache, desc);\n\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *ccp_prep_dma_memcpy(\n\tstruct dma_chan *dma_chan, dma_addr_t dst, dma_addr_t src, size_t len,\n\tunsigned long flags)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_dma_desc *desc;\n\tstruct scatterlist dst_sg, src_sg;\n\n\tdev_dbg(chan->ccp->dev,\n\t\t\"%s - src=%pad, dst=%pad, len=%zu, flags=%#lx\\n\",\n\t\t__func__, &src, &dst, len, flags);\n\n\tsg_init_table(&dst_sg, 1);\n\tsg_dma_address(&dst_sg) = dst;\n\tsg_dma_len(&dst_sg) = len;\n\n\tsg_init_table(&src_sg, 1);\n\tsg_dma_address(&src_sg) = src;\n\tsg_dma_len(&src_sg) = len;\n\n\tdesc = ccp_create_desc(dma_chan, &dst_sg, 1, &src_sg, 1, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\treturn &desc->tx_desc;\n}\n\nstatic struct dma_async_tx_descriptor *ccp_prep_dma_interrupt(\n\tstruct dma_chan *dma_chan, unsigned long flags)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_dma_desc *desc;\n\n\tdesc = ccp_alloc_dma_desc(chan, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\treturn &desc->tx_desc;\n}\n\nstatic void ccp_issue_pending(struct dma_chan *dma_chan)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_dma_desc *desc;\n\tunsigned long flags;\n\n\tdev_dbg(chan->ccp->dev, \"%s\\n\", __func__);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tdesc = __ccp_pending_to_active(chan);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\t \n\tif (desc)\n\t\tccp_cmd_callback(desc, 0);\n}\n\nstatic enum dma_status ccp_tx_status(struct dma_chan *dma_chan,\n\t\t\t\t     dma_cookie_t cookie,\n\t\t\t\t     struct dma_tx_state *state)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_dma_desc *desc;\n\tenum dma_status ret;\n\tunsigned long flags;\n\n\tif (chan->status == DMA_PAUSED) {\n\t\tret = DMA_PAUSED;\n\t\tgoto out;\n\t}\n\n\tret = dma_cookie_status(dma_chan, cookie, state);\n\tif (ret == DMA_COMPLETE) {\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\n\t\t \n\t\tlist_for_each_entry(desc, &chan->complete, entry) {\n\t\t\tif (desc->tx_desc.cookie != cookie)\n\t\t\t\tcontinue;\n\n\t\t\tret = desc->status;\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\t}\n\nout:\n\tdev_dbg(chan->ccp->dev, \"%s - %u\\n\", __func__, ret);\n\n\treturn ret;\n}\n\nstatic int ccp_pause(struct dma_chan *dma_chan)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\n\tchan->status = DMA_PAUSED;\n\n\t \n\n\treturn 0;\n}\n\nstatic int ccp_resume(struct dma_chan *dma_chan)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tstruct ccp_dma_desc *desc;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tdesc = list_first_entry_or_null(&chan->active, struct ccp_dma_desc,\n\t\t\t\t\tentry);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\t \n\tchan->status = DMA_IN_PROGRESS;\n\n\t \n\tif (desc)\n\t\tccp_cmd_callback(desc, 0);\n\n\treturn 0;\n}\n\nstatic int ccp_terminate_all(struct dma_chan *dma_chan)\n{\n\tstruct ccp_dma_chan *chan = container_of(dma_chan, struct ccp_dma_chan,\n\t\t\t\t\t\t dma_chan);\n\tunsigned long flags;\n\n\tdev_dbg(chan->ccp->dev, \"%s\\n\", __func__);\n\n\t \n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\t \n\tccp_free_desc_resources(chan->ccp, &chan->active);\n\tccp_free_desc_resources(chan->ccp, &chan->pending);\n\tccp_free_desc_resources(chan->ccp, &chan->created);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn 0;\n}\n\nstatic void ccp_dma_release(struct ccp_device *ccp)\n{\n\tstruct ccp_dma_chan *chan;\n\tstruct dma_chan *dma_chan;\n\tunsigned int i;\n\n\tfor (i = 0; i < ccp->cmd_q_count; i++) {\n\t\tchan = ccp->ccp_dma_chan + i;\n\t\tdma_chan = &chan->dma_chan;\n\n\t\ttasklet_kill(&chan->cleanup_tasklet);\n\t\tlist_del_rcu(&dma_chan->device_node);\n\t}\n}\n\nstatic void ccp_dma_release_channels(struct ccp_device *ccp)\n{\n\tstruct ccp_dma_chan *chan;\n\tstruct dma_chan *dma_chan;\n\tunsigned int i;\n\n\tfor (i = 0; i < ccp->cmd_q_count; i++) {\n\t\tchan = ccp->ccp_dma_chan + i;\n\t\tdma_chan = &chan->dma_chan;\n\n\t\tif (dma_chan->client_count)\n\t\t\tdma_release_channel(dma_chan);\n\t}\n}\n\nint ccp_dmaengine_register(struct ccp_device *ccp)\n{\n\tstruct ccp_dma_chan *chan;\n\tstruct dma_device *dma_dev = &ccp->dma_dev;\n\tstruct dma_chan *dma_chan;\n\tchar *dma_cmd_cache_name;\n\tchar *dma_desc_cache_name;\n\tunsigned int i;\n\tint ret;\n\n\tif (!dmaengine)\n\t\treturn 0;\n\n\tccp->ccp_dma_chan = devm_kcalloc(ccp->dev, ccp->cmd_q_count,\n\t\t\t\t\t sizeof(*(ccp->ccp_dma_chan)),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!ccp->ccp_dma_chan)\n\t\treturn -ENOMEM;\n\n\tdma_cmd_cache_name = devm_kasprintf(ccp->dev, GFP_KERNEL,\n\t\t\t\t\t    \"%s-dmaengine-cmd-cache\",\n\t\t\t\t\t    ccp->name);\n\tif (!dma_cmd_cache_name)\n\t\treturn -ENOMEM;\n\n\tccp->dma_cmd_cache = kmem_cache_create(dma_cmd_cache_name,\n\t\t\t\t\t       sizeof(struct ccp_dma_cmd),\n\t\t\t\t\t       sizeof(void *),\n\t\t\t\t\t       SLAB_HWCACHE_ALIGN, NULL);\n\tif (!ccp->dma_cmd_cache)\n\t\treturn -ENOMEM;\n\n\tdma_desc_cache_name = devm_kasprintf(ccp->dev, GFP_KERNEL,\n\t\t\t\t\t     \"%s-dmaengine-desc-cache\",\n\t\t\t\t\t     ccp->name);\n\tif (!dma_desc_cache_name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_cache;\n\t}\n\n\tccp->dma_desc_cache = kmem_cache_create(dma_desc_cache_name,\n\t\t\t\t\t\tsizeof(struct ccp_dma_desc),\n\t\t\t\t\t\tsizeof(void *),\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN, NULL);\n\tif (!ccp->dma_desc_cache) {\n\t\tret = -ENOMEM;\n\t\tgoto err_cache;\n\t}\n\n\tdma_dev->dev = ccp->dev;\n\tdma_dev->src_addr_widths = CCP_DMA_WIDTH(dma_get_mask(ccp->dev));\n\tdma_dev->dst_addr_widths = CCP_DMA_WIDTH(dma_get_mask(ccp->dev));\n\tdma_dev->directions = DMA_MEM_TO_MEM;\n\tdma_dev->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\tdma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\n\tdma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);\n\n\t \n\tif (ccp_get_dma_chan_attr(ccp) == DMA_PRIVATE)\n\t\tdma_cap_set(DMA_PRIVATE, dma_dev->cap_mask);\n\n\tINIT_LIST_HEAD(&dma_dev->channels);\n\tfor (i = 0; i < ccp->cmd_q_count; i++) {\n\t\tchan = ccp->ccp_dma_chan + i;\n\t\tdma_chan = &chan->dma_chan;\n\n\t\tchan->ccp = ccp;\n\n\t\tspin_lock_init(&chan->lock);\n\t\tINIT_LIST_HEAD(&chan->created);\n\t\tINIT_LIST_HEAD(&chan->pending);\n\t\tINIT_LIST_HEAD(&chan->active);\n\t\tINIT_LIST_HEAD(&chan->complete);\n\n\t\ttasklet_init(&chan->cleanup_tasklet, ccp_do_cleanup,\n\t\t\t     (unsigned long)chan);\n\n\t\tdma_chan->device = dma_dev;\n\t\tdma_cookie_init(dma_chan);\n\n\t\tlist_add_tail(&dma_chan->device_node, &dma_dev->channels);\n\t}\n\n\tdma_dev->device_free_chan_resources = ccp_free_chan_resources;\n\tdma_dev->device_prep_dma_memcpy = ccp_prep_dma_memcpy;\n\tdma_dev->device_prep_dma_interrupt = ccp_prep_dma_interrupt;\n\tdma_dev->device_issue_pending = ccp_issue_pending;\n\tdma_dev->device_tx_status = ccp_tx_status;\n\tdma_dev->device_pause = ccp_pause;\n\tdma_dev->device_resume = ccp_resume;\n\tdma_dev->device_terminate_all = ccp_terminate_all;\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret)\n\t\tgoto err_reg;\n\n\treturn 0;\n\nerr_reg:\n\tccp_dma_release(ccp);\n\tkmem_cache_destroy(ccp->dma_desc_cache);\n\nerr_cache:\n\tkmem_cache_destroy(ccp->dma_cmd_cache);\n\n\treturn ret;\n}\n\nvoid ccp_dmaengine_unregister(struct ccp_device *ccp)\n{\n\tstruct dma_device *dma_dev = &ccp->dma_dev;\n\n\tif (!dmaengine)\n\t\treturn;\n\n\tccp_dma_release_channels(ccp);\n\tdma_async_device_unregister(dma_dev);\n\tccp_dma_release(ccp);\n\n\tkmem_cache_destroy(ccp->dma_desc_cache);\n\tkmem_cache_destroy(ccp->dma_cmd_cache);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}