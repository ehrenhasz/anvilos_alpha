{
  "module_name": "ccp-crypto-main.c",
  "hash_id": "940b653085f061bff1168070711957885541c5a5a03d251e6e3d6a58a5c9ef1d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccp/ccp-crypto-main.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/ccp.h>\n#include <linux/scatterlist.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/akcipher.h>\n\n#include \"ccp-crypto.h\"\n\nMODULE_AUTHOR(\"Tom Lendacky <thomas.lendacky@amd.com>\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(\"1.0.0\");\nMODULE_DESCRIPTION(\"AMD Cryptographic Coprocessor crypto API support\");\n\nstatic unsigned int aes_disable;\nmodule_param(aes_disable, uint, 0444);\nMODULE_PARM_DESC(aes_disable, \"Disable use of AES - any non-zero value\");\n\nstatic unsigned int sha_disable;\nmodule_param(sha_disable, uint, 0444);\nMODULE_PARM_DESC(sha_disable, \"Disable use of SHA - any non-zero value\");\n\nstatic unsigned int des3_disable;\nmodule_param(des3_disable, uint, 0444);\nMODULE_PARM_DESC(des3_disable, \"Disable use of 3DES - any non-zero value\");\n\nstatic unsigned int rsa_disable;\nmodule_param(rsa_disable, uint, 0444);\nMODULE_PARM_DESC(rsa_disable, \"Disable use of RSA - any non-zero value\");\n\n \nstatic LIST_HEAD(hash_algs);\nstatic LIST_HEAD(skcipher_algs);\nstatic LIST_HEAD(aead_algs);\nstatic LIST_HEAD(akcipher_algs);\n\n \nstruct ccp_crypto_queue {\n\tstruct list_head cmds;\n\tstruct list_head *backlog;\n\tunsigned int cmd_count;\n};\n\n#define CCP_CRYPTO_MAX_QLEN\t100\n\nstatic struct ccp_crypto_queue req_queue;\nstatic DEFINE_SPINLOCK(req_queue_lock);\n\nstruct ccp_crypto_cmd {\n\tstruct list_head entry;\n\n\tstruct ccp_cmd *cmd;\n\n\t \n\tstruct crypto_async_request *req;\n\tstruct crypto_tfm *tfm;\n\n\t \n\tint ret;\n};\n\nstatic inline bool ccp_crypto_success(int err)\n{\n\tif (err && (err != -EINPROGRESS) && (err != -EBUSY))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct ccp_crypto_cmd *ccp_crypto_cmd_complete(\n\tstruct ccp_crypto_cmd *crypto_cmd, struct ccp_crypto_cmd **backlog)\n{\n\tstruct ccp_crypto_cmd *held = NULL, *tmp;\n\tunsigned long flags;\n\n\t*backlog = NULL;\n\n\tspin_lock_irqsave(&req_queue_lock, flags);\n\n\t \n\ttmp = crypto_cmd;\n\tlist_for_each_entry_continue(tmp, &req_queue.cmds, entry) {\n\t\tif (crypto_cmd->tfm != tmp->tfm)\n\t\t\tcontinue;\n\t\theld = tmp;\n\t\tbreak;\n\t}\n\n\t \n\tif (req_queue.backlog != &req_queue.cmds) {\n\t\t \n\t\tif (req_queue.backlog == &crypto_cmd->entry)\n\t\t\treq_queue.backlog = crypto_cmd->entry.next;\n\n\t\t*backlog = container_of(req_queue.backlog,\n\t\t\t\t\tstruct ccp_crypto_cmd, entry);\n\t\treq_queue.backlog = req_queue.backlog->next;\n\n\t\t \n\t\tif (req_queue.backlog == &crypto_cmd->entry)\n\t\t\treq_queue.backlog = crypto_cmd->entry.next;\n\t}\n\n\t \n\treq_queue.cmd_count--;\n\tlist_del(&crypto_cmd->entry);\n\n\tspin_unlock_irqrestore(&req_queue_lock, flags);\n\n\treturn held;\n}\n\nstatic void ccp_crypto_complete(void *data, int err)\n{\n\tstruct ccp_crypto_cmd *crypto_cmd = data;\n\tstruct ccp_crypto_cmd *held, *next, *backlog;\n\tstruct crypto_async_request *req = crypto_cmd->req;\n\tstruct ccp_ctx *ctx = crypto_tfm_ctx_dma(req->tfm);\n\tint ret;\n\n\tif (err == -EINPROGRESS) {\n\t\t \n\t\tif (crypto_cmd->ret == -EBUSY) {\n\t\t\tcrypto_cmd->ret = -EINPROGRESS;\n\t\t\tcrypto_request_complete(req, -EINPROGRESS);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t \n\theld = ccp_crypto_cmd_complete(crypto_cmd, &backlog);\n\tif (backlog) {\n\t\tbacklog->ret = -EINPROGRESS;\n\t\tcrypto_request_complete(backlog->req, -EINPROGRESS);\n\t}\n\n\t \n\tif (crypto_cmd->ret == -EBUSY)\n\t\tcrypto_request_complete(req, -EINPROGRESS);\n\n\t \n\tret = err;\n\tif (ctx->complete)\n\t\tret = ctx->complete(req, ret);\n\tcrypto_request_complete(req, ret);\n\n\t \n\twhile (held) {\n\t\t \n\t\theld->cmd->flags |= CCP_CMD_MAY_BACKLOG;\n\t\tret = ccp_enqueue_cmd(held->cmd);\n\t\tif (ccp_crypto_success(ret))\n\t\t\tbreak;\n\n\t\t \n\t\tctx = crypto_tfm_ctx_dma(held->req->tfm);\n\t\tif (ctx->complete)\n\t\t\tret = ctx->complete(held->req, ret);\n\t\tcrypto_request_complete(held->req, ret);\n\n\t\tnext = ccp_crypto_cmd_complete(held, &backlog);\n\t\tif (backlog) {\n\t\t\tbacklog->ret = -EINPROGRESS;\n\t\t\tcrypto_request_complete(backlog->req, -EINPROGRESS);\n\t\t}\n\n\t\tkfree(held);\n\t\theld = next;\n\t}\n\n\tkfree(crypto_cmd);\n}\n\nstatic int ccp_crypto_enqueue_cmd(struct ccp_crypto_cmd *crypto_cmd)\n{\n\tstruct ccp_crypto_cmd *active = NULL, *tmp;\n\tunsigned long flags;\n\tbool free_cmd = true;\n\tint ret;\n\n\tspin_lock_irqsave(&req_queue_lock, flags);\n\n\t \n\tif (req_queue.cmd_count >= CCP_CRYPTO_MAX_QLEN) {\n\t\tif (!(crypto_cmd->cmd->flags & CCP_CMD_MAY_BACKLOG)) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto e_lock;\n\t\t}\n\t}\n\n\t \n\tlist_for_each_entry(tmp, &req_queue.cmds, entry) {\n\t\tif (crypto_cmd->tfm != tmp->tfm)\n\t\t\tcontinue;\n\t\tactive = tmp;\n\t\tbreak;\n\t}\n\n\tret = -EINPROGRESS;\n\tif (!active) {\n\t\tret = ccp_enqueue_cmd(crypto_cmd->cmd);\n\t\tif (!ccp_crypto_success(ret))\n\t\t\tgoto e_lock;\t \n\t}\n\n\tif (req_queue.cmd_count >= CCP_CRYPTO_MAX_QLEN) {\n\t\tret = -EBUSY;\n\t\tif (req_queue.backlog == &req_queue.cmds)\n\t\t\treq_queue.backlog = &crypto_cmd->entry;\n\t}\n\tcrypto_cmd->ret = ret;\n\n\treq_queue.cmd_count++;\n\tlist_add_tail(&crypto_cmd->entry, &req_queue.cmds);\n\n\tfree_cmd = false;\n\ne_lock:\n\tspin_unlock_irqrestore(&req_queue_lock, flags);\n\n\tif (free_cmd)\n\t\tkfree(crypto_cmd);\n\n\treturn ret;\n}\n\n \nint ccp_crypto_enqueue_request(struct crypto_async_request *req,\n\t\t\t       struct ccp_cmd *cmd)\n{\n\tstruct ccp_crypto_cmd *crypto_cmd;\n\tgfp_t gfp;\n\n\tgfp = req->flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL : GFP_ATOMIC;\n\n\tcrypto_cmd = kzalloc(sizeof(*crypto_cmd), gfp);\n\tif (!crypto_cmd)\n\t\treturn -ENOMEM;\n\n\t \n\tcrypto_cmd->cmd = cmd;\n\tcrypto_cmd->req = req;\n\tcrypto_cmd->tfm = req->tfm;\n\n\tcmd->callback = ccp_crypto_complete;\n\tcmd->data = crypto_cmd;\n\n\tif (req->flags & CRYPTO_TFM_REQ_MAY_BACKLOG)\n\t\tcmd->flags |= CCP_CMD_MAY_BACKLOG;\n\telse\n\t\tcmd->flags &= ~CCP_CMD_MAY_BACKLOG;\n\n\treturn ccp_crypto_enqueue_cmd(crypto_cmd);\n}\n\nstruct scatterlist *ccp_crypto_sg_table_add(struct sg_table *table,\n\t\t\t\t\t    struct scatterlist *sg_add)\n{\n\tstruct scatterlist *sg, *sg_last = NULL;\n\n\tfor (sg = table->sgl; sg; sg = sg_next(sg))\n\t\tif (!sg_page(sg))\n\t\t\tbreak;\n\tif (WARN_ON(!sg))\n\t\treturn NULL;\n\n\tfor (; sg && sg_add; sg = sg_next(sg), sg_add = sg_next(sg_add)) {\n\t\tsg_set_page(sg, sg_page(sg_add), sg_add->length,\n\t\t\t    sg_add->offset);\n\t\tsg_last = sg;\n\t}\n\tif (WARN_ON(sg_add))\n\t\treturn NULL;\n\n\treturn sg_last;\n}\n\nstatic int ccp_register_algs(void)\n{\n\tint ret;\n\n\tif (!aes_disable) {\n\t\tret = ccp_register_aes_algs(&skcipher_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = ccp_register_aes_cmac_algs(&hash_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = ccp_register_aes_xts_algs(&skcipher_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = ccp_register_aes_aeads(&aead_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!des3_disable) {\n\t\tret = ccp_register_des3_algs(&skcipher_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!sha_disable) {\n\t\tret = ccp_register_sha_algs(&hash_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!rsa_disable) {\n\t\tret = ccp_register_rsa_algs(&akcipher_algs);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void ccp_unregister_algs(void)\n{\n\tstruct ccp_crypto_ahash_alg *ahash_alg, *ahash_tmp;\n\tstruct ccp_crypto_skcipher_alg *ablk_alg, *ablk_tmp;\n\tstruct ccp_crypto_aead *aead_alg, *aead_tmp;\n\tstruct ccp_crypto_akcipher_alg *akc_alg, *akc_tmp;\n\n\tlist_for_each_entry_safe(ahash_alg, ahash_tmp, &hash_algs, entry) {\n\t\tcrypto_unregister_ahash(&ahash_alg->alg);\n\t\tlist_del(&ahash_alg->entry);\n\t\tkfree(ahash_alg);\n\t}\n\n\tlist_for_each_entry_safe(ablk_alg, ablk_tmp, &skcipher_algs, entry) {\n\t\tcrypto_unregister_skcipher(&ablk_alg->alg);\n\t\tlist_del(&ablk_alg->entry);\n\t\tkfree(ablk_alg);\n\t}\n\n\tlist_for_each_entry_safe(aead_alg, aead_tmp, &aead_algs, entry) {\n\t\tcrypto_unregister_aead(&aead_alg->alg);\n\t\tlist_del(&aead_alg->entry);\n\t\tkfree(aead_alg);\n\t}\n\n\tlist_for_each_entry_safe(akc_alg, akc_tmp, &akcipher_algs, entry) {\n\t\tcrypto_unregister_akcipher(&akc_alg->alg);\n\t\tlist_del(&akc_alg->entry);\n\t\tkfree(akc_alg);\n\t}\n}\n\nstatic int __init ccp_crypto_init(void)\n{\n\tint ret;\n\n\tret = ccp_present();\n\tif (ret) {\n\t\tpr_err(\"Cannot load: there are no available CCPs\\n\");\n\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&req_queue.cmds);\n\treq_queue.backlog = &req_queue.cmds;\n\treq_queue.cmd_count = 0;\n\n\tret = ccp_register_algs();\n\tif (ret)\n\t\tccp_unregister_algs();\n\n\treturn ret;\n}\n\nstatic void __exit ccp_crypto_exit(void)\n{\n\tccp_unregister_algs();\n}\n\nmodule_init(ccp_crypto_init);\nmodule_exit(ccp_crypto_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}