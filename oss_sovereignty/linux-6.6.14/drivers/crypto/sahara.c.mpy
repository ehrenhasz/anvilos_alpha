{
  "module_name": "sahara.c",
  "hash_id": "35bcdae12102fa3b32707aceec92bfd191df8c975bbe0cef61a17cb030325dc5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/sahara.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/spinlock.h>\n\n#define SHA_BUFFER_LEN\t\tPAGE_SIZE\n#define SAHARA_MAX_SHA_BLOCK_SIZE\tSHA256_BLOCK_SIZE\n\n#define SAHARA_NAME \"sahara\"\n#define SAHARA_VERSION_3\t3\n#define SAHARA_VERSION_4\t4\n#define SAHARA_TIMEOUT_MS\t1000\n#define SAHARA_MAX_HW_DESC\t2\n#define SAHARA_MAX_HW_LINK\t20\n\n#define FLAGS_MODE_MASK\t\t0x000f\n#define FLAGS_ENCRYPT\t\tBIT(0)\n#define FLAGS_CBC\t\tBIT(1)\n\n#define SAHARA_HDR_BASE\t\t\t0x00800000\n#define SAHARA_HDR_SKHA_ALG_AES\t0\n#define SAHARA_HDR_SKHA_OP_ENC\t\t(1 << 2)\n#define SAHARA_HDR_SKHA_MODE_ECB\t(0 << 3)\n#define SAHARA_HDR_SKHA_MODE_CBC\t(1 << 3)\n#define SAHARA_HDR_FORM_DATA\t\t(5 << 16)\n#define SAHARA_HDR_FORM_KEY\t\t(8 << 16)\n#define SAHARA_HDR_LLO\t\t\t(1 << 24)\n#define SAHARA_HDR_CHA_SKHA\t\t(1 << 28)\n#define SAHARA_HDR_CHA_MDHA\t\t(2 << 28)\n#define SAHARA_HDR_PARITY_BIT\t\t(1 << 31)\n\n#define SAHARA_HDR_MDHA_SET_MODE_MD_KEY\t0x20880000\n#define SAHARA_HDR_MDHA_SET_MODE_HASH\t0x208D0000\n#define SAHARA_HDR_MDHA_HASH\t\t0xA0850000\n#define SAHARA_HDR_MDHA_STORE_DIGEST\t0x20820000\n#define SAHARA_HDR_MDHA_ALG_SHA1\t0\n#define SAHARA_HDR_MDHA_ALG_MD5\t\t1\n#define SAHARA_HDR_MDHA_ALG_SHA256\t2\n#define SAHARA_HDR_MDHA_ALG_SHA224\t3\n#define SAHARA_HDR_MDHA_PDATA\t\t(1 << 2)\n#define SAHARA_HDR_MDHA_HMAC\t\t(1 << 3)\n#define SAHARA_HDR_MDHA_INIT\t\t(1 << 5)\n#define SAHARA_HDR_MDHA_IPAD\t\t(1 << 6)\n#define SAHARA_HDR_MDHA_OPAD\t\t(1 << 7)\n#define SAHARA_HDR_MDHA_SWAP\t\t(1 << 8)\n#define SAHARA_HDR_MDHA_MAC_FULL\t(1 << 9)\n#define SAHARA_HDR_MDHA_SSL\t\t(1 << 10)\n\n \n#define SAHARA_QUEUE_LENGTH\t1\n\n#define SAHARA_REG_VERSION\t0x00\n#define SAHARA_REG_DAR\t\t0x04\n#define SAHARA_REG_CONTROL\t0x08\n#define\t\tSAHARA_CONTROL_SET_THROTTLE(x)\t(((x) & 0xff) << 24)\n#define\t\tSAHARA_CONTROL_SET_MAXBURST(x)\t(((x) & 0xff) << 16)\n#define\t\tSAHARA_CONTROL_RNG_AUTORSD\t(1 << 7)\n#define\t\tSAHARA_CONTROL_ENABLE_INT\t(1 << 4)\n#define SAHARA_REG_CMD\t\t0x0C\n#define\t\tSAHARA_CMD_RESET\t\t(1 << 0)\n#define\t\tSAHARA_CMD_CLEAR_INT\t\t(1 << 8)\n#define\t\tSAHARA_CMD_CLEAR_ERR\t\t(1 << 9)\n#define\t\tSAHARA_CMD_SINGLE_STEP\t\t(1 << 10)\n#define\t\tSAHARA_CMD_MODE_BATCH\t\t(1 << 16)\n#define\t\tSAHARA_CMD_MODE_DEBUG\t\t(1 << 18)\n#define\tSAHARA_REG_STATUS\t0x10\n#define\t\tSAHARA_STATUS_GET_STATE(x)\t((x) & 0x7)\n#define\t\t\tSAHARA_STATE_IDLE\t0\n#define\t\t\tSAHARA_STATE_BUSY\t1\n#define\t\t\tSAHARA_STATE_ERR\t2\n#define\t\t\tSAHARA_STATE_FAULT\t3\n#define\t\t\tSAHARA_STATE_COMPLETE\t4\n#define\t\t\tSAHARA_STATE_COMP_FLAG\t(1 << 2)\n#define\t\tSAHARA_STATUS_DAR_FULL\t\t(1 << 3)\n#define\t\tSAHARA_STATUS_ERROR\t\t(1 << 4)\n#define\t\tSAHARA_STATUS_SECURE\t\t(1 << 5)\n#define\t\tSAHARA_STATUS_FAIL\t\t(1 << 6)\n#define\t\tSAHARA_STATUS_INIT\t\t(1 << 7)\n#define\t\tSAHARA_STATUS_RNG_RESEED\t(1 << 8)\n#define\t\tSAHARA_STATUS_ACTIVE_RNG\t(1 << 9)\n#define\t\tSAHARA_STATUS_ACTIVE_MDHA\t(1 << 10)\n#define\t\tSAHARA_STATUS_ACTIVE_SKHA\t(1 << 11)\n#define\t\tSAHARA_STATUS_MODE_BATCH\t(1 << 16)\n#define\t\tSAHARA_STATUS_MODE_DEDICATED\t(1 << 17)\n#define\t\tSAHARA_STATUS_MODE_DEBUG\t(1 << 18)\n#define\t\tSAHARA_STATUS_GET_ISTATE(x)\t(((x) >> 24) & 0xff)\n#define SAHARA_REG_ERRSTATUS\t0x14\n#define\t\tSAHARA_ERRSTATUS_GET_SOURCE(x)\t((x) & 0xf)\n#define\t\t\tSAHARA_ERRSOURCE_CHA\t14\n#define\t\t\tSAHARA_ERRSOURCE_DMA\t15\n#define\t\tSAHARA_ERRSTATUS_DMA_DIR\t(1 << 8)\n#define\t\tSAHARA_ERRSTATUS_GET_DMASZ(x)(((x) >> 9) & 0x3)\n#define\t\tSAHARA_ERRSTATUS_GET_DMASRC(x) (((x) >> 13) & 0x7)\n#define\t\tSAHARA_ERRSTATUS_GET_CHASRC(x)\t(((x) >> 16) & 0xfff)\n#define\t\tSAHARA_ERRSTATUS_GET_CHAERR(x)\t(((x) >> 28) & 0x3)\n#define SAHARA_REG_FADDR\t0x18\n#define SAHARA_REG_CDAR\t\t0x1C\n#define SAHARA_REG_IDAR\t\t0x20\n\nstruct sahara_hw_desc {\n\tu32\thdr;\n\tu32\tlen1;\n\tu32\tp1;\n\tu32\tlen2;\n\tu32\tp2;\n\tu32\tnext;\n};\n\nstruct sahara_hw_link {\n\tu32\tlen;\n\tu32\tp;\n\tu32\tnext;\n};\n\nstruct sahara_ctx {\n\t \n\tint keylen;\n\tu8 key[AES_KEYSIZE_128];\n\tstruct crypto_skcipher *fallback;\n};\n\nstruct sahara_aes_reqctx {\n\tunsigned long mode;\n\tu8 iv_out[AES_BLOCK_SIZE];\n\tstruct skcipher_request fallback_req;\t\n};\n\n \nstruct sahara_sha_reqctx {\n\tu8\t\t\tbuf[SAHARA_MAX_SHA_BLOCK_SIZE];\n\tu8\t\t\trembuf[SAHARA_MAX_SHA_BLOCK_SIZE];\n\tu8\t\t\tcontext[SHA256_DIGEST_SIZE + 4];\n\tunsigned int\t\tmode;\n\tunsigned int\t\tdigest_size;\n\tunsigned int\t\tcontext_size;\n\tunsigned int\t\tbuf_cnt;\n\tunsigned int\t\tsg_in_idx;\n\tstruct scatterlist\t*in_sg;\n\tstruct scatterlist\tin_sg_chain[2];\n\tsize_t\t\t\ttotal;\n\tunsigned int\t\tlast;\n\tunsigned int\t\tfirst;\n\tunsigned int\t\tactive;\n};\n\nstruct sahara_dev {\n\tstruct device\t\t*device;\n\tunsigned int\t\tversion;\n\tvoid __iomem\t\t*regs_base;\n\tstruct clk\t\t*clk_ipg;\n\tstruct clk\t\t*clk_ahb;\n\tspinlock_t\t\tqueue_spinlock;\n\tstruct task_struct\t*kthread;\n\tstruct completion\tdma_completion;\n\n\tstruct sahara_ctx\t*ctx;\n\tstruct crypto_queue\tqueue;\n\tunsigned long\t\tflags;\n\n\tstruct sahara_hw_desc\t*hw_desc[SAHARA_MAX_HW_DESC];\n\tdma_addr_t\t\thw_phys_desc[SAHARA_MAX_HW_DESC];\n\n\tu8\t\t\t*key_base;\n\tdma_addr_t\t\tkey_phys_base;\n\n\tu8\t\t\t*iv_base;\n\tdma_addr_t\t\tiv_phys_base;\n\n\tu8\t\t\t*context_base;\n\tdma_addr_t\t\tcontext_phys_base;\n\n\tstruct sahara_hw_link\t*hw_link[SAHARA_MAX_HW_LINK];\n\tdma_addr_t\t\thw_phys_link[SAHARA_MAX_HW_LINK];\n\n\tsize_t\t\t\ttotal;\n\tstruct scatterlist\t*in_sg;\n\tint\t\tnb_in_sg;\n\tstruct scatterlist\t*out_sg;\n\tint\t\tnb_out_sg;\n\n\tu32\t\t\terror;\n};\n\nstatic struct sahara_dev *dev_ptr;\n\nstatic inline void sahara_write(struct sahara_dev *dev, u32 data, u32 reg)\n{\n\twritel(data, dev->regs_base + reg);\n}\n\nstatic inline unsigned int sahara_read(struct sahara_dev *dev, u32 reg)\n{\n\treturn readl(dev->regs_base + reg);\n}\n\nstatic u32 sahara_aes_key_hdr(struct sahara_dev *dev)\n{\n\tu32 hdr = SAHARA_HDR_BASE | SAHARA_HDR_SKHA_ALG_AES |\n\t\t\tSAHARA_HDR_FORM_KEY | SAHARA_HDR_LLO |\n\t\t\tSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\n\n\tif (dev->flags & FLAGS_CBC) {\n\t\thdr |= SAHARA_HDR_SKHA_MODE_CBC;\n\t\thdr ^= SAHARA_HDR_PARITY_BIT;\n\t}\n\n\tif (dev->flags & FLAGS_ENCRYPT) {\n\t\thdr |= SAHARA_HDR_SKHA_OP_ENC;\n\t\thdr ^= SAHARA_HDR_PARITY_BIT;\n\t}\n\n\treturn hdr;\n}\n\nstatic u32 sahara_aes_data_link_hdr(struct sahara_dev *dev)\n{\n\treturn SAHARA_HDR_BASE | SAHARA_HDR_FORM_DATA |\n\t\t\tSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\n}\n\nstatic const char *sahara_err_src[16] = {\n\t\"No error\",\n\t\"Header error\",\n\t\"Descriptor length error\",\n\t\"Descriptor length or pointer error\",\n\t\"Link length error\",\n\t\"Link pointer error\",\n\t\"Input buffer error\",\n\t\"Output buffer error\",\n\t\"Output buffer starvation\",\n\t\"Internal state fault\",\n\t\"General descriptor problem\",\n\t\"Reserved\",\n\t\"Descriptor address error\",\n\t\"Link address error\",\n\t\"CHA error\",\n\t\"DMA error\"\n};\n\nstatic const char *sahara_err_dmasize[4] = {\n\t\"Byte transfer\",\n\t\"Half-word transfer\",\n\t\"Word transfer\",\n\t\"Reserved\"\n};\n\nstatic const char *sahara_err_dmasrc[8] = {\n\t\"No error\",\n\t\"AHB bus error\",\n\t\"Internal IP bus error\",\n\t\"Parity error\",\n\t\"DMA crosses 256 byte boundary\",\n\t\"DMA is busy\",\n\t\"Reserved\",\n\t\"DMA HW error\"\n};\n\nstatic const char *sahara_cha_errsrc[12] = {\n\t\"Input buffer non-empty\",\n\t\"Illegal address\",\n\t\"Illegal mode\",\n\t\"Illegal data size\",\n\t\"Illegal key size\",\n\t\"Write during processing\",\n\t\"CTX read during processing\",\n\t\"HW error\",\n\t\"Input buffer disabled/underflow\",\n\t\"Output buffer disabled/overflow\",\n\t\"DES key parity error\",\n\t\"Reserved\"\n};\n\nstatic const char *sahara_cha_err[4] = { \"No error\", \"SKHA\", \"MDHA\", \"RNG\" };\n\nstatic void sahara_decode_error(struct sahara_dev *dev, unsigned int error)\n{\n\tu8 source = SAHARA_ERRSTATUS_GET_SOURCE(error);\n\tu16 chasrc = ffs(SAHARA_ERRSTATUS_GET_CHASRC(error));\n\n\tdev_err(dev->device, \"%s: Error Register = 0x%08x\\n\", __func__, error);\n\n\tdev_err(dev->device, \"\t- %s.\\n\", sahara_err_src[source]);\n\n\tif (source == SAHARA_ERRSOURCE_DMA) {\n\t\tif (error & SAHARA_ERRSTATUS_DMA_DIR)\n\t\t\tdev_err(dev->device, \"\t\t* DMA read.\\n\");\n\t\telse\n\t\t\tdev_err(dev->device, \"\t\t* DMA write.\\n\");\n\n\t\tdev_err(dev->device, \"\t\t* %s.\\n\",\n\t\t       sahara_err_dmasize[SAHARA_ERRSTATUS_GET_DMASZ(error)]);\n\t\tdev_err(dev->device, \"\t\t* %s.\\n\",\n\t\t       sahara_err_dmasrc[SAHARA_ERRSTATUS_GET_DMASRC(error)]);\n\t} else if (source == SAHARA_ERRSOURCE_CHA) {\n\t\tdev_err(dev->device, \"\t\t* %s.\\n\",\n\t\t\tsahara_cha_errsrc[chasrc]);\n\t\tdev_err(dev->device, \"\t\t* %s.\\n\",\n\t\t       sahara_cha_err[SAHARA_ERRSTATUS_GET_CHAERR(error)]);\n\t}\n\tdev_err(dev->device, \"\\n\");\n}\n\nstatic const char *sahara_state[4] = { \"Idle\", \"Busy\", \"Error\", \"HW Fault\" };\n\nstatic void sahara_decode_status(struct sahara_dev *dev, unsigned int status)\n{\n\tu8 state;\n\n\tif (!__is_defined(DEBUG))\n\t\treturn;\n\n\tstate = SAHARA_STATUS_GET_STATE(status);\n\n\tdev_dbg(dev->device, \"%s: Status Register = 0x%08x\\n\",\n\t\t__func__, status);\n\n\tdev_dbg(dev->device, \"\t- State = %d:\\n\", state);\n\tif (state & SAHARA_STATE_COMP_FLAG)\n\t\tdev_dbg(dev->device, \"\t\t* Descriptor completed. IRQ pending.\\n\");\n\n\tdev_dbg(dev->device, \"\t\t* %s.\\n\",\n\t       sahara_state[state & ~SAHARA_STATE_COMP_FLAG]);\n\n\tif (status & SAHARA_STATUS_DAR_FULL)\n\t\tdev_dbg(dev->device, \"\t- DAR Full.\\n\");\n\tif (status & SAHARA_STATUS_ERROR)\n\t\tdev_dbg(dev->device, \"\t- Error.\\n\");\n\tif (status & SAHARA_STATUS_SECURE)\n\t\tdev_dbg(dev->device, \"\t- Secure.\\n\");\n\tif (status & SAHARA_STATUS_FAIL)\n\t\tdev_dbg(dev->device, \"\t- Fail.\\n\");\n\tif (status & SAHARA_STATUS_RNG_RESEED)\n\t\tdev_dbg(dev->device, \"\t- RNG Reseed Request.\\n\");\n\tif (status & SAHARA_STATUS_ACTIVE_RNG)\n\t\tdev_dbg(dev->device, \"\t- RNG Active.\\n\");\n\tif (status & SAHARA_STATUS_ACTIVE_MDHA)\n\t\tdev_dbg(dev->device, \"\t- MDHA Active.\\n\");\n\tif (status & SAHARA_STATUS_ACTIVE_SKHA)\n\t\tdev_dbg(dev->device, \"\t- SKHA Active.\\n\");\n\n\tif (status & SAHARA_STATUS_MODE_BATCH)\n\t\tdev_dbg(dev->device, \"\t- Batch Mode.\\n\");\n\telse if (status & SAHARA_STATUS_MODE_DEDICATED)\n\t\tdev_dbg(dev->device, \"\t- Dedicated Mode.\\n\");\n\telse if (status & SAHARA_STATUS_MODE_DEBUG)\n\t\tdev_dbg(dev->device, \"\t- Debug Mode.\\n\");\n\n\tdev_dbg(dev->device, \"\t- Internal state = 0x%02x\\n\",\n\t       SAHARA_STATUS_GET_ISTATE(status));\n\n\tdev_dbg(dev->device, \"Current DAR: 0x%08x\\n\",\n\t\tsahara_read(dev, SAHARA_REG_CDAR));\n\tdev_dbg(dev->device, \"Initial DAR: 0x%08x\\n\\n\",\n\t\tsahara_read(dev, SAHARA_REG_IDAR));\n}\n\nstatic void sahara_dump_descriptors(struct sahara_dev *dev)\n{\n\tint i;\n\n\tif (!__is_defined(DEBUG))\n\t\treturn;\n\n\tfor (i = 0; i < SAHARA_MAX_HW_DESC; i++) {\n\t\tdev_dbg(dev->device, \"Descriptor (%d) (%pad):\\n\",\n\t\t\ti, &dev->hw_phys_desc[i]);\n\t\tdev_dbg(dev->device, \"\\thdr = 0x%08x\\n\", dev->hw_desc[i]->hdr);\n\t\tdev_dbg(dev->device, \"\\tlen1 = %u\\n\", dev->hw_desc[i]->len1);\n\t\tdev_dbg(dev->device, \"\\tp1 = 0x%08x\\n\", dev->hw_desc[i]->p1);\n\t\tdev_dbg(dev->device, \"\\tlen2 = %u\\n\", dev->hw_desc[i]->len2);\n\t\tdev_dbg(dev->device, \"\\tp2 = 0x%08x\\n\", dev->hw_desc[i]->p2);\n\t\tdev_dbg(dev->device, \"\\tnext = 0x%08x\\n\",\n\t\t\tdev->hw_desc[i]->next);\n\t}\n\tdev_dbg(dev->device, \"\\n\");\n}\n\nstatic void sahara_dump_links(struct sahara_dev *dev)\n{\n\tint i;\n\n\tif (!__is_defined(DEBUG))\n\t\treturn;\n\n\tfor (i = 0; i < SAHARA_MAX_HW_LINK; i++) {\n\t\tdev_dbg(dev->device, \"Link (%d) (%pad):\\n\",\n\t\t\ti, &dev->hw_phys_link[i]);\n\t\tdev_dbg(dev->device, \"\\tlen = %u\\n\", dev->hw_link[i]->len);\n\t\tdev_dbg(dev->device, \"\\tp = 0x%08x\\n\", dev->hw_link[i]->p);\n\t\tdev_dbg(dev->device, \"\\tnext = 0x%08x\\n\",\n\t\t\tdev->hw_link[i]->next);\n\t}\n\tdev_dbg(dev->device, \"\\n\");\n}\n\nstatic int sahara_hw_descriptor_create(struct sahara_dev *dev)\n{\n\tstruct sahara_ctx *ctx = dev->ctx;\n\tstruct scatterlist *sg;\n\tint ret;\n\tint i, j;\n\tint idx = 0;\n\tu32 len;\n\n\tmemcpy(dev->key_base, ctx->key, ctx->keylen);\n\n\tif (dev->flags & FLAGS_CBC) {\n\t\tdev->hw_desc[idx]->len1 = AES_BLOCK_SIZE;\n\t\tdev->hw_desc[idx]->p1 = dev->iv_phys_base;\n\t} else {\n\t\tdev->hw_desc[idx]->len1 = 0;\n\t\tdev->hw_desc[idx]->p1 = 0;\n\t}\n\tdev->hw_desc[idx]->len2 = ctx->keylen;\n\tdev->hw_desc[idx]->p2 = dev->key_phys_base;\n\tdev->hw_desc[idx]->next = dev->hw_phys_desc[1];\n\tdev->hw_desc[idx]->hdr = sahara_aes_key_hdr(dev);\n\n\tidx++;\n\n\n\tdev->nb_in_sg = sg_nents_for_len(dev->in_sg, dev->total);\n\tif (dev->nb_in_sg < 0) {\n\t\tdev_err(dev->device, \"Invalid numbers of src SG.\\n\");\n\t\treturn dev->nb_in_sg;\n\t}\n\tdev->nb_out_sg = sg_nents_for_len(dev->out_sg, dev->total);\n\tif (dev->nb_out_sg < 0) {\n\t\tdev_err(dev->device, \"Invalid numbers of dst SG.\\n\");\n\t\treturn dev->nb_out_sg;\n\t}\n\tif ((dev->nb_in_sg + dev->nb_out_sg) > SAHARA_MAX_HW_LINK) {\n\t\tdev_err(dev->device, \"not enough hw links (%d)\\n\",\n\t\t\tdev->nb_in_sg + dev->nb_out_sg);\n\t\treturn -EINVAL;\n\t}\n\n\tret = dma_map_sg(dev->device, dev->in_sg, dev->nb_in_sg,\n\t\t\t DMA_TO_DEVICE);\n\tif (!ret) {\n\t\tdev_err(dev->device, \"couldn't map in sg\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = dma_map_sg(dev->device, dev->out_sg, dev->nb_out_sg,\n\t\t\t DMA_FROM_DEVICE);\n\tif (!ret) {\n\t\tdev_err(dev->device, \"couldn't map out sg\\n\");\n\t\tgoto unmap_in;\n\t}\n\n\t \n\tdev->hw_desc[idx]->p1 = dev->hw_phys_link[0];\n\tsg = dev->in_sg;\n\tlen = dev->total;\n\tfor (i = 0; i < dev->nb_in_sg; i++) {\n\t\tdev->hw_link[i]->len = min(len, sg->length);\n\t\tdev->hw_link[i]->p = sg->dma_address;\n\t\tif (i == (dev->nb_in_sg - 1)) {\n\t\t\tdev->hw_link[i]->next = 0;\n\t\t} else {\n\t\t\tlen -= min(len, sg->length);\n\t\t\tdev->hw_link[i]->next = dev->hw_phys_link[i + 1];\n\t\t\tsg = sg_next(sg);\n\t\t}\n\t}\n\n\t \n\tdev->hw_desc[idx]->p2 = dev->hw_phys_link[i];\n\tsg = dev->out_sg;\n\tlen = dev->total;\n\tfor (j = i; j < dev->nb_out_sg + i; j++) {\n\t\tdev->hw_link[j]->len = min(len, sg->length);\n\t\tdev->hw_link[j]->p = sg->dma_address;\n\t\tif (j == (dev->nb_out_sg + i - 1)) {\n\t\t\tdev->hw_link[j]->next = 0;\n\t\t} else {\n\t\t\tlen -= min(len, sg->length);\n\t\t\tdev->hw_link[j]->next = dev->hw_phys_link[j + 1];\n\t\t\tsg = sg_next(sg);\n\t\t}\n\t}\n\n\t \n\tdev->hw_desc[idx]->hdr = sahara_aes_data_link_hdr(dev);\n\tdev->hw_desc[idx]->len1 = dev->total;\n\tdev->hw_desc[idx]->len2 = dev->total;\n\tdev->hw_desc[idx]->next = 0;\n\n\tsahara_dump_descriptors(dev);\n\tsahara_dump_links(dev);\n\n\tsahara_write(dev, dev->hw_phys_desc[0], SAHARA_REG_DAR);\n\n\treturn 0;\n\nunmap_in:\n\tdma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\n\t\tDMA_TO_DEVICE);\n\n\treturn -EINVAL;\n}\n\nstatic void sahara_aes_cbc_update_iv(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct sahara_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(skcipher);\n\n\t \n\tif (rctx->mode & FLAGS_ENCRYPT) {\n\t\tsg_pcopy_to_buffer(req->dst, sg_nents(req->dst), req->iv,\n\t\t\t\t   ivsize, req->cryptlen - ivsize);\n\t} else {\n\t\tmemcpy(req->iv, rctx->iv_out, ivsize);\n\t}\n}\n\nstatic int sahara_aes_process(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct sahara_dev *dev = dev_ptr;\n\tstruct sahara_ctx *ctx;\n\tstruct sahara_aes_reqctx *rctx;\n\tint ret;\n\tunsigned long timeout;\n\n\t \n\tdev_dbg(dev->device,\n\t\t\"dispatch request (nbytes=%d, src=%p, dst=%p)\\n\",\n\t\treq->cryptlen, req->src, req->dst);\n\n\t \n\tdev->total = req->cryptlen;\n\tdev->in_sg = req->src;\n\tdev->out_sg = req->dst;\n\n\trctx = skcipher_request_ctx(req);\n\tctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));\n\trctx->mode &= FLAGS_MODE_MASK;\n\tdev->flags = (dev->flags & ~FLAGS_MODE_MASK) | rctx->mode;\n\n\tif ((dev->flags & FLAGS_CBC) && req->iv) {\n\t\tunsigned int ivsize = crypto_skcipher_ivsize(skcipher);\n\n\t\tmemcpy(dev->iv_base, req->iv, ivsize);\n\n\t\tif (!(dev->flags & FLAGS_ENCRYPT)) {\n\t\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src),\n\t\t\t\t\t   rctx->iv_out, ivsize,\n\t\t\t\t\t   req->cryptlen - ivsize);\n\t\t}\n\t}\n\n\t \n\tdev->ctx = ctx;\n\n\treinit_completion(&dev->dma_completion);\n\n\tret = sahara_hw_descriptor_create(dev);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\ttimeout = wait_for_completion_timeout(&dev->dma_completion,\n\t\t\t\tmsecs_to_jiffies(SAHARA_TIMEOUT_MS));\n\n\tdma_unmap_sg(dev->device, dev->out_sg, dev->nb_out_sg,\n\t\tDMA_FROM_DEVICE);\n\tdma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\n\t\tDMA_TO_DEVICE);\n\n\tif (!timeout) {\n\t\tdev_err(dev->device, \"AES timeout\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tif ((dev->flags & FLAGS_CBC) && req->iv)\n\t\tsahara_aes_cbc_update_iv(req);\n\n\treturn 0;\n}\n\nstatic int sahara_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct sahara_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tctx->keylen = keylen;\n\n\t \n\tif (keylen == AES_KEYSIZE_128) {\n\t\tmemcpy(ctx->key, key, keylen);\n\t\treturn 0;\n\t}\n\n\tif (keylen != AES_KEYSIZE_192 && keylen != AES_KEYSIZE_256)\n\t\treturn -EINVAL;\n\n\t \n\tcrypto_skcipher_clear_flags(ctx->fallback, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(ctx->fallback, tfm->base.crt_flags &\n\t\t\t\t\t\t CRYPTO_TFM_REQ_MASK);\n\treturn crypto_skcipher_setkey(ctx->fallback, key, keylen);\n}\n\nstatic int sahara_aes_fallback(struct skcipher_request *req, unsigned long mode)\n{\n\tstruct sahara_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct sahara_ctx *ctx = crypto_skcipher_ctx(\n\t\tcrypto_skcipher_reqtfm(req));\n\n\tskcipher_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\tskcipher_request_set_callback(&rctx->fallback_req,\n\t\t\t\t      req->base.flags,\n\t\t\t\t      req->base.complete,\n\t\t\t\t      req->base.data);\n\tskcipher_request_set_crypt(&rctx->fallback_req, req->src,\n\t\t\t\t   req->dst, req->cryptlen, req->iv);\n\n\tif (mode & FLAGS_ENCRYPT)\n\t\treturn crypto_skcipher_encrypt(&rctx->fallback_req);\n\n\treturn crypto_skcipher_decrypt(&rctx->fallback_req);\n}\n\nstatic int sahara_aes_crypt(struct skcipher_request *req, unsigned long mode)\n{\n\tstruct sahara_aes_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct sahara_ctx *ctx = crypto_skcipher_ctx(\n\t\tcrypto_skcipher_reqtfm(req));\n\tstruct sahara_dev *dev = dev_ptr;\n\tint err = 0;\n\n\tif (!req->cryptlen)\n\t\treturn 0;\n\n\tif (unlikely(ctx->keylen != AES_KEYSIZE_128))\n\t\treturn sahara_aes_fallback(req, mode);\n\n\tdev_dbg(dev->device, \"nbytes: %d, enc: %d, cbc: %d\\n\",\n\t\treq->cryptlen, !!(mode & FLAGS_ENCRYPT), !!(mode & FLAGS_CBC));\n\n\tif (!IS_ALIGNED(req->cryptlen, AES_BLOCK_SIZE)) {\n\t\tdev_err(dev->device,\n\t\t\t\"request size is not exact amount of AES blocks\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trctx->mode = mode;\n\n\tspin_lock_bh(&dev->queue_spinlock);\n\terr = crypto_enqueue_request(&dev->queue, &req->base);\n\tspin_unlock_bh(&dev->queue_spinlock);\n\n\twake_up_process(dev->kthread);\n\n\treturn err;\n}\n\nstatic int sahara_aes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn sahara_aes_crypt(req, FLAGS_ENCRYPT);\n}\n\nstatic int sahara_aes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn sahara_aes_crypt(req, 0);\n}\n\nstatic int sahara_aes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn sahara_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\n}\n\nstatic int sahara_aes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn sahara_aes_crypt(req, FLAGS_CBC);\n}\n\nstatic int sahara_aes_init_tfm(struct crypto_skcipher *tfm)\n{\n\tconst char *name = crypto_tfm_alg_name(&tfm->base);\n\tstruct sahara_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tctx->fallback = crypto_alloc_skcipher(name, 0,\n\t\t\t\t\t      CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fallback)) {\n\t\tpr_err(\"Error allocating fallback algo %s\\n\", name);\n\t\treturn PTR_ERR(ctx->fallback);\n\t}\n\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct sahara_aes_reqctx) +\n\t\t\t\t\t crypto_skcipher_reqsize(ctx->fallback));\n\n\treturn 0;\n}\n\nstatic void sahara_aes_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct sahara_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tcrypto_free_skcipher(ctx->fallback);\n}\n\nstatic u32 sahara_sha_init_hdr(struct sahara_dev *dev,\n\t\t\t      struct sahara_sha_reqctx *rctx)\n{\n\tu32 hdr = 0;\n\n\thdr = rctx->mode;\n\n\tif (rctx->first) {\n\t\thdr |= SAHARA_HDR_MDHA_SET_MODE_HASH;\n\t\thdr |= SAHARA_HDR_MDHA_INIT;\n\t} else {\n\t\thdr |= SAHARA_HDR_MDHA_SET_MODE_MD_KEY;\n\t}\n\n\tif (rctx->last)\n\t\thdr |= SAHARA_HDR_MDHA_PDATA;\n\n\tif (hweight_long(hdr) % 2 == 0)\n\t\thdr |= SAHARA_HDR_PARITY_BIT;\n\n\treturn hdr;\n}\n\nstatic int sahara_sha_hw_links_create(struct sahara_dev *dev,\n\t\t\t\t       struct sahara_sha_reqctx *rctx,\n\t\t\t\t       int start)\n{\n\tstruct scatterlist *sg;\n\tunsigned int len;\n\tunsigned int i;\n\tint ret;\n\n\tdev->in_sg = rctx->in_sg;\n\n\tdev->nb_in_sg = sg_nents_for_len(dev->in_sg, rctx->total);\n\tif (dev->nb_in_sg < 0) {\n\t\tdev_err(dev->device, \"Invalid numbers of src SG.\\n\");\n\t\treturn dev->nb_in_sg;\n\t}\n\tif ((dev->nb_in_sg) > SAHARA_MAX_HW_LINK) {\n\t\tdev_err(dev->device, \"not enough hw links (%d)\\n\",\n\t\t\tdev->nb_in_sg + dev->nb_out_sg);\n\t\treturn -EINVAL;\n\t}\n\n\tsg = dev->in_sg;\n\tret = dma_map_sg(dev->device, dev->in_sg, dev->nb_in_sg, DMA_TO_DEVICE);\n\tif (!ret)\n\t\treturn -EFAULT;\n\n\tlen = rctx->total;\n\tfor (i = start; i < dev->nb_in_sg + start; i++) {\n\t\tdev->hw_link[i]->len = min(len, sg->length);\n\t\tdev->hw_link[i]->p = sg->dma_address;\n\t\tif (i == (dev->nb_in_sg + start - 1)) {\n\t\t\tdev->hw_link[i]->next = 0;\n\t\t} else {\n\t\t\tlen -= min(len, sg->length);\n\t\t\tdev->hw_link[i]->next = dev->hw_phys_link[i + 1];\n\t\t\tsg = sg_next(sg);\n\t\t}\n\t}\n\n\treturn i;\n}\n\nstatic int sahara_sha_hw_data_descriptor_create(struct sahara_dev *dev,\n\t\t\t\t\t\tstruct sahara_sha_reqctx *rctx,\n\t\t\t\t\t\tstruct ahash_request *req,\n\t\t\t\t\t\tint index)\n{\n\tunsigned result_len;\n\tint i = index;\n\n\tif (rctx->first)\n\t\t \n\t\tdev->hw_desc[index]->hdr = sahara_sha_init_hdr(dev, rctx);\n\telse\n\t\t \n\t\tdev->hw_desc[index]->hdr = SAHARA_HDR_MDHA_HASH;\n\n\tdev->hw_desc[index]->len1 = rctx->total;\n\tif (dev->hw_desc[index]->len1 == 0) {\n\t\t \n\t\tdev->hw_desc[index]->p1 = 0;\n\t\trctx->sg_in_idx = 0;\n\t} else {\n\t\t \n\t\tdev->hw_desc[index]->p1 = dev->hw_phys_link[index];\n\t\ti = sahara_sha_hw_links_create(dev, rctx, index);\n\n\t\trctx->sg_in_idx = index;\n\t\tif (i < 0)\n\t\t\treturn i;\n\t}\n\n\tdev->hw_desc[index]->p2 = dev->hw_phys_link[i];\n\n\t \n\tresult_len = rctx->context_size;\n\tdev->hw_link[i]->p = dev->context_phys_base;\n\n\tdev->hw_link[i]->len = result_len;\n\tdev->hw_desc[index]->len2 = result_len;\n\n\tdev->hw_link[i]->next = 0;\n\n\treturn 0;\n}\n\n \nstatic int sahara_sha_hw_context_descriptor_create(struct sahara_dev *dev,\n\t\t\t\t\t\tstruct sahara_sha_reqctx *rctx,\n\t\t\t\t\t\tstruct ahash_request *req,\n\t\t\t\t\t\tint index)\n{\n\tdev->hw_desc[index]->hdr = sahara_sha_init_hdr(dev, rctx);\n\n\tdev->hw_desc[index]->len1 = rctx->context_size;\n\tdev->hw_desc[index]->p1 = dev->hw_phys_link[index];\n\tdev->hw_desc[index]->len2 = 0;\n\tdev->hw_desc[index]->p2 = 0;\n\n\tdev->hw_link[index]->len = rctx->context_size;\n\tdev->hw_link[index]->p = dev->context_phys_base;\n\tdev->hw_link[index]->next = 0;\n\n\treturn 0;\n}\n\nstatic int sahara_sha_prepare_request(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\tunsigned int hash_later;\n\tunsigned int block_size;\n\tunsigned int len;\n\n\tblock_size = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\n\n\t \n\tlen = rctx->buf_cnt + req->nbytes;\n\n\t \n\tif (!rctx->last && (len < block_size)) {\n\t\t \n\t\tscatterwalk_map_and_copy(rctx->buf + rctx->buf_cnt, req->src,\n\t\t\t\t\t 0, req->nbytes, 0);\n\t\trctx->buf_cnt += req->nbytes;\n\n\t\treturn 0;\n\t}\n\n\t \n\tif (rctx->buf_cnt)\n\t\tmemcpy(rctx->rembuf, rctx->buf, rctx->buf_cnt);\n\n\t \n\thash_later = rctx->last ? 0 : len & (block_size - 1);\n\tif (hash_later) {\n\t\tunsigned int offset = req->nbytes - hash_later;\n\t\t \n\t\tscatterwalk_map_and_copy(rctx->buf, req->src, offset,\n\t\t\t\t\thash_later, 0);\n\t}\n\n\trctx->total = len - hash_later;\n\t \n\tif (rctx->buf_cnt && req->nbytes) {\n\t\tsg_init_table(rctx->in_sg_chain, 2);\n\t\tsg_set_buf(rctx->in_sg_chain, rctx->rembuf, rctx->buf_cnt);\n\t\tsg_chain(rctx->in_sg_chain, 2, req->src);\n\t\trctx->in_sg = rctx->in_sg_chain;\n\t \n\t} else if (rctx->buf_cnt) {\n\t\trctx->in_sg = rctx->in_sg_chain;\n\t\tsg_init_one(rctx->in_sg, rctx->rembuf, rctx->buf_cnt);\n\t \n\t} else {\n\t\trctx->in_sg = req->src;\n\t}\n\n\t \n\trctx->buf_cnt = hash_later;\n\n\treturn -EINPROGRESS;\n}\n\nstatic int sahara_sha_process(struct ahash_request *req)\n{\n\tstruct sahara_dev *dev = dev_ptr;\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\tint ret;\n\tunsigned long timeout;\n\n\tret = sahara_sha_prepare_request(req);\n\tif (!ret)\n\t\treturn ret;\n\n\tif (rctx->first) {\n\t\tret = sahara_sha_hw_data_descriptor_create(dev, rctx, req, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tdev->hw_desc[0]->next = 0;\n\t\trctx->first = 0;\n\t} else {\n\t\tmemcpy(dev->context_base, rctx->context, rctx->context_size);\n\n\t\tsahara_sha_hw_context_descriptor_create(dev, rctx, req, 0);\n\t\tdev->hw_desc[0]->next = dev->hw_phys_desc[1];\n\t\tret = sahara_sha_hw_data_descriptor_create(dev, rctx, req, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tdev->hw_desc[1]->next = 0;\n\t}\n\n\tsahara_dump_descriptors(dev);\n\tsahara_dump_links(dev);\n\n\treinit_completion(&dev->dma_completion);\n\n\tsahara_write(dev, dev->hw_phys_desc[0], SAHARA_REG_DAR);\n\n\ttimeout = wait_for_completion_timeout(&dev->dma_completion,\n\t\t\t\tmsecs_to_jiffies(SAHARA_TIMEOUT_MS));\n\n\tif (rctx->sg_in_idx)\n\t\tdma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\n\t\t\t     DMA_TO_DEVICE);\n\n\tif (!timeout) {\n\t\tdev_err(dev->device, \"SHA timeout\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tmemcpy(rctx->context, dev->context_base, rctx->context_size);\n\n\tif (req->result && rctx->last)\n\t\tmemcpy(req->result, rctx->context, rctx->digest_size);\n\n\treturn 0;\n}\n\nstatic int sahara_queue_manage(void *data)\n{\n\tstruct sahara_dev *dev = data;\n\tstruct crypto_async_request *async_req;\n\tstruct crypto_async_request *backlog;\n\tint ret = 0;\n\n\tdo {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_lock_bh(&dev->queue_spinlock);\n\t\tbacklog = crypto_get_backlog(&dev->queue);\n\t\tasync_req = crypto_dequeue_request(&dev->queue);\n\t\tspin_unlock_bh(&dev->queue_spinlock);\n\n\t\tif (backlog)\n\t\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\t\tif (async_req) {\n\t\t\tif (crypto_tfm_alg_type(async_req->tfm) ==\n\t\t\t    CRYPTO_ALG_TYPE_AHASH) {\n\t\t\t\tstruct ahash_request *req =\n\t\t\t\t\tahash_request_cast(async_req);\n\n\t\t\t\tret = sahara_sha_process(req);\n\t\t\t} else {\n\t\t\t\tstruct skcipher_request *req =\n\t\t\t\t\tskcipher_request_cast(async_req);\n\n\t\t\t\tret = sahara_aes_process(req);\n\t\t\t}\n\n\t\t\tcrypto_request_complete(async_req, ret);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tschedule();\n\t} while (!kthread_should_stop());\n\n\treturn 0;\n}\n\nstatic int sahara_sha_enqueue(struct ahash_request *req, int last)\n{\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\tstruct sahara_dev *dev = dev_ptr;\n\tint ret;\n\n\tif (!req->nbytes && !last)\n\t\treturn 0;\n\n\trctx->last = last;\n\n\tif (!rctx->active) {\n\t\trctx->active = 1;\n\t\trctx->first = 1;\n\t}\n\n\tspin_lock_bh(&dev->queue_spinlock);\n\tret = crypto_enqueue_request(&dev->queue, &req->base);\n\tspin_unlock_bh(&dev->queue_spinlock);\n\n\twake_up_process(dev->kthread);\n\n\treturn ret;\n}\n\nstatic int sahara_sha_init(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\n\tmemset(rctx, 0, sizeof(*rctx));\n\n\tswitch (crypto_ahash_digestsize(tfm)) {\n\tcase SHA1_DIGEST_SIZE:\n\t\trctx->mode |= SAHARA_HDR_MDHA_ALG_SHA1;\n\t\trctx->digest_size = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\trctx->mode |= SAHARA_HDR_MDHA_ALG_SHA256;\n\t\trctx->digest_size = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\trctx->context_size = rctx->digest_size + 4;\n\trctx->active = 0;\n\n\treturn 0;\n}\n\nstatic int sahara_sha_update(struct ahash_request *req)\n{\n\treturn sahara_sha_enqueue(req, 0);\n}\n\nstatic int sahara_sha_final(struct ahash_request *req)\n{\n\treq->nbytes = 0;\n\treturn sahara_sha_enqueue(req, 1);\n}\n\nstatic int sahara_sha_finup(struct ahash_request *req)\n{\n\treturn sahara_sha_enqueue(req, 1);\n}\n\nstatic int sahara_sha_digest(struct ahash_request *req)\n{\n\tsahara_sha_init(req);\n\n\treturn sahara_sha_finup(req);\n}\n\nstatic int sahara_sha_export(struct ahash_request *req, void *out)\n{\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\n\tmemcpy(out, rctx, sizeof(struct sahara_sha_reqctx));\n\n\treturn 0;\n}\n\nstatic int sahara_sha_import(struct ahash_request *req, const void *in)\n{\n\tstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\n\n\tmemcpy(rctx, in, sizeof(struct sahara_sha_reqctx));\n\n\treturn 0;\n}\n\nstatic int sahara_sha_cra_init(struct crypto_tfm *tfm)\n{\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct sahara_sha_reqctx));\n\n\treturn 0;\n}\n\nstatic struct skcipher_alg aes_algs[] = {\n{\n\t.base.cra_name\t\t= \"ecb(aes)\",\n\t.base.cra_driver_name\t= \"sahara-ecb-aes\",\n\t.base.cra_priority\t= 300,\n\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct sahara_ctx),\n\t.base.cra_alignmask\t= 0x0,\n\t.base.cra_module\t= THIS_MODULE,\n\n\t.init\t\t\t= sahara_aes_init_tfm,\n\t.exit\t\t\t= sahara_aes_exit_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE ,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.setkey\t\t\t= sahara_aes_setkey,\n\t.encrypt\t\t= sahara_aes_ecb_encrypt,\n\t.decrypt\t\t= sahara_aes_ecb_decrypt,\n}, {\n\t.base.cra_name\t\t= \"cbc(aes)\",\n\t.base.cra_driver_name\t= \"sahara-cbc-aes\",\n\t.base.cra_priority\t= 300,\n\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t= sizeof(struct sahara_ctx),\n\t.base.cra_alignmask\t= 0x0,\n\t.base.cra_module\t= THIS_MODULE,\n\n\t.init\t\t\t= sahara_aes_init_tfm,\n\t.exit\t\t\t= sahara_aes_exit_tfm,\n\t.min_keysize\t\t= AES_MIN_KEY_SIZE ,\n\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t.setkey\t\t\t= sahara_aes_setkey,\n\t.encrypt\t\t= sahara_aes_cbc_encrypt,\n\t.decrypt\t\t= sahara_aes_cbc_decrypt,\n}\n};\n\nstatic struct ahash_alg sha_v3_algs[] = {\n{\n\t.init\t\t= sahara_sha_init,\n\t.update\t\t= sahara_sha_update,\n\t.final\t\t= sahara_sha_final,\n\t.finup\t\t= sahara_sha_finup,\n\t.digest\t\t= sahara_sha_digest,\n\t.export\t\t= sahara_sha_export,\n\t.import\t\t= sahara_sha_import,\n\t.halg.digestsize\t= SHA1_DIGEST_SIZE,\n\t.halg.statesize         = sizeof(struct sahara_sha_reqctx),\n\t.halg.base\t= {\n\t\t.cra_name\t\t= \"sha1\",\n\t\t.cra_driver_name\t= \"sahara-sha1\",\n\t\t.cra_priority\t\t= 300,\n\t\t.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t= SHA1_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct sahara_ctx),\n\t\t.cra_alignmask\t\t= 0,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t\t.cra_init\t\t= sahara_sha_cra_init,\n\t}\n},\n};\n\nstatic struct ahash_alg sha_v4_algs[] = {\n{\n\t.init\t\t= sahara_sha_init,\n\t.update\t\t= sahara_sha_update,\n\t.final\t\t= sahara_sha_final,\n\t.finup\t\t= sahara_sha_finup,\n\t.digest\t\t= sahara_sha_digest,\n\t.export\t\t= sahara_sha_export,\n\t.import\t\t= sahara_sha_import,\n\t.halg.digestsize\t= SHA256_DIGEST_SIZE,\n\t.halg.statesize         = sizeof(struct sahara_sha_reqctx),\n\t.halg.base\t= {\n\t\t.cra_name\t\t= \"sha256\",\n\t\t.cra_driver_name\t= \"sahara-sha256\",\n\t\t.cra_priority\t\t= 300,\n\t\t.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t= SHA256_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct sahara_ctx),\n\t\t.cra_alignmask\t\t= 0,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t\t.cra_init\t\t= sahara_sha_cra_init,\n\t}\n},\n};\n\nstatic irqreturn_t sahara_irq_handler(int irq, void *data)\n{\n\tstruct sahara_dev *dev = data;\n\tunsigned int stat = sahara_read(dev, SAHARA_REG_STATUS);\n\tunsigned int err = sahara_read(dev, SAHARA_REG_ERRSTATUS);\n\n\tsahara_write(dev, SAHARA_CMD_CLEAR_INT | SAHARA_CMD_CLEAR_ERR,\n\t\t     SAHARA_REG_CMD);\n\n\tsahara_decode_status(dev, stat);\n\n\tif (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_BUSY) {\n\t\treturn IRQ_NONE;\n\t} else if (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_COMPLETE) {\n\t\tdev->error = 0;\n\t} else {\n\t\tsahara_decode_error(dev, err);\n\t\tdev->error = -EINVAL;\n\t}\n\n\tcomplete(&dev->dma_completion);\n\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int sahara_register_algs(struct sahara_dev *dev)\n{\n\tint err;\n\tunsigned int i, j, k, l;\n\n\tfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\n\t\terr = crypto_register_skcipher(&aes_algs[i]);\n\t\tif (err)\n\t\t\tgoto err_aes_algs;\n\t}\n\n\tfor (k = 0; k < ARRAY_SIZE(sha_v3_algs); k++) {\n\t\terr = crypto_register_ahash(&sha_v3_algs[k]);\n\t\tif (err)\n\t\t\tgoto err_sha_v3_algs;\n\t}\n\n\tif (dev->version > SAHARA_VERSION_3)\n\t\tfor (l = 0; l < ARRAY_SIZE(sha_v4_algs); l++) {\n\t\t\terr = crypto_register_ahash(&sha_v4_algs[l]);\n\t\t\tif (err)\n\t\t\t\tgoto err_sha_v4_algs;\n\t\t}\n\n\treturn 0;\n\nerr_sha_v4_algs:\n\tfor (j = 0; j < l; j++)\n\t\tcrypto_unregister_ahash(&sha_v4_algs[j]);\n\nerr_sha_v3_algs:\n\tfor (j = 0; j < k; j++)\n\t\tcrypto_unregister_ahash(&sha_v3_algs[j]);\n\nerr_aes_algs:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_skcipher(&aes_algs[j]);\n\n\treturn err;\n}\n\nstatic void sahara_unregister_algs(struct sahara_dev *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\n\t\tcrypto_unregister_skcipher(&aes_algs[i]);\n\n\tfor (i = 0; i < ARRAY_SIZE(sha_v3_algs); i++)\n\t\tcrypto_unregister_ahash(&sha_v3_algs[i]);\n\n\tif (dev->version > SAHARA_VERSION_3)\n\t\tfor (i = 0; i < ARRAY_SIZE(sha_v4_algs); i++)\n\t\t\tcrypto_unregister_ahash(&sha_v4_algs[i]);\n}\n\nstatic const struct of_device_id sahara_dt_ids[] = {\n\t{ .compatible = \"fsl,imx53-sahara\" },\n\t{ .compatible = \"fsl,imx27-sahara\" },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, sahara_dt_ids);\n\nstatic int sahara_probe(struct platform_device *pdev)\n{\n\tstruct sahara_dev *dev;\n\tu32 version;\n\tint irq;\n\tint err;\n\tint i;\n\n\tdev = devm_kzalloc(&pdev->dev, sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->device = &pdev->dev;\n\tplatform_set_drvdata(pdev, dev);\n\n\t \n\tdev->regs_base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(dev->regs_base))\n\t\treturn PTR_ERR(dev->regs_base);\n\n\t \n\tirq = platform_get_irq(pdev,  0);\n\tif (irq < 0)\n\t\treturn irq;\n\n\terr = devm_request_irq(&pdev->dev, irq, sahara_irq_handler,\n\t\t\t       0, dev_name(&pdev->dev), dev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to request irq\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tdev->clk_ipg = devm_clk_get(&pdev->dev, \"ipg\");\n\tif (IS_ERR(dev->clk_ipg)) {\n\t\tdev_err(&pdev->dev, \"Could not get ipg clock\\n\");\n\t\treturn PTR_ERR(dev->clk_ipg);\n\t}\n\n\tdev->clk_ahb = devm_clk_get(&pdev->dev, \"ahb\");\n\tif (IS_ERR(dev->clk_ahb)) {\n\t\tdev_err(&pdev->dev, \"Could not get ahb clock\\n\");\n\t\treturn PTR_ERR(dev->clk_ahb);\n\t}\n\n\t \n\tdev->hw_desc[0] = dmam_alloc_coherent(&pdev->dev,\n\t\t\tSAHARA_MAX_HW_DESC * sizeof(struct sahara_hw_desc),\n\t\t\t&dev->hw_phys_desc[0], GFP_KERNEL);\n\tif (!dev->hw_desc[0]) {\n\t\tdev_err(&pdev->dev, \"Could not allocate hw descriptors\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdev->hw_desc[1] = dev->hw_desc[0] + 1;\n\tdev->hw_phys_desc[1] = dev->hw_phys_desc[0] +\n\t\t\t\tsizeof(struct sahara_hw_desc);\n\n\t \n\tdev->key_base = dmam_alloc_coherent(&pdev->dev, 2 * AES_KEYSIZE_128,\n\t\t\t\t&dev->key_phys_base, GFP_KERNEL);\n\tif (!dev->key_base) {\n\t\tdev_err(&pdev->dev, \"Could not allocate memory for key\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdev->iv_base = dev->key_base + AES_KEYSIZE_128;\n\tdev->iv_phys_base = dev->key_phys_base + AES_KEYSIZE_128;\n\n\t \n\tdev->context_base = dmam_alloc_coherent(&pdev->dev,\n\t\t\t\t\tSHA256_DIGEST_SIZE + 4,\n\t\t\t\t\t&dev->context_phys_base, GFP_KERNEL);\n\tif (!dev->context_base) {\n\t\tdev_err(&pdev->dev, \"Could not allocate memory for MDHA context\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tdev->hw_link[0] = dmam_alloc_coherent(&pdev->dev,\n\t\t\tSAHARA_MAX_HW_LINK * sizeof(struct sahara_hw_link),\n\t\t\t&dev->hw_phys_link[0], GFP_KERNEL);\n\tif (!dev->hw_link[0]) {\n\t\tdev_err(&pdev->dev, \"Could not allocate hw links\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tfor (i = 1; i < SAHARA_MAX_HW_LINK; i++) {\n\t\tdev->hw_phys_link[i] = dev->hw_phys_link[i - 1] +\n\t\t\t\t\tsizeof(struct sahara_hw_link);\n\t\tdev->hw_link[i] = dev->hw_link[i - 1] + 1;\n\t}\n\n\tcrypto_init_queue(&dev->queue, SAHARA_QUEUE_LENGTH);\n\n\tspin_lock_init(&dev->queue_spinlock);\n\n\tdev_ptr = dev;\n\n\tdev->kthread = kthread_run(sahara_queue_manage, dev, \"sahara_crypto\");\n\tif (IS_ERR(dev->kthread)) {\n\t\treturn PTR_ERR(dev->kthread);\n\t}\n\n\tinit_completion(&dev->dma_completion);\n\n\terr = clk_prepare_enable(dev->clk_ipg);\n\tif (err)\n\t\treturn err;\n\terr = clk_prepare_enable(dev->clk_ahb);\n\tif (err)\n\t\tgoto clk_ipg_disable;\n\n\tversion = sahara_read(dev, SAHARA_REG_VERSION);\n\tif (of_device_is_compatible(pdev->dev.of_node, \"fsl,imx27-sahara\")) {\n\t\tif (version != SAHARA_VERSION_3)\n\t\t\terr = -ENODEV;\n\t} else if (of_device_is_compatible(pdev->dev.of_node,\n\t\t\t\"fsl,imx53-sahara\")) {\n\t\tif (((version >> 8) & 0xff) != SAHARA_VERSION_4)\n\t\t\terr = -ENODEV;\n\t\tversion = (version >> 8) & 0xff;\n\t}\n\tif (err == -ENODEV) {\n\t\tdev_err(&pdev->dev, \"SAHARA version %d not supported\\n\",\n\t\t\t\tversion);\n\t\tgoto err_algs;\n\t}\n\n\tdev->version = version;\n\n\tsahara_write(dev, SAHARA_CMD_RESET | SAHARA_CMD_MODE_BATCH,\n\t\t     SAHARA_REG_CMD);\n\tsahara_write(dev, SAHARA_CONTROL_SET_THROTTLE(0) |\n\t\t\tSAHARA_CONTROL_SET_MAXBURST(8) |\n\t\t\tSAHARA_CONTROL_RNG_AUTORSD |\n\t\t\tSAHARA_CONTROL_ENABLE_INT,\n\t\t\tSAHARA_REG_CONTROL);\n\n\terr = sahara_register_algs(dev);\n\tif (err)\n\t\tgoto err_algs;\n\n\tdev_info(&pdev->dev, \"SAHARA version %d initialized\\n\", version);\n\n\treturn 0;\n\nerr_algs:\n\tkthread_stop(dev->kthread);\n\tdev_ptr = NULL;\n\tclk_disable_unprepare(dev->clk_ahb);\nclk_ipg_disable:\n\tclk_disable_unprepare(dev->clk_ipg);\n\n\treturn err;\n}\n\nstatic int sahara_remove(struct platform_device *pdev)\n{\n\tstruct sahara_dev *dev = platform_get_drvdata(pdev);\n\n\tkthread_stop(dev->kthread);\n\n\tsahara_unregister_algs(dev);\n\n\tclk_disable_unprepare(dev->clk_ipg);\n\tclk_disable_unprepare(dev->clk_ahb);\n\n\tdev_ptr = NULL;\n\n\treturn 0;\n}\n\nstatic struct platform_driver sahara_driver = {\n\t.probe\t\t= sahara_probe,\n\t.remove\t\t= sahara_remove,\n\t.driver\t\t= {\n\t\t.name\t= SAHARA_NAME,\n\t\t.of_match_table = sahara_dt_ids,\n\t},\n};\n\nmodule_platform_driver(sahara_driver);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Javier Martin <javier.martin@vista-silicon.com>\");\nMODULE_AUTHOR(\"Steffen Trumtrar <s.trumtrar@pengutronix.de>\");\nMODULE_DESCRIPTION(\"SAHARA2 HW crypto accelerator\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}