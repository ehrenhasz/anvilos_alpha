{
  "module_name": "padlock-aes.c",
  "hash_id": "a90196744f7d58f6a7c419f5d1a365843216eff6e9120ef23106b76e363fff1b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/padlock-aes.c",
  "human_readable_source": "\n \n\n#include <crypto/algapi.h>\n#include <crypto/aes.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/padlock.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/percpu.h>\n#include <linux/smp.h>\n#include <linux/slab.h>\n#include <asm/cpu_device_id.h>\n#include <asm/byteorder.h>\n#include <asm/processor.h>\n#include <asm/fpu/api.h>\n\n \nstatic unsigned int ecb_fetch_blocks = 2;\n#define MAX_ECB_FETCH_BLOCKS (8)\n#define ecb_fetch_bytes (ecb_fetch_blocks * AES_BLOCK_SIZE)\n\nstatic unsigned int cbc_fetch_blocks = 1;\n#define MAX_CBC_FETCH_BLOCKS (4)\n#define cbc_fetch_bytes (cbc_fetch_blocks * AES_BLOCK_SIZE)\n\n \nstruct cword {\n\tunsigned int __attribute__ ((__packed__))\n\t\trounds:4,\n\t\talgo:3,\n\t\tkeygen:1,\n\t\tinterm:1,\n\t\tencdec:1,\n\t\tksize:2;\n} __attribute__ ((__aligned__(PADLOCK_ALIGNMENT)));\n\n \nstruct aes_ctx {\n\tu32 E[AES_MAX_KEYLENGTH_U32]\n\t\t__attribute__ ((__aligned__(PADLOCK_ALIGNMENT)));\n\tu32 d_data[AES_MAX_KEYLENGTH_U32]\n\t\t__attribute__ ((__aligned__(PADLOCK_ALIGNMENT)));\n\tstruct {\n\t\tstruct cword encrypt;\n\t\tstruct cword decrypt;\n\t} cword;\n\tu32 *D;\n};\n\nstatic DEFINE_PER_CPU(struct cword *, paes_last_cword);\n\n \nstatic inline int\naes_hw_extkey_available(uint8_t key_len)\n{\n\t \n\tif (key_len == 16)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline struct aes_ctx *aes_ctx_common(void *ctx)\n{\n\tunsigned long addr = (unsigned long)ctx;\n\tunsigned long align = PADLOCK_ALIGNMENT;\n\n\tif (align <= crypto_tfm_ctx_alignment())\n\t\talign = 1;\n\treturn (struct aes_ctx *)ALIGN(addr, align);\n}\n\nstatic inline struct aes_ctx *aes_ctx(struct crypto_tfm *tfm)\n{\n\treturn aes_ctx_common(crypto_tfm_ctx(tfm));\n}\n\nstatic inline struct aes_ctx *skcipher_aes_ctx(struct crypto_skcipher *tfm)\n{\n\treturn aes_ctx_common(crypto_skcipher_ctx(tfm));\n}\n\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\n\t\t       unsigned int key_len)\n{\n\tstruct aes_ctx *ctx = aes_ctx(tfm);\n\tconst __le32 *key = (const __le32 *)in_key;\n\tstruct crypto_aes_ctx gen_aes;\n\tint cpu;\n\n\tif (key_len % 8)\n\t\treturn -EINVAL;\n\n\t \n\tctx->D = ctx->E;\n\n\tctx->E[0] = le32_to_cpu(key[0]);\n\tctx->E[1] = le32_to_cpu(key[1]);\n\tctx->E[2] = le32_to_cpu(key[2]);\n\tctx->E[3] = le32_to_cpu(key[3]);\n\n\t \n\tmemset(&ctx->cword, 0, sizeof(ctx->cword));\n\n\tctx->cword.decrypt.encdec = 1;\n\tctx->cword.encrypt.rounds = 10 + (key_len - 16) / 4;\n\tctx->cword.decrypt.rounds = ctx->cword.encrypt.rounds;\n\tctx->cword.encrypt.ksize = (key_len - 16) / 8;\n\tctx->cword.decrypt.ksize = ctx->cword.encrypt.ksize;\n\n\t \n\tif (aes_hw_extkey_available(key_len))\n\t\tgoto ok;\n\n\tctx->D = ctx->d_data;\n\tctx->cword.encrypt.keygen = 1;\n\tctx->cword.decrypt.keygen = 1;\n\n\tif (aes_expandkey(&gen_aes, in_key, key_len))\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->E, gen_aes.key_enc, AES_MAX_KEYLENGTH);\n\tmemcpy(ctx->D, gen_aes.key_dec, AES_MAX_KEYLENGTH);\n\nok:\n\tfor_each_online_cpu(cpu)\n\t\tif (&ctx->cword.encrypt == per_cpu(paes_last_cword, cpu) ||\n\t\t    &ctx->cword.decrypt == per_cpu(paes_last_cword, cpu))\n\t\t\tper_cpu(paes_last_cword, cpu) = NULL;\n\n\treturn 0;\n}\n\nstatic int aes_set_key_skcipher(struct crypto_skcipher *tfm, const u8 *in_key,\n\t\t\t\tunsigned int key_len)\n{\n\treturn aes_set_key(crypto_skcipher_tfm(tfm), in_key, key_len);\n}\n\n \n\n \nstatic inline void padlock_reset_key(struct cword *cword)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (cword != per_cpu(paes_last_cword, cpu))\n#ifndef CONFIG_X86_64\n\t\tasm volatile (\"pushfl; popfl\");\n#else\n\t\tasm volatile (\"pushfq; popfq\");\n#endif\n}\n\nstatic inline void padlock_store_cword(struct cword *cword)\n{\n\tper_cpu(paes_last_cword, raw_smp_processor_id()) = cword;\n}\n\n \n\nstatic inline void rep_xcrypt_ecb(const u8 *input, u8 *output, void *key,\n\t\t\t\t  struct cword *control_word, int count)\n{\n\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xc8\"\t \n\t\t      : \"+S\"(input), \"+D\"(output)\n\t\t      : \"d\"(control_word), \"b\"(key), \"c\"(count));\n}\n\nstatic inline u8 *rep_xcrypt_cbc(const u8 *input, u8 *output, void *key,\n\t\t\t\t u8 *iv, struct cword *control_word, int count)\n{\n\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xd0\"\t \n\t\t      : \"+S\" (input), \"+D\" (output), \"+a\" (iv)\n\t\t      : \"d\" (control_word), \"b\" (key), \"c\" (count));\n\treturn iv;\n}\n\nstatic void ecb_crypt_copy(const u8 *in, u8 *out, u32 *key,\n\t\t\t   struct cword *cword, int count)\n{\n\t \n\tu8 buf[AES_BLOCK_SIZE * (MAX_ECB_FETCH_BLOCKS - 1) + PADLOCK_ALIGNMENT - 1];\n\tu8 *tmp = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\n\tmemcpy(tmp, in, count * AES_BLOCK_SIZE);\n\trep_xcrypt_ecb(tmp, out, key, cword, count);\n}\n\nstatic u8 *cbc_crypt_copy(const u8 *in, u8 *out, u32 *key,\n\t\t\t   u8 *iv, struct cword *cword, int count)\n{\n\t \n\tu8 buf[AES_BLOCK_SIZE * (MAX_CBC_FETCH_BLOCKS - 1) + PADLOCK_ALIGNMENT - 1];\n\tu8 *tmp = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\n\tmemcpy(tmp, in, count * AES_BLOCK_SIZE);\n\treturn rep_xcrypt_cbc(tmp, out, key, iv, cword, count);\n}\n\nstatic inline void ecb_crypt(const u8 *in, u8 *out, u32 *key,\n\t\t\t     struct cword *cword, int count)\n{\n\t \n\tif (unlikely(offset_in_page(in) + ecb_fetch_bytes > PAGE_SIZE)) {\n\t\tecb_crypt_copy(in, out, key, cword, count);\n\t\treturn;\n\t}\n\n\trep_xcrypt_ecb(in, out, key, cword, count);\n}\n\nstatic inline u8 *cbc_crypt(const u8 *in, u8 *out, u32 *key,\n\t\t\t    u8 *iv, struct cword *cword, int count)\n{\n\t \n\tif (unlikely(offset_in_page(in) + cbc_fetch_bytes > PAGE_SIZE))\n\t\treturn cbc_crypt_copy(in, out, key, iv, cword, count);\n\n\treturn rep_xcrypt_cbc(in, out, key, iv, cword, count);\n}\n\nstatic inline void padlock_xcrypt_ecb(const u8 *input, u8 *output, void *key,\n\t\t\t\t      void *control_word, u32 count)\n{\n\tu32 initial = count & (ecb_fetch_blocks - 1);\n\n\tif (count < ecb_fetch_blocks) {\n\t\tecb_crypt(input, output, key, control_word, count);\n\t\treturn;\n\t}\n\n\tcount -= initial;\n\n\tif (initial)\n\t\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xc8\"\t \n\t\t\t      : \"+S\"(input), \"+D\"(output)\n\t\t\t      : \"d\"(control_word), \"b\"(key), \"c\"(initial));\n\n\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xc8\"\t \n\t\t      : \"+S\"(input), \"+D\"(output)\n\t\t      : \"d\"(control_word), \"b\"(key), \"c\"(count));\n}\n\nstatic inline u8 *padlock_xcrypt_cbc(const u8 *input, u8 *output, void *key,\n\t\t\t\t     u8 *iv, void *control_word, u32 count)\n{\n\tu32 initial = count & (cbc_fetch_blocks - 1);\n\n\tif (count < cbc_fetch_blocks)\n\t\treturn cbc_crypt(input, output, key, iv, control_word, count);\n\n\tcount -= initial;\n\n\tif (initial)\n\t\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xd0\"\t \n\t\t\t      : \"+S\" (input), \"+D\" (output), \"+a\" (iv)\n\t\t\t      : \"d\" (control_word), \"b\" (key), \"c\" (initial));\n\n\tasm volatile (\".byte 0xf3,0x0f,0xa7,0xd0\"\t \n\t\t      : \"+S\" (input), \"+D\" (output), \"+a\" (iv)\n\t\t      : \"d\" (control_word), \"b\" (key), \"c\" (count));\n\treturn iv;\n}\n\nstatic void padlock_aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\n{\n\tstruct aes_ctx *ctx = aes_ctx(tfm);\n\n\tpadlock_reset_key(&ctx->cword.encrypt);\n\tecb_crypt(in, out, ctx->E, &ctx->cword.encrypt, 1);\n\tpadlock_store_cword(&ctx->cword.encrypt);\n}\n\nstatic void padlock_aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\n{\n\tstruct aes_ctx *ctx = aes_ctx(tfm);\n\n\tpadlock_reset_key(&ctx->cword.encrypt);\n\tecb_crypt(in, out, ctx->D, &ctx->cword.decrypt, 1);\n\tpadlock_store_cword(&ctx->cword.encrypt);\n}\n\nstatic struct crypto_alg aes_alg = {\n\t.cra_name\t\t=\t\"aes\",\n\t.cra_driver_name\t=\t\"aes-padlock\",\n\t.cra_priority\t\t=\tPADLOCK_CRA_PRIORITY,\n\t.cra_flags\t\t=\tCRYPTO_ALG_TYPE_CIPHER,\n\t.cra_blocksize\t\t=\tAES_BLOCK_SIZE,\n\t.cra_ctxsize\t\t=\tsizeof(struct aes_ctx),\n\t.cra_alignmask\t\t=\tPADLOCK_ALIGNMENT - 1,\n\t.cra_module\t\t=\tTHIS_MODULE,\n\t.cra_u\t\t\t=\t{\n\t\t.cipher = {\n\t\t\t.cia_min_keysize\t=\tAES_MIN_KEY_SIZE,\n\t\t\t.cia_max_keysize\t=\tAES_MAX_KEY_SIZE,\n\t\t\t.cia_setkey\t   \t= \taes_set_key,\n\t\t\t.cia_encrypt\t \t=\tpadlock_aes_encrypt,\n\t\t\t.cia_decrypt\t  \t=\tpadlock_aes_decrypt,\n\t\t}\n\t}\n};\n\nstatic int ecb_aes_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aes_ctx *ctx = skcipher_aes_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\tpadlock_reset_key(&ctx->cword.encrypt);\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) != 0) {\n\t\tpadlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,\n\t\t\t\t   ctx->E, &ctx->cword.encrypt,\n\t\t\t\t   nbytes / AES_BLOCK_SIZE);\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\tpadlock_store_cword(&ctx->cword.encrypt);\n\n\treturn err;\n}\n\nstatic int ecb_aes_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aes_ctx *ctx = skcipher_aes_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\tpadlock_reset_key(&ctx->cword.decrypt);\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) != 0) {\n\t\tpadlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,\n\t\t\t\t   ctx->D, &ctx->cword.decrypt,\n\t\t\t\t   nbytes / AES_BLOCK_SIZE);\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\tpadlock_store_cword(&ctx->cword.encrypt);\n\n\treturn err;\n}\n\nstatic struct skcipher_alg ecb_aes_alg = {\n\t.base.cra_name\t\t=\t\"ecb(aes)\",\n\t.base.cra_driver_name\t=\t\"ecb-aes-padlock\",\n\t.base.cra_priority\t=\tPADLOCK_COMPOSITE_PRIORITY,\n\t.base.cra_blocksize\t=\tAES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t=\tsizeof(struct aes_ctx),\n\t.base.cra_alignmask\t=\tPADLOCK_ALIGNMENT - 1,\n\t.base.cra_module\t=\tTHIS_MODULE,\n\t.min_keysize\t\t=\tAES_MIN_KEY_SIZE,\n\t.max_keysize\t\t=\tAES_MAX_KEY_SIZE,\n\t.setkey\t\t\t=\taes_set_key_skcipher,\n\t.encrypt\t\t=\tecb_aes_encrypt,\n\t.decrypt\t\t=\tecb_aes_decrypt,\n};\n\nstatic int cbc_aes_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aes_ctx *ctx = skcipher_aes_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\tpadlock_reset_key(&ctx->cword.encrypt);\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) != 0) {\n\t\tu8 *iv = padlock_xcrypt_cbc(walk.src.virt.addr,\n\t\t\t\t\t    walk.dst.virt.addr, ctx->E,\n\t\t\t\t\t    walk.iv, &ctx->cword.encrypt,\n\t\t\t\t\t    nbytes / AES_BLOCK_SIZE);\n\t\tmemcpy(walk.iv, iv, AES_BLOCK_SIZE);\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\tpadlock_store_cword(&ctx->cword.decrypt);\n\n\treturn err;\n}\n\nstatic int cbc_aes_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aes_ctx *ctx = skcipher_aes_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\tpadlock_reset_key(&ctx->cword.encrypt);\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) != 0) {\n\t\tpadlock_xcrypt_cbc(walk.src.virt.addr, walk.dst.virt.addr,\n\t\t\t\t   ctx->D, walk.iv, &ctx->cword.decrypt,\n\t\t\t\t   nbytes / AES_BLOCK_SIZE);\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\tpadlock_store_cword(&ctx->cword.encrypt);\n\n\treturn err;\n}\n\nstatic struct skcipher_alg cbc_aes_alg = {\n\t.base.cra_name\t\t=\t\"cbc(aes)\",\n\t.base.cra_driver_name\t=\t\"cbc-aes-padlock\",\n\t.base.cra_priority\t=\tPADLOCK_COMPOSITE_PRIORITY,\n\t.base.cra_blocksize\t=\tAES_BLOCK_SIZE,\n\t.base.cra_ctxsize\t=\tsizeof(struct aes_ctx),\n\t.base.cra_alignmask\t=\tPADLOCK_ALIGNMENT - 1,\n\t.base.cra_module\t=\tTHIS_MODULE,\n\t.min_keysize\t\t=\tAES_MIN_KEY_SIZE,\n\t.max_keysize\t\t=\tAES_MAX_KEY_SIZE,\n\t.ivsize\t\t\t=\tAES_BLOCK_SIZE,\n\t.setkey\t\t\t=\taes_set_key_skcipher,\n\t.encrypt\t\t=\tcbc_aes_encrypt,\n\t.decrypt\t\t=\tcbc_aes_decrypt,\n};\n\nstatic const struct x86_cpu_id padlock_cpu_id[] = {\n\tX86_MATCH_FEATURE(X86_FEATURE_XCRYPT, NULL),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, padlock_cpu_id);\n\nstatic int __init padlock_init(void)\n{\n\tint ret;\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\n\tif (!x86_match_cpu(padlock_cpu_id))\n\t\treturn -ENODEV;\n\n\tif (!boot_cpu_has(X86_FEATURE_XCRYPT_EN)) {\n\t\tprintk(KERN_NOTICE PFX \"VIA PadLock detected, but not enabled. Hmm, strange...\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif ((ret = crypto_register_alg(&aes_alg)) != 0)\n\t\tgoto aes_err;\n\n\tif ((ret = crypto_register_skcipher(&ecb_aes_alg)) != 0)\n\t\tgoto ecb_aes_err;\n\n\tif ((ret = crypto_register_skcipher(&cbc_aes_alg)) != 0)\n\t\tgoto cbc_aes_err;\n\n\tprintk(KERN_NOTICE PFX \"Using VIA PadLock ACE for AES algorithm.\\n\");\n\n\tif (c->x86 == 6 && c->x86_model == 15 && c->x86_stepping == 2) {\n\t\tecb_fetch_blocks = MAX_ECB_FETCH_BLOCKS;\n\t\tcbc_fetch_blocks = MAX_CBC_FETCH_BLOCKS;\n\t\tprintk(KERN_NOTICE PFX \"VIA Nano stepping 2 detected: enabling workaround.\\n\");\n\t}\n\nout:\n\treturn ret;\n\ncbc_aes_err:\n\tcrypto_unregister_skcipher(&ecb_aes_alg);\necb_aes_err:\n\tcrypto_unregister_alg(&aes_alg);\naes_err:\n\tprintk(KERN_ERR PFX \"VIA PadLock AES initialization failed.\\n\");\n\tgoto out;\n}\n\nstatic void __exit padlock_fini(void)\n{\n\tcrypto_unregister_skcipher(&cbc_aes_alg);\n\tcrypto_unregister_skcipher(&ecb_aes_alg);\n\tcrypto_unregister_alg(&aes_alg);\n}\n\nmodule_init(padlock_init);\nmodule_exit(padlock_fini);\n\nMODULE_DESCRIPTION(\"VIA PadLock AES algorithm support\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Michal Ludvig\");\n\nMODULE_ALIAS_CRYPTO(\"aes\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}