{
  "module_name": "cc_buffer_mgr.c",
  "hash_id": "d82ff13c1c54e6cba769469449fb065583b59d19faa5db08bff303cb4ef2cb8d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccree/cc_buffer_mgr.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/aead.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <linux/dmapool.h>\n#include <linux/dma-mapping.h>\n\n#include \"cc_buffer_mgr.h\"\n#include \"cc_lli_defs.h\"\n#include \"cc_cipher.h\"\n#include \"cc_hash.h\"\n#include \"cc_aead.h\"\n\nunion buffer_array_entry {\n\tstruct scatterlist *sgl;\n\tdma_addr_t buffer_dma;\n};\n\nstruct buffer_array {\n\tunsigned int num_of_buffers;\n\tunion buffer_array_entry entry[MAX_NUM_OF_BUFFERS_IN_MLLI];\n\tunsigned int offset[MAX_NUM_OF_BUFFERS_IN_MLLI];\n\tint nents[MAX_NUM_OF_BUFFERS_IN_MLLI];\n\tint total_data_len[MAX_NUM_OF_BUFFERS_IN_MLLI];\n\tbool is_last[MAX_NUM_OF_BUFFERS_IN_MLLI];\n\tu32 *mlli_nents[MAX_NUM_OF_BUFFERS_IN_MLLI];\n};\n\nstatic inline char *cc_dma_buf_type(enum cc_req_dma_buf_type type)\n{\n\tswitch (type) {\n\tcase CC_DMA_BUF_NULL:\n\t\treturn \"BUF_NULL\";\n\tcase CC_DMA_BUF_DLLI:\n\t\treturn \"BUF_DLLI\";\n\tcase CC_DMA_BUF_MLLI:\n\t\treturn \"BUF_MLLI\";\n\tdefault:\n\t\treturn \"BUF_INVALID\";\n\t}\n}\n\n \nstatic void cc_copy_mac(struct device *dev, struct aead_request *req,\n\t\t\tenum cc_sg_cpy_direct dir)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tu32 skip = req->assoclen + req->cryptlen;\n\n\tcc_copy_sg_portion(dev, areq_ctx->backup_mac, req->src,\n\t\t\t   (skip - areq_ctx->req_authsize), skip, dir);\n}\n\n \nstatic unsigned int cc_get_sgl_nents(struct device *dev,\n\t\t\t\t     struct scatterlist *sg_list,\n\t\t\t\t     unsigned int nbytes, u32 *lbytes)\n{\n\tunsigned int nents = 0;\n\n\t*lbytes = 0;\n\n\twhile (nbytes && sg_list) {\n\t\tnents++;\n\t\t \n\t\t*lbytes = nbytes;\n\t\tnbytes -= (sg_list->length > nbytes) ?\n\t\t\t\tnbytes : sg_list->length;\n\t\tsg_list = sg_next(sg_list);\n\t}\n\n\tdev_dbg(dev, \"nents %d last bytes %d\\n\", nents, *lbytes);\n\treturn nents;\n}\n\n \nvoid cc_copy_sg_portion(struct device *dev, u8 *dest, struct scatterlist *sg,\n\t\t\tu32 to_skip, u32 end, enum cc_sg_cpy_direct direct)\n{\n\tu32 nents;\n\n\tnents = sg_nents_for_len(sg, end);\n\tsg_copy_buffer(sg, nents, dest, (end - to_skip + 1), to_skip,\n\t\t       (direct == CC_SG_TO_BUF));\n}\n\nstatic int cc_render_buff_to_mlli(struct device *dev, dma_addr_t buff_dma,\n\t\t\t\t  u32 buff_size, u32 *curr_nents,\n\t\t\t\t  u32 **mlli_entry_pp)\n{\n\tu32 *mlli_entry_p = *mlli_entry_pp;\n\tu32 new_nents;\n\n\t \n\tnew_nents = (*curr_nents + buff_size / CC_MAX_MLLI_ENTRY_SIZE + 1);\n\tif (new_nents > MAX_NUM_OF_TOTAL_MLLI_ENTRIES) {\n\t\tdev_err(dev, \"Too many mlli entries. current %d max %d\\n\",\n\t\t\tnew_nents, MAX_NUM_OF_TOTAL_MLLI_ENTRIES);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\twhile (buff_size > CC_MAX_MLLI_ENTRY_SIZE) {\n\t\tcc_lli_set_addr(mlli_entry_p, buff_dma);\n\t\tcc_lli_set_size(mlli_entry_p, CC_MAX_MLLI_ENTRY_SIZE);\n\t\tdev_dbg(dev, \"entry[%d]: single_buff=0x%08X size=%08X\\n\",\n\t\t\t*curr_nents, mlli_entry_p[LLI_WORD0_OFFSET],\n\t\t\tmlli_entry_p[LLI_WORD1_OFFSET]);\n\t\tbuff_dma += CC_MAX_MLLI_ENTRY_SIZE;\n\t\tbuff_size -= CC_MAX_MLLI_ENTRY_SIZE;\n\t\tmlli_entry_p = mlli_entry_p + 2;\n\t\t(*curr_nents)++;\n\t}\n\t \n\tcc_lli_set_addr(mlli_entry_p, buff_dma);\n\tcc_lli_set_size(mlli_entry_p, buff_size);\n\tdev_dbg(dev, \"entry[%d]: single_buff=0x%08X size=%08X\\n\",\n\t\t*curr_nents, mlli_entry_p[LLI_WORD0_OFFSET],\n\t\tmlli_entry_p[LLI_WORD1_OFFSET]);\n\tmlli_entry_p = mlli_entry_p + 2;\n\t*mlli_entry_pp = mlli_entry_p;\n\t(*curr_nents)++;\n\treturn 0;\n}\n\nstatic int cc_render_sg_to_mlli(struct device *dev, struct scatterlist *sgl,\n\t\t\t\tu32 sgl_data_len, u32 sgl_offset,\n\t\t\t\tu32 *curr_nents, u32 **mlli_entry_pp)\n{\n\tstruct scatterlist *curr_sgl = sgl;\n\tu32 *mlli_entry_p = *mlli_entry_pp;\n\ts32 rc = 0;\n\n\tfor ( ; (curr_sgl && sgl_data_len);\n\t      curr_sgl = sg_next(curr_sgl)) {\n\t\tu32 entry_data_len =\n\t\t\t(sgl_data_len > sg_dma_len(curr_sgl) - sgl_offset) ?\n\t\t\t\tsg_dma_len(curr_sgl) - sgl_offset :\n\t\t\t\tsgl_data_len;\n\t\tsgl_data_len -= entry_data_len;\n\t\trc = cc_render_buff_to_mlli(dev, sg_dma_address(curr_sgl) +\n\t\t\t\t\t    sgl_offset, entry_data_len,\n\t\t\t\t\t    curr_nents, &mlli_entry_p);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tsgl_offset = 0;\n\t}\n\t*mlli_entry_pp = mlli_entry_p;\n\treturn 0;\n}\n\nstatic int cc_generate_mlli(struct device *dev, struct buffer_array *sg_data,\n\t\t\t    struct mlli_params *mlli_params, gfp_t flags)\n{\n\tu32 *mlli_p;\n\tu32 total_nents = 0, prev_total_nents = 0;\n\tint rc = 0, i;\n\n\tdev_dbg(dev, \"NUM of SG's = %d\\n\", sg_data->num_of_buffers);\n\n\t \n\tmlli_params->mlli_virt_addr =\n\t\tdma_pool_alloc(mlli_params->curr_pool, flags,\n\t\t\t       &mlli_params->mlli_dma_addr);\n\tif (!mlli_params->mlli_virt_addr) {\n\t\tdev_err(dev, \"dma_pool_alloc() failed\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto build_mlli_exit;\n\t}\n\t \n\tmlli_p = mlli_params->mlli_virt_addr;\n\t \n\tfor (i = 0; i < sg_data->num_of_buffers; i++) {\n\t\tunion buffer_array_entry *entry = &sg_data->entry[i];\n\t\tu32 tot_len = sg_data->total_data_len[i];\n\t\tu32 offset = sg_data->offset[i];\n\n\t\trc = cc_render_sg_to_mlli(dev, entry->sgl, tot_len, offset,\n\t\t\t\t\t  &total_nents, &mlli_p);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\tif (sg_data->mlli_nents[i]) {\n\t\t\t \n\t\t\t*sg_data->mlli_nents[i] +=\n\t\t\t\t(total_nents - prev_total_nents);\n\t\t\tprev_total_nents = total_nents;\n\t\t}\n\t}\n\n\t \n\tmlli_params->mlli_len = (total_nents * LLI_ENTRY_BYTE_SIZE);\n\n\tdev_dbg(dev, \"MLLI params: virt_addr=%pK dma_addr=%pad mlli_len=0x%X\\n\",\n\t\tmlli_params->mlli_virt_addr, &mlli_params->mlli_dma_addr,\n\t\tmlli_params->mlli_len);\n\nbuild_mlli_exit:\n\treturn rc;\n}\n\nstatic void cc_add_sg_entry(struct device *dev, struct buffer_array *sgl_data,\n\t\t\t    unsigned int nents, struct scatterlist *sgl,\n\t\t\t    unsigned int data_len, unsigned int data_offset,\n\t\t\t    bool is_last_table, u32 *mlli_nents)\n{\n\tunsigned int index = sgl_data->num_of_buffers;\n\n\tdev_dbg(dev, \"index=%u nents=%u sgl=%pK data_len=0x%08X is_last=%d\\n\",\n\t\tindex, nents, sgl, data_len, is_last_table);\n\tsgl_data->nents[index] = nents;\n\tsgl_data->entry[index].sgl = sgl;\n\tsgl_data->offset[index] = data_offset;\n\tsgl_data->total_data_len[index] = data_len;\n\tsgl_data->is_last[index] = is_last_table;\n\tsgl_data->mlli_nents[index] = mlli_nents;\n\tif (sgl_data->mlli_nents[index])\n\t\t*sgl_data->mlli_nents[index] = 0;\n\tsgl_data->num_of_buffers++;\n}\n\nstatic int cc_map_sg(struct device *dev, struct scatterlist *sg,\n\t\t     unsigned int nbytes, int direction, u32 *nents,\n\t\t     u32 max_sg_nents, u32 *lbytes, u32 *mapped_nents)\n{\n\tint ret = 0;\n\n\tif (!nbytes) {\n\t\t*mapped_nents = 0;\n\t\t*lbytes = 0;\n\t\t*nents = 0;\n\t\treturn 0;\n\t}\n\n\t*nents = cc_get_sgl_nents(dev, sg, nbytes, lbytes);\n\tif (*nents > max_sg_nents) {\n\t\t*nents = 0;\n\t\tdev_err(dev, \"Too many fragments. current %d max %d\\n\",\n\t\t\t*nents, max_sg_nents);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = dma_map_sg(dev, sg, *nents, direction);\n\tif (!ret) {\n\t\t*nents = 0;\n\t\tdev_err(dev, \"dma_map_sg() sg buffer failed %d\\n\", ret);\n\t\treturn -ENOMEM;\n\t}\n\n\t*mapped_nents = ret;\n\n\treturn 0;\n}\n\nstatic int\ncc_set_aead_conf_buf(struct device *dev, struct aead_req_ctx *areq_ctx,\n\t\t     u8 *config_data, struct buffer_array *sg_data,\n\t\t     unsigned int assoclen)\n{\n\tdev_dbg(dev, \" handle additional data config set to DLLI\\n\");\n\t \n\tsg_init_one(&areq_ctx->ccm_adata_sg, config_data,\n\t\t    AES_BLOCK_SIZE + areq_ctx->ccm_hdr_size);\n\tif (dma_map_sg(dev, &areq_ctx->ccm_adata_sg, 1, DMA_TO_DEVICE) != 1) {\n\t\tdev_err(dev, \"dma_map_sg() config buffer failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdev_dbg(dev, \"Mapped curr_buff: dma_address=%pad page=%p addr=%pK offset=%u length=%u\\n\",\n\t\t&sg_dma_address(&areq_ctx->ccm_adata_sg),\n\t\tsg_page(&areq_ctx->ccm_adata_sg),\n\t\tsg_virt(&areq_ctx->ccm_adata_sg),\n\t\tareq_ctx->ccm_adata_sg.offset, areq_ctx->ccm_adata_sg.length);\n\t \n\tif (assoclen > 0) {\n\t\tcc_add_sg_entry(dev, sg_data, 1, &areq_ctx->ccm_adata_sg,\n\t\t\t\t(AES_BLOCK_SIZE + areq_ctx->ccm_hdr_size),\n\t\t\t\t0, false, NULL);\n\t}\n\treturn 0;\n}\n\nstatic int cc_set_hash_buf(struct device *dev, struct ahash_req_ctx *areq_ctx,\n\t\t\t   u8 *curr_buff, u32 curr_buff_cnt,\n\t\t\t   struct buffer_array *sg_data)\n{\n\tdev_dbg(dev, \" handle curr buff %x set to   DLLI\\n\", curr_buff_cnt);\n\t \n\tsg_init_one(areq_ctx->buff_sg, curr_buff, curr_buff_cnt);\n\tif (dma_map_sg(dev, areq_ctx->buff_sg, 1, DMA_TO_DEVICE) != 1) {\n\t\tdev_err(dev, \"dma_map_sg() src buffer failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdev_dbg(dev, \"Mapped curr_buff: dma_address=%pad page=%p addr=%pK offset=%u length=%u\\n\",\n\t\t&sg_dma_address(areq_ctx->buff_sg), sg_page(areq_ctx->buff_sg),\n\t\tsg_virt(areq_ctx->buff_sg), areq_ctx->buff_sg->offset,\n\t\tareq_ctx->buff_sg->length);\n\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_DLLI;\n\tareq_ctx->curr_sg = areq_ctx->buff_sg;\n\tareq_ctx->in_nents = 0;\n\t \n\tcc_add_sg_entry(dev, sg_data, 1, areq_ctx->buff_sg, curr_buff_cnt, 0,\n\t\t\tfalse, NULL);\n\treturn 0;\n}\n\nvoid cc_unmap_cipher_request(struct device *dev, void *ctx,\n\t\t\t\tunsigned int ivsize, struct scatterlist *src,\n\t\t\t\tstruct scatterlist *dst)\n{\n\tstruct cipher_req_ctx *req_ctx = (struct cipher_req_ctx *)ctx;\n\n\tif (req_ctx->gen_ctx.iv_dma_addr) {\n\t\tdev_dbg(dev, \"Unmapped iv: iv_dma_addr=%pad iv_size=%u\\n\",\n\t\t\t&req_ctx->gen_ctx.iv_dma_addr, ivsize);\n\t\tdma_unmap_single(dev, req_ctx->gen_ctx.iv_dma_addr,\n\t\t\t\t ivsize, DMA_BIDIRECTIONAL);\n\t}\n\t \n\tif (req_ctx->dma_buf_type == CC_DMA_BUF_MLLI &&\n\t    req_ctx->mlli_params.mlli_virt_addr) {\n\t\tdma_pool_free(req_ctx->mlli_params.curr_pool,\n\t\t\t      req_ctx->mlli_params.mlli_virt_addr,\n\t\t\t      req_ctx->mlli_params.mlli_dma_addr);\n\t}\n\n\tif (src != dst) {\n\t\tdma_unmap_sg(dev, src, req_ctx->in_nents, DMA_TO_DEVICE);\n\t\tdma_unmap_sg(dev, dst, req_ctx->out_nents, DMA_FROM_DEVICE);\n\t\tdev_dbg(dev, \"Unmapped req->dst=%pK\\n\", sg_virt(dst));\n\t\tdev_dbg(dev, \"Unmapped req->src=%pK\\n\", sg_virt(src));\n\t} else {\n\t\tdma_unmap_sg(dev, src, req_ctx->in_nents, DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped req->src=%pK\\n\", sg_virt(src));\n\t}\n}\n\nint cc_map_cipher_request(struct cc_drvdata *drvdata, void *ctx,\n\t\t\t  unsigned int ivsize, unsigned int nbytes,\n\t\t\t  void *info, struct scatterlist *src,\n\t\t\t  struct scatterlist *dst, gfp_t flags)\n{\n\tstruct cipher_req_ctx *req_ctx = (struct cipher_req_ctx *)ctx;\n\tstruct mlli_params *mlli_params = &req_ctx->mlli_params;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tstruct buffer_array sg_data;\n\tu32 dummy = 0;\n\tint rc = 0;\n\tu32 mapped_nents = 0;\n\tint src_direction = (src != dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL);\n\n\treq_ctx->dma_buf_type = CC_DMA_BUF_DLLI;\n\tmlli_params->curr_pool = NULL;\n\tsg_data.num_of_buffers = 0;\n\n\t \n\tif (ivsize) {\n\t\tdump_byte_array(\"iv\", info, ivsize);\n\t\treq_ctx->gen_ctx.iv_dma_addr =\n\t\t\tdma_map_single(dev, info, ivsize, DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, req_ctx->gen_ctx.iv_dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping iv %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tivsize, info);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tdev_dbg(dev, \"Mapped iv %u B at va=%pK to dma=%pad\\n\",\n\t\t\tivsize, info, &req_ctx->gen_ctx.iv_dma_addr);\n\t} else {\n\t\treq_ctx->gen_ctx.iv_dma_addr = 0;\n\t}\n\n\t \n\trc = cc_map_sg(dev, src, nbytes, src_direction, &req_ctx->in_nents,\n\t\t       LLI_MAX_NUM_OF_DATA_ENTRIES, &dummy, &mapped_nents);\n\tif (rc)\n\t\tgoto cipher_exit;\n\tif (mapped_nents > 1)\n\t\treq_ctx->dma_buf_type = CC_DMA_BUF_MLLI;\n\n\tif (src == dst) {\n\t\t \n\t\tif (req_ctx->dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\t\treq_ctx->out_nents = 0;\n\t\t\tcc_add_sg_entry(dev, &sg_data, req_ctx->in_nents, src,\n\t\t\t\t\tnbytes, 0, true,\n\t\t\t\t\t&req_ctx->in_mlli_nents);\n\t\t}\n\t} else {\n\t\t \n\t\trc = cc_map_sg(dev, dst, nbytes, DMA_FROM_DEVICE,\n\t\t\t       &req_ctx->out_nents, LLI_MAX_NUM_OF_DATA_ENTRIES,\n\t\t\t       &dummy, &mapped_nents);\n\t\tif (rc)\n\t\t\tgoto cipher_exit;\n\t\tif (mapped_nents > 1)\n\t\t\treq_ctx->dma_buf_type = CC_DMA_BUF_MLLI;\n\n\t\tif (req_ctx->dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\t\tcc_add_sg_entry(dev, &sg_data, req_ctx->in_nents, src,\n\t\t\t\t\tnbytes, 0, true,\n\t\t\t\t\t&req_ctx->in_mlli_nents);\n\t\t\tcc_add_sg_entry(dev, &sg_data, req_ctx->out_nents, dst,\n\t\t\t\t\tnbytes, 0, true,\n\t\t\t\t\t&req_ctx->out_mlli_nents);\n\t\t}\n\t}\n\n\tif (req_ctx->dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\tmlli_params->curr_pool = drvdata->mlli_buffs_pool;\n\t\trc = cc_generate_mlli(dev, &sg_data, mlli_params, flags);\n\t\tif (rc)\n\t\t\tgoto cipher_exit;\n\t}\n\n\tdev_dbg(dev, \"areq_ctx->dma_buf_type = %s\\n\",\n\t\tcc_dma_buf_type(req_ctx->dma_buf_type));\n\n\treturn 0;\n\ncipher_exit:\n\tcc_unmap_cipher_request(dev, req_ctx, ivsize, src, dst);\n\treturn rc;\n}\n\nvoid cc_unmap_aead_request(struct device *dev, struct aead_request *req)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tunsigned int hw_iv_size = areq_ctx->hw_iv_size;\n\tstruct cc_drvdata *drvdata = dev_get_drvdata(dev);\n\tint src_direction = (req->src != req->dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL);\n\n\tif (areq_ctx->mac_buf_dma_addr) {\n\t\tdma_unmap_single(dev, areq_ctx->mac_buf_dma_addr,\n\t\t\t\t MAX_MAC_SIZE, DMA_BIDIRECTIONAL);\n\t}\n\n\tif (areq_ctx->cipher_mode == DRV_CIPHER_GCTR) {\n\t\tif (areq_ctx->hkey_dma_addr) {\n\t\t\tdma_unmap_single(dev, areq_ctx->hkey_dma_addr,\n\t\t\t\t\t AES_BLOCK_SIZE, DMA_BIDIRECTIONAL);\n\t\t}\n\n\t\tif (areq_ctx->gcm_block_len_dma_addr) {\n\t\t\tdma_unmap_single(dev, areq_ctx->gcm_block_len_dma_addr,\n\t\t\t\t\t AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\t\t}\n\n\t\tif (areq_ctx->gcm_iv_inc1_dma_addr) {\n\t\t\tdma_unmap_single(dev, areq_ctx->gcm_iv_inc1_dma_addr,\n\t\t\t\t\t AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\t\t}\n\n\t\tif (areq_ctx->gcm_iv_inc2_dma_addr) {\n\t\t\tdma_unmap_single(dev, areq_ctx->gcm_iv_inc2_dma_addr,\n\t\t\t\t\t AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\t\t}\n\t}\n\n\tif (areq_ctx->ccm_hdr_size != ccm_header_size_null) {\n\t\tif (areq_ctx->ccm_iv0_dma_addr) {\n\t\t\tdma_unmap_single(dev, areq_ctx->ccm_iv0_dma_addr,\n\t\t\t\t\t AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\t\t}\n\n\t\tdma_unmap_sg(dev, &areq_ctx->ccm_adata_sg, 1, DMA_TO_DEVICE);\n\t}\n\tif (areq_ctx->gen_ctx.iv_dma_addr) {\n\t\tdma_unmap_single(dev, areq_ctx->gen_ctx.iv_dma_addr,\n\t\t\t\t hw_iv_size, DMA_BIDIRECTIONAL);\n\t\tkfree_sensitive(areq_ctx->gen_ctx.iv);\n\t}\n\n\t \n\tif ((areq_ctx->assoc_buff_type == CC_DMA_BUF_MLLI ||\n\t     areq_ctx->data_buff_type == CC_DMA_BUF_MLLI) &&\n\t    (areq_ctx->mlli_params.mlli_virt_addr)) {\n\t\tdev_dbg(dev, \"free MLLI buffer: dma=%pad virt=%pK\\n\",\n\t\t\t&areq_ctx->mlli_params.mlli_dma_addr,\n\t\t\tareq_ctx->mlli_params.mlli_virt_addr);\n\t\tdma_pool_free(areq_ctx->mlli_params.curr_pool,\n\t\t\t      areq_ctx->mlli_params.mlli_virt_addr,\n\t\t\t      areq_ctx->mlli_params.mlli_dma_addr);\n\t}\n\n\tdev_dbg(dev, \"Unmapping src sgl: req->src=%pK areq_ctx->src.nents=%u areq_ctx->assoc.nents=%u assoclen:%u cryptlen=%u\\n\",\n\t\tsg_virt(req->src), areq_ctx->src.nents, areq_ctx->assoc.nents,\n\t\tareq_ctx->assoclen, req->cryptlen);\n\n\tdma_unmap_sg(dev, req->src, areq_ctx->src.mapped_nents, src_direction);\n\tif (req->src != req->dst) {\n\t\tdev_dbg(dev, \"Unmapping dst sgl: req->dst=%pK\\n\",\n\t\t\tsg_virt(req->dst));\n\t\tdma_unmap_sg(dev, req->dst, areq_ctx->dst.mapped_nents, DMA_FROM_DEVICE);\n\t}\n\tif (drvdata->coherent &&\n\t    areq_ctx->gen_ctx.op_type == DRV_CRYPTO_DIRECTION_DECRYPT &&\n\t    req->src == req->dst) {\n\t\t \n\t\tcc_copy_mac(dev, req, CC_SG_FROM_BUF);\n\t}\n}\n\nstatic bool cc_is_icv_frag(unsigned int sgl_nents, unsigned int authsize,\n\t\t\t   u32 last_entry_data_size)\n{\n\treturn ((sgl_nents > 1) && (last_entry_data_size < authsize));\n}\n\nstatic int cc_aead_chain_iv(struct cc_drvdata *drvdata,\n\t\t\t    struct aead_request *req,\n\t\t\t    struct buffer_array *sg_data,\n\t\t\t    bool is_last, bool do_chain)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tunsigned int hw_iv_size = areq_ctx->hw_iv_size;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\tint rc = 0;\n\n\tif (!req->iv) {\n\t\tareq_ctx->gen_ctx.iv_dma_addr = 0;\n\t\tareq_ctx->gen_ctx.iv = NULL;\n\t\tgoto chain_iv_exit;\n\t}\n\n\tareq_ctx->gen_ctx.iv = kmemdup(req->iv, hw_iv_size, flags);\n\tif (!areq_ctx->gen_ctx.iv)\n\t\treturn -ENOMEM;\n\n\tareq_ctx->gen_ctx.iv_dma_addr =\n\t\tdma_map_single(dev, areq_ctx->gen_ctx.iv, hw_iv_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, areq_ctx->gen_ctx.iv_dma_addr)) {\n\t\tdev_err(dev, \"Mapping iv %u B at va=%pK for DMA failed\\n\",\n\t\t\thw_iv_size, req->iv);\n\t\tkfree_sensitive(areq_ctx->gen_ctx.iv);\n\t\tareq_ctx->gen_ctx.iv = NULL;\n\t\trc = -ENOMEM;\n\t\tgoto chain_iv_exit;\n\t}\n\n\tdev_dbg(dev, \"Mapped iv %u B at va=%pK to dma=%pad\\n\",\n\t\thw_iv_size, req->iv, &areq_ctx->gen_ctx.iv_dma_addr);\n\nchain_iv_exit:\n\treturn rc;\n}\n\nstatic int cc_aead_chain_assoc(struct cc_drvdata *drvdata,\n\t\t\t       struct aead_request *req,\n\t\t\t       struct buffer_array *sg_data,\n\t\t\t       bool is_last, bool do_chain)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tint rc = 0;\n\tint mapped_nents = 0;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\tif (!sg_data) {\n\t\trc = -EINVAL;\n\t\tgoto chain_assoc_exit;\n\t}\n\n\tif (areq_ctx->assoclen == 0) {\n\t\tareq_ctx->assoc_buff_type = CC_DMA_BUF_NULL;\n\t\tareq_ctx->assoc.nents = 0;\n\t\tareq_ctx->assoc.mlli_nents = 0;\n\t\tdev_dbg(dev, \"Chain assoc of length 0: buff_type=%s nents=%u\\n\",\n\t\t\tcc_dma_buf_type(areq_ctx->assoc_buff_type),\n\t\t\tareq_ctx->assoc.nents);\n\t\tgoto chain_assoc_exit;\n\t}\n\n\tmapped_nents = sg_nents_for_len(req->src, areq_ctx->assoclen);\n\tif (mapped_nents < 0)\n\t\treturn mapped_nents;\n\n\tif (mapped_nents > LLI_MAX_NUM_OF_ASSOC_DATA_ENTRIES) {\n\t\tdev_err(dev, \"Too many fragments. current %d max %d\\n\",\n\t\t\tmapped_nents, LLI_MAX_NUM_OF_ASSOC_DATA_ENTRIES);\n\t\treturn -ENOMEM;\n\t}\n\tareq_ctx->assoc.nents = mapped_nents;\n\n\t \n\tif (areq_ctx->ccm_hdr_size != ccm_header_size_null) {\n\t\tif ((mapped_nents + 1) > LLI_MAX_NUM_OF_ASSOC_DATA_ENTRIES) {\n\t\t\tdev_err(dev, \"CCM case.Too many fragments. Current %d max %d\\n\",\n\t\t\t\t(areq_ctx->assoc.nents + 1),\n\t\t\t\tLLI_MAX_NUM_OF_ASSOC_DATA_ENTRIES);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto chain_assoc_exit;\n\t\t}\n\t}\n\n\tif (mapped_nents == 1 && areq_ctx->ccm_hdr_size == ccm_header_size_null)\n\t\tareq_ctx->assoc_buff_type = CC_DMA_BUF_DLLI;\n\telse\n\t\tareq_ctx->assoc_buff_type = CC_DMA_BUF_MLLI;\n\n\tif (do_chain || areq_ctx->assoc_buff_type == CC_DMA_BUF_MLLI) {\n\t\tdev_dbg(dev, \"Chain assoc: buff_type=%s nents=%u\\n\",\n\t\t\tcc_dma_buf_type(areq_ctx->assoc_buff_type),\n\t\t\tareq_ctx->assoc.nents);\n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->assoc.nents, req->src,\n\t\t\t\tareq_ctx->assoclen, 0, is_last,\n\t\t\t\t&areq_ctx->assoc.mlli_nents);\n\t\tareq_ctx->assoc_buff_type = CC_DMA_BUF_MLLI;\n\t}\n\nchain_assoc_exit:\n\treturn rc;\n}\n\nstatic void cc_prepare_aead_data_dlli(struct aead_request *req,\n\t\t\t\t      u32 *src_last_bytes, u32 *dst_last_bytes)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tenum drv_crypto_direction direct = areq_ctx->gen_ctx.op_type;\n\tunsigned int authsize = areq_ctx->req_authsize;\n\tstruct scatterlist *sg;\n\tssize_t offset;\n\n\tareq_ctx->is_icv_fragmented = false;\n\n\tif ((req->src == req->dst) || direct == DRV_CRYPTO_DIRECTION_DECRYPT) {\n\t\tsg = areq_ctx->src_sgl;\n\t\toffset = *src_last_bytes - authsize;\n\t} else {\n\t\tsg = areq_ctx->dst_sgl;\n\t\toffset = *dst_last_bytes - authsize;\n\t}\n\n\tareq_ctx->icv_dma_addr = sg_dma_address(sg) + offset;\n\tareq_ctx->icv_virt_addr = sg_virt(sg) + offset;\n}\n\nstatic void cc_prepare_aead_data_mlli(struct cc_drvdata *drvdata,\n\t\t\t\t      struct aead_request *req,\n\t\t\t\t      struct buffer_array *sg_data,\n\t\t\t\t      u32 *src_last_bytes, u32 *dst_last_bytes,\n\t\t\t\t      bool is_last_table)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tenum drv_crypto_direction direct = areq_ctx->gen_ctx.op_type;\n\tunsigned int authsize = areq_ctx->req_authsize;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tstruct scatterlist *sg;\n\n\tif (req->src == req->dst) {\n\t\t \n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->src.nents,\n\t\t\t\tareq_ctx->src_sgl, areq_ctx->cryptlen,\n\t\t\t\tareq_ctx->src_offset, is_last_table,\n\t\t\t\t&areq_ctx->src.mlli_nents);\n\n\t\tareq_ctx->is_icv_fragmented =\n\t\t\tcc_is_icv_frag(areq_ctx->src.nents, authsize,\n\t\t\t\t       *src_last_bytes);\n\n\t\tif (areq_ctx->is_icv_fragmented) {\n\t\t\t \n\t\t\tif (direct == DRV_CRYPTO_DIRECTION_DECRYPT) {\n\t\t\t\t \n\t\t\t\tif (!drvdata->coherent)\n\t\t\t\t\tcc_copy_mac(dev, req, CC_SG_TO_BUF);\n\n\t\t\t\tareq_ctx->icv_virt_addr = areq_ctx->backup_mac;\n\t\t\t} else {\n\t\t\t\tareq_ctx->icv_virt_addr = areq_ctx->mac_buf;\n\t\t\t\tareq_ctx->icv_dma_addr =\n\t\t\t\t\tareq_ctx->mac_buf_dma_addr;\n\t\t\t}\n\t\t} else {  \n\t\t\tsg = &areq_ctx->src_sgl[areq_ctx->src.nents - 1];\n\t\t\t \n\t\t\tareq_ctx->icv_dma_addr = sg_dma_address(sg) +\n\t\t\t\t(*src_last_bytes - authsize);\n\t\t\tareq_ctx->icv_virt_addr = sg_virt(sg) +\n\t\t\t\t(*src_last_bytes - authsize);\n\t\t}\n\n\t} else if (direct == DRV_CRYPTO_DIRECTION_DECRYPT) {\n\t\t \n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->src.nents,\n\t\t\t\tareq_ctx->src_sgl, areq_ctx->cryptlen,\n\t\t\t\tareq_ctx->src_offset, is_last_table,\n\t\t\t\t&areq_ctx->src.mlli_nents);\n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->dst.nents,\n\t\t\t\tareq_ctx->dst_sgl, areq_ctx->cryptlen,\n\t\t\t\tareq_ctx->dst_offset, is_last_table,\n\t\t\t\t&areq_ctx->dst.mlli_nents);\n\n\t\tareq_ctx->is_icv_fragmented =\n\t\t\tcc_is_icv_frag(areq_ctx->src.nents, authsize,\n\t\t\t\t       *src_last_bytes);\n\t\t \n\t\tif (areq_ctx->is_icv_fragmented) {\n\t\t\tcc_copy_mac(dev, req, CC_SG_TO_BUF);\n\t\t\tareq_ctx->icv_virt_addr = areq_ctx->backup_mac;\n\n\t\t} else {  \n\t\t\tsg = &areq_ctx->src_sgl[areq_ctx->src.nents - 1];\n\t\t\t \n\t\t\tareq_ctx->icv_dma_addr = sg_dma_address(sg) +\n\t\t\t\t(*src_last_bytes - authsize);\n\t\t\tareq_ctx->icv_virt_addr = sg_virt(sg) +\n\t\t\t\t(*src_last_bytes - authsize);\n\t\t}\n\n\t} else {\n\t\t \n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->dst.nents,\n\t\t\t\tareq_ctx->dst_sgl, areq_ctx->cryptlen,\n\t\t\t\tareq_ctx->dst_offset, is_last_table,\n\t\t\t\t&areq_ctx->dst.mlli_nents);\n\t\tcc_add_sg_entry(dev, sg_data, areq_ctx->src.nents,\n\t\t\t\tareq_ctx->src_sgl, areq_ctx->cryptlen,\n\t\t\t\tareq_ctx->src_offset, is_last_table,\n\t\t\t\t&areq_ctx->src.mlli_nents);\n\n\t\tareq_ctx->is_icv_fragmented =\n\t\t\tcc_is_icv_frag(areq_ctx->dst.nents, authsize,\n\t\t\t\t       *dst_last_bytes);\n\n\t\tif (!areq_ctx->is_icv_fragmented) {\n\t\t\tsg = &areq_ctx->dst_sgl[areq_ctx->dst.nents - 1];\n\t\t\t \n\t\t\tareq_ctx->icv_dma_addr = sg_dma_address(sg) +\n\t\t\t\t(*dst_last_bytes - authsize);\n\t\t\tareq_ctx->icv_virt_addr = sg_virt(sg) +\n\t\t\t\t(*dst_last_bytes - authsize);\n\t\t} else {\n\t\t\tareq_ctx->icv_dma_addr = areq_ctx->mac_buf_dma_addr;\n\t\t\tareq_ctx->icv_virt_addr = areq_ctx->mac_buf;\n\t\t}\n\t}\n}\n\nstatic int cc_aead_chain_data(struct cc_drvdata *drvdata,\n\t\t\t      struct aead_request *req,\n\t\t\t      struct buffer_array *sg_data,\n\t\t\t      bool is_last_table, bool do_chain)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tenum drv_crypto_direction direct = areq_ctx->gen_ctx.op_type;\n\tunsigned int authsize = areq_ctx->req_authsize;\n\tunsigned int src_last_bytes = 0, dst_last_bytes = 0;\n\tint rc = 0;\n\tu32 src_mapped_nents = 0, dst_mapped_nents = 0;\n\tu32 offset = 0;\n\t \n\tunsigned int size_for_map = req->assoclen + req->cryptlen;\n\tu32 sg_index = 0;\n\tu32 size_to_skip = req->assoclen;\n\tstruct scatterlist *sgl;\n\n\toffset = size_to_skip;\n\n\tif (!sg_data)\n\t\treturn -EINVAL;\n\n\tareq_ctx->src_sgl = req->src;\n\tareq_ctx->dst_sgl = req->dst;\n\n\tsize_for_map += (direct == DRV_CRYPTO_DIRECTION_ENCRYPT) ?\n\t\t\tauthsize : 0;\n\tsrc_mapped_nents = cc_get_sgl_nents(dev, req->src, size_for_map,\n\t\t\t\t\t    &src_last_bytes);\n\tsg_index = areq_ctx->src_sgl->length;\n\t\n\twhile (src_mapped_nents && (sg_index <= size_to_skip)) {\n\t\tsrc_mapped_nents--;\n\t\toffset -= areq_ctx->src_sgl->length;\n\t\tsgl = sg_next(areq_ctx->src_sgl);\n\t\tif (!sgl)\n\t\t\tbreak;\n\t\tareq_ctx->src_sgl = sgl;\n\t\tsg_index += areq_ctx->src_sgl->length;\n\t}\n\tif (src_mapped_nents > LLI_MAX_NUM_OF_DATA_ENTRIES) {\n\t\tdev_err(dev, \"Too many fragments. current %d max %d\\n\",\n\t\t\tsrc_mapped_nents, LLI_MAX_NUM_OF_DATA_ENTRIES);\n\t\treturn -ENOMEM;\n\t}\n\n\tareq_ctx->src.nents = src_mapped_nents;\n\n\tareq_ctx->src_offset = offset;\n\n\tif (req->src != req->dst) {\n\t\tsize_for_map = req->assoclen + req->cryptlen;\n\n\t\tif (direct == DRV_CRYPTO_DIRECTION_ENCRYPT)\n\t\t\tsize_for_map += authsize;\n\t\telse\n\t\t\tsize_for_map -= authsize;\n\n\t\trc = cc_map_sg(dev, req->dst, size_for_map, DMA_FROM_DEVICE,\n\t\t\t       &areq_ctx->dst.mapped_nents,\n\t\t\t       LLI_MAX_NUM_OF_DATA_ENTRIES, &dst_last_bytes,\n\t\t\t       &dst_mapped_nents);\n\t\tif (rc)\n\t\t\tgoto chain_data_exit;\n\t}\n\n\tdst_mapped_nents = cc_get_sgl_nents(dev, req->dst, size_for_map,\n\t\t\t\t\t    &dst_last_bytes);\n\tsg_index = areq_ctx->dst_sgl->length;\n\toffset = size_to_skip;\n\n\t\n\twhile (dst_mapped_nents && sg_index <= size_to_skip) {\n\t\tdst_mapped_nents--;\n\t\toffset -= areq_ctx->dst_sgl->length;\n\t\tsgl = sg_next(areq_ctx->dst_sgl);\n\t\tif (!sgl)\n\t\t\tbreak;\n\t\tareq_ctx->dst_sgl = sgl;\n\t\tsg_index += areq_ctx->dst_sgl->length;\n\t}\n\tif (dst_mapped_nents > LLI_MAX_NUM_OF_DATA_ENTRIES) {\n\t\tdev_err(dev, \"Too many fragments. current %d max %d\\n\",\n\t\t\tdst_mapped_nents, LLI_MAX_NUM_OF_DATA_ENTRIES);\n\t\treturn -ENOMEM;\n\t}\n\tareq_ctx->dst.nents = dst_mapped_nents;\n\tareq_ctx->dst_offset = offset;\n\tif (src_mapped_nents > 1 ||\n\t    dst_mapped_nents  > 1 ||\n\t    do_chain) {\n\t\tareq_ctx->data_buff_type = CC_DMA_BUF_MLLI;\n\t\tcc_prepare_aead_data_mlli(drvdata, req, sg_data,\n\t\t\t\t\t  &src_last_bytes, &dst_last_bytes,\n\t\t\t\t\t  is_last_table);\n\t} else {\n\t\tareq_ctx->data_buff_type = CC_DMA_BUF_DLLI;\n\t\tcc_prepare_aead_data_dlli(req, &src_last_bytes,\n\t\t\t\t\t  &dst_last_bytes);\n\t}\n\nchain_data_exit:\n\treturn rc;\n}\n\nstatic void cc_update_aead_mlli_nents(struct cc_drvdata *drvdata,\n\t\t\t\t      struct aead_request *req)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tu32 curr_mlli_size = 0;\n\n\tif (areq_ctx->assoc_buff_type == CC_DMA_BUF_MLLI) {\n\t\tareq_ctx->assoc.sram_addr = drvdata->mlli_sram_addr;\n\t\tcurr_mlli_size = areq_ctx->assoc.mlli_nents *\n\t\t\t\t\t\tLLI_ENTRY_BYTE_SIZE;\n\t}\n\n\tif (areq_ctx->data_buff_type == CC_DMA_BUF_MLLI) {\n\t\t \n\t\tif (req->src == req->dst) {\n\t\t\tareq_ctx->dst.mlli_nents = areq_ctx->src.mlli_nents;\n\t\t\tareq_ctx->src.sram_addr = drvdata->mlli_sram_addr +\n\t\t\t\t\t\t\t\tcurr_mlli_size;\n\t\t\tareq_ctx->dst.sram_addr = areq_ctx->src.sram_addr;\n\t\t\tif (!areq_ctx->is_single_pass)\n\t\t\t\tareq_ctx->assoc.mlli_nents +=\n\t\t\t\t\tareq_ctx->src.mlli_nents;\n\t\t} else {\n\t\t\tif (areq_ctx->gen_ctx.op_type ==\n\t\t\t\t\tDRV_CRYPTO_DIRECTION_DECRYPT) {\n\t\t\t\tareq_ctx->src.sram_addr =\n\t\t\t\t\t\tdrvdata->mlli_sram_addr +\n\t\t\t\t\t\t\t\tcurr_mlli_size;\n\t\t\t\tareq_ctx->dst.sram_addr =\n\t\t\t\t\t\tareq_ctx->src.sram_addr +\n\t\t\t\t\t\tareq_ctx->src.mlli_nents *\n\t\t\t\t\t\tLLI_ENTRY_BYTE_SIZE;\n\t\t\t\tif (!areq_ctx->is_single_pass)\n\t\t\t\t\tareq_ctx->assoc.mlli_nents +=\n\t\t\t\t\t\tareq_ctx->src.mlli_nents;\n\t\t\t} else {\n\t\t\t\tareq_ctx->dst.sram_addr =\n\t\t\t\t\t\tdrvdata->mlli_sram_addr +\n\t\t\t\t\t\t\t\tcurr_mlli_size;\n\t\t\t\tareq_ctx->src.sram_addr =\n\t\t\t\t\t\tareq_ctx->dst.sram_addr +\n\t\t\t\t\t\tareq_ctx->dst.mlli_nents *\n\t\t\t\t\t\tLLI_ENTRY_BYTE_SIZE;\n\t\t\t\tif (!areq_ctx->is_single_pass)\n\t\t\t\t\tareq_ctx->assoc.mlli_nents +=\n\t\t\t\t\t\tareq_ctx->dst.mlli_nents;\n\t\t\t}\n\t\t}\n\t}\n}\n\nint cc_map_aead_request(struct cc_drvdata *drvdata, struct aead_request *req)\n{\n\tstruct aead_req_ctx *areq_ctx = aead_request_ctx_dma(req);\n\tstruct mlli_params *mlli_params = &areq_ctx->mlli_params;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tstruct buffer_array sg_data;\n\tunsigned int authsize = areq_ctx->req_authsize;\n\tint rc = 0;\n\tdma_addr_t dma_addr;\n\tu32 mapped_nents = 0;\n\tu32 dummy = 0;  \n\tu32 size_to_map;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tmlli_params->curr_pool = NULL;\n\tsg_data.num_of_buffers = 0;\n\n\t \n\tif (drvdata->coherent &&\n\t    areq_ctx->gen_ctx.op_type == DRV_CRYPTO_DIRECTION_DECRYPT &&\n\t    req->src == req->dst)\n\t\tcc_copy_mac(dev, req, CC_SG_TO_BUF);\n\n\t \n\tareq_ctx->cryptlen = (areq_ctx->gen_ctx.op_type ==\n\t\t\t\t DRV_CRYPTO_DIRECTION_ENCRYPT) ?\n\t\t\t\treq->cryptlen :\n\t\t\t\t(req->cryptlen - authsize);\n\n\tdma_addr = dma_map_single(dev, areq_ctx->mac_buf, MAX_MAC_SIZE,\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tdev_err(dev, \"Mapping mac_buf %u B at va=%pK for DMA failed\\n\",\n\t\t\tMAX_MAC_SIZE, areq_ctx->mac_buf);\n\t\trc = -ENOMEM;\n\t\tgoto aead_map_failure;\n\t}\n\tareq_ctx->mac_buf_dma_addr = dma_addr;\n\n\tif (areq_ctx->ccm_hdr_size != ccm_header_size_null) {\n\t\tvoid *addr = areq_ctx->ccm_config + CCM_CTR_COUNT_0_OFFSET;\n\n\t\tdma_addr = dma_map_single(dev, addr, AES_BLOCK_SIZE,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping mac_buf %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tAES_BLOCK_SIZE, addr);\n\t\t\tareq_ctx->ccm_iv0_dma_addr = 0;\n\t\t\trc = -ENOMEM;\n\t\t\tgoto aead_map_failure;\n\t\t}\n\t\tareq_ctx->ccm_iv0_dma_addr = dma_addr;\n\n\t\trc = cc_set_aead_conf_buf(dev, areq_ctx, areq_ctx->ccm_config,\n\t\t\t\t\t  &sg_data, areq_ctx->assoclen);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t}\n\n\tif (areq_ctx->cipher_mode == DRV_CIPHER_GCTR) {\n\t\tdma_addr = dma_map_single(dev, areq_ctx->hkey, AES_BLOCK_SIZE,\n\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping hkey %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tAES_BLOCK_SIZE, areq_ctx->hkey);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto aead_map_failure;\n\t\t}\n\t\tareq_ctx->hkey_dma_addr = dma_addr;\n\n\t\tdma_addr = dma_map_single(dev, &areq_ctx->gcm_len_block,\n\t\t\t\t\t  AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping gcm_len_block %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tAES_BLOCK_SIZE, &areq_ctx->gcm_len_block);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto aead_map_failure;\n\t\t}\n\t\tareq_ctx->gcm_block_len_dma_addr = dma_addr;\n\n\t\tdma_addr = dma_map_single(dev, areq_ctx->gcm_iv_inc1,\n\t\t\t\t\t  AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping gcm_iv_inc1 %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tAES_BLOCK_SIZE, (areq_ctx->gcm_iv_inc1));\n\t\t\tareq_ctx->gcm_iv_inc1_dma_addr = 0;\n\t\t\trc = -ENOMEM;\n\t\t\tgoto aead_map_failure;\n\t\t}\n\t\tareq_ctx->gcm_iv_inc1_dma_addr = dma_addr;\n\n\t\tdma_addr = dma_map_single(dev, areq_ctx->gcm_iv_inc2,\n\t\t\t\t\t  AES_BLOCK_SIZE, DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping gcm_iv_inc2 %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tAES_BLOCK_SIZE, (areq_ctx->gcm_iv_inc2));\n\t\t\tareq_ctx->gcm_iv_inc2_dma_addr = 0;\n\t\t\trc = -ENOMEM;\n\t\t\tgoto aead_map_failure;\n\t\t}\n\t\tareq_ctx->gcm_iv_inc2_dma_addr = dma_addr;\n\t}\n\n\tsize_to_map = req->cryptlen + req->assoclen;\n\t \n\tif ((areq_ctx->gen_ctx.op_type == DRV_CRYPTO_DIRECTION_ENCRYPT) &&\n\t   (req->src == req->dst)) {\n\t\tsize_to_map += authsize;\n\t}\n\n\trc = cc_map_sg(dev, req->src, size_to_map,\n\t\t       (req->src != req->dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL),\n\t\t       &areq_ctx->src.mapped_nents,\n\t\t       (LLI_MAX_NUM_OF_ASSOC_DATA_ENTRIES +\n\t\t\tLLI_MAX_NUM_OF_DATA_ENTRIES),\n\t\t       &dummy, &mapped_nents);\n\tif (rc)\n\t\tgoto aead_map_failure;\n\n\tif (areq_ctx->is_single_pass) {\n\t\t \n\t\trc = cc_aead_chain_assoc(drvdata, req, &sg_data, true, false);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t\trc = cc_aead_chain_iv(drvdata, req, &sg_data, true, false);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t\trc = cc_aead_chain_data(drvdata, req, &sg_data, true, false);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t} else {  \n\t\t \n\t\trc = cc_aead_chain_assoc(drvdata, req, &sg_data, false, true);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t\trc = cc_aead_chain_iv(drvdata, req, &sg_data, false, true);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t\trc = cc_aead_chain_data(drvdata, req, &sg_data, true, true);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\t}\n\n\t \n\tif (areq_ctx->assoc_buff_type == CC_DMA_BUF_MLLI ||\n\t    areq_ctx->data_buff_type == CC_DMA_BUF_MLLI) {\n\t\tmlli_params->curr_pool = drvdata->mlli_buffs_pool;\n\t\trc = cc_generate_mlli(dev, &sg_data, mlli_params, flags);\n\t\tif (rc)\n\t\t\tgoto aead_map_failure;\n\n\t\tcc_update_aead_mlli_nents(drvdata, req);\n\t\tdev_dbg(dev, \"assoc params mn %d\\n\",\n\t\t\tareq_ctx->assoc.mlli_nents);\n\t\tdev_dbg(dev, \"src params mn %d\\n\", areq_ctx->src.mlli_nents);\n\t\tdev_dbg(dev, \"dst params mn %d\\n\", areq_ctx->dst.mlli_nents);\n\t}\n\treturn 0;\n\naead_map_failure:\n\tcc_unmap_aead_request(dev, req);\n\treturn rc;\n}\n\nint cc_map_hash_request_final(struct cc_drvdata *drvdata, void *ctx,\n\t\t\t      struct scatterlist *src, unsigned int nbytes,\n\t\t\t      bool do_update, gfp_t flags)\n{\n\tstruct ahash_req_ctx *areq_ctx = (struct ahash_req_ctx *)ctx;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tu8 *curr_buff = cc_hash_buf(areq_ctx);\n\tu32 *curr_buff_cnt = cc_hash_buf_cnt(areq_ctx);\n\tstruct mlli_params *mlli_params = &areq_ctx->mlli_params;\n\tstruct buffer_array sg_data;\n\tint rc = 0;\n\tu32 dummy = 0;\n\tu32 mapped_nents = 0;\n\n\tdev_dbg(dev, \"final params : curr_buff=%pK curr_buff_cnt=0x%X nbytes = 0x%X src=%pK curr_index=%u\\n\",\n\t\tcurr_buff, *curr_buff_cnt, nbytes, src, areq_ctx->buff_index);\n\t \n\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_NULL;\n\tmlli_params->curr_pool = NULL;\n\tsg_data.num_of_buffers = 0;\n\tareq_ctx->in_nents = 0;\n\n\tif (nbytes == 0 && *curr_buff_cnt == 0) {\n\t\t \n\t\treturn 0;\n\t}\n\n\t \n\tif (*curr_buff_cnt) {\n\t\trc = cc_set_hash_buf(dev, areq_ctx, curr_buff, *curr_buff_cnt,\n\t\t\t\t     &sg_data);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (src && nbytes > 0 && do_update) {\n\t\trc = cc_map_sg(dev, src, nbytes, DMA_TO_DEVICE,\n\t\t\t       &areq_ctx->in_nents, LLI_MAX_NUM_OF_DATA_ENTRIES,\n\t\t\t       &dummy, &mapped_nents);\n\t\tif (rc)\n\t\t\tgoto unmap_curr_buff;\n\t\tif (src && mapped_nents == 1 &&\n\t\t    areq_ctx->data_dma_buf_type == CC_DMA_BUF_NULL) {\n\t\t\tmemcpy(areq_ctx->buff_sg, src,\n\t\t\t       sizeof(struct scatterlist));\n\t\t\tareq_ctx->buff_sg->length = nbytes;\n\t\t\tareq_ctx->curr_sg = areq_ctx->buff_sg;\n\t\t\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_DLLI;\n\t\t} else {\n\t\t\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_MLLI;\n\t\t}\n\t}\n\n\t \n\tif (areq_ctx->data_dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\tmlli_params->curr_pool = drvdata->mlli_buffs_pool;\n\t\t \n\t\tcc_add_sg_entry(dev, &sg_data, areq_ctx->in_nents, src, nbytes,\n\t\t\t\t0, true, &areq_ctx->mlli_nents);\n\t\trc = cc_generate_mlli(dev, &sg_data, mlli_params, flags);\n\t\tif (rc)\n\t\t\tgoto fail_unmap_din;\n\t}\n\t \n\tareq_ctx->buff_index = (areq_ctx->buff_index ^ 1);\n\tdev_dbg(dev, \"areq_ctx->data_dma_buf_type = %s\\n\",\n\t\tcc_dma_buf_type(areq_ctx->data_dma_buf_type));\n\treturn 0;\n\nfail_unmap_din:\n\tdma_unmap_sg(dev, src, areq_ctx->in_nents, DMA_TO_DEVICE);\n\nunmap_curr_buff:\n\tif (*curr_buff_cnt)\n\t\tdma_unmap_sg(dev, areq_ctx->buff_sg, 1, DMA_TO_DEVICE);\n\n\treturn rc;\n}\n\nint cc_map_hash_request_update(struct cc_drvdata *drvdata, void *ctx,\n\t\t\t       struct scatterlist *src, unsigned int nbytes,\n\t\t\t       unsigned int block_size, gfp_t flags)\n{\n\tstruct ahash_req_ctx *areq_ctx = (struct ahash_req_ctx *)ctx;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tu8 *curr_buff = cc_hash_buf(areq_ctx);\n\tu32 *curr_buff_cnt = cc_hash_buf_cnt(areq_ctx);\n\tu8 *next_buff = cc_next_buf(areq_ctx);\n\tu32 *next_buff_cnt = cc_next_buf_cnt(areq_ctx);\n\tstruct mlli_params *mlli_params = &areq_ctx->mlli_params;\n\tunsigned int update_data_len;\n\tu32 total_in_len = nbytes + *curr_buff_cnt;\n\tstruct buffer_array sg_data;\n\tunsigned int swap_index = 0;\n\tint rc = 0;\n\tu32 dummy = 0;\n\tu32 mapped_nents = 0;\n\n\tdev_dbg(dev, \" update params : curr_buff=%pK curr_buff_cnt=0x%X nbytes=0x%X src=%pK curr_index=%u\\n\",\n\t\tcurr_buff, *curr_buff_cnt, nbytes, src, areq_ctx->buff_index);\n\t \n\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_NULL;\n\tmlli_params->curr_pool = NULL;\n\tareq_ctx->curr_sg = NULL;\n\tsg_data.num_of_buffers = 0;\n\tareq_ctx->in_nents = 0;\n\n\tif (total_in_len < block_size) {\n\t\tdev_dbg(dev, \" less than one block: curr_buff=%pK *curr_buff_cnt=0x%X copy_to=%pK\\n\",\n\t\t\tcurr_buff, *curr_buff_cnt, &curr_buff[*curr_buff_cnt]);\n\t\tareq_ctx->in_nents = sg_nents_for_len(src, nbytes);\n\t\tsg_copy_to_buffer(src, areq_ctx->in_nents,\n\t\t\t\t  &curr_buff[*curr_buff_cnt], nbytes);\n\t\t*curr_buff_cnt += nbytes;\n\t\treturn 1;\n\t}\n\n\t \n\t*next_buff_cnt = total_in_len & (block_size - 1);\n\t \n\tupdate_data_len = total_in_len - *next_buff_cnt;\n\n\tdev_dbg(dev, \" temp length : *next_buff_cnt=0x%X update_data_len=0x%X\\n\",\n\t\t*next_buff_cnt, update_data_len);\n\n\t \n\tif (*next_buff_cnt) {\n\t\tdev_dbg(dev, \" handle residue: next buff %pK skip data %u residue %u\\n\",\n\t\t\tnext_buff, (update_data_len - *curr_buff_cnt),\n\t\t\t*next_buff_cnt);\n\t\tcc_copy_sg_portion(dev, next_buff, src,\n\t\t\t\t   (update_data_len - *curr_buff_cnt),\n\t\t\t\t   nbytes, CC_SG_TO_BUF);\n\t\t \n\t\tswap_index = 1;\n\t}\n\n\tif (*curr_buff_cnt) {\n\t\trc = cc_set_hash_buf(dev, areq_ctx, curr_buff, *curr_buff_cnt,\n\t\t\t\t     &sg_data);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\t \n\t\tswap_index = 1;\n\t}\n\n\tif (update_data_len > *curr_buff_cnt) {\n\t\trc = cc_map_sg(dev, src, (update_data_len - *curr_buff_cnt),\n\t\t\t       DMA_TO_DEVICE, &areq_ctx->in_nents,\n\t\t\t       LLI_MAX_NUM_OF_DATA_ENTRIES, &dummy,\n\t\t\t       &mapped_nents);\n\t\tif (rc)\n\t\t\tgoto unmap_curr_buff;\n\t\tif (mapped_nents == 1 &&\n\t\t    areq_ctx->data_dma_buf_type == CC_DMA_BUF_NULL) {\n\t\t\t \n\t\t\tmemcpy(areq_ctx->buff_sg, src,\n\t\t\t       sizeof(struct scatterlist));\n\t\t\tareq_ctx->buff_sg->length = update_data_len;\n\t\t\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_DLLI;\n\t\t\tareq_ctx->curr_sg = areq_ctx->buff_sg;\n\t\t} else {\n\t\t\tareq_ctx->data_dma_buf_type = CC_DMA_BUF_MLLI;\n\t\t}\n\t}\n\n\tif (areq_ctx->data_dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\tmlli_params->curr_pool = drvdata->mlli_buffs_pool;\n\t\t \n\t\tcc_add_sg_entry(dev, &sg_data, areq_ctx->in_nents, src,\n\t\t\t\t(update_data_len - *curr_buff_cnt), 0, true,\n\t\t\t\t&areq_ctx->mlli_nents);\n\t\trc = cc_generate_mlli(dev, &sg_data, mlli_params, flags);\n\t\tif (rc)\n\t\t\tgoto fail_unmap_din;\n\t}\n\tareq_ctx->buff_index = (areq_ctx->buff_index ^ swap_index);\n\n\treturn 0;\n\nfail_unmap_din:\n\tdma_unmap_sg(dev, src, areq_ctx->in_nents, DMA_TO_DEVICE);\n\nunmap_curr_buff:\n\tif (*curr_buff_cnt)\n\t\tdma_unmap_sg(dev, areq_ctx->buff_sg, 1, DMA_TO_DEVICE);\n\n\treturn rc;\n}\n\nvoid cc_unmap_hash_request(struct device *dev, void *ctx,\n\t\t\t   struct scatterlist *src, bool do_revert)\n{\n\tstruct ahash_req_ctx *areq_ctx = (struct ahash_req_ctx *)ctx;\n\tu32 *prev_len = cc_next_buf_cnt(areq_ctx);\n\n\t \n\tif (areq_ctx->mlli_params.curr_pool) {\n\t\tdev_dbg(dev, \"free MLLI buffer: dma=%pad virt=%pK\\n\",\n\t\t\t&areq_ctx->mlli_params.mlli_dma_addr,\n\t\t\tareq_ctx->mlli_params.mlli_virt_addr);\n\t\tdma_pool_free(areq_ctx->mlli_params.curr_pool,\n\t\t\t      areq_ctx->mlli_params.mlli_virt_addr,\n\t\t\t      areq_ctx->mlli_params.mlli_dma_addr);\n\t}\n\n\tif (src && areq_ctx->in_nents) {\n\t\tdev_dbg(dev, \"Unmapped sg src: virt=%pK dma=%pad len=0x%X\\n\",\n\t\t\tsg_virt(src), &sg_dma_address(src), sg_dma_len(src));\n\t\tdma_unmap_sg(dev, src,\n\t\t\t     areq_ctx->in_nents, DMA_TO_DEVICE);\n\t}\n\n\tif (*prev_len) {\n\t\tdev_dbg(dev, \"Unmapped buffer: areq_ctx->buff_sg=%pK dma=%pad len 0x%X\\n\",\n\t\t\tsg_virt(areq_ctx->buff_sg),\n\t\t\t&sg_dma_address(areq_ctx->buff_sg),\n\t\t\tsg_dma_len(areq_ctx->buff_sg));\n\t\tdma_unmap_sg(dev, areq_ctx->buff_sg, 1, DMA_TO_DEVICE);\n\t\tif (!do_revert) {\n\t\t\t \n\t\t\t*prev_len = 0;\n\t\t} else {\n\t\t\tareq_ctx->buff_index ^= 1;\n\t\t}\n\t}\n}\n\nint cc_buffer_mgr_init(struct cc_drvdata *drvdata)\n{\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\tdrvdata->mlli_buffs_pool =\n\t\tdma_pool_create(\"dx_single_mlli_tables\", dev,\n\t\t\t\tMAX_NUM_OF_TOTAL_MLLI_ENTRIES *\n\t\t\t\tLLI_ENTRY_BYTE_SIZE,\n\t\t\t\tMLLI_TABLE_MIN_ALIGNMENT, 0);\n\n\tif (!drvdata->mlli_buffs_pool)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint cc_buffer_mgr_fini(struct cc_drvdata *drvdata)\n{\n\tdma_pool_destroy(drvdata->mlli_buffs_pool);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}