{
  "module_name": "cc_cipher.c",
  "hash_id": "7e3d94d0dcd81521db808fc8c4082ddb3371fc5b1da4634c8bc6c947c31fd897",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccree/cc_cipher.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <crypto/algapi.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/internal/des.h>\n#include <crypto/xts.h>\n#include <crypto/sm4.h>\n#include <crypto/scatterwalk.h>\n\n#include \"cc_driver.h\"\n#include \"cc_lli_defs.h\"\n#include \"cc_buffer_mgr.h\"\n#include \"cc_cipher.h\"\n#include \"cc_request_mgr.h\"\n\n#define MAX_SKCIPHER_SEQ_LEN 6\n\n#define template_skcipher\ttemplate_u.skcipher\n\nstruct cc_user_key_info {\n\tu8 *key;\n\tdma_addr_t key_dma_addr;\n};\n\nstruct cc_hw_key_info {\n\tenum cc_hw_crypto_key key1_slot;\n\tenum cc_hw_crypto_key key2_slot;\n};\n\nstruct cc_cpp_key_info {\n\tu8 slot;\n\tenum cc_cpp_alg alg;\n};\n\nenum cc_key_type {\n\tCC_UNPROTECTED_KEY,\t\t \n\tCC_HW_PROTECTED_KEY,\t\t \n\tCC_POLICY_PROTECTED_KEY,\t \n\tCC_INVALID_PROTECTED_KEY\t \n};\n\nstruct cc_cipher_ctx {\n\tstruct cc_drvdata *drvdata;\n\tint keylen;\n\tint cipher_mode;\n\tint flow_mode;\n\tunsigned int flags;\n\tenum cc_key_type key_type;\n\tstruct cc_user_key_info user;\n\tunion {\n\t\tstruct cc_hw_key_info hw;\n\t\tstruct cc_cpp_key_info cpp;\n\t};\n\tstruct crypto_shash *shash_tfm;\n\tstruct crypto_skcipher *fallback_tfm;\n\tbool fallback_on;\n};\n\nstatic void cc_cipher_complete(struct device *dev, void *cc_req, int err);\n\nstatic inline enum cc_key_type cc_key_type(struct crypto_tfm *tfm)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\n\treturn ctx_p->key_type;\n}\n\nstatic int validate_keys_sizes(struct cc_cipher_ctx *ctx_p, u32 size)\n{\n\tswitch (ctx_p->flow_mode) {\n\tcase S_DIN_to_AES:\n\t\tswitch (size) {\n\t\tcase CC_AES_128_BIT_KEY_SIZE:\n\t\tcase CC_AES_192_BIT_KEY_SIZE:\n\t\t\tif (ctx_p->cipher_mode != DRV_CIPHER_XTS)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase CC_AES_256_BIT_KEY_SIZE:\n\t\t\treturn 0;\n\t\tcase (CC_AES_192_BIT_KEY_SIZE * 2):\n\t\tcase (CC_AES_256_BIT_KEY_SIZE * 2):\n\t\t\tif (ctx_p->cipher_mode == DRV_CIPHER_XTS ||\n\t\t\t    ctx_p->cipher_mode == DRV_CIPHER_ESSIV)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase S_DIN_to_DES:\n\t\tif (size == DES3_EDE_KEY_SIZE || size == DES_KEY_SIZE)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase S_DIN_to_SM4:\n\t\tif (size == SM4_KEY_SIZE)\n\t\t\treturn 0;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int validate_data_size(struct cc_cipher_ctx *ctx_p,\n\t\t\t      unsigned int size)\n{\n\tswitch (ctx_p->flow_mode) {\n\tcase S_DIN_to_AES:\n\t\tswitch (ctx_p->cipher_mode) {\n\t\tcase DRV_CIPHER_XTS:\n\t\tcase DRV_CIPHER_CBC_CTS:\n\t\t\tif (size >= AES_BLOCK_SIZE)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase DRV_CIPHER_OFB:\n\t\tcase DRV_CIPHER_CTR:\n\t\t\t\treturn 0;\n\t\tcase DRV_CIPHER_ECB:\n\t\tcase DRV_CIPHER_CBC:\n\t\tcase DRV_CIPHER_ESSIV:\n\t\t\tif (IS_ALIGNED(size, AES_BLOCK_SIZE))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase S_DIN_to_DES:\n\t\tif (IS_ALIGNED(size, DES_BLOCK_SIZE))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase S_DIN_to_SM4:\n\t\tswitch (ctx_p->cipher_mode) {\n\t\tcase DRV_CIPHER_CTR:\n\t\t\treturn 0;\n\t\tcase DRV_CIPHER_ECB:\n\t\tcase DRV_CIPHER_CBC:\n\t\t\tif (IS_ALIGNED(size, SM4_BLOCK_SIZE))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int cc_cipher_init(struct crypto_tfm *tfm)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct cc_crypto_alg *cc_alg =\n\t\t\tcontainer_of(tfm->__crt_alg, struct cc_crypto_alg,\n\t\t\t\t     skcipher_alg.base);\n\tstruct device *dev = drvdata_to_dev(cc_alg->drvdata);\n\tunsigned int max_key_buf_size = cc_alg->skcipher_alg.max_keysize;\n\tunsigned int fallback_req_size = 0;\n\n\tdev_dbg(dev, \"Initializing context @%p for %s\\n\", ctx_p,\n\t\tcrypto_tfm_alg_name(tfm));\n\n\tctx_p->cipher_mode = cc_alg->cipher_mode;\n\tctx_p->flow_mode = cc_alg->flow_mode;\n\tctx_p->drvdata = cc_alg->drvdata;\n\n\tif (ctx_p->cipher_mode == DRV_CIPHER_ESSIV) {\n\t\tconst char *name = crypto_tfm_alg_name(tfm);\n\n\t\t \n\t\tctx_p->shash_tfm = crypto_alloc_shash(\"sha256\", 0, 0);\n\t\tif (IS_ERR(ctx_p->shash_tfm)) {\n\t\t\tdev_err(dev, \"Error allocating hash tfm for ESSIV.\\n\");\n\t\t\treturn PTR_ERR(ctx_p->shash_tfm);\n\t\t}\n\t\tmax_key_buf_size <<= 1;\n\n\t\t \n\t\tctx_p->fallback_tfm =\n\t\t\tcrypto_alloc_skcipher(name, 0, CRYPTO_ALG_NEED_FALLBACK | CRYPTO_ALG_ASYNC);\n\n\t\tif (IS_ERR(ctx_p->fallback_tfm)) {\n\t\t\t \n\t\t\tdev_warn(dev, \"Error allocating fallback algo %s. Some modes may be available.\\n\",\n\t\t\t       name);\n\t\t\tctx_p->fallback_tfm = NULL;\n\t\t} else {\n\t\t\tfallback_req_size = crypto_skcipher_reqsize(ctx_p->fallback_tfm);\n\t\t}\n\t}\n\n\tcrypto_skcipher_set_reqsize(__crypto_skcipher_cast(tfm),\n\t\t\t\t    sizeof(struct cipher_req_ctx) + fallback_req_size);\n\n\t \n\tctx_p->user.key = kzalloc(max_key_buf_size, GFP_KERNEL);\n\tif (!ctx_p->user.key)\n\t\tgoto free_fallback;\n\n\tdev_dbg(dev, \"Allocated key buffer in context. key=@%p\\n\",\n\t\tctx_p->user.key);\n\n\t \n\tctx_p->user.key_dma_addr = dma_map_single(dev, ctx_p->user.key,\n\t\t\t\t\t\t  max_key_buf_size,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, ctx_p->user.key_dma_addr)) {\n\t\tdev_err(dev, \"Mapping Key %u B at va=%pK for DMA failed\\n\",\n\t\t\tmax_key_buf_size, ctx_p->user.key);\n\t\tgoto free_key;\n\t}\n\tdev_dbg(dev, \"Mapped key %u B at va=%pK to dma=%pad\\n\",\n\t\tmax_key_buf_size, ctx_p->user.key, &ctx_p->user.key_dma_addr);\n\n\treturn 0;\n\nfree_key:\n\tkfree(ctx_p->user.key);\nfree_fallback:\n\tcrypto_free_skcipher(ctx_p->fallback_tfm);\n\tcrypto_free_shash(ctx_p->shash_tfm);\n\n\treturn -ENOMEM;\n}\n\nstatic void cc_cipher_exit(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\tstruct cc_crypto_alg *cc_alg =\n\t\t\tcontainer_of(alg, struct cc_crypto_alg,\n\t\t\t\t     skcipher_alg.base);\n\tunsigned int max_key_buf_size = cc_alg->skcipher_alg.max_keysize;\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\n\tdev_dbg(dev, \"Clearing context @%p for %s\\n\",\n\t\tcrypto_tfm_ctx(tfm), crypto_tfm_alg_name(tfm));\n\n\tif (ctx_p->cipher_mode == DRV_CIPHER_ESSIV) {\n\t\t \n\t\tcrypto_free_shash(ctx_p->shash_tfm);\n\t\tctx_p->shash_tfm = NULL;\n\t\tcrypto_free_skcipher(ctx_p->fallback_tfm);\n\t\tctx_p->fallback_tfm = NULL;\n\t}\n\n\t \n\tdma_unmap_single(dev, ctx_p->user.key_dma_addr, max_key_buf_size,\n\t\t\t DMA_TO_DEVICE);\n\tdev_dbg(dev, \"Unmapped key buffer key_dma_addr=%pad\\n\",\n\t\t&ctx_p->user.key_dma_addr);\n\n\t \n\tdev_dbg(dev, \"Free key buffer in context. key=@%p\\n\", ctx_p->user.key);\n\tkfree_sensitive(ctx_p->user.key);\n}\n\nstruct tdes_keys {\n\tu8\tkey1[DES_KEY_SIZE];\n\tu8\tkey2[DES_KEY_SIZE];\n\tu8\tkey3[DES_KEY_SIZE];\n};\n\nstatic enum cc_hw_crypto_key cc_slot_to_hw_key(u8 slot_num)\n{\n\tswitch (slot_num) {\n\tcase 0:\n\t\treturn KFDE0_KEY;\n\tcase 1:\n\t\treturn KFDE1_KEY;\n\tcase 2:\n\t\treturn KFDE2_KEY;\n\tcase 3:\n\t\treturn KFDE3_KEY;\n\t}\n\treturn END_OF_KEYS;\n}\n\nstatic u8 cc_slot_to_cpp_key(u8 slot_num)\n{\n\treturn (slot_num - CC_FIRST_CPP_KEY_SLOT);\n}\n\nstatic inline enum cc_key_type cc_slot_to_key_type(u8 slot_num)\n{\n\tif (slot_num >= CC_FIRST_HW_KEY_SLOT && slot_num <= CC_LAST_HW_KEY_SLOT)\n\t\treturn CC_HW_PROTECTED_KEY;\n\telse if (slot_num >=  CC_FIRST_CPP_KEY_SLOT &&\n\t\t slot_num <=  CC_LAST_CPP_KEY_SLOT)\n\t\treturn CC_POLICY_PROTECTED_KEY;\n\telse\n\t\treturn CC_INVALID_PROTECTED_KEY;\n}\n\nstatic int cc_cipher_sethkey(struct crypto_skcipher *sktfm, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(sktfm);\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tstruct cc_hkey_info hki;\n\n\tdev_dbg(dev, \"Setting HW key in context @%p for %s. keylen=%u\\n\",\n\t\tctx_p, crypto_tfm_alg_name(tfm), keylen);\n\tdump_byte_array(\"key\", key, keylen);\n\n\t \n\n\t \n\tif (keylen != sizeof(hki)) {\n\t\tdev_err(dev, \"Unsupported protected key size %d.\\n\", keylen);\n\t\treturn -EINVAL;\n\t}\n\n\tmemcpy(&hki, key, keylen);\n\n\t \n\tkeylen = hki.keylen;\n\n\tif (validate_keys_sizes(ctx_p, keylen)) {\n\t\tdev_dbg(dev, \"Unsupported key size %d.\\n\", keylen);\n\t\treturn -EINVAL;\n\t}\n\n\tctx_p->keylen = keylen;\n\tctx_p->fallback_on = false;\n\n\tswitch (cc_slot_to_key_type(hki.hw_key1)) {\n\tcase CC_HW_PROTECTED_KEY:\n\t\tif (ctx_p->flow_mode == S_DIN_to_SM4) {\n\t\t\tdev_err(dev, \"Only AES HW protected keys are supported\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tctx_p->hw.key1_slot = cc_slot_to_hw_key(hki.hw_key1);\n\t\tif (ctx_p->hw.key1_slot == END_OF_KEYS) {\n\t\t\tdev_err(dev, \"Unsupported hw key1 number (%d)\\n\",\n\t\t\t\thki.hw_key1);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ctx_p->cipher_mode == DRV_CIPHER_XTS ||\n\t\t    ctx_p->cipher_mode == DRV_CIPHER_ESSIV) {\n\t\t\tif (hki.hw_key1 == hki.hw_key2) {\n\t\t\t\tdev_err(dev, \"Illegal hw key numbers (%d,%d)\\n\",\n\t\t\t\t\thki.hw_key1, hki.hw_key2);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tctx_p->hw.key2_slot = cc_slot_to_hw_key(hki.hw_key2);\n\t\t\tif (ctx_p->hw.key2_slot == END_OF_KEYS) {\n\t\t\t\tdev_err(dev, \"Unsupported hw key2 number (%d)\\n\",\n\t\t\t\t\thki.hw_key2);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tctx_p->key_type = CC_HW_PROTECTED_KEY;\n\t\tdev_dbg(dev, \"HW protected key  %d/%d set\\n.\",\n\t\t\tctx_p->hw.key1_slot, ctx_p->hw.key2_slot);\n\t\tbreak;\n\n\tcase CC_POLICY_PROTECTED_KEY:\n\t\tif (ctx_p->drvdata->hw_rev < CC_HW_REV_713) {\n\t\t\tdev_err(dev, \"CPP keys not supported in this hardware revision.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ctx_p->cipher_mode != DRV_CIPHER_CBC &&\n\t\t    ctx_p->cipher_mode != DRV_CIPHER_CTR) {\n\t\t\tdev_err(dev, \"CPP keys only supported in CBC or CTR modes.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tctx_p->cpp.slot = cc_slot_to_cpp_key(hki.hw_key1);\n\t\tif (ctx_p->flow_mode == S_DIN_to_AES)\n\t\t\tctx_p->cpp.alg = CC_CPP_AES;\n\t\telse  \n\t\t\tctx_p->cpp.alg = CC_CPP_SM4;\n\t\tctx_p->key_type = CC_POLICY_PROTECTED_KEY;\n\t\tdev_dbg(dev, \"policy protected key alg: %d slot: %d.\\n\",\n\t\t\tctx_p->cpp.alg, ctx_p->cpp.slot);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(dev, \"Unsupported protected key (%d)\\n\", hki.hw_key1);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int cc_cipher_setkey(struct crypto_skcipher *sktfm, const u8 *key,\n\t\t\t    unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(sktfm);\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tstruct cc_crypto_alg *cc_alg =\n\t\t\tcontainer_of(tfm->__crt_alg, struct cc_crypto_alg,\n\t\t\t\t     skcipher_alg.base);\n\tunsigned int max_key_buf_size = cc_alg->skcipher_alg.max_keysize;\n\n\tdev_dbg(dev, \"Setting key in context @%p for %s. keylen=%u\\n\",\n\t\tctx_p, crypto_tfm_alg_name(tfm), keylen);\n\tdump_byte_array(\"key\", key, keylen);\n\n\t \n\n\tif (validate_keys_sizes(ctx_p, keylen)) {\n\t\tdev_dbg(dev, \"Invalid key size %d.\\n\", keylen);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx_p->cipher_mode == DRV_CIPHER_ESSIV) {\n\n\t\t \n\t\tif (keylen != AES_KEYSIZE_256)  {\n\t\t\tunsigned int flags = crypto_tfm_get_flags(tfm) & CRYPTO_TFM_REQ_MASK;\n\n\t\t\tif (likely(ctx_p->fallback_tfm)) {\n\t\t\t\tctx_p->fallback_on = true;\n\t\t\t\tcrypto_skcipher_clear_flags(ctx_p->fallback_tfm,\n\t\t\t\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\t\t\t\tcrypto_skcipher_clear_flags(ctx_p->fallback_tfm, flags);\n\t\t\t\treturn crypto_skcipher_setkey(ctx_p->fallback_tfm, key, keylen);\n\t\t\t}\n\n\t\t\tdev_dbg(dev, \"Unsupported key size %d and no fallback.\\n\", keylen);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tmax_key_buf_size <<= 1;\n\t}\n\n\tctx_p->fallback_on = false;\n\tctx_p->key_type = CC_UNPROTECTED_KEY;\n\n\t \n\tif (ctx_p->flow_mode == S_DIN_to_DES) {\n\t\tif ((keylen == DES3_EDE_KEY_SIZE &&\n\t\t     verify_skcipher_des3_key(sktfm, key)) ||\n\t\t    verify_skcipher_des_key(sktfm, key)) {\n\t\t\tdev_dbg(dev, \"weak DES key\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (ctx_p->cipher_mode == DRV_CIPHER_XTS &&\n\t    xts_verify_key(sktfm, key, keylen)) {\n\t\tdev_dbg(dev, \"weak XTS key\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tdma_sync_single_for_cpu(dev, ctx_p->user.key_dma_addr,\n\t\t\t\tmax_key_buf_size, DMA_TO_DEVICE);\n\n\tmemcpy(ctx_p->user.key, key, keylen);\n\n\tif (ctx_p->cipher_mode == DRV_CIPHER_ESSIV) {\n\t\t \n\t\tint err;\n\n\t\terr = crypto_shash_tfm_digest(ctx_p->shash_tfm,\n\t\t\t\t\t      ctx_p->user.key, keylen,\n\t\t\t\t\t      ctx_p->user.key + keylen);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"Failed to hash ESSIV key.\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\tkeylen <<= 1;\n\t}\n\tdma_sync_single_for_device(dev, ctx_p->user.key_dma_addr,\n\t\t\t\t   max_key_buf_size, DMA_TO_DEVICE);\n\tctx_p->keylen = keylen;\n\n\tdev_dbg(dev, \"return safely\");\n\treturn 0;\n}\n\nstatic int cc_out_setup_mode(struct cc_cipher_ctx *ctx_p)\n{\n\tswitch (ctx_p->flow_mode) {\n\tcase S_DIN_to_AES:\n\t\treturn S_AES_to_DOUT;\n\tcase S_DIN_to_DES:\n\t\treturn S_DES_to_DOUT;\n\tcase S_DIN_to_SM4:\n\t\treturn S_SM4_to_DOUT;\n\tdefault:\n\t\treturn ctx_p->flow_mode;\n\t}\n}\n\nstatic void cc_setup_readiv_desc(struct crypto_tfm *tfm,\n\t\t\t\t struct cipher_req_ctx *req_ctx,\n\t\t\t\t unsigned int ivsize, struct cc_hw_desc desc[],\n\t\t\t\t unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tint cipher_mode = ctx_p->cipher_mode;\n\tint flow_mode = cc_out_setup_mode(ctx_p);\n\tint direction = req_ctx->gen_ctx.op_type;\n\tdma_addr_t iv_dma_addr = req_ctx->gen_ctx.iv_dma_addr;\n\n\tif (ctx_p->key_type == CC_POLICY_PROTECTED_KEY)\n\t\treturn;\n\n\tswitch (cipher_mode) {\n\tcase DRV_CIPHER_ECB:\n\t\tbreak;\n\tcase DRV_CIPHER_CBC:\n\tcase DRV_CIPHER_CBC_CTS:\n\tcase DRV_CIPHER_CTR:\n\tcase DRV_CIPHER_OFB:\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_dout_dlli(&desc[*seq_size], iv_dma_addr, ivsize, NS_BIT, 1);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tif (cipher_mode == DRV_CIPHER_CTR ||\n\t\t    cipher_mode == DRV_CIPHER_OFB) {\n\t\t\tset_setup_mode(&desc[*seq_size], SETUP_WRITE_STATE1);\n\t\t} else {\n\t\t\tset_setup_mode(&desc[*seq_size], SETUP_WRITE_STATE0);\n\t\t}\n\t\tset_queue_last_ind(ctx_p->drvdata, &desc[*seq_size]);\n\t\t(*seq_size)++;\n\t\tbreak;\n\tcase DRV_CIPHER_XTS:\n\tcase DRV_CIPHER_ESSIV:\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_setup_mode(&desc[*seq_size], SETUP_WRITE_STATE1);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\tset_dout_dlli(&desc[*seq_size], iv_dma_addr, CC_AES_BLOCK_SIZE,\n\t\t\t     NS_BIT, 1);\n\t\tset_queue_last_ind(ctx_p->drvdata, &desc[*seq_size]);\n\t\t(*seq_size)++;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Unsupported cipher mode (%d)\\n\", cipher_mode);\n\t}\n}\n\n\nstatic void cc_setup_state_desc(struct crypto_tfm *tfm,\n\t\t\t\t struct cipher_req_ctx *req_ctx,\n\t\t\t\t unsigned int ivsize, unsigned int nbytes,\n\t\t\t\t struct cc_hw_desc desc[],\n\t\t\t\t unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tint cipher_mode = ctx_p->cipher_mode;\n\tint flow_mode = ctx_p->flow_mode;\n\tint direction = req_ctx->gen_ctx.op_type;\n\tdma_addr_t iv_dma_addr = req_ctx->gen_ctx.iv_dma_addr;\n\n\tswitch (cipher_mode) {\n\tcase DRV_CIPHER_ECB:\n\t\tbreak;\n\tcase DRV_CIPHER_CBC:\n\tcase DRV_CIPHER_CBC_CTS:\n\tcase DRV_CIPHER_CTR:\n\tcase DRV_CIPHER_OFB:\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_din_type(&desc[*seq_size], DMA_DLLI, iv_dma_addr, ivsize,\n\t\t\t     NS_BIT);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tif (cipher_mode == DRV_CIPHER_CTR ||\n\t\t    cipher_mode == DRV_CIPHER_OFB) {\n\t\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_STATE1);\n\t\t} else {\n\t\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_STATE0);\n\t\t}\n\t\t(*seq_size)++;\n\t\tbreak;\n\tcase DRV_CIPHER_XTS:\n\tcase DRV_CIPHER_ESSIV:\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Unsupported cipher mode (%d)\\n\", cipher_mode);\n\t}\n}\n\n\nstatic void cc_setup_xex_state_desc(struct crypto_tfm *tfm,\n\t\t\t\t struct cipher_req_ctx *req_ctx,\n\t\t\t\t unsigned int ivsize, unsigned int nbytes,\n\t\t\t\t struct cc_hw_desc desc[],\n\t\t\t\t unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tint cipher_mode = ctx_p->cipher_mode;\n\tint flow_mode = ctx_p->flow_mode;\n\tint direction = req_ctx->gen_ctx.op_type;\n\tdma_addr_t key_dma_addr = ctx_p->user.key_dma_addr;\n\tunsigned int key_len = (ctx_p->keylen / 2);\n\tdma_addr_t iv_dma_addr = req_ctx->gen_ctx.iv_dma_addr;\n\tunsigned int key_offset = key_len;\n\n\tswitch (cipher_mode) {\n\tcase DRV_CIPHER_ECB:\n\t\tbreak;\n\tcase DRV_CIPHER_CBC:\n\tcase DRV_CIPHER_CBC_CTS:\n\tcase DRV_CIPHER_CTR:\n\tcase DRV_CIPHER_OFB:\n\t\tbreak;\n\tcase DRV_CIPHER_XTS:\n\tcase DRV_CIPHER_ESSIV:\n\n\t\tif (cipher_mode == DRV_CIPHER_ESSIV)\n\t\t\tkey_len = SHA256_DIGEST_SIZE;\n\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tif (cc_key_type(tfm) == CC_HW_PROTECTED_KEY) {\n\t\t\tset_hw_crypto_key(&desc[*seq_size],\n\t\t\t\t\t  ctx_p->hw.key2_slot);\n\t\t} else {\n\t\t\tset_din_type(&desc[*seq_size], DMA_DLLI,\n\t\t\t\t     (key_dma_addr + key_offset),\n\t\t\t\t     key_len, NS_BIT);\n\t\t}\n\t\tset_xex_data_unit_size(&desc[*seq_size], nbytes);\n\t\tset_flow_mode(&desc[*seq_size], S_DIN_to_AES2);\n\t\tset_key_size_aes(&desc[*seq_size], key_len);\n\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_XEX_KEY);\n\t\t(*seq_size)++;\n\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_STATE1);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tset_key_size_aes(&desc[*seq_size], key_len);\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\tset_din_type(&desc[*seq_size], DMA_DLLI, iv_dma_addr,\n\t\t\t     CC_AES_BLOCK_SIZE, NS_BIT);\n\t\t(*seq_size)++;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Unsupported cipher mode (%d)\\n\", cipher_mode);\n\t}\n}\n\nstatic int cc_out_flow_mode(struct cc_cipher_ctx *ctx_p)\n{\n\tswitch (ctx_p->flow_mode) {\n\tcase S_DIN_to_AES:\n\t\treturn DIN_AES_DOUT;\n\tcase S_DIN_to_DES:\n\t\treturn DIN_DES_DOUT;\n\tcase S_DIN_to_SM4:\n\t\treturn DIN_SM4_DOUT;\n\tdefault:\n\t\treturn ctx_p->flow_mode;\n\t}\n}\n\nstatic void cc_setup_key_desc(struct crypto_tfm *tfm,\n\t\t\t      struct cipher_req_ctx *req_ctx,\n\t\t\t      unsigned int nbytes, struct cc_hw_desc desc[],\n\t\t\t      unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tint cipher_mode = ctx_p->cipher_mode;\n\tint flow_mode = ctx_p->flow_mode;\n\tint direction = req_ctx->gen_ctx.op_type;\n\tdma_addr_t key_dma_addr = ctx_p->user.key_dma_addr;\n\tunsigned int key_len = ctx_p->keylen;\n\tunsigned int din_size;\n\n\tswitch (cipher_mode) {\n\tcase DRV_CIPHER_CBC:\n\tcase DRV_CIPHER_CBC_CTS:\n\tcase DRV_CIPHER_CTR:\n\tcase DRV_CIPHER_OFB:\n\tcase DRV_CIPHER_ECB:\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\n\t\tif (cc_key_type(tfm) == CC_POLICY_PROTECTED_KEY) {\n\t\t\t \n\t\t\tset_key_size_aes(&desc[*seq_size], key_len);\n\t\t\tset_cpp_crypto_key(&desc[*seq_size], ctx_p->cpp.slot);\n\t\t\tflow_mode = cc_out_flow_mode(ctx_p);\n\t\t} else {\n\t\t\tif (flow_mode == S_DIN_to_AES) {\n\t\t\t\tif (cc_key_type(tfm) == CC_HW_PROTECTED_KEY) {\n\t\t\t\t\tset_hw_crypto_key(&desc[*seq_size],\n\t\t\t\t\t\t\t  ctx_p->hw.key1_slot);\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tdin_size = (key_len == 24) ?\n\t\t\t\t\t\tAES_MAX_KEY_SIZE : key_len;\n\n\t\t\t\t\tset_din_type(&desc[*seq_size], DMA_DLLI,\n\t\t\t\t\t\t     key_dma_addr, din_size,\n\t\t\t\t\t\t     NS_BIT);\n\t\t\t\t}\n\t\t\t\tset_key_size_aes(&desc[*seq_size], key_len);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tset_din_type(&desc[*seq_size], DMA_DLLI,\n\t\t\t\t\t     key_dma_addr, key_len, NS_BIT);\n\t\t\t\tset_key_size_des(&desc[*seq_size], key_len);\n\t\t\t}\n\t\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_KEY0);\n\t\t}\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\t(*seq_size)++;\n\t\tbreak;\n\tcase DRV_CIPHER_XTS:\n\tcase DRV_CIPHER_ESSIV:\n\t\t \n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_cipher_mode(&desc[*seq_size], cipher_mode);\n\t\tset_cipher_config0(&desc[*seq_size], direction);\n\t\tif (cc_key_type(tfm) == CC_HW_PROTECTED_KEY) {\n\t\t\tset_hw_crypto_key(&desc[*seq_size],\n\t\t\t\t\t  ctx_p->hw.key1_slot);\n\t\t} else {\n\t\t\tset_din_type(&desc[*seq_size], DMA_DLLI, key_dma_addr,\n\t\t\t\t     (key_len / 2), NS_BIT);\n\t\t}\n\t\tset_key_size_aes(&desc[*seq_size], (key_len / 2));\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\tset_setup_mode(&desc[*seq_size], SETUP_LOAD_KEY0);\n\t\t(*seq_size)++;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Unsupported cipher mode (%d)\\n\", cipher_mode);\n\t}\n}\n\nstatic void cc_setup_mlli_desc(struct crypto_tfm *tfm,\n\t\t\t       struct cipher_req_ctx *req_ctx,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes, void *areq,\n\t\t\t       struct cc_hw_desc desc[], unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\n\tif (req_ctx->dma_buf_type == CC_DMA_BUF_MLLI) {\n\t\t \n\t\tdev_dbg(dev, \" bypass params addr %pad length 0x%X addr 0x%08X\\n\",\n\t\t\t&req_ctx->mlli_params.mlli_dma_addr,\n\t\t\treq_ctx->mlli_params.mlli_len,\n\t\t\tctx_p->drvdata->mlli_sram_addr);\n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_din_type(&desc[*seq_size], DMA_DLLI,\n\t\t\t     req_ctx->mlli_params.mlli_dma_addr,\n\t\t\t     req_ctx->mlli_params.mlli_len, NS_BIT);\n\t\tset_dout_sram(&desc[*seq_size],\n\t\t\t      ctx_p->drvdata->mlli_sram_addr,\n\t\t\t      req_ctx->mlli_params.mlli_len);\n\t\tset_flow_mode(&desc[*seq_size], BYPASS);\n\t\t(*seq_size)++;\n\t}\n}\n\nstatic void cc_setup_flow_desc(struct crypto_tfm *tfm,\n\t\t\t       struct cipher_req_ctx *req_ctx,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes, struct cc_hw_desc desc[],\n\t\t\t       unsigned int *seq_size)\n{\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tunsigned int flow_mode = cc_out_flow_mode(ctx_p);\n\tbool last_desc = (ctx_p->key_type == CC_POLICY_PROTECTED_KEY ||\n\t\t\t  ctx_p->cipher_mode == DRV_CIPHER_ECB);\n\n\t \n\tif (req_ctx->dma_buf_type == CC_DMA_BUF_DLLI) {\n\t\tdev_dbg(dev, \" data params addr %pad length 0x%X\\n\",\n\t\t\t&sg_dma_address(src), nbytes);\n\t\tdev_dbg(dev, \" data params addr %pad length 0x%X\\n\",\n\t\t\t&sg_dma_address(dst), nbytes);\n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_din_type(&desc[*seq_size], DMA_DLLI, sg_dma_address(src),\n\t\t\t     nbytes, NS_BIT);\n\t\tset_dout_dlli(&desc[*seq_size], sg_dma_address(dst),\n\t\t\t      nbytes, NS_BIT, (!last_desc ? 0 : 1));\n\t\tif (last_desc)\n\t\t\tset_queue_last_ind(ctx_p->drvdata, &desc[*seq_size]);\n\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\t(*seq_size)++;\n\t} else {\n\t\thw_desc_init(&desc[*seq_size]);\n\t\tset_din_type(&desc[*seq_size], DMA_MLLI,\n\t\t\t     ctx_p->drvdata->mlli_sram_addr,\n\t\t\t     req_ctx->in_mlli_nents, NS_BIT);\n\t\tif (req_ctx->out_nents == 0) {\n\t\t\tdev_dbg(dev, \" din/dout params addr 0x%08X addr 0x%08X\\n\",\n\t\t\t\tctx_p->drvdata->mlli_sram_addr,\n\t\t\t\tctx_p->drvdata->mlli_sram_addr);\n\t\t\tset_dout_mlli(&desc[*seq_size],\n\t\t\t\t      ctx_p->drvdata->mlli_sram_addr,\n\t\t\t\t      req_ctx->in_mlli_nents, NS_BIT,\n\t\t\t\t      (!last_desc ? 0 : 1));\n\t\t} else {\n\t\t\tdev_dbg(dev, \" din/dout params addr 0x%08X addr 0x%08X\\n\",\n\t\t\t\tctx_p->drvdata->mlli_sram_addr,\n\t\t\t\tctx_p->drvdata->mlli_sram_addr +\n\t\t\t\t(u32)LLI_ENTRY_BYTE_SIZE * req_ctx->in_nents);\n\t\t\tset_dout_mlli(&desc[*seq_size],\n\t\t\t\t      (ctx_p->drvdata->mlli_sram_addr +\n\t\t\t\t       (LLI_ENTRY_BYTE_SIZE *\n\t\t\t\t\treq_ctx->in_mlli_nents)),\n\t\t\t\t      req_ctx->out_mlli_nents, NS_BIT,\n\t\t\t\t      (!last_desc ? 0 : 1));\n\t\t}\n\t\tif (last_desc)\n\t\t\tset_queue_last_ind(ctx_p->drvdata, &desc[*seq_size]);\n\n\t\tset_flow_mode(&desc[*seq_size], flow_mode);\n\t\t(*seq_size)++;\n\t}\n}\n\nstatic void cc_cipher_complete(struct device *dev, void *cc_req, int err)\n{\n\tstruct skcipher_request *req = (struct skcipher_request *)cc_req;\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *src = req->src;\n\tstruct cipher_req_ctx *req_ctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *sk_tfm = crypto_skcipher_reqtfm(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(sk_tfm);\n\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\tcc_unmap_cipher_request(dev, req_ctx, ivsize, src, dst);\n\t\tmemcpy(req->iv, req_ctx->iv, ivsize);\n\t\tkfree_sensitive(req_ctx->iv);\n\t}\n\n\tskcipher_request_complete(req, err);\n}\n\nstatic int cc_cipher_process(struct skcipher_request *req,\n\t\t\t     enum drv_crypto_direction direction)\n{\n\tstruct crypto_skcipher *sk_tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(sk_tfm);\n\tstruct cipher_req_ctx *req_ctx = skcipher_request_ctx(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(sk_tfm);\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *src = req->src;\n\tunsigned int nbytes = req->cryptlen;\n\tvoid *iv = req->iv;\n\tstruct cc_cipher_ctx *ctx_p = crypto_tfm_ctx(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx_p->drvdata);\n\tstruct cc_hw_desc desc[MAX_SKCIPHER_SEQ_LEN];\n\tstruct cc_crypto_req cc_req = {};\n\tint rc;\n\tunsigned int seq_len = 0;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"%s req=%p iv=%p nbytes=%d\\n\",\n\t\t((direction == DRV_CRYPTO_DIRECTION_ENCRYPT) ?\n\t\t\"Encrypt\" : \"Decrypt\"), req, iv, nbytes);\n\n\t \n\n\tif (validate_data_size(ctx_p, nbytes)) {\n\t\tdev_dbg(dev, \"Unsupported data size %d.\\n\", nbytes);\n\t\trc = -EINVAL;\n\t\tgoto exit_process;\n\t}\n\tif (nbytes == 0) {\n\t\t \n\t\trc = 0;\n\t\tgoto exit_process;\n\t}\n\n\tif (ctx_p->fallback_on) {\n\t\tstruct skcipher_request *subreq = skcipher_request_ctx(req);\n\n\t\t*subreq = *req;\n\t\tskcipher_request_set_tfm(subreq, ctx_p->fallback_tfm);\n\t\tif (direction == DRV_CRYPTO_DIRECTION_ENCRYPT)\n\t\t\treturn crypto_skcipher_encrypt(subreq);\n\t\telse\n\t\t\treturn crypto_skcipher_decrypt(subreq);\n\t}\n\n\t \n\treq_ctx->iv = kmemdup(iv, ivsize, flags);\n\tif (!req_ctx->iv) {\n\t\trc = -ENOMEM;\n\t\tgoto exit_process;\n\t}\n\n\t \n\tcc_req.user_cb = cc_cipher_complete;\n\tcc_req.user_arg = req;\n\n\t \n\tif (ctx_p->key_type == CC_POLICY_PROTECTED_KEY) {\n\t\tcc_req.cpp.is_cpp = true;\n\t\tcc_req.cpp.alg = ctx_p->cpp.alg;\n\t\tcc_req.cpp.slot = ctx_p->cpp.slot;\n\t}\n\n\t \n\treq_ctx->gen_ctx.op_type = direction;\n\n\t \n\n\trc = cc_map_cipher_request(ctx_p->drvdata, req_ctx, ivsize, nbytes,\n\t\t\t\t      req_ctx->iv, src, dst, flags);\n\tif (rc) {\n\t\tdev_err(dev, \"map_request() failed\\n\");\n\t\tgoto exit_process;\n\t}\n\n\t \n\n\t \n\tcc_setup_state_desc(tfm, req_ctx, ivsize, nbytes, desc, &seq_len);\n\t \n\tcc_setup_mlli_desc(tfm, req_ctx, dst, src, nbytes, req, desc, &seq_len);\n\t \n\tcc_setup_key_desc(tfm, req_ctx, nbytes, desc, &seq_len);\n\t \n\tcc_setup_xex_state_desc(tfm, req_ctx, ivsize, nbytes, desc, &seq_len);\n\t \n\tcc_setup_flow_desc(tfm, req_ctx, dst, src, nbytes, desc, &seq_len);\n\t \n\tcc_setup_readiv_desc(tfm, req_ctx, ivsize, desc, &seq_len);\n\n\t \n\n\trc = cc_send_request(ctx_p->drvdata, &cc_req, desc, seq_len,\n\t\t\t     &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\t \n\t\tcc_unmap_cipher_request(dev, req_ctx, ivsize, src, dst);\n\t}\n\nexit_process:\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tkfree_sensitive(req_ctx->iv);\n\t}\n\n\treturn rc;\n}\n\nstatic int cc_cipher_encrypt(struct skcipher_request *req)\n{\n\tstruct cipher_req_ctx *req_ctx = skcipher_request_ctx(req);\n\n\tmemset(req_ctx, 0, sizeof(*req_ctx));\n\n\treturn cc_cipher_process(req, DRV_CRYPTO_DIRECTION_ENCRYPT);\n}\n\nstatic int cc_cipher_decrypt(struct skcipher_request *req)\n{\n\tstruct cipher_req_ctx *req_ctx = skcipher_request_ctx(req);\n\n\tmemset(req_ctx, 0, sizeof(*req_ctx));\n\n\treturn cc_cipher_process(req, DRV_CRYPTO_DIRECTION_DECRYPT);\n}\n\n \nstatic const struct cc_alg_template skcipher_algs[] = {\n\t{\n\t\t.name = \"xts(paes)\",\n\t\t.driver_name = \"xts-paes-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_XTS,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"essiv(cbc(paes),sha256)\",\n\t\t.driver_name = \"essiv-paes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ESSIV,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"ecb(paes)\",\n\t\t.driver_name = \"ecb-paes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = 0,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ECB,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"cbc(paes)\",\n\t\t.driver_name = \"cbc-paes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"ofb(paes)\",\n\t\t.driver_name = \"ofb-paes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_OFB,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"cts(cbc(paes))\",\n\t\t.driver_name = \"cts-cbc-paes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC_CTS,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"ctr(paes)\",\n\t\t.driver_name = \"ctr-paes-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CTR,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t \n\t\t.name = \"xts(aes)\",\n\t\t.driver_name = \"xts-aes-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE * 2,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE * 2,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_XTS,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"essiv(cbc(aes),sha256)\",\n\t\t.driver_name = \"essiv-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ESSIV,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"ecb(aes)\",\n\t\t.driver_name = \"ecb-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = 0,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ECB,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"cbc(aes)\",\n\t\t.driver_name = \"cbc-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"ofb(aes)\",\n\t\t.driver_name = \"ofb-aes-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_OFB,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"cts(cbc(aes))\",\n\t\t.driver_name = \"cts-cbc-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC_CTS,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"ctr(aes)\",\n\t\t.driver_name = \"ctr-aes-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize = AES_MAX_KEY_SIZE,\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CTR,\n\t\t.flow_mode = S_DIN_to_AES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"cbc(des3_ede)\",\n\t\t.driver_name = \"cbc-3des-ccree\",\n\t\t.blocksize = DES3_EDE_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t\t\t.ivsize = DES3_EDE_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_DES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"ecb(des3_ede)\",\n\t\t.driver_name = \"ecb-3des-ccree\",\n\t\t.blocksize = DES3_EDE_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t\t\t.ivsize = 0,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ECB,\n\t\t.flow_mode = S_DIN_to_DES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"cbc(des)\",\n\t\t.driver_name = \"cbc-des-ccree\",\n\t\t.blocksize = DES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = DES_KEY_SIZE,\n\t\t\t.max_keysize = DES_KEY_SIZE,\n\t\t\t.ivsize = DES_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_DES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"ecb(des)\",\n\t\t.driver_name = \"ecb-des-ccree\",\n\t\t.blocksize = DES_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = DES_KEY_SIZE,\n\t\t\t.max_keysize = DES_KEY_SIZE,\n\t\t\t.ivsize = 0,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ECB,\n\t\t.flow_mode = S_DIN_to_DES,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"cbc(sm4)\",\n\t\t.driver_name = \"cbc-sm4-ccree\",\n\t\t.blocksize = SM4_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = SM4_KEY_SIZE,\n\t\t\t.max_keysize = SM4_KEY_SIZE,\n\t\t\t.ivsize = SM4_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_SM4,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t},\n\t{\n\t\t.name = \"ecb(sm4)\",\n\t\t.driver_name = \"ecb-sm4-ccree\",\n\t\t.blocksize = SM4_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = SM4_KEY_SIZE,\n\t\t\t.max_keysize = SM4_KEY_SIZE,\n\t\t\t.ivsize = 0,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_ECB,\n\t\t.flow_mode = S_DIN_to_SM4,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t},\n\t{\n\t\t.name = \"ctr(sm4)\",\n\t\t.driver_name = \"ctr-sm4-ccree\",\n\t\t.blocksize = 1,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_setkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = SM4_KEY_SIZE,\n\t\t\t.max_keysize = SM4_KEY_SIZE,\n\t\t\t.ivsize = SM4_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CTR,\n\t\t.flow_mode = S_DIN_to_SM4,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t},\n\t{\n\t\t.name = \"cbc(psm4)\",\n\t\t.driver_name = \"cbc-psm4-ccree\",\n\t\t.blocksize = SM4_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = SM4_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CBC,\n\t\t.flow_mode = S_DIN_to_SM4,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t\t.sec_func = true,\n\t},\n\t{\n\t\t.name = \"ctr(psm4)\",\n\t\t.driver_name = \"ctr-psm4-ccree\",\n\t\t.blocksize = SM4_BLOCK_SIZE,\n\t\t.template_skcipher = {\n\t\t\t.setkey = cc_cipher_sethkey,\n\t\t\t.encrypt = cc_cipher_encrypt,\n\t\t\t.decrypt = cc_cipher_decrypt,\n\t\t\t.min_keysize = CC_HW_KEY_SIZE,\n\t\t\t.max_keysize = CC_HW_KEY_SIZE,\n\t\t\t.ivsize = SM4_BLOCK_SIZE,\n\t\t\t},\n\t\t.cipher_mode = DRV_CIPHER_CTR,\n\t\t.flow_mode = S_DIN_to_SM4,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t\t.sec_func = true,\n\t},\n};\n\nstatic struct cc_crypto_alg *cc_create_alg(const struct cc_alg_template *tmpl,\n\t\t\t\t\t   struct device *dev)\n{\n\tstruct cc_crypto_alg *t_alg;\n\tstruct skcipher_alg *alg;\n\n\tt_alg = devm_kzalloc(dev, sizeof(*t_alg), GFP_KERNEL);\n\tif (!t_alg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\talg = &t_alg->skcipher_alg;\n\n\tmemcpy(alg, &tmpl->template_skcipher, sizeof(*alg));\n\n\tsnprintf(alg->base.cra_name, CRYPTO_MAX_ALG_NAME, \"%s\", tmpl->name);\n\tsnprintf(alg->base.cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t tmpl->driver_name);\n\talg->base.cra_module = THIS_MODULE;\n\talg->base.cra_priority = CC_CRA_PRIO;\n\talg->base.cra_blocksize = tmpl->blocksize;\n\talg->base.cra_alignmask = 0;\n\talg->base.cra_ctxsize = sizeof(struct cc_cipher_ctx);\n\n\talg->base.cra_init = cc_cipher_init;\n\talg->base.cra_exit = cc_cipher_exit;\n\talg->base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY;\n\n\tt_alg->cipher_mode = tmpl->cipher_mode;\n\tt_alg->flow_mode = tmpl->flow_mode;\n\n\treturn t_alg;\n}\n\nint cc_cipher_free(struct cc_drvdata *drvdata)\n{\n\tstruct cc_crypto_alg *t_alg, *n;\n\n\t \n\tlist_for_each_entry_safe(t_alg, n, &drvdata->alg_list, entry) {\n\t\tcrypto_unregister_skcipher(&t_alg->skcipher_alg);\n\t\tlist_del(&t_alg->entry);\n\t}\n\treturn 0;\n}\n\nint cc_cipher_alloc(struct cc_drvdata *drvdata)\n{\n\tstruct cc_crypto_alg *t_alg;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tint rc = -ENOMEM;\n\tint alg;\n\n\tINIT_LIST_HEAD(&drvdata->alg_list);\n\n\t \n\tdev_dbg(dev, \"Number of algorithms = %zu\\n\",\n\t\tARRAY_SIZE(skcipher_algs));\n\tfor (alg = 0; alg < ARRAY_SIZE(skcipher_algs); alg++) {\n\t\tif ((skcipher_algs[alg].min_hw_rev > drvdata->hw_rev) ||\n\t\t    !(drvdata->std_bodies & skcipher_algs[alg].std_body) ||\n\t\t    (drvdata->sec_disabled && skcipher_algs[alg].sec_func))\n\t\t\tcontinue;\n\n\t\tdev_dbg(dev, \"creating %s\\n\", skcipher_algs[alg].driver_name);\n\t\tt_alg = cc_create_alg(&skcipher_algs[alg], dev);\n\t\tif (IS_ERR(t_alg)) {\n\t\t\trc = PTR_ERR(t_alg);\n\t\t\tdev_err(dev, \"%s alg allocation failed\\n\",\n\t\t\t\tskcipher_algs[alg].driver_name);\n\t\t\tgoto fail0;\n\t\t}\n\t\tt_alg->drvdata = drvdata;\n\n\t\tdev_dbg(dev, \"registering %s\\n\",\n\t\t\tskcipher_algs[alg].driver_name);\n\t\trc = crypto_register_skcipher(&t_alg->skcipher_alg);\n\t\tdev_dbg(dev, \"%s alg registration rc = %x\\n\",\n\t\t\tt_alg->skcipher_alg.base.cra_driver_name, rc);\n\t\tif (rc) {\n\t\t\tdev_err(dev, \"%s alg registration failed\\n\",\n\t\t\t\tt_alg->skcipher_alg.base.cra_driver_name);\n\t\t\tgoto fail0;\n\t\t}\n\n\t\tlist_add_tail(&t_alg->entry, &drvdata->alg_list);\n\t\tdev_dbg(dev, \"Registered %s\\n\",\n\t\t\tt_alg->skcipher_alg.base.cra_driver_name);\n\t}\n\treturn 0;\n\nfail0:\n\tcc_cipher_free(drvdata);\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}