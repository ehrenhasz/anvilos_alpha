{
  "module_name": "cc_hash.c",
  "hash_id": "a900742cc4ff7b536ee9628854bddcacfe405da5162e113150d9273a2720e9ee",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccree/cc_hash.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <crypto/algapi.h>\n#include <crypto/hash.h>\n#include <crypto/md5.h>\n#include <crypto/sm3.h>\n#include <crypto/internal/hash.h>\n\n#include \"cc_driver.h\"\n#include \"cc_request_mgr.h\"\n#include \"cc_buffer_mgr.h\"\n#include \"cc_hash.h\"\n#include \"cc_sram_mgr.h\"\n\n#define CC_MAX_HASH_SEQ_LEN 12\n#define CC_MAX_OPAD_KEYS_SIZE CC_MAX_HASH_BLCK_SIZE\n#define CC_SM3_HASH_LEN_SIZE 8\n\nstruct cc_hash_handle {\n\tu32 digest_len_sram_addr;\t \n\tu32 larval_digest_sram_addr;    \n\tstruct list_head hash_list;\n};\n\nstatic const u32 cc_digest_len_init[] = {\n\t0x00000040, 0x00000000, 0x00000000, 0x00000000 };\nstatic const u32 cc_md5_init[] = {\n\tSHA1_H3, SHA1_H2, SHA1_H1, SHA1_H0 };\nstatic const u32 cc_sha1_init[] = {\n\tSHA1_H4, SHA1_H3, SHA1_H2, SHA1_H1, SHA1_H0 };\nstatic const u32 cc_sha224_init[] = {\n\tSHA224_H7, SHA224_H6, SHA224_H5, SHA224_H4,\n\tSHA224_H3, SHA224_H2, SHA224_H1, SHA224_H0 };\nstatic const u32 cc_sha256_init[] = {\n\tSHA256_H7, SHA256_H6, SHA256_H5, SHA256_H4,\n\tSHA256_H3, SHA256_H2, SHA256_H1, SHA256_H0 };\nstatic const u32 cc_digest_len_sha512_init[] = {\n\t0x00000080, 0x00000000, 0x00000000, 0x00000000 };\n\n \n#define hilo(x)\tupper_32_bits(x), lower_32_bits(x)\nstatic const u32 cc_sha384_init[] = {\n\thilo(SHA384_H7), hilo(SHA384_H6), hilo(SHA384_H5), hilo(SHA384_H4),\n\thilo(SHA384_H3), hilo(SHA384_H2), hilo(SHA384_H1), hilo(SHA384_H0) };\nstatic const u32 cc_sha512_init[] = {\n\thilo(SHA512_H7), hilo(SHA512_H6), hilo(SHA512_H5), hilo(SHA512_H4),\n\thilo(SHA512_H3), hilo(SHA512_H2), hilo(SHA512_H1), hilo(SHA512_H0) };\n\nstatic const u32 cc_sm3_init[] = {\n\tSM3_IVH, SM3_IVG, SM3_IVF, SM3_IVE,\n\tSM3_IVD, SM3_IVC, SM3_IVB, SM3_IVA };\n\nstatic void cc_setup_xcbc(struct ahash_request *areq, struct cc_hw_desc desc[],\n\t\t\t  unsigned int *seq_size);\n\nstatic void cc_setup_cmac(struct ahash_request *areq, struct cc_hw_desc desc[],\n\t\t\t  unsigned int *seq_size);\n\nstatic const void *cc_larval_digest(struct device *dev, u32 mode);\n\nstruct cc_hash_alg {\n\tstruct list_head entry;\n\tint hash_mode;\n\tint hw_mode;\n\tint inter_digestsize;\n\tstruct cc_drvdata *drvdata;\n\tstruct ahash_alg ahash_alg;\n};\n\nstruct hash_key_req_ctx {\n\tu32 keylen;\n\tdma_addr_t key_dma_addr;\n\tu8 *key;\n};\n\n \nstruct cc_hash_ctx {\n\tstruct cc_drvdata *drvdata;\n\t \n\tu8 digest_buff[CC_MAX_HASH_DIGEST_SIZE]  ____cacheline_aligned;\n\tu8 opad_tmp_keys_buff[CC_MAX_OPAD_KEYS_SIZE]  ____cacheline_aligned;\n\n\tdma_addr_t opad_tmp_keys_dma_addr  ____cacheline_aligned;\n\tdma_addr_t digest_buff_dma_addr;\n\t \n\tstruct hash_key_req_ctx key_params;\n\tint hash_mode;\n\tint hw_mode;\n\tint inter_digestsize;\n\tunsigned int hash_len;\n\tstruct completion setkey_comp;\n\tbool is_hmac;\n};\n\nstatic void cc_set_desc(struct ahash_req_ctx *areq_ctx, struct cc_hash_ctx *ctx,\n\t\t\tunsigned int flow_mode, struct cc_hw_desc desc[],\n\t\t\tbool is_not_last_data, unsigned int *seq_size);\n\nstatic void cc_set_endianity(u32 mode, struct cc_hw_desc *desc)\n{\n\tif (mode == DRV_HASH_MD5 || mode == DRV_HASH_SHA384 ||\n\t    mode == DRV_HASH_SHA512) {\n\t\tset_bytes_swap(desc, 1);\n\t} else {\n\t\tset_cipher_config0(desc, HASH_DIGEST_RESULT_LITTLE_ENDIAN);\n\t}\n}\n\nstatic int cc_map_result(struct device *dev, struct ahash_req_ctx *state,\n\t\t\t unsigned int digestsize)\n{\n\tstate->digest_result_dma_addr =\n\t\tdma_map_single(dev, state->digest_result_buff,\n\t\t\t       digestsize, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, state->digest_result_dma_addr)) {\n\t\tdev_err(dev, \"Mapping digest result buffer %u B for DMA failed\\n\",\n\t\t\tdigestsize);\n\t\treturn -ENOMEM;\n\t}\n\tdev_dbg(dev, \"Mapped digest result buffer %u B at va=%pK to dma=%pad\\n\",\n\t\tdigestsize, state->digest_result_buff,\n\t\t&state->digest_result_dma_addr);\n\n\treturn 0;\n}\n\nstatic void cc_init_req(struct device *dev, struct ahash_req_ctx *state,\n\t\t\tstruct cc_hash_ctx *ctx)\n{\n\tbool is_hmac = ctx->is_hmac;\n\n\tmemset(state, 0, sizeof(*state));\n\n\tif (is_hmac) {\n\t\tif (ctx->hw_mode != DRV_CIPHER_XCBC_MAC &&\n\t\t    ctx->hw_mode != DRV_CIPHER_CMAC) {\n\t\t\tdma_sync_single_for_cpu(dev, ctx->digest_buff_dma_addr,\n\t\t\t\t\t\tctx->inter_digestsize,\n\t\t\t\t\t\tDMA_BIDIRECTIONAL);\n\n\t\t\tmemcpy(state->digest_buff, ctx->digest_buff,\n\t\t\t       ctx->inter_digestsize);\n\t\t\tif (ctx->hash_mode == DRV_HASH_SHA512 ||\n\t\t\t    ctx->hash_mode == DRV_HASH_SHA384)\n\t\t\t\tmemcpy(state->digest_bytes_len,\n\t\t\t\t       cc_digest_len_sha512_init,\n\t\t\t\t       ctx->hash_len);\n\t\t\telse\n\t\t\t\tmemcpy(state->digest_bytes_len,\n\t\t\t\t       cc_digest_len_init,\n\t\t\t\t       ctx->hash_len);\n\t\t}\n\n\t\tif (ctx->hash_mode != DRV_HASH_NULL) {\n\t\t\tdma_sync_single_for_cpu(dev,\n\t\t\t\t\t\tctx->opad_tmp_keys_dma_addr,\n\t\t\t\t\t\tctx->inter_digestsize,\n\t\t\t\t\t\tDMA_BIDIRECTIONAL);\n\t\t\tmemcpy(state->opad_digest_buff,\n\t\t\t       ctx->opad_tmp_keys_buff, ctx->inter_digestsize);\n\t\t}\n\t} else {  \n\t\t \n\t\tconst void *larval = cc_larval_digest(dev, ctx->hash_mode);\n\n\t\tmemcpy(state->digest_buff, larval, ctx->inter_digestsize);\n\t}\n}\n\nstatic int cc_map_req(struct device *dev, struct ahash_req_ctx *state,\n\t\t      struct cc_hash_ctx *ctx)\n{\n\tbool is_hmac = ctx->is_hmac;\n\n\tstate->digest_buff_dma_addr =\n\t\tdma_map_single(dev, state->digest_buff,\n\t\t\t       ctx->inter_digestsize, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, state->digest_buff_dma_addr)) {\n\t\tdev_err(dev, \"Mapping digest len %d B at va=%pK for DMA failed\\n\",\n\t\t\tctx->inter_digestsize, state->digest_buff);\n\t\treturn -EINVAL;\n\t}\n\tdev_dbg(dev, \"Mapped digest %d B at va=%pK to dma=%pad\\n\",\n\t\tctx->inter_digestsize, state->digest_buff,\n\t\t&state->digest_buff_dma_addr);\n\n\tif (ctx->hw_mode != DRV_CIPHER_XCBC_MAC) {\n\t\tstate->digest_bytes_len_dma_addr =\n\t\t\tdma_map_single(dev, state->digest_bytes_len,\n\t\t\t\t       HASH_MAX_LEN_SIZE, DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, state->digest_bytes_len_dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping digest len %u B at va=%pK for DMA failed\\n\",\n\t\t\t\tHASH_MAX_LEN_SIZE, state->digest_bytes_len);\n\t\t\tgoto unmap_digest_buf;\n\t\t}\n\t\tdev_dbg(dev, \"Mapped digest len %u B at va=%pK to dma=%pad\\n\",\n\t\t\tHASH_MAX_LEN_SIZE, state->digest_bytes_len,\n\t\t\t&state->digest_bytes_len_dma_addr);\n\t}\n\n\tif (is_hmac && ctx->hash_mode != DRV_HASH_NULL) {\n\t\tstate->opad_digest_dma_addr =\n\t\t\tdma_map_single(dev, state->opad_digest_buff,\n\t\t\t\t       ctx->inter_digestsize,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, state->opad_digest_dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping opad digest %d B at va=%pK for DMA failed\\n\",\n\t\t\t\tctx->inter_digestsize,\n\t\t\t\tstate->opad_digest_buff);\n\t\t\tgoto unmap_digest_len;\n\t\t}\n\t\tdev_dbg(dev, \"Mapped opad digest %d B at va=%pK to dma=%pad\\n\",\n\t\t\tctx->inter_digestsize, state->opad_digest_buff,\n\t\t\t&state->opad_digest_dma_addr);\n\t}\n\n\treturn 0;\n\nunmap_digest_len:\n\tif (state->digest_bytes_len_dma_addr) {\n\t\tdma_unmap_single(dev, state->digest_bytes_len_dma_addr,\n\t\t\t\t HASH_MAX_LEN_SIZE, DMA_BIDIRECTIONAL);\n\t\tstate->digest_bytes_len_dma_addr = 0;\n\t}\nunmap_digest_buf:\n\tif (state->digest_buff_dma_addr) {\n\t\tdma_unmap_single(dev, state->digest_buff_dma_addr,\n\t\t\t\t ctx->inter_digestsize, DMA_BIDIRECTIONAL);\n\t\tstate->digest_buff_dma_addr = 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void cc_unmap_req(struct device *dev, struct ahash_req_ctx *state,\n\t\t\t struct cc_hash_ctx *ctx)\n{\n\tif (state->digest_buff_dma_addr) {\n\t\tdma_unmap_single(dev, state->digest_buff_dma_addr,\n\t\t\t\t ctx->inter_digestsize, DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped digest-buffer: digest_buff_dma_addr=%pad\\n\",\n\t\t\t&state->digest_buff_dma_addr);\n\t\tstate->digest_buff_dma_addr = 0;\n\t}\n\tif (state->digest_bytes_len_dma_addr) {\n\t\tdma_unmap_single(dev, state->digest_bytes_len_dma_addr,\n\t\t\t\t HASH_MAX_LEN_SIZE, DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped digest-bytes-len buffer: digest_bytes_len_dma_addr=%pad\\n\",\n\t\t\t&state->digest_bytes_len_dma_addr);\n\t\tstate->digest_bytes_len_dma_addr = 0;\n\t}\n\tif (state->opad_digest_dma_addr) {\n\t\tdma_unmap_single(dev, state->opad_digest_dma_addr,\n\t\t\t\t ctx->inter_digestsize, DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped opad-digest: opad_digest_dma_addr=%pad\\n\",\n\t\t\t&state->opad_digest_dma_addr);\n\t\tstate->opad_digest_dma_addr = 0;\n\t}\n}\n\nstatic void cc_unmap_result(struct device *dev, struct ahash_req_ctx *state,\n\t\t\t    unsigned int digestsize, u8 *result)\n{\n\tif (state->digest_result_dma_addr) {\n\t\tdma_unmap_single(dev, state->digest_result_dma_addr, digestsize,\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"unmpa digest result buffer va (%pK) pa (%pad) len %u\\n\",\n\t\t\tstate->digest_result_buff,\n\t\t\t&state->digest_result_dma_addr, digestsize);\n\t\tmemcpy(result, state->digest_result_buff, digestsize);\n\t}\n\tstate->digest_result_dma_addr = 0;\n}\n\nstatic void cc_update_complete(struct device *dev, void *cc_req, int err)\n{\n\tstruct ahash_request *req = (struct ahash_request *)cc_req;\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\n\tdev_dbg(dev, \"req=%pK\\n\", req);\n\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\tcc_unmap_hash_request(dev, state, req->src, false);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\n\tahash_request_complete(req, err);\n}\n\nstatic void cc_digest_complete(struct device *dev, void *cc_req, int err)\n{\n\tstruct ahash_request *req = (struct ahash_request *)cc_req;\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\n\tdev_dbg(dev, \"req=%pK\\n\", req);\n\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\tcc_unmap_hash_request(dev, state, req->src, false);\n\t\tcc_unmap_result(dev, state, digestsize, req->result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\n\tahash_request_complete(req, err);\n}\n\nstatic void cc_hash_complete(struct device *dev, void *cc_req, int err)\n{\n\tstruct ahash_request *req = (struct ahash_request *)cc_req;\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\n\tdev_dbg(dev, \"req=%pK\\n\", req);\n\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\tcc_unmap_hash_request(dev, state, req->src, false);\n\t\tcc_unmap_result(dev, state, digestsize, req->result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\n\tahash_request_complete(req, err);\n}\n\nstatic int cc_fin_result(struct cc_hw_desc *desc, struct ahash_request *req,\n\t\t\t int idx)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_dout_dlli(&desc[idx], state->digest_result_dma_addr, digestsize,\n\t\t      NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tset_cipher_config1(&desc[idx], HASH_PADDING_DISABLED);\n\tcc_set_endianity(ctx->hash_mode, &desc[idx]);\n\tidx++;\n\n\treturn idx;\n}\n\nstatic int cc_fin_hmac(struct cc_hw_desc *desc, struct ahash_request *req,\n\t\t       int idx)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tset_dout_dlli(&desc[idx], state->digest_buff_dma_addr, digestsize,\n\t\t      NS_BIT, 0);\n\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\tcc_set_endianity(ctx->hash_mode, &desc[idx]);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tset_din_type(&desc[idx], DMA_DLLI, state->opad_digest_dma_addr,\n\t\t     ctx->inter_digestsize, NS_BIT);\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tset_din_sram(&desc[idx],\n\t\t     cc_digest_len_addr(ctx->drvdata, ctx->hash_mode),\n\t\t     ctx->hash_len);\n\tset_cipher_config1(&desc[idx], HASH_PADDING_ENABLED);\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_no_dma(&desc[idx], 0, 0xfffff0);\n\tset_dout_no_dma(&desc[idx], 0, 0, 1);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t     digestsize, NS_BIT);\n\tset_flow_mode(&desc[idx], DIN_HASH);\n\tidx++;\n\n\treturn idx;\n}\n\nstatic int cc_hash_digest(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\tstruct scatterlist *src = req->src;\n\tunsigned int nbytes = req->nbytes;\n\tu8 *result = req->result;\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tbool is_hmac = ctx->is_hmac;\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tu32 larval_digest_addr;\n\tint idx = 0;\n\tint rc = 0;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"===== %s-digest (%d) ====\\n\", is_hmac ? \"hmac\" : \"hash\",\n\t\tnbytes);\n\n\tcc_init_req(dev, state, ctx);\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_result(dev, state, digestsize)) {\n\t\tdev_err(dev, \"map_ahash_digest() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_hash_request_final(ctx->drvdata, state, src, nbytes, 1,\n\t\t\t\t      flags)) {\n\t\tdev_err(dev, \"map_ahash_request_final() failed\\n\");\n\t\tcc_unmap_result(dev, state, digestsize, result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcc_req.user_cb = cc_digest_complete;\n\tcc_req.user_arg = req;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tif (is_hmac) {\n\t\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t\t     ctx->inter_digestsize, NS_BIT);\n\t} else {\n\t\tlarval_digest_addr = cc_larval_digest_addr(ctx->drvdata,\n\t\t\t\t\t\t\t   ctx->hash_mode);\n\t\tset_din_sram(&desc[idx], larval_digest_addr,\n\t\t\t     ctx->inter_digestsize);\n\t}\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\n\tif (is_hmac) {\n\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t     state->digest_bytes_len_dma_addr,\n\t\t\t     ctx->hash_len, NS_BIT);\n\t} else {\n\t\tset_din_const(&desc[idx], 0, ctx->hash_len);\n\t\tif (nbytes)\n\t\t\tset_cipher_config1(&desc[idx], HASH_PADDING_ENABLED);\n\t\telse\n\t\t\tset_cipher_do(&desc[idx], DO_PAD);\n\t}\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tidx++;\n\n\tcc_set_desc(state, ctx, DIN_HASH, desc, false, &idx);\n\n\tif (is_hmac) {\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_dout_dlli(&desc[idx], state->digest_buff_dma_addr,\n\t\t\t      ctx->hash_len, NS_BIT, 0);\n\t\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\t\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE1);\n\t\tset_cipher_do(&desc[idx], DO_PAD);\n\t\tidx++;\n\n\t\tidx = cc_fin_hmac(desc, req, idx);\n\t}\n\n\tidx = cc_fin_result(desc, req, idx);\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, src, true);\n\t\tcc_unmap_result(dev, state, digestsize, result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_restore_hash(struct cc_hw_desc *desc, struct cc_hash_ctx *ctx,\n\t\t\t   struct ahash_req_ctx *state, unsigned int idx)\n{\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t     ctx->inter_digestsize, NS_BIT);\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_cipher_config1(&desc[idx], HASH_PADDING_DISABLED);\n\tset_din_type(&desc[idx], DMA_DLLI, state->digest_bytes_len_dma_addr,\n\t\t     ctx->hash_len, NS_BIT);\n\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tidx++;\n\n\tcc_set_desc(state, ctx, DIN_HASH, desc, false, &idx);\n\n\treturn idx;\n}\n\nstatic int cc_hash_update(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tunsigned int block_size = crypto_tfm_alg_blocksize(&tfm->base);\n\tstruct scatterlist *src = req->src;\n\tunsigned int nbytes = req->nbytes;\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tu32 idx = 0;\n\tint rc;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"===== %s-update (%d) ====\\n\", ctx->is_hmac ?\n\t\t\"hmac\" : \"hash\", nbytes);\n\n\tif (nbytes == 0) {\n\t\t \n\t\treturn 0;\n\t}\n\n\trc = cc_map_hash_request_update(ctx->drvdata, state, src, nbytes,\n\t\t\t\t\tblock_size, flags);\n\tif (rc) {\n\t\tif (rc == 1) {\n\t\t\tdev_dbg(dev, \" data size not require HW update %x\\n\",\n\t\t\t\tnbytes);\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t\tdev_err(dev, \"map_ahash_request_update() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\tcc_unmap_hash_request(dev, state, src, true);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tcc_req.user_cb = cc_update_complete;\n\tcc_req.user_arg = req;\n\n\tidx = cc_restore_hash(desc, ctx, state, idx);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_dout_dlli(&desc[idx], state->digest_buff_dma_addr,\n\t\t      ctx->inter_digestsize, NS_BIT, 0);\n\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_dout_dlli(&desc[idx], state->digest_bytes_len_dma_addr,\n\t\t      ctx->hash_len, NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE1);\n\tidx++;\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, src, true);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_do_finup(struct ahash_request *req, bool update)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\tstruct scatterlist *src = req->src;\n\tunsigned int nbytes = req->nbytes;\n\tu8 *result = req->result;\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tbool is_hmac = ctx->is_hmac;\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tunsigned int idx = 0;\n\tint rc;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"===== %s-%s (%d) ====\\n\", is_hmac ? \"hmac\" : \"hash\",\n\t\tupdate ? \"finup\" : \"final\", nbytes);\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cc_map_hash_request_final(ctx->drvdata, state, src, nbytes, update,\n\t\t\t\t      flags)) {\n\t\tdev_err(dev, \"map_ahash_request_final() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\tif (cc_map_result(dev, state, digestsize)) {\n\t\tdev_err(dev, \"map_ahash_digest() failed\\n\");\n\t\tcc_unmap_hash_request(dev, state, src, true);\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcc_req.user_cb = cc_hash_complete;\n\tcc_req.user_arg = req;\n\n\tidx = cc_restore_hash(desc, ctx, state, idx);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_cipher_do(&desc[idx], DO_PAD);\n\tset_hash_cipher_mode(&desc[idx], ctx->hw_mode, ctx->hash_mode);\n\tset_dout_dlli(&desc[idx], state->digest_bytes_len_dma_addr,\n\t\t      ctx->hash_len, NS_BIT, 0);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE1);\n\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\tidx++;\n\n\tif (is_hmac)\n\t\tidx = cc_fin_hmac(desc, req, idx);\n\n\tidx = cc_fin_result(desc, req, idx);\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, src, true);\n\t\tcc_unmap_result(dev, state, digestsize, result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_hash_finup(struct ahash_request *req)\n{\n\treturn cc_do_finup(req, true);\n}\n\n\nstatic int cc_hash_final(struct ahash_request *req)\n{\n\treturn cc_do_finup(req, false);\n}\n\nstatic int cc_hash_init(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tdev_dbg(dev, \"===== init (%d) ====\\n\", req->nbytes);\n\n\tcc_init_req(dev, state, ctx);\n\n\treturn 0;\n}\n\nstatic int cc_hash_setkey(struct crypto_ahash *ahash, const u8 *key,\n\t\t\t  unsigned int keylen)\n{\n\tunsigned int hmac_pad_const[2] = { HMAC_IPAD_CONST, HMAC_OPAD_CONST };\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hash_ctx *ctx = NULL;\n\tint blocksize = 0;\n\tint digestsize = 0;\n\tint i, idx = 0, rc = 0;\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tu32 larval_addr;\n\tstruct device *dev;\n\n\tctx = crypto_ahash_ctx_dma(ahash);\n\tdev = drvdata_to_dev(ctx->drvdata);\n\tdev_dbg(dev, \"start keylen: %d\", keylen);\n\n\tblocksize = crypto_tfm_alg_blocksize(&ahash->base);\n\tdigestsize = crypto_ahash_digestsize(ahash);\n\n\tlarval_addr = cc_larval_digest_addr(ctx->drvdata, ctx->hash_mode);\n\n\t \n\tctx->key_params.keylen = keylen;\n\tctx->key_params.key_dma_addr = 0;\n\tctx->is_hmac = true;\n\tctx->key_params.key = NULL;\n\n\tif (keylen) {\n\t\tctx->key_params.key = kmemdup(key, keylen, GFP_KERNEL);\n\t\tif (!ctx->key_params.key)\n\t\t\treturn -ENOMEM;\n\n\t\tctx->key_params.key_dma_addr =\n\t\t\tdma_map_single(dev, ctx->key_params.key, keylen,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, ctx->key_params.key_dma_addr)) {\n\t\t\tdev_err(dev, \"Mapping key va=0x%p len=%u for DMA failed\\n\",\n\t\t\t\tctx->key_params.key, keylen);\n\t\t\tkfree_sensitive(ctx->key_params.key);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tdev_dbg(dev, \"mapping key-buffer: key_dma_addr=%pad keylen=%u\\n\",\n\t\t\t&ctx->key_params.key_dma_addr, ctx->key_params.keylen);\n\n\t\tif (keylen > blocksize) {\n\t\t\t \n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\t\tset_din_sram(&desc[idx], larval_addr,\n\t\t\t\t     ctx->inter_digestsize);\n\t\t\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\t\t\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\t\t\tidx++;\n\n\t\t\t \n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\t\tset_din_const(&desc[idx], 0, ctx->hash_len);\n\t\t\tset_cipher_config1(&desc[idx], HASH_PADDING_ENABLED);\n\t\t\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\t\t\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\t\t\tidx++;\n\n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t\t     ctx->key_params.key_dma_addr, keylen,\n\t\t\t\t     NS_BIT);\n\t\t\tset_flow_mode(&desc[idx], DIN_HASH);\n\t\t\tidx++;\n\n\t\t\t \n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\t\tset_dout_dlli(&desc[idx], ctx->opad_tmp_keys_dma_addr,\n\t\t\t\t      digestsize, NS_BIT, 0);\n\t\t\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\t\t\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\t\t\tset_cipher_config1(&desc[idx], HASH_PADDING_DISABLED);\n\t\t\tcc_set_endianity(ctx->hash_mode, &desc[idx]);\n\t\t\tidx++;\n\n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_din_const(&desc[idx], 0, (blocksize - digestsize));\n\t\t\tset_flow_mode(&desc[idx], BYPASS);\n\t\t\tset_dout_dlli(&desc[idx],\n\t\t\t\t      (ctx->opad_tmp_keys_dma_addr +\n\t\t\t\t       digestsize),\n\t\t\t\t      (blocksize - digestsize), NS_BIT, 0);\n\t\t\tidx++;\n\t\t} else {\n\t\t\thw_desc_init(&desc[idx]);\n\t\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t\t     ctx->key_params.key_dma_addr, keylen,\n\t\t\t\t     NS_BIT);\n\t\t\tset_flow_mode(&desc[idx], BYPASS);\n\t\t\tset_dout_dlli(&desc[idx], ctx->opad_tmp_keys_dma_addr,\n\t\t\t\t      keylen, NS_BIT, 0);\n\t\t\tidx++;\n\n\t\t\tif ((blocksize - keylen)) {\n\t\t\t\thw_desc_init(&desc[idx]);\n\t\t\t\tset_din_const(&desc[idx], 0,\n\t\t\t\t\t      (blocksize - keylen));\n\t\t\t\tset_flow_mode(&desc[idx], BYPASS);\n\t\t\t\tset_dout_dlli(&desc[idx],\n\t\t\t\t\t      (ctx->opad_tmp_keys_dma_addr +\n\t\t\t\t\t       keylen), (blocksize - keylen),\n\t\t\t\t\t      NS_BIT, 0);\n\t\t\t\tidx++;\n\t\t\t}\n\t\t}\n\t} else {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_const(&desc[idx], 0, blocksize);\n\t\tset_flow_mode(&desc[idx], BYPASS);\n\t\tset_dout_dlli(&desc[idx], (ctx->opad_tmp_keys_dma_addr),\n\t\t\t      blocksize, NS_BIT, 0);\n\t\tidx++;\n\t}\n\n\trc = cc_send_sync_request(ctx->drvdata, &cc_req, desc, idx);\n\tif (rc) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tgoto out;\n\t}\n\n\t \n\tfor (idx = 0, i = 0; i < 2; i++) {\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_din_sram(&desc[idx], larval_addr, ctx->inter_digestsize);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\t\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_din_const(&desc[idx], 0, ctx->hash_len);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\t\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_xor_val(&desc[idx], hmac_pad_const[i]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_HASH);\n\t\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE1);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_type(&desc[idx], DMA_DLLI, ctx->opad_tmp_keys_dma_addr,\n\t\t\t     blocksize, NS_BIT);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_xor_active(&desc[idx]);\n\t\tset_flow_mode(&desc[idx], DIN_HASH);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tif (i > 0)  \n\t\t\tset_dout_dlli(&desc[idx], ctx->opad_tmp_keys_dma_addr,\n\t\t\t\t      ctx->inter_digestsize, NS_BIT, 0);\n\t\telse  \n\t\t\tset_dout_dlli(&desc[idx], ctx->digest_buff_dma_addr,\n\t\t\t\t      ctx->inter_digestsize, NS_BIT, 0);\n\t\tset_flow_mode(&desc[idx], S_HASH_to_DOUT);\n\t\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\t\tidx++;\n\t}\n\n\trc = cc_send_sync_request(ctx->drvdata, &cc_req, desc, idx);\n\nout:\n\tif (ctx->key_params.key_dma_addr) {\n\t\tdma_unmap_single(dev, ctx->key_params.key_dma_addr,\n\t\t\t\t ctx->key_params.keylen, DMA_TO_DEVICE);\n\t\tdev_dbg(dev, \"Unmapped key-buffer: key_dma_addr=%pad keylen=%u\\n\",\n\t\t\t&ctx->key_params.key_dma_addr, ctx->key_params.keylen);\n\t}\n\n\tkfree_sensitive(ctx->key_params.key);\n\n\treturn rc;\n}\n\nstatic int cc_xcbc_setkey(struct crypto_ahash *ahash,\n\t\t\t  const u8 *key, unsigned int keylen)\n{\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tint rc = 0;\n\tunsigned int idx = 0;\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\n\tdev_dbg(dev, \"===== setkey (%d) ====\\n\", keylen);\n\n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128:\n\tcase AES_KEYSIZE_192:\n\tcase AES_KEYSIZE_256:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tctx->key_params.keylen = keylen;\n\n\tctx->key_params.key = kmemdup(key, keylen, GFP_KERNEL);\n\tif (!ctx->key_params.key)\n\t\treturn -ENOMEM;\n\n\tctx->key_params.key_dma_addr =\n\t\tdma_map_single(dev, ctx->key_params.key, keylen, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, ctx->key_params.key_dma_addr)) {\n\t\tdev_err(dev, \"Mapping key va=0x%p len=%u for DMA failed\\n\",\n\t\t\tkey, keylen);\n\t\tkfree_sensitive(ctx->key_params.key);\n\t\treturn -ENOMEM;\n\t}\n\tdev_dbg(dev, \"mapping key-buffer: key_dma_addr=%pad keylen=%u\\n\",\n\t\t&ctx->key_params.key_dma_addr, ctx->key_params.keylen);\n\n\tctx->is_hmac = true;\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, ctx->key_params.key_dma_addr,\n\t\t     keylen, NS_BIT);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_ECB);\n\tset_cipher_config0(&desc[idx], DRV_CRYPTO_DIRECTION_ENCRYPT);\n\tset_key_size_aes(&desc[idx], keylen);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tidx++;\n\n\thw_desc_init(&desc[idx]);\n\tset_din_const(&desc[idx], 0x01010101, CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], DIN_AES_DOUT);\n\tset_dout_dlli(&desc[idx],\n\t\t      (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K1_OFFSET),\n\t\t      CC_AES_128_BIT_KEY_SIZE, NS_BIT, 0);\n\tidx++;\n\n\thw_desc_init(&desc[idx]);\n\tset_din_const(&desc[idx], 0x02020202, CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], DIN_AES_DOUT);\n\tset_dout_dlli(&desc[idx],\n\t\t      (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K2_OFFSET),\n\t\t      CC_AES_128_BIT_KEY_SIZE, NS_BIT, 0);\n\tidx++;\n\n\thw_desc_init(&desc[idx]);\n\tset_din_const(&desc[idx], 0x03030303, CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], DIN_AES_DOUT);\n\tset_dout_dlli(&desc[idx],\n\t\t      (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K3_OFFSET),\n\t\t      CC_AES_128_BIT_KEY_SIZE, NS_BIT, 0);\n\tidx++;\n\n\trc = cc_send_sync_request(ctx->drvdata, &cc_req, desc, idx);\n\n\tdma_unmap_single(dev, ctx->key_params.key_dma_addr,\n\t\t\t ctx->key_params.keylen, DMA_TO_DEVICE);\n\tdev_dbg(dev, \"Unmapped key-buffer: key_dma_addr=%pad keylen=%u\\n\",\n\t\t&ctx->key_params.key_dma_addr, ctx->key_params.keylen);\n\n\tkfree_sensitive(ctx->key_params.key);\n\n\treturn rc;\n}\n\nstatic int cc_cmac_setkey(struct crypto_ahash *ahash,\n\t\t\t  const u8 *key, unsigned int keylen)\n{\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tdev_dbg(dev, \"===== setkey (%d) ====\\n\", keylen);\n\n\tctx->is_hmac = true;\n\n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128:\n\tcase AES_KEYSIZE_192:\n\tcase AES_KEYSIZE_256:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tctx->key_params.keylen = keylen;\n\n\t \n\n\tdma_sync_single_for_cpu(dev, ctx->opad_tmp_keys_dma_addr,\n\t\t\t\tkeylen, DMA_TO_DEVICE);\n\n\tmemcpy(ctx->opad_tmp_keys_buff, key, keylen);\n\tif (keylen == 24) {\n\t\tmemset(ctx->opad_tmp_keys_buff + 24, 0,\n\t\t       CC_AES_KEY_SIZE_MAX - 24);\n\t}\n\n\tdma_sync_single_for_device(dev, ctx->opad_tmp_keys_dma_addr,\n\t\t\t\t   keylen, DMA_TO_DEVICE);\n\n\tctx->key_params.keylen = keylen;\n\n\treturn 0;\n}\n\nstatic void cc_free_ctx(struct cc_hash_ctx *ctx)\n{\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tif (ctx->digest_buff_dma_addr) {\n\t\tdma_unmap_single(dev, ctx->digest_buff_dma_addr,\n\t\t\t\t sizeof(ctx->digest_buff), DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped digest-buffer: digest_buff_dma_addr=%pad\\n\",\n\t\t\t&ctx->digest_buff_dma_addr);\n\t\tctx->digest_buff_dma_addr = 0;\n\t}\n\tif (ctx->opad_tmp_keys_dma_addr) {\n\t\tdma_unmap_single(dev, ctx->opad_tmp_keys_dma_addr,\n\t\t\t\t sizeof(ctx->opad_tmp_keys_buff),\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tdev_dbg(dev, \"Unmapped opad-digest: opad_tmp_keys_dma_addr=%pad\\n\",\n\t\t\t&ctx->opad_tmp_keys_dma_addr);\n\t\tctx->opad_tmp_keys_dma_addr = 0;\n\t}\n\n\tctx->key_params.keylen = 0;\n}\n\nstatic int cc_alloc_ctx(struct cc_hash_ctx *ctx)\n{\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tctx->key_params.keylen = 0;\n\n\tctx->digest_buff_dma_addr =\n\t\tdma_map_single(dev, ctx->digest_buff, sizeof(ctx->digest_buff),\n\t\t\t       DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, ctx->digest_buff_dma_addr)) {\n\t\tdev_err(dev, \"Mapping digest len %zu B at va=%pK for DMA failed\\n\",\n\t\t\tsizeof(ctx->digest_buff), ctx->digest_buff);\n\t\tgoto fail;\n\t}\n\tdev_dbg(dev, \"Mapped digest %zu B at va=%pK to dma=%pad\\n\",\n\t\tsizeof(ctx->digest_buff), ctx->digest_buff,\n\t\t&ctx->digest_buff_dma_addr);\n\n\tctx->opad_tmp_keys_dma_addr =\n\t\tdma_map_single(dev, ctx->opad_tmp_keys_buff,\n\t\t\t       sizeof(ctx->opad_tmp_keys_buff),\n\t\t\t       DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, ctx->opad_tmp_keys_dma_addr)) {\n\t\tdev_err(dev, \"Mapping opad digest %zu B at va=%pK for DMA failed\\n\",\n\t\t\tsizeof(ctx->opad_tmp_keys_buff),\n\t\t\tctx->opad_tmp_keys_buff);\n\t\tgoto fail;\n\t}\n\tdev_dbg(dev, \"Mapped opad_tmp_keys %zu B at va=%pK to dma=%pad\\n\",\n\t\tsizeof(ctx->opad_tmp_keys_buff), ctx->opad_tmp_keys_buff,\n\t\t&ctx->opad_tmp_keys_dma_addr);\n\n\tctx->is_hmac = false;\n\treturn 0;\n\nfail:\n\tcc_free_ctx(ctx);\n\treturn -ENOMEM;\n}\n\nstatic int cc_get_hash_len(struct crypto_tfm *tfm)\n{\n\tstruct cc_hash_ctx *ctx = crypto_tfm_ctx_dma(tfm);\n\n\tif (ctx->hash_mode == DRV_HASH_SM3)\n\t\treturn CC_SM3_HASH_LEN_SIZE;\n\telse\n\t\treturn cc_get_default_hash_len(ctx->drvdata);\n}\n\nstatic int cc_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct cc_hash_ctx *ctx = crypto_tfm_ctx_dma(tfm);\n\tstruct hash_alg_common *hash_alg_common =\n\t\tcontainer_of(tfm->__crt_alg, struct hash_alg_common, base);\n\tstruct ahash_alg *ahash_alg =\n\t\tcontainer_of(hash_alg_common, struct ahash_alg, halg);\n\tstruct cc_hash_alg *cc_alg =\n\t\t\tcontainer_of(ahash_alg, struct cc_hash_alg, ahash_alg);\n\n\tcrypto_ahash_set_reqsize_dma(__crypto_ahash_cast(tfm),\n\t\t\t\t     sizeof(struct ahash_req_ctx));\n\n\tctx->hash_mode = cc_alg->hash_mode;\n\tctx->hw_mode = cc_alg->hw_mode;\n\tctx->inter_digestsize = cc_alg->inter_digestsize;\n\tctx->drvdata = cc_alg->drvdata;\n\tctx->hash_len = cc_get_hash_len(tfm);\n\treturn cc_alloc_ctx(ctx);\n}\n\nstatic void cc_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct cc_hash_ctx *ctx = crypto_tfm_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tdev_dbg(dev, \"cc_cra_exit\");\n\tcc_free_ctx(ctx);\n}\n\nstatic int cc_mac_update(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tunsigned int block_size = crypto_tfm_alg_blocksize(&tfm->base);\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tint rc;\n\tu32 idx = 0;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tif (req->nbytes == 0) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tstate->xcbc_count++;\n\n\trc = cc_map_hash_request_update(ctx->drvdata, state, req->src,\n\t\t\t\t\treq->nbytes, block_size, flags);\n\tif (rc) {\n\t\tif (rc == 1) {\n\t\t\tdev_dbg(dev, \" data size not require HW update %x\\n\",\n\t\t\t\treq->nbytes);\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t\tdev_err(dev, \"map_ahash_request_update() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx->hw_mode == DRV_CIPHER_XCBC_MAC)\n\t\tcc_setup_xcbc(req, desc, &idx);\n\telse\n\t\tcc_setup_cmac(req, desc, &idx);\n\n\tcc_set_desc(state, ctx, DIN_AES_DOUT, desc, true, &idx);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tset_dout_dlli(&desc[idx], state->digest_buff_dma_addr,\n\t\t      ctx->inter_digestsize, NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_AES_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tidx++;\n\n\t \n\tcc_req.user_cb = cc_update_complete;\n\tcc_req.user_arg = req;\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_mac_final(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tint idx = 0;\n\tint rc = 0;\n\tu32 key_size, key_len;\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\tu32 rem_cnt = *cc_hash_buf_cnt(state);\n\n\tif (ctx->hw_mode == DRV_CIPHER_XCBC_MAC) {\n\t\tkey_size = CC_AES_128_BIT_KEY_SIZE;\n\t\tkey_len  = CC_AES_128_BIT_KEY_SIZE;\n\t} else {\n\t\tkey_size = (ctx->key_params.keylen == 24) ? AES_MAX_KEY_SIZE :\n\t\t\tctx->key_params.keylen;\n\t\tkey_len =  ctx->key_params.keylen;\n\t}\n\n\tdev_dbg(dev, \"===== final  xcbc reminder (%d) ====\\n\", rem_cnt);\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cc_map_hash_request_final(ctx->drvdata, state, req->src,\n\t\t\t\t      req->nbytes, 0, flags)) {\n\t\tdev_err(dev, \"map_ahash_request_final() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_result(dev, state, digestsize)) {\n\t\tdev_err(dev, \"map_ahash_digest() failed\\n\");\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcc_req.user_cb = cc_hash_complete;\n\tcc_req.user_arg = req;\n\n\tif (state->xcbc_count && rem_cnt == 0) {\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], DRV_CIPHER_ECB);\n\t\tset_cipher_config0(&desc[idx], DRV_CRYPTO_DIRECTION_DECRYPT);\n\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t     (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K1_OFFSET),\n\t\t\t     key_size, NS_BIT);\n\t\tset_key_size_aes(&desc[idx], key_len);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\t\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t\t     CC_AES_BLOCK_SIZE, NS_BIT);\n\t\tset_dout_dlli(&desc[idx], state->digest_buff_dma_addr,\n\t\t\t      CC_AES_BLOCK_SIZE, NS_BIT, 0);\n\t\tset_flow_mode(&desc[idx], DIN_AES_DOUT);\n\t\tidx++;\n\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_no_dma(&desc[idx], 0, 0xfffff0);\n\t\tset_dout_no_dma(&desc[idx], 0, 0, 1);\n\t\tidx++;\n\t}\n\n\tif (ctx->hw_mode == DRV_CIPHER_XCBC_MAC)\n\t\tcc_setup_xcbc(req, desc, &idx);\n\telse\n\t\tcc_setup_cmac(req, desc, &idx);\n\n\tif (state->xcbc_count == 0) {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_key_size_aes(&desc[idx], key_len);\n\t\tset_cmac_size0_mode(&desc[idx]);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\t\tidx++;\n\t} else if (rem_cnt > 0) {\n\t\tcc_set_desc(state, ctx, DIN_AES_DOUT, desc, false, &idx);\n\t} else {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_const(&desc[idx], 0x00, CC_AES_BLOCK_SIZE);\n\t\tset_flow_mode(&desc[idx], DIN_AES_DOUT);\n\t\tidx++;\n\t}\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_dout_dlli(&desc[idx], state->digest_result_dma_addr,\n\t\t      digestsize, NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_AES_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tidx++;\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_result(dev, state, digestsize, req->result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_mac_finup(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tint idx = 0;\n\tint rc = 0;\n\tu32 key_len = 0;\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"===== finup xcbc(%d) ====\\n\", req->nbytes);\n\tif (state->xcbc_count > 0 && req->nbytes == 0) {\n\t\tdev_dbg(dev, \"No data to update. Call to fdx_mac_final\\n\");\n\t\treturn cc_mac_final(req);\n\t}\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cc_map_hash_request_final(ctx->drvdata, state, req->src,\n\t\t\t\t      req->nbytes, 1, flags)) {\n\t\tdev_err(dev, \"map_ahash_request_final() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\tif (cc_map_result(dev, state, digestsize)) {\n\t\tdev_err(dev, \"map_ahash_digest() failed\\n\");\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcc_req.user_cb = cc_hash_complete;\n\tcc_req.user_arg = req;\n\n\tif (ctx->hw_mode == DRV_CIPHER_XCBC_MAC) {\n\t\tkey_len = CC_AES_128_BIT_KEY_SIZE;\n\t\tcc_setup_xcbc(req, desc, &idx);\n\t} else {\n\t\tkey_len = ctx->key_params.keylen;\n\t\tcc_setup_cmac(req, desc, &idx);\n\t}\n\n\tif (req->nbytes == 0) {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_key_size_aes(&desc[idx], key_len);\n\t\tset_cmac_size0_mode(&desc[idx]);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\t\tidx++;\n\t} else {\n\t\tcc_set_desc(state, ctx, DIN_AES_DOUT, desc, false, &idx);\n\t}\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_dout_dlli(&desc[idx], state->digest_result_dma_addr,\n\t\t      digestsize, NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_AES_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tidx++;\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_result(dev, state, digestsize, req->result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_mac_digest(struct ahash_request *req)\n{\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tu32 digestsize = crypto_ahash_digestsize(tfm);\n\tstruct cc_crypto_req cc_req = {};\n\tstruct cc_hw_desc desc[CC_MAX_HASH_SEQ_LEN];\n\tu32 key_len;\n\tunsigned int idx = 0;\n\tint rc;\n\tgfp_t flags = cc_gfp_flags(&req->base);\n\n\tdev_dbg(dev, \"===== -digest mac (%d) ====\\n\",  req->nbytes);\n\n\tcc_init_req(dev, state, ctx);\n\n\tif (cc_map_req(dev, state, ctx)) {\n\t\tdev_err(dev, \"map_ahash_source() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tif (cc_map_result(dev, state, digestsize)) {\n\t\tdev_err(dev, \"map_ahash_digest() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cc_map_hash_request_final(ctx->drvdata, state, req->src,\n\t\t\t\t      req->nbytes, 1, flags)) {\n\t\tdev_err(dev, \"map_ahash_request_final() failed\\n\");\n\t\tcc_unmap_req(dev, state, ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcc_req.user_cb = cc_digest_complete;\n\tcc_req.user_arg = req;\n\n\tif (ctx->hw_mode == DRV_CIPHER_XCBC_MAC) {\n\t\tkey_len = CC_AES_128_BIT_KEY_SIZE;\n\t\tcc_setup_xcbc(req, desc, &idx);\n\t} else {\n\t\tkey_len = ctx->key_params.keylen;\n\t\tcc_setup_cmac(req, desc, &idx);\n\t}\n\n\tif (req->nbytes == 0) {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\t\tset_key_size_aes(&desc[idx], key_len);\n\t\tset_cmac_size0_mode(&desc[idx]);\n\t\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\t\tidx++;\n\t} else {\n\t\tcc_set_desc(state, ctx, DIN_AES_DOUT, desc, false, &idx);\n\t}\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_dout_dlli(&desc[idx], state->digest_result_dma_addr,\n\t\t      CC_AES_BLOCK_SIZE, NS_BIT, 1);\n\tset_queue_last_ind(ctx->drvdata, &desc[idx]);\n\tset_flow_mode(&desc[idx], S_AES_to_DOUT);\n\tset_setup_mode(&desc[idx], SETUP_WRITE_STATE0);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_cipher_mode(&desc[idx], ctx->hw_mode);\n\tidx++;\n\n\trc = cc_send_request(ctx->drvdata, &cc_req, desc, idx, &req->base);\n\tif (rc != -EINPROGRESS && rc != -EBUSY) {\n\t\tdev_err(dev, \"send_request() failed (rc=%d)\\n\", rc);\n\t\tcc_unmap_hash_request(dev, state, req->src, true);\n\t\tcc_unmap_result(dev, state, digestsize, req->result);\n\t\tcc_unmap_req(dev, state, ctx);\n\t}\n\treturn rc;\n}\n\nstatic int cc_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tu8 *curr_buff = cc_hash_buf(state);\n\tu32 curr_buff_cnt = *cc_hash_buf_cnt(state);\n\tconst u32 tmp = CC_EXPORT_MAGIC;\n\n\tmemcpy(out, &tmp, sizeof(u32));\n\tout += sizeof(u32);\n\n\tmemcpy(out, state->digest_buff, ctx->inter_digestsize);\n\tout += ctx->inter_digestsize;\n\n\tmemcpy(out, state->digest_bytes_len, ctx->hash_len);\n\tout += ctx->hash_len;\n\n\tmemcpy(out, &curr_buff_cnt, sizeof(u32));\n\tout += sizeof(u32);\n\n\tmemcpy(out, curr_buff, curr_buff_cnt);\n\n\treturn 0;\n}\n\nstatic int cc_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(ahash);\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(req);\n\tu32 tmp;\n\n\tmemcpy(&tmp, in, sizeof(u32));\n\tif (tmp != CC_EXPORT_MAGIC)\n\t\treturn -EINVAL;\n\tin += sizeof(u32);\n\n\tcc_init_req(dev, state, ctx);\n\n\tmemcpy(state->digest_buff, in, ctx->inter_digestsize);\n\tin += ctx->inter_digestsize;\n\n\tmemcpy(state->digest_bytes_len, in, ctx->hash_len);\n\tin += ctx->hash_len;\n\n\t \n\tmemcpy(&tmp, in, sizeof(u32));\n\tif (tmp > CC_MAX_HASH_BLCK_SIZE)\n\t\treturn -EINVAL;\n\tin += sizeof(u32);\n\n\tstate->buf_cnt[0] = tmp;\n\tmemcpy(state->buffers[0], in, tmp);\n\n\treturn 0;\n}\n\nstruct cc_hash_template {\n\tchar name[CRYPTO_MAX_ALG_NAME];\n\tchar driver_name[CRYPTO_MAX_ALG_NAME];\n\tchar mac_name[CRYPTO_MAX_ALG_NAME];\n\tchar mac_driver_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int blocksize;\n\tbool is_mac;\n\tbool synchronize;\n\tstruct ahash_alg template_ahash;\n\tint hash_mode;\n\tint hw_mode;\n\tint inter_digestsize;\n\tstruct cc_drvdata *drvdata;\n\tu32 min_hw_rev;\n\tenum cc_std_body std_body;\n};\n\n#define CC_STATE_SIZE(_x) \\\n\t((_x) + HASH_MAX_LEN_SIZE + CC_MAX_HASH_BLCK_SIZE + (2 * sizeof(u32)))\n\n \nstatic struct cc_hash_template driver_hash[] = {\n\t\n\t{\n\t\t.name = \"sha1\",\n\t\t.driver_name = \"sha1-ccree\",\n\t\t.mac_name = \"hmac(sha1)\",\n\t\t.mac_driver_name = \"hmac-sha1-ccree\",\n\t\t.blocksize = SHA1_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.synchronize = false,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SHA1_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SHA1,\n\t\t.hw_mode = DRV_HASH_HW_SHA1,\n\t\t.inter_digestsize = SHA1_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"sha256\",\n\t\t.driver_name = \"sha256-ccree\",\n\t\t.mac_name = \"hmac(sha256)\",\n\t\t.mac_driver_name = \"hmac-sha256-ccree\",\n\t\t.blocksize = SHA256_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SHA256_DIGEST_SIZE)\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SHA256,\n\t\t.hw_mode = DRV_HASH_HW_SHA256,\n\t\t.inter_digestsize = SHA256_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"sha224\",\n\t\t.driver_name = \"sha224-ccree\",\n\t\t.mac_name = \"hmac(sha224)\",\n\t\t.mac_driver_name = \"hmac-sha224-ccree\",\n\t\t.blocksize = SHA224_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SHA256_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SHA224,\n\t\t.hw_mode = DRV_HASH_HW_SHA256,\n\t\t.inter_digestsize = SHA256_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"sha384\",\n\t\t.driver_name = \"sha384-ccree\",\n\t\t.mac_name = \"hmac(sha384)\",\n\t\t.mac_driver_name = \"hmac-sha384-ccree\",\n\t\t.blocksize = SHA384_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SHA512_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SHA384,\n\t\t.hw_mode = DRV_HASH_HW_SHA512,\n\t\t.inter_digestsize = SHA512_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"sha512\",\n\t\t.driver_name = \"sha512-ccree\",\n\t\t.mac_name = \"hmac(sha512)\",\n\t\t.mac_driver_name = \"hmac-sha512-ccree\",\n\t\t.blocksize = SHA512_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SHA512_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SHA512,\n\t\t.hw_mode = DRV_HASH_HW_SHA512,\n\t\t.inter_digestsize = SHA512_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_712,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"md5\",\n\t\t.driver_name = \"md5-ccree\",\n\t\t.mac_name = \"hmac(md5)\",\n\t\t.mac_driver_name = \"hmac-md5-ccree\",\n\t\t.blocksize = MD5_HMAC_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = MD5_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(MD5_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_MD5,\n\t\t.hw_mode = DRV_HASH_HW_MD5,\n\t\t.inter_digestsize = MD5_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.name = \"sm3\",\n\t\t.driver_name = \"sm3-ccree\",\n\t\t.blocksize = SM3_BLOCK_SIZE,\n\t\t.is_mac = false,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_hash_update,\n\t\t\t.final = cc_hash_final,\n\t\t\t.finup = cc_hash_finup,\n\t\t\t.digest = cc_hash_digest,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.setkey = cc_hash_setkey,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = SM3_DIGEST_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(SM3_DIGEST_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_SM3,\n\t\t.hw_mode = DRV_HASH_HW_SM3,\n\t\t.inter_digestsize = SM3_DIGEST_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_713,\n\t\t.std_body = CC_STD_OSCCA,\n\t},\n\t{\n\t\t.mac_name = \"xcbc(aes)\",\n\t\t.mac_driver_name = \"xcbc-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_mac_update,\n\t\t\t.final = cc_mac_final,\n\t\t\t.finup = cc_mac_finup,\n\t\t\t.digest = cc_mac_digest,\n\t\t\t.setkey = cc_xcbc_setkey,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(AES_BLOCK_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_NULL,\n\t\t.hw_mode = DRV_CIPHER_XCBC_MAC,\n\t\t.inter_digestsize = AES_BLOCK_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n\t{\n\t\t.mac_name = \"cmac(aes)\",\n\t\t.mac_driver_name = \"cmac-aes-ccree\",\n\t\t.blocksize = AES_BLOCK_SIZE,\n\t\t.is_mac = true,\n\t\t.template_ahash = {\n\t\t\t.init = cc_hash_init,\n\t\t\t.update = cc_mac_update,\n\t\t\t.final = cc_mac_final,\n\t\t\t.finup = cc_mac_finup,\n\t\t\t.digest = cc_mac_digest,\n\t\t\t.setkey = cc_cmac_setkey,\n\t\t\t.export = cc_hash_export,\n\t\t\t.import = cc_hash_import,\n\t\t\t.halg = {\n\t\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t\t.statesize = CC_STATE_SIZE(AES_BLOCK_SIZE),\n\t\t\t},\n\t\t},\n\t\t.hash_mode = DRV_HASH_NULL,\n\t\t.hw_mode = DRV_CIPHER_CMAC,\n\t\t.inter_digestsize = AES_BLOCK_SIZE,\n\t\t.min_hw_rev = CC_HW_REV_630,\n\t\t.std_body = CC_STD_NIST,\n\t},\n};\n\nstatic struct cc_hash_alg *cc_alloc_hash_alg(struct cc_hash_template *template,\n\t\t\t\t\t     struct device *dev, bool keyed)\n{\n\tstruct cc_hash_alg *t_crypto_alg;\n\tstruct crypto_alg *alg;\n\tstruct ahash_alg *halg;\n\n\tt_crypto_alg = devm_kzalloc(dev, sizeof(*t_crypto_alg), GFP_KERNEL);\n\tif (!t_crypto_alg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tt_crypto_alg->ahash_alg = template->template_ahash;\n\thalg = &t_crypto_alg->ahash_alg;\n\talg = &halg->halg.base;\n\n\tif (keyed) {\n\t\tsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->mac_name);\n\t\tsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->mac_driver_name);\n\t} else {\n\t\thalg->setkey = NULL;\n\t\tsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->name);\n\t\tsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s\",\n\t\t\t template->driver_name);\n\t}\n\talg->cra_module = THIS_MODULE;\n\talg->cra_ctxsize = sizeof(struct cc_hash_ctx) + crypto_dma_padding();\n\talg->cra_priority = CC_CRA_PRIO;\n\talg->cra_blocksize = template->blocksize;\n\talg->cra_alignmask = 0;\n\talg->cra_exit = cc_cra_exit;\n\n\talg->cra_init = cc_cra_init;\n\talg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY;\n\n\tt_crypto_alg->hash_mode = template->hash_mode;\n\tt_crypto_alg->hw_mode = template->hw_mode;\n\tt_crypto_alg->inter_digestsize = template->inter_digestsize;\n\n\treturn t_crypto_alg;\n}\n\nstatic int cc_init_copy_sram(struct cc_drvdata *drvdata, const u32 *data,\n\t\t\t     unsigned int size, u32 *sram_buff_ofs)\n{\n\tstruct cc_hw_desc larval_seq[CC_DIGEST_SIZE_MAX / sizeof(u32)];\n\tunsigned int larval_seq_len = 0;\n\tint rc;\n\n\tcc_set_sram_desc(data, *sram_buff_ofs, size / sizeof(*data),\n\t\t\t larval_seq, &larval_seq_len);\n\trc = send_request_init(drvdata, larval_seq, larval_seq_len);\n\tif (rc)\n\t\treturn rc;\n\n\t*sram_buff_ofs += size;\n\treturn 0;\n}\n\nint cc_init_hash_sram(struct cc_drvdata *drvdata)\n{\n\tstruct cc_hash_handle *hash_handle = drvdata->hash_handle;\n\tu32 sram_buff_ofs = hash_handle->digest_len_sram_addr;\n\tbool large_sha_supported = (drvdata->hw_rev >= CC_HW_REV_712);\n\tbool sm3_supported = (drvdata->hw_rev >= CC_HW_REV_713);\n\tint rc = 0;\n\n\t \n\trc = cc_init_copy_sram(drvdata, cc_digest_len_init,\n\t\t\t       sizeof(cc_digest_len_init), &sram_buff_ofs);\n\tif (rc)\n\t\tgoto init_digest_const_err;\n\n\tif (large_sha_supported) {\n\t\t \n\t\trc = cc_init_copy_sram(drvdata, cc_digest_len_sha512_init,\n\t\t\t\t       sizeof(cc_digest_len_sha512_init),\n\t\t\t\t       &sram_buff_ofs);\n\t\tif (rc)\n\t\t\tgoto init_digest_const_err;\n\t}\n\n\t \n\thash_handle->larval_digest_sram_addr = sram_buff_ofs;\n\n\t \n\trc = cc_init_copy_sram(drvdata, cc_md5_init, sizeof(cc_md5_init),\n\t\t\t       &sram_buff_ofs);\n\tif (rc)\n\t\tgoto init_digest_const_err;\n\n\trc = cc_init_copy_sram(drvdata, cc_sha1_init, sizeof(cc_sha1_init),\n\t\t\t       &sram_buff_ofs);\n\tif (rc)\n\t\tgoto init_digest_const_err;\n\n\trc = cc_init_copy_sram(drvdata, cc_sha224_init, sizeof(cc_sha224_init),\n\t\t\t       &sram_buff_ofs);\n\tif (rc)\n\t\tgoto init_digest_const_err;\n\n\trc = cc_init_copy_sram(drvdata, cc_sha256_init, sizeof(cc_sha256_init),\n\t\t\t       &sram_buff_ofs);\n\tif (rc)\n\t\tgoto init_digest_const_err;\n\n\tif (sm3_supported) {\n\t\trc = cc_init_copy_sram(drvdata, cc_sm3_init,\n\t\t\t\t       sizeof(cc_sm3_init), &sram_buff_ofs);\n\t\tif (rc)\n\t\t\tgoto init_digest_const_err;\n\t}\n\n\tif (large_sha_supported) {\n\t\trc = cc_init_copy_sram(drvdata, cc_sha384_init,\n\t\t\t\t       sizeof(cc_sha384_init), &sram_buff_ofs);\n\t\tif (rc)\n\t\t\tgoto init_digest_const_err;\n\n\t\trc = cc_init_copy_sram(drvdata, cc_sha512_init,\n\t\t\t\t       sizeof(cc_sha512_init), &sram_buff_ofs);\n\t\tif (rc)\n\t\t\tgoto init_digest_const_err;\n\t}\n\ninit_digest_const_err:\n\treturn rc;\n}\n\nint cc_hash_alloc(struct cc_drvdata *drvdata)\n{\n\tstruct cc_hash_handle *hash_handle;\n\tu32 sram_buff;\n\tu32 sram_size_to_alloc;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tint rc = 0;\n\tint alg;\n\n\thash_handle = devm_kzalloc(dev, sizeof(*hash_handle), GFP_KERNEL);\n\tif (!hash_handle)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&hash_handle->hash_list);\n\tdrvdata->hash_handle = hash_handle;\n\n\tsram_size_to_alloc = sizeof(cc_digest_len_init) +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init) +\n\t\t\tsizeof(cc_sha224_init) +\n\t\t\tsizeof(cc_sha256_init);\n\n\tif (drvdata->hw_rev >= CC_HW_REV_713)\n\t\tsram_size_to_alloc += sizeof(cc_sm3_init);\n\n\tif (drvdata->hw_rev >= CC_HW_REV_712)\n\t\tsram_size_to_alloc += sizeof(cc_digest_len_sha512_init) +\n\t\t\tsizeof(cc_sha384_init) + sizeof(cc_sha512_init);\n\n\tsram_buff = cc_sram_alloc(drvdata, sram_size_to_alloc);\n\tif (sram_buff == NULL_SRAM_ADDR) {\n\t\trc = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\thash_handle->digest_len_sram_addr = sram_buff;\n\n\t \n\trc = cc_init_hash_sram(drvdata);\n\tif (rc) {\n\t\tdev_err(dev, \"Init digest CONST failed (rc=%d)\\n\", rc);\n\t\tgoto fail;\n\t}\n\n\t \n\tfor (alg = 0; alg < ARRAY_SIZE(driver_hash); alg++) {\n\t\tstruct cc_hash_alg *t_alg;\n\t\tint hw_mode = driver_hash[alg].hw_mode;\n\n\t\t \n\t\tif ((driver_hash[alg].min_hw_rev > drvdata->hw_rev) ||\n\t\t    !(drvdata->std_bodies & driver_hash[alg].std_body))\n\t\t\tcontinue;\n\n\t\tif (driver_hash[alg].is_mac) {\n\t\t\t \n\t\t\tt_alg = cc_alloc_hash_alg(&driver_hash[alg], dev, true);\n\t\t\tif (IS_ERR(t_alg)) {\n\t\t\t\trc = PTR_ERR(t_alg);\n\t\t\t\tdev_err(dev, \"%s alg allocation failed\\n\",\n\t\t\t\t\tdriver_hash[alg].driver_name);\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tt_alg->drvdata = drvdata;\n\n\t\t\trc = crypto_register_ahash(&t_alg->ahash_alg);\n\t\t\tif (rc) {\n\t\t\t\tdev_err(dev, \"%s alg registration failed\\n\",\n\t\t\t\t\tdriver_hash[alg].driver_name);\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tlist_add_tail(&t_alg->entry, &hash_handle->hash_list);\n\t\t}\n\t\tif (hw_mode == DRV_CIPHER_XCBC_MAC ||\n\t\t    hw_mode == DRV_CIPHER_CMAC)\n\t\t\tcontinue;\n\n\t\t \n\t\tt_alg = cc_alloc_hash_alg(&driver_hash[alg], dev, false);\n\t\tif (IS_ERR(t_alg)) {\n\t\t\trc = PTR_ERR(t_alg);\n\t\t\tdev_err(dev, \"%s alg allocation failed\\n\",\n\t\t\t\tdriver_hash[alg].driver_name);\n\t\t\tgoto fail;\n\t\t}\n\t\tt_alg->drvdata = drvdata;\n\n\t\trc = crypto_register_ahash(&t_alg->ahash_alg);\n\t\tif (rc) {\n\t\t\tdev_err(dev, \"%s alg registration failed\\n\",\n\t\t\t\tdriver_hash[alg].driver_name);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tlist_add_tail(&t_alg->entry, &hash_handle->hash_list);\n\t}\n\n\treturn 0;\n\nfail:\n\tcc_hash_free(drvdata);\n\treturn rc;\n}\n\nint cc_hash_free(struct cc_drvdata *drvdata)\n{\n\tstruct cc_hash_alg *t_hash_alg, *hash_n;\n\tstruct cc_hash_handle *hash_handle = drvdata->hash_handle;\n\n\tlist_for_each_entry_safe(t_hash_alg, hash_n, &hash_handle->hash_list,\n\t\t\t\t entry) {\n\t\tcrypto_unregister_ahash(&t_hash_alg->ahash_alg);\n\t\tlist_del(&t_hash_alg->entry);\n\t}\n\n\treturn 0;\n}\n\nstatic void cc_setup_xcbc(struct ahash_request *areq, struct cc_hw_desc desc[],\n\t\t\t  unsigned int *seq_size)\n{\n\tunsigned int idx = *seq_size;\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(areq);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, (ctx->opad_tmp_keys_dma_addr +\n\t\t\t\t\t    XCBC_MAC_K1_OFFSET),\n\t\t     CC_AES_128_BIT_KEY_SIZE, NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tset_hash_cipher_mode(&desc[idx], DRV_CIPHER_XCBC_MAC, ctx->hash_mode);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t     (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K2_OFFSET),\n\t\t     CC_AES_128_BIT_KEY_SIZE, NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE1);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_XCBC_MAC);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t     (ctx->opad_tmp_keys_dma_addr + XCBC_MAC_K3_OFFSET),\n\t\t     CC_AES_128_BIT_KEY_SIZE, NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE2);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_XCBC_MAC);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t     CC_AES_BLOCK_SIZE, NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_XCBC_MAC);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], CC_AES_128_BIT_KEY_SIZE);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\t*seq_size = idx;\n}\n\nstatic void cc_setup_cmac(struct ahash_request *areq, struct cc_hw_desc desc[],\n\t\t\t  unsigned int *seq_size)\n{\n\tunsigned int idx = *seq_size;\n\tstruct ahash_req_ctx *state = ahash_request_ctx_dma(areq);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct cc_hash_ctx *ctx = crypto_ahash_ctx_dma(tfm);\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, ctx->opad_tmp_keys_dma_addr,\n\t\t     ((ctx->key_params.keylen == 24) ? AES_MAX_KEY_SIZE :\n\t\t      ctx->key_params.keylen), NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_KEY0);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_CMAC);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], ctx->key_params.keylen);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\n\t \n\thw_desc_init(&desc[idx]);\n\tset_din_type(&desc[idx], DMA_DLLI, state->digest_buff_dma_addr,\n\t\t     CC_AES_BLOCK_SIZE, NS_BIT);\n\tset_setup_mode(&desc[idx], SETUP_LOAD_STATE0);\n\tset_cipher_mode(&desc[idx], DRV_CIPHER_CMAC);\n\tset_cipher_config0(&desc[idx], DESC_DIRECTION_ENCRYPT_ENCRYPT);\n\tset_key_size_aes(&desc[idx], ctx->key_params.keylen);\n\tset_flow_mode(&desc[idx], S_DIN_to_AES);\n\tidx++;\n\t*seq_size = idx;\n}\n\nstatic void cc_set_desc(struct ahash_req_ctx *areq_ctx,\n\t\t\tstruct cc_hash_ctx *ctx, unsigned int flow_mode,\n\t\t\tstruct cc_hw_desc desc[], bool is_not_last_data,\n\t\t\tunsigned int *seq_size)\n{\n\tunsigned int idx = *seq_size;\n\tstruct device *dev = drvdata_to_dev(ctx->drvdata);\n\n\tif (areq_ctx->data_dma_buf_type == CC_DMA_BUF_DLLI) {\n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t     sg_dma_address(areq_ctx->curr_sg),\n\t\t\t     areq_ctx->curr_sg->length, NS_BIT);\n\t\tset_flow_mode(&desc[idx], flow_mode);\n\t\tidx++;\n\t} else {\n\t\tif (areq_ctx->data_dma_buf_type == CC_DMA_BUF_NULL) {\n\t\t\tdev_dbg(dev, \" NULL mode\\n\");\n\t\t\t \n\t\t\treturn;\n\t\t}\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_type(&desc[idx], DMA_DLLI,\n\t\t\t     areq_ctx->mlli_params.mlli_dma_addr,\n\t\t\t     areq_ctx->mlli_params.mlli_len, NS_BIT);\n\t\tset_dout_sram(&desc[idx], ctx->drvdata->mlli_sram_addr,\n\t\t\t      areq_ctx->mlli_params.mlli_len);\n\t\tset_flow_mode(&desc[idx], BYPASS);\n\t\tidx++;\n\t\t \n\t\thw_desc_init(&desc[idx]);\n\t\tset_din_type(&desc[idx], DMA_MLLI,\n\t\t\t     ctx->drvdata->mlli_sram_addr,\n\t\t\t     areq_ctx->mlli_nents, NS_BIT);\n\t\tset_flow_mode(&desc[idx], flow_mode);\n\t\tidx++;\n\t}\n\tif (is_not_last_data)\n\t\tset_din_not_last_indication(&desc[(idx - 1)]);\n\t \n\t*seq_size = idx;\n}\n\nstatic const void *cc_larval_digest(struct device *dev, u32 mode)\n{\n\tswitch (mode) {\n\tcase DRV_HASH_MD5:\n\t\treturn cc_md5_init;\n\tcase DRV_HASH_SHA1:\n\t\treturn cc_sha1_init;\n\tcase DRV_HASH_SHA224:\n\t\treturn cc_sha224_init;\n\tcase DRV_HASH_SHA256:\n\t\treturn cc_sha256_init;\n\tcase DRV_HASH_SHA384:\n\t\treturn cc_sha384_init;\n\tcase DRV_HASH_SHA512:\n\t\treturn cc_sha512_init;\n\tcase DRV_HASH_SM3:\n\t\treturn cc_sm3_init;\n\tdefault:\n\t\tdev_err(dev, \"Invalid hash mode (%d)\\n\", mode);\n\t\treturn cc_md5_init;\n\t}\n}\n\n \nu32 cc_larval_digest_addr(void *drvdata, u32 mode)\n{\n\tstruct cc_drvdata *_drvdata = (struct cc_drvdata *)drvdata;\n\tstruct cc_hash_handle *hash_handle = _drvdata->hash_handle;\n\tstruct device *dev = drvdata_to_dev(_drvdata);\n\tbool sm3_supported = (_drvdata->hw_rev >= CC_HW_REV_713);\n\tu32 addr;\n\n\tswitch (mode) {\n\tcase DRV_HASH_NULL:\n\t\tbreak;  \n\tcase DRV_HASH_MD5:\n\t\treturn (hash_handle->larval_digest_sram_addr);\n\tcase DRV_HASH_SHA1:\n\t\treturn (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init));\n\tcase DRV_HASH_SHA224:\n\t\treturn (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init));\n\tcase DRV_HASH_SHA256:\n\t\treturn (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init) +\n\t\t\tsizeof(cc_sha224_init));\n\tcase DRV_HASH_SM3:\n\t\treturn (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init) +\n\t\t\tsizeof(cc_sha224_init) +\n\t\t\tsizeof(cc_sha256_init));\n\tcase DRV_HASH_SHA384:\n\t\taddr = (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init) +\n\t\t\tsizeof(cc_sha224_init) +\n\t\t\tsizeof(cc_sha256_init));\n\t\tif (sm3_supported)\n\t\t\taddr += sizeof(cc_sm3_init);\n\t\treturn addr;\n\tcase DRV_HASH_SHA512:\n\t\taddr = (hash_handle->larval_digest_sram_addr +\n\t\t\tsizeof(cc_md5_init) +\n\t\t\tsizeof(cc_sha1_init) +\n\t\t\tsizeof(cc_sha224_init) +\n\t\t\tsizeof(cc_sha256_init) +\n\t\t\tsizeof(cc_sha384_init));\n\t\tif (sm3_supported)\n\t\t\taddr += sizeof(cc_sm3_init);\n\t\treturn addr;\n\tdefault:\n\t\tdev_err(dev, \"Invalid hash mode (%d)\\n\", mode);\n\t}\n\n\t \n\treturn hash_handle->larval_digest_sram_addr;\n}\n\nu32 cc_digest_len_addr(void *drvdata, u32 mode)\n{\n\tstruct cc_drvdata *_drvdata = (struct cc_drvdata *)drvdata;\n\tstruct cc_hash_handle *hash_handle = _drvdata->hash_handle;\n\tu32 digest_len_addr = hash_handle->digest_len_sram_addr;\n\n\tswitch (mode) {\n\tcase DRV_HASH_SHA1:\n\tcase DRV_HASH_SHA224:\n\tcase DRV_HASH_SHA256:\n\tcase DRV_HASH_MD5:\n\t\treturn digest_len_addr;\n\tcase DRV_HASH_SHA384:\n\tcase DRV_HASH_SHA512:\n\t\treturn  digest_len_addr + sizeof(cc_digest_len_init);\n\tdefault:\n\t\treturn digest_len_addr;  \n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}