{
  "module_name": "cc_request_mgr.c",
  "hash_id": "6628504727533a3c9e2bdd523a4ca5f442e35b73a9801f4a3dcfc6a110d1f3a9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/ccree/cc_request_mgr.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/nospec.h>\n#include \"cc_driver.h\"\n#include \"cc_buffer_mgr.h\"\n#include \"cc_request_mgr.h\"\n#include \"cc_pm.h\"\n\n#define CC_MAX_POLL_ITER\t10\n \n#define CC_MAX_DESC_SEQ_LEN\t23\n\nstruct cc_req_mgr_handle {\n\t \n\tunsigned int hw_queue_size;  \n\tunsigned int min_free_hw_slots;\n\tunsigned int max_used_sw_slots;\n\tstruct cc_crypto_req req_queue[MAX_REQUEST_QUEUE_SIZE];\n\tu32 req_queue_head;\n\tu32 req_queue_tail;\n\tu32 axi_completed;\n\tu32 q_free_slots;\n\t \n\tspinlock_t hw_lock;\n\tstruct cc_hw_desc compl_desc;\n\tu8 *dummy_comp_buff;\n\tdma_addr_t dummy_comp_buff_dma;\n\n\t \n\tstruct list_head backlog;\n\tunsigned int bl_len;\n\tspinlock_t bl_lock;  \n\n#ifdef COMP_IN_WQ\n\tstruct workqueue_struct *workq;\n\tstruct delayed_work compwork;\n#else\n\tstruct tasklet_struct comptask;\n#endif\n};\n\nstruct cc_bl_item {\n\tstruct cc_crypto_req creq;\n\tstruct cc_hw_desc desc[CC_MAX_DESC_SEQ_LEN];\n\tunsigned int len;\n\tstruct list_head list;\n\tbool notif;\n};\n\nstatic const u32 cc_cpp_int_masks[CC_CPP_NUM_ALGS][CC_CPP_NUM_SLOTS] = {\n\t{ BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_0_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_1_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_2_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_3_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_4_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_5_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_6_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_AES_7_INT_BIT_SHIFT) },\n\t{ BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_0_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_1_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_2_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_3_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_4_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_5_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_6_INT_BIT_SHIFT),\n\t  BIT(CC_HOST_IRR_REE_OP_ABORTED_SM_7_INT_BIT_SHIFT) }\n};\n\nstatic void comp_handler(unsigned long devarg);\n#ifdef COMP_IN_WQ\nstatic void comp_work_handler(struct work_struct *work);\n#endif\n\nstatic inline u32 cc_cpp_int_mask(enum cc_cpp_alg alg, int slot)\n{\n\talg = array_index_nospec(alg, CC_CPP_NUM_ALGS);\n\tslot = array_index_nospec(slot, CC_CPP_NUM_SLOTS);\n\n\treturn cc_cpp_int_masks[alg][slot];\n}\n\nvoid cc_req_mgr_fini(struct cc_drvdata *drvdata)\n{\n\tstruct cc_req_mgr_handle *req_mgr_h = drvdata->request_mgr_handle;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\tif (!req_mgr_h)\n\t\treturn;  \n\n\tif (req_mgr_h->dummy_comp_buff_dma) {\n\t\tdma_free_coherent(dev, sizeof(u32), req_mgr_h->dummy_comp_buff,\n\t\t\t\t  req_mgr_h->dummy_comp_buff_dma);\n\t}\n\n\tdev_dbg(dev, \"max_used_hw_slots=%d\\n\", (req_mgr_h->hw_queue_size -\n\t\t\t\t\t\treq_mgr_h->min_free_hw_slots));\n\tdev_dbg(dev, \"max_used_sw_slots=%d\\n\", req_mgr_h->max_used_sw_slots);\n\n#ifdef COMP_IN_WQ\n\tdestroy_workqueue(req_mgr_h->workq);\n#else\n\t \n\ttasklet_kill(&req_mgr_h->comptask);\n#endif\n\tkfree_sensitive(req_mgr_h);\n\tdrvdata->request_mgr_handle = NULL;\n}\n\nint cc_req_mgr_init(struct cc_drvdata *drvdata)\n{\n\tstruct cc_req_mgr_handle *req_mgr_h;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tint rc = 0;\n\n\treq_mgr_h = kzalloc(sizeof(*req_mgr_h), GFP_KERNEL);\n\tif (!req_mgr_h) {\n\t\trc = -ENOMEM;\n\t\tgoto req_mgr_init_err;\n\t}\n\n\tdrvdata->request_mgr_handle = req_mgr_h;\n\n\tspin_lock_init(&req_mgr_h->hw_lock);\n\tspin_lock_init(&req_mgr_h->bl_lock);\n\tINIT_LIST_HEAD(&req_mgr_h->backlog);\n\n#ifdef COMP_IN_WQ\n\tdev_dbg(dev, \"Initializing completion workqueue\\n\");\n\treq_mgr_h->workq = create_singlethread_workqueue(\"ccree\");\n\tif (!req_mgr_h->workq) {\n\t\tdev_err(dev, \"Failed creating work queue\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto req_mgr_init_err;\n\t}\n\tINIT_DELAYED_WORK(&req_mgr_h->compwork, comp_work_handler);\n#else\n\tdev_dbg(dev, \"Initializing completion tasklet\\n\");\n\ttasklet_init(&req_mgr_h->comptask, comp_handler,\n\t\t     (unsigned long)drvdata);\n#endif\n\treq_mgr_h->hw_queue_size = cc_ioread(drvdata,\n\t\t\t\t\t     CC_REG(DSCRPTR_QUEUE_SRAM_SIZE));\n\tdev_dbg(dev, \"hw_queue_size=0x%08X\\n\", req_mgr_h->hw_queue_size);\n\tif (req_mgr_h->hw_queue_size < MIN_HW_QUEUE_SIZE) {\n\t\tdev_err(dev, \"Invalid HW queue size = %u (Min. required is %u)\\n\",\n\t\t\treq_mgr_h->hw_queue_size, MIN_HW_QUEUE_SIZE);\n\t\trc = -ENOMEM;\n\t\tgoto req_mgr_init_err;\n\t}\n\treq_mgr_h->min_free_hw_slots = req_mgr_h->hw_queue_size;\n\treq_mgr_h->max_used_sw_slots = 0;\n\n\t \n\treq_mgr_h->dummy_comp_buff =\n\t\tdma_alloc_coherent(dev, sizeof(u32),\n\t\t\t\t   &req_mgr_h->dummy_comp_buff_dma,\n\t\t\t\t   GFP_KERNEL);\n\tif (!req_mgr_h->dummy_comp_buff) {\n\t\tdev_err(dev, \"Not enough memory to allocate DMA (%zu) dropped buffer\\n\",\n\t\t\tsizeof(u32));\n\t\trc = -ENOMEM;\n\t\tgoto req_mgr_init_err;\n\t}\n\n\t \n\thw_desc_init(&req_mgr_h->compl_desc);\n\tset_din_const(&req_mgr_h->compl_desc, 0, sizeof(u32));\n\tset_dout_dlli(&req_mgr_h->compl_desc, req_mgr_h->dummy_comp_buff_dma,\n\t\t      sizeof(u32), NS_BIT, 1);\n\tset_flow_mode(&req_mgr_h->compl_desc, BYPASS);\n\tset_queue_last_ind(drvdata, &req_mgr_h->compl_desc);\n\n\treturn 0;\n\nreq_mgr_init_err:\n\tcc_req_mgr_fini(drvdata);\n\treturn rc;\n}\n\nstatic void enqueue_seq(struct cc_drvdata *drvdata, struct cc_hw_desc seq[],\n\t\t\tunsigned int seq_len)\n{\n\tint i, w;\n\tvoid __iomem *reg = drvdata->cc_base + CC_REG(DSCRPTR_QUEUE_WORD0);\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\t \n\n\tfor (i = 0; i < seq_len; i++) {\n\t\tfor (w = 0; w <= 5; w++)\n\t\t\twritel_relaxed(seq[i].word[w], reg);\n\n\t\tif (cc_dump_desc)\n\t\t\tdev_dbg(dev, \"desc[%02d]: 0x%08X 0x%08X 0x%08X 0x%08X 0x%08X 0x%08X\\n\",\n\t\t\t\ti, seq[i].word[0], seq[i].word[1],\n\t\t\t\tseq[i].word[2], seq[i].word[3],\n\t\t\t\tseq[i].word[4], seq[i].word[5]);\n\t}\n}\n\n \nstatic void request_mgr_complete(struct device *dev, void *dx_compl_h,\n\t\t\t\t int dummy)\n{\n\tstruct completion *this_compl = dx_compl_h;\n\n\tcomplete(this_compl);\n}\n\nstatic int cc_queues_status(struct cc_drvdata *drvdata,\n\t\t\t    struct cc_req_mgr_handle *req_mgr_h,\n\t\t\t    unsigned int total_seq_len)\n{\n\tunsigned long poll_queue;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\t \n\tif (((req_mgr_h->req_queue_head + 1) & (MAX_REQUEST_QUEUE_SIZE - 1)) ==\n\t    req_mgr_h->req_queue_tail) {\n\t\tdev_err(dev, \"SW FIFO is full. req_queue_head=%d sw_fifo_len=%d\\n\",\n\t\t\treq_mgr_h->req_queue_head, MAX_REQUEST_QUEUE_SIZE);\n\t\treturn -ENOSPC;\n\t}\n\n\tif (req_mgr_h->q_free_slots >= total_seq_len)\n\t\treturn 0;\n\n\t \n\tfor (poll_queue = 0; poll_queue < CC_MAX_POLL_ITER ; poll_queue++) {\n\t\treq_mgr_h->q_free_slots =\n\t\t\tcc_ioread(drvdata, CC_REG(DSCRPTR_QUEUE_CONTENT));\n\t\tif (req_mgr_h->q_free_slots < req_mgr_h->min_free_hw_slots)\n\t\t\treq_mgr_h->min_free_hw_slots = req_mgr_h->q_free_slots;\n\n\t\tif (req_mgr_h->q_free_slots >= total_seq_len) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\n\t\tdev_dbg(dev, \"HW FIFO is full. q_free_slots=%d total_seq_len=%d\\n\",\n\t\t\treq_mgr_h->q_free_slots, total_seq_len);\n\t}\n\t \n\tdev_dbg(dev, \"HW FIFO full, timeout. req_queue_head=%d sw_fifo_len=%d q_free_slots=%d total_seq_len=%d\\n\",\n\t\treq_mgr_h->req_queue_head, MAX_REQUEST_QUEUE_SIZE,\n\t\treq_mgr_h->q_free_slots, total_seq_len);\n\treturn -ENOSPC;\n}\n\n \nstatic void cc_do_send_request(struct cc_drvdata *drvdata,\n\t\t\t       struct cc_crypto_req *cc_req,\n\t\t\t       struct cc_hw_desc *desc, unsigned int len,\n\t\t\t       bool add_comp)\n{\n\tstruct cc_req_mgr_handle *req_mgr_h = drvdata->request_mgr_handle;\n\tunsigned int used_sw_slots;\n\tunsigned int total_seq_len = len;  \n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\tused_sw_slots = ((req_mgr_h->req_queue_head -\n\t\t\t  req_mgr_h->req_queue_tail) &\n\t\t\t (MAX_REQUEST_QUEUE_SIZE - 1));\n\tif (used_sw_slots > req_mgr_h->max_used_sw_slots)\n\t\treq_mgr_h->max_used_sw_slots = used_sw_slots;\n\n\t \n\treq_mgr_h->req_queue[req_mgr_h->req_queue_head] = *cc_req;\n\treq_mgr_h->req_queue_head = (req_mgr_h->req_queue_head + 1) &\n\t\t\t\t    (MAX_REQUEST_QUEUE_SIZE - 1);\n\n\tdev_dbg(dev, \"Enqueue request head=%u\\n\", req_mgr_h->req_queue_head);\n\n\t \n\twmb();\n\n\t \n\n\tenqueue_seq(drvdata, desc, len);\n\n\tif (add_comp) {\n\t\tenqueue_seq(drvdata, &req_mgr_h->compl_desc, 1);\n\t\ttotal_seq_len++;\n\t}\n\n\tif (req_mgr_h->q_free_slots < total_seq_len) {\n\t\t \n\t\tdev_err(dev, \"HW free slot count mismatch.\");\n\t\treq_mgr_h->q_free_slots = 0;\n\t} else {\n\t\t \n\t\treq_mgr_h->q_free_slots -= total_seq_len;\n\t}\n}\n\nstatic void cc_enqueue_backlog(struct cc_drvdata *drvdata,\n\t\t\t       struct cc_bl_item *bli)\n{\n\tstruct cc_req_mgr_handle *mgr = drvdata->request_mgr_handle;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\n\tspin_lock_bh(&mgr->bl_lock);\n\tlist_add_tail(&bli->list, &mgr->backlog);\n\t++mgr->bl_len;\n\tdev_dbg(dev, \"+++bl len: %d\\n\", mgr->bl_len);\n\tspin_unlock_bh(&mgr->bl_lock);\n\ttasklet_schedule(&mgr->comptask);\n}\n\nstatic void cc_proc_backlog(struct cc_drvdata *drvdata)\n{\n\tstruct cc_req_mgr_handle *mgr = drvdata->request_mgr_handle;\n\tstruct cc_bl_item *bli;\n\tstruct cc_crypto_req *creq;\n\tvoid *req;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tint rc;\n\n\tspin_lock(&mgr->bl_lock);\n\n\twhile (mgr->bl_len) {\n\t\tbli = list_first_entry(&mgr->backlog, struct cc_bl_item, list);\n\t\tdev_dbg(dev, \"---bl len: %d\\n\", mgr->bl_len);\n\n\t\tspin_unlock(&mgr->bl_lock);\n\n\n\t\tcreq = &bli->creq;\n\t\treq = creq->user_arg;\n\n\t\t \n\t\tif (!bli->notif) {\n\t\t\tcreq->user_cb(dev, req, -EINPROGRESS);\n\t\t\tbli->notif = true;\n\t\t}\n\n\t\tspin_lock(&mgr->hw_lock);\n\n\t\trc = cc_queues_status(drvdata, mgr, bli->len);\n\t\tif (rc) {\n\t\t\t \n\t\t\tspin_unlock(&mgr->hw_lock);\n\t\t\treturn;\n\t\t}\n\n\t\tcc_do_send_request(drvdata, &bli->creq, bli->desc, bli->len,\n\t\t\t\t   false);\n\t\tspin_unlock(&mgr->hw_lock);\n\n\t\t \n\t\tspin_lock(&mgr->bl_lock);\n\t\tlist_del(&bli->list);\n\t\t--mgr->bl_len;\n\t\tkfree(bli);\n\t}\n\n\tspin_unlock(&mgr->bl_lock);\n}\n\nint cc_send_request(struct cc_drvdata *drvdata, struct cc_crypto_req *cc_req,\n\t\t    struct cc_hw_desc *desc, unsigned int len,\n\t\t    struct crypto_async_request *req)\n{\n\tint rc;\n\tstruct cc_req_mgr_handle *mgr = drvdata->request_mgr_handle;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tbool backlog_ok = req->flags & CRYPTO_TFM_REQ_MAY_BACKLOG;\n\tgfp_t flags = cc_gfp_flags(req);\n\tstruct cc_bl_item *bli;\n\n\trc = cc_pm_get(dev);\n\tif (rc) {\n\t\tdev_err(dev, \"cc_pm_get returned %x\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tspin_lock_bh(&mgr->hw_lock);\n\trc = cc_queues_status(drvdata, mgr, len);\n\n#ifdef CC_DEBUG_FORCE_BACKLOG\n\tif (backlog_ok)\n\t\trc = -ENOSPC;\n#endif  \n\n\tif (rc == -ENOSPC && backlog_ok) {\n\t\tspin_unlock_bh(&mgr->hw_lock);\n\n\t\tbli = kmalloc(sizeof(*bli), flags);\n\t\tif (!bli) {\n\t\t\tcc_pm_put_suspend(dev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmemcpy(&bli->creq, cc_req, sizeof(*cc_req));\n\t\tmemcpy(&bli->desc, desc, len * sizeof(*desc));\n\t\tbli->len = len;\n\t\tbli->notif = false;\n\t\tcc_enqueue_backlog(drvdata, bli);\n\t\treturn -EBUSY;\n\t}\n\n\tif (!rc) {\n\t\tcc_do_send_request(drvdata, cc_req, desc, len, false);\n\t\trc = -EINPROGRESS;\n\t}\n\n\tspin_unlock_bh(&mgr->hw_lock);\n\treturn rc;\n}\n\nint cc_send_sync_request(struct cc_drvdata *drvdata,\n\t\t\t struct cc_crypto_req *cc_req, struct cc_hw_desc *desc,\n\t\t\t unsigned int len)\n{\n\tint rc;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tstruct cc_req_mgr_handle *mgr = drvdata->request_mgr_handle;\n\n\tinit_completion(&cc_req->seq_compl);\n\tcc_req->user_cb = request_mgr_complete;\n\tcc_req->user_arg = &cc_req->seq_compl;\n\n\trc = cc_pm_get(dev);\n\tif (rc) {\n\t\tdev_err(dev, \"cc_pm_get returned %x\\n\", rc);\n\t\treturn rc;\n\t}\n\n\twhile (true) {\n\t\tspin_lock_bh(&mgr->hw_lock);\n\t\trc = cc_queues_status(drvdata, mgr, len + 1);\n\n\t\tif (!rc)\n\t\t\tbreak;\n\n\t\tspin_unlock_bh(&mgr->hw_lock);\n\t\twait_for_completion_interruptible(&drvdata->hw_queue_avail);\n\t\treinit_completion(&drvdata->hw_queue_avail);\n\t}\n\n\tcc_do_send_request(drvdata, cc_req, desc, len, true);\n\tspin_unlock_bh(&mgr->hw_lock);\n\twait_for_completion(&cc_req->seq_compl);\n\treturn 0;\n}\n\n \nint send_request_init(struct cc_drvdata *drvdata, struct cc_hw_desc *desc,\n\t\t      unsigned int len)\n{\n\tstruct cc_req_mgr_handle *req_mgr_h = drvdata->request_mgr_handle;\n\tunsigned int total_seq_len = len;  \n\tint rc = 0;\n\n\t \n\trc = cc_queues_status(drvdata, req_mgr_h, total_seq_len);\n\tif (rc)\n\t\treturn rc;\n\n\tset_queue_last_ind(drvdata, &desc[(len - 1)]);\n\n\t \n\twmb();\n\tenqueue_seq(drvdata, desc, len);\n\n\t \n\treq_mgr_h->q_free_slots =\n\t\tcc_ioread(drvdata, CC_REG(DSCRPTR_QUEUE_CONTENT));\n\n\treturn 0;\n}\n\nvoid complete_request(struct cc_drvdata *drvdata)\n{\n\tstruct cc_req_mgr_handle *request_mgr_handle =\n\t\t\t\t\t\tdrvdata->request_mgr_handle;\n\n\tcomplete(&drvdata->hw_queue_avail);\n#ifdef COMP_IN_WQ\n\tqueue_delayed_work(request_mgr_handle->workq,\n\t\t\t   &request_mgr_handle->compwork, 0);\n#else\n\ttasklet_schedule(&request_mgr_handle->comptask);\n#endif\n}\n\n#ifdef COMP_IN_WQ\nstatic void comp_work_handler(struct work_struct *work)\n{\n\tstruct cc_drvdata *drvdata =\n\t\tcontainer_of(work, struct cc_drvdata, compwork.work);\n\n\tcomp_handler((unsigned long)drvdata);\n}\n#endif\n\nstatic void proc_completions(struct cc_drvdata *drvdata)\n{\n\tstruct cc_crypto_req *cc_req;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tstruct cc_req_mgr_handle *request_mgr_handle =\n\t\t\t\t\t\tdrvdata->request_mgr_handle;\n\tunsigned int *tail = &request_mgr_handle->req_queue_tail;\n\tunsigned int *head = &request_mgr_handle->req_queue_head;\n\tint rc;\n\tu32 mask;\n\n\twhile (request_mgr_handle->axi_completed) {\n\t\trequest_mgr_handle->axi_completed--;\n\n\t\t \n\t\tif (*head == *tail) {\n\t\t\t \n\t\t\tdev_err(dev, \"Request queue is empty head == tail %u\\n\",\n\t\t\t\t*head);\n\t\t\tbreak;\n\t\t}\n\n\t\tcc_req = &request_mgr_handle->req_queue[*tail];\n\n\t\tif (cc_req->cpp.is_cpp) {\n\n\t\t\tdev_dbg(dev, \"CPP request completion slot: %d alg:%d\\n\",\n\t\t\t\tcc_req->cpp.slot, cc_req->cpp.alg);\n\t\t\tmask = cc_cpp_int_mask(cc_req->cpp.alg,\n\t\t\t\t\t       cc_req->cpp.slot);\n\t\t\trc = (drvdata->irq & mask ? -EPERM : 0);\n\t\t\tdev_dbg(dev, \"Got mask: %x irq: %x rc: %d\\n\", mask,\n\t\t\t\tdrvdata->irq, rc);\n\t\t} else {\n\t\t\tdev_dbg(dev, \"None CPP request completion\\n\");\n\t\t\trc = 0;\n\t\t}\n\n\t\tif (cc_req->user_cb)\n\t\t\tcc_req->user_cb(dev, cc_req->user_arg, rc);\n\t\t*tail = (*tail + 1) & (MAX_REQUEST_QUEUE_SIZE - 1);\n\t\tdev_dbg(dev, \"Dequeue request tail=%u\\n\", *tail);\n\t\tdev_dbg(dev, \"Request completed. axi_completed=%d\\n\",\n\t\t\trequest_mgr_handle->axi_completed);\n\t\tcc_pm_put_suspend(dev);\n\t}\n}\n\nstatic inline u32 cc_axi_comp_count(struct cc_drvdata *drvdata)\n{\n\treturn FIELD_GET(AXIM_MON_COMP_VALUE,\n\t\t\t cc_ioread(drvdata, drvdata->axim_mon_offset));\n}\n\n \nstatic void comp_handler(unsigned long devarg)\n{\n\tstruct cc_drvdata *drvdata = (struct cc_drvdata *)devarg;\n\tstruct cc_req_mgr_handle *request_mgr_handle =\n\t\t\t\t\t\tdrvdata->request_mgr_handle;\n\tstruct device *dev = drvdata_to_dev(drvdata);\n\tu32 irq;\n\n\tdev_dbg(dev, \"Completion handler called!\\n\");\n\tirq = (drvdata->irq & drvdata->comp_mask);\n\n\t \n\tcc_iowrite(drvdata, CC_REG(HOST_ICR), irq);\n\n\t \n\n\trequest_mgr_handle->axi_completed += cc_axi_comp_count(drvdata);\n\n\tdev_dbg(dev, \"AXI completion after updated: %d\\n\",\n\t\trequest_mgr_handle->axi_completed);\n\n\twhile (request_mgr_handle->axi_completed) {\n\t\tdo {\n\t\t\tdrvdata->irq |= cc_ioread(drvdata, CC_REG(HOST_IRR));\n\t\t\tirq = (drvdata->irq & drvdata->comp_mask);\n\t\t\tproc_completions(drvdata);\n\n\t\t\t \n\t\t\trequest_mgr_handle->axi_completed +=\n\t\t\t\t\t\tcc_axi_comp_count(drvdata);\n\t\t} while (request_mgr_handle->axi_completed > 0);\n\n\t\tcc_iowrite(drvdata, CC_REG(HOST_ICR), irq);\n\n\t\trequest_mgr_handle->axi_completed += cc_axi_comp_count(drvdata);\n\t}\n\n\t \n\tcc_iowrite(drvdata, CC_REG(HOST_IMR),\n\t\t   cc_ioread(drvdata, CC_REG(HOST_IMR)) & ~drvdata->comp_mask);\n\n\tcc_proc_backlog(drvdata);\n\tdev_dbg(dev, \"Comp. handler done.\\n\");\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}