{
  "module_name": "padlock-sha.c",
  "hash_id": "aa7e909843fcd0b46fe8755383b6d8190c1deadcfd74cb3ccbeeee96d440c824",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/padlock-sha.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/hash.h>\n#include <crypto/padlock.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/scatterlist.h>\n#include <asm/cpu_device_id.h>\n#include <asm/fpu/api.h>\n\nstruct padlock_sha_desc {\n\tstruct shash_desc fallback;\n};\n\nstruct padlock_sha_ctx {\n\tstruct crypto_shash *fallback;\n};\n\nstatic int padlock_sha_init(struct shash_desc *desc)\n{\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\tstruct padlock_sha_ctx *ctx = crypto_shash_ctx(desc->tfm);\n\n\tdctx->fallback.tfm = ctx->fallback;\n\treturn crypto_shash_init(&dctx->fallback);\n}\n\nstatic int padlock_sha_update(struct shash_desc *desc,\n\t\t\t      const u8 *data, unsigned int length)\n{\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\n\treturn crypto_shash_update(&dctx->fallback, data, length);\n}\n\nstatic int padlock_sha_export(struct shash_desc *desc, void *out)\n{\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\n\treturn crypto_shash_export(&dctx->fallback, out);\n}\n\nstatic int padlock_sha_import(struct shash_desc *desc, const void *in)\n{\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\tstruct padlock_sha_ctx *ctx = crypto_shash_ctx(desc->tfm);\n\n\tdctx->fallback.tfm = ctx->fallback;\n\treturn crypto_shash_import(&dctx->fallback, in);\n}\n\nstatic inline void padlock_output_block(uint32_t *src,\n\t\t \tuint32_t *dst, size_t count)\n{\n\twhile (count--)\n\t\t*dst++ = swab32(*src++);\n}\n\nstatic int padlock_sha1_finup(struct shash_desc *desc, const u8 *in,\n\t\t\t      unsigned int count, u8 *out)\n{\n\t \n\t \n\tchar buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\n\t\t((aligned(STACK_ALIGN)));\n\tchar *result = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\tstruct sha1_state state;\n\tunsigned int space;\n\tunsigned int leftover;\n\tint err;\n\n\terr = crypto_shash_export(&dctx->fallback, &state);\n\tif (err)\n\t\tgoto out;\n\n\tif (state.count + count > ULONG_MAX)\n\t\treturn crypto_shash_finup(&dctx->fallback, in, count, out);\n\n\tleftover = ((state.count - 1) & (SHA1_BLOCK_SIZE - 1)) + 1;\n\tspace =  SHA1_BLOCK_SIZE - leftover;\n\tif (space) {\n\t\tif (count > space) {\n\t\t\terr = crypto_shash_update(&dctx->fallback, in, space) ?:\n\t\t\t      crypto_shash_export(&dctx->fallback, &state);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tcount -= space;\n\t\t\tin += space;\n\t\t} else {\n\t\t\tmemcpy(state.buffer + leftover, in, count);\n\t\t\tin = state.buffer;\n\t\t\tcount += leftover;\n\t\t\tstate.count &= ~(SHA1_BLOCK_SIZE - 1);\n\t\t}\n\t}\n\n\tmemcpy(result, &state.state, SHA1_DIGEST_SIZE);\n\n\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xc8\"  \n\t\t      : \\\n\t\t      : \"c\"((unsigned long)state.count + count), \\\n\t\t\t\"a\"((unsigned long)state.count), \\\n\t\t\t\"S\"(in), \"D\"(result));\n\n\tpadlock_output_block((uint32_t *)result, (uint32_t *)out, 5);\n\nout:\n\treturn err;\n}\n\nstatic int padlock_sha1_final(struct shash_desc *desc, u8 *out)\n{\n\tu8 buf[4];\n\n\treturn padlock_sha1_finup(desc, buf, 0, out);\n}\n\nstatic int padlock_sha256_finup(struct shash_desc *desc, const u8 *in,\n\t\t\t\tunsigned int count, u8 *out)\n{\n\t \n\t \n\tchar buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\n\t\t((aligned(STACK_ALIGN)));\n\tchar *result = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\tstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\n\tstruct sha256_state state;\n\tunsigned int space;\n\tunsigned int leftover;\n\tint err;\n\n\terr = crypto_shash_export(&dctx->fallback, &state);\n\tif (err)\n\t\tgoto out;\n\n\tif (state.count + count > ULONG_MAX)\n\t\treturn crypto_shash_finup(&dctx->fallback, in, count, out);\n\n\tleftover = ((state.count - 1) & (SHA256_BLOCK_SIZE - 1)) + 1;\n\tspace =  SHA256_BLOCK_SIZE - leftover;\n\tif (space) {\n\t\tif (count > space) {\n\t\t\terr = crypto_shash_update(&dctx->fallback, in, space) ?:\n\t\t\t      crypto_shash_export(&dctx->fallback, &state);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tcount -= space;\n\t\t\tin += space;\n\t\t} else {\n\t\t\tmemcpy(state.buf + leftover, in, count);\n\t\t\tin = state.buf;\n\t\t\tcount += leftover;\n\t\t\tstate.count &= ~(SHA1_BLOCK_SIZE - 1);\n\t\t}\n\t}\n\n\tmemcpy(result, &state.state, SHA256_DIGEST_SIZE);\n\n\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xd0\"  \n\t\t      : \\\n\t\t      : \"c\"((unsigned long)state.count + count), \\\n\t\t\t\"a\"((unsigned long)state.count), \\\n\t\t\t\"S\"(in), \"D\"(result));\n\n\tpadlock_output_block((uint32_t *)result, (uint32_t *)out, 8);\n\nout:\n\treturn err;\n}\n\nstatic int padlock_sha256_final(struct shash_desc *desc, u8 *out)\n{\n\tu8 buf[4];\n\n\treturn padlock_sha256_finup(desc, buf, 0, out);\n}\n\nstatic int padlock_init_tfm(struct crypto_shash *hash)\n{\n\tconst char *fallback_driver_name = crypto_shash_alg_name(hash);\n\tstruct padlock_sha_ctx *ctx = crypto_shash_ctx(hash);\n\tstruct crypto_shash *fallback_tfm;\n\n\t \n\tfallback_tfm = crypto_alloc_shash(fallback_driver_name, 0,\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(fallback_tfm)) {\n\t\tprintk(KERN_WARNING PFX \"Fallback driver '%s' could not be loaded!\\n\",\n\t\t       fallback_driver_name);\n\t\treturn PTR_ERR(fallback_tfm);\n\t}\n\n\tctx->fallback = fallback_tfm;\n\thash->descsize += crypto_shash_descsize(fallback_tfm);\n\treturn 0;\n}\n\nstatic void padlock_exit_tfm(struct crypto_shash *hash)\n{\n\tstruct padlock_sha_ctx *ctx = crypto_shash_ctx(hash);\n\n\tcrypto_free_shash(ctx->fallback);\n}\n\nstatic struct shash_alg sha1_alg = {\n\t.digestsize\t=\tSHA1_DIGEST_SIZE,\n\t.init   \t= \tpadlock_sha_init,\n\t.update \t=\tpadlock_sha_update,\n\t.finup  \t=\tpadlock_sha1_finup,\n\t.final  \t=\tpadlock_sha1_final,\n\t.export\t\t=\tpadlock_sha_export,\n\t.import\t\t=\tpadlock_sha_import,\n\t.init_tfm\t=\tpadlock_init_tfm,\n\t.exit_tfm\t=\tpadlock_exit_tfm,\n\t.descsize\t=\tsizeof(struct padlock_sha_desc),\n\t.statesize\t=\tsizeof(struct sha1_state),\n\t.base\t\t=\t{\n\t\t.cra_name\t\t=\t\"sha1\",\n\t\t.cra_driver_name\t=\t\"sha1-padlock\",\n\t\t.cra_priority\t\t=\tPADLOCK_CRA_PRIORITY,\n\t\t.cra_flags\t\t=\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t=\tSHA1_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t=\tsizeof(struct padlock_sha_ctx),\n\t\t.cra_module\t\t=\tTHIS_MODULE,\n\t}\n};\n\nstatic struct shash_alg sha256_alg = {\n\t.digestsize\t=\tSHA256_DIGEST_SIZE,\n\t.init   \t= \tpadlock_sha_init,\n\t.update \t=\tpadlock_sha_update,\n\t.finup  \t=\tpadlock_sha256_finup,\n\t.final  \t=\tpadlock_sha256_final,\n\t.export\t\t=\tpadlock_sha_export,\n\t.import\t\t=\tpadlock_sha_import,\n\t.init_tfm\t=\tpadlock_init_tfm,\n\t.exit_tfm\t=\tpadlock_exit_tfm,\n\t.descsize\t=\tsizeof(struct padlock_sha_desc),\n\t.statesize\t=\tsizeof(struct sha256_state),\n\t.base\t\t=\t{\n\t\t.cra_name\t\t=\t\"sha256\",\n\t\t.cra_driver_name\t=\t\"sha256-padlock\",\n\t\t.cra_priority\t\t=\tPADLOCK_CRA_PRIORITY,\n\t\t.cra_flags\t\t=\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t=\tSHA256_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t=\tsizeof(struct padlock_sha_ctx),\n\t\t.cra_module\t\t=\tTHIS_MODULE,\n\t}\n};\n\n \nstatic int padlock_sha1_init_nano(struct shash_desc *desc)\n{\n\tstruct sha1_state *sctx = shash_desc_ctx(desc);\n\n\t*sctx = (struct sha1_state){\n\t\t.state = { SHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3, SHA1_H4 },\n\t};\n\n\treturn 0;\n}\n\nstatic int padlock_sha1_update_nano(struct shash_desc *desc,\n\t\t\tconst u8 *data,\tunsigned int len)\n{\n\tstruct sha1_state *sctx = shash_desc_ctx(desc);\n\tunsigned int partial, done;\n\tconst u8 *src;\n\t \n\tu8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\n\t\t((aligned(STACK_ALIGN)));\n\tu8 *dst = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\n\tpartial = sctx->count & 0x3f;\n\tsctx->count += len;\n\tdone = 0;\n\tsrc = data;\n\tmemcpy(dst, (u8 *)(sctx->state), SHA1_DIGEST_SIZE);\n\n\tif ((partial + len) >= SHA1_BLOCK_SIZE) {\n\n\t\t \n\t\tif (partial) {\n\t\t\tdone = -partial;\n\t\t\tmemcpy(sctx->buffer + partial, data,\n\t\t\t\tdone + SHA1_BLOCK_SIZE);\n\t\t\tsrc = sctx->buffer;\n\t\t\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xc8\"\n\t\t\t: \"+S\"(src), \"+D\"(dst) \\\n\t\t\t: \"a\"((long)-1), \"c\"((unsigned long)1));\n\t\t\tdone += SHA1_BLOCK_SIZE;\n\t\t\tsrc = data + done;\n\t\t}\n\n\t\t \n\t\tif (len - done >= SHA1_BLOCK_SIZE) {\n\t\t\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xc8\"\n\t\t\t: \"+S\"(src), \"+D\"(dst)\n\t\t\t: \"a\"((long)-1),\n\t\t\t\"c\"((unsigned long)((len - done) / SHA1_BLOCK_SIZE)));\n\t\t\tdone += ((len - done) - (len - done) % SHA1_BLOCK_SIZE);\n\t\t\tsrc = data + done;\n\t\t}\n\t\tpartial = 0;\n\t}\n\tmemcpy((u8 *)(sctx->state), dst, SHA1_DIGEST_SIZE);\n\tmemcpy(sctx->buffer + partial, src, len - done);\n\n\treturn 0;\n}\n\nstatic int padlock_sha1_final_nano(struct shash_desc *desc, u8 *out)\n{\n\tstruct sha1_state *state = (struct sha1_state *)shash_desc_ctx(desc);\n\tunsigned int partial, padlen;\n\t__be64 bits;\n\tstatic const u8 padding[64] = { 0x80, };\n\n\tbits = cpu_to_be64(state->count << 3);\n\n\t \n\tpartial = state->count & 0x3f;\n\tpadlen = (partial < 56) ? (56 - partial) : ((64+56) - partial);\n\tpadlock_sha1_update_nano(desc, padding, padlen);\n\n\t \n\tpadlock_sha1_update_nano(desc, (const u8 *)&bits, sizeof(bits));\n\n\t \n\tpadlock_output_block((uint32_t *)(state->state), (uint32_t *)out, 5);\n\n\treturn 0;\n}\n\nstatic int padlock_sha256_init_nano(struct shash_desc *desc)\n{\n\tstruct sha256_state *sctx = shash_desc_ctx(desc);\n\n\t*sctx = (struct sha256_state){\n\t\t.state = { SHA256_H0, SHA256_H1, SHA256_H2, SHA256_H3, \\\n\t\t\t\tSHA256_H4, SHA256_H5, SHA256_H6, SHA256_H7},\n\t};\n\n\treturn 0;\n}\n\nstatic int padlock_sha256_update_nano(struct shash_desc *desc, const u8 *data,\n\t\t\t  unsigned int len)\n{\n\tstruct sha256_state *sctx = shash_desc_ctx(desc);\n\tunsigned int partial, done;\n\tconst u8 *src;\n\t \n\tu8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\n\t\t((aligned(STACK_ALIGN)));\n\tu8 *dst = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\n\n\tpartial = sctx->count & 0x3f;\n\tsctx->count += len;\n\tdone = 0;\n\tsrc = data;\n\tmemcpy(dst, (u8 *)(sctx->state), SHA256_DIGEST_SIZE);\n\n\tif ((partial + len) >= SHA256_BLOCK_SIZE) {\n\n\t\t \n\t\tif (partial) {\n\t\t\tdone = -partial;\n\t\t\tmemcpy(sctx->buf + partial, data,\n\t\t\t\tdone + SHA256_BLOCK_SIZE);\n\t\t\tsrc = sctx->buf;\n\t\t\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xd0\"\n\t\t\t: \"+S\"(src), \"+D\"(dst)\n\t\t\t: \"a\"((long)-1), \"c\"((unsigned long)1));\n\t\t\tdone += SHA256_BLOCK_SIZE;\n\t\t\tsrc = data + done;\n\t\t}\n\n\t\t \n\t\tif (len - done >= SHA256_BLOCK_SIZE) {\n\t\t\tasm volatile (\".byte 0xf3,0x0f,0xa6,0xd0\"\n\t\t\t: \"+S\"(src), \"+D\"(dst)\n\t\t\t: \"a\"((long)-1),\n\t\t\t\"c\"((unsigned long)((len - done) / 64)));\n\t\t\tdone += ((len - done) - (len - done) % 64);\n\t\t\tsrc = data + done;\n\t\t}\n\t\tpartial = 0;\n\t}\n\tmemcpy((u8 *)(sctx->state), dst, SHA256_DIGEST_SIZE);\n\tmemcpy(sctx->buf + partial, src, len - done);\n\n\treturn 0;\n}\n\nstatic int padlock_sha256_final_nano(struct shash_desc *desc, u8 *out)\n{\n\tstruct sha256_state *state =\n\t\t(struct sha256_state *)shash_desc_ctx(desc);\n\tunsigned int partial, padlen;\n\t__be64 bits;\n\tstatic const u8 padding[64] = { 0x80, };\n\n\tbits = cpu_to_be64(state->count << 3);\n\n\t \n\tpartial = state->count & 0x3f;\n\tpadlen = (partial < 56) ? (56 - partial) : ((64+56) - partial);\n\tpadlock_sha256_update_nano(desc, padding, padlen);\n\n\t \n\tpadlock_sha256_update_nano(desc, (const u8 *)&bits, sizeof(bits));\n\n\t \n\tpadlock_output_block((uint32_t *)(state->state), (uint32_t *)out, 8);\n\n\treturn 0;\n}\n\nstatic int padlock_sha_export_nano(struct shash_desc *desc,\n\t\t\t\tvoid *out)\n{\n\tint statesize = crypto_shash_statesize(desc->tfm);\n\tvoid *sctx = shash_desc_ctx(desc);\n\n\tmemcpy(out, sctx, statesize);\n\treturn 0;\n}\n\nstatic int padlock_sha_import_nano(struct shash_desc *desc,\n\t\t\t\tconst void *in)\n{\n\tint statesize = crypto_shash_statesize(desc->tfm);\n\tvoid *sctx = shash_desc_ctx(desc);\n\n\tmemcpy(sctx, in, statesize);\n\treturn 0;\n}\n\nstatic struct shash_alg sha1_alg_nano = {\n\t.digestsize\t=\tSHA1_DIGEST_SIZE,\n\t.init\t\t=\tpadlock_sha1_init_nano,\n\t.update\t\t=\tpadlock_sha1_update_nano,\n\t.final\t\t=\tpadlock_sha1_final_nano,\n\t.export\t\t=\tpadlock_sha_export_nano,\n\t.import\t\t=\tpadlock_sha_import_nano,\n\t.descsize\t=\tsizeof(struct sha1_state),\n\t.statesize\t=\tsizeof(struct sha1_state),\n\t.base\t\t=\t{\n\t\t.cra_name\t\t=\t\"sha1\",\n\t\t.cra_driver_name\t=\t\"sha1-padlock-nano\",\n\t\t.cra_priority\t\t=\tPADLOCK_CRA_PRIORITY,\n\t\t.cra_blocksize\t\t=\tSHA1_BLOCK_SIZE,\n\t\t.cra_module\t\t=\tTHIS_MODULE,\n\t}\n};\n\nstatic struct shash_alg sha256_alg_nano = {\n\t.digestsize\t=\tSHA256_DIGEST_SIZE,\n\t.init\t\t=\tpadlock_sha256_init_nano,\n\t.update\t\t=\tpadlock_sha256_update_nano,\n\t.final\t\t=\tpadlock_sha256_final_nano,\n\t.export\t\t=\tpadlock_sha_export_nano,\n\t.import\t\t=\tpadlock_sha_import_nano,\n\t.descsize\t=\tsizeof(struct sha256_state),\n\t.statesize\t=\tsizeof(struct sha256_state),\n\t.base\t\t=\t{\n\t\t.cra_name\t\t=\t\"sha256\",\n\t\t.cra_driver_name\t=\t\"sha256-padlock-nano\",\n\t\t.cra_priority\t\t=\tPADLOCK_CRA_PRIORITY,\n\t\t.cra_blocksize\t\t=\tSHA256_BLOCK_SIZE,\n\t\t.cra_module\t\t=\tTHIS_MODULE,\n\t}\n};\n\nstatic const struct x86_cpu_id padlock_sha_ids[] = {\n\tX86_MATCH_FEATURE(X86_FEATURE_PHE, NULL),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, padlock_sha_ids);\n\nstatic int __init padlock_init(void)\n{\n\tint rc = -ENODEV;\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\tstruct shash_alg *sha1;\n\tstruct shash_alg *sha256;\n\n\tif (!x86_match_cpu(padlock_sha_ids) || !boot_cpu_has(X86_FEATURE_PHE_EN))\n\t\treturn -ENODEV;\n\n\t \n\tif (c->x86_model < 0x0f) {\n\t\tsha1 = &sha1_alg;\n\t\tsha256 = &sha256_alg;\n\t} else {\n\t\tsha1 = &sha1_alg_nano;\n\t\tsha256 = &sha256_alg_nano;\n\t}\n\n\trc = crypto_register_shash(sha1);\n\tif (rc)\n\t\tgoto out;\n\n\trc = crypto_register_shash(sha256);\n\tif (rc)\n\t\tgoto out_unreg1;\n\n\tprintk(KERN_NOTICE PFX \"Using VIA PadLock ACE for SHA1/SHA256 algorithms.\\n\");\n\n\treturn 0;\n\nout_unreg1:\n\tcrypto_unregister_shash(sha1);\n\nout:\n\tprintk(KERN_ERR PFX \"VIA PadLock SHA1/SHA256 initialization failed.\\n\");\n\treturn rc;\n}\n\nstatic void __exit padlock_fini(void)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\n\tif (c->x86_model >= 0x0f) {\n\t\tcrypto_unregister_shash(&sha1_alg_nano);\n\t\tcrypto_unregister_shash(&sha256_alg_nano);\n\t} else {\n\t\tcrypto_unregister_shash(&sha1_alg);\n\t\tcrypto_unregister_shash(&sha256_alg);\n\t}\n}\n\nmodule_init(padlock_init);\nmodule_exit(padlock_fini);\n\nMODULE_DESCRIPTION(\"VIA PadLock SHA1/SHA256 algorithms support.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Michal Ludvig\");\n\nMODULE_ALIAS_CRYPTO(\"sha1-all\");\nMODULE_ALIAS_CRYPTO(\"sha256-all\");\nMODULE_ALIAS_CRYPTO(\"sha1-padlock\");\nMODULE_ALIAS_CRYPTO(\"sha256-padlock\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}