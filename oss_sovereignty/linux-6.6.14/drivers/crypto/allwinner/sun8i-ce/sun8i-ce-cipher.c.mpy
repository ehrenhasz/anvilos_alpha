{
  "module_name": "sun8i-ce-cipher.c",
  "hash_id": "86245bb2eb005d5abd0ef7ef23aab7781e744b9dca6d047974a4aaf8bc4e57ad",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c",
  "human_readable_source": "\n \n\n#include <linux/bottom_half.h>\n#include <linux/crypto.h>\n#include <linux/dma-mapping.h>\n#include <linux/io.h>\n#include <linux/pm_runtime.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/internal/des.h>\n#include <crypto/internal/skcipher.h>\n#include \"sun8i-ce.h\"\n\nstatic int sun8i_ce_cipher_need_fallback(struct skcipher_request *areq)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct scatterlist *sg;\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tstruct sun8i_ce_alg_template *algt;\n\tunsigned int todo, len;\n\n\talgt = container_of(alg, struct sun8i_ce_alg_template, alg.skcipher.base);\n\n\tif (sg_nents_for_len(areq->src, areq->cryptlen) > MAX_SG ||\n\t    sg_nents_for_len(areq->dst, areq->cryptlen) > MAX_SG) {\n\t\talgt->stat_fb_maxsg++;\n\t\treturn true;\n\t}\n\n\tif (areq->cryptlen < crypto_skcipher_ivsize(tfm)) {\n\t\talgt->stat_fb_leniv++;\n\t\treturn true;\n\t}\n\n\tif (areq->cryptlen == 0) {\n\t\talgt->stat_fb_len0++;\n\t\treturn true;\n\t}\n\n\tif (areq->cryptlen % 16) {\n\t\talgt->stat_fb_mod16++;\n\t\treturn true;\n\t}\n\n\tlen = areq->cryptlen;\n\tsg = areq->src;\n\twhile (sg) {\n\t\tif (!IS_ALIGNED(sg->offset, sizeof(u32))) {\n\t\t\talgt->stat_fb_srcali++;\n\t\t\treturn true;\n\t\t}\n\t\ttodo = min(len, sg->length);\n\t\tif (todo % 4) {\n\t\t\talgt->stat_fb_srclen++;\n\t\t\treturn true;\n\t\t}\n\t\tlen -= todo;\n\t\tsg = sg_next(sg);\n\t}\n\n\tlen = areq->cryptlen;\n\tsg = areq->dst;\n\twhile (sg) {\n\t\tif (!IS_ALIGNED(sg->offset, sizeof(u32))) {\n\t\t\talgt->stat_fb_dstali++;\n\t\t\treturn true;\n\t\t}\n\t\ttodo = min(len, sg->length);\n\t\tif (todo % 4) {\n\t\t\talgt->stat_fb_dstlen++;\n\t\t\treturn true;\n\t\t}\n\t\tlen -= todo;\n\t\tsg = sg_next(sg);\n\t}\n\treturn false;\n}\n\nstatic int sun8i_ce_cipher_fallback(struct skcipher_request *areq)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);\n\tint err;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG)) {\n\t\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\t\tstruct sun8i_ce_alg_template *algt __maybe_unused;\n\n\t\talgt = container_of(alg, struct sun8i_ce_alg_template,\n\t\t\t\t    alg.skcipher.base);\n\n#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG\n\t\talgt->stat_fb++;\n#endif\n\t}\n\n\tskcipher_request_set_tfm(&rctx->fallback_req, op->fallback_tfm);\n\tskcipher_request_set_callback(&rctx->fallback_req, areq->base.flags,\n\t\t\t\t      areq->base.complete, areq->base.data);\n\tskcipher_request_set_crypt(&rctx->fallback_req, areq->src, areq->dst,\n\t\t\t\t   areq->cryptlen, areq->iv);\n\tif (rctx->op_dir & CE_DECRYPTION)\n\t\terr = crypto_skcipher_decrypt(&rctx->fallback_req);\n\telse\n\t\terr = crypto_skcipher_encrypt(&rctx->fallback_req);\n\treturn err;\n}\n\nstatic int sun8i_ce_cipher_prepare(struct crypto_engine *engine, void *async_req)\n{\n\tstruct skcipher_request *areq = container_of(async_req, struct skcipher_request, base);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_ce_dev *ce = op->ce;\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tstruct sun8i_ce_alg_template *algt;\n\tstruct sun8i_ce_flow *chan;\n\tstruct ce_task *cet;\n\tstruct scatterlist *sg;\n\tunsigned int todo, len, offset, ivsize;\n\tu32 common, sym;\n\tint flow, i;\n\tint nr_sgs = 0;\n\tint nr_sgd = 0;\n\tint err = 0;\n\tint ns = sg_nents_for_len(areq->src, areq->cryptlen);\n\tint nd = sg_nents_for_len(areq->dst, areq->cryptlen);\n\n\talgt = container_of(alg, struct sun8i_ce_alg_template, alg.skcipher.base);\n\n\tdev_dbg(ce->dev, \"%s %s %u %x IV(%p %u) key=%u\\n\", __func__,\n\t\tcrypto_tfm_alg_name(areq->base.tfm),\n\t\tareq->cryptlen,\n\t\trctx->op_dir, areq->iv, crypto_skcipher_ivsize(tfm),\n\t\top->keylen);\n\n#ifdef CONFIG_CRYPTO_DEV_SUN8I_CE_DEBUG\n\talgt->stat_req++;\n#endif\n\n\tflow = rctx->flow;\n\n\tchan = &ce->chanlist[flow];\n\n\tcet = chan->tl;\n\tmemset(cet, 0, sizeof(struct ce_task));\n\n\tcet->t_id = cpu_to_le32(flow);\n\tcommon = ce->variant->alg_cipher[algt->ce_algo_id];\n\tcommon |= rctx->op_dir | CE_COMM_INT;\n\tcet->t_common_ctl = cpu_to_le32(common);\n\t \n\tif (ce->variant->cipher_t_dlen_in_bytes)\n\t\tcet->t_dlen = cpu_to_le32(areq->cryptlen);\n\telse\n\t\tcet->t_dlen = cpu_to_le32(areq->cryptlen / 4);\n\n\tsym = ce->variant->op_mode[algt->ce_blockmode];\n\tlen = op->keylen;\n\tswitch (len) {\n\tcase 128 / 8:\n\t\tsym |= CE_AES_128BITS;\n\t\tbreak;\n\tcase 192 / 8:\n\t\tsym |= CE_AES_192BITS;\n\t\tbreak;\n\tcase 256 / 8:\n\t\tsym |= CE_AES_256BITS;\n\t\tbreak;\n\t}\n\n\tcet->t_sym_ctl = cpu_to_le32(sym);\n\tcet->t_asym_ctl = 0;\n\n\trctx->addr_key = dma_map_single(ce->dev, op->key, op->keylen, DMA_TO_DEVICE);\n\tif (dma_mapping_error(ce->dev, rctx->addr_key)) {\n\t\tdev_err(ce->dev, \"Cannot DMA MAP KEY\\n\");\n\t\terr = -EFAULT;\n\t\tgoto theend;\n\t}\n\tcet->t_key = cpu_to_le32(rctx->addr_key);\n\n\tivsize = crypto_skcipher_ivsize(tfm);\n\tif (areq->iv && crypto_skcipher_ivsize(tfm) > 0) {\n\t\trctx->ivlen = ivsize;\n\t\tif (rctx->op_dir & CE_DECRYPTION) {\n\t\t\toffset = areq->cryptlen - ivsize;\n\t\t\tscatterwalk_map_and_copy(chan->backup_iv, areq->src,\n\t\t\t\t\t\t offset, ivsize, 0);\n\t\t}\n\t\tmemcpy(chan->bounce_iv, areq->iv, ivsize);\n\t\trctx->addr_iv = dma_map_single(ce->dev, chan->bounce_iv, rctx->ivlen,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ce->dev, rctx->addr_iv)) {\n\t\t\tdev_err(ce->dev, \"Cannot DMA MAP IV\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto theend_iv;\n\t\t}\n\t\tcet->t_iv = cpu_to_le32(rctx->addr_iv);\n\t}\n\n\tif (areq->src == areq->dst) {\n\t\tnr_sgs = dma_map_sg(ce->dev, areq->src, ns, DMA_BIDIRECTIONAL);\n\t\tif (nr_sgs <= 0 || nr_sgs > MAX_SG) {\n\t\t\tdev_err(ce->dev, \"Invalid sg number %d\\n\", nr_sgs);\n\t\t\terr = -EINVAL;\n\t\t\tgoto theend_iv;\n\t\t}\n\t\tnr_sgd = nr_sgs;\n\t} else {\n\t\tnr_sgs = dma_map_sg(ce->dev, areq->src, ns, DMA_TO_DEVICE);\n\t\tif (nr_sgs <= 0 || nr_sgs > MAX_SG) {\n\t\t\tdev_err(ce->dev, \"Invalid sg number %d\\n\", nr_sgs);\n\t\t\terr = -EINVAL;\n\t\t\tgoto theend_iv;\n\t\t}\n\t\tnr_sgd = dma_map_sg(ce->dev, areq->dst, nd, DMA_FROM_DEVICE);\n\t\tif (nr_sgd <= 0 || nr_sgd > MAX_SG) {\n\t\t\tdev_err(ce->dev, \"Invalid sg number %d\\n\", nr_sgd);\n\t\t\terr = -EINVAL;\n\t\t\tgoto theend_sgs;\n\t\t}\n\t}\n\n\tlen = areq->cryptlen;\n\tfor_each_sg(areq->src, sg, nr_sgs, i) {\n\t\tcet->t_src[i].addr = cpu_to_le32(sg_dma_address(sg));\n\t\ttodo = min(len, sg_dma_len(sg));\n\t\tcet->t_src[i].len = cpu_to_le32(todo / 4);\n\t\tdev_dbg(ce->dev, \"%s total=%u SG(%d %u off=%d) todo=%u\\n\", __func__,\n\t\t\tareq->cryptlen, i, cet->t_src[i].len, sg->offset, todo);\n\t\tlen -= todo;\n\t}\n\tif (len > 0) {\n\t\tdev_err(ce->dev, \"remaining len %d\\n\", len);\n\t\terr = -EINVAL;\n\t\tgoto theend_sgs;\n\t}\n\n\tlen = areq->cryptlen;\n\tfor_each_sg(areq->dst, sg, nr_sgd, i) {\n\t\tcet->t_dst[i].addr = cpu_to_le32(sg_dma_address(sg));\n\t\ttodo = min(len, sg_dma_len(sg));\n\t\tcet->t_dst[i].len = cpu_to_le32(todo / 4);\n\t\tdev_dbg(ce->dev, \"%s total=%u SG(%d %u off=%d) todo=%u\\n\", __func__,\n\t\t\tareq->cryptlen, i, cet->t_dst[i].len, sg->offset, todo);\n\t\tlen -= todo;\n\t}\n\tif (len > 0) {\n\t\tdev_err(ce->dev, \"remaining len %d\\n\", len);\n\t\terr = -EINVAL;\n\t\tgoto theend_sgs;\n\t}\n\n\tchan->timeout = areq->cryptlen;\n\trctx->nr_sgs = nr_sgs;\n\trctx->nr_sgd = nr_sgd;\n\treturn 0;\n\ntheend_sgs:\n\tif (areq->src == areq->dst) {\n\t\tdma_unmap_sg(ce->dev, areq->src, ns, DMA_BIDIRECTIONAL);\n\t} else {\n\t\tif (nr_sgs > 0)\n\t\t\tdma_unmap_sg(ce->dev, areq->src, ns, DMA_TO_DEVICE);\n\t\tdma_unmap_sg(ce->dev, areq->dst, nd, DMA_FROM_DEVICE);\n\t}\n\ntheend_iv:\n\tif (areq->iv && ivsize > 0) {\n\t\tif (rctx->addr_iv)\n\t\t\tdma_unmap_single(ce->dev, rctx->addr_iv, rctx->ivlen, DMA_TO_DEVICE);\n\t\toffset = areq->cryptlen - ivsize;\n\t\tif (rctx->op_dir & CE_DECRYPTION) {\n\t\t\tmemcpy(areq->iv, chan->backup_iv, ivsize);\n\t\t\tmemzero_explicit(chan->backup_iv, ivsize);\n\t\t} else {\n\t\t\tscatterwalk_map_and_copy(areq->iv, areq->dst, offset,\n\t\t\t\t\t\t ivsize, 0);\n\t\t}\n\t\tmemzero_explicit(chan->bounce_iv, ivsize);\n\t}\n\n\tdma_unmap_single(ce->dev, rctx->addr_key, op->keylen, DMA_TO_DEVICE);\n\ntheend:\n\treturn err;\n}\n\nstatic void sun8i_ce_cipher_run(struct crypto_engine *engine, void *areq)\n{\n\tstruct skcipher_request *breq = container_of(areq, struct skcipher_request, base);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(breq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_ce_dev *ce = op->ce;\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(breq);\n\tint flow, err;\n\n\tflow = rctx->flow;\n\terr = sun8i_ce_run_task(ce, flow, crypto_tfm_alg_name(breq->base.tfm));\n\tlocal_bh_disable();\n\tcrypto_finalize_skcipher_request(engine, breq, err);\n\tlocal_bh_enable();\n}\n\nstatic void sun8i_ce_cipher_unprepare(struct crypto_engine *engine,\n\t\t\t\t      void *async_req)\n{\n\tstruct skcipher_request *areq = container_of(async_req, struct skcipher_request, base);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_ce_dev *ce = op->ce;\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);\n\tstruct sun8i_ce_flow *chan;\n\tstruct ce_task *cet;\n\tunsigned int ivsize, offset;\n\tint nr_sgs = rctx->nr_sgs;\n\tint nr_sgd = rctx->nr_sgd;\n\tint flow;\n\n\tflow = rctx->flow;\n\tchan = &ce->chanlist[flow];\n\tcet = chan->tl;\n\tivsize = crypto_skcipher_ivsize(tfm);\n\n\tif (areq->src == areq->dst) {\n\t\tdma_unmap_sg(ce->dev, areq->src, nr_sgs, DMA_BIDIRECTIONAL);\n\t} else {\n\t\tif (nr_sgs > 0)\n\t\t\tdma_unmap_sg(ce->dev, areq->src, nr_sgs, DMA_TO_DEVICE);\n\t\tdma_unmap_sg(ce->dev, areq->dst, nr_sgd, DMA_FROM_DEVICE);\n\t}\n\n\tif (areq->iv && ivsize > 0) {\n\t\tif (cet->t_iv)\n\t\t\tdma_unmap_single(ce->dev, rctx->addr_iv, rctx->ivlen, DMA_TO_DEVICE);\n\t\toffset = areq->cryptlen - ivsize;\n\t\tif (rctx->op_dir & CE_DECRYPTION) {\n\t\t\tmemcpy(areq->iv, chan->backup_iv, ivsize);\n\t\t\tmemzero_explicit(chan->backup_iv, ivsize);\n\t\t} else {\n\t\t\tscatterwalk_map_and_copy(areq->iv, areq->dst, offset,\n\t\t\t\t\t\t ivsize, 0);\n\t\t}\n\t\tmemzero_explicit(chan->bounce_iv, ivsize);\n\t}\n\n\tdma_unmap_single(ce->dev, rctx->addr_key, op->keylen, DMA_TO_DEVICE);\n}\n\nint sun8i_ce_cipher_do_one(struct crypto_engine *engine, void *areq)\n{\n\tint err = sun8i_ce_cipher_prepare(engine, areq);\n\n\tif (err)\n\t\treturn err;\n\n\tsun8i_ce_cipher_run(engine, areq);\n\tsun8i_ce_cipher_unprepare(engine, areq);\n\treturn 0;\n}\n\nint sun8i_ce_skdecrypt(struct skcipher_request *areq)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);\n\tstruct crypto_engine *engine;\n\tint e;\n\n\trctx->op_dir = CE_DECRYPTION;\n\tif (sun8i_ce_cipher_need_fallback(areq))\n\t\treturn sun8i_ce_cipher_fallback(areq);\n\n\te = sun8i_ce_get_engine_number(op->ce);\n\trctx->flow = e;\n\tengine = op->ce->chanlist[e].engine;\n\n\treturn crypto_transfer_skcipher_request_to_engine(engine, areq);\n}\n\nint sun8i_ce_skencrypt(struct skcipher_request *areq)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_cipher_req_ctx *rctx = skcipher_request_ctx(areq);\n\tstruct crypto_engine *engine;\n\tint e;\n\n\trctx->op_dir = CE_ENCRYPTION;\n\tif (sun8i_ce_cipher_need_fallback(areq))\n\t\treturn sun8i_ce_cipher_fallback(areq);\n\n\te = sun8i_ce_get_engine_number(op->ce);\n\trctx->flow = e;\n\tengine = op->ce->chanlist[e].engine;\n\n\treturn crypto_transfer_skcipher_request_to_engine(engine, areq);\n}\n\nint sun8i_ce_cipher_init(struct crypto_tfm *tfm)\n{\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_tfm_ctx(tfm);\n\tstruct sun8i_ce_alg_template *algt;\n\tconst char *name = crypto_tfm_alg_name(tfm);\n\tstruct crypto_skcipher *sktfm = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(sktfm);\n\tint err;\n\n\tmemset(op, 0, sizeof(struct sun8i_cipher_tfm_ctx));\n\n\talgt = container_of(alg, struct sun8i_ce_alg_template, alg.skcipher.base);\n\top->ce = algt->ce;\n\n\top->fallback_tfm = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(op->fallback_tfm)) {\n\t\tdev_err(op->ce->dev, \"ERROR: Cannot allocate fallback for %s %ld\\n\",\n\t\t\tname, PTR_ERR(op->fallback_tfm));\n\t\treturn PTR_ERR(op->fallback_tfm);\n\t}\n\n\tsktfm->reqsize = sizeof(struct sun8i_cipher_req_ctx) +\n\t\t\t crypto_skcipher_reqsize(op->fallback_tfm);\n\n\tmemcpy(algt->fbname,\n\t       crypto_tfm_alg_driver_name(crypto_skcipher_tfm(op->fallback_tfm)),\n\t       CRYPTO_MAX_ALG_NAME);\n\n\terr = pm_runtime_get_sync(op->ce->dev);\n\tif (err < 0)\n\t\tgoto error_pm;\n\n\treturn 0;\nerror_pm:\n\tpm_runtime_put_noidle(op->ce->dev);\n\tcrypto_free_skcipher(op->fallback_tfm);\n\treturn err;\n}\n\nvoid sun8i_ce_cipher_exit(struct crypto_tfm *tfm)\n{\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_tfm_ctx(tfm);\n\n\tkfree_sensitive(op->key);\n\tcrypto_free_skcipher(op->fallback_tfm);\n\tpm_runtime_put_sync_suspend(op->ce->dev);\n}\n\nint sun8i_ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\tunsigned int keylen)\n{\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tstruct sun8i_ce_dev *ce = op->ce;\n\n\tswitch (keylen) {\n\tcase 128 / 8:\n\t\tbreak;\n\tcase 192 / 8:\n\t\tbreak;\n\tcase 256 / 8:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(ce->dev, \"ERROR: Invalid keylen %u\\n\", keylen);\n\t\treturn -EINVAL;\n\t}\n\tkfree_sensitive(op->key);\n\top->keylen = keylen;\n\top->key = kmemdup(key, keylen, GFP_KERNEL | GFP_DMA);\n\tif (!op->key)\n\t\treturn -ENOMEM;\n\n\tcrypto_skcipher_clear_flags(op->fallback_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(op->fallback_tfm, tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\n\n\treturn crypto_skcipher_setkey(op->fallback_tfm, key, keylen);\n}\n\nint sun8i_ce_des3_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t unsigned int keylen)\n{\n\tstruct sun8i_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);\n\tint err;\n\n\terr = verify_skcipher_des3_key(tfm, key);\n\tif (err)\n\t\treturn err;\n\n\tkfree_sensitive(op->key);\n\top->keylen = keylen;\n\top->key = kmemdup(key, keylen, GFP_KERNEL | GFP_DMA);\n\tif (!op->key)\n\t\treturn -ENOMEM;\n\n\tcrypto_skcipher_clear_flags(op->fallback_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(op->fallback_tfm, tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\n\n\treturn crypto_skcipher_setkey(op->fallback_tfm, key, keylen);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}