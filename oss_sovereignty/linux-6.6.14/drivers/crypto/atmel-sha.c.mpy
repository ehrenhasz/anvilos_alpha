{
  "module_name": "atmel-sha.c",
  "hash_id": "4a1678acf1952f779d7a5cae7fe1dd47939ccf52792c74969b5cd3ccc9a24fa8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/atmel-sha.c",
  "human_readable_source": "\n \n\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/clk.h>\n#include <linux/io.h>\n#include <linux/hw_random.h>\n#include <linux/platform_device.h>\n\n#include <linux/device.h>\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-mapping.h>\n#include <linux/mod_devicetable.h>\n#include <linux/delay.h>\n#include <linux/crypto.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/algapi.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/hash.h>\n#include <crypto/internal/hash.h>\n#include \"atmel-sha-regs.h\"\n#include \"atmel-authenc.h\"\n\n#define ATMEL_SHA_PRIORITY\t300\n\n \n#define SHA_FLAGS_BUSY\t\t\tBIT(0)\n#define\tSHA_FLAGS_FINAL\t\t\tBIT(1)\n#define SHA_FLAGS_DMA_ACTIVE\tBIT(2)\n#define SHA_FLAGS_OUTPUT_READY\tBIT(3)\n#define SHA_FLAGS_INIT\t\t\tBIT(4)\n#define SHA_FLAGS_CPU\t\t\tBIT(5)\n#define SHA_FLAGS_DMA_READY\t\tBIT(6)\n#define SHA_FLAGS_DUMP_REG\tBIT(7)\n\n \n\n#define SHA_FLAGS_FINUP\t\tBIT(16)\n#define SHA_FLAGS_SG\t\tBIT(17)\n#define SHA_FLAGS_ERROR\t\tBIT(23)\n#define SHA_FLAGS_PAD\t\tBIT(24)\n#define SHA_FLAGS_RESTORE\tBIT(25)\n#define SHA_FLAGS_IDATAR0\tBIT(26)\n#define SHA_FLAGS_WAIT_DATARDY\tBIT(27)\n\n#define SHA_OP_INIT\t0\n#define SHA_OP_UPDATE\t1\n#define SHA_OP_FINAL\t2\n#define SHA_OP_DIGEST\t3\n\n#define SHA_BUFFER_LEN\t\t(PAGE_SIZE / 16)\n\n#define ATMEL_SHA_DMA_THRESHOLD\t\t56\n\nstruct atmel_sha_caps {\n\tbool\thas_dma;\n\tbool\thas_dualbuff;\n\tbool\thas_sha224;\n\tbool\thas_sha_384_512;\n\tbool\thas_uihv;\n\tbool\thas_hmac;\n};\n\nstruct atmel_sha_dev;\n\n \nstruct atmel_sha_reqctx {\n\tstruct atmel_sha_dev\t*dd;\n\tunsigned long\tflags;\n\tunsigned long\top;\n\n\tu8\tdigest[SHA512_DIGEST_SIZE] __aligned(sizeof(u32));\n\tu64\tdigcnt[2];\n\tsize_t\tbufcnt;\n\tsize_t\tbuflen;\n\tdma_addr_t\tdma_addr;\n\n\t \n\tstruct scatterlist\t*sg;\n\tunsigned int\toffset;\t \n\tunsigned int\ttotal;\t \n\n\tsize_t block_size;\n\tsize_t hash_size;\n\n\tu8 buffer[SHA_BUFFER_LEN + SHA512_BLOCK_SIZE] __aligned(sizeof(u32));\n};\n\ntypedef int (*atmel_sha_fn_t)(struct atmel_sha_dev *);\n\nstruct atmel_sha_ctx {\n\tstruct atmel_sha_dev\t*dd;\n\tatmel_sha_fn_t\t\tstart;\n\n\tunsigned long\t\tflags;\n};\n\n#define ATMEL_SHA_QUEUE_LENGTH\t50\n\nstruct atmel_sha_dma {\n\tstruct dma_chan\t\t\t*chan;\n\tstruct dma_slave_config dma_conf;\n\tstruct scatterlist\t*sg;\n\tint\t\t\tnents;\n\tunsigned int\t\tlast_sg_length;\n};\n\nstruct atmel_sha_dev {\n\tstruct list_head\tlist;\n\tunsigned long\t\tphys_base;\n\tstruct device\t\t*dev;\n\tstruct clk\t\t\t*iclk;\n\tint\t\t\t\t\tirq;\n\tvoid __iomem\t\t*io_base;\n\n\tspinlock_t\t\tlock;\n\tstruct tasklet_struct\tdone_task;\n\tstruct tasklet_struct\tqueue_task;\n\n\tunsigned long\t\tflags;\n\tstruct crypto_queue\tqueue;\n\tstruct ahash_request\t*req;\n\tbool\t\t\tis_async;\n\tbool\t\t\tforce_complete;\n\tatmel_sha_fn_t\t\tresume;\n\tatmel_sha_fn_t\t\tcpu_transfer_complete;\n\n\tstruct atmel_sha_dma\tdma_lch_in;\n\n\tstruct atmel_sha_caps\tcaps;\n\n\tstruct scatterlist\ttmp;\n\n\tu32\thw_version;\n};\n\nstruct atmel_sha_drv {\n\tstruct list_head\tdev_list;\n\tspinlock_t\t\tlock;\n};\n\nstatic struct atmel_sha_drv atmel_sha = {\n\t.dev_list = LIST_HEAD_INIT(atmel_sha.dev_list),\n\t.lock = __SPIN_LOCK_UNLOCKED(atmel_sha.lock),\n};\n\n#ifdef VERBOSE_DEBUG\nstatic const char *atmel_sha_reg_name(u32 offset, char *tmp, size_t sz, bool wr)\n{\n\tswitch (offset) {\n\tcase SHA_CR:\n\t\treturn \"CR\";\n\n\tcase SHA_MR:\n\t\treturn \"MR\";\n\n\tcase SHA_IER:\n\t\treturn \"IER\";\n\n\tcase SHA_IDR:\n\t\treturn \"IDR\";\n\n\tcase SHA_IMR:\n\t\treturn \"IMR\";\n\n\tcase SHA_ISR:\n\t\treturn \"ISR\";\n\n\tcase SHA_MSR:\n\t\treturn \"MSR\";\n\n\tcase SHA_BCR:\n\t\treturn \"BCR\";\n\n\tcase SHA_REG_DIN(0):\n\tcase SHA_REG_DIN(1):\n\tcase SHA_REG_DIN(2):\n\tcase SHA_REG_DIN(3):\n\tcase SHA_REG_DIN(4):\n\tcase SHA_REG_DIN(5):\n\tcase SHA_REG_DIN(6):\n\tcase SHA_REG_DIN(7):\n\tcase SHA_REG_DIN(8):\n\tcase SHA_REG_DIN(9):\n\tcase SHA_REG_DIN(10):\n\tcase SHA_REG_DIN(11):\n\tcase SHA_REG_DIN(12):\n\tcase SHA_REG_DIN(13):\n\tcase SHA_REG_DIN(14):\n\tcase SHA_REG_DIN(15):\n\t\tsnprintf(tmp, sz, \"IDATAR[%u]\", (offset - SHA_REG_DIN(0)) >> 2);\n\t\tbreak;\n\n\tcase SHA_REG_DIGEST(0):\n\tcase SHA_REG_DIGEST(1):\n\tcase SHA_REG_DIGEST(2):\n\tcase SHA_REG_DIGEST(3):\n\tcase SHA_REG_DIGEST(4):\n\tcase SHA_REG_DIGEST(5):\n\tcase SHA_REG_DIGEST(6):\n\tcase SHA_REG_DIGEST(7):\n\tcase SHA_REG_DIGEST(8):\n\tcase SHA_REG_DIGEST(9):\n\tcase SHA_REG_DIGEST(10):\n\tcase SHA_REG_DIGEST(11):\n\tcase SHA_REG_DIGEST(12):\n\tcase SHA_REG_DIGEST(13):\n\tcase SHA_REG_DIGEST(14):\n\tcase SHA_REG_DIGEST(15):\n\t\tif (wr)\n\t\t\tsnprintf(tmp, sz, \"IDATAR[%u]\",\n\t\t\t\t 16u + ((offset - SHA_REG_DIGEST(0)) >> 2));\n\t\telse\n\t\t\tsnprintf(tmp, sz, \"ODATAR[%u]\",\n\t\t\t\t (offset - SHA_REG_DIGEST(0)) >> 2);\n\t\tbreak;\n\n\tcase SHA_HW_VERSION:\n\t\treturn \"HWVER\";\n\n\tdefault:\n\t\tsnprintf(tmp, sz, \"0x%02x\", offset);\n\t\tbreak;\n\t}\n\n\treturn tmp;\n}\n\n#endif  \n\nstatic inline u32 atmel_sha_read(struct atmel_sha_dev *dd, u32 offset)\n{\n\tu32 value = readl_relaxed(dd->io_base + offset);\n\n#ifdef VERBOSE_DEBUG\n\tif (dd->flags & SHA_FLAGS_DUMP_REG) {\n\t\tchar tmp[16];\n\n\t\tdev_vdbg(dd->dev, \"read 0x%08x from %s\\n\", value,\n\t\t\t atmel_sha_reg_name(offset, tmp, sizeof(tmp), false));\n\t}\n#endif  \n\n\treturn value;\n}\n\nstatic inline void atmel_sha_write(struct atmel_sha_dev *dd,\n\t\t\t\t\tu32 offset, u32 value)\n{\n#ifdef VERBOSE_DEBUG\n\tif (dd->flags & SHA_FLAGS_DUMP_REG) {\n\t\tchar tmp[16];\n\n\t\tdev_vdbg(dd->dev, \"write 0x%08x into %s\\n\", value,\n\t\t\t atmel_sha_reg_name(offset, tmp, sizeof(tmp), true));\n\t}\n#endif  \n\n\twritel_relaxed(value, dd->io_base + offset);\n}\n\nstatic inline int atmel_sha_complete(struct atmel_sha_dev *dd, int err)\n{\n\tstruct ahash_request *req = dd->req;\n\n\tdd->flags &= ~(SHA_FLAGS_BUSY | SHA_FLAGS_FINAL | SHA_FLAGS_CPU |\n\t\t       SHA_FLAGS_DMA_READY | SHA_FLAGS_OUTPUT_READY |\n\t\t       SHA_FLAGS_DUMP_REG);\n\n\tclk_disable(dd->iclk);\n\n\tif ((dd->is_async || dd->force_complete) && req->base.complete)\n\t\tahash_request_complete(req, err);\n\n\t \n\ttasklet_schedule(&dd->queue_task);\n\n\treturn err;\n}\n\nstatic size_t atmel_sha_append_sg(struct atmel_sha_reqctx *ctx)\n{\n\tsize_t count;\n\n\twhile ((ctx->bufcnt < ctx->buflen) && ctx->total) {\n\t\tcount = min(ctx->sg->length - ctx->offset, ctx->total);\n\t\tcount = min(count, ctx->buflen - ctx->bufcnt);\n\n\t\tif (count <= 0) {\n\t\t\t \n\t\t\tif ((ctx->sg->length == 0) && !sg_is_last(ctx->sg)) {\n\t\t\t\tctx->sg = sg_next(ctx->sg);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, ctx->sg,\n\t\t\tctx->offset, count, 0);\n\n\t\tctx->bufcnt += count;\n\t\tctx->offset += count;\n\t\tctx->total -= count;\n\n\t\tif (ctx->offset == ctx->sg->length) {\n\t\t\tctx->sg = sg_next(ctx->sg);\n\t\t\tif (ctx->sg)\n\t\t\t\tctx->offset = 0;\n\t\t\telse\n\t\t\t\tctx->total = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void atmel_sha_fill_padding(struct atmel_sha_reqctx *ctx, int length)\n{\n\tunsigned int index, padlen;\n\t__be64 bits[2];\n\tu64 size[2];\n\n\tsize[0] = ctx->digcnt[0];\n\tsize[1] = ctx->digcnt[1];\n\n\tsize[0] += ctx->bufcnt;\n\tif (size[0] < ctx->bufcnt)\n\t\tsize[1]++;\n\n\tsize[0] += length;\n\tif (size[0]  < length)\n\t\tsize[1]++;\n\n\tbits[1] = cpu_to_be64(size[0] << 3);\n\tbits[0] = cpu_to_be64(size[1] << 3 | size[0] >> 61);\n\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tcase SHA_FLAGS_SHA384:\n\tcase SHA_FLAGS_SHA512:\n\t\tindex = ctx->bufcnt & 0x7f;\n\t\tpadlen = (index < 112) ? (112 - index) : ((128+112) - index);\n\t\t*(ctx->buffer + ctx->bufcnt) = 0x80;\n\t\tmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\n\t\tmemcpy(ctx->buffer + ctx->bufcnt + padlen, bits, 16);\n\t\tctx->bufcnt += padlen + 16;\n\t\tctx->flags |= SHA_FLAGS_PAD;\n\t\tbreak;\n\n\tdefault:\n\t\tindex = ctx->bufcnt & 0x3f;\n\t\tpadlen = (index < 56) ? (56 - index) : ((64+56) - index);\n\t\t*(ctx->buffer + ctx->bufcnt) = 0x80;\n\t\tmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\n\t\tmemcpy(ctx->buffer + ctx->bufcnt + padlen, &bits[1], 8);\n\t\tctx->bufcnt += padlen + 8;\n\t\tctx->flags |= SHA_FLAGS_PAD;\n\t\tbreak;\n\t}\n}\n\nstatic struct atmel_sha_dev *atmel_sha_find_dev(struct atmel_sha_ctx *tctx)\n{\n\tstruct atmel_sha_dev *dd = NULL;\n\tstruct atmel_sha_dev *tmp;\n\n\tspin_lock_bh(&atmel_sha.lock);\n\tif (!tctx->dd) {\n\t\tlist_for_each_entry(tmp, &atmel_sha.dev_list, list) {\n\t\t\tdd = tmp;\n\t\t\tbreak;\n\t\t}\n\t\ttctx->dd = dd;\n\t} else {\n\t\tdd = tctx->dd;\n\t}\n\n\tspin_unlock_bh(&atmel_sha.lock);\n\n\treturn dd;\n}\n\nstatic int atmel_sha_init(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_ctx *tctx = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct atmel_sha_dev *dd = atmel_sha_find_dev(tctx);\n\n\tctx->dd = dd;\n\n\tctx->flags = 0;\n\n\tdev_dbg(dd->dev, \"init: digest size: %u\\n\",\n\t\tcrypto_ahash_digestsize(tfm));\n\n\tswitch (crypto_ahash_digestsize(tfm)) {\n\tcase SHA1_DIGEST_SIZE:\n\t\tctx->flags |= SHA_FLAGS_SHA1;\n\t\tctx->block_size = SHA1_BLOCK_SIZE;\n\t\tbreak;\n\tcase SHA224_DIGEST_SIZE:\n\t\tctx->flags |= SHA_FLAGS_SHA224;\n\t\tctx->block_size = SHA224_BLOCK_SIZE;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tctx->flags |= SHA_FLAGS_SHA256;\n\t\tctx->block_size = SHA256_BLOCK_SIZE;\n\t\tbreak;\n\tcase SHA384_DIGEST_SIZE:\n\t\tctx->flags |= SHA_FLAGS_SHA384;\n\t\tctx->block_size = SHA384_BLOCK_SIZE;\n\t\tbreak;\n\tcase SHA512_DIGEST_SIZE:\n\t\tctx->flags |= SHA_FLAGS_SHA512;\n\t\tctx->block_size = SHA512_BLOCK_SIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tctx->bufcnt = 0;\n\tctx->digcnt[0] = 0;\n\tctx->digcnt[1] = 0;\n\tctx->buflen = SHA_BUFFER_LEN;\n\n\treturn 0;\n}\n\nstatic void atmel_sha_write_ctrl(struct atmel_sha_dev *dd, int dma)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tu32 valmr = SHA_MR_MODE_AUTO;\n\tunsigned int i, hashsize = 0;\n\n\tif (likely(dma)) {\n\t\tif (!dd->caps.has_dma)\n\t\t\tatmel_sha_write(dd, SHA_IER, SHA_INT_TXBUFE);\n\t\tvalmr = SHA_MR_MODE_PDC;\n\t\tif (dd->caps.has_dualbuff)\n\t\t\tvalmr |= SHA_MR_DUALBUFF;\n\t} else {\n\t\tatmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\n\t}\n\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tcase SHA_FLAGS_SHA1:\n\t\tvalmr |= SHA_MR_ALGO_SHA1;\n\t\thashsize = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA224:\n\t\tvalmr |= SHA_MR_ALGO_SHA224;\n\t\thashsize = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA256:\n\t\tvalmr |= SHA_MR_ALGO_SHA256;\n\t\thashsize = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA384:\n\t\tvalmr |= SHA_MR_ALGO_SHA384;\n\t\thashsize = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA512:\n\t\tvalmr |= SHA_MR_ALGO_SHA512;\n\t\thashsize = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (!(ctx->digcnt[0] || ctx->digcnt[1])) {\n\t\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\t} else if (dd->caps.has_uihv && (ctx->flags & SHA_FLAGS_RESTORE)) {\n\t\tconst u32 *hash = (const u32 *)ctx->digest;\n\n\t\t \n\t\tctx->flags &= ~SHA_FLAGS_RESTORE;\n\t\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\n\t\tfor (i = 0; i < hashsize / sizeof(u32); ++i)\n\t\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hash[i]);\n\t\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\t\tvalmr |= SHA_MR_UIHV;\n\t}\n\t \n\n\tatmel_sha_write(dd, SHA_MR, valmr);\n}\n\nstatic inline int atmel_sha_wait_for_data_ready(struct atmel_sha_dev *dd,\n\t\t\t\t\t\tatmel_sha_fn_t resume)\n{\n\tu32 isr = atmel_sha_read(dd, SHA_ISR);\n\n\tif (unlikely(isr & SHA_INT_DATARDY))\n\t\treturn resume(dd);\n\n\tdd->resume = resume;\n\tatmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\n\treturn -EINPROGRESS;\n}\n\nstatic int atmel_sha_xmit_cpu(struct atmel_sha_dev *dd, const u8 *buf,\n\t\t\t      size_t length, int final)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tint count, len32;\n\tconst u32 *buffer = (const u32 *)buf;\n\n\tdev_dbg(dd->dev, \"xmit_cpu: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\\n\",\n\t\tctx->digcnt[1], ctx->digcnt[0], length, final);\n\n\tatmel_sha_write_ctrl(dd, 0);\n\n\t \n\tctx->digcnt[0] += length;\n\tif (ctx->digcnt[0] < length)\n\t\tctx->digcnt[1]++;\n\n\tif (final)\n\t\tdd->flags |= SHA_FLAGS_FINAL;  \n\n\tlen32 = DIV_ROUND_UP(length, sizeof(u32));\n\n\tdd->flags |= SHA_FLAGS_CPU;\n\n\tfor (count = 0; count < len32; count++)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(count), buffer[count]);\n\n\treturn -EINPROGRESS;\n}\n\nstatic int atmel_sha_xmit_pdc(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\n\t\tsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tint len32;\n\n\tdev_dbg(dd->dev, \"xmit_pdc: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\\n\",\n\t\tctx->digcnt[1], ctx->digcnt[0], length1, final);\n\n\tlen32 = DIV_ROUND_UP(length1, sizeof(u32));\n\tatmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTDIS);\n\tatmel_sha_write(dd, SHA_TPR, dma_addr1);\n\tatmel_sha_write(dd, SHA_TCR, len32);\n\n\tlen32 = DIV_ROUND_UP(length2, sizeof(u32));\n\tatmel_sha_write(dd, SHA_TNPR, dma_addr2);\n\tatmel_sha_write(dd, SHA_TNCR, len32);\n\n\tatmel_sha_write_ctrl(dd, 1);\n\n\t \n\tctx->digcnt[0] += length1;\n\tif (ctx->digcnt[0] < length1)\n\t\tctx->digcnt[1]++;\n\n\tif (final)\n\t\tdd->flags |= SHA_FLAGS_FINAL;  \n\n\tdd->flags |=  SHA_FLAGS_DMA_ACTIVE;\n\n\t \n\tatmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTEN);\n\n\treturn -EINPROGRESS;\n}\n\nstatic void atmel_sha_dma_callback(void *data)\n{\n\tstruct atmel_sha_dev *dd = data;\n\n\tdd->is_async = true;\n\n\t \n\tatmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\n}\n\nstatic int atmel_sha_xmit_dma(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\n\t\tsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tstruct dma_async_tx_descriptor\t*in_desc;\n\tstruct scatterlist sg[2];\n\n\tdev_dbg(dd->dev, \"xmit_dma: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\\n\",\n\t\tctx->digcnt[1], ctx->digcnt[0], length1, final);\n\n\tdd->dma_lch_in.dma_conf.src_maxburst = 16;\n\tdd->dma_lch_in.dma_conf.dst_maxburst = 16;\n\n\tdmaengine_slave_config(dd->dma_lch_in.chan, &dd->dma_lch_in.dma_conf);\n\n\tif (length2) {\n\t\tsg_init_table(sg, 2);\n\t\tsg_dma_address(&sg[0]) = dma_addr1;\n\t\tsg_dma_len(&sg[0]) = length1;\n\t\tsg_dma_address(&sg[1]) = dma_addr2;\n\t\tsg_dma_len(&sg[1]) = length2;\n\t\tin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 2,\n\t\t\tDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\t} else {\n\t\tsg_init_table(sg, 1);\n\t\tsg_dma_address(&sg[0]) = dma_addr1;\n\t\tsg_dma_len(&sg[0]) = length1;\n\t\tin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 1,\n\t\t\tDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\t}\n\tif (!in_desc)\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\n\tin_desc->callback = atmel_sha_dma_callback;\n\tin_desc->callback_param = dd;\n\n\tatmel_sha_write_ctrl(dd, 1);\n\n\t \n\tctx->digcnt[0] += length1;\n\tif (ctx->digcnt[0] < length1)\n\t\tctx->digcnt[1]++;\n\n\tif (final)\n\t\tdd->flags |= SHA_FLAGS_FINAL;  \n\n\tdd->flags |=  SHA_FLAGS_DMA_ACTIVE;\n\n\t \n\tdmaengine_submit(in_desc);\n\tdma_async_issue_pending(dd->dma_lch_in.chan);\n\n\treturn -EINPROGRESS;\n}\n\nstatic int atmel_sha_xmit_start(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\n\t\tsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\n{\n\tif (dd->caps.has_dma)\n\t\treturn atmel_sha_xmit_dma(dd, dma_addr1, length1,\n\t\t\t\tdma_addr2, length2, final);\n\telse\n\t\treturn atmel_sha_xmit_pdc(dd, dma_addr1, length1,\n\t\t\t\tdma_addr2, length2, final);\n}\n\nstatic int atmel_sha_update_cpu(struct atmel_sha_dev *dd)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tint bufcnt;\n\n\tatmel_sha_append_sg(ctx);\n\tatmel_sha_fill_padding(ctx, 0);\n\tbufcnt = ctx->bufcnt;\n\tctx->bufcnt = 0;\n\n\treturn atmel_sha_xmit_cpu(dd, ctx->buffer, bufcnt, 1);\n}\n\nstatic int atmel_sha_xmit_dma_map(struct atmel_sha_dev *dd,\n\t\t\t\t\tstruct atmel_sha_reqctx *ctx,\n\t\t\t\t\tsize_t length, int final)\n{\n\tctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\n\t\t\t\tctx->buflen + ctx->block_size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\n\t\tdev_err(dd->dev, \"dma %zu bytes error\\n\", ctx->buflen +\n\t\t\t\tctx->block_size);\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t}\n\n\tctx->flags &= ~SHA_FLAGS_SG;\n\n\t \n\treturn atmel_sha_xmit_start(dd, ctx->dma_addr, length, 0, 0, final);\n}\n\nstatic int atmel_sha_update_dma_slow(struct atmel_sha_dev *dd)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tunsigned int final;\n\tsize_t count;\n\n\tatmel_sha_append_sg(ctx);\n\n\tfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\n\n\tdev_dbg(dd->dev, \"slow: bufcnt: %zu, digcnt: 0x%llx 0x%llx, final: %d\\n\",\n\t\t ctx->bufcnt, ctx->digcnt[1], ctx->digcnt[0], final);\n\n\tif (final)\n\t\tatmel_sha_fill_padding(ctx, 0);\n\n\tif (final || (ctx->bufcnt == ctx->buflen)) {\n\t\tcount = ctx->bufcnt;\n\t\tctx->bufcnt = 0;\n\t\treturn atmel_sha_xmit_dma_map(dd, ctx, count, final);\n\t}\n\n\treturn 0;\n}\n\nstatic int atmel_sha_update_dma_start(struct atmel_sha_dev *dd)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\tunsigned int length, final, tail;\n\tstruct scatterlist *sg;\n\tunsigned int count;\n\n\tif (!ctx->total)\n\t\treturn 0;\n\n\tif (ctx->bufcnt || ctx->offset)\n\t\treturn atmel_sha_update_dma_slow(dd);\n\n\tdev_dbg(dd->dev, \"fast: digcnt: 0x%llx 0x%llx, bufcnt: %zd, total: %u\\n\",\n\t\tctx->digcnt[1], ctx->digcnt[0], ctx->bufcnt, ctx->total);\n\n\tsg = ctx->sg;\n\n\tif (!IS_ALIGNED(sg->offset, sizeof(u32)))\n\t\treturn atmel_sha_update_dma_slow(dd);\n\n\tif (!sg_is_last(sg) && !IS_ALIGNED(sg->length, ctx->block_size))\n\t\t \n\t\treturn atmel_sha_update_dma_slow(dd);\n\n\tlength = min(ctx->total, sg->length);\n\n\tif (sg_is_last(sg)) {\n\t\tif (!(ctx->flags & SHA_FLAGS_FINUP)) {\n\t\t\t \n\t\t\ttail = length & (ctx->block_size - 1);\n\t\t\tlength -= tail;\n\t\t}\n\t}\n\n\tctx->total -= length;\n\tctx->offset = length;  \n\n\tfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\n\n\t \n\tif (final) {\n\t\ttail = length & (ctx->block_size - 1);\n\t\tlength -= tail;\n\t\tctx->total += tail;\n\t\tctx->offset = length;  \n\n\t\tsg = ctx->sg;\n\t\tatmel_sha_append_sg(ctx);\n\n\t\tatmel_sha_fill_padding(ctx, length);\n\n\t\tctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\n\t\t\tctx->buflen + ctx->block_size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\n\t\t\tdev_err(dd->dev, \"dma %zu bytes error\\n\",\n\t\t\t\tctx->buflen + ctx->block_size);\n\t\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t\t}\n\n\t\tif (length == 0) {\n\t\t\tctx->flags &= ~SHA_FLAGS_SG;\n\t\t\tcount = ctx->bufcnt;\n\t\t\tctx->bufcnt = 0;\n\t\t\treturn atmel_sha_xmit_start(dd, ctx->dma_addr, count, 0,\n\t\t\t\t\t0, final);\n\t\t} else {\n\t\t\tctx->sg = sg;\n\t\t\tif (!dma_map_sg(dd->dev, ctx->sg, 1,\n\t\t\t\tDMA_TO_DEVICE)) {\n\t\t\t\t\tdev_err(dd->dev, \"dma_map_sg  error\\n\");\n\t\t\t\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t\t\t}\n\n\t\t\tctx->flags |= SHA_FLAGS_SG;\n\n\t\t\tcount = ctx->bufcnt;\n\t\t\tctx->bufcnt = 0;\n\t\t\treturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg),\n\t\t\t\t\tlength, ctx->dma_addr, count, final);\n\t\t}\n\t}\n\n\tif (!dma_map_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\n\t\tdev_err(dd->dev, \"dma_map_sg  error\\n\");\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t}\n\n\tctx->flags |= SHA_FLAGS_SG;\n\n\t \n\treturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg), length, 0,\n\t\t\t\t\t\t\t\t0, final);\n}\n\nstatic void atmel_sha_update_dma_stop(struct atmel_sha_dev *dd)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\n\n\tif (ctx->flags & SHA_FLAGS_SG) {\n\t\tdma_unmap_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE);\n\t\tif (ctx->sg->length == ctx->offset) {\n\t\t\tctx->sg = sg_next(ctx->sg);\n\t\t\tif (ctx->sg)\n\t\t\t\tctx->offset = 0;\n\t\t}\n\t\tif (ctx->flags & SHA_FLAGS_PAD) {\n\t\t\tdma_unmap_single(dd->dev, ctx->dma_addr,\n\t\t\t\tctx->buflen + ctx->block_size, DMA_TO_DEVICE);\n\t\t}\n\t} else {\n\t\tdma_unmap_single(dd->dev, ctx->dma_addr, ctx->buflen +\n\t\t\t\t\t\tctx->block_size, DMA_TO_DEVICE);\n\t}\n}\n\nstatic int atmel_sha_update_req(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tint err;\n\n\tdev_dbg(dd->dev, \"update_req: total: %u, digcnt: 0x%llx 0x%llx\\n\",\n\t\tctx->total, ctx->digcnt[1], ctx->digcnt[0]);\n\n\tif (ctx->flags & SHA_FLAGS_CPU)\n\t\terr = atmel_sha_update_cpu(dd);\n\telse\n\t\terr = atmel_sha_update_dma_start(dd);\n\n\t \n\tdev_dbg(dd->dev, \"update: err: %d, digcnt: 0x%llx 0%llx\\n\",\n\t\t\terr, ctx->digcnt[1], ctx->digcnt[0]);\n\n\treturn err;\n}\n\nstatic int atmel_sha_final_req(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tint err = 0;\n\tint count;\n\n\tif (ctx->bufcnt >= ATMEL_SHA_DMA_THRESHOLD) {\n\t\tatmel_sha_fill_padding(ctx, 0);\n\t\tcount = ctx->bufcnt;\n\t\tctx->bufcnt = 0;\n\t\terr = atmel_sha_xmit_dma_map(dd, ctx, count, 1);\n\t}\n\t \n\telse {\n\t\tatmel_sha_fill_padding(ctx, 0);\n\t\tcount = ctx->bufcnt;\n\t\tctx->bufcnt = 0;\n\t\terr = atmel_sha_xmit_cpu(dd, ctx->buffer, count, 1);\n\t}\n\n\tdev_dbg(dd->dev, \"final_req: err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic void atmel_sha_copy_hash(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tu32 *hash = (u32 *)ctx->digest;\n\tunsigned int i, hashsize;\n\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tcase SHA_FLAGS_SHA1:\n\t\thashsize = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA224:\n\tcase SHA_FLAGS_SHA256:\n\t\thashsize = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA384:\n\tcase SHA_FLAGS_SHA512:\n\t\thashsize = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\treturn;\n\t}\n\n\tfor (i = 0; i < hashsize / sizeof(u32); ++i)\n\t\thash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\n\tctx->flags |= SHA_FLAGS_RESTORE;\n}\n\nstatic void atmel_sha_copy_ready_hash(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tif (!req->result)\n\t\treturn;\n\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tdefault:\n\tcase SHA_FLAGS_SHA1:\n\t\tmemcpy(req->result, ctx->digest, SHA1_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA224:\n\t\tmemcpy(req->result, ctx->digest, SHA224_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA256:\n\t\tmemcpy(req->result, ctx->digest, SHA256_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA384:\n\t\tmemcpy(req->result, ctx->digest, SHA384_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA512:\n\t\tmemcpy(req->result, ctx->digest, SHA512_DIGEST_SIZE);\n\t\tbreak;\n\t}\n}\n\nstatic int atmel_sha_finish(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct atmel_sha_dev *dd = ctx->dd;\n\n\tif (ctx->digcnt[0] || ctx->digcnt[1])\n\t\tatmel_sha_copy_ready_hash(req);\n\n\tdev_dbg(dd->dev, \"digcnt: 0x%llx 0x%llx, bufcnt: %zd\\n\", ctx->digcnt[1],\n\t\tctx->digcnt[0], ctx->bufcnt);\n\n\treturn 0;\n}\n\nstatic void atmel_sha_finish_req(struct ahash_request *req, int err)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct atmel_sha_dev *dd = ctx->dd;\n\n\tif (!err) {\n\t\tatmel_sha_copy_hash(req);\n\t\tif (SHA_FLAGS_FINAL & dd->flags)\n\t\t\terr = atmel_sha_finish(req);\n\t} else {\n\t\tctx->flags |= SHA_FLAGS_ERROR;\n\t}\n\n\t \n\t(void)atmel_sha_complete(dd, err);\n}\n\nstatic int atmel_sha_hw_init(struct atmel_sha_dev *dd)\n{\n\tint err;\n\n\terr = clk_enable(dd->iclk);\n\tif (err)\n\t\treturn err;\n\n\tif (!(SHA_FLAGS_INIT & dd->flags)) {\n\t\tatmel_sha_write(dd, SHA_CR, SHA_CR_SWRST);\n\t\tdd->flags |= SHA_FLAGS_INIT;\n\t}\n\n\treturn 0;\n}\n\nstatic inline unsigned int atmel_sha_get_version(struct atmel_sha_dev *dd)\n{\n\treturn atmel_sha_read(dd, SHA_HW_VERSION) & 0x00000fff;\n}\n\nstatic int atmel_sha_hw_version_init(struct atmel_sha_dev *dd)\n{\n\tint err;\n\n\terr = atmel_sha_hw_init(dd);\n\tif (err)\n\t\treturn err;\n\n\tdd->hw_version = atmel_sha_get_version(dd);\n\n\tdev_info(dd->dev,\n\t\t\t\"version: 0x%x\\n\", dd->hw_version);\n\n\tclk_disable(dd->iclk);\n\n\treturn 0;\n}\n\nstatic int atmel_sha_handle_queue(struct atmel_sha_dev *dd,\n\t\t\t\t  struct ahash_request *req)\n{\n\tstruct crypto_async_request *async_req, *backlog;\n\tstruct atmel_sha_ctx *ctx;\n\tunsigned long flags;\n\tbool start_async;\n\tint err = 0, ret = 0;\n\n\tspin_lock_irqsave(&dd->lock, flags);\n\tif (req)\n\t\tret = ahash_enqueue_request(&dd->queue, req);\n\n\tif (SHA_FLAGS_BUSY & dd->flags) {\n\t\tspin_unlock_irqrestore(&dd->lock, flags);\n\t\treturn ret;\n\t}\n\n\tbacklog = crypto_get_backlog(&dd->queue);\n\tasync_req = crypto_dequeue_request(&dd->queue);\n\tif (async_req)\n\t\tdd->flags |= SHA_FLAGS_BUSY;\n\n\tspin_unlock_irqrestore(&dd->lock, flags);\n\n\tif (!async_req)\n\t\treturn ret;\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\tctx = crypto_tfm_ctx(async_req->tfm);\n\n\tdd->req = ahash_request_cast(async_req);\n\tstart_async = (dd->req != req);\n\tdd->is_async = start_async;\n\tdd->force_complete = false;\n\n\t \n\terr = ctx->start(dd);\n\treturn (start_async) ? ret : err;\n}\n\nstatic int atmel_sha_done(struct atmel_sha_dev *dd);\n\nstatic int atmel_sha_start(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tint err;\n\n\tdev_dbg(dd->dev, \"handling new req, op: %lu, nbytes: %u\\n\",\n\t\t\t\t\t\tctx->op, req->nbytes);\n\n\terr = atmel_sha_hw_init(dd);\n\tif (err)\n\t\treturn atmel_sha_complete(dd, err);\n\n\t \n\n\tdd->resume = atmel_sha_done;\n\tif (ctx->op == SHA_OP_UPDATE) {\n\t\terr = atmel_sha_update_req(dd);\n\t\tif (!err && (ctx->flags & SHA_FLAGS_FINUP))\n\t\t\t \n\t\t\terr = atmel_sha_final_req(dd);\n\t} else if (ctx->op == SHA_OP_FINAL) {\n\t\terr = atmel_sha_final_req(dd);\n\t}\n\n\tif (!err)\n\t\t \n\t\tatmel_sha_finish_req(req, err);\n\n\tdev_dbg(dd->dev, \"exit, err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int atmel_sha_enqueue(struct ahash_request *req, unsigned int op)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\n\tstruct atmel_sha_dev *dd = tctx->dd;\n\n\tctx->op = op;\n\n\treturn atmel_sha_handle_queue(dd, req);\n}\n\nstatic int atmel_sha_update(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tif (!req->nbytes)\n\t\treturn 0;\n\n\tctx->total = req->nbytes;\n\tctx->sg = req->src;\n\tctx->offset = 0;\n\n\tif (ctx->flags & SHA_FLAGS_FINUP) {\n\t\tif (ctx->bufcnt + ctx->total < ATMEL_SHA_DMA_THRESHOLD)\n\t\t\t \n\t\t\tctx->flags |= SHA_FLAGS_CPU;\n\t} else if (ctx->bufcnt + ctx->total < ctx->buflen) {\n\t\tatmel_sha_append_sg(ctx);\n\t\treturn 0;\n\t}\n\treturn atmel_sha_enqueue(req, SHA_OP_UPDATE);\n}\n\nstatic int atmel_sha_final(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tctx->flags |= SHA_FLAGS_FINUP;\n\n\tif (ctx->flags & SHA_FLAGS_ERROR)\n\t\treturn 0;  \n\n\tif (ctx->flags & SHA_FLAGS_PAD)\n\t\t \n\t\treturn atmel_sha_finish(req);\n\n\treturn atmel_sha_enqueue(req, SHA_OP_FINAL);\n}\n\nstatic int atmel_sha_finup(struct ahash_request *req)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tint err1, err2;\n\n\tctx->flags |= SHA_FLAGS_FINUP;\n\n\terr1 = atmel_sha_update(req);\n\tif (err1 == -EINPROGRESS ||\n\t    (err1 == -EBUSY && (ahash_request_flags(req) &\n\t\t\t\tCRYPTO_TFM_REQ_MAY_BACKLOG)))\n\t\treturn err1;\n\n\t \n\terr2 = atmel_sha_final(req);\n\n\treturn err1 ?: err2;\n}\n\nstatic int atmel_sha_digest(struct ahash_request *req)\n{\n\treturn atmel_sha_init(req) ?: atmel_sha_finup(req);\n}\n\n\nstatic int atmel_sha_export(struct ahash_request *req, void *out)\n{\n\tconst struct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tmemcpy(out, ctx, sizeof(*ctx));\n\treturn 0;\n}\n\nstatic int atmel_sha_import(struct ahash_request *req, const void *in)\n{\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tmemcpy(ctx, in, sizeof(*ctx));\n\treturn 0;\n}\n\nstatic int atmel_sha_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct atmel_sha_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct atmel_sha_reqctx));\n\tctx->start = atmel_sha_start;\n\n\treturn 0;\n}\n\nstatic void atmel_sha_alg_init(struct ahash_alg *alg)\n{\n\talg->halg.base.cra_priority = ATMEL_SHA_PRIORITY;\n\talg->halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\talg->halg.base.cra_ctxsize = sizeof(struct atmel_sha_ctx);\n\talg->halg.base.cra_module = THIS_MODULE;\n\talg->halg.base.cra_init = atmel_sha_cra_init;\n\n\talg->halg.statesize = sizeof(struct atmel_sha_reqctx);\n\n\talg->init = atmel_sha_init;\n\talg->update = atmel_sha_update;\n\talg->final = atmel_sha_final;\n\talg->finup = atmel_sha_finup;\n\talg->digest = atmel_sha_digest;\n\talg->export = atmel_sha_export;\n\talg->import = atmel_sha_import;\n}\n\nstatic struct ahash_alg sha_1_256_algs[] = {\n{\n\t.halg.base.cra_name\t\t= \"sha1\",\n\t.halg.base.cra_driver_name\t= \"atmel-sha1\",\n\t.halg.base.cra_blocksize\t= SHA1_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA1_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"sha256\",\n\t.halg.base.cra_driver_name\t= \"atmel-sha256\",\n\t.halg.base.cra_blocksize\t= SHA256_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA256_DIGEST_SIZE,\n},\n};\n\nstatic struct ahash_alg sha_224_alg = {\n\t.halg.base.cra_name\t\t= \"sha224\",\n\t.halg.base.cra_driver_name\t= \"atmel-sha224\",\n\t.halg.base.cra_blocksize\t= SHA224_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA224_DIGEST_SIZE,\n};\n\nstatic struct ahash_alg sha_384_512_algs[] = {\n{\n\t.halg.base.cra_name\t\t= \"sha384\",\n\t.halg.base.cra_driver_name\t= \"atmel-sha384\",\n\t.halg.base.cra_blocksize\t= SHA384_BLOCK_SIZE,\n\t.halg.base.cra_alignmask\t= 0x3,\n\n\t.halg.digestsize = SHA384_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"sha512\",\n\t.halg.base.cra_driver_name\t= \"atmel-sha512\",\n\t.halg.base.cra_blocksize\t= SHA512_BLOCK_SIZE,\n\t.halg.base.cra_alignmask\t= 0x3,\n\n\t.halg.digestsize = SHA512_DIGEST_SIZE,\n},\n};\n\nstatic void atmel_sha_queue_task(unsigned long data)\n{\n\tstruct atmel_sha_dev *dd = (struct atmel_sha_dev *)data;\n\n\tatmel_sha_handle_queue(dd, NULL);\n}\n\nstatic int atmel_sha_done(struct atmel_sha_dev *dd)\n{\n\tint err = 0;\n\n\tif (SHA_FLAGS_CPU & dd->flags) {\n\t\tif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\n\t\t\tdd->flags &= ~SHA_FLAGS_OUTPUT_READY;\n\t\t\tgoto finish;\n\t\t}\n\t} else if (SHA_FLAGS_DMA_READY & dd->flags) {\n\t\tif (SHA_FLAGS_DMA_ACTIVE & dd->flags) {\n\t\t\tdd->flags &= ~SHA_FLAGS_DMA_ACTIVE;\n\t\t\tatmel_sha_update_dma_stop(dd);\n\t\t}\n\t\tif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\n\t\t\t \n\t\t\tdd->flags &= ~(SHA_FLAGS_DMA_READY |\n\t\t\t\t\t\tSHA_FLAGS_OUTPUT_READY);\n\t\t\terr = atmel_sha_update_dma_start(dd);\n\t\t\tif (err != -EINPROGRESS)\n\t\t\t\tgoto finish;\n\t\t}\n\t}\n\treturn err;\n\nfinish:\n\t \n\tatmel_sha_finish_req(dd->req, err);\n\n\treturn err;\n}\n\nstatic void atmel_sha_done_task(unsigned long data)\n{\n\tstruct atmel_sha_dev *dd = (struct atmel_sha_dev *)data;\n\n\tdd->is_async = true;\n\t(void)dd->resume(dd);\n}\n\nstatic irqreturn_t atmel_sha_irq(int irq, void *dev_id)\n{\n\tstruct atmel_sha_dev *sha_dd = dev_id;\n\tu32 reg;\n\n\treg = atmel_sha_read(sha_dd, SHA_ISR);\n\tif (reg & atmel_sha_read(sha_dd, SHA_IMR)) {\n\t\tatmel_sha_write(sha_dd, SHA_IDR, reg);\n\t\tif (SHA_FLAGS_BUSY & sha_dd->flags) {\n\t\t\tsha_dd->flags |= SHA_FLAGS_OUTPUT_READY;\n\t\t\tif (!(SHA_FLAGS_CPU & sha_dd->flags))\n\t\t\t\tsha_dd->flags |= SHA_FLAGS_DMA_READY;\n\t\t\ttasklet_schedule(&sha_dd->done_task);\n\t\t} else {\n\t\t\tdev_warn(sha_dd->dev, \"SHA interrupt when no active requests.\\n\");\n\t\t}\n\t\treturn IRQ_HANDLED;\n\t}\n\n\treturn IRQ_NONE;\n}\n\n\n \n\nstatic bool atmel_sha_dma_check_aligned(struct atmel_sha_dev *dd,\n\t\t\t\t\tstruct scatterlist *sg,\n\t\t\t\t\tsize_t len)\n{\n\tstruct atmel_sha_dma *dma = &dd->dma_lch_in;\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tsize_t bs = ctx->block_size;\n\tint nents;\n\n\tfor (nents = 0; sg; sg = sg_next(sg), ++nents) {\n\t\tif (!IS_ALIGNED(sg->offset, sizeof(u32)))\n\t\t\treturn false;\n\n\t\t \n\t\tif (len <= sg->length) {\n\t\t\tdma->nents = nents + 1;\n\t\t\tdma->last_sg_length = sg->length;\n\t\t\tsg->length = ALIGN(len, sizeof(u32));\n\t\t\treturn true;\n\t\t}\n\n\t\t \n\t\tif (!IS_ALIGNED(sg->length, bs))\n\t\t\treturn false;\n\n\t\tlen -= sg->length;\n\t}\n\n\treturn false;\n}\n\nstatic void atmel_sha_dma_callback2(void *data)\n{\n\tstruct atmel_sha_dev *dd = data;\n\tstruct atmel_sha_dma *dma = &dd->dma_lch_in;\n\tstruct scatterlist *sg;\n\tint nents;\n\n\tdma_unmap_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\n\n\tsg = dma->sg;\n\tfor (nents = 0; nents < dma->nents - 1; ++nents)\n\t\tsg = sg_next(sg);\n\tsg->length = dma->last_sg_length;\n\n\tdd->is_async = true;\n\t(void)atmel_sha_wait_for_data_ready(dd, dd->resume);\n}\n\nstatic int atmel_sha_dma_start(struct atmel_sha_dev *dd,\n\t\t\t       struct scatterlist *src,\n\t\t\t       size_t len,\n\t\t\t       atmel_sha_fn_t resume)\n{\n\tstruct atmel_sha_dma *dma = &dd->dma_lch_in;\n\tstruct dma_slave_config *config = &dma->dma_conf;\n\tstruct dma_chan *chan = dma->chan;\n\tstruct dma_async_tx_descriptor *desc;\n\tdma_cookie_t cookie;\n\tunsigned int sg_len;\n\tint err;\n\n\tdd->resume = resume;\n\n\t \n\tdma->sg = src;\n\tsg_len = dma_map_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\n\tif (!sg_len) {\n\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tconfig->src_maxburst = 16;\n\tconfig->dst_maxburst = 16;\n\terr = dmaengine_slave_config(chan, config);\n\tif (err)\n\t\tgoto unmap_sg;\n\n\tdesc = dmaengine_prep_slave_sg(chan, dma->sg, sg_len, DMA_MEM_TO_DEV,\n\t\t\t\t       DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\tif (!desc) {\n\t\terr = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tdesc->callback = atmel_sha_dma_callback2;\n\tdesc->callback_param = dd;\n\tcookie = dmaengine_submit(desc);\n\terr = dma_submit_error(cookie);\n\tif (err)\n\t\tgoto unmap_sg;\n\n\tdma_async_issue_pending(chan);\n\n\treturn -EINPROGRESS;\n\nunmap_sg:\n\tdma_unmap_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\nexit:\n\treturn atmel_sha_complete(dd, err);\n}\n\n\n \n\nstatic int atmel_sha_cpu_transfer(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tconst u32 *words = (const u32 *)ctx->buffer;\n\tsize_t i, num_words;\n\tu32 isr, din, din_inc;\n\n\tdin_inc = (ctx->flags & SHA_FLAGS_IDATAR0) ? 0 : 1;\n\tfor (;;) {\n\t\t \n\t\tnum_words = DIV_ROUND_UP(ctx->bufcnt, sizeof(u32));\n\t\tfor (i = 0, din = 0; i < num_words; ++i, din += din_inc)\n\t\t\tatmel_sha_write(dd, SHA_REG_DIN(din), words[i]);\n\n\t\tctx->offset += ctx->bufcnt;\n\t\tctx->total -= ctx->bufcnt;\n\n\t\tif (!ctx->total)\n\t\t\tbreak;\n\n\t\t \n\t\tctx->bufcnt = min_t(size_t, ctx->block_size, ctx->total);\n\t\tscatterwalk_map_and_copy(ctx->buffer, ctx->sg,\n\t\t\t\t\t ctx->offset, ctx->bufcnt, 0);\n\n\t\t \n\t\tisr = atmel_sha_read(dd, SHA_ISR);\n\t\tif (!(isr & SHA_INT_DATARDY)) {\n\t\t\t \n\t\t\tdd->resume = atmel_sha_cpu_transfer;\n\t\t\tatmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\n\t\t\treturn -EINPROGRESS;\n\t\t}\n\t}\n\n\tif (unlikely(!(ctx->flags & SHA_FLAGS_WAIT_DATARDY)))\n\t\treturn dd->cpu_transfer_complete(dd);\n\n\treturn atmel_sha_wait_for_data_ready(dd, dd->cpu_transfer_complete);\n}\n\nstatic int atmel_sha_cpu_start(struct atmel_sha_dev *dd,\n\t\t\t       struct scatterlist *sg,\n\t\t\t       unsigned int len,\n\t\t\t       bool idatar0_only,\n\t\t\t       bool wait_data_ready,\n\t\t\t       atmel_sha_fn_t resume)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\n\tif (!len)\n\t\treturn resume(dd);\n\n\tctx->flags &= ~(SHA_FLAGS_IDATAR0 | SHA_FLAGS_WAIT_DATARDY);\n\n\tif (idatar0_only)\n\t\tctx->flags |= SHA_FLAGS_IDATAR0;\n\n\tif (wait_data_ready)\n\t\tctx->flags |= SHA_FLAGS_WAIT_DATARDY;\n\n\tctx->sg = sg;\n\tctx->total = len;\n\tctx->offset = 0;\n\n\t \n\tctx->bufcnt = min_t(size_t, ctx->block_size, ctx->total);\n\tscatterwalk_map_and_copy(ctx->buffer, ctx->sg,\n\t\t\t\t ctx->offset, ctx->bufcnt, 0);\n\n\tdd->cpu_transfer_complete = resume;\n\treturn atmel_sha_cpu_transfer(dd);\n}\n\nstatic int atmel_sha_cpu_hash(struct atmel_sha_dev *dd,\n\t\t\t      const void *data, unsigned int datalen,\n\t\t\t      bool auto_padding,\n\t\t\t      atmel_sha_fn_t resume)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tu32 msglen = (auto_padding) ? datalen : 0;\n\tu32 mr = SHA_MR_MODE_AUTO;\n\n\tif (!(IS_ALIGNED(datalen, ctx->block_size) || auto_padding))\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\n\tmr |= (ctx->flags & SHA_FLAGS_ALGO_MASK);\n\tatmel_sha_write(dd, SHA_MR, mr);\n\tatmel_sha_write(dd, SHA_MSR, msglen);\n\tatmel_sha_write(dd, SHA_BCR, msglen);\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\n\tsg_init_one(&dd->tmp, data, datalen);\n\treturn atmel_sha_cpu_start(dd, &dd->tmp, datalen, false, true, resume);\n}\n\n\n \n\nstruct atmel_sha_hmac_key {\n\tbool\t\t\tvalid;\n\tunsigned int\t\tkeylen;\n\tu8\t\t\tbuffer[SHA512_BLOCK_SIZE];\n\tu8\t\t\t*keydup;\n};\n\nstatic inline void atmel_sha_hmac_key_init(struct atmel_sha_hmac_key *hkey)\n{\n\tmemset(hkey, 0, sizeof(*hkey));\n}\n\nstatic inline void atmel_sha_hmac_key_release(struct atmel_sha_hmac_key *hkey)\n{\n\tkfree(hkey->keydup);\n\tmemset(hkey, 0, sizeof(*hkey));\n}\n\nstatic inline int atmel_sha_hmac_key_set(struct atmel_sha_hmac_key *hkey,\n\t\t\t\t\t const u8 *key,\n\t\t\t\t\t unsigned int keylen)\n{\n\tatmel_sha_hmac_key_release(hkey);\n\n\tif (keylen > sizeof(hkey->buffer)) {\n\t\thkey->keydup = kmemdup(key, keylen, GFP_KERNEL);\n\t\tif (!hkey->keydup)\n\t\t\treturn -ENOMEM;\n\n\t} else {\n\t\tmemcpy(hkey->buffer, key, keylen);\n\t}\n\n\thkey->valid = true;\n\thkey->keylen = keylen;\n\treturn 0;\n}\n\nstatic inline bool atmel_sha_hmac_key_get(const struct atmel_sha_hmac_key *hkey,\n\t\t\t\t\t  const u8 **key,\n\t\t\t\t\t  unsigned int *keylen)\n{\n\tif (!hkey->valid)\n\t\treturn false;\n\n\t*keylen = hkey->keylen;\n\t*key = (hkey->keydup) ? hkey->keydup : hkey->buffer;\n\treturn true;\n}\n\n\nstruct atmel_sha_hmac_ctx {\n\tstruct atmel_sha_ctx\tbase;\n\n\tstruct atmel_sha_hmac_key\thkey;\n\tu32\t\t\tipad[SHA512_BLOCK_SIZE / sizeof(u32)];\n\tu32\t\t\topad[SHA512_BLOCK_SIZE / sizeof(u32)];\n\tatmel_sha_fn_t\t\tresume;\n};\n\nstatic int atmel_sha_hmac_setup(struct atmel_sha_dev *dd,\n\t\t\t\tatmel_sha_fn_t resume);\nstatic int atmel_sha_hmac_prehash_key(struct atmel_sha_dev *dd,\n\t\t\t\t      const u8 *key, unsigned int keylen);\nstatic int atmel_sha_hmac_prehash_key_done(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_compute_ipad_hash(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_compute_opad_hash(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_setup_done(struct atmel_sha_dev *dd);\n\nstatic int atmel_sha_hmac_init_done(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_final(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_final_done(struct atmel_sha_dev *dd);\nstatic int atmel_sha_hmac_digest2(struct atmel_sha_dev *dd);\n\nstatic int atmel_sha_hmac_setup(struct atmel_sha_dev *dd,\n\t\t\t\tatmel_sha_fn_t resume)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tunsigned int keylen;\n\tconst u8 *key;\n\tsize_t bs;\n\n\thmac->resume = resume;\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tcase SHA_FLAGS_SHA1:\n\t\tctx->block_size = SHA1_BLOCK_SIZE;\n\t\tctx->hash_size = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA224:\n\t\tctx->block_size = SHA224_BLOCK_SIZE;\n\t\tctx->hash_size = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA256:\n\t\tctx->block_size = SHA256_BLOCK_SIZE;\n\t\tctx->hash_size = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA384:\n\t\tctx->block_size = SHA384_BLOCK_SIZE;\n\t\tctx->hash_size = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA512:\n\t\tctx->block_size = SHA512_BLOCK_SIZE;\n\t\tctx->hash_size = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tdefault:\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t}\n\tbs = ctx->block_size;\n\n\tif (likely(!atmel_sha_hmac_key_get(&hmac->hkey, &key, &keylen)))\n\t\treturn resume(dd);\n\n\t \n\tif (unlikely(keylen > bs))\n\t\treturn atmel_sha_hmac_prehash_key(dd, key, keylen);\n\n\t \n\tmemcpy((u8 *)hmac->ipad, key, keylen);\n\tmemset((u8 *)hmac->ipad + keylen, 0, bs - keylen);\n\treturn atmel_sha_hmac_compute_ipad_hash(dd);\n}\n\nstatic int atmel_sha_hmac_prehash_key(struct atmel_sha_dev *dd,\n\t\t\t\t      const u8 *key, unsigned int keylen)\n{\n\treturn atmel_sha_cpu_hash(dd, key, keylen, true,\n\t\t\t\t  atmel_sha_hmac_prehash_key_done);\n}\n\nstatic int atmel_sha_hmac_prehash_key_done(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tsize_t ds = crypto_ahash_digestsize(tfm);\n\tsize_t bs = ctx->block_size;\n\tsize_t i, num_words = ds / sizeof(u32);\n\n\t \n\tfor (i = 0; i < num_words; ++i)\n\t\thmac->ipad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\n\tmemset((u8 *)hmac->ipad + ds, 0, bs - ds);\n\treturn atmel_sha_hmac_compute_ipad_hash(dd);\n}\n\nstatic int atmel_sha_hmac_compute_ipad_hash(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tsize_t bs = ctx->block_size;\n\tsize_t i, num_words = bs / sizeof(u32);\n\n\tunsafe_memcpy(hmac->opad, hmac->ipad, bs,\n\t\t      \"fortified memcpy causes -Wrestrict warning\");\n\tfor (i = 0; i < num_words; ++i) {\n\t\thmac->ipad[i] ^= 0x36363636;\n\t\thmac->opad[i] ^= 0x5c5c5c5c;\n\t}\n\n\treturn atmel_sha_cpu_hash(dd, hmac->ipad, bs, false,\n\t\t\t\t  atmel_sha_hmac_compute_opad_hash);\n}\n\nstatic int atmel_sha_hmac_compute_opad_hash(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tsize_t bs = ctx->block_size;\n\tsize_t hs = ctx->hash_size;\n\tsize_t i, num_words = hs / sizeof(u32);\n\n\tfor (i = 0; i < num_words; ++i)\n\t\thmac->ipad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\n\treturn atmel_sha_cpu_hash(dd, hmac->opad, bs, false,\n\t\t\t\t  atmel_sha_hmac_setup_done);\n}\n\nstatic int atmel_sha_hmac_setup_done(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tsize_t hs = ctx->hash_size;\n\tsize_t i, num_words = hs / sizeof(u32);\n\n\tfor (i = 0; i < num_words; ++i)\n\t\thmac->opad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\n\tatmel_sha_hmac_key_release(&hmac->hkey);\n\treturn hmac->resume(dd);\n}\n\nstatic int atmel_sha_hmac_start(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tint err;\n\n\terr = atmel_sha_hw_init(dd);\n\tif (err)\n\t\treturn atmel_sha_complete(dd, err);\n\n\tswitch (ctx->op) {\n\tcase SHA_OP_INIT:\n\t\terr = atmel_sha_hmac_setup(dd, atmel_sha_hmac_init_done);\n\t\tbreak;\n\n\tcase SHA_OP_UPDATE:\n\t\tdd->resume = atmel_sha_done;\n\t\terr = atmel_sha_update_req(dd);\n\t\tbreak;\n\n\tcase SHA_OP_FINAL:\n\t\tdd->resume = atmel_sha_hmac_final;\n\t\terr = atmel_sha_final_req(dd);\n\t\tbreak;\n\n\tcase SHA_OP_DIGEST:\n\t\terr = atmel_sha_hmac_setup(dd, atmel_sha_hmac_digest2);\n\t\tbreak;\n\n\tdefault:\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t}\n\n\treturn err;\n}\n\nstatic int atmel_sha_hmac_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\n\treturn atmel_sha_hmac_key_set(&hmac->hkey, key, keylen);\n}\n\nstatic int atmel_sha_hmac_init(struct ahash_request *req)\n{\n\tint err;\n\n\terr = atmel_sha_init(req);\n\tif (err)\n\t\treturn err;\n\n\treturn atmel_sha_enqueue(req, SHA_OP_INIT);\n}\n\nstatic int atmel_sha_hmac_init_done(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tsize_t bs = ctx->block_size;\n\tsize_t hs = ctx->hash_size;\n\n\tctx->bufcnt = 0;\n\tctx->digcnt[0] = bs;\n\tctx->digcnt[1] = 0;\n\tctx->flags |= SHA_FLAGS_RESTORE;\n\tmemcpy(ctx->digest, hmac->ipad, hs);\n\treturn atmel_sha_complete(dd, 0);\n}\n\nstatic int atmel_sha_hmac_final(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tu32 *digest = (u32 *)ctx->digest;\n\tsize_t ds = crypto_ahash_digestsize(tfm);\n\tsize_t bs = ctx->block_size;\n\tsize_t hs = ctx->hash_size;\n\tsize_t i, num_words;\n\tu32 mr;\n\n\t \n\tnum_words = ds / sizeof(u32);\n\tfor (i = 0; i < num_words; ++i)\n\t\tdigest[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\n\n\t \n\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\n\tnum_words = hs / sizeof(u32);\n\tfor (i = 0; i < num_words; ++i)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\n\n\tmr = SHA_MR_MODE_AUTO | SHA_MR_UIHV;\n\tmr |= (ctx->flags & SHA_FLAGS_ALGO_MASK);\n\tatmel_sha_write(dd, SHA_MR, mr);\n\tatmel_sha_write(dd, SHA_MSR, bs + ds);\n\tatmel_sha_write(dd, SHA_BCR, ds);\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\n\tsg_init_one(&dd->tmp, digest, ds);\n\treturn atmel_sha_cpu_start(dd, &dd->tmp, ds, false, true,\n\t\t\t\t   atmel_sha_hmac_final_done);\n}\n\nstatic int atmel_sha_hmac_final_done(struct atmel_sha_dev *dd)\n{\n\t \n\tatmel_sha_copy_hash(dd->req);\n\tatmel_sha_copy_ready_hash(dd->req);\n\treturn atmel_sha_complete(dd, 0);\n}\n\nstatic int atmel_sha_hmac_digest(struct ahash_request *req)\n{\n\tint err;\n\n\terr = atmel_sha_init(req);\n\tif (err)\n\t\treturn err;\n\n\treturn atmel_sha_enqueue(req, SHA_OP_DIGEST);\n}\n\nstatic int atmel_sha_hmac_digest2(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct scatterlist *sgbuf;\n\tsize_t hs = ctx->hash_size;\n\tsize_t i, num_words = hs / sizeof(u32);\n\tbool use_dma = false;\n\tu32 mr;\n\n\t \n\tif (!req->nbytes) {\n\t\treq->nbytes = 0;\n\t\tctx->bufcnt = 0;\n\t\tctx->digcnt[0] = 0;\n\t\tctx->digcnt[1] = 0;\n\t\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\t\tcase SHA_FLAGS_SHA1:\n\t\tcase SHA_FLAGS_SHA224:\n\t\tcase SHA_FLAGS_SHA256:\n\t\t\tatmel_sha_fill_padding(ctx, 64);\n\t\t\tbreak;\n\n\t\tcase SHA_FLAGS_SHA384:\n\t\tcase SHA_FLAGS_SHA512:\n\t\t\tatmel_sha_fill_padding(ctx, 128);\n\t\t\tbreak;\n\t\t}\n\t\tsg_init_one(&dd->tmp, ctx->buffer, ctx->bufcnt);\n\t}\n\n\t \n\tif (req->nbytes > ATMEL_SHA_DMA_THRESHOLD &&\n\t    atmel_sha_dma_check_aligned(dd, req->src, req->nbytes))\n\t\tuse_dma = true;\n\n\t \n\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\n\tfor (i = 0; i < num_words; ++i)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hmac->ipad[i]);\n\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIEHV);\n\tfor (i = 0; i < num_words; ++i)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\n\n\t \n\tmr = (SHA_MR_HMAC | SHA_MR_DUALBUFF);\n\tmr |= ctx->flags & SHA_FLAGS_ALGO_MASK;\n\tif (use_dma)\n\t\tmr |= SHA_MR_MODE_IDATAR0;\n\telse\n\t\tmr |= SHA_MR_MODE_AUTO;\n\tatmel_sha_write(dd, SHA_MR, mr);\n\n\tatmel_sha_write(dd, SHA_MSR, req->nbytes);\n\tatmel_sha_write(dd, SHA_BCR, req->nbytes);\n\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\n\t \n\tif (!req->nbytes) {\n\t\tsgbuf = &dd->tmp;\n\t\treq->nbytes = ctx->bufcnt;\n\t} else {\n\t\tsgbuf = req->src;\n\t}\n\n\t \n\tif (use_dma)\n\t\treturn atmel_sha_dma_start(dd, sgbuf, req->nbytes,\n\t\t\t\t\t   atmel_sha_hmac_final_done);\n\n\treturn atmel_sha_cpu_start(dd, sgbuf, req->nbytes, false, true,\n\t\t\t\t   atmel_sha_hmac_final_done);\n}\n\nstatic int atmel_sha_hmac_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_tfm_ctx(tfm);\n\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct atmel_sha_reqctx));\n\thmac->base.start = atmel_sha_hmac_start;\n\tatmel_sha_hmac_key_init(&hmac->hkey);\n\n\treturn 0;\n}\n\nstatic void atmel_sha_hmac_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_tfm_ctx(tfm);\n\n\tatmel_sha_hmac_key_release(&hmac->hkey);\n}\n\nstatic void atmel_sha_hmac_alg_init(struct ahash_alg *alg)\n{\n\talg->halg.base.cra_priority = ATMEL_SHA_PRIORITY;\n\talg->halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\talg->halg.base.cra_ctxsize = sizeof(struct atmel_sha_hmac_ctx);\n\talg->halg.base.cra_module = THIS_MODULE;\n\talg->halg.base.cra_init\t= atmel_sha_hmac_cra_init;\n\talg->halg.base.cra_exit\t= atmel_sha_hmac_cra_exit;\n\n\talg->halg.statesize = sizeof(struct atmel_sha_reqctx);\n\n\talg->init = atmel_sha_hmac_init;\n\talg->update = atmel_sha_update;\n\talg->final = atmel_sha_final;\n\talg->digest = atmel_sha_hmac_digest;\n\talg->setkey = atmel_sha_hmac_setkey;\n\talg->export = atmel_sha_export;\n\talg->import = atmel_sha_import;\n}\n\nstatic struct ahash_alg sha_hmac_algs[] = {\n{\n\t.halg.base.cra_name\t\t= \"hmac(sha1)\",\n\t.halg.base.cra_driver_name\t= \"atmel-hmac-sha1\",\n\t.halg.base.cra_blocksize\t= SHA1_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA1_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"hmac(sha224)\",\n\t.halg.base.cra_driver_name\t= \"atmel-hmac-sha224\",\n\t.halg.base.cra_blocksize\t= SHA224_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA224_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"hmac(sha256)\",\n\t.halg.base.cra_driver_name\t= \"atmel-hmac-sha256\",\n\t.halg.base.cra_blocksize\t= SHA256_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA256_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"hmac(sha384)\",\n\t.halg.base.cra_driver_name\t= \"atmel-hmac-sha384\",\n\t.halg.base.cra_blocksize\t= SHA384_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA384_DIGEST_SIZE,\n},\n{\n\t.halg.base.cra_name\t\t= \"hmac(sha512)\",\n\t.halg.base.cra_driver_name\t= \"atmel-hmac-sha512\",\n\t.halg.base.cra_blocksize\t= SHA512_BLOCK_SIZE,\n\n\t.halg.digestsize = SHA512_DIGEST_SIZE,\n},\n};\n\n#if IS_ENABLED(CONFIG_CRYPTO_DEV_ATMEL_AUTHENC)\n \n\nstatic int atmel_sha_authenc_init2(struct atmel_sha_dev *dd);\nstatic int atmel_sha_authenc_init_done(struct atmel_sha_dev *dd);\nstatic int atmel_sha_authenc_final_done(struct atmel_sha_dev *dd);\n\n\nstruct atmel_sha_authenc_ctx {\n\tstruct crypto_ahash\t*tfm;\n};\n\nstruct atmel_sha_authenc_reqctx {\n\tstruct atmel_sha_reqctx\tbase;\n\n\tatmel_aes_authenc_fn_t\tcb;\n\tstruct atmel_aes_dev\t*aes_dev;\n\n\t \n\tstruct scatterlist\t*assoc;\n\tu32\t\t\tassoclen;\n\tu32\t\t\ttextlen;\n\n\t \n\tu32\t\t\t*digest;\n\tunsigned int\t\tdigestlen;\n};\n\nstatic void atmel_sha_authenc_complete(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct atmel_sha_authenc_reqctx *authctx  = ahash_request_ctx(req);\n\n\tauthctx->cb(authctx->aes_dev, err, authctx->base.dd->is_async);\n}\n\nstatic int atmel_sha_authenc_start(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tint err;\n\n\t \n\tdd->force_complete = true;\n\n\terr = atmel_sha_hw_init(dd);\n\treturn authctx->cb(authctx->aes_dev, err, dd->is_async);\n}\n\nbool atmel_sha_authenc_is_ready(void)\n{\n\tstruct atmel_sha_ctx dummy;\n\n\tdummy.dd = NULL;\n\treturn (atmel_sha_find_dev(&dummy) != NULL);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_is_ready);\n\nunsigned int atmel_sha_authenc_get_reqsize(void)\n{\n\treturn sizeof(struct atmel_sha_authenc_reqctx);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_get_reqsize);\n\nstruct atmel_sha_authenc_ctx *atmel_sha_authenc_spawn(unsigned long mode)\n{\n\tstruct atmel_sha_authenc_ctx *auth;\n\tstruct crypto_ahash *tfm;\n\tstruct atmel_sha_ctx *tctx;\n\tconst char *name;\n\tint err = -EINVAL;\n\n\tswitch (mode & SHA_FLAGS_MODE_MASK) {\n\tcase SHA_FLAGS_HMAC_SHA1:\n\t\tname = \"atmel-hmac-sha1\";\n\t\tbreak;\n\n\tcase SHA_FLAGS_HMAC_SHA224:\n\t\tname = \"atmel-hmac-sha224\";\n\t\tbreak;\n\n\tcase SHA_FLAGS_HMAC_SHA256:\n\t\tname = \"atmel-hmac-sha256\";\n\t\tbreak;\n\n\tcase SHA_FLAGS_HMAC_SHA384:\n\t\tname = \"atmel-hmac-sha384\";\n\t\tbreak;\n\n\tcase SHA_FLAGS_HMAC_SHA512:\n\t\tname = \"atmel-hmac-sha512\";\n\t\tbreak;\n\n\tdefault:\n\t\tgoto error;\n\t}\n\n\ttfm = crypto_alloc_ahash(name, 0, 0);\n\tif (IS_ERR(tfm)) {\n\t\terr = PTR_ERR(tfm);\n\t\tgoto error;\n\t}\n\ttctx = crypto_ahash_ctx(tfm);\n\ttctx->start = atmel_sha_authenc_start;\n\ttctx->flags = mode;\n\n\tauth = kzalloc(sizeof(*auth), GFP_KERNEL);\n\tif (!auth) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_ahash;\n\t}\n\tauth->tfm = tfm;\n\n\treturn auth;\n\nerr_free_ahash:\n\tcrypto_free_ahash(tfm);\nerror:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_spawn);\n\nvoid atmel_sha_authenc_free(struct atmel_sha_authenc_ctx *auth)\n{\n\tif (auth)\n\t\tcrypto_free_ahash(auth->tfm);\n\tkfree(auth);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_free);\n\nint atmel_sha_authenc_setkey(struct atmel_sha_authenc_ctx *auth,\n\t\t\t     const u8 *key, unsigned int keylen, u32 flags)\n{\n\tstruct crypto_ahash *tfm = auth->tfm;\n\n\tcrypto_ahash_clear_flags(tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(tfm, flags & CRYPTO_TFM_REQ_MASK);\n\treturn crypto_ahash_setkey(tfm, key, keylen);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_setkey);\n\nint atmel_sha_authenc_schedule(struct ahash_request *req,\n\t\t\t       struct atmel_sha_authenc_ctx *auth,\n\t\t\t       atmel_aes_authenc_fn_t cb,\n\t\t\t       struct atmel_aes_dev *aes_dev)\n{\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tstruct atmel_sha_reqctx *ctx = &authctx->base;\n\tstruct crypto_ahash *tfm = auth->tfm;\n\tstruct atmel_sha_ctx *tctx = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_dev *dd;\n\n\t \n\tmemset(authctx, 0, sizeof(*authctx));\n\n\t \n\tdd = atmel_sha_find_dev(tctx);\n\tif (!dd)\n\t\treturn cb(aes_dev, -ENODEV, false);\n\n\t \n\tctx->dd = dd;\n\tctx->buflen = SHA_BUFFER_LEN;\n\tauthctx->cb = cb;\n\tauthctx->aes_dev = aes_dev;\n\tahash_request_set_tfm(req, tfm);\n\tahash_request_set_callback(req, 0, atmel_sha_authenc_complete, req);\n\n\treturn atmel_sha_handle_queue(dd, req);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_schedule);\n\nint atmel_sha_authenc_init(struct ahash_request *req,\n\t\t\t   struct scatterlist *assoc, unsigned int assoclen,\n\t\t\t   unsigned int textlen,\n\t\t\t   atmel_aes_authenc_fn_t cb,\n\t\t\t   struct atmel_aes_dev *aes_dev)\n{\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tstruct atmel_sha_reqctx *ctx = &authctx->base;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tstruct atmel_sha_dev *dd = ctx->dd;\n\n\tif (unlikely(!IS_ALIGNED(assoclen, sizeof(u32))))\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\n\tauthctx->cb = cb;\n\tauthctx->aes_dev = aes_dev;\n\tauthctx->assoc = assoc;\n\tauthctx->assoclen = assoclen;\n\tauthctx->textlen = textlen;\n\n\tctx->flags = hmac->base.flags;\n\treturn atmel_sha_hmac_setup(dd, atmel_sha_authenc_init2);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_init);\n\nstatic int atmel_sha_authenc_init2(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tstruct atmel_sha_reqctx *ctx = &authctx->base;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\n\tsize_t hs = ctx->hash_size;\n\tsize_t i, num_words = hs / sizeof(u32);\n\tu32 mr, msg_size;\n\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\n\tfor (i = 0; i < num_words; ++i)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hmac->ipad[i]);\n\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_WUIEHV);\n\tfor (i = 0; i < num_words; ++i)\n\t\tatmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\n\n\tmr = (SHA_MR_MODE_IDATAR0 |\n\t      SHA_MR_HMAC |\n\t      SHA_MR_DUALBUFF);\n\tmr |= ctx->flags & SHA_FLAGS_ALGO_MASK;\n\tatmel_sha_write(dd, SHA_MR, mr);\n\n\tmsg_size = authctx->assoclen + authctx->textlen;\n\tatmel_sha_write(dd, SHA_MSR, msg_size);\n\tatmel_sha_write(dd, SHA_BCR, msg_size);\n\n\tatmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\n\n\t \n\treturn atmel_sha_cpu_start(dd, authctx->assoc, authctx->assoclen,\n\t\t\t\t   true, false,\n\t\t\t\t   atmel_sha_authenc_init_done);\n}\n\nstatic int atmel_sha_authenc_init_done(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\n\treturn authctx->cb(authctx->aes_dev, 0, dd->is_async);\n}\n\nint atmel_sha_authenc_final(struct ahash_request *req,\n\t\t\t    u32 *digest, unsigned int digestlen,\n\t\t\t    atmel_aes_authenc_fn_t cb,\n\t\t\t    struct atmel_aes_dev *aes_dev)\n{\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tstruct atmel_sha_reqctx *ctx = &authctx->base;\n\tstruct atmel_sha_dev *dd = ctx->dd;\n\n\tswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\n\tcase SHA_FLAGS_SHA1:\n\t\tauthctx->digestlen = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA224:\n\t\tauthctx->digestlen = SHA224_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA256:\n\t\tauthctx->digestlen = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA384:\n\t\tauthctx->digestlen = SHA384_DIGEST_SIZE;\n\t\tbreak;\n\n\tcase SHA_FLAGS_SHA512:\n\t\tauthctx->digestlen = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\n\tdefault:\n\t\treturn atmel_sha_complete(dd, -EINVAL);\n\t}\n\tif (authctx->digestlen > digestlen)\n\t\tauthctx->digestlen = digestlen;\n\n\tauthctx->cb = cb;\n\tauthctx->aes_dev = aes_dev;\n\tauthctx->digest = digest;\n\treturn atmel_sha_wait_for_data_ready(dd,\n\t\t\t\t\t     atmel_sha_authenc_final_done);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_final);\n\nstatic int atmel_sha_authenc_final_done(struct atmel_sha_dev *dd)\n{\n\tstruct ahash_request *req = dd->req;\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tsize_t i, num_words = authctx->digestlen / sizeof(u32);\n\n\tfor (i = 0; i < num_words; ++i)\n\t\tauthctx->digest[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\n\n\treturn atmel_sha_complete(dd, 0);\n}\n\nvoid atmel_sha_authenc_abort(struct ahash_request *req)\n{\n\tstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\n\tstruct atmel_sha_reqctx *ctx = &authctx->base;\n\tstruct atmel_sha_dev *dd = ctx->dd;\n\n\t \n\tdd->is_async = false;\n\tdd->force_complete = false;\n\t(void)atmel_sha_complete(dd, 0);\n}\nEXPORT_SYMBOL_GPL(atmel_sha_authenc_abort);\n\n#endif  \n\n\nstatic void atmel_sha_unregister_algs(struct atmel_sha_dev *dd)\n{\n\tint i;\n\n\tif (dd->caps.has_hmac)\n\t\tfor (i = 0; i < ARRAY_SIZE(sha_hmac_algs); i++)\n\t\t\tcrypto_unregister_ahash(&sha_hmac_algs[i]);\n\n\tfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++)\n\t\tcrypto_unregister_ahash(&sha_1_256_algs[i]);\n\n\tif (dd->caps.has_sha224)\n\t\tcrypto_unregister_ahash(&sha_224_alg);\n\n\tif (dd->caps.has_sha_384_512) {\n\t\tfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++)\n\t\t\tcrypto_unregister_ahash(&sha_384_512_algs[i]);\n\t}\n}\n\nstatic int atmel_sha_register_algs(struct atmel_sha_dev *dd)\n{\n\tint err, i, j;\n\n\tfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++) {\n\t\tatmel_sha_alg_init(&sha_1_256_algs[i]);\n\n\t\terr = crypto_register_ahash(&sha_1_256_algs[i]);\n\t\tif (err)\n\t\t\tgoto err_sha_1_256_algs;\n\t}\n\n\tif (dd->caps.has_sha224) {\n\t\tatmel_sha_alg_init(&sha_224_alg);\n\n\t\terr = crypto_register_ahash(&sha_224_alg);\n\t\tif (err)\n\t\t\tgoto err_sha_224_algs;\n\t}\n\n\tif (dd->caps.has_sha_384_512) {\n\t\tfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++) {\n\t\t\tatmel_sha_alg_init(&sha_384_512_algs[i]);\n\n\t\t\terr = crypto_register_ahash(&sha_384_512_algs[i]);\n\t\t\tif (err)\n\t\t\t\tgoto err_sha_384_512_algs;\n\t\t}\n\t}\n\n\tif (dd->caps.has_hmac) {\n\t\tfor (i = 0; i < ARRAY_SIZE(sha_hmac_algs); i++) {\n\t\t\tatmel_sha_hmac_alg_init(&sha_hmac_algs[i]);\n\n\t\t\terr = crypto_register_ahash(&sha_hmac_algs[i]);\n\t\t\tif (err)\n\t\t\t\tgoto err_sha_hmac_algs;\n\t\t}\n\t}\n\n\treturn 0;\n\n\t \nerr_sha_hmac_algs:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_ahash(&sha_hmac_algs[j]);\n\ti = ARRAY_SIZE(sha_384_512_algs);\nerr_sha_384_512_algs:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_ahash(&sha_384_512_algs[j]);\n\tcrypto_unregister_ahash(&sha_224_alg);\nerr_sha_224_algs:\n\ti = ARRAY_SIZE(sha_1_256_algs);\nerr_sha_1_256_algs:\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_ahash(&sha_1_256_algs[j]);\n\n\treturn err;\n}\n\nstatic int atmel_sha_dma_init(struct atmel_sha_dev *dd)\n{\n\tdd->dma_lch_in.chan = dma_request_chan(dd->dev, \"tx\");\n\tif (IS_ERR(dd->dma_lch_in.chan)) {\n\t\treturn dev_err_probe(dd->dev, PTR_ERR(dd->dma_lch_in.chan),\n\t\t\t\"DMA channel is not available\\n\");\n\t}\n\n\tdd->dma_lch_in.dma_conf.dst_addr = dd->phys_base +\n\t\tSHA_REG_DIN(0);\n\tdd->dma_lch_in.dma_conf.src_maxburst = 1;\n\tdd->dma_lch_in.dma_conf.src_addr_width =\n\t\tDMA_SLAVE_BUSWIDTH_4_BYTES;\n\tdd->dma_lch_in.dma_conf.dst_maxburst = 1;\n\tdd->dma_lch_in.dma_conf.dst_addr_width =\n\t\tDMA_SLAVE_BUSWIDTH_4_BYTES;\n\tdd->dma_lch_in.dma_conf.device_fc = false;\n\n\treturn 0;\n}\n\nstatic void atmel_sha_dma_cleanup(struct atmel_sha_dev *dd)\n{\n\tdma_release_channel(dd->dma_lch_in.chan);\n}\n\nstatic void atmel_sha_get_cap(struct atmel_sha_dev *dd)\n{\n\n\tdd->caps.has_dma = 0;\n\tdd->caps.has_dualbuff = 0;\n\tdd->caps.has_sha224 = 0;\n\tdd->caps.has_sha_384_512 = 0;\n\tdd->caps.has_uihv = 0;\n\tdd->caps.has_hmac = 0;\n\n\t \n\tswitch (dd->hw_version & 0xff0) {\n\tcase 0x700:\n\tcase 0x600:\n\tcase 0x510:\n\t\tdd->caps.has_dma = 1;\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_sha224 = 1;\n\t\tdd->caps.has_sha_384_512 = 1;\n\t\tdd->caps.has_uihv = 1;\n\t\tdd->caps.has_hmac = 1;\n\t\tbreak;\n\tcase 0x420:\n\t\tdd->caps.has_dma = 1;\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_sha224 = 1;\n\t\tdd->caps.has_sha_384_512 = 1;\n\t\tdd->caps.has_uihv = 1;\n\t\tbreak;\n\tcase 0x410:\n\t\tdd->caps.has_dma = 1;\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_sha224 = 1;\n\t\tdd->caps.has_sha_384_512 = 1;\n\t\tbreak;\n\tcase 0x400:\n\t\tdd->caps.has_dma = 1;\n\t\tdd->caps.has_dualbuff = 1;\n\t\tdd->caps.has_sha224 = 1;\n\t\tbreak;\n\tcase 0x320:\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(dd->dev,\n\t\t\t\t\"Unmanaged sha version, set minimum capabilities\\n\");\n\t\tbreak;\n\t}\n}\n\nstatic const struct of_device_id atmel_sha_dt_ids[] = {\n\t{ .compatible = \"atmel,at91sam9g46-sha\" },\n\t{   }\n};\n\nMODULE_DEVICE_TABLE(of, atmel_sha_dt_ids);\n\nstatic int atmel_sha_probe(struct platform_device *pdev)\n{\n\tstruct atmel_sha_dev *sha_dd;\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *sha_res;\n\tint err;\n\n\tsha_dd = devm_kzalloc(&pdev->dev, sizeof(*sha_dd), GFP_KERNEL);\n\tif (!sha_dd)\n\t\treturn -ENOMEM;\n\n\tsha_dd->dev = dev;\n\n\tplatform_set_drvdata(pdev, sha_dd);\n\n\tINIT_LIST_HEAD(&sha_dd->list);\n\tspin_lock_init(&sha_dd->lock);\n\n\ttasklet_init(&sha_dd->done_task, atmel_sha_done_task,\n\t\t\t\t\t(unsigned long)sha_dd);\n\ttasklet_init(&sha_dd->queue_task, atmel_sha_queue_task,\n\t\t\t\t\t(unsigned long)sha_dd);\n\n\tcrypto_init_queue(&sha_dd->queue, ATMEL_SHA_QUEUE_LENGTH);\n\n\tsha_dd->io_base = devm_platform_get_and_ioremap_resource(pdev, 0, &sha_res);\n\tif (IS_ERR(sha_dd->io_base)) {\n\t\terr = PTR_ERR(sha_dd->io_base);\n\t\tgoto err_tasklet_kill;\n\t}\n\tsha_dd->phys_base = sha_res->start;\n\n\t \n\tsha_dd->irq = platform_get_irq(pdev,  0);\n\tif (sha_dd->irq < 0) {\n\t\terr = sha_dd->irq;\n\t\tgoto err_tasklet_kill;\n\t}\n\n\terr = devm_request_irq(&pdev->dev, sha_dd->irq, atmel_sha_irq,\n\t\t\t       IRQF_SHARED, \"atmel-sha\", sha_dd);\n\tif (err) {\n\t\tdev_err(dev, \"unable to request sha irq.\\n\");\n\t\tgoto err_tasklet_kill;\n\t}\n\n\t \n\tsha_dd->iclk = devm_clk_get(&pdev->dev, \"sha_clk\");\n\tif (IS_ERR(sha_dd->iclk)) {\n\t\tdev_err(dev, \"clock initialization failed.\\n\");\n\t\terr = PTR_ERR(sha_dd->iclk);\n\t\tgoto err_tasklet_kill;\n\t}\n\n\terr = clk_prepare(sha_dd->iclk);\n\tif (err)\n\t\tgoto err_tasklet_kill;\n\n\terr = atmel_sha_hw_version_init(sha_dd);\n\tif (err)\n\t\tgoto err_iclk_unprepare;\n\n\tatmel_sha_get_cap(sha_dd);\n\n\tif (sha_dd->caps.has_dma) {\n\t\terr = atmel_sha_dma_init(sha_dd);\n\t\tif (err)\n\t\t\tgoto err_iclk_unprepare;\n\n\t\tdev_info(dev, \"using %s for DMA transfers\\n\",\n\t\t\t\tdma_chan_name(sha_dd->dma_lch_in.chan));\n\t}\n\n\tspin_lock(&atmel_sha.lock);\n\tlist_add_tail(&sha_dd->list, &atmel_sha.dev_list);\n\tspin_unlock(&atmel_sha.lock);\n\n\terr = atmel_sha_register_algs(sha_dd);\n\tif (err)\n\t\tgoto err_algs;\n\n\tdev_info(dev, \"Atmel SHA1/SHA256%s%s\\n\",\n\t\t\tsha_dd->caps.has_sha224 ? \"/SHA224\" : \"\",\n\t\t\tsha_dd->caps.has_sha_384_512 ? \"/SHA384/SHA512\" : \"\");\n\n\treturn 0;\n\nerr_algs:\n\tspin_lock(&atmel_sha.lock);\n\tlist_del(&sha_dd->list);\n\tspin_unlock(&atmel_sha.lock);\n\tif (sha_dd->caps.has_dma)\n\t\tatmel_sha_dma_cleanup(sha_dd);\nerr_iclk_unprepare:\n\tclk_unprepare(sha_dd->iclk);\nerr_tasklet_kill:\n\ttasklet_kill(&sha_dd->queue_task);\n\ttasklet_kill(&sha_dd->done_task);\n\n\treturn err;\n}\n\nstatic int atmel_sha_remove(struct platform_device *pdev)\n{\n\tstruct atmel_sha_dev *sha_dd = platform_get_drvdata(pdev);\n\n\tspin_lock(&atmel_sha.lock);\n\tlist_del(&sha_dd->list);\n\tspin_unlock(&atmel_sha.lock);\n\n\tatmel_sha_unregister_algs(sha_dd);\n\n\ttasklet_kill(&sha_dd->queue_task);\n\ttasklet_kill(&sha_dd->done_task);\n\n\tif (sha_dd->caps.has_dma)\n\t\tatmel_sha_dma_cleanup(sha_dd);\n\n\tclk_unprepare(sha_dd->iclk);\n\n\treturn 0;\n}\n\nstatic struct platform_driver atmel_sha_driver = {\n\t.probe\t\t= atmel_sha_probe,\n\t.remove\t\t= atmel_sha_remove,\n\t.driver\t\t= {\n\t\t.name\t= \"atmel_sha\",\n\t\t.of_match_table\t= atmel_sha_dt_ids,\n\t},\n};\n\nmodule_platform_driver(atmel_sha_driver);\n\nMODULE_DESCRIPTION(\"Atmel SHA (1/256/224/384/512) hw acceleration support.\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Nicolas Royer - Eukr\u00e9a Electromatique\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}