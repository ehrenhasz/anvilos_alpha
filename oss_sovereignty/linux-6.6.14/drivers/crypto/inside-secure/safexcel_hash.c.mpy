{
  "module_name": "safexcel_hash.c",
  "hash_id": "22f1c6dcc72f48f927fe4110fd6d802063dae7029a6400997863d6cbfe6306ff",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/inside-secure/safexcel_hash.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/hmac.h>\n#include <crypto/md5.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/sha3.h>\n#include <crypto/skcipher.h>\n#include <crypto/sm3.h>\n#include <crypto/internal/cipher.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n\n#include \"safexcel.h\"\n\nstruct safexcel_ahash_ctx {\n\tstruct safexcel_context base;\n\n\tu32 alg;\n\tu8  key_sz;\n\tbool cbcmac;\n\tbool do_fallback;\n\tbool fb_init_done;\n\tbool fb_do_setkey;\n\n\tstruct crypto_aes_ctx *aes;\n\tstruct crypto_ahash *fback;\n\tstruct crypto_shash *shpre;\n\tstruct shash_desc *shdesc;\n};\n\nstruct safexcel_ahash_req {\n\tbool last_req;\n\tbool finish;\n\tbool hmac;\n\tbool needs_inv;\n\tbool hmac_zlen;\n\tbool len_is_le;\n\tbool not_first;\n\tbool xcbcmac;\n\n\tint nents;\n\tdma_addr_t result_dma;\n\n\tu32 digest;\n\n\tu8 state_sz;     \n\tu8 block_sz;     \n\tu8 digest_sz;    \n\t__le32 state[SHA3_512_BLOCK_SIZE /\n\t\t     sizeof(__le32)] __aligned(sizeof(__le32));\n\n\tu64 len;\n\tu64 processed;\n\n\tu8 cache[HASH_CACHE_SIZE] __aligned(sizeof(u32));\n\tdma_addr_t cache_dma;\n\tunsigned int cache_sz;\n\n\tu8 cache_next[HASH_CACHE_SIZE] __aligned(sizeof(u32));\n};\n\nstatic inline u64 safexcel_queued_len(struct safexcel_ahash_req *req)\n{\n\treturn req->len - req->processed;\n}\n\nstatic void safexcel_hash_token(struct safexcel_command_desc *cdesc,\n\t\t\t\tu32 input_length, u32 result_length,\n\t\t\t\tbool cbcmac)\n{\n\tstruct safexcel_token *token =\n\t\t(struct safexcel_token *)cdesc->control_data.token;\n\n\ttoken[0].opcode = EIP197_TOKEN_OPCODE_DIRECTION;\n\ttoken[0].packet_length = input_length;\n\ttoken[0].instructions = EIP197_TOKEN_INS_TYPE_HASH;\n\n\tinput_length &= 15;\n\tif (unlikely(cbcmac && input_length)) {\n\t\ttoken[0].stat =  0;\n\t\ttoken[1].opcode = EIP197_TOKEN_OPCODE_INSERT;\n\t\ttoken[1].packet_length = 16 - input_length;\n\t\ttoken[1].stat = EIP197_TOKEN_STAT_LAST_HASH;\n\t\ttoken[1].instructions = EIP197_TOKEN_INS_TYPE_HASH;\n\t} else {\n\t\ttoken[0].stat = EIP197_TOKEN_STAT_LAST_HASH;\n\t\teip197_noop_token(&token[1]);\n\t}\n\n\ttoken[2].opcode = EIP197_TOKEN_OPCODE_INSERT;\n\ttoken[2].stat = EIP197_TOKEN_STAT_LAST_HASH |\n\t\t\tEIP197_TOKEN_STAT_LAST_PACKET;\n\ttoken[2].packet_length = result_length;\n\ttoken[2].instructions = EIP197_TOKEN_INS_TYPE_OUTPUT |\n\t\t\t\tEIP197_TOKEN_INS_INSERT_HASH_DIGEST;\n\n\teip197_noop_token(&token[3]);\n}\n\nstatic void safexcel_context_control(struct safexcel_ahash_ctx *ctx,\n\t\t\t\t     struct safexcel_ahash_req *req,\n\t\t\t\t     struct safexcel_command_desc *cdesc)\n{\n\tstruct safexcel_crypto_priv *priv = ctx->base.priv;\n\tu64 count = 0;\n\n\tcdesc->control_data.control0 = ctx->alg;\n\tcdesc->control_data.control1 = 0;\n\n\t \n\tif (unlikely(req->digest == CONTEXT_CONTROL_DIGEST_XCM)) {\n\t\tif (req->xcbcmac)\n\t\t\tmemcpy(ctx->base.ctxr->data, &ctx->base.ipad, ctx->key_sz);\n\t\telse\n\t\t\tmemcpy(ctx->base.ctxr->data, req->state, req->state_sz);\n\n\t\tif (!req->finish && req->xcbcmac)\n\t\t\tcdesc->control_data.control0 |=\n\t\t\t\tCONTEXT_CONTROL_DIGEST_XCM |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT  |\n\t\t\t\tCONTEXT_CONTROL_NO_FINISH_HASH |\n\t\t\t\tCONTEXT_CONTROL_SIZE(req->state_sz /\n\t\t\t\t\t\t     sizeof(u32));\n\t\telse\n\t\t\tcdesc->control_data.control0 |=\n\t\t\t\tCONTEXT_CONTROL_DIGEST_XCM |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT  |\n\t\t\t\tCONTEXT_CONTROL_SIZE(req->state_sz /\n\t\t\t\t\t\t     sizeof(u32));\n\t\treturn;\n\t} else if (!req->processed) {\n\t\t \n\t\tif (req->finish)\n\t\t\tcdesc->control_data.control0 |= req->digest |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT |\n\t\t\t\tCONTEXT_CONTROL_RESTART_HASH  |\n\t\t\t\t \n\t\t\t\tCONTEXT_CONTROL_SIZE(1);\n\t\telse\n\t\t\tcdesc->control_data.control0 |= req->digest |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT  |\n\t\t\t\tCONTEXT_CONTROL_RESTART_HASH   |\n\t\t\t\tCONTEXT_CONTROL_NO_FINISH_HASH |\n\t\t\t\t \n\t\t\t\tCONTEXT_CONTROL_SIZE(1);\n\t\treturn;\n\t}\n\n\t \n\tmemcpy(ctx->base.ctxr->data, req->state, req->state_sz);\n\n\tif (req->finish) {\n\t\t \n\t\tif ((req->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED) ||\n\t\t    req->hmac_zlen || (req->processed != req->block_sz)) {\n\t\t\tcount = req->processed / EIP197_COUNTER_BLOCK_SIZE;\n\n\t\t\t \n\t\t\tif (unlikely(count & 0xffffffff00000000ULL)) {\n\t\t\t\tdev_warn(priv->dev,\n\t\t\t\t\t \"Input data is too big\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tif ((req->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED) ||\n\t\t     \n\t\t    req->hmac_zlen ||\n\t\t     \n\t\t    (req->processed != req->block_sz)) {\n\t\t\t \n\t\t\tcdesc->control_data.control0 |=\n\t\t\t\tCONTEXT_CONTROL_SIZE((req->state_sz >> 2) + 1) |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT |\n\t\t\t\tCONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\t\t\t \n\t\t\tif (req->hmac_zlen)\n\t\t\t\tcdesc->control_data.control0 |=\n\t\t\t\t\tCONTEXT_CONTROL_NO_FINISH_HASH;\n\t\t\tcdesc->control_data.control1 |=\n\t\t\t\tCONTEXT_CONTROL_DIGEST_CNT;\n\t\t\tctx->base.ctxr->data[req->state_sz >> 2] =\n\t\t\t\tcpu_to_le32(count);\n\t\t\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\n\t\t\t \n\t\t\treq->hmac_zlen = false;\n\t\t} else {  \n\t\t\t \n\t\t\tmemcpy(ctx->base.ctxr->data + (req->state_sz >> 2),\n\t\t\t       &ctx->base.opad, req->state_sz);\n\n\t\t\t \n\t\t\tcdesc->control_data.control0 |=\n\t\t\t\tCONTEXT_CONTROL_SIZE(req->state_sz >> 1) |\n\t\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT |\n\t\t\t\tCONTEXT_CONTROL_DIGEST_HMAC;\n\t\t}\n\t} else {  \n\t\tcdesc->control_data.control0 |=\n\t\t\tCONTEXT_CONTROL_SIZE(req->state_sz >> 2) |\n\t\t\tCONTEXT_CONTROL_DIGEST_PRECOMPUTED |\n\t\t\tCONTEXT_CONTROL_TYPE_HASH_OUT |\n\t\t\tCONTEXT_CONTROL_NO_FINISH_HASH;\n\t}\n}\n\nstatic int safexcel_ahash_enqueue(struct ahash_request *areq);\n\nstatic int safexcel_handle_req_result(struct safexcel_crypto_priv *priv,\n\t\t\t\t      int ring,\n\t\t\t\t      struct crypto_async_request *async,\n\t\t\t\t      bool *should_complete, int *ret)\n{\n\tstruct safexcel_result_desc *rdesc;\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_req *sreq = ahash_request_ctx_dma(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(ahash);\n\tu64 cache_len;\n\n\t*ret = 0;\n\n\trdesc = safexcel_ring_next_rptr(priv, &priv->ring[ring].rdr);\n\tif (IS_ERR(rdesc)) {\n\t\tdev_err(priv->dev,\n\t\t\t\"hash: result: could not retrieve the result descriptor\\n\");\n\t\t*ret = PTR_ERR(rdesc);\n\t} else {\n\t\t*ret = safexcel_rdesc_check_errors(priv, rdesc);\n\t}\n\n\tsafexcel_complete(priv, ring);\n\n\tif (sreq->nents) {\n\t\tdma_unmap_sg(priv->dev, areq->src, sreq->nents, DMA_TO_DEVICE);\n\t\tsreq->nents = 0;\n\t}\n\n\tif (sreq->result_dma) {\n\t\tdma_unmap_single(priv->dev, sreq->result_dma, sreq->digest_sz,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tsreq->result_dma = 0;\n\t}\n\n\tif (sreq->cache_dma) {\n\t\tdma_unmap_single(priv->dev, sreq->cache_dma, sreq->cache_sz,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tsreq->cache_dma = 0;\n\t\tsreq->cache_sz = 0;\n\t}\n\n\tif (sreq->finish) {\n\t\tif (sreq->hmac &&\n\t\t    (sreq->digest != CONTEXT_CONTROL_DIGEST_HMAC)) {\n\t\t\t \n\t\t\tmemcpy(sreq->cache, sreq->state,\n\t\t\t       crypto_ahash_digestsize(ahash));\n\n\t\t\tmemcpy(sreq->state, &ctx->base.opad, sreq->digest_sz);\n\n\t\t\tsreq->len = sreq->block_sz +\n\t\t\t\t    crypto_ahash_digestsize(ahash);\n\t\t\tsreq->processed = sreq->block_sz;\n\t\t\tsreq->hmac = 0;\n\n\t\t\tif (priv->flags & EIP197_TRC_CACHE)\n\t\t\t\tctx->base.needs_inv = true;\n\t\t\tareq->nbytes = 0;\n\t\t\tsafexcel_ahash_enqueue(areq);\n\n\t\t\t*should_complete = false;  \n\t\t\treturn 1;\n\t\t}\n\n\t\tif (unlikely(sreq->digest == CONTEXT_CONTROL_DIGEST_XCM &&\n\t\t\t     ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_CRC32)) {\n\t\t\t \n\t\t\t*(__le32 *)areq->result = ~sreq->state[0];\n\t\t} else {\n\t\t\tmemcpy(areq->result, sreq->state,\n\t\t\t       crypto_ahash_digestsize(ahash));\n\t\t}\n\t}\n\n\tcache_len = safexcel_queued_len(sreq);\n\tif (cache_len)\n\t\tmemcpy(sreq->cache, sreq->cache_next, cache_len);\n\n\t*should_complete = true;\n\n\treturn 1;\n}\n\nstatic int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,\n\t\t\t\t   int *commands, int *results)\n{\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_crypto_priv *priv = ctx->base.priv;\n\tstruct safexcel_command_desc *cdesc, *first_cdesc = NULL;\n\tstruct safexcel_result_desc *rdesc;\n\tstruct scatterlist *sg;\n\tstruct safexcel_token *dmmy;\n\tint i, extra = 0, n_cdesc = 0, ret = 0, cache_len, skip = 0;\n\tu64 queued, len;\n\n\tqueued = safexcel_queued_len(req);\n\tif (queued <= HASH_CACHE_SIZE)\n\t\tcache_len = queued;\n\telse\n\t\tcache_len = queued - areq->nbytes;\n\n\tif (!req->finish && !req->last_req) {\n\t\t \n\t\textra = queued & (HASH_CACHE_SIZE - 1);\n\n\t\t \n\t\tif (!extra)\n\t\t\textra = HASH_CACHE_SIZE;\n\n\t\tsg_pcopy_to_buffer(areq->src, sg_nents(areq->src),\n\t\t\t\t   req->cache_next, extra,\n\t\t\t\t   areq->nbytes - extra);\n\n\t\tqueued -= extra;\n\n\t\tif (!queued) {\n\t\t\t*commands = 0;\n\t\t\t*results = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\textra = 0;\n\t}\n\n\tif (unlikely(req->xcbcmac && req->processed > AES_BLOCK_SIZE)) {\n\t\tif (unlikely(cache_len < AES_BLOCK_SIZE)) {\n\t\t\t \n\t\t\textra = AES_BLOCK_SIZE - cache_len;\n\t\t\tif (queued > cache_len) {\n\t\t\t\t \n\t\t\t\tu64 tmp = queued - cache_len;\n\n\t\t\t\tskip = min_t(u64, tmp, extra);\n\t\t\t\tsg_pcopy_to_buffer(areq->src,\n\t\t\t\t\tsg_nents(areq->src),\n\t\t\t\t\treq->cache + cache_len,\n\t\t\t\t\tskip, 0);\n\t\t\t}\n\t\t\textra -= skip;\n\t\t\tmemset(req->cache + cache_len + skip, 0, extra);\n\t\t\tif (!ctx->cbcmac && extra) {\n\t\t\t\t\n\t\t\t\treq->cache[cache_len + skip] = 0x80;\n\t\t\t\t\n\t\t\t\tfor (i = 0; i < AES_BLOCK_SIZE / 4; i++) {\n\t\t\t\t\tu32 *cache = (void *)req->cache;\n\t\t\t\t\tu32 *ipad = ctx->base.ipad.word;\n\t\t\t\t\tu32 x;\n\n\t\t\t\t\tx = ipad[i] ^ ipad[i + 4];\n\t\t\t\t\tcache[i] ^= swab32(x);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcache_len = AES_BLOCK_SIZE;\n\t\t\tqueued = queued + extra;\n\t\t}\n\n\t\t \n\t\tcrypto_xor(req->cache, (const u8 *)req->state, AES_BLOCK_SIZE);\n\t}\n\n\tlen = queued;\n\t \n\tif (cache_len) {\n\t\treq->cache_dma = dma_map_single(priv->dev, req->cache,\n\t\t\t\t\t\tcache_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->dev, req->cache_dma))\n\t\t\treturn -EINVAL;\n\n\t\treq->cache_sz = cache_len;\n\t\tfirst_cdesc = safexcel_add_cdesc(priv, ring, 1,\n\t\t\t\t\t\t (cache_len == len),\n\t\t\t\t\t\t req->cache_dma, cache_len,\n\t\t\t\t\t\t len, ctx->base.ctxr_dma,\n\t\t\t\t\t\t &dmmy);\n\t\tif (IS_ERR(first_cdesc)) {\n\t\t\tret = PTR_ERR(first_cdesc);\n\t\t\tgoto unmap_cache;\n\t\t}\n\t\tn_cdesc++;\n\n\t\tqueued -= cache_len;\n\t\tif (!queued)\n\t\t\tgoto send_command;\n\t}\n\n\t \n\treq->nents = dma_map_sg(priv->dev, areq->src,\n\t\t\t\tsg_nents_for_len(areq->src,\n\t\t\t\t\t\t areq->nbytes),\n\t\t\t\tDMA_TO_DEVICE);\n\tif (!req->nents) {\n\t\tret = -ENOMEM;\n\t\tgoto cdesc_rollback;\n\t}\n\n\tfor_each_sg(areq->src, sg, req->nents, i) {\n\t\tint sglen = sg_dma_len(sg);\n\n\t\tif (unlikely(sglen <= skip)) {\n\t\t\tskip -= sglen;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif ((queued + skip) <= sglen)\n\t\t\tsglen = queued;\n\t\telse\n\t\t\tsglen -= skip;\n\n\t\tcdesc = safexcel_add_cdesc(priv, ring, !n_cdesc,\n\t\t\t\t\t   !(queued - sglen),\n\t\t\t\t\t   sg_dma_address(sg) + skip, sglen,\n\t\t\t\t\t   len, ctx->base.ctxr_dma, &dmmy);\n\t\tif (IS_ERR(cdesc)) {\n\t\t\tret = PTR_ERR(cdesc);\n\t\t\tgoto unmap_sg;\n\t\t}\n\n\t\tif (!n_cdesc)\n\t\t\tfirst_cdesc = cdesc;\n\t\tn_cdesc++;\n\n\t\tqueued -= sglen;\n\t\tif (!queued)\n\t\t\tbreak;\n\t\tskip = 0;\n\t}\n\nsend_command:\n\t \n\tsafexcel_context_control(ctx, req, first_cdesc);\n\n\t \n\tsafexcel_hash_token(first_cdesc, len, req->digest_sz, ctx->cbcmac);\n\n\treq->result_dma = dma_map_single(priv->dev, req->state, req->digest_sz,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\tif (dma_mapping_error(priv->dev, req->result_dma)) {\n\t\tret = -EINVAL;\n\t\tgoto unmap_sg;\n\t}\n\n\t \n\trdesc = safexcel_add_rdesc(priv, ring, 1, 1, req->result_dma,\n\t\t\t\t   req->digest_sz);\n\tif (IS_ERR(rdesc)) {\n\t\tret = PTR_ERR(rdesc);\n\t\tgoto unmap_result;\n\t}\n\n\tsafexcel_rdr_req_set(priv, ring, rdesc, &areq->base);\n\n\treq->processed += len - extra;\n\n\t*commands = n_cdesc;\n\t*results = 1;\n\treturn 0;\n\nunmap_result:\n\tdma_unmap_single(priv->dev, req->result_dma, req->digest_sz,\n\t\t\t DMA_FROM_DEVICE);\nunmap_sg:\n\tif (req->nents) {\n\t\tdma_unmap_sg(priv->dev, areq->src, req->nents, DMA_TO_DEVICE);\n\t\treq->nents = 0;\n\t}\ncdesc_rollback:\n\tfor (i = 0; i < n_cdesc; i++)\n\t\tsafexcel_ring_rollback_wptr(priv, &priv->ring[ring].cdr);\nunmap_cache:\n\tif (req->cache_dma) {\n\t\tdma_unmap_single(priv->dev, req->cache_dma, req->cache_sz,\n\t\t\t\t DMA_TO_DEVICE);\n\t\treq->cache_dma = 0;\n\t\treq->cache_sz = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int safexcel_handle_inv_result(struct safexcel_crypto_priv *priv,\n\t\t\t\t      int ring,\n\t\t\t\t      struct crypto_async_request *async,\n\t\t\t\t      bool *should_complete, int *ret)\n{\n\tstruct safexcel_result_desc *rdesc;\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(ahash);\n\tint enq_ret;\n\n\t*ret = 0;\n\n\trdesc = safexcel_ring_next_rptr(priv, &priv->ring[ring].rdr);\n\tif (IS_ERR(rdesc)) {\n\t\tdev_err(priv->dev,\n\t\t\t\"hash: invalidate: could not retrieve the result descriptor\\n\");\n\t\t*ret = PTR_ERR(rdesc);\n\t} else {\n\t\t*ret = safexcel_rdesc_check_errors(priv, rdesc);\n\t}\n\n\tsafexcel_complete(priv, ring);\n\n\tif (ctx->base.exit_inv) {\n\t\tdma_pool_free(priv->context_pool, ctx->base.ctxr,\n\t\t\t      ctx->base.ctxr_dma);\n\n\t\t*should_complete = true;\n\t\treturn 1;\n\t}\n\n\tring = safexcel_select_ring(priv);\n\tctx->base.ring = ring;\n\n\tspin_lock_bh(&priv->ring[ring].queue_lock);\n\tenq_ret = crypto_enqueue_request(&priv->ring[ring].queue, async);\n\tspin_unlock_bh(&priv->ring[ring].queue_lock);\n\n\tif (enq_ret != -EINPROGRESS)\n\t\t*ret = enq_ret;\n\n\tqueue_work(priv->ring[ring].workqueue,\n\t\t   &priv->ring[ring].work_data.work);\n\n\t*should_complete = false;\n\n\treturn 1;\n}\n\nstatic int safexcel_handle_result(struct safexcel_crypto_priv *priv, int ring,\n\t\t\t\t  struct crypto_async_request *async,\n\t\t\t\t  bool *should_complete, int *ret)\n{\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tint err;\n\n\tBUG_ON(!(priv->flags & EIP197_TRC_CACHE) && req->needs_inv);\n\n\tif (req->needs_inv) {\n\t\treq->needs_inv = false;\n\t\terr = safexcel_handle_inv_result(priv, ring, async,\n\t\t\t\t\t\t should_complete, ret);\n\t} else {\n\t\terr = safexcel_handle_req_result(priv, ring, async,\n\t\t\t\t\t\t should_complete, ret);\n\t}\n\n\treturn err;\n}\n\nstatic int safexcel_ahash_send_inv(struct crypto_async_request *async,\n\t\t\t\t   int ring, int *commands, int *results)\n{\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tint ret;\n\n\tret = safexcel_invalidate_cache(async, ctx->base.priv,\n\t\t\t\t\tctx->base.ctxr_dma, ring);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t*commands = 1;\n\t*results = 1;\n\n\treturn 0;\n}\n\nstatic int safexcel_ahash_send(struct crypto_async_request *async,\n\t\t\t       int ring, int *commands, int *results)\n{\n\tstruct ahash_request *areq = ahash_request_cast(async);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tint ret;\n\n\tif (req->needs_inv)\n\t\tret = safexcel_ahash_send_inv(async, ring, commands, results);\n\telse\n\t\tret = safexcel_ahash_send_req(async, ring, commands, results);\n\n\treturn ret;\n}\n\nstatic int safexcel_ahash_exit_inv(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct safexcel_crypto_priv *priv = ctx->base.priv;\n\tEIP197_REQUEST_ON_STACK(req, ahash, EIP197_AHASH_REQ_SIZE);\n\tstruct safexcel_ahash_req *rctx = ahash_request_ctx_dma(req);\n\tDECLARE_CRYPTO_WAIT(result);\n\tint ring = ctx->base.ring;\n\tint err;\n\n\tmemset(req, 0, EIP197_AHASH_REQ_SIZE);\n\n\t \n\tinit_completion(&result.completion);\n\tahash_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t   crypto_req_done, &result);\n\n\tahash_request_set_tfm(req, __crypto_ahash_cast(tfm));\n\tctx = crypto_tfm_ctx(req->base.tfm);\n\tctx->base.exit_inv = true;\n\trctx->needs_inv = true;\n\n\tspin_lock_bh(&priv->ring[ring].queue_lock);\n\tcrypto_enqueue_request(&priv->ring[ring].queue, &req->base);\n\tspin_unlock_bh(&priv->ring[ring].queue_lock);\n\n\tqueue_work(priv->ring[ring].workqueue,\n\t\t   &priv->ring[ring].work_data.work);\n\n\terr = crypto_wait_req(-EINPROGRESS, &result);\n\n\tif (err) {\n\t\tdev_warn(priv->dev, \"hash: completion error (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int safexcel_ahash_cache(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tu64 cache_len;\n\n\t \n\tcache_len = safexcel_queued_len(req);\n\n\t \n\tif (cache_len + areq->nbytes <= HASH_CACHE_SIZE) {\n\t\tsg_pcopy_to_buffer(areq->src, sg_nents(areq->src),\n\t\t\t\t   req->cache + cache_len,\n\t\t\t\t   areq->nbytes, 0);\n\t\treturn 0;\n\t}\n\n\t \n\treturn -E2BIG;\n}\n\nstatic int safexcel_ahash_enqueue(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tstruct safexcel_crypto_priv *priv = ctx->base.priv;\n\tint ret, ring;\n\n\treq->needs_inv = false;\n\n\tif (ctx->base.ctxr) {\n\t\tif (priv->flags & EIP197_TRC_CACHE && !ctx->base.needs_inv &&\n\t\t      \n\t\t   ((req->not_first && !req->xcbcmac) ||\n\t\t      \n\t\t     memcmp(ctx->base.ctxr->data, req->state, req->state_sz) ||\n\t\t      \n\t\t     (req->finish && req->hmac &&\n\t\t      memcmp(ctx->base.ctxr->data + (req->state_sz>>2),\n\t\t\t     &ctx->base.opad, req->state_sz))))\n\t\t\t \n\t\t\tctx->base.needs_inv = true;\n\n\t\tif (ctx->base.needs_inv) {\n\t\t\tctx->base.needs_inv = false;\n\t\t\treq->needs_inv = true;\n\t\t}\n\t} else {\n\t\tctx->base.ring = safexcel_select_ring(priv);\n\t\tctx->base.ctxr = dma_pool_zalloc(priv->context_pool,\n\t\t\t\t\t\t EIP197_GFP_FLAGS(areq->base),\n\t\t\t\t\t\t &ctx->base.ctxr_dma);\n\t\tif (!ctx->base.ctxr)\n\t\t\treturn -ENOMEM;\n\t}\n\treq->not_first = true;\n\n\tring = ctx->base.ring;\n\n\tspin_lock_bh(&priv->ring[ring].queue_lock);\n\tret = crypto_enqueue_request(&priv->ring[ring].queue, &areq->base);\n\tspin_unlock_bh(&priv->ring[ring].queue_lock);\n\n\tqueue_work(priv->ring[ring].workqueue,\n\t\t   &priv->ring[ring].work_data.work);\n\n\treturn ret;\n}\n\nstatic int safexcel_ahash_update(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tint ret;\n\n\t \n\tif (!areq->nbytes)\n\t\treturn 0;\n\n\t \n\tret = safexcel_ahash_cache(areq);\n\n\t \n\treq->len += areq->nbytes;\n\n\t \n\tif ((ret && !req->finish) || req->last_req)\n\t\treturn safexcel_ahash_enqueue(areq);\n\n\treturn 0;\n}\n\nstatic int safexcel_ahash_final(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\n\treq->finish = true;\n\n\tif (unlikely(!req->len && !areq->nbytes)) {\n\t\t \n\t\tif (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_MD5)\n\t\t\tmemcpy(areq->result, md5_zero_message_hash,\n\t\t\t       MD5_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA1)\n\t\t\tmemcpy(areq->result, sha1_zero_message_hash,\n\t\t\t       SHA1_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA224)\n\t\t\tmemcpy(areq->result, sha224_zero_message_hash,\n\t\t\t       SHA224_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA256)\n\t\t\tmemcpy(areq->result, sha256_zero_message_hash,\n\t\t\t       SHA256_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA384)\n\t\t\tmemcpy(areq->result, sha384_zero_message_hash,\n\t\t\t       SHA384_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA512)\n\t\t\tmemcpy(areq->result, sha512_zero_message_hash,\n\t\t\t       SHA512_DIGEST_SIZE);\n\t\telse if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SM3) {\n\t\t\tmemcpy(areq->result,\n\t\t\t       EIP197_SM3_ZEROM_HASH, SM3_DIGEST_SIZE);\n\t\t}\n\n\t\treturn 0;\n\t} else if (unlikely(req->digest == CONTEXT_CONTROL_DIGEST_XCM &&\n\t\t\t    ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_MD5 &&\n\t\t\t    req->len == sizeof(u32) && !areq->nbytes)) {\n\t\t \n\t\tmemcpy(areq->result, &ctx->base.ipad, sizeof(u32));\n\t\treturn 0;\n\t} else if (unlikely(ctx->cbcmac && req->len == AES_BLOCK_SIZE &&\n\t\t\t    !areq->nbytes)) {\n\t\t \n\t\tmemset(areq->result, 0, AES_BLOCK_SIZE);\n\t\treturn 0;\n\t} else if (unlikely(req->xcbcmac && req->len == AES_BLOCK_SIZE &&\n\t\t\t    !areq->nbytes)) {\n\t\t \n\t\tint i;\n\n\t\tfor (i = 0; i < AES_BLOCK_SIZE / sizeof(u32); i++) {\n\t\t\tu32 *result = (void *)areq->result;\n\n\t\t\t \n\t\t\tresult[i] = swab32(ctx->base.ipad.word[i + 4]);\n\t\t}\n\t\tareq->result[0] ^= 0x80;\t\t\t \n\t\taes_encrypt(ctx->aes, areq->result, areq->result);\n\t\treturn 0;\n\t} else if (unlikely(req->hmac &&\n\t\t\t    (req->len == req->block_sz) &&\n\t\t\t    !areq->nbytes)) {\n\t\t \n\n\t\t \n\t\t \n\t\tmemset(req->cache, 0, req->block_sz);\n\t\t \n\t\treq->cache[0] = 0x80;\n\t\t \n\t\tif (req->len_is_le) {\n\t\t\t \n\t\t\treq->cache[req->block_sz-8] = (req->block_sz << 3) &\n\t\t\t\t\t\t      255;\n\t\t\treq->cache[req->block_sz-7] = (req->block_sz >> 5);\n\t\t} else {\n\t\t\t \n\t\t\treq->cache[req->block_sz-2] = (req->block_sz >> 5);\n\t\t\treq->cache[req->block_sz-1] = (req->block_sz << 3) &\n\t\t\t\t\t\t      255;\n\t\t}\n\n\t\treq->len += req->block_sz;  \n\n\t\t \n\t\treq->hmac_zlen = true;\n\n\t\t \n\t\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\t} else if (req->hmac) {\n\t\t \n\t\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\t}\n\n\treturn safexcel_ahash_enqueue(areq);\n}\n\nstatic int safexcel_ahash_finup(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\treq->finish = true;\n\n\tsafexcel_ahash_update(areq);\n\treturn safexcel_ahash_final(areq);\n}\n\nstatic int safexcel_ahash_export(struct ahash_request *areq, void *out)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tstruct safexcel_ahash_export_state *export = out;\n\n\texport->len = req->len;\n\texport->processed = req->processed;\n\n\texport->digest = req->digest;\n\n\tmemcpy(export->state, req->state, req->state_sz);\n\tmemcpy(export->cache, req->cache, HASH_CACHE_SIZE);\n\n\treturn 0;\n}\n\nstatic int safexcel_ahash_import(struct ahash_request *areq, const void *in)\n{\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\tconst struct safexcel_ahash_export_state *export = in;\n\tint ret;\n\n\tret = crypto_ahash_init(areq);\n\tif (ret)\n\t\treturn ret;\n\n\treq->len = export->len;\n\treq->processed = export->processed;\n\n\treq->digest = export->digest;\n\n\tmemcpy(req->cache, export->cache, HASH_CACHE_SIZE);\n\tmemcpy(req->state, export->state, req->state_sz);\n\n\treturn 0;\n}\n\nstatic int safexcel_ahash_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct safexcel_alg_template *tmpl =\n\t\tcontainer_of(__crypto_ahash_alg(tfm->__crt_alg),\n\t\t\t     struct safexcel_alg_template, alg.ahash);\n\n\tctx->base.priv = tmpl->priv;\n\tctx->base.send = safexcel_ahash_send;\n\tctx->base.handle_result = safexcel_handle_result;\n\tctx->fb_do_setkey = false;\n\n\tcrypto_ahash_set_reqsize_dma(__crypto_ahash_cast(tfm),\n\t\t\t\t     sizeof(struct safexcel_ahash_req));\n\treturn 0;\n}\n\nstatic int safexcel_sha1_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA1;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA1_DIGEST_SIZE;\n\treq->digest_sz = SHA1_DIGEST_SIZE;\n\treq->block_sz = SHA1_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sha1_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sha1_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstatic void safexcel_ahash_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct safexcel_crypto_priv *priv = ctx->base.priv;\n\tint ret;\n\n\t \n\tif (!ctx->base.ctxr)\n\t\treturn;\n\n\tif (priv->flags & EIP197_TRC_CACHE) {\n\t\tret = safexcel_ahash_exit_inv(tfm);\n\t\tif (ret)\n\t\t\tdev_warn(priv->dev, \"hash: invalidation error %d\\n\", ret);\n\t} else {\n\t\tdma_pool_free(priv->context_pool, ctx->base.ctxr,\n\t\t\t      ctx->base.ctxr_dma);\n\t}\n}\n\nstruct safexcel_alg_template safexcel_alg_sha1 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA1,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha1_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sha1_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha1\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha1\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha1_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA1_DIGEST_SIZE);\n\t \n\treq->len\t= SHA1_BLOCK_SIZE;\n\treq->processed\t= SHA1_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA1;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA1_DIGEST_SIZE;\n\treq->digest_sz = SHA1_DIGEST_SIZE;\n\treq->block_sz = SHA1_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha1_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sha1_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstatic int safexcel_hmac_init_pad(struct ahash_request *areq,\n\t\t\t\t  unsigned int blocksize, const u8 *key,\n\t\t\t\t  unsigned int keylen, u8 *ipad, u8 *opad)\n{\n\tDECLARE_CRYPTO_WAIT(result);\n\tstruct scatterlist sg;\n\tint ret, i;\n\tu8 *keydup;\n\n\tif (keylen <= blocksize) {\n\t\tmemcpy(ipad, key, keylen);\n\t} else {\n\t\tkeydup = kmemdup(key, keylen, GFP_KERNEL);\n\t\tif (!keydup)\n\t\t\treturn -ENOMEM;\n\n\t\tahash_request_set_callback(areq, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t\t   crypto_req_done, &result);\n\t\tsg_init_one(&sg, keydup, keylen);\n\t\tahash_request_set_crypt(areq, &sg, ipad, keylen);\n\n\t\tret = crypto_ahash_digest(areq);\n\t\tret = crypto_wait_req(ret, &result);\n\n\t\t \n\t\tkfree_sensitive(keydup);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tkeylen = crypto_ahash_digestsize(crypto_ahash_reqtfm(areq));\n\t}\n\n\tmemset(ipad + keylen, 0, blocksize - keylen);\n\tmemcpy(opad, ipad, blocksize);\n\n\tfor (i = 0; i < blocksize; i++) {\n\t\tipad[i] ^= HMAC_IPAD_VALUE;\n\t\topad[i] ^= HMAC_OPAD_VALUE;\n\t}\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_init_iv(struct ahash_request *areq,\n\t\t\t\t unsigned int blocksize, u8 *pad, void *state)\n{\n\tstruct safexcel_ahash_req *req;\n\tDECLARE_CRYPTO_WAIT(result);\n\tstruct scatterlist sg;\n\tint ret;\n\n\tahash_request_set_callback(areq, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t   crypto_req_done, &result);\n\tsg_init_one(&sg, pad, blocksize);\n\tahash_request_set_crypt(areq, &sg, pad, blocksize);\n\n\tret = crypto_ahash_init(areq);\n\tif (ret)\n\t\treturn ret;\n\n\treq = ahash_request_ctx_dma(areq);\n\treq->hmac = true;\n\treq->last_req = true;\n\n\tret = crypto_ahash_update(areq);\n\tret = crypto_wait_req(ret, &result);\n\n\treturn ret ?: crypto_ahash_export(areq, state);\n}\n\nstatic int __safexcel_hmac_setkey(const char *alg, const u8 *key,\n\t\t\t\t  unsigned int keylen,\n\t\t\t\t  void *istate, void *ostate)\n{\n\tstruct ahash_request *areq;\n\tstruct crypto_ahash *tfm;\n\tunsigned int blocksize;\n\tu8 *ipad, *opad;\n\tint ret;\n\n\ttfm = crypto_alloc_ahash(alg, 0, 0);\n\tif (IS_ERR(tfm))\n\t\treturn PTR_ERR(tfm);\n\n\tareq = ahash_request_alloc(tfm, GFP_KERNEL);\n\tif (!areq) {\n\t\tret = -ENOMEM;\n\t\tgoto free_ahash;\n\t}\n\n\tcrypto_ahash_clear_flags(tfm, ~0);\n\tblocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\n\n\tipad = kcalloc(2, blocksize, GFP_KERNEL);\n\tif (!ipad) {\n\t\tret = -ENOMEM;\n\t\tgoto free_request;\n\t}\n\n\topad = ipad + blocksize;\n\n\tret = safexcel_hmac_init_pad(areq, blocksize, key, keylen, ipad, opad);\n\tif (ret)\n\t\tgoto free_ipad;\n\n\tret = safexcel_hmac_init_iv(areq, blocksize, ipad, istate);\n\tif (ret)\n\t\tgoto free_ipad;\n\n\tret = safexcel_hmac_init_iv(areq, blocksize, opad, ostate);\n\nfree_ipad:\n\tkfree(ipad);\nfree_request:\n\tahash_request_free(areq);\nfree_ahash:\n\tcrypto_free_ahash(tfm);\n\n\treturn ret;\n}\n\nint safexcel_hmac_setkey(struct safexcel_context *base, const u8 *key,\n\t\t\t unsigned int keylen, const char *alg,\n\t\t\t unsigned int state_sz)\n{\n\tstruct safexcel_crypto_priv *priv = base->priv;\n\tstruct safexcel_ahash_export_state istate, ostate;\n\tint ret;\n\n\tret = __safexcel_hmac_setkey(alg, key, keylen, &istate, &ostate);\n\tif (ret)\n\t\treturn ret;\n\n\tif (priv->flags & EIP197_TRC_CACHE && base->ctxr &&\n\t    (memcmp(&base->ipad, istate.state, state_sz) ||\n\t     memcmp(&base->opad, ostate.state, state_sz)))\n\t\tbase->needs_inv = true;\n\n\tmemcpy(&base->ipad, &istate.state, state_sz);\n\tmemcpy(&base->opad, &ostate.state, state_sz);\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_alg_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t    unsigned int keylen, const char *alg,\n\t\t\t\t    unsigned int state_sz)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\treturn safexcel_hmac_setkey(&ctx->base, key, keylen, alg, state_sz);\n}\n\nstatic int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sha1\",\n\t\t\t\t\tSHA1_DIGEST_SIZE);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha1 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA1,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha1_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sha1_digest,\n\t\t.setkey = safexcel_hmac_sha1_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha1)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha1\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha256_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA256;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA256_DIGEST_SIZE;\n\treq->digest_sz = SHA256_DIGEST_SIZE;\n\treq->block_sz = SHA256_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sha256_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sha256_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha256 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_256,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha256_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sha256_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha256\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha256\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha224_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA224;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA256_DIGEST_SIZE;\n\treq->digest_sz = SHA256_DIGEST_SIZE;\n\treq->block_sz = SHA256_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sha224_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sha224_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha224 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_256,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha224_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sha224_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha224\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha224\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA224_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha224_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t       unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sha224\",\n\t\t\t\t\tSHA256_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_sha224_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA256_DIGEST_SIZE);\n\t \n\treq->len\t= SHA256_BLOCK_SIZE;\n\treq->processed\t= SHA256_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA224;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA256_DIGEST_SIZE;\n\treq->digest_sz = SHA256_DIGEST_SIZE;\n\treq->block_sz = SHA256_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha224_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sha224_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha224 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_256,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha224_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sha224_digest,\n\t\t.setkey = safexcel_hmac_sha224_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha224)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha224\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA224_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha256_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sha256\",\n\t\t\t\t\tSHA256_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_sha256_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA256_DIGEST_SIZE);\n\t \n\treq->len\t= SHA256_BLOCK_SIZE;\n\treq->processed\t= SHA256_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA256;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA256_DIGEST_SIZE;\n\treq->digest_sz = SHA256_DIGEST_SIZE;\n\treq->block_sz = SHA256_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha256_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sha256_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha256 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_256,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha256_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sha256_digest,\n\t\t.setkey = safexcel_hmac_sha256_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha256)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha256\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha512_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA512_DIGEST_SIZE;\n\treq->digest_sz = SHA512_DIGEST_SIZE;\n\treq->block_sz = SHA512_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sha512_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sha512_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha512 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_512,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha512_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sha512_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha512\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha512\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA512_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha384_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA512_DIGEST_SIZE;\n\treq->digest_sz = SHA512_DIGEST_SIZE;\n\treq->block_sz = SHA512_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sha384_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sha384_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha384 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_512,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha384_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sha384_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha384\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha384\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA384_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha512_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t       unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sha512\",\n\t\t\t\t\tSHA512_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_sha512_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA512_DIGEST_SIZE);\n\t \n\treq->len\t= SHA512_BLOCK_SIZE;\n\treq->processed\t= SHA512_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA512_DIGEST_SIZE;\n\treq->digest_sz = SHA512_DIGEST_SIZE;\n\treq->block_sz = SHA512_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha512_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sha512_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha512 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_512,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha512_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sha512_digest,\n\t\t.setkey = safexcel_hmac_sha512_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha512)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha512\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA512_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha384_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t       unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sha384\",\n\t\t\t\t\tSHA512_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_sha384_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA512_DIGEST_SIZE);\n\t \n\treq->len\t= SHA512_BLOCK_SIZE;\n\treq->processed\t= SHA512_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SHA512_DIGEST_SIZE;\n\treq->digest_sz = SHA512_DIGEST_SIZE;\n\treq->block_sz = SHA512_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha384_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sha384_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha384 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA2_512,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha384_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sha384_digest,\n\t\t.setkey = safexcel_hmac_sha384_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha384)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha384\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SHA384_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_md5_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = MD5_DIGEST_SIZE;\n\treq->digest_sz = MD5_DIGEST_SIZE;\n\treq->block_sz = MD5_HMAC_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_md5_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_md5_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_md5 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_MD5,\n\t.alg.ahash = {\n\t\t.init = safexcel_md5_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_md5_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = MD5_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"md5\",\n\t\t\t\t.cra_driver_name = \"safexcel-md5\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = MD5_HMAC_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_md5_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, MD5_DIGEST_SIZE);\n\t \n\treq->len\t= MD5_HMAC_BLOCK_SIZE;\n\treq->processed\t= MD5_HMAC_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = MD5_DIGEST_SIZE;\n\treq->digest_sz = MD5_DIGEST_SIZE;\n\treq->block_sz = MD5_HMAC_BLOCK_SIZE;\n\treq->len_is_le = true;  \n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_md5_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-md5\",\n\t\t\t\t\tMD5_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_md5_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_md5_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_md5 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_MD5,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_md5_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_md5_digest,\n\t\t.setkey = safexcel_hmac_md5_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = MD5_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(md5)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-md5\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = MD5_HMAC_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_crc32_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tint ret = safexcel_ahash_cra_init(tfm);\n\n\t \n\tmemset(&ctx->base.ipad, 0, sizeof(u32));\n\treturn ret;\n}\n\nstatic int safexcel_crc32_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\treq->state[0]\t= cpu_to_le32(~ctx->base.ipad.word[0]);\n\t \n\treq->len\t= sizeof(u32);\n\treq->processed\t= sizeof(u32);\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_CRC32;\n\treq->digest = CONTEXT_CONTROL_DIGEST_XCM;\n\treq->state_sz = sizeof(u32);\n\treq->digest_sz = sizeof(u32);\n\treq->block_sz = sizeof(u32);\n\n\treturn 0;\n}\n\nstatic int safexcel_crc32_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\n\n\tif (keylen != sizeof(u32))\n\t\treturn -EINVAL;\n\n\tmemcpy(&ctx->base.ipad, key, sizeof(u32));\n\treturn 0;\n}\n\nstatic int safexcel_crc32_digest(struct ahash_request *areq)\n{\n\treturn safexcel_crc32_init(areq) ?: safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_crc32 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = 0,\n\t.alg.ahash = {\n\t\t.init = safexcel_crc32_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_crc32_digest,\n\t\t.setkey = safexcel_crc32_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = sizeof(u32),\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"crc32\",\n\t\t\t\t.cra_driver_name = \"safexcel-crc32\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_OPTIONAL_KEY |\n\t\t\t\t\t     CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = 1,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_crc32_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_cbcmac_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, ctx->key_sz);\n\t \n\treq->len\t= AES_BLOCK_SIZE;\n\treq->processed\t= AES_BLOCK_SIZE;\n\n\treq->digest   = CONTEXT_CONTROL_DIGEST_XCM;\n\treq->state_sz = ctx->key_sz;\n\treq->digest_sz = AES_BLOCK_SIZE;\n\treq->block_sz = AES_BLOCK_SIZE;\n\treq->xcbcmac  = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_cbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t unsigned int len)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\n\tstruct crypto_aes_ctx aes;\n\tint ret, i;\n\n\tret = aes_expandkey(&aes, key, len);\n\tif (ret)\n\t\treturn ret;\n\n\tmemset(&ctx->base.ipad, 0, 2 * AES_BLOCK_SIZE);\n\tfor (i = 0; i < len / sizeof(u32); i++)\n\t\tctx->base.ipad.be[i + 8] = cpu_to_be32(aes.key_enc[i]);\n\n\tif (len == AES_KEYSIZE_192) {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;\n\t\tctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t} else if (len == AES_KEYSIZE_256) {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;\n\t\tctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t} else {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;\n\t\tctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t}\n\tctx->cbcmac  = true;\n\n\tmemzero_explicit(&aes, sizeof(aes));\n\treturn 0;\n}\n\nstatic int safexcel_cbcmac_digest(struct ahash_request *areq)\n{\n\treturn safexcel_cbcmac_init(areq) ?: safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_cbcmac = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = 0,\n\t.alg.ahash = {\n\t\t.init = safexcel_cbcmac_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_cbcmac_digest,\n\t\t.setkey = safexcel_cbcmac_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"cbcmac(aes)\",\n\t\t\t\t.cra_driver_name = \"safexcel-cbcmac-aes\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = 1,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_xcbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t unsigned int len)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\n\tu32 key_tmp[3 * AES_BLOCK_SIZE / sizeof(u32)];\n\tint ret, i;\n\n\tret = aes_expandkey(ctx->aes, key, len);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\taes_encrypt(ctx->aes, (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,\n\t\t    \"\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\\x1\");\n\taes_encrypt(ctx->aes, (u8 *)key_tmp,\n\t\t    \"\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\\x2\");\n\taes_encrypt(ctx->aes, (u8 *)key_tmp + AES_BLOCK_SIZE,\n\t\t    \"\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\\x3\");\n\tfor (i = 0; i < 3 * AES_BLOCK_SIZE / sizeof(u32); i++)\n\t\tctx->base.ipad.word[i] = swab32(key_tmp[i]);\n\n\tret = aes_expandkey(ctx->aes,\n\t\t\t    (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,\n\t\t\t    AES_MIN_KEY_SIZE);\n\tif (ret)\n\t\treturn ret;\n\n\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;\n\tctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\tctx->cbcmac = false;\n\n\treturn 0;\n}\n\nstatic int safexcel_xcbcmac_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tsafexcel_ahash_cra_init(tfm);\n\tctx->aes = kmalloc(sizeof(*ctx->aes), GFP_KERNEL);\n\treturn PTR_ERR_OR_ZERO(ctx->aes);\n}\n\nstatic void safexcel_xcbcmac_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tkfree(ctx->aes);\n\tsafexcel_ahash_cra_exit(tfm);\n}\n\nstruct safexcel_alg_template safexcel_alg_xcbcmac = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = 0,\n\t.alg.ahash = {\n\t\t.init = safexcel_cbcmac_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_cbcmac_digest,\n\t\t.setkey = safexcel_xcbcmac_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"xcbc(aes)\",\n\t\t\t\t.cra_driver_name = \"safexcel-xcbc-aes\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_xcbcmac_cra_init,\n\t\t\t\t.cra_exit = safexcel_xcbcmac_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_cmac_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\tunsigned int len)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\n\t__be64 consts[4];\n\tu64 _const[2];\n\tu8 msb_mask, gfmask;\n\tint ret, i;\n\n\t \n\tret = aes_expandkey(ctx->aes, key, len);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < len / sizeof(u32); i++)\n\t\tctx->base.ipad.word[i + 8] = swab32(ctx->aes->key_enc[i]);\n\n\t \n\t \n\tmemset(consts, 0, AES_BLOCK_SIZE);\n\taes_encrypt(ctx->aes, (u8 *)consts, (u8 *)consts);\n\n\tgfmask = 0x87;\n\t_const[0] = be64_to_cpu(consts[1]);\n\t_const[1] = be64_to_cpu(consts[0]);\n\n\t \n\tfor (i = 0; i < 4; i += 2) {\n\t\tmsb_mask = ((s64)_const[1] >> 63) & gfmask;\n\t\t_const[1] = (_const[1] << 1) | (_const[0] >> 63);\n\t\t_const[0] = (_const[0] << 1) ^ msb_mask;\n\n\t\tconsts[i + 0] = cpu_to_be64(_const[1]);\n\t\tconsts[i + 1] = cpu_to_be64(_const[0]);\n\t}\n\t \n\n\tfor (i = 0; i < 2 * AES_BLOCK_SIZE / sizeof(u32); i++)\n\t\tctx->base.ipad.be[i] = cpu_to_be32(((u32 *)consts)[i]);\n\n\tif (len == AES_KEYSIZE_192) {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;\n\t\tctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t} else if (len == AES_KEYSIZE_256) {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;\n\t\tctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t} else {\n\t\tctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;\n\t\tctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;\n\t}\n\tctx->cbcmac = false;\n\n\treturn 0;\n}\n\nstruct safexcel_alg_template safexcel_alg_cmac = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = 0,\n\t.alg.ahash = {\n\t\t.init = safexcel_cbcmac_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_cbcmac_digest,\n\t\t.setkey = safexcel_cmac_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = AES_BLOCK_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"cmac(aes)\",\n\t\t\t\t.cra_driver_name = \"safexcel-cmac-aes\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_xcbcmac_cra_init,\n\t\t\t\t.cra_exit = safexcel_xcbcmac_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sm3_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SM3_DIGEST_SIZE;\n\treq->digest_sz = SM3_DIGEST_SIZE;\n\treq->block_sz = SM3_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int safexcel_sm3_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_sm3_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_sm3 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SM3,\n\t.alg.ahash = {\n\t\t.init = safexcel_sm3_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_sm3_digest,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SM3_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sm3\",\n\t\t\t\t.cra_driver_name = \"safexcel-sm3\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SM3_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sm3_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t    unsigned int keylen)\n{\n\treturn safexcel_hmac_alg_setkey(tfm, key, keylen, \"safexcel-sm3\",\n\t\t\t\t\tSM3_DIGEST_SIZE);\n}\n\nstatic int safexcel_hmac_sm3_init(struct ahash_request *areq)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SM3_DIGEST_SIZE);\n\t \n\treq->len\t= SM3_BLOCK_SIZE;\n\treq->processed\t= SM3_BLOCK_SIZE;\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;\n\treq->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;\n\treq->state_sz = SM3_DIGEST_SIZE;\n\treq->digest_sz = SM3_DIGEST_SIZE;\n\treq->block_sz = SM3_BLOCK_SIZE;\n\treq->hmac = true;\n\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sm3_digest(struct ahash_request *areq)\n{\n\tint ret = safexcel_hmac_sm3_init(areq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn safexcel_ahash_finup(areq);\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sm3 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SM3,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sm3_init,\n\t\t.update = safexcel_ahash_update,\n\t\t.final = safexcel_ahash_final,\n\t\t.finup = safexcel_ahash_finup,\n\t\t.digest = safexcel_hmac_sm3_digest,\n\t\t.setkey = safexcel_hmac_sm3_setkey,\n\t\t.export = safexcel_ahash_export,\n\t\t.import = safexcel_ahash_import,\n\t\t.halg = {\n\t\t\t.digestsize = SM3_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sm3)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sm3\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t\t\t.cra_blocksize = SM3_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_ahash_cra_init,\n\t\t\t\t.cra_exit = safexcel_ahash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha3_224_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;\n\treq->digest = CONTEXT_CONTROL_DIGEST_INITIAL;\n\treq->state_sz = SHA3_224_DIGEST_SIZE;\n\treq->digest_sz = SHA3_224_DIGEST_SIZE;\n\treq->block_sz = SHA3_224_BLOCK_SIZE;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_sha3_fbcheck(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\tint ret = 0;\n\n\tif (ctx->do_fallback) {\n\t\tahash_request_set_tfm(subreq, ctx->fback);\n\t\tahash_request_set_callback(subreq, req->base.flags,\n\t\t\t\t\t   req->base.complete, req->base.data);\n\t\tahash_request_set_crypt(subreq, req->src, req->result,\n\t\t\t\t\treq->nbytes);\n\t\tif (!ctx->fb_init_done) {\n\t\t\tif (ctx->fb_do_setkey) {\n\t\t\t\t \n\t\t\t\tu8 key[SHA3_224_BLOCK_SIZE];\n\n\t\t\t\tmemcpy(key, &ctx->base.ipad,\n\t\t\t\t       crypto_ahash_blocksize(ctx->fback) / 2);\n\t\t\t\tmemcpy(key +\n\t\t\t\t       crypto_ahash_blocksize(ctx->fback) / 2,\n\t\t\t\t       &ctx->base.opad,\n\t\t\t\t       crypto_ahash_blocksize(ctx->fback) / 2);\n\t\t\t\tret = crypto_ahash_setkey(ctx->fback, key,\n\t\t\t\t\tcrypto_ahash_blocksize(ctx->fback));\n\t\t\t\tmemzero_explicit(key,\n\t\t\t\t\tcrypto_ahash_blocksize(ctx->fback));\n\t\t\t\tctx->fb_do_setkey = false;\n\t\t\t}\n\t\t\tret = ret ?: crypto_ahash_init(subreq);\n\t\t\tctx->fb_init_done = true;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int safexcel_sha3_update(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback = true;\n\treturn safexcel_sha3_fbcheck(req) ?: crypto_ahash_update(subreq);\n}\n\nstatic int safexcel_sha3_final(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback = true;\n\treturn safexcel_sha3_fbcheck(req) ?: crypto_ahash_final(subreq);\n}\n\nstatic int safexcel_sha3_finup(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback |= !req->nbytes;\n\tif (ctx->do_fallback)\n\t\t \n\t\treturn safexcel_sha3_fbcheck(req) ?:\n\t\t       crypto_ahash_finup(subreq);\n\telse\n\t\treturn safexcel_ahash_finup(req);\n}\n\nstatic int safexcel_sha3_digest_fallback(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback = true;\n\tctx->fb_init_done = false;\n\treturn safexcel_sha3_fbcheck(req) ?: crypto_ahash_finup(subreq);\n}\n\nstatic int safexcel_sha3_224_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_sha3_224_init(req) ?: safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstatic int safexcel_sha3_export(struct ahash_request *req, void *out)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback = true;\n\treturn safexcel_sha3_fbcheck(req) ?: crypto_ahash_export(subreq, out);\n}\n\nstatic int safexcel_sha3_import(struct ahash_request *req, const void *in)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct ahash_request *subreq = ahash_request_ctx_dma(req);\n\n\tctx->do_fallback = true;\n\treturn safexcel_sha3_fbcheck(req) ?: crypto_ahash_import(subreq, in);\n\t\n}\n\nstatic int safexcel_sha3_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tsafexcel_ahash_cra_init(tfm);\n\n\t \n\tctx->fback = crypto_alloc_ahash(crypto_tfm_alg_name(tfm), 0,\n\t\t\t\t\tCRYPTO_ALG_ASYNC |\n\t\t\t\t\tCRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fback))\n\t\treturn PTR_ERR(ctx->fback);\n\n\t \n\tcrypto_hash_alg_common(ahash)->statesize =\n\t\tcrypto_ahash_statesize(ctx->fback);\n\tcrypto_ahash_set_reqsize_dma(\n\t\tahash, max(sizeof(struct safexcel_ahash_req),\n\t\t\t   sizeof(struct ahash_request) +\n\t\t\t   crypto_ahash_reqsize(ctx->fback)));\n\treturn 0;\n}\n\nstatic void safexcel_sha3_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->fback);\n\tsafexcel_ahash_cra_exit(tfm);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha3_224 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha3_224_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_sha3_224_digest,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_224_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha3-224\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha3-224\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_224_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_sha3_cra_init,\n\t\t\t\t.cra_exit = safexcel_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha3_256_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;\n\treq->digest = CONTEXT_CONTROL_DIGEST_INITIAL;\n\treq->state_sz = SHA3_256_DIGEST_SIZE;\n\treq->digest_sz = SHA3_256_DIGEST_SIZE;\n\treq->block_sz = SHA3_256_BLOCK_SIZE;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_sha3_256_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_sha3_256_init(req) ?: safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha3_256 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha3_256_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_sha3_256_digest,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_256_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha3-256\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha3-256\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_256_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_sha3_cra_init,\n\t\t\t\t.cra_exit = safexcel_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha3_384_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;\n\treq->digest = CONTEXT_CONTROL_DIGEST_INITIAL;\n\treq->state_sz = SHA3_384_DIGEST_SIZE;\n\treq->digest_sz = SHA3_384_DIGEST_SIZE;\n\treq->block_sz = SHA3_384_BLOCK_SIZE;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_sha3_384_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_sha3_384_init(req) ?: safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha3_384 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha3_384_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_sha3_384_digest,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_384_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha3-384\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha3-384\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_384_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_sha3_cra_init,\n\t\t\t\t.cra_exit = safexcel_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_sha3_512_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;\n\treq->digest = CONTEXT_CONTROL_DIGEST_INITIAL;\n\treq->state_sz = SHA3_512_DIGEST_SIZE;\n\treq->digest_sz = SHA3_512_DIGEST_SIZE;\n\treq->block_sz = SHA3_512_BLOCK_SIZE;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_sha3_512_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_sha3_512_init(req) ?: safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstruct safexcel_alg_template safexcel_alg_sha3_512 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_sha3_512_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_sha3_512_digest,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_512_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha3-512\",\n\t\t\t\t.cra_driver_name = \"safexcel-sha3-512\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_512_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_sha3_cra_init,\n\t\t\t\t.cra_exit = safexcel_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha3_cra_init(struct crypto_tfm *tfm, const char *alg)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tint ret;\n\n\tret = safexcel_sha3_cra_init(tfm);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tctx->shpre = crypto_alloc_shash(alg, 0, CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->shpre))\n\t\treturn PTR_ERR(ctx->shpre);\n\n\tctx->shdesc = kmalloc(sizeof(*ctx->shdesc) +\n\t\t\t      crypto_shash_descsize(ctx->shpre), GFP_KERNEL);\n\tif (!ctx->shdesc) {\n\t\tcrypto_free_shash(ctx->shpre);\n\t\treturn -ENOMEM;\n\t}\n\tctx->shdesc->tfm = ctx->shpre;\n\treturn 0;\n}\n\nstatic void safexcel_hmac_sha3_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->fback);\n\tcrypto_free_shash(ctx->shpre);\n\tkfree(ctx->shdesc);\n\tsafexcel_ahash_cra_exit(tfm);\n}\n\nstatic int safexcel_hmac_sha3_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tint ret = 0;\n\n\tif (keylen > crypto_ahash_blocksize(tfm)) {\n\t\t \n\t\tret = crypto_shash_digest(ctx->shdesc, key, keylen,\n\t\t\t\t\t  ctx->base.ipad.byte);\n\t\tkeylen = crypto_shash_digestsize(ctx->shpre);\n\n\t\t \n\t\tif (keylen > crypto_ahash_blocksize(tfm) / 2)\n\t\t\t \n\t\t\tmemmove(&ctx->base.opad,\n\t\t\t\tctx->base.ipad.byte +\n\t\t\t\t\tcrypto_ahash_blocksize(tfm) / 2,\n\t\t\t\tkeylen - crypto_ahash_blocksize(tfm) / 2);\n\t} else {\n\t\t \n\t\tif (keylen <= crypto_ahash_blocksize(tfm) / 2) {\n\t\t\tmemcpy(&ctx->base.ipad, key, keylen);\n\t\t} else {\n\t\t\tmemcpy(&ctx->base.ipad, key,\n\t\t\t       crypto_ahash_blocksize(tfm) / 2);\n\t\t\tmemcpy(&ctx->base.opad,\n\t\t\t       key + crypto_ahash_blocksize(tfm) / 2,\n\t\t\t       keylen - crypto_ahash_blocksize(tfm) / 2);\n\t\t}\n\t}\n\n\t \n\tif (keylen <= crypto_ahash_blocksize(tfm) / 2) {\n\t\tmemset(ctx->base.ipad.byte + keylen, 0,\n\t\t       crypto_ahash_blocksize(tfm) / 2 - keylen);\n\t\tmemset(&ctx->base.opad, 0, crypto_ahash_blocksize(tfm) / 2);\n\t} else {\n\t\tmemset(ctx->base.opad.byte + keylen -\n\t\t       crypto_ahash_blocksize(tfm) / 2, 0,\n\t\t       crypto_ahash_blocksize(tfm) - keylen);\n\t}\n\n\t \n\tctx->fb_do_setkey = true;\n\treturn ret;\n}\n\nstatic int safexcel_hmac_sha3_224_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA3_224_BLOCK_SIZE / 2);\n\t \n\treq->len\t= SHA3_224_BLOCK_SIZE;\n\treq->processed\t= SHA3_224_BLOCK_SIZE;\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;\n\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\treq->state_sz = SHA3_224_BLOCK_SIZE / 2;\n\treq->digest_sz = SHA3_224_DIGEST_SIZE;\n\treq->block_sz = SHA3_224_BLOCK_SIZE;\n\treq->hmac = true;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha3_224_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_hmac_sha3_224_init(req) ?:\n\t\t       safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstatic int safexcel_hmac_sha3_224_cra_init(struct crypto_tfm *tfm)\n{\n\treturn safexcel_hmac_sha3_cra_init(tfm, \"sha3-224\");\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha3_224 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha3_224_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_hmac_sha3_224_digest,\n\t\t.setkey = safexcel_hmac_sha3_setkey,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_224_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha3-224)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha3-224\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_224_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_hmac_sha3_224_cra_init,\n\t\t\t\t.cra_exit = safexcel_hmac_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha3_256_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA3_256_BLOCK_SIZE / 2);\n\t \n\treq->len\t= SHA3_256_BLOCK_SIZE;\n\treq->processed\t= SHA3_256_BLOCK_SIZE;\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;\n\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\treq->state_sz = SHA3_256_BLOCK_SIZE / 2;\n\treq->digest_sz = SHA3_256_DIGEST_SIZE;\n\treq->block_sz = SHA3_256_BLOCK_SIZE;\n\treq->hmac = true;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha3_256_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_hmac_sha3_256_init(req) ?:\n\t\t       safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstatic int safexcel_hmac_sha3_256_cra_init(struct crypto_tfm *tfm)\n{\n\treturn safexcel_hmac_sha3_cra_init(tfm, \"sha3-256\");\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha3_256 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha3_256_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_hmac_sha3_256_digest,\n\t\t.setkey = safexcel_hmac_sha3_setkey,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_256_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha3-256)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha3-256\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_256_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_hmac_sha3_256_cra_init,\n\t\t\t\t.cra_exit = safexcel_hmac_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha3_384_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA3_384_BLOCK_SIZE / 2);\n\t \n\treq->len\t= SHA3_384_BLOCK_SIZE;\n\treq->processed\t= SHA3_384_BLOCK_SIZE;\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;\n\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\treq->state_sz = SHA3_384_BLOCK_SIZE / 2;\n\treq->digest_sz = SHA3_384_DIGEST_SIZE;\n\treq->block_sz = SHA3_384_BLOCK_SIZE;\n\treq->hmac = true;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha3_384_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_hmac_sha3_384_init(req) ?:\n\t\t       safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstatic int safexcel_hmac_sha3_384_cra_init(struct crypto_tfm *tfm)\n{\n\treturn safexcel_hmac_sha3_cra_init(tfm, \"sha3-384\");\n}\n\nstruct safexcel_alg_template safexcel_alg_hmac_sha3_384 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha3_384_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_hmac_sha3_384_digest,\n\t\t.setkey = safexcel_hmac_sha3_setkey,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_384_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha3-384)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha3-384\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_384_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_hmac_sha3_384_cra_init,\n\t\t\t\t.cra_exit = safexcel_hmac_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n\nstatic int safexcel_hmac_sha3_512_init(struct ahash_request *areq)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tstruct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct safexcel_ahash_req *req = ahash_request_ctx_dma(areq);\n\n\tmemset(req, 0, sizeof(*req));\n\n\t \n\tmemcpy(req->state, &ctx->base.ipad, SHA3_512_BLOCK_SIZE / 2);\n\t \n\treq->len\t= SHA3_512_BLOCK_SIZE;\n\treq->processed\t= SHA3_512_BLOCK_SIZE;\n\tctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;\n\treq->digest = CONTEXT_CONTROL_DIGEST_HMAC;\n\treq->state_sz = SHA3_512_BLOCK_SIZE / 2;\n\treq->digest_sz = SHA3_512_DIGEST_SIZE;\n\treq->block_sz = SHA3_512_BLOCK_SIZE;\n\treq->hmac = true;\n\tctx->do_fallback = false;\n\tctx->fb_init_done = false;\n\treturn 0;\n}\n\nstatic int safexcel_hmac_sha3_512_digest(struct ahash_request *req)\n{\n\tif (req->nbytes)\n\t\treturn safexcel_hmac_sha3_512_init(req) ?:\n\t\t       safexcel_ahash_finup(req);\n\n\t \n\treturn safexcel_sha3_digest_fallback(req);\n}\n\nstatic int safexcel_hmac_sha3_512_cra_init(struct crypto_tfm *tfm)\n{\n\treturn safexcel_hmac_sha3_cra_init(tfm, \"sha3-512\");\n}\nstruct safexcel_alg_template safexcel_alg_hmac_sha3_512 = {\n\t.type = SAFEXCEL_ALG_TYPE_AHASH,\n\t.algo_mask = SAFEXCEL_ALG_SHA3,\n\t.alg.ahash = {\n\t\t.init = safexcel_hmac_sha3_512_init,\n\t\t.update = safexcel_sha3_update,\n\t\t.final = safexcel_sha3_final,\n\t\t.finup = safexcel_sha3_finup,\n\t\t.digest = safexcel_hmac_sha3_512_digest,\n\t\t.setkey = safexcel_hmac_sha3_setkey,\n\t\t.export = safexcel_sha3_export,\n\t\t.import = safexcel_sha3_import,\n\t\t.halg = {\n\t\t\t.digestsize = SHA3_512_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct safexcel_ahash_export_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"hmac(sha3-512)\",\n\t\t\t\t.cra_driver_name = \"safexcel-hmac-sha3-512\",\n\t\t\t\t.cra_priority = SAFEXCEL_CRA_PRIORITY,\n\t\t\t\t.cra_flags = CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA3_512_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),\n\t\t\t\t.cra_init = safexcel_hmac_sha3_512_cra_init,\n\t\t\t\t.cra_exit = safexcel_hmac_sha3_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t},\n\t\t},\n\t},\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}