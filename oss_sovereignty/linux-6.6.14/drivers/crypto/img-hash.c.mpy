{
  "module_name": "img-hash.c",
  "hash_id": "abbbb3eea04b7fb8b2479bd0e30908a4dce91d734acfd6d8a7ea907d0dc317a9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/img-hash.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/mod_devicetable.h>\n#include <linux/platform_device.h>\n#include <linux/scatterlist.h>\n\n#include <crypto/internal/hash.h>\n#include <crypto/md5.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n\n#define CR_RESET\t\t\t0\n#define CR_RESET_SET\t\t\t1\n#define CR_RESET_UNSET\t\t\t0\n\n#define CR_MESSAGE_LENGTH_H\t\t0x4\n#define CR_MESSAGE_LENGTH_L\t\t0x8\n\n#define CR_CONTROL\t\t\t0xc\n#define CR_CONTROL_BYTE_ORDER_3210\t0\n#define CR_CONTROL_BYTE_ORDER_0123\t1\n#define CR_CONTROL_BYTE_ORDER_2310\t2\n#define CR_CONTROL_BYTE_ORDER_1032\t3\n#define CR_CONTROL_BYTE_ORDER_SHIFT\t8\n#define CR_CONTROL_ALGO_MD5\t0\n#define CR_CONTROL_ALGO_SHA1\t1\n#define CR_CONTROL_ALGO_SHA224\t2\n#define CR_CONTROL_ALGO_SHA256\t3\n\n#define CR_INTSTAT\t\t\t0x10\n#define CR_INTENAB\t\t\t0x14\n#define CR_INTCLEAR\t\t\t0x18\n#define CR_INT_RESULTS_AVAILABLE\tBIT(0)\n#define CR_INT_NEW_RESULTS_SET\t\tBIT(1)\n#define CR_INT_RESULT_READ_ERR\t\tBIT(2)\n#define CR_INT_MESSAGE_WRITE_ERROR\tBIT(3)\n#define CR_INT_STATUS\t\t\tBIT(8)\n\n#define CR_RESULT_QUEUE\t\t0x1c\n#define CR_RSD0\t\t\t\t0x40\n#define CR_CORE_REV\t\t\t0x50\n#define CR_CORE_DES1\t\t0x60\n#define CR_CORE_DES2\t\t0x70\n\n#define DRIVER_FLAGS_BUSY\t\tBIT(0)\n#define DRIVER_FLAGS_FINAL\t\tBIT(1)\n#define DRIVER_FLAGS_DMA_ACTIVE\t\tBIT(2)\n#define DRIVER_FLAGS_OUTPUT_READY\tBIT(3)\n#define DRIVER_FLAGS_INIT\t\tBIT(4)\n#define DRIVER_FLAGS_CPU\t\tBIT(5)\n#define DRIVER_FLAGS_DMA_READY\t\tBIT(6)\n#define DRIVER_FLAGS_ERROR\t\tBIT(7)\n#define DRIVER_FLAGS_SG\t\t\tBIT(8)\n#define DRIVER_FLAGS_SHA1\t\tBIT(18)\n#define DRIVER_FLAGS_SHA224\t\tBIT(19)\n#define DRIVER_FLAGS_SHA256\t\tBIT(20)\n#define DRIVER_FLAGS_MD5\t\tBIT(21)\n\n#define IMG_HASH_QUEUE_LENGTH\t\t20\n#define IMG_HASH_DMA_BURST\t\t4\n#define IMG_HASH_DMA_THRESHOLD\t\t64\n\n#ifdef __LITTLE_ENDIAN\n#define IMG_HASH_BYTE_ORDER\t\tCR_CONTROL_BYTE_ORDER_3210\n#else\n#define IMG_HASH_BYTE_ORDER\t\tCR_CONTROL_BYTE_ORDER_0123\n#endif\n\nstruct img_hash_dev;\n\nstruct img_hash_request_ctx {\n\tstruct img_hash_dev\t*hdev;\n\tu8 digest[SHA256_DIGEST_SIZE] __aligned(sizeof(u32));\n\tunsigned long\t\tflags;\n\tsize_t\t\t\tdigsize;\n\n\tdma_addr_t\t\tdma_addr;\n\tsize_t\t\t\tdma_ct;\n\n\t \n\tstruct scatterlist\t*sgfirst;\n\t \n\tstruct scatterlist\t*sg;\n\tsize_t\t\t\tnents;\n\tsize_t\t\t\toffset;\n\tunsigned int\t\ttotal;\n\tsize_t\t\t\tsent;\n\n\tunsigned long\t\top;\n\n\tsize_t\t\t\tbufcnt;\n\tstruct ahash_request\tfallback_req;\n\n\t \n\tu8 buffer[] __aligned(sizeof(u32));\n};\n\nstruct img_hash_ctx {\n\tstruct img_hash_dev\t*hdev;\n\tunsigned long\t\tflags;\n\tstruct crypto_ahash\t*fallback;\n};\n\nstruct img_hash_dev {\n\tstruct list_head\tlist;\n\tstruct device\t\t*dev;\n\tstruct clk\t\t*hash_clk;\n\tstruct clk\t\t*sys_clk;\n\tvoid __iomem\t\t*io_base;\n\n\tphys_addr_t\t\tbus_addr;\n\tvoid __iomem\t\t*cpu_addr;\n\n\tspinlock_t\t\tlock;\n\tint\t\t\terr;\n\tstruct tasklet_struct\tdone_task;\n\tstruct tasklet_struct\tdma_task;\n\n\tunsigned long\t\tflags;\n\tstruct crypto_queue\tqueue;\n\tstruct ahash_request\t*req;\n\n\tstruct dma_chan\t\t*dma_lch;\n};\n\nstruct img_hash_drv {\n\tstruct list_head dev_list;\n\tspinlock_t lock;\n};\n\nstatic struct img_hash_drv img_hash = {\n\t.dev_list = LIST_HEAD_INIT(img_hash.dev_list),\n\t.lock = __SPIN_LOCK_UNLOCKED(img_hash.lock),\n};\n\nstatic inline u32 img_hash_read(struct img_hash_dev *hdev, u32 offset)\n{\n\treturn readl_relaxed(hdev->io_base + offset);\n}\n\nstatic inline void img_hash_write(struct img_hash_dev *hdev,\n\t\t\t\t  u32 offset, u32 value)\n{\n\twritel_relaxed(value, hdev->io_base + offset);\n}\n\nstatic inline __be32 img_hash_read_result_queue(struct img_hash_dev *hdev)\n{\n\treturn cpu_to_be32(img_hash_read(hdev, CR_RESULT_QUEUE));\n}\n\nstatic void img_hash_start(struct img_hash_dev *hdev, bool dma)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\tu32 cr = IMG_HASH_BYTE_ORDER << CR_CONTROL_BYTE_ORDER_SHIFT;\n\n\tif (ctx->flags & DRIVER_FLAGS_MD5)\n\t\tcr |= CR_CONTROL_ALGO_MD5;\n\telse if (ctx->flags & DRIVER_FLAGS_SHA1)\n\t\tcr |= CR_CONTROL_ALGO_SHA1;\n\telse if (ctx->flags & DRIVER_FLAGS_SHA224)\n\t\tcr |= CR_CONTROL_ALGO_SHA224;\n\telse if (ctx->flags & DRIVER_FLAGS_SHA256)\n\t\tcr |= CR_CONTROL_ALGO_SHA256;\n\tdev_dbg(hdev->dev, \"Starting hash process\\n\");\n\timg_hash_write(hdev, CR_CONTROL, cr);\n\n\t \n\tif (!dma)\n\t\timg_hash_read(hdev, CR_CONTROL);\n}\n\nstatic int img_hash_xmit_cpu(struct img_hash_dev *hdev, const u8 *buf,\n\t\t\t     size_t length, int final)\n{\n\tu32 count, len32;\n\tconst u32 *buffer = (const u32 *)buf;\n\n\tdev_dbg(hdev->dev, \"xmit_cpu:  length: %zu bytes\\n\", length);\n\n\tif (final)\n\t\thdev->flags |= DRIVER_FLAGS_FINAL;\n\n\tlen32 = DIV_ROUND_UP(length, sizeof(u32));\n\n\tfor (count = 0; count < len32; count++)\n\t\twritel_relaxed(buffer[count], hdev->cpu_addr);\n\n\treturn -EINPROGRESS;\n}\n\nstatic void img_hash_dma_callback(void *data)\n{\n\tstruct img_hash_dev *hdev = data;\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\n\tif (ctx->bufcnt) {\n\t\timg_hash_xmit_cpu(hdev, ctx->buffer, ctx->bufcnt, 0);\n\t\tctx->bufcnt = 0;\n\t}\n\tif (ctx->sg)\n\t\ttasklet_schedule(&hdev->dma_task);\n}\n\nstatic int img_hash_xmit_dma(struct img_hash_dev *hdev, struct scatterlist *sg)\n{\n\tstruct dma_async_tx_descriptor *desc;\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\n\tctx->dma_ct = dma_map_sg(hdev->dev, sg, 1, DMA_TO_DEVICE);\n\tif (ctx->dma_ct == 0) {\n\t\tdev_err(hdev->dev, \"Invalid DMA sg\\n\");\n\t\thdev->err = -EINVAL;\n\t\treturn -EINVAL;\n\t}\n\n\tdesc = dmaengine_prep_slave_sg(hdev->dma_lch,\n\t\t\t\t       sg,\n\t\t\t\t       ctx->dma_ct,\n\t\t\t\t       DMA_MEM_TO_DEV,\n\t\t\t\t       DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\n\tif (!desc) {\n\t\tdev_err(hdev->dev, \"Null DMA descriptor\\n\");\n\t\thdev->err = -EINVAL;\n\t\tdma_unmap_sg(hdev->dev, sg, 1, DMA_TO_DEVICE);\n\t\treturn -EINVAL;\n\t}\n\tdesc->callback = img_hash_dma_callback;\n\tdesc->callback_param = hdev;\n\tdmaengine_submit(desc);\n\tdma_async_issue_pending(hdev->dma_lch);\n\n\treturn 0;\n}\n\nstatic int img_hash_write_via_cpu(struct img_hash_dev *hdev)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\n\tctx->bufcnt = sg_copy_to_buffer(hdev->req->src, sg_nents(ctx->sg),\n\t\t\t\t\tctx->buffer, hdev->req->nbytes);\n\n\tctx->total = hdev->req->nbytes;\n\tctx->bufcnt = 0;\n\n\thdev->flags |= (DRIVER_FLAGS_CPU | DRIVER_FLAGS_FINAL);\n\n\timg_hash_start(hdev, false);\n\n\treturn img_hash_xmit_cpu(hdev, ctx->buffer, ctx->total, 1);\n}\n\nstatic int img_hash_finish(struct ahash_request *req)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\n\n\tif (!req->result)\n\t\treturn -EINVAL;\n\n\tmemcpy(req->result, ctx->digest, ctx->digsize);\n\n\treturn 0;\n}\n\nstatic void img_hash_copy_hash(struct ahash_request *req)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\n\t__be32 *hash = (__be32 *)ctx->digest;\n\tint i;\n\n\tfor (i = (ctx->digsize / sizeof(*hash)) - 1; i >= 0; i--)\n\t\thash[i] = img_hash_read_result_queue(ctx->hdev);\n}\n\nstatic void img_hash_finish_req(struct ahash_request *req, int err)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\n\tstruct img_hash_dev *hdev =  ctx->hdev;\n\n\tif (!err) {\n\t\timg_hash_copy_hash(req);\n\t\tif (DRIVER_FLAGS_FINAL & hdev->flags)\n\t\t\terr = img_hash_finish(req);\n\t} else {\n\t\tdev_warn(hdev->dev, \"Hash failed with error %d\\n\", err);\n\t\tctx->flags |= DRIVER_FLAGS_ERROR;\n\t}\n\n\thdev->flags &= ~(DRIVER_FLAGS_DMA_READY | DRIVER_FLAGS_OUTPUT_READY |\n\t\tDRIVER_FLAGS_CPU | DRIVER_FLAGS_BUSY | DRIVER_FLAGS_FINAL);\n\n\tif (req->base.complete)\n\t\tahash_request_complete(req, err);\n}\n\nstatic int img_hash_write_via_dma(struct img_hash_dev *hdev)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\n\timg_hash_start(hdev, true);\n\n\tdev_dbg(hdev->dev, \"xmit dma size: %d\\n\", ctx->total);\n\n\tif (!ctx->total)\n\t\thdev->flags |= DRIVER_FLAGS_FINAL;\n\n\thdev->flags |= DRIVER_FLAGS_DMA_ACTIVE | DRIVER_FLAGS_FINAL;\n\n\ttasklet_schedule(&hdev->dma_task);\n\n\treturn -EINPROGRESS;\n}\n\nstatic int img_hash_dma_init(struct img_hash_dev *hdev)\n{\n\tstruct dma_slave_config dma_conf;\n\tint err;\n\n\thdev->dma_lch = dma_request_chan(hdev->dev, \"tx\");\n\tif (IS_ERR(hdev->dma_lch)) {\n\t\tdev_err(hdev->dev, \"Couldn't acquire a slave DMA channel.\\n\");\n\t\treturn PTR_ERR(hdev->dma_lch);\n\t}\n\tdma_conf.direction = DMA_MEM_TO_DEV;\n\tdma_conf.dst_addr = hdev->bus_addr;\n\tdma_conf.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\tdma_conf.dst_maxburst = IMG_HASH_DMA_BURST;\n\tdma_conf.device_fc = false;\n\n\terr = dmaengine_slave_config(hdev->dma_lch,  &dma_conf);\n\tif (err) {\n\t\tdev_err(hdev->dev, \"Couldn't configure DMA slave.\\n\");\n\t\tdma_release_channel(hdev->dma_lch);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void img_hash_dma_task(unsigned long d)\n{\n\tstruct img_hash_dev *hdev = (struct img_hash_dev *)d;\n\tstruct img_hash_request_ctx *ctx;\n\tu8 *addr;\n\tsize_t nbytes, bleft, wsend, len, tbc;\n\tstruct scatterlist tsg;\n\n\tif (!hdev->req)\n\t\treturn;\n\n\tctx = ahash_request_ctx(hdev->req);\n\tif (!ctx->sg)\n\t\treturn;\n\n\taddr = sg_virt(ctx->sg);\n\tnbytes = ctx->sg->length - ctx->offset;\n\n\t \n\n\tbleft = nbytes % 4;\n\twsend = (nbytes / 4);\n\n\tif (wsend) {\n\t\tsg_init_one(&tsg, addr + ctx->offset, wsend * 4);\n\t\tif (img_hash_xmit_dma(hdev, &tsg)) {\n\t\t\tdev_err(hdev->dev, \"DMA failed, falling back to CPU\");\n\t\t\tctx->flags |= DRIVER_FLAGS_CPU;\n\t\t\thdev->err = 0;\n\t\t\timg_hash_xmit_cpu(hdev, addr + ctx->offset,\n\t\t\t\t\t  wsend * 4, 0);\n\t\t\tctx->sent += wsend * 4;\n\t\t\twsend = 0;\n\t\t} else {\n\t\t\tctx->sent += wsend * 4;\n\t\t}\n\t}\n\n\tif (bleft) {\n\t\tctx->bufcnt = sg_pcopy_to_buffer(ctx->sgfirst, ctx->nents,\n\t\t\t\t\t\t ctx->buffer, bleft, ctx->sent);\n\t\ttbc = 0;\n\t\tctx->sg = sg_next(ctx->sg);\n\t\twhile (ctx->sg && (ctx->bufcnt < 4)) {\n\t\t\tlen = ctx->sg->length;\n\t\t\tif (likely(len > (4 - ctx->bufcnt)))\n\t\t\t\tlen = 4 - ctx->bufcnt;\n\t\t\ttbc = sg_pcopy_to_buffer(ctx->sgfirst, ctx->nents,\n\t\t\t\t\t\t ctx->buffer + ctx->bufcnt, len,\n\t\t\t\t\tctx->sent + ctx->bufcnt);\n\t\t\tctx->bufcnt += tbc;\n\t\t\tif (tbc >= ctx->sg->length) {\n\t\t\t\tctx->sg = sg_next(ctx->sg);\n\t\t\t\ttbc = 0;\n\t\t\t}\n\t\t}\n\n\t\tctx->sent += ctx->bufcnt;\n\t\tctx->offset = tbc;\n\n\t\tif (!wsend)\n\t\t\timg_hash_dma_callback(hdev);\n\t} else {\n\t\tctx->offset = 0;\n\t\tctx->sg = sg_next(ctx->sg);\n\t}\n}\n\nstatic int img_hash_write_via_dma_stop(struct img_hash_dev *hdev)\n{\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\n\n\tif (ctx->flags & DRIVER_FLAGS_SG)\n\t\tdma_unmap_sg(hdev->dev, ctx->sg, ctx->dma_ct, DMA_TO_DEVICE);\n\n\treturn 0;\n}\n\nstatic int img_hash_process_data(struct img_hash_dev *hdev)\n{\n\tstruct ahash_request *req = hdev->req;\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\n\tint err = 0;\n\n\tctx->bufcnt = 0;\n\n\tif (req->nbytes >= IMG_HASH_DMA_THRESHOLD) {\n\t\tdev_dbg(hdev->dev, \"process data request(%d bytes) using DMA\\n\",\n\t\t\treq->nbytes);\n\t\terr = img_hash_write_via_dma(hdev);\n\t} else {\n\t\tdev_dbg(hdev->dev, \"process data request(%d bytes) using CPU\\n\",\n\t\t\treq->nbytes);\n\t\terr = img_hash_write_via_cpu(hdev);\n\t}\n\treturn err;\n}\n\nstatic int img_hash_hw_init(struct img_hash_dev *hdev)\n{\n\tunsigned long long nbits;\n\tu32 u, l;\n\n\timg_hash_write(hdev, CR_RESET, CR_RESET_SET);\n\timg_hash_write(hdev, CR_RESET, CR_RESET_UNSET);\n\timg_hash_write(hdev, CR_INTENAB, CR_INT_NEW_RESULTS_SET);\n\n\tnbits = (u64)hdev->req->nbytes << 3;\n\tu = nbits >> 32;\n\tl = nbits;\n\timg_hash_write(hdev, CR_MESSAGE_LENGTH_H, u);\n\timg_hash_write(hdev, CR_MESSAGE_LENGTH_L, l);\n\n\tif (!(DRIVER_FLAGS_INIT & hdev->flags)) {\n\t\thdev->flags |= DRIVER_FLAGS_INIT;\n\t\thdev->err = 0;\n\t}\n\tdev_dbg(hdev->dev, \"hw initialized, nbits: %llx\\n\", nbits);\n\treturn 0;\n}\n\nstatic int img_hash_init(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags =\treq->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_init(&rctx->fallback_req);\n}\n\nstatic int img_hash_handle_queue(struct img_hash_dev *hdev,\n\t\t\t\t struct ahash_request *req)\n{\n\tstruct crypto_async_request *async_req, *backlog;\n\tstruct img_hash_request_ctx *ctx;\n\tunsigned long flags;\n\tint err = 0, res = 0;\n\n\tspin_lock_irqsave(&hdev->lock, flags);\n\n\tif (req)\n\t\tres = ahash_enqueue_request(&hdev->queue, req);\n\n\tif (DRIVER_FLAGS_BUSY & hdev->flags) {\n\t\tspin_unlock_irqrestore(&hdev->lock, flags);\n\t\treturn res;\n\t}\n\n\tbacklog = crypto_get_backlog(&hdev->queue);\n\tasync_req = crypto_dequeue_request(&hdev->queue);\n\tif (async_req)\n\t\thdev->flags |= DRIVER_FLAGS_BUSY;\n\n\tspin_unlock_irqrestore(&hdev->lock, flags);\n\n\tif (!async_req)\n\t\treturn res;\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\treq = ahash_request_cast(async_req);\n\thdev->req = req;\n\n\tctx = ahash_request_ctx(req);\n\n\tdev_info(hdev->dev, \"processing req, op: %lu, bytes: %d\\n\",\n\t\t ctx->op, req->nbytes);\n\n\terr = img_hash_hw_init(hdev);\n\n\tif (!err)\n\t\terr = img_hash_process_data(hdev);\n\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\timg_hash_finish_req(req, err);\n\t}\n\treturn res;\n}\n\nstatic int img_hash_update(struct ahash_request *req)\n{\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags = req->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\n\treturn crypto_ahash_update(&rctx->fallback_req);\n}\n\nstatic int img_hash_final(struct ahash_request *req)\n{\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags = req->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_final(&rctx->fallback_req);\n}\n\nstatic int img_hash_finup(struct ahash_request *req)\n{\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags = req->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\trctx->fallback_req.nbytes = req->nbytes;\n\trctx->fallback_req.src = req->src;\n\trctx->fallback_req.result = req->result;\n\n\treturn crypto_ahash_finup(&rctx->fallback_req);\n}\n\nstatic int img_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags = req->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_import(&rctx->fallback_req, in);\n}\n\nstatic int img_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\trctx->fallback_req.base.flags = req->base.flags\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_ahash_export(&rctx->fallback_req, out);\n}\n\nstatic int img_hash_digest(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct img_hash_ctx *tctx = crypto_ahash_ctx(tfm);\n\tstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\n\tstruct img_hash_dev *hdev = NULL;\n\tstruct img_hash_dev *tmp;\n\tint err;\n\n\tspin_lock(&img_hash.lock);\n\tif (!tctx->hdev) {\n\t\tlist_for_each_entry(tmp, &img_hash.dev_list, list) {\n\t\t\thdev = tmp;\n\t\t\tbreak;\n\t\t}\n\t\ttctx->hdev = hdev;\n\n\t} else {\n\t\thdev = tctx->hdev;\n\t}\n\n\tspin_unlock(&img_hash.lock);\n\tctx->hdev = hdev;\n\tctx->flags = 0;\n\tctx->digsize = crypto_ahash_digestsize(tfm);\n\n\tswitch (ctx->digsize) {\n\tcase SHA1_DIGEST_SIZE:\n\t\tctx->flags |= DRIVER_FLAGS_SHA1;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tctx->flags |= DRIVER_FLAGS_SHA256;\n\t\tbreak;\n\tcase SHA224_DIGEST_SIZE:\n\t\tctx->flags |= DRIVER_FLAGS_SHA224;\n\t\tbreak;\n\tcase MD5_DIGEST_SIZE:\n\t\tctx->flags |= DRIVER_FLAGS_MD5;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tctx->bufcnt = 0;\n\tctx->offset = 0;\n\tctx->sent = 0;\n\tctx->total = req->nbytes;\n\tctx->sg = req->src;\n\tctx->sgfirst = req->src;\n\tctx->nents = sg_nents(ctx->sg);\n\n\terr = img_hash_handle_queue(tctx->hdev, req);\n\n\treturn err;\n}\n\nstatic int img_hash_cra_init(struct crypto_tfm *tfm, const char *alg_name)\n{\n\tstruct img_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tctx->fallback = crypto_alloc_ahash(alg_name, 0,\n\t\t\t\t\t   CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fallback)) {\n\t\tpr_err(\"img_hash: Could not load fallback driver.\\n\");\n\t\treturn PTR_ERR(ctx->fallback);\n\t}\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct img_hash_request_ctx) +\n\t\t\t\t crypto_ahash_reqsize(ctx->fallback) +\n\t\t\t\t IMG_HASH_DMA_THRESHOLD);\n\n\treturn 0;\n}\n\nstatic int img_hash_cra_md5_init(struct crypto_tfm *tfm)\n{\n\treturn img_hash_cra_init(tfm, \"md5-generic\");\n}\n\nstatic int img_hash_cra_sha1_init(struct crypto_tfm *tfm)\n{\n\treturn img_hash_cra_init(tfm, \"sha1-generic\");\n}\n\nstatic int img_hash_cra_sha224_init(struct crypto_tfm *tfm)\n{\n\treturn img_hash_cra_init(tfm, \"sha224-generic\");\n}\n\nstatic int img_hash_cra_sha256_init(struct crypto_tfm *tfm)\n{\n\treturn img_hash_cra_init(tfm, \"sha256-generic\");\n}\n\nstatic void img_hash_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct img_hash_ctx *tctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(tctx->fallback);\n}\n\nstatic irqreturn_t img_irq_handler(int irq, void *dev_id)\n{\n\tstruct img_hash_dev *hdev = dev_id;\n\tu32 reg;\n\n\treg = img_hash_read(hdev, CR_INTSTAT);\n\timg_hash_write(hdev, CR_INTCLEAR, reg);\n\n\tif (reg & CR_INT_NEW_RESULTS_SET) {\n\t\tdev_dbg(hdev->dev, \"IRQ CR_INT_NEW_RESULTS_SET\\n\");\n\t\tif (DRIVER_FLAGS_BUSY & hdev->flags) {\n\t\t\thdev->flags |= DRIVER_FLAGS_OUTPUT_READY;\n\t\t\tif (!(DRIVER_FLAGS_CPU & hdev->flags))\n\t\t\t\thdev->flags |= DRIVER_FLAGS_DMA_READY;\n\t\t\ttasklet_schedule(&hdev->done_task);\n\t\t} else {\n\t\t\tdev_warn(hdev->dev,\n\t\t\t\t \"HASH interrupt when no active requests.\\n\");\n\t\t}\n\t} else if (reg & CR_INT_RESULTS_AVAILABLE) {\n\t\tdev_warn(hdev->dev,\n\t\t\t \"IRQ triggered before the hash had completed\\n\");\n\t} else if (reg & CR_INT_RESULT_READ_ERR) {\n\t\tdev_warn(hdev->dev,\n\t\t\t \"Attempt to read from an empty result queue\\n\");\n\t} else if (reg & CR_INT_MESSAGE_WRITE_ERROR) {\n\t\tdev_warn(hdev->dev,\n\t\t\t \"Data written before the hardware was configured\\n\");\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic struct ahash_alg img_algs[] = {\n\t{\n\t\t.init = img_hash_init,\n\t\t.update = img_hash_update,\n\t\t.final = img_hash_final,\n\t\t.finup = img_hash_finup,\n\t\t.export = img_hash_export,\n\t\t.import = img_hash_import,\n\t\t.digest = img_hash_digest,\n\t\t.halg = {\n\t\t\t.digestsize = MD5_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct md5_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"md5\",\n\t\t\t\t.cra_driver_name = \"img-md5\",\n\t\t\t\t.cra_priority = 300,\n\t\t\t\t.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC |\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = MD5_HMAC_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct img_hash_ctx),\n\t\t\t\t.cra_init = img_hash_cra_md5_init,\n\t\t\t\t.cra_exit = img_hash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.init = img_hash_init,\n\t\t.update = img_hash_update,\n\t\t.final = img_hash_final,\n\t\t.finup = img_hash_finup,\n\t\t.export = img_hash_export,\n\t\t.import = img_hash_import,\n\t\t.digest = img_hash_digest,\n\t\t.halg = {\n\t\t\t.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct sha1_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha1\",\n\t\t\t\t.cra_driver_name = \"img-sha1\",\n\t\t\t\t.cra_priority = 300,\n\t\t\t\t.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC |\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct img_hash_ctx),\n\t\t\t\t.cra_init = img_hash_cra_sha1_init,\n\t\t\t\t.cra_exit = img_hash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.init = img_hash_init,\n\t\t.update = img_hash_update,\n\t\t.final = img_hash_final,\n\t\t.finup = img_hash_finup,\n\t\t.export = img_hash_export,\n\t\t.import = img_hash_import,\n\t\t.digest = img_hash_digest,\n\t\t.halg = {\n\t\t\t.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct sha256_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha224\",\n\t\t\t\t.cra_driver_name = \"img-sha224\",\n\t\t\t\t.cra_priority = 300,\n\t\t\t\t.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC |\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA224_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct img_hash_ctx),\n\t\t\t\t.cra_init = img_hash_cra_sha224_init,\n\t\t\t\t.cra_exit = img_hash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.init = img_hash_init,\n\t\t.update = img_hash_update,\n\t\t.final = img_hash_final,\n\t\t.finup = img_hash_finup,\n\t\t.export = img_hash_export,\n\t\t.import = img_hash_import,\n\t\t.digest = img_hash_digest,\n\t\t.halg = {\n\t\t\t.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t.statesize = sizeof(struct sha256_state),\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"sha256\",\n\t\t\t\t.cra_driver_name = \"img-sha256\",\n\t\t\t\t.cra_priority = 300,\n\t\t\t\t.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC |\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize = sizeof(struct img_hash_ctx),\n\t\t\t\t.cra_init = img_hash_cra_sha256_init,\n\t\t\t\t.cra_exit = img_hash_cra_exit,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t}\n\t\t}\n\t}\n};\n\nstatic int img_register_algs(struct img_hash_dev *hdev)\n{\n\tint i, err;\n\n\tfor (i = 0; i < ARRAY_SIZE(img_algs); i++) {\n\t\terr = crypto_register_ahash(&img_algs[i]);\n\t\tif (err)\n\t\t\tgoto err_reg;\n\t}\n\treturn 0;\n\nerr_reg:\n\tfor (; i--; )\n\t\tcrypto_unregister_ahash(&img_algs[i]);\n\n\treturn err;\n}\n\nstatic int img_unregister_algs(struct img_hash_dev *hdev)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(img_algs); i++)\n\t\tcrypto_unregister_ahash(&img_algs[i]);\n\treturn 0;\n}\n\nstatic void img_hash_done_task(unsigned long data)\n{\n\tstruct img_hash_dev *hdev = (struct img_hash_dev *)data;\n\tint err = 0;\n\n\tif (hdev->err == -EINVAL) {\n\t\terr = hdev->err;\n\t\tgoto finish;\n\t}\n\n\tif (!(DRIVER_FLAGS_BUSY & hdev->flags)) {\n\t\timg_hash_handle_queue(hdev, NULL);\n\t\treturn;\n\t}\n\n\tif (DRIVER_FLAGS_CPU & hdev->flags) {\n\t\tif (DRIVER_FLAGS_OUTPUT_READY & hdev->flags) {\n\t\t\thdev->flags &= ~DRIVER_FLAGS_OUTPUT_READY;\n\t\t\tgoto finish;\n\t\t}\n\t} else if (DRIVER_FLAGS_DMA_READY & hdev->flags) {\n\t\tif (DRIVER_FLAGS_DMA_ACTIVE & hdev->flags) {\n\t\t\thdev->flags &= ~DRIVER_FLAGS_DMA_ACTIVE;\n\t\t\timg_hash_write_via_dma_stop(hdev);\n\t\t\tif (hdev->err) {\n\t\t\t\terr = hdev->err;\n\t\t\t\tgoto finish;\n\t\t\t}\n\t\t}\n\t\tif (DRIVER_FLAGS_OUTPUT_READY & hdev->flags) {\n\t\t\thdev->flags &= ~(DRIVER_FLAGS_DMA_READY |\n\t\t\t\t\tDRIVER_FLAGS_OUTPUT_READY);\n\t\t\tgoto finish;\n\t\t}\n\t}\n\treturn;\n\nfinish:\n\timg_hash_finish_req(hdev->req, err);\n}\n\nstatic const struct of_device_id img_hash_match[] __maybe_unused = {\n\t{ .compatible = \"img,hash-accelerator\" },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, img_hash_match);\n\nstatic int img_hash_probe(struct platform_device *pdev)\n{\n\tstruct img_hash_dev *hdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *hash_res;\n\tint\tirq;\n\tint err;\n\n\thdev = devm_kzalloc(dev, sizeof(*hdev), GFP_KERNEL);\n\tif (hdev == NULL)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&hdev->lock);\n\n\thdev->dev = dev;\n\n\tplatform_set_drvdata(pdev, hdev);\n\n\tINIT_LIST_HEAD(&hdev->list);\n\n\ttasklet_init(&hdev->done_task, img_hash_done_task, (unsigned long)hdev);\n\ttasklet_init(&hdev->dma_task, img_hash_dma_task, (unsigned long)hdev);\n\n\tcrypto_init_queue(&hdev->queue, IMG_HASH_QUEUE_LENGTH);\n\n\t \n\thdev->io_base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(hdev->io_base)) {\n\t\terr = PTR_ERR(hdev->io_base);\n\t\tgoto res_err;\n\t}\n\n\t \n\thdev->cpu_addr = devm_platform_get_and_ioremap_resource(pdev, 1, &hash_res);\n\tif (IS_ERR(hdev->cpu_addr)) {\n\t\terr = PTR_ERR(hdev->cpu_addr);\n\t\tgoto res_err;\n\t}\n\thdev->bus_addr = hash_res->start;\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0) {\n\t\terr = irq;\n\t\tgoto res_err;\n\t}\n\n\terr = devm_request_irq(dev, irq, img_irq_handler, 0,\n\t\t\t       dev_name(dev), hdev);\n\tif (err) {\n\t\tdev_err(dev, \"unable to request irq\\n\");\n\t\tgoto res_err;\n\t}\n\tdev_dbg(dev, \"using IRQ channel %d\\n\", irq);\n\n\thdev->hash_clk = devm_clk_get(&pdev->dev, \"hash\");\n\tif (IS_ERR(hdev->hash_clk)) {\n\t\tdev_err(dev, \"clock initialization failed.\\n\");\n\t\terr = PTR_ERR(hdev->hash_clk);\n\t\tgoto res_err;\n\t}\n\n\thdev->sys_clk = devm_clk_get(&pdev->dev, \"sys\");\n\tif (IS_ERR(hdev->sys_clk)) {\n\t\tdev_err(dev, \"clock initialization failed.\\n\");\n\t\terr = PTR_ERR(hdev->sys_clk);\n\t\tgoto res_err;\n\t}\n\n\terr = clk_prepare_enable(hdev->hash_clk);\n\tif (err)\n\t\tgoto res_err;\n\n\terr = clk_prepare_enable(hdev->sys_clk);\n\tif (err)\n\t\tgoto clk_err;\n\n\terr = img_hash_dma_init(hdev);\n\tif (err)\n\t\tgoto dma_err;\n\n\tdev_dbg(dev, \"using %s for DMA transfers\\n\",\n\t\tdma_chan_name(hdev->dma_lch));\n\n\tspin_lock(&img_hash.lock);\n\tlist_add_tail(&hdev->list, &img_hash.dev_list);\n\tspin_unlock(&img_hash.lock);\n\n\terr = img_register_algs(hdev);\n\tif (err)\n\t\tgoto err_algs;\n\tdev_info(dev, \"Img MD5/SHA1/SHA224/SHA256 Hardware accelerator initialized\\n\");\n\n\treturn 0;\n\nerr_algs:\n\tspin_lock(&img_hash.lock);\n\tlist_del(&hdev->list);\n\tspin_unlock(&img_hash.lock);\n\tdma_release_channel(hdev->dma_lch);\ndma_err:\n\tclk_disable_unprepare(hdev->sys_clk);\nclk_err:\n\tclk_disable_unprepare(hdev->hash_clk);\nres_err:\n\ttasklet_kill(&hdev->done_task);\n\ttasklet_kill(&hdev->dma_task);\n\n\treturn err;\n}\n\nstatic int img_hash_remove(struct platform_device *pdev)\n{\n\tstruct img_hash_dev *hdev;\n\n\thdev = platform_get_drvdata(pdev);\n\tspin_lock(&img_hash.lock);\n\tlist_del(&hdev->list);\n\tspin_unlock(&img_hash.lock);\n\n\timg_unregister_algs(hdev);\n\n\ttasklet_kill(&hdev->done_task);\n\ttasklet_kill(&hdev->dma_task);\n\n\tdma_release_channel(hdev->dma_lch);\n\n\tclk_disable_unprepare(hdev->hash_clk);\n\tclk_disable_unprepare(hdev->sys_clk);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int img_hash_suspend(struct device *dev)\n{\n\tstruct img_hash_dev *hdev = dev_get_drvdata(dev);\n\n\tclk_disable_unprepare(hdev->hash_clk);\n\tclk_disable_unprepare(hdev->sys_clk);\n\n\treturn 0;\n}\n\nstatic int img_hash_resume(struct device *dev)\n{\n\tstruct img_hash_dev *hdev = dev_get_drvdata(dev);\n\tint ret;\n\n\tret = clk_prepare_enable(hdev->hash_clk);\n\tif (ret)\n\t\treturn ret;\n\n\tret = clk_prepare_enable(hdev->sys_clk);\n\tif (ret) {\n\t\tclk_disable_unprepare(hdev->hash_clk);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif  \n\nstatic const struct dev_pm_ops img_hash_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(img_hash_suspend, img_hash_resume)\n};\n\nstatic struct platform_driver img_hash_driver = {\n\t.probe\t\t= img_hash_probe,\n\t.remove\t\t= img_hash_remove,\n\t.driver\t\t= {\n\t\t.name\t= \"img-hash-accelerator\",\n\t\t.pm\t= &img_hash_pm_ops,\n\t\t.of_match_table\t= img_hash_match,\n\t}\n};\nmodule_platform_driver(img_hash_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"Imgtec SHA1/224/256 & MD5 hw accelerator driver\");\nMODULE_AUTHOR(\"Will Thomas.\");\nMODULE_AUTHOR(\"James Hartley <james.hartley@imgtec.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}