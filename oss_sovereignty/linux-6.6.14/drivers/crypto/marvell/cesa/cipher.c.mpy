{
  "module_name": "cipher.c",
  "hash_id": "70e22e5c76488d1b2c42bf472e395e54352a8427d9635fc17c3fcc8c53ca2c4e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/marvell/cesa/cipher.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/internal/des.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n\n#include \"cesa.h\"\n\nstruct mv_cesa_des_ctx {\n\tstruct mv_cesa_ctx base;\n\tu8 key[DES_KEY_SIZE];\n};\n\nstruct mv_cesa_des3_ctx {\n\tstruct mv_cesa_ctx base;\n\tu8 key[DES3_EDE_KEY_SIZE];\n};\n\nstruct mv_cesa_aes_ctx {\n\tstruct mv_cesa_ctx base;\n\tstruct crypto_aes_ctx aes;\n};\n\nstruct mv_cesa_skcipher_dma_iter {\n\tstruct mv_cesa_dma_iter base;\n\tstruct mv_cesa_sg_dma_iter src;\n\tstruct mv_cesa_sg_dma_iter dst;\n};\n\nstatic inline void\nmv_cesa_skcipher_req_iter_init(struct mv_cesa_skcipher_dma_iter *iter,\n\t\t\t       struct skcipher_request *req)\n{\n\tmv_cesa_req_dma_iter_init(&iter->base, req->cryptlen);\n\tmv_cesa_sg_dma_iter_init(&iter->src, req->src, DMA_TO_DEVICE);\n\tmv_cesa_sg_dma_iter_init(&iter->dst, req->dst, DMA_FROM_DEVICE);\n}\n\nstatic inline bool\nmv_cesa_skcipher_req_iter_next_op(struct mv_cesa_skcipher_dma_iter *iter)\n{\n\titer->src.op_offset = 0;\n\titer->dst.op_offset = 0;\n\n\treturn mv_cesa_req_dma_iter_next_op(&iter->base);\n}\n\nstatic inline void\nmv_cesa_skcipher_dma_cleanup(struct skcipher_request *req)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\n\tif (req->dst != req->src) {\n\t\tdma_unmap_sg(cesa_dev->dev, req->dst, creq->dst_nents,\n\t\t\t     DMA_FROM_DEVICE);\n\t\tdma_unmap_sg(cesa_dev->dev, req->src, creq->src_nents,\n\t\t\t     DMA_TO_DEVICE);\n\t} else {\n\t\tdma_unmap_sg(cesa_dev->dev, req->src, creq->src_nents,\n\t\t\t     DMA_BIDIRECTIONAL);\n\t}\n\tmv_cesa_dma_cleanup(&creq->base);\n}\n\nstatic inline void mv_cesa_skcipher_cleanup(struct skcipher_request *req)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\n\tif (mv_cesa_req_get_type(&creq->base) == CESA_DMA_REQ)\n\t\tmv_cesa_skcipher_dma_cleanup(req);\n}\n\nstatic void mv_cesa_skcipher_std_step(struct skcipher_request *req)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_skcipher_std_req *sreq = &creq->std;\n\tstruct mv_cesa_engine *engine = creq->base.engine;\n\tsize_t  len = min_t(size_t, req->cryptlen - sreq->offset,\n\t\t\t    CESA_SA_SRAM_PAYLOAD_SIZE);\n\n\tmv_cesa_adjust_op(engine, &sreq->op);\n\tif (engine->pool)\n\t\tmemcpy(engine->sram_pool, &sreq->op, sizeof(sreq->op));\n\telse\n\t\tmemcpy_toio(engine->sram, &sreq->op, sizeof(sreq->op));\n\n\tlen = mv_cesa_sg_copy_to_sram(engine, req->src, creq->src_nents,\n\t\t\t\t      CESA_SA_DATA_SRAM_OFFSET, len,\n\t\t\t\t      sreq->offset);\n\n\tsreq->size = len;\n\tmv_cesa_set_crypt_op_len(&sreq->op, len);\n\n\t \n\tif (!sreq->skip_ctx) {\n\t\tif (engine->pool)\n\t\t\tmemcpy(engine->sram_pool, &sreq->op, sizeof(sreq->op));\n\t\telse\n\t\t\tmemcpy_toio(engine->sram, &sreq->op, sizeof(sreq->op));\n\t\tsreq->skip_ctx = true;\n\t} else if (engine->pool)\n\t\tmemcpy(engine->sram_pool, &sreq->op, sizeof(sreq->op.desc));\n\telse\n\t\tmemcpy_toio(engine->sram, &sreq->op, sizeof(sreq->op.desc));\n\n\tmv_cesa_set_int_mask(engine, CESA_SA_INT_ACCEL0_DONE);\n\twritel_relaxed(CESA_SA_CFG_PARA_DIS, engine->regs + CESA_SA_CFG);\n\tWARN_ON(readl(engine->regs + CESA_SA_CMD) &\n\t\tCESA_SA_CMD_EN_CESA_SA_ACCL0);\n\twritel(CESA_SA_CMD_EN_CESA_SA_ACCL0, engine->regs + CESA_SA_CMD);\n}\n\nstatic int mv_cesa_skcipher_std_process(struct skcipher_request *req,\n\t\t\t\t\tu32 status)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_skcipher_std_req *sreq = &creq->std;\n\tstruct mv_cesa_engine *engine = creq->base.engine;\n\tsize_t len;\n\n\tlen = mv_cesa_sg_copy_from_sram(engine, req->dst, creq->dst_nents,\n\t\t\t\t\tCESA_SA_DATA_SRAM_OFFSET, sreq->size,\n\t\t\t\t\tsreq->offset);\n\n\tsreq->offset += len;\n\tif (sreq->offset < req->cryptlen)\n\t\treturn -EINPROGRESS;\n\n\treturn 0;\n}\n\nstatic int mv_cesa_skcipher_process(struct crypto_async_request *req,\n\t\t\t\t    u32 status)\n{\n\tstruct skcipher_request *skreq = skcipher_request_cast(req);\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(skreq);\n\tstruct mv_cesa_req *basereq = &creq->base;\n\n\tif (mv_cesa_req_get_type(basereq) == CESA_STD_REQ)\n\t\treturn mv_cesa_skcipher_std_process(skreq, status);\n\n\treturn mv_cesa_dma_process(basereq, status);\n}\n\nstatic void mv_cesa_skcipher_step(struct crypto_async_request *req)\n{\n\tstruct skcipher_request *skreq = skcipher_request_cast(req);\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(skreq);\n\n\tif (mv_cesa_req_get_type(&creq->base) == CESA_DMA_REQ)\n\t\tmv_cesa_dma_step(&creq->base);\n\telse\n\t\tmv_cesa_skcipher_std_step(skreq);\n}\n\nstatic inline void\nmv_cesa_skcipher_dma_prepare(struct skcipher_request *req)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_req *basereq = &creq->base;\n\n\tmv_cesa_dma_prepare(basereq, basereq->engine);\n}\n\nstatic inline void\nmv_cesa_skcipher_std_prepare(struct skcipher_request *req)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_skcipher_std_req *sreq = &creq->std;\n\n\tsreq->size = 0;\n\tsreq->offset = 0;\n}\n\nstatic inline void mv_cesa_skcipher_prepare(struct crypto_async_request *req,\n\t\t\t\t\t    struct mv_cesa_engine *engine)\n{\n\tstruct skcipher_request *skreq = skcipher_request_cast(req);\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(skreq);\n\n\tcreq->base.engine = engine;\n\n\tif (mv_cesa_req_get_type(&creq->base) == CESA_DMA_REQ)\n\t\tmv_cesa_skcipher_dma_prepare(skreq);\n\telse\n\t\tmv_cesa_skcipher_std_prepare(skreq);\n}\n\nstatic inline void\nmv_cesa_skcipher_req_cleanup(struct crypto_async_request *req)\n{\n\tstruct skcipher_request *skreq = skcipher_request_cast(req);\n\n\tmv_cesa_skcipher_cleanup(skreq);\n}\n\nstatic void\nmv_cesa_skcipher_complete(struct crypto_async_request *req)\n{\n\tstruct skcipher_request *skreq = skcipher_request_cast(req);\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(skreq);\n\tstruct mv_cesa_engine *engine = creq->base.engine;\n\tunsigned int ivsize;\n\n\tatomic_sub(skreq->cryptlen, &engine->load);\n\tivsize = crypto_skcipher_ivsize(crypto_skcipher_reqtfm(skreq));\n\n\tif (mv_cesa_req_get_type(&creq->base) == CESA_DMA_REQ) {\n\t\tstruct mv_cesa_req *basereq;\n\n\t\tbasereq = &creq->base;\n\t\tmemcpy(skreq->iv, basereq->chain.last->op->ctx.skcipher.iv,\n\t\t       ivsize);\n\t} else if (engine->pool)\n\t\tmemcpy(skreq->iv,\n\t\t       engine->sram_pool + CESA_SA_CRYPT_IV_SRAM_OFFSET,\n\t\t       ivsize);\n\telse\n\t\tmemcpy_fromio(skreq->iv,\n\t\t\t      engine->sram + CESA_SA_CRYPT_IV_SRAM_OFFSET,\n\t\t\t      ivsize);\n}\n\nstatic const struct mv_cesa_req_ops mv_cesa_skcipher_req_ops = {\n\t.step = mv_cesa_skcipher_step,\n\t.process = mv_cesa_skcipher_process,\n\t.cleanup = mv_cesa_skcipher_req_cleanup,\n\t.complete = mv_cesa_skcipher_complete,\n};\n\nstatic void mv_cesa_skcipher_cra_exit(struct crypto_tfm *tfm)\n{\n\tvoid *ctx = crypto_tfm_ctx(tfm);\n\n\tmemzero_explicit(ctx, tfm->__crt_alg->cra_ctxsize);\n}\n\nstatic int mv_cesa_skcipher_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct mv_cesa_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tctx->ops = &mv_cesa_skcipher_req_ops;\n\n\tcrypto_skcipher_set_reqsize(__crypto_skcipher_cast(tfm),\n\t\t\t\t    sizeof(struct mv_cesa_skcipher_req));\n\n\treturn 0;\n}\n\nstatic int mv_cesa_aes_setkey(struct crypto_skcipher *cipher, const u8 *key,\n\t\t\t      unsigned int len)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(cipher);\n\tstruct mv_cesa_aes_ctx *ctx = crypto_tfm_ctx(tfm);\n\tint remaining;\n\tint offset;\n\tint ret;\n\tint i;\n\n\tret = aes_expandkey(&ctx->aes, key, len);\n\tif (ret)\n\t\treturn ret;\n\n\tremaining = (ctx->aes.key_length - 16) / 4;\n\toffset = ctx->aes.key_length + 24 - remaining;\n\tfor (i = 0; i < remaining; i++)\n\t\tctx->aes.key_dec[4 + i] = ctx->aes.key_enc[offset + i];\n\n\treturn 0;\n}\n\nstatic int mv_cesa_des_setkey(struct crypto_skcipher *cipher, const u8 *key,\n\t\t\t      unsigned int len)\n{\n\tstruct mv_cesa_des_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tint err;\n\n\terr = verify_skcipher_des_key(cipher, key);\n\tif (err)\n\t\treturn err;\n\n\tmemcpy(ctx->key, key, DES_KEY_SIZE);\n\n\treturn 0;\n}\n\nstatic int mv_cesa_des3_ede_setkey(struct crypto_skcipher *cipher,\n\t\t\t\t   const u8 *key, unsigned int len)\n{\n\tstruct mv_cesa_des3_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tint err;\n\n\terr = verify_skcipher_des3_key(cipher, key);\n\tif (err)\n\t\treturn err;\n\n\tmemcpy(ctx->key, key, DES3_EDE_KEY_SIZE);\n\n\treturn 0;\n}\n\nstatic int mv_cesa_skcipher_dma_req_init(struct skcipher_request *req,\n\t\t\t\t\t const struct mv_cesa_op_ctx *op_templ)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tgfp_t flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t      GFP_KERNEL : GFP_ATOMIC;\n\tstruct mv_cesa_req *basereq = &creq->base;\n\tstruct mv_cesa_skcipher_dma_iter iter;\n\tbool skip_ctx = false;\n\tint ret;\n\n\tbasereq->chain.first = NULL;\n\tbasereq->chain.last = NULL;\n\n\tif (req->src != req->dst) {\n\t\tret = dma_map_sg(cesa_dev->dev, req->src, creq->src_nents,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tif (!ret)\n\t\t\treturn -ENOMEM;\n\n\t\tret = dma_map_sg(cesa_dev->dev, req->dst, creq->dst_nents,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tif (!ret) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_unmap_src;\n\t\t}\n\t} else {\n\t\tret = dma_map_sg(cesa_dev->dev, req->src, creq->src_nents,\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tif (!ret)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tmv_cesa_tdma_desc_iter_init(&basereq->chain);\n\tmv_cesa_skcipher_req_iter_init(&iter, req);\n\n\tdo {\n\t\tstruct mv_cesa_op_ctx *op;\n\n\t\top = mv_cesa_dma_add_op(&basereq->chain, op_templ, skip_ctx,\n\t\t\t\t\tflags);\n\t\tif (IS_ERR(op)) {\n\t\t\tret = PTR_ERR(op);\n\t\t\tgoto err_free_tdma;\n\t\t}\n\t\tskip_ctx = true;\n\n\t\tmv_cesa_set_crypt_op_len(op, iter.base.op_len);\n\n\t\t \n\t\tret = mv_cesa_dma_add_op_transfers(&basereq->chain, &iter.base,\n\t\t\t\t\t\t   &iter.src, flags);\n\t\tif (ret)\n\t\t\tgoto err_free_tdma;\n\n\t\t \n\t\tret = mv_cesa_dma_add_dummy_launch(&basereq->chain, flags);\n\t\tif (ret)\n\t\t\tgoto err_free_tdma;\n\n\t\t \n\t\tret = mv_cesa_dma_add_op_transfers(&basereq->chain, &iter.base,\n\t\t\t\t\t\t   &iter.dst, flags);\n\t\tif (ret)\n\t\t\tgoto err_free_tdma;\n\n\t} while (mv_cesa_skcipher_req_iter_next_op(&iter));\n\n\t \n\tret = mv_cesa_dma_add_result_op(&basereq->chain,\n\t\t\t\t\tCESA_SA_CFG_SRAM_OFFSET,\n\t\t\t\t\tCESA_SA_DATA_SRAM_OFFSET,\n\t\t\t\t\tCESA_TDMA_SRC_IN_SRAM, flags);\n\n\tif (ret)\n\t\tgoto err_free_tdma;\n\n\tbasereq->chain.last->flags |= CESA_TDMA_END_OF_REQ;\n\n\treturn 0;\n\nerr_free_tdma:\n\tmv_cesa_dma_cleanup(basereq);\n\tif (req->dst != req->src)\n\t\tdma_unmap_sg(cesa_dev->dev, req->dst, creq->dst_nents,\n\t\t\t     DMA_FROM_DEVICE);\n\nerr_unmap_src:\n\tdma_unmap_sg(cesa_dev->dev, req->src, creq->src_nents,\n\t\t     req->dst != req->src ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL);\n\n\treturn ret;\n}\n\nstatic inline int\nmv_cesa_skcipher_std_req_init(struct skcipher_request *req,\n\t\t\t      const struct mv_cesa_op_ctx *op_templ)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_skcipher_std_req *sreq = &creq->std;\n\tstruct mv_cesa_req *basereq = &creq->base;\n\n\tsreq->op = *op_templ;\n\tsreq->skip_ctx = false;\n\tbasereq->chain.first = NULL;\n\tbasereq->chain.last = NULL;\n\n\treturn 0;\n}\n\nstatic int mv_cesa_skcipher_req_init(struct skcipher_request *req,\n\t\t\t\t     struct mv_cesa_op_ctx *tmpl)\n{\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tunsigned int blksize = crypto_skcipher_blocksize(tfm);\n\tint ret;\n\n\tif (!IS_ALIGNED(req->cryptlen, blksize))\n\t\treturn -EINVAL;\n\n\tcreq->src_nents = sg_nents_for_len(req->src, req->cryptlen);\n\tif (creq->src_nents < 0) {\n\t\tdev_err(cesa_dev->dev, \"Invalid number of src SG\");\n\t\treturn creq->src_nents;\n\t}\n\tcreq->dst_nents = sg_nents_for_len(req->dst, req->cryptlen);\n\tif (creq->dst_nents < 0) {\n\t\tdev_err(cesa_dev->dev, \"Invalid number of dst SG\");\n\t\treturn creq->dst_nents;\n\t}\n\n\tmv_cesa_update_op_cfg(tmpl, CESA_SA_DESC_CFG_OP_CRYPT_ONLY,\n\t\t\t      CESA_SA_DESC_CFG_OP_MSK);\n\n\tif (cesa_dev->caps->has_tdma)\n\t\tret = mv_cesa_skcipher_dma_req_init(req, tmpl);\n\telse\n\t\tret = mv_cesa_skcipher_std_req_init(req, tmpl);\n\n\treturn ret;\n}\n\nstatic int mv_cesa_skcipher_queue_req(struct skcipher_request *req,\n\t\t\t\t      struct mv_cesa_op_ctx *tmpl)\n{\n\tint ret;\n\tstruct mv_cesa_skcipher_req *creq = skcipher_request_ctx(req);\n\tstruct mv_cesa_engine *engine;\n\n\tret = mv_cesa_skcipher_req_init(req, tmpl);\n\tif (ret)\n\t\treturn ret;\n\n\tengine = mv_cesa_select_engine(req->cryptlen);\n\tmv_cesa_skcipher_prepare(&req->base, engine);\n\n\tret = mv_cesa_queue_req(&req->base, &creq->base);\n\n\tif (mv_cesa_req_needs_cleanup(&req->base, ret))\n\t\tmv_cesa_skcipher_cleanup(req);\n\n\treturn ret;\n}\n\nstatic int mv_cesa_des_op(struct skcipher_request *req,\n\t\t\t  struct mv_cesa_op_ctx *tmpl)\n{\n\tstruct mv_cesa_des_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\n\n\tmv_cesa_update_op_cfg(tmpl, CESA_SA_DESC_CFG_CRYPTM_DES,\n\t\t\t      CESA_SA_DESC_CFG_CRYPTM_MSK);\n\n\tmemcpy(tmpl->ctx.skcipher.key, ctx->key, DES_KEY_SIZE);\n\n\treturn mv_cesa_skcipher_queue_req(req, tmpl);\n}\n\nstatic int mv_cesa_ecb_des_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_des_op(req, &tmpl);\n}\n\nstatic int mv_cesa_ecb_des_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_des_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_ecb_des_alg = {\n\t.setkey = mv_cesa_des_setkey,\n\t.encrypt = mv_cesa_ecb_des_encrypt,\n\t.decrypt = mv_cesa_ecb_des_decrypt,\n\t.min_keysize = DES_KEY_SIZE,\n\t.max_keysize = DES_KEY_SIZE,\n\t.base = {\n\t\t.cra_name = \"ecb(des)\",\n\t\t.cra_driver_name = \"mv-ecb-des\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = DES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_des_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n\nstatic int mv_cesa_cbc_des_op(struct skcipher_request *req,\n\t\t\t      struct mv_cesa_op_ctx *tmpl)\n{\n\tmv_cesa_update_op_cfg(tmpl, CESA_SA_DESC_CFG_CRYPTCM_CBC,\n\t\t\t      CESA_SA_DESC_CFG_CRYPTCM_MSK);\n\n\tmemcpy(tmpl->ctx.skcipher.iv, req->iv, DES_BLOCK_SIZE);\n\n\treturn mv_cesa_des_op(req, tmpl);\n}\n\nstatic int mv_cesa_cbc_des_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl, CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_cbc_des_op(req, &tmpl);\n}\n\nstatic int mv_cesa_cbc_des_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl, CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_cbc_des_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_cbc_des_alg = {\n\t.setkey = mv_cesa_des_setkey,\n\t.encrypt = mv_cesa_cbc_des_encrypt,\n\t.decrypt = mv_cesa_cbc_des_decrypt,\n\t.min_keysize = DES_KEY_SIZE,\n\t.max_keysize = DES_KEY_SIZE,\n\t.ivsize = DES_BLOCK_SIZE,\n\t.base = {\n\t\t.cra_name = \"cbc(des)\",\n\t\t.cra_driver_name = \"mv-cbc-des\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = DES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_des_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n\nstatic int mv_cesa_des3_op(struct skcipher_request *req,\n\t\t\t   struct mv_cesa_op_ctx *tmpl)\n{\n\tstruct mv_cesa_des3_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\n\n\tmv_cesa_update_op_cfg(tmpl, CESA_SA_DESC_CFG_CRYPTM_3DES,\n\t\t\t      CESA_SA_DESC_CFG_CRYPTM_MSK);\n\n\tmemcpy(tmpl->ctx.skcipher.key, ctx->key, DES3_EDE_KEY_SIZE);\n\n\treturn mv_cesa_skcipher_queue_req(req, tmpl);\n}\n\nstatic int mv_cesa_ecb_des3_ede_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_3DES_EDE |\n\t\t\t   CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_des3_op(req, &tmpl);\n}\n\nstatic int mv_cesa_ecb_des3_ede_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_3DES_EDE |\n\t\t\t   CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_des3_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_ecb_des3_ede_alg = {\n\t.setkey = mv_cesa_des3_ede_setkey,\n\t.encrypt = mv_cesa_ecb_des3_ede_encrypt,\n\t.decrypt = mv_cesa_ecb_des3_ede_decrypt,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.base = {\n\t\t.cra_name = \"ecb(des3_ede)\",\n\t\t.cra_driver_name = \"mv-ecb-des3-ede\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_des3_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n\nstatic int mv_cesa_cbc_des3_op(struct skcipher_request *req,\n\t\t\t       struct mv_cesa_op_ctx *tmpl)\n{\n\tmemcpy(tmpl->ctx.skcipher.iv, req->iv, DES3_EDE_BLOCK_SIZE);\n\n\treturn mv_cesa_des3_op(req, tmpl);\n}\n\nstatic int mv_cesa_cbc_des3_ede_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_CBC |\n\t\t\t   CESA_SA_DESC_CFG_3DES_EDE |\n\t\t\t   CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_cbc_des3_op(req, &tmpl);\n}\n\nstatic int mv_cesa_cbc_des3_ede_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_CBC |\n\t\t\t   CESA_SA_DESC_CFG_3DES_EDE |\n\t\t\t   CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_cbc_des3_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_cbc_des3_ede_alg = {\n\t.setkey = mv_cesa_des3_ede_setkey,\n\t.encrypt = mv_cesa_cbc_des3_ede_encrypt,\n\t.decrypt = mv_cesa_cbc_des3_ede_decrypt,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.ivsize = DES3_EDE_BLOCK_SIZE,\n\t.base = {\n\t\t.cra_name = \"cbc(des3_ede)\",\n\t\t.cra_driver_name = \"mv-cbc-des3-ede\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_des3_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n\nstatic int mv_cesa_aes_op(struct skcipher_request *req,\n\t\t\t  struct mv_cesa_op_ctx *tmpl)\n{\n\tstruct mv_cesa_aes_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\n\tint i;\n\tu32 *key;\n\tu32 cfg;\n\n\tcfg = CESA_SA_DESC_CFG_CRYPTM_AES;\n\n\tif (mv_cesa_get_op_cfg(tmpl) & CESA_SA_DESC_CFG_DIR_DEC)\n\t\tkey = ctx->aes.key_dec;\n\telse\n\t\tkey = ctx->aes.key_enc;\n\n\tfor (i = 0; i < ctx->aes.key_length / sizeof(u32); i++)\n\t\ttmpl->ctx.skcipher.key[i] = cpu_to_le32(key[i]);\n\n\tif (ctx->aes.key_length == 24)\n\t\tcfg |= CESA_SA_DESC_CFG_AES_LEN_192;\n\telse if (ctx->aes.key_length == 32)\n\t\tcfg |= CESA_SA_DESC_CFG_AES_LEN_256;\n\n\tmv_cesa_update_op_cfg(tmpl, cfg,\n\t\t\t      CESA_SA_DESC_CFG_CRYPTM_MSK |\n\t\t\t      CESA_SA_DESC_CFG_AES_LEN_MSK);\n\n\treturn mv_cesa_skcipher_queue_req(req, tmpl);\n}\n\nstatic int mv_cesa_ecb_aes_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_aes_op(req, &tmpl);\n}\n\nstatic int mv_cesa_ecb_aes_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl,\n\t\t\t   CESA_SA_DESC_CFG_CRYPTCM_ECB |\n\t\t\t   CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_aes_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_ecb_aes_alg = {\n\t.setkey = mv_cesa_aes_setkey,\n\t.encrypt = mv_cesa_ecb_aes_encrypt,\n\t.decrypt = mv_cesa_ecb_aes_decrypt,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.base = {\n\t\t.cra_name = \"ecb(aes)\",\n\t\t.cra_driver_name = \"mv-ecb-aes\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_aes_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n\nstatic int mv_cesa_cbc_aes_op(struct skcipher_request *req,\n\t\t\t      struct mv_cesa_op_ctx *tmpl)\n{\n\tmv_cesa_update_op_cfg(tmpl, CESA_SA_DESC_CFG_CRYPTCM_CBC,\n\t\t\t      CESA_SA_DESC_CFG_CRYPTCM_MSK);\n\tmemcpy(tmpl->ctx.skcipher.iv, req->iv, AES_BLOCK_SIZE);\n\n\treturn mv_cesa_aes_op(req, tmpl);\n}\n\nstatic int mv_cesa_cbc_aes_encrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl, CESA_SA_DESC_CFG_DIR_ENC);\n\n\treturn mv_cesa_cbc_aes_op(req, &tmpl);\n}\n\nstatic int mv_cesa_cbc_aes_decrypt(struct skcipher_request *req)\n{\n\tstruct mv_cesa_op_ctx tmpl;\n\n\tmv_cesa_set_op_cfg(&tmpl, CESA_SA_DESC_CFG_DIR_DEC);\n\n\treturn mv_cesa_cbc_aes_op(req, &tmpl);\n}\n\nstruct skcipher_alg mv_cesa_cbc_aes_alg = {\n\t.setkey = mv_cesa_aes_setkey,\n\t.encrypt = mv_cesa_cbc_aes_encrypt,\n\t.decrypt = mv_cesa_cbc_aes_decrypt,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.base = {\n\t\t.cra_name = \"cbc(aes)\",\n\t\t.cra_driver_name = \"mv-cbc-aes\",\n\t\t.cra_priority = 300,\n\t\t.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |\n\t\t\t     CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct mv_cesa_aes_ctx),\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_init = mv_cesa_skcipher_cra_init,\n\t\t.cra_exit = mv_cesa_skcipher_cra_exit,\n\t},\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}