{
  "module_name": "otx2_cptvf_algs.c",
  "hash_id": "e09a76b0b22df14296e8ce702c9aad08cd28b29272f5a4c9b63a4370d21d517a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/marvell/octeontx2/otx2_cptvf_algs.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/authenc.h>\n#include <crypto/cryptd.h>\n#include <crypto/des.h>\n#include <crypto/internal/aead.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/xts.h>\n#include <crypto/gcm.h>\n#include <crypto/scatterwalk.h>\n#include <linux/rtnetlink.h>\n#include <linux/sort.h>\n#include <linux/module.h>\n#include \"otx2_cptvf.h\"\n#include \"otx2_cptvf_algs.h\"\n#include \"otx2_cpt_reqmgr.h\"\n\n \n#define AES_GCM_SALT_SIZE 4\n \n#define AES_GCM_IV_SIZE 8\n \n#define AES_GCM_ICV_SIZE 16\n \n#define AES_GCM_IV_OFFSET 8\n#define CONTROL_WORD_LEN 8\n#define KEY2_OFFSET 48\n#define DMA_MODE_FLAG(dma_mode) \\\n\t(((dma_mode) == OTX2_CPT_DMA_MODE_SG) ? (1 << 7) : 0)\n\n \n#define SHA1_TRUNC_DIGEST_SIZE 12\n#define SHA256_TRUNC_DIGEST_SIZE 16\n#define SHA384_TRUNC_DIGEST_SIZE 24\n#define SHA512_TRUNC_DIGEST_SIZE 32\n\nstatic DEFINE_MUTEX(mutex);\nstatic int is_crypto_registered;\n\nstruct cpt_device_desc {\n\tstruct pci_dev *dev;\n\tint num_queues;\n};\n\nstruct cpt_device_table {\n\tatomic_t count;\n\tstruct cpt_device_desc desc[OTX2_CPT_MAX_LFS_NUM];\n};\n\nstatic struct cpt_device_table se_devices = {\n\t.count = ATOMIC_INIT(0)\n};\n\nstatic inline int get_se_device(struct pci_dev **pdev, int *cpu_num)\n{\n\tint count;\n\n\tcount = atomic_read(&se_devices.count);\n\tif (count < 1)\n\t\treturn -ENODEV;\n\n\t*cpu_num = get_cpu();\n\t \n\tif (*cpu_num >= se_devices.desc[0].num_queues)\n\t\t*cpu_num %= se_devices.desc[0].num_queues;\n\t*pdev = se_devices.desc[0].dev;\n\n\tput_cpu();\n\n\treturn 0;\n}\n\nstatic inline int validate_hmac_cipher_null(struct otx2_cpt_req_info *cpt_req)\n{\n\tstruct otx2_cpt_req_ctx *rctx;\n\tstruct aead_request *req;\n\tstruct crypto_aead *tfm;\n\n\treq = container_of(cpt_req->areq, struct aead_request, base);\n\ttfm = crypto_aead_reqtfm(req);\n\trctx = aead_request_ctx_dma(req);\n\tif (memcmp(rctx->fctx.hmac.s.hmac_calc,\n\t\t   rctx->fctx.hmac.s.hmac_recv,\n\t\t   crypto_aead_authsize(tfm)) != 0)\n\t\treturn -EBADMSG;\n\n\treturn 0;\n}\n\nstatic void otx2_cpt_aead_callback(int status, void *arg1, void *arg2)\n{\n\tstruct otx2_cpt_inst_info *inst_info = arg2;\n\tstruct crypto_async_request *areq = arg1;\n\tstruct otx2_cpt_req_info *cpt_req;\n\tstruct pci_dev *pdev;\n\n\tif (inst_info) {\n\t\tcpt_req = inst_info->req;\n\t\tif (!status) {\n\t\t\t \n\t\t\tif (cpt_req->req_type ==\n\t\t\t    OTX2_CPT_AEAD_ENC_DEC_NULL_REQ &&\n\t\t\t    !cpt_req->is_enc)\n\t\t\t\tstatus = validate_hmac_cipher_null(cpt_req);\n\t\t}\n\t\tpdev = inst_info->pdev;\n\t\totx2_cpt_info_destroy(pdev, inst_info);\n\t}\n\tif (areq)\n\t\tcrypto_request_complete(areq, status);\n}\n\nstatic void output_iv_copyback(struct crypto_async_request *areq)\n{\n\tstruct otx2_cpt_req_info *req_info;\n\tstruct otx2_cpt_req_ctx *rctx;\n\tstruct skcipher_request *sreq;\n\tstruct crypto_skcipher *stfm;\n\tstruct otx2_cpt_enc_ctx *ctx;\n\tu32 start, ivsize;\n\n\tsreq = container_of(areq, struct skcipher_request, base);\n\tstfm = crypto_skcipher_reqtfm(sreq);\n\tctx = crypto_skcipher_ctx(stfm);\n\tif (ctx->cipher_type == OTX2_CPT_AES_CBC ||\n\t    ctx->cipher_type == OTX2_CPT_DES3_CBC) {\n\t\trctx = skcipher_request_ctx_dma(sreq);\n\t\treq_info = &rctx->cpt_req;\n\t\tivsize = crypto_skcipher_ivsize(stfm);\n\t\tstart = sreq->cryptlen - ivsize;\n\n\t\tif (req_info->is_enc) {\n\t\t\tscatterwalk_map_and_copy(sreq->iv, sreq->dst, start,\n\t\t\t\t\t\t ivsize, 0);\n\t\t} else {\n\t\t\tif (sreq->src != sreq->dst) {\n\t\t\t\tscatterwalk_map_and_copy(sreq->iv, sreq->src,\n\t\t\t\t\t\t\t start, ivsize, 0);\n\t\t\t} else {\n\t\t\t\tmemcpy(sreq->iv, req_info->iv_out, ivsize);\n\t\t\t\tkfree(req_info->iv_out);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void otx2_cpt_skcipher_callback(int status, void *arg1, void *arg2)\n{\n\tstruct otx2_cpt_inst_info *inst_info = arg2;\n\tstruct crypto_async_request *areq = arg1;\n\tstruct pci_dev *pdev;\n\n\tif (areq) {\n\t\tif (!status)\n\t\t\toutput_iv_copyback(areq);\n\t\tif (inst_info) {\n\t\t\tpdev = inst_info->pdev;\n\t\t\totx2_cpt_info_destroy(pdev, inst_info);\n\t\t}\n\t\tcrypto_request_complete(areq, status);\n\t}\n}\n\nstatic inline void update_input_data(struct otx2_cpt_req_info *req_info,\n\t\t\t\t     struct scatterlist *inp_sg,\n\t\t\t\t     u32 nbytes, u32 *argcnt)\n{\n\treq_info->req.dlen += nbytes;\n\n\twhile (nbytes) {\n\t\tu32 len = (nbytes < inp_sg->length) ? nbytes : inp_sg->length;\n\t\tu8 *ptr = sg_virt(inp_sg);\n\n\t\treq_info->in[*argcnt].vptr = (void *)ptr;\n\t\treq_info->in[*argcnt].size = len;\n\t\tnbytes -= len;\n\t\t++(*argcnt);\n\t\tinp_sg = sg_next(inp_sg);\n\t}\n}\n\nstatic inline void update_output_data(struct otx2_cpt_req_info *req_info,\n\t\t\t\t      struct scatterlist *outp_sg,\n\t\t\t\t      u32 offset, u32 nbytes, u32 *argcnt)\n{\n\tu32 len, sg_len;\n\tu8 *ptr;\n\n\treq_info->rlen += nbytes;\n\n\twhile (nbytes) {\n\t\tsg_len = outp_sg->length - offset;\n\t\tlen = (nbytes < sg_len) ? nbytes : sg_len;\n\t\tptr = sg_virt(outp_sg);\n\n\t\treq_info->out[*argcnt].vptr = (void *) (ptr + offset);\n\t\treq_info->out[*argcnt].size = len;\n\t\tnbytes -= len;\n\t\t++(*argcnt);\n\t\toffset = 0;\n\t\toutp_sg = sg_next(outp_sg);\n\t}\n}\n\nstatic inline int create_ctx_hdr(struct skcipher_request *req, u32 enc,\n\t\t\t\t u32 *argcnt)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct otx2_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct otx2_cpt_fc_ctx *fctx = &rctx->fctx;\n\tint ivsize = crypto_skcipher_ivsize(stfm);\n\tu32 start = req->cryptlen - ivsize;\n\tgfp_t flags;\n\n\tflags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t\tGFP_KERNEL : GFP_ATOMIC;\n\treq_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;\n\treq_info->ctrl.s.se_req = 1;\n\n\treq_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_FC |\n\t\t\t\tDMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);\n\tif (enc) {\n\t\treq_info->req.opcode.s.minor = 2;\n\t} else {\n\t\treq_info->req.opcode.s.minor = 3;\n\t\tif ((ctx->cipher_type == OTX2_CPT_AES_CBC ||\n\t\t    ctx->cipher_type == OTX2_CPT_DES3_CBC) &&\n\t\t    req->src == req->dst) {\n\t\t\treq_info->iv_out = kmalloc(ivsize, flags);\n\t\t\tif (!req_info->iv_out)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tscatterwalk_map_and_copy(req_info->iv_out, req->src,\n\t\t\t\t\t\t start, ivsize, 0);\n\t\t}\n\t}\n\t \n\treq_info->req.param1 = req->cryptlen;\n\t \n\treq_info->req.param2 = 0;\n\n\tfctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;\n\tfctx->enc.enc_ctrl.e.aes_key = ctx->key_type;\n\tfctx->enc.enc_ctrl.e.iv_source = OTX2_CPT_FROM_CPTR;\n\n\tif (ctx->cipher_type == OTX2_CPT_AES_XTS)\n\t\tmemcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len * 2);\n\telse\n\t\tmemcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len);\n\n\tmemcpy(fctx->enc.encr_iv, req->iv, crypto_skcipher_ivsize(stfm));\n\n\tcpu_to_be64s(&fctx->enc.enc_ctrl.u);\n\n\t \n\treq_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;\n\treq_info->in[*argcnt].size = CONTROL_WORD_LEN;\n\treq_info->req.dlen += CONTROL_WORD_LEN;\n\t++(*argcnt);\n\n\treq_info->in[*argcnt].vptr = (u8 *)fctx;\n\treq_info->in[*argcnt].size = sizeof(struct otx2_cpt_fc_ctx);\n\treq_info->req.dlen += sizeof(struct otx2_cpt_fc_ctx);\n\n\t++(*argcnt);\n\n\treturn 0;\n}\n\nstatic inline int create_input_list(struct skcipher_request *req, u32 enc,\n\t\t\t\t    u32 enc_iv_len)\n{\n\tstruct otx2_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 argcnt =  0;\n\tint ret;\n\n\tret = create_ctx_hdr(req, enc, &argcnt);\n\tif (ret)\n\t\treturn ret;\n\n\tupdate_input_data(req_info, req->src, req->cryptlen, &argcnt);\n\treq_info->in_cnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline void create_output_list(struct skcipher_request *req,\n\t\t\t\t      u32 enc_iv_len)\n{\n\tstruct otx2_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 argcnt = 0;\n\n\t \n\tupdate_output_data(req_info, req->dst, 0, req->cryptlen, &argcnt);\n\treq_info->out_cnt = argcnt;\n}\n\nstatic int skcipher_do_fallback(struct skcipher_request *req, bool is_enc)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct otx2_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tint ret;\n\n\tif (ctx->fbk_cipher) {\n\t\tskcipher_request_set_tfm(&rctx->sk_fbk_req, ctx->fbk_cipher);\n\t\tskcipher_request_set_callback(&rctx->sk_fbk_req,\n\t\t\t\t\t      req->base.flags,\n\t\t\t\t\t      req->base.complete,\n\t\t\t\t\t      req->base.data);\n\t\tskcipher_request_set_crypt(&rctx->sk_fbk_req, req->src,\n\t\t\t\t\t   req->dst, req->cryptlen, req->iv);\n\t\tret = is_enc ? crypto_skcipher_encrypt(&rctx->sk_fbk_req) :\n\t\t\t       crypto_skcipher_decrypt(&rctx->sk_fbk_req);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\treturn ret;\n}\n\nstatic inline int cpt_enc_dec(struct skcipher_request *req, u32 enc)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct otx2_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 enc_iv_len = crypto_skcipher_ivsize(stfm);\n\tstruct pci_dev *pdev;\n\tint status, cpu_num;\n\n\tif (req->cryptlen == 0)\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(req->cryptlen, ctx->enc_align_len))\n\t\treturn -EINVAL;\n\n\tif (req->cryptlen > OTX2_CPT_MAX_REQ_SIZE)\n\t\treturn skcipher_do_fallback(req, enc);\n\n\t \n\trctx->ctrl_word.flags = 0;\n\trctx->fctx.enc.enc_ctrl.u = 0;\n\n\tstatus = create_input_list(req, enc, enc_iv_len);\n\tif (status)\n\t\treturn status;\n\tcreate_output_list(req, enc_iv_len);\n\n\tstatus = get_se_device(&pdev, &cpu_num);\n\tif (status)\n\t\treturn status;\n\n\treq_info->callback = otx2_cpt_skcipher_callback;\n\treq_info->areq = &req->base;\n\treq_info->req_type = OTX2_CPT_ENC_DEC_REQ;\n\treq_info->is_enc = enc;\n\treq_info->is_trunc_hmac = false;\n\treq_info->ctrl.s.grp = otx2_cpt_get_kcrypto_eng_grp_num(pdev);\n\n\t \n\tstatus = otx2_cpt_do_request(pdev, req_info, cpu_num);\n\n\treturn status;\n}\n\nstatic int otx2_cpt_skcipher_encrypt(struct skcipher_request *req)\n{\n\treturn cpt_enc_dec(req, true);\n}\n\nstatic int otx2_cpt_skcipher_decrypt(struct skcipher_request *req)\n{\n\treturn cpt_enc_dec(req, false);\n}\n\nstatic int otx2_cpt_skcipher_xts_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t       const u8 *key, u32 keylen)\n{\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tconst u8 *key2 = key + (keylen / 2);\n\tconst u8 *key1 = key;\n\tint ret;\n\n\tret = xts_verify_key(tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\tctx->key_len = keylen;\n\tctx->enc_align_len = 1;\n\tmemcpy(ctx->enc_key, key1, keylen / 2);\n\tmemcpy(ctx->enc_key + KEY2_OFFSET, key2, keylen / 2);\n\tctx->cipher_type = OTX2_CPT_AES_XTS;\n\tswitch (ctx->key_len) {\n\tcase 2 * AES_KEYSIZE_128:\n\t\tctx->key_type = OTX2_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase 2 * AES_KEYSIZE_192:\n\t\tctx->key_type = OTX2_CPT_AES_192_BIT;\n\t\tbreak;\n\tcase 2 * AES_KEYSIZE_256:\n\t\tctx->key_type = OTX2_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);\n}\n\nstatic int cpt_des_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t  u32 keylen, u8 cipher_type)\n{\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (keylen != DES3_EDE_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tctx->key_len = keylen;\n\tctx->cipher_type = cipher_type;\n\tctx->enc_align_len = 8;\n\n\tmemcpy(ctx->enc_key, key, keylen);\n\n\treturn crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);\n}\n\nstatic int cpt_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t  u32 keylen, u8 cipher_type)\n{\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128:\n\t\tctx->key_type = OTX2_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tctx->key_type = OTX2_CPT_AES_192_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tctx->key_type = OTX2_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (cipher_type == OTX2_CPT_AES_CBC || cipher_type == OTX2_CPT_AES_ECB)\n\t\tctx->enc_align_len = 16;\n\telse\n\t\tctx->enc_align_len = 1;\n\n\tctx->key_len = keylen;\n\tctx->cipher_type = cipher_type;\n\n\tmemcpy(ctx->enc_key, key, keylen);\n\n\treturn crypto_skcipher_setkey(ctx->fbk_cipher, key, keylen);\n}\n\nstatic int otx2_cpt_skcipher_cbc_aes_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t    const u8 *key, u32 keylen)\n{\n\treturn cpt_aes_setkey(tfm, key, keylen, OTX2_CPT_AES_CBC);\n}\n\nstatic int otx2_cpt_skcipher_ecb_aes_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t    const u8 *key, u32 keylen)\n{\n\treturn cpt_aes_setkey(tfm, key, keylen, OTX2_CPT_AES_ECB);\n}\n\nstatic int otx2_cpt_skcipher_cbc_des3_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t     const u8 *key, u32 keylen)\n{\n\treturn cpt_des_setkey(tfm, key, keylen, OTX2_CPT_DES3_CBC);\n}\n\nstatic int otx2_cpt_skcipher_ecb_des3_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t     const u8 *key, u32 keylen)\n{\n\treturn cpt_des_setkey(tfm, key, keylen, OTX2_CPT_DES3_ECB);\n}\n\nstatic int cpt_skcipher_fallback_init(struct otx2_cpt_enc_ctx *ctx,\n\t\t\t\t      struct crypto_alg *alg)\n{\n\tif (alg->cra_flags & CRYPTO_ALG_NEED_FALLBACK) {\n\t\tctx->fbk_cipher =\n\t\t\t\tcrypto_alloc_skcipher(alg->cra_name, 0,\n\t\t\t\t\t\t      CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t      CRYPTO_ALG_NEED_FALLBACK);\n\t\tif (IS_ERR(ctx->fbk_cipher)) {\n\t\t\tpr_err(\"%s() failed to allocate fallback for %s\\n\",\n\t\t\t\t__func__, alg->cra_name);\n\t\t\treturn PTR_ERR(ctx->fbk_cipher);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int otx2_cpt_enc_dec_init(struct crypto_skcipher *stfm)\n{\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(stfm);\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\n\tmemset(ctx, 0, sizeof(*ctx));\n\t \n\tcrypto_skcipher_set_reqsize_dma(\n\t\tstfm, sizeof(struct otx2_cpt_req_ctx) +\n\t\t      sizeof(struct skcipher_request));\n\n\treturn cpt_skcipher_fallback_init(ctx, alg);\n}\n\nstatic void otx2_cpt_skcipher_exit(struct crypto_skcipher *tfm)\n{\n\tstruct otx2_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (ctx->fbk_cipher) {\n\t\tcrypto_free_skcipher(ctx->fbk_cipher);\n\t\tctx->fbk_cipher = NULL;\n\t}\n}\n\nstatic int cpt_aead_fallback_init(struct otx2_cpt_aead_ctx *ctx,\n\t\t\t\t  struct crypto_alg *alg)\n{\n\tif (alg->cra_flags & CRYPTO_ALG_NEED_FALLBACK) {\n\t\tctx->fbk_cipher =\n\t\t\t    crypto_alloc_aead(alg->cra_name, 0,\n\t\t\t\t\t      CRYPTO_ALG_ASYNC |\n\t\t\t\t\t      CRYPTO_ALG_NEED_FALLBACK);\n\t\tif (IS_ERR(ctx->fbk_cipher)) {\n\t\t\tpr_err(\"%s() failed to allocate fallback for %s\\n\",\n\t\t\t\t__func__, alg->cra_name);\n\t\t\treturn PTR_ERR(ctx->fbk_cipher);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int cpt_aead_init(struct crypto_aead *atfm, u8 cipher_type, u8 mac_type)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(atfm);\n\tstruct crypto_tfm *tfm = crypto_aead_tfm(atfm);\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\n\tctx->cipher_type = cipher_type;\n\tctx->mac_type = mac_type;\n\n\t \n\tif (ctx->cipher_type != OTX2_CPT_CIPHER_NULL) {\n\t\tswitch (ctx->mac_type) {\n\t\tcase OTX2_CPT_SHA1:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha1\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX2_CPT_SHA256:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha256\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX2_CPT_SHA384:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha384\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX2_CPT_SHA512:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha512\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\t\t}\n\t}\n\tswitch (ctx->cipher_type) {\n\tcase OTX2_CPT_AES_CBC:\n\tcase OTX2_CPT_AES_ECB:\n\t\tctx->enc_align_len = 16;\n\t\tbreak;\n\tcase OTX2_CPT_DES3_CBC:\n\tcase OTX2_CPT_DES3_ECB:\n\t\tctx->enc_align_len = 8;\n\t\tbreak;\n\tcase OTX2_CPT_AES_GCM:\n\tcase OTX2_CPT_CIPHER_NULL:\n\t\tctx->enc_align_len = 1;\n\t\tbreak;\n\t}\n\tcrypto_aead_set_reqsize_dma(atfm, sizeof(struct otx2_cpt_req_ctx));\n\n\treturn cpt_aead_fallback_init(ctx, alg);\n}\n\nstatic int otx2_cpt_aead_cbc_aes_sha1_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_AES_CBC, OTX2_CPT_SHA1);\n}\n\nstatic int otx2_cpt_aead_cbc_aes_sha256_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_AES_CBC, OTX2_CPT_SHA256);\n}\n\nstatic int otx2_cpt_aead_cbc_aes_sha384_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_AES_CBC, OTX2_CPT_SHA384);\n}\n\nstatic int otx2_cpt_aead_cbc_aes_sha512_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_AES_CBC, OTX2_CPT_SHA512);\n}\n\nstatic int otx2_cpt_aead_ecb_null_sha1_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_CIPHER_NULL, OTX2_CPT_SHA1);\n}\n\nstatic int otx2_cpt_aead_ecb_null_sha256_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_CIPHER_NULL, OTX2_CPT_SHA256);\n}\n\nstatic int otx2_cpt_aead_ecb_null_sha384_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_CIPHER_NULL, OTX2_CPT_SHA384);\n}\n\nstatic int otx2_cpt_aead_ecb_null_sha512_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_CIPHER_NULL, OTX2_CPT_SHA512);\n}\n\nstatic int otx2_cpt_aead_gcm_aes_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX2_CPT_AES_GCM, OTX2_CPT_MAC_NULL);\n}\n\nstatic void otx2_cpt_aead_exit(struct crypto_aead *tfm)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tkfree(ctx->ipad);\n\tkfree(ctx->opad);\n\tif (ctx->hashalg)\n\t\tcrypto_free_shash(ctx->hashalg);\n\tkfree(ctx->sdesc);\n\n\tif (ctx->fbk_cipher) {\n\t\tcrypto_free_aead(ctx->fbk_cipher);\n\t\tctx->fbk_cipher = NULL;\n\t}\n}\n\nstatic int otx2_cpt_aead_gcm_set_authsize(struct crypto_aead *tfm,\n\t\t\t\t\t  unsigned int authsize)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tif (crypto_rfc4106_check_authsize(authsize))\n\t\treturn -EINVAL;\n\n\ttfm->authsize = authsize;\n\t \n\tif (ctx->fbk_cipher)\n\t\tctx->fbk_cipher->authsize = authsize;\n\n\treturn 0;\n}\n\nstatic int otx2_cpt_aead_set_authsize(struct crypto_aead *tfm,\n\t\t\t\t      unsigned int authsize)\n{\n\ttfm->authsize = authsize;\n\n\treturn 0;\n}\n\nstatic int otx2_cpt_aead_null_set_authsize(struct crypto_aead *tfm,\n\t\t\t\t\t   unsigned int authsize)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tctx->is_trunc_hmac = true;\n\ttfm->authsize = authsize;\n\n\treturn 0;\n}\n\nstatic struct otx2_cpt_sdesc *alloc_sdesc(struct crypto_shash *alg)\n{\n\tstruct otx2_cpt_sdesc *sdesc;\n\tint size;\n\n\tsize = sizeof(struct shash_desc) + crypto_shash_descsize(alg);\n\tsdesc = kmalloc(size, GFP_KERNEL);\n\tif (!sdesc)\n\t\treturn NULL;\n\n\tsdesc->shash.tfm = alg;\n\n\treturn sdesc;\n}\n\nstatic inline void swap_data32(void *buf, u32 len)\n{\n\tcpu_to_be32_array(buf, buf, len / 4);\n}\n\nstatic inline void swap_data64(void *buf, u32 len)\n{\n\tu64 *src = buf;\n\tint i = 0;\n\n\tfor (i = 0 ; i < len / 8; i++, src++)\n\t\tcpu_to_be64s(src);\n}\n\nstatic int copy_pad(u8 mac_type, u8 *out_pad, u8 *in_pad)\n{\n\tstruct sha512_state *sha512;\n\tstruct sha256_state *sha256;\n\tstruct sha1_state *sha1;\n\n\tswitch (mac_type) {\n\tcase OTX2_CPT_SHA1:\n\t\tsha1 = (struct sha1_state *) in_pad;\n\t\tswap_data32(sha1->state, SHA1_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha1->state, SHA1_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase OTX2_CPT_SHA256:\n\t\tsha256 = (struct sha256_state *) in_pad;\n\t\tswap_data32(sha256->state, SHA256_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha256->state, SHA256_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase OTX2_CPT_SHA384:\n\tcase OTX2_CPT_SHA512:\n\t\tsha512 = (struct sha512_state *) in_pad;\n\t\tswap_data64(sha512->state, SHA512_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha512->state, SHA512_DIGEST_SIZE);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int aead_hmac_init(struct crypto_aead *cipher)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tint state_size = crypto_shash_statesize(ctx->hashalg);\n\tint ds = crypto_shash_digestsize(ctx->hashalg);\n\tint bs = crypto_shash_blocksize(ctx->hashalg);\n\tint authkeylen = ctx->auth_key_len;\n\tu8 *ipad = NULL, *opad = NULL;\n\tint ret = 0, icount = 0;\n\n\tctx->sdesc = alloc_sdesc(ctx->hashalg);\n\tif (!ctx->sdesc)\n\t\treturn -ENOMEM;\n\n\tctx->ipad = kzalloc(bs, GFP_KERNEL);\n\tif (!ctx->ipad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tctx->opad = kzalloc(bs, GFP_KERNEL);\n\tif (!ctx->opad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tipad = kzalloc(state_size, GFP_KERNEL);\n\tif (!ipad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\topad = kzalloc(state_size, GFP_KERNEL);\n\tif (!opad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tif (authkeylen > bs) {\n\t\tret = crypto_shash_digest(&ctx->sdesc->shash, ctx->key,\n\t\t\t\t\t  authkeylen, ipad);\n\t\tif (ret)\n\t\t\tgoto calc_fail;\n\n\t\tauthkeylen = ds;\n\t} else {\n\t\tmemcpy(ipad, ctx->key, authkeylen);\n\t}\n\n\tmemset(ipad + authkeylen, 0, bs - authkeylen);\n\tmemcpy(opad, ipad, bs);\n\n\tfor (icount = 0; icount < bs; icount++) {\n\t\tipad[icount] ^= 0x36;\n\t\topad[icount] ^= 0x5c;\n\t}\n\n\t \n\n\t \n\tcrypto_shash_init(&ctx->sdesc->shash);\n\tcrypto_shash_update(&ctx->sdesc->shash, ipad, bs);\n\tcrypto_shash_export(&ctx->sdesc->shash, ipad);\n\tret = copy_pad(ctx->mac_type, ctx->ipad, ipad);\n\tif (ret)\n\t\tgoto calc_fail;\n\n\t \n\tcrypto_shash_init(&ctx->sdesc->shash);\n\tcrypto_shash_update(&ctx->sdesc->shash, opad, bs);\n\tcrypto_shash_export(&ctx->sdesc->shash, opad);\n\tret = copy_pad(ctx->mac_type, ctx->opad, opad);\n\tif (ret)\n\t\tgoto calc_fail;\n\n\tkfree(ipad);\n\tkfree(opad);\n\n\treturn 0;\n\ncalc_fail:\n\tkfree(ctx->ipad);\n\tctx->ipad = NULL;\n\tkfree(ctx->opad);\n\tctx->opad = NULL;\n\tkfree(ipad);\n\tkfree(opad);\n\tkfree(ctx->sdesc);\n\tctx->sdesc = NULL;\n\n\treturn ret;\n}\n\nstatic int otx2_cpt_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,\n\t\t\t\t\t    const unsigned char *key,\n\t\t\t\t\t    unsigned int keylen)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tstruct crypto_authenc_key_param *param;\n\tint enckeylen = 0, authkeylen = 0;\n\tstruct rtattr *rta = (void *)key;\n\n\tif (!RTA_OK(rta, keylen))\n\t\treturn -EINVAL;\n\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\treturn -EINVAL;\n\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\treturn -EINVAL;\n\n\tparam = RTA_DATA(rta);\n\tenckeylen = be32_to_cpu(param->enckeylen);\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\tif (keylen < enckeylen)\n\t\treturn -EINVAL;\n\n\tif (keylen > OTX2_CPT_MAX_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tauthkeylen = keylen - enckeylen;\n\tmemcpy(ctx->key, key, keylen);\n\n\tswitch (enckeylen) {\n\tcase AES_KEYSIZE_128:\n\t\tctx->key_type = OTX2_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tctx->key_type = OTX2_CPT_AES_192_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tctx->key_type = OTX2_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tctx->enc_key_len = enckeylen;\n\tctx->auth_key_len = authkeylen;\n\n\treturn aead_hmac_init(cipher);\n}\n\nstatic int otx2_cpt_aead_ecb_null_sha_setkey(struct crypto_aead *cipher,\n\t\t\t\t\t     const unsigned char *key,\n\t\t\t\t\t     unsigned int keylen)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta = (void *)key;\n\tint enckeylen = 0;\n\n\tif (!RTA_OK(rta, keylen))\n\t\treturn -EINVAL;\n\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\treturn -EINVAL;\n\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\treturn -EINVAL;\n\n\tparam = RTA_DATA(rta);\n\tenckeylen = be32_to_cpu(param->enckeylen);\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\tif (enckeylen != 0)\n\t\treturn -EINVAL;\n\n\tif (keylen > OTX2_CPT_MAX_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->key, key, keylen);\n\tctx->enc_key_len = enckeylen;\n\tctx->auth_key_len = keylen;\n\n\treturn 0;\n}\n\nstatic int otx2_cpt_aead_gcm_aes_setkey(struct crypto_aead *cipher,\n\t\t\t\t\tconst unsigned char *key,\n\t\t\t\t\tunsigned int keylen)\n{\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\n\t \n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX2_CPT_AES_128_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_128;\n\t\tbreak;\n\tcase AES_KEYSIZE_192 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX2_CPT_AES_192_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_192;\n\t\tbreak;\n\tcase AES_KEYSIZE_256 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX2_CPT_AES_256_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_256;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmemcpy(ctx->key, key, keylen);\n\n\treturn crypto_aead_setkey(ctx->fbk_cipher, key, keylen);\n}\n\nstatic inline int create_aead_ctx_hdr(struct aead_request *req, u32 enc,\n\t\t\t\t      u32 *argcnt)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct otx2_cpt_fc_ctx *fctx = &rctx->fctx;\n\tint mac_len = crypto_aead_authsize(tfm);\n\tint ds;\n\n\trctx->ctrl_word.e.enc_data_offset = req->assoclen;\n\n\tswitch (ctx->cipher_type) {\n\tcase OTX2_CPT_AES_CBC:\n\t\tif (req->assoclen > 248 || !IS_ALIGNED(req->assoclen, 8))\n\t\t\treturn -EINVAL;\n\n\t\tfctx->enc.enc_ctrl.e.iv_source = OTX2_CPT_FROM_CPTR;\n\t\t \n\t\tmemcpy(fctx->enc.encr_key, ctx->key + ctx->auth_key_len,\n\t\t       ctx->enc_key_len);\n\t\t \n\t\tmemcpy(fctx->enc.encr_iv, req->iv, crypto_aead_ivsize(tfm));\n\n\t\tds = crypto_shash_digestsize(ctx->hashalg);\n\t\tif (ctx->mac_type == OTX2_CPT_SHA384)\n\t\t\tds = SHA512_DIGEST_SIZE;\n\t\tif (ctx->ipad)\n\t\t\tmemcpy(fctx->hmac.e.ipad, ctx->ipad, ds);\n\t\tif (ctx->opad)\n\t\t\tmemcpy(fctx->hmac.e.opad, ctx->opad, ds);\n\t\tbreak;\n\n\tcase OTX2_CPT_AES_GCM:\n\t\tif (crypto_ipsec_check_assoclen(req->assoclen))\n\t\t\treturn -EINVAL;\n\n\t\tfctx->enc.enc_ctrl.e.iv_source = OTX2_CPT_FROM_DPTR;\n\t\t \n\t\tmemcpy(fctx->enc.encr_key, ctx->key, ctx->enc_key_len);\n\t\t \n\t\tmemcpy(fctx->enc.encr_iv, ctx->key + ctx->enc_key_len,\n\t\t       AES_GCM_SALT_SIZE);\n\n\t\trctx->ctrl_word.e.iv_offset = req->assoclen - AES_GCM_IV_OFFSET;\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\tcpu_to_be64s(&rctx->ctrl_word.flags);\n\n\treq_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;\n\treq_info->ctrl.s.se_req = 1;\n\treq_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_FC |\n\t\t\t\t DMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);\n\tif (enc) {\n\t\treq_info->req.opcode.s.minor = 2;\n\t\treq_info->req.param1 = req->cryptlen;\n\t\treq_info->req.param2 = req->cryptlen + req->assoclen;\n\t} else {\n\t\treq_info->req.opcode.s.minor = 3;\n\t\treq_info->req.param1 = req->cryptlen - mac_len;\n\t\treq_info->req.param2 = req->cryptlen + req->assoclen - mac_len;\n\t}\n\n\tfctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;\n\tfctx->enc.enc_ctrl.e.aes_key = ctx->key_type;\n\tfctx->enc.enc_ctrl.e.mac_type = ctx->mac_type;\n\tfctx->enc.enc_ctrl.e.mac_len = mac_len;\n\tcpu_to_be64s(&fctx->enc.enc_ctrl.u);\n\n\t \n\treq_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;\n\treq_info->in[*argcnt].size = CONTROL_WORD_LEN;\n\treq_info->req.dlen += CONTROL_WORD_LEN;\n\t++(*argcnt);\n\n\treq_info->in[*argcnt].vptr = (u8 *)fctx;\n\treq_info->in[*argcnt].size = sizeof(struct otx2_cpt_fc_ctx);\n\treq_info->req.dlen += sizeof(struct otx2_cpt_fc_ctx);\n\t++(*argcnt);\n\n\treturn 0;\n}\n\nstatic inline void create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,\n\t\t\t\t      u32 enc)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\n\treq_info->ctrl.s.dma_mode = OTX2_CPT_DMA_MODE_SG;\n\treq_info->ctrl.s.se_req = 1;\n\treq_info->req.opcode.s.major = OTX2_CPT_MAJOR_OP_HMAC |\n\t\t\t\t DMA_MODE_FLAG(OTX2_CPT_DMA_MODE_SG);\n\treq_info->is_trunc_hmac = ctx->is_trunc_hmac;\n\n\treq_info->req.opcode.s.minor = 0;\n\treq_info->req.param1 = ctx->auth_key_len;\n\treq_info->req.param2 = ctx->mac_type << 8;\n\n\t \n\treq_info->in[*argcnt].vptr = ctx->key;\n\treq_info->in[*argcnt].size = round_up(ctx->auth_key_len, 8);\n\treq_info->req.dlen += round_up(ctx->auth_key_len, 8);\n\t++(*argcnt);\n}\n\nstatic inline int create_aead_input_list(struct aead_request *req, u32 enc)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 inputlen =  req->cryptlen + req->assoclen;\n\tu32 status, argcnt = 0;\n\n\tstatus = create_aead_ctx_hdr(req, enc, &argcnt);\n\tif (status)\n\t\treturn status;\n\tupdate_input_data(req_info, req->src, inputlen, &argcnt);\n\treq_info->in_cnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline void create_aead_output_list(struct aead_request *req, u32 enc,\n\t\t\t\t\t   u32 mac_len)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info =  &rctx->cpt_req;\n\tu32 argcnt = 0, outputlen = 0;\n\n\tif (enc)\n\t\toutputlen = req->cryptlen +  req->assoclen + mac_len;\n\telse\n\t\toutputlen = req->cryptlen + req->assoclen - mac_len;\n\n\tupdate_output_data(req_info, req->dst, 0, outputlen, &argcnt);\n\treq_info->out_cnt = argcnt;\n}\n\nstatic inline void create_aead_null_input_list(struct aead_request *req,\n\t\t\t\t\t       u32 enc, u32 mac_len)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 inputlen, argcnt = 0;\n\n\tif (enc)\n\t\tinputlen =  req->cryptlen + req->assoclen;\n\telse\n\t\tinputlen =  req->cryptlen + req->assoclen - mac_len;\n\n\tcreate_hmac_ctx_hdr(req, &argcnt, enc);\n\tupdate_input_data(req_info, req->src, inputlen, &argcnt);\n\treq_info->in_cnt = argcnt;\n}\n\nstatic inline int create_aead_null_output_list(struct aead_request *req,\n\t\t\t\t\t       u32 enc, u32 mac_len)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info =  &rctx->cpt_req;\n\tstruct scatterlist *dst;\n\tu8 *ptr = NULL;\n\tint argcnt = 0, status, offset;\n\tu32 inputlen;\n\n\tif (enc)\n\t\tinputlen =  req->cryptlen + req->assoclen;\n\telse\n\t\tinputlen =  req->cryptlen + req->assoclen - mac_len;\n\n\t \n\tif (req->src != req->dst) {\n\n\t\tptr = kmalloc(inputlen, (req_info->areq->flags &\n\t\t\t\t\t CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t\t\t\t GFP_KERNEL : GFP_ATOMIC);\n\t\tif (!ptr)\n\t\t\treturn -ENOMEM;\n\n\t\tstatus = sg_copy_to_buffer(req->src, sg_nents(req->src), ptr,\n\t\t\t\t\t   inputlen);\n\t\tif (status != inputlen) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto error_free;\n\t\t}\n\t\tstatus = sg_copy_from_buffer(req->dst, sg_nents(req->dst), ptr,\n\t\t\t\t\t     inputlen);\n\t\tif (status != inputlen) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto error_free;\n\t\t}\n\t\tkfree(ptr);\n\t}\n\n\tif (enc) {\n\t\t \n\t\tdst = req->dst;\n\t\toffset = inputlen;\n\t\twhile (offset >= dst->length) {\n\t\t\toffset -= dst->length;\n\t\t\tdst = sg_next(dst);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tupdate_output_data(req_info, dst, offset, mac_len, &argcnt);\n\t} else {\n\t\t \n\t\tstatus = sg_copy_buffer(req->src, sg_nents(req->src),\n\t\t\t\t\trctx->fctx.hmac.s.hmac_recv, mac_len,\n\t\t\t\t\tinputlen, true);\n\t\tif (status != mac_len)\n\t\t\treturn -EINVAL;\n\n\t\treq_info->out[argcnt].vptr = rctx->fctx.hmac.s.hmac_calc;\n\t\treq_info->out[argcnt].size = mac_len;\n\t\targcnt++;\n\t}\n\n\treq_info->out_cnt = argcnt;\n\treturn 0;\n\nerror_free:\n\tkfree(ptr);\n\treturn status;\n}\n\nstatic int aead_do_fallback(struct aead_request *req, bool is_enc)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(aead);\n\tint ret;\n\n\tif (ctx->fbk_cipher) {\n\t\t \n\t\taead_request_set_tfm(&rctx->fbk_req, ctx->fbk_cipher);\n\t\taead_request_set_callback(&rctx->fbk_req, req->base.flags,\n\t\t\t\t\t  req->base.complete, req->base.data);\n\t\taead_request_set_crypt(&rctx->fbk_req, req->src,\n\t\t\t\t       req->dst, req->cryptlen, req->iv);\n\t\taead_request_set_ad(&rctx->fbk_req, req->assoclen);\n\t\tret = is_enc ? crypto_aead_encrypt(&rctx->fbk_req) :\n\t\t\t       crypto_aead_decrypt(&rctx->fbk_req);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)\n{\n\tstruct otx2_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx2_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct otx2_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\tstruct pci_dev *pdev;\n\tint status, cpu_num;\n\n\t \n\trctx->ctrl_word.flags = 0;\n\trctx->fctx.enc.enc_ctrl.u = 0;\n\n\treq_info->callback = otx2_cpt_aead_callback;\n\treq_info->areq = &req->base;\n\treq_info->req_type = reg_type;\n\treq_info->is_enc = enc;\n\treq_info->is_trunc_hmac = false;\n\n\tswitch (reg_type) {\n\tcase OTX2_CPT_AEAD_ENC_DEC_REQ:\n\t\tstatus = create_aead_input_list(req, enc);\n\t\tif (status)\n\t\t\treturn status;\n\t\tcreate_aead_output_list(req, enc, crypto_aead_authsize(tfm));\n\t\tbreak;\n\n\tcase OTX2_CPT_AEAD_ENC_DEC_NULL_REQ:\n\t\tcreate_aead_null_input_list(req, enc,\n\t\t\t\t\t    crypto_aead_authsize(tfm));\n\t\tstatus = create_aead_null_output_list(req, enc,\n\t\t\t\t\t\tcrypto_aead_authsize(tfm));\n\t\tif (status)\n\t\t\treturn status;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (!IS_ALIGNED(req_info->req.param1, ctx->enc_align_len))\n\t\treturn -EINVAL;\n\n\tif (!req_info->req.param2 ||\n\t    (req_info->req.param1 > OTX2_CPT_MAX_REQ_SIZE) ||\n\t    (req_info->req.param2 > OTX2_CPT_MAX_REQ_SIZE))\n\t\treturn aead_do_fallback(req, enc);\n\n\tstatus = get_se_device(&pdev, &cpu_num);\n\tif (status)\n\t\treturn status;\n\n\treq_info->ctrl.s.grp = otx2_cpt_get_kcrypto_eng_grp_num(pdev);\n\n\t \n\treturn otx2_cpt_do_request(pdev, req_info, cpu_num);\n}\n\nstatic int otx2_cpt_aead_encrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX2_CPT_AEAD_ENC_DEC_REQ, true);\n}\n\nstatic int otx2_cpt_aead_decrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX2_CPT_AEAD_ENC_DEC_REQ, false);\n}\n\nstatic int otx2_cpt_aead_null_encrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX2_CPT_AEAD_ENC_DEC_NULL_REQ, true);\n}\n\nstatic int otx2_cpt_aead_null_decrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX2_CPT_AEAD_ENC_DEC_NULL_REQ, false);\n}\n\nstatic struct skcipher_alg otx2_cpt_skciphers[] = { {\n\t.base.cra_name = \"xts(aes)\",\n\t.base.cra_driver_name = \"cpt_xts_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx2_cpt_enc_dec_init,\n\t.exit = otx2_cpt_skcipher_exit,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.min_keysize = 2 * AES_MIN_KEY_SIZE,\n\t.max_keysize = 2 * AES_MAX_KEY_SIZE,\n\t.setkey = otx2_cpt_skcipher_xts_setkey,\n\t.encrypt = otx2_cpt_skcipher_encrypt,\n\t.decrypt = otx2_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"cbc(aes)\",\n\t.base.cra_driver_name = \"cpt_cbc_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx2_cpt_enc_dec_init,\n\t.exit = otx2_cpt_skcipher_exit,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.setkey = otx2_cpt_skcipher_cbc_aes_setkey,\n\t.encrypt = otx2_cpt_skcipher_encrypt,\n\t.decrypt = otx2_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"ecb(aes)\",\n\t.base.cra_driver_name = \"cpt_ecb_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx2_cpt_enc_dec_init,\n\t.exit = otx2_cpt_skcipher_exit,\n\t.ivsize = 0,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.setkey = otx2_cpt_skcipher_ecb_aes_setkey,\n\t.encrypt = otx2_cpt_skcipher_encrypt,\n\t.decrypt = otx2_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"cbc(des3_ede)\",\n\t.base.cra_driver_name = \"cpt_cbc_des3_ede\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx2_cpt_enc_dec_init,\n\t.exit = otx2_cpt_skcipher_exit,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.ivsize = DES_BLOCK_SIZE,\n\t.setkey = otx2_cpt_skcipher_cbc_des3_setkey,\n\t.encrypt = otx2_cpt_skcipher_encrypt,\n\t.decrypt = otx2_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"ecb(des3_ede)\",\n\t.base.cra_driver_name = \"cpt_ecb_des3_ede\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx2_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx2_cpt_enc_dec_init,\n\t.exit = otx2_cpt_skcipher_exit,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.ivsize = 0,\n\t.setkey = otx2_cpt_skcipher_ecb_des3_setkey,\n\t.encrypt = otx2_cpt_skcipher_encrypt,\n\t.decrypt = otx2_cpt_skcipher_decrypt,\n} };\n\nstatic struct aead_alg otx2_cpt_aeads[] = { {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha1),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha1_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_cbc_aes_sha1_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_set_authsize,\n\t.encrypt = otx2_cpt_aead_encrypt,\n\t.decrypt = otx2_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA1_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha256),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha256_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_cbc_aes_sha256_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_set_authsize,\n\t.encrypt = otx2_cpt_aead_encrypt,\n\t.decrypt = otx2_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA256_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha384),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha384_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_cbc_aes_sha384_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_set_authsize,\n\t.encrypt = otx2_cpt_aead_encrypt,\n\t.decrypt = otx2_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA384_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha512),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha512_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_cbc_aes_sha512_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_set_authsize,\n\t.encrypt = otx2_cpt_aead_encrypt,\n\t.decrypt = otx2_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA512_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha1),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha1_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_ecb_null_sha1_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_null_set_authsize,\n\t.encrypt = otx2_cpt_aead_null_encrypt,\n\t.decrypt = otx2_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA1_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha256),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha256_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_ecb_null_sha256_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_null_set_authsize,\n\t.encrypt = otx2_cpt_aead_null_encrypt,\n\t.decrypt = otx2_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA256_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha384),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha384_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_ecb_null_sha384_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_null_set_authsize,\n\t.encrypt = otx2_cpt_aead_null_encrypt,\n\t.decrypt = otx2_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA384_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha512),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha512_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_ecb_null_sha512_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx2_cpt_aead_null_set_authsize,\n\t.encrypt = otx2_cpt_aead_null_encrypt,\n\t.decrypt = otx2_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA512_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"rfc4106(gcm(aes))\",\n\t\t.cra_driver_name = \"cpt_rfc4106_gcm_aes\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_ctxsize = sizeof(struct otx2_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx2_cpt_aead_gcm_aes_init,\n\t.exit = otx2_cpt_aead_exit,\n\t.setkey = otx2_cpt_aead_gcm_aes_setkey,\n\t.setauthsize = otx2_cpt_aead_gcm_set_authsize,\n\t.encrypt = otx2_cpt_aead_encrypt,\n\t.decrypt = otx2_cpt_aead_decrypt,\n\t.ivsize = AES_GCM_IV_SIZE,\n\t.maxauthsize = AES_GCM_ICV_SIZE,\n} };\n\nstatic inline int cpt_register_algs(void)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(otx2_cpt_skciphers); i++)\n\t\totx2_cpt_skciphers[i].base.cra_flags &= ~CRYPTO_ALG_DEAD;\n\n\terr = crypto_register_skciphers(otx2_cpt_skciphers,\n\t\t\t\t\tARRAY_SIZE(otx2_cpt_skciphers));\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < ARRAY_SIZE(otx2_cpt_aeads); i++)\n\t\totx2_cpt_aeads[i].base.cra_flags &= ~CRYPTO_ALG_DEAD;\n\n\terr = crypto_register_aeads(otx2_cpt_aeads,\n\t\t\t\t    ARRAY_SIZE(otx2_cpt_aeads));\n\tif (err) {\n\t\tcrypto_unregister_skciphers(otx2_cpt_skciphers,\n\t\t\t\t\t    ARRAY_SIZE(otx2_cpt_skciphers));\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void cpt_unregister_algs(void)\n{\n\tcrypto_unregister_skciphers(otx2_cpt_skciphers,\n\t\t\t\t    ARRAY_SIZE(otx2_cpt_skciphers));\n\tcrypto_unregister_aeads(otx2_cpt_aeads, ARRAY_SIZE(otx2_cpt_aeads));\n}\n\nstatic int compare_func(const void *lptr, const void *rptr)\n{\n\tconst struct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;\n\tconst struct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;\n\n\tif (ldesc->dev->devfn < rdesc->dev->devfn)\n\t\treturn -1;\n\tif (ldesc->dev->devfn > rdesc->dev->devfn)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void swap_func(void *lptr, void *rptr, int size)\n{\n\tstruct cpt_device_desc *ldesc = lptr;\n\tstruct cpt_device_desc *rdesc = rptr;\n\n\tswap(*ldesc, *rdesc);\n}\n\nint otx2_cpt_crypto_init(struct pci_dev *pdev, struct module *mod,\n\t\t\t int num_queues, int num_devices)\n{\n\tint ret = 0;\n\tint count;\n\n\tmutex_lock(&mutex);\n\tcount = atomic_read(&se_devices.count);\n\tif (count >= OTX2_CPT_MAX_LFS_NUM) {\n\t\tdev_err(&pdev->dev, \"No space to add a new device\\n\");\n\t\tret = -ENOSPC;\n\t\tgoto unlock;\n\t}\n\tse_devices.desc[count].num_queues = num_queues;\n\tse_devices.desc[count++].dev = pdev;\n\tatomic_inc(&se_devices.count);\n\n\tif (atomic_read(&se_devices.count) == num_devices &&\n\t    is_crypto_registered == false) {\n\t\tif (cpt_register_algs()) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Error in registering crypto algorithms\\n\");\n\t\t\tret =  -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\ttry_module_get(mod);\n\t\tis_crypto_registered = true;\n\t}\n\tsort(se_devices.desc, count, sizeof(struct cpt_device_desc),\n\t     compare_func, swap_func);\n\nunlock:\n\tmutex_unlock(&mutex);\n\treturn ret;\n}\n\nvoid otx2_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod)\n{\n\tstruct cpt_device_table *dev_tbl;\n\tbool dev_found = false;\n\tint i, j, count;\n\n\tmutex_lock(&mutex);\n\n\tdev_tbl = &se_devices;\n\tcount = atomic_read(&dev_tbl->count);\n\tfor (i = 0; i < count; i++) {\n\t\tif (pdev == dev_tbl->desc[i].dev) {\n\t\t\tfor (j = i; j < count-1; j++)\n\t\t\t\tdev_tbl->desc[j] = dev_tbl->desc[j+1];\n\t\t\tdev_found = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!dev_found) {\n\t\tdev_err(&pdev->dev, \"%s device not found\\n\", __func__);\n\t\tgoto unlock;\n\t}\n\tif (atomic_dec_and_test(&se_devices.count)) {\n\t\tcpt_unregister_algs();\n\t\tmodule_put(mod);\n\t\tis_crypto_registered = false;\n\t}\n\nunlock:\n\tmutex_unlock(&mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}