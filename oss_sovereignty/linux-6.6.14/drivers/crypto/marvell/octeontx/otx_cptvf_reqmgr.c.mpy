{
  "module_name": "otx_cptvf_reqmgr.c",
  "hash_id": "3c96f4bd160ae8b6ca7f4370cd58251ab4a754c64c2e1897675889532537e21b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/marvell/octeontx/otx_cptvf_reqmgr.c",
  "human_readable_source": "\n \n\n#include \"otx_cptvf.h\"\n#include \"otx_cptvf_algs.h\"\n\n \n#define COMPLETION_CODE_SIZE\t8\n#define COMPLETION_CODE_INIT\t0\n\n \n#define SG_LIST_HDR_SIZE\t8\n\n \n#define CPT_PENTRY_TIMEOUT\t1000\n#define CPT_PENTRY_STEP\t\t50\n\n \n#define CPT_IQ_STOP_MARGIN\t128\n#define CPT_IQ_RESUME_MARGIN\t512\n\n#define CPT_DMA_ALIGN\t\t128\n\nvoid otx_cpt_dump_sg_list(struct pci_dev *pdev, struct otx_cpt_req_info *req)\n{\n\tint i;\n\n\tpr_debug(\"Gather list size %d\\n\", req->incnt);\n\tfor (i = 0; i < req->incnt; i++) {\n\t\tpr_debug(\"Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\\n\", i,\n\t\t\t req->in[i].size, req->in[i].vptr,\n\t\t\t (void *) req->in[i].dma_addr);\n\t\tpr_debug(\"Buffer hexdump (%d bytes)\\n\",\n\t\t\t req->in[i].size);\n\t\tprint_hex_dump_debug(\"\", DUMP_PREFIX_NONE, 16, 1,\n\t\t\t\t     req->in[i].vptr, req->in[i].size, false);\n\t}\n\n\tpr_debug(\"Scatter list size %d\\n\", req->outcnt);\n\tfor (i = 0; i < req->outcnt; i++) {\n\t\tpr_debug(\"Buffer %d size %d, vptr 0x%p, dmaptr 0x%p\\n\", i,\n\t\t\t req->out[i].size, req->out[i].vptr,\n\t\t\t (void *) req->out[i].dma_addr);\n\t\tpr_debug(\"Buffer hexdump (%d bytes)\\n\", req->out[i].size);\n\t\tprint_hex_dump_debug(\"\", DUMP_PREFIX_NONE, 16, 1,\n\t\t\t\t     req->out[i].vptr, req->out[i].size, false);\n\t}\n}\n\nstatic inline struct otx_cpt_pending_entry *get_free_pending_entry(\n\t\t\t\t\t\tstruct otx_cpt_pending_queue *q,\n\t\t\t\t\t\tint qlen)\n{\n\tstruct otx_cpt_pending_entry *ent = NULL;\n\n\tent = &q->head[q->rear];\n\tif (unlikely(ent->busy))\n\t\treturn NULL;\n\n\tq->rear++;\n\tif (unlikely(q->rear == qlen))\n\t\tq->rear = 0;\n\n\treturn ent;\n}\n\nstatic inline u32 modulo_inc(u32 index, u32 length, u32 inc)\n{\n\tif (WARN_ON(inc > length))\n\t\tinc = length;\n\n\tindex += inc;\n\tif (unlikely(index >= length))\n\t\tindex -= length;\n\n\treturn index;\n}\n\nstatic inline void free_pentry(struct otx_cpt_pending_entry *pentry)\n{\n\tpentry->completion_addr = NULL;\n\tpentry->info = NULL;\n\tpentry->callback = NULL;\n\tpentry->areq = NULL;\n\tpentry->resume_sender = false;\n\tpentry->busy = false;\n}\n\nstatic inline int setup_sgio_components(struct pci_dev *pdev,\n\t\t\t\t\tstruct otx_cpt_buf_ptr *list,\n\t\t\t\t\tint buf_count, u8 *buffer)\n{\n\tstruct otx_cpt_sglist_component *sg_ptr = NULL;\n\tint ret = 0, i, j;\n\tint components;\n\n\tif (unlikely(!list)) {\n\t\tdev_err(&pdev->dev, \"Input list pointer is NULL\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < buf_count; i++) {\n\t\tif (likely(list[i].vptr)) {\n\t\t\tlist[i].dma_addr = dma_map_single(&pdev->dev,\n\t\t\t\t\t\t\t  list[i].vptr,\n\t\t\t\t\t\t\t  list[i].size,\n\t\t\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\t\t\tif (unlikely(dma_mapping_error(&pdev->dev,\n\t\t\t\t\t\t       list[i].dma_addr))) {\n\t\t\t\tdev_err(&pdev->dev, \"Dma mapping failed\\n\");\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto sg_cleanup;\n\t\t\t}\n\t\t}\n\t}\n\n\tcomponents = buf_count / 4;\n\tsg_ptr = (struct otx_cpt_sglist_component *)buffer;\n\tfor (i = 0; i < components; i++) {\n\t\tsg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);\n\t\tsg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);\n\t\tsg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);\n\t\tsg_ptr->u.s.len3 = cpu_to_be16(list[i * 4 + 3].size);\n\t\tsg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);\n\t\tsg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);\n\t\tsg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);\n\t\tsg_ptr->ptr3 = cpu_to_be64(list[i * 4 + 3].dma_addr);\n\t\tsg_ptr++;\n\t}\n\tcomponents = buf_count % 4;\n\n\tswitch (components) {\n\tcase 3:\n\t\tsg_ptr->u.s.len2 = cpu_to_be16(list[i * 4 + 2].size);\n\t\tsg_ptr->ptr2 = cpu_to_be64(list[i * 4 + 2].dma_addr);\n\t\tfallthrough;\n\tcase 2:\n\t\tsg_ptr->u.s.len1 = cpu_to_be16(list[i * 4 + 1].size);\n\t\tsg_ptr->ptr1 = cpu_to_be64(list[i * 4 + 1].dma_addr);\n\t\tfallthrough;\n\tcase 1:\n\t\tsg_ptr->u.s.len0 = cpu_to_be16(list[i * 4 + 0].size);\n\t\tsg_ptr->ptr0 = cpu_to_be64(list[i * 4 + 0].dma_addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n\nsg_cleanup:\n\tfor (j = 0; j < i; j++) {\n\t\tif (list[j].dma_addr) {\n\t\t\tdma_unmap_single(&pdev->dev, list[i].dma_addr,\n\t\t\t\t\t list[i].size, DMA_BIDIRECTIONAL);\n\t\t}\n\n\t\tlist[j].dma_addr = 0;\n\t}\n\treturn ret;\n}\n\nstatic inline int setup_sgio_list(struct pci_dev *pdev,\n\t\t\t\t  struct otx_cpt_info_buffer **pinfo,\n\t\t\t\t  struct otx_cpt_req_info *req, gfp_t gfp)\n{\n\tu32 dlen, align_dlen, info_len, rlen;\n\tstruct otx_cpt_info_buffer *info;\n\tu16 g_sz_bytes, s_sz_bytes;\n\tint align = CPT_DMA_ALIGN;\n\tu32 total_mem_len;\n\n\tif (unlikely(req->incnt > OTX_CPT_MAX_SG_IN_CNT ||\n\t\t     req->outcnt > OTX_CPT_MAX_SG_OUT_CNT)) {\n\t\tdev_err(&pdev->dev, \"Error too many sg components\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tg_sz_bytes = ((req->incnt + 3) / 4) *\n\t\t      sizeof(struct otx_cpt_sglist_component);\n\ts_sz_bytes = ((req->outcnt + 3) / 4) *\n\t\t      sizeof(struct otx_cpt_sglist_component);\n\n\tdlen = g_sz_bytes + s_sz_bytes + SG_LIST_HDR_SIZE;\n\talign_dlen = ALIGN(dlen, align);\n\tinfo_len = ALIGN(sizeof(*info), align);\n\trlen = ALIGN(sizeof(union otx_cpt_res_s), align);\n\ttotal_mem_len = align_dlen + info_len + rlen + COMPLETION_CODE_SIZE;\n\n\tinfo = kzalloc(total_mem_len, gfp);\n\tif (unlikely(!info)) {\n\t\tdev_err(&pdev->dev, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\t*pinfo = info;\n\tinfo->dlen = dlen;\n\tinfo->in_buffer = (u8 *)info + info_len;\n\n\t((__be16 *)info->in_buffer)[0] = cpu_to_be16(req->outcnt);\n\t((__be16 *)info->in_buffer)[1] = cpu_to_be16(req->incnt);\n\t((u16 *)info->in_buffer)[2] = 0;\n\t((u16 *)info->in_buffer)[3] = 0;\n\n\t \n\tif (setup_sgio_components(pdev, req->in, req->incnt,\n\t\t\t\t  &info->in_buffer[8])) {\n\t\tdev_err(&pdev->dev, \"Failed to setup gather list\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (setup_sgio_components(pdev, req->out, req->outcnt,\n\t\t\t\t  &info->in_buffer[8 + g_sz_bytes])) {\n\t\tdev_err(&pdev->dev, \"Failed to setup scatter list\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tinfo->dma_len = total_mem_len - info_len;\n\tinfo->dptr_baddr = dma_map_single(&pdev->dev, (void *)info->in_buffer,\n\t\t\t\t\t  info->dma_len, DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(&pdev->dev, info->dptr_baddr))) {\n\t\tdev_err(&pdev->dev, \"DMA Mapping failed for cpt req\\n\");\n\t\treturn -EIO;\n\t}\n\t \n\tinfo->completion_addr = (u64 *)(info->in_buffer + align_dlen);\n\tinfo->comp_baddr = info->dptr_baddr + align_dlen;\n\n\t \n\tinfo->out_buffer = (u8 *)info->completion_addr + rlen;\n\tinfo->rptr_baddr = info->comp_baddr + rlen;\n\n\t*((u64 *) info->out_buffer) = ~((u64) COMPLETION_CODE_INIT);\n\n\treturn 0;\n}\n\n\nstatic void cpt_fill_inst(union otx_cpt_inst_s *inst,\n\t\t\t  struct otx_cpt_info_buffer *info,\n\t\t\t  struct otx_cpt_iq_cmd *cmd)\n{\n\tinst->u[0] = 0x0;\n\tinst->s.doneint = true;\n\tinst->s.res_addr = (u64)info->comp_baddr;\n\tinst->u[2] = 0x0;\n\tinst->s.wq_ptr = 0;\n\tinst->s.ei0 = cmd->cmd.u64;\n\tinst->s.ei1 = cmd->dptr;\n\tinst->s.ei2 = cmd->rptr;\n\tinst->s.ei3 = cmd->cptr.u64;\n}\n\n \nstatic void cpt_send_cmd(union otx_cpt_inst_s *cptinst, struct otx_cptvf *cptvf)\n{\n\tstruct otx_cpt_cmd_qinfo *qinfo = &cptvf->cqinfo;\n\tstruct otx_cpt_cmd_queue *queue;\n\tstruct otx_cpt_cmd_chunk *curr;\n\tu8 *ent;\n\n\tqueue = &qinfo->queue[0];\n\t \n\tent = &queue->qhead->head[queue->idx * OTX_CPT_INST_SIZE];\n\tmemcpy(ent, (void *) cptinst, OTX_CPT_INST_SIZE);\n\n\tif (++queue->idx >= queue->qhead->size / 64) {\n\t\tcurr = queue->qhead;\n\n\t\tif (list_is_last(&curr->nextchunk, &queue->chead))\n\t\t\tqueue->qhead = queue->base;\n\t\telse\n\t\t\tqueue->qhead = list_next_entry(queue->qhead, nextchunk);\n\t\tqueue->idx = 0;\n\t}\n\t \n\tsmp_wmb();\n\totx_cptvf_write_vq_doorbell(cptvf, 1);\n}\n\nstatic int process_request(struct pci_dev *pdev, struct otx_cpt_req_info *req,\n\t\t\t   struct otx_cpt_pending_queue *pqueue,\n\t\t\t   struct otx_cptvf *cptvf)\n{\n\tstruct otx_cptvf_request *cpt_req = &req->req;\n\tstruct otx_cpt_pending_entry *pentry = NULL;\n\tunion otx_cpt_ctrl_info *ctrl = &req->ctrl;\n\tstruct otx_cpt_info_buffer *info = NULL;\n\tunion otx_cpt_res_s *result = NULL;\n\tstruct otx_cpt_iq_cmd iq_cmd;\n\tunion otx_cpt_inst_s cptinst;\n\tint retry, ret = 0;\n\tu8 resume_sender;\n\tgfp_t gfp;\n\n\tgfp = (req->areq->flags & CRYPTO_TFM_REQ_MAY_SLEEP) ? GFP_KERNEL :\n\t\t\t\t\t\t\t      GFP_ATOMIC;\n\tret = setup_sgio_list(pdev, &info, req, gfp);\n\tif (unlikely(ret)) {\n\t\tdev_err(&pdev->dev, \"Setting up SG list failed\\n\");\n\t\tgoto request_cleanup;\n\t}\n\tcpt_req->dlen = info->dlen;\n\n\tresult = (union otx_cpt_res_s *) info->completion_addr;\n\tresult->s.compcode = COMPLETION_CODE_INIT;\n\n\tspin_lock_bh(&pqueue->lock);\n\tpentry = get_free_pending_entry(pqueue, pqueue->qlen);\n\tretry = CPT_PENTRY_TIMEOUT / CPT_PENTRY_STEP;\n\twhile (unlikely(!pentry) && retry--) {\n\t\tspin_unlock_bh(&pqueue->lock);\n\t\tudelay(CPT_PENTRY_STEP);\n\t\tspin_lock_bh(&pqueue->lock);\n\t\tpentry = get_free_pending_entry(pqueue, pqueue->qlen);\n\t}\n\n\tif (unlikely(!pentry)) {\n\t\tret = -ENOSPC;\n\t\tspin_unlock_bh(&pqueue->lock);\n\t\tgoto request_cleanup;\n\t}\n\n\t \n\tif (gfp == GFP_KERNEL &&\n\t    pqueue->pending_count > (pqueue->qlen - CPT_IQ_STOP_MARGIN)) {\n\t\tpentry->resume_sender = true;\n\t} else\n\t\tpentry->resume_sender = false;\n\tresume_sender = pentry->resume_sender;\n\tpqueue->pending_count++;\n\n\tpentry->completion_addr = info->completion_addr;\n\tpentry->info = info;\n\tpentry->callback = req->callback;\n\tpentry->areq = req->areq;\n\tpentry->busy = true;\n\tinfo->pentry = pentry;\n\tinfo->time_in = jiffies;\n\tinfo->req = req;\n\n\t \n\tiq_cmd.cmd.u64 = 0;\n\tiq_cmd.cmd.s.opcode = cpu_to_be16(cpt_req->opcode.flags);\n\tiq_cmd.cmd.s.param1 = cpu_to_be16(cpt_req->param1);\n\tiq_cmd.cmd.s.param2 = cpu_to_be16(cpt_req->param2);\n\tiq_cmd.cmd.s.dlen   = cpu_to_be16(cpt_req->dlen);\n\n\tiq_cmd.dptr = info->dptr_baddr;\n\tiq_cmd.rptr = info->rptr_baddr;\n\tiq_cmd.cptr.u64 = 0;\n\tiq_cmd.cptr.s.grp = ctrl->s.grp;\n\n\t \n\tcpt_fill_inst(&cptinst, info, &iq_cmd);\n\n\t \n\totx_cpt_dump_sg_list(pdev, req);\n\tpr_debug(\"Cpt_inst_s hexdump (%d bytes)\\n\", OTX_CPT_INST_SIZE);\n\tprint_hex_dump_debug(\"\", 0, 16, 1, &cptinst, OTX_CPT_INST_SIZE, false);\n\tpr_debug(\"Dptr hexdump (%d bytes)\\n\", cpt_req->dlen);\n\tprint_hex_dump_debug(\"\", 0, 16, 1, info->in_buffer,\n\t\t\t     cpt_req->dlen, false);\n\n\t \n\tcpt_send_cmd(&cptinst, cptvf);\n\n\t \n\tspin_unlock_bh(&pqueue->lock);\n\n\tret = resume_sender ? -EBUSY : -EINPROGRESS;\n\treturn ret;\n\nrequest_cleanup:\n\tdo_request_cleanup(pdev, info);\n\treturn ret;\n}\n\nint otx_cpt_do_request(struct pci_dev *pdev, struct otx_cpt_req_info *req,\n\t\t       int cpu_num)\n{\n\tstruct otx_cptvf *cptvf = pci_get_drvdata(pdev);\n\n\tif (!otx_cpt_device_ready(cptvf)) {\n\t\tdev_err(&pdev->dev, \"CPT Device is not ready\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif ((cptvf->vftype == OTX_CPT_SE_TYPES) && (!req->ctrl.s.se_req)) {\n\t\tdev_err(&pdev->dev, \"CPTVF-%d of SE TYPE got AE request\\n\",\n\t\t\tcptvf->vfid);\n\t\treturn -EINVAL;\n\t} else if ((cptvf->vftype == OTX_CPT_AE_TYPES) &&\n\t\t   (req->ctrl.s.se_req)) {\n\t\tdev_err(&pdev->dev, \"CPTVF-%d of AE TYPE got SE request\\n\",\n\t\t\tcptvf->vfid);\n\t\treturn -EINVAL;\n\t}\n\n\treturn process_request(pdev, req, &cptvf->pqinfo.queue[0], cptvf);\n}\n\nstatic int cpt_process_ccode(struct pci_dev *pdev,\n\t\t\t     union otx_cpt_res_s *cpt_status,\n\t\t\t     struct otx_cpt_info_buffer *cpt_info,\n\t\t\t     struct otx_cpt_req_info *req, u32 *res_code)\n{\n\tu8 ccode = cpt_status->s.compcode;\n\tunion otx_cpt_error_code ecode;\n\n\tecode.u = be64_to_cpup((__be64 *)cpt_info->out_buffer);\n\tswitch (ccode) {\n\tcase CPT_COMP_E_FAULT:\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Request failed with DMA fault\\n\");\n\t\totx_cpt_dump_sg_list(pdev, req);\n\t\tbreak;\n\n\tcase CPT_COMP_E_SWERR:\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Request failed with software error code %d\\n\",\n\t\t\tecode.s.ccode);\n\t\totx_cpt_dump_sg_list(pdev, req);\n\t\tbreak;\n\n\tcase CPT_COMP_E_HWERR:\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Request failed with hardware error\\n\");\n\t\totx_cpt_dump_sg_list(pdev, req);\n\t\tbreak;\n\n\tcase COMPLETION_CODE_INIT:\n\t\t \n\t\tif (time_after_eq(jiffies, cpt_info->time_in +\n\t\t\t\t  OTX_CPT_COMMAND_TIMEOUT * HZ))\n\t\t\tdev_warn(&pdev->dev, \"Request timed out 0x%p\\n\", req);\n\t\telse if (cpt_info->extra_time < OTX_CPT_TIME_IN_RESET_COUNT) {\n\t\t\tcpt_info->time_in = jiffies;\n\t\t\tcpt_info->extra_time++;\n\t\t}\n\t\treturn 1;\n\n\tcase CPT_COMP_E_GOOD:\n\t\t \n\t\tif (ecode.s.ccode) {\n\t\t\t \n\t\t\tif (req->is_trunc_hmac &&\n\t\t\t    ecode.s.ccode == ERR_SCATTER_GATHER_WRITE_LENGTH) {\n\t\t\t\t*res_code = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Request failed with software error code 0x%x\\n\",\n\t\t\t\tecode.s.ccode);\n\t\t\totx_cpt_dump_sg_list(pdev, req);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\t*res_code = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&pdev->dev, \"Request returned invalid status\\n\");\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void process_pending_queue(struct pci_dev *pdev,\n\t\t\t\t\t struct otx_cpt_pending_queue *pqueue)\n{\n\tvoid (*callback)(int status, void *arg1, void *arg2);\n\tstruct otx_cpt_pending_entry *resume_pentry = NULL;\n\tstruct otx_cpt_pending_entry *pentry = NULL;\n\tstruct otx_cpt_info_buffer *cpt_info = NULL;\n\tunion otx_cpt_res_s *cpt_status = NULL;\n\tstruct otx_cpt_req_info *req = NULL;\n\tstruct crypto_async_request *areq;\n\tu32 res_code, resume_index;\n\n\twhile (1) {\n\t\tspin_lock_bh(&pqueue->lock);\n\t\tpentry = &pqueue->head[pqueue->front];\n\n\t\tif (WARN_ON(!pentry)) {\n\t\t\tspin_unlock_bh(&pqueue->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tres_code = -EINVAL;\n\t\tif (unlikely(!pentry->busy)) {\n\t\t\tspin_unlock_bh(&pqueue->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!pentry->callback)) {\n\t\t\tdev_err(&pdev->dev, \"Callback NULL\\n\");\n\t\t\tgoto process_pentry;\n\t\t}\n\n\t\tcpt_info = pentry->info;\n\t\tif (unlikely(!cpt_info)) {\n\t\t\tdev_err(&pdev->dev, \"Pending entry post arg NULL\\n\");\n\t\t\tgoto process_pentry;\n\t\t}\n\n\t\treq = cpt_info->req;\n\t\tif (unlikely(!req)) {\n\t\t\tdev_err(&pdev->dev, \"Request NULL\\n\");\n\t\t\tgoto process_pentry;\n\t\t}\n\n\t\tcpt_status = (union otx_cpt_res_s *) pentry->completion_addr;\n\t\tif (unlikely(!cpt_status)) {\n\t\t\tdev_err(&pdev->dev, \"Completion address NULL\\n\");\n\t\t\tgoto process_pentry;\n\t\t}\n\n\t\tif (cpt_process_ccode(pdev, cpt_status, cpt_info, req,\n\t\t\t\t      &res_code)) {\n\t\t\tspin_unlock_bh(&pqueue->lock);\n\t\t\treturn;\n\t\t}\n\t\tcpt_info->pdev = pdev;\n\nprocess_pentry:\n\t\t \n\t\tresume_index = modulo_inc(pqueue->front, pqueue->qlen,\n\t\t\t\t\t  CPT_IQ_RESUME_MARGIN);\n\t\tresume_pentry = &pqueue->head[resume_index];\n\t\tif (resume_pentry &&\n\t\t    resume_pentry->resume_sender) {\n\t\t\tresume_pentry->resume_sender = false;\n\t\t\tcallback = resume_pentry->callback;\n\t\t\tareq = resume_pentry->areq;\n\n\t\t\tif (callback) {\n\t\t\t\tspin_unlock_bh(&pqueue->lock);\n\n\t\t\t\t \n\t\t\t\tcallback(-EINPROGRESS, areq, cpt_info);\n\t\t\t\tspin_lock_bh(&pqueue->lock);\n\t\t\t}\n\t\t}\n\n\t\tcallback = pentry->callback;\n\t\tareq = pentry->areq;\n\t\tfree_pentry(pentry);\n\n\t\tpqueue->pending_count--;\n\t\tpqueue->front = modulo_inc(pqueue->front, pqueue->qlen, 1);\n\t\tspin_unlock_bh(&pqueue->lock);\n\n\t\t \n\t\tif (callback)\n\t\t\tcallback(res_code, areq, cpt_info);\n\t}\n}\n\nvoid otx_cpt_post_process(struct otx_cptvf_wqe *wqe)\n{\n\tprocess_pending_queue(wqe->cptvf->pdev, &wqe->cptvf->pqinfo.queue[0]);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}