{
  "module_name": "otx_cptvf_algs.c",
  "hash_id": "13b86875c82374d56d4c0843746595fee6dd12d842a33fb810fd26bf46db4f75",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/marvell/octeontx/otx_cptvf_algs.c",
  "human_readable_source": "\n \n\n#include <crypto/aes.h>\n#include <crypto/authenc.h>\n#include <crypto/cryptd.h>\n#include <crypto/des.h>\n#include <crypto/internal/aead.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/xts.h>\n#include <crypto/scatterwalk.h>\n#include <linux/rtnetlink.h>\n#include <linux/sort.h>\n#include <linux/module.h>\n#include \"otx_cptvf.h\"\n#include \"otx_cptvf_algs.h\"\n#include \"otx_cptvf_reqmgr.h\"\n\n#define CPT_MAX_VF_NUM\t64\n \n#define AES_GCM_SALT_SIZE\t4\n \n#define AES_GCM_IV_SIZE\t\t8\n \n#define AES_GCM_ICV_SIZE\t16\n \n#define AES_GCM_IV_OFFSET\t8\n#define CONTROL_WORD_LEN\t8\n#define KEY2_OFFSET\t\t48\n#define DMA_MODE_FLAG(dma_mode) \\\n\t(((dma_mode) == OTX_CPT_DMA_GATHER_SCATTER) ? (1 << 7) : 0)\n\n \n#define SHA1_TRUNC_DIGEST_SIZE\t\t12\n#define SHA256_TRUNC_DIGEST_SIZE\t16\n#define SHA384_TRUNC_DIGEST_SIZE\t24\n#define SHA512_TRUNC_DIGEST_SIZE\t32\n\nstatic DEFINE_MUTEX(mutex);\nstatic int is_crypto_registered;\n\nstruct cpt_device_desc {\n\tenum otx_cptpf_type pf_type;\n\tstruct pci_dev *dev;\n\tint num_queues;\n};\n\nstruct cpt_device_table {\n\tatomic_t count;\n\tstruct cpt_device_desc desc[CPT_MAX_VF_NUM];\n};\n\nstatic struct cpt_device_table se_devices = {\n\t.count = ATOMIC_INIT(0)\n};\n\nstatic struct cpt_device_table ae_devices = {\n\t.count = ATOMIC_INIT(0)\n};\n\nstatic inline int get_se_device(struct pci_dev **pdev, int *cpu_num)\n{\n\tint count, ret = 0;\n\n\tcount = atomic_read(&se_devices.count);\n\tif (count < 1)\n\t\treturn -ENODEV;\n\n\t*cpu_num = get_cpu();\n\n\tif (se_devices.desc[0].pf_type == OTX_CPT_SE) {\n\t\t \n\t\tif (*cpu_num >= count)\n\t\t\t*cpu_num %= count;\n\t\t*pdev = se_devices.desc[*cpu_num].dev;\n\t} else {\n\t\tpr_err(\"Unknown PF type %d\\n\", se_devices.desc[0].pf_type);\n\t\tret = -EINVAL;\n\t}\n\tput_cpu();\n\n\treturn ret;\n}\n\nstatic inline int validate_hmac_cipher_null(struct otx_cpt_req_info *cpt_req)\n{\n\tstruct otx_cpt_req_ctx *rctx;\n\tstruct aead_request *req;\n\tstruct crypto_aead *tfm;\n\n\treq = container_of(cpt_req->areq, struct aead_request, base);\n\ttfm = crypto_aead_reqtfm(req);\n\trctx = aead_request_ctx_dma(req);\n\tif (memcmp(rctx->fctx.hmac.s.hmac_calc,\n\t\t   rctx->fctx.hmac.s.hmac_recv,\n\t\t   crypto_aead_authsize(tfm)) != 0)\n\t\treturn -EBADMSG;\n\n\treturn 0;\n}\n\nstatic void otx_cpt_aead_callback(int status, void *arg1, void *arg2)\n{\n\tstruct otx_cpt_info_buffer *cpt_info = arg2;\n\tstruct crypto_async_request *areq = arg1;\n\tstruct otx_cpt_req_info *cpt_req;\n\tstruct pci_dev *pdev;\n\n\tif (!cpt_info)\n\t\tgoto complete;\n\n\tcpt_req = cpt_info->req;\n\tif (!status) {\n\t\t \n\t\tif (cpt_req->req_type == OTX_CPT_AEAD_ENC_DEC_NULL_REQ &&\n\t\t    !cpt_req->is_enc)\n\t\t\tstatus = validate_hmac_cipher_null(cpt_req);\n\t}\n\tpdev = cpt_info->pdev;\n\tdo_request_cleanup(pdev, cpt_info);\n\ncomplete:\n\tif (areq)\n\t\tcrypto_request_complete(areq, status);\n}\n\nstatic void output_iv_copyback(struct crypto_async_request *areq)\n{\n\tstruct otx_cpt_req_info *req_info;\n\tstruct skcipher_request *sreq;\n\tstruct crypto_skcipher *stfm;\n\tstruct otx_cpt_req_ctx *rctx;\n\tstruct otx_cpt_enc_ctx *ctx;\n\tu32 start, ivsize;\n\n\tsreq = container_of(areq, struct skcipher_request, base);\n\tstfm = crypto_skcipher_reqtfm(sreq);\n\tctx = crypto_skcipher_ctx(stfm);\n\tif (ctx->cipher_type == OTX_CPT_AES_CBC ||\n\t    ctx->cipher_type == OTX_CPT_DES3_CBC) {\n\t\trctx = skcipher_request_ctx_dma(sreq);\n\t\treq_info = &rctx->cpt_req;\n\t\tivsize = crypto_skcipher_ivsize(stfm);\n\t\tstart = sreq->cryptlen - ivsize;\n\n\t\tif (req_info->is_enc) {\n\t\t\tscatterwalk_map_and_copy(sreq->iv, sreq->dst, start,\n\t\t\t\t\t\t ivsize, 0);\n\t\t} else {\n\t\t\tif (sreq->src != sreq->dst) {\n\t\t\t\tscatterwalk_map_and_copy(sreq->iv, sreq->src,\n\t\t\t\t\t\t\t start, ivsize, 0);\n\t\t\t} else {\n\t\t\t\tmemcpy(sreq->iv, req_info->iv_out, ivsize);\n\t\t\t\tkfree(req_info->iv_out);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void otx_cpt_skcipher_callback(int status, void *arg1, void *arg2)\n{\n\tstruct otx_cpt_info_buffer *cpt_info = arg2;\n\tstruct crypto_async_request *areq = arg1;\n\tstruct pci_dev *pdev;\n\n\tif (areq) {\n\t\tif (!status)\n\t\t\toutput_iv_copyback(areq);\n\t\tif (cpt_info) {\n\t\t\tpdev = cpt_info->pdev;\n\t\t\tdo_request_cleanup(pdev, cpt_info);\n\t\t}\n\t\tcrypto_request_complete(areq, status);\n\t}\n}\n\nstatic inline void update_input_data(struct otx_cpt_req_info *req_info,\n\t\t\t\t     struct scatterlist *inp_sg,\n\t\t\t\t     u32 nbytes, u32 *argcnt)\n{\n\treq_info->req.dlen += nbytes;\n\n\twhile (nbytes) {\n\t\tu32 len = min(nbytes, inp_sg->length);\n\t\tu8 *ptr = sg_virt(inp_sg);\n\n\t\treq_info->in[*argcnt].vptr = (void *)ptr;\n\t\treq_info->in[*argcnt].size = len;\n\t\tnbytes -= len;\n\t\t++(*argcnt);\n\t\tinp_sg = sg_next(inp_sg);\n\t}\n}\n\nstatic inline void update_output_data(struct otx_cpt_req_info *req_info,\n\t\t\t\t      struct scatterlist *outp_sg,\n\t\t\t\t      u32 offset, u32 nbytes, u32 *argcnt)\n{\n\treq_info->rlen += nbytes;\n\n\twhile (nbytes) {\n\t\tu32 len = min(nbytes, outp_sg->length - offset);\n\t\tu8 *ptr = sg_virt(outp_sg);\n\n\t\treq_info->out[*argcnt].vptr = (void *) (ptr + offset);\n\t\treq_info->out[*argcnt].size = len;\n\t\tnbytes -= len;\n\t\t++(*argcnt);\n\t\toffset = 0;\n\t\toutp_sg = sg_next(outp_sg);\n\t}\n}\n\nstatic inline u32 create_ctx_hdr(struct skcipher_request *req, u32 enc,\n\t\t\t\t u32 *argcnt)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct otx_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(stfm);\n\tstruct otx_cpt_enc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct otx_cpt_fc_ctx *fctx = &rctx->fctx;\n\tint ivsize = crypto_skcipher_ivsize(stfm);\n\tu32 start = req->cryptlen - ivsize;\n\tgfp_t flags;\n\n\tflags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t\tGFP_KERNEL : GFP_ATOMIC;\n\treq_info->ctrl.s.dma_mode = OTX_CPT_DMA_GATHER_SCATTER;\n\treq_info->ctrl.s.se_req = OTX_CPT_SE_CORE_REQ;\n\n\treq_info->req.opcode.s.major = OTX_CPT_MAJOR_OP_FC |\n\t\t\t\tDMA_MODE_FLAG(OTX_CPT_DMA_GATHER_SCATTER);\n\tif (enc) {\n\t\treq_info->req.opcode.s.minor = 2;\n\t} else {\n\t\treq_info->req.opcode.s.minor = 3;\n\t\tif ((ctx->cipher_type == OTX_CPT_AES_CBC ||\n\t\t    ctx->cipher_type == OTX_CPT_DES3_CBC) &&\n\t\t    req->src == req->dst) {\n\t\t\treq_info->iv_out = kmalloc(ivsize, flags);\n\t\t\tif (!req_info->iv_out)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tscatterwalk_map_and_copy(req_info->iv_out, req->src,\n\t\t\t\t\t\t start, ivsize, 0);\n\t\t}\n\t}\n\t \n\treq_info->req.param1 = req->cryptlen;\n\t \n\treq_info->req.param2 = 0;\n\n\tfctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;\n\tfctx->enc.enc_ctrl.e.aes_key = ctx->key_type;\n\tfctx->enc.enc_ctrl.e.iv_source = OTX_CPT_FROM_CPTR;\n\n\tif (ctx->cipher_type == OTX_CPT_AES_XTS)\n\t\tmemcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len * 2);\n\telse\n\t\tmemcpy(fctx->enc.encr_key, ctx->enc_key, ctx->key_len);\n\n\tmemcpy(fctx->enc.encr_iv, req->iv, crypto_skcipher_ivsize(stfm));\n\n\tfctx->enc.enc_ctrl.flags = cpu_to_be64(fctx->enc.enc_ctrl.cflags);\n\n\t \n\treq_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;\n\treq_info->in[*argcnt].size = CONTROL_WORD_LEN;\n\treq_info->req.dlen += CONTROL_WORD_LEN;\n\t++(*argcnt);\n\n\treq_info->in[*argcnt].vptr = (u8 *)fctx;\n\treq_info->in[*argcnt].size = sizeof(struct otx_cpt_fc_ctx);\n\treq_info->req.dlen += sizeof(struct otx_cpt_fc_ctx);\n\n\t++(*argcnt);\n\n\treturn 0;\n}\n\nstatic inline u32 create_input_list(struct skcipher_request *req, u32 enc,\n\t\t\t\t    u32 enc_iv_len)\n{\n\tstruct otx_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 argcnt =  0;\n\tint ret;\n\n\tret = create_ctx_hdr(req, enc, &argcnt);\n\tif (ret)\n\t\treturn ret;\n\n\tupdate_input_data(req_info, req->src, req->cryptlen, &argcnt);\n\treq_info->incnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline void create_output_list(struct skcipher_request *req,\n\t\t\t\t      u32 enc_iv_len)\n{\n\tstruct otx_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 argcnt = 0;\n\n\t \n\tupdate_output_data(req_info, req->dst, 0, req->cryptlen, &argcnt);\n\treq_info->outcnt = argcnt;\n}\n\nstatic inline int cpt_enc_dec(struct skcipher_request *req, u32 enc)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct otx_cpt_req_ctx *rctx = skcipher_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 enc_iv_len = crypto_skcipher_ivsize(stfm);\n\tstruct pci_dev *pdev;\n\tint status, cpu_num;\n\n\t \n\tif (req->cryptlen > OTX_CPT_MAX_REQ_SIZE)\n\t\treturn -E2BIG;\n\n\t \n\trctx->ctrl_word.flags = 0;\n\trctx->fctx.enc.enc_ctrl.flags = 0;\n\n\tstatus = create_input_list(req, enc, enc_iv_len);\n\tif (status)\n\t\treturn status;\n\tcreate_output_list(req, enc_iv_len);\n\n\tstatus = get_se_device(&pdev, &cpu_num);\n\tif (status)\n\t\treturn status;\n\n\treq_info->callback = (void *)otx_cpt_skcipher_callback;\n\treq_info->areq = &req->base;\n\treq_info->req_type = OTX_CPT_ENC_DEC_REQ;\n\treq_info->is_enc = enc;\n\treq_info->is_trunc_hmac = false;\n\treq_info->ctrl.s.grp = 0;\n\n\t \n\tstatus = otx_cpt_do_request(pdev, req_info, cpu_num);\n\n\treturn status;\n}\n\nstatic int otx_cpt_skcipher_encrypt(struct skcipher_request *req)\n{\n\treturn cpt_enc_dec(req, true);\n}\n\nstatic int otx_cpt_skcipher_decrypt(struct skcipher_request *req)\n{\n\treturn cpt_enc_dec(req, false);\n}\n\nstatic int otx_cpt_skcipher_xts_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t       const u8 *key, u32 keylen)\n{\n\tstruct otx_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tconst u8 *key2 = key + (keylen / 2);\n\tconst u8 *key1 = key;\n\tint ret;\n\n\tret = xts_verify_key(tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\tctx->key_len = keylen;\n\tmemcpy(ctx->enc_key, key1, keylen / 2);\n\tmemcpy(ctx->enc_key + KEY2_OFFSET, key2, keylen / 2);\n\tctx->cipher_type = OTX_CPT_AES_XTS;\n\tswitch (ctx->key_len) {\n\tcase 2 * AES_KEYSIZE_128:\n\t\tctx->key_type = OTX_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase 2 * AES_KEYSIZE_256:\n\t\tctx->key_type = OTX_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int cpt_des_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t  u32 keylen, u8 cipher_type)\n{\n\tstruct otx_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (keylen != DES3_EDE_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\tctx->key_len = keylen;\n\tctx->cipher_type = cipher_type;\n\n\tmemcpy(ctx->enc_key, key, keylen);\n\n\treturn 0;\n}\n\nstatic int cpt_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t  u32 keylen, u8 cipher_type)\n{\n\tstruct otx_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128:\n\t\tctx->key_type = OTX_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tctx->key_type = OTX_CPT_AES_192_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tctx->key_type = OTX_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tctx->key_len = keylen;\n\tctx->cipher_type = cipher_type;\n\n\tmemcpy(ctx->enc_key, key, keylen);\n\n\treturn 0;\n}\n\nstatic int otx_cpt_skcipher_cbc_aes_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t   const u8 *key, u32 keylen)\n{\n\treturn cpt_aes_setkey(tfm, key, keylen, OTX_CPT_AES_CBC);\n}\n\nstatic int otx_cpt_skcipher_ecb_aes_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t   const u8 *key, u32 keylen)\n{\n\treturn cpt_aes_setkey(tfm, key, keylen, OTX_CPT_AES_ECB);\n}\n\nstatic int otx_cpt_skcipher_cfb_aes_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t   const u8 *key, u32 keylen)\n{\n\treturn cpt_aes_setkey(tfm, key, keylen, OTX_CPT_AES_CFB);\n}\n\nstatic int otx_cpt_skcipher_cbc_des3_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t    const u8 *key, u32 keylen)\n{\n\treturn cpt_des_setkey(tfm, key, keylen, OTX_CPT_DES3_CBC);\n}\n\nstatic int otx_cpt_skcipher_ecb_des3_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t\t    const u8 *key, u32 keylen)\n{\n\treturn cpt_des_setkey(tfm, key, keylen, OTX_CPT_DES3_ECB);\n}\n\nstatic int otx_cpt_enc_dec_init(struct crypto_skcipher *tfm)\n{\n\tstruct otx_cpt_enc_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tmemset(ctx, 0, sizeof(*ctx));\n\t \n\tcrypto_skcipher_set_reqsize_dma(\n\t\ttfm, sizeof(struct otx_cpt_req_ctx) +\n\t\t     sizeof(struct skcipher_request));\n\n\treturn 0;\n}\n\nstatic int cpt_aead_init(struct crypto_aead *tfm, u8 cipher_type, u8 mac_type)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tctx->cipher_type = cipher_type;\n\tctx->mac_type = mac_type;\n\n\t \n\tif (ctx->cipher_type != OTX_CPT_CIPHER_NULL) {\n\t\tswitch (ctx->mac_type) {\n\t\tcase OTX_CPT_SHA1:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha1\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX_CPT_SHA256:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha256\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX_CPT_SHA384:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha384\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\n\t\tcase OTX_CPT_SHA512:\n\t\t\tctx->hashalg = crypto_alloc_shash(\"sha512\", 0,\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC);\n\t\t\tif (IS_ERR(ctx->hashalg))\n\t\t\t\treturn PTR_ERR(ctx->hashalg);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcrypto_aead_set_reqsize_dma(tfm, sizeof(struct otx_cpt_req_ctx));\n\n\treturn 0;\n}\n\nstatic int otx_cpt_aead_cbc_aes_sha1_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_AES_CBC, OTX_CPT_SHA1);\n}\n\nstatic int otx_cpt_aead_cbc_aes_sha256_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_AES_CBC, OTX_CPT_SHA256);\n}\n\nstatic int otx_cpt_aead_cbc_aes_sha384_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_AES_CBC, OTX_CPT_SHA384);\n}\n\nstatic int otx_cpt_aead_cbc_aes_sha512_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_AES_CBC, OTX_CPT_SHA512);\n}\n\nstatic int otx_cpt_aead_ecb_null_sha1_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_CIPHER_NULL, OTX_CPT_SHA1);\n}\n\nstatic int otx_cpt_aead_ecb_null_sha256_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_CIPHER_NULL, OTX_CPT_SHA256);\n}\n\nstatic int otx_cpt_aead_ecb_null_sha384_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_CIPHER_NULL, OTX_CPT_SHA384);\n}\n\nstatic int otx_cpt_aead_ecb_null_sha512_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_CIPHER_NULL, OTX_CPT_SHA512);\n}\n\nstatic int otx_cpt_aead_gcm_aes_init(struct crypto_aead *tfm)\n{\n\treturn cpt_aead_init(tfm, OTX_CPT_AES_GCM, OTX_CPT_MAC_NULL);\n}\n\nstatic void otx_cpt_aead_exit(struct crypto_aead *tfm)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tkfree(ctx->ipad);\n\tkfree(ctx->opad);\n\tif (ctx->hashalg)\n\t\tcrypto_free_shash(ctx->hashalg);\n\tkfree(ctx->sdesc);\n}\n\n \nstatic int otx_cpt_aead_set_authsize(struct crypto_aead *tfm,\n\t\t\t\t     unsigned int authsize)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\n\tswitch (ctx->mac_type) {\n\tcase OTX_CPT_SHA1:\n\t\tif (authsize != SHA1_DIGEST_SIZE &&\n\t\t    authsize != SHA1_TRUNC_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (authsize == SHA1_TRUNC_DIGEST_SIZE)\n\t\t\tctx->is_trunc_hmac = true;\n\t\tbreak;\n\n\tcase OTX_CPT_SHA256:\n\t\tif (authsize != SHA256_DIGEST_SIZE &&\n\t\t    authsize != SHA256_TRUNC_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (authsize == SHA256_TRUNC_DIGEST_SIZE)\n\t\t\tctx->is_trunc_hmac = true;\n\t\tbreak;\n\n\tcase OTX_CPT_SHA384:\n\t\tif (authsize != SHA384_DIGEST_SIZE &&\n\t\t    authsize != SHA384_TRUNC_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (authsize == SHA384_TRUNC_DIGEST_SIZE)\n\t\t\tctx->is_trunc_hmac = true;\n\t\tbreak;\n\n\tcase OTX_CPT_SHA512:\n\t\tif (authsize != SHA512_DIGEST_SIZE &&\n\t\t    authsize != SHA512_TRUNC_DIGEST_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (authsize == SHA512_TRUNC_DIGEST_SIZE)\n\t\t\tctx->is_trunc_hmac = true;\n\t\tbreak;\n\n\tcase OTX_CPT_MAC_NULL:\n\t\tif (ctx->cipher_type == OTX_CPT_AES_GCM) {\n\t\t\tif (authsize != AES_GCM_ICV_SIZE)\n\t\t\t\treturn -EINVAL;\n\t\t} else\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\ttfm->authsize = authsize;\n\treturn 0;\n}\n\nstatic struct otx_cpt_sdesc *alloc_sdesc(struct crypto_shash *alg)\n{\n\tstruct otx_cpt_sdesc *sdesc;\n\tint size;\n\n\tsize = sizeof(struct shash_desc) + crypto_shash_descsize(alg);\n\tsdesc = kmalloc(size, GFP_KERNEL);\n\tif (!sdesc)\n\t\treturn NULL;\n\n\tsdesc->shash.tfm = alg;\n\n\treturn sdesc;\n}\n\nstatic inline void swap_data32(void *buf, u32 len)\n{\n\tcpu_to_be32_array(buf, buf, len / 4);\n}\n\nstatic inline void swap_data64(void *buf, u32 len)\n{\n\t__be64 *dst = buf;\n\tu64 *src = buf;\n\tint i = 0;\n\n\tfor (i = 0 ; i < len / 8; i++, src++, dst++)\n\t\t*dst = cpu_to_be64p(src);\n}\n\nstatic int copy_pad(u8 mac_type, u8 *out_pad, u8 *in_pad)\n{\n\tstruct sha512_state *sha512;\n\tstruct sha256_state *sha256;\n\tstruct sha1_state *sha1;\n\n\tswitch (mac_type) {\n\tcase OTX_CPT_SHA1:\n\t\tsha1 = (struct sha1_state *) in_pad;\n\t\tswap_data32(sha1->state, SHA1_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha1->state, SHA1_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase OTX_CPT_SHA256:\n\t\tsha256 = (struct sha256_state *) in_pad;\n\t\tswap_data32(sha256->state, SHA256_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha256->state, SHA256_DIGEST_SIZE);\n\t\tbreak;\n\n\tcase OTX_CPT_SHA384:\n\tcase OTX_CPT_SHA512:\n\t\tsha512 = (struct sha512_state *) in_pad;\n\t\tswap_data64(sha512->state, SHA512_DIGEST_SIZE);\n\t\tmemcpy(out_pad, &sha512->state, SHA512_DIGEST_SIZE);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int aead_hmac_init(struct crypto_aead *cipher)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tint state_size = crypto_shash_statesize(ctx->hashalg);\n\tint ds = crypto_shash_digestsize(ctx->hashalg);\n\tint bs = crypto_shash_blocksize(ctx->hashalg);\n\tint authkeylen = ctx->auth_key_len;\n\tu8 *ipad = NULL, *opad = NULL;\n\tint ret = 0, icount = 0;\n\n\tctx->sdesc = alloc_sdesc(ctx->hashalg);\n\tif (!ctx->sdesc)\n\t\treturn -ENOMEM;\n\n\tctx->ipad = kzalloc(bs, GFP_KERNEL);\n\tif (!ctx->ipad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tctx->opad = kzalloc(bs, GFP_KERNEL);\n\tif (!ctx->opad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tipad = kzalloc(state_size, GFP_KERNEL);\n\tif (!ipad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\topad = kzalloc(state_size, GFP_KERNEL);\n\tif (!opad) {\n\t\tret = -ENOMEM;\n\t\tgoto calc_fail;\n\t}\n\n\tif (authkeylen > bs) {\n\t\tret = crypto_shash_digest(&ctx->sdesc->shash, ctx->key,\n\t\t\t\t\t  authkeylen, ipad);\n\t\tif (ret)\n\t\t\tgoto calc_fail;\n\n\t\tauthkeylen = ds;\n\t} else {\n\t\tmemcpy(ipad, ctx->key, authkeylen);\n\t}\n\n\tmemset(ipad + authkeylen, 0, bs - authkeylen);\n\tmemcpy(opad, ipad, bs);\n\n\tfor (icount = 0; icount < bs; icount++) {\n\t\tipad[icount] ^= 0x36;\n\t\topad[icount] ^= 0x5c;\n\t}\n\n\t \n\n\t \n\tcrypto_shash_init(&ctx->sdesc->shash);\n\tcrypto_shash_update(&ctx->sdesc->shash, ipad, bs);\n\tcrypto_shash_export(&ctx->sdesc->shash, ipad);\n\tret = copy_pad(ctx->mac_type, ctx->ipad, ipad);\n\tif (ret)\n\t\tgoto calc_fail;\n\n\t \n\tcrypto_shash_init(&ctx->sdesc->shash);\n\tcrypto_shash_update(&ctx->sdesc->shash, opad, bs);\n\tcrypto_shash_export(&ctx->sdesc->shash, opad);\n\tret = copy_pad(ctx->mac_type, ctx->opad, opad);\n\tif (ret)\n\t\tgoto calc_fail;\n\n\tkfree(ipad);\n\tkfree(opad);\n\n\treturn 0;\n\ncalc_fail:\n\tkfree(ctx->ipad);\n\tctx->ipad = NULL;\n\tkfree(ctx->opad);\n\tctx->opad = NULL;\n\tkfree(ipad);\n\tkfree(opad);\n\tkfree(ctx->sdesc);\n\tctx->sdesc = NULL;\n\n\treturn ret;\n}\n\nstatic int otx_cpt_aead_cbc_aes_sha_setkey(struct crypto_aead *cipher,\n\t\t\t\t\t   const unsigned char *key,\n\t\t\t\t\t   unsigned int keylen)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tstruct crypto_authenc_key_param *param;\n\tint enckeylen = 0, authkeylen = 0;\n\tstruct rtattr *rta = (void *)key;\n\tint status = -EINVAL;\n\n\tif (!RTA_OK(rta, keylen))\n\t\tgoto badkey;\n\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\tgoto badkey;\n\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\tgoto badkey;\n\n\tparam = RTA_DATA(rta);\n\tenckeylen = be32_to_cpu(param->enckeylen);\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\tif (keylen < enckeylen)\n\t\tgoto badkey;\n\n\tif (keylen > OTX_CPT_MAX_KEY_SIZE)\n\t\tgoto badkey;\n\n\tauthkeylen = keylen - enckeylen;\n\tmemcpy(ctx->key, key, keylen);\n\n\tswitch (enckeylen) {\n\tcase AES_KEYSIZE_128:\n\t\tctx->key_type = OTX_CPT_AES_128_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tctx->key_type = OTX_CPT_AES_192_BIT;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tctx->key_type = OTX_CPT_AES_256_BIT;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tgoto badkey;\n\t}\n\n\tctx->enc_key_len = enckeylen;\n\tctx->auth_key_len = authkeylen;\n\n\tstatus = aead_hmac_init(cipher);\n\tif (status)\n\t\tgoto badkey;\n\n\treturn 0;\nbadkey:\n\treturn status;\n}\n\nstatic int otx_cpt_aead_ecb_null_sha_setkey(struct crypto_aead *cipher,\n\t\t\t\t\t    const unsigned char *key,\n\t\t\t\t\t    unsigned int keylen)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta = (void *)key;\n\tint enckeylen = 0;\n\n\tif (!RTA_OK(rta, keylen))\n\t\tgoto badkey;\n\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\tgoto badkey;\n\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\tgoto badkey;\n\n\tparam = RTA_DATA(rta);\n\tenckeylen = be32_to_cpu(param->enckeylen);\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\tif (enckeylen != 0)\n\t\tgoto badkey;\n\n\tif (keylen > OTX_CPT_MAX_KEY_SIZE)\n\t\tgoto badkey;\n\n\tmemcpy(ctx->key, key, keylen);\n\tctx->enc_key_len = enckeylen;\n\tctx->auth_key_len = keylen;\n\treturn 0;\nbadkey:\n\treturn -EINVAL;\n}\n\nstatic int otx_cpt_aead_gcm_aes_setkey(struct crypto_aead *cipher,\n\t\t\t\t       const unsigned char *key,\n\t\t\t\t       unsigned int keylen)\n{\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(cipher);\n\n\t \n\tswitch (keylen) {\n\tcase AES_KEYSIZE_128 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX_CPT_AES_128_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_128;\n\t\tbreak;\n\tcase AES_KEYSIZE_192 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX_CPT_AES_192_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_192;\n\t\tbreak;\n\tcase AES_KEYSIZE_256 + AES_GCM_SALT_SIZE:\n\t\tctx->key_type = OTX_CPT_AES_256_BIT;\n\t\tctx->enc_key_len = AES_KEYSIZE_256;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmemcpy(ctx->key, key, keylen);\n\n\treturn 0;\n}\n\nstatic inline u32 create_aead_ctx_hdr(struct aead_request *req, u32 enc,\n\t\t\t\t      u32 *argcnt)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct otx_cpt_fc_ctx *fctx = &rctx->fctx;\n\tint mac_len = crypto_aead_authsize(tfm);\n\tint ds;\n\n\trctx->ctrl_word.e.enc_data_offset = req->assoclen;\n\n\tswitch (ctx->cipher_type) {\n\tcase OTX_CPT_AES_CBC:\n\t\tfctx->enc.enc_ctrl.e.iv_source = OTX_CPT_FROM_CPTR;\n\t\t \n\t\tmemcpy(fctx->enc.encr_key, ctx->key + ctx->auth_key_len,\n\t\t       ctx->enc_key_len);\n\t\t \n\t\tmemcpy(fctx->enc.encr_iv, req->iv, crypto_aead_ivsize(tfm));\n\n\t\tds = crypto_shash_digestsize(ctx->hashalg);\n\t\tif (ctx->mac_type == OTX_CPT_SHA384)\n\t\t\tds = SHA512_DIGEST_SIZE;\n\t\tif (ctx->ipad)\n\t\t\tmemcpy(fctx->hmac.e.ipad, ctx->ipad, ds);\n\t\tif (ctx->opad)\n\t\t\tmemcpy(fctx->hmac.e.opad, ctx->opad, ds);\n\t\tbreak;\n\n\tcase OTX_CPT_AES_GCM:\n\t\tfctx->enc.enc_ctrl.e.iv_source = OTX_CPT_FROM_DPTR;\n\t\t \n\t\tmemcpy(fctx->enc.encr_key, ctx->key, ctx->enc_key_len);\n\t\t \n\t\tmemcpy(fctx->enc.encr_iv, ctx->key + ctx->enc_key_len,\n\t\t       AES_GCM_SALT_SIZE);\n\n\t\trctx->ctrl_word.e.iv_offset = req->assoclen - AES_GCM_IV_OFFSET;\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\trctx->ctrl_word.flags = cpu_to_be64(rctx->ctrl_word.cflags);\n\n\treq_info->ctrl.s.dma_mode = OTX_CPT_DMA_GATHER_SCATTER;\n\treq_info->ctrl.s.se_req = OTX_CPT_SE_CORE_REQ;\n\treq_info->req.opcode.s.major = OTX_CPT_MAJOR_OP_FC |\n\t\t\t\t DMA_MODE_FLAG(OTX_CPT_DMA_GATHER_SCATTER);\n\tif (enc) {\n\t\treq_info->req.opcode.s.minor = 2;\n\t\treq_info->req.param1 = req->cryptlen;\n\t\treq_info->req.param2 = req->cryptlen + req->assoclen;\n\t} else {\n\t\treq_info->req.opcode.s.minor = 3;\n\t\treq_info->req.param1 = req->cryptlen - mac_len;\n\t\treq_info->req.param2 = req->cryptlen + req->assoclen - mac_len;\n\t}\n\n\tfctx->enc.enc_ctrl.e.enc_cipher = ctx->cipher_type;\n\tfctx->enc.enc_ctrl.e.aes_key = ctx->key_type;\n\tfctx->enc.enc_ctrl.e.mac_type = ctx->mac_type;\n\tfctx->enc.enc_ctrl.e.mac_len = mac_len;\n\tfctx->enc.enc_ctrl.flags = cpu_to_be64(fctx->enc.enc_ctrl.cflags);\n\n\t \n\treq_info->in[*argcnt].vptr = (u8 *)&rctx->ctrl_word;\n\treq_info->in[*argcnt].size = CONTROL_WORD_LEN;\n\treq_info->req.dlen += CONTROL_WORD_LEN;\n\t++(*argcnt);\n\n\treq_info->in[*argcnt].vptr = (u8 *)fctx;\n\treq_info->in[*argcnt].size = sizeof(struct otx_cpt_fc_ctx);\n\treq_info->req.dlen += sizeof(struct otx_cpt_fc_ctx);\n\t++(*argcnt);\n\n\treturn 0;\n}\n\nstatic inline u32 create_hmac_ctx_hdr(struct aead_request *req, u32 *argcnt,\n\t\t\t\t      u32 enc)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct otx_cpt_aead_ctx *ctx = crypto_aead_ctx_dma(tfm);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\n\treq_info->ctrl.s.dma_mode = OTX_CPT_DMA_GATHER_SCATTER;\n\treq_info->ctrl.s.se_req = OTX_CPT_SE_CORE_REQ;\n\treq_info->req.opcode.s.major = OTX_CPT_MAJOR_OP_HMAC |\n\t\t\t\t DMA_MODE_FLAG(OTX_CPT_DMA_GATHER_SCATTER);\n\treq_info->is_trunc_hmac = ctx->is_trunc_hmac;\n\n\treq_info->req.opcode.s.minor = 0;\n\treq_info->req.param1 = ctx->auth_key_len;\n\treq_info->req.param2 = ctx->mac_type << 8;\n\n\t \n\treq_info->in[*argcnt].vptr = ctx->key;\n\treq_info->in[*argcnt].size = round_up(ctx->auth_key_len, 8);\n\treq_info->req.dlen += round_up(ctx->auth_key_len, 8);\n\t++(*argcnt);\n\n\treturn 0;\n}\n\nstatic inline u32 create_aead_input_list(struct aead_request *req, u32 enc)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 inputlen =  req->cryptlen + req->assoclen;\n\tu32 status, argcnt = 0;\n\n\tstatus = create_aead_ctx_hdr(req, enc, &argcnt);\n\tif (status)\n\t\treturn status;\n\tupdate_input_data(req_info, req->src, inputlen, &argcnt);\n\treq_info->incnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline u32 create_aead_output_list(struct aead_request *req, u32 enc,\n\t\t\t\t\t  u32 mac_len)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info =  &rctx->cpt_req;\n\tu32 argcnt = 0, outputlen = 0;\n\n\tif (enc)\n\t\toutputlen = req->cryptlen +  req->assoclen + mac_len;\n\telse\n\t\toutputlen = req->cryptlen + req->assoclen - mac_len;\n\n\tupdate_output_data(req_info, req->dst, 0, outputlen, &argcnt);\n\treq_info->outcnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline u32 create_aead_null_input_list(struct aead_request *req,\n\t\t\t\t\t      u32 enc, u32 mac_len)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tu32 inputlen, argcnt = 0;\n\n\tif (enc)\n\t\tinputlen =  req->cryptlen + req->assoclen;\n\telse\n\t\tinputlen =  req->cryptlen + req->assoclen - mac_len;\n\n\tcreate_hmac_ctx_hdr(req, &argcnt, enc);\n\tupdate_input_data(req_info, req->src, inputlen, &argcnt);\n\treq_info->incnt = argcnt;\n\n\treturn 0;\n}\n\nstatic inline u32 create_aead_null_output_list(struct aead_request *req,\n\t\t\t\t\t       u32 enc, u32 mac_len)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info =  &rctx->cpt_req;\n\tstruct scatterlist *dst;\n\tu8 *ptr = NULL;\n\tint argcnt = 0, status, offset;\n\tu32 inputlen;\n\n\tif (enc)\n\t\tinputlen =  req->cryptlen + req->assoclen;\n\telse\n\t\tinputlen =  req->cryptlen + req->assoclen - mac_len;\n\n\t \n\tif (req->src != req->dst) {\n\n\t\tptr = kmalloc(inputlen, (req_info->areq->flags &\n\t\t\t\t\t CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t\t\t\t GFP_KERNEL : GFP_ATOMIC);\n\t\tif (!ptr) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto error;\n\t\t}\n\n\t\tstatus = sg_copy_to_buffer(req->src, sg_nents(req->src), ptr,\n\t\t\t\t\t   inputlen);\n\t\tif (status != inputlen) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto error_free;\n\t\t}\n\t\tstatus = sg_copy_from_buffer(req->dst, sg_nents(req->dst), ptr,\n\t\t\t\t\t     inputlen);\n\t\tif (status != inputlen) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto error_free;\n\t\t}\n\t\tkfree(ptr);\n\t}\n\n\tif (enc) {\n\t\t \n\t\tdst = req->dst;\n\t\toffset = inputlen;\n\t\twhile (offset >= dst->length) {\n\t\t\toffset -= dst->length;\n\t\t\tdst = sg_next(dst);\n\t\t\tif (!dst) {\n\t\t\t\tstatus = -ENOENT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\tupdate_output_data(req_info, dst, offset, mac_len, &argcnt);\n\t} else {\n\t\t \n\t\tstatus = sg_copy_buffer(req->src, sg_nents(req->src),\n\t\t\t\t\trctx->fctx.hmac.s.hmac_recv, mac_len,\n\t\t\t\t\tinputlen, true);\n\t\tif (status != mac_len) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\n\t\treq_info->out[argcnt].vptr = rctx->fctx.hmac.s.hmac_calc;\n\t\treq_info->out[argcnt].size = mac_len;\n\t\targcnt++;\n\t}\n\n\treq_info->outcnt = argcnt;\n\treturn 0;\n\nerror_free:\n\tkfree(ptr);\nerror:\n\treturn status;\n}\n\nstatic u32 cpt_aead_enc_dec(struct aead_request *req, u8 reg_type, u8 enc)\n{\n\tstruct otx_cpt_req_ctx *rctx = aead_request_ctx_dma(req);\n\tstruct otx_cpt_req_info *req_info = &rctx->cpt_req;\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct pci_dev *pdev;\n\tu32 status, cpu_num;\n\n\t \n\trctx->ctrl_word.flags = 0;\n\trctx->fctx.enc.enc_ctrl.flags = 0;\n\n\treq_info->callback = otx_cpt_aead_callback;\n\treq_info->areq = &req->base;\n\treq_info->req_type = reg_type;\n\treq_info->is_enc = enc;\n\treq_info->is_trunc_hmac = false;\n\n\tswitch (reg_type) {\n\tcase OTX_CPT_AEAD_ENC_DEC_REQ:\n\t\tstatus = create_aead_input_list(req, enc);\n\t\tif (status)\n\t\t\treturn status;\n\t\tstatus = create_aead_output_list(req, enc,\n\t\t\t\t\t\t crypto_aead_authsize(tfm));\n\t\tif (status)\n\t\t\treturn status;\n\t\tbreak;\n\n\tcase OTX_CPT_AEAD_ENC_DEC_NULL_REQ:\n\t\tstatus = create_aead_null_input_list(req, enc,\n\t\t\t\t\t\t     crypto_aead_authsize(tfm));\n\t\tif (status)\n\t\t\treturn status;\n\t\tstatus = create_aead_null_output_list(req, enc,\n\t\t\t\t\t\tcrypto_aead_authsize(tfm));\n\t\tif (status)\n\t\t\treturn status;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (req_info->req.param1 > OTX_CPT_MAX_REQ_SIZE ||\n\t    req_info->req.param2 > OTX_CPT_MAX_REQ_SIZE)\n\t\treturn -E2BIG;\n\n\tstatus = get_se_device(&pdev, &cpu_num);\n\tif (status)\n\t\treturn status;\n\n\treq_info->ctrl.s.grp = 0;\n\n\tstatus = otx_cpt_do_request(pdev, req_info, cpu_num);\n\t \n\treturn status;\n}\n\nstatic int otx_cpt_aead_encrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX_CPT_AEAD_ENC_DEC_REQ, true);\n}\n\nstatic int otx_cpt_aead_decrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX_CPT_AEAD_ENC_DEC_REQ, false);\n}\n\nstatic int otx_cpt_aead_null_encrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX_CPT_AEAD_ENC_DEC_NULL_REQ, true);\n}\n\nstatic int otx_cpt_aead_null_decrypt(struct aead_request *req)\n{\n\treturn cpt_aead_enc_dec(req, OTX_CPT_AEAD_ENC_DEC_NULL_REQ, false);\n}\n\nstatic struct skcipher_alg otx_cpt_skciphers[] = { {\n\t.base.cra_name = \"xts(aes)\",\n\t.base.cra_driver_name = \"cpt_xts_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.min_keysize = 2 * AES_MIN_KEY_SIZE,\n\t.max_keysize = 2 * AES_MAX_KEY_SIZE,\n\t.setkey = otx_cpt_skcipher_xts_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"cbc(aes)\",\n\t.base.cra_driver_name = \"cpt_cbc_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.setkey = otx_cpt_skcipher_cbc_aes_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"ecb(aes)\",\n\t.base.cra_driver_name = \"cpt_ecb_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.ivsize = 0,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.setkey = otx_cpt_skcipher_ecb_aes_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"cfb(aes)\",\n\t.base.cra_driver_name = \"cpt_cfb_aes\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_enc_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.setkey = otx_cpt_skcipher_cfb_aes_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"cbc(des3_ede)\",\n\t.base.cra_driver_name = \"cpt_cbc_des3_ede\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_des3_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.ivsize = DES_BLOCK_SIZE,\n\t.setkey = otx_cpt_skcipher_cbc_des3_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n}, {\n\t.base.cra_name = \"ecb(des3_ede)\",\n\t.base.cra_driver_name = \"cpt_ecb_des3_ede\",\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct otx_cpt_des3_ctx),\n\t.base.cra_alignmask = 7,\n\t.base.cra_priority = 4001,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = otx_cpt_enc_dec_init,\n\t.min_keysize = DES3_EDE_KEY_SIZE,\n\t.max_keysize = DES3_EDE_KEY_SIZE,\n\t.ivsize = 0,\n\t.setkey = otx_cpt_skcipher_ecb_des3_setkey,\n\t.encrypt = otx_cpt_skcipher_encrypt,\n\t.decrypt = otx_cpt_skcipher_decrypt,\n} };\n\nstatic struct aead_alg otx_cpt_aeads[] = { {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha1),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha1_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_cbc_aes_sha1_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_encrypt,\n\t.decrypt = otx_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA1_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha256),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha256_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_cbc_aes_sha256_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_encrypt,\n\t.decrypt = otx_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA256_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha384),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha384_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_cbc_aes_sha384_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_encrypt,\n\t.decrypt = otx_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA384_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha512),cbc(aes))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha512_cbc_aes\",\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_cbc_aes_sha512_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_cbc_aes_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_encrypt,\n\t.decrypt = otx_cpt_aead_decrypt,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA512_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha1),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha1_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_ecb_null_sha1_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_null_encrypt,\n\t.decrypt = otx_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA1_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha256),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha256_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_ecb_null_sha256_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_null_encrypt,\n\t.decrypt = otx_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA256_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha384),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha384_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_ecb_null_sha384_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_null_encrypt,\n\t.decrypt = otx_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA384_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha512),ecb(cipher_null))\",\n\t\t.cra_driver_name = \"cpt_hmac_sha512_ecb_null\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_ecb_null_sha512_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_ecb_null_sha_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_null_encrypt,\n\t.decrypt = otx_cpt_aead_null_decrypt,\n\t.ivsize = 0,\n\t.maxauthsize = SHA512_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"rfc4106(gcm(aes))\",\n\t\t.cra_driver_name = \"cpt_rfc4106_gcm_aes\",\n\t\t.cra_blocksize = 1,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_ctxsize = sizeof(struct otx_cpt_aead_ctx) + CRYPTO_DMA_PADDING,\n\t\t.cra_priority = 4001,\n\t\t.cra_alignmask = 0,\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = otx_cpt_aead_gcm_aes_init,\n\t.exit = otx_cpt_aead_exit,\n\t.setkey = otx_cpt_aead_gcm_aes_setkey,\n\t.setauthsize = otx_cpt_aead_set_authsize,\n\t.encrypt = otx_cpt_aead_encrypt,\n\t.decrypt = otx_cpt_aead_decrypt,\n\t.ivsize = AES_GCM_IV_SIZE,\n\t.maxauthsize = AES_GCM_ICV_SIZE,\n} };\n\nstatic inline int is_any_alg_used(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(otx_cpt_skciphers); i++)\n\t\tif (refcount_read(&otx_cpt_skciphers[i].base.cra_refcnt) != 1)\n\t\t\treturn true;\n\tfor (i = 0; i < ARRAY_SIZE(otx_cpt_aeads); i++)\n\t\tif (refcount_read(&otx_cpt_aeads[i].base.cra_refcnt) != 1)\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic inline int cpt_register_algs(void)\n{\n\tint i, err = 0;\n\n\tif (!IS_ENABLED(CONFIG_DM_CRYPT)) {\n\t\tfor (i = 0; i < ARRAY_SIZE(otx_cpt_skciphers); i++)\n\t\t\totx_cpt_skciphers[i].base.cra_flags &= ~CRYPTO_ALG_DEAD;\n\n\t\terr = crypto_register_skciphers(otx_cpt_skciphers,\n\t\t\t\t\t\tARRAY_SIZE(otx_cpt_skciphers));\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(otx_cpt_aeads); i++)\n\t\totx_cpt_aeads[i].base.cra_flags &= ~CRYPTO_ALG_DEAD;\n\n\terr = crypto_register_aeads(otx_cpt_aeads, ARRAY_SIZE(otx_cpt_aeads));\n\tif (err) {\n\t\tcrypto_unregister_skciphers(otx_cpt_skciphers,\n\t\t\t\t\t    ARRAY_SIZE(otx_cpt_skciphers));\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void cpt_unregister_algs(void)\n{\n\tcrypto_unregister_skciphers(otx_cpt_skciphers,\n\t\t\t\t    ARRAY_SIZE(otx_cpt_skciphers));\n\tcrypto_unregister_aeads(otx_cpt_aeads, ARRAY_SIZE(otx_cpt_aeads));\n}\n\nstatic int compare_func(const void *lptr, const void *rptr)\n{\n\tstruct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;\n\tstruct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;\n\n\tif (ldesc->dev->devfn < rdesc->dev->devfn)\n\t\treturn -1;\n\tif (ldesc->dev->devfn > rdesc->dev->devfn)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void swap_func(void *lptr, void *rptr, int size)\n{\n\tstruct cpt_device_desc *ldesc = (struct cpt_device_desc *) lptr;\n\tstruct cpt_device_desc *rdesc = (struct cpt_device_desc *) rptr;\n\n\tswap(*ldesc, *rdesc);\n}\n\nint otx_cpt_crypto_init(struct pci_dev *pdev, struct module *mod,\n\t\t\tenum otx_cptpf_type pf_type,\n\t\t\tenum otx_cptvf_type engine_type,\n\t\t\tint num_queues, int num_devices)\n{\n\tint ret = 0;\n\tint count;\n\n\tmutex_lock(&mutex);\n\tswitch (engine_type) {\n\tcase OTX_CPT_SE_TYPES:\n\t\tcount = atomic_read(&se_devices.count);\n\t\tif (count >= CPT_MAX_VF_NUM) {\n\t\t\tdev_err(&pdev->dev, \"No space to add a new device\\n\");\n\t\t\tret = -ENOSPC;\n\t\t\tgoto err;\n\t\t}\n\t\tse_devices.desc[count].pf_type = pf_type;\n\t\tse_devices.desc[count].num_queues = num_queues;\n\t\tse_devices.desc[count++].dev = pdev;\n\t\tatomic_inc(&se_devices.count);\n\n\t\tif (atomic_read(&se_devices.count) == num_devices &&\n\t\t    is_crypto_registered == false) {\n\t\t\tif (cpt_register_algs()) {\n\t\t\t\tdev_err(&pdev->dev,\n\t\t\t\t   \"Error in registering crypto algorithms\\n\");\n\t\t\t\tret =  -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\ttry_module_get(mod);\n\t\t\tis_crypto_registered = true;\n\t\t}\n\t\tsort(se_devices.desc, count, sizeof(struct cpt_device_desc),\n\t\t     compare_func, swap_func);\n\t\tbreak;\n\n\tcase OTX_CPT_AE_TYPES:\n\t\tcount = atomic_read(&ae_devices.count);\n\t\tif (count >= CPT_MAX_VF_NUM) {\n\t\t\tdev_err(&pdev->dev, \"No space to a add new device\\n\");\n\t\t\tret = -ENOSPC;\n\t\t\tgoto err;\n\t\t}\n\t\tae_devices.desc[count].pf_type = pf_type;\n\t\tae_devices.desc[count].num_queues = num_queues;\n\t\tae_devices.desc[count++].dev = pdev;\n\t\tatomic_inc(&ae_devices.count);\n\t\tsort(ae_devices.desc, count, sizeof(struct cpt_device_desc),\n\t\t     compare_func, swap_func);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&pdev->dev, \"Unknown VF type %d\\n\", engine_type);\n\t\tret = BAD_OTX_CPTVF_TYPE;\n\t}\nerr:\n\tmutex_unlock(&mutex);\n\treturn ret;\n}\n\nvoid otx_cpt_crypto_exit(struct pci_dev *pdev, struct module *mod,\n\t\t\t enum otx_cptvf_type engine_type)\n{\n\tstruct cpt_device_table *dev_tbl;\n\tbool dev_found = false;\n\tint i, j, count;\n\n\tmutex_lock(&mutex);\n\n\tdev_tbl = (engine_type == OTX_CPT_AE_TYPES) ? &ae_devices : &se_devices;\n\tcount = atomic_read(&dev_tbl->count);\n\tfor (i = 0; i < count; i++)\n\t\tif (pdev == dev_tbl->desc[i].dev) {\n\t\t\tfor (j = i; j < count-1; j++)\n\t\t\t\tdev_tbl->desc[j] = dev_tbl->desc[j+1];\n\t\t\tdev_found = true;\n\t\t\tbreak;\n\t\t}\n\n\tif (!dev_found) {\n\t\tdev_err(&pdev->dev, \"%s device not found\\n\", __func__);\n\t\tgoto exit;\n\t}\n\n\tif (engine_type != OTX_CPT_AE_TYPES) {\n\t\tif (atomic_dec_and_test(&se_devices.count) &&\n\t\t    !is_any_alg_used()) {\n\t\t\tcpt_unregister_algs();\n\t\t\tmodule_put(mod);\n\t\t\tis_crypto_registered = false;\n\t\t}\n\t} else\n\t\tatomic_dec(&ae_devices.count);\nexit:\n\tmutex_unlock(&mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}