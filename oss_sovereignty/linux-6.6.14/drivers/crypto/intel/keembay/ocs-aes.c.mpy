{
  "module_name": "ocs-aes.c",
  "hash_id": "fa038cacf722149c706b3e598d266f758917c263f229a19465e1e0a9fefe2d8d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/intel/keembay/ocs-aes.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/swab.h>\n\n#include <asm/byteorder.h>\n#include <asm/errno.h>\n\n#include <crypto/aes.h>\n#include <crypto/gcm.h>\n\n#include \"ocs-aes.h\"\n\n#define AES_COMMAND_OFFSET\t\t\t0x0000\n#define AES_KEY_0_OFFSET\t\t\t0x0004\n#define AES_KEY_1_OFFSET\t\t\t0x0008\n#define AES_KEY_2_OFFSET\t\t\t0x000C\n#define AES_KEY_3_OFFSET\t\t\t0x0010\n#define AES_KEY_4_OFFSET\t\t\t0x0014\n#define AES_KEY_5_OFFSET\t\t\t0x0018\n#define AES_KEY_6_OFFSET\t\t\t0x001C\n#define AES_KEY_7_OFFSET\t\t\t0x0020\n#define AES_IV_0_OFFSET\t\t\t\t0x0024\n#define AES_IV_1_OFFSET\t\t\t\t0x0028\n#define AES_IV_2_OFFSET\t\t\t\t0x002C\n#define AES_IV_3_OFFSET\t\t\t\t0x0030\n#define AES_ACTIVE_OFFSET\t\t\t0x0034\n#define AES_STATUS_OFFSET\t\t\t0x0038\n#define AES_KEY_SIZE_OFFSET\t\t\t0x0044\n#define AES_IER_OFFSET\t\t\t\t0x0048\n#define AES_ISR_OFFSET\t\t\t\t0x005C\n#define AES_MULTIPURPOSE1_0_OFFSET\t\t0x0200\n#define AES_MULTIPURPOSE1_1_OFFSET\t\t0x0204\n#define AES_MULTIPURPOSE1_2_OFFSET\t\t0x0208\n#define AES_MULTIPURPOSE1_3_OFFSET\t\t0x020C\n#define AES_MULTIPURPOSE2_0_OFFSET\t\t0x0220\n#define AES_MULTIPURPOSE2_1_OFFSET\t\t0x0224\n#define AES_MULTIPURPOSE2_2_OFFSET\t\t0x0228\n#define AES_MULTIPURPOSE2_3_OFFSET\t\t0x022C\n#define AES_BYTE_ORDER_CFG_OFFSET\t\t0x02C0\n#define AES_TLEN_OFFSET\t\t\t\t0x0300\n#define AES_T_MAC_0_OFFSET\t\t\t0x0304\n#define AES_T_MAC_1_OFFSET\t\t\t0x0308\n#define AES_T_MAC_2_OFFSET\t\t\t0x030C\n#define AES_T_MAC_3_OFFSET\t\t\t0x0310\n#define AES_PLEN_OFFSET\t\t\t\t0x0314\n#define AES_A_DMA_SRC_ADDR_OFFSET\t\t0x0400\n#define AES_A_DMA_DST_ADDR_OFFSET\t\t0x0404\n#define AES_A_DMA_SRC_SIZE_OFFSET\t\t0x0408\n#define AES_A_DMA_DST_SIZE_OFFSET\t\t0x040C\n#define AES_A_DMA_DMA_MODE_OFFSET\t\t0x0410\n#define AES_A_DMA_NEXT_SRC_DESCR_OFFSET\t\t0x0418\n#define AES_A_DMA_NEXT_DST_DESCR_OFFSET\t\t0x041C\n#define AES_A_DMA_WHILE_ACTIVE_MODE_OFFSET\t0x0420\n#define AES_A_DMA_LOG_OFFSET\t\t\t0x0424\n#define AES_A_DMA_STATUS_OFFSET\t\t\t0x0428\n#define AES_A_DMA_PERF_CNTR_OFFSET\t\t0x042C\n#define AES_A_DMA_MSI_ISR_OFFSET\t\t0x0480\n#define AES_A_DMA_MSI_IER_OFFSET\t\t0x0484\n#define AES_A_DMA_MSI_MASK_OFFSET\t\t0x0488\n#define AES_A_DMA_INBUFFER_WRITE_FIFO_OFFSET\t0x0600\n#define AES_A_DMA_OUTBUFFER_READ_FIFO_OFFSET\t0x0700\n\n \n#define AES_A_DMA_DMA_MODE_ACTIVE\t\tBIT(31)\n#define AES_A_DMA_DMA_MODE_SRC_LINK_LIST_EN\tBIT(25)\n#define AES_A_DMA_DMA_MODE_DST_LINK_LIST_EN\tBIT(24)\n\n \n#define AES_ACTIVE_LAST_ADATA\t\t\tBIT(9)\n#define AES_ACTIVE_LAST_CCM_GCM\t\t\tBIT(8)\n#define AES_ACTIVE_TERMINATION\t\t\tBIT(1)\n#define AES_ACTIVE_TRIGGER\t\t\tBIT(0)\n\n#define AES_DISABLE_INT\t\t\t\t0x00000000\n#define AES_DMA_CPD_ERR_INT\t\t\tBIT(8)\n#define AES_DMA_OUTBUF_RD_ERR_INT\t\tBIT(7)\n#define AES_DMA_OUTBUF_WR_ERR_INT\t\tBIT(6)\n#define AES_DMA_INBUF_RD_ERR_INT\t\tBIT(5)\n#define AES_DMA_INBUF_WR_ERR_INT\t\tBIT(4)\n#define AES_DMA_BAD_COMP_INT\t\t\tBIT(3)\n#define AES_DMA_SAI_INT\t\t\t\tBIT(2)\n#define AES_DMA_SRC_DONE_INT\t\t\tBIT(0)\n#define AES_COMPLETE_INT\t\t\tBIT(1)\n\n#define AES_DMA_MSI_MASK_CLEAR\t\t\tBIT(0)\n\n#define AES_128_BIT_KEY\t\t\t\t0x00000000\n#define AES_256_BIT_KEY\t\t\t\tBIT(0)\n\n#define AES_DEACTIVATE_PERF_CNTR\t\t0x00000000\n#define AES_ACTIVATE_PERF_CNTR\t\t\tBIT(0)\n\n#define AES_MAX_TAG_SIZE_U32\t\t\t4\n\n#define OCS_LL_DMA_FLAG_TERMINATE\t\tBIT(31)\n\n \n#define AES_DMA_STATUS_INPUT_BUFFER_OCCUPANCY_MASK\t0x3FF\n\n \n#define CCM_DECRYPT_DELAY_TAG_CLK_COUNT\t\t36UL\n\n \n#define CCM_DECRYPT_DELAY_LAST_GCX_CLK_COUNT\t42UL\n\n \n#define L_PRIME_MIN (1)\n#define L_PRIME_MAX (7)\n \n#define L_PRIME_IDX\t\t0\n#define COUNTER_START(lprime)\t(16 - ((lprime) + 1))\n#define COUNTER_LEN(lprime)\t((lprime) + 1)\n\nenum aes_counter_mode {\n\tAES_CTR_M_NO_INC = 0,\n\tAES_CTR_M_32_INC = 1,\n\tAES_CTR_M_64_INC = 2,\n\tAES_CTR_M_128_INC = 3,\n};\n\n \nstruct ocs_dma_linked_list {\n\tu32 src_addr;\n\tu32 src_len;\n\tu32 next;\n\tu32 ll_flags;\n} __packed;\n\n \nstatic inline void aes_a_set_endianness(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(0x7FF, aes_dev->base_reg + AES_BYTE_ORDER_CFG_OFFSET);\n}\n\n \nstatic inline void aes_a_op_trigger(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_ACTIVE_TRIGGER, aes_dev->base_reg + AES_ACTIVE_OFFSET);\n}\n\n \nstatic inline void aes_a_op_termination(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_ACTIVE_TERMINATION,\n\t\t  aes_dev->base_reg + AES_ACTIVE_OFFSET);\n}\n\n \nstatic inline void aes_a_set_last_gcx(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_ACTIVE_LAST_CCM_GCM,\n\t\t  aes_dev->base_reg + AES_ACTIVE_OFFSET);\n}\n\n \nstatic inline void aes_a_wait_last_gcx(const struct ocs_aes_dev *aes_dev)\n{\n\tu32 aes_active_reg;\n\n\tdo {\n\t\taes_active_reg = ioread32(aes_dev->base_reg +\n\t\t\t\t\t  AES_ACTIVE_OFFSET);\n\t} while (aes_active_reg & AES_ACTIVE_LAST_CCM_GCM);\n}\n\n \nstatic void aes_a_dma_wait_input_buffer_occupancy(const struct ocs_aes_dev *aes_dev)\n{\n\tu32 reg;\n\n\tdo {\n\t\treg = ioread32(aes_dev->base_reg + AES_A_DMA_STATUS_OFFSET);\n\t} while (reg & AES_DMA_STATUS_INPUT_BUFFER_OCCUPANCY_MASK);\n}\n\n  \nstatic inline void aes_a_set_last_gcx_and_adata(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_ACTIVE_LAST_ADATA | AES_ACTIVE_LAST_CCM_GCM,\n\t\t  aes_dev->base_reg + AES_ACTIVE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_set_xfer_size_zero(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(0, aes_dev->base_reg + AES_A_DMA_SRC_SIZE_OFFSET);\n\tiowrite32(0, aes_dev->base_reg + AES_A_DMA_DST_SIZE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_active(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_A_DMA_DMA_MODE_ACTIVE,\n\t\t  aes_dev->base_reg + AES_A_DMA_DMA_MODE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_active_src_ll_en(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_A_DMA_DMA_MODE_ACTIVE |\n\t\t  AES_A_DMA_DMA_MODE_SRC_LINK_LIST_EN,\n\t\t  aes_dev->base_reg + AES_A_DMA_DMA_MODE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_active_dst_ll_en(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_A_DMA_DMA_MODE_ACTIVE |\n\t\t  AES_A_DMA_DMA_MODE_DST_LINK_LIST_EN,\n\t\t  aes_dev->base_reg + AES_A_DMA_DMA_MODE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_active_src_dst_ll_en(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(AES_A_DMA_DMA_MODE_ACTIVE |\n\t\t  AES_A_DMA_DMA_MODE_SRC_LINK_LIST_EN |\n\t\t  AES_A_DMA_DMA_MODE_DST_LINK_LIST_EN,\n\t\t  aes_dev->base_reg + AES_A_DMA_DMA_MODE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_reset_and_activate_perf_cntr(const struct ocs_aes_dev *aes_dev)\n{\n\tiowrite32(0x00000000, aes_dev->base_reg + AES_A_DMA_PERF_CNTR_OFFSET);\n\tiowrite32(AES_ACTIVATE_PERF_CNTR,\n\t\t  aes_dev->base_reg + AES_A_DMA_WHILE_ACTIVE_MODE_OFFSET);\n}\n\n \nstatic inline void aes_a_dma_wait_and_deactivate_perf_cntr(const struct ocs_aes_dev *aes_dev,\n\t\t\t\t\t\t\t   int delay)\n{\n\twhile (ioread32(aes_dev->base_reg + AES_A_DMA_PERF_CNTR_OFFSET) < delay)\n\t\t;\n\tiowrite32(AES_DEACTIVATE_PERF_CNTR,\n\t\t  aes_dev->base_reg + AES_A_DMA_WHILE_ACTIVE_MODE_OFFSET);\n}\n\n \nstatic void aes_irq_disable(struct ocs_aes_dev *aes_dev)\n{\n\tu32 isr_val = 0;\n\n\t \n\tiowrite32(AES_DISABLE_INT,\n\t\t  aes_dev->base_reg + AES_A_DMA_MSI_IER_OFFSET);\n\tiowrite32(AES_DISABLE_INT, aes_dev->base_reg + AES_IER_OFFSET);\n\n\t \n\tisr_val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);\n\tif (isr_val)\n\t\tiowrite32(isr_val,\n\t\t\t  aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);\n\n\tisr_val = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_MASK_OFFSET);\n\tif (isr_val)\n\t\tiowrite32(isr_val,\n\t\t\t  aes_dev->base_reg + AES_A_DMA_MSI_MASK_OFFSET);\n\n\tisr_val = ioread32(aes_dev->base_reg + AES_ISR_OFFSET);\n\tif (isr_val)\n\t\tiowrite32(isr_val, aes_dev->base_reg + AES_ISR_OFFSET);\n}\n\n \nstatic void aes_irq_enable(struct ocs_aes_dev *aes_dev, u8 irq)\n{\n\tif (irq == AES_COMPLETE_INT) {\n\t\t \n\t\tiowrite32(AES_DMA_CPD_ERR_INT |\n\t\t\t  AES_DMA_OUTBUF_RD_ERR_INT |\n\t\t\t  AES_DMA_OUTBUF_WR_ERR_INT |\n\t\t\t  AES_DMA_INBUF_RD_ERR_INT |\n\t\t\t  AES_DMA_INBUF_WR_ERR_INT |\n\t\t\t  AES_DMA_BAD_COMP_INT |\n\t\t\t  AES_DMA_SAI_INT,\n\t\t\t  aes_dev->base_reg + AES_A_DMA_MSI_IER_OFFSET);\n\t\t \n\t\tiowrite32(AES_COMPLETE_INT, aes_dev->base_reg + AES_IER_OFFSET);\n\t\treturn;\n\t}\n\tif (irq == AES_DMA_SRC_DONE_INT) {\n\t\t \n\t\tiowrite32(AES_DISABLE_INT, aes_dev->base_reg + AES_IER_OFFSET);\n\t\t \n\t\tiowrite32(AES_DMA_CPD_ERR_INT |\n\t\t\t  AES_DMA_OUTBUF_RD_ERR_INT |\n\t\t\t  AES_DMA_OUTBUF_WR_ERR_INT |\n\t\t\t  AES_DMA_INBUF_RD_ERR_INT |\n\t\t\t  AES_DMA_INBUF_WR_ERR_INT |\n\t\t\t  AES_DMA_BAD_COMP_INT |\n\t\t\t  AES_DMA_SAI_INT |\n\t\t\t  AES_DMA_SRC_DONE_INT,\n\t\t\t  aes_dev->base_reg + AES_A_DMA_MSI_IER_OFFSET);\n\t}\n}\n\n \nstatic int ocs_aes_irq_enable_and_wait(struct ocs_aes_dev *aes_dev, u8 irq)\n{\n\tint rc;\n\n\treinit_completion(&aes_dev->irq_completion);\n\taes_irq_enable(aes_dev, irq);\n\trc = wait_for_completion_interruptible(&aes_dev->irq_completion);\n\tif (rc)\n\t\treturn rc;\n\n\treturn aes_dev->dma_err_mask ? -EIO : 0;\n}\n\n \nstatic inline void dma_to_ocs_aes_ll(struct ocs_aes_dev *aes_dev,\n\t\t\t\t     dma_addr_t dma_list)\n{\n\tiowrite32(0, aes_dev->base_reg + AES_A_DMA_SRC_SIZE_OFFSET);\n\tiowrite32(dma_list,\n\t\t  aes_dev->base_reg + AES_A_DMA_NEXT_SRC_DESCR_OFFSET);\n}\n\n \nstatic inline void dma_from_ocs_aes_ll(struct ocs_aes_dev *aes_dev,\n\t\t\t\t       dma_addr_t dma_list)\n{\n\tiowrite32(0, aes_dev->base_reg + AES_A_DMA_DST_SIZE_OFFSET);\n\tiowrite32(dma_list,\n\t\t  aes_dev->base_reg + AES_A_DMA_NEXT_DST_DESCR_OFFSET);\n}\n\nirqreturn_t ocs_aes_irq_handler(int irq, void *dev_id)\n{\n\tstruct ocs_aes_dev *aes_dev = dev_id;\n\tu32 aes_dma_isr;\n\n\t \n\taes_dma_isr = ioread32(aes_dev->base_reg + AES_A_DMA_MSI_ISR_OFFSET);\n\n\t \n\taes_irq_disable(aes_dev);\n\n\t \n\taes_dev->dma_err_mask = aes_dma_isr &\n\t\t\t\t(AES_DMA_CPD_ERR_INT |\n\t\t\t\t AES_DMA_OUTBUF_RD_ERR_INT |\n\t\t\t\t AES_DMA_OUTBUF_WR_ERR_INT |\n\t\t\t\t AES_DMA_INBUF_RD_ERR_INT |\n\t\t\t\t AES_DMA_INBUF_WR_ERR_INT |\n\t\t\t\t AES_DMA_BAD_COMP_INT |\n\t\t\t\t AES_DMA_SAI_INT);\n\n\t \n\tcomplete(&aes_dev->irq_completion);\n\n\treturn IRQ_HANDLED;\n}\n\n \nint ocs_aes_set_key(struct ocs_aes_dev *aes_dev, u32 key_size, const u8 *key,\n\t\t    enum ocs_cipher cipher)\n{\n\tconst u32 *key_u32;\n\tu32 val;\n\tint i;\n\n\t \n\tif (cipher == OCS_AES && !(key_size == 32 || key_size == 16)) {\n\t\tdev_err(aes_dev->dev,\n\t\t\t\"%d-bit keys not supported by AES cipher\\n\",\n\t\t\tkey_size * 8);\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (cipher == OCS_SM4 && key_size != 16) {\n\t\tdev_err(aes_dev->dev,\n\t\t\t\"%d-bit keys not supported for SM4 cipher\\n\",\n\t\t\tkey_size * 8);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!key)\n\t\treturn -EINVAL;\n\n\tkey_u32 = (const u32 *)key;\n\n\t \n\tfor (i = 0; i < (key_size / sizeof(u32)); i++) {\n\t\tiowrite32(key_u32[i],\n\t\t\t  aes_dev->base_reg + AES_KEY_0_OFFSET +\n\t\t\t  (i * sizeof(u32)));\n\t}\n\t \n\tval = (key_size == 16) ? AES_128_BIT_KEY : AES_256_BIT_KEY;\n\tiowrite32(val, aes_dev->base_reg + AES_KEY_SIZE_OFFSET);\n\n\treturn 0;\n}\n\n \nstatic inline void set_ocs_aes_command(struct ocs_aes_dev *aes_dev,\n\t\t\t\t       enum ocs_cipher cipher,\n\t\t\t\t       enum ocs_mode mode,\n\t\t\t\t       enum ocs_instruction instruction)\n{\n\tu32 val;\n\n\t \n\tval = (cipher << 14) | (mode << 8) | (instruction << 6) |\n\t      (AES_CTR_M_128_INC << 2);\n\tiowrite32(val, aes_dev->base_reg + AES_COMMAND_OFFSET);\n}\n\nstatic void ocs_aes_init(struct ocs_aes_dev *aes_dev,\n\t\t\t enum ocs_mode mode,\n\t\t\t enum ocs_cipher cipher,\n\t\t\t enum ocs_instruction instruction)\n{\n\t \n\taes_irq_disable(aes_dev);\n\n\t \n\taes_a_set_endianness(aes_dev);\n\n\t \n\tset_ocs_aes_command(aes_dev, cipher, mode, instruction);\n}\n\n \nstatic inline void ocs_aes_write_last_data_blk_len(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\t\t   u32 size)\n{\n\tu32 val;\n\n\tif (size == 0) {\n\t\tval = 0;\n\t\tgoto exit;\n\t}\n\n\tval = size % AES_BLOCK_SIZE;\n\tif (val == 0)\n\t\tval = AES_BLOCK_SIZE;\n\nexit:\n\tiowrite32(val, aes_dev->base_reg + AES_PLEN_OFFSET);\n}\n\n \nstatic int ocs_aes_validate_inputs(dma_addr_t src_dma_list, u32 src_size,\n\t\t\t\t   const u8 *iv, u32 iv_size,\n\t\t\t\t   dma_addr_t aad_dma_list, u32 aad_size,\n\t\t\t\t   const u8 *tag, u32 tag_size,\n\t\t\t\t   enum ocs_cipher cipher, enum ocs_mode mode,\n\t\t\t\t   enum ocs_instruction instruction,\n\t\t\t\t   dma_addr_t dst_dma_list)\n{\n\t \n\tif (!(cipher == OCS_AES || cipher == OCS_SM4))\n\t\treturn -EINVAL;\n\n\tif (mode != OCS_MODE_ECB && mode != OCS_MODE_CBC &&\n\t    mode != OCS_MODE_CTR && mode != OCS_MODE_CCM &&\n\t    mode != OCS_MODE_GCM && mode != OCS_MODE_CTS)\n\t\treturn -EINVAL;\n\n\tif (instruction != OCS_ENCRYPT && instruction != OCS_DECRYPT &&\n\t    instruction != OCS_EXPAND  && instruction != OCS_BYPASS)\n\t\treturn -EINVAL;\n\n\t \n\tif (instruction == OCS_BYPASS) {\n\t\tif (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t    dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\t}\n\n\t \n\tswitch (mode) {\n\tcase OCS_MODE_ECB:\n\t\t \n\t\tif (src_size % AES_BLOCK_SIZE != 0)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t    dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tcase OCS_MODE_CBC:\n\t\t \n\t\tif (src_size % AES_BLOCK_SIZE != 0)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t    dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!iv || iv_size != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tcase OCS_MODE_CTR:\n\t\t \n\t\tif (src_size == 0)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t    dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!iv || iv_size != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tcase OCS_MODE_CTS:\n\t\t \n\t\tif (src_size < AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t    dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!iv || iv_size != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tcase OCS_MODE_GCM:\n\t\t \n\t\tif (!iv || iv_size != GCM_AES_IV_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_size && (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t\t\t dst_dma_list == DMA_MAPPING_ERROR))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (aad_size && aad_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!tag)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (tag_size > (AES_MAX_TAG_SIZE_U32 * sizeof(u32)))\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tcase OCS_MODE_CCM:\n\t\t \n\t\tif (!iv || iv_size != AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (iv[L_PRIME_IDX] < L_PRIME_MIN ||\n\t\t    iv[L_PRIME_IDX] > L_PRIME_MAX)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (aad_size && aad_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (tag_size > (AES_MAX_TAG_SIZE_U32 * sizeof(u32)))\n\t\t\treturn -EINVAL;\n\n\t\tif (instruction == OCS_DECRYPT) {\n\t\t\t \n\t\t\tif (src_size && (src_dma_list == DMA_MAPPING_ERROR ||\n\t\t\t\t\t dst_dma_list == DMA_MAPPING_ERROR))\n\t\t\t\treturn -EINVAL;\n\n\t\t\t \n\t\t\tif (!tag)\n\t\t\t\treturn -EINVAL;\n\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\n\t\t \n\t\tif (dst_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (src_size && src_dma_list == DMA_MAPPING_ERROR)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \nint ocs_aes_op(struct ocs_aes_dev *aes_dev,\n\t       enum ocs_mode mode,\n\t       enum ocs_cipher cipher,\n\t       enum ocs_instruction instruction,\n\t       dma_addr_t dst_dma_list,\n\t       dma_addr_t src_dma_list,\n\t       u32 src_size,\n\t       u8 *iv,\n\t       u32 iv_size)\n{\n\tu32 *iv32;\n\tint rc;\n\n\trc = ocs_aes_validate_inputs(src_dma_list, src_size, iv, iv_size, 0, 0,\n\t\t\t\t     NULL, 0, cipher, mode, instruction,\n\t\t\t\t     dst_dma_list);\n\tif (rc)\n\t\treturn rc;\n\t \n\tif (mode == OCS_MODE_GCM || mode == OCS_MODE_CCM)\n\t\treturn -EINVAL;\n\n\t \n\tiv32 = (u32 *)iv;\n\n\tocs_aes_init(aes_dev, mode, cipher, instruction);\n\n\tif (mode == OCS_MODE_CTS) {\n\t\t \n\t\tocs_aes_write_last_data_blk_len(aes_dev, src_size);\n\t}\n\n\t \n\tif (mode != OCS_MODE_ECB) {\n\t\tiowrite32(iv32[0], aes_dev->base_reg + AES_IV_0_OFFSET);\n\t\tiowrite32(iv32[1], aes_dev->base_reg + AES_IV_1_OFFSET);\n\t\tiowrite32(iv32[2], aes_dev->base_reg + AES_IV_2_OFFSET);\n\t\tiowrite32(iv32[3], aes_dev->base_reg + AES_IV_3_OFFSET);\n\t}\n\n\t \n\taes_a_op_trigger(aes_dev);\n\n\t \n\tdma_to_ocs_aes_ll(aes_dev, src_dma_list);\n\tdma_from_ocs_aes_ll(aes_dev, dst_dma_list);\n\taes_a_dma_active_src_dst_ll_en(aes_dev);\n\n\tif (mode == OCS_MODE_CTS) {\n\t\t \n\t\taes_a_set_last_gcx(aes_dev);\n\t} else {\n\t\t \n\t\taes_a_op_termination(aes_dev);\n\t}\n\n\t \n\trc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);\n\tif (rc)\n\t\treturn rc;\n\n\tif (mode == OCS_MODE_CTR) {\n\t\t \n\t\tiv32[0] = ioread32(aes_dev->base_reg + AES_IV_0_OFFSET);\n\t\tiv32[1] = ioread32(aes_dev->base_reg + AES_IV_1_OFFSET);\n\t\tiv32[2] = ioread32(aes_dev->base_reg + AES_IV_2_OFFSET);\n\t\tiv32[3] = ioread32(aes_dev->base_reg + AES_IV_3_OFFSET);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ocs_aes_gcm_write_j0(const struct ocs_aes_dev *aes_dev,\n\t\t\t\t const u8 *iv)\n{\n\tconst u32 *j0 = (u32 *)iv;\n\n\t \n\tiowrite32(0x00000001, aes_dev->base_reg + AES_IV_0_OFFSET);\n\tiowrite32(__swab32(j0[2]), aes_dev->base_reg + AES_IV_1_OFFSET);\n\tiowrite32(__swab32(j0[1]), aes_dev->base_reg + AES_IV_2_OFFSET);\n\tiowrite32(__swab32(j0[0]), aes_dev->base_reg + AES_IV_3_OFFSET);\n}\n\n \nstatic inline void ocs_aes_gcm_read_tag(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\tu8 *tag, u32 tag_size)\n{\n\tu32 tag_u32[AES_MAX_TAG_SIZE_U32];\n\n\t \n\ttag_u32[0] = __swab32(ioread32(aes_dev->base_reg + AES_T_MAC_3_OFFSET));\n\ttag_u32[1] = __swab32(ioread32(aes_dev->base_reg + AES_T_MAC_2_OFFSET));\n\ttag_u32[2] = __swab32(ioread32(aes_dev->base_reg + AES_T_MAC_1_OFFSET));\n\ttag_u32[3] = __swab32(ioread32(aes_dev->base_reg + AES_T_MAC_0_OFFSET));\n\n\tmemcpy(tag, tag_u32, tag_size);\n}\n\n \nint ocs_aes_gcm_op(struct ocs_aes_dev *aes_dev,\n\t\t   enum ocs_cipher cipher,\n\t\t   enum ocs_instruction instruction,\n\t\t   dma_addr_t dst_dma_list,\n\t\t   dma_addr_t src_dma_list,\n\t\t   u32 src_size,\n\t\t   const u8 *iv,\n\t\t   dma_addr_t aad_dma_list,\n\t\t   u32 aad_size,\n\t\t   u8 *out_tag,\n\t\t   u32 tag_size)\n{\n\tu64 bit_len;\n\tu32 val;\n\tint rc;\n\n\trc = ocs_aes_validate_inputs(src_dma_list, src_size, iv,\n\t\t\t\t     GCM_AES_IV_SIZE, aad_dma_list,\n\t\t\t\t     aad_size, out_tag, tag_size, cipher,\n\t\t\t\t     OCS_MODE_GCM, instruction,\n\t\t\t\t     dst_dma_list);\n\tif (rc)\n\t\treturn rc;\n\n\tocs_aes_init(aes_dev, OCS_MODE_GCM, cipher, instruction);\n\n\t \n\tocs_aes_gcm_write_j0(aes_dev, iv);\n\n\t \n\tiowrite32(tag_size, aes_dev->base_reg + AES_TLEN_OFFSET);\n\n\t \n\tocs_aes_write_last_data_blk_len(aes_dev, src_size);\n\n\t \n\tbit_len = (u64)src_size * 8;\n\tval = bit_len & 0xFFFFFFFF;\n\tiowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_0_OFFSET);\n\tval = bit_len >> 32;\n\tiowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_1_OFFSET);\n\n\t \n\tbit_len = (u64)aad_size * 8;\n\tval = bit_len & 0xFFFFFFFF;\n\tiowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_2_OFFSET);\n\tval = bit_len >> 32;\n\tiowrite32(val, aes_dev->base_reg + AES_MULTIPURPOSE2_3_OFFSET);\n\n\t \n\taes_a_op_trigger(aes_dev);\n\n\t \n\tif (aad_size) {\n\t\t \n\t\tdma_to_ocs_aes_ll(aes_dev, aad_dma_list);\n\t\taes_a_dma_active_src_ll_en(aes_dev);\n\n\t\t \n\t\taes_a_set_last_gcx_and_adata(aes_dev);\n\n\t\t \n\t\trc = ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else {\n\t\taes_a_set_last_gcx_and_adata(aes_dev);\n\t}\n\n\t \n\taes_a_wait_last_gcx(aes_dev);\n\taes_a_dma_wait_input_buffer_occupancy(aes_dev);\n\n\t \n\tif (src_size) {\n\t\t \n\t\tdma_to_ocs_aes_ll(aes_dev, src_dma_list);\n\t\tdma_from_ocs_aes_ll(aes_dev, dst_dma_list);\n\t\taes_a_dma_active_src_dst_ll_en(aes_dev);\n\t} else {\n\t\taes_a_dma_set_xfer_size_zero(aes_dev);\n\t\taes_a_dma_active(aes_dev);\n\t}\n\n\t \n\taes_a_set_last_gcx(aes_dev);\n\n\t \n\trc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);\n\tif (rc)\n\t\treturn rc;\n\n\tocs_aes_gcm_read_tag(aes_dev, out_tag, tag_size);\n\n\treturn 0;\n}\n\n \nstatic void ocs_aes_ccm_write_encrypted_tag(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\t    const u8 *in_tag, u32 tag_size)\n{\n\tint i;\n\n\t \n\taes_a_dma_wait_input_buffer_occupancy(aes_dev);\n\n\t \n\taes_a_dma_reset_and_activate_perf_cntr(aes_dev);\n\taes_a_dma_wait_and_deactivate_perf_cntr(aes_dev,\n\t\t\t\t\t\tCCM_DECRYPT_DELAY_TAG_CLK_COUNT);\n\n\t \n\tfor (i = 0; i < tag_size; i++) {\n\t\tiowrite8(in_tag[i], aes_dev->base_reg +\n\t\t\t\t    AES_A_DMA_INBUFFER_WRITE_FIFO_OFFSET);\n\t}\n}\n\n \nstatic int ocs_aes_ccm_write_b0(const struct ocs_aes_dev *aes_dev,\n\t\t\t\tconst u8 *iv, u32 adata_size, u32 tag_size,\n\t\t\t\tu32 cryptlen)\n{\n\tu8 b0[16];  \n\tint i, q;\n\n\t \n\tmemset(b0, 0, sizeof(b0));\n\n\t \n\t \n\tif (adata_size)\n\t\tb0[0] |= BIT(6);\n\t \n\tb0[0] |= (((tag_size - 2) / 2) & 0x7)  << 3;\n\t \n\tb0[0] |= iv[0] & 0x7;\n\t \n\tq = (iv[0] & 0x7) + 1;\n\tfor (i = 1; i <= 15 - q; i++)\n\t\tb0[i] = iv[i];\n\t \n\ti = sizeof(b0) - 1;\n\twhile (q) {\n\t\tb0[i] = cryptlen & 0xff;\n\t\tcryptlen >>= 8;\n\t\ti--;\n\t\tq--;\n\t}\n\t \n\tif (cryptlen)\n\t\treturn -EOVERFLOW;\n\t \n\tfor (i = 0; i < sizeof(b0); i++)\n\t\tiowrite8(b0[i], aes_dev->base_reg +\n\t\t\t\tAES_A_DMA_INBUFFER_WRITE_FIFO_OFFSET);\n\treturn 0;\n}\n\n \nstatic void ocs_aes_ccm_write_adata_len(const struct ocs_aes_dev *aes_dev,\n\t\t\t\t\tu64 adata_len)\n{\n\tu8 enc_a[10];  \n\tint i, len;\n\n\t \n\tif (adata_len < 65280) {\n\t\tlen = 2;\n\t\t*(__be16 *)enc_a = cpu_to_be16(adata_len);\n\t} else if (adata_len <= 0xFFFFFFFF) {\n\t\tlen = 6;\n\t\t*(__be16 *)enc_a = cpu_to_be16(0xfffe);\n\t\t*(__be32 *)&enc_a[2] = cpu_to_be32(adata_len);\n\t} else {  \n\t\tlen = 10;\n\t\t*(__be16 *)enc_a = cpu_to_be16(0xffff);\n\t\t*(__be64 *)&enc_a[2] = cpu_to_be64(adata_len);\n\t}\n\tfor (i = 0; i < len; i++)\n\t\tiowrite8(enc_a[i],\n\t\t\t aes_dev->base_reg +\n\t\t\t AES_A_DMA_INBUFFER_WRITE_FIFO_OFFSET);\n}\n\nstatic int ocs_aes_ccm_do_adata(struct ocs_aes_dev *aes_dev,\n\t\t\t\tdma_addr_t adata_dma_list, u32 adata_size)\n{\n\tint rc;\n\n\tif (!adata_size) {\n\t\t \n\t\taes_a_set_last_gcx_and_adata(aes_dev);\n\t\tgoto exit;\n\t}\n\n\t \n\n\t \n\tocs_aes_ccm_write_adata_len(aes_dev, adata_size);\n\n\t \n\tdma_to_ocs_aes_ll(aes_dev, adata_dma_list);\n\n\t \n\taes_a_dma_active_src_ll_en(aes_dev);\n\n\t \n\taes_a_set_last_gcx_and_adata(aes_dev);\n\n\t \n\trc = ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);\n\tif (rc)\n\t\treturn rc;\n\nexit:\n\t \n\taes_a_wait_last_gcx(aes_dev);\n\taes_a_dma_wait_input_buffer_occupancy(aes_dev);\n\n\treturn 0;\n}\n\nstatic int ocs_aes_ccm_encrypt_do_payload(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\t  dma_addr_t dst_dma_list,\n\t\t\t\t\t  dma_addr_t src_dma_list,\n\t\t\t\t\t  u32 src_size)\n{\n\tif (src_size) {\n\t\t \n\t\tdma_to_ocs_aes_ll(aes_dev, src_dma_list);\n\t\tdma_from_ocs_aes_ll(aes_dev, dst_dma_list);\n\t\taes_a_dma_active_src_dst_ll_en(aes_dev);\n\t} else {\n\t\t \n\t\tdma_from_ocs_aes_ll(aes_dev, dst_dma_list);\n\t\taes_a_dma_active_dst_ll_en(aes_dev);\n\t}\n\n\t \n\taes_a_set_last_gcx(aes_dev);\n\n\t \n\treturn ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);\n}\n\nstatic int ocs_aes_ccm_decrypt_do_payload(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\t  dma_addr_t dst_dma_list,\n\t\t\t\t\t  dma_addr_t src_dma_list,\n\t\t\t\t\t  u32 src_size)\n{\n\tif (!src_size) {\n\t\t \n\t\taes_a_dma_set_xfer_size_zero(aes_dev);\n\t\taes_a_dma_active(aes_dev);\n\t\taes_a_set_last_gcx(aes_dev);\n\n\t\treturn 0;\n\t}\n\n\t \n\tdma_to_ocs_aes_ll(aes_dev, src_dma_list);\n\tdma_from_ocs_aes_ll(aes_dev, dst_dma_list);\n\taes_a_dma_active_src_dst_ll_en(aes_dev);\n\t \n\taes_a_set_last_gcx(aes_dev);\n\t  \n\treturn ocs_aes_irq_enable_and_wait(aes_dev, AES_DMA_SRC_DONE_INT);\n}\n\n \nstatic inline int ccm_compare_tag_to_yr(struct ocs_aes_dev *aes_dev,\n\t\t\t\t\tu8 tag_size_bytes)\n{\n\tu32 tag[AES_MAX_TAG_SIZE_U32];\n\tu32 yr[AES_MAX_TAG_SIZE_U32];\n\tu8 i;\n\n\t \n\tfor (i = 0; i < AES_MAX_TAG_SIZE_U32; i++) {\n\t\ttag[i] = ioread32(aes_dev->base_reg +\n\t\t\t\t  AES_T_MAC_0_OFFSET + (i * sizeof(u32)));\n\t\tyr[i] = ioread32(aes_dev->base_reg +\n\t\t\t\t AES_MULTIPURPOSE2_0_OFFSET +\n\t\t\t\t (i * sizeof(u32)));\n\t}\n\n\treturn memcmp(tag, yr, tag_size_bytes) ? -EBADMSG : 0;\n}\n\n \nint ocs_aes_ccm_op(struct ocs_aes_dev *aes_dev,\n\t\t   enum ocs_cipher cipher,\n\t\t   enum ocs_instruction instruction,\n\t\t   dma_addr_t dst_dma_list,\n\t\t   dma_addr_t src_dma_list,\n\t\t   u32 src_size,\n\t\t   u8 *iv,\n\t\t   dma_addr_t adata_dma_list,\n\t\t   u32 adata_size,\n\t\t   u8 *in_tag,\n\t\t   u32 tag_size)\n{\n\tu32 *iv_32;\n\tu8 lprime;\n\tint rc;\n\n\trc = ocs_aes_validate_inputs(src_dma_list, src_size, iv,\n\t\t\t\t     AES_BLOCK_SIZE, adata_dma_list, adata_size,\n\t\t\t\t     in_tag, tag_size, cipher, OCS_MODE_CCM,\n\t\t\t\t     instruction, dst_dma_list);\n\tif (rc)\n\t\treturn rc;\n\n\tocs_aes_init(aes_dev, OCS_MODE_CCM, cipher, instruction);\n\n\t \n\tlprime = iv[L_PRIME_IDX];\n\tmemset(&iv[COUNTER_START(lprime)], 0, COUNTER_LEN(lprime));\n\n\t \n\tiv_32 = (u32 *)iv;\n\tiowrite32(__swab32(iv_32[0]),\n\t\t  aes_dev->base_reg + AES_MULTIPURPOSE1_3_OFFSET);\n\tiowrite32(__swab32(iv_32[1]),\n\t\t  aes_dev->base_reg + AES_MULTIPURPOSE1_2_OFFSET);\n\tiowrite32(__swab32(iv_32[2]),\n\t\t  aes_dev->base_reg + AES_MULTIPURPOSE1_1_OFFSET);\n\tiowrite32(__swab32(iv_32[3]),\n\t\t  aes_dev->base_reg + AES_MULTIPURPOSE1_0_OFFSET);\n\n\t \n\tiowrite32(tag_size, aes_dev->base_reg + AES_TLEN_OFFSET);\n\t \n\tocs_aes_write_last_data_blk_len(aes_dev, src_size);\n\n\t \n\taes_a_op_trigger(aes_dev);\n\n\taes_a_dma_reset_and_activate_perf_cntr(aes_dev);\n\n\t \n\trc = ocs_aes_ccm_write_b0(aes_dev, iv, adata_size, tag_size, src_size);\n\tif (rc)\n\t\treturn rc;\n\t \n\taes_a_dma_wait_and_deactivate_perf_cntr(aes_dev,\n\t\t\t\t\t\tCCM_DECRYPT_DELAY_LAST_GCX_CLK_COUNT);\n\n\t \n\tocs_aes_ccm_do_adata(aes_dev, adata_dma_list, adata_size);\n\n\t \n\tif (instruction == OCS_ENCRYPT) {\n\t\treturn ocs_aes_ccm_encrypt_do_payload(aes_dev, dst_dma_list,\n\t\t\t\t\t\t      src_dma_list, src_size);\n\t}\n\t \n\trc = ocs_aes_ccm_decrypt_do_payload(aes_dev, dst_dma_list,\n\t\t\t\t\t    src_dma_list, src_size);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tocs_aes_ccm_write_encrypted_tag(aes_dev, in_tag, tag_size);\n\trc = ocs_aes_irq_enable_and_wait(aes_dev, AES_COMPLETE_INT);\n\tif (rc)\n\t\treturn rc;\n\n\treturn ccm_compare_tag_to_yr(aes_dev, tag_size);\n}\n\n \nint ocs_create_linked_list_from_sg(const struct ocs_aes_dev *aes_dev,\n\t\t\t\t   struct scatterlist *sg,\n\t\t\t\t   int sg_dma_count,\n\t\t\t\t   struct ocs_dll_desc *dll_desc,\n\t\t\t\t   size_t data_size, size_t data_offset)\n{\n\tstruct ocs_dma_linked_list *ll = NULL;\n\tstruct scatterlist *sg_tmp;\n\tunsigned int tmp;\n\tint dma_nents;\n\tint i;\n\n\tif (!dll_desc || !sg || !aes_dev)\n\t\treturn -EINVAL;\n\n\t \n\tdll_desc->vaddr = NULL;\n\tdll_desc->dma_addr = DMA_MAPPING_ERROR;\n\tdll_desc->size = 0;\n\n\tif (data_size == 0)\n\t\treturn 0;\n\n\t \n\twhile (data_offset >= sg_dma_len(sg)) {\n\t\tdata_offset -= sg_dma_len(sg);\n\t\tsg_dma_count--;\n\t\tsg = sg_next(sg);\n\t\t \n\t\tif (!sg || sg_dma_count == 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tdma_nents = 0;\n\ttmp = 0;\n\tsg_tmp = sg;\n\twhile (tmp < data_offset + data_size) {\n\t\t \n\t\tif (!sg_tmp)\n\t\t\treturn -EINVAL;\n\t\ttmp += sg_dma_len(sg_tmp);\n\t\tdma_nents++;\n\t\tsg_tmp = sg_next(sg_tmp);\n\t}\n\tif (dma_nents > sg_dma_count)\n\t\treturn -EINVAL;\n\n\t \n\tdll_desc->size = sizeof(struct ocs_dma_linked_list) * dma_nents;\n\tdll_desc->vaddr = dma_alloc_coherent(aes_dev->dev, dll_desc->size,\n\t\t\t\t\t     &dll_desc->dma_addr, GFP_KERNEL);\n\tif (!dll_desc->vaddr)\n\t\treturn -ENOMEM;\n\n\t \n\tll = dll_desc->vaddr;\n\tfor (i = 0; i < dma_nents; i++, sg = sg_next(sg)) {\n\t\tll[i].src_addr = sg_dma_address(sg) + data_offset;\n\t\tll[i].src_len = (sg_dma_len(sg) - data_offset) < data_size ?\n\t\t\t\t(sg_dma_len(sg) - data_offset) : data_size;\n\t\tdata_offset = 0;\n\t\tdata_size -= ll[i].src_len;\n\t\t \n\t\tll[i].next = dll_desc->dma_addr + (sizeof(*ll) * (i + 1));\n\t\tll[i].ll_flags = 0;\n\t}\n\t \n\tll[i - 1].next = 0;\n\tll[i - 1].ll_flags = OCS_LL_DMA_FLAG_TERMINATE;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}