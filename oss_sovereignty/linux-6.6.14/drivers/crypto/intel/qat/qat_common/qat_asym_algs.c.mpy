{
  "module_name": "qat_asym_algs.c",
  "hash_id": "ea382d8aea35f08fe983f03b348b1443fce61b82e7387ea80a692872091e4fc8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/intel/qat/qat_common/qat_asym_algs.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <crypto/internal/rsa.h>\n#include <crypto/internal/akcipher.h>\n#include <crypto/akcipher.h>\n#include <crypto/kpp.h>\n#include <crypto/internal/kpp.h>\n#include <crypto/dh.h>\n#include <linux/dma-mapping.h>\n#include <linux/fips.h>\n#include <crypto/scatterwalk.h>\n#include \"icp_qat_fw_pke.h\"\n#include \"adf_accel_devices.h\"\n#include \"qat_algs_send.h\"\n#include \"adf_transport.h\"\n#include \"adf_common_drv.h\"\n#include \"qat_crypto.h\"\n\nstatic DEFINE_MUTEX(algs_lock);\nstatic unsigned int active_devs;\n\nstruct qat_rsa_input_params {\n\tunion {\n\t\tstruct {\n\t\t\tdma_addr_t m;\n\t\t\tdma_addr_t e;\n\t\t\tdma_addr_t n;\n\t\t} enc;\n\t\tstruct {\n\t\t\tdma_addr_t c;\n\t\t\tdma_addr_t d;\n\t\t\tdma_addr_t n;\n\t\t} dec;\n\t\tstruct {\n\t\t\tdma_addr_t c;\n\t\t\tdma_addr_t p;\n\t\t\tdma_addr_t q;\n\t\t\tdma_addr_t dp;\n\t\t\tdma_addr_t dq;\n\t\t\tdma_addr_t qinv;\n\t\t} dec_crt;\n\t\tu64 in_tab[8];\n\t};\n} __packed __aligned(64);\n\nstruct qat_rsa_output_params {\n\tunion {\n\t\tstruct {\n\t\t\tdma_addr_t c;\n\t\t} enc;\n\t\tstruct {\n\t\t\tdma_addr_t m;\n\t\t} dec;\n\t\tu64 out_tab[8];\n\t};\n} __packed __aligned(64);\n\nstruct qat_rsa_ctx {\n\tchar *n;\n\tchar *e;\n\tchar *d;\n\tchar *p;\n\tchar *q;\n\tchar *dp;\n\tchar *dq;\n\tchar *qinv;\n\tdma_addr_t dma_n;\n\tdma_addr_t dma_e;\n\tdma_addr_t dma_d;\n\tdma_addr_t dma_p;\n\tdma_addr_t dma_q;\n\tdma_addr_t dma_dp;\n\tdma_addr_t dma_dq;\n\tdma_addr_t dma_qinv;\n\tunsigned int key_sz;\n\tbool crt_mode;\n\tstruct qat_crypto_instance *inst;\n} __packed __aligned(64);\n\nstruct qat_dh_input_params {\n\tunion {\n\t\tstruct {\n\t\t\tdma_addr_t b;\n\t\t\tdma_addr_t xa;\n\t\t\tdma_addr_t p;\n\t\t} in;\n\t\tstruct {\n\t\t\tdma_addr_t xa;\n\t\t\tdma_addr_t p;\n\t\t} in_g2;\n\t\tu64 in_tab[8];\n\t};\n} __packed __aligned(64);\n\nstruct qat_dh_output_params {\n\tunion {\n\t\tdma_addr_t r;\n\t\tu64 out_tab[8];\n\t};\n} __packed __aligned(64);\n\nstruct qat_dh_ctx {\n\tchar *g;\n\tchar *xa;\n\tchar *p;\n\tdma_addr_t dma_g;\n\tdma_addr_t dma_xa;\n\tdma_addr_t dma_p;\n\tunsigned int p_size;\n\tbool g2;\n\tstruct qat_crypto_instance *inst;\n} __packed __aligned(64);\n\nstruct qat_asym_request {\n\tunion {\n\t\tstruct qat_rsa_input_params rsa;\n\t\tstruct qat_dh_input_params dh;\n\t} in;\n\tunion {\n\t\tstruct qat_rsa_output_params rsa;\n\t\tstruct qat_dh_output_params dh;\n\t} out;\n\tdma_addr_t phy_in;\n\tdma_addr_t phy_out;\n\tchar *src_align;\n\tchar *dst_align;\n\tstruct icp_qat_fw_pke_request req;\n\tunion {\n\t\tstruct qat_rsa_ctx *rsa;\n\t\tstruct qat_dh_ctx *dh;\n\t} ctx;\n\tunion {\n\t\tstruct akcipher_request *rsa;\n\t\tstruct kpp_request *dh;\n\t} areq;\n\tint err;\n\tvoid (*cb)(struct icp_qat_fw_pke_resp *resp);\n\tstruct qat_alg_req alg_req;\n} __aligned(64);\n\nstatic int qat_alg_send_asym_message(struct qat_asym_request *qat_req,\n\t\t\t\t     struct qat_crypto_instance *inst,\n\t\t\t\t     struct crypto_async_request *base)\n{\n\tstruct qat_alg_req *alg_req = &qat_req->alg_req;\n\n\talg_req->fw_req = (u32 *)&qat_req->req;\n\talg_req->tx_ring = inst->pke_tx;\n\talg_req->base = base;\n\talg_req->backlog = &inst->backlog;\n\n\treturn qat_alg_send_message(alg_req);\n}\n\nstatic void qat_dh_cb(struct icp_qat_fw_pke_resp *resp)\n{\n\tstruct qat_asym_request *req = (void *)(__force long)resp->opaque;\n\tstruct kpp_request *areq = req->areq.dh;\n\tstruct device *dev = &GET_DEV(req->ctx.dh->inst->accel_dev);\n\tint err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(\n\t\t\t\tresp->pke_resp_hdr.comn_resp_flags);\n\n\terr = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;\n\n\tif (areq->src) {\n\t\tdma_unmap_single(dev, req->in.dh.in.b, req->ctx.dh->p_size,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tkfree_sensitive(req->src_align);\n\t}\n\n\tareq->dst_len = req->ctx.dh->p_size;\n\tdma_unmap_single(dev, req->out.dh.r, req->ctx.dh->p_size,\n\t\t\t DMA_FROM_DEVICE);\n\tif (req->dst_align) {\n\t\tscatterwalk_map_and_copy(req->dst_align, areq->dst, 0,\n\t\t\t\t\t areq->dst_len, 1);\n\t\tkfree_sensitive(req->dst_align);\n\t}\n\n\tdma_unmap_single(dev, req->phy_in, sizeof(struct qat_dh_input_params),\n\t\t\t DMA_TO_DEVICE);\n\tdma_unmap_single(dev, req->phy_out,\n\t\t\t sizeof(struct qat_dh_output_params),\n\t\t\t DMA_TO_DEVICE);\n\n\tkpp_request_complete(areq, err);\n}\n\n#define PKE_DH_1536 0x390c1a49\n#define PKE_DH_G2_1536 0x2e0b1a3e\n#define PKE_DH_2048 0x4d0c1a60\n#define PKE_DH_G2_2048 0x3e0b1a55\n#define PKE_DH_3072 0x510c1a77\n#define PKE_DH_G2_3072 0x3a0b1a6c\n#define PKE_DH_4096 0x690c1a8e\n#define PKE_DH_G2_4096 0x4a0b1a83\n\nstatic unsigned long qat_dh_fn_id(unsigned int len, bool g2)\n{\n\tunsigned int bitslen = len << 3;\n\n\tswitch (bitslen) {\n\tcase 1536:\n\t\treturn g2 ? PKE_DH_G2_1536 : PKE_DH_1536;\n\tcase 2048:\n\t\treturn g2 ? PKE_DH_G2_2048 : PKE_DH_2048;\n\tcase 3072:\n\t\treturn g2 ? PKE_DH_G2_3072 : PKE_DH_3072;\n\tcase 4096:\n\t\treturn g2 ? PKE_DH_G2_4096 : PKE_DH_4096;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int qat_dh_compute_value(struct kpp_request *req)\n{\n\tstruct crypto_kpp *tfm = crypto_kpp_reqtfm(req);\n\tstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tstruct qat_asym_request *qat_req =\n\t\t\tPTR_ALIGN(kpp_request_ctx(req), 64);\n\tstruct icp_qat_fw_pke_request *msg = &qat_req->req;\n\tgfp_t flags = qat_algs_alloc_flags(&req->base);\n\tint n_input_params = 0;\n\tu8 *vaddr;\n\tint ret;\n\n\tif (unlikely(!ctx->xa))\n\t\treturn -EINVAL;\n\n\tif (req->dst_len < ctx->p_size) {\n\t\treq->dst_len = ctx->p_size;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tif (req->src_len > ctx->p_size)\n\t\treturn -EINVAL;\n\n\tmemset(msg, '\\0', sizeof(*msg));\n\tICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\n\t\t\t\t\t  ICP_QAT_FW_COMN_REQ_FLAG_SET);\n\n\tmsg->pke_hdr.cd_pars.func_id = qat_dh_fn_id(ctx->p_size,\n\t\t\t\t\t\t    !req->src && ctx->g2);\n\tif (unlikely(!msg->pke_hdr.cd_pars.func_id))\n\t\treturn -EINVAL;\n\n\tqat_req->cb = qat_dh_cb;\n\tqat_req->ctx.dh = ctx;\n\tqat_req->areq.dh = req;\n\tmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\n\tmsg->pke_hdr.comn_req_flags =\n\t\tICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\n\t\t\t\t\t    QAT_COMN_CD_FLD_TYPE_64BIT_ADR);\n\n\t \n\tif (req->src) {\n\t\tqat_req->in.dh.in.xa = ctx->dma_xa;\n\t\tqat_req->in.dh.in.p = ctx->dma_p;\n\t\tn_input_params = 3;\n\t} else {\n\t\tif (ctx->g2) {\n\t\t\tqat_req->in.dh.in_g2.xa = ctx->dma_xa;\n\t\t\tqat_req->in.dh.in_g2.p = ctx->dma_p;\n\t\t\tn_input_params = 2;\n\t\t} else {\n\t\t\tqat_req->in.dh.in.b = ctx->dma_g;\n\t\t\tqat_req->in.dh.in.xa = ctx->dma_xa;\n\t\t\tqat_req->in.dh.in.p = ctx->dma_p;\n\t\t\tn_input_params = 3;\n\t\t}\n\t}\n\n\tret = -ENOMEM;\n\tif (req->src) {\n\t\t \n\t\tif (sg_is_last(req->src) && req->src_len == ctx->p_size) {\n\t\t\tqat_req->src_align = NULL;\n\t\t\tvaddr = sg_virt(req->src);\n\t\t} else {\n\t\t\tint shift = ctx->p_size - req->src_len;\n\n\t\t\tqat_req->src_align = kzalloc(ctx->p_size, flags);\n\t\t\tif (unlikely(!qat_req->src_align))\n\t\t\t\treturn ret;\n\n\t\t\tscatterwalk_map_and_copy(qat_req->src_align + shift,\n\t\t\t\t\t\t req->src, 0, req->src_len, 0);\n\n\t\t\tvaddr = qat_req->src_align;\n\t\t}\n\n\t\tqat_req->in.dh.in.b = dma_map_single(dev, vaddr, ctx->p_size,\n\t\t\t\t\t\t     DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev, qat_req->in.dh.in.b)))\n\t\t\tgoto unmap_src;\n\t}\n\t \n\tif (sg_is_last(req->dst) && req->dst_len == ctx->p_size) {\n\t\tqat_req->dst_align = NULL;\n\t\tvaddr = sg_virt(req->dst);\n\t} else {\n\t\tqat_req->dst_align = kzalloc(ctx->p_size, flags);\n\t\tif (unlikely(!qat_req->dst_align))\n\t\t\tgoto unmap_src;\n\n\t\tvaddr = qat_req->dst_align;\n\t}\n\tqat_req->out.dh.r = dma_map_single(dev, vaddr, ctx->p_size,\n\t\t\t\t\t   DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->out.dh.r)))\n\t\tgoto unmap_dst;\n\n\tqat_req->in.dh.in_tab[n_input_params] = 0;\n\tqat_req->out.dh.out_tab[1] = 0;\n\t \n\tqat_req->phy_in = dma_map_single(dev, &qat_req->in.dh,\n\t\t\t\t\t sizeof(struct qat_dh_input_params),\n\t\t\t\t\t DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\n\t\tgoto unmap_dst;\n\n\tqat_req->phy_out = dma_map_single(dev, &qat_req->out.dh,\n\t\t\t\t\t  sizeof(struct qat_dh_output_params),\n\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\n\t\tgoto unmap_in_params;\n\n\tmsg->pke_mid.src_data_addr = qat_req->phy_in;\n\tmsg->pke_mid.dest_data_addr = qat_req->phy_out;\n\tmsg->pke_mid.opaque = (u64)(__force long)qat_req;\n\tmsg->input_param_count = n_input_params;\n\tmsg->output_param_count = 1;\n\n\tret = qat_alg_send_asym_message(qat_req, inst, &req->base);\n\tif (ret == -ENOSPC)\n\t\tgoto unmap_all;\n\n\treturn ret;\n\nunmap_all:\n\tif (!dma_mapping_error(dev, qat_req->phy_out))\n\t\tdma_unmap_single(dev, qat_req->phy_out,\n\t\t\t\t sizeof(struct qat_dh_output_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_in_params:\n\tif (!dma_mapping_error(dev, qat_req->phy_in))\n\t\tdma_unmap_single(dev, qat_req->phy_in,\n\t\t\t\t sizeof(struct qat_dh_input_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_dst:\n\tif (!dma_mapping_error(dev, qat_req->out.dh.r))\n\t\tdma_unmap_single(dev, qat_req->out.dh.r, ctx->p_size,\n\t\t\t\t DMA_FROM_DEVICE);\n\tkfree_sensitive(qat_req->dst_align);\nunmap_src:\n\tif (req->src) {\n\t\tif (!dma_mapping_error(dev, qat_req->in.dh.in.b))\n\t\t\tdma_unmap_single(dev, qat_req->in.dh.in.b,\n\t\t\t\t\t ctx->p_size,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tkfree_sensitive(qat_req->src_align);\n\t}\n\treturn ret;\n}\n\nstatic int qat_dh_check_params_length(unsigned int p_len)\n{\n\tswitch (p_len) {\n\tcase 1536:\n\tcase 2048:\n\tcase 3072:\n\tcase 4096:\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int qat_dh_set_params(struct qat_dh_ctx *ctx, struct dh *params)\n{\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\n\tif (qat_dh_check_params_length(params->p_size << 3))\n\t\treturn -EINVAL;\n\n\tctx->p_size = params->p_size;\n\tctx->p = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_p, GFP_KERNEL);\n\tif (!ctx->p)\n\t\treturn -ENOMEM;\n\tmemcpy(ctx->p, params->p, ctx->p_size);\n\n\t \n\tif (params->g_size == 1 && *(char *)params->g == 0x02) {\n\t\tctx->g2 = true;\n\t\treturn 0;\n\t}\n\n\tctx->g = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_g, GFP_KERNEL);\n\tif (!ctx->g)\n\t\treturn -ENOMEM;\n\tmemcpy(ctx->g + (ctx->p_size - params->g_size), params->g,\n\t       params->g_size);\n\n\treturn 0;\n}\n\nstatic void qat_dh_clear_ctx(struct device *dev, struct qat_dh_ctx *ctx)\n{\n\tif (ctx->g) {\n\t\tmemset(ctx->g, 0, ctx->p_size);\n\t\tdma_free_coherent(dev, ctx->p_size, ctx->g, ctx->dma_g);\n\t\tctx->g = NULL;\n\t}\n\tif (ctx->xa) {\n\t\tmemset(ctx->xa, 0, ctx->p_size);\n\t\tdma_free_coherent(dev, ctx->p_size, ctx->xa, ctx->dma_xa);\n\t\tctx->xa = NULL;\n\t}\n\tif (ctx->p) {\n\t\tmemset(ctx->p, 0, ctx->p_size);\n\t\tdma_free_coherent(dev, ctx->p_size, ctx->p, ctx->dma_p);\n\t\tctx->p = NULL;\n\t}\n\tctx->p_size = 0;\n\tctx->g2 = false;\n}\n\nstatic int qat_dh_set_secret(struct crypto_kpp *tfm, const void *buf,\n\t\t\t     unsigned int len)\n{\n\tstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\n\tstruct dh params;\n\tint ret;\n\n\tif (crypto_dh_decode_key(buf, len, &params) < 0)\n\t\treturn -EINVAL;\n\n\t \n\tqat_dh_clear_ctx(dev, ctx);\n\n\tret = qat_dh_set_params(ctx, &params);\n\tif (ret < 0)\n\t\tgoto err_clear_ctx;\n\n\tctx->xa = dma_alloc_coherent(dev, ctx->p_size, &ctx->dma_xa,\n\t\t\t\t     GFP_KERNEL);\n\tif (!ctx->xa) {\n\t\tret = -ENOMEM;\n\t\tgoto err_clear_ctx;\n\t}\n\tmemcpy(ctx->xa + (ctx->p_size - params.key_size), params.key,\n\t       params.key_size);\n\n\treturn 0;\n\nerr_clear_ctx:\n\tqat_dh_clear_ctx(dev, ctx);\n\treturn ret;\n}\n\nstatic unsigned int qat_dh_max_size(struct crypto_kpp *tfm)\n{\n\tstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\n\n\treturn ctx->p_size;\n}\n\nstatic int qat_dh_init_tfm(struct crypto_kpp *tfm)\n{\n\tstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct qat_crypto_instance *inst =\n\t\t\tqat_crypto_get_instance_node(numa_node_id());\n\n\tif (!inst)\n\t\treturn -EINVAL;\n\n\tkpp_set_reqsize(tfm, sizeof(struct qat_asym_request) + 64);\n\n\tctx->p_size = 0;\n\tctx->g2 = false;\n\tctx->inst = inst;\n\treturn 0;\n}\n\nstatic void qat_dh_exit_tfm(struct crypto_kpp *tfm)\n{\n\tstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\n\tstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\n\n\tqat_dh_clear_ctx(dev, ctx);\n\tqat_crypto_put_instance(ctx->inst);\n}\n\nstatic void qat_rsa_cb(struct icp_qat_fw_pke_resp *resp)\n{\n\tstruct qat_asym_request *req = (void *)(__force long)resp->opaque;\n\tstruct akcipher_request *areq = req->areq.rsa;\n\tstruct device *dev = &GET_DEV(req->ctx.rsa->inst->accel_dev);\n\tint err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(\n\t\t\t\tresp->pke_resp_hdr.comn_resp_flags);\n\n\terr = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;\n\n\tdma_unmap_single(dev, req->in.rsa.enc.m, req->ctx.rsa->key_sz,\n\t\t\t DMA_TO_DEVICE);\n\n\tkfree_sensitive(req->src_align);\n\n\tareq->dst_len = req->ctx.rsa->key_sz;\n\tdma_unmap_single(dev, req->out.rsa.enc.c, req->ctx.rsa->key_sz,\n\t\t\t DMA_FROM_DEVICE);\n\tif (req->dst_align) {\n\t\tscatterwalk_map_and_copy(req->dst_align, areq->dst, 0,\n\t\t\t\t\t areq->dst_len, 1);\n\n\t\tkfree_sensitive(req->dst_align);\n\t}\n\n\tdma_unmap_single(dev, req->phy_in, sizeof(struct qat_rsa_input_params),\n\t\t\t DMA_TO_DEVICE);\n\tdma_unmap_single(dev, req->phy_out,\n\t\t\t sizeof(struct qat_rsa_output_params),\n\t\t\t DMA_TO_DEVICE);\n\n\takcipher_request_complete(areq, err);\n}\n\nvoid qat_alg_asym_callback(void *_resp)\n{\n\tstruct icp_qat_fw_pke_resp *resp = _resp;\n\tstruct qat_asym_request *areq = (void *)(__force long)resp->opaque;\n\tstruct qat_instance_backlog *backlog = areq->alg_req.backlog;\n\n\tareq->cb(resp);\n\n\tqat_alg_send_backlog(backlog);\n}\n\n#define PKE_RSA_EP_512 0x1c161b21\n#define PKE_RSA_EP_1024 0x35111bf7\n#define PKE_RSA_EP_1536 0x4d111cdc\n#define PKE_RSA_EP_2048 0x6e111dba\n#define PKE_RSA_EP_3072 0x7d111ea3\n#define PKE_RSA_EP_4096 0xa5101f7e\n\nstatic unsigned long qat_rsa_enc_fn_id(unsigned int len)\n{\n\tunsigned int bitslen = len << 3;\n\n\tswitch (bitslen) {\n\tcase 512:\n\t\treturn PKE_RSA_EP_512;\n\tcase 1024:\n\t\treturn PKE_RSA_EP_1024;\n\tcase 1536:\n\t\treturn PKE_RSA_EP_1536;\n\tcase 2048:\n\t\treturn PKE_RSA_EP_2048;\n\tcase 3072:\n\t\treturn PKE_RSA_EP_3072;\n\tcase 4096:\n\t\treturn PKE_RSA_EP_4096;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n#define PKE_RSA_DP1_512 0x1c161b3c\n#define PKE_RSA_DP1_1024 0x35111c12\n#define PKE_RSA_DP1_1536 0x4d111cf7\n#define PKE_RSA_DP1_2048 0x6e111dda\n#define PKE_RSA_DP1_3072 0x7d111ebe\n#define PKE_RSA_DP1_4096 0xa5101f98\n\nstatic unsigned long qat_rsa_dec_fn_id(unsigned int len)\n{\n\tunsigned int bitslen = len << 3;\n\n\tswitch (bitslen) {\n\tcase 512:\n\t\treturn PKE_RSA_DP1_512;\n\tcase 1024:\n\t\treturn PKE_RSA_DP1_1024;\n\tcase 1536:\n\t\treturn PKE_RSA_DP1_1536;\n\tcase 2048:\n\t\treturn PKE_RSA_DP1_2048;\n\tcase 3072:\n\t\treturn PKE_RSA_DP1_3072;\n\tcase 4096:\n\t\treturn PKE_RSA_DP1_4096;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n#define PKE_RSA_DP2_512 0x1c131b57\n#define PKE_RSA_DP2_1024 0x26131c2d\n#define PKE_RSA_DP2_1536 0x45111d12\n#define PKE_RSA_DP2_2048 0x59121dfa\n#define PKE_RSA_DP2_3072 0x81121ed9\n#define PKE_RSA_DP2_4096 0xb1111fb2\n\nstatic unsigned long qat_rsa_dec_fn_id_crt(unsigned int len)\n{\n\tunsigned int bitslen = len << 3;\n\n\tswitch (bitslen) {\n\tcase 512:\n\t\treturn PKE_RSA_DP2_512;\n\tcase 1024:\n\t\treturn PKE_RSA_DP2_1024;\n\tcase 1536:\n\t\treturn PKE_RSA_DP2_1536;\n\tcase 2048:\n\t\treturn PKE_RSA_DP2_2048;\n\tcase 3072:\n\t\treturn PKE_RSA_DP2_3072;\n\tcase 4096:\n\t\treturn PKE_RSA_DP2_4096;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int qat_rsa_enc(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tstruct qat_asym_request *qat_req =\n\t\t\tPTR_ALIGN(akcipher_request_ctx(req), 64);\n\tstruct icp_qat_fw_pke_request *msg = &qat_req->req;\n\tgfp_t flags = qat_algs_alloc_flags(&req->base);\n\tu8 *vaddr;\n\tint ret;\n\n\tif (unlikely(!ctx->n || !ctx->e))\n\t\treturn -EINVAL;\n\n\tif (req->dst_len < ctx->key_sz) {\n\t\treq->dst_len = ctx->key_sz;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tif (req->src_len > ctx->key_sz)\n\t\treturn -EINVAL;\n\n\tmemset(msg, '\\0', sizeof(*msg));\n\tICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\n\t\t\t\t\t  ICP_QAT_FW_COMN_REQ_FLAG_SET);\n\tmsg->pke_hdr.cd_pars.func_id = qat_rsa_enc_fn_id(ctx->key_sz);\n\tif (unlikely(!msg->pke_hdr.cd_pars.func_id))\n\t\treturn -EINVAL;\n\n\tqat_req->cb = qat_rsa_cb;\n\tqat_req->ctx.rsa = ctx;\n\tqat_req->areq.rsa = req;\n\tmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\n\tmsg->pke_hdr.comn_req_flags =\n\t\tICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\n\t\t\t\t\t    QAT_COMN_CD_FLD_TYPE_64BIT_ADR);\n\n\tqat_req->in.rsa.enc.e = ctx->dma_e;\n\tqat_req->in.rsa.enc.n = ctx->dma_n;\n\tret = -ENOMEM;\n\n\t \n\tif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\n\t\tqat_req->src_align = NULL;\n\t\tvaddr = sg_virt(req->src);\n\t} else {\n\t\tint shift = ctx->key_sz - req->src_len;\n\n\t\tqat_req->src_align = kzalloc(ctx->key_sz, flags);\n\t\tif (unlikely(!qat_req->src_align))\n\t\t\treturn ret;\n\n\t\tscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\n\t\t\t\t\t 0, req->src_len, 0);\n\t\tvaddr = qat_req->src_align;\n\t}\n\n\tqat_req->in.rsa.enc.m = dma_map_single(dev, vaddr, ctx->key_sz,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->in.rsa.enc.m)))\n\t\tgoto unmap_src;\n\n\tif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\n\t\tqat_req->dst_align = NULL;\n\t\tvaddr = sg_virt(req->dst);\n\t} else {\n\t\tqat_req->dst_align = kzalloc(ctx->key_sz, flags);\n\t\tif (unlikely(!qat_req->dst_align))\n\t\t\tgoto unmap_src;\n\t\tvaddr = qat_req->dst_align;\n\t}\n\n\tqat_req->out.rsa.enc.c = dma_map_single(dev, vaddr, ctx->key_sz,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->out.rsa.enc.c)))\n\t\tgoto unmap_dst;\n\n\tqat_req->in.rsa.in_tab[3] = 0;\n\tqat_req->out.rsa.out_tab[1] = 0;\n\tqat_req->phy_in = dma_map_single(dev, &qat_req->in.rsa,\n\t\t\t\t\t sizeof(struct qat_rsa_input_params),\n\t\t\t\t\t DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\n\t\tgoto unmap_dst;\n\n\tqat_req->phy_out = dma_map_single(dev, &qat_req->out.rsa,\n\t\t\t\t\t  sizeof(struct qat_rsa_output_params),\n\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\n\t\tgoto unmap_in_params;\n\n\tmsg->pke_mid.src_data_addr = qat_req->phy_in;\n\tmsg->pke_mid.dest_data_addr = qat_req->phy_out;\n\tmsg->pke_mid.opaque = (u64)(__force long)qat_req;\n\tmsg->input_param_count = 3;\n\tmsg->output_param_count = 1;\n\n\tret = qat_alg_send_asym_message(qat_req, inst, &req->base);\n\tif (ret == -ENOSPC)\n\t\tgoto unmap_all;\n\n\treturn ret;\n\nunmap_all:\n\tif (!dma_mapping_error(dev, qat_req->phy_out))\n\t\tdma_unmap_single(dev, qat_req->phy_out,\n\t\t\t\t sizeof(struct qat_rsa_output_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_in_params:\n\tif (!dma_mapping_error(dev, qat_req->phy_in))\n\t\tdma_unmap_single(dev, qat_req->phy_in,\n\t\t\t\t sizeof(struct qat_rsa_input_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_dst:\n\tif (!dma_mapping_error(dev, qat_req->out.rsa.enc.c))\n\t\tdma_unmap_single(dev, qat_req->out.rsa.enc.c,\n\t\t\t\t ctx->key_sz, DMA_FROM_DEVICE);\n\tkfree_sensitive(qat_req->dst_align);\nunmap_src:\n\tif (!dma_mapping_error(dev, qat_req->in.rsa.enc.m))\n\t\tdma_unmap_single(dev, qat_req->in.rsa.enc.m, ctx->key_sz,\n\t\t\t\t DMA_TO_DEVICE);\n\tkfree_sensitive(qat_req->src_align);\n\treturn ret;\n}\n\nstatic int qat_rsa_dec(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tstruct qat_asym_request *qat_req =\n\t\t\tPTR_ALIGN(akcipher_request_ctx(req), 64);\n\tstruct icp_qat_fw_pke_request *msg = &qat_req->req;\n\tgfp_t flags = qat_algs_alloc_flags(&req->base);\n\tu8 *vaddr;\n\tint ret;\n\n\tif (unlikely(!ctx->n || !ctx->d))\n\t\treturn -EINVAL;\n\n\tif (req->dst_len < ctx->key_sz) {\n\t\treq->dst_len = ctx->key_sz;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tif (req->src_len > ctx->key_sz)\n\t\treturn -EINVAL;\n\n\tmemset(msg, '\\0', sizeof(*msg));\n\tICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\n\t\t\t\t\t  ICP_QAT_FW_COMN_REQ_FLAG_SET);\n\tmsg->pke_hdr.cd_pars.func_id = ctx->crt_mode ?\n\t\tqat_rsa_dec_fn_id_crt(ctx->key_sz) :\n\t\tqat_rsa_dec_fn_id(ctx->key_sz);\n\tif (unlikely(!msg->pke_hdr.cd_pars.func_id))\n\t\treturn -EINVAL;\n\n\tqat_req->cb = qat_rsa_cb;\n\tqat_req->ctx.rsa = ctx;\n\tqat_req->areq.rsa = req;\n\tmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\n\tmsg->pke_hdr.comn_req_flags =\n\t\tICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\n\t\t\t\t\t    QAT_COMN_CD_FLD_TYPE_64BIT_ADR);\n\n\tif (ctx->crt_mode) {\n\t\tqat_req->in.rsa.dec_crt.p = ctx->dma_p;\n\t\tqat_req->in.rsa.dec_crt.q = ctx->dma_q;\n\t\tqat_req->in.rsa.dec_crt.dp = ctx->dma_dp;\n\t\tqat_req->in.rsa.dec_crt.dq = ctx->dma_dq;\n\t\tqat_req->in.rsa.dec_crt.qinv = ctx->dma_qinv;\n\t} else {\n\t\tqat_req->in.rsa.dec.d = ctx->dma_d;\n\t\tqat_req->in.rsa.dec.n = ctx->dma_n;\n\t}\n\tret = -ENOMEM;\n\n\t \n\tif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\n\t\tqat_req->src_align = NULL;\n\t\tvaddr = sg_virt(req->src);\n\t} else {\n\t\tint shift = ctx->key_sz - req->src_len;\n\n\t\tqat_req->src_align = kzalloc(ctx->key_sz, flags);\n\t\tif (unlikely(!qat_req->src_align))\n\t\t\treturn ret;\n\n\t\tscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\n\t\t\t\t\t 0, req->src_len, 0);\n\t\tvaddr = qat_req->src_align;\n\t}\n\n\tqat_req->in.rsa.dec.c = dma_map_single(dev, vaddr, ctx->key_sz,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->in.rsa.dec.c)))\n\t\tgoto unmap_src;\n\n\tif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\n\t\tqat_req->dst_align = NULL;\n\t\tvaddr = sg_virt(req->dst);\n\t} else {\n\t\tqat_req->dst_align = kzalloc(ctx->key_sz, flags);\n\t\tif (unlikely(!qat_req->dst_align))\n\t\t\tgoto unmap_src;\n\t\tvaddr = qat_req->dst_align;\n\t}\n\tqat_req->out.rsa.dec.m = dma_map_single(dev, vaddr, ctx->key_sz,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->out.rsa.dec.m)))\n\t\tgoto unmap_dst;\n\n\tif (ctx->crt_mode)\n\t\tqat_req->in.rsa.in_tab[6] = 0;\n\telse\n\t\tqat_req->in.rsa.in_tab[3] = 0;\n\tqat_req->out.rsa.out_tab[1] = 0;\n\tqat_req->phy_in = dma_map_single(dev, &qat_req->in.rsa,\n\t\t\t\t\t sizeof(struct qat_rsa_input_params),\n\t\t\t\t\t DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\n\t\tgoto unmap_dst;\n\n\tqat_req->phy_out = dma_map_single(dev, &qat_req->out.rsa,\n\t\t\t\t\t  sizeof(struct qat_rsa_output_params),\n\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\n\t\tgoto unmap_in_params;\n\n\tmsg->pke_mid.src_data_addr = qat_req->phy_in;\n\tmsg->pke_mid.dest_data_addr = qat_req->phy_out;\n\tmsg->pke_mid.opaque = (u64)(__force long)qat_req;\n\tif (ctx->crt_mode)\n\t\tmsg->input_param_count = 6;\n\telse\n\t\tmsg->input_param_count = 3;\n\n\tmsg->output_param_count = 1;\n\n\tret = qat_alg_send_asym_message(qat_req, inst, &req->base);\n\tif (ret == -ENOSPC)\n\t\tgoto unmap_all;\n\n\treturn ret;\n\nunmap_all:\n\tif (!dma_mapping_error(dev, qat_req->phy_out))\n\t\tdma_unmap_single(dev, qat_req->phy_out,\n\t\t\t\t sizeof(struct qat_rsa_output_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_in_params:\n\tif (!dma_mapping_error(dev, qat_req->phy_in))\n\t\tdma_unmap_single(dev, qat_req->phy_in,\n\t\t\t\t sizeof(struct qat_rsa_input_params),\n\t\t\t\t DMA_TO_DEVICE);\nunmap_dst:\n\tif (!dma_mapping_error(dev, qat_req->out.rsa.dec.m))\n\t\tdma_unmap_single(dev, qat_req->out.rsa.dec.m,\n\t\t\t\t ctx->key_sz, DMA_FROM_DEVICE);\n\tkfree_sensitive(qat_req->dst_align);\nunmap_src:\n\tif (!dma_mapping_error(dev, qat_req->in.rsa.dec.c))\n\t\tdma_unmap_single(dev, qat_req->in.rsa.dec.c, ctx->key_sz,\n\t\t\t\t DMA_TO_DEVICE);\n\tkfree_sensitive(qat_req->src_align);\n\treturn ret;\n}\n\nstatic int qat_rsa_set_n(struct qat_rsa_ctx *ctx, const char *value,\n\t\t\t size_t vlen)\n{\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tconst char *ptr = value;\n\tint ret;\n\n\twhile (!*ptr && vlen) {\n\t\tptr++;\n\t\tvlen--;\n\t}\n\n\tctx->key_sz = vlen;\n\tret = -EINVAL;\n\t \n\tif (!qat_rsa_enc_fn_id(ctx->key_sz))\n\t\tgoto err;\n\n\tret = -ENOMEM;\n\tctx->n = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_n, GFP_KERNEL);\n\tif (!ctx->n)\n\t\tgoto err;\n\n\tmemcpy(ctx->n, ptr, ctx->key_sz);\n\treturn 0;\nerr:\n\tctx->key_sz = 0;\n\tctx->n = NULL;\n\treturn ret;\n}\n\nstatic int qat_rsa_set_e(struct qat_rsa_ctx *ctx, const char *value,\n\t\t\t size_t vlen)\n{\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tconst char *ptr = value;\n\n\twhile (!*ptr && vlen) {\n\t\tptr++;\n\t\tvlen--;\n\t}\n\n\tif (!ctx->key_sz || !vlen || vlen > ctx->key_sz) {\n\t\tctx->e = NULL;\n\t\treturn -EINVAL;\n\t}\n\n\tctx->e = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_e, GFP_KERNEL);\n\tif (!ctx->e)\n\t\treturn -ENOMEM;\n\n\tmemcpy(ctx->e + (ctx->key_sz - vlen), ptr, vlen);\n\treturn 0;\n}\n\nstatic int qat_rsa_set_d(struct qat_rsa_ctx *ctx, const char *value,\n\t\t\t size_t vlen)\n{\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tconst char *ptr = value;\n\tint ret;\n\n\twhile (!*ptr && vlen) {\n\t\tptr++;\n\t\tvlen--;\n\t}\n\n\tret = -EINVAL;\n\tif (!ctx->key_sz || !vlen || vlen > ctx->key_sz)\n\t\tgoto err;\n\n\tret = -ENOMEM;\n\tctx->d = dma_alloc_coherent(dev, ctx->key_sz, &ctx->dma_d, GFP_KERNEL);\n\tif (!ctx->d)\n\t\tgoto err;\n\n\tmemcpy(ctx->d + (ctx->key_sz - vlen), ptr, vlen);\n\treturn 0;\nerr:\n\tctx->d = NULL;\n\treturn ret;\n}\n\nstatic void qat_rsa_drop_leading_zeros(const char **ptr, unsigned int *len)\n{\n\twhile (!**ptr && *len) {\n\t\t(*ptr)++;\n\t\t(*len)--;\n\t}\n}\n\nstatic void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)\n{\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev = &GET_DEV(inst->accel_dev);\n\tconst char *ptr;\n\tunsigned int len;\n\tunsigned int half_key_sz = ctx->key_sz / 2;\n\n\t \n\tptr = rsa_key->p;\n\tlen = rsa_key->p_sz;\n\tqat_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len)\n\t\tgoto err;\n\tctx->p = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_p, GFP_KERNEL);\n\tif (!ctx->p)\n\t\tgoto err;\n\tmemcpy(ctx->p + (half_key_sz - len), ptr, len);\n\n\t \n\tptr = rsa_key->q;\n\tlen = rsa_key->q_sz;\n\tqat_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len)\n\t\tgoto free_p;\n\tctx->q = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_q, GFP_KERNEL);\n\tif (!ctx->q)\n\t\tgoto free_p;\n\tmemcpy(ctx->q + (half_key_sz - len), ptr, len);\n\n\t \n\tptr = rsa_key->dp;\n\tlen = rsa_key->dp_sz;\n\tqat_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len)\n\t\tgoto free_q;\n\tctx->dp = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_dp,\n\t\t\t\t     GFP_KERNEL);\n\tif (!ctx->dp)\n\t\tgoto free_q;\n\tmemcpy(ctx->dp + (half_key_sz - len), ptr, len);\n\n\t \n\tptr = rsa_key->dq;\n\tlen = rsa_key->dq_sz;\n\tqat_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len)\n\t\tgoto free_dp;\n\tctx->dq = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_dq,\n\t\t\t\t     GFP_KERNEL);\n\tif (!ctx->dq)\n\t\tgoto free_dp;\n\tmemcpy(ctx->dq + (half_key_sz - len), ptr, len);\n\n\t \n\tptr = rsa_key->qinv;\n\tlen = rsa_key->qinv_sz;\n\tqat_rsa_drop_leading_zeros(&ptr, &len);\n\tif (!len)\n\t\tgoto free_dq;\n\tctx->qinv = dma_alloc_coherent(dev, half_key_sz, &ctx->dma_qinv,\n\t\t\t\t       GFP_KERNEL);\n\tif (!ctx->qinv)\n\t\tgoto free_dq;\n\tmemcpy(ctx->qinv + (half_key_sz - len), ptr, len);\n\n\tctx->crt_mode = true;\n\treturn;\n\nfree_dq:\n\tmemset(ctx->dq, '\\0', half_key_sz);\n\tdma_free_coherent(dev, half_key_sz, ctx->dq, ctx->dma_dq);\n\tctx->dq = NULL;\nfree_dp:\n\tmemset(ctx->dp, '\\0', half_key_sz);\n\tdma_free_coherent(dev, half_key_sz, ctx->dp, ctx->dma_dp);\n\tctx->dp = NULL;\nfree_q:\n\tmemset(ctx->q, '\\0', half_key_sz);\n\tdma_free_coherent(dev, half_key_sz, ctx->q, ctx->dma_q);\n\tctx->q = NULL;\nfree_p:\n\tmemset(ctx->p, '\\0', half_key_sz);\n\tdma_free_coherent(dev, half_key_sz, ctx->p, ctx->dma_p);\n\tctx->p = NULL;\nerr:\n\tctx->crt_mode = false;\n}\n\nstatic void qat_rsa_clear_ctx(struct device *dev, struct qat_rsa_ctx *ctx)\n{\n\tunsigned int half_key_sz = ctx->key_sz / 2;\n\n\t \n\tif (ctx->n)\n\t\tdma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\n\tif (ctx->e)\n\t\tdma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\n\tif (ctx->d) {\n\t\tmemset(ctx->d, '\\0', ctx->key_sz);\n\t\tdma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\n\t}\n\tif (ctx->p) {\n\t\tmemset(ctx->p, '\\0', half_key_sz);\n\t\tdma_free_coherent(dev, half_key_sz, ctx->p, ctx->dma_p);\n\t}\n\tif (ctx->q) {\n\t\tmemset(ctx->q, '\\0', half_key_sz);\n\t\tdma_free_coherent(dev, half_key_sz, ctx->q, ctx->dma_q);\n\t}\n\tif (ctx->dp) {\n\t\tmemset(ctx->dp, '\\0', half_key_sz);\n\t\tdma_free_coherent(dev, half_key_sz, ctx->dp, ctx->dma_dp);\n\t}\n\tif (ctx->dq) {\n\t\tmemset(ctx->dq, '\\0', half_key_sz);\n\t\tdma_free_coherent(dev, half_key_sz, ctx->dq, ctx->dma_dq);\n\t}\n\tif (ctx->qinv) {\n\t\tmemset(ctx->qinv, '\\0', half_key_sz);\n\t\tdma_free_coherent(dev, half_key_sz, ctx->qinv, ctx->dma_qinv);\n\t}\n\n\tctx->n = NULL;\n\tctx->e = NULL;\n\tctx->d = NULL;\n\tctx->p = NULL;\n\tctx->q = NULL;\n\tctx->dp = NULL;\n\tctx->dq = NULL;\n\tctx->qinv = NULL;\n\tctx->crt_mode = false;\n\tctx->key_sz = 0;\n}\n\nstatic int qat_rsa_setkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t  unsigned int keylen, bool private)\n{\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\n\tstruct rsa_key rsa_key;\n\tint ret;\n\n\tqat_rsa_clear_ctx(dev, ctx);\n\n\tif (private)\n\t\tret = rsa_parse_priv_key(&rsa_key, key, keylen);\n\telse\n\t\tret = rsa_parse_pub_key(&rsa_key, key, keylen);\n\tif (ret < 0)\n\t\tgoto free;\n\n\tret = qat_rsa_set_n(ctx, rsa_key.n, rsa_key.n_sz);\n\tif (ret < 0)\n\t\tgoto free;\n\tret = qat_rsa_set_e(ctx, rsa_key.e, rsa_key.e_sz);\n\tif (ret < 0)\n\t\tgoto free;\n\tif (private) {\n\t\tret = qat_rsa_set_d(ctx, rsa_key.d, rsa_key.d_sz);\n\t\tif (ret < 0)\n\t\t\tgoto free;\n\t\tqat_rsa_setkey_crt(ctx, &rsa_key);\n\t}\n\n\tif (!ctx->n || !ctx->e) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\tif (private && !ctx->d) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\n\treturn 0;\nfree:\n\tqat_rsa_clear_ctx(dev, ctx);\n\treturn ret;\n}\n\nstatic int qat_rsa_setpubkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t     unsigned int keylen)\n{\n\treturn qat_rsa_setkey(tfm, key, keylen, false);\n}\n\nstatic int qat_rsa_setprivkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t      unsigned int keylen)\n{\n\treturn qat_rsa_setkey(tfm, key, keylen, true);\n}\n\nstatic unsigned int qat_rsa_max_size(struct crypto_akcipher *tfm)\n{\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\n\treturn ctx->key_sz;\n}\n\nstatic int qat_rsa_init_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct qat_crypto_instance *inst =\n\t\t\tqat_crypto_get_instance_node(numa_node_id());\n\n\tif (!inst)\n\t\treturn -EINVAL;\n\n\takcipher_set_reqsize(tfm, sizeof(struct qat_asym_request) + 64);\n\n\tctx->key_sz = 0;\n\tctx->inst = inst;\n\treturn 0;\n}\n\nstatic void qat_rsa_exit_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\n\n\tqat_rsa_clear_ctx(dev, ctx);\n\tqat_crypto_put_instance(ctx->inst);\n}\n\nstatic struct akcipher_alg rsa = {\n\t.encrypt = qat_rsa_enc,\n\t.decrypt = qat_rsa_dec,\n\t.set_pub_key = qat_rsa_setpubkey,\n\t.set_priv_key = qat_rsa_setprivkey,\n\t.max_size = qat_rsa_max_size,\n\t.init = qat_rsa_init_tfm,\n\t.exit = qat_rsa_exit_tfm,\n\t.base = {\n\t\t.cra_name = \"rsa\",\n\t\t.cra_driver_name = \"qat-rsa\",\n\t\t.cra_priority = 1000,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_ctxsize = sizeof(struct qat_rsa_ctx),\n\t},\n};\n\nstatic struct kpp_alg dh = {\n\t.set_secret = qat_dh_set_secret,\n\t.generate_public_key = qat_dh_compute_value,\n\t.compute_shared_secret = qat_dh_compute_value,\n\t.max_size = qat_dh_max_size,\n\t.init = qat_dh_init_tfm,\n\t.exit = qat_dh_exit_tfm,\n\t.base = {\n\t\t.cra_name = \"dh\",\n\t\t.cra_driver_name = \"qat-dh\",\n\t\t.cra_priority = 1000,\n\t\t.cra_module = THIS_MODULE,\n\t\t.cra_ctxsize = sizeof(struct qat_dh_ctx),\n\t},\n};\n\nint qat_asym_algs_register(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&algs_lock);\n\tif (++active_devs == 1) {\n\t\trsa.base.cra_flags = 0;\n\t\tret = crypto_register_akcipher(&rsa);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t\tret = crypto_register_kpp(&dh);\n\t}\nunlock:\n\tmutex_unlock(&algs_lock);\n\treturn ret;\n}\n\nvoid qat_asym_algs_unregister(void)\n{\n\tmutex_lock(&algs_lock);\n\tif (--active_devs == 0) {\n\t\tcrypto_unregister_akcipher(&rsa);\n\t\tcrypto_unregister_kpp(&dh);\n\t}\n\tmutex_unlock(&algs_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}