{
  "module_name": "qat_algs.c",
  "hash_id": "295c066c4fef0be047dc388bbd0467e4f24001d3c55d90989c76f26a17d820ed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/intel/qat/qat_common/qat_algs.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/crypto.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/cipher.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/aes.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/hash.h>\n#include <crypto/hmac.h>\n#include <crypto/algapi.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/xts.h>\n#include <linux/dma-mapping.h>\n#include \"adf_accel_devices.h\"\n#include \"qat_algs_send.h\"\n#include \"adf_common_drv.h\"\n#include \"qat_crypto.h\"\n#include \"icp_qat_hw.h\"\n#include \"icp_qat_fw.h\"\n#include \"icp_qat_fw_la.h\"\n#include \"qat_bl.h\"\n\n#define QAT_AES_HW_CONFIG_ENC(alg, mode) \\\n\tICP_QAT_HW_CIPHER_CONFIG_BUILD(mode, alg, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_NO_CONVERT, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_ENCRYPT)\n\n#define QAT_AES_HW_CONFIG_DEC(alg, mode) \\\n\tICP_QAT_HW_CIPHER_CONFIG_BUILD(mode, alg, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_KEY_CONVERT, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_DECRYPT)\n\n#define QAT_AES_HW_CONFIG_DEC_NO_CONV(alg, mode) \\\n\tICP_QAT_HW_CIPHER_CONFIG_BUILD(mode, alg, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_NO_CONVERT, \\\n\t\t\t\t       ICP_QAT_HW_CIPHER_DECRYPT)\n\n#define HW_CAP_AES_V2(accel_dev) \\\n\t(GET_HW_DATA(accel_dev)->accel_capabilities_mask & \\\n\t ICP_ACCEL_CAPABILITIES_AES_V2)\n\nstatic DEFINE_MUTEX(algs_lock);\nstatic unsigned int active_devs;\n\n \nstruct qat_alg_cd {\n\tunion {\n\t\tstruct qat_enc {  \n\t\t\tstruct icp_qat_hw_cipher_algo_blk cipher;\n\t\t\tstruct icp_qat_hw_auth_algo_blk hash;\n\t\t} qat_enc_cd;\n\t\tstruct qat_dec {  \n\t\t\tstruct icp_qat_hw_auth_algo_blk hash;\n\t\t\tstruct icp_qat_hw_cipher_algo_blk cipher;\n\t\t} qat_dec_cd;\n\t};\n} __aligned(64);\n\nstruct qat_alg_aead_ctx {\n\tstruct qat_alg_cd *enc_cd;\n\tstruct qat_alg_cd *dec_cd;\n\tdma_addr_t enc_cd_paddr;\n\tdma_addr_t dec_cd_paddr;\n\tstruct icp_qat_fw_la_bulk_req enc_fw_req;\n\tstruct icp_qat_fw_la_bulk_req dec_fw_req;\n\tstruct crypto_shash *hash_tfm;\n\tenum icp_qat_hw_auth_algo qat_hash_alg;\n\tstruct qat_crypto_instance *inst;\n\tunion {\n\t\tstruct sha1_state sha1;\n\t\tstruct sha256_state sha256;\n\t\tstruct sha512_state sha512;\n\t};\n\tchar ipad[SHA512_BLOCK_SIZE];  \n\tchar opad[SHA512_BLOCK_SIZE];\n};\n\nstruct qat_alg_skcipher_ctx {\n\tstruct icp_qat_hw_cipher_algo_blk *enc_cd;\n\tstruct icp_qat_hw_cipher_algo_blk *dec_cd;\n\tdma_addr_t enc_cd_paddr;\n\tdma_addr_t dec_cd_paddr;\n\tstruct icp_qat_fw_la_bulk_req enc_fw_req;\n\tstruct icp_qat_fw_la_bulk_req dec_fw_req;\n\tstruct qat_crypto_instance *inst;\n\tstruct crypto_skcipher *ftfm;\n\tstruct crypto_cipher *tweak;\n\tbool fallback;\n\tint mode;\n};\n\nstatic int qat_get_inter_state_size(enum icp_qat_hw_auth_algo qat_hash_alg)\n{\n\tswitch (qat_hash_alg) {\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA1:\n\t\treturn ICP_QAT_HW_SHA1_STATE1_SZ;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA256:\n\t\treturn ICP_QAT_HW_SHA256_STATE1_SZ;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA512:\n\t\treturn ICP_QAT_HW_SHA512_STATE1_SZ;\n\tdefault:\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic int qat_alg_do_precomputes(struct icp_qat_hw_auth_algo_blk *hash,\n\t\t\t\t  struct qat_alg_aead_ctx *ctx,\n\t\t\t\t  const u8 *auth_key,\n\t\t\t\t  unsigned int auth_keylen)\n{\n\tSHASH_DESC_ON_STACK(shash, ctx->hash_tfm);\n\tint block_size = crypto_shash_blocksize(ctx->hash_tfm);\n\tint digest_size = crypto_shash_digestsize(ctx->hash_tfm);\n\t__be32 *hash_state_out;\n\t__be64 *hash512_state_out;\n\tint i, offset;\n\n\tmemset(ctx->ipad, 0, block_size);\n\tmemset(ctx->opad, 0, block_size);\n\tshash->tfm = ctx->hash_tfm;\n\n\tif (auth_keylen > block_size) {\n\t\tint ret = crypto_shash_digest(shash, auth_key,\n\t\t\t\t\t      auth_keylen, ctx->ipad);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmemcpy(ctx->opad, ctx->ipad, digest_size);\n\t} else {\n\t\tmemcpy(ctx->ipad, auth_key, auth_keylen);\n\t\tmemcpy(ctx->opad, auth_key, auth_keylen);\n\t}\n\n\tfor (i = 0; i < block_size; i++) {\n\t\tchar *ipad_ptr = ctx->ipad + i;\n\t\tchar *opad_ptr = ctx->opad + i;\n\t\t*ipad_ptr ^= HMAC_IPAD_VALUE;\n\t\t*opad_ptr ^= HMAC_OPAD_VALUE;\n\t}\n\n\tif (crypto_shash_init(shash))\n\t\treturn -EFAULT;\n\n\tif (crypto_shash_update(shash, ctx->ipad, block_size))\n\t\treturn -EFAULT;\n\n\thash_state_out = (__be32 *)hash->sha.state1;\n\thash512_state_out = (__be64 *)hash_state_out;\n\n\tswitch (ctx->qat_hash_alg) {\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA1:\n\t\tif (crypto_shash_export(shash, &ctx->sha1))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\n\t\t\t*hash_state_out = cpu_to_be32(ctx->sha1.state[i]);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA256:\n\t\tif (crypto_shash_export(shash, &ctx->sha256))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\n\t\t\t*hash_state_out = cpu_to_be32(ctx->sha256.state[i]);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA512:\n\t\tif (crypto_shash_export(shash, &ctx->sha512))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 3; i++, hash512_state_out++)\n\t\t\t*hash512_state_out = cpu_to_be64(ctx->sha512.state[i]);\n\t\tbreak;\n\tdefault:\n\t\treturn -EFAULT;\n\t}\n\n\tif (crypto_shash_init(shash))\n\t\treturn -EFAULT;\n\n\tif (crypto_shash_update(shash, ctx->opad, block_size))\n\t\treturn -EFAULT;\n\n\toffset = round_up(qat_get_inter_state_size(ctx->qat_hash_alg), 8);\n\tif (offset < 0)\n\t\treturn -EFAULT;\n\n\thash_state_out = (__be32 *)(hash->sha.state1 + offset);\n\thash512_state_out = (__be64 *)hash_state_out;\n\n\tswitch (ctx->qat_hash_alg) {\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA1:\n\t\tif (crypto_shash_export(shash, &ctx->sha1))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\n\t\t\t*hash_state_out = cpu_to_be32(ctx->sha1.state[i]);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA256:\n\t\tif (crypto_shash_export(shash, &ctx->sha256))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\n\t\t\t*hash_state_out = cpu_to_be32(ctx->sha256.state[i]);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA512:\n\t\tif (crypto_shash_export(shash, &ctx->sha512))\n\t\t\treturn -EFAULT;\n\t\tfor (i = 0; i < digest_size >> 3; i++, hash512_state_out++)\n\t\t\t*hash512_state_out = cpu_to_be64(ctx->sha512.state[i]);\n\t\tbreak;\n\tdefault:\n\t\treturn -EFAULT;\n\t}\n\tmemzero_explicit(ctx->ipad, block_size);\n\tmemzero_explicit(ctx->opad, block_size);\n\treturn 0;\n}\n\nstatic void qat_alg_init_common_hdr(struct icp_qat_fw_comn_req_hdr *header)\n{\n\theader->hdr_flags =\n\t\tICP_QAT_FW_COMN_HDR_FLAGS_BUILD(ICP_QAT_FW_COMN_REQ_FLAG_SET);\n\theader->service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_LA;\n\theader->comn_req_flags =\n\t\tICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_CD_FLD_TYPE_64BIT_ADR,\n\t\t\t\t\t    QAT_COMN_PTR_TYPE_SGL);\n\tICP_QAT_FW_LA_PARTIAL_SET(header->serv_specif_flags,\n\t\t\t\t  ICP_QAT_FW_LA_PARTIAL_NONE);\n\tICP_QAT_FW_LA_CIPH_IV_FLD_FLAG_SET(header->serv_specif_flags,\n\t\t\t\t\t   ICP_QAT_FW_CIPH_IV_16BYTE_DATA);\n\tICP_QAT_FW_LA_PROTO_SET(header->serv_specif_flags,\n\t\t\t\tICP_QAT_FW_LA_NO_PROTO);\n\tICP_QAT_FW_LA_UPDATE_STATE_SET(header->serv_specif_flags,\n\t\t\t\t       ICP_QAT_FW_LA_NO_UPDATE_STATE);\n}\n\nstatic int qat_alg_aead_init_enc_session(struct crypto_aead *aead_tfm,\n\t\t\t\t\t int alg,\n\t\t\t\t\t struct crypto_authenc_keys *keys,\n\t\t\t\t\t int mode)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(aead_tfm);\n\tunsigned int digestsize = crypto_aead_authsize(aead_tfm);\n\tstruct qat_enc *enc_ctx = &ctx->enc_cd->qat_enc_cd;\n\tstruct icp_qat_hw_cipher_algo_blk *cipher = &enc_ctx->cipher;\n\tstruct icp_qat_hw_auth_algo_blk *hash =\n\t\t(struct icp_qat_hw_auth_algo_blk *)((char *)enc_ctx +\n\t\tsizeof(struct icp_qat_hw_auth_setup) + keys->enckeylen);\n\tstruct icp_qat_fw_la_bulk_req *req_tmpl = &ctx->enc_fw_req;\n\tstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req_tmpl->cd_pars;\n\tstruct icp_qat_fw_comn_req_hdr *header = &req_tmpl->comn_hdr;\n\tvoid *ptr = &req_tmpl->cd_ctrl;\n\tstruct icp_qat_fw_cipher_cd_ctrl_hdr *cipher_cd_ctrl = ptr;\n\tstruct icp_qat_fw_auth_cd_ctrl_hdr *hash_cd_ctrl = ptr;\n\n\t \n\tcipher->aes.cipher_config.val = QAT_AES_HW_CONFIG_ENC(alg, mode);\n\tmemcpy(cipher->aes.key, keys->enckey, keys->enckeylen);\n\thash->sha.inner_setup.auth_config.config =\n\t\tICP_QAT_HW_AUTH_CONFIG_BUILD(ICP_QAT_HW_AUTH_MODE1,\n\t\t\t\t\t     ctx->qat_hash_alg, digestsize);\n\thash->sha.inner_setup.auth_counter.counter =\n\t\tcpu_to_be32(crypto_shash_blocksize(ctx->hash_tfm));\n\n\tif (qat_alg_do_precomputes(hash, ctx, keys->authkey, keys->authkeylen))\n\t\treturn -EFAULT;\n\n\t \n\tqat_alg_init_common_hdr(header);\n\theader->service_cmd_id = ICP_QAT_FW_LA_CMD_CIPHER_HASH;\n\tICP_QAT_FW_LA_DIGEST_IN_BUFFER_SET(header->serv_specif_flags,\n\t\t\t\t\t   ICP_QAT_FW_LA_DIGEST_IN_BUFFER);\n\tICP_QAT_FW_LA_RET_AUTH_SET(header->serv_specif_flags,\n\t\t\t\t   ICP_QAT_FW_LA_RET_AUTH_RES);\n\tICP_QAT_FW_LA_CMP_AUTH_SET(header->serv_specif_flags,\n\t\t\t\t   ICP_QAT_FW_LA_NO_CMP_AUTH_RES);\n\tcd_pars->u.s.content_desc_addr = ctx->enc_cd_paddr;\n\tcd_pars->u.s.content_desc_params_sz = sizeof(struct qat_alg_cd) >> 3;\n\n\t \n\tcipher_cd_ctrl->cipher_key_sz = keys->enckeylen >> 3;\n\tcipher_cd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\n\tcipher_cd_ctrl->cipher_cfg_offset = 0;\n\tICP_QAT_FW_COMN_CURR_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\n\tICP_QAT_FW_COMN_NEXT_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\n\t \n\thash_cd_ctrl->hash_cfg_offset = ((char *)hash - (char *)cipher) >> 3;\n\thash_cd_ctrl->hash_flags = ICP_QAT_FW_AUTH_HDR_FLAG_NO_NESTED;\n\thash_cd_ctrl->inner_res_sz = digestsize;\n\thash_cd_ctrl->final_sz = digestsize;\n\n\tswitch (ctx->qat_hash_alg) {\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA1:\n\t\thash_cd_ctrl->inner_state1_sz =\n\t\t\tround_up(ICP_QAT_HW_SHA1_STATE1_SZ, 8);\n\t\thash_cd_ctrl->inner_state2_sz =\n\t\t\tround_up(ICP_QAT_HW_SHA1_STATE2_SZ, 8);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA256:\n\t\thash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA256_STATE1_SZ;\n\t\thash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA256_STATE2_SZ;\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA512:\n\t\thash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA512_STATE1_SZ;\n\t\thash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA512_STATE2_SZ;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\thash_cd_ctrl->inner_state2_offset = hash_cd_ctrl->hash_cfg_offset +\n\t\t\t((sizeof(struct icp_qat_hw_auth_setup) +\n\t\t\t round_up(hash_cd_ctrl->inner_state1_sz, 8)) >> 3);\n\tICP_QAT_FW_COMN_CURR_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\n\tICP_QAT_FW_COMN_NEXT_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\n\treturn 0;\n}\n\nstatic int qat_alg_aead_init_dec_session(struct crypto_aead *aead_tfm,\n\t\t\t\t\t int alg,\n\t\t\t\t\t struct crypto_authenc_keys *keys,\n\t\t\t\t\t int mode)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(aead_tfm);\n\tunsigned int digestsize = crypto_aead_authsize(aead_tfm);\n\tstruct qat_dec *dec_ctx = &ctx->dec_cd->qat_dec_cd;\n\tstruct icp_qat_hw_auth_algo_blk *hash = &dec_ctx->hash;\n\tstruct icp_qat_hw_cipher_algo_blk *cipher =\n\t\t(struct icp_qat_hw_cipher_algo_blk *)((char *)dec_ctx +\n\t\tsizeof(struct icp_qat_hw_auth_setup) +\n\t\troundup(crypto_shash_digestsize(ctx->hash_tfm), 8) * 2);\n\tstruct icp_qat_fw_la_bulk_req *req_tmpl = &ctx->dec_fw_req;\n\tstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req_tmpl->cd_pars;\n\tstruct icp_qat_fw_comn_req_hdr *header = &req_tmpl->comn_hdr;\n\tvoid *ptr = &req_tmpl->cd_ctrl;\n\tstruct icp_qat_fw_cipher_cd_ctrl_hdr *cipher_cd_ctrl = ptr;\n\tstruct icp_qat_fw_auth_cd_ctrl_hdr *hash_cd_ctrl = ptr;\n\tstruct icp_qat_fw_la_auth_req_params *auth_param =\n\t\t(struct icp_qat_fw_la_auth_req_params *)\n\t\t((char *)&req_tmpl->serv_specif_rqpars +\n\t\tsizeof(struct icp_qat_fw_la_cipher_req_params));\n\n\t \n\tcipher->aes.cipher_config.val = QAT_AES_HW_CONFIG_DEC(alg, mode);\n\tmemcpy(cipher->aes.key, keys->enckey, keys->enckeylen);\n\thash->sha.inner_setup.auth_config.config =\n\t\tICP_QAT_HW_AUTH_CONFIG_BUILD(ICP_QAT_HW_AUTH_MODE1,\n\t\t\t\t\t     ctx->qat_hash_alg,\n\t\t\t\t\t     digestsize);\n\thash->sha.inner_setup.auth_counter.counter =\n\t\tcpu_to_be32(crypto_shash_blocksize(ctx->hash_tfm));\n\n\tif (qat_alg_do_precomputes(hash, ctx, keys->authkey, keys->authkeylen))\n\t\treturn -EFAULT;\n\n\t \n\tqat_alg_init_common_hdr(header);\n\theader->service_cmd_id = ICP_QAT_FW_LA_CMD_HASH_CIPHER;\n\tICP_QAT_FW_LA_DIGEST_IN_BUFFER_SET(header->serv_specif_flags,\n\t\t\t\t\t   ICP_QAT_FW_LA_DIGEST_IN_BUFFER);\n\tICP_QAT_FW_LA_RET_AUTH_SET(header->serv_specif_flags,\n\t\t\t\t   ICP_QAT_FW_LA_NO_RET_AUTH_RES);\n\tICP_QAT_FW_LA_CMP_AUTH_SET(header->serv_specif_flags,\n\t\t\t\t   ICP_QAT_FW_LA_CMP_AUTH_RES);\n\tcd_pars->u.s.content_desc_addr = ctx->dec_cd_paddr;\n\tcd_pars->u.s.content_desc_params_sz = sizeof(struct qat_alg_cd) >> 3;\n\n\t \n\tcipher_cd_ctrl->cipher_key_sz = keys->enckeylen >> 3;\n\tcipher_cd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\n\tcipher_cd_ctrl->cipher_cfg_offset =\n\t\t(sizeof(struct icp_qat_hw_auth_setup) +\n\t\t roundup(crypto_shash_digestsize(ctx->hash_tfm), 8) * 2) >> 3;\n\tICP_QAT_FW_COMN_CURR_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\n\tICP_QAT_FW_COMN_NEXT_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\n\n\t \n\thash_cd_ctrl->hash_cfg_offset = 0;\n\thash_cd_ctrl->hash_flags = ICP_QAT_FW_AUTH_HDR_FLAG_NO_NESTED;\n\thash_cd_ctrl->inner_res_sz = digestsize;\n\thash_cd_ctrl->final_sz = digestsize;\n\n\tswitch (ctx->qat_hash_alg) {\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA1:\n\t\thash_cd_ctrl->inner_state1_sz =\n\t\t\tround_up(ICP_QAT_HW_SHA1_STATE1_SZ, 8);\n\t\thash_cd_ctrl->inner_state2_sz =\n\t\t\tround_up(ICP_QAT_HW_SHA1_STATE2_SZ, 8);\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA256:\n\t\thash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA256_STATE1_SZ;\n\t\thash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA256_STATE2_SZ;\n\t\tbreak;\n\tcase ICP_QAT_HW_AUTH_ALGO_SHA512:\n\t\thash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA512_STATE1_SZ;\n\t\thash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA512_STATE2_SZ;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\thash_cd_ctrl->inner_state2_offset = hash_cd_ctrl->hash_cfg_offset +\n\t\t\t((sizeof(struct icp_qat_hw_auth_setup) +\n\t\t\t round_up(hash_cd_ctrl->inner_state1_sz, 8)) >> 3);\n\tauth_param->auth_res_sz = digestsize;\n\tICP_QAT_FW_COMN_CURR_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\n\tICP_QAT_FW_COMN_NEXT_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\n\treturn 0;\n}\n\nstatic void qat_alg_skcipher_init_com(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t      struct icp_qat_fw_la_bulk_req *req,\n\t\t\t\t      struct icp_qat_hw_cipher_algo_blk *cd,\n\t\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\n\tstruct icp_qat_fw_comn_req_hdr *header = &req->comn_hdr;\n\tstruct icp_qat_fw_cipher_cd_ctrl_hdr *cd_ctrl = (void *)&req->cd_ctrl;\n\tbool aes_v2_capable = HW_CAP_AES_V2(ctx->inst->accel_dev);\n\tint mode = ctx->mode;\n\n\tqat_alg_init_common_hdr(header);\n\theader->service_cmd_id = ICP_QAT_FW_LA_CMD_CIPHER;\n\tcd_pars->u.s.content_desc_params_sz =\n\t\t\t\tsizeof(struct icp_qat_hw_cipher_algo_blk) >> 3;\n\n\tif (aes_v2_capable && mode == ICP_QAT_HW_CIPHER_XTS_MODE) {\n\t\tICP_QAT_FW_LA_SLICE_TYPE_SET(header->serv_specif_flags,\n\t\t\t\t\t     ICP_QAT_FW_LA_USE_UCS_SLICE_TYPE);\n\n\t\t \n\t\tmemcpy(cd->ucs_aes.key, key, keylen);\n\t\tkeylen = keylen / 2;\n\t} else if (aes_v2_capable && mode == ICP_QAT_HW_CIPHER_CTR_MODE) {\n\t\tICP_QAT_FW_LA_SLICE_TYPE_SET(header->serv_specif_flags,\n\t\t\t\t\t     ICP_QAT_FW_LA_USE_UCS_SLICE_TYPE);\n\t\tmemcpy(cd->ucs_aes.key, key, keylen);\n\t\tkeylen = round_up(keylen, 16);\n\t} else {\n\t\tmemcpy(cd->aes.key, key, keylen);\n\t}\n\n\t \n\tcd_ctrl->cipher_key_sz = keylen >> 3;\n\tcd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\n\tcd_ctrl->cipher_cfg_offset = 0;\n\tICP_QAT_FW_COMN_CURR_ID_SET(cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\n\tICP_QAT_FW_COMN_NEXT_ID_SET(cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\n}\n\nstatic void qat_alg_skcipher_init_enc(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t      int alg, const u8 *key,\n\t\t\t\t      unsigned int keylen, int mode)\n{\n\tstruct icp_qat_hw_cipher_algo_blk *enc_cd = ctx->enc_cd;\n\tstruct icp_qat_fw_la_bulk_req *req = &ctx->enc_fw_req;\n\tstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\n\n\tqat_alg_skcipher_init_com(ctx, req, enc_cd, key, keylen);\n\tcd_pars->u.s.content_desc_addr = ctx->enc_cd_paddr;\n\tenc_cd->aes.cipher_config.val = QAT_AES_HW_CONFIG_ENC(alg, mode);\n}\n\nstatic void qat_alg_xts_reverse_key(const u8 *key_forward, unsigned int keylen,\n\t\t\t\t    u8 *key_reverse)\n{\n\tstruct crypto_aes_ctx aes_expanded;\n\tint nrounds;\n\tu8 *key;\n\n\taes_expandkey(&aes_expanded, key_forward, keylen);\n\tif (keylen == AES_KEYSIZE_128) {\n\t\tnrounds = 10;\n\t\tkey = (u8 *)aes_expanded.key_enc + (AES_BLOCK_SIZE * nrounds);\n\t\tmemcpy(key_reverse, key, AES_BLOCK_SIZE);\n\t} else {\n\t\t \n\t\tnrounds = 14;\n\t\tkey = (u8 *)aes_expanded.key_enc + (AES_BLOCK_SIZE * nrounds);\n\t\tmemcpy(key_reverse, key, AES_BLOCK_SIZE);\n\t\tmemcpy(key_reverse + AES_BLOCK_SIZE, key - AES_BLOCK_SIZE,\n\t\t       AES_BLOCK_SIZE);\n\t}\n}\n\nstatic void qat_alg_skcipher_init_dec(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t      int alg, const u8 *key,\n\t\t\t\t      unsigned int keylen, int mode)\n{\n\tstruct icp_qat_hw_cipher_algo_blk *dec_cd = ctx->dec_cd;\n\tstruct icp_qat_fw_la_bulk_req *req = &ctx->dec_fw_req;\n\tstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\n\tbool aes_v2_capable = HW_CAP_AES_V2(ctx->inst->accel_dev);\n\n\tqat_alg_skcipher_init_com(ctx, req, dec_cd, key, keylen);\n\tcd_pars->u.s.content_desc_addr = ctx->dec_cd_paddr;\n\n\tif (aes_v2_capable && mode == ICP_QAT_HW_CIPHER_XTS_MODE) {\n\t\t \n\t\tdec_cd->aes.cipher_config.val =\n\t\t\t\tQAT_AES_HW_CONFIG_DEC_NO_CONV(alg, mode);\n\n\t\t \n\t\tqat_alg_xts_reverse_key(dec_cd->ucs_aes.key, keylen / 2,\n\t\t\t\t\tdec_cd->ucs_aes.key);\n\t} else if (mode != ICP_QAT_HW_CIPHER_CTR_MODE) {\n\t\tdec_cd->aes.cipher_config.val =\n\t\t\t\t\tQAT_AES_HW_CONFIG_DEC(alg, mode);\n\t} else {\n\t\tdec_cd->aes.cipher_config.val =\n\t\t\t\t\tQAT_AES_HW_CONFIG_ENC(alg, mode);\n\t}\n}\n\nstatic int qat_alg_validate_key(int key_len, int *alg, int mode)\n{\n\tif (mode != ICP_QAT_HW_CIPHER_XTS_MODE) {\n\t\tswitch (key_len) {\n\t\tcase AES_KEYSIZE_128:\n\t\t\t*alg = ICP_QAT_HW_CIPHER_ALGO_AES128;\n\t\t\tbreak;\n\t\tcase AES_KEYSIZE_192:\n\t\t\t*alg = ICP_QAT_HW_CIPHER_ALGO_AES192;\n\t\t\tbreak;\n\t\tcase AES_KEYSIZE_256:\n\t\t\t*alg = ICP_QAT_HW_CIPHER_ALGO_AES256;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tswitch (key_len) {\n\t\tcase AES_KEYSIZE_128 << 1:\n\t\t\t*alg = ICP_QAT_HW_CIPHER_ALGO_AES128;\n\t\t\tbreak;\n\t\tcase AES_KEYSIZE_256 << 1:\n\t\t\t*alg = ICP_QAT_HW_CIPHER_ALGO_AES256;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int qat_alg_aead_init_sessions(struct crypto_aead *tfm, const u8 *key,\n\t\t\t\t      unsigned int keylen,  int mode)\n{\n\tstruct crypto_authenc_keys keys;\n\tint alg;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen))\n\t\tgoto bad_key;\n\n\tif (qat_alg_validate_key(keys.enckeylen, &alg, mode))\n\t\tgoto bad_key;\n\n\tif (qat_alg_aead_init_enc_session(tfm, alg, &keys, mode))\n\t\tgoto error;\n\n\tif (qat_alg_aead_init_dec_session(tfm, alg, &keys, mode))\n\t\tgoto error;\n\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn 0;\nbad_key:\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn -EINVAL;\nerror:\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn -EFAULT;\n}\n\nstatic int qat_alg_skcipher_init_sessions(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t\t  const u8 *key,\n\t\t\t\t\t  unsigned int keylen,\n\t\t\t\t\t  int mode)\n{\n\tint alg;\n\n\tif (qat_alg_validate_key(keylen, &alg, mode))\n\t\treturn -EINVAL;\n\n\tqat_alg_skcipher_init_enc(ctx, alg, key, keylen, mode);\n\tqat_alg_skcipher_init_dec(ctx, alg, key, keylen, mode);\n\treturn 0;\n}\n\nstatic int qat_alg_aead_rekey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t      unsigned int keylen)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\n\tmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\n\tmemset(&ctx->enc_fw_req, 0, sizeof(ctx->enc_fw_req));\n\tmemset(&ctx->dec_fw_req, 0, sizeof(ctx->dec_fw_req));\n\n\treturn qat_alg_aead_init_sessions(tfm, key, keylen,\n\t\t\t\t\t  ICP_QAT_HW_CIPHER_CBC_MODE);\n}\n\nstatic int qat_alg_aead_newkey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct qat_crypto_instance *inst = NULL;\n\tint node = numa_node_id();\n\tstruct device *dev;\n\tint ret;\n\n\tinst = qat_crypto_get_instance_node(node);\n\tif (!inst)\n\t\treturn -EINVAL;\n\tdev = &GET_DEV(inst->accel_dev);\n\tctx->inst = inst;\n\tctx->enc_cd = dma_alloc_coherent(dev, sizeof(*ctx->enc_cd),\n\t\t\t\t\t &ctx->enc_cd_paddr,\n\t\t\t\t\t GFP_ATOMIC);\n\tif (!ctx->enc_cd) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_inst;\n\t}\n\tctx->dec_cd = dma_alloc_coherent(dev, sizeof(*ctx->dec_cd),\n\t\t\t\t\t &ctx->dec_cd_paddr,\n\t\t\t\t\t GFP_ATOMIC);\n\tif (!ctx->dec_cd) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_enc;\n\t}\n\n\tret = qat_alg_aead_init_sessions(tfm, key, keylen,\n\t\t\t\t\t ICP_QAT_HW_CIPHER_CBC_MODE);\n\tif (ret)\n\t\tgoto out_free_all;\n\n\treturn 0;\n\nout_free_all:\n\tmemset(ctx->dec_cd, 0, sizeof(struct qat_alg_cd));\n\tdma_free_coherent(dev, sizeof(struct qat_alg_cd),\n\t\t\t  ctx->dec_cd, ctx->dec_cd_paddr);\n\tctx->dec_cd = NULL;\nout_free_enc:\n\tmemset(ctx->enc_cd, 0, sizeof(struct qat_alg_cd));\n\tdma_free_coherent(dev, sizeof(struct qat_alg_cd),\n\t\t\t  ctx->enc_cd, ctx->enc_cd_paddr);\n\tctx->enc_cd = NULL;\nout_free_inst:\n\tctx->inst = NULL;\n\tqat_crypto_put_instance(inst);\n\treturn ret;\n}\n\nstatic int qat_alg_aead_setkey(struct crypto_aead *tfm, const u8 *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tif (ctx->enc_cd)\n\t\treturn qat_alg_aead_rekey(tfm, key, keylen);\n\telse\n\t\treturn qat_alg_aead_newkey(tfm, key, keylen);\n}\n\nstatic void qat_aead_alg_callback(struct icp_qat_fw_la_resp *qat_resp,\n\t\t\t\t  struct qat_crypto_request *qat_req)\n{\n\tstruct qat_alg_aead_ctx *ctx = qat_req->aead_ctx;\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct aead_request *areq = qat_req->aead_req;\n\tu8 stat_filed = qat_resp->comn_resp.comn_status;\n\tint res = 0, qat_res = ICP_QAT_FW_COMN_RESP_CRYPTO_STAT_GET(stat_filed);\n\n\tqat_bl_free_bufl(inst->accel_dev, &qat_req->buf);\n\tif (unlikely(qat_res != ICP_QAT_FW_COMN_STATUS_FLAG_OK))\n\t\tres = -EBADMSG;\n\taead_request_complete(areq, res);\n}\n\nstatic void qat_alg_update_iv_ctr_mode(struct qat_crypto_request *qat_req)\n{\n\tstruct skcipher_request *sreq = qat_req->skcipher_req;\n\tu64 iv_lo_prev;\n\tu64 iv_lo;\n\tu64 iv_hi;\n\n\tmemcpy(qat_req->iv, sreq->iv, AES_BLOCK_SIZE);\n\n\tiv_lo = be64_to_cpu(qat_req->iv_lo);\n\tiv_hi = be64_to_cpu(qat_req->iv_hi);\n\n\tiv_lo_prev = iv_lo;\n\tiv_lo += DIV_ROUND_UP(sreq->cryptlen, AES_BLOCK_SIZE);\n\tif (iv_lo < iv_lo_prev)\n\t\tiv_hi++;\n\n\tqat_req->iv_lo = cpu_to_be64(iv_lo);\n\tqat_req->iv_hi = cpu_to_be64(iv_hi);\n}\n\nstatic void qat_alg_update_iv_cbc_mode(struct qat_crypto_request *qat_req)\n{\n\tstruct skcipher_request *sreq = qat_req->skcipher_req;\n\tint offset = sreq->cryptlen - AES_BLOCK_SIZE;\n\tstruct scatterlist *sgl;\n\n\tif (qat_req->encryption)\n\t\tsgl = sreq->dst;\n\telse\n\t\tsgl = sreq->src;\n\n\tscatterwalk_map_and_copy(qat_req->iv, sgl, offset, AES_BLOCK_SIZE, 0);\n}\n\nstatic void qat_alg_update_iv(struct qat_crypto_request *qat_req)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = qat_req->skcipher_ctx;\n\tstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\n\n\tswitch (ctx->mode) {\n\tcase ICP_QAT_HW_CIPHER_CTR_MODE:\n\t\tqat_alg_update_iv_ctr_mode(qat_req);\n\t\tbreak;\n\tcase ICP_QAT_HW_CIPHER_CBC_MODE:\n\t\tqat_alg_update_iv_cbc_mode(qat_req);\n\t\tbreak;\n\tcase ICP_QAT_HW_CIPHER_XTS_MODE:\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(dev, \"Unsupported IV update for cipher mode %d\\n\",\n\t\t\t ctx->mode);\n\t}\n}\n\nstatic void qat_skcipher_alg_callback(struct icp_qat_fw_la_resp *qat_resp,\n\t\t\t\t      struct qat_crypto_request *qat_req)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = qat_req->skcipher_ctx;\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct skcipher_request *sreq = qat_req->skcipher_req;\n\tu8 stat_filed = qat_resp->comn_resp.comn_status;\n\tint res = 0, qat_res = ICP_QAT_FW_COMN_RESP_CRYPTO_STAT_GET(stat_filed);\n\n\tqat_bl_free_bufl(inst->accel_dev, &qat_req->buf);\n\tif (unlikely(qat_res != ICP_QAT_FW_COMN_STATUS_FLAG_OK))\n\t\tres = -EINVAL;\n\n\tif (qat_req->encryption)\n\t\tqat_alg_update_iv(qat_req);\n\n\tmemcpy(sreq->iv, qat_req->iv, AES_BLOCK_SIZE);\n\n\tskcipher_request_complete(sreq, res);\n}\n\nvoid qat_alg_callback(void *resp)\n{\n\tstruct icp_qat_fw_la_resp *qat_resp = resp;\n\tstruct qat_crypto_request *qat_req =\n\t\t\t\t(void *)(__force long)qat_resp->opaque_data;\n\tstruct qat_instance_backlog *backlog = qat_req->alg_req.backlog;\n\n\tqat_req->cb(qat_resp, qat_req);\n\n\tqat_alg_send_backlog(backlog);\n}\n\nstatic int qat_alg_send_sym_message(struct qat_crypto_request *qat_req,\n\t\t\t\t    struct qat_crypto_instance *inst,\n\t\t\t\t    struct crypto_async_request *base)\n{\n\tstruct qat_alg_req *alg_req = &qat_req->alg_req;\n\n\talg_req->fw_req = (u32 *)&qat_req->req;\n\talg_req->tx_ring = inst->sym_tx;\n\talg_req->base = base;\n\talg_req->backlog = &inst->backlog;\n\n\treturn qat_alg_send_message(alg_req);\n}\n\nstatic int qat_alg_aead_dec(struct aead_request *areq)\n{\n\tstruct crypto_aead *aead_tfm = crypto_aead_reqtfm(areq);\n\tstruct crypto_tfm *tfm = crypto_aead_tfm(aead_tfm);\n\tstruct qat_alg_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct qat_crypto_request *qat_req = aead_request_ctx(areq);\n\tstruct icp_qat_fw_la_cipher_req_params *cipher_param;\n\tstruct icp_qat_fw_la_auth_req_params *auth_param;\n\tstruct icp_qat_fw_la_bulk_req *msg;\n\tint digst_size = crypto_aead_authsize(aead_tfm);\n\tgfp_t f = qat_algs_alloc_flags(&areq->base);\n\tint ret;\n\tu32 cipher_len;\n\n\tcipher_len = areq->cryptlen - digst_size;\n\tif (cipher_len % AES_BLOCK_SIZE != 0)\n\t\treturn -EINVAL;\n\n\tret = qat_bl_sgl_to_bufl(ctx->inst->accel_dev, areq->src, areq->dst,\n\t\t\t\t &qat_req->buf, NULL, f);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg = &qat_req->req;\n\t*msg = ctx->dec_fw_req;\n\tqat_req->aead_ctx = ctx;\n\tqat_req->aead_req = areq;\n\tqat_req->cb = qat_aead_alg_callback;\n\tqat_req->req.comn_mid.opaque_data = (u64)(__force long)qat_req;\n\tqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\n\tqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\n\tcipher_param = (void *)&qat_req->req.serv_specif_rqpars;\n\tcipher_param->cipher_length = cipher_len;\n\tcipher_param->cipher_offset = areq->assoclen;\n\tmemcpy(cipher_param->u.cipher_IV_array, areq->iv, AES_BLOCK_SIZE);\n\tauth_param = (void *)((u8 *)cipher_param + sizeof(*cipher_param));\n\tauth_param->auth_off = 0;\n\tauth_param->auth_len = areq->assoclen + cipher_param->cipher_length;\n\n\tret = qat_alg_send_sym_message(qat_req, ctx->inst, &areq->base);\n\tif (ret == -ENOSPC)\n\t\tqat_bl_free_bufl(ctx->inst->accel_dev, &qat_req->buf);\n\n\treturn ret;\n}\n\nstatic int qat_alg_aead_enc(struct aead_request *areq)\n{\n\tstruct crypto_aead *aead_tfm = crypto_aead_reqtfm(areq);\n\tstruct crypto_tfm *tfm = crypto_aead_tfm(aead_tfm);\n\tstruct qat_alg_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct qat_crypto_request *qat_req = aead_request_ctx(areq);\n\tstruct icp_qat_fw_la_cipher_req_params *cipher_param;\n\tstruct icp_qat_fw_la_auth_req_params *auth_param;\n\tgfp_t f = qat_algs_alloc_flags(&areq->base);\n\tstruct icp_qat_fw_la_bulk_req *msg;\n\tu8 *iv = areq->iv;\n\tint ret;\n\n\tif (areq->cryptlen % AES_BLOCK_SIZE != 0)\n\t\treturn -EINVAL;\n\n\tret = qat_bl_sgl_to_bufl(ctx->inst->accel_dev, areq->src, areq->dst,\n\t\t\t\t &qat_req->buf, NULL, f);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg = &qat_req->req;\n\t*msg = ctx->enc_fw_req;\n\tqat_req->aead_ctx = ctx;\n\tqat_req->aead_req = areq;\n\tqat_req->cb = qat_aead_alg_callback;\n\tqat_req->req.comn_mid.opaque_data = (u64)(__force long)qat_req;\n\tqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\n\tqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\n\tcipher_param = (void *)&qat_req->req.serv_specif_rqpars;\n\tauth_param = (void *)((u8 *)cipher_param + sizeof(*cipher_param));\n\n\tmemcpy(cipher_param->u.cipher_IV_array, iv, AES_BLOCK_SIZE);\n\tcipher_param->cipher_length = areq->cryptlen;\n\tcipher_param->cipher_offset = areq->assoclen;\n\n\tauth_param->auth_off = 0;\n\tauth_param->auth_len = areq->assoclen + areq->cryptlen;\n\n\tret = qat_alg_send_sym_message(qat_req, ctx->inst, &areq->base);\n\tif (ret == -ENOSPC)\n\t\tqat_bl_free_bufl(ctx->inst->accel_dev, &qat_req->buf);\n\n\treturn ret;\n}\n\nstatic int qat_alg_skcipher_rekey(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t  const u8 *key, unsigned int keylen,\n\t\t\t\t  int mode)\n{\n\tmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\n\tmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\n\tmemset(&ctx->enc_fw_req, 0, sizeof(ctx->enc_fw_req));\n\tmemset(&ctx->dec_fw_req, 0, sizeof(ctx->dec_fw_req));\n\n\treturn qat_alg_skcipher_init_sessions(ctx, key, keylen, mode);\n}\n\nstatic int qat_alg_skcipher_newkey(struct qat_alg_skcipher_ctx *ctx,\n\t\t\t\t   const u8 *key, unsigned int keylen,\n\t\t\t\t   int mode)\n{\n\tstruct qat_crypto_instance *inst = NULL;\n\tstruct device *dev;\n\tint node = numa_node_id();\n\tint ret;\n\n\tinst = qat_crypto_get_instance_node(node);\n\tif (!inst)\n\t\treturn -EINVAL;\n\tdev = &GET_DEV(inst->accel_dev);\n\tctx->inst = inst;\n\tctx->enc_cd = dma_alloc_coherent(dev, sizeof(*ctx->enc_cd),\n\t\t\t\t\t &ctx->enc_cd_paddr,\n\t\t\t\t\t GFP_ATOMIC);\n\tif (!ctx->enc_cd) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_instance;\n\t}\n\tctx->dec_cd = dma_alloc_coherent(dev, sizeof(*ctx->dec_cd),\n\t\t\t\t\t &ctx->dec_cd_paddr,\n\t\t\t\t\t GFP_ATOMIC);\n\tif (!ctx->dec_cd) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_enc;\n\t}\n\n\tret = qat_alg_skcipher_init_sessions(ctx, key, keylen, mode);\n\tif (ret)\n\t\tgoto out_free_all;\n\n\treturn 0;\n\nout_free_all:\n\tmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\n\tdma_free_coherent(dev, sizeof(*ctx->dec_cd),\n\t\t\t  ctx->dec_cd, ctx->dec_cd_paddr);\n\tctx->dec_cd = NULL;\nout_free_enc:\n\tmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\n\tdma_free_coherent(dev, sizeof(*ctx->enc_cd),\n\t\t\t  ctx->enc_cd, ctx->enc_cd_paddr);\n\tctx->enc_cd = NULL;\nout_free_instance:\n\tctx->inst = NULL;\n\tqat_crypto_put_instance(inst);\n\treturn ret;\n}\n\nstatic int qat_alg_skcipher_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t   const u8 *key, unsigned int keylen,\n\t\t\t\t   int mode)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tctx->mode = mode;\n\n\tif (ctx->enc_cd)\n\t\treturn qat_alg_skcipher_rekey(ctx, key, keylen, mode);\n\telse\n\t\treturn qat_alg_skcipher_newkey(ctx, key, keylen, mode);\n}\n\nstatic int qat_alg_skcipher_cbc_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t       const u8 *key, unsigned int keylen)\n{\n\treturn qat_alg_skcipher_setkey(tfm, key, keylen,\n\t\t\t\t       ICP_QAT_HW_CIPHER_CBC_MODE);\n}\n\nstatic int qat_alg_skcipher_ctr_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t       const u8 *key, unsigned int keylen)\n{\n\treturn qat_alg_skcipher_setkey(tfm, key, keylen,\n\t\t\t\t       ICP_QAT_HW_CIPHER_CTR_MODE);\n}\n\nstatic int qat_alg_skcipher_xts_setkey(struct crypto_skcipher *tfm,\n\t\t\t\t       const u8 *key, unsigned int keylen)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint ret;\n\n\tret = xts_verify_key(tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\n\tif (keylen >> 1 == AES_KEYSIZE_192) {\n\t\tret = crypto_skcipher_setkey(ctx->ftfm, key, keylen);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tctx->fallback = true;\n\n\t\treturn 0;\n\t}\n\n\tctx->fallback = false;\n\n\tret = qat_alg_skcipher_setkey(tfm, key, keylen,\n\t\t\t\t      ICP_QAT_HW_CIPHER_XTS_MODE);\n\tif (ret)\n\t\treturn ret;\n\n\tif (HW_CAP_AES_V2(ctx->inst->accel_dev))\n\t\tret = crypto_cipher_setkey(ctx->tweak, key + (keylen / 2),\n\t\t\t\t\t   keylen / 2);\n\n\treturn ret;\n}\n\nstatic void qat_alg_set_req_iv(struct qat_crypto_request *qat_req)\n{\n\tstruct icp_qat_fw_la_cipher_req_params *cipher_param;\n\tstruct qat_alg_skcipher_ctx *ctx = qat_req->skcipher_ctx;\n\tbool aes_v2_capable = HW_CAP_AES_V2(ctx->inst->accel_dev);\n\tu8 *iv = qat_req->skcipher_req->iv;\n\n\tcipher_param = (void *)&qat_req->req.serv_specif_rqpars;\n\n\tif (aes_v2_capable && ctx->mode == ICP_QAT_HW_CIPHER_XTS_MODE)\n\t\tcrypto_cipher_encrypt_one(ctx->tweak,\n\t\t\t\t\t  (u8 *)cipher_param->u.cipher_IV_array,\n\t\t\t\t\t  iv);\n\telse\n\t\tmemcpy(cipher_param->u.cipher_IV_array, iv, AES_BLOCK_SIZE);\n}\n\nstatic int qat_alg_skcipher_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(stfm);\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct qat_crypto_request *qat_req = skcipher_request_ctx(req);\n\tstruct icp_qat_fw_la_cipher_req_params *cipher_param;\n\tgfp_t f = qat_algs_alloc_flags(&req->base);\n\tstruct icp_qat_fw_la_bulk_req *msg;\n\tint ret;\n\n\tif (req->cryptlen == 0)\n\t\treturn 0;\n\n\tret = qat_bl_sgl_to_bufl(ctx->inst->accel_dev, req->src, req->dst,\n\t\t\t\t &qat_req->buf, NULL, f);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg = &qat_req->req;\n\t*msg = ctx->enc_fw_req;\n\tqat_req->skcipher_ctx = ctx;\n\tqat_req->skcipher_req = req;\n\tqat_req->cb = qat_skcipher_alg_callback;\n\tqat_req->req.comn_mid.opaque_data = (u64)(__force long)qat_req;\n\tqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\n\tqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\n\tqat_req->encryption = true;\n\tcipher_param = (void *)&qat_req->req.serv_specif_rqpars;\n\tcipher_param->cipher_length = req->cryptlen;\n\tcipher_param->cipher_offset = 0;\n\n\tqat_alg_set_req_iv(qat_req);\n\n\tret = qat_alg_send_sym_message(qat_req, ctx->inst, &req->base);\n\tif (ret == -ENOSPC)\n\t\tqat_bl_free_bufl(ctx->inst->accel_dev, &qat_req->buf);\n\n\treturn ret;\n}\n\nstatic int qat_alg_skcipher_blk_encrypt(struct skcipher_request *req)\n{\n\tif (req->cryptlen % AES_BLOCK_SIZE != 0)\n\t\treturn -EINVAL;\n\n\treturn qat_alg_skcipher_encrypt(req);\n}\n\nstatic int qat_alg_skcipher_xts_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tstruct skcipher_request *nreq = skcipher_request_ctx(req);\n\n\tif (req->cryptlen < XTS_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (ctx->fallback) {\n\t\tmemcpy(nreq, req, sizeof(*req));\n\t\tskcipher_request_set_tfm(nreq, ctx->ftfm);\n\t\treturn crypto_skcipher_encrypt(nreq);\n\t}\n\n\treturn qat_alg_skcipher_encrypt(req);\n}\n\nstatic int qat_alg_skcipher_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(stfm);\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct qat_crypto_request *qat_req = skcipher_request_ctx(req);\n\tstruct icp_qat_fw_la_cipher_req_params *cipher_param;\n\tgfp_t f = qat_algs_alloc_flags(&req->base);\n\tstruct icp_qat_fw_la_bulk_req *msg;\n\tint ret;\n\n\tif (req->cryptlen == 0)\n\t\treturn 0;\n\n\tret = qat_bl_sgl_to_bufl(ctx->inst->accel_dev, req->src, req->dst,\n\t\t\t\t &qat_req->buf, NULL, f);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg = &qat_req->req;\n\t*msg = ctx->dec_fw_req;\n\tqat_req->skcipher_ctx = ctx;\n\tqat_req->skcipher_req = req;\n\tqat_req->cb = qat_skcipher_alg_callback;\n\tqat_req->req.comn_mid.opaque_data = (u64)(__force long)qat_req;\n\tqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\n\tqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\n\tqat_req->encryption = false;\n\tcipher_param = (void *)&qat_req->req.serv_specif_rqpars;\n\tcipher_param->cipher_length = req->cryptlen;\n\tcipher_param->cipher_offset = 0;\n\n\tqat_alg_set_req_iv(qat_req);\n\tqat_alg_update_iv(qat_req);\n\n\tret = qat_alg_send_sym_message(qat_req, ctx->inst, &req->base);\n\tif (ret == -ENOSPC)\n\t\tqat_bl_free_bufl(ctx->inst->accel_dev, &qat_req->buf);\n\n\treturn ret;\n}\n\nstatic int qat_alg_skcipher_blk_decrypt(struct skcipher_request *req)\n{\n\tif (req->cryptlen % AES_BLOCK_SIZE != 0)\n\t\treturn -EINVAL;\n\n\treturn qat_alg_skcipher_decrypt(req);\n}\n\nstatic int qat_alg_skcipher_xts_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *stfm = crypto_skcipher_reqtfm(req);\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(stfm);\n\tstruct skcipher_request *nreq = skcipher_request_ctx(req);\n\n\tif (req->cryptlen < XTS_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tif (ctx->fallback) {\n\t\tmemcpy(nreq, req, sizeof(*req));\n\t\tskcipher_request_set_tfm(nreq, ctx->ftfm);\n\t\treturn crypto_skcipher_decrypt(nreq);\n\t}\n\n\treturn qat_alg_skcipher_decrypt(req);\n}\n\nstatic int qat_alg_aead_init(struct crypto_aead *tfm,\n\t\t\t     enum icp_qat_hw_auth_algo hash,\n\t\t\t     const char *hash_name)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\n\tctx->hash_tfm = crypto_alloc_shash(hash_name, 0, 0);\n\tif (IS_ERR(ctx->hash_tfm))\n\t\treturn PTR_ERR(ctx->hash_tfm);\n\tctx->qat_hash_alg = hash;\n\tcrypto_aead_set_reqsize(tfm, sizeof(struct qat_crypto_request));\n\treturn 0;\n}\n\nstatic int qat_alg_aead_sha1_init(struct crypto_aead *tfm)\n{\n\treturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA1, \"sha1\");\n}\n\nstatic int qat_alg_aead_sha256_init(struct crypto_aead *tfm)\n{\n\treturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA256, \"sha256\");\n}\n\nstatic int qat_alg_aead_sha512_init(struct crypto_aead *tfm)\n{\n\treturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA512, \"sha512\");\n}\n\nstatic void qat_alg_aead_exit(struct crypto_aead *tfm)\n{\n\tstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev;\n\n\tcrypto_free_shash(ctx->hash_tfm);\n\n\tif (!inst)\n\t\treturn;\n\n\tdev = &GET_DEV(inst->accel_dev);\n\tif (ctx->enc_cd) {\n\t\tmemset(ctx->enc_cd, 0, sizeof(struct qat_alg_cd));\n\t\tdma_free_coherent(dev, sizeof(struct qat_alg_cd),\n\t\t\t\t  ctx->enc_cd, ctx->enc_cd_paddr);\n\t}\n\tif (ctx->dec_cd) {\n\t\tmemset(ctx->dec_cd, 0, sizeof(struct qat_alg_cd));\n\t\tdma_free_coherent(dev, sizeof(struct qat_alg_cd),\n\t\t\t\t  ctx->dec_cd, ctx->dec_cd_paddr);\n\t}\n\tqat_crypto_put_instance(inst);\n}\n\nstatic int qat_alg_skcipher_init_tfm(struct crypto_skcipher *tfm)\n{\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct qat_crypto_request));\n\treturn 0;\n}\n\nstatic int qat_alg_skcipher_init_xts_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint reqsize;\n\n\tctx->ftfm = crypto_alloc_skcipher(\"xts(aes)\", 0,\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->ftfm))\n\t\treturn PTR_ERR(ctx->ftfm);\n\n\tctx->tweak = crypto_alloc_cipher(\"aes\", 0, 0);\n\tif (IS_ERR(ctx->tweak)) {\n\t\tcrypto_free_skcipher(ctx->ftfm);\n\t\treturn PTR_ERR(ctx->tweak);\n\t}\n\n\treqsize = max(sizeof(struct qat_crypto_request),\n\t\t      sizeof(struct skcipher_request) +\n\t\t      crypto_skcipher_reqsize(ctx->ftfm));\n\tcrypto_skcipher_set_reqsize(tfm, reqsize);\n\n\treturn 0;\n}\n\nstatic void qat_alg_skcipher_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct qat_crypto_instance *inst = ctx->inst;\n\tstruct device *dev;\n\n\tif (!inst)\n\t\treturn;\n\n\tdev = &GET_DEV(inst->accel_dev);\n\tif (ctx->enc_cd) {\n\t\tmemset(ctx->enc_cd, 0,\n\t\t       sizeof(struct icp_qat_hw_cipher_algo_blk));\n\t\tdma_free_coherent(dev,\n\t\t\t\t  sizeof(struct icp_qat_hw_cipher_algo_blk),\n\t\t\t\t  ctx->enc_cd, ctx->enc_cd_paddr);\n\t}\n\tif (ctx->dec_cd) {\n\t\tmemset(ctx->dec_cd, 0,\n\t\t       sizeof(struct icp_qat_hw_cipher_algo_blk));\n\t\tdma_free_coherent(dev,\n\t\t\t\t  sizeof(struct icp_qat_hw_cipher_algo_blk),\n\t\t\t\t  ctx->dec_cd, ctx->dec_cd_paddr);\n\t}\n\tqat_crypto_put_instance(inst);\n}\n\nstatic void qat_alg_skcipher_exit_xts_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct qat_alg_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tif (ctx->ftfm)\n\t\tcrypto_free_skcipher(ctx->ftfm);\n\n\tif (ctx->tweak)\n\t\tcrypto_free_cipher(ctx->tweak);\n\n\tqat_alg_skcipher_exit_tfm(tfm);\n}\n\nstatic struct aead_alg qat_aeads[] = { {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha1),cbc(aes))\",\n\t\t.cra_driver_name = \"qat_aes_cbc_hmac_sha1\",\n\t\t.cra_priority = 4001,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = qat_alg_aead_sha1_init,\n\t.exit = qat_alg_aead_exit,\n\t.setkey = qat_alg_aead_setkey,\n\t.decrypt = qat_alg_aead_dec,\n\t.encrypt = qat_alg_aead_enc,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA1_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha256),cbc(aes))\",\n\t\t.cra_driver_name = \"qat_aes_cbc_hmac_sha256\",\n\t\t.cra_priority = 4001,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = qat_alg_aead_sha256_init,\n\t.exit = qat_alg_aead_exit,\n\t.setkey = qat_alg_aead_setkey,\n\t.decrypt = qat_alg_aead_dec,\n\t.encrypt = qat_alg_aead_enc,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA256_DIGEST_SIZE,\n}, {\n\t.base = {\n\t\t.cra_name = \"authenc(hmac(sha512),cbc(aes))\",\n\t\t.cra_driver_name = \"qat_aes_cbc_hmac_sha512\",\n\t\t.cra_priority = 4001,\n\t\t.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t\t.cra_blocksize = AES_BLOCK_SIZE,\n\t\t.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),\n\t\t.cra_module = THIS_MODULE,\n\t},\n\t.init = qat_alg_aead_sha512_init,\n\t.exit = qat_alg_aead_exit,\n\t.setkey = qat_alg_aead_setkey,\n\t.decrypt = qat_alg_aead_dec,\n\t.encrypt = qat_alg_aead_enc,\n\t.ivsize = AES_BLOCK_SIZE,\n\t.maxauthsize = SHA512_DIGEST_SIZE,\n} };\n\nstatic struct skcipher_alg qat_skciphers[] = { {\n\t.base.cra_name = \"cbc(aes)\",\n\t.base.cra_driver_name = \"qat_aes_cbc\",\n\t.base.cra_priority = 4001,\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),\n\t.base.cra_alignmask = 0,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = qat_alg_skcipher_init_tfm,\n\t.exit = qat_alg_skcipher_exit_tfm,\n\t.setkey = qat_alg_skcipher_cbc_setkey,\n\t.decrypt = qat_alg_skcipher_blk_decrypt,\n\t.encrypt = qat_alg_skcipher_blk_encrypt,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.ivsize = AES_BLOCK_SIZE,\n}, {\n\t.base.cra_name = \"ctr(aes)\",\n\t.base.cra_driver_name = \"qat_aes_ctr\",\n\t.base.cra_priority = 4001,\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = 1,\n\t.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),\n\t.base.cra_alignmask = 0,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = qat_alg_skcipher_init_tfm,\n\t.exit = qat_alg_skcipher_exit_tfm,\n\t.setkey = qat_alg_skcipher_ctr_setkey,\n\t.decrypt = qat_alg_skcipher_decrypt,\n\t.encrypt = qat_alg_skcipher_encrypt,\n\t.min_keysize = AES_MIN_KEY_SIZE,\n\t.max_keysize = AES_MAX_KEY_SIZE,\n\t.ivsize = AES_BLOCK_SIZE,\n}, {\n\t.base.cra_name = \"xts(aes)\",\n\t.base.cra_driver_name = \"qat_aes_xts\",\n\t.base.cra_priority = 4001,\n\t.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK |\n\t\t\t  CRYPTO_ALG_ALLOCATES_MEMORY,\n\t.base.cra_blocksize = AES_BLOCK_SIZE,\n\t.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),\n\t.base.cra_alignmask = 0,\n\t.base.cra_module = THIS_MODULE,\n\n\t.init = qat_alg_skcipher_init_xts_tfm,\n\t.exit = qat_alg_skcipher_exit_xts_tfm,\n\t.setkey = qat_alg_skcipher_xts_setkey,\n\t.decrypt = qat_alg_skcipher_xts_decrypt,\n\t.encrypt = qat_alg_skcipher_xts_encrypt,\n\t.min_keysize = 2 * AES_MIN_KEY_SIZE,\n\t.max_keysize = 2 * AES_MAX_KEY_SIZE,\n\t.ivsize = AES_BLOCK_SIZE,\n} };\n\nint qat_algs_register(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&algs_lock);\n\tif (++active_devs != 1)\n\t\tgoto unlock;\n\n\tret = crypto_register_skciphers(qat_skciphers,\n\t\t\t\t\tARRAY_SIZE(qat_skciphers));\n\tif (ret)\n\t\tgoto unlock;\n\n\tret = crypto_register_aeads(qat_aeads, ARRAY_SIZE(qat_aeads));\n\tif (ret)\n\t\tgoto unreg_algs;\n\nunlock:\n\tmutex_unlock(&algs_lock);\n\treturn ret;\n\nunreg_algs:\n\tcrypto_unregister_skciphers(qat_skciphers, ARRAY_SIZE(qat_skciphers));\n\tgoto unlock;\n}\n\nvoid qat_algs_unregister(void)\n{\n\tmutex_lock(&algs_lock);\n\tif (--active_devs != 0)\n\t\tgoto unlock;\n\n\tcrypto_unregister_aeads(qat_aeads, ARRAY_SIZE(qat_aeads));\n\tcrypto_unregister_skciphers(qat_skciphers, ARRAY_SIZE(qat_skciphers));\n\nunlock:\n\tmutex_unlock(&algs_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}