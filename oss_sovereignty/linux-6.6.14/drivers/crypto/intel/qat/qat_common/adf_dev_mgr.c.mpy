{
  "module_name": "adf_dev_mgr.c",
  "hash_id": "dc7b01c0c2ceba293d503bd086233ace240edaeaf17f104f690711d3e94cd2d4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/intel/qat/qat_common/adf_dev_mgr.c",
  "human_readable_source": "\n \n#include <linux/mutex.h>\n#include <linux/list.h>\n#include \"adf_cfg.h\"\n#include \"adf_common_drv.h\"\n\nstatic LIST_HEAD(accel_table);\nstatic LIST_HEAD(vfs_table);\nstatic DEFINE_MUTEX(table_lock);\nstatic u32 num_devices;\nstatic u8 id_map[ADF_MAX_DEVICES];\n\nstruct vf_id_map {\n\tu32 bdf;\n\tu32 id;\n\tu32 fake_id;\n\tbool attached;\n\tstruct list_head list;\n};\n\nstatic int adf_get_vf_id(struct adf_accel_dev *vf)\n{\n\treturn ((7 * (PCI_SLOT(accel_to_pci_dev(vf)->devfn) - 1)) +\n\t\tPCI_FUNC(accel_to_pci_dev(vf)->devfn) +\n\t\t(PCI_SLOT(accel_to_pci_dev(vf)->devfn) - 1));\n}\n\nstatic int adf_get_vf_num(struct adf_accel_dev *vf)\n{\n\treturn (accel_to_pci_dev(vf)->bus->number << 8) | adf_get_vf_id(vf);\n}\n\nstatic struct vf_id_map *adf_find_vf(u32 bdf)\n{\n\tstruct list_head *itr;\n\n\tlist_for_each(itr, &vfs_table) {\n\t\tstruct vf_id_map *ptr =\n\t\t\tlist_entry(itr, struct vf_id_map, list);\n\n\t\tif (ptr->bdf == bdf)\n\t\t\treturn ptr;\n\t}\n\treturn NULL;\n}\n\nstatic int adf_get_vf_real_id(u32 fake)\n{\n\tstruct list_head *itr;\n\n\tlist_for_each(itr, &vfs_table) {\n\t\tstruct vf_id_map *ptr =\n\t\t\tlist_entry(itr, struct vf_id_map, list);\n\t\tif (ptr->fake_id == fake)\n\t\t\treturn ptr->id;\n\t}\n\treturn -1;\n}\n\n \nvoid adf_clean_vf_map(bool vf)\n{\n\tstruct vf_id_map *map;\n\tstruct list_head *ptr, *tmp;\n\n\tmutex_lock(&table_lock);\n\tlist_for_each_safe(ptr, tmp, &vfs_table) {\n\t\tmap = list_entry(ptr, struct vf_id_map, list);\n\t\tif (map->bdf != -1) {\n\t\t\tid_map[map->id] = 0;\n\t\t\tnum_devices--;\n\t\t}\n\n\t\tif (vf && map->bdf == -1)\n\t\t\tcontinue;\n\n\t\tlist_del(ptr);\n\t\tkfree(map);\n\t}\n\tmutex_unlock(&table_lock);\n}\nEXPORT_SYMBOL_GPL(adf_clean_vf_map);\n\n \nvoid adf_devmgr_update_class_index(struct adf_hw_device_data *hw_data)\n{\n\tstruct adf_hw_device_class *class = hw_data->dev_class;\n\tstruct list_head *itr;\n\tint i = 0;\n\n\tlist_for_each(itr, &accel_table) {\n\t\tstruct adf_accel_dev *ptr =\n\t\t\t\tlist_entry(itr, struct adf_accel_dev, list);\n\n\t\tif (ptr->hw_device->dev_class == class)\n\t\t\tptr->hw_device->instance_id = i++;\n\n\t\tif (i == class->instances)\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL_GPL(adf_devmgr_update_class_index);\n\nstatic unsigned int adf_find_free_id(void)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ADF_MAX_DEVICES; i++) {\n\t\tif (!id_map[i]) {\n\t\t\tid_map[i] = 1;\n\t\t\treturn i;\n\t\t}\n\t}\n\treturn ADF_MAX_DEVICES + 1;\n}\n\n \nint adf_devmgr_add_dev(struct adf_accel_dev *accel_dev,\n\t\t       struct adf_accel_dev *pf)\n{\n\tstruct list_head *itr;\n\tint ret = 0;\n\n\tif (num_devices == ADF_MAX_DEVICES) {\n\t\tdev_err(&GET_DEV(accel_dev), \"Only support up to %d devices\\n\",\n\t\t\tADF_MAX_DEVICES);\n\t\treturn -EFAULT;\n\t}\n\n\tmutex_lock(&table_lock);\n\tatomic_set(&accel_dev->ref_count, 0);\n\n\t \n\tif (!accel_dev->is_vf || !pf) {\n\t\tstruct vf_id_map *map;\n\n\t\tlist_for_each(itr, &accel_table) {\n\t\t\tstruct adf_accel_dev *ptr =\n\t\t\t\tlist_entry(itr, struct adf_accel_dev, list);\n\n\t\t\tif (ptr == accel_dev) {\n\t\t\t\tret = -EEXIST;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\n\t\tlist_add_tail(&accel_dev->list, &accel_table);\n\t\taccel_dev->accel_id = adf_find_free_id();\n\t\tif (accel_dev->accel_id > ADF_MAX_DEVICES) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\t\tnum_devices++;\n\t\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\t\tif (!map) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\t\tmap->bdf = ~0;\n\t\tmap->id = accel_dev->accel_id;\n\t\tmap->fake_id = map->id;\n\t\tmap->attached = true;\n\t\tlist_add_tail(&map->list, &vfs_table);\n\t} else if (accel_dev->is_vf && pf) {\n\t\t \n\t\tstruct vf_id_map *map;\n\n\t\tmap = adf_find_vf(adf_get_vf_num(accel_dev));\n\t\tif (map) {\n\t\t\tstruct vf_id_map *next;\n\n\t\t\taccel_dev->accel_id = map->id;\n\t\t\tlist_add_tail(&accel_dev->list, &accel_table);\n\t\t\tmap->fake_id++;\n\t\t\tmap->attached = true;\n\t\t\tnext = list_next_entry(map, list);\n\t\t\twhile (next && &next->list != &vfs_table) {\n\t\t\t\tnext->fake_id++;\n\t\t\t\tnext = list_next_entry(next, list);\n\t\t\t}\n\n\t\t\tret = 0;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\t\tif (!map) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\t\taccel_dev->accel_id = adf_find_free_id();\n\t\tif (accel_dev->accel_id > ADF_MAX_DEVICES) {\n\t\t\tkfree(map);\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\t\tnum_devices++;\n\t\tlist_add_tail(&accel_dev->list, &accel_table);\n\t\tmap->bdf = adf_get_vf_num(accel_dev);\n\t\tmap->id = accel_dev->accel_id;\n\t\tmap->fake_id = map->id;\n\t\tmap->attached = true;\n\t\tlist_add_tail(&map->list, &vfs_table);\n\t}\n\tmutex_init(&accel_dev->state_lock);\nunlock:\n\tmutex_unlock(&table_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(adf_devmgr_add_dev);\n\nstruct list_head *adf_devmgr_get_head(void)\n{\n\treturn &accel_table;\n}\n\n \nvoid adf_devmgr_rm_dev(struct adf_accel_dev *accel_dev,\n\t\t       struct adf_accel_dev *pf)\n{\n\tmutex_lock(&table_lock);\n\t \n\tif (!accel_dev->is_vf || !pf) {\n\t\tid_map[accel_dev->accel_id] = 0;\n\t\tnum_devices--;\n\t} else if (accel_dev->is_vf && pf) {\n\t\tstruct vf_id_map *map, *next;\n\n\t\tmap = adf_find_vf(adf_get_vf_num(accel_dev));\n\t\tif (!map) {\n\t\t\tdev_err(&GET_DEV(accel_dev), \"Failed to find VF map\\n\");\n\t\t\tgoto unlock;\n\t\t}\n\t\tmap->fake_id--;\n\t\tmap->attached = false;\n\t\tnext = list_next_entry(map, list);\n\t\twhile (next && &next->list != &vfs_table) {\n\t\t\tnext->fake_id--;\n\t\t\tnext = list_next_entry(next, list);\n\t\t}\n\t}\nunlock:\n\tmutex_destroy(&accel_dev->state_lock);\n\tlist_del(&accel_dev->list);\n\tmutex_unlock(&table_lock);\n}\nEXPORT_SYMBOL_GPL(adf_devmgr_rm_dev);\n\nstruct adf_accel_dev *adf_devmgr_get_first(void)\n{\n\tstruct adf_accel_dev *dev = NULL;\n\n\tif (!list_empty(&accel_table))\n\t\tdev = list_first_entry(&accel_table, struct adf_accel_dev,\n\t\t\t\t       list);\n\treturn dev;\n}\n\n \nstruct adf_accel_dev *adf_devmgr_pci_to_accel_dev(struct pci_dev *pci_dev)\n{\n\tstruct list_head *itr;\n\n\tmutex_lock(&table_lock);\n\tlist_for_each(itr, &accel_table) {\n\t\tstruct adf_accel_dev *ptr =\n\t\t\t\tlist_entry(itr, struct adf_accel_dev, list);\n\n\t\tif (ptr->accel_pci_dev.pci_dev == pci_dev) {\n\t\t\tmutex_unlock(&table_lock);\n\t\t\treturn ptr;\n\t\t}\n\t}\n\tmutex_unlock(&table_lock);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(adf_devmgr_pci_to_accel_dev);\n\nstruct adf_accel_dev *adf_devmgr_get_dev_by_id(u32 id)\n{\n\tstruct list_head *itr;\n\tint real_id;\n\n\tmutex_lock(&table_lock);\n\treal_id = adf_get_vf_real_id(id);\n\tif (real_id < 0)\n\t\tgoto unlock;\n\n\tid = real_id;\n\n\tlist_for_each(itr, &accel_table) {\n\t\tstruct adf_accel_dev *ptr =\n\t\t\t\tlist_entry(itr, struct adf_accel_dev, list);\n\t\tif (ptr->accel_id == id) {\n\t\t\tmutex_unlock(&table_lock);\n\t\t\treturn ptr;\n\t\t}\n\t}\nunlock:\n\tmutex_unlock(&table_lock);\n\treturn NULL;\n}\n\nint adf_devmgr_verify_id(u32 id)\n{\n\tif (id == ADF_CFG_ALL_DEVICES)\n\t\treturn 0;\n\n\tif (adf_devmgr_get_dev_by_id(id))\n\t\treturn 0;\n\n\treturn -ENODEV;\n}\n\nstatic int adf_get_num_dettached_vfs(void)\n{\n\tstruct list_head *itr;\n\tint vfs = 0;\n\n\tmutex_lock(&table_lock);\n\tlist_for_each(itr, &vfs_table) {\n\t\tstruct vf_id_map *ptr =\n\t\t\tlist_entry(itr, struct vf_id_map, list);\n\t\tif (ptr->bdf != ~0 && !ptr->attached)\n\t\t\tvfs++;\n\t}\n\tmutex_unlock(&table_lock);\n\treturn vfs;\n}\n\nvoid adf_devmgr_get_num_dev(u32 *num)\n{\n\t*num = num_devices - adf_get_num_dettached_vfs();\n}\n\n \nint adf_dev_in_use(struct adf_accel_dev *accel_dev)\n{\n\treturn atomic_read(&accel_dev->ref_count) != 0;\n}\nEXPORT_SYMBOL_GPL(adf_dev_in_use);\n\n \nint adf_dev_get(struct adf_accel_dev *accel_dev)\n{\n\tif (atomic_add_return(1, &accel_dev->ref_count) == 1)\n\t\tif (!try_module_get(accel_dev->owner))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(adf_dev_get);\n\n \nvoid adf_dev_put(struct adf_accel_dev *accel_dev)\n{\n\tif (atomic_sub_return(1, &accel_dev->ref_count) == 0)\n\t\tmodule_put(accel_dev->owner);\n}\nEXPORT_SYMBOL_GPL(adf_dev_put);\n\n \nint adf_devmgr_in_reset(struct adf_accel_dev *accel_dev)\n{\n\treturn test_bit(ADF_STATUS_RESTARTING, &accel_dev->status);\n}\nEXPORT_SYMBOL_GPL(adf_devmgr_in_reset);\n\n \nint adf_dev_started(struct adf_accel_dev *accel_dev)\n{\n\treturn test_bit(ADF_STATUS_STARTED, &accel_dev->status);\n}\nEXPORT_SYMBOL_GPL(adf_dev_started);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}