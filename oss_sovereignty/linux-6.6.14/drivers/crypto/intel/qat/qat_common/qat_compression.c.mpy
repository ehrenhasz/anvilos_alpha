{
  "module_name": "qat_compression.c",
  "hash_id": "6934cc602b4cce6985e7fa72cce221bc3dfef6a6537a473b619c2b8e49887287",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/intel/qat/qat_common/qat_compression.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/slab.h>\n#include \"adf_accel_devices.h\"\n#include \"adf_common_drv.h\"\n#include \"adf_transport.h\"\n#include \"adf_transport_access_macros.h\"\n#include \"adf_cfg.h\"\n#include \"adf_cfg_strings.h\"\n#include \"qat_compression.h\"\n#include \"icp_qat_fw.h\"\n\n#define SEC ADF_KERNEL_SEC\n\nstatic struct service_hndl qat_compression;\n\nvoid qat_compression_put_instance(struct qat_compression_instance *inst)\n{\n\tatomic_dec(&inst->refctr);\n\tadf_dev_put(inst->accel_dev);\n}\n\nstatic int qat_compression_free_instances(struct adf_accel_dev *accel_dev)\n{\n\tstruct qat_compression_instance *inst;\n\tstruct list_head *list_ptr, *tmp;\n\tint i;\n\n\tlist_for_each_safe(list_ptr, tmp, &accel_dev->compression_list) {\n\t\tinst = list_entry(list_ptr,\n\t\t\t\t  struct qat_compression_instance, list);\n\n\t\tfor (i = 0; i < atomic_read(&inst->refctr); i++)\n\t\t\tqat_compression_put_instance(inst);\n\n\t\tif (inst->dc_tx)\n\t\t\tadf_remove_ring(inst->dc_tx);\n\n\t\tif (inst->dc_rx)\n\t\t\tadf_remove_ring(inst->dc_rx);\n\n\t\tlist_del(list_ptr);\n\t\tkfree(inst);\n\t}\n\treturn 0;\n}\n\nstruct qat_compression_instance *qat_compression_get_instance_node(int node)\n{\n\tstruct qat_compression_instance *inst = NULL;\n\tstruct adf_accel_dev *accel_dev = NULL;\n\tunsigned long best = ~0;\n\tstruct list_head *itr;\n\n\tlist_for_each(itr, adf_devmgr_get_head()) {\n\t\tstruct adf_accel_dev *tmp_dev;\n\t\tunsigned long ctr;\n\t\tint tmp_dev_node;\n\n\t\ttmp_dev = list_entry(itr, struct adf_accel_dev, list);\n\t\ttmp_dev_node = dev_to_node(&GET_DEV(tmp_dev));\n\n\t\tif ((node == tmp_dev_node || tmp_dev_node < 0) &&\n\t\t    adf_dev_started(tmp_dev) && !list_empty(&tmp_dev->compression_list)) {\n\t\t\tctr = atomic_read(&tmp_dev->ref_count);\n\t\t\tif (best > ctr) {\n\t\t\t\taccel_dev = tmp_dev;\n\t\t\t\tbest = ctr;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!accel_dev) {\n\t\tpr_debug_ratelimited(\"QAT: Could not find a device on node %d\\n\", node);\n\t\t \n\t\tlist_for_each(itr, adf_devmgr_get_head()) {\n\t\t\tstruct adf_accel_dev *tmp_dev;\n\n\t\t\ttmp_dev = list_entry(itr, struct adf_accel_dev, list);\n\t\t\tif (adf_dev_started(tmp_dev) &&\n\t\t\t    !list_empty(&tmp_dev->compression_list)) {\n\t\t\t\taccel_dev = tmp_dev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!accel_dev)\n\t\treturn NULL;\n\n\tbest = ~0;\n\tlist_for_each(itr, &accel_dev->compression_list) {\n\t\tstruct qat_compression_instance *tmp_inst;\n\t\tunsigned long ctr;\n\n\t\ttmp_inst = list_entry(itr, struct qat_compression_instance, list);\n\t\tctr = atomic_read(&tmp_inst->refctr);\n\t\tif (best > ctr) {\n\t\t\tinst = tmp_inst;\n\t\t\tbest = ctr;\n\t\t}\n\t}\n\tif (inst) {\n\t\tif (adf_dev_get(accel_dev)) {\n\t\t\tdev_err(&GET_DEV(accel_dev), \"Could not increment dev refctr\\n\");\n\t\t\treturn NULL;\n\t\t}\n\t\tatomic_inc(&inst->refctr);\n\t}\n\treturn inst;\n}\n\nstatic int qat_compression_create_instances(struct adf_accel_dev *accel_dev)\n{\n\tstruct qat_compression_instance *inst;\n\tchar key[ADF_CFG_MAX_KEY_LEN_IN_BYTES];\n\tchar val[ADF_CFG_MAX_VAL_LEN_IN_BYTES];\n\tunsigned long num_inst, num_msg_dc;\n\tunsigned long bank;\n\tint msg_size;\n\tint ret;\n\tint i;\n\n\tINIT_LIST_HEAD(&accel_dev->compression_list);\n\tstrscpy(key, ADF_NUM_DC, sizeof(key));\n\tret = adf_cfg_get_param_value(accel_dev, SEC, key, val);\n\tif (ret)\n\t\treturn ret;\n\n\tret = kstrtoul(val, 10, &num_inst);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < num_inst; i++) {\n\t\tinst = kzalloc_node(sizeof(*inst), GFP_KERNEL,\n\t\t\t\t    dev_to_node(&GET_DEV(accel_dev)));\n\t\tif (!inst) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&inst->list, &accel_dev->compression_list);\n\t\tinst->id = i;\n\t\tatomic_set(&inst->refctr, 0);\n\t\tinst->accel_dev = accel_dev;\n\t\tinst->build_deflate_ctx = GET_DC_OPS(accel_dev)->build_deflate_ctx;\n\n\t\tsnprintf(key, sizeof(key), ADF_DC \"%d\" ADF_RING_DC_BANK_NUM, i);\n\t\tret = adf_cfg_get_param_value(accel_dev, SEC, key, val);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = kstrtoul(val, 10, &bank);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tsnprintf(key, sizeof(key), ADF_DC \"%d\" ADF_RING_DC_SIZE, i);\n\t\tret = adf_cfg_get_param_value(accel_dev, SEC, key, val);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = kstrtoul(val, 10, &num_msg_dc);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmsg_size = ICP_QAT_FW_REQ_DEFAULT_SZ;\n\t\tsnprintf(key, sizeof(key), ADF_DC \"%d\" ADF_RING_DC_TX, i);\n\t\tret = adf_create_ring(accel_dev, SEC, bank, num_msg_dc,\n\t\t\t\t      msg_size, key, NULL, 0, &inst->dc_tx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmsg_size = ICP_QAT_FW_RESP_DEFAULT_SZ;\n\t\tsnprintf(key, sizeof(key), ADF_DC \"%d\" ADF_RING_DC_RX, i);\n\t\tret = adf_create_ring(accel_dev, SEC, bank, num_msg_dc,\n\t\t\t\t      msg_size, key, qat_comp_alg_callback, 0,\n\t\t\t\t      &inst->dc_rx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tinst->dc_data = accel_dev->dc_data;\n\t\tINIT_LIST_HEAD(&inst->backlog.list);\n\t\tspin_lock_init(&inst->backlog.lock);\n\t}\n\treturn 0;\nerr:\n\tqat_compression_free_instances(accel_dev);\n\treturn ret;\n}\n\nstatic int qat_compression_alloc_dc_data(struct adf_accel_dev *accel_dev)\n{\n\tstruct device *dev = &GET_DEV(accel_dev);\n\tdma_addr_t obuff_p = DMA_MAPPING_ERROR;\n\tsize_t ovf_buff_sz = QAT_COMP_MAX_SKID;\n\tstruct adf_dc_data *dc_data = NULL;\n\tu8 *obuff = NULL;\n\n\tdc_data = devm_kzalloc(dev, sizeof(*dc_data), GFP_KERNEL);\n\tif (!dc_data)\n\t\tgoto err;\n\n\tobuff = kzalloc_node(ovf_buff_sz, GFP_KERNEL, dev_to_node(dev));\n\tif (!obuff)\n\t\tgoto err;\n\n\tobuff_p = dma_map_single(dev, obuff, ovf_buff_sz, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, obuff_p)))\n\t\tgoto err;\n\n\tdc_data->ovf_buff = obuff;\n\tdc_data->ovf_buff_p = obuff_p;\n\tdc_data->ovf_buff_sz = ovf_buff_sz;\n\n\taccel_dev->dc_data = dc_data;\n\n\treturn 0;\n\nerr:\n\taccel_dev->dc_data = NULL;\n\tkfree(obuff);\n\tdevm_kfree(dev, dc_data);\n\treturn -ENOMEM;\n}\n\nstatic void qat_free_dc_data(struct adf_accel_dev *accel_dev)\n{\n\tstruct adf_dc_data *dc_data = accel_dev->dc_data;\n\tstruct device *dev = &GET_DEV(accel_dev);\n\n\tif (!dc_data)\n\t\treturn;\n\n\tdma_unmap_single(dev, dc_data->ovf_buff_p, dc_data->ovf_buff_sz,\n\t\t\t DMA_FROM_DEVICE);\n\tkfree_sensitive(dc_data->ovf_buff);\n\tdevm_kfree(dev, dc_data);\n\taccel_dev->dc_data = NULL;\n}\n\nstatic int qat_compression_init(struct adf_accel_dev *accel_dev)\n{\n\tint ret;\n\n\tret = qat_compression_alloc_dc_data(accel_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = qat_compression_create_instances(accel_dev);\n\tif (ret)\n\t\tqat_free_dc_data(accel_dev);\n\n\treturn ret;\n}\n\nstatic int qat_compression_shutdown(struct adf_accel_dev *accel_dev)\n{\n\tqat_free_dc_data(accel_dev);\n\treturn qat_compression_free_instances(accel_dev);\n}\n\nstatic int qat_compression_event_handler(struct adf_accel_dev *accel_dev,\n\t\t\t\t\t enum adf_event event)\n{\n\tint ret;\n\n\tswitch (event) {\n\tcase ADF_EVENT_INIT:\n\t\tret = qat_compression_init(accel_dev);\n\t\tbreak;\n\tcase ADF_EVENT_SHUTDOWN:\n\t\tret = qat_compression_shutdown(accel_dev);\n\t\tbreak;\n\tcase ADF_EVENT_RESTARTING:\n\tcase ADF_EVENT_RESTARTED:\n\tcase ADF_EVENT_START:\n\tcase ADF_EVENT_STOP:\n\tdefault:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nint qat_compression_register(void)\n{\n\tmemset(&qat_compression, 0, sizeof(qat_compression));\n\tqat_compression.event_hld = qat_compression_event_handler;\n\tqat_compression.name = \"qat_compression\";\n\treturn adf_service_register(&qat_compression);\n}\n\nint qat_compression_unregister(void)\n{\n\treturn adf_service_unregister(&qat_compression);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}