{
  "module_name": "s5p-sss.c",
  "hash_id": "0859b2afac172994497cd82726c6e9d6c68da01feba0e6d54fdedb4632779d3c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/s5p-sss.c",
  "human_readable_source": "\n\n\n\n\n\n\n\n\n\n\n#include <linux/clk.h>\n#include <linux/crypto.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/scatterlist.h>\n\n#include <crypto/ctr.h>\n#include <crypto/aes.h>\n#include <crypto/algapi.h>\n#include <crypto/scatterwalk.h>\n\n#include <crypto/hash.h>\n#include <crypto/md5.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/internal/hash.h>\n\n#define _SBF(s, v)\t\t\t((v) << (s))\n\n \n#define SSS_REG_FCINTSTAT\t\t0x0000\n#define SSS_FCINTSTAT_HPARTINT\t\tBIT(7)\n#define SSS_FCINTSTAT_HDONEINT\t\tBIT(5)\n#define SSS_FCINTSTAT_BRDMAINT\t\tBIT(3)\n#define SSS_FCINTSTAT_BTDMAINT\t\tBIT(2)\n#define SSS_FCINTSTAT_HRDMAINT\t\tBIT(1)\n#define SSS_FCINTSTAT_PKDMAINT\t\tBIT(0)\n\n#define SSS_REG_FCINTENSET\t\t0x0004\n#define SSS_FCINTENSET_HPARTINTENSET\tBIT(7)\n#define SSS_FCINTENSET_HDONEINTENSET\tBIT(5)\n#define SSS_FCINTENSET_BRDMAINTENSET\tBIT(3)\n#define SSS_FCINTENSET_BTDMAINTENSET\tBIT(2)\n#define SSS_FCINTENSET_HRDMAINTENSET\tBIT(1)\n#define SSS_FCINTENSET_PKDMAINTENSET\tBIT(0)\n\n#define SSS_REG_FCINTENCLR\t\t0x0008\n#define SSS_FCINTENCLR_HPARTINTENCLR\tBIT(7)\n#define SSS_FCINTENCLR_HDONEINTENCLR\tBIT(5)\n#define SSS_FCINTENCLR_BRDMAINTENCLR\tBIT(3)\n#define SSS_FCINTENCLR_BTDMAINTENCLR\tBIT(2)\n#define SSS_FCINTENCLR_HRDMAINTENCLR\tBIT(1)\n#define SSS_FCINTENCLR_PKDMAINTENCLR\tBIT(0)\n\n#define SSS_REG_FCINTPEND\t\t0x000C\n#define SSS_FCINTPEND_HPARTINTP\t\tBIT(7)\n#define SSS_FCINTPEND_HDONEINTP\t\tBIT(5)\n#define SSS_FCINTPEND_BRDMAINTP\t\tBIT(3)\n#define SSS_FCINTPEND_BTDMAINTP\t\tBIT(2)\n#define SSS_FCINTPEND_HRDMAINTP\t\tBIT(1)\n#define SSS_FCINTPEND_PKDMAINTP\t\tBIT(0)\n\n#define SSS_REG_FCFIFOSTAT\t\t0x0010\n#define SSS_FCFIFOSTAT_BRFIFOFUL\tBIT(7)\n#define SSS_FCFIFOSTAT_BRFIFOEMP\tBIT(6)\n#define SSS_FCFIFOSTAT_BTFIFOFUL\tBIT(5)\n#define SSS_FCFIFOSTAT_BTFIFOEMP\tBIT(4)\n#define SSS_FCFIFOSTAT_HRFIFOFUL\tBIT(3)\n#define SSS_FCFIFOSTAT_HRFIFOEMP\tBIT(2)\n#define SSS_FCFIFOSTAT_PKFIFOFUL\tBIT(1)\n#define SSS_FCFIFOSTAT_PKFIFOEMP\tBIT(0)\n\n#define SSS_REG_FCFIFOCTRL\t\t0x0014\n#define SSS_FCFIFOCTRL_DESSEL\t\tBIT(2)\n#define SSS_HASHIN_INDEPENDENT\t\t_SBF(0, 0x00)\n#define SSS_HASHIN_CIPHER_INPUT\t\t_SBF(0, 0x01)\n#define SSS_HASHIN_CIPHER_OUTPUT\t_SBF(0, 0x02)\n#define SSS_HASHIN_MASK\t\t\t_SBF(0, 0x03)\n\n#define SSS_REG_FCBRDMAS\t\t0x0020\n#define SSS_REG_FCBRDMAL\t\t0x0024\n#define SSS_REG_FCBRDMAC\t\t0x0028\n#define SSS_FCBRDMAC_BYTESWAP\t\tBIT(1)\n#define SSS_FCBRDMAC_FLUSH\t\tBIT(0)\n\n#define SSS_REG_FCBTDMAS\t\t0x0030\n#define SSS_REG_FCBTDMAL\t\t0x0034\n#define SSS_REG_FCBTDMAC\t\t0x0038\n#define SSS_FCBTDMAC_BYTESWAP\t\tBIT(1)\n#define SSS_FCBTDMAC_FLUSH\t\tBIT(0)\n\n#define SSS_REG_FCHRDMAS\t\t0x0040\n#define SSS_REG_FCHRDMAL\t\t0x0044\n#define SSS_REG_FCHRDMAC\t\t0x0048\n#define SSS_FCHRDMAC_BYTESWAP\t\tBIT(1)\n#define SSS_FCHRDMAC_FLUSH\t\tBIT(0)\n\n#define SSS_REG_FCPKDMAS\t\t0x0050\n#define SSS_REG_FCPKDMAL\t\t0x0054\n#define SSS_REG_FCPKDMAC\t\t0x0058\n#define SSS_FCPKDMAC_BYTESWAP\t\tBIT(3)\n#define SSS_FCPKDMAC_DESCEND\t\tBIT(2)\n#define SSS_FCPKDMAC_TRANSMIT\t\tBIT(1)\n#define SSS_FCPKDMAC_FLUSH\t\tBIT(0)\n\n#define SSS_REG_FCPKDMAO\t\t0x005C\n\n \n#define SSS_REG_AES_CONTROL\t\t0x00\n#define SSS_AES_BYTESWAP_DI\t\tBIT(11)\n#define SSS_AES_BYTESWAP_DO\t\tBIT(10)\n#define SSS_AES_BYTESWAP_IV\t\tBIT(9)\n#define SSS_AES_BYTESWAP_CNT\t\tBIT(8)\n#define SSS_AES_BYTESWAP_KEY\t\tBIT(7)\n#define SSS_AES_KEY_CHANGE_MODE\t\tBIT(6)\n#define SSS_AES_KEY_SIZE_128\t\t_SBF(4, 0x00)\n#define SSS_AES_KEY_SIZE_192\t\t_SBF(4, 0x01)\n#define SSS_AES_KEY_SIZE_256\t\t_SBF(4, 0x02)\n#define SSS_AES_FIFO_MODE\t\tBIT(3)\n#define SSS_AES_CHAIN_MODE_ECB\t\t_SBF(1, 0x00)\n#define SSS_AES_CHAIN_MODE_CBC\t\t_SBF(1, 0x01)\n#define SSS_AES_CHAIN_MODE_CTR\t\t_SBF(1, 0x02)\n#define SSS_AES_MODE_DECRYPT\t\tBIT(0)\n\n#define SSS_REG_AES_STATUS\t\t0x04\n#define SSS_AES_BUSY\t\t\tBIT(2)\n#define SSS_AES_INPUT_READY\t\tBIT(1)\n#define SSS_AES_OUTPUT_READY\t\tBIT(0)\n\n#define SSS_REG_AES_IN_DATA(s)\t\t(0x10 + (s << 2))\n#define SSS_REG_AES_OUT_DATA(s)\t\t(0x20 + (s << 2))\n#define SSS_REG_AES_IV_DATA(s)\t\t(0x30 + (s << 2))\n#define SSS_REG_AES_CNT_DATA(s)\t\t(0x40 + (s << 2))\n#define SSS_REG_AES_KEY_DATA(s)\t\t(0x80 + (s << 2))\n\n#define SSS_REG(dev, reg)\t\t((dev)->ioaddr + (SSS_REG_##reg))\n#define SSS_READ(dev, reg)\t\t__raw_readl(SSS_REG(dev, reg))\n#define SSS_WRITE(dev, reg, val)\t__raw_writel((val), SSS_REG(dev, reg))\n\n#define SSS_AES_REG(dev, reg)\t\t((dev)->aes_ioaddr + SSS_REG_##reg)\n#define SSS_AES_WRITE(dev, reg, val)    __raw_writel((val), \\\n\t\t\t\t\t\tSSS_AES_REG(dev, reg))\n\n \n#define FLAGS_AES_DECRYPT\t\tBIT(0)\n#define FLAGS_AES_MODE_MASK\t\t_SBF(1, 0x03)\n#define FLAGS_AES_CBC\t\t\t_SBF(1, 0x01)\n#define FLAGS_AES_CTR\t\t\t_SBF(1, 0x02)\n\n#define AES_KEY_LEN\t\t\t16\n#define CRYPTO_QUEUE_LEN\t\t1\n\n \n#define SSS_REG_HASH_CTRL\t\t0x00\n\n#define SSS_HASH_USER_IV_EN\t\tBIT(5)\n#define SSS_HASH_INIT_BIT\t\tBIT(4)\n#define SSS_HASH_ENGINE_SHA1\t\t_SBF(1, 0x00)\n#define SSS_HASH_ENGINE_MD5\t\t_SBF(1, 0x01)\n#define SSS_HASH_ENGINE_SHA256\t\t_SBF(1, 0x02)\n\n#define SSS_HASH_ENGINE_MASK\t\t_SBF(1, 0x03)\n\n#define SSS_REG_HASH_CTRL_PAUSE\t\t0x04\n\n#define SSS_HASH_PAUSE\t\t\tBIT(0)\n\n#define SSS_REG_HASH_CTRL_FIFO\t\t0x08\n\n#define SSS_HASH_FIFO_MODE_DMA\t\tBIT(0)\n#define SSS_HASH_FIFO_MODE_CPU          0\n\n#define SSS_REG_HASH_CTRL_SWAP\t\t0x0C\n\n#define SSS_HASH_BYTESWAP_DI\t\tBIT(3)\n#define SSS_HASH_BYTESWAP_DO\t\tBIT(2)\n#define SSS_HASH_BYTESWAP_IV\t\tBIT(1)\n#define SSS_HASH_BYTESWAP_KEY\t\tBIT(0)\n\n#define SSS_REG_HASH_STATUS\t\t0x10\n\n#define SSS_HASH_STATUS_MSG_DONE\tBIT(6)\n#define SSS_HASH_STATUS_PARTIAL_DONE\tBIT(4)\n#define SSS_HASH_STATUS_BUFFER_READY\tBIT(0)\n\n#define SSS_REG_HASH_MSG_SIZE_LOW\t0x20\n#define SSS_REG_HASH_MSG_SIZE_HIGH\t0x24\n\n#define SSS_REG_HASH_PRE_MSG_SIZE_LOW\t0x28\n#define SSS_REG_HASH_PRE_MSG_SIZE_HIGH\t0x2C\n\n#define SSS_REG_HASH_IV(s)\t\t(0xB0 + ((s) << 2))\n#define SSS_REG_HASH_OUT(s)\t\t(0x100 + ((s) << 2))\n\n#define HASH_BLOCK_SIZE\t\t\t64\n#define HASH_REG_SIZEOF\t\t\t4\n#define HASH_MD5_MAX_REG\t\t(MD5_DIGEST_SIZE / HASH_REG_SIZEOF)\n#define HASH_SHA1_MAX_REG\t\t(SHA1_DIGEST_SIZE / HASH_REG_SIZEOF)\n#define HASH_SHA256_MAX_REG\t\t(SHA256_DIGEST_SIZE / HASH_REG_SIZEOF)\n\n \n#define HASH_FLAGS_BUSY\t\t0\n#define HASH_FLAGS_FINAL\t1\n#define HASH_FLAGS_DMA_ACTIVE\t2\n#define HASH_FLAGS_OUTPUT_READY\t3\n#define HASH_FLAGS_DMA_READY\t4\n#define HASH_FLAGS_SGS_COPIED\t5\n#define HASH_FLAGS_SGS_ALLOCED\t6\n\n \n#define BUFLEN\t\t\tHASH_BLOCK_SIZE\n\n#define SSS_HASH_DMA_LEN_ALIGN\t8\n#define SSS_HASH_DMA_ALIGN_MASK\t(SSS_HASH_DMA_LEN_ALIGN - 1)\n\n#define SSS_HASH_QUEUE_LENGTH\t10\n\n \nstruct samsung_aes_variant {\n\tunsigned int\t\t\taes_offset;\n\tunsigned int\t\t\thash_offset;\n\tconst char\t\t\t*clk_names[2];\n};\n\nstruct s5p_aes_reqctx {\n\tunsigned long\t\t\tmode;\n};\n\nstruct s5p_aes_ctx {\n\tstruct s5p_aes_dev\t\t*dev;\n\n\tu8\t\t\t\taes_key[AES_MAX_KEY_SIZE];\n\tu8\t\t\t\tnonce[CTR_RFC3686_NONCE_SIZE];\n\tint\t\t\t\tkeylen;\n};\n\n \nstruct s5p_aes_dev {\n\tstruct device\t\t\t*dev;\n\tstruct clk\t\t\t*clk;\n\tstruct clk\t\t\t*pclk;\n\tvoid __iomem\t\t\t*ioaddr;\n\tvoid __iomem\t\t\t*aes_ioaddr;\n\tint\t\t\t\tirq_fc;\n\n\tstruct skcipher_request\t\t*req;\n\tstruct s5p_aes_ctx\t\t*ctx;\n\tstruct scatterlist\t\t*sg_src;\n\tstruct scatterlist\t\t*sg_dst;\n\n\tstruct scatterlist\t\t*sg_src_cpy;\n\tstruct scatterlist\t\t*sg_dst_cpy;\n\n\tstruct tasklet_struct\t\ttasklet;\n\tstruct crypto_queue\t\tqueue;\n\tbool\t\t\t\tbusy;\n\tspinlock_t\t\t\tlock;\n\n\tstruct resource\t\t\t*res;\n\tvoid __iomem\t\t\t*io_hash_base;\n\n\tspinlock_t\t\t\thash_lock;  \n\tunsigned long\t\t\thash_flags;\n\tstruct crypto_queue\t\thash_queue;\n\tstruct tasklet_struct\t\thash_tasklet;\n\n\tu8\t\t\t\txmit_buf[BUFLEN];\n\tstruct ahash_request\t\t*hash_req;\n\tstruct scatterlist\t\t*hash_sg_iter;\n\tunsigned int\t\t\thash_sg_cnt;\n\n\tbool\t\t\t\tuse_hash;\n};\n\n \nstruct s5p_hash_reqctx {\n\tstruct s5p_aes_dev\t*dd;\n\tbool\t\t\top_update;\n\n\tu64\t\t\tdigcnt;\n\tu8\t\t\tdigest[SHA256_DIGEST_SIZE];\n\n\tunsigned int\t\tnregs;  \n\tu32\t\t\tengine;\n\n\tstruct scatterlist\t*sg;\n\tunsigned int\t\tsg_len;\n\tstruct scatterlist\tsgl[2];\n\tunsigned int\t\tskip;\n\tunsigned int\t\ttotal;\n\tbool\t\t\tfinup;\n\tbool\t\t\terror;\n\n\tu32\t\t\tbufcnt;\n\tu8\t\t\tbuffer[];\n};\n\n \nstruct s5p_hash_ctx {\n\tstruct s5p_aes_dev\t*dd;\n\tunsigned long\t\tflags;\n\tstruct crypto_shash\t*fallback;\n};\n\nstatic const struct samsung_aes_variant s5p_aes_data = {\n\t.aes_offset\t= 0x4000,\n\t.hash_offset\t= 0x6000,\n\t.clk_names\t= { \"secss\", },\n};\n\nstatic const struct samsung_aes_variant exynos_aes_data = {\n\t.aes_offset\t= 0x200,\n\t.hash_offset\t= 0x400,\n\t.clk_names\t= { \"secss\", },\n};\n\nstatic const struct samsung_aes_variant exynos5433_slim_aes_data = {\n\t.aes_offset\t= 0x400,\n\t.hash_offset\t= 0x800,\n\t.clk_names\t= { \"aclk\", \"pclk\", },\n};\n\nstatic const struct of_device_id s5p_sss_dt_match[] = {\n\t{\n\t\t.compatible = \"samsung,s5pv210-secss\",\n\t\t.data = &s5p_aes_data,\n\t},\n\t{\n\t\t.compatible = \"samsung,exynos4210-secss\",\n\t\t.data = &exynos_aes_data,\n\t},\n\t{\n\t\t.compatible = \"samsung,exynos5433-slim-sss\",\n\t\t.data = &exynos5433_slim_aes_data,\n\t},\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, s5p_sss_dt_match);\n\nstatic inline const struct samsung_aes_variant *find_s5p_sss_version\n\t\t\t\t   (const struct platform_device *pdev)\n{\n\tif (IS_ENABLED(CONFIG_OF) && (pdev->dev.of_node))\n\t\treturn of_device_get_match_data(&pdev->dev);\n\n\treturn (const struct samsung_aes_variant *)\n\t\t\tplatform_get_device_id(pdev)->driver_data;\n}\n\nstatic struct s5p_aes_dev *s5p_dev;\n\nstatic void s5p_set_dma_indata(struct s5p_aes_dev *dev,\n\t\t\t       const struct scatterlist *sg)\n{\n\tSSS_WRITE(dev, FCBRDMAS, sg_dma_address(sg));\n\tSSS_WRITE(dev, FCBRDMAL, sg_dma_len(sg));\n}\n\nstatic void s5p_set_dma_outdata(struct s5p_aes_dev *dev,\n\t\t\t\tconst struct scatterlist *sg)\n{\n\tSSS_WRITE(dev, FCBTDMAS, sg_dma_address(sg));\n\tSSS_WRITE(dev, FCBTDMAL, sg_dma_len(sg));\n}\n\nstatic void s5p_free_sg_cpy(struct s5p_aes_dev *dev, struct scatterlist **sg)\n{\n\tint len;\n\n\tif (!*sg)\n\t\treturn;\n\n\tlen = ALIGN(dev->req->cryptlen, AES_BLOCK_SIZE);\n\tfree_pages((unsigned long)sg_virt(*sg), get_order(len));\n\n\tkfree(*sg);\n\t*sg = NULL;\n}\n\nstatic void s5p_sg_copy_buf(void *buf, struct scatterlist *sg,\n\t\t\t    unsigned int nbytes, int out)\n{\n\tstruct scatter_walk walk;\n\n\tif (!nbytes)\n\t\treturn;\n\n\tscatterwalk_start(&walk, sg);\n\tscatterwalk_copychunks(buf, &walk, nbytes, out);\n\tscatterwalk_done(&walk, out, 0);\n}\n\nstatic void s5p_sg_done(struct s5p_aes_dev *dev)\n{\n\tstruct skcipher_request *req = dev->req;\n\tstruct s5p_aes_reqctx *reqctx = skcipher_request_ctx(req);\n\n\tif (dev->sg_dst_cpy) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"Copying %d bytes of output data back to original place\\n\",\n\t\t\tdev->req->cryptlen);\n\t\ts5p_sg_copy_buf(sg_virt(dev->sg_dst_cpy), dev->req->dst,\n\t\t\t\tdev->req->cryptlen, 1);\n\t}\n\ts5p_free_sg_cpy(dev, &dev->sg_src_cpy);\n\ts5p_free_sg_cpy(dev, &dev->sg_dst_cpy);\n\tif (reqctx->mode & FLAGS_AES_CBC)\n\t\tmemcpy_fromio(req->iv, dev->aes_ioaddr + SSS_REG_AES_IV_DATA(0), AES_BLOCK_SIZE);\n\n\telse if (reqctx->mode & FLAGS_AES_CTR)\n\t\tmemcpy_fromio(req->iv, dev->aes_ioaddr + SSS_REG_AES_CNT_DATA(0), AES_BLOCK_SIZE);\n}\n\n \nstatic void s5p_aes_complete(struct skcipher_request *req, int err)\n{\n\tskcipher_request_complete(req, err);\n}\n\nstatic void s5p_unset_outdata(struct s5p_aes_dev *dev)\n{\n\tdma_unmap_sg(dev->dev, dev->sg_dst, 1, DMA_FROM_DEVICE);\n}\n\nstatic void s5p_unset_indata(struct s5p_aes_dev *dev)\n{\n\tdma_unmap_sg(dev->dev, dev->sg_src, 1, DMA_TO_DEVICE);\n}\n\nstatic int s5p_make_sg_cpy(struct s5p_aes_dev *dev, struct scatterlist *src,\n\t\t\t   struct scatterlist **dst)\n{\n\tvoid *pages;\n\tint len;\n\n\t*dst = kmalloc(sizeof(**dst), GFP_ATOMIC);\n\tif (!*dst)\n\t\treturn -ENOMEM;\n\n\tlen = ALIGN(dev->req->cryptlen, AES_BLOCK_SIZE);\n\tpages = (void *)__get_free_pages(GFP_ATOMIC, get_order(len));\n\tif (!pages) {\n\t\tkfree(*dst);\n\t\t*dst = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\ts5p_sg_copy_buf(pages, src, dev->req->cryptlen, 0);\n\n\tsg_init_table(*dst, 1);\n\tsg_set_buf(*dst, pages, len);\n\n\treturn 0;\n}\n\nstatic int s5p_set_outdata(struct s5p_aes_dev *dev, struct scatterlist *sg)\n{\n\tif (!sg->length)\n\t\treturn -EINVAL;\n\n\tif (!dma_map_sg(dev->dev, sg, 1, DMA_FROM_DEVICE))\n\t\treturn -ENOMEM;\n\n\tdev->sg_dst = sg;\n\n\treturn 0;\n}\n\nstatic int s5p_set_indata(struct s5p_aes_dev *dev, struct scatterlist *sg)\n{\n\tif (!sg->length)\n\t\treturn -EINVAL;\n\n\tif (!dma_map_sg(dev->dev, sg, 1, DMA_TO_DEVICE))\n\t\treturn -ENOMEM;\n\n\tdev->sg_src = sg;\n\n\treturn 0;\n}\n\n \nstatic int s5p_aes_tx(struct s5p_aes_dev *dev)\n{\n\tint ret = 0;\n\n\ts5p_unset_outdata(dev);\n\n\tif (!sg_is_last(dev->sg_dst)) {\n\t\tret = s5p_set_outdata(dev, sg_next(dev->sg_dst));\n\t\tif (!ret)\n\t\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int s5p_aes_rx(struct s5p_aes_dev *dev )\n{\n\tint ret = 0;\n\n\ts5p_unset_indata(dev);\n\n\tif (!sg_is_last(dev->sg_src)) {\n\t\tret = s5p_set_indata(dev, sg_next(dev->sg_src));\n\t\tif (!ret)\n\t\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\nstatic inline u32 s5p_hash_read(struct s5p_aes_dev *dd, u32 offset)\n{\n\treturn __raw_readl(dd->io_hash_base + offset);\n}\n\nstatic inline void s5p_hash_write(struct s5p_aes_dev *dd,\n\t\t\t\t  u32 offset, u32 value)\n{\n\t__raw_writel(value, dd->io_hash_base + offset);\n}\n\n \nstatic void s5p_set_dma_hashdata(struct s5p_aes_dev *dev,\n\t\t\t\t const struct scatterlist *sg)\n{\n\tdev->hash_sg_cnt--;\n\tSSS_WRITE(dev, FCHRDMAS, sg_dma_address(sg));\n\tSSS_WRITE(dev, FCHRDMAL, sg_dma_len(sg));  \n}\n\n \nstatic int s5p_hash_rx(struct s5p_aes_dev *dev)\n{\n\tif (dev->hash_sg_cnt > 0) {\n\t\tdev->hash_sg_iter = sg_next(dev->hash_sg_iter);\n\t\treturn 1;\n\t}\n\n\tset_bit(HASH_FLAGS_DMA_READY, &dev->hash_flags);\n\tif (test_bit(HASH_FLAGS_FINAL, &dev->hash_flags))\n\t\treturn 0;\n\n\treturn 2;\n}\n\nstatic irqreturn_t s5p_aes_interrupt(int irq, void *dev_id)\n{\n\tstruct platform_device *pdev = dev_id;\n\tstruct s5p_aes_dev *dev = platform_get_drvdata(pdev);\n\tstruct skcipher_request *req;\n\tint err_dma_tx = 0;\n\tint err_dma_rx = 0;\n\tint err_dma_hx = 0;\n\tbool tx_end = false;\n\tbool hx_end = false;\n\tunsigned long flags;\n\tu32 status, st_bits;\n\tint err;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\t \n\tstatus = SSS_READ(dev, FCINTSTAT);\n\tif (status & SSS_FCINTSTAT_BRDMAINT)\n\t\terr_dma_rx = s5p_aes_rx(dev);\n\n\tif (status & SSS_FCINTSTAT_BTDMAINT) {\n\t\tif (sg_is_last(dev->sg_dst))\n\t\t\ttx_end = true;\n\t\terr_dma_tx = s5p_aes_tx(dev);\n\t}\n\n\tif (status & SSS_FCINTSTAT_HRDMAINT)\n\t\terr_dma_hx = s5p_hash_rx(dev);\n\n\tst_bits = status & (SSS_FCINTSTAT_BRDMAINT | SSS_FCINTSTAT_BTDMAINT |\n\t\t\t\tSSS_FCINTSTAT_HRDMAINT);\n\t \n\tSSS_WRITE(dev, FCINTPEND, st_bits);\n\n\t \n\tif (status & (SSS_FCINTSTAT_HDONEINT | SSS_FCINTSTAT_HPARTINT)) {\n\t\t \n\t\tif (status & SSS_FCINTSTAT_HPARTINT)\n\t\t\tst_bits = SSS_HASH_STATUS_PARTIAL_DONE;\n\n\t\tif (status & SSS_FCINTSTAT_HDONEINT)\n\t\t\tst_bits = SSS_HASH_STATUS_MSG_DONE;\n\n\t\tset_bit(HASH_FLAGS_OUTPUT_READY, &dev->hash_flags);\n\t\ts5p_hash_write(dev, SSS_REG_HASH_STATUS, st_bits);\n\t\thx_end = true;\n\t\t \n\t\terr_dma_hx = 0;\n\t}\n\n\tif (err_dma_rx < 0) {\n\t\terr = err_dma_rx;\n\t\tgoto error;\n\t}\n\tif (err_dma_tx < 0) {\n\t\terr = err_dma_tx;\n\t\tgoto error;\n\t}\n\n\tif (tx_end) {\n\t\ts5p_sg_done(dev);\n\t\tif (err_dma_hx == 1)\n\t\t\ts5p_set_dma_hashdata(dev, dev->hash_sg_iter);\n\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\t\ts5p_aes_complete(dev->req, 0);\n\t\t \n\t\ttasklet_schedule(&dev->tasklet);\n\t} else {\n\t\t \n\t\tif (err_dma_tx == 1)\n\t\t\ts5p_set_dma_outdata(dev, dev->sg_dst);\n\t\tif (err_dma_rx == 1)\n\t\t\ts5p_set_dma_indata(dev, dev->sg_src);\n\t\tif (err_dma_hx == 1)\n\t\t\ts5p_set_dma_hashdata(dev, dev->hash_sg_iter);\n\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\t}\n\n\tgoto hash_irq_end;\n\nerror:\n\ts5p_sg_done(dev);\n\tdev->busy = false;\n\treq = dev->req;\n\tif (err_dma_hx == 1)\n\t\ts5p_set_dma_hashdata(dev, dev->hash_sg_iter);\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\ts5p_aes_complete(req, err);\n\nhash_irq_end:\n\t \n\tif (hx_end)\n\t\ttasklet_schedule(&dev->hash_tasklet);\n\telse if (err_dma_hx == 2)\n\t\ts5p_hash_write(dev, SSS_REG_HASH_CTRL_PAUSE,\n\t\t\t       SSS_HASH_PAUSE);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void s5p_hash_read_msg(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct s5p_aes_dev *dd = ctx->dd;\n\tu32 *hash = (u32 *)ctx->digest;\n\tunsigned int i;\n\n\tfor (i = 0; i < ctx->nregs; i++)\n\t\thash[i] = s5p_hash_read(dd, SSS_REG_HASH_OUT(i));\n}\n\n \nstatic void s5p_hash_write_ctx_iv(struct s5p_aes_dev *dd,\n\t\t\t\t  const struct s5p_hash_reqctx *ctx)\n{\n\tconst u32 *hash = (const u32 *)ctx->digest;\n\tunsigned int i;\n\n\tfor (i = 0; i < ctx->nregs; i++)\n\t\ts5p_hash_write(dd, SSS_REG_HASH_IV(i), hash[i]);\n}\n\n \nstatic void s5p_hash_write_iv(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\n\ts5p_hash_write_ctx_iv(ctx->dd, ctx);\n}\n\n \nstatic void s5p_hash_copy_result(struct ahash_request *req)\n{\n\tconst struct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\n\tif (!req->result)\n\t\treturn;\n\n\tmemcpy(req->result, ctx->digest, ctx->nregs * HASH_REG_SIZEOF);\n}\n\n \nstatic void s5p_hash_dma_flush(struct s5p_aes_dev *dev)\n{\n\tSSS_WRITE(dev, FCHRDMAC, SSS_FCHRDMAC_FLUSH);\n}\n\n \nstatic void s5p_hash_dma_enable(struct s5p_aes_dev *dev)\n{\n\ts5p_hash_write(dev, SSS_REG_HASH_CTRL_FIFO, SSS_HASH_FIFO_MODE_DMA);\n}\n\n \nstatic void s5p_hash_irq_disable(struct s5p_aes_dev *dev, u32 flags)\n{\n\tSSS_WRITE(dev, FCINTENCLR, flags);\n}\n\n \nstatic void s5p_hash_irq_enable(struct s5p_aes_dev *dev, int flags)\n{\n\tSSS_WRITE(dev, FCINTENSET, flags);\n}\n\n \nstatic void s5p_hash_set_flow(struct s5p_aes_dev *dev, u32 hashflow)\n{\n\tunsigned long flags;\n\tu32 flow;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tflow = SSS_READ(dev, FCFIFOCTRL);\n\tflow &= ~SSS_HASHIN_MASK;\n\tflow |= hashflow;\n\tSSS_WRITE(dev, FCFIFOCTRL, flow);\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n}\n\n \nstatic void s5p_ahash_dma_init(struct s5p_aes_dev *dev, u32 hashflow)\n{\n\ts5p_hash_irq_disable(dev, SSS_FCINTENCLR_HRDMAINTENCLR |\n\t\t\t     SSS_FCINTENCLR_HDONEINTENCLR |\n\t\t\t     SSS_FCINTENCLR_HPARTINTENCLR);\n\ts5p_hash_dma_flush(dev);\n\n\ts5p_hash_dma_enable(dev);\n\ts5p_hash_set_flow(dev, hashflow & SSS_HASHIN_MASK);\n\ts5p_hash_irq_enable(dev, SSS_FCINTENSET_HRDMAINTENSET |\n\t\t\t    SSS_FCINTENSET_HDONEINTENSET |\n\t\t\t    SSS_FCINTENSET_HPARTINTENSET);\n}\n\n \nstatic void s5p_hash_write_ctrl(struct s5p_aes_dev *dd, size_t length,\n\t\t\t\tbool final)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(dd->hash_req);\n\tu32 prelow, prehigh, low, high;\n\tu32 configflags, swapflags;\n\tu64 tmplen;\n\n\tconfigflags = ctx->engine | SSS_HASH_INIT_BIT;\n\n\tif (likely(ctx->digcnt)) {\n\t\ts5p_hash_write_ctx_iv(dd, ctx);\n\t\tconfigflags |= SSS_HASH_USER_IV_EN;\n\t}\n\n\tif (final) {\n\t\t \n\t\tlow = length;\n\t\thigh = 0;\n\t\t \n\t\ttmplen = ctx->digcnt * 8;\n\t\tprelow = (u32)tmplen;\n\t\tprehigh = (u32)(tmplen >> 32);\n\t} else {\n\t\tprelow = 0;\n\t\tprehigh = 0;\n\t\tlow = 0;\n\t\thigh = BIT(31);\n\t}\n\n\tswapflags = SSS_HASH_BYTESWAP_DI | SSS_HASH_BYTESWAP_DO |\n\t\t    SSS_HASH_BYTESWAP_IV | SSS_HASH_BYTESWAP_KEY;\n\n\ts5p_hash_write(dd, SSS_REG_HASH_MSG_SIZE_LOW, low);\n\ts5p_hash_write(dd, SSS_REG_HASH_MSG_SIZE_HIGH, high);\n\ts5p_hash_write(dd, SSS_REG_HASH_PRE_MSG_SIZE_LOW, prelow);\n\ts5p_hash_write(dd, SSS_REG_HASH_PRE_MSG_SIZE_HIGH, prehigh);\n\n\ts5p_hash_write(dd, SSS_REG_HASH_CTRL_SWAP, swapflags);\n\ts5p_hash_write(dd, SSS_REG_HASH_CTRL, configflags);\n}\n\n \nstatic int s5p_hash_xmit_dma(struct s5p_aes_dev *dd, size_t length,\n\t\t\t     bool final)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(dd->hash_req);\n\tunsigned int cnt;\n\n\tcnt = dma_map_sg(dd->dev, ctx->sg, ctx->sg_len, DMA_TO_DEVICE);\n\tif (!cnt) {\n\t\tdev_err(dd->dev, \"dma_map_sg error\\n\");\n\t\tctx->error = true;\n\t\treturn -EINVAL;\n\t}\n\n\tset_bit(HASH_FLAGS_DMA_ACTIVE, &dd->hash_flags);\n\tdd->hash_sg_iter = ctx->sg;\n\tdd->hash_sg_cnt = cnt;\n\ts5p_hash_write_ctrl(dd, length, final);\n\tctx->digcnt += length;\n\tctx->total -= length;\n\n\t \n\tif (final)\n\t\tset_bit(HASH_FLAGS_FINAL, &dd->hash_flags);\n\n\ts5p_set_dma_hashdata(dd, dd->hash_sg_iter);  \n\n\treturn -EINPROGRESS;\n}\n\n \nstatic int s5p_hash_copy_sgs(struct s5p_hash_reqctx *ctx,\n\t\t\t     struct scatterlist *sg, unsigned int new_len)\n{\n\tunsigned int pages, len;\n\tvoid *buf;\n\n\tlen = new_len + ctx->bufcnt;\n\tpages = get_order(len);\n\n\tbuf = (void *)__get_free_pages(GFP_ATOMIC, pages);\n\tif (!buf) {\n\t\tdev_err(ctx->dd->dev, \"alloc pages for unaligned case.\\n\");\n\t\tctx->error = true;\n\t\treturn -ENOMEM;\n\t}\n\n\tif (ctx->bufcnt)\n\t\tmemcpy(buf, ctx->dd->xmit_buf, ctx->bufcnt);\n\n\tscatterwalk_map_and_copy(buf + ctx->bufcnt, sg, ctx->skip,\n\t\t\t\t new_len, 0);\n\tsg_init_table(ctx->sgl, 1);\n\tsg_set_buf(ctx->sgl, buf, len);\n\tctx->sg = ctx->sgl;\n\tctx->sg_len = 1;\n\tctx->bufcnt = 0;\n\tctx->skip = 0;\n\tset_bit(HASH_FLAGS_SGS_COPIED, &ctx->dd->hash_flags);\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_copy_sg_lists(struct s5p_hash_reqctx *ctx,\n\t\t\t\t  struct scatterlist *sg, unsigned int new_len)\n{\n\tunsigned int skip = ctx->skip, n = sg_nents(sg);\n\tstruct scatterlist *tmp;\n\tunsigned int len;\n\n\tif (ctx->bufcnt)\n\t\tn++;\n\n\tctx->sg = kmalloc_array(n, sizeof(*sg), GFP_KERNEL);\n\tif (!ctx->sg) {\n\t\tctx->error = true;\n\t\treturn -ENOMEM;\n\t}\n\n\tsg_init_table(ctx->sg, n);\n\n\ttmp = ctx->sg;\n\n\tctx->sg_len = 0;\n\n\tif (ctx->bufcnt) {\n\t\tsg_set_buf(tmp, ctx->dd->xmit_buf, ctx->bufcnt);\n\t\ttmp = sg_next(tmp);\n\t\tctx->sg_len++;\n\t}\n\n\twhile (sg && skip >= sg->length) {\n\t\tskip -= sg->length;\n\t\tsg = sg_next(sg);\n\t}\n\n\twhile (sg && new_len) {\n\t\tlen = sg->length - skip;\n\t\tif (new_len < len)\n\t\t\tlen = new_len;\n\n\t\tnew_len -= len;\n\t\tsg_set_page(tmp, sg_page(sg), len, sg->offset + skip);\n\t\tskip = 0;\n\t\tif (new_len <= 0)\n\t\t\tsg_mark_end(tmp);\n\n\t\ttmp = sg_next(tmp);\n\t\tctx->sg_len++;\n\t\tsg = sg_next(sg);\n\t}\n\n\tset_bit(HASH_FLAGS_SGS_ALLOCED, &ctx->dd->hash_flags);\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_prepare_sgs(struct s5p_hash_reqctx *ctx,\n\t\t\t\tstruct scatterlist *sg,\n\t\t\t\tunsigned int new_len, bool final)\n{\n\tunsigned int skip = ctx->skip, nbytes = new_len, n = 0;\n\tbool aligned = true, list_ok = true;\n\tstruct scatterlist *sg_tmp = sg;\n\n\tif (!sg || !sg->length || !new_len)\n\t\treturn 0;\n\n\tif (skip || !final)\n\t\tlist_ok = false;\n\n\twhile (nbytes > 0 && sg_tmp) {\n\t\tn++;\n\t\tif (skip >= sg_tmp->length) {\n\t\t\tskip -= sg_tmp->length;\n\t\t\tif (!sg_tmp->length) {\n\t\t\t\taligned = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!IS_ALIGNED(sg_tmp->length - skip, BUFLEN)) {\n\t\t\t\taligned = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (nbytes < sg_tmp->length - skip) {\n\t\t\t\tlist_ok = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tnbytes -= sg_tmp->length - skip;\n\t\t\tskip = 0;\n\t\t}\n\n\t\tsg_tmp = sg_next(sg_tmp);\n\t}\n\n\tif (!aligned)\n\t\treturn s5p_hash_copy_sgs(ctx, sg, new_len);\n\telse if (!list_ok)\n\t\treturn s5p_hash_copy_sg_lists(ctx, sg, new_len);\n\n\t \n\tif (ctx->bufcnt) {\n\t\tctx->sg_len = n;\n\t\tsg_init_table(ctx->sgl, 2);\n\t\tsg_set_buf(ctx->sgl, ctx->dd->xmit_buf, ctx->bufcnt);\n\t\tsg_chain(ctx->sgl, 2, sg);\n\t\tctx->sg = ctx->sgl;\n\t\tctx->sg_len++;\n\t} else {\n\t\tctx->sg = sg;\n\t\tctx->sg_len = n;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_prepare_request(struct ahash_request *req, bool update)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tbool final = ctx->finup;\n\tint xmit_len, hash_later, nbytes;\n\tint ret;\n\n\tif (update)\n\t\tnbytes = req->nbytes;\n\telse\n\t\tnbytes = 0;\n\n\tctx->total = nbytes + ctx->bufcnt;\n\tif (!ctx->total)\n\t\treturn 0;\n\n\tif (nbytes && (!IS_ALIGNED(ctx->bufcnt, BUFLEN))) {\n\t\t \n\t\tint len = BUFLEN - ctx->bufcnt % BUFLEN;\n\n\t\tif (len > nbytes)\n\t\t\tlen = nbytes;\n\n\t\tscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, req->src,\n\t\t\t\t\t 0, len, 0);\n\t\tctx->bufcnt += len;\n\t\tnbytes -= len;\n\t\tctx->skip = len;\n\t} else {\n\t\tctx->skip = 0;\n\t}\n\n\tif (ctx->bufcnt)\n\t\tmemcpy(ctx->dd->xmit_buf, ctx->buffer, ctx->bufcnt);\n\n\txmit_len = ctx->total;\n\tif (final) {\n\t\thash_later = 0;\n\t} else {\n\t\tif (IS_ALIGNED(xmit_len, BUFLEN))\n\t\t\txmit_len -= BUFLEN;\n\t\telse\n\t\t\txmit_len -= xmit_len & (BUFLEN - 1);\n\n\t\thash_later = ctx->total - xmit_len;\n\t\t \n\t\t \n\t\tscatterwalk_map_and_copy(ctx->buffer, req->src,\n\t\t\t\t\t req->nbytes - hash_later,\n\t\t\t\t\t hash_later, 0);\n\t}\n\n\tif (xmit_len > BUFLEN) {\n\t\tret = s5p_hash_prepare_sgs(ctx, req->src, nbytes - hash_later,\n\t\t\t\t\t   final);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\t \n\t\tif (unlikely(!ctx->bufcnt)) {\n\t\t\t \n\t\t\tscatterwalk_map_and_copy(ctx->dd->xmit_buf, req->src,\n\t\t\t\t\t\t 0, xmit_len, 0);\n\t\t}\n\n\t\tsg_init_table(ctx->sgl, 1);\n\t\tsg_set_buf(ctx->sgl, ctx->dd->xmit_buf, xmit_len);\n\n\t\tctx->sg = ctx->sgl;\n\t\tctx->sg_len = 1;\n\t}\n\n\tctx->bufcnt = hash_later;\n\tif (!final)\n\t\tctx->total = xmit_len;\n\n\treturn 0;\n}\n\n \nstatic void s5p_hash_update_dma_stop(struct s5p_aes_dev *dd)\n{\n\tconst struct s5p_hash_reqctx *ctx = ahash_request_ctx(dd->hash_req);\n\n\tdma_unmap_sg(dd->dev, ctx->sg, ctx->sg_len, DMA_TO_DEVICE);\n\tclear_bit(HASH_FLAGS_DMA_ACTIVE, &dd->hash_flags);\n}\n\n \nstatic void s5p_hash_finish(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct s5p_aes_dev *dd = ctx->dd;\n\n\tif (ctx->digcnt)\n\t\ts5p_hash_copy_result(req);\n\n\tdev_dbg(dd->dev, \"hash_finish digcnt: %lld\\n\", ctx->digcnt);\n}\n\n \nstatic void s5p_hash_finish_req(struct ahash_request *req, int err)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct s5p_aes_dev *dd = ctx->dd;\n\tunsigned long flags;\n\n\tif (test_bit(HASH_FLAGS_SGS_COPIED, &dd->hash_flags))\n\t\tfree_pages((unsigned long)sg_virt(ctx->sg),\n\t\t\t   get_order(ctx->sg->length));\n\n\tif (test_bit(HASH_FLAGS_SGS_ALLOCED, &dd->hash_flags))\n\t\tkfree(ctx->sg);\n\n\tctx->sg = NULL;\n\tdd->hash_flags &= ~(BIT(HASH_FLAGS_SGS_ALLOCED) |\n\t\t\t    BIT(HASH_FLAGS_SGS_COPIED));\n\n\tif (!err && !ctx->error) {\n\t\ts5p_hash_read_msg(req);\n\t\tif (test_bit(HASH_FLAGS_FINAL, &dd->hash_flags))\n\t\t\ts5p_hash_finish(req);\n\t} else {\n\t\tctx->error = true;\n\t}\n\n\tspin_lock_irqsave(&dd->hash_lock, flags);\n\tdd->hash_flags &= ~(BIT(HASH_FLAGS_BUSY) | BIT(HASH_FLAGS_FINAL) |\n\t\t\t    BIT(HASH_FLAGS_DMA_READY) |\n\t\t\t    BIT(HASH_FLAGS_OUTPUT_READY));\n\tspin_unlock_irqrestore(&dd->hash_lock, flags);\n\n\tif (req->base.complete)\n\t\tahash_request_complete(req, err);\n}\n\n \nstatic int s5p_hash_handle_queue(struct s5p_aes_dev *dd,\n\t\t\t\t struct ahash_request *req)\n{\n\tstruct crypto_async_request *async_req, *backlog;\n\tstruct s5p_hash_reqctx *ctx;\n\tunsigned long flags;\n\tint err = 0, ret = 0;\n\nretry:\n\tspin_lock_irqsave(&dd->hash_lock, flags);\n\tif (req)\n\t\tret = ahash_enqueue_request(&dd->hash_queue, req);\n\n\tif (test_bit(HASH_FLAGS_BUSY, &dd->hash_flags)) {\n\t\tspin_unlock_irqrestore(&dd->hash_lock, flags);\n\t\treturn ret;\n\t}\n\n\tbacklog = crypto_get_backlog(&dd->hash_queue);\n\tasync_req = crypto_dequeue_request(&dd->hash_queue);\n\tif (async_req)\n\t\tset_bit(HASH_FLAGS_BUSY, &dd->hash_flags);\n\n\tspin_unlock_irqrestore(&dd->hash_lock, flags);\n\n\tif (!async_req)\n\t\treturn ret;\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\treq = ahash_request_cast(async_req);\n\tdd->hash_req = req;\n\tctx = ahash_request_ctx(req);\n\n\terr = s5p_hash_prepare_request(req, ctx->op_update);\n\tif (err || !ctx->total)\n\t\tgoto out;\n\n\tdev_dbg(dd->dev, \"handling new req, op_update: %u, nbytes: %d\\n\",\n\t\tctx->op_update, req->nbytes);\n\n\ts5p_ahash_dma_init(dd, SSS_HASHIN_INDEPENDENT);\n\tif (ctx->digcnt)\n\t\ts5p_hash_write_iv(req);  \n\n\tif (ctx->op_update) {  \n\t\terr = s5p_hash_xmit_dma(dd, ctx->total, ctx->finup);\n\t\tif (err != -EINPROGRESS && ctx->finup && !ctx->error)\n\t\t\t \n\t\t\terr = s5p_hash_xmit_dma(dd, ctx->total, true);\n\t} else {  \n\t\terr = s5p_hash_xmit_dma(dd, ctx->total, true);\n\t}\nout:\n\tif (err != -EINPROGRESS) {\n\t\t \n\t\ts5p_hash_finish_req(req, err);\n\t\treq = NULL;\n\n\t\t \n\t\tgoto retry;\n\t}\n\n\treturn ret;\n}\n\n \nstatic void s5p_hash_tasklet_cb(unsigned long data)\n{\n\tstruct s5p_aes_dev *dd = (struct s5p_aes_dev *)data;\n\n\tif (!test_bit(HASH_FLAGS_BUSY, &dd->hash_flags)) {\n\t\ts5p_hash_handle_queue(dd, NULL);\n\t\treturn;\n\t}\n\n\tif (test_bit(HASH_FLAGS_DMA_READY, &dd->hash_flags)) {\n\t\tif (test_and_clear_bit(HASH_FLAGS_DMA_ACTIVE,\n\t\t\t\t       &dd->hash_flags)) {\n\t\t\ts5p_hash_update_dma_stop(dd);\n\t\t}\n\n\t\tif (test_and_clear_bit(HASH_FLAGS_OUTPUT_READY,\n\t\t\t\t       &dd->hash_flags)) {\n\t\t\t \n\t\t\tclear_bit(HASH_FLAGS_DMA_READY, &dd->hash_flags);\n\t\t\tgoto finish;\n\t\t}\n\t}\n\n\treturn;\n\nfinish:\n\t \n\ts5p_hash_finish_req(dd->hash_req, 0);\n\n\t \n\tif (!test_bit(HASH_FLAGS_BUSY, &dd->hash_flags))\n\t\ts5p_hash_handle_queue(dd, NULL);\n}\n\n \nstatic int s5p_hash_enqueue(struct ahash_request *req, bool op)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct s5p_hash_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\n\n\tctx->op_update = op;\n\n\treturn s5p_hash_handle_queue(tctx->dd, req);\n}\n\n \nstatic int s5p_hash_update(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\n\tif (!req->nbytes)\n\t\treturn 0;\n\n\tif (ctx->bufcnt + req->nbytes <= BUFLEN) {\n\t\tscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, req->src,\n\t\t\t\t\t 0, req->nbytes, 0);\n\t\tctx->bufcnt += req->nbytes;\n\t\treturn 0;\n\t}\n\n\treturn s5p_hash_enqueue(req, true);  \n}\n\n \nstatic int s5p_hash_final(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\n\tctx->finup = true;\n\tif (ctx->error)\n\t\treturn -EINVAL;  \n\n\tif (!ctx->digcnt && ctx->bufcnt < BUFLEN) {\n\t\tstruct s5p_hash_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\n\n\t\treturn crypto_shash_tfm_digest(tctx->fallback, ctx->buffer,\n\t\t\t\t\t       ctx->bufcnt, req->result);\n\t}\n\n\treturn s5p_hash_enqueue(req, false);  \n}\n\n \nstatic int s5p_hash_finup(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tint err1, err2;\n\n\tctx->finup = true;\n\n\terr1 = s5p_hash_update(req);\n\tif (err1 == -EINPROGRESS || err1 == -EBUSY)\n\t\treturn err1;\n\n\t \n\terr2 = s5p_hash_final(req);\n\n\treturn err1 ?: err2;\n}\n\n \nstatic int s5p_hash_init(struct ahash_request *req)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct s5p_hash_ctx *tctx = crypto_ahash_ctx(tfm);\n\n\tctx->dd = tctx->dd;\n\tctx->error = false;\n\tctx->finup = false;\n\tctx->bufcnt = 0;\n\tctx->digcnt = 0;\n\tctx->total = 0;\n\tctx->skip = 0;\n\n\tdev_dbg(tctx->dd->dev, \"init: digest size: %d\\n\",\n\t\tcrypto_ahash_digestsize(tfm));\n\n\tswitch (crypto_ahash_digestsize(tfm)) {\n\tcase MD5_DIGEST_SIZE:\n\t\tctx->engine = SSS_HASH_ENGINE_MD5;\n\t\tctx->nregs = HASH_MD5_MAX_REG;\n\t\tbreak;\n\tcase SHA1_DIGEST_SIZE:\n\t\tctx->engine = SSS_HASH_ENGINE_SHA1;\n\t\tctx->nregs = HASH_SHA1_MAX_REG;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tctx->engine = SSS_HASH_ENGINE_SHA256;\n\t\tctx->nregs = HASH_SHA256_MAX_REG;\n\t\tbreak;\n\tdefault:\n\t\tctx->error = true;\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_digest(struct ahash_request *req)\n{\n\treturn s5p_hash_init(req) ?: s5p_hash_finup(req);\n}\n\n \nstatic int s5p_hash_cra_init_alg(struct crypto_tfm *tfm)\n{\n\tstruct s5p_hash_ctx *tctx = crypto_tfm_ctx(tfm);\n\tconst char *alg_name = crypto_tfm_alg_name(tfm);\n\n\ttctx->dd = s5p_dev;\n\t \n\ttctx->fallback = crypto_alloc_shash(alg_name, 0,\n\t\t\t\t\t    CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(tctx->fallback)) {\n\t\tpr_err(\"fallback alloc fails for '%s'\\n\", alg_name);\n\t\treturn PTR_ERR(tctx->fallback);\n\t}\n\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct s5p_hash_reqctx) + BUFLEN);\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_cra_init(struct crypto_tfm *tfm)\n{\n\treturn s5p_hash_cra_init_alg(tfm);\n}\n\n \nstatic void s5p_hash_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct s5p_hash_ctx *tctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(tctx->fallback);\n\ttctx->fallback = NULL;\n}\n\n \nstatic int s5p_hash_export(struct ahash_request *req, void *out)\n{\n\tconst struct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\n\tmemcpy(out, ctx, sizeof(*ctx) + ctx->bufcnt);\n\n\treturn 0;\n}\n\n \nstatic int s5p_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct s5p_hash_reqctx *ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct s5p_hash_ctx *tctx = crypto_ahash_ctx(tfm);\n\tconst struct s5p_hash_reqctx *ctx_in = in;\n\n\tmemcpy(ctx, in, sizeof(*ctx) + BUFLEN);\n\tif (ctx_in->bufcnt > BUFLEN) {\n\t\tctx->error = true;\n\t\treturn -EINVAL;\n\t}\n\n\tctx->dd = tctx->dd;\n\tctx->error = false;\n\n\treturn 0;\n}\n\nstatic struct ahash_alg algs_sha1_md5_sha256[] = {\n{\n\t.init\t\t= s5p_hash_init,\n\t.update\t\t= s5p_hash_update,\n\t.final\t\t= s5p_hash_final,\n\t.finup\t\t= s5p_hash_finup,\n\t.digest\t\t= s5p_hash_digest,\n\t.export\t\t= s5p_hash_export,\n\t.import\t\t= s5p_hash_import,\n\t.halg.statesize = sizeof(struct s5p_hash_reqctx) + BUFLEN,\n\t.halg.digestsize\t= SHA1_DIGEST_SIZE,\n\t.halg.base\t= {\n\t\t.cra_name\t\t= \"sha1\",\n\t\t.cra_driver_name\t= \"exynos-sha1\",\n\t\t.cra_priority\t\t= 100,\n\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t= HASH_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct s5p_hash_ctx),\n\t\t.cra_alignmask\t\t= SSS_HASH_DMA_ALIGN_MASK,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t\t.cra_init\t\t= s5p_hash_cra_init,\n\t\t.cra_exit\t\t= s5p_hash_cra_exit,\n\t}\n},\n{\n\t.init\t\t= s5p_hash_init,\n\t.update\t\t= s5p_hash_update,\n\t.final\t\t= s5p_hash_final,\n\t.finup\t\t= s5p_hash_finup,\n\t.digest\t\t= s5p_hash_digest,\n\t.export\t\t= s5p_hash_export,\n\t.import\t\t= s5p_hash_import,\n\t.halg.statesize = sizeof(struct s5p_hash_reqctx) + BUFLEN,\n\t.halg.digestsize\t= MD5_DIGEST_SIZE,\n\t.halg.base\t= {\n\t\t.cra_name\t\t= \"md5\",\n\t\t.cra_driver_name\t= \"exynos-md5\",\n\t\t.cra_priority\t\t= 100,\n\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t= HASH_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct s5p_hash_ctx),\n\t\t.cra_alignmask\t\t= SSS_HASH_DMA_ALIGN_MASK,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t\t.cra_init\t\t= s5p_hash_cra_init,\n\t\t.cra_exit\t\t= s5p_hash_cra_exit,\n\t}\n},\n{\n\t.init\t\t= s5p_hash_init,\n\t.update\t\t= s5p_hash_update,\n\t.final\t\t= s5p_hash_final,\n\t.finup\t\t= s5p_hash_finup,\n\t.digest\t\t= s5p_hash_digest,\n\t.export\t\t= s5p_hash_export,\n\t.import\t\t= s5p_hash_import,\n\t.halg.statesize = sizeof(struct s5p_hash_reqctx) + BUFLEN,\n\t.halg.digestsize\t= SHA256_DIGEST_SIZE,\n\t.halg.base\t= {\n\t\t.cra_name\t\t= \"sha256\",\n\t\t.cra_driver_name\t= \"exynos-sha256\",\n\t\t.cra_priority\t\t= 100,\n\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize\t\t= HASH_BLOCK_SIZE,\n\t\t.cra_ctxsize\t\t= sizeof(struct s5p_hash_ctx),\n\t\t.cra_alignmask\t\t= SSS_HASH_DMA_ALIGN_MASK,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t\t.cra_init\t\t= s5p_hash_cra_init,\n\t\t.cra_exit\t\t= s5p_hash_cra_exit,\n\t}\n}\n\n};\n\nstatic void s5p_set_aes(struct s5p_aes_dev *dev,\n\t\t\tconst u8 *key, const u8 *iv, const u8 *ctr,\n\t\t\tunsigned int keylen)\n{\n\tvoid __iomem *keystart;\n\n\tif (iv)\n\t\tmemcpy_toio(dev->aes_ioaddr + SSS_REG_AES_IV_DATA(0), iv,\n\t\t\t    AES_BLOCK_SIZE);\n\n\tif (ctr)\n\t\tmemcpy_toio(dev->aes_ioaddr + SSS_REG_AES_CNT_DATA(0), ctr,\n\t\t\t    AES_BLOCK_SIZE);\n\n\tif (keylen == AES_KEYSIZE_256)\n\t\tkeystart = dev->aes_ioaddr + SSS_REG_AES_KEY_DATA(0);\n\telse if (keylen == AES_KEYSIZE_192)\n\t\tkeystart = dev->aes_ioaddr + SSS_REG_AES_KEY_DATA(2);\n\telse\n\t\tkeystart = dev->aes_ioaddr + SSS_REG_AES_KEY_DATA(4);\n\n\tmemcpy_toio(keystart, key, keylen);\n}\n\nstatic bool s5p_is_sg_aligned(struct scatterlist *sg)\n{\n\twhile (sg) {\n\t\tif (!IS_ALIGNED(sg->length, AES_BLOCK_SIZE))\n\t\t\treturn false;\n\t\tsg = sg_next(sg);\n\t}\n\n\treturn true;\n}\n\nstatic int s5p_set_indata_start(struct s5p_aes_dev *dev,\n\t\t\t\tstruct skcipher_request *req)\n{\n\tstruct scatterlist *sg;\n\tint err;\n\n\tdev->sg_src_cpy = NULL;\n\tsg = req->src;\n\tif (!s5p_is_sg_aligned(sg)) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"At least one unaligned source scatter list, making a copy\\n\");\n\t\terr = s5p_make_sg_cpy(dev, sg, &dev->sg_src_cpy);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tsg = dev->sg_src_cpy;\n\t}\n\n\terr = s5p_set_indata(dev, sg);\n\tif (err) {\n\t\ts5p_free_sg_cpy(dev, &dev->sg_src_cpy);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int s5p_set_outdata_start(struct s5p_aes_dev *dev,\n\t\t\t\t struct skcipher_request *req)\n{\n\tstruct scatterlist *sg;\n\tint err;\n\n\tdev->sg_dst_cpy = NULL;\n\tsg = req->dst;\n\tif (!s5p_is_sg_aligned(sg)) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"At least one unaligned dest scatter list, making a copy\\n\");\n\t\terr = s5p_make_sg_cpy(dev, sg, &dev->sg_dst_cpy);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tsg = dev->sg_dst_cpy;\n\t}\n\n\terr = s5p_set_outdata(dev, sg);\n\tif (err) {\n\t\ts5p_free_sg_cpy(dev, &dev->sg_dst_cpy);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void s5p_aes_crypt_start(struct s5p_aes_dev *dev, unsigned long mode)\n{\n\tstruct skcipher_request *req = dev->req;\n\tu32 aes_control;\n\tunsigned long flags;\n\tint err;\n\tu8 *iv, *ctr;\n\n\t \n\taes_control = SSS_AES_KEY_CHANGE_MODE;\n\tif (mode & FLAGS_AES_DECRYPT)\n\t\taes_control |= SSS_AES_MODE_DECRYPT;\n\n\tif ((mode & FLAGS_AES_MODE_MASK) == FLAGS_AES_CBC) {\n\t\taes_control |= SSS_AES_CHAIN_MODE_CBC;\n\t\tiv = req->iv;\n\t\tctr = NULL;\n\t} else if ((mode & FLAGS_AES_MODE_MASK) == FLAGS_AES_CTR) {\n\t\taes_control |= SSS_AES_CHAIN_MODE_CTR;\n\t\tiv = NULL;\n\t\tctr = req->iv;\n\t} else {\n\t\tiv = NULL;  \n\t\tctr = NULL;\n\t}\n\n\tif (dev->ctx->keylen == AES_KEYSIZE_192)\n\t\taes_control |= SSS_AES_KEY_SIZE_192;\n\telse if (dev->ctx->keylen == AES_KEYSIZE_256)\n\t\taes_control |= SSS_AES_KEY_SIZE_256;\n\n\taes_control |= SSS_AES_FIFO_MODE;\n\n\t \n\taes_control |= SSS_AES_BYTESWAP_DI\n\t\t    |  SSS_AES_BYTESWAP_DO\n\t\t    |  SSS_AES_BYTESWAP_IV\n\t\t    |  SSS_AES_BYTESWAP_KEY\n\t\t    |  SSS_AES_BYTESWAP_CNT;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tSSS_WRITE(dev, FCINTENCLR,\n\t\t  SSS_FCINTENCLR_BTDMAINTENCLR | SSS_FCINTENCLR_BRDMAINTENCLR);\n\tSSS_WRITE(dev, FCFIFOCTRL, 0x00);\n\n\terr = s5p_set_indata_start(dev, req);\n\tif (err)\n\t\tgoto indata_error;\n\n\terr = s5p_set_outdata_start(dev, req);\n\tif (err)\n\t\tgoto outdata_error;\n\n\tSSS_AES_WRITE(dev, AES_CONTROL, aes_control);\n\ts5p_set_aes(dev, dev->ctx->aes_key, iv, ctr, dev->ctx->keylen);\n\n\ts5p_set_dma_indata(dev,  dev->sg_src);\n\ts5p_set_dma_outdata(dev, dev->sg_dst);\n\n\tSSS_WRITE(dev, FCINTENSET,\n\t\t  SSS_FCINTENSET_BTDMAINTENSET | SSS_FCINTENSET_BRDMAINTENSET);\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\treturn;\n\noutdata_error:\n\ts5p_unset_indata(dev);\n\nindata_error:\n\ts5p_sg_done(dev);\n\tdev->busy = false;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\ts5p_aes_complete(req, err);\n}\n\nstatic void s5p_tasklet_cb(unsigned long data)\n{\n\tstruct s5p_aes_dev *dev = (struct s5p_aes_dev *)data;\n\tstruct crypto_async_request *async_req, *backlog;\n\tstruct s5p_aes_reqctx *reqctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tbacklog   = crypto_get_backlog(&dev->queue);\n\tasync_req = crypto_dequeue_request(&dev->queue);\n\n\tif (!async_req) {\n\t\tdev->busy = false;\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\tdev->req = skcipher_request_cast(async_req);\n\tdev->ctx = crypto_tfm_ctx(dev->req->base.tfm);\n\treqctx   = skcipher_request_ctx(dev->req);\n\n\ts5p_aes_crypt_start(dev, reqctx->mode);\n}\n\nstatic int s5p_aes_handle_req(struct s5p_aes_dev *dev,\n\t\t\t      struct skcipher_request *req)\n{\n\tunsigned long flags;\n\tint err;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\terr = crypto_enqueue_request(&dev->queue, &req->base);\n\tif (dev->busy) {\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\t\treturn err;\n\t}\n\tdev->busy = true;\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\ttasklet_schedule(&dev->tasklet);\n\n\treturn err;\n}\n\nstatic int s5p_aes_crypt(struct skcipher_request *req, unsigned long mode)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct s5p_aes_reqctx *reqctx = skcipher_request_ctx(req);\n\tstruct s5p_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct s5p_aes_dev *dev = ctx->dev;\n\n\tif (!req->cryptlen)\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(req->cryptlen, AES_BLOCK_SIZE) &&\n\t\t\t((mode & FLAGS_AES_MODE_MASK) != FLAGS_AES_CTR)) {\n\t\tdev_dbg(dev->dev, \"request size is not exact amount of AES blocks\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treqctx->mode = mode;\n\n\treturn s5p_aes_handle_req(dev, req);\n}\n\nstatic int s5p_aes_setkey(struct crypto_skcipher *cipher,\n\t\t\t  const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(cipher);\n\tstruct s5p_aes_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tif (keylen != AES_KEYSIZE_128 &&\n\t    keylen != AES_KEYSIZE_192 &&\n\t    keylen != AES_KEYSIZE_256)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->aes_key, key, keylen);\n\tctx->keylen = keylen;\n\n\treturn 0;\n}\n\nstatic int s5p_aes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn s5p_aes_crypt(req, 0);\n}\n\nstatic int s5p_aes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn s5p_aes_crypt(req, FLAGS_AES_DECRYPT);\n}\n\nstatic int s5p_aes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn s5p_aes_crypt(req, FLAGS_AES_CBC);\n}\n\nstatic int s5p_aes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn s5p_aes_crypt(req, FLAGS_AES_DECRYPT | FLAGS_AES_CBC);\n}\n\nstatic int s5p_aes_ctr_crypt(struct skcipher_request *req)\n{\n\treturn s5p_aes_crypt(req, FLAGS_AES_CTR);\n}\n\nstatic int s5p_aes_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct s5p_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tctx->dev = s5p_dev;\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct s5p_aes_reqctx));\n\n\treturn 0;\n}\n\nstatic struct skcipher_alg algs[] = {\n\t{\n\t\t.base.cra_name\t\t= \"ecb(aes)\",\n\t\t.base.cra_driver_name\t= \"ecb-aes-s5p\",\n\t\t.base.cra_priority\t= 100,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t\t.base.cra_ctxsize\t= sizeof(struct s5p_aes_ctx),\n\t\t.base.cra_alignmask\t= 0x0f,\n\t\t.base.cra_module\t= THIS_MODULE,\n\n\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t.setkey\t\t\t= s5p_aes_setkey,\n\t\t.encrypt\t\t= s5p_aes_ecb_encrypt,\n\t\t.decrypt\t\t= s5p_aes_ecb_decrypt,\n\t\t.init\t\t\t= s5p_aes_init_tfm,\n\t},\n\t{\n\t\t.base.cra_name\t\t= \"cbc(aes)\",\n\t\t.base.cra_driver_name\t= \"cbc-aes-s5p\",\n\t\t.base.cra_priority\t= 100,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t\t.base.cra_ctxsize\t= sizeof(struct s5p_aes_ctx),\n\t\t.base.cra_alignmask\t= 0x0f,\n\t\t.base.cra_module\t= THIS_MODULE,\n\n\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t.setkey\t\t\t= s5p_aes_setkey,\n\t\t.encrypt\t\t= s5p_aes_cbc_encrypt,\n\t\t.decrypt\t\t= s5p_aes_cbc_decrypt,\n\t\t.init\t\t\t= s5p_aes_init_tfm,\n\t},\n\t{\n\t\t.base.cra_name\t\t= \"ctr(aes)\",\n\t\t.base.cra_driver_name\t= \"ctr-aes-s5p\",\n\t\t.base.cra_priority\t= 100,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_KERN_DRIVER_ONLY,\n\t\t.base.cra_blocksize\t= 1,\n\t\t.base.cra_ctxsize\t= sizeof(struct s5p_aes_ctx),\n\t\t.base.cra_alignmask\t= 0x0f,\n\t\t.base.cra_module\t= THIS_MODULE,\n\n\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t.setkey\t\t\t= s5p_aes_setkey,\n\t\t.encrypt\t\t= s5p_aes_ctr_crypt,\n\t\t.decrypt\t\t= s5p_aes_ctr_crypt,\n\t\t.init\t\t\t= s5p_aes_init_tfm,\n\t},\n};\n\nstatic int s5p_aes_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tint i, j, err;\n\tconst struct samsung_aes_variant *variant;\n\tstruct s5p_aes_dev *pdata;\n\tstruct resource *res;\n\tunsigned int hash_i;\n\n\tif (s5p_dev)\n\t\treturn -EEXIST;\n\n\tpdata = devm_kzalloc(dev, sizeof(*pdata), GFP_KERNEL);\n\tif (!pdata)\n\t\treturn -ENOMEM;\n\n\tvariant = find_s5p_sss_version(pdev);\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res)\n\t\treturn -EINVAL;\n\n\t \n\tif (IS_ENABLED(CONFIG_CRYPTO_DEV_EXYNOS_HASH)) {\n\t\tif (variant == &exynos_aes_data) {\n\t\t\tres->end += 0x300;\n\t\t\tpdata->use_hash = true;\n\t\t}\n\t}\n\n\tpdata->res = res;\n\tpdata->ioaddr = devm_ioremap_resource(dev, res);\n\tif (IS_ERR(pdata->ioaddr)) {\n\t\tif (!pdata->use_hash)\n\t\t\treturn PTR_ERR(pdata->ioaddr);\n\t\t \n\t\tres->end -= 0x300;\n\t\tpdata->use_hash = false;\n\t\tpdata->ioaddr = devm_ioremap_resource(dev, res);\n\t\tif (IS_ERR(pdata->ioaddr))\n\t\t\treturn PTR_ERR(pdata->ioaddr);\n\t}\n\n\tpdata->clk = devm_clk_get(dev, variant->clk_names[0]);\n\tif (IS_ERR(pdata->clk))\n\t\treturn dev_err_probe(dev, PTR_ERR(pdata->clk),\n\t\t\t\t     \"failed to find secss clock %s\\n\",\n\t\t\t\t     variant->clk_names[0]);\n\n\terr = clk_prepare_enable(pdata->clk);\n\tif (err < 0) {\n\t\tdev_err(dev, \"Enabling clock %s failed, err %d\\n\",\n\t\t\tvariant->clk_names[0], err);\n\t\treturn err;\n\t}\n\n\tif (variant->clk_names[1]) {\n\t\tpdata->pclk = devm_clk_get(dev, variant->clk_names[1]);\n\t\tif (IS_ERR(pdata->pclk)) {\n\t\t\terr = dev_err_probe(dev, PTR_ERR(pdata->pclk),\n\t\t\t\t\t    \"failed to find clock %s\\n\",\n\t\t\t\t\t    variant->clk_names[1]);\n\t\t\tgoto err_clk;\n\t\t}\n\n\t\terr = clk_prepare_enable(pdata->pclk);\n\t\tif (err < 0) {\n\t\t\tdev_err(dev, \"Enabling clock %s failed, err %d\\n\",\n\t\t\t\tvariant->clk_names[0], err);\n\t\t\tgoto err_clk;\n\t\t}\n\t} else {\n\t\tpdata->pclk = NULL;\n\t}\n\n\tspin_lock_init(&pdata->lock);\n\tspin_lock_init(&pdata->hash_lock);\n\n\tpdata->aes_ioaddr = pdata->ioaddr + variant->aes_offset;\n\tpdata->io_hash_base = pdata->ioaddr + variant->hash_offset;\n\n\tpdata->irq_fc = platform_get_irq(pdev, 0);\n\tif (pdata->irq_fc < 0) {\n\t\terr = pdata->irq_fc;\n\t\tdev_warn(dev, \"feed control interrupt is not available.\\n\");\n\t\tgoto err_irq;\n\t}\n\terr = devm_request_threaded_irq(dev, pdata->irq_fc, NULL,\n\t\t\t\t\ts5p_aes_interrupt, IRQF_ONESHOT,\n\t\t\t\t\tpdev->name, pdev);\n\tif (err < 0) {\n\t\tdev_warn(dev, \"feed control interrupt is not available.\\n\");\n\t\tgoto err_irq;\n\t}\n\n\tpdata->busy = false;\n\tpdata->dev = dev;\n\tplatform_set_drvdata(pdev, pdata);\n\ts5p_dev = pdata;\n\n\ttasklet_init(&pdata->tasklet, s5p_tasklet_cb, (unsigned long)pdata);\n\tcrypto_init_queue(&pdata->queue, CRYPTO_QUEUE_LEN);\n\n\tfor (i = 0; i < ARRAY_SIZE(algs); i++) {\n\t\terr = crypto_register_skcipher(&algs[i]);\n\t\tif (err)\n\t\t\tgoto err_algs;\n\t}\n\n\tif (pdata->use_hash) {\n\t\ttasklet_init(&pdata->hash_tasklet, s5p_hash_tasklet_cb,\n\t\t\t     (unsigned long)pdata);\n\t\tcrypto_init_queue(&pdata->hash_queue, SSS_HASH_QUEUE_LENGTH);\n\n\t\tfor (hash_i = 0; hash_i < ARRAY_SIZE(algs_sha1_md5_sha256);\n\t\t     hash_i++) {\n\t\t\tstruct ahash_alg *alg;\n\n\t\t\talg = &algs_sha1_md5_sha256[hash_i];\n\t\t\terr = crypto_register_ahash(alg);\n\t\t\tif (err) {\n\t\t\t\tdev_err(dev, \"can't register '%s': %d\\n\",\n\t\t\t\t\talg->halg.base.cra_driver_name, err);\n\t\t\t\tgoto err_hash;\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_info(dev, \"s5p-sss driver registered\\n\");\n\n\treturn 0;\n\nerr_hash:\n\tfor (j = hash_i - 1; j >= 0; j--)\n\t\tcrypto_unregister_ahash(&algs_sha1_md5_sha256[j]);\n\n\ttasklet_kill(&pdata->hash_tasklet);\n\tres->end -= 0x300;\n\nerr_algs:\n\tif (i < ARRAY_SIZE(algs))\n\t\tdev_err(dev, \"can't register '%s': %d\\n\", algs[i].base.cra_name,\n\t\t\terr);\n\n\tfor (j = 0; j < i; j++)\n\t\tcrypto_unregister_skcipher(&algs[j]);\n\n\ttasklet_kill(&pdata->tasklet);\n\nerr_irq:\n\tclk_disable_unprepare(pdata->pclk);\n\nerr_clk:\n\tclk_disable_unprepare(pdata->clk);\n\ts5p_dev = NULL;\n\n\treturn err;\n}\n\nstatic int s5p_aes_remove(struct platform_device *pdev)\n{\n\tstruct s5p_aes_dev *pdata = platform_get_drvdata(pdev);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(algs); i++)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n\n\ttasklet_kill(&pdata->tasklet);\n\tif (pdata->use_hash) {\n\t\tfor (i = ARRAY_SIZE(algs_sha1_md5_sha256) - 1; i >= 0; i--)\n\t\t\tcrypto_unregister_ahash(&algs_sha1_md5_sha256[i]);\n\n\t\tpdata->res->end -= 0x300;\n\t\ttasklet_kill(&pdata->hash_tasklet);\n\t\tpdata->use_hash = false;\n\t}\n\n\tclk_disable_unprepare(pdata->pclk);\n\n\tclk_disable_unprepare(pdata->clk);\n\ts5p_dev = NULL;\n\n\treturn 0;\n}\n\nstatic struct platform_driver s5p_aes_crypto = {\n\t.probe\t= s5p_aes_probe,\n\t.remove\t= s5p_aes_remove,\n\t.driver\t= {\n\t\t.name\t= \"s5p-secss\",\n\t\t.of_match_table = s5p_sss_dt_match,\n\t},\n};\n\nmodule_platform_driver(s5p_aes_crypto);\n\nMODULE_DESCRIPTION(\"S5PV210 AES hw acceleration support.\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Vladimir Zapolskiy <vzapolskiy@gmail.com>\");\nMODULE_AUTHOR(\"Kamil Konieczny <k.konieczny@partner.samsung.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}