{
  "module_name": "cptvf_main.c",
  "hash_id": "e0286a903a3942cfeb49cc3645b4e810a027297e48f07905ec9e712e6b4535ed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/cavium/cpt/cptvf_main.c",
  "human_readable_source": "\n \n\n#include <linux/interrupt.h>\n#include <linux/module.h>\n\n#include \"cptvf.h\"\n\n#define DRV_NAME\t\"thunder-cptvf\"\n#define DRV_VERSION\t\"1.0\"\n\nstruct cptvf_wqe {\n\tstruct tasklet_struct twork;\n\tvoid *cptvf;\n\tu32 qno;\n};\n\nstruct cptvf_wqe_info {\n\tstruct cptvf_wqe vq_wqe[CPT_NUM_QS_PER_VF];\n};\n\nstatic void vq_work_handler(unsigned long data)\n{\n\tstruct cptvf_wqe_info *cwqe_info = (struct cptvf_wqe_info *)data;\n\tstruct cptvf_wqe *cwqe = &cwqe_info->vq_wqe[0];\n\n\tvq_post_process(cwqe->cptvf, cwqe->qno);\n}\n\nstatic int init_worker_threads(struct cpt_vf *cptvf)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tstruct cptvf_wqe_info *cwqe_info;\n\tint i;\n\n\tcwqe_info = kzalloc(sizeof(*cwqe_info), GFP_KERNEL);\n\tif (!cwqe_info)\n\t\treturn -ENOMEM;\n\n\tif (cptvf->nr_queues) {\n\t\tdev_info(&pdev->dev, \"Creating VQ worker threads (%d)\\n\",\n\t\t\t cptvf->nr_queues);\n\t}\n\n\tfor (i = 0; i < cptvf->nr_queues; i++) {\n\t\ttasklet_init(&cwqe_info->vq_wqe[i].twork, vq_work_handler,\n\t\t\t     (u64)cwqe_info);\n\t\tcwqe_info->vq_wqe[i].qno = i;\n\t\tcwqe_info->vq_wqe[i].cptvf = cptvf;\n\t}\n\n\tcptvf->wqe_info = cwqe_info;\n\n\treturn 0;\n}\n\nstatic void cleanup_worker_threads(struct cpt_vf *cptvf)\n{\n\tstruct cptvf_wqe_info *cwqe_info;\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tint i;\n\n\tcwqe_info = (struct cptvf_wqe_info *)cptvf->wqe_info;\n\tif (!cwqe_info)\n\t\treturn;\n\n\tif (cptvf->nr_queues) {\n\t\tdev_info(&pdev->dev, \"Cleaning VQ worker threads (%u)\\n\",\n\t\t\t cptvf->nr_queues);\n\t}\n\n\tfor (i = 0; i < cptvf->nr_queues; i++)\n\t\ttasklet_kill(&cwqe_info->vq_wqe[i].twork);\n\n\tkfree_sensitive(cwqe_info);\n\tcptvf->wqe_info = NULL;\n}\n\nstatic void free_pending_queues(struct pending_qinfo *pqinfo)\n{\n\tint i;\n\tstruct pending_queue *queue;\n\n\tfor_each_pending_queue(pqinfo, queue, i) {\n\t\tif (!queue->head)\n\t\t\tcontinue;\n\n\t\t \n\t\tkfree_sensitive((queue->head));\n\n\t\tqueue->front = 0;\n\t\tqueue->rear = 0;\n\n\t\treturn;\n\t}\n\n\tpqinfo->qlen = 0;\n\tpqinfo->nr_queues = 0;\n}\n\nstatic int alloc_pending_queues(struct pending_qinfo *pqinfo, u32 qlen,\n\t\t\t\tu32 nr_queues)\n{\n\tu32 i;\n\tint ret;\n\tstruct pending_queue *queue = NULL;\n\n\tpqinfo->nr_queues = nr_queues;\n\tpqinfo->qlen = qlen;\n\n\tfor_each_pending_queue(pqinfo, queue, i) {\n\t\tqueue->head = kcalloc(qlen, sizeof(*queue->head), GFP_KERNEL);\n\t\tif (!queue->head) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pending_qfail;\n\t\t}\n\n\t\tqueue->front = 0;\n\t\tqueue->rear = 0;\n\t\tatomic64_set((&queue->pending_count), (0));\n\n\t\t \n\t\tspin_lock_init(&queue->lock);\n\t}\n\n\treturn 0;\n\npending_qfail:\n\tfree_pending_queues(pqinfo);\n\n\treturn ret;\n}\n\nstatic int init_pending_queues(struct cpt_vf *cptvf, u32 qlen, u32 nr_queues)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tint ret;\n\n\tif (!nr_queues)\n\t\treturn 0;\n\n\tret = alloc_pending_queues(&cptvf->pqinfo, qlen, nr_queues);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to setup pending queues (%u)\\n\",\n\t\t\tnr_queues);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void cleanup_pending_queues(struct cpt_vf *cptvf)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\n\tif (!cptvf->nr_queues)\n\t\treturn;\n\n\tdev_info(&pdev->dev, \"Cleaning VQ pending queue (%u)\\n\",\n\t\t cptvf->nr_queues);\n\tfree_pending_queues(&cptvf->pqinfo);\n}\n\nstatic void free_command_queues(struct cpt_vf *cptvf,\n\t\t\t\tstruct command_qinfo *cqinfo)\n{\n\tint i;\n\tstruct command_queue *queue = NULL;\n\tstruct command_chunk *chunk = NULL;\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tstruct hlist_node *node;\n\n\t \n\tfor (i = 0; i < cptvf->nr_queues; i++) {\n\t\tqueue = &cqinfo->queue[i];\n\t\tif (hlist_empty(&cqinfo->queue[i].chead))\n\t\t\tcontinue;\n\n\t\thlist_for_each_entry_safe(chunk, node, &cqinfo->queue[i].chead,\n\t\t\t\t\t  nextchunk) {\n\t\t\tdma_free_coherent(&pdev->dev, chunk->size,\n\t\t\t\t\t  chunk->head,\n\t\t\t\t\t  chunk->dma_addr);\n\t\t\tchunk->head = NULL;\n\t\t\tchunk->dma_addr = 0;\n\t\t\thlist_del(&chunk->nextchunk);\n\t\t\tkfree_sensitive(chunk);\n\t\t}\n\n\t\tqueue->nchunks = 0;\n\t\tqueue->idx = 0;\n\t}\n\n\t \n\tcqinfo->cmd_size = 0;\n}\n\nstatic int alloc_command_queues(struct cpt_vf *cptvf,\n\t\t\t\tstruct command_qinfo *cqinfo, size_t cmd_size,\n\t\t\t\tu32 qlen)\n{\n\tint i;\n\tsize_t q_size;\n\tstruct command_queue *queue = NULL;\n\tstruct pci_dev *pdev = cptvf->pdev;\n\n\t \n\tcqinfo->cmd_size = cmd_size;\n\t \n\tcptvf->qsize = min(qlen, cqinfo->qchunksize) *\n\t\t\tCPT_NEXT_CHUNK_PTR_SIZE + 1;\n\t \n\tq_size = qlen * cqinfo->cmd_size;\n\n\t \n\tfor (i = 0; i < cptvf->nr_queues; i++) {\n\t\tsize_t c_size = 0;\n\t\tsize_t rem_q_size = q_size;\n\t\tstruct command_chunk *curr = NULL, *first = NULL, *last = NULL;\n\t\tu32 qcsize_bytes = cqinfo->qchunksize * cqinfo->cmd_size;\n\n\t\tqueue = &cqinfo->queue[i];\n\t\tINIT_HLIST_HEAD(&cqinfo->queue[i].chead);\n\t\tdo {\n\t\t\tcurr = kzalloc(sizeof(*curr), GFP_KERNEL);\n\t\t\tif (!curr)\n\t\t\t\tgoto cmd_qfail;\n\n\t\t\tc_size = (rem_q_size > qcsize_bytes) ? qcsize_bytes :\n\t\t\t\t\trem_q_size;\n\t\t\tcurr->head = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\t\tc_size + CPT_NEXT_CHUNK_PTR_SIZE,\n\t\t\t\t\t\t\t&curr->dma_addr,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!curr->head) {\n\t\t\t\tdev_err(&pdev->dev, \"Command Q (%d) chunk (%d) allocation failed\\n\",\n\t\t\t\t\ti, queue->nchunks);\n\t\t\t\tkfree(curr);\n\t\t\t\tgoto cmd_qfail;\n\t\t\t}\n\n\t\t\tcurr->size = c_size;\n\t\t\tif (queue->nchunks == 0) {\n\t\t\t\thlist_add_head(&curr->nextchunk,\n\t\t\t\t\t       &cqinfo->queue[i].chead);\n\t\t\t\tfirst = curr;\n\t\t\t} else {\n\t\t\t\thlist_add_behind(&curr->nextchunk,\n\t\t\t\t\t\t &last->nextchunk);\n\t\t\t}\n\n\t\t\tqueue->nchunks++;\n\t\t\trem_q_size -= c_size;\n\t\t\tif (last)\n\t\t\t\t*((u64 *)(&last->head[last->size])) = (u64)curr->dma_addr;\n\n\t\t\tlast = curr;\n\t\t} while (rem_q_size);\n\n\t\t \n\t\t \n\t\tcurr = first;\n\t\t*((u64 *)(&last->head[last->size])) = (u64)curr->dma_addr;\n\t\tqueue->qhead = curr;\n\t\tspin_lock_init(&queue->lock);\n\t}\n\treturn 0;\n\ncmd_qfail:\n\tfree_command_queues(cptvf, cqinfo);\n\treturn -ENOMEM;\n}\n\nstatic int init_command_queues(struct cpt_vf *cptvf, u32 qlen)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tint ret;\n\n\t \n\tret = alloc_command_queues(cptvf, &cptvf->cqinfo, CPT_INST_SIZE,\n\t\t\t\t   qlen);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to allocate AE command queues (%u)\\n\",\n\t\t\tcptvf->nr_queues);\n\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic void cleanup_command_queues(struct cpt_vf *cptvf)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\n\tif (!cptvf->nr_queues)\n\t\treturn;\n\n\tdev_info(&pdev->dev, \"Cleaning VQ command queue (%u)\\n\",\n\t\t cptvf->nr_queues);\n\tfree_command_queues(cptvf, &cptvf->cqinfo);\n}\n\nstatic void cptvf_sw_cleanup(struct cpt_vf *cptvf)\n{\n\tcleanup_worker_threads(cptvf);\n\tcleanup_pending_queues(cptvf);\n\tcleanup_command_queues(cptvf);\n}\n\nstatic int cptvf_sw_init(struct cpt_vf *cptvf, u32 qlen, u32 nr_queues)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tint ret = 0;\n\tu32 max_dev_queues = 0;\n\n\tmax_dev_queues = CPT_NUM_QS_PER_VF;\n\t \n\tnr_queues = min_t(u32, nr_queues, max_dev_queues);\n\tcptvf->nr_queues = nr_queues;\n\n\tret = init_command_queues(cptvf, qlen);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Failed to setup command queues (%u)\\n\",\n\t\t\tnr_queues);\n\t\treturn ret;\n\t}\n\n\tret = init_pending_queues(cptvf, qlen, nr_queues);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Failed to setup pending queues (%u)\\n\",\n\t\t\tnr_queues);\n\t\tgoto setup_pqfail;\n\t}\n\n\t \n\tret = init_worker_threads(cptvf);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Failed to setup worker threads\\n\");\n\t\tgoto init_work_fail;\n\t}\n\n\treturn 0;\n\ninit_work_fail:\n\tcleanup_worker_threads(cptvf);\n\tcleanup_pending_queues(cptvf);\n\nsetup_pqfail:\n\tcleanup_command_queues(cptvf);\n\n\treturn ret;\n}\n\nstatic void cptvf_free_irq_affinity(struct cpt_vf *cptvf, int vec)\n{\n\tirq_set_affinity_hint(pci_irq_vector(cptvf->pdev, vec), NULL);\n\tfree_cpumask_var(cptvf->affinity_mask[vec]);\n}\n\nstatic void cptvf_write_vq_ctl(struct cpt_vf *cptvf, bool val)\n{\n\tunion cptx_vqx_ctl vqx_ctl;\n\n\tvqx_ctl.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0));\n\tvqx_ctl.s.ena = val;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0), vqx_ctl.u);\n}\n\nvoid cptvf_write_vq_doorbell(struct cpt_vf *cptvf, u32 val)\n{\n\tunion cptx_vqx_doorbell vqx_dbell;\n\n\tvqx_dbell.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t     CPTX_VQX_DOORBELL(0, 0));\n\tvqx_dbell.s.dbell_cnt = val * 8;  \n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_DOORBELL(0, 0),\n\t\t\tvqx_dbell.u);\n}\n\nstatic void cptvf_write_vq_inprog(struct cpt_vf *cptvf, u8 val)\n{\n\tunion cptx_vqx_inprog vqx_inprg;\n\n\tvqx_inprg.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0));\n\tvqx_inprg.s.inflight = val;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0), vqx_inprg.u);\n}\n\nstatic void cptvf_write_vq_done_numwait(struct cpt_vf *cptvf, u32 val)\n{\n\tunion cptx_vqx_done_wait vqx_dwait;\n\n\tvqx_dwait.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t     CPTX_VQX_DONE_WAIT(0, 0));\n\tvqx_dwait.s.num_wait = val;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),\n\t\t\tvqx_dwait.u);\n}\n\nstatic void cptvf_write_vq_done_timewait(struct cpt_vf *cptvf, u16 time)\n{\n\tunion cptx_vqx_done_wait vqx_dwait;\n\n\tvqx_dwait.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t     CPTX_VQX_DONE_WAIT(0, 0));\n\tvqx_dwait.s.time_wait = time;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),\n\t\t\tvqx_dwait.u);\n}\n\nstatic void cptvf_enable_swerr_interrupts(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_ena_w1s vqx_misc_ena;\n\n\tvqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_ENA_W1S(0, 0));\n\t \n\tvqx_misc_ena.s.swerr = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),\n\t\t\tvqx_misc_ena.u);\n}\n\nstatic void cptvf_enable_mbox_interrupts(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_ena_w1s vqx_misc_ena;\n\n\tvqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_ENA_W1S(0, 0));\n\t \n\tvqx_misc_ena.s.mbox = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),\n\t\t\tvqx_misc_ena.u);\n}\n\nstatic void cptvf_enable_done_interrupts(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_done_ena_w1s vqx_done_ena;\n\n\tvqx_done_ena.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_DONE_ENA_W1S(0, 0));\n\t \n\tvqx_done_ena.s.done = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ENA_W1S(0, 0),\n\t\t\tvqx_done_ena.u);\n}\n\nstatic void cptvf_clear_dovf_intr(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_int vqx_misc_int;\n\n\tvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_INT(0, 0));\n\t \n\tvqx_misc_int.s.dovf = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\n\t\t\tvqx_misc_int.u);\n}\n\nstatic void cptvf_clear_irde_intr(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_int vqx_misc_int;\n\n\tvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_INT(0, 0));\n\t \n\tvqx_misc_int.s.irde = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\n\t\t\tvqx_misc_int.u);\n}\n\nstatic void cptvf_clear_nwrp_intr(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_int vqx_misc_int;\n\n\tvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_INT(0, 0));\n\t \n\tvqx_misc_int.s.nwrp = 1;\n\tcpt_write_csr64(cptvf->reg_base,\n\t\t\tCPTX_VQX_MISC_INT(0, 0), vqx_misc_int.u);\n}\n\nstatic void cptvf_clear_mbox_intr(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_int vqx_misc_int;\n\n\tvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_INT(0, 0));\n\t \n\tvqx_misc_int.s.mbox = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\n\t\t\tvqx_misc_int.u);\n}\n\nstatic void cptvf_clear_swerr_intr(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_misc_int vqx_misc_int;\n\n\tvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_MISC_INT(0, 0));\n\t \n\tvqx_misc_int.s.swerr = 1;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\n\t\t\tvqx_misc_int.u);\n}\n\nstatic u64 cptvf_read_vf_misc_intr_status(struct cpt_vf *cptvf)\n{\n\treturn cpt_read_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0));\n}\n\nstatic irqreturn_t cptvf_misc_intr_handler(int irq, void *cptvf_irq)\n{\n\tstruct cpt_vf *cptvf = (struct cpt_vf *)cptvf_irq;\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tu64 intr;\n\n\tintr = cptvf_read_vf_misc_intr_status(cptvf);\n\t \n\tif (likely(intr & CPT_VF_INTR_MBOX_MASK)) {\n\t\tdev_dbg(&pdev->dev, \"Mailbox interrupt 0x%llx on CPT VF %d\\n\",\n\t\t\tintr, cptvf->vfid);\n\t\tcptvf_handle_mbox_intr(cptvf);\n\t\tcptvf_clear_mbox_intr(cptvf);\n\t} else if (unlikely(intr & CPT_VF_INTR_DOVF_MASK)) {\n\t\tcptvf_clear_dovf_intr(cptvf);\n\t\t \n\t\tcptvf_write_vq_doorbell(cptvf, 0);\n\t\tdev_err(&pdev->dev, \"Doorbell overflow error interrupt 0x%llx on CPT VF %d\\n\",\n\t\t\tintr, cptvf->vfid);\n\t} else if (unlikely(intr & CPT_VF_INTR_IRDE_MASK)) {\n\t\tcptvf_clear_irde_intr(cptvf);\n\t\tdev_err(&pdev->dev, \"Instruction NCB read error interrupt 0x%llx on CPT VF %d\\n\",\n\t\t\tintr, cptvf->vfid);\n\t} else if (unlikely(intr & CPT_VF_INTR_NWRP_MASK)) {\n\t\tcptvf_clear_nwrp_intr(cptvf);\n\t\tdev_err(&pdev->dev, \"NCB response write error interrupt 0x%llx on CPT VF %d\\n\",\n\t\t\tintr, cptvf->vfid);\n\t} else if (unlikely(intr & CPT_VF_INTR_SERR_MASK)) {\n\t\tcptvf_clear_swerr_intr(cptvf);\n\t\tdev_err(&pdev->dev, \"Software error interrupt 0x%llx on CPT VF %d\\n\",\n\t\t\tintr, cptvf->vfid);\n\t} else {\n\t\tdev_err(&pdev->dev, \"Unhandled interrupt in CPT VF %d\\n\",\n\t\t\tcptvf->vfid);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic inline struct cptvf_wqe *get_cptvf_vq_wqe(struct cpt_vf *cptvf,\n\t\t\t\t\t\t int qno)\n{\n\tstruct cptvf_wqe_info *nwqe_info;\n\n\tif (unlikely(qno >= cptvf->nr_queues))\n\t\treturn NULL;\n\tnwqe_info = (struct cptvf_wqe_info *)cptvf->wqe_info;\n\n\treturn &nwqe_info->vq_wqe[qno];\n}\n\nstatic inline u32 cptvf_read_vq_done_count(struct cpt_vf *cptvf)\n{\n\tunion cptx_vqx_done vqx_done;\n\n\tvqx_done.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_DONE(0, 0));\n\treturn vqx_done.s.done;\n}\n\nstatic inline void cptvf_write_vq_done_ack(struct cpt_vf *cptvf,\n\t\t\t\t\t   u32 ackcnt)\n{\n\tunion cptx_vqx_done_ack vqx_dack_cnt;\n\n\tvqx_dack_cnt.u = cpt_read_csr64(cptvf->reg_base,\n\t\t\t\t\tCPTX_VQX_DONE_ACK(0, 0));\n\tvqx_dack_cnt.s.done_ack = ackcnt;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ACK(0, 0),\n\t\t\tvqx_dack_cnt.u);\n}\n\nstatic irqreturn_t cptvf_done_intr_handler(int irq, void *cptvf_irq)\n{\n\tstruct cpt_vf *cptvf = (struct cpt_vf *)cptvf_irq;\n\tstruct pci_dev *pdev = cptvf->pdev;\n\t \n\tu32 intr = cptvf_read_vq_done_count(cptvf);\n\n\tif (intr) {\n\t\tstruct cptvf_wqe *wqe;\n\n\t\t \n\t\tcptvf_write_vq_done_ack(cptvf, intr);\n\t\twqe = get_cptvf_vq_wqe(cptvf, 0);\n\t\tif (unlikely(!wqe)) {\n\t\t\tdev_err(&pdev->dev, \"No work to schedule for VF (%d)\",\n\t\t\t\tcptvf->vfid);\n\t\t\treturn IRQ_NONE;\n\t\t}\n\t\ttasklet_hi_schedule(&wqe->twork);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void cptvf_set_irq_affinity(struct cpt_vf *cptvf, int vec)\n{\n\tstruct pci_dev *pdev = cptvf->pdev;\n\tint cpu;\n\n\tif (!zalloc_cpumask_var(&cptvf->affinity_mask[vec],\n\t\t\t\tGFP_KERNEL)) {\n\t\tdev_err(&pdev->dev, \"Allocation failed for affinity_mask for VF %d\",\n\t\t\tcptvf->vfid);\n\t\treturn;\n\t}\n\n\tcpu = cptvf->vfid % num_online_cpus();\n\tcpumask_set_cpu(cpumask_local_spread(cpu, cptvf->node),\n\t\t\tcptvf->affinity_mask[vec]);\n\tirq_set_affinity_hint(pci_irq_vector(pdev, vec),\n\t\t\tcptvf->affinity_mask[vec]);\n}\n\nstatic void cptvf_write_vq_saddr(struct cpt_vf *cptvf, u64 val)\n{\n\tunion cptx_vqx_saddr vqx_saddr;\n\n\tvqx_saddr.u = val;\n\tcpt_write_csr64(cptvf->reg_base, CPTX_VQX_SADDR(0, 0), vqx_saddr.u);\n}\n\nstatic void cptvf_device_init(struct cpt_vf *cptvf)\n{\n\tu64 base_addr = 0;\n\n\t \n\tcptvf_write_vq_ctl(cptvf, 0);\n\t \n\tcptvf_write_vq_doorbell(cptvf, 0);\n\t \n\tcptvf_write_vq_inprog(cptvf, 0);\n\t \n\t \n\tbase_addr = (u64)(cptvf->cqinfo.queue[0].qhead->dma_addr);\n\tcptvf_write_vq_saddr(cptvf, base_addr);\n\t \n\tcptvf_write_vq_done_timewait(cptvf, CPT_TIMER_THOLD);\n\tcptvf_write_vq_done_numwait(cptvf, 1);\n\t \n\tcptvf_write_vq_ctl(cptvf, 1);\n\t \n\tcptvf->flags |= CPT_FLAG_DEVICE_READY;\n}\n\nstatic int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct cpt_vf *cptvf;\n\tint    err;\n\n\tcptvf = devm_kzalloc(dev, sizeof(*cptvf), GFP_KERNEL);\n\tif (!cptvf)\n\t\treturn -ENOMEM;\n\n\tpci_set_drvdata(pdev, cptvf);\n\tcptvf->pdev = pdev;\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to enable PCI device\\n\");\n\t\tpci_set_drvdata(pdev, NULL);\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(dev, \"PCI request regions failed 0x%x\\n\", err);\n\t\tgoto cptvf_err_disable_device;\n\t}\n\t \n\tcptvf->flags |= CPT_FLAG_VF_DRIVER;\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(48));\n\tif (err) {\n\t\tdev_err(dev, \"Unable to get usable 48-bit DMA configuration\\n\");\n\t\tgoto cptvf_err_release_regions;\n\t}\n\n\t \n\tcptvf->reg_base = pcim_iomap(pdev, 0, 0);\n\tif (!cptvf->reg_base) {\n\t\tdev_err(dev, \"Cannot map config register space, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto cptvf_err_release_regions;\n\t}\n\n\tcptvf->node = dev_to_node(&pdev->dev);\n\terr = pci_alloc_irq_vectors(pdev, CPT_VF_MSIX_VECTORS,\n\t\t\tCPT_VF_MSIX_VECTORS, PCI_IRQ_MSIX);\n\tif (err < 0) {\n\t\tdev_err(dev, \"Request for #%d msix vectors failed\\n\",\n\t\t\tCPT_VF_MSIX_VECTORS);\n\t\tgoto cptvf_err_release_regions;\n\t}\n\n\terr = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC),\n\t\t\t  cptvf_misc_intr_handler, 0, \"CPT VF misc intr\",\n\t\t\t  cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"Request misc irq failed\");\n\t\tgoto cptvf_free_vectors;\n\t}\n\n\t \n\tcptvf_enable_mbox_interrupts(cptvf);\n\tcptvf_enable_swerr_interrupts(cptvf);\n\n\t \n\t \n\terr = cptvf_check_pf_ready(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"PF not responding to READY msg\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\n\t \n\tcptvf->cqinfo.qchunksize = CPT_CMD_QCHUNK_SIZE;\n\terr = cptvf_sw_init(cptvf, CPT_CMD_QLEN, CPT_NUM_QS_PER_VF);\n\tif (err) {\n\t\tdev_err(dev, \"cptvf_sw_init() failed\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\t \n\terr = cptvf_send_vq_size_msg(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"PF not responding to QLEN msg\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\n\t \n\tcptvf_device_init(cptvf);\n\t \n\tcptvf->vfgrp = 1;\n\terr = cptvf_send_vf_to_grp_msg(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"PF not responding to VF_GRP msg\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\n\tcptvf->priority = 1;\n\terr = cptvf_send_vf_priority_msg(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"PF not responding to VF_PRIO msg\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\n\terr = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE),\n\t\t\t  cptvf_done_intr_handler, 0, \"CPT VF done intr\",\n\t\t\t  cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"Request done irq failed\\n\");\n\t\tgoto cptvf_free_misc_irq;\n\t}\n\n\t \n\tcptvf_enable_done_interrupts(cptvf);\n\n\t \n\tcptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\n\tcptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\n\n\terr = cptvf_send_vf_up(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"PF not responding to UP msg\");\n\t\tgoto cptvf_free_irq_affinity;\n\t}\n\terr = cvm_crypto_init(cptvf);\n\tif (err) {\n\t\tdev_err(dev, \"Algorithm register failed\\n\");\n\t\tgoto cptvf_free_irq_affinity;\n\t}\n\treturn 0;\n\ncptvf_free_irq_affinity:\n\tcptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\n\tcptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\ncptvf_free_misc_irq:\n\tfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);\ncptvf_free_vectors:\n\tpci_free_irq_vectors(cptvf->pdev);\ncptvf_err_release_regions:\n\tpci_release_regions(pdev);\ncptvf_err_disable_device:\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n\n\treturn err;\n}\n\nstatic void cptvf_remove(struct pci_dev *pdev)\n{\n\tstruct cpt_vf *cptvf = pci_get_drvdata(pdev);\n\n\tif (!cptvf) {\n\t\tdev_err(&pdev->dev, \"Invalid CPT-VF device\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (cptvf_send_vf_down(cptvf)) {\n\t\tdev_err(&pdev->dev, \"PF not responding to DOWN msg\");\n\t} else {\n\t\tcptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\n\t\tcptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\n\t\tfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE), cptvf);\n\t\tfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);\n\t\tpci_free_irq_vectors(cptvf->pdev);\n\t\tcptvf_sw_cleanup(cptvf);\n\t\tpci_set_drvdata(pdev, NULL);\n\t\tpci_release_regions(pdev);\n\t\tpci_disable_device(pdev);\n\t\tcvm_crypto_exit();\n\t}\n}\n\nstatic void cptvf_shutdown(struct pci_dev *pdev)\n{\n\tcptvf_remove(pdev);\n}\n\n \nstatic const struct pci_device_id cptvf_id_table[] = {\n\t{PCI_VDEVICE(CAVIUM, CPT_81XX_PCI_VF_DEVICE_ID), 0},\n\t{ 0, }   \n};\n\nstatic struct pci_driver cptvf_pci_driver = {\n\t.name = DRV_NAME,\n\t.id_table = cptvf_id_table,\n\t.probe = cptvf_probe,\n\t.remove = cptvf_remove,\n\t.shutdown = cptvf_shutdown,\n};\n\nmodule_pci_driver(cptvf_pci_driver);\n\nMODULE_AUTHOR(\"George Cherian <george.cherian@cavium.com>\");\nMODULE_DESCRIPTION(\"Cavium Thunder CPT Virtual Function Driver\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_VERSION(DRV_VERSION);\nMODULE_DEVICE_TABLE(pci, cptvf_id_table);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}