{
  "module_name": "nitrox_reqmgr.c",
  "hash_id": "ee00db6dff9c5b282e0c8678631ee243ed2967d1c50a4f21081602f3a904cac0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/cavium/nitrox/nitrox_reqmgr.c",
  "human_readable_source": "\n#include <linux/gfp.h>\n#include <linux/workqueue.h>\n#include <crypto/internal/skcipher.h>\n\n#include \"nitrox_common.h\"\n#include \"nitrox_dev.h\"\n#include \"nitrox_req.h\"\n#include \"nitrox_csr.h\"\n\n \n#define MIN_UDD_LEN 16\n \n#define FDATA_SIZE 32\n \n#define SOLICIT_BASE_DPORT 256\n\n#define REQ_NOT_POSTED 1\n#define REQ_BACKLOG    2\n#define REQ_POSTED     3\n\n \n\nstatic inline int incr_index(int index, int count, int max)\n{\n\tif ((index + count) >= max)\n\t\tindex = index + count - max;\n\telse\n\t\tindex += count;\n\n\treturn index;\n}\n\nstatic void softreq_unmap_sgbufs(struct nitrox_softreq *sr)\n{\n\tstruct nitrox_device *ndev = sr->ndev;\n\tstruct device *dev = DEV(ndev);\n\n\n\tdma_unmap_sg(dev, sr->in.sg, sg_nents(sr->in.sg),\n\t\t     DMA_BIDIRECTIONAL);\n\tdma_unmap_single(dev, sr->in.sgcomp_dma, sr->in.sgcomp_len,\n\t\t\t DMA_TO_DEVICE);\n\tkfree(sr->in.sgcomp);\n\tsr->in.sg = NULL;\n\tsr->in.sgmap_cnt = 0;\n\n\tdma_unmap_sg(dev, sr->out.sg, sg_nents(sr->out.sg),\n\t\t     DMA_BIDIRECTIONAL);\n\tdma_unmap_single(dev, sr->out.sgcomp_dma, sr->out.sgcomp_len,\n\t\t\t DMA_TO_DEVICE);\n\tkfree(sr->out.sgcomp);\n\tsr->out.sg = NULL;\n\tsr->out.sgmap_cnt = 0;\n}\n\nstatic void softreq_destroy(struct nitrox_softreq *sr)\n{\n\tsoftreq_unmap_sgbufs(sr);\n\tkfree(sr);\n}\n\n \nstatic int create_sg_component(struct nitrox_softreq *sr,\n\t\t\t       struct nitrox_sgtable *sgtbl, int map_nents)\n{\n\tstruct nitrox_device *ndev = sr->ndev;\n\tstruct nitrox_sgcomp *sgcomp;\n\tstruct scatterlist *sg;\n\tdma_addr_t dma;\n\tsize_t sz_comp;\n\tint i, j, nr_sgcomp;\n\n\tnr_sgcomp = roundup(map_nents, 4) / 4;\n\n\t \n\tsz_comp = nr_sgcomp * sizeof(*sgcomp);\n\tsgcomp = kzalloc(sz_comp, sr->gfp);\n\tif (!sgcomp)\n\t\treturn -ENOMEM;\n\n\tsgtbl->sgcomp = sgcomp;\n\n\tsg = sgtbl->sg;\n\t \n\tfor (i = 0; i < nr_sgcomp; i++) {\n\t\tfor (j = 0; j < 4 && sg; j++) {\n\t\t\tsgcomp[i].len[j] = cpu_to_be16(sg_dma_len(sg));\n\t\t\tsgcomp[i].dma[j] = cpu_to_be64(sg_dma_address(sg));\n\t\t\tsg = sg_next(sg);\n\t\t}\n\t}\n\t \n\tdma = dma_map_single(DEV(ndev), sgtbl->sgcomp, sz_comp, DMA_TO_DEVICE);\n\tif (dma_mapping_error(DEV(ndev), dma)) {\n\t\tkfree(sgtbl->sgcomp);\n\t\tsgtbl->sgcomp = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tsgtbl->sgcomp_dma = dma;\n\tsgtbl->sgcomp_len = sz_comp;\n\n\treturn 0;\n}\n\n \nstatic int dma_map_inbufs(struct nitrox_softreq *sr,\n\t\t\t  struct se_crypto_request *req)\n{\n\tstruct device *dev = DEV(sr->ndev);\n\tstruct scatterlist *sg;\n\tint i, nents, ret = 0;\n\n\tnents = dma_map_sg(dev, req->src, sg_nents(req->src),\n\t\t\t   DMA_BIDIRECTIONAL);\n\tif (!nents)\n\t\treturn -EINVAL;\n\n\tfor_each_sg(req->src, sg, nents, i)\n\t\tsr->in.total_bytes += sg_dma_len(sg);\n\n\tsr->in.sg = req->src;\n\tsr->in.sgmap_cnt = nents;\n\tret = create_sg_component(sr, &sr->in, sr->in.sgmap_cnt);\n\tif (ret)\n\t\tgoto incomp_err;\n\n\treturn 0;\n\nincomp_err:\n\tdma_unmap_sg(dev, req->src, sg_nents(req->src), DMA_BIDIRECTIONAL);\n\tsr->in.sgmap_cnt = 0;\n\treturn ret;\n}\n\nstatic int dma_map_outbufs(struct nitrox_softreq *sr,\n\t\t\t   struct se_crypto_request *req)\n{\n\tstruct device *dev = DEV(sr->ndev);\n\tint nents, ret = 0;\n\n\tnents = dma_map_sg(dev, req->dst, sg_nents(req->dst),\n\t\t\t   DMA_BIDIRECTIONAL);\n\tif (!nents)\n\t\treturn -EINVAL;\n\n\tsr->out.sg = req->dst;\n\tsr->out.sgmap_cnt = nents;\n\tret = create_sg_component(sr, &sr->out, sr->out.sgmap_cnt);\n\tif (ret)\n\t\tgoto outcomp_map_err;\n\n\treturn 0;\n\noutcomp_map_err:\n\tdma_unmap_sg(dev, req->dst, sg_nents(req->dst), DMA_BIDIRECTIONAL);\n\tsr->out.sgmap_cnt = 0;\n\tsr->out.sg = NULL;\n\treturn ret;\n}\n\nstatic inline int softreq_map_iobuf(struct nitrox_softreq *sr,\n\t\t\t\t    struct se_crypto_request *creq)\n{\n\tint ret;\n\n\tret = dma_map_inbufs(sr, creq);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dma_map_outbufs(sr, creq);\n\tif (ret)\n\t\tsoftreq_unmap_sgbufs(sr);\n\n\treturn ret;\n}\n\nstatic inline void backlog_list_add(struct nitrox_softreq *sr,\n\t\t\t\t    struct nitrox_cmdq *cmdq)\n{\n\tINIT_LIST_HEAD(&sr->backlog);\n\n\tspin_lock_bh(&cmdq->backlog_qlock);\n\tlist_add_tail(&sr->backlog, &cmdq->backlog_head);\n\tatomic_inc(&cmdq->backlog_count);\n\tatomic_set(&sr->status, REQ_BACKLOG);\n\tspin_unlock_bh(&cmdq->backlog_qlock);\n}\n\nstatic inline void response_list_add(struct nitrox_softreq *sr,\n\t\t\t\t     struct nitrox_cmdq *cmdq)\n{\n\tINIT_LIST_HEAD(&sr->response);\n\n\tspin_lock_bh(&cmdq->resp_qlock);\n\tlist_add_tail(&sr->response, &cmdq->response_head);\n\tspin_unlock_bh(&cmdq->resp_qlock);\n}\n\nstatic inline void response_list_del(struct nitrox_softreq *sr,\n\t\t\t\t     struct nitrox_cmdq *cmdq)\n{\n\tspin_lock_bh(&cmdq->resp_qlock);\n\tlist_del(&sr->response);\n\tspin_unlock_bh(&cmdq->resp_qlock);\n}\n\nstatic struct nitrox_softreq *\nget_first_response_entry(struct nitrox_cmdq *cmdq)\n{\n\treturn list_first_entry_or_null(&cmdq->response_head,\n\t\t\t\t\tstruct nitrox_softreq, response);\n}\n\nstatic inline bool cmdq_full(struct nitrox_cmdq *cmdq, int qlen)\n{\n\tif (atomic_inc_return(&cmdq->pending_count) > qlen) {\n\t\tatomic_dec(&cmdq->pending_count);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\treturn true;\n\t}\n\t \n\tsmp_mb__after_atomic();\n\treturn false;\n}\n\n \nstatic void post_se_instr(struct nitrox_softreq *sr,\n\t\t\t  struct nitrox_cmdq *cmdq)\n{\n\tstruct nitrox_device *ndev = sr->ndev;\n\tint idx;\n\tu8 *ent;\n\n\tspin_lock_bh(&cmdq->cmd_qlock);\n\n\tidx = cmdq->write_idx;\n\t \n\tent = cmdq->base + (idx * cmdq->instr_size);\n\tmemcpy(ent, &sr->instr, cmdq->instr_size);\n\n\tatomic_set(&sr->status, REQ_POSTED);\n\tresponse_list_add(sr, cmdq);\n\tsr->tstamp = jiffies;\n\t \n\tdma_wmb();\n\n\t \n\twriteq(1, cmdq->dbell_csr_addr);\n\n\tcmdq->write_idx = incr_index(idx, 1, ndev->qlen);\n\n\tspin_unlock_bh(&cmdq->cmd_qlock);\n\n\t \n\tatomic64_inc(&ndev->stats.posted);\n}\n\nstatic int post_backlog_cmds(struct nitrox_cmdq *cmdq)\n{\n\tstruct nitrox_device *ndev = cmdq->ndev;\n\tstruct nitrox_softreq *sr, *tmp;\n\tint ret = 0;\n\n\tif (!atomic_read(&cmdq->backlog_count))\n\t\treturn 0;\n\n\tspin_lock_bh(&cmdq->backlog_qlock);\n\n\tlist_for_each_entry_safe(sr, tmp, &cmdq->backlog_head, backlog) {\n\t\t \n\t\tif (unlikely(cmdq_full(cmdq, ndev->qlen))) {\n\t\t\tret = -ENOSPC;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tlist_del(&sr->backlog);\n\t\tatomic_dec(&cmdq->backlog_count);\n\t\t \n\t\tsmp_mb__after_atomic();\n\n\t\t \n\t\tpost_se_instr(sr, cmdq);\n\t}\n\tspin_unlock_bh(&cmdq->backlog_qlock);\n\n\treturn ret;\n}\n\nstatic int nitrox_enqueue_request(struct nitrox_softreq *sr)\n{\n\tstruct nitrox_cmdq *cmdq = sr->cmdq;\n\tstruct nitrox_device *ndev = sr->ndev;\n\n\t \n\tpost_backlog_cmds(cmdq);\n\n\tif (unlikely(cmdq_full(cmdq, ndev->qlen))) {\n\t\tif (!(sr->flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {\n\t\t\t \n\t\t\tatomic64_inc(&ndev->stats.dropped);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\t \n\t\tbacklog_list_add(sr, cmdq);\n\t\treturn -EINPROGRESS;\n\t}\n\tpost_se_instr(sr, cmdq);\n\n\treturn -EINPROGRESS;\n}\n\n \nint nitrox_process_se_request(struct nitrox_device *ndev,\n\t\t\t      struct se_crypto_request *req,\n\t\t\t      completion_t callback,\n\t\t\t      void *cb_arg)\n{\n\tstruct nitrox_softreq *sr;\n\tdma_addr_t ctx_handle = 0;\n\tint qno, ret = 0;\n\n\tif (!nitrox_ready(ndev))\n\t\treturn -ENODEV;\n\n\tsr = kzalloc(sizeof(*sr), req->gfp);\n\tif (!sr)\n\t\treturn -ENOMEM;\n\n\tsr->ndev = ndev;\n\tsr->flags = req->flags;\n\tsr->gfp = req->gfp;\n\tsr->callback = callback;\n\tsr->cb_arg = cb_arg;\n\n\tatomic_set(&sr->status, REQ_NOT_POSTED);\n\n\tsr->resp.orh = req->orh;\n\tsr->resp.completion = req->comp;\n\n\tret = softreq_map_iobuf(sr, req);\n\tif (ret) {\n\t\tkfree(sr);\n\t\treturn ret;\n\t}\n\n\t \n\tif (req->ctx_handle) {\n\t\tstruct ctx_hdr *hdr;\n\t\tu8 *ctx_ptr;\n\n\t\tctx_ptr = (u8 *)(uintptr_t)req->ctx_handle;\n\t\thdr = (struct ctx_hdr *)(ctx_ptr - sizeof(struct ctx_hdr));\n\t\tctx_handle = hdr->ctx_dma;\n\t}\n\n\t \n\tqno = smp_processor_id() % ndev->nr_queues;\n\n\tsr->cmdq = &ndev->pkt_inq[qno];\n\n\t \n\n\t \n\t \n\tsr->instr.dptr0 = cpu_to_be64(sr->in.sgcomp_dma);\n\n\t \n\tsr->instr.ih.value = 0;\n\tsr->instr.ih.s.g = 1;\n\tsr->instr.ih.s.gsz = sr->in.sgmap_cnt;\n\tsr->instr.ih.s.ssz = sr->out.sgmap_cnt;\n\tsr->instr.ih.s.fsz = FDATA_SIZE + sizeof(struct gphdr);\n\tsr->instr.ih.s.tlen = sr->instr.ih.s.fsz + sr->in.total_bytes;\n\tsr->instr.ih.bev = cpu_to_be64(sr->instr.ih.value);\n\n\t \n\tsr->instr.irh.value[0] = 0;\n\tsr->instr.irh.s.uddl = MIN_UDD_LEN;\n\t \n\tsr->instr.irh.s.ctxl = (req->ctrl.s.ctxl / 8);\n\t \n\tsr->instr.irh.s.destport = SOLICIT_BASE_DPORT + qno;\n\tsr->instr.irh.s.ctxc = req->ctrl.s.ctxc;\n\tsr->instr.irh.s.arg = req->ctrl.s.arg;\n\tsr->instr.irh.s.opcode = req->opcode;\n\tsr->instr.irh.bev[0] = cpu_to_be64(sr->instr.irh.value[0]);\n\n\t \n\tsr->instr.irh.s.ctxp = cpu_to_be64(ctx_handle);\n\n\t \n\tsr->instr.slc.value[0] = 0;\n\tsr->instr.slc.s.ssz = sr->out.sgmap_cnt;\n\tsr->instr.slc.bev[0] = cpu_to_be64(sr->instr.slc.value[0]);\n\n\t \n\tsr->instr.slc.s.rptr = cpu_to_be64(sr->out.sgcomp_dma);\n\n\t \n\tsr->instr.fdata[0] = *((u64 *)&req->gph);\n\tsr->instr.fdata[1] = 0;\n\n\tret = nitrox_enqueue_request(sr);\n\tif (ret == -ENOSPC)\n\t\tgoto send_fail;\n\n\treturn ret;\n\nsend_fail:\n\tsoftreq_destroy(sr);\n\treturn ret;\n}\n\nstatic inline int cmd_timeout(unsigned long tstamp, unsigned long timeout)\n{\n\treturn time_after_eq(jiffies, (tstamp + timeout));\n}\n\nvoid backlog_qflush_work(struct work_struct *work)\n{\n\tstruct nitrox_cmdq *cmdq;\n\n\tcmdq = container_of(work, struct nitrox_cmdq, backlog_qflush);\n\tpost_backlog_cmds(cmdq);\n}\n\nstatic bool sr_completed(struct nitrox_softreq *sr)\n{\n\tu64 orh = READ_ONCE(*sr->resp.orh);\n\tunsigned long timeout = jiffies + msecs_to_jiffies(1);\n\n\tif ((orh != PENDING_SIG) && (orh & 0xff))\n\t\treturn true;\n\n\twhile (READ_ONCE(*sr->resp.completion) == PENDING_SIG) {\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tpr_err(\"comp not done\\n\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\n \nstatic void process_response_list(struct nitrox_cmdq *cmdq)\n{\n\tstruct nitrox_device *ndev = cmdq->ndev;\n\tstruct nitrox_softreq *sr;\n\tint req_completed = 0, err = 0, budget;\n\tcompletion_t callback;\n\tvoid *cb_arg;\n\n\t \n\tbudget = atomic_read(&cmdq->pending_count);\n\n\twhile (req_completed < budget) {\n\t\tsr = get_first_response_entry(cmdq);\n\t\tif (!sr)\n\t\t\tbreak;\n\n\t\tif (atomic_read(&sr->status) != REQ_POSTED)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!sr_completed(sr)) {\n\t\t\t \n\t\t\tif (!cmd_timeout(sr->tstamp, ndev->timeout))\n\t\t\t\tbreak;\n\t\t\tdev_err_ratelimited(DEV(ndev),\n\t\t\t\t\t    \"Request timeout, orh 0x%016llx\\n\",\n\t\t\t\t\t    READ_ONCE(*sr->resp.orh));\n\t\t}\n\t\tatomic_dec(&cmdq->pending_count);\n\t\tatomic64_inc(&ndev->stats.completed);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\t \n\t\tresponse_list_del(sr, cmdq);\n\t\t \n\t\terr = READ_ONCE(*sr->resp.orh) & 0xff;\n\t\tcallback = sr->callback;\n\t\tcb_arg = sr->cb_arg;\n\t\tsoftreq_destroy(sr);\n\t\tif (callback)\n\t\t\tcallback(cb_arg, err);\n\n\t\treq_completed++;\n\t}\n}\n\n \nvoid pkt_slc_resp_tasklet(unsigned long data)\n{\n\tstruct nitrox_q_vector *qvec = (void *)(uintptr_t)(data);\n\tstruct nitrox_cmdq *cmdq = qvec->cmdq;\n\tunion nps_pkt_slc_cnts slc_cnts;\n\n\t \n\tslc_cnts.value = readq(cmdq->compl_cnt_csr_addr);\n\t \n\tslc_cnts.s.resend = 1;\n\n\tprocess_response_list(cmdq);\n\n\t \n\twriteq(slc_cnts.value, cmdq->compl_cnt_csr_addr);\n\n\tif (atomic_read(&cmdq->backlog_count))\n\t\tschedule_work(&cmdq->backlog_qflush);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}