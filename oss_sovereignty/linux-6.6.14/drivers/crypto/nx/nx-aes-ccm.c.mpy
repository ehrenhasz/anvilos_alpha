{
  "module_name": "nx-aes-ccm.c",
  "hash_id": "435fc36934124f15652a9841e26dc1766cfb0a2e300a74984cfb356a3d651116",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/nx/nx-aes-ccm.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/aead.h>\n#include <crypto/aes.h>\n#include <crypto/algapi.h>\n#include <crypto/scatterwalk.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/crypto.h>\n#include <asm/vio.h>\n\n#include \"nx_csbcpb.h\"\n#include \"nx.h\"\n\n\nstatic int ccm_aes_nx_set_key(struct crypto_aead *tfm,\n\t\t\t      const u8           *in_key,\n\t\t\t      unsigned int        key_len)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&tfm->base);\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\n\n\tnx_ctx_init(nx_ctx, HCOP_FC_AES);\n\n\tswitch (key_len) {\n\tcase AES_KEYSIZE_128:\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_128);\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_128);\n\t\tnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_128];\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_CCM;\n\tmemcpy(csbcpb->cpb.aes_ccm.key, in_key, key_len);\n\n\tcsbcpb_aead->cpb.hdr.mode = NX_MODE_AES_CCA;\n\tmemcpy(csbcpb_aead->cpb.aes_cca.key, in_key, key_len);\n\n\treturn 0;\n\n}\n\nstatic int ccm4309_aes_nx_set_key(struct crypto_aead *tfm,\n\t\t\t\t  const u8           *in_key,\n\t\t\t\t  unsigned int        key_len)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&tfm->base);\n\n\tif (key_len < 3)\n\t\treturn -EINVAL;\n\n\tkey_len -= 3;\n\n\tmemcpy(nx_ctx->priv.ccm.nonce, in_key + key_len, 3);\n\n\treturn ccm_aes_nx_set_key(tfm, in_key, key_len);\n}\n\nstatic int ccm_aes_nx_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t  unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 6:\n\tcase 8:\n\tcase 10:\n\tcase 12:\n\tcase 14:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int ccm4309_aes_nx_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t      unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\n{\n\t__be32 data;\n\n\tmemset(block, 0, csize);\n\tblock += csize;\n\n\tif (csize >= 4)\n\t\tcsize = 4;\n\telse if (msglen > (unsigned int)(1 << (8 * csize)))\n\t\treturn -EOVERFLOW;\n\n\tdata = cpu_to_be32(msglen);\n\tmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\n\n\treturn 0;\n}\n\n \nstatic inline int crypto_ccm_check_iv(const u8 *iv)\n{\n\t \n\tif (1 > iv[0] || iv[0] > 7)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic int generate_b0(u8 *iv, unsigned int assoclen, unsigned int authsize,\n\t\t       unsigned int cryptlen, u8 *b0)\n{\n\tunsigned int l, lp, m = authsize;\n\n\tmemcpy(b0, iv, 16);\n\n\tlp = b0[0];\n\tl = lp + 1;\n\n\t \n\t*b0 |= (8 * ((m - 2) / 2));\n\n\t \n\tif (assoclen)\n\t\t*b0 |= 64;\n\n\treturn set_msg_len(b0 + 16 - l, cryptlen, l);\n}\n\nstatic int generate_pat(u8                   *iv,\n\t\t\tstruct aead_request  *req,\n\t\t\tstruct nx_crypto_ctx *nx_ctx,\n\t\t\tunsigned int          authsize,\n\t\t\tunsigned int          nbytes,\n\t\t\tunsigned int\t      assoclen,\n\t\t\tu8                   *out)\n{\n\tstruct nx_sg *nx_insg = nx_ctx->in_sg;\n\tstruct nx_sg *nx_outsg = nx_ctx->out_sg;\n\tunsigned int iauth_len = 0;\n\tu8 tmp[16], *b1 = NULL, *b0 = NULL, *result = NULL;\n\tint rc;\n\tunsigned int max_sg_len;\n\n\t \n\tmemset(iv + 15 - iv[0], 0, iv[0] + 1);\n\n\t \n\n\tif (!assoclen) {\n\t\tb0 = nx_ctx->csbcpb->cpb.aes_ccm.in_pat_or_b0;\n\t} else if (assoclen <= 14) {\n\t\t \n\t\tb0 = nx_ctx->csbcpb->cpb.aes_ccm.in_pat_or_b0;\n\t\tb1 = nx_ctx->priv.ccm.iauth_tag;\n\t\tiauth_len = assoclen;\n\t} else if (assoclen <= 65280) {\n\t\t \n\t\tb0 = nx_ctx->csbcpb_aead->cpb.aes_cca.b0;\n\t\tb1 = nx_ctx->csbcpb_aead->cpb.aes_cca.b1;\n\t\tiauth_len = 14;\n\t} else {\n\t\tb0 = nx_ctx->csbcpb_aead->cpb.aes_cca.b0;\n\t\tb1 = nx_ctx->csbcpb_aead->cpb.aes_cca.b1;\n\t\tiauth_len = 10;\n\t}\n\n\t \n\trc = generate_b0(iv, assoclen, authsize, nbytes, b0);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (b1) {\n\t\tmemset(b1, 0, 16);\n\t\tif (assoclen <= 65280) {\n\t\t\t*(u16 *)b1 = assoclen;\n\t\t\tscatterwalk_map_and_copy(b1 + 2, req->src, 0,\n\t\t\t\t\t iauth_len, SCATTERWALK_FROM_SG);\n\t\t} else {\n\t\t\t*(u16 *)b1 = (u16)(0xfffe);\n\t\t\t*(u32 *)&b1[2] = assoclen;\n\t\t\tscatterwalk_map_and_copy(b1 + 6, req->src, 0,\n\t\t\t\t\t iauth_len, SCATTERWALK_FROM_SG);\n\t\t}\n\t}\n\n\t \n\tif (!assoclen) {\n\t\treturn rc;\n\t} else if (assoclen <= 14) {\n\t\tunsigned int len = 16;\n\n\t\tnx_insg = nx_build_sg_list(nx_insg, b1, &len, nx_ctx->ap->sglen);\n\n\t\tif (len != 16)\n\t\t\treturn -EINVAL;\n\n\t\tnx_outsg = nx_build_sg_list(nx_outsg, tmp, &len,\n\t\t\t\t\t    nx_ctx->ap->sglen);\n\n\t\tif (len != 16)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tnx_ctx->op.inlen = (nx_ctx->in_sg - nx_insg) *\n\t\t\t\t\tsizeof(struct nx_sg);\n\t\tnx_ctx->op.outlen = (nx_ctx->out_sg - nx_outsg) *\n\t\t\t\t\tsizeof(struct nx_sg);\n\n\t\tNX_CPB_FDM(nx_ctx->csbcpb) |= NX_FDM_ENDE_ENCRYPT;\n\t\tNX_CPB_FDM(nx_ctx->csbcpb) |= NX_FDM_INTERMEDIATE;\n\n\t\tresult = nx_ctx->csbcpb->cpb.aes_ccm.out_pat_or_mac;\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(assoclen, &nx_ctx->stats->aes_bytes);\n\n\t} else {\n\t\tunsigned int processed = 0, to_process;\n\n\t\tprocessed += iauth_len;\n\n\t\t \n\t\tmax_sg_len = min_t(u64, nx_ctx->ap->sglen,\n\t\t\t\tnx_driver.of.max_sg_len/sizeof(struct nx_sg));\n\t\tmax_sg_len = min_t(u64, max_sg_len,\n\t\t\t\tnx_ctx->ap->databytelen/NX_PAGE_SIZE);\n\n\t\tdo {\n\t\t\tto_process = min_t(u32, assoclen - processed,\n\t\t\t\t\t   nx_ctx->ap->databytelen);\n\n\t\t\tnx_insg = nx_walk_and_build(nx_ctx->in_sg,\n\t\t\t\t\t\t    nx_ctx->ap->sglen,\n\t\t\t\t\t\t    req->src, processed,\n\t\t\t\t\t\t    &to_process);\n\n\t\t\tif ((to_process + processed) < assoclen) {\n\t\t\t\tNX_CPB_FDM(nx_ctx->csbcpb_aead) |=\n\t\t\t\t\tNX_FDM_INTERMEDIATE;\n\t\t\t} else {\n\t\t\t\tNX_CPB_FDM(nx_ctx->csbcpb_aead) &=\n\t\t\t\t\t~NX_FDM_INTERMEDIATE;\n\t\t\t}\n\n\n\t\t\tnx_ctx->op_aead.inlen = (nx_ctx->in_sg - nx_insg) *\n\t\t\t\t\t\tsizeof(struct nx_sg);\n\n\t\t\tresult = nx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0;\n\n\t\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op_aead,\n\t\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\n\t\t\tmemcpy(nx_ctx->csbcpb_aead->cpb.aes_cca.b0,\n\t\t\t\tnx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0,\n\t\t\t\tAES_BLOCK_SIZE);\n\n\t\t\tNX_CPB_FDM(nx_ctx->csbcpb_aead) |= NX_FDM_CONTINUATION;\n\n\t\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\t\tatomic64_add(assoclen, &nx_ctx->stats->aes_bytes);\n\n\t\t\tprocessed += to_process;\n\t\t} while (processed < assoclen);\n\n\t\tresult = nx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0;\n\t}\n\n\tmemcpy(out, result, AES_BLOCK_SIZE);\n\n\treturn rc;\n}\n\nstatic int ccm_nx_decrypt(struct aead_request   *req,\n\t\t\t  u8                    *iv,\n\t\t\t  unsigned int assoclen)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tunsigned int nbytes = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\n\tstruct nx_ccm_priv *priv = &nx_ctx->priv.ccm;\n\tunsigned long irq_flags;\n\tunsigned int processed = 0, to_process;\n\tint rc = -1;\n\n\tspin_lock_irqsave(&nx_ctx->lock, irq_flags);\n\n\tnbytes -= authsize;\n\n\t \n\tscatterwalk_map_and_copy(priv->oauth_tag,\n\t\t\t\t req->src, nbytes + req->assoclen, authsize,\n\t\t\t\t SCATTERWALK_FROM_SG);\n\n\trc = generate_pat(iv, req, nx_ctx, authsize, nbytes, assoclen,\n\t\t\t  csbcpb->cpb.aes_ccm.in_pat_or_b0);\n\tif (rc)\n\t\tgoto out;\n\n\tdo {\n\n\t\t \n\t\tto_process = nbytes - processed;\n\n\t\tif ((to_process + processed) < nbytes)\n\t\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\n\t\telse\n\t\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\n\n\t\tNX_CPB_FDM(nx_ctx->csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\n\n\t\trc = nx_build_sg_lists(nx_ctx, iv, req->dst, req->src,\n\t\t\t\t       &to_process, processed + req->assoclen,\n\t\t\t\t       csbcpb->cpb.aes_ccm.iv_or_ctr);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\t \n\t\tmemcpy(iv, csbcpb->cpb.aes_ccm.out_ctr, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_ccm.in_pat_or_b0,\n\t\t\tcsbcpb->cpb.aes_ccm.out_pat_or_mac, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_ccm.in_s0,\n\t\t\tcsbcpb->cpb.aes_ccm.out_s0, AES_BLOCK_SIZE);\n\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\n\n\t\t \n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(be32_to_cpu(csbcpb->csb.processed_byte_count),\n\t\t\t     &(nx_ctx->stats->aes_bytes));\n\n\t\tprocessed += to_process;\n\t} while (processed < nbytes);\n\n\trc = crypto_memneq(csbcpb->cpb.aes_ccm.out_pat_or_mac, priv->oauth_tag,\n\t\t    authsize) ? -EBADMSG : 0;\nout:\n\tspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\n\treturn rc;\n}\n\nstatic int ccm_nx_encrypt(struct aead_request   *req,\n\t\t\t  u8                    *iv,\n\t\t\t  unsigned int assoclen)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tunsigned int nbytes = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\n\tunsigned long irq_flags;\n\tunsigned int processed = 0, to_process;\n\tint rc = -1;\n\n\tspin_lock_irqsave(&nx_ctx->lock, irq_flags);\n\n\trc = generate_pat(iv, req, nx_ctx, authsize, nbytes, assoclen,\n\t\t\t  csbcpb->cpb.aes_ccm.in_pat_or_b0);\n\tif (rc)\n\t\tgoto out;\n\n\tdo {\n\t\t \n\t\tto_process = nbytes - processed;\n\n\t\tif ((to_process + processed) < nbytes)\n\t\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\n\t\telse\n\t\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\n\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\n\n\t\trc = nx_build_sg_lists(nx_ctx, iv, req->dst, req->src,\n\t\t\t\t       &to_process, processed + req->assoclen,\n\t\t\t\t       csbcpb->cpb.aes_ccm.iv_or_ctr);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\t \n\t\tmemcpy(iv, csbcpb->cpb.aes_ccm.out_ctr, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_ccm.in_pat_or_b0,\n\t\t\tcsbcpb->cpb.aes_ccm.out_pat_or_mac, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_ccm.in_s0,\n\t\t\tcsbcpb->cpb.aes_ccm.out_s0, AES_BLOCK_SIZE);\n\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\n\n\t\t \n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(be32_to_cpu(csbcpb->csb.processed_byte_count),\n\t\t\t     &(nx_ctx->stats->aes_bytes));\n\n\t\tprocessed += to_process;\n\n\t} while (processed < nbytes);\n\n\t \n\tscatterwalk_map_and_copy(csbcpb->cpb.aes_ccm.out_pat_or_mac,\n\t\t\t\t req->dst, nbytes + req->assoclen, authsize,\n\t\t\t\t SCATTERWALK_TO_SG);\n\nout:\n\tspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\n\treturn rc;\n}\n\nstatic int ccm4309_aes_nx_encrypt(struct aead_request *req)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tu8 *iv = rctx->iv;\n\n\tiv[0] = 3;\n\tmemcpy(iv + 1, nx_ctx->priv.ccm.nonce, 3);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\treturn ccm_nx_encrypt(req, iv, req->assoclen - 8);\n}\n\nstatic int ccm_aes_nx_encrypt(struct aead_request *req)\n{\n\tint rc;\n\n\trc = crypto_ccm_check_iv(req->iv);\n\tif (rc)\n\t\treturn rc;\n\n\treturn ccm_nx_encrypt(req, req->iv, req->assoclen);\n}\n\nstatic int ccm4309_aes_nx_decrypt(struct aead_request *req)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tu8 *iv = rctx->iv;\n\n\tiv[0] = 3;\n\tmemcpy(iv + 1, nx_ctx->priv.ccm.nonce, 3);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\treturn ccm_nx_decrypt(req, iv, req->assoclen - 8);\n}\n\nstatic int ccm_aes_nx_decrypt(struct aead_request *req)\n{\n\tint rc;\n\n\trc = crypto_ccm_check_iv(req->iv);\n\tif (rc)\n\t\treturn rc;\n\n\treturn ccm_nx_decrypt(req, req->iv, req->assoclen);\n}\n\nstruct aead_alg nx_ccm_aes_alg = {\n\t.base = {\n\t\t.cra_name        = \"ccm(aes)\",\n\t\t.cra_driver_name = \"ccm-aes-nx\",\n\t\t.cra_priority    = 300,\n\t\t.cra_flags       = CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize   = 1,\n\t\t.cra_ctxsize     = sizeof(struct nx_crypto_ctx),\n\t\t.cra_module      = THIS_MODULE,\n\t},\n\t.init        = nx_crypto_ctx_aes_ccm_init,\n\t.exit        = nx_crypto_ctx_aead_exit,\n\t.ivsize      = AES_BLOCK_SIZE,\n\t.maxauthsize = AES_BLOCK_SIZE,\n\t.setkey      = ccm_aes_nx_set_key,\n\t.setauthsize = ccm_aes_nx_setauthsize,\n\t.encrypt     = ccm_aes_nx_encrypt,\n\t.decrypt     = ccm_aes_nx_decrypt,\n};\n\nstruct aead_alg nx_ccm4309_aes_alg = {\n\t.base = {\n\t\t.cra_name        = \"rfc4309(ccm(aes))\",\n\t\t.cra_driver_name = \"rfc4309-ccm-aes-nx\",\n\t\t.cra_priority    = 300,\n\t\t.cra_flags       = CRYPTO_ALG_NEED_FALLBACK,\n\t\t.cra_blocksize   = 1,\n\t\t.cra_ctxsize     = sizeof(struct nx_crypto_ctx),\n\t\t.cra_module      = THIS_MODULE,\n\t},\n\t.init        = nx_crypto_ctx_aes_ccm_init,\n\t.exit        = nx_crypto_ctx_aead_exit,\n\t.ivsize      = 8,\n\t.maxauthsize = AES_BLOCK_SIZE,\n\t.setkey      = ccm4309_aes_nx_set_key,\n\t.setauthsize = ccm4309_aes_nx_setauthsize,\n\t.encrypt     = ccm4309_aes_nx_encrypt,\n\t.decrypt     = ccm4309_aes_nx_decrypt,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}