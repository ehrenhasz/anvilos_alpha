{
  "module_name": "nx-aes-gcm.c",
  "hash_id": "b7e56a1f72b8ec4fd4be2e425c5363f7acc17b5fe55c89a241096c5ec8984064",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/nx/nx-aes-gcm.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/aead.h>\n#include <crypto/aes.h>\n#include <crypto/algapi.h>\n#include <crypto/gcm.h>\n#include <crypto/scatterwalk.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <asm/vio.h>\n\n#include \"nx_csbcpb.h\"\n#include \"nx.h\"\n\n\nstatic int gcm_aes_nx_set_key(struct crypto_aead *tfm,\n\t\t\t      const u8           *in_key,\n\t\t\t      unsigned int        key_len)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_aead_ctx(tfm);\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\n\n\tnx_ctx_init(nx_ctx, HCOP_FC_AES);\n\n\tswitch (key_len) {\n\tcase AES_KEYSIZE_128:\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_128);\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_128);\n\t\tnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_128];\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_192);\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_192);\n\t\tnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_192];\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_256);\n\t\tNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_256);\n\t\tnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_256];\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\n\tmemcpy(csbcpb->cpb.aes_gcm.key, in_key, key_len);\n\n\tcsbcpb_aead->cpb.hdr.mode = NX_MODE_AES_GCA;\n\tmemcpy(csbcpb_aead->cpb.aes_gca.key, in_key, key_len);\n\n\treturn 0;\n}\n\nstatic int gcm4106_aes_nx_set_key(struct crypto_aead *tfm,\n\t\t\t\t  const u8           *in_key,\n\t\t\t\t  unsigned int        key_len)\n{\n\tstruct nx_crypto_ctx *nx_ctx = crypto_aead_ctx(tfm);\n\tchar *nonce = nx_ctx->priv.gcm.nonce;\n\tint rc;\n\n\tif (key_len < 4)\n\t\treturn -EINVAL;\n\n\tkey_len -= 4;\n\n\trc = gcm_aes_nx_set_key(tfm, in_key, key_len);\n\tif (rc)\n\t\tgoto out;\n\n\tmemcpy(nonce, in_key + key_len, 4);\nout:\n\treturn rc;\n}\n\nstatic int gcm4106_aes_nx_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t      unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int nx_gca(struct nx_crypto_ctx  *nx_ctx,\n\t\t  struct aead_request   *req,\n\t\t  u8                    *out,\n\t\t  unsigned int assoclen)\n{\n\tint rc;\n\tstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\n\tstruct scatter_walk walk;\n\tstruct nx_sg *nx_sg = nx_ctx->in_sg;\n\tunsigned int nbytes = assoclen;\n\tunsigned int processed = 0, to_process;\n\tunsigned int max_sg_len;\n\n\tif (nbytes <= AES_BLOCK_SIZE) {\n\t\tscatterwalk_start(&walk, req->src);\n\t\tscatterwalk_copychunks(out, &walk, nbytes, SCATTERWALK_FROM_SG);\n\t\tscatterwalk_done(&walk, SCATTERWALK_FROM_SG, 0);\n\t\treturn 0;\n\t}\n\n\tNX_CPB_FDM(csbcpb_aead) &= ~NX_FDM_CONTINUATION;\n\n\t \n\tmax_sg_len = min_t(u64, nx_driver.of.max_sg_len/sizeof(struct nx_sg),\n\t\t\t   nx_ctx->ap->sglen);\n\tmax_sg_len = min_t(u64, max_sg_len,\n\t\t\t   nx_ctx->ap->databytelen/NX_PAGE_SIZE);\n\n\tdo {\n\t\t \n\t\tto_process = min_t(u64, nbytes - processed,\n\t\t\t\t   nx_ctx->ap->databytelen);\n\t\tto_process = min_t(u64, to_process,\n\t\t\t\t   NX_PAGE_SIZE * (max_sg_len - 1));\n\n\t\tnx_sg = nx_walk_and_build(nx_ctx->in_sg, max_sg_len,\n\t\t\t\t\t  req->src, processed, &to_process);\n\n\t\tif ((to_process + processed) < nbytes)\n\t\t\tNX_CPB_FDM(csbcpb_aead) |= NX_FDM_INTERMEDIATE;\n\t\telse\n\t\t\tNX_CPB_FDM(csbcpb_aead) &= ~NX_FDM_INTERMEDIATE;\n\n\t\tnx_ctx->op_aead.inlen = (nx_ctx->in_sg - nx_sg)\n\t\t\t\t\t* sizeof(struct nx_sg);\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op_aead,\n\t\t\t\treq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tmemcpy(csbcpb_aead->cpb.aes_gca.in_pat,\n\t\t\t\tcsbcpb_aead->cpb.aes_gca.out_pat,\n\t\t\t\tAES_BLOCK_SIZE);\n\t\tNX_CPB_FDM(csbcpb_aead) |= NX_FDM_CONTINUATION;\n\n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(assoclen, &(nx_ctx->stats->aes_bytes));\n\n\t\tprocessed += to_process;\n\t} while (processed < nbytes);\n\n\tmemcpy(out, csbcpb_aead->cpb.aes_gca.out_pat, AES_BLOCK_SIZE);\n\n\treturn rc;\n}\n\nstatic int gmac(struct aead_request *req, const u8 *iv, unsigned int assoclen)\n{\n\tint rc;\n\tstruct nx_crypto_ctx *nx_ctx =\n\t\tcrypto_aead_ctx(crypto_aead_reqtfm(req));\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tstruct nx_sg *nx_sg;\n\tunsigned int nbytes = assoclen;\n\tunsigned int processed = 0, to_process;\n\tunsigned int max_sg_len;\n\n\t \n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_GMAC;\n\n\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_CONTINUATION;\n\n\t \n\tmax_sg_len = min_t(u64, nx_driver.of.max_sg_len/sizeof(struct nx_sg),\n\t\t\t   nx_ctx->ap->sglen);\n\tmax_sg_len = min_t(u64, max_sg_len,\n\t\t\t   nx_ctx->ap->databytelen/NX_PAGE_SIZE);\n\n\t \n\tmemcpy(csbcpb->cpb.aes_gcm.iv_or_cnt, iv, AES_BLOCK_SIZE);\n\n\tdo {\n\t\t \n\t\tto_process = min_t(u64, nbytes - processed,\n\t\t\t\t   nx_ctx->ap->databytelen);\n\t\tto_process = min_t(u64, to_process,\n\t\t\t\t   NX_PAGE_SIZE * (max_sg_len - 1));\n\n\t\tnx_sg = nx_walk_and_build(nx_ctx->in_sg, max_sg_len,\n\t\t\t\t\t  req->src, processed, &to_process);\n\n\t\tif ((to_process + processed) < nbytes)\n\t\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\n\t\telse\n\t\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\n\n\t\tnx_ctx->op.inlen = (nx_ctx->in_sg - nx_sg)\n\t\t\t\t\t* sizeof(struct nx_sg);\n\n\t\tcsbcpb->cpb.aes_gcm.bit_length_data = 0;\n\t\tcsbcpb->cpb.aes_gcm.bit_length_aad = 8 * nbytes;\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t\treq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\tmemcpy(csbcpb->cpb.aes_gcm.in_pat_or_aad,\n\t\t\tcsbcpb->cpb.aes_gcm.out_pat_or_mac, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_gcm.in_s0,\n\t\t\tcsbcpb->cpb.aes_gcm.out_s0, AES_BLOCK_SIZE);\n\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\n\n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(assoclen, &(nx_ctx->stats->aes_bytes));\n\n\t\tprocessed += to_process;\n\t} while (processed < nbytes);\n\nout:\n\t \n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\n\treturn rc;\n}\n\nstatic int gcm_empty(struct aead_request *req, const u8 *iv, int enc)\n{\n\tint rc;\n\tstruct nx_crypto_ctx *nx_ctx =\n\t\tcrypto_aead_ctx(crypto_aead_reqtfm(req));\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tchar out[AES_BLOCK_SIZE];\n\tstruct nx_sg *in_sg, *out_sg;\n\tint len;\n\n\t \n\n\t \n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_ECB;\n\tmemcpy(csbcpb->cpb.aes_ecb.key, csbcpb->cpb.aes_gcm.key,\n\t\t\tsizeof(csbcpb->cpb.aes_ecb.key));\n\tif (enc)\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\n\telse\n\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\n\n\tlen = AES_BLOCK_SIZE;\n\n\t \n\tin_sg = nx_build_sg_list(nx_ctx->in_sg, (u8 *) iv,\n\t\t\t\t &len, nx_ctx->ap->sglen);\n\n\tif (len != AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tlen = sizeof(out);\n\tout_sg = nx_build_sg_list(nx_ctx->out_sg, (u8 *) out, &len,\n\t\t\t\t  nx_ctx->ap->sglen);\n\n\tif (len != sizeof(out))\n\t\treturn -EINVAL;\n\n\tnx_ctx->op.inlen = (nx_ctx->in_sg - in_sg) * sizeof(struct nx_sg);\n\tnx_ctx->op.outlen = (nx_ctx->out_sg - out_sg) * sizeof(struct nx_sg);\n\n\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\tif (rc)\n\t\tgoto out;\n\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\n\t \n\tmemcpy(csbcpb->cpb.aes_gcm.out_pat_or_mac, out,\n\t\t\tcrypto_aead_authsize(crypto_aead_reqtfm(req)));\nout:\n\t \n\tcsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\n\n\t \n\tmemset(csbcpb->cpb.aes_ecb.key, 0, sizeof(csbcpb->cpb.aes_ecb.key));\n\n\treturn rc;\n}\n\nstatic int gcm_aes_nx_crypt(struct aead_request *req, int enc,\n\t\t\t    unsigned int assoclen)\n{\n\tstruct nx_crypto_ctx *nx_ctx =\n\t\tcrypto_aead_ctx(crypto_aead_reqtfm(req));\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\n\tunsigned int nbytes = req->cryptlen;\n\tunsigned int processed = 0, to_process;\n\tunsigned long irq_flags;\n\tint rc = -EINVAL;\n\n\tspin_lock_irqsave(&nx_ctx->lock, irq_flags);\n\n\t \n\t*(u32 *)&rctx->iv[NX_GCM_CTR_OFFSET] = 1;\n\n\tif (nbytes == 0) {\n\t\tif (assoclen == 0)\n\t\t\trc = gcm_empty(req, rctx->iv, enc);\n\t\telse\n\t\t\trc = gmac(req, rctx->iv, assoclen);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\telse\n\t\t\tgoto mac;\n\t}\n\n\t \n\tcsbcpb->cpb.aes_gcm.bit_length_aad = assoclen * 8;\n\tif (assoclen) {\n\t\trc = nx_gca(nx_ctx, req, csbcpb->cpb.aes_gcm.in_pat_or_aad,\n\t\t\t    assoclen);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\t \n\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_CONTINUATION;\n\tif (enc) {\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\n\t} else {\n\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\n\t\tnbytes -= crypto_aead_authsize(crypto_aead_reqtfm(req));\n\t}\n\n\tdo {\n\t\tto_process = nbytes - processed;\n\n\t\tcsbcpb->cpb.aes_gcm.bit_length_data = nbytes * 8;\n\t\trc = nx_build_sg_lists(nx_ctx, rctx->iv, req->dst,\n\t\t\t\t       req->src, &to_process,\n\t\t\t\t       processed + req->assoclen,\n\t\t\t\t       csbcpb->cpb.aes_gcm.iv_or_cnt);\n\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\tif ((to_process + processed) < nbytes)\n\t\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\n\t\telse\n\t\t\tNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\n\n\n\t\trc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\n\t\t\t\t   req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\tmemcpy(rctx->iv, csbcpb->cpb.aes_gcm.out_cnt, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_gcm.in_pat_or_aad,\n\t\t\tcsbcpb->cpb.aes_gcm.out_pat_or_mac, AES_BLOCK_SIZE);\n\t\tmemcpy(csbcpb->cpb.aes_gcm.in_s0,\n\t\t\tcsbcpb->cpb.aes_gcm.out_s0, AES_BLOCK_SIZE);\n\n\t\tNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\n\n\t\tatomic_inc(&(nx_ctx->stats->aes_ops));\n\t\tatomic64_add(be32_to_cpu(csbcpb->csb.processed_byte_count),\n\t\t\t     &(nx_ctx->stats->aes_bytes));\n\n\t\tprocessed += to_process;\n\t} while (processed < nbytes);\n\nmac:\n\tif (enc) {\n\t\t \n\t\tscatterwalk_map_and_copy(\n\t\t\tcsbcpb->cpb.aes_gcm.out_pat_or_mac,\n\t\t\treq->dst, req->assoclen + nbytes,\n\t\t\tcrypto_aead_authsize(crypto_aead_reqtfm(req)),\n\t\t\tSCATTERWALK_TO_SG);\n\t} else {\n\t\tu8 *itag = nx_ctx->priv.gcm.iauth_tag;\n\t\tu8 *otag = csbcpb->cpb.aes_gcm.out_pat_or_mac;\n\n\t\tscatterwalk_map_and_copy(\n\t\t\titag, req->src, req->assoclen + nbytes,\n\t\t\tcrypto_aead_authsize(crypto_aead_reqtfm(req)),\n\t\t\tSCATTERWALK_FROM_SG);\n\t\trc = crypto_memneq(itag, otag,\n\t\t\t    crypto_aead_authsize(crypto_aead_reqtfm(req))) ?\n\t\t     -EBADMSG : 0;\n\t}\nout:\n\tspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\n\treturn rc;\n}\n\nstatic int gcm_aes_nx_encrypt(struct aead_request *req)\n{\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tchar *iv = rctx->iv;\n\n\tmemcpy(iv, req->iv, GCM_AES_IV_SIZE);\n\n\treturn gcm_aes_nx_crypt(req, 1, req->assoclen);\n}\n\nstatic int gcm_aes_nx_decrypt(struct aead_request *req)\n{\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tchar *iv = rctx->iv;\n\n\tmemcpy(iv, req->iv, GCM_AES_IV_SIZE);\n\n\treturn gcm_aes_nx_crypt(req, 0, req->assoclen);\n}\n\nstatic int gcm4106_aes_nx_encrypt(struct aead_request *req)\n{\n\tstruct nx_crypto_ctx *nx_ctx =\n\t\tcrypto_aead_ctx(crypto_aead_reqtfm(req));\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tchar *iv = rctx->iv;\n\tchar *nonce = nx_ctx->priv.gcm.nonce;\n\n\tmemcpy(iv, nonce, NX_GCM4106_NONCE_LEN);\n\tmemcpy(iv + NX_GCM4106_NONCE_LEN, req->iv, 8);\n\n\tif (req->assoclen < 8)\n\t\treturn -EINVAL;\n\n\treturn gcm_aes_nx_crypt(req, 1, req->assoclen - 8);\n}\n\nstatic int gcm4106_aes_nx_decrypt(struct aead_request *req)\n{\n\tstruct nx_crypto_ctx *nx_ctx =\n\t\tcrypto_aead_ctx(crypto_aead_reqtfm(req));\n\tstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\n\tchar *iv = rctx->iv;\n\tchar *nonce = nx_ctx->priv.gcm.nonce;\n\n\tmemcpy(iv, nonce, NX_GCM4106_NONCE_LEN);\n\tmemcpy(iv + NX_GCM4106_NONCE_LEN, req->iv, 8);\n\n\tif (req->assoclen < 8)\n\t\treturn -EINVAL;\n\n\treturn gcm_aes_nx_crypt(req, 0, req->assoclen - 8);\n}\n\nstruct aead_alg nx_gcm_aes_alg = {\n\t.base = {\n\t\t.cra_name        = \"gcm(aes)\",\n\t\t.cra_driver_name = \"gcm-aes-nx\",\n\t\t.cra_priority    = 300,\n\t\t.cra_blocksize   = 1,\n\t\t.cra_ctxsize     = sizeof(struct nx_crypto_ctx),\n\t\t.cra_module      = THIS_MODULE,\n\t},\n\t.init        = nx_crypto_ctx_aes_gcm_init,\n\t.exit        = nx_crypto_ctx_aead_exit,\n\t.ivsize      = GCM_AES_IV_SIZE,\n\t.maxauthsize = AES_BLOCK_SIZE,\n\t.setkey      = gcm_aes_nx_set_key,\n\t.encrypt     = gcm_aes_nx_encrypt,\n\t.decrypt     = gcm_aes_nx_decrypt,\n};\n\nstruct aead_alg nx_gcm4106_aes_alg = {\n\t.base = {\n\t\t.cra_name        = \"rfc4106(gcm(aes))\",\n\t\t.cra_driver_name = \"rfc4106-gcm-aes-nx\",\n\t\t.cra_priority    = 300,\n\t\t.cra_blocksize   = 1,\n\t\t.cra_ctxsize     = sizeof(struct nx_crypto_ctx),\n\t\t.cra_module      = THIS_MODULE,\n\t},\n\t.init        = nx_crypto_ctx_aes_gcm_init,\n\t.exit        = nx_crypto_ctx_aead_exit,\n\t.ivsize      = GCM_RFC4106_IV_SIZE,\n\t.maxauthsize = AES_BLOCK_SIZE,\n\t.setkey      = gcm4106_aes_nx_set_key,\n\t.setauthsize = gcm4106_aes_nx_setauthsize,\n\t.encrypt     = gcm4106_aes_nx_encrypt,\n\t.decrypt     = gcm4106_aes_nx_decrypt,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}