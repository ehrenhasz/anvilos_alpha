{
  "module_name": "aspeed-hace-crypto.c",
  "hash_id": "db84387dc4d9f0f32aca39ea0f917ad30731acc3e44fc55e0582cdff94fb4c4f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/aspeed/aspeed-hace-crypto.c",
  "human_readable_source": "\n \n\n#include \"aspeed-hace.h\"\n#include <crypto/des.h>\n#include <crypto/engine.h>\n#include <crypto/internal/des.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/string.h>\n\n#ifdef CONFIG_CRYPTO_DEV_ASPEED_HACE_CRYPTO_DEBUG\n#define CIPHER_DBG(h, fmt, ...)\t\\\n\tdev_info((h)->dev, \"%s() \" fmt, __func__, ##__VA_ARGS__)\n#else\n#define CIPHER_DBG(h, fmt, ...)\t\\\n\tdev_dbg((h)->dev, \"%s() \" fmt, __func__, ##__VA_ARGS__)\n#endif\n\nstatic int aspeed_crypto_do_fallback(struct skcipher_request *areq)\n{\n\tstruct aspeed_cipher_reqctx *rctx = skcipher_request_ctx(areq);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(areq);\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint err;\n\n\tskcipher_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\n\tskcipher_request_set_callback(&rctx->fallback_req, areq->base.flags,\n\t\t\t\t      areq->base.complete, areq->base.data);\n\tskcipher_request_set_crypt(&rctx->fallback_req, areq->src, areq->dst,\n\t\t\t\t   areq->cryptlen, areq->iv);\n\n\tif (rctx->enc_cmd & HACE_CMD_ENCRYPT)\n\t\terr = crypto_skcipher_encrypt(&rctx->fallback_req);\n\telse\n\t\terr = crypto_skcipher_decrypt(&rctx->fallback_req);\n\n\treturn err;\n}\n\nstatic bool aspeed_crypto_need_fallback(struct skcipher_request *areq)\n{\n\tstruct aspeed_cipher_reqctx *rctx = skcipher_request_ctx(areq);\n\n\tif (areq->cryptlen == 0)\n\t\treturn true;\n\n\tif ((rctx->enc_cmd & HACE_CMD_DES_SELECT) &&\n\t    !IS_ALIGNED(areq->cryptlen, DES_BLOCK_SIZE))\n\t\treturn true;\n\n\tif ((!(rctx->enc_cmd & HACE_CMD_DES_SELECT)) &&\n\t    !IS_ALIGNED(areq->cryptlen, AES_BLOCK_SIZE))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int aspeed_hace_crypto_handle_queue(struct aspeed_hace_dev *hace_dev,\n\t\t\t\t\t   struct skcipher_request *req)\n{\n\tif (hace_dev->version == AST2500_VERSION &&\n\t    aspeed_crypto_need_fallback(req)) {\n\t\tCIPHER_DBG(hace_dev, \"SW fallback\\n\");\n\t\treturn aspeed_crypto_do_fallback(req);\n\t}\n\n\treturn crypto_transfer_skcipher_request_to_engine(\n\t\t\thace_dev->crypt_engine_crypto, req);\n}\n\nstatic int aspeed_crypto_do_request(struct crypto_engine *engine, void *areq)\n{\n\tstruct skcipher_request *req = skcipher_request_cast(areq);\n\tstruct crypto_skcipher *cipher = crypto_skcipher_reqtfm(req);\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\tstruct aspeed_engine_crypto *crypto_engine;\n\tint rc;\n\n\tcrypto_engine = &hace_dev->crypto_engine;\n\tcrypto_engine->req = req;\n\tcrypto_engine->flags |= CRYPTO_FLAGS_BUSY;\n\n\trc = ctx->start(hace_dev);\n\n\tif (rc != -EINPROGRESS)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic int aspeed_sk_complete(struct aspeed_hace_dev *hace_dev, int err)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct skcipher_request *req;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\n\tif (rctx->enc_cmd & HACE_CMD_IV_REQUIRE) {\n\t\tif (rctx->enc_cmd & HACE_CMD_DES_SELECT)\n\t\t\tmemcpy(req->iv, crypto_engine->cipher_ctx +\n\t\t\t       DES_KEY_SIZE, DES_KEY_SIZE);\n\t\telse\n\t\t\tmemcpy(req->iv, crypto_engine->cipher_ctx,\n\t\t\t       AES_BLOCK_SIZE);\n\t}\n\n\tcrypto_engine->flags &= ~CRYPTO_FLAGS_BUSY;\n\n\tcrypto_finalize_skcipher_request(hace_dev->crypt_engine_crypto, req,\n\t\t\t\t\t err);\n\n\treturn err;\n}\n\nstatic int aspeed_sk_transfer_sg(struct aspeed_hace_dev *hace_dev)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct device *dev = hace_dev->dev;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct skcipher_request *req;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\n\tif (req->src == req->dst) {\n\t\tdma_unmap_sg(dev, req->src, rctx->src_nents, DMA_BIDIRECTIONAL);\n\t} else {\n\t\tdma_unmap_sg(dev, req->src, rctx->src_nents, DMA_TO_DEVICE);\n\t\tdma_unmap_sg(dev, req->dst, rctx->dst_nents, DMA_FROM_DEVICE);\n\t}\n\n\treturn aspeed_sk_complete(hace_dev, 0);\n}\n\nstatic int aspeed_sk_transfer(struct aspeed_hace_dev *hace_dev)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct skcipher_request *req;\n\tstruct scatterlist *out_sg;\n\tint nbytes = 0;\n\tint rc = 0;\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\tout_sg = req->dst;\n\n\t \n\tnbytes = sg_copy_from_buffer(out_sg, rctx->dst_nents,\n\t\t\t\t     crypto_engine->cipher_addr, req->cryptlen);\n\tif (!nbytes) {\n\t\tdev_warn(hace_dev->dev, \"invalid sg copy, %s:0x%x, %s:0x%x\\n\",\n\t\t\t \"nbytes\", nbytes, \"cryptlen\", req->cryptlen);\n\t\trc = -EINVAL;\n\t}\n\n\tCIPHER_DBG(hace_dev, \"%s:%d, %s:%d, %s:%d, %s:%p\\n\",\n\t\t   \"nbytes\", nbytes, \"req->cryptlen\", req->cryptlen,\n\t\t   \"nb_out_sg\", rctx->dst_nents,\n\t\t   \"cipher addr\", crypto_engine->cipher_addr);\n\n\treturn aspeed_sk_complete(hace_dev, rc);\n}\n\nstatic int aspeed_sk_start(struct aspeed_hace_dev *hace_dev)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct skcipher_request *req;\n\tstruct scatterlist *in_sg;\n\tint nbytes;\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\tin_sg = req->src;\n\n\tnbytes = sg_copy_to_buffer(in_sg, rctx->src_nents,\n\t\t\t\t   crypto_engine->cipher_addr, req->cryptlen);\n\n\tCIPHER_DBG(hace_dev, \"%s:%d, %s:%d, %s:%d, %s:%p\\n\",\n\t\t   \"nbytes\", nbytes, \"req->cryptlen\", req->cryptlen,\n\t\t   \"nb_in_sg\", rctx->src_nents,\n\t\t   \"cipher addr\", crypto_engine->cipher_addr);\n\n\tif (!nbytes) {\n\t\tdev_warn(hace_dev->dev, \"invalid sg copy, %s:0x%x, %s:0x%x\\n\",\n\t\t\t \"nbytes\", nbytes, \"cryptlen\", req->cryptlen);\n\t\treturn -EINVAL;\n\t}\n\n\tcrypto_engine->resume = aspeed_sk_transfer;\n\n\t \n\tast_hace_write(hace_dev, crypto_engine->cipher_dma_addr,\n\t\t       ASPEED_HACE_SRC);\n\tast_hace_write(hace_dev, crypto_engine->cipher_dma_addr,\n\t\t       ASPEED_HACE_DEST);\n\tast_hace_write(hace_dev, req->cryptlen, ASPEED_HACE_DATA_LEN);\n\tast_hace_write(hace_dev, rctx->enc_cmd, ASPEED_HACE_CMD);\n\n\treturn -EINPROGRESS;\n}\n\nstatic int aspeed_sk_start_sg(struct aspeed_hace_dev *hace_dev)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct aspeed_sg_list *src_list, *dst_list;\n\tdma_addr_t src_dma_addr, dst_dma_addr;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct skcipher_request *req;\n\tstruct scatterlist *s;\n\tint src_sg_len;\n\tint dst_sg_len;\n\tint total, i;\n\tint rc;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\n\trctx->enc_cmd |= HACE_CMD_DES_SG_CTRL | HACE_CMD_SRC_SG_CTRL |\n\t\t\t HACE_CMD_AES_KEY_HW_EXP | HACE_CMD_MBUS_REQ_SYNC_EN;\n\n\t \n\tif (req->dst == req->src) {\n\t\tsrc_sg_len = dma_map_sg(hace_dev->dev, req->src,\n\t\t\t\t\trctx->src_nents, DMA_BIDIRECTIONAL);\n\t\tdst_sg_len = src_sg_len;\n\t\tif (!src_sg_len) {\n\t\t\tdev_warn(hace_dev->dev, \"dma_map_sg() src error\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t} else {\n\t\tsrc_sg_len = dma_map_sg(hace_dev->dev, req->src,\n\t\t\t\t\trctx->src_nents, DMA_TO_DEVICE);\n\t\tif (!src_sg_len) {\n\t\t\tdev_warn(hace_dev->dev, \"dma_map_sg() src error\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdst_sg_len = dma_map_sg(hace_dev->dev, req->dst,\n\t\t\t\t\trctx->dst_nents, DMA_FROM_DEVICE);\n\t\tif (!dst_sg_len) {\n\t\t\tdev_warn(hace_dev->dev, \"dma_map_sg() dst error\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_req_src;\n\t\t}\n\t}\n\n\tsrc_list = (struct aspeed_sg_list *)crypto_engine->cipher_addr;\n\tsrc_dma_addr = crypto_engine->cipher_dma_addr;\n\ttotal = req->cryptlen;\n\n\tfor_each_sg(req->src, s, src_sg_len, i) {\n\t\tu32 phy_addr = sg_dma_address(s);\n\t\tu32 len = sg_dma_len(s);\n\n\t\tif (total > len)\n\t\t\ttotal -= len;\n\t\telse {\n\t\t\t \n\t\t\tlen = total;\n\t\t\tlen |= BIT(31);\n\t\t\ttotal = 0;\n\t\t}\n\n\t\tsrc_list[i].phy_addr = cpu_to_le32(phy_addr);\n\t\tsrc_list[i].len = cpu_to_le32(len);\n\t}\n\n\tif (total != 0) {\n\t\trc = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\tif (req->dst == req->src) {\n\t\tdst_list = src_list;\n\t\tdst_dma_addr = src_dma_addr;\n\n\t} else {\n\t\tdst_list = (struct aspeed_sg_list *)crypto_engine->dst_sg_addr;\n\t\tdst_dma_addr = crypto_engine->dst_sg_dma_addr;\n\t\ttotal = req->cryptlen;\n\n\t\tfor_each_sg(req->dst, s, dst_sg_len, i) {\n\t\t\tu32 phy_addr = sg_dma_address(s);\n\t\t\tu32 len = sg_dma_len(s);\n\n\t\t\tif (total > len)\n\t\t\t\ttotal -= len;\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tlen = total;\n\t\t\t\tlen |= BIT(31);\n\t\t\t\ttotal = 0;\n\t\t\t}\n\n\t\t\tdst_list[i].phy_addr = cpu_to_le32(phy_addr);\n\t\t\tdst_list[i].len = cpu_to_le32(len);\n\n\t\t}\n\n\t\tdst_list[dst_sg_len].phy_addr = 0;\n\t\tdst_list[dst_sg_len].len = 0;\n\t}\n\n\tif (total != 0) {\n\t\trc = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\tcrypto_engine->resume = aspeed_sk_transfer_sg;\n\n\t \n\tmb();\n\n\t \n\tast_hace_write(hace_dev, src_dma_addr, ASPEED_HACE_SRC);\n\tast_hace_write(hace_dev, dst_dma_addr, ASPEED_HACE_DEST);\n\tast_hace_write(hace_dev, req->cryptlen, ASPEED_HACE_DATA_LEN);\n\tast_hace_write(hace_dev, rctx->enc_cmd, ASPEED_HACE_CMD);\n\n\treturn -EINPROGRESS;\n\nfree_req:\n\tif (req->dst == req->src) {\n\t\tdma_unmap_sg(hace_dev->dev, req->src, rctx->src_nents,\n\t\t\t     DMA_BIDIRECTIONAL);\n\n\t} else {\n\t\tdma_unmap_sg(hace_dev->dev, req->dst, rctx->dst_nents,\n\t\t\t     DMA_TO_DEVICE);\n\t\tdma_unmap_sg(hace_dev->dev, req->src, rctx->src_nents,\n\t\t\t     DMA_TO_DEVICE);\n\t}\n\n\treturn rc;\n\nfree_req_src:\n\tdma_unmap_sg(hace_dev->dev, req->src, rctx->src_nents, DMA_TO_DEVICE);\n\n\treturn rc;\n}\n\nstatic int aspeed_hace_skcipher_trigger(struct aspeed_hace_dev *hace_dev)\n{\n\tstruct aspeed_engine_crypto *crypto_engine = &hace_dev->crypto_engine;\n\tstruct aspeed_cipher_reqctx *rctx;\n\tstruct crypto_skcipher *cipher;\n\tstruct aspeed_cipher_ctx *ctx;\n\tstruct skcipher_request *req;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\treq = crypto_engine->req;\n\trctx = skcipher_request_ctx(req);\n\tcipher = crypto_skcipher_reqtfm(req);\n\tctx = crypto_skcipher_ctx(cipher);\n\n\t \n\trctx->enc_cmd |= HACE_CMD_ISR_EN;\n\n\trctx->dst_nents = sg_nents(req->dst);\n\trctx->src_nents = sg_nents(req->src);\n\n\tast_hace_write(hace_dev, crypto_engine->cipher_ctx_dma,\n\t\t       ASPEED_HACE_CONTEXT);\n\n\tif (rctx->enc_cmd & HACE_CMD_IV_REQUIRE) {\n\t\tif (rctx->enc_cmd & HACE_CMD_DES_SELECT)\n\t\t\tmemcpy(crypto_engine->cipher_ctx + DES_BLOCK_SIZE,\n\t\t\t       req->iv, DES_BLOCK_SIZE);\n\t\telse\n\t\t\tmemcpy(crypto_engine->cipher_ctx, req->iv,\n\t\t\t       AES_BLOCK_SIZE);\n\t}\n\n\tif (hace_dev->version == AST2600_VERSION) {\n\t\tmemcpy(crypto_engine->cipher_ctx + 16, ctx->key, ctx->key_len);\n\n\t\treturn aspeed_sk_start_sg(hace_dev);\n\t}\n\n\tmemcpy(crypto_engine->cipher_ctx + 16, ctx->key, AES_MAX_KEYLENGTH);\n\n\treturn aspeed_sk_start(hace_dev);\n}\n\nstatic int aspeed_des_crypt(struct skcipher_request *req, u32 cmd)\n{\n\tstruct aspeed_cipher_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *cipher = crypto_skcipher_reqtfm(req);\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\tu32 crypto_alg = cmd & HACE_CMD_OP_MODE_MASK;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\tif (crypto_alg == HACE_CMD_CBC || crypto_alg == HACE_CMD_ECB) {\n\t\tif (!IS_ALIGNED(req->cryptlen, DES_BLOCK_SIZE))\n\t\t\treturn -EINVAL;\n\t}\n\n\trctx->enc_cmd = cmd | HACE_CMD_DES_SELECT | HACE_CMD_RI_WO_DATA_ENABLE |\n\t\t\tHACE_CMD_DES | HACE_CMD_CONTEXT_LOAD_ENABLE |\n\t\t\tHACE_CMD_CONTEXT_SAVE_ENABLE;\n\n\treturn aspeed_hace_crypto_handle_queue(hace_dev, req);\n}\n\nstatic int aspeed_des_setkey(struct crypto_skcipher *cipher, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(cipher);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\tint rc;\n\n\tCIPHER_DBG(hace_dev, \"keylen: %d bits\\n\", keylen);\n\n\tif (keylen != DES_KEY_SIZE && keylen != DES3_EDE_KEY_SIZE) {\n\t\tdev_warn(hace_dev->dev, \"invalid keylen: %d bits\\n\", keylen);\n\t\treturn -EINVAL;\n\t}\n\n\tif (keylen == DES_KEY_SIZE) {\n\t\trc = crypto_des_verify_key(tfm, key);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t} else if (keylen == DES3_EDE_KEY_SIZE) {\n\t\trc = crypto_des3_ede_verify_key(tfm, key);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tmemcpy(ctx->key, key, keylen);\n\tctx->key_len = keylen;\n\n\tcrypto_skcipher_clear_flags(ctx->fallback_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(ctx->fallback_tfm, cipher->base.crt_flags &\n\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\n\treturn crypto_skcipher_setkey(ctx->fallback_tfm, key, keylen);\n}\n\nstatic int aspeed_tdes_ctr_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CTR |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_ctr_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CTR |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_ofb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_OFB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_ofb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_OFB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_cfb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CFB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_cfb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CFB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CBC |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CBC |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_ECB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_tdes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_ECB |\n\t\t\t\tHACE_CMD_TRIPLE_DES);\n}\n\nstatic int aspeed_des_ctr_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CTR |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_ctr_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CTR |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_ofb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_OFB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_ofb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_OFB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_cfb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CFB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_cfb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CFB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CBC |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CBC |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_ECB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_des_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_des_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_ECB |\n\t\t\t\tHACE_CMD_SINGLE_DES);\n}\n\nstatic int aspeed_aes_crypt(struct skcipher_request *req, u32 cmd)\n{\n\tstruct aspeed_cipher_reqctx *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *cipher = crypto_skcipher_reqtfm(req);\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\tu32 crypto_alg = cmd & HACE_CMD_OP_MODE_MASK;\n\n\tif (crypto_alg == HACE_CMD_CBC || crypto_alg == HACE_CMD_ECB) {\n\t\tif (!IS_ALIGNED(req->cryptlen, AES_BLOCK_SIZE))\n\t\t\treturn -EINVAL;\n\t}\n\n\tCIPHER_DBG(hace_dev, \"%s\\n\",\n\t\t   (cmd & HACE_CMD_ENCRYPT) ? \"encrypt\" : \"decrypt\");\n\n\tcmd |= HACE_CMD_AES_SELECT | HACE_CMD_RI_WO_DATA_ENABLE |\n\t       HACE_CMD_CONTEXT_LOAD_ENABLE | HACE_CMD_CONTEXT_SAVE_ENABLE;\n\n\tswitch (ctx->key_len) {\n\tcase AES_KEYSIZE_128:\n\t\tcmd |= HACE_CMD_AES128;\n\t\tbreak;\n\tcase AES_KEYSIZE_192:\n\t\tcmd |= HACE_CMD_AES192;\n\t\tbreak;\n\tcase AES_KEYSIZE_256:\n\t\tcmd |= HACE_CMD_AES256;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\trctx->enc_cmd = cmd;\n\n\treturn aspeed_hace_crypto_handle_queue(hace_dev, req);\n}\n\nstatic int aspeed_aes_setkey(struct crypto_skcipher *cipher, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(cipher);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\tstruct crypto_aes_ctx gen_aes_key;\n\n\tCIPHER_DBG(hace_dev, \"keylen: %d bits\\n\", (keylen * 8));\n\n\tif (keylen != AES_KEYSIZE_128 && keylen != AES_KEYSIZE_192 &&\n\t    keylen != AES_KEYSIZE_256)\n\t\treturn -EINVAL;\n\n\tif (ctx->hace_dev->version == AST2500_VERSION) {\n\t\taes_expandkey(&gen_aes_key, key, keylen);\n\t\tmemcpy(ctx->key, gen_aes_key.key_enc, AES_MAX_KEYLENGTH);\n\n\t} else {\n\t\tmemcpy(ctx->key, key, keylen);\n\t}\n\n\tctx->key_len = keylen;\n\n\tcrypto_skcipher_clear_flags(ctx->fallback_tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(ctx->fallback_tfm, cipher->base.crt_flags &\n\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\n\treturn crypto_skcipher_setkey(ctx->fallback_tfm, key, keylen);\n}\n\nstatic int aspeed_aes_ctr_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CTR);\n}\n\nstatic int aspeed_aes_ctr_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CTR);\n}\n\nstatic int aspeed_aes_ofb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_OFB);\n}\n\nstatic int aspeed_aes_ofb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_OFB);\n}\n\nstatic int aspeed_aes_cfb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CFB);\n}\n\nstatic int aspeed_aes_cfb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CFB);\n}\n\nstatic int aspeed_aes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_CBC);\n}\n\nstatic int aspeed_aes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_CBC);\n}\n\nstatic int aspeed_aes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_DECRYPT | HACE_CMD_ECB);\n}\n\nstatic int aspeed_aes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn aspeed_aes_crypt(req, HACE_CMD_ENCRYPT | HACE_CMD_ECB);\n}\n\nstatic int aspeed_crypto_cra_init(struct crypto_skcipher *tfm)\n{\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tconst char *name = crypto_tfm_alg_name(&tfm->base);\n\tstruct aspeed_hace_alg *crypto_alg;\n\n\n\tcrypto_alg = container_of(alg, struct aspeed_hace_alg, alg.skcipher.base);\n\tctx->hace_dev = crypto_alg->hace_dev;\n\tctx->start = aspeed_hace_skcipher_trigger;\n\n\tCIPHER_DBG(ctx->hace_dev, \"%s\\n\", name);\n\n\tctx->fallback_tfm = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fallback_tfm)) {\n\t\tdev_err(ctx->hace_dev->dev, \"ERROR: Cannot allocate fallback for %s %ld\\n\",\n\t\t\tname, PTR_ERR(ctx->fallback_tfm));\n\t\treturn PTR_ERR(ctx->fallback_tfm);\n\t}\n\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct aspeed_cipher_reqctx) +\n\t\t\t crypto_skcipher_reqsize(ctx->fallback_tfm));\n\n\treturn 0;\n}\n\nstatic void aspeed_crypto_cra_exit(struct crypto_skcipher *tfm)\n{\n\tstruct aspeed_cipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct aspeed_hace_dev *hace_dev = ctx->hace_dev;\n\n\tCIPHER_DBG(hace_dev, \"%s\\n\", crypto_tfm_alg_name(&tfm->base));\n\tcrypto_free_skcipher(ctx->fallback_tfm);\n}\n\nstatic struct aspeed_hace_alg aspeed_crypto_algs[] = {\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_aes_setkey,\n\t\t\t.encrypt\t= aspeed_aes_ecb_encrypt,\n\t\t\t.decrypt\t= aspeed_aes_ecb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ecb(aes)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ecb-aes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_aes_setkey,\n\t\t\t.encrypt\t= aspeed_aes_cbc_encrypt,\n\t\t\t.decrypt\t= aspeed_aes_cbc_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cbc(aes)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cbc-aes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_aes_setkey,\n\t\t\t.encrypt\t= aspeed_aes_cfb_encrypt,\n\t\t\t.decrypt\t= aspeed_aes_cfb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cfb(aes)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cfb-aes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_aes_setkey,\n\t\t\t.encrypt\t= aspeed_aes_ofb_encrypt,\n\t\t\t.decrypt\t= aspeed_aes_ofb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ofb(aes)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ofb-aes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_des_ecb_encrypt,\n\t\t\t.decrypt\t= aspeed_des_ecb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ecb(des)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ecb-des\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_des_cbc_encrypt,\n\t\t\t.decrypt\t= aspeed_des_cbc_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cbc(des)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cbc-des\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_des_cfb_encrypt,\n\t\t\t.decrypt\t= aspeed_des_cfb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cfb(des)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cfb-des\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_des_ofb_encrypt,\n\t\t\t.decrypt\t= aspeed_des_ofb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ofb(des)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ofb-des\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.min_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_tdes_ecb_encrypt,\n\t\t\t.decrypt\t= aspeed_tdes_ecb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ecb(des3_ede)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ecb-tdes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_tdes_cbc_encrypt,\n\t\t\t.decrypt\t= aspeed_tdes_cbc_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cbc(des3_ede)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cbc-tdes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_tdes_cfb_encrypt,\n\t\t\t.decrypt\t= aspeed_tdes_cfb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"cfb(des3_ede)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-cfb-tdes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_tdes_ofb_encrypt,\n\t\t\t.decrypt\t= aspeed_tdes_ofb_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ofb(des3_ede)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ofb-tdes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_blocksize\t\t= DES_BLOCK_SIZE,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n};\n\nstatic struct aspeed_hace_alg aspeed_crypto_algs_g6[] = {\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_aes_setkey,\n\t\t\t.encrypt\t= aspeed_aes_ctr_encrypt,\n\t\t\t.decrypt\t= aspeed_aes_ctr_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ctr(aes)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ctr-aes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC,\n\t\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES_KEY_SIZE,\n\t\t\t.max_keysize\t= DES_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_des_ctr_encrypt,\n\t\t\t.decrypt\t= aspeed_des_ctr_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ctr(des)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ctr-des\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC,\n\t\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\t{\n\t\t.alg.skcipher.base = {\n\t\t\t.ivsize\t\t= DES_BLOCK_SIZE,\n\t\t\t.min_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.max_keysize\t= DES3_EDE_KEY_SIZE,\n\t\t\t.setkey\t\t= aspeed_des_setkey,\n\t\t\t.encrypt\t= aspeed_tdes_ctr_encrypt,\n\t\t\t.decrypt\t= aspeed_tdes_ctr_decrypt,\n\t\t\t.init\t\t= aspeed_crypto_cra_init,\n\t\t\t.exit\t\t= aspeed_crypto_cra_exit,\n\t\t\t.base = {\n\t\t\t\t.cra_name\t\t= \"ctr(des3_ede)\",\n\t\t\t\t.cra_driver_name\t= \"aspeed-ctr-tdes\",\n\t\t\t\t.cra_priority\t\t= 300,\n\t\t\t\t.cra_flags\t\t= CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t\t\t  CRYPTO_ALG_ASYNC,\n\t\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t\t.cra_ctxsize\t\t= sizeof(struct aspeed_cipher_ctx),\n\t\t\t\t.cra_alignmask\t\t= 0x0f,\n\t\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t}\n\t\t},\n\t\t.alg.skcipher.op = {\n\t\t\t.do_one_request = aspeed_crypto_do_request,\n\t\t},\n\t},\n\n};\n\nvoid aspeed_unregister_hace_crypto_algs(struct aspeed_hace_dev *hace_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_crypto_algs); i++)\n\t\tcrypto_engine_unregister_skcipher(&aspeed_crypto_algs[i].alg.skcipher);\n\n\tif (hace_dev->version != AST2600_VERSION)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_crypto_algs_g6); i++)\n\t\tcrypto_engine_unregister_skcipher(&aspeed_crypto_algs_g6[i].alg.skcipher);\n}\n\nvoid aspeed_register_hace_crypto_algs(struct aspeed_hace_dev *hace_dev)\n{\n\tint rc, i;\n\n\tCIPHER_DBG(hace_dev, \"\\n\");\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_crypto_algs); i++) {\n\t\taspeed_crypto_algs[i].hace_dev = hace_dev;\n\t\trc = crypto_engine_register_skcipher(&aspeed_crypto_algs[i].alg.skcipher);\n\t\tif (rc) {\n\t\t\tCIPHER_DBG(hace_dev, \"Failed to register %s\\n\",\n\t\t\t\t   aspeed_crypto_algs[i].alg.skcipher.base.base.cra_name);\n\t\t}\n\t}\n\n\tif (hace_dev->version != AST2600_VERSION)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_crypto_algs_g6); i++) {\n\t\taspeed_crypto_algs_g6[i].hace_dev = hace_dev;\n\t\trc = crypto_engine_register_skcipher(&aspeed_crypto_algs_g6[i].alg.skcipher);\n\t\tif (rc) {\n\t\t\tCIPHER_DBG(hace_dev, \"Failed to register %s\\n\",\n\t\t\t\t   aspeed_crypto_algs_g6[i].alg.skcipher.base.base.cra_name);\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}