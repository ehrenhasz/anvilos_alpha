{
  "module_name": "aspeed-acry.c",
  "hash_id": "5ffa53b6240f0e1706b727fc426df427d5a929bb12f190cf79dc998b6795f634",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/aspeed/aspeed-acry.c",
  "human_readable_source": "\n \n#include <crypto/engine.h>\n#include <crypto/internal/akcipher.h>\n#include <crypto/internal/rsa.h>\n#include <crypto/scatterwalk.h>\n#include <linux/clk.h>\n#include <linux/count_zeros.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/mfd/syscon.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/regmap.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n\n#ifdef CONFIG_CRYPTO_DEV_ASPEED_DEBUG\n#define ACRY_DBG(d, fmt, ...)\t\\\n\tdev_info((d)->dev, \"%s() \" fmt, __func__, ##__VA_ARGS__)\n#else\n#define ACRY_DBG(d, fmt, ...)\t\\\n\tdev_dbg((d)->dev, \"%s() \" fmt, __func__, ##__VA_ARGS__)\n#endif\n\n \n#define ASPEED_ACRY_TRIGGER\t\t0x000\t \n#define ASPEED_ACRY_DMA_CMD\t\t0x048\t \n#define ASPEED_ACRY_DMA_SRC_BASE\t0x04C\t \n#define ASPEED_ACRY_DMA_LEN\t\t0x050\t \n#define ASPEED_ACRY_RSA_KEY_LEN\t\t0x058\t \n#define ASPEED_ACRY_INT_MASK\t\t0x3F8\t \n#define ASPEED_ACRY_STATUS\t\t0x3FC\t \n\n \n#define  ACRY_CMD_RSA_TRIGGER\t\tBIT(0)\n#define  ACRY_CMD_DMA_RSA_TRIGGER\tBIT(1)\n\n \n#define  ACRY_CMD_DMA_SRAM_MODE_RSA\t(0x3 << 4)\n#define  ACRY_CMD_DMEM_AHB\t\tBIT(8)\n#define  ACRY_CMD_DMA_SRAM_AHB_ENGINE\t0\n\n \n#define  RSA_E_BITS_LEN(x)\t\t((x) << 16)\n#define  RSA_M_BITS_LEN(x)\t\t(x)\n\n \n#define  ACRY_RSA_ISR\t\t\tBIT(1)\n\n#define ASPEED_ACRY_BUFF_SIZE\t\t0x1800\t \n#define ASPEED_ACRY_SRAM_MAX_LEN\t2048\t \n#define ASPEED_ACRY_RSA_MAX_KEY_LEN\t512\t \n\n#define CRYPTO_FLAGS_BUSY\t\tBIT(1)\n#define BYTES_PER_DWORD\t\t\t4\n\n \n#define AHBC_REGION_PROT\t\t0x240\n#define REGION_ACRYM\t\t\tBIT(23)\n\n#define ast_acry_write(acry, val, offset)\t\\\n\twritel((val), (acry)->regs + (offset))\n\n#define ast_acry_read(acry, offset)\t\t\\\n\treadl((acry)->regs + (offset))\n\nstruct aspeed_acry_dev;\n\ntypedef int (*aspeed_acry_fn_t)(struct aspeed_acry_dev *);\n\nstruct aspeed_acry_dev {\n\tvoid __iomem\t\t\t*regs;\n\tstruct device\t\t\t*dev;\n\tint\t\t\t\tirq;\n\tstruct clk\t\t\t*clk;\n\tstruct regmap\t\t\t*ahbc;\n\n\tstruct akcipher_request\t\t*req;\n\tstruct tasklet_struct\t\tdone_task;\n\taspeed_acry_fn_t\t\tresume;\n\tunsigned long\t\t\tflags;\n\n\t \n\tvoid __iomem\t\t\t*acry_sram;\n\n\t \n\tvoid\t\t\t\t*buf_addr;\n\tdma_addr_t\t\t\tbuf_dma_addr;\n\n\tstruct crypto_engine\t\t*crypt_engine_rsa;\n\n\t \n\tint\t\t\t\texp_dw_mapping[ASPEED_ACRY_RSA_MAX_KEY_LEN];\n\tint\t\t\t\tmod_dw_mapping[ASPEED_ACRY_RSA_MAX_KEY_LEN];\n\tint\t\t\t\tdata_byte_mapping[ASPEED_ACRY_SRAM_MAX_LEN];\n};\n\nstruct aspeed_acry_ctx {\n\tstruct aspeed_acry_dev\t\t*acry_dev;\n\n\tstruct rsa_key\t\t\tkey;\n\tint\t\t\t\tenc;\n\tu8\t\t\t\t*n;\n\tu8\t\t\t\t*e;\n\tu8\t\t\t\t*d;\n\tsize_t\t\t\t\tn_sz;\n\tsize_t\t\t\t\te_sz;\n\tsize_t\t\t\t\td_sz;\n\n\taspeed_acry_fn_t\t\ttrigger;\n\n\tstruct crypto_akcipher          *fallback_tfm;\n};\n\nstruct aspeed_acry_alg {\n\tstruct aspeed_acry_dev\t\t*acry_dev;\n\tstruct akcipher_engine_alg\takcipher;\n};\n\nenum aspeed_rsa_key_mode {\n\tASPEED_RSA_EXP_MODE = 0,\n\tASPEED_RSA_MOD_MODE,\n\tASPEED_RSA_DATA_MODE,\n};\n\nstatic inline struct akcipher_request *\n\takcipher_request_cast(struct crypto_async_request *req)\n{\n\treturn container_of(req, struct akcipher_request, base);\n}\n\nstatic int aspeed_acry_do_fallback(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\tint err;\n\n\takcipher_request_set_tfm(req, ctx->fallback_tfm);\n\n\tif (ctx->enc)\n\t\terr = crypto_akcipher_encrypt(req);\n\telse\n\t\terr = crypto_akcipher_decrypt(req);\n\n\takcipher_request_set_tfm(req, cipher);\n\n\treturn err;\n}\n\nstatic bool aspeed_acry_need_fallback(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\n\treturn ctx->key.n_sz > ASPEED_ACRY_RSA_MAX_KEY_LEN;\n}\n\nstatic int aspeed_acry_handle_queue(struct aspeed_acry_dev *acry_dev,\n\t\t\t\t    struct akcipher_request *req)\n{\n\tif (aspeed_acry_need_fallback(req)) {\n\t\tACRY_DBG(acry_dev, \"SW fallback\\n\");\n\t\treturn aspeed_acry_do_fallback(req);\n\t}\n\n\treturn crypto_transfer_akcipher_request_to_engine(acry_dev->crypt_engine_rsa, req);\n}\n\nstatic int aspeed_acry_do_request(struct crypto_engine *engine, void *areq)\n{\n\tstruct akcipher_request *req = akcipher_request_cast(areq);\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\tstruct aspeed_acry_dev *acry_dev = ctx->acry_dev;\n\n\tacry_dev->req = req;\n\tacry_dev->flags |= CRYPTO_FLAGS_BUSY;\n\n\treturn ctx->trigger(acry_dev);\n}\n\nstatic int aspeed_acry_complete(struct aspeed_acry_dev *acry_dev, int err)\n{\n\tstruct akcipher_request *req = acry_dev->req;\n\n\tacry_dev->flags &= ~CRYPTO_FLAGS_BUSY;\n\n\tcrypto_finalize_akcipher_request(acry_dev->crypt_engine_rsa, req, err);\n\n\treturn err;\n}\n\n \nstatic void aspeed_acry_rsa_sg_copy_to_buffer(struct aspeed_acry_dev *acry_dev,\n\t\t\t\t\t      u8 *buf, struct scatterlist *src,\n\t\t\t\t\t      size_t nbytes)\n{\n\tstatic u8 dram_buffer[ASPEED_ACRY_SRAM_MAX_LEN];\n\tint i = 0, j;\n\tint data_idx;\n\n\tACRY_DBG(acry_dev, \"\\n\");\n\n\tscatterwalk_map_and_copy(dram_buffer, src, 0, nbytes, 0);\n\n\tfor (j = nbytes - 1; j >= 0; j--) {\n\t\tdata_idx = acry_dev->data_byte_mapping[i];\n\t\tbuf[data_idx] =  dram_buffer[j];\n\t\ti++;\n\t}\n\n\tfor (; i < ASPEED_ACRY_SRAM_MAX_LEN; i++) {\n\t\tdata_idx = acry_dev->data_byte_mapping[i];\n\t\tbuf[data_idx] = 0;\n\t}\n}\n\n \nstatic int aspeed_acry_rsa_ctx_copy(struct aspeed_acry_dev *acry_dev, void *buf,\n\t\t\t\t    const void *xbuf, size_t nbytes,\n\t\t\t\t    enum aspeed_rsa_key_mode mode)\n{\n\tconst u8 *src = xbuf;\n\t__le32 *dw_buf = buf;\n\tint nbits, ndw;\n\tint i, j, idx;\n\tu32 data = 0;\n\n\tACRY_DBG(acry_dev, \"nbytes:%zu, mode:%d\\n\", nbytes, mode);\n\n\tif (nbytes > ASPEED_ACRY_RSA_MAX_KEY_LEN)\n\t\treturn -ENOMEM;\n\n\t \n\twhile (nbytes > 0 && src[0] == 0) {\n\t\tsrc++;\n\t\tnbytes--;\n\t}\n\n\tnbits = nbytes * 8;\n\tif (nbytes > 0)\n\t\tnbits -= count_leading_zeros(src[0]) - (BITS_PER_LONG - 8);\n\n\t \n\tndw = DIV_ROUND_UP(nbytes, BYTES_PER_DWORD);\n\n\tif (nbytes > 0) {\n\t\ti = BYTES_PER_DWORD - nbytes % BYTES_PER_DWORD;\n\t\ti %= BYTES_PER_DWORD;\n\n\t\tfor (j = ndw; j > 0; j--) {\n\t\t\tfor (; i < BYTES_PER_DWORD; i++) {\n\t\t\t\tdata <<= 8;\n\t\t\t\tdata |= *src++;\n\t\t\t}\n\n\t\t\ti = 0;\n\n\t\t\tif (mode == ASPEED_RSA_EXP_MODE)\n\t\t\t\tidx = acry_dev->exp_dw_mapping[j - 1];\n\t\t\telse  \n\t\t\t\tidx = acry_dev->mod_dw_mapping[j - 1];\n\n\t\t\tdw_buf[idx] = cpu_to_le32(data);\n\t\t}\n\t}\n\n\treturn nbits;\n}\n\nstatic int aspeed_acry_rsa_transfer(struct aspeed_acry_dev *acry_dev)\n{\n\tstruct akcipher_request *req = acry_dev->req;\n\tu8 __iomem *sram_buffer = acry_dev->acry_sram;\n\tstruct scatterlist *out_sg = req->dst;\n\tstatic u8 dram_buffer[ASPEED_ACRY_SRAM_MAX_LEN];\n\tint leading_zero = 1;\n\tint result_nbytes;\n\tint i = 0, j;\n\tint data_idx;\n\n\t \n\tast_acry_write(acry_dev, ACRY_CMD_DMEM_AHB, ASPEED_ACRY_DMA_CMD);\n\n\t \n\tregmap_update_bits(acry_dev->ahbc, AHBC_REGION_PROT,\n\t\t\t   REGION_ACRYM, 0);\n\n\tresult_nbytes = ASPEED_ACRY_SRAM_MAX_LEN;\n\n\tfor (j = ASPEED_ACRY_SRAM_MAX_LEN - 1; j >= 0; j--) {\n\t\tdata_idx = acry_dev->data_byte_mapping[j];\n\t\tif (readb(sram_buffer + data_idx) == 0 && leading_zero) {\n\t\t\tresult_nbytes--;\n\t\t} else {\n\t\t\tleading_zero = 0;\n\t\t\tdram_buffer[i] = readb(sram_buffer + data_idx);\n\t\t\ti++;\n\t\t}\n\t}\n\n\tACRY_DBG(acry_dev, \"result_nbytes:%d, req->dst_len:%d\\n\",\n\t\t result_nbytes, req->dst_len);\n\n\tif (result_nbytes <= req->dst_len) {\n\t\tscatterwalk_map_and_copy(dram_buffer, out_sg, 0, result_nbytes,\n\t\t\t\t\t 1);\n\t\treq->dst_len = result_nbytes;\n\n\t} else {\n\t\tdev_err(acry_dev->dev, \"RSA engine error!\\n\");\n\t}\n\n\tmemzero_explicit(acry_dev->buf_addr, ASPEED_ACRY_BUFF_SIZE);\n\n\treturn aspeed_acry_complete(acry_dev, 0);\n}\n\nstatic int aspeed_acry_rsa_trigger(struct aspeed_acry_dev *acry_dev)\n{\n\tstruct akcipher_request *req = acry_dev->req;\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\tint ne, nm;\n\n\tif (!ctx->n || !ctx->n_sz) {\n\t\tdev_err(acry_dev->dev, \"%s: key n is not set\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tmemzero_explicit(acry_dev->buf_addr, ASPEED_ACRY_BUFF_SIZE);\n\n\t \n\taspeed_acry_rsa_sg_copy_to_buffer(acry_dev, acry_dev->buf_addr,\n\t\t\t\t\t  req->src, req->src_len);\n\n\tnm = aspeed_acry_rsa_ctx_copy(acry_dev, acry_dev->buf_addr, ctx->n,\n\t\t\t\t      ctx->n_sz, ASPEED_RSA_MOD_MODE);\n\tif (ctx->enc) {\n\t\tif (!ctx->e || !ctx->e_sz) {\n\t\t\tdev_err(acry_dev->dev, \"%s: key e is not set\\n\",\n\t\t\t\t__func__);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t\tne = aspeed_acry_rsa_ctx_copy(acry_dev, acry_dev->buf_addr,\n\t\t\t\t\t      ctx->e, ctx->e_sz,\n\t\t\t\t\t      ASPEED_RSA_EXP_MODE);\n\t} else {\n\t\tif (!ctx->d || !ctx->d_sz) {\n\t\t\tdev_err(acry_dev->dev, \"%s: key d is not set\\n\",\n\t\t\t\t__func__);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t\tne = aspeed_acry_rsa_ctx_copy(acry_dev, acry_dev->buf_addr,\n\t\t\t\t\t      ctx->key.d, ctx->key.d_sz,\n\t\t\t\t\t      ASPEED_RSA_EXP_MODE);\n\t}\n\n\tast_acry_write(acry_dev, acry_dev->buf_dma_addr,\n\t\t       ASPEED_ACRY_DMA_SRC_BASE);\n\tast_acry_write(acry_dev, (ne << 16) + nm,\n\t\t       ASPEED_ACRY_RSA_KEY_LEN);\n\tast_acry_write(acry_dev, ASPEED_ACRY_BUFF_SIZE,\n\t\t       ASPEED_ACRY_DMA_LEN);\n\n\tacry_dev->resume = aspeed_acry_rsa_transfer;\n\n\t \n\tregmap_update_bits(acry_dev->ahbc, AHBC_REGION_PROT,\n\t\t\t   REGION_ACRYM, REGION_ACRYM);\n\n\tast_acry_write(acry_dev, ACRY_RSA_ISR, ASPEED_ACRY_INT_MASK);\n\tast_acry_write(acry_dev, ACRY_CMD_DMA_SRAM_MODE_RSA |\n\t\t\t  ACRY_CMD_DMA_SRAM_AHB_ENGINE, ASPEED_ACRY_DMA_CMD);\n\n\t \n\tast_acry_write(acry_dev, ACRY_CMD_RSA_TRIGGER |\n\t\t\t  ACRY_CMD_DMA_RSA_TRIGGER, ASPEED_ACRY_TRIGGER);\n\n\treturn 0;\n}\n\nstatic int aspeed_acry_rsa_enc(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\tstruct aspeed_acry_dev *acry_dev = ctx->acry_dev;\n\n\tctx->trigger = aspeed_acry_rsa_trigger;\n\tctx->enc = 1;\n\n\treturn aspeed_acry_handle_queue(acry_dev, req);\n}\n\nstatic int aspeed_acry_rsa_dec(struct akcipher_request *req)\n{\n\tstruct crypto_akcipher *cipher = crypto_akcipher_reqtfm(req);\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(cipher);\n\tstruct aspeed_acry_dev *acry_dev = ctx->acry_dev;\n\n\tctx->trigger = aspeed_acry_rsa_trigger;\n\tctx->enc = 0;\n\n\treturn aspeed_acry_handle_queue(acry_dev, req);\n}\n\nstatic u8 *aspeed_rsa_key_copy(u8 *src, size_t len)\n{\n\treturn kmemdup(src, len, GFP_KERNEL);\n}\n\nstatic int aspeed_rsa_set_n(struct aspeed_acry_ctx *ctx, u8 *value,\n\t\t\t    size_t len)\n{\n\tctx->n_sz = len;\n\tctx->n = aspeed_rsa_key_copy(value, len);\n\tif (!ctx->n)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int aspeed_rsa_set_e(struct aspeed_acry_ctx *ctx, u8 *value,\n\t\t\t    size_t len)\n{\n\tctx->e_sz = len;\n\tctx->e = aspeed_rsa_key_copy(value, len);\n\tif (!ctx->e)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int aspeed_rsa_set_d(struct aspeed_acry_ctx *ctx, u8 *value,\n\t\t\t    size_t len)\n{\n\tctx->d_sz = len;\n\tctx->d = aspeed_rsa_key_copy(value, len);\n\tif (!ctx->d)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void aspeed_rsa_key_free(struct aspeed_acry_ctx *ctx)\n{\n\tkfree_sensitive(ctx->n);\n\tkfree_sensitive(ctx->e);\n\tkfree_sensitive(ctx->d);\n\tctx->n_sz = 0;\n\tctx->e_sz = 0;\n\tctx->d_sz = 0;\n}\n\nstatic int aspeed_acry_rsa_setkey(struct crypto_akcipher *tfm, const void *key,\n\t\t\t\t  unsigned int keylen, int priv)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct aspeed_acry_dev *acry_dev = ctx->acry_dev;\n\tint ret;\n\n\tif (priv)\n\t\tret = rsa_parse_priv_key(&ctx->key, key, keylen);\n\telse\n\t\tret = rsa_parse_pub_key(&ctx->key, key, keylen);\n\n\tif (ret) {\n\t\tdev_err(acry_dev->dev, \"rsa parse key failed, ret:0x%x\\n\",\n\t\t\tret);\n\t\treturn ret;\n\t}\n\n\t \n\tif (ctx->key.n_sz > ASPEED_ACRY_RSA_MAX_KEY_LEN)\n\t\treturn 0;\n\n\tret = aspeed_rsa_set_n(ctx, (u8 *)ctx->key.n, ctx->key.n_sz);\n\tif (ret)\n\t\tgoto err;\n\n\tret = aspeed_rsa_set_e(ctx, (u8 *)ctx->key.e, ctx->key.e_sz);\n\tif (ret)\n\t\tgoto err;\n\n\tif (priv) {\n\t\tret = aspeed_rsa_set_d(ctx, (u8 *)ctx->key.d, ctx->key.d_sz);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tdev_err(acry_dev->dev, \"rsa set key failed\\n\");\n\taspeed_rsa_key_free(ctx);\n\n\treturn ret;\n}\n\nstatic int aspeed_acry_rsa_set_pub_key(struct crypto_akcipher *tfm,\n\t\t\t\t       const void *key,\n\t\t\t\t       unsigned int keylen)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tint ret;\n\n\tret = crypto_akcipher_set_pub_key(ctx->fallback_tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn aspeed_acry_rsa_setkey(tfm, key, keylen, 0);\n}\n\nstatic int aspeed_acry_rsa_set_priv_key(struct crypto_akcipher *tfm,\n\t\t\t\t\tconst void *key,\n\t\t\t\t\tunsigned int keylen)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tint ret;\n\n\tret = crypto_akcipher_set_priv_key(ctx->fallback_tfm, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn aspeed_acry_rsa_setkey(tfm, key, keylen, 1);\n}\n\nstatic unsigned int aspeed_acry_rsa_max_size(struct crypto_akcipher *tfm)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\n\tif (ctx->key.n_sz > ASPEED_ACRY_RSA_MAX_KEY_LEN)\n\t\treturn crypto_akcipher_maxsize(ctx->fallback_tfm);\n\n\treturn ctx->n_sz;\n}\n\nstatic int aspeed_acry_rsa_init_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\tstruct akcipher_alg *alg = crypto_akcipher_alg(tfm);\n\tconst char *name = crypto_tfm_alg_name(&tfm->base);\n\tstruct aspeed_acry_alg *acry_alg;\n\n\tacry_alg = container_of(alg, struct aspeed_acry_alg, akcipher.base);\n\n\tctx->acry_dev = acry_alg->acry_dev;\n\n\tctx->fallback_tfm = crypto_alloc_akcipher(name, 0, CRYPTO_ALG_ASYNC |\n\t\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ctx->fallback_tfm)) {\n\t\tdev_err(ctx->acry_dev->dev, \"ERROR: Cannot allocate fallback for %s %ld\\n\",\n\t\t\tname, PTR_ERR(ctx->fallback_tfm));\n\t\treturn PTR_ERR(ctx->fallback_tfm);\n\t}\n\n\treturn 0;\n}\n\nstatic void aspeed_acry_rsa_exit_tfm(struct crypto_akcipher *tfm)\n{\n\tstruct aspeed_acry_ctx *ctx = akcipher_tfm_ctx(tfm);\n\n\tcrypto_free_akcipher(ctx->fallback_tfm);\n}\n\nstatic struct aspeed_acry_alg aspeed_acry_akcipher_algs[] = {\n\t{\n\t\t.akcipher.base = {\n\t\t\t.encrypt = aspeed_acry_rsa_enc,\n\t\t\t.decrypt = aspeed_acry_rsa_dec,\n\t\t\t.sign = aspeed_acry_rsa_dec,\n\t\t\t.verify = aspeed_acry_rsa_enc,\n\t\t\t.set_pub_key = aspeed_acry_rsa_set_pub_key,\n\t\t\t.set_priv_key = aspeed_acry_rsa_set_priv_key,\n\t\t\t.max_size = aspeed_acry_rsa_max_size,\n\t\t\t.init = aspeed_acry_rsa_init_tfm,\n\t\t\t.exit = aspeed_acry_rsa_exit_tfm,\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"rsa\",\n\t\t\t\t.cra_driver_name = \"aspeed-rsa\",\n\t\t\t\t.cra_priority = 300,\n\t\t\t\t.cra_flags = CRYPTO_ALG_TYPE_AKCIPHER |\n\t\t\t\t\t     CRYPTO_ALG_ASYNC |\n\t\t\t\t\t     CRYPTO_ALG_KERN_DRIVER_ONLY |\n\t\t\t\t\t     CRYPTO_ALG_NEED_FALLBACK,\n\t\t\t\t.cra_module = THIS_MODULE,\n\t\t\t\t.cra_ctxsize = sizeof(struct aspeed_acry_ctx),\n\t\t\t},\n\t\t},\n\t\t.akcipher.op = {\n\t\t\t.do_one_request = aspeed_acry_do_request,\n\t\t},\n\t},\n};\n\nstatic void aspeed_acry_register(struct aspeed_acry_dev *acry_dev)\n{\n\tint i, rc;\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_acry_akcipher_algs); i++) {\n\t\taspeed_acry_akcipher_algs[i].acry_dev = acry_dev;\n\t\trc = crypto_engine_register_akcipher(&aspeed_acry_akcipher_algs[i].akcipher);\n\t\tif (rc) {\n\t\t\tACRY_DBG(acry_dev, \"Failed to register %s\\n\",\n\t\t\t\t aspeed_acry_akcipher_algs[i].akcipher.base.base.cra_name);\n\t\t}\n\t}\n}\n\nstatic void aspeed_acry_unregister(struct aspeed_acry_dev *acry_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(aspeed_acry_akcipher_algs); i++)\n\t\tcrypto_engine_unregister_akcipher(&aspeed_acry_akcipher_algs[i].akcipher);\n}\n\n \nstatic irqreturn_t aspeed_acry_irq(int irq, void *dev)\n{\n\tstruct aspeed_acry_dev *acry_dev = (struct aspeed_acry_dev *)dev;\n\tu32 sts;\n\n\tsts = ast_acry_read(acry_dev, ASPEED_ACRY_STATUS);\n\tast_acry_write(acry_dev, sts, ASPEED_ACRY_STATUS);\n\n\tACRY_DBG(acry_dev, \"irq sts:0x%x\\n\", sts);\n\n\tif (sts & ACRY_RSA_ISR) {\n\t\t \n\t\tast_acry_write(acry_dev, 0, ASPEED_ACRY_TRIGGER);\n\n\t\tif (acry_dev->flags & CRYPTO_FLAGS_BUSY)\n\t\t\ttasklet_schedule(&acry_dev->done_task);\n\t\telse\n\t\t\tdev_err(acry_dev->dev, \"RSA no active requests.\\n\");\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void aspeed_acry_sram_mapping(struct aspeed_acry_dev *acry_dev)\n{\n\tint i, j = 0;\n\n\tfor (i = 0; i < (ASPEED_ACRY_SRAM_MAX_LEN / BYTES_PER_DWORD); i++) {\n\t\tacry_dev->exp_dw_mapping[i] = j;\n\t\tacry_dev->mod_dw_mapping[i] = j + 4;\n\t\tacry_dev->data_byte_mapping[(i * 4)] = (j + 8) * 4;\n\t\tacry_dev->data_byte_mapping[(i * 4) + 1] = (j + 8) * 4 + 1;\n\t\tacry_dev->data_byte_mapping[(i * 4) + 2] = (j + 8) * 4 + 2;\n\t\tacry_dev->data_byte_mapping[(i * 4) + 3] = (j + 8) * 4 + 3;\n\t\tj++;\n\t\tj = j % 4 ? j : j + 8;\n\t}\n}\n\nstatic void aspeed_acry_done_task(unsigned long data)\n{\n\tstruct aspeed_acry_dev *acry_dev = (struct aspeed_acry_dev *)data;\n\n\t(void)acry_dev->resume(acry_dev);\n}\n\nstatic const struct of_device_id aspeed_acry_of_matches[] = {\n\t{ .compatible = \"aspeed,ast2600-acry\", },\n\t{},\n};\n\nstatic int aspeed_acry_probe(struct platform_device *pdev)\n{\n\tstruct aspeed_acry_dev *acry_dev;\n\tstruct device *dev = &pdev->dev;\n\tint rc;\n\n\tacry_dev = devm_kzalloc(dev, sizeof(struct aspeed_acry_dev),\n\t\t\t\tGFP_KERNEL);\n\tif (!acry_dev)\n\t\treturn -ENOMEM;\n\n\tacry_dev->dev = dev;\n\n\tplatform_set_drvdata(pdev, acry_dev);\n\n\tacry_dev->regs = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(acry_dev->regs))\n\t\treturn PTR_ERR(acry_dev->regs);\n\n\tacry_dev->acry_sram = devm_platform_ioremap_resource(pdev, 1);\n\tif (IS_ERR(acry_dev->acry_sram))\n\t\treturn PTR_ERR(acry_dev->acry_sram);\n\n\t \n\tacry_dev->irq = platform_get_irq(pdev, 0);\n\tif (acry_dev->irq < 0)\n\t\treturn -ENXIO;\n\n\trc = devm_request_irq(dev, acry_dev->irq, aspeed_acry_irq, 0,\n\t\t\t      dev_name(dev), acry_dev);\n\tif (rc) {\n\t\tdev_err(dev, \"Failed to request irq.\\n\");\n\t\treturn rc;\n\t}\n\n\tacry_dev->clk = devm_clk_get_enabled(dev, NULL);\n\tif (IS_ERR(acry_dev->clk)) {\n\t\tdev_err(dev, \"Failed to get acry clk\\n\");\n\t\treturn PTR_ERR(acry_dev->clk);\n\t}\n\n\tacry_dev->ahbc = syscon_regmap_lookup_by_phandle(dev->of_node,\n\t\t\t\t\t\t\t \"aspeed,ahbc\");\n\tif (IS_ERR(acry_dev->ahbc)) {\n\t\tdev_err(dev, \"Failed to get AHBC regmap\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tacry_dev->crypt_engine_rsa = crypto_engine_alloc_init(dev, true);\n\tif (!acry_dev->crypt_engine_rsa) {\n\t\trc = -ENOMEM;\n\t\tgoto clk_exit;\n\t}\n\n\trc = crypto_engine_start(acry_dev->crypt_engine_rsa);\n\tif (rc)\n\t\tgoto err_engine_rsa_start;\n\n\ttasklet_init(&acry_dev->done_task, aspeed_acry_done_task,\n\t\t     (unsigned long)acry_dev);\n\n\t \n\tast_acry_write(acry_dev, ACRY_CMD_DMEM_AHB, ASPEED_ACRY_DMA_CMD);\n\n\t \n\taspeed_acry_sram_mapping(acry_dev);\n\n\tacry_dev->buf_addr = dmam_alloc_coherent(dev, ASPEED_ACRY_BUFF_SIZE,\n\t\t\t\t\t\t &acry_dev->buf_dma_addr,\n\t\t\t\t\t\t GFP_KERNEL);\n\tif (!acry_dev->buf_addr) {\n\t\trc = -ENOMEM;\n\t\tgoto err_engine_rsa_start;\n\t}\n\n\taspeed_acry_register(acry_dev);\n\n\tdev_info(dev, \"Aspeed ACRY Accelerator successfully registered\\n\");\n\n\treturn 0;\n\nerr_engine_rsa_start:\n\tcrypto_engine_exit(acry_dev->crypt_engine_rsa);\nclk_exit:\n\tclk_disable_unprepare(acry_dev->clk);\n\n\treturn rc;\n}\n\nstatic int aspeed_acry_remove(struct platform_device *pdev)\n{\n\tstruct aspeed_acry_dev *acry_dev = platform_get_drvdata(pdev);\n\n\taspeed_acry_unregister(acry_dev);\n\tcrypto_engine_exit(acry_dev->crypt_engine_rsa);\n\ttasklet_kill(&acry_dev->done_task);\n\tclk_disable_unprepare(acry_dev->clk);\n\n\treturn 0;\n}\n\nMODULE_DEVICE_TABLE(of, aspeed_acry_of_matches);\n\nstatic struct platform_driver aspeed_acry_driver = {\n\t.probe\t\t= aspeed_acry_probe,\n\t.remove\t\t= aspeed_acry_remove,\n\t.driver\t\t= {\n\t\t.name   = KBUILD_MODNAME,\n\t\t.of_match_table = aspeed_acry_of_matches,\n\t},\n};\n\nmodule_platform_driver(aspeed_acry_driver);\n\nMODULE_AUTHOR(\"Neal Liu <neal_liu@aspeedtech.com>\");\nMODULE_DESCRIPTION(\"ASPEED ACRY driver for hardware RSA Engine\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}