{
  "module_name": "chcr_core.c",
  "hash_id": "4bd9e2779be7ac53526996d4d031d929312c943675fd3288c604bffbb21f3382",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/chelsio/chcr_core.c",
  "human_readable_source": " \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n\n#include <crypto/aes.h>\n#include <crypto/hash.h>\n\n#include \"t4_msg.h\"\n#include \"chcr_core.h\"\n#include \"cxgb4_uld.h\"\n\nstatic struct chcr_driver_data drv_data;\n\ntypedef int (*chcr_handler_func)(struct adapter *adap, unsigned char *input);\nstatic int cpl_fw6_pld_handler(struct adapter *adap, unsigned char *input);\nstatic void *chcr_uld_add(const struct cxgb4_lld_info *lld);\nstatic int chcr_uld_state_change(void *handle, enum cxgb4_state state);\n\nstatic chcr_handler_func work_handlers[NUM_CPL_CMDS] = {\n\t[CPL_FW6_PLD] = cpl_fw6_pld_handler,\n};\n\nstatic struct cxgb4_uld_info chcr_uld_info = {\n\t.name = DRV_MODULE_NAME,\n\t.nrxq = MAX_ULD_QSETS,\n\t \n\t.rxq_size = 1024,\n\t.add = chcr_uld_add,\n\t.state_change = chcr_uld_state_change,\n\t.rx_handler = chcr_uld_rx_handler,\n};\n\nstatic void detach_work_fn(struct work_struct *work)\n{\n\tstruct chcr_dev *dev;\n\n\tdev = container_of(work, struct chcr_dev, detach_work.work);\n\n\tif (atomic_read(&dev->inflight)) {\n\t\tdev->wqretry--;\n\t\tif (dev->wqretry) {\n\t\t\tpr_debug(\"Request Inflight Count %d\\n\",\n\t\t\t\tatomic_read(&dev->inflight));\n\n\t\t\tschedule_delayed_work(&dev->detach_work, WQ_DETACH_TM);\n\t\t} else {\n\t\t\tWARN(1, \"CHCR:%d request Still Pending\\n\",\n\t\t\t\tatomic_read(&dev->inflight));\n\t\t\tcomplete(&dev->detach_comp);\n\t\t}\n\t} else {\n\t\tcomplete(&dev->detach_comp);\n\t}\n}\n\nstruct uld_ctx *assign_chcr_device(void)\n{\n\tstruct uld_ctx *u_ctx = NULL;\n\n\t \n\tmutex_lock(&drv_data.drv_mutex);\n\tif (!list_empty(&drv_data.act_dev)) {\n\t\tu_ctx = drv_data.last_dev;\n\t\tif (list_is_last(&drv_data.last_dev->entry, &drv_data.act_dev))\n\t\t\tdrv_data.last_dev = list_first_entry(&drv_data.act_dev,\n\t\t\t\t\t\t  struct uld_ctx, entry);\n\t\telse\n\t\t\tdrv_data.last_dev =\n\t\t\t\tlist_next_entry(drv_data.last_dev, entry);\n\t}\n\tmutex_unlock(&drv_data.drv_mutex);\n\treturn u_ctx;\n}\n\nstatic void chcr_dev_add(struct uld_ctx *u_ctx)\n{\n\tstruct chcr_dev *dev;\n\n\tdev = &u_ctx->dev;\n\tdev->state = CHCR_ATTACH;\n\tatomic_set(&dev->inflight, 0);\n\tmutex_lock(&drv_data.drv_mutex);\n\tlist_move(&u_ctx->entry, &drv_data.act_dev);\n\tif (!drv_data.last_dev)\n\t\tdrv_data.last_dev = u_ctx;\n\tmutex_unlock(&drv_data.drv_mutex);\n}\n\nstatic void chcr_dev_init(struct uld_ctx *u_ctx)\n{\n\tstruct chcr_dev *dev;\n\n\tdev = &u_ctx->dev;\n\tspin_lock_init(&dev->lock_chcr_dev);\n\tINIT_DELAYED_WORK(&dev->detach_work, detach_work_fn);\n\tinit_completion(&dev->detach_comp);\n\tdev->state = CHCR_INIT;\n\tdev->wqretry = WQ_RETRY;\n\tatomic_inc(&drv_data.dev_count);\n\tatomic_set(&dev->inflight, 0);\n\tmutex_lock(&drv_data.drv_mutex);\n\tlist_add_tail(&u_ctx->entry, &drv_data.inact_dev);\n\tmutex_unlock(&drv_data.drv_mutex);\n}\n\nstatic int chcr_dev_move(struct uld_ctx *u_ctx)\n{\n\tmutex_lock(&drv_data.drv_mutex);\n\tif (drv_data.last_dev == u_ctx) {\n\t\tif (list_is_last(&drv_data.last_dev->entry, &drv_data.act_dev))\n\t\t\tdrv_data.last_dev = list_first_entry(&drv_data.act_dev,\n\t\t\t\t\t\t  struct uld_ctx, entry);\n\t\telse\n\t\t\tdrv_data.last_dev =\n\t\t\t\tlist_next_entry(drv_data.last_dev, entry);\n\t}\n\tlist_move(&u_ctx->entry, &drv_data.inact_dev);\n\tif (list_empty(&drv_data.act_dev))\n\t\tdrv_data.last_dev = NULL;\n\tatomic_dec(&drv_data.dev_count);\n\tmutex_unlock(&drv_data.drv_mutex);\n\n\treturn 0;\n}\n\nstatic int cpl_fw6_pld_handler(struct adapter *adap,\n\t\t\t       unsigned char *input)\n{\n\tstruct crypto_async_request *req;\n\tstruct cpl_fw6_pld *fw6_pld;\n\tu32 ack_err_status = 0;\n\tint error_status = 0;\n\n\tfw6_pld = (struct cpl_fw6_pld *)input;\n\treq = (struct crypto_async_request *)(uintptr_t)be64_to_cpu(\n\t\t\t\t\t\t    fw6_pld->data[1]);\n\n\tack_err_status =\n\t\tntohl(*(__be32 *)((unsigned char *)&fw6_pld->data[0] + 4));\n\tif (CHK_MAC_ERR_BIT(ack_err_status) || CHK_PAD_ERR_BIT(ack_err_status))\n\t\terror_status = -EBADMSG;\n\t \n\tif (req) {\n\t\terror_status = chcr_handle_resp(req, input, error_status);\n\t} else {\n\t\tpr_err(\"Incorrect request address from the firmware\\n\");\n\t\treturn -EFAULT;\n\t}\n\tif (error_status)\n\t\tatomic_inc(&adap->chcr_stats.error);\n\n\treturn 0;\n}\n\nint chcr_send_wr(struct sk_buff *skb)\n{\n\treturn cxgb4_crypto_send(skb->dev, skb);\n}\n\nstatic void *chcr_uld_add(const struct cxgb4_lld_info *lld)\n{\n\tstruct uld_ctx *u_ctx;\n\n\t \n\tpr_info_once(\"%s\\n\", DRV_DESC);\n\tif (!(lld->ulp_crypto & ULP_CRYPTO_LOOKASIDE))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\t \n\tu_ctx = kzalloc(sizeof(*u_ctx), GFP_KERNEL);\n\tif (!u_ctx) {\n\t\tu_ctx = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tu_ctx->lldi = *lld;\n\tchcr_dev_init(u_ctx);\nout:\n\treturn u_ctx;\n}\n\nint chcr_uld_rx_handler(void *handle, const __be64 *rsp,\n\t\t\tconst struct pkt_gl *pgl)\n{\n\tstruct uld_ctx *u_ctx = (struct uld_ctx *)handle;\n\tstruct chcr_dev *dev = &u_ctx->dev;\n\tstruct adapter *adap = padap(dev);\n\tconst struct cpl_fw6_pld *rpl = (struct cpl_fw6_pld *)rsp;\n\n\tif (!work_handlers[rpl->opcode]) {\n\t\tpr_err(\"Unsupported opcode %d received\\n\", rpl->opcode);\n\t\treturn 0;\n\t}\n\n\tif (!pgl)\n\t\twork_handlers[rpl->opcode](adap, (unsigned char *)&rsp[1]);\n\telse\n\t\twork_handlers[rpl->opcode](adap, pgl->va);\n\treturn 0;\n}\n\nstatic void chcr_detach_device(struct uld_ctx *u_ctx)\n{\n\tstruct chcr_dev *dev = &u_ctx->dev;\n\n\tif (dev->state == CHCR_DETACH) {\n\t\tpr_debug(\"Detached Event received for already detach device\\n\");\n\t\treturn;\n\t}\n\tdev->state = CHCR_DETACH;\n\tif (atomic_read(&dev->inflight) != 0) {\n\t\tschedule_delayed_work(&dev->detach_work, WQ_DETACH_TM);\n\t\twait_for_completion(&dev->detach_comp);\n\t}\n\n\t\n\tchcr_dev_move(u_ctx);\n}\n\nstatic int chcr_uld_state_change(void *handle, enum cxgb4_state state)\n{\n\tstruct uld_ctx *u_ctx = handle;\n\tint ret = 0;\n\n\tswitch (state) {\n\tcase CXGB4_STATE_UP:\n\t\tif (u_ctx->dev.state != CHCR_INIT) {\n\t\t\t\n\t\t\treturn 0;\n\t\t}\n\t\tchcr_dev_add(u_ctx);\n\t\tret = start_crypto();\n\t\tbreak;\n\n\tcase CXGB4_STATE_DETACH:\n\t\tchcr_detach_device(u_ctx);\n\t\tif (!atomic_read(&drv_data.dev_count))\n\t\t\tstop_crypto();\n\t\tbreak;\n\n\tcase CXGB4_STATE_START_RECOVERY:\n\tcase CXGB4_STATE_DOWN:\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int __init chcr_crypto_init(void)\n{\n\tINIT_LIST_HEAD(&drv_data.act_dev);\n\tINIT_LIST_HEAD(&drv_data.inact_dev);\n\tatomic_set(&drv_data.dev_count, 0);\n\tmutex_init(&drv_data.drv_mutex);\n\tdrv_data.last_dev = NULL;\n\tcxgb4_register_uld(CXGB4_ULD_CRYPTO, &chcr_uld_info);\n\n\treturn 0;\n}\n\nstatic void __exit chcr_crypto_exit(void)\n{\n\tstruct uld_ctx *u_ctx, *tmp;\n\tstruct adapter *adap;\n\n\tstop_crypto();\n\tcxgb4_unregister_uld(CXGB4_ULD_CRYPTO);\n\t \n\tmutex_lock(&drv_data.drv_mutex);\n\tlist_for_each_entry_safe(u_ctx, tmp, &drv_data.act_dev, entry) {\n\t\tadap = padap(&u_ctx->dev);\n\t\tmemset(&adap->chcr_stats, 0, sizeof(adap->chcr_stats));\n\t\tlist_del(&u_ctx->entry);\n\t\tkfree(u_ctx);\n\t}\n\tlist_for_each_entry_safe(u_ctx, tmp, &drv_data.inact_dev, entry) {\n\t\tadap = padap(&u_ctx->dev);\n\t\tmemset(&adap->chcr_stats, 0, sizeof(adap->chcr_stats));\n\t\tlist_del(&u_ctx->entry);\n\t\tkfree(u_ctx);\n\t}\n\tmutex_unlock(&drv_data.drv_mutex);\n}\n\nmodule_init(chcr_crypto_init);\nmodule_exit(chcr_crypto_exit);\n\nMODULE_DESCRIPTION(\"Crypto Co-processor for Chelsio Terminator cards.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chelsio Communications\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}