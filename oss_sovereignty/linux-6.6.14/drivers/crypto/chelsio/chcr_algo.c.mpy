{
  "module_name": "chcr_algo.c",
  "hash_id": "01db764d792baf033e399c5ebbb1b2dbccd40d9176ac2192cf3bd3dacf68e06b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/chelsio/chcr_algo.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) \"chcr:\" fmt\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/crypto.h>\n#include <linux/skbuff.h>\n#include <linux/rtnetlink.h>\n#include <linux/highmem.h>\n#include <linux/scatterlist.h>\n\n#include <crypto/aes.h>\n#include <crypto/algapi.h>\n#include <crypto/hash.h>\n#include <crypto/gcm.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/authenc.h>\n#include <crypto/ctr.h>\n#include <crypto/gf128mul.h>\n#include <crypto/internal/aead.h>\n#include <crypto/null.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/aead.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/internal/hash.h>\n\n#include \"t4fw_api.h\"\n#include \"t4_msg.h\"\n#include \"chcr_core.h\"\n#include \"chcr_algo.h\"\n#include \"chcr_crypto.h\"\n\n#define IV AES_BLOCK_SIZE\n\nstatic unsigned int sgl_ent_len[] = {\n\t0, 0, 16, 24, 40, 48, 64, 72, 88,\n\t96, 112, 120, 136, 144, 160, 168, 184,\n\t192, 208, 216, 232, 240, 256, 264, 280,\n\t288, 304, 312, 328, 336, 352, 360, 376\n};\n\nstatic unsigned int dsgl_ent_len[] = {\n\t0, 32, 32, 48, 48, 64, 64, 80, 80,\n\t112, 112, 128, 128, 144, 144, 160, 160,\n\t192, 192, 208, 208, 224, 224, 240, 240,\n\t272, 272, 288, 288, 304, 304, 320, 320\n};\n\nstatic u32 round_constant[11] = {\n\t0x01000000, 0x02000000, 0x04000000, 0x08000000,\n\t0x10000000, 0x20000000, 0x40000000, 0x80000000,\n\t0x1B000000, 0x36000000, 0x6C000000\n};\n\nstatic int chcr_handle_cipher_resp(struct skcipher_request *req,\n\t\t\t\t   unsigned char *input, int err);\n\nstatic inline  struct chcr_aead_ctx *AEAD_CTX(struct chcr_context *ctx)\n{\n\treturn &ctx->crypto_ctx->aeadctx;\n}\n\nstatic inline struct ablk_ctx *ABLK_CTX(struct chcr_context *ctx)\n{\n\treturn &ctx->crypto_ctx->ablkctx;\n}\n\nstatic inline struct hmac_ctx *HMAC_CTX(struct chcr_context *ctx)\n{\n\treturn &ctx->crypto_ctx->hmacctx;\n}\n\nstatic inline struct chcr_gcm_ctx *GCM_CTX(struct chcr_aead_ctx *gctx)\n{\n\treturn gctx->ctx->gcm;\n}\n\nstatic inline struct chcr_authenc_ctx *AUTHENC_CTX(struct chcr_aead_ctx *gctx)\n{\n\treturn gctx->ctx->authenc;\n}\n\nstatic inline struct uld_ctx *ULD_CTX(struct chcr_context *ctx)\n{\n\treturn container_of(ctx->dev, struct uld_ctx, dev);\n}\n\nstatic inline void chcr_init_hctx_per_wr(struct chcr_ahash_req_ctx *reqctx)\n{\n\tmemset(&reqctx->hctx_wr, 0, sizeof(struct chcr_hctx_per_wr));\n}\n\nstatic int sg_nents_xlen(struct scatterlist *sg, unsigned int reqlen,\n\t\t\t unsigned int entlen,\n\t\t\t unsigned int skip)\n{\n\tint nents = 0;\n\tunsigned int less;\n\tunsigned int skip_len = 0;\n\n\twhile (sg && skip) {\n\t\tif (sg_dma_len(sg) <= skip) {\n\t\t\tskip -= sg_dma_len(sg);\n\t\t\tskip_len = 0;\n\t\t\tsg = sg_next(sg);\n\t\t} else {\n\t\t\tskip_len = skip;\n\t\t\tskip = 0;\n\t\t}\n\t}\n\n\twhile (sg && reqlen) {\n\t\tless = min(reqlen, sg_dma_len(sg) - skip_len);\n\t\tnents += DIV_ROUND_UP(less, entlen);\n\t\treqlen -= less;\n\t\tskip_len = 0;\n\t\tsg = sg_next(sg);\n\t}\n\treturn nents;\n}\n\nstatic inline int get_aead_subtype(struct crypto_aead *aead)\n{\n\tstruct aead_alg *alg = crypto_aead_alg(aead);\n\tstruct chcr_alg_template *chcr_crypto_alg =\n\t\tcontainer_of(alg, struct chcr_alg_template, alg.aead);\n\treturn chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;\n}\n\nvoid chcr_verify_tag(struct aead_request *req, u8 *input, int *err)\n{\n\tu8 temp[SHA512_DIGEST_SIZE];\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tint authsize = crypto_aead_authsize(tfm);\n\tstruct cpl_fw6_pld *fw6_pld;\n\tint cmp = 0;\n\n\tfw6_pld = (struct cpl_fw6_pld *)input;\n\tif ((get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) ||\n\t    (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_GCM)) {\n\t\tcmp = crypto_memneq(&fw6_pld->data[2], (fw6_pld + 1), authsize);\n\t} else {\n\n\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src), temp,\n\t\t\t\tauthsize, req->assoclen +\n\t\t\t\treq->cryptlen - authsize);\n\t\tcmp = crypto_memneq(temp, (fw6_pld + 1), authsize);\n\t}\n\tif (cmp)\n\t\t*err = -EBADMSG;\n\telse\n\t\t*err = 0;\n}\n\nstatic int chcr_inc_wrcount(struct chcr_dev *dev)\n{\n\tif (dev->state == CHCR_DETACH)\n\t\treturn 1;\n\tatomic_inc(&dev->inflight);\n\treturn 0;\n}\n\nstatic inline void chcr_dec_wrcount(struct chcr_dev *dev)\n{\n\tatomic_dec(&dev->inflight);\n}\n\nstatic inline int chcr_handle_aead_resp(struct aead_request *req,\n\t\t\t\t\t unsigned char *input,\n\t\t\t\t\t int err)\n{\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_dev *dev = a_ctx(tfm)->dev;\n\n\tchcr_aead_common_exit(req);\n\tif (reqctx->verify == VERIFY_SW) {\n\t\tchcr_verify_tag(req, input, &err);\n\t\treqctx->verify = VERIFY_HW;\n\t}\n\tchcr_dec_wrcount(dev);\n\taead_request_complete(req, err);\n\n\treturn err;\n}\n\nstatic void get_aes_decrypt_key(unsigned char *dec_key,\n\t\t\t\t       const unsigned char *key,\n\t\t\t\t       unsigned int keylength)\n{\n\tu32 temp;\n\tu32 w_ring[MAX_NK];\n\tint i, j, k;\n\tu8  nr, nk;\n\n\tswitch (keylength) {\n\tcase AES_KEYLENGTH_128BIT:\n\t\tnk = KEYLENGTH_4BYTES;\n\t\tnr = NUMBER_OF_ROUNDS_10;\n\t\tbreak;\n\tcase AES_KEYLENGTH_192BIT:\n\t\tnk = KEYLENGTH_6BYTES;\n\t\tnr = NUMBER_OF_ROUNDS_12;\n\t\tbreak;\n\tcase AES_KEYLENGTH_256BIT:\n\t\tnk = KEYLENGTH_8BYTES;\n\t\tnr = NUMBER_OF_ROUNDS_14;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\tfor (i = 0; i < nk; i++)\n\t\tw_ring[i] = get_unaligned_be32(&key[i * 4]);\n\n\ti = 0;\n\ttemp = w_ring[nk - 1];\n\twhile (i + nk < (nr + 1) * 4) {\n\t\tif (!(i % nk)) {\n\t\t\t \n\t\t\ttemp = (temp << 8) | (temp >> 24);\n\t\t\ttemp = aes_ks_subword(temp);\n\t\t\ttemp ^= round_constant[i / nk];\n\t\t} else if (nk == 8 && (i % 4 == 0)) {\n\t\t\ttemp = aes_ks_subword(temp);\n\t\t}\n\t\tw_ring[i % nk] ^= temp;\n\t\ttemp = w_ring[i % nk];\n\t\ti++;\n\t}\n\ti--;\n\tfor (k = 0, j = i % nk; k < nk; k++) {\n\t\tput_unaligned_be32(w_ring[j], &dec_key[k * 4]);\n\t\tj--;\n\t\tif (j < 0)\n\t\t\tj += nk;\n\t}\n}\n\nstatic struct crypto_shash *chcr_alloc_shash(unsigned int ds)\n{\n\tstruct crypto_shash *base_hash = ERR_PTR(-EINVAL);\n\n\tswitch (ds) {\n\tcase SHA1_DIGEST_SIZE:\n\t\tbase_hash = crypto_alloc_shash(\"sha1\", 0, 0);\n\t\tbreak;\n\tcase SHA224_DIGEST_SIZE:\n\t\tbase_hash = crypto_alloc_shash(\"sha224\", 0, 0);\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tbase_hash = crypto_alloc_shash(\"sha256\", 0, 0);\n\t\tbreak;\n\tcase SHA384_DIGEST_SIZE:\n\t\tbase_hash = crypto_alloc_shash(\"sha384\", 0, 0);\n\t\tbreak;\n\tcase SHA512_DIGEST_SIZE:\n\t\tbase_hash = crypto_alloc_shash(\"sha512\", 0, 0);\n\t\tbreak;\n\t}\n\n\treturn base_hash;\n}\n\nstatic int chcr_compute_partial_hash(struct shash_desc *desc,\n\t\t\t\t     char *iopad, char *result_hash,\n\t\t\t\t     int digest_size)\n{\n\tstruct sha1_state sha1_st;\n\tstruct sha256_state sha256_st;\n\tstruct sha512_state sha512_st;\n\tint error;\n\n\tif (digest_size == SHA1_DIGEST_SIZE) {\n\t\terror = crypto_shash_init(desc) ?:\n\t\t\tcrypto_shash_update(desc, iopad, SHA1_BLOCK_SIZE) ?:\n\t\t\tcrypto_shash_export(desc, (void *)&sha1_st);\n\t\tmemcpy(result_hash, sha1_st.state, SHA1_DIGEST_SIZE);\n\t} else if (digest_size == SHA224_DIGEST_SIZE) {\n\t\terror = crypto_shash_init(desc) ?:\n\t\t\tcrypto_shash_update(desc, iopad, SHA256_BLOCK_SIZE) ?:\n\t\t\tcrypto_shash_export(desc, (void *)&sha256_st);\n\t\tmemcpy(result_hash, sha256_st.state, SHA256_DIGEST_SIZE);\n\n\t} else if (digest_size == SHA256_DIGEST_SIZE) {\n\t\terror = crypto_shash_init(desc) ?:\n\t\t\tcrypto_shash_update(desc, iopad, SHA256_BLOCK_SIZE) ?:\n\t\t\tcrypto_shash_export(desc, (void *)&sha256_st);\n\t\tmemcpy(result_hash, sha256_st.state, SHA256_DIGEST_SIZE);\n\n\t} else if (digest_size == SHA384_DIGEST_SIZE) {\n\t\terror = crypto_shash_init(desc) ?:\n\t\t\tcrypto_shash_update(desc, iopad, SHA512_BLOCK_SIZE) ?:\n\t\t\tcrypto_shash_export(desc, (void *)&sha512_st);\n\t\tmemcpy(result_hash, sha512_st.state, SHA512_DIGEST_SIZE);\n\n\t} else if (digest_size == SHA512_DIGEST_SIZE) {\n\t\terror = crypto_shash_init(desc) ?:\n\t\t\tcrypto_shash_update(desc, iopad, SHA512_BLOCK_SIZE) ?:\n\t\t\tcrypto_shash_export(desc, (void *)&sha512_st);\n\t\tmemcpy(result_hash, sha512_st.state, SHA512_DIGEST_SIZE);\n\t} else {\n\t\terror = -EINVAL;\n\t\tpr_err(\"Unknown digest size %d\\n\", digest_size);\n\t}\n\treturn error;\n}\n\nstatic void chcr_change_order(char *buf, int ds)\n{\n\tint i;\n\n\tif (ds == SHA512_DIGEST_SIZE) {\n\t\tfor (i = 0; i < (ds / sizeof(u64)); i++)\n\t\t\t*((__be64 *)buf + i) =\n\t\t\t\tcpu_to_be64(*((u64 *)buf + i));\n\t} else {\n\t\tfor (i = 0; i < (ds / sizeof(u32)); i++)\n\t\t\t*((__be32 *)buf + i) =\n\t\t\t\tcpu_to_be32(*((u32 *)buf + i));\n\t}\n}\n\nstatic inline int is_hmac(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *alg = tfm->__crt_alg;\n\tstruct chcr_alg_template *chcr_crypto_alg =\n\t\tcontainer_of(__crypto_ahash_alg(alg), struct chcr_alg_template,\n\t\t\t     alg.hash);\n\tif (chcr_crypto_alg->type == CRYPTO_ALG_TYPE_HMAC)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline void dsgl_walk_init(struct dsgl_walk *walk,\n\t\t\t\t   struct cpl_rx_phys_dsgl *dsgl)\n{\n\twalk->dsgl = dsgl;\n\twalk->nents = 0;\n\twalk->to = (struct phys_sge_pairs *)(dsgl + 1);\n}\n\nstatic inline void dsgl_walk_end(struct dsgl_walk *walk, unsigned short qid,\n\t\t\t\t int pci_chan_id)\n{\n\tstruct cpl_rx_phys_dsgl *phys_cpl;\n\n\tphys_cpl = walk->dsgl;\n\n\tphys_cpl->op_to_tid = htonl(CPL_RX_PHYS_DSGL_OPCODE_V(CPL_RX_PHYS_DSGL)\n\t\t\t\t    | CPL_RX_PHYS_DSGL_ISRDMA_V(0));\n\tphys_cpl->pcirlxorder_to_noofsgentr =\n\t\thtonl(CPL_RX_PHYS_DSGL_PCIRLXORDER_V(0) |\n\t\t      CPL_RX_PHYS_DSGL_PCINOSNOOP_V(0) |\n\t\t      CPL_RX_PHYS_DSGL_PCITPHNTENB_V(0) |\n\t\t      CPL_RX_PHYS_DSGL_PCITPHNT_V(0) |\n\t\t      CPL_RX_PHYS_DSGL_DCAID_V(0) |\n\t\t      CPL_RX_PHYS_DSGL_NOOFSGENTR_V(walk->nents));\n\tphys_cpl->rss_hdr_int.opcode = CPL_RX_PHYS_ADDR;\n\tphys_cpl->rss_hdr_int.qid = htons(qid);\n\tphys_cpl->rss_hdr_int.hash_val = 0;\n\tphys_cpl->rss_hdr_int.channel = pci_chan_id;\n}\n\nstatic inline void dsgl_walk_add_page(struct dsgl_walk *walk,\n\t\t\t\t\tsize_t size,\n\t\t\t\t\tdma_addr_t addr)\n{\n\tint j;\n\n\tif (!size)\n\t\treturn;\n\tj = walk->nents;\n\twalk->to->len[j % 8] = htons(size);\n\twalk->to->addr[j % 8] = cpu_to_be64(addr);\n\tj++;\n\tif ((j % 8) == 0)\n\t\twalk->to++;\n\twalk->nents = j;\n}\n\nstatic void  dsgl_walk_add_sg(struct dsgl_walk *walk,\n\t\t\t   struct scatterlist *sg,\n\t\t\t      unsigned int slen,\n\t\t\t      unsigned int skip)\n{\n\tint skip_len = 0;\n\tunsigned int left_size = slen, len = 0;\n\tunsigned int j = walk->nents;\n\tint offset, ent_len;\n\n\tif (!slen)\n\t\treturn;\n\twhile (sg && skip) {\n\t\tif (sg_dma_len(sg) <= skip) {\n\t\t\tskip -= sg_dma_len(sg);\n\t\t\tskip_len = 0;\n\t\t\tsg = sg_next(sg);\n\t\t} else {\n\t\t\tskip_len = skip;\n\t\t\tskip = 0;\n\t\t}\n\t}\n\n\twhile (left_size && sg) {\n\t\tlen = min_t(u32, left_size, sg_dma_len(sg) - skip_len);\n\t\toffset = 0;\n\t\twhile (len) {\n\t\t\tent_len =  min_t(u32, len, CHCR_DST_SG_SIZE);\n\t\t\twalk->to->len[j % 8] = htons(ent_len);\n\t\t\twalk->to->addr[j % 8] = cpu_to_be64(sg_dma_address(sg) +\n\t\t\t\t\t\t      offset + skip_len);\n\t\t\toffset += ent_len;\n\t\t\tlen -= ent_len;\n\t\t\tj++;\n\t\t\tif ((j % 8) == 0)\n\t\t\t\twalk->to++;\n\t\t}\n\t\twalk->last_sg = sg;\n\t\twalk->last_sg_len = min_t(u32, left_size, sg_dma_len(sg) -\n\t\t\t\t\t  skip_len) + skip_len;\n\t\tleft_size -= min_t(u32, left_size, sg_dma_len(sg) - skip_len);\n\t\tskip_len = 0;\n\t\tsg = sg_next(sg);\n\t}\n\twalk->nents = j;\n}\n\nstatic inline void ulptx_walk_init(struct ulptx_walk *walk,\n\t\t\t\t   struct ulptx_sgl *ulp)\n{\n\twalk->sgl = ulp;\n\twalk->nents = 0;\n\twalk->pair_idx = 0;\n\twalk->pair = ulp->sge;\n\twalk->last_sg = NULL;\n\twalk->last_sg_len = 0;\n}\n\nstatic inline void ulptx_walk_end(struct ulptx_walk *walk)\n{\n\twalk->sgl->cmd_nsge = htonl(ULPTX_CMD_V(ULP_TX_SC_DSGL) |\n\t\t\t      ULPTX_NSGE_V(walk->nents));\n}\n\n\nstatic inline void ulptx_walk_add_page(struct ulptx_walk *walk,\n\t\t\t\t\tsize_t size,\n\t\t\t\t\tdma_addr_t addr)\n{\n\tif (!size)\n\t\treturn;\n\n\tif (walk->nents == 0) {\n\t\twalk->sgl->len0 = cpu_to_be32(size);\n\t\twalk->sgl->addr0 = cpu_to_be64(addr);\n\t} else {\n\t\twalk->pair->addr[walk->pair_idx] = cpu_to_be64(addr);\n\t\twalk->pair->len[walk->pair_idx] = cpu_to_be32(size);\n\t\twalk->pair_idx = !walk->pair_idx;\n\t\tif (!walk->pair_idx)\n\t\t\twalk->pair++;\n\t}\n\twalk->nents++;\n}\n\nstatic void  ulptx_walk_add_sg(struct ulptx_walk *walk,\n\t\t\t\t\tstruct scatterlist *sg,\n\t\t\t       unsigned int len,\n\t\t\t       unsigned int skip)\n{\n\tint small;\n\tint skip_len = 0;\n\tunsigned int sgmin;\n\n\tif (!len)\n\t\treturn;\n\twhile (sg && skip) {\n\t\tif (sg_dma_len(sg) <= skip) {\n\t\t\tskip -= sg_dma_len(sg);\n\t\t\tskip_len = 0;\n\t\t\tsg = sg_next(sg);\n\t\t} else {\n\t\t\tskip_len = skip;\n\t\t\tskip = 0;\n\t\t}\n\t}\n\tWARN(!sg, \"SG should not be null here\\n\");\n\tif (sg && (walk->nents == 0)) {\n\t\tsmall = min_t(unsigned int, sg_dma_len(sg) - skip_len, len);\n\t\tsgmin = min_t(unsigned int, small, CHCR_SRC_SG_SIZE);\n\t\twalk->sgl->len0 = cpu_to_be32(sgmin);\n\t\twalk->sgl->addr0 = cpu_to_be64(sg_dma_address(sg) + skip_len);\n\t\twalk->nents++;\n\t\tlen -= sgmin;\n\t\twalk->last_sg = sg;\n\t\twalk->last_sg_len = sgmin + skip_len;\n\t\tskip_len += sgmin;\n\t\tif (sg_dma_len(sg) == skip_len) {\n\t\t\tsg = sg_next(sg);\n\t\t\tskip_len = 0;\n\t\t}\n\t}\n\n\twhile (sg && len) {\n\t\tsmall = min(sg_dma_len(sg) - skip_len, len);\n\t\tsgmin = min_t(unsigned int, small, CHCR_SRC_SG_SIZE);\n\t\twalk->pair->len[walk->pair_idx] = cpu_to_be32(sgmin);\n\t\twalk->pair->addr[walk->pair_idx] =\n\t\t\tcpu_to_be64(sg_dma_address(sg) + skip_len);\n\t\twalk->pair_idx = !walk->pair_idx;\n\t\twalk->nents++;\n\t\tif (!walk->pair_idx)\n\t\t\twalk->pair++;\n\t\tlen -= sgmin;\n\t\tskip_len += sgmin;\n\t\twalk->last_sg = sg;\n\t\twalk->last_sg_len = skip_len;\n\t\tif (sg_dma_len(sg) == skip_len) {\n\t\t\tsg = sg_next(sg);\n\t\t\tskip_len = 0;\n\t\t}\n\t}\n}\n\nstatic inline int get_cryptoalg_subtype(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tstruct chcr_alg_template *chcr_crypto_alg =\n\t\tcontainer_of(alg, struct chcr_alg_template, alg.skcipher);\n\n\treturn chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;\n}\n\nstatic int cxgb4_is_crypto_q_full(struct net_device *dev, unsigned int idx)\n{\n\tstruct adapter *adap = netdev2adap(dev);\n\tstruct sge_uld_txq_info *txq_info =\n\t\tadap->sge.uld_txq_info[CXGB4_TX_CRYPTO];\n\tstruct sge_uld_txq *txq;\n\tint ret = 0;\n\n\tlocal_bh_disable();\n\ttxq = &txq_info->uldtxq[idx];\n\tspin_lock(&txq->sendq.lock);\n\tif (txq->full)\n\t\tret = -1;\n\tspin_unlock(&txq->sendq.lock);\n\tlocal_bh_enable();\n\treturn ret;\n}\n\nstatic int generate_copy_rrkey(struct ablk_ctx *ablkctx,\n\t\t\t       struct _key_ctx *key_ctx)\n{\n\tif (ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC) {\n\t\tmemcpy(key_ctx->key, ablkctx->rrkey, ablkctx->enckey_len);\n\t} else {\n\t\tmemcpy(key_ctx->key,\n\t\t       ablkctx->key + (ablkctx->enckey_len >> 1),\n\t\t       ablkctx->enckey_len >> 1);\n\t\tmemcpy(key_ctx->key + (ablkctx->enckey_len >> 1),\n\t\t       ablkctx->rrkey, ablkctx->enckey_len >> 1);\n\t}\n\treturn 0;\n}\n\nstatic int chcr_hash_ent_in_wr(struct scatterlist *src,\n\t\t\t     unsigned int minsg,\n\t\t\t     unsigned int space,\n\t\t\t     unsigned int srcskip)\n{\n\tint srclen = 0;\n\tint srcsg = minsg;\n\tint soffset = 0, sless;\n\n\tif (sg_dma_len(src) == srcskip) {\n\t\tsrc = sg_next(src);\n\t\tsrcskip = 0;\n\t}\n\twhile (src && space > (sgl_ent_len[srcsg + 1])) {\n\t\tsless = min_t(unsigned int, sg_dma_len(src) - soffset -\tsrcskip,\n\t\t\t\t\t\t\tCHCR_SRC_SG_SIZE);\n\t\tsrclen += sless;\n\t\tsoffset += sless;\n\t\tsrcsg++;\n\t\tif (sg_dma_len(src) == (soffset + srcskip)) {\n\t\t\tsrc = sg_next(src);\n\t\t\tsoffset = 0;\n\t\t\tsrcskip = 0;\n\t\t}\n\t}\n\treturn srclen;\n}\n\nstatic int chcr_sg_ent_in_wr(struct scatterlist *src,\n\t\t\t     struct scatterlist *dst,\n\t\t\t     unsigned int minsg,\n\t\t\t     unsigned int space,\n\t\t\t     unsigned int srcskip,\n\t\t\t     unsigned int dstskip)\n{\n\tint srclen = 0, dstlen = 0;\n\tint srcsg = minsg, dstsg = minsg;\n\tint offset = 0, soffset = 0, less, sless = 0;\n\n\tif (sg_dma_len(src) == srcskip) {\n\t\tsrc = sg_next(src);\n\t\tsrcskip = 0;\n\t}\n\tif (sg_dma_len(dst) == dstskip) {\n\t\tdst = sg_next(dst);\n\t\tdstskip = 0;\n\t}\n\n\twhile (src && dst &&\n\t       space > (sgl_ent_len[srcsg + 1] + dsgl_ent_len[dstsg])) {\n\t\tsless = min_t(unsigned int, sg_dma_len(src) - srcskip - soffset,\n\t\t\t\tCHCR_SRC_SG_SIZE);\n\t\tsrclen += sless;\n\t\tsrcsg++;\n\t\toffset = 0;\n\t\twhile (dst && ((dstsg + 1) <= MAX_DSGL_ENT) &&\n\t\t       space > (sgl_ent_len[srcsg] + dsgl_ent_len[dstsg + 1])) {\n\t\t\tif (srclen <= dstlen)\n\t\t\t\tbreak;\n\t\t\tless = min_t(unsigned int, sg_dma_len(dst) - offset -\n\t\t\t\t     dstskip, CHCR_DST_SG_SIZE);\n\t\t\tdstlen += less;\n\t\t\toffset += less;\n\t\t\tif ((offset + dstskip) == sg_dma_len(dst)) {\n\t\t\t\tdst = sg_next(dst);\n\t\t\t\toffset = 0;\n\t\t\t}\n\t\t\tdstsg++;\n\t\t\tdstskip = 0;\n\t\t}\n\t\tsoffset += sless;\n\t\tif ((soffset + srcskip) == sg_dma_len(src)) {\n\t\t\tsrc = sg_next(src);\n\t\t\tsrcskip = 0;\n\t\t\tsoffset = 0;\n\t\t}\n\n\t}\n\treturn min(srclen, dstlen);\n}\n\nstatic int chcr_cipher_fallback(struct crypto_skcipher *cipher,\n\t\t\t\tstruct skcipher_request *req,\n\t\t\t\tu8 *iv,\n\t\t\t\tunsigned short op_type)\n{\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tint err;\n\n\tskcipher_request_set_tfm(&reqctx->fallback_req, cipher);\n\tskcipher_request_set_callback(&reqctx->fallback_req, req->base.flags,\n\t\t\t\t      req->base.complete, req->base.data);\n\tskcipher_request_set_crypt(&reqctx->fallback_req, req->src, req->dst,\n\t\t\t\t   req->cryptlen, iv);\n\n\terr = op_type ? crypto_skcipher_decrypt(&reqctx->fallback_req) :\n\t\t\tcrypto_skcipher_encrypt(&reqctx->fallback_req);\n\n\treturn err;\n\n}\n\nstatic inline int get_qidxs(struct crypto_async_request *req,\n\t\t\t    unsigned int *txqidx, unsigned int *rxqidx)\n{\n\tstruct crypto_tfm *tfm = req->tfm;\n\tint ret = 0;\n\n\tswitch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t{\n\t\tstruct aead_request *aead_req =\n\t\t\tcontainer_of(req, struct aead_request, base);\n\t\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(aead_req);\n\t\t*txqidx = reqctx->txqidx;\n\t\t*rxqidx = reqctx->rxqidx;\n\t\tbreak;\n\t}\n\tcase CRYPTO_ALG_TYPE_SKCIPHER:\n\t{\n\t\tstruct skcipher_request *sk_req =\n\t\t\tcontainer_of(req, struct skcipher_request, base);\n\t\tstruct chcr_skcipher_req_ctx *reqctx =\n\t\t\tskcipher_request_ctx(sk_req);\n\t\t*txqidx = reqctx->txqidx;\n\t\t*rxqidx = reqctx->rxqidx;\n\t\tbreak;\n\t}\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t{\n\t\tstruct ahash_request *ahash_req =\n\t\t\tcontainer_of(req, struct ahash_request, base);\n\t\tstruct chcr_ahash_req_ctx *reqctx =\n\t\t\tahash_request_ctx(ahash_req);\n\t\t*txqidx = reqctx->txqidx;\n\t\t*rxqidx = reqctx->rxqidx;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tret = -EINVAL;\n\t\t \n\t\tBUG();\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic inline void create_wreq(struct chcr_context *ctx,\n\t\t\t       struct chcr_wr *chcr_req,\n\t\t\t       struct crypto_async_request *req,\n\t\t\t       unsigned int imm,\n\t\t\t       int hash_sz,\n\t\t\t       unsigned int len16,\n\t\t\t       unsigned int sc_len,\n\t\t\t       unsigned int lcb)\n{\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tunsigned int tx_channel_id, rx_channel_id;\n\tunsigned int txqidx = 0, rxqidx = 0;\n\tunsigned int qid, fid, portno;\n\n\tget_qidxs(req, &txqidx, &rxqidx);\n\tqid = u_ctx->lldi.rxq_ids[rxqidx];\n\tfid = u_ctx->lldi.rxq_ids[0];\n\tportno = rxqidx / ctx->rxq_perchan;\n\ttx_channel_id = txqidx / ctx->txq_perchan;\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[portno]);\n\n\n\tchcr_req->wreq.op_to_cctx_size = FILL_WR_OP_CCTX_SIZE;\n\tchcr_req->wreq.pld_size_hash_size =\n\t\thtonl(FW_CRYPTO_LOOKASIDE_WR_HASH_SIZE_V(hash_sz));\n\tchcr_req->wreq.len16_pkd =\n\t\thtonl(FW_CRYPTO_LOOKASIDE_WR_LEN16_V(DIV_ROUND_UP(len16, 16)));\n\tchcr_req->wreq.cookie = cpu_to_be64((uintptr_t)req);\n\tchcr_req->wreq.rx_chid_to_rx_q_id = FILL_WR_RX_Q_ID(rx_channel_id, qid,\n\t\t\t\t\t\t\t    !!lcb, txqidx);\n\n\tchcr_req->ulptx.cmd_dest = FILL_ULPTX_CMD_DEST(tx_channel_id, fid);\n\tchcr_req->ulptx.len = htonl((DIV_ROUND_UP(len16, 16) -\n\t\t\t\t((sizeof(chcr_req->wreq)) >> 4)));\n\tchcr_req->sc_imm.cmd_more = FILL_CMD_MORE(!imm);\n\tchcr_req->sc_imm.len = cpu_to_be32(sizeof(struct cpl_tx_sec_pdu) +\n\t\t\t\t\t   sizeof(chcr_req->key_ctx) + sc_len);\n}\n\n \nstatic struct sk_buff *create_cipher_wr(struct cipher_wr_param *wrparam)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(wrparam->req);\n\tstruct chcr_context *ctx = c_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\n\tstruct sk_buff *skb = NULL;\n\tstruct chcr_wr *chcr_req;\n\tstruct cpl_rx_phys_dsgl *phys_cpl;\n\tstruct ulptx_sgl *ulptx;\n\tstruct chcr_skcipher_req_ctx *reqctx =\n\t\tskcipher_request_ctx(wrparam->req);\n\tunsigned int temp = 0, transhdr_len, dst_size;\n\tint error;\n\tint nents;\n\tunsigned int kctx_len;\n\tgfp_t flags = wrparam->req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\n\t\t\tGFP_KERNEL : GFP_ATOMIC;\n\tstruct adapter *adap = padap(ctx->dev);\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\tnents = sg_nents_xlen(reqctx->dstsg,  wrparam->bytes, CHCR_DST_SG_SIZE,\n\t\t\t      reqctx->dst_ofst);\n\tdst_size = get_space_for_phys_dsgl(nents);\n\tkctx_len = roundup(ablkctx->enckey_len, 16);\n\ttranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\n\tnents = sg_nents_xlen(reqctx->srcsg, wrparam->bytes,\n\t\t\t\t  CHCR_SRC_SG_SIZE, reqctx->src_ofst);\n\ttemp = reqctx->imm ? roundup(wrparam->bytes, 16) :\n\t\t\t\t     (sgl_len(nents) * 8);\n\ttranshdr_len += temp;\n\ttranshdr_len = roundup(transhdr_len, 16);\n\tskb = alloc_skb(SGE_MAX_WR_LEN, flags);\n\tif (!skb) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\tchcr_req = __skb_put_zero(skb, transhdr_len);\n\tchcr_req->sec_cpl.op_ivinsrtofst =\n\t\t\tFILL_SEC_CPL_OP_IVINSR(rx_channel_id, 2, 1);\n\n\tchcr_req->sec_cpl.pldlen = htonl(IV + wrparam->bytes);\n\tchcr_req->sec_cpl.aadstart_cipherstop_hi =\n\t\t\tFILL_SEC_CPL_CIPHERSTOP_HI(0, 0, IV + 1, 0);\n\n\tchcr_req->sec_cpl.cipherstop_lo_authinsert =\n\t\t\tFILL_SEC_CPL_AUTHINSERT(0, 0, 0, 0);\n\tchcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(reqctx->op, 0,\n\t\t\t\t\t\t\t ablkctx->ciph_mode,\n\t\t\t\t\t\t\t 0, 0, IV >> 1);\n\tchcr_req->sec_cpl.ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 0,\n\t\t\t\t\t\t\t  0, 1, dst_size);\n\n\tchcr_req->key_ctx.ctx_hdr = ablkctx->key_ctx_hdr;\n\tif ((reqctx->op == CHCR_DECRYPT_OP) &&\n\t    (!(get_cryptoalg_subtype(tfm) ==\n\t       CRYPTO_ALG_SUB_TYPE_CTR)) &&\n\t    (!(get_cryptoalg_subtype(tfm) ==\n\t       CRYPTO_ALG_SUB_TYPE_CTR_RFC3686))) {\n\t\tgenerate_copy_rrkey(ablkctx, &chcr_req->key_ctx);\n\t} else {\n\t\tif ((ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC) ||\n\t\t    (ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CTR)) {\n\t\t\tmemcpy(chcr_req->key_ctx.key, ablkctx->key,\n\t\t\t       ablkctx->enckey_len);\n\t\t} else {\n\t\t\tmemcpy(chcr_req->key_ctx.key, ablkctx->key +\n\t\t\t       (ablkctx->enckey_len >> 1),\n\t\t\t       ablkctx->enckey_len >> 1);\n\t\t\tmemcpy(chcr_req->key_ctx.key +\n\t\t\t       (ablkctx->enckey_len >> 1),\n\t\t\t       ablkctx->key,\n\t\t\t       ablkctx->enckey_len >> 1);\n\t\t}\n\t}\n\tphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\n\tulptx = (struct ulptx_sgl *)((u8 *)(phys_cpl + 1) + dst_size);\n\tchcr_add_cipher_src_ent(wrparam->req, ulptx, wrparam);\n\tchcr_add_cipher_dst_ent(wrparam->req, phys_cpl, wrparam, wrparam->qid);\n\n\tatomic_inc(&adap->chcr_stats.cipher_rqst);\n\ttemp = sizeof(struct cpl_rx_phys_dsgl) + dst_size + kctx_len + IV\n\t\t+ (reqctx->imm ? (wrparam->bytes) : 0);\n\tcreate_wreq(c_ctx(tfm), chcr_req, &(wrparam->req->base), reqctx->imm, 0,\n\t\t    transhdr_len, temp,\n\t\t\tablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC);\n\treqctx->skb = skb;\n\n\tif (reqctx->op && (ablkctx->ciph_mode ==\n\t\t\t   CHCR_SCMD_CIPHER_MODE_AES_CBC))\n\t\tsg_pcopy_to_buffer(wrparam->req->src,\n\t\t\tsg_nents(wrparam->req->src), wrparam->req->iv, 16,\n\t\t\treqctx->processed + wrparam->bytes - AES_BLOCK_SIZE);\n\n\treturn skb;\nerr:\n\treturn ERR_PTR(error);\n}\n\nstatic inline int chcr_keyctx_ck_size(unsigned int keylen)\n{\n\tint ck_size = 0;\n\n\tif (keylen == AES_KEYSIZE_128)\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\telse if (keylen == AES_KEYSIZE_192)\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\telse if (keylen == AES_KEYSIZE_256)\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\telse\n\t\tck_size = 0;\n\n\treturn ck_size;\n}\nstatic int chcr_cipher_fallback_setkey(struct crypto_skcipher *cipher,\n\t\t\t\t       const u8 *key,\n\t\t\t\t       unsigned int keylen)\n{\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(cipher));\n\n\tcrypto_skcipher_clear_flags(ablkctx->sw_cipher,\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(ablkctx->sw_cipher,\n\t\t\t\tcipher->base.crt_flags & CRYPTO_TFM_REQ_MASK);\n\treturn crypto_skcipher_setkey(ablkctx->sw_cipher, key, keylen);\n}\n\nstatic int chcr_aes_cbc_setkey(struct crypto_skcipher *cipher,\n\t\t\t       const u8 *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(cipher));\n\tunsigned int ck_size, context_size;\n\tu16 alignment = 0;\n\tint err;\n\n\terr = chcr_cipher_fallback_setkey(cipher, key, keylen);\n\tif (err)\n\t\tgoto badkey_err;\n\n\tck_size = chcr_keyctx_ck_size(keylen);\n\talignment = ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192 ? 8 : 0;\n\tmemcpy(ablkctx->key, key, keylen);\n\tablkctx->enckey_len = keylen;\n\tget_aes_decrypt_key(ablkctx->rrkey, ablkctx->key, keylen << 3);\n\tcontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\n\t\t\tkeylen + alignment) >> 4;\n\n\tablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\n\t\t\t\t\t\t0, 0, context_size);\n\tablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CBC;\n\treturn 0;\nbadkey_err:\n\tablkctx->enckey_len = 0;\n\n\treturn err;\n}\n\nstatic int chcr_aes_ctr_setkey(struct crypto_skcipher *cipher,\n\t\t\t\t   const u8 *key,\n\t\t\t\t   unsigned int keylen)\n{\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(cipher));\n\tunsigned int ck_size, context_size;\n\tu16 alignment = 0;\n\tint err;\n\n\terr = chcr_cipher_fallback_setkey(cipher, key, keylen);\n\tif (err)\n\t\tgoto badkey_err;\n\tck_size = chcr_keyctx_ck_size(keylen);\n\talignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;\n\tmemcpy(ablkctx->key, key, keylen);\n\tablkctx->enckey_len = keylen;\n\tcontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\n\t\t\tkeylen + alignment) >> 4;\n\n\tablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\n\t\t\t\t\t\t0, 0, context_size);\n\tablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;\n\n\treturn 0;\nbadkey_err:\n\tablkctx->enckey_len = 0;\n\n\treturn err;\n}\n\nstatic int chcr_aes_rfc3686_setkey(struct crypto_skcipher *cipher,\n\t\t\t\t   const u8 *key,\n\t\t\t\t   unsigned int keylen)\n{\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(cipher));\n\tunsigned int ck_size, context_size;\n\tu16 alignment = 0;\n\tint err;\n\n\tif (keylen < CTR_RFC3686_NONCE_SIZE)\n\t\treturn -EINVAL;\n\tmemcpy(ablkctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),\n\t       CTR_RFC3686_NONCE_SIZE);\n\n\tkeylen -= CTR_RFC3686_NONCE_SIZE;\n\terr = chcr_cipher_fallback_setkey(cipher, key, keylen);\n\tif (err)\n\t\tgoto badkey_err;\n\n\tck_size = chcr_keyctx_ck_size(keylen);\n\talignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;\n\tmemcpy(ablkctx->key, key, keylen);\n\tablkctx->enckey_len = keylen;\n\tcontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\n\t\t\tkeylen + alignment) >> 4;\n\n\tablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\n\t\t\t\t\t\t0, 0, context_size);\n\tablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;\n\n\treturn 0;\nbadkey_err:\n\tablkctx->enckey_len = 0;\n\n\treturn err;\n}\nstatic void ctr_add_iv(u8 *dstiv, u8 *srciv, u32 add)\n{\n\tunsigned int size = AES_BLOCK_SIZE;\n\t__be32 *b = (__be32 *)(dstiv + size);\n\tu32 c, prev;\n\n\tmemcpy(dstiv, srciv, AES_BLOCK_SIZE);\n\tfor (; size >= 4; size -= 4) {\n\t\tprev = be32_to_cpu(*--b);\n\t\tc = prev + add;\n\t\t*b = cpu_to_be32(c);\n\t\tif (prev < c)\n\t\t\tbreak;\n\t\tadd = 1;\n\t}\n\n}\n\nstatic unsigned int adjust_ctr_overflow(u8 *iv, u32 bytes)\n{\n\t__be32 *b = (__be32 *)(iv + AES_BLOCK_SIZE);\n\tu64 c;\n\tu32 temp = be32_to_cpu(*--b);\n\n\ttemp = ~temp;\n\tc = (u64)temp +  1;  \n\tif ((bytes / AES_BLOCK_SIZE) >= c)\n\t\tbytes = c * AES_BLOCK_SIZE;\n\treturn bytes;\n}\n\nstatic int chcr_update_tweak(struct skcipher_request *req, u8 *iv,\n\t\t\t     u32 isfinal)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(tfm));\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct crypto_aes_ctx aes;\n\tint ret, i;\n\tu8 *key;\n\tunsigned int keylen;\n\tint round = reqctx->last_req_len / AES_BLOCK_SIZE;\n\tint round8 = round / 8;\n\n\tmemcpy(iv, reqctx->iv, AES_BLOCK_SIZE);\n\n\tkeylen = ablkctx->enckey_len / 2;\n\tkey = ablkctx->key + keylen;\n\t \n\tif (KEY_CONTEXT_CK_SIZE_G(ntohl(ablkctx->key_ctx_hdr))\n\t\t\t== CHCR_KEYCTX_CIPHER_KEY_SIZE_192)\n\t\tret = aes_expandkey(&aes, key, keylen - 8);\n\telse\n\t\tret = aes_expandkey(&aes, key, keylen);\n\tif (ret)\n\t\treturn ret;\n\taes_encrypt(&aes, iv, iv);\n\tfor (i = 0; i < round8; i++)\n\t\tgf128mul_x8_ble((le128 *)iv, (le128 *)iv);\n\n\tfor (i = 0; i < (round % 8); i++)\n\t\tgf128mul_x_ble((le128 *)iv, (le128 *)iv);\n\n\tif (!isfinal)\n\t\taes_decrypt(&aes, iv, iv);\n\n\tmemzero_explicit(&aes, sizeof(aes));\n\treturn 0;\n}\n\nstatic int chcr_update_cipher_iv(struct skcipher_request *req,\n\t\t\t\t   struct cpl_fw6_pld *fw6_pld, u8 *iv)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tint subtype = get_cryptoalg_subtype(tfm);\n\tint ret = 0;\n\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR)\n\t\tctr_add_iv(iv, req->iv, (reqctx->processed /\n\t\t\t   AES_BLOCK_SIZE));\n\telse if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_RFC3686)\n\t\t*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +\n\t\t\tCTR_RFC3686_IV_SIZE) = cpu_to_be32((reqctx->processed /\n\t\t\t\t\t\tAES_BLOCK_SIZE) + 1);\n\telse if (subtype == CRYPTO_ALG_SUB_TYPE_XTS)\n\t\tret = chcr_update_tweak(req, iv, 0);\n\telse if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {\n\t\tif (reqctx->op)\n\t\t\t \n\t\t\tmemcpy(iv, req->iv, AES_BLOCK_SIZE);\n\t\telse\n\t\t\tmemcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);\n\t}\n\n\treturn ret;\n\n}\n\n \n\nstatic int chcr_final_cipher_iv(struct skcipher_request *req,\n\t\t\t\t   struct cpl_fw6_pld *fw6_pld, u8 *iv)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tint subtype = get_cryptoalg_subtype(tfm);\n\tint ret = 0;\n\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR)\n\t\tctr_add_iv(iv, req->iv, DIV_ROUND_UP(reqctx->processed,\n\t\t\t\t\t\t       AES_BLOCK_SIZE));\n\telse if (subtype == CRYPTO_ALG_SUB_TYPE_XTS) {\n\t\tif (!reqctx->partial_req)\n\t\t\tmemcpy(iv, reqctx->iv, AES_BLOCK_SIZE);\n\t\telse\n\t\t\tret = chcr_update_tweak(req, iv, 1);\n\t}\n\telse if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {\n\t\t \n\t\tif (!reqctx->op)\n\t\t\tmemcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);\n\n\t}\n\treturn ret;\n\n}\n\nstatic int chcr_handle_cipher_resp(struct skcipher_request *req,\n\t\t\t\t   unsigned char *input, int err)\n{\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct cpl_fw6_pld *fw6_pld = (struct cpl_fw6_pld *)input;\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(tfm));\n\tstruct uld_ctx *u_ctx = ULD_CTX(c_ctx(tfm));\n\tstruct chcr_dev *dev = c_ctx(tfm)->dev;\n\tstruct chcr_context *ctx = c_ctx(tfm);\n\tstruct adapter *adap = padap(ctx->dev);\n\tstruct cipher_wr_param wrparam;\n\tstruct sk_buff *skb;\n\tint bytes;\n\n\tif (err)\n\t\tgoto unmap;\n\tif (req->cryptlen == reqctx->processed) {\n\t\tchcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,\n\t\t\t\t      req);\n\t\terr = chcr_final_cipher_iv(req, fw6_pld, req->iv);\n\t\tgoto complete;\n\t}\n\n\tif (!reqctx->imm) {\n\t\tbytes = chcr_sg_ent_in_wr(reqctx->srcsg, reqctx->dstsg, 0,\n\t\t\t\t\t  CIP_SPACE_LEFT(ablkctx->enckey_len),\n\t\t\t\t\t  reqctx->src_ofst, reqctx->dst_ofst);\n\t\tif ((bytes + reqctx->processed) >= req->cryptlen)\n\t\t\tbytes  = req->cryptlen - reqctx->processed;\n\t\telse\n\t\t\tbytes = rounddown(bytes, 16);\n\t} else {\n\t\t \n\t\tbytes  = req->cryptlen - reqctx->processed;\n\t}\n\terr = chcr_update_cipher_iv(req, fw6_pld, reqctx->iv);\n\tif (err)\n\t\tgoto unmap;\n\n\tif (unlikely(bytes == 0)) {\n\t\tchcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,\n\t\t\t\t      req);\n\t\tmemcpy(req->iv, reqctx->init_iv, IV);\n\t\tatomic_inc(&adap->chcr_stats.fallback);\n\t\terr = chcr_cipher_fallback(ablkctx->sw_cipher, req, req->iv,\n\t\t\t\t\t   reqctx->op);\n\t\tgoto complete;\n\t}\n\n\tif (get_cryptoalg_subtype(tfm) ==\n\t    CRYPTO_ALG_SUB_TYPE_CTR)\n\t\tbytes = adjust_ctr_overflow(reqctx->iv, bytes);\n\twrparam.qid = u_ctx->lldi.rxq_ids[reqctx->rxqidx];\n\twrparam.req = req;\n\twrparam.bytes = bytes;\n\tskb = create_cipher_wr(&wrparam);\n\tif (IS_ERR(skb)) {\n\t\tpr_err(\"%s : Failed to form WR. No memory\\n\", __func__);\n\t\terr = PTR_ERR(skb);\n\t\tgoto unmap;\n\t}\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, reqctx->txqidx);\n\tchcr_send_wr(skb);\n\treqctx->last_req_len = bytes;\n\treqctx->processed += bytes;\n\tif (get_cryptoalg_subtype(tfm) ==\n\t\tCRYPTO_ALG_SUB_TYPE_CBC && req->base.flags ==\n\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP ) {\n\t\tcomplete(&ctx->cbc_aes_aio_done);\n\t}\n\treturn 0;\nunmap:\n\tchcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev, req);\ncomplete:\n\tif (get_cryptoalg_subtype(tfm) ==\n\t\tCRYPTO_ALG_SUB_TYPE_CBC && req->base.flags ==\n\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP ) {\n\t\tcomplete(&ctx->cbc_aes_aio_done);\n\t}\n\tchcr_dec_wrcount(dev);\n\tskcipher_request_complete(req, err);\n\treturn err;\n}\n\nstatic int process_cipher(struct skcipher_request *req,\n\t\t\t\t  unsigned short qid,\n\t\t\t\t  struct sk_buff **skb,\n\t\t\t\t  unsigned short op_type)\n{\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tunsigned int ivsize = crypto_skcipher_ivsize(tfm);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(tfm));\n\tstruct adapter *adap = padap(c_ctx(tfm)->dev);\n\tstruct\tcipher_wr_param wrparam;\n\tint bytes, err = -EINVAL;\n\tint subtype;\n\n\treqctx->processed = 0;\n\treqctx->partial_req = 0;\n\tif (!req->iv)\n\t\tgoto error;\n\tsubtype = get_cryptoalg_subtype(tfm);\n\tif ((ablkctx->enckey_len == 0) || (ivsize > AES_BLOCK_SIZE) ||\n\t    (req->cryptlen == 0) ||\n\t    (req->cryptlen % crypto_skcipher_blocksize(tfm))) {\n\t\tif (req->cryptlen == 0 && subtype != CRYPTO_ALG_SUB_TYPE_XTS)\n\t\t\tgoto fallback;\n\t\telse if (req->cryptlen % crypto_skcipher_blocksize(tfm) &&\n\t\t\t subtype == CRYPTO_ALG_SUB_TYPE_XTS)\n\t\t\tgoto fallback;\n\t\tpr_err(\"AES: Invalid value of Key Len %d nbytes %d IV Len %d\\n\",\n\t\t       ablkctx->enckey_len, req->cryptlen, ivsize);\n\t\tgoto error;\n\t}\n\n\terr = chcr_cipher_dma_map(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev, req);\n\tif (err)\n\t\tgoto error;\n\tif (req->cryptlen < (SGE_MAX_WR_LEN - (sizeof(struct chcr_wr) +\n\t\t\t\t\t    AES_MIN_KEY_SIZE +\n\t\t\t\t\t    sizeof(struct cpl_rx_phys_dsgl) +\n\t\t\t\t\t \n\t\t\t\t\t    32))) {\n\t\t \n\t\tunsigned int dnents = 0, transhdr_len, phys_dsgl, kctx_len;\n\n\t\tdnents = sg_nents_xlen(req->dst, req->cryptlen,\n\t\t\t\t       CHCR_DST_SG_SIZE, 0);\n\t\tphys_dsgl = get_space_for_phys_dsgl(dnents);\n\t\tkctx_len = roundup(ablkctx->enckey_len, 16);\n\t\ttranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, phys_dsgl);\n\t\treqctx->imm = (transhdr_len + IV + req->cryptlen) <=\n\t\t\tSGE_MAX_WR_LEN;\n\t\tbytes = IV + req->cryptlen;\n\n\t} else {\n\t\treqctx->imm = 0;\n\t}\n\n\tif (!reqctx->imm) {\n\t\tbytes = chcr_sg_ent_in_wr(req->src, req->dst, 0,\n\t\t\t\t\t  CIP_SPACE_LEFT(ablkctx->enckey_len),\n\t\t\t\t\t  0, 0);\n\t\tif ((bytes + reqctx->processed) >= req->cryptlen)\n\t\t\tbytes  = req->cryptlen - reqctx->processed;\n\t\telse\n\t\t\tbytes = rounddown(bytes, 16);\n\t} else {\n\t\tbytes = req->cryptlen;\n\t}\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR) {\n\t\tbytes = adjust_ctr_overflow(req->iv, bytes);\n\t}\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR_RFC3686) {\n\t\tmemcpy(reqctx->iv, ablkctx->nonce, CTR_RFC3686_NONCE_SIZE);\n\t\tmemcpy(reqctx->iv + CTR_RFC3686_NONCE_SIZE, req->iv,\n\t\t\t\tCTR_RFC3686_IV_SIZE);\n\n\t\t \n\t\t*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +\n\t\t\tCTR_RFC3686_IV_SIZE) = cpu_to_be32(1);\n\t\tmemcpy(reqctx->init_iv, reqctx->iv, IV);\n\n\t} else {\n\n\t\tmemcpy(reqctx->iv, req->iv, IV);\n\t\tmemcpy(reqctx->init_iv, req->iv, IV);\n\t}\n\tif (unlikely(bytes == 0)) {\n\t\tchcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,\n\t\t\t\t      req);\nfallback:       atomic_inc(&adap->chcr_stats.fallback);\n\t\terr = chcr_cipher_fallback(ablkctx->sw_cipher, req,\n\t\t\t\t\t   subtype ==\n\t\t\t\t\t   CRYPTO_ALG_SUB_TYPE_CTR_RFC3686 ?\n\t\t\t\t\t   reqctx->iv : req->iv,\n\t\t\t\t\t   op_type);\n\t\tgoto error;\n\t}\n\treqctx->op = op_type;\n\treqctx->srcsg = req->src;\n\treqctx->dstsg = req->dst;\n\treqctx->src_ofst = 0;\n\treqctx->dst_ofst = 0;\n\twrparam.qid = qid;\n\twrparam.req = req;\n\twrparam.bytes = bytes;\n\t*skb = create_cipher_wr(&wrparam);\n\tif (IS_ERR(*skb)) {\n\t\terr = PTR_ERR(*skb);\n\t\tgoto unmap;\n\t}\n\treqctx->processed = bytes;\n\treqctx->last_req_len = bytes;\n\treqctx->partial_req = !!(req->cryptlen - reqctx->processed);\n\n\treturn 0;\nunmap:\n\tchcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev, req);\nerror:\n\treturn err;\n}\n\nstatic int chcr_aes_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct chcr_dev *dev = c_ctx(tfm)->dev;\n\tstruct sk_buff *skb = NULL;\n\tint err;\n\tstruct uld_ctx *u_ctx = ULD_CTX(c_ctx(tfm));\n\tstruct chcr_context *ctx = c_ctx(tfm);\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treqctx->txqidx = cpu % ctx->ntxq;\n\treqctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\terr = chcr_inc_wrcount(dev);\n\tif (err)\n\t\treturn -ENXIO;\n\tif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\t\treqctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)))) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto error;\n\t}\n\n\terr = process_cipher(req, u_ctx->lldi.rxq_ids[reqctx->rxqidx],\n\t\t\t     &skb, CHCR_ENCRYPT_OP);\n\tif (err || !skb)\n\t\treturn  err;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, reqctx->txqidx);\n\tchcr_send_wr(skb);\n\tif (get_cryptoalg_subtype(tfm) ==\n\t\tCRYPTO_ALG_SUB_TYPE_CBC && req->base.flags ==\n\t\t\tCRYPTO_TFM_REQ_MAY_SLEEP ) {\n\t\t\treqctx->partial_req = 1;\n\t\t\twait_for_completion(&ctx->cbc_aes_aio_done);\n        }\n\treturn -EINPROGRESS;\nerror:\n\tchcr_dec_wrcount(dev);\n\treturn err;\n}\n\nstatic int chcr_aes_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct uld_ctx *u_ctx = ULD_CTX(c_ctx(tfm));\n\tstruct chcr_dev *dev = c_ctx(tfm)->dev;\n\tstruct sk_buff *skb = NULL;\n\tint err;\n\tstruct chcr_context *ctx = c_ctx(tfm);\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treqctx->txqidx = cpu % ctx->ntxq;\n\treqctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\terr = chcr_inc_wrcount(dev);\n\tif (err)\n\t\treturn -ENXIO;\n\n\tif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\t\treqctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))))\n\t\t\treturn -ENOSPC;\n\terr = process_cipher(req, u_ctx->lldi.rxq_ids[reqctx->rxqidx],\n\t\t\t     &skb, CHCR_DECRYPT_OP);\n\tif (err || !skb)\n\t\treturn err;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, reqctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\n}\nstatic int chcr_device_init(struct chcr_context *ctx)\n{\n\tstruct uld_ctx *u_ctx = NULL;\n\tint txq_perchan, ntxq;\n\tint err = 0, rxq_perchan;\n\n\tif (!ctx->dev) {\n\t\tu_ctx = assign_chcr_device();\n\t\tif (!u_ctx) {\n\t\t\terr = -ENXIO;\n\t\t\tpr_err(\"chcr device assignment fails\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tctx->dev = &u_ctx->dev;\n\t\tntxq = u_ctx->lldi.ntxq;\n\t\trxq_perchan = u_ctx->lldi.nrxq / u_ctx->lldi.nchan;\n\t\ttxq_perchan = ntxq / u_ctx->lldi.nchan;\n\t\tctx->ntxq = ntxq;\n\t\tctx->nrxq = u_ctx->lldi.nrxq;\n\t\tctx->rxq_perchan = rxq_perchan;\n\t\tctx->txq_perchan = txq_perchan;\n\t}\nout:\n\treturn err;\n}\n\nstatic int chcr_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tstruct chcr_context *ctx = crypto_skcipher_ctx(tfm);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\n\n\tablkctx->sw_cipher = crypto_alloc_skcipher(alg->base.cra_name, 0,\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ablkctx->sw_cipher)) {\n\t\tpr_err(\"failed to allocate fallback for %s\\n\", alg->base.cra_name);\n\t\treturn PTR_ERR(ablkctx->sw_cipher);\n\t}\n\tinit_completion(&ctx->cbc_aes_aio_done);\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct chcr_skcipher_req_ctx) +\n\t\t\t\t\t crypto_skcipher_reqsize(ablkctx->sw_cipher));\n\n\treturn chcr_device_init(ctx);\n}\n\nstatic int chcr_rfc3686_init(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tstruct chcr_context *ctx = crypto_skcipher_ctx(tfm);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\n\n\t \n\tablkctx->sw_cipher = crypto_alloc_skcipher(\"ctr(aes)\", 0,\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(ablkctx->sw_cipher)) {\n\t\tpr_err(\"failed to allocate fallback for %s\\n\", alg->base.cra_name);\n\t\treturn PTR_ERR(ablkctx->sw_cipher);\n\t}\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct chcr_skcipher_req_ctx) +\n\t\t\t\t    crypto_skcipher_reqsize(ablkctx->sw_cipher));\n\treturn chcr_device_init(ctx);\n}\n\n\nstatic void chcr_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct chcr_context *ctx = crypto_skcipher_ctx(tfm);\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\n\n\tcrypto_free_skcipher(ablkctx->sw_cipher);\n}\n\nstatic int get_alg_config(struct algo_param *params,\n\t\t\t  unsigned int auth_size)\n{\n\tswitch (auth_size) {\n\tcase SHA1_DIGEST_SIZE:\n\t\tparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_160;\n\t\tparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA1;\n\t\tparams->result_size = SHA1_DIGEST_SIZE;\n\t\tbreak;\n\tcase SHA224_DIGEST_SIZE:\n\t\tparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\n\t\tparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA224;\n\t\tparams->result_size = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\tcase SHA256_DIGEST_SIZE:\n\t\tparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\n\t\tparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA256;\n\t\tparams->result_size = SHA256_DIGEST_SIZE;\n\t\tbreak;\n\tcase SHA384_DIGEST_SIZE:\n\t\tparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_512;\n\t\tparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA512_384;\n\t\tparams->result_size = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\tcase SHA512_DIGEST_SIZE:\n\t\tparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_512;\n\t\tparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA512_512;\n\t\tparams->result_size = SHA512_DIGEST_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"ERROR, unsupported digest size\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic inline void chcr_free_shash(struct crypto_shash *base_hash)\n{\n\t\tcrypto_free_shash(base_hash);\n}\n\n \nstatic struct sk_buff *create_hash_wr(struct ahash_request *req,\n\t\t\t\t      struct hash_wr_param *param)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct chcr_context *ctx = h_ctx(tfm);\n\tstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\n\tstruct sk_buff *skb = NULL;\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct chcr_wr *chcr_req;\n\tstruct ulptx_sgl *ulptx;\n\tunsigned int nents = 0, transhdr_len;\n\tunsigned int temp = 0;\n\tgfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\n\t\tGFP_ATOMIC;\n\tstruct adapter *adap = padap(h_ctx(tfm)->dev);\n\tint error = 0;\n\tunsigned int rx_channel_id = req_ctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\ttranshdr_len = HASH_TRANSHDR_SIZE(param->kctx_len);\n\treq_ctx->hctx_wr.imm = (transhdr_len + param->bfr_len +\n\t\t\t\tparam->sg_len) <= SGE_MAX_WR_LEN;\n\tnents = sg_nents_xlen(req_ctx->hctx_wr.srcsg, param->sg_len,\n\t\t      CHCR_SRC_SG_SIZE, req_ctx->hctx_wr.src_ofst);\n\tnents += param->bfr_len ? 1 : 0;\n\ttranshdr_len += req_ctx->hctx_wr.imm ? roundup(param->bfr_len +\n\t\t\t\tparam->sg_len, 16) : (sgl_len(nents) * 8);\n\ttranshdr_len = roundup(transhdr_len, 16);\n\n\tskb = alloc_skb(transhdr_len, flags);\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\tchcr_req = __skb_put_zero(skb, transhdr_len);\n\n\tchcr_req->sec_cpl.op_ivinsrtofst =\n\t\tFILL_SEC_CPL_OP_IVINSR(rx_channel_id, 2, 0);\n\n\tchcr_req->sec_cpl.pldlen = htonl(param->bfr_len + param->sg_len);\n\n\tchcr_req->sec_cpl.aadstart_cipherstop_hi =\n\t\tFILL_SEC_CPL_CIPHERSTOP_HI(0, 0, 0, 0);\n\tchcr_req->sec_cpl.cipherstop_lo_authinsert =\n\t\tFILL_SEC_CPL_AUTHINSERT(0, 1, 0, 0);\n\tchcr_req->sec_cpl.seqno_numivs =\n\t\tFILL_SEC_CPL_SCMD0_SEQNO(0, 0, 0, param->alg_prm.auth_mode,\n\t\t\t\t\t param->opad_needed, 0);\n\n\tchcr_req->sec_cpl.ivgen_hdrlen =\n\t\tFILL_SEC_CPL_IVGEN_HDRLEN(param->last, param->more, 0, 1, 0, 0);\n\n\tmemcpy(chcr_req->key_ctx.key, req_ctx->partial_hash,\n\t       param->alg_prm.result_size);\n\n\tif (param->opad_needed)\n\t\tmemcpy(chcr_req->key_ctx.key +\n\t\t       ((param->alg_prm.result_size <= 32) ? 32 :\n\t\t\tCHCR_HASH_MAX_DIGEST_SIZE),\n\t\t       hmacctx->opad, param->alg_prm.result_size);\n\n\tchcr_req->key_ctx.ctx_hdr = FILL_KEY_CTX_HDR(CHCR_KEYCTX_NO_KEY,\n\t\t\t\t\t    param->alg_prm.mk_size, 0,\n\t\t\t\t\t    param->opad_needed,\n\t\t\t\t\t    ((param->kctx_len +\n\t\t\t\t\t     sizeof(chcr_req->key_ctx)) >> 4));\n\tchcr_req->sec_cpl.scmd1 = cpu_to_be64((u64)param->scmd1);\n\tulptx = (struct ulptx_sgl *)((u8 *)(chcr_req + 1) + param->kctx_len +\n\t\t\t\t     DUMMY_BYTES);\n\tif (param->bfr_len != 0) {\n\t\treq_ctx->hctx_wr.dma_addr =\n\t\t\tdma_map_single(&u_ctx->lldi.pdev->dev, req_ctx->reqbfr,\n\t\t\t\t       param->bfr_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&u_ctx->lldi.pdev->dev,\n\t\t\t\t       req_ctx->hctx_wr. dma_addr)) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\t\treq_ctx->hctx_wr.dma_len = param->bfr_len;\n\t} else {\n\t\treq_ctx->hctx_wr.dma_addr = 0;\n\t}\n\tchcr_add_hash_src_ent(req, ulptx, param);\n\t \n\ttemp = param->kctx_len + DUMMY_BYTES + (req_ctx->hctx_wr.imm ?\n\t\t\t\t(param->sg_len + param->bfr_len) : 0);\n\tatomic_inc(&adap->chcr_stats.digest_rqst);\n\tcreate_wreq(h_ctx(tfm), chcr_req, &req->base, req_ctx->hctx_wr.imm,\n\t\t    param->hash_size, transhdr_len,\n\t\t    temp,  0);\n\treq_ctx->hctx_wr.skb = skb;\n\treturn skb;\nerr:\n\tkfree_skb(skb);\n\treturn  ERR_PTR(error);\n}\n\nstatic int chcr_ahash_update(struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\n\tstruct uld_ctx *u_ctx = ULD_CTX(h_ctx(rtfm));\n\tstruct chcr_context *ctx = h_ctx(rtfm);\n\tstruct chcr_dev *dev = h_ctx(rtfm)->dev;\n\tstruct sk_buff *skb;\n\tu8 remainder = 0, bs;\n\tunsigned int nbytes = req->nbytes;\n\tstruct hash_wr_param params;\n\tint error;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treq_ctx->txqidx = cpu % ctx->ntxq;\n\treq_ctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\tbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\n\tif (nbytes + req_ctx->reqlen >= bs) {\n\t\tremainder = (nbytes + req_ctx->reqlen) % bs;\n\t\tnbytes = nbytes + req_ctx->reqlen - remainder;\n\t} else {\n\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src), req_ctx->reqbfr\n\t\t\t\t   + req_ctx->reqlen, nbytes, 0);\n\t\treq_ctx->reqlen += nbytes;\n\t\treturn 0;\n\t}\n\terror = chcr_inc_wrcount(dev);\n\tif (error)\n\t\treturn -ENXIO;\n\t \n\tif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\t\treq_ctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)))) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto err;\n\t}\n\n\tchcr_init_hctx_per_wr(req_ctx);\n\terror = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);\n\tif (error) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\tget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\n\tparams.kctx_len = roundup(params.alg_prm.result_size, 16);\n\tparams.sg_len = chcr_hash_ent_in_wr(req->src, !!req_ctx->reqlen,\n\t\t\t\t     HASH_SPACE_LEFT(params.kctx_len), 0);\n\tif (params.sg_len > req->nbytes)\n\t\tparams.sg_len = req->nbytes;\n\tparams.sg_len = rounddown(params.sg_len + req_ctx->reqlen, bs) -\n\t\t\treq_ctx->reqlen;\n\tparams.opad_needed = 0;\n\tparams.more = 1;\n\tparams.last = 0;\n\tparams.bfr_len = req_ctx->reqlen;\n\tparams.scmd1 = 0;\n\treq_ctx->hctx_wr.srcsg = req->src;\n\n\tparams.hash_size = params.alg_prm.result_size;\n\treq_ctx->data_len += params.sg_len + params.bfr_len;\n\tskb = create_hash_wr(req, &params);\n\tif (IS_ERR(skb)) {\n\t\terror = PTR_ERR(skb);\n\t\tgoto unmap;\n\t}\n\n\treq_ctx->hctx_wr.processed += params.sg_len;\n\tif (remainder) {\n\t\t \n\t\tswap(req_ctx->reqbfr, req_ctx->skbfr);\n\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src),\n\t\t\t\t   req_ctx->reqbfr, remainder, req->nbytes -\n\t\t\t\t   remainder);\n\t}\n\treq_ctx->reqlen = remainder;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, req_ctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\nunmap:\n\tchcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);\nerr:\n\tchcr_dec_wrcount(dev);\n\treturn error;\n}\n\nstatic void create_last_hash_block(char *bfr_ptr, unsigned int bs, u64 scmd1)\n{\n\tmemset(bfr_ptr, 0, bs);\n\t*bfr_ptr = 0x80;\n\tif (bs == 64)\n\t\t*(__be64 *)(bfr_ptr + 56) = cpu_to_be64(scmd1  << 3);\n\telse\n\t\t*(__be64 *)(bfr_ptr + 120) =  cpu_to_be64(scmd1  << 3);\n}\n\nstatic int chcr_ahash_final(struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\n\tstruct chcr_dev *dev = h_ctx(rtfm)->dev;\n\tstruct hash_wr_param params;\n\tstruct sk_buff *skb;\n\tstruct uld_ctx *u_ctx = ULD_CTX(h_ctx(rtfm));\n\tstruct chcr_context *ctx = h_ctx(rtfm);\n\tu8 bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\tint error;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treq_ctx->txqidx = cpu % ctx->ntxq;\n\treq_ctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\terror = chcr_inc_wrcount(dev);\n\tif (error)\n\t\treturn -ENXIO;\n\n\tchcr_init_hctx_per_wr(req_ctx);\n\tif (is_hmac(crypto_ahash_tfm(rtfm)))\n\t\tparams.opad_needed = 1;\n\telse\n\t\tparams.opad_needed = 0;\n\tparams.sg_len = 0;\n\treq_ctx->hctx_wr.isfinal = 1;\n\tget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\n\tparams.kctx_len = roundup(params.alg_prm.result_size, 16);\n\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\tparams.opad_needed = 1;\n\t\tparams.kctx_len *= 2;\n\t} else {\n\t\tparams.opad_needed = 0;\n\t}\n\n\treq_ctx->hctx_wr.result = 1;\n\tparams.bfr_len = req_ctx->reqlen;\n\treq_ctx->data_len += params.bfr_len + params.sg_len;\n\treq_ctx->hctx_wr.srcsg = req->src;\n\tif (req_ctx->reqlen == 0) {\n\t\tcreate_last_hash_block(req_ctx->reqbfr, bs, req_ctx->data_len);\n\t\tparams.last = 0;\n\t\tparams.more = 1;\n\t\tparams.scmd1 = 0;\n\t\tparams.bfr_len = bs;\n\n\t} else {\n\t\tparams.scmd1 = req_ctx->data_len;\n\t\tparams.last = 1;\n\t\tparams.more = 0;\n\t}\n\tparams.hash_size = crypto_ahash_digestsize(rtfm);\n\tskb = create_hash_wr(req, &params);\n\tif (IS_ERR(skb)) {\n\t\terror = PTR_ERR(skb);\n\t\tgoto err;\n\t}\n\treq_ctx->reqlen = 0;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, req_ctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\nerr:\n\tchcr_dec_wrcount(dev);\n\treturn error;\n}\n\nstatic int chcr_ahash_finup(struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\n\tstruct chcr_dev *dev = h_ctx(rtfm)->dev;\n\tstruct uld_ctx *u_ctx = ULD_CTX(h_ctx(rtfm));\n\tstruct chcr_context *ctx = h_ctx(rtfm);\n\tstruct sk_buff *skb;\n\tstruct hash_wr_param params;\n\tu8  bs;\n\tint error;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treq_ctx->txqidx = cpu % ctx->ntxq;\n\treq_ctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\tbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\terror = chcr_inc_wrcount(dev);\n\tif (error)\n\t\treturn -ENXIO;\n\n\tif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\t\treq_ctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)))) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto err;\n\t}\n\tchcr_init_hctx_per_wr(req_ctx);\n\terror = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);\n\tif (error) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\n\tparams.kctx_len = roundup(params.alg_prm.result_size, 16);\n\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\tparams.kctx_len *= 2;\n\t\tparams.opad_needed = 1;\n\t} else {\n\t\tparams.opad_needed = 0;\n\t}\n\n\tparams.sg_len = chcr_hash_ent_in_wr(req->src, !!req_ctx->reqlen,\n\t\t\t\t    HASH_SPACE_LEFT(params.kctx_len), 0);\n\tif (params.sg_len < req->nbytes) {\n\t\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\t\tparams.kctx_len /= 2;\n\t\t\tparams.opad_needed = 0;\n\t\t}\n\t\tparams.last = 0;\n\t\tparams.more = 1;\n\t\tparams.sg_len = rounddown(params.sg_len + req_ctx->reqlen, bs)\n\t\t\t\t\t- req_ctx->reqlen;\n\t\tparams.hash_size = params.alg_prm.result_size;\n\t\tparams.scmd1 = 0;\n\t} else {\n\t\tparams.last = 1;\n\t\tparams.more = 0;\n\t\tparams.sg_len = req->nbytes;\n\t\tparams.hash_size = crypto_ahash_digestsize(rtfm);\n\t\tparams.scmd1 = req_ctx->data_len + req_ctx->reqlen +\n\t\t\t\tparams.sg_len;\n\t}\n\tparams.bfr_len = req_ctx->reqlen;\n\treq_ctx->data_len += params.bfr_len + params.sg_len;\n\treq_ctx->hctx_wr.result = 1;\n\treq_ctx->hctx_wr.srcsg = req->src;\n\tif ((req_ctx->reqlen + req->nbytes) == 0) {\n\t\tcreate_last_hash_block(req_ctx->reqbfr, bs, req_ctx->data_len);\n\t\tparams.last = 0;\n\t\tparams.more = 1;\n\t\tparams.scmd1 = 0;\n\t\tparams.bfr_len = bs;\n\t}\n\tskb = create_hash_wr(req, &params);\n\tif (IS_ERR(skb)) {\n\t\terror = PTR_ERR(skb);\n\t\tgoto unmap;\n\t}\n\treq_ctx->reqlen = 0;\n\treq_ctx->hctx_wr.processed += params.sg_len;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, req_ctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\nunmap:\n\tchcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);\nerr:\n\tchcr_dec_wrcount(dev);\n\treturn error;\n}\n\nstatic int chcr_ahash_digest(struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\n\tstruct chcr_dev *dev = h_ctx(rtfm)->dev;\n\tstruct uld_ctx *u_ctx = ULD_CTX(h_ctx(rtfm));\n\tstruct chcr_context *ctx = h_ctx(rtfm);\n\tstruct sk_buff *skb;\n\tstruct hash_wr_param params;\n\tu8  bs;\n\tint error;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treq_ctx->txqidx = cpu % ctx->ntxq;\n\treq_ctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\trtfm->init(req);\n\tbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\terror = chcr_inc_wrcount(dev);\n\tif (error)\n\t\treturn -ENXIO;\n\n\tif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\t\treq_ctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)))) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto err;\n\t}\n\n\tchcr_init_hctx_per_wr(req_ctx);\n\terror = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);\n\tif (error) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\n\tparams.kctx_len = roundup(params.alg_prm.result_size, 16);\n\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\tparams.kctx_len *= 2;\n\t\tparams.opad_needed = 1;\n\t} else {\n\t\tparams.opad_needed = 0;\n\t}\n\tparams.sg_len = chcr_hash_ent_in_wr(req->src, !!req_ctx->reqlen,\n\t\t\t\tHASH_SPACE_LEFT(params.kctx_len), 0);\n\tif (params.sg_len < req->nbytes) {\n\t\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\t\tparams.kctx_len /= 2;\n\t\t\tparams.opad_needed = 0;\n\t\t}\n\t\tparams.last = 0;\n\t\tparams.more = 1;\n\t\tparams.scmd1 = 0;\n\t\tparams.sg_len = rounddown(params.sg_len, bs);\n\t\tparams.hash_size = params.alg_prm.result_size;\n\t} else {\n\t\tparams.sg_len = req->nbytes;\n\t\tparams.hash_size = crypto_ahash_digestsize(rtfm);\n\t\tparams.last = 1;\n\t\tparams.more = 0;\n\t\tparams.scmd1 = req->nbytes + req_ctx->data_len;\n\n\t}\n\tparams.bfr_len = 0;\n\treq_ctx->hctx_wr.result = 1;\n\treq_ctx->hctx_wr.srcsg = req->src;\n\treq_ctx->data_len += params.bfr_len + params.sg_len;\n\n\tif (req->nbytes == 0) {\n\t\tcreate_last_hash_block(req_ctx->reqbfr, bs, req_ctx->data_len);\n\t\tparams.more = 1;\n\t\tparams.bfr_len = bs;\n\t}\n\n\tskb = create_hash_wr(req, &params);\n\tif (IS_ERR(skb)) {\n\t\terror = PTR_ERR(skb);\n\t\tgoto unmap;\n\t}\n\treq_ctx->hctx_wr.processed += params.sg_len;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, req_ctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\nunmap:\n\tchcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);\nerr:\n\tchcr_dec_wrcount(dev);\n\treturn error;\n}\n\nstatic int chcr_ahash_continue(struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);\n\tstruct chcr_hctx_per_wr *hctx_wr = &reqctx->hctx_wr;\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\n\tstruct chcr_context *ctx = h_ctx(rtfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct sk_buff *skb;\n\tstruct hash_wr_param params;\n\tu8  bs;\n\tint error;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treqctx->txqidx = cpu % ctx->ntxq;\n\treqctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\tbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\tget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\n\tparams.kctx_len = roundup(params.alg_prm.result_size, 16);\n\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\tparams.kctx_len *= 2;\n\t\tparams.opad_needed = 1;\n\t} else {\n\t\tparams.opad_needed = 0;\n\t}\n\tparams.sg_len = chcr_hash_ent_in_wr(hctx_wr->srcsg, 0,\n\t\t\t\t\t    HASH_SPACE_LEFT(params.kctx_len),\n\t\t\t\t\t    hctx_wr->src_ofst);\n\tif ((params.sg_len + hctx_wr->processed) > req->nbytes)\n\t\tparams.sg_len = req->nbytes - hctx_wr->processed;\n\tif (!hctx_wr->result ||\n\t    ((params.sg_len + hctx_wr->processed) < req->nbytes)) {\n\t\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\t\tparams.kctx_len /= 2;\n\t\t\tparams.opad_needed = 0;\n\t\t}\n\t\tparams.last = 0;\n\t\tparams.more = 1;\n\t\tparams.sg_len = rounddown(params.sg_len, bs);\n\t\tparams.hash_size = params.alg_prm.result_size;\n\t\tparams.scmd1 = 0;\n\t} else {\n\t\tparams.last = 1;\n\t\tparams.more = 0;\n\t\tparams.hash_size = crypto_ahash_digestsize(rtfm);\n\t\tparams.scmd1 = reqctx->data_len + params.sg_len;\n\t}\n\tparams.bfr_len = 0;\n\treqctx->data_len += params.sg_len;\n\tskb = create_hash_wr(req, &params);\n\tif (IS_ERR(skb)) {\n\t\terror = PTR_ERR(skb);\n\t\tgoto err;\n\t}\n\thctx_wr->processed += params.sg_len;\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, reqctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn 0;\nerr:\n\treturn error;\n}\n\nstatic inline void chcr_handle_ahash_resp(struct ahash_request *req,\n\t\t\t\t\t  unsigned char *input,\n\t\t\t\t\t  int err)\n{\n\tstruct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);\n\tstruct chcr_hctx_per_wr *hctx_wr = &reqctx->hctx_wr;\n\tint digestsize, updated_digestsize;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct uld_ctx *u_ctx = ULD_CTX(h_ctx(tfm));\n\tstruct chcr_dev *dev = h_ctx(tfm)->dev;\n\n\tif (input == NULL)\n\t\tgoto out;\n\tdigestsize = crypto_ahash_digestsize(crypto_ahash_reqtfm(req));\n\tupdated_digestsize = digestsize;\n\tif (digestsize == SHA224_DIGEST_SIZE)\n\t\tupdated_digestsize = SHA256_DIGEST_SIZE;\n\telse if (digestsize == SHA384_DIGEST_SIZE)\n\t\tupdated_digestsize = SHA512_DIGEST_SIZE;\n\n\tif (hctx_wr->dma_addr) {\n\t\tdma_unmap_single(&u_ctx->lldi.pdev->dev, hctx_wr->dma_addr,\n\t\t\t\t hctx_wr->dma_len, DMA_TO_DEVICE);\n\t\thctx_wr->dma_addr = 0;\n\t}\n\tif (hctx_wr->isfinal || ((hctx_wr->processed + reqctx->reqlen) ==\n\t\t\t\t req->nbytes)) {\n\t\tif (hctx_wr->result == 1) {\n\t\t\thctx_wr->result = 0;\n\t\t\tmemcpy(req->result, input + sizeof(struct cpl_fw6_pld),\n\t\t\t       digestsize);\n\t\t} else {\n\t\t\tmemcpy(reqctx->partial_hash,\n\t\t\t       input + sizeof(struct cpl_fw6_pld),\n\t\t\t       updated_digestsize);\n\n\t\t}\n\t\tgoto unmap;\n\t}\n\tmemcpy(reqctx->partial_hash, input + sizeof(struct cpl_fw6_pld),\n\t       updated_digestsize);\n\n\terr = chcr_ahash_continue(req);\n\tif (err)\n\t\tgoto unmap;\n\treturn;\nunmap:\n\tif (hctx_wr->is_sg_map)\n\t\tchcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);\n\n\nout:\n\tchcr_dec_wrcount(dev);\n\tahash_request_complete(req, err);\n}\n\n \nint chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,\n\t\t\t int err)\n{\n\tstruct crypto_tfm *tfm = req->tfm;\n\tstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct adapter *adap = padap(ctx->dev);\n\n\tswitch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\terr = chcr_handle_aead_resp(aead_request_cast(req), input, err);\n\t\tbreak;\n\n\tcase CRYPTO_ALG_TYPE_SKCIPHER:\n\t\t chcr_handle_cipher_resp(skcipher_request_cast(req),\n\t\t\t\t\t       input, err);\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tchcr_handle_ahash_resp(ahash_request_cast(req), input, err);\n\t\t}\n\tatomic_inc(&adap->chcr_stats.complete);\n\treturn err;\n}\nstatic int chcr_ahash_export(struct ahash_request *areq, void *out)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\n\tstruct chcr_ahash_req_ctx *state = out;\n\n\tstate->reqlen = req_ctx->reqlen;\n\tstate->data_len = req_ctx->data_len;\n\tmemcpy(state->bfr1, req_ctx->reqbfr, req_ctx->reqlen);\n\tmemcpy(state->partial_hash, req_ctx->partial_hash,\n\t       CHCR_HASH_MAX_DIGEST_SIZE);\n\tchcr_init_hctx_per_wr(state);\n\treturn 0;\n}\n\nstatic int chcr_ahash_import(struct ahash_request *areq, const void *in)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\n\tstruct chcr_ahash_req_ctx *state = (struct chcr_ahash_req_ctx *)in;\n\n\treq_ctx->reqlen = state->reqlen;\n\treq_ctx->data_len = state->data_len;\n\treq_ctx->reqbfr = req_ctx->bfr1;\n\treq_ctx->skbfr = req_ctx->bfr2;\n\tmemcpy(req_ctx->bfr1, state->bfr1, CHCR_HASH_MAX_BLOCK_SIZE_128);\n\tmemcpy(req_ctx->partial_hash, state->partial_hash,\n\t       CHCR_HASH_MAX_DIGEST_SIZE);\n\tchcr_init_hctx_per_wr(req_ctx);\n\treturn 0;\n}\n\nstatic int chcr_ahash_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct hmac_ctx *hmacctx = HMAC_CTX(h_ctx(tfm));\n\tunsigned int digestsize = crypto_ahash_digestsize(tfm);\n\tunsigned int bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\n\tunsigned int i, err = 0, updated_digestsize;\n\n\tSHASH_DESC_ON_STACK(shash, hmacctx->base_hash);\n\n\t \n\tshash->tfm = hmacctx->base_hash;\n\tif (keylen > bs) {\n\t\terr = crypto_shash_digest(shash, key, keylen,\n\t\t\t\t\t  hmacctx->ipad);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tkeylen = digestsize;\n\t} else {\n\t\tmemcpy(hmacctx->ipad, key, keylen);\n\t}\n\tmemset(hmacctx->ipad + keylen, 0, bs - keylen);\n\tunsafe_memcpy(hmacctx->opad, hmacctx->ipad, bs,\n\t\t      \"fortified memcpy causes -Wrestrict warning\");\n\n\tfor (i = 0; i < bs / sizeof(int); i++) {\n\t\t*((unsigned int *)(&hmacctx->ipad) + i) ^= IPAD_DATA;\n\t\t*((unsigned int *)(&hmacctx->opad) + i) ^= OPAD_DATA;\n\t}\n\n\tupdated_digestsize = digestsize;\n\tif (digestsize == SHA224_DIGEST_SIZE)\n\t\tupdated_digestsize = SHA256_DIGEST_SIZE;\n\telse if (digestsize == SHA384_DIGEST_SIZE)\n\t\tupdated_digestsize = SHA512_DIGEST_SIZE;\n\terr = chcr_compute_partial_hash(shash, hmacctx->ipad,\n\t\t\t\t\thmacctx->ipad, digestsize);\n\tif (err)\n\t\tgoto out;\n\tchcr_change_order(hmacctx->ipad, updated_digestsize);\n\n\terr = chcr_compute_partial_hash(shash, hmacctx->opad,\n\t\t\t\t\thmacctx->opad, digestsize);\n\tif (err)\n\t\tgoto out;\n\tchcr_change_order(hmacctx->opad, updated_digestsize);\nout:\n\treturn err;\n}\n\nstatic int chcr_aes_xts_setkey(struct crypto_skcipher *cipher, const u8 *key,\n\t\t\t       unsigned int key_len)\n{\n\tstruct ablk_ctx *ablkctx = ABLK_CTX(c_ctx(cipher));\n\tunsigned short context_size = 0;\n\tint err;\n\n\terr = chcr_cipher_fallback_setkey(cipher, key, key_len);\n\tif (err)\n\t\tgoto badkey_err;\n\n\tmemcpy(ablkctx->key, key, key_len);\n\tablkctx->enckey_len = key_len;\n\tget_aes_decrypt_key(ablkctx->rrkey, ablkctx->key, key_len << 2);\n\tcontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD + key_len) >> 4;\n\t \n\tif (key_len == 48) {\n\t\tcontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD + key_len\n\t\t\t\t+ 16) >> 4;\n\t\tmemmove(ablkctx->key + 32, ablkctx->key + 24, 24);\n\t\tmemset(ablkctx->key + 24, 0, 8);\n\t\tmemset(ablkctx->key + 56, 0, 8);\n\t\tablkctx->enckey_len = 64;\n\t\tablkctx->key_ctx_hdr =\n\t\t\tFILL_KEY_CTX_HDR(CHCR_KEYCTX_CIPHER_KEY_SIZE_192,\n\t\t\t\t\t CHCR_KEYCTX_NO_KEY, 1,\n\t\t\t\t\t 0, context_size);\n\t} else {\n\t\tablkctx->key_ctx_hdr =\n\t\tFILL_KEY_CTX_HDR((key_len == AES_KEYSIZE_256) ?\n\t\t\t\t CHCR_KEYCTX_CIPHER_KEY_SIZE_128 :\n\t\t\t\t CHCR_KEYCTX_CIPHER_KEY_SIZE_256,\n\t\t\t\t CHCR_KEYCTX_NO_KEY, 1,\n\t\t\t\t 0, context_size);\n\t}\n\tablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_XTS;\n\treturn 0;\nbadkey_err:\n\tablkctx->enckey_len = 0;\n\n\treturn err;\n}\n\nstatic int chcr_sha_init(struct ahash_request *areq)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\n\tint digestsize =  crypto_ahash_digestsize(tfm);\n\n\treq_ctx->data_len = 0;\n\treq_ctx->reqlen = 0;\n\treq_ctx->reqbfr = req_ctx->bfr1;\n\treq_ctx->skbfr = req_ctx->bfr2;\n\tcopy_hash_init_values(req_ctx->partial_hash, digestsize);\n\n\treturn 0;\n}\n\nstatic int chcr_sha_cra_init(struct crypto_tfm *tfm)\n{\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct chcr_ahash_req_ctx));\n\treturn chcr_device_init(crypto_tfm_ctx(tfm));\n}\n\nstatic int chcr_hmac_init(struct ahash_request *areq)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\n\tstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(areq);\n\tstruct hmac_ctx *hmacctx = HMAC_CTX(h_ctx(rtfm));\n\tunsigned int digestsize = crypto_ahash_digestsize(rtfm);\n\tunsigned int bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\n\n\tchcr_sha_init(areq);\n\treq_ctx->data_len = bs;\n\tif (is_hmac(crypto_ahash_tfm(rtfm))) {\n\t\tif (digestsize == SHA224_DIGEST_SIZE)\n\t\t\tmemcpy(req_ctx->partial_hash, hmacctx->ipad,\n\t\t\t       SHA256_DIGEST_SIZE);\n\t\telse if (digestsize == SHA384_DIGEST_SIZE)\n\t\t\tmemcpy(req_ctx->partial_hash, hmacctx->ipad,\n\t\t\t       SHA512_DIGEST_SIZE);\n\t\telse\n\t\t\tmemcpy(req_ctx->partial_hash, hmacctx->ipad,\n\t\t\t       digestsize);\n\t}\n\treturn 0;\n}\n\nstatic int chcr_hmac_cra_init(struct crypto_tfm *tfm)\n{\n\tstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\n\tunsigned int digestsize =\n\t\tcrypto_ahash_digestsize(__crypto_ahash_cast(tfm));\n\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct chcr_ahash_req_ctx));\n\thmacctx->base_hash = chcr_alloc_shash(digestsize);\n\tif (IS_ERR(hmacctx->base_hash))\n\t\treturn PTR_ERR(hmacctx->base_hash);\n\treturn chcr_device_init(crypto_tfm_ctx(tfm));\n}\n\nstatic void chcr_hmac_cra_exit(struct crypto_tfm *tfm)\n{\n\tstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\n\tstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\n\n\tif (hmacctx->base_hash) {\n\t\tchcr_free_shash(hmacctx->base_hash);\n\t\thmacctx->base_hash = NULL;\n\t}\n}\n\ninline void chcr_aead_common_exit(struct aead_request *req)\n{\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct uld_ctx *u_ctx = ULD_CTX(a_ctx(tfm));\n\n\tchcr_aead_dma_unmap(&u_ctx->lldi.pdev->dev, req, reqctx->op);\n}\n\nstatic int chcr_aead_common_init(struct aead_request *req)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tint error = -EINVAL;\n\n\t \n\tif (aeadctx->enckey_len == 0)\n\t\tgoto err;\n\tif (reqctx->op && req->cryptlen < authsize)\n\t\tgoto err;\n\tif (reqctx->b0_len)\n\t\treqctx->scratch_pad = reqctx->iv + IV;\n\telse\n\t\treqctx->scratch_pad = NULL;\n\n\terror = chcr_aead_dma_map(&ULD_CTX(a_ctx(tfm))->lldi.pdev->dev, req,\n\t\t\t\t  reqctx->op);\n\tif (error) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\treturn error;\n}\n\nstatic int chcr_aead_need_fallback(struct aead_request *req, int dst_nents,\n\t\t\t\t   int aadmax, int wrlen,\n\t\t\t\t   unsigned short op_type)\n{\n\tunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\n\n\tif (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||\n\t    dst_nents > MAX_DSGL_ENT ||\n\t    (req->assoclen > aadmax) ||\n\t    (wrlen > SGE_MAX_WR_LEN))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tstruct aead_request *subreq = aead_request_ctx_dma(req);\n\n\taead_request_set_tfm(subreq, aeadctx->sw_cipher);\n\taead_request_set_callback(subreq, req->base.flags,\n\t\t\t\t  req->base.complete, req->base.data);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\n\t\t\t\t req->iv);\n\taead_request_set_ad(subreq, req->assoclen);\n\treturn op_type ? crypto_aead_decrypt(subreq) :\n\t\tcrypto_aead_encrypt(subreq);\n}\n\nstatic struct sk_buff *create_authenc_wr(struct aead_request *req,\n\t\t\t\t\t unsigned short qid,\n\t\t\t\t\t int size)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\n\tstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct sk_buff *skb = NULL;\n\tstruct chcr_wr *chcr_req;\n\tstruct cpl_rx_phys_dsgl *phys_cpl;\n\tstruct ulptx_sgl *ulptx;\n\tunsigned int transhdr_len;\n\tunsigned int dst_size = 0, temp, subtype = get_aead_subtype(tfm);\n\tunsigned int   kctx_len = 0, dnents, snents;\n\tunsigned int  authsize = crypto_aead_authsize(tfm);\n\tint error = -EINVAL;\n\tu8 *ivptr;\n\tint null = 0;\n\tgfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\n\t\tGFP_ATOMIC;\n\tstruct adapter *adap = padap(ctx->dev);\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\tif (req->cryptlen == 0)\n\t\treturn NULL;\n\n\treqctx->b0_len = 0;\n\terror = chcr_aead_common_init(req);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CBC_NULL ||\n\t\tsubtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {\n\t\tnull = 1;\n\t}\n\tdnents = sg_nents_xlen(req->dst, req->assoclen + req->cryptlen +\n\t\t(reqctx->op ? -authsize : authsize), CHCR_DST_SG_SIZE, 0);\n\tdnents += MIN_AUTH_SG;  \n\tsnents = sg_nents_xlen(req->src, req->assoclen + req->cryptlen,\n\t\t\t       CHCR_SRC_SG_SIZE, 0);\n\tdst_size = get_space_for_phys_dsgl(dnents);\n\tkctx_len = (KEY_CONTEXT_CTX_LEN_G(ntohl(aeadctx->key_ctx_hdr)) << 4)\n\t\t- sizeof(chcr_req->key_ctx);\n\ttranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\n\treqctx->imm = (transhdr_len + req->assoclen + req->cryptlen) <\n\t\t\tSGE_MAX_WR_LEN;\n\ttemp = reqctx->imm ? roundup(req->assoclen + req->cryptlen, 16)\n\t\t\t: (sgl_len(snents) * 8);\n\ttranshdr_len += temp;\n\ttranshdr_len = roundup(transhdr_len, 16);\n\n\tif (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE,\n\t\t\t\t    transhdr_len, reqctx->op)) {\n\t\tatomic_inc(&adap->chcr_stats.fallback);\n\t\tchcr_aead_common_exit(req);\n\t\treturn ERR_PTR(chcr_aead_fallback(req, reqctx->op));\n\t}\n\tskb = alloc_skb(transhdr_len, flags);\n\tif (!skb) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tchcr_req = __skb_put_zero(skb, transhdr_len);\n\n\ttemp  = (reqctx->op == CHCR_ENCRYPT_OP) ? 0 : authsize;\n\n\t \n\tchcr_req->sec_cpl.op_ivinsrtofst =\n\t\t\t\tFILL_SEC_CPL_OP_IVINSR(rx_channel_id, 2, 1);\n\tchcr_req->sec_cpl.pldlen = htonl(req->assoclen + IV + req->cryptlen);\n\tchcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\n\t\t\t\t\tnull ? 0 : 1 + IV,\n\t\t\t\t\tnull ? 0 : IV + req->assoclen,\n\t\t\t\t\treq->assoclen + IV + 1,\n\t\t\t\t\t(temp & 0x1F0) >> 4);\n\tchcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(\n\t\t\t\t\ttemp & 0xF,\n\t\t\t\t\tnull ? 0 : req->assoclen + IV + 1,\n\t\t\t\t\ttemp, temp);\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL ||\n\t    subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA)\n\t\ttemp = CHCR_SCMD_CIPHER_MODE_AES_CTR;\n\telse\n\t\ttemp = CHCR_SCMD_CIPHER_MODE_AES_CBC;\n\tchcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(reqctx->op,\n\t\t\t\t\t(reqctx->op == CHCR_ENCRYPT_OP) ? 1 : 0,\n\t\t\t\t\ttemp,\n\t\t\t\t\tactx->auth_mode, aeadctx->hmac_ctrl,\n\t\t\t\t\tIV >> 1);\n\tchcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,\n\t\t\t\t\t 0, 0, dst_size);\n\n\tchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\n\tif (reqctx->op == CHCR_ENCRYPT_OP ||\n\t\tsubtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||\n\t\tsubtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL)\n\t\tmemcpy(chcr_req->key_ctx.key, aeadctx->key,\n\t\t       aeadctx->enckey_len);\n\telse\n\t\tmemcpy(chcr_req->key_ctx.key, actx->dec_rrkey,\n\t\t       aeadctx->enckey_len);\n\n\tmemcpy(chcr_req->key_ctx.key + roundup(aeadctx->enckey_len, 16),\n\t       actx->h_iopad, kctx_len - roundup(aeadctx->enckey_len, 16));\n\tphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\n\tivptr = (u8 *)(phys_cpl + 1) + dst_size;\n\tulptx = (struct ulptx_sgl *)(ivptr + IV);\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||\n\t    subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {\n\t\tmemcpy(ivptr, aeadctx->nonce, CTR_RFC3686_NONCE_SIZE);\n\t\tmemcpy(ivptr + CTR_RFC3686_NONCE_SIZE, req->iv,\n\t\t\t\tCTR_RFC3686_IV_SIZE);\n\t\t*(__be32 *)(ivptr + CTR_RFC3686_NONCE_SIZE +\n\t\t\tCTR_RFC3686_IV_SIZE) = cpu_to_be32(1);\n\t} else {\n\t\tmemcpy(ivptr, req->iv, IV);\n\t}\n\tchcr_add_aead_dst_ent(req, phys_cpl, qid);\n\tchcr_add_aead_src_ent(req, ulptx);\n\tatomic_inc(&adap->chcr_stats.cipher_rqst);\n\ttemp = sizeof(struct cpl_rx_phys_dsgl) + dst_size + IV +\n\t\tkctx_len + (reqctx->imm ? (req->assoclen + req->cryptlen) : 0);\n\tcreate_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, size,\n\t\t   transhdr_len, temp, 0);\n\treqctx->skb = skb;\n\n\treturn skb;\nerr:\n\tchcr_aead_common_exit(req);\n\n\treturn ERR_PTR(error);\n}\n\nint chcr_aead_dma_map(struct device *dev,\n\t\t      struct aead_request *req,\n\t\t      unsigned short op_type)\n{\n\tint error;\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tint src_len, dst_len;\n\n\t \n\tif (req->src == req->dst) {\n\t\tsrc_len = req->assoclen + req->cryptlen + (op_type ?\n\t\t\t\t\t\t\t0 : authsize);\n\t\tdst_len = src_len;\n\t} else {\n\t\tsrc_len = req->assoclen + req->cryptlen;\n\t\tdst_len = req->assoclen + req->cryptlen + (op_type ?\n\t\t\t\t\t\t\t-authsize : authsize);\n\t}\n\n\tif (!req->cryptlen || !src_len || !dst_len)\n\t\treturn 0;\n\treqctx->iv_dma = dma_map_single(dev, reqctx->iv, (IV + reqctx->b0_len),\n\t\t\t\t\tDMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, reqctx->iv_dma))\n\t\treturn -ENOMEM;\n\tif (reqctx->b0_len)\n\t\treqctx->b0_dma = reqctx->iv_dma + IV;\n\telse\n\t\treqctx->b0_dma = 0;\n\tif (req->src == req->dst) {\n\t\terror = dma_map_sg(dev, req->src,\n\t\t\t\tsg_nents_for_len(req->src, src_len),\n\t\t\t\t\tDMA_BIDIRECTIONAL);\n\t\tif (!error)\n\t\t\tgoto err;\n\t} else {\n\t\terror = dma_map_sg(dev, req->src,\n\t\t\t\t   sg_nents_for_len(req->src, src_len),\n\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (!error)\n\t\t\tgoto err;\n\t\terror = dma_map_sg(dev, req->dst,\n\t\t\t\t   sg_nents_for_len(req->dst, dst_len),\n\t\t\t\t   DMA_FROM_DEVICE);\n\t\tif (!error) {\n\t\t\tdma_unmap_sg(dev, req->src,\n\t\t\t\t     sg_nents_for_len(req->src, src_len),\n\t\t\t\t     DMA_TO_DEVICE);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tdma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);\n\treturn -ENOMEM;\n}\n\nvoid chcr_aead_dma_unmap(struct device *dev,\n\t\t\t struct aead_request *req,\n\t\t\t unsigned short op_type)\n{\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tint src_len, dst_len;\n\n\t \n\tif (req->src == req->dst) {\n\t\tsrc_len = req->assoclen + req->cryptlen + (op_type ?\n\t\t\t\t\t\t\t0 : authsize);\n\t\tdst_len = src_len;\n\t} else {\n\t\tsrc_len = req->assoclen + req->cryptlen;\n\t\tdst_len = req->assoclen + req->cryptlen + (op_type ?\n\t\t\t\t\t\t-authsize : authsize);\n\t}\n\n\tif (!req->cryptlen || !src_len || !dst_len)\n\t\treturn;\n\n\tdma_unmap_single(dev, reqctx->iv_dma, (IV + reqctx->b0_len),\n\t\t\t\t\tDMA_BIDIRECTIONAL);\n\tif (req->src == req->dst) {\n\t\tdma_unmap_sg(dev, req->src,\n\t\t\t     sg_nents_for_len(req->src, src_len),\n\t\t\t     DMA_BIDIRECTIONAL);\n\t} else {\n\t\tdma_unmap_sg(dev, req->src,\n\t\t\t     sg_nents_for_len(req->src, src_len),\n\t\t\t     DMA_TO_DEVICE);\n\t\tdma_unmap_sg(dev, req->dst,\n\t\t\t     sg_nents_for_len(req->dst, dst_len),\n\t\t\t     DMA_FROM_DEVICE);\n\t}\n}\n\nvoid chcr_add_aead_src_ent(struct aead_request *req,\n\t\t\t   struct ulptx_sgl *ulptx)\n{\n\tstruct ulptx_walk ulp_walk;\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\n\tif (reqctx->imm) {\n\t\tu8 *buf = (u8 *)ulptx;\n\n\t\tif (reqctx->b0_len) {\n\t\t\tmemcpy(buf, reqctx->scratch_pad, reqctx->b0_len);\n\t\t\tbuf += reqctx->b0_len;\n\t\t}\n\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src),\n\t\t\t\t   buf, req->cryptlen + req->assoclen, 0);\n\t} else {\n\t\tulptx_walk_init(&ulp_walk, ulptx);\n\t\tif (reqctx->b0_len)\n\t\t\tulptx_walk_add_page(&ulp_walk, reqctx->b0_len,\n\t\t\t\t\t    reqctx->b0_dma);\n\t\tulptx_walk_add_sg(&ulp_walk, req->src, req->cryptlen +\n\t\t\t\t  req->assoclen,  0);\n\t\tulptx_walk_end(&ulp_walk);\n\t}\n}\n\nvoid chcr_add_aead_dst_ent(struct aead_request *req,\n\t\t\t   struct cpl_rx_phys_dsgl *phys_cpl,\n\t\t\t   unsigned short qid)\n{\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct dsgl_walk dsgl_walk;\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tu32 temp;\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\tdsgl_walk_init(&dsgl_walk, phys_cpl);\n\tdsgl_walk_add_page(&dsgl_walk, IV + reqctx->b0_len, reqctx->iv_dma);\n\ttemp = req->assoclen + req->cryptlen +\n\t\t(reqctx->op ? -authsize : authsize);\n\tdsgl_walk_add_sg(&dsgl_walk, req->dst, temp, 0);\n\tdsgl_walk_end(&dsgl_walk, qid, rx_channel_id);\n}\n\nvoid chcr_add_cipher_src_ent(struct skcipher_request *req,\n\t\t\t     void *ulptx,\n\t\t\t     struct  cipher_wr_param *wrparam)\n{\n\tstruct ulptx_walk ulp_walk;\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tu8 *buf = ulptx;\n\n\tmemcpy(buf, reqctx->iv, IV);\n\tbuf += IV;\n\tif (reqctx->imm) {\n\t\tsg_pcopy_to_buffer(req->src, sg_nents(req->src),\n\t\t\t\t   buf, wrparam->bytes, reqctx->processed);\n\t} else {\n\t\tulptx_walk_init(&ulp_walk, (struct ulptx_sgl *)buf);\n\t\tulptx_walk_add_sg(&ulp_walk, reqctx->srcsg, wrparam->bytes,\n\t\t\t\t  reqctx->src_ofst);\n\t\treqctx->srcsg = ulp_walk.last_sg;\n\t\treqctx->src_ofst = ulp_walk.last_sg_len;\n\t\tulptx_walk_end(&ulp_walk);\n\t}\n}\n\nvoid chcr_add_cipher_dst_ent(struct skcipher_request *req,\n\t\t\t     struct cpl_rx_phys_dsgl *phys_cpl,\n\t\t\t     struct  cipher_wr_param *wrparam,\n\t\t\t     unsigned short qid)\n{\n\tstruct chcr_skcipher_req_ctx *reqctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(wrparam->req);\n\tstruct chcr_context *ctx = c_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct dsgl_walk dsgl_walk;\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\tdsgl_walk_init(&dsgl_walk, phys_cpl);\n\tdsgl_walk_add_sg(&dsgl_walk, reqctx->dstsg, wrparam->bytes,\n\t\t\t reqctx->dst_ofst);\n\treqctx->dstsg = dsgl_walk.last_sg;\n\treqctx->dst_ofst = dsgl_walk.last_sg_len;\n\tdsgl_walk_end(&dsgl_walk, qid, rx_channel_id);\n}\n\nvoid chcr_add_hash_src_ent(struct ahash_request *req,\n\t\t\t   struct ulptx_sgl *ulptx,\n\t\t\t   struct hash_wr_param *param)\n{\n\tstruct ulptx_walk ulp_walk;\n\tstruct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);\n\n\tif (reqctx->hctx_wr.imm) {\n\t\tu8 *buf = (u8 *)ulptx;\n\n\t\tif (param->bfr_len) {\n\t\t\tmemcpy(buf, reqctx->reqbfr, param->bfr_len);\n\t\t\tbuf += param->bfr_len;\n\t\t}\n\n\t\tsg_pcopy_to_buffer(reqctx->hctx_wr.srcsg,\n\t\t\t\t   sg_nents(reqctx->hctx_wr.srcsg), buf,\n\t\t\t\t   param->sg_len, 0);\n\t} else {\n\t\tulptx_walk_init(&ulp_walk, ulptx);\n\t\tif (param->bfr_len)\n\t\t\tulptx_walk_add_page(&ulp_walk, param->bfr_len,\n\t\t\t\t\t    reqctx->hctx_wr.dma_addr);\n\t\tulptx_walk_add_sg(&ulp_walk, reqctx->hctx_wr.srcsg,\n\t\t\t\t  param->sg_len, reqctx->hctx_wr.src_ofst);\n\t\treqctx->hctx_wr.srcsg = ulp_walk.last_sg;\n\t\treqctx->hctx_wr.src_ofst = ulp_walk.last_sg_len;\n\t\tulptx_walk_end(&ulp_walk);\n\t}\n}\n\nint chcr_hash_dma_map(struct device *dev,\n\t\t      struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\tint error = 0;\n\n\tif (!req->nbytes)\n\t\treturn 0;\n\terror = dma_map_sg(dev, req->src, sg_nents(req->src),\n\t\t\t   DMA_TO_DEVICE);\n\tif (!error)\n\t\treturn -ENOMEM;\n\treq_ctx->hctx_wr.is_sg_map = 1;\n\treturn 0;\n}\n\nvoid chcr_hash_dma_unmap(struct device *dev,\n\t\t\t struct ahash_request *req)\n{\n\tstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\n\n\tif (!req->nbytes)\n\t\treturn;\n\n\tdma_unmap_sg(dev, req->src, sg_nents(req->src),\n\t\t\t   DMA_TO_DEVICE);\n\treq_ctx->hctx_wr.is_sg_map = 0;\n\n}\n\nint chcr_cipher_dma_map(struct device *dev,\n\t\t\tstruct skcipher_request *req)\n{\n\tint error;\n\n\tif (req->src == req->dst) {\n\t\terror = dma_map_sg(dev, req->src, sg_nents(req->src),\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (!error)\n\t\t\tgoto err;\n\t} else {\n\t\terror = dma_map_sg(dev, req->src, sg_nents(req->src),\n\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (!error)\n\t\t\tgoto err;\n\t\terror = dma_map_sg(dev, req->dst, sg_nents(req->dst),\n\t\t\t\t   DMA_FROM_DEVICE);\n\t\tif (!error) {\n\t\t\tdma_unmap_sg(dev, req->src, sg_nents(req->src),\n\t\t\t\t   DMA_TO_DEVICE);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\treturn -ENOMEM;\n}\n\nvoid chcr_cipher_dma_unmap(struct device *dev,\n\t\t\t   struct skcipher_request *req)\n{\n\tif (req->src == req->dst) {\n\t\tdma_unmap_sg(dev, req->src, sg_nents(req->src),\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\t} else {\n\t\tdma_unmap_sg(dev, req->src, sg_nents(req->src),\n\t\t\t\t   DMA_TO_DEVICE);\n\t\tdma_unmap_sg(dev, req->dst, sg_nents(req->dst),\n\t\t\t\t   DMA_FROM_DEVICE);\n\t}\n}\n\nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\n{\n\t__be32 data;\n\n\tmemset(block, 0, csize);\n\tblock += csize;\n\n\tif (csize >= 4)\n\t\tcsize = 4;\n\telse if (msglen > (unsigned int)(1 << (8 * csize)))\n\t\treturn -EOVERFLOW;\n\n\tdata = cpu_to_be32(msglen);\n\tmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\n\n\treturn 0;\n}\n\nstatic int generate_b0(struct aead_request *req, u8 *ivptr,\n\t\t\tunsigned short op_type)\n{\n\tunsigned int l, lp, m;\n\tint rc;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tu8 *b0 = reqctx->scratch_pad;\n\n\tm = crypto_aead_authsize(aead);\n\n\tmemcpy(b0, ivptr, 16);\n\n\tlp = b0[0];\n\tl = lp + 1;\n\n\t \n\t*b0 |= (8 * ((m - 2) / 2));\n\n\t \n\tif (req->assoclen)\n\t\t*b0 |= 64;\n\trc = set_msg_len(b0 + 16 - l,\n\t\t\t (op_type == CHCR_DECRYPT_OP) ?\n\t\t\t req->cryptlen - m : req->cryptlen, l);\n\n\treturn rc;\n}\n\nstatic inline int crypto_ccm_check_iv(const u8 *iv)\n{\n\t \n\tif (iv[0] < 1 || iv[0] > 7)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ccm_format_packet(struct aead_request *req,\n\t\t\t     u8 *ivptr,\n\t\t\t     unsigned int sub_type,\n\t\t\t     unsigned short op_type,\n\t\t\t     unsigned int assoclen)\n{\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tint rc = 0;\n\n\tif (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {\n\t\tivptr[0] = 3;\n\t\tmemcpy(ivptr + 1, &aeadctx->salt[0], 3);\n\t\tmemcpy(ivptr + 4, req->iv, 8);\n\t\tmemset(ivptr + 12, 0, 4);\n\t} else {\n\t\tmemcpy(ivptr, req->iv, 16);\n\t}\n\tif (assoclen)\n\t\tput_unaligned_be16(assoclen, &reqctx->scratch_pad[16]);\n\n\trc = generate_b0(req, ivptr, op_type);\n\t \n\tmemset(ivptr + 15 - ivptr[0], 0, ivptr[0] + 1);\n\treturn rc;\n}\n\nstatic void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,\n\t\t\t\t  unsigned int dst_size,\n\t\t\t\t  struct aead_request *req,\n\t\t\t\t  unsigned short op_type)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tunsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;\n\tunsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\tunsigned int ccm_xtra;\n\tunsigned int tag_offset = 0, auth_offset = 0;\n\tunsigned int assoclen;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\n\tif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)\n\t\tassoclen = req->assoclen - 8;\n\telse\n\t\tassoclen = req->assoclen;\n\tccm_xtra = CCM_B0_SIZE +\n\t\t((assoclen) ? CCM_AAD_FIELD_SIZE : 0);\n\n\tauth_offset = req->cryptlen ?\n\t\t(req->assoclen + IV + 1 + ccm_xtra) : 0;\n\tif (op_type == CHCR_DECRYPT_OP) {\n\t\tif (crypto_aead_authsize(tfm) != req->cryptlen)\n\t\t\ttag_offset = crypto_aead_authsize(tfm);\n\t\telse\n\t\t\tauth_offset = 0;\n\t}\n\n\tsec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(rx_channel_id, 2, 1);\n\tsec_cpl->pldlen =\n\t\thtonl(req->assoclen + IV + req->cryptlen + ccm_xtra);\n\t \n\tsec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\n\t\t\t\t1 + IV,\tIV + assoclen + ccm_xtra,\n\t\t\t\treq->assoclen + IV + 1 + ccm_xtra, 0);\n\n\tsec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,\n\t\t\t\t\tauth_offset, tag_offset,\n\t\t\t\t\t(op_type == CHCR_ENCRYPT_OP) ? 0 :\n\t\t\t\t\tcrypto_aead_authsize(tfm));\n\tsec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,\n\t\t\t\t\t(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,\n\t\t\t\t\tcipher_mode, mac_mode,\n\t\t\t\t\taeadctx->hmac_ctrl, IV >> 1);\n\n\tsec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,\n\t\t\t\t\t0, dst_size);\n}\n\nstatic int aead_ccm_validate_input(unsigned short op_type,\n\t\t\t\t   struct aead_request *req,\n\t\t\t\t   struct chcr_aead_ctx *aeadctx,\n\t\t\t\t   unsigned int sub_type)\n{\n\tif (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {\n\t\tif (crypto_ccm_check_iv(req->iv)) {\n\t\t\tpr_err(\"CCM: IV check fails\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (req->assoclen != 16 && req->assoclen != 20) {\n\t\t\tpr_err(\"RFC4309: Invalid AAD length %d\\n\",\n\t\t\t       req->assoclen);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic struct sk_buff *create_aead_ccm_wr(struct aead_request *req,\n\t\t\t\t\t  unsigned short qid,\n\t\t\t\t\t  int size)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct sk_buff *skb = NULL;\n\tstruct chcr_wr *chcr_req;\n\tstruct cpl_rx_phys_dsgl *phys_cpl;\n\tstruct ulptx_sgl *ulptx;\n\tunsigned int transhdr_len;\n\tunsigned int dst_size = 0, kctx_len, dnents, temp, snents;\n\tunsigned int sub_type, assoclen = req->assoclen;\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tint error = -EINVAL;\n\tu8 *ivptr;\n\tgfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\n\t\tGFP_ATOMIC;\n\tstruct adapter *adap = padap(a_ctx(tfm)->dev);\n\n\tsub_type = get_aead_subtype(tfm);\n\tif (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)\n\t\tassoclen -= 8;\n\treqctx->b0_len = CCM_B0_SIZE + (assoclen ? CCM_AAD_FIELD_SIZE : 0);\n\terror = chcr_aead_common_init(req);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\terror = aead_ccm_validate_input(reqctx->op, req, aeadctx, sub_type);\n\tif (error)\n\t\tgoto err;\n\tdnents = sg_nents_xlen(req->dst, req->assoclen + req->cryptlen\n\t\t\t+ (reqctx->op ? -authsize : authsize),\n\t\t\tCHCR_DST_SG_SIZE, 0);\n\tdnents += MIN_CCM_SG; \n\tdst_size = get_space_for_phys_dsgl(dnents);\n\tsnents = sg_nents_xlen(req->src, req->assoclen + req->cryptlen,\n\t\t\t       CHCR_SRC_SG_SIZE, 0);\n\tsnents += MIN_CCM_SG; \n\tkctx_len = roundup(aeadctx->enckey_len, 16) * 2;\n\ttranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\n\treqctx->imm = (transhdr_len + req->assoclen + req->cryptlen +\n\t\t       reqctx->b0_len) <= SGE_MAX_WR_LEN;\n\ttemp = reqctx->imm ? roundup(req->assoclen + req->cryptlen +\n\t\t\t\t     reqctx->b0_len, 16) :\n\t\t(sgl_len(snents) *  8);\n\ttranshdr_len += temp;\n\ttranshdr_len = roundup(transhdr_len, 16);\n\n\tif (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE -\n\t\t\t\treqctx->b0_len, transhdr_len, reqctx->op)) {\n\t\tatomic_inc(&adap->chcr_stats.fallback);\n\t\tchcr_aead_common_exit(req);\n\t\treturn ERR_PTR(chcr_aead_fallback(req, reqctx->op));\n\t}\n\tskb = alloc_skb(transhdr_len,  flags);\n\n\tif (!skb) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tchcr_req = __skb_put_zero(skb, transhdr_len);\n\n\tfill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, reqctx->op);\n\n\tchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\n\tmemcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);\n\tmemcpy(chcr_req->key_ctx.key + roundup(aeadctx->enckey_len, 16),\n\t\t\taeadctx->key, aeadctx->enckey_len);\n\n\tphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\n\tivptr = (u8 *)(phys_cpl + 1) + dst_size;\n\tulptx = (struct ulptx_sgl *)(ivptr + IV);\n\terror = ccm_format_packet(req, ivptr, sub_type, reqctx->op, assoclen);\n\tif (error)\n\t\tgoto dstmap_fail;\n\tchcr_add_aead_dst_ent(req, phys_cpl, qid);\n\tchcr_add_aead_src_ent(req, ulptx);\n\n\tatomic_inc(&adap->chcr_stats.aead_rqst);\n\ttemp = sizeof(struct cpl_rx_phys_dsgl) + dst_size + IV +\n\t\tkctx_len + (reqctx->imm ? (req->assoclen + req->cryptlen +\n\t\treqctx->b0_len) : 0);\n\tcreate_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, 0,\n\t\t    transhdr_len, temp, 0);\n\treqctx->skb = skb;\n\n\treturn skb;\ndstmap_fail:\n\tkfree_skb(skb);\nerr:\n\tchcr_aead_common_exit(req);\n\treturn ERR_PTR(error);\n}\n\nstatic struct sk_buff *create_gcm_wr(struct aead_request *req,\n\t\t\t\t     unsigned short qid,\n\t\t\t\t     int size)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct sk_buff *skb = NULL;\n\tstruct chcr_wr *chcr_req;\n\tstruct cpl_rx_phys_dsgl *phys_cpl;\n\tstruct ulptx_sgl *ulptx;\n\tunsigned int transhdr_len, dnents = 0, snents;\n\tunsigned int dst_size = 0, temp = 0, kctx_len, assoclen = req->assoclen;\n\tunsigned int authsize = crypto_aead_authsize(tfm);\n\tint error = -EINVAL;\n\tu8 *ivptr;\n\tgfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\n\t\tGFP_ATOMIC;\n\tstruct adapter *adap = padap(ctx->dev);\n\tunsigned int rx_channel_id = reqctx->rxqidx / ctx->rxq_perchan;\n\n\trx_channel_id = cxgb4_port_e2cchan(u_ctx->lldi.ports[rx_channel_id]);\n\tif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)\n\t\tassoclen = req->assoclen - 8;\n\n\treqctx->b0_len = 0;\n\terror = chcr_aead_common_init(req);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\tdnents = sg_nents_xlen(req->dst, req->assoclen + req->cryptlen +\n\t\t\t\t(reqctx->op ? -authsize : authsize),\n\t\t\t\tCHCR_DST_SG_SIZE, 0);\n\tsnents = sg_nents_xlen(req->src, req->assoclen + req->cryptlen,\n\t\t\t       CHCR_SRC_SG_SIZE, 0);\n\tdnents += MIN_GCM_SG; \n\tdst_size = get_space_for_phys_dsgl(dnents);\n\tkctx_len = roundup(aeadctx->enckey_len, 16) + AEAD_H_SIZE;\n\ttranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\n\treqctx->imm = (transhdr_len + req->assoclen + req->cryptlen) <=\n\t\t\tSGE_MAX_WR_LEN;\n\ttemp = reqctx->imm ? roundup(req->assoclen + req->cryptlen, 16) :\n\t\t(sgl_len(snents) * 8);\n\ttranshdr_len += temp;\n\ttranshdr_len = roundup(transhdr_len, 16);\n\tif (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE,\n\t\t\t    transhdr_len, reqctx->op)) {\n\n\t\tatomic_inc(&adap->chcr_stats.fallback);\n\t\tchcr_aead_common_exit(req);\n\t\treturn ERR_PTR(chcr_aead_fallback(req, reqctx->op));\n\t}\n\tskb = alloc_skb(transhdr_len, flags);\n\tif (!skb) {\n\t\terror = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tchcr_req = __skb_put_zero(skb, transhdr_len);\n\n\t\n\ttemp = (reqctx->op == CHCR_ENCRYPT_OP) ? 0 : authsize;\n\tchcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(\n\t\t\t\t\t\trx_channel_id, 2, 1);\n\tchcr_req->sec_cpl.pldlen =\n\t\thtonl(req->assoclen + IV + req->cryptlen);\n\tchcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\n\t\t\t\t\tassoclen ? 1 + IV : 0,\n\t\t\t\t\tassoclen ? IV + assoclen : 0,\n\t\t\t\t\treq->assoclen + IV + 1, 0);\n\tchcr_req->sec_cpl.cipherstop_lo_authinsert =\n\t\t\tFILL_SEC_CPL_AUTHINSERT(0, req->assoclen + IV + 1,\n\t\t\t\t\t\ttemp, temp);\n\tchcr_req->sec_cpl.seqno_numivs =\n\t\t\tFILL_SEC_CPL_SCMD0_SEQNO(reqctx->op, (reqctx->op ==\n\t\t\t\t\tCHCR_ENCRYPT_OP) ? 1 : 0,\n\t\t\t\t\tCHCR_SCMD_CIPHER_MODE_AES_GCM,\n\t\t\t\t\tCHCR_SCMD_AUTH_MODE_GHASH,\n\t\t\t\t\taeadctx->hmac_ctrl, IV >> 1);\n\tchcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,\n\t\t\t\t\t0, 0, dst_size);\n\tchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\n\tmemcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);\n\tmemcpy(chcr_req->key_ctx.key + roundup(aeadctx->enckey_len, 16),\n\t       GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);\n\n\tphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\n\tivptr = (u8 *)(phys_cpl + 1) + dst_size;\n\t \n\t \n\tif (get_aead_subtype(tfm) ==\n\t    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {\n\t\tmemcpy(ivptr, aeadctx->salt, 4);\n\t\tmemcpy(ivptr + 4, req->iv, GCM_RFC4106_IV_SIZE);\n\t} else {\n\t\tmemcpy(ivptr, req->iv, GCM_AES_IV_SIZE);\n\t}\n\tput_unaligned_be32(0x01, &ivptr[12]);\n\tulptx = (struct ulptx_sgl *)(ivptr + 16);\n\n\tchcr_add_aead_dst_ent(req, phys_cpl, qid);\n\tchcr_add_aead_src_ent(req, ulptx);\n\tatomic_inc(&adap->chcr_stats.aead_rqst);\n\ttemp = sizeof(struct cpl_rx_phys_dsgl) + dst_size + IV +\n\t\tkctx_len + (reqctx->imm ? (req->assoclen + req->cryptlen) : 0);\n\tcreate_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, size,\n\t\t    transhdr_len, temp, reqctx->verify);\n\treqctx->skb = skb;\n\treturn skb;\n\nerr:\n\tchcr_aead_common_exit(req);\n\treturn ERR_PTR(error);\n}\n\n\n\nstatic int chcr_aead_cra_init(struct crypto_aead *tfm)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tstruct aead_alg *alg = crypto_aead_alg(tfm);\n\n\taeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,\n\t\t\t\t\t       CRYPTO_ALG_NEED_FALLBACK |\n\t\t\t\t\t       CRYPTO_ALG_ASYNC);\n\tif  (IS_ERR(aeadctx->sw_cipher))\n\t\treturn PTR_ERR(aeadctx->sw_cipher);\n\tcrypto_aead_set_reqsize_dma(\n\t\ttfm, max(sizeof(struct chcr_aead_reqctx),\n\t\t\t sizeof(struct aead_request) +\n\t\t\t crypto_aead_reqsize(aeadctx->sw_cipher)));\n\treturn chcr_device_init(a_ctx(tfm));\n}\n\nstatic void chcr_aead_cra_exit(struct crypto_aead *tfm)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\n\tcrypto_free_aead(aeadctx->sw_cipher);\n}\n\nstatic int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t\tunsigned int authsize)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\n\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;\n\taeadctx->mayverify = VERIFY_HW;\n\treturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\n}\nstatic int chcr_authenc_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t    unsigned int authsize)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\tu32 maxauth = crypto_aead_maxauthsize(tfm);\n\n\t \n\tif (authsize == ICV_4) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == ICV_6) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == ICV_10) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == ICV_12) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == ICV_14) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == (maxauth >> 1)) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else if (authsize == maxauth) {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t} else {\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_SW;\n\t}\n\treturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\n}\n\n\nstatic int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\n\tswitch (authsize) {\n\tcase ICV_4:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_8:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_12:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_14:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_16:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_13:\n\tcase ICV_15:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_SW;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\n}\n\nstatic int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t\t  unsigned int authsize)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\n\tswitch (authsize) {\n\tcase ICV_8:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_12:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_16:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\n}\n\nstatic int chcr_ccm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\tunsigned int authsize)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));\n\n\tswitch (authsize) {\n\tcase ICV_4:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_6:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_8:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_10:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_12:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_14:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tcase ICV_16:\n\t\taeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\taeadctx->mayverify = VERIFY_HW;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\n}\n\nstatic int chcr_ccm_common_setkey(struct crypto_aead *aead,\n\t\t\t\tconst u8 *key,\n\t\t\t\tunsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(aead));\n\tunsigned char ck_size, mk_size;\n\tint key_ctx_size = 0;\n\n\tkey_ctx_size = sizeof(struct _key_ctx) + roundup(keylen, 16) * 2;\n\tif (keylen == AES_KEYSIZE_128) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t\tmk_size = CHCR_KEYCTX_MAC_KEY_SIZE_128;\n\t} else if (keylen == AES_KEYSIZE_192) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\t\tmk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;\n\t} else if (keylen == AES_KEYSIZE_256) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\t\tmk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\n\t} else {\n\t\taeadctx->enckey_len = 0;\n\t\treturn\t-EINVAL;\n\t}\n\taeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,\n\t\t\t\t\t\tkey_ctx_size >> 4);\n\tmemcpy(aeadctx->key, key, keylen);\n\taeadctx->enckey_len = keylen;\n\n\treturn 0;\n}\n\nstatic int chcr_aead_ccm_setkey(struct crypto_aead *aead,\n\t\t\t\tconst u8 *key,\n\t\t\t\tunsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(aead));\n\tint error;\n\n\tcrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &\n\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terror = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\n\tif (error)\n\t\treturn error;\n\treturn chcr_ccm_common_setkey(aead, key, keylen);\n}\n\nstatic int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t\t    unsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(aead));\n\tint error;\n\n\tif (keylen < 3) {\n\t\taeadctx->enckey_len = 0;\n\t\treturn\t-EINVAL;\n\t}\n\tcrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &\n\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terror = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\n\tif (error)\n\t\treturn error;\n\tkeylen -= 3;\n\tmemcpy(aeadctx->salt, key + keylen, 3);\n\treturn chcr_ccm_common_setkey(aead, key, keylen);\n}\n\nstatic int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t   unsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(aead));\n\tstruct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);\n\tunsigned int ck_size;\n\tint ret = 0, key_ctx_size = 0;\n\tstruct crypto_aes_ctx aes;\n\n\taeadctx->enckey_len = 0;\n\tcrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)\n\t\t\t      & CRYPTO_TFM_REQ_MASK);\n\tret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\n\tif (ret)\n\t\tgoto out;\n\n\tif (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&\n\t    keylen > 3) {\n\t\tkeylen -= 4;   \n\t\tmemcpy(aeadctx->salt, key + keylen, 4);\n\t}\n\tif (keylen == AES_KEYSIZE_128) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t} else if (keylen == AES_KEYSIZE_192) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\t} else if (keylen == AES_KEYSIZE_256) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\t} else {\n\t\tpr_err(\"GCM: Invalid key length %d\\n\", keylen);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemcpy(aeadctx->key, key, keylen);\n\taeadctx->enckey_len = keylen;\n\tkey_ctx_size = sizeof(struct _key_ctx) + roundup(keylen, 16) +\n\t\tAEAD_H_SIZE;\n\taeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,\n\t\t\t\t\t\tCHCR_KEYCTX_MAC_KEY_SIZE_128,\n\t\t\t\t\t\t0, 0,\n\t\t\t\t\t\tkey_ctx_size >> 4);\n\t \n\tret = aes_expandkey(&aes, key, keylen);\n\tif (ret) {\n\t\taeadctx->enckey_len = 0;\n\t\tgoto out;\n\t}\n\tmemset(gctx->ghash_h, 0, AEAD_H_SIZE);\n\taes_encrypt(&aes, gctx->ghash_h, gctx->ghash_h);\n\tmemzero_explicit(&aes, sizeof(aes));\n\nout:\n\treturn ret;\n}\n\nstatic int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,\n\t\t\t\t   unsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(authenc));\n\tstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\n\t \n\tstruct crypto_authenc_keys keys;\n\tunsigned int bs, subtype;\n\tunsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;\n\tint err = 0, i, key_ctx_len = 0;\n\tunsigned char ck_size = 0;\n\tunsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };\n\tstruct crypto_shash *base_hash = ERR_PTR(-EINVAL);\n\tstruct algo_param param;\n\tint align;\n\tu8 *o_ptr = NULL;\n\n\tcrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)\n\t\t\t      & CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\n\tif (err)\n\t\tgoto out;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto out;\n\n\tif (get_alg_config(&param, max_authsize)) {\n\t\tpr_err(\"Unsupported digest size\\n\");\n\t\tgoto out;\n\t}\n\tsubtype = get_aead_subtype(authenc);\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||\n\t\tsubtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {\n\t\tif (keys.enckeylen < CTR_RFC3686_NONCE_SIZE)\n\t\t\tgoto out;\n\t\tmemcpy(aeadctx->nonce, keys.enckey + (keys.enckeylen\n\t\t- CTR_RFC3686_NONCE_SIZE), CTR_RFC3686_NONCE_SIZE);\n\t\tkeys.enckeylen -= CTR_RFC3686_NONCE_SIZE;\n\t}\n\tif (keys.enckeylen == AES_KEYSIZE_128) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t} else if (keys.enckeylen == AES_KEYSIZE_192) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\t} else if (keys.enckeylen == AES_KEYSIZE_256) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\t} else {\n\t\tpr_err(\"Unsupported cipher key\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tmemcpy(aeadctx->key, keys.enckey, keys.enckeylen);\n\taeadctx->enckey_len = keys.enckeylen;\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CBC_SHA ||\n\t\tsubtype == CRYPTO_ALG_SUB_TYPE_CBC_NULL) {\n\n\t\tget_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,\n\t\t\t    aeadctx->enckey_len << 3);\n\t}\n\tbase_hash  = chcr_alloc_shash(max_authsize);\n\tif (IS_ERR(base_hash)) {\n\t\tpr_err(\"Base driver cannot be loaded\\n\");\n\t\tgoto out;\n\t}\n\t{\n\t\tSHASH_DESC_ON_STACK(shash, base_hash);\n\n\t\tshash->tfm = base_hash;\n\t\tbs = crypto_shash_blocksize(base_hash);\n\t\talign = KEYCTX_ALIGN_PAD(max_authsize);\n\t\to_ptr =  actx->h_iopad + param.result_size + align;\n\n\t\tif (keys.authkeylen > bs) {\n\t\t\terr = crypto_shash_digest(shash, keys.authkey,\n\t\t\t\t\t\t  keys.authkeylen,\n\t\t\t\t\t\t  o_ptr);\n\t\t\tif (err) {\n\t\t\t\tpr_err(\"Base driver cannot be loaded\\n\");\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tkeys.authkeylen = max_authsize;\n\t\t} else\n\t\t\tmemcpy(o_ptr, keys.authkey, keys.authkeylen);\n\n\t\t \n\t\tmemset(pad + keys.authkeylen, 0, bs - keys.authkeylen);\n\t\tmemcpy(pad, o_ptr, keys.authkeylen);\n\t\tfor (i = 0; i < bs >> 2; i++)\n\t\t\t*((unsigned int *)pad + i) ^= IPAD_DATA;\n\n\t\tif (chcr_compute_partial_hash(shash, pad, actx->h_iopad,\n\t\t\t\t\t      max_authsize))\n\t\t\tgoto out;\n\t\t \n\t\tmemset(pad + keys.authkeylen, 0, bs - keys.authkeylen);\n\t\tmemcpy(pad, o_ptr, keys.authkeylen);\n\t\tfor (i = 0; i < bs >> 2; i++)\n\t\t\t*((unsigned int *)pad + i) ^= OPAD_DATA;\n\n\t\tif (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))\n\t\t\tgoto out;\n\n\t\t \n\t\tchcr_change_order(actx->h_iopad, param.result_size);\n\t\tchcr_change_order(o_ptr, param.result_size);\n\t\tkey_ctx_len = sizeof(struct _key_ctx) +\n\t\t\troundup(keys.enckeylen, 16) +\n\t\t\t(param.result_size + align) * 2;\n\t\taeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,\n\t\t\t\t\t\t0, 1, key_ctx_len >> 4);\n\t\tactx->auth_mode = param.auth_mode;\n\t\tchcr_free_shash(base_hash);\n\n\t\tmemzero_explicit(&keys, sizeof(keys));\n\t\treturn 0;\n\t}\nout:\n\taeadctx->enckey_len = 0;\n\tmemzero_explicit(&keys, sizeof(keys));\n\tif (!IS_ERR(base_hash))\n\t\tchcr_free_shash(base_hash);\n\treturn -EINVAL;\n}\n\nstatic int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,\n\t\t\t\t\tconst u8 *key, unsigned int keylen)\n{\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(authenc));\n\tstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\n\tstruct crypto_authenc_keys keys;\n\tint err;\n\t \n\tunsigned int subtype;\n\tint key_ctx_len = 0;\n\tunsigned char ck_size = 0;\n\n\tcrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)\n\t\t\t      & CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\n\tif (err)\n\t\tgoto out;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto out;\n\n\tsubtype = get_aead_subtype(authenc);\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||\n\t    subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {\n\t\tif (keys.enckeylen < CTR_RFC3686_NONCE_SIZE)\n\t\t\tgoto out;\n\t\tmemcpy(aeadctx->nonce, keys.enckey + (keys.enckeylen\n\t\t\t- CTR_RFC3686_NONCE_SIZE), CTR_RFC3686_NONCE_SIZE);\n\t\tkeys.enckeylen -= CTR_RFC3686_NONCE_SIZE;\n\t}\n\tif (keys.enckeylen == AES_KEYSIZE_128) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t} else if (keys.enckeylen == AES_KEYSIZE_192) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\t} else if (keys.enckeylen == AES_KEYSIZE_256) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\t} else {\n\t\tpr_err(\"Unsupported cipher key %d\\n\", keys.enckeylen);\n\t\tgoto out;\n\t}\n\tmemcpy(aeadctx->key, keys.enckey, keys.enckeylen);\n\taeadctx->enckey_len = keys.enckeylen;\n\tif (subtype == CRYPTO_ALG_SUB_TYPE_CBC_SHA ||\n\t    subtype == CRYPTO_ALG_SUB_TYPE_CBC_NULL) {\n\t\tget_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,\n\t\t\t\taeadctx->enckey_len << 3);\n\t}\n\tkey_ctx_len =  sizeof(struct _key_ctx) + roundup(keys.enckeylen, 16);\n\n\taeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,\n\t\t\t\t\t\t0, key_ctx_len >> 4);\n\tactx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn 0;\nout:\n\taeadctx->enckey_len = 0;\n\tmemzero_explicit(&keys, sizeof(keys));\n\treturn -EINVAL;\n}\n\nstatic int chcr_aead_op(struct aead_request *req,\n\t\t\tint size,\n\t\t\tcreate_wr_t create_wr_fn)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct uld_ctx *u_ctx = ULD_CTX(ctx);\n\tstruct sk_buff *skb;\n\tstruct chcr_dev *cdev;\n\n\tcdev = a_ctx(tfm)->dev;\n\tif (!cdev) {\n\t\tpr_err(\"%s : No crypto device.\\n\", __func__);\n\t\treturn -ENXIO;\n\t}\n\n\tif (chcr_inc_wrcount(cdev)) {\n\t \n\t\treturn chcr_aead_fallback(req, reqctx->op);\n\t}\n\n\tif (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\n\t\t\t\t\treqctx->txqidx) &&\n\t\t(!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))) {\n\t\t\tchcr_dec_wrcount(cdev);\n\t\t\treturn -ENOSPC;\n\t}\n\n\tif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&\n\t    crypto_ipsec_check_assoclen(req->assoclen) != 0) {\n\t\tpr_err(\"RFC4106: Invalid value of assoclen %d\\n\",\n\t\t       req->assoclen);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tskb = create_wr_fn(req, u_ctx->lldi.rxq_ids[reqctx->rxqidx], size);\n\n\tif (IS_ERR_OR_NULL(skb)) {\n\t\tchcr_dec_wrcount(cdev);\n\t\treturn PTR_ERR_OR_ZERO(skb);\n\t}\n\n\tskb->dev = u_ctx->lldi.ports[0];\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, reqctx->txqidx);\n\tchcr_send_wr(skb);\n\treturn -EINPROGRESS;\n}\n\nstatic int chcr_aead_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treqctx->txqidx = cpu % ctx->ntxq;\n\treqctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\treqctx->verify = VERIFY_HW;\n\treqctx->op = CHCR_ENCRYPT_OP;\n\n\tswitch (get_aead_subtype(tfm)) {\n\tcase CRYPTO_ALG_SUB_TYPE_CTR_SHA:\n\tcase CRYPTO_ALG_SUB_TYPE_CBC_SHA:\n\tcase CRYPTO_ALG_SUB_TYPE_CBC_NULL:\n\tcase CRYPTO_ALG_SUB_TYPE_CTR_NULL:\n\t\treturn chcr_aead_op(req, 0, create_authenc_wr);\n\tcase CRYPTO_ALG_SUB_TYPE_AEAD_CCM:\n\tcase CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:\n\t\treturn chcr_aead_op(req, 0, create_aead_ccm_wr);\n\tdefault:\n\t\treturn chcr_aead_op(req, 0, create_gcm_wr);\n\t}\n}\n\nstatic int chcr_aead_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct chcr_context *ctx = a_ctx(tfm);\n\tstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\n\tstruct chcr_aead_reqctx *reqctx = aead_request_ctx_dma(req);\n\tint size;\n\tunsigned int cpu;\n\n\tcpu = get_cpu();\n\treqctx->txqidx = cpu % ctx->ntxq;\n\treqctx->rxqidx = cpu % ctx->nrxq;\n\tput_cpu();\n\n\tif (aeadctx->mayverify == VERIFY_SW) {\n\t\tsize = crypto_aead_maxauthsize(tfm);\n\t\treqctx->verify = VERIFY_SW;\n\t} else {\n\t\tsize = 0;\n\t\treqctx->verify = VERIFY_HW;\n\t}\n\treqctx->op = CHCR_DECRYPT_OP;\n\tswitch (get_aead_subtype(tfm)) {\n\tcase CRYPTO_ALG_SUB_TYPE_CBC_SHA:\n\tcase CRYPTO_ALG_SUB_TYPE_CTR_SHA:\n\tcase CRYPTO_ALG_SUB_TYPE_CBC_NULL:\n\tcase CRYPTO_ALG_SUB_TYPE_CTR_NULL:\n\t\treturn chcr_aead_op(req, size, create_authenc_wr);\n\tcase CRYPTO_ALG_SUB_TYPE_AEAD_CCM:\n\tcase CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:\n\t\treturn chcr_aead_op(req, size, create_aead_ccm_wr);\n\tdefault:\n\t\treturn chcr_aead_op(req, size, create_gcm_wr);\n\t}\n}\n\nstatic struct chcr_alg_template driver_algs[] = {\n\t \n\t{\n\t\t.type = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_SUB_TYPE_CBC,\n\t\t.is_registered = 0,\n\t\t.alg.skcipher = {\n\t\t\t.base.cra_name\t\t= \"cbc(aes)\",\n\t\t\t.base.cra_driver_name\t= \"cbc-aes-chcr\",\n\t\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\n\t\t\t.init\t\t\t= chcr_init_tfm,\n\t\t\t.exit\t\t\t= chcr_exit_tfm,\n\t\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t\t.setkey\t\t\t= chcr_aes_cbc_setkey,\n\t\t\t.encrypt\t\t= chcr_aes_encrypt,\n\t\t\t.decrypt\t\t= chcr_aes_decrypt,\n\t\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_SUB_TYPE_XTS,\n\t\t.is_registered = 0,\n\t\t.alg.skcipher = {\n\t\t\t.base.cra_name\t\t= \"xts(aes)\",\n\t\t\t.base.cra_driver_name\t= \"xts-aes-chcr\",\n\t\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\n\t\t\t.init\t\t\t= chcr_init_tfm,\n\t\t\t.exit\t\t\t= chcr_exit_tfm,\n\t\t\t.min_keysize\t\t= 2 * AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t\t= 2 * AES_MAX_KEY_SIZE,\n\t\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t\t.setkey\t\t\t= chcr_aes_xts_setkey,\n\t\t\t.encrypt\t\t= chcr_aes_encrypt,\n\t\t\t.decrypt\t\t= chcr_aes_decrypt,\n\t\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_SUB_TYPE_CTR,\n\t\t.is_registered = 0,\n\t\t.alg.skcipher = {\n\t\t\t.base.cra_name\t\t= \"ctr(aes)\",\n\t\t\t.base.cra_driver_name\t= \"ctr-aes-chcr\",\n\t\t\t.base.cra_blocksize\t= 1,\n\n\t\t\t.init\t\t\t= chcr_init_tfm,\n\t\t\t.exit\t\t\t= chcr_exit_tfm,\n\t\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t\t.setkey\t\t\t= chcr_aes_ctr_setkey,\n\t\t\t.encrypt\t\t= chcr_aes_encrypt,\n\t\t\t.decrypt\t\t= chcr_aes_decrypt,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_SKCIPHER |\n\t\t\tCRYPTO_ALG_SUB_TYPE_CTR_RFC3686,\n\t\t.is_registered = 0,\n\t\t.alg.skcipher = {\n\t\t\t.base.cra_name\t\t= \"rfc3686(ctr(aes))\",\n\t\t\t.base.cra_driver_name\t= \"rfc3686-ctr-aes-chcr\",\n\t\t\t.base.cra_blocksize\t= 1,\n\n\t\t\t.init\t\t\t= chcr_rfc3686_init,\n\t\t\t.exit\t\t\t= chcr_exit_tfm,\n\t\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,\n\t\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,\n\t\t\t.ivsize\t\t\t= CTR_RFC3686_IV_SIZE,\n\t\t\t.setkey\t\t\t= chcr_aes_rfc3686_setkey,\n\t\t\t.encrypt\t\t= chcr_aes_encrypt,\n\t\t\t.decrypt\t\t= chcr_aes_decrypt,\n\t\t}\n\t},\n\t \n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"sha1\",\n\t\t\t\t.cra_driver_name = \"sha1-chcr\",\n\t\t\t\t.cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"sha256\",\n\t\t\t\t.cra_driver_name = \"sha256-chcr\",\n\t\t\t\t.cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"sha224\",\n\t\t\t\t.cra_driver_name = \"sha224-chcr\",\n\t\t\t\t.cra_blocksize = SHA224_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"sha384\",\n\t\t\t\t.cra_driver_name = \"sha384-chcr\",\n\t\t\t\t.cra_blocksize = SHA384_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AHASH,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"sha512\",\n\t\t\t\t.cra_driver_name = \"sha512-chcr\",\n\t\t\t\t.cra_blocksize = SHA512_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t \n\t{\n\t\t.type = CRYPTO_ALG_TYPE_HMAC,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA1_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"hmac(sha1)\",\n\t\t\t\t.cra_driver_name = \"hmac-sha1-chcr\",\n\t\t\t\t.cra_blocksize = SHA1_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_HMAC,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA224_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"hmac(sha224)\",\n\t\t\t\t.cra_driver_name = \"hmac-sha224-chcr\",\n\t\t\t\t.cra_blocksize = SHA224_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_HMAC,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA256_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"hmac(sha256)\",\n\t\t\t\t.cra_driver_name = \"hmac-sha256-chcr\",\n\t\t\t\t.cra_blocksize = SHA256_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_HMAC,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA384_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"hmac(sha384)\",\n\t\t\t\t.cra_driver_name = \"hmac-sha384-chcr\",\n\t\t\t\t.cra_blocksize = SHA384_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_HMAC,\n\t\t.is_registered = 0,\n\t\t.alg.hash = {\n\t\t\t.halg.digestsize = SHA512_DIGEST_SIZE,\n\t\t\t.halg.base = {\n\t\t\t\t.cra_name = \"hmac(sha512)\",\n\t\t\t\t.cra_driver_name = \"hmac-sha512-chcr\",\n\t\t\t\t.cra_blocksize = SHA512_BLOCK_SIZE,\n\t\t\t}\n\t\t}\n\t},\n\t \n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_GCM,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"gcm(aes)\",\n\t\t\t\t.cra_driver_name = \"gcm-aes-chcr\",\n\t\t\t\t.cra_blocksize\t= 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_gcm_ctx),\n\t\t\t},\n\t\t\t.ivsize = GCM_AES_IV_SIZE,\n\t\t\t.maxauthsize = GHASH_DIGEST_SIZE,\n\t\t\t.setkey = chcr_gcm_setkey,\n\t\t\t.setauthsize = chcr_gcm_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"rfc4106(gcm(aes))\",\n\t\t\t\t.cra_driver_name = \"rfc4106-gcm-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY + 1,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_gcm_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = GCM_RFC4106_IV_SIZE,\n\t\t\t.maxauthsize\t= GHASH_DIGEST_SIZE,\n\t\t\t.setkey = chcr_gcm_setkey,\n\t\t\t.setauthsize\t= chcr_4106_4309_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_CCM,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"ccm(aes)\",\n\t\t\t\t.cra_driver_name = \"ccm-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize\t= GHASH_DIGEST_SIZE,\n\t\t\t.setkey = chcr_aead_ccm_setkey,\n\t\t\t.setauthsize\t= chcr_ccm_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"rfc4309(ccm(aes))\",\n\t\t\t\t.cra_driver_name = \"rfc4309-ccm-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY + 1,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = 8,\n\t\t\t.maxauthsize\t= GHASH_DIGEST_SIZE,\n\t\t\t.setkey = chcr_aead_rfc4309_setkey,\n\t\t\t.setauthsize = chcr_4106_4309_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha1),cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-hmac-sha1-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize = SHA1_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\n\t\t\t\t.cra_name = \"authenc(hmac(sha256),cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-hmac-sha256-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize\t= SHA256_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha224),cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-hmac-sha224-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize = SHA224_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha384),cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-hmac-sha384-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize = SHA384_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha512),cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-hmac-sha512-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize = SHA512_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_NULL,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(digest_null,cbc(aes))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\t\"authenc-digest_null-cbc-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = AES_BLOCK_SIZE,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize  = AES_BLOCK_SIZE,\n\t\t\t.maxauthsize = 0,\n\t\t\t.setkey  = chcr_aead_digest_null_setkey,\n\t\t\t.setauthsize = chcr_authenc_null_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha1),rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-hmac-sha1-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize = SHA1_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\n\t\t\t\t.cra_name = \"authenc(hmac(sha256),rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-hmac-sha256-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize\t= SHA256_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha224),rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-hmac-sha224-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\t\t\t},\n\t\t\t.ivsize = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize = SHA224_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha384),rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-hmac-sha384-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize = SHA384_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(hmac(sha512),rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-hmac-sha512-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize = SHA512_DIGEST_SIZE,\n\t\t\t.setkey = chcr_authenc_setkey,\n\t\t\t.setauthsize = chcr_authenc_setauthsize,\n\t\t}\n\t},\n\t{\n\t\t.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_NULL,\n\t\t.is_registered = 0,\n\t\t.alg.aead = {\n\t\t\t.base = {\n\t\t\t\t.cra_name = \"authenc(digest_null,rfc3686(ctr(aes)))\",\n\t\t\t\t.cra_driver_name =\n\t\t\t\t\"authenc-digest_null-rfc3686-ctr-aes-chcr\",\n\t\t\t\t.cra_blocksize\t = 1,\n\t\t\t\t.cra_priority = CHCR_AEAD_PRIORITY,\n\t\t\t\t.cra_ctxsize =\tsizeof(struct chcr_context) +\n\t\t\t\t\t\tsizeof(struct chcr_aead_ctx) +\n\t\t\t\t\t\tsizeof(struct chcr_authenc_ctx),\n\n\t\t\t},\n\t\t\t.ivsize  = CTR_RFC3686_IV_SIZE,\n\t\t\t.maxauthsize = 0,\n\t\t\t.setkey  = chcr_aead_digest_null_setkey,\n\t\t\t.setauthsize = chcr_authenc_null_setauthsize,\n\t\t}\n\t},\n};\n\n \nstatic int chcr_unregister_alg(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\n\t\tswitch (driver_algs[i].type & CRYPTO_ALG_TYPE_MASK) {\n\t\tcase CRYPTO_ALG_TYPE_SKCIPHER:\n\t\t\tif (driver_algs[i].is_registered && refcount_read(\n\t\t\t    &driver_algs[i].alg.skcipher.base.cra_refcnt)\n\t\t\t    == 1) {\n\t\t\t\tcrypto_unregister_skcipher(\n\t\t\t\t\t\t&driver_algs[i].alg.skcipher);\n\t\t\t\tdriver_algs[i].is_registered = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\t\tif (driver_algs[i].is_registered && refcount_read(\n\t\t\t    &driver_algs[i].alg.aead.base.cra_refcnt) == 1) {\n\t\t\t\tcrypto_unregister_aead(\n\t\t\t\t\t\t&driver_algs[i].alg.aead);\n\t\t\t\tdriver_algs[i].is_registered = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\t\tif (driver_algs[i].is_registered && refcount_read(\n\t\t\t    &driver_algs[i].alg.hash.halg.base.cra_refcnt)\n\t\t\t    == 1) {\n\t\t\t\tcrypto_unregister_ahash(\n\t\t\t\t\t\t&driver_algs[i].alg.hash);\n\t\t\t\tdriver_algs[i].is_registered = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\n#define SZ_AHASH_CTX sizeof(struct chcr_context)\n#define SZ_AHASH_H_CTX (sizeof(struct chcr_context) + sizeof(struct hmac_ctx))\n#define SZ_AHASH_REQ_CTX sizeof(struct chcr_ahash_req_ctx)\n\n \nstatic int chcr_register_alg(void)\n{\n\tstruct crypto_alg ai;\n\tstruct ahash_alg *a_hash;\n\tint err = 0, i;\n\tchar *name = NULL;\n\n\tfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\n\t\tif (driver_algs[i].is_registered)\n\t\t\tcontinue;\n\t\tswitch (driver_algs[i].type & CRYPTO_ALG_TYPE_MASK) {\n\t\tcase CRYPTO_ALG_TYPE_SKCIPHER:\n\t\t\tdriver_algs[i].alg.skcipher.base.cra_priority =\n\t\t\t\tCHCR_CRA_PRIORITY;\n\t\t\tdriver_algs[i].alg.skcipher.base.cra_module = THIS_MODULE;\n\t\t\tdriver_algs[i].alg.skcipher.base.cra_flags =\n\t\t\t\tCRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_ASYNC |\n\t\t\t\tCRYPTO_ALG_ALLOCATES_MEMORY |\n\t\t\t\tCRYPTO_ALG_NEED_FALLBACK;\n\t\t\tdriver_algs[i].alg.skcipher.base.cra_ctxsize =\n\t\t\t\tsizeof(struct chcr_context) +\n\t\t\t\tsizeof(struct ablk_ctx);\n\t\t\tdriver_algs[i].alg.skcipher.base.cra_alignmask = 0;\n\n\t\t\terr = crypto_register_skcipher(&driver_algs[i].alg.skcipher);\n\t\t\tname = driver_algs[i].alg.skcipher.base.cra_driver_name;\n\t\t\tbreak;\n\t\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\t\tdriver_algs[i].alg.aead.base.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK |\n\t\t\t\tCRYPTO_ALG_ALLOCATES_MEMORY;\n\t\t\tdriver_algs[i].alg.aead.encrypt = chcr_aead_encrypt;\n\t\t\tdriver_algs[i].alg.aead.decrypt = chcr_aead_decrypt;\n\t\t\tdriver_algs[i].alg.aead.init = chcr_aead_cra_init;\n\t\t\tdriver_algs[i].alg.aead.exit = chcr_aead_cra_exit;\n\t\t\tdriver_algs[i].alg.aead.base.cra_module = THIS_MODULE;\n\t\t\terr = crypto_register_aead(&driver_algs[i].alg.aead);\n\t\t\tname = driver_algs[i].alg.aead.base.cra_driver_name;\n\t\t\tbreak;\n\t\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\t\ta_hash = &driver_algs[i].alg.hash;\n\t\t\ta_hash->update = chcr_ahash_update;\n\t\t\ta_hash->final = chcr_ahash_final;\n\t\t\ta_hash->finup = chcr_ahash_finup;\n\t\t\ta_hash->digest = chcr_ahash_digest;\n\t\t\ta_hash->export = chcr_ahash_export;\n\t\t\ta_hash->import = chcr_ahash_import;\n\t\t\ta_hash->halg.statesize = SZ_AHASH_REQ_CTX;\n\t\t\ta_hash->halg.base.cra_priority = CHCR_CRA_PRIORITY;\n\t\t\ta_hash->halg.base.cra_module = THIS_MODULE;\n\t\t\ta_hash->halg.base.cra_flags =\n\t\t\t\tCRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY;\n\t\t\ta_hash->halg.base.cra_alignmask = 0;\n\t\t\ta_hash->halg.base.cra_exit = NULL;\n\n\t\t\tif (driver_algs[i].type == CRYPTO_ALG_TYPE_HMAC) {\n\t\t\t\ta_hash->halg.base.cra_init = chcr_hmac_cra_init;\n\t\t\t\ta_hash->halg.base.cra_exit = chcr_hmac_cra_exit;\n\t\t\t\ta_hash->init = chcr_hmac_init;\n\t\t\t\ta_hash->setkey = chcr_ahash_setkey;\n\t\t\t\ta_hash->halg.base.cra_ctxsize = SZ_AHASH_H_CTX;\n\t\t\t} else {\n\t\t\t\ta_hash->init = chcr_sha_init;\n\t\t\t\ta_hash->halg.base.cra_ctxsize = SZ_AHASH_CTX;\n\t\t\t\ta_hash->halg.base.cra_init = chcr_sha_cra_init;\n\t\t\t}\n\t\t\terr = crypto_register_ahash(&driver_algs[i].alg.hash);\n\t\t\tai = driver_algs[i].alg.hash.halg.base;\n\t\t\tname = ai.cra_driver_name;\n\t\t\tbreak;\n\t\t}\n\t\tif (err) {\n\t\t\tpr_err(\"%s : Algorithm registration failed\\n\", name);\n\t\t\tgoto register_err;\n\t\t} else {\n\t\t\tdriver_algs[i].is_registered = 1;\n\t\t}\n\t}\n\treturn 0;\n\nregister_err:\n\tchcr_unregister_alg();\n\treturn err;\n}\n\n \nint start_crypto(void)\n{\n\treturn chcr_register_alg();\n}\n\n \nint stop_crypto(void)\n{\n\tchcr_unregister_alg();\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}