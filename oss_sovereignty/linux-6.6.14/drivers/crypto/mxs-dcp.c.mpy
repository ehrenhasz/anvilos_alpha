{
  "module_name": "mxs-dcp.c",
  "hash_id": "a9e9eef65bc24200781f8b8a415d25413f0b3182f478a15f1fce0ba2f9648fc3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/crypto/mxs-dcp.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/stmp_device.h>\n#include <linux/clk.h>\n\n#include <crypto/aes.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n\n#define DCP_MAX_CHANS\t4\n#define DCP_BUF_SZ\tPAGE_SIZE\n#define DCP_SHA_PAY_SZ  64\n\n#define DCP_ALIGNMENT\t64\n\n \nstatic const uint8_t sha1_null_hash[] =\n\t\"\\x09\\x07\\xd8\\xaf\\x90\\x18\\x60\\x95\\xef\\xbf\"\n\t\"\\x55\\x32\\x0d\\x4b\\x6b\\x5e\\xee\\xa3\\x39\\xda\";\n\nstatic const uint8_t sha256_null_hash[] =\n\t\"\\x55\\xb8\\x52\\x78\\x1b\\x99\\x95\\xa4\"\n\t\"\\x4c\\x93\\x9b\\x64\\xe4\\x41\\xae\\x27\"\n\t\"\\x24\\xb9\\x6f\\x99\\xc8\\xf4\\xfb\\x9a\"\n\t\"\\x14\\x1c\\xfc\\x98\\x42\\xc4\\xb0\\xe3\";\n\n \nstruct dcp_dma_desc {\n\tuint32_t\tnext_cmd_addr;\n\tuint32_t\tcontrol0;\n\tuint32_t\tcontrol1;\n\tuint32_t\tsource;\n\tuint32_t\tdestination;\n\tuint32_t\tsize;\n\tuint32_t\tpayload;\n\tuint32_t\tstatus;\n};\n\n \nstruct dcp_coherent_block {\n\tuint8_t\t\t\taes_in_buf[DCP_BUF_SZ];\n\tuint8_t\t\t\taes_out_buf[DCP_BUF_SZ];\n\tuint8_t\t\t\tsha_in_buf[DCP_BUF_SZ];\n\tuint8_t\t\t\tsha_out_buf[DCP_SHA_PAY_SZ];\n\n\tuint8_t\t\t\taes_key[2 * AES_KEYSIZE_128];\n\n\tstruct dcp_dma_desc\tdesc[DCP_MAX_CHANS];\n};\n\nstruct dcp {\n\tstruct device\t\t\t*dev;\n\tvoid __iomem\t\t\t*base;\n\n\tuint32_t\t\t\tcaps;\n\n\tstruct dcp_coherent_block\t*coh;\n\n\tstruct completion\t\tcompletion[DCP_MAX_CHANS];\n\tspinlock_t\t\t\tlock[DCP_MAX_CHANS];\n\tstruct task_struct\t\t*thread[DCP_MAX_CHANS];\n\tstruct crypto_queue\t\tqueue[DCP_MAX_CHANS];\n\tstruct clk\t\t\t*dcp_clk;\n};\n\nenum dcp_chan {\n\tDCP_CHAN_HASH_SHA\t= 0,\n\tDCP_CHAN_CRYPTO\t\t= 2,\n};\n\nstruct dcp_async_ctx {\n\t \n\tenum dcp_chan\tchan;\n\tuint32_t\tfill;\n\n\t \n\tstruct mutex\t\t\tmutex;\n\tuint32_t\t\t\talg;\n\tunsigned int\t\t\thot:1;\n\n\t \n\tstruct crypto_skcipher\t\t*fallback;\n\tunsigned int\t\t\tkey_len;\n\tuint8_t\t\t\t\tkey[AES_KEYSIZE_128];\n};\n\nstruct dcp_aes_req_ctx {\n\tunsigned int\tenc:1;\n\tunsigned int\tecb:1;\n\tstruct skcipher_request fallback_req;\t\n};\n\nstruct dcp_sha_req_ctx {\n\tunsigned int\tinit:1;\n\tunsigned int\tfini:1;\n};\n\nstruct dcp_export_state {\n\tstruct dcp_sha_req_ctx req_ctx;\n\tstruct dcp_async_ctx async_ctx;\n};\n\n \nstatic struct dcp *global_sdcp;\n\n \n#define MXS_DCP_CTRL\t\t\t\t0x00\n#define MXS_DCP_CTRL_GATHER_RESIDUAL_WRITES\t(1 << 23)\n#define MXS_DCP_CTRL_ENABLE_CONTEXT_CACHING\t(1 << 22)\n\n#define MXS_DCP_STAT\t\t\t\t0x10\n#define MXS_DCP_STAT_CLR\t\t\t0x18\n#define MXS_DCP_STAT_IRQ_MASK\t\t\t0xf\n\n#define MXS_DCP_CHANNELCTRL\t\t\t0x20\n#define MXS_DCP_CHANNELCTRL_ENABLE_CHANNEL_MASK\t0xff\n\n#define MXS_DCP_CAPABILITY1\t\t\t0x40\n#define MXS_DCP_CAPABILITY1_SHA256\t\t(4 << 16)\n#define MXS_DCP_CAPABILITY1_SHA1\t\t(1 << 16)\n#define MXS_DCP_CAPABILITY1_AES128\t\t(1 << 0)\n\n#define MXS_DCP_CONTEXT\t\t\t\t0x50\n\n#define MXS_DCP_CH_N_CMDPTR(n)\t\t\t(0x100 + ((n) * 0x40))\n\n#define MXS_DCP_CH_N_SEMA(n)\t\t\t(0x110 + ((n) * 0x40))\n\n#define MXS_DCP_CH_N_STAT(n)\t\t\t(0x120 + ((n) * 0x40))\n#define MXS_DCP_CH_N_STAT_CLR(n)\t\t(0x128 + ((n) * 0x40))\n\n \n#define MXS_DCP_CONTROL0_HASH_TERM\t\t(1 << 13)\n#define MXS_DCP_CONTROL0_HASH_INIT\t\t(1 << 12)\n#define MXS_DCP_CONTROL0_PAYLOAD_KEY\t\t(1 << 11)\n#define MXS_DCP_CONTROL0_CIPHER_ENCRYPT\t\t(1 << 8)\n#define MXS_DCP_CONTROL0_CIPHER_INIT\t\t(1 << 9)\n#define MXS_DCP_CONTROL0_ENABLE_HASH\t\t(1 << 6)\n#define MXS_DCP_CONTROL0_ENABLE_CIPHER\t\t(1 << 5)\n#define MXS_DCP_CONTROL0_DECR_SEMAPHORE\t\t(1 << 1)\n#define MXS_DCP_CONTROL0_INTERRUPT\t\t(1 << 0)\n\n#define MXS_DCP_CONTROL1_HASH_SELECT_SHA256\t(2 << 16)\n#define MXS_DCP_CONTROL1_HASH_SELECT_SHA1\t(0 << 16)\n#define MXS_DCP_CONTROL1_CIPHER_MODE_CBC\t(1 << 4)\n#define MXS_DCP_CONTROL1_CIPHER_MODE_ECB\t(0 << 4)\n#define MXS_DCP_CONTROL1_CIPHER_SELECT_AES128\t(0 << 0)\n\nstatic int mxs_dcp_start_dma(struct dcp_async_ctx *actx)\n{\n\tint dma_err;\n\tstruct dcp *sdcp = global_sdcp;\n\tconst int chan = actx->chan;\n\tuint32_t stat;\n\tunsigned long ret;\n\tstruct dcp_dma_desc *desc = &sdcp->coh->desc[actx->chan];\n\tdma_addr_t desc_phys = dma_map_single(sdcp->dev, desc, sizeof(*desc),\n\t\t\t\t\t      DMA_TO_DEVICE);\n\n\tdma_err = dma_mapping_error(sdcp->dev, desc_phys);\n\tif (dma_err)\n\t\treturn dma_err;\n\n\treinit_completion(&sdcp->completion[chan]);\n\n\t \n\twritel(0xffffffff, sdcp->base + MXS_DCP_CH_N_STAT_CLR(chan));\n\n\t \n\twritel(desc_phys, sdcp->base + MXS_DCP_CH_N_CMDPTR(chan));\n\n\t \n\twritel(1, sdcp->base + MXS_DCP_CH_N_SEMA(chan));\n\n\tret = wait_for_completion_timeout(&sdcp->completion[chan],\n\t\t\t\t\t  msecs_to_jiffies(1000));\n\tif (!ret) {\n\t\tdev_err(sdcp->dev, \"Channel %i timeout (DCP_STAT=0x%08x)\\n\",\n\t\t\tchan, readl(sdcp->base + MXS_DCP_STAT));\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tstat = readl(sdcp->base + MXS_DCP_CH_N_STAT(chan));\n\tif (stat & 0xff) {\n\t\tdev_err(sdcp->dev, \"Channel %i error (CH_STAT=0x%08x)\\n\",\n\t\t\tchan, stat);\n\t\treturn -EINVAL;\n\t}\n\n\tdma_unmap_single(sdcp->dev, desc_phys, sizeof(*desc), DMA_TO_DEVICE);\n\n\treturn 0;\n}\n\n \nstatic int mxs_dcp_run_aes(struct dcp_async_ctx *actx,\n\t\t\t   struct skcipher_request *req, int init)\n{\n\tdma_addr_t key_phys, src_phys, dst_phys;\n\tstruct dcp *sdcp = global_sdcp;\n\tstruct dcp_dma_desc *desc = &sdcp->coh->desc[actx->chan];\n\tstruct dcp_aes_req_ctx *rctx = skcipher_request_ctx(req);\n\tint ret;\n\n\tkey_phys = dma_map_single(sdcp->dev, sdcp->coh->aes_key,\n\t\t\t\t  2 * AES_KEYSIZE_128, DMA_TO_DEVICE);\n\tret = dma_mapping_error(sdcp->dev, key_phys);\n\tif (ret)\n\t\treturn ret;\n\n\tsrc_phys = dma_map_single(sdcp->dev, sdcp->coh->aes_in_buf,\n\t\t\t\t  DCP_BUF_SZ, DMA_TO_DEVICE);\n\tret = dma_mapping_error(sdcp->dev, src_phys);\n\tif (ret)\n\t\tgoto err_src;\n\n\tdst_phys = dma_map_single(sdcp->dev, sdcp->coh->aes_out_buf,\n\t\t\t\t  DCP_BUF_SZ, DMA_FROM_DEVICE);\n\tret = dma_mapping_error(sdcp->dev, dst_phys);\n\tif (ret)\n\t\tgoto err_dst;\n\n\tif (actx->fill % AES_BLOCK_SIZE) {\n\t\tdev_err(sdcp->dev, \"Invalid block size!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto aes_done_run;\n\t}\n\n\t \n\tdesc->control0 = MXS_DCP_CONTROL0_DECR_SEMAPHORE |\n\t\t    MXS_DCP_CONTROL0_INTERRUPT |\n\t\t    MXS_DCP_CONTROL0_ENABLE_CIPHER;\n\n\t \n\tdesc->control0 |= MXS_DCP_CONTROL0_PAYLOAD_KEY;\n\n\tif (rctx->enc)\n\t\tdesc->control0 |= MXS_DCP_CONTROL0_CIPHER_ENCRYPT;\n\tif (init)\n\t\tdesc->control0 |= MXS_DCP_CONTROL0_CIPHER_INIT;\n\n\tdesc->control1 = MXS_DCP_CONTROL1_CIPHER_SELECT_AES128;\n\n\tif (rctx->ecb)\n\t\tdesc->control1 |= MXS_DCP_CONTROL1_CIPHER_MODE_ECB;\n\telse\n\t\tdesc->control1 |= MXS_DCP_CONTROL1_CIPHER_MODE_CBC;\n\n\tdesc->next_cmd_addr = 0;\n\tdesc->source = src_phys;\n\tdesc->destination = dst_phys;\n\tdesc->size = actx->fill;\n\tdesc->payload = key_phys;\n\tdesc->status = 0;\n\n\tret = mxs_dcp_start_dma(actx);\n\naes_done_run:\n\tdma_unmap_single(sdcp->dev, dst_phys, DCP_BUF_SZ, DMA_FROM_DEVICE);\nerr_dst:\n\tdma_unmap_single(sdcp->dev, src_phys, DCP_BUF_SZ, DMA_TO_DEVICE);\nerr_src:\n\tdma_unmap_single(sdcp->dev, key_phys, 2 * AES_KEYSIZE_128,\n\t\t\t DMA_TO_DEVICE);\n\n\treturn ret;\n}\n\nstatic int mxs_dcp_aes_block_crypt(struct crypto_async_request *arq)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\n\tstruct skcipher_request *req = skcipher_request_cast(arq);\n\tstruct dcp_async_ctx *actx = crypto_tfm_ctx(arq->tfm);\n\tstruct dcp_aes_req_ctx *rctx = skcipher_request_ctx(req);\n\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *src = req->src;\n\tint dst_nents = sg_nents(dst);\n\n\tconst int out_off = DCP_BUF_SZ;\n\tuint8_t *in_buf = sdcp->coh->aes_in_buf;\n\tuint8_t *out_buf = sdcp->coh->aes_out_buf;\n\n\tuint32_t dst_off = 0;\n\tuint8_t *src_buf = NULL;\n\tuint32_t last_out_len = 0;\n\n\tuint8_t *key = sdcp->coh->aes_key;\n\n\tint ret = 0;\n\tunsigned int i, len, clen, tlen = 0;\n\tint init = 0;\n\tbool limit_hit = false;\n\n\tactx->fill = 0;\n\n\t \n\tmemcpy(key, actx->key, actx->key_len);\n\n\tif (!rctx->ecb) {\n\t\t \n\t\tmemcpy(key + AES_KEYSIZE_128, req->iv, AES_KEYSIZE_128);\n\t\t \n\t\tinit = 1;\n\t} else {\n\t\tmemset(key + AES_KEYSIZE_128, 0, AES_KEYSIZE_128);\n\t}\n\n\tfor_each_sg(req->src, src, sg_nents(req->src), i) {\n\t\tsrc_buf = sg_virt(src);\n\t\tlen = sg_dma_len(src);\n\t\ttlen += len;\n\t\tlimit_hit = tlen > req->cryptlen;\n\n\t\tif (limit_hit)\n\t\t\tlen = req->cryptlen - (tlen - len);\n\n\t\tdo {\n\t\t\tif (actx->fill + len > out_off)\n\t\t\t\tclen = out_off - actx->fill;\n\t\t\telse\n\t\t\t\tclen = len;\n\n\t\t\tmemcpy(in_buf + actx->fill, src_buf, clen);\n\t\t\tlen -= clen;\n\t\t\tsrc_buf += clen;\n\t\t\tactx->fill += clen;\n\n\t\t\t \n\t\t\tif (actx->fill == out_off || sg_is_last(src) ||\n\t\t\t    limit_hit) {\n\t\t\t\tret = mxs_dcp_run_aes(actx, req, init);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tinit = 0;\n\n\t\t\t\tsg_pcopy_from_buffer(dst, dst_nents, out_buf,\n\t\t\t\t\t\t     actx->fill, dst_off);\n\t\t\t\tdst_off += actx->fill;\n\t\t\t\tlast_out_len = actx->fill;\n\t\t\t\tactx->fill = 0;\n\t\t\t}\n\t\t} while (len);\n\n\t\tif (limit_hit)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (!rctx->ecb) {\n\t\tif (rctx->enc)\n\t\t\tmemcpy(req->iv, out_buf+(last_out_len-AES_BLOCK_SIZE),\n\t\t\t\tAES_BLOCK_SIZE);\n\t\telse\n\t\t\tmemcpy(req->iv, in_buf+(last_out_len-AES_BLOCK_SIZE),\n\t\t\t\tAES_BLOCK_SIZE);\n\t}\n\n\treturn ret;\n}\n\nstatic int dcp_chan_thread_aes(void *data)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\tconst int chan = DCP_CHAN_CRYPTO;\n\n\tstruct crypto_async_request *backlog;\n\tstruct crypto_async_request *arq;\n\n\tint ret;\n\n\twhile (!kthread_should_stop()) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_lock(&sdcp->lock[chan]);\n\t\tbacklog = crypto_get_backlog(&sdcp->queue[chan]);\n\t\tarq = crypto_dequeue_request(&sdcp->queue[chan]);\n\t\tspin_unlock(&sdcp->lock[chan]);\n\n\t\tif (!backlog && !arq) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tset_current_state(TASK_RUNNING);\n\n\t\tif (backlog)\n\t\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\t\tif (arq) {\n\t\t\tret = mxs_dcp_aes_block_crypt(arq);\n\t\t\tcrypto_request_complete(arq, ret);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mxs_dcp_block_fallback(struct skcipher_request *req, int enc)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct dcp_aes_req_ctx *rctx = skcipher_request_ctx(req);\n\tstruct dcp_async_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint ret;\n\n\tskcipher_request_set_tfm(&rctx->fallback_req, ctx->fallback);\n\tskcipher_request_set_callback(&rctx->fallback_req, req->base.flags,\n\t\t\t\t      req->base.complete, req->base.data);\n\tskcipher_request_set_crypt(&rctx->fallback_req, req->src, req->dst,\n\t\t\t\t   req->cryptlen, req->iv);\n\n\tif (enc)\n\t\tret = crypto_skcipher_encrypt(&rctx->fallback_req);\n\telse\n\t\tret = crypto_skcipher_decrypt(&rctx->fallback_req);\n\n\treturn ret;\n}\n\nstatic int mxs_dcp_aes_enqueue(struct skcipher_request *req, int enc, int ecb)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\tstruct crypto_async_request *arq = &req->base;\n\tstruct dcp_async_ctx *actx = crypto_tfm_ctx(arq->tfm);\n\tstruct dcp_aes_req_ctx *rctx = skcipher_request_ctx(req);\n\tint ret;\n\n\tif (unlikely(actx->key_len != AES_KEYSIZE_128))\n\t\treturn mxs_dcp_block_fallback(req, enc);\n\n\trctx->enc = enc;\n\trctx->ecb = ecb;\n\tactx->chan = DCP_CHAN_CRYPTO;\n\n\tspin_lock(&sdcp->lock[actx->chan]);\n\tret = crypto_enqueue_request(&sdcp->queue[actx->chan], &req->base);\n\tspin_unlock(&sdcp->lock[actx->chan]);\n\n\twake_up_process(sdcp->thread[actx->chan]);\n\n\treturn ret;\n}\n\nstatic int mxs_dcp_aes_ecb_decrypt(struct skcipher_request *req)\n{\n\treturn mxs_dcp_aes_enqueue(req, 0, 1);\n}\n\nstatic int mxs_dcp_aes_ecb_encrypt(struct skcipher_request *req)\n{\n\treturn mxs_dcp_aes_enqueue(req, 1, 1);\n}\n\nstatic int mxs_dcp_aes_cbc_decrypt(struct skcipher_request *req)\n{\n\treturn mxs_dcp_aes_enqueue(req, 0, 0);\n}\n\nstatic int mxs_dcp_aes_cbc_encrypt(struct skcipher_request *req)\n{\n\treturn mxs_dcp_aes_enqueue(req, 1, 0);\n}\n\nstatic int mxs_dcp_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t      unsigned int len)\n{\n\tstruct dcp_async_ctx *actx = crypto_skcipher_ctx(tfm);\n\n\t \n\tactx->key_len = len;\n\tif (len == AES_KEYSIZE_128) {\n\t\tmemcpy(actx->key, key, len);\n\t\treturn 0;\n\t}\n\n\t \n\tcrypto_skcipher_clear_flags(actx->fallback, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(actx->fallback,\n\t\t\t\t  tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\n\treturn crypto_skcipher_setkey(actx->fallback, key, len);\n}\n\nstatic int mxs_dcp_aes_fallback_init_tfm(struct crypto_skcipher *tfm)\n{\n\tconst char *name = crypto_tfm_alg_name(crypto_skcipher_tfm(tfm));\n\tstruct dcp_async_ctx *actx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_skcipher *blk;\n\n\tblk = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_NEED_FALLBACK);\n\tif (IS_ERR(blk))\n\t\treturn PTR_ERR(blk);\n\n\tactx->fallback = blk;\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct dcp_aes_req_ctx) +\n\t\t\t\t\t crypto_skcipher_reqsize(blk));\n\treturn 0;\n}\n\nstatic void mxs_dcp_aes_fallback_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct dcp_async_ctx *actx = crypto_skcipher_ctx(tfm);\n\n\tcrypto_free_skcipher(actx->fallback);\n}\n\n \nstatic int mxs_dcp_run_sha(struct ahash_request *req)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\tint ret;\n\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx = crypto_ahash_ctx(tfm);\n\tstruct dcp_sha_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct dcp_dma_desc *desc = &sdcp->coh->desc[actx->chan];\n\n\tdma_addr_t digest_phys = 0;\n\tdma_addr_t buf_phys = dma_map_single(sdcp->dev, sdcp->coh->sha_in_buf,\n\t\t\t\t\t     DCP_BUF_SZ, DMA_TO_DEVICE);\n\n\tret = dma_mapping_error(sdcp->dev, buf_phys);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tdesc->control0 = MXS_DCP_CONTROL0_DECR_SEMAPHORE |\n\t\t    MXS_DCP_CONTROL0_INTERRUPT |\n\t\t    MXS_DCP_CONTROL0_ENABLE_HASH;\n\tif (rctx->init)\n\t\tdesc->control0 |= MXS_DCP_CONTROL0_HASH_INIT;\n\n\tdesc->control1 = actx->alg;\n\tdesc->next_cmd_addr = 0;\n\tdesc->source = buf_phys;\n\tdesc->destination = 0;\n\tdesc->size = actx->fill;\n\tdesc->payload = 0;\n\tdesc->status = 0;\n\n\t \n\tif (rctx->init && rctx->fini && desc->size == 0) {\n\t\tstruct hash_alg_common *halg = crypto_hash_alg_common(tfm);\n\t\tconst uint8_t *sha_buf =\n\t\t\t(actx->alg == MXS_DCP_CONTROL1_HASH_SELECT_SHA1) ?\n\t\t\tsha1_null_hash : sha256_null_hash;\n\t\tmemcpy(sdcp->coh->sha_out_buf, sha_buf, halg->digestsize);\n\t\tret = 0;\n\t\tgoto done_run;\n\t}\n\n\t \n\tif (rctx->fini) {\n\t\tdigest_phys = dma_map_single(sdcp->dev, sdcp->coh->sha_out_buf,\n\t\t\t\t\t     DCP_SHA_PAY_SZ, DMA_FROM_DEVICE);\n\t\tret = dma_mapping_error(sdcp->dev, digest_phys);\n\t\tif (ret)\n\t\t\tgoto done_run;\n\n\t\tdesc->control0 |= MXS_DCP_CONTROL0_HASH_TERM;\n\t\tdesc->payload = digest_phys;\n\t}\n\n\tret = mxs_dcp_start_dma(actx);\n\n\tif (rctx->fini)\n\t\tdma_unmap_single(sdcp->dev, digest_phys, DCP_SHA_PAY_SZ,\n\t\t\t\t DMA_FROM_DEVICE);\n\ndone_run:\n\tdma_unmap_single(sdcp->dev, buf_phys, DCP_BUF_SZ, DMA_TO_DEVICE);\n\n\treturn ret;\n}\n\nstatic int dcp_sha_req_to_buf(struct crypto_async_request *arq)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\n\tstruct ahash_request *req = ahash_request_cast(arq);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx = crypto_ahash_ctx(tfm);\n\tstruct dcp_sha_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct hash_alg_common *halg = crypto_hash_alg_common(tfm);\n\n\tuint8_t *in_buf = sdcp->coh->sha_in_buf;\n\tuint8_t *out_buf = sdcp->coh->sha_out_buf;\n\n\tstruct scatterlist *src;\n\n\tunsigned int i, len, clen, oft = 0;\n\tint ret;\n\n\tint fin = rctx->fini;\n\tif (fin)\n\t\trctx->fini = 0;\n\n\tsrc = req->src;\n\tlen = req->nbytes;\n\n\twhile (len) {\n\t\tif (actx->fill + len > DCP_BUF_SZ)\n\t\t\tclen = DCP_BUF_SZ - actx->fill;\n\t\telse\n\t\t\tclen = len;\n\n\t\tscatterwalk_map_and_copy(in_buf + actx->fill, src, oft, clen,\n\t\t\t\t\t 0);\n\n\t\tlen -= clen;\n\t\toft += clen;\n\t\tactx->fill += clen;\n\n\t\t \n\t\tif (len && actx->fill == DCP_BUF_SZ) {\n\t\t\tret = mxs_dcp_run_sha(req);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tactx->fill = 0;\n\t\t\trctx->init = 0;\n\t\t}\n\t}\n\n\tif (fin) {\n\t\trctx->fini = 1;\n\n\t\t \n\t\tif (!req->result)\n\t\t\treturn -EINVAL;\n\n\t\tret = mxs_dcp_run_sha(req);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tactx->fill = 0;\n\n\t\t \n\t\tfor (i = 0; i < halg->digestsize; i++)\n\t\t\treq->result[i] = out_buf[halg->digestsize - i - 1];\n\t}\n\n\treturn 0;\n}\n\nstatic int dcp_chan_thread_sha(void *data)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\tconst int chan = DCP_CHAN_HASH_SHA;\n\n\tstruct crypto_async_request *backlog;\n\tstruct crypto_async_request *arq;\n\tint ret;\n\n\twhile (!kthread_should_stop()) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_lock(&sdcp->lock[chan]);\n\t\tbacklog = crypto_get_backlog(&sdcp->queue[chan]);\n\t\tarq = crypto_dequeue_request(&sdcp->queue[chan]);\n\t\tspin_unlock(&sdcp->lock[chan]);\n\n\t\tif (!backlog && !arq) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tset_current_state(TASK_RUNNING);\n\n\t\tif (backlog)\n\t\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\t\tif (arq) {\n\t\t\tret = dcp_sha_req_to_buf(arq);\n\t\t\tcrypto_request_complete(arq, ret);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int dcp_sha_init(struct ahash_request *req)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx = crypto_ahash_ctx(tfm);\n\n\tstruct hash_alg_common *halg = crypto_hash_alg_common(tfm);\n\n\t \n\tmemset(actx, 0, sizeof(*actx));\n\n\tif (strcmp(halg->base.cra_name, \"sha1\") == 0)\n\t\tactx->alg = MXS_DCP_CONTROL1_HASH_SELECT_SHA1;\n\telse\n\t\tactx->alg = MXS_DCP_CONTROL1_HASH_SELECT_SHA256;\n\n\tactx->fill = 0;\n\tactx->hot = 0;\n\tactx->chan = DCP_CHAN_HASH_SHA;\n\n\tmutex_init(&actx->mutex);\n\n\treturn 0;\n}\n\nstatic int dcp_sha_update_fx(struct ahash_request *req, int fini)\n{\n\tstruct dcp *sdcp = global_sdcp;\n\n\tstruct dcp_sha_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx = crypto_ahash_ctx(tfm);\n\n\tint ret;\n\n\t \n\tif (!req->nbytes && !fini)\n\t\treturn 0;\n\n\tmutex_lock(&actx->mutex);\n\n\trctx->fini = fini;\n\n\tif (!actx->hot) {\n\t\tactx->hot = 1;\n\t\trctx->init = 1;\n\t}\n\n\tspin_lock(&sdcp->lock[actx->chan]);\n\tret = crypto_enqueue_request(&sdcp->queue[actx->chan], &req->base);\n\tspin_unlock(&sdcp->lock[actx->chan]);\n\n\twake_up_process(sdcp->thread[actx->chan]);\n\tmutex_unlock(&actx->mutex);\n\n\treturn ret;\n}\n\nstatic int dcp_sha_update(struct ahash_request *req)\n{\n\treturn dcp_sha_update_fx(req, 0);\n}\n\nstatic int dcp_sha_final(struct ahash_request *req)\n{\n\tahash_request_set_crypt(req, NULL, req->result, 0);\n\treq->nbytes = 0;\n\treturn dcp_sha_update_fx(req, 1);\n}\n\nstatic int dcp_sha_finup(struct ahash_request *req)\n{\n\treturn dcp_sha_update_fx(req, 1);\n}\n\nstatic int dcp_sha_digest(struct ahash_request *req)\n{\n\tint ret;\n\n\tret = dcp_sha_init(req);\n\tif (ret)\n\t\treturn ret;\n\n\treturn dcp_sha_finup(req);\n}\n\nstatic int dcp_sha_import(struct ahash_request *req, const void *in)\n{\n\tstruct dcp_sha_req_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx = crypto_ahash_ctx(tfm);\n\tconst struct dcp_export_state *export = in;\n\n\tmemset(rctx, 0, sizeof(struct dcp_sha_req_ctx));\n\tmemset(actx, 0, sizeof(struct dcp_async_ctx));\n\tmemcpy(rctx, &export->req_ctx, sizeof(struct dcp_sha_req_ctx));\n\tmemcpy(actx, &export->async_ctx, sizeof(struct dcp_async_ctx));\n\n\treturn 0;\n}\n\nstatic int dcp_sha_export(struct ahash_request *req, void *out)\n{\n\tstruct dcp_sha_req_ctx *rctx_state = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct dcp_async_ctx *actx_state = crypto_ahash_ctx(tfm);\n\tstruct dcp_export_state *export = out;\n\n\tmemcpy(&export->req_ctx, rctx_state, sizeof(struct dcp_sha_req_ctx));\n\tmemcpy(&export->async_ctx, actx_state, sizeof(struct dcp_async_ctx));\n\n\treturn 0;\n}\n\nstatic int dcp_sha_cra_init(struct crypto_tfm *tfm)\n{\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct dcp_sha_req_ctx));\n\treturn 0;\n}\n\nstatic void dcp_sha_cra_exit(struct crypto_tfm *tfm)\n{\n}\n\n \nstatic struct skcipher_alg dcp_aes_algs[] = {\n\t{\n\t\t.base.cra_name\t\t= \"ecb(aes)\",\n\t\t.base.cra_driver_name\t= \"ecb-aes-dcp\",\n\t\t.base.cra_priority\t= 400,\n\t\t.base.cra_alignmask\t= 15,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t\t.base.cra_ctxsize\t= sizeof(struct dcp_async_ctx),\n\t\t.base.cra_module\t= THIS_MODULE,\n\n\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t.setkey\t\t\t= mxs_dcp_aes_setkey,\n\t\t.encrypt\t\t= mxs_dcp_aes_ecb_encrypt,\n\t\t.decrypt\t\t= mxs_dcp_aes_ecb_decrypt,\n\t\t.init\t\t\t= mxs_dcp_aes_fallback_init_tfm,\n\t\t.exit\t\t\t= mxs_dcp_aes_fallback_exit_tfm,\n\t}, {\n\t\t.base.cra_name\t\t= \"cbc(aes)\",\n\t\t.base.cra_driver_name\t= \"cbc-aes-dcp\",\n\t\t.base.cra_priority\t= 400,\n\t\t.base.cra_alignmask\t= 15,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_ASYNC |\n\t\t\t\t\t  CRYPTO_ALG_NEED_FALLBACK,\n\t\t.base.cra_blocksize\t= AES_BLOCK_SIZE,\n\t\t.base.cra_ctxsize\t= sizeof(struct dcp_async_ctx),\n\t\t.base.cra_module\t= THIS_MODULE,\n\n\t\t.min_keysize\t\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= AES_MAX_KEY_SIZE,\n\t\t.setkey\t\t\t= mxs_dcp_aes_setkey,\n\t\t.encrypt\t\t= mxs_dcp_aes_cbc_encrypt,\n\t\t.decrypt\t\t= mxs_dcp_aes_cbc_decrypt,\n\t\t.ivsize\t\t\t= AES_BLOCK_SIZE,\n\t\t.init\t\t\t= mxs_dcp_aes_fallback_init_tfm,\n\t\t.exit\t\t\t= mxs_dcp_aes_fallback_exit_tfm,\n\t},\n};\n\n \nstatic struct ahash_alg dcp_sha1_alg = {\n\t.init\t= dcp_sha_init,\n\t.update\t= dcp_sha_update,\n\t.final\t= dcp_sha_final,\n\t.finup\t= dcp_sha_finup,\n\t.digest\t= dcp_sha_digest,\n\t.import = dcp_sha_import,\n\t.export = dcp_sha_export,\n\t.halg\t= {\n\t\t.digestsize\t= SHA1_DIGEST_SIZE,\n\t\t.statesize\t= sizeof(struct dcp_export_state),\n\t\t.base\t\t= {\n\t\t\t.cra_name\t\t= \"sha1\",\n\t\t\t.cra_driver_name\t= \"sha1-dcp\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_alignmask\t\t= 63,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_ASYNC,\n\t\t\t.cra_blocksize\t\t= SHA1_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct dcp_async_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t.cra_init\t\t= dcp_sha_cra_init,\n\t\t\t.cra_exit\t\t= dcp_sha_cra_exit,\n\t\t},\n\t},\n};\n\n \nstatic struct ahash_alg dcp_sha256_alg = {\n\t.init\t= dcp_sha_init,\n\t.update\t= dcp_sha_update,\n\t.final\t= dcp_sha_final,\n\t.finup\t= dcp_sha_finup,\n\t.digest\t= dcp_sha_digest,\n\t.import = dcp_sha_import,\n\t.export = dcp_sha_export,\n\t.halg\t= {\n\t\t.digestsize\t= SHA256_DIGEST_SIZE,\n\t\t.statesize\t= sizeof(struct dcp_export_state),\n\t\t.base\t\t= {\n\t\t\t.cra_name\t\t= \"sha256\",\n\t\t\t.cra_driver_name\t= \"sha256-dcp\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_alignmask\t\t= 63,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_ASYNC,\n\t\t\t.cra_blocksize\t\t= SHA256_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct dcp_async_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t\t.cra_init\t\t= dcp_sha_cra_init,\n\t\t\t.cra_exit\t\t= dcp_sha_cra_exit,\n\t\t},\n\t},\n};\n\nstatic irqreturn_t mxs_dcp_irq(int irq, void *context)\n{\n\tstruct dcp *sdcp = context;\n\tuint32_t stat;\n\tint i;\n\n\tstat = readl(sdcp->base + MXS_DCP_STAT);\n\tstat &= MXS_DCP_STAT_IRQ_MASK;\n\tif (!stat)\n\t\treturn IRQ_NONE;\n\n\t \n\twritel(stat, sdcp->base + MXS_DCP_STAT_CLR);\n\n\t \n\tfor (i = 0; i < DCP_MAX_CHANS; i++)\n\t\tif (stat & (1 << i))\n\t\t\tcomplete(&sdcp->completion[i]);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int mxs_dcp_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct dcp *sdcp = NULL;\n\tint i, ret;\n\tint dcp_vmi_irq, dcp_irq;\n\n\tif (global_sdcp) {\n\t\tdev_err(dev, \"Only one DCP instance allowed!\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tdcp_vmi_irq = platform_get_irq(pdev, 0);\n\tif (dcp_vmi_irq < 0)\n\t\treturn dcp_vmi_irq;\n\n\tdcp_irq = platform_get_irq(pdev, 1);\n\tif (dcp_irq < 0)\n\t\treturn dcp_irq;\n\n\tsdcp = devm_kzalloc(dev, sizeof(*sdcp), GFP_KERNEL);\n\tif (!sdcp)\n\t\treturn -ENOMEM;\n\n\tsdcp->dev = dev;\n\tsdcp->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(sdcp->base))\n\t\treturn PTR_ERR(sdcp->base);\n\n\n\tret = devm_request_irq(dev, dcp_vmi_irq, mxs_dcp_irq, 0,\n\t\t\t       \"dcp-vmi-irq\", sdcp);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to claim DCP VMI IRQ!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = devm_request_irq(dev, dcp_irq, mxs_dcp_irq, 0,\n\t\t\t       \"dcp-irq\", sdcp);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to claim DCP IRQ!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tsdcp->coh = devm_kzalloc(dev, sizeof(*sdcp->coh) + DCP_ALIGNMENT,\n\t\t\t\t   GFP_KERNEL);\n\tif (!sdcp->coh)\n\t\treturn -ENOMEM;\n\n\t \n\tsdcp->coh = PTR_ALIGN(sdcp->coh, DCP_ALIGNMENT);\n\n\t \n\tsdcp->dcp_clk = devm_clk_get_optional_enabled(dev, \"dcp\");\n\tif (IS_ERR(sdcp->dcp_clk))\n\t\treturn PTR_ERR(sdcp->dcp_clk);\n\n\t \n\tret = stmp_reset_block(sdcp->base);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed reset\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\twritel(MXS_DCP_CTRL_GATHER_RESIDUAL_WRITES |\n\t       MXS_DCP_CTRL_ENABLE_CONTEXT_CACHING | 0xf,\n\t       sdcp->base + MXS_DCP_CTRL);\n\n\t \n\twritel(MXS_DCP_CHANNELCTRL_ENABLE_CHANNEL_MASK,\n\t       sdcp->base + MXS_DCP_CHANNELCTRL);\n\n\t \n\twritel(0xffff0000, sdcp->base + MXS_DCP_CONTEXT);\n\tfor (i = 0; i < DCP_MAX_CHANS; i++)\n\t\twritel(0xffffffff, sdcp->base + MXS_DCP_CH_N_STAT_CLR(i));\n\twritel(0xffffffff, sdcp->base + MXS_DCP_STAT_CLR);\n\n\tglobal_sdcp = sdcp;\n\n\tplatform_set_drvdata(pdev, sdcp);\n\n\tfor (i = 0; i < DCP_MAX_CHANS; i++) {\n\t\tspin_lock_init(&sdcp->lock[i]);\n\t\tinit_completion(&sdcp->completion[i]);\n\t\tcrypto_init_queue(&sdcp->queue[i], 50);\n\t}\n\n\t \n\tsdcp->thread[DCP_CHAN_HASH_SHA] = kthread_run(dcp_chan_thread_sha,\n\t\t\t\t\t\t      NULL, \"mxs_dcp_chan/sha\");\n\tif (IS_ERR(sdcp->thread[DCP_CHAN_HASH_SHA])) {\n\t\tdev_err(dev, \"Error starting SHA thread!\\n\");\n\t\tret = PTR_ERR(sdcp->thread[DCP_CHAN_HASH_SHA]);\n\t\treturn ret;\n\t}\n\n\tsdcp->thread[DCP_CHAN_CRYPTO] = kthread_run(dcp_chan_thread_aes,\n\t\t\t\t\t\t    NULL, \"mxs_dcp_chan/aes\");\n\tif (IS_ERR(sdcp->thread[DCP_CHAN_CRYPTO])) {\n\t\tdev_err(dev, \"Error starting SHA thread!\\n\");\n\t\tret = PTR_ERR(sdcp->thread[DCP_CHAN_CRYPTO]);\n\t\tgoto err_destroy_sha_thread;\n\t}\n\n\t \n\tsdcp->caps = readl(sdcp->base + MXS_DCP_CAPABILITY1);\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_AES128) {\n\t\tret = crypto_register_skciphers(dcp_aes_algs,\n\t\t\t\t\t\tARRAY_SIZE(dcp_aes_algs));\n\t\tif (ret) {\n\t\t\t \n\t\t\tdev_err(dev, \"Failed to register AES crypto!\\n\");\n\t\t\tgoto err_destroy_aes_thread;\n\t\t}\n\t}\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_SHA1) {\n\t\tret = crypto_register_ahash(&dcp_sha1_alg);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Failed to register %s hash!\\n\",\n\t\t\t\tdcp_sha1_alg.halg.base.cra_name);\n\t\t\tgoto err_unregister_aes;\n\t\t}\n\t}\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_SHA256) {\n\t\tret = crypto_register_ahash(&dcp_sha256_alg);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Failed to register %s hash!\\n\",\n\t\t\t\tdcp_sha256_alg.halg.base.cra_name);\n\t\t\tgoto err_unregister_sha1;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_unregister_sha1:\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_SHA1)\n\t\tcrypto_unregister_ahash(&dcp_sha1_alg);\n\nerr_unregister_aes:\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_AES128)\n\t\tcrypto_unregister_skciphers(dcp_aes_algs, ARRAY_SIZE(dcp_aes_algs));\n\nerr_destroy_aes_thread:\n\tkthread_stop(sdcp->thread[DCP_CHAN_CRYPTO]);\n\nerr_destroy_sha_thread:\n\tkthread_stop(sdcp->thread[DCP_CHAN_HASH_SHA]);\n\n\treturn ret;\n}\n\nstatic int mxs_dcp_remove(struct platform_device *pdev)\n{\n\tstruct dcp *sdcp = platform_get_drvdata(pdev);\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_SHA256)\n\t\tcrypto_unregister_ahash(&dcp_sha256_alg);\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_SHA1)\n\t\tcrypto_unregister_ahash(&dcp_sha1_alg);\n\n\tif (sdcp->caps & MXS_DCP_CAPABILITY1_AES128)\n\t\tcrypto_unregister_skciphers(dcp_aes_algs, ARRAY_SIZE(dcp_aes_algs));\n\n\tkthread_stop(sdcp->thread[DCP_CHAN_HASH_SHA]);\n\tkthread_stop(sdcp->thread[DCP_CHAN_CRYPTO]);\n\n\tplatform_set_drvdata(pdev, NULL);\n\n\tglobal_sdcp = NULL;\n\n\treturn 0;\n}\n\nstatic const struct of_device_id mxs_dcp_dt_ids[] = {\n\t{ .compatible = \"fsl,imx23-dcp\", .data = NULL, },\n\t{ .compatible = \"fsl,imx28-dcp\", .data = NULL, },\n\t{   }\n};\n\nMODULE_DEVICE_TABLE(of, mxs_dcp_dt_ids);\n\nstatic struct platform_driver mxs_dcp_driver = {\n\t.probe\t= mxs_dcp_probe,\n\t.remove\t= mxs_dcp_remove,\n\t.driver\t= {\n\t\t.name\t\t= \"mxs-dcp\",\n\t\t.of_match_table\t= mxs_dcp_dt_ids,\n\t},\n};\n\nmodule_platform_driver(mxs_dcp_driver);\n\nMODULE_AUTHOR(\"Marek Vasut <marex@denx.de>\");\nMODULE_DESCRIPTION(\"Freescale MXS DCP Driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"platform:mxs-dcp\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}