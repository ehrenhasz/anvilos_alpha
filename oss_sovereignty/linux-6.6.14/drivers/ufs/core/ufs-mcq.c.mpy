{
  "module_name": "ufs-mcq.c",
  "hash_id": "6801db6570b529092fa5f1a1d194db32025cfdcfd10bfe2aefb20b88b306ddb1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ufs/core/ufs-mcq.c",
  "human_readable_source": "\n \n\n#include <asm/unaligned.h>\n#include <linux/dma-mapping.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include \"ufshcd-priv.h\"\n#include <linux/delay.h>\n#include <scsi/scsi_cmnd.h>\n#include <linux/bitfield.h>\n#include <linux/iopoll.h>\n\n#define MAX_QUEUE_SUP GENMASK(7, 0)\n#define UFS_MCQ_MIN_RW_QUEUES 2\n#define UFS_MCQ_MIN_READ_QUEUES 0\n#define UFS_MCQ_MIN_POLL_QUEUES 0\n#define QUEUE_EN_OFFSET 31\n#define QUEUE_ID_OFFSET 16\n\n#define MCQ_CFG_MAC_MASK\tGENMASK(16, 8)\n#define MCQ_QCFG_SIZE\t\t0x40\n#define MCQ_ENTRY_SIZE_IN_DWORD\t8\n#define CQE_UCD_BA GENMASK_ULL(63, 7)\n\n \n#define MCQ_POLL_US 500000\n\nstatic int rw_queue_count_set(const char *val, const struct kernel_param *kp)\n{\n\treturn param_set_uint_minmax(val, kp, UFS_MCQ_MIN_RW_QUEUES,\n\t\t\t\t     num_possible_cpus());\n}\n\nstatic const struct kernel_param_ops rw_queue_count_ops = {\n\t.set = rw_queue_count_set,\n\t.get = param_get_uint,\n};\n\nstatic unsigned int rw_queues;\nmodule_param_cb(rw_queues, &rw_queue_count_ops, &rw_queues, 0644);\nMODULE_PARM_DESC(rw_queues,\n\t\t \"Number of interrupt driven I/O queues used for rw. Default value is nr_cpus\");\n\nstatic int read_queue_count_set(const char *val, const struct kernel_param *kp)\n{\n\treturn param_set_uint_minmax(val, kp, UFS_MCQ_MIN_READ_QUEUES,\n\t\t\t\t     num_possible_cpus());\n}\n\nstatic const struct kernel_param_ops read_queue_count_ops = {\n\t.set = read_queue_count_set,\n\t.get = param_get_uint,\n};\n\nstatic unsigned int read_queues;\nmodule_param_cb(read_queues, &read_queue_count_ops, &read_queues, 0644);\nMODULE_PARM_DESC(read_queues,\n\t\t \"Number of interrupt driven read queues used for read. Default value is 0\");\n\nstatic int poll_queue_count_set(const char *val, const struct kernel_param *kp)\n{\n\treturn param_set_uint_minmax(val, kp, UFS_MCQ_MIN_POLL_QUEUES,\n\t\t\t\t     num_possible_cpus());\n}\n\nstatic const struct kernel_param_ops poll_queue_count_ops = {\n\t.set = poll_queue_count_set,\n\t.get = param_get_uint,\n};\n\nstatic unsigned int poll_queues = 1;\nmodule_param_cb(poll_queues, &poll_queue_count_ops, &poll_queues, 0644);\nMODULE_PARM_DESC(poll_queues,\n\t\t \"Number of poll queues used for r/w. Default value is 1\");\n\n \nvoid ufshcd_mcq_config_mac(struct ufs_hba *hba, u32 max_active_cmds)\n{\n\tu32 val;\n\n\tval = ufshcd_readl(hba, REG_UFS_MCQ_CFG);\n\tval &= ~MCQ_CFG_MAC_MASK;\n\tval |= FIELD_PREP(MCQ_CFG_MAC_MASK, max_active_cmds);\n\tufshcd_writel(hba, val, REG_UFS_MCQ_CFG);\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_config_mac);\n\n \nstruct ufs_hw_queue *ufshcd_mcq_req_to_hwq(struct ufs_hba *hba,\n\t\t\t\t\t struct request *req)\n{\n\tu32 utag = blk_mq_unique_tag(req);\n\tu32 hwq = blk_mq_unique_tag_to_hwq(utag);\n\n\treturn &hba->uhq[hwq];\n}\n\n \nint ufshcd_mcq_decide_queue_depth(struct ufs_hba *hba)\n{\n\tint mac;\n\n\t \n\tmac = ufshcd_mcq_vops_get_hba_mac(hba);\n\tif (mac < 0) {\n\t\tdev_err(hba->dev, \"Failed to get mac, err=%d\\n\", mac);\n\t\treturn mac;\n\t}\n\n\tWARN_ON_ONCE(!hba->dev_info.bqueuedepth);\n\t \n\treturn min_t(int, mac, hba->dev_info.bqueuedepth);\n}\n\nstatic int ufshcd_mcq_config_nr_queues(struct ufs_hba *hba)\n{\n\tint i;\n\tu32 hba_maxq, rem, tot_queues;\n\tstruct Scsi_Host *host = hba->host;\n\n\t \n\thba_maxq = FIELD_GET(MAX_QUEUE_SUP, hba->mcq_capabilities) + 1;\n\n\ttot_queues = read_queues + poll_queues + rw_queues;\n\n\tif (hba_maxq < tot_queues) {\n\t\tdev_err(hba->dev, \"Total queues (%d) exceeds HC capacity (%d)\\n\",\n\t\t\ttot_queues, hba_maxq);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\trem = hba_maxq;\n\n\tif (rw_queues) {\n\t\thba->nr_queues[HCTX_TYPE_DEFAULT] = rw_queues;\n\t\trem -= hba->nr_queues[HCTX_TYPE_DEFAULT];\n\t} else {\n\t\trw_queues = num_possible_cpus();\n\t}\n\n\tif (poll_queues) {\n\t\thba->nr_queues[HCTX_TYPE_POLL] = poll_queues;\n\t\trem -= hba->nr_queues[HCTX_TYPE_POLL];\n\t}\n\n\tif (read_queues) {\n\t\thba->nr_queues[HCTX_TYPE_READ] = read_queues;\n\t\trem -= hba->nr_queues[HCTX_TYPE_READ];\n\t}\n\n\tif (!hba->nr_queues[HCTX_TYPE_DEFAULT])\n\t\thba->nr_queues[HCTX_TYPE_DEFAULT] = min3(rem, rw_queues,\n\t\t\t\t\t\t\t num_possible_cpus());\n\n\tfor (i = 0; i < HCTX_MAX_TYPES; i++)\n\t\thost->nr_hw_queues += hba->nr_queues[i];\n\n\thba->nr_hw_queues = host->nr_hw_queues;\n\treturn 0;\n}\n\nint ufshcd_mcq_memory_alloc(struct ufs_hba *hba)\n{\n\tstruct ufs_hw_queue *hwq;\n\tsize_t utrdl_size, cqe_size;\n\tint i;\n\n\tfor (i = 0; i < hba->nr_hw_queues; i++) {\n\t\thwq = &hba->uhq[i];\n\n\t\tutrdl_size = sizeof(struct utp_transfer_req_desc) *\n\t\t\t     hwq->max_entries;\n\t\thwq->sqe_base_addr = dmam_alloc_coherent(hba->dev, utrdl_size,\n\t\t\t\t\t\t\t &hwq->sqe_dma_addr,\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!hwq->sqe_dma_addr) {\n\t\t\tdev_err(hba->dev, \"SQE allocation failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tcqe_size = sizeof(struct cq_entry) * hwq->max_entries;\n\t\thwq->cqe_base_addr = dmam_alloc_coherent(hba->dev, cqe_size,\n\t\t\t\t\t\t\t &hwq->cqe_dma_addr,\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!hwq->cqe_dma_addr) {\n\t\t\tdev_err(hba->dev, \"CQE allocation failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n\n \n#define MCQ_CFG_n(r, i)\t((r) + MCQ_QCFG_SIZE * (i))\n#define MCQ_OPR_OFFSET_n(p, i) \\\n\t(hba->mcq_opr[(p)].offset + hba->mcq_opr[(p)].stride * (i))\n\nstatic void __iomem *mcq_opr_base(struct ufs_hba *hba,\n\t\t\t\t\t enum ufshcd_mcq_opr n, int i)\n{\n\tstruct ufshcd_mcq_opr_info_t *opr = &hba->mcq_opr[n];\n\n\treturn opr->base + opr->stride * i;\n}\n\nu32 ufshcd_mcq_read_cqis(struct ufs_hba *hba, int i)\n{\n\treturn readl(mcq_opr_base(hba, OPR_CQIS, i) + REG_CQIS);\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_read_cqis);\n\nvoid ufshcd_mcq_write_cqis(struct ufs_hba *hba, u32 val, int i)\n{\n\twritel(val, mcq_opr_base(hba, OPR_CQIS, i) + REG_CQIS);\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_write_cqis);\n\n \nstatic int ufshcd_mcq_get_tag(struct ufs_hba *hba,\n\t\t\t\t     struct ufs_hw_queue *hwq,\n\t\t\t\t     struct cq_entry *cqe)\n{\n\tu64 addr;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct utp_transfer_cmd_desc) & GENMASK(6, 0));\n\n\t \n\taddr = (le64_to_cpu(cqe->command_desc_base_addr) & CQE_UCD_BA) -\n\t\thba->ucdl_dma_addr;\n\n\treturn div_u64(addr, ufshcd_get_ucd_size(hba));\n}\n\nstatic void ufshcd_mcq_process_cqe(struct ufs_hba *hba,\n\t\t\t\t   struct ufs_hw_queue *hwq)\n{\n\tstruct cq_entry *cqe = ufshcd_mcq_cur_cqe(hwq);\n\tint tag = ufshcd_mcq_get_tag(hba, hwq, cqe);\n\n\tif (cqe->command_desc_base_addr) {\n\t\tufshcd_compl_one_cqe(hba, tag, cqe);\n\t\t \n\t\tcqe->command_desc_base_addr = 0;\n\t}\n}\n\nvoid ufshcd_mcq_compl_all_cqes_lock(struct ufs_hba *hba,\n\t\t\t\t    struct ufs_hw_queue *hwq)\n{\n\tunsigned long flags;\n\tu32 entries = hwq->max_entries;\n\n\tspin_lock_irqsave(&hwq->cq_lock, flags);\n\twhile (entries > 0) {\n\t\tufshcd_mcq_process_cqe(hba, hwq);\n\t\tufshcd_mcq_inc_cq_head_slot(hwq);\n\t\tentries--;\n\t}\n\n\tufshcd_mcq_update_cq_tail_slot(hwq);\n\thwq->cq_head_slot = hwq->cq_tail_slot;\n\tspin_unlock_irqrestore(&hwq->cq_lock, flags);\n}\n\nunsigned long ufshcd_mcq_poll_cqe_lock(struct ufs_hba *hba,\n\t\t\t\t       struct ufs_hw_queue *hwq)\n{\n\tunsigned long completed_reqs = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&hwq->cq_lock, flags);\n\tufshcd_mcq_update_cq_tail_slot(hwq);\n\twhile (!ufshcd_mcq_is_cq_empty(hwq)) {\n\t\tufshcd_mcq_process_cqe(hba, hwq);\n\t\tufshcd_mcq_inc_cq_head_slot(hwq);\n\t\tcompleted_reqs++;\n\t}\n\n\tif (completed_reqs)\n\t\tufshcd_mcq_update_cq_head(hwq);\n\tspin_unlock_irqrestore(&hwq->cq_lock, flags);\n\n\treturn completed_reqs;\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_poll_cqe_lock);\n\nvoid ufshcd_mcq_make_queues_operational(struct ufs_hba *hba)\n{\n\tstruct ufs_hw_queue *hwq;\n\tu16 qsize;\n\tint i;\n\n\tfor (i = 0; i < hba->nr_hw_queues; i++) {\n\t\thwq = &hba->uhq[i];\n\t\thwq->id = i;\n\t\tqsize = hwq->max_entries * MCQ_ENTRY_SIZE_IN_DWORD - 1;\n\n\t\t \n\t\tufsmcq_writelx(hba, lower_32_bits(hwq->sqe_dma_addr),\n\t\t\t      MCQ_CFG_n(REG_SQLBA, i));\n\t\t \n\t\tufsmcq_writelx(hba, upper_32_bits(hwq->sqe_dma_addr),\n\t\t\t      MCQ_CFG_n(REG_SQUBA, i));\n\t\t \n\t\tufsmcq_writelx(hba, MCQ_OPR_OFFSET_n(OPR_SQD, i),\n\t\t\t      MCQ_CFG_n(REG_SQDAO, i));\n\t\t \n\t\tufsmcq_writelx(hba, MCQ_OPR_OFFSET_n(OPR_SQIS, i),\n\t\t\t      MCQ_CFG_n(REG_SQISAO, i));\n\n\t\t \n\t\tufsmcq_writelx(hba, lower_32_bits(hwq->cqe_dma_addr),\n\t\t\t      MCQ_CFG_n(REG_CQLBA, i));\n\t\t \n\t\tufsmcq_writelx(hba, upper_32_bits(hwq->cqe_dma_addr),\n\t\t\t      MCQ_CFG_n(REG_CQUBA, i));\n\t\t \n\t\tufsmcq_writelx(hba, MCQ_OPR_OFFSET_n(OPR_CQD, i),\n\t\t\t      MCQ_CFG_n(REG_CQDAO, i));\n\t\t \n\t\tufsmcq_writelx(hba, MCQ_OPR_OFFSET_n(OPR_CQIS, i),\n\t\t\t      MCQ_CFG_n(REG_CQISAO, i));\n\n\t\t \n\t\thwq->mcq_sq_head = mcq_opr_base(hba, OPR_SQD, i) + REG_SQHP;\n\t\thwq->mcq_sq_tail = mcq_opr_base(hba, OPR_SQD, i) + REG_SQTP;\n\t\thwq->mcq_cq_head = mcq_opr_base(hba, OPR_CQD, i) + REG_CQHP;\n\t\thwq->mcq_cq_tail = mcq_opr_base(hba, OPR_CQD, i) + REG_CQTP;\n\n\t\t \n\t\thwq->sq_tail_slot = hwq->cq_tail_slot = hwq->cq_head_slot = 0;\n\n\t\t \n\t\tif (i < hba->nr_hw_queues - hba->nr_queues[HCTX_TYPE_POLL])\n\t\t\twritel(1, mcq_opr_base(hba, OPR_CQIS, i) + REG_CQIE);\n\n\t\t \n\t\tufsmcq_writel(hba, (1 << QUEUE_EN_OFFSET) | qsize,\n\t\t\t      MCQ_CFG_n(REG_CQATTR, i));\n\n\t\t \n\t\tufsmcq_writel(hba, (1 << QUEUE_EN_OFFSET) | qsize |\n\t\t\t      (i << QUEUE_ID_OFFSET),\n\t\t\t      MCQ_CFG_n(REG_SQATTR, i));\n\t}\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_make_queues_operational);\n\nvoid ufshcd_mcq_enable_esi(struct ufs_hba *hba)\n{\n\tufshcd_writel(hba, ufshcd_readl(hba, REG_UFS_MEM_CFG) | 0x2,\n\t\t      REG_UFS_MEM_CFG);\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_enable_esi);\n\nvoid ufshcd_mcq_config_esi(struct ufs_hba *hba, struct msi_msg *msg)\n{\n\tufshcd_writel(hba, msg->address_lo, REG_UFS_ESILBA);\n\tufshcd_writel(hba, msg->address_hi, REG_UFS_ESIUBA);\n}\nEXPORT_SYMBOL_GPL(ufshcd_mcq_config_esi);\n\nint ufshcd_mcq_init(struct ufs_hba *hba)\n{\n\tstruct Scsi_Host *host = hba->host;\n\tstruct ufs_hw_queue *hwq;\n\tint ret, i;\n\n\tret = ufshcd_mcq_config_nr_queues(hba);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ufshcd_vops_mcq_config_resource(hba);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ufshcd_mcq_vops_op_runtime_config(hba);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"Operation runtime config failed, ret=%d\\n\",\n\t\t\tret);\n\t\treturn ret;\n\t}\n\thba->uhq = devm_kzalloc(hba->dev,\n\t\t\t\thba->nr_hw_queues * sizeof(struct ufs_hw_queue),\n\t\t\t\tGFP_KERNEL);\n\tif (!hba->uhq) {\n\t\tdev_err(hba->dev, \"ufs hw queue memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < hba->nr_hw_queues; i++) {\n\t\thwq = &hba->uhq[i];\n\t\thwq->max_entries = hba->nutrs + 1;\n\t\tspin_lock_init(&hwq->sq_lock);\n\t\tspin_lock_init(&hwq->cq_lock);\n\t\tmutex_init(&hwq->sq_mutex);\n\t}\n\n\t \n\thba->dev_cmd_queue = &hba->uhq[0];\n\n\thost->host_tagset = 1;\n\treturn 0;\n}\n\nstatic int ufshcd_mcq_sq_stop(struct ufs_hba *hba, struct ufs_hw_queue *hwq)\n{\n\tvoid __iomem *reg;\n\tu32 id = hwq->id, val;\n\tint err;\n\n\tif (hba->quirks & UFSHCD_QUIRK_MCQ_BROKEN_RTC)\n\t\treturn -ETIMEDOUT;\n\n\twritel(SQ_STOP, mcq_opr_base(hba, OPR_SQD, id) + REG_SQRTC);\n\treg = mcq_opr_base(hba, OPR_SQD, id) + REG_SQRTS;\n\terr = read_poll_timeout(readl, val, val & SQ_STS, 20,\n\t\t\t\tMCQ_POLL_US, false, reg);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed. hwq-id=%d, err=%d\\n\",\n\t\t\t__func__, id, err);\n\treturn err;\n}\n\nstatic int ufshcd_mcq_sq_start(struct ufs_hba *hba, struct ufs_hw_queue *hwq)\n{\n\tvoid __iomem *reg;\n\tu32 id = hwq->id, val;\n\tint err;\n\n\tif (hba->quirks & UFSHCD_QUIRK_MCQ_BROKEN_RTC)\n\t\treturn -ETIMEDOUT;\n\n\twritel(SQ_START, mcq_opr_base(hba, OPR_SQD, id) + REG_SQRTC);\n\treg = mcq_opr_base(hba, OPR_SQD, id) + REG_SQRTS;\n\terr = read_poll_timeout(readl, val, !(val & SQ_STS), 20,\n\t\t\t\tMCQ_POLL_US, false, reg);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed. hwq-id=%d, err=%d\\n\",\n\t\t\t__func__, id, err);\n\treturn err;\n}\n\n \nint ufshcd_mcq_sq_cleanup(struct ufs_hba *hba, int task_tag)\n{\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[task_tag];\n\tstruct scsi_cmnd *cmd = lrbp->cmd;\n\tstruct ufs_hw_queue *hwq;\n\tvoid __iomem *reg, *opr_sqd_base;\n\tu32 nexus, id, val;\n\tint err;\n\n\tif (hba->quirks & UFSHCD_QUIRK_MCQ_BROKEN_RTC)\n\t\treturn -ETIMEDOUT;\n\n\tif (task_tag != hba->nutrs - UFSHCD_NUM_RESERVED) {\n\t\tif (!cmd)\n\t\t\treturn -EINVAL;\n\t\thwq = ufshcd_mcq_req_to_hwq(hba, scsi_cmd_to_rq(cmd));\n\t} else {\n\t\thwq = hba->dev_cmd_queue;\n\t}\n\n\tid = hwq->id;\n\n\tmutex_lock(&hwq->sq_mutex);\n\n\t \n\terr = ufshcd_mcq_sq_stop(hba, hwq);\n\tif (err)\n\t\tgoto unlock;\n\n\t \n\tnexus = lrbp->lun << 8 | task_tag;\n\topr_sqd_base = mcq_opr_base(hba, OPR_SQD, id);\n\twritel(nexus, opr_sqd_base + REG_SQCTI);\n\n\t \n\twritel(SQ_ICU, opr_sqd_base + REG_SQRTC);\n\n\t \n\treg = opr_sqd_base + REG_SQRTS;\n\terr = read_poll_timeout(readl, val, val & SQ_CUS, 20,\n\t\t\t\tMCQ_POLL_US, false, reg);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed. hwq=%d, tag=%d err=%ld\\n\",\n\t\t\t__func__, id, task_tag,\n\t\t\tFIELD_GET(SQ_ICU_ERR_CODE_MASK, readl(reg)));\n\n\tif (ufshcd_mcq_sq_start(hba, hwq))\n\t\terr = -ETIMEDOUT;\n\nunlock:\n\tmutex_unlock(&hwq->sq_mutex);\n\treturn err;\n}\n\n \nstatic void ufshcd_mcq_nullify_sqe(struct utp_transfer_req_desc *utrd)\n{\n\tutrd->header.command_type = 0xf;\n}\n\n \nstatic bool ufshcd_mcq_sqe_search(struct ufs_hba *hba,\n\t\t\t\t  struct ufs_hw_queue *hwq, int task_tag)\n{\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[task_tag];\n\tstruct utp_transfer_req_desc *utrd;\n\t__le64  cmd_desc_base_addr;\n\tbool ret = false;\n\tu64 addr, match;\n\tu32 sq_head_slot;\n\n\tif (hba->quirks & UFSHCD_QUIRK_MCQ_BROKEN_RTC)\n\t\treturn true;\n\n\tmutex_lock(&hwq->sq_mutex);\n\n\tufshcd_mcq_sq_stop(hba, hwq);\n\tsq_head_slot = ufshcd_mcq_get_sq_head_slot(hwq);\n\tif (sq_head_slot == hwq->sq_tail_slot)\n\t\tgoto out;\n\n\tcmd_desc_base_addr = lrbp->utr_descriptor_ptr->command_desc_base_addr;\n\taddr = le64_to_cpu(cmd_desc_base_addr) & CQE_UCD_BA;\n\n\twhile (sq_head_slot != hwq->sq_tail_slot) {\n\t\tutrd = hwq->sqe_base_addr +\n\t\t\t\tsq_head_slot * sizeof(struct utp_transfer_req_desc);\n\t\tmatch = le64_to_cpu(utrd->command_desc_base_addr) & CQE_UCD_BA;\n\t\tif (addr == match) {\n\t\t\tufshcd_mcq_nullify_sqe(utrd);\n\t\t\tret = true;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsq_head_slot++;\n\t\tif (sq_head_slot == hwq->max_entries)\n\t\t\tsq_head_slot = 0;\n\t}\n\nout:\n\tufshcd_mcq_sq_start(hba, hwq);\n\tmutex_unlock(&hwq->sq_mutex);\n\treturn ret;\n}\n\n \nint ufshcd_mcq_abort(struct scsi_cmnd *cmd)\n{\n\tstruct Scsi_Host *host = cmd->device->host;\n\tstruct ufs_hba *hba = shost_priv(host);\n\tint tag = scsi_cmd_to_rq(cmd)->tag;\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[tag];\n\tstruct ufs_hw_queue *hwq;\n\tunsigned long flags;\n\tint err = FAILED;\n\n\tif (!ufshcd_cmd_inflight(lrbp->cmd)) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: skip abort. cmd at tag %d already completed.\\n\",\n\t\t\t__func__, tag);\n\t\tgoto out;\n\t}\n\n\t \n\tif (lrbp->req_abort_skip) {\n\t\tdev_err(hba->dev, \"%s: skip abort. tag %d failed earlier\\n\",\n\t\t\t__func__, tag);\n\t\tgoto out;\n\t}\n\n\thwq = ufshcd_mcq_req_to_hwq(hba, scsi_cmd_to_rq(cmd));\n\n\tif (ufshcd_mcq_sqe_search(hba, hwq, tag)) {\n\t\t \n\t\tdev_err(hba->dev, \"%s: cmd found in sq. hwq=%d, tag=%d\\n\",\n\t\t\t__func__, hwq->id, tag);\n\t\tgoto out;\n\t}\n\n\t \n\tif (ufshcd_try_to_abort_task(hba, tag)) {\n\t\tdev_err(hba->dev, \"%s: device abort failed %d\\n\", __func__, err);\n\t\tlrbp->req_abort_skip = true;\n\t\tgoto out;\n\t}\n\n\terr = SUCCESS;\n\tspin_lock_irqsave(&hwq->cq_lock, flags);\n\tif (ufshcd_cmd_inflight(lrbp->cmd))\n\t\tufshcd_release_scsi_cmd(hba, lrbp);\n\tspin_unlock_irqrestore(&hwq->cq_lock, flags);\n\nout:\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}