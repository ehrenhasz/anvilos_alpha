{
  "module_name": "ufshcd.c",
  "hash_id": "23ffd7493743d6df80cfd0b40af8f53657d6efd8fbb5ec3e06b4d3206d315bd1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ufs/core/ufshcd.c",
  "human_readable_source": "\n \n\n#include <linux/async.h>\n#include <linux/devfreq.h>\n#include <linux/nls.h>\n#include <linux/of.h>\n#include <linux/bitfield.h>\n#include <linux/blk-pm.h>\n#include <linux/blkdev.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/regulator/consumer.h>\n#include <linux/sched/clock.h>\n#include <linux/iopoll.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_eh.h>\n#include \"ufshcd-priv.h\"\n#include <ufs/ufs_quirks.h>\n#include <ufs/unipro.h>\n#include \"ufs-sysfs.h\"\n#include \"ufs-debugfs.h\"\n#include \"ufs-fault-injection.h\"\n#include \"ufs_bsg.h\"\n#include \"ufshcd-crypto.h\"\n#include <asm/unaligned.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ufs.h>\n\n#define UFSHCD_ENABLE_INTRS\t(UTP_TRANSFER_REQ_COMPL |\\\n\t\t\t\t UTP_TASK_REQ_COMPL |\\\n\t\t\t\t UFSHCD_ERROR_MASK)\n\n#define UFSHCD_ENABLE_MCQ_INTRS\t(UTP_TASK_REQ_COMPL |\\\n\t\t\t\t UFSHCD_ERROR_MASK |\\\n\t\t\t\t MCQ_CQ_EVENT_STATUS)\n\n\n \n#define UIC_CMD_TIMEOUT\t500\n\n \n#define NOP_OUT_RETRIES    10\n \n#define NOP_OUT_TIMEOUT    50  \n\n \n#define QUERY_REQ_RETRIES 3\n \n#define QUERY_REQ_TIMEOUT 1500  \n\n \n#define ADVANCED_RPMB_REQ_TIMEOUT  3000  \n\n \n#define TM_CMD_TIMEOUT\t100  \n\n \n#define UFS_UIC_COMMAND_RETRIES 3\n\n \n#define DME_LINKSTARTUP_RETRIES 3\n\n \n#define MAX_HOST_RESET_RETRIES 5\n\n \n#define MAX_ERR_HANDLER_RETRIES 5\n\n \n#define MASK_QUERY_UPIU_FLAG_LOC 0xFF\n\n \n#define INT_AGGR_DEF_TO\t0x02\n\n \n#define RPM_AUTOSUSPEND_DELAY_MS 2000\n\n \n#define RPM_DEV_FLUSH_RECHECK_WORK_DELAY_MS 5000\n\n \n#define UFSHCD_REF_CLK_GATING_WAIT_US 0xFF  \n\n \n#define FDEVICEINIT_COMPL_TIMEOUT 1500  \n\n \nstatic bool use_mcq_mode = true;\n\nstatic bool is_mcq_supported(struct ufs_hba *hba)\n{\n\treturn hba->mcq_sup && use_mcq_mode;\n}\n\nmodule_param(use_mcq_mode, bool, 0644);\nMODULE_PARM_DESC(use_mcq_mode, \"Control MCQ mode for controllers starting from UFSHCI 4.0. 1 - enable MCQ, 0 - disable MCQ. MCQ is enabled by default\");\n\n#define ufshcd_toggle_vreg(_dev, _vreg, _on)\t\t\t\t\\\n\t({                                                              \\\n\t\tint _ret;                                               \\\n\t\tif (_on)                                                \\\n\t\t\t_ret = ufshcd_enable_vreg(_dev, _vreg);         \\\n\t\telse                                                    \\\n\t\t\t_ret = ufshcd_disable_vreg(_dev, _vreg);        \\\n\t\t_ret;                                                   \\\n\t})\n\n#define ufshcd_hex_dump(prefix_str, buf, len) do {                       \\\n\tsize_t __len = (len);                                            \\\n\tprint_hex_dump(KERN_ERR, prefix_str,                             \\\n\t\t       __len > 4 ? DUMP_PREFIX_OFFSET : DUMP_PREFIX_NONE,\\\n\t\t       16, 4, buf, __len, false);                        \\\n} while (0)\n\nint ufshcd_dump_regs(struct ufs_hba *hba, size_t offset, size_t len,\n\t\t     const char *prefix)\n{\n\tu32 *regs;\n\tsize_t pos;\n\n\tif (offset % 4 != 0 || len % 4 != 0)  \n\t\treturn -EINVAL;\n\n\tregs = kzalloc(len, GFP_ATOMIC);\n\tif (!regs)\n\t\treturn -ENOMEM;\n\n\tfor (pos = 0; pos < len; pos += 4) {\n\t\tif (offset == 0 &&\n\t\t    pos >= REG_UIC_ERROR_CODE_PHY_ADAPTER_LAYER &&\n\t\t    pos <= REG_UIC_ERROR_CODE_DME)\n\t\t\tcontinue;\n\t\tregs[pos / 4] = ufshcd_readl(hba, offset + pos);\n\t}\n\n\tufshcd_hex_dump(prefix, regs, len);\n\tkfree(regs);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ufshcd_dump_regs);\n\nenum {\n\tUFSHCD_MAX_CHANNEL\t= 0,\n\tUFSHCD_MAX_ID\t\t= 1,\n\tUFSHCD_CMD_PER_LUN\t= 32 - UFSHCD_NUM_RESERVED,\n\tUFSHCD_CAN_QUEUE\t= 32 - UFSHCD_NUM_RESERVED,\n};\n\nstatic const char *const ufshcd_state_name[] = {\n\t[UFSHCD_STATE_RESET]\t\t\t= \"reset\",\n\t[UFSHCD_STATE_OPERATIONAL]\t\t= \"operational\",\n\t[UFSHCD_STATE_ERROR]\t\t\t= \"error\",\n\t[UFSHCD_STATE_EH_SCHEDULED_FATAL]\t= \"eh_fatal\",\n\t[UFSHCD_STATE_EH_SCHEDULED_NON_FATAL]\t= \"eh_non_fatal\",\n};\n\n \nenum {\n\tUFSHCD_EH_IN_PROGRESS = (1 << 0),\n};\n\n \nenum {\n\tUFSHCD_UIC_DL_PA_INIT_ERROR = (1 << 0),  \n\tUFSHCD_UIC_DL_NAC_RECEIVED_ERROR = (1 << 1),  \n\tUFSHCD_UIC_DL_TCx_REPLAY_ERROR = (1 << 2),  \n\tUFSHCD_UIC_NL_ERROR = (1 << 3),  \n\tUFSHCD_UIC_TL_ERROR = (1 << 4),  \n\tUFSHCD_UIC_DME_ERROR = (1 << 5),  \n\tUFSHCD_UIC_PA_GENERIC_ERROR = (1 << 6),  \n};\n\n#define ufshcd_set_eh_in_progress(h) \\\n\t((h)->eh_flags |= UFSHCD_EH_IN_PROGRESS)\n#define ufshcd_eh_in_progress(h) \\\n\t((h)->eh_flags & UFSHCD_EH_IN_PROGRESS)\n#define ufshcd_clear_eh_in_progress(h) \\\n\t((h)->eh_flags &= ~UFSHCD_EH_IN_PROGRESS)\n\nconst struct ufs_pm_lvl_states ufs_pm_lvl_states[] = {\n\t[UFS_PM_LVL_0] = {UFS_ACTIVE_PWR_MODE, UIC_LINK_ACTIVE_STATE},\n\t[UFS_PM_LVL_1] = {UFS_ACTIVE_PWR_MODE, UIC_LINK_HIBERN8_STATE},\n\t[UFS_PM_LVL_2] = {UFS_SLEEP_PWR_MODE, UIC_LINK_ACTIVE_STATE},\n\t[UFS_PM_LVL_3] = {UFS_SLEEP_PWR_MODE, UIC_LINK_HIBERN8_STATE},\n\t[UFS_PM_LVL_4] = {UFS_POWERDOWN_PWR_MODE, UIC_LINK_HIBERN8_STATE},\n\t[UFS_PM_LVL_5] = {UFS_POWERDOWN_PWR_MODE, UIC_LINK_OFF_STATE},\n\t \n\t[UFS_PM_LVL_6] = {UFS_DEEPSLEEP_PWR_MODE, UIC_LINK_OFF_STATE},\n};\n\nstatic inline enum ufs_dev_pwr_mode\nufs_get_pm_lvl_to_dev_pwr_mode(enum ufs_pm_level lvl)\n{\n\treturn ufs_pm_lvl_states[lvl].dev_state;\n}\n\nstatic inline enum uic_link_state\nufs_get_pm_lvl_to_link_pwr_state(enum ufs_pm_level lvl)\n{\n\treturn ufs_pm_lvl_states[lvl].link_state;\n}\n\nstatic inline enum ufs_pm_level\nufs_get_desired_pm_lvl_for_dev_link_state(enum ufs_dev_pwr_mode dev_state,\n\t\t\t\t\tenum uic_link_state link_state)\n{\n\tenum ufs_pm_level lvl;\n\n\tfor (lvl = UFS_PM_LVL_0; lvl < UFS_PM_LVL_MAX; lvl++) {\n\t\tif ((ufs_pm_lvl_states[lvl].dev_state == dev_state) &&\n\t\t\t(ufs_pm_lvl_states[lvl].link_state == link_state))\n\t\t\treturn lvl;\n\t}\n\n\t \n\treturn UFS_PM_LVL_0;\n}\n\nstatic const struct ufs_dev_quirk ufs_fixups[] = {\n\t \n\t{ .wmanufacturerid = UFS_VENDOR_MICRON,\n\t  .model = UFS_ANY_MODEL,\n\t  .quirk = UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM },\n\t{ .wmanufacturerid = UFS_VENDOR_SAMSUNG,\n\t  .model = UFS_ANY_MODEL,\n\t  .quirk = UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM |\n\t\t   UFS_DEVICE_QUIRK_HOST_PA_TACTIVATE |\n\t\t   UFS_DEVICE_QUIRK_RECOVERY_FROM_DL_NAC_ERRORS },\n\t{ .wmanufacturerid = UFS_VENDOR_SKHYNIX,\n\t  .model = UFS_ANY_MODEL,\n\t  .quirk = UFS_DEVICE_QUIRK_HOST_PA_SAVECONFIGTIME },\n\t{ .wmanufacturerid = UFS_VENDOR_SKHYNIX,\n\t  .model = \"hB8aL1\"  ,\n\t  .quirk = UFS_DEVICE_QUIRK_HOST_VS_DEBUGSAVECONFIGTIME },\n\t{ .wmanufacturerid = UFS_VENDOR_TOSHIBA,\n\t  .model = UFS_ANY_MODEL,\n\t  .quirk = UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM },\n\t{ .wmanufacturerid = UFS_VENDOR_TOSHIBA,\n\t  .model = \"THGLF2G9C8KBADG\",\n\t  .quirk = UFS_DEVICE_QUIRK_PA_TACTIVATE },\n\t{ .wmanufacturerid = UFS_VENDOR_TOSHIBA,\n\t  .model = \"THGLF2G9D8KBADG\",\n\t  .quirk = UFS_DEVICE_QUIRK_PA_TACTIVATE },\n\t{}\n};\n\nstatic irqreturn_t ufshcd_tmc_handler(struct ufs_hba *hba);\nstatic void ufshcd_async_scan(void *data, async_cookie_t cookie);\nstatic int ufshcd_reset_and_restore(struct ufs_hba *hba);\nstatic int ufshcd_eh_host_reset_handler(struct scsi_cmnd *cmd);\nstatic int ufshcd_clear_tm_cmd(struct ufs_hba *hba, int tag);\nstatic void ufshcd_hba_exit(struct ufs_hba *hba);\nstatic int ufshcd_probe_hba(struct ufs_hba *hba, bool init_dev_params);\nstatic int ufshcd_setup_clocks(struct ufs_hba *hba, bool on);\nstatic inline void ufshcd_add_delay_before_dme_cmd(struct ufs_hba *hba);\nstatic int ufshcd_host_reset_and_restore(struct ufs_hba *hba);\nstatic void ufshcd_resume_clkscaling(struct ufs_hba *hba);\nstatic void ufshcd_suspend_clkscaling(struct ufs_hba *hba);\nstatic void __ufshcd_suspend_clkscaling(struct ufs_hba *hba);\nstatic int ufshcd_scale_clks(struct ufs_hba *hba, bool scale_up);\nstatic irqreturn_t ufshcd_intr(int irq, void *__hba);\nstatic int ufshcd_change_power_mode(struct ufs_hba *hba,\n\t\t\t     struct ufs_pa_layer_attr *pwr_mode);\nstatic int ufshcd_setup_hba_vreg(struct ufs_hba *hba, bool on);\nstatic int ufshcd_setup_vreg(struct ufs_hba *hba, bool on);\nstatic inline int ufshcd_config_vreg_hpm(struct ufs_hba *hba,\n\t\t\t\t\t struct ufs_vreg *vreg);\nstatic void ufshcd_wb_toggle_buf_flush_during_h8(struct ufs_hba *hba,\n\t\t\t\t\t\t bool enable);\nstatic void ufshcd_hba_vreg_set_lpm(struct ufs_hba *hba);\nstatic void ufshcd_hba_vreg_set_hpm(struct ufs_hba *hba);\n\nstatic inline void ufshcd_enable_irq(struct ufs_hba *hba)\n{\n\tif (!hba->is_irq_enabled) {\n\t\tenable_irq(hba->irq);\n\t\thba->is_irq_enabled = true;\n\t}\n}\n\nstatic inline void ufshcd_disable_irq(struct ufs_hba *hba)\n{\n\tif (hba->is_irq_enabled) {\n\t\tdisable_irq(hba->irq);\n\t\thba->is_irq_enabled = false;\n\t}\n}\n\nstatic void ufshcd_configure_wb(struct ufs_hba *hba)\n{\n\tif (!ufshcd_is_wb_allowed(hba))\n\t\treturn;\n\n\tufshcd_wb_toggle(hba, true);\n\n\tufshcd_wb_toggle_buf_flush_during_h8(hba, true);\n\n\tif (ufshcd_is_wb_buf_flush_allowed(hba))\n\t\tufshcd_wb_toggle_buf_flush(hba, true);\n}\n\nstatic void ufshcd_scsi_unblock_requests(struct ufs_hba *hba)\n{\n\tif (atomic_dec_and_test(&hba->scsi_block_reqs_cnt))\n\t\tscsi_unblock_requests(hba->host);\n}\n\nstatic void ufshcd_scsi_block_requests(struct ufs_hba *hba)\n{\n\tif (atomic_inc_return(&hba->scsi_block_reqs_cnt) == 1)\n\t\tscsi_block_requests(hba->host);\n}\n\nstatic void ufshcd_add_cmd_upiu_trace(struct ufs_hba *hba, unsigned int tag,\n\t\t\t\t      enum ufs_trace_str_t str_t)\n{\n\tstruct utp_upiu_req *rq = hba->lrb[tag].ucd_req_ptr;\n\tstruct utp_upiu_header *header;\n\n\tif (!trace_ufshcd_upiu_enabled())\n\t\treturn;\n\n\tif (str_t == UFS_CMD_SEND)\n\t\theader = &rq->header;\n\telse\n\t\theader = &hba->lrb[tag].ucd_rsp_ptr->header;\n\n\ttrace_ufshcd_upiu(dev_name(hba->dev), str_t, header, &rq->sc.cdb,\n\t\t\t  UFS_TSF_CDB);\n}\n\nstatic void ufshcd_add_query_upiu_trace(struct ufs_hba *hba,\n\t\t\t\t\tenum ufs_trace_str_t str_t,\n\t\t\t\t\tstruct utp_upiu_req *rq_rsp)\n{\n\tif (!trace_ufshcd_upiu_enabled())\n\t\treturn;\n\n\ttrace_ufshcd_upiu(dev_name(hba->dev), str_t, &rq_rsp->header,\n\t\t\t  &rq_rsp->qr, UFS_TSF_OSF);\n}\n\nstatic void ufshcd_add_tm_upiu_trace(struct ufs_hba *hba, unsigned int tag,\n\t\t\t\t     enum ufs_trace_str_t str_t)\n{\n\tstruct utp_task_req_desc *descp = &hba->utmrdl_base_addr[tag];\n\n\tif (!trace_ufshcd_upiu_enabled())\n\t\treturn;\n\n\tif (str_t == UFS_TM_SEND)\n\t\ttrace_ufshcd_upiu(dev_name(hba->dev), str_t,\n\t\t\t\t  &descp->upiu_req.req_header,\n\t\t\t\t  &descp->upiu_req.input_param1,\n\t\t\t\t  UFS_TSF_TM_INPUT);\n\telse\n\t\ttrace_ufshcd_upiu(dev_name(hba->dev), str_t,\n\t\t\t\t  &descp->upiu_rsp.rsp_header,\n\t\t\t\t  &descp->upiu_rsp.output_param1,\n\t\t\t\t  UFS_TSF_TM_OUTPUT);\n}\n\nstatic void ufshcd_add_uic_command_trace(struct ufs_hba *hba,\n\t\t\t\t\t const struct uic_command *ucmd,\n\t\t\t\t\t enum ufs_trace_str_t str_t)\n{\n\tu32 cmd;\n\n\tif (!trace_ufshcd_uic_command_enabled())\n\t\treturn;\n\n\tif (str_t == UFS_CMD_SEND)\n\t\tcmd = ucmd->command;\n\telse\n\t\tcmd = ufshcd_readl(hba, REG_UIC_COMMAND);\n\n\ttrace_ufshcd_uic_command(dev_name(hba->dev), str_t, cmd,\n\t\t\t\t ufshcd_readl(hba, REG_UIC_COMMAND_ARG_1),\n\t\t\t\t ufshcd_readl(hba, REG_UIC_COMMAND_ARG_2),\n\t\t\t\t ufshcd_readl(hba, REG_UIC_COMMAND_ARG_3));\n}\n\nstatic void ufshcd_add_command_trace(struct ufs_hba *hba, unsigned int tag,\n\t\t\t\t     enum ufs_trace_str_t str_t)\n{\n\tu64 lba = 0;\n\tu8 opcode = 0, group_id = 0;\n\tu32 doorbell = 0;\n\tu32 intr;\n\tint hwq_id = -1;\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[tag];\n\tstruct scsi_cmnd *cmd = lrbp->cmd;\n\tstruct request *rq = scsi_cmd_to_rq(cmd);\n\tint transfer_len = -1;\n\n\tif (!cmd)\n\t\treturn;\n\n\t \n\tufshcd_add_cmd_upiu_trace(hba, tag, str_t);\n\tif (!trace_ufshcd_command_enabled())\n\t\treturn;\n\n\topcode = cmd->cmnd[0];\n\n\tif (opcode == READ_10 || opcode == WRITE_10) {\n\t\t \n\t\ttransfer_len =\n\t\t       be32_to_cpu(lrbp->ucd_req_ptr->sc.exp_data_transfer_len);\n\t\tlba = scsi_get_lba(cmd);\n\t\tif (opcode == WRITE_10)\n\t\t\tgroup_id = lrbp->cmd->cmnd[6];\n\t} else if (opcode == UNMAP) {\n\t\t \n\t\ttransfer_len = blk_rq_bytes(rq);\n\t\tlba = scsi_get_lba(cmd);\n\t}\n\n\tintr = ufshcd_readl(hba, REG_INTERRUPT_STATUS);\n\n\tif (is_mcq_enabled(hba)) {\n\t\tstruct ufs_hw_queue *hwq = ufshcd_mcq_req_to_hwq(hba, rq);\n\n\t\thwq_id = hwq->id;\n\t} else {\n\t\tdoorbell = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\t}\n\ttrace_ufshcd_command(dev_name(hba->dev), str_t, tag,\n\t\t\tdoorbell, hwq_id, transfer_len, intr, lba, opcode, group_id);\n}\n\nstatic void ufshcd_print_clk_freqs(struct ufs_hba *hba)\n{\n\tstruct ufs_clk_info *clki;\n\tstruct list_head *head = &hba->clk_list_head;\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry(clki, head, list) {\n\t\tif (!IS_ERR_OR_NULL(clki->clk) && clki->min_freq &&\n\t\t\t\tclki->max_freq)\n\t\t\tdev_err(hba->dev, \"clk: %s, rate: %u\\n\",\n\t\t\t\t\tclki->name, clki->curr_freq);\n\t}\n}\n\nstatic void ufshcd_print_evt(struct ufs_hba *hba, u32 id,\n\t\t\t     const char *err_name)\n{\n\tint i;\n\tbool found = false;\n\tconst struct ufs_event_hist *e;\n\n\tif (id >= UFS_EVT_CNT)\n\t\treturn;\n\n\te = &hba->ufs_stats.event[id];\n\n\tfor (i = 0; i < UFS_EVENT_HIST_LENGTH; i++) {\n\t\tint p = (i + e->pos) % UFS_EVENT_HIST_LENGTH;\n\n\t\tif (e->tstamp[p] == 0)\n\t\t\tcontinue;\n\t\tdev_err(hba->dev, \"%s[%d] = 0x%x at %lld us\\n\", err_name, p,\n\t\t\te->val[p], div_u64(e->tstamp[p], 1000));\n\t\tfound = true;\n\t}\n\n\tif (!found)\n\t\tdev_err(hba->dev, \"No record of %s\\n\", err_name);\n\telse\n\t\tdev_err(hba->dev, \"%s: total cnt=%llu\\n\", err_name, e->cnt);\n}\n\nstatic void ufshcd_print_evt_hist(struct ufs_hba *hba)\n{\n\tufshcd_dump_regs(hba, 0, UFSHCI_REG_SPACE_SIZE, \"host_regs: \");\n\n\tufshcd_print_evt(hba, UFS_EVT_PA_ERR, \"pa_err\");\n\tufshcd_print_evt(hba, UFS_EVT_DL_ERR, \"dl_err\");\n\tufshcd_print_evt(hba, UFS_EVT_NL_ERR, \"nl_err\");\n\tufshcd_print_evt(hba, UFS_EVT_TL_ERR, \"tl_err\");\n\tufshcd_print_evt(hba, UFS_EVT_DME_ERR, \"dme_err\");\n\tufshcd_print_evt(hba, UFS_EVT_AUTO_HIBERN8_ERR,\n\t\t\t \"auto_hibern8_err\");\n\tufshcd_print_evt(hba, UFS_EVT_FATAL_ERR, \"fatal_err\");\n\tufshcd_print_evt(hba, UFS_EVT_LINK_STARTUP_FAIL,\n\t\t\t \"link_startup_fail\");\n\tufshcd_print_evt(hba, UFS_EVT_RESUME_ERR, \"resume_fail\");\n\tufshcd_print_evt(hba, UFS_EVT_SUSPEND_ERR,\n\t\t\t \"suspend_fail\");\n\tufshcd_print_evt(hba, UFS_EVT_WL_RES_ERR, \"wlun resume_fail\");\n\tufshcd_print_evt(hba, UFS_EVT_WL_SUSP_ERR,\n\t\t\t \"wlun suspend_fail\");\n\tufshcd_print_evt(hba, UFS_EVT_DEV_RESET, \"dev_reset\");\n\tufshcd_print_evt(hba, UFS_EVT_HOST_RESET, \"host_reset\");\n\tufshcd_print_evt(hba, UFS_EVT_ABORT, \"task_abort\");\n\n\tufshcd_vops_dbg_register_dump(hba);\n}\n\nstatic\nvoid ufshcd_print_tr(struct ufs_hba *hba, int tag, bool pr_prdt)\n{\n\tconst struct ufshcd_lrb *lrbp;\n\tint prdt_length;\n\n\tlrbp = &hba->lrb[tag];\n\n\tdev_err(hba->dev, \"UPIU[%d] - issue time %lld us\\n\",\n\t\t\ttag, div_u64(lrbp->issue_time_stamp_local_clock, 1000));\n\tdev_err(hba->dev, \"UPIU[%d] - complete time %lld us\\n\",\n\t\t\ttag, div_u64(lrbp->compl_time_stamp_local_clock, 1000));\n\tdev_err(hba->dev,\n\t\t\"UPIU[%d] - Transfer Request Descriptor phys@0x%llx\\n\",\n\t\ttag, (u64)lrbp->utrd_dma_addr);\n\n\tufshcd_hex_dump(\"UPIU TRD: \", lrbp->utr_descriptor_ptr,\n\t\t\tsizeof(struct utp_transfer_req_desc));\n\tdev_err(hba->dev, \"UPIU[%d] - Request UPIU phys@0x%llx\\n\", tag,\n\t\t(u64)lrbp->ucd_req_dma_addr);\n\tufshcd_hex_dump(\"UPIU REQ: \", lrbp->ucd_req_ptr,\n\t\t\tsizeof(struct utp_upiu_req));\n\tdev_err(hba->dev, \"UPIU[%d] - Response UPIU phys@0x%llx\\n\", tag,\n\t\t(u64)lrbp->ucd_rsp_dma_addr);\n\tufshcd_hex_dump(\"UPIU RSP: \", lrbp->ucd_rsp_ptr,\n\t\t\tsizeof(struct utp_upiu_rsp));\n\n\tprdt_length = le16_to_cpu(\n\t\tlrbp->utr_descriptor_ptr->prd_table_length);\n\tif (hba->quirks & UFSHCD_QUIRK_PRDT_BYTE_GRAN)\n\t\tprdt_length /= ufshcd_sg_entry_size(hba);\n\n\tdev_err(hba->dev,\n\t\t\"UPIU[%d] - PRDT - %d entries  phys@0x%llx\\n\",\n\t\ttag, prdt_length,\n\t\t(u64)lrbp->ucd_prdt_dma_addr);\n\n\tif (pr_prdt)\n\t\tufshcd_hex_dump(\"UPIU PRDT: \", lrbp->ucd_prdt_ptr,\n\t\t\tufshcd_sg_entry_size(hba) * prdt_length);\n}\n\nstatic bool ufshcd_print_tr_iter(struct request *req, void *priv)\n{\n\tstruct scsi_device *sdev = req->q->queuedata;\n\tstruct Scsi_Host *shost = sdev->host;\n\tstruct ufs_hba *hba = shost_priv(shost);\n\n\tufshcd_print_tr(hba, req->tag, *(bool *)priv);\n\n\treturn true;\n}\n\n \nstatic void ufshcd_print_trs_all(struct ufs_hba *hba, bool pr_prdt)\n{\n\tblk_mq_tagset_busy_iter(&hba->host->tag_set, ufshcd_print_tr_iter, &pr_prdt);\n}\n\nstatic void ufshcd_print_tmrs(struct ufs_hba *hba, unsigned long bitmap)\n{\n\tint tag;\n\n\tfor_each_set_bit(tag, &bitmap, hba->nutmrs) {\n\t\tstruct utp_task_req_desc *tmrdp = &hba->utmrdl_base_addr[tag];\n\n\t\tdev_err(hba->dev, \"TM[%d] - Task Management Header\\n\", tag);\n\t\tufshcd_hex_dump(\"\", tmrdp, sizeof(*tmrdp));\n\t}\n}\n\nstatic void ufshcd_print_host_state(struct ufs_hba *hba)\n{\n\tconst struct scsi_device *sdev_ufs = hba->ufs_device_wlun;\n\n\tdev_err(hba->dev, \"UFS Host state=%d\\n\", hba->ufshcd_state);\n\tdev_err(hba->dev, \"outstanding reqs=0x%lx tasks=0x%lx\\n\",\n\t\thba->outstanding_reqs, hba->outstanding_tasks);\n\tdev_err(hba->dev, \"saved_err=0x%x, saved_uic_err=0x%x\\n\",\n\t\thba->saved_err, hba->saved_uic_err);\n\tdev_err(hba->dev, \"Device power mode=%d, UIC link state=%d\\n\",\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\tdev_err(hba->dev, \"PM in progress=%d, sys. suspended=%d\\n\",\n\t\thba->pm_op_in_progress, hba->is_sys_suspended);\n\tdev_err(hba->dev, \"Auto BKOPS=%d, Host self-block=%d\\n\",\n\t\thba->auto_bkops_enabled, hba->host->host_self_blocked);\n\tdev_err(hba->dev, \"Clk gate=%d\\n\", hba->clk_gating.state);\n\tdev_err(hba->dev,\n\t\t\"last_hibern8_exit_tstamp at %lld us, hibern8_exit_cnt=%d\\n\",\n\t\tdiv_u64(hba->ufs_stats.last_hibern8_exit_tstamp, 1000),\n\t\thba->ufs_stats.hibern8_exit_cnt);\n\tdev_err(hba->dev, \"last intr at %lld us, last intr status=0x%x\\n\",\n\t\tdiv_u64(hba->ufs_stats.last_intr_ts, 1000),\n\t\thba->ufs_stats.last_intr_status);\n\tdev_err(hba->dev, \"error handling flags=0x%x, req. abort count=%d\\n\",\n\t\thba->eh_flags, hba->req_abort_count);\n\tdev_err(hba->dev, \"hba->ufs_version=0x%x, Host capabilities=0x%x, caps=0x%x\\n\",\n\t\thba->ufs_version, hba->capabilities, hba->caps);\n\tdev_err(hba->dev, \"quirks=0x%x, dev. quirks=0x%x\\n\", hba->quirks,\n\t\thba->dev_quirks);\n\tif (sdev_ufs)\n\t\tdev_err(hba->dev, \"UFS dev info: %.8s %.16s rev %.4s\\n\",\n\t\t\tsdev_ufs->vendor, sdev_ufs->model, sdev_ufs->rev);\n\n\tufshcd_print_clk_freqs(hba);\n}\n\n \nstatic void ufshcd_print_pwr_info(struct ufs_hba *hba)\n{\n\tstatic const char * const names[] = {\n\t\t\"INVALID MODE\",\n\t\t\"FAST MODE\",\n\t\t\"SLOW_MODE\",\n\t\t\"INVALID MODE\",\n\t\t\"FASTAUTO_MODE\",\n\t\t\"SLOWAUTO_MODE\",\n\t\t\"INVALID MODE\",\n\t};\n\n\t \n\tdev_dbg(hba->dev, \"%s:[RX, TX]: gear=[%d, %d], lane[%d, %d], pwr[%s, %s], rate = %d\\n\",\n\t\t __func__,\n\t\t hba->pwr_info.gear_rx, hba->pwr_info.gear_tx,\n\t\t hba->pwr_info.lane_rx, hba->pwr_info.lane_tx,\n\t\t names[hba->pwr_info.pwr_rx],\n\t\t names[hba->pwr_info.pwr_tx],\n\t\t hba->pwr_info.hs_rate);\n}\n\nstatic void ufshcd_device_reset(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufshcd_vops_device_reset(hba);\n\n\tif (!err) {\n\t\tufshcd_set_ufs_dev_active(hba);\n\t\tif (ufshcd_is_wb_allowed(hba)) {\n\t\t\thba->dev_info.wb_enabled = false;\n\t\t\thba->dev_info.wb_buf_flush_enabled = false;\n\t\t}\n\t}\n\tif (err != -EOPNOTSUPP)\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_DEV_RESET, err);\n}\n\nvoid ufshcd_delay_us(unsigned long us, unsigned long tolerance)\n{\n\tif (!us)\n\t\treturn;\n\n\tif (us < 10)\n\t\tudelay(us);\n\telse\n\t\tusleep_range(us, us + tolerance);\n}\nEXPORT_SYMBOL_GPL(ufshcd_delay_us);\n\n \nstatic int ufshcd_wait_for_register(struct ufs_hba *hba, u32 reg, u32 mask,\n\t\t\t\tu32 val, unsigned long interval_us,\n\t\t\t\tunsigned long timeout_ms)\n{\n\tint err = 0;\n\tunsigned long timeout = jiffies + msecs_to_jiffies(timeout_ms);\n\n\t \n\tval = val & mask;\n\n\twhile ((ufshcd_readl(hba, reg) & mask) != val) {\n\t\tusleep_range(interval_us, interval_us + 50);\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tif ((ufshcd_readl(hba, reg) & mask) != val)\n\t\t\t\terr = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nstatic inline u32 ufshcd_get_intr_mask(struct ufs_hba *hba)\n{\n\tif (hba->ufs_version == ufshci_version(1, 0))\n\t\treturn INTERRUPT_MASK_ALL_VER_10;\n\tif (hba->ufs_version <= ufshci_version(2, 0))\n\t\treturn INTERRUPT_MASK_ALL_VER_11;\n\n\treturn INTERRUPT_MASK_ALL_VER_21;\n}\n\n \nstatic inline u32 ufshcd_get_ufs_version(struct ufs_hba *hba)\n{\n\tu32 ufshci_ver;\n\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_UFS_HCI_VERSION)\n\t\tufshci_ver = ufshcd_vops_get_ufs_hci_version(hba);\n\telse\n\t\tufshci_ver = ufshcd_readl(hba, REG_UFS_VERSION);\n\n\t \n\tif (ufshci_ver & 0x00010000)\n\t\treturn ufshci_version(1, ufshci_ver & 0x00000100);\n\n\treturn ufshci_ver;\n}\n\n \nstatic inline bool ufshcd_is_device_present(struct ufs_hba *hba)\n{\n\treturn ufshcd_readl(hba, REG_CONTROLLER_STATUS) & DEVICE_PRESENT;\n}\n\n \nstatic enum utp_ocs ufshcd_get_tr_ocs(struct ufshcd_lrb *lrbp,\n\t\t\t\t      struct cq_entry *cqe)\n{\n\tif (cqe)\n\t\treturn le32_to_cpu(cqe->status) & MASK_OCS;\n\n\treturn lrbp->utr_descriptor_ptr->header.ocs & MASK_OCS;\n}\n\n \nstatic inline void ufshcd_utrl_clear(struct ufs_hba *hba, u32 mask)\n{\n\tif (hba->quirks & UFSHCI_QUIRK_BROKEN_REQ_LIST_CLR)\n\t\tmask = ~mask;\n\t \n\tufshcd_writel(hba, ~mask, REG_UTP_TRANSFER_REQ_LIST_CLEAR);\n}\n\n \nstatic inline void ufshcd_utmrl_clear(struct ufs_hba *hba, u32 pos)\n{\n\tif (hba->quirks & UFSHCI_QUIRK_BROKEN_REQ_LIST_CLR)\n\t\tufshcd_writel(hba, (1 << pos), REG_UTP_TASK_REQ_LIST_CLEAR);\n\telse\n\t\tufshcd_writel(hba, ~(1 << pos), REG_UTP_TASK_REQ_LIST_CLEAR);\n}\n\n \nstatic inline int ufshcd_get_lists_status(u32 reg)\n{\n\treturn !((reg & UFSHCD_STATUS_READY) == UFSHCD_STATUS_READY);\n}\n\n \nstatic inline int ufshcd_get_uic_cmd_result(struct ufs_hba *hba)\n{\n\treturn ufshcd_readl(hba, REG_UIC_COMMAND_ARG_2) &\n\t       MASK_UIC_COMMAND_RESULT;\n}\n\n \nstatic inline u32 ufshcd_get_dme_attr_val(struct ufs_hba *hba)\n{\n\treturn ufshcd_readl(hba, REG_UIC_COMMAND_ARG_3);\n}\n\n \nstatic inline enum upiu_response_transaction\nufshcd_get_req_rsp(struct utp_upiu_rsp *ucd_rsp_ptr)\n{\n\treturn ucd_rsp_ptr->header.transaction_code;\n}\n\n \nstatic inline bool ufshcd_is_exception_event(struct utp_upiu_rsp *ucd_rsp_ptr)\n{\n\treturn ucd_rsp_ptr->header.device_information & 1;\n}\n\n \nstatic inline void\nufshcd_reset_intr_aggr(struct ufs_hba *hba)\n{\n\tufshcd_writel(hba, INT_AGGR_ENABLE |\n\t\t      INT_AGGR_COUNTER_AND_TIMER_RESET,\n\t\t      REG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\n}\n\n \nstatic inline void\nufshcd_config_intr_aggr(struct ufs_hba *hba, u8 cnt, u8 tmout)\n{\n\tufshcd_writel(hba, INT_AGGR_ENABLE | INT_AGGR_PARAM_WRITE |\n\t\t      INT_AGGR_COUNTER_THLD_VAL(cnt) |\n\t\t      INT_AGGR_TIMEOUT_VAL(tmout),\n\t\t      REG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\n}\n\n \nstatic inline void ufshcd_disable_intr_aggr(struct ufs_hba *hba)\n{\n\tufshcd_writel(hba, 0, REG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\n}\n\n \nstatic void ufshcd_enable_run_stop_reg(struct ufs_hba *hba)\n{\n\tufshcd_writel(hba, UTP_TASK_REQ_LIST_RUN_STOP_BIT,\n\t\t      REG_UTP_TASK_REQ_LIST_RUN_STOP);\n\tufshcd_writel(hba, UTP_TRANSFER_REQ_LIST_RUN_STOP_BIT,\n\t\t      REG_UTP_TRANSFER_REQ_LIST_RUN_STOP);\n}\n\n \nstatic inline void ufshcd_hba_start(struct ufs_hba *hba)\n{\n\tu32 val = CONTROLLER_ENABLE;\n\n\tif (ufshcd_crypto_enable(hba))\n\t\tval |= CRYPTO_GENERAL_ENABLE;\n\n\tufshcd_writel(hba, val, REG_CONTROLLER_ENABLE);\n}\n\n \nbool ufshcd_is_hba_active(struct ufs_hba *hba)\n{\n\treturn ufshcd_readl(hba, REG_CONTROLLER_ENABLE) & CONTROLLER_ENABLE;\n}\nEXPORT_SYMBOL_GPL(ufshcd_is_hba_active);\n\nu32 ufshcd_get_local_unipro_ver(struct ufs_hba *hba)\n{\n\t \n\tif (hba->ufs_version <= ufshci_version(1, 1))\n\t\treturn UFS_UNIPRO_VER_1_41;\n\telse\n\t\treturn UFS_UNIPRO_VER_1_6;\n}\nEXPORT_SYMBOL(ufshcd_get_local_unipro_ver);\n\nstatic bool ufshcd_is_unipro_pa_params_tuning_req(struct ufs_hba *hba)\n{\n\t \n\treturn ufshcd_get_local_unipro_ver(hba) < UFS_UNIPRO_VER_1_6;\n}\n\n \nstatic int ufshcd_set_clk_freq(struct ufs_hba *hba, bool scale_up)\n{\n\tint ret = 0;\n\tstruct ufs_clk_info *clki;\n\tstruct list_head *head = &hba->clk_list_head;\n\n\tif (list_empty(head))\n\t\tgoto out;\n\n\tlist_for_each_entry(clki, head, list) {\n\t\tif (!IS_ERR_OR_NULL(clki->clk)) {\n\t\t\tif (scale_up && clki->max_freq) {\n\t\t\t\tif (clki->curr_freq == clki->max_freq)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tret = clk_set_rate(clki->clk, clki->max_freq);\n\t\t\t\tif (ret) {\n\t\t\t\t\tdev_err(hba->dev, \"%s: %s clk set rate(%dHz) failed, %d\\n\",\n\t\t\t\t\t\t__func__, clki->name,\n\t\t\t\t\t\tclki->max_freq, ret);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrace_ufshcd_clk_scaling(dev_name(hba->dev),\n\t\t\t\t\t\t\"scaled up\", clki->name,\n\t\t\t\t\t\tclki->curr_freq,\n\t\t\t\t\t\tclki->max_freq);\n\n\t\t\t\tclki->curr_freq = clki->max_freq;\n\n\t\t\t} else if (!scale_up && clki->min_freq) {\n\t\t\t\tif (clki->curr_freq == clki->min_freq)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tret = clk_set_rate(clki->clk, clki->min_freq);\n\t\t\t\tif (ret) {\n\t\t\t\t\tdev_err(hba->dev, \"%s: %s clk set rate(%dHz) failed, %d\\n\",\n\t\t\t\t\t\t__func__, clki->name,\n\t\t\t\t\t\tclki->min_freq, ret);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrace_ufshcd_clk_scaling(dev_name(hba->dev),\n\t\t\t\t\t\t\"scaled down\", clki->name,\n\t\t\t\t\t\tclki->curr_freq,\n\t\t\t\t\t\tclki->min_freq);\n\t\t\t\tclki->curr_freq = clki->min_freq;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(hba->dev, \"%s: clk: %s, rate: %lu\\n\", __func__,\n\t\t\t\tclki->name, clk_get_rate(clki->clk));\n\t}\n\nout:\n\treturn ret;\n}\n\n \nstatic int ufshcd_scale_clks(struct ufs_hba *hba, bool scale_up)\n{\n\tint ret = 0;\n\tktime_t start = ktime_get();\n\n\tret = ufshcd_vops_clk_scale_notify(hba, scale_up, PRE_CHANGE);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_set_clk_freq(hba, scale_up);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_vops_clk_scale_notify(hba, scale_up, POST_CHANGE);\n\tif (ret)\n\t\tufshcd_set_clk_freq(hba, !scale_up);\n\nout:\n\ttrace_ufshcd_profile_clk_scaling(dev_name(hba->dev),\n\t\t\t(scale_up ? \"up\" : \"down\"),\n\t\t\tktime_to_us(ktime_sub(ktime_get(), start)), ret);\n\treturn ret;\n}\n\n \nstatic bool ufshcd_is_devfreq_scaling_required(struct ufs_hba *hba,\n\t\t\t\t\t       bool scale_up)\n{\n\tstruct ufs_clk_info *clki;\n\tstruct list_head *head = &hba->clk_list_head;\n\n\tif (list_empty(head))\n\t\treturn false;\n\n\tlist_for_each_entry(clki, head, list) {\n\t\tif (!IS_ERR_OR_NULL(clki->clk)) {\n\t\t\tif (scale_up && clki->max_freq) {\n\t\t\t\tif (clki->curr_freq == clki->max_freq)\n\t\t\t\t\tcontinue;\n\t\t\t\treturn true;\n\t\t\t} else if (!scale_up && clki->min_freq) {\n\t\t\t\tif (clki->curr_freq == clki->min_freq)\n\t\t\t\t\tcontinue;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \nstatic u32 ufshcd_pending_cmds(struct ufs_hba *hba)\n{\n\tconst struct scsi_device *sdev;\n\tu32 pending = 0;\n\n\tlockdep_assert_held(hba->host->host_lock);\n\t__shost_for_each_device(sdev, hba->host)\n\t\tpending += sbitmap_weight(&sdev->budget_map);\n\n\treturn pending;\n}\n\n \nstatic int ufshcd_wait_for_doorbell_clr(struct ufs_hba *hba,\n\t\t\t\t\tu64 wait_timeout_us)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\tu32 tm_doorbell;\n\tu32 tr_pending;\n\tbool timeout = false, do_last_check = false;\n\tktime_t start;\n\n\tufshcd_hold(hba);\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t \n\tstart = ktime_get();\n\tdo {\n\t\tif (hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttm_doorbell = ufshcd_readl(hba, REG_UTP_TASK_REQ_DOOR_BELL);\n\t\ttr_pending = ufshcd_pending_cmds(hba);\n\t\tif (!tm_doorbell && !tr_pending) {\n\t\t\ttimeout = false;\n\t\t\tbreak;\n\t\t} else if (do_last_check) {\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tio_schedule_timeout(msecs_to_jiffies(20));\n\t\tif (ktime_to_us(ktime_sub(ktime_get(), start)) >\n\t\t    wait_timeout_us) {\n\t\t\ttimeout = true;\n\t\t\t \n\t\t\tdo_last_check = true;\n\t\t}\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t} while (tm_doorbell || tr_pending);\n\n\tif (timeout) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: timedout waiting for doorbell to clear (tm=0x%x, tr=0x%x)\\n\",\n\t\t\t__func__, tm_doorbell, tr_pending);\n\t\tret = -EBUSY;\n\t}\nout:\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tufshcd_release(hba);\n\treturn ret;\n}\n\n \nstatic int ufshcd_scale_gear(struct ufs_hba *hba, bool scale_up)\n{\n\tint ret = 0;\n\tstruct ufs_pa_layer_attr new_pwr_info;\n\n\tif (scale_up) {\n\t\tmemcpy(&new_pwr_info, &hba->clk_scaling.saved_pwr_info,\n\t\t       sizeof(struct ufs_pa_layer_attr));\n\t} else {\n\t\tmemcpy(&new_pwr_info, &hba->pwr_info,\n\t\t       sizeof(struct ufs_pa_layer_attr));\n\n\t\tif (hba->pwr_info.gear_tx > hba->clk_scaling.min_gear ||\n\t\t    hba->pwr_info.gear_rx > hba->clk_scaling.min_gear) {\n\t\t\t \n\t\t\tmemcpy(&hba->clk_scaling.saved_pwr_info,\n\t\t\t\t&hba->pwr_info,\n\t\t\t\tsizeof(struct ufs_pa_layer_attr));\n\n\t\t\t \n\t\t\tnew_pwr_info.gear_tx = hba->clk_scaling.min_gear;\n\t\t\tnew_pwr_info.gear_rx = hba->clk_scaling.min_gear;\n\t\t}\n\t}\n\n\t \n\tret = ufshcd_config_pwr_mode(hba, &new_pwr_info);\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: failed err %d, old gear: (tx %d rx %d), new gear: (tx %d rx %d)\",\n\t\t\t__func__, ret,\n\t\t\thba->pwr_info.gear_tx, hba->pwr_info.gear_rx,\n\t\t\tnew_pwr_info.gear_tx, new_pwr_info.gear_rx);\n\n\treturn ret;\n}\n\n \nstatic int ufshcd_clock_scaling_prepare(struct ufs_hba *hba, u64 timeout_us)\n{\n\tint ret = 0;\n\t \n\tufshcd_scsi_block_requests(hba);\n\tmutex_lock(&hba->wb_mutex);\n\tdown_write(&hba->clk_scaling_lock);\n\n\tif (!hba->clk_scaling.is_allowed ||\n\t    ufshcd_wait_for_doorbell_clr(hba, timeout_us)) {\n\t\tret = -EBUSY;\n\t\tup_write(&hba->clk_scaling_lock);\n\t\tmutex_unlock(&hba->wb_mutex);\n\t\tufshcd_scsi_unblock_requests(hba);\n\t\tgoto out;\n\t}\n\n\t \n\tufshcd_hold(hba);\n\nout:\n\treturn ret;\n}\n\nstatic void ufshcd_clock_scaling_unprepare(struct ufs_hba *hba, int err, bool scale_up)\n{\n\tup_write(&hba->clk_scaling_lock);\n\n\t \n\tif (ufshcd_enable_wb_if_scaling_up(hba) && !err)\n\t\tufshcd_wb_toggle(hba, scale_up);\n\n\tmutex_unlock(&hba->wb_mutex);\n\n\tufshcd_scsi_unblock_requests(hba);\n\tufshcd_release(hba);\n}\n\n \nstatic int ufshcd_devfreq_scale(struct ufs_hba *hba, bool scale_up)\n{\n\tint ret = 0;\n\n\tret = ufshcd_clock_scaling_prepare(hba, 1 * USEC_PER_SEC);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!scale_up) {\n\t\tret = ufshcd_scale_gear(hba, false);\n\t\tif (ret)\n\t\t\tgoto out_unprepare;\n\t}\n\n\tret = ufshcd_scale_clks(hba, scale_up);\n\tif (ret) {\n\t\tif (!scale_up)\n\t\t\tufshcd_scale_gear(hba, true);\n\t\tgoto out_unprepare;\n\t}\n\n\t \n\tif (scale_up) {\n\t\tret = ufshcd_scale_gear(hba, true);\n\t\tif (ret) {\n\t\t\tufshcd_scale_clks(hba, false);\n\t\t\tgoto out_unprepare;\n\t\t}\n\t}\n\nout_unprepare:\n\tufshcd_clock_scaling_unprepare(hba, ret, scale_up);\n\treturn ret;\n}\n\nstatic void ufshcd_clk_scaling_suspend_work(struct work_struct *work)\n{\n\tstruct ufs_hba *hba = container_of(work, struct ufs_hba,\n\t\t\t\t\t   clk_scaling.suspend_work);\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, irq_flags);\n\tif (hba->clk_scaling.active_reqs || hba->clk_scaling.is_suspended) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\t\treturn;\n\t}\n\thba->clk_scaling.is_suspended = true;\n\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\n\t__ufshcd_suspend_clkscaling(hba);\n}\n\nstatic void ufshcd_clk_scaling_resume_work(struct work_struct *work)\n{\n\tstruct ufs_hba *hba = container_of(work, struct ufs_hba,\n\t\t\t\t\t   clk_scaling.resume_work);\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, irq_flags);\n\tif (!hba->clk_scaling.is_suspended) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\t\treturn;\n\t}\n\thba->clk_scaling.is_suspended = false;\n\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\n\tdevfreq_resume_device(hba->devfreq);\n}\n\nstatic int ufshcd_devfreq_target(struct device *dev,\n\t\t\t\tunsigned long *freq, u32 flags)\n{\n\tint ret = 0;\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tktime_t start;\n\tbool scale_up, sched_clk_scaling_suspend_work = false;\n\tstruct list_head *clk_list = &hba->clk_list_head;\n\tstruct ufs_clk_info *clki;\n\tunsigned long irq_flags;\n\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn -EINVAL;\n\n\tclki = list_first_entry(&hba->clk_list_head, struct ufs_clk_info, list);\n\t \n\t*freq = (unsigned long) clk_round_rate(clki->clk, *freq);\n\tspin_lock_irqsave(hba->host->host_lock, irq_flags);\n\tif (ufshcd_eh_in_progress(hba)) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\t\treturn 0;\n\t}\n\n\tif (!hba->clk_scaling.active_reqs)\n\t\tsched_clk_scaling_suspend_work = true;\n\n\tif (list_empty(clk_list)) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\t\tgoto out;\n\t}\n\n\t \n\tscale_up = *freq == clki->max_freq;\n\tif (!scale_up)\n\t\t*freq = clki->min_freq;\n\t \n\tif (!ufshcd_is_devfreq_scaling_required(hba, scale_up)) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\t\tret = 0;\n\t\tgoto out;  \n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, irq_flags);\n\n\tstart = ktime_get();\n\tret = ufshcd_devfreq_scale(hba, scale_up);\n\n\ttrace_ufshcd_profile_clk_scaling(dev_name(hba->dev),\n\t\t(scale_up ? \"up\" : \"down\"),\n\t\tktime_to_us(ktime_sub(ktime_get(), start)), ret);\n\nout:\n\tif (sched_clk_scaling_suspend_work)\n\t\tqueue_work(hba->clk_scaling.workq,\n\t\t\t   &hba->clk_scaling.suspend_work);\n\n\treturn ret;\n}\n\nstatic int ufshcd_devfreq_get_dev_status(struct device *dev,\n\t\tstruct devfreq_dev_status *stat)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tstruct ufs_clk_scaling *scaling = &hba->clk_scaling;\n\tunsigned long flags;\n\tstruct list_head *clk_list = &hba->clk_list_head;\n\tstruct ufs_clk_info *clki;\n\tktime_t curr_t;\n\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn -EINVAL;\n\n\tmemset(stat, 0, sizeof(*stat));\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tcurr_t = ktime_get();\n\tif (!scaling->window_start_t)\n\t\tgoto start_window;\n\n\tclki = list_first_entry(clk_list, struct ufs_clk_info, list);\n\t \n\tstat->current_frequency = clki->curr_freq;\n\tif (scaling->is_busy_started)\n\t\tscaling->tot_busy_t += ktime_us_delta(curr_t,\n\t\t\t\tscaling->busy_start_t);\n\n\tstat->total_time = ktime_us_delta(curr_t, scaling->window_start_t);\n\tstat->busy_time = scaling->tot_busy_t;\nstart_window:\n\tscaling->window_start_t = curr_t;\n\tscaling->tot_busy_t = 0;\n\n\tif (scaling->active_reqs) {\n\t\tscaling->busy_start_t = curr_t;\n\t\tscaling->is_busy_started = true;\n\t} else {\n\t\tscaling->busy_start_t = 0;\n\t\tscaling->is_busy_started = false;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\treturn 0;\n}\n\nstatic int ufshcd_devfreq_init(struct ufs_hba *hba)\n{\n\tstruct list_head *clk_list = &hba->clk_list_head;\n\tstruct ufs_clk_info *clki;\n\tstruct devfreq *devfreq;\n\tint ret;\n\n\t \n\tif (list_empty(clk_list))\n\t\treturn 0;\n\n\tclki = list_first_entry(clk_list, struct ufs_clk_info, list);\n\tdev_pm_opp_add(hba->dev, clki->min_freq, 0);\n\tdev_pm_opp_add(hba->dev, clki->max_freq, 0);\n\n\tufshcd_vops_config_scaling_param(hba, &hba->vps->devfreq_profile,\n\t\t\t\t\t &hba->vps->ondemand_data);\n\tdevfreq = devfreq_add_device(hba->dev,\n\t\t\t&hba->vps->devfreq_profile,\n\t\t\tDEVFREQ_GOV_SIMPLE_ONDEMAND,\n\t\t\t&hba->vps->ondemand_data);\n\tif (IS_ERR(devfreq)) {\n\t\tret = PTR_ERR(devfreq);\n\t\tdev_err(hba->dev, \"Unable to register with devfreq %d\\n\", ret);\n\n\t\tdev_pm_opp_remove(hba->dev, clki->min_freq);\n\t\tdev_pm_opp_remove(hba->dev, clki->max_freq);\n\t\treturn ret;\n\t}\n\n\thba->devfreq = devfreq;\n\n\treturn 0;\n}\n\nstatic void ufshcd_devfreq_remove(struct ufs_hba *hba)\n{\n\tstruct list_head *clk_list = &hba->clk_list_head;\n\tstruct ufs_clk_info *clki;\n\n\tif (!hba->devfreq)\n\t\treturn;\n\n\tdevfreq_remove_device(hba->devfreq);\n\thba->devfreq = NULL;\n\n\tclki = list_first_entry(clk_list, struct ufs_clk_info, list);\n\tdev_pm_opp_remove(hba->dev, clki->min_freq);\n\tdev_pm_opp_remove(hba->dev, clki->max_freq);\n}\n\nstatic void __ufshcd_suspend_clkscaling(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\n\tdevfreq_suspend_device(hba->devfreq);\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->clk_scaling.window_start_t = 0;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\n\nstatic void ufshcd_suspend_clkscaling(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tbool suspend = false;\n\n\tcancel_work_sync(&hba->clk_scaling.suspend_work);\n\tcancel_work_sync(&hba->clk_scaling.resume_work);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (!hba->clk_scaling.is_suspended) {\n\t\tsuspend = true;\n\t\thba->clk_scaling.is_suspended = true;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tif (suspend)\n\t\t__ufshcd_suspend_clkscaling(hba);\n}\n\nstatic void ufshcd_resume_clkscaling(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tbool resume = false;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (hba->clk_scaling.is_suspended) {\n\t\tresume = true;\n\t\thba->clk_scaling.is_suspended = false;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tif (resume)\n\t\tdevfreq_resume_device(hba->devfreq);\n}\n\nstatic ssize_t ufshcd_clkscale_enable_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", hba->clk_scaling.is_enabled);\n}\n\nstatic ssize_t ufshcd_clkscale_enable_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tu32 value;\n\tint err = 0;\n\n\tif (kstrtou32(buf, 0, &value))\n\t\treturn -EINVAL;\n\n\tdown(&hba->host_sem);\n\tif (!ufshcd_is_user_access_allowed(hba)) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tvalue = !!value;\n\tif (value == hba->clk_scaling.is_enabled)\n\t\tgoto out;\n\n\tufshcd_rpm_get_sync(hba);\n\tufshcd_hold(hba);\n\n\thba->clk_scaling.is_enabled = value;\n\n\tif (value) {\n\t\tufshcd_resume_clkscaling(hba);\n\t} else {\n\t\tufshcd_suspend_clkscaling(hba);\n\t\terr = ufshcd_devfreq_scale(hba, true);\n\t\tif (err)\n\t\t\tdev_err(hba->dev, \"%s: failed to scale clocks up %d\\n\",\n\t\t\t\t\t__func__, err);\n\t}\n\n\tufshcd_release(hba);\n\tufshcd_rpm_put_sync(hba);\nout:\n\tup(&hba->host_sem);\n\treturn err ? err : count;\n}\n\nstatic void ufshcd_init_clk_scaling_sysfs(struct ufs_hba *hba)\n{\n\thba->clk_scaling.enable_attr.show = ufshcd_clkscale_enable_show;\n\thba->clk_scaling.enable_attr.store = ufshcd_clkscale_enable_store;\n\tsysfs_attr_init(&hba->clk_scaling.enable_attr.attr);\n\thba->clk_scaling.enable_attr.attr.name = \"clkscale_enable\";\n\thba->clk_scaling.enable_attr.attr.mode = 0644;\n\tif (device_create_file(hba->dev, &hba->clk_scaling.enable_attr))\n\t\tdev_err(hba->dev, \"Failed to create sysfs for clkscale_enable\\n\");\n}\n\nstatic void ufshcd_remove_clk_scaling_sysfs(struct ufs_hba *hba)\n{\n\tif (hba->clk_scaling.enable_attr.attr.name)\n\t\tdevice_remove_file(hba->dev, &hba->clk_scaling.enable_attr);\n}\n\nstatic void ufshcd_init_clk_scaling(struct ufs_hba *hba)\n{\n\tchar wq_name[sizeof(\"ufs_clkscaling_00\")];\n\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn;\n\n\tif (!hba->clk_scaling.min_gear)\n\t\thba->clk_scaling.min_gear = UFS_HS_G1;\n\n\tINIT_WORK(&hba->clk_scaling.suspend_work,\n\t\t  ufshcd_clk_scaling_suspend_work);\n\tINIT_WORK(&hba->clk_scaling.resume_work,\n\t\t  ufshcd_clk_scaling_resume_work);\n\n\tsnprintf(wq_name, sizeof(wq_name), \"ufs_clkscaling_%d\",\n\t\t hba->host->host_no);\n\thba->clk_scaling.workq = create_singlethread_workqueue(wq_name);\n\n\thba->clk_scaling.is_initialized = true;\n}\n\nstatic void ufshcd_exit_clk_scaling(struct ufs_hba *hba)\n{\n\tif (!hba->clk_scaling.is_initialized)\n\t\treturn;\n\n\tufshcd_remove_clk_scaling_sysfs(hba);\n\tdestroy_workqueue(hba->clk_scaling.workq);\n\tufshcd_devfreq_remove(hba);\n\thba->clk_scaling.is_initialized = false;\n}\n\nstatic void ufshcd_ungate_work(struct work_struct *work)\n{\n\tint ret;\n\tunsigned long flags;\n\tstruct ufs_hba *hba = container_of(work, struct ufs_hba,\n\t\t\tclk_gating.ungate_work);\n\n\tcancel_delayed_work_sync(&hba->clk_gating.gate_work);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (hba->clk_gating.state == CLKS_ON) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\treturn;\n\t}\n\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tufshcd_hba_vreg_set_hpm(hba);\n\tufshcd_setup_clocks(hba, true);\n\n\tufshcd_enable_irq(hba);\n\n\t \n\tif (ufshcd_can_hibern8_during_gating(hba)) {\n\t\t \n\t\thba->clk_gating.is_suspended = true;\n\t\tif (ufshcd_is_link_hibern8(hba)) {\n\t\t\tret = ufshcd_uic_hibern8_exit(hba);\n\t\t\tif (ret)\n\t\t\t\tdev_err(hba->dev, \"%s: hibern8 exit failed %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\telse\n\t\t\t\tufshcd_set_link_active(hba);\n\t\t}\n\t\thba->clk_gating.is_suspended = false;\n\t}\n}\n\n \nvoid ufshcd_hold(struct ufs_hba *hba)\n{\n\tbool flush_result;\n\tunsigned long flags;\n\n\tif (!ufshcd_is_clkgating_allowed(hba) ||\n\t    !hba->clk_gating.is_initialized)\n\t\treturn;\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->clk_gating.active_reqs++;\n\nstart:\n\tswitch (hba->clk_gating.state) {\n\tcase CLKS_ON:\n\t\t \n\t\tif (ufshcd_can_hibern8_during_gating(hba) &&\n\t\t    ufshcd_is_link_hibern8(hba)) {\n\t\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\t\tflush_result = flush_work(&hba->clk_gating.ungate_work);\n\t\t\tif (hba->clk_gating.is_suspended && !flush_result)\n\t\t\t\treturn;\n\t\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\t\tgoto start;\n\t\t}\n\t\tbreak;\n\tcase REQ_CLKS_OFF:\n\t\tif (cancel_delayed_work(&hba->clk_gating.gate_work)) {\n\t\t\thba->clk_gating.state = CLKS_ON;\n\t\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\t\thba->clk_gating.state);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tfallthrough;\n\tcase CLKS_OFF:\n\t\thba->clk_gating.state = REQ_CLKS_ON;\n\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\thba->clk_gating.state);\n\t\tqueue_work(hba->clk_gating.clk_gating_workq,\n\t\t\t   &hba->clk_gating.ungate_work);\n\t\t \n\t\tfallthrough;\n\tcase REQ_CLKS_ON:\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tflush_work(&hba->clk_gating.ungate_work);\n\t\t \n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tgoto start;\n\tdefault:\n\t\tdev_err(hba->dev, \"%s: clk gating is in invalid state %d\\n\",\n\t\t\t\t__func__, hba->clk_gating.state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ufshcd_hold);\n\nstatic void ufshcd_gate_work(struct work_struct *work)\n{\n\tstruct ufs_hba *hba = container_of(work, struct ufs_hba,\n\t\t\tclk_gating.gate_work.work);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t \n\tif (hba->clk_gating.is_suspended ||\n\t\t(hba->clk_gating.state != REQ_CLKS_OFF)) {\n\t\thba->clk_gating.state = CLKS_ON;\n\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\thba->clk_gating.state);\n\t\tgoto rel_lock;\n\t}\n\n\tif (hba->clk_gating.active_reqs\n\t\t|| hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL\n\t\t|| hba->outstanding_reqs || hba->outstanding_tasks\n\t\t|| hba->active_uic_cmd || hba->uic_async_done)\n\t\tgoto rel_lock;\n\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t \n\tif (ufshcd_can_hibern8_during_gating(hba)) {\n\t\tret = ufshcd_uic_hibern8_enter(hba);\n\t\tif (ret) {\n\t\t\thba->clk_gating.state = CLKS_ON;\n\t\t\tdev_err(hba->dev, \"%s: hibern8 enter failed %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\t\thba->clk_gating.state);\n\t\t\tgoto out;\n\t\t}\n\t\tufshcd_set_link_hibern8(hba);\n\t}\n\n\tufshcd_disable_irq(hba);\n\n\tufshcd_setup_clocks(hba, false);\n\n\t \n\tufshcd_hba_vreg_set_lpm(hba);\n\t \n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (hba->clk_gating.state == REQ_CLKS_OFF) {\n\t\thba->clk_gating.state = CLKS_OFF;\n\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\thba->clk_gating.state);\n\t}\nrel_lock:\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\nout:\n\treturn;\n}\n\n \nstatic void __ufshcd_release(struct ufs_hba *hba)\n{\n\tif (!ufshcd_is_clkgating_allowed(hba))\n\t\treturn;\n\n\thba->clk_gating.active_reqs--;\n\n\tif (hba->clk_gating.active_reqs || hba->clk_gating.is_suspended ||\n\t    hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL ||\n\t    hba->outstanding_tasks || !hba->clk_gating.is_initialized ||\n\t    hba->active_uic_cmd || hba->uic_async_done ||\n\t    hba->clk_gating.state == CLKS_OFF)\n\t\treturn;\n\n\thba->clk_gating.state = REQ_CLKS_OFF;\n\ttrace_ufshcd_clk_gating(dev_name(hba->dev), hba->clk_gating.state);\n\tqueue_delayed_work(hba->clk_gating.clk_gating_workq,\n\t\t\t   &hba->clk_gating.gate_work,\n\t\t\t   msecs_to_jiffies(hba->clk_gating.delay_ms));\n}\n\nvoid ufshcd_release(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t__ufshcd_release(hba);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ufshcd_release);\n\nstatic ssize_t ufshcd_clkgate_delay_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\treturn sysfs_emit(buf, \"%lu\\n\", hba->clk_gating.delay_ms);\n}\n\nvoid ufshcd_clkgate_delay_set(struct device *dev, unsigned long value)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->clk_gating.delay_ms = value;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ufshcd_clkgate_delay_set);\n\nstatic ssize_t ufshcd_clkgate_delay_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tunsigned long value;\n\n\tif (kstrtoul(buf, 0, &value))\n\t\treturn -EINVAL;\n\n\tufshcd_clkgate_delay_set(dev, value);\n\treturn count;\n}\n\nstatic ssize_t ufshcd_clkgate_enable_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", hba->clk_gating.is_enabled);\n}\n\nstatic ssize_t ufshcd_clkgate_enable_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tunsigned long flags;\n\tu32 value;\n\n\tif (kstrtou32(buf, 0, &value))\n\t\treturn -EINVAL;\n\n\tvalue = !!value;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (value == hba->clk_gating.is_enabled)\n\t\tgoto out;\n\n\tif (value)\n\t\t__ufshcd_release(hba);\n\telse\n\t\thba->clk_gating.active_reqs++;\n\n\thba->clk_gating.is_enabled = value;\nout:\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\treturn count;\n}\n\nstatic void ufshcd_init_clk_gating_sysfs(struct ufs_hba *hba)\n{\n\thba->clk_gating.delay_attr.show = ufshcd_clkgate_delay_show;\n\thba->clk_gating.delay_attr.store = ufshcd_clkgate_delay_store;\n\tsysfs_attr_init(&hba->clk_gating.delay_attr.attr);\n\thba->clk_gating.delay_attr.attr.name = \"clkgate_delay_ms\";\n\thba->clk_gating.delay_attr.attr.mode = 0644;\n\tif (device_create_file(hba->dev, &hba->clk_gating.delay_attr))\n\t\tdev_err(hba->dev, \"Failed to create sysfs for clkgate_delay\\n\");\n\n\thba->clk_gating.enable_attr.show = ufshcd_clkgate_enable_show;\n\thba->clk_gating.enable_attr.store = ufshcd_clkgate_enable_store;\n\tsysfs_attr_init(&hba->clk_gating.enable_attr.attr);\n\thba->clk_gating.enable_attr.attr.name = \"clkgate_enable\";\n\thba->clk_gating.enable_attr.attr.mode = 0644;\n\tif (device_create_file(hba->dev, &hba->clk_gating.enable_attr))\n\t\tdev_err(hba->dev, \"Failed to create sysfs for clkgate_enable\\n\");\n}\n\nstatic void ufshcd_remove_clk_gating_sysfs(struct ufs_hba *hba)\n{\n\tif (hba->clk_gating.delay_attr.attr.name)\n\t\tdevice_remove_file(hba->dev, &hba->clk_gating.delay_attr);\n\tif (hba->clk_gating.enable_attr.attr.name)\n\t\tdevice_remove_file(hba->dev, &hba->clk_gating.enable_attr);\n}\n\nstatic void ufshcd_init_clk_gating(struct ufs_hba *hba)\n{\n\tchar wq_name[sizeof(\"ufs_clk_gating_00\")];\n\n\tif (!ufshcd_is_clkgating_allowed(hba))\n\t\treturn;\n\n\thba->clk_gating.state = CLKS_ON;\n\n\thba->clk_gating.delay_ms = 150;\n\tINIT_DELAYED_WORK(&hba->clk_gating.gate_work, ufshcd_gate_work);\n\tINIT_WORK(&hba->clk_gating.ungate_work, ufshcd_ungate_work);\n\n\tsnprintf(wq_name, ARRAY_SIZE(wq_name), \"ufs_clk_gating_%d\",\n\t\t hba->host->host_no);\n\thba->clk_gating.clk_gating_workq = alloc_ordered_workqueue(wq_name,\n\t\t\t\t\tWQ_MEM_RECLAIM | WQ_HIGHPRI);\n\n\tufshcd_init_clk_gating_sysfs(hba);\n\n\thba->clk_gating.is_enabled = true;\n\thba->clk_gating.is_initialized = true;\n}\n\nstatic void ufshcd_exit_clk_gating(struct ufs_hba *hba)\n{\n\tif (!hba->clk_gating.is_initialized)\n\t\treturn;\n\n\tufshcd_remove_clk_gating_sysfs(hba);\n\n\t \n\tufshcd_hold(hba);\n\thba->clk_gating.is_initialized = false;\n\tufshcd_release(hba);\n\n\tdestroy_workqueue(hba->clk_gating.clk_gating_workq);\n}\n\nstatic void ufshcd_clk_scaling_start_busy(struct ufs_hba *hba)\n{\n\tbool queue_resume_work = false;\n\tktime_t curr_t = ktime_get();\n\tunsigned long flags;\n\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (!hba->clk_scaling.active_reqs++)\n\t\tqueue_resume_work = true;\n\n\tif (!hba->clk_scaling.is_enabled || hba->pm_op_in_progress) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\treturn;\n\t}\n\n\tif (queue_resume_work)\n\t\tqueue_work(hba->clk_scaling.workq,\n\t\t\t   &hba->clk_scaling.resume_work);\n\n\tif (!hba->clk_scaling.window_start_t) {\n\t\thba->clk_scaling.window_start_t = curr_t;\n\t\thba->clk_scaling.tot_busy_t = 0;\n\t\thba->clk_scaling.is_busy_started = false;\n\t}\n\n\tif (!hba->clk_scaling.is_busy_started) {\n\t\thba->clk_scaling.busy_start_t = curr_t;\n\t\thba->clk_scaling.is_busy_started = true;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\n\nstatic void ufshcd_clk_scaling_update_busy(struct ufs_hba *hba)\n{\n\tstruct ufs_clk_scaling *scaling = &hba->clk_scaling;\n\tunsigned long flags;\n\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->clk_scaling.active_reqs--;\n\tif (!scaling->active_reqs && scaling->is_busy_started) {\n\t\tscaling->tot_busy_t += ktime_to_us(ktime_sub(ktime_get(),\n\t\t\t\t\tscaling->busy_start_t));\n\t\tscaling->busy_start_t = 0;\n\t\tscaling->is_busy_started = false;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\n\nstatic inline int ufshcd_monitor_opcode2dir(u8 opcode)\n{\n\tif (opcode == READ_6 || opcode == READ_10 || opcode == READ_16)\n\t\treturn READ;\n\telse if (opcode == WRITE_6 || opcode == WRITE_10 || opcode == WRITE_16)\n\t\treturn WRITE;\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic inline bool ufshcd_should_inform_monitor(struct ufs_hba *hba,\n\t\t\t\t\t\tstruct ufshcd_lrb *lrbp)\n{\n\tconst struct ufs_hba_monitor *m = &hba->monitor;\n\n\treturn (m->enabled && lrbp && lrbp->cmd &&\n\t\t(!m->chunk_size || m->chunk_size == lrbp->cmd->sdb.length) &&\n\t\tktime_before(hba->monitor.enabled_ts, lrbp->issue_time_stamp));\n}\n\nstatic void ufshcd_start_monitor(struct ufs_hba *hba,\n\t\t\t\t const struct ufshcd_lrb *lrbp)\n{\n\tint dir = ufshcd_monitor_opcode2dir(*lrbp->cmd->cmnd);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (dir >= 0 && hba->monitor.nr_queued[dir]++ == 0)\n\t\thba->monitor.busy_start_ts[dir] = ktime_get();\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\n\nstatic void ufshcd_update_monitor(struct ufs_hba *hba, const struct ufshcd_lrb *lrbp)\n{\n\tint dir = ufshcd_monitor_opcode2dir(*lrbp->cmd->cmnd);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (dir >= 0 && hba->monitor.nr_queued[dir] > 0) {\n\t\tconst struct request *req = scsi_cmd_to_rq(lrbp->cmd);\n\t\tstruct ufs_hba_monitor *m = &hba->monitor;\n\t\tktime_t now, inc, lat;\n\n\t\tnow = lrbp->compl_time_stamp;\n\t\tinc = ktime_sub(now, m->busy_start_ts[dir]);\n\t\tm->total_busy[dir] = ktime_add(m->total_busy[dir], inc);\n\t\tm->nr_sec_rw[dir] += blk_rq_sectors(req);\n\n\t\t \n\t\tm->nr_req[dir]++;\n\t\tlat = ktime_sub(now, lrbp->issue_time_stamp);\n\t\tm->lat_sum[dir] += lat;\n\t\tif (m->lat_max[dir] < lat || !m->lat_max[dir])\n\t\t\tm->lat_max[dir] = lat;\n\t\tif (m->lat_min[dir] > lat || !m->lat_min[dir])\n\t\t\tm->lat_min[dir] = lat;\n\n\t\tm->nr_queued[dir]--;\n\t\t \n\t\tm->busy_start_ts[dir] = now;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n}\n\n \nstatic inline\nvoid ufshcd_send_command(struct ufs_hba *hba, unsigned int task_tag,\n\t\t\t struct ufs_hw_queue *hwq)\n{\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[task_tag];\n\tunsigned long flags;\n\n\tlrbp->issue_time_stamp = ktime_get();\n\tlrbp->issue_time_stamp_local_clock = local_clock();\n\tlrbp->compl_time_stamp = ktime_set(0, 0);\n\tlrbp->compl_time_stamp_local_clock = 0;\n\tufshcd_add_command_trace(hba, task_tag, UFS_CMD_SEND);\n\tufshcd_clk_scaling_start_busy(hba);\n\tif (unlikely(ufshcd_should_inform_monitor(hba, lrbp)))\n\t\tufshcd_start_monitor(hba, lrbp);\n\n\tif (is_mcq_enabled(hba)) {\n\t\tint utrd_size = sizeof(struct utp_transfer_req_desc);\n\t\tstruct utp_transfer_req_desc *src = lrbp->utr_descriptor_ptr;\n\t\tstruct utp_transfer_req_desc *dest;\n\n\t\tspin_lock(&hwq->sq_lock);\n\t\tdest = hwq->sqe_base_addr + hwq->sq_tail_slot;\n\t\tmemcpy(dest, src, utrd_size);\n\t\tufshcd_inc_sq_tail(hwq);\n\t\tspin_unlock(&hwq->sq_lock);\n\t} else {\n\t\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\t\tif (hba->vops && hba->vops->setup_xfer_req)\n\t\t\thba->vops->setup_xfer_req(hba, lrbp->task_tag,\n\t\t\t\t\t\t  !!lrbp->cmd);\n\t\t__set_bit(lrbp->task_tag, &hba->outstanding_reqs);\n\t\tufshcd_writel(hba, 1 << lrbp->task_tag,\n\t\t\t      REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\t\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\t}\n}\n\n \nstatic inline void ufshcd_copy_sense_data(struct ufshcd_lrb *lrbp)\n{\n\tu8 *const sense_buffer = lrbp->cmd->sense_buffer;\n\tu16 resp_len;\n\tint len;\n\n\tresp_len = be16_to_cpu(lrbp->ucd_rsp_ptr->header.data_segment_length);\n\tif (sense_buffer && resp_len) {\n\t\tint len_to_copy;\n\n\t\tlen = be16_to_cpu(lrbp->ucd_rsp_ptr->sr.sense_data_len);\n\t\tlen_to_copy = min_t(int, UFS_SENSE_SIZE, len);\n\n\t\tmemcpy(sense_buffer, lrbp->ucd_rsp_ptr->sr.sense_data,\n\t\t       len_to_copy);\n\t}\n}\n\n \nstatic\nint ufshcd_copy_query_response(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\n{\n\tstruct ufs_query_res *query_res = &hba->dev_cmd.query.response;\n\n\tmemcpy(&query_res->upiu_res, &lrbp->ucd_rsp_ptr->qr, QUERY_OSF_SIZE);\n\n\t \n\tif (hba->dev_cmd.query.descriptor &&\n\t    lrbp->ucd_rsp_ptr->qr.opcode == UPIU_QUERY_OPCODE_READ_DESC) {\n\t\tu8 *descp = (u8 *)lrbp->ucd_rsp_ptr +\n\t\t\t\tGENERAL_UPIU_REQUEST_SIZE;\n\t\tu16 resp_len;\n\t\tu16 buf_len;\n\n\t\t \n\t\tresp_len = be16_to_cpu(lrbp->ucd_rsp_ptr->header\n\t\t\t\t       .data_segment_length);\n\t\tbuf_len = be16_to_cpu(\n\t\t\t\thba->dev_cmd.query.request.upiu_req.length);\n\t\tif (likely(buf_len >= resp_len)) {\n\t\t\tmemcpy(hba->dev_cmd.query.descriptor, descp, resp_len);\n\t\t} else {\n\t\t\tdev_warn(hba->dev,\n\t\t\t\t \"%s: rsp size %d is bigger than buffer size %d\",\n\t\t\t\t __func__, resp_len, buf_len);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline int ufshcd_hba_capabilities(struct ufs_hba *hba)\n{\n\tint err;\n\n\thba->capabilities = ufshcd_readl(hba, REG_CONTROLLER_CAPABILITIES);\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_64BIT_ADDRESS)\n\t\thba->capabilities &= ~MASK_64_ADDRESSING_SUPPORT;\n\n\t \n\thba->nutrs = (hba->capabilities & MASK_TRANSFER_REQUESTS_SLOTS) + 1;\n\thba->nutmrs =\n\t((hba->capabilities & MASK_TASK_MANAGEMENT_REQUEST_SLOTS) >> 16) + 1;\n\thba->reserved_slot = hba->nutrs - 1;\n\n\t \n\terr = ufshcd_hba_init_crypto_capabilities(hba);\n\tif (err) {\n\t\tdev_err(hba->dev, \"crypto setup failed\\n\");\n\t\treturn err;\n\t}\n\n\thba->mcq_sup = FIELD_GET(MASK_MCQ_SUPPORT, hba->capabilities);\n\tif (!hba->mcq_sup)\n\t\treturn 0;\n\n\thba->mcq_capabilities = ufshcd_readl(hba, REG_MCQCAP);\n\thba->ext_iid_sup = FIELD_GET(MASK_EXT_IID_SUPPORT,\n\t\t\t\t     hba->mcq_capabilities);\n\n\treturn 0;\n}\n\n \nstatic inline bool ufshcd_ready_for_uic_cmd(struct ufs_hba *hba)\n{\n\tu32 val;\n\tint ret = read_poll_timeout(ufshcd_readl, val, val & UIC_COMMAND_READY,\n\t\t\t\t    500, UIC_CMD_TIMEOUT * 1000, false, hba,\n\t\t\t\t    REG_CONTROLLER_STATUS);\n\treturn ret == 0 ? true : false;\n}\n\n \nstatic inline u8 ufshcd_get_upmcrs(struct ufs_hba *hba)\n{\n\treturn (ufshcd_readl(hba, REG_CONTROLLER_STATUS) >> 8) & 0x7;\n}\n\n \nstatic inline void\nufshcd_dispatch_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\n{\n\tlockdep_assert_held(&hba->uic_cmd_mutex);\n\n\tWARN_ON(hba->active_uic_cmd);\n\n\thba->active_uic_cmd = uic_cmd;\n\n\t \n\tufshcd_writel(hba, uic_cmd->argument1, REG_UIC_COMMAND_ARG_1);\n\tufshcd_writel(hba, uic_cmd->argument2, REG_UIC_COMMAND_ARG_2);\n\tufshcd_writel(hba, uic_cmd->argument3, REG_UIC_COMMAND_ARG_3);\n\n\tufshcd_add_uic_command_trace(hba, uic_cmd, UFS_CMD_SEND);\n\n\t \n\tufshcd_writel(hba, uic_cmd->command & COMMAND_OPCODE_MASK,\n\t\t      REG_UIC_COMMAND);\n}\n\n \nstatic int\nufshcd_wait_for_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tlockdep_assert_held(&hba->uic_cmd_mutex);\n\n\tif (wait_for_completion_timeout(&uic_cmd->done,\n\t\t\t\t\tmsecs_to_jiffies(UIC_CMD_TIMEOUT))) {\n\t\tret = uic_cmd->argument2 & MASK_UIC_COMMAND_RESULT;\n\t} else {\n\t\tret = -ETIMEDOUT;\n\t\tdev_err(hba->dev,\n\t\t\t\"uic cmd 0x%x with arg3 0x%x completion timeout\\n\",\n\t\t\tuic_cmd->command, uic_cmd->argument3);\n\n\t\tif (!uic_cmd->cmd_active) {\n\t\t\tdev_err(hba->dev, \"%s: UIC cmd has been completed, return the result\\n\",\n\t\t\t\t__func__);\n\t\t\tret = uic_cmd->argument2 & MASK_UIC_COMMAND_RESULT;\n\t\t}\n\t}\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->active_uic_cmd = NULL;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int\n__ufshcd_send_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd,\n\t\t      bool completion)\n{\n\tlockdep_assert_held(&hba->uic_cmd_mutex);\n\n\tif (!ufshcd_ready_for_uic_cmd(hba)) {\n\t\tdev_err(hba->dev,\n\t\t\t\"Controller not ready to accept UIC commands\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (completion)\n\t\tinit_completion(&uic_cmd->done);\n\n\tuic_cmd->cmd_active = 1;\n\tufshcd_dispatch_uic_cmd(hba, uic_cmd);\n\n\treturn 0;\n}\n\n \nint ufshcd_send_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\n{\n\tint ret;\n\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_UIC_CMD)\n\t\treturn 0;\n\n\tufshcd_hold(hba);\n\tmutex_lock(&hba->uic_cmd_mutex);\n\tufshcd_add_delay_before_dme_cmd(hba);\n\n\tret = __ufshcd_send_uic_cmd(hba, uic_cmd, true);\n\tif (!ret)\n\t\tret = ufshcd_wait_for_uic_cmd(hba, uic_cmd);\n\n\tmutex_unlock(&hba->uic_cmd_mutex);\n\n\tufshcd_release(hba);\n\treturn ret;\n}\n\n \nstatic void ufshcd_sgl_to_prdt(struct ufs_hba *hba, struct ufshcd_lrb *lrbp, int sg_entries,\n\t\t\t       struct scatterlist *sg_list)\n{\n\tstruct ufshcd_sg_entry *prd;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tif (sg_entries) {\n\n\t\tif (hba->quirks & UFSHCD_QUIRK_PRDT_BYTE_GRAN)\n\t\t\tlrbp->utr_descriptor_ptr->prd_table_length =\n\t\t\t\tcpu_to_le16(sg_entries * ufshcd_sg_entry_size(hba));\n\t\telse\n\t\t\tlrbp->utr_descriptor_ptr->prd_table_length = cpu_to_le16(sg_entries);\n\n\t\tprd = lrbp->ucd_prdt_ptr;\n\n\t\tfor_each_sg(sg_list, sg, sg_entries, i) {\n\t\t\tconst unsigned int len = sg_dma_len(sg);\n\n\t\t\t \n\t\t\tWARN_ONCE(len > SZ_256K, \"len = %#x\\n\", len);\n\t\t\tprd->size = cpu_to_le32(len - 1);\n\t\t\tprd->addr = cpu_to_le64(sg->dma_address);\n\t\t\tprd->reserved = 0;\n\t\t\tprd = (void *)prd + ufshcd_sg_entry_size(hba);\n\t\t}\n\t} else {\n\t\tlrbp->utr_descriptor_ptr->prd_table_length = 0;\n\t}\n}\n\n \nstatic int ufshcd_map_sg(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\n{\n\tstruct scsi_cmnd *cmd = lrbp->cmd;\n\tint sg_segments = scsi_dma_map(cmd);\n\n\tif (sg_segments < 0)\n\t\treturn sg_segments;\n\n\tufshcd_sgl_to_prdt(hba, lrbp, sg_segments, scsi_sglist(cmd));\n\n\treturn 0;\n}\n\n \nstatic void ufshcd_enable_intr(struct ufs_hba *hba, u32 intrs)\n{\n\tu32 set = ufshcd_readl(hba, REG_INTERRUPT_ENABLE);\n\n\tif (hba->ufs_version == ufshci_version(1, 0)) {\n\t\tu32 rw;\n\t\trw = set & INTERRUPT_MASK_RW_VER_10;\n\t\tset = rw | ((set ^ intrs) & intrs);\n\t} else {\n\t\tset |= intrs;\n\t}\n\n\tufshcd_writel(hba, set, REG_INTERRUPT_ENABLE);\n}\n\n \nstatic void ufshcd_disable_intr(struct ufs_hba *hba, u32 intrs)\n{\n\tu32 set = ufshcd_readl(hba, REG_INTERRUPT_ENABLE);\n\n\tif (hba->ufs_version == ufshci_version(1, 0)) {\n\t\tu32 rw;\n\t\trw = (set & INTERRUPT_MASK_RW_VER_10) &\n\t\t\t~(intrs & INTERRUPT_MASK_RW_VER_10);\n\t\tset = rw | ((set & intrs) & ~INTERRUPT_MASK_RW_VER_10);\n\n\t} else {\n\t\tset &= ~intrs;\n\t}\n\n\tufshcd_writel(hba, set, REG_INTERRUPT_ENABLE);\n}\n\n \nstatic void ufshcd_prepare_req_desc_hdr(struct ufshcd_lrb *lrbp, u8 *upiu_flags,\n\t\t\t\t\tenum dma_data_direction cmd_dir, int ehs_length)\n{\n\tstruct utp_transfer_req_desc *req_desc = lrbp->utr_descriptor_ptr;\n\tstruct request_desc_header *h = &req_desc->header;\n\tenum utp_data_direction data_direction;\n\n\t*h = (typeof(*h)){ };\n\n\tif (cmd_dir == DMA_FROM_DEVICE) {\n\t\tdata_direction = UTP_DEVICE_TO_HOST;\n\t\t*upiu_flags = UPIU_CMD_FLAGS_READ;\n\t} else if (cmd_dir == DMA_TO_DEVICE) {\n\t\tdata_direction = UTP_HOST_TO_DEVICE;\n\t\t*upiu_flags = UPIU_CMD_FLAGS_WRITE;\n\t} else {\n\t\tdata_direction = UTP_NO_DATA_TRANSFER;\n\t\t*upiu_flags = UPIU_CMD_FLAGS_NONE;\n\t}\n\n\th->command_type = lrbp->command_type;\n\th->data_direction = data_direction;\n\th->ehs_length = ehs_length;\n\n\tif (lrbp->intr_cmd)\n\t\th->interrupt = 1;\n\n\t \n\tufshcd_prepare_req_desc_hdr_crypto(lrbp, h);\n\n\t \n\th->ocs = OCS_INVALID_COMMAND_STATUS;\n\n\treq_desc->prd_table_length = 0;\n}\n\n \nstatic\nvoid ufshcd_prepare_utp_scsi_cmd_upiu(struct ufshcd_lrb *lrbp, u8 upiu_flags)\n{\n\tstruct scsi_cmnd *cmd = lrbp->cmd;\n\tstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\n\tunsigned short cdb_len;\n\n\tucd_req_ptr->header = (struct utp_upiu_header){\n\t\t.transaction_code = UPIU_TRANSACTION_COMMAND,\n\t\t.flags = upiu_flags,\n\t\t.lun = lrbp->lun,\n\t\t.task_tag = lrbp->task_tag,\n\t\t.command_set_type = UPIU_COMMAND_SET_TYPE_SCSI,\n\t};\n\n\tucd_req_ptr->sc.exp_data_transfer_len = cpu_to_be32(cmd->sdb.length);\n\n\tcdb_len = min_t(unsigned short, cmd->cmd_len, UFS_CDB_SIZE);\n\tmemset(ucd_req_ptr->sc.cdb, 0, UFS_CDB_SIZE);\n\tmemcpy(ucd_req_ptr->sc.cdb, cmd->cmnd, cdb_len);\n\n\tmemset(lrbp->ucd_rsp_ptr, 0, sizeof(struct utp_upiu_rsp));\n}\n\n \nstatic void ufshcd_prepare_utp_query_req_upiu(struct ufs_hba *hba,\n\t\t\t\tstruct ufshcd_lrb *lrbp, u8 upiu_flags)\n{\n\tstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\n\tstruct ufs_query *query = &hba->dev_cmd.query;\n\tu16 len = be16_to_cpu(query->request.upiu_req.length);\n\n\t \n\tucd_req_ptr->header = (struct utp_upiu_header){\n\t\t.transaction_code = UPIU_TRANSACTION_QUERY_REQ,\n\t\t.flags = upiu_flags,\n\t\t.lun = lrbp->lun,\n\t\t.task_tag = lrbp->task_tag,\n\t\t.query_function = query->request.query_func,\n\t\t \n\t\t.data_segment_length =\n\t\t\tquery->request.upiu_req.opcode ==\n\t\t\t\t\tUPIU_QUERY_OPCODE_WRITE_DESC ?\n\t\t\t\tcpu_to_be16(len) :\n\t\t\t\t0,\n\t};\n\n\t \n\tmemcpy(&ucd_req_ptr->qr, &query->request.upiu_req,\n\t\t\tQUERY_OSF_SIZE);\n\n\t \n\tif (query->request.upiu_req.opcode == UPIU_QUERY_OPCODE_WRITE_DESC)\n\t\tmemcpy(ucd_req_ptr + 1, query->descriptor, len);\n\n\tmemset(lrbp->ucd_rsp_ptr, 0, sizeof(struct utp_upiu_rsp));\n}\n\nstatic inline void ufshcd_prepare_utp_nop_upiu(struct ufshcd_lrb *lrbp)\n{\n\tstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\n\n\tmemset(ucd_req_ptr, 0, sizeof(struct utp_upiu_req));\n\n\tucd_req_ptr->header = (struct utp_upiu_header){\n\t\t.transaction_code = UPIU_TRANSACTION_NOP_OUT,\n\t\t.task_tag = lrbp->task_tag,\n\t};\n\n\tmemset(lrbp->ucd_rsp_ptr, 0, sizeof(struct utp_upiu_rsp));\n}\n\n \nstatic int ufshcd_compose_devman_upiu(struct ufs_hba *hba,\n\t\t\t\t      struct ufshcd_lrb *lrbp)\n{\n\tu8 upiu_flags;\n\tint ret = 0;\n\n\tif (hba->ufs_version <= ufshci_version(1, 1))\n\t\tlrbp->command_type = UTP_CMD_TYPE_DEV_MANAGE;\n\telse\n\t\tlrbp->command_type = UTP_CMD_TYPE_UFS_STORAGE;\n\n\tufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, DMA_NONE, 0);\n\tif (hba->dev_cmd.type == DEV_CMD_TYPE_QUERY)\n\t\tufshcd_prepare_utp_query_req_upiu(hba, lrbp, upiu_flags);\n\telse if (hba->dev_cmd.type == DEV_CMD_TYPE_NOP)\n\t\tufshcd_prepare_utp_nop_upiu(lrbp);\n\telse\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\n\n \nstatic int ufshcd_comp_scsi_upiu(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\n{\n\tu8 upiu_flags;\n\tint ret = 0;\n\n\tif (hba->ufs_version <= ufshci_version(1, 1))\n\t\tlrbp->command_type = UTP_CMD_TYPE_SCSI;\n\telse\n\t\tlrbp->command_type = UTP_CMD_TYPE_UFS_STORAGE;\n\n\tif (likely(lrbp->cmd)) {\n\t\tufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, lrbp->cmd->sc_data_direction, 0);\n\t\tufshcd_prepare_utp_scsi_cmd_upiu(lrbp, upiu_flags);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\n \nstatic inline u16 ufshcd_upiu_wlun_to_scsi_wlun(u8 upiu_wlun_id)\n{\n\treturn (upiu_wlun_id & ~UFS_UPIU_WLUN_ID) | SCSI_W_LUN_BASE;\n}\n\nstatic inline bool is_device_wlun(struct scsi_device *sdev)\n{\n\treturn sdev->lun ==\n\t\tufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_UFS_DEVICE_WLUN);\n}\n\n \nstatic void ufshcd_map_queues(struct Scsi_Host *shost)\n{\n\tstruct ufs_hba *hba = shost_priv(shost);\n\tint i, queue_offset = 0;\n\n\tif (!is_mcq_supported(hba)) {\n\t\thba->nr_queues[HCTX_TYPE_DEFAULT] = 1;\n\t\thba->nr_queues[HCTX_TYPE_READ] = 0;\n\t\thba->nr_queues[HCTX_TYPE_POLL] = 1;\n\t\thba->nr_hw_queues = 1;\n\t}\n\n\tfor (i = 0; i < shost->nr_maps; i++) {\n\t\tstruct blk_mq_queue_map *map = &shost->tag_set.map[i];\n\n\t\tmap->nr_queues = hba->nr_queues[i];\n\t\tif (!map->nr_queues)\n\t\t\tcontinue;\n\t\tmap->queue_offset = queue_offset;\n\t\tif (i == HCTX_TYPE_POLL && !is_mcq_supported(hba))\n\t\t\tmap->queue_offset = 0;\n\n\t\tblk_mq_map_queues(map);\n\t\tqueue_offset += map->nr_queues;\n\t}\n}\n\nstatic void ufshcd_init_lrb(struct ufs_hba *hba, struct ufshcd_lrb *lrb, int i)\n{\n\tstruct utp_transfer_cmd_desc *cmd_descp = (void *)hba->ucdl_base_addr +\n\t\ti * ufshcd_get_ucd_size(hba);\n\tstruct utp_transfer_req_desc *utrdlp = hba->utrdl_base_addr;\n\tdma_addr_t cmd_desc_element_addr = hba->ucdl_dma_addr +\n\t\ti * ufshcd_get_ucd_size(hba);\n\tu16 response_offset = offsetof(struct utp_transfer_cmd_desc,\n\t\t\t\t       response_upiu);\n\tu16 prdt_offset = offsetof(struct utp_transfer_cmd_desc, prd_table);\n\n\tlrb->utr_descriptor_ptr = utrdlp + i;\n\tlrb->utrd_dma_addr = hba->utrdl_dma_addr +\n\t\ti * sizeof(struct utp_transfer_req_desc);\n\tlrb->ucd_req_ptr = (struct utp_upiu_req *)cmd_descp->command_upiu;\n\tlrb->ucd_req_dma_addr = cmd_desc_element_addr;\n\tlrb->ucd_rsp_ptr = (struct utp_upiu_rsp *)cmd_descp->response_upiu;\n\tlrb->ucd_rsp_dma_addr = cmd_desc_element_addr + response_offset;\n\tlrb->ucd_prdt_ptr = (struct ufshcd_sg_entry *)cmd_descp->prd_table;\n\tlrb->ucd_prdt_dma_addr = cmd_desc_element_addr + prdt_offset;\n}\n\n \nstatic int ufshcd_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *cmd)\n{\n\tstruct ufs_hba *hba = shost_priv(host);\n\tint tag = scsi_cmd_to_rq(cmd)->tag;\n\tstruct ufshcd_lrb *lrbp;\n\tint err = 0;\n\tstruct ufs_hw_queue *hwq = NULL;\n\n\tWARN_ONCE(tag < 0 || tag >= hba->nutrs, \"Invalid tag %d\\n\", tag);\n\n\tswitch (hba->ufshcd_state) {\n\tcase UFSHCD_STATE_OPERATIONAL:\n\t\tbreak;\n\tcase UFSHCD_STATE_EH_SCHEDULED_NON_FATAL:\n\t\t \n\t\tif (ufshcd_eh_in_progress(hba)) {\n\t\t\terr = SCSI_MLQUEUE_HOST_BUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase UFSHCD_STATE_EH_SCHEDULED_FATAL:\n\t\t \n\t\tif (hba->pm_op_in_progress) {\n\t\t\thba->force_reset = true;\n\t\t\tset_host_byte(cmd, DID_BAD_TARGET);\n\t\t\tscsi_done(cmd);\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\n\tcase UFSHCD_STATE_RESET:\n\t\terr = SCSI_MLQUEUE_HOST_BUSY;\n\t\tgoto out;\n\tcase UFSHCD_STATE_ERROR:\n\t\tset_host_byte(cmd, DID_ERROR);\n\t\tscsi_done(cmd);\n\t\tgoto out;\n\t}\n\n\thba->req_abort_count = 0;\n\n\tufshcd_hold(hba);\n\n\tlrbp = &hba->lrb[tag];\n\tlrbp->cmd = cmd;\n\tlrbp->task_tag = tag;\n\tlrbp->lun = ufshcd_scsi_to_upiu_lun(cmd->device->lun);\n\tlrbp->intr_cmd = !ufshcd_is_intr_aggr_allowed(hba);\n\n\tufshcd_prepare_lrbp_crypto(scsi_cmd_to_rq(cmd), lrbp);\n\n\tlrbp->req_abort_skip = false;\n\n\tufshcd_comp_scsi_upiu(hba, lrbp);\n\n\terr = ufshcd_map_sg(hba, lrbp);\n\tif (err) {\n\t\tufshcd_release(hba);\n\t\tgoto out;\n\t}\n\n\tif (is_mcq_enabled(hba))\n\t\thwq = ufshcd_mcq_req_to_hwq(hba, scsi_cmd_to_rq(cmd));\n\n\tufshcd_send_command(hba, tag, hwq);\n\nout:\n\tif (ufs_trigger_eh()) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tufshcd_schedule_eh_work(hba);\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t}\n\n\treturn err;\n}\n\nstatic int ufshcd_compose_dev_cmd(struct ufs_hba *hba,\n\t\tstruct ufshcd_lrb *lrbp, enum dev_cmd_type cmd_type, int tag)\n{\n\tlrbp->cmd = NULL;\n\tlrbp->task_tag = tag;\n\tlrbp->lun = 0;  \n\tlrbp->intr_cmd = true;  \n\tufshcd_prepare_lrbp_crypto(NULL, lrbp);\n\thba->dev_cmd.type = cmd_type;\n\n\treturn ufshcd_compose_devman_upiu(hba, lrbp);\n}\n\n \nbool ufshcd_cmd_inflight(struct scsi_cmnd *cmd)\n{\n\tstruct request *rq;\n\n\tif (!cmd)\n\t\treturn false;\n\n\trq = scsi_cmd_to_rq(cmd);\n\tif (!blk_mq_request_started(rq))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic int ufshcd_clear_cmd(struct ufs_hba *hba, u32 task_tag)\n{\n\tu32 mask = 1U << task_tag;\n\tunsigned long flags;\n\tint err;\n\n\tif (is_mcq_enabled(hba)) {\n\t\t \n\t\terr = ufshcd_mcq_sq_cleanup(hba, task_tag);\n\t\tif (err) {\n\t\t\tdev_err(hba->dev, \"%s: failed tag=%d. err=%d\\n\",\n\t\t\t\t__func__, task_tag, err);\n\t\t\treturn err;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tufshcd_utrl_clear(hba, mask);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t \n\treturn ufshcd_wait_for_register(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL,\n\t\t\t\t\tmask, ~mask, 1000, 1000);\n}\n\n \nstatic int\nufshcd_dev_cmd_completion(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\n{\n\tenum upiu_response_transaction resp;\n\tint err = 0;\n\n\thba->ufs_stats.last_hibern8_exit_tstamp = ktime_set(0, 0);\n\tresp = ufshcd_get_req_rsp(lrbp->ucd_rsp_ptr);\n\n\tswitch (resp) {\n\tcase UPIU_TRANSACTION_NOP_IN:\n\t\tif (hba->dev_cmd.type != DEV_CMD_TYPE_NOP) {\n\t\t\terr = -EINVAL;\n\t\t\tdev_err(hba->dev, \"%s: unexpected response %x\\n\",\n\t\t\t\t\t__func__, resp);\n\t\t}\n\t\tbreak;\n\tcase UPIU_TRANSACTION_QUERY_RSP: {\n\t\tu8 response = lrbp->ucd_rsp_ptr->header.response;\n\n\t\tif (response == 0)\n\t\t\terr = ufshcd_copy_query_response(hba, lrbp);\n\t\tbreak;\n\t}\n\tcase UPIU_TRANSACTION_REJECT_UPIU:\n\t\t \n\t\terr = -EPERM;\n\t\tdev_err(hba->dev, \"%s: Reject UPIU not fully implemented\\n\",\n\t\t\t\t__func__);\n\t\tbreak;\n\tcase UPIU_TRANSACTION_RESPONSE:\n\t\tif (hba->dev_cmd.type != DEV_CMD_TYPE_RPMB) {\n\t\t\terr = -EINVAL;\n\t\t\tdev_err(hba->dev, \"%s: unexpected response %x\\n\", __func__, resp);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tdev_err(hba->dev, \"%s: Invalid device management cmd response: %x\\n\",\n\t\t\t\t__func__, resp);\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int ufshcd_wait_for_dev_cmd(struct ufs_hba *hba,\n\t\tstruct ufshcd_lrb *lrbp, int max_timeout)\n{\n\tunsigned long time_left = msecs_to_jiffies(max_timeout);\n\tunsigned long flags;\n\tbool pending;\n\tint err;\n\nretry:\n\ttime_left = wait_for_completion_timeout(hba->dev_cmd.complete,\n\t\t\t\t\t\ttime_left);\n\n\tif (likely(time_left)) {\n\t\t \n\t\thba->dev_cmd.complete = NULL;\n\t\terr = ufshcd_get_tr_ocs(lrbp, NULL);\n\t\tif (!err)\n\t\t\terr = ufshcd_dev_cmd_completion(hba, lrbp);\n\t} else {\n\t\terr = -ETIMEDOUT;\n\t\tdev_dbg(hba->dev, \"%s: dev_cmd request timedout, tag %d\\n\",\n\t\t\t__func__, lrbp->task_tag);\n\n\t\t \n\t\tif (is_mcq_enabled(hba)) {\n\t\t\terr = ufshcd_clear_cmd(hba, lrbp->task_tag);\n\t\t\thba->dev_cmd.complete = NULL;\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tif (ufshcd_clear_cmd(hba, lrbp->task_tag) == 0) {\n\t\t\t \n\t\t\terr = -EAGAIN;\n\t\t\t \n\t\t\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\t\t\tpending = test_bit(lrbp->task_tag,\n\t\t\t\t\t   &hba->outstanding_reqs);\n\t\t\tif (pending) {\n\t\t\t\thba->dev_cmd.complete = NULL;\n\t\t\t\t__clear_bit(lrbp->task_tag,\n\t\t\t\t\t    &hba->outstanding_reqs);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\t\t\tif (!pending) {\n\t\t\t\t \n\t\t\t\ttime_left = 1;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t} else {\n\t\t\tdev_err(hba->dev, \"%s: failed to clear tag %d\\n\",\n\t\t\t\t__func__, lrbp->task_tag);\n\n\t\t\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\t\t\tpending = test_bit(lrbp->task_tag,\n\t\t\t\t\t   &hba->outstanding_reqs);\n\t\t\tif (pending)\n\t\t\t\thba->dev_cmd.complete = NULL;\n\t\t\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\t\t\tif (!pending) {\n\t\t\t\t \n\t\t\t\ttime_left = 1;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nstatic int ufshcd_exec_dev_cmd(struct ufs_hba *hba,\n\t\tenum dev_cmd_type cmd_type, int timeout)\n{\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\tconst u32 tag = hba->reserved_slot;\n\tstruct ufshcd_lrb *lrbp;\n\tint err;\n\n\t \n\tlockdep_assert_held(&hba->dev_cmd.lock);\n\n\tdown_read(&hba->clk_scaling_lock);\n\n\tlrbp = &hba->lrb[tag];\n\tlrbp->cmd = NULL;\n\terr = ufshcd_compose_dev_cmd(hba, lrbp, cmd_type, tag);\n\tif (unlikely(err))\n\t\tgoto out;\n\n\thba->dev_cmd.complete = &wait;\n\n\tufshcd_add_query_upiu_trace(hba, UFS_QUERY_SEND, lrbp->ucd_req_ptr);\n\n\tufshcd_send_command(hba, tag, hba->dev_cmd_queue);\n\terr = ufshcd_wait_for_dev_cmd(hba, lrbp, timeout);\n\tufshcd_add_query_upiu_trace(hba, err ? UFS_QUERY_ERR : UFS_QUERY_COMP,\n\t\t\t\t    (struct utp_upiu_req *)lrbp->ucd_rsp_ptr);\n\nout:\n\tup_read(&hba->clk_scaling_lock);\n\treturn err;\n}\n\n \nstatic inline void ufshcd_init_query(struct ufs_hba *hba,\n\t\tstruct ufs_query_req **request, struct ufs_query_res **response,\n\t\tenum query_opcode opcode, u8 idn, u8 index, u8 selector)\n{\n\t*request = &hba->dev_cmd.query.request;\n\t*response = &hba->dev_cmd.query.response;\n\tmemset(*request, 0, sizeof(struct ufs_query_req));\n\tmemset(*response, 0, sizeof(struct ufs_query_res));\n\t(*request)->upiu_req.opcode = opcode;\n\t(*request)->upiu_req.idn = idn;\n\t(*request)->upiu_req.index = index;\n\t(*request)->upiu_req.selector = selector;\n}\n\nstatic int ufshcd_query_flag_retry(struct ufs_hba *hba,\n\tenum query_opcode opcode, enum flag_idn idn, u8 index, bool *flag_res)\n{\n\tint ret;\n\tint retries;\n\n\tfor (retries = 0; retries < QUERY_REQ_RETRIES; retries++) {\n\t\tret = ufshcd_query_flag(hba, opcode, idn, index, flag_res);\n\t\tif (ret)\n\t\t\tdev_dbg(hba->dev,\n\t\t\t\t\"%s: failed with error %d, retries %d\\n\",\n\t\t\t\t__func__, ret, retries);\n\t\telse\n\t\t\tbreak;\n\t}\n\n\tif (ret)\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: query flag, opcode %d, idn %d, failed with error %d after %d retries\\n\",\n\t\t\t__func__, opcode, idn, ret, retries);\n\treturn ret;\n}\n\n \nint ufshcd_query_flag(struct ufs_hba *hba, enum query_opcode opcode,\n\t\t\tenum flag_idn idn, u8 index, bool *flag_res)\n{\n\tstruct ufs_query_req *request = NULL;\n\tstruct ufs_query_res *response = NULL;\n\tint err, selector = 0;\n\tint timeout = QUERY_REQ_TIMEOUT;\n\n\tBUG_ON(!hba);\n\n\tufshcd_hold(hba);\n\tmutex_lock(&hba->dev_cmd.lock);\n\tufshcd_init_query(hba, &request, &response, opcode, idn, index,\n\t\t\tselector);\n\n\tswitch (opcode) {\n\tcase UPIU_QUERY_OPCODE_SET_FLAG:\n\tcase UPIU_QUERY_OPCODE_CLEAR_FLAG:\n\tcase UPIU_QUERY_OPCODE_TOGGLE_FLAG:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\n\t\tbreak;\n\tcase UPIU_QUERY_OPCODE_READ_FLAG:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\n\t\tif (!flag_res) {\n\t\t\t \n\t\t\tdev_err(hba->dev, \"%s: Invalid argument for read request\\n\",\n\t\t\t\t\t__func__);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Expected query flag opcode but got = %d\\n\",\n\t\t\t__func__, opcode);\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\terr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, timeout);\n\n\tif (err) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Sending flag query for idn %d failed, err = %d\\n\",\n\t\t\t__func__, idn, err);\n\t\tgoto out_unlock;\n\t}\n\n\tif (flag_res)\n\t\t*flag_res = (be32_to_cpu(response->upiu_res.value) &\n\t\t\t\tMASK_QUERY_UPIU_FLAG_LOC) & 0x1;\n\nout_unlock:\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n\treturn err;\n}\n\n \nint ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,\n\t\t      enum attr_idn idn, u8 index, u8 selector, u32 *attr_val)\n{\n\tstruct ufs_query_req *request = NULL;\n\tstruct ufs_query_res *response = NULL;\n\tint err;\n\n\tBUG_ON(!hba);\n\n\tif (!attr_val) {\n\t\tdev_err(hba->dev, \"%s: attribute value required for opcode 0x%x\\n\",\n\t\t\t\t__func__, opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tufshcd_hold(hba);\n\n\tmutex_lock(&hba->dev_cmd.lock);\n\tufshcd_init_query(hba, &request, &response, opcode, idn, index,\n\t\t\tselector);\n\n\tswitch (opcode) {\n\tcase UPIU_QUERY_OPCODE_WRITE_ATTR:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\n\t\trequest->upiu_req.value = cpu_to_be32(*attr_val);\n\t\tbreak;\n\tcase UPIU_QUERY_OPCODE_READ_ATTR:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hba->dev, \"%s: Expected query attr opcode but got = 0x%.2x\\n\",\n\t\t\t\t__func__, opcode);\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\terr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\n\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: opcode 0x%.2x for idn %d failed, index %d, err = %d\\n\",\n\t\t\t\t__func__, opcode, idn, index, err);\n\t\tgoto out_unlock;\n\t}\n\n\t*attr_val = be32_to_cpu(response->upiu_res.value);\n\nout_unlock:\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n\treturn err;\n}\n\n \nint ufshcd_query_attr_retry(struct ufs_hba *hba,\n\tenum query_opcode opcode, enum attr_idn idn, u8 index, u8 selector,\n\tu32 *attr_val)\n{\n\tint ret = 0;\n\tu32 retries;\n\n\tfor (retries = QUERY_REQ_RETRIES; retries > 0; retries--) {\n\t\tret = ufshcd_query_attr(hba, opcode, idn, index,\n\t\t\t\t\t\tselector, attr_val);\n\t\tif (ret)\n\t\t\tdev_dbg(hba->dev, \"%s: failed with error %d, retries %d\\n\",\n\t\t\t\t__func__, ret, retries);\n\t\telse\n\t\t\tbreak;\n\t}\n\n\tif (ret)\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: query attribute, idn %d, failed with error %d after %d retries\\n\",\n\t\t\t__func__, idn, ret, QUERY_REQ_RETRIES);\n\treturn ret;\n}\n\nstatic int __ufshcd_query_descriptor(struct ufs_hba *hba,\n\t\t\tenum query_opcode opcode, enum desc_idn idn, u8 index,\n\t\t\tu8 selector, u8 *desc_buf, int *buf_len)\n{\n\tstruct ufs_query_req *request = NULL;\n\tstruct ufs_query_res *response = NULL;\n\tint err;\n\n\tBUG_ON(!hba);\n\n\tif (!desc_buf) {\n\t\tdev_err(hba->dev, \"%s: descriptor buffer required for opcode 0x%x\\n\",\n\t\t\t\t__func__, opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (*buf_len < QUERY_DESC_MIN_SIZE || *buf_len > QUERY_DESC_MAX_SIZE) {\n\t\tdev_err(hba->dev, \"%s: descriptor buffer size (%d) is out of range\\n\",\n\t\t\t\t__func__, *buf_len);\n\t\treturn -EINVAL;\n\t}\n\n\tufshcd_hold(hba);\n\n\tmutex_lock(&hba->dev_cmd.lock);\n\tufshcd_init_query(hba, &request, &response, opcode, idn, index,\n\t\t\tselector);\n\thba->dev_cmd.query.descriptor = desc_buf;\n\trequest->upiu_req.length = cpu_to_be16(*buf_len);\n\n\tswitch (opcode) {\n\tcase UPIU_QUERY_OPCODE_WRITE_DESC:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\n\t\tbreak;\n\tcase UPIU_QUERY_OPCODE_READ_DESC:\n\t\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hba->dev,\n\t\t\t\t\"%s: Expected query descriptor opcode but got = 0x%.2x\\n\",\n\t\t\t\t__func__, opcode);\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\terr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\n\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: opcode 0x%.2x for idn %d failed, index %d, err = %d\\n\",\n\t\t\t\t__func__, opcode, idn, index, err);\n\t\tgoto out_unlock;\n\t}\n\n\t*buf_len = be16_to_cpu(response->upiu_res.length);\n\nout_unlock:\n\thba->dev_cmd.query.descriptor = NULL;\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n\treturn err;\n}\n\n \nint ufshcd_query_descriptor_retry(struct ufs_hba *hba,\n\t\t\t\t  enum query_opcode opcode,\n\t\t\t\t  enum desc_idn idn, u8 index,\n\t\t\t\t  u8 selector,\n\t\t\t\t  u8 *desc_buf, int *buf_len)\n{\n\tint err;\n\tint retries;\n\n\tfor (retries = QUERY_REQ_RETRIES; retries > 0; retries--) {\n\t\terr = __ufshcd_query_descriptor(hba, opcode, idn, index,\n\t\t\t\t\t\tselector, desc_buf, buf_len);\n\t\tif (!err || err == -EINVAL)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nint ufshcd_read_desc_param(struct ufs_hba *hba,\n\t\t\t   enum desc_idn desc_id,\n\t\t\t   int desc_index,\n\t\t\t   u8 param_offset,\n\t\t\t   u8 *param_read_buf,\n\t\t\t   u8 param_size)\n{\n\tint ret;\n\tu8 *desc_buf;\n\tint buff_len = QUERY_DESC_MAX_SIZE;\n\tbool is_kmalloc = true;\n\n\t \n\tif (desc_id >= QUERY_DESC_IDN_MAX || !param_size)\n\t\treturn -EINVAL;\n\n\t \n\tif (param_offset != 0 || param_size < buff_len) {\n\t\tdesc_buf = kzalloc(buff_len, GFP_KERNEL);\n\t\tif (!desc_buf)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tdesc_buf = param_read_buf;\n\t\tis_kmalloc = false;\n\t}\n\n\t \n\tret = ufshcd_query_descriptor_retry(hba, UPIU_QUERY_OPCODE_READ_DESC,\n\t\t\t\t\t    desc_id, desc_index, 0,\n\t\t\t\t\t    desc_buf, &buff_len);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: Failed reading descriptor. desc_id %d, desc_index %d, param_offset %d, ret %d\\n\",\n\t\t\t__func__, desc_id, desc_index, param_offset, ret);\n\t\tgoto out;\n\t}\n\n\t \n\tbuff_len = desc_buf[QUERY_DESC_LENGTH_OFFSET];\n\n\tif (param_offset >= buff_len) {\n\t\tdev_err(hba->dev, \"%s: Invalid offset 0x%x in descriptor IDN 0x%x, length 0x%x\\n\",\n\t\t\t__func__, param_offset, desc_id, buff_len);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (desc_buf[QUERY_DESC_DESC_TYPE_OFFSET] != desc_id) {\n\t\tdev_err(hba->dev, \"%s: invalid desc_id %d in descriptor header\\n\",\n\t\t\t__func__, desc_buf[QUERY_DESC_DESC_TYPE_OFFSET]);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (is_kmalloc) {\n\t\t \n\t\tif (param_offset >= buff_len)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tmemcpy(param_read_buf, &desc_buf[param_offset],\n\t\t\t       min_t(u32, param_size, buff_len - param_offset));\n\t}\nout:\n\tif (is_kmalloc)\n\t\tkfree(desc_buf);\n\treturn ret;\n}\n\n \nstruct uc_string_id {\n\tu8 len;\n\tu8 type;\n\twchar_t uc[];\n} __packed;\n\n \nstatic inline char ufshcd_remove_non_printable(u8 ch)\n{\n\treturn (ch >= 0x20 && ch <= 0x7e) ? ch : ' ';\n}\n\n \nint ufshcd_read_string_desc(struct ufs_hba *hba, u8 desc_index,\n\t\t\t    u8 **buf, bool ascii)\n{\n\tstruct uc_string_id *uc_str;\n\tu8 *str;\n\tint ret;\n\n\tif (!buf)\n\t\treturn -EINVAL;\n\n\tuc_str = kzalloc(QUERY_DESC_MAX_SIZE, GFP_KERNEL);\n\tif (!uc_str)\n\t\treturn -ENOMEM;\n\n\tret = ufshcd_read_desc_param(hba, QUERY_DESC_IDN_STRING, desc_index, 0,\n\t\t\t\t     (u8 *)uc_str, QUERY_DESC_MAX_SIZE);\n\tif (ret < 0) {\n\t\tdev_err(hba->dev, \"Reading String Desc failed after %d retries. err = %d\\n\",\n\t\t\tQUERY_REQ_RETRIES, ret);\n\t\tstr = NULL;\n\t\tgoto out;\n\t}\n\n\tif (uc_str->len <= QUERY_DESC_HDR_SIZE) {\n\t\tdev_dbg(hba->dev, \"String Desc is of zero length\\n\");\n\t\tstr = NULL;\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (ascii) {\n\t\tssize_t ascii_len;\n\t\tint i;\n\t\t \n\t\tascii_len = (uc_str->len - QUERY_DESC_HDR_SIZE) / 2 + 1;\n\t\tstr = kzalloc(ascii_len, GFP_KERNEL);\n\t\tif (!str) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tret = utf16s_to_utf8s(uc_str->uc,\n\t\t\t\t      uc_str->len - QUERY_DESC_HDR_SIZE,\n\t\t\t\t      UTF16_BIG_ENDIAN, str, ascii_len - 1);\n\n\t\t \n\t\tfor (i = 0; i < ret; i++)\n\t\t\tstr[i] = ufshcd_remove_non_printable(str[i]);\n\n\t\tstr[ret++] = '\\0';\n\n\t} else {\n\t\tstr = kmemdup(uc_str, uc_str->len, GFP_KERNEL);\n\t\tif (!str) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = uc_str->len;\n\t}\nout:\n\t*buf = str;\n\tkfree(uc_str);\n\treturn ret;\n}\n\n \nstatic inline int ufshcd_read_unit_desc_param(struct ufs_hba *hba,\n\t\t\t\t\t      int lun,\n\t\t\t\t\t      enum unit_desc_param param_offset,\n\t\t\t\t\t      u8 *param_read_buf,\n\t\t\t\t\t      u32 param_size)\n{\n\t \n\tif (!ufs_is_valid_unit_desc_lun(&hba->dev_info, lun))\n\t\treturn -EOPNOTSUPP;\n\n\treturn ufshcd_read_desc_param(hba, QUERY_DESC_IDN_UNIT, lun,\n\t\t\t\t      param_offset, param_read_buf, param_size);\n}\n\nstatic int ufshcd_get_ref_clk_gating_wait(struct ufs_hba *hba)\n{\n\tint err = 0;\n\tu32 gating_wait = UFSHCD_REF_CLK_GATING_WAIT_US;\n\n\tif (hba->dev_info.wspecversion >= 0x300) {\n\t\terr = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\tQUERY_ATTR_IDN_REF_CLK_GATING_WAIT_TIME, 0, 0,\n\t\t\t\t&gating_wait);\n\t\tif (err)\n\t\t\tdev_err(hba->dev, \"Failed reading bRefClkGatingWait. err = %d, use default %uus\\n\",\n\t\t\t\t\t err, gating_wait);\n\n\t\tif (gating_wait == 0) {\n\t\t\tgating_wait = UFSHCD_REF_CLK_GATING_WAIT_US;\n\t\t\tdev_err(hba->dev, \"Undefined ref clk gating wait time, use default %uus\\n\",\n\t\t\t\t\t gating_wait);\n\t\t}\n\n\t\thba->dev_info.clk_gating_wait_us = gating_wait;\n\t}\n\n\treturn err;\n}\n\n \nstatic int ufshcd_memory_alloc(struct ufs_hba *hba)\n{\n\tsize_t utmrdl_size, utrdl_size, ucdl_size;\n\n\t \n\tucdl_size = ufshcd_get_ucd_size(hba) * hba->nutrs;\n\thba->ucdl_base_addr = dmam_alloc_coherent(hba->dev,\n\t\t\t\t\t\t  ucdl_size,\n\t\t\t\t\t\t  &hba->ucdl_dma_addr,\n\t\t\t\t\t\t  GFP_KERNEL);\n\n\t \n\tif (!hba->ucdl_base_addr ||\n\t    WARN_ON(hba->ucdl_dma_addr & (128 - 1))) {\n\t\tdev_err(hba->dev,\n\t\t\t\"Command Descriptor Memory allocation failed\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tutrdl_size = (sizeof(struct utp_transfer_req_desc) * hba->nutrs);\n\thba->utrdl_base_addr = dmam_alloc_coherent(hba->dev,\n\t\t\t\t\t\t   utrdl_size,\n\t\t\t\t\t\t   &hba->utrdl_dma_addr,\n\t\t\t\t\t\t   GFP_KERNEL);\n\tif (!hba->utrdl_base_addr ||\n\t    WARN_ON(hba->utrdl_dma_addr & (SZ_1K - 1))) {\n\t\tdev_err(hba->dev,\n\t\t\t\"Transfer Descriptor Memory allocation failed\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tif (hba->utmrdl_base_addr)\n\t\tgoto skip_utmrdl;\n\t \n\tutmrdl_size = sizeof(struct utp_task_req_desc) * hba->nutmrs;\n\thba->utmrdl_base_addr = dmam_alloc_coherent(hba->dev,\n\t\t\t\t\t\t    utmrdl_size,\n\t\t\t\t\t\t    &hba->utmrdl_dma_addr,\n\t\t\t\t\t\t    GFP_KERNEL);\n\tif (!hba->utmrdl_base_addr ||\n\t    WARN_ON(hba->utmrdl_dma_addr & (SZ_1K - 1))) {\n\t\tdev_err(hba->dev,\n\t\t\"Task Management Descriptor Memory allocation failed\\n\");\n\t\tgoto out;\n\t}\n\nskip_utmrdl:\n\t \n\thba->lrb = devm_kcalloc(hba->dev,\n\t\t\t\thba->nutrs, sizeof(struct ufshcd_lrb),\n\t\t\t\tGFP_KERNEL);\n\tif (!hba->lrb) {\n\t\tdev_err(hba->dev, \"LRB Memory allocation failed\\n\");\n\t\tgoto out;\n\t}\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}\n\n \nstatic void ufshcd_host_memory_configure(struct ufs_hba *hba)\n{\n\tstruct utp_transfer_req_desc *utrdlp;\n\tdma_addr_t cmd_desc_dma_addr;\n\tdma_addr_t cmd_desc_element_addr;\n\tu16 response_offset;\n\tu16 prdt_offset;\n\tint cmd_desc_size;\n\tint i;\n\n\tutrdlp = hba->utrdl_base_addr;\n\n\tresponse_offset =\n\t\toffsetof(struct utp_transfer_cmd_desc, response_upiu);\n\tprdt_offset =\n\t\toffsetof(struct utp_transfer_cmd_desc, prd_table);\n\n\tcmd_desc_size = ufshcd_get_ucd_size(hba);\n\tcmd_desc_dma_addr = hba->ucdl_dma_addr;\n\n\tfor (i = 0; i < hba->nutrs; i++) {\n\t\t \n\t\tcmd_desc_element_addr =\n\t\t\t\t(cmd_desc_dma_addr + (cmd_desc_size * i));\n\t\tutrdlp[i].command_desc_base_addr =\n\t\t\t\tcpu_to_le64(cmd_desc_element_addr);\n\n\t\t \n\t\tif (hba->quirks & UFSHCD_QUIRK_PRDT_BYTE_GRAN) {\n\t\t\tutrdlp[i].response_upiu_offset =\n\t\t\t\tcpu_to_le16(response_offset);\n\t\t\tutrdlp[i].prd_table_offset =\n\t\t\t\tcpu_to_le16(prdt_offset);\n\t\t\tutrdlp[i].response_upiu_length =\n\t\t\t\tcpu_to_le16(ALIGNED_UPIU_SIZE);\n\t\t} else {\n\t\t\tutrdlp[i].response_upiu_offset =\n\t\t\t\tcpu_to_le16(response_offset >> 2);\n\t\t\tutrdlp[i].prd_table_offset =\n\t\t\t\tcpu_to_le16(prdt_offset >> 2);\n\t\t\tutrdlp[i].response_upiu_length =\n\t\t\t\tcpu_to_le16(ALIGNED_UPIU_SIZE >> 2);\n\t\t}\n\n\t\tufshcd_init_lrb(hba, &hba->lrb[i], i);\n\t}\n}\n\n \nstatic int ufshcd_dme_link_startup(struct ufs_hba *hba)\n{\n\tstruct uic_command uic_cmd = {0};\n\tint ret;\n\n\tuic_cmd.command = UIC_CMD_DME_LINK_STARTUP;\n\n\tret = ufshcd_send_uic_cmd(hba, &uic_cmd);\n\tif (ret)\n\t\tdev_dbg(hba->dev,\n\t\t\t\"dme-link-startup: error code %d\\n\", ret);\n\treturn ret;\n}\n \nstatic int ufshcd_dme_reset(struct ufs_hba *hba)\n{\n\tstruct uic_command uic_cmd = {0};\n\tint ret;\n\n\tuic_cmd.command = UIC_CMD_DME_RESET;\n\n\tret = ufshcd_send_uic_cmd(hba, &uic_cmd);\n\tif (ret)\n\t\tdev_err(hba->dev,\n\t\t\t\"dme-reset: error code %d\\n\", ret);\n\n\treturn ret;\n}\n\nint ufshcd_dme_configure_adapt(struct ufs_hba *hba,\n\t\t\t       int agreed_gear,\n\t\t\t       int adapt_val)\n{\n\tint ret;\n\n\tif (agreed_gear < UFS_HS_G4)\n\t\tadapt_val = PA_NO_ADAPT;\n\n\tret = ufshcd_dme_set(hba,\n\t\t\t     UIC_ARG_MIB(PA_TXHSADAPTTYPE),\n\t\t\t     adapt_val);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_dme_configure_adapt);\n\n \nstatic int ufshcd_dme_enable(struct ufs_hba *hba)\n{\n\tstruct uic_command uic_cmd = {0};\n\tint ret;\n\n\tuic_cmd.command = UIC_CMD_DME_ENABLE;\n\n\tret = ufshcd_send_uic_cmd(hba, &uic_cmd);\n\tif (ret)\n\t\tdev_err(hba->dev,\n\t\t\t\"dme-enable: error code %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic inline void ufshcd_add_delay_before_dme_cmd(struct ufs_hba *hba)\n{\n\t#define MIN_DELAY_BEFORE_DME_CMDS_US\t1000\n\tunsigned long min_sleep_time_us;\n\n\tif (!(hba->quirks & UFSHCD_QUIRK_DELAY_BEFORE_DME_CMDS))\n\t\treturn;\n\n\t \n\tif (unlikely(!ktime_to_us(hba->last_dme_cmd_tstamp))) {\n\t\tmin_sleep_time_us = MIN_DELAY_BEFORE_DME_CMDS_US;\n\t} else {\n\t\tunsigned long delta =\n\t\t\t(unsigned long) ktime_to_us(\n\t\t\t\tktime_sub(ktime_get(),\n\t\t\t\thba->last_dme_cmd_tstamp));\n\n\t\tif (delta < MIN_DELAY_BEFORE_DME_CMDS_US)\n\t\t\tmin_sleep_time_us =\n\t\t\t\tMIN_DELAY_BEFORE_DME_CMDS_US - delta;\n\t\telse\n\t\t\treturn;  \n\t}\n\n\t \n\tusleep_range(min_sleep_time_us, min_sleep_time_us + 50);\n}\n\n \nint ufshcd_dme_set_attr(struct ufs_hba *hba, u32 attr_sel,\n\t\t\tu8 attr_set, u32 mib_val, u8 peer)\n{\n\tstruct uic_command uic_cmd = {0};\n\tstatic const char *const action[] = {\n\t\t\"dme-set\",\n\t\t\"dme-peer-set\"\n\t};\n\tconst char *set = action[!!peer];\n\tint ret;\n\tint retries = UFS_UIC_COMMAND_RETRIES;\n\n\tuic_cmd.command = peer ?\n\t\tUIC_CMD_DME_PEER_SET : UIC_CMD_DME_SET;\n\tuic_cmd.argument1 = attr_sel;\n\tuic_cmd.argument2 = UIC_ARG_ATTR_TYPE(attr_set);\n\tuic_cmd.argument3 = mib_val;\n\n\tdo {\n\t\t \n\t\tret = ufshcd_send_uic_cmd(hba, &uic_cmd);\n\t\tif (ret)\n\t\t\tdev_dbg(hba->dev, \"%s: attr-id 0x%x val 0x%x error code %d\\n\",\n\t\t\t\tset, UIC_GET_ATTR_ID(attr_sel), mib_val, ret);\n\t} while (ret && peer && --retries);\n\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: attr-id 0x%x val 0x%x failed %d retries\\n\",\n\t\t\tset, UIC_GET_ATTR_ID(attr_sel), mib_val,\n\t\t\tUFS_UIC_COMMAND_RETRIES - retries);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_dme_set_attr);\n\n \nint ufshcd_dme_get_attr(struct ufs_hba *hba, u32 attr_sel,\n\t\t\tu32 *mib_val, u8 peer)\n{\n\tstruct uic_command uic_cmd = {0};\n\tstatic const char *const action[] = {\n\t\t\"dme-get\",\n\t\t\"dme-peer-get\"\n\t};\n\tconst char *get = action[!!peer];\n\tint ret;\n\tint retries = UFS_UIC_COMMAND_RETRIES;\n\tstruct ufs_pa_layer_attr orig_pwr_info;\n\tstruct ufs_pa_layer_attr temp_pwr_info;\n\tbool pwr_mode_change = false;\n\n\tif (peer && (hba->quirks & UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE)) {\n\t\torig_pwr_info = hba->pwr_info;\n\t\ttemp_pwr_info = orig_pwr_info;\n\n\t\tif (orig_pwr_info.pwr_tx == FAST_MODE ||\n\t\t    orig_pwr_info.pwr_rx == FAST_MODE) {\n\t\t\ttemp_pwr_info.pwr_tx = FASTAUTO_MODE;\n\t\t\ttemp_pwr_info.pwr_rx = FASTAUTO_MODE;\n\t\t\tpwr_mode_change = true;\n\t\t} else if (orig_pwr_info.pwr_tx == SLOW_MODE ||\n\t\t    orig_pwr_info.pwr_rx == SLOW_MODE) {\n\t\t\ttemp_pwr_info.pwr_tx = SLOWAUTO_MODE;\n\t\t\ttemp_pwr_info.pwr_rx = SLOWAUTO_MODE;\n\t\t\tpwr_mode_change = true;\n\t\t}\n\t\tif (pwr_mode_change) {\n\t\t\tret = ufshcd_change_power_mode(hba, &temp_pwr_info);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tuic_cmd.command = peer ?\n\t\tUIC_CMD_DME_PEER_GET : UIC_CMD_DME_GET;\n\tuic_cmd.argument1 = attr_sel;\n\n\tdo {\n\t\t \n\t\tret = ufshcd_send_uic_cmd(hba, &uic_cmd);\n\t\tif (ret)\n\t\t\tdev_dbg(hba->dev, \"%s: attr-id 0x%x error code %d\\n\",\n\t\t\t\tget, UIC_GET_ATTR_ID(attr_sel), ret);\n\t} while (ret && peer && --retries);\n\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: attr-id 0x%x failed %d retries\\n\",\n\t\t\tget, UIC_GET_ATTR_ID(attr_sel),\n\t\t\tUFS_UIC_COMMAND_RETRIES - retries);\n\n\tif (mib_val && !ret)\n\t\t*mib_val = uic_cmd.argument3;\n\n\tif (peer && (hba->quirks & UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE)\n\t    && pwr_mode_change)\n\t\tufshcd_change_power_mode(hba, &orig_pwr_info);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_dme_get_attr);\n\n \nstatic int ufshcd_uic_pwr_ctrl(struct ufs_hba *hba, struct uic_command *cmd)\n{\n\tDECLARE_COMPLETION_ONSTACK(uic_async_done);\n\tunsigned long flags;\n\tu8 status;\n\tint ret;\n\tbool reenable_intr = false;\n\n\tmutex_lock(&hba->uic_cmd_mutex);\n\tufshcd_add_delay_before_dme_cmd(hba);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (ufshcd_is_link_broken(hba)) {\n\t\tret = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\thba->uic_async_done = &uic_async_done;\n\tif (ufshcd_readl(hba, REG_INTERRUPT_ENABLE) & UIC_COMMAND_COMPL) {\n\t\tufshcd_disable_intr(hba, UIC_COMMAND_COMPL);\n\t\t \n\t\twmb();\n\t\treenable_intr = true;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tret = __ufshcd_send_uic_cmd(hba, cmd, false);\n\tif (ret) {\n\t\tdev_err(hba->dev,\n\t\t\t\"pwr ctrl cmd 0x%x with mode 0x%x uic error %d\\n\",\n\t\t\tcmd->command, cmd->argument3, ret);\n\t\tgoto out;\n\t}\n\n\tif (!wait_for_completion_timeout(hba->uic_async_done,\n\t\t\t\t\t msecs_to_jiffies(UIC_CMD_TIMEOUT))) {\n\t\tdev_err(hba->dev,\n\t\t\t\"pwr ctrl cmd 0x%x with mode 0x%x completion timeout\\n\",\n\t\t\tcmd->command, cmd->argument3);\n\n\t\tif (!cmd->cmd_active) {\n\t\t\tdev_err(hba->dev, \"%s: Power Mode Change operation has been completed, go check UPMCRS\\n\",\n\t\t\t\t__func__);\n\t\t\tgoto check_upmcrs;\n\t\t}\n\n\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\ncheck_upmcrs:\n\tstatus = ufshcd_get_upmcrs(hba);\n\tif (status != PWR_LOCAL) {\n\t\tdev_err(hba->dev,\n\t\t\t\"pwr ctrl cmd 0x%x failed, host upmcrs:0x%x\\n\",\n\t\t\tcmd->command, status);\n\t\tret = (status != PWR_OK) ? status : -1;\n\t}\nout:\n\tif (ret) {\n\t\tufshcd_print_host_state(hba);\n\t\tufshcd_print_pwr_info(hba);\n\t\tufshcd_print_evt_hist(hba);\n\t}\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->active_uic_cmd = NULL;\n\thba->uic_async_done = NULL;\n\tif (reenable_intr)\n\t\tufshcd_enable_intr(hba, UIC_COMMAND_COMPL);\n\tif (ret) {\n\t\tufshcd_set_link_broken(hba);\n\t\tufshcd_schedule_eh_work(hba);\n\t}\nout_unlock:\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tmutex_unlock(&hba->uic_cmd_mutex);\n\n\treturn ret;\n}\n\n \nint ufshcd_uic_change_pwr_mode(struct ufs_hba *hba, u8 mode)\n{\n\tstruct uic_command uic_cmd = {0};\n\tint ret;\n\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_PA_RXHSUNTERMCAP) {\n\t\tret = ufshcd_dme_set(hba,\n\t\t\t\tUIC_ARG_MIB_SEL(PA_RXHSUNTERMCAP, 0), 1);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: failed to enable PA_RXHSUNTERMCAP ret %d\\n\",\n\t\t\t\t\t\t__func__, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tuic_cmd.command = UIC_CMD_DME_SET;\n\tuic_cmd.argument1 = UIC_ARG_MIB(PA_PWRMODE);\n\tuic_cmd.argument3 = mode;\n\tufshcd_hold(hba);\n\tret = ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\n\tufshcd_release(hba);\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_uic_change_pwr_mode);\n\nint ufshcd_link_recovery(struct ufs_hba *hba)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->ufshcd_state = UFSHCD_STATE_RESET;\n\tufshcd_set_eh_in_progress(hba);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t \n\tufshcd_device_reset(hba);\n\n\tret = ufshcd_host_reset_and_restore(hba);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (ret)\n\t\thba->ufshcd_state = UFSHCD_STATE_ERROR;\n\tufshcd_clear_eh_in_progress(hba);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: link recovery failed, err %d\",\n\t\t\t__func__, ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_link_recovery);\n\nint ufshcd_uic_hibern8_enter(struct ufs_hba *hba)\n{\n\tint ret;\n\tstruct uic_command uic_cmd = {0};\n\tktime_t start = ktime_get();\n\n\tufshcd_vops_hibern8_notify(hba, UIC_CMD_DME_HIBER_ENTER, PRE_CHANGE);\n\n\tuic_cmd.command = UIC_CMD_DME_HIBER_ENTER;\n\tret = ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\n\ttrace_ufshcd_profile_hibern8(dev_name(hba->dev), \"enter\",\n\t\t\t     ktime_to_us(ktime_sub(ktime_get(), start)), ret);\n\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: hibern8 enter failed. ret = %d\\n\",\n\t\t\t__func__, ret);\n\telse\n\t\tufshcd_vops_hibern8_notify(hba, UIC_CMD_DME_HIBER_ENTER,\n\t\t\t\t\t\t\t\tPOST_CHANGE);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_uic_hibern8_enter);\n\nint ufshcd_uic_hibern8_exit(struct ufs_hba *hba)\n{\n\tstruct uic_command uic_cmd = {0};\n\tint ret;\n\tktime_t start = ktime_get();\n\n\tufshcd_vops_hibern8_notify(hba, UIC_CMD_DME_HIBER_EXIT, PRE_CHANGE);\n\n\tuic_cmd.command = UIC_CMD_DME_HIBER_EXIT;\n\tret = ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\n\ttrace_ufshcd_profile_hibern8(dev_name(hba->dev), \"exit\",\n\t\t\t     ktime_to_us(ktime_sub(ktime_get(), start)), ret);\n\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: hibern8 exit failed. ret = %d\\n\",\n\t\t\t__func__, ret);\n\t} else {\n\t\tufshcd_vops_hibern8_notify(hba, UIC_CMD_DME_HIBER_EXIT,\n\t\t\t\t\t\t\t\tPOST_CHANGE);\n\t\thba->ufs_stats.last_hibern8_exit_tstamp = local_clock();\n\t\thba->ufs_stats.hibern8_exit_cnt++;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_uic_hibern8_exit);\n\nvoid ufshcd_auto_hibern8_update(struct ufs_hba *hba, u32 ahit)\n{\n\tunsigned long flags;\n\tbool update = false;\n\n\tif (!ufshcd_is_auto_hibern8_supported(hba))\n\t\treturn;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (hba->ahit != ahit) {\n\t\thba->ahit = ahit;\n\t\tupdate = true;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tif (update &&\n\t    !pm_runtime_suspended(&hba->ufs_device_wlun->sdev_gendev)) {\n\t\tufshcd_rpm_get_sync(hba);\n\t\tufshcd_hold(hba);\n\t\tufshcd_auto_hibern8_enable(hba);\n\t\tufshcd_release(hba);\n\t\tufshcd_rpm_put_sync(hba);\n\t}\n}\nEXPORT_SYMBOL_GPL(ufshcd_auto_hibern8_update);\n\nvoid ufshcd_auto_hibern8_enable(struct ufs_hba *hba)\n{\n\tif (!ufshcd_is_auto_hibern8_supported(hba))\n\t\treturn;\n\n\tufshcd_writel(hba, hba->ahit, REG_AUTO_HIBERNATE_IDLE_TIMER);\n}\n\n  \nstatic void ufshcd_init_pwr_info(struct ufs_hba *hba)\n{\n\thba->pwr_info.gear_rx = UFS_PWM_G1;\n\thba->pwr_info.gear_tx = UFS_PWM_G1;\n\thba->pwr_info.lane_rx = UFS_LANE_1;\n\thba->pwr_info.lane_tx = UFS_LANE_1;\n\thba->pwr_info.pwr_rx = SLOWAUTO_MODE;\n\thba->pwr_info.pwr_tx = SLOWAUTO_MODE;\n\thba->pwr_info.hs_rate = 0;\n}\n\n \nstatic int ufshcd_get_max_pwr_mode(struct ufs_hba *hba)\n{\n\tstruct ufs_pa_layer_attr *pwr_info = &hba->max_pwr_info.info;\n\n\tif (hba->max_pwr_info.is_valid)\n\t\treturn 0;\n\n\tif (hba->quirks & UFSHCD_QUIRK_HIBERN_FASTAUTO) {\n\t\tpwr_info->pwr_tx = FASTAUTO_MODE;\n\t\tpwr_info->pwr_rx = FASTAUTO_MODE;\n\t} else {\n\t\tpwr_info->pwr_tx = FAST_MODE;\n\t\tpwr_info->pwr_rx = FAST_MODE;\n\t}\n\tpwr_info->hs_rate = PA_HS_MODE_B;\n\n\t \n\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDRXDATALANES),\n\t\t\t&pwr_info->lane_rx);\n\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\n\t\t\t&pwr_info->lane_tx);\n\n\tif (!pwr_info->lane_rx || !pwr_info->lane_tx) {\n\t\tdev_err(hba->dev, \"%s: invalid connected lanes value. rx=%d, tx=%d\\n\",\n\t\t\t\t__func__,\n\t\t\t\tpwr_info->lane_rx,\n\t\t\t\tpwr_info->lane_tx);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_MAXRXHSGEAR), &pwr_info->gear_rx);\n\tif (!pwr_info->gear_rx) {\n\t\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_MAXRXPWMGEAR),\n\t\t\t\t&pwr_info->gear_rx);\n\t\tif (!pwr_info->gear_rx) {\n\t\t\tdev_err(hba->dev, \"%s: invalid max pwm rx gear read = %d\\n\",\n\t\t\t\t__func__, pwr_info->gear_rx);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpwr_info->pwr_rx = SLOW_MODE;\n\t}\n\n\tufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_MAXRXHSGEAR),\n\t\t\t&pwr_info->gear_tx);\n\tif (!pwr_info->gear_tx) {\n\t\tufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_MAXRXPWMGEAR),\n\t\t\t\t&pwr_info->gear_tx);\n\t\tif (!pwr_info->gear_tx) {\n\t\t\tdev_err(hba->dev, \"%s: invalid max pwm tx gear read = %d\\n\",\n\t\t\t\t__func__, pwr_info->gear_tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpwr_info->pwr_tx = SLOW_MODE;\n\t}\n\n\thba->max_pwr_info.is_valid = true;\n\treturn 0;\n}\n\nstatic int ufshcd_change_power_mode(struct ufs_hba *hba,\n\t\t\t     struct ufs_pa_layer_attr *pwr_mode)\n{\n\tint ret;\n\n\t \n\tif (!hba->force_pmc &&\n\t    pwr_mode->gear_rx == hba->pwr_info.gear_rx &&\n\t    pwr_mode->gear_tx == hba->pwr_info.gear_tx &&\n\t    pwr_mode->lane_rx == hba->pwr_info.lane_rx &&\n\t    pwr_mode->lane_tx == hba->pwr_info.lane_tx &&\n\t    pwr_mode->pwr_rx == hba->pwr_info.pwr_rx &&\n\t    pwr_mode->pwr_tx == hba->pwr_info.pwr_tx &&\n\t    pwr_mode->hs_rate == hba->pwr_info.hs_rate) {\n\t\tdev_dbg(hba->dev, \"%s: power already configured\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\t \n\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXGEAR), pwr_mode->gear_rx);\n\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVERXDATALANES),\n\t\t\tpwr_mode->lane_rx);\n\tif (pwr_mode->pwr_rx == FASTAUTO_MODE ||\n\t\t\tpwr_mode->pwr_rx == FAST_MODE)\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXTERMINATION), true);\n\telse\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXTERMINATION), false);\n\n\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXGEAR), pwr_mode->gear_tx);\n\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVETXDATALANES),\n\t\t\tpwr_mode->lane_tx);\n\tif (pwr_mode->pwr_tx == FASTAUTO_MODE ||\n\t\t\tpwr_mode->pwr_tx == FAST_MODE)\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXTERMINATION), true);\n\telse\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXTERMINATION), false);\n\n\tif (pwr_mode->pwr_rx == FASTAUTO_MODE ||\n\t    pwr_mode->pwr_tx == FASTAUTO_MODE ||\n\t    pwr_mode->pwr_rx == FAST_MODE ||\n\t    pwr_mode->pwr_tx == FAST_MODE)\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_HSSERIES),\n\t\t\t\t\t\tpwr_mode->hs_rate);\n\n\tif (!(hba->quirks & UFSHCD_QUIRK_SKIP_DEF_UNIPRO_TIMEOUT_SETTING)) {\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA0),\n\t\t\t\tDL_FC0ProtectionTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA1),\n\t\t\t\tDL_TC0ReplayTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA2),\n\t\t\t\tDL_AFC0ReqTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA3),\n\t\t\t\tDL_FC1ProtectionTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA4),\n\t\t\t\tDL_TC1ReplayTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_PWRMODEUSERDATA5),\n\t\t\t\tDL_AFC1ReqTimeOutVal_Default);\n\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(DME_LocalFC0ProtectionTimeOutVal),\n\t\t\t\tDL_FC0ProtectionTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(DME_LocalTC0ReplayTimeOutVal),\n\t\t\t\tDL_TC0ReplayTimeOutVal_Default);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(DME_LocalAFC0ReqTimeOutVal),\n\t\t\t\tDL_AFC0ReqTimeOutVal_Default);\n\t}\n\n\tret = ufshcd_uic_change_pwr_mode(hba, pwr_mode->pwr_rx << 4\n\t\t\t| pwr_mode->pwr_tx);\n\n\tif (ret) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: power mode change failed %d\\n\", __func__, ret);\n\t} else {\n\t\tufshcd_vops_pwr_change_notify(hba, POST_CHANGE, NULL,\n\t\t\t\t\t\t\t\tpwr_mode);\n\n\t\tmemcpy(&hba->pwr_info, pwr_mode,\n\t\t\tsizeof(struct ufs_pa_layer_attr));\n\t}\n\n\treturn ret;\n}\n\n \nint ufshcd_config_pwr_mode(struct ufs_hba *hba,\n\t\tstruct ufs_pa_layer_attr *desired_pwr_mode)\n{\n\tstruct ufs_pa_layer_attr final_params = { 0 };\n\tint ret;\n\n\tret = ufshcd_vops_pwr_change_notify(hba, PRE_CHANGE,\n\t\t\t\t\tdesired_pwr_mode, &final_params);\n\n\tif (ret)\n\t\tmemcpy(&final_params, desired_pwr_mode, sizeof(final_params));\n\n\tret = ufshcd_change_power_mode(hba, &final_params);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_config_pwr_mode);\n\n \nstatic int ufshcd_complete_dev_init(struct ufs_hba *hba)\n{\n\tint err;\n\tbool flag_res = true;\n\tktime_t timeout;\n\n\terr = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_SET_FLAG,\n\t\tQUERY_FLAG_IDN_FDEVICEINIT, 0, NULL);\n\tif (err) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: setting fDeviceInit flag failed with error %d\\n\",\n\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\t \n\ttimeout = ktime_add_ms(ktime_get(), FDEVICEINIT_COMPL_TIMEOUT);\n\tdo {\n\t\terr = ufshcd_query_flag(hba, UPIU_QUERY_OPCODE_READ_FLAG,\n\t\t\t\t\tQUERY_FLAG_IDN_FDEVICEINIT, 0, &flag_res);\n\t\tif (!flag_res)\n\t\t\tbreak;\n\t\tusleep_range(500, 1000);\n\t} while (ktime_before(ktime_get(), timeout));\n\n\tif (err) {\n\t\tdev_err(hba->dev,\n\t\t\t\t\"%s: reading fDeviceInit flag failed with error %d\\n\",\n\t\t\t\t__func__, err);\n\t} else if (flag_res) {\n\t\tdev_err(hba->dev,\n\t\t\t\t\"%s: fDeviceInit was not cleared by the device\\n\",\n\t\t\t\t__func__);\n\t\terr = -EBUSY;\n\t}\nout:\n\treturn err;\n}\n\n \nint ufshcd_make_hba_operational(struct ufs_hba *hba)\n{\n\tint err = 0;\n\tu32 reg;\n\n\t \n\tufshcd_enable_intr(hba, UFSHCD_ENABLE_INTRS);\n\n\t \n\tif (ufshcd_is_intr_aggr_allowed(hba))\n\t\tufshcd_config_intr_aggr(hba, hba->nutrs - 1, INT_AGGR_DEF_TO);\n\telse\n\t\tufshcd_disable_intr_aggr(hba);\n\n\t \n\tufshcd_writel(hba, lower_32_bits(hba->utrdl_dma_addr),\n\t\t\tREG_UTP_TRANSFER_REQ_LIST_BASE_L);\n\tufshcd_writel(hba, upper_32_bits(hba->utrdl_dma_addr),\n\t\t\tREG_UTP_TRANSFER_REQ_LIST_BASE_H);\n\tufshcd_writel(hba, lower_32_bits(hba->utmrdl_dma_addr),\n\t\t\tREG_UTP_TASK_REQ_LIST_BASE_L);\n\tufshcd_writel(hba, upper_32_bits(hba->utmrdl_dma_addr),\n\t\t\tREG_UTP_TASK_REQ_LIST_BASE_H);\n\n\t \n\twmb();\n\n\t \n\treg = ufshcd_readl(hba, REG_CONTROLLER_STATUS);\n\tif (!(ufshcd_get_lists_status(reg))) {\n\t\tufshcd_enable_run_stop_reg(hba);\n\t} else {\n\t\tdev_err(hba->dev,\n\t\t\t\"Host controller not ready to process requests\");\n\t\terr = -EIO;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ufshcd_make_hba_operational);\n\n \nvoid ufshcd_hba_stop(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tint err;\n\n\t \n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tufshcd_writel(hba, CONTROLLER_DISABLE,  REG_CONTROLLER_ENABLE);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\terr = ufshcd_wait_for_register(hba, REG_CONTROLLER_ENABLE,\n\t\t\t\t\tCONTROLLER_ENABLE, CONTROLLER_DISABLE,\n\t\t\t\t\t10, 1);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: Controller disable failed\\n\", __func__);\n}\nEXPORT_SYMBOL_GPL(ufshcd_hba_stop);\n\n \nstatic int ufshcd_hba_execute_hce(struct ufs_hba *hba)\n{\n\tint retry_outer = 3;\n\tint retry_inner;\n\nstart:\n\tif (ufshcd_is_hba_active(hba))\n\t\t \n\t\tufshcd_hba_stop(hba);\n\n\t \n\tufshcd_set_link_off(hba);\n\n\tufshcd_vops_hce_enable_notify(hba, PRE_CHANGE);\n\n\t \n\tufshcd_hba_start(hba);\n\n\t \n\tufshcd_delay_us(hba->vps->hba_enable_delay_us, 100);\n\n\t \n\tretry_inner = 50;\n\twhile (!ufshcd_is_hba_active(hba)) {\n\t\tif (retry_inner) {\n\t\t\tretry_inner--;\n\t\t} else {\n\t\t\tdev_err(hba->dev,\n\t\t\t\t\"Controller enable failed\\n\");\n\t\t\tif (retry_outer) {\n\t\t\t\tretry_outer--;\n\t\t\t\tgoto start;\n\t\t\t}\n\t\t\treturn -EIO;\n\t\t}\n\t\tusleep_range(1000, 1100);\n\t}\n\n\t \n\tufshcd_enable_intr(hba, UFSHCD_UIC_MASK);\n\n\tufshcd_vops_hce_enable_notify(hba, POST_CHANGE);\n\n\treturn 0;\n}\n\nint ufshcd_hba_enable(struct ufs_hba *hba)\n{\n\tint ret;\n\n\tif (hba->quirks & UFSHCI_QUIRK_BROKEN_HCE) {\n\t\tufshcd_set_link_off(hba);\n\t\tufshcd_vops_hce_enable_notify(hba, PRE_CHANGE);\n\n\t\t \n\t\tufshcd_enable_intr(hba, UFSHCD_UIC_MASK);\n\t\tret = ufshcd_dme_reset(hba);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"DME_RESET failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = ufshcd_dme_enable(hba);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"Enabling DME failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tufshcd_vops_hce_enable_notify(hba, POST_CHANGE);\n\t} else {\n\t\tret = ufshcd_hba_execute_hce(hba);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_hba_enable);\n\nstatic int ufshcd_disable_tx_lcc(struct ufs_hba *hba, bool peer)\n{\n\tint tx_lanes = 0, i, err = 0;\n\n\tif (!peer)\n\t\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\n\t\t\t       &tx_lanes);\n\telse\n\t\tufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\n\t\t\t\t    &tx_lanes);\n\tfor (i = 0; i < tx_lanes; i++) {\n\t\tif (!peer)\n\t\t\terr = ufshcd_dme_set(hba,\n\t\t\t\tUIC_ARG_MIB_SEL(TX_LCC_ENABLE,\n\t\t\t\t\tUIC_ARG_MPHY_TX_GEN_SEL_INDEX(i)),\n\t\t\t\t\t0);\n\t\telse\n\t\t\terr = ufshcd_dme_peer_set(hba,\n\t\t\t\tUIC_ARG_MIB_SEL(TX_LCC_ENABLE,\n\t\t\t\t\tUIC_ARG_MPHY_TX_GEN_SEL_INDEX(i)),\n\t\t\t\t\t0);\n\t\tif (err) {\n\t\t\tdev_err(hba->dev, \"%s: TX LCC Disable failed, peer = %d, lane = %d, err = %d\",\n\t\t\t\t__func__, peer, i, err);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic inline int ufshcd_disable_device_tx_lcc(struct ufs_hba *hba)\n{\n\treturn ufshcd_disable_tx_lcc(hba, true);\n}\n\nvoid ufshcd_update_evt_hist(struct ufs_hba *hba, u32 id, u32 val)\n{\n\tstruct ufs_event_hist *e;\n\n\tif (id >= UFS_EVT_CNT)\n\t\treturn;\n\n\te = &hba->ufs_stats.event[id];\n\te->val[e->pos] = val;\n\te->tstamp[e->pos] = local_clock();\n\te->cnt += 1;\n\te->pos = (e->pos + 1) % UFS_EVENT_HIST_LENGTH;\n\n\tufshcd_vops_event_notify(hba, id, &val);\n}\nEXPORT_SYMBOL_GPL(ufshcd_update_evt_hist);\n\n \nstatic int ufshcd_link_startup(struct ufs_hba *hba)\n{\n\tint ret;\n\tint retries = DME_LINKSTARTUP_RETRIES;\n\tbool link_startup_again = false;\n\n\t \n\tif (!ufshcd_is_ufs_dev_active(hba))\n\t\tlink_startup_again = true;\n\nlink_startup:\n\tdo {\n\t\tufshcd_vops_link_startup_notify(hba, PRE_CHANGE);\n\n\t\tret = ufshcd_dme_link_startup(hba);\n\n\t\t \n\t\tif (!ret && !ufshcd_is_device_present(hba)) {\n\t\t\tufshcd_update_evt_hist(hba,\n\t\t\t\t\t       UFS_EVT_LINK_STARTUP_FAIL,\n\t\t\t\t\t       0);\n\t\t\tdev_err(hba->dev, \"%s: Device not present\\n\", __func__);\n\t\t\tret = -ENXIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (ret && retries && ufshcd_hba_enable(hba)) {\n\t\t\tufshcd_update_evt_hist(hba,\n\t\t\t\t\t       UFS_EVT_LINK_STARTUP_FAIL,\n\t\t\t\t\t       (u32)ret);\n\t\t\tgoto out;\n\t\t}\n\t} while (ret && retries--);\n\n\tif (ret) {\n\t\t \n\t\tufshcd_update_evt_hist(hba,\n\t\t\t\t       UFS_EVT_LINK_STARTUP_FAIL,\n\t\t\t\t       (u32)ret);\n\t\tgoto out;\n\t}\n\n\tif (link_startup_again) {\n\t\tlink_startup_again = false;\n\t\tretries = DME_LINKSTARTUP_RETRIES;\n\t\tgoto link_startup;\n\t}\n\n\t \n\tufshcd_init_pwr_info(hba);\n\tufshcd_print_pwr_info(hba);\n\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_LCC) {\n\t\tret = ufshcd_disable_device_tx_lcc(hba);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t \n\tret = ufshcd_vops_link_startup_notify(hba, POST_CHANGE);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tufshcd_readl(hba, REG_UIC_ERROR_CODE_PHY_ADAPTER_LAYER);\n\tret = ufshcd_make_hba_operational(hba);\nout:\n\tif (ret) {\n\t\tdev_err(hba->dev, \"link startup failed %d\\n\", ret);\n\t\tufshcd_print_host_state(hba);\n\t\tufshcd_print_pwr_info(hba);\n\t\tufshcd_print_evt_hist(hba);\n\t}\n\treturn ret;\n}\n\n \nstatic int ufshcd_verify_dev_init(struct ufs_hba *hba)\n{\n\tint err = 0;\n\tint retries;\n\n\tufshcd_hold(hba);\n\tmutex_lock(&hba->dev_cmd.lock);\n\tfor (retries = NOP_OUT_RETRIES; retries > 0; retries--) {\n\t\terr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_NOP,\n\t\t\t\t\t  hba->nop_out_timeout);\n\n\t\tif (!err || err == -ETIMEDOUT)\n\t\t\tbreak;\n\n\t\tdev_dbg(hba->dev, \"%s: error %d retrying\\n\", __func__, err);\n\t}\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: NOP OUT failed %d\\n\", __func__, err);\n\treturn err;\n}\n\n \nstatic void ufshcd_setup_links(struct ufs_hba *hba, struct scsi_device *sdev)\n{\n\tstruct device_link *link;\n\n\t \n\tif (hba->ufs_device_wlun) {\n\t\tlink = device_link_add(&sdev->sdev_gendev,\n\t\t\t\t       &hba->ufs_device_wlun->sdev_gendev,\n\t\t\t\t       DL_FLAG_PM_RUNTIME | DL_FLAG_RPM_ACTIVE);\n\t\tif (!link) {\n\t\t\tdev_err(&sdev->sdev_gendev, \"Failed establishing link - %s\\n\",\n\t\t\t\tdev_name(&hba->ufs_device_wlun->sdev_gendev));\n\t\t\treturn;\n\t\t}\n\t\thba->luns_avail--;\n\t\t \n\t\tif (hba->luns_avail == 1) {\n\t\t\tufshcd_rpm_put(hba);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t \n\t\thba->luns_avail--;\n\t}\n}\n\n \nstatic void ufshcd_lu_init(struct ufs_hba *hba, struct scsi_device *sdev)\n{\n\tint len = QUERY_DESC_MAX_SIZE;\n\tu8 lun = ufshcd_scsi_to_upiu_lun(sdev->lun);\n\tu8 lun_qdepth = hba->nutrs;\n\tu8 *desc_buf;\n\tint ret;\n\n\tdesc_buf = kzalloc(len, GFP_KERNEL);\n\tif (!desc_buf)\n\t\tgoto set_qdepth;\n\n\tret = ufshcd_read_unit_desc_param(hba, lun, 0, desc_buf, len);\n\tif (ret < 0) {\n\t\tif (ret == -EOPNOTSUPP)\n\t\t\t \n\t\t\tlun_qdepth = 1;\n\t\tkfree(desc_buf);\n\t\tgoto set_qdepth;\n\t}\n\n\tif (desc_buf[UNIT_DESC_PARAM_LU_Q_DEPTH]) {\n\t\t \n\t\tlun_qdepth = min_t(int, desc_buf[UNIT_DESC_PARAM_LU_Q_DEPTH], hba->nutrs);\n\t}\n\t \n\tif (hba->dev_info.f_power_on_wp_en && lun < hba->dev_info.max_lu_supported &&\n\t    !hba->dev_info.is_lu_power_on_wp &&\n\t    desc_buf[UNIT_DESC_PARAM_LU_WR_PROTECT] == UFS_LU_POWER_ON_WP)\n\t\thba->dev_info.is_lu_power_on_wp = true;\n\n\t \n\tif (desc_buf[UNIT_DESC_PARAM_UNIT_INDEX] == UFS_UPIU_RPMB_WLUN &&\n\t    desc_buf[RPMB_UNIT_DESC_PARAM_REGION_EN] & BIT(4))\n\t\thba->dev_info.b_advanced_rpmb_en = true;\n\n\n\tkfree(desc_buf);\nset_qdepth:\n\t \n\tdev_dbg(hba->dev, \"Set LU %x queue depth %d\\n\", lun, lun_qdepth);\n\tscsi_change_queue_depth(sdev, lun_qdepth);\n}\n\n \nstatic int ufshcd_slave_alloc(struct scsi_device *sdev)\n{\n\tstruct ufs_hba *hba;\n\n\thba = shost_priv(sdev->host);\n\n\t \n\tsdev->use_10_for_ms = 1;\n\n\t \n\tsdev->set_dbd_for_ms = 1;\n\n\t \n\tsdev->allow_restart = 1;\n\n\t \n\tsdev->no_report_opcodes = 1;\n\n\t \n\tsdev->no_write_same = 1;\n\n\tufshcd_lu_init(hba, sdev);\n\n\tufshcd_setup_links(hba, sdev);\n\n\treturn 0;\n}\n\n \nstatic int ufshcd_change_queue_depth(struct scsi_device *sdev, int depth)\n{\n\treturn scsi_change_queue_depth(sdev, min(depth, sdev->host->can_queue));\n}\n\n \nstatic int ufshcd_slave_configure(struct scsi_device *sdev)\n{\n\tstruct ufs_hba *hba = shost_priv(sdev->host);\n\tstruct request_queue *q = sdev->request_queue;\n\n\tblk_queue_update_dma_pad(q, PRDT_DATA_BYTE_COUNT_PAD - 1);\n\tif (hba->quirks & UFSHCD_QUIRK_4KB_DMA_ALIGNMENT)\n\t\tblk_queue_update_dma_alignment(q, SZ_4K - 1);\n\t \n\tif (is_device_wlun(sdev))\n\t\tpm_runtime_get_noresume(&sdev->sdev_gendev);\n\telse if (ufshcd_is_rpm_autosuspend_allowed(hba))\n\t\tsdev->rpm_autosuspend = 1;\n\t \n\tsdev->silence_suspend = 1;\n\n\tufshcd_crypto_register(hba, q);\n\n\treturn 0;\n}\n\n \nstatic void ufshcd_slave_destroy(struct scsi_device *sdev)\n{\n\tstruct ufs_hba *hba;\n\tunsigned long flags;\n\n\thba = shost_priv(sdev->host);\n\n\t \n\tif (ufshcd_scsi_to_upiu_lun(sdev->lun) == UFS_UPIU_UFS_DEVICE_WLUN) {\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\thba->ufs_device_wlun = NULL;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t} else if (hba->ufs_device_wlun) {\n\t\tstruct device *supplier = NULL;\n\n\t\t \n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tif (hba->ufs_device_wlun) {\n\t\t\tsupplier = &hba->ufs_device_wlun->sdev_gendev;\n\t\t\tget_device(supplier);\n\t\t}\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t\tif (supplier) {\n\t\t\t \n\t\t\tdevice_link_remove(&sdev->sdev_gendev, supplier);\n\t\t\tput_device(supplier);\n\t\t}\n\t}\n}\n\n \nstatic inline int\nufshcd_scsi_cmd_status(struct ufshcd_lrb *lrbp, int scsi_status)\n{\n\tint result = 0;\n\n\tswitch (scsi_status) {\n\tcase SAM_STAT_CHECK_CONDITION:\n\t\tufshcd_copy_sense_data(lrbp);\n\t\tfallthrough;\n\tcase SAM_STAT_GOOD:\n\t\tresult |= DID_OK << 16 | scsi_status;\n\t\tbreak;\n\tcase SAM_STAT_TASK_SET_FULL:\n\tcase SAM_STAT_BUSY:\n\tcase SAM_STAT_TASK_ABORTED:\n\t\tufshcd_copy_sense_data(lrbp);\n\t\tresult |= scsi_status;\n\t\tbreak;\n\tdefault:\n\t\tresult |= DID_ERROR << 16;\n\t\tbreak;\n\t}  \n\n\treturn result;\n}\n\n \nstatic inline int\nufshcd_transfer_rsp_status(struct ufs_hba *hba, struct ufshcd_lrb *lrbp,\n\t\t\t   struct cq_entry *cqe)\n{\n\tint result = 0;\n\tint scsi_status;\n\tenum utp_ocs ocs;\n\tu8 upiu_flags;\n\tu32 resid;\n\n\tupiu_flags = lrbp->ucd_rsp_ptr->header.flags;\n\tresid = be32_to_cpu(lrbp->ucd_rsp_ptr->sr.residual_transfer_count);\n\t \n\tif (resid && !(upiu_flags & UPIU_RSP_FLAG_OVERFLOW))\n\t\tscsi_set_resid(lrbp->cmd, resid);\n\n\t \n\tocs = ufshcd_get_tr_ocs(lrbp, cqe);\n\n\tif (hba->quirks & UFSHCD_QUIRK_BROKEN_OCS_FATAL_ERROR) {\n\t\tif (lrbp->ucd_rsp_ptr->header.response ||\n\t\t    lrbp->ucd_rsp_ptr->header.status)\n\t\t\tocs = OCS_SUCCESS;\n\t}\n\n\tswitch (ocs) {\n\tcase OCS_SUCCESS:\n\t\thba->ufs_stats.last_hibern8_exit_tstamp = ktime_set(0, 0);\n\t\tswitch (ufshcd_get_req_rsp(lrbp->ucd_rsp_ptr)) {\n\t\tcase UPIU_TRANSACTION_RESPONSE:\n\t\t\t \n\t\t\tscsi_status = lrbp->ucd_rsp_ptr->header.status;\n\t\t\tresult = ufshcd_scsi_cmd_status(lrbp, scsi_status);\n\n\t\t\t \n\t\t\tif (!hba->pm_op_in_progress &&\n\t\t\t    !ufshcd_eh_in_progress(hba) &&\n\t\t\t    ufshcd_is_exception_event(lrbp->ucd_rsp_ptr))\n\t\t\t\t \n\t\t\t\tschedule_work(&hba->eeh_work);\n\t\t\tbreak;\n\t\tcase UPIU_TRANSACTION_REJECT_UPIU:\n\t\t\t \n\t\t\tresult = DID_ERROR << 16;\n\t\t\tdev_err(hba->dev,\n\t\t\t\t\"Reject UPIU not fully implemented\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hba->dev,\n\t\t\t\t\"Unexpected request response code = %x\\n\",\n\t\t\t\tresult);\n\t\t\tresult = DID_ERROR << 16;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase OCS_ABORTED:\n\t\tresult |= DID_ABORT << 16;\n\t\tbreak;\n\tcase OCS_INVALID_COMMAND_STATUS:\n\t\tresult |= DID_REQUEUE << 16;\n\t\tbreak;\n\tcase OCS_INVALID_CMD_TABLE_ATTR:\n\tcase OCS_INVALID_PRDT_ATTR:\n\tcase OCS_MISMATCH_DATA_BUF_SIZE:\n\tcase OCS_MISMATCH_RESP_UPIU_SIZE:\n\tcase OCS_PEER_COMM_FAILURE:\n\tcase OCS_FATAL_ERROR:\n\tcase OCS_DEVICE_FATAL_ERROR:\n\tcase OCS_INVALID_CRYPTO_CONFIG:\n\tcase OCS_GENERAL_CRYPTO_ERROR:\n\tdefault:\n\t\tresult |= DID_ERROR << 16;\n\t\tdev_err(hba->dev,\n\t\t\t\t\"OCS error from controller = %x for tag %d\\n\",\n\t\t\t\tocs, lrbp->task_tag);\n\t\tufshcd_print_evt_hist(hba);\n\t\tufshcd_print_host_state(hba);\n\t\tbreak;\n\t}  \n\n\tif ((host_byte(result) != DID_OK) &&\n\t    (host_byte(result) != DID_REQUEUE) && !hba->silence_err_logs)\n\t\tufshcd_print_tr(hba, lrbp->task_tag, true);\n\treturn result;\n}\n\nstatic bool ufshcd_is_auto_hibern8_error(struct ufs_hba *hba,\n\t\t\t\t\t u32 intr_mask)\n{\n\tif (!ufshcd_is_auto_hibern8_supported(hba) ||\n\t    !ufshcd_is_auto_hibern8_enabled(hba))\n\t\treturn false;\n\n\tif (!(intr_mask & UFSHCD_UIC_HIBERN8_MASK))\n\t\treturn false;\n\n\tif (hba->active_uic_cmd &&\n\t    (hba->active_uic_cmd->command == UIC_CMD_DME_HIBER_ENTER ||\n\t    hba->active_uic_cmd->command == UIC_CMD_DME_HIBER_EXIT))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic irqreturn_t ufshcd_uic_cmd_compl(struct ufs_hba *hba, u32 intr_status)\n{\n\tirqreturn_t retval = IRQ_NONE;\n\n\tspin_lock(hba->host->host_lock);\n\tif (ufshcd_is_auto_hibern8_error(hba, intr_status))\n\t\thba->errors |= (UFSHCD_UIC_HIBERN8_MASK & intr_status);\n\n\tif ((intr_status & UIC_COMMAND_COMPL) && hba->active_uic_cmd) {\n\t\thba->active_uic_cmd->argument2 |=\n\t\t\tufshcd_get_uic_cmd_result(hba);\n\t\thba->active_uic_cmd->argument3 =\n\t\t\tufshcd_get_dme_attr_val(hba);\n\t\tif (!hba->uic_async_done)\n\t\t\thba->active_uic_cmd->cmd_active = 0;\n\t\tcomplete(&hba->active_uic_cmd->done);\n\t\tretval = IRQ_HANDLED;\n\t}\n\n\tif ((intr_status & UFSHCD_UIC_PWR_MASK) && hba->uic_async_done) {\n\t\thba->active_uic_cmd->cmd_active = 0;\n\t\tcomplete(hba->uic_async_done);\n\t\tretval = IRQ_HANDLED;\n\t}\n\n\tif (retval == IRQ_HANDLED)\n\t\tufshcd_add_uic_command_trace(hba, hba->active_uic_cmd,\n\t\t\t\t\t     UFS_CMD_COMP);\n\tspin_unlock(hba->host->host_lock);\n\treturn retval;\n}\n\n \nvoid ufshcd_release_scsi_cmd(struct ufs_hba *hba,\n\t\t\t     struct ufshcd_lrb *lrbp)\n{\n\tstruct scsi_cmnd *cmd = lrbp->cmd;\n\n\tscsi_dma_unmap(cmd);\n\tufshcd_release(hba);\n\tufshcd_clk_scaling_update_busy(hba);\n}\n\n \nvoid ufshcd_compl_one_cqe(struct ufs_hba *hba, int task_tag,\n\t\t\t  struct cq_entry *cqe)\n{\n\tstruct ufshcd_lrb *lrbp;\n\tstruct scsi_cmnd *cmd;\n\tenum utp_ocs ocs;\n\n\tlrbp = &hba->lrb[task_tag];\n\tlrbp->compl_time_stamp = ktime_get();\n\tcmd = lrbp->cmd;\n\tif (cmd) {\n\t\tif (unlikely(ufshcd_should_inform_monitor(hba, lrbp)))\n\t\t\tufshcd_update_monitor(hba, lrbp);\n\t\tufshcd_add_command_trace(hba, task_tag, UFS_CMD_COMP);\n\t\tcmd->result = ufshcd_transfer_rsp_status(hba, lrbp, cqe);\n\t\tufshcd_release_scsi_cmd(hba, lrbp);\n\t\t \n\t\tscsi_done(cmd);\n\t} else if (lrbp->command_type == UTP_CMD_TYPE_DEV_MANAGE ||\n\t\t   lrbp->command_type == UTP_CMD_TYPE_UFS_STORAGE) {\n\t\tif (hba->dev_cmd.complete) {\n\t\t\tif (cqe) {\n\t\t\t\tocs = le32_to_cpu(cqe->status) & MASK_OCS;\n\t\t\t\tlrbp->utr_descriptor_ptr->header.ocs = ocs;\n\t\t\t}\n\t\t\tcomplete(hba->dev_cmd.complete);\n\t\t\tufshcd_clk_scaling_update_busy(hba);\n\t\t}\n\t}\n}\n\n \nstatic void __ufshcd_transfer_req_compl(struct ufs_hba *hba,\n\t\t\t\t\tunsigned long completed_reqs)\n{\n\tint tag;\n\n\tfor_each_set_bit(tag, &completed_reqs, hba->nutrs)\n\t\tufshcd_compl_one_cqe(hba, tag, NULL);\n}\n\n \nenum {\n\tUFSHCD_POLL_FROM_INTERRUPT_CONTEXT = -1\n};\n\nstatic void ufshcd_clear_polled(struct ufs_hba *hba,\n\t\t\t\tunsigned long *completed_reqs)\n{\n\tint tag;\n\n\tfor_each_set_bit(tag, completed_reqs, hba->nutrs) {\n\t\tstruct scsi_cmnd *cmd = hba->lrb[tag].cmd;\n\n\t\tif (!cmd)\n\t\t\tcontinue;\n\t\tif (scsi_cmd_to_rq(cmd)->cmd_flags & REQ_POLLED)\n\t\t\t__clear_bit(tag, completed_reqs);\n\t}\n}\n\n \nstatic int ufshcd_poll(struct Scsi_Host *shost, unsigned int queue_num)\n{\n\tstruct ufs_hba *hba = shost_priv(shost);\n\tunsigned long completed_reqs, flags;\n\tu32 tr_doorbell;\n\tstruct ufs_hw_queue *hwq;\n\n\tif (is_mcq_enabled(hba)) {\n\t\thwq = &hba->uhq[queue_num];\n\n\t\treturn ufshcd_mcq_poll_cqe_lock(hba, hwq);\n\t}\n\n\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\ttr_doorbell = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\tcompleted_reqs = ~tr_doorbell & hba->outstanding_reqs;\n\tWARN_ONCE(completed_reqs & ~hba->outstanding_reqs,\n\t\t  \"completed: %#lx; outstanding: %#lx\\n\", completed_reqs,\n\t\t  hba->outstanding_reqs);\n\tif (queue_num == UFSHCD_POLL_FROM_INTERRUPT_CONTEXT) {\n\t\t \n\t\tufshcd_clear_polled(hba, &completed_reqs);\n\t}\n\thba->outstanding_reqs &= ~completed_reqs;\n\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\tif (completed_reqs)\n\t\t__ufshcd_transfer_req_compl(hba, completed_reqs);\n\n\treturn completed_reqs != 0;\n}\n\n \nstatic void ufshcd_mcq_compl_pending_transfer(struct ufs_hba *hba,\n\t\t\t\t\t      bool force_compl)\n{\n\tstruct ufs_hw_queue *hwq;\n\tstruct ufshcd_lrb *lrbp;\n\tstruct scsi_cmnd *cmd;\n\tunsigned long flags;\n\tu32 hwq_num, utag;\n\tint tag;\n\n\tfor (tag = 0; tag < hba->nutrs; tag++) {\n\t\tlrbp = &hba->lrb[tag];\n\t\tcmd = lrbp->cmd;\n\t\tif (!ufshcd_cmd_inflight(cmd) ||\n\t\t    test_bit(SCMD_STATE_COMPLETE, &cmd->state))\n\t\t\tcontinue;\n\n\t\tutag = blk_mq_unique_tag(scsi_cmd_to_rq(cmd));\n\t\thwq_num = blk_mq_unique_tag_to_hwq(utag);\n\t\thwq = &hba->uhq[hwq_num];\n\n\t\tif (force_compl) {\n\t\t\tufshcd_mcq_compl_all_cqes_lock(hba, hwq);\n\t\t\t \n\t\t\tif (cmd && !test_bit(SCMD_STATE_COMPLETE, &cmd->state)) {\n\t\t\t\tspin_lock_irqsave(&hwq->cq_lock, flags);\n\t\t\t\tset_host_byte(cmd, DID_REQUEUE);\n\t\t\t\tufshcd_release_scsi_cmd(hba, lrbp);\n\t\t\t\tscsi_done(cmd);\n\t\t\t\tspin_unlock_irqrestore(&hwq->cq_lock, flags);\n\t\t\t}\n\t\t} else {\n\t\t\tufshcd_mcq_poll_cqe_lock(hba, hwq);\n\t\t}\n\t}\n}\n\n \nstatic irqreturn_t ufshcd_transfer_req_compl(struct ufs_hba *hba)\n{\n\t \n\tif (ufshcd_is_intr_aggr_allowed(hba) &&\n\t    !(hba->quirks & UFSHCI_QUIRK_SKIP_RESET_INTR_AGGR))\n\t\tufshcd_reset_intr_aggr(hba);\n\n\tif (ufs_fail_completion())\n\t\treturn IRQ_HANDLED;\n\n\t \n\tufshcd_poll(hba->host, UFSHCD_POLL_FROM_INTERRUPT_CONTEXT);\n\n\treturn IRQ_HANDLED;\n}\n\nint __ufshcd_write_ee_control(struct ufs_hba *hba, u32 ee_ctrl_mask)\n{\n\treturn ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\n\t\t\t\t       QUERY_ATTR_IDN_EE_CONTROL, 0, 0,\n\t\t\t\t       &ee_ctrl_mask);\n}\n\nint ufshcd_write_ee_control(struct ufs_hba *hba)\n{\n\tint err;\n\n\tmutex_lock(&hba->ee_ctrl_mutex);\n\terr = __ufshcd_write_ee_control(hba, hba->ee_ctrl_mask);\n\tmutex_unlock(&hba->ee_ctrl_mutex);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed to write ee control %d\\n\",\n\t\t\t__func__, err);\n\treturn err;\n}\n\nint ufshcd_update_ee_control(struct ufs_hba *hba, u16 *mask,\n\t\t\t     const u16 *other_mask, u16 set, u16 clr)\n{\n\tu16 new_mask, ee_ctrl_mask;\n\tint err = 0;\n\n\tmutex_lock(&hba->ee_ctrl_mutex);\n\tnew_mask = (*mask & ~clr) | set;\n\tee_ctrl_mask = new_mask | *other_mask;\n\tif (ee_ctrl_mask != hba->ee_ctrl_mask)\n\t\terr = __ufshcd_write_ee_control(hba, ee_ctrl_mask);\n\t \n\tif (!err) {\n\t\thba->ee_ctrl_mask = ee_ctrl_mask;\n\t\t*mask = new_mask;\n\t}\n\tmutex_unlock(&hba->ee_ctrl_mutex);\n\treturn err;\n}\n\n \nstatic inline int ufshcd_disable_ee(struct ufs_hba *hba, u16 mask)\n{\n\treturn ufshcd_update_ee_drv_mask(hba, 0, mask);\n}\n\n \nstatic inline int ufshcd_enable_ee(struct ufs_hba *hba, u16 mask)\n{\n\treturn ufshcd_update_ee_drv_mask(hba, mask, 0);\n}\n\n \nstatic int ufshcd_enable_auto_bkops(struct ufs_hba *hba)\n{\n\tint err = 0;\n\n\tif (hba->auto_bkops_enabled)\n\t\tgoto out;\n\n\terr = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_SET_FLAG,\n\t\t\tQUERY_FLAG_IDN_BKOPS_EN, 0, NULL);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to enable bkops %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\thba->auto_bkops_enabled = true;\n\ttrace_ufshcd_auto_bkops_state(dev_name(hba->dev), \"Enabled\");\n\n\t \n\terr = ufshcd_disable_ee(hba, MASK_EE_URGENT_BKOPS);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed to disable exception event %d\\n\",\n\t\t\t\t__func__, err);\nout:\n\treturn err;\n}\n\n \nstatic int ufshcd_disable_auto_bkops(struct ufs_hba *hba)\n{\n\tint err = 0;\n\n\tif (!hba->auto_bkops_enabled)\n\t\tgoto out;\n\n\t \n\terr = ufshcd_enable_ee(hba, MASK_EE_URGENT_BKOPS);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to enable exception event %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\terr = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_CLEAR_FLAG,\n\t\t\tQUERY_FLAG_IDN_BKOPS_EN, 0, NULL);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to disable bkops %d\\n\",\n\t\t\t\t__func__, err);\n\t\tufshcd_disable_ee(hba, MASK_EE_URGENT_BKOPS);\n\t\tgoto out;\n\t}\n\n\thba->auto_bkops_enabled = false;\n\ttrace_ufshcd_auto_bkops_state(dev_name(hba->dev), \"Disabled\");\n\thba->is_urgent_bkops_lvl_checked = false;\nout:\n\treturn err;\n}\n\n \nstatic void ufshcd_force_reset_auto_bkops(struct ufs_hba *hba)\n{\n\tif (ufshcd_keep_autobkops_enabled_except_suspend(hba)) {\n\t\thba->auto_bkops_enabled = false;\n\t\thba->ee_ctrl_mask |= MASK_EE_URGENT_BKOPS;\n\t\tufshcd_enable_auto_bkops(hba);\n\t} else {\n\t\thba->auto_bkops_enabled = true;\n\t\thba->ee_ctrl_mask &= ~MASK_EE_URGENT_BKOPS;\n\t\tufshcd_disable_auto_bkops(hba);\n\t}\n\thba->urgent_bkops_lvl = BKOPS_STATUS_PERF_IMPACT;\n\thba->is_urgent_bkops_lvl_checked = false;\n}\n\nstatic inline int ufshcd_get_bkops_status(struct ufs_hba *hba, u32 *status)\n{\n\treturn ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\tQUERY_ATTR_IDN_BKOPS_STATUS, 0, 0, status);\n}\n\n \nstatic int ufshcd_bkops_ctrl(struct ufs_hba *hba,\n\t\t\t     enum bkops_status status)\n{\n\tint err;\n\tu32 curr_status = 0;\n\n\terr = ufshcd_get_bkops_status(hba, &curr_status);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to get BKOPS status %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t} else if (curr_status > BKOPS_STATUS_MAX) {\n\t\tdev_err(hba->dev, \"%s: invalid BKOPS status %d\\n\",\n\t\t\t\t__func__, curr_status);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (curr_status >= status)\n\t\terr = ufshcd_enable_auto_bkops(hba);\n\telse\n\t\terr = ufshcd_disable_auto_bkops(hba);\nout:\n\treturn err;\n}\n\n \nstatic int ufshcd_urgent_bkops(struct ufs_hba *hba)\n{\n\treturn ufshcd_bkops_ctrl(hba, hba->urgent_bkops_lvl);\n}\n\nstatic inline int ufshcd_get_ee_status(struct ufs_hba *hba, u32 *status)\n{\n\treturn ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\tQUERY_ATTR_IDN_EE_STATUS, 0, 0, status);\n}\n\nstatic void ufshcd_bkops_exception_event_handler(struct ufs_hba *hba)\n{\n\tint err;\n\tu32 curr_status = 0;\n\n\tif (hba->is_urgent_bkops_lvl_checked)\n\t\tgoto enable_auto_bkops;\n\n\terr = ufshcd_get_bkops_status(hba, &curr_status);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to get BKOPS status %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\t \n\tif (curr_status < BKOPS_STATUS_PERF_IMPACT) {\n\t\tdev_err(hba->dev, \"%s: device raised urgent BKOPS exception for bkops status %d\\n\",\n\t\t\t\t__func__, curr_status);\n\t\t \n\t\thba->urgent_bkops_lvl = curr_status;\n\t\thba->is_urgent_bkops_lvl_checked = true;\n\t}\n\nenable_auto_bkops:\n\terr = ufshcd_enable_auto_bkops(hba);\nout:\n\tif (err < 0)\n\t\tdev_err(hba->dev, \"%s: failed to handle urgent bkops %d\\n\",\n\t\t\t\t__func__, err);\n}\n\nstatic void ufshcd_temp_exception_event_handler(struct ufs_hba *hba, u16 status)\n{\n\tu32 value;\n\n\tif (ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\tQUERY_ATTR_IDN_CASE_ROUGH_TEMP, 0, 0, &value))\n\t\treturn;\n\n\tdev_info(hba->dev, \"exception Tcase %d\\n\", value - 80);\n\n\tufs_hwmon_notify_event(hba, status & MASK_EE_URGENT_TEMP);\n\n\t \n}\n\nstatic int __ufshcd_wb_toggle(struct ufs_hba *hba, bool set, enum flag_idn idn)\n{\n\tu8 index;\n\tenum query_opcode opcode = set ? UPIU_QUERY_OPCODE_SET_FLAG :\n\t\t\t\t   UPIU_QUERY_OPCODE_CLEAR_FLAG;\n\n\tindex = ufshcd_wb_get_query_index(hba);\n\treturn ufshcd_query_flag_retry(hba, opcode, idn, index, NULL);\n}\n\nint ufshcd_wb_toggle(struct ufs_hba *hba, bool enable)\n{\n\tint ret;\n\n\tif (!ufshcd_is_wb_allowed(hba) ||\n\t    hba->dev_info.wb_enabled == enable)\n\t\treturn 0;\n\n\tret = __ufshcd_wb_toggle(hba, enable, QUERY_FLAG_IDN_WB_EN);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: Write Booster %s failed %d\\n\",\n\t\t\t__func__, enable ? \"enabling\" : \"disabling\", ret);\n\t\treturn ret;\n\t}\n\n\thba->dev_info.wb_enabled = enable;\n\tdev_dbg(hba->dev, \"%s: Write Booster %s\\n\",\n\t\t\t__func__, enable ? \"enabled\" : \"disabled\");\n\n\treturn ret;\n}\n\nstatic void ufshcd_wb_toggle_buf_flush_during_h8(struct ufs_hba *hba,\n\t\t\t\t\t\t bool enable)\n{\n\tint ret;\n\n\tret = __ufshcd_wb_toggle(hba, enable,\n\t\t\tQUERY_FLAG_IDN_WB_BUFF_FLUSH_DURING_HIBERN8);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: WB-Buf Flush during H8 %s failed %d\\n\",\n\t\t\t__func__, enable ? \"enabling\" : \"disabling\", ret);\n\t\treturn;\n\t}\n\tdev_dbg(hba->dev, \"%s: WB-Buf Flush during H8 %s\\n\",\n\t\t\t__func__, enable ? \"enabled\" : \"disabled\");\n}\n\nint ufshcd_wb_toggle_buf_flush(struct ufs_hba *hba, bool enable)\n{\n\tint ret;\n\n\tif (!ufshcd_is_wb_allowed(hba) ||\n\t    hba->dev_info.wb_buf_flush_enabled == enable)\n\t\treturn 0;\n\n\tret = __ufshcd_wb_toggle(hba, enable, QUERY_FLAG_IDN_WB_BUFF_FLUSH_EN);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: WB-Buf Flush %s failed %d\\n\",\n\t\t\t__func__, enable ? \"enabling\" : \"disabling\", ret);\n\t\treturn ret;\n\t}\n\n\thba->dev_info.wb_buf_flush_enabled = enable;\n\tdev_dbg(hba->dev, \"%s: WB-Buf Flush %s\\n\",\n\t\t\t__func__, enable ? \"enabled\" : \"disabled\");\n\n\treturn ret;\n}\n\nstatic bool ufshcd_wb_presrv_usrspc_keep_vcc_on(struct ufs_hba *hba,\n\t\t\t\t\t\tu32 avail_buf)\n{\n\tu32 cur_buf;\n\tint ret;\n\tu8 index;\n\n\tindex = ufshcd_wb_get_query_index(hba);\n\tret = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\t\t      QUERY_ATTR_IDN_CURR_WB_BUFF_SIZE,\n\t\t\t\t\t      index, 0, &cur_buf);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: dCurWriteBoosterBufferSize read failed %d\\n\",\n\t\t\t__func__, ret);\n\t\treturn false;\n\t}\n\n\tif (!cur_buf) {\n\t\tdev_info(hba->dev, \"dCurWBBuf: %d WB disabled until free-space is available\\n\",\n\t\t\t cur_buf);\n\t\treturn false;\n\t}\n\t \n\treturn avail_buf < hba->vps->wb_flush_threshold;\n}\n\nstatic void ufshcd_wb_force_disable(struct ufs_hba *hba)\n{\n\tif (ufshcd_is_wb_buf_flush_allowed(hba))\n\t\tufshcd_wb_toggle_buf_flush(hba, false);\n\n\tufshcd_wb_toggle_buf_flush_during_h8(hba, false);\n\tufshcd_wb_toggle(hba, false);\n\thba->caps &= ~UFSHCD_CAP_WB_EN;\n\n\tdev_info(hba->dev, \"%s: WB force disabled\\n\", __func__);\n}\n\nstatic bool ufshcd_is_wb_buf_lifetime_available(struct ufs_hba *hba)\n{\n\tu32 lifetime;\n\tint ret;\n\tu8 index;\n\n\tindex = ufshcd_wb_get_query_index(hba);\n\tret = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\t      QUERY_ATTR_IDN_WB_BUFF_LIFE_TIME_EST,\n\t\t\t\t      index, 0, &lifetime);\n\tif (ret) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: bWriteBoosterBufferLifeTimeEst read failed %d\\n\",\n\t\t\t__func__, ret);\n\t\treturn false;\n\t}\n\n\tif (lifetime == UFS_WB_EXCEED_LIFETIME) {\n\t\tdev_err(hba->dev, \"%s: WB buf lifetime is exhausted 0x%02X\\n\",\n\t\t\t__func__, lifetime);\n\t\treturn false;\n\t}\n\n\tdev_dbg(hba->dev, \"%s: WB buf lifetime is 0x%02X\\n\",\n\t\t__func__, lifetime);\n\n\treturn true;\n}\n\nstatic bool ufshcd_wb_need_flush(struct ufs_hba *hba)\n{\n\tint ret;\n\tu32 avail_buf;\n\tu8 index;\n\n\tif (!ufshcd_is_wb_allowed(hba))\n\t\treturn false;\n\n\tif (!ufshcd_is_wb_buf_lifetime_available(hba)) {\n\t\tufshcd_wb_force_disable(hba);\n\t\treturn false;\n\t}\n\n\t \n\tindex = ufshcd_wb_get_query_index(hba);\n\tret = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\t      QUERY_ATTR_IDN_AVAIL_WB_BUFF_SIZE,\n\t\t\t\t      index, 0, &avail_buf);\n\tif (ret) {\n\t\tdev_warn(hba->dev, \"%s: dAvailableWriteBoosterBufferSize read failed %d\\n\",\n\t\t\t __func__, ret);\n\t\treturn false;\n\t}\n\n\tif (!hba->dev_info.b_presrv_uspc_en)\n\t\treturn avail_buf <= UFS_WB_BUF_REMAIN_PERCENT(10);\n\n\treturn ufshcd_wb_presrv_usrspc_keep_vcc_on(hba, avail_buf);\n}\n\nstatic void ufshcd_rpm_dev_flush_recheck_work(struct work_struct *work)\n{\n\tstruct ufs_hba *hba = container_of(to_delayed_work(work),\n\t\t\t\t\t   struct ufs_hba,\n\t\t\t\t\t   rpm_dev_flush_recheck_work);\n\t \n\tufshcd_rpm_get_sync(hba);\n\tufshcd_rpm_put_sync(hba);\n}\n\n \nstatic void ufshcd_exception_event_handler(struct work_struct *work)\n{\n\tstruct ufs_hba *hba;\n\tint err;\n\tu32 status = 0;\n\thba = container_of(work, struct ufs_hba, eeh_work);\n\n\tufshcd_scsi_block_requests(hba);\n\terr = ufshcd_get_ee_status(hba, &status);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed to get exception status %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\ttrace_ufshcd_exception_event(dev_name(hba->dev), status);\n\n\tif (status & hba->ee_drv_mask & MASK_EE_URGENT_BKOPS)\n\t\tufshcd_bkops_exception_event_handler(hba);\n\n\tif (status & hba->ee_drv_mask & MASK_EE_URGENT_TEMP)\n\t\tufshcd_temp_exception_event_handler(hba, status);\n\n\tufs_debugfs_exception_event(hba, status);\nout:\n\tufshcd_scsi_unblock_requests(hba);\n}\n\n \nstatic void ufshcd_complete_requests(struct ufs_hba *hba, bool force_compl)\n{\n\tif (is_mcq_enabled(hba))\n\t\tufshcd_mcq_compl_pending_transfer(hba, force_compl);\n\telse\n\t\tufshcd_transfer_req_compl(hba);\n\n\tufshcd_tmc_handler(hba);\n}\n\n \nstatic bool ufshcd_quirk_dl_nac_errors(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tbool err_handling = true;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t \n\tif (hba->saved_err & (CONTROLLER_FATAL_ERROR | SYSTEM_BUS_FATAL_ERROR))\n\t\tgoto out;\n\n\tif ((hba->saved_err & DEVICE_FATAL_ERROR) ||\n\t    ((hba->saved_err & UIC_ERROR) &&\n\t     (hba->saved_uic_err & UFSHCD_UIC_DL_TCx_REPLAY_ERROR)))\n\t\tgoto out;\n\n\tif ((hba->saved_err & UIC_ERROR) &&\n\t    (hba->saved_uic_err & UFSHCD_UIC_DL_NAC_RECEIVED_ERROR)) {\n\t\tint err;\n\t\t \n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tmsleep(50);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\n\t\t \n\t\tif ((hba->saved_err & INT_FATAL_ERRORS) ||\n\t\t    ((hba->saved_err & UIC_ERROR) &&\n\t\t    (hba->saved_uic_err & ~UFSHCD_UIC_DL_NAC_RECEIVED_ERROR)))\n\t\t\tgoto out;\n\n\t\t \n\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\terr = ufshcd_verify_dev_init(hba);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (hba->saved_uic_err == UFSHCD_UIC_DL_NAC_RECEIVED_ERROR)\n\t\t\thba->saved_err &= ~UIC_ERROR;\n\t\t \n\t\thba->saved_uic_err &= ~UFSHCD_UIC_DL_NAC_RECEIVED_ERROR;\n\t\tif (!hba->saved_uic_err)\n\t\t\terr_handling = false;\n\t}\nout:\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\treturn err_handling;\n}\n\n \nstatic inline bool ufshcd_is_saved_err_fatal(struct ufs_hba *hba)\n{\n\treturn (hba->saved_uic_err & UFSHCD_UIC_DL_PA_INIT_ERROR) ||\n\t       (hba->saved_err & (INT_FATAL_ERRORS | UFSHCD_UIC_HIBERN8_MASK));\n}\n\nvoid ufshcd_schedule_eh_work(struct ufs_hba *hba)\n{\n\tlockdep_assert_held(hba->host->host_lock);\n\n\t \n\tif (hba->ufshcd_state != UFSHCD_STATE_ERROR) {\n\t\tif (hba->force_reset || ufshcd_is_link_broken(hba) ||\n\t\t    ufshcd_is_saved_err_fatal(hba))\n\t\t\thba->ufshcd_state = UFSHCD_STATE_EH_SCHEDULED_FATAL;\n\t\telse\n\t\t\thba->ufshcd_state = UFSHCD_STATE_EH_SCHEDULED_NON_FATAL;\n\t\tqueue_work(hba->eh_wq, &hba->eh_work);\n\t}\n}\n\nstatic void ufshcd_force_error_recovery(struct ufs_hba *hba)\n{\n\tspin_lock_irq(hba->host->host_lock);\n\thba->force_reset = true;\n\tufshcd_schedule_eh_work(hba);\n\tspin_unlock_irq(hba->host->host_lock);\n}\n\nstatic void ufshcd_clk_scaling_allow(struct ufs_hba *hba, bool allow)\n{\n\tmutex_lock(&hba->wb_mutex);\n\tdown_write(&hba->clk_scaling_lock);\n\thba->clk_scaling.is_allowed = allow;\n\tup_write(&hba->clk_scaling_lock);\n\tmutex_unlock(&hba->wb_mutex);\n}\n\nstatic void ufshcd_clk_scaling_suspend(struct ufs_hba *hba, bool suspend)\n{\n\tif (suspend) {\n\t\tif (hba->clk_scaling.is_enabled)\n\t\t\tufshcd_suspend_clkscaling(hba);\n\t\tufshcd_clk_scaling_allow(hba, false);\n\t} else {\n\t\tufshcd_clk_scaling_allow(hba, true);\n\t\tif (hba->clk_scaling.is_enabled)\n\t\t\tufshcd_resume_clkscaling(hba);\n\t}\n}\n\nstatic void ufshcd_err_handling_prepare(struct ufs_hba *hba)\n{\n\tufshcd_rpm_get_sync(hba);\n\tif (pm_runtime_status_suspended(&hba->ufs_device_wlun->sdev_gendev) ||\n\t    hba->is_sys_suspended) {\n\t\tenum ufs_pm_op pm_op;\n\n\t\t \n\t\tufshcd_setup_hba_vreg(hba, true);\n\t\tufshcd_enable_irq(hba);\n\t\tufshcd_setup_vreg(hba, true);\n\t\tufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq);\n\t\tufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq2);\n\t\tufshcd_hold(hba);\n\t\tif (!ufshcd_is_clkgating_allowed(hba))\n\t\t\tufshcd_setup_clocks(hba, true);\n\t\tufshcd_release(hba);\n\t\tpm_op = hba->is_sys_suspended ? UFS_SYSTEM_PM : UFS_RUNTIME_PM;\n\t\tufshcd_vops_resume(hba, pm_op);\n\t} else {\n\t\tufshcd_hold(hba);\n\t\tif (ufshcd_is_clkscaling_supported(hba) &&\n\t\t    hba->clk_scaling.is_enabled)\n\t\t\tufshcd_suspend_clkscaling(hba);\n\t\tufshcd_clk_scaling_allow(hba, false);\n\t}\n\tufshcd_scsi_block_requests(hba);\n\t \n\tblk_mq_wait_quiesce_done(&hba->host->tag_set);\n\tcancel_work_sync(&hba->eeh_work);\n}\n\nstatic void ufshcd_err_handling_unprepare(struct ufs_hba *hba)\n{\n\tufshcd_scsi_unblock_requests(hba);\n\tufshcd_release(hba);\n\tif (ufshcd_is_clkscaling_supported(hba))\n\t\tufshcd_clk_scaling_suspend(hba, false);\n\tufshcd_rpm_put(hba);\n}\n\nstatic inline bool ufshcd_err_handling_should_stop(struct ufs_hba *hba)\n{\n\treturn (!hba->is_powered || hba->shutting_down ||\n\t\t!hba->ufs_device_wlun ||\n\t\thba->ufshcd_state == UFSHCD_STATE_ERROR ||\n\t\t(!(hba->saved_err || hba->saved_uic_err || hba->force_reset ||\n\t\t   ufshcd_is_link_broken(hba))));\n}\n\n#ifdef CONFIG_PM\nstatic void ufshcd_recover_pm_error(struct ufs_hba *hba)\n{\n\tstruct Scsi_Host *shost = hba->host;\n\tstruct scsi_device *sdev;\n\tstruct request_queue *q;\n\tint ret;\n\n\thba->is_sys_suspended = false;\n\t \n\tret = pm_runtime_set_active(&hba->ufs_device_wlun->sdev_gendev);\n\n\t \n\tif (ret)\n\t\tret = pm_runtime_set_active(hba->dev);\n\t \n\tif (!ret) {\n\t\tshost_for_each_device(sdev, shost) {\n\t\t\tq = sdev->request_queue;\n\t\t\tif (q->dev && (q->rpm_status == RPM_SUSPENDED ||\n\t\t\t\t       q->rpm_status == RPM_SUSPENDING))\n\t\t\t\tpm_request_resume(q->dev);\n\t\t}\n\t}\n}\n#else\nstatic inline void ufshcd_recover_pm_error(struct ufs_hba *hba)\n{\n}\n#endif\n\nstatic bool ufshcd_is_pwr_mode_restore_needed(struct ufs_hba *hba)\n{\n\tstruct ufs_pa_layer_attr *pwr_info = &hba->pwr_info;\n\tu32 mode;\n\n\tufshcd_dme_get(hba, UIC_ARG_MIB(PA_PWRMODE), &mode);\n\n\tif (pwr_info->pwr_rx != ((mode >> PWRMODE_RX_OFFSET) & PWRMODE_MASK))\n\t\treturn true;\n\n\tif (pwr_info->pwr_tx != (mode & PWRMODE_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool ufshcd_abort_one(struct request *rq, void *priv)\n{\n\tint *ret = priv;\n\tu32 tag = rq->tag;\n\tstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\n\tstruct scsi_device *sdev = cmd->device;\n\tstruct Scsi_Host *shost = sdev->host;\n\tstruct ufs_hba *hba = shost_priv(shost);\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[tag];\n\tstruct ufs_hw_queue *hwq;\n\tunsigned long flags;\n\n\t*ret = ufshcd_try_to_abort_task(hba, tag);\n\tdev_err(hba->dev, \"Aborting tag %d / CDB %#02x %s\\n\", tag,\n\t\thba->lrb[tag].cmd ? hba->lrb[tag].cmd->cmnd[0] : -1,\n\t\t*ret ? \"failed\" : \"succeeded\");\n\n\t \n\tif (is_mcq_enabled(hba) && (*ret == 0)) {\n\t\thwq = ufshcd_mcq_req_to_hwq(hba, scsi_cmd_to_rq(lrbp->cmd));\n\t\tspin_lock_irqsave(&hwq->cq_lock, flags);\n\t\tif (ufshcd_cmd_inflight(lrbp->cmd))\n\t\t\tufshcd_release_scsi_cmd(hba, lrbp);\n\t\tspin_unlock_irqrestore(&hwq->cq_lock, flags);\n\t}\n\n\treturn *ret == 0;\n}\n\n \nstatic bool ufshcd_abort_all(struct ufs_hba *hba)\n{\n\tint tag, ret = 0;\n\n\tblk_mq_tagset_busy_iter(&hba->host->tag_set, ufshcd_abort_one, &ret);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tfor_each_set_bit(tag, &hba->outstanding_tasks, hba->nutmrs) {\n\t\tret = ufshcd_clear_tm_cmd(hba, tag);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\t \n\tufshcd_complete_requests(hba, false);\n\n\treturn ret != 0;\n}\n\n \nstatic void ufshcd_err_handler(struct work_struct *work)\n{\n\tint retries = MAX_ERR_HANDLER_RETRIES;\n\tstruct ufs_hba *hba;\n\tunsigned long flags;\n\tbool needs_restore;\n\tbool needs_reset;\n\tint pmc_err;\n\n\thba = container_of(work, struct ufs_hba, eh_work);\n\n\tdev_info(hba->dev,\n\t\t \"%s started; HBA state %s; powered %d; shutting down %d; saved_err = %d; saved_uic_err = %d; force_reset = %d%s\\n\",\n\t\t __func__, ufshcd_state_name[hba->ufshcd_state],\n\t\t hba->is_powered, hba->shutting_down, hba->saved_err,\n\t\t hba->saved_uic_err, hba->force_reset,\n\t\t ufshcd_is_link_broken(hba) ? \"; link is broken\" : \"\");\n\n\tdown(&hba->host_sem);\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (ufshcd_err_handling_should_stop(hba)) {\n\t\tif (hba->ufshcd_state != UFSHCD_STATE_ERROR)\n\t\t\thba->ufshcd_state = UFSHCD_STATE_OPERATIONAL;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tup(&hba->host_sem);\n\t\treturn;\n\t}\n\tufshcd_set_eh_in_progress(hba);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tufshcd_err_handling_prepare(hba);\n\t \n\tufshcd_complete_requests(hba, false);\n\tspin_lock_irqsave(hba->host->host_lock, flags);\nagain:\n\tneeds_restore = false;\n\tneeds_reset = false;\n\n\tif (hba->ufshcd_state != UFSHCD_STATE_ERROR)\n\t\thba->ufshcd_state = UFSHCD_STATE_RESET;\n\t \n\tif (ufshcd_err_handling_should_stop(hba))\n\t\tgoto skip_err_handling;\n\n\tif (hba->dev_quirks & UFS_DEVICE_QUIRK_RECOVERY_FROM_DL_NAC_ERRORS) {\n\t\tbool ret;\n\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\t \n\t\tret = ufshcd_quirk_dl_nac_errors(hba);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tif (!ret && ufshcd_err_handling_should_stop(hba))\n\t\t\tgoto skip_err_handling;\n\t}\n\n\tif ((hba->saved_err & (INT_FATAL_ERRORS | UFSHCD_UIC_HIBERN8_MASK)) ||\n\t    (hba->saved_uic_err &&\n\t     (hba->saved_uic_err != UFSHCD_UIC_PA_GENERIC_ERROR))) {\n\t\tbool pr_prdt = !!(hba->saved_err & SYSTEM_BUS_FATAL_ERROR);\n\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tufshcd_print_host_state(hba);\n\t\tufshcd_print_pwr_info(hba);\n\t\tufshcd_print_evt_hist(hba);\n\t\tufshcd_print_tmrs(hba, hba->outstanding_tasks);\n\t\tufshcd_print_trs_all(hba, pr_prdt);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t}\n\n\t \n\tif (hba->force_reset || ufshcd_is_link_broken(hba) ||\n\t    ufshcd_is_saved_err_fatal(hba) ||\n\t    ((hba->saved_err & UIC_ERROR) &&\n\t     (hba->saved_uic_err & (UFSHCD_UIC_DL_NAC_RECEIVED_ERROR |\n\t\t\t\t    UFSHCD_UIC_DL_TCx_REPLAY_ERROR)))) {\n\t\tneeds_reset = true;\n\t\tgoto do_reset;\n\t}\n\n\t \n\tif (hba->saved_uic_err & UFSHCD_UIC_PA_GENERIC_ERROR) {\n\t\thba->saved_uic_err &= ~UFSHCD_UIC_PA_GENERIC_ERROR;\n\t\tif (!hba->saved_uic_err)\n\t\t\thba->saved_err &= ~UIC_ERROR;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\tif (ufshcd_is_pwr_mode_restore_needed(hba))\n\t\t\tneeds_restore = true;\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tif (!hba->saved_err && !needs_restore)\n\t\t\tgoto skip_err_handling;\n\t}\n\n\thba->silence_err_logs = true;\n\t \n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tneeds_reset = ufshcd_abort_all(hba);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->silence_err_logs = false;\n\tif (needs_reset)\n\t\tgoto do_reset;\n\n\t \n\tif (needs_restore) {\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\t \n\t\tdown_write(&hba->clk_scaling_lock);\n\t\thba->force_pmc = true;\n\t\tpmc_err = ufshcd_config_pwr_mode(hba, &(hba->pwr_info));\n\t\tif (pmc_err) {\n\t\t\tneeds_reset = true;\n\t\t\tdev_err(hba->dev, \"%s: Failed to restore power mode, err = %d\\n\",\n\t\t\t\t\t__func__, pmc_err);\n\t\t}\n\t\thba->force_pmc = false;\n\t\tufshcd_print_pwr_info(hba);\n\t\tup_write(&hba->clk_scaling_lock);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t}\n\ndo_reset:\n\t \n\tif (needs_reset) {\n\t\tint err;\n\n\t\thba->force_reset = false;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t\terr = ufshcd_reset_and_restore(hba);\n\t\tif (err)\n\t\t\tdev_err(hba->dev, \"%s: reset and restore failed with err %d\\n\",\n\t\t\t\t\t__func__, err);\n\t\telse\n\t\t\tufshcd_recover_pm_error(hba);\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t}\n\nskip_err_handling:\n\tif (!needs_reset) {\n\t\tif (hba->ufshcd_state == UFSHCD_STATE_RESET)\n\t\t\thba->ufshcd_state = UFSHCD_STATE_OPERATIONAL;\n\t\tif (hba->saved_err || hba->saved_uic_err)\n\t\t\tdev_err_ratelimited(hba->dev, \"%s: exit: saved_err 0x%x saved_uic_err 0x%x\",\n\t\t\t    __func__, hba->saved_err, hba->saved_uic_err);\n\t}\n\t \n\tif (hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL &&\n\t    hba->ufshcd_state != UFSHCD_STATE_ERROR) {\n\t\tif (--retries)\n\t\t\tgoto again;\n\t\thba->ufshcd_state = UFSHCD_STATE_ERROR;\n\t}\n\tufshcd_clear_eh_in_progress(hba);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\tufshcd_err_handling_unprepare(hba);\n\tup(&hba->host_sem);\n\n\tdev_info(hba->dev, \"%s finished; HBA state %s\\n\", __func__,\n\t\t ufshcd_state_name[hba->ufshcd_state]);\n}\n\n \nstatic irqreturn_t ufshcd_update_uic_error(struct ufs_hba *hba)\n{\n\tu32 reg;\n\tirqreturn_t retval = IRQ_NONE;\n\n\t \n\treg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_PHY_ADAPTER_LAYER);\n\tif ((reg & UIC_PHY_ADAPTER_LAYER_ERROR) &&\n\t    (reg & UIC_PHY_ADAPTER_LAYER_ERROR_CODE_MASK)) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_PA_ERR, reg);\n\t\t \n\t\tif (reg & UIC_PHY_ADAPTER_LAYER_LANE_ERR_MASK)\n\t\t\tdev_dbg(hba->dev, \"%s: UIC Lane error reported\\n\",\n\t\t\t\t\t__func__);\n\n\t\t \n\t\tif (reg & UIC_PHY_ADAPTER_LAYER_GENERIC_ERROR) {\n\t\t\tstruct uic_command *cmd = NULL;\n\n\t\t\thba->uic_error |= UFSHCD_UIC_PA_GENERIC_ERROR;\n\t\t\tif (hba->uic_async_done && hba->active_uic_cmd)\n\t\t\t\tcmd = hba->active_uic_cmd;\n\t\t\t \n\t\t\tif (cmd && (cmd->command == UIC_CMD_DME_SET))\n\t\t\t\thba->uic_error &= ~UFSHCD_UIC_PA_GENERIC_ERROR;\n\t\t}\n\t\tretval |= IRQ_HANDLED;\n\t}\n\n\t \n\treg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_DATA_LINK_LAYER);\n\tif ((reg & UIC_DATA_LINK_LAYER_ERROR) &&\n\t    (reg & UIC_DATA_LINK_LAYER_ERROR_CODE_MASK)) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_DL_ERR, reg);\n\n\t\tif (reg & UIC_DATA_LINK_LAYER_ERROR_PA_INIT)\n\t\t\thba->uic_error |= UFSHCD_UIC_DL_PA_INIT_ERROR;\n\t\telse if (hba->dev_quirks &\n\t\t\t\tUFS_DEVICE_QUIRK_RECOVERY_FROM_DL_NAC_ERRORS) {\n\t\t\tif (reg & UIC_DATA_LINK_LAYER_ERROR_NAC_RECEIVED)\n\t\t\t\thba->uic_error |=\n\t\t\t\t\tUFSHCD_UIC_DL_NAC_RECEIVED_ERROR;\n\t\t\telse if (reg & UIC_DATA_LINK_LAYER_ERROR_TCx_REPLAY_TIMEOUT)\n\t\t\t\thba->uic_error |= UFSHCD_UIC_DL_TCx_REPLAY_ERROR;\n\t\t}\n\t\tretval |= IRQ_HANDLED;\n\t}\n\n\t \n\treg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_NETWORK_LAYER);\n\tif ((reg & UIC_NETWORK_LAYER_ERROR) &&\n\t    (reg & UIC_NETWORK_LAYER_ERROR_CODE_MASK)) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_NL_ERR, reg);\n\t\thba->uic_error |= UFSHCD_UIC_NL_ERROR;\n\t\tretval |= IRQ_HANDLED;\n\t}\n\n\treg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_TRANSPORT_LAYER);\n\tif ((reg & UIC_TRANSPORT_LAYER_ERROR) &&\n\t    (reg & UIC_TRANSPORT_LAYER_ERROR_CODE_MASK)) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_TL_ERR, reg);\n\t\thba->uic_error |= UFSHCD_UIC_TL_ERROR;\n\t\tretval |= IRQ_HANDLED;\n\t}\n\n\treg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_DME);\n\tif ((reg & UIC_DME_ERROR) &&\n\t    (reg & UIC_DME_ERROR_CODE_MASK)) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_DME_ERR, reg);\n\t\thba->uic_error |= UFSHCD_UIC_DME_ERROR;\n\t\tretval |= IRQ_HANDLED;\n\t}\n\n\tdev_dbg(hba->dev, \"%s: UIC error flags = 0x%08x\\n\",\n\t\t\t__func__, hba->uic_error);\n\treturn retval;\n}\n\n \nstatic irqreturn_t ufshcd_check_errors(struct ufs_hba *hba, u32 intr_status)\n{\n\tbool queue_eh_work = false;\n\tirqreturn_t retval = IRQ_NONE;\n\n\tspin_lock(hba->host->host_lock);\n\thba->errors |= UFSHCD_ERROR_MASK & intr_status;\n\n\tif (hba->errors & INT_FATAL_ERRORS) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_FATAL_ERR,\n\t\t\t\t       hba->errors);\n\t\tqueue_eh_work = true;\n\t}\n\n\tif (hba->errors & UIC_ERROR) {\n\t\thba->uic_error = 0;\n\t\tretval = ufshcd_update_uic_error(hba);\n\t\tif (hba->uic_error)\n\t\t\tqueue_eh_work = true;\n\t}\n\n\tif (hba->errors & UFSHCD_UIC_HIBERN8_MASK) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Auto Hibern8 %s failed - status: 0x%08x, upmcrs: 0x%08x\\n\",\n\t\t\t__func__, (hba->errors & UIC_HIBERNATE_ENTER) ?\n\t\t\t\"Enter\" : \"Exit\",\n\t\t\thba->errors, ufshcd_get_upmcrs(hba));\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_AUTO_HIBERN8_ERR,\n\t\t\t\t       hba->errors);\n\t\tufshcd_set_link_broken(hba);\n\t\tqueue_eh_work = true;\n\t}\n\n\tif (queue_eh_work) {\n\t\t \n\t\thba->saved_err |= hba->errors;\n\t\thba->saved_uic_err |= hba->uic_error;\n\n\t\t \n\t\tif ((hba->saved_err &\n\t\t     (INT_FATAL_ERRORS | UFSHCD_UIC_HIBERN8_MASK)) ||\n\t\t    (hba->saved_uic_err &&\n\t\t     (hba->saved_uic_err != UFSHCD_UIC_PA_GENERIC_ERROR))) {\n\t\t\tdev_err(hba->dev, \"%s: saved_err 0x%x saved_uic_err 0x%x\\n\",\n\t\t\t\t\t__func__, hba->saved_err,\n\t\t\t\t\thba->saved_uic_err);\n\t\t\tufshcd_dump_regs(hba, 0, UFSHCI_REG_SPACE_SIZE,\n\t\t\t\t\t \"host_regs: \");\n\t\t\tufshcd_print_pwr_info(hba);\n\t\t}\n\t\tufshcd_schedule_eh_work(hba);\n\t\tretval |= IRQ_HANDLED;\n\t}\n\t \n\thba->errors = 0;\n\thba->uic_error = 0;\n\tspin_unlock(hba->host->host_lock);\n\treturn retval;\n}\n\n \nstatic irqreturn_t ufshcd_tmc_handler(struct ufs_hba *hba)\n{\n\tunsigned long flags, pending, issued;\n\tirqreturn_t ret = IRQ_NONE;\n\tint tag;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tpending = ufshcd_readl(hba, REG_UTP_TASK_REQ_DOOR_BELL);\n\tissued = hba->outstanding_tasks & ~pending;\n\tfor_each_set_bit(tag, &issued, hba->nutmrs) {\n\t\tstruct request *req = hba->tmf_rqs[tag];\n\t\tstruct completion *c = req->end_io_data;\n\n\t\tcomplete(c);\n\t\tret = IRQ_HANDLED;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic irqreturn_t ufshcd_handle_mcq_cq_events(struct ufs_hba *hba)\n{\n\tstruct ufs_hw_queue *hwq;\n\tunsigned long outstanding_cqs;\n\tunsigned int nr_queues;\n\tint i, ret;\n\tu32 events;\n\n\tret = ufshcd_vops_get_outstanding_cqs(hba, &outstanding_cqs);\n\tif (ret)\n\t\toutstanding_cqs = (1U << hba->nr_hw_queues) - 1;\n\n\t \n\tnr_queues = hba->nr_hw_queues - hba->nr_queues[HCTX_TYPE_POLL];\n\tfor_each_set_bit(i, &outstanding_cqs, nr_queues) {\n\t\thwq = &hba->uhq[i];\n\n\t\tevents = ufshcd_mcq_read_cqis(hba, i);\n\t\tif (events)\n\t\t\tufshcd_mcq_write_cqis(hba, events, i);\n\n\t\tif (events & UFSHCD_MCQ_CQIS_TAIL_ENT_PUSH_STS)\n\t\t\tufshcd_mcq_poll_cqe_lock(hba, hwq);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t ufshcd_sl_intr(struct ufs_hba *hba, u32 intr_status)\n{\n\tirqreturn_t retval = IRQ_NONE;\n\n\tif (intr_status & UFSHCD_UIC_MASK)\n\t\tretval |= ufshcd_uic_cmd_compl(hba, intr_status);\n\n\tif (intr_status & UFSHCD_ERROR_MASK || hba->errors)\n\t\tretval |= ufshcd_check_errors(hba, intr_status);\n\n\tif (intr_status & UTP_TASK_REQ_COMPL)\n\t\tretval |= ufshcd_tmc_handler(hba);\n\n\tif (intr_status & UTP_TRANSFER_REQ_COMPL)\n\t\tretval |= ufshcd_transfer_req_compl(hba);\n\n\tif (intr_status & MCQ_CQ_EVENT_STATUS)\n\t\tretval |= ufshcd_handle_mcq_cq_events(hba);\n\n\treturn retval;\n}\n\n \nstatic irqreturn_t ufshcd_intr(int irq, void *__hba)\n{\n\tu32 intr_status, enabled_intr_status = 0;\n\tirqreturn_t retval = IRQ_NONE;\n\tstruct ufs_hba *hba = __hba;\n\tint retries = hba->nutrs;\n\n\tintr_status = ufshcd_readl(hba, REG_INTERRUPT_STATUS);\n\thba->ufs_stats.last_intr_status = intr_status;\n\thba->ufs_stats.last_intr_ts = local_clock();\n\n\t \n\twhile (intr_status && retries--) {\n\t\tenabled_intr_status =\n\t\t\tintr_status & ufshcd_readl(hba, REG_INTERRUPT_ENABLE);\n\t\tufshcd_writel(hba, intr_status, REG_INTERRUPT_STATUS);\n\t\tif (enabled_intr_status)\n\t\t\tretval |= ufshcd_sl_intr(hba, enabled_intr_status);\n\n\t\tintr_status = ufshcd_readl(hba, REG_INTERRUPT_STATUS);\n\t}\n\n\tif (enabled_intr_status && retval == IRQ_NONE &&\n\t    (!(enabled_intr_status & UTP_TRANSFER_REQ_COMPL) ||\n\t     hba->outstanding_reqs) && !ufshcd_eh_in_progress(hba)) {\n\t\tdev_err(hba->dev, \"%s: Unhandled interrupt 0x%08x (0x%08x, 0x%08x)\\n\",\n\t\t\t\t\t__func__,\n\t\t\t\t\tintr_status,\n\t\t\t\t\thba->ufs_stats.last_intr_status,\n\t\t\t\t\tenabled_intr_status);\n\t\tufshcd_dump_regs(hba, 0, UFSHCI_REG_SPACE_SIZE, \"host_regs: \");\n\t}\n\n\treturn retval;\n}\n\nstatic int ufshcd_clear_tm_cmd(struct ufs_hba *hba, int tag)\n{\n\tint err = 0;\n\tu32 mask = 1 << tag;\n\tunsigned long flags;\n\n\tif (!test_bit(tag, &hba->outstanding_tasks))\n\t\tgoto out;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tufshcd_utmrl_clear(hba, tag);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t \n\terr = ufshcd_wait_for_register(hba,\n\t\t\tREG_UTP_TASK_REQ_DOOR_BELL,\n\t\t\tmask, 0, 1000, 1000);\n\n\tdev_err(hba->dev, \"Clearing task management function with tag %d %s\\n\",\n\t\ttag, err < 0 ? \"failed\" : \"succeeded\");\n\nout:\n\treturn err;\n}\n\nstatic int __ufshcd_issue_tm_cmd(struct ufs_hba *hba,\n\t\tstruct utp_task_req_desc *treq, u8 tm_function)\n{\n\tstruct request_queue *q = hba->tmf_queue;\n\tstruct Scsi_Host *host = hba->host;\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\tstruct request *req;\n\tunsigned long flags;\n\tint task_tag, err;\n\n\t \n\treq = blk_mq_alloc_request(q, REQ_OP_DRV_OUT, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq->end_io_data = &wait;\n\tufshcd_hold(hba);\n\n\tspin_lock_irqsave(host->host_lock, flags);\n\n\ttask_tag = req->tag;\n\tWARN_ONCE(task_tag < 0 || task_tag >= hba->nutmrs, \"Invalid tag %d\\n\",\n\t\t  task_tag);\n\thba->tmf_rqs[req->tag] = req;\n\ttreq->upiu_req.req_header.task_tag = task_tag;\n\n\tmemcpy(hba->utmrdl_base_addr + task_tag, treq, sizeof(*treq));\n\tufshcd_vops_setup_task_mgmt(hba, task_tag, tm_function);\n\n\t \n\t__set_bit(task_tag, &hba->outstanding_tasks);\n\n\tufshcd_writel(hba, 1 << task_tag, REG_UTP_TASK_REQ_DOOR_BELL);\n\t \n\twmb();\n\n\tspin_unlock_irqrestore(host->host_lock, flags);\n\n\tufshcd_add_tm_upiu_trace(hba, task_tag, UFS_TM_SEND);\n\n\t \n\terr = wait_for_completion_io_timeout(&wait,\n\t\t\tmsecs_to_jiffies(TM_CMD_TIMEOUT));\n\tif (!err) {\n\t\tufshcd_add_tm_upiu_trace(hba, task_tag, UFS_TM_ERR);\n\t\tdev_err(hba->dev, \"%s: task management cmd 0x%.2x timed-out\\n\",\n\t\t\t\t__func__, tm_function);\n\t\tif (ufshcd_clear_tm_cmd(hba, task_tag))\n\t\t\tdev_WARN(hba->dev, \"%s: unable to clear tm cmd (slot %d) after timeout\\n\",\n\t\t\t\t\t__func__, task_tag);\n\t\terr = -ETIMEDOUT;\n\t} else {\n\t\terr = 0;\n\t\tmemcpy(treq, hba->utmrdl_base_addr + task_tag, sizeof(*treq));\n\n\t\tufshcd_add_tm_upiu_trace(hba, task_tag, UFS_TM_COMP);\n\t}\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->tmf_rqs[req->tag] = NULL;\n\t__clear_bit(task_tag, &hba->outstanding_tasks);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tufshcd_release(hba);\n\tblk_mq_free_request(req);\n\n\treturn err;\n}\n\n \nstatic int ufshcd_issue_tm_cmd(struct ufs_hba *hba, int lun_id, int task_id,\n\t\tu8 tm_function, u8 *tm_response)\n{\n\tstruct utp_task_req_desc treq = { };\n\tenum utp_ocs ocs_value;\n\tint err;\n\n\t \n\ttreq.header.interrupt = 1;\n\ttreq.header.ocs = OCS_INVALID_COMMAND_STATUS;\n\n\t \n\ttreq.upiu_req.req_header.transaction_code = UPIU_TRANSACTION_TASK_REQ;\n\ttreq.upiu_req.req_header.lun = lun_id;\n\ttreq.upiu_req.req_header.tm_function = tm_function;\n\n\t \n\ttreq.upiu_req.input_param1 = cpu_to_be32(lun_id);\n\ttreq.upiu_req.input_param2 = cpu_to_be32(task_id);\n\n\terr = __ufshcd_issue_tm_cmd(hba, &treq, tm_function);\n\tif (err == -ETIMEDOUT)\n\t\treturn err;\n\n\tocs_value = treq.header.ocs & MASK_OCS;\n\tif (ocs_value != OCS_SUCCESS)\n\t\tdev_err(hba->dev, \"%s: failed, ocs = 0x%x\\n\",\n\t\t\t\t__func__, ocs_value);\n\telse if (tm_response)\n\t\t*tm_response = be32_to_cpu(treq.upiu_rsp.output_param1) &\n\t\t\t\tMASK_TM_SERVICE_RESP;\n\treturn err;\n}\n\n \nstatic int ufshcd_issue_devman_upiu_cmd(struct ufs_hba *hba,\n\t\t\t\t\tstruct utp_upiu_req *req_upiu,\n\t\t\t\t\tstruct utp_upiu_req *rsp_upiu,\n\t\t\t\t\tu8 *desc_buff, int *buff_len,\n\t\t\t\t\tenum dev_cmd_type cmd_type,\n\t\t\t\t\tenum query_opcode desc_op)\n{\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\tconst u32 tag = hba->reserved_slot;\n\tstruct ufshcd_lrb *lrbp;\n\tint err = 0;\n\tu8 upiu_flags;\n\n\t \n\tlockdep_assert_held(&hba->dev_cmd.lock);\n\n\tdown_read(&hba->clk_scaling_lock);\n\n\tlrbp = &hba->lrb[tag];\n\tlrbp->cmd = NULL;\n\tlrbp->task_tag = tag;\n\tlrbp->lun = 0;\n\tlrbp->intr_cmd = true;\n\tufshcd_prepare_lrbp_crypto(NULL, lrbp);\n\thba->dev_cmd.type = cmd_type;\n\n\tif (hba->ufs_version <= ufshci_version(1, 1))\n\t\tlrbp->command_type = UTP_CMD_TYPE_DEV_MANAGE;\n\telse\n\t\tlrbp->command_type = UTP_CMD_TYPE_UFS_STORAGE;\n\n\t \n\treq_upiu->header.task_tag = tag;\n\n\tufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, DMA_NONE, 0);\n\n\t \n\tmemcpy(lrbp->ucd_req_ptr, req_upiu, sizeof(*lrbp->ucd_req_ptr));\n\tif (desc_buff && desc_op == UPIU_QUERY_OPCODE_WRITE_DESC) {\n\t\t \n\t\tmemcpy(lrbp->ucd_req_ptr + 1, desc_buff, *buff_len);\n\t\t*buff_len = 0;\n\t}\n\n\tmemset(lrbp->ucd_rsp_ptr, 0, sizeof(struct utp_upiu_rsp));\n\n\thba->dev_cmd.complete = &wait;\n\n\tufshcd_add_query_upiu_trace(hba, UFS_QUERY_SEND, lrbp->ucd_req_ptr);\n\n\tufshcd_send_command(hba, tag, hba->dev_cmd_queue);\n\t \n\tufshcd_wait_for_dev_cmd(hba, lrbp, QUERY_REQ_TIMEOUT);\n\n\t \n\tmemcpy(rsp_upiu, lrbp->ucd_rsp_ptr, sizeof(*rsp_upiu));\n\tif (desc_buff && desc_op == UPIU_QUERY_OPCODE_READ_DESC) {\n\t\tu8 *descp = (u8 *)lrbp->ucd_rsp_ptr + sizeof(*rsp_upiu);\n\t\tu16 resp_len = be16_to_cpu(lrbp->ucd_rsp_ptr->header\n\t\t\t\t\t   .data_segment_length);\n\n\t\tif (*buff_len >= resp_len) {\n\t\t\tmemcpy(desc_buff, descp, resp_len);\n\t\t\t*buff_len = resp_len;\n\t\t} else {\n\t\t\tdev_warn(hba->dev,\n\t\t\t\t \"%s: rsp size %d is bigger than buffer size %d\",\n\t\t\t\t __func__, resp_len, *buff_len);\n\t\t\t*buff_len = 0;\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\tufshcd_add_query_upiu_trace(hba, err ? UFS_QUERY_ERR : UFS_QUERY_COMP,\n\t\t\t\t    (struct utp_upiu_req *)lrbp->ucd_rsp_ptr);\n\n\tup_read(&hba->clk_scaling_lock);\n\treturn err;\n}\n\n \nint ufshcd_exec_raw_upiu_cmd(struct ufs_hba *hba,\n\t\t\t     struct utp_upiu_req *req_upiu,\n\t\t\t     struct utp_upiu_req *rsp_upiu,\n\t\t\t     enum upiu_request_transaction msgcode,\n\t\t\t     u8 *desc_buff, int *buff_len,\n\t\t\t     enum query_opcode desc_op)\n{\n\tint err;\n\tenum dev_cmd_type cmd_type = DEV_CMD_TYPE_QUERY;\n\tstruct utp_task_req_desc treq = { };\n\tenum utp_ocs ocs_value;\n\tu8 tm_f = req_upiu->header.tm_function;\n\n\tswitch (msgcode) {\n\tcase UPIU_TRANSACTION_NOP_OUT:\n\t\tcmd_type = DEV_CMD_TYPE_NOP;\n\t\tfallthrough;\n\tcase UPIU_TRANSACTION_QUERY_REQ:\n\t\tufshcd_hold(hba);\n\t\tmutex_lock(&hba->dev_cmd.lock);\n\t\terr = ufshcd_issue_devman_upiu_cmd(hba, req_upiu, rsp_upiu,\n\t\t\t\t\t\t   desc_buff, buff_len,\n\t\t\t\t\t\t   cmd_type, desc_op);\n\t\tmutex_unlock(&hba->dev_cmd.lock);\n\t\tufshcd_release(hba);\n\n\t\tbreak;\n\tcase UPIU_TRANSACTION_TASK_REQ:\n\t\ttreq.header.interrupt = 1;\n\t\ttreq.header.ocs = OCS_INVALID_COMMAND_STATUS;\n\n\t\tmemcpy(&treq.upiu_req, req_upiu, sizeof(*req_upiu));\n\n\t\terr = __ufshcd_issue_tm_cmd(hba, &treq, tm_f);\n\t\tif (err == -ETIMEDOUT)\n\t\t\tbreak;\n\n\t\tocs_value = treq.header.ocs & MASK_OCS;\n\t\tif (ocs_value != OCS_SUCCESS) {\n\t\t\tdev_err(hba->dev, \"%s: failed, ocs = 0x%x\\n\", __func__,\n\t\t\t\tocs_value);\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(rsp_upiu, &treq.upiu_rsp, sizeof(*rsp_upiu));\n\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nint ufshcd_advanced_rpmb_req_handler(struct ufs_hba *hba, struct utp_upiu_req *req_upiu,\n\t\t\t struct utp_upiu_req *rsp_upiu, struct ufs_ehs *req_ehs,\n\t\t\t struct ufs_ehs *rsp_ehs, int sg_cnt, struct scatterlist *sg_list,\n\t\t\t enum dma_data_direction dir)\n{\n\tDECLARE_COMPLETION_ONSTACK(wait);\n\tconst u32 tag = hba->reserved_slot;\n\tstruct ufshcd_lrb *lrbp;\n\tint err = 0;\n\tint result;\n\tu8 upiu_flags;\n\tu8 *ehs_data;\n\tu16 ehs_len;\n\n\t \n\tufshcd_hold(hba);\n\tmutex_lock(&hba->dev_cmd.lock);\n\tdown_read(&hba->clk_scaling_lock);\n\n\tlrbp = &hba->lrb[tag];\n\tlrbp->cmd = NULL;\n\tlrbp->task_tag = tag;\n\tlrbp->lun = UFS_UPIU_RPMB_WLUN;\n\n\tlrbp->intr_cmd = true;\n\tufshcd_prepare_lrbp_crypto(NULL, lrbp);\n\thba->dev_cmd.type = DEV_CMD_TYPE_RPMB;\n\n\t \n\tlrbp->command_type = UTP_CMD_TYPE_UFS_STORAGE;\n\n\t \n\tif (hba->capabilities & MASK_EHSLUTRD_SUPPORTED)\n\t\tufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, dir, 2);\n\telse\n\t\tufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, dir, 0);\n\n\t \n\treq_upiu->header.task_tag = tag;\n\n\t \n\tmemcpy(lrbp->ucd_req_ptr, req_upiu, sizeof(*lrbp->ucd_req_ptr));\n\t \n\tmemcpy(lrbp->ucd_req_ptr + 1, req_ehs, sizeof(*req_ehs));\n\n\tif (dir != DMA_NONE && sg_list)\n\t\tufshcd_sgl_to_prdt(hba, lrbp, sg_cnt, sg_list);\n\n\tmemset(lrbp->ucd_rsp_ptr, 0, sizeof(struct utp_upiu_rsp));\n\n\thba->dev_cmd.complete = &wait;\n\n\tufshcd_send_command(hba, tag, hba->dev_cmd_queue);\n\n\terr = ufshcd_wait_for_dev_cmd(hba, lrbp, ADVANCED_RPMB_REQ_TIMEOUT);\n\n\tif (!err) {\n\t\t \n\t\tmemcpy(rsp_upiu, lrbp->ucd_rsp_ptr, sizeof(*rsp_upiu));\n\t\t \n\t\tresult = (lrbp->ucd_rsp_ptr->header.response << 8) |\n\t\t\tlrbp->ucd_rsp_ptr->header.status;\n\n\t\tehs_len = lrbp->ucd_rsp_ptr->header.ehs_length;\n\t\t \n\t\tif (ehs_len == 2 && rsp_ehs) {\n\t\t\t \n\t\t\tehs_data = (u8 *)lrbp->ucd_rsp_ptr + EHS_OFFSET_IN_RESPONSE;\n\t\t\tmemcpy(rsp_ehs, ehs_data, ehs_len * 32);\n\t\t}\n\t}\n\n\tup_read(&hba->clk_scaling_lock);\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n\treturn err ? : result;\n}\n\n \nstatic int ufshcd_eh_device_reset_handler(struct scsi_cmnd *cmd)\n{\n\tunsigned long flags, pending_reqs = 0, not_cleared = 0;\n\tstruct Scsi_Host *host;\n\tstruct ufs_hba *hba;\n\tstruct ufs_hw_queue *hwq;\n\tstruct ufshcd_lrb *lrbp;\n\tu32 pos, not_cleared_mask = 0;\n\tint err;\n\tu8 resp = 0xF, lun;\n\n\thost = cmd->device->host;\n\thba = shost_priv(host);\n\n\tlun = ufshcd_scsi_to_upiu_lun(cmd->device->lun);\n\terr = ufshcd_issue_tm_cmd(hba, lun, 0, UFS_LOGICAL_RESET, &resp);\n\tif (err || resp != UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\n\t\tif (!err)\n\t\t\terr = resp;\n\t\tgoto out;\n\t}\n\n\tif (is_mcq_enabled(hba)) {\n\t\tfor (pos = 0; pos < hba->nutrs; pos++) {\n\t\t\tlrbp = &hba->lrb[pos];\n\t\t\tif (ufshcd_cmd_inflight(lrbp->cmd) &&\n\t\t\t    lrbp->lun == lun) {\n\t\t\t\tufshcd_clear_cmd(hba, pos);\n\t\t\t\thwq = ufshcd_mcq_req_to_hwq(hba, scsi_cmd_to_rq(lrbp->cmd));\n\t\t\t\tufshcd_mcq_poll_cqe_lock(hba, hwq);\n\t\t\t}\n\t\t}\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t \n\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\tfor_each_set_bit(pos, &hba->outstanding_reqs, hba->nutrs)\n\t\tif (hba->lrb[pos].lun == lun)\n\t\t\t__set_bit(pos, &pending_reqs);\n\thba->outstanding_reqs &= ~pending_reqs;\n\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\tfor_each_set_bit(pos, &pending_reqs, hba->nutrs) {\n\t\tif (ufshcd_clear_cmd(hba, pos) < 0) {\n\t\t\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\t\t\tnot_cleared = 1U << pos &\n\t\t\t\tufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\t\t\thba->outstanding_reqs |= not_cleared;\n\t\t\tnot_cleared_mask |= not_cleared;\n\t\t\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\t\t\tdev_err(hba->dev, \"%s: failed to clear request %d\\n\",\n\t\t\t\t__func__, pos);\n\t\t}\n\t}\n\t__ufshcd_transfer_req_compl(hba, pending_reqs & ~not_cleared_mask);\n\nout:\n\thba->req_abort_count = 0;\n\tufshcd_update_evt_hist(hba, UFS_EVT_DEV_RESET, (u32)err);\n\tif (!err) {\n\t\terr = SUCCESS;\n\t} else {\n\t\tdev_err(hba->dev, \"%s: failed with err %d\\n\", __func__, err);\n\t\terr = FAILED;\n\t}\n\treturn err;\n}\n\nstatic void ufshcd_set_req_abort_skip(struct ufs_hba *hba, unsigned long bitmap)\n{\n\tstruct ufshcd_lrb *lrbp;\n\tint tag;\n\n\tfor_each_set_bit(tag, &bitmap, hba->nutrs) {\n\t\tlrbp = &hba->lrb[tag];\n\t\tlrbp->req_abort_skip = true;\n\t}\n}\n\n \nint ufshcd_try_to_abort_task(struct ufs_hba *hba, int tag)\n{\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[tag];\n\tint err = 0;\n\tint poll_cnt;\n\tu8 resp = 0xF;\n\tu32 reg;\n\n\tfor (poll_cnt = 100; poll_cnt; poll_cnt--) {\n\t\terr = ufshcd_issue_tm_cmd(hba, lrbp->lun, lrbp->task_tag,\n\t\t\t\tUFS_QUERY_TASK, &resp);\n\t\tif (!err && resp == UPIU_TASK_MANAGEMENT_FUNC_SUCCEEDED) {\n\t\t\t \n\t\t\tdev_err(hba->dev, \"%s: cmd pending in the device. tag = %d\\n\",\n\t\t\t\t__func__, tag);\n\t\t\tbreak;\n\t\t} else if (!err && resp == UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\n\t\t\t \n\t\t\tdev_err(hba->dev, \"%s: cmd at tag %d not pending in the device.\\n\",\n\t\t\t\t__func__, tag);\n\t\t\tif (is_mcq_enabled(hba)) {\n\t\t\t\t \n\t\t\t\tif (ufshcd_cmd_inflight(lrbp->cmd)) {\n\t\t\t\t\t \n\t\t\t\t\tusleep_range(100, 200);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tdev_err(hba->dev, \"%s: cmd at tag=%d is cleared.\\n\",\n\t\t\t\t\t__func__, tag);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t \n\t\t\treg = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\t\t\tif (reg & (1 << tag)) {\n\t\t\t\t \n\t\t\t\tusleep_range(100, 200);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tdev_err(hba->dev, \"%s: cmd at tag %d successfully cleared from DB.\\n\",\n\t\t\t\t__func__, tag);\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tdev_err(hba->dev,\n\t\t\t\t\"%s: no response from device. tag = %d, err %d\\n\",\n\t\t\t\t__func__, tag, err);\n\t\t\tif (!err)\n\t\t\t\terr = resp;  \n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!poll_cnt) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\terr = ufshcd_issue_tm_cmd(hba, lrbp->lun, lrbp->task_tag,\n\t\t\tUFS_ABORT_TASK, &resp);\n\tif (err || resp != UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\n\t\tif (!err) {\n\t\t\terr = resp;  \n\t\t\tdev_err(hba->dev, \"%s: issued. tag = %d, err %d\\n\",\n\t\t\t\t__func__, tag, err);\n\t\t}\n\t\tgoto out;\n\t}\n\n\terr = ufshcd_clear_cmd(hba, tag);\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: Failed clearing cmd at tag %d, err %d\\n\",\n\t\t\t__func__, tag, err);\n\nout:\n\treturn err;\n}\n\n \nstatic int ufshcd_abort(struct scsi_cmnd *cmd)\n{\n\tstruct Scsi_Host *host = cmd->device->host;\n\tstruct ufs_hba *hba = shost_priv(host);\n\tint tag = scsi_cmd_to_rq(cmd)->tag;\n\tstruct ufshcd_lrb *lrbp = &hba->lrb[tag];\n\tunsigned long flags;\n\tint err = FAILED;\n\tbool outstanding;\n\tu32 reg;\n\n\tWARN_ONCE(tag < 0, \"Invalid tag %d\\n\", tag);\n\n\tufshcd_hold(hba);\n\n\tif (!is_mcq_enabled(hba)) {\n\t\treg = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\n\t\tif (!test_bit(tag, &hba->outstanding_reqs)) {\n\t\t\t \n\t\t\tdev_err(hba->dev,\n\t\t\t\t\"%s: cmd at tag %d already completed, outstanding=0x%lx, doorbell=0x%x\\n\",\n\t\t\t\t__func__, tag, hba->outstanding_reqs, reg);\n\t\t\tgoto release;\n\t\t}\n\t}\n\n\t \n\tdev_info(hba->dev, \"%s: Device abort task at tag %d\\n\", __func__, tag);\n\n\t \n\tscsi_print_command(cmd);\n\tif (!hba->req_abort_count) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_ABORT, tag);\n\t\tufshcd_print_evt_hist(hba);\n\t\tufshcd_print_host_state(hba);\n\t\tufshcd_print_pwr_info(hba);\n\t\tufshcd_print_tr(hba, tag, true);\n\t} else {\n\t\tufshcd_print_tr(hba, tag, false);\n\t}\n\thba->req_abort_count++;\n\n\tif (!is_mcq_enabled(hba) && !(reg & (1 << tag))) {\n\t\t \n\t\tdev_err(hba->dev,\n\t\t\"%s: cmd was completed, but without a notifying intr, tag = %d\",\n\t\t__func__, tag);\n\t\t__ufshcd_transfer_req_compl(hba, 1UL << tag);\n\t\tgoto release;\n\t}\n\n\t \n\tif (lrbp->lun == UFS_UPIU_UFS_DEVICE_WLUN) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_ABORT, lrbp->lun);\n\n\t\tspin_lock_irqsave(host->host_lock, flags);\n\t\thba->force_reset = true;\n\t\tufshcd_schedule_eh_work(hba);\n\t\tspin_unlock_irqrestore(host->host_lock, flags);\n\t\tgoto release;\n\t}\n\n\tif (is_mcq_enabled(hba)) {\n\t\t \n\t\terr = ufshcd_mcq_abort(cmd);\n\t\tgoto release;\n\t}\n\n\t \n\tif (lrbp->req_abort_skip) {\n\t\tdev_err(hba->dev, \"%s: skipping abort\\n\", __func__);\n\t\tufshcd_set_req_abort_skip(hba, hba->outstanding_reqs);\n\t\tgoto release;\n\t}\n\n\terr = ufshcd_try_to_abort_task(hba, tag);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: failed with err %d\\n\", __func__, err);\n\t\tufshcd_set_req_abort_skip(hba, hba->outstanding_reqs);\n\t\terr = FAILED;\n\t\tgoto release;\n\t}\n\n\t \n\tspin_lock_irqsave(&hba->outstanding_lock, flags);\n\toutstanding = __test_and_clear_bit(tag, &hba->outstanding_reqs);\n\tspin_unlock_irqrestore(&hba->outstanding_lock, flags);\n\n\tif (outstanding)\n\t\tufshcd_release_scsi_cmd(hba, lrbp);\n\n\terr = SUCCESS;\n\nrelease:\n\t \n\tufshcd_release(hba);\n\treturn err;\n}\n\n \nstatic int ufshcd_host_reset_and_restore(struct ufs_hba *hba)\n{\n\tint err;\n\n\t \n\tufshcd_hba_stop(hba);\n\thba->silence_err_logs = true;\n\tufshcd_complete_requests(hba, true);\n\thba->silence_err_logs = false;\n\n\t \n\tufshcd_scale_clks(hba, true);\n\n\terr = ufshcd_hba_enable(hba);\n\n\t \n\tif (!err)\n\t\terr = ufshcd_probe_hba(hba, false);\n\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: Host init failed %d\\n\", __func__, err);\n\tufshcd_update_evt_hist(hba, UFS_EVT_HOST_RESET, (u32)err);\n\treturn err;\n}\n\n \nstatic int ufshcd_reset_and_restore(struct ufs_hba *hba)\n{\n\tu32 saved_err = 0;\n\tu32 saved_uic_err = 0;\n\tint err = 0;\n\tunsigned long flags;\n\tint retries = MAX_HOST_RESET_RETRIES;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tdo {\n\t\t \n\t\tsaved_err |= hba->saved_err;\n\t\tsaved_uic_err |= hba->saved_uic_err;\n\t\thba->saved_err = 0;\n\t\thba->saved_uic_err = 0;\n\t\thba->force_reset = false;\n\t\thba->ufshcd_state = UFSHCD_STATE_RESET;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t\t \n\t\tufshcd_device_reset(hba);\n\n\t\terr = ufshcd_host_reset_and_restore(hba);\n\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\tif (err)\n\t\t\tcontinue;\n\t\t \n\t\tif (hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL &&\n\t\t    hba->ufshcd_state != UFSHCD_STATE_ERROR &&\n\t\t    hba->ufshcd_state != UFSHCD_STATE_EH_SCHEDULED_NON_FATAL)\n\t\t\terr = -EAGAIN;\n\t} while (err && --retries);\n\n\t \n\tscsi_report_bus_reset(hba->host, 0);\n\tif (err) {\n\t\thba->ufshcd_state = UFSHCD_STATE_ERROR;\n\t\thba->saved_err |= saved_err;\n\t\thba->saved_uic_err |= saved_uic_err;\n\t}\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\treturn err;\n}\n\n \nstatic int ufshcd_eh_host_reset_handler(struct scsi_cmnd *cmd)\n{\n\tint err = SUCCESS;\n\tunsigned long flags;\n\tstruct ufs_hba *hba;\n\n\thba = shost_priv(cmd->device->host);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\thba->force_reset = true;\n\tufshcd_schedule_eh_work(hba);\n\tdev_err(hba->dev, \"%s: reset in progress - 1\\n\", __func__);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tflush_work(&hba->eh_work);\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (hba->ufshcd_state == UFSHCD_STATE_ERROR)\n\t\terr = FAILED;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\treturn err;\n}\n\n \nstatic u32 ufshcd_get_max_icc_level(int sup_curr_uA, u32 start_scan,\n\t\t\t\t    const char *buff)\n{\n\tint i;\n\tint curr_uA;\n\tu16 data;\n\tu16 unit;\n\n\tfor (i = start_scan; i >= 0; i--) {\n\t\tdata = get_unaligned_be16(&buff[2 * i]);\n\t\tunit = (data & ATTR_ICC_LVL_UNIT_MASK) >>\n\t\t\t\t\t\tATTR_ICC_LVL_UNIT_OFFSET;\n\t\tcurr_uA = data & ATTR_ICC_LVL_VALUE_MASK;\n\t\tswitch (unit) {\n\t\tcase UFSHCD_NANO_AMP:\n\t\t\tcurr_uA = curr_uA / 1000;\n\t\t\tbreak;\n\t\tcase UFSHCD_MILI_AMP:\n\t\t\tcurr_uA = curr_uA * 1000;\n\t\t\tbreak;\n\t\tcase UFSHCD_AMP:\n\t\t\tcurr_uA = curr_uA * 1000 * 1000;\n\t\t\tbreak;\n\t\tcase UFSHCD_MICRO_AMP:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tif (sup_curr_uA >= curr_uA)\n\t\t\tbreak;\n\t}\n\tif (i < 0) {\n\t\ti = 0;\n\t\tpr_err(\"%s: Couldn't find valid icc_level = %d\", __func__, i);\n\t}\n\n\treturn (u32)i;\n}\n\n \nstatic u32 ufshcd_find_max_sup_active_icc_level(struct ufs_hba *hba,\n\t\t\t\t\t\tconst u8 *desc_buf)\n{\n\tu32 icc_level = 0;\n\n\tif (!hba->vreg_info.vcc || !hba->vreg_info.vccq ||\n\t\t\t\t\t\t!hba->vreg_info.vccq2) {\n\t\t \n\t\tdev_dbg(hba->dev,\n\t\t\t\"%s: Regulator capability was not set, actvIccLevel=%d\",\n\t\t\t\t\t\t\t__func__, icc_level);\n\t\tgoto out;\n\t}\n\n\tif (hba->vreg_info.vcc->max_uA)\n\t\ticc_level = ufshcd_get_max_icc_level(\n\t\t\t\thba->vreg_info.vcc->max_uA,\n\t\t\t\tPOWER_DESC_MAX_ACTV_ICC_LVLS - 1,\n\t\t\t\t&desc_buf[PWR_DESC_ACTIVE_LVLS_VCC_0]);\n\n\tif (hba->vreg_info.vccq->max_uA)\n\t\ticc_level = ufshcd_get_max_icc_level(\n\t\t\t\thba->vreg_info.vccq->max_uA,\n\t\t\t\ticc_level,\n\t\t\t\t&desc_buf[PWR_DESC_ACTIVE_LVLS_VCCQ_0]);\n\n\tif (hba->vreg_info.vccq2->max_uA)\n\t\ticc_level = ufshcd_get_max_icc_level(\n\t\t\t\thba->vreg_info.vccq2->max_uA,\n\t\t\t\ticc_level,\n\t\t\t\t&desc_buf[PWR_DESC_ACTIVE_LVLS_VCCQ2_0]);\nout:\n\treturn icc_level;\n}\n\nstatic void ufshcd_set_active_icc_lvl(struct ufs_hba *hba)\n{\n\tint ret;\n\tu8 *desc_buf;\n\tu32 icc_level;\n\n\tdesc_buf = kzalloc(QUERY_DESC_MAX_SIZE, GFP_KERNEL);\n\tif (!desc_buf)\n\t\treturn;\n\n\tret = ufshcd_read_desc_param(hba, QUERY_DESC_IDN_POWER, 0, 0,\n\t\t\t\t     desc_buf, QUERY_DESC_MAX_SIZE);\n\tif (ret) {\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Failed reading power descriptor ret = %d\",\n\t\t\t__func__, ret);\n\t\tgoto out;\n\t}\n\n\ticc_level = ufshcd_find_max_sup_active_icc_level(hba, desc_buf);\n\tdev_dbg(hba->dev, \"%s: setting icc_level 0x%x\", __func__, icc_level);\n\n\tret = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\n\t\tQUERY_ATTR_IDN_ACTIVE_ICC_LVL, 0, 0, &icc_level);\n\n\tif (ret)\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Failed configuring bActiveICCLevel = %d ret = %d\",\n\t\t\t__func__, icc_level, ret);\n\nout:\n\tkfree(desc_buf);\n}\n\nstatic inline void ufshcd_blk_pm_runtime_init(struct scsi_device *sdev)\n{\n\tscsi_autopm_get_device(sdev);\n\tblk_pm_runtime_init(sdev->request_queue, &sdev->sdev_gendev);\n\tif (sdev->rpm_autosuspend)\n\t\tpm_runtime_set_autosuspend_delay(&sdev->sdev_gendev,\n\t\t\t\t\t\t RPM_AUTOSUSPEND_DELAY_MS);\n\tscsi_autopm_put_device(sdev);\n}\n\n \nstatic int ufshcd_scsi_add_wlus(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tstruct scsi_device *sdev_boot, *sdev_rpmb;\n\n\thba->ufs_device_wlun = __scsi_add_device(hba->host, 0, 0,\n\t\tufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_UFS_DEVICE_WLUN), NULL);\n\tif (IS_ERR(hba->ufs_device_wlun)) {\n\t\tret = PTR_ERR(hba->ufs_device_wlun);\n\t\thba->ufs_device_wlun = NULL;\n\t\tgoto out;\n\t}\n\tscsi_device_put(hba->ufs_device_wlun);\n\n\tsdev_rpmb = __scsi_add_device(hba->host, 0, 0,\n\t\tufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_RPMB_WLUN), NULL);\n\tif (IS_ERR(sdev_rpmb)) {\n\t\tret = PTR_ERR(sdev_rpmb);\n\t\tgoto remove_ufs_device_wlun;\n\t}\n\tufshcd_blk_pm_runtime_init(sdev_rpmb);\n\tscsi_device_put(sdev_rpmb);\n\n\tsdev_boot = __scsi_add_device(hba->host, 0, 0,\n\t\tufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_BOOT_WLUN), NULL);\n\tif (IS_ERR(sdev_boot)) {\n\t\tdev_err(hba->dev, \"%s: BOOT WLUN not found\\n\", __func__);\n\t} else {\n\t\tufshcd_blk_pm_runtime_init(sdev_boot);\n\t\tscsi_device_put(sdev_boot);\n\t}\n\tgoto out;\n\nremove_ufs_device_wlun:\n\tscsi_remove_device(hba->ufs_device_wlun);\nout:\n\treturn ret;\n}\n\nstatic void ufshcd_wb_probe(struct ufs_hba *hba, const u8 *desc_buf)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu8 lun;\n\tu32 d_lu_wb_buf_alloc;\n\tu32 ext_ufs_feature;\n\n\tif (!ufshcd_is_wb_allowed(hba))\n\t\treturn;\n\n\t \n\tif (!(dev_info->wspecversion >= 0x310 ||\n\t      dev_info->wspecversion == 0x220 ||\n\t     (hba->dev_quirks & UFS_DEVICE_QUIRK_SUPPORT_EXTENDED_FEATURES)))\n\t\tgoto wb_disabled;\n\n\text_ufs_feature = get_unaligned_be32(desc_buf +\n\t\t\t\t\tDEVICE_DESC_PARAM_EXT_UFS_FEATURE_SUP);\n\n\tif (!(ext_ufs_feature & UFS_DEV_WRITE_BOOSTER_SUP))\n\t\tgoto wb_disabled;\n\n\t \n\tdev_info->wb_buffer_type = desc_buf[DEVICE_DESC_PARAM_WB_TYPE];\n\n\tdev_info->b_presrv_uspc_en =\n\t\tdesc_buf[DEVICE_DESC_PARAM_WB_PRESRV_USRSPC_EN];\n\n\tif (dev_info->wb_buffer_type == WB_BUF_MODE_SHARED) {\n\t\tif (!get_unaligned_be32(desc_buf +\n\t\t\t\t   DEVICE_DESC_PARAM_WB_SHARED_ALLOC_UNITS))\n\t\t\tgoto wb_disabled;\n\t} else {\n\t\tfor (lun = 0; lun < UFS_UPIU_MAX_WB_LUN_ID; lun++) {\n\t\t\td_lu_wb_buf_alloc = 0;\n\t\t\tufshcd_read_unit_desc_param(hba,\n\t\t\t\t\tlun,\n\t\t\t\t\tUNIT_DESC_PARAM_WB_BUF_ALLOC_UNITS,\n\t\t\t\t\t(u8 *)&d_lu_wb_buf_alloc,\n\t\t\t\t\tsizeof(d_lu_wb_buf_alloc));\n\t\t\tif (d_lu_wb_buf_alloc) {\n\t\t\t\tdev_info->wb_dedicated_lu = lun;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!d_lu_wb_buf_alloc)\n\t\t\tgoto wb_disabled;\n\t}\n\n\tif (!ufshcd_is_wb_buf_lifetime_available(hba))\n\t\tgoto wb_disabled;\n\n\treturn;\n\nwb_disabled:\n\thba->caps &= ~UFSHCD_CAP_WB_EN;\n}\n\nstatic void ufshcd_temp_notif_probe(struct ufs_hba *hba, const u8 *desc_buf)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu32 ext_ufs_feature;\n\tu8 mask = 0;\n\n\tif (!(hba->caps & UFSHCD_CAP_TEMP_NOTIF) || dev_info->wspecversion < 0x300)\n\t\treturn;\n\n\text_ufs_feature = get_unaligned_be32(desc_buf + DEVICE_DESC_PARAM_EXT_UFS_FEATURE_SUP);\n\n\tif (ext_ufs_feature & UFS_DEV_LOW_TEMP_NOTIF)\n\t\tmask |= MASK_EE_TOO_LOW_TEMP;\n\n\tif (ext_ufs_feature & UFS_DEV_HIGH_TEMP_NOTIF)\n\t\tmask |= MASK_EE_TOO_HIGH_TEMP;\n\n\tif (mask) {\n\t\tufshcd_enable_ee(hba, mask);\n\t\tufs_hwmon_probe(hba, mask);\n\t}\n}\n\nstatic void ufshcd_ext_iid_probe(struct ufs_hba *hba, u8 *desc_buf)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu32 ext_ufs_feature;\n\tu32 ext_iid_en = 0;\n\tint err;\n\n\t \n\tif (dev_info->wspecversion < 0x400)\n\t\tgoto out;\n\n\text_ufs_feature = get_unaligned_be32(desc_buf +\n\t\t\t\t     DEVICE_DESC_PARAM_EXT_UFS_FEATURE_SUP);\n\tif (!(ext_ufs_feature & UFS_DEV_EXT_IID_SUP))\n\t\tgoto out;\n\n\terr = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\t\t      QUERY_ATTR_IDN_EXT_IID_EN, 0, 0, &ext_iid_en);\n\tif (err)\n\t\tdev_err(hba->dev, \"failed reading bEXTIIDEn. err = %d\\n\", err);\n\nout:\n\tdev_info->b_ext_iid_en = ext_iid_en;\n}\n\nvoid ufshcd_fixup_dev_quirks(struct ufs_hba *hba,\n\t\t\t     const struct ufs_dev_quirk *fixups)\n{\n\tconst struct ufs_dev_quirk *f;\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\n\tif (!fixups)\n\t\treturn;\n\n\tfor (f = fixups; f->quirk; f++) {\n\t\tif ((f->wmanufacturerid == dev_info->wmanufacturerid ||\n\t\t     f->wmanufacturerid == UFS_ANY_VENDOR) &&\n\t\t     ((dev_info->model &&\n\t\t       STR_PRFX_EQUAL(f->model, dev_info->model)) ||\n\t\t      !strcmp(f->model, UFS_ANY_MODEL)))\n\t\t\thba->dev_quirks |= f->quirk;\n\t}\n}\nEXPORT_SYMBOL_GPL(ufshcd_fixup_dev_quirks);\n\nstatic void ufs_fixup_device_setup(struct ufs_hba *hba)\n{\n\t \n\tufshcd_fixup_dev_quirks(hba, ufs_fixups);\n\n\t \n\tufshcd_vops_fixup_dev_quirks(hba);\n}\n\nstatic int ufs_get_device_desc(struct ufs_hba *hba)\n{\n\tint err;\n\tu8 model_index;\n\tu8 *desc_buf;\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\n\tdesc_buf = kzalloc(QUERY_DESC_MAX_SIZE, GFP_KERNEL);\n\tif (!desc_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = ufshcd_read_desc_param(hba, QUERY_DESC_IDN_DEVICE, 0, 0, desc_buf,\n\t\t\t\t     QUERY_DESC_MAX_SIZE);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: Failed reading Device Desc. err = %d\\n\",\n\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\t \n\tdev_info->wmanufacturerid = desc_buf[DEVICE_DESC_PARAM_MANF_ID] << 8 |\n\t\t\t\t     desc_buf[DEVICE_DESC_PARAM_MANF_ID + 1];\n\n\t \n\tdev_info->wspecversion = desc_buf[DEVICE_DESC_PARAM_SPEC_VER] << 8 |\n\t\t\t\t      desc_buf[DEVICE_DESC_PARAM_SPEC_VER + 1];\n\tdev_info->bqueuedepth = desc_buf[DEVICE_DESC_PARAM_Q_DPTH];\n\n\tmodel_index = desc_buf[DEVICE_DESC_PARAM_PRDCT_NAME];\n\n\terr = ufshcd_read_string_desc(hba, model_index,\n\t\t\t\t      &dev_info->model, SD_ASCII_STD);\n\tif (err < 0) {\n\t\tdev_err(hba->dev, \"%s: Failed reading Product Name. err = %d\\n\",\n\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\thba->luns_avail = desc_buf[DEVICE_DESC_PARAM_NUM_LU] +\n\t\tdesc_buf[DEVICE_DESC_PARAM_NUM_WLU];\n\n\tufs_fixup_device_setup(hba);\n\n\tufshcd_wb_probe(hba, desc_buf);\n\n\tufshcd_temp_notif_probe(hba, desc_buf);\n\n\tif (hba->ext_iid_sup)\n\t\tufshcd_ext_iid_probe(hba, desc_buf);\n\n\t \n\terr = 0;\n\nout:\n\tkfree(desc_buf);\n\treturn err;\n}\n\nstatic void ufs_put_device_desc(struct ufs_hba *hba)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\n\tkfree(dev_info->model);\n\tdev_info->model = NULL;\n}\n\n \nstatic int ufshcd_tune_pa_tactivate(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tu32 peer_rx_min_activatetime = 0, tuned_pa_tactivate;\n\n\tret = ufshcd_dme_peer_get(hba,\n\t\t\t\t  UIC_ARG_MIB_SEL(\n\t\t\t\t\tRX_MIN_ACTIVATETIME_CAPABILITY,\n\t\t\t\t\tUIC_ARG_MPHY_RX_GEN_SEL_INDEX(0)),\n\t\t\t\t  &peer_rx_min_activatetime);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\ttuned_pa_tactivate =\n\t\t((peer_rx_min_activatetime * RX_MIN_ACTIVATETIME_UNIT_US)\n\t\t / PA_TACTIVATE_TIME_UNIT_US);\n\tret = ufshcd_dme_set(hba, UIC_ARG_MIB(PA_TACTIVATE),\n\t\t\t     tuned_pa_tactivate);\n\nout:\n\treturn ret;\n}\n\n \nstatic int ufshcd_tune_pa_hibern8time(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tu32 local_tx_hibern8_time_cap = 0, peer_rx_hibern8_time_cap = 0;\n\tu32 max_hibern8_time, tuned_pa_hibern8time;\n\n\tret = ufshcd_dme_get(hba,\n\t\t\t     UIC_ARG_MIB_SEL(TX_HIBERN8TIME_CAPABILITY,\n\t\t\t\t\tUIC_ARG_MPHY_TX_GEN_SEL_INDEX(0)),\n\t\t\t\t  &local_tx_hibern8_time_cap);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_peer_get(hba,\n\t\t\t\t  UIC_ARG_MIB_SEL(RX_HIBERN8TIME_CAPABILITY,\n\t\t\t\t\tUIC_ARG_MPHY_RX_GEN_SEL_INDEX(0)),\n\t\t\t\t  &peer_rx_hibern8_time_cap);\n\tif (ret)\n\t\tgoto out;\n\n\tmax_hibern8_time = max(local_tx_hibern8_time_cap,\n\t\t\t       peer_rx_hibern8_time_cap);\n\t \n\ttuned_pa_hibern8time = ((max_hibern8_time * HIBERN8TIME_UNIT_US)\n\t\t\t\t/ PA_HIBERN8_TIME_UNIT_US);\n\tret = ufshcd_dme_set(hba, UIC_ARG_MIB(PA_HIBERN8TIME),\n\t\t\t     tuned_pa_hibern8time);\nout:\n\treturn ret;\n}\n\n \nstatic int ufshcd_quirk_tune_host_pa_tactivate(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tu32 granularity, peer_granularity;\n\tu32 pa_tactivate, peer_pa_tactivate;\n\tu32 pa_tactivate_us, peer_pa_tactivate_us;\n\tstatic const u8 gran_to_us_table[] = {1, 4, 8, 16, 32, 100};\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_GRANULARITY),\n\t\t\t\t  &granularity);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_GRANULARITY),\n\t\t\t\t  &peer_granularity);\n\tif (ret)\n\t\tgoto out;\n\n\tif ((granularity < PA_GRANULARITY_MIN_VAL) ||\n\t    (granularity > PA_GRANULARITY_MAX_VAL)) {\n\t\tdev_err(hba->dev, \"%s: invalid host PA_GRANULARITY %d\",\n\t\t\t__func__, granularity);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((peer_granularity < PA_GRANULARITY_MIN_VAL) ||\n\t    (peer_granularity > PA_GRANULARITY_MAX_VAL)) {\n\t\tdev_err(hba->dev, \"%s: invalid device PA_GRANULARITY %d\",\n\t\t\t__func__, peer_granularity);\n\t\treturn -EINVAL;\n\t}\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_TACTIVATE), &pa_tactivate);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_TACTIVATE),\n\t\t\t\t  &peer_pa_tactivate);\n\tif (ret)\n\t\tgoto out;\n\n\tpa_tactivate_us = pa_tactivate * gran_to_us_table[granularity - 1];\n\tpeer_pa_tactivate_us = peer_pa_tactivate *\n\t\t\t     gran_to_us_table[peer_granularity - 1];\n\n\tif (pa_tactivate_us >= peer_pa_tactivate_us) {\n\t\tu32 new_peer_pa_tactivate;\n\n\t\tnew_peer_pa_tactivate = pa_tactivate_us /\n\t\t\t\t      gran_to_us_table[peer_granularity - 1];\n\t\tnew_peer_pa_tactivate++;\n\t\tret = ufshcd_dme_peer_set(hba, UIC_ARG_MIB(PA_TACTIVATE),\n\t\t\t\t\t  new_peer_pa_tactivate);\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void ufshcd_tune_unipro_params(struct ufs_hba *hba)\n{\n\tif (ufshcd_is_unipro_pa_params_tuning_req(hba)) {\n\t\tufshcd_tune_pa_tactivate(hba);\n\t\tufshcd_tune_pa_hibern8time(hba);\n\t}\n\n\tufshcd_vops_apply_dev_quirks(hba);\n\n\tif (hba->dev_quirks & UFS_DEVICE_QUIRK_PA_TACTIVATE)\n\t\t \n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TACTIVATE), 10);\n\n\tif (hba->dev_quirks & UFS_DEVICE_QUIRK_HOST_PA_TACTIVATE)\n\t\tufshcd_quirk_tune_host_pa_tactivate(hba);\n}\n\nstatic void ufshcd_clear_dbg_ufs_stats(struct ufs_hba *hba)\n{\n\thba->ufs_stats.hibern8_exit_cnt = 0;\n\thba->ufs_stats.last_hibern8_exit_tstamp = ktime_set(0, 0);\n\thba->req_abort_count = 0;\n}\n\nstatic int ufshcd_device_geo_params_init(struct ufs_hba *hba)\n{\n\tint err;\n\tu8 *desc_buf;\n\n\tdesc_buf = kzalloc(QUERY_DESC_MAX_SIZE, GFP_KERNEL);\n\tif (!desc_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = ufshcd_read_desc_param(hba, QUERY_DESC_IDN_GEOMETRY, 0, 0,\n\t\t\t\t     desc_buf, QUERY_DESC_MAX_SIZE);\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: Failed reading Geometry Desc. err = %d\\n\",\n\t\t\t\t__func__, err);\n\t\tgoto out;\n\t}\n\n\tif (desc_buf[GEOMETRY_DESC_PARAM_MAX_NUM_LUN] == 1)\n\t\thba->dev_info.max_lu_supported = 32;\n\telse if (desc_buf[GEOMETRY_DESC_PARAM_MAX_NUM_LUN] == 0)\n\t\thba->dev_info.max_lu_supported = 8;\n\nout:\n\tkfree(desc_buf);\n\treturn err;\n}\n\nstruct ufs_ref_clk {\n\tunsigned long freq_hz;\n\tenum ufs_ref_clk_freq val;\n};\n\nstatic const struct ufs_ref_clk ufs_ref_clk_freqs[] = {\n\t{19200000, REF_CLK_FREQ_19_2_MHZ},\n\t{26000000, REF_CLK_FREQ_26_MHZ},\n\t{38400000, REF_CLK_FREQ_38_4_MHZ},\n\t{52000000, REF_CLK_FREQ_52_MHZ},\n\t{0, REF_CLK_FREQ_INVAL},\n};\n\nstatic enum ufs_ref_clk_freq\nufs_get_bref_clk_from_hz(unsigned long freq)\n{\n\tint i;\n\n\tfor (i = 0; ufs_ref_clk_freqs[i].freq_hz; i++)\n\t\tif (ufs_ref_clk_freqs[i].freq_hz == freq)\n\t\t\treturn ufs_ref_clk_freqs[i].val;\n\n\treturn REF_CLK_FREQ_INVAL;\n}\n\nvoid ufshcd_parse_dev_ref_clk_freq(struct ufs_hba *hba, struct clk *refclk)\n{\n\tunsigned long freq;\n\n\tfreq = clk_get_rate(refclk);\n\n\thba->dev_ref_clk_freq =\n\t\tufs_get_bref_clk_from_hz(freq);\n\n\tif (hba->dev_ref_clk_freq == REF_CLK_FREQ_INVAL)\n\t\tdev_err(hba->dev,\n\t\t\"invalid ref_clk setting = %ld\\n\", freq);\n}\n\nstatic int ufshcd_set_dev_ref_clk(struct ufs_hba *hba)\n{\n\tint err;\n\tu32 ref_clk;\n\tu32 freq = hba->dev_ref_clk_freq;\n\n\terr = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_READ_ATTR,\n\t\t\tQUERY_ATTR_IDN_REF_CLK_FREQ, 0, 0, &ref_clk);\n\n\tif (err) {\n\t\tdev_err(hba->dev, \"failed reading bRefClkFreq. err = %d\\n\",\n\t\t\terr);\n\t\tgoto out;\n\t}\n\n\tif (ref_clk == freq)\n\t\tgoto out;  \n\n\terr = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\n\t\t\tQUERY_ATTR_IDN_REF_CLK_FREQ, 0, 0, &freq);\n\n\tif (err) {\n\t\tdev_err(hba->dev, \"bRefClkFreq setting to %lu Hz failed\\n\",\n\t\t\tufs_ref_clk_freqs[freq].freq_hz);\n\t\tgoto out;\n\t}\n\n\tdev_dbg(hba->dev, \"bRefClkFreq setting to %lu Hz succeeded\\n\",\n\t\t\tufs_ref_clk_freqs[freq].freq_hz);\n\nout:\n\treturn err;\n}\n\nstatic int ufshcd_device_params_init(struct ufs_hba *hba)\n{\n\tbool flag;\n\tint ret;\n\n\t \n\tret = ufshcd_device_geo_params_init(hba);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = ufs_get_device_desc(hba);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: Failed getting device info. err = %d\\n\",\n\t\t\t__func__, ret);\n\t\tgoto out;\n\t}\n\n\tufshcd_get_ref_clk_gating_wait(hba);\n\n\tif (!ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_READ_FLAG,\n\t\t\tQUERY_FLAG_IDN_PWR_ON_WPE, 0, &flag))\n\t\thba->dev_info.f_power_on_wp_en = flag;\n\n\t \n\tif (ufshcd_get_max_pwr_mode(hba))\n\t\tdev_err(hba->dev,\n\t\t\t\"%s: Failed getting max supported power mode\\n\",\n\t\t\t__func__);\nout:\n\treturn ret;\n}\n\nstatic void ufshcd_set_timestamp_attr(struct ufs_hba *hba)\n{\n\tint err;\n\tstruct ufs_query_req *request = NULL;\n\tstruct ufs_query_res *response = NULL;\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tstruct utp_upiu_query_v4_0 *upiu_data;\n\n\tif (dev_info->wspecversion < 0x400)\n\t\treturn;\n\n\tufshcd_hold(hba);\n\n\tmutex_lock(&hba->dev_cmd.lock);\n\n\tufshcd_init_query(hba, &request, &response,\n\t\t\t  UPIU_QUERY_OPCODE_WRITE_ATTR,\n\t\t\t  QUERY_ATTR_IDN_TIMESTAMP, 0, 0);\n\n\trequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\n\n\tupiu_data = (struct utp_upiu_query_v4_0 *)&request->upiu_req;\n\n\tput_unaligned_be64(ktime_get_real_ns(), &upiu_data->osf3);\n\n\terr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\n\n\tif (err)\n\t\tdev_err(hba->dev, \"%s: failed to set timestamp %d\\n\",\n\t\t\t__func__, err);\n\n\tmutex_unlock(&hba->dev_cmd.lock);\n\tufshcd_release(hba);\n}\n\n \nstatic int ufshcd_add_lus(struct ufs_hba *hba)\n{\n\tint ret;\n\n\t \n\tret = ufshcd_scsi_add_wlus(hba);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (ufshcd_is_clkscaling_supported(hba)) {\n\t\tmemcpy(&hba->clk_scaling.saved_pwr_info,\n\t\t\t&hba->pwr_info,\n\t\t\tsizeof(struct ufs_pa_layer_attr));\n\t\thba->clk_scaling.is_allowed = true;\n\n\t\tret = ufshcd_devfreq_init(hba);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\thba->clk_scaling.is_enabled = true;\n\t\tufshcd_init_clk_scaling_sysfs(hba);\n\t}\n\n\tufs_bsg_probe(hba);\n\tscsi_scan_host(hba->host);\n\nout:\n\treturn ret;\n}\n\n \nstatic void ufshcd_release_sdb_queue(struct ufs_hba *hba, int nutrs)\n{\n\tsize_t ucdl_size, utrdl_size;\n\n\tucdl_size = ufshcd_get_ucd_size(hba) * nutrs;\n\tdmam_free_coherent(hba->dev, ucdl_size, hba->ucdl_base_addr,\n\t\t\t   hba->ucdl_dma_addr);\n\n\tutrdl_size = sizeof(struct utp_transfer_req_desc) * nutrs;\n\tdmam_free_coherent(hba->dev, utrdl_size, hba->utrdl_base_addr,\n\t\t\t   hba->utrdl_dma_addr);\n\n\tdevm_kfree(hba->dev, hba->lrb);\n}\n\nstatic int ufshcd_alloc_mcq(struct ufs_hba *hba)\n{\n\tint ret;\n\tint old_nutrs = hba->nutrs;\n\n\tret = ufshcd_mcq_decide_queue_depth(hba);\n\tif (ret < 0)\n\t\treturn ret;\n\n\thba->nutrs = ret;\n\tret = ufshcd_mcq_init(hba);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tif (hba->nutrs != old_nutrs) {\n\t\tufshcd_release_sdb_queue(hba, old_nutrs);\n\t\tret = ufshcd_memory_alloc(hba);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tufshcd_host_memory_configure(hba);\n\t}\n\n\tret = ufshcd_mcq_memory_alloc(hba);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\thba->nutrs = old_nutrs;\n\treturn ret;\n}\n\nstatic void ufshcd_config_mcq(struct ufs_hba *hba)\n{\n\tint ret;\n\tu32 intrs;\n\n\tret = ufshcd_mcq_vops_config_esi(hba);\n\tdev_info(hba->dev, \"ESI %sconfigured\\n\", ret ? \"is not \" : \"\");\n\n\tintrs = UFSHCD_ENABLE_MCQ_INTRS;\n\tif (hba->quirks & UFSHCD_QUIRK_MCQ_BROKEN_INTR)\n\t\tintrs &= ~MCQ_CQ_EVENT_STATUS;\n\tufshcd_enable_intr(hba, intrs);\n\tufshcd_mcq_make_queues_operational(hba);\n\tufshcd_mcq_config_mac(hba, hba->nutrs);\n\n\thba->host->can_queue = hba->nutrs - UFSHCD_NUM_RESERVED;\n\thba->reserved_slot = hba->nutrs - UFSHCD_NUM_RESERVED;\n\n\t \n\tufshcd_writel(hba, ufshcd_readl(hba, REG_UFS_MEM_CFG) | 0x1,\n\t\t      REG_UFS_MEM_CFG);\n\thba->mcq_enabled = true;\n\n\tdev_info(hba->dev, \"MCQ configured, nr_queues=%d, io_queues=%d, read_queue=%d, poll_queues=%d, queue_depth=%d\\n\",\n\t\t hba->nr_hw_queues, hba->nr_queues[HCTX_TYPE_DEFAULT],\n\t\t hba->nr_queues[HCTX_TYPE_READ], hba->nr_queues[HCTX_TYPE_POLL],\n\t\t hba->nutrs);\n}\n\nstatic int ufshcd_device_init(struct ufs_hba *hba, bool init_dev_params)\n{\n\tint ret;\n\tstruct Scsi_Host *host = hba->host;\n\n\thba->ufshcd_state = UFSHCD_STATE_RESET;\n\n\tret = ufshcd_link_startup(hba);\n\tif (ret)\n\t\treturn ret;\n\n\tif (hba->quirks & UFSHCD_QUIRK_SKIP_PH_CONFIGURATION)\n\t\treturn ret;\n\n\t \n\tufshcd_clear_dbg_ufs_stats(hba);\n\n\t \n\tufshcd_set_link_active(hba);\n\n\t \n\tif (is_mcq_enabled(hba) && !init_dev_params)\n\t\tufshcd_config_mcq(hba);\n\n\t \n\tret = ufshcd_verify_dev_init(hba);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = ufshcd_complete_dev_init(hba);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (init_dev_params) {\n\t\tret = ufshcd_device_params_init(hba);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (is_mcq_supported(hba) && !hba->scsi_host_added) {\n\t\t\tret = ufshcd_alloc_mcq(hba);\n\t\t\tif (!ret) {\n\t\t\t\tufshcd_config_mcq(hba);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tuse_mcq_mode = false;\n\t\t\t\tdev_err(hba->dev, \"MCQ mode is disabled, err=%d\\n\",\n\t\t\t\t\t ret);\n\t\t\t}\n\t\t\tret = scsi_add_host(host, hba->dev);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(hba->dev, \"scsi_add_host failed\\n\");\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t\thba->scsi_host_added = true;\n\t\t} else if (is_mcq_supported(hba)) {\n\t\t\t \n\t\t\tufshcd_config_mcq(hba);\n\t\t}\n\t}\n\n\tufshcd_tune_unipro_params(hba);\n\n\t \n\tufshcd_set_ufs_dev_active(hba);\n\tufshcd_force_reset_auto_bkops(hba);\n\n\tufshcd_set_timestamp_attr(hba);\n\n\t \n\tif (hba->max_pwr_info.is_valid) {\n\t\t \n\t\tif (hba->dev_ref_clk_freq != REF_CLK_FREQ_INVAL)\n\t\t\tufshcd_set_dev_ref_clk(hba);\n\t\tret = ufshcd_config_pwr_mode(hba, &hba->max_pwr_info.info);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: Failed setting power mode, err = %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ufshcd_probe_hba(struct ufs_hba *hba, bool init_dev_params)\n{\n\tktime_t start = ktime_get();\n\tunsigned long flags;\n\tint ret;\n\n\tret = ufshcd_device_init(hba, init_dev_params);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!hba->pm_op_in_progress &&\n\t    (hba->quirks & UFSHCD_QUIRK_REINIT_AFTER_MAX_GEAR_SWITCH)) {\n\t\t \n\t\tufshcd_device_reset(hba);\n\t\tufshcd_hba_stop(hba);\n\t\tufshcd_vops_reinit_notify(hba);\n\t\tret = ufshcd_hba_enable(hba);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"Host controller enable failed\\n\");\n\t\t\tufshcd_print_evt_hist(hba);\n\t\t\tufshcd_print_host_state(hba);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tret = ufshcd_device_init(hba, init_dev_params);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tufshcd_print_pwr_info(hba);\n\n\t \n\tufshcd_set_active_icc_lvl(hba);\n\n\t \n\tufshcd_configure_wb(hba);\n\n\tif (hba->ee_usr_mask)\n\t\tufshcd_write_ee_control(hba);\n\t \n\tufshcd_auto_hibern8_enable(hba);\n\nout:\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tif (ret)\n\t\thba->ufshcd_state = UFSHCD_STATE_ERROR;\n\telse if (hba->ufshcd_state == UFSHCD_STATE_RESET)\n\t\thba->ufshcd_state = UFSHCD_STATE_OPERATIONAL;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\ttrace_ufshcd_init(dev_name(hba->dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\treturn ret;\n}\n\n \nstatic void ufshcd_async_scan(void *data, async_cookie_t cookie)\n{\n\tstruct ufs_hba *hba = (struct ufs_hba *)data;\n\tint ret;\n\n\tdown(&hba->host_sem);\n\t \n\tret = ufshcd_probe_hba(hba, true);\n\tup(&hba->host_sem);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = ufshcd_add_lus(hba);\n\nout:\n\tpm_runtime_put_sync(hba->dev);\n\t \n\tif (ret)\n\t\tufshcd_hba_exit(hba);\n}\n\nstatic enum scsi_timeout_action ufshcd_eh_timed_out(struct scsi_cmnd *scmd)\n{\n\tstruct ufs_hba *hba = shost_priv(scmd->device->host);\n\n\tif (!hba->system_suspending) {\n\t\t \n\t\treturn SCSI_EH_NOT_HANDLED;\n\t}\n\n\t \n\tufshcd_link_recovery(hba);\n\tdev_info(hba->dev, \"%s() finished; outstanding_tasks = %#lx.\\n\",\n\t\t __func__, hba->outstanding_tasks);\n\n\treturn hba->outstanding_reqs ? SCSI_EH_RESET_TIMER : SCSI_EH_DONE;\n}\n\nstatic const struct attribute_group *ufshcd_driver_groups[] = {\n\t&ufs_sysfs_unit_descriptor_group,\n\t&ufs_sysfs_lun_attributes_group,\n\tNULL,\n};\n\nstatic struct ufs_hba_variant_params ufs_hba_vps = {\n\t.hba_enable_delay_us\t\t= 1000,\n\t.wb_flush_threshold\t\t= UFS_WB_BUF_REMAIN_PERCENT(40),\n\t.devfreq_profile.polling_ms\t= 100,\n\t.devfreq_profile.target\t\t= ufshcd_devfreq_target,\n\t.devfreq_profile.get_dev_status\t= ufshcd_devfreq_get_dev_status,\n\t.ondemand_data.upthreshold\t= 70,\n\t.ondemand_data.downdifferential\t= 5,\n};\n\nstatic const struct scsi_host_template ufshcd_driver_template = {\n\t.module\t\t\t= THIS_MODULE,\n\t.name\t\t\t= UFSHCD,\n\t.proc_name\t\t= UFSHCD,\n\t.map_queues\t\t= ufshcd_map_queues,\n\t.queuecommand\t\t= ufshcd_queuecommand,\n\t.mq_poll\t\t= ufshcd_poll,\n\t.slave_alloc\t\t= ufshcd_slave_alloc,\n\t.slave_configure\t= ufshcd_slave_configure,\n\t.slave_destroy\t\t= ufshcd_slave_destroy,\n\t.change_queue_depth\t= ufshcd_change_queue_depth,\n\t.eh_abort_handler\t= ufshcd_abort,\n\t.eh_device_reset_handler = ufshcd_eh_device_reset_handler,\n\t.eh_host_reset_handler   = ufshcd_eh_host_reset_handler,\n\t.eh_timed_out\t\t= ufshcd_eh_timed_out,\n\t.this_id\t\t= -1,\n\t.sg_tablesize\t\t= SG_ALL,\n\t.cmd_per_lun\t\t= UFSHCD_CMD_PER_LUN,\n\t.can_queue\t\t= UFSHCD_CAN_QUEUE,\n\t.max_segment_size\t= PRDT_DATA_BYTE_COUNT_MAX,\n\t.max_sectors\t\t= SZ_1M / SECTOR_SIZE,\n\t.max_host_blocked\t= 1,\n\t.track_queue_depth\t= 1,\n\t.skip_settle_delay\t= 1,\n\t.sdev_groups\t\t= ufshcd_driver_groups,\n\t.rpm_autosuspend_delay\t= RPM_AUTOSUSPEND_DELAY_MS,\n};\n\nstatic int ufshcd_config_vreg_load(struct device *dev, struct ufs_vreg *vreg,\n\t\t\t\t   int ua)\n{\n\tint ret;\n\n\tif (!vreg)\n\t\treturn 0;\n\n\t \n\tif (!vreg->max_uA)\n\t\treturn 0;\n\n\tret = regulator_set_load(vreg->reg, ua);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"%s: %s set load (ua=%d) failed, err=%d\\n\",\n\t\t\t\t__func__, vreg->name, ua, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int ufshcd_config_vreg_lpm(struct ufs_hba *hba,\n\t\t\t\t\t struct ufs_vreg *vreg)\n{\n\treturn ufshcd_config_vreg_load(hba->dev, vreg, UFS_VREG_LPM_LOAD_UA);\n}\n\nstatic inline int ufshcd_config_vreg_hpm(struct ufs_hba *hba,\n\t\t\t\t\t struct ufs_vreg *vreg)\n{\n\tif (!vreg)\n\t\treturn 0;\n\n\treturn ufshcd_config_vreg_load(hba->dev, vreg, vreg->max_uA);\n}\n\nstatic int ufshcd_config_vreg(struct device *dev,\n\t\tstruct ufs_vreg *vreg, bool on)\n{\n\tif (regulator_count_voltages(vreg->reg) <= 0)\n\t\treturn 0;\n\n\treturn ufshcd_config_vreg_load(dev, vreg, on ? vreg->max_uA : 0);\n}\n\nstatic int ufshcd_enable_vreg(struct device *dev, struct ufs_vreg *vreg)\n{\n\tint ret = 0;\n\n\tif (!vreg || vreg->enabled)\n\t\tgoto out;\n\n\tret = ufshcd_config_vreg(dev, vreg, true);\n\tif (!ret)\n\t\tret = regulator_enable(vreg->reg);\n\n\tif (!ret)\n\t\tvreg->enabled = true;\n\telse\n\t\tdev_err(dev, \"%s: %s enable failed, err=%d\\n\",\n\t\t\t\t__func__, vreg->name, ret);\nout:\n\treturn ret;\n}\n\nstatic int ufshcd_disable_vreg(struct device *dev, struct ufs_vreg *vreg)\n{\n\tint ret = 0;\n\n\tif (!vreg || !vreg->enabled || vreg->always_on)\n\t\tgoto out;\n\n\tret = regulator_disable(vreg->reg);\n\n\tif (!ret) {\n\t\t \n\t\tufshcd_config_vreg(dev, vreg, false);\n\t\tvreg->enabled = false;\n\t} else {\n\t\tdev_err(dev, \"%s: %s disable failed, err=%d\\n\",\n\t\t\t\t__func__, vreg->name, ret);\n\t}\nout:\n\treturn ret;\n}\n\nstatic int ufshcd_setup_vreg(struct ufs_hba *hba, bool on)\n{\n\tint ret = 0;\n\tstruct device *dev = hba->dev;\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\n\tret = ufshcd_toggle_vreg(dev, info->vcc, on);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_toggle_vreg(dev, info->vccq, on);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_toggle_vreg(dev, info->vccq2, on);\n\nout:\n\tif (ret) {\n\t\tufshcd_toggle_vreg(dev, info->vccq2, false);\n\t\tufshcd_toggle_vreg(dev, info->vccq, false);\n\t\tufshcd_toggle_vreg(dev, info->vcc, false);\n\t}\n\treturn ret;\n}\n\nstatic int ufshcd_setup_hba_vreg(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\n\treturn ufshcd_toggle_vreg(hba->dev, info->vdd_hba, on);\n}\n\nint ufshcd_get_vreg(struct device *dev, struct ufs_vreg *vreg)\n{\n\tint ret = 0;\n\n\tif (!vreg)\n\t\tgoto out;\n\n\tvreg->reg = devm_regulator_get(dev, vreg->name);\n\tif (IS_ERR(vreg->reg)) {\n\t\tret = PTR_ERR(vreg->reg);\n\t\tdev_err(dev, \"%s: %s get failed, err=%d\\n\",\n\t\t\t\t__func__, vreg->name, ret);\n\t}\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ufshcd_get_vreg);\n\nstatic int ufshcd_init_vreg(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tstruct device *dev = hba->dev;\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\n\tret = ufshcd_get_vreg(dev, info->vcc);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_get_vreg(dev, info->vccq);\n\tif (!ret)\n\t\tret = ufshcd_get_vreg(dev, info->vccq2);\nout:\n\treturn ret;\n}\n\nstatic int ufshcd_init_hba_vreg(struct ufs_hba *hba)\n{\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\n\treturn ufshcd_get_vreg(hba->dev, info->vdd_hba);\n}\n\nstatic int ufshcd_setup_clocks(struct ufs_hba *hba, bool on)\n{\n\tint ret = 0;\n\tstruct ufs_clk_info *clki;\n\tstruct list_head *head = &hba->clk_list_head;\n\tunsigned long flags;\n\tktime_t start = ktime_get();\n\tbool clk_state_changed = false;\n\n\tif (list_empty(head))\n\t\tgoto out;\n\n\tret = ufshcd_vops_setup_clocks(hba, on, PRE_CHANGE);\n\tif (ret)\n\t\treturn ret;\n\n\tlist_for_each_entry(clki, head, list) {\n\t\tif (!IS_ERR_OR_NULL(clki->clk)) {\n\t\t\t \n\t\t\tif (ufshcd_is_link_active(hba) &&\n\t\t\t    clki->keep_link_active)\n\t\t\t\tcontinue;\n\n\t\t\tclk_state_changed = on ^ clki->enabled;\n\t\t\tif (on && !clki->enabled) {\n\t\t\t\tret = clk_prepare_enable(clki->clk);\n\t\t\t\tif (ret) {\n\t\t\t\t\tdev_err(hba->dev, \"%s: %s prepare enable failed, %d\\n\",\n\t\t\t\t\t\t__func__, clki->name, ret);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else if (!on && clki->enabled) {\n\t\t\t\tclk_disable_unprepare(clki->clk);\n\t\t\t}\n\t\t\tclki->enabled = on;\n\t\t\tdev_dbg(hba->dev, \"%s: clk: %s %sabled\\n\", __func__,\n\t\t\t\t\tclki->name, on ? \"en\" : \"dis\");\n\t\t}\n\t}\n\n\tret = ufshcd_vops_setup_clocks(hba, on, POST_CHANGE);\n\tif (ret)\n\t\treturn ret;\n\nout:\n\tif (ret) {\n\t\tlist_for_each_entry(clki, head, list) {\n\t\t\tif (!IS_ERR_OR_NULL(clki->clk) && clki->enabled)\n\t\t\t\tclk_disable_unprepare(clki->clk);\n\t\t}\n\t} else if (!ret && on) {\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\thba->clk_gating.state = CLKS_ON;\n\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\thba->clk_gating.state);\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t}\n\n\tif (clk_state_changed)\n\t\ttrace_ufshcd_profile_clk_gating(dev_name(hba->dev),\n\t\t\t(on ? \"on\" : \"off\"),\n\t\t\tktime_to_us(ktime_sub(ktime_get(), start)), ret);\n\treturn ret;\n}\n\nstatic enum ufs_ref_clk_freq ufshcd_parse_ref_clk_property(struct ufs_hba *hba)\n{\n\tu32 freq;\n\tint ret = device_property_read_u32(hba->dev, \"ref-clk-freq\", &freq);\n\n\tif (ret) {\n\t\tdev_dbg(hba->dev, \"Cannot query 'ref-clk-freq' property = %d\", ret);\n\t\treturn REF_CLK_FREQ_INVAL;\n\t}\n\n\treturn ufs_get_bref_clk_from_hz(freq);\n}\n\nstatic int ufshcd_init_clocks(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tstruct ufs_clk_info *clki;\n\tstruct device *dev = hba->dev;\n\tstruct list_head *head = &hba->clk_list_head;\n\n\tif (list_empty(head))\n\t\tgoto out;\n\n\tlist_for_each_entry(clki, head, list) {\n\t\tif (!clki->name)\n\t\t\tcontinue;\n\n\t\tclki->clk = devm_clk_get(dev, clki->name);\n\t\tif (IS_ERR(clki->clk)) {\n\t\t\tret = PTR_ERR(clki->clk);\n\t\t\tdev_err(dev, \"%s: %s clk get failed, %d\\n\",\n\t\t\t\t\t__func__, clki->name, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (!strcmp(clki->name, \"ref_clk\"))\n\t\t\tufshcd_parse_dev_ref_clk_freq(hba, clki->clk);\n\n\t\tif (clki->max_freq) {\n\t\t\tret = clk_set_rate(clki->clk, clki->max_freq);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(hba->dev, \"%s: %s clk set rate(%dHz) failed, %d\\n\",\n\t\t\t\t\t__func__, clki->name,\n\t\t\t\t\tclki->max_freq, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tclki->curr_freq = clki->max_freq;\n\t\t}\n\t\tdev_dbg(dev, \"%s: clk: %s, rate: %lu\\n\", __func__,\n\t\t\t\tclki->name, clk_get_rate(clki->clk));\n\t}\nout:\n\treturn ret;\n}\n\nstatic int ufshcd_variant_hba_init(struct ufs_hba *hba)\n{\n\tint err = 0;\n\n\tif (!hba->vops)\n\t\tgoto out;\n\n\terr = ufshcd_vops_init(hba);\n\tif (err)\n\t\tdev_err_probe(hba->dev, err,\n\t\t\t      \"%s: variant %s init failed with err %d\\n\",\n\t\t\t      __func__, ufshcd_get_var_name(hba), err);\nout:\n\treturn err;\n}\n\nstatic void ufshcd_variant_hba_exit(struct ufs_hba *hba)\n{\n\tif (!hba->vops)\n\t\treturn;\n\n\tufshcd_vops_exit(hba);\n}\n\nstatic int ufshcd_hba_init(struct ufs_hba *hba)\n{\n\tint err;\n\n\t \n\terr = ufshcd_init_hba_vreg(hba);\n\tif (err)\n\t\tgoto out;\n\n\terr = ufshcd_setup_hba_vreg(hba, true);\n\tif (err)\n\t\tgoto out;\n\n\terr = ufshcd_init_clocks(hba);\n\tif (err)\n\t\tgoto out_disable_hba_vreg;\n\n\tif (hba->dev_ref_clk_freq == REF_CLK_FREQ_INVAL)\n\t\thba->dev_ref_clk_freq = ufshcd_parse_ref_clk_property(hba);\n\n\terr = ufshcd_setup_clocks(hba, true);\n\tif (err)\n\t\tgoto out_disable_hba_vreg;\n\n\terr = ufshcd_init_vreg(hba);\n\tif (err)\n\t\tgoto out_disable_clks;\n\n\terr = ufshcd_setup_vreg(hba, true);\n\tif (err)\n\t\tgoto out_disable_clks;\n\n\terr = ufshcd_variant_hba_init(hba);\n\tif (err)\n\t\tgoto out_disable_vreg;\n\n\tufs_debugfs_hba_init(hba);\n\n\thba->is_powered = true;\n\tgoto out;\n\nout_disable_vreg:\n\tufshcd_setup_vreg(hba, false);\nout_disable_clks:\n\tufshcd_setup_clocks(hba, false);\nout_disable_hba_vreg:\n\tufshcd_setup_hba_vreg(hba, false);\nout:\n\treturn err;\n}\n\nstatic void ufshcd_hba_exit(struct ufs_hba *hba)\n{\n\tif (hba->is_powered) {\n\t\tufshcd_exit_clk_scaling(hba);\n\t\tufshcd_exit_clk_gating(hba);\n\t\tif (hba->eh_wq)\n\t\t\tdestroy_workqueue(hba->eh_wq);\n\t\tufs_debugfs_hba_exit(hba);\n\t\tufshcd_variant_hba_exit(hba);\n\t\tufshcd_setup_vreg(hba, false);\n\t\tufshcd_setup_clocks(hba, false);\n\t\tufshcd_setup_hba_vreg(hba, false);\n\t\thba->is_powered = false;\n\t\tufs_put_device_desc(hba);\n\t}\n}\n\nstatic int ufshcd_execute_start_stop(struct scsi_device *sdev,\n\t\t\t\t     enum ufs_dev_pwr_mode pwr_mode,\n\t\t\t\t     struct scsi_sense_hdr *sshdr)\n{\n\tconst unsigned char cdb[6] = { START_STOP, 0, 0, 0, pwr_mode << 4, 0 };\n\tconst struct scsi_exec_args args = {\n\t\t.sshdr = sshdr,\n\t\t.req_flags = BLK_MQ_REQ_PM,\n\t\t.scmd_flags = SCMD_FAIL_IF_RECOVERING,\n\t};\n\n\treturn scsi_execute_cmd(sdev, cdb, REQ_OP_DRV_IN,  NULL,\n\t\t\t 0,  10 * HZ,  0,\n\t\t\t&args);\n}\n\n \nstatic int ufshcd_set_dev_pwr_mode(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_dev_pwr_mode pwr_mode)\n{\n\tstruct scsi_sense_hdr sshdr;\n\tstruct scsi_device *sdp;\n\tunsigned long flags;\n\tint ret, retries;\n\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tsdp = hba->ufs_device_wlun;\n\tif (sdp && scsi_device_online(sdp))\n\t\tret = scsi_device_get(sdp);\n\telse\n\t\tret = -ENODEV;\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\thba->host->eh_noresume = 1;\n\n\t \n\tfor (retries = 3; retries > 0; --retries) {\n\t\tret = ufshcd_execute_start_stop(sdp, pwr_mode, &sshdr);\n\t\t \n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t}\n\tif (ret) {\n\t\tsdev_printk(KERN_WARNING, sdp,\n\t\t\t    \"START_STOP failed for power mode: %d, result %x\\n\",\n\t\t\t    pwr_mode, ret);\n\t\tif (ret > 0) {\n\t\t\tif (scsi_sense_valid(&sshdr))\n\t\t\t\tscsi_print_sense_hdr(sdp, NULL, &sshdr);\n\t\t\tret = -EIO;\n\t\t}\n\t} else {\n\t\thba->curr_dev_pwr_mode = pwr_mode;\n\t}\n\n\tscsi_device_put(sdp);\n\thba->host->eh_noresume = 0;\n\treturn ret;\n}\n\nstatic int ufshcd_link_state_transition(struct ufs_hba *hba,\n\t\t\t\t\tenum uic_link_state req_link_state,\n\t\t\t\t\tbool check_for_bkops)\n{\n\tint ret = 0;\n\n\tif (req_link_state == hba->uic_link_state)\n\t\treturn 0;\n\n\tif (req_link_state == UIC_LINK_HIBERN8_STATE) {\n\t\tret = ufshcd_uic_hibern8_enter(hba);\n\t\tif (!ret) {\n\t\t\tufshcd_set_link_hibern8(hba);\n\t\t} else {\n\t\t\tdev_err(hba->dev, \"%s: hibern8 enter failed %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\t \n\telse if ((req_link_state == UIC_LINK_OFF_STATE) &&\n\t\t (!check_for_bkops || !hba->auto_bkops_enabled)) {\n\t\t \n\t\tret = ufshcd_uic_hibern8_enter(hba);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: hibern8 enter failed %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tufshcd_hba_stop(hba);\n\t\t \n\t\tufshcd_set_link_off(hba);\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void ufshcd_vreg_set_lpm(struct ufs_hba *hba)\n{\n\tbool vcc_off = false;\n\n\t \n\tif (!ufshcd_is_link_active(hba) &&\n\t    hba->dev_quirks & UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM)\n\t\tusleep_range(2000, 2100);\n\n\t \n\tif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba) &&\n\t    !hba->dev_info.is_lu_power_on_wp) {\n\t\tufshcd_setup_vreg(hba, false);\n\t\tvcc_off = true;\n\t} else if (!ufshcd_is_ufs_dev_active(hba)) {\n\t\tufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, false);\n\t\tvcc_off = true;\n\t\tif (ufshcd_is_link_hibern8(hba) || ufshcd_is_link_off(hba)) {\n\t\t\tufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq);\n\t\t\tufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq2);\n\t\t}\n\t}\n\n\t \n\tif (vcc_off && hba->vreg_info.vcc &&\n\t\thba->dev_quirks & UFS_DEVICE_QUIRK_DELAY_AFTER_LPM)\n\t\tusleep_range(5000, 5100);\n}\n\n#ifdef CONFIG_PM\nstatic int ufshcd_vreg_set_hpm(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\n\tif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba) &&\n\t    !hba->dev_info.is_lu_power_on_wp) {\n\t\tret = ufshcd_setup_vreg(hba, true);\n\t} else if (!ufshcd_is_ufs_dev_active(hba)) {\n\t\tif (!ufshcd_is_link_active(hba)) {\n\t\t\tret = ufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq);\n\t\t\tif (ret)\n\t\t\t\tgoto vcc_disable;\n\t\t\tret = ufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq2);\n\t\t\tif (ret)\n\t\t\t\tgoto vccq_lpm;\n\t\t}\n\t\tret = ufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, true);\n\t}\n\tgoto out;\n\nvccq_lpm:\n\tufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq);\nvcc_disable:\n\tufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, false);\nout:\n\treturn ret;\n}\n#endif  \n\nstatic void ufshcd_hba_vreg_set_lpm(struct ufs_hba *hba)\n{\n\tif (ufshcd_is_link_off(hba) || ufshcd_can_aggressive_pc(hba))\n\t\tufshcd_setup_hba_vreg(hba, false);\n}\n\nstatic void ufshcd_hba_vreg_set_hpm(struct ufs_hba *hba)\n{\n\tif (ufshcd_is_link_off(hba) || ufshcd_can_aggressive_pc(hba))\n\t\tufshcd_setup_hba_vreg(hba, true);\n}\n\nstatic int __ufshcd_wl_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tint ret = 0;\n\tbool check_for_bkops;\n\tenum ufs_pm_level pm_lvl;\n\tenum ufs_dev_pwr_mode req_dev_pwr_mode;\n\tenum uic_link_state req_link_state;\n\n\thba->pm_op_in_progress = true;\n\tif (pm_op != UFS_SHUTDOWN_PM) {\n\t\tpm_lvl = pm_op == UFS_RUNTIME_PM ?\n\t\t\t hba->rpm_lvl : hba->spm_lvl;\n\t\treq_dev_pwr_mode = ufs_get_pm_lvl_to_dev_pwr_mode(pm_lvl);\n\t\treq_link_state = ufs_get_pm_lvl_to_link_pwr_state(pm_lvl);\n\t} else {\n\t\treq_dev_pwr_mode = UFS_POWERDOWN_PWR_MODE;\n\t\treq_link_state = UIC_LINK_OFF_STATE;\n\t}\n\n\t \n\tufshcd_hold(hba);\n\thba->clk_gating.is_suspended = true;\n\n\tif (ufshcd_is_clkscaling_supported(hba))\n\t\tufshcd_clk_scaling_suspend(hba, true);\n\n\tif (req_dev_pwr_mode == UFS_ACTIVE_PWR_MODE &&\n\t\t\treq_link_state == UIC_LINK_ACTIVE_STATE) {\n\t\tgoto vops_suspend;\n\t}\n\n\tif ((req_dev_pwr_mode == hba->curr_dev_pwr_mode) &&\n\t    (req_link_state == hba->uic_link_state))\n\t\tgoto enable_scaling;\n\n\t \n\tif (!ufshcd_is_ufs_dev_active(hba) || !ufshcd_is_link_active(hba)) {\n\t\tret = -EINVAL;\n\t\tgoto enable_scaling;\n\t}\n\n\tif (pm_op == UFS_RUNTIME_PM) {\n\t\tif (ufshcd_can_autobkops_during_suspend(hba)) {\n\t\t\t \n\t\t\tret = ufshcd_urgent_bkops(hba);\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\tufshcd_force_error_recovery(hba);\n\t\t\t\tret = -EBUSY;\n\t\t\t\tgoto enable_scaling;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tufshcd_disable_auto_bkops(hba);\n\t\t}\n\t\t \n\t\thba->dev_info.b_rpm_dev_flush_capable =\n\t\t\thba->auto_bkops_enabled ||\n\t\t\t(((req_link_state == UIC_LINK_HIBERN8_STATE) ||\n\t\t\t((req_link_state == UIC_LINK_ACTIVE_STATE) &&\n\t\t\tufshcd_is_auto_hibern8_enabled(hba))) &&\n\t\t\tufshcd_wb_need_flush(hba));\n\t}\n\n\tflush_work(&hba->eeh_work);\n\n\tret = ufshcd_vops_suspend(hba, pm_op, PRE_CHANGE);\n\tif (ret)\n\t\tgoto enable_scaling;\n\n\tif (req_dev_pwr_mode != hba->curr_dev_pwr_mode) {\n\t\tif (pm_op != UFS_RUNTIME_PM)\n\t\t\t \n\t\t\tufshcd_disable_auto_bkops(hba);\n\n\t\tif (!hba->dev_info.b_rpm_dev_flush_capable) {\n\t\t\tret = ufshcd_set_dev_pwr_mode(hba, req_dev_pwr_mode);\n\t\t\tif (ret && pm_op != UFS_SHUTDOWN_PM) {\n\t\t\t\t \n\t\t\t\tufshcd_force_error_recovery(hba);\n\t\t\t\tret = -EBUSY;\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tgoto enable_scaling;\n\t\t}\n\t}\n\n\t \n\tcheck_for_bkops = !ufshcd_is_ufs_dev_deepsleep(hba);\n\tret = ufshcd_link_state_transition(hba, req_link_state, check_for_bkops);\n\tif (ret && pm_op != UFS_SHUTDOWN_PM) {\n\t\t \n\t\tufshcd_force_error_recovery(hba);\n\t\tret = -EBUSY;\n\t}\n\tif (ret)\n\t\tgoto set_dev_active;\n\nvops_suspend:\n\t \n\tret = ufshcd_vops_suspend(hba, pm_op, POST_CHANGE);\n\tif (ret)\n\t\tgoto set_link_active;\n\tgoto out;\n\nset_link_active:\n\t \n\tif (ufshcd_is_ufs_dev_deepsleep(hba)) {\n\t\tufshcd_device_reset(hba);\n\t\tWARN_ON(!ufshcd_is_link_off(hba));\n\t}\n\tif (ufshcd_is_link_hibern8(hba) && !ufshcd_uic_hibern8_exit(hba))\n\t\tufshcd_set_link_active(hba);\n\telse if (ufshcd_is_link_off(hba))\n\t\tufshcd_host_reset_and_restore(hba);\nset_dev_active:\n\t \n\tif (ufshcd_is_ufs_dev_deepsleep(hba)) {\n\t\tufshcd_device_reset(hba);\n\t\tufshcd_host_reset_and_restore(hba);\n\t}\n\tif (!ufshcd_set_dev_pwr_mode(hba, UFS_ACTIVE_PWR_MODE))\n\t\tufshcd_disable_auto_bkops(hba);\nenable_scaling:\n\tif (ufshcd_is_clkscaling_supported(hba))\n\t\tufshcd_clk_scaling_suspend(hba, false);\n\n\thba->dev_info.b_rpm_dev_flush_capable = false;\nout:\n\tif (hba->dev_info.b_rpm_dev_flush_capable) {\n\t\tschedule_delayed_work(&hba->rpm_dev_flush_recheck_work,\n\t\t\tmsecs_to_jiffies(RPM_DEV_FLUSH_RECHECK_WORK_DELAY_MS));\n\t}\n\n\tif (ret) {\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_WL_SUSP_ERR, (u32)ret);\n\t\thba->clk_gating.is_suspended = false;\n\t\tufshcd_release(hba);\n\t}\n\thba->pm_op_in_progress = false;\n\treturn ret;\n}\n\n#ifdef CONFIG_PM\nstatic int __ufshcd_wl_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tint ret;\n\tenum uic_link_state old_link_state = hba->uic_link_state;\n\n\thba->pm_op_in_progress = true;\n\n\t \n\tret = ufshcd_vops_resume(hba, pm_op);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tWARN_ON(ufshcd_is_ufs_dev_deepsleep(hba) && !ufshcd_is_link_off(hba));\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\tret = ufshcd_uic_hibern8_exit(hba);\n\t\tif (!ret) {\n\t\t\tufshcd_set_link_active(hba);\n\t\t} else {\n\t\t\tdev_err(hba->dev, \"%s: hibern8 exit failed %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\tgoto vendor_suspend;\n\t\t}\n\t} else if (ufshcd_is_link_off(hba)) {\n\t\t \n\t\tret = ufshcd_reset_and_restore(hba);\n\t\t \n\t\tif (ret || !ufshcd_is_link_active(hba))\n\t\t\tgoto vendor_suspend;\n\t}\n\n\tif (!ufshcd_is_ufs_dev_active(hba)) {\n\t\tret = ufshcd_set_dev_pwr_mode(hba, UFS_ACTIVE_PWR_MODE);\n\t\tif (ret)\n\t\t\tgoto set_old_link_state;\n\t\tufshcd_set_timestamp_attr(hba);\n\t}\n\n\tif (ufshcd_keep_autobkops_enabled_except_suspend(hba))\n\t\tufshcd_enable_auto_bkops(hba);\n\telse\n\t\t \n\t\tufshcd_urgent_bkops(hba);\n\n\tif (hba->ee_usr_mask)\n\t\tufshcd_write_ee_control(hba);\n\n\tif (ufshcd_is_clkscaling_supported(hba))\n\t\tufshcd_clk_scaling_suspend(hba, false);\n\n\tif (hba->dev_info.b_rpm_dev_flush_capable) {\n\t\thba->dev_info.b_rpm_dev_flush_capable = false;\n\t\tcancel_delayed_work(&hba->rpm_dev_flush_recheck_work);\n\t}\n\n\t \n\tufshcd_auto_hibern8_enable(hba);\n\n\tgoto out;\n\nset_old_link_state:\n\tufshcd_link_state_transition(hba, old_link_state, 0);\nvendor_suspend:\n\tufshcd_vops_suspend(hba, pm_op, PRE_CHANGE);\n\tufshcd_vops_suspend(hba, pm_op, POST_CHANGE);\nout:\n\tif (ret)\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_WL_RES_ERR, (u32)ret);\n\thba->clk_gating.is_suspended = false;\n\tufshcd_release(hba);\n\thba->pm_op_in_progress = false;\n\treturn ret;\n}\n\nstatic int ufshcd_wl_runtime_suspend(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba;\n\tint ret;\n\tktime_t start = ktime_get();\n\n\thba = shost_priv(sdev->host);\n\n\tret = __ufshcd_wl_suspend(hba, UFS_RUNTIME_PM);\n\tif (ret)\n\t\tdev_err(&sdev->sdev_gendev, \"%s failed: %d\\n\", __func__, ret);\n\n\ttrace_ufshcd_wl_runtime_suspend(dev_name(dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\n\treturn ret;\n}\n\nstatic int ufshcd_wl_runtime_resume(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba;\n\tint ret = 0;\n\tktime_t start = ktime_get();\n\n\thba = shost_priv(sdev->host);\n\n\tret = __ufshcd_wl_resume(hba, UFS_RUNTIME_PM);\n\tif (ret)\n\t\tdev_err(&sdev->sdev_gendev, \"%s failed: %d\\n\", __func__, ret);\n\n\ttrace_ufshcd_wl_runtime_resume(dev_name(dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_PM_SLEEP\nstatic int ufshcd_wl_suspend(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba;\n\tint ret = 0;\n\tktime_t start = ktime_get();\n\n\thba = shost_priv(sdev->host);\n\tdown(&hba->host_sem);\n\thba->system_suspending = true;\n\n\tif (pm_runtime_suspended(dev))\n\t\tgoto out;\n\n\tret = __ufshcd_wl_suspend(hba, UFS_SYSTEM_PM);\n\tif (ret) {\n\t\tdev_err(&sdev->sdev_gendev, \"%s failed: %d\\n\", __func__,  ret);\n\t\tup(&hba->host_sem);\n\t}\n\nout:\n\tif (!ret)\n\t\thba->is_sys_suspended = true;\n\ttrace_ufshcd_wl_suspend(dev_name(dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\n\treturn ret;\n}\n\nstatic int ufshcd_wl_resume(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba;\n\tint ret = 0;\n\tktime_t start = ktime_get();\n\n\thba = shost_priv(sdev->host);\n\n\tif (pm_runtime_suspended(dev))\n\t\tgoto out;\n\n\tret = __ufshcd_wl_resume(hba, UFS_SYSTEM_PM);\n\tif (ret)\n\t\tdev_err(&sdev->sdev_gendev, \"%s failed: %d\\n\", __func__, ret);\nout:\n\ttrace_ufshcd_wl_resume(dev_name(dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\tif (!ret)\n\t\thba->is_sys_suspended = false;\n\thba->system_suspending = false;\n\tup(&hba->host_sem);\n\treturn ret;\n}\n#endif\n\n \nstatic int ufshcd_suspend(struct ufs_hba *hba)\n{\n\tint ret;\n\n\tif (!hba->is_powered)\n\t\treturn 0;\n\t \n\tufshcd_disable_irq(hba);\n\tret = ufshcd_setup_clocks(hba, false);\n\tif (ret) {\n\t\tufshcd_enable_irq(hba);\n\t\treturn ret;\n\t}\n\tif (ufshcd_is_clkgating_allowed(hba)) {\n\t\thba->clk_gating.state = CLKS_OFF;\n\t\ttrace_ufshcd_clk_gating(dev_name(hba->dev),\n\t\t\t\t\thba->clk_gating.state);\n\t}\n\n\tufshcd_vreg_set_lpm(hba);\n\t \n\tufshcd_hba_vreg_set_lpm(hba);\n\treturn ret;\n}\n\n#ifdef CONFIG_PM\n \nstatic int ufshcd_resume(struct ufs_hba *hba)\n{\n\tint ret;\n\n\tif (!hba->is_powered)\n\t\treturn 0;\n\n\tufshcd_hba_vreg_set_hpm(hba);\n\tret = ufshcd_vreg_set_hpm(hba);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = ufshcd_setup_clocks(hba, true);\n\tif (ret)\n\t\tgoto disable_vreg;\n\n\t \n\tufshcd_enable_irq(hba);\n\n\tgoto out;\n\ndisable_vreg:\n\tufshcd_vreg_set_lpm(hba);\nout:\n\tif (ret)\n\t\tufshcd_update_evt_hist(hba, UFS_EVT_RESUME_ERR, (u32)ret);\n\treturn ret;\n}\n#endif  \n\n#ifdef CONFIG_PM_SLEEP\n \nint ufshcd_system_suspend(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret = 0;\n\tktime_t start = ktime_get();\n\n\tif (pm_runtime_suspended(hba->dev))\n\t\tgoto out;\n\n\tret = ufshcd_suspend(hba);\nout:\n\ttrace_ufshcd_system_suspend(dev_name(hba->dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\treturn ret;\n}\nEXPORT_SYMBOL(ufshcd_system_suspend);\n\n \nint ufshcd_system_resume(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tktime_t start = ktime_get();\n\tint ret = 0;\n\n\tif (pm_runtime_suspended(hba->dev))\n\t\tgoto out;\n\n\tret = ufshcd_resume(hba);\n\nout:\n\ttrace_ufshcd_system_resume(dev_name(hba->dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ufshcd_system_resume);\n#endif  \n\n#ifdef CONFIG_PM\n \nint ufshcd_runtime_suspend(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret;\n\tktime_t start = ktime_get();\n\n\tret = ufshcd_suspend(hba);\n\n\ttrace_ufshcd_runtime_suspend(dev_name(hba->dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\treturn ret;\n}\nEXPORT_SYMBOL(ufshcd_runtime_suspend);\n\n \nint ufshcd_runtime_resume(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret;\n\tktime_t start = ktime_get();\n\n\tret = ufshcd_resume(hba);\n\n\ttrace_ufshcd_runtime_resume(dev_name(hba->dev), ret,\n\t\tktime_to_us(ktime_sub(ktime_get(), start)),\n\t\thba->curr_dev_pwr_mode, hba->uic_link_state);\n\treturn ret;\n}\nEXPORT_SYMBOL(ufshcd_runtime_resume);\n#endif  \n\nstatic void ufshcd_wl_shutdown(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba = shost_priv(sdev->host);\n\n\tdown(&hba->host_sem);\n\thba->shutting_down = true;\n\tup(&hba->host_sem);\n\n\t \n\tufshcd_rpm_get_sync(hba);\n\tscsi_device_quiesce(sdev);\n\tshost_for_each_device(sdev, hba->host) {\n\t\tif (sdev == hba->ufs_device_wlun)\n\t\t\tcontinue;\n\t\tscsi_device_quiesce(sdev);\n\t}\n\t__ufshcd_wl_suspend(hba, UFS_SHUTDOWN_PM);\n\n\t \n\tif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba))\n\t\tufshcd_suspend(hba);\n\n\thba->is_powered = false;\n}\n\n \nvoid ufshcd_remove(struct ufs_hba *hba)\n{\n\tif (hba->ufs_device_wlun)\n\t\tufshcd_rpm_get_sync(hba);\n\tufs_hwmon_remove(hba);\n\tufs_bsg_remove(hba);\n\tufs_sysfs_remove_nodes(hba->dev);\n\tblk_mq_destroy_queue(hba->tmf_queue);\n\tblk_put_queue(hba->tmf_queue);\n\tblk_mq_free_tag_set(&hba->tmf_tag_set);\n\tscsi_remove_host(hba->host);\n\t \n\tufshcd_disable_intr(hba, hba->intr_mask);\n\tufshcd_hba_stop(hba);\n\tufshcd_hba_exit(hba);\n}\nEXPORT_SYMBOL_GPL(ufshcd_remove);\n\n#ifdef CONFIG_PM_SLEEP\nint ufshcd_system_freeze(struct device *dev)\n{\n\n\treturn ufshcd_system_suspend(dev);\n\n}\nEXPORT_SYMBOL_GPL(ufshcd_system_freeze);\n\nint ufshcd_system_restore(struct device *dev)\n{\n\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret;\n\n\tret = ufshcd_system_resume(dev);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tufshcd_writel(hba, lower_32_bits(hba->utrdl_dma_addr),\n\t\t\tREG_UTP_TRANSFER_REQ_LIST_BASE_L);\n\tufshcd_writel(hba, upper_32_bits(hba->utrdl_dma_addr),\n\t\t\tREG_UTP_TRANSFER_REQ_LIST_BASE_H);\n\tufshcd_writel(hba, lower_32_bits(hba->utmrdl_dma_addr),\n\t\t\tREG_UTP_TASK_REQ_LIST_BASE_L);\n\tufshcd_writel(hba, upper_32_bits(hba->utmrdl_dma_addr),\n\t\t\tREG_UTP_TASK_REQ_LIST_BASE_H);\n\t \n\tmb();\n\n\t \n\tufshcd_set_link_off(hba);\n\n\treturn 0;\n\n}\nEXPORT_SYMBOL_GPL(ufshcd_system_restore);\n\nint ufshcd_system_thaw(struct device *dev)\n{\n\treturn ufshcd_system_resume(dev);\n}\nEXPORT_SYMBOL_GPL(ufshcd_system_thaw);\n#endif  \n\n \nvoid ufshcd_dealloc_host(struct ufs_hba *hba)\n{\n\tscsi_host_put(hba->host);\n}\nEXPORT_SYMBOL_GPL(ufshcd_dealloc_host);\n\n \nstatic int ufshcd_set_dma_mask(struct ufs_hba *hba)\n{\n\tif (hba->capabilities & MASK_64_ADDRESSING_SUPPORT) {\n\t\tif (!dma_set_mask_and_coherent(hba->dev, DMA_BIT_MASK(64)))\n\t\t\treturn 0;\n\t}\n\treturn dma_set_mask_and_coherent(hba->dev, DMA_BIT_MASK(32));\n}\n\n \nint ufshcd_alloc_host(struct device *dev, struct ufs_hba **hba_handle)\n{\n\tstruct Scsi_Host *host;\n\tstruct ufs_hba *hba;\n\tint err = 0;\n\n\tif (!dev) {\n\t\tdev_err(dev,\n\t\t\"Invalid memory reference for dev is NULL\\n\");\n\t\terr = -ENODEV;\n\t\tgoto out_error;\n\t}\n\n\thost = scsi_host_alloc(&ufshcd_driver_template,\n\t\t\t\tsizeof(struct ufs_hba));\n\tif (!host) {\n\t\tdev_err(dev, \"scsi_host_alloc failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_error;\n\t}\n\thost->nr_maps = HCTX_TYPE_POLL + 1;\n\thba = shost_priv(host);\n\thba->host = host;\n\thba->dev = dev;\n\thba->dev_ref_clk_freq = REF_CLK_FREQ_INVAL;\n\thba->nop_out_timeout = NOP_OUT_TIMEOUT;\n\tufshcd_set_sg_entry_size(hba, sizeof(struct ufshcd_sg_entry));\n\tINIT_LIST_HEAD(&hba->clk_list_head);\n\tspin_lock_init(&hba->outstanding_lock);\n\n\t*hba_handle = hba;\n\nout_error:\n\treturn err;\n}\nEXPORT_SYMBOL(ufshcd_alloc_host);\n\n \nstatic blk_status_t ufshcd_queue_tmf(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t     const struct blk_mq_queue_data *qd)\n{\n\tWARN_ON_ONCE(true);\n\treturn BLK_STS_NOTSUPP;\n}\n\nstatic const struct blk_mq_ops ufshcd_tmf_ops = {\n\t.queue_rq = ufshcd_queue_tmf,\n};\n\n \nint ufshcd_init(struct ufs_hba *hba, void __iomem *mmio_base, unsigned int irq)\n{\n\tint err;\n\tstruct Scsi_Host *host = hba->host;\n\tstruct device *dev = hba->dev;\n\tchar eh_wq_name[sizeof(\"ufs_eh_wq_00\")];\n\n\t \n\tdev_set_drvdata(dev, hba);\n\n\tif (!mmio_base) {\n\t\tdev_err(hba->dev,\n\t\t\"Invalid memory reference for mmio_base is NULL\\n\");\n\t\terr = -ENODEV;\n\t\tgoto out_error;\n\t}\n\n\thba->mmio_base = mmio_base;\n\thba->irq = irq;\n\thba->vps = &ufs_hba_vps;\n\n\terr = ufshcd_hba_init(hba);\n\tif (err)\n\t\tgoto out_error;\n\n\t \n\terr = ufshcd_hba_capabilities(hba);\n\tif (err)\n\t\tgoto out_disable;\n\n\t \n\thba->ufs_version = ufshcd_get_ufs_version(hba);\n\n\t \n\thba->intr_mask = ufshcd_get_intr_mask(hba);\n\n\terr = ufshcd_set_dma_mask(hba);\n\tif (err) {\n\t\tdev_err(hba->dev, \"set dma mask failed\\n\");\n\t\tgoto out_disable;\n\t}\n\n\t \n\terr = ufshcd_memory_alloc(hba);\n\tif (err) {\n\t\tdev_err(hba->dev, \"Memory allocation failed\\n\");\n\t\tgoto out_disable;\n\t}\n\n\t \n\tufshcd_host_memory_configure(hba);\n\n\thost->can_queue = hba->nutrs - UFSHCD_NUM_RESERVED;\n\thost->cmd_per_lun = hba->nutrs - UFSHCD_NUM_RESERVED;\n\thost->max_id = UFSHCD_MAX_ID;\n\thost->max_lun = UFS_MAX_LUNS;\n\thost->max_channel = UFSHCD_MAX_CHANNEL;\n\thost->unique_id = host->host_no;\n\thost->max_cmd_len = UFS_CDB_SIZE;\n\thost->queuecommand_may_block = !!(hba->caps & UFSHCD_CAP_CLK_GATING);\n\n\thba->max_pwr_info.is_valid = false;\n\n\t \n\tsnprintf(eh_wq_name, sizeof(eh_wq_name), \"ufs_eh_wq_%d\",\n\t\t hba->host->host_no);\n\thba->eh_wq = create_singlethread_workqueue(eh_wq_name);\n\tif (!hba->eh_wq) {\n\t\tdev_err(hba->dev, \"%s: failed to create eh workqueue\\n\",\n\t\t\t__func__);\n\t\terr = -ENOMEM;\n\t\tgoto out_disable;\n\t}\n\tINIT_WORK(&hba->eh_work, ufshcd_err_handler);\n\tINIT_WORK(&hba->eeh_work, ufshcd_exception_event_handler);\n\n\tsema_init(&hba->host_sem, 1);\n\n\t \n\tmutex_init(&hba->uic_cmd_mutex);\n\n\t \n\tmutex_init(&hba->dev_cmd.lock);\n\n\t \n\tmutex_init(&hba->ee_ctrl_mutex);\n\n\tmutex_init(&hba->wb_mutex);\n\tinit_rwsem(&hba->clk_scaling_lock);\n\n\tufshcd_init_clk_gating(hba);\n\n\tufshcd_init_clk_scaling(hba);\n\n\t \n\tufshcd_writel(hba, ufshcd_readl(hba, REG_INTERRUPT_STATUS),\n\t\t      REG_INTERRUPT_STATUS);\n\tufshcd_writel(hba, 0, REG_INTERRUPT_ENABLE);\n\t \n\tmb();\n\n\t \n\terr = devm_request_irq(dev, irq, ufshcd_intr, IRQF_SHARED, UFSHCD, hba);\n\tif (err) {\n\t\tdev_err(hba->dev, \"request irq failed\\n\");\n\t\tgoto out_disable;\n\t} else {\n\t\thba->is_irq_enabled = true;\n\t}\n\n\tif (!is_mcq_supported(hba)) {\n\t\terr = scsi_add_host(host, hba->dev);\n\t\tif (err) {\n\t\t\tdev_err(hba->dev, \"scsi_add_host failed\\n\");\n\t\t\tgoto out_disable;\n\t\t}\n\t}\n\n\thba->tmf_tag_set = (struct blk_mq_tag_set) {\n\t\t.nr_hw_queues\t= 1,\n\t\t.queue_depth\t= hba->nutmrs,\n\t\t.ops\t\t= &ufshcd_tmf_ops,\n\t\t.flags\t\t= BLK_MQ_F_NO_SCHED,\n\t};\n\terr = blk_mq_alloc_tag_set(&hba->tmf_tag_set);\n\tif (err < 0)\n\t\tgoto out_remove_scsi_host;\n\thba->tmf_queue = blk_mq_init_queue(&hba->tmf_tag_set);\n\tif (IS_ERR(hba->tmf_queue)) {\n\t\terr = PTR_ERR(hba->tmf_queue);\n\t\tgoto free_tmf_tag_set;\n\t}\n\thba->tmf_rqs = devm_kcalloc(hba->dev, hba->nutmrs,\n\t\t\t\t    sizeof(*hba->tmf_rqs), GFP_KERNEL);\n\tif (!hba->tmf_rqs) {\n\t\terr = -ENOMEM;\n\t\tgoto free_tmf_queue;\n\t}\n\n\t \n\tufshcd_device_reset(hba);\n\n\tufshcd_init_crypto(hba);\n\n\t \n\terr = ufshcd_hba_enable(hba);\n\tif (err) {\n\t\tdev_err(hba->dev, \"Host controller enable failed\\n\");\n\t\tufshcd_print_evt_hist(hba);\n\t\tufshcd_print_host_state(hba);\n\t\tgoto free_tmf_queue;\n\t}\n\n\t \n\thba->rpm_lvl = ufs_get_desired_pm_lvl_for_dev_link_state(\n\t\t\t\t\t\tUFS_SLEEP_PWR_MODE,\n\t\t\t\t\t\tUIC_LINK_HIBERN8_STATE);\n\thba->spm_lvl = ufs_get_desired_pm_lvl_for_dev_link_state(\n\t\t\t\t\t\tUFS_SLEEP_PWR_MODE,\n\t\t\t\t\t\tUIC_LINK_HIBERN8_STATE);\n\n\tINIT_DELAYED_WORK(&hba->rpm_dev_flush_recheck_work,\n\t\t\t  ufshcd_rpm_dev_flush_recheck_work);\n\n\t \n\tif (ufshcd_is_auto_hibern8_supported(hba) && !hba->ahit) {\n\t\thba->ahit = FIELD_PREP(UFSHCI_AHIBERN8_TIMER_MASK, 150) |\n\t\t\t    FIELD_PREP(UFSHCI_AHIBERN8_SCALE_MASK, 3);\n\t}\n\n\t \n\tpm_runtime_get_sync(dev);\n\tatomic_set(&hba->scsi_block_reqs_cnt, 0);\n\t \n\tufshcd_set_ufs_dev_active(hba);\n\n\tasync_schedule(ufshcd_async_scan, hba);\n\tufs_sysfs_add_nodes(hba->dev);\n\n\tdevice_enable_async_suspend(dev);\n\treturn 0;\n\nfree_tmf_queue:\n\tblk_mq_destroy_queue(hba->tmf_queue);\n\tblk_put_queue(hba->tmf_queue);\nfree_tmf_tag_set:\n\tblk_mq_free_tag_set(&hba->tmf_tag_set);\nout_remove_scsi_host:\n\tscsi_remove_host(hba->host);\nout_disable:\n\thba->is_irq_enabled = false;\n\tufshcd_hba_exit(hba);\nout_error:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ufshcd_init);\n\nvoid ufshcd_resume_complete(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\tif (hba->complete_put) {\n\t\tufshcd_rpm_put(hba);\n\t\thba->complete_put = false;\n\t}\n}\nEXPORT_SYMBOL_GPL(ufshcd_resume_complete);\n\nstatic bool ufshcd_rpm_ok_for_spm(struct ufs_hba *hba)\n{\n\tstruct device *dev = &hba->ufs_device_wlun->sdev_gendev;\n\tenum ufs_dev_pwr_mode dev_pwr_mode;\n\tenum uic_link_state link_state;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&dev->power.lock, flags);\n\tdev_pwr_mode = ufs_get_pm_lvl_to_dev_pwr_mode(hba->spm_lvl);\n\tlink_state = ufs_get_pm_lvl_to_link_pwr_state(hba->spm_lvl);\n\tres = pm_runtime_suspended(dev) &&\n\t      hba->curr_dev_pwr_mode == dev_pwr_mode &&\n\t      hba->uic_link_state == link_state &&\n\t      !hba->dev_info.b_rpm_dev_flush_capable;\n\tspin_unlock_irqrestore(&dev->power.lock, flags);\n\n\treturn res;\n}\n\nint __ufshcd_suspend_prepare(struct device *dev, bool rpm_ok_for_spm)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret;\n\n\t \n\tif (hba->ufs_device_wlun) {\n\t\t \n\t\tufshcd_rpm_get_noresume(hba);\n\t\t \n\t\tif (!rpm_ok_for_spm || !ufshcd_rpm_ok_for_spm(hba)) {\n\t\t\t \n\t\t\tret = ufshcd_rpm_resume(hba);\n\t\t\tif (ret < 0 && ret != -EACCES) {\n\t\t\t\tufshcd_rpm_put(hba);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\thba->complete_put = true;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__ufshcd_suspend_prepare);\n\nint ufshcd_suspend_prepare(struct device *dev)\n{\n\treturn __ufshcd_suspend_prepare(dev, true);\n}\nEXPORT_SYMBOL_GPL(ufshcd_suspend_prepare);\n\n#ifdef CONFIG_PM_SLEEP\nstatic int ufshcd_wl_poweroff(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\tstruct ufs_hba *hba = shost_priv(sdev->host);\n\n\t__ufshcd_wl_suspend(hba, UFS_SHUTDOWN_PM);\n\treturn 0;\n}\n#endif\n\nstatic int ufshcd_wl_probe(struct device *dev)\n{\n\tstruct scsi_device *sdev = to_scsi_device(dev);\n\n\tif (!is_device_wlun(sdev))\n\t\treturn -ENODEV;\n\n\tblk_pm_runtime_init(sdev->request_queue, dev);\n\tpm_runtime_set_autosuspend_delay(dev, 0);\n\tpm_runtime_allow(dev);\n\n\treturn  0;\n}\n\nstatic int ufshcd_wl_remove(struct device *dev)\n{\n\tpm_runtime_forbid(dev);\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops ufshcd_wl_pm_ops = {\n#ifdef CONFIG_PM_SLEEP\n\t.suspend = ufshcd_wl_suspend,\n\t.resume = ufshcd_wl_resume,\n\t.freeze = ufshcd_wl_suspend,\n\t.thaw = ufshcd_wl_resume,\n\t.poweroff = ufshcd_wl_poweroff,\n\t.restore = ufshcd_wl_resume,\n#endif\n\tSET_RUNTIME_PM_OPS(ufshcd_wl_runtime_suspend, ufshcd_wl_runtime_resume, NULL)\n};\n\nstatic void ufshcd_check_header_layout(void)\n{\n\t \n\tif (IS_ENABLED(CONFIG_CC_IS_GCC) && CONFIG_GCC_VERSION < 100000)\n\t\treturn;\n\n\tBUILD_BUG_ON(((u8 *)&(struct request_desc_header){\n\t\t\t\t.cci = 3})[0] != 3);\n\n\tBUILD_BUG_ON(((u8 *)&(struct request_desc_header){\n\t\t\t\t.ehs_length = 2})[1] != 2);\n\n\tBUILD_BUG_ON(((u8 *)&(struct request_desc_header){\n\t\t\t\t.enable_crypto = 1})[2]\n\t\t     != 0x80);\n\n\tBUILD_BUG_ON((((u8 *)&(struct request_desc_header){\n\t\t\t\t\t.command_type = 5,\n\t\t\t\t\t.data_direction = 3,\n\t\t\t\t\t.interrupt = 1,\n\t\t\t\t})[3]) != ((5 << 4) | (3 << 1) | 1));\n\n\tBUILD_BUG_ON(((__le32 *)&(struct request_desc_header){\n\t\t\t\t.dunl = cpu_to_le32(0xdeadbeef)})[1] !=\n\t\tcpu_to_le32(0xdeadbeef));\n\n\tBUILD_BUG_ON(((u8 *)&(struct request_desc_header){\n\t\t\t\t.ocs = 4})[8] != 4);\n\n\tBUILD_BUG_ON(((u8 *)&(struct request_desc_header){\n\t\t\t\t.cds = 5})[9] != 5);\n\n\tBUILD_BUG_ON(((__le32 *)&(struct request_desc_header){\n\t\t\t\t.dunu = cpu_to_le32(0xbadcafe)})[3] !=\n\t\tcpu_to_le32(0xbadcafe));\n\n\tBUILD_BUG_ON(((u8 *)&(struct utp_upiu_header){\n\t\t\t     .iid = 0xf })[4] != 0xf0);\n\n\tBUILD_BUG_ON(((u8 *)&(struct utp_upiu_header){\n\t\t\t     .command_set_type = 0xf })[4] != 0xf);\n}\n\n \nstatic struct scsi_driver ufs_dev_wlun_template = {\n\t.gendrv = {\n\t\t.name = \"ufs_device_wlun\",\n\t\t.owner = THIS_MODULE,\n\t\t.probe = ufshcd_wl_probe,\n\t\t.remove = ufshcd_wl_remove,\n\t\t.pm = &ufshcd_wl_pm_ops,\n\t\t.shutdown = ufshcd_wl_shutdown,\n\t},\n};\n\nstatic int __init ufshcd_core_init(void)\n{\n\tint ret;\n\n\tufshcd_check_header_layout();\n\n\tufs_debugfs_init();\n\n\tret = scsi_register_driver(&ufs_dev_wlun_template.gendrv);\n\tif (ret)\n\t\tufs_debugfs_exit();\n\treturn ret;\n}\n\nstatic void __exit ufshcd_core_exit(void)\n{\n\tufs_debugfs_exit();\n\tscsi_unregister_driver(&ufs_dev_wlun_template.gendrv);\n}\n\nmodule_init(ufshcd_core_init);\nmodule_exit(ufshcd_core_exit);\n\nMODULE_AUTHOR(\"Santosh Yaragnavi <santosh.sy@samsung.com>\");\nMODULE_AUTHOR(\"Vinayak Holikatti <h.vinayak@samsung.com>\");\nMODULE_DESCRIPTION(\"Generic UFS host controller driver Core\");\nMODULE_SOFTDEP(\"pre: governor_simpleondemand\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}