{
  "module_name": "ufs-mediatek.c",
  "hash_id": "168e0ea8ccb381a49ef7c1160708934bd6eda08cb0814afb31a17e2db4664224",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ufs/host/ufs-mediatek.c",
  "human_readable_source": "\n \n\n#include <linux/arm-smccc.h>\n#include <linux/bitfield.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_device.h>\n#include <linux/of_platform.h>\n#include <linux/phy/phy.h>\n#include <linux/platform_device.h>\n#include <linux/pm_qos.h>\n#include <linux/regulator/consumer.h>\n#include <linux/reset.h>\n#include <linux/soc/mediatek/mtk_sip_svc.h>\n\n#include <ufs/ufshcd.h>\n#include \"ufshcd-pltfrm.h\"\n#include <ufs/ufs_quirks.h>\n#include <ufs/unipro.h>\n#include \"ufs-mediatek.h\"\n\nstatic int  ufs_mtk_config_mcq(struct ufs_hba *hba, bool irq);\n\n#define CREATE_TRACE_POINTS\n#include \"ufs-mediatek-trace.h\"\n#undef CREATE_TRACE_POINTS\n\n#define MAX_SUPP_MAC 64\n#define MCQ_QUEUE_OFFSET(c) ((((c) >> 16) & 0xFF) * 0x200)\n\nstatic const struct ufs_dev_quirk ufs_mtk_dev_fixups[] = {\n\t{ .wmanufacturerid = UFS_ANY_VENDOR,\n\t  .model = UFS_ANY_MODEL,\n\t  .quirk = UFS_DEVICE_QUIRK_DELAY_AFTER_LPM |\n\t\tUFS_DEVICE_QUIRK_DELAY_BEFORE_LPM },\n\t{ .wmanufacturerid = UFS_VENDOR_SKHYNIX,\n\t  .model = \"H9HQ21AFAMZDAR\",\n\t  .quirk = UFS_DEVICE_QUIRK_SUPPORT_EXTENDED_FEATURES },\n\t{}\n};\n\nstatic const struct of_device_id ufs_mtk_of_match[] = {\n\t{ .compatible = \"mediatek,mt8183-ufshci\" },\n\t{},\n};\n\n \nstatic const char *const ufs_uic_err_str[] = {\n\t\"PHY Adapter Layer\",\n\t\"Data Link Layer\",\n\t\"Network Link Layer\",\n\t\"Transport Link Layer\",\n\t\"DME\"\n};\n\nstatic const char *const ufs_uic_pa_err_str[] = {\n\t\"PHY error on Lane 0\",\n\t\"PHY error on Lane 1\",\n\t\"PHY error on Lane 2\",\n\t\"PHY error on Lane 3\",\n\t\"Generic PHY Adapter Error. This should be the LINERESET indication\"\n};\n\nstatic const char *const ufs_uic_dl_err_str[] = {\n\t\"NAC_RECEIVED\",\n\t\"TCx_REPLAY_TIMER_EXPIRED\",\n\t\"AFCx_REQUEST_TIMER_EXPIRED\",\n\t\"FCx_PROTECTION_TIMER_EXPIRED\",\n\t\"CRC_ERROR\",\n\t\"RX_BUFFER_OVERFLOW\",\n\t\"MAX_FRAME_LENGTH_EXCEEDED\",\n\t\"WRONG_SEQUENCE_NUMBER\",\n\t\"AFC_FRAME_SYNTAX_ERROR\",\n\t\"NAC_FRAME_SYNTAX_ERROR\",\n\t\"EOF_SYNTAX_ERROR\",\n\t\"FRAME_SYNTAX_ERROR\",\n\t\"BAD_CTRL_SYMBOL_TYPE\",\n\t\"PA_INIT_ERROR\",\n\t\"PA_ERROR_IND_RECEIVED\",\n\t\"PA_INIT\"\n};\n\nstatic bool ufs_mtk_is_boost_crypt_enabled(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BOOST_CRYPT_ENGINE);\n}\n\nstatic bool ufs_mtk_is_va09_supported(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_VA09_PWR_CTRL);\n}\n\nstatic bool ufs_mtk_is_broken_vcc(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BROKEN_VCC);\n}\n\nstatic bool ufs_mtk_is_pmc_via_fastauto(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_PMC_VIA_FASTAUTO);\n}\n\nstatic void ufs_mtk_cfg_unipro_cg(struct ufs_hba *hba, bool enable)\n{\n\tu32 tmp;\n\n\tif (enable) {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp |\n\t\t      (1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t      (1 << TX_CLK_GATE_EN);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp & ~(1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t} else {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp & ~((1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t\t      (1 << TX_CLK_GATE_EN));\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp | (1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t}\n}\n\nstatic void ufs_mtk_crypto_enable(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\tufs_mtk_crypto_ctrl(res, 1);\n\tif (res.a0) {\n\t\tdev_info(hba->dev, \"%s: crypto enable failed, err: %lu\\n\",\n\t\t\t __func__, res.a0);\n\t\thba->caps &= ~UFSHCD_CAP_CRYPTO;\n\t}\n}\n\nstatic void ufs_mtk_host_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treset_control_assert(host->hci_reset);\n\treset_control_assert(host->crypto_reset);\n\treset_control_assert(host->unipro_reset);\n\n\tusleep_range(100, 110);\n\n\treset_control_deassert(host->unipro_reset);\n\treset_control_deassert(host->crypto_reset);\n\treset_control_deassert(host->hci_reset);\n}\n\nstatic void ufs_mtk_init_reset_control(struct ufs_hba *hba,\n\t\t\t\t       struct reset_control **rc,\n\t\t\t\t       char *str)\n{\n\t*rc = devm_reset_control_get(hba->dev, str);\n\tif (IS_ERR(*rc)) {\n\t\tdev_info(hba->dev, \"Failed to get reset control %s: %ld\\n\",\n\t\t\t str, PTR_ERR(*rc));\n\t\t*rc = NULL;\n\t}\n}\n\nstatic void ufs_mtk_init_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tufs_mtk_init_reset_control(hba, &host->hci_reset,\n\t\t\t\t   \"hci_rst\");\n\tufs_mtk_init_reset_control(hba, &host->unipro_reset,\n\t\t\t\t   \"unipro_rst\");\n\tufs_mtk_init_reset_control(hba, &host->crypto_reset,\n\t\t\t\t   \"crypto_rst\");\n}\n\nstatic int ufs_mtk_hce_enable_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (status == PRE_CHANGE) {\n\t\tif (host->unipro_lpm) {\n\t\t\thba->vps->hba_enable_delay_us = 0;\n\t\t} else {\n\t\t\thba->vps->hba_enable_delay_us = 600;\n\t\t\tufs_mtk_host_reset(hba);\n\t\t}\n\n\t\tif (hba->caps & UFSHCD_CAP_CRYPTO)\n\t\t\tufs_mtk_crypto_enable(hba);\n\n\t\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8) {\n\t\t\tufshcd_writel(hba, 0,\n\t\t\t\t      REG_AUTO_HIBERNATE_IDLE_TIMER);\n\t\t\thba->capabilities &= ~MASK_AUTO_HIBERN8_SUPPORT;\n\t\t\thba->ahit = 0;\n\t\t}\n\n\t\t \n\t\tufshcd_writel(hba,\n\t\t\t      ufshcd_readl(hba, REG_UFS_XOUFS_CTRL) | 0x80,\n\t\t\t      REG_UFS_XOUFS_CTRL);\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_bind_mphy(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device *dev = hba->dev;\n\tstruct device_node *np = dev->of_node;\n\tint err = 0;\n\n\thost->mphy = devm_of_phy_get_by_index(dev, np, 0);\n\n\tif (host->mphy == ERR_PTR(-EPROBE_DEFER)) {\n\t\t \n\t\terr = -EPROBE_DEFER;\n\t\tdev_info(dev,\n\t\t\t \"%s: required phy hasn't probed yet. err = %d\\n\",\n\t\t\t__func__, err);\n\t} else if (IS_ERR(host->mphy)) {\n\t\terr = PTR_ERR(host->mphy);\n\t\tif (err != -ENODEV) {\n\t\t\tdev_info(dev, \"%s: PHY get failed %d\\n\", __func__,\n\t\t\t\t err);\n\t\t}\n\t}\n\n\tif (err)\n\t\thost->mphy = NULL;\n\t \n\tif (err == -ENODEV)\n\t\terr = 0;\n\n\treturn err;\n}\n\nstatic int ufs_mtk_setup_ref_clk(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct arm_smccc_res res;\n\tktime_t timeout, time_checked;\n\tu32 value;\n\n\tif (host->ref_clk_enabled == on)\n\t\treturn 0;\n\n\tufs_mtk_ref_clk_notify(on, PRE_CHANGE, res);\n\n\tif (on) {\n\t\tufshcd_writel(hba, REFCLK_REQUEST, REG_UFS_REFCLK_CTRL);\n\t} else {\n\t\tufshcd_delay_us(host->ref_clk_gating_wait_us, 10);\n\t\tufshcd_writel(hba, REFCLK_RELEASE, REG_UFS_REFCLK_CTRL);\n\t}\n\n\t \n\ttimeout = ktime_add_us(ktime_get(), REFCLK_REQ_TIMEOUT_US);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tvalue = ufshcd_readl(hba, REG_UFS_REFCLK_CTRL);\n\n\t\t \n\t\tif (((value & REFCLK_ACK) >> 1) == (value & REFCLK_REQUEST))\n\t\t\tgoto out;\n\n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\tdev_err(hba->dev, \"missing ack of refclk req, reg: 0x%x\\n\", value);\n\n\tufs_mtk_ref_clk_notify(host->ref_clk_enabled, POST_CHANGE, res);\n\n\treturn -ETIMEDOUT;\n\nout:\n\thost->ref_clk_enabled = on;\n\tif (on)\n\t\tufshcd_delay_us(host->ref_clk_ungating_wait_us, 10);\n\n\tufs_mtk_ref_clk_notify(on, POST_CHANGE, res);\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_setup_ref_clk_wait_us(struct ufs_hba *hba,\n\t\t\t\t\t  u16 gating_us)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (hba->dev_info.clk_gating_wait_us) {\n\t\thost->ref_clk_gating_wait_us =\n\t\t\thba->dev_info.clk_gating_wait_us;\n\t} else {\n\t\thost->ref_clk_gating_wait_us = gating_us;\n\t}\n\n\thost->ref_clk_ungating_wait_us = REFCLK_DEFAULT_WAIT_US;\n}\n\nstatic void ufs_mtk_dbg_sel(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (((host->ip_ver >> 16) & 0xFF) >= 0x36) {\n\t\tufshcd_writel(hba, 0x820820, REG_UFS_DEBUG_SEL);\n\t\tufshcd_writel(hba, 0x0, REG_UFS_DEBUG_SEL_B0);\n\t\tufshcd_writel(hba, 0x55555555, REG_UFS_DEBUG_SEL_B1);\n\t\tufshcd_writel(hba, 0xaaaaaaaa, REG_UFS_DEBUG_SEL_B2);\n\t\tufshcd_writel(hba, 0xffffffff, REG_UFS_DEBUG_SEL_B3);\n\t} else {\n\t\tufshcd_writel(hba, 0x20, REG_UFS_DEBUG_SEL);\n\t}\n}\n\nstatic void ufs_mtk_wait_idle_state(struct ufs_hba *hba,\n\t\t\t    unsigned long retry_ms)\n{\n\tu64 timeout, time_checked;\n\tu32 val, sm;\n\tbool wait_idle;\n\n\t \n\ttimeout = ktime_get_mono_fast_ns() + retry_ms * 1000000UL;\n\n\t \n\tudelay(10);\n\twait_idle = false;\n\n\tdo {\n\t\ttime_checked = ktime_get_mono_fast_ns();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\n\t\tsm = val & 0x1f;\n\n\t\t \n\t\tif ((sm >= VS_HIB_ENTER) && (sm <= VS_HIB_EXIT)) {\n\t\t\twait_idle = true;\n\t\t\tudelay(50);\n\t\t\tcontinue;\n\t\t} else if (!wait_idle)\n\t\t\tbreak;\n\n\t\tif (wait_idle && (sm == VS_HCE_BASE))\n\t\t\tbreak;\n\t} while (time_checked < timeout);\n\n\tif (wait_idle && sm != VS_HCE_BASE)\n\t\tdev_info(hba->dev, \"wait idle tmo: 0x%x\\n\", val);\n}\n\nstatic int ufs_mtk_wait_link_state(struct ufs_hba *hba, u32 state,\n\t\t\t\t   unsigned long max_wait_ms)\n{\n\tktime_t timeout, time_checked;\n\tu32 val;\n\n\ttimeout = ktime_add_ms(ktime_get(), max_wait_ms);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\t\tval = val >> 28;\n\n\t\tif (val == state)\n\t\t\treturn 0;\n\n\t\t \n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int ufs_mtk_mphy_power_on(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct phy *mphy = host->mphy;\n\tstruct arm_smccc_res res;\n\tint ret = 0;\n\n\tif (!mphy || !(on ^ host->mphy_powered_on))\n\t\treturn 0;\n\n\tif (on) {\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tret = regulator_enable(host->reg_va09);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\t \n\t\t\tusleep_range(200, 210);\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 1);\n\t\t}\n\t\tphy_power_on(mphy);\n\t} else {\n\t\tphy_power_off(mphy);\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 0);\n\t\t\tret = regulator_disable(host->reg_va09);\n\t\t}\n\t}\nout:\n\tif (ret) {\n\t\tdev_info(hba->dev,\n\t\t\t \"failed to %s va09: %d\\n\",\n\t\t\t on ? \"enable\" : \"disable\",\n\t\t\t ret);\n\t} else {\n\t\thost->mphy_powered_on = on;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_get_host_clk(struct device *dev, const char *name,\n\t\t\t\tstruct clk **clk_out)\n{\n\tstruct clk *clk;\n\tint err = 0;\n\n\tclk = devm_clk_get(dev, name);\n\tif (IS_ERR(clk))\n\t\terr = PTR_ERR(clk);\n\telse\n\t\t*clk_out = clk;\n\n\treturn err;\n}\n\nstatic void ufs_mtk_boost_crypt(struct ufs_hba *hba, bool boost)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct regulator *reg;\n\tint volt, ret;\n\n\tif (!ufs_mtk_is_boost_crypt_enabled(hba))\n\t\treturn;\n\n\tcfg = host->crypt;\n\tvolt = cfg->vcore_volt;\n\treg = cfg->reg_vcore;\n\n\tret = clk_prepare_enable(cfg->clk_crypt_mux);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"clk_prepare_enable(): %d\\n\",\n\t\t\t ret);\n\t\treturn;\n\t}\n\n\tif (boost) {\n\t\tret = regulator_set_voltage(reg, volt, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to %d\\n\", volt);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_perf);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_perf\\n\");\n\t\t\tregulator_set_voltage(reg, 0, INT_MAX);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_lp);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_lp\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = regulator_set_voltage(reg, 0, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to MIN\\n\");\n\t\t}\n\t}\nout:\n\tclk_disable_unprepare(cfg->clk_crypt_mux);\n}\n\nstatic int ufs_mtk_init_host_clk(struct ufs_hba *hba, const char *name,\n\t\t\t\t struct clk **clk)\n{\n\tint ret;\n\n\tret = ufs_mtk_get_host_clk(hba->dev, name, clk);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"%s: failed to get %s: %d\", __func__,\n\t\t\t name, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_init_boost_crypt(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct device *dev = hba->dev;\n\tstruct regulator *reg;\n\tu32 volt;\n\n\thost->crypt = devm_kzalloc(dev, sizeof(*(host->crypt)),\n\t\t\t\t   GFP_KERNEL);\n\tif (!host->crypt)\n\t\tgoto disable_caps;\n\n\treg = devm_regulator_get_optional(dev, \"dvfsrc-vcore\");\n\tif (IS_ERR(reg)) {\n\t\tdev_info(dev, \"failed to get dvfsrc-vcore: %ld\",\n\t\t\t PTR_ERR(reg));\n\t\tgoto disable_caps;\n\t}\n\n\tif (of_property_read_u32(dev->of_node, \"boost-crypt-vcore-min\",\n\t\t\t\t &volt)) {\n\t\tdev_info(dev, \"failed to get boost-crypt-vcore-min\");\n\t\tgoto disable_caps;\n\t}\n\n\tcfg = host->crypt;\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_mux\",\n\t\t\t\t  &cfg->clk_crypt_mux))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_lp\",\n\t\t\t\t  &cfg->clk_crypt_lp))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_perf\",\n\t\t\t\t  &cfg->clk_crypt_perf))\n\t\tgoto disable_caps;\n\n\tcfg->reg_vcore = reg;\n\tcfg->vcore_volt = volt;\n\thost->caps |= UFS_MTK_CAP_BOOST_CRYPT_ENGINE;\n\ndisable_caps:\n\treturn;\n}\n\nstatic void ufs_mtk_init_va09_pwr_ctrl(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\thost->reg_va09 = regulator_get(hba->dev, \"va09\");\n\tif (IS_ERR(host->reg_va09))\n\t\tdev_info(hba->dev, \"failed to get va09\");\n\telse\n\t\thost->caps |= UFS_MTK_CAP_VA09_PWR_CTRL;\n}\n\nstatic void ufs_mtk_init_host_caps(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device_node *np = hba->dev->of_node;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-boost-crypt\"))\n\t\tufs_mtk_init_boost_crypt(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-support-va09\"))\n\t\tufs_mtk_init_va09_pwr_ctrl(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-disable-ah8\"))\n\t\thost->caps |= UFS_MTK_CAP_DISABLE_AH8;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-broken-vcc\"))\n\t\thost->caps |= UFS_MTK_CAP_BROKEN_VCC;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-pmc-via-fastauto\"))\n\t\thost->caps |= UFS_MTK_CAP_PMC_VIA_FASTAUTO;\n\n\tdev_info(hba->dev, \"caps: 0x%x\", host->caps);\n}\n\nstatic void ufs_mtk_boost_pm_qos(struct ufs_hba *hba, bool boost)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (!host || !host->pm_qos_init)\n\t\treturn;\n\n\tcpu_latency_qos_update_request(&host->pm_qos_req,\n\t\t\t\t       boost ? 0 : PM_QOS_DEFAULT_VALUE);\n}\n\nstatic void ufs_mtk_scale_perf(struct ufs_hba *hba, bool scale_up)\n{\n\tufs_mtk_boost_crypt(hba, scale_up);\n\tufs_mtk_boost_pm_qos(hba, scale_up);\n}\n\nstatic void ufs_mtk_pwr_ctrl(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (on) {\n\t\tphy_power_on(host->mphy);\n\t\tufs_mtk_setup_ref_clk(hba, on);\n\t\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\t\tufs_mtk_scale_perf(hba, on);\n\t} else {\n\t\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\t\tufs_mtk_scale_perf(hba, on);\n\t\tufs_mtk_setup_ref_clk(hba, on);\n\t\tphy_power_off(host->mphy);\n\t}\n}\n\n \nstatic int ufs_mtk_setup_clocks(struct ufs_hba *hba, bool on,\n\t\t\t\tenum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tbool clk_pwr_off = false;\n\tint ret = 0;\n\n\t \n\tif (!host)\n\t\treturn 0;\n\n\tif (!on && status == PRE_CHANGE) {\n\t\tif (ufshcd_is_link_off(hba)) {\n\t\t\tclk_pwr_off = true;\n\t\t} else if (ufshcd_is_link_hibern8(hba) ||\n\t\t\t (!ufshcd_can_hibern8_during_gating(hba) &&\n\t\t\t ufshcd_is_auto_hibern8_enabled(hba))) {\n\t\t\t \n\t\t\tret = ufs_mtk_wait_link_state(hba,\n\t\t\t\t\t\t      VS_LINK_HIBERN8,\n\t\t\t\t\t\t      15);\n\t\t\tif (!ret)\n\t\t\t\tclk_pwr_off = true;\n\t\t}\n\n\t\tif (clk_pwr_off)\n\t\t\tufs_mtk_pwr_ctrl(hba, false);\n\t} else if (on && status == POST_CHANGE) {\n\t\tufs_mtk_pwr_ctrl(hba, true);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_get_controller_version(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tint ret, ver = 0;\n\n\tif (host->hw_ver.major)\n\t\treturn;\n\n\t \n\thost->hw_ver.major = 2;\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_LOCALVERINFO), &ver);\n\tif (!ret) {\n\t\tif (ver >= UFS_UNIPRO_VER_1_8) {\n\t\t\thost->hw_ver.major = 3;\n\t\t\t \n\t\t\tif (hba->ufs_version < ufshci_version(3, 0))\n\t\t\t\thba->ufs_version = ufshci_version(3, 0);\n\t\t}\n\t}\n}\n\nstatic u32 ufs_mtk_get_ufs_hci_version(struct ufs_hba *hba)\n{\n\treturn hba->ufs_version;\n}\n\n \nstatic void ufs_mtk_init_clocks(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct list_head *head = &hba->clk_list_head;\n\tstruct ufs_mtk_clk *mclk = &host->mclk;\n\tstruct ufs_clk_info *clki, *clki_tmp;\n\n\t \n\tlist_for_each_entry_safe(clki, clki_tmp, head, list) {\n\t\tif (!strcmp(clki->name, \"ufs_sel\")) {\n\t\t\thost->mclk.ufs_sel_clki = clki;\n\t\t} else if (!strcmp(clki->name, \"ufs_sel_max_src\")) {\n\t\t\thost->mclk.ufs_sel_max_clki = clki;\n\t\t\tclk_disable_unprepare(clki->clk);\n\t\t\tlist_del(&clki->list);\n\t\t} else if (!strcmp(clki->name, \"ufs_sel_min_src\")) {\n\t\t\thost->mclk.ufs_sel_min_clki = clki;\n\t\t\tclk_disable_unprepare(clki->clk);\n\t\t\tlist_del(&clki->list);\n\t\t}\n\t}\n\n\tif (!mclk->ufs_sel_clki || !mclk->ufs_sel_max_clki ||\n\t    !mclk->ufs_sel_min_clki) {\n\t\thba->caps &= ~UFSHCD_CAP_CLK_SCALING;\n\t\tdev_info(hba->dev,\n\t\t\t \"%s: Clk-scaling not ready. Feature disabled.\",\n\t\t\t __func__);\n\t}\n}\n\n#define MAX_VCC_NAME 30\nstatic int ufs_mtk_vreg_fix_vcc(struct ufs_hba *hba)\n{\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\tstruct device_node *np = hba->dev->of_node;\n\tstruct device *dev = hba->dev;\n\tchar vcc_name[MAX_VCC_NAME];\n\tstruct arm_smccc_res res;\n\tint err, ver;\n\n\tif (hba->vreg_info.vcc)\n\t\treturn 0;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-vcc-by-num\")) {\n\t\tufs_mtk_get_vcc_num(res);\n\t\tif (res.a1 > UFS_VCC_NONE && res.a1 < UFS_VCC_MAX)\n\t\t\tsnprintf(vcc_name, MAX_VCC_NAME, \"vcc-opt%lu\", res.a1);\n\t\telse\n\t\t\treturn -ENODEV;\n\t} else if (of_property_read_bool(np, \"mediatek,ufs-vcc-by-ver\")) {\n\t\tver = (hba->dev_info.wspecversion & 0xF00) >> 8;\n\t\tsnprintf(vcc_name, MAX_VCC_NAME, \"vcc-ufs%u\", ver);\n\t} else {\n\t\treturn 0;\n\t}\n\n\terr = ufshcd_populate_vreg(dev, vcc_name, &info->vcc);\n\tif (err)\n\t\treturn err;\n\n\terr = ufshcd_get_vreg(dev, info->vcc);\n\tif (err)\n\t\treturn err;\n\n\terr = regulator_enable(info->vcc->reg);\n\tif (!err) {\n\t\tinfo->vcc->enabled = true;\n\t\tdev_info(dev, \"%s: %s enabled\\n\", __func__, vcc_name);\n\t}\n\n\treturn err;\n}\n\nstatic void ufs_mtk_vreg_fix_vccqx(struct ufs_hba *hba)\n{\n\tstruct ufs_vreg_info *info = &hba->vreg_info;\n\tstruct ufs_vreg **vreg_on, **vreg_off;\n\n\tif (hba->dev_info.wspecversion >= 0x0300) {\n\t\tvreg_on = &info->vccq;\n\t\tvreg_off = &info->vccq2;\n\t} else {\n\t\tvreg_on = &info->vccq2;\n\t\tvreg_off = &info->vccq;\n\t}\n\n\tif (*vreg_on)\n\t\t(*vreg_on)->always_on = true;\n\n\tif (*vreg_off) {\n\t\tregulator_disable((*vreg_off)->reg);\n\t\tdevm_kfree(hba->dev, (*vreg_off)->name);\n\t\tdevm_kfree(hba->dev, *vreg_off);\n\t\t*vreg_off = NULL;\n\t}\n}\n\nstatic void ufs_mtk_init_mcq_irq(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct platform_device *pdev;\n\tint i;\n\tint irq;\n\n\thost->mcq_nr_intr = UFSHCD_MAX_Q_NR;\n\tpdev = container_of(hba->dev, struct platform_device, dev);\n\n\tfor (i = 0; i < host->mcq_nr_intr; i++) {\n\t\t \n\t\tirq = platform_get_irq(pdev, i + 1);\n\t\tif (irq < 0) {\n\t\t\thost->mcq_intr_info[i].irq = MTK_MCQ_INVALID_IRQ;\n\t\t\tgoto failed;\n\t\t}\n\t\thost->mcq_intr_info[i].hba = hba;\n\t\thost->mcq_intr_info[i].irq = irq;\n\t\tdev_info(hba->dev, \"get platform mcq irq: %d, %d\\n\", i, irq);\n\t}\n\n\treturn;\nfailed:\n        \n\tfor (i = 0; i < host->mcq_nr_intr; i++)\n\t\thost->mcq_intr_info[i].irq = MTK_MCQ_INVALID_IRQ;\n\n\thost->mcq_nr_intr = 0;\n}\n\n \nstatic int ufs_mtk_init(struct ufs_hba *hba)\n{\n\tconst struct of_device_id *id;\n\tstruct device *dev = hba->dev;\n\tstruct ufs_mtk_host *host;\n\tint err = 0;\n\n\thost = devm_kzalloc(dev, sizeof(*host), GFP_KERNEL);\n\tif (!host) {\n\t\terr = -ENOMEM;\n\t\tdev_info(dev, \"%s: no memory for mtk ufs host\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\thost->hba = hba;\n\tufshcd_set_variant(hba, host);\n\n\tid = of_match_device(ufs_mtk_of_match, dev);\n\tif (!id) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tufs_mtk_init_host_caps(hba);\n\n\tufs_mtk_init_mcq_irq(hba);\n\n\terr = ufs_mtk_bind_mphy(hba);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\tufs_mtk_init_reset(hba);\n\n\t \n\thba->caps |= UFSHCD_CAP_RPM_AUTOSUSPEND;\n\n\t \n\thba->caps |= UFSHCD_CAP_CLK_GATING;\n\n\t \n\thba->caps |= UFSHCD_CAP_CRYPTO;\n\n\t \n\thba->caps |= UFSHCD_CAP_WB_EN;\n\n\t \n\thba->caps |= UFSHCD_CAP_CLK_SCALING;\n\n\thba->quirks |= UFSHCI_QUIRK_SKIP_MANUAL_WB_FLUSH_CTRL;\n\thba->quirks |= UFSHCD_QUIRK_MCQ_BROKEN_INTR;\n\thba->quirks |= UFSHCD_QUIRK_MCQ_BROKEN_RTC;\n\thba->vps->wb_flush_threshold = UFS_WB_BUF_REMAIN_PERCENT(80);\n\n\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8)\n\t\thba->caps |= UFSHCD_CAP_HIBERN8_WITH_CLK_GATING;\n\n\tufs_mtk_init_clocks(hba);\n\n\t \n\tufs_mtk_mphy_power_on(hba, true);\n\tufs_mtk_setup_clocks(hba, true, POST_CHANGE);\n\n\thost->ip_ver = ufshcd_readl(hba, REG_UFS_MTK_IP_VER);\n\n\t \n\tcpu_latency_qos_add_request(&host->pm_qos_req, PM_QOS_DEFAULT_VALUE);\n\thost->pm_qos_init = true;\n\n\tgoto out;\n\nout_variant_clear:\n\tufshcd_set_variant(hba, NULL);\nout:\n\treturn err;\n}\n\nstatic bool ufs_mtk_pmc_via_fastauto(struct ufs_hba *hba,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_req_params)\n{\n\tif (!ufs_mtk_is_pmc_via_fastauto(hba))\n\t\treturn false;\n\n\tif (dev_req_params->hs_rate == hba->pwr_info.hs_rate)\n\t\treturn false;\n\n\tif (dev_req_params->pwr_tx != FAST_MODE &&\n\t    dev_req_params->gear_tx < UFS_HS_G4)\n\t\treturn false;\n\n\tif (dev_req_params->pwr_rx != FAST_MODE &&\n\t    dev_req_params->gear_rx < UFS_HS_G4)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int ufs_mtk_pre_pwr_change(struct ufs_hba *hba,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_req_params)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_dev_params host_cap;\n\tint ret;\n\n\tufshcd_init_pwr_dev_param(&host_cap);\n\thost_cap.hs_rx_gear = UFS_HS_G5;\n\thost_cap.hs_tx_gear = UFS_HS_G5;\n\n\tret = ufshcd_get_pwr_dev_param(&host_cap,\n\t\t\t\t       dev_max_params,\n\t\t\t\t       dev_req_params);\n\tif (ret) {\n\t\tpr_info(\"%s: failed to determine capabilities\\n\",\n\t\t\t__func__);\n\t}\n\n\tif (ufs_mtk_pmc_via_fastauto(hba, dev_req_params)) {\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXTERMINATION), true);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXGEAR), UFS_HS_G1);\n\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXTERMINATION), true);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXGEAR), UFS_HS_G1);\n\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVETXDATALANES),\n\t\t\t       dev_req_params->lane_tx);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVERXDATALANES),\n\t\t\t       dev_req_params->lane_rx);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_HSSERIES),\n\t\t\t       dev_req_params->hs_rate);\n\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXHSADAPTTYPE),\n\t\t\t       PA_NO_ADAPT);\n\n\t\tret = ufshcd_uic_change_pwr_mode(hba,\n\t\t\t\t\tFASTAUTO_MODE << 4 | FASTAUTO_MODE);\n\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: HSG1B FASTAUTO failed ret=%d\\n\",\n\t\t\t\t__func__, ret);\n\t\t}\n\t}\n\n\tif (host->hw_ver.major >= 3) {\n\t\tret = ufshcd_dme_configure_adapt(hba,\n\t\t\t\t\t   dev_req_params->gear_tx,\n\t\t\t\t\t   PA_INITIAL_ADAPT);\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pwr_change_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status stage,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_req_params)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_pwr_change(hba, dev_max_params,\n\t\t\t\t\t     dev_req_params);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_unipro_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tint ret;\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tret = ufshcd_dme_set(hba,\n\t\t\t     UIC_ARG_MIB_SEL(VS_UNIPROPOWERDOWNCONTROL, 0),\n\t\t\t     lpm ? 1 : 0);\n\tif (!ret || !lpm) {\n\t\t \n\t\thost->unipro_lpm = lpm;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pre_link(struct ufs_hba *hba)\n{\n\tint ret;\n\tu32 tmp;\n\n\tufs_mtk_get_controller_version(hba);\n\n\tret = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = ufshcd_disable_host_tx_lcc(hba);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\tif (ret)\n\t\treturn ret;\n\n\ttmp &= ~(1 << 6);\n\n\tret = ufshcd_dme_set(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_setup_clk_gating(struct ufs_hba *hba)\n{\n\tu32 ah_ms;\n\n\tif (ufshcd_is_clkgating_allowed(hba)) {\n\t\tif (ufshcd_is_auto_hibern8_supported(hba) && hba->ahit)\n\t\t\tah_ms = FIELD_GET(UFSHCI_AHIBERN8_TIMER_MASK,\n\t\t\t\t\t  hba->ahit);\n\t\telse\n\t\t\tah_ms = 10;\n\t\tufshcd_clkgate_delay_set(hba->dev, ah_ms + 5);\n\t}\n}\n\nstatic void ufs_mtk_post_link(struct ufs_hba *hba)\n{\n\t \n\tufs_mtk_cfg_unipro_cg(hba, true);\n\n\t \n\tif (ufshcd_is_auto_hibern8_supported(hba))\n\t\thba->ahit = FIELD_PREP(UFSHCI_AHIBERN8_TIMER_MASK, 10) |\n\t\t\tFIELD_PREP(UFSHCI_AHIBERN8_SCALE_MASK, 3);\n\n\tufs_mtk_setup_clk_gating(hba);\n}\n\nstatic int ufs_mtk_link_startup_notify(struct ufs_hba *hba,\n\t\t\t\t       enum ufs_notify_change_status stage)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_link(hba);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tufs_mtk_post_link(hba);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_device_reset(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\t \n\tufshcd_hba_stop(hba);\n\n\tufs_mtk_device_reset_ctrl(0, res);\n\n\t \n\tusleep_range(10, 15);\n\n\tufs_mtk_device_reset_ctrl(1, res);\n\n\t \n\tusleep_range(10000, 15000);\n\n\tdev_info(hba->dev, \"device reset done\\n\");\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_hpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufshcd_hba_enable(hba);\n\tif (err)\n\t\treturn err;\n\n\terr = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (err)\n\t\treturn err;\n\n\terr = ufshcd_uic_hibern8_exit(hba);\n\tif (!err)\n\t\tufshcd_set_link_active(hba);\n\telse\n\t\treturn err;\n\n\tif (!hba->mcq_enabled) {\n\t\terr = ufshcd_make_hba_operational(hba);\n\t} else {\n\t\tufs_mtk_config_mcq(hba, false);\n\t\tufshcd_mcq_make_queues_operational(hba);\n\t\tufshcd_mcq_config_mac(hba, hba->nutrs);\n\t\t \n\t\tufshcd_writel(hba, ufshcd_readl(hba, REG_UFS_MEM_CFG) | 0x1,\n\t\t\t      REG_UFS_MEM_CFG);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_lpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\t \n\tufshcd_writel(hba,\n\t\t      (ufshcd_readl(hba, REG_UFS_XOUFS_CTRL) & ~0x100),\n\t\t      REG_UFS_XOUFS_CTRL);\n\n\terr = ufs_mtk_unipro_set_lpm(hba, true);\n\tif (err) {\n\t\t \n\t\tufs_mtk_unipro_set_lpm(hba, false);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_vccqx_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tstruct ufs_vreg *vccqx = NULL;\n\n\tif (hba->vreg_info.vccq)\n\t\tvccqx = hba->vreg_info.vccq;\n\telse\n\t\tvccqx = hba->vreg_info.vccq2;\n\n\tregulator_set_mode(vccqx->reg,\n\t\t\t   lpm ? REGULATOR_MODE_IDLE : REGULATOR_MODE_NORMAL);\n}\n\nstatic void ufs_mtk_vsx_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tstruct arm_smccc_res res;\n\n\tufs_mtk_device_pwr_ctrl(!lpm,\n\t\t\t\t(unsigned long)hba->dev_info.wspecversion,\n\t\t\t\tres);\n}\n\nstatic void ufs_mtk_dev_vreg_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tif (!hba->vreg_info.vccq && !hba->vreg_info.vccq2)\n\t\treturn;\n\n\t \n\tif (!hba->vreg_info.vcc)\n\t\treturn;\n\n\t \n\tif (lpm && ufshcd_is_ufs_dev_active(hba))\n\t\treturn;\n\n\t \n\tif (lpm && hba->vreg_info.vcc->enabled)\n\t\treturn;\n\n\tif (lpm) {\n\t\tufs_mtk_vccqx_set_lpm(hba, lpm);\n\t\tufs_mtk_vsx_set_lpm(hba, lpm);\n\t} else {\n\t\tufs_mtk_vsx_set_lpm(hba, lpm);\n\t\tufs_mtk_vccqx_set_lpm(hba, lpm);\n\t}\n}\n\nstatic void ufs_mtk_auto_hibern8_disable(struct ufs_hba *hba)\n{\n\tint ret;\n\n\t \n\tufshcd_writel(hba, 0, REG_AUTO_HIBERNATE_IDLE_TIMER);\n\n\t \n\tufs_mtk_wait_idle_state(hba, 5);\n\n\tret = ufs_mtk_wait_link_state(hba, VS_LINK_UP, 100);\n\tif (ret)\n\t\tdev_warn(hba->dev, \"exit h8 state fail, ret=%d\\n\", ret);\n}\n\nstatic int ufs_mtk_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op,\n\tenum ufs_notify_change_status status)\n{\n\tint err;\n\tstruct arm_smccc_res res;\n\n\tif (status == PRE_CHANGE) {\n\t\tif (ufshcd_is_auto_hibern8_supported(hba))\n\t\t\tufs_mtk_auto_hibern8_disable(hba);\n\t\treturn 0;\n\t}\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_lpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (!ufshcd_is_link_active(hba)) {\n\t\t \n\t\terr = ufs_mtk_mphy_power_on(hba, false);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (ufshcd_is_link_off(hba))\n\t\tufs_mtk_device_reset_ctrl(0, res);\n\n\tufs_mtk_host_pwr_ctrl(HOST_PWR_HCI, false, res);\n\n\treturn 0;\nfail:\n\t \n\tufshcd_set_link_off(hba);\n\treturn -EAGAIN;\n}\n\nstatic int ufs_mtk_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tint err;\n\tstruct arm_smccc_res res;\n\n\tif (hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL)\n\t\tufs_mtk_dev_vreg_set_lpm(hba, false);\n\n\tufs_mtk_host_pwr_ctrl(HOST_PWR_HCI, true, res);\n\n\terr = ufs_mtk_mphy_power_on(hba, true);\n\tif (err)\n\t\tgoto fail;\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_hpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\nfail:\n\treturn ufshcd_link_recovery(hba);\n}\n\nstatic void ufs_mtk_dbg_register_dump(struct ufs_hba *hba)\n{\n\t \n\tufshcd_dump_regs(hba, REG_UFS_XOUFS_CTRL, 0x10,\n\t\t\t \"XOUFS Ctrl (0x140): \");\n\n\tufshcd_dump_regs(hba, REG_UFS_EXTREG, 0x4, \"Ext Reg \");\n\n\t \n\tufshcd_dump_regs(hba, REG_UFS_MPHYCTRL,\n\t\t\t REG_UFS_REJECT_MON - REG_UFS_MPHYCTRL + 4,\n\t\t\t \"MPHY Ctrl (0x2200): \");\n\n\t \n\tufs_mtk_dbg_sel(hba);\n\tufshcd_dump_regs(hba, REG_UFS_PROBE, 0x4, \"Debug Probe \");\n}\n\nstatic int ufs_mtk_apply_dev_quirks(struct ufs_hba *hba)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu16 mid = dev_info->wmanufacturerid;\n\n\tif (mid == UFS_VENDOR_SAMSUNG) {\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TACTIVATE), 6);\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_HIBERN8TIME), 10);\n\t}\n\n\t \n\tif (mid == UFS_VENDOR_SAMSUNG)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 1);\n\telse if (mid == UFS_VENDOR_SKHYNIX)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 30);\n\telse if (mid == UFS_VENDOR_TOSHIBA)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 100);\n\telse\n\t\tufs_mtk_setup_ref_clk_wait_us(hba,\n\t\t\t\t\t      REFCLK_DEFAULT_WAIT_US);\n\treturn 0;\n}\n\nstatic void ufs_mtk_fixup_dev_quirks(struct ufs_hba *hba)\n{\n\tufshcd_fixup_dev_quirks(hba, ufs_mtk_dev_fixups);\n\n\tif (ufs_mtk_is_broken_vcc(hba) && hba->vreg_info.vcc &&\n\t    (hba->dev_quirks & UFS_DEVICE_QUIRK_DELAY_AFTER_LPM)) {\n\t\thba->vreg_info.vcc->always_on = true;\n\t\t \n\t\thba->dev_quirks &= ~(UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM |\n\t\t\tUFS_DEVICE_QUIRK_DELAY_AFTER_LPM);\n\t}\n\n\tufs_mtk_vreg_fix_vcc(hba);\n\tufs_mtk_vreg_fix_vccqx(hba);\n}\n\nstatic void ufs_mtk_event_notify(struct ufs_hba *hba,\n\t\t\t\t enum ufs_event_type evt, void *data)\n{\n\tunsigned int val = *(u32 *)data;\n\tunsigned long reg;\n\tu8 bit;\n\n\ttrace_ufs_mtk_event(evt, val);\n\n\t \n\tif (evt <= UFS_EVT_DME_ERR) {\n\t\tdev_info(hba->dev,\n\t\t\t \"Host UIC Error Code (%s): %08x\\n\",\n\t\t\t ufs_uic_err_str[evt], val);\n\t\treg = val;\n\t}\n\n\tif (evt == UFS_EVT_PA_ERR) {\n\t\tfor_each_set_bit(bit, &reg, ARRAY_SIZE(ufs_uic_pa_err_str))\n\t\t\tdev_info(hba->dev, \"%s\\n\", ufs_uic_pa_err_str[bit]);\n\t}\n\n\tif (evt == UFS_EVT_DL_ERR) {\n\t\tfor_each_set_bit(bit, &reg, ARRAY_SIZE(ufs_uic_dl_err_str))\n\t\t\tdev_info(hba->dev, \"%s\\n\", ufs_uic_dl_err_str[bit]);\n\t}\n}\n\nstatic void ufs_mtk_config_scaling_param(struct ufs_hba *hba,\n\t\t\t\tstruct devfreq_dev_profile *profile,\n\t\t\t\tstruct devfreq_simple_ondemand_data *data)\n{\n\t \n\thba->clk_scaling.min_gear = UFS_HS_G4;\n\n\thba->vps->devfreq_profile.polling_ms = 200;\n\thba->vps->ondemand_data.upthreshold = 50;\n\thba->vps->ondemand_data.downdifferential = 20;\n}\n\n \nstatic void ufs_mtk_clk_scale(struct ufs_hba *hba, bool scale_up)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_clk *mclk = &host->mclk;\n\tstruct ufs_clk_info *clki = mclk->ufs_sel_clki;\n\tint ret = 0;\n\n\tret = clk_prepare_enable(clki->clk);\n\tif (ret) {\n\t\tdev_info(hba->dev,\n\t\t\t \"clk_prepare_enable() fail, ret: %d\\n\", ret);\n\t\treturn;\n\t}\n\n\tif (scale_up) {\n\t\tret = clk_set_parent(clki->clk, mclk->ufs_sel_max_clki->clk);\n\t\tclki->curr_freq = clki->max_freq;\n\t} else {\n\t\tret = clk_set_parent(clki->clk, mclk->ufs_sel_min_clki->clk);\n\t\tclki->curr_freq = clki->min_freq;\n\t}\n\n\tif (ret) {\n\t\tdev_info(hba->dev,\n\t\t\t \"Failed to set ufs_sel_clki, ret: %d\\n\", ret);\n\t}\n\n\tclk_disable_unprepare(clki->clk);\n\n\ttrace_ufs_mtk_clk_scale(clki->name, scale_up, clk_get_rate(clki->clk));\n}\n\nstatic int ufs_mtk_clk_scale_notify(struct ufs_hba *hba, bool scale_up,\n\t\t\t\t    enum ufs_notify_change_status status)\n{\n\tif (!ufshcd_is_clkscaling_supported(hba))\n\t\treturn 0;\n\n\tif (status == PRE_CHANGE) {\n\t\t \n\t\tufs_mtk_clk_scale(hba, scale_up);\n\t} else {\n\t\t \n\t\tufs_mtk_scale_perf(hba, scale_up);\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_get_hba_mac(struct ufs_hba *hba)\n{\n\treturn MAX_SUPP_MAC;\n}\n\nstatic int ufs_mtk_op_runtime_config(struct ufs_hba *hba)\n{\n\tstruct ufshcd_mcq_opr_info_t *opr;\n\tint i;\n\n\thba->mcq_opr[OPR_SQD].offset = REG_UFS_MTK_SQD;\n\thba->mcq_opr[OPR_SQIS].offset = REG_UFS_MTK_SQIS;\n\thba->mcq_opr[OPR_CQD].offset = REG_UFS_MTK_CQD;\n\thba->mcq_opr[OPR_CQIS].offset = REG_UFS_MTK_CQIS;\n\n\tfor (i = 0; i < OPR_MAX; i++) {\n\t\topr = &hba->mcq_opr[i];\n\t\topr->stride = REG_UFS_MCQ_STRIDE;\n\t\topr->base = hba->mmio_base + opr->offset;\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_mcq_config_resource(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\t \n\tif (!host->mcq_nr_intr) {\n\t\tdev_info(hba->dev, \"IRQs not ready. MCQ disabled.\");\n\t\treturn -EINVAL;\n\t}\n\n\thba->mcq_base = hba->mmio_base + MCQ_QUEUE_OFFSET(hba->mcq_capabilities);\n\treturn 0;\n}\n\nstatic irqreturn_t ufs_mtk_mcq_intr(int irq, void *__intr_info)\n{\n\tstruct ufs_mtk_mcq_intr_info *mcq_intr_info = __intr_info;\n\tstruct ufs_hba *hba = mcq_intr_info->hba;\n\tstruct ufs_hw_queue *hwq;\n\tu32 events;\n\tint qid = mcq_intr_info->qid;\n\n\thwq = &hba->uhq[qid];\n\n\tevents = ufshcd_mcq_read_cqis(hba, qid);\n\tif (events)\n\t\tufshcd_mcq_write_cqis(hba, events, qid);\n\n\tif (events & UFSHCD_MCQ_CQIS_TAIL_ENT_PUSH_STS)\n\t\tufshcd_mcq_poll_cqe_lock(hba, hwq);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int ufs_mtk_config_mcq_irq(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tu32 irq, i;\n\tint ret;\n\n\tfor (i = 0; i < host->mcq_nr_intr; i++) {\n\t\tirq = host->mcq_intr_info[i].irq;\n\t\tif (irq == MTK_MCQ_INVALID_IRQ) {\n\t\t\tdev_err(hba->dev, \"invalid irq. %d\\n\", i);\n\t\t\treturn -ENOPARAM;\n\t\t}\n\n\t\thost->mcq_intr_info[i].qid = i;\n\t\tret = devm_request_irq(hba->dev, irq, ufs_mtk_mcq_intr, 0, UFSHCD,\n\t\t\t\t       &host->mcq_intr_info[i]);\n\n\t\tdev_dbg(hba->dev, \"request irq %d intr %s\\n\", irq, ret ? \"failed\" : \"\");\n\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"Cannot request irq %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_config_mcq(struct ufs_hba *hba, bool irq)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tint ret = 0;\n\n\tif (!host->mcq_set_intr) {\n\t\t \n\t\tufshcd_rmwl(hba, MCQ_INTR_EN_MSK, 0, REG_UFS_MMIO_OPT_CTRL_0);\n\n\t\tif (irq) {\n\t\t\tret = ufs_mtk_config_mcq_irq(hba);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\thost->mcq_set_intr = true;\n\t}\n\n\tufshcd_rmwl(hba, MCQ_AH8, MCQ_AH8, REG_UFS_MMIO_OPT_CTRL_0);\n\tufshcd_rmwl(hba, MCQ_INTR_EN_MSK, MCQ_MULTI_INTR_EN, REG_UFS_MMIO_OPT_CTRL_0);\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_config_esi(struct ufs_hba *hba)\n{\n\treturn ufs_mtk_config_mcq(hba, true);\n}\n\n \nstatic const struct ufs_hba_variant_ops ufs_hba_mtk_vops = {\n\t.name                = \"mediatek.ufshci\",\n\t.init                = ufs_mtk_init,\n\t.get_ufs_hci_version = ufs_mtk_get_ufs_hci_version,\n\t.setup_clocks        = ufs_mtk_setup_clocks,\n\t.hce_enable_notify   = ufs_mtk_hce_enable_notify,\n\t.link_startup_notify = ufs_mtk_link_startup_notify,\n\t.pwr_change_notify   = ufs_mtk_pwr_change_notify,\n\t.apply_dev_quirks    = ufs_mtk_apply_dev_quirks,\n\t.fixup_dev_quirks    = ufs_mtk_fixup_dev_quirks,\n\t.suspend             = ufs_mtk_suspend,\n\t.resume              = ufs_mtk_resume,\n\t.dbg_register_dump   = ufs_mtk_dbg_register_dump,\n\t.device_reset        = ufs_mtk_device_reset,\n\t.event_notify        = ufs_mtk_event_notify,\n\t.config_scaling_param = ufs_mtk_config_scaling_param,\n\t.clk_scale_notify    = ufs_mtk_clk_scale_notify,\n\t \n\t.get_hba_mac         = ufs_mtk_get_hba_mac,\n\t.op_runtime_config   = ufs_mtk_op_runtime_config,\n\t.mcq_config_resource = ufs_mtk_mcq_config_resource,\n\t.config_esi          = ufs_mtk_config_esi,\n};\n\n \nstatic int ufs_mtk_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *reset_node;\n\tstruct platform_device *reset_pdev;\n\tstruct device_link *link;\n\n\treset_node = of_find_compatible_node(NULL, NULL,\n\t\t\t\t\t     \"ti,syscon-reset\");\n\tif (!reset_node) {\n\t\tdev_notice(dev, \"find ti,syscon-reset fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\treset_pdev = of_find_device_by_node(reset_node);\n\tif (!reset_pdev) {\n\t\tdev_notice(dev, \"find reset_pdev fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\tlink = device_link_add(dev, &reset_pdev->dev,\n\t\tDL_FLAG_AUTOPROBE_CONSUMER);\n\tput_device(&reset_pdev->dev);\n\tif (!link) {\n\t\tdev_notice(dev, \"add reset device_link fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\t \n\tif (link->status == DL_STATE_DORMANT) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto out;\n\t}\n\nskip_reset:\n\t \n\terr = ufshcd_pltfrm_init(pdev, &ufs_hba_mtk_vops);\n\nout:\n\tif (err)\n\t\tdev_err(dev, \"probe failed %d\\n\", err);\n\n\tof_node_put(reset_node);\n\treturn err;\n}\n\n \nstatic int ufs_mtk_remove(struct platform_device *pdev)\n{\n\tstruct ufs_hba *hba =  platform_get_drvdata(pdev);\n\n\tpm_runtime_get_sync(&(pdev)->dev);\n\tufshcd_remove(hba);\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int ufs_mtk_system_suspend(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret;\n\n\tret = ufshcd_system_suspend(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tufs_mtk_dev_vreg_set_lpm(hba, true);\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_system_resume(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\tufs_mtk_dev_vreg_set_lpm(hba, false);\n\n\treturn ufshcd_system_resume(dev);\n}\n#endif\n\n#ifdef CONFIG_PM\nstatic int ufs_mtk_runtime_suspend(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tret = ufshcd_runtime_suspend(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tufs_mtk_dev_vreg_set_lpm(hba, true);\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_runtime_resume(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\tufs_mtk_dev_vreg_set_lpm(hba, false);\n\n\treturn ufshcd_runtime_resume(dev);\n}\n#endif\n\nstatic const struct dev_pm_ops ufs_mtk_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(ufs_mtk_system_suspend,\n\t\t\t\tufs_mtk_system_resume)\n\tSET_RUNTIME_PM_OPS(ufs_mtk_runtime_suspend,\n\t\t\t   ufs_mtk_runtime_resume, NULL)\n\t.prepare\t = ufshcd_suspend_prepare,\n\t.complete\t = ufshcd_resume_complete,\n};\n\nstatic struct platform_driver ufs_mtk_pltform = {\n\t.probe      = ufs_mtk_probe,\n\t.remove     = ufs_mtk_remove,\n\t.driver = {\n\t\t.name   = \"ufshcd-mtk\",\n\t\t.pm     = &ufs_mtk_pm_ops,\n\t\t.of_match_table = ufs_mtk_of_match,\n\t},\n};\n\nMODULE_AUTHOR(\"Stanley Chu <stanley.chu@mediatek.com>\");\nMODULE_AUTHOR(\"Peter Wang <peter.wang@mediatek.com>\");\nMODULE_DESCRIPTION(\"MediaTek UFS Host Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\nmodule_platform_driver(ufs_mtk_pltform);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}