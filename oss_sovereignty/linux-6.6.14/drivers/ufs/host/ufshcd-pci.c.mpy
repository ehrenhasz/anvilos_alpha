{
  "module_name": "ufshcd-pci.c",
  "hash_id": "e5b2b959ea4914d5459fab88f13027d1a80e4c558cb01d1b86b01cb8a6847728",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ufs/host/ufshcd-pci.c",
  "human_readable_source": "\n \n\n#include <ufs/ufshcd.h>\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/pm_qos.h>\n#include <linux/debugfs.h>\n#include <linux/uuid.h>\n#include <linux/acpi.h>\n#include <linux/gpio/consumer.h>\n\nstruct ufs_host {\n\tvoid (*late_init)(struct ufs_hba *hba);\n};\n\nenum intel_ufs_dsm_func_id {\n\tINTEL_DSM_FNS\t\t=  0,\n\tINTEL_DSM_RESET\t\t=  1,\n};\n\nstruct intel_host {\n\tstruct ufs_host ufs_host;\n\tu32\t\tdsm_fns;\n\tu32\t\tactive_ltr;\n\tu32\t\tidle_ltr;\n\tstruct dentry\t*debugfs_root;\n\tstruct gpio_desc *reset_gpio;\n};\n\nstatic const guid_t intel_dsm_guid =\n\tGUID_INIT(0x1A4832A0, 0x7D03, 0x43CA,\n\t\t  0xB0, 0x20, 0xF6, 0xDC, 0xD1, 0x2A, 0x19, 0x50);\n\nstatic bool __intel_dsm_supported(struct intel_host *host,\n\t\t\t\t  enum intel_ufs_dsm_func_id fn)\n{\n\treturn fn < 32 && fn >= 0 && (host->dsm_fns & (1u << fn));\n}\n\n#define INTEL_DSM_SUPPORTED(host, name) \\\n\t__intel_dsm_supported(host, INTEL_DSM_##name)\n\nstatic int __intel_dsm(struct intel_host *intel_host, struct device *dev,\n\t\t       unsigned int fn, u32 *result)\n{\n\tunion acpi_object *obj;\n\tint err = 0;\n\tsize_t len;\n\n\tobj = acpi_evaluate_dsm(ACPI_HANDLE(dev), &intel_dsm_guid, 0, fn, NULL);\n\tif (!obj)\n\t\treturn -EOPNOTSUPP;\n\n\tif (obj->type != ACPI_TYPE_BUFFER || obj->buffer.length < 1) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlen = min_t(size_t, obj->buffer.length, 4);\n\n\t*result = 0;\n\tmemcpy(result, obj->buffer.pointer, len);\nout:\n\tACPI_FREE(obj);\n\n\treturn err;\n}\n\nstatic int intel_dsm(struct intel_host *intel_host, struct device *dev,\n\t\t     unsigned int fn, u32 *result)\n{\n\tif (!__intel_dsm_supported(intel_host, fn))\n\t\treturn -EOPNOTSUPP;\n\n\treturn __intel_dsm(intel_host, dev, fn, result);\n}\n\nstatic void intel_dsm_init(struct intel_host *intel_host, struct device *dev)\n{\n\tint err;\n\n\terr = __intel_dsm(intel_host, dev, INTEL_DSM_FNS, &intel_host->dsm_fns);\n\tdev_dbg(dev, \"DSM fns %#x, error %d\\n\", intel_host->dsm_fns, err);\n}\n\nstatic int ufs_intel_hce_enable_notify(struct ufs_hba *hba,\n\t\t\t\t       enum ufs_notify_change_status status)\n{\n\t \n\tif (status == POST_CHANGE && hba->caps & UFSHCD_CAP_CRYPTO) {\n\t\tu32 hce = ufshcd_readl(hba, REG_CONTROLLER_ENABLE);\n\n\t\thce |= CRYPTO_GENERAL_ENABLE;\n\t\tufshcd_writel(hba, hce, REG_CONTROLLER_ENABLE);\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_intel_disable_lcc(struct ufs_hba *hba)\n{\n\tu32 attr = UIC_ARG_MIB(PA_LOCAL_TX_LCC_ENABLE);\n\tu32 lcc_enable = 0;\n\n\tufshcd_dme_get(hba, attr, &lcc_enable);\n\tif (lcc_enable)\n\t\tufshcd_disable_host_tx_lcc(hba);\n\n\treturn 0;\n}\n\nstatic int ufs_intel_link_startup_notify(struct ufs_hba *hba,\n\t\t\t\t\t enum ufs_notify_change_status status)\n{\n\tint err = 0;\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\terr = ufs_intel_disable_lcc(hba);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int ufs_intel_set_lanes(struct ufs_hba *hba, u32 lanes)\n{\n\tstruct ufs_pa_layer_attr pwr_info = hba->pwr_info;\n\tint ret;\n\n\tpwr_info.lane_rx = lanes;\n\tpwr_info.lane_tx = lanes;\n\tret = ufshcd_config_pwr_mode(hba, &pwr_info);\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: Setting %u lanes, err = %d\\n\",\n\t\t\t__func__, lanes, ret);\n\treturn ret;\n}\n\nstatic int ufs_intel_lkf_pwr_change_notify(struct ufs_hba *hba,\n\t\t\t\tenum ufs_notify_change_status status,\n\t\t\t\tstruct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\tstruct ufs_pa_layer_attr *dev_req_params)\n{\n\tint err = 0;\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\tif (ufshcd_is_hs_mode(dev_max_params) &&\n\t\t    (hba->pwr_info.lane_rx != 2 || hba->pwr_info.lane_tx != 2))\n\t\t\tufs_intel_set_lanes(hba, 2);\n\t\tmemcpy(dev_req_params, dev_max_params, sizeof(*dev_req_params));\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tif (ufshcd_is_hs_mode(dev_req_params)) {\n\t\t\tu32 peer_granularity;\n\n\t\t\tusleep_range(1000, 1250);\n\t\t\terr = ufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_GRANULARITY),\n\t\t\t\t\t\t  &peer_granularity);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int ufs_intel_lkf_apply_dev_quirks(struct ufs_hba *hba)\n{\n\tu32 granularity, peer_granularity;\n\tu32 pa_tactivate, peer_pa_tactivate;\n\tint ret;\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_GRANULARITY), &granularity);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_GRANULARITY), &peer_granularity);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_TACTIVATE), &pa_tactivate);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_TACTIVATE), &peer_pa_tactivate);\n\tif (ret)\n\t\tgoto out;\n\n\tif (granularity == peer_granularity) {\n\t\tu32 new_peer_pa_tactivate = pa_tactivate + 2;\n\n\t\tret = ufshcd_dme_peer_set(hba, UIC_ARG_MIB(PA_TACTIVATE), new_peer_pa_tactivate);\n\t}\nout:\n\treturn ret;\n}\n\n#define INTEL_ACTIVELTR\t\t0x804\n#define INTEL_IDLELTR\t\t0x808\n\n#define INTEL_LTR_REQ\t\tBIT(15)\n#define INTEL_LTR_SCALE_MASK\tGENMASK(11, 10)\n#define INTEL_LTR_SCALE_1US\t(2 << 10)\n#define INTEL_LTR_SCALE_32US\t(3 << 10)\n#define INTEL_LTR_VALUE_MASK\tGENMASK(9, 0)\n\nstatic void intel_cache_ltr(struct ufs_hba *hba)\n{\n\tstruct intel_host *host = ufshcd_get_variant(hba);\n\n\thost->active_ltr = readl(hba->mmio_base + INTEL_ACTIVELTR);\n\thost->idle_ltr = readl(hba->mmio_base + INTEL_IDLELTR);\n}\n\nstatic void intel_ltr_set(struct device *dev, s32 val)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tstruct intel_host *host = ufshcd_get_variant(hba);\n\tu32 ltr;\n\n\tpm_runtime_get_sync(dev);\n\n\t \n\tltr = readl(hba->mmio_base + INTEL_ACTIVELTR);\n\n\tif (val == PM_QOS_LATENCY_ANY || val < 0) {\n\t\tltr &= ~INTEL_LTR_REQ;\n\t} else {\n\t\tltr |= INTEL_LTR_REQ;\n\t\tltr &= ~INTEL_LTR_SCALE_MASK;\n\t\tltr &= ~INTEL_LTR_VALUE_MASK;\n\n\t\tif (val > INTEL_LTR_VALUE_MASK) {\n\t\t\tval >>= 5;\n\t\t\tif (val > INTEL_LTR_VALUE_MASK)\n\t\t\t\tval = INTEL_LTR_VALUE_MASK;\n\t\t\tltr |= INTEL_LTR_SCALE_32US | val;\n\t\t} else {\n\t\t\tltr |= INTEL_LTR_SCALE_1US | val;\n\t\t}\n\t}\n\n\tif (ltr == host->active_ltr)\n\t\tgoto out;\n\n\twritel(ltr, hba->mmio_base + INTEL_ACTIVELTR);\n\twritel(ltr, hba->mmio_base + INTEL_IDLELTR);\n\n\t \n\tintel_cache_ltr(hba);\nout:\n\tpm_runtime_put(dev);\n}\n\nstatic void intel_ltr_expose(struct device *dev)\n{\n\tdev->power.set_latency_tolerance = intel_ltr_set;\n\tdev_pm_qos_expose_latency_tolerance(dev);\n}\n\nstatic void intel_ltr_hide(struct device *dev)\n{\n\tdev_pm_qos_hide_latency_tolerance(dev);\n\tdev->power.set_latency_tolerance = NULL;\n}\n\nstatic void intel_add_debugfs(struct ufs_hba *hba)\n{\n\tstruct dentry *dir = debugfs_create_dir(dev_name(hba->dev), NULL);\n\tstruct intel_host *host = ufshcd_get_variant(hba);\n\n\tintel_cache_ltr(hba);\n\n\thost->debugfs_root = dir;\n\tdebugfs_create_x32(\"active_ltr\", 0444, dir, &host->active_ltr);\n\tdebugfs_create_x32(\"idle_ltr\", 0444, dir, &host->idle_ltr);\n}\n\nstatic void intel_remove_debugfs(struct ufs_hba *hba)\n{\n\tstruct intel_host *host = ufshcd_get_variant(hba);\n\n\tdebugfs_remove_recursive(host->debugfs_root);\n}\n\nstatic int ufs_intel_device_reset(struct ufs_hba *hba)\n{\n\tstruct intel_host *host = ufshcd_get_variant(hba);\n\n\tif (INTEL_DSM_SUPPORTED(host, RESET)) {\n\t\tu32 result = 0;\n\t\tint err;\n\n\t\terr = intel_dsm(host, hba->dev, INTEL_DSM_RESET, &result);\n\t\tif (!err && !result)\n\t\t\terr = -EIO;\n\t\tif (err)\n\t\t\tdev_err(hba->dev, \"%s: DSM error %d result %u\\n\",\n\t\t\t\t__func__, err, result);\n\t\treturn err;\n\t}\n\n\tif (!host->reset_gpio)\n\t\treturn -EOPNOTSUPP;\n\n\tgpiod_set_value_cansleep(host->reset_gpio, 1);\n\tusleep_range(10, 15);\n\n\tgpiod_set_value_cansleep(host->reset_gpio, 0);\n\tusleep_range(10, 15);\n\n\treturn 0;\n}\n\nstatic struct gpio_desc *ufs_intel_get_reset_gpio(struct device *dev)\n{\n\t \n\treturn devm_gpiod_get_optional(dev, \"reset\", GPIOD_OUT_LOW);\n}\n\nstatic int ufs_intel_common_init(struct ufs_hba *hba)\n{\n\tstruct intel_host *host;\n\n\thba->caps |= UFSHCD_CAP_RPM_AUTOSUSPEND;\n\n\thost = devm_kzalloc(hba->dev, sizeof(*host), GFP_KERNEL);\n\tif (!host)\n\t\treturn -ENOMEM;\n\tufshcd_set_variant(hba, host);\n\tintel_dsm_init(host, hba->dev);\n\tif (INTEL_DSM_SUPPORTED(host, RESET)) {\n\t\tif (hba->vops->device_reset)\n\t\t\thba->caps |= UFSHCD_CAP_DEEPSLEEP;\n\t} else {\n\t\tif (hba->vops->device_reset)\n\t\t\thost->reset_gpio = ufs_intel_get_reset_gpio(hba->dev);\n\t\tif (IS_ERR(host->reset_gpio)) {\n\t\t\tdev_err(hba->dev, \"%s: failed to get reset GPIO, error %ld\\n\",\n\t\t\t\t__func__, PTR_ERR(host->reset_gpio));\n\t\t\thost->reset_gpio = NULL;\n\t\t}\n\t\tif (host->reset_gpio) {\n\t\t\tgpiod_set_value_cansleep(host->reset_gpio, 0);\n\t\t\thba->caps |= UFSHCD_CAP_DEEPSLEEP;\n\t\t}\n\t}\n\tintel_ltr_expose(hba->dev);\n\tintel_add_debugfs(hba);\n\treturn 0;\n}\n\nstatic void ufs_intel_common_exit(struct ufs_hba *hba)\n{\n\tintel_remove_debugfs(hba);\n\tintel_ltr_hide(hba->dev);\n}\n\nstatic int ufs_intel_resume(struct ufs_hba *hba, enum ufs_pm_op op)\n{\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\tint ret = ufshcd_uic_hibern8_exit(hba);\n\n\t\tif (!ret) {\n\t\t\tufshcd_set_link_active(hba);\n\t\t} else {\n\t\t\tdev_err(hba->dev, \"%s: hibern8 exit failed %d\\n\",\n\t\t\t\t__func__, ret);\n\t\t\t \n\t\t\tufshcd_set_link_off(hba);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_intel_ehl_init(struct ufs_hba *hba)\n{\n\thba->quirks |= UFSHCD_QUIRK_BROKEN_AUTO_HIBERN8;\n\treturn ufs_intel_common_init(hba);\n}\n\nstatic void ufs_intel_lkf_late_init(struct ufs_hba *hba)\n{\n\t \n\tif (hba->caps & UFSHCD_CAP_DEEPSLEEP) {\n\t\thba->spm_lvl = UFS_PM_LVL_6;\n\t\thba->rpm_lvl = UFS_PM_LVL_6;\n\t} else {\n\t\thba->spm_lvl = UFS_PM_LVL_5;\n\t\thba->rpm_lvl = UFS_PM_LVL_5;\n\t}\n}\n\nstatic int ufs_intel_lkf_init(struct ufs_hba *hba)\n{\n\tstruct ufs_host *ufs_host;\n\tint err;\n\n\thba->nop_out_timeout = 200;\n\thba->quirks |= UFSHCD_QUIRK_BROKEN_AUTO_HIBERN8;\n\thba->caps |= UFSHCD_CAP_CRYPTO;\n\terr = ufs_intel_common_init(hba);\n\tufs_host = ufshcd_get_variant(hba);\n\tufs_host->late_init = ufs_intel_lkf_late_init;\n\treturn err;\n}\n\nstatic int ufs_intel_adl_init(struct ufs_hba *hba)\n{\n\thba->nop_out_timeout = 200;\n\thba->quirks |= UFSHCD_QUIRK_BROKEN_AUTO_HIBERN8;\n\thba->caps |= UFSHCD_CAP_WB_EN;\n\treturn ufs_intel_common_init(hba);\n}\n\nstatic int ufs_intel_mtl_init(struct ufs_hba *hba)\n{\n\thba->caps |= UFSHCD_CAP_CRYPTO | UFSHCD_CAP_WB_EN;\n\treturn ufs_intel_common_init(hba);\n}\n\nstatic struct ufs_hba_variant_ops ufs_intel_cnl_hba_vops = {\n\t.name                   = \"intel-pci\",\n\t.init\t\t\t= ufs_intel_common_init,\n\t.exit\t\t\t= ufs_intel_common_exit,\n\t.link_startup_notify\t= ufs_intel_link_startup_notify,\n\t.resume\t\t\t= ufs_intel_resume,\n};\n\nstatic struct ufs_hba_variant_ops ufs_intel_ehl_hba_vops = {\n\t.name                   = \"intel-pci\",\n\t.init\t\t\t= ufs_intel_ehl_init,\n\t.exit\t\t\t= ufs_intel_common_exit,\n\t.link_startup_notify\t= ufs_intel_link_startup_notify,\n\t.resume\t\t\t= ufs_intel_resume,\n};\n\nstatic struct ufs_hba_variant_ops ufs_intel_lkf_hba_vops = {\n\t.name                   = \"intel-pci\",\n\t.init\t\t\t= ufs_intel_lkf_init,\n\t.exit\t\t\t= ufs_intel_common_exit,\n\t.hce_enable_notify\t= ufs_intel_hce_enable_notify,\n\t.link_startup_notify\t= ufs_intel_link_startup_notify,\n\t.pwr_change_notify\t= ufs_intel_lkf_pwr_change_notify,\n\t.apply_dev_quirks\t= ufs_intel_lkf_apply_dev_quirks,\n\t.resume\t\t\t= ufs_intel_resume,\n\t.device_reset\t\t= ufs_intel_device_reset,\n};\n\nstatic struct ufs_hba_variant_ops ufs_intel_adl_hba_vops = {\n\t.name\t\t\t= \"intel-pci\",\n\t.init\t\t\t= ufs_intel_adl_init,\n\t.exit\t\t\t= ufs_intel_common_exit,\n\t.link_startup_notify\t= ufs_intel_link_startup_notify,\n\t.resume\t\t\t= ufs_intel_resume,\n\t.device_reset\t\t= ufs_intel_device_reset,\n};\n\nstatic struct ufs_hba_variant_ops ufs_intel_mtl_hba_vops = {\n\t.name                   = \"intel-pci\",\n\t.init\t\t\t= ufs_intel_mtl_init,\n\t.exit\t\t\t= ufs_intel_common_exit,\n\t.hce_enable_notify\t= ufs_intel_hce_enable_notify,\n\t.link_startup_notify\t= ufs_intel_link_startup_notify,\n\t.resume\t\t\t= ufs_intel_resume,\n\t.device_reset\t\t= ufs_intel_device_reset,\n};\n\n#ifdef CONFIG_PM_SLEEP\nstatic int ufshcd_pci_restore(struct device *dev)\n{\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\t \n\tufshcd_set_link_off(hba);\n\n\treturn ufshcd_system_resume(dev);\n}\n#endif\n\n \nstatic void ufshcd_pci_remove(struct pci_dev *pdev)\n{\n\tstruct ufs_hba *hba = pci_get_drvdata(pdev);\n\n\tpm_runtime_forbid(&pdev->dev);\n\tpm_runtime_get_noresume(&pdev->dev);\n\tufshcd_remove(hba);\n\tufshcd_dealloc_host(hba);\n}\n\n \nstatic int\nufshcd_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct ufs_host *ufs_host;\n\tstruct ufs_hba *hba;\n\tvoid __iomem *mmio_base;\n\tint err;\n\n\terr = pcim_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"pcim_enable_device failed\\n\");\n\t\treturn err;\n\t}\n\n\tpci_set_master(pdev);\n\n\terr = pcim_iomap_regions(pdev, 1 << 0, UFSHCD);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"request and iomap failed\\n\");\n\t\treturn err;\n\t}\n\n\tmmio_base = pcim_iomap_table(pdev)[0];\n\n\terr = ufshcd_alloc_host(&pdev->dev, &hba);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Allocation failed\\n\");\n\t\treturn err;\n\t}\n\n\thba->vops = (struct ufs_hba_variant_ops *)id->driver_data;\n\n\terr = ufshcd_init(hba, mmio_base, pdev->irq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Initialization failed\\n\");\n\t\tufshcd_dealloc_host(hba);\n\t\treturn err;\n\t}\n\n\tufs_host = ufshcd_get_variant(hba);\n\tif (ufs_host && ufs_host->late_init)\n\t\tufs_host->late_init(hba);\n\n\tpm_runtime_put_noidle(&pdev->dev);\n\tpm_runtime_allow(&pdev->dev);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops ufshcd_pci_pm_ops = {\n\tSET_RUNTIME_PM_OPS(ufshcd_runtime_suspend, ufshcd_runtime_resume, NULL)\n#ifdef CONFIG_PM_SLEEP\n\t.suspend\t= ufshcd_system_suspend,\n\t.resume\t\t= ufshcd_system_resume,\n\t.freeze\t\t= ufshcd_system_suspend,\n\t.thaw\t\t= ufshcd_system_resume,\n\t.poweroff\t= ufshcd_system_suspend,\n\t.restore\t= ufshcd_pci_restore,\n\t.prepare\t= ufshcd_suspend_prepare,\n\t.complete\t= ufshcd_resume_complete,\n#endif\n};\n\nstatic const struct pci_device_id ufshcd_pci_tbl[] = {\n\t{ PCI_VENDOR_ID_REDHAT, 0x0013, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0 },\n\t{ PCI_VENDOR_ID_SAMSUNG, 0xC00C, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0 },\n\t{ PCI_VDEVICE(INTEL, 0x9DFA), (kernel_ulong_t)&ufs_intel_cnl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x4B41), (kernel_ulong_t)&ufs_intel_ehl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x4B43), (kernel_ulong_t)&ufs_intel_ehl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x98FA), (kernel_ulong_t)&ufs_intel_lkf_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x51FF), (kernel_ulong_t)&ufs_intel_adl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x54FF), (kernel_ulong_t)&ufs_intel_adl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x7E47), (kernel_ulong_t)&ufs_intel_mtl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0xA847), (kernel_ulong_t)&ufs_intel_mtl_hba_vops },\n\t{ PCI_VDEVICE(INTEL, 0x7747), (kernel_ulong_t)&ufs_intel_mtl_hba_vops },\n\t{ }\t \n};\n\nMODULE_DEVICE_TABLE(pci, ufshcd_pci_tbl);\n\nstatic struct pci_driver ufshcd_pci_driver = {\n\t.name = UFSHCD,\n\t.id_table = ufshcd_pci_tbl,\n\t.probe = ufshcd_pci_probe,\n\t.remove = ufshcd_pci_remove,\n\t.driver = {\n\t\t.pm = &ufshcd_pci_pm_ops\n\t},\n};\n\nmodule_pci_driver(ufshcd_pci_driver);\n\nMODULE_AUTHOR(\"Santosh Yaragnavi <santosh.sy@samsung.com>\");\nMODULE_AUTHOR(\"Vinayak Holikatti <h.vinayak@samsung.com>\");\nMODULE_DESCRIPTION(\"UFS host controller PCI glue driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}