{
  "module_name": "ufs-qcom.c",
  "hash_id": "634603d19a51017f5ccd4d27147df6f05119f4627d9e011cad016da4a6a29368",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ufs/host/ufs-qcom.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/time.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/interconnect.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/phy/phy.h>\n#include <linux/gpio/consumer.h>\n#include <linux/reset-controller.h>\n#include <linux/devfreq.h>\n\n#include <soc/qcom/ice.h>\n\n#include <ufs/ufshcd.h>\n#include \"ufshcd-pltfrm.h\"\n#include <ufs/unipro.h>\n#include \"ufs-qcom.h\"\n#include <ufs/ufshci.h>\n#include <ufs/ufs_quirks.h>\n\n#define MCQ_QCFGPTR_MASK\tGENMASK(7, 0)\n#define MCQ_QCFGPTR_UNIT\t0x200\n#define MCQ_SQATTR_OFFSET(c) \\\n\t((((c) >> 16) & MCQ_QCFGPTR_MASK) * MCQ_QCFGPTR_UNIT)\n#define MCQ_QCFG_SIZE\t0x40\n\nenum {\n\tTSTBUS_UAWM,\n\tTSTBUS_UARM,\n\tTSTBUS_TXUC,\n\tTSTBUS_RXUC,\n\tTSTBUS_DFC,\n\tTSTBUS_TRLUT,\n\tTSTBUS_TMRLUT,\n\tTSTBUS_OCSC,\n\tTSTBUS_UTP_HCI,\n\tTSTBUS_COMBINED,\n\tTSTBUS_WRAPPER,\n\tTSTBUS_UNIPRO,\n\tTSTBUS_MAX,\n};\n\n#define QCOM_UFS_MAX_GEAR 4\n#define QCOM_UFS_MAX_LANE 2\n\nenum {\n\tMODE_MIN,\n\tMODE_PWM,\n\tMODE_HS_RA,\n\tMODE_HS_RB,\n\tMODE_MAX,\n};\n\nstatic const struct __ufs_qcom_bw_table {\n\tu32 mem_bw;\n\tu32 cfg_bw;\n} ufs_qcom_bw_table[MODE_MAX + 1][QCOM_UFS_MAX_GEAR + 1][QCOM_UFS_MAX_LANE + 1] = {\n\t[MODE_MIN][0][0]\t\t   = { 0,\t\t0 },  \n\t[MODE_PWM][UFS_PWM_G1][UFS_LANE_1] = { 922,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G2][UFS_LANE_1] = { 1844,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G3][UFS_LANE_1] = { 3688,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G4][UFS_LANE_1] = { 7376,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G1][UFS_LANE_2] = { 1844,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G2][UFS_LANE_2] = { 3688,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G3][UFS_LANE_2] = { 7376,\t\t1000 },\n\t[MODE_PWM][UFS_PWM_G4][UFS_LANE_2] = { 14752,\t\t1000 },\n\t[MODE_HS_RA][UFS_HS_G1][UFS_LANE_1] = { 127796,\t\t1000 },\n\t[MODE_HS_RA][UFS_HS_G2][UFS_LANE_1] = { 255591,\t\t1000 },\n\t[MODE_HS_RA][UFS_HS_G3][UFS_LANE_1] = { 1492582,\t102400 },\n\t[MODE_HS_RA][UFS_HS_G4][UFS_LANE_1] = { 2915200,\t204800 },\n\t[MODE_HS_RA][UFS_HS_G1][UFS_LANE_2] = { 255591,\t\t1000 },\n\t[MODE_HS_RA][UFS_HS_G2][UFS_LANE_2] = { 511181,\t\t1000 },\n\t[MODE_HS_RA][UFS_HS_G3][UFS_LANE_2] = { 1492582,\t204800 },\n\t[MODE_HS_RA][UFS_HS_G4][UFS_LANE_2] = { 2915200,\t409600 },\n\t[MODE_HS_RB][UFS_HS_G1][UFS_LANE_1] = { 149422,\t\t1000 },\n\t[MODE_HS_RB][UFS_HS_G2][UFS_LANE_1] = { 298189,\t\t1000 },\n\t[MODE_HS_RB][UFS_HS_G3][UFS_LANE_1] = { 1492582,\t102400 },\n\t[MODE_HS_RB][UFS_HS_G4][UFS_LANE_1] = { 2915200,\t204800 },\n\t[MODE_HS_RB][UFS_HS_G1][UFS_LANE_2] = { 298189,\t\t1000 },\n\t[MODE_HS_RB][UFS_HS_G2][UFS_LANE_2] = { 596378,\t\t1000 },\n\t[MODE_HS_RB][UFS_HS_G3][UFS_LANE_2] = { 1492582,\t204800 },\n\t[MODE_HS_RB][UFS_HS_G4][UFS_LANE_2] = { 2915200,\t409600 },\n\t[MODE_MAX][0][0]\t\t    = { 7643136,\t307200 },\n};\n\nstatic struct ufs_qcom_host *ufs_qcom_hosts[MAX_UFS_QCOM_HOSTS];\n\nstatic void ufs_qcom_get_default_testbus_cfg(struct ufs_qcom_host *host);\nstatic int ufs_qcom_set_dme_vs_core_clk_ctrl_clear_div(struct ufs_hba *hba,\n\t\t\t\t\t\t       u32 clk_cycles);\n\nstatic struct ufs_qcom_host *rcdev_to_ufs_host(struct reset_controller_dev *rcd)\n{\n\treturn container_of(rcd, struct ufs_qcom_host, rcdev);\n}\n\n#ifdef CONFIG_SCSI_UFS_CRYPTO\n\nstatic inline void ufs_qcom_ice_enable(struct ufs_qcom_host *host)\n{\n\tif (host->hba->caps & UFSHCD_CAP_CRYPTO)\n\t\tqcom_ice_enable(host->ice);\n}\n\nstatic int ufs_qcom_ice_init(struct ufs_qcom_host *host)\n{\n\tstruct ufs_hba *hba = host->hba;\n\tstruct device *dev = hba->dev;\n\tstruct qcom_ice *ice;\n\n\tice = of_qcom_ice_get(dev);\n\tif (ice == ERR_PTR(-EOPNOTSUPP)) {\n\t\tdev_warn(dev, \"Disabling inline encryption support\\n\");\n\t\tice = NULL;\n\t}\n\n\tif (IS_ERR_OR_NULL(ice))\n\t\treturn PTR_ERR_OR_ZERO(ice);\n\n\thost->ice = ice;\n\thba->caps |= UFSHCD_CAP_CRYPTO;\n\n\treturn 0;\n}\n\nstatic inline int ufs_qcom_ice_resume(struct ufs_qcom_host *host)\n{\n\tif (host->hba->caps & UFSHCD_CAP_CRYPTO)\n\t\treturn qcom_ice_resume(host->ice);\n\n\treturn 0;\n}\n\nstatic inline int ufs_qcom_ice_suspend(struct ufs_qcom_host *host)\n{\n\tif (host->hba->caps & UFSHCD_CAP_CRYPTO)\n\t\treturn qcom_ice_suspend(host->ice);\n\n\treturn 0;\n}\n\nstatic int ufs_qcom_ice_program_key(struct ufs_hba *hba,\n\t\t\t\t    const union ufs_crypto_cfg_entry *cfg,\n\t\t\t\t    int slot)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tunion ufs_crypto_cap_entry cap;\n\tbool config_enable =\n\t\tcfg->config_enable & UFS_CRYPTO_CONFIGURATION_ENABLE;\n\n\t \n\tcap = hba->crypto_cap_array[cfg->crypto_cap_idx];\n\tif (cap.algorithm_id != UFS_CRYPTO_ALG_AES_XTS ||\n\t    cap.key_size != UFS_CRYPTO_KEY_SIZE_256)\n\t\treturn -EOPNOTSUPP;\n\n\tif (config_enable)\n\t\treturn qcom_ice_program_key(host->ice,\n\t\t\t\t\t    QCOM_ICE_CRYPTO_ALG_AES_XTS,\n\t\t\t\t\t    QCOM_ICE_CRYPTO_KEY_SIZE_256,\n\t\t\t\t\t    cfg->crypto_key,\n\t\t\t\t\t    cfg->data_unit_size, slot);\n\telse\n\t\treturn qcom_ice_evict_key(host->ice, slot);\n}\n\n#else\n\n#define ufs_qcom_ice_program_key NULL\n\nstatic inline void ufs_qcom_ice_enable(struct ufs_qcom_host *host)\n{\n}\n\nstatic int ufs_qcom_ice_init(struct ufs_qcom_host *host)\n{\n\treturn 0;\n}\n\nstatic inline int ufs_qcom_ice_resume(struct ufs_qcom_host *host)\n{\n\treturn 0;\n}\n\nstatic inline int ufs_qcom_ice_suspend(struct ufs_qcom_host *host)\n{\n\treturn 0;\n}\n#endif\n\nstatic int ufs_qcom_host_clk_get(struct device *dev,\n\t\tconst char *name, struct clk **clk_out, bool optional)\n{\n\tstruct clk *clk;\n\tint err = 0;\n\n\tclk = devm_clk_get(dev, name);\n\tif (!IS_ERR(clk)) {\n\t\t*clk_out = clk;\n\t\treturn 0;\n\t}\n\n\terr = PTR_ERR(clk);\n\n\tif (optional && err == -ENOENT) {\n\t\t*clk_out = NULL;\n\t\treturn 0;\n\t}\n\n\tif (err != -EPROBE_DEFER)\n\t\tdev_err(dev, \"failed to get %s err %d\\n\", name, err);\n\n\treturn err;\n}\n\nstatic int ufs_qcom_host_clk_enable(struct device *dev,\n\t\tconst char *name, struct clk *clk)\n{\n\tint err = 0;\n\n\terr = clk_prepare_enable(clk);\n\tif (err)\n\t\tdev_err(dev, \"%s: %s enable failed %d\\n\", __func__, name, err);\n\n\treturn err;\n}\n\nstatic void ufs_qcom_disable_lane_clks(struct ufs_qcom_host *host)\n{\n\tif (!host->is_lane_clks_enabled)\n\t\treturn;\n\n\tclk_disable_unprepare(host->tx_l1_sync_clk);\n\tclk_disable_unprepare(host->tx_l0_sync_clk);\n\tclk_disable_unprepare(host->rx_l1_sync_clk);\n\tclk_disable_unprepare(host->rx_l0_sync_clk);\n\n\thost->is_lane_clks_enabled = false;\n}\n\nstatic int ufs_qcom_enable_lane_clks(struct ufs_qcom_host *host)\n{\n\tint err;\n\tstruct device *dev = host->hba->dev;\n\n\tif (host->is_lane_clks_enabled)\n\t\treturn 0;\n\n\terr = ufs_qcom_host_clk_enable(dev, \"rx_lane0_sync_clk\",\n\t\thost->rx_l0_sync_clk);\n\tif (err)\n\t\treturn err;\n\n\terr = ufs_qcom_host_clk_enable(dev, \"tx_lane0_sync_clk\",\n\t\thost->tx_l0_sync_clk);\n\tif (err)\n\t\tgoto disable_rx_l0;\n\n\terr = ufs_qcom_host_clk_enable(dev, \"rx_lane1_sync_clk\",\n\t\t\thost->rx_l1_sync_clk);\n\tif (err)\n\t\tgoto disable_tx_l0;\n\n\terr = ufs_qcom_host_clk_enable(dev, \"tx_lane1_sync_clk\",\n\t\t\thost->tx_l1_sync_clk);\n\tif (err)\n\t\tgoto disable_rx_l1;\n\n\thost->is_lane_clks_enabled = true;\n\n\treturn 0;\n\ndisable_rx_l1:\n\tclk_disable_unprepare(host->rx_l1_sync_clk);\ndisable_tx_l0:\n\tclk_disable_unprepare(host->tx_l0_sync_clk);\ndisable_rx_l0:\n\tclk_disable_unprepare(host->rx_l0_sync_clk);\n\n\treturn err;\n}\n\nstatic int ufs_qcom_init_lane_clks(struct ufs_qcom_host *host)\n{\n\tint err = 0;\n\tstruct device *dev = host->hba->dev;\n\n\tif (has_acpi_companion(dev))\n\t\treturn 0;\n\n\terr = ufs_qcom_host_clk_get(dev, \"rx_lane0_sync_clk\",\n\t\t\t\t\t&host->rx_l0_sync_clk, false);\n\tif (err)\n\t\treturn err;\n\n\terr = ufs_qcom_host_clk_get(dev, \"tx_lane0_sync_clk\",\n\t\t\t\t\t&host->tx_l0_sync_clk, false);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (host->hba->lanes_per_direction > 1) {\n\t\terr = ufs_qcom_host_clk_get(dev, \"rx_lane1_sync_clk\",\n\t\t\t&host->rx_l1_sync_clk, false);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = ufs_qcom_host_clk_get(dev, \"tx_lane1_sync_clk\",\n\t\t\t&host->tx_l1_sync_clk, true);\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_qcom_check_hibern8(struct ufs_hba *hba)\n{\n\tint err;\n\tu32 tx_fsm_val = 0;\n\tunsigned long timeout = jiffies + msecs_to_jiffies(HBRN8_POLL_TOUT_MS);\n\n\tdo {\n\t\terr = ufshcd_dme_get(hba,\n\t\t\t\tUIC_ARG_MIB_SEL(MPHY_TX_FSM_STATE,\n\t\t\t\t\tUIC_ARG_MPHY_TX_GEN_SEL_INDEX(0)),\n\t\t\t\t&tx_fsm_val);\n\t\tif (err || tx_fsm_val == TX_FSM_HIBERN8)\n\t\t\tbreak;\n\n\t\t \n\t\tusleep_range(100, 200);\n\t} while (time_before(jiffies, timeout));\n\n\t \n\tif (time_after(jiffies, timeout))\n\t\terr = ufshcd_dme_get(hba,\n\t\t\t\tUIC_ARG_MIB_SEL(MPHY_TX_FSM_STATE,\n\t\t\t\t\tUIC_ARG_MPHY_TX_GEN_SEL_INDEX(0)),\n\t\t\t\t&tx_fsm_val);\n\n\tif (err) {\n\t\tdev_err(hba->dev, \"%s: unable to get TX_FSM_STATE, err %d\\n\",\n\t\t\t\t__func__, err);\n\t} else if (tx_fsm_val != TX_FSM_HIBERN8) {\n\t\terr = tx_fsm_val;\n\t\tdev_err(hba->dev, \"%s: invalid TX_FSM_STATE = %d\\n\",\n\t\t\t\t__func__, err);\n\t}\n\n\treturn err;\n}\n\nstatic void ufs_qcom_select_unipro_mode(struct ufs_qcom_host *host)\n{\n\tufshcd_rmwl(host->hba, QUNIPRO_SEL,\n\t\t   ufs_qcom_cap_qunipro(host) ? QUNIPRO_SEL : 0,\n\t\t   REG_UFS_CFG1);\n\n\tif (host->hw_ver.major >= 0x05)\n\t\tufshcd_rmwl(host->hba, QUNIPRO_G4_SEL, 0, REG_UFS_CFG0);\n\n\t \n\tmb();\n}\n\n \nstatic int ufs_qcom_host_reset(struct ufs_hba *hba)\n{\n\tint ret = 0;\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tbool reenable_intr = false;\n\n\tif (!host->core_reset) {\n\t\tdev_warn(hba->dev, \"%s: reset control not set\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\treenable_intr = hba->is_irq_enabled;\n\tdisable_irq(hba->irq);\n\thba->is_irq_enabled = false;\n\n\tret = reset_control_assert(host->core_reset);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: core_reset assert failed, err = %d\\n\",\n\t\t\t\t __func__, ret);\n\t\treturn ret;\n\t}\n\n\t \n\tusleep_range(200, 210);\n\n\tret = reset_control_deassert(host->core_reset);\n\tif (ret)\n\t\tdev_err(hba->dev, \"%s: core_reset deassert failed, err = %d\\n\",\n\t\t\t\t __func__, ret);\n\n\tusleep_range(1000, 1100);\n\n\tif (reenable_intr) {\n\t\tenable_irq(hba->irq);\n\t\thba->is_irq_enabled = true;\n\t}\n\n\treturn 0;\n}\n\nstatic u32 ufs_qcom_get_hs_gear(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tif (host->hw_ver.major == 0x1) {\n\t\t \n\t\treturn UFS_HS_G2;\n\t} else if (host->hw_ver.major >= 0x4) {\n\t\treturn UFS_QCOM_MAX_GEAR(ufshcd_readl(hba, REG_UFS_PARAM0));\n\t}\n\n\t \n\treturn UFS_HS_G3;\n}\n\nstatic int ufs_qcom_power_up_sequence(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct phy *phy = host->generic_phy;\n\tint ret;\n\n\t \n\tret = ufs_qcom_host_reset(hba);\n\tif (ret)\n\t\tdev_warn(hba->dev, \"%s: host reset returned %d\\n\",\n\t\t\t\t  __func__, ret);\n\n\t \n\tret = phy_init(phy);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: phy init failed, ret = %d\\n\",\n\t\t\t__func__, ret);\n\t\treturn ret;\n\t}\n\n\tphy_set_mode_ext(phy, PHY_MODE_UFS_HS_B, host->hs_gear);\n\n\t \n\tret = phy_power_on(phy);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"%s: phy power on failed, ret = %d\\n\",\n\t\t\t__func__, ret);\n\t\tgoto out_disable_phy;\n\t}\n\n\tufs_qcom_select_unipro_mode(host);\n\n\treturn 0;\n\nout_disable_phy:\n\tphy_exit(phy);\n\n\treturn ret;\n}\n\n \nstatic void ufs_qcom_enable_hw_clk_gating(struct ufs_hba *hba)\n{\n\tufshcd_writel(hba,\n\t\tufshcd_readl(hba, REG_UFS_CFG2) | REG_UFS_CFG2_CGC_EN_ALL,\n\t\tREG_UFS_CFG2);\n\n\t \n\tmb();\n}\n\nstatic int ufs_qcom_hce_enable_notify(struct ufs_hba *hba,\n\t\t\t\t      enum ufs_notify_change_status status)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tint err = 0;\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\tufs_qcom_power_up_sequence(hba);\n\t\t \n\t\terr = ufs_qcom_enable_lane_clks(host);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\t \n\t\terr = ufs_qcom_check_hibern8(hba);\n\t\tufs_qcom_enable_hw_clk_gating(hba);\n\t\tufs_qcom_ice_enable(host);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hba->dev, \"%s: invalid status %d\\n\", __func__, status);\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n \nstatic int ufs_qcom_cfg_timers(struct ufs_hba *hba, u32 gear,\n\t\t\t       u32 hs, u32 rate, bool update_link_startup_timer)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_clk_info *clki;\n\tu32 core_clk_period_in_ns;\n\tu32 tx_clk_cycles_per_us = 0;\n\tunsigned long core_clk_rate = 0;\n\tu32 core_clk_cycles_per_us = 0;\n\n\tstatic u32 pwm_fr_table[][2] = {\n\t\t{UFS_PWM_G1, 0x1},\n\t\t{UFS_PWM_G2, 0x1},\n\t\t{UFS_PWM_G3, 0x1},\n\t\t{UFS_PWM_G4, 0x1},\n\t};\n\n\tstatic u32 hs_fr_table_rA[][2] = {\n\t\t{UFS_HS_G1, 0x1F},\n\t\t{UFS_HS_G2, 0x3e},\n\t\t{UFS_HS_G3, 0x7D},\n\t};\n\n\tstatic u32 hs_fr_table_rB[][2] = {\n\t\t{UFS_HS_G1, 0x24},\n\t\t{UFS_HS_G2, 0x49},\n\t\t{UFS_HS_G3, 0x92},\n\t};\n\n\t \n\tif (ufs_qcom_cap_qunipro(host) && !ufshcd_is_intr_aggr_allowed(hba))\n\t\treturn 0;\n\n\tif (gear == 0) {\n\t\tdev_err(hba->dev, \"%s: invalid gear = %d\\n\", __func__, gear);\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(clki, &hba->clk_list_head, list) {\n\t\tif (!strcmp(clki->name, \"core_clk\"))\n\t\t\tcore_clk_rate = clk_get_rate(clki->clk);\n\t}\n\n\t \n\tif (core_clk_rate < DEFAULT_CLK_RATE_HZ)\n\t\tcore_clk_rate = DEFAULT_CLK_RATE_HZ;\n\n\tcore_clk_cycles_per_us = core_clk_rate / USEC_PER_SEC;\n\tif (ufshcd_readl(hba, REG_UFS_SYS1CLK_1US) != core_clk_cycles_per_us) {\n\t\tufshcd_writel(hba, core_clk_cycles_per_us, REG_UFS_SYS1CLK_1US);\n\t\t \n\t\tmb();\n\t}\n\n\tif (ufs_qcom_cap_qunipro(host))\n\t\treturn 0;\n\n\tcore_clk_period_in_ns = NSEC_PER_SEC / core_clk_rate;\n\tcore_clk_period_in_ns <<= OFFSET_CLK_NS_REG;\n\tcore_clk_period_in_ns &= MASK_CLK_NS_REG;\n\n\tswitch (hs) {\n\tcase FASTAUTO_MODE:\n\tcase FAST_MODE:\n\t\tif (rate == PA_HS_MODE_A) {\n\t\t\tif (gear > ARRAY_SIZE(hs_fr_table_rA)) {\n\t\t\t\tdev_err(hba->dev,\n\t\t\t\t\t\"%s: index %d exceeds table size %zu\\n\",\n\t\t\t\t\t__func__, gear,\n\t\t\t\t\tARRAY_SIZE(hs_fr_table_rA));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\ttx_clk_cycles_per_us = hs_fr_table_rA[gear-1][1];\n\t\t} else if (rate == PA_HS_MODE_B) {\n\t\t\tif (gear > ARRAY_SIZE(hs_fr_table_rB)) {\n\t\t\t\tdev_err(hba->dev,\n\t\t\t\t\t\"%s: index %d exceeds table size %zu\\n\",\n\t\t\t\t\t__func__, gear,\n\t\t\t\t\tARRAY_SIZE(hs_fr_table_rB));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\ttx_clk_cycles_per_us = hs_fr_table_rB[gear-1][1];\n\t\t} else {\n\t\t\tdev_err(hba->dev, \"%s: invalid rate = %d\\n\",\n\t\t\t\t__func__, rate);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase SLOWAUTO_MODE:\n\tcase SLOW_MODE:\n\t\tif (gear > ARRAY_SIZE(pwm_fr_table)) {\n\t\t\tdev_err(hba->dev,\n\t\t\t\t\t\"%s: index %d exceeds table size %zu\\n\",\n\t\t\t\t\t__func__, gear,\n\t\t\t\t\tARRAY_SIZE(pwm_fr_table));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttx_clk_cycles_per_us = pwm_fr_table[gear-1][1];\n\t\tbreak;\n\tcase UNCHANGED:\n\tdefault:\n\t\tdev_err(hba->dev, \"%s: invalid mode = %d\\n\", __func__, hs);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ufshcd_readl(hba, REG_UFS_TX_SYMBOL_CLK_NS_US) !=\n\t    (core_clk_period_in_ns | tx_clk_cycles_per_us)) {\n\t\t \n\t\tufshcd_writel(hba, core_clk_period_in_ns | tx_clk_cycles_per_us,\n\t\t\t      REG_UFS_TX_SYMBOL_CLK_NS_US);\n\t\t \n\t\tmb();\n\t}\n\n\tif (update_link_startup_timer && host->hw_ver.major != 0x5) {\n\t\tufshcd_writel(hba, ((core_clk_rate / MSEC_PER_SEC) * 100),\n\t\t\t      REG_UFS_CFG0);\n\t\t \n\t\tmb();\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_qcom_link_startup_notify(struct ufs_hba *hba,\n\t\t\t\t\tenum ufs_notify_change_status status)\n{\n\tint err = 0;\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\tif (ufs_qcom_cfg_timers(hba, UFS_PWM_G1, SLOWAUTO_MODE,\n\t\t\t\t\t0, true)) {\n\t\t\tdev_err(hba->dev, \"%s: ufs_qcom_cfg_timers() failed\\n\",\n\t\t\t\t__func__);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ufs_qcom_cap_qunipro(host))\n\t\t\t \n\t\t\terr = ufs_qcom_set_dme_vs_core_clk_ctrl_clear_div(hba,\n\t\t\t\t\t\t\t\t\t  150);\n\n\t\t \n\t\tif (ufshcd_get_local_unipro_ver(hba) != UFS_UNIPRO_VER_1_41)\n\t\t\terr = ufshcd_disable_host_tx_lcc(hba);\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic void ufs_qcom_device_reset_ctrl(struct ufs_hba *hba, bool asserted)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\t \n\tif (!host->device_reset)\n\t\treturn;\n\n\tgpiod_set_value_cansleep(host->device_reset, asserted);\n}\n\nstatic int ufs_qcom_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op,\n\tenum ufs_notify_change_status status)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct phy *phy = host->generic_phy;\n\n\tif (status == PRE_CHANGE)\n\t\treturn 0;\n\n\tif (ufs_qcom_is_link_off(hba)) {\n\t\t \n\t\tufs_qcom_disable_lane_clks(host);\n\t\tphy_power_off(phy);\n\n\t\t \n\t\tufs_qcom_device_reset_ctrl(hba, true);\n\n\t} else if (!ufs_qcom_is_link_active(hba)) {\n\t\tufs_qcom_disable_lane_clks(host);\n\t}\n\n\treturn ufs_qcom_ice_suspend(host);\n}\n\nstatic int ufs_qcom_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct phy *phy = host->generic_phy;\n\tint err;\n\n\tif (ufs_qcom_is_link_off(hba)) {\n\t\terr = phy_power_on(phy);\n\t\tif (err) {\n\t\t\tdev_err(hba->dev, \"%s: failed PHY power on: %d\\n\",\n\t\t\t\t__func__, err);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = ufs_qcom_enable_lane_clks(host);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (!ufs_qcom_is_link_active(hba)) {\n\t\terr = ufs_qcom_enable_lane_clks(host);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn ufs_qcom_ice_resume(host);\n}\n\nstatic void ufs_qcom_dev_ref_clk_ctrl(struct ufs_qcom_host *host, bool enable)\n{\n\tif (host->dev_ref_clk_ctrl_mmio &&\n\t    (enable ^ host->is_dev_ref_clk_enabled)) {\n\t\tu32 temp = readl_relaxed(host->dev_ref_clk_ctrl_mmio);\n\n\t\tif (enable)\n\t\t\ttemp |= host->dev_ref_clk_en_mask;\n\t\telse\n\t\t\ttemp &= ~host->dev_ref_clk_en_mask;\n\n\t\t \n\t\tif (!enable) {\n\t\t\tunsigned long gating_wait;\n\n\t\t\tgating_wait = host->hba->dev_info.clk_gating_wait_us;\n\t\t\tif (!gating_wait) {\n\t\t\t\tudelay(1);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tgating_wait += 10;\n\t\t\t\tusleep_range(gating_wait, gating_wait + 10);\n\t\t\t}\n\t\t}\n\n\t\twritel_relaxed(temp, host->dev_ref_clk_ctrl_mmio);\n\n\t\t \n\t\treadl(host->dev_ref_clk_ctrl_mmio);\n\n\t\t \n\t\tif (enable)\n\t\t\tudelay(1);\n\n\t\thost->is_dev_ref_clk_enabled = enable;\n\t}\n}\n\nstatic int ufs_qcom_icc_set_bw(struct ufs_qcom_host *host, u32 mem_bw, u32 cfg_bw)\n{\n\tstruct device *dev = host->hba->dev;\n\tint ret;\n\n\tret = icc_set_bw(host->icc_ddr, 0, mem_bw);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"failed to set bandwidth request: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = icc_set_bw(host->icc_cpu, 0, cfg_bw);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"failed to set bandwidth request: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic struct __ufs_qcom_bw_table ufs_qcom_get_bw_table(struct ufs_qcom_host *host)\n{\n\tstruct ufs_pa_layer_attr *p = &host->dev_req_params;\n\tint gear = max_t(u32, p->gear_rx, p->gear_tx);\n\tint lane = max_t(u32, p->lane_rx, p->lane_tx);\n\n\tif (ufshcd_is_hs_mode(p)) {\n\t\tif (p->hs_rate == PA_HS_MODE_B)\n\t\t\treturn ufs_qcom_bw_table[MODE_HS_RB][gear][lane];\n\t\telse\n\t\t\treturn ufs_qcom_bw_table[MODE_HS_RA][gear][lane];\n\t} else {\n\t\treturn ufs_qcom_bw_table[MODE_PWM][gear][lane];\n\t}\n}\n\nstatic int ufs_qcom_icc_update_bw(struct ufs_qcom_host *host)\n{\n\tstruct __ufs_qcom_bw_table bw_table;\n\n\tbw_table = ufs_qcom_get_bw_table(host);\n\n\treturn ufs_qcom_icc_set_bw(host, bw_table.mem_bw, bw_table.cfg_bw);\n}\n\nstatic int ufs_qcom_pwr_change_notify(struct ufs_hba *hba,\n\t\t\t\tenum ufs_notify_change_status status,\n\t\t\t\tstruct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\tstruct ufs_pa_layer_attr *dev_req_params)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_dev_params ufs_qcom_cap;\n\tint ret = 0;\n\n\tif (!dev_req_params) {\n\t\tpr_err(\"%s: incoming dev_req_params is NULL\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\tufshcd_init_pwr_dev_param(&ufs_qcom_cap);\n\t\tufs_qcom_cap.hs_rate = UFS_QCOM_LIMIT_HS_RATE;\n\n\t\t \n\t\tufs_qcom_cap.hs_tx_gear = ufs_qcom_cap.hs_rx_gear = ufs_qcom_get_hs_gear(hba);\n\n\t\tret = ufshcd_get_pwr_dev_param(&ufs_qcom_cap,\n\t\t\t\t\t       dev_max_params,\n\t\t\t\t\t       dev_req_params);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: failed to determine capabilities\\n\",\n\t\t\t\t\t__func__);\n\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\tif (dev_req_params->gear_tx > host->hs_gear)\n\t\t\thost->hs_gear = dev_req_params->gear_tx;\n\n\t\t \n\t\tif (!ufshcd_is_hs_mode(&hba->pwr_info) &&\n\t\t\tufshcd_is_hs_mode(dev_req_params))\n\t\t\tufs_qcom_dev_ref_clk_ctrl(host, true);\n\n\t\tif (host->hw_ver.major >= 0x4) {\n\t\t\tufshcd_dme_configure_adapt(hba,\n\t\t\t\t\t\tdev_req_params->gear_tx,\n\t\t\t\t\t\tPA_INITIAL_ADAPT);\n\t\t}\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tif (ufs_qcom_cfg_timers(hba, dev_req_params->gear_rx,\n\t\t\t\t\tdev_req_params->pwr_rx,\n\t\t\t\t\tdev_req_params->hs_rate, false)) {\n\t\t\tdev_err(hba->dev, \"%s: ufs_qcom_cfg_timers() failed\\n\",\n\t\t\t\t__func__);\n\t\t\t \n\t\t\tret = -EINVAL;\n\t\t}\n\n\t\t \n\t\tmemcpy(&host->dev_req_params,\n\t\t\t\tdev_req_params, sizeof(*dev_req_params));\n\n\t\tufs_qcom_icc_update_bw(host);\n\n\t\t \n\t\tif (ufshcd_is_hs_mode(&hba->pwr_info) &&\n\t\t\t!ufshcd_is_hs_mode(dev_req_params))\n\t\t\tufs_qcom_dev_ref_clk_ctrl(host, false);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_qcom_quirk_host_pa_saveconfigtime(struct ufs_hba *hba)\n{\n\tint err;\n\tu32 pa_vs_config_reg1;\n\n\terr = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_VS_CONFIG_REG1),\n\t\t\t     &pa_vs_config_reg1);\n\tif (err)\n\t\treturn err;\n\n\t \n\treturn ufshcd_dme_set(hba, UIC_ARG_MIB(PA_VS_CONFIG_REG1),\n\t\t\t    (pa_vs_config_reg1 | (1 << 12)));\n}\n\nstatic int ufs_qcom_apply_dev_quirks(struct ufs_hba *hba)\n{\n\tint err = 0;\n\n\tif (hba->dev_quirks & UFS_DEVICE_QUIRK_HOST_PA_SAVECONFIGTIME)\n\t\terr = ufs_qcom_quirk_host_pa_saveconfigtime(hba);\n\n\tif (hba->dev_info.wmanufacturerid == UFS_VENDOR_WDC)\n\t\thba->dev_quirks |= UFS_DEVICE_QUIRK_HOST_PA_TACTIVATE;\n\n\treturn err;\n}\n\nstatic u32 ufs_qcom_get_ufs_hci_version(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tif (host->hw_ver.major == 0x1)\n\t\treturn ufshci_version(1, 1);\n\telse\n\t\treturn ufshci_version(2, 0);\n}\n\n \nstatic void ufs_qcom_advertise_quirks(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tif (host->hw_ver.major == 0x01) {\n\t\thba->quirks |= UFSHCD_QUIRK_DELAY_BEFORE_DME_CMDS\n\t\t\t    | UFSHCD_QUIRK_BROKEN_PA_RXHSUNTERMCAP\n\t\t\t    | UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE;\n\n\t\tif (host->hw_ver.minor == 0x0001 && host->hw_ver.step == 0x0001)\n\t\t\thba->quirks |= UFSHCD_QUIRK_BROKEN_INTR_AGGR;\n\n\t\thba->quirks |= UFSHCD_QUIRK_BROKEN_LCC;\n\t}\n\n\tif (host->hw_ver.major == 0x2) {\n\t\thba->quirks |= UFSHCD_QUIRK_BROKEN_UFS_HCI_VERSION;\n\n\t\tif (!ufs_qcom_cap_qunipro(host))\n\t\t\t \n\t\t\thba->quirks |= (UFSHCD_QUIRK_DELAY_BEFORE_DME_CMDS\n\t\t\t\t| UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE\n\t\t\t\t| UFSHCD_QUIRK_BROKEN_PA_RXHSUNTERMCAP);\n\t}\n\n\tif (host->hw_ver.major > 0x3)\n\t\thba->quirks |= UFSHCD_QUIRK_REINIT_AFTER_MAX_GEAR_SWITCH;\n}\n\nstatic void ufs_qcom_set_caps(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\thba->caps |= UFSHCD_CAP_CLK_GATING | UFSHCD_CAP_HIBERN8_WITH_CLK_GATING;\n\thba->caps |= UFSHCD_CAP_CLK_SCALING | UFSHCD_CAP_WB_WITH_CLK_SCALING;\n\thba->caps |= UFSHCD_CAP_AUTO_BKOPS_SUSPEND;\n\thba->caps |= UFSHCD_CAP_WB_EN;\n\thba->caps |= UFSHCD_CAP_AGGR_POWER_COLLAPSE;\n\thba->caps |= UFSHCD_CAP_RPM_AUTOSUSPEND;\n\n\tif (host->hw_ver.major >= 0x2) {\n\t\thost->caps = UFS_QCOM_CAP_QUNIPRO |\n\t\t\t     UFS_QCOM_CAP_RETAIN_SEC_CFG_AFTER_PWR_COLLAPSE;\n\t}\n}\n\n \nstatic int ufs_qcom_setup_clocks(struct ufs_hba *hba, bool on,\n\t\t\t\t enum ufs_notify_change_status status)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\t \n\tif (!host)\n\t\treturn 0;\n\n\tswitch (status) {\n\tcase PRE_CHANGE:\n\t\tif (on) {\n\t\t\tufs_qcom_icc_update_bw(host);\n\t\t} else {\n\t\t\tif (!ufs_qcom_is_link_active(hba)) {\n\t\t\t\t \n\t\t\t\tufs_qcom_dev_ref_clk_ctrl(host, false);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tif (on) {\n\t\t\t \n\t\t\tif (ufshcd_is_hs_mode(&hba->pwr_info))\n\t\t\t\tufs_qcom_dev_ref_clk_ctrl(host, true);\n\t\t} else {\n\t\t\tufs_qcom_icc_set_bw(host, ufs_qcom_bw_table[MODE_MIN][0][0].mem_bw,\n\t\t\t\t\t    ufs_qcom_bw_table[MODE_MIN][0][0].cfg_bw);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nufs_qcom_reset_assert(struct reset_controller_dev *rcdev, unsigned long id)\n{\n\tstruct ufs_qcom_host *host = rcdev_to_ufs_host(rcdev);\n\n\tufs_qcom_assert_reset(host->hba);\n\t \n\tusleep_range(1000, 1100);\n\treturn 0;\n}\n\nstatic int\nufs_qcom_reset_deassert(struct reset_controller_dev *rcdev, unsigned long id)\n{\n\tstruct ufs_qcom_host *host = rcdev_to_ufs_host(rcdev);\n\n\tufs_qcom_deassert_reset(host->hba);\n\n\t \n\tusleep_range(1000, 1100);\n\treturn 0;\n}\n\nstatic const struct reset_control_ops ufs_qcom_reset_ops = {\n\t.assert = ufs_qcom_reset_assert,\n\t.deassert = ufs_qcom_reset_deassert,\n};\n\nstatic int ufs_qcom_icc_init(struct ufs_qcom_host *host)\n{\n\tstruct device *dev = host->hba->dev;\n\tint ret;\n\n\thost->icc_ddr = devm_of_icc_get(dev, \"ufs-ddr\");\n\tif (IS_ERR(host->icc_ddr))\n\t\treturn dev_err_probe(dev, PTR_ERR(host->icc_ddr),\n\t\t\t\t    \"failed to acquire interconnect path\\n\");\n\n\thost->icc_cpu = devm_of_icc_get(dev, \"cpu-ufs\");\n\tif (IS_ERR(host->icc_cpu))\n\t\treturn dev_err_probe(dev, PTR_ERR(host->icc_cpu),\n\t\t\t\t    \"failed to acquire interconnect path\\n\");\n\n\t \n\tret = ufs_qcom_icc_set_bw(host, ufs_qcom_bw_table[MODE_MAX][0][0].mem_bw,\n\t\t\t\t  ufs_qcom_bw_table[MODE_MAX][0][0].cfg_bw);\n\tif (ret < 0)\n\t\treturn dev_err_probe(dev, ret, \"failed to set bandwidth request\\n\");\n\n\treturn 0;\n}\n\n \nstatic int ufs_qcom_init(struct ufs_hba *hba)\n{\n\tint err;\n\tstruct device *dev = hba->dev;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tstruct ufs_qcom_host *host;\n\tstruct resource *res;\n\tstruct ufs_clk_info *clki;\n\n\thost = devm_kzalloc(dev, sizeof(*host), GFP_KERNEL);\n\tif (!host) {\n\t\tdev_err(dev, \"%s: no memory for qcom ufs host\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\thost->hba = hba;\n\tufshcd_set_variant(hba, host);\n\n\t \n\thost->core_reset = devm_reset_control_get_optional(hba->dev, \"rst\");\n\tif (IS_ERR(host->core_reset)) {\n\t\terr = dev_err_probe(dev, PTR_ERR(host->core_reset),\n\t\t\t\t    \"Failed to get reset control\\n\");\n\t\tgoto out_variant_clear;\n\t}\n\n\t \n\thost->rcdev.of_node = dev->of_node;\n\thost->rcdev.ops = &ufs_qcom_reset_ops;\n\thost->rcdev.owner = dev->driver->owner;\n\thost->rcdev.nr_resets = 1;\n\terr = devm_reset_controller_register(dev, &host->rcdev);\n\tif (err)\n\t\tdev_warn(dev, \"Failed to register reset controller\\n\");\n\n\tif (!has_acpi_companion(dev)) {\n\t\thost->generic_phy = devm_phy_get(dev, \"ufsphy\");\n\t\tif (IS_ERR(host->generic_phy)) {\n\t\t\terr = dev_err_probe(dev, PTR_ERR(host->generic_phy), \"Failed to get PHY\\n\");\n\t\t\tgoto out_variant_clear;\n\t\t}\n\t}\n\n\terr = ufs_qcom_icc_init(host);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\thost->device_reset = devm_gpiod_get_optional(dev, \"reset\",\n\t\t\t\t\t\t     GPIOD_OUT_HIGH);\n\tif (IS_ERR(host->device_reset)) {\n\t\terr = PTR_ERR(host->device_reset);\n\t\tif (err != -EPROBE_DEFER)\n\t\t\tdev_err(dev, \"failed to acquire reset gpio: %d\\n\", err);\n\t\tgoto out_variant_clear;\n\t}\n\n\tufs_qcom_get_controller_revision(hba, &host->hw_ver.major,\n\t\t&host->hw_ver.minor, &host->hw_ver.step);\n\n\t \n\tif (host->hw_ver.major >= 0x02) {\n\t\thost->dev_ref_clk_ctrl_mmio = hba->mmio_base + REG_UFS_CFG1;\n\t\thost->dev_ref_clk_en_mask = BIT(26);\n\t} else {\n\t\t \n\t\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM,\n\t\t\t\t\t\t   \"dev_ref_clk_ctrl_mem\");\n\t\tif (res) {\n\t\t\thost->dev_ref_clk_ctrl_mmio =\n\t\t\t\t\tdevm_ioremap_resource(dev, res);\n\t\t\tif (IS_ERR(host->dev_ref_clk_ctrl_mmio))\n\t\t\t\thost->dev_ref_clk_ctrl_mmio = NULL;\n\t\t\thost->dev_ref_clk_en_mask = BIT(5);\n\t\t}\n\t}\n\n\tlist_for_each_entry(clki, &hba->clk_list_head, list) {\n\t\tif (!strcmp(clki->name, \"core_clk_unipro\"))\n\t\t\tclki->keep_link_active = true;\n\t}\n\n\terr = ufs_qcom_init_lane_clks(host);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\tufs_qcom_set_caps(hba);\n\tufs_qcom_advertise_quirks(hba);\n\n\terr = ufs_qcom_ice_init(host);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\tufs_qcom_setup_clocks(hba, true, POST_CHANGE);\n\n\tif (hba->dev->id < MAX_UFS_QCOM_HOSTS)\n\t\tufs_qcom_hosts[hba->dev->id] = host;\n\n\tufs_qcom_get_default_testbus_cfg(host);\n\terr = ufs_qcom_testbus_config(host);\n\tif (err)\n\t\t \n\t\tdev_warn(dev, \"%s: failed to configure the testbus %d\\n\",\n\t\t\t\t__func__, err);\n\n\t \n\thost->hs_gear = UFS_HS_G2;\n\n\treturn 0;\n\nout_variant_clear:\n\tufshcd_set_variant(hba, NULL);\n\n\treturn err;\n}\n\nstatic void ufs_qcom_exit(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tufs_qcom_disable_lane_clks(host);\n\tphy_power_off(host->generic_phy);\n\tphy_exit(host->generic_phy);\n}\n\nstatic int ufs_qcom_set_dme_vs_core_clk_ctrl_clear_div(struct ufs_hba *hba,\n\t\t\t\t\t\t       u32 clk_cycles)\n{\n\tint err;\n\tu32 core_clk_ctrl_reg;\n\n\tif (clk_cycles > DME_VS_CORE_CLK_CTRL_MAX_CORE_CLK_1US_CYCLES_MASK)\n\t\treturn -EINVAL;\n\n\terr = ufshcd_dme_get(hba,\n\t\t\t    UIC_ARG_MIB(DME_VS_CORE_CLK_CTRL),\n\t\t\t    &core_clk_ctrl_reg);\n\tif (err)\n\t\treturn err;\n\n\tcore_clk_ctrl_reg &= ~DME_VS_CORE_CLK_CTRL_MAX_CORE_CLK_1US_CYCLES_MASK;\n\tcore_clk_ctrl_reg |= clk_cycles;\n\n\t \n\tcore_clk_ctrl_reg &= ~DME_VS_CORE_CLK_CTRL_CORE_CLK_DIV_EN_BIT;\n\n\treturn ufshcd_dme_set(hba,\n\t\t\t    UIC_ARG_MIB(DME_VS_CORE_CLK_CTRL),\n\t\t\t    core_clk_ctrl_reg);\n}\n\nstatic int ufs_qcom_clk_scale_up_pre_change(struct ufs_hba *hba)\n{\n\t \n\treturn 0;\n}\n\nstatic int ufs_qcom_clk_scale_up_post_change(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tif (!ufs_qcom_cap_qunipro(host))\n\t\treturn 0;\n\n\t \n\treturn ufs_qcom_set_dme_vs_core_clk_ctrl_clear_div(hba, 150);\n}\n\nstatic int ufs_qcom_clk_scale_down_pre_change(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tint err;\n\tu32 core_clk_ctrl_reg;\n\n\tif (!ufs_qcom_cap_qunipro(host))\n\t\treturn 0;\n\n\terr = ufshcd_dme_get(hba,\n\t\t\t    UIC_ARG_MIB(DME_VS_CORE_CLK_CTRL),\n\t\t\t    &core_clk_ctrl_reg);\n\n\t \n\tif (!err &&\n\t    (core_clk_ctrl_reg & DME_VS_CORE_CLK_CTRL_CORE_CLK_DIV_EN_BIT)) {\n\t\tcore_clk_ctrl_reg &= ~DME_VS_CORE_CLK_CTRL_CORE_CLK_DIV_EN_BIT;\n\t\terr = ufshcd_dme_set(hba,\n\t\t\t\t    UIC_ARG_MIB(DME_VS_CORE_CLK_CTRL),\n\t\t\t\t    core_clk_ctrl_reg);\n\t}\n\n\treturn err;\n}\n\nstatic int ufs_qcom_clk_scale_down_post_change(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tif (!ufs_qcom_cap_qunipro(host))\n\t\treturn 0;\n\n\t \n\treturn ufs_qcom_set_dme_vs_core_clk_ctrl_clear_div(hba, 75);\n}\n\nstatic int ufs_qcom_clk_scale_notify(struct ufs_hba *hba,\n\t\tbool scale_up, enum ufs_notify_change_status status)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_pa_layer_attr *dev_req_params = &host->dev_req_params;\n\tint err = 0;\n\n\t \n\tif (!ufshcd_is_hba_active(hba))\n\t\treturn 0;\n\n\tif (status == PRE_CHANGE) {\n\t\terr = ufshcd_uic_hibern8_enter(hba);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (scale_up)\n\t\t\terr = ufs_qcom_clk_scale_up_pre_change(hba);\n\t\telse\n\t\t\terr = ufs_qcom_clk_scale_down_pre_change(hba);\n\n\t\tif (err) {\n\t\t\tufshcd_uic_hibern8_exit(hba);\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tif (scale_up)\n\t\t\terr = ufs_qcom_clk_scale_up_post_change(hba);\n\t\telse\n\t\t\terr = ufs_qcom_clk_scale_down_post_change(hba);\n\n\n\t\tif (err) {\n\t\t\tufshcd_uic_hibern8_exit(hba);\n\t\t\treturn err;\n\t\t}\n\n\t\tufs_qcom_cfg_timers(hba,\n\t\t\t\t    dev_req_params->gear_rx,\n\t\t\t\t    dev_req_params->pwr_rx,\n\t\t\t\t    dev_req_params->hs_rate,\n\t\t\t\t    false);\n\t\tufs_qcom_icc_update_bw(host);\n\t\tufshcd_uic_hibern8_exit(hba);\n\t}\n\n\treturn 0;\n}\n\nstatic void ufs_qcom_enable_test_bus(struct ufs_qcom_host *host)\n{\n\tufshcd_rmwl(host->hba, UFS_REG_TEST_BUS_EN,\n\t\t\tUFS_REG_TEST_BUS_EN, REG_UFS_CFG1);\n\tufshcd_rmwl(host->hba, TEST_BUS_EN, TEST_BUS_EN, REG_UFS_CFG1);\n}\n\nstatic void ufs_qcom_get_default_testbus_cfg(struct ufs_qcom_host *host)\n{\n\t \n\thost->testbus.select_major = TSTBUS_UNIPRO;\n\thost->testbus.select_minor = 37;\n}\n\nstatic bool ufs_qcom_testbus_cfg_is_ok(struct ufs_qcom_host *host)\n{\n\tif (host->testbus.select_major >= TSTBUS_MAX) {\n\t\tdev_err(host->hba->dev,\n\t\t\t\"%s: UFS_CFG1[TEST_BUS_SEL} may not equal 0x%05X\\n\",\n\t\t\t__func__, host->testbus.select_major);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nint ufs_qcom_testbus_config(struct ufs_qcom_host *host)\n{\n\tint reg;\n\tint offset;\n\tu32 mask = TEST_BUS_SUB_SEL_MASK;\n\n\tif (!host)\n\t\treturn -EINVAL;\n\n\tif (!ufs_qcom_testbus_cfg_is_ok(host))\n\t\treturn -EPERM;\n\n\tswitch (host->testbus.select_major) {\n\tcase TSTBUS_UAWM:\n\t\treg = UFS_TEST_BUS_CTRL_0;\n\t\toffset = 24;\n\t\tbreak;\n\tcase TSTBUS_UARM:\n\t\treg = UFS_TEST_BUS_CTRL_0;\n\t\toffset = 16;\n\t\tbreak;\n\tcase TSTBUS_TXUC:\n\t\treg = UFS_TEST_BUS_CTRL_0;\n\t\toffset = 8;\n\t\tbreak;\n\tcase TSTBUS_RXUC:\n\t\treg = UFS_TEST_BUS_CTRL_0;\n\t\toffset = 0;\n\t\tbreak;\n\tcase TSTBUS_DFC:\n\t\treg = UFS_TEST_BUS_CTRL_1;\n\t\toffset = 24;\n\t\tbreak;\n\tcase TSTBUS_TRLUT:\n\t\treg = UFS_TEST_BUS_CTRL_1;\n\t\toffset = 16;\n\t\tbreak;\n\tcase TSTBUS_TMRLUT:\n\t\treg = UFS_TEST_BUS_CTRL_1;\n\t\toffset = 8;\n\t\tbreak;\n\tcase TSTBUS_OCSC:\n\t\treg = UFS_TEST_BUS_CTRL_1;\n\t\toffset = 0;\n\t\tbreak;\n\tcase TSTBUS_WRAPPER:\n\t\treg = UFS_TEST_BUS_CTRL_2;\n\t\toffset = 16;\n\t\tbreak;\n\tcase TSTBUS_COMBINED:\n\t\treg = UFS_TEST_BUS_CTRL_2;\n\t\toffset = 8;\n\t\tbreak;\n\tcase TSTBUS_UTP_HCI:\n\t\treg = UFS_TEST_BUS_CTRL_2;\n\t\toffset = 0;\n\t\tbreak;\n\tcase TSTBUS_UNIPRO:\n\t\treg = UFS_UNIPRO_CFG;\n\t\toffset = 20;\n\t\tmask = 0xFFF;\n\t\tbreak;\n\t \n\t}\n\tmask <<= offset;\n\tufshcd_rmwl(host->hba, TEST_BUS_SEL,\n\t\t    (u32)host->testbus.select_major << 19,\n\t\t    REG_UFS_CFG1);\n\tufshcd_rmwl(host->hba, mask,\n\t\t    (u32)host->testbus.select_minor << offset,\n\t\t    reg);\n\tufs_qcom_enable_test_bus(host);\n\t \n\tmb();\n\n\treturn 0;\n}\n\nstatic void ufs_qcom_dump_dbg_regs(struct ufs_hba *hba)\n{\n\tu32 reg;\n\tstruct ufs_qcom_host *host;\n\n\thost = ufshcd_get_variant(hba);\n\n\tufshcd_dump_regs(hba, REG_UFS_SYS1CLK_1US, 16 * 4,\n\t\t\t \"HCI Vendor Specific Registers \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_UFS_DBG_RD_REG_OCSC);\n\tufshcd_dump_regs(hba, reg, 44 * 4, \"UFS_UFS_DBG_RD_REG_OCSC \");\n\n\treg = ufshcd_readl(hba, REG_UFS_CFG1);\n\treg |= UTP_DBG_RAMS_EN;\n\tufshcd_writel(hba, reg, REG_UFS_CFG1);\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_UFS_DBG_RD_EDTL_RAM);\n\tufshcd_dump_regs(hba, reg, 32 * 4, \"UFS_UFS_DBG_RD_EDTL_RAM \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_UFS_DBG_RD_DESC_RAM);\n\tufshcd_dump_regs(hba, reg, 128 * 4, \"UFS_UFS_DBG_RD_DESC_RAM \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_UFS_DBG_RD_PRDT_RAM);\n\tufshcd_dump_regs(hba, reg, 64 * 4, \"UFS_UFS_DBG_RD_PRDT_RAM \");\n\n\t \n\tufshcd_rmwl(hba, UTP_DBG_RAMS_EN, 0, REG_UFS_CFG1);\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_UAWM);\n\tufshcd_dump_regs(hba, reg, 4 * 4, \"UFS_DBG_RD_REG_UAWM \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_UARM);\n\tufshcd_dump_regs(hba, reg, 4 * 4, \"UFS_DBG_RD_REG_UARM \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_TXUC);\n\tufshcd_dump_regs(hba, reg, 48 * 4, \"UFS_DBG_RD_REG_TXUC \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_RXUC);\n\tufshcd_dump_regs(hba, reg, 27 * 4, \"UFS_DBG_RD_REG_RXUC \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_DFC);\n\tufshcd_dump_regs(hba, reg, 19 * 4, \"UFS_DBG_RD_REG_DFC \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_TRLUT);\n\tufshcd_dump_regs(hba, reg, 34 * 4, \"UFS_DBG_RD_REG_TRLUT \");\n\n\treg = ufs_qcom_get_debug_reg_offset(host, UFS_DBG_RD_REG_TMRLUT);\n\tufshcd_dump_regs(hba, reg, 9 * 4, \"UFS_DBG_RD_REG_TMRLUT \");\n}\n\n \nstatic int ufs_qcom_device_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\t \n\tif (!host->device_reset)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tufs_qcom_device_reset_ctrl(hba, true);\n\tusleep_range(10, 15);\n\n\tufs_qcom_device_reset_ctrl(hba, false);\n\tusleep_range(10, 15);\n\n\treturn 0;\n}\n\n#if IS_ENABLED(CONFIG_DEVFREQ_GOV_SIMPLE_ONDEMAND)\nstatic void ufs_qcom_config_scaling_param(struct ufs_hba *hba,\n\t\t\t\t\tstruct devfreq_dev_profile *p,\n\t\t\t\t\tstruct devfreq_simple_ondemand_data *d)\n{\n\tp->polling_ms = 60;\n\tp->timer = DEVFREQ_TIMER_DELAYED;\n\td->upthreshold = 70;\n\td->downdifferential = 5;\n}\n#else\nstatic void ufs_qcom_config_scaling_param(struct ufs_hba *hba,\n\t\tstruct devfreq_dev_profile *p,\n\t\tstruct devfreq_simple_ondemand_data *data)\n{\n}\n#endif\n\nstatic void ufs_qcom_reinit_notify(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\n\tphy_power_off(host->generic_phy);\n}\n\n \nstatic const struct ufshcd_res_info ufs_res_info[RES_MAX] = {\n\t{.name = \"ufs_mem\",},\n\t{.name = \"mcq\",},\n\t \n\t{.name = \"mcq_sqd\",},\n\t \n\t{.name = \"mcq_sqis\",},\n\t \n\t{.name = \"mcq_cqd\",},\n\t \n\t{.name = \"mcq_cqis\",},\n\t \n\t{.name = \"mcq_vs\",},\n};\n\nstatic int ufs_qcom_mcq_config_resource(struct ufs_hba *hba)\n{\n\tstruct platform_device *pdev = to_platform_device(hba->dev);\n\tstruct ufshcd_res_info *res;\n\tstruct resource *res_mem, *res_mcq;\n\tint i, ret = 0;\n\n\tmemcpy(hba->res, ufs_res_info, sizeof(ufs_res_info));\n\n\tfor (i = 0; i < RES_MAX; i++) {\n\t\tres = &hba->res[i];\n\t\tres->resource = platform_get_resource_byname(pdev,\n\t\t\t\t\t\t\t     IORESOURCE_MEM,\n\t\t\t\t\t\t\t     res->name);\n\t\tif (!res->resource) {\n\t\t\tdev_info(hba->dev, \"Resource %s not provided\\n\", res->name);\n\t\t\tif (i == RES_UFS)\n\t\t\t\treturn -ENODEV;\n\t\t\tcontinue;\n\t\t} else if (i == RES_UFS) {\n\t\t\tres_mem = res->resource;\n\t\t\tres->base = hba->mmio_base;\n\t\t\tcontinue;\n\t\t}\n\n\t\tres->base = devm_ioremap_resource(hba->dev, res->resource);\n\t\tif (IS_ERR(res->base)) {\n\t\t\tdev_err(hba->dev, \"Failed to map res %s, err=%d\\n\",\n\t\t\t\t\t res->name, (int)PTR_ERR(res->base));\n\t\t\tret = PTR_ERR(res->base);\n\t\t\tres->base = NULL;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tres = &hba->res[RES_MCQ];\n\t \n\tif (res->base)\n\t\tgoto out;\n\n\t \n\tres_mcq = devm_kzalloc(hba->dev, sizeof(*res_mcq), GFP_KERNEL);\n\tif (!res_mcq)\n\t\treturn -ENOMEM;\n\n\tres_mcq->start = res_mem->start +\n\t\t\t MCQ_SQATTR_OFFSET(hba->mcq_capabilities);\n\tres_mcq->end = res_mcq->start + hba->nr_hw_queues * MCQ_QCFG_SIZE - 1;\n\tres_mcq->flags = res_mem->flags;\n\tres_mcq->name = \"mcq\";\n\n\tret = insert_resource(&iomem_resource, res_mcq);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"Failed to insert MCQ resource, err=%d\\n\",\n\t\t\tret);\n\t\treturn ret;\n\t}\n\n\tres->base = devm_ioremap_resource(hba->dev, res_mcq);\n\tif (IS_ERR(res->base)) {\n\t\tdev_err(hba->dev, \"MCQ registers mapping failed, err=%d\\n\",\n\t\t\t(int)PTR_ERR(res->base));\n\t\tret = PTR_ERR(res->base);\n\t\tgoto ioremap_err;\n\t}\n\nout:\n\thba->mcq_base = res->base;\n\treturn 0;\nioremap_err:\n\tres->base = NULL;\n\tremove_resource(res_mcq);\n\treturn ret;\n}\n\nstatic int ufs_qcom_op_runtime_config(struct ufs_hba *hba)\n{\n\tstruct ufshcd_res_info *mem_res, *sqdao_res;\n\tstruct ufshcd_mcq_opr_info_t *opr;\n\tint i;\n\n\tmem_res = &hba->res[RES_UFS];\n\tsqdao_res = &hba->res[RES_MCQ_SQD];\n\n\tif (!mem_res->base || !sqdao_res->base)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < OPR_MAX; i++) {\n\t\topr = &hba->mcq_opr[i];\n\t\topr->offset = sqdao_res->resource->start -\n\t\t\t      mem_res->resource->start + 0x40 * i;\n\t\topr->stride = 0x100;\n\t\topr->base = sqdao_res->base + 0x40 * i;\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_qcom_get_hba_mac(struct ufs_hba *hba)\n{\n\t \n\treturn MAX_SUPP_MAC;\n}\n\nstatic int ufs_qcom_get_outstanding_cqs(struct ufs_hba *hba,\n\t\t\t\t\tunsigned long *ocqs)\n{\n\tstruct ufshcd_res_info *mcq_vs_res = &hba->res[RES_MCQ_VS];\n\n\tif (!mcq_vs_res->base)\n\t\treturn -EINVAL;\n\n\t*ocqs = readl(mcq_vs_res->base + UFS_MEM_CQIS_VS);\n\n\treturn 0;\n}\n\nstatic void ufs_qcom_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\n\tufshcd_mcq_config_esi(hba, msg);\n}\n\nstatic irqreturn_t ufs_qcom_mcq_esi_handler(int irq, void *data)\n{\n\tstruct msi_desc *desc = data;\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct ufs_hba *hba = dev_get_drvdata(dev);\n\tu32 id = desc->msi_index;\n\tstruct ufs_hw_queue *hwq = &hba->uhq[id];\n\n\tufshcd_mcq_write_cqis(hba, 0x1, id);\n\tufshcd_mcq_poll_cqe_lock(hba, hwq);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int ufs_qcom_config_esi(struct ufs_hba *hba)\n{\n\tstruct ufs_qcom_host *host = ufshcd_get_variant(hba);\n\tstruct msi_desc *desc;\n\tstruct msi_desc *failed_desc = NULL;\n\tint nr_irqs, ret;\n\n\tif (host->esi_enabled)\n\t\treturn 0;\n\n\t \n\tnr_irqs = hba->nr_hw_queues - hba->nr_queues[HCTX_TYPE_POLL];\n\tret = platform_msi_domain_alloc_irqs(hba->dev, nr_irqs,\n\t\t\t\t\t     ufs_qcom_write_msi_msg);\n\tif (ret) {\n\t\tdev_err(hba->dev, \"Failed to request Platform MSI %d\\n\", ret);\n\t\tgoto out;\n\t}\n\n\tmsi_lock_descs(hba->dev);\n\tmsi_for_each_desc(desc, hba->dev, MSI_DESC_ALL) {\n\t\tret = devm_request_irq(hba->dev, desc->irq,\n\t\t\t\t       ufs_qcom_mcq_esi_handler,\n\t\t\t\t       IRQF_SHARED, \"qcom-mcq-esi\", desc);\n\t\tif (ret) {\n\t\t\tdev_err(hba->dev, \"%s: Fail to request IRQ for %d, err = %d\\n\",\n\t\t\t\t__func__, desc->irq, ret);\n\t\t\tfailed_desc = desc;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmsi_unlock_descs(hba->dev);\n\n\tif (ret) {\n\t\t \n\t\tmsi_lock_descs(hba->dev);\n\t\tmsi_for_each_desc(desc, hba->dev, MSI_DESC_ALL) {\n\t\t\tif (desc == failed_desc)\n\t\t\t\tbreak;\n\t\t\tdevm_free_irq(hba->dev, desc->irq, hba);\n\t\t}\n\t\tmsi_unlock_descs(hba->dev);\n\t\tplatform_msi_domain_free_irqs(hba->dev);\n\t} else {\n\t\tif (host->hw_ver.major == 6 && host->hw_ver.minor == 0 &&\n\t\t    host->hw_ver.step == 0) {\n\t\t\tufshcd_writel(hba,\n\t\t\t\t      ufshcd_readl(hba, REG_UFS_CFG3) | 0x1F000,\n\t\t\t\t      REG_UFS_CFG3);\n\t\t}\n\t\tufshcd_mcq_enable_esi(hba);\n\t}\n\nout:\n\tif (!ret)\n\t\thost->esi_enabled = true;\n\n\treturn ret;\n}\n\n \nstatic const struct ufs_hba_variant_ops ufs_hba_qcom_vops = {\n\t.name                   = \"qcom\",\n\t.init                   = ufs_qcom_init,\n\t.exit                   = ufs_qcom_exit,\n\t.get_ufs_hci_version\t= ufs_qcom_get_ufs_hci_version,\n\t.clk_scale_notify\t= ufs_qcom_clk_scale_notify,\n\t.setup_clocks           = ufs_qcom_setup_clocks,\n\t.hce_enable_notify      = ufs_qcom_hce_enable_notify,\n\t.link_startup_notify    = ufs_qcom_link_startup_notify,\n\t.pwr_change_notify\t= ufs_qcom_pwr_change_notify,\n\t.apply_dev_quirks\t= ufs_qcom_apply_dev_quirks,\n\t.suspend\t\t= ufs_qcom_suspend,\n\t.resume\t\t\t= ufs_qcom_resume,\n\t.dbg_register_dump\t= ufs_qcom_dump_dbg_regs,\n\t.device_reset\t\t= ufs_qcom_device_reset,\n\t.config_scaling_param = ufs_qcom_config_scaling_param,\n\t.program_key\t\t= ufs_qcom_ice_program_key,\n\t.reinit_notify\t\t= ufs_qcom_reinit_notify,\n\t.mcq_config_resource\t= ufs_qcom_mcq_config_resource,\n\t.get_hba_mac\t\t= ufs_qcom_get_hba_mac,\n\t.op_runtime_config\t= ufs_qcom_op_runtime_config,\n\t.get_outstanding_cqs\t= ufs_qcom_get_outstanding_cqs,\n\t.config_esi\t\t= ufs_qcom_config_esi,\n};\n\n \nstatic int ufs_qcom_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct device *dev = &pdev->dev;\n\n\t \n\terr = ufshcd_pltfrm_init(pdev, &ufs_hba_qcom_vops);\n\tif (err)\n\t\treturn dev_err_probe(dev, err, \"ufshcd_pltfrm_init() failed\\n\");\n\n\treturn 0;\n}\n\n \nstatic int ufs_qcom_remove(struct platform_device *pdev)\n{\n\tstruct ufs_hba *hba =  platform_get_drvdata(pdev);\n\n\tpm_runtime_get_sync(&(pdev)->dev);\n\tufshcd_remove(hba);\n\tplatform_msi_domain_free_irqs(hba->dev);\n\treturn 0;\n}\n\nstatic const struct of_device_id ufs_qcom_of_match[] __maybe_unused = {\n\t{ .compatible = \"qcom,ufshc\"},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, ufs_qcom_of_match);\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id ufs_qcom_acpi_match[] = {\n\t{ \"QCOM24A5\" },\n\t{ },\n};\nMODULE_DEVICE_TABLE(acpi, ufs_qcom_acpi_match);\n#endif\n\nstatic const struct dev_pm_ops ufs_qcom_pm_ops = {\n\tSET_RUNTIME_PM_OPS(ufshcd_runtime_suspend, ufshcd_runtime_resume, NULL)\n\t.prepare\t = ufshcd_suspend_prepare,\n\t.complete\t = ufshcd_resume_complete,\n#ifdef CONFIG_PM_SLEEP\n\t.suspend         = ufshcd_system_suspend,\n\t.resume          = ufshcd_system_resume,\n\t.freeze          = ufshcd_system_freeze,\n\t.restore         = ufshcd_system_restore,\n\t.thaw            = ufshcd_system_thaw,\n#endif\n};\n\nstatic struct platform_driver ufs_qcom_pltform = {\n\t.probe\t= ufs_qcom_probe,\n\t.remove\t= ufs_qcom_remove,\n\t.driver\t= {\n\t\t.name\t= \"ufshcd-qcom\",\n\t\t.pm\t= &ufs_qcom_pm_ops,\n\t\t.of_match_table = of_match_ptr(ufs_qcom_of_match),\n\t\t.acpi_match_table = ACPI_PTR(ufs_qcom_acpi_match),\n\t},\n};\nmodule_platform_driver(ufs_qcom_pltform);\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}