{
  "module_name": "acpi-cpufreq.c",
  "hash_id": "333ffcdf5fd4b54c2c1563084efd94345c06a2db0f1e0809ea8dcf5ab2a429a5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cpufreq/acpi-cpufreq.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/sched.h>\n#include <linux/cpufreq.h>\n#include <linux/compiler.h>\n#include <linux/dmi.h>\n#include <linux/slab.h>\n#include <linux/string_helpers.h>\n#include <linux/platform_device.h>\n\n#include <linux/acpi.h>\n#include <linux/io.h>\n#include <linux/delay.h>\n#include <linux/uaccess.h>\n\n#include <acpi/processor.h>\n#include <acpi/cppc_acpi.h>\n\n#include <asm/msr.h>\n#include <asm/processor.h>\n#include <asm/cpufeature.h>\n#include <asm/cpu_device_id.h>\n\nMODULE_AUTHOR(\"Paul Diefenbaugh, Dominik Brodowski\");\nMODULE_DESCRIPTION(\"ACPI Processor P-States Driver\");\nMODULE_LICENSE(\"GPL\");\n\nenum {\n\tUNDEFINED_CAPABLE = 0,\n\tSYSTEM_INTEL_MSR_CAPABLE,\n\tSYSTEM_AMD_MSR_CAPABLE,\n\tSYSTEM_IO_CAPABLE,\n};\n\n#define INTEL_MSR_RANGE\t\t(0xffff)\n#define AMD_MSR_RANGE\t\t(0x7)\n#define HYGON_MSR_RANGE\t\t(0x7)\n\n#define MSR_K7_HWCR_CPB_DIS\t(1ULL << 25)\n\nstruct acpi_cpufreq_data {\n\tunsigned int resume;\n\tunsigned int cpu_feature;\n\tunsigned int acpi_perf_cpu;\n\tcpumask_var_t freqdomain_cpus;\n\tvoid (*cpu_freq_write)(struct acpi_pct_register *reg, u32 val);\n\tu32 (*cpu_freq_read)(struct acpi_pct_register *reg);\n};\n\n \nstatic struct acpi_processor_performance __percpu *acpi_perf_data;\n\nstatic inline struct acpi_processor_performance *to_perf_data(struct acpi_cpufreq_data *data)\n{\n\treturn per_cpu_ptr(acpi_perf_data, data->acpi_perf_cpu);\n}\n\nstatic struct cpufreq_driver acpi_cpufreq_driver;\n\nstatic unsigned int acpi_pstate_strict;\n\nstatic bool boost_state(unsigned int cpu)\n{\n\tu32 lo, hi;\n\tu64 msr;\n\n\tswitch (boot_cpu_data.x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\tcase X86_VENDOR_CENTAUR:\n\tcase X86_VENDOR_ZHAOXIN:\n\t\trdmsr_on_cpu(cpu, MSR_IA32_MISC_ENABLE, &lo, &hi);\n\t\tmsr = lo | ((u64)hi << 32);\n\t\treturn !(msr & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);\n\tcase X86_VENDOR_HYGON:\n\tcase X86_VENDOR_AMD:\n\t\trdmsr_on_cpu(cpu, MSR_K7_HWCR, &lo, &hi);\n\t\tmsr = lo | ((u64)hi << 32);\n\t\treturn !(msr & MSR_K7_HWCR_CPB_DIS);\n\t}\n\treturn false;\n}\n\nstatic int boost_set_msr(bool enable)\n{\n\tu32 msr_addr;\n\tu64 msr_mask, val;\n\n\tswitch (boot_cpu_data.x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\tcase X86_VENDOR_CENTAUR:\n\tcase X86_VENDOR_ZHAOXIN:\n\t\tmsr_addr = MSR_IA32_MISC_ENABLE;\n\t\tmsr_mask = MSR_IA32_MISC_ENABLE_TURBO_DISABLE;\n\t\tbreak;\n\tcase X86_VENDOR_HYGON:\n\tcase X86_VENDOR_AMD:\n\t\tmsr_addr = MSR_K7_HWCR;\n\t\tmsr_mask = MSR_K7_HWCR_CPB_DIS;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\trdmsrl(msr_addr, val);\n\n\tif (enable)\n\t\tval &= ~msr_mask;\n\telse\n\t\tval |= msr_mask;\n\n\twrmsrl(msr_addr, val);\n\treturn 0;\n}\n\nstatic void boost_set_msr_each(void *p_en)\n{\n\tbool enable = (bool) p_en;\n\n\tboost_set_msr(enable);\n}\n\nstatic int set_boost(struct cpufreq_policy *policy, int val)\n{\n\ton_each_cpu_mask(policy->cpus, boost_set_msr_each,\n\t\t\t (void *)(long)val, 1);\n\tpr_debug(\"CPU %*pbl: Core Boosting %s.\\n\",\n\t\t cpumask_pr_args(policy->cpus), str_enabled_disabled(val));\n\n\treturn 0;\n}\n\nstatic ssize_t show_freqdomain_cpus(struct cpufreq_policy *policy, char *buf)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\n\tif (unlikely(!data))\n\t\treturn -ENODEV;\n\n\treturn cpufreq_show_cpus(data->freqdomain_cpus, buf);\n}\n\ncpufreq_freq_attr_ro(freqdomain_cpus);\n\n#ifdef CONFIG_X86_ACPI_CPUFREQ_CPB\nstatic ssize_t store_cpb(struct cpufreq_policy *policy, const char *buf,\n\t\t\t size_t count)\n{\n\tint ret;\n\tunsigned int val = 0;\n\n\tif (!acpi_cpufreq_driver.set_boost)\n\t\treturn -EINVAL;\n\n\tret = kstrtouint(buf, 10, &val);\n\tif (ret || val > 1)\n\t\treturn -EINVAL;\n\n\tcpus_read_lock();\n\tset_boost(policy, val);\n\tcpus_read_unlock();\n\n\treturn count;\n}\n\nstatic ssize_t show_cpb(struct cpufreq_policy *policy, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", acpi_cpufreq_driver.boost_enabled);\n}\n\ncpufreq_freq_attr_rw(cpb);\n#endif\n\nstatic int check_est_cpu(unsigned int cpuid)\n{\n\tstruct cpuinfo_x86 *cpu = &cpu_data(cpuid);\n\n\treturn cpu_has(cpu, X86_FEATURE_EST);\n}\n\nstatic int check_amd_hwpstate_cpu(unsigned int cpuid)\n{\n\tstruct cpuinfo_x86 *cpu = &cpu_data(cpuid);\n\n\treturn cpu_has(cpu, X86_FEATURE_HW_PSTATE);\n}\n\nstatic unsigned extract_io(struct cpufreq_policy *policy, u32 value)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\tstruct acpi_processor_performance *perf;\n\tint i;\n\n\tperf = to_perf_data(data);\n\n\tfor (i = 0; i < perf->state_count; i++) {\n\t\tif (value == perf->states[i].status)\n\t\t\treturn policy->freq_table[i].frequency;\n\t}\n\treturn 0;\n}\n\nstatic unsigned extract_msr(struct cpufreq_policy *policy, u32 msr)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\tstruct cpufreq_frequency_table *pos;\n\tstruct acpi_processor_performance *perf;\n\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\tmsr &= AMD_MSR_RANGE;\n\telse if (boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)\n\t\tmsr &= HYGON_MSR_RANGE;\n\telse\n\t\tmsr &= INTEL_MSR_RANGE;\n\n\tperf = to_perf_data(data);\n\n\tcpufreq_for_each_entry(pos, policy->freq_table)\n\t\tif (msr == perf->states[pos->driver_data].status)\n\t\t\treturn pos->frequency;\n\treturn policy->freq_table[0].frequency;\n}\n\nstatic unsigned extract_freq(struct cpufreq_policy *policy, u32 val)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\n\tswitch (data->cpu_feature) {\n\tcase SYSTEM_INTEL_MSR_CAPABLE:\n\tcase SYSTEM_AMD_MSR_CAPABLE:\n\t\treturn extract_msr(policy, val);\n\tcase SYSTEM_IO_CAPABLE:\n\t\treturn extract_io(policy, val);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic u32 cpu_freq_read_intel(struct acpi_pct_register *not_used)\n{\n\tu32 val, dummy __always_unused;\n\n\trdmsr(MSR_IA32_PERF_CTL, val, dummy);\n\treturn val;\n}\n\nstatic void cpu_freq_write_intel(struct acpi_pct_register *not_used, u32 val)\n{\n\tu32 lo, hi;\n\n\trdmsr(MSR_IA32_PERF_CTL, lo, hi);\n\tlo = (lo & ~INTEL_MSR_RANGE) | (val & INTEL_MSR_RANGE);\n\twrmsr(MSR_IA32_PERF_CTL, lo, hi);\n}\n\nstatic u32 cpu_freq_read_amd(struct acpi_pct_register *not_used)\n{\n\tu32 val, dummy __always_unused;\n\n\trdmsr(MSR_AMD_PERF_CTL, val, dummy);\n\treturn val;\n}\n\nstatic void cpu_freq_write_amd(struct acpi_pct_register *not_used, u32 val)\n{\n\twrmsr(MSR_AMD_PERF_CTL, val, 0);\n}\n\nstatic u32 cpu_freq_read_io(struct acpi_pct_register *reg)\n{\n\tu32 val;\n\n\tacpi_os_read_port(reg->address, &val, reg->bit_width);\n\treturn val;\n}\n\nstatic void cpu_freq_write_io(struct acpi_pct_register *reg, u32 val)\n{\n\tacpi_os_write_port(reg->address, val, reg->bit_width);\n}\n\nstruct drv_cmd {\n\tstruct acpi_pct_register *reg;\n\tu32 val;\n\tunion {\n\t\tvoid (*write)(struct acpi_pct_register *reg, u32 val);\n\t\tu32 (*read)(struct acpi_pct_register *reg);\n\t} func;\n};\n\n \nstatic void do_drv_read(void *_cmd)\n{\n\tstruct drv_cmd *cmd = _cmd;\n\n\tcmd->val = cmd->func.read(cmd->reg);\n}\n\nstatic u32 drv_read(struct acpi_cpufreq_data *data, const struct cpumask *mask)\n{\n\tstruct acpi_processor_performance *perf = to_perf_data(data);\n\tstruct drv_cmd cmd = {\n\t\t.reg = &perf->control_register,\n\t\t.func.read = data->cpu_freq_read,\n\t};\n\tint err;\n\n\terr = smp_call_function_any(mask, do_drv_read, &cmd, 1);\n\tWARN_ON_ONCE(err);\t \n\treturn cmd.val;\n}\n\n \nstatic void do_drv_write(void *_cmd)\n{\n\tstruct drv_cmd *cmd = _cmd;\n\n\tcmd->func.write(cmd->reg, cmd->val);\n}\n\nstatic void drv_write(struct acpi_cpufreq_data *data,\n\t\t      const struct cpumask *mask, u32 val)\n{\n\tstruct acpi_processor_performance *perf = to_perf_data(data);\n\tstruct drv_cmd cmd = {\n\t\t.reg = &perf->control_register,\n\t\t.val = val,\n\t\t.func.write = data->cpu_freq_write,\n\t};\n\tint this_cpu;\n\n\tthis_cpu = get_cpu();\n\tif (cpumask_test_cpu(this_cpu, mask))\n\t\tdo_drv_write(&cmd);\n\n\tsmp_call_function_many(mask, do_drv_write, &cmd, 1);\n\tput_cpu();\n}\n\nstatic u32 get_cur_val(const struct cpumask *mask, struct acpi_cpufreq_data *data)\n{\n\tu32 val;\n\n\tif (unlikely(cpumask_empty(mask)))\n\t\treturn 0;\n\n\tval = drv_read(data, mask);\n\n\tpr_debug(\"%s = %u\\n\", __func__, val);\n\n\treturn val;\n}\n\nstatic unsigned int get_cur_freq_on_cpu(unsigned int cpu)\n{\n\tstruct acpi_cpufreq_data *data;\n\tstruct cpufreq_policy *policy;\n\tunsigned int freq;\n\tunsigned int cached_freq;\n\n\tpr_debug(\"%s (%d)\\n\", __func__, cpu);\n\n\tpolicy = cpufreq_cpu_get_raw(cpu);\n\tif (unlikely(!policy))\n\t\treturn 0;\n\n\tdata = policy->driver_data;\n\tif (unlikely(!data || !policy->freq_table))\n\t\treturn 0;\n\n\tcached_freq = policy->freq_table[to_perf_data(data)->state].frequency;\n\tfreq = extract_freq(policy, get_cur_val(cpumask_of(cpu), data));\n\tif (freq != cached_freq) {\n\t\t \n\t\tdata->resume = 1;\n\t}\n\n\tpr_debug(\"cur freq = %u\\n\", freq);\n\n\treturn freq;\n}\n\nstatic unsigned int check_freqs(struct cpufreq_policy *policy,\n\t\t\t\tconst struct cpumask *mask, unsigned int freq)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\tunsigned int cur_freq;\n\tunsigned int i;\n\n\tfor (i = 0; i < 100; i++) {\n\t\tcur_freq = extract_freq(policy, get_cur_val(mask, data));\n\t\tif (cur_freq == freq)\n\t\t\treturn 1;\n\t\tudelay(10);\n\t}\n\treturn 0;\n}\n\nstatic int acpi_cpufreq_target(struct cpufreq_policy *policy,\n\t\t\t       unsigned int index)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\tstruct acpi_processor_performance *perf;\n\tconst struct cpumask *mask;\n\tunsigned int next_perf_state = 0;  \n\tint result = 0;\n\n\tif (unlikely(!data)) {\n\t\treturn -ENODEV;\n\t}\n\n\tperf = to_perf_data(data);\n\tnext_perf_state = policy->freq_table[index].driver_data;\n\tif (perf->state == next_perf_state) {\n\t\tif (unlikely(data->resume)) {\n\t\t\tpr_debug(\"Called after resume, resetting to P%d\\n\",\n\t\t\t\tnext_perf_state);\n\t\t\tdata->resume = 0;\n\t\t} else {\n\t\t\tpr_debug(\"Already at target state (P%d)\\n\",\n\t\t\t\tnext_perf_state);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tmask = policy->shared_type == CPUFREQ_SHARED_TYPE_ANY ?\n\t\tcpumask_of(policy->cpu) : policy->cpus;\n\n\tdrv_write(data, mask, perf->states[next_perf_state].control);\n\n\tif (acpi_pstate_strict) {\n\t\tif (!check_freqs(policy, mask,\n\t\t\t\t policy->freq_table[index].frequency)) {\n\t\t\tpr_debug(\"%s (%d)\\n\", __func__, policy->cpu);\n\t\t\tresult = -EAGAIN;\n\t\t}\n\t}\n\n\tif (!result)\n\t\tperf->state = next_perf_state;\n\n\treturn result;\n}\n\nstatic unsigned int acpi_cpufreq_fast_switch(struct cpufreq_policy *policy,\n\t\t\t\t\t     unsigned int target_freq)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\tstruct acpi_processor_performance *perf;\n\tstruct cpufreq_frequency_table *entry;\n\tunsigned int next_perf_state, next_freq, index;\n\n\t \n\tif (policy->cached_target_freq == target_freq)\n\t\tindex = policy->cached_resolved_idx;\n\telse\n\t\tindex = cpufreq_table_find_index_dl(policy, target_freq,\n\t\t\t\t\t\t    false);\n\n\tentry = &policy->freq_table[index];\n\tnext_freq = entry->frequency;\n\tnext_perf_state = entry->driver_data;\n\n\tperf = to_perf_data(data);\n\tif (perf->state == next_perf_state) {\n\t\tif (unlikely(data->resume))\n\t\t\tdata->resume = 0;\n\t\telse\n\t\t\treturn next_freq;\n\t}\n\n\tdata->cpu_freq_write(&perf->control_register,\n\t\t\t     perf->states[next_perf_state].control);\n\tperf->state = next_perf_state;\n\treturn next_freq;\n}\n\nstatic unsigned long\nacpi_cpufreq_guess_freq(struct acpi_cpufreq_data *data, unsigned int cpu)\n{\n\tstruct acpi_processor_performance *perf;\n\n\tperf = to_perf_data(data);\n\tif (cpu_khz) {\n\t\t \n\t\tunsigned int i;\n\t\tunsigned long freq;\n\t\tunsigned long freqn = perf->states[0].core_frequency * 1000;\n\n\t\tfor (i = 0; i < (perf->state_count-1); i++) {\n\t\t\tfreq = freqn;\n\t\t\tfreqn = perf->states[i+1].core_frequency * 1000;\n\t\t\tif ((2 * cpu_khz) > (freqn + freq)) {\n\t\t\t\tperf->state = i;\n\t\t\t\treturn freq;\n\t\t\t}\n\t\t}\n\t\tperf->state = perf->state_count-1;\n\t\treturn freqn;\n\t} else {\n\t\t \n\t\tperf->state = 0;\n\t\treturn perf->states[0].core_frequency * 1000;\n\t}\n}\n\nstatic void free_acpi_perf_data(void)\n{\n\tunsigned int i;\n\n\t \n\tfor_each_possible_cpu(i)\n\t\tfree_cpumask_var(per_cpu_ptr(acpi_perf_data, i)\n\t\t\t\t ->shared_cpu_map);\n\tfree_percpu(acpi_perf_data);\n}\n\nstatic int cpufreq_boost_down_prep(unsigned int cpu)\n{\n\t \n\treturn boost_set_msr(1);\n}\n\n \nstatic int __init acpi_cpufreq_early_init(void)\n{\n\tunsigned int i;\n\tpr_debug(\"%s\\n\", __func__);\n\n\tacpi_perf_data = alloc_percpu(struct acpi_processor_performance);\n\tif (!acpi_perf_data) {\n\t\tpr_debug(\"Memory allocation error for acpi_perf_data.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tfor_each_possible_cpu(i) {\n\t\tif (!zalloc_cpumask_var_node(\n\t\t\t&per_cpu_ptr(acpi_perf_data, i)->shared_cpu_map,\n\t\t\tGFP_KERNEL, cpu_to_node(i))) {\n\n\t\t\t \n\t\t\tfree_acpi_perf_data();\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t \n\tacpi_processor_preregister_performance(acpi_perf_data);\n\treturn 0;\n}\n\n#ifdef CONFIG_SMP\n \nstatic int bios_with_sw_any_bug;\n\nstatic int sw_any_bug_found(const struct dmi_system_id *d)\n{\n\tbios_with_sw_any_bug = 1;\n\treturn 0;\n}\n\nstatic const struct dmi_system_id sw_any_bug_dmi_table[] = {\n\t{\n\t\t.callback = sw_any_bug_found,\n\t\t.ident = \"Supermicro Server X6DLP\",\n\t\t.matches = {\n\t\t\tDMI_MATCH(DMI_SYS_VENDOR, \"Supermicro\"),\n\t\t\tDMI_MATCH(DMI_BIOS_VERSION, \"080010\"),\n\t\t\tDMI_MATCH(DMI_PRODUCT_NAME, \"X6DLP\"),\n\t\t},\n\t},\n\t{ }\n};\n\nstatic int acpi_cpufreq_blacklist(struct cpuinfo_x86 *c)\n{\n\t \n\tif (c->x86_vendor == X86_VENDOR_INTEL) {\n\t\tif ((c->x86 == 15) &&\n\t\t    (c->x86_model == 6) &&\n\t\t    (c->x86_stepping == 8)) {\n\t\t\tpr_info(\"Intel(R) Xeon(R) 7100 Errata AL30, processors may lock up on frequency changes: disabling acpi-cpufreq\\n\");\n\t\t\treturn -ENODEV;\n\t\t    }\n\t\t}\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_ACPI_CPPC_LIB\nstatic u64 get_max_boost_ratio(unsigned int cpu)\n{\n\tstruct cppc_perf_caps perf_caps;\n\tu64 highest_perf, nominal_perf;\n\tint ret;\n\n\tif (acpi_pstate_strict)\n\t\treturn 0;\n\n\tret = cppc_get_perf_caps(cpu, &perf_caps);\n\tif (ret) {\n\t\tpr_debug(\"CPU%d: Unable to get performance capabilities (%d)\\n\",\n\t\t\t cpu, ret);\n\t\treturn 0;\n\t}\n\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\thighest_perf = amd_get_highest_perf();\n\telse\n\t\thighest_perf = perf_caps.highest_perf;\n\n\tnominal_perf = perf_caps.nominal_perf;\n\n\tif (!highest_perf || !nominal_perf) {\n\t\tpr_debug(\"CPU%d: highest or nominal performance missing\\n\", cpu);\n\t\treturn 0;\n\t}\n\n\tif (highest_perf < nominal_perf) {\n\t\tpr_debug(\"CPU%d: nominal performance above highest\\n\", cpu);\n\t\treturn 0;\n\t}\n\n\treturn div_u64(highest_perf << SCHED_CAPACITY_SHIFT, nominal_perf);\n}\n#else\nstatic inline u64 get_max_boost_ratio(unsigned int cpu) { return 0; }\n#endif\n\nstatic int acpi_cpufreq_cpu_init(struct cpufreq_policy *policy)\n{\n\tstruct cpufreq_frequency_table *freq_table;\n\tstruct acpi_processor_performance *perf;\n\tstruct acpi_cpufreq_data *data;\n\tunsigned int cpu = policy->cpu;\n\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\tunsigned int valid_states = 0;\n\tunsigned int result = 0;\n\tu64 max_boost_ratio;\n\tunsigned int i;\n#ifdef CONFIG_SMP\n\tstatic int blacklisted;\n#endif\n\n\tpr_debug(\"%s\\n\", __func__);\n\n#ifdef CONFIG_SMP\n\tif (blacklisted)\n\t\treturn blacklisted;\n\tblacklisted = acpi_cpufreq_blacklist(c);\n\tif (blacklisted)\n\t\treturn blacklisted;\n#endif\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tif (!zalloc_cpumask_var(&data->freqdomain_cpus, GFP_KERNEL)) {\n\t\tresult = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tperf = per_cpu_ptr(acpi_perf_data, cpu);\n\tdata->acpi_perf_cpu = cpu;\n\tpolicy->driver_data = data;\n\n\tif (cpu_has(c, X86_FEATURE_CONSTANT_TSC))\n\t\tacpi_cpufreq_driver.flags |= CPUFREQ_CONST_LOOPS;\n\n\tresult = acpi_processor_register_performance(perf, cpu);\n\tif (result)\n\t\tgoto err_free_mask;\n\n\tpolicy->shared_type = perf->shared_type;\n\n\t \n\tif (policy->shared_type == CPUFREQ_SHARED_TYPE_ALL ||\n\t    policy->shared_type == CPUFREQ_SHARED_TYPE_ANY) {\n\t\tcpumask_copy(policy->cpus, perf->shared_cpu_map);\n\t}\n\tcpumask_copy(data->freqdomain_cpus, perf->shared_cpu_map);\n\n#ifdef CONFIG_SMP\n\tdmi_check_system(sw_any_bug_dmi_table);\n\tif (bios_with_sw_any_bug && !policy_is_shared(policy)) {\n\t\tpolicy->shared_type = CPUFREQ_SHARED_TYPE_ALL;\n\t\tcpumask_copy(policy->cpus, topology_core_cpumask(cpu));\n\t}\n\n\tif (check_amd_hwpstate_cpu(cpu) && boot_cpu_data.x86 < 0x19 &&\n\t    !acpi_pstate_strict) {\n\t\tcpumask_clear(policy->cpus);\n\t\tcpumask_set_cpu(cpu, policy->cpus);\n\t\tcpumask_copy(data->freqdomain_cpus,\n\t\t\t     topology_sibling_cpumask(cpu));\n\t\tpolicy->shared_type = CPUFREQ_SHARED_TYPE_HW;\n\t\tpr_info_once(\"overriding BIOS provided _PSD data\\n\");\n\t}\n#endif\n\n\t \n\tif (perf->state_count <= 1) {\n\t\tpr_debug(\"No P-States\\n\");\n\t\tresult = -ENODEV;\n\t\tgoto err_unreg;\n\t}\n\n\tif (perf->control_register.space_id != perf->status_register.space_id) {\n\t\tresult = -ENODEV;\n\t\tgoto err_unreg;\n\t}\n\n\tswitch (perf->control_register.space_id) {\n\tcase ACPI_ADR_SPACE_SYSTEM_IO:\n\t\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD &&\n\t\t    boot_cpu_data.x86 == 0xf) {\n\t\t\tpr_debug(\"AMD K8 systems must use native drivers.\\n\");\n\t\t\tresult = -ENODEV;\n\t\t\tgoto err_unreg;\n\t\t}\n\t\tpr_debug(\"SYSTEM IO addr space\\n\");\n\t\tdata->cpu_feature = SYSTEM_IO_CAPABLE;\n\t\tdata->cpu_freq_read = cpu_freq_read_io;\n\t\tdata->cpu_freq_write = cpu_freq_write_io;\n\t\tbreak;\n\tcase ACPI_ADR_SPACE_FIXED_HARDWARE:\n\t\tpr_debug(\"HARDWARE addr space\\n\");\n\t\tif (check_est_cpu(cpu)) {\n\t\t\tdata->cpu_feature = SYSTEM_INTEL_MSR_CAPABLE;\n\t\t\tdata->cpu_freq_read = cpu_freq_read_intel;\n\t\t\tdata->cpu_freq_write = cpu_freq_write_intel;\n\t\t\tbreak;\n\t\t}\n\t\tif (check_amd_hwpstate_cpu(cpu)) {\n\t\t\tdata->cpu_feature = SYSTEM_AMD_MSR_CAPABLE;\n\t\t\tdata->cpu_freq_read = cpu_freq_read_amd;\n\t\t\tdata->cpu_freq_write = cpu_freq_write_amd;\n\t\t\tbreak;\n\t\t}\n\t\tresult = -ENODEV;\n\t\tgoto err_unreg;\n\tdefault:\n\t\tpr_debug(\"Unknown addr space %d\\n\",\n\t\t\t(u32) (perf->control_register.space_id));\n\t\tresult = -ENODEV;\n\t\tgoto err_unreg;\n\t}\n\n\tfreq_table = kcalloc(perf->state_count + 1, sizeof(*freq_table),\n\t\t\t     GFP_KERNEL);\n\tif (!freq_table) {\n\t\tresult = -ENOMEM;\n\t\tgoto err_unreg;\n\t}\n\n\t \n\tpolicy->cpuinfo.transition_latency = 0;\n\tfor (i = 0; i < perf->state_count; i++) {\n\t\tif ((perf->states[i].transition_latency * 1000) >\n\t\t    policy->cpuinfo.transition_latency)\n\t\t\tpolicy->cpuinfo.transition_latency =\n\t\t\t    perf->states[i].transition_latency * 1000;\n\t}\n\n\t \n\tif (perf->control_register.space_id == ACPI_ADR_SPACE_FIXED_HARDWARE &&\n\t    policy->cpuinfo.transition_latency > 20 * 1000) {\n\t\tpolicy->cpuinfo.transition_latency = 20 * 1000;\n\t\tpr_info_once(\"P-state transition latency capped at 20 uS\\n\");\n\t}\n\n\t \n\tfor (i = 0; i < perf->state_count; i++) {\n\t\tif (i > 0 && perf->states[i].core_frequency >=\n\t\t    freq_table[valid_states-1].frequency / 1000)\n\t\t\tcontinue;\n\n\t\tfreq_table[valid_states].driver_data = i;\n\t\tfreq_table[valid_states].frequency =\n\t\t    perf->states[i].core_frequency * 1000;\n\t\tvalid_states++;\n\t}\n\tfreq_table[valid_states].frequency = CPUFREQ_TABLE_END;\n\n\tmax_boost_ratio = get_max_boost_ratio(cpu);\n\tif (max_boost_ratio) {\n\t\tunsigned int freq = freq_table[0].frequency;\n\n\t\t \n\t\tpolicy->cpuinfo.max_freq = freq * max_boost_ratio >> SCHED_CAPACITY_SHIFT;\n\t} else {\n\t\t \n\t\tarch_set_max_freq_ratio(true);\n\t}\n\n\tpolicy->freq_table = freq_table;\n\tperf->state = 0;\n\n\tswitch (perf->control_register.space_id) {\n\tcase ACPI_ADR_SPACE_SYSTEM_IO:\n\t\t \n\t\tpolicy->cur = acpi_cpufreq_guess_freq(data, policy->cpu);\n\t\tbreak;\n\tcase ACPI_ADR_SPACE_FIXED_HARDWARE:\n\t\tacpi_cpufreq_driver.get = get_cur_freq_on_cpu;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tacpi_processor_notify_smm(THIS_MODULE);\n\n\tpr_debug(\"CPU%u - ACPI performance management activated.\\n\", cpu);\n\tfor (i = 0; i < perf->state_count; i++)\n\t\tpr_debug(\"     %cP%d: %d MHz, %d mW, %d uS\\n\",\n\t\t\t(i == perf->state ? '*' : ' '), i,\n\t\t\t(u32) perf->states[i].core_frequency,\n\t\t\t(u32) perf->states[i].power,\n\t\t\t(u32) perf->states[i].transition_latency);\n\n\t \n\tdata->resume = 1;\n\n\tpolicy->fast_switch_possible = !acpi_pstate_strict &&\n\t\t!(policy_is_shared(policy) && policy->shared_type != CPUFREQ_SHARED_TYPE_ANY);\n\n\tif (perf->states[0].core_frequency * 1000 != freq_table[0].frequency)\n\t\tpr_warn(FW_WARN \"P-state 0 is not max freq\\n\");\n\n\tif (acpi_cpufreq_driver.set_boost)\n\t\tset_boost(policy, acpi_cpufreq_driver.boost_enabled);\n\n\treturn result;\n\nerr_unreg:\n\tacpi_processor_unregister_performance(cpu);\nerr_free_mask:\n\tfree_cpumask_var(data->freqdomain_cpus);\nerr_free:\n\tkfree(data);\n\tpolicy->driver_data = NULL;\n\n\treturn result;\n}\n\nstatic int acpi_cpufreq_cpu_exit(struct cpufreq_policy *policy)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\n\tpr_debug(\"%s\\n\", __func__);\n\n\tcpufreq_boost_down_prep(policy->cpu);\n\tpolicy->fast_switch_possible = false;\n\tpolicy->driver_data = NULL;\n\tacpi_processor_unregister_performance(data->acpi_perf_cpu);\n\tfree_cpumask_var(data->freqdomain_cpus);\n\tkfree(policy->freq_table);\n\tkfree(data);\n\n\treturn 0;\n}\n\nstatic int acpi_cpufreq_resume(struct cpufreq_policy *policy)\n{\n\tstruct acpi_cpufreq_data *data = policy->driver_data;\n\n\tpr_debug(\"%s\\n\", __func__);\n\n\tdata->resume = 1;\n\n\treturn 0;\n}\n\nstatic struct freq_attr *acpi_cpufreq_attr[] = {\n\t&cpufreq_freq_attr_scaling_available_freqs,\n\t&freqdomain_cpus,\n#ifdef CONFIG_X86_ACPI_CPUFREQ_CPB\n\t&cpb,\n#endif\n\tNULL,\n};\n\nstatic struct cpufreq_driver acpi_cpufreq_driver = {\n\t.verify\t\t= cpufreq_generic_frequency_table_verify,\n\t.target_index\t= acpi_cpufreq_target,\n\t.fast_switch\t= acpi_cpufreq_fast_switch,\n\t.bios_limit\t= acpi_processor_get_bios_limit,\n\t.init\t\t= acpi_cpufreq_cpu_init,\n\t.exit\t\t= acpi_cpufreq_cpu_exit,\n\t.resume\t\t= acpi_cpufreq_resume,\n\t.name\t\t= \"acpi-cpufreq\",\n\t.attr\t\t= acpi_cpufreq_attr,\n};\n\nstatic void __init acpi_cpufreq_boost_init(void)\n{\n\tif (!(boot_cpu_has(X86_FEATURE_CPB) || boot_cpu_has(X86_FEATURE_IDA))) {\n\t\tpr_debug(\"Boost capabilities not present in the processor\\n\");\n\t\treturn;\n\t}\n\n\tacpi_cpufreq_driver.set_boost = set_boost;\n\tacpi_cpufreq_driver.boost_enabled = boost_state(0);\n}\n\nstatic int __init acpi_cpufreq_probe(struct platform_device *pdev)\n{\n\tint ret;\n\n\tif (acpi_disabled)\n\t\treturn -ENODEV;\n\n\t \n\tif (cpufreq_get_current_driver())\n\t\treturn -ENODEV;\n\n\tpr_debug(\"%s\\n\", __func__);\n\n\tret = acpi_cpufreq_early_init();\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_X86_ACPI_CPUFREQ_CPB\n\t \n\tif (!check_amd_hwpstate_cpu(0)) {\n\t\tstruct freq_attr **attr;\n\n\t\tpr_debug(\"CPB unsupported, do not expose it\\n\");\n\n\t\tfor (attr = acpi_cpufreq_attr; *attr; attr++)\n\t\t\tif (*attr == &cpb) {\n\t\t\t\t*attr = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n#endif\n\tacpi_cpufreq_boost_init();\n\n\tret = cpufreq_register_driver(&acpi_cpufreq_driver);\n\tif (ret) {\n\t\tfree_acpi_perf_data();\n\t}\n\treturn ret;\n}\n\nstatic void acpi_cpufreq_remove(struct platform_device *pdev)\n{\n\tpr_debug(\"%s\\n\", __func__);\n\n\tcpufreq_unregister_driver(&acpi_cpufreq_driver);\n\n\tfree_acpi_perf_data();\n}\n\nstatic struct platform_driver acpi_cpufreq_platdrv = {\n\t.driver = {\n\t\t.name\t= \"acpi-cpufreq\",\n\t},\n\t.remove_new\t= acpi_cpufreq_remove,\n};\n\nstatic int __init acpi_cpufreq_init(void)\n{\n\treturn platform_driver_probe(&acpi_cpufreq_platdrv, acpi_cpufreq_probe);\n}\n\nstatic void __exit acpi_cpufreq_exit(void)\n{\n\tplatform_driver_unregister(&acpi_cpufreq_platdrv);\n}\n\nmodule_param(acpi_pstate_strict, uint, 0644);\nMODULE_PARM_DESC(acpi_pstate_strict,\n\t\"value 0 or non-zero. non-zero -> strict ACPI checks are \"\n\t\"performed during frequency changes.\");\n\nlate_initcall(acpi_cpufreq_init);\nmodule_exit(acpi_cpufreq_exit);\n\nMODULE_ALIAS(\"platform:acpi-cpufreq\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}