{
  "module_name": "amd-pstate.c",
  "hash_id": "47bc52b5a7a333433d460c3bc41e1b9ea6be12d761ee47f8242c0dab279f4c46",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cpufreq/amd-pstate.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/sched.h>\n#include <linux/cpufreq.h>\n#include <linux/compiler.h>\n#include <linux/dmi.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/io.h>\n#include <linux/delay.h>\n#include <linux/uaccess.h>\n#include <linux/static_call.h>\n#include <linux/amd-pstate.h>\n\n#include <acpi/processor.h>\n#include <acpi/cppc_acpi.h>\n\n#include <asm/msr.h>\n#include <asm/processor.h>\n#include <asm/cpufeature.h>\n#include <asm/cpu_device_id.h>\n#include \"amd-pstate-trace.h\"\n\n#define AMD_PSTATE_TRANSITION_LATENCY\t20000\n#define AMD_PSTATE_TRANSITION_DELAY\t1000\n\n \nstatic struct cpufreq_driver *current_pstate_driver;\nstatic struct cpufreq_driver amd_pstate_driver;\nstatic struct cpufreq_driver amd_pstate_epp_driver;\nstatic int cppc_state = AMD_PSTATE_UNDEFINED;\nstatic bool cppc_enabled;\n\n \nenum energy_perf_value_index {\n\tEPP_INDEX_DEFAULT = 0,\n\tEPP_INDEX_PERFORMANCE,\n\tEPP_INDEX_BALANCE_PERFORMANCE,\n\tEPP_INDEX_BALANCE_POWERSAVE,\n\tEPP_INDEX_POWERSAVE,\n};\n\nstatic const char * const energy_perf_strings[] = {\n\t[EPP_INDEX_DEFAULT] = \"default\",\n\t[EPP_INDEX_PERFORMANCE] = \"performance\",\n\t[EPP_INDEX_BALANCE_PERFORMANCE] = \"balance_performance\",\n\t[EPP_INDEX_BALANCE_POWERSAVE] = \"balance_power\",\n\t[EPP_INDEX_POWERSAVE] = \"power\",\n\tNULL\n};\n\nstatic unsigned int epp_values[] = {\n\t[EPP_INDEX_DEFAULT] = 0,\n\t[EPP_INDEX_PERFORMANCE] = AMD_CPPC_EPP_PERFORMANCE,\n\t[EPP_INDEX_BALANCE_PERFORMANCE] = AMD_CPPC_EPP_BALANCE_PERFORMANCE,\n\t[EPP_INDEX_BALANCE_POWERSAVE] = AMD_CPPC_EPP_BALANCE_POWERSAVE,\n\t[EPP_INDEX_POWERSAVE] = AMD_CPPC_EPP_POWERSAVE,\n };\n\ntypedef int (*cppc_mode_transition_fn)(int);\n\nstatic inline int get_mode_idx_from_str(const char *str, size_t size)\n{\n\tint i;\n\n\tfor (i=0; i < AMD_PSTATE_MAX; i++) {\n\t\tif (!strncmp(str, amd_pstate_mode_string[i], size))\n\t\t\treturn i;\n\t}\n\treturn -EINVAL;\n}\n\nstatic DEFINE_MUTEX(amd_pstate_limits_lock);\nstatic DEFINE_MUTEX(amd_pstate_driver_lock);\n\nstatic s16 amd_pstate_get_epp(struct amd_cpudata *cpudata, u64 cppc_req_cached)\n{\n\tu64 epp;\n\tint ret;\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tif (!cppc_req_cached) {\n\t\t\tepp = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ,\n\t\t\t\t\t&cppc_req_cached);\n\t\t\tif (epp)\n\t\t\t\treturn epp;\n\t\t}\n\t\tepp = (cppc_req_cached >> 24) & 0xFF;\n\t} else {\n\t\tret = cppc_get_epp_perf(cpudata->cpu, &epp);\n\t\tif (ret < 0) {\n\t\t\tpr_debug(\"Could not retrieve energy perf value (%d)\\n\", ret);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn (s16)(epp & 0xff);\n}\n\nstatic int amd_pstate_get_energy_pref_index(struct amd_cpudata *cpudata)\n{\n\ts16 epp;\n\tint index = -EINVAL;\n\n\tepp = amd_pstate_get_epp(cpudata, 0);\n\tif (epp < 0)\n\t\treturn epp;\n\n\tswitch (epp) {\n\tcase AMD_CPPC_EPP_PERFORMANCE:\n\t\tindex = EPP_INDEX_PERFORMANCE;\n\t\tbreak;\n\tcase AMD_CPPC_EPP_BALANCE_PERFORMANCE:\n\t\tindex = EPP_INDEX_BALANCE_PERFORMANCE;\n\t\tbreak;\n\tcase AMD_CPPC_EPP_BALANCE_POWERSAVE:\n\t\tindex = EPP_INDEX_BALANCE_POWERSAVE;\n\t\tbreak;\n\tcase AMD_CPPC_EPP_POWERSAVE:\n\t\tindex = EPP_INDEX_POWERSAVE;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn index;\n}\n\nstatic int amd_pstate_set_epp(struct amd_cpudata *cpudata, u32 epp)\n{\n\tint ret;\n\tstruct cppc_perf_ctrls perf_ctrls;\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tu64 value = READ_ONCE(cpudata->cppc_req_cached);\n\n\t\tvalue &= ~GENMASK_ULL(31, 24);\n\t\tvalue |= (u64)epp << 24;\n\t\tWRITE_ONCE(cpudata->cppc_req_cached, value);\n\n\t\tret = wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);\n\t\tif (!ret)\n\t\t\tcpudata->epp_cached = epp;\n\t} else {\n\t\tperf_ctrls.energy_perf = epp;\n\t\tret = cppc_set_epp_perf(cpudata->cpu, &perf_ctrls, 1);\n\t\tif (ret) {\n\t\t\tpr_debug(\"failed to set energy perf value (%d)\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tcpudata->epp_cached = epp;\n\t}\n\n\treturn ret;\n}\n\nstatic int amd_pstate_set_energy_pref_index(struct amd_cpudata *cpudata,\n\t\tint pref_index)\n{\n\tint epp = -EINVAL;\n\tint ret;\n\n\tif (!pref_index) {\n\t\tpr_debug(\"EPP pref_index is invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (epp == -EINVAL)\n\t\tepp = epp_values[pref_index];\n\n\tif (epp > 0 && cpudata->policy == CPUFREQ_POLICY_PERFORMANCE) {\n\t\tpr_debug(\"EPP cannot be set under performance policy\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tret = amd_pstate_set_epp(cpudata, epp);\n\n\treturn ret;\n}\n\nstatic inline int pstate_enable(bool enable)\n{\n\tint ret, cpu;\n\tunsigned long logical_proc_id_mask = 0;\n\n\tif (enable == cppc_enabled)\n\t\treturn 0;\n\n\tfor_each_present_cpu(cpu) {\n\t\tunsigned long logical_id = topology_logical_die_id(cpu);\n\n\t\tif (test_bit(logical_id, &logical_proc_id_mask))\n\t\t\tcontinue;\n\n\t\tset_bit(logical_id, &logical_proc_id_mask);\n\n\t\tret = wrmsrl_safe_on_cpu(cpu, MSR_AMD_CPPC_ENABLE,\n\t\t\t\tenable);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tcppc_enabled = enable;\n\treturn 0;\n}\n\nstatic int cppc_enable(bool enable)\n{\n\tint cpu, ret = 0;\n\tstruct cppc_perf_ctrls perf_ctrls;\n\n\tif (enable == cppc_enabled)\n\t\treturn 0;\n\n\tfor_each_present_cpu(cpu) {\n\t\tret = cppc_set_enable(cpu, enable);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (cppc_state == AMD_PSTATE_ACTIVE) {\n\t\t\t \n\t\t\tperf_ctrls.desired_perf = 0;\n\t\t\tret = cppc_set_perf(cpu, &perf_ctrls);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tcppc_enabled = enable;\n\treturn ret;\n}\n\nDEFINE_STATIC_CALL(amd_pstate_enable, pstate_enable);\n\nstatic inline int amd_pstate_enable(bool enable)\n{\n\treturn static_call(amd_pstate_enable)(enable);\n}\n\nstatic int pstate_init_perf(struct amd_cpudata *cpudata)\n{\n\tu64 cap1;\n\tu32 highest_perf;\n\n\tint ret = rdmsrl_safe_on_cpu(cpudata->cpu, MSR_AMD_CPPC_CAP1,\n\t\t\t\t     &cap1);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\thighest_perf = amd_get_highest_perf();\n\tif (highest_perf > AMD_CPPC_HIGHEST_PERF(cap1))\n\t\thighest_perf = AMD_CPPC_HIGHEST_PERF(cap1);\n\n\tWRITE_ONCE(cpudata->highest_perf, highest_perf);\n\tWRITE_ONCE(cpudata->max_limit_perf, highest_perf);\n\tWRITE_ONCE(cpudata->nominal_perf, AMD_CPPC_NOMINAL_PERF(cap1));\n\tWRITE_ONCE(cpudata->lowest_nonlinear_perf, AMD_CPPC_LOWNONLIN_PERF(cap1));\n\tWRITE_ONCE(cpudata->lowest_perf, AMD_CPPC_LOWEST_PERF(cap1));\n\tWRITE_ONCE(cpudata->min_limit_perf, AMD_CPPC_LOWEST_PERF(cap1));\n\treturn 0;\n}\n\nstatic int cppc_init_perf(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_caps cppc_perf;\n\tu32 highest_perf;\n\n\tint ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);\n\tif (ret)\n\t\treturn ret;\n\n\thighest_perf = amd_get_highest_perf();\n\tif (highest_perf > cppc_perf.highest_perf)\n\t\thighest_perf = cppc_perf.highest_perf;\n\n\tWRITE_ONCE(cpudata->highest_perf, highest_perf);\n\tWRITE_ONCE(cpudata->max_limit_perf, highest_perf);\n\tWRITE_ONCE(cpudata->nominal_perf, cppc_perf.nominal_perf);\n\tWRITE_ONCE(cpudata->lowest_nonlinear_perf,\n\t\t   cppc_perf.lowest_nonlinear_perf);\n\tWRITE_ONCE(cpudata->lowest_perf, cppc_perf.lowest_perf);\n\tWRITE_ONCE(cpudata->min_limit_perf, cppc_perf.lowest_perf);\n\n\tif (cppc_state == AMD_PSTATE_ACTIVE)\n\t\treturn 0;\n\n\tret = cppc_get_auto_sel_caps(cpudata->cpu, &cppc_perf);\n\tif (ret) {\n\t\tpr_warn(\"failed to get auto_sel, ret: %d\\n\", ret);\n\t\treturn 0;\n\t}\n\n\tret = cppc_set_auto_sel(cpudata->cpu,\n\t\t\t(cppc_state == AMD_PSTATE_PASSIVE) ? 0 : 1);\n\n\tif (ret)\n\t\tpr_warn(\"failed to set auto_sel, ret: %d\\n\", ret);\n\n\treturn ret;\n}\n\nDEFINE_STATIC_CALL(amd_pstate_init_perf, pstate_init_perf);\n\nstatic inline int amd_pstate_init_perf(struct amd_cpudata *cpudata)\n{\n\treturn static_call(amd_pstate_init_perf)(cpudata);\n}\n\nstatic void pstate_update_perf(struct amd_cpudata *cpudata, u32 min_perf,\n\t\t\t       u32 des_perf, u32 max_perf, bool fast_switch)\n{\n\tif (fast_switch)\n\t\twrmsrl(MSR_AMD_CPPC_REQ, READ_ONCE(cpudata->cppc_req_cached));\n\telse\n\t\twrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ,\n\t\t\t      READ_ONCE(cpudata->cppc_req_cached));\n}\n\nstatic void cppc_update_perf(struct amd_cpudata *cpudata,\n\t\t\t     u32 min_perf, u32 des_perf,\n\t\t\t     u32 max_perf, bool fast_switch)\n{\n\tstruct cppc_perf_ctrls perf_ctrls;\n\n\tperf_ctrls.max_perf = max_perf;\n\tperf_ctrls.min_perf = min_perf;\n\tperf_ctrls.desired_perf = des_perf;\n\n\tcppc_set_perf(cpudata->cpu, &perf_ctrls);\n}\n\nDEFINE_STATIC_CALL(amd_pstate_update_perf, pstate_update_perf);\n\nstatic inline void amd_pstate_update_perf(struct amd_cpudata *cpudata,\n\t\t\t\t\t  u32 min_perf, u32 des_perf,\n\t\t\t\t\t  u32 max_perf, bool fast_switch)\n{\n\tstatic_call(amd_pstate_update_perf)(cpudata, min_perf, des_perf,\n\t\t\t\t\t    max_perf, fast_switch);\n}\n\nstatic inline bool amd_pstate_sample(struct amd_cpudata *cpudata)\n{\n\tu64 aperf, mperf, tsc;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\trdmsrl(MSR_IA32_APERF, aperf);\n\trdmsrl(MSR_IA32_MPERF, mperf);\n\ttsc = rdtsc();\n\n\tif (cpudata->prev.mperf == mperf || cpudata->prev.tsc == tsc) {\n\t\tlocal_irq_restore(flags);\n\t\treturn false;\n\t}\n\n\tlocal_irq_restore(flags);\n\n\tcpudata->cur.aperf = aperf;\n\tcpudata->cur.mperf = mperf;\n\tcpudata->cur.tsc =  tsc;\n\tcpudata->cur.aperf -= cpudata->prev.aperf;\n\tcpudata->cur.mperf -= cpudata->prev.mperf;\n\tcpudata->cur.tsc -= cpudata->prev.tsc;\n\n\tcpudata->prev.aperf = aperf;\n\tcpudata->prev.mperf = mperf;\n\tcpudata->prev.tsc = tsc;\n\n\tcpudata->freq = div64_u64((cpudata->cur.aperf * cpu_khz), cpudata->cur.mperf);\n\n\treturn true;\n}\n\nstatic void amd_pstate_update(struct amd_cpudata *cpudata, u32 min_perf,\n\t\t\t      u32 des_perf, u32 max_perf, bool fast_switch, int gov_flags)\n{\n\tu64 prev = READ_ONCE(cpudata->cppc_req_cached);\n\tu64 value = prev;\n\n\tmin_perf = clamp_t(unsigned long, min_perf, cpudata->min_limit_perf,\n\t\t\tcpudata->max_limit_perf);\n\tmax_perf = clamp_t(unsigned long, max_perf, cpudata->min_limit_perf,\n\t\t\tcpudata->max_limit_perf);\n\tdes_perf = clamp_t(unsigned long, des_perf, min_perf, max_perf);\n\n\tif ((cppc_state == AMD_PSTATE_GUIDED) && (gov_flags & CPUFREQ_GOV_DYNAMIC_SWITCHING)) {\n\t\tmin_perf = des_perf;\n\t\tdes_perf = 0;\n\t}\n\n\tvalue &= ~AMD_CPPC_MIN_PERF(~0L);\n\tvalue |= AMD_CPPC_MIN_PERF(min_perf);\n\n\tvalue &= ~AMD_CPPC_DES_PERF(~0L);\n\tvalue |= AMD_CPPC_DES_PERF(des_perf);\n\n\tvalue &= ~AMD_CPPC_MAX_PERF(~0L);\n\tvalue |= AMD_CPPC_MAX_PERF(max_perf);\n\n\tif (trace_amd_pstate_perf_enabled() && amd_pstate_sample(cpudata)) {\n\t\ttrace_amd_pstate_perf(min_perf, des_perf, max_perf, cpudata->freq,\n\t\t\tcpudata->cur.mperf, cpudata->cur.aperf, cpudata->cur.tsc,\n\t\t\t\tcpudata->cpu, (value != prev), fast_switch);\n\t}\n\n\tif (value == prev)\n\t\treturn;\n\n\tWRITE_ONCE(cpudata->cppc_req_cached, value);\n\n\tamd_pstate_update_perf(cpudata, min_perf, des_perf,\n\t\t\t       max_perf, fast_switch);\n}\n\nstatic int amd_pstate_verify(struct cpufreq_policy_data *policy)\n{\n\tcpufreq_verify_within_cpu_limits(policy);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_update_min_max_limit(struct cpufreq_policy *policy)\n{\n\tu32 max_limit_perf, min_limit_perf;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tmax_limit_perf = div_u64(policy->max * cpudata->highest_perf, cpudata->max_freq);\n\tmin_limit_perf = div_u64(policy->min * cpudata->highest_perf, cpudata->max_freq);\n\n\tWRITE_ONCE(cpudata->max_limit_perf, max_limit_perf);\n\tWRITE_ONCE(cpudata->min_limit_perf, min_limit_perf);\n\tWRITE_ONCE(cpudata->max_limit_freq, policy->max);\n\tWRITE_ONCE(cpudata->min_limit_freq, policy->min);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_update_freq(struct cpufreq_policy *policy,\n\t\t\t\t  unsigned int target_freq, bool fast_switch)\n{\n\tstruct cpufreq_freqs freqs;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tunsigned long max_perf, min_perf, des_perf, cap_perf;\n\n\tif (!cpudata->max_freq)\n\t\treturn -ENODEV;\n\n\tif (policy->min != cpudata->min_limit_freq || policy->max != cpudata->max_limit_freq)\n\t\tamd_pstate_update_min_max_limit(policy);\n\n\tcap_perf = READ_ONCE(cpudata->highest_perf);\n\tmin_perf = READ_ONCE(cpudata->lowest_perf);\n\tmax_perf = cap_perf;\n\n\tfreqs.old = policy->cur;\n\tfreqs.new = target_freq;\n\n\tdes_perf = DIV_ROUND_CLOSEST(target_freq * cap_perf,\n\t\t\t\t     cpudata->max_freq);\n\n\tWARN_ON(fast_switch && !policy->fast_switch_enabled);\n\t \n\tif (!fast_switch)\n\t\tcpufreq_freq_transition_begin(policy, &freqs);\n\n\tamd_pstate_update(cpudata, min_perf, des_perf,\n\t\t\tmax_perf, fast_switch, policy->governor->flags);\n\n\tif (!fast_switch)\n\t\tcpufreq_freq_transition_end(policy, &freqs, false);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_target(struct cpufreq_policy *policy,\n\t\t\t     unsigned int target_freq,\n\t\t\t     unsigned int relation)\n{\n\treturn amd_pstate_update_freq(policy, target_freq, false);\n}\n\nstatic unsigned int amd_pstate_fast_switch(struct cpufreq_policy *policy,\n\t\t\t\t  unsigned int target_freq)\n{\n\tif (!amd_pstate_update_freq(policy, target_freq, true))\n\t\treturn target_freq;\n\treturn policy->cur;\n}\n\nstatic void amd_pstate_adjust_perf(unsigned int cpu,\n\t\t\t\t   unsigned long _min_perf,\n\t\t\t\t   unsigned long target_perf,\n\t\t\t\t   unsigned long capacity)\n{\n\tunsigned long max_perf, min_perf, des_perf,\n\t\t      cap_perf, lowest_nonlinear_perf, max_freq;\n\tstruct cpufreq_policy *policy = cpufreq_cpu_get(cpu);\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tunsigned int target_freq;\n\n\tif (policy->min != cpudata->min_limit_freq || policy->max != cpudata->max_limit_freq)\n\t\tamd_pstate_update_min_max_limit(policy);\n\n\n\tcap_perf = READ_ONCE(cpudata->highest_perf);\n\tlowest_nonlinear_perf = READ_ONCE(cpudata->lowest_nonlinear_perf);\n\tmax_freq = READ_ONCE(cpudata->max_freq);\n\n\tdes_perf = cap_perf;\n\tif (target_perf < capacity)\n\t\tdes_perf = DIV_ROUND_UP(cap_perf * target_perf, capacity);\n\n\tmin_perf = READ_ONCE(cpudata->highest_perf);\n\tif (_min_perf < capacity)\n\t\tmin_perf = DIV_ROUND_UP(cap_perf * _min_perf, capacity);\n\n\tif (min_perf < lowest_nonlinear_perf)\n\t\tmin_perf = lowest_nonlinear_perf;\n\n\tmax_perf = cap_perf;\n\tif (max_perf < min_perf)\n\t\tmax_perf = min_perf;\n\n\tdes_perf = clamp_t(unsigned long, des_perf, min_perf, max_perf);\n\ttarget_freq = div_u64(des_perf * max_freq, max_perf);\n\tpolicy->cur = target_freq;\n\n\tamd_pstate_update(cpudata, min_perf, des_perf, max_perf, true,\n\t\t\tpolicy->governor->flags);\n\tcpufreq_cpu_put(policy);\n}\n\nstatic int amd_get_min_freq(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_caps cppc_perf;\n\n\tint ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn cppc_perf.lowest_freq * 1000;\n}\n\nstatic int amd_get_max_freq(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_caps cppc_perf;\n\tu32 max_perf, max_freq, nominal_freq, nominal_perf;\n\tu64 boost_ratio;\n\n\tint ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);\n\tif (ret)\n\t\treturn ret;\n\n\tnominal_freq = cppc_perf.nominal_freq;\n\tnominal_perf = READ_ONCE(cpudata->nominal_perf);\n\tmax_perf = READ_ONCE(cpudata->highest_perf);\n\n\tboost_ratio = div_u64(max_perf << SCHED_CAPACITY_SHIFT,\n\t\t\t      nominal_perf);\n\n\tmax_freq = nominal_freq * boost_ratio >> SCHED_CAPACITY_SHIFT;\n\n\t \n\treturn max_freq * 1000;\n}\n\nstatic int amd_get_nominal_freq(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_caps cppc_perf;\n\n\tint ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn cppc_perf.nominal_freq * 1000;\n}\n\nstatic int amd_get_lowest_nonlinear_freq(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_caps cppc_perf;\n\tu32 lowest_nonlinear_freq, lowest_nonlinear_perf,\n\t    nominal_freq, nominal_perf;\n\tu64 lowest_nonlinear_ratio;\n\n\tint ret = cppc_get_perf_caps(cpudata->cpu, &cppc_perf);\n\tif (ret)\n\t\treturn ret;\n\n\tnominal_freq = cppc_perf.nominal_freq;\n\tnominal_perf = READ_ONCE(cpudata->nominal_perf);\n\n\tlowest_nonlinear_perf = cppc_perf.lowest_nonlinear_perf;\n\n\tlowest_nonlinear_ratio = div_u64(lowest_nonlinear_perf << SCHED_CAPACITY_SHIFT,\n\t\t\t\t\t nominal_perf);\n\n\tlowest_nonlinear_freq = nominal_freq * lowest_nonlinear_ratio >> SCHED_CAPACITY_SHIFT;\n\n\t \n\treturn lowest_nonlinear_freq * 1000;\n}\n\nstatic int amd_pstate_set_boost(struct cpufreq_policy *policy, int state)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tint ret;\n\n\tif (!cpudata->boost_supported) {\n\t\tpr_err(\"Boost mode is not supported by this processor or SBIOS\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (state)\n\t\tpolicy->cpuinfo.max_freq = cpudata->max_freq;\n\telse\n\t\tpolicy->cpuinfo.max_freq = cpudata->nominal_freq;\n\n\tpolicy->max = policy->cpuinfo.max_freq;\n\n\tret = freq_qos_update_request(&cpudata->req[1],\n\t\t\t\t      policy->cpuinfo.max_freq);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic void amd_pstate_boost_init(struct amd_cpudata *cpudata)\n{\n\tu32 highest_perf, nominal_perf;\n\n\thighest_perf = READ_ONCE(cpudata->highest_perf);\n\tnominal_perf = READ_ONCE(cpudata->nominal_perf);\n\n\tif (highest_perf <= nominal_perf)\n\t\treturn;\n\n\tcpudata->boost_supported = true;\n\tcurrent_pstate_driver->boost_enabled = true;\n}\n\nstatic void amd_perf_ctl_reset(unsigned int cpu)\n{\n\twrmsrl_on_cpu(cpu, MSR_AMD_PERF_CTL, 0);\n}\n\nstatic int amd_pstate_cpu_init(struct cpufreq_policy *policy)\n{\n\tint min_freq, max_freq, nominal_freq, lowest_nonlinear_freq, ret;\n\tstruct device *dev;\n\tstruct amd_cpudata *cpudata;\n\n\t \n\tamd_perf_ctl_reset(policy->cpu);\n\tdev = get_cpu_device(policy->cpu);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tcpudata = kzalloc(sizeof(*cpudata), GFP_KERNEL);\n\tif (!cpudata)\n\t\treturn -ENOMEM;\n\n\tcpudata->cpu = policy->cpu;\n\n\tret = amd_pstate_init_perf(cpudata);\n\tif (ret)\n\t\tgoto free_cpudata1;\n\n\tmin_freq = amd_get_min_freq(cpudata);\n\tmax_freq = amd_get_max_freq(cpudata);\n\tnominal_freq = amd_get_nominal_freq(cpudata);\n\tlowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);\n\n\tif (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {\n\t\tdev_err(dev, \"min_freq(%d) or max_freq(%d) value is incorrect\\n\",\n\t\t\tmin_freq, max_freq);\n\t\tret = -EINVAL;\n\t\tgoto free_cpudata1;\n\t}\n\n\tpolicy->cpuinfo.transition_latency = AMD_PSTATE_TRANSITION_LATENCY;\n\tpolicy->transition_delay_us = AMD_PSTATE_TRANSITION_DELAY;\n\n\tpolicy->min = min_freq;\n\tpolicy->max = max_freq;\n\n\tpolicy->cpuinfo.min_freq = min_freq;\n\tpolicy->cpuinfo.max_freq = max_freq;\n\n\t \n\tpolicy->cur = policy->cpuinfo.min_freq;\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC))\n\t\tpolicy->fast_switch_possible = true;\n\n\tret = freq_qos_add_request(&policy->constraints, &cpudata->req[0],\n\t\t\t\t   FREQ_QOS_MIN, policy->cpuinfo.min_freq);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"Failed to add min-freq constraint (%d)\\n\", ret);\n\t\tgoto free_cpudata1;\n\t}\n\n\tret = freq_qos_add_request(&policy->constraints, &cpudata->req[1],\n\t\t\t\t   FREQ_QOS_MAX, policy->cpuinfo.max_freq);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"Failed to add max-freq constraint (%d)\\n\", ret);\n\t\tgoto free_cpudata2;\n\t}\n\n\t \n\tcpudata->max_freq = max_freq;\n\tcpudata->min_freq = min_freq;\n\tcpudata->max_limit_freq = max_freq;\n\tcpudata->min_limit_freq = min_freq;\n\tcpudata->nominal_freq = nominal_freq;\n\tcpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;\n\n\tpolicy->driver_data = cpudata;\n\n\tamd_pstate_boost_init(cpudata);\n\tif (!current_pstate_driver->adjust_perf)\n\t\tcurrent_pstate_driver->adjust_perf = amd_pstate_adjust_perf;\n\n\treturn 0;\n\nfree_cpudata2:\n\tfreq_qos_remove_request(&cpudata->req[0]);\nfree_cpudata1:\n\tkfree(cpudata);\n\treturn ret;\n}\n\nstatic int amd_pstate_cpu_exit(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tfreq_qos_remove_request(&cpudata->req[1]);\n\tfreq_qos_remove_request(&cpudata->req[0]);\n\tpolicy->fast_switch_possible = false;\n\tkfree(cpudata);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_cpu_resume(struct cpufreq_policy *policy)\n{\n\tint ret;\n\n\tret = amd_pstate_enable(true);\n\tif (ret)\n\t\tpr_err(\"failed to enable amd-pstate during resume, return %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int amd_pstate_cpu_suspend(struct cpufreq_policy *policy)\n{\n\tint ret;\n\n\tret = amd_pstate_enable(false);\n\tif (ret)\n\t\tpr_err(\"failed to disable amd-pstate during suspend, return %d\\n\", ret);\n\n\treturn ret;\n}\n\n \n\n \nstatic ssize_t show_amd_pstate_max_freq(struct cpufreq_policy *policy,\n\t\t\t\t\tchar *buf)\n{\n\tint max_freq;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tmax_freq = amd_get_max_freq(cpudata);\n\tif (max_freq < 0)\n\t\treturn max_freq;\n\n\treturn sysfs_emit(buf, \"%u\\n\", max_freq);\n}\n\nstatic ssize_t show_amd_pstate_lowest_nonlinear_freq(struct cpufreq_policy *policy,\n\t\t\t\t\t\t     char *buf)\n{\n\tint freq;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tfreq = amd_get_lowest_nonlinear_freq(cpudata);\n\tif (freq < 0)\n\t\treturn freq;\n\n\treturn sysfs_emit(buf, \"%u\\n\", freq);\n}\n\n \nstatic ssize_t show_amd_pstate_highest_perf(struct cpufreq_policy *policy,\n\t\t\t\t\t    char *buf)\n{\n\tu32 perf;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tperf = READ_ONCE(cpudata->highest_perf);\n\n\treturn sysfs_emit(buf, \"%u\\n\", perf);\n}\n\nstatic ssize_t show_energy_performance_available_preferences(\n\t\t\t\tstruct cpufreq_policy *policy, char *buf)\n{\n\tint i = 0;\n\tint offset = 0;\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tif (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)\n\t\treturn sysfs_emit_at(buf, offset, \"%s\\n\",\n\t\t\t\tenergy_perf_strings[EPP_INDEX_PERFORMANCE]);\n\n\twhile (energy_perf_strings[i] != NULL)\n\t\toffset += sysfs_emit_at(buf, offset, \"%s \", energy_perf_strings[i++]);\n\n\toffset += sysfs_emit_at(buf, offset, \"\\n\");\n\n\treturn offset;\n}\n\nstatic ssize_t store_energy_performance_preference(\n\t\tstruct cpufreq_policy *policy, const char *buf, size_t count)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tchar str_preference[21];\n\tssize_t ret;\n\n\tret = sscanf(buf, \"%20s\", str_preference);\n\tif (ret != 1)\n\t\treturn -EINVAL;\n\n\tret = match_string(energy_perf_strings, -1, str_preference);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&amd_pstate_limits_lock);\n\tret = amd_pstate_set_energy_pref_index(cpudata, ret);\n\tmutex_unlock(&amd_pstate_limits_lock);\n\n\treturn ret ?: count;\n}\n\nstatic ssize_t show_energy_performance_preference(\n\t\t\t\tstruct cpufreq_policy *policy, char *buf)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tint preference;\n\n\tpreference = amd_pstate_get_energy_pref_index(cpudata);\n\tif (preference < 0)\n\t\treturn preference;\n\n\treturn sysfs_emit(buf, \"%s\\n\", energy_perf_strings[preference]);\n}\n\nstatic void amd_pstate_driver_cleanup(void)\n{\n\tamd_pstate_enable(false);\n\tcppc_state = AMD_PSTATE_DISABLE;\n\tcurrent_pstate_driver = NULL;\n}\n\nstatic int amd_pstate_register_driver(int mode)\n{\n\tint ret;\n\n\tif (mode == AMD_PSTATE_PASSIVE || mode == AMD_PSTATE_GUIDED)\n\t\tcurrent_pstate_driver = &amd_pstate_driver;\n\telse if (mode == AMD_PSTATE_ACTIVE)\n\t\tcurrent_pstate_driver = &amd_pstate_epp_driver;\n\telse\n\t\treturn -EINVAL;\n\n\tcppc_state = mode;\n\tret = cpufreq_register_driver(current_pstate_driver);\n\tif (ret) {\n\t\tamd_pstate_driver_cleanup();\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int amd_pstate_unregister_driver(int dummy)\n{\n\tcpufreq_unregister_driver(current_pstate_driver);\n\tamd_pstate_driver_cleanup();\n\treturn 0;\n}\n\nstatic int amd_pstate_change_mode_without_dvr_change(int mode)\n{\n\tint cpu = 0;\n\n\tcppc_state = mode;\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC) || cppc_state == AMD_PSTATE_ACTIVE)\n\t\treturn 0;\n\n\tfor_each_present_cpu(cpu) {\n\t\tcppc_set_auto_sel(cpu, (cppc_state == AMD_PSTATE_PASSIVE) ? 0 : 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_pstate_change_driver_mode(int mode)\n{\n\tint ret;\n\n\tret = amd_pstate_unregister_driver(0);\n\tif (ret)\n\t\treturn ret;\n\n\tret = amd_pstate_register_driver(mode);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic cppc_mode_transition_fn mode_state_machine[AMD_PSTATE_MAX][AMD_PSTATE_MAX] = {\n\t[AMD_PSTATE_DISABLE]         = {\n\t\t[AMD_PSTATE_DISABLE]     = NULL,\n\t\t[AMD_PSTATE_PASSIVE]     = amd_pstate_register_driver,\n\t\t[AMD_PSTATE_ACTIVE]      = amd_pstate_register_driver,\n\t\t[AMD_PSTATE_GUIDED]      = amd_pstate_register_driver,\n\t},\n\t[AMD_PSTATE_PASSIVE]         = {\n\t\t[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,\n\t\t[AMD_PSTATE_PASSIVE]     = NULL,\n\t\t[AMD_PSTATE_ACTIVE]      = amd_pstate_change_driver_mode,\n\t\t[AMD_PSTATE_GUIDED]      = amd_pstate_change_mode_without_dvr_change,\n\t},\n\t[AMD_PSTATE_ACTIVE]          = {\n\t\t[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,\n\t\t[AMD_PSTATE_PASSIVE]     = amd_pstate_change_driver_mode,\n\t\t[AMD_PSTATE_ACTIVE]      = NULL,\n\t\t[AMD_PSTATE_GUIDED]      = amd_pstate_change_driver_mode,\n\t},\n\t[AMD_PSTATE_GUIDED]          = {\n\t\t[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,\n\t\t[AMD_PSTATE_PASSIVE]     = amd_pstate_change_mode_without_dvr_change,\n\t\t[AMD_PSTATE_ACTIVE]      = amd_pstate_change_driver_mode,\n\t\t[AMD_PSTATE_GUIDED]      = NULL,\n\t},\n};\n\nstatic ssize_t amd_pstate_show_status(char *buf)\n{\n\tif (!current_pstate_driver)\n\t\treturn sysfs_emit(buf, \"disable\\n\");\n\n\treturn sysfs_emit(buf, \"%s\\n\", amd_pstate_mode_string[cppc_state]);\n}\n\nstatic int amd_pstate_update_status(const char *buf, size_t size)\n{\n\tint mode_idx;\n\n\tif (size > strlen(\"passive\") || size < strlen(\"active\"))\n\t\treturn -EINVAL;\n\n\tmode_idx = get_mode_idx_from_str(buf, size);\n\n\tif (mode_idx < 0 || mode_idx >= AMD_PSTATE_MAX)\n\t\treturn -EINVAL;\n\n\tif (mode_state_machine[cppc_state][mode_idx])\n\t\treturn mode_state_machine[cppc_state][mode_idx](mode_idx);\n\n\treturn 0;\n}\n\nstatic ssize_t status_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tssize_t ret;\n\n\tmutex_lock(&amd_pstate_driver_lock);\n\tret = amd_pstate_show_status(buf);\n\tmutex_unlock(&amd_pstate_driver_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t status_store(struct device *a, struct device_attribute *b,\n\t\t\t    const char *buf, size_t count)\n{\n\tchar *p = memchr(buf, '\\n', count);\n\tint ret;\n\n\tmutex_lock(&amd_pstate_driver_lock);\n\tret = amd_pstate_update_status(buf, p ? p - buf : count);\n\tmutex_unlock(&amd_pstate_driver_lock);\n\n\treturn ret < 0 ? ret : count;\n}\n\ncpufreq_freq_attr_ro(amd_pstate_max_freq);\ncpufreq_freq_attr_ro(amd_pstate_lowest_nonlinear_freq);\n\ncpufreq_freq_attr_ro(amd_pstate_highest_perf);\ncpufreq_freq_attr_rw(energy_performance_preference);\ncpufreq_freq_attr_ro(energy_performance_available_preferences);\nstatic DEVICE_ATTR_RW(status);\n\nstatic struct freq_attr *amd_pstate_attr[] = {\n\t&amd_pstate_max_freq,\n\t&amd_pstate_lowest_nonlinear_freq,\n\t&amd_pstate_highest_perf,\n\tNULL,\n};\n\nstatic struct freq_attr *amd_pstate_epp_attr[] = {\n\t&amd_pstate_max_freq,\n\t&amd_pstate_lowest_nonlinear_freq,\n\t&amd_pstate_highest_perf,\n\t&energy_performance_preference,\n\t&energy_performance_available_preferences,\n\tNULL,\n};\n\nstatic struct attribute *pstate_global_attributes[] = {\n\t&dev_attr_status.attr,\n\tNULL\n};\n\nstatic const struct attribute_group amd_pstate_global_attr_group = {\n\t.name = \"amd_pstate\",\n\t.attrs = pstate_global_attributes,\n};\n\nstatic bool amd_pstate_acpi_pm_profile_server(void)\n{\n\tswitch (acpi_gbl_FADT.preferred_profile) {\n\tcase PM_ENTERPRISE_SERVER:\n\tcase PM_SOHO_SERVER:\n\tcase PM_PERFORMANCE_SERVER:\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool amd_pstate_acpi_pm_profile_undefined(void)\n{\n\tif (acpi_gbl_FADT.preferred_profile == PM_UNSPECIFIED)\n\t\treturn true;\n\tif (acpi_gbl_FADT.preferred_profile >= NR_PM_PROFILES)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)\n{\n\tint min_freq, max_freq, nominal_freq, lowest_nonlinear_freq, ret;\n\tstruct amd_cpudata *cpudata;\n\tstruct device *dev;\n\tu64 value;\n\n\t \n\tamd_perf_ctl_reset(policy->cpu);\n\tdev = get_cpu_device(policy->cpu);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tcpudata = kzalloc(sizeof(*cpudata), GFP_KERNEL);\n\tif (!cpudata)\n\t\treturn -ENOMEM;\n\n\tcpudata->cpu = policy->cpu;\n\tcpudata->epp_policy = 0;\n\n\tret = amd_pstate_init_perf(cpudata);\n\tif (ret)\n\t\tgoto free_cpudata1;\n\n\tmin_freq = amd_get_min_freq(cpudata);\n\tmax_freq = amd_get_max_freq(cpudata);\n\tnominal_freq = amd_get_nominal_freq(cpudata);\n\tlowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);\n\tif (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {\n\t\tdev_err(dev, \"min_freq(%d) or max_freq(%d) value is incorrect\\n\",\n\t\t\t\tmin_freq, max_freq);\n\t\tret = -EINVAL;\n\t\tgoto free_cpudata1;\n\t}\n\n\tpolicy->cpuinfo.min_freq = min_freq;\n\tpolicy->cpuinfo.max_freq = max_freq;\n\t \n\tpolicy->cur = policy->cpuinfo.min_freq;\n\n\t \n\tcpudata->max_freq = max_freq;\n\tcpudata->min_freq = min_freq;\n\tcpudata->nominal_freq = nominal_freq;\n\tcpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;\n\n\tpolicy->driver_data = cpudata;\n\n\tcpudata->epp_cached = amd_pstate_get_epp(cpudata, 0);\n\n\tpolicy->min = policy->cpuinfo.min_freq;\n\tpolicy->max = policy->cpuinfo.max_freq;\n\n\t \n\tif (amd_pstate_acpi_pm_profile_server() ||\n\t    amd_pstate_acpi_pm_profile_undefined())\n\t\tpolicy->policy = CPUFREQ_POLICY_PERFORMANCE;\n\telse\n\t\tpolicy->policy = CPUFREQ_POLICY_POWERSAVE;\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, &value);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tWRITE_ONCE(cpudata->cppc_req_cached, value);\n\n\t\tret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_CAP1, &value);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tWRITE_ONCE(cpudata->cppc_cap1_cached, value);\n\t}\n\tamd_pstate_boost_init(cpudata);\n\n\treturn 0;\n\nfree_cpudata1:\n\tkfree(cpudata);\n\treturn ret;\n}\n\nstatic int amd_pstate_epp_cpu_exit(struct cpufreq_policy *policy)\n{\n\tpr_debug(\"CPU %d exiting\\n\", policy->cpu);\n\treturn 0;\n}\n\nstatic void amd_pstate_epp_update_limit(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tu32 max_perf, min_perf, min_limit_perf, max_limit_perf;\n\tu64 value;\n\ts16 epp;\n\n\tmax_perf = READ_ONCE(cpudata->highest_perf);\n\tmin_perf = READ_ONCE(cpudata->lowest_perf);\n\tmax_limit_perf = div_u64(policy->max * cpudata->highest_perf, cpudata->max_freq);\n\tmin_limit_perf = div_u64(policy->min * cpudata->highest_perf, cpudata->max_freq);\n\n\tmax_perf = clamp_t(unsigned long, max_perf, cpudata->min_limit_perf,\n\t\t\tcpudata->max_limit_perf);\n\tmin_perf = clamp_t(unsigned long, min_perf, cpudata->min_limit_perf,\n\t\t\tcpudata->max_limit_perf);\n\n\tWRITE_ONCE(cpudata->max_limit_perf, max_limit_perf);\n\tWRITE_ONCE(cpudata->min_limit_perf, min_limit_perf);\n\n\tvalue = READ_ONCE(cpudata->cppc_req_cached);\n\n\tif (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)\n\t\tmin_perf = max_perf;\n\n\t \n\tvalue &= ~AMD_CPPC_MIN_PERF(~0L);\n\tvalue |= AMD_CPPC_MIN_PERF(min_perf);\n\n\tvalue &= ~AMD_CPPC_MAX_PERF(~0L);\n\tvalue |= AMD_CPPC_MAX_PERF(max_perf);\n\n\t \n\tvalue &= ~AMD_CPPC_DES_PERF(~0L);\n\tvalue |= AMD_CPPC_DES_PERF(0);\n\n\tcpudata->epp_policy = cpudata->policy;\n\n\t \n\tepp = amd_pstate_get_epp(cpudata, value);\n\tif (epp < 0) {\n\t\t \n\t\treturn;\n\t}\n\n\tif (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)\n\t\tepp = 0;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tvalue &= ~GENMASK_ULL(31, 24);\n\t\tvalue |= (u64)epp << 24;\n\t}\n\n\tWRITE_ONCE(cpudata->cppc_req_cached, value);\n\tamd_pstate_set_epp(cpudata, epp);\n}\n\nstatic int amd_pstate_epp_set_policy(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tif (!policy->cpuinfo.max_freq)\n\t\treturn -ENODEV;\n\n\tpr_debug(\"set_policy: cpuinfo.max %u policy->max %u\\n\",\n\t\t\t\tpolicy->cpuinfo.max_freq, policy->max);\n\n\tcpudata->policy = policy->policy;\n\n\tamd_pstate_epp_update_limit(policy);\n\n\treturn 0;\n}\n\nstatic void amd_pstate_epp_reenable(struct amd_cpudata *cpudata)\n{\n\tstruct cppc_perf_ctrls perf_ctrls;\n\tu64 value, max_perf;\n\tint ret;\n\n\tret = amd_pstate_enable(true);\n\tif (ret)\n\t\tpr_err(\"failed to enable amd pstate during resume, return %d\\n\", ret);\n\n\tvalue = READ_ONCE(cpudata->cppc_req_cached);\n\tmax_perf = READ_ONCE(cpudata->highest_perf);\n\n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\twrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);\n\t} else {\n\t\tperf_ctrls.max_perf = max_perf;\n\t\tperf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(cpudata->epp_cached);\n\t\tcppc_set_perf(cpudata->cpu, &perf_ctrls);\n\t}\n}\n\nstatic int amd_pstate_epp_cpu_online(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tpr_debug(\"AMD CPU Core %d going online\\n\", cpudata->cpu);\n\n\tif (cppc_state == AMD_PSTATE_ACTIVE) {\n\t\tamd_pstate_epp_reenable(cpudata);\n\t\tcpudata->suspended = false;\n\t}\n\n\treturn 0;\n}\n\nstatic void amd_pstate_epp_offline(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tstruct cppc_perf_ctrls perf_ctrls;\n\tint min_perf;\n\tu64 value;\n\n\tmin_perf = READ_ONCE(cpudata->lowest_perf);\n\tvalue = READ_ONCE(cpudata->cppc_req_cached);\n\n\tmutex_lock(&amd_pstate_limits_lock);\n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tcpudata->epp_policy = CPUFREQ_POLICY_UNKNOWN;\n\n\t\t \n\t\tvalue &= ~AMD_CPPC_MAX_PERF(~0L);\n\t\tvalue |= AMD_CPPC_MAX_PERF(min_perf);\n\t\tvalue &= ~AMD_CPPC_MIN_PERF(~0L);\n\t\tvalue |= AMD_CPPC_MIN_PERF(min_perf);\n\t\twrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);\n\t} else {\n\t\tperf_ctrls.desired_perf = 0;\n\t\tperf_ctrls.max_perf = min_perf;\n\t\tperf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(HWP_EPP_BALANCE_POWERSAVE);\n\t\tcppc_set_perf(cpudata->cpu, &perf_ctrls);\n\t}\n\tmutex_unlock(&amd_pstate_limits_lock);\n}\n\nstatic int amd_pstate_epp_cpu_offline(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tpr_debug(\"AMD CPU Core %d going offline\\n\", cpudata->cpu);\n\n\tif (cpudata->suspended)\n\t\treturn 0;\n\n\tif (cppc_state == AMD_PSTATE_ACTIVE)\n\t\tamd_pstate_epp_offline(policy);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_epp_verify_policy(struct cpufreq_policy_data *policy)\n{\n\tcpufreq_verify_within_cpu_limits(policy);\n\tpr_debug(\"policy_max =%d, policy_min=%d\\n\", policy->max, policy->min);\n\treturn 0;\n}\n\nstatic int amd_pstate_epp_suspend(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\tint ret;\n\n\t \n\tif (cppc_state != AMD_PSTATE_ACTIVE)\n\t\treturn 0;\n\n\t \n\tcpudata->suspended = true;\n\n\t \n\tret = amd_pstate_enable(false);\n\tif (ret)\n\t\tpr_err(\"failed to suspend, return %d\\n\", ret);\n\n\treturn 0;\n}\n\nstatic int amd_pstate_epp_resume(struct cpufreq_policy *policy)\n{\n\tstruct amd_cpudata *cpudata = policy->driver_data;\n\n\tif (cpudata->suspended) {\n\t\tmutex_lock(&amd_pstate_limits_lock);\n\n\t\t \n\t\tamd_pstate_epp_reenable(cpudata);\n\n\t\tmutex_unlock(&amd_pstate_limits_lock);\n\n\t\tcpudata->suspended = false;\n\t}\n\n\treturn 0;\n}\n\nstatic struct cpufreq_driver amd_pstate_driver = {\n\t.flags\t\t= CPUFREQ_CONST_LOOPS | CPUFREQ_NEED_UPDATE_LIMITS,\n\t.verify\t\t= amd_pstate_verify,\n\t.target\t\t= amd_pstate_target,\n\t.fast_switch    = amd_pstate_fast_switch,\n\t.init\t\t= amd_pstate_cpu_init,\n\t.exit\t\t= amd_pstate_cpu_exit,\n\t.suspend\t= amd_pstate_cpu_suspend,\n\t.resume\t\t= amd_pstate_cpu_resume,\n\t.set_boost\t= amd_pstate_set_boost,\n\t.name\t\t= \"amd-pstate\",\n\t.attr\t\t= amd_pstate_attr,\n};\n\nstatic struct cpufreq_driver amd_pstate_epp_driver = {\n\t.flags\t\t= CPUFREQ_CONST_LOOPS,\n\t.verify\t\t= amd_pstate_epp_verify_policy,\n\t.setpolicy\t= amd_pstate_epp_set_policy,\n\t.init\t\t= amd_pstate_epp_cpu_init,\n\t.exit\t\t= amd_pstate_epp_cpu_exit,\n\t.offline\t= amd_pstate_epp_cpu_offline,\n\t.online\t\t= amd_pstate_epp_cpu_online,\n\t.suspend\t= amd_pstate_epp_suspend,\n\t.resume\t\t= amd_pstate_epp_resume,\n\t.name\t\t= \"amd-pstate-epp\",\n\t.attr\t\t= amd_pstate_epp_attr,\n};\n\nstatic int __init amd_pstate_set_driver(int mode_idx)\n{\n\tif (mode_idx >= AMD_PSTATE_DISABLE && mode_idx < AMD_PSTATE_MAX) {\n\t\tcppc_state = mode_idx;\n\t\tif (cppc_state == AMD_PSTATE_DISABLE)\n\t\t\tpr_info(\"driver is explicitly disabled\\n\");\n\n\t\tif (cppc_state == AMD_PSTATE_ACTIVE)\n\t\t\tcurrent_pstate_driver = &amd_pstate_epp_driver;\n\n\t\tif (cppc_state == AMD_PSTATE_PASSIVE || cppc_state == AMD_PSTATE_GUIDED)\n\t\t\tcurrent_pstate_driver = &amd_pstate_driver;\n\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int __init amd_pstate_init(void)\n{\n\tstruct device *dev_root;\n\tint ret;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)\n\t\treturn -ENODEV;\n\n\tif (!acpi_cpc_valid()) {\n\t\tpr_warn_once(\"the _CPC object is not present in SBIOS or ACPI disabled\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tif (cpufreq_get_current_driver())\n\t\treturn -EEXIST;\n\n\tswitch (cppc_state) {\n\tcase AMD_PSTATE_UNDEFINED:\n\t\t \n\t\tif (amd_pstate_acpi_pm_profile_undefined() ||\n\t\t    amd_pstate_acpi_pm_profile_server() ||\n\t\t    !boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\t\tpr_info(\"driver load is disabled, boot with specific mode to enable this\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tret = amd_pstate_set_driver(CONFIG_X86_AMD_PSTATE_DEFAULT_MODE);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase AMD_PSTATE_DISABLE:\n\t\treturn -ENODEV;\n\tcase AMD_PSTATE_PASSIVE:\n\tcase AMD_PSTATE_ACTIVE:\n\tcase AMD_PSTATE_GUIDED:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_CPPC)) {\n\t\tpr_debug(\"AMD CPPC MSR based functionality is supported\\n\");\n\t\tif (cppc_state != AMD_PSTATE_ACTIVE)\n\t\t\tcurrent_pstate_driver->adjust_perf = amd_pstate_adjust_perf;\n\t} else {\n\t\tpr_debug(\"AMD CPPC shared memory based functionality is supported\\n\");\n\t\tstatic_call_update(amd_pstate_enable, cppc_enable);\n\t\tstatic_call_update(amd_pstate_init_perf, cppc_init_perf);\n\t\tstatic_call_update(amd_pstate_update_perf, cppc_update_perf);\n\t}\n\n\t \n\tret = amd_pstate_enable(true);\n\tif (ret) {\n\t\tpr_err(\"failed to enable with return %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = cpufreq_register_driver(current_pstate_driver);\n\tif (ret)\n\t\tpr_err(\"failed to register with return %d\\n\", ret);\n\n\tdev_root = bus_get_dev_root(&cpu_subsys);\n\tif (dev_root) {\n\t\tret = sysfs_create_group(&dev_root->kobj, &amd_pstate_global_attr_group);\n\t\tput_device(dev_root);\n\t\tif (ret) {\n\t\t\tpr_err(\"sysfs attribute export failed with error %d.\\n\", ret);\n\t\t\tgoto global_attr_free;\n\t\t}\n\t}\n\n\treturn ret;\n\nglobal_attr_free:\n\tcpufreq_unregister_driver(current_pstate_driver);\n\treturn ret;\n}\ndevice_initcall(amd_pstate_init);\n\nstatic int __init amd_pstate_param(char *str)\n{\n\tsize_t size;\n\tint mode_idx;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tsize = strlen(str);\n\tmode_idx = get_mode_idx_from_str(str, size);\n\n\treturn amd_pstate_set_driver(mode_idx);\n}\nearly_param(\"amd_pstate\", amd_pstate_param);\n\nMODULE_AUTHOR(\"Huang Rui <ray.huang@amd.com>\");\nMODULE_DESCRIPTION(\"AMD Processor P-state Frequency Driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}