{
  "module_name": "cppc_cpufreq.c",
  "hash_id": "65b82f77672118b12eeee3e60624133fa47d7ff744ff9d50915031aabd924ff0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cpufreq/cppc_cpufreq.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"CPPC Cpufreq:\"\tfmt\n\n#include <linux/arch_topology.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/cpu.h>\n#include <linux/cpufreq.h>\n#include <linux/dmi.h>\n#include <linux/irq_work.h>\n#include <linux/kthread.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <uapi/linux/sched/types.h>\n\n#include <asm/unaligned.h>\n\n#include <acpi/cppc_acpi.h>\n\n \n#define DMI_ENTRY_PROCESSOR_MIN_LENGTH\t48\n\n \n#define DMI_PROCESSOR_MAX_SPEED\t\t0x14\n\n \nstatic LIST_HEAD(cpu_data_list);\n\nstatic bool boost_supported;\n\nstruct cppc_workaround_oem_info {\n\tchar oem_id[ACPI_OEM_ID_SIZE + 1];\n\tchar oem_table_id[ACPI_OEM_TABLE_ID_SIZE + 1];\n\tu32 oem_revision;\n};\n\nstatic struct cppc_workaround_oem_info wa_info[] = {\n\t{\n\t\t.oem_id\t\t= \"HISI  \",\n\t\t.oem_table_id\t= \"HIP07   \",\n\t\t.oem_revision\t= 0,\n\t}, {\n\t\t.oem_id\t\t= \"HISI  \",\n\t\t.oem_table_id\t= \"HIP08   \",\n\t\t.oem_revision\t= 0,\n\t}\n};\n\nstatic struct cpufreq_driver cppc_cpufreq_driver;\n\nstatic enum {\n\tFIE_UNSET = -1,\n\tFIE_ENABLED,\n\tFIE_DISABLED\n} fie_disabled = FIE_UNSET;\n\n#ifdef CONFIG_ACPI_CPPC_CPUFREQ_FIE\nmodule_param(fie_disabled, int, 0444);\nMODULE_PARM_DESC(fie_disabled, \"Disable Frequency Invariance Engine (FIE)\");\n\n \nstruct cppc_freq_invariance {\n\tint cpu;\n\tstruct irq_work irq_work;\n\tstruct kthread_work work;\n\tstruct cppc_perf_fb_ctrs prev_perf_fb_ctrs;\n\tstruct cppc_cpudata *cpu_data;\n};\n\nstatic DEFINE_PER_CPU(struct cppc_freq_invariance, cppc_freq_inv);\nstatic struct kthread_worker *kworker_fie;\n\nstatic unsigned int hisi_cppc_cpufreq_get_rate(unsigned int cpu);\nstatic int cppc_perf_from_fbctrs(struct cppc_cpudata *cpu_data,\n\t\t\t\t struct cppc_perf_fb_ctrs *fb_ctrs_t0,\n\t\t\t\t struct cppc_perf_fb_ctrs *fb_ctrs_t1);\n\n \nstatic void cppc_scale_freq_workfn(struct kthread_work *work)\n{\n\tstruct cppc_freq_invariance *cppc_fi;\n\tstruct cppc_perf_fb_ctrs fb_ctrs = {0};\n\tstruct cppc_cpudata *cpu_data;\n\tunsigned long local_freq_scale;\n\tu64 perf;\n\n\tcppc_fi = container_of(work, struct cppc_freq_invariance, work);\n\tcpu_data = cppc_fi->cpu_data;\n\n\tif (cppc_get_perf_ctrs(cppc_fi->cpu, &fb_ctrs)) {\n\t\tpr_warn(\"%s: failed to read perf counters\\n\", __func__);\n\t\treturn;\n\t}\n\n\tperf = cppc_perf_from_fbctrs(cpu_data, &cppc_fi->prev_perf_fb_ctrs,\n\t\t\t\t     &fb_ctrs);\n\tcppc_fi->prev_perf_fb_ctrs = fb_ctrs;\n\n\tperf <<= SCHED_CAPACITY_SHIFT;\n\tlocal_freq_scale = div64_u64(perf, cpu_data->perf_caps.highest_perf);\n\n\t \n\tif (unlikely(local_freq_scale > 1024))\n\t\tlocal_freq_scale = 1024;\n\n\tper_cpu(arch_freq_scale, cppc_fi->cpu) = local_freq_scale;\n}\n\nstatic void cppc_irq_work(struct irq_work *irq_work)\n{\n\tstruct cppc_freq_invariance *cppc_fi;\n\n\tcppc_fi = container_of(irq_work, struct cppc_freq_invariance, irq_work);\n\tkthread_queue_work(kworker_fie, &cppc_fi->work);\n}\n\nstatic void cppc_scale_freq_tick(void)\n{\n\tstruct cppc_freq_invariance *cppc_fi = &per_cpu(cppc_freq_inv, smp_processor_id());\n\n\t \n\tirq_work_queue(&cppc_fi->irq_work);\n}\n\nstatic struct scale_freq_data cppc_sftd = {\n\t.source = SCALE_FREQ_SOURCE_CPPC,\n\t.set_freq_scale = cppc_scale_freq_tick,\n};\n\nstatic void cppc_cpufreq_cpu_fie_init(struct cpufreq_policy *policy)\n{\n\tstruct cppc_freq_invariance *cppc_fi;\n\tint cpu, ret;\n\n\tif (fie_disabled)\n\t\treturn;\n\n\tfor_each_cpu(cpu, policy->cpus) {\n\t\tcppc_fi = &per_cpu(cppc_freq_inv, cpu);\n\t\tcppc_fi->cpu = cpu;\n\t\tcppc_fi->cpu_data = policy->driver_data;\n\t\tkthread_init_work(&cppc_fi->work, cppc_scale_freq_workfn);\n\t\tinit_irq_work(&cppc_fi->irq_work, cppc_irq_work);\n\n\t\tret = cppc_get_perf_ctrs(cpu, &cppc_fi->prev_perf_fb_ctrs);\n\t\tif (ret) {\n\t\t\tpr_warn(\"%s: failed to read perf counters for cpu:%d: %d\\n\",\n\t\t\t\t__func__, cpu, ret);\n\n\t\t\t \n\t\t\tif (cpu_online(cpu))\n\t\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\ttopology_set_scale_freq_source(&cppc_sftd, policy->cpus);\n}\n\n \nstatic void cppc_cpufreq_cpu_fie_exit(struct cpufreq_policy *policy)\n{\n\tstruct cppc_freq_invariance *cppc_fi;\n\tint cpu;\n\n\tif (fie_disabled)\n\t\treturn;\n\n\t \n\ttopology_clear_scale_freq_source(SCALE_FREQ_SOURCE_CPPC, policy->related_cpus);\n\n\tfor_each_cpu(cpu, policy->related_cpus) {\n\t\tcppc_fi = &per_cpu(cppc_freq_inv, cpu);\n\t\tirq_work_sync(&cppc_fi->irq_work);\n\t\tkthread_cancel_work_sync(&cppc_fi->work);\n\t}\n}\n\nstatic void __init cppc_freq_invariance_init(void)\n{\n\tstruct sched_attr attr = {\n\t\t.size\t\t= sizeof(struct sched_attr),\n\t\t.sched_policy\t= SCHED_DEADLINE,\n\t\t.sched_nice\t= 0,\n\t\t.sched_priority\t= 0,\n\t\t \n\t\t.sched_runtime\t= 1000000,\n\t\t.sched_deadline = 10000000,\n\t\t.sched_period\t= 10000000,\n\t};\n\tint ret;\n\n\tif (fie_disabled != FIE_ENABLED && fie_disabled != FIE_DISABLED) {\n\t\tfie_disabled = FIE_ENABLED;\n\t\tif (cppc_perf_ctrs_in_pcc()) {\n\t\t\tpr_info(\"FIE not enabled on systems with registers in PCC\\n\");\n\t\t\tfie_disabled = FIE_DISABLED;\n\t\t}\n\t}\n\n\tif (fie_disabled)\n\t\treturn;\n\n\tkworker_fie = kthread_create_worker(0, \"cppc_fie\");\n\tif (IS_ERR(kworker_fie)) {\n\t\tpr_warn(\"%s: failed to create kworker_fie: %ld\\n\", __func__,\n\t\t\tPTR_ERR(kworker_fie));\n\t\tfie_disabled = FIE_DISABLED;\n\t\treturn;\n\t}\n\n\tret = sched_setattr_nocheck(kworker_fie->task, &attr);\n\tif (ret) {\n\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE: %d\\n\", __func__,\n\t\t\tret);\n\t\tkthread_destroy_worker(kworker_fie);\n\t\tfie_disabled = FIE_DISABLED;\n\t}\n}\n\nstatic void cppc_freq_invariance_exit(void)\n{\n\tif (fie_disabled)\n\t\treturn;\n\n\tkthread_destroy_worker(kworker_fie);\n}\n\n#else\nstatic inline void cppc_cpufreq_cpu_fie_init(struct cpufreq_policy *policy)\n{\n}\n\nstatic inline void cppc_cpufreq_cpu_fie_exit(struct cpufreq_policy *policy)\n{\n}\n\nstatic inline void cppc_freq_invariance_init(void)\n{\n}\n\nstatic inline void cppc_freq_invariance_exit(void)\n{\n}\n#endif  \n\n \nstatic void cppc_find_dmi_mhz(const struct dmi_header *dm, void *private)\n{\n\tconst u8 *dmi_data = (const u8 *)dm;\n\tu16 *mhz = (u16 *)private;\n\n\tif (dm->type == DMI_ENTRY_PROCESSOR &&\n\t    dm->length >= DMI_ENTRY_PROCESSOR_MIN_LENGTH) {\n\t\tu16 val = (u16)get_unaligned((const u16 *)\n\t\t\t\t(dmi_data + DMI_PROCESSOR_MAX_SPEED));\n\t\t*mhz = val > *mhz ? val : *mhz;\n\t}\n}\n\n \nstatic u64 cppc_get_dmi_max_khz(void)\n{\n\tu16 mhz = 0;\n\n\tdmi_walk(cppc_find_dmi_mhz, &mhz);\n\n\t \n\tmhz = mhz ? mhz : 1;\n\n\treturn (1000 * mhz);\n}\n\n \nstatic unsigned int cppc_cpufreq_perf_to_khz(struct cppc_cpudata *cpu_data,\n\t\t\t\t\t     unsigned int perf)\n{\n\tstruct cppc_perf_caps *caps = &cpu_data->perf_caps;\n\ts64 retval, offset = 0;\n\tstatic u64 max_khz;\n\tu64 mul, div;\n\n\tif (caps->lowest_freq && caps->nominal_freq) {\n\t\tmul = caps->nominal_freq - caps->lowest_freq;\n\t\tdiv = caps->nominal_perf - caps->lowest_perf;\n\t\toffset = caps->nominal_freq - div64_u64(caps->nominal_perf * mul, div);\n\t} else {\n\t\tif (!max_khz)\n\t\t\tmax_khz = cppc_get_dmi_max_khz();\n\t\tmul = max_khz;\n\t\tdiv = caps->highest_perf;\n\t}\n\n\tretval = offset + div64_u64(perf * mul, div);\n\tif (retval >= 0)\n\t\treturn retval;\n\treturn 0;\n}\n\nstatic unsigned int cppc_cpufreq_khz_to_perf(struct cppc_cpudata *cpu_data,\n\t\t\t\t\t     unsigned int freq)\n{\n\tstruct cppc_perf_caps *caps = &cpu_data->perf_caps;\n\ts64 retval, offset = 0;\n\tstatic u64 max_khz;\n\tu64  mul, div;\n\n\tif (caps->lowest_freq && caps->nominal_freq) {\n\t\tmul = caps->nominal_perf - caps->lowest_perf;\n\t\tdiv = caps->nominal_freq - caps->lowest_freq;\n\t\toffset = caps->nominal_perf - div64_u64(caps->nominal_freq * mul, div);\n\t} else {\n\t\tif (!max_khz)\n\t\t\tmax_khz = cppc_get_dmi_max_khz();\n\t\tmul = caps->highest_perf;\n\t\tdiv = max_khz;\n\t}\n\n\tretval = offset + div64_u64(freq * mul, div);\n\tif (retval >= 0)\n\t\treturn retval;\n\treturn 0;\n}\n\nstatic int cppc_cpufreq_set_target(struct cpufreq_policy *policy,\n\t\t\t\t   unsigned int target_freq,\n\t\t\t\t   unsigned int relation)\n\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tunsigned int cpu = policy->cpu;\n\tstruct cpufreq_freqs freqs;\n\tu32 desired_perf;\n\tint ret = 0;\n\n\tdesired_perf = cppc_cpufreq_khz_to_perf(cpu_data, target_freq);\n\t \n\tif (desired_perf == cpu_data->perf_ctrls.desired_perf)\n\t\treturn ret;\n\n\tcpu_data->perf_ctrls.desired_perf = desired_perf;\n\tfreqs.old = policy->cur;\n\tfreqs.new = target_freq;\n\n\tcpufreq_freq_transition_begin(policy, &freqs);\n\tret = cppc_set_perf(cpu, &cpu_data->perf_ctrls);\n\tcpufreq_freq_transition_end(policy, &freqs, ret != 0);\n\n\tif (ret)\n\t\tpr_debug(\"Failed to set target on CPU:%d. ret:%d\\n\",\n\t\t\t cpu, ret);\n\n\treturn ret;\n}\n\nstatic unsigned int cppc_cpufreq_fast_switch(struct cpufreq_policy *policy,\n\t\t\t\t\t      unsigned int target_freq)\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tunsigned int cpu = policy->cpu;\n\tu32 desired_perf;\n\tint ret;\n\n\tdesired_perf = cppc_cpufreq_khz_to_perf(cpu_data, target_freq);\n\tcpu_data->perf_ctrls.desired_perf = desired_perf;\n\tret = cppc_set_perf(cpu, &cpu_data->perf_ctrls);\n\n\tif (ret) {\n\t\tpr_debug(\"Failed to set target on CPU:%d. ret:%d\\n\",\n\t\t\t cpu, ret);\n\t\treturn 0;\n\t}\n\n\treturn target_freq;\n}\n\nstatic int cppc_verify_policy(struct cpufreq_policy_data *policy)\n{\n\tcpufreq_verify_within_cpu_limits(policy);\n\treturn 0;\n}\n\n \n#ifdef CONFIG_ARM64\n#include <asm/cputype.h>\n\nstatic unsigned int cppc_cpufreq_get_transition_delay_us(unsigned int cpu)\n{\n\tunsigned long implementor = read_cpuid_implementor();\n\tunsigned long part_num = read_cpuid_part_number();\n\n\tswitch (implementor) {\n\tcase ARM_CPU_IMP_QCOM:\n\t\tswitch (part_num) {\n\t\tcase QCOM_CPU_PART_FALKOR_V1:\n\t\tcase QCOM_CPU_PART_FALKOR:\n\t\t\treturn 10000;\n\t\t}\n\t}\n\treturn cppc_get_transition_latency(cpu) / NSEC_PER_USEC;\n}\n#else\nstatic unsigned int cppc_cpufreq_get_transition_delay_us(unsigned int cpu)\n{\n\treturn cppc_get_transition_latency(cpu) / NSEC_PER_USEC;\n}\n#endif\n\n#if defined(CONFIG_ARM64) && defined(CONFIG_ENERGY_MODEL)\n\nstatic DEFINE_PER_CPU(unsigned int, efficiency_class);\nstatic void cppc_cpufreq_register_em(struct cpufreq_policy *policy);\n\n \n#define CPPC_EM_CAP_STEP\t(20)\n \n#define CPPC_EM_COST_STEP\t(1)\n \n#define CPPC_EM_COST_GAP\t(4 * SCHED_CAPACITY_SCALE * CPPC_EM_COST_STEP \\\n\t\t\t\t/ CPPC_EM_CAP_STEP)\n\nstatic unsigned int get_perf_level_count(struct cpufreq_policy *policy)\n{\n\tstruct cppc_perf_caps *perf_caps;\n\tunsigned int min_cap, max_cap;\n\tstruct cppc_cpudata *cpu_data;\n\tint cpu = policy->cpu;\n\n\tcpu_data = policy->driver_data;\n\tperf_caps = &cpu_data->perf_caps;\n\tmax_cap = arch_scale_cpu_capacity(cpu);\n\tmin_cap = div_u64((u64)max_cap * perf_caps->lowest_perf,\n\t\t\t  perf_caps->highest_perf);\n\tif ((min_cap == 0) || (max_cap < min_cap))\n\t\treturn 0;\n\treturn 1 + max_cap / CPPC_EM_CAP_STEP - min_cap / CPPC_EM_CAP_STEP;\n}\n\n \nstatic inline unsigned long compute_cost(int cpu, int step)\n{\n\treturn CPPC_EM_COST_GAP * per_cpu(efficiency_class, cpu) +\n\t\t\tstep * CPPC_EM_COST_STEP;\n}\n\nstatic int cppc_get_cpu_power(struct device *cpu_dev,\n\t\tunsigned long *power, unsigned long *KHz)\n{\n\tunsigned long perf_step, perf_prev, perf, perf_check;\n\tunsigned int min_step, max_step, step, step_check;\n\tunsigned long prev_freq = *KHz;\n\tunsigned int min_cap, max_cap;\n\tstruct cpufreq_policy *policy;\n\n\tstruct cppc_perf_caps *perf_caps;\n\tstruct cppc_cpudata *cpu_data;\n\n\tpolicy = cpufreq_cpu_get_raw(cpu_dev->id);\n\tcpu_data = policy->driver_data;\n\tperf_caps = &cpu_data->perf_caps;\n\tmax_cap = arch_scale_cpu_capacity(cpu_dev->id);\n\tmin_cap = div_u64((u64)max_cap * perf_caps->lowest_perf,\n\t\t\t  perf_caps->highest_perf);\n\tperf_step = div_u64((u64)CPPC_EM_CAP_STEP * perf_caps->highest_perf,\n\t\t\t    max_cap);\n\tmin_step = min_cap / CPPC_EM_CAP_STEP;\n\tmax_step = max_cap / CPPC_EM_CAP_STEP;\n\n\tperf_prev = cppc_cpufreq_khz_to_perf(cpu_data, *KHz);\n\tstep = perf_prev / perf_step;\n\n\tif (step > max_step)\n\t\treturn -EINVAL;\n\n\tif (min_step == max_step) {\n\t\tstep = max_step;\n\t\tperf = perf_caps->highest_perf;\n\t} else if (step < min_step) {\n\t\tstep = min_step;\n\t\tperf = perf_caps->lowest_perf;\n\t} else {\n\t\tstep++;\n\t\tif (step == max_step)\n\t\t\tperf = perf_caps->highest_perf;\n\t\telse\n\t\t\tperf = step * perf_step;\n\t}\n\n\t*KHz = cppc_cpufreq_perf_to_khz(cpu_data, perf);\n\tperf_check = cppc_cpufreq_khz_to_perf(cpu_data, *KHz);\n\tstep_check = perf_check / perf_step;\n\n\t \n\twhile ((*KHz == prev_freq) || (step_check != step)) {\n\t\tperf++;\n\t\t*KHz = cppc_cpufreq_perf_to_khz(cpu_data, perf);\n\t\tperf_check = cppc_cpufreq_khz_to_perf(cpu_data, *KHz);\n\t\tstep_check = perf_check / perf_step;\n\t}\n\n\t \n\t*power = compute_cost(cpu_dev->id, step);\n\n\treturn 0;\n}\n\nstatic int cppc_get_cpu_cost(struct device *cpu_dev, unsigned long KHz,\n\t\tunsigned long *cost)\n{\n\tunsigned long perf_step, perf_prev;\n\tstruct cppc_perf_caps *perf_caps;\n\tstruct cpufreq_policy *policy;\n\tstruct cppc_cpudata *cpu_data;\n\tunsigned int max_cap;\n\tint step;\n\n\tpolicy = cpufreq_cpu_get_raw(cpu_dev->id);\n\tcpu_data = policy->driver_data;\n\tperf_caps = &cpu_data->perf_caps;\n\tmax_cap = arch_scale_cpu_capacity(cpu_dev->id);\n\n\tperf_prev = cppc_cpufreq_khz_to_perf(cpu_data, KHz);\n\tperf_step = CPPC_EM_CAP_STEP * perf_caps->highest_perf / max_cap;\n\tstep = perf_prev / perf_step;\n\n\t*cost = compute_cost(cpu_dev->id, step);\n\n\treturn 0;\n}\n\nstatic int populate_efficiency_class(void)\n{\n\tstruct acpi_madt_generic_interrupt *gicc;\n\tDECLARE_BITMAP(used_classes, 256) = {};\n\tint class, cpu, index;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tgicc = acpi_cpu_get_madt_gicc(cpu);\n\t\tclass = gicc->efficiency_class;\n\t\tbitmap_set(used_classes, class, 1);\n\t}\n\n\tif (bitmap_weight(used_classes, 256) <= 1) {\n\t\tpr_debug(\"Efficiency classes are all equal (=%d). \"\n\t\t\t\"No EM registered\", class);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tindex = 0;\n\tfor_each_set_bit(class, used_classes, 256) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tgicc = acpi_cpu_get_madt_gicc(cpu);\n\t\t\tif (gicc->efficiency_class == class)\n\t\t\t\tper_cpu(efficiency_class, cpu) = index;\n\t\t}\n\t\tindex++;\n\t}\n\tcppc_cpufreq_driver.register_em = cppc_cpufreq_register_em;\n\n\treturn 0;\n}\n\nstatic void cppc_cpufreq_register_em(struct cpufreq_policy *policy)\n{\n\tstruct cppc_cpudata *cpu_data;\n\tstruct em_data_callback em_cb =\n\t\tEM_ADV_DATA_CB(cppc_get_cpu_power, cppc_get_cpu_cost);\n\n\tcpu_data = policy->driver_data;\n\tem_dev_register_perf_domain(get_cpu_device(policy->cpu),\n\t\t\tget_perf_level_count(policy), &em_cb,\n\t\t\tcpu_data->shared_cpu_map, 0);\n}\n\n#else\nstatic int populate_efficiency_class(void)\n{\n\treturn 0;\n}\n#endif\n\nstatic struct cppc_cpudata *cppc_cpufreq_get_cpu_data(unsigned int cpu)\n{\n\tstruct cppc_cpudata *cpu_data;\n\tint ret;\n\n\tcpu_data = kzalloc(sizeof(struct cppc_cpudata), GFP_KERNEL);\n\tif (!cpu_data)\n\t\tgoto out;\n\n\tif (!zalloc_cpumask_var(&cpu_data->shared_cpu_map, GFP_KERNEL))\n\t\tgoto free_cpu;\n\n\tret = acpi_get_psd_map(cpu, cpu_data);\n\tif (ret) {\n\t\tpr_debug(\"Err parsing CPU%d PSD data: ret:%d\\n\", cpu, ret);\n\t\tgoto free_mask;\n\t}\n\n\tret = cppc_get_perf_caps(cpu, &cpu_data->perf_caps);\n\tif (ret) {\n\t\tpr_debug(\"Err reading CPU%d perf caps: ret:%d\\n\", cpu, ret);\n\t\tgoto free_mask;\n\t}\n\n\t \n\tcpu_data->perf_caps.lowest_freq *= 1000;\n\tcpu_data->perf_caps.nominal_freq *= 1000;\n\n\tlist_add(&cpu_data->node, &cpu_data_list);\n\n\treturn cpu_data;\n\nfree_mask:\n\tfree_cpumask_var(cpu_data->shared_cpu_map);\nfree_cpu:\n\tkfree(cpu_data);\nout:\n\treturn NULL;\n}\n\nstatic void cppc_cpufreq_put_cpu_data(struct cpufreq_policy *policy)\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\n\tlist_del(&cpu_data->node);\n\tfree_cpumask_var(cpu_data->shared_cpu_map);\n\tkfree(cpu_data);\n\tpolicy->driver_data = NULL;\n}\n\nstatic int cppc_cpufreq_cpu_init(struct cpufreq_policy *policy)\n{\n\tunsigned int cpu = policy->cpu;\n\tstruct cppc_cpudata *cpu_data;\n\tstruct cppc_perf_caps *caps;\n\tint ret;\n\n\tcpu_data = cppc_cpufreq_get_cpu_data(cpu);\n\tif (!cpu_data) {\n\t\tpr_err(\"Error in acquiring _CPC/_PSD data for CPU%d.\\n\", cpu);\n\t\treturn -ENODEV;\n\t}\n\tcaps = &cpu_data->perf_caps;\n\tpolicy->driver_data = cpu_data;\n\n\t \n\tpolicy->min = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t       caps->lowest_nonlinear_perf);\n\tpolicy->max = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t       caps->nominal_perf);\n\n\t \n\tpolicy->cpuinfo.min_freq = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t\t\t    caps->lowest_perf);\n\tpolicy->cpuinfo.max_freq = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t\t\t    caps->nominal_perf);\n\n\tpolicy->transition_delay_us = cppc_cpufreq_get_transition_delay_us(cpu);\n\tpolicy->shared_type = cpu_data->shared_type;\n\n\tswitch (policy->shared_type) {\n\tcase CPUFREQ_SHARED_TYPE_HW:\n\tcase CPUFREQ_SHARED_TYPE_NONE:\n\t\t \n\t\tbreak;\n\tcase CPUFREQ_SHARED_TYPE_ANY:\n\t\t \n\t\tcpumask_copy(policy->cpus, cpu_data->shared_cpu_map);\n\t\tbreak;\n\tdefault:\n\t\tpr_debug(\"Unsupported CPU co-ord type: %d\\n\",\n\t\t\t policy->shared_type);\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tpolicy->fast_switch_possible = cppc_allow_fast_switch();\n\tpolicy->dvfs_possible_from_any_cpu = true;\n\n\t \n\tif (caps->highest_perf > caps->nominal_perf)\n\t\tboost_supported = true;\n\n\t \n\tpolicy->cur = cppc_cpufreq_perf_to_khz(cpu_data, caps->highest_perf);\n\tcpu_data->perf_ctrls.desired_perf =  caps->highest_perf;\n\n\tret = cppc_set_perf(cpu, &cpu_data->perf_ctrls);\n\tif (ret) {\n\t\tpr_debug(\"Err setting perf value:%d on CPU:%d. ret:%d\\n\",\n\t\t\t caps->highest_perf, cpu, ret);\n\t\tgoto out;\n\t}\n\n\tcppc_cpufreq_cpu_fie_init(policy);\n\treturn 0;\n\nout:\n\tcppc_cpufreq_put_cpu_data(policy);\n\treturn ret;\n}\n\nstatic int cppc_cpufreq_cpu_exit(struct cpufreq_policy *policy)\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tstruct cppc_perf_caps *caps = &cpu_data->perf_caps;\n\tunsigned int cpu = policy->cpu;\n\tint ret;\n\n\tcppc_cpufreq_cpu_fie_exit(policy);\n\n\tcpu_data->perf_ctrls.desired_perf = caps->lowest_perf;\n\n\tret = cppc_set_perf(cpu, &cpu_data->perf_ctrls);\n\tif (ret)\n\t\tpr_debug(\"Err setting perf value:%d on CPU:%d. ret:%d\\n\",\n\t\t\t caps->lowest_perf, cpu, ret);\n\n\tcppc_cpufreq_put_cpu_data(policy);\n\treturn 0;\n}\n\nstatic inline u64 get_delta(u64 t1, u64 t0)\n{\n\tif (t1 > t0 || t0 > ~(u32)0)\n\t\treturn t1 - t0;\n\n\treturn (u32)t1 - (u32)t0;\n}\n\nstatic int cppc_perf_from_fbctrs(struct cppc_cpudata *cpu_data,\n\t\t\t\t struct cppc_perf_fb_ctrs *fb_ctrs_t0,\n\t\t\t\t struct cppc_perf_fb_ctrs *fb_ctrs_t1)\n{\n\tu64 delta_reference, delta_delivered;\n\tu64 reference_perf;\n\n\treference_perf = fb_ctrs_t0->reference_perf;\n\n\tdelta_reference = get_delta(fb_ctrs_t1->reference,\n\t\t\t\t    fb_ctrs_t0->reference);\n\tdelta_delivered = get_delta(fb_ctrs_t1->delivered,\n\t\t\t\t    fb_ctrs_t0->delivered);\n\n\t \n\tif (!delta_reference || !delta_delivered)\n\t\treturn cpu_data->perf_ctrls.desired_perf;\n\n\treturn (reference_perf * delta_delivered) / delta_reference;\n}\n\nstatic unsigned int cppc_cpufreq_get_rate(unsigned int cpu)\n{\n\tstruct cppc_perf_fb_ctrs fb_ctrs_t0 = {0}, fb_ctrs_t1 = {0};\n\tstruct cpufreq_policy *policy = cpufreq_cpu_get(cpu);\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tu64 delivered_perf;\n\tint ret;\n\n\tcpufreq_cpu_put(policy);\n\n\tret = cppc_get_perf_ctrs(cpu, &fb_ctrs_t0);\n\tif (ret)\n\t\treturn 0;\n\n\tudelay(2);  \n\n\tret = cppc_get_perf_ctrs(cpu, &fb_ctrs_t1);\n\tif (ret)\n\t\treturn 0;\n\n\tdelivered_perf = cppc_perf_from_fbctrs(cpu_data, &fb_ctrs_t0,\n\t\t\t\t\t       &fb_ctrs_t1);\n\n\treturn cppc_cpufreq_perf_to_khz(cpu_data, delivered_perf);\n}\n\nstatic int cppc_cpufreq_set_boost(struct cpufreq_policy *policy, int state)\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tstruct cppc_perf_caps *caps = &cpu_data->perf_caps;\n\tint ret;\n\n\tif (!boost_supported) {\n\t\tpr_err(\"BOOST not supported by CPU or firmware\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (state)\n\t\tpolicy->max = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t\t       caps->highest_perf);\n\telse\n\t\tpolicy->max = cppc_cpufreq_perf_to_khz(cpu_data,\n\t\t\t\t\t\t       caps->nominal_perf);\n\tpolicy->cpuinfo.max_freq = policy->max;\n\n\tret = freq_qos_update_request(policy->max_freq_req, policy->max);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic ssize_t show_freqdomain_cpus(struct cpufreq_policy *policy, char *buf)\n{\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\n\treturn cpufreq_show_cpus(cpu_data->shared_cpu_map, buf);\n}\ncpufreq_freq_attr_ro(freqdomain_cpus);\n\nstatic struct freq_attr *cppc_cpufreq_attr[] = {\n\t&freqdomain_cpus,\n\tNULL,\n};\n\nstatic struct cpufreq_driver cppc_cpufreq_driver = {\n\t.flags = CPUFREQ_CONST_LOOPS,\n\t.verify = cppc_verify_policy,\n\t.target = cppc_cpufreq_set_target,\n\t.get = cppc_cpufreq_get_rate,\n\t.fast_switch = cppc_cpufreq_fast_switch,\n\t.init = cppc_cpufreq_cpu_init,\n\t.exit = cppc_cpufreq_cpu_exit,\n\t.set_boost = cppc_cpufreq_set_boost,\n\t.attr = cppc_cpufreq_attr,\n\t.name = \"cppc_cpufreq\",\n};\n\n \nstatic unsigned int hisi_cppc_cpufreq_get_rate(unsigned int cpu)\n{\n\tstruct cpufreq_policy *policy = cpufreq_cpu_get(cpu);\n\tstruct cppc_cpudata *cpu_data = policy->driver_data;\n\tu64 desired_perf;\n\tint ret;\n\n\tcpufreq_cpu_put(policy);\n\n\tret = cppc_get_desired_perf(cpu, &desired_perf);\n\tif (ret < 0)\n\t\treturn -EIO;\n\n\treturn cppc_cpufreq_perf_to_khz(cpu_data, desired_perf);\n}\n\nstatic void cppc_check_hisi_workaround(void)\n{\n\tstruct acpi_table_header *tbl;\n\tacpi_status status = AE_OK;\n\tint i;\n\n\tstatus = acpi_get_table(ACPI_SIG_PCCT, 0, &tbl);\n\tif (ACPI_FAILURE(status) || !tbl)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(wa_info); i++) {\n\t\tif (!memcmp(wa_info[i].oem_id, tbl->oem_id, ACPI_OEM_ID_SIZE) &&\n\t\t    !memcmp(wa_info[i].oem_table_id, tbl->oem_table_id, ACPI_OEM_TABLE_ID_SIZE) &&\n\t\t    wa_info[i].oem_revision == tbl->oem_revision) {\n\t\t\t \n\t\t\tcppc_cpufreq_driver.get = hisi_cppc_cpufreq_get_rate;\n\t\t\tfie_disabled = FIE_DISABLED;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tacpi_put_table(tbl);\n}\n\nstatic int __init cppc_cpufreq_init(void)\n{\n\tint ret;\n\n\tif (!acpi_cpc_valid())\n\t\treturn -ENODEV;\n\n\tcppc_check_hisi_workaround();\n\tcppc_freq_invariance_init();\n\tpopulate_efficiency_class();\n\n\tret = cpufreq_register_driver(&cppc_cpufreq_driver);\n\tif (ret)\n\t\tcppc_freq_invariance_exit();\n\n\treturn ret;\n}\n\nstatic inline void free_cpu_data(void)\n{\n\tstruct cppc_cpudata *iter, *tmp;\n\n\tlist_for_each_entry_safe(iter, tmp, &cpu_data_list, node) {\n\t\tfree_cpumask_var(iter->shared_cpu_map);\n\t\tlist_del(&iter->node);\n\t\tkfree(iter);\n\t}\n\n}\n\nstatic void __exit cppc_cpufreq_exit(void)\n{\n\tcpufreq_unregister_driver(&cppc_cpufreq_driver);\n\tcppc_freq_invariance_exit();\n\n\tfree_cpu_data();\n}\n\nmodule_exit(cppc_cpufreq_exit);\nMODULE_AUTHOR(\"Ashwin Chaugule\");\nMODULE_DESCRIPTION(\"CPUFreq driver based on the ACPI CPPC v5.0+ spec\");\nMODULE_LICENSE(\"GPL\");\n\nlate_initcall(cppc_cpufreq_init);\n\nstatic const struct acpi_device_id cppc_acpi_ids[] __used = {\n\t{ACPI_PROCESSOR_DEVICE_HID, },\n\t{}\n};\n\nMODULE_DEVICE_TABLE(acpi, cppc_acpi_ids);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}