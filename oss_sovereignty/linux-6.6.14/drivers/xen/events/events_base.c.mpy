{
  "module_name": "events_base.c",
  "hash_id": "25a131aca2dfd485972db4f0cb5797a9c7c7a7a73486abb630baed256802e18f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/events/events_base.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"xen:\" KBUILD_MODNAME \": \" fmt\n\n#include <linux/linkage.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/moduleparam.h>\n#include <linux/string.h>\n#include <linux/memblock.h>\n#include <linux/slab.h>\n#include <linux/irqnr.h>\n#include <linux/pci.h>\n#include <linux/rcupdate.h>\n#include <linux/spinlock.h>\n#include <linux/cpuhotplug.h>\n#include <linux/atomic.h>\n#include <linux/ktime.h>\n\n#ifdef CONFIG_X86\n#include <asm/desc.h>\n#include <asm/ptrace.h>\n#include <asm/idtentry.h>\n#include <asm/irq.h>\n#include <asm/io_apic.h>\n#include <asm/i8259.h>\n#include <asm/xen/cpuid.h>\n#include <asm/xen/pci.h>\n#endif\n#include <asm/sync_bitops.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n#include <xen/page.h>\n\n#include <xen/xen.h>\n#include <xen/hvm.h>\n#include <xen/xen-ops.h>\n#include <xen/events.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/event_channel.h>\n#include <xen/interface/hvm/hvm_op.h>\n#include <xen/interface/hvm/params.h>\n#include <xen/interface/physdev.h>\n#include <xen/interface/sched.h>\n#include <xen/interface/vcpu.h>\n#include <xen/xenbus.h>\n#include <asm/hw_irq.h>\n\n#include \"events_internal.h\"\n\n#undef MODULE_PARAM_PREFIX\n#define MODULE_PARAM_PREFIX \"xen.\"\n\n \nenum xen_irq_type {\n\tIRQT_UNBOUND = 0,\n\tIRQT_PIRQ,\n\tIRQT_VIRQ,\n\tIRQT_IPI,\n\tIRQT_EVTCHN\n};\n\n \nstruct irq_info {\n\tstruct list_head list;\n\tstruct list_head eoi_list;\n\tstruct rcu_work rwork;\n\tshort refcnt;\n\tu8 spurious_cnt;\n\tu8 is_accounted;\n\tshort type;\t\t \n\tu8 mask_reason;\t\t \n#define EVT_MASK_REASON_EXPLICIT\t0x01\n#define EVT_MASK_REASON_TEMPORARY\t0x02\n#define EVT_MASK_REASON_EOI_PENDING\t0x04\n\tu8 is_active;\t\t \n\tunsigned irq;\n\tevtchn_port_t evtchn;    \n\tunsigned short cpu;      \n\tunsigned short eoi_cpu;  \n\tunsigned int irq_epoch;  \n\tu64 eoi_time;            \n\traw_spinlock_t lock;\n\tbool is_static;            \n\n\tunion {\n\t\tunsigned short virq;\n\t\tenum ipi_vector ipi;\n\t\tstruct {\n\t\t\tunsigned short pirq;\n\t\t\tunsigned short gsi;\n\t\t\tunsigned char vector;\n\t\t\tunsigned char flags;\n\t\t\tuint16_t domid;\n\t\t} pirq;\n\t\tstruct xenbus_device *interdomain;\n\t} u;\n};\n\n#define PIRQ_NEEDS_EOI\t(1 << 0)\n#define PIRQ_SHAREABLE\t(1 << 1)\n#define PIRQ_MSI_GROUP\t(1 << 2)\n\nstatic uint __read_mostly event_loop_timeout = 2;\nmodule_param(event_loop_timeout, uint, 0644);\n\nstatic uint __read_mostly event_eoi_delay = 10;\nmodule_param(event_eoi_delay, uint, 0644);\n\nconst struct evtchn_ops *evtchn_ops;\n\n \nstatic DEFINE_MUTEX(irq_mapping_update_lock);\n\n \n\nstatic LIST_HEAD(xen_irq_list_head);\n\n \nstatic DEFINE_PER_CPU(int [NR_VIRQS], virq_to_irq) = {[0 ... NR_VIRQS-1] = -1};\n\n \nstatic DEFINE_PER_CPU(int [XEN_NR_IPIS], ipi_to_irq) = {[0 ... XEN_NR_IPIS-1] = -1};\n \nstatic DEFINE_PER_CPU(evtchn_port_t [XEN_NR_IPIS], ipi_to_evtchn) = {[0 ... XEN_NR_IPIS-1] = 0};\n\n \nstatic atomic_t channels_on_cpu[NR_CPUS];\n\nstatic int **evtchn_to_irq;\n#ifdef CONFIG_X86\nstatic unsigned long *pirq_eoi_map;\n#endif\nstatic bool (*pirq_needs_eoi)(unsigned irq);\n\n#define EVTCHN_ROW(e)  (e / (PAGE_SIZE/sizeof(**evtchn_to_irq)))\n#define EVTCHN_COL(e)  (e % (PAGE_SIZE/sizeof(**evtchn_to_irq)))\n#define EVTCHN_PER_ROW (PAGE_SIZE / sizeof(**evtchn_to_irq))\n\n \n#define VALID_EVTCHN(chn)\t((chn) != 0)\n\nstatic struct irq_info *legacy_info_ptrs[NR_IRQS_LEGACY];\n\nstatic struct irq_chip xen_dynamic_chip;\nstatic struct irq_chip xen_lateeoi_chip;\nstatic struct irq_chip xen_percpu_chip;\nstatic struct irq_chip xen_pirq_chip;\nstatic void enable_dynirq(struct irq_data *data);\nstatic void disable_dynirq(struct irq_data *data);\n\nstatic DEFINE_PER_CPU(unsigned int, irq_epoch);\n\nstatic void clear_evtchn_to_irq_row(int *evtchn_row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_row[col], -1);\n}\n\nstatic void clear_evtchn_to_irq_all(void)\n{\n\tunsigned row;\n\n\tfor (row = 0; row < EVTCHN_ROW(xen_evtchn_max_channels()); row++) {\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\tcontinue;\n\t\tclear_evtchn_to_irq_row(evtchn_to_irq[row]);\n\t}\n}\n\nstatic int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\tint *evtchn_row;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t \n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_row = (int *) __get_free_pages(GFP_KERNEL, 0);\n\t\tif (evtchn_row == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(evtchn_row);\n\n\t\t \n\t\tif (cmpxchg(&evtchn_to_irq[row], NULL, evtchn_row) != NULL)\n\t\t\tfree_page((unsigned long) evtchn_row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}\n\nint get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}\n\n \nstatic struct irq_info *info_for_irq(unsigned irq)\n{\n\tif (irq < nr_legacy_irqs())\n\t\treturn legacy_info_ptrs[irq];\n\telse\n\t\treturn irq_get_chip_data(irq);\n}\n\nstatic void set_info_for_irq(unsigned int irq, struct irq_info *info)\n{\n\tif (irq < nr_legacy_irqs())\n\t\tlegacy_info_ptrs[irq] = info;\n\telse\n\t\tirq_set_chip_data(irq, info);\n}\n\n \nstatic void channels_on_cpu_dec(struct irq_info *info)\n{\n\tif (!info->is_accounted)\n\t\treturn;\n\n\tinfo->is_accounted = 0;\n\n\tif (WARN_ON_ONCE(info->cpu >= nr_cpu_ids))\n\t\treturn;\n\n\tWARN_ON_ONCE(!atomic_add_unless(&channels_on_cpu[info->cpu], -1 , 0));\n}\n\nstatic void channels_on_cpu_inc(struct irq_info *info)\n{\n\tif (WARN_ON_ONCE(info->cpu >= nr_cpu_ids))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!atomic_add_unless(&channels_on_cpu[info->cpu], 1,\n\t\t\t\t\t    INT_MAX)))\n\t\treturn;\n\n\tinfo->is_accounted = 1;\n}\n\nstatic void delayed_free_irq(struct work_struct *work)\n{\n\tstruct irq_info *info = container_of(to_rcu_work(work), struct irq_info,\n\t\t\t\t\t     rwork);\n\tunsigned int irq = info->irq;\n\n\t \n\tset_info_for_irq(irq, NULL);\n\n\tkfree(info);\n\n\t \n\tif (irq >= nr_legacy_irqs())\n\t\tirq_free_desc(irq);\n}\n\n \nstatic int xen_irq_info_common_setup(struct irq_info *info,\n\t\t\t\t     unsigned irq,\n\t\t\t\t     enum xen_irq_type type,\n\t\t\t\t     evtchn_port_t evtchn,\n\t\t\t\t     unsigned short cpu)\n{\n\tint ret;\n\n\tBUG_ON(info->type != IRQT_UNBOUND && info->type != type);\n\n\tinfo->type = type;\n\tinfo->irq = irq;\n\tinfo->evtchn = evtchn;\n\tinfo->cpu = cpu;\n\tinfo->mask_reason = EVT_MASK_REASON_EXPLICIT;\n\traw_spin_lock_init(&info->lock);\n\n\tret = set_evtchn_to_irq(evtchn, irq);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tirq_clear_status_flags(irq, IRQ_NOREQUEST|IRQ_NOAUTOEN);\n\n\treturn xen_evtchn_port_setup(evtchn);\n}\n\nstatic int xen_irq_info_evtchn_setup(unsigned irq,\n\t\t\t\t     evtchn_port_t evtchn,\n\t\t\t\t     struct xenbus_device *dev)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tint ret;\n\n\tret = xen_irq_info_common_setup(info, irq, IRQT_EVTCHN, evtchn, 0);\n\tinfo->u.interdomain = dev;\n\tif (dev)\n\t\tatomic_inc(&dev->event_channels);\n\n\treturn ret;\n}\n\nstatic int xen_irq_info_ipi_setup(unsigned cpu,\n\t\t\t\t  unsigned irq,\n\t\t\t\t  evtchn_port_t evtchn,\n\t\t\t\t  enum ipi_vector ipi)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tinfo->u.ipi = ipi;\n\n\tper_cpu(ipi_to_irq, cpu)[ipi] = irq;\n\tper_cpu(ipi_to_evtchn, cpu)[ipi] = evtchn;\n\n\treturn xen_irq_info_common_setup(info, irq, IRQT_IPI, evtchn, 0);\n}\n\nstatic int xen_irq_info_virq_setup(unsigned cpu,\n\t\t\t\t   unsigned irq,\n\t\t\t\t   evtchn_port_t evtchn,\n\t\t\t\t   unsigned virq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tinfo->u.virq = virq;\n\n\tper_cpu(virq_to_irq, cpu)[virq] = irq;\n\n\treturn xen_irq_info_common_setup(info, irq, IRQT_VIRQ, evtchn, 0);\n}\n\nstatic int xen_irq_info_pirq_setup(unsigned irq,\n\t\t\t\t   evtchn_port_t evtchn,\n\t\t\t\t   unsigned pirq,\n\t\t\t\t   unsigned gsi,\n\t\t\t\t   uint16_t domid,\n\t\t\t\t   unsigned char flags)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tinfo->u.pirq.pirq = pirq;\n\tinfo->u.pirq.gsi = gsi;\n\tinfo->u.pirq.domid = domid;\n\tinfo->u.pirq.flags = flags;\n\n\treturn xen_irq_info_common_setup(info, irq, IRQT_PIRQ, evtchn, 0);\n}\n\nstatic void xen_irq_info_cleanup(struct irq_info *info)\n{\n\tset_evtchn_to_irq(info->evtchn, -1);\n\txen_evtchn_port_remove(info->evtchn, info->cpu);\n\tinfo->evtchn = 0;\n\tchannels_on_cpu_dec(info);\n}\n\n \nevtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}\n\nunsigned int irq_from_evtchn(evtchn_port_t evtchn)\n{\n\treturn get_evtchn_to_irq(evtchn);\n}\nEXPORT_SYMBOL_GPL(irq_from_evtchn);\n\nint irq_from_virq(unsigned int cpu, unsigned int virq)\n{\n\treturn per_cpu(virq_to_irq, cpu)[virq];\n}\n\nstatic enum ipi_vector ipi_from_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tBUG_ON(info == NULL);\n\tBUG_ON(info->type != IRQT_IPI);\n\n\treturn info->u.ipi;\n}\n\nstatic unsigned virq_from_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tBUG_ON(info == NULL);\n\tBUG_ON(info->type != IRQT_VIRQ);\n\n\treturn info->u.virq;\n}\n\nstatic unsigned pirq_from_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tBUG_ON(info == NULL);\n\tBUG_ON(info->type != IRQT_PIRQ);\n\n\treturn info->u.pirq.pirq;\n}\n\nstatic enum xen_irq_type type_from_irq(unsigned irq)\n{\n\treturn info_for_irq(irq)->type;\n}\n\nstatic unsigned cpu_from_irq(unsigned irq)\n{\n\treturn info_for_irq(irq)->cpu;\n}\n\nunsigned int cpu_from_evtchn(evtchn_port_t evtchn)\n{\n\tint irq = get_evtchn_to_irq(evtchn);\n\tunsigned ret = 0;\n\n\tif (irq != -1)\n\t\tret = cpu_from_irq(irq);\n\n\treturn ret;\n}\n\nstatic void do_mask(struct irq_info *info, u8 reason)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&info->lock, flags);\n\n\tif (!info->mask_reason)\n\t\tmask_evtchn(info->evtchn);\n\n\tinfo->mask_reason |= reason;\n\n\traw_spin_unlock_irqrestore(&info->lock, flags);\n}\n\nstatic void do_unmask(struct irq_info *info, u8 reason)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&info->lock, flags);\n\n\tinfo->mask_reason &= ~reason;\n\n\tif (!info->mask_reason)\n\t\tunmask_evtchn(info->evtchn);\n\n\traw_spin_unlock_irqrestore(&info->lock, flags);\n}\n\n#ifdef CONFIG_X86\nstatic bool pirq_check_eoi_map(unsigned irq)\n{\n\treturn test_bit(pirq_from_irq(irq), pirq_eoi_map);\n}\n#endif\n\nstatic bool pirq_needs_eoi_flag(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tBUG_ON(info->type != IRQT_PIRQ);\n\n\treturn info->u.pirq.flags & PIRQ_NEEDS_EOI;\n}\n\nstatic void bind_evtchn_to_cpu(evtchn_port_t evtchn, unsigned int cpu,\n\t\t\t       bool force_affinity)\n{\n\tint irq = get_evtchn_to_irq(evtchn);\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tBUG_ON(irq == -1);\n\n\tif (IS_ENABLED(CONFIG_SMP) && force_affinity) {\n\t\tstruct irq_data *data = irq_get_irq_data(irq);\n\n\t\tirq_data_update_affinity(data, cpumask_of(cpu));\n\t\tirq_data_update_effective_affinity(data, cpumask_of(cpu));\n\t}\n\n\txen_evtchn_port_bind_to_cpu(evtchn, cpu, info->cpu);\n\n\tchannels_on_cpu_dec(info);\n\tinfo->cpu = cpu;\n\tchannels_on_cpu_inc(info);\n}\n\n \nvoid notify_remote_via_irq(int irq)\n{\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tnotify_remote_via_evtchn(evtchn);\n}\nEXPORT_SYMBOL_GPL(notify_remote_via_irq);\n\nstruct lateeoi_work {\n\tstruct delayed_work delayed;\n\tspinlock_t eoi_list_lock;\n\tstruct list_head eoi_list;\n};\n\nstatic DEFINE_PER_CPU(struct lateeoi_work, lateeoi);\n\nstatic void lateeoi_list_del(struct irq_info *info)\n{\n\tstruct lateeoi_work *eoi = &per_cpu(lateeoi, info->eoi_cpu);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&eoi->eoi_list_lock, flags);\n\tlist_del_init(&info->eoi_list);\n\tspin_unlock_irqrestore(&eoi->eoi_list_lock, flags);\n}\n\nstatic void lateeoi_list_add(struct irq_info *info)\n{\n\tstruct lateeoi_work *eoi = &per_cpu(lateeoi, info->eoi_cpu);\n\tstruct irq_info *elem;\n\tu64 now = get_jiffies_64();\n\tunsigned long delay;\n\tunsigned long flags;\n\n\tif (now < info->eoi_time)\n\t\tdelay = info->eoi_time - now;\n\telse\n\t\tdelay = 1;\n\n\tspin_lock_irqsave(&eoi->eoi_list_lock, flags);\n\n\telem = list_first_entry_or_null(&eoi->eoi_list, struct irq_info,\n\t\t\t\t\teoi_list);\n\tif (!elem || info->eoi_time < elem->eoi_time) {\n\t\tlist_add(&info->eoi_list, &eoi->eoi_list);\n\t\tmod_delayed_work_on(info->eoi_cpu, system_wq,\n\t\t\t\t    &eoi->delayed, delay);\n\t} else {\n\t\tlist_for_each_entry_reverse(elem, &eoi->eoi_list, eoi_list) {\n\t\t\tif (elem->eoi_time <= info->eoi_time)\n\t\t\t\tbreak;\n\t\t}\n\t\tlist_add(&info->eoi_list, &elem->eoi_list);\n\t}\n\n\tspin_unlock_irqrestore(&eoi->eoi_list_lock, flags);\n}\n\nstatic void xen_irq_lateeoi_locked(struct irq_info *info, bool spurious)\n{\n\tevtchn_port_t evtchn;\n\tunsigned int cpu;\n\tunsigned int delay = 0;\n\n\tevtchn = info->evtchn;\n\tif (!VALID_EVTCHN(evtchn) || !list_empty(&info->eoi_list))\n\t\treturn;\n\n\tif (spurious) {\n\t\tstruct xenbus_device *dev = info->u.interdomain;\n\t\tunsigned int threshold = 1;\n\n\t\tif (dev && dev->spurious_threshold)\n\t\t\tthreshold = dev->spurious_threshold;\n\n\t\tif ((1 << info->spurious_cnt) < (HZ << 2)) {\n\t\t\tif (info->spurious_cnt != 0xFF)\n\t\t\t\tinfo->spurious_cnt++;\n\t\t}\n\t\tif (info->spurious_cnt > threshold) {\n\t\t\tdelay = 1 << (info->spurious_cnt - 1 - threshold);\n\t\t\tif (delay > HZ)\n\t\t\t\tdelay = HZ;\n\t\t\tif (!info->eoi_time)\n\t\t\t\tinfo->eoi_cpu = smp_processor_id();\n\t\t\tinfo->eoi_time = get_jiffies_64() + delay;\n\t\t\tif (dev)\n\t\t\t\tatomic_add(delay, &dev->jiffies_eoi_delayed);\n\t\t}\n\t\tif (dev)\n\t\t\tatomic_inc(&dev->spurious_events);\n\t} else {\n\t\tinfo->spurious_cnt = 0;\n\t}\n\n\tcpu = info->eoi_cpu;\n\tif (info->eoi_time &&\n\t    (info->irq_epoch == per_cpu(irq_epoch, cpu) || delay)) {\n\t\tlateeoi_list_add(info);\n\t\treturn;\n\t}\n\n\tinfo->eoi_time = 0;\n\n\t \n\tsmp_store_release(&info->is_active, 0);\n\tdo_unmask(info, EVT_MASK_REASON_EOI_PENDING);\n}\n\nstatic void xen_irq_lateeoi_worker(struct work_struct *work)\n{\n\tstruct lateeoi_work *eoi;\n\tstruct irq_info *info;\n\tu64 now = get_jiffies_64();\n\tunsigned long flags;\n\n\teoi = container_of(to_delayed_work(work), struct lateeoi_work, delayed);\n\n\trcu_read_lock();\n\n\twhile (true) {\n\t\tspin_lock_irqsave(&eoi->eoi_list_lock, flags);\n\n\t\tinfo = list_first_entry_or_null(&eoi->eoi_list, struct irq_info,\n\t\t\t\t\t\teoi_list);\n\n\t\tif (info == NULL)\n\t\t\tbreak;\n\n\t\tif (now < info->eoi_time) {\n\t\t\tmod_delayed_work_on(info->eoi_cpu, system_wq,\n\t\t\t\t\t    &eoi->delayed,\n\t\t\t\t\t    info->eoi_time - now);\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_del_init(&info->eoi_list);\n\n\t\tspin_unlock_irqrestore(&eoi->eoi_list_lock, flags);\n\n\t\tinfo->eoi_time = 0;\n\n\t\txen_irq_lateeoi_locked(info, false);\n\t}\n\n\tspin_unlock_irqrestore(&eoi->eoi_list_lock, flags);\n\n\trcu_read_unlock();\n}\n\nstatic void xen_cpu_init_eoi(unsigned int cpu)\n{\n\tstruct lateeoi_work *eoi = &per_cpu(lateeoi, cpu);\n\n\tINIT_DELAYED_WORK(&eoi->delayed, xen_irq_lateeoi_worker);\n\tspin_lock_init(&eoi->eoi_list_lock);\n\tINIT_LIST_HEAD(&eoi->eoi_list);\n}\n\nvoid xen_irq_lateeoi(unsigned int irq, unsigned int eoi_flags)\n{\n\tstruct irq_info *info;\n\n\trcu_read_lock();\n\n\tinfo = info_for_irq(irq);\n\n\tif (info)\n\t\txen_irq_lateeoi_locked(info, eoi_flags & XEN_EOI_FLAG_SPURIOUS);\n\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(xen_irq_lateeoi);\n\nstatic void xen_irq_init(unsigned irq)\n{\n\tstruct irq_info *info;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (info == NULL)\n\t\tpanic(\"Unable to allocate metadata for IRQ%d\\n\", irq);\n\n\tinfo->type = IRQT_UNBOUND;\n\tinfo->refcnt = -1;\n\tINIT_RCU_WORK(&info->rwork, delayed_free_irq);\n\n\tset_info_for_irq(irq, info);\n\t \n\tirq_set_status_flags(irq, IRQ_MOVE_PCNTXT);\n\n\tINIT_LIST_HEAD(&info->eoi_list);\n\tlist_add_tail(&info->list, &xen_irq_list_head);\n}\n\nstatic int __must_check xen_allocate_irqs_dynamic(int nvec)\n{\n\tint i, irq = irq_alloc_descs(-1, 0, nvec, -1);\n\n\tif (irq >= 0) {\n\t\tfor (i = 0; i < nvec; i++)\n\t\t\txen_irq_init(irq + i);\n\t}\n\n\treturn irq;\n}\n\nstatic inline int __must_check xen_allocate_irq_dynamic(void)\n{\n\n\treturn xen_allocate_irqs_dynamic(1);\n}\n\nstatic int __must_check xen_allocate_irq_gsi(unsigned gsi)\n{\n\tint irq;\n\n\t \n\tif (xen_pv_domain() && !xen_initial_domain())\n\t\treturn xen_allocate_irq_dynamic();\n\n\t \n\tif (gsi < nr_legacy_irqs())\n\t\tirq = gsi;\n\telse\n\t\tirq = irq_alloc_desc_at(gsi, -1);\n\n\txen_irq_init(irq);\n\n\treturn irq;\n}\n\nstatic void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tif (!list_empty(&info->eoi_list))\n\t\tlateeoi_list_del(info);\n\n\tlist_del(&info->list);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tqueue_rcu_work(system_wq, &info->rwork);\n}\n\n \nstatic void event_handler_exit(struct irq_info *info)\n{\n\tsmp_store_release(&info->is_active, 0);\n\tclear_evtchn(info->evtchn);\n}\n\nstatic void pirq_query_unmask(int irq)\n{\n\tstruct physdev_irq_status_query irq_status;\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tBUG_ON(info->type != IRQT_PIRQ);\n\n\tirq_status.irq = pirq_from_irq(irq);\n\tif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\n\t\tirq_status.flags = 0;\n\n\tinfo->u.pirq.flags &= ~PIRQ_NEEDS_EOI;\n\tif (irq_status.flags & XENIRQSTAT_needs_eoi)\n\t\tinfo->u.pirq.flags |= PIRQ_NEEDS_EOI;\n}\n\nstatic void eoi_pirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\tstruct physdev_eoi eoi = { .irq = pirq_from_irq(data->irq) };\n\tint rc = 0;\n\n\tif (!VALID_EVTCHN(evtchn))\n\t\treturn;\n\n\tevent_handler_exit(info);\n\n\tif (pirq_needs_eoi(data->irq)) {\n\t\trc = HYPERVISOR_physdev_op(PHYSDEVOP_eoi, &eoi);\n\t\tWARN_ON(rc);\n\t}\n}\n\nstatic void mask_ack_pirq(struct irq_data *data)\n{\n\tdisable_dynirq(data);\n\teoi_pirq(data);\n}\n\nstatic unsigned int __startup_pirq(unsigned int irq)\n{\n\tstruct evtchn_bind_pirq bind_pirq;\n\tstruct irq_info *info = info_for_irq(irq);\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\tint rc;\n\n\tBUG_ON(info->type != IRQT_PIRQ);\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tgoto out;\n\n\tbind_pirq.pirq = pirq_from_irq(irq);\n\t \n\tbind_pirq.flags = info->u.pirq.flags & PIRQ_SHAREABLE ?\n\t\t\t\t\tBIND_PIRQ__WILL_SHARE : 0;\n\trc = HYPERVISOR_event_channel_op(EVTCHNOP_bind_pirq, &bind_pirq);\n\tif (rc != 0) {\n\t\tpr_warn(\"Failed to obtain physical IRQ %d\\n\", irq);\n\t\treturn 0;\n\t}\n\tevtchn = bind_pirq.port;\n\n\tpirq_query_unmask(irq);\n\n\trc = set_evtchn_to_irq(evtchn, irq);\n\tif (rc)\n\t\tgoto err;\n\n\tinfo->evtchn = evtchn;\n\tbind_evtchn_to_cpu(evtchn, 0, false);\n\n\trc = xen_evtchn_port_setup(evtchn);\n\tif (rc)\n\t\tgoto err;\n\nout:\n\tdo_unmask(info, EVT_MASK_REASON_EXPLICIT);\n\n\teoi_pirq(irq_get_irq_data(irq));\n\n\treturn 0;\n\nerr:\n\tpr_err(\"irq%d: Failed to set port to irq mapping (%d)\\n\", irq, rc);\n\txen_evtchn_close(evtchn);\n\treturn 0;\n}\n\nstatic unsigned int startup_pirq(struct irq_data *data)\n{\n\treturn __startup_pirq(data->irq);\n}\n\nstatic void shutdown_pirq(struct irq_data *data)\n{\n\tunsigned int irq = data->irq;\n\tstruct irq_info *info = info_for_irq(irq);\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\n\tBUG_ON(info->type != IRQT_PIRQ);\n\n\tif (!VALID_EVTCHN(evtchn))\n\t\treturn;\n\n\tdo_mask(info, EVT_MASK_REASON_EXPLICIT);\n\txen_evtchn_close(evtchn);\n\txen_irq_info_cleanup(info);\n}\n\nstatic void enable_pirq(struct irq_data *data)\n{\n\tenable_dynirq(data);\n}\n\nstatic void disable_pirq(struct irq_data *data)\n{\n\tdisable_dynirq(data);\n}\n\nint xen_irq_from_gsi(unsigned gsi)\n{\n\tstruct irq_info *info;\n\n\tlist_for_each_entry(info, &xen_irq_list_head, list) {\n\t\tif (info->type != IRQT_PIRQ)\n\t\t\tcontinue;\n\n\t\tif (info->u.pirq.gsi == gsi)\n\t\t\treturn info->irq;\n\t}\n\n\treturn -1;\n}\nEXPORT_SYMBOL_GPL(xen_irq_from_gsi);\n\nstatic void __unbind_from_irq(unsigned int irq)\n{\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (info->refcnt > 0) {\n\t\tinfo->refcnt--;\n\t\tif (info->refcnt != 0)\n\t\t\treturn;\n\t}\n\n\tif (VALID_EVTCHN(evtchn)) {\n\t\tunsigned int cpu = cpu_from_irq(irq);\n\t\tstruct xenbus_device *dev;\n\n\t\tif (!info->is_static)\n\t\t\txen_evtchn_close(evtchn);\n\n\t\tswitch (type_from_irq(irq)) {\n\t\tcase IRQT_VIRQ:\n\t\t\tper_cpu(virq_to_irq, cpu)[virq_from_irq(irq)] = -1;\n\t\t\tbreak;\n\t\tcase IRQT_IPI:\n\t\t\tper_cpu(ipi_to_irq, cpu)[ipi_from_irq(irq)] = -1;\n\t\t\tper_cpu(ipi_to_evtchn, cpu)[ipi_from_irq(irq)] = 0;\n\t\t\tbreak;\n\t\tcase IRQT_EVTCHN:\n\t\t\tdev = info->u.interdomain;\n\t\t\tif (dev)\n\t\t\t\tatomic_dec(&dev->event_channels);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\txen_irq_info_cleanup(info);\n\t}\n\n\txen_free_irq(irq);\n}\n\n \nint xen_bind_pirq_gsi_to_irq(unsigned gsi,\n\t\t\t     unsigned pirq, int shareable, char *name)\n{\n\tint irq;\n\tstruct physdev_irq irq_op;\n\tint ret;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = xen_irq_from_gsi(gsi);\n\tif (irq != -1) {\n\t\tpr_info(\"%s: returning irq %d for gsi %u\\n\",\n\t\t\t__func__, irq, gsi);\n\t\tgoto out;\n\t}\n\n\tirq = xen_allocate_irq_gsi(gsi);\n\tif (irq < 0)\n\t\tgoto out;\n\n\tirq_op.irq = irq;\n\tirq_op.vector = 0;\n\n\t \n\tif (xen_initial_domain() &&\n\t    HYPERVISOR_physdev_op(PHYSDEVOP_alloc_irq_vector, &irq_op)) {\n\t\txen_free_irq(irq);\n\t\tirq = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tret = xen_irq_info_pirq_setup(irq, 0, pirq, gsi, DOMID_SELF,\n\t\t\t       shareable ? PIRQ_SHAREABLE : 0);\n\tif (ret < 0) {\n\t\t__unbind_from_irq(irq);\n\t\tirq = ret;\n\t\tgoto out;\n\t}\n\n\tpirq_query_unmask(irq);\n\t \n\tif (shareable)\n\t\tirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\n\t\t\t\thandle_fasteoi_irq, name);\n\telse\n\t\tirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\n\t\t\t\thandle_edge_irq, name);\n\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\treturn irq;\n}\n\n#ifdef CONFIG_PCI_MSI\nint xen_allocate_pirq_msi(struct pci_dev *dev, struct msi_desc *msidesc)\n{\n\tint rc;\n\tstruct physdev_get_free_pirq op_get_free_pirq;\n\n\top_get_free_pirq.type = MAP_PIRQ_TYPE_MSI;\n\trc = HYPERVISOR_physdev_op(PHYSDEVOP_get_free_pirq, &op_get_free_pirq);\n\n\tWARN_ONCE(rc == -ENOSYS,\n\t\t  \"hypervisor does not support the PHYSDEVOP_get_free_pirq interface\\n\");\n\n\treturn rc ? -1 : op_get_free_pirq.pirq;\n}\n\nint xen_bind_pirq_msi_to_irq(struct pci_dev *dev, struct msi_desc *msidesc,\n\t\t\t     int pirq, int nvec, const char *name, domid_t domid)\n{\n\tint i, irq, ret;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = xen_allocate_irqs_dynamic(nvec);\n\tif (irq < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < nvec; i++) {\n\t\tirq_set_chip_and_handler_name(irq + i, &xen_pirq_chip, handle_edge_irq, name);\n\n\t\tret = xen_irq_info_pirq_setup(irq + i, 0, pirq + i, 0, domid,\n\t\t\t\t\t      i == 0 ? 0 : PIRQ_MSI_GROUP);\n\t\tif (ret < 0)\n\t\t\tgoto error_irq;\n\t}\n\n\tret = irq_set_msi_desc(irq, msidesc);\n\tif (ret < 0)\n\t\tgoto error_irq;\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\treturn irq;\nerror_irq:\n\twhile (nvec--)\n\t\t__unbind_from_irq(irq + nvec);\n\tmutex_unlock(&irq_mapping_update_lock);\n\treturn ret;\n}\n#endif\n\nint xen_destroy_irq(int irq)\n{\n\tstruct physdev_unmap_pirq unmap_irq;\n\tstruct irq_info *info = info_for_irq(irq);\n\tint rc = -ENOENT;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\t \n\tif (xen_initial_domain() && !(info->u.pirq.flags & PIRQ_MSI_GROUP)) {\n\t\tunmap_irq.pirq = info->u.pirq.pirq;\n\t\tunmap_irq.domid = info->u.pirq.domid;\n\t\trc = HYPERVISOR_physdev_op(PHYSDEVOP_unmap_pirq, &unmap_irq);\n\t\t \n\t\tif ((rc == -ESRCH && info->u.pirq.domid != DOMID_SELF))\n\t\t\tpr_info(\"domain %d does not have %d anymore\\n\",\n\t\t\t\tinfo->u.pirq.domid, info->u.pirq.pirq);\n\t\telse if (rc) {\n\t\t\tpr_warn(\"unmap irq failed %d\\n\", rc);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\txen_free_irq(irq);\n\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\treturn rc;\n}\n\nint xen_irq_from_pirq(unsigned pirq)\n{\n\tint irq;\n\n\tstruct irq_info *info;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tlist_for_each_entry(info, &xen_irq_list_head, list) {\n\t\tif (info->type != IRQT_PIRQ)\n\t\t\tcontinue;\n\t\tirq = info->irq;\n\t\tif (info->u.pirq.pirq == pirq)\n\t\t\tgoto out;\n\t}\n\tirq = -1;\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\treturn irq;\n}\n\n\nint xen_pirq_from_irq(unsigned irq)\n{\n\treturn pirq_from_irq(irq);\n}\nEXPORT_SYMBOL_GPL(xen_pirq_from_irq);\n\nstatic int bind_evtchn_to_irq_chip(evtchn_port_t evtchn, struct irq_chip *chip,\n\t\t\t\t   struct xenbus_device *dev)\n{\n\tint irq;\n\tint ret;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = get_evtchn_to_irq(evtchn);\n\n\tif (irq == -1) {\n\t\tirq = xen_allocate_irq_dynamic();\n\t\tif (irq < 0)\n\t\t\tgoto out;\n\n\t\tirq_set_chip_and_handler_name(irq, chip,\n\t\t\t\t\t      handle_edge_irq, \"event\");\n\n\t\tret = xen_irq_info_evtchn_setup(irq, evtchn, dev);\n\t\tif (ret < 0) {\n\t\t\t__unbind_from_irq(irq);\n\t\t\tirq = ret;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tbind_evtchn_to_cpu(evtchn, 0, false);\n\t} else {\n\t\tstruct irq_info *info = info_for_irq(irq);\n\t\tWARN_ON(info == NULL || info->type != IRQT_EVTCHN);\n\t}\n\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\treturn irq;\n}\n\nint bind_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\treturn bind_evtchn_to_irq_chip(evtchn, &xen_dynamic_chip, NULL);\n}\nEXPORT_SYMBOL_GPL(bind_evtchn_to_irq);\n\nint bind_evtchn_to_irq_lateeoi(evtchn_port_t evtchn)\n{\n\treturn bind_evtchn_to_irq_chip(evtchn, &xen_lateeoi_chip, NULL);\n}\nEXPORT_SYMBOL_GPL(bind_evtchn_to_irq_lateeoi);\n\nstatic int bind_ipi_to_irq(unsigned int ipi, unsigned int cpu)\n{\n\tstruct evtchn_bind_ipi bind_ipi;\n\tevtchn_port_t evtchn;\n\tint ret, irq;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = per_cpu(ipi_to_irq, cpu)[ipi];\n\n\tif (irq == -1) {\n\t\tirq = xen_allocate_irq_dynamic();\n\t\tif (irq < 0)\n\t\t\tgoto out;\n\n\t\tirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\n\t\t\t\t\t      handle_percpu_irq, \"ipi\");\n\n\t\tbind_ipi.vcpu = xen_vcpu_nr(cpu);\n\t\tif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\n\t\t\t\t\t\t&bind_ipi) != 0)\n\t\t\tBUG();\n\t\tevtchn = bind_ipi.port;\n\n\t\tret = xen_irq_info_ipi_setup(cpu, irq, evtchn, ipi);\n\t\tif (ret < 0) {\n\t\t\t__unbind_from_irq(irq);\n\t\t\tirq = ret;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tbind_evtchn_to_cpu(evtchn, cpu, true);\n\t} else {\n\t\tstruct irq_info *info = info_for_irq(irq);\n\t\tWARN_ON(info == NULL || info->type != IRQT_IPI);\n\t}\n\n out:\n\tmutex_unlock(&irq_mapping_update_lock);\n\treturn irq;\n}\n\nstatic int bind_interdomain_evtchn_to_irq_chip(struct xenbus_device *dev,\n\t\t\t\t\t       evtchn_port_t remote_port,\n\t\t\t\t\t       struct irq_chip *chip)\n{\n\tstruct evtchn_bind_interdomain bind_interdomain;\n\tint err;\n\n\tbind_interdomain.remote_dom  = dev->otherend_id;\n\tbind_interdomain.remote_port = remote_port;\n\n\terr = HYPERVISOR_event_channel_op(EVTCHNOP_bind_interdomain,\n\t\t\t\t\t  &bind_interdomain);\n\n\treturn err ? : bind_evtchn_to_irq_chip(bind_interdomain.local_port,\n\t\t\t\t\t       chip, dev);\n}\n\nint bind_interdomain_evtchn_to_irq_lateeoi(struct xenbus_device *dev,\n\t\t\t\t\t   evtchn_port_t remote_port)\n{\n\treturn bind_interdomain_evtchn_to_irq_chip(dev, remote_port,\n\t\t\t\t\t\t   &xen_lateeoi_chip);\n}\nEXPORT_SYMBOL_GPL(bind_interdomain_evtchn_to_irq_lateeoi);\n\nstatic int find_virq(unsigned int virq, unsigned int cpu, evtchn_port_t *evtchn)\n{\n\tstruct evtchn_status status;\n\tevtchn_port_t port;\n\tint rc = -ENOENT;\n\n\tmemset(&status, 0, sizeof(status));\n\tfor (port = 0; port < xen_evtchn_max_channels(); port++) {\n\t\tstatus.dom = DOMID_SELF;\n\t\tstatus.port = port;\n\t\trc = HYPERVISOR_event_channel_op(EVTCHNOP_status, &status);\n\t\tif (rc < 0)\n\t\t\tcontinue;\n\t\tif (status.status != EVTCHNSTAT_virq)\n\t\t\tcontinue;\n\t\tif (status.u.virq == virq && status.vcpu == xen_vcpu_nr(cpu)) {\n\t\t\t*evtchn = port;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn rc;\n}\n\n \nunsigned xen_evtchn_nr_channels(void)\n{\n        return evtchn_ops->nr_channels();\n}\nEXPORT_SYMBOL_GPL(xen_evtchn_nr_channels);\n\nint bind_virq_to_irq(unsigned int virq, unsigned int cpu, bool percpu)\n{\n\tstruct evtchn_bind_virq bind_virq;\n\tevtchn_port_t evtchn = 0;\n\tint irq, ret;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = per_cpu(virq_to_irq, cpu)[virq];\n\n\tif (irq == -1) {\n\t\tirq = xen_allocate_irq_dynamic();\n\t\tif (irq < 0)\n\t\t\tgoto out;\n\n\t\tif (percpu)\n\t\t\tirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\n\t\t\t\t\t\t      handle_percpu_irq, \"virq\");\n\t\telse\n\t\t\tirq_set_chip_and_handler_name(irq, &xen_dynamic_chip,\n\t\t\t\t\t\t      handle_edge_irq, \"virq\");\n\n\t\tbind_virq.virq = virq;\n\t\tbind_virq.vcpu = xen_vcpu_nr(cpu);\n\t\tret = HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\n\t\t\t\t\t\t&bind_virq);\n\t\tif (ret == 0)\n\t\t\tevtchn = bind_virq.port;\n\t\telse {\n\t\t\tif (ret == -EEXIST)\n\t\t\t\tret = find_virq(virq, cpu, &evtchn);\n\t\t\tBUG_ON(ret < 0);\n\t\t}\n\n\t\tret = xen_irq_info_virq_setup(cpu, irq, evtchn, virq);\n\t\tif (ret < 0) {\n\t\t\t__unbind_from_irq(irq);\n\t\t\tirq = ret;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tbind_evtchn_to_cpu(evtchn, cpu, percpu);\n\t} else {\n\t\tstruct irq_info *info = info_for_irq(irq);\n\t\tWARN_ON(info == NULL || info->type != IRQT_VIRQ);\n\t}\n\nout:\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\treturn irq;\n}\n\nstatic void unbind_from_irq(unsigned int irq)\n{\n\tmutex_lock(&irq_mapping_update_lock);\n\t__unbind_from_irq(irq);\n\tmutex_unlock(&irq_mapping_update_lock);\n}\n\nstatic int bind_evtchn_to_irqhandler_chip(evtchn_port_t evtchn,\n\t\t\t\t\t  irq_handler_t handler,\n\t\t\t\t\t  unsigned long irqflags,\n\t\t\t\t\t  const char *devname, void *dev_id,\n\t\t\t\t\t  struct irq_chip *chip)\n{\n\tint irq, retval;\n\n\tirq = bind_evtchn_to_irq_chip(evtchn, chip, NULL);\n\tif (irq < 0)\n\t\treturn irq;\n\tretval = request_irq(irq, handler, irqflags, devname, dev_id);\n\tif (retval != 0) {\n\t\tunbind_from_irq(irq);\n\t\treturn retval;\n\t}\n\n\treturn irq;\n}\n\nint bind_evtchn_to_irqhandler(evtchn_port_t evtchn,\n\t\t\t      irq_handler_t handler,\n\t\t\t      unsigned long irqflags,\n\t\t\t      const char *devname, void *dev_id)\n{\n\treturn bind_evtchn_to_irqhandler_chip(evtchn, handler, irqflags,\n\t\t\t\t\t      devname, dev_id,\n\t\t\t\t\t      &xen_dynamic_chip);\n}\nEXPORT_SYMBOL_GPL(bind_evtchn_to_irqhandler);\n\nint bind_evtchn_to_irqhandler_lateeoi(evtchn_port_t evtchn,\n\t\t\t\t      irq_handler_t handler,\n\t\t\t\t      unsigned long irqflags,\n\t\t\t\t      const char *devname, void *dev_id)\n{\n\treturn bind_evtchn_to_irqhandler_chip(evtchn, handler, irqflags,\n\t\t\t\t\t      devname, dev_id,\n\t\t\t\t\t      &xen_lateeoi_chip);\n}\nEXPORT_SYMBOL_GPL(bind_evtchn_to_irqhandler_lateeoi);\n\nstatic int bind_interdomain_evtchn_to_irqhandler_chip(\n\t\tstruct xenbus_device *dev, evtchn_port_t remote_port,\n\t\tirq_handler_t handler, unsigned long irqflags,\n\t\tconst char *devname, void *dev_id, struct irq_chip *chip)\n{\n\tint irq, retval;\n\n\tirq = bind_interdomain_evtchn_to_irq_chip(dev, remote_port, chip);\n\tif (irq < 0)\n\t\treturn irq;\n\n\tretval = request_irq(irq, handler, irqflags, devname, dev_id);\n\tif (retval != 0) {\n\t\tunbind_from_irq(irq);\n\t\treturn retval;\n\t}\n\n\treturn irq;\n}\n\nint bind_interdomain_evtchn_to_irqhandler_lateeoi(struct xenbus_device *dev,\n\t\t\t\t\t\t  evtchn_port_t remote_port,\n\t\t\t\t\t\t  irq_handler_t handler,\n\t\t\t\t\t\t  unsigned long irqflags,\n\t\t\t\t\t\t  const char *devname,\n\t\t\t\t\t\t  void *dev_id)\n{\n\treturn bind_interdomain_evtchn_to_irqhandler_chip(dev,\n\t\t\t\tremote_port, handler, irqflags, devname,\n\t\t\t\tdev_id, &xen_lateeoi_chip);\n}\nEXPORT_SYMBOL_GPL(bind_interdomain_evtchn_to_irqhandler_lateeoi);\n\nint bind_virq_to_irqhandler(unsigned int virq, unsigned int cpu,\n\t\t\t    irq_handler_t handler,\n\t\t\t    unsigned long irqflags, const char *devname, void *dev_id)\n{\n\tint irq, retval;\n\n\tirq = bind_virq_to_irq(virq, cpu, irqflags & IRQF_PERCPU);\n\tif (irq < 0)\n\t\treturn irq;\n\tretval = request_irq(irq, handler, irqflags, devname, dev_id);\n\tif (retval != 0) {\n\t\tunbind_from_irq(irq);\n\t\treturn retval;\n\t}\n\n\treturn irq;\n}\nEXPORT_SYMBOL_GPL(bind_virq_to_irqhandler);\n\nint bind_ipi_to_irqhandler(enum ipi_vector ipi,\n\t\t\t   unsigned int cpu,\n\t\t\t   irq_handler_t handler,\n\t\t\t   unsigned long irqflags,\n\t\t\t   const char *devname,\n\t\t\t   void *dev_id)\n{\n\tint irq, retval;\n\n\tirq = bind_ipi_to_irq(ipi, cpu);\n\tif (irq < 0)\n\t\treturn irq;\n\n\tirqflags |= IRQF_NO_SUSPEND | IRQF_FORCE_RESUME | IRQF_EARLY_RESUME;\n\tretval = request_irq(irq, handler, irqflags, devname, dev_id);\n\tif (retval != 0) {\n\t\tunbind_from_irq(irq);\n\t\treturn retval;\n\t}\n\n\treturn irq;\n}\n\nvoid unbind_from_irqhandler(unsigned int irq, void *dev_id)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\tfree_irq(irq, dev_id);\n\tunbind_from_irq(irq);\n}\nEXPORT_SYMBOL_GPL(unbind_from_irqhandler);\n\n \nint xen_set_irq_priority(unsigned irq, unsigned priority)\n{\n\tstruct evtchn_set_priority set_priority;\n\n\tset_priority.port = evtchn_from_irq(irq);\n\tset_priority.priority = priority;\n\n\treturn HYPERVISOR_event_channel_op(EVTCHNOP_set_priority,\n\t\t\t\t\t   &set_priority);\n}\nEXPORT_SYMBOL_GPL(xen_set_irq_priority);\n\nint evtchn_make_refcounted(evtchn_port_t evtchn, bool is_static)\n{\n\tint irq = get_evtchn_to_irq(evtchn);\n\tstruct irq_info *info;\n\n\tif (irq == -1)\n\t\treturn -ENOENT;\n\n\tinfo = info_for_irq(irq);\n\n\tif (!info)\n\t\treturn -ENOENT;\n\n\tWARN_ON(info->refcnt != -1);\n\n\tinfo->refcnt = 1;\n\tinfo->is_static = is_static;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(evtchn_make_refcounted);\n\nint evtchn_get(evtchn_port_t evtchn)\n{\n\tint irq;\n\tstruct irq_info *info;\n\tint err = -ENOENT;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\tirq = get_evtchn_to_irq(evtchn);\n\tif (irq == -1)\n\t\tgoto done;\n\n\tinfo = info_for_irq(irq);\n\n\tif (!info)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (info->refcnt <= 0 || info->refcnt == SHRT_MAX)\n\t\tgoto done;\n\n\tinfo->refcnt++;\n\terr = 0;\n done:\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(evtchn_get);\n\nvoid evtchn_put(evtchn_port_t evtchn)\n{\n\tint irq = get_evtchn_to_irq(evtchn);\n\tif (WARN_ON(irq == -1))\n\t\treturn;\n\tunbind_from_irq(irq);\n}\nEXPORT_SYMBOL_GPL(evtchn_put);\n\nvoid xen_send_IPI_one(unsigned int cpu, enum ipi_vector vector)\n{\n\tevtchn_port_t evtchn;\n\n#ifdef CONFIG_X86\n\tif (unlikely(vector == XEN_NMI_VECTOR)) {\n\t\tint rc =  HYPERVISOR_vcpu_op(VCPUOP_send_nmi, xen_vcpu_nr(cpu),\n\t\t\t\t\t     NULL);\n\t\tif (rc < 0)\n\t\t\tprintk(KERN_WARNING \"Sending nmi to CPU%d failed (rc:%d)\\n\", cpu, rc);\n\t\treturn;\n\t}\n#endif\n\tevtchn = per_cpu(ipi_to_evtchn, cpu)[vector];\n\tBUG_ON(evtchn == 0);\n\tnotify_remote_via_evtchn(evtchn);\n}\n\nstruct evtchn_loop_ctrl {\n\tktime_t timeout;\n\tunsigned count;\n\tbool defer_eoi;\n};\n\nvoid handle_irq_for_port(evtchn_port_t port, struct evtchn_loop_ctrl *ctrl)\n{\n\tint irq;\n\tstruct irq_info *info;\n\tstruct xenbus_device *dev;\n\n\tirq = get_evtchn_to_irq(port);\n\tif (irq == -1)\n\t\treturn;\n\n\t \n\tif (!ctrl->defer_eoi && !(++ctrl->count & 0xff)) {\n\t\tktime_t kt = ktime_get();\n\n\t\tif (!ctrl->timeout) {\n\t\t\tkt = ktime_add_ms(kt,\n\t\t\t\t\t  jiffies_to_msecs(event_loop_timeout));\n\t\t\tctrl->timeout = kt;\n\t\t} else if (kt > ctrl->timeout) {\n\t\t\tctrl->defer_eoi = true;\n\t\t}\n\t}\n\n\tinfo = info_for_irq(irq);\n\tif (xchg_acquire(&info->is_active, 1))\n\t\treturn;\n\n\tdev = (info->type == IRQT_EVTCHN) ? info->u.interdomain : NULL;\n\tif (dev)\n\t\tatomic_inc(&dev->events);\n\n\tif (ctrl->defer_eoi) {\n\t\tinfo->eoi_cpu = smp_processor_id();\n\t\tinfo->irq_epoch = __this_cpu_read(irq_epoch);\n\t\tinfo->eoi_time = get_jiffies_64() + event_eoi_delay;\n\t}\n\n\tgeneric_handle_irq(irq);\n}\n\nint xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint ret = vcpu_info->evtchn_upcall_pending ? IRQ_HANDLED : IRQ_NONE;\n\tint cpu = smp_processor_id();\n\tstruct evtchn_loop_ctrl ctrl = { 0 };\n\n\t \n\trcu_read_lock();\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu, &ctrl);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb();  \n\n\t} while (vcpu_info->evtchn_upcall_pending);\n\n\trcu_read_unlock();\n\n\t \n\t__this_cpu_inc(irq_epoch);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xen_evtchn_do_upcall);\n\n \nvoid rebind_evtchn_irq(evtchn_port_t evtchn, int irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\t \n\tdisable_irq(irq);\n\n\tmutex_lock(&irq_mapping_update_lock);\n\n\t \n\tBUG_ON(get_evtchn_to_irq(evtchn) != -1);\n\t \n\tBUG_ON(info->type == IRQT_UNBOUND);\n\n\t(void)xen_irq_info_evtchn_setup(irq, evtchn, NULL);\n\n\tmutex_unlock(&irq_mapping_update_lock);\n\n\tbind_evtchn_to_cpu(evtchn, info->cpu, false);\n\n\t \n\tenable_irq(irq);\n}\n\n \nstatic int xen_rebind_evtchn_to_cpu(struct irq_info *info, unsigned int tcpu)\n{\n\tstruct evtchn_bind_vcpu bind_vcpu;\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (!VALID_EVTCHN(evtchn))\n\t\treturn -1;\n\n\tif (!xen_support_evtchn_rebind())\n\t\treturn -1;\n\n\t \n\tbind_vcpu.port = evtchn;\n\tbind_vcpu.vcpu = xen_vcpu_nr(tcpu);\n\n\t \n\tdo_mask(info, EVT_MASK_REASON_TEMPORARY);\n\n\t \n\tif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_vcpu, &bind_vcpu) >= 0)\n\t\tbind_evtchn_to_cpu(evtchn, tcpu, false);\n\n\tdo_unmask(info, EVT_MASK_REASON_TEMPORARY);\n\n\treturn 0;\n}\n\n \nstatic unsigned int select_target_cpu(const struct cpumask *dest)\n{\n\tunsigned int cpu, best_cpu = UINT_MAX, minch = UINT_MAX;\n\n\tfor_each_cpu_and(cpu, dest, cpu_online_mask) {\n\t\tunsigned int curch = atomic_read(&channels_on_cpu[cpu]);\n\n\t\tif (curch < minch) {\n\t\t\tminch = curch;\n\t\t\tbest_cpu = cpu;\n\t\t}\n\t}\n\n\t \n\tif (best_cpu == UINT_MAX)\n\t\treturn select_target_cpu(cpu_online_mask);\n\n\treturn best_cpu;\n}\n\nstatic int set_affinity_irq(struct irq_data *data, const struct cpumask *dest,\n\t\t\t    bool force)\n{\n\tunsigned int tcpu = select_target_cpu(dest);\n\tint ret;\n\n\tret = xen_rebind_evtchn_to_cpu(info_for_irq(data->irq), tcpu);\n\tif (!ret)\n\t\tirq_data_update_effective_affinity(data, cpumask_of(tcpu));\n\n\treturn ret;\n}\n\nstatic void enable_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tdo_unmask(info, EVT_MASK_REASON_EXPLICIT);\n}\n\nstatic void disable_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tdo_mask(info, EVT_MASK_REASON_EXPLICIT);\n}\n\nstatic void ack_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tevent_handler_exit(info);\n}\n\nstatic void mask_ack_dynirq(struct irq_data *data)\n{\n\tdisable_dynirq(data);\n\tack_dynirq(data);\n}\n\nstatic void lateeoi_ack_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn)) {\n\t\tdo_mask(info, EVT_MASK_REASON_EOI_PENDING);\n\t\t \n\t\tclear_evtchn(evtchn);\n\t}\n}\n\nstatic void lateeoi_mask_ack_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn)) {\n\t\tdo_mask(info, EVT_MASK_REASON_EXPLICIT);\n\t\tevent_handler_exit(info);\n\t}\n}\n\nstatic int retrigger_dynirq(struct irq_data *data)\n{\n\tstruct irq_info *info = info_for_irq(data->irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (!VALID_EVTCHN(evtchn))\n\t\treturn 0;\n\n\tdo_mask(info, EVT_MASK_REASON_TEMPORARY);\n\tset_evtchn(evtchn);\n\tdo_unmask(info, EVT_MASK_REASON_TEMPORARY);\n\n\treturn 1;\n}\n\nstatic void restore_pirqs(void)\n{\n\tint pirq, rc, irq, gsi;\n\tstruct physdev_map_pirq map_irq;\n\tstruct irq_info *info;\n\n\tlist_for_each_entry(info, &xen_irq_list_head, list) {\n\t\tif (info->type != IRQT_PIRQ)\n\t\t\tcontinue;\n\n\t\tpirq = info->u.pirq.pirq;\n\t\tgsi = info->u.pirq.gsi;\n\t\tirq = info->irq;\n\n\t\t \n\t\tif (!gsi)\n\t\t\tcontinue;\n\n\t\tmap_irq.domid = DOMID_SELF;\n\t\tmap_irq.type = MAP_PIRQ_TYPE_GSI;\n\t\tmap_irq.index = gsi;\n\t\tmap_irq.pirq = pirq;\n\n\t\trc = HYPERVISOR_physdev_op(PHYSDEVOP_map_pirq, &map_irq);\n\t\tif (rc) {\n\t\t\tpr_warn(\"xen map irq failed gsi=%d irq=%d pirq=%d rc=%d\\n\",\n\t\t\t\tgsi, irq, pirq, rc);\n\t\t\txen_free_irq(irq);\n\t\t\tcontinue;\n\t\t}\n\n\t\tprintk(KERN_DEBUG \"xen: --> irq=%d, pirq=%d\\n\", irq, map_irq.pirq);\n\n\t\t__startup_pirq(irq);\n\t}\n}\n\nstatic void restore_cpu_virqs(unsigned int cpu)\n{\n\tstruct evtchn_bind_virq bind_virq;\n\tevtchn_port_t evtchn;\n\tint virq, irq;\n\n\tfor (virq = 0; virq < NR_VIRQS; virq++) {\n\t\tif ((irq = per_cpu(virq_to_irq, cpu)[virq]) == -1)\n\t\t\tcontinue;\n\n\t\tBUG_ON(virq_from_irq(irq) != virq);\n\n\t\t \n\t\tbind_virq.virq = virq;\n\t\tbind_virq.vcpu = xen_vcpu_nr(cpu);\n\t\tif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\n\t\t\t\t\t\t&bind_virq) != 0)\n\t\t\tBUG();\n\t\tevtchn = bind_virq.port;\n\n\t\t \n\t\t(void)xen_irq_info_virq_setup(cpu, irq, evtchn, virq);\n\t\t \n\t\tbind_evtchn_to_cpu(evtchn, cpu, false);\n\t}\n}\n\nstatic void restore_cpu_ipis(unsigned int cpu)\n{\n\tstruct evtchn_bind_ipi bind_ipi;\n\tevtchn_port_t evtchn;\n\tint ipi, irq;\n\n\tfor (ipi = 0; ipi < XEN_NR_IPIS; ipi++) {\n\t\tif ((irq = per_cpu(ipi_to_irq, cpu)[ipi]) == -1)\n\t\t\tcontinue;\n\n\t\tBUG_ON(ipi_from_irq(irq) != ipi);\n\n\t\t \n\t\tbind_ipi.vcpu = xen_vcpu_nr(cpu);\n\t\tif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\n\t\t\t\t\t\t&bind_ipi) != 0)\n\t\t\tBUG();\n\t\tevtchn = bind_ipi.port;\n\n\t\t \n\t\t(void)xen_irq_info_ipi_setup(cpu, irq, evtchn, ipi);\n\t\t \n\t\tbind_evtchn_to_cpu(evtchn, cpu, false);\n\t}\n}\n\n \nvoid xen_clear_irq_pending(int irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tevtchn_port_t evtchn = info ? info->evtchn : 0;\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tevent_handler_exit(info);\n}\nEXPORT_SYMBOL(xen_clear_irq_pending);\nvoid xen_set_irq_pending(int irq)\n{\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tset_evtchn(evtchn);\n}\n\nbool xen_test_irq_pending(int irq)\n{\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\tbool ret = false;\n\n\tif (VALID_EVTCHN(evtchn))\n\t\tret = test_evtchn(evtchn);\n\n\treturn ret;\n}\n\n \nvoid xen_poll_irq_timeout(int irq, u64 timeout)\n{\n\tevtchn_port_t evtchn = evtchn_from_irq(irq);\n\n\tif (VALID_EVTCHN(evtchn)) {\n\t\tstruct sched_poll poll;\n\n\t\tpoll.nr_ports = 1;\n\t\tpoll.timeout = timeout;\n\t\tset_xen_guest_handle(poll.ports, &evtchn);\n\n\t\tif (HYPERVISOR_sched_op(SCHEDOP_poll, &poll) != 0)\n\t\t\tBUG();\n\t}\n}\nEXPORT_SYMBOL(xen_poll_irq_timeout);\n \nvoid xen_poll_irq(int irq)\n{\n\txen_poll_irq_timeout(irq, 0  );\n}\n\n \nint xen_test_irq_shared(int irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tstruct physdev_irq_status_query irq_status;\n\n\tif (WARN_ON(!info))\n\t\treturn -ENOENT;\n\n\tirq_status.irq = info->u.pirq.pirq;\n\n\tif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\n\t\treturn 0;\n\treturn !(irq_status.flags & XENIRQSTAT_shared);\n}\nEXPORT_SYMBOL_GPL(xen_test_irq_shared);\n\nvoid xen_irq_resume(void)\n{\n\tunsigned int cpu;\n\tstruct irq_info *info;\n\n\t \n\txen_evtchn_resume();\n\n\t \n\tlist_for_each_entry(info, &xen_irq_list_head, list) {\n\t\t \n\t\tinfo->evtchn = 0;\n\t\t \n\t\tchannels_on_cpu_dec(info);\n\t}\n\n\tclear_evtchn_to_irq_all();\n\n\tfor_each_possible_cpu(cpu) {\n\t\trestore_cpu_virqs(cpu);\n\t\trestore_cpu_ipis(cpu);\n\t}\n\n\trestore_pirqs();\n}\n\nstatic struct irq_chip xen_dynamic_chip __read_mostly = {\n\t.name\t\t\t= \"xen-dyn\",\n\n\t.irq_disable\t\t= disable_dynirq,\n\t.irq_mask\t\t= disable_dynirq,\n\t.irq_unmask\t\t= enable_dynirq,\n\n\t.irq_ack\t\t= ack_dynirq,\n\t.irq_mask_ack\t\t= mask_ack_dynirq,\n\n\t.irq_set_affinity\t= set_affinity_irq,\n\t.irq_retrigger\t\t= retrigger_dynirq,\n};\n\nstatic struct irq_chip xen_lateeoi_chip __read_mostly = {\n\t \n\t.name\t\t\t= \"xen-dyn-lateeoi\",\n\n\t.irq_disable\t\t= disable_dynirq,\n\t.irq_mask\t\t= disable_dynirq,\n\t.irq_unmask\t\t= enable_dynirq,\n\n\t.irq_ack\t\t= lateeoi_ack_dynirq,\n\t.irq_mask_ack\t\t= lateeoi_mask_ack_dynirq,\n\n\t.irq_set_affinity\t= set_affinity_irq,\n\t.irq_retrigger\t\t= retrigger_dynirq,\n};\n\nstatic struct irq_chip xen_pirq_chip __read_mostly = {\n\t.name\t\t\t= \"xen-pirq\",\n\n\t.irq_startup\t\t= startup_pirq,\n\t.irq_shutdown\t\t= shutdown_pirq,\n\t.irq_enable\t\t= enable_pirq,\n\t.irq_disable\t\t= disable_pirq,\n\n\t.irq_mask\t\t= disable_dynirq,\n\t.irq_unmask\t\t= enable_dynirq,\n\n\t.irq_ack\t\t= eoi_pirq,\n\t.irq_eoi\t\t= eoi_pirq,\n\t.irq_mask_ack\t\t= mask_ack_pirq,\n\n\t.irq_set_affinity\t= set_affinity_irq,\n\n\t.irq_retrigger\t\t= retrigger_dynirq,\n};\n\nstatic struct irq_chip xen_percpu_chip __read_mostly = {\n\t.name\t\t\t= \"xen-percpu\",\n\n\t.irq_disable\t\t= disable_dynirq,\n\t.irq_mask\t\t= disable_dynirq,\n\t.irq_unmask\t\t= enable_dynirq,\n\n\t.irq_ack\t\t= ack_dynirq,\n};\n\n#ifdef CONFIG_X86\n#ifdef CONFIG_XEN_PVHVM\n \nvoid xen_setup_callback_vector(void)\n{\n\tuint64_t callback_via;\n\n\tif (xen_have_vector_callback) {\n\t\tcallback_via = HVM_CALLBACK_VECTOR(HYPERVISOR_CALLBACK_VECTOR);\n\t\tif (xen_set_callback_via(callback_via)) {\n\t\t\tpr_err(\"Request for Xen HVM callback vector failed\\n\");\n\t\t\txen_have_vector_callback = false;\n\t\t}\n\t}\n}\n\n \nstatic __init void xen_init_setup_upcall_vector(void)\n{\n\tif (!xen_have_vector_callback)\n\t\treturn;\n\n\tif ((cpuid_eax(xen_cpuid_base() + 4) & XEN_HVM_CPUID_UPCALL_VECTOR) &&\n\t    !xen_set_upcall_vector(0))\n\t\txen_percpu_upcall = true;\n\telse if (xen_feature(XENFEAT_hvm_callback_vector))\n\t\txen_setup_callback_vector();\n\telse\n\t\txen_have_vector_callback = false;\n}\n\nint xen_set_upcall_vector(unsigned int cpu)\n{\n\tint rc;\n\txen_hvm_evtchn_upcall_vector_t op = {\n\t\t.vector = HYPERVISOR_CALLBACK_VECTOR,\n\t\t.vcpu = per_cpu(xen_vcpu_id, cpu),\n\t};\n\n\trc = HYPERVISOR_hvm_op(HVMOP_set_evtchn_upcall_vector, &op);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (!cpu)\n\t\trc = xen_set_callback_via(1);\n\n\treturn rc;\n}\n\nstatic __init void xen_alloc_callback_vector(void)\n{\n\tif (!xen_have_vector_callback)\n\t\treturn;\n\n\tpr_info(\"Xen HVM callback vector for event delivery is enabled\\n\");\n\talloc_intr_gate(HYPERVISOR_CALLBACK_VECTOR, asm_sysvec_xen_hvm_callback);\n}\n#else\nvoid xen_setup_callback_vector(void) {}\nstatic inline void xen_init_setup_upcall_vector(void) {}\nint xen_set_upcall_vector(unsigned int cpu) {}\nstatic inline void xen_alloc_callback_vector(void) {}\n#endif  \n#endif  \n\nbool xen_fifo_events = true;\nmodule_param_named(fifo_events, xen_fifo_events, bool, 0);\n\nstatic int xen_evtchn_cpu_prepare(unsigned int cpu)\n{\n\tint ret = 0;\n\n\txen_cpu_init_eoi(cpu);\n\n\tif (evtchn_ops->percpu_init)\n\t\tret = evtchn_ops->percpu_init(cpu);\n\n\treturn ret;\n}\n\nstatic int xen_evtchn_cpu_dead(unsigned int cpu)\n{\n\tint ret = 0;\n\n\tif (evtchn_ops->percpu_deinit)\n\t\tret = evtchn_ops->percpu_deinit(cpu);\n\n\treturn ret;\n}\n\nvoid __init xen_init_IRQ(void)\n{\n\tint ret = -EINVAL;\n\tevtchn_port_t evtchn;\n\n\tif (xen_fifo_events)\n\t\tret = xen_evtchn_fifo_init();\n\tif (ret < 0) {\n\t\txen_evtchn_2l_init();\n\t\txen_fifo_events = false;\n\t}\n\n\txen_cpu_init_eoi(smp_processor_id());\n\n\tcpuhp_setup_state_nocalls(CPUHP_XEN_EVTCHN_PREPARE,\n\t\t\t\t  \"xen/evtchn:prepare\",\n\t\t\t\t  xen_evtchn_cpu_prepare, xen_evtchn_cpu_dead);\n\n\tevtchn_to_irq = kcalloc(EVTCHN_ROW(xen_evtchn_max_channels()),\n\t\t\t\tsizeof(*evtchn_to_irq), GFP_KERNEL);\n\tBUG_ON(!evtchn_to_irq);\n\n\t \n\tfor (evtchn = 0; evtchn < xen_evtchn_nr_channels(); evtchn++)\n\t\tmask_evtchn(evtchn);\n\n\tpirq_needs_eoi = pirq_needs_eoi_flag;\n\n#ifdef CONFIG_X86\n\tif (xen_pv_domain()) {\n\t\tif (xen_initial_domain())\n\t\t\tpci_xen_initial_domain();\n\t}\n\txen_init_setup_upcall_vector();\n\txen_alloc_callback_vector();\n\n\n\tif (xen_hvm_domain()) {\n\t\tnative_init_IRQ();\n\t\t \n\t\tpci_xen_hvm_init();\n\t} else {\n\t\tint rc;\n\t\tstruct physdev_pirq_eoi_gmfn eoi_gmfn;\n\n\t\tpirq_eoi_map = (void *)__get_free_page(GFP_KERNEL|__GFP_ZERO);\n\t\teoi_gmfn.gmfn = virt_to_gfn(pirq_eoi_map);\n\t\trc = HYPERVISOR_physdev_op(PHYSDEVOP_pirq_eoi_gmfn_v2, &eoi_gmfn);\n\t\tif (rc != 0) {\n\t\t\tfree_page((unsigned long) pirq_eoi_map);\n\t\t\tpirq_eoi_map = NULL;\n\t\t} else\n\t\t\tpirq_needs_eoi = pirq_check_eoi_map;\n\t}\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}