{
  "module_name": "xenbus_client.c",
  "hash_id": "e067fe4915bd08c754edd53ffbebc631083c7361ab022e856ea789ef255856bf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/xenbus/xenbus_client.c",
  "human_readable_source": " \n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <asm/xen/hypervisor.h>\n#include <xen/page.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/event_channel.h>\n#include <xen/balloon.h>\n#include <xen/events.h>\n#include <xen/grant_table.h>\n#include <xen/xenbus.h>\n#include <xen/xen.h>\n#include <xen/features.h>\n\n#include \"xenbus.h\"\n\n#define XENBUS_PAGES(_grants)\t(DIV_ROUND_UP(_grants, XEN_PFN_PER_PAGE))\n\n#define XENBUS_MAX_RING_PAGES\t(XENBUS_PAGES(XENBUS_MAX_RING_GRANTS))\n\nstruct xenbus_map_node {\n\tstruct list_head next;\n\tunion {\n\t\tstruct {\n\t\t\tstruct vm_struct *area;\n\t\t} pv;\n\t\tstruct {\n\t\t\tstruct page *pages[XENBUS_MAX_RING_PAGES];\n\t\t\tunsigned long addrs[XENBUS_MAX_RING_GRANTS];\n\t\t\tvoid *addr;\n\t\t} hvm;\n\t};\n\tgrant_handle_t handles[XENBUS_MAX_RING_GRANTS];\n\tunsigned int   nr_handles;\n};\n\nstruct map_ring_valloc {\n\tstruct xenbus_map_node *node;\n\n\t \n\tunsigned long addrs[XENBUS_MAX_RING_GRANTS];\n\tphys_addr_t phys_addrs[XENBUS_MAX_RING_GRANTS];\n\n\tstruct gnttab_map_grant_ref map[XENBUS_MAX_RING_GRANTS];\n\tstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\n\n\tunsigned int idx;\n};\n\nstatic DEFINE_SPINLOCK(xenbus_valloc_lock);\nstatic LIST_HEAD(xenbus_valloc_pages);\n\nstruct xenbus_ring_ops {\n\tint (*map)(struct xenbus_device *dev, struct map_ring_valloc *info,\n\t\t   grant_ref_t *gnt_refs, unsigned int nr_grefs,\n\t\t   void **vaddr);\n\tint (*unmap)(struct xenbus_device *dev, void *vaddr);\n};\n\nstatic const struct xenbus_ring_ops *ring_ops __read_mostly;\n\nconst char *xenbus_strstate(enum xenbus_state state)\n{\n\tstatic const char *const name[] = {\n\t\t[ XenbusStateUnknown      ] = \"Unknown\",\n\t\t[ XenbusStateInitialising ] = \"Initialising\",\n\t\t[ XenbusStateInitWait     ] = \"InitWait\",\n\t\t[ XenbusStateInitialised  ] = \"Initialised\",\n\t\t[ XenbusStateConnected    ] = \"Connected\",\n\t\t[ XenbusStateClosing      ] = \"Closing\",\n\t\t[ XenbusStateClosed\t  ] = \"Closed\",\n\t\t[XenbusStateReconfiguring] = \"Reconfiguring\",\n\t\t[XenbusStateReconfigured] = \"Reconfigured\",\n\t};\n\treturn (state < ARRAY_SIZE(name)) ? name[state] : \"INVALID\";\n}\nEXPORT_SYMBOL_GPL(xenbus_strstate);\n\n \nint xenbus_watch_path(struct xenbus_device *dev, const char *path,\n\t\t      struct xenbus_watch *watch,\n\t\t      bool (*will_handle)(struct xenbus_watch *,\n\t\t\t\t\t  const char *, const char *),\n\t\t      void (*callback)(struct xenbus_watch *,\n\t\t\t\t       const char *, const char *))\n{\n\tint err;\n\n\twatch->node = path;\n\twatch->will_handle = will_handle;\n\twatch->callback = callback;\n\n\terr = register_xenbus_watch(watch);\n\n\tif (err) {\n\t\twatch->node = NULL;\n\t\twatch->will_handle = NULL;\n\t\twatch->callback = NULL;\n\t\txenbus_dev_fatal(dev, err, \"adding watch on %s\", path);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xenbus_watch_path);\n\n\n \nint xenbus_watch_pathfmt(struct xenbus_device *dev,\n\t\t\t struct xenbus_watch *watch,\n\t\t\t bool (*will_handle)(struct xenbus_watch *,\n\t\t\t\t\tconst char *, const char *),\n\t\t\t void (*callback)(struct xenbus_watch *,\n\t\t\t\t\t  const char *, const char *),\n\t\t\t const char *pathfmt, ...)\n{\n\tint err;\n\tva_list ap;\n\tchar *path;\n\n\tva_start(ap, pathfmt);\n\tpath = kvasprintf(GFP_NOIO | __GFP_HIGH, pathfmt, ap);\n\tva_end(ap);\n\n\tif (!path) {\n\t\txenbus_dev_fatal(dev, -ENOMEM, \"allocating path for watch\");\n\t\treturn -ENOMEM;\n\t}\n\terr = xenbus_watch_path(dev, path, watch, will_handle, callback);\n\n\tif (err)\n\t\tkfree(path);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xenbus_watch_pathfmt);\n\nstatic void xenbus_switch_fatal(struct xenbus_device *, int, int,\n\t\t\t\tconst char *, ...);\n\nstatic int\n__xenbus_switch_state(struct xenbus_device *dev,\n\t\t      enum xenbus_state state, int depth)\n{\n\t \n\n\tstruct xenbus_transaction xbt;\n\tint current_state;\n\tint err, abort;\n\n\tif (state == dev->state)\n\t\treturn 0;\n\nagain:\n\tabort = 1;\n\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_switch_fatal(dev, depth, err, \"starting transaction\");\n\t\treturn 0;\n\t}\n\n\terr = xenbus_scanf(xbt, dev->nodename, \"state\", \"%d\", &current_state);\n\tif (err != 1)\n\t\tgoto abort;\n\n\terr = xenbus_printf(xbt, dev->nodename, \"state\", \"%d\", state);\n\tif (err) {\n\t\txenbus_switch_fatal(dev, depth, err, \"writing new state\");\n\t\tgoto abort;\n\t}\n\n\tabort = 0;\nabort:\n\terr = xenbus_transaction_end(xbt, abort);\n\tif (err) {\n\t\tif (err == -EAGAIN && !abort)\n\t\t\tgoto again;\n\t\txenbus_switch_fatal(dev, depth, err, \"ending transaction\");\n\t} else\n\t\tdev->state = state;\n\n\treturn 0;\n}\n\n \nint xenbus_switch_state(struct xenbus_device *dev, enum xenbus_state state)\n{\n\treturn __xenbus_switch_state(dev, state, 0);\n}\n\nEXPORT_SYMBOL_GPL(xenbus_switch_state);\n\nint xenbus_frontend_closed(struct xenbus_device *dev)\n{\n\txenbus_switch_state(dev, XenbusStateClosed);\n\tcomplete(&dev->down);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xenbus_frontend_closed);\n\nstatic void xenbus_va_dev_error(struct xenbus_device *dev, int err,\n\t\t\t\tconst char *fmt, va_list ap)\n{\n\tunsigned int len;\n\tchar *printf_buffer;\n\tchar *path_buffer;\n\n#define PRINTF_BUFFER_SIZE 4096\n\n\tprintf_buffer = kmalloc(PRINTF_BUFFER_SIZE, GFP_KERNEL);\n\tif (!printf_buffer)\n\t\treturn;\n\n\tlen = sprintf(printf_buffer, \"%i \", -err);\n\tvsnprintf(printf_buffer + len, PRINTF_BUFFER_SIZE - len, fmt, ap);\n\n\tdev_err(&dev->dev, \"%s\\n\", printf_buffer);\n\n\tpath_buffer = kasprintf(GFP_KERNEL, \"error/%s\", dev->nodename);\n\tif (path_buffer)\n\t\txenbus_write(XBT_NIL, path_buffer, \"error\", printf_buffer);\n\n\tkfree(printf_buffer);\n\tkfree(path_buffer);\n}\n\n \nvoid xenbus_dev_error(struct xenbus_device *dev, int err, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\txenbus_va_dev_error(dev, err, fmt, ap);\n\tva_end(ap);\n}\nEXPORT_SYMBOL_GPL(xenbus_dev_error);\n\n \n\nvoid xenbus_dev_fatal(struct xenbus_device *dev, int err, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\txenbus_va_dev_error(dev, err, fmt, ap);\n\tva_end(ap);\n\n\txenbus_switch_state(dev, XenbusStateClosing);\n}\nEXPORT_SYMBOL_GPL(xenbus_dev_fatal);\n\n \nstatic void xenbus_switch_fatal(struct xenbus_device *dev, int depth, int err,\n\t\t\t\tconst char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\txenbus_va_dev_error(dev, err, fmt, ap);\n\tva_end(ap);\n\n\tif (!depth)\n\t\t__xenbus_switch_state(dev, XenbusStateClosing, 1);\n}\n\n \nint xenbus_setup_ring(struct xenbus_device *dev, gfp_t gfp, void **vaddr,\n\t\t      unsigned int nr_pages, grant_ref_t *grefs)\n{\n\tunsigned long ring_size = nr_pages * XEN_PAGE_SIZE;\n\tgrant_ref_t gref_head;\n\tunsigned int i;\n\tvoid *addr;\n\tint ret;\n\n\taddr = *vaddr = alloc_pages_exact(ring_size, gfp | __GFP_ZERO);\n\tif (!*vaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tret = gnttab_alloc_grant_references(nr_pages, &gref_head);\n\tif (ret) {\n\t\txenbus_dev_fatal(dev, ret, \"granting access to %u ring pages\",\n\t\t\t\t nr_pages);\n\t\tgoto err;\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long gfn;\n\n\t\tif (is_vmalloc_addr(*vaddr))\n\t\t\tgfn = pfn_to_gfn(vmalloc_to_pfn(addr));\n\t\telse\n\t\t\tgfn = virt_to_gfn(addr);\n\n\t\tgrefs[i] = gnttab_claim_grant_reference(&gref_head);\n\t\tgnttab_grant_foreign_access_ref(grefs[i], dev->otherend_id,\n\t\t\t\t\t\tgfn, 0);\n\n\t\taddr += XEN_PAGE_SIZE;\n\t}\n\n\treturn 0;\n\n err:\n\tif (*vaddr)\n\t\tfree_pages_exact(*vaddr, ring_size);\n\tfor (i = 0; i < nr_pages; i++)\n\t\tgrefs[i] = INVALID_GRANT_REF;\n\t*vaddr = NULL;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xenbus_setup_ring);\n\n \nvoid xenbus_teardown_ring(void **vaddr, unsigned int nr_pages,\n\t\t\t  grant_ref_t *grefs)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (grefs[i] != INVALID_GRANT_REF) {\n\t\t\tgnttab_end_foreign_access(grefs[i], NULL);\n\t\t\tgrefs[i] = INVALID_GRANT_REF;\n\t\t}\n\t}\n\n\tif (*vaddr)\n\t\tfree_pages_exact(*vaddr, nr_pages * XEN_PAGE_SIZE);\n\t*vaddr = NULL;\n}\nEXPORT_SYMBOL_GPL(xenbus_teardown_ring);\n\n \nint xenbus_alloc_evtchn(struct xenbus_device *dev, evtchn_port_t *port)\n{\n\tstruct evtchn_alloc_unbound alloc_unbound;\n\tint err;\n\n\talloc_unbound.dom = DOMID_SELF;\n\talloc_unbound.remote_dom = dev->otherend_id;\n\n\terr = HYPERVISOR_event_channel_op(EVTCHNOP_alloc_unbound,\n\t\t\t\t\t  &alloc_unbound);\n\tif (err)\n\t\txenbus_dev_fatal(dev, err, \"allocating event channel\");\n\telse\n\t\t*port = alloc_unbound.port;\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xenbus_alloc_evtchn);\n\n\n \nint xenbus_free_evtchn(struct xenbus_device *dev, evtchn_port_t port)\n{\n\tstruct evtchn_close close;\n\tint err;\n\n\tclose.port = port;\n\n\terr = HYPERVISOR_event_channel_op(EVTCHNOP_close, &close);\n\tif (err)\n\t\txenbus_dev_error(dev, err, \"freeing event channel %u\", port);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xenbus_free_evtchn);\n\n\n \nint xenbus_map_ring_valloc(struct xenbus_device *dev, grant_ref_t *gnt_refs,\n\t\t\t   unsigned int nr_grefs, void **vaddr)\n{\n\tint err;\n\tstruct map_ring_valloc *info;\n\n\t*vaddr = NULL;\n\n\tif (nr_grefs > XENBUS_MAX_RING_GRANTS)\n\t\treturn -EINVAL;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->node = kzalloc(sizeof(*info->node), GFP_KERNEL);\n\tif (!info->node)\n\t\terr = -ENOMEM;\n\telse\n\t\terr = ring_ops->map(dev, info, gnt_refs, nr_grefs, vaddr);\n\n\tkfree(info->node);\n\tkfree(info);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xenbus_map_ring_valloc);\n\n \nstatic int __xenbus_map_ring(struct xenbus_device *dev,\n\t\t\t     grant_ref_t *gnt_refs,\n\t\t\t     unsigned int nr_grefs,\n\t\t\t     grant_handle_t *handles,\n\t\t\t     struct map_ring_valloc *info,\n\t\t\t     unsigned int flags,\n\t\t\t     bool *leaked)\n{\n\tint i, j;\n\n\tif (nr_grefs > XENBUS_MAX_RING_GRANTS)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < nr_grefs; i++) {\n\t\tgnttab_set_map_op(&info->map[i], info->phys_addrs[i], flags,\n\t\t\t\t  gnt_refs[i], dev->otherend_id);\n\t\thandles[i] = INVALID_GRANT_HANDLE;\n\t}\n\n\tgnttab_batch_map(info->map, i);\n\n\tfor (i = 0; i < nr_grefs; i++) {\n\t\tif (info->map[i].status != GNTST_okay) {\n\t\t\txenbus_dev_fatal(dev, info->map[i].status,\n\t\t\t\t\t \"mapping in shared page %d from domain %d\",\n\t\t\t\t\t gnt_refs[i], dev->otherend_id);\n\t\t\tgoto fail;\n\t\t} else\n\t\t\thandles[i] = info->map[i].handle;\n\t}\n\n\treturn 0;\n\n fail:\n\tfor (i = j = 0; i < nr_grefs; i++) {\n\t\tif (handles[i] != INVALID_GRANT_HANDLE) {\n\t\t\tgnttab_set_unmap_op(&info->unmap[j],\n\t\t\t\t\t    info->phys_addrs[i],\n\t\t\t\t\t    GNTMAP_host_map, handles[i]);\n\t\t\tj++;\n\t\t}\n\t}\n\n\tBUG_ON(HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, info->unmap, j));\n\n\t*leaked = false;\n\tfor (i = 0; i < j; i++) {\n\t\tif (info->unmap[i].status != GNTST_okay) {\n\t\t\t*leaked = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n \nstatic int xenbus_unmap_ring(struct xenbus_device *dev, grant_handle_t *handles,\n\t\t\t     unsigned int nr_handles, unsigned long *vaddrs)\n{\n\tstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\n\tint i;\n\tint err;\n\n\tif (nr_handles > XENBUS_MAX_RING_GRANTS)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < nr_handles; i++)\n\t\tgnttab_set_unmap_op(&unmap[i], vaddrs[i],\n\t\t\t\t    GNTMAP_host_map, handles[i]);\n\n\tBUG_ON(HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap, i));\n\n\terr = GNTST_okay;\n\tfor (i = 0; i < nr_handles; i++) {\n\t\tif (unmap[i].status != GNTST_okay) {\n\t\t\txenbus_dev_error(dev, unmap[i].status,\n\t\t\t\t\t \"unmapping page at handle %d error %d\",\n\t\t\t\t\t handles[i], unmap[i].status);\n\t\t\terr = unmap[i].status;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic void xenbus_map_ring_setup_grant_hvm(unsigned long gfn,\n\t\t\t\t\t    unsigned int goffset,\n\t\t\t\t\t    unsigned int len,\n\t\t\t\t\t    void *data)\n{\n\tstruct map_ring_valloc *info = data;\n\tunsigned long vaddr = (unsigned long)gfn_to_virt(gfn);\n\n\tinfo->phys_addrs[info->idx] = vaddr;\n\tinfo->addrs[info->idx] = vaddr;\n\n\tinfo->idx++;\n}\n\nstatic int xenbus_map_ring_hvm(struct xenbus_device *dev,\n\t\t\t       struct map_ring_valloc *info,\n\t\t\t       grant_ref_t *gnt_ref,\n\t\t\t       unsigned int nr_grefs,\n\t\t\t       void **vaddr)\n{\n\tstruct xenbus_map_node *node = info->node;\n\tint err;\n\tvoid *addr;\n\tbool leaked = false;\n\tunsigned int nr_pages = XENBUS_PAGES(nr_grefs);\n\n\terr = xen_alloc_unpopulated_pages(nr_pages, node->hvm.pages);\n\tif (err)\n\t\tgoto out_err;\n\n\tgnttab_foreach_grant(node->hvm.pages, nr_grefs,\n\t\t\t     xenbus_map_ring_setup_grant_hvm,\n\t\t\t     info);\n\n\terr = __xenbus_map_ring(dev, gnt_ref, nr_grefs, node->handles,\n\t\t\t\tinfo, GNTMAP_host_map, &leaked);\n\tnode->nr_handles = nr_grefs;\n\n\tif (err)\n\t\tgoto out_free_ballooned_pages;\n\n\taddr = vmap(node->hvm.pages, nr_pages, VM_MAP | VM_IOREMAP,\n\t\t    PAGE_KERNEL);\n\tif (!addr) {\n\t\terr = -ENOMEM;\n\t\tgoto out_xenbus_unmap_ring;\n\t}\n\n\tnode->hvm.addr = addr;\n\n\tspin_lock(&xenbus_valloc_lock);\n\tlist_add(&node->next, &xenbus_valloc_pages);\n\tspin_unlock(&xenbus_valloc_lock);\n\n\t*vaddr = addr;\n\tinfo->node = NULL;\n\n\treturn 0;\n\n out_xenbus_unmap_ring:\n\tif (!leaked)\n\t\txenbus_unmap_ring(dev, node->handles, nr_grefs, info->addrs);\n\telse\n\t\tpr_alert(\"leaking %p size %u page(s)\",\n\t\t\t addr, nr_pages);\n out_free_ballooned_pages:\n\tif (!leaked)\n\t\txen_free_unpopulated_pages(nr_pages, node->hvm.pages);\n out_err:\n\treturn err;\n}\n\n \nint xenbus_unmap_ring_vfree(struct xenbus_device *dev, void *vaddr)\n{\n\treturn ring_ops->unmap(dev, vaddr);\n}\nEXPORT_SYMBOL_GPL(xenbus_unmap_ring_vfree);\n\n#ifdef CONFIG_XEN_PV\nstatic int map_ring_apply(pte_t *pte, unsigned long addr, void *data)\n{\n\tstruct map_ring_valloc *info = data;\n\n\tinfo->phys_addrs[info->idx++] = arbitrary_virt_to_machine(pte).maddr;\n\treturn 0;\n}\n\nstatic int xenbus_map_ring_pv(struct xenbus_device *dev,\n\t\t\t      struct map_ring_valloc *info,\n\t\t\t      grant_ref_t *gnt_refs,\n\t\t\t      unsigned int nr_grefs,\n\t\t\t      void **vaddr)\n{\n\tstruct xenbus_map_node *node = info->node;\n\tstruct vm_struct *area;\n\tbool leaked = false;\n\tint err = -ENOMEM;\n\n\tarea = get_vm_area(XEN_PAGE_SIZE * nr_grefs, VM_IOREMAP);\n\tif (!area)\n\t\treturn -ENOMEM;\n\tif (apply_to_page_range(&init_mm, (unsigned long)area->addr,\n\t\t\t\tXEN_PAGE_SIZE * nr_grefs, map_ring_apply, info))\n\t\tgoto failed;\n\terr = __xenbus_map_ring(dev, gnt_refs, nr_grefs, node->handles,\n\t\t\t\tinfo, GNTMAP_host_map | GNTMAP_contains_pte,\n\t\t\t\t&leaked);\n\tif (err)\n\t\tgoto failed;\n\n\tnode->nr_handles = nr_grefs;\n\tnode->pv.area = area;\n\n\tspin_lock(&xenbus_valloc_lock);\n\tlist_add(&node->next, &xenbus_valloc_pages);\n\tspin_unlock(&xenbus_valloc_lock);\n\n\t*vaddr = area->addr;\n\tinfo->node = NULL;\n\n\treturn 0;\n\nfailed:\n\tif (!leaked)\n\t\tfree_vm_area(area);\n\telse\n\t\tpr_alert(\"leaking VM area %p size %u page(s)\", area, nr_grefs);\n\n\treturn err;\n}\n\nstatic int xenbus_unmap_ring_pv(struct xenbus_device *dev, void *vaddr)\n{\n\tstruct xenbus_map_node *node;\n\tstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\n\tunsigned int level;\n\tint i;\n\tbool leaked = false;\n\tint err;\n\n\tspin_lock(&xenbus_valloc_lock);\n\tlist_for_each_entry(node, &xenbus_valloc_pages, next) {\n\t\tif (node->pv.area->addr == vaddr) {\n\t\t\tlist_del(&node->next);\n\t\t\tgoto found;\n\t\t}\n\t}\n\tnode = NULL;\n found:\n\tspin_unlock(&xenbus_valloc_lock);\n\n\tif (!node) {\n\t\txenbus_dev_error(dev, -ENOENT,\n\t\t\t\t \"can't find mapped virtual address %p\", vaddr);\n\t\treturn GNTST_bad_virt_addr;\n\t}\n\n\tfor (i = 0; i < node->nr_handles; i++) {\n\t\tunsigned long addr;\n\n\t\tmemset(&unmap[i], 0, sizeof(unmap[i]));\n\t\taddr = (unsigned long)vaddr + (XEN_PAGE_SIZE * i);\n\t\tunmap[i].host_addr = arbitrary_virt_to_machine(\n\t\t\tlookup_address(addr, &level)).maddr;\n\t\tunmap[i].dev_bus_addr = 0;\n\t\tunmap[i].handle = node->handles[i];\n\t}\n\n\tBUG_ON(HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap, i));\n\n\terr = GNTST_okay;\n\tleaked = false;\n\tfor (i = 0; i < node->nr_handles; i++) {\n\t\tif (unmap[i].status != GNTST_okay) {\n\t\t\tleaked = true;\n\t\t\txenbus_dev_error(dev, unmap[i].status,\n\t\t\t\t\t \"unmapping page at handle %d error %d\",\n\t\t\t\t\t node->handles[i], unmap[i].status);\n\t\t\terr = unmap[i].status;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!leaked)\n\t\tfree_vm_area(node->pv.area);\n\telse\n\t\tpr_alert(\"leaking VM area %p size %u page(s)\",\n\t\t\t node->pv.area, node->nr_handles);\n\n\tkfree(node);\n\treturn err;\n}\n\nstatic const struct xenbus_ring_ops ring_ops_pv = {\n\t.map = xenbus_map_ring_pv,\n\t.unmap = xenbus_unmap_ring_pv,\n};\n#endif\n\nstruct unmap_ring_hvm\n{\n\tunsigned int idx;\n\tunsigned long addrs[XENBUS_MAX_RING_GRANTS];\n};\n\nstatic void xenbus_unmap_ring_setup_grant_hvm(unsigned long gfn,\n\t\t\t\t\t      unsigned int goffset,\n\t\t\t\t\t      unsigned int len,\n\t\t\t\t\t      void *data)\n{\n\tstruct unmap_ring_hvm *info = data;\n\n\tinfo->addrs[info->idx] = (unsigned long)gfn_to_virt(gfn);\n\n\tinfo->idx++;\n}\n\nstatic int xenbus_unmap_ring_hvm(struct xenbus_device *dev, void *vaddr)\n{\n\tint rv;\n\tstruct xenbus_map_node *node;\n\tvoid *addr;\n\tstruct unmap_ring_hvm info = {\n\t\t.idx = 0,\n\t};\n\tunsigned int nr_pages;\n\n\tspin_lock(&xenbus_valloc_lock);\n\tlist_for_each_entry(node, &xenbus_valloc_pages, next) {\n\t\taddr = node->hvm.addr;\n\t\tif (addr == vaddr) {\n\t\t\tlist_del(&node->next);\n\t\t\tgoto found;\n\t\t}\n\t}\n\tnode = addr = NULL;\n found:\n\tspin_unlock(&xenbus_valloc_lock);\n\n\tif (!node) {\n\t\txenbus_dev_error(dev, -ENOENT,\n\t\t\t\t \"can't find mapped virtual address %p\", vaddr);\n\t\treturn GNTST_bad_virt_addr;\n\t}\n\n\tnr_pages = XENBUS_PAGES(node->nr_handles);\n\n\tgnttab_foreach_grant(node->hvm.pages, node->nr_handles,\n\t\t\t     xenbus_unmap_ring_setup_grant_hvm,\n\t\t\t     &info);\n\n\trv = xenbus_unmap_ring(dev, node->handles, node->nr_handles,\n\t\t\t       info.addrs);\n\tif (!rv) {\n\t\tvunmap(vaddr);\n\t\txen_free_unpopulated_pages(nr_pages, node->hvm.pages);\n\t}\n\telse\n\t\tWARN(1, \"Leaking %p, size %u page(s)\\n\", vaddr, nr_pages);\n\n\tkfree(node);\n\treturn rv;\n}\n\n \nenum xenbus_state xenbus_read_driver_state(const char *path)\n{\n\tenum xenbus_state result;\n\tint err = xenbus_gather(XBT_NIL, path, \"state\", \"%d\", &result, NULL);\n\tif (err)\n\t\tresult = XenbusStateUnknown;\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xenbus_read_driver_state);\n\nstatic const struct xenbus_ring_ops ring_ops_hvm = {\n\t.map = xenbus_map_ring_hvm,\n\t.unmap = xenbus_unmap_ring_hvm,\n};\n\nvoid __init xenbus_ring_ops_init(void)\n{\n#ifdef CONFIG_XEN_PV\n\tif (!xen_feature(XENFEAT_auto_translated_physmap))\n\t\tring_ops = &ring_ops_pv;\n\telse\n#endif\n\t\tring_ops = &ring_ops_hvm;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}