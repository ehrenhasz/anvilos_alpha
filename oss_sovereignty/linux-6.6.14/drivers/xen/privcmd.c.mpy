{
  "module_name": "privcmd.c",
  "hash_id": "9c25be51368a902704621299582ec0b666430c0b9cfa7e6f34670e9723bebe87",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/privcmd.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"xen:\" KBUILD_MODNAME \": \" fmt\n\n#include <linux/eventfd.h>\n#include <linux/file.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/poll.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/workqueue.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/uaccess.h>\n#include <linux/swap.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/seq_file.h>\n#include <linux/miscdevice.h>\n#include <linux/moduleparam.h>\n\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/hypercall.h>\n\n#include <xen/xen.h>\n#include <xen/privcmd.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/hvm/dm_op.h>\n#include <xen/features.h>\n#include <xen/page.h>\n#include <xen/xen-ops.h>\n#include <xen/balloon.h>\n\n#include \"privcmd.h\"\n\nMODULE_LICENSE(\"GPL\");\n\n#define PRIV_VMA_LOCKED ((void *)1)\n\nstatic unsigned int privcmd_dm_op_max_num = 16;\nmodule_param_named(dm_op_max_nr_bufs, privcmd_dm_op_max_num, uint, 0644);\nMODULE_PARM_DESC(dm_op_max_nr_bufs,\n\t\t \"Maximum number of buffers per dm_op hypercall\");\n\nstatic unsigned int privcmd_dm_op_buf_max_size = 4096;\nmodule_param_named(dm_op_buf_max_size, privcmd_dm_op_buf_max_size, uint,\n\t\t   0644);\nMODULE_PARM_DESC(dm_op_buf_max_size,\n\t\t \"Maximum size of a dm_op hypercall buffer\");\n\nstruct privcmd_data {\n\tdomid_t domid;\n};\n\nstatic int privcmd_vma_range_is_mapped(\n               struct vm_area_struct *vma,\n               unsigned long addr,\n               unsigned long nr_pages);\n\nstatic long privcmd_ioctl_hypercall(struct file *file, void __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tstruct privcmd_hypercall hypercall;\n\tlong ret;\n\n\t \n\tif (data->domid != DOMID_INVALID)\n\t\treturn -EPERM;\n\n\tif (copy_from_user(&hypercall, udata, sizeof(hypercall)))\n\t\treturn -EFAULT;\n\n\txen_preemptible_hcall_begin();\n\tret = privcmd_call(hypercall.op,\n\t\t\t   hypercall.arg[0], hypercall.arg[1],\n\t\t\t   hypercall.arg[2], hypercall.arg[3],\n\t\t\t   hypercall.arg[4]);\n\txen_preemptible_hcall_end();\n\n\treturn ret;\n}\n\nstatic void free_page_list(struct list_head *pages)\n{\n\tstruct page *p, *n;\n\n\tlist_for_each_entry_safe(p, n, pages, lru)\n\t\t__free_page(p);\n\n\tINIT_LIST_HEAD(pages);\n}\n\n \nstatic int gather_array(struct list_head *pagelist,\n\t\t\tunsigned nelem, size_t size,\n\t\t\tconst void __user *data)\n{\n\tunsigned pageidx;\n\tvoid *pagedata;\n\tint ret;\n\n\tif (size > PAGE_SIZE)\n\t\treturn 0;\n\n\tpageidx = PAGE_SIZE;\n\tpagedata = NULL;\t \n\twhile (nelem--) {\n\t\tif (pageidx > PAGE_SIZE-size) {\n\t\t\tstruct page *page = alloc_page(GFP_KERNEL);\n\n\t\t\tret = -ENOMEM;\n\t\t\tif (page == NULL)\n\t\t\t\tgoto fail;\n\n\t\t\tpagedata = page_address(page);\n\n\t\t\tlist_add_tail(&page->lru, pagelist);\n\t\t\tpageidx = 0;\n\t\t}\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(pagedata + pageidx, data, size))\n\t\t\tgoto fail;\n\n\t\tdata += size;\n\t\tpageidx += size;\n\t}\n\n\tret = 0;\n\nfail:\n\treturn ret;\n}\n\n \nstatic int traverse_pages(unsigned nelem, size_t size,\n\t\t\t  struct list_head *pos,\n\t\t\t  int (*fn)(void *data, void *state),\n\t\t\t  void *state)\n{\n\tvoid *pagedata;\n\tunsigned pageidx;\n\tint ret = 0;\n\n\tBUG_ON(size > PAGE_SIZE);\n\n\tpageidx = PAGE_SIZE;\n\tpagedata = NULL;\t \n\n\twhile (nelem--) {\n\t\tif (pageidx > PAGE_SIZE-size) {\n\t\t\tstruct page *page;\n\t\t\tpos = pos->next;\n\t\t\tpage = list_entry(pos, struct page, lru);\n\t\t\tpagedata = page_address(page);\n\t\t\tpageidx = 0;\n\t\t}\n\n\t\tret = (*fn)(pagedata + pageidx, state);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tpageidx += size;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int traverse_pages_block(unsigned nelem, size_t size,\n\t\t\t\tstruct list_head *pos,\n\t\t\t\tint (*fn)(void *data, int nr, void *state),\n\t\t\t\tvoid *state)\n{\n\tvoid *pagedata;\n\tint ret = 0;\n\n\tBUG_ON(size > PAGE_SIZE);\n\n\twhile (nelem) {\n\t\tint nr = (PAGE_SIZE/size);\n\t\tstruct page *page;\n\t\tif (nr > nelem)\n\t\t\tnr = nelem;\n\t\tpos = pos->next;\n\t\tpage = list_entry(pos, struct page, lru);\n\t\tpagedata = page_address(page);\n\t\tret = (*fn)(pagedata, nr, state);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tnelem -= nr;\n\t}\n\n\treturn ret;\n}\n\nstruct mmap_gfn_state {\n\tunsigned long va;\n\tstruct vm_area_struct *vma;\n\tdomid_t domain;\n};\n\nstatic int mmap_gfn_range(void *data, void *state)\n{\n\tstruct privcmd_mmap_entry *msg = data;\n\tstruct mmap_gfn_state *st = state;\n\tstruct vm_area_struct *vma = st->vma;\n\tint rc;\n\n\t \n\tif ((msg->npages > (LONG_MAX >> PAGE_SHIFT)) ||\n\t    ((unsigned long)(msg->npages << PAGE_SHIFT) >= -st->va))\n\t\treturn -EINVAL;\n\n\t \n\tif ((msg->va != st->va) ||\n\t    ((msg->va+(msg->npages<<PAGE_SHIFT)) > vma->vm_end))\n\t\treturn -EINVAL;\n\n\trc = xen_remap_domain_gfn_range(vma,\n\t\t\t\t\tmsg->va & PAGE_MASK,\n\t\t\t\t\tmsg->mfn, msg->npages,\n\t\t\t\t\tvma->vm_page_prot,\n\t\t\t\t\tst->domain, NULL);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tst->va += msg->npages << PAGE_SHIFT;\n\n\treturn 0;\n}\n\nstatic long privcmd_ioctl_mmap(struct file *file, void __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tstruct privcmd_mmap mmapcmd;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tint rc;\n\tLIST_HEAD(pagelist);\n\tstruct mmap_gfn_state state;\n\n\t \n\tif (xen_feature(XENFEAT_auto_translated_physmap))\n\t\treturn -ENOSYS;\n\n\tif (copy_from_user(&mmapcmd, udata, sizeof(mmapcmd)))\n\t\treturn -EFAULT;\n\n\t \n\tif (data->domid != DOMID_INVALID && data->domid != mmapcmd.dom)\n\t\treturn -EPERM;\n\n\trc = gather_array(&pagelist,\n\t\t\t  mmapcmd.num, sizeof(struct privcmd_mmap_entry),\n\t\t\t  mmapcmd.entry);\n\n\tif (rc || list_empty(&pagelist))\n\t\tgoto out;\n\n\tmmap_write_lock(mm);\n\n\t{\n\t\tstruct page *page = list_first_entry(&pagelist,\n\t\t\t\t\t\t     struct page, lru);\n\t\tstruct privcmd_mmap_entry *msg = page_address(page);\n\n\t\tvma = vma_lookup(mm, msg->va);\n\t\trc = -EINVAL;\n\n\t\tif (!vma || (msg->va != vma->vm_start) || vma->vm_private_data)\n\t\t\tgoto out_up;\n\t\tvma->vm_private_data = PRIV_VMA_LOCKED;\n\t}\n\n\tstate.va = vma->vm_start;\n\tstate.vma = vma;\n\tstate.domain = mmapcmd.dom;\n\n\trc = traverse_pages(mmapcmd.num, sizeof(struct privcmd_mmap_entry),\n\t\t\t    &pagelist,\n\t\t\t    mmap_gfn_range, &state);\n\n\nout_up:\n\tmmap_write_unlock(mm);\n\nout:\n\tfree_page_list(&pagelist);\n\n\treturn rc;\n}\n\nstruct mmap_batch_state {\n\tdomid_t domain;\n\tunsigned long va;\n\tstruct vm_area_struct *vma;\n\tint index;\n\t \n\tint global_error;\n\tint version;\n\n\t \n\txen_pfn_t __user *user_gfn;\n\t \n\tint __user *user_err;\n};\n\n \nstatic int mmap_batch_fn(void *data, int nr, void *state)\n{\n\txen_pfn_t *gfnp = data;\n\tstruct mmap_batch_state *st = state;\n\tstruct vm_area_struct *vma = st->vma;\n\tstruct page **pages = vma->vm_private_data;\n\tstruct page **cur_pages = NULL;\n\tint ret;\n\n\tif (xen_feature(XENFEAT_auto_translated_physmap))\n\t\tcur_pages = &pages[st->index];\n\n\tBUG_ON(nr < 0);\n\tret = xen_remap_domain_gfn_array(st->vma, st->va & PAGE_MASK, gfnp, nr,\n\t\t\t\t\t (int *)gfnp, st->vma->vm_page_prot,\n\t\t\t\t\t st->domain, cur_pages);\n\n\t \n\tif (ret != nr) {\n\t\tif (ret == -ENOENT)\n\t\t\tst->global_error = -ENOENT;\n\t\telse {\n\t\t\t \n\t\t\tif (st->global_error == 0)\n\t\t\t\tst->global_error = 1;\n\t\t}\n\t}\n\tst->va += XEN_PAGE_SIZE * nr;\n\tst->index += nr / XEN_PFN_PER_PAGE;\n\n\treturn 0;\n}\n\nstatic int mmap_return_error(int err, struct mmap_batch_state *st)\n{\n\tint ret;\n\n\tif (st->version == 1) {\n\t\tif (err) {\n\t\t\txen_pfn_t gfn;\n\n\t\t\tret = get_user(gfn, st->user_gfn);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\t \n\t\t\tgfn |= (err == -ENOENT) ?\n\t\t\t\tPRIVCMD_MMAPBATCH_PAGED_ERROR :\n\t\t\t\tPRIVCMD_MMAPBATCH_MFN_ERROR;\n\t\t\treturn __put_user(gfn, st->user_gfn++);\n\t\t} else\n\t\t\tst->user_gfn++;\n\t} else {  \n\t\tif (err)\n\t\t\treturn __put_user(err, st->user_err++);\n\t\telse\n\t\t\tst->user_err++;\n\t}\n\n\treturn 0;\n}\n\nstatic int mmap_return_errors(void *data, int nr, void *state)\n{\n\tstruct mmap_batch_state *st = state;\n\tint *errs = data;\n\tint i;\n\tint ret;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tret = mmap_return_error(errs[i], st);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nstatic int alloc_empty_pages(struct vm_area_struct *vma, int numpgs)\n{\n\tint rc;\n\tstruct page **pages;\n\n\tpages = kvcalloc(numpgs, sizeof(pages[0]), GFP_KERNEL);\n\tif (pages == NULL)\n\t\treturn -ENOMEM;\n\n\trc = xen_alloc_unpopulated_pages(numpgs, pages);\n\tif (rc != 0) {\n\t\tpr_warn(\"%s Could not alloc %d pfns rc:%d\\n\", __func__,\n\t\t\tnumpgs, rc);\n\t\tkvfree(pages);\n\t\treturn -ENOMEM;\n\t}\n\tBUG_ON(vma->vm_private_data != NULL);\n\tvma->vm_private_data = pages;\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct privcmd_vm_ops;\n\nstatic long privcmd_ioctl_mmap_batch(\n\tstruct file *file, void __user *udata, int version)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tint ret;\n\tstruct privcmd_mmapbatch_v2 m;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_pages;\n\tLIST_HEAD(pagelist);\n\tstruct mmap_batch_state state;\n\n\tswitch (version) {\n\tcase 1:\n\t\tif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch)))\n\t\t\treturn -EFAULT;\n\t\t \n\t\tm.err = NULL;\n\t\tif (!access_ok(m.arr, m.num * sizeof(*m.arr)))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\tcase 2:\n\t\tif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch_v2)))\n\t\t\treturn -EFAULT;\n\t\t \n\t\tif (!access_ok(m.err, m.num * (sizeof(*m.err))))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (data->domid != DOMID_INVALID && data->domid != m.dom)\n\t\treturn -EPERM;\n\n\tnr_pages = DIV_ROUND_UP(m.num, XEN_PFN_PER_PAGE);\n\tif ((m.num <= 0) || (nr_pages > (LONG_MAX >> PAGE_SHIFT)))\n\t\treturn -EINVAL;\n\n\tret = gather_array(&pagelist, m.num, sizeof(xen_pfn_t), m.arr);\n\n\tif (ret)\n\t\tgoto out;\n\tif (list_empty(&pagelist)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (version == 2) {\n\t\t \n\t\tif (clear_user(m.err, sizeof(int) * m.num)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmmap_write_lock(mm);\n\n\tvma = find_vma(mm, m.addr);\n\tif (!vma ||\n\t    vma->vm_ops != &privcmd_vm_ops) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (vma->vm_private_data == NULL) {\n\t\tif (m.addr != vma->vm_start ||\n\t\t    m.addr + (nr_pages << PAGE_SHIFT) != vma->vm_end) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (xen_feature(XENFEAT_auto_translated_physmap)) {\n\t\t\tret = alloc_empty_pages(vma, nr_pages);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_unlock;\n\t\t} else\n\t\t\tvma->vm_private_data = PRIV_VMA_LOCKED;\n\t} else {\n\t\tif (m.addr < vma->vm_start ||\n\t\t    m.addr + (nr_pages << PAGE_SHIFT) > vma->vm_end) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (privcmd_vma_range_is_mapped(vma, m.addr, nr_pages)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tstate.domain        = m.dom;\n\tstate.vma           = vma;\n\tstate.va            = m.addr;\n\tstate.index         = 0;\n\tstate.global_error  = 0;\n\tstate.version       = version;\n\n\tBUILD_BUG_ON(((PAGE_SIZE / sizeof(xen_pfn_t)) % XEN_PFN_PER_PAGE) != 0);\n\t \n\tBUG_ON(traverse_pages_block(m.num, sizeof(xen_pfn_t),\n\t\t\t\t    &pagelist, mmap_batch_fn, &state));\n\n\tmmap_write_unlock(mm);\n\n\tif (state.global_error) {\n\t\t \n\t\tstate.user_gfn = (xen_pfn_t *)m.arr;\n\t\tstate.user_err = m.err;\n\t\tret = traverse_pages_block(m.num, sizeof(xen_pfn_t),\n\t\t\t\t\t   &pagelist, mmap_return_errors, &state);\n\t} else\n\t\tret = 0;\n\n\t \n\tif ((ret == 0) && (state.global_error == -ENOENT))\n\t\tret = -ENOENT;\n\nout:\n\tfree_page_list(&pagelist);\n\treturn ret;\n\nout_unlock:\n\tmmap_write_unlock(mm);\n\tgoto out;\n}\n\nstatic int lock_pages(\n\tstruct privcmd_dm_op_buf kbufs[], unsigned int num,\n\tstruct page *pages[], unsigned int nr_pages, unsigned int *pinned)\n{\n\tunsigned int i, off = 0;\n\n\tfor (i = 0; i < num; ) {\n\t\tunsigned int requested;\n\t\tint page_count;\n\n\t\trequested = DIV_ROUND_UP(\n\t\t\toffset_in_page(kbufs[i].uptr) + kbufs[i].size,\n\t\t\tPAGE_SIZE) - off;\n\t\tif (requested > nr_pages)\n\t\t\treturn -ENOSPC;\n\n\t\tpage_count = pin_user_pages_fast(\n\t\t\t(unsigned long)kbufs[i].uptr + off * PAGE_SIZE,\n\t\t\trequested, FOLL_WRITE, pages);\n\t\tif (page_count <= 0)\n\t\t\treturn page_count ? : -EFAULT;\n\n\t\t*pinned += page_count;\n\t\tnr_pages -= page_count;\n\t\tpages += page_count;\n\n\t\toff = (requested == page_count) ? 0 : off + page_count;\n\t\ti += !off;\n\t}\n\n\treturn 0;\n}\n\nstatic void unlock_pages(struct page *pages[], unsigned int nr_pages)\n{\n\tunpin_user_pages_dirty_lock(pages, nr_pages, true);\n}\n\nstatic long privcmd_ioctl_dm_op(struct file *file, void __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tstruct privcmd_dm_op kdata;\n\tstruct privcmd_dm_op_buf *kbufs;\n\tunsigned int nr_pages = 0;\n\tstruct page **pages = NULL;\n\tstruct xen_dm_op_buf *xbufs = NULL;\n\tunsigned int i;\n\tlong rc;\n\tunsigned int pinned = 0;\n\n\tif (copy_from_user(&kdata, udata, sizeof(kdata)))\n\t\treturn -EFAULT;\n\n\t \n\tif (data->domid != DOMID_INVALID && data->domid != kdata.dom)\n\t\treturn -EPERM;\n\n\tif (kdata.num == 0)\n\t\treturn 0;\n\n\tif (kdata.num > privcmd_dm_op_max_num)\n\t\treturn -E2BIG;\n\n\tkbufs = kcalloc(kdata.num, sizeof(*kbufs), GFP_KERNEL);\n\tif (!kbufs)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(kbufs, kdata.ubufs,\n\t\t\t   sizeof(*kbufs) * kdata.num)) {\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < kdata.num; i++) {\n\t\tif (kbufs[i].size > privcmd_dm_op_buf_max_size) {\n\t\t\trc = -E2BIG;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!access_ok(kbufs[i].uptr,\n\t\t\t       kbufs[i].size)) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnr_pages += DIV_ROUND_UP(\n\t\t\toffset_in_page(kbufs[i].uptr) + kbufs[i].size,\n\t\t\tPAGE_SIZE);\n\t}\n\n\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\txbufs = kcalloc(kdata.num, sizeof(*xbufs), GFP_KERNEL);\n\tif (!xbufs) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = lock_pages(kbufs, kdata.num, pages, nr_pages, &pinned);\n\tif (rc < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < kdata.num; i++) {\n\t\tset_xen_guest_handle(xbufs[i].h, kbufs[i].uptr);\n\t\txbufs[i].size = kbufs[i].size;\n\t}\n\n\txen_preemptible_hcall_begin();\n\trc = HYPERVISOR_dm_op(kdata.dom, kdata.num, xbufs);\n\txen_preemptible_hcall_end();\n\nout:\n\tunlock_pages(pages, pinned);\n\tkfree(xbufs);\n\tkfree(pages);\n\tkfree(kbufs);\n\n\treturn rc;\n}\n\nstatic long privcmd_ioctl_restrict(struct file *file, void __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tdomid_t dom;\n\n\tif (copy_from_user(&dom, udata, sizeof(dom)))\n\t\treturn -EFAULT;\n\n\t \n\tif (data->domid == DOMID_INVALID)\n\t\tdata->domid = dom;\n\telse if (data->domid != dom)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic long privcmd_ioctl_mmap_resource(struct file *file,\n\t\t\t\tstruct privcmd_mmap_resource __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct privcmd_mmap_resource kdata;\n\txen_pfn_t *pfns = NULL;\n\tstruct xen_mem_acquire_resource xdata = { };\n\tint rc;\n\n\tif (copy_from_user(&kdata, udata, sizeof(kdata)))\n\t\treturn -EFAULT;\n\n\t \n\tif (data->domid != DOMID_INVALID && data->domid != kdata.dom)\n\t\treturn -EPERM;\n\n\t \n\tif (!!kdata.addr != !!kdata.num)\n\t\treturn -EINVAL;\n\n\txdata.domid = kdata.dom;\n\txdata.type = kdata.type;\n\txdata.id = kdata.id;\n\n\tif (!kdata.addr && !kdata.num) {\n\t\t \n\t\trc = HYPERVISOR_memory_op(XENMEM_acquire_resource, &xdata);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\treturn __put_user(xdata.nr_frames, &udata->num);\n\t}\n\n\tmmap_write_lock(mm);\n\n\tvma = find_vma(mm, kdata.addr);\n\tif (!vma || vma->vm_ops != &privcmd_vm_ops) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tpfns = kcalloc(kdata.num, sizeof(*pfns), GFP_KERNEL | __GFP_NOWARN);\n\tif (!pfns) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (IS_ENABLED(CONFIG_XEN_AUTO_XLATE) &&\n\t    xen_feature(XENFEAT_auto_translated_physmap)) {\n\t\tunsigned int nr = DIV_ROUND_UP(kdata.num, XEN_PFN_PER_PAGE);\n\t\tstruct page **pages;\n\t\tunsigned int i;\n\n\t\trc = alloc_empty_pages(vma, nr);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\n\t\tpages = vma->vm_private_data;\n\t\tfor (i = 0; i < kdata.num; i++) {\n\t\t\txen_pfn_t pfn =\n\t\t\t\tpage_to_xen_pfn(pages[i / XEN_PFN_PER_PAGE]);\n\n\t\t\tpfns[i] = pfn + (i % XEN_PFN_PER_PAGE);\n\t\t}\n\t} else\n\t\tvma->vm_private_data = PRIV_VMA_LOCKED;\n\n\txdata.frame = kdata.idx;\n\txdata.nr_frames = kdata.num;\n\tset_xen_guest_handle(xdata.frame_list, pfns);\n\n\txen_preemptible_hcall_begin();\n\trc = HYPERVISOR_memory_op(XENMEM_acquire_resource, &xdata);\n\txen_preemptible_hcall_end();\n\n\tif (rc)\n\t\tgoto out;\n\n\tif (IS_ENABLED(CONFIG_XEN_AUTO_XLATE) &&\n\t    xen_feature(XENFEAT_auto_translated_physmap)) {\n\t\trc = xen_remap_vma_range(vma, kdata.addr, kdata.num << PAGE_SHIFT);\n\t} else {\n\t\tunsigned int domid =\n\t\t\t(xdata.flags & XENMEM_rsrc_acq_caller_owned) ?\n\t\t\tDOMID_SELF : kdata.dom;\n\t\tint num, *errs = (int *)pfns;\n\n\t\tBUILD_BUG_ON(sizeof(*errs) > sizeof(*pfns));\n\t\tnum = xen_remap_domain_mfn_array(vma,\n\t\t\t\t\t\t kdata.addr & PAGE_MASK,\n\t\t\t\t\t\t pfns, kdata.num, errs,\n\t\t\t\t\t\t vma->vm_page_prot,\n\t\t\t\t\t\t domid);\n\t\tif (num < 0)\n\t\t\trc = num;\n\t\telse if (num != kdata.num) {\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < num; i++) {\n\t\t\t\trc = errs[i];\n\t\t\t\tif (rc < 0)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} else\n\t\t\trc = 0;\n\t}\n\nout:\n\tmmap_write_unlock(mm);\n\tkfree(pfns);\n\n\treturn rc;\n}\n\n#ifdef CONFIG_XEN_PRIVCMD_IRQFD\n \nstatic struct workqueue_struct *irqfd_cleanup_wq;\nstatic DEFINE_MUTEX(irqfds_lock);\nstatic LIST_HEAD(irqfds_list);\n\nstruct privcmd_kernel_irqfd {\n\tstruct xen_dm_op_buf xbufs;\n\tdomid_t dom;\n\tbool error;\n\tstruct eventfd_ctx *eventfd;\n\tstruct work_struct shutdown;\n\twait_queue_entry_t wait;\n\tstruct list_head list;\n\tpoll_table pt;\n};\n\nstatic void irqfd_deactivate(struct privcmd_kernel_irqfd *kirqfd)\n{\n\tlockdep_assert_held(&irqfds_lock);\n\n\tlist_del_init(&kirqfd->list);\n\tqueue_work(irqfd_cleanup_wq, &kirqfd->shutdown);\n}\n\nstatic void irqfd_shutdown(struct work_struct *work)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd =\n\t\tcontainer_of(work, struct privcmd_kernel_irqfd, shutdown);\n\tu64 cnt;\n\n\teventfd_ctx_remove_wait_queue(kirqfd->eventfd, &kirqfd->wait, &cnt);\n\teventfd_ctx_put(kirqfd->eventfd);\n\tkfree(kirqfd);\n}\n\nstatic void irqfd_inject(struct privcmd_kernel_irqfd *kirqfd)\n{\n\tu64 cnt;\n\tlong rc;\n\n\teventfd_ctx_do_read(kirqfd->eventfd, &cnt);\n\n\txen_preemptible_hcall_begin();\n\trc = HYPERVISOR_dm_op(kirqfd->dom, 1, &kirqfd->xbufs);\n\txen_preemptible_hcall_end();\n\n\t \n\tif (rc && !kirqfd->error) {\n\t\tpr_err(\"Failed to configure irq for guest domain: %d\\n\",\n\t\t       kirqfd->dom);\n\t}\n\n\tkirqfd->error = rc;\n}\n\nstatic int\nirqfd_wakeup(wait_queue_entry_t *wait, unsigned int mode, int sync, void *key)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd =\n\t\tcontainer_of(wait, struct privcmd_kernel_irqfd, wait);\n\t__poll_t flags = key_to_poll(key);\n\n\tif (flags & EPOLLIN)\n\t\tirqfd_inject(kirqfd);\n\n\tif (flags & EPOLLHUP) {\n\t\tmutex_lock(&irqfds_lock);\n\t\tirqfd_deactivate(kirqfd);\n\t\tmutex_unlock(&irqfds_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nirqfd_poll_func(struct file *file, wait_queue_head_t *wqh, poll_table *pt)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd =\n\t\tcontainer_of(pt, struct privcmd_kernel_irqfd, pt);\n\n\tadd_wait_queue_priority(wqh, &kirqfd->wait);\n}\n\nstatic int privcmd_irqfd_assign(struct privcmd_irqfd *irqfd)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd, *tmp;\n\t__poll_t events;\n\tstruct fd f;\n\tvoid *dm_op;\n\tint ret;\n\n\tkirqfd = kzalloc(sizeof(*kirqfd) + irqfd->size, GFP_KERNEL);\n\tif (!kirqfd)\n\t\treturn -ENOMEM;\n\tdm_op = kirqfd + 1;\n\n\tif (copy_from_user(dm_op, u64_to_user_ptr(irqfd->dm_op), irqfd->size)) {\n\t\tret = -EFAULT;\n\t\tgoto error_kfree;\n\t}\n\n\tkirqfd->xbufs.size = irqfd->size;\n\tset_xen_guest_handle(kirqfd->xbufs.h, dm_op);\n\tkirqfd->dom = irqfd->dom;\n\tINIT_WORK(&kirqfd->shutdown, irqfd_shutdown);\n\n\tf = fdget(irqfd->fd);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto error_kfree;\n\t}\n\n\tkirqfd->eventfd = eventfd_ctx_fileget(f.file);\n\tif (IS_ERR(kirqfd->eventfd)) {\n\t\tret = PTR_ERR(kirqfd->eventfd);\n\t\tgoto error_fd_put;\n\t}\n\n\t \n\tinit_waitqueue_func_entry(&kirqfd->wait, irqfd_wakeup);\n\tinit_poll_funcptr(&kirqfd->pt, irqfd_poll_func);\n\n\tmutex_lock(&irqfds_lock);\n\n\tlist_for_each_entry(tmp, &irqfds_list, list) {\n\t\tif (kirqfd->eventfd == tmp->eventfd) {\n\t\t\tret = -EBUSY;\n\t\t\tmutex_unlock(&irqfds_lock);\n\t\t\tgoto error_eventfd;\n\t\t}\n\t}\n\n\tlist_add_tail(&kirqfd->list, &irqfds_list);\n\tmutex_unlock(&irqfds_lock);\n\n\t \n\tevents = vfs_poll(f.file, &kirqfd->pt);\n\tif (events & EPOLLIN)\n\t\tirqfd_inject(kirqfd);\n\n\t \n\tfdput(f);\n\treturn 0;\n\nerror_eventfd:\n\teventfd_ctx_put(kirqfd->eventfd);\n\nerror_fd_put:\n\tfdput(f);\n\nerror_kfree:\n\tkfree(kirqfd);\n\treturn ret;\n}\n\nstatic int privcmd_irqfd_deassign(struct privcmd_irqfd *irqfd)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd;\n\tstruct eventfd_ctx *eventfd;\n\n\teventfd = eventfd_ctx_fdget(irqfd->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tmutex_lock(&irqfds_lock);\n\n\tlist_for_each_entry(kirqfd, &irqfds_list, list) {\n\t\tif (kirqfd->eventfd == eventfd) {\n\t\t\tirqfd_deactivate(kirqfd);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmutex_unlock(&irqfds_lock);\n\n\teventfd_ctx_put(eventfd);\n\n\t \n\tflush_workqueue(irqfd_cleanup_wq);\n\n\treturn 0;\n}\n\nstatic long privcmd_ioctl_irqfd(struct file *file, void __user *udata)\n{\n\tstruct privcmd_data *data = file->private_data;\n\tstruct privcmd_irqfd irqfd;\n\n\tif (copy_from_user(&irqfd, udata, sizeof(irqfd)))\n\t\treturn -EFAULT;\n\n\t \n\tif (irqfd.flags & ~PRIVCMD_IRQFD_FLAG_DEASSIGN)\n\t\treturn -EINVAL;\n\n\t \n\tif (data->domid != DOMID_INVALID && data->domid != irqfd.dom)\n\t\treturn -EPERM;\n\n\tif (irqfd.flags & PRIVCMD_IRQFD_FLAG_DEASSIGN)\n\t\treturn privcmd_irqfd_deassign(&irqfd);\n\n\treturn privcmd_irqfd_assign(&irqfd);\n}\n\nstatic int privcmd_irqfd_init(void)\n{\n\tirqfd_cleanup_wq = alloc_workqueue(\"privcmd-irqfd-cleanup\", 0, 0);\n\tif (!irqfd_cleanup_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void privcmd_irqfd_exit(void)\n{\n\tstruct privcmd_kernel_irqfd *kirqfd, *tmp;\n\n\tmutex_lock(&irqfds_lock);\n\n\tlist_for_each_entry_safe(kirqfd, tmp, &irqfds_list, list)\n\t\tirqfd_deactivate(kirqfd);\n\n\tmutex_unlock(&irqfds_lock);\n\n\tdestroy_workqueue(irqfd_cleanup_wq);\n}\n#else\nstatic inline long privcmd_ioctl_irqfd(struct file *file, void __user *udata)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int privcmd_irqfd_init(void)\n{\n\treturn 0;\n}\n\nstatic inline void privcmd_irqfd_exit(void)\n{\n}\n#endif  \n\nstatic long privcmd_ioctl(struct file *file,\n\t\t\t  unsigned int cmd, unsigned long data)\n{\n\tint ret = -ENOTTY;\n\tvoid __user *udata = (void __user *) data;\n\n\tswitch (cmd) {\n\tcase IOCTL_PRIVCMD_HYPERCALL:\n\t\tret = privcmd_ioctl_hypercall(file, udata);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_MMAP:\n\t\tret = privcmd_ioctl_mmap(file, udata);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_MMAPBATCH:\n\t\tret = privcmd_ioctl_mmap_batch(file, udata, 1);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_MMAPBATCH_V2:\n\t\tret = privcmd_ioctl_mmap_batch(file, udata, 2);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_DM_OP:\n\t\tret = privcmd_ioctl_dm_op(file, udata);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_RESTRICT:\n\t\tret = privcmd_ioctl_restrict(file, udata);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_MMAP_RESOURCE:\n\t\tret = privcmd_ioctl_mmap_resource(file, udata);\n\t\tbreak;\n\n\tcase IOCTL_PRIVCMD_IRQFD:\n\t\tret = privcmd_ioctl_irqfd(file, udata);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int privcmd_open(struct inode *ino, struct file *file)\n{\n\tstruct privcmd_data *data = kzalloc(sizeof(*data), GFP_KERNEL);\n\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\t \n\tdata->domid = DOMID_INVALID;\n\n\tfile->private_data = data;\n\treturn 0;\n}\n\nstatic int privcmd_release(struct inode *ino, struct file *file)\n{\n\tstruct privcmd_data *data = file->private_data;\n\n\tkfree(data);\n\treturn 0;\n}\n\nstatic void privcmd_close(struct vm_area_struct *vma)\n{\n\tstruct page **pages = vma->vm_private_data;\n\tint numpgs = vma_pages(vma);\n\tint numgfns = (vma->vm_end - vma->vm_start) >> XEN_PAGE_SHIFT;\n\tint rc;\n\n\tif (!xen_feature(XENFEAT_auto_translated_physmap) || !numpgs || !pages)\n\t\treturn;\n\n\trc = xen_unmap_domain_gfn_range(vma, numgfns, pages);\n\tif (rc == 0)\n\t\txen_free_unpopulated_pages(numpgs, pages);\n\telse\n\t\tpr_crit(\"unable to unmap MFN range: leaking %d pages. rc=%d\\n\",\n\t\t\tnumpgs, rc);\n\tkvfree(pages);\n}\n\nstatic vm_fault_t privcmd_fault(struct vm_fault *vmf)\n{\n\tprintk(KERN_DEBUG \"privcmd_fault: vma=%p %lx-%lx, pgoff=%lx, uv=%p\\n\",\n\t       vmf->vma, vmf->vma->vm_start, vmf->vma->vm_end,\n\t       vmf->pgoff, (void *)vmf->address);\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct privcmd_vm_ops = {\n\t.close = privcmd_close,\n\t.fault = privcmd_fault\n};\n\nstatic int privcmd_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\t \n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTCOPY |\n\t\t\t VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_ops = &privcmd_vm_ops;\n\tvma->vm_private_data = NULL;\n\n\treturn 0;\n}\n\n \nstatic int is_mapped_fn(pte_t *pte, unsigned long addr, void *data)\n{\n\treturn pte_none(ptep_get(pte)) ? 0 : -EBUSY;\n}\n\nstatic int privcmd_vma_range_is_mapped(\n\t           struct vm_area_struct *vma,\n\t           unsigned long addr,\n\t           unsigned long nr_pages)\n{\n\treturn apply_to_page_range(vma->vm_mm, addr, nr_pages << PAGE_SHIFT,\n\t\t\t\t   is_mapped_fn, NULL) != 0;\n}\n\nconst struct file_operations xen_privcmd_fops = {\n\t.owner = THIS_MODULE,\n\t.unlocked_ioctl = privcmd_ioctl,\n\t.open = privcmd_open,\n\t.release = privcmd_release,\n\t.mmap = privcmd_mmap,\n};\nEXPORT_SYMBOL_GPL(xen_privcmd_fops);\n\nstatic struct miscdevice privcmd_dev = {\n\t.minor = MISC_DYNAMIC_MINOR,\n\t.name = \"xen/privcmd\",\n\t.fops = &xen_privcmd_fops,\n};\n\nstatic int __init privcmd_init(void)\n{\n\tint err;\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\terr = misc_register(&privcmd_dev);\n\tif (err != 0) {\n\t\tpr_err(\"Could not register Xen privcmd device\\n\");\n\t\treturn err;\n\t}\n\n\terr = misc_register(&xen_privcmdbuf_dev);\n\tif (err != 0) {\n\t\tpr_err(\"Could not register Xen hypercall-buf device\\n\");\n\t\tgoto err_privcmdbuf;\n\t}\n\n\terr = privcmd_irqfd_init();\n\tif (err != 0) {\n\t\tpr_err(\"irqfd init failed\\n\");\n\t\tgoto err_irqfd;\n\t}\n\n\treturn 0;\n\nerr_irqfd:\n\tmisc_deregister(&xen_privcmdbuf_dev);\nerr_privcmdbuf:\n\tmisc_deregister(&privcmd_dev);\n\treturn err;\n}\n\nstatic void __exit privcmd_exit(void)\n{\n\tprivcmd_irqfd_exit();\n\tmisc_deregister(&privcmd_dev);\n\tmisc_deregister(&xen_privcmdbuf_dev);\n}\n\nmodule_init(privcmd_init);\nmodule_exit(privcmd_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}