{
  "module_name": "grant-table.c",
  "hash_id": "67bada7983ea3ab89e0b980e15c6a16b5e12baed4957face9b85a0395e300815",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/grant-table.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) \"xen:\" KBUILD_MODNAME \": \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/memblock.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n#include <linux/io.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/workqueue.h>\n#include <linux/ratelimit.h>\n#include <linux/moduleparam.h>\n#ifdef CONFIG_XEN_GRANT_DMA_ALLOC\n#include <linux/dma-mapping.h>\n#endif\n\n#include <xen/xen.h>\n#include <xen/interface/xen.h>\n#include <xen/page.h>\n#include <xen/grant_table.h>\n#include <xen/interface/memory.h>\n#include <xen/hvc-console.h>\n#include <xen/swiotlb-xen.h>\n#include <xen/balloon.h>\n#ifdef CONFIG_X86\n#include <asm/xen/cpuid.h>\n#endif\n#include <xen/mem-reservation.h>\n#include <asm/xen/hypercall.h>\n#include <asm/xen/interface.h>\n\n#include <asm/sync_bitops.h>\n\n#define GNTTAB_LIST_END 0xffffffff\n\nstatic grant_ref_t **gnttab_list;\nstatic unsigned int nr_grant_frames;\n\n \nstatic int gnttab_free_count;\nstatic unsigned int gnttab_size;\nstatic grant_ref_t gnttab_free_head = GNTTAB_LIST_END;\nstatic grant_ref_t gnttab_last_free = GNTTAB_LIST_END;\nstatic grant_ref_t *gnttab_free_tail_ptr;\nstatic unsigned long *gnttab_free_bitmap;\nstatic DEFINE_SPINLOCK(gnttab_list_lock);\n\nstruct grant_frames xen_auto_xlat_grant_frames;\nstatic unsigned int xen_gnttab_version;\nmodule_param_named(version, xen_gnttab_version, uint, 0);\n\nstatic union {\n\tstruct grant_entry_v1 *v1;\n\tunion grant_entry_v2 *v2;\n\tvoid *addr;\n} gnttab_shared;\n\n \nstruct gnttab_ops {\n\t \n\tunsigned int version;\n\t \n\tunsigned int grefs_per_grant_frame;\n\t \n\tint (*map_frames)(xen_pfn_t *frames, unsigned int nr_gframes);\n\t \n\tvoid (*unmap_frames)(void);\n\t \n\tvoid (*update_entry)(grant_ref_t ref, domid_t domid,\n\t\t\t     unsigned long frame, unsigned flags);\n\t \n\tint (*end_foreign_access_ref)(grant_ref_t ref);\n\t \n\tunsigned long (*read_frame)(grant_ref_t ref);\n};\n\nstruct unmap_refs_callback_data {\n\tstruct completion completion;\n\tint result;\n};\n\nstatic const struct gnttab_ops *gnttab_interface;\n\n \nstatic grant_status_t *grstatus;\n\nstatic struct gnttab_free_callback *gnttab_free_callback_list;\n\nstatic int gnttab_expand(unsigned int req_entries);\n\n#define RPP (PAGE_SIZE / sizeof(grant_ref_t))\n#define SPP (PAGE_SIZE / sizeof(grant_status_t))\n\nstatic inline grant_ref_t *__gnttab_entry(grant_ref_t entry)\n{\n\treturn &gnttab_list[(entry) / RPP][(entry) % RPP];\n}\n \n#define gnttab_entry(entry) (*__gnttab_entry(entry))\n\nstatic int get_free_entries(unsigned count)\n{\n\tunsigned long flags;\n\tint ref, rc = 0;\n\tgrant_ref_t head;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\n\tif ((gnttab_free_count < count) &&\n\t    ((rc = gnttab_expand(count - gnttab_free_count)) < 0)) {\n\t\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\t\treturn rc;\n\t}\n\n\tref = head = gnttab_free_head;\n\tgnttab_free_count -= count;\n\twhile (count--) {\n\t\tbitmap_clear(gnttab_free_bitmap, head, 1);\n\t\tif (gnttab_free_tail_ptr == __gnttab_entry(head))\n\t\t\tgnttab_free_tail_ptr = &gnttab_free_head;\n\t\tif (count)\n\t\t\thead = gnttab_entry(head);\n\t}\n\tgnttab_free_head = gnttab_entry(head);\n\tgnttab_entry(head) = GNTTAB_LIST_END;\n\n\tif (!gnttab_free_count) {\n\t\tgnttab_last_free = GNTTAB_LIST_END;\n\t\tgnttab_free_tail_ptr = NULL;\n\t}\n\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\n\treturn ref;\n}\n\nstatic int get_seq_entry_count(void)\n{\n\tif (gnttab_last_free == GNTTAB_LIST_END || !gnttab_free_tail_ptr ||\n\t    *gnttab_free_tail_ptr == GNTTAB_LIST_END)\n\t\treturn 0;\n\n\treturn gnttab_last_free - *gnttab_free_tail_ptr + 1;\n}\n\n \nstatic int get_free_seq(unsigned int count)\n{\n\tint ret = -ENOSPC;\n\tunsigned int from, to;\n\tgrant_ref_t *last;\n\n\tgnttab_free_tail_ptr = &gnttab_free_head;\n\tlast = &gnttab_free_head;\n\n\tfor (from = find_first_bit(gnttab_free_bitmap, gnttab_size);\n\t     from < gnttab_size;\n\t     from = find_next_bit(gnttab_free_bitmap, gnttab_size, to + 1)) {\n\t\tto = find_next_zero_bit(gnttab_free_bitmap, gnttab_size,\n\t\t\t\t\tfrom + 1);\n\t\tif (ret < 0 && to - from >= count) {\n\t\t\tret = from;\n\t\t\tbitmap_clear(gnttab_free_bitmap, ret, count);\n\t\t\tfrom += count;\n\t\t\tgnttab_free_count -= count;\n\t\t\tif (from == to)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\twhile (from < to) {\n\t\t\t*last = from;\n\t\t\tlast = __gnttab_entry(from);\n\t\t\tgnttab_last_free = from;\n\t\t\tfrom++;\n\t\t}\n\t\tif (to < gnttab_size)\n\t\t\tgnttab_free_tail_ptr = __gnttab_entry(to - 1);\n\t}\n\n\t*last = GNTTAB_LIST_END;\n\tif (gnttab_last_free != gnttab_size - 1)\n\t\tgnttab_free_tail_ptr = NULL;\n\n\treturn ret;\n}\n\nstatic int get_free_entries_seq(unsigned int count)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\n\tif (gnttab_free_count < count) {\n\t\tret = gnttab_expand(count - gnttab_free_count);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tif (get_seq_entry_count() < count) {\n\t\tret = get_free_seq(count);\n\t\tif (ret >= 0)\n\t\t\tgoto out;\n\t\tret = gnttab_expand(count - get_seq_entry_count());\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = *gnttab_free_tail_ptr;\n\t*gnttab_free_tail_ptr = gnttab_entry(ret + count - 1);\n\tgnttab_free_count -= count;\n\tif (!gnttab_free_count)\n\t\tgnttab_free_tail_ptr = NULL;\n\tbitmap_clear(gnttab_free_bitmap, ret, count);\n\n out:\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\n\treturn ret;\n}\n\nstatic void do_free_callbacks(void)\n{\n\tstruct gnttab_free_callback *callback, *next;\n\n\tcallback = gnttab_free_callback_list;\n\tgnttab_free_callback_list = NULL;\n\n\twhile (callback != NULL) {\n\t\tnext = callback->next;\n\t\tif (gnttab_free_count >= callback->count) {\n\t\t\tcallback->next = NULL;\n\t\t\tcallback->fn(callback->arg);\n\t\t} else {\n\t\t\tcallback->next = gnttab_free_callback_list;\n\t\t\tgnttab_free_callback_list = callback;\n\t\t}\n\t\tcallback = next;\n\t}\n}\n\nstatic inline void check_free_callbacks(void)\n{\n\tif (unlikely(gnttab_free_callback_list))\n\t\tdo_free_callbacks();\n}\n\nstatic void put_free_entry_locked(grant_ref_t ref)\n{\n\tif (unlikely(ref < GNTTAB_NR_RESERVED_ENTRIES))\n\t\treturn;\n\n\tgnttab_entry(ref) = gnttab_free_head;\n\tgnttab_free_head = ref;\n\tif (!gnttab_free_count)\n\t\tgnttab_last_free = ref;\n\tif (gnttab_free_tail_ptr == &gnttab_free_head)\n\t\tgnttab_free_tail_ptr = __gnttab_entry(ref);\n\tgnttab_free_count++;\n\tbitmap_set(gnttab_free_bitmap, ref, 1);\n}\n\nstatic void put_free_entry(grant_ref_t ref)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\tput_free_entry_locked(ref);\n\tcheck_free_callbacks();\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n}\n\nstatic void gnttab_set_free(unsigned int start, unsigned int n)\n{\n\tunsigned int i;\n\n\tfor (i = start; i < start + n - 1; i++)\n\t\tgnttab_entry(i) = i + 1;\n\n\tgnttab_entry(i) = GNTTAB_LIST_END;\n\tif (!gnttab_free_count) {\n\t\tgnttab_free_head = start;\n\t\tgnttab_free_tail_ptr = &gnttab_free_head;\n\t} else {\n\t\tgnttab_entry(gnttab_last_free) = start;\n\t}\n\tgnttab_free_count += n;\n\tgnttab_last_free = i;\n\n\tbitmap_set(gnttab_free_bitmap, start, n);\n}\n\n \nstatic void gnttab_update_entry_v1(grant_ref_t ref, domid_t domid,\n\t\t\t\t   unsigned long frame, unsigned flags)\n{\n\tgnttab_shared.v1[ref].domid = domid;\n\tgnttab_shared.v1[ref].frame = frame;\n\twmb();\n\tgnttab_shared.v1[ref].flags = flags;\n}\n\nstatic void gnttab_update_entry_v2(grant_ref_t ref, domid_t domid,\n\t\t\t\t   unsigned long frame, unsigned int flags)\n{\n\tgnttab_shared.v2[ref].hdr.domid = domid;\n\tgnttab_shared.v2[ref].full_page.frame = frame;\n\twmb();\t \n\tgnttab_shared.v2[ref].hdr.flags = GTF_permit_access | flags;\n}\n\n \nvoid gnttab_grant_foreign_access_ref(grant_ref_t ref, domid_t domid,\n\t\t\t\t     unsigned long frame, int readonly)\n{\n\tgnttab_interface->update_entry(ref, domid, frame,\n\t\t\t   GTF_permit_access | (readonly ? GTF_readonly : 0));\n}\nEXPORT_SYMBOL_GPL(gnttab_grant_foreign_access_ref);\n\nint gnttab_grant_foreign_access(domid_t domid, unsigned long frame,\n\t\t\t\tint readonly)\n{\n\tint ref;\n\n\tref = get_free_entries(1);\n\tif (unlikely(ref < 0))\n\t\treturn -ENOSPC;\n\n\tgnttab_grant_foreign_access_ref(ref, domid, frame, readonly);\n\n\treturn ref;\n}\nEXPORT_SYMBOL_GPL(gnttab_grant_foreign_access);\n\nstatic int gnttab_end_foreign_access_ref_v1(grant_ref_t ref)\n{\n\tu16 flags, nflags;\n\tu16 *pflags;\n\n\tpflags = &gnttab_shared.v1[ref].flags;\n\tnflags = *pflags;\n\tdo {\n\t\tflags = nflags;\n\t\tif (flags & (GTF_reading|GTF_writing))\n\t\t\treturn 0;\n\t} while ((nflags = sync_cmpxchg(pflags, flags, 0)) != flags);\n\n\treturn 1;\n}\n\nstatic int gnttab_end_foreign_access_ref_v2(grant_ref_t ref)\n{\n\tgnttab_shared.v2[ref].hdr.flags = 0;\n\tmb();\t \n\tif (grstatus[ref] & (GTF_reading|GTF_writing)) {\n\t\treturn 0;\n\t} else {\n\t\t \n#ifdef CONFIG_X86\n\t\tbarrier();\n#else\n\t\tmb();\n#endif\n\t}\n\n\treturn 1;\n}\n\nstatic inline int _gnttab_end_foreign_access_ref(grant_ref_t ref)\n{\n\treturn gnttab_interface->end_foreign_access_ref(ref);\n}\n\nint gnttab_end_foreign_access_ref(grant_ref_t ref)\n{\n\tif (_gnttab_end_foreign_access_ref(ref))\n\t\treturn 1;\n\tpr_warn(\"WARNING: g.e. %#x still in use!\\n\", ref);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_end_foreign_access_ref);\n\nstatic unsigned long gnttab_read_frame_v1(grant_ref_t ref)\n{\n\treturn gnttab_shared.v1[ref].frame;\n}\n\nstatic unsigned long gnttab_read_frame_v2(grant_ref_t ref)\n{\n\treturn gnttab_shared.v2[ref].full_page.frame;\n}\n\nstruct deferred_entry {\n\tstruct list_head list;\n\tgrant_ref_t ref;\n\tuint16_t warn_delay;\n\tstruct page *page;\n};\nstatic LIST_HEAD(deferred_list);\nstatic void gnttab_handle_deferred(struct timer_list *);\nstatic DEFINE_TIMER(deferred_timer, gnttab_handle_deferred);\n\nstatic atomic64_t deferred_count;\nstatic atomic64_t leaked_count;\nstatic unsigned int free_per_iteration = 10;\nmodule_param(free_per_iteration, uint, 0600);\n\nstatic void gnttab_handle_deferred(struct timer_list *unused)\n{\n\tunsigned int nr = READ_ONCE(free_per_iteration);\n\tconst bool ignore_limit = nr == 0;\n\tstruct deferred_entry *first = NULL;\n\tunsigned long flags;\n\tsize_t freed = 0;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\twhile ((ignore_limit || nr--) && !list_empty(&deferred_list)) {\n\t\tstruct deferred_entry *entry\n\t\t\t= list_first_entry(&deferred_list,\n\t\t\t\t\t   struct deferred_entry, list);\n\n\t\tif (entry == first)\n\t\t\tbreak;\n\t\tlist_del(&entry->list);\n\t\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\t\tif (_gnttab_end_foreign_access_ref(entry->ref)) {\n\t\t\tuint64_t ret = atomic64_dec_return(&deferred_count);\n\n\t\t\tput_free_entry(entry->ref);\n\t\t\tpr_debug(\"freeing g.e. %#x (pfn %#lx), %llu remaining\\n\",\n\t\t\t\t entry->ref, page_to_pfn(entry->page),\n\t\t\t\t (unsigned long long)ret);\n\t\t\tput_page(entry->page);\n\t\t\tfreed++;\n\t\t\tkfree(entry);\n\t\t\tentry = NULL;\n\t\t} else {\n\t\t\tif (!--entry->warn_delay)\n\t\t\t\tpr_info(\"g.e. %#x still pending\\n\", entry->ref);\n\t\t\tif (!first)\n\t\t\t\tfirst = entry;\n\t\t}\n\t\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\t\tif (entry)\n\t\t\tlist_add_tail(&entry->list, &deferred_list);\n\t}\n\tif (list_empty(&deferred_list))\n\t\tWARN_ON(atomic64_read(&deferred_count));\n\telse if (!timer_pending(&deferred_timer)) {\n\t\tdeferred_timer.expires = jiffies + HZ;\n\t\tadd_timer(&deferred_timer);\n\t}\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\tpr_debug(\"Freed %zu references\", freed);\n}\n\nstatic void gnttab_add_deferred(grant_ref_t ref, struct page *page)\n{\n\tstruct deferred_entry *entry;\n\tgfp_t gfp = (in_atomic() || irqs_disabled()) ? GFP_ATOMIC : GFP_KERNEL;\n\tuint64_t leaked, deferred;\n\n\tentry = kmalloc(sizeof(*entry), gfp);\n\tif (!page) {\n\t\tunsigned long gfn = gnttab_interface->read_frame(ref);\n\n\t\tpage = pfn_to_page(gfn_to_pfn(gfn));\n\t\tget_page(page);\n\t}\n\n\tif (entry) {\n\t\tunsigned long flags;\n\n\t\tentry->ref = ref;\n\t\tentry->page = page;\n\t\tentry->warn_delay = 60;\n\t\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\t\tlist_add_tail(&entry->list, &deferred_list);\n\t\tif (!timer_pending(&deferred_timer)) {\n\t\t\tdeferred_timer.expires = jiffies + HZ;\n\t\t\tadd_timer(&deferred_timer);\n\t\t}\n\t\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n\t\tdeferred = atomic64_inc_return(&deferred_count);\n\t\tleaked = atomic64_read(&leaked_count);\n\t\tpr_debug(\"deferring g.e. %#x (pfn %#lx) (total deferred %llu, total leaked %llu)\\n\",\n\t\t\t ref, page ? page_to_pfn(page) : -1, deferred, leaked);\n\t} else {\n\t\tdeferred = atomic64_read(&deferred_count);\n\t\tleaked = atomic64_inc_return(&leaked_count);\n\t\tpr_warn(\"leaking g.e. %#x (pfn %#lx) (total deferred %llu, total leaked %llu)\\n\",\n\t\t\tref, page ? page_to_pfn(page) : -1, deferred, leaked);\n\t}\n}\n\nint gnttab_try_end_foreign_access(grant_ref_t ref)\n{\n\tint ret = _gnttab_end_foreign_access_ref(ref);\n\n\tif (ret)\n\t\tput_free_entry(ref);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gnttab_try_end_foreign_access);\n\nvoid gnttab_end_foreign_access(grant_ref_t ref, struct page *page)\n{\n\tif (gnttab_try_end_foreign_access(ref)) {\n\t\tif (page)\n\t\t\tput_page(page);\n\t} else\n\t\tgnttab_add_deferred(ref, page);\n}\nEXPORT_SYMBOL_GPL(gnttab_end_foreign_access);\n\nvoid gnttab_free_grant_reference(grant_ref_t ref)\n{\n\tput_free_entry(ref);\n}\nEXPORT_SYMBOL_GPL(gnttab_free_grant_reference);\n\nvoid gnttab_free_grant_references(grant_ref_t head)\n{\n\tgrant_ref_t ref;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\twhile (head != GNTTAB_LIST_END) {\n\t\tref = gnttab_entry(head);\n\t\tput_free_entry_locked(head);\n\t\thead = ref;\n\t}\n\tcheck_free_callbacks();\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n}\nEXPORT_SYMBOL_GPL(gnttab_free_grant_references);\n\nvoid gnttab_free_grant_reference_seq(grant_ref_t head, unsigned int count)\n{\n\tunsigned long flags;\n\tunsigned int i;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\tfor (i = count; i > 0; i--)\n\t\tput_free_entry_locked(head + i - 1);\n\tcheck_free_callbacks();\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n}\nEXPORT_SYMBOL_GPL(gnttab_free_grant_reference_seq);\n\nint gnttab_alloc_grant_references(u16 count, grant_ref_t *head)\n{\n\tint h = get_free_entries(count);\n\n\tif (h < 0)\n\t\treturn -ENOSPC;\n\n\t*head = h;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_alloc_grant_references);\n\nint gnttab_alloc_grant_reference_seq(unsigned int count, grant_ref_t *first)\n{\n\tint h;\n\n\tif (count == 1)\n\t\th = get_free_entries(1);\n\telse\n\t\th = get_free_entries_seq(count);\n\n\tif (h < 0)\n\t\treturn -ENOSPC;\n\n\t*first = h;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_alloc_grant_reference_seq);\n\nint gnttab_empty_grant_references(const grant_ref_t *private_head)\n{\n\treturn (*private_head == GNTTAB_LIST_END);\n}\nEXPORT_SYMBOL_GPL(gnttab_empty_grant_references);\n\nint gnttab_claim_grant_reference(grant_ref_t *private_head)\n{\n\tgrant_ref_t g = *private_head;\n\tif (unlikely(g == GNTTAB_LIST_END))\n\t\treturn -ENOSPC;\n\t*private_head = gnttab_entry(g);\n\treturn g;\n}\nEXPORT_SYMBOL_GPL(gnttab_claim_grant_reference);\n\nvoid gnttab_release_grant_reference(grant_ref_t *private_head,\n\t\t\t\t    grant_ref_t release)\n{\n\tgnttab_entry(release) = *private_head;\n\t*private_head = release;\n}\nEXPORT_SYMBOL_GPL(gnttab_release_grant_reference);\n\nvoid gnttab_request_free_callback(struct gnttab_free_callback *callback,\n\t\t\t\t  void (*fn)(void *), void *arg, u16 count)\n{\n\tunsigned long flags;\n\tstruct gnttab_free_callback *cb;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\n\t \n\tcb = gnttab_free_callback_list;\n\twhile (cb) {\n\t\tif (cb == callback)\n\t\t\tgoto out;\n\t\tcb = cb->next;\n\t}\n\n\tcallback->fn = fn;\n\tcallback->arg = arg;\n\tcallback->count = count;\n\tcallback->next = gnttab_free_callback_list;\n\tgnttab_free_callback_list = callback;\n\tcheck_free_callbacks();\nout:\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n}\nEXPORT_SYMBOL_GPL(gnttab_request_free_callback);\n\nvoid gnttab_cancel_free_callback(struct gnttab_free_callback *callback)\n{\n\tstruct gnttab_free_callback **pcb;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&gnttab_list_lock, flags);\n\tfor (pcb = &gnttab_free_callback_list; *pcb; pcb = &(*pcb)->next) {\n\t\tif (*pcb == callback) {\n\t\t\t*pcb = callback->next;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&gnttab_list_lock, flags);\n}\nEXPORT_SYMBOL_GPL(gnttab_cancel_free_callback);\n\nstatic unsigned int gnttab_frames(unsigned int frames, unsigned int align)\n{\n\treturn (frames * gnttab_interface->grefs_per_grant_frame + align - 1) /\n\t       align;\n}\n\nstatic int grow_gnttab_list(unsigned int more_frames)\n{\n\tunsigned int new_nr_grant_frames, extra_entries, i;\n\tunsigned int nr_glist_frames, new_nr_glist_frames;\n\tunsigned int grefs_per_frame;\n\n\tgrefs_per_frame = gnttab_interface->grefs_per_grant_frame;\n\n\tnew_nr_grant_frames = nr_grant_frames + more_frames;\n\textra_entries = more_frames * grefs_per_frame;\n\n\tnr_glist_frames = gnttab_frames(nr_grant_frames, RPP);\n\tnew_nr_glist_frames = gnttab_frames(new_nr_grant_frames, RPP);\n\tfor (i = nr_glist_frames; i < new_nr_glist_frames; i++) {\n\t\tgnttab_list[i] = (grant_ref_t *)__get_free_page(GFP_ATOMIC);\n\t\tif (!gnttab_list[i])\n\t\t\tgoto grow_nomem;\n\t}\n\n\tgnttab_set_free(gnttab_size, extra_entries);\n\n\tif (!gnttab_free_tail_ptr)\n\t\tgnttab_free_tail_ptr = __gnttab_entry(gnttab_size);\n\n\tnr_grant_frames = new_nr_grant_frames;\n\tgnttab_size += extra_entries;\n\n\tcheck_free_callbacks();\n\n\treturn 0;\n\ngrow_nomem:\n\twhile (i-- > nr_glist_frames)\n\t\tfree_page((unsigned long) gnttab_list[i]);\n\treturn -ENOMEM;\n}\n\nstatic unsigned int __max_nr_grant_frames(void)\n{\n\tstruct gnttab_query_size query;\n\tint rc;\n\n\tquery.dom = DOMID_SELF;\n\n\trc = HYPERVISOR_grant_table_op(GNTTABOP_query_size, &query, 1);\n\tif ((rc < 0) || (query.status != GNTST_okay))\n\t\treturn 4;  \n\n\treturn query.max_nr_frames;\n}\n\nunsigned int gnttab_max_grant_frames(void)\n{\n\tunsigned int xen_max = __max_nr_grant_frames();\n\tstatic unsigned int boot_max_nr_grant_frames;\n\n\t \n\tif (!boot_max_nr_grant_frames)\n\t\tboot_max_nr_grant_frames = __max_nr_grant_frames();\n\n\tif (xen_max > boot_max_nr_grant_frames)\n\t\treturn boot_max_nr_grant_frames;\n\treturn xen_max;\n}\nEXPORT_SYMBOL_GPL(gnttab_max_grant_frames);\n\nint gnttab_setup_auto_xlat_frames(phys_addr_t addr)\n{\n\txen_pfn_t *pfn;\n\tunsigned int max_nr_gframes = __max_nr_grant_frames();\n\tunsigned int i;\n\tvoid *vaddr;\n\n\tif (xen_auto_xlat_grant_frames.count)\n\t\treturn -EINVAL;\n\n\tvaddr = memremap(addr, XEN_PAGE_SIZE * max_nr_gframes, MEMREMAP_WB);\n\tif (vaddr == NULL) {\n\t\tpr_warn(\"Failed to ioremap gnttab share frames (addr=%pa)!\\n\",\n\t\t\t&addr);\n\t\treturn -ENOMEM;\n\t}\n\tpfn = kcalloc(max_nr_gframes, sizeof(pfn[0]), GFP_KERNEL);\n\tif (!pfn) {\n\t\tmemunmap(vaddr);\n\t\treturn -ENOMEM;\n\t}\n\tfor (i = 0; i < max_nr_gframes; i++)\n\t\tpfn[i] = XEN_PFN_DOWN(addr) + i;\n\n\txen_auto_xlat_grant_frames.vaddr = vaddr;\n\txen_auto_xlat_grant_frames.pfn = pfn;\n\txen_auto_xlat_grant_frames.count = max_nr_gframes;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_setup_auto_xlat_frames);\n\nvoid gnttab_free_auto_xlat_frames(void)\n{\n\tif (!xen_auto_xlat_grant_frames.count)\n\t\treturn;\n\tkfree(xen_auto_xlat_grant_frames.pfn);\n\tmemunmap(xen_auto_xlat_grant_frames.vaddr);\n\n\txen_auto_xlat_grant_frames.pfn = NULL;\n\txen_auto_xlat_grant_frames.count = 0;\n\txen_auto_xlat_grant_frames.vaddr = NULL;\n}\nEXPORT_SYMBOL_GPL(gnttab_free_auto_xlat_frames);\n\nint gnttab_pages_set_private(int nr_pages, struct page **pages)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n#if BITS_PER_LONG < 64\n\t\tstruct xen_page_foreign *foreign;\n\n\t\tforeign = kzalloc(sizeof(*foreign), GFP_KERNEL);\n\t\tif (!foreign)\n\t\t\treturn -ENOMEM;\n\n\t\tset_page_private(pages[i], (unsigned long)foreign);\n#endif\n\t\tSetPagePrivate(pages[i]);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_pages_set_private);\n\n \nint gnttab_alloc_pages(int nr_pages, struct page **pages)\n{\n\tint ret;\n\n\tret = xen_alloc_unpopulated_pages(nr_pages, pages);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = gnttab_pages_set_private(nr_pages, pages);\n\tif (ret < 0)\n\t\tgnttab_free_pages(nr_pages, pages);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gnttab_alloc_pages);\n\n#ifdef CONFIG_XEN_UNPOPULATED_ALLOC\nstatic inline void cache_init(struct gnttab_page_cache *cache)\n{\n\tcache->pages = NULL;\n}\n\nstatic inline bool cache_empty(struct gnttab_page_cache *cache)\n{\n\treturn !cache->pages;\n}\n\nstatic inline struct page *cache_deq(struct gnttab_page_cache *cache)\n{\n\tstruct page *page;\n\n\tpage = cache->pages;\n\tcache->pages = page->zone_device_data;\n\n\treturn page;\n}\n\nstatic inline void cache_enq(struct gnttab_page_cache *cache, struct page *page)\n{\n\tpage->zone_device_data = cache->pages;\n\tcache->pages = page;\n}\n#else\nstatic inline void cache_init(struct gnttab_page_cache *cache)\n{\n\tINIT_LIST_HEAD(&cache->pages);\n}\n\nstatic inline bool cache_empty(struct gnttab_page_cache *cache)\n{\n\treturn list_empty(&cache->pages);\n}\n\nstatic inline struct page *cache_deq(struct gnttab_page_cache *cache)\n{\n\tstruct page *page;\n\n\tpage = list_first_entry(&cache->pages, struct page, lru);\n\tlist_del(&page->lru);\n\n\treturn page;\n}\n\nstatic inline void cache_enq(struct gnttab_page_cache *cache, struct page *page)\n{\n\tlist_add(&page->lru, &cache->pages);\n}\n#endif\n\nvoid gnttab_page_cache_init(struct gnttab_page_cache *cache)\n{\n\tspin_lock_init(&cache->lock);\n\tcache_init(cache);\n\tcache->num_pages = 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_page_cache_init);\n\nint gnttab_page_cache_get(struct gnttab_page_cache *cache, struct page **page)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cache->lock, flags);\n\n\tif (cache_empty(cache)) {\n\t\tspin_unlock_irqrestore(&cache->lock, flags);\n\t\treturn gnttab_alloc_pages(1, page);\n\t}\n\n\tpage[0] = cache_deq(cache);\n\tcache->num_pages--;\n\n\tspin_unlock_irqrestore(&cache->lock, flags);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gnttab_page_cache_get);\n\nvoid gnttab_page_cache_put(struct gnttab_page_cache *cache, struct page **page,\n\t\t\t   unsigned int num)\n{\n\tunsigned long flags;\n\tunsigned int i;\n\n\tspin_lock_irqsave(&cache->lock, flags);\n\n\tfor (i = 0; i < num; i++)\n\t\tcache_enq(cache, page[i]);\n\tcache->num_pages += num;\n\n\tspin_unlock_irqrestore(&cache->lock, flags);\n}\nEXPORT_SYMBOL_GPL(gnttab_page_cache_put);\n\nvoid gnttab_page_cache_shrink(struct gnttab_page_cache *cache, unsigned int num)\n{\n\tstruct page *page[10];\n\tunsigned int i = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cache->lock, flags);\n\n\twhile (cache->num_pages > num) {\n\t\tpage[i] = cache_deq(cache);\n\t\tcache->num_pages--;\n\t\tif (++i == ARRAY_SIZE(page)) {\n\t\t\tspin_unlock_irqrestore(&cache->lock, flags);\n\t\t\tgnttab_free_pages(i, page);\n\t\t\ti = 0;\n\t\t\tspin_lock_irqsave(&cache->lock, flags);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&cache->lock, flags);\n\n\tif (i != 0)\n\t\tgnttab_free_pages(i, page);\n}\nEXPORT_SYMBOL_GPL(gnttab_page_cache_shrink);\n\nvoid gnttab_pages_clear_private(int nr_pages, struct page **pages)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (PagePrivate(pages[i])) {\n#if BITS_PER_LONG < 64\n\t\t\tkfree((void *)page_private(pages[i]));\n#endif\n\t\t\tClearPagePrivate(pages[i]);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(gnttab_pages_clear_private);\n\n \nvoid gnttab_free_pages(int nr_pages, struct page **pages)\n{\n\tgnttab_pages_clear_private(nr_pages, pages);\n\txen_free_unpopulated_pages(nr_pages, pages);\n}\nEXPORT_SYMBOL_GPL(gnttab_free_pages);\n\n#ifdef CONFIG_XEN_GRANT_DMA_ALLOC\n \nint gnttab_dma_alloc_pages(struct gnttab_dma_alloc_args *args)\n{\n\tunsigned long pfn, start_pfn;\n\tsize_t size;\n\tint i, ret;\n\n\tif (args->nr_pages < 0 || args->nr_pages > (INT_MAX >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\tsize = args->nr_pages << PAGE_SHIFT;\n\tif (args->coherent)\n\t\targs->vaddr = dma_alloc_coherent(args->dev, size,\n\t\t\t\t\t\t &args->dev_bus_addr,\n\t\t\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\telse\n\t\targs->vaddr = dma_alloc_wc(args->dev, size,\n\t\t\t\t\t   &args->dev_bus_addr,\n\t\t\t\t\t   GFP_KERNEL | __GFP_NOWARN);\n\tif (!args->vaddr) {\n\t\tpr_debug(\"Failed to allocate DMA buffer of size %zu\\n\", size);\n\t\treturn -ENOMEM;\n\t}\n\n\tstart_pfn = __phys_to_pfn(args->dev_bus_addr);\n\tfor (pfn = start_pfn, i = 0; pfn < start_pfn + args->nr_pages;\n\t\t\tpfn++, i++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\targs->pages[i] = page;\n\t\targs->frames[i] = xen_page_to_gfn(page);\n\t\txenmem_reservation_scrub_page(page);\n\t}\n\n\txenmem_reservation_va_mapping_reset(args->nr_pages, args->pages);\n\n\tret = xenmem_reservation_decrease(args->nr_pages, args->frames);\n\tif (ret != args->nr_pages) {\n\t\tpr_debug(\"Failed to decrease reservation for DMA buffer\\n\");\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tret = gnttab_pages_set_private(args->nr_pages, args->pages);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tgnttab_dma_free_pages(args);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gnttab_dma_alloc_pages);\n\n \nint gnttab_dma_free_pages(struct gnttab_dma_alloc_args *args)\n{\n\tsize_t size;\n\tint i, ret;\n\n\tgnttab_pages_clear_private(args->nr_pages, args->pages);\n\n\tfor (i = 0; i < args->nr_pages; i++)\n\t\targs->frames[i] = page_to_xen_pfn(args->pages[i]);\n\n\tret = xenmem_reservation_increase(args->nr_pages, args->frames);\n\tif (ret != args->nr_pages) {\n\t\tpr_debug(\"Failed to increase reservation for DMA buffer\\n\");\n\t\tret = -EFAULT;\n\t} else {\n\t\tret = 0;\n\t}\n\n\txenmem_reservation_va_mapping_update(args->nr_pages, args->pages,\n\t\t\t\t\t     args->frames);\n\n\tsize = args->nr_pages << PAGE_SHIFT;\n\tif (args->coherent)\n\t\tdma_free_coherent(args->dev, size,\n\t\t\t\t  args->vaddr, args->dev_bus_addr);\n\telse\n\t\tdma_free_wc(args->dev, size,\n\t\t\t    args->vaddr, args->dev_bus_addr);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gnttab_dma_free_pages);\n#endif\n\n \n#define MAX_DELAY 256\nstatic inline void\ngnttab_retry_eagain_gop(unsigned int cmd, void *gop, int16_t *status,\n\t\t\t\t\t\tconst char *func)\n{\n\tunsigned delay = 1;\n\n\tdo {\n\t\tBUG_ON(HYPERVISOR_grant_table_op(cmd, gop, 1));\n\t\tif (*status == GNTST_eagain)\n\t\t\tmsleep(delay++);\n\t} while ((*status == GNTST_eagain) && (delay < MAX_DELAY));\n\n\tif (delay >= MAX_DELAY) {\n\t\tpr_err(\"%s: %s eagain grant\\n\", func, current->comm);\n\t\t*status = GNTST_bad_page;\n\t}\n}\n\nvoid gnttab_batch_map(struct gnttab_map_grant_ref *batch, unsigned count)\n{\n\tstruct gnttab_map_grant_ref *op;\n\n\tif (HYPERVISOR_grant_table_op(GNTTABOP_map_grant_ref, batch, count))\n\t\tBUG();\n\tfor (op = batch; op < batch + count; op++)\n\t\tif (op->status == GNTST_eagain)\n\t\t\tgnttab_retry_eagain_gop(GNTTABOP_map_grant_ref, op,\n\t\t\t\t\t\t&op->status, __func__);\n}\nEXPORT_SYMBOL_GPL(gnttab_batch_map);\n\nvoid gnttab_batch_copy(struct gnttab_copy *batch, unsigned count)\n{\n\tstruct gnttab_copy *op;\n\n\tif (HYPERVISOR_grant_table_op(GNTTABOP_copy, batch, count))\n\t\tBUG();\n\tfor (op = batch; op < batch + count; op++)\n\t\tif (op->status == GNTST_eagain)\n\t\t\tgnttab_retry_eagain_gop(GNTTABOP_copy, op,\n\t\t\t\t\t\t&op->status, __func__);\n}\nEXPORT_SYMBOL_GPL(gnttab_batch_copy);\n\nvoid gnttab_foreach_grant_in_range(struct page *page,\n\t\t\t\t   unsigned int offset,\n\t\t\t\t   unsigned int len,\n\t\t\t\t   xen_grant_fn_t fn,\n\t\t\t\t   void *data)\n{\n\tunsigned int goffset;\n\tunsigned int glen;\n\tunsigned long xen_pfn;\n\n\tlen = min_t(unsigned int, PAGE_SIZE - offset, len);\n\tgoffset = xen_offset_in_page(offset);\n\n\txen_pfn = page_to_xen_pfn(page) + XEN_PFN_DOWN(offset);\n\n\twhile (len) {\n\t\tglen = min_t(unsigned int, XEN_PAGE_SIZE - goffset, len);\n\t\tfn(pfn_to_gfn(xen_pfn), goffset, glen, data);\n\n\t\tgoffset = 0;\n\t\txen_pfn++;\n\t\tlen -= glen;\n\t}\n}\nEXPORT_SYMBOL_GPL(gnttab_foreach_grant_in_range);\n\nvoid gnttab_foreach_grant(struct page **pages,\n\t\t\t  unsigned int nr_grefs,\n\t\t\t  xen_grant_fn_t fn,\n\t\t\t  void *data)\n{\n\tunsigned int goffset = 0;\n\tunsigned long xen_pfn = 0;\n\tunsigned int i;\n\n\tfor (i = 0; i < nr_grefs; i++) {\n\t\tif ((i % XEN_PFN_PER_PAGE) == 0) {\n\t\t\txen_pfn = page_to_xen_pfn(pages[i / XEN_PFN_PER_PAGE]);\n\t\t\tgoffset = 0;\n\t\t}\n\n\t\tfn(pfn_to_gfn(xen_pfn), goffset, XEN_PAGE_SIZE, data);\n\n\t\tgoffset += XEN_PAGE_SIZE;\n\t\txen_pfn++;\n\t}\n}\n\nint gnttab_map_refs(struct gnttab_map_grant_ref *map_ops,\n\t\t    struct gnttab_map_grant_ref *kmap_ops,\n\t\t    struct page **pages, unsigned int count)\n{\n\tint i, ret;\n\n\tret = HYPERVISOR_grant_table_op(GNTTABOP_map_grant_ref, map_ops, count);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tswitch (map_ops[i].status) {\n\t\tcase GNTST_okay:\n\t\t{\n\t\t\tstruct xen_page_foreign *foreign;\n\n\t\t\tSetPageForeign(pages[i]);\n\t\t\tforeign = xen_page_foreign(pages[i]);\n\t\t\tforeign->domid = map_ops[i].dom;\n\t\t\tforeign->gref = map_ops[i].ref;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase GNTST_no_device_space:\n\t\t\tpr_warn_ratelimited(\"maptrack limit reached, can't map all guest pages\\n\");\n\t\t\tbreak;\n\n\t\tcase GNTST_eagain:\n\t\t\t \n\t\t\tgnttab_retry_eagain_gop(GNTTABOP_map_grant_ref,\n\t\t\t\t\t\tmap_ops + i,\n\t\t\t\t\t\t&map_ops[i].status, __func__);\n\t\t\t \n\t\t\ti--;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn set_foreign_p2m_mapping(map_ops, kmap_ops, pages, count);\n}\nEXPORT_SYMBOL_GPL(gnttab_map_refs);\n\nint gnttab_unmap_refs(struct gnttab_unmap_grant_ref *unmap_ops,\n\t\t      struct gnttab_unmap_grant_ref *kunmap_ops,\n\t\t      struct page **pages, unsigned int count)\n{\n\tunsigned int i;\n\tint ret;\n\n\tret = HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap_ops, count);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < count; i++)\n\t\tClearPageForeign(pages[i]);\n\n\treturn clear_foreign_p2m_mapping(unmap_ops, kunmap_ops, pages, count);\n}\nEXPORT_SYMBOL_GPL(gnttab_unmap_refs);\n\n#define GNTTAB_UNMAP_REFS_DELAY 5\n\nstatic void __gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item);\n\nstatic void gnttab_unmap_work(struct work_struct *work)\n{\n\tstruct gntab_unmap_queue_data\n\t\t*unmap_data = container_of(work, \n\t\t\t\t\t   struct gntab_unmap_queue_data,\n\t\t\t\t\t   gnttab_work.work);\n\tif (unmap_data->age != UINT_MAX)\n\t\tunmap_data->age++;\n\t__gnttab_unmap_refs_async(unmap_data);\n}\n\nstatic void __gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item)\n{\n\tint ret;\n\tint pc;\n\n\tfor (pc = 0; pc < item->count; pc++) {\n\t\tif (page_count(item->pages[pc]) > 1) {\n\t\t\tunsigned long delay = GNTTAB_UNMAP_REFS_DELAY * (item->age + 1);\n\t\t\tschedule_delayed_work(&item->gnttab_work,\n\t\t\t\t\t      msecs_to_jiffies(delay));\n\t\t\treturn;\n\t\t}\n\t}\n\n\tret = gnttab_unmap_refs(item->unmap_ops, item->kunmap_ops,\n\t\t\t\titem->pages, item->count);\n\titem->done(ret, item);\n}\n\nvoid gnttab_unmap_refs_async(struct gntab_unmap_queue_data* item)\n{\n\tINIT_DELAYED_WORK(&item->gnttab_work, gnttab_unmap_work);\n\titem->age = 0;\n\n\t__gnttab_unmap_refs_async(item);\n}\nEXPORT_SYMBOL_GPL(gnttab_unmap_refs_async);\n\nstatic void unmap_refs_callback(int result,\n\t\tstruct gntab_unmap_queue_data *data)\n{\n\tstruct unmap_refs_callback_data *d = data->data;\n\n\td->result = result;\n\tcomplete(&d->completion);\n}\n\nint gnttab_unmap_refs_sync(struct gntab_unmap_queue_data *item)\n{\n\tstruct unmap_refs_callback_data data;\n\n\tinit_completion(&data.completion);\n\titem->data = &data;\n\titem->done = &unmap_refs_callback;\n\tgnttab_unmap_refs_async(item);\n\twait_for_completion(&data.completion);\n\n\treturn data.result;\n}\nEXPORT_SYMBOL_GPL(gnttab_unmap_refs_sync);\n\nstatic unsigned int nr_status_frames(unsigned int nr_grant_frames)\n{\n\treturn gnttab_frames(nr_grant_frames, SPP);\n}\n\nstatic int gnttab_map_frames_v1(xen_pfn_t *frames, unsigned int nr_gframes)\n{\n\tint rc;\n\n\trc = arch_gnttab_map_shared(frames, nr_gframes,\n\t\t\t\t    gnttab_max_grant_frames(),\n\t\t\t\t    &gnttab_shared.addr);\n\tBUG_ON(rc);\n\n\treturn 0;\n}\n\nstatic void gnttab_unmap_frames_v1(void)\n{\n\tarch_gnttab_unmap(gnttab_shared.addr, nr_grant_frames);\n}\n\nstatic int gnttab_map_frames_v2(xen_pfn_t *frames, unsigned int nr_gframes)\n{\n\tuint64_t *sframes;\n\tunsigned int nr_sframes;\n\tstruct gnttab_get_status_frames getframes;\n\tint rc;\n\n\tnr_sframes = nr_status_frames(nr_gframes);\n\n\t \n\tsframes = kmalloc_array(nr_sframes, sizeof(uint64_t), GFP_ATOMIC);\n\tif (!sframes)\n\t\treturn -ENOMEM;\n\n\tgetframes.dom        = DOMID_SELF;\n\tgetframes.nr_frames  = nr_sframes;\n\tset_xen_guest_handle(getframes.frame_list, sframes);\n\n\trc = HYPERVISOR_grant_table_op(GNTTABOP_get_status_frames,\n\t\t\t\t       &getframes, 1);\n\tif (rc == -ENOSYS) {\n\t\tkfree(sframes);\n\t\treturn -ENOSYS;\n\t}\n\n\tBUG_ON(rc || getframes.status);\n\n\trc = arch_gnttab_map_status(sframes, nr_sframes,\n\t\t\t\t    nr_status_frames(gnttab_max_grant_frames()),\n\t\t\t\t    &grstatus);\n\tBUG_ON(rc);\n\tkfree(sframes);\n\n\trc = arch_gnttab_map_shared(frames, nr_gframes,\n\t\t\t\t    gnttab_max_grant_frames(),\n\t\t\t\t    &gnttab_shared.addr);\n\tBUG_ON(rc);\n\n\treturn 0;\n}\n\nstatic void gnttab_unmap_frames_v2(void)\n{\n\tarch_gnttab_unmap(gnttab_shared.addr, nr_grant_frames);\n\tarch_gnttab_unmap(grstatus, nr_status_frames(nr_grant_frames));\n}\n\nstatic int gnttab_map(unsigned int start_idx, unsigned int end_idx)\n{\n\tstruct gnttab_setup_table setup;\n\txen_pfn_t *frames;\n\tunsigned int nr_gframes = end_idx + 1;\n\tint rc;\n\n\tif (xen_feature(XENFEAT_auto_translated_physmap)) {\n\t\tstruct xen_add_to_physmap xatp;\n\t\tunsigned int i = end_idx;\n\t\trc = 0;\n\t\tBUG_ON(xen_auto_xlat_grant_frames.count < nr_gframes);\n\t\t \n\t\tdo {\n\t\t\txatp.domid = DOMID_SELF;\n\t\t\txatp.idx = i;\n\t\t\txatp.space = XENMAPSPACE_grant_table;\n\t\t\txatp.gpfn = xen_auto_xlat_grant_frames.pfn[i];\n\t\t\trc = HYPERVISOR_memory_op(XENMEM_add_to_physmap, &xatp);\n\t\t\tif (rc != 0) {\n\t\t\t\tpr_warn(\"grant table add_to_physmap failed, err=%d\\n\",\n\t\t\t\t\trc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (i-- > start_idx);\n\n\t\treturn rc;\n\t}\n\n\t \n\tframes = kmalloc_array(nr_gframes, sizeof(unsigned long), GFP_ATOMIC);\n\tif (!frames)\n\t\treturn -ENOMEM;\n\n\tsetup.dom        = DOMID_SELF;\n\tsetup.nr_frames  = nr_gframes;\n\tset_xen_guest_handle(setup.frame_list, frames);\n\n\trc = HYPERVISOR_grant_table_op(GNTTABOP_setup_table, &setup, 1);\n\tif (rc == -ENOSYS) {\n\t\tkfree(frames);\n\t\treturn -ENOSYS;\n\t}\n\n\tBUG_ON(rc || setup.status);\n\n\trc = gnttab_interface->map_frames(frames, nr_gframes);\n\n\tkfree(frames);\n\n\treturn rc;\n}\n\nstatic const struct gnttab_ops gnttab_v1_ops = {\n\t.version\t\t\t= 1,\n\t.grefs_per_grant_frame\t\t= XEN_PAGE_SIZE /\n\t\t\t\t\t  sizeof(struct grant_entry_v1),\n\t.map_frames\t\t\t= gnttab_map_frames_v1,\n\t.unmap_frames\t\t\t= gnttab_unmap_frames_v1,\n\t.update_entry\t\t\t= gnttab_update_entry_v1,\n\t.end_foreign_access_ref\t\t= gnttab_end_foreign_access_ref_v1,\n\t.read_frame\t\t\t= gnttab_read_frame_v1,\n};\n\nstatic const struct gnttab_ops gnttab_v2_ops = {\n\t.version\t\t\t= 2,\n\t.grefs_per_grant_frame\t\t= XEN_PAGE_SIZE /\n\t\t\t\t\t  sizeof(union grant_entry_v2),\n\t.map_frames\t\t\t= gnttab_map_frames_v2,\n\t.unmap_frames\t\t\t= gnttab_unmap_frames_v2,\n\t.update_entry\t\t\t= gnttab_update_entry_v2,\n\t.end_foreign_access_ref\t\t= gnttab_end_foreign_access_ref_v2,\n\t.read_frame\t\t\t= gnttab_read_frame_v2,\n};\n\nstatic bool gnttab_need_v2(void)\n{\n#ifdef CONFIG_X86\n\tuint32_t base, width;\n\n\tif (xen_pv_domain()) {\n\t\tbase = xen_cpuid_base();\n\t\tif (cpuid_eax(base) < 5)\n\t\t\treturn false;\t \n\t\twidth = cpuid_ebx(base + 5) &\n\t\t\tXEN_CPUID_MACHINE_ADDRESS_WIDTH_MASK;\n\t\treturn width > 32 + PAGE_SHIFT;\n\t}\n#endif\n\treturn !!(max_possible_pfn >> 32);\n}\n\nstatic void gnttab_request_version(void)\n{\n\tlong rc;\n\tstruct gnttab_set_version gsv;\n\n\tif (gnttab_need_v2())\n\t\tgsv.version = 2;\n\telse\n\t\tgsv.version = 1;\n\n\t \n\tif (xen_gnttab_version >= 1 && xen_gnttab_version <= 2)\n\t\tgsv.version = xen_gnttab_version;\n\n\trc = HYPERVISOR_grant_table_op(GNTTABOP_set_version, &gsv, 1);\n\tif (rc == 0 && gsv.version == 2)\n\t\tgnttab_interface = &gnttab_v2_ops;\n\telse\n\t\tgnttab_interface = &gnttab_v1_ops;\n\tpr_info(\"Grant tables using version %d layout\\n\",\n\t\tgnttab_interface->version);\n}\n\nstatic int gnttab_setup(void)\n{\n\tunsigned int max_nr_gframes;\n\n\tmax_nr_gframes = gnttab_max_grant_frames();\n\tif (max_nr_gframes < nr_grant_frames)\n\t\treturn -ENOSYS;\n\n\tif (xen_feature(XENFEAT_auto_translated_physmap) && gnttab_shared.addr == NULL) {\n\t\tgnttab_shared.addr = xen_auto_xlat_grant_frames.vaddr;\n\t\tif (gnttab_shared.addr == NULL) {\n\t\t\tpr_warn(\"gnttab share frames is not mapped!\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\treturn gnttab_map(0, nr_grant_frames - 1);\n}\n\nint gnttab_resume(void)\n{\n\tgnttab_request_version();\n\treturn gnttab_setup();\n}\n\nint gnttab_suspend(void)\n{\n\tif (!xen_feature(XENFEAT_auto_translated_physmap))\n\t\tgnttab_interface->unmap_frames();\n\treturn 0;\n}\n\nstatic int gnttab_expand(unsigned int req_entries)\n{\n\tint rc;\n\tunsigned int cur, extra;\n\n\tcur = nr_grant_frames;\n\textra = ((req_entries + gnttab_interface->grefs_per_grant_frame - 1) /\n\t\t gnttab_interface->grefs_per_grant_frame);\n\tif (cur + extra > gnttab_max_grant_frames()) {\n\t\tpr_warn_ratelimited(\"xen/grant-table: max_grant_frames reached\"\n\t\t\t\t    \" cur=%u extra=%u limit=%u\"\n\t\t\t\t    \" gnttab_free_count=%u req_entries=%u\\n\",\n\t\t\t\t    cur, extra, gnttab_max_grant_frames(),\n\t\t\t\t    gnttab_free_count, req_entries);\n\t\treturn -ENOSPC;\n\t}\n\n\trc = gnttab_map(cur, cur + extra - 1);\n\tif (rc == 0)\n\t\trc = grow_gnttab_list(extra);\n\n\treturn rc;\n}\n\nint gnttab_init(void)\n{\n\tint i;\n\tunsigned long max_nr_grant_frames, max_nr_grefs;\n\tunsigned int max_nr_glist_frames, nr_glist_frames;\n\tint ret;\n\n\tgnttab_request_version();\n\tmax_nr_grant_frames = gnttab_max_grant_frames();\n\tmax_nr_grefs = max_nr_grant_frames *\n\t\t\tgnttab_interface->grefs_per_grant_frame;\n\tnr_grant_frames = 1;\n\n\t \n\tmax_nr_glist_frames = max_nr_grefs / RPP;\n\n\tgnttab_list = kmalloc_array(max_nr_glist_frames,\n\t\t\t\t    sizeof(grant_ref_t *),\n\t\t\t\t    GFP_KERNEL);\n\tif (gnttab_list == NULL)\n\t\treturn -ENOMEM;\n\n\tnr_glist_frames = gnttab_frames(nr_grant_frames, RPP);\n\tfor (i = 0; i < nr_glist_frames; i++) {\n\t\tgnttab_list[i] = (grant_ref_t *)__get_free_page(GFP_KERNEL);\n\t\tif (gnttab_list[i] == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto ini_nomem;\n\t\t}\n\t}\n\n\tgnttab_free_bitmap = bitmap_zalloc(max_nr_grefs, GFP_KERNEL);\n\tif (!gnttab_free_bitmap) {\n\t\tret = -ENOMEM;\n\t\tgoto ini_nomem;\n\t}\n\n\tret = arch_gnttab_init(max_nr_grant_frames,\n\t\t\t       nr_status_frames(max_nr_grant_frames));\n\tif (ret < 0)\n\t\tgoto ini_nomem;\n\n\tif (gnttab_setup() < 0) {\n\t\tret = -ENODEV;\n\t\tgoto ini_nomem;\n\t}\n\n\tgnttab_size = nr_grant_frames * gnttab_interface->grefs_per_grant_frame;\n\n\tgnttab_set_free(GNTTAB_NR_RESERVED_ENTRIES,\n\t\t\tgnttab_size - GNTTAB_NR_RESERVED_ENTRIES);\n\n\tprintk(\"Grant table initialized\\n\");\n\treturn 0;\n\n ini_nomem:\n\tfor (i--; i >= 0; i--)\n\t\tfree_page((unsigned long)gnttab_list[i]);\n\tkfree(gnttab_list);\n\tbitmap_free(gnttab_free_bitmap);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gnttab_init);\n\nstatic int __gnttab_init(void)\n{\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\t \n\tif (xen_hvm_domain() && !xen_pvh_domain())\n\t\treturn 0;\n\n\treturn gnttab_init();\n}\n \ncore_initcall_sync(__gnttab_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}