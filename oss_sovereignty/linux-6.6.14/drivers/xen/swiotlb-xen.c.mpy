{
  "module_name": "swiotlb-xen.c",
  "hash_id": "789b5064e80cc846077a53267abb5e62edacf420a0cfc88a86fb3521825b5d4e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/swiotlb-xen.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"xen:\" KBUILD_MODNAME \": \" fmt\n\n#include <linux/memblock.h>\n#include <linux/dma-direct.h>\n#include <linux/dma-map-ops.h>\n#include <linux/export.h>\n#include <xen/swiotlb-xen.h>\n#include <xen/page.h>\n#include <xen/xen-ops.h>\n#include <xen/hvc-console.h>\n\n#include <asm/dma-mapping.h>\n\n#include <trace/events/swiotlb.h>\n#define MAX_DMA_BITS 32\n\n \n\nstatic inline phys_addr_t xen_phys_to_bus(struct device *dev, phys_addr_t paddr)\n{\n\tunsigned long bfn = pfn_to_bfn(XEN_PFN_DOWN(paddr));\n\tphys_addr_t baddr = (phys_addr_t)bfn << XEN_PAGE_SHIFT;\n\n\tbaddr |= paddr & ~XEN_PAGE_MASK;\n\treturn baddr;\n}\n\nstatic inline dma_addr_t xen_phys_to_dma(struct device *dev, phys_addr_t paddr)\n{\n\treturn phys_to_dma(dev, xen_phys_to_bus(dev, paddr));\n}\n\nstatic inline phys_addr_t xen_bus_to_phys(struct device *dev,\n\t\t\t\t\t  phys_addr_t baddr)\n{\n\tunsigned long xen_pfn = bfn_to_pfn(XEN_PFN_DOWN(baddr));\n\tphys_addr_t paddr = (xen_pfn << XEN_PAGE_SHIFT) |\n\t\t\t    (baddr & ~XEN_PAGE_MASK);\n\n\treturn paddr;\n}\n\nstatic inline phys_addr_t xen_dma_to_phys(struct device *dev,\n\t\t\t\t\t  dma_addr_t dma_addr)\n{\n\treturn xen_bus_to_phys(dev, dma_to_phys(dev, dma_addr));\n}\n\nstatic inline int range_straddles_page_boundary(phys_addr_t p, size_t size)\n{\n\tunsigned long next_bfn, xen_pfn = XEN_PFN_DOWN(p);\n\tunsigned int i, nr_pages = XEN_PFN_UP(xen_offset_in_page(p) + size);\n\n\tnext_bfn = pfn_to_bfn(xen_pfn);\n\n\tfor (i = 1; i < nr_pages; i++)\n\t\tif (pfn_to_bfn(++xen_pfn) != ++next_bfn)\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)\n{\n\tunsigned long bfn = XEN_PFN_DOWN(dma_to_phys(dev, dma_addr));\n\tunsigned long xen_pfn = bfn_to_local_pfn(bfn);\n\tphys_addr_t paddr = (phys_addr_t)xen_pfn << XEN_PAGE_SHIFT;\n\n\t \n\tif (pfn_valid(PFN_DOWN(paddr)))\n\t\treturn is_swiotlb_buffer(dev, paddr);\n\treturn 0;\n}\n\n#ifdef CONFIG_X86\nint xen_swiotlb_fixup(void *buf, unsigned long nslabs)\n{\n\tint rc;\n\tunsigned int order = get_order(IO_TLB_SEGSIZE << IO_TLB_SHIFT);\n\tunsigned int i, dma_bits = order + PAGE_SHIFT;\n\tdma_addr_t dma_handle;\n\tphys_addr_t p = virt_to_phys(buf);\n\n\tBUILD_BUG_ON(IO_TLB_SEGSIZE & (IO_TLB_SEGSIZE - 1));\n\tBUG_ON(nslabs % IO_TLB_SEGSIZE);\n\n\ti = 0;\n\tdo {\n\t\tdo {\n\t\t\trc = xen_create_contiguous_region(\n\t\t\t\tp + (i << IO_TLB_SHIFT), order,\n\t\t\t\tdma_bits, &dma_handle);\n\t\t} while (rc && dma_bits++ < MAX_DMA_BITS);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\ti += IO_TLB_SEGSIZE;\n\t} while (i < nslabs);\n\treturn 0;\n}\n\nstatic void *\nxen_swiotlb_alloc_coherent(struct device *dev, size_t size,\n\t\tdma_addr_t *dma_handle, gfp_t flags, unsigned long attrs)\n{\n\tu64 dma_mask = dev->coherent_dma_mask;\n\tint order = get_order(size);\n\tphys_addr_t phys;\n\tvoid *ret;\n\n\t \n\tsize = 1UL << (order + XEN_PAGE_SHIFT);\n\n\tret = (void *)__get_free_pages(flags, get_order(size));\n\tif (!ret)\n\t\treturn ret;\n\tphys = virt_to_phys(ret);\n\n\t*dma_handle = xen_phys_to_dma(dev, phys);\n\tif (*dma_handle + size - 1 > dma_mask ||\n\t    range_straddles_page_boundary(phys, size)) {\n\t\tif (xen_create_contiguous_region(phys, order, fls64(dma_mask),\n\t\t\t\tdma_handle) != 0)\n\t\t\tgoto out_free_pages;\n\t\tSetPageXenRemapped(virt_to_page(ret));\n\t}\n\n\tmemset(ret, 0, size);\n\treturn ret;\n\nout_free_pages:\n\tfree_pages((unsigned long)ret, get_order(size));\n\treturn NULL;\n}\n\nstatic void\nxen_swiotlb_free_coherent(struct device *dev, size_t size, void *vaddr,\n\t\tdma_addr_t dma_handle, unsigned long attrs)\n{\n\tphys_addr_t phys = virt_to_phys(vaddr);\n\tint order = get_order(size);\n\n\t \n\tsize = 1UL << (order + XEN_PAGE_SHIFT);\n\n\tif (WARN_ON_ONCE(dma_handle + size - 1 > dev->coherent_dma_mask) ||\n\t    WARN_ON_ONCE(range_straddles_page_boundary(phys, size)))\n\t    \treturn;\n\n\tif (TestClearPageXenRemapped(virt_to_page(vaddr)))\n\t\txen_destroy_contiguous_region(phys, order);\n\tfree_pages((unsigned long)vaddr, get_order(size));\n}\n#endif  \n\n \nstatic dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,\n\t\t\t\tunsigned long offset, size_t size,\n\t\t\t\tenum dma_data_direction dir,\n\t\t\t\tunsigned long attrs)\n{\n\tphys_addr_t map, phys = page_to_phys(page) + offset;\n\tdma_addr_t dev_addr = xen_phys_to_dma(dev, phys);\n\n\tBUG_ON(dir == DMA_NONE);\n\t \n\tif (dma_capable(dev, dev_addr, size, true) &&\n\t    !range_straddles_page_boundary(phys, size) &&\n\t\t!xen_arch_need_swiotlb(dev, phys, dev_addr) &&\n\t\t!is_swiotlb_force_bounce(dev))\n\t\tgoto done;\n\n\t \n\ttrace_swiotlb_bounced(dev, dev_addr, size);\n\n\tmap = swiotlb_tbl_map_single(dev, phys, size, size, 0, dir, attrs);\n\tif (map == (phys_addr_t)DMA_MAPPING_ERROR)\n\t\treturn DMA_MAPPING_ERROR;\n\n\tphys = map;\n\tdev_addr = xen_phys_to_dma(dev, map);\n\n\t \n\tif (unlikely(!dma_capable(dev, dev_addr, size, true))) {\n\t\tswiotlb_tbl_unmap_single(dev, map, size, dir,\n\t\t\t\tattrs | DMA_ATTR_SKIP_CPU_SYNC);\n\t\treturn DMA_MAPPING_ERROR;\n\t}\n\ndone:\n\tif (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\n\t\tif (pfn_valid(PFN_DOWN(dma_to_phys(dev, dev_addr))))\n\t\t\tarch_sync_dma_for_device(phys, size, dir);\n\t\telse\n\t\t\txen_dma_sync_for_device(dev, dev_addr, size, dir);\n\t}\n\treturn dev_addr;\n}\n\n \nstatic void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\tphys_addr_t paddr = xen_dma_to_phys(hwdev, dev_addr);\n\n\tBUG_ON(dir == DMA_NONE);\n\n\tif (!dev_is_dma_coherent(hwdev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\n\t\tif (pfn_valid(PFN_DOWN(dma_to_phys(hwdev, dev_addr))))\n\t\t\tarch_sync_dma_for_cpu(paddr, size, dir);\n\t\telse\n\t\t\txen_dma_sync_for_cpu(hwdev, dev_addr, size, dir);\n\t}\n\n\t \n\tif (is_xen_swiotlb_buffer(hwdev, dev_addr))\n\t\tswiotlb_tbl_unmap_single(hwdev, paddr, size, dir, attrs);\n}\n\nstatic void\nxen_swiotlb_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);\n\n\tif (!dev_is_dma_coherent(dev)) {\n\t\tif (pfn_valid(PFN_DOWN(dma_to_phys(dev, dma_addr))))\n\t\t\tarch_sync_dma_for_cpu(paddr, size, dir);\n\t\telse\n\t\t\txen_dma_sync_for_cpu(dev, dma_addr, size, dir);\n\t}\n\n\tif (is_xen_swiotlb_buffer(dev, dma_addr))\n\t\tswiotlb_sync_single_for_cpu(dev, paddr, size, dir);\n}\n\nstatic void\nxen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);\n\n\tif (is_xen_swiotlb_buffer(dev, dma_addr))\n\t\tswiotlb_sync_single_for_device(dev, paddr, size, dir);\n\n\tif (!dev_is_dma_coherent(dev)) {\n\t\tif (pfn_valid(PFN_DOWN(dma_to_phys(dev, dma_addr))))\n\t\t\tarch_sync_dma_for_device(paddr, size, dir);\n\t\telse\n\t\t\txen_dma_sync_for_device(dev, dma_addr, size, dir);\n\t}\n}\n\n \nstatic void\nxen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tBUG_ON(dir == DMA_NONE);\n\n\tfor_each_sg(sgl, sg, nelems, i)\n\t\txen_swiotlb_unmap_page(hwdev, sg->dma_address, sg_dma_len(sg),\n\t\t\t\tdir, attrs);\n\n}\n\nstatic int\nxen_swiotlb_map_sg(struct device *dev, struct scatterlist *sgl, int nelems,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tBUG_ON(dir == DMA_NONE);\n\n\tfor_each_sg(sgl, sg, nelems, i) {\n\t\tsg->dma_address = xen_swiotlb_map_page(dev, sg_page(sg),\n\t\t\t\tsg->offset, sg->length, dir, attrs);\n\t\tif (sg->dma_address == DMA_MAPPING_ERROR)\n\t\t\tgoto out_unmap;\n\t\tsg_dma_len(sg) = sg->length;\n\t}\n\n\treturn nelems;\nout_unmap:\n\txen_swiotlb_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_dma_len(sgl) = 0;\n\treturn -EIO;\n}\n\nstatic void\nxen_swiotlb_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,\n\t\t\t    int nelems, enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tfor_each_sg(sgl, sg, nelems, i) {\n\t\txen_swiotlb_sync_single_for_cpu(dev, sg->dma_address,\n\t\t\t\tsg->length, dir);\n\t}\n}\n\nstatic void\nxen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,\n\t\t\t       int nelems, enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\tfor_each_sg(sgl, sg, nelems, i) {\n\t\txen_swiotlb_sync_single_for_device(dev, sg->dma_address,\n\t\t\t\tsg->length, dir);\n\t}\n}\n\n \nstatic int\nxen_swiotlb_dma_supported(struct device *hwdev, u64 mask)\n{\n\treturn xen_phys_to_dma(hwdev, default_swiotlb_limit()) <= mask;\n}\n\nconst struct dma_map_ops xen_swiotlb_dma_ops = {\n#ifdef CONFIG_X86\n\t.alloc = xen_swiotlb_alloc_coherent,\n\t.free = xen_swiotlb_free_coherent,\n#else\n\t.alloc = dma_direct_alloc,\n\t.free = dma_direct_free,\n#endif\n\t.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu,\n\t.sync_single_for_device = xen_swiotlb_sync_single_for_device,\n\t.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu,\n\t.sync_sg_for_device = xen_swiotlb_sync_sg_for_device,\n\t.map_sg = xen_swiotlb_map_sg,\n\t.unmap_sg = xen_swiotlb_unmap_sg,\n\t.map_page = xen_swiotlb_map_page,\n\t.unmap_page = xen_swiotlb_unmap_page,\n\t.dma_supported = xen_swiotlb_dma_supported,\n\t.mmap = dma_common_mmap,\n\t.get_sgtable = dma_common_get_sgtable,\n\t.alloc_pages = dma_common_alloc_pages,\n\t.free_pages = dma_common_free_pages,\n\t.max_mapping_size = swiotlb_max_mapping_size,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}