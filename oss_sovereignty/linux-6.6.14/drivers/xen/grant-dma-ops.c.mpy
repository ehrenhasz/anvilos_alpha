{
  "module_name": "grant-dma-ops.c",
  "hash_id": "fa98ad54ddd4e0884d2d08c035ac5ec8a8d4a307177e9dc8c3ad1abeea036a5b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/xen/grant-dma-ops.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/dma-map-ops.h>\n#include <linux/of.h>\n#include <linux/pci.h>\n#include <linux/pfn.h>\n#include <linux/xarray.h>\n#include <linux/virtio_anchor.h>\n#include <linux/virtio.h>\n#include <xen/xen.h>\n#include <xen/xen-ops.h>\n#include <xen/grant_table.h>\n\nstruct xen_grant_dma_data {\n\t \n\tdomid_t backend_domid;\n\t \n\tbool broken;\n};\n\nstatic DEFINE_XARRAY_FLAGS(xen_grant_dma_devices, XA_FLAGS_LOCK_IRQ);\n\n#define XEN_GRANT_DMA_ADDR_OFF\t(1ULL << 63)\n\nstatic inline dma_addr_t grant_to_dma(grant_ref_t grant)\n{\n\treturn XEN_GRANT_DMA_ADDR_OFF | ((dma_addr_t)grant << XEN_PAGE_SHIFT);\n}\n\nstatic inline grant_ref_t dma_to_grant(dma_addr_t dma)\n{\n\treturn (grant_ref_t)((dma & ~XEN_GRANT_DMA_ADDR_OFF) >> XEN_PAGE_SHIFT);\n}\n\nstatic struct xen_grant_dma_data *find_xen_grant_dma_data(struct device *dev)\n{\n\tstruct xen_grant_dma_data *data;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&xen_grant_dma_devices, flags);\n\tdata = xa_load(&xen_grant_dma_devices, (unsigned long)dev);\n\txa_unlock_irqrestore(&xen_grant_dma_devices, flags);\n\n\treturn data;\n}\n\nstatic int store_xen_grant_dma_data(struct device *dev,\n\t\t\t\t    struct xen_grant_dma_data *data)\n{\n\tunsigned long flags;\n\tint ret;\n\n\txa_lock_irqsave(&xen_grant_dma_devices, flags);\n\tret = xa_err(__xa_store(&xen_grant_dma_devices, (unsigned long)dev, data,\n\t\t\tGFP_ATOMIC));\n\txa_unlock_irqrestore(&xen_grant_dma_devices, flags);\n\n\treturn ret;\n}\n\n \nstatic void *xen_grant_dma_alloc(struct device *dev, size_t size,\n\t\t\t\t dma_addr_t *dma_handle, gfp_t gfp,\n\t\t\t\t unsigned long attrs)\n{\n\tstruct xen_grant_dma_data *data;\n\tunsigned int i, n_pages = XEN_PFN_UP(size);\n\tunsigned long pfn;\n\tgrant_ref_t grant;\n\tvoid *ret;\n\n\tdata = find_xen_grant_dma_data(dev);\n\tif (!data)\n\t\treturn NULL;\n\n\tif (unlikely(data->broken))\n\t\treturn NULL;\n\n\tret = alloc_pages_exact(n_pages * XEN_PAGE_SIZE, gfp);\n\tif (!ret)\n\t\treturn NULL;\n\n\tpfn = virt_to_pfn(ret);\n\n\tif (gnttab_alloc_grant_reference_seq(n_pages, &grant)) {\n\t\tfree_pages_exact(ret, n_pages * XEN_PAGE_SIZE);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < n_pages; i++) {\n\t\tgnttab_grant_foreign_access_ref(grant + i, data->backend_domid,\n\t\t\t\tpfn_to_gfn(pfn + i), 0);\n\t}\n\n\t*dma_handle = grant_to_dma(grant);\n\n\treturn ret;\n}\n\nstatic void xen_grant_dma_free(struct device *dev, size_t size, void *vaddr,\n\t\t\t       dma_addr_t dma_handle, unsigned long attrs)\n{\n\tstruct xen_grant_dma_data *data;\n\tunsigned int i, n_pages = XEN_PFN_UP(size);\n\tgrant_ref_t grant;\n\n\tdata = find_xen_grant_dma_data(dev);\n\tif (!data)\n\t\treturn;\n\n\tif (unlikely(data->broken))\n\t\treturn;\n\n\tgrant = dma_to_grant(dma_handle);\n\n\tfor (i = 0; i < n_pages; i++) {\n\t\tif (unlikely(!gnttab_end_foreign_access_ref(grant + i))) {\n\t\t\tdev_alert(dev, \"Grant still in use by backend domain, disabled for further use\\n\");\n\t\t\tdata->broken = true;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tgnttab_free_grant_reference_seq(grant, n_pages);\n\n\tfree_pages_exact(vaddr, n_pages * XEN_PAGE_SIZE);\n}\n\nstatic struct page *xen_grant_dma_alloc_pages(struct device *dev, size_t size,\n\t\t\t\t\t      dma_addr_t *dma_handle,\n\t\t\t\t\t      enum dma_data_direction dir,\n\t\t\t\t\t      gfp_t gfp)\n{\n\tvoid *vaddr;\n\n\tvaddr = xen_grant_dma_alloc(dev, size, dma_handle, gfp, 0);\n\tif (!vaddr)\n\t\treturn NULL;\n\n\treturn virt_to_page(vaddr);\n}\n\nstatic void xen_grant_dma_free_pages(struct device *dev, size_t size,\n\t\t\t\t     struct page *vaddr, dma_addr_t dma_handle,\n\t\t\t\t     enum dma_data_direction dir)\n{\n\txen_grant_dma_free(dev, size, page_to_virt(vaddr), dma_handle, 0);\n}\n\nstatic dma_addr_t xen_grant_dma_map_page(struct device *dev, struct page *page,\n\t\t\t\t\t unsigned long offset, size_t size,\n\t\t\t\t\t enum dma_data_direction dir,\n\t\t\t\t\t unsigned long attrs)\n{\n\tstruct xen_grant_dma_data *data;\n\tunsigned long dma_offset = xen_offset_in_page(offset),\n\t\t\tpfn_offset = XEN_PFN_DOWN(offset);\n\tunsigned int i, n_pages = XEN_PFN_UP(dma_offset + size);\n\tgrant_ref_t grant;\n\tdma_addr_t dma_handle;\n\n\tif (WARN_ON(dir == DMA_NONE))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tdata = find_xen_grant_dma_data(dev);\n\tif (!data)\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (unlikely(data->broken))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (gnttab_alloc_grant_reference_seq(n_pages, &grant))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tfor (i = 0; i < n_pages; i++) {\n\t\tgnttab_grant_foreign_access_ref(grant + i, data->backend_domid,\n\t\t\t\tpfn_to_gfn(page_to_xen_pfn(page) + i + pfn_offset),\n\t\t\t\tdir == DMA_TO_DEVICE);\n\t}\n\n\tdma_handle = grant_to_dma(grant) + dma_offset;\n\n\treturn dma_handle;\n}\n\nstatic void xen_grant_dma_unmap_page(struct device *dev, dma_addr_t dma_handle,\n\t\t\t\t     size_t size, enum dma_data_direction dir,\n\t\t\t\t     unsigned long attrs)\n{\n\tstruct xen_grant_dma_data *data;\n\tunsigned long dma_offset = xen_offset_in_page(dma_handle);\n\tunsigned int i, n_pages = XEN_PFN_UP(dma_offset + size);\n\tgrant_ref_t grant;\n\n\tif (WARN_ON(dir == DMA_NONE))\n\t\treturn;\n\n\tdata = find_xen_grant_dma_data(dev);\n\tif (!data)\n\t\treturn;\n\n\tif (unlikely(data->broken))\n\t\treturn;\n\n\tgrant = dma_to_grant(dma_handle);\n\n\tfor (i = 0; i < n_pages; i++) {\n\t\tif (unlikely(!gnttab_end_foreign_access_ref(grant + i))) {\n\t\t\tdev_alert(dev, \"Grant still in use by backend domain, disabled for further use\\n\");\n\t\t\tdata->broken = true;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tgnttab_free_grant_reference_seq(grant, n_pages);\n}\n\nstatic void xen_grant_dma_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\t\t\t   int nents, enum dma_data_direction dir,\n\t\t\t\t   unsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tunsigned int i;\n\n\tif (WARN_ON(dir == DMA_NONE))\n\t\treturn;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\txen_grant_dma_unmap_page(dev, s->dma_address, sg_dma_len(s), dir,\n\t\t\t\tattrs);\n}\n\nstatic int xen_grant_dma_map_sg(struct device *dev, struct scatterlist *sg,\n\t\t\t\tint nents, enum dma_data_direction dir,\n\t\t\t\tunsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tunsigned int i;\n\n\tif (WARN_ON(dir == DMA_NONE))\n\t\treturn -EINVAL;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\ts->dma_address = xen_grant_dma_map_page(dev, sg_page(s), s->offset,\n\t\t\t\ts->length, dir, attrs);\n\t\tif (s->dma_address == DMA_MAPPING_ERROR)\n\t\t\tgoto out;\n\n\t\tsg_dma_len(s) = s->length;\n\t}\n\n\treturn nents;\n\nout:\n\txen_grant_dma_unmap_sg(dev, sg, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_dma_len(sg) = 0;\n\n\treturn -EIO;\n}\n\nstatic int xen_grant_dma_supported(struct device *dev, u64 mask)\n{\n\treturn mask == DMA_BIT_MASK(64);\n}\n\nstatic const struct dma_map_ops xen_grant_dma_ops = {\n\t.alloc = xen_grant_dma_alloc,\n\t.free = xen_grant_dma_free,\n\t.alloc_pages = xen_grant_dma_alloc_pages,\n\t.free_pages = xen_grant_dma_free_pages,\n\t.mmap = dma_common_mmap,\n\t.get_sgtable = dma_common_get_sgtable,\n\t.map_page = xen_grant_dma_map_page,\n\t.unmap_page = xen_grant_dma_unmap_page,\n\t.map_sg = xen_grant_dma_map_sg,\n\t.unmap_sg = xen_grant_dma_unmap_sg,\n\t.dma_supported = xen_grant_dma_supported,\n};\n\nstatic struct device_node *xen_dt_get_node(struct device *dev)\n{\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\t\tstruct pci_bus *bus = pdev->bus;\n\n\t\t \n\t\twhile (!pci_is_root_bus(bus))\n\t\t\tbus = bus->parent;\n\n\t\tif (!bus->bridge->parent)\n\t\t\treturn NULL;\n\t\treturn of_node_get(bus->bridge->parent->of_node);\n\t}\n\n\treturn of_node_get(dev->of_node);\n}\n\nstatic int xen_dt_grant_init_backend_domid(struct device *dev,\n\t\t\t\t\t   struct device_node *np,\n\t\t\t\t\t   domid_t *backend_domid)\n{\n\tstruct of_phandle_args iommu_spec = { .args_count = 1 };\n\n\tif (dev_is_pci(dev)) {\n\t\tstruct pci_dev *pdev = to_pci_dev(dev);\n\t\tu32 rid = PCI_DEVID(pdev->bus->number, pdev->devfn);\n\n\t\tif (of_map_id(np, rid, \"iommu-map\", \"iommu-map-mask\", &iommu_spec.np,\n\t\t\t\tiommu_spec.args)) {\n\t\t\tdev_dbg(dev, \"Cannot translate ID\\n\");\n\t\t\treturn -ESRCH;\n\t\t}\n\t} else {\n\t\tif (of_parse_phandle_with_args(np, \"iommus\", \"#iommu-cells\",\n\t\t\t\t0, &iommu_spec)) {\n\t\t\tdev_dbg(dev, \"Cannot parse iommus property\\n\");\n\t\t\treturn -ESRCH;\n\t\t}\n\t}\n\n\tif (!of_device_is_compatible(iommu_spec.np, \"xen,grant-dma\") ||\n\t\t\tiommu_spec.args_count != 1) {\n\t\tdev_dbg(dev, \"Incompatible IOMMU node\\n\");\n\t\tof_node_put(iommu_spec.np);\n\t\treturn -ESRCH;\n\t}\n\n\tof_node_put(iommu_spec.np);\n\n\t \n\t*backend_domid = iommu_spec.args[0];\n\n\treturn 0;\n}\n\nstatic int xen_grant_init_backend_domid(struct device *dev,\n\t\t\t\t\tdomid_t *backend_domid)\n{\n\tstruct device_node *np;\n\tint ret = -ENODEV;\n\n\tnp = xen_dt_get_node(dev);\n\tif (np) {\n\t\tret = xen_dt_grant_init_backend_domid(dev, np, backend_domid);\n\t\tof_node_put(np);\n\t} else if (IS_ENABLED(CONFIG_XEN_VIRTIO_FORCE_GRANT) || xen_pv_domain()) {\n\t\tdev_info(dev, \"Using dom0 as backend\\n\");\n\t\t*backend_domid = 0;\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic void xen_grant_setup_dma_ops(struct device *dev, domid_t backend_domid)\n{\n\tstruct xen_grant_dma_data *data;\n\n\tdata = find_xen_grant_dma_data(dev);\n\tif (data) {\n\t\tdev_err(dev, \"Xen grant DMA data is already created\\n\");\n\t\treturn;\n\t}\n\n\tdata = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\tgoto err;\n\n\tdata->backend_domid = backend_domid;\n\n\tif (store_xen_grant_dma_data(dev, data)) {\n\t\tdev_err(dev, \"Cannot store Xen grant DMA data\\n\");\n\t\tgoto err;\n\t}\n\n\tdev->dma_ops = &xen_grant_dma_ops;\n\n\treturn;\n\nerr:\n\tdevm_kfree(dev, data);\n\tdev_err(dev, \"Cannot set up Xen grant DMA ops, retain platform DMA ops\\n\");\n}\n\nbool xen_virtio_restricted_mem_acc(struct virtio_device *dev)\n{\n\tdomid_t backend_domid;\n\n\tif (!xen_grant_init_backend_domid(dev->dev.parent, &backend_domid)) {\n\t\txen_grant_setup_dma_ops(dev->dev.parent, backend_domid);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nMODULE_DESCRIPTION(\"Xen grant DMA-mapping layer\");\nMODULE_AUTHOR(\"Juergen Gross <jgross@suse.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}