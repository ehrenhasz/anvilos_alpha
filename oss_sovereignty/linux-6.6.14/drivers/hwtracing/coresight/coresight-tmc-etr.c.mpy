{
  "module_name": "coresight-tmc-etr.c",
  "hash_id": "ace3128270139be969fc7ff67c23b446bb06060ad3bff92346df023a450b5375",
  "original_prompt": "Ingested from linux-6.6.14/drivers/hwtracing/coresight/coresight-tmc-etr.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/coresight.h>\n#include <linux/dma-mapping.h>\n#include <linux/iommu.h>\n#include <linux/idr.h>\n#include <linux/mutex.h>\n#include <linux/refcount.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/vmalloc.h>\n#include \"coresight-catu.h\"\n#include \"coresight-etm-perf.h\"\n#include \"coresight-priv.h\"\n#include \"coresight-tmc.h\"\n\nstruct etr_flat_buf {\n\tstruct device\t*dev;\n\tdma_addr_t\tdaddr;\n\tvoid\t\t*vaddr;\n\tsize_t\t\tsize;\n};\n\n \nstruct etr_perf_buffer {\n\tstruct tmc_drvdata\t*drvdata;\n\tstruct etr_buf\t\t*etr_buf;\n\tpid_t\t\t\tpid;\n\tbool\t\t\tsnapshot;\n\tint\t\t\tnr_pages;\n\tvoid\t\t\t**pages;\n};\n\n \n#define PERF_IDX2OFF(idx, buf)\t\t\\\n\t\t((idx) % ((unsigned long)(buf)->nr_pages << PAGE_SHIFT))\n\n \n#define TMC_ETR_PERF_MIN_BUF_SIZE\tSZ_1M\n\n \n\ntypedef u32 sgte_t;\n\n#define ETR_SG_PAGE_SHIFT\t\t12\n#define ETR_SG_PAGE_SIZE\t\t(1UL << ETR_SG_PAGE_SHIFT)\n#define ETR_SG_PAGES_PER_SYSPAGE\t(PAGE_SIZE / ETR_SG_PAGE_SIZE)\n#define ETR_SG_PTRS_PER_PAGE\t\t(ETR_SG_PAGE_SIZE / sizeof(sgte_t))\n#define ETR_SG_PTRS_PER_SYSPAGE\t\t(PAGE_SIZE / sizeof(sgte_t))\n\n#define ETR_SG_ET_MASK\t\t\t0x3\n#define ETR_SG_ET_LAST\t\t\t0x1\n#define ETR_SG_ET_NORMAL\t\t0x2\n#define ETR_SG_ET_LINK\t\t\t0x3\n\n#define ETR_SG_ADDR_SHIFT\t\t4\n\n#define ETR_SG_ENTRY(addr, type) \\\n\t(sgte_t)((((addr) >> ETR_SG_PAGE_SHIFT) << ETR_SG_ADDR_SHIFT) | \\\n\t\t (type & ETR_SG_ET_MASK))\n\n#define ETR_SG_ADDR(entry) \\\n\t(((dma_addr_t)(entry) >> ETR_SG_ADDR_SHIFT) << ETR_SG_PAGE_SHIFT)\n#define ETR_SG_ET(entry)\t\t((entry) & ETR_SG_ET_MASK)\n\n \nstruct etr_sg_table {\n\tstruct tmc_sg_table\t*sg_table;\n\tdma_addr_t\t\thwaddr;\n};\n\n \nstatic inline unsigned long __attribute_const__\ntmc_etr_sg_table_entries(int nr_pages)\n{\n\tunsigned long nr_sgpages = nr_pages * ETR_SG_PAGES_PER_SYSPAGE;\n\tunsigned long nr_sglinks = nr_sgpages / (ETR_SG_PTRS_PER_PAGE - 1);\n\t \n\tif (nr_sglinks && (nr_sgpages % (ETR_SG_PTRS_PER_PAGE - 1) < 2))\n\t\tnr_sglinks--;\n\treturn nr_sgpages + nr_sglinks;\n}\n\n \nstatic long\ntmc_pages_get_offset(struct tmc_pages *tmc_pages, dma_addr_t addr)\n{\n\tint i;\n\tdma_addr_t page_start;\n\n\tfor (i = 0; i < tmc_pages->nr_pages; i++) {\n\t\tpage_start = tmc_pages->daddrs[i];\n\t\tif (addr >= page_start && addr < (page_start + PAGE_SIZE))\n\t\t\treturn i * PAGE_SIZE + (addr - page_start);\n\t}\n\n\treturn -EINVAL;\n}\n\n \nstatic void tmc_pages_free(struct tmc_pages *tmc_pages,\n\t\t\t   struct device *dev, enum dma_data_direction dir)\n{\n\tint i;\n\tstruct device *real_dev = dev->parent;\n\n\tfor (i = 0; i < tmc_pages->nr_pages; i++) {\n\t\tif (tmc_pages->daddrs && tmc_pages->daddrs[i])\n\t\t\tdma_unmap_page(real_dev, tmc_pages->daddrs[i],\n\t\t\t\t\t PAGE_SIZE, dir);\n\t\tif (tmc_pages->pages && tmc_pages->pages[i])\n\t\t\t__free_page(tmc_pages->pages[i]);\n\t}\n\n\tkfree(tmc_pages->pages);\n\tkfree(tmc_pages->daddrs);\n\ttmc_pages->pages = NULL;\n\ttmc_pages->daddrs = NULL;\n\ttmc_pages->nr_pages = 0;\n}\n\n \nstatic int tmc_pages_alloc(struct tmc_pages *tmc_pages,\n\t\t\t   struct device *dev, int node,\n\t\t\t   enum dma_data_direction dir, void **pages)\n{\n\tint i, nr_pages;\n\tdma_addr_t paddr;\n\tstruct page *page;\n\tstruct device *real_dev = dev->parent;\n\n\tnr_pages = tmc_pages->nr_pages;\n\ttmc_pages->daddrs = kcalloc(nr_pages, sizeof(*tmc_pages->daddrs),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!tmc_pages->daddrs)\n\t\treturn -ENOMEM;\n\ttmc_pages->pages = kcalloc(nr_pages, sizeof(*tmc_pages->pages),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!tmc_pages->pages) {\n\t\tkfree(tmc_pages->daddrs);\n\t\ttmc_pages->daddrs = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (pages && pages[i]) {\n\t\t\tpage = virt_to_page(pages[i]);\n\t\t\t \n\t\t\tget_page(page);\n\t\t} else {\n\t\t\tpage = alloc_pages_node(node,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO, 0);\n\t\t\tif (!page)\n\t\t\t\tgoto err;\n\t\t}\n\t\tpaddr = dma_map_page(real_dev, page, 0, PAGE_SIZE, dir);\n\t\tif (dma_mapping_error(real_dev, paddr))\n\t\t\tgoto err;\n\t\ttmc_pages->daddrs[i] = paddr;\n\t\ttmc_pages->pages[i] = page;\n\t}\n\treturn 0;\nerr:\n\ttmc_pages_free(tmc_pages, dev, dir);\n\treturn -ENOMEM;\n}\n\nstatic inline long\ntmc_sg_get_data_page_offset(struct tmc_sg_table *sg_table, dma_addr_t addr)\n{\n\treturn tmc_pages_get_offset(&sg_table->data_pages, addr);\n}\n\nstatic inline void tmc_free_table_pages(struct tmc_sg_table *sg_table)\n{\n\tif (sg_table->table_vaddr)\n\t\tvunmap(sg_table->table_vaddr);\n\ttmc_pages_free(&sg_table->table_pages, sg_table->dev, DMA_TO_DEVICE);\n}\n\nstatic void tmc_free_data_pages(struct tmc_sg_table *sg_table)\n{\n\tif (sg_table->data_vaddr)\n\t\tvunmap(sg_table->data_vaddr);\n\ttmc_pages_free(&sg_table->data_pages, sg_table->dev, DMA_FROM_DEVICE);\n}\n\nvoid tmc_free_sg_table(struct tmc_sg_table *sg_table)\n{\n\ttmc_free_table_pages(sg_table);\n\ttmc_free_data_pages(sg_table);\n}\nEXPORT_SYMBOL_GPL(tmc_free_sg_table);\n\n \nstatic int tmc_alloc_table_pages(struct tmc_sg_table *sg_table)\n{\n\tint rc;\n\tstruct tmc_pages *table_pages = &sg_table->table_pages;\n\n\trc = tmc_pages_alloc(table_pages, sg_table->dev,\n\t\t\t     dev_to_node(sg_table->dev),\n\t\t\t     DMA_TO_DEVICE, NULL);\n\tif (rc)\n\t\treturn rc;\n\tsg_table->table_vaddr = vmap(table_pages->pages,\n\t\t\t\t     table_pages->nr_pages,\n\t\t\t\t     VM_MAP,\n\t\t\t\t     PAGE_KERNEL);\n\tif (!sg_table->table_vaddr)\n\t\trc = -ENOMEM;\n\telse\n\t\tsg_table->table_daddr = table_pages->daddrs[0];\n\treturn rc;\n}\n\nstatic int tmc_alloc_data_pages(struct tmc_sg_table *sg_table, void **pages)\n{\n\tint rc;\n\n\t \n\trc = tmc_pages_alloc(&sg_table->data_pages,\n\t\t\t     sg_table->dev, sg_table->node,\n\t\t\t     DMA_FROM_DEVICE, pages);\n\tif (!rc) {\n\t\tsg_table->data_vaddr = vmap(sg_table->data_pages.pages,\n\t\t\t\t\t    sg_table->data_pages.nr_pages,\n\t\t\t\t\t    VM_MAP,\n\t\t\t\t\t    PAGE_KERNEL);\n\t\tif (!sg_table->data_vaddr)\n\t\t\trc = -ENOMEM;\n\t}\n\treturn rc;\n}\n\n \nstruct tmc_sg_table *tmc_alloc_sg_table(struct device *dev,\n\t\t\t\t\tint node,\n\t\t\t\t\tint nr_tpages,\n\t\t\t\t\tint nr_dpages,\n\t\t\t\t\tvoid **pages)\n{\n\tlong rc;\n\tstruct tmc_sg_table *sg_table;\n\n\tsg_table = kzalloc(sizeof(*sg_table), GFP_KERNEL);\n\tif (!sg_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\tsg_table->data_pages.nr_pages = nr_dpages;\n\tsg_table->table_pages.nr_pages = nr_tpages;\n\tsg_table->node = node;\n\tsg_table->dev = dev;\n\n\trc  = tmc_alloc_data_pages(sg_table, pages);\n\tif (!rc)\n\t\trc = tmc_alloc_table_pages(sg_table);\n\tif (rc) {\n\t\ttmc_free_sg_table(sg_table);\n\t\tkfree(sg_table);\n\t\treturn ERR_PTR(rc);\n\t}\n\n\treturn sg_table;\n}\nEXPORT_SYMBOL_GPL(tmc_alloc_sg_table);\n\n \nvoid tmc_sg_table_sync_data_range(struct tmc_sg_table *table,\n\t\t\t\t  u64 offset, u64 size)\n{\n\tint i, index, start;\n\tint npages = DIV_ROUND_UP(size, PAGE_SIZE);\n\tstruct device *real_dev = table->dev->parent;\n\tstruct tmc_pages *data = &table->data_pages;\n\n\tstart = offset >> PAGE_SHIFT;\n\tfor (i = start; i < (start + npages); i++) {\n\t\tindex = i % data->nr_pages;\n\t\tdma_sync_single_for_cpu(real_dev, data->daddrs[index],\n\t\t\t\t\tPAGE_SIZE, DMA_FROM_DEVICE);\n\t}\n}\nEXPORT_SYMBOL_GPL(tmc_sg_table_sync_data_range);\n\n \nvoid tmc_sg_table_sync_table(struct tmc_sg_table *sg_table)\n{\n\tint i;\n\tstruct device *real_dev = sg_table->dev->parent;\n\tstruct tmc_pages *table_pages = &sg_table->table_pages;\n\n\tfor (i = 0; i < table_pages->nr_pages; i++)\n\t\tdma_sync_single_for_device(real_dev, table_pages->daddrs[i],\n\t\t\t\t\t   PAGE_SIZE, DMA_TO_DEVICE);\n}\nEXPORT_SYMBOL_GPL(tmc_sg_table_sync_table);\n\n \nssize_t tmc_sg_table_get_data(struct tmc_sg_table *sg_table,\n\t\t\t      u64 offset, size_t len, char **bufpp)\n{\n\tsize_t size;\n\tint pg_idx = offset >> PAGE_SHIFT;\n\tint pg_offset = offset & (PAGE_SIZE - 1);\n\tstruct tmc_pages *data_pages = &sg_table->data_pages;\n\n\tsize = tmc_sg_table_buf_size(sg_table);\n\tif (offset >= size)\n\t\treturn -EINVAL;\n\n\t \n\tlen = (len < (size - offset)) ? len : size - offset;\n\t \n\tlen = (len < (PAGE_SIZE - pg_offset)) ? len : (PAGE_SIZE - pg_offset);\n\tif (len > 0)\n\t\t*bufpp = page_address(data_pages->pages[pg_idx]) + pg_offset;\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(tmc_sg_table_get_data);\n\n#ifdef ETR_SG_DEBUG\n \nstatic unsigned long\ntmc_sg_daddr_to_vaddr(struct tmc_sg_table *sg_table,\n\t\t      dma_addr_t addr, bool table)\n{\n\tlong offset;\n\tunsigned long base;\n\tstruct tmc_pages *tmc_pages;\n\n\tif (table) {\n\t\ttmc_pages = &sg_table->table_pages;\n\t\tbase = (unsigned long)sg_table->table_vaddr;\n\t} else {\n\t\ttmc_pages = &sg_table->data_pages;\n\t\tbase = (unsigned long)sg_table->data_vaddr;\n\t}\n\n\toffset = tmc_pages_get_offset(tmc_pages, addr);\n\tif (offset < 0)\n\t\treturn 0;\n\treturn base + offset;\n}\n\n \nstatic void tmc_etr_sg_table_dump(struct etr_sg_table *etr_table)\n{\n\tsgte_t *ptr;\n\tint i = 0;\n\tdma_addr_t addr;\n\tstruct tmc_sg_table *sg_table = etr_table->sg_table;\n\n\tptr = (sgte_t *)tmc_sg_daddr_to_vaddr(sg_table,\n\t\t\t\t\t      etr_table->hwaddr, true);\n\twhile (ptr) {\n\t\taddr = ETR_SG_ADDR(*ptr);\n\t\tswitch (ETR_SG_ET(*ptr)) {\n\t\tcase ETR_SG_ET_NORMAL:\n\t\t\tdev_dbg(sg_table->dev,\n\t\t\t\t\"%05d: %p\\t:[N] 0x%llx\\n\", i, ptr, addr);\n\t\t\tptr++;\n\t\t\tbreak;\n\t\tcase ETR_SG_ET_LINK:\n\t\t\tdev_dbg(sg_table->dev,\n\t\t\t\t\"%05d: *** %p\\t:{L} 0x%llx ***\\n\",\n\t\t\t\t i, ptr, addr);\n\t\t\tptr = (sgte_t *)tmc_sg_daddr_to_vaddr(sg_table,\n\t\t\t\t\t\t\t      addr, true);\n\t\t\tbreak;\n\t\tcase ETR_SG_ET_LAST:\n\t\t\tdev_dbg(sg_table->dev,\n\t\t\t\t\"%05d: ### %p\\t:[L] 0x%llx ###\\n\",\n\t\t\t\t i, ptr, addr);\n\t\t\treturn;\n\t\tdefault:\n\t\t\tdev_dbg(sg_table->dev,\n\t\t\t\t\"%05d: xxx %p\\t:[INVALID] 0x%llx xxx\\n\",\n\t\t\t\t i, ptr, addr);\n\t\t\treturn;\n\t\t}\n\t\ti++;\n\t}\n\tdev_dbg(sg_table->dev, \"******* End of Table *****\\n\");\n}\n#else\nstatic inline void tmc_etr_sg_table_dump(struct etr_sg_table *etr_table) {}\n#endif\n\n \n#define INC_IDX_ROUND(idx, size) ((idx) = ((idx) + 1) % (size))\nstatic void tmc_etr_sg_table_populate(struct etr_sg_table *etr_table)\n{\n\tdma_addr_t paddr;\n\tint i, type, nr_entries;\n\tint tpidx = 0;  \n\tint sgtidx = 0;\t \n\tint sgtentry = 0;  \n\tint dpidx = 0;  \n\tint spidx = 0;  \n\tsgte_t *ptr;  \n\tstruct tmc_sg_table *sg_table = etr_table->sg_table;\n\tdma_addr_t *table_daddrs = sg_table->table_pages.daddrs;\n\tdma_addr_t *data_daddrs = sg_table->data_pages.daddrs;\n\n\tnr_entries = tmc_etr_sg_table_entries(sg_table->data_pages.nr_pages);\n\t \n\tptr = sg_table->table_vaddr;\n\t \n\tfor (i = 0; i < nr_entries - 1; i++) {\n\t\tif (sgtentry == ETR_SG_PTRS_PER_PAGE - 1) {\n\t\t\t \n\t\t\tif (sgtidx == ETR_SG_PAGES_PER_SYSPAGE - 1) {\n\t\t\t\tpaddr = table_daddrs[tpidx + 1];\n\t\t\t} else {\n\t\t\t\tpaddr = table_daddrs[tpidx] +\n\t\t\t\t\t(ETR_SG_PAGE_SIZE * (sgtidx + 1));\n\t\t\t}\n\t\t\ttype = ETR_SG_ET_LINK;\n\t\t} else {\n\t\t\t \n\t\t\ttype = ETR_SG_ET_NORMAL;\n\t\t\tpaddr = data_daddrs[dpidx] + spidx * ETR_SG_PAGE_SIZE;\n\t\t\tif (!INC_IDX_ROUND(spidx, ETR_SG_PAGES_PER_SYSPAGE))\n\t\t\t\tdpidx++;\n\t\t}\n\t\t*ptr++ = ETR_SG_ENTRY(paddr, type);\n\t\t \n\t\tif (!INC_IDX_ROUND(sgtentry, ETR_SG_PTRS_PER_PAGE)) {\n\t\t\tif (!INC_IDX_ROUND(sgtidx, ETR_SG_PAGES_PER_SYSPAGE))\n\t\t\t\ttpidx++;\n\t\t}\n\t}\n\n\t \n\tpaddr = data_daddrs[dpidx] + spidx * ETR_SG_PAGE_SIZE;\n\t*ptr++ = ETR_SG_ENTRY(paddr, ETR_SG_ET_LAST);\n}\n\n \nstatic struct etr_sg_table *\ntmc_init_etr_sg_table(struct device *dev, int node,\n\t\t      unsigned long size, void **pages)\n{\n\tint nr_entries, nr_tpages;\n\tint nr_dpages = size >> PAGE_SHIFT;\n\tstruct tmc_sg_table *sg_table;\n\tstruct etr_sg_table *etr_table;\n\n\tetr_table = kzalloc(sizeof(*etr_table), GFP_KERNEL);\n\tif (!etr_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\tnr_entries = tmc_etr_sg_table_entries(nr_dpages);\n\tnr_tpages = DIV_ROUND_UP(nr_entries, ETR_SG_PTRS_PER_SYSPAGE);\n\n\tsg_table = tmc_alloc_sg_table(dev, node, nr_tpages, nr_dpages, pages);\n\tif (IS_ERR(sg_table)) {\n\t\tkfree(etr_table);\n\t\treturn ERR_CAST(sg_table);\n\t}\n\n\tetr_table->sg_table = sg_table;\n\t \n\tetr_table->hwaddr = sg_table->table_daddr;\n\ttmc_etr_sg_table_populate(etr_table);\n\t \n\ttmc_sg_table_sync_table(sg_table);\n\ttmc_etr_sg_table_dump(etr_table);\n\n\treturn etr_table;\n}\n\n \nstatic int tmc_etr_alloc_flat_buf(struct tmc_drvdata *drvdata,\n\t\t\t\t  struct etr_buf *etr_buf, int node,\n\t\t\t\t  void **pages)\n{\n\tstruct etr_flat_buf *flat_buf;\n\tstruct device *real_dev = drvdata->csdev->dev.parent;\n\n\t \n\tif (pages)\n\t\treturn -EINVAL;\n\n\tflat_buf = kzalloc(sizeof(*flat_buf), GFP_KERNEL);\n\tif (!flat_buf)\n\t\treturn -ENOMEM;\n\n\tflat_buf->vaddr = dma_alloc_noncoherent(real_dev, etr_buf->size,\n\t\t\t\t\t\t&flat_buf->daddr,\n\t\t\t\t\t\tDMA_FROM_DEVICE,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_NOWARN);\n\tif (!flat_buf->vaddr) {\n\t\tkfree(flat_buf);\n\t\treturn -ENOMEM;\n\t}\n\n\tflat_buf->size = etr_buf->size;\n\tflat_buf->dev = &drvdata->csdev->dev;\n\tetr_buf->hwaddr = flat_buf->daddr;\n\tetr_buf->mode = ETR_MODE_FLAT;\n\tetr_buf->private = flat_buf;\n\treturn 0;\n}\n\nstatic void tmc_etr_free_flat_buf(struct etr_buf *etr_buf)\n{\n\tstruct etr_flat_buf *flat_buf = etr_buf->private;\n\n\tif (flat_buf && flat_buf->daddr) {\n\t\tstruct device *real_dev = flat_buf->dev->parent;\n\n\t\tdma_free_noncoherent(real_dev, etr_buf->size,\n\t\t\t\t     flat_buf->vaddr, flat_buf->daddr,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t}\n\tkfree(flat_buf);\n}\n\nstatic void tmc_etr_sync_flat_buf(struct etr_buf *etr_buf, u64 rrp, u64 rwp)\n{\n\tstruct etr_flat_buf *flat_buf = etr_buf->private;\n\tstruct device *real_dev = flat_buf->dev->parent;\n\n\t \n\tetr_buf->offset = rrp - etr_buf->hwaddr;\n\tif (etr_buf->full)\n\t\tetr_buf->len = etr_buf->size;\n\telse\n\t\tetr_buf->len = rwp - rrp;\n\n\t \n\tif (etr_buf->offset + etr_buf->len > etr_buf->size)\n\t\tdma_sync_single_for_cpu(real_dev, flat_buf->daddr,\n\t\t\t\t\tetr_buf->size, DMA_FROM_DEVICE);\n\telse\n\t\tdma_sync_single_for_cpu(real_dev,\n\t\t\t\t\tflat_buf->daddr + etr_buf->offset,\n\t\t\t\t\tetr_buf->len, DMA_FROM_DEVICE);\n}\n\nstatic ssize_t tmc_etr_get_data_flat_buf(struct etr_buf *etr_buf,\n\t\t\t\t\t u64 offset, size_t len, char **bufpp)\n{\n\tstruct etr_flat_buf *flat_buf = etr_buf->private;\n\n\t*bufpp = (char *)flat_buf->vaddr + offset;\n\t \n\treturn len;\n}\n\nstatic const struct etr_buf_operations etr_flat_buf_ops = {\n\t.alloc = tmc_etr_alloc_flat_buf,\n\t.free = tmc_etr_free_flat_buf,\n\t.sync = tmc_etr_sync_flat_buf,\n\t.get_data = tmc_etr_get_data_flat_buf,\n};\n\n \nstatic int tmc_etr_alloc_sg_buf(struct tmc_drvdata *drvdata,\n\t\t\t\tstruct etr_buf *etr_buf, int node,\n\t\t\t\tvoid **pages)\n{\n\tstruct etr_sg_table *etr_table;\n\tstruct device *dev = &drvdata->csdev->dev;\n\n\tetr_table = tmc_init_etr_sg_table(dev, node,\n\t\t\t\t\t  etr_buf->size, pages);\n\tif (IS_ERR(etr_table))\n\t\treturn -ENOMEM;\n\tetr_buf->hwaddr = etr_table->hwaddr;\n\tetr_buf->mode = ETR_MODE_ETR_SG;\n\tetr_buf->private = etr_table;\n\treturn 0;\n}\n\nstatic void tmc_etr_free_sg_buf(struct etr_buf *etr_buf)\n{\n\tstruct etr_sg_table *etr_table = etr_buf->private;\n\n\tif (etr_table) {\n\t\ttmc_free_sg_table(etr_table->sg_table);\n\t\tkfree(etr_table);\n\t}\n}\n\nstatic ssize_t tmc_etr_get_data_sg_buf(struct etr_buf *etr_buf, u64 offset,\n\t\t\t\t       size_t len, char **bufpp)\n{\n\tstruct etr_sg_table *etr_table = etr_buf->private;\n\n\treturn tmc_sg_table_get_data(etr_table->sg_table, offset, len, bufpp);\n}\n\nstatic void tmc_etr_sync_sg_buf(struct etr_buf *etr_buf, u64 rrp, u64 rwp)\n{\n\tlong r_offset, w_offset;\n\tstruct etr_sg_table *etr_table = etr_buf->private;\n\tstruct tmc_sg_table *table = etr_table->sg_table;\n\n\t \n\tr_offset = tmc_sg_get_data_page_offset(table, rrp);\n\tif (r_offset < 0) {\n\t\tdev_warn(table->dev,\n\t\t\t \"Unable to map RRP %llx to offset\\n\", rrp);\n\t\tetr_buf->len = 0;\n\t\treturn;\n\t}\n\n\tw_offset = tmc_sg_get_data_page_offset(table, rwp);\n\tif (w_offset < 0) {\n\t\tdev_warn(table->dev,\n\t\t\t \"Unable to map RWP %llx to offset\\n\", rwp);\n\t\tetr_buf->len = 0;\n\t\treturn;\n\t}\n\n\tetr_buf->offset = r_offset;\n\tif (etr_buf->full)\n\t\tetr_buf->len = etr_buf->size;\n\telse\n\t\tetr_buf->len = ((w_offset < r_offset) ? etr_buf->size : 0) +\n\t\t\t\tw_offset - r_offset;\n\ttmc_sg_table_sync_data_range(table, r_offset, etr_buf->len);\n}\n\nstatic const struct etr_buf_operations etr_sg_buf_ops = {\n\t.alloc = tmc_etr_alloc_sg_buf,\n\t.free = tmc_etr_free_sg_buf,\n\t.sync = tmc_etr_sync_sg_buf,\n\t.get_data = tmc_etr_get_data_sg_buf,\n};\n\n \nstruct coresight_device *\ntmc_etr_get_catu_device(struct tmc_drvdata *drvdata)\n{\n\tstruct coresight_device *etr = drvdata->csdev;\n\tunion coresight_dev_subtype catu_subtype = {\n\t\t.helper_subtype = CORESIGHT_DEV_SUBTYPE_HELPER_CATU\n\t};\n\n\tif (!IS_ENABLED(CONFIG_CORESIGHT_CATU))\n\t\treturn NULL;\n\n\treturn coresight_find_output_type(etr->pdata, CORESIGHT_DEV_TYPE_HELPER,\n\t\t\t\t\t  catu_subtype);\n}\nEXPORT_SYMBOL_GPL(tmc_etr_get_catu_device);\n\nstatic const struct etr_buf_operations *etr_buf_ops[] = {\n\t[ETR_MODE_FLAT] = &etr_flat_buf_ops,\n\t[ETR_MODE_ETR_SG] = &etr_sg_buf_ops,\n\t[ETR_MODE_CATU] = NULL,\n};\n\nvoid tmc_etr_set_catu_ops(const struct etr_buf_operations *catu)\n{\n\tetr_buf_ops[ETR_MODE_CATU] = catu;\n}\nEXPORT_SYMBOL_GPL(tmc_etr_set_catu_ops);\n\nvoid tmc_etr_remove_catu_ops(void)\n{\n\tetr_buf_ops[ETR_MODE_CATU] = NULL;\n}\nEXPORT_SYMBOL_GPL(tmc_etr_remove_catu_ops);\n\nstatic inline int tmc_etr_mode_alloc_buf(int mode,\n\t\t\t\t\t struct tmc_drvdata *drvdata,\n\t\t\t\t\t struct etr_buf *etr_buf, int node,\n\t\t\t\t\t void **pages)\n{\n\tint rc = -EINVAL;\n\n\tswitch (mode) {\n\tcase ETR_MODE_FLAT:\n\tcase ETR_MODE_ETR_SG:\n\tcase ETR_MODE_CATU:\n\t\tif (etr_buf_ops[mode] && etr_buf_ops[mode]->alloc)\n\t\t\trc = etr_buf_ops[mode]->alloc(drvdata, etr_buf,\n\t\t\t\t\t\t      node, pages);\n\t\tif (!rc)\n\t\t\tetr_buf->ops = etr_buf_ops[mode];\n\t\treturn rc;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \nstatic struct etr_buf *tmc_alloc_etr_buf(struct tmc_drvdata *drvdata,\n\t\t\t\t\t ssize_t size, int flags,\n\t\t\t\t\t int node, void **pages)\n{\n\tint rc = -ENOMEM;\n\tbool has_etr_sg, has_iommu;\n\tbool has_sg, has_catu;\n\tstruct etr_buf *etr_buf;\n\tstruct device *dev = &drvdata->csdev->dev;\n\n\thas_etr_sg = tmc_etr_has_cap(drvdata, TMC_ETR_SG);\n\thas_iommu = iommu_get_domain_for_dev(dev->parent);\n\thas_catu = !!tmc_etr_get_catu_device(drvdata);\n\n\thas_sg = has_catu || has_etr_sg;\n\n\tetr_buf = kzalloc(sizeof(*etr_buf), GFP_KERNEL);\n\tif (!etr_buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tetr_buf->size = size;\n\n\t \n\tif (!pages &&\n\t    (!has_sg || has_iommu || size < SZ_1M))\n\t\trc = tmc_etr_mode_alloc_buf(ETR_MODE_FLAT, drvdata,\n\t\t\t\t\t    etr_buf, node, pages);\n\tif (rc && has_etr_sg)\n\t\trc = tmc_etr_mode_alloc_buf(ETR_MODE_ETR_SG, drvdata,\n\t\t\t\t\t    etr_buf, node, pages);\n\tif (rc && has_catu)\n\t\trc = tmc_etr_mode_alloc_buf(ETR_MODE_CATU, drvdata,\n\t\t\t\t\t    etr_buf, node, pages);\n\tif (rc) {\n\t\tkfree(etr_buf);\n\t\treturn ERR_PTR(rc);\n\t}\n\n\trefcount_set(&etr_buf->refcount, 1);\n\tdev_dbg(dev, \"allocated buffer of size %ldKB in mode %d\\n\",\n\t\t(unsigned long)size >> 10, etr_buf->mode);\n\treturn etr_buf;\n}\n\nstatic void tmc_free_etr_buf(struct etr_buf *etr_buf)\n{\n\tWARN_ON(!etr_buf->ops || !etr_buf->ops->free);\n\tetr_buf->ops->free(etr_buf);\n\tkfree(etr_buf);\n}\n\n \nstatic ssize_t tmc_etr_buf_get_data(struct etr_buf *etr_buf,\n\t\t\t\t    u64 offset, size_t len, char **bufpp)\n{\n\t \n\tlen = (len < (etr_buf->size - offset)) ? len : etr_buf->size - offset;\n\n\treturn etr_buf->ops->get_data(etr_buf, (u64)offset, len, bufpp);\n}\n\nstatic inline s64\ntmc_etr_buf_insert_barrier_packet(struct etr_buf *etr_buf, u64 offset)\n{\n\tssize_t len;\n\tchar *bufp;\n\n\tlen = tmc_etr_buf_get_data(etr_buf, offset,\n\t\t\t\t   CORESIGHT_BARRIER_PKT_SIZE, &bufp);\n\tif (WARN_ON(len < 0 || len < CORESIGHT_BARRIER_PKT_SIZE))\n\t\treturn -EINVAL;\n\tcoresight_insert_barrier_packet(bufp);\n\treturn offset + CORESIGHT_BARRIER_PKT_SIZE;\n}\n\n \nstatic void tmc_sync_etr_buf(struct tmc_drvdata *drvdata)\n{\n\tstruct etr_buf *etr_buf = drvdata->etr_buf;\n\tu64 rrp, rwp;\n\tu32 status;\n\n\trrp = tmc_read_rrp(drvdata);\n\trwp = tmc_read_rwp(drvdata);\n\tstatus = readl_relaxed(drvdata->base + TMC_STS);\n\n\t \n\tif (WARN_ON_ONCE(status & TMC_STS_MEMERR)) {\n\t\tdev_dbg(&drvdata->csdev->dev,\n\t\t\t\"tmc memory error detected, truncating buffer\\n\");\n\t\tetr_buf->len = 0;\n\t\tetr_buf->full = false;\n\t\treturn;\n\t}\n\n\tetr_buf->full = !!(status & TMC_STS_FULL);\n\n\tWARN_ON(!etr_buf->ops || !etr_buf->ops->sync);\n\n\tetr_buf->ops->sync(etr_buf, rrp, rwp);\n}\n\nstatic int __tmc_etr_enable_hw(struct tmc_drvdata *drvdata)\n{\n\tu32 axictl, sts;\n\tstruct etr_buf *etr_buf = drvdata->etr_buf;\n\tint rc = 0;\n\n\tCS_UNLOCK(drvdata->base);\n\n\t \n\trc = tmc_wait_for_tmcready(drvdata);\n\tif (rc) {\n\t\tdev_err(&drvdata->csdev->dev,\n\t\t\t\"Failed to enable : TMC not ready\\n\");\n\t\tCS_LOCK(drvdata->base);\n\t\treturn rc;\n\t}\n\n\twritel_relaxed(etr_buf->size / 4, drvdata->base + TMC_RSZ);\n\twritel_relaxed(TMC_MODE_CIRCULAR_BUFFER, drvdata->base + TMC_MODE);\n\n\taxictl = readl_relaxed(drvdata->base + TMC_AXICTL);\n\taxictl &= ~TMC_AXICTL_CLEAR_MASK;\n\taxictl |= TMC_AXICTL_PROT_CTL_B1;\n\taxictl |= TMC_AXICTL_WR_BURST(drvdata->max_burst_size);\n\taxictl |= TMC_AXICTL_AXCACHE_OS;\n\n\tif (tmc_etr_has_cap(drvdata, TMC_ETR_AXI_ARCACHE)) {\n\t\taxictl &= ~TMC_AXICTL_ARCACHE_MASK;\n\t\taxictl |= TMC_AXICTL_ARCACHE_OS;\n\t}\n\n\tif (etr_buf->mode == ETR_MODE_ETR_SG)\n\t\taxictl |= TMC_AXICTL_SCT_GAT_MODE;\n\n\twritel_relaxed(axictl, drvdata->base + TMC_AXICTL);\n\ttmc_write_dba(drvdata, etr_buf->hwaddr);\n\t \n\tif (tmc_etr_has_cap(drvdata, TMC_ETR_SAVE_RESTORE)) {\n\t\ttmc_write_rrp(drvdata, etr_buf->hwaddr);\n\t\ttmc_write_rwp(drvdata, etr_buf->hwaddr);\n\t\tsts = readl_relaxed(drvdata->base + TMC_STS) & ~TMC_STS_FULL;\n\t\twritel_relaxed(sts, drvdata->base + TMC_STS);\n\t}\n\n\twritel_relaxed(TMC_FFCR_EN_FMT | TMC_FFCR_EN_TI |\n\t\t       TMC_FFCR_FON_FLIN | TMC_FFCR_FON_TRIG_EVT |\n\t\t       TMC_FFCR_TRIGON_TRIGIN,\n\t\t       drvdata->base + TMC_FFCR);\n\twritel_relaxed(drvdata->trigger_cntr, drvdata->base + TMC_TRG);\n\ttmc_enable_hw(drvdata);\n\n\tCS_LOCK(drvdata->base);\n\treturn rc;\n}\n\nstatic int tmc_etr_enable_hw(struct tmc_drvdata *drvdata,\n\t\t\t     struct etr_buf *etr_buf)\n{\n\tint rc;\n\n\t \n\tif (WARN_ON(!etr_buf))\n\t\treturn -EINVAL;\n\n\tif ((etr_buf->mode == ETR_MODE_ETR_SG) &&\n\t    WARN_ON(!tmc_etr_has_cap(drvdata, TMC_ETR_SG)))\n\t\treturn -EINVAL;\n\n\tif (WARN_ON(drvdata->etr_buf))\n\t\treturn -EBUSY;\n\n\trc = coresight_claim_device(drvdata->csdev);\n\tif (!rc) {\n\t\tdrvdata->etr_buf = etr_buf;\n\t\trc = __tmc_etr_enable_hw(drvdata);\n\t\tif (rc) {\n\t\t\tdrvdata->etr_buf = NULL;\n\t\t\tcoresight_disclaim_device(drvdata->csdev);\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n \nssize_t tmc_etr_get_sysfs_trace(struct tmc_drvdata *drvdata,\n\t\t\t\tloff_t pos, size_t len, char **bufpp)\n{\n\ts64 offset;\n\tssize_t actual = len;\n\tstruct etr_buf *etr_buf = drvdata->sysfs_buf;\n\n\tif (pos + actual > etr_buf->len)\n\t\tactual = etr_buf->len - pos;\n\tif (actual <= 0)\n\t\treturn actual;\n\n\t \n\toffset = etr_buf->offset + pos;\n\tif (offset >= etr_buf->size)\n\t\toffset -= etr_buf->size;\n\treturn tmc_etr_buf_get_data(etr_buf, offset, actual, bufpp);\n}\n\nstatic struct etr_buf *\ntmc_etr_setup_sysfs_buf(struct tmc_drvdata *drvdata)\n{\n\treturn tmc_alloc_etr_buf(drvdata, drvdata->size,\n\t\t\t\t 0, cpu_to_node(0), NULL);\n}\n\nstatic void\ntmc_etr_free_sysfs_buf(struct etr_buf *buf)\n{\n\tif (buf)\n\t\ttmc_free_etr_buf(buf);\n}\n\nstatic void tmc_etr_sync_sysfs_buf(struct tmc_drvdata *drvdata)\n{\n\tstruct etr_buf *etr_buf = drvdata->etr_buf;\n\n\tif (WARN_ON(drvdata->sysfs_buf != etr_buf)) {\n\t\ttmc_etr_free_sysfs_buf(drvdata->sysfs_buf);\n\t\tdrvdata->sysfs_buf = NULL;\n\t} else {\n\t\ttmc_sync_etr_buf(drvdata);\n\t\t \n\t\tif (etr_buf->full)\n\t\t\ttmc_etr_buf_insert_barrier_packet(etr_buf,\n\t\t\t\t\t\t\t  etr_buf->offset);\n\t}\n}\n\nstatic void __tmc_etr_disable_hw(struct tmc_drvdata *drvdata)\n{\n\tCS_UNLOCK(drvdata->base);\n\n\ttmc_flush_and_stop(drvdata);\n\t \n\tif (drvdata->mode == CS_MODE_SYSFS)\n\t\ttmc_etr_sync_sysfs_buf(drvdata);\n\n\ttmc_disable_hw(drvdata);\n\n\tCS_LOCK(drvdata->base);\n\n}\n\nvoid tmc_etr_disable_hw(struct tmc_drvdata *drvdata)\n{\n\t__tmc_etr_disable_hw(drvdata);\n\tcoresight_disclaim_device(drvdata->csdev);\n\t \n\tdrvdata->etr_buf = NULL;\n}\n\nstatic struct etr_buf *tmc_etr_get_sysfs_buffer(struct coresight_device *csdev)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\tstruct etr_buf *sysfs_buf = NULL, *new_buf = NULL, *free_buf = NULL;\n\n\t \n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\tsysfs_buf = READ_ONCE(drvdata->sysfs_buf);\n\tif (!sysfs_buf || (sysfs_buf->size != drvdata->size)) {\n\t\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\t\t \n\t\tfree_buf = new_buf = tmc_etr_setup_sysfs_buf(drvdata);\n\t\tif (IS_ERR(new_buf))\n\t\t\treturn new_buf;\n\n\t\t \n\t\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\t}\n\n\tif (drvdata->reading || drvdata->mode == CS_MODE_PERF) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tsysfs_buf = READ_ONCE(drvdata->sysfs_buf);\n\tif (!sysfs_buf || (new_buf && sysfs_buf->size != new_buf->size)) {\n\t\tfree_buf = sysfs_buf;\n\t\tdrvdata->sysfs_buf = new_buf;\n\t}\n\nout:\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\t \n\tif (free_buf)\n\t\ttmc_etr_free_sysfs_buf(free_buf);\n\treturn ret ? ERR_PTR(ret) : drvdata->sysfs_buf;\n}\n\nstatic int tmc_enable_etr_sink_sysfs(struct coresight_device *csdev)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\tstruct etr_buf *sysfs_buf = tmc_etr_get_sysfs_buffer(csdev);\n\n\tif (IS_ERR(sysfs_buf))\n\t\treturn PTR_ERR(sysfs_buf);\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\n\t \n\tif (drvdata->mode == CS_MODE_SYSFS) {\n\t\tatomic_inc(&csdev->refcnt);\n\t\tgoto out;\n\t}\n\n\tret = tmc_etr_enable_hw(drvdata, sysfs_buf);\n\tif (!ret) {\n\t\tdrvdata->mode = CS_MODE_SYSFS;\n\t\tatomic_inc(&csdev->refcnt);\n\t}\n\nout:\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\tif (!ret)\n\t\tdev_dbg(&csdev->dev, \"TMC-ETR enabled\\n\");\n\n\treturn ret;\n}\n\nstruct etr_buf *tmc_etr_get_buffer(struct coresight_device *csdev,\n\t\t\t\t   enum cs_mode mode, void *data)\n{\n\tstruct perf_output_handle *handle = data;\n\tstruct etr_perf_buffer *etr_perf;\n\n\tswitch (mode) {\n\tcase CS_MODE_SYSFS:\n\t\treturn tmc_etr_get_sysfs_buffer(csdev);\n\tcase CS_MODE_PERF:\n\t\tetr_perf = etm_perf_sink_config(handle);\n\t\tif (WARN_ON(!etr_perf || !etr_perf->etr_buf))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\treturn etr_perf->etr_buf;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n}\nEXPORT_SYMBOL_GPL(tmc_etr_get_buffer);\n\n \nstatic struct etr_buf *\nalloc_etr_buf(struct tmc_drvdata *drvdata, struct perf_event *event,\n\t      int nr_pages, void **pages, bool snapshot)\n{\n\tint node;\n\tstruct etr_buf *etr_buf;\n\tunsigned long size;\n\n\tnode = (event->cpu == -1) ? NUMA_NO_NODE : cpu_to_node(event->cpu);\n\t \n\tif ((nr_pages << PAGE_SHIFT) > drvdata->size) {\n\t\tetr_buf = tmc_alloc_etr_buf(drvdata, ((ssize_t)nr_pages << PAGE_SHIFT),\n\t\t\t\t\t    0, node, NULL);\n\t\tif (!IS_ERR(etr_buf))\n\t\t\tgoto done;\n\t}\n\n\t \n\tsize = drvdata->size;\n\tdo {\n\t\tetr_buf = tmc_alloc_etr_buf(drvdata, size, 0, node, NULL);\n\t\tif (!IS_ERR(etr_buf))\n\t\t\tgoto done;\n\t\tsize /= 2;\n\t} while (size >= TMC_ETR_PERF_MIN_BUF_SIZE);\n\n\treturn ERR_PTR(-ENOMEM);\n\ndone:\n\treturn etr_buf;\n}\n\nstatic struct etr_buf *\nget_perf_etr_buf_cpu_wide(struct tmc_drvdata *drvdata,\n\t\t\t  struct perf_event *event, int nr_pages,\n\t\t\t  void **pages, bool snapshot)\n{\n\tint ret;\n\tpid_t pid = task_pid_nr(event->owner);\n\tstruct etr_buf *etr_buf;\n\nretry:\n\t \n\n\t \n\tmutex_lock(&drvdata->idr_mutex);\n\tetr_buf = idr_find(&drvdata->idr, pid);\n\tif (etr_buf) {\n\t\trefcount_inc(&etr_buf->refcount);\n\t\tmutex_unlock(&drvdata->idr_mutex);\n\t\treturn etr_buf;\n\t}\n\n\t \n\tmutex_unlock(&drvdata->idr_mutex);\n\n\tetr_buf = alloc_etr_buf(drvdata, event, nr_pages, pages, snapshot);\n\tif (IS_ERR(etr_buf))\n\t\treturn etr_buf;\n\n\t \n\tmutex_lock(&drvdata->idr_mutex);\n\tret = idr_alloc(&drvdata->idr, etr_buf, pid, pid + 1, GFP_KERNEL);\n\tmutex_unlock(&drvdata->idr_mutex);\n\n\t \n\tif (ret == -ENOSPC) {\n\t\ttmc_free_etr_buf(etr_buf);\n\t\tgoto retry;\n\t}\n\n\t \n\tif (ret == -ENOMEM) {\n\t\ttmc_free_etr_buf(etr_buf);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\n\treturn etr_buf;\n}\n\nstatic struct etr_buf *\nget_perf_etr_buf_per_thread(struct tmc_drvdata *drvdata,\n\t\t\t    struct perf_event *event, int nr_pages,\n\t\t\t    void **pages, bool snapshot)\n{\n\t \n\treturn alloc_etr_buf(drvdata, event, nr_pages, pages, snapshot);\n}\n\nstatic struct etr_buf *\nget_perf_etr_buf(struct tmc_drvdata *drvdata, struct perf_event *event,\n\t\t int nr_pages, void **pages, bool snapshot)\n{\n\tif (event->cpu == -1)\n\t\treturn get_perf_etr_buf_per_thread(drvdata, event, nr_pages,\n\t\t\t\t\t\t   pages, snapshot);\n\n\treturn get_perf_etr_buf_cpu_wide(drvdata, event, nr_pages,\n\t\t\t\t\t pages, snapshot);\n}\n\nstatic struct etr_perf_buffer *\ntmc_etr_setup_perf_buf(struct tmc_drvdata *drvdata, struct perf_event *event,\n\t\t       int nr_pages, void **pages, bool snapshot)\n{\n\tint node;\n\tstruct etr_buf *etr_buf;\n\tstruct etr_perf_buffer *etr_perf;\n\n\tnode = (event->cpu == -1) ? NUMA_NO_NODE : cpu_to_node(event->cpu);\n\n\tetr_perf = kzalloc_node(sizeof(*etr_perf), GFP_KERNEL, node);\n\tif (!etr_perf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tetr_buf = get_perf_etr_buf(drvdata, event, nr_pages, pages, snapshot);\n\tif (!IS_ERR(etr_buf))\n\t\tgoto done;\n\n\tkfree(etr_perf);\n\treturn ERR_PTR(-ENOMEM);\n\ndone:\n\t \n\tetr_perf->drvdata = drvdata;\n\tetr_perf->etr_buf = etr_buf;\n\n\treturn etr_perf;\n}\n\n\nstatic void *tmc_alloc_etr_buffer(struct coresight_device *csdev,\n\t\t\t\t  struct perf_event *event, void **pages,\n\t\t\t\t  int nr_pages, bool snapshot)\n{\n\tstruct etr_perf_buffer *etr_perf;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\n\tetr_perf = tmc_etr_setup_perf_buf(drvdata, event,\n\t\t\t\t\t  nr_pages, pages, snapshot);\n\tif (IS_ERR(etr_perf)) {\n\t\tdev_dbg(&csdev->dev, \"Unable to allocate ETR buffer\\n\");\n\t\treturn NULL;\n\t}\n\n\tetr_perf->pid = task_pid_nr(event->owner);\n\tetr_perf->snapshot = snapshot;\n\tetr_perf->nr_pages = nr_pages;\n\tetr_perf->pages = pages;\n\n\treturn etr_perf;\n}\n\nstatic void tmc_free_etr_buffer(void *config)\n{\n\tstruct etr_perf_buffer *etr_perf = config;\n\tstruct tmc_drvdata *drvdata = etr_perf->drvdata;\n\tstruct etr_buf *buf, *etr_buf = etr_perf->etr_buf;\n\n\tif (!etr_buf)\n\t\tgoto free_etr_perf_buffer;\n\n\tmutex_lock(&drvdata->idr_mutex);\n\t \n\tif (!refcount_dec_and_test(&etr_buf->refcount)) {\n\t\tmutex_unlock(&drvdata->idr_mutex);\n\t\tgoto free_etr_perf_buffer;\n\t}\n\n\t \n\tbuf = idr_remove(&drvdata->idr, etr_perf->pid);\n\tmutex_unlock(&drvdata->idr_mutex);\n\n\t \n\tif (buf && WARN_ON(buf != etr_buf))\n\t\tgoto free_etr_perf_buffer;\n\n\ttmc_free_etr_buf(etr_perf->etr_buf);\n\nfree_etr_perf_buffer:\n\tkfree(etr_perf);\n}\n\n \nstatic void tmc_etr_sync_perf_buffer(struct etr_perf_buffer *etr_perf,\n\t\t\t\t     unsigned long head,\n\t\t\t\t     unsigned long src_offset,\n\t\t\t\t     unsigned long to_copy)\n{\n\tlong bytes;\n\tlong pg_idx, pg_offset;\n\tchar **dst_pages, *src_buf;\n\tstruct etr_buf *etr_buf = etr_perf->etr_buf;\n\n\thead = PERF_IDX2OFF(head, etr_perf);\n\tpg_idx = head >> PAGE_SHIFT;\n\tpg_offset = head & (PAGE_SIZE - 1);\n\tdst_pages = (char **)etr_perf->pages;\n\n\twhile (to_copy > 0) {\n\t\t \n\t\tif (src_offset >= etr_buf->size)\n\t\t\tsrc_offset -= etr_buf->size;\n\t\tbytes = tmc_etr_buf_get_data(etr_buf, src_offset, to_copy,\n\t\t\t\t\t     &src_buf);\n\t\tif (WARN_ON_ONCE(bytes <= 0))\n\t\t\tbreak;\n\t\tbytes = min(bytes, (long)(PAGE_SIZE - pg_offset));\n\n\t\tmemcpy(dst_pages[pg_idx] + pg_offset, src_buf, bytes);\n\n\t\tto_copy -= bytes;\n\n\t\t \n\t\tpg_offset += bytes;\n\t\tif (pg_offset == PAGE_SIZE) {\n\t\t\tpg_offset = 0;\n\t\t\tif (++pg_idx == etr_perf->nr_pages)\n\t\t\t\tpg_idx = 0;\n\t\t}\n\n\t\t \n\t\tsrc_offset += bytes;\n\t}\n}\n\n \nstatic unsigned long\ntmc_update_etr_buffer(struct coresight_device *csdev,\n\t\t      struct perf_output_handle *handle,\n\t\t      void *config)\n{\n\tbool lost = false;\n\tunsigned long flags, offset, size = 0;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\tstruct etr_perf_buffer *etr_perf = config;\n\tstruct etr_buf *etr_buf = etr_perf->etr_buf;\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\n\t \n\tif (atomic_read(&csdev->refcnt) != 1) {\n\t\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\t\tgoto out;\n\t}\n\n\tif (WARN_ON(drvdata->perf_buf != etr_buf)) {\n\t\tlost = true;\n\t\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\t\tgoto out;\n\t}\n\n\tCS_UNLOCK(drvdata->base);\n\n\ttmc_flush_and_stop(drvdata);\n\ttmc_sync_etr_buf(drvdata);\n\n\tCS_LOCK(drvdata->base);\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\tlost = etr_buf->full;\n\toffset = etr_buf->offset;\n\tsize = etr_buf->len;\n\n\t \n\tif (!etr_perf->snapshot && size > handle->size) {\n\t\tu32 mask = tmc_get_memwidth_mask(drvdata);\n\n\t\t \n\t\tsize = handle->size & mask;\n\t\toffset = etr_buf->offset + etr_buf->len - size;\n\n\t\tif (offset >= etr_buf->size)\n\t\t\toffset -= etr_buf->size;\n\t\tlost = true;\n\t}\n\n\t \n\tif (lost)\n\t\ttmc_etr_buf_insert_barrier_packet(etr_buf, offset);\n\ttmc_etr_sync_perf_buffer(etr_perf, handle->head, offset, size);\n\n\t \n\tif (etr_perf->snapshot)\n\t\thandle->head += size;\n\n\t \n\tsmp_wmb();\n\nout:\n\t \n\tif (!etr_perf->snapshot && lost)\n\t\tperf_aux_output_flag(handle, PERF_AUX_FLAG_TRUNCATED);\n\treturn size;\n}\n\nstatic int tmc_enable_etr_sink_perf(struct coresight_device *csdev, void *data)\n{\n\tint rc = 0;\n\tpid_t pid;\n\tunsigned long flags;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\tstruct perf_output_handle *handle = data;\n\tstruct etr_perf_buffer *etr_perf = etm_perf_sink_config(handle);\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\t  \n\tif (drvdata->mode == CS_MODE_SYSFS) {\n\t\trc = -EBUSY;\n\t\tgoto unlock_out;\n\t}\n\n\tif (WARN_ON(!etr_perf || !etr_perf->etr_buf)) {\n\t\trc = -EINVAL;\n\t\tgoto unlock_out;\n\t}\n\n\t \n\tpid = etr_perf->pid;\n\n\t \n\tif (drvdata->pid != -1 && drvdata->pid != pid) {\n\t\trc = -EBUSY;\n\t\tgoto unlock_out;\n\t}\n\n\t \n\tif (drvdata->pid == pid) {\n\t\tatomic_inc(&csdev->refcnt);\n\t\tgoto unlock_out;\n\t}\n\n\trc = tmc_etr_enable_hw(drvdata, etr_perf->etr_buf);\n\tif (!rc) {\n\t\t \n\t\tdrvdata->pid = pid;\n\t\tdrvdata->mode = CS_MODE_PERF;\n\t\tdrvdata->perf_buf = etr_perf->etr_buf;\n\t\tatomic_inc(&csdev->refcnt);\n\t}\n\nunlock_out:\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\treturn rc;\n}\n\nstatic int tmc_enable_etr_sink(struct coresight_device *csdev,\n\t\t\t       enum cs_mode mode, void *data)\n{\n\tswitch (mode) {\n\tcase CS_MODE_SYSFS:\n\t\treturn tmc_enable_etr_sink_sysfs(csdev);\n\tcase CS_MODE_PERF:\n\t\treturn tmc_enable_etr_sink_perf(csdev, data);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int tmc_disable_etr_sink(struct coresight_device *csdev)\n{\n\tunsigned long flags;\n\tstruct tmc_drvdata *drvdata = dev_get_drvdata(csdev->dev.parent);\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\n\tif (drvdata->reading) {\n\t\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\t\treturn -EBUSY;\n\t}\n\n\tif (atomic_dec_return(&csdev->refcnt)) {\n\t\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tWARN_ON_ONCE(drvdata->mode == CS_MODE_DISABLED);\n\ttmc_etr_disable_hw(drvdata);\n\t \n\tdrvdata->pid = -1;\n\tdrvdata->mode = CS_MODE_DISABLED;\n\t \n\tdrvdata->perf_buf = NULL;\n\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\tdev_dbg(&csdev->dev, \"TMC-ETR disabled\\n\");\n\treturn 0;\n}\n\nstatic const struct coresight_ops_sink tmc_etr_sink_ops = {\n\t.enable\t\t= tmc_enable_etr_sink,\n\t.disable\t= tmc_disable_etr_sink,\n\t.alloc_buffer\t= tmc_alloc_etr_buffer,\n\t.update_buffer\t= tmc_update_etr_buffer,\n\t.free_buffer\t= tmc_free_etr_buffer,\n};\n\nconst struct coresight_ops tmc_etr_cs_ops = {\n\t.sink_ops\t= &tmc_etr_sink_ops,\n};\n\nint tmc_read_prepare_etr(struct tmc_drvdata *drvdata)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\n\t \n\tif (WARN_ON_ONCE(drvdata->config_type != TMC_CONFIG_TYPE_ETR))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\tif (drvdata->reading) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!drvdata->sysfs_buf) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (drvdata->mode == CS_MODE_SYSFS)\n\t\t__tmc_etr_disable_hw(drvdata);\n\n\tdrvdata->reading = true;\nout:\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\treturn ret;\n}\n\nint tmc_read_unprepare_etr(struct tmc_drvdata *drvdata)\n{\n\tunsigned long flags;\n\tstruct etr_buf *sysfs_buf = NULL;\n\n\t \n\tif (WARN_ON_ONCE(drvdata->config_type != TMC_CONFIG_TYPE_ETR))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&drvdata->spinlock, flags);\n\n\t \n\tif (drvdata->mode == CS_MODE_SYSFS) {\n\t\t \n\t\t__tmc_etr_enable_hw(drvdata);\n\t} else {\n\t\t \n\t\tsysfs_buf = drvdata->sysfs_buf;\n\t\tdrvdata->sysfs_buf = NULL;\n\t}\n\n\tdrvdata->reading = false;\n\tspin_unlock_irqrestore(&drvdata->spinlock, flags);\n\n\t \n\tif (sysfs_buf)\n\t\ttmc_etr_free_sysfs_buf(sysfs_buf);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}