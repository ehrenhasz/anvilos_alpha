{
  "module_name": "sata_mv.c",
  "hash_id": "a2861849ce8ff6797119e2164b9ad1d32584d3bb18049769ae662446402de211",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ata/sata_mv.c",
  "human_readable_source": "\n \n\n \n\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/dmapool.h>\n#include <linux/dma-mapping.h>\n#include <linux/device.h>\n#include <linux/clk.h>\n#include <linux/phy/phy.h>\n#include <linux/platform_device.h>\n#include <linux/ata_platform.h>\n#include <linux/mbus.h>\n#include <linux/bitops.h>\n#include <linux/gfp.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <linux/libata.h>\n\n#define DRV_NAME\t\"sata_mv\"\n#define DRV_VERSION\t\"1.28\"\n\n \n\n#ifdef CONFIG_PCI\nstatic int msi;\nmodule_param(msi, int, S_IRUGO);\nMODULE_PARM_DESC(msi, \"Enable use of PCI MSI (0=off, 1=on)\");\n#endif\n\nstatic int irq_coalescing_io_count;\nmodule_param(irq_coalescing_io_count, int, S_IRUGO);\nMODULE_PARM_DESC(irq_coalescing_io_count,\n\t\t \"IRQ coalescing I/O count threshold (0..255)\");\n\nstatic int irq_coalescing_usecs;\nmodule_param(irq_coalescing_usecs, int, S_IRUGO);\nMODULE_PARM_DESC(irq_coalescing_usecs,\n\t\t \"IRQ coalescing time threshold in usecs\");\n\nenum {\n\t \n\tMV_PRIMARY_BAR\t\t= 0,\t \n\tMV_IO_BAR\t\t= 2,\t \n\tMV_MISC_BAR\t\t= 3,\t \n\n\tMV_MAJOR_REG_AREA_SZ\t= 0x10000,\t \n\tMV_MINOR_REG_AREA_SZ\t= 0x2000,\t \n\n\t \n\tCOAL_CLOCKS_PER_USEC\t= 150,\t\t \n\tMAX_COAL_TIME_THRESHOLD\t= ((1 << 24) - 1),  \n\tMAX_COAL_IO_COUNT\t= 255,\t\t \n\n\tMV_PCI_REG_BASE\t\t= 0,\n\n\t \n\tCOAL_REG_BASE\t\t= 0x18000,\n\tIRQ_COAL_CAUSE\t\t= (COAL_REG_BASE + 0x08),\n\tALL_PORTS_COAL_IRQ\t= (1 << 4),\t \n\n\tIRQ_COAL_IO_THRESHOLD   = (COAL_REG_BASE + 0xcc),\n\tIRQ_COAL_TIME_THRESHOLD = (COAL_REG_BASE + 0xd0),\n\n\t \n\tTRAN_COAL_CAUSE_LO\t= (COAL_REG_BASE + 0x88),\n\tTRAN_COAL_CAUSE_HI\t= (COAL_REG_BASE + 0x8c),\n\n\tSATAHC0_REG_BASE\t= 0x20000,\n\tFLASH_CTL\t\t= 0x1046c,\n\tGPIO_PORT_CTL\t\t= 0x104f0,\n\tRESET_CFG\t\t= 0x180d8,\n\n\tMV_PCI_REG_SZ\t\t= MV_MAJOR_REG_AREA_SZ,\n\tMV_SATAHC_REG_SZ\t= MV_MAJOR_REG_AREA_SZ,\n\tMV_SATAHC_ARBTR_REG_SZ\t= MV_MINOR_REG_AREA_SZ,\t\t \n\tMV_PORT_REG_SZ\t\t= MV_MINOR_REG_AREA_SZ,\n\n\tMV_MAX_Q_DEPTH\t\t= 32,\n\tMV_MAX_Q_DEPTH_MASK\t= MV_MAX_Q_DEPTH - 1,\n\n\t \n\tMV_CRQB_Q_SZ\t\t= (32 * MV_MAX_Q_DEPTH),\n\tMV_CRPB_Q_SZ\t\t= (8 * MV_MAX_Q_DEPTH),\n\tMV_MAX_SG_CT\t\t= 256,\n\tMV_SG_TBL_SZ\t\t= (16 * MV_MAX_SG_CT),\n\n\t \n\tMV_PORT_HC_SHIFT\t= 2,\n\tMV_PORTS_PER_HC\t\t= (1 << MV_PORT_HC_SHIFT),  \n\t \n\tMV_PORT_MASK\t\t= (MV_PORTS_PER_HC - 1),    \n\n\t \n\tMV_FLAG_DUAL_HC\t\t= (1 << 30),   \n\n\tMV_COMMON_FLAGS\t\t= ATA_FLAG_SATA | ATA_FLAG_PIO_POLLING,\n\n\tMV_GEN_I_FLAGS\t\t= MV_COMMON_FLAGS | ATA_FLAG_NO_ATAPI,\n\n\tMV_GEN_II_FLAGS\t\t= MV_COMMON_FLAGS | ATA_FLAG_NCQ |\n\t\t\t\t  ATA_FLAG_PMP | ATA_FLAG_ACPI_SATA,\n\n\tMV_GEN_IIE_FLAGS\t= MV_GEN_II_FLAGS | ATA_FLAG_AN,\n\n\tCRQB_FLAG_READ\t\t= (1 << 0),\n\tCRQB_TAG_SHIFT\t\t= 1,\n\tCRQB_IOID_SHIFT\t\t= 6,\t \n\tCRQB_PMP_SHIFT\t\t= 12,\t \n\tCRQB_HOSTQ_SHIFT\t= 17,\t \n\tCRQB_CMD_ADDR_SHIFT\t= 8,\n\tCRQB_CMD_CS\t\t= (0x2 << 11),\n\tCRQB_CMD_LAST\t\t= (1 << 15),\n\n\tCRPB_FLAG_STATUS_SHIFT\t= 8,\n\tCRPB_IOID_SHIFT_6\t= 5,\t \n\tCRPB_IOID_SHIFT_7\t= 7,\t \n\n\tEPRD_FLAG_END_OF_TBL\t= (1 << 31),\n\n\t \n\n\tMV_PCI_COMMAND\t\t= 0xc00,\n\tMV_PCI_COMMAND_MWRCOM\t= (1 << 4),\t \n\tMV_PCI_COMMAND_MRDTRIG\t= (1 << 7),\t \n\n\tPCI_MAIN_CMD_STS\t= 0xd30,\n\tSTOP_PCI_MASTER\t\t= (1 << 2),\n\tPCI_MASTER_EMPTY\t= (1 << 3),\n\tGLOB_SFT_RST\t\t= (1 << 4),\n\n\tMV_PCI_MODE\t\t= 0xd00,\n\tMV_PCI_MODE_MASK\t= 0x30,\n\n\tMV_PCI_EXP_ROM_BAR_CTL\t= 0xd2c,\n\tMV_PCI_DISC_TIMER\t= 0xd04,\n\tMV_PCI_MSI_TRIGGER\t= 0xc38,\n\tMV_PCI_SERR_MASK\t= 0xc28,\n\tMV_PCI_XBAR_TMOUT\t= 0x1d04,\n\tMV_PCI_ERR_LOW_ADDRESS\t= 0x1d40,\n\tMV_PCI_ERR_HIGH_ADDRESS\t= 0x1d44,\n\tMV_PCI_ERR_ATTRIBUTE\t= 0x1d48,\n\tMV_PCI_ERR_COMMAND\t= 0x1d50,\n\n\tPCI_IRQ_CAUSE\t\t= 0x1d58,\n\tPCI_IRQ_MASK\t\t= 0x1d5c,\n\tPCI_UNMASK_ALL_IRQS\t= 0x7fffff,\t \n\n\tPCIE_IRQ_CAUSE\t\t= 0x1900,\n\tPCIE_IRQ_MASK\t\t= 0x1910,\n\tPCIE_UNMASK_ALL_IRQS\t= 0x40a,\t \n\n\t \n\tPCI_HC_MAIN_IRQ_CAUSE\t= 0x1d60,\n\tPCI_HC_MAIN_IRQ_MASK\t= 0x1d64,\n\tSOC_HC_MAIN_IRQ_CAUSE\t= 0x20020,\n\tSOC_HC_MAIN_IRQ_MASK\t= 0x20024,\n\tERR_IRQ\t\t\t= (1 << 0),\t \n\tDONE_IRQ\t\t= (1 << 1),\t \n\tHC0_IRQ_PEND\t\t= 0x1ff,\t \n\tHC_SHIFT\t\t= 9,\t\t \n\tDONE_IRQ_0_3\t\t= 0x000000aa,\t \n\tDONE_IRQ_4_7\t\t= (DONE_IRQ_0_3 << HC_SHIFT),   \n\tPCI_ERR\t\t\t= (1 << 18),\n\tTRAN_COAL_LO_DONE\t= (1 << 19),\t \n\tTRAN_COAL_HI_DONE\t= (1 << 20),\t \n\tPORTS_0_3_COAL_DONE\t= (1 << 8),\t \n\tPORTS_4_7_COAL_DONE\t= (1 << 17),\t \n\tALL_PORTS_COAL_DONE\t= (1 << 21),\t \n\tGPIO_INT\t\t= (1 << 22),\n\tSELF_INT\t\t= (1 << 23),\n\tTWSI_INT\t\t= (1 << 24),\n\tHC_MAIN_RSVD\t\t= (0x7f << 25),\t \n\tHC_MAIN_RSVD_5\t\t= (0x1fff << 19),  \n\tHC_MAIN_RSVD_SOC\t= (0x3fffffb << 6),      \n\n\t \n\tHC_CFG\t\t\t= 0x00,\n\n\tHC_IRQ_CAUSE\t\t= 0x14,\n\tDMA_IRQ\t\t\t= (1 << 0),\t \n\tHC_COAL_IRQ\t\t= (1 << 4),\t \n\tDEV_IRQ\t\t\t= (1 << 8),\t \n\n\t \n\tHC_IRQ_COAL_IO_THRESHOLD\t= 0x000c,\n\tHC_IRQ_COAL_TIME_THRESHOLD\t= 0x0010,\n\n\tSOC_LED_CTRL\t\t= 0x2c,\n\tSOC_LED_CTRL_BLINK\t= (1 << 0),\t \n\tSOC_LED_CTRL_ACT_PRESENCE = (1 << 2),\t \n\t\t\t\t\t\t \n\n\t \n\tSHD_BLK\t\t\t= 0x100,\n\tSHD_CTL_AST\t\t= 0x20,\t\t \n\n\t \n\tSATA_STATUS\t\t= 0x300,   \n\tSATA_ACTIVE\t\t= 0x350,\n\tFIS_IRQ_CAUSE\t\t= 0x364,\n\tFIS_IRQ_CAUSE_AN\t= (1 << 9),\t \n\n\tLTMODE\t\t\t= 0x30c,\t \n\tLTMODE_BIT8\t\t= (1 << 8),\t \n\n\tPHY_MODE2\t\t= 0x330,\n\tPHY_MODE3\t\t= 0x310,\n\n\tPHY_MODE4\t\t= 0x314,\t \n\tPHY_MODE4_CFG_MASK\t= 0x00000003,\t \n\tPHY_MODE4_CFG_VALUE\t= 0x00000001,\t \n\tPHY_MODE4_RSVD_ZEROS\t= 0x5de3fffa,\t \n\tPHY_MODE4_RSVD_ONES\t= 0x00000005,\t \n\n\tSATA_IFCTL\t\t= 0x344,\n\tSATA_TESTCTL\t\t= 0x348,\n\tSATA_IFSTAT\t\t= 0x34c,\n\tVENDOR_UNIQUE_FIS\t= 0x35c,\n\n\tFISCFG\t\t\t= 0x360,\n\tFISCFG_WAIT_DEV_ERR\t= (1 << 8),\t \n\tFISCFG_SINGLE_SYNC\t= (1 << 16),\t \n\n\tPHY_MODE9_GEN2\t\t= 0x398,\n\tPHY_MODE9_GEN1\t\t= 0x39c,\n\tPHYCFG_OFS\t\t= 0x3a0,\t \n\n\tMV5_PHY_MODE\t\t= 0x74,\n\tMV5_LTMODE\t\t= 0x30,\n\tMV5_PHY_CTL\t\t= 0x0C,\n\tSATA_IFCFG\t\t= 0x050,\n\tLP_PHY_CTL\t\t= 0x058,\n\tLP_PHY_CTL_PIN_PU_PLL   = (1 << 0),\n\tLP_PHY_CTL_PIN_PU_RX    = (1 << 1),\n\tLP_PHY_CTL_PIN_PU_TX    = (1 << 2),\n\tLP_PHY_CTL_GEN_TX_3G    = (1 << 5),\n\tLP_PHY_CTL_GEN_RX_3G    = (1 << 9),\n\n\tMV_M2_PREAMP_MASK\t= 0x7e0,\n\n\t \n\tEDMA_CFG\t\t= 0,\n\tEDMA_CFG_Q_DEPTH\t= 0x1f,\t\t \n\tEDMA_CFG_NCQ\t\t= (1 << 5),\t \n\tEDMA_CFG_NCQ_GO_ON_ERR\t= (1 << 14),\t \n\tEDMA_CFG_RD_BRST_EXT\t= (1 << 11),\t \n\tEDMA_CFG_WR_BUFF_LEN\t= (1 << 13),\t \n\tEDMA_CFG_EDMA_FBS\t= (1 << 16),\t \n\tEDMA_CFG_FBS\t\t= (1 << 26),\t \n\n\tEDMA_ERR_IRQ_CAUSE\t= 0x8,\n\tEDMA_ERR_IRQ_MASK\t= 0xc,\n\tEDMA_ERR_D_PAR\t\t= (1 << 0),\t \n\tEDMA_ERR_PRD_PAR\t= (1 << 1),\t \n\tEDMA_ERR_DEV\t\t= (1 << 2),\t \n\tEDMA_ERR_DEV_DCON\t= (1 << 3),\t \n\tEDMA_ERR_DEV_CON\t= (1 << 4),\t \n\tEDMA_ERR_SERR\t\t= (1 << 5),\t \n\tEDMA_ERR_SELF_DIS\t= (1 << 7),\t \n\tEDMA_ERR_SELF_DIS_5\t= (1 << 8),\t \n\tEDMA_ERR_BIST_ASYNC\t= (1 << 8),\t \n\tEDMA_ERR_TRANS_IRQ_7\t= (1 << 8),\t \n\tEDMA_ERR_CRQB_PAR\t= (1 << 9),\t \n\tEDMA_ERR_CRPB_PAR\t= (1 << 10),\t \n\tEDMA_ERR_INTRL_PAR\t= (1 << 11),\t \n\tEDMA_ERR_IORDY\t\t= (1 << 12),\t \n\n\tEDMA_ERR_LNK_CTRL_RX\t= (0xf << 13),\t \n\tEDMA_ERR_LNK_CTRL_RX_0\t= (1 << 13),\t \n\tEDMA_ERR_LNK_CTRL_RX_1\t= (1 << 14),\t \n\tEDMA_ERR_LNK_CTRL_RX_2\t= (1 << 15),\t \n\tEDMA_ERR_LNK_CTRL_RX_3\t= (1 << 16),\t \n\n\tEDMA_ERR_LNK_DATA_RX\t= (0xf << 17),\t \n\n\tEDMA_ERR_LNK_CTRL_TX\t= (0x1f << 21),\t \n\tEDMA_ERR_LNK_CTRL_TX_0\t= (1 << 21),\t \n\tEDMA_ERR_LNK_CTRL_TX_1\t= (1 << 22),\t \n\tEDMA_ERR_LNK_CTRL_TX_2\t= (1 << 23),\t \n\tEDMA_ERR_LNK_CTRL_TX_3\t= (1 << 24),\t \n\tEDMA_ERR_LNK_CTRL_TX_4\t= (1 << 25),\t \n\n\tEDMA_ERR_LNK_DATA_TX\t= (0x1f << 26),\t \n\n\tEDMA_ERR_TRANS_PROTO\t= (1 << 31),\t \n\tEDMA_ERR_OVERRUN_5\t= (1 << 5),\n\tEDMA_ERR_UNDERRUN_5\t= (1 << 6),\n\n\tEDMA_ERR_IRQ_TRANSIENT  = EDMA_ERR_LNK_CTRL_RX_0 |\n\t\t\t\t  EDMA_ERR_LNK_CTRL_RX_1 |\n\t\t\t\t  EDMA_ERR_LNK_CTRL_RX_3 |\n\t\t\t\t  EDMA_ERR_LNK_CTRL_TX,\n\n\tEDMA_EH_FREEZE\t\t= EDMA_ERR_D_PAR |\n\t\t\t\t  EDMA_ERR_PRD_PAR |\n\t\t\t\t  EDMA_ERR_DEV_DCON |\n\t\t\t\t  EDMA_ERR_DEV_CON |\n\t\t\t\t  EDMA_ERR_SERR |\n\t\t\t\t  EDMA_ERR_SELF_DIS |\n\t\t\t\t  EDMA_ERR_CRQB_PAR |\n\t\t\t\t  EDMA_ERR_CRPB_PAR |\n\t\t\t\t  EDMA_ERR_INTRL_PAR |\n\t\t\t\t  EDMA_ERR_IORDY |\n\t\t\t\t  EDMA_ERR_LNK_CTRL_RX_2 |\n\t\t\t\t  EDMA_ERR_LNK_DATA_RX |\n\t\t\t\t  EDMA_ERR_LNK_DATA_TX |\n\t\t\t\t  EDMA_ERR_TRANS_PROTO,\n\n\tEDMA_EH_FREEZE_5\t= EDMA_ERR_D_PAR |\n\t\t\t\t  EDMA_ERR_PRD_PAR |\n\t\t\t\t  EDMA_ERR_DEV_DCON |\n\t\t\t\t  EDMA_ERR_DEV_CON |\n\t\t\t\t  EDMA_ERR_OVERRUN_5 |\n\t\t\t\t  EDMA_ERR_UNDERRUN_5 |\n\t\t\t\t  EDMA_ERR_SELF_DIS_5 |\n\t\t\t\t  EDMA_ERR_CRQB_PAR |\n\t\t\t\t  EDMA_ERR_CRPB_PAR |\n\t\t\t\t  EDMA_ERR_INTRL_PAR |\n\t\t\t\t  EDMA_ERR_IORDY,\n\n\tEDMA_REQ_Q_BASE_HI\t= 0x10,\n\tEDMA_REQ_Q_IN_PTR\t= 0x14,\t\t \n\n\tEDMA_REQ_Q_OUT_PTR\t= 0x18,\n\tEDMA_REQ_Q_PTR_SHIFT\t= 5,\n\n\tEDMA_RSP_Q_BASE_HI\t= 0x1c,\n\tEDMA_RSP_Q_IN_PTR\t= 0x20,\n\tEDMA_RSP_Q_OUT_PTR\t= 0x24,\t\t \n\tEDMA_RSP_Q_PTR_SHIFT\t= 3,\n\n\tEDMA_CMD\t\t= 0x28,\t\t \n\tEDMA_EN\t\t\t= (1 << 0),\t \n\tEDMA_DS\t\t\t= (1 << 1),\t \n\tEDMA_RESET\t\t= (1 << 2),\t \n\n\tEDMA_STATUS\t\t= 0x30,\t\t \n\tEDMA_STATUS_CACHE_EMPTY\t= (1 << 6),\t \n\tEDMA_STATUS_IDLE\t= (1 << 7),\t \n\n\tEDMA_IORDY_TMOUT\t= 0x34,\n\tEDMA_ARB_CFG\t\t= 0x38,\n\n\tEDMA_HALTCOND\t\t= 0x60,\t\t \n\tEDMA_UNKNOWN_RSVD\t= 0x6C,\t\t \n\n\tBMDMA_CMD\t\t= 0x224,\t \n\tBMDMA_STATUS\t\t= 0x228,\t \n\tBMDMA_PRD_LOW\t\t= 0x22c,\t \n\tBMDMA_PRD_HIGH\t\t= 0x230,\t \n\n\t \n\tMV_HP_FLAG_MSI\t\t= (1 << 0),\n\tMV_HP_ERRATA_50XXB0\t= (1 << 1),\n\tMV_HP_ERRATA_50XXB2\t= (1 << 2),\n\tMV_HP_ERRATA_60X1B2\t= (1 << 3),\n\tMV_HP_ERRATA_60X1C0\t= (1 << 4),\n\tMV_HP_GEN_I\t\t= (1 << 6),\t \n\tMV_HP_GEN_II\t\t= (1 << 7),\t \n\tMV_HP_GEN_IIE\t\t= (1 << 8),\t \n\tMV_HP_PCIE\t\t= (1 << 9),\t \n\tMV_HP_CUT_THROUGH\t= (1 << 10),\t \n\tMV_HP_FLAG_SOC\t\t= (1 << 11),\t \n\tMV_HP_QUIRK_LED_BLINK_EN = (1 << 12),\t \n\tMV_HP_FIX_LP_PHY_CTL\t= (1 << 13),\t \n\n\t \n\tMV_PP_FLAG_EDMA_EN\t= (1 << 0),\t \n\tMV_PP_FLAG_NCQ_EN\t= (1 << 1),\t \n\tMV_PP_FLAG_FBS_EN\t= (1 << 2),\t \n\tMV_PP_FLAG_DELAYED_EH\t= (1 << 3),\t \n\tMV_PP_FLAG_FAKE_ATA_BUSY = (1 << 4),\t \n};\n\n#define IS_GEN_I(hpriv) ((hpriv)->hp_flags & MV_HP_GEN_I)\n#define IS_GEN_II(hpriv) ((hpriv)->hp_flags & MV_HP_GEN_II)\n#define IS_GEN_IIE(hpriv) ((hpriv)->hp_flags & MV_HP_GEN_IIE)\n#define IS_PCIE(hpriv) ((hpriv)->hp_flags & MV_HP_PCIE)\n#define IS_SOC(hpriv) ((hpriv)->hp_flags & MV_HP_FLAG_SOC)\n\n#define WINDOW_CTRL(i)\t\t(0x20030 + ((i) << 4))\n#define WINDOW_BASE(i)\t\t(0x20034 + ((i) << 4))\n\nenum {\n\t \n\tMV_DMA_BOUNDARY\t\t= 0xffffU,\n\n\t \n\tEDMA_REQ_Q_BASE_LO_MASK\t= 0xfffffc00U,\n\n\t \n\tEDMA_RSP_Q_BASE_LO_MASK\t= 0xffffff00U,\n};\n\nenum chip_type {\n\tchip_504x,\n\tchip_508x,\n\tchip_5080,\n\tchip_604x,\n\tchip_608x,\n\tchip_6042,\n\tchip_7042,\n\tchip_soc,\n};\n\n \nstruct mv_crqb {\n\t__le32\t\t\tsg_addr;\n\t__le32\t\t\tsg_addr_hi;\n\t__le16\t\t\tctrl_flags;\n\t__le16\t\t\tata_cmd[11];\n};\n\nstruct mv_crqb_iie {\n\t__le32\t\t\taddr;\n\t__le32\t\t\taddr_hi;\n\t__le32\t\t\tflags;\n\t__le32\t\t\tlen;\n\t__le32\t\t\tata_cmd[4];\n};\n\n \nstruct mv_crpb {\n\t__le16\t\t\tid;\n\t__le16\t\t\tflags;\n\t__le32\t\t\ttmstmp;\n};\n\n \nstruct mv_sg {\n\t__le32\t\t\taddr;\n\t__le32\t\t\tflags_size;\n\t__le32\t\t\taddr_hi;\n\t__le32\t\t\treserved;\n};\n\n \nstruct mv_cached_regs {\n\tu32\t\t\tfiscfg;\n\tu32\t\t\tltmode;\n\tu32\t\t\thaltcond;\n\tu32\t\t\tunknown_rsvd;\n};\n\nstruct mv_port_priv {\n\tstruct mv_crqb\t\t*crqb;\n\tdma_addr_t\t\tcrqb_dma;\n\tstruct mv_crpb\t\t*crpb;\n\tdma_addr_t\t\tcrpb_dma;\n\tstruct mv_sg\t\t*sg_tbl[MV_MAX_Q_DEPTH];\n\tdma_addr_t\t\tsg_tbl_dma[MV_MAX_Q_DEPTH];\n\n\tunsigned int\t\treq_idx;\n\tunsigned int\t\tresp_idx;\n\n\tu32\t\t\tpp_flags;\n\tstruct mv_cached_regs\tcached;\n\tunsigned int\t\tdelayed_eh_pmp_map;\n};\n\nstruct mv_port_signal {\n\tu32\t\t\tamps;\n\tu32\t\t\tpre;\n};\n\nstruct mv_host_priv {\n\tu32\t\t\thp_flags;\n\tunsigned int \t\tboard_idx;\n\tu32\t\t\tmain_irq_mask;\n\tstruct mv_port_signal\tsignal[8];\n\tconst struct mv_hw_ops\t*ops;\n\tint\t\t\tn_ports;\n\tvoid __iomem\t\t*base;\n\tvoid __iomem\t\t*main_irq_cause_addr;\n\tvoid __iomem\t\t*main_irq_mask_addr;\n\tu32\t\t\tirq_cause_offset;\n\tu32\t\t\tirq_mask_offset;\n\tu32\t\t\tunmask_all_irqs;\n\n\t \n\tstruct clk\t\t*clk;\n\tstruct clk              **port_clks;\n\t \n\tstruct phy\t\t**port_phys;\n\t \n\tstruct dma_pool\t\t*crqb_pool;\n\tstruct dma_pool\t\t*crpb_pool;\n\tstruct dma_pool\t\t*sg_tbl_pool;\n};\n\nstruct mv_hw_ops {\n\tvoid (*phy_errata)(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t   unsigned int port);\n\tvoid (*enable_leds)(struct mv_host_priv *hpriv, void __iomem *mmio);\n\tvoid (*read_preamp)(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio);\n\tint (*reset_hc)(struct ata_host *host, void __iomem *mmio,\n\t\t\tunsigned int n_hc);\n\tvoid (*reset_flash)(struct mv_host_priv *hpriv, void __iomem *mmio);\n\tvoid (*reset_bus)(struct ata_host *host, void __iomem *mmio);\n};\n\nstatic int mv_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val);\nstatic int mv_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val);\nstatic int mv5_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val);\nstatic int mv5_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val);\nstatic int mv_port_start(struct ata_port *ap);\nstatic void mv_port_stop(struct ata_port *ap);\nstatic int mv_qc_defer(struct ata_queued_cmd *qc);\nstatic enum ata_completion_errors mv_qc_prep(struct ata_queued_cmd *qc);\nstatic enum ata_completion_errors mv_qc_prep_iie(struct ata_queued_cmd *qc);\nstatic unsigned int mv_qc_issue(struct ata_queued_cmd *qc);\nstatic int mv_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\tunsigned long deadline);\nstatic void mv_eh_freeze(struct ata_port *ap);\nstatic void mv_eh_thaw(struct ata_port *ap);\nstatic void mv6_dev_config(struct ata_device *dev);\n\nstatic void mv5_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t   unsigned int port);\nstatic void mv5_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio);\nstatic void mv5_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio);\nstatic int mv5_reset_hc(struct ata_host *host, void __iomem *mmio,\n\t\t\tunsigned int n_hc);\nstatic void mv5_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio);\nstatic void mv5_reset_bus(struct ata_host *host, void __iomem *mmio);\n\nstatic void mv6_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t   unsigned int port);\nstatic void mv6_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio);\nstatic void mv6_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio);\nstatic int mv6_reset_hc(struct ata_host *host, void __iomem *mmio,\n\t\t\tunsigned int n_hc);\nstatic void mv6_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio);\nstatic void mv_soc_enable_leds(struct mv_host_priv *hpriv,\n\t\t\t\t      void __iomem *mmio);\nstatic void mv_soc_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t\t      void __iomem *mmio);\nstatic int mv_soc_reset_hc(struct ata_host *host,\n\t\t\t\t  void __iomem *mmio, unsigned int n_hc);\nstatic void mv_soc_reset_flash(struct mv_host_priv *hpriv,\n\t\t\t\t      void __iomem *mmio);\nstatic void mv_soc_reset_bus(struct ata_host *host, void __iomem *mmio);\nstatic void mv_soc_65n_phy_errata(struct mv_host_priv *hpriv,\n\t\t\t\t  void __iomem *mmio, unsigned int port);\nstatic void mv_reset_pci_bus(struct ata_host *host, void __iomem *mmio);\nstatic void mv_reset_channel(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t     unsigned int port_no);\nstatic int mv_stop_edma(struct ata_port *ap);\nstatic int mv_stop_edma_engine(void __iomem *port_mmio);\nstatic void mv_edma_cfg(struct ata_port *ap, int want_ncq, int want_edma);\n\nstatic void mv_pmp_select(struct ata_port *ap, int pmp);\nstatic int mv_pmp_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\t\tunsigned long deadline);\nstatic int  mv_softreset(struct ata_link *link, unsigned int *class,\n\t\t\t\tunsigned long deadline);\nstatic void mv_pmp_error_handler(struct ata_port *ap);\nstatic void mv_process_crpb_entries(struct ata_port *ap,\n\t\t\t\t\tstruct mv_port_priv *pp);\n\nstatic void mv_sff_irq_clear(struct ata_port *ap);\nstatic int mv_check_atapi_dma(struct ata_queued_cmd *qc);\nstatic void mv_bmdma_setup(struct ata_queued_cmd *qc);\nstatic void mv_bmdma_start(struct ata_queued_cmd *qc);\nstatic void mv_bmdma_stop(struct ata_queued_cmd *qc);\nstatic u8   mv_bmdma_status(struct ata_port *ap);\nstatic u8 mv_sff_check_status(struct ata_port *ap);\n\n \n#ifdef CONFIG_PCI\nstatic const struct scsi_host_template mv5_sht = {\n\tATA_BASE_SHT(DRV_NAME),\n\t.sg_tablesize\t\t= MV_MAX_SG_CT / 2,\n\t.dma_boundary\t\t= MV_DMA_BOUNDARY,\n};\n#endif\nstatic const struct scsi_host_template mv6_sht = {\n\t__ATA_BASE_SHT(DRV_NAME),\n\t.can_queue\t\t= MV_MAX_Q_DEPTH - 1,\n\t.sg_tablesize\t\t= MV_MAX_SG_CT / 2,\n\t.dma_boundary\t\t= MV_DMA_BOUNDARY,\n\t.sdev_groups\t\t= ata_ncq_sdev_groups,\n\t.change_queue_depth\t= ata_scsi_change_queue_depth,\n\t.tag_alloc_policy\t= BLK_TAG_ALLOC_RR,\n\t.slave_configure\t= ata_scsi_slave_config\n};\n\nstatic struct ata_port_operations mv5_ops = {\n\t.inherits\t\t= &ata_sff_port_ops,\n\n\t.lost_interrupt\t\t= ATA_OP_NULL,\n\n\t.qc_defer\t\t= mv_qc_defer,\n\t.qc_prep\t\t= mv_qc_prep,\n\t.qc_issue\t\t= mv_qc_issue,\n\n\t.freeze\t\t\t= mv_eh_freeze,\n\t.thaw\t\t\t= mv_eh_thaw,\n\t.hardreset\t\t= mv_hardreset,\n\n\t.scr_read\t\t= mv5_scr_read,\n\t.scr_write\t\t= mv5_scr_write,\n\n\t.port_start\t\t= mv_port_start,\n\t.port_stop\t\t= mv_port_stop,\n};\n\nstatic struct ata_port_operations mv6_ops = {\n\t.inherits\t\t= &ata_bmdma_port_ops,\n\n\t.lost_interrupt\t\t= ATA_OP_NULL,\n\n\t.qc_defer\t\t= mv_qc_defer,\n\t.qc_prep\t\t= mv_qc_prep,\n\t.qc_issue\t\t= mv_qc_issue,\n\n\t.dev_config             = mv6_dev_config,\n\n\t.freeze\t\t\t= mv_eh_freeze,\n\t.thaw\t\t\t= mv_eh_thaw,\n\t.hardreset\t\t= mv_hardreset,\n\t.softreset\t\t= mv_softreset,\n\t.pmp_hardreset\t\t= mv_pmp_hardreset,\n\t.pmp_softreset\t\t= mv_softreset,\n\t.error_handler\t\t= mv_pmp_error_handler,\n\n\t.scr_read\t\t= mv_scr_read,\n\t.scr_write\t\t= mv_scr_write,\n\n\t.sff_check_status\t= mv_sff_check_status,\n\t.sff_irq_clear\t\t= mv_sff_irq_clear,\n\t.check_atapi_dma\t= mv_check_atapi_dma,\n\t.bmdma_setup\t\t= mv_bmdma_setup,\n\t.bmdma_start\t\t= mv_bmdma_start,\n\t.bmdma_stop\t\t= mv_bmdma_stop,\n\t.bmdma_status\t\t= mv_bmdma_status,\n\n\t.port_start\t\t= mv_port_start,\n\t.port_stop\t\t= mv_port_stop,\n};\n\nstatic struct ata_port_operations mv_iie_ops = {\n\t.inherits\t\t= &mv6_ops,\n\t.dev_config\t\t= ATA_OP_NULL,\n\t.qc_prep\t\t= mv_qc_prep_iie,\n};\n\nstatic const struct ata_port_info mv_port_info[] = {\n\t{   \n\t\t.flags\t\t= MV_GEN_I_FLAGS,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv5_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_I_FLAGS | MV_FLAG_DUAL_HC,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv5_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_I_FLAGS | MV_FLAG_DUAL_HC,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv5_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_II_FLAGS,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv6_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_II_FLAGS | MV_FLAG_DUAL_HC,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv6_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_IIE_FLAGS,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv_iie_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_IIE_FLAGS,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv_iie_ops,\n\t},\n\t{   \n\t\t.flags\t\t= MV_GEN_IIE_FLAGS,\n\t\t.pio_mask\t= ATA_PIO4,\n\t\t.udma_mask\t= ATA_UDMA6,\n\t\t.port_ops\t= &mv_iie_ops,\n\t},\n};\n\nstatic const struct pci_device_id mv_pci_tbl[] = {\n\t{ PCI_VDEVICE(MARVELL, 0x5040), chip_504x },\n\t{ PCI_VDEVICE(MARVELL, 0x5041), chip_504x },\n\t{ PCI_VDEVICE(MARVELL, 0x5080), chip_5080 },\n\t{ PCI_VDEVICE(MARVELL, 0x5081), chip_508x },\n\t \n\t{ PCI_VDEVICE(TTI, 0x1720), chip_6042 },\n\t{ PCI_VDEVICE(TTI, 0x1740), chip_6042 },\n\t{ PCI_VDEVICE(TTI, 0x1742), chip_6042 },\n\n\t{ PCI_VDEVICE(MARVELL, 0x6040), chip_604x },\n\t{ PCI_VDEVICE(MARVELL, 0x6041), chip_604x },\n\t{ PCI_VDEVICE(MARVELL, 0x6042), chip_6042 },\n\t{ PCI_VDEVICE(MARVELL, 0x6080), chip_608x },\n\t{ PCI_VDEVICE(MARVELL, 0x6081), chip_608x },\n\n\t{ PCI_VDEVICE(ADAPTEC2, 0x0241), chip_604x },\n\n\t \n\t{ PCI_VDEVICE(ADAPTEC2, 0x0243), chip_7042 },\n\n\t \n\t{ PCI_VDEVICE(MARVELL, 0x7042), chip_7042 },\n\n\t \n\t{ PCI_VDEVICE(TTI, 0x2300), chip_7042 },\n\t{ PCI_VDEVICE(TTI, 0x2310), chip_7042 },\n\n\t{ }\t\t\t \n};\n\nstatic const struct mv_hw_ops mv5xxx_ops = {\n\t.phy_errata\t\t= mv5_phy_errata,\n\t.enable_leds\t\t= mv5_enable_leds,\n\t.read_preamp\t\t= mv5_read_preamp,\n\t.reset_hc\t\t= mv5_reset_hc,\n\t.reset_flash\t\t= mv5_reset_flash,\n\t.reset_bus\t\t= mv5_reset_bus,\n};\n\nstatic const struct mv_hw_ops mv6xxx_ops = {\n\t.phy_errata\t\t= mv6_phy_errata,\n\t.enable_leds\t\t= mv6_enable_leds,\n\t.read_preamp\t\t= mv6_read_preamp,\n\t.reset_hc\t\t= mv6_reset_hc,\n\t.reset_flash\t\t= mv6_reset_flash,\n\t.reset_bus\t\t= mv_reset_pci_bus,\n};\n\nstatic const struct mv_hw_ops mv_soc_ops = {\n\t.phy_errata\t\t= mv6_phy_errata,\n\t.enable_leds\t\t= mv_soc_enable_leds,\n\t.read_preamp\t\t= mv_soc_read_preamp,\n\t.reset_hc\t\t= mv_soc_reset_hc,\n\t.reset_flash\t\t= mv_soc_reset_flash,\n\t.reset_bus\t\t= mv_soc_reset_bus,\n};\n\nstatic const struct mv_hw_ops mv_soc_65n_ops = {\n\t.phy_errata\t\t= mv_soc_65n_phy_errata,\n\t.enable_leds\t\t= mv_soc_enable_leds,\n\t.reset_hc\t\t= mv_soc_reset_hc,\n\t.reset_flash\t\t= mv_soc_reset_flash,\n\t.reset_bus\t\t= mv_soc_reset_bus,\n};\n\n \n\nstatic inline void writelfl(unsigned long data, void __iomem *addr)\n{\n\twritel(data, addr);\n\t(void) readl(addr);\t \n}\n\nstatic inline unsigned int mv_hc_from_port(unsigned int port)\n{\n\treturn port >> MV_PORT_HC_SHIFT;\n}\n\nstatic inline unsigned int mv_hardport_from_port(unsigned int port)\n{\n\treturn port & MV_PORT_MASK;\n}\n\n \n#define MV_PORT_TO_SHIFT_AND_HARDPORT(port, shift, hardport)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tshift    = mv_hc_from_port(port) * HC_SHIFT;\t\t\\\n\thardport = mv_hardport_from_port(port);\t\t\t\\\n\tshift   += hardport * 2;\t\t\t\t\\\n}\n\nstatic inline void __iomem *mv_hc_base(void __iomem *base, unsigned int hc)\n{\n\treturn (base + SATAHC0_REG_BASE + (hc * MV_SATAHC_REG_SZ));\n}\n\nstatic inline void __iomem *mv_hc_base_from_port(void __iomem *base,\n\t\t\t\t\t\t unsigned int port)\n{\n\treturn mv_hc_base(base, mv_hc_from_port(port));\n}\n\nstatic inline void __iomem *mv_port_base(void __iomem *base, unsigned int port)\n{\n\treturn  mv_hc_base_from_port(base, port) +\n\t\tMV_SATAHC_ARBTR_REG_SZ +\n\t\t(mv_hardport_from_port(port) * MV_PORT_REG_SZ);\n}\n\nstatic void __iomem *mv5_phy_base(void __iomem *mmio, unsigned int port)\n{\n\tvoid __iomem *hc_mmio = mv_hc_base_from_port(mmio, port);\n\tunsigned long ofs = (mv_hardport_from_port(port) + 1) * 0x100UL;\n\n\treturn hc_mmio + ofs;\n}\n\nstatic inline void __iomem *mv_host_base(struct ata_host *host)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\treturn hpriv->base;\n}\n\nstatic inline void __iomem *mv_ap_base(struct ata_port *ap)\n{\n\treturn mv_port_base(mv_host_base(ap->host), ap->port_no);\n}\n\nstatic inline int mv_get_hc_count(unsigned long port_flags)\n{\n\treturn ((port_flags & MV_FLAG_DUAL_HC) ? 2 : 1);\n}\n\n \nstatic void mv_save_cached_regs(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\tpp->cached.fiscfg = readl(port_mmio + FISCFG);\n\tpp->cached.ltmode = readl(port_mmio + LTMODE);\n\tpp->cached.haltcond = readl(port_mmio + EDMA_HALTCOND);\n\tpp->cached.unknown_rsvd = readl(port_mmio + EDMA_UNKNOWN_RSVD);\n}\n\n \nstatic inline void mv_write_cached_reg(void __iomem *addr, u32 *old, u32 new)\n{\n\tif (new != *old) {\n\t\tunsigned long laddr;\n\t\t*old = new;\n\t\t \n\t\tladdr = (unsigned long)addr & 0xffff;\n\t\tif (laddr >= 0x300 && laddr <= 0x33c) {\n\t\t\tladdr &= 0x000f;\n\t\t\tif (laddr == 0x4 || laddr == 0xc) {\n\t\t\t\twritelfl(new, addr);  \n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\twritel(new, addr);  \n\t}\n}\n\nstatic void mv_set_edma_ptrs(void __iomem *port_mmio,\n\t\t\t     struct mv_host_priv *hpriv,\n\t\t\t     struct mv_port_priv *pp)\n{\n\tu32 index;\n\n\t \n\tpp->req_idx &= MV_MAX_Q_DEPTH_MASK;\t \n\tindex = pp->req_idx << EDMA_REQ_Q_PTR_SHIFT;\n\n\tWARN_ON(pp->crqb_dma & 0x3ff);\n\twritel((pp->crqb_dma >> 16) >> 16, port_mmio + EDMA_REQ_Q_BASE_HI);\n\twritelfl((pp->crqb_dma & EDMA_REQ_Q_BASE_LO_MASK) | index,\n\t\t port_mmio + EDMA_REQ_Q_IN_PTR);\n\twritelfl(index, port_mmio + EDMA_REQ_Q_OUT_PTR);\n\n\t \n\tpp->resp_idx &= MV_MAX_Q_DEPTH_MASK;\t \n\tindex = pp->resp_idx << EDMA_RSP_Q_PTR_SHIFT;\n\n\tWARN_ON(pp->crpb_dma & 0xff);\n\twritel((pp->crpb_dma >> 16) >> 16, port_mmio + EDMA_RSP_Q_BASE_HI);\n\twritelfl(index, port_mmio + EDMA_RSP_Q_IN_PTR);\n\twritelfl((pp->crpb_dma & EDMA_RSP_Q_BASE_LO_MASK) | index,\n\t\t port_mmio + EDMA_RSP_Q_OUT_PTR);\n}\n\nstatic void mv_write_main_irq_mask(u32 mask, struct mv_host_priv *hpriv)\n{\n\t \n\tif (mask & (ALL_PORTS_COAL_DONE | PORTS_0_3_COAL_DONE))\n\t\tmask &= ~DONE_IRQ_0_3;\n\tif (mask & (ALL_PORTS_COAL_DONE | PORTS_4_7_COAL_DONE))\n\t\tmask &= ~DONE_IRQ_4_7;\n\twritelfl(mask, hpriv->main_irq_mask_addr);\n}\n\nstatic void mv_set_main_irq_mask(struct ata_host *host,\n\t\t\t\t u32 disable_bits, u32 enable_bits)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tu32 old_mask, new_mask;\n\n\told_mask = hpriv->main_irq_mask;\n\tnew_mask = (old_mask & ~disable_bits) | enable_bits;\n\tif (new_mask != old_mask) {\n\t\thpriv->main_irq_mask = new_mask;\n\t\tmv_write_main_irq_mask(new_mask, hpriv);\n\t}\n}\n\nstatic void mv_enable_port_irqs(struct ata_port *ap,\n\t\t\t\t     unsigned int port_bits)\n{\n\tunsigned int shift, hardport, port = ap->port_no;\n\tu32 disable_bits, enable_bits;\n\n\tMV_PORT_TO_SHIFT_AND_HARDPORT(port, shift, hardport);\n\n\tdisable_bits = (DONE_IRQ | ERR_IRQ) << shift;\n\tenable_bits  = port_bits << shift;\n\tmv_set_main_irq_mask(ap->host, disable_bits, enable_bits);\n}\n\nstatic void mv_clear_and_enable_port_irqs(struct ata_port *ap,\n\t\t\t\t\t  void __iomem *port_mmio,\n\t\t\t\t\t  unsigned int port_irqs)\n{\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tint hardport = mv_hardport_from_port(ap->port_no);\n\tvoid __iomem *hc_mmio = mv_hc_base_from_port(\n\t\t\t\tmv_host_base(ap->host), ap->port_no);\n\tu32 hc_irq_cause;\n\n\t \n\twritelfl(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\n\n\t \n\thc_irq_cause = ~((DEV_IRQ | DMA_IRQ) << hardport);\n\twritelfl(hc_irq_cause, hc_mmio + HC_IRQ_CAUSE);\n\n\t \n\tif (IS_GEN_IIE(hpriv))\n\t\twritelfl(0, port_mmio + FIS_IRQ_CAUSE);\n\n\tmv_enable_port_irqs(ap, port_irqs);\n}\n\nstatic void mv_set_irq_coalescing(struct ata_host *host,\n\t\t\t\t  unsigned int count, unsigned int usecs)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base, *hc_mmio;\n\tu32 coal_enable = 0;\n\tunsigned long flags;\n\tunsigned int clks, is_dual_hc = hpriv->n_ports > MV_PORTS_PER_HC;\n\tconst u32 coal_disable = PORTS_0_3_COAL_DONE | PORTS_4_7_COAL_DONE |\n\t\t\t\t\t\t\tALL_PORTS_COAL_DONE;\n\n\t \n\tif (!usecs || !count) {\n\t\tclks = count = 0;\n\t} else {\n\t\t \n\t\tclks = usecs * COAL_CLOCKS_PER_USEC;\n\t\tif (clks > MAX_COAL_TIME_THRESHOLD)\n\t\t\tclks = MAX_COAL_TIME_THRESHOLD;\n\t\tif (count > MAX_COAL_IO_COUNT)\n\t\t\tcount = MAX_COAL_IO_COUNT;\n\t}\n\n\tspin_lock_irqsave(&host->lock, flags);\n\tmv_set_main_irq_mask(host, coal_disable, 0);\n\n\tif (is_dual_hc && !IS_GEN_I(hpriv)) {\n\t\t \n\t\twritel(clks,  mmio + IRQ_COAL_TIME_THRESHOLD);\n\t\twritel(count, mmio + IRQ_COAL_IO_THRESHOLD);\n\t\t \n\t\twritel(~ALL_PORTS_COAL_IRQ, mmio + IRQ_COAL_CAUSE);\n\t\tif (count)\n\t\t\tcoal_enable = ALL_PORTS_COAL_DONE;\n\t\tclks = count = 0;  \n\t}\n\n\t \n\thc_mmio = mv_hc_base_from_port(mmio, 0);\n\twritel(clks,  hc_mmio + HC_IRQ_COAL_TIME_THRESHOLD);\n\twritel(count, hc_mmio + HC_IRQ_COAL_IO_THRESHOLD);\n\twritel(~HC_COAL_IRQ, hc_mmio + HC_IRQ_CAUSE);\n\tif (count)\n\t\tcoal_enable |= PORTS_0_3_COAL_DONE;\n\tif (is_dual_hc) {\n\t\thc_mmio = mv_hc_base_from_port(mmio, MV_PORTS_PER_HC);\n\t\twritel(clks,  hc_mmio + HC_IRQ_COAL_TIME_THRESHOLD);\n\t\twritel(count, hc_mmio + HC_IRQ_COAL_IO_THRESHOLD);\n\t\twritel(~HC_COAL_IRQ, hc_mmio + HC_IRQ_CAUSE);\n\t\tif (count)\n\t\t\tcoal_enable |= PORTS_4_7_COAL_DONE;\n\t}\n\n\tmv_set_main_irq_mask(host, 0, coal_enable);\n\tspin_unlock_irqrestore(&host->lock, flags);\n}\n\n \nstatic void mv_start_edma(struct ata_port *ap, void __iomem *port_mmio,\n\t\t\t struct mv_port_priv *pp, u8 protocol)\n{\n\tint want_ncq = (protocol == ATA_PROT_NCQ);\n\n\tif (pp->pp_flags & MV_PP_FLAG_EDMA_EN) {\n\t\tint using_ncq = ((pp->pp_flags & MV_PP_FLAG_NCQ_EN) != 0);\n\t\tif (want_ncq != using_ncq)\n\t\t\tmv_stop_edma(ap);\n\t}\n\tif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN)) {\n\t\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\n\t\tmv_edma_cfg(ap, want_ncq, 1);\n\n\t\tmv_set_edma_ptrs(port_mmio, hpriv, pp);\n\t\tmv_clear_and_enable_port_irqs(ap, port_mmio, DONE_IRQ|ERR_IRQ);\n\n\t\twritelfl(EDMA_EN, port_mmio + EDMA_CMD);\n\t\tpp->pp_flags |= MV_PP_FLAG_EDMA_EN;\n\t}\n}\n\nstatic void mv_wait_for_edma_empty_idle(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tconst u32 empty_idle = (EDMA_STATUS_CACHE_EMPTY | EDMA_STATUS_IDLE);\n\tconst int per_loop = 5, timeout = (15 * 1000 / per_loop);\n\tint i;\n\n\t \n\tfor (i = 0; i < timeout; ++i) {\n\t\tu32 edma_stat = readl(port_mmio + EDMA_STATUS);\n\t\tif ((edma_stat & empty_idle) == empty_idle)\n\t\t\tbreak;\n\t\tudelay(per_loop);\n\t}\n\t \n}\n\n \nstatic int mv_stop_edma_engine(void __iomem *port_mmio)\n{\n\tint i;\n\n\t \n\twritelfl(EDMA_DS, port_mmio + EDMA_CMD);\n\n\t \n\tfor (i = 10000; i > 0; i--) {\n\t\tu32 reg = readl(port_mmio + EDMA_CMD);\n\t\tif (!(reg & EDMA_EN))\n\t\t\treturn 0;\n\t\tudelay(10);\n\t}\n\treturn -EIO;\n}\n\nstatic int mv_stop_edma(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tstruct mv_port_priv *pp = ap->private_data;\n\tint err = 0;\n\n\tif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN))\n\t\treturn 0;\n\tpp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\n\tmv_wait_for_edma_empty_idle(ap);\n\tif (mv_stop_edma_engine(port_mmio)) {\n\t\tata_port_err(ap, \"Unable to stop eDMA\\n\");\n\t\terr = -EIO;\n\t}\n\tmv_edma_cfg(ap, 0, 0);\n\treturn err;\n}\n\nstatic void mv_dump_mem(struct device *dev, void __iomem *start, unsigned bytes)\n{\n\tint b, w, o;\n\tunsigned char linebuf[38];\n\n\tfor (b = 0; b < bytes; ) {\n\t\tfor (w = 0, o = 0; b < bytes && w < 4; w++) {\n\t\t\to += scnprintf(linebuf + o, sizeof(linebuf) - o,\n\t\t\t\t       \"%08x \", readl(start + b));\n\t\t\tb += sizeof(u32);\n\t\t}\n\t\tdev_dbg(dev, \"%s: %p: %s\\n\",\n\t\t\t__func__, start + b, linebuf);\n\t}\n}\n\nstatic void mv_dump_pci_cfg(struct pci_dev *pdev, unsigned bytes)\n{\n\tint b, w, o;\n\tu32 dw = 0;\n\tunsigned char linebuf[38];\n\n\tfor (b = 0; b < bytes; ) {\n\t\tfor (w = 0, o = 0; b < bytes && w < 4; w++) {\n\t\t\t(void) pci_read_config_dword(pdev, b, &dw);\n\t\t\to += snprintf(linebuf + o, sizeof(linebuf) - o,\n\t\t\t\t      \"%08x \", dw);\n\t\t\tb += sizeof(u32);\n\t\t}\n\t\tdev_dbg(&pdev->dev, \"%s: %02x: %s\\n\",\n\t\t\t__func__, b, linebuf);\n\t}\n}\n\nstatic void mv_dump_all_regs(void __iomem *mmio_base,\n\t\t\t     struct pci_dev *pdev)\n{\n\tvoid __iomem *hc_base;\n\tvoid __iomem *port_base;\n\tint start_port, num_ports, p, start_hc, num_hcs, hc;\n\n\tstart_hc = start_port = 0;\n\tnum_ports = 8;\t\t \n\tnum_hcs = 2;\n\tdev_dbg(&pdev->dev,\n\t\t\"%s: All registers for port(s) %u-%u:\\n\", __func__,\n\t\tstart_port, num_ports > 1 ? num_ports - 1 : start_port);\n\n\tdev_dbg(&pdev->dev, \"%s: PCI config space regs:\\n\", __func__);\n\tmv_dump_pci_cfg(pdev, 0x68);\n\n\tdev_dbg(&pdev->dev, \"%s: PCI regs:\\n\", __func__);\n\tmv_dump_mem(&pdev->dev, mmio_base+0xc00, 0x3c);\n\tmv_dump_mem(&pdev->dev, mmio_base+0xd00, 0x34);\n\tmv_dump_mem(&pdev->dev, mmio_base+0xf00, 0x4);\n\tmv_dump_mem(&pdev->dev, mmio_base+0x1d00, 0x6c);\n\tfor (hc = start_hc; hc < start_hc + num_hcs; hc++) {\n\t\thc_base = mv_hc_base(mmio_base, hc);\n\t\tdev_dbg(&pdev->dev, \"%s: HC regs (HC %i):\\n\", __func__, hc);\n\t\tmv_dump_mem(&pdev->dev, hc_base, 0x1c);\n\t}\n\tfor (p = start_port; p < start_port + num_ports; p++) {\n\t\tport_base = mv_port_base(mmio_base, p);\n\t\tdev_dbg(&pdev->dev, \"%s: EDMA regs (port %i):\\n\", __func__, p);\n\t\tmv_dump_mem(&pdev->dev, port_base, 0x54);\n\t\tdev_dbg(&pdev->dev, \"%s: SATA regs (port %i):\\n\", __func__, p);\n\t\tmv_dump_mem(&pdev->dev, port_base+0x300, 0x60);\n\t}\n}\n\nstatic unsigned int mv_scr_offset(unsigned int sc_reg_in)\n{\n\tunsigned int ofs;\n\n\tswitch (sc_reg_in) {\n\tcase SCR_STATUS:\n\tcase SCR_CONTROL:\n\tcase SCR_ERROR:\n\t\tofs = SATA_STATUS + (sc_reg_in * sizeof(u32));\n\t\tbreak;\n\tcase SCR_ACTIVE:\n\t\tofs = SATA_ACTIVE;    \n\t\tbreak;\n\tdefault:\n\t\tofs = 0xffffffffU;\n\t\tbreak;\n\t}\n\treturn ofs;\n}\n\nstatic int mv_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val)\n{\n\tunsigned int ofs = mv_scr_offset(sc_reg_in);\n\n\tif (ofs != 0xffffffffU) {\n\t\t*val = readl(mv_ap_base(link->ap) + ofs);\n\t\treturn 0;\n\t} else\n\t\treturn -EINVAL;\n}\n\nstatic int mv_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val)\n{\n\tunsigned int ofs = mv_scr_offset(sc_reg_in);\n\n\tif (ofs != 0xffffffffU) {\n\t\tvoid __iomem *addr = mv_ap_base(link->ap) + ofs;\n\t\tstruct mv_host_priv *hpriv = link->ap->host->private_data;\n\t\tif (sc_reg_in == SCR_CONTROL) {\n\t\t\t \n\t\t\tif ((val & 0xf) == 1 || (readl(addr) & 0xf) == 1)\n\t\t\t\tval |= 0xf000;\n\n\t\t\tif (hpriv->hp_flags & MV_HP_FIX_LP_PHY_CTL) {\n\t\t\t\tvoid __iomem *lp_phy_addr =\n\t\t\t\t\tmv_ap_base(link->ap) + LP_PHY_CTL;\n\t\t\t\t \n\t\t\t\tu32 lp_phy_val =\n\t\t\t\t\tLP_PHY_CTL_PIN_PU_PLL |\n\t\t\t\t\tLP_PHY_CTL_PIN_PU_RX  |\n\t\t\t\t\tLP_PHY_CTL_PIN_PU_TX;\n\n\t\t\t\tif ((val & 0xf0) != 0x10)\n\t\t\t\t\tlp_phy_val |=\n\t\t\t\t\t\tLP_PHY_CTL_GEN_TX_3G |\n\t\t\t\t\t\tLP_PHY_CTL_GEN_RX_3G;\n\n\t\t\t\twritelfl(lp_phy_val, lp_phy_addr);\n\t\t\t}\n\t\t}\n\t\twritelfl(val, addr);\n\t\treturn 0;\n\t} else\n\t\treturn -EINVAL;\n}\n\nstatic void mv6_dev_config(struct ata_device *adev)\n{\n\t \n\tif (adev->flags & ATA_DFLAG_NCQ) {\n\t\tif (sata_pmp_attached(adev->link->ap)) {\n\t\t\tadev->flags &= ~ATA_DFLAG_NCQ;\n\t\t\tata_dev_info(adev,\n\t\t\t\t\"NCQ disabled for command-based switching\\n\");\n\t\t}\n\t}\n}\n\nstatic int mv_qc_defer(struct ata_queued_cmd *qc)\n{\n\tstruct ata_link *link = qc->dev->link;\n\tstruct ata_port *ap = link->ap;\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\t \n\tif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH)\n\t\treturn ATA_DEFER_PORT;\n\n\t \n\tif (unlikely(ap->excl_link)) {\n\t\tif (link == ap->excl_link) {\n\t\t\tif (ap->nr_active_links)\n\t\t\t\treturn ATA_DEFER_PORT;\n\t\t\tqc->flags |= ATA_QCFLAG_CLEAR_EXCL;\n\t\t\treturn 0;\n\t\t} else\n\t\t\treturn ATA_DEFER_PORT;\n\t}\n\n\t \n\tif (ap->nr_active_links == 0)\n\t\treturn 0;\n\n\t \n\tif ((pp->pp_flags & MV_PP_FLAG_EDMA_EN) &&\n\t    (pp->pp_flags & MV_PP_FLAG_NCQ_EN)) {\n\t\tif (ata_is_ncq(qc->tf.protocol))\n\t\t\treturn 0;\n\t\telse {\n\t\t\tap->excl_link = link;\n\t\t\treturn ATA_DEFER_PORT;\n\t\t}\n\t}\n\n\treturn ATA_DEFER_PORT;\n}\n\nstatic void mv_config_fbs(struct ata_port *ap, int want_ncq, int want_fbs)\n{\n\tstruct mv_port_priv *pp = ap->private_data;\n\tvoid __iomem *port_mmio;\n\n\tu32 fiscfg,   *old_fiscfg   = &pp->cached.fiscfg;\n\tu32 ltmode,   *old_ltmode   = &pp->cached.ltmode;\n\tu32 haltcond, *old_haltcond = &pp->cached.haltcond;\n\n\tltmode   = *old_ltmode & ~LTMODE_BIT8;\n\thaltcond = *old_haltcond | EDMA_ERR_DEV;\n\n\tif (want_fbs) {\n\t\tfiscfg = *old_fiscfg | FISCFG_SINGLE_SYNC;\n\t\tltmode = *old_ltmode | LTMODE_BIT8;\n\t\tif (want_ncq)\n\t\t\thaltcond &= ~EDMA_ERR_DEV;\n\t\telse\n\t\t\tfiscfg |=  FISCFG_WAIT_DEV_ERR;\n\t} else {\n\t\tfiscfg = *old_fiscfg & ~(FISCFG_SINGLE_SYNC | FISCFG_WAIT_DEV_ERR);\n\t}\n\n\tport_mmio = mv_ap_base(ap);\n\tmv_write_cached_reg(port_mmio + FISCFG, old_fiscfg, fiscfg);\n\tmv_write_cached_reg(port_mmio + LTMODE, old_ltmode, ltmode);\n\tmv_write_cached_reg(port_mmio + EDMA_HALTCOND, old_haltcond, haltcond);\n}\n\nstatic void mv_60x1_errata_sata25(struct ata_port *ap, int want_ncq)\n{\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tu32 old, new;\n\n\t \n\told = readl(hpriv->base + GPIO_PORT_CTL);\n\tif (want_ncq)\n\t\tnew = old | (1 << 22);\n\telse\n\t\tnew = old & ~(1 << 22);\n\tif (new != old)\n\t\twritel(new, hpriv->base + GPIO_PORT_CTL);\n}\n\n \nstatic void mv_bmdma_enable_iie(struct ata_port *ap, int enable_bmdma)\n{\n\tstruct mv_port_priv *pp = ap->private_data;\n\tu32 new, *old = &pp->cached.unknown_rsvd;\n\n\tif (enable_bmdma)\n\t\tnew = *old | 1;\n\telse\n\t\tnew = *old & ~1;\n\tmv_write_cached_reg(mv_ap_base(ap) + EDMA_UNKNOWN_RSVD, old, new);\n}\n\n \nstatic void mv_soc_led_blink_enable(struct ata_port *ap)\n{\n\tstruct ata_host *host = ap->host;\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *hc_mmio;\n\tu32 led_ctrl;\n\n\tif (hpriv->hp_flags & MV_HP_QUIRK_LED_BLINK_EN)\n\t\treturn;\n\thpriv->hp_flags |= MV_HP_QUIRK_LED_BLINK_EN;\n\thc_mmio = mv_hc_base_from_port(mv_host_base(host), ap->port_no);\n\tled_ctrl = readl(hc_mmio + SOC_LED_CTRL);\n\twritel(led_ctrl | SOC_LED_CTRL_BLINK, hc_mmio + SOC_LED_CTRL);\n}\n\nstatic void mv_soc_led_blink_disable(struct ata_port *ap)\n{\n\tstruct ata_host *host = ap->host;\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *hc_mmio;\n\tu32 led_ctrl;\n\tunsigned int port;\n\n\tif (!(hpriv->hp_flags & MV_HP_QUIRK_LED_BLINK_EN))\n\t\treturn;\n\n\t \n\tfor (port = 0; port < hpriv->n_ports; port++) {\n\t\tstruct ata_port *this_ap = host->ports[port];\n\t\tstruct mv_port_priv *pp = this_ap->private_data;\n\n\t\tif (pp->pp_flags & MV_PP_FLAG_NCQ_EN)\n\t\t\treturn;\n\t}\n\n\thpriv->hp_flags &= ~MV_HP_QUIRK_LED_BLINK_EN;\n\thc_mmio = mv_hc_base_from_port(mv_host_base(host), ap->port_no);\n\tled_ctrl = readl(hc_mmio + SOC_LED_CTRL);\n\twritel(led_ctrl & ~SOC_LED_CTRL_BLINK, hc_mmio + SOC_LED_CTRL);\n}\n\nstatic void mv_edma_cfg(struct ata_port *ap, int want_ncq, int want_edma)\n{\n\tu32 cfg;\n\tstruct mv_port_priv *pp    = ap->private_data;\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tvoid __iomem *port_mmio    = mv_ap_base(ap);\n\n\t \n\tcfg = EDMA_CFG_Q_DEPTH;\t\t \n\tpp->pp_flags &=\n\t  ~(MV_PP_FLAG_FBS_EN | MV_PP_FLAG_NCQ_EN | MV_PP_FLAG_FAKE_ATA_BUSY);\n\n\tif (IS_GEN_I(hpriv))\n\t\tcfg |= (1 << 8);\t \n\n\telse if (IS_GEN_II(hpriv)) {\n\t\tcfg |= EDMA_CFG_RD_BRST_EXT | EDMA_CFG_WR_BUFF_LEN;\n\t\tmv_60x1_errata_sata25(ap, want_ncq);\n\n\t} else if (IS_GEN_IIE(hpriv)) {\n\t\tint want_fbs = sata_pmp_attached(ap);\n\t\t \n\t\twant_fbs &= want_ncq;\n\n\t\tmv_config_fbs(ap, want_ncq, want_fbs);\n\n\t\tif (want_fbs) {\n\t\t\tpp->pp_flags |= MV_PP_FLAG_FBS_EN;\n\t\t\tcfg |= EDMA_CFG_EDMA_FBS;  \n\t\t}\n\n\t\tcfg |= (1 << 23);\t \n\t\tif (want_edma) {\n\t\t\tcfg |= (1 << 22);  \n\t\t\tif (!IS_SOC(hpriv))\n\t\t\t\tcfg |= (1 << 18);  \n\t\t}\n\t\tif (hpriv->hp_flags & MV_HP_CUT_THROUGH)\n\t\t\tcfg |= (1 << 17);  \n\t\tmv_bmdma_enable_iie(ap, !want_edma);\n\n\t\tif (IS_SOC(hpriv)) {\n\t\t\tif (want_ncq)\n\t\t\t\tmv_soc_led_blink_enable(ap);\n\t\t\telse\n\t\t\t\tmv_soc_led_blink_disable(ap);\n\t\t}\n\t}\n\n\tif (want_ncq) {\n\t\tcfg |= EDMA_CFG_NCQ;\n\t\tpp->pp_flags |=  MV_PP_FLAG_NCQ_EN;\n\t}\n\n\twritelfl(cfg, port_mmio + EDMA_CFG);\n}\n\nstatic void mv_port_free_dma_mem(struct ata_port *ap)\n{\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tstruct mv_port_priv *pp = ap->private_data;\n\tint tag;\n\n\tif (pp->crqb) {\n\t\tdma_pool_free(hpriv->crqb_pool, pp->crqb, pp->crqb_dma);\n\t\tpp->crqb = NULL;\n\t}\n\tif (pp->crpb) {\n\t\tdma_pool_free(hpriv->crpb_pool, pp->crpb, pp->crpb_dma);\n\t\tpp->crpb = NULL;\n\t}\n\t \n\tfor (tag = 0; tag < MV_MAX_Q_DEPTH; ++tag) {\n\t\tif (pp->sg_tbl[tag]) {\n\t\t\tif (tag == 0 || !IS_GEN_I(hpriv))\n\t\t\t\tdma_pool_free(hpriv->sg_tbl_pool,\n\t\t\t\t\t      pp->sg_tbl[tag],\n\t\t\t\t\t      pp->sg_tbl_dma[tag]);\n\t\t\tpp->sg_tbl[tag] = NULL;\n\t\t}\n\t}\n}\n\n \nstatic int mv_port_start(struct ata_port *ap)\n{\n\tstruct device *dev = ap->host->dev;\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tstruct mv_port_priv *pp;\n\tunsigned long flags;\n\tint tag;\n\n\tpp = devm_kzalloc(dev, sizeof(*pp), GFP_KERNEL);\n\tif (!pp)\n\t\treturn -ENOMEM;\n\tap->private_data = pp;\n\n\tpp->crqb = dma_pool_zalloc(hpriv->crqb_pool, GFP_KERNEL, &pp->crqb_dma);\n\tif (!pp->crqb)\n\t\treturn -ENOMEM;\n\n\tpp->crpb = dma_pool_zalloc(hpriv->crpb_pool, GFP_KERNEL, &pp->crpb_dma);\n\tif (!pp->crpb)\n\t\tgoto out_port_free_dma_mem;\n\n\t \n\tif (hpriv->hp_flags & MV_HP_ERRATA_60X1C0)\n\t\tap->flags |= ATA_FLAG_AN;\n\t \n\tfor (tag = 0; tag < MV_MAX_Q_DEPTH; ++tag) {\n\t\tif (tag == 0 || !IS_GEN_I(hpriv)) {\n\t\t\tpp->sg_tbl[tag] = dma_pool_alloc(hpriv->sg_tbl_pool,\n\t\t\t\t\t      GFP_KERNEL, &pp->sg_tbl_dma[tag]);\n\t\t\tif (!pp->sg_tbl[tag])\n\t\t\t\tgoto out_port_free_dma_mem;\n\t\t} else {\n\t\t\tpp->sg_tbl[tag]     = pp->sg_tbl[0];\n\t\t\tpp->sg_tbl_dma[tag] = pp->sg_tbl_dma[0];\n\t\t}\n\t}\n\n\tspin_lock_irqsave(ap->lock, flags);\n\tmv_save_cached_regs(ap);\n\tmv_edma_cfg(ap, 0, 0);\n\tspin_unlock_irqrestore(ap->lock, flags);\n\n\treturn 0;\n\nout_port_free_dma_mem:\n\tmv_port_free_dma_mem(ap);\n\treturn -ENOMEM;\n}\n\n \nstatic void mv_port_stop(struct ata_port *ap)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(ap->lock, flags);\n\tmv_stop_edma(ap);\n\tmv_enable_port_irqs(ap, 0);\n\tspin_unlock_irqrestore(ap->lock, flags);\n\tmv_port_free_dma_mem(ap);\n}\n\n \nstatic void mv_fill_sg(struct ata_queued_cmd *qc)\n{\n\tstruct mv_port_priv *pp = qc->ap->private_data;\n\tstruct scatterlist *sg;\n\tstruct mv_sg *mv_sg, *last_sg = NULL;\n\tunsigned int si;\n\n\tmv_sg = pp->sg_tbl[qc->hw_tag];\n\tfor_each_sg(qc->sg, sg, qc->n_elem, si) {\n\t\tdma_addr_t addr = sg_dma_address(sg);\n\t\tu32 sg_len = sg_dma_len(sg);\n\n\t\twhile (sg_len) {\n\t\t\tu32 offset = addr & 0xffff;\n\t\t\tu32 len = sg_len;\n\n\t\t\tif (offset + len > 0x10000)\n\t\t\t\tlen = 0x10000 - offset;\n\n\t\t\tmv_sg->addr = cpu_to_le32(addr & 0xffffffff);\n\t\t\tmv_sg->addr_hi = cpu_to_le32((addr >> 16) >> 16);\n\t\t\tmv_sg->flags_size = cpu_to_le32(len & 0xffff);\n\t\t\tmv_sg->reserved = 0;\n\n\t\t\tsg_len -= len;\n\t\t\taddr += len;\n\n\t\t\tlast_sg = mv_sg;\n\t\t\tmv_sg++;\n\t\t}\n\t}\n\n\tif (likely(last_sg))\n\t\tlast_sg->flags_size |= cpu_to_le32(EPRD_FLAG_END_OF_TBL);\n\tmb();  \n}\n\nstatic void mv_crqb_pack_cmd(__le16 *cmdw, u8 data, u8 addr, unsigned last)\n{\n\tu16 tmp = data | (addr << CRQB_CMD_ADDR_SHIFT) | CRQB_CMD_CS |\n\t\t(last ? CRQB_CMD_LAST : 0);\n\t*cmdw = cpu_to_le16(tmp);\n}\n\n \nstatic void mv_sff_irq_clear(struct ata_port *ap)\n{\n\tmv_clear_and_enable_port_irqs(ap, mv_ap_base(ap), ERR_IRQ);\n}\n\n \nstatic int mv_check_atapi_dma(struct ata_queued_cmd *qc)\n{\n\tstruct scsi_cmnd *scmd = qc->scsicmd;\n\n\tif (scmd) {\n\t\tswitch (scmd->cmnd[0]) {\n\t\tcase READ_6:\n\t\tcase READ_10:\n\t\tcase READ_12:\n\t\tcase WRITE_6:\n\t\tcase WRITE_10:\n\t\tcase WRITE_12:\n\t\tcase GPCMD_READ_CD:\n\t\tcase GPCMD_SEND_DVD_STRUCTURE:\n\t\tcase GPCMD_SEND_CUE_SHEET:\n\t\t\treturn 0;  \n\t\t}\n\t}\n\treturn -EOPNOTSUPP;  \n}\n\n \nstatic void mv_bmdma_setup(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\tmv_fill_sg(qc);\n\n\t \n\twritel(0, port_mmio + BMDMA_CMD);\n\n\t \n\twritel((pp->sg_tbl_dma[qc->hw_tag] >> 16) >> 16,\n\t\tport_mmio + BMDMA_PRD_HIGH);\n\twritelfl(pp->sg_tbl_dma[qc->hw_tag],\n\t\tport_mmio + BMDMA_PRD_LOW);\n\n\t \n\tap->ops->sff_exec_command(ap, &qc->tf);\n}\n\n \nstatic void mv_bmdma_start(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tunsigned int rw = (qc->tf.flags & ATA_TFLAG_WRITE);\n\tu32 cmd = (rw ? 0 : ATA_DMA_WR) | ATA_DMA_START;\n\n\t \n\twritelfl(cmd, port_mmio + BMDMA_CMD);\n}\n\n \nstatic void mv_bmdma_stop_ap(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 cmd;\n\n\t \n\tcmd = readl(port_mmio + BMDMA_CMD);\n\tif (cmd & ATA_DMA_START) {\n\t\tcmd &= ~ATA_DMA_START;\n\t\twritelfl(cmd, port_mmio + BMDMA_CMD);\n\n\t\t \n\t\tata_sff_dma_pause(ap);\n\t}\n}\n\nstatic void mv_bmdma_stop(struct ata_queued_cmd *qc)\n{\n\tmv_bmdma_stop_ap(qc->ap);\n}\n\n \nstatic u8 mv_bmdma_status(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 reg, status;\n\n\t \n\treg = readl(port_mmio + BMDMA_STATUS);\n\tif (reg & ATA_DMA_ACTIVE)\n\t\tstatus = ATA_DMA_ACTIVE;\n\telse if (reg & ATA_DMA_ERR)\n\t\tstatus = (reg & ATA_DMA_ERR) | ATA_DMA_INTR;\n\telse {\n\t\t \n\t\tmv_bmdma_stop_ap(ap);\n\t\tif (ioread8(ap->ioaddr.altstatus_addr) & ATA_BUSY)\n\t\t\tstatus = 0;\n\t\telse\n\t\t\tstatus = ATA_DMA_INTR;\n\t}\n\treturn status;\n}\n\nstatic void mv_rw_multi_errata_sata24(struct ata_queued_cmd *qc)\n{\n\tstruct ata_taskfile *tf = &qc->tf;\n\t \n\tif ((tf->flags & ATA_TFLAG_WRITE) && is_multi_taskfile(tf)) {\n\t\tif (qc->dev->multi_count > 7) {\n\t\t\tswitch (tf->command) {\n\t\t\tcase ATA_CMD_WRITE_MULTI:\n\t\t\t\ttf->command = ATA_CMD_PIO_WRITE;\n\t\t\t\tbreak;\n\t\t\tcase ATA_CMD_WRITE_MULTI_FUA_EXT:\n\t\t\t\ttf->flags &= ~ATA_TFLAG_FUA;  \n\t\t\t\tfallthrough;\n\t\t\tcase ATA_CMD_WRITE_MULTI_EXT:\n\t\t\t\ttf->command = ATA_CMD_PIO_WRITE_EXT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic enum ata_completion_errors mv_qc_prep(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct mv_port_priv *pp = ap->private_data;\n\t__le16 *cw;\n\tstruct ata_taskfile *tf = &qc->tf;\n\tu16 flags = 0;\n\tunsigned in_index;\n\n\tswitch (tf->protocol) {\n\tcase ATA_PROT_DMA:\n\t\tif (tf->command == ATA_CMD_DSM)\n\t\t\treturn AC_ERR_OK;\n\t\tfallthrough;\n\tcase ATA_PROT_NCQ:\n\t\tbreak;\t \n\tcase ATA_PROT_PIO:\n\t\tmv_rw_multi_errata_sata24(qc);\n\t\treturn AC_ERR_OK;\n\tdefault:\n\t\treturn AC_ERR_OK;\n\t}\n\n\t \n\tif (!(tf->flags & ATA_TFLAG_WRITE))\n\t\tflags |= CRQB_FLAG_READ;\n\tWARN_ON(MV_MAX_Q_DEPTH <= qc->hw_tag);\n\tflags |= qc->hw_tag << CRQB_TAG_SHIFT;\n\tflags |= (qc->dev->link->pmp & 0xf) << CRQB_PMP_SHIFT;\n\n\t \n\tin_index = pp->req_idx;\n\n\tpp->crqb[in_index].sg_addr =\n\t\tcpu_to_le32(pp->sg_tbl_dma[qc->hw_tag] & 0xffffffff);\n\tpp->crqb[in_index].sg_addr_hi =\n\t\tcpu_to_le32((pp->sg_tbl_dma[qc->hw_tag] >> 16) >> 16);\n\tpp->crqb[in_index].ctrl_flags = cpu_to_le16(flags);\n\n\tcw = &pp->crqb[in_index].ata_cmd[0];\n\n\t \n\tswitch (tf->command) {\n\tcase ATA_CMD_READ:\n\tcase ATA_CMD_READ_EXT:\n\tcase ATA_CMD_WRITE:\n\tcase ATA_CMD_WRITE_EXT:\n\tcase ATA_CMD_WRITE_FUA_EXT:\n\t\tmv_crqb_pack_cmd(cw++, tf->hob_nsect, ATA_REG_NSECT, 0);\n\t\tbreak;\n\tcase ATA_CMD_FPDMA_READ:\n\tcase ATA_CMD_FPDMA_WRITE:\n\t\tmv_crqb_pack_cmd(cw++, tf->hob_feature, ATA_REG_FEATURE, 0);\n\t\tmv_crqb_pack_cmd(cw++, tf->feature, ATA_REG_FEATURE, 0);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tata_port_err(ap, \"%s: unsupported command: %.2x\\n\", __func__,\n\t\t\t\ttf->command);\n\t\treturn AC_ERR_INVALID;\n\t}\n\tmv_crqb_pack_cmd(cw++, tf->nsect, ATA_REG_NSECT, 0);\n\tmv_crqb_pack_cmd(cw++, tf->hob_lbal, ATA_REG_LBAL, 0);\n\tmv_crqb_pack_cmd(cw++, tf->lbal, ATA_REG_LBAL, 0);\n\tmv_crqb_pack_cmd(cw++, tf->hob_lbam, ATA_REG_LBAM, 0);\n\tmv_crqb_pack_cmd(cw++, tf->lbam, ATA_REG_LBAM, 0);\n\tmv_crqb_pack_cmd(cw++, tf->hob_lbah, ATA_REG_LBAH, 0);\n\tmv_crqb_pack_cmd(cw++, tf->lbah, ATA_REG_LBAH, 0);\n\tmv_crqb_pack_cmd(cw++, tf->device, ATA_REG_DEVICE, 0);\n\tmv_crqb_pack_cmd(cw++, tf->command, ATA_REG_CMD, 1);\t \n\n\tif (!(qc->flags & ATA_QCFLAG_DMAMAP))\n\t\treturn AC_ERR_OK;\n\tmv_fill_sg(qc);\n\n\treturn AC_ERR_OK;\n}\n\n \nstatic enum ata_completion_errors mv_qc_prep_iie(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct mv_port_priv *pp = ap->private_data;\n\tstruct mv_crqb_iie *crqb;\n\tstruct ata_taskfile *tf = &qc->tf;\n\tunsigned in_index;\n\tu32 flags = 0;\n\n\tif ((tf->protocol != ATA_PROT_DMA) &&\n\t    (tf->protocol != ATA_PROT_NCQ))\n\t\treturn AC_ERR_OK;\n\tif (tf->command == ATA_CMD_DSM)\n\t\treturn AC_ERR_OK;   \n\n\t \n\tif (!(tf->flags & ATA_TFLAG_WRITE))\n\t\tflags |= CRQB_FLAG_READ;\n\n\tWARN_ON(MV_MAX_Q_DEPTH <= qc->hw_tag);\n\tflags |= qc->hw_tag << CRQB_TAG_SHIFT;\n\tflags |= qc->hw_tag << CRQB_HOSTQ_SHIFT;\n\tflags |= (qc->dev->link->pmp & 0xf) << CRQB_PMP_SHIFT;\n\n\t \n\tin_index = pp->req_idx;\n\n\tcrqb = (struct mv_crqb_iie *) &pp->crqb[in_index];\n\tcrqb->addr = cpu_to_le32(pp->sg_tbl_dma[qc->hw_tag] & 0xffffffff);\n\tcrqb->addr_hi = cpu_to_le32((pp->sg_tbl_dma[qc->hw_tag] >> 16) >> 16);\n\tcrqb->flags = cpu_to_le32(flags);\n\n\tcrqb->ata_cmd[0] = cpu_to_le32(\n\t\t\t(tf->command << 16) |\n\t\t\t(tf->feature << 24)\n\t\t);\n\tcrqb->ata_cmd[1] = cpu_to_le32(\n\t\t\t(tf->lbal << 0) |\n\t\t\t(tf->lbam << 8) |\n\t\t\t(tf->lbah << 16) |\n\t\t\t(tf->device << 24)\n\t\t);\n\tcrqb->ata_cmd[2] = cpu_to_le32(\n\t\t\t(tf->hob_lbal << 0) |\n\t\t\t(tf->hob_lbam << 8) |\n\t\t\t(tf->hob_lbah << 16) |\n\t\t\t(tf->hob_feature << 24)\n\t\t);\n\tcrqb->ata_cmd[3] = cpu_to_le32(\n\t\t\t(tf->nsect << 0) |\n\t\t\t(tf->hob_nsect << 8)\n\t\t);\n\n\tif (!(qc->flags & ATA_QCFLAG_DMAMAP))\n\t\treturn AC_ERR_OK;\n\tmv_fill_sg(qc);\n\n\treturn AC_ERR_OK;\n}\n\n \nstatic u8 mv_sff_check_status(struct ata_port *ap)\n{\n\tu8 stat = ioread8(ap->ioaddr.status_addr);\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\tif (pp->pp_flags & MV_PP_FLAG_FAKE_ATA_BUSY) {\n\t\tif (stat & (ATA_BUSY | ATA_DRQ | ATA_ERR))\n\t\t\tpp->pp_flags &= ~MV_PP_FLAG_FAKE_ATA_BUSY;\n\t\telse\n\t\t\tstat = ATA_BUSY;\n\t}\n\treturn stat;\n}\n\n \nstatic unsigned int mv_send_fis(struct ata_port *ap, u32 *fis, int nwords)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 ifctl, old_ifctl, ifstat;\n\tint i, timeout = 200, final_word = nwords - 1;\n\n\t \n\told_ifctl = readl(port_mmio + SATA_IFCTL);\n\tifctl = 0x100 | (old_ifctl & 0xf);\n\twritelfl(ifctl, port_mmio + SATA_IFCTL);\n\n\t \n\tfor (i = 0; i < final_word; ++i)\n\t\twritel(fis[i], port_mmio + VENDOR_UNIQUE_FIS);\n\n\t \n\twritelfl(ifctl | 0x200, port_mmio + SATA_IFCTL);\n\twritelfl(fis[final_word], port_mmio + VENDOR_UNIQUE_FIS);\n\n\t \n\tdo {\n\t\tifstat = readl(port_mmio + SATA_IFSTAT);\n\t} while (!(ifstat & 0x1000) && --timeout);\n\n\t \n\twritelfl(old_ifctl, port_mmio + SATA_IFCTL);\n\n\t \n\tif ((ifstat & 0x3000) != 0x1000) {\n\t\tata_port_warn(ap, \"%s transmission error, ifstat=%08x\\n\",\n\t\t\t      __func__, ifstat);\n\t\treturn AC_ERR_OTHER;\n\t}\n\treturn 0;\n}\n\n \nstatic unsigned int mv_qc_issue_fis(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct mv_port_priv *pp = ap->private_data;\n\tstruct ata_link *link = qc->dev->link;\n\tu32 fis[5];\n\tint err = 0;\n\n\tata_tf_to_fis(&qc->tf, link->pmp, 1, (void *)fis);\n\terr = mv_send_fis(ap, fis, ARRAY_SIZE(fis));\n\tif (err)\n\t\treturn err;\n\n\tswitch (qc->tf.protocol) {\n\tcase ATAPI_PROT_PIO:\n\t\tpp->pp_flags |= MV_PP_FLAG_FAKE_ATA_BUSY;\n\t\tfallthrough;\n\tcase ATAPI_PROT_NODATA:\n\t\tap->hsm_task_state = HSM_ST_FIRST;\n\t\tbreak;\n\tcase ATA_PROT_PIO:\n\t\tpp->pp_flags |= MV_PP_FLAG_FAKE_ATA_BUSY;\n\t\tif (qc->tf.flags & ATA_TFLAG_WRITE)\n\t\t\tap->hsm_task_state = HSM_ST_FIRST;\n\t\telse\n\t\t\tap->hsm_task_state = HSM_ST;\n\t\tbreak;\n\tdefault:\n\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\tbreak;\n\t}\n\n\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\tata_sff_queue_pio_task(link, 0);\n\treturn 0;\n}\n\n \nstatic unsigned int mv_qc_issue(struct ata_queued_cmd *qc)\n{\n\tstatic int limit_warnings = 10;\n\tstruct ata_port *ap = qc->ap;\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tstruct mv_port_priv *pp = ap->private_data;\n\tu32 in_index;\n\tunsigned int port_irqs;\n\n\tpp->pp_flags &= ~MV_PP_FLAG_FAKE_ATA_BUSY;  \n\n\tswitch (qc->tf.protocol) {\n\tcase ATA_PROT_DMA:\n\t\tif (qc->tf.command == ATA_CMD_DSM) {\n\t\t\tif (!ap->ops->bmdma_setup)   \n\t\t\t\treturn AC_ERR_OTHER;\n\t\t\tbreak;   \n\t\t}\n\t\tfallthrough;\n\tcase ATA_PROT_NCQ:\n\t\tmv_start_edma(ap, port_mmio, pp, qc->tf.protocol);\n\t\tpp->req_idx = (pp->req_idx + 1) & MV_MAX_Q_DEPTH_MASK;\n\t\tin_index = pp->req_idx << EDMA_REQ_Q_PTR_SHIFT;\n\n\t\t \n\t\twritelfl((pp->crqb_dma & EDMA_REQ_Q_BASE_LO_MASK) | in_index,\n\t\t\t\t\tport_mmio + EDMA_REQ_Q_IN_PTR);\n\t\treturn 0;\n\n\tcase ATA_PROT_PIO:\n\t\t \n\t\tif (limit_warnings > 0 && (qc->nbytes / qc->sect_size) > 1) {\n\t\t\t--limit_warnings;\n\t\t\tata_link_warn(qc->dev->link, DRV_NAME\n\t\t\t\t      \": attempting PIO w/multiple DRQ: \"\n\t\t\t\t      \"this may fail due to h/w errata\\n\");\n\t\t}\n\t\tfallthrough;\n\tcase ATA_PROT_NODATA:\n\tcase ATAPI_PROT_PIO:\n\tcase ATAPI_PROT_NODATA:\n\t\tif (ap->flags & ATA_FLAG_PIO_POLLING)\n\t\t\tqc->tf.flags |= ATA_TFLAG_POLLING;\n\t\tbreak;\n\t}\n\n\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\tport_irqs = ERR_IRQ;\t \n\telse\n\t\tport_irqs = ERR_IRQ | DONE_IRQ;\t \n\n\t \n\tmv_stop_edma(ap);\n\tmv_clear_and_enable_port_irqs(ap, mv_ap_base(ap), port_irqs);\n\tmv_pmp_select(ap, qc->dev->link->pmp);\n\n\tif (qc->tf.command == ATA_CMD_READ_LOG_EXT) {\n\t\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\t\t \n\t\tif (IS_GEN_II(hpriv))\n\t\t\treturn mv_qc_issue_fis(qc);\n\t}\n\treturn ata_bmdma_qc_issue(qc);\n}\n\nstatic struct ata_queued_cmd *mv_get_active_qc(struct ata_port *ap)\n{\n\tstruct mv_port_priv *pp = ap->private_data;\n\tstruct ata_queued_cmd *qc;\n\n\tif (pp->pp_flags & MV_PP_FLAG_NCQ_EN)\n\t\treturn NULL;\n\tqc = ata_qc_from_tag(ap, ap->link.active_tag);\n\tif (qc && !(qc->tf.flags & ATA_TFLAG_POLLING))\n\t\treturn qc;\n\treturn NULL;\n}\n\nstatic void mv_pmp_error_handler(struct ata_port *ap)\n{\n\tunsigned int pmp, pmp_map;\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\tif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH) {\n\t\t \n\t\tpmp_map = pp->delayed_eh_pmp_map;\n\t\tpp->pp_flags &= ~MV_PP_FLAG_DELAYED_EH;\n\t\tfor (pmp = 0; pmp_map != 0; pmp++) {\n\t\t\tunsigned int this_pmp = (1 << pmp);\n\t\t\tif (pmp_map & this_pmp) {\n\t\t\t\tstruct ata_link *link = &ap->pmp_link[pmp];\n\t\t\t\tpmp_map &= ~this_pmp;\n\t\t\t\tata_eh_analyze_ncq_error(link);\n\t\t\t}\n\t\t}\n\t\tata_port_freeze(ap);\n\t}\n\tsata_pmp_error_handler(ap);\n}\n\nstatic unsigned int mv_get_err_pmp_map(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\n\treturn readl(port_mmio + SATA_TESTCTL) >> 16;\n}\n\nstatic void mv_pmp_eh_prep(struct ata_port *ap, unsigned int pmp_map)\n{\n\tunsigned int pmp;\n\n\t \n\tfor (pmp = 0; pmp_map != 0; pmp++) {\n\t\tunsigned int this_pmp = (1 << pmp);\n\t\tif (pmp_map & this_pmp) {\n\t\t\tstruct ata_link *link = &ap->pmp_link[pmp];\n\t\t\tstruct ata_eh_info *ehi = &link->eh_info;\n\n\t\t\tpmp_map &= ~this_pmp;\n\t\t\tata_ehi_clear_desc(ehi);\n\t\t\tata_ehi_push_desc(ehi, \"dev err\");\n\t\t\tehi->err_mask |= AC_ERR_DEV;\n\t\t\tehi->action |= ATA_EH_RESET;\n\t\t\tata_link_abort(link);\n\t\t}\n\t}\n}\n\nstatic int mv_req_q_empty(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 in_ptr, out_ptr;\n\n\tin_ptr  = (readl(port_mmio + EDMA_REQ_Q_IN_PTR)\n\t\t\t>> EDMA_REQ_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\n\tout_ptr = (readl(port_mmio + EDMA_REQ_Q_OUT_PTR)\n\t\t\t>> EDMA_REQ_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\n\treturn (in_ptr == out_ptr);\t \n}\n\nstatic int mv_handle_fbs_ncq_dev_err(struct ata_port *ap)\n{\n\tstruct mv_port_priv *pp = ap->private_data;\n\tint failed_links;\n\tunsigned int old_map, new_map;\n\n\t \n\tif (!(pp->pp_flags & MV_PP_FLAG_DELAYED_EH)) {\n\t\tpp->pp_flags |= MV_PP_FLAG_DELAYED_EH;\n\t\tpp->delayed_eh_pmp_map = 0;\n\t}\n\told_map = pp->delayed_eh_pmp_map;\n\tnew_map = old_map | mv_get_err_pmp_map(ap);\n\n\tif (old_map != new_map) {\n\t\tpp->delayed_eh_pmp_map = new_map;\n\t\tmv_pmp_eh_prep(ap, new_map & ~old_map);\n\t}\n\tfailed_links = hweight16(new_map);\n\n\tata_port_info(ap,\n\t\t      \"%s: pmp_map=%04x qc_map=%04llx failed_links=%d nr_active_links=%d\\n\",\n\t\t      __func__, pp->delayed_eh_pmp_map,\n\t\t      ap->qc_active, failed_links,\n\t\t      ap->nr_active_links);\n\n\tif (ap->nr_active_links <= failed_links && mv_req_q_empty(ap)) {\n\t\tmv_process_crpb_entries(ap, pp);\n\t\tmv_stop_edma(ap);\n\t\tmv_eh_freeze(ap);\n\t\tata_port_info(ap, \"%s: done\\n\", __func__);\n\t\treturn 1;\t \n\t}\n\tata_port_info(ap, \"%s: waiting\\n\", __func__);\n\treturn 1;\t \n}\n\nstatic int mv_handle_fbs_non_ncq_dev_err(struct ata_port *ap)\n{\n\t \n\treturn 0;\t \n}\n\nstatic int mv_handle_dev_err(struct ata_port *ap, u32 edma_err_cause)\n{\n\tstruct mv_port_priv *pp = ap->private_data;\n\n\tif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN))\n\t\treturn 0;\t \n\tif (!(pp->pp_flags & MV_PP_FLAG_FBS_EN))\n\t\treturn 0;\t \n\n\tif (!(edma_err_cause & EDMA_ERR_DEV))\n\t\treturn 0;\t \n\tedma_err_cause &= ~EDMA_ERR_IRQ_TRANSIENT;\n\tif (edma_err_cause & ~(EDMA_ERR_DEV | EDMA_ERR_SELF_DIS))\n\t\treturn 0;\t \n\n\tif (pp->pp_flags & MV_PP_FLAG_NCQ_EN) {\n\t\t \n\t\tif (edma_err_cause & EDMA_ERR_SELF_DIS) {\n\t\t\tata_port_warn(ap, \"%s: err_cause=0x%x pp_flags=0x%x\\n\",\n\t\t\t\t      __func__, edma_err_cause, pp->pp_flags);\n\t\t\treturn 0;  \n\t\t}\n\t\treturn mv_handle_fbs_ncq_dev_err(ap);\n\t} else {\n\t\t \n\t\tif (!(edma_err_cause & EDMA_ERR_SELF_DIS)) {\n\t\t\tata_port_warn(ap, \"%s: err_cause=0x%x pp_flags=0x%x\\n\",\n\t\t\t\t      __func__, edma_err_cause, pp->pp_flags);\n\t\t\treturn 0;  \n\t\t}\n\t\treturn mv_handle_fbs_non_ncq_dev_err(ap);\n\t}\n\treturn 0;\t \n}\n\nstatic void mv_unexpected_intr(struct ata_port *ap, int edma_was_enabled)\n{\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\tchar *when = \"idle\";\n\n\tata_ehi_clear_desc(ehi);\n\tif (edma_was_enabled) {\n\t\twhen = \"EDMA enabled\";\n\t} else {\n\t\tstruct ata_queued_cmd *qc = ata_qc_from_tag(ap, ap->link.active_tag);\n\t\tif (qc && (qc->tf.flags & ATA_TFLAG_POLLING))\n\t\t\twhen = \"polling\";\n\t}\n\tata_ehi_push_desc(ehi, \"unexpected device interrupt while %s\", when);\n\tehi->err_mask |= AC_ERR_OTHER;\n\tehi->action   |= ATA_EH_RESET;\n\tata_port_freeze(ap);\n}\n\n \nstatic void mv_err_intr(struct ata_port *ap)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 edma_err_cause, eh_freeze_mask, serr = 0;\n\tu32 fis_cause = 0;\n\tstruct mv_port_priv *pp = ap->private_data;\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tunsigned int action = 0, err_mask = 0;\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\tstruct ata_queued_cmd *qc;\n\tint abort = 0;\n\n\t \n\tsata_scr_read(&ap->link, SCR_ERROR, &serr);\n\tsata_scr_write_flush(&ap->link, SCR_ERROR, serr);\n\n\tedma_err_cause = readl(port_mmio + EDMA_ERR_IRQ_CAUSE);\n\tif (IS_GEN_IIE(hpriv) && (edma_err_cause & EDMA_ERR_TRANS_IRQ_7)) {\n\t\tfis_cause = readl(port_mmio + FIS_IRQ_CAUSE);\n\t\twritelfl(~fis_cause, port_mmio + FIS_IRQ_CAUSE);\n\t}\n\twritelfl(~edma_err_cause, port_mmio + EDMA_ERR_IRQ_CAUSE);\n\n\tif (edma_err_cause & EDMA_ERR_DEV) {\n\t\t \n\t\tif (mv_handle_dev_err(ap, edma_err_cause))\n\t\t\treturn;\n\t}\n\n\tqc = mv_get_active_qc(ap);\n\tata_ehi_clear_desc(ehi);\n\tata_ehi_push_desc(ehi, \"edma_err_cause=%08x pp_flags=%08x\",\n\t\t\t  edma_err_cause, pp->pp_flags);\n\n\tif (IS_GEN_IIE(hpriv) && (edma_err_cause & EDMA_ERR_TRANS_IRQ_7)) {\n\t\tata_ehi_push_desc(ehi, \"fis_cause=%08x\", fis_cause);\n\t\tif (fis_cause & FIS_IRQ_CAUSE_AN) {\n\t\t\tu32 ec = edma_err_cause &\n\t\t\t       ~(EDMA_ERR_TRANS_IRQ_7 | EDMA_ERR_IRQ_TRANSIENT);\n\t\t\tsata_async_notification(ap);\n\t\t\tif (!ec)\n\t\t\t\treturn;  \n\t\t\tata_ehi_push_desc(ehi, \"SDB notify\");\n\t\t}\n\t}\n\t \n\tif (edma_err_cause & EDMA_ERR_DEV) {\n\t\terr_mask |= AC_ERR_DEV;\n\t\taction |= ATA_EH_RESET;\n\t\tata_ehi_push_desc(ehi, \"dev error\");\n\t}\n\tif (edma_err_cause & (EDMA_ERR_D_PAR | EDMA_ERR_PRD_PAR |\n\t\t\tEDMA_ERR_CRQB_PAR | EDMA_ERR_CRPB_PAR |\n\t\t\tEDMA_ERR_INTRL_PAR)) {\n\t\terr_mask |= AC_ERR_ATA_BUS;\n\t\taction |= ATA_EH_RESET;\n\t\tata_ehi_push_desc(ehi, \"parity error\");\n\t}\n\tif (edma_err_cause & (EDMA_ERR_DEV_DCON | EDMA_ERR_DEV_CON)) {\n\t\tata_ehi_hotplugged(ehi);\n\t\tata_ehi_push_desc(ehi, edma_err_cause & EDMA_ERR_DEV_DCON ?\n\t\t\t\"dev disconnect\" : \"dev connect\");\n\t\taction |= ATA_EH_RESET;\n\t}\n\n\t \n\tif (IS_GEN_I(hpriv)) {\n\t\teh_freeze_mask = EDMA_EH_FREEZE_5;\n\t\tif (edma_err_cause & EDMA_ERR_SELF_DIS_5) {\n\t\t\tpp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\n\t\t\tata_ehi_push_desc(ehi, \"EDMA self-disable\");\n\t\t}\n\t} else {\n\t\teh_freeze_mask = EDMA_EH_FREEZE;\n\t\tif (edma_err_cause & EDMA_ERR_SELF_DIS) {\n\t\t\tpp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\n\t\t\tata_ehi_push_desc(ehi, \"EDMA self-disable\");\n\t\t}\n\t\tif (edma_err_cause & EDMA_ERR_SERR) {\n\t\t\tata_ehi_push_desc(ehi, \"SError=%08x\", serr);\n\t\t\terr_mask |= AC_ERR_ATA_BUS;\n\t\t\taction |= ATA_EH_RESET;\n\t\t}\n\t}\n\n\tif (!err_mask) {\n\t\terr_mask = AC_ERR_OTHER;\n\t\taction |= ATA_EH_RESET;\n\t}\n\n\tehi->serror |= serr;\n\tehi->action |= action;\n\n\tif (qc)\n\t\tqc->err_mask |= err_mask;\n\telse\n\t\tehi->err_mask |= err_mask;\n\n\tif (err_mask == AC_ERR_DEV) {\n\t\t \n\t\tmv_eh_freeze(ap);\n\t\tabort = 1;\n\t} else if (edma_err_cause & eh_freeze_mask) {\n\t\t \n\t\tata_port_freeze(ap);\n\t} else {\n\t\tabort = 1;\n\t}\n\n\tif (abort) {\n\t\tif (qc)\n\t\t\tata_link_abort(qc->dev->link);\n\t\telse\n\t\t\tata_port_abort(ap);\n\t}\n}\n\nstatic bool mv_process_crpb_response(struct ata_port *ap,\n\t\tstruct mv_crpb *response, unsigned int tag, int ncq_enabled)\n{\n\tu8 ata_status;\n\tu16 edma_status = le16_to_cpu(response->flags);\n\n\t \n\tif (!ncq_enabled) {\n\t\tu8 err_cause = edma_status & 0xff & ~EDMA_ERR_DEV;\n\t\tif (err_cause) {\n\t\t\t \n\t\t\treturn false;\n\t\t}\n\t}\n\tata_status = edma_status >> CRPB_FLAG_STATUS_SHIFT;\n\tif (!ac_err_mask(ata_status))\n\t\treturn true;\n\t \n\treturn false;\n}\n\nstatic void mv_process_crpb_entries(struct ata_port *ap, struct mv_port_priv *pp)\n{\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tu32 in_index;\n\tbool work_done = false;\n\tu32 done_mask = 0;\n\tint ncq_enabled = (pp->pp_flags & MV_PP_FLAG_NCQ_EN);\n\n\t \n\tin_index = (readl(port_mmio + EDMA_RSP_Q_IN_PTR)\n\t\t\t>> EDMA_RSP_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\n\n\t \n\twhile (in_index != pp->resp_idx) {\n\t\tunsigned int tag;\n\t\tstruct mv_crpb *response = &pp->crpb[pp->resp_idx];\n\n\t\tpp->resp_idx = (pp->resp_idx + 1) & MV_MAX_Q_DEPTH_MASK;\n\n\t\tif (IS_GEN_I(hpriv)) {\n\t\t\t \n\t\t\ttag = ap->link.active_tag;\n\t\t} else {\n\t\t\t \n\t\t\ttag = le16_to_cpu(response->id) & 0x1f;\n\t\t}\n\t\tif (mv_process_crpb_response(ap, response, tag, ncq_enabled))\n\t\t\tdone_mask |= 1 << tag;\n\t\twork_done = true;\n\t}\n\n\tif (work_done) {\n\t\tata_qc_complete_multiple(ap, ata_qc_get_active(ap) ^ done_mask);\n\n\t\t \n\t\twritelfl((pp->crpb_dma & EDMA_RSP_Q_BASE_LO_MASK) |\n\t\t\t (pp->resp_idx << EDMA_RSP_Q_PTR_SHIFT),\n\t\t\t port_mmio + EDMA_RSP_Q_OUT_PTR);\n\t}\n}\n\nstatic void mv_port_intr(struct ata_port *ap, u32 port_cause)\n{\n\tstruct mv_port_priv *pp;\n\tint edma_was_enabled;\n\n\t \n\tpp = ap->private_data;\n\tedma_was_enabled = (pp->pp_flags & MV_PP_FLAG_EDMA_EN);\n\t \n\tif (edma_was_enabled && (port_cause & DONE_IRQ)) {\n\t\tmv_process_crpb_entries(ap, pp);\n\t\tif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH)\n\t\t\tmv_handle_fbs_ncq_dev_err(ap);\n\t}\n\t \n\tif (unlikely(port_cause & ERR_IRQ)) {\n\t\tmv_err_intr(ap);\n\t} else if (!edma_was_enabled) {\n\t\tstruct ata_queued_cmd *qc = mv_get_active_qc(ap);\n\t\tif (qc)\n\t\t\tata_bmdma_port_intr(ap, qc);\n\t\telse\n\t\t\tmv_unexpected_intr(ap, edma_was_enabled);\n\t}\n}\n\n \nstatic int mv_host_intr(struct ata_host *host, u32 main_irq_cause)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base, *hc_mmio;\n\tunsigned int handled = 0, port;\n\n\t \n\tif (main_irq_cause & ALL_PORTS_COAL_DONE)\n\t\twritel(~ALL_PORTS_COAL_IRQ, mmio + IRQ_COAL_CAUSE);\n\n\tfor (port = 0; port < hpriv->n_ports; port++) {\n\t\tstruct ata_port *ap = host->ports[port];\n\t\tunsigned int p, shift, hardport, port_cause;\n\n\t\tMV_PORT_TO_SHIFT_AND_HARDPORT(port, shift, hardport);\n\t\t \n\t\tif (hardport == 0) {\t \n\t\t\tu32 hc_cause = (main_irq_cause >> shift) & HC0_IRQ_PEND;\n\t\t\tu32 port_mask, ack_irqs;\n\t\t\t \n\t\t\tif (!hc_cause) {\n\t\t\t\tport += MV_PORTS_PER_HC - 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tack_irqs = 0;\n\t\t\tif (hc_cause & PORTS_0_3_COAL_DONE)\n\t\t\t\tack_irqs = HC_COAL_IRQ;\n\t\t\tfor (p = 0; p < MV_PORTS_PER_HC; ++p) {\n\t\t\t\tif ((port + p) >= hpriv->n_ports)\n\t\t\t\t\tbreak;\n\t\t\t\tport_mask = (DONE_IRQ | ERR_IRQ) << (p * 2);\n\t\t\t\tif (hc_cause & port_mask)\n\t\t\t\t\tack_irqs |= (DMA_IRQ | DEV_IRQ) << p;\n\t\t\t}\n\t\t\thc_mmio = mv_hc_base_from_port(mmio, port);\n\t\t\twritelfl(~ack_irqs, hc_mmio + HC_IRQ_CAUSE);\n\t\t\thandled = 1;\n\t\t}\n\t\t \n\t\tport_cause = (main_irq_cause >> shift) & (DONE_IRQ | ERR_IRQ);\n\t\tif (port_cause)\n\t\t\tmv_port_intr(ap, port_cause);\n\t}\n\treturn handled;\n}\n\nstatic int mv_pci_error(struct ata_host *host, void __iomem *mmio)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tstruct ata_port *ap;\n\tstruct ata_queued_cmd *qc;\n\tstruct ata_eh_info *ehi;\n\tunsigned int i, err_mask, printed = 0;\n\tu32 err_cause;\n\n\terr_cause = readl(mmio + hpriv->irq_cause_offset);\n\n\tdev_err(host->dev, \"PCI ERROR; PCI IRQ cause=0x%08x\\n\", err_cause);\n\n\tdev_dbg(host->dev, \"%s: All regs @ PCI error\\n\", __func__);\n\tmv_dump_all_regs(mmio, to_pci_dev(host->dev));\n\n\twritelfl(0, mmio + hpriv->irq_cause_offset);\n\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\tap = host->ports[i];\n\t\tif (!ata_link_offline(&ap->link)) {\n\t\t\tehi = &ap->link.eh_info;\n\t\t\tata_ehi_clear_desc(ehi);\n\t\t\tif (!printed++)\n\t\t\t\tata_ehi_push_desc(ehi,\n\t\t\t\t\t\"PCI err cause 0x%08x\", err_cause);\n\t\t\terr_mask = AC_ERR_HOST_BUS;\n\t\t\tehi->action = ATA_EH_RESET;\n\t\t\tqc = ata_qc_from_tag(ap, ap->link.active_tag);\n\t\t\tif (qc)\n\t\t\t\tqc->err_mask |= err_mask;\n\t\t\telse\n\t\t\t\tehi->err_mask |= err_mask;\n\n\t\t\tata_port_freeze(ap);\n\t\t}\n\t}\n\treturn 1;\t \n}\n\n \nstatic irqreturn_t mv_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tunsigned int handled = 0;\n\tint using_msi = hpriv->hp_flags & MV_HP_FLAG_MSI;\n\tu32 main_irq_cause, pending_irqs;\n\n\tspin_lock(&host->lock);\n\n\t \n\tif (using_msi)\n\t\tmv_write_main_irq_mask(0, hpriv);\n\n\tmain_irq_cause = readl(hpriv->main_irq_cause_addr);\n\tpending_irqs   = main_irq_cause & hpriv->main_irq_mask;\n\t \n\tif (pending_irqs && main_irq_cause != 0xffffffffU) {\n\t\tif (unlikely((pending_irqs & PCI_ERR) && !IS_SOC(hpriv)))\n\t\t\thandled = mv_pci_error(host, hpriv->base);\n\t\telse\n\t\t\thandled = mv_host_intr(host, pending_irqs);\n\t}\n\n\t \n\tif (using_msi)\n\t\tmv_write_main_irq_mask(hpriv->main_irq_mask, hpriv);\n\n\tspin_unlock(&host->lock);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic unsigned int mv5_scr_offset(unsigned int sc_reg_in)\n{\n\tunsigned int ofs;\n\n\tswitch (sc_reg_in) {\n\tcase SCR_STATUS:\n\tcase SCR_ERROR:\n\tcase SCR_CONTROL:\n\t\tofs = sc_reg_in * sizeof(u32);\n\t\tbreak;\n\tdefault:\n\t\tofs = 0xffffffffU;\n\t\tbreak;\n\t}\n\treturn ofs;\n}\n\nstatic int mv5_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val)\n{\n\tstruct mv_host_priv *hpriv = link->ap->host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\tvoid __iomem *addr = mv5_phy_base(mmio, link->ap->port_no);\n\tunsigned int ofs = mv5_scr_offset(sc_reg_in);\n\n\tif (ofs != 0xffffffffU) {\n\t\t*val = readl(addr + ofs);\n\t\treturn 0;\n\t} else\n\t\treturn -EINVAL;\n}\n\nstatic int mv5_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val)\n{\n\tstruct mv_host_priv *hpriv = link->ap->host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\tvoid __iomem *addr = mv5_phy_base(mmio, link->ap->port_no);\n\tunsigned int ofs = mv5_scr_offset(sc_reg_in);\n\n\tif (ofs != 0xffffffffU) {\n\t\twritelfl(val, addr + ofs);\n\t\treturn 0;\n\t} else\n\t\treturn -EINVAL;\n}\n\nstatic void mv5_reset_bus(struct ata_host *host, void __iomem *mmio)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tint early_5080;\n\n\tearly_5080 = (pdev->device == 0x5080) && (pdev->revision == 0);\n\n\tif (!early_5080) {\n\t\tu32 tmp = readl(mmio + MV_PCI_EXP_ROM_BAR_CTL);\n\t\ttmp |= (1 << 0);\n\t\twritel(tmp, mmio + MV_PCI_EXP_ROM_BAR_CTL);\n\t}\n\n\tmv_reset_pci_bus(host, mmio);\n}\n\nstatic void mv5_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio)\n{\n\twritel(0x0fcfffff, mmio + FLASH_CTL);\n}\n\nstatic void mv5_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio)\n{\n\tvoid __iomem *phy_mmio = mv5_phy_base(mmio, idx);\n\tu32 tmp;\n\n\ttmp = readl(phy_mmio + MV5_PHY_MODE);\n\n\thpriv->signal[idx].pre = tmp & 0x1800;\t \n\thpriv->signal[idx].amps = tmp & 0xe0;\t \n}\n\nstatic void mv5_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio)\n{\n\tu32 tmp;\n\n\twritel(0, mmio + GPIO_PORT_CTL);\n\n\t \n\n\ttmp = readl(mmio + MV_PCI_EXP_ROM_BAR_CTL);\n\ttmp |= ~(1 << 0);\n\twritel(tmp, mmio + MV_PCI_EXP_ROM_BAR_CTL);\n}\n\nstatic void mv5_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t   unsigned int port)\n{\n\tvoid __iomem *phy_mmio = mv5_phy_base(mmio, port);\n\tconst u32 mask = (1<<12) | (1<<11) | (1<<7) | (1<<6) | (1<<5);\n\tu32 tmp;\n\tint fix_apm_sq = (hpriv->hp_flags & MV_HP_ERRATA_50XXB0);\n\n\tif (fix_apm_sq) {\n\t\ttmp = readl(phy_mmio + MV5_LTMODE);\n\t\ttmp |= (1 << 19);\n\t\twritel(tmp, phy_mmio + MV5_LTMODE);\n\n\t\ttmp = readl(phy_mmio + MV5_PHY_CTL);\n\t\ttmp &= ~0x3;\n\t\ttmp |= 0x1;\n\t\twritel(tmp, phy_mmio + MV5_PHY_CTL);\n\t}\n\n\ttmp = readl(phy_mmio + MV5_PHY_MODE);\n\ttmp &= ~mask;\n\ttmp |= hpriv->signal[port].pre;\n\ttmp |= hpriv->signal[port].amps;\n\twritel(tmp, phy_mmio + MV5_PHY_MODE);\n}\n\n\n#undef ZERO\n#define ZERO(reg) writel(0, port_mmio + (reg))\nstatic void mv5_reset_hc_port(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t     unsigned int port)\n{\n\tvoid __iomem *port_mmio = mv_port_base(mmio, port);\n\n\tmv_reset_channel(hpriv, mmio, port);\n\n\tZERO(0x028);\t \n\twritel(0x11f, port_mmio + EDMA_CFG);\n\tZERO(0x004);\t \n\tZERO(0x008);\t \n\tZERO(0x00c);\t \n\tZERO(0x010);\t \n\tZERO(0x014);\t \n\tZERO(0x018);\t \n\tZERO(0x01c);\t \n\tZERO(0x024);\t \n\tZERO(0x020);\t \n\tZERO(0x02c);\t \n\twritel(0xbc, port_mmio + EDMA_IORDY_TMOUT);\n}\n#undef ZERO\n\n#define ZERO(reg) writel(0, hc_mmio + (reg))\nstatic void mv5_reset_one_hc(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\tunsigned int hc)\n{\n\tvoid __iomem *hc_mmio = mv_hc_base(mmio, hc);\n\tu32 tmp;\n\n\tZERO(0x00c);\n\tZERO(0x010);\n\tZERO(0x014);\n\tZERO(0x018);\n\n\ttmp = readl(hc_mmio + 0x20);\n\ttmp &= 0x1c1c1c1c;\n\ttmp |= 0x03030303;\n\twritel(tmp, hc_mmio + 0x20);\n}\n#undef ZERO\n\nstatic int mv5_reset_hc(struct ata_host *host, void __iomem *mmio,\n\t\t\tunsigned int n_hc)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tunsigned int hc, port;\n\n\tfor (hc = 0; hc < n_hc; hc++) {\n\t\tfor (port = 0; port < MV_PORTS_PER_HC; port++)\n\t\t\tmv5_reset_hc_port(hpriv, mmio,\n\t\t\t\t\t  (hc * MV_PORTS_PER_HC) + port);\n\n\t\tmv5_reset_one_hc(hpriv, mmio, hc);\n\t}\n\n\treturn 0;\n}\n\n#undef ZERO\n#define ZERO(reg) writel(0, mmio + (reg))\nstatic void mv_reset_pci_bus(struct ata_host *host, void __iomem *mmio)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tu32 tmp;\n\n\ttmp = readl(mmio + MV_PCI_MODE);\n\ttmp &= 0xff00ffff;\n\twritel(tmp, mmio + MV_PCI_MODE);\n\n\tZERO(MV_PCI_DISC_TIMER);\n\tZERO(MV_PCI_MSI_TRIGGER);\n\twritel(0x000100ff, mmio + MV_PCI_XBAR_TMOUT);\n\tZERO(MV_PCI_SERR_MASK);\n\tZERO(hpriv->irq_cause_offset);\n\tZERO(hpriv->irq_mask_offset);\n\tZERO(MV_PCI_ERR_LOW_ADDRESS);\n\tZERO(MV_PCI_ERR_HIGH_ADDRESS);\n\tZERO(MV_PCI_ERR_ATTRIBUTE);\n\tZERO(MV_PCI_ERR_COMMAND);\n}\n#undef ZERO\n\nstatic void mv6_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio)\n{\n\tu32 tmp;\n\n\tmv5_reset_flash(hpriv, mmio);\n\n\ttmp = readl(mmio + GPIO_PORT_CTL);\n\ttmp &= 0x3;\n\ttmp |= (1 << 5) | (1 << 6);\n\twritel(tmp, mmio + GPIO_PORT_CTL);\n}\n\n \nstatic int mv6_reset_hc(struct ata_host *host, void __iomem *mmio,\n\t\t\tunsigned int n_hc)\n{\n\tvoid __iomem *reg = mmio + PCI_MAIN_CMD_STS;\n\tint i, rc = 0;\n\tu32 t;\n\n\t \n\tt = readl(reg);\n\twritel(t | STOP_PCI_MASTER, reg);\n\n\tfor (i = 0; i < 1000; i++) {\n\t\tudelay(1);\n\t\tt = readl(reg);\n\t\tif (PCI_MASTER_EMPTY & t)\n\t\t\tbreak;\n\t}\n\tif (!(PCI_MASTER_EMPTY & t)) {\n\t\tdev_err(host->dev, \"PCI master won't flush\\n\");\n\t\trc = 1;\n\t\tgoto done;\n\t}\n\n\t \n\ti = 5;\n\tdo {\n\t\twritel(t | GLOB_SFT_RST, reg);\n\t\tt = readl(reg);\n\t\tudelay(1);\n\t} while (!(GLOB_SFT_RST & t) && (i-- > 0));\n\n\tif (!(GLOB_SFT_RST & t)) {\n\t\tdev_err(host->dev, \"can't set global reset\\n\");\n\t\trc = 1;\n\t\tgoto done;\n\t}\n\n\t \n\ti = 5;\n\tdo {\n\t\twritel(t & ~(GLOB_SFT_RST | STOP_PCI_MASTER), reg);\n\t\tt = readl(reg);\n\t\tudelay(1);\n\t} while ((GLOB_SFT_RST & t) && (i-- > 0));\n\n\tif (GLOB_SFT_RST & t) {\n\t\tdev_err(host->dev, \"can't clear global reset\\n\");\n\t\trc = 1;\n\t}\ndone:\n\treturn rc;\n}\n\nstatic void mv6_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio)\n{\n\tvoid __iomem *port_mmio;\n\tu32 tmp;\n\n\ttmp = readl(mmio + RESET_CFG);\n\tif ((tmp & (1 << 0)) == 0) {\n\t\thpriv->signal[idx].amps = 0x7 << 8;\n\t\thpriv->signal[idx].pre = 0x1 << 5;\n\t\treturn;\n\t}\n\n\tport_mmio = mv_port_base(mmio, idx);\n\ttmp = readl(port_mmio + PHY_MODE2);\n\n\thpriv->signal[idx].amps = tmp & 0x700;\t \n\thpriv->signal[idx].pre = tmp & 0xe0;\t \n}\n\nstatic void mv6_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio)\n{\n\twritel(0x00000060, mmio + GPIO_PORT_CTL);\n}\n\nstatic void mv6_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t   unsigned int port)\n{\n\tvoid __iomem *port_mmio = mv_port_base(mmio, port);\n\n\tu32 hp_flags = hpriv->hp_flags;\n\tint fix_phy_mode2 =\n\t\thp_flags & (MV_HP_ERRATA_60X1B2 | MV_HP_ERRATA_60X1C0);\n\tint fix_phy_mode4 =\n\t\thp_flags & (MV_HP_ERRATA_60X1B2 | MV_HP_ERRATA_60X1C0);\n\tu32 m2, m3;\n\n\tif (fix_phy_mode2) {\n\t\tm2 = readl(port_mmio + PHY_MODE2);\n\t\tm2 &= ~(1 << 16);\n\t\tm2 |= (1 << 31);\n\t\twritel(m2, port_mmio + PHY_MODE2);\n\n\t\tudelay(200);\n\n\t\tm2 = readl(port_mmio + PHY_MODE2);\n\t\tm2 &= ~((1 << 16) | (1 << 31));\n\t\twritel(m2, port_mmio + PHY_MODE2);\n\n\t\tudelay(200);\n\t}\n\n\t \n\tm3 = readl(port_mmio + PHY_MODE3);\n\tm3 = (m3 & 0x1f) | (0x5555601 << 5);\n\n\t \n\tif (IS_SOC(hpriv))\n\t\tm3 &= ~0x1c;\n\n\tif (fix_phy_mode4) {\n\t\tu32 m4 = readl(port_mmio + PHY_MODE4);\n\t\t \n\t\tif (IS_GEN_IIE(hpriv))\n\t\t\tm4 = (m4 & ~PHY_MODE4_RSVD_ZEROS) | PHY_MODE4_RSVD_ONES;\n\t\telse\n\t\t\tm4 = (m4 & ~PHY_MODE4_CFG_MASK) | PHY_MODE4_CFG_VALUE;\n\t\twritel(m4, port_mmio + PHY_MODE4);\n\t}\n\t \n\twritel(m3, port_mmio + PHY_MODE3);\n\n\t \n\tm2 = readl(port_mmio + PHY_MODE2);\n\n\tm2 &= ~MV_M2_PREAMP_MASK;\n\tm2 |= hpriv->signal[port].amps;\n\tm2 |= hpriv->signal[port].pre;\n\tm2 &= ~(1 << 16);\n\n\t \n\tif (IS_GEN_IIE(hpriv)) {\n\t\tm2 &= ~0xC30FF01F;\n\t\tm2 |= 0x0000900F;\n\t}\n\n\twritel(m2, port_mmio + PHY_MODE2);\n}\n\n \n \nstatic void mv_soc_enable_leds(struct mv_host_priv *hpriv,\n\t\t\t\t      void __iomem *mmio)\n{\n\treturn;\n}\n\nstatic void mv_soc_read_preamp(struct mv_host_priv *hpriv, int idx,\n\t\t\t   void __iomem *mmio)\n{\n\tvoid __iomem *port_mmio;\n\tu32 tmp;\n\n\tport_mmio = mv_port_base(mmio, idx);\n\ttmp = readl(port_mmio + PHY_MODE2);\n\n\thpriv->signal[idx].amps = tmp & 0x700;\t \n\thpriv->signal[idx].pre = tmp & 0xe0;\t \n}\n\n#undef ZERO\n#define ZERO(reg) writel(0, port_mmio + (reg))\nstatic void mv_soc_reset_hc_port(struct mv_host_priv *hpriv,\n\t\t\t\t\tvoid __iomem *mmio, unsigned int port)\n{\n\tvoid __iomem *port_mmio = mv_port_base(mmio, port);\n\n\tmv_reset_channel(hpriv, mmio, port);\n\n\tZERO(0x028);\t\t \n\twritel(0x101f, port_mmio + EDMA_CFG);\n\tZERO(0x004);\t\t \n\tZERO(0x008);\t\t \n\tZERO(0x00c);\t\t \n\tZERO(0x010);\t\t \n\tZERO(0x014);\t\t \n\tZERO(0x018);\t\t \n\tZERO(0x01c);\t\t \n\tZERO(0x024);\t\t \n\tZERO(0x020);\t\t \n\tZERO(0x02c);\t\t \n\twritel(0x800, port_mmio + EDMA_IORDY_TMOUT);\n}\n\n#undef ZERO\n\n#define ZERO(reg) writel(0, hc_mmio + (reg))\nstatic void mv_soc_reset_one_hc(struct mv_host_priv *hpriv,\n\t\t\t\t       void __iomem *mmio)\n{\n\tvoid __iomem *hc_mmio = mv_hc_base(mmio, 0);\n\n\tZERO(0x00c);\n\tZERO(0x010);\n\tZERO(0x014);\n\n}\n\n#undef ZERO\n\nstatic int mv_soc_reset_hc(struct ata_host *host,\n\t\t\t\t  void __iomem *mmio, unsigned int n_hc)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tunsigned int port;\n\n\tfor (port = 0; port < hpriv->n_ports; port++)\n\t\tmv_soc_reset_hc_port(hpriv, mmio, port);\n\n\tmv_soc_reset_one_hc(hpriv, mmio);\n\n\treturn 0;\n}\n\nstatic void mv_soc_reset_flash(struct mv_host_priv *hpriv,\n\t\t\t\t      void __iomem *mmio)\n{\n\treturn;\n}\n\nstatic void mv_soc_reset_bus(struct ata_host *host, void __iomem *mmio)\n{\n\treturn;\n}\n\nstatic void mv_soc_65n_phy_errata(struct mv_host_priv *hpriv,\n\t\t\t\t  void __iomem *mmio, unsigned int port)\n{\n\tvoid __iomem *port_mmio = mv_port_base(mmio, port);\n\tu32\treg;\n\n\treg = readl(port_mmio + PHY_MODE3);\n\treg &= ~(0x3 << 27);\t \n\treg |= (0x1 << 27);\n\treg &= ~(0x3 << 29);\t \n\treg |= (0x1 << 29);\n\twritel(reg, port_mmio + PHY_MODE3);\n\n\treg = readl(port_mmio + PHY_MODE4);\n\treg &= ~0x1;\t \n\treg |= (0x1 << 16);\n\twritel(reg, port_mmio + PHY_MODE4);\n\n\treg = readl(port_mmio + PHY_MODE9_GEN2);\n\treg &= ~0xf;\t \n\treg |= 0x8;\n\treg &= ~(0x1 << 14);\t \n\twritel(reg, port_mmio + PHY_MODE9_GEN2);\n\n\treg = readl(port_mmio + PHY_MODE9_GEN1);\n\treg &= ~0xf;\t \n\treg |= 0x8;\n\treg &= ~(0x1 << 14);\t \n\twritel(reg, port_mmio + PHY_MODE9_GEN1);\n}\n\n \nstatic bool soc_is_65n(struct mv_host_priv *hpriv)\n{\n\tvoid __iomem *port0_mmio = mv_port_base(hpriv->base, 0);\n\n\tif (readl(port0_mmio + PHYCFG_OFS))\n\t\treturn true;\n\treturn false;\n}\n\nstatic void mv_setup_ifcfg(void __iomem *port_mmio, int want_gen2i)\n{\n\tu32 ifcfg = readl(port_mmio + SATA_IFCFG);\n\n\tifcfg = (ifcfg & 0xf7f) | 0x9b1000;\t \n\tif (want_gen2i)\n\t\tifcfg |= (1 << 7);\t\t \n\twritelfl(ifcfg, port_mmio + SATA_IFCFG);\n}\n\nstatic void mv_reset_channel(struct mv_host_priv *hpriv, void __iomem *mmio,\n\t\t\t     unsigned int port_no)\n{\n\tvoid __iomem *port_mmio = mv_port_base(mmio, port_no);\n\n\t \n\tmv_stop_edma_engine(port_mmio);\n\twritelfl(EDMA_RESET, port_mmio + EDMA_CMD);\n\n\tif (!IS_GEN_I(hpriv)) {\n\t\t \n\t\tmv_setup_ifcfg(port_mmio, 1);\n\t}\n\t \n\twritelfl(EDMA_RESET, port_mmio + EDMA_CMD);\n\tudelay(25);\t \n\twritelfl(0, port_mmio + EDMA_CMD);\n\n\thpriv->ops->phy_errata(hpriv, mmio, port_no);\n\n\tif (IS_GEN_I(hpriv))\n\t\tusleep_range(500, 1000);\n}\n\nstatic void mv_pmp_select(struct ata_port *ap, int pmp)\n{\n\tif (sata_pmp_supported(ap)) {\n\t\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\t\tu32 reg = readl(port_mmio + SATA_IFCTL);\n\t\tint old = reg & 0xf;\n\n\t\tif (old != pmp) {\n\t\t\treg = (reg & ~0xf) | pmp;\n\t\t\twritelfl(reg, port_mmio + SATA_IFCTL);\n\t\t}\n\t}\n}\n\nstatic int mv_pmp_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\t\tunsigned long deadline)\n{\n\tmv_pmp_select(link->ap, sata_srst_pmp(link));\n\treturn sata_std_hardreset(link, class, deadline);\n}\n\nstatic int mv_softreset(struct ata_link *link, unsigned int *class,\n\t\t\t\tunsigned long deadline)\n{\n\tmv_pmp_select(link->ap, sata_srst_pmp(link));\n\treturn ata_sff_softreset(link, class, deadline);\n}\n\nstatic int mv_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\tunsigned long deadline)\n{\n\tstruct ata_port *ap = link->ap;\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tstruct mv_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\tint rc, attempts = 0, extra = 0;\n\tu32 sstatus;\n\tbool online;\n\n\tmv_reset_channel(hpriv, mmio, ap->port_no);\n\tpp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\n\tpp->pp_flags &=\n\t  ~(MV_PP_FLAG_FBS_EN | MV_PP_FLAG_NCQ_EN | MV_PP_FLAG_FAKE_ATA_BUSY);\n\n\t \n\tdo {\n\t\tconst unsigned int *timing =\n\t\t\t\tsata_ehc_deb_timing(&link->eh_context);\n\n\t\trc = sata_link_hardreset(link, timing, deadline + extra,\n\t\t\t\t\t &online, NULL);\n\t\trc = online ? -EAGAIN : rc;\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tsata_scr_read(link, SCR_STATUS, &sstatus);\n\t\tif (!IS_GEN_I(hpriv) && ++attempts >= 5 && sstatus == 0x121) {\n\t\t\t \n\t\t\tmv_setup_ifcfg(mv_ap_base(ap), 0);\n\t\t\tif (time_after(jiffies + HZ, deadline))\n\t\t\t\textra = HZ;  \n\t\t}\n\t} while (sstatus != 0x0 && sstatus != 0x113 && sstatus != 0x123);\n\tmv_save_cached_regs(ap);\n\tmv_edma_cfg(ap, 0, 0);\n\n\treturn rc;\n}\n\nstatic void mv_eh_freeze(struct ata_port *ap)\n{\n\tmv_stop_edma(ap);\n\tmv_enable_port_irqs(ap, 0);\n}\n\nstatic void mv_eh_thaw(struct ata_port *ap)\n{\n\tstruct mv_host_priv *hpriv = ap->host->private_data;\n\tunsigned int port = ap->port_no;\n\tunsigned int hardport = mv_hardport_from_port(port);\n\tvoid __iomem *hc_mmio = mv_hc_base_from_port(hpriv->base, port);\n\tvoid __iomem *port_mmio = mv_ap_base(ap);\n\tu32 hc_irq_cause;\n\n\t \n\twritel(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\n\n\t \n\thc_irq_cause = ~((DEV_IRQ | DMA_IRQ) << hardport);\n\twritelfl(hc_irq_cause, hc_mmio + HC_IRQ_CAUSE);\n\n\tmv_enable_port_irqs(ap, ERR_IRQ);\n}\n\n \nstatic void mv_port_init(struct ata_ioports *port,  void __iomem *port_mmio)\n{\n\tvoid __iomem *serr, *shd_base = port_mmio + SHD_BLK;\n\n\t \n\tport->data_addr = shd_base + (sizeof(u32) * ATA_REG_DATA);\n\tport->error_addr =\n\t\tport->feature_addr = shd_base + (sizeof(u32) * ATA_REG_ERR);\n\tport->nsect_addr = shd_base + (sizeof(u32) * ATA_REG_NSECT);\n\tport->lbal_addr = shd_base + (sizeof(u32) * ATA_REG_LBAL);\n\tport->lbam_addr = shd_base + (sizeof(u32) * ATA_REG_LBAM);\n\tport->lbah_addr = shd_base + (sizeof(u32) * ATA_REG_LBAH);\n\tport->device_addr = shd_base + (sizeof(u32) * ATA_REG_DEVICE);\n\tport->status_addr =\n\t\tport->command_addr = shd_base + (sizeof(u32) * ATA_REG_STATUS);\n\t \n\tport->altstatus_addr = port->ctl_addr = shd_base + SHD_CTL_AST;\n\n\t \n\tserr = port_mmio + mv_scr_offset(SCR_ERROR);\n\twritelfl(readl(serr), serr);\n\twritelfl(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\n\n\t \n\twritelfl(~EDMA_ERR_IRQ_TRANSIENT, port_mmio + EDMA_ERR_IRQ_MASK);\n}\n\nstatic unsigned int mv_in_pcix_mode(struct ata_host *host)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\tu32 reg;\n\n\tif (IS_SOC(hpriv) || !IS_PCIE(hpriv))\n\t\treturn 0;\t \n\treg = readl(mmio + MV_PCI_MODE);\n\tif ((reg & MV_PCI_MODE_MASK) == 0)\n\t\treturn 0;\t \n\treturn 1;\t \n}\n\nstatic int mv_pci_cut_through_okay(struct ata_host *host)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\tu32 reg;\n\n\tif (!mv_in_pcix_mode(host)) {\n\t\treg = readl(mmio + MV_PCI_COMMAND);\n\t\tif (reg & MV_PCI_COMMAND_MRDTRIG)\n\t\t\treturn 0;  \n\t}\n\treturn 1;  \n}\n\nstatic void mv_60x1b2_errata_pci7(struct ata_host *host)\n{\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\n\t \n\tif (mv_in_pcix_mode(host)) {\n\t\tu32 reg = readl(mmio + MV_PCI_COMMAND);\n\t\twritelfl(reg & ~MV_PCI_COMMAND_MWRCOM, mmio + MV_PCI_COMMAND);\n\t}\n}\n\nstatic int mv_chip_id(struct ata_host *host, unsigned int board_idx)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tu32 hp_flags = hpriv->hp_flags;\n\n\tswitch (board_idx) {\n\tcase chip_5080:\n\t\thpriv->ops = &mv5xxx_ops;\n\t\thp_flags |= MV_HP_GEN_I;\n\n\t\tswitch (pdev->revision) {\n\t\tcase 0x1:\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB0;\n\t\t\tbreak;\n\t\tcase 0x3:\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Applying 50XXB2 workarounds to unknown rev\\n\");\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB2;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase chip_504x:\n\tcase chip_508x:\n\t\thpriv->ops = &mv5xxx_ops;\n\t\thp_flags |= MV_HP_GEN_I;\n\n\t\tswitch (pdev->revision) {\n\t\tcase 0x0:\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB0;\n\t\t\tbreak;\n\t\tcase 0x3:\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Applying B2 workarounds to unknown rev\\n\");\n\t\t\thp_flags |= MV_HP_ERRATA_50XXB2;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase chip_604x:\n\tcase chip_608x:\n\t\thpriv->ops = &mv6xxx_ops;\n\t\thp_flags |= MV_HP_GEN_II;\n\n\t\tswitch (pdev->revision) {\n\t\tcase 0x7:\n\t\t\tmv_60x1b2_errata_pci7(host);\n\t\t\thp_flags |= MV_HP_ERRATA_60X1B2;\n\t\t\tbreak;\n\t\tcase 0x9:\n\t\t\thp_flags |= MV_HP_ERRATA_60X1C0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Applying B2 workarounds to unknown rev\\n\");\n\t\t\thp_flags |= MV_HP_ERRATA_60X1B2;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase chip_7042:\n\t\thp_flags |= MV_HP_PCIE | MV_HP_CUT_THROUGH;\n\t\tif (pdev->vendor == PCI_VENDOR_ID_TTI &&\n\t\t    (pdev->device == 0x2300 || pdev->device == 0x2310))\n\t\t{\n\t\t\t \n\t\t\tdev_warn(&pdev->dev, \"Highpoint RocketRAID\"\n\t\t\t\t\" BIOS CORRUPTS DATA on all attached drives,\"\n\t\t\t\t\" regardless of if/how they are configured.\"\n\t\t\t\t\" BEWARE!\\n\");\n\t\t\tdev_warn(&pdev->dev, \"For data safety, do not\"\n\t\t\t\t\" use sectors 8-9 on \\\"Legacy\\\" drives,\"\n\t\t\t\t\" and avoid the final two gigabytes on\"\n\t\t\t\t\" all RocketRAID BIOS initialized drives.\\n\");\n\t\t}\n\t\tfallthrough;\n\tcase chip_6042:\n\t\thpriv->ops = &mv6xxx_ops;\n\t\thp_flags |= MV_HP_GEN_IIE;\n\t\tif (board_idx == chip_6042 && mv_pci_cut_through_okay(host))\n\t\t\thp_flags |= MV_HP_CUT_THROUGH;\n\n\t\tswitch (pdev->revision) {\n\t\tcase 0x2:  \n\t\t\thp_flags |= MV_HP_ERRATA_60X1C0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Applying 60X1C0 workarounds to unknown rev\\n\");\n\t\t\thp_flags |= MV_HP_ERRATA_60X1C0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase chip_soc:\n\t\tif (soc_is_65n(hpriv))\n\t\t\thpriv->ops = &mv_soc_65n_ops;\n\t\telse\n\t\t\thpriv->ops = &mv_soc_ops;\n\t\thp_flags |= MV_HP_FLAG_SOC | MV_HP_GEN_IIE |\n\t\t\tMV_HP_ERRATA_60X1C0;\n\t\tbreak;\n\n\tdefault:\n\t\tdev_alert(host->dev, \"BUG: invalid board index %u\\n\", board_idx);\n\t\treturn -EINVAL;\n\t}\n\n\thpriv->hp_flags = hp_flags;\n\tif (hp_flags & MV_HP_PCIE) {\n\t\thpriv->irq_cause_offset\t= PCIE_IRQ_CAUSE;\n\t\thpriv->irq_mask_offset\t= PCIE_IRQ_MASK;\n\t\thpriv->unmask_all_irqs\t= PCIE_UNMASK_ALL_IRQS;\n\t} else {\n\t\thpriv->irq_cause_offset\t= PCI_IRQ_CAUSE;\n\t\thpriv->irq_mask_offset\t= PCI_IRQ_MASK;\n\t\thpriv->unmask_all_irqs\t= PCI_UNMASK_ALL_IRQS;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mv_init_host(struct ata_host *host)\n{\n\tint rc = 0, n_hc, port, hc;\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tvoid __iomem *mmio = hpriv->base;\n\n\trc = mv_chip_id(host, hpriv->board_idx);\n\tif (rc)\n\t\tgoto done;\n\n\tif (IS_SOC(hpriv)) {\n\t\thpriv->main_irq_cause_addr = mmio + SOC_HC_MAIN_IRQ_CAUSE;\n\t\thpriv->main_irq_mask_addr  = mmio + SOC_HC_MAIN_IRQ_MASK;\n\t} else {\n\t\thpriv->main_irq_cause_addr = mmio + PCI_HC_MAIN_IRQ_CAUSE;\n\t\thpriv->main_irq_mask_addr  = mmio + PCI_HC_MAIN_IRQ_MASK;\n\t}\n\n\t \n\thpriv->main_irq_mask = readl(hpriv->main_irq_mask_addr);\n\n\t \n\tmv_set_main_irq_mask(host, ~0, 0);\n\n\tn_hc = mv_get_hc_count(host->ports[0]->flags);\n\n\tfor (port = 0; port < host->n_ports; port++)\n\t\tif (hpriv->ops->read_preamp)\n\t\t\thpriv->ops->read_preamp(hpriv, port, mmio);\n\n\trc = hpriv->ops->reset_hc(host, mmio, n_hc);\n\tif (rc)\n\t\tgoto done;\n\n\thpriv->ops->reset_flash(hpriv, mmio);\n\thpriv->ops->reset_bus(host, mmio);\n\thpriv->ops->enable_leds(hpriv, mmio);\n\n\tfor (port = 0; port < host->n_ports; port++) {\n\t\tstruct ata_port *ap = host->ports[port];\n\t\tvoid __iomem *port_mmio = mv_port_base(mmio, port);\n\n\t\tmv_port_init(&ap->ioaddr, port_mmio);\n\t}\n\n\tfor (hc = 0; hc < n_hc; hc++) {\n\t\tvoid __iomem *hc_mmio = mv_hc_base(mmio, hc);\n\n\t\tdev_dbg(host->dev, \"HC%i: HC config=0x%08x HC IRQ cause \"\n\t\t\t\"(before clear)=0x%08x\\n\", hc,\n\t\t\treadl(hc_mmio + HC_CFG),\n\t\t\treadl(hc_mmio + HC_IRQ_CAUSE));\n\n\t\t \n\t\twritelfl(0, hc_mmio + HC_IRQ_CAUSE);\n\t}\n\n\tif (!IS_SOC(hpriv)) {\n\t\t \n\t\twritelfl(0, mmio + hpriv->irq_cause_offset);\n\n\t\t \n\t\twritelfl(hpriv->unmask_all_irqs, mmio + hpriv->irq_mask_offset);\n\t}\n\n\t \n\tmv_set_main_irq_mask(host, 0, PCI_ERR);\n\tmv_set_irq_coalescing(host, irq_coalescing_io_count,\n\t\t\t\t    irq_coalescing_usecs);\ndone:\n\treturn rc;\n}\n\nstatic int mv_create_dma_pools(struct mv_host_priv *hpriv, struct device *dev)\n{\n\thpriv->crqb_pool   = dmam_pool_create(\"crqb_q\", dev, MV_CRQB_Q_SZ,\n\t\t\t\t\t\t\t     MV_CRQB_Q_SZ, 0);\n\tif (!hpriv->crqb_pool)\n\t\treturn -ENOMEM;\n\n\thpriv->crpb_pool   = dmam_pool_create(\"crpb_q\", dev, MV_CRPB_Q_SZ,\n\t\t\t\t\t\t\t     MV_CRPB_Q_SZ, 0);\n\tif (!hpriv->crpb_pool)\n\t\treturn -ENOMEM;\n\n\thpriv->sg_tbl_pool = dmam_pool_create(\"sg_tbl\", dev, MV_SG_TBL_SZ,\n\t\t\t\t\t\t\t     MV_SG_TBL_SZ, 0);\n\tif (!hpriv->sg_tbl_pool)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void mv_conf_mbus_windows(struct mv_host_priv *hpriv,\n\t\t\t\t const struct mbus_dram_target_info *dram)\n{\n\tint i;\n\n\tfor (i = 0; i < 4; i++) {\n\t\twritel(0, hpriv->base + WINDOW_CTRL(i));\n\t\twritel(0, hpriv->base + WINDOW_BASE(i));\n\t}\n\n\tfor (i = 0; i < dram->num_cs; i++) {\n\t\tconst struct mbus_dram_window *cs = dram->cs + i;\n\n\t\twritel(((cs->size - 1) & 0xffff0000) |\n\t\t\t(cs->mbus_attr << 8) |\n\t\t\t(dram->mbus_dram_target_id << 4) | 1,\n\t\t\thpriv->base + WINDOW_CTRL(i));\n\t\twritel(cs->base, hpriv->base + WINDOW_BASE(i));\n\t}\n}\n\n \nstatic int mv_platform_probe(struct platform_device *pdev)\n{\n\tconst struct mv_sata_platform_data *mv_platform_data;\n\tconst struct mbus_dram_target_info *dram;\n\tconst struct ata_port_info *ppi[] =\n\t    { &mv_port_info[chip_soc], NULL };\n\tstruct ata_host *host;\n\tstruct mv_host_priv *hpriv;\n\tstruct resource *res;\n\tint n_ports = 0, irq = 0;\n\tint rc;\n\tint port;\n\n\tata_print_version_once(&pdev->dev, DRV_VERSION);\n\n\t \n\tif (unlikely(pdev->num_resources != 1)) {\n\t\tdev_err(&pdev->dev, \"invalid number of resources\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (res == NULL)\n\t\treturn -EINVAL;\n\n\t \n\tif (pdev->dev.of_node) {\n\t\trc = of_property_read_u32(pdev->dev.of_node, \"nr-ports\",\n\t\t\t\t\t   &n_ports);\n\t\tif (rc) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"error parsing nr-ports property: %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\n\t\tif (n_ports <= 0) {\n\t\t\tdev_err(&pdev->dev, \"nr-ports must be positive: %d\\n\",\n\t\t\t\tn_ports);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tirq = irq_of_parse_and_map(pdev->dev.of_node, 0);\n\t} else {\n\t\tmv_platform_data = dev_get_platdata(&pdev->dev);\n\t\tn_ports = mv_platform_data->n_ports;\n\t\tirq = platform_get_irq(pdev, 0);\n\t}\n\tif (irq < 0)\n\t\treturn irq;\n\tif (!irq)\n\t\treturn -EINVAL;\n\n\thost = ata_host_alloc_pinfo(&pdev->dev, ppi, n_ports);\n\thpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\n\n\tif (!host || !hpriv)\n\t\treturn -ENOMEM;\n\thpriv->port_clks = devm_kcalloc(&pdev->dev,\n\t\t\t\t\tn_ports, sizeof(struct clk *),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!hpriv->port_clks)\n\t\treturn -ENOMEM;\n\thpriv->port_phys = devm_kcalloc(&pdev->dev,\n\t\t\t\t\tn_ports, sizeof(struct phy *),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!hpriv->port_phys)\n\t\treturn -ENOMEM;\n\thost->private_data = hpriv;\n\thpriv->board_idx = chip_soc;\n\n\thost->iomap = NULL;\n\thpriv->base = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t   resource_size(res));\n\tif (!hpriv->base)\n\t\treturn -ENOMEM;\n\n\thpriv->base -= SATAHC0_REG_BASE;\n\n\thpriv->clk = clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(hpriv->clk))\n\t\tdev_notice(&pdev->dev, \"cannot get optional clkdev\\n\");\n\telse\n\t\tclk_prepare_enable(hpriv->clk);\n\n\tfor (port = 0; port < n_ports; port++) {\n\t\tchar port_number[16];\n\t\tsprintf(port_number, \"%d\", port);\n\t\thpriv->port_clks[port] = clk_get(&pdev->dev, port_number);\n\t\tif (!IS_ERR(hpriv->port_clks[port]))\n\t\t\tclk_prepare_enable(hpriv->port_clks[port]);\n\n\t\tsprintf(port_number, \"port%d\", port);\n\t\thpriv->port_phys[port] = devm_phy_optional_get(&pdev->dev,\n\t\t\t\t\t\t\t       port_number);\n\t\tif (IS_ERR(hpriv->port_phys[port])) {\n\t\t\trc = PTR_ERR(hpriv->port_phys[port]);\n\t\t\thpriv->port_phys[port] = NULL;\n\t\t\tif (rc != -EPROBE_DEFER)\n\t\t\t\tdev_warn(&pdev->dev, \"error getting phy %d\", rc);\n\n\t\t\t \n\t\t\thpriv->n_ports = port;\n\t\t\tgoto err;\n\t\t} else\n\t\t\tphy_power_on(hpriv->port_phys[port]);\n\t}\n\n\t \n\thpriv->n_ports = n_ports;\n\n\t \n\tdram = mv_mbus_dram_info();\n\tif (dram)\n\t\tmv_conf_mbus_windows(hpriv, dram);\n\n\trc = mv_create_dma_pools(hpriv, &pdev->dev);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tif (pdev->dev.of_node &&\n\t\tof_device_is_compatible(pdev->dev.of_node,\n\t\t\t\t\t\"marvell,armada-370-sata\"))\n\t\thpriv->hp_flags |= MV_HP_FIX_LP_PHY_CTL;\n\n\t \n\trc = mv_init_host(host);\n\tif (rc)\n\t\tgoto err;\n\n\tdev_info(&pdev->dev, \"slots %u ports %d\\n\",\n\t\t (unsigned)MV_MAX_Q_DEPTH, host->n_ports);\n\n\trc = ata_host_activate(host, irq, mv_interrupt, IRQF_SHARED, &mv6_sht);\n\tif (!rc)\n\t\treturn 0;\n\nerr:\n\tif (!IS_ERR(hpriv->clk)) {\n\t\tclk_disable_unprepare(hpriv->clk);\n\t\tclk_put(hpriv->clk);\n\t}\n\tfor (port = 0; port < hpriv->n_ports; port++) {\n\t\tif (!IS_ERR(hpriv->port_clks[port])) {\n\t\t\tclk_disable_unprepare(hpriv->port_clks[port]);\n\t\t\tclk_put(hpriv->port_clks[port]);\n\t\t}\n\t\tphy_power_off(hpriv->port_phys[port]);\n\t}\n\n\treturn rc;\n}\n\n \nstatic void mv_platform_remove(struct platform_device *pdev)\n{\n\tstruct ata_host *host = platform_get_drvdata(pdev);\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tint port;\n\tata_host_detach(host);\n\n\tif (!IS_ERR(hpriv->clk)) {\n\t\tclk_disable_unprepare(hpriv->clk);\n\t\tclk_put(hpriv->clk);\n\t}\n\tfor (port = 0; port < host->n_ports; port++) {\n\t\tif (!IS_ERR(hpriv->port_clks[port])) {\n\t\t\tclk_disable_unprepare(hpriv->port_clks[port]);\n\t\t\tclk_put(hpriv->port_clks[port]);\n\t\t}\n\t\tphy_power_off(hpriv->port_phys[port]);\n\t}\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int mv_platform_suspend(struct platform_device *pdev, pm_message_t state)\n{\n\tstruct ata_host *host = platform_get_drvdata(pdev);\n\n\tif (host)\n\t\tata_host_suspend(host, state);\n\treturn 0;\n}\n\nstatic int mv_platform_resume(struct platform_device *pdev)\n{\n\tstruct ata_host *host = platform_get_drvdata(pdev);\n\tconst struct mbus_dram_target_info *dram;\n\tint ret;\n\n\tif (host) {\n\t\tstruct mv_host_priv *hpriv = host->private_data;\n\n\t\t \n\t\tdram = mv_mbus_dram_info();\n\t\tif (dram)\n\t\t\tmv_conf_mbus_windows(hpriv, dram);\n\n\t\t \n\t\tret = mv_init_host(host);\n\t\tif (ret) {\n\t\t\tdev_err(&pdev->dev, \"Error during HW init\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tata_host_resume(host);\n\t}\n\n\treturn 0;\n}\n#else\n#define mv_platform_suspend NULL\n#define mv_platform_resume NULL\n#endif\n\n#ifdef CONFIG_OF\nstatic const struct of_device_id mv_sata_dt_ids[] = {\n\t{ .compatible = \"marvell,armada-370-sata\", },\n\t{ .compatible = \"marvell,orion-sata\", },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, mv_sata_dt_ids);\n#endif\n\nstatic struct platform_driver mv_platform_driver = {\n\t.probe\t\t= mv_platform_probe,\n\t.remove_new\t= mv_platform_remove,\n\t.suspend\t= mv_platform_suspend,\n\t.resume\t\t= mv_platform_resume,\n\t.driver\t\t= {\n\t\t.name = DRV_NAME,\n\t\t.of_match_table = of_match_ptr(mv_sata_dt_ids),\n\t},\n};\n\n\n#ifdef CONFIG_PCI\nstatic int mv_pci_init_one(struct pci_dev *pdev,\n\t\t\t   const struct pci_device_id *ent);\n#ifdef CONFIG_PM_SLEEP\nstatic int mv_pci_device_resume(struct pci_dev *pdev);\n#endif\n\n\nstatic struct pci_driver mv_pci_driver = {\n\t.name\t\t\t= DRV_NAME,\n\t.id_table\t\t= mv_pci_tbl,\n\t.probe\t\t\t= mv_pci_init_one,\n\t.remove\t\t\t= ata_pci_remove_one,\n#ifdef CONFIG_PM_SLEEP\n\t.suspend\t\t= ata_pci_device_suspend,\n\t.resume\t\t\t= mv_pci_device_resume,\n#endif\n\n};\n\n \nstatic void mv_print_info(struct ata_host *host)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tstruct mv_host_priv *hpriv = host->private_data;\n\tu8 scc;\n\tconst char *scc_s, *gen;\n\n\t \n\tpci_read_config_byte(pdev, PCI_CLASS_DEVICE, &scc);\n\tif (scc == 0)\n\t\tscc_s = \"SCSI\";\n\telse if (scc == 0x01)\n\t\tscc_s = \"RAID\";\n\telse\n\t\tscc_s = \"?\";\n\n\tif (IS_GEN_I(hpriv))\n\t\tgen = \"I\";\n\telse if (IS_GEN_II(hpriv))\n\t\tgen = \"II\";\n\telse if (IS_GEN_IIE(hpriv))\n\t\tgen = \"IIE\";\n\telse\n\t\tgen = \"?\";\n\n\tdev_info(&pdev->dev, \"Gen-%s %u slots %u ports %s mode IRQ via %s\\n\",\n\t\t gen, (unsigned)MV_MAX_Q_DEPTH, host->n_ports,\n\t\t scc_s, (MV_HP_FLAG_MSI & hpriv->hp_flags) ? \"MSI\" : \"INTx\");\n}\n\n \nstatic int mv_pci_init_one(struct pci_dev *pdev,\n\t\t\t   const struct pci_device_id *ent)\n{\n\tunsigned int board_idx = (unsigned int)ent->driver_data;\n\tconst struct ata_port_info *ppi[] = { &mv_port_info[board_idx], NULL };\n\tstruct ata_host *host;\n\tstruct mv_host_priv *hpriv;\n\tint n_ports, port, rc;\n\n\tata_print_version_once(&pdev->dev, DRV_VERSION);\n\n\t \n\tn_ports = mv_get_hc_count(ppi[0]->flags) * MV_PORTS_PER_HC;\n\n\thost = ata_host_alloc_pinfo(&pdev->dev, ppi, n_ports);\n\thpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\n\tif (!host || !hpriv)\n\t\treturn -ENOMEM;\n\thost->private_data = hpriv;\n\thpriv->n_ports = n_ports;\n\thpriv->board_idx = board_idx;\n\n\t \n\trc = pcim_enable_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pcim_iomap_regions(pdev, 1 << MV_PRIMARY_BAR, DRV_NAME);\n\tif (rc == -EBUSY)\n\t\tpcim_pin_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\thost->iomap = pcim_iomap_table(pdev);\n\thpriv->base = host->iomap[MV_PRIMARY_BAR];\n\n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"DMA enable failed\\n\");\n\t\treturn rc;\n\t}\n\n\trc = mv_create_dma_pools(hpriv, &pdev->dev);\n\tif (rc)\n\t\treturn rc;\n\n\tfor (port = 0; port < host->n_ports; port++) {\n\t\tstruct ata_port *ap = host->ports[port];\n\t\tvoid __iomem *port_mmio = mv_port_base(hpriv->base, port);\n\t\tunsigned int offset = port_mmio - hpriv->base;\n\n\t\tata_port_pbar_desc(ap, MV_PRIMARY_BAR, -1, \"mmio\");\n\t\tata_port_pbar_desc(ap, MV_PRIMARY_BAR, offset, \"port\");\n\t}\n\n\t \n\trc = mv_init_host(host);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (msi && pci_enable_msi(pdev) == 0)\n\t\thpriv->hp_flags |= MV_HP_FLAG_MSI;\n\n\tmv_dump_pci_cfg(pdev, 0x68);\n\tmv_print_info(host);\n\n\tpci_set_master(pdev);\n\tpci_try_set_mwi(pdev);\n\treturn ata_host_activate(host, pdev->irq, mv_interrupt, IRQF_SHARED,\n\t\t\t\t IS_GEN_I(hpriv) ? &mv5_sht : &mv6_sht);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int mv_pci_device_resume(struct pci_dev *pdev)\n{\n\tstruct ata_host *host = pci_get_drvdata(pdev);\n\tint rc;\n\n\trc = ata_pci_device_do_resume(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = mv_init_host(host);\n\tif (rc)\n\t\treturn rc;\n\n\tata_host_resume(host);\n\n\treturn 0;\n}\n#endif\n#endif\n\nstatic int __init mv_init(void)\n{\n\tint rc = -ENODEV;\n#ifdef CONFIG_PCI\n\trc = pci_register_driver(&mv_pci_driver);\n\tif (rc < 0)\n\t\treturn rc;\n#endif\n\trc = platform_driver_register(&mv_platform_driver);\n\n#ifdef CONFIG_PCI\n\tif (rc < 0)\n\t\tpci_unregister_driver(&mv_pci_driver);\n#endif\n\treturn rc;\n}\n\nstatic void __exit mv_exit(void)\n{\n#ifdef CONFIG_PCI\n\tpci_unregister_driver(&mv_pci_driver);\n#endif\n\tplatform_driver_unregister(&mv_platform_driver);\n}\n\nMODULE_AUTHOR(\"Brett Russ\");\nMODULE_DESCRIPTION(\"SCSI low-level driver for Marvell SATA controllers\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DEVICE_TABLE(pci, mv_pci_tbl);\nMODULE_VERSION(DRV_VERSION);\nMODULE_ALIAS(\"platform:\" DRV_NAME);\n\nmodule_init(mv_init);\nmodule_exit(mv_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}