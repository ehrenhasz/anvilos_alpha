{
  "module_name": "libata-sff.c",
  "hash_id": "9fde2ed5141954864b49c7e4c72a6d3245093cfa54fa5d8df18c453540c8d788",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ata/libata-sff.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/gfp.h>\n#include <linux/pci.h>\n#include <linux/module.h>\n#include <linux/libata.h>\n#include <linux/highmem.h>\n#include <trace/events/libata.h>\n#include \"libata.h\"\n\nstatic struct workqueue_struct *ata_sff_wq;\n\nconst struct ata_port_operations ata_sff_port_ops = {\n\t.inherits\t\t= &ata_base_port_ops,\n\n\t.qc_prep\t\t= ata_noop_qc_prep,\n\t.qc_issue\t\t= ata_sff_qc_issue,\n\t.qc_fill_rtf\t\t= ata_sff_qc_fill_rtf,\n\n\t.freeze\t\t\t= ata_sff_freeze,\n\t.thaw\t\t\t= ata_sff_thaw,\n\t.prereset\t\t= ata_sff_prereset,\n\t.softreset\t\t= ata_sff_softreset,\n\t.hardreset\t\t= sata_sff_hardreset,\n\t.postreset\t\t= ata_sff_postreset,\n\t.error_handler\t\t= ata_sff_error_handler,\n\n\t.sff_dev_select\t\t= ata_sff_dev_select,\n\t.sff_check_status\t= ata_sff_check_status,\n\t.sff_tf_load\t\t= ata_sff_tf_load,\n\t.sff_tf_read\t\t= ata_sff_tf_read,\n\t.sff_exec_command\t= ata_sff_exec_command,\n\t.sff_data_xfer\t\t= ata_sff_data_xfer,\n\t.sff_drain_fifo\t\t= ata_sff_drain_fifo,\n\n\t.lost_interrupt\t\t= ata_sff_lost_interrupt,\n};\nEXPORT_SYMBOL_GPL(ata_sff_port_ops);\n\n \nu8 ata_sff_check_status(struct ata_port *ap)\n{\n\treturn ioread8(ap->ioaddr.status_addr);\n}\nEXPORT_SYMBOL_GPL(ata_sff_check_status);\n\n \nstatic bool ata_sff_altstatus(struct ata_port *ap, u8 *status)\n{\n\tu8 tmp;\n\n\tif (ap->ops->sff_check_altstatus) {\n\t\ttmp = ap->ops->sff_check_altstatus(ap);\n\t\tgoto read;\n\t}\n\tif (ap->ioaddr.altstatus_addr) {\n\t\ttmp = ioread8(ap->ioaddr.altstatus_addr);\n\t\tgoto read;\n\t}\n\treturn false;\n\nread:\n\tif (status)\n\t\t*status = tmp;\n\treturn true;\n}\n\n \nstatic u8 ata_sff_irq_status(struct ata_port *ap)\n{\n\tu8 status;\n\n\t \n\tif (ata_sff_altstatus(ap, &status) && (status & ATA_BUSY))\n\t\treturn status;\n\t \n\tstatus = ap->ops->sff_check_status(ap);\n\treturn status;\n}\n\n \n\nstatic void ata_sff_sync(struct ata_port *ap)\n{\n\tata_sff_altstatus(ap, NULL);\n}\n\n \n\nvoid ata_sff_pause(struct ata_port *ap)\n{\n\tata_sff_sync(ap);\n\tndelay(400);\n}\nEXPORT_SYMBOL_GPL(ata_sff_pause);\n\n \n\nvoid ata_sff_dma_pause(struct ata_port *ap)\n{\n\t \n\tif (ata_sff_altstatus(ap, NULL))\n\t\treturn;\n\t \n\tBUG();\n}\nEXPORT_SYMBOL_GPL(ata_sff_dma_pause);\n\nstatic int ata_sff_check_ready(struct ata_link *link)\n{\n\tu8 status = link->ap->ops->sff_check_status(link->ap);\n\n\treturn ata_check_ready(status);\n}\n\n \nint ata_sff_wait_ready(struct ata_link *link, unsigned long deadline)\n{\n\treturn ata_wait_ready(link, deadline, ata_sff_check_ready);\n}\nEXPORT_SYMBOL_GPL(ata_sff_wait_ready);\n\n \nstatic bool ata_sff_set_devctl(struct ata_port *ap, u8 ctl)\n{\n\tif (ap->ops->sff_set_devctl) {\n\t\tap->ops->sff_set_devctl(ap, ctl);\n\t\treturn true;\n\t}\n\tif (ap->ioaddr.ctl_addr) {\n\t\tiowrite8(ctl, ap->ioaddr.ctl_addr);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid ata_sff_dev_select(struct ata_port *ap, unsigned int device)\n{\n\tu8 tmp;\n\n\tif (device == 0)\n\t\ttmp = ATA_DEVICE_OBS;\n\telse\n\t\ttmp = ATA_DEVICE_OBS | ATA_DEV1;\n\n\tiowrite8(tmp, ap->ioaddr.device_addr);\n\tata_sff_pause(ap);\t \n}\nEXPORT_SYMBOL_GPL(ata_sff_dev_select);\n\n \nstatic void ata_dev_select(struct ata_port *ap, unsigned int device,\n\t\t\t   unsigned int wait, unsigned int can_sleep)\n{\n\tif (wait)\n\t\tata_wait_idle(ap);\n\n\tap->ops->sff_dev_select(ap, device);\n\n\tif (wait) {\n\t\tif (can_sleep && ap->link.device[device].class == ATA_DEV_ATAPI)\n\t\t\tata_msleep(ap, 150);\n\t\tata_wait_idle(ap);\n\t}\n}\n\n \nvoid ata_sff_irq_on(struct ata_port *ap)\n{\n\tif (ap->ops->sff_irq_on) {\n\t\tap->ops->sff_irq_on(ap);\n\t\treturn;\n\t}\n\n\tap->ctl &= ~ATA_NIEN;\n\tap->last_ctl = ap->ctl;\n\n\tata_sff_set_devctl(ap, ap->ctl);\n\tata_wait_idle(ap);\n\n\tif (ap->ops->sff_irq_clear)\n\t\tap->ops->sff_irq_clear(ap);\n}\nEXPORT_SYMBOL_GPL(ata_sff_irq_on);\n\n \nvoid ata_sff_tf_load(struct ata_port *ap, const struct ata_taskfile *tf)\n{\n\tstruct ata_ioports *ioaddr = &ap->ioaddr;\n\tunsigned int is_addr = tf->flags & ATA_TFLAG_ISADDR;\n\n\tif (tf->ctl != ap->last_ctl) {\n\t\tif (ioaddr->ctl_addr)\n\t\t\tiowrite8(tf->ctl, ioaddr->ctl_addr);\n\t\tap->last_ctl = tf->ctl;\n\t\tata_wait_idle(ap);\n\t}\n\n\tif (is_addr && (tf->flags & ATA_TFLAG_LBA48)) {\n\t\tWARN_ON_ONCE(!ioaddr->ctl_addr);\n\t\tiowrite8(tf->hob_feature, ioaddr->feature_addr);\n\t\tiowrite8(tf->hob_nsect, ioaddr->nsect_addr);\n\t\tiowrite8(tf->hob_lbal, ioaddr->lbal_addr);\n\t\tiowrite8(tf->hob_lbam, ioaddr->lbam_addr);\n\t\tiowrite8(tf->hob_lbah, ioaddr->lbah_addr);\n\t}\n\n\tif (is_addr) {\n\t\tiowrite8(tf->feature, ioaddr->feature_addr);\n\t\tiowrite8(tf->nsect, ioaddr->nsect_addr);\n\t\tiowrite8(tf->lbal, ioaddr->lbal_addr);\n\t\tiowrite8(tf->lbam, ioaddr->lbam_addr);\n\t\tiowrite8(tf->lbah, ioaddr->lbah_addr);\n\t}\n\n\tif (tf->flags & ATA_TFLAG_DEVICE)\n\t\tiowrite8(tf->device, ioaddr->device_addr);\n\n\tata_wait_idle(ap);\n}\nEXPORT_SYMBOL_GPL(ata_sff_tf_load);\n\n \nvoid ata_sff_tf_read(struct ata_port *ap, struct ata_taskfile *tf)\n{\n\tstruct ata_ioports *ioaddr = &ap->ioaddr;\n\n\ttf->status = ata_sff_check_status(ap);\n\ttf->error = ioread8(ioaddr->error_addr);\n\ttf->nsect = ioread8(ioaddr->nsect_addr);\n\ttf->lbal = ioread8(ioaddr->lbal_addr);\n\ttf->lbam = ioread8(ioaddr->lbam_addr);\n\ttf->lbah = ioread8(ioaddr->lbah_addr);\n\ttf->device = ioread8(ioaddr->device_addr);\n\n\tif (tf->flags & ATA_TFLAG_LBA48) {\n\t\tif (likely(ioaddr->ctl_addr)) {\n\t\t\tiowrite8(tf->ctl | ATA_HOB, ioaddr->ctl_addr);\n\t\t\ttf->hob_feature = ioread8(ioaddr->error_addr);\n\t\t\ttf->hob_nsect = ioread8(ioaddr->nsect_addr);\n\t\t\ttf->hob_lbal = ioread8(ioaddr->lbal_addr);\n\t\t\ttf->hob_lbam = ioread8(ioaddr->lbam_addr);\n\t\t\ttf->hob_lbah = ioread8(ioaddr->lbah_addr);\n\t\t\tiowrite8(tf->ctl, ioaddr->ctl_addr);\n\t\t\tap->last_ctl = tf->ctl;\n\t\t} else\n\t\t\tWARN_ON_ONCE(1);\n\t}\n}\nEXPORT_SYMBOL_GPL(ata_sff_tf_read);\n\n \nvoid ata_sff_exec_command(struct ata_port *ap, const struct ata_taskfile *tf)\n{\n\tiowrite8(tf->command, ap->ioaddr.command_addr);\n\tata_sff_pause(ap);\n}\nEXPORT_SYMBOL_GPL(ata_sff_exec_command);\n\n \nstatic inline void ata_tf_to_host(struct ata_port *ap,\n\t\t\t\t  const struct ata_taskfile *tf,\n\t\t\t\t  unsigned int tag)\n{\n\ttrace_ata_tf_load(ap, tf);\n\tap->ops->sff_tf_load(ap, tf);\n\ttrace_ata_exec_command(ap, tf, tag);\n\tap->ops->sff_exec_command(ap, tf);\n}\n\n \nunsigned int ata_sff_data_xfer(struct ata_queued_cmd *qc, unsigned char *buf,\n\t\t\t       unsigned int buflen, int rw)\n{\n\tstruct ata_port *ap = qc->dev->link->ap;\n\tvoid __iomem *data_addr = ap->ioaddr.data_addr;\n\tunsigned int words = buflen >> 1;\n\n\t \n\tif (rw == READ)\n\t\tioread16_rep(data_addr, buf, words);\n\telse\n\t\tiowrite16_rep(data_addr, buf, words);\n\n\t \n\tif (unlikely(buflen & 0x01)) {\n\t\tunsigned char pad[2] = { };\n\n\t\t \n\t\tbuf += buflen - 1;\n\n\t\t \n\t\tif (rw == READ) {\n\t\t\tioread16_rep(data_addr, pad, 1);\n\t\t\t*buf = pad[0];\n\t\t} else {\n\t\t\tpad[0] = *buf;\n\t\t\tiowrite16_rep(data_addr, pad, 1);\n\t\t}\n\t\twords++;\n\t}\n\n\treturn words << 1;\n}\nEXPORT_SYMBOL_GPL(ata_sff_data_xfer);\n\n \n\nunsigned int ata_sff_data_xfer32(struct ata_queued_cmd *qc, unsigned char *buf,\n\t\t\t       unsigned int buflen, int rw)\n{\n\tstruct ata_device *dev = qc->dev;\n\tstruct ata_port *ap = dev->link->ap;\n\tvoid __iomem *data_addr = ap->ioaddr.data_addr;\n\tunsigned int words = buflen >> 2;\n\tint slop = buflen & 3;\n\n\tif (!(ap->pflags & ATA_PFLAG_PIO32))\n\t\treturn ata_sff_data_xfer(qc, buf, buflen, rw);\n\n\t \n\tif (rw == READ)\n\t\tioread32_rep(data_addr, buf, words);\n\telse\n\t\tiowrite32_rep(data_addr, buf, words);\n\n\t \n\tif (unlikely(slop)) {\n\t\tunsigned char pad[4] = { };\n\n\t\t \n\t\tbuf += buflen - slop;\n\n\t\t \n\t\tif (rw == READ) {\n\t\t\tif (slop < 3)\n\t\t\t\tioread16_rep(data_addr, pad, 1);\n\t\t\telse\n\t\t\t\tioread32_rep(data_addr, pad, 1);\n\t\t\tmemcpy(buf, pad, slop);\n\t\t} else {\n\t\t\tmemcpy(pad, buf, slop);\n\t\t\tif (slop < 3)\n\t\t\t\tiowrite16_rep(data_addr, pad, 1);\n\t\t\telse\n\t\t\t\tiowrite32_rep(data_addr, pad, 1);\n\t\t}\n\t}\n\treturn (buflen + 1) & ~1;\n}\nEXPORT_SYMBOL_GPL(ata_sff_data_xfer32);\n\nstatic void ata_pio_xfer(struct ata_queued_cmd *qc, struct page *page,\n\t\tunsigned int offset, size_t xfer_size)\n{\n\tbool do_write = (qc->tf.flags & ATA_TFLAG_WRITE);\n\tunsigned char *buf;\n\n\tbuf = kmap_atomic(page);\n\tqc->ap->ops->sff_data_xfer(qc, buf + offset, xfer_size, do_write);\n\tkunmap_atomic(buf);\n\n\tif (!do_write && !PageSlab(page))\n\t\tflush_dcache_page(page);\n}\n\n \nstatic void ata_pio_sector(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tif (!qc->cursg) {\n\t\tqc->curbytes = qc->nbytes;\n\t\treturn;\n\t}\n\tif (qc->curbytes == qc->nbytes - qc->sect_size)\n\t\tap->hsm_task_state = HSM_ST_LAST;\n\n\tpage = sg_page(qc->cursg);\n\toffset = qc->cursg->offset + qc->cursg_ofs;\n\n\t \n\tpage = nth_page(page, (offset >> PAGE_SHIFT));\n\toffset %= PAGE_SIZE;\n\n\ttrace_ata_sff_pio_transfer_data(qc, offset, qc->sect_size);\n\n\t \n\tWARN_ON_ONCE(offset % 4);\n\tif (offset + qc->sect_size > PAGE_SIZE) {\n\t\tunsigned int split_len = PAGE_SIZE - offset;\n\n\t\tata_pio_xfer(qc, page, offset, split_len);\n\t\tata_pio_xfer(qc, nth_page(page, 1), 0,\n\t\t\t     qc->sect_size - split_len);\n\t} else {\n\t\tata_pio_xfer(qc, page, offset, qc->sect_size);\n\t}\n\n\tqc->curbytes += qc->sect_size;\n\tqc->cursg_ofs += qc->sect_size;\n\n\tif (qc->cursg_ofs == qc->cursg->length) {\n\t\tqc->cursg = sg_next(qc->cursg);\n\t\tif (!qc->cursg)\n\t\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\tqc->cursg_ofs = 0;\n\t}\n}\n\n \nstatic void ata_pio_sectors(struct ata_queued_cmd *qc)\n{\n\tif (is_multi_taskfile(&qc->tf)) {\n\t\t \n\t\tunsigned int nsect;\n\n\t\tWARN_ON_ONCE(qc->dev->multi_count == 0);\n\n\t\tnsect = min((qc->nbytes - qc->curbytes) / qc->sect_size,\n\t\t\t    qc->dev->multi_count);\n\t\twhile (nsect--)\n\t\t\tata_pio_sector(qc);\n\t} else\n\t\tata_pio_sector(qc);\n\n\tata_sff_sync(qc->ap);  \n}\n\n \nstatic void atapi_send_cdb(struct ata_port *ap, struct ata_queued_cmd *qc)\n{\n\t \n\ttrace_atapi_send_cdb(qc, 0, qc->dev->cdb_len);\n\tWARN_ON_ONCE(qc->dev->cdb_len < 12);\n\n\tap->ops->sff_data_xfer(qc, qc->cdb, qc->dev->cdb_len, 1);\n\tata_sff_sync(ap);\n\t \n\tswitch (qc->tf.protocol) {\n\tcase ATAPI_PROT_PIO:\n\t\tap->hsm_task_state = HSM_ST;\n\t\tbreak;\n\tcase ATAPI_PROT_NODATA:\n\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\tbreak;\n#ifdef CONFIG_ATA_BMDMA\n\tcase ATAPI_PROT_DMA:\n\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\t \n\t\ttrace_ata_bmdma_start(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_start(qc);\n\t\tbreak;\n#endif  \n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)\n{\n\tint rw = (qc->tf.flags & ATA_TFLAG_WRITE) ? WRITE : READ;\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_device *dev = qc->dev;\n\tstruct ata_eh_info *ehi = &dev->link->eh_info;\n\tstruct scatterlist *sg;\n\tstruct page *page;\n\tunsigned char *buf;\n\tunsigned int offset, count, consumed;\n\nnext_sg:\n\tsg = qc->cursg;\n\tif (unlikely(!sg)) {\n\t\tata_ehi_push_desc(ehi, \"unexpected or too much trailing data \"\n\t\t\t\t  \"buf=%u cur=%u bytes=%u\",\n\t\t\t\t  qc->nbytes, qc->curbytes, bytes);\n\t\treturn -1;\n\t}\n\n\tpage = sg_page(sg);\n\toffset = sg->offset + qc->cursg_ofs;\n\n\t \n\tpage = nth_page(page, (offset >> PAGE_SHIFT));\n\toffset %= PAGE_SIZE;\n\n\t \n\tcount = min(sg->length - qc->cursg_ofs, bytes);\n\n\t \n\tcount = min(count, (unsigned int)PAGE_SIZE - offset);\n\n\ttrace_atapi_pio_transfer_data(qc, offset, count);\n\n\t \n\tbuf = kmap_atomic(page);\n\tconsumed = ap->ops->sff_data_xfer(qc, buf + offset, count, rw);\n\tkunmap_atomic(buf);\n\n\tbytes -= min(bytes, consumed);\n\tqc->curbytes += count;\n\tqc->cursg_ofs += count;\n\n\tif (qc->cursg_ofs == sg->length) {\n\t\tqc->cursg = sg_next(qc->cursg);\n\t\tqc->cursg_ofs = 0;\n\t}\n\n\t \n\tif (bytes)\n\t\tgoto next_sg;\n\treturn 0;\n}\n\n \nstatic void atapi_pio_bytes(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_device *dev = qc->dev;\n\tstruct ata_eh_info *ehi = &dev->link->eh_info;\n\tunsigned int ireason, bc_lo, bc_hi, bytes;\n\tint i_write, do_write = (qc->tf.flags & ATA_TFLAG_WRITE) ? 1 : 0;\n\n\t \n\tap->ops->sff_tf_read(ap, &qc->result_tf);\n\tireason = qc->result_tf.nsect;\n\tbc_lo = qc->result_tf.lbam;\n\tbc_hi = qc->result_tf.lbah;\n\tbytes = (bc_hi << 8) | bc_lo;\n\n\t \n\tif (unlikely(ireason & ATAPI_COD))\n\t\tgoto atapi_check;\n\n\t \n\ti_write = ((ireason & ATAPI_IO) == 0) ? 1 : 0;\n\tif (unlikely(do_write != i_write))\n\t\tgoto atapi_check;\n\n\tif (unlikely(!bytes))\n\t\tgoto atapi_check;\n\n\tif (unlikely(__atapi_pio_bytes(qc, bytes)))\n\t\tgoto err_out;\n\tata_sff_sync(ap);  \n\n\treturn;\n\n atapi_check:\n\tata_ehi_push_desc(ehi, \"ATAPI check failed (ireason=0x%x bytes=%u)\",\n\t\t\t  ireason, bytes);\n err_out:\n\tqc->err_mask |= AC_ERR_HSM;\n\tap->hsm_task_state = HSM_ST_ERR;\n}\n\n \nstatic inline int ata_hsm_ok_in_wq(struct ata_port *ap,\n\t\t\t\t\t\tstruct ata_queued_cmd *qc)\n{\n\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\treturn 1;\n\n\tif (ap->hsm_task_state == HSM_ST_FIRST) {\n\t\tif (qc->tf.protocol == ATA_PROT_PIO &&\n\t\t   (qc->tf.flags & ATA_TFLAG_WRITE))\n\t\t    return 1;\n\n\t\tif (ata_is_atapi(qc->tf.protocol) &&\n\t\t   !(qc->dev->flags & ATA_DFLAG_CDB_INTR))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ata_hsm_qc_complete(struct ata_queued_cmd *qc, int in_wq)\n{\n\tstruct ata_port *ap = qc->ap;\n\n\tif (in_wq) {\n\t\t \n\t\tqc = ata_qc_from_tag(ap, qc->tag);\n\t\tif (qc) {\n\t\t\tif (likely(!(qc->err_mask & AC_ERR_HSM))) {\n\t\t\t\tata_sff_irq_on(ap);\n\t\t\t\tata_qc_complete(qc);\n\t\t\t} else\n\t\t\t\tata_port_freeze(ap);\n\t\t}\n\t} else {\n\t\tif (likely(!(qc->err_mask & AC_ERR_HSM)))\n\t\t\tata_qc_complete(qc);\n\t\telse\n\t\t\tata_port_freeze(ap);\n\t}\n}\n\n \nint ata_sff_hsm_move(struct ata_port *ap, struct ata_queued_cmd *qc,\n\t\t     u8 status, int in_wq)\n{\n\tstruct ata_link *link = qc->dev->link;\n\tstruct ata_eh_info *ehi = &link->eh_info;\n\tint poll_next;\n\n\tlockdep_assert_held(ap->lock);\n\n\tWARN_ON_ONCE((qc->flags & ATA_QCFLAG_ACTIVE) == 0);\n\n\t \n\tWARN_ON_ONCE(in_wq != ata_hsm_ok_in_wq(ap, qc));\n\nfsm_start:\n\ttrace_ata_sff_hsm_state(qc, status);\n\n\tswitch (ap->hsm_task_state) {\n\tcase HSM_ST_FIRST:\n\t\t \n\n\t\t \n\t\tpoll_next = (qc->tf.flags & ATA_TFLAG_POLLING);\n\n\t\t \n\t\tif (unlikely((status & ATA_DRQ) == 0)) {\n\t\t\t \n\t\t\tif (likely(status & (ATA_ERR | ATA_DF)))\n\t\t\t\t \n\t\t\t\tqc->err_mask |= AC_ERR_DEV;\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tata_ehi_push_desc(ehi,\n\t\t\t\t\t\"ST_FIRST: !(DRQ|ERR|DF)\");\n\t\t\t\tqc->err_mask |= AC_ERR_HSM;\n\t\t\t}\n\n\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\tgoto fsm_start;\n\t\t}\n\n\t\t \n\t\tif (unlikely(status & (ATA_ERR | ATA_DF))) {\n\t\t\t \n\t\t\tif (!(qc->dev->horkage & ATA_HORKAGE_STUCK_ERR)) {\n\t\t\t\tata_ehi_push_desc(ehi, \"ST_FIRST: \"\n\t\t\t\t\t\"DRQ=1 with device error, \"\n\t\t\t\t\t\"dev_stat 0x%X\", status);\n\t\t\t\tqc->err_mask |= AC_ERR_HSM;\n\t\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\t\t}\n\n\t\tif (qc->tf.protocol == ATA_PROT_PIO) {\n\t\t\t \n\n\t\t\t \n\t\t\tap->hsm_task_state = HSM_ST;\n\t\t\tata_pio_sectors(qc);\n\t\t} else\n\t\t\t \n\t\t\tatapi_send_cdb(ap, qc);\n\n\t\t \n\t\tbreak;\n\n\tcase HSM_ST:\n\t\t \n\t\tif (qc->tf.protocol == ATAPI_PROT_PIO) {\n\t\t\t \n\t\t\tif ((status & ATA_DRQ) == 0) {\n\t\t\t\t \n\t\t\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (unlikely(status & (ATA_ERR | ATA_DF))) {\n\t\t\t\tata_ehi_push_desc(ehi, \"ST-ATAPI: \"\n\t\t\t\t\t\"DRQ=1 with device error, \"\n\t\t\t\t\t\"dev_stat 0x%X\", status);\n\t\t\t\tqc->err_mask |= AC_ERR_HSM;\n\t\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\n\t\t\tatapi_pio_bytes(qc);\n\n\t\t\tif (unlikely(ap->hsm_task_state == HSM_ST_ERR))\n\t\t\t\t \n\t\t\t\tgoto fsm_start;\n\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely((status & ATA_DRQ) == 0)) {\n\t\t\t\t \n\t\t\t\tif (likely(status & (ATA_ERR | ATA_DF))) {\n\t\t\t\t\t \n\t\t\t\t\tqc->err_mask |= AC_ERR_DEV;\n\n\t\t\t\t\t \n\t\t\t\t\tif (qc->dev->horkage &\n\t\t\t\t\t    ATA_HORKAGE_DIAGNOSTIC)\n\t\t\t\t\t\tqc->err_mask |=\n\t\t\t\t\t\t\tAC_ERR_NODEV_HINT;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tata_ehi_push_desc(ehi, \"ST-ATA: \"\n\t\t\t\t\t\t\"DRQ=0 without device error, \"\n\t\t\t\t\t\t\"dev_stat 0x%X\", status);\n\t\t\t\t\tqc->err_mask |= AC_ERR_HSM |\n\t\t\t\t\t\t\tAC_ERR_NODEV_HINT;\n\t\t\t\t}\n\n\t\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (unlikely(status & (ATA_ERR | ATA_DF))) {\n\t\t\t\t \n\t\t\t\tqc->err_mask |= AC_ERR_DEV;\n\n\t\t\t\tif (!(qc->tf.flags & ATA_TFLAG_WRITE)) {\n\t\t\t\t\tata_pio_sectors(qc);\n\t\t\t\t\tstatus = ata_wait_idle(ap);\n\t\t\t\t}\n\n\t\t\t\tif (status & (ATA_BUSY | ATA_DRQ)) {\n\t\t\t\t\tata_ehi_push_desc(ehi, \"ST-ATA: \"\n\t\t\t\t\t\t\"BUSY|DRQ persists on ERR|DF, \"\n\t\t\t\t\t\t\"dev_stat 0x%X\", status);\n\t\t\t\t\tqc->err_mask |= AC_ERR_HSM;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tif (status == 0x7f)\n\t\t\t\t\tqc->err_mask |= AC_ERR_NODEV_HINT;\n\n\t\t\t\t \n\t\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\n\t\t\tata_pio_sectors(qc);\n\n\t\t\tif (ap->hsm_task_state == HSM_ST_LAST &&\n\t\t\t    (!(qc->tf.flags & ATA_TFLAG_WRITE))) {\n\t\t\t\t \n\t\t\t\tstatus = ata_wait_idle(ap);\n\t\t\t\tgoto fsm_start;\n\t\t\t}\n\t\t}\n\n\t\tpoll_next = 1;\n\t\tbreak;\n\n\tcase HSM_ST_LAST:\n\t\tif (unlikely(!ata_ok(status))) {\n\t\t\tqc->err_mask |= __ac_err_mask(status);\n\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t\tgoto fsm_start;\n\t\t}\n\n\t\t \n\t\ttrace_ata_sff_hsm_command_complete(qc, status);\n\n\t\tWARN_ON_ONCE(qc->err_mask & (AC_ERR_DEV | AC_ERR_HSM));\n\n\t\tap->hsm_task_state = HSM_ST_IDLE;\n\n\t\t \n\t\tata_hsm_qc_complete(qc, in_wq);\n\n\t\tpoll_next = 0;\n\t\tbreak;\n\n\tcase HSM_ST_ERR:\n\t\tap->hsm_task_state = HSM_ST_IDLE;\n\n\t\t \n\t\tata_hsm_qc_complete(qc, in_wq);\n\n\t\tpoll_next = 0;\n\t\tbreak;\n\tdefault:\n\t\tpoll_next = 0;\n\t\tWARN(true, \"ata%d: SFF host state machine in invalid state %d\",\n\t\t     ap->print_id, ap->hsm_task_state);\n\t}\n\n\treturn poll_next;\n}\nEXPORT_SYMBOL_GPL(ata_sff_hsm_move);\n\nvoid ata_sff_queue_work(struct work_struct *work)\n{\n\tqueue_work(ata_sff_wq, work);\n}\nEXPORT_SYMBOL_GPL(ata_sff_queue_work);\n\nvoid ata_sff_queue_delayed_work(struct delayed_work *dwork, unsigned long delay)\n{\n\tqueue_delayed_work(ata_sff_wq, dwork, delay);\n}\nEXPORT_SYMBOL_GPL(ata_sff_queue_delayed_work);\n\nvoid ata_sff_queue_pio_task(struct ata_link *link, unsigned long delay)\n{\n\tstruct ata_port *ap = link->ap;\n\n\tWARN_ON((ap->sff_pio_task_link != NULL) &&\n\t\t(ap->sff_pio_task_link != link));\n\tap->sff_pio_task_link = link;\n\n\t \n\tata_sff_queue_delayed_work(&ap->sff_pio_task, msecs_to_jiffies(delay));\n}\nEXPORT_SYMBOL_GPL(ata_sff_queue_pio_task);\n\nvoid ata_sff_flush_pio_task(struct ata_port *ap)\n{\n\ttrace_ata_sff_flush_pio_task(ap);\n\n\tcancel_delayed_work_sync(&ap->sff_pio_task);\n\n\t \n\tspin_lock_irq(ap->lock);\n\tap->hsm_task_state = HSM_ST_IDLE;\n\tspin_unlock_irq(ap->lock);\n\n\tap->sff_pio_task_link = NULL;\n}\n\nstatic void ata_sff_pio_task(struct work_struct *work)\n{\n\tstruct ata_port *ap =\n\t\tcontainer_of(work, struct ata_port, sff_pio_task.work);\n\tstruct ata_link *link = ap->sff_pio_task_link;\n\tstruct ata_queued_cmd *qc;\n\tu8 status;\n\tint poll_next;\n\n\tspin_lock_irq(ap->lock);\n\n\tBUG_ON(ap->sff_pio_task_link == NULL);\n\t \n\tqc = ata_qc_from_tag(ap, link->active_tag);\n\tif (!qc) {\n\t\tap->sff_pio_task_link = NULL;\n\t\tgoto out_unlock;\n\t}\n\nfsm_start:\n\tWARN_ON_ONCE(ap->hsm_task_state == HSM_ST_IDLE);\n\n\t \n\tstatus = ata_sff_busy_wait(ap, ATA_BUSY, 5);\n\tif (status & ATA_BUSY) {\n\t\tspin_unlock_irq(ap->lock);\n\t\tata_msleep(ap, 2);\n\t\tspin_lock_irq(ap->lock);\n\n\t\tstatus = ata_sff_busy_wait(ap, ATA_BUSY, 10);\n\t\tif (status & ATA_BUSY) {\n\t\t\tata_sff_queue_pio_task(link, ATA_SHORT_PAUSE);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t \n\tap->sff_pio_task_link = NULL;\n\t \n\tpoll_next = ata_sff_hsm_move(ap, qc, status, 1);\n\n\t \n\tif (poll_next)\n\t\tgoto fsm_start;\nout_unlock:\n\tspin_unlock_irq(ap->lock);\n}\n\n \nunsigned int ata_sff_qc_issue(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_link *link = qc->dev->link;\n\n\t \n\tif (ap->flags & ATA_FLAG_PIO_POLLING)\n\t\tqc->tf.flags |= ATA_TFLAG_POLLING;\n\n\t \n\tata_dev_select(ap, qc->dev->devno, 1, 0);\n\n\t \n\tswitch (qc->tf.protocol) {\n\tcase ATA_PROT_NODATA:\n\t\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\t\tata_qc_set_polling(qc);\n\n\t\tata_tf_to_host(ap, &qc->tf, qc->tag);\n\t\tap->hsm_task_state = HSM_ST_LAST;\n\n\t\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\t\tata_sff_queue_pio_task(link, 0);\n\n\t\tbreak;\n\n\tcase ATA_PROT_PIO:\n\t\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\t\tata_qc_set_polling(qc);\n\n\t\tata_tf_to_host(ap, &qc->tf, qc->tag);\n\n\t\tif (qc->tf.flags & ATA_TFLAG_WRITE) {\n\t\t\t \n\t\t\tap->hsm_task_state = HSM_ST_FIRST;\n\t\t\tata_sff_queue_pio_task(link, 0);\n\n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tap->hsm_task_state = HSM_ST;\n\n\t\t\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\t\t\tata_sff_queue_pio_task(link, 0);\n\n\t\t\t \n\t\t}\n\n\t\tbreak;\n\n\tcase ATAPI_PROT_PIO:\n\tcase ATAPI_PROT_NODATA:\n\t\tif (qc->tf.flags & ATA_TFLAG_POLLING)\n\t\t\tata_qc_set_polling(qc);\n\n\t\tata_tf_to_host(ap, &qc->tf, qc->tag);\n\n\t\tap->hsm_task_state = HSM_ST_FIRST;\n\n\t\t \n\t\tif ((!(qc->dev->flags & ATA_DFLAG_CDB_INTR)) ||\n\t\t    (qc->tf.flags & ATA_TFLAG_POLLING))\n\t\t\tata_sff_queue_pio_task(link, 0);\n\t\tbreak;\n\n\tdefault:\n\t\treturn AC_ERR_SYSTEM;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_sff_qc_issue);\n\n \nvoid ata_sff_qc_fill_rtf(struct ata_queued_cmd *qc)\n{\n\tqc->ap->ops->sff_tf_read(qc->ap, &qc->result_tf);\n}\nEXPORT_SYMBOL_GPL(ata_sff_qc_fill_rtf);\n\nstatic unsigned int ata_sff_idle_irq(struct ata_port *ap)\n{\n\tap->stats.idle_irq++;\n\n#ifdef ATA_IRQ_TRAP\n\tif ((ap->stats.idle_irq % 1000) == 0) {\n\t\tap->ops->sff_check_status(ap);\n\t\tif (ap->ops->sff_irq_clear)\n\t\t\tap->ops->sff_irq_clear(ap);\n\t\tata_port_warn(ap, \"irq trap\\n\");\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\t \n}\n\nstatic unsigned int __ata_sff_port_intr(struct ata_port *ap,\n\t\t\t\t\tstruct ata_queued_cmd *qc,\n\t\t\t\t\tbool hsmv_on_idle)\n{\n\tu8 status;\n\n\ttrace_ata_sff_port_intr(qc, hsmv_on_idle);\n\n\t \n\tswitch (ap->hsm_task_state) {\n\tcase HSM_ST_FIRST:\n\t\t \n\n\t\t \n\t\tif (!(qc->dev->flags & ATA_DFLAG_CDB_INTR))\n\t\t\treturn ata_sff_idle_irq(ap);\n\t\tbreak;\n\tcase HSM_ST_IDLE:\n\t\treturn ata_sff_idle_irq(ap);\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tstatus = ata_sff_irq_status(ap);\n\tif (status & ATA_BUSY) {\n\t\tif (hsmv_on_idle) {\n\t\t\t \n\t\t\tqc->err_mask |= AC_ERR_HSM;\n\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t} else\n\t\t\treturn ata_sff_idle_irq(ap);\n\t}\n\n\t \n\tif (ap->ops->sff_irq_clear)\n\t\tap->ops->sff_irq_clear(ap);\n\n\tata_sff_hsm_move(ap, qc, status, 0);\n\n\treturn 1;\t \n}\n\n \nunsigned int ata_sff_port_intr(struct ata_port *ap, struct ata_queued_cmd *qc)\n{\n\treturn __ata_sff_port_intr(ap, qc, false);\n}\nEXPORT_SYMBOL_GPL(ata_sff_port_intr);\n\nstatic inline irqreturn_t __ata_sff_interrupt(int irq, void *dev_instance,\n\tunsigned int (*port_intr)(struct ata_port *, struct ata_queued_cmd *))\n{\n\tstruct ata_host *host = dev_instance;\n\tbool retried = false;\n\tunsigned int i;\n\tunsigned int handled, idle, polling;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&host->lock, flags);\n\nretry:\n\thandled = idle = polling = 0;\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\t\tstruct ata_queued_cmd *qc;\n\n\t\tqc = ata_qc_from_tag(ap, ap->link.active_tag);\n\t\tif (qc) {\n\t\t\tif (!(qc->tf.flags & ATA_TFLAG_POLLING))\n\t\t\t\thandled |= port_intr(ap, qc);\n\t\t\telse\n\t\t\t\tpolling |= 1 << i;\n\t\t} else\n\t\t\tidle |= 1 << i;\n\t}\n\n\t \n\tif (!handled && !retried) {\n\t\tbool retry = false;\n\n\t\tfor (i = 0; i < host->n_ports; i++) {\n\t\t\tstruct ata_port *ap = host->ports[i];\n\n\t\t\tif (polling & (1 << i))\n\t\t\t\tcontinue;\n\n\t\t\tif (!ap->ops->sff_irq_check ||\n\t\t\t    !ap->ops->sff_irq_check(ap))\n\t\t\t\tcontinue;\n\n\t\t\tif (idle & (1 << i)) {\n\t\t\t\tap->ops->sff_check_status(ap);\n\t\t\t\tif (ap->ops->sff_irq_clear)\n\t\t\t\t\tap->ops->sff_irq_clear(ap);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (!(ap->ops->sff_check_status(ap) & ATA_BUSY))\n\t\t\t\t\tretry |= true;\n\t\t\t\t \n\t\t\t}\n\t\t}\n\n\t\tif (retry) {\n\t\t\tretried = true;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&host->lock, flags);\n\n\treturn IRQ_RETVAL(handled);\n}\n\n \nirqreturn_t ata_sff_interrupt(int irq, void *dev_instance)\n{\n\treturn __ata_sff_interrupt(irq, dev_instance, ata_sff_port_intr);\n}\nEXPORT_SYMBOL_GPL(ata_sff_interrupt);\n\n \n\nvoid ata_sff_lost_interrupt(struct ata_port *ap)\n{\n\tu8 status = 0;\n\tstruct ata_queued_cmd *qc;\n\n\t \n\tqc = ata_qc_from_tag(ap, ap->link.active_tag);\n\t \n\tif (!qc || qc->tf.flags & ATA_TFLAG_POLLING)\n\t\treturn;\n\t \n\tif (WARN_ON_ONCE(!ata_sff_altstatus(ap, &status)))\n\t\treturn;\n\tif (status & ATA_BUSY)\n\t\treturn;\n\n\t \n\tata_port_warn(ap, \"lost interrupt (Status 0x%x)\\n\", status);\n\t \n\tata_sff_port_intr(ap, qc);\n}\nEXPORT_SYMBOL_GPL(ata_sff_lost_interrupt);\n\n \nvoid ata_sff_freeze(struct ata_port *ap)\n{\n\tap->ctl |= ATA_NIEN;\n\tap->last_ctl = ap->ctl;\n\n\tata_sff_set_devctl(ap, ap->ctl);\n\n\t \n\tap->ops->sff_check_status(ap);\n\n\tif (ap->ops->sff_irq_clear)\n\t\tap->ops->sff_irq_clear(ap);\n}\nEXPORT_SYMBOL_GPL(ata_sff_freeze);\n\n \nvoid ata_sff_thaw(struct ata_port *ap)\n{\n\t \n\tap->ops->sff_check_status(ap);\n\tif (ap->ops->sff_irq_clear)\n\t\tap->ops->sff_irq_clear(ap);\n\tata_sff_irq_on(ap);\n}\nEXPORT_SYMBOL_GPL(ata_sff_thaw);\n\n \nint ata_sff_prereset(struct ata_link *link, unsigned long deadline)\n{\n\tstruct ata_eh_context *ehc = &link->eh_context;\n\tint rc;\n\n\t \n\tata_std_prereset(link, deadline);\n\n\t \n\tif (ehc->i.action & ATA_EH_HARDRESET)\n\t\treturn 0;\n\n\t \n\tif (!ata_link_offline(link)) {\n\t\trc = ata_sff_wait_ready(link, deadline);\n\t\tif (rc && rc != -ENODEV) {\n\t\t\tata_link_warn(link,\n\t\t\t\t      \"device not ready (errno=%d), forcing hardreset\\n\",\n\t\t\t\t      rc);\n\t\t\tehc->i.action |= ATA_EH_HARDRESET;\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_sff_prereset);\n\n \nstatic bool ata_devchk(struct ata_port *ap, unsigned int device)\n{\n\tstruct ata_ioports *ioaddr = &ap->ioaddr;\n\tu8 nsect, lbal;\n\n\tap->ops->sff_dev_select(ap, device);\n\n\tiowrite8(0x55, ioaddr->nsect_addr);\n\tiowrite8(0xaa, ioaddr->lbal_addr);\n\n\tiowrite8(0xaa, ioaddr->nsect_addr);\n\tiowrite8(0x55, ioaddr->lbal_addr);\n\n\tiowrite8(0x55, ioaddr->nsect_addr);\n\tiowrite8(0xaa, ioaddr->lbal_addr);\n\n\tnsect = ioread8(ioaddr->nsect_addr);\n\tlbal = ioread8(ioaddr->lbal_addr);\n\n\tif ((nsect == 0x55) && (lbal == 0xaa))\n\t\treturn true;\t \n\n\treturn false;\t\t \n}\n\n \nunsigned int ata_sff_dev_classify(struct ata_device *dev, int present,\n\t\t\t\t  u8 *r_err)\n{\n\tstruct ata_port *ap = dev->link->ap;\n\tstruct ata_taskfile tf;\n\tunsigned int class;\n\tu8 err;\n\n\tap->ops->sff_dev_select(ap, dev->devno);\n\n\tmemset(&tf, 0, sizeof(tf));\n\n\tap->ops->sff_tf_read(ap, &tf);\n\terr = tf.error;\n\tif (r_err)\n\t\t*r_err = err;\n\n\t \n\tif (err == 0)\n\t\t \n\t\tdev->horkage |= ATA_HORKAGE_DIAGNOSTIC;\n\telse if (err == 1)\n\t\t  ;\n\telse if ((dev->devno == 0) && (err == 0x81))\n\t\t  ;\n\telse\n\t\treturn ATA_DEV_NONE;\n\n\t \n\tclass = ata_port_classify(ap, &tf);\n\tswitch (class) {\n\tcase ATA_DEV_UNKNOWN:\n\t\t \n\t\tif (present && (dev->horkage & ATA_HORKAGE_DIAGNOSTIC))\n\t\t\tclass = ATA_DEV_ATA;\n\t\telse\n\t\t\tclass = ATA_DEV_NONE;\n\t\tbreak;\n\tcase ATA_DEV_ATA:\n\t\tif (ap->ops->sff_check_status(ap) == 0)\n\t\t\tclass = ATA_DEV_NONE;\n\t\tbreak;\n\t}\n\treturn class;\n}\nEXPORT_SYMBOL_GPL(ata_sff_dev_classify);\n\n \nint ata_sff_wait_after_reset(struct ata_link *link, unsigned int devmask,\n\t\t\t     unsigned long deadline)\n{\n\tstruct ata_port *ap = link->ap;\n\tstruct ata_ioports *ioaddr = &ap->ioaddr;\n\tunsigned int dev0 = devmask & (1 << 0);\n\tunsigned int dev1 = devmask & (1 << 1);\n\tint rc, ret = 0;\n\n\tata_msleep(ap, ATA_WAIT_AFTER_RESET);\n\n\t \n\trc = ata_sff_wait_ready(link, deadline);\n\t \n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (dev1) {\n\t\tint i;\n\n\t\tap->ops->sff_dev_select(ap, 1);\n\n\t\t \n\t\tfor (i = 0; i < 2; i++) {\n\t\t\tu8 nsect, lbal;\n\n\t\t\tnsect = ioread8(ioaddr->nsect_addr);\n\t\t\tlbal = ioread8(ioaddr->lbal_addr);\n\t\t\tif ((nsect == 1) && (lbal == 1))\n\t\t\t\tbreak;\n\t\t\tata_msleep(ap, 50);\t \n\t\t}\n\n\t\trc = ata_sff_wait_ready(link, deadline);\n\t\tif (rc) {\n\t\t\tif (rc != -ENODEV)\n\t\t\t\treturn rc;\n\t\t\tret = rc;\n\t\t}\n\t}\n\n\t \n\tap->ops->sff_dev_select(ap, 0);\n\tif (dev1)\n\t\tap->ops->sff_dev_select(ap, 1);\n\tif (dev0)\n\t\tap->ops->sff_dev_select(ap, 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ata_sff_wait_after_reset);\n\nstatic int ata_bus_softreset(struct ata_port *ap, unsigned int devmask,\n\t\t\t     unsigned long deadline)\n{\n\tstruct ata_ioports *ioaddr = &ap->ioaddr;\n\n\tif (ap->ioaddr.ctl_addr) {\n\t\t \n\t\tiowrite8(ap->ctl, ioaddr->ctl_addr);\n\t\tudelay(20);\t \n\t\tiowrite8(ap->ctl | ATA_SRST, ioaddr->ctl_addr);\n\t\tudelay(20);\t \n\t\tiowrite8(ap->ctl, ioaddr->ctl_addr);\n\t\tap->last_ctl = ap->ctl;\n\t}\n\n\t \n\treturn ata_sff_wait_after_reset(&ap->link, devmask, deadline);\n}\n\n \nint ata_sff_softreset(struct ata_link *link, unsigned int *classes,\n\t\t      unsigned long deadline)\n{\n\tstruct ata_port *ap = link->ap;\n\tunsigned int slave_possible = ap->flags & ATA_FLAG_SLAVE_POSS;\n\tunsigned int devmask = 0;\n\tint rc;\n\tu8 err;\n\n\t \n\tif (ata_devchk(ap, 0))\n\t\tdevmask |= (1 << 0);\n\tif (slave_possible && ata_devchk(ap, 1))\n\t\tdevmask |= (1 << 1);\n\n\t \n\tap->ops->sff_dev_select(ap, 0);\n\n\t \n\trc = ata_bus_softreset(ap, devmask, deadline);\n\t \n\tif (rc && (rc != -ENODEV || sata_scr_valid(link))) {\n\t\tata_link_err(link, \"SRST failed (errno=%d)\\n\", rc);\n\t\treturn rc;\n\t}\n\n\t \n\tclasses[0] = ata_sff_dev_classify(&link->device[0],\n\t\t\t\t\t  devmask & (1 << 0), &err);\n\tif (slave_possible && err != 0x81)\n\t\tclasses[1] = ata_sff_dev_classify(&link->device[1],\n\t\t\t\t\t\t  devmask & (1 << 1), &err);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_sff_softreset);\n\n \nint sata_sff_hardreset(struct ata_link *link, unsigned int *class,\n\t\t       unsigned long deadline)\n{\n\tstruct ata_eh_context *ehc = &link->eh_context;\n\tconst unsigned int *timing = sata_ehc_deb_timing(ehc);\n\tbool online;\n\tint rc;\n\n\trc = sata_link_hardreset(link, timing, deadline, &online,\n\t\t\t\t ata_sff_check_ready);\n\tif (online)\n\t\t*class = ata_sff_dev_classify(link->device, 1, NULL);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(sata_sff_hardreset);\n\n \nvoid ata_sff_postreset(struct ata_link *link, unsigned int *classes)\n{\n\tstruct ata_port *ap = link->ap;\n\n\tata_std_postreset(link, classes);\n\n\t \n\tif (classes[0] != ATA_DEV_NONE)\n\t\tap->ops->sff_dev_select(ap, 1);\n\tif (classes[1] != ATA_DEV_NONE)\n\t\tap->ops->sff_dev_select(ap, 0);\n\n\t \n\tif (classes[0] == ATA_DEV_NONE && classes[1] == ATA_DEV_NONE)\n\t\treturn;\n\n\t \n\tif (ata_sff_set_devctl(ap, ap->ctl))\n\t\tap->last_ctl = ap->ctl;\n}\nEXPORT_SYMBOL_GPL(ata_sff_postreset);\n\n \n\nvoid ata_sff_drain_fifo(struct ata_queued_cmd *qc)\n{\n\tint count;\n\tstruct ata_port *ap;\n\n\t \n\tif (qc == NULL || qc->dma_dir == DMA_TO_DEVICE)\n\t\treturn;\n\n\tap = qc->ap;\n\t \n\tfor (count = 0; (ap->ops->sff_check_status(ap) & ATA_DRQ)\n\t\t\t\t\t\t&& count < 65536; count += 2)\n\t\tioread16(ap->ioaddr.data_addr);\n\n\tif (count)\n\t\tata_port_dbg(ap, \"drained %d bytes to clear DRQ\\n\", count);\n\n}\nEXPORT_SYMBOL_GPL(ata_sff_drain_fifo);\n\n \nvoid ata_sff_error_handler(struct ata_port *ap)\n{\n\tata_reset_fn_t softreset = ap->ops->softreset;\n\tata_reset_fn_t hardreset = ap->ops->hardreset;\n\tstruct ata_queued_cmd *qc;\n\tunsigned long flags;\n\n\tqc = __ata_qc_from_tag(ap, ap->link.active_tag);\n\tif (qc && !(qc->flags & ATA_QCFLAG_EH))\n\t\tqc = NULL;\n\n\tspin_lock_irqsave(ap->lock, flags);\n\n\t \n\tif (ap->ops->sff_drain_fifo)\n\t\tap->ops->sff_drain_fifo(qc);\n\n\tspin_unlock_irqrestore(ap->lock, flags);\n\n\t \n\tif ((hardreset == sata_std_hardreset ||\n\t     hardreset == sata_sff_hardreset) && !sata_scr_valid(&ap->link))\n\t\thardreset = NULL;\n\n\tata_do_eh(ap, ap->ops->prereset, softreset, hardreset,\n\t\t  ap->ops->postreset);\n}\nEXPORT_SYMBOL_GPL(ata_sff_error_handler);\n\n \nvoid ata_sff_std_ports(struct ata_ioports *ioaddr)\n{\n\tioaddr->data_addr = ioaddr->cmd_addr + ATA_REG_DATA;\n\tioaddr->error_addr = ioaddr->cmd_addr + ATA_REG_ERR;\n\tioaddr->feature_addr = ioaddr->cmd_addr + ATA_REG_FEATURE;\n\tioaddr->nsect_addr = ioaddr->cmd_addr + ATA_REG_NSECT;\n\tioaddr->lbal_addr = ioaddr->cmd_addr + ATA_REG_LBAL;\n\tioaddr->lbam_addr = ioaddr->cmd_addr + ATA_REG_LBAM;\n\tioaddr->lbah_addr = ioaddr->cmd_addr + ATA_REG_LBAH;\n\tioaddr->device_addr = ioaddr->cmd_addr + ATA_REG_DEVICE;\n\tioaddr->status_addr = ioaddr->cmd_addr + ATA_REG_STATUS;\n\tioaddr->command_addr = ioaddr->cmd_addr + ATA_REG_CMD;\n}\nEXPORT_SYMBOL_GPL(ata_sff_std_ports);\n\n#ifdef CONFIG_PCI\n\nstatic bool ata_resources_present(struct pci_dev *pdev, int port)\n{\n\tint i;\n\n\t \n\tport *= 2;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (pci_resource_start(pdev, port + i) == 0 ||\n\t\t    pci_resource_len(pdev, port + i) == 0)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nint ata_pci_sff_init_host(struct ata_host *host)\n{\n\tstruct device *gdev = host->dev;\n\tstruct pci_dev *pdev = to_pci_dev(gdev);\n\tunsigned int mask = 0;\n\tint i, rc;\n\n\t \n\tfor (i = 0; i < 2; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\t\tint base = i * 2;\n\t\tvoid __iomem * const *iomap;\n\n\t\tif (ata_port_is_dummy(ap))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!ata_resources_present(pdev, i)) {\n\t\t\tap->ops = &ata_dummy_port_ops;\n\t\t\tcontinue;\n\t\t}\n\n\t\trc = pcim_iomap_regions(pdev, 0x3 << base,\n\t\t\t\t\tdev_driver_string(gdev));\n\t\tif (rc) {\n\t\t\tdev_warn(gdev,\n\t\t\t\t \"failed to request/iomap BARs for port %d (errno=%d)\\n\",\n\t\t\t\t i, rc);\n\t\t\tif (rc == -EBUSY)\n\t\t\t\tpcim_pin_device(pdev);\n\t\t\tap->ops = &ata_dummy_port_ops;\n\t\t\tcontinue;\n\t\t}\n\t\thost->iomap = iomap = pcim_iomap_table(pdev);\n\n\t\tap->ioaddr.cmd_addr = iomap[base];\n\t\tap->ioaddr.altstatus_addr =\n\t\tap->ioaddr.ctl_addr = (void __iomem *)\n\t\t\t((unsigned long)iomap[base + 1] | ATA_PCI_CTL_OFS);\n\t\tata_sff_std_ports(&ap->ioaddr);\n\n\t\tata_port_desc(ap, \"cmd 0x%llx ctl 0x%llx\",\n\t\t\t(unsigned long long)pci_resource_start(pdev, base),\n\t\t\t(unsigned long long)pci_resource_start(pdev, base + 1));\n\n\t\tmask |= 1 << i;\n\t}\n\n\tif (!mask) {\n\t\tdev_err(gdev, \"no available native port\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_pci_sff_init_host);\n\n \nint ata_pci_sff_prepare_host(struct pci_dev *pdev,\n\t\t\t     const struct ata_port_info * const *ppi,\n\t\t\t     struct ata_host **r_host)\n{\n\tstruct ata_host *host;\n\tint rc;\n\n\tif (!devres_open_group(&pdev->dev, NULL, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\thost = ata_host_alloc_pinfo(&pdev->dev, ppi, 2);\n\tif (!host) {\n\t\tdev_err(&pdev->dev, \"failed to allocate ATA host\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\trc = ata_pci_sff_init_host(host);\n\tif (rc)\n\t\tgoto err_out;\n\n\tdevres_remove_group(&pdev->dev, NULL);\n\t*r_host = host;\n\treturn 0;\n\nerr_out:\n\tdevres_release_group(&pdev->dev, NULL);\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ata_pci_sff_prepare_host);\n\n \nint ata_pci_sff_activate_host(struct ata_host *host,\n\t\t\t      irq_handler_t irq_handler,\n\t\t\t      const struct scsi_host_template *sht)\n{\n\tstruct device *dev = host->dev;\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tconst char *drv_name = dev_driver_string(host->dev);\n\tint legacy_mode = 0, rc;\n\n\trc = ata_host_start(host);\n\tif (rc)\n\t\treturn rc;\n\n\tif ((pdev->class >> 8) == PCI_CLASS_STORAGE_IDE) {\n\t\tu8 tmp8, mask = 0;\n\n\t\t \n\t\tpci_read_config_byte(pdev, PCI_CLASS_PROG, &tmp8);\n\t\tif (!ata_port_is_dummy(host->ports[0]))\n\t\t\tmask |= (1 << 0);\n\t\tif (!ata_port_is_dummy(host->ports[1]))\n\t\t\tmask |= (1 << 2);\n\t\tif ((tmp8 & mask) != mask)\n\t\t\tlegacy_mode = 1;\n\t}\n\n\tif (!devres_open_group(dev, NULL, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tif (!legacy_mode && pdev->irq) {\n\t\tint i;\n\n\t\trc = devm_request_irq(dev, pdev->irq, irq_handler,\n\t\t\t\t      IRQF_SHARED, drv_name, host);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\tif (ata_port_is_dummy(host->ports[i]))\n\t\t\t\tcontinue;\n\t\t\tata_port_desc(host->ports[i], \"irq %d\", pdev->irq);\n\t\t}\n\t} else if (legacy_mode) {\n\t\tif (!ata_port_is_dummy(host->ports[0])) {\n\t\t\trc = devm_request_irq(dev, ATA_PRIMARY_IRQ(pdev),\n\t\t\t\t\t      irq_handler, IRQF_SHARED,\n\t\t\t\t\t      drv_name, host);\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\n\t\t\tata_port_desc(host->ports[0], \"irq %d\",\n\t\t\t\t      ATA_PRIMARY_IRQ(pdev));\n\t\t}\n\n\t\tif (!ata_port_is_dummy(host->ports[1])) {\n\t\t\trc = devm_request_irq(dev, ATA_SECONDARY_IRQ(pdev),\n\t\t\t\t\t      irq_handler, IRQF_SHARED,\n\t\t\t\t\t      drv_name, host);\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\n\t\t\tata_port_desc(host->ports[1], \"irq %d\",\n\t\t\t\t      ATA_SECONDARY_IRQ(pdev));\n\t\t}\n\t}\n\n\trc = ata_host_register(host, sht);\nout:\n\tif (rc == 0)\n\t\tdevres_remove_group(dev, NULL);\n\telse\n\t\tdevres_release_group(dev, NULL);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ata_pci_sff_activate_host);\n\nstatic const struct ata_port_info *ata_sff_find_valid_pi(\n\t\t\t\t\tconst struct ata_port_info * const *ppi)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < 2 && ppi[i]; i++)\n\t\tif (ppi[i]->port_ops != &ata_dummy_port_ops)\n\t\t\treturn ppi[i];\n\n\treturn NULL;\n}\n\nstatic int ata_pci_init_one(struct pci_dev *pdev,\n\t\tconst struct ata_port_info * const *ppi,\n\t\tconst struct scsi_host_template *sht, void *host_priv,\n\t\tint hflags, bool bmdma)\n{\n\tstruct device *dev = &pdev->dev;\n\tconst struct ata_port_info *pi;\n\tstruct ata_host *host = NULL;\n\tint rc;\n\n\tpi = ata_sff_find_valid_pi(ppi);\n\tif (!pi) {\n\t\tdev_err(&pdev->dev, \"no valid port_info specified\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!devres_open_group(dev, NULL, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\trc = pcim_enable_device(pdev);\n\tif (rc)\n\t\tgoto out;\n\n#ifdef CONFIG_ATA_BMDMA\n\tif (bmdma)\n\t\t \n\t\trc = ata_pci_bmdma_prepare_host(pdev, ppi, &host);\n\telse\n#endif\n\t\t \n\t\trc = ata_pci_sff_prepare_host(pdev, ppi, &host);\n\tif (rc)\n\t\tgoto out;\n\thost->private_data = host_priv;\n\thost->flags |= hflags;\n\n#ifdef CONFIG_ATA_BMDMA\n\tif (bmdma) {\n\t\tpci_set_master(pdev);\n\t\trc = ata_pci_sff_activate_host(host, ata_bmdma_interrupt, sht);\n\t} else\n#endif\n\t\trc = ata_pci_sff_activate_host(host, ata_sff_interrupt, sht);\nout:\n\tif (rc == 0)\n\t\tdevres_remove_group(&pdev->dev, NULL);\n\telse\n\t\tdevres_release_group(&pdev->dev, NULL);\n\n\treturn rc;\n}\n\n \nint ata_pci_sff_init_one(struct pci_dev *pdev,\n\t\t const struct ata_port_info * const *ppi,\n\t\t const struct scsi_host_template *sht, void *host_priv, int hflag)\n{\n\treturn ata_pci_init_one(pdev, ppi, sht, host_priv, hflag, 0);\n}\nEXPORT_SYMBOL_GPL(ata_pci_sff_init_one);\n\n#endif  \n\n \n\n#ifdef CONFIG_ATA_BMDMA\n\nconst struct ata_port_operations ata_bmdma_port_ops = {\n\t.inherits\t\t= &ata_sff_port_ops,\n\n\t.error_handler\t\t= ata_bmdma_error_handler,\n\t.post_internal_cmd\t= ata_bmdma_post_internal_cmd,\n\n\t.qc_prep\t\t= ata_bmdma_qc_prep,\n\t.qc_issue\t\t= ata_bmdma_qc_issue,\n\n\t.sff_irq_clear\t\t= ata_bmdma_irq_clear,\n\t.bmdma_setup\t\t= ata_bmdma_setup,\n\t.bmdma_start\t\t= ata_bmdma_start,\n\t.bmdma_stop\t\t= ata_bmdma_stop,\n\t.bmdma_status\t\t= ata_bmdma_status,\n\n\t.port_start\t\t= ata_bmdma_port_start,\n};\nEXPORT_SYMBOL_GPL(ata_bmdma_port_ops);\n\nconst struct ata_port_operations ata_bmdma32_port_ops = {\n\t.inherits\t\t= &ata_bmdma_port_ops,\n\n\t.sff_data_xfer\t\t= ata_sff_data_xfer32,\n\t.port_start\t\t= ata_bmdma_port_start32,\n};\nEXPORT_SYMBOL_GPL(ata_bmdma32_port_ops);\n\n \nstatic void ata_bmdma_fill_sg(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_bmdma_prd *prd = ap->bmdma_prd;\n\tstruct scatterlist *sg;\n\tunsigned int si, pi;\n\n\tpi = 0;\n\tfor_each_sg(qc->sg, sg, qc->n_elem, si) {\n\t\tu32 addr, offset;\n\t\tu32 sg_len, len;\n\n\t\t \n\t\taddr = (u32) sg_dma_address(sg);\n\t\tsg_len = sg_dma_len(sg);\n\n\t\twhile (sg_len) {\n\t\t\toffset = addr & 0xffff;\n\t\t\tlen = sg_len;\n\t\t\tif ((offset + sg_len) > 0x10000)\n\t\t\t\tlen = 0x10000 - offset;\n\n\t\t\tprd[pi].addr = cpu_to_le32(addr);\n\t\t\tprd[pi].flags_len = cpu_to_le32(len & 0xffff);\n\n\t\t\tpi++;\n\t\t\tsg_len -= len;\n\t\t\taddr += len;\n\t\t}\n\t}\n\n\tprd[pi - 1].flags_len |= cpu_to_le32(ATA_PRD_EOT);\n}\n\n \nstatic void ata_bmdma_fill_sg_dumb(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_bmdma_prd *prd = ap->bmdma_prd;\n\tstruct scatterlist *sg;\n\tunsigned int si, pi;\n\n\tpi = 0;\n\tfor_each_sg(qc->sg, sg, qc->n_elem, si) {\n\t\tu32 addr, offset;\n\t\tu32 sg_len, len, blen;\n\n\t\t \n\t\taddr = (u32) sg_dma_address(sg);\n\t\tsg_len = sg_dma_len(sg);\n\n\t\twhile (sg_len) {\n\t\t\toffset = addr & 0xffff;\n\t\t\tlen = sg_len;\n\t\t\tif ((offset + sg_len) > 0x10000)\n\t\t\t\tlen = 0x10000 - offset;\n\n\t\t\tblen = len & 0xffff;\n\t\t\tprd[pi].addr = cpu_to_le32(addr);\n\t\t\tif (blen == 0) {\n\t\t\t\t \n\t\t\t\tprd[pi].flags_len = cpu_to_le32(0x8000);\n\t\t\t\tblen = 0x8000;\n\t\t\t\tprd[++pi].addr = cpu_to_le32(addr + 0x8000);\n\t\t\t}\n\t\t\tprd[pi].flags_len = cpu_to_le32(blen);\n\n\t\t\tpi++;\n\t\t\tsg_len -= len;\n\t\t\taddr += len;\n\t\t}\n\t}\n\n\tprd[pi - 1].flags_len |= cpu_to_le32(ATA_PRD_EOT);\n}\n\n \nenum ata_completion_errors ata_bmdma_qc_prep(struct ata_queued_cmd *qc)\n{\n\tif (!(qc->flags & ATA_QCFLAG_DMAMAP))\n\t\treturn AC_ERR_OK;\n\n\tata_bmdma_fill_sg(qc);\n\n\treturn AC_ERR_OK;\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_qc_prep);\n\n \nenum ata_completion_errors ata_bmdma_dumb_qc_prep(struct ata_queued_cmd *qc)\n{\n\tif (!(qc->flags & ATA_QCFLAG_DMAMAP))\n\t\treturn AC_ERR_OK;\n\n\tata_bmdma_fill_sg_dumb(qc);\n\n\treturn AC_ERR_OK;\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_dumb_qc_prep);\n\n \nunsigned int ata_bmdma_qc_issue(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct ata_link *link = qc->dev->link;\n\n\t \n\tif (!ata_is_dma(qc->tf.protocol))\n\t\treturn ata_sff_qc_issue(qc);\n\n\t \n\tata_dev_select(ap, qc->dev->devno, 1, 0);\n\n\t \n\tswitch (qc->tf.protocol) {\n\tcase ATA_PROT_DMA:\n\t\tWARN_ON_ONCE(qc->tf.flags & ATA_TFLAG_POLLING);\n\n\t\ttrace_ata_tf_load(ap, &qc->tf);\n\t\tap->ops->sff_tf_load(ap, &qc->tf);   \n\t\ttrace_ata_bmdma_setup(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_setup(qc);\t     \n\t\ttrace_ata_bmdma_start(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_start(qc);\t     \n\t\tap->hsm_task_state = HSM_ST_LAST;\n\t\tbreak;\n\n\tcase ATAPI_PROT_DMA:\n\t\tWARN_ON_ONCE(qc->tf.flags & ATA_TFLAG_POLLING);\n\n\t\ttrace_ata_tf_load(ap, &qc->tf);\n\t\tap->ops->sff_tf_load(ap, &qc->tf);   \n\t\ttrace_ata_bmdma_setup(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_setup(qc);\t     \n\t\tap->hsm_task_state = HSM_ST_FIRST;\n\n\t\t \n\t\tif (!(qc->dev->flags & ATA_DFLAG_CDB_INTR))\n\t\t\tata_sff_queue_pio_task(link, 0);\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn AC_ERR_SYSTEM;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_qc_issue);\n\n \nunsigned int ata_bmdma_port_intr(struct ata_port *ap, struct ata_queued_cmd *qc)\n{\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\tu8 host_stat = 0;\n\tbool bmdma_stopped = false;\n\tunsigned int handled;\n\n\tif (ap->hsm_task_state == HSM_ST_LAST && ata_is_dma(qc->tf.protocol)) {\n\t\t \n\t\thost_stat = ap->ops->bmdma_status(ap);\n\t\ttrace_ata_bmdma_status(ap, host_stat);\n\n\t\t \n\t\tif (!(host_stat & ATA_DMA_INTR))\n\t\t\treturn ata_sff_idle_irq(ap);\n\n\t\t \n\t\ttrace_ata_bmdma_stop(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_stop(qc);\n\t\tbmdma_stopped = true;\n\n\t\tif (unlikely(host_stat & ATA_DMA_ERR)) {\n\t\t\t \n\t\t\tqc->err_mask |= AC_ERR_HOST_BUS;\n\t\t\tap->hsm_task_state = HSM_ST_ERR;\n\t\t}\n\t}\n\n\thandled = __ata_sff_port_intr(ap, qc, bmdma_stopped);\n\n\tif (unlikely(qc->err_mask) && ata_is_dma(qc->tf.protocol))\n\t\tata_ehi_push_desc(ehi, \"BMDMA stat 0x%x\", host_stat);\n\n\treturn handled;\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_port_intr);\n\n \nirqreturn_t ata_bmdma_interrupt(int irq, void *dev_instance)\n{\n\treturn __ata_sff_interrupt(irq, dev_instance, ata_bmdma_port_intr);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_interrupt);\n\n \nvoid ata_bmdma_error_handler(struct ata_port *ap)\n{\n\tstruct ata_queued_cmd *qc;\n\tunsigned long flags;\n\tbool thaw = false;\n\n\tqc = __ata_qc_from_tag(ap, ap->link.active_tag);\n\tif (qc && !(qc->flags & ATA_QCFLAG_EH))\n\t\tqc = NULL;\n\n\t \n\tspin_lock_irqsave(ap->lock, flags);\n\n\tif (qc && ata_is_dma(qc->tf.protocol)) {\n\t\tu8 host_stat;\n\n\t\thost_stat = ap->ops->bmdma_status(ap);\n\t\ttrace_ata_bmdma_status(ap, host_stat);\n\n\t\t \n\t\tif (qc->err_mask == AC_ERR_TIMEOUT && (host_stat & ATA_DMA_ERR)) {\n\t\t\tqc->err_mask = AC_ERR_HOST_BUS;\n\t\t\tthaw = true;\n\t\t}\n\n\t\ttrace_ata_bmdma_stop(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_stop(qc);\n\n\t\t \n\t\tif (thaw) {\n\t\t\tap->ops->sff_check_status(ap);\n\t\t\tif (ap->ops->sff_irq_clear)\n\t\t\t\tap->ops->sff_irq_clear(ap);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(ap->lock, flags);\n\n\tif (thaw)\n\t\tata_eh_thaw_port(ap);\n\n\tata_sff_error_handler(ap);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_error_handler);\n\n \nvoid ata_bmdma_post_internal_cmd(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tunsigned long flags;\n\n\tif (ata_is_dma(qc->tf.protocol)) {\n\t\tspin_lock_irqsave(ap->lock, flags);\n\t\ttrace_ata_bmdma_stop(ap, &qc->tf, qc->tag);\n\t\tap->ops->bmdma_stop(qc);\n\t\tspin_unlock_irqrestore(ap->lock, flags);\n\t}\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_post_internal_cmd);\n\n \nvoid ata_bmdma_irq_clear(struct ata_port *ap)\n{\n\tvoid __iomem *mmio = ap->ioaddr.bmdma_addr;\n\n\tif (!mmio)\n\t\treturn;\n\n\tiowrite8(ioread8(mmio + ATA_DMA_STATUS), mmio + ATA_DMA_STATUS);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_irq_clear);\n\n \nvoid ata_bmdma_setup(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tunsigned int rw = (qc->tf.flags & ATA_TFLAG_WRITE);\n\tu8 dmactl;\n\n\t \n\tmb();\t \n\tiowrite32(ap->bmdma_prd_dma, ap->ioaddr.bmdma_addr + ATA_DMA_TABLE_OFS);\n\n\t \n\tdmactl = ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n\tdmactl &= ~(ATA_DMA_WR | ATA_DMA_START);\n\tif (!rw)\n\t\tdmactl |= ATA_DMA_WR;\n\tiowrite8(dmactl, ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n\n\t \n\tap->ops->sff_exec_command(ap, &qc->tf);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_setup);\n\n \nvoid ata_bmdma_start(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tu8 dmactl;\n\n\t \n\tdmactl = ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n\tiowrite8(dmactl | ATA_DMA_START, ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n\n\t \n}\nEXPORT_SYMBOL_GPL(ata_bmdma_start);\n\n \nvoid ata_bmdma_stop(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tvoid __iomem *mmio = ap->ioaddr.bmdma_addr;\n\n\t \n\tiowrite8(ioread8(mmio + ATA_DMA_CMD) & ~ATA_DMA_START,\n\t\t mmio + ATA_DMA_CMD);\n\n\t \n\tata_sff_dma_pause(ap);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_stop);\n\n \nu8 ata_bmdma_status(struct ata_port *ap)\n{\n\treturn ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_STATUS);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_status);\n\n\n \nint ata_bmdma_port_start(struct ata_port *ap)\n{\n\tif (ap->mwdma_mask || ap->udma_mask) {\n\t\tap->bmdma_prd =\n\t\t\tdmam_alloc_coherent(ap->host->dev, ATA_PRD_TBL_SZ,\n\t\t\t\t\t    &ap->bmdma_prd_dma, GFP_KERNEL);\n\t\tif (!ap->bmdma_prd)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_port_start);\n\n \nint ata_bmdma_port_start32(struct ata_port *ap)\n{\n\tap->pflags |= ATA_PFLAG_PIO32 | ATA_PFLAG_PIO32CHANGE;\n\treturn ata_bmdma_port_start(ap);\n}\nEXPORT_SYMBOL_GPL(ata_bmdma_port_start32);\n\n#ifdef CONFIG_PCI\n\n \nint ata_pci_bmdma_clear_simplex(struct pci_dev *pdev)\n{\n\tunsigned long bmdma = pci_resource_start(pdev, 4);\n\tu8 simplex;\n\n\tif (bmdma == 0)\n\t\treturn -ENOENT;\n\n\tsimplex = inb(bmdma + 0x02);\n\toutb(simplex & 0x60, bmdma + 0x02);\n\tsimplex = inb(bmdma + 0x02);\n\tif (simplex & 0x80)\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_pci_bmdma_clear_simplex);\n\nstatic void ata_bmdma_nodma(struct ata_host *host, const char *reason)\n{\n\tint i;\n\n\tdev_err(host->dev, \"BMDMA: %s, falling back to PIO\\n\", reason);\n\n\tfor (i = 0; i < 2; i++) {\n\t\thost->ports[i]->mwdma_mask = 0;\n\t\thost->ports[i]->udma_mask = 0;\n\t}\n}\n\n \nvoid ata_pci_bmdma_init(struct ata_host *host)\n{\n\tstruct device *gdev = host->dev;\n\tstruct pci_dev *pdev = to_pci_dev(gdev);\n\tint i, rc;\n\n\t \n\tif (pci_resource_start(pdev, 4) == 0) {\n\t\tata_bmdma_nodma(host, \"BAR4 is zero\");\n\t\treturn;\n\t}\n\n\t \n\trc = dma_set_mask_and_coherent(&pdev->dev, ATA_DMA_MASK);\n\tif (rc)\n\t\tata_bmdma_nodma(host, \"failed to set dma mask\");\n\n\t \n\trc = pcim_iomap_regions(pdev, 1 << 4, dev_driver_string(gdev));\n\tif (rc) {\n\t\tata_bmdma_nodma(host, \"failed to request/iomap BAR4\");\n\t\treturn;\n\t}\n\thost->iomap = pcim_iomap_table(pdev);\n\n\tfor (i = 0; i < 2; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\t\tvoid __iomem *bmdma = host->iomap[4] + 8 * i;\n\n\t\tif (ata_port_is_dummy(ap))\n\t\t\tcontinue;\n\n\t\tap->ioaddr.bmdma_addr = bmdma;\n\t\tif ((!(ap->flags & ATA_FLAG_IGN_SIMPLEX)) &&\n\t\t    (ioread8(bmdma + 2) & 0x80))\n\t\t\thost->flags |= ATA_HOST_SIMPLEX;\n\n\t\tata_port_desc(ap, \"bmdma 0x%llx\",\n\t\t    (unsigned long long)pci_resource_start(pdev, 4) + 8 * i);\n\t}\n}\nEXPORT_SYMBOL_GPL(ata_pci_bmdma_init);\n\n \nint ata_pci_bmdma_prepare_host(struct pci_dev *pdev,\n\t\t\t       const struct ata_port_info * const * ppi,\n\t\t\t       struct ata_host **r_host)\n{\n\tint rc;\n\n\trc = ata_pci_sff_prepare_host(pdev, ppi, r_host);\n\tif (rc)\n\t\treturn rc;\n\n\tata_pci_bmdma_init(*r_host);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ata_pci_bmdma_prepare_host);\n\n \nint ata_pci_bmdma_init_one(struct pci_dev *pdev,\n\t\t\t   const struct ata_port_info * const * ppi,\n\t\t\t   const struct scsi_host_template *sht, void *host_priv,\n\t\t\t   int hflags)\n{\n\treturn ata_pci_init_one(pdev, ppi, sht, host_priv, hflags, 1);\n}\nEXPORT_SYMBOL_GPL(ata_pci_bmdma_init_one);\n\n#endif  \n#endif  \n\n \nvoid ata_sff_port_init(struct ata_port *ap)\n{\n\tINIT_DELAYED_WORK(&ap->sff_pio_task, ata_sff_pio_task);\n\tap->ctl = ATA_DEVCTL_OBS;\n\tap->last_ctl = 0xFF;\n}\n\nint __init ata_sff_init(void)\n{\n\tata_sff_wq = alloc_workqueue(\"ata_sff\", WQ_MEM_RECLAIM, WQ_MAX_ACTIVE);\n\tif (!ata_sff_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid ata_sff_exit(void)\n{\n\tdestroy_workqueue(ata_sff_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}