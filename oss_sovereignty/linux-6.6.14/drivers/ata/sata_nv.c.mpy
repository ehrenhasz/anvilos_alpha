{
  "module_name": "sata_nv.c",
  "hash_id": "e1378217df845ad1b5ca0b4b017c6b77c641bd1c2400eda734880016ff75ccab",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ata/sata_nv.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/gfp.h>\n#include <linux/pci.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/device.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_device.h>\n#include <linux/libata.h>\n#include <trace/events/libata.h>\n\n#define DRV_NAME\t\t\t\"sata_nv\"\n#define DRV_VERSION\t\t\t\"3.5\"\n\n#define NV_ADMA_DMA_BOUNDARY\t\t0xffffffffUL\n\nenum {\n\tNV_MMIO_BAR\t\t\t= 5,\n\n\tNV_PORTS\t\t\t= 2,\n\tNV_PIO_MASK\t\t\t= ATA_PIO4,\n\tNV_MWDMA_MASK\t\t\t= ATA_MWDMA2,\n\tNV_UDMA_MASK\t\t\t= ATA_UDMA6,\n\tNV_PORT0_SCR_REG_OFFSET\t\t= 0x00,\n\tNV_PORT1_SCR_REG_OFFSET\t\t= 0x40,\n\n\t \n\tNV_INT_STATUS\t\t\t= 0x10,\n\tNV_INT_ENABLE\t\t\t= 0x11,\n\tNV_INT_STATUS_CK804\t\t= 0x440,\n\tNV_INT_ENABLE_CK804\t\t= 0x441,\n\n\t \n\tNV_INT_DEV\t\t\t= 0x01,\n\tNV_INT_PM\t\t\t= 0x02,\n\tNV_INT_ADDED\t\t\t= 0x04,\n\tNV_INT_REMOVED\t\t\t= 0x08,\n\n\tNV_INT_PORT_SHIFT\t\t= 4,\t \n\n\tNV_INT_ALL\t\t\t= 0x0f,\n\tNV_INT_MASK\t\t\t= NV_INT_DEV |\n\t\t\t\t\t  NV_INT_ADDED | NV_INT_REMOVED,\n\n\t \n\tNV_INT_CONFIG\t\t\t= 0x12,\n\tNV_INT_CONFIG_METHD\t\t= 0x01, \n\n\t\n\tNV_MCP_SATA_CFG_20\t\t= 0x50,\n\tNV_MCP_SATA_CFG_20_SATA_SPACE_EN = 0x04,\n\tNV_MCP_SATA_CFG_20_PORT0_EN\t= (1 << 17),\n\tNV_MCP_SATA_CFG_20_PORT1_EN\t= (1 << 16),\n\tNV_MCP_SATA_CFG_20_PORT0_PWB_EN\t= (1 << 14),\n\tNV_MCP_SATA_CFG_20_PORT1_PWB_EN\t= (1 << 12),\n\n\tNV_ADMA_MAX_CPBS\t\t= 32,\n\tNV_ADMA_CPB_SZ\t\t\t= 128,\n\tNV_ADMA_APRD_SZ\t\t\t= 16,\n\tNV_ADMA_SGTBL_LEN\t\t= (1024 - NV_ADMA_CPB_SZ) /\n\t\t\t\t\t   NV_ADMA_APRD_SZ,\n\tNV_ADMA_SGTBL_TOTAL_LEN\t\t= NV_ADMA_SGTBL_LEN + 5,\n\tNV_ADMA_SGTBL_SZ                = NV_ADMA_SGTBL_LEN * NV_ADMA_APRD_SZ,\n\tNV_ADMA_PORT_PRIV_DMA_SZ        = NV_ADMA_MAX_CPBS *\n\t\t\t\t\t   (NV_ADMA_CPB_SZ + NV_ADMA_SGTBL_SZ),\n\n\t \n\tNV_ADMA_GEN\t\t\t= 0x400,\n\tNV_ADMA_GEN_CTL\t\t\t= 0x00,\n\tNV_ADMA_NOTIFIER_CLEAR\t\t= 0x30,\n\n\t \n\tNV_ADMA_PORT\t\t\t= 0x480,\n\n\t \n\tNV_ADMA_PORT_SIZE\t\t= 0x100,\n\n\t \n\tNV_ADMA_CTL\t\t\t= 0x40,\n\tNV_ADMA_CPB_COUNT\t\t= 0x42,\n\tNV_ADMA_NEXT_CPB_IDX\t\t= 0x43,\n\tNV_ADMA_STAT\t\t\t= 0x44,\n\tNV_ADMA_CPB_BASE_LOW\t\t= 0x48,\n\tNV_ADMA_CPB_BASE_HIGH\t\t= 0x4C,\n\tNV_ADMA_APPEND\t\t\t= 0x50,\n\tNV_ADMA_NOTIFIER\t\t= 0x68,\n\tNV_ADMA_NOTIFIER_ERROR\t\t= 0x6C,\n\n\t \n\tNV_ADMA_CTL_HOTPLUG_IEN\t\t= (1 << 0),\n\tNV_ADMA_CTL_CHANNEL_RESET\t= (1 << 5),\n\tNV_ADMA_CTL_GO\t\t\t= (1 << 7),\n\tNV_ADMA_CTL_AIEN\t\t= (1 << 8),\n\tNV_ADMA_CTL_READ_NON_COHERENT\t= (1 << 11),\n\tNV_ADMA_CTL_WRITE_NON_COHERENT\t= (1 << 12),\n\n\t \n\tNV_CPB_RESP_DONE\t\t= (1 << 0),\n\tNV_CPB_RESP_ATA_ERR\t\t= (1 << 3),\n\tNV_CPB_RESP_CMD_ERR\t\t= (1 << 4),\n\tNV_CPB_RESP_CPB_ERR\t\t= (1 << 7),\n\n\t \n\tNV_CPB_CTL_CPB_VALID\t\t= (1 << 0),\n\tNV_CPB_CTL_QUEUE\t\t= (1 << 1),\n\tNV_CPB_CTL_APRD_VALID\t\t= (1 << 2),\n\tNV_CPB_CTL_IEN\t\t\t= (1 << 3),\n\tNV_CPB_CTL_FPDMA\t\t= (1 << 4),\n\n\t \n\tNV_APRD_WRITE\t\t\t= (1 << 1),\n\tNV_APRD_END\t\t\t= (1 << 2),\n\tNV_APRD_CONT\t\t\t= (1 << 3),\n\n\t \n\tNV_ADMA_STAT_TIMEOUT\t\t= (1 << 0),\n\tNV_ADMA_STAT_HOTUNPLUG\t\t= (1 << 1),\n\tNV_ADMA_STAT_HOTPLUG\t\t= (1 << 2),\n\tNV_ADMA_STAT_CPBERR\t\t= (1 << 4),\n\tNV_ADMA_STAT_SERROR\t\t= (1 << 5),\n\tNV_ADMA_STAT_CMD_COMPLETE\t= (1 << 6),\n\tNV_ADMA_STAT_IDLE\t\t= (1 << 8),\n\tNV_ADMA_STAT_LEGACY\t\t= (1 << 9),\n\tNV_ADMA_STAT_STOPPED\t\t= (1 << 10),\n\tNV_ADMA_STAT_DONE\t\t= (1 << 12),\n\tNV_ADMA_STAT_ERR\t\t= NV_ADMA_STAT_CPBERR |\n\t\t\t\t\t  NV_ADMA_STAT_TIMEOUT,\n\n\t \n\tNV_ADMA_PORT_REGISTER_MODE\t= (1 << 0),\n\tNV_ADMA_ATAPI_SETUP_COMPLETE\t= (1 << 1),\n\n\t \n\tNV_CTL_MCP55\t\t\t= 0x400,\n\tNV_INT_STATUS_MCP55\t\t= 0x440,\n\tNV_INT_ENABLE_MCP55\t\t= 0x444,\n\tNV_NCQ_REG_MCP55\t\t= 0x448,\n\n\t \n\tNV_INT_ALL_MCP55\t\t= 0xffff,\n\tNV_INT_PORT_SHIFT_MCP55\t\t= 16,\t \n\tNV_INT_MASK_MCP55\t\t= NV_INT_ALL_MCP55 & 0xfffd,\n\n\t \n\tNV_CTL_PRI_SWNCQ\t\t= 0x02,\n\tNV_CTL_SEC_SWNCQ\t\t= 0x04,\n\n\t \n\tNV_SWNCQ_IRQ_DEV\t\t= (1 << 0),\n\tNV_SWNCQ_IRQ_PM\t\t\t= (1 << 1),\n\tNV_SWNCQ_IRQ_ADDED\t\t= (1 << 2),\n\tNV_SWNCQ_IRQ_REMOVED\t\t= (1 << 3),\n\n\tNV_SWNCQ_IRQ_BACKOUT\t\t= (1 << 4),\n\tNV_SWNCQ_IRQ_SDBFIS\t\t= (1 << 5),\n\tNV_SWNCQ_IRQ_DHREGFIS\t\t= (1 << 6),\n\tNV_SWNCQ_IRQ_DMASETUP\t\t= (1 << 7),\n\n\tNV_SWNCQ_IRQ_HOTPLUG\t\t= NV_SWNCQ_IRQ_ADDED |\n\t\t\t\t\t  NV_SWNCQ_IRQ_REMOVED,\n\n};\n\n \nstruct nv_adma_prd {\n\t__le64\t\t\taddr;\n\t__le32\t\t\tlen;\n\tu8\t\t\tflags;\n\tu8\t\t\tpacket_len;\n\t__le16\t\t\treserved;\n};\n\nenum nv_adma_regbits {\n\tCMDEND\t= (1 << 15),\t\t \n\tWNB\t= (1 << 14),\t\t \n\tIGN\t= (1 << 13),\t\t \n\tCS1n\t= (1 << (4 + 8)),\t \n\tDA2\t= (1 << (2 + 8)),\n\tDA1\t= (1 << (1 + 8)),\n\tDA0\t= (1 << (0 + 8)),\n};\n\n \nstruct nv_adma_cpb {\n\tu8\t\t\tresp_flags;     \n\tu8\t\t\treserved1;      \n\tu8\t\t\tctl_flags;      \n\t \n\tu8\t\t\tlen;\t\t \n\tu8\t\t\ttag;            \n\tu8\t\t\tnext_cpb_idx;   \n\t__le16\t\t\treserved2;      \n\t__le16\t\t\ttf[12];         \n\tstruct nv_adma_prd\taprd[5];        \n\t__le64\t\t\tnext_aprd;      \n\t__le64\t\t\treserved3;      \n};\n\n\nstruct nv_adma_port_priv {\n\tstruct nv_adma_cpb\t*cpb;\n\tdma_addr_t\t\tcpb_dma;\n\tstruct nv_adma_prd\t*aprd;\n\tdma_addr_t\t\taprd_dma;\n\tvoid __iomem\t\t*ctl_block;\n\tvoid __iomem\t\t*gen_block;\n\tvoid __iomem\t\t*notifier_clear_block;\n\tu64\t\t\tadma_dma_mask;\n\tu8\t\t\tflags;\n\tint\t\t\tlast_issue_ncq;\n};\n\nstruct nv_host_priv {\n\tunsigned long\t\ttype;\n};\n\nstruct defer_queue {\n\tu32\t\tdefer_bits;\n\tunsigned int\thead;\n\tunsigned int\ttail;\n\tunsigned int\ttag[ATA_MAX_QUEUE];\n};\n\nenum ncq_saw_flag_list {\n\tncq_saw_d2h\t= (1U << 0),\n\tncq_saw_dmas\t= (1U << 1),\n\tncq_saw_sdb\t= (1U << 2),\n\tncq_saw_backout\t= (1U << 3),\n};\n\nstruct nv_swncq_port_priv {\n\tstruct ata_bmdma_prd *prd;\t  \n\tdma_addr_t\tprd_dma;  \n\tvoid __iomem\t*sactive_block;\n\tvoid __iomem\t*irq_block;\n\tvoid __iomem\t*tag_block;\n\tu32\t\tqc_active;\n\n\tunsigned int\tlast_issue_tag;\n\n\t \n\tstruct defer_queue defer_queue;\n\n\t \n\tu32\t\tdhfis_bits;\n\tu32\t\tdmafis_bits;\n\tu32\t\tsdbfis_bits;\n\n\tunsigned int\tncq_flags;\n};\n\n\n#define NV_ADMA_CHECK_INTR(GCTL, PORT) ((GCTL) & (1 << (19 + (12 * (PORT)))))\n\nstatic int nv_init_one(struct pci_dev *pdev, const struct pci_device_id *ent);\n#ifdef CONFIG_PM_SLEEP\nstatic int nv_pci_device_resume(struct pci_dev *pdev);\n#endif\nstatic void nv_ck804_host_stop(struct ata_host *host);\nstatic irqreturn_t nv_generic_interrupt(int irq, void *dev_instance);\nstatic irqreturn_t nv_nf2_interrupt(int irq, void *dev_instance);\nstatic irqreturn_t nv_ck804_interrupt(int irq, void *dev_instance);\nstatic int nv_scr_read(struct ata_link *link, unsigned int sc_reg, u32 *val);\nstatic int nv_scr_write(struct ata_link *link, unsigned int sc_reg, u32 val);\n\nstatic int nv_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\tunsigned long deadline);\nstatic void nv_nf2_freeze(struct ata_port *ap);\nstatic void nv_nf2_thaw(struct ata_port *ap);\nstatic void nv_ck804_freeze(struct ata_port *ap);\nstatic void nv_ck804_thaw(struct ata_port *ap);\nstatic int nv_adma_slave_config(struct scsi_device *sdev);\nstatic int nv_adma_check_atapi_dma(struct ata_queued_cmd *qc);\nstatic enum ata_completion_errors nv_adma_qc_prep(struct ata_queued_cmd *qc);\nstatic unsigned int nv_adma_qc_issue(struct ata_queued_cmd *qc);\nstatic irqreturn_t nv_adma_interrupt(int irq, void *dev_instance);\nstatic void nv_adma_irq_clear(struct ata_port *ap);\nstatic int nv_adma_port_start(struct ata_port *ap);\nstatic void nv_adma_port_stop(struct ata_port *ap);\n#ifdef CONFIG_PM\nstatic int nv_adma_port_suspend(struct ata_port *ap, pm_message_t mesg);\nstatic int nv_adma_port_resume(struct ata_port *ap);\n#endif\nstatic void nv_adma_freeze(struct ata_port *ap);\nstatic void nv_adma_thaw(struct ata_port *ap);\nstatic void nv_adma_error_handler(struct ata_port *ap);\nstatic void nv_adma_host_stop(struct ata_host *host);\nstatic void nv_adma_post_internal_cmd(struct ata_queued_cmd *qc);\nstatic void nv_adma_tf_read(struct ata_port *ap, struct ata_taskfile *tf);\n\nstatic void nv_mcp55_thaw(struct ata_port *ap);\nstatic void nv_mcp55_freeze(struct ata_port *ap);\nstatic void nv_swncq_error_handler(struct ata_port *ap);\nstatic int nv_swncq_slave_config(struct scsi_device *sdev);\nstatic int nv_swncq_port_start(struct ata_port *ap);\nstatic enum ata_completion_errors nv_swncq_qc_prep(struct ata_queued_cmd *qc);\nstatic void nv_swncq_fill_sg(struct ata_queued_cmd *qc);\nstatic unsigned int nv_swncq_qc_issue(struct ata_queued_cmd *qc);\nstatic void nv_swncq_irq_clear(struct ata_port *ap, u16 fis);\nstatic irqreturn_t nv_swncq_interrupt(int irq, void *dev_instance);\n#ifdef CONFIG_PM\nstatic int nv_swncq_port_suspend(struct ata_port *ap, pm_message_t mesg);\nstatic int nv_swncq_port_resume(struct ata_port *ap);\n#endif\n\nenum nv_host_type\n{\n\tGENERIC,\n\tNFORCE2,\n\tNFORCE3 = NFORCE2,\t \n\tCK804,\n\tADMA,\n\tMCP5x,\n\tSWNCQ,\n};\n\nstatic const struct pci_device_id nv_pci_tbl[] = {\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE2S_SATA), NFORCE2 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE3S_SATA), NFORCE3 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE3S_SATA2), NFORCE3 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_CK804_SATA), CK804 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_CK804_SATA2), CK804 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP04_SATA), CK804 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP04_SATA2), CK804 },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP51_SATA), MCP5x },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP51_SATA2), MCP5x },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP55_SATA), MCP5x },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP55_SATA2), MCP5x },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP61_SATA), GENERIC },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP61_SATA2), GENERIC },\n\t{ PCI_VDEVICE(NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_MCP61_SATA3), GENERIC },\n\n\t{ }  \n};\n\nstatic struct pci_driver nv_pci_driver = {\n\t.name\t\t\t= DRV_NAME,\n\t.id_table\t\t= nv_pci_tbl,\n\t.probe\t\t\t= nv_init_one,\n#ifdef CONFIG_PM_SLEEP\n\t.suspend\t\t= ata_pci_device_suspend,\n\t.resume\t\t\t= nv_pci_device_resume,\n#endif\n\t.remove\t\t\t= ata_pci_remove_one,\n};\n\nstatic const struct scsi_host_template nv_sht = {\n\tATA_BMDMA_SHT(DRV_NAME),\n};\n\nstatic const struct scsi_host_template nv_adma_sht = {\n\t__ATA_BASE_SHT(DRV_NAME),\n\t.can_queue\t\t= NV_ADMA_MAX_CPBS,\n\t.sg_tablesize\t\t= NV_ADMA_SGTBL_TOTAL_LEN,\n\t.dma_boundary\t\t= NV_ADMA_DMA_BOUNDARY,\n\t.slave_configure\t= nv_adma_slave_config,\n\t.sdev_groups\t\t= ata_ncq_sdev_groups,\n\t.change_queue_depth     = ata_scsi_change_queue_depth,\n\t.tag_alloc_policy\t= BLK_TAG_ALLOC_RR,\n};\n\nstatic const struct scsi_host_template nv_swncq_sht = {\n\t__ATA_BASE_SHT(DRV_NAME),\n\t.can_queue\t\t= ATA_MAX_QUEUE - 1,\n\t.sg_tablesize\t\t= LIBATA_MAX_PRD,\n\t.dma_boundary\t\t= ATA_DMA_BOUNDARY,\n\t.slave_configure\t= nv_swncq_slave_config,\n\t.sdev_groups\t\t= ata_ncq_sdev_groups,\n\t.change_queue_depth     = ata_scsi_change_queue_depth,\n\t.tag_alloc_policy\t= BLK_TAG_ALLOC_RR,\n};\n\n \nstatic struct ata_port_operations nv_generic_ops = {\n\t.inherits\t\t= &ata_bmdma_port_ops,\n\t.lost_interrupt\t\t= ATA_OP_NULL,\n\t.scr_read\t\t= nv_scr_read,\n\t.scr_write\t\t= nv_scr_write,\n\t.hardreset\t\t= nv_hardreset,\n};\n\nstatic struct ata_port_operations nv_nf2_ops = {\n\t.inherits\t\t= &nv_generic_ops,\n\t.freeze\t\t\t= nv_nf2_freeze,\n\t.thaw\t\t\t= nv_nf2_thaw,\n};\n\nstatic struct ata_port_operations nv_ck804_ops = {\n\t.inherits\t\t= &nv_generic_ops,\n\t.freeze\t\t\t= nv_ck804_freeze,\n\t.thaw\t\t\t= nv_ck804_thaw,\n\t.host_stop\t\t= nv_ck804_host_stop,\n};\n\nstatic struct ata_port_operations nv_adma_ops = {\n\t.inherits\t\t= &nv_ck804_ops,\n\n\t.check_atapi_dma\t= nv_adma_check_atapi_dma,\n\t.sff_tf_read\t\t= nv_adma_tf_read,\n\t.qc_defer\t\t= ata_std_qc_defer,\n\t.qc_prep\t\t= nv_adma_qc_prep,\n\t.qc_issue\t\t= nv_adma_qc_issue,\n\t.sff_irq_clear\t\t= nv_adma_irq_clear,\n\n\t.freeze\t\t\t= nv_adma_freeze,\n\t.thaw\t\t\t= nv_adma_thaw,\n\t.error_handler\t\t= nv_adma_error_handler,\n\t.post_internal_cmd\t= nv_adma_post_internal_cmd,\n\n\t.port_start\t\t= nv_adma_port_start,\n\t.port_stop\t\t= nv_adma_port_stop,\n#ifdef CONFIG_PM\n\t.port_suspend\t\t= nv_adma_port_suspend,\n\t.port_resume\t\t= nv_adma_port_resume,\n#endif\n\t.host_stop\t\t= nv_adma_host_stop,\n};\n\nstatic struct ata_port_operations nv_swncq_ops = {\n\t.inherits\t\t= &nv_generic_ops,\n\n\t.qc_defer\t\t= ata_std_qc_defer,\n\t.qc_prep\t\t= nv_swncq_qc_prep,\n\t.qc_issue\t\t= nv_swncq_qc_issue,\n\n\t.freeze\t\t\t= nv_mcp55_freeze,\n\t.thaw\t\t\t= nv_mcp55_thaw,\n\t.error_handler\t\t= nv_swncq_error_handler,\n\n#ifdef CONFIG_PM\n\t.port_suspend\t\t= nv_swncq_port_suspend,\n\t.port_resume\t\t= nv_swncq_port_resume,\n#endif\n\t.port_start\t\t= nv_swncq_port_start,\n};\n\nstruct nv_pi_priv {\n\tirq_handler_t\t\t\tirq_handler;\n\tconst struct scsi_host_template\t*sht;\n};\n\n#define NV_PI_PRIV(_irq_handler, _sht) \\\n\t&(struct nv_pi_priv){ .irq_handler = _irq_handler, .sht = _sht }\n\nstatic const struct ata_port_info nv_port_info[] = {\n\t \n\t{\n\t\t.flags\t\t= ATA_FLAG_SATA,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_generic_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_generic_interrupt, &nv_sht),\n\t},\n\t \n\t{\n\t\t.flags\t\t= ATA_FLAG_SATA,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_nf2_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_nf2_interrupt, &nv_sht),\n\t},\n\t \n\t{\n\t\t.flags\t\t= ATA_FLAG_SATA,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_ck804_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_ck804_interrupt, &nv_sht),\n\t},\n\t \n\t{\n\t\t.flags\t\t= ATA_FLAG_SATA | ATA_FLAG_NCQ,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_adma_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_adma_interrupt, &nv_adma_sht),\n\t},\n\t \n\t{\n\t\t.flags\t\t= ATA_FLAG_SATA,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_generic_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_generic_interrupt, &nv_sht),\n\t},\n\t \n\t{\n\t\t.flags\t        = ATA_FLAG_SATA | ATA_FLAG_NCQ,\n\t\t.pio_mask\t= NV_PIO_MASK,\n\t\t.mwdma_mask\t= NV_MWDMA_MASK,\n\t\t.udma_mask\t= NV_UDMA_MASK,\n\t\t.port_ops\t= &nv_swncq_ops,\n\t\t.private_data\t= NV_PI_PRIV(nv_swncq_interrupt, &nv_swncq_sht),\n\t},\n};\n\nMODULE_AUTHOR(\"NVIDIA\");\nMODULE_DESCRIPTION(\"low-level driver for NVIDIA nForce SATA controller\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DEVICE_TABLE(pci, nv_pci_tbl);\nMODULE_VERSION(DRV_VERSION);\n\nstatic bool adma_enabled;\nstatic bool swncq_enabled = true;\nstatic bool msi_enabled;\n\nstatic void nv_adma_register_mode(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu16 tmp, status;\n\tint count = 0;\n\n\tif (pp->flags & NV_ADMA_PORT_REGISTER_MODE)\n\t\treturn;\n\n\tstatus = readw(mmio + NV_ADMA_STAT);\n\twhile (!(status & NV_ADMA_STAT_IDLE) && count < 20) {\n\t\tndelay(50);\n\t\tstatus = readw(mmio + NV_ADMA_STAT);\n\t\tcount++;\n\t}\n\tif (count == 20)\n\t\tata_port_warn(ap, \"timeout waiting for ADMA IDLE, stat=0x%hx\\n\",\n\t\t\t      status);\n\n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp & ~NV_ADMA_CTL_GO, mmio + NV_ADMA_CTL);\n\n\tcount = 0;\n\tstatus = readw(mmio + NV_ADMA_STAT);\n\twhile (!(status & NV_ADMA_STAT_LEGACY) && count < 20) {\n\t\tndelay(50);\n\t\tstatus = readw(mmio + NV_ADMA_STAT);\n\t\tcount++;\n\t}\n\tif (count == 20)\n\t\tata_port_warn(ap,\n\t\t\t      \"timeout waiting for ADMA LEGACY, stat=0x%hx\\n\",\n\t\t\t      status);\n\n\tpp->flags |= NV_ADMA_PORT_REGISTER_MODE;\n}\n\nstatic void nv_adma_mode(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu16 tmp, status;\n\tint count = 0;\n\n\tif (!(pp->flags & NV_ADMA_PORT_REGISTER_MODE))\n\t\treturn;\n\n\tWARN_ON(pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE);\n\n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp | NV_ADMA_CTL_GO, mmio + NV_ADMA_CTL);\n\n\tstatus = readw(mmio + NV_ADMA_STAT);\n\twhile (((status & NV_ADMA_STAT_LEGACY) ||\n\t      !(status & NV_ADMA_STAT_IDLE)) && count < 20) {\n\t\tndelay(50);\n\t\tstatus = readw(mmio + NV_ADMA_STAT);\n\t\tcount++;\n\t}\n\tif (count == 20)\n\t\tata_port_warn(ap,\n\t\t\t\"timeout waiting for ADMA LEGACY clear and IDLE, stat=0x%hx\\n\",\n\t\t\tstatus);\n\n\tpp->flags &= ~NV_ADMA_PORT_REGISTER_MODE;\n}\n\nstatic int nv_adma_slave_config(struct scsi_device *sdev)\n{\n\tstruct ata_port *ap = ata_shost_to_port(sdev->host);\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tstruct nv_adma_port_priv *port0, *port1;\n\tstruct pci_dev *pdev = to_pci_dev(ap->host->dev);\n\tunsigned long segment_boundary, flags;\n\tunsigned short sg_tablesize;\n\tint rc;\n\tint adma_enable;\n\tu32 current_reg, new_reg, config_mask;\n\n\trc = ata_scsi_slave_config(sdev);\n\n\tif (sdev->id >= ATA_MAX_DEVICES || sdev->channel || sdev->lun)\n\t\t \n\t\treturn rc;\n\n\tspin_lock_irqsave(ap->lock, flags);\n\n\tif (ap->link.device[sdev->id].class == ATA_DEV_ATAPI) {\n\t\t \n\t\tsegment_boundary = ATA_DMA_BOUNDARY;\n\t\t \n\t\tsg_tablesize = LIBATA_MAX_PRD - 1;\n\n\t\t \n\t\tadma_enable = 0;\n\t\tnv_adma_register_mode(ap);\n\t} else {\n\t\tsegment_boundary = NV_ADMA_DMA_BOUNDARY;\n\t\tsg_tablesize = NV_ADMA_SGTBL_TOTAL_LEN;\n\t\tadma_enable = 1;\n\t}\n\n\tpci_read_config_dword(pdev, NV_MCP_SATA_CFG_20, &current_reg);\n\n\tif (ap->port_no == 1)\n\t\tconfig_mask = NV_MCP_SATA_CFG_20_PORT1_EN |\n\t\t\t      NV_MCP_SATA_CFG_20_PORT1_PWB_EN;\n\telse\n\t\tconfig_mask = NV_MCP_SATA_CFG_20_PORT0_EN |\n\t\t\t      NV_MCP_SATA_CFG_20_PORT0_PWB_EN;\n\n\tif (adma_enable) {\n\t\tnew_reg = current_reg | config_mask;\n\t\tpp->flags &= ~NV_ADMA_ATAPI_SETUP_COMPLETE;\n\t} else {\n\t\tnew_reg = current_reg & ~config_mask;\n\t\tpp->flags |= NV_ADMA_ATAPI_SETUP_COMPLETE;\n\t}\n\n\tif (current_reg != new_reg)\n\t\tpci_write_config_dword(pdev, NV_MCP_SATA_CFG_20, new_reg);\n\n\tport0 = ap->host->ports[0]->private_data;\n\tport1 = ap->host->ports[1]->private_data;\n\tif ((port0->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) ||\n\t    (port1->flags & NV_ADMA_ATAPI_SETUP_COMPLETE)) {\n\t\t \n\t\trc = dma_set_mask(&pdev->dev, ATA_DMA_MASK);\n\t} else {\n\t\trc = dma_set_mask(&pdev->dev, pp->adma_dma_mask);\n\t}\n\n\tblk_queue_segment_boundary(sdev->request_queue, segment_boundary);\n\tblk_queue_max_segments(sdev->request_queue, sg_tablesize);\n\tata_port_info(ap,\n\t\t      \"DMA mask 0x%llX, segment boundary 0x%lX, hw segs %hu\\n\",\n\t\t      (unsigned long long)*ap->host->dev->dma_mask,\n\t\t      segment_boundary, sg_tablesize);\n\n\tspin_unlock_irqrestore(ap->lock, flags);\n\n\treturn rc;\n}\n\nstatic int nv_adma_check_atapi_dma(struct ata_queued_cmd *qc)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\treturn !(pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE);\n}\n\nstatic void nv_adma_tf_read(struct ata_port *ap, struct ata_taskfile *tf)\n{\n\t \n\tnv_adma_register_mode(ap);\n\n\tata_sff_tf_read(ap, tf);\n}\n\nstatic unsigned int nv_adma_tf_to_cpb(struct ata_taskfile *tf, __le16 *cpb)\n{\n\tunsigned int idx = 0;\n\n\tif (tf->flags & ATA_TFLAG_ISADDR) {\n\t\tif (tf->flags & ATA_TFLAG_LBA48) {\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_ERR   << 8) | tf->hob_feature | WNB);\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_NSECT << 8) | tf->hob_nsect);\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAL  << 8) | tf->hob_lbal);\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAM  << 8) | tf->hob_lbam);\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAH  << 8) | tf->hob_lbah);\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_ERR    << 8) | tf->feature);\n\t\t} else\n\t\t\tcpb[idx++] = cpu_to_le16((ATA_REG_ERR    << 8) | tf->feature | WNB);\n\n\t\tcpb[idx++] = cpu_to_le16((ATA_REG_NSECT  << 8) | tf->nsect);\n\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAL   << 8) | tf->lbal);\n\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAM   << 8) | tf->lbam);\n\t\tcpb[idx++] = cpu_to_le16((ATA_REG_LBAH   << 8) | tf->lbah);\n\t}\n\n\tif (tf->flags & ATA_TFLAG_DEVICE)\n\t\tcpb[idx++] = cpu_to_le16((ATA_REG_DEVICE << 8) | tf->device);\n\n\tcpb[idx++] = cpu_to_le16((ATA_REG_CMD    << 8) | tf->command | CMDEND);\n\n\twhile (idx < 12)\n\t\tcpb[idx++] = cpu_to_le16(IGN);\n\n\treturn idx;\n}\n\nstatic int nv_adma_check_cpb(struct ata_port *ap, int cpb_num, int force_err)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tu8 flags = pp->cpb[cpb_num].resp_flags;\n\n\tata_port_dbg(ap, \"CPB %d, flags=0x%x\\n\", cpb_num, flags);\n\n\tif (unlikely((force_err ||\n\t\t     flags & (NV_CPB_RESP_ATA_ERR |\n\t\t\t      NV_CPB_RESP_CMD_ERR |\n\t\t\t      NV_CPB_RESP_CPB_ERR)))) {\n\t\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\t\tint freeze = 0;\n\n\t\tata_ehi_clear_desc(ehi);\n\t\t__ata_ehi_push_desc(ehi, \"CPB resp_flags 0x%x: \", flags);\n\t\tif (flags & NV_CPB_RESP_ATA_ERR) {\n\t\t\tata_ehi_push_desc(ehi, \"ATA error\");\n\t\t\tehi->err_mask |= AC_ERR_DEV;\n\t\t} else if (flags & NV_CPB_RESP_CMD_ERR) {\n\t\t\tata_ehi_push_desc(ehi, \"CMD error\");\n\t\t\tehi->err_mask |= AC_ERR_DEV;\n\t\t} else if (flags & NV_CPB_RESP_CPB_ERR) {\n\t\t\tata_ehi_push_desc(ehi, \"CPB error\");\n\t\t\tehi->err_mask |= AC_ERR_SYSTEM;\n\t\t\tfreeze = 1;\n\t\t} else {\n\t\t\t \n\t\t\tata_ehi_push_desc(ehi, \"unknown\");\n\t\t\tehi->err_mask |= AC_ERR_OTHER;\n\t\t\tfreeze = 1;\n\t\t}\n\t\t \n\t\tif (freeze)\n\t\t\tata_port_freeze(ap);\n\t\telse\n\t\t\tata_port_abort(ap);\n\t\treturn -1;\n\t}\n\n\tif (likely(flags & NV_CPB_RESP_DONE))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int nv_host_intr(struct ata_port *ap, u8 irq_stat)\n{\n\tstruct ata_queued_cmd *qc = ata_qc_from_tag(ap, ap->link.active_tag);\n\n\t \n\tif (unlikely(irq_stat & (NV_INT_ADDED | NV_INT_REMOVED))) {\n\t\tata_port_freeze(ap);\n\t\treturn 1;\n\t}\n\n\t \n\tif (!(irq_stat & NV_INT_DEV))\n\t\treturn 0;\n\n\t \n\tif (unlikely(!qc || (qc->tf.flags & ATA_TFLAG_POLLING))) {\n\t\tata_sff_check_status(ap);\n\t\treturn 1;\n\t}\n\n\t \n\treturn ata_bmdma_port_intr(ap, qc);\n}\n\nstatic irqreturn_t nv_adma_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tint i, handled = 0;\n\tu32 notifier_clears[2];\n\n\tspin_lock(&host->lock);\n\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\t\tstruct nv_adma_port_priv *pp = ap->private_data;\n\t\tvoid __iomem *mmio = pp->ctl_block;\n\t\tu16 status;\n\t\tu32 gen_ctl;\n\t\tu32 notifier, notifier_error;\n\n\t\tnotifier_clears[i] = 0;\n\n\t\t \n\t\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) {\n\t\t\tu8 irq_stat = readb(host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_CK804)\n\t\t\t\t>> (NV_INT_PORT_SHIFT * i);\n\t\t\thandled += nv_host_intr(ap, irq_stat);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (pp->flags & NV_ADMA_PORT_REGISTER_MODE) {\n\t\t\tu8 irq_stat = readb(host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_CK804)\n\t\t\t\t>> (NV_INT_PORT_SHIFT * i);\n\t\t\tif (ata_tag_valid(ap->link.active_tag))\n\t\t\t\t \n\t\t\t\tirq_stat |= NV_INT_DEV;\n\t\t\thandled += nv_host_intr(ap, irq_stat);\n\t\t}\n\n\t\tnotifier = readl(mmio + NV_ADMA_NOTIFIER);\n\t\tnotifier_error = readl(mmio + NV_ADMA_NOTIFIER_ERROR);\n\t\tnotifier_clears[i] = notifier | notifier_error;\n\n\t\tgen_ctl = readl(pp->gen_block + NV_ADMA_GEN_CTL);\n\n\t\tif (!NV_ADMA_CHECK_INTR(gen_ctl, ap->port_no) && !notifier &&\n\t\t    !notifier_error)\n\t\t\t \n\t\t\tcontinue;\n\n\t\tstatus = readw(mmio + NV_ADMA_STAT);\n\n\t\t \n\t\twritew(status, mmio + NV_ADMA_STAT);\n\t\treadw(mmio + NV_ADMA_STAT);  \n\t\trmb();\n\n\t\thandled++;  \n\n\t\t \n\t\tif (unlikely(status & (NV_ADMA_STAT_HOTPLUG |\n\t\t\t\t       NV_ADMA_STAT_HOTUNPLUG |\n\t\t\t\t       NV_ADMA_STAT_TIMEOUT |\n\t\t\t\t       NV_ADMA_STAT_SERROR))) {\n\t\t\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\n\t\t\tata_ehi_clear_desc(ehi);\n\t\t\t__ata_ehi_push_desc(ehi, \"ADMA status 0x%08x: \", status);\n\t\t\tif (status & NV_ADMA_STAT_TIMEOUT) {\n\t\t\t\tehi->err_mask |= AC_ERR_SYSTEM;\n\t\t\t\tata_ehi_push_desc(ehi, \"timeout\");\n\t\t\t} else if (status & NV_ADMA_STAT_HOTPLUG) {\n\t\t\t\tata_ehi_hotplugged(ehi);\n\t\t\t\tata_ehi_push_desc(ehi, \"hotplug\");\n\t\t\t} else if (status & NV_ADMA_STAT_HOTUNPLUG) {\n\t\t\t\tata_ehi_hotplugged(ehi);\n\t\t\t\tata_ehi_push_desc(ehi, \"hot unplug\");\n\t\t\t} else if (status & NV_ADMA_STAT_SERROR) {\n\t\t\t\t \n\t\t\t\tata_ehi_push_desc(ehi, \"SError\");\n\t\t\t} else\n\t\t\t\tata_ehi_push_desc(ehi, \"unknown\");\n\t\t\tata_port_freeze(ap);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (status & (NV_ADMA_STAT_DONE |\n\t\t\t      NV_ADMA_STAT_CPBERR |\n\t\t\t      NV_ADMA_STAT_CMD_COMPLETE)) {\n\t\t\tu32 check_commands = notifier_clears[i];\n\t\t\tu32 done_mask = 0;\n\t\t\tint pos, rc;\n\n\t\t\tif (status & NV_ADMA_STAT_CPBERR) {\n\t\t\t\t \n\t\t\t\tif (ata_tag_valid(ap->link.active_tag))\n\t\t\t\t\tcheck_commands = 1 <<\n\t\t\t\t\t\tap->link.active_tag;\n\t\t\t\telse\n\t\t\t\t\tcheck_commands = ap->link.sactive;\n\t\t\t}\n\n\t\t\t \n\t\t\twhile ((pos = ffs(check_commands))) {\n\t\t\t\tpos--;\n\t\t\t\trc = nv_adma_check_cpb(ap, pos,\n\t\t\t\t\t\tnotifier_error & (1 << pos));\n\t\t\t\tif (rc > 0)\n\t\t\t\t\tdone_mask |= 1 << pos;\n\t\t\t\telse if (unlikely(rc < 0))\n\t\t\t\t\tcheck_commands = 0;\n\t\t\t\tcheck_commands &= ~(1 << pos);\n\t\t\t}\n\t\t\tata_qc_complete_multiple(ap, ata_qc_get_active(ap) ^ done_mask);\n\t\t}\n\t}\n\n\tif (notifier_clears[0] || notifier_clears[1]) {\n\t\t \n\t\tstruct nv_adma_port_priv *pp = host->ports[0]->private_data;\n\t\twritel(notifier_clears[0], pp->notifier_clear_block);\n\t\tpp = host->ports[1]->private_data;\n\t\twritel(notifier_clears[1], pp->notifier_clear_block);\n\t}\n\n\tspin_unlock(&host->lock);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic void nv_adma_freeze(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu16 tmp;\n\n\tnv_ck804_freeze(ap);\n\n\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE)\n\t\treturn;\n\n\t \n\twriteb(NV_INT_ALL << (ap->port_no * NV_INT_PORT_SHIFT),\n\t\tap->host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_CK804);\n\n\t \n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp & ~(NV_ADMA_CTL_AIEN | NV_ADMA_CTL_HOTPLUG_IEN),\n\t\tmmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n}\n\nstatic void nv_adma_thaw(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu16 tmp;\n\n\tnv_ck804_thaw(ap);\n\n\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE)\n\t\treturn;\n\n\t \n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp | (NV_ADMA_CTL_AIEN | NV_ADMA_CTL_HOTPLUG_IEN),\n\t\tmmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n}\n\nstatic void nv_adma_irq_clear(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu32 notifier_clears[2];\n\n\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) {\n\t\tata_bmdma_irq_clear(ap);\n\t\treturn;\n\t}\n\n\t \n\twriteb(NV_INT_ALL << (ap->port_no * NV_INT_PORT_SHIFT),\n\t\tap->host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_CK804);\n\n\t \n\twritew(0xffff, mmio + NV_ADMA_STAT);\n\n\t \n\tif (ap->port_no == 0) {\n\t\tnotifier_clears[0] = 0xFFFFFFFF;\n\t\tnotifier_clears[1] = 0;\n\t} else {\n\t\tnotifier_clears[0] = 0;\n\t\tnotifier_clears[1] = 0xFFFFFFFF;\n\t}\n\tpp = ap->host->ports[0]->private_data;\n\twritel(notifier_clears[0], pp->notifier_clear_block);\n\tpp = ap->host->ports[1]->private_data;\n\twritel(notifier_clears[1], pp->notifier_clear_block);\n}\n\nstatic void nv_adma_post_internal_cmd(struct ata_queued_cmd *qc)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\n\tif (pp->flags & NV_ADMA_PORT_REGISTER_MODE)\n\t\tata_bmdma_post_internal_cmd(qc);\n}\n\nstatic int nv_adma_port_start(struct ata_port *ap)\n{\n\tstruct device *dev = ap->host->dev;\n\tstruct nv_adma_port_priv *pp;\n\tint rc;\n\tvoid *mem;\n\tdma_addr_t mem_dma;\n\tvoid __iomem *mmio;\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tu16 tmp;\n\n\t \n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = ata_bmdma_port_start(ap);\n\tif (rc)\n\t\treturn rc;\n\n\tpp = devm_kzalloc(dev, sizeof(*pp), GFP_KERNEL);\n\tif (!pp)\n\t\treturn -ENOMEM;\n\n\tmmio = ap->host->iomap[NV_MMIO_BAR] + NV_ADMA_PORT +\n\t       ap->port_no * NV_ADMA_PORT_SIZE;\n\tpp->ctl_block = mmio;\n\tpp->gen_block = ap->host->iomap[NV_MMIO_BAR] + NV_ADMA_GEN;\n\tpp->notifier_clear_block = pp->gen_block +\n\t       NV_ADMA_NOTIFIER_CLEAR + (4 * ap->port_no);\n\n\t \n\tdma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\n\tpp->adma_dma_mask = *dev->dma_mask;\n\n\tmem = dmam_alloc_coherent(dev, NV_ADMA_PORT_PRIV_DMA_SZ,\n\t\t\t\t  &mem_dma, GFP_KERNEL);\n\tif (!mem)\n\t\treturn -ENOMEM;\n\n\t \n\tpp->cpb     = mem;\n\tpp->cpb_dma = mem_dma;\n\n\twritel(mem_dma & 0xFFFFFFFF, \tmmio + NV_ADMA_CPB_BASE_LOW);\n\twritel((mem_dma >> 16) >> 16,\tmmio + NV_ADMA_CPB_BASE_HIGH);\n\n\tmem     += NV_ADMA_MAX_CPBS * NV_ADMA_CPB_SZ;\n\tmem_dma += NV_ADMA_MAX_CPBS * NV_ADMA_CPB_SZ;\n\n\t \n\tpp->aprd = mem;\n\tpp->aprd_dma = mem_dma;\n\n\tap->private_data = pp;\n\n\t \n\twritew(0xffff, mmio + NV_ADMA_STAT);\n\n\t \n\tpp->flags = NV_ADMA_PORT_REGISTER_MODE;\n\n\t \n\twritew(0, mmio + NV_ADMA_CPB_COUNT);\n\n\t \n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew((tmp & ~NV_ADMA_CTL_GO) | NV_ADMA_CTL_AIEN |\n\t\tNV_ADMA_CTL_HOTPLUG_IEN, mmio + NV_ADMA_CTL);\n\n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp | NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n\tudelay(1);\n\twritew(tmp & ~NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n\n\treturn 0;\n}\n\nstatic void nv_adma_port_stop(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\n\twritew(0, mmio + NV_ADMA_CTL);\n}\n\n#ifdef CONFIG_PM\nstatic int nv_adma_port_suspend(struct ata_port *ap, pm_message_t mesg)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\n\t \n\tnv_adma_register_mode(ap);\n\n\t \n\twritew(0, mmio + NV_ADMA_CPB_COUNT);\n\n\t \n\twritew(0, mmio + NV_ADMA_CTL);\n\n\treturn 0;\n}\n\nstatic int nv_adma_port_resume(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tu16 tmp;\n\n\t \n\twritel(pp->cpb_dma & 0xFFFFFFFF, \tmmio + NV_ADMA_CPB_BASE_LOW);\n\twritel((pp->cpb_dma >> 16) >> 16,\tmmio + NV_ADMA_CPB_BASE_HIGH);\n\n\t \n\twritew(0xffff, mmio + NV_ADMA_STAT);\n\n\t \n\tpp->flags |= NV_ADMA_PORT_REGISTER_MODE;\n\n\t \n\twritew(0, mmio + NV_ADMA_CPB_COUNT);\n\n\t \n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew((tmp & ~NV_ADMA_CTL_GO) | NV_ADMA_CTL_AIEN |\n\t\tNV_ADMA_CTL_HOTPLUG_IEN, mmio + NV_ADMA_CTL);\n\n\ttmp = readw(mmio + NV_ADMA_CTL);\n\twritew(tmp | NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n\tudelay(1);\n\twritew(tmp & ~NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\treadw(mmio + NV_ADMA_CTL);\t \n\n\treturn 0;\n}\n#endif\n\nstatic void nv_adma_setup_port(struct ata_port *ap)\n{\n\tvoid __iomem *mmio = ap->host->iomap[NV_MMIO_BAR];\n\tstruct ata_ioports *ioport = &ap->ioaddr;\n\n\tmmio += NV_ADMA_PORT + ap->port_no * NV_ADMA_PORT_SIZE;\n\n\tioport->cmd_addr\t= mmio;\n\tioport->data_addr\t= mmio + (ATA_REG_DATA * 4);\n\tioport->error_addr\t=\n\tioport->feature_addr\t= mmio + (ATA_REG_ERR * 4);\n\tioport->nsect_addr\t= mmio + (ATA_REG_NSECT * 4);\n\tioport->lbal_addr\t= mmio + (ATA_REG_LBAL * 4);\n\tioport->lbam_addr\t= mmio + (ATA_REG_LBAM * 4);\n\tioport->lbah_addr\t= mmio + (ATA_REG_LBAH * 4);\n\tioport->device_addr\t= mmio + (ATA_REG_DEVICE * 4);\n\tioport->status_addr\t=\n\tioport->command_addr\t= mmio + (ATA_REG_STATUS * 4);\n\tioport->altstatus_addr\t=\n\tioport->ctl_addr\t= mmio + 0x20;\n}\n\nstatic int nv_adma_host_init(struct ata_host *host)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tunsigned int i;\n\tu32 tmp32;\n\n\t \n\tpci_read_config_dword(pdev, NV_MCP_SATA_CFG_20, &tmp32);\n\ttmp32 |= NV_MCP_SATA_CFG_20_PORT0_EN |\n\t\t NV_MCP_SATA_CFG_20_PORT0_PWB_EN |\n\t\t NV_MCP_SATA_CFG_20_PORT1_EN |\n\t\t NV_MCP_SATA_CFG_20_PORT1_PWB_EN;\n\n\tpci_write_config_dword(pdev, NV_MCP_SATA_CFG_20, tmp32);\n\n\tfor (i = 0; i < host->n_ports; i++)\n\t\tnv_adma_setup_port(host->ports[i]);\n\n\treturn 0;\n}\n\nstatic void nv_adma_fill_aprd(struct ata_queued_cmd *qc,\n\t\t\t      struct scatterlist *sg,\n\t\t\t      int idx,\n\t\t\t      struct nv_adma_prd *aprd)\n{\n\tu8 flags = 0;\n\tif (qc->tf.flags & ATA_TFLAG_WRITE)\n\t\tflags |= NV_APRD_WRITE;\n\tif (idx == qc->n_elem - 1)\n\t\tflags |= NV_APRD_END;\n\telse if (idx != 4)\n\t\tflags |= NV_APRD_CONT;\n\n\taprd->addr  = cpu_to_le64(((u64)sg_dma_address(sg)));\n\taprd->len   = cpu_to_le32(((u32)sg_dma_len(sg)));  \n\taprd->flags = flags;\n\taprd->packet_len = 0;\n}\n\nstatic void nv_adma_fill_sg(struct ata_queued_cmd *qc, struct nv_adma_cpb *cpb)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\tstruct nv_adma_prd *aprd;\n\tstruct scatterlist *sg;\n\tunsigned int si;\n\n\tfor_each_sg(qc->sg, sg, qc->n_elem, si) {\n\t\taprd = (si < 5) ? &cpb->aprd[si] :\n\t\t\t&pp->aprd[NV_ADMA_SGTBL_LEN * qc->hw_tag + (si-5)];\n\t\tnv_adma_fill_aprd(qc, sg, si, aprd);\n\t}\n\tif (si > 5)\n\t\tcpb->next_aprd = cpu_to_le64(((u64)(pp->aprd_dma + NV_ADMA_SGTBL_SZ * qc->hw_tag)));\n\telse\n\t\tcpb->next_aprd = cpu_to_le64(0);\n}\n\nstatic int nv_adma_use_reg_mode(struct ata_queued_cmd *qc)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\n\t \n\tif ((pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) ||\n\t   (qc->tf.flags & ATA_TFLAG_POLLING))\n\t\treturn 1;\n\n\tif ((qc->flags & ATA_QCFLAG_DMAMAP) ||\n\t   (qc->tf.protocol == ATA_PROT_NODATA))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic enum ata_completion_errors nv_adma_qc_prep(struct ata_queued_cmd *qc)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\tstruct nv_adma_cpb *cpb = &pp->cpb[qc->hw_tag];\n\tu8 ctl_flags = NV_CPB_CTL_CPB_VALID |\n\t\t       NV_CPB_CTL_IEN;\n\n\tif (nv_adma_use_reg_mode(qc)) {\n\t\tBUG_ON(!(pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) &&\n\t\t\t(qc->flags & ATA_QCFLAG_DMAMAP));\n\t\tnv_adma_register_mode(qc->ap);\n\t\tata_bmdma_qc_prep(qc);\n\t\treturn AC_ERR_OK;\n\t}\n\n\tcpb->resp_flags = NV_CPB_RESP_DONE;\n\twmb();\n\tcpb->ctl_flags = 0;\n\twmb();\n\n\tcpb->len\t\t= 3;\n\tcpb->tag\t\t= qc->hw_tag;\n\tcpb->next_cpb_idx\t= 0;\n\n\t \n\tif (qc->tf.protocol == ATA_PROT_NCQ)\n\t\tctl_flags |= NV_CPB_CTL_QUEUE | NV_CPB_CTL_FPDMA;\n\n\tnv_adma_tf_to_cpb(&qc->tf, cpb->tf);\n\n\tif (qc->flags & ATA_QCFLAG_DMAMAP) {\n\t\tnv_adma_fill_sg(qc, cpb);\n\t\tctl_flags |= NV_CPB_CTL_APRD_VALID;\n\t} else\n\t\tmemset(&cpb->aprd[0], 0, sizeof(struct nv_adma_prd) * 5);\n\n\t \n\twmb();\n\tcpb->ctl_flags = ctl_flags;\n\twmb();\n\tcpb->resp_flags = 0;\n\n\treturn AC_ERR_OK;\n}\n\nstatic unsigned int nv_adma_qc_issue(struct ata_queued_cmd *qc)\n{\n\tstruct nv_adma_port_priv *pp = qc->ap->private_data;\n\tvoid __iomem *mmio = pp->ctl_block;\n\tint curr_ncq = (qc->tf.protocol == ATA_PROT_NCQ);\n\n\t \n\tif (unlikely(qc->tf.protocol == ATA_PROT_NCQ &&\n\t\t     (qc->flags & ATA_QCFLAG_RESULT_TF))) {\n\t\tata_dev_err(qc->dev, \"NCQ w/ RESULT_TF not allowed\\n\");\n\t\treturn AC_ERR_SYSTEM;\n\t}\n\n\tif (nv_adma_use_reg_mode(qc)) {\n\t\t \n\t\tBUG_ON(!(pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE) &&\n\t\t\t(qc->flags & ATA_QCFLAG_DMAMAP));\n\t\tnv_adma_register_mode(qc->ap);\n\t\treturn ata_bmdma_qc_issue(qc);\n\t} else\n\t\tnv_adma_mode(qc->ap);\n\n\t \n\twmb();\n\n\tif (curr_ncq != pp->last_issue_ncq) {\n\t\t \n\t\tudelay(20);\n\t\tpp->last_issue_ncq = curr_ncq;\n\t}\n\n\twritew(qc->hw_tag, mmio + NV_ADMA_APPEND);\n\n\treturn 0;\n}\n\nstatic irqreturn_t nv_generic_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tunsigned int i;\n\tunsigned int handled = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&host->lock, flags);\n\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\t\tstruct ata_queued_cmd *qc;\n\n\t\tqc = ata_qc_from_tag(ap, ap->link.active_tag);\n\t\tif (qc && (!(qc->tf.flags & ATA_TFLAG_POLLING))) {\n\t\t\thandled += ata_bmdma_port_intr(ap, qc);\n\t\t} else {\n\t\t\t \n\t\t\tap->ops->sff_check_status(ap);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&host->lock, flags);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic irqreturn_t nv_do_interrupt(struct ata_host *host, u8 irq_stat)\n{\n\tint i, handled = 0;\n\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\thandled += nv_host_intr(host->ports[i], irq_stat);\n\t\tirq_stat >>= NV_INT_PORT_SHIFT;\n\t}\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic irqreturn_t nv_nf2_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tu8 irq_stat;\n\tirqreturn_t ret;\n\n\tspin_lock(&host->lock);\n\tirq_stat = ioread8(host->ports[0]->ioaddr.scr_addr + NV_INT_STATUS);\n\tret = nv_do_interrupt(host, irq_stat);\n\tspin_unlock(&host->lock);\n\n\treturn ret;\n}\n\nstatic irqreturn_t nv_ck804_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tu8 irq_stat;\n\tirqreturn_t ret;\n\n\tspin_lock(&host->lock);\n\tirq_stat = readb(host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_CK804);\n\tret = nv_do_interrupt(host, irq_stat);\n\tspin_unlock(&host->lock);\n\n\treturn ret;\n}\n\nstatic int nv_scr_read(struct ata_link *link, unsigned int sc_reg, u32 *val)\n{\n\tif (sc_reg > SCR_CONTROL)\n\t\treturn -EINVAL;\n\n\t*val = ioread32(link->ap->ioaddr.scr_addr + (sc_reg * 4));\n\treturn 0;\n}\n\nstatic int nv_scr_write(struct ata_link *link, unsigned int sc_reg, u32 val)\n{\n\tif (sc_reg > SCR_CONTROL)\n\t\treturn -EINVAL;\n\n\tiowrite32(val, link->ap->ioaddr.scr_addr + (sc_reg * 4));\n\treturn 0;\n}\n\nstatic int nv_hardreset(struct ata_link *link, unsigned int *class,\n\t\t\tunsigned long deadline)\n{\n\tstruct ata_eh_context *ehc = &link->eh_context;\n\n\t \n\tif (!(link->ap->pflags & ATA_PFLAG_LOADING) &&\n\t    !ata_dev_enabled(link->device))\n\t\tsata_link_hardreset(link, sata_deb_timing_hotplug, deadline,\n\t\t\t\t    NULL, NULL);\n\telse {\n\t\tconst unsigned int *timing = sata_ehc_deb_timing(ehc);\n\t\tint rc;\n\n\t\tif (!(ehc->i.flags & ATA_EHI_QUIET))\n\t\t\tata_link_info(link,\n\t\t\t\t      \"nv: skipping hardreset on occupied port\\n\");\n\n\t\t \n\t\trc = sata_link_resume(link, timing, deadline);\n\t\t \n\t\tif (rc && rc != -EOPNOTSUPP)\n\t\t\tata_link_warn(link, \"failed to resume link (errno=%d)\\n\",\n\t\t\t\t      rc);\n\t}\n\n\t \n\treturn -EAGAIN;\n}\n\nstatic void nv_nf2_freeze(struct ata_port *ap)\n{\n\tvoid __iomem *scr_addr = ap->host->ports[0]->ioaddr.scr_addr;\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT;\n\tu8 mask;\n\n\tmask = ioread8(scr_addr + NV_INT_ENABLE);\n\tmask &= ~(NV_INT_ALL << shift);\n\tiowrite8(mask, scr_addr + NV_INT_ENABLE);\n}\n\nstatic void nv_nf2_thaw(struct ata_port *ap)\n{\n\tvoid __iomem *scr_addr = ap->host->ports[0]->ioaddr.scr_addr;\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT;\n\tu8 mask;\n\n\tiowrite8(NV_INT_ALL << shift, scr_addr + NV_INT_STATUS);\n\n\tmask = ioread8(scr_addr + NV_INT_ENABLE);\n\tmask |= (NV_INT_MASK << shift);\n\tiowrite8(mask, scr_addr + NV_INT_ENABLE);\n}\n\nstatic void nv_ck804_freeze(struct ata_port *ap)\n{\n\tvoid __iomem *mmio_base = ap->host->iomap[NV_MMIO_BAR];\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT;\n\tu8 mask;\n\n\tmask = readb(mmio_base + NV_INT_ENABLE_CK804);\n\tmask &= ~(NV_INT_ALL << shift);\n\twriteb(mask, mmio_base + NV_INT_ENABLE_CK804);\n}\n\nstatic void nv_ck804_thaw(struct ata_port *ap)\n{\n\tvoid __iomem *mmio_base = ap->host->iomap[NV_MMIO_BAR];\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT;\n\tu8 mask;\n\n\twriteb(NV_INT_ALL << shift, mmio_base + NV_INT_STATUS_CK804);\n\n\tmask = readb(mmio_base + NV_INT_ENABLE_CK804);\n\tmask |= (NV_INT_MASK << shift);\n\twriteb(mask, mmio_base + NV_INT_ENABLE_CK804);\n}\n\nstatic void nv_mcp55_freeze(struct ata_port *ap)\n{\n\tvoid __iomem *mmio_base = ap->host->iomap[NV_MMIO_BAR];\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT_MCP55;\n\tu32 mask;\n\n\twritel(NV_INT_ALL_MCP55 << shift, mmio_base + NV_INT_STATUS_MCP55);\n\n\tmask = readl(mmio_base + NV_INT_ENABLE_MCP55);\n\tmask &= ~(NV_INT_ALL_MCP55 << shift);\n\twritel(mask, mmio_base + NV_INT_ENABLE_MCP55);\n}\n\nstatic void nv_mcp55_thaw(struct ata_port *ap)\n{\n\tvoid __iomem *mmio_base = ap->host->iomap[NV_MMIO_BAR];\n\tint shift = ap->port_no * NV_INT_PORT_SHIFT_MCP55;\n\tu32 mask;\n\n\twritel(NV_INT_ALL_MCP55 << shift, mmio_base + NV_INT_STATUS_MCP55);\n\n\tmask = readl(mmio_base + NV_INT_ENABLE_MCP55);\n\tmask |= (NV_INT_MASK_MCP55 << shift);\n\twritel(mask, mmio_base + NV_INT_ENABLE_MCP55);\n}\n\nstatic void nv_adma_error_handler(struct ata_port *ap)\n{\n\tstruct nv_adma_port_priv *pp = ap->private_data;\n\tif (!(pp->flags & NV_ADMA_PORT_REGISTER_MODE)) {\n\t\tvoid __iomem *mmio = pp->ctl_block;\n\t\tint i;\n\t\tu16 tmp;\n\n\t\tif (ata_tag_valid(ap->link.active_tag) || ap->link.sactive) {\n\t\t\tu32 notifier = readl(mmio + NV_ADMA_NOTIFIER);\n\t\t\tu32 notifier_error = readl(mmio + NV_ADMA_NOTIFIER_ERROR);\n\t\t\tu32 gen_ctl = readl(pp->gen_block + NV_ADMA_GEN_CTL);\n\t\t\tu32 status = readw(mmio + NV_ADMA_STAT);\n\t\t\tu8 cpb_count = readb(mmio + NV_ADMA_CPB_COUNT);\n\t\t\tu8 next_cpb_idx = readb(mmio + NV_ADMA_NEXT_CPB_IDX);\n\n\t\t\tata_port_err(ap,\n\t\t\t\t\"EH in ADMA mode, notifier 0x%X \"\n\t\t\t\t\"notifier_error 0x%X gen_ctl 0x%X status 0x%X \"\n\t\t\t\t\"next cpb count 0x%X next cpb idx 0x%x\\n\",\n\t\t\t\tnotifier, notifier_error, gen_ctl, status,\n\t\t\t\tcpb_count, next_cpb_idx);\n\n\t\t\tfor (i = 0; i < NV_ADMA_MAX_CPBS; i++) {\n\t\t\t\tstruct nv_adma_cpb *cpb = &pp->cpb[i];\n\t\t\t\tif ((ata_tag_valid(ap->link.active_tag) && i == ap->link.active_tag) ||\n\t\t\t\t    ap->link.sactive & (1 << i))\n\t\t\t\t\tata_port_err(ap,\n\t\t\t\t\t\t\"CPB %d: ctl_flags 0x%x, resp_flags 0x%x\\n\",\n\t\t\t\t\t\ti, cpb->ctl_flags, cpb->resp_flags);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tnv_adma_register_mode(ap);\n\n\t\t \n\t\tfor (i = 0; i < NV_ADMA_MAX_CPBS; i++)\n\t\t\tpp->cpb[i].ctl_flags &= ~NV_CPB_CTL_CPB_VALID;\n\n\t\t \n\t\twritew(0, mmio + NV_ADMA_CPB_COUNT);\n\n\t\t \n\t\ttmp = readw(mmio + NV_ADMA_CTL);\n\t\twritew(tmp | NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\t\treadw(mmio + NV_ADMA_CTL);\t \n\t\tudelay(1);\n\t\twritew(tmp & ~NV_ADMA_CTL_CHANNEL_RESET, mmio + NV_ADMA_CTL);\n\t\treadw(mmio + NV_ADMA_CTL);\t \n\t}\n\n\tata_bmdma_error_handler(ap);\n}\n\nstatic void nv_swncq_qc_to_dq(struct ata_port *ap, struct ata_queued_cmd *qc)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct defer_queue *dq = &pp->defer_queue;\n\n\t \n\tWARN_ON(dq->tail - dq->head == ATA_MAX_QUEUE);\n\tdq->defer_bits |= (1 << qc->hw_tag);\n\tdq->tag[dq->tail++ & (ATA_MAX_QUEUE - 1)] = qc->hw_tag;\n}\n\nstatic struct ata_queued_cmd *nv_swncq_qc_from_dq(struct ata_port *ap)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct defer_queue *dq = &pp->defer_queue;\n\tunsigned int tag;\n\n\tif (dq->head == dq->tail)\t \n\t\treturn NULL;\n\n\ttag = dq->tag[dq->head & (ATA_MAX_QUEUE - 1)];\n\tdq->tag[dq->head++ & (ATA_MAX_QUEUE - 1)] = ATA_TAG_POISON;\n\tWARN_ON(!(dq->defer_bits & (1 << tag)));\n\tdq->defer_bits &= ~(1 << tag);\n\n\treturn ata_qc_from_tag(ap, tag);\n}\n\nstatic void nv_swncq_fis_reinit(struct ata_port *ap)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\n\tpp->dhfis_bits = 0;\n\tpp->dmafis_bits = 0;\n\tpp->sdbfis_bits = 0;\n\tpp->ncq_flags = 0;\n}\n\nstatic void nv_swncq_pp_reinit(struct ata_port *ap)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct defer_queue *dq = &pp->defer_queue;\n\n\tdq->head = 0;\n\tdq->tail = 0;\n\tdq->defer_bits = 0;\n\tpp->qc_active = 0;\n\tpp->last_issue_tag = ATA_TAG_POISON;\n\tnv_swncq_fis_reinit(ap);\n}\n\nstatic void nv_swncq_irq_clear(struct ata_port *ap, u16 fis)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\n\twritew(fis, pp->irq_block);\n}\n\nstatic void __ata_bmdma_stop(struct ata_port *ap)\n{\n\tstruct ata_queued_cmd qc;\n\n\tqc.ap = ap;\n\tata_bmdma_stop(&qc);\n}\n\nstatic void nv_swncq_ncq_stop(struct ata_port *ap)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tunsigned int i;\n\tu32 sactive;\n\tu32 done_mask;\n\n\tata_port_err(ap, \"EH in SWNCQ mode,QC:qc_active 0x%llX sactive 0x%X\\n\",\n\t\t     ap->qc_active, ap->link.sactive);\n\tata_port_err(ap,\n\t\t\"SWNCQ:qc_active 0x%X defer_bits 0x%X last_issue_tag 0x%x\\n  \"\n\t\t\"dhfis 0x%X dmafis 0x%X sdbfis 0x%X\\n\",\n\t\tpp->qc_active, pp->defer_queue.defer_bits, pp->last_issue_tag,\n\t\tpp->dhfis_bits, pp->dmafis_bits, pp->sdbfis_bits);\n\n\tata_port_err(ap, \"ATA_REG 0x%X ERR_REG 0x%X\\n\",\n\t\t     ap->ops->sff_check_status(ap),\n\t\t     ioread8(ap->ioaddr.error_addr));\n\n\tsactive = readl(pp->sactive_block);\n\tdone_mask = pp->qc_active ^ sactive;\n\n\tata_port_err(ap, \"tag : dhfis dmafis sdbfis sactive\\n\");\n\tfor (i = 0; i < ATA_MAX_QUEUE; i++) {\n\t\tu8 err = 0;\n\t\tif (pp->qc_active & (1 << i))\n\t\t\terr = 0;\n\t\telse if (done_mask & (1 << i))\n\t\t\terr = 1;\n\t\telse\n\t\t\tcontinue;\n\n\t\tata_port_err(ap,\n\t\t\t     \"tag 0x%x: %01x %01x %01x %01x %s\\n\", i,\n\t\t\t     (pp->dhfis_bits >> i) & 0x1,\n\t\t\t     (pp->dmafis_bits >> i) & 0x1,\n\t\t\t     (pp->sdbfis_bits >> i) & 0x1,\n\t\t\t     (sactive >> i) & 0x1,\n\t\t\t     (err ? \"error! tag doesn't exit\" : \" \"));\n\t}\n\n\tnv_swncq_pp_reinit(ap);\n\tap->ops->sff_irq_clear(ap);\n\t__ata_bmdma_stop(ap);\n\tnv_swncq_irq_clear(ap, 0xffff);\n}\n\nstatic void nv_swncq_error_handler(struct ata_port *ap)\n{\n\tstruct ata_eh_context *ehc = &ap->link.eh_context;\n\n\tif (ap->link.sactive) {\n\t\tnv_swncq_ncq_stop(ap);\n\t\tehc->i.action |= ATA_EH_RESET;\n\t}\n\n\tata_bmdma_error_handler(ap);\n}\n\n#ifdef CONFIG_PM\nstatic int nv_swncq_port_suspend(struct ata_port *ap, pm_message_t mesg)\n{\n\tvoid __iomem *mmio = ap->host->iomap[NV_MMIO_BAR];\n\tu32 tmp;\n\n\t \n\twritel(~0, mmio + NV_INT_STATUS_MCP55);\n\n\t \n\twritel(0, mmio + NV_INT_ENABLE_MCP55);\n\n\t \n\ttmp = readl(mmio + NV_CTL_MCP55);\n\ttmp &= ~(NV_CTL_PRI_SWNCQ | NV_CTL_SEC_SWNCQ);\n\twritel(tmp, mmio + NV_CTL_MCP55);\n\n\treturn 0;\n}\n\nstatic int nv_swncq_port_resume(struct ata_port *ap)\n{\n\tvoid __iomem *mmio = ap->host->iomap[NV_MMIO_BAR];\n\tu32 tmp;\n\n\t \n\twritel(~0, mmio + NV_INT_STATUS_MCP55);\n\n\t \n\twritel(0x00fd00fd, mmio + NV_INT_ENABLE_MCP55);\n\n\t \n\ttmp = readl(mmio + NV_CTL_MCP55);\n\twritel(tmp | NV_CTL_PRI_SWNCQ | NV_CTL_SEC_SWNCQ, mmio + NV_CTL_MCP55);\n\n\treturn 0;\n}\n#endif\n\nstatic void nv_swncq_host_init(struct ata_host *host)\n{\n\tu32 tmp;\n\tvoid __iomem *mmio = host->iomap[NV_MMIO_BAR];\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tu8 regval;\n\n\t \n\tpci_read_config_byte(pdev, 0x7f, &regval);\n\tregval &= ~(1 << 7);\n\tpci_write_config_byte(pdev, 0x7f, regval);\n\n\t \n\ttmp = readl(mmio + NV_CTL_MCP55);\n\tdev_dbg(&pdev->dev, \"HOST_CTL:0x%X\\n\", tmp);\n\twritel(tmp | NV_CTL_PRI_SWNCQ | NV_CTL_SEC_SWNCQ, mmio + NV_CTL_MCP55);\n\n\t \n\ttmp = readl(mmio + NV_INT_ENABLE_MCP55);\n\tdev_dbg(&pdev->dev, \"HOST_ENABLE:0x%X\\n\", tmp);\n\twritel(tmp | 0x00fd00fd, mmio + NV_INT_ENABLE_MCP55);\n\n\t \n\twritel(~0x0, mmio + NV_INT_STATUS_MCP55);\n}\n\nstatic int nv_swncq_slave_config(struct scsi_device *sdev)\n{\n\tstruct ata_port *ap = ata_shost_to_port(sdev->host);\n\tstruct pci_dev *pdev = to_pci_dev(ap->host->dev);\n\tstruct ata_device *dev;\n\tint rc;\n\tu8 rev;\n\tu8 check_maxtor = 0;\n\tunsigned char model_num[ATA_ID_PROD_LEN + 1];\n\n\trc = ata_scsi_slave_config(sdev);\n\tif (sdev->id >= ATA_MAX_DEVICES || sdev->channel || sdev->lun)\n\t\t \n\t\treturn rc;\n\n\tdev = &ap->link.device[sdev->id];\n\tif (!(ap->flags & ATA_FLAG_NCQ) || dev->class == ATA_DEV_ATAPI)\n\t\treturn rc;\n\n\t \n\tif (pdev->device == PCI_DEVICE_ID_NVIDIA_NFORCE_MCP51_SATA ||\n\t\tpdev->device == PCI_DEVICE_ID_NVIDIA_NFORCE_MCP51_SATA2)\n\t\tcheck_maxtor = 1;\n\n\t \n\tif (pdev->device == PCI_DEVICE_ID_NVIDIA_NFORCE_MCP55_SATA ||\n\t\tpdev->device == PCI_DEVICE_ID_NVIDIA_NFORCE_MCP55_SATA2) {\n\t\tpci_read_config_byte(pdev, 0x8, &rev);\n\t\tif (rev <= 0xa2)\n\t\t\tcheck_maxtor = 1;\n\t}\n\n\tif (!check_maxtor)\n\t\treturn rc;\n\n\tata_id_c_string(dev->id, model_num, ATA_ID_PROD, sizeof(model_num));\n\n\tif (strncmp(model_num, \"Maxtor\", 6) == 0) {\n\t\tata_scsi_change_queue_depth(sdev, 1);\n\t\tata_dev_notice(dev, \"Disabling SWNCQ mode (depth %x)\\n\",\n\t\t\t       sdev->queue_depth);\n\t}\n\n\treturn rc;\n}\n\nstatic int nv_swncq_port_start(struct ata_port *ap)\n{\n\tstruct device *dev = ap->host->dev;\n\tvoid __iomem *mmio = ap->host->iomap[NV_MMIO_BAR];\n\tstruct nv_swncq_port_priv *pp;\n\tint rc;\n\n\t \n\trc = ata_bmdma_port_start(ap);\n\tif (rc)\n\t\treturn rc;\n\n\tpp = devm_kzalloc(dev, sizeof(*pp), GFP_KERNEL);\n\tif (!pp)\n\t\treturn -ENOMEM;\n\n\tpp->prd = dmam_alloc_coherent(dev, ATA_PRD_TBL_SZ * ATA_MAX_QUEUE,\n\t\t\t\t      &pp->prd_dma, GFP_KERNEL);\n\tif (!pp->prd)\n\t\treturn -ENOMEM;\n\n\tap->private_data = pp;\n\tpp->sactive_block = ap->ioaddr.scr_addr + 4 * SCR_ACTIVE;\n\tpp->irq_block = mmio + NV_INT_STATUS_MCP55 + ap->port_no * 2;\n\tpp->tag_block = mmio + NV_NCQ_REG_MCP55 + ap->port_no * 2;\n\n\treturn 0;\n}\n\nstatic enum ata_completion_errors nv_swncq_qc_prep(struct ata_queued_cmd *qc)\n{\n\tif (qc->tf.protocol != ATA_PROT_NCQ) {\n\t\tata_bmdma_qc_prep(qc);\n\t\treturn AC_ERR_OK;\n\t}\n\n\tif (!(qc->flags & ATA_QCFLAG_DMAMAP))\n\t\treturn AC_ERR_OK;\n\n\tnv_swncq_fill_sg(qc);\n\n\treturn AC_ERR_OK;\n}\n\nstatic void nv_swncq_fill_sg(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct scatterlist *sg;\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct ata_bmdma_prd *prd;\n\tunsigned int si, idx;\n\n\tprd = pp->prd + ATA_MAX_PRD * qc->hw_tag;\n\n\tidx = 0;\n\tfor_each_sg(qc->sg, sg, qc->n_elem, si) {\n\t\tu32 addr, offset;\n\t\tu32 sg_len, len;\n\n\t\taddr = (u32)sg_dma_address(sg);\n\t\tsg_len = sg_dma_len(sg);\n\n\t\twhile (sg_len) {\n\t\t\toffset = addr & 0xffff;\n\t\t\tlen = sg_len;\n\t\t\tif ((offset + sg_len) > 0x10000)\n\t\t\t\tlen = 0x10000 - offset;\n\n\t\t\tprd[idx].addr = cpu_to_le32(addr);\n\t\t\tprd[idx].flags_len = cpu_to_le32(len & 0xffff);\n\n\t\t\tidx++;\n\t\t\tsg_len -= len;\n\t\t\taddr += len;\n\t\t}\n\t}\n\n\tprd[idx - 1].flags_len |= cpu_to_le32(ATA_PRD_EOT);\n}\n\nstatic unsigned int nv_swncq_issue_atacmd(struct ata_port *ap,\n\t\t\t\t\t  struct ata_queued_cmd *qc)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\n\tif (qc == NULL)\n\t\treturn 0;\n\n\twritel((1 << qc->hw_tag), pp->sactive_block);\n\tpp->last_issue_tag = qc->hw_tag;\n\tpp->dhfis_bits &= ~(1 << qc->hw_tag);\n\tpp->dmafis_bits &= ~(1 << qc->hw_tag);\n\tpp->qc_active |= (0x1 << qc->hw_tag);\n\n\ttrace_ata_tf_load(ap, &qc->tf);\n\tap->ops->sff_tf_load(ap, &qc->tf);\t  \n\ttrace_ata_exec_command(ap, &qc->tf, qc->hw_tag);\n\tap->ops->sff_exec_command(ap, &qc->tf);\n\n\treturn 0;\n}\n\nstatic unsigned int nv_swncq_qc_issue(struct ata_queued_cmd *qc)\n{\n\tstruct ata_port *ap = qc->ap;\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\n\tif (qc->tf.protocol != ATA_PROT_NCQ)\n\t\treturn ata_bmdma_qc_issue(qc);\n\n\tif (!pp->qc_active)\n\t\tnv_swncq_issue_atacmd(ap, qc);\n\telse\n\t\tnv_swncq_qc_to_dq(ap, qc);\t \n\n\treturn 0;\n}\n\nstatic void nv_swncq_hotplug(struct ata_port *ap, u32 fis)\n{\n\tu32 serror;\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\n\tata_ehi_clear_desc(ehi);\n\n\t \n\tsata_scr_read(&ap->link, SCR_ERROR, &serror);\n\tsata_scr_write(&ap->link, SCR_ERROR, serror);\n\n\t \n\tif (fis & NV_SWNCQ_IRQ_ADDED)\n\t\tata_ehi_push_desc(ehi, \"hot plug\");\n\telse if (fis & NV_SWNCQ_IRQ_REMOVED)\n\t\tata_ehi_push_desc(ehi, \"hot unplug\");\n\n\tata_ehi_hotplugged(ehi);\n\n\t \n\tehi->serror |= serror;\n\n\tata_port_freeze(ap);\n}\n\nstatic int nv_swncq_sdbfis(struct ata_port *ap)\n{\n\tstruct ata_queued_cmd *qc;\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\tu32 sactive;\n\tu32 done_mask;\n\tu8 host_stat;\n\tu8 lack_dhfis = 0;\n\n\thost_stat = ap->ops->bmdma_status(ap);\n\ttrace_ata_bmdma_status(ap, host_stat);\n\tif (unlikely(host_stat & ATA_DMA_ERR)) {\n\t\t \n\t\tata_ehi_clear_desc(ehi);\n\t\tata_ehi_push_desc(ehi, \"BMDMA stat 0x%x\", host_stat);\n\t\tehi->err_mask |= AC_ERR_HOST_BUS;\n\t\tehi->action |= ATA_EH_RESET;\n\t\treturn -EINVAL;\n\t}\n\n\tap->ops->sff_irq_clear(ap);\n\t__ata_bmdma_stop(ap);\n\n\tsactive = readl(pp->sactive_block);\n\tdone_mask = pp->qc_active ^ sactive;\n\n\tpp->qc_active &= ~done_mask;\n\tpp->dhfis_bits &= ~done_mask;\n\tpp->dmafis_bits &= ~done_mask;\n\tpp->sdbfis_bits |= done_mask;\n\tata_qc_complete_multiple(ap, ata_qc_get_active(ap) ^ done_mask);\n\n\tif (!ap->qc_active) {\n\t\tata_port_dbg(ap, \"over\\n\");\n\t\tnv_swncq_pp_reinit(ap);\n\t\treturn 0;\n\t}\n\n\tif (pp->qc_active & pp->dhfis_bits)\n\t\treturn 0;\n\n\tif ((pp->ncq_flags & ncq_saw_backout) ||\n\t    (pp->qc_active ^ pp->dhfis_bits))\n\t\t \n\t\tlack_dhfis = 1;\n\n\tata_port_dbg(ap, \"QC: qc_active 0x%llx,\"\n\t\t     \"SWNCQ:qc_active 0x%X defer_bits %X \"\n\t\t     \"dhfis 0x%X dmafis 0x%X last_issue_tag %x\\n\",\n\t\t     ap->qc_active, pp->qc_active,\n\t\t     pp->defer_queue.defer_bits, pp->dhfis_bits,\n\t\t     pp->dmafis_bits, pp->last_issue_tag);\n\n\tnv_swncq_fis_reinit(ap);\n\n\tif (lack_dhfis) {\n\t\tqc = ata_qc_from_tag(ap, pp->last_issue_tag);\n\t\tnv_swncq_issue_atacmd(ap, qc);\n\t\treturn 0;\n\t}\n\n\tif (pp->defer_queue.defer_bits) {\n\t\t \n\t\tqc = nv_swncq_qc_from_dq(ap);\n\t\tWARN_ON(qc == NULL);\n\t\tnv_swncq_issue_atacmd(ap, qc);\n\t}\n\n\treturn 0;\n}\n\nstatic inline u32 nv_swncq_tag(struct ata_port *ap)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tu32 tag;\n\n\ttag = readb(pp->tag_block) >> 2;\n\treturn (tag & 0x1f);\n}\n\nstatic void nv_swncq_dmafis(struct ata_port *ap)\n{\n\tstruct ata_queued_cmd *qc;\n\tunsigned int rw;\n\tu8 dmactl;\n\tu32 tag;\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\n\t__ata_bmdma_stop(ap);\n\ttag = nv_swncq_tag(ap);\n\n\tata_port_dbg(ap, \"dma setup tag 0x%x\\n\", tag);\n\tqc = ata_qc_from_tag(ap, tag);\n\n\tif (unlikely(!qc))\n\t\treturn;\n\n\trw = qc->tf.flags & ATA_TFLAG_WRITE;\n\n\t \n\tiowrite32(pp->prd_dma + ATA_PRD_TBL_SZ * qc->hw_tag,\n\t\t  ap->ioaddr.bmdma_addr + ATA_DMA_TABLE_OFS);\n\n\t \n\tdmactl = ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n\tdmactl &= ~ATA_DMA_WR;\n\tif (!rw)\n\t\tdmactl |= ATA_DMA_WR;\n\n\tiowrite8(dmactl | ATA_DMA_START, ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\n}\n\nstatic void nv_swncq_host_interrupt(struct ata_port *ap, u16 fis)\n{\n\tstruct nv_swncq_port_priv *pp = ap->private_data;\n\tstruct ata_queued_cmd *qc;\n\tstruct ata_eh_info *ehi = &ap->link.eh_info;\n\tu32 serror;\n\tu8 ata_stat;\n\n\tata_stat = ap->ops->sff_check_status(ap);\n\tnv_swncq_irq_clear(ap, fis);\n\tif (!fis)\n\t\treturn;\n\n\tif (ata_port_is_frozen(ap))\n\t\treturn;\n\n\tif (fis & NV_SWNCQ_IRQ_HOTPLUG) {\n\t\tnv_swncq_hotplug(ap, fis);\n\t\treturn;\n\t}\n\n\tif (!pp->qc_active)\n\t\treturn;\n\n\tif (ap->ops->scr_read(&ap->link, SCR_ERROR, &serror))\n\t\treturn;\n\tap->ops->scr_write(&ap->link, SCR_ERROR, serror);\n\n\tif (ata_stat & ATA_ERR) {\n\t\tata_ehi_clear_desc(ehi);\n\t\tata_ehi_push_desc(ehi, \"Ata error. fis:0x%X\", fis);\n\t\tehi->err_mask |= AC_ERR_DEV;\n\t\tehi->serror |= serror;\n\t\tehi->action |= ATA_EH_RESET;\n\t\tata_port_freeze(ap);\n\t\treturn;\n\t}\n\n\tif (fis & NV_SWNCQ_IRQ_BACKOUT) {\n\t\t \n\t\tpp->ncq_flags |= ncq_saw_backout;\n\t}\n\n\tif (fis & NV_SWNCQ_IRQ_SDBFIS) {\n\t\tpp->ncq_flags |= ncq_saw_sdb;\n\t\tata_port_dbg(ap, \"SWNCQ: qc_active 0x%X \"\n\t\t\t\"dhfis 0x%X dmafis 0x%X sactive 0x%X\\n\",\n\t\t\tpp->qc_active, pp->dhfis_bits,\n\t\t\tpp->dmafis_bits, readl(pp->sactive_block));\n\t\tif (nv_swncq_sdbfis(ap) < 0)\n\t\t\tgoto irq_error;\n\t}\n\n\tif (fis & NV_SWNCQ_IRQ_DHREGFIS) {\n\t\t \n\t\tpp->dhfis_bits |= (0x1 << pp->last_issue_tag);\n\t\tpp->ncq_flags |= ncq_saw_d2h;\n\t\tif (pp->ncq_flags & (ncq_saw_sdb | ncq_saw_backout)) {\n\t\t\tata_ehi_push_desc(ehi, \"illegal fis transaction\");\n\t\t\tehi->err_mask |= AC_ERR_HSM;\n\t\t\tehi->action |= ATA_EH_RESET;\n\t\t\tgoto irq_error;\n\t\t}\n\n\t\tif (!(fis & NV_SWNCQ_IRQ_DMASETUP) &&\n\t\t    !(pp->ncq_flags & ncq_saw_dmas)) {\n\t\t\tata_stat = ap->ops->sff_check_status(ap);\n\t\t\tif (ata_stat & ATA_BUSY)\n\t\t\t\tgoto irq_exit;\n\n\t\t\tif (pp->defer_queue.defer_bits) {\n\t\t\t\tata_port_dbg(ap, \"send next command\\n\");\n\t\t\t\tqc = nv_swncq_qc_from_dq(ap);\n\t\t\t\tnv_swncq_issue_atacmd(ap, qc);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (fis & NV_SWNCQ_IRQ_DMASETUP) {\n\t\t \n\t\tpp->dmafis_bits |= (0x1 << nv_swncq_tag(ap));\n\t\tpp->ncq_flags |= ncq_saw_dmas;\n\t\tnv_swncq_dmafis(ap);\n\t}\n\nirq_exit:\n\treturn;\nirq_error:\n\tata_ehi_push_desc(ehi, \"fis:0x%x\", fis);\n\tata_port_freeze(ap);\n\treturn;\n}\n\nstatic irqreturn_t nv_swncq_interrupt(int irq, void *dev_instance)\n{\n\tstruct ata_host *host = dev_instance;\n\tunsigned int i;\n\tunsigned int handled = 0;\n\tunsigned long flags;\n\tu32 irq_stat;\n\n\tspin_lock_irqsave(&host->lock, flags);\n\n\tirq_stat = readl(host->iomap[NV_MMIO_BAR] + NV_INT_STATUS_MCP55);\n\n\tfor (i = 0; i < host->n_ports; i++) {\n\t\tstruct ata_port *ap = host->ports[i];\n\n\t\tif (ap->link.sactive) {\n\t\t\tnv_swncq_host_interrupt(ap, (u16)irq_stat);\n\t\t\thandled = 1;\n\t\t} else {\n\t\t\tif (irq_stat)\t \n\t\t\t\tnv_swncq_irq_clear(ap, 0xfff0);\n\n\t\t\thandled += nv_host_intr(ap, (u8)irq_stat);\n\t\t}\n\t\tirq_stat >>= NV_INT_PORT_SHIFT_MCP55;\n\t}\n\n\tspin_unlock_irqrestore(&host->lock, flags);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic int nv_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tconst struct ata_port_info *ppi[] = { NULL, NULL };\n\tstruct nv_pi_priv *ipriv;\n\tstruct ata_host *host;\n\tstruct nv_host_priv *hpriv;\n\tint rc;\n\tu32 bar;\n\tvoid __iomem *base;\n\tunsigned long type = ent->driver_data;\n\n        \n        \n        \n\tfor (bar = 0; bar < PCI_STD_NUM_BARS; bar++)\n\t\tif (pci_resource_start(pdev, bar) == 0)\n\t\t\treturn -ENODEV;\n\n\tata_print_version_once(&pdev->dev, DRV_VERSION);\n\n\trc = pcim_enable_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (type == CK804 && adma_enabled) {\n\t\tdev_notice(&pdev->dev, \"Using ADMA mode\\n\");\n\t\ttype = ADMA;\n\t} else if (type == MCP5x && swncq_enabled) {\n\t\tdev_notice(&pdev->dev, \"Using SWNCQ mode\\n\");\n\t\ttype = SWNCQ;\n\t}\n\n\tppi[0] = &nv_port_info[type];\n\tipriv = ppi[0]->private_data;\n\trc = ata_pci_bmdma_prepare_host(pdev, ppi, &host);\n\tif (rc)\n\t\treturn rc;\n\n\thpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\n\tif (!hpriv)\n\t\treturn -ENOMEM;\n\thpriv->type = type;\n\thost->private_data = hpriv;\n\n\t \n\trc = pcim_iomap_regions(pdev, 1 << NV_MMIO_BAR, DRV_NAME);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tbase = host->iomap[NV_MMIO_BAR];\n\thost->ports[0]->ioaddr.scr_addr = base + NV_PORT0_SCR_REG_OFFSET;\n\thost->ports[1]->ioaddr.scr_addr = base + NV_PORT1_SCR_REG_OFFSET;\n\n\t \n\tif (type >= CK804) {\n\t\tu8 regval;\n\n\t\tpci_read_config_byte(pdev, NV_MCP_SATA_CFG_20, &regval);\n\t\tregval |= NV_MCP_SATA_CFG_20_SATA_SPACE_EN;\n\t\tpci_write_config_byte(pdev, NV_MCP_SATA_CFG_20, regval);\n\t}\n\n\t \n\tif (type == ADMA) {\n\t\trc = nv_adma_host_init(host);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else if (type == SWNCQ)\n\t\tnv_swncq_host_init(host);\n\n\tif (msi_enabled) {\n\t\tdev_notice(&pdev->dev, \"Using MSI\\n\");\n\t\tpci_enable_msi(pdev);\n\t}\n\n\tpci_set_master(pdev);\n\treturn ata_pci_sff_activate_host(host, ipriv->irq_handler, ipriv->sht);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int nv_pci_device_resume(struct pci_dev *pdev)\n{\n\tstruct ata_host *host = pci_get_drvdata(pdev);\n\tstruct nv_host_priv *hpriv = host->private_data;\n\tint rc;\n\n\trc = ata_pci_device_do_resume(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\tif (pdev->dev.power.power_state.event == PM_EVENT_SUSPEND) {\n\t\tif (hpriv->type >= CK804) {\n\t\t\tu8 regval;\n\n\t\t\tpci_read_config_byte(pdev, NV_MCP_SATA_CFG_20, &regval);\n\t\t\tregval |= NV_MCP_SATA_CFG_20_SATA_SPACE_EN;\n\t\t\tpci_write_config_byte(pdev, NV_MCP_SATA_CFG_20, regval);\n\t\t}\n\t\tif (hpriv->type == ADMA) {\n\t\t\tu32 tmp32;\n\t\t\tstruct nv_adma_port_priv *pp;\n\t\t\t \n\t\t\tpci_read_config_dword(pdev, NV_MCP_SATA_CFG_20, &tmp32);\n\n\t\t\tpp = host->ports[0]->private_data;\n\t\t\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE)\n\t\t\t\ttmp32 &= ~(NV_MCP_SATA_CFG_20_PORT0_EN |\n\t\t\t\t\t   NV_MCP_SATA_CFG_20_PORT0_PWB_EN);\n\t\t\telse\n\t\t\t\ttmp32 |=  (NV_MCP_SATA_CFG_20_PORT0_EN |\n\t\t\t\t\t   NV_MCP_SATA_CFG_20_PORT0_PWB_EN);\n\t\t\tpp = host->ports[1]->private_data;\n\t\t\tif (pp->flags & NV_ADMA_ATAPI_SETUP_COMPLETE)\n\t\t\t\ttmp32 &= ~(NV_MCP_SATA_CFG_20_PORT1_EN |\n\t\t\t\t\t   NV_MCP_SATA_CFG_20_PORT1_PWB_EN);\n\t\t\telse\n\t\t\t\ttmp32 |=  (NV_MCP_SATA_CFG_20_PORT1_EN |\n\t\t\t\t\t   NV_MCP_SATA_CFG_20_PORT1_PWB_EN);\n\n\t\t\tpci_write_config_dword(pdev, NV_MCP_SATA_CFG_20, tmp32);\n\t\t}\n\t}\n\n\tata_host_resume(host);\n\n\treturn 0;\n}\n#endif\n\nstatic void nv_ck804_host_stop(struct ata_host *host)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tu8 regval;\n\n\t \n\tpci_read_config_byte(pdev, NV_MCP_SATA_CFG_20, &regval);\n\tregval &= ~NV_MCP_SATA_CFG_20_SATA_SPACE_EN;\n\tpci_write_config_byte(pdev, NV_MCP_SATA_CFG_20, regval);\n}\n\nstatic void nv_adma_host_stop(struct ata_host *host)\n{\n\tstruct pci_dev *pdev = to_pci_dev(host->dev);\n\tu32 tmp32;\n\n\t \n\tpci_read_config_dword(pdev, NV_MCP_SATA_CFG_20, &tmp32);\n\ttmp32 &= ~(NV_MCP_SATA_CFG_20_PORT0_EN |\n\t\t   NV_MCP_SATA_CFG_20_PORT0_PWB_EN |\n\t\t   NV_MCP_SATA_CFG_20_PORT1_EN |\n\t\t   NV_MCP_SATA_CFG_20_PORT1_PWB_EN);\n\n\tpci_write_config_dword(pdev, NV_MCP_SATA_CFG_20, tmp32);\n\n\tnv_ck804_host_stop(host);\n}\n\nmodule_pci_driver(nv_pci_driver);\n\nmodule_param_named(adma, adma_enabled, bool, 0444);\nMODULE_PARM_DESC(adma, \"Enable use of ADMA (Default: false)\");\nmodule_param_named(swncq, swncq_enabled, bool, 0444);\nMODULE_PARM_DESC(swncq, \"Enable use of SWNCQ (Default: true)\");\nmodule_param_named(msi, msi_enabled, bool, 0444);\nMODULE_PARM_DESC(msi, \"Enable use of MSI (Default: false)\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}