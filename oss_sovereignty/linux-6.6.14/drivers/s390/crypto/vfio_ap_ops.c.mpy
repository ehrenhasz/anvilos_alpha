{
  "module_name": "vfio_ap_ops.c",
  "hash_id": "dd1171f6e1b0dd18070433d510020f71a9181f474970a9cb5b7077c02f666d8c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/s390/crypto/vfio_ap_ops.c",
  "human_readable_source": "\n \n#include <linux/string.h>\n#include <linux/vfio.h>\n#include <linux/device.h>\n#include <linux/list.h>\n#include <linux/ctype.h>\n#include <linux/bitops.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/uuid.h>\n#include <asm/kvm.h>\n#include <asm/zcrypt.h>\n\n#include \"vfio_ap_private.h\"\n#include \"vfio_ap_debug.h\"\n\n#define VFIO_AP_MDEV_TYPE_HWVIRT \"passthrough\"\n#define VFIO_AP_MDEV_NAME_HWVIRT \"VFIO AP Passthrough Device\"\n\n#define AP_QUEUE_ASSIGNED \"assigned\"\n#define AP_QUEUE_UNASSIGNED \"unassigned\"\n#define AP_QUEUE_IN_USE \"in use\"\n\n#define AP_RESET_INTERVAL\t\t20\t \n\nstatic int vfio_ap_mdev_reset_queues(struct ap_queue_table *qtable);\nstatic struct vfio_ap_queue *vfio_ap_find_queue(int apqn);\nstatic const struct vfio_device_ops vfio_ap_matrix_dev_ops;\nstatic void vfio_ap_mdev_reset_queue(struct vfio_ap_queue *q);\n\n \nstatic inline void get_update_locks_for_kvm(struct kvm *kvm)\n{\n\tmutex_lock(&matrix_dev->guests_lock);\n\tif (kvm)\n\t\tmutex_lock(&kvm->lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n}\n\n \nstatic inline void release_update_locks_for_kvm(struct kvm *kvm)\n{\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\tif (kvm)\n\t\tmutex_unlock(&kvm->lock);\n\tmutex_unlock(&matrix_dev->guests_lock);\n}\n\n \nstatic inline void get_update_locks_for_mdev(struct ap_matrix_mdev *matrix_mdev)\n{\n\tmutex_lock(&matrix_dev->guests_lock);\n\tif (matrix_mdev && matrix_mdev->kvm)\n\t\tmutex_lock(&matrix_mdev->kvm->lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n}\n\n \nstatic inline void release_update_locks_for_mdev(struct ap_matrix_mdev *matrix_mdev)\n{\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\tif (matrix_mdev && matrix_mdev->kvm)\n\t\tmutex_unlock(&matrix_mdev->kvm->lock);\n\tmutex_unlock(&matrix_dev->guests_lock);\n}\n\n \nstatic struct ap_matrix_mdev *get_update_locks_by_apqn(int apqn)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tmutex_lock(&matrix_dev->guests_lock);\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\tif (test_bit_inv(AP_QID_CARD(apqn), matrix_mdev->matrix.apm) &&\n\t\t    test_bit_inv(AP_QID_QUEUE(apqn), matrix_mdev->matrix.aqm)) {\n\t\t\tif (matrix_mdev->kvm)\n\t\t\t\tmutex_lock(&matrix_mdev->kvm->lock);\n\n\t\t\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\t\t\treturn matrix_mdev;\n\t\t}\n\t}\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\treturn NULL;\n}\n\n \nstatic inline void get_update_locks_for_queue(struct vfio_ap_queue *q)\n{\n\tmutex_lock(&matrix_dev->guests_lock);\n\tif (q->matrix_mdev && q->matrix_mdev->kvm)\n\t\tmutex_lock(&q->matrix_mdev->kvm->lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n}\n\n \nstatic struct vfio_ap_queue *vfio_ap_mdev_get_queue(\n\t\t\t\t\tstruct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\tint apqn)\n{\n\tstruct vfio_ap_queue *q;\n\n\thash_for_each_possible(matrix_mdev->qtable.queues, q, mdev_qnode,\n\t\t\t       apqn) {\n\t\tif (q && q->apqn == apqn)\n\t\t\treturn q;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void vfio_ap_wait_for_irqclear(int apqn)\n{\n\tstruct ap_queue_status status;\n\tint retry = 5;\n\n\tdo {\n\t\tstatus = ap_tapq(apqn, NULL);\n\t\tswitch (status.response_code) {\n\t\tcase AP_RESPONSE_NORMAL:\n\t\tcase AP_RESPONSE_RESET_IN_PROGRESS:\n\t\t\tif (!status.irq_enabled)\n\t\t\t\treturn;\n\t\t\tfallthrough;\n\t\tcase AP_RESPONSE_BUSY:\n\t\t\tmsleep(20);\n\t\t\tbreak;\n\t\tcase AP_RESPONSE_Q_NOT_AVAIL:\n\t\tcase AP_RESPONSE_DECONFIGURED:\n\t\tcase AP_RESPONSE_CHECKSTOPPED:\n\t\tdefault:\n\t\t\tWARN_ONCE(1, \"%s: tapq rc %02x: %04x\\n\", __func__,\n\t\t\t\t  status.response_code, apqn);\n\t\t\treturn;\n\t\t}\n\t} while (--retry);\n\n\tWARN_ONCE(1, \"%s: tapq rc %02x: %04x could not clear IR bit\\n\",\n\t\t  __func__, status.response_code, apqn);\n}\n\n \nstatic void vfio_ap_free_aqic_resources(struct vfio_ap_queue *q)\n{\n\tif (!q)\n\t\treturn;\n\tif (q->saved_isc != VFIO_AP_ISC_INVALID &&\n\t    !WARN_ON(!(q->matrix_mdev && q->matrix_mdev->kvm))) {\n\t\tkvm_s390_gisc_unregister(q->matrix_mdev->kvm, q->saved_isc);\n\t\tq->saved_isc = VFIO_AP_ISC_INVALID;\n\t}\n\tif (q->saved_iova && !WARN_ON(!q->matrix_mdev)) {\n\t\tvfio_unpin_pages(&q->matrix_mdev->vdev, q->saved_iova, 1);\n\t\tq->saved_iova = 0;\n\t}\n}\n\n \nstatic struct ap_queue_status vfio_ap_irq_disable(struct vfio_ap_queue *q)\n{\n\tunion ap_qirq_ctrl aqic_gisa = { .value = 0 };\n\tstruct ap_queue_status status;\n\tint retries = 5;\n\n\tdo {\n\t\tstatus = ap_aqic(q->apqn, aqic_gisa, 0);\n\t\tswitch (status.response_code) {\n\t\tcase AP_RESPONSE_OTHERWISE_CHANGED:\n\t\tcase AP_RESPONSE_NORMAL:\n\t\t\tvfio_ap_wait_for_irqclear(q->apqn);\n\t\t\tgoto end_free;\n\t\tcase AP_RESPONSE_RESET_IN_PROGRESS:\n\t\tcase AP_RESPONSE_BUSY:\n\t\t\tmsleep(20);\n\t\t\tbreak;\n\t\tcase AP_RESPONSE_Q_NOT_AVAIL:\n\t\tcase AP_RESPONSE_DECONFIGURED:\n\t\tcase AP_RESPONSE_CHECKSTOPPED:\n\t\tcase AP_RESPONSE_INVALID_ADDRESS:\n\t\tdefault:\n\t\t\t \n\t\t\tWARN_ONCE(1, \"%s: ap_aqic status %d\\n\", __func__,\n\t\t\t\t  status.response_code);\n\t\t\tgoto end_free;\n\t\t}\n\t} while (retries--);\n\n\tWARN_ONCE(1, \"%s: ap_aqic status %d\\n\", __func__,\n\t\t  status.response_code);\nend_free:\n\tvfio_ap_free_aqic_resources(q);\n\treturn status;\n}\n\n \nstatic int vfio_ap_validate_nib(struct kvm_vcpu *vcpu, dma_addr_t *nib)\n{\n\t*nib = vcpu->run->s.regs.gprs[2];\n\n\tif (!*nib)\n\t\treturn -EINVAL;\n\tif (kvm_is_error_hva(gfn_to_hva(vcpu->kvm, *nib >> PAGE_SHIFT)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int ensure_nib_shared(unsigned long addr, struct gmap *gmap)\n{\n\tint ret;\n\n\t \n\tret = uv_pin_shared(addr);\n\tif (ret) {\n\t\t \n\t\tgmap_convert_to_secure(gmap, addr);\n\t}\n\treturn ret;\n}\n\n \nstatic struct ap_queue_status vfio_ap_irq_enable(struct vfio_ap_queue *q,\n\t\t\t\t\t\t int isc,\n\t\t\t\t\t\t struct kvm_vcpu *vcpu)\n{\n\tunion ap_qirq_ctrl aqic_gisa = { .value = 0 };\n\tstruct ap_queue_status status = {};\n\tstruct kvm_s390_gisa *gisa;\n\tstruct page *h_page;\n\tint nisc;\n\tstruct kvm *kvm;\n\tphys_addr_t h_nib;\n\tdma_addr_t nib;\n\tint ret;\n\n\t \n\tif (vfio_ap_validate_nib(vcpu, &nib)) {\n\t\tVFIO_AP_DBF_WARN(\"%s: invalid NIB address: nib=%pad, apqn=%#04x\\n\",\n\t\t\t\t __func__, &nib, q->apqn);\n\n\t\tstatus.response_code = AP_RESPONSE_INVALID_ADDRESS;\n\t\treturn status;\n\t}\n\n\tret = vfio_pin_pages(&q->matrix_mdev->vdev, nib, 1,\n\t\t\t     IOMMU_READ | IOMMU_WRITE, &h_page);\n\tswitch (ret) {\n\tcase 1:\n\t\tbreak;\n\tdefault:\n\t\tVFIO_AP_DBF_WARN(\"%s: vfio_pin_pages failed: rc=%d,\"\n\t\t\t\t \"nib=%pad, apqn=%#04x\\n\",\n\t\t\t\t __func__, ret, &nib, q->apqn);\n\n\t\tstatus.response_code = AP_RESPONSE_INVALID_ADDRESS;\n\t\treturn status;\n\t}\n\n\tkvm = q->matrix_mdev->kvm;\n\tgisa = kvm->arch.gisa_int.origin;\n\n\th_nib = page_to_phys(h_page) | (nib & ~PAGE_MASK);\n\taqic_gisa.gisc = isc;\n\n\t \n\tif (kvm_s390_pv_cpu_is_protected(vcpu) &&\n\t    ensure_nib_shared(h_nib & PAGE_MASK, kvm->arch.gmap)) {\n\t\tvfio_unpin_pages(&q->matrix_mdev->vdev, nib, 1);\n\t\tstatus.response_code = AP_RESPONSE_INVALID_ADDRESS;\n\t\treturn status;\n\t}\n\n\tnisc = kvm_s390_gisc_register(kvm, isc);\n\tif (nisc < 0) {\n\t\tVFIO_AP_DBF_WARN(\"%s: gisc registration failed: nisc=%d, isc=%d, apqn=%#04x\\n\",\n\t\t\t\t __func__, nisc, isc, q->apqn);\n\n\t\tstatus.response_code = AP_RESPONSE_INVALID_GISA;\n\t\treturn status;\n\t}\n\n\taqic_gisa.isc = nisc;\n\taqic_gisa.ir = 1;\n\taqic_gisa.gisa = virt_to_phys(gisa) >> 4;\n\n\tstatus = ap_aqic(q->apqn, aqic_gisa, h_nib);\n\tswitch (status.response_code) {\n\tcase AP_RESPONSE_NORMAL:\n\t\t \n\t\tvfio_ap_free_aqic_resources(q);\n\t\tq->saved_iova = nib;\n\t\tq->saved_isc = isc;\n\t\tbreak;\n\tcase AP_RESPONSE_OTHERWISE_CHANGED:\n\t\t \n\t\tvfio_unpin_pages(&q->matrix_mdev->vdev, nib, 1);\n\t\tkvm_s390_gisc_unregister(kvm, isc);\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"%s: apqn %04x: response: %02x\\n\", __func__, q->apqn,\n\t\t\tstatus.response_code);\n\t\tvfio_ap_irq_disable(q);\n\t\tbreak;\n\t}\n\n\tif (status.response_code != AP_RESPONSE_NORMAL) {\n\t\tVFIO_AP_DBF_WARN(\"%s: PQAP(AQIC) failed with status=%#02x: \"\n\t\t\t\t \"zone=%#x, ir=%#x, gisc=%#x, f=%#x,\"\n\t\t\t\t \"gisa=%#x, isc=%#x, apqn=%#04x\\n\",\n\t\t\t\t __func__, status.response_code,\n\t\t\t\t aqic_gisa.zone, aqic_gisa.ir, aqic_gisa.gisc,\n\t\t\t\t aqic_gisa.gf, aqic_gisa.gisa, aqic_gisa.isc,\n\t\t\t\t q->apqn);\n\t}\n\n\treturn status;\n}\n\n \nstatic void vfio_ap_le_guid_to_be_uuid(guid_t *guid, unsigned long *uuid)\n{\n\t \n\tuuid[0] = le32_to_cpup((__le32 *)guid);\n\tuuid[1] = le16_to_cpup((__le16 *)&guid->b[4]);\n\tuuid[2] = le16_to_cpup((__le16 *)&guid->b[6]);\n\tuuid[3] = *((__u16 *)&guid->b[8]);\n\tuuid[4] = *((__u16 *)&guid->b[10]);\n\tuuid[5] = *((__u32 *)&guid->b[12]);\n}\n\n \nstatic int handle_pqap(struct kvm_vcpu *vcpu)\n{\n\tuint64_t status;\n\tuint16_t apqn;\n\tunsigned long uuid[6];\n\tstruct vfio_ap_queue *q;\n\tstruct ap_queue_status qstatus = {\n\t\t\t       .response_code = AP_RESPONSE_Q_NOT_AVAIL, };\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tapqn = vcpu->run->s.regs.gprs[0] & 0xffff;\n\n\t \n\tif (!(vcpu->arch.sie_block->eca & ECA_AIV)) {\n\t\tVFIO_AP_DBF_WARN(\"%s: AIV facility not installed: apqn=0x%04x, eca=0x%04x\\n\",\n\t\t\t\t __func__, apqn, vcpu->arch.sie_block->eca);\n\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\tif (!vcpu->kvm->arch.crypto.pqap_hook) {\n\t\tVFIO_AP_DBF_WARN(\"%s: PQAP(AQIC) hook not registered with the vfio_ap driver: apqn=0x%04x\\n\",\n\t\t\t\t __func__, apqn);\n\n\t\tgoto out_unlock;\n\t}\n\n\tmatrix_mdev = container_of(vcpu->kvm->arch.crypto.pqap_hook,\n\t\t\t\t   struct ap_matrix_mdev, pqap_hook);\n\n\t \n\tif (!matrix_mdev->kvm) {\n\t\tvfio_ap_le_guid_to_be_uuid(&matrix_mdev->mdev->uuid, uuid);\n\t\tVFIO_AP_DBF_WARN(\"%s: mdev %08lx-%04lx-%04lx-%04lx-%04lx%08lx not in use: apqn=0x%04x\\n\",\n\t\t\t\t __func__, uuid[0],  uuid[1], uuid[2],\n\t\t\t\t uuid[3], uuid[4], uuid[5], apqn);\n\t\tgoto out_unlock;\n\t}\n\n\tq = vfio_ap_mdev_get_queue(matrix_mdev, apqn);\n\tif (!q) {\n\t\tVFIO_AP_DBF_WARN(\"%s: Queue %02x.%04x not bound to the vfio_ap driver\\n\",\n\t\t\t\t __func__, AP_QID_CARD(apqn),\n\t\t\t\t AP_QID_QUEUE(apqn));\n\t\tgoto out_unlock;\n\t}\n\n\tstatus = vcpu->run->s.regs.gprs[1];\n\n\t \n\tif ((status >> (63 - 16)) & 0x01)\n\t\tqstatus = vfio_ap_irq_enable(q, status & 0x07, vcpu);\n\telse\n\t\tqstatus = vfio_ap_irq_disable(q);\n\nout_unlock:\n\tmemcpy(&vcpu->run->s.regs.gprs[1], &qstatus, sizeof(qstatus));\n\tvcpu->run->s.regs.gprs[1] >>= 32;\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\treturn 0;\n}\n\nstatic void vfio_ap_matrix_init(struct ap_config_info *info,\n\t\t\t\tstruct ap_matrix *matrix)\n{\n\tmatrix->apm_max = info->apxa ? info->na : 63;\n\tmatrix->aqm_max = info->apxa ? info->nd : 15;\n\tmatrix->adm_max = info->apxa ? info->nd : 15;\n}\n\nstatic void vfio_ap_mdev_update_guest_apcb(struct ap_matrix_mdev *matrix_mdev)\n{\n\tif (matrix_mdev->kvm)\n\t\tkvm_arch_crypto_set_masks(matrix_mdev->kvm,\n\t\t\t\t\t  matrix_mdev->shadow_apcb.apm,\n\t\t\t\t\t  matrix_mdev->shadow_apcb.aqm,\n\t\t\t\t\t  matrix_mdev->shadow_apcb.adm);\n}\n\nstatic bool vfio_ap_mdev_filter_cdoms(struct ap_matrix_mdev *matrix_mdev)\n{\n\tDECLARE_BITMAP(prev_shadow_adm, AP_DOMAINS);\n\n\tbitmap_copy(prev_shadow_adm, matrix_mdev->shadow_apcb.adm, AP_DOMAINS);\n\tbitmap_and(matrix_mdev->shadow_apcb.adm, matrix_mdev->matrix.adm,\n\t\t   (unsigned long *)matrix_dev->info.adm, AP_DOMAINS);\n\n\treturn !bitmap_equal(prev_shadow_adm, matrix_mdev->shadow_apcb.adm,\n\t\t\t     AP_DOMAINS);\n}\n\n \nstatic bool vfio_ap_mdev_filter_matrix(unsigned long *apm, unsigned long *aqm,\n\t\t\t\t       struct ap_matrix_mdev *matrix_mdev)\n{\n\tunsigned long apid, apqi, apqn;\n\tDECLARE_BITMAP(prev_shadow_apm, AP_DEVICES);\n\tDECLARE_BITMAP(prev_shadow_aqm, AP_DOMAINS);\n\tstruct vfio_ap_queue *q;\n\n\tbitmap_copy(prev_shadow_apm, matrix_mdev->shadow_apcb.apm, AP_DEVICES);\n\tbitmap_copy(prev_shadow_aqm, matrix_mdev->shadow_apcb.aqm, AP_DOMAINS);\n\tvfio_ap_matrix_init(&matrix_dev->info, &matrix_mdev->shadow_apcb);\n\n\t \n\tbitmap_and(matrix_mdev->shadow_apcb.apm, matrix_mdev->matrix.apm,\n\t\t   (unsigned long *)matrix_dev->info.apm, AP_DEVICES);\n\tbitmap_and(matrix_mdev->shadow_apcb.aqm, matrix_mdev->matrix.aqm,\n\t\t   (unsigned long *)matrix_dev->info.aqm, AP_DOMAINS);\n\n\tfor_each_set_bit_inv(apid, apm, AP_DEVICES) {\n\t\tfor_each_set_bit_inv(apqi, aqm, AP_DOMAINS) {\n\t\t\t \n\t\t\tapqn = AP_MKQID(apid, apqi);\n\t\t\tq = vfio_ap_mdev_get_queue(matrix_mdev, apqn);\n\t\t\tif (!q || q->reset_status.response_code) {\n\t\t\t\tclear_bit_inv(apid,\n\t\t\t\t\t      matrix_mdev->shadow_apcb.apm);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn !bitmap_equal(prev_shadow_apm, matrix_mdev->shadow_apcb.apm,\n\t\t\t     AP_DEVICES) ||\n\t       !bitmap_equal(prev_shadow_aqm, matrix_mdev->shadow_apcb.aqm,\n\t\t\t     AP_DOMAINS);\n}\n\nstatic int vfio_ap_mdev_init_dev(struct vfio_device *vdev)\n{\n\tstruct ap_matrix_mdev *matrix_mdev =\n\t\tcontainer_of(vdev, struct ap_matrix_mdev, vdev);\n\n\tmatrix_mdev->mdev = to_mdev_device(vdev->dev);\n\tvfio_ap_matrix_init(&matrix_dev->info, &matrix_mdev->matrix);\n\tmatrix_mdev->pqap_hook = handle_pqap;\n\tvfio_ap_matrix_init(&matrix_dev->info, &matrix_mdev->shadow_apcb);\n\thash_init(matrix_mdev->qtable.queues);\n\n\treturn 0;\n}\n\nstatic int vfio_ap_mdev_probe(struct mdev_device *mdev)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\tint ret;\n\n\tmatrix_mdev = vfio_alloc_device(ap_matrix_mdev, vdev, &mdev->dev,\n\t\t\t\t\t&vfio_ap_matrix_dev_ops);\n\tif (IS_ERR(matrix_mdev))\n\t\treturn PTR_ERR(matrix_mdev);\n\n\tret = vfio_register_emulated_iommu_dev(&matrix_mdev->vdev);\n\tif (ret)\n\t\tgoto err_put_vdev;\n\tmatrix_mdev->req_trigger = NULL;\n\tdev_set_drvdata(&mdev->dev, matrix_mdev);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tlist_add(&matrix_mdev->node, &matrix_dev->mdev_list);\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\treturn 0;\n\nerr_put_vdev:\n\tvfio_put_device(&matrix_mdev->vdev);\n\treturn ret;\n}\n\nstatic void vfio_ap_mdev_link_queue(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t    struct vfio_ap_queue *q)\n{\n\tif (q) {\n\t\tq->matrix_mdev = matrix_mdev;\n\t\thash_add(matrix_mdev->qtable.queues, &q->mdev_qnode, q->apqn);\n\t}\n}\n\nstatic void vfio_ap_mdev_link_apqn(struct ap_matrix_mdev *matrix_mdev, int apqn)\n{\n\tstruct vfio_ap_queue *q;\n\n\tq = vfio_ap_find_queue(apqn);\n\tvfio_ap_mdev_link_queue(matrix_mdev, q);\n}\n\nstatic void vfio_ap_unlink_queue_fr_mdev(struct vfio_ap_queue *q)\n{\n\thash_del(&q->mdev_qnode);\n}\n\nstatic void vfio_ap_unlink_mdev_fr_queue(struct vfio_ap_queue *q)\n{\n\tq->matrix_mdev = NULL;\n}\n\nstatic void vfio_ap_mdev_unlink_fr_queues(struct ap_matrix_mdev *matrix_mdev)\n{\n\tstruct vfio_ap_queue *q;\n\tunsigned long apid, apqi;\n\n\tfor_each_set_bit_inv(apid, matrix_mdev->matrix.apm, AP_DEVICES) {\n\t\tfor_each_set_bit_inv(apqi, matrix_mdev->matrix.aqm,\n\t\t\t\t     AP_DOMAINS) {\n\t\t\tq = vfio_ap_mdev_get_queue(matrix_mdev,\n\t\t\t\t\t\t   AP_MKQID(apid, apqi));\n\t\t\tif (q)\n\t\t\t\tq->matrix_mdev = NULL;\n\t\t}\n\t}\n}\n\nstatic void vfio_ap_mdev_remove(struct mdev_device *mdev)\n{\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(&mdev->dev);\n\n\tvfio_unregister_group_dev(&matrix_mdev->vdev);\n\n\tmutex_lock(&matrix_dev->guests_lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tvfio_ap_mdev_reset_queues(&matrix_mdev->qtable);\n\tvfio_ap_mdev_unlink_fr_queues(matrix_mdev);\n\tlist_del(&matrix_mdev->node);\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\tmutex_unlock(&matrix_dev->guests_lock);\n\tvfio_put_device(&matrix_mdev->vdev);\n}\n\n#define MDEV_SHARING_ERR \"Userspace may not re-assign queue %02lx.%04lx \" \\\n\t\t\t \"already assigned to %s\"\n\nstatic void vfio_ap_mdev_log_sharing_err(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\t unsigned long *apm,\n\t\t\t\t\t unsigned long *aqm)\n{\n\tunsigned long apid, apqi;\n\tconst struct device *dev = mdev_dev(matrix_mdev->mdev);\n\tconst char *mdev_name = dev_name(dev);\n\n\tfor_each_set_bit_inv(apid, apm, AP_DEVICES)\n\t\tfor_each_set_bit_inv(apqi, aqm, AP_DOMAINS)\n\t\t\tdev_warn(dev, MDEV_SHARING_ERR, apid, apqi, mdev_name);\n}\n\n \nstatic int vfio_ap_mdev_verify_no_sharing(unsigned long *mdev_apm,\n\t\t\t\t\t  unsigned long *mdev_aqm)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\tDECLARE_BITMAP(apm, AP_DEVICES);\n\tDECLARE_BITMAP(aqm, AP_DOMAINS);\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\t \n\t\tif (mdev_apm == matrix_mdev->matrix.apm &&\n\t\t    mdev_aqm == matrix_mdev->matrix.aqm)\n\t\t\tcontinue;\n\n\t\tmemset(apm, 0, sizeof(apm));\n\t\tmemset(aqm, 0, sizeof(aqm));\n\n\t\t \n\t\tif (!bitmap_and(apm, mdev_apm, matrix_mdev->matrix.apm,\n\t\t\t\tAP_DEVICES))\n\t\t\tcontinue;\n\n\t\tif (!bitmap_and(aqm, mdev_aqm, matrix_mdev->matrix.aqm,\n\t\t\t\tAP_DOMAINS))\n\t\t\tcontinue;\n\n\t\tvfio_ap_mdev_log_sharing_err(matrix_mdev, apm, aqm);\n\n\t\treturn -EADDRINUSE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int vfio_ap_mdev_validate_masks(struct ap_matrix_mdev *matrix_mdev)\n{\n\tif (ap_apqn_in_matrix_owned_by_def_drv(matrix_mdev->matrix.apm,\n\t\t\t\t\t       matrix_mdev->matrix.aqm))\n\t\treturn -EADDRNOTAVAIL;\n\n\treturn vfio_ap_mdev_verify_no_sharing(matrix_mdev->matrix.apm,\n\t\t\t\t\t      matrix_mdev->matrix.aqm);\n}\n\nstatic void vfio_ap_mdev_link_adapter(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t      unsigned long apid)\n{\n\tunsigned long apqi;\n\n\tfor_each_set_bit_inv(apqi, matrix_mdev->matrix.aqm, AP_DOMAINS)\n\t\tvfio_ap_mdev_link_apqn(matrix_mdev,\n\t\t\t\t       AP_MKQID(apid, apqi));\n}\n\n \nstatic ssize_t assign_adapter_store(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long apid;\n\tDECLARE_BITMAP(apm_delta, AP_DEVICES);\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tmutex_lock(&ap_perms_mutex);\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &apid);\n\tif (ret)\n\t\tgoto done;\n\n\tif (apid > matrix_mdev->matrix.apm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (test_bit_inv(apid, matrix_mdev->matrix.apm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\tset_bit_inv(apid, matrix_mdev->matrix.apm);\n\n\tret = vfio_ap_mdev_validate_masks(matrix_mdev);\n\tif (ret) {\n\t\tclear_bit_inv(apid, matrix_mdev->matrix.apm);\n\t\tgoto done;\n\t}\n\n\tvfio_ap_mdev_link_adapter(matrix_mdev, apid);\n\tmemset(apm_delta, 0, sizeof(apm_delta));\n\tset_bit_inv(apid, apm_delta);\n\n\tif (vfio_ap_mdev_filter_matrix(apm_delta,\n\t\t\t\t       matrix_mdev->matrix.aqm, matrix_mdev))\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\n\tret = count;\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\tmutex_unlock(&ap_perms_mutex);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(assign_adapter);\n\nstatic struct vfio_ap_queue\n*vfio_ap_unlink_apqn_fr_mdev(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t     unsigned long apid, unsigned long apqi)\n{\n\tstruct vfio_ap_queue *q = NULL;\n\n\tq = vfio_ap_mdev_get_queue(matrix_mdev, AP_MKQID(apid, apqi));\n\t \n\tif (q)\n\t\tvfio_ap_unlink_queue_fr_mdev(q);\n\n\treturn q;\n}\n\n \nstatic void vfio_ap_mdev_unlink_adapter(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\tunsigned long apid,\n\t\t\t\t\tstruct ap_queue_table *qtable)\n{\n\tunsigned long apqi;\n\tstruct vfio_ap_queue *q;\n\n\tfor_each_set_bit_inv(apqi, matrix_mdev->matrix.aqm, AP_DOMAINS) {\n\t\tq = vfio_ap_unlink_apqn_fr_mdev(matrix_mdev, apid, apqi);\n\n\t\tif (q && qtable) {\n\t\t\tif (test_bit_inv(apid, matrix_mdev->shadow_apcb.apm) &&\n\t\t\t    test_bit_inv(apqi, matrix_mdev->shadow_apcb.aqm))\n\t\t\t\thash_add(qtable->queues, &q->mdev_qnode,\n\t\t\t\t\t q->apqn);\n\t\t}\n\t}\n}\n\nstatic void vfio_ap_mdev_hot_unplug_adapter(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\t    unsigned long apid)\n{\n\tint loop_cursor;\n\tstruct vfio_ap_queue *q;\n\tstruct ap_queue_table *qtable = kzalloc(sizeof(*qtable), GFP_KERNEL);\n\n\thash_init(qtable->queues);\n\tvfio_ap_mdev_unlink_adapter(matrix_mdev, apid, qtable);\n\n\tif (test_bit_inv(apid, matrix_mdev->shadow_apcb.apm)) {\n\t\tclear_bit_inv(apid, matrix_mdev->shadow_apcb.apm);\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\t}\n\n\tvfio_ap_mdev_reset_queues(qtable);\n\n\thash_for_each(qtable->queues, loop_cursor, q, mdev_qnode) {\n\t\tvfio_ap_unlink_mdev_fr_queue(q);\n\t\thash_del(&q->mdev_qnode);\n\t}\n\n\tkfree(qtable);\n}\n\n \nstatic ssize_t unassign_adapter_store(struct device *dev,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long apid;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &apid);\n\tif (ret)\n\t\tgoto done;\n\n\tif (apid > matrix_mdev->matrix.apm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (!test_bit_inv(apid, matrix_mdev->matrix.apm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\tclear_bit_inv((unsigned long)apid, matrix_mdev->matrix.apm);\n\tvfio_ap_mdev_hot_unplug_adapter(matrix_mdev, apid);\n\tret = count;\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(unassign_adapter);\n\nstatic void vfio_ap_mdev_link_domain(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t     unsigned long apqi)\n{\n\tunsigned long apid;\n\n\tfor_each_set_bit_inv(apid, matrix_mdev->matrix.apm, AP_DEVICES)\n\t\tvfio_ap_mdev_link_apqn(matrix_mdev,\n\t\t\t\t       AP_MKQID(apid, apqi));\n}\n\n \nstatic ssize_t assign_domain_store(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long apqi;\n\tDECLARE_BITMAP(aqm_delta, AP_DOMAINS);\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tmutex_lock(&ap_perms_mutex);\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &apqi);\n\tif (ret)\n\t\tgoto done;\n\n\tif (apqi > matrix_mdev->matrix.aqm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (test_bit_inv(apqi, matrix_mdev->matrix.aqm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\tset_bit_inv(apqi, matrix_mdev->matrix.aqm);\n\n\tret = vfio_ap_mdev_validate_masks(matrix_mdev);\n\tif (ret) {\n\t\tclear_bit_inv(apqi, matrix_mdev->matrix.aqm);\n\t\tgoto done;\n\t}\n\n\tvfio_ap_mdev_link_domain(matrix_mdev, apqi);\n\tmemset(aqm_delta, 0, sizeof(aqm_delta));\n\tset_bit_inv(apqi, aqm_delta);\n\n\tif (vfio_ap_mdev_filter_matrix(matrix_mdev->matrix.apm, aqm_delta,\n\t\t\t\t       matrix_mdev))\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\n\tret = count;\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\tmutex_unlock(&ap_perms_mutex);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(assign_domain);\n\nstatic void vfio_ap_mdev_unlink_domain(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t       unsigned long apqi,\n\t\t\t\t       struct ap_queue_table *qtable)\n{\n\tunsigned long apid;\n\tstruct vfio_ap_queue *q;\n\n\tfor_each_set_bit_inv(apid, matrix_mdev->matrix.apm, AP_DEVICES) {\n\t\tq = vfio_ap_unlink_apqn_fr_mdev(matrix_mdev, apid, apqi);\n\n\t\tif (q && qtable) {\n\t\t\tif (test_bit_inv(apid, matrix_mdev->shadow_apcb.apm) &&\n\t\t\t    test_bit_inv(apqi, matrix_mdev->shadow_apcb.aqm))\n\t\t\t\thash_add(qtable->queues, &q->mdev_qnode,\n\t\t\t\t\t q->apqn);\n\t\t}\n\t}\n}\n\nstatic void vfio_ap_mdev_hot_unplug_domain(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\t   unsigned long apqi)\n{\n\tint loop_cursor;\n\tstruct vfio_ap_queue *q;\n\tstruct ap_queue_table *qtable = kzalloc(sizeof(*qtable), GFP_KERNEL);\n\n\thash_init(qtable->queues);\n\tvfio_ap_mdev_unlink_domain(matrix_mdev, apqi, qtable);\n\n\tif (test_bit_inv(apqi, matrix_mdev->shadow_apcb.aqm)) {\n\t\tclear_bit_inv(apqi, matrix_mdev->shadow_apcb.aqm);\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\t}\n\n\tvfio_ap_mdev_reset_queues(qtable);\n\n\thash_for_each(qtable->queues, loop_cursor, q, mdev_qnode) {\n\t\tvfio_ap_unlink_mdev_fr_queue(q);\n\t\thash_del(&q->mdev_qnode);\n\t}\n\n\tkfree(qtable);\n}\n\n \nstatic ssize_t unassign_domain_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long apqi;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &apqi);\n\tif (ret)\n\t\tgoto done;\n\n\tif (apqi > matrix_mdev->matrix.aqm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (!test_bit_inv(apqi, matrix_mdev->matrix.aqm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\tclear_bit_inv((unsigned long)apqi, matrix_mdev->matrix.aqm);\n\tvfio_ap_mdev_hot_unplug_domain(matrix_mdev, apqi);\n\tret = count;\n\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(unassign_domain);\n\n \nstatic ssize_t assign_control_domain_store(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long id;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &id);\n\tif (ret)\n\t\tgoto done;\n\n\tif (id > matrix_mdev->matrix.adm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (test_bit_inv(id, matrix_mdev->matrix.adm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\t \n\tset_bit_inv(id, matrix_mdev->matrix.adm);\n\tif (vfio_ap_mdev_filter_cdoms(matrix_mdev))\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\n\tret = count;\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(assign_control_domain);\n\n \nstatic ssize_t unassign_control_domain_store(struct device *dev,\n\t\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t\t     const char *buf, size_t count)\n{\n\tint ret;\n\tunsigned long domid;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tget_update_locks_for_mdev(matrix_mdev);\n\n\tret = kstrtoul(buf, 0, &domid);\n\tif (ret)\n\t\tgoto done;\n\n\tif (domid > matrix_mdev->matrix.adm_max) {\n\t\tret = -ENODEV;\n\t\tgoto done;\n\t}\n\n\tif (!test_bit_inv(domid, matrix_mdev->matrix.adm)) {\n\t\tret = count;\n\t\tgoto done;\n\t}\n\n\tclear_bit_inv(domid, matrix_mdev->matrix.adm);\n\n\tif (test_bit_inv(domid, matrix_mdev->shadow_apcb.adm)) {\n\t\tclear_bit_inv(domid, matrix_mdev->shadow_apcb.adm);\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\t}\n\n\tret = count;\ndone:\n\trelease_update_locks_for_mdev(matrix_mdev);\n\treturn ret;\n}\nstatic DEVICE_ATTR_WO(unassign_control_domain);\n\nstatic ssize_t control_domains_show(struct device *dev,\n\t\t\t\t    struct device_attribute *dev_attr,\n\t\t\t\t    char *buf)\n{\n\tunsigned long id;\n\tint nchars = 0;\n\tint n;\n\tchar *bufpos = buf;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\tunsigned long max_domid = matrix_mdev->matrix.adm_max;\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tfor_each_set_bit_inv(id, matrix_mdev->matrix.adm, max_domid + 1) {\n\t\tn = sprintf(bufpos, \"%04lx\\n\", id);\n\t\tbufpos += n;\n\t\tnchars += n;\n\t}\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\n\treturn nchars;\n}\nstatic DEVICE_ATTR_RO(control_domains);\n\nstatic ssize_t vfio_ap_mdev_matrix_show(struct ap_matrix *matrix, char *buf)\n{\n\tchar *bufpos = buf;\n\tunsigned long apid;\n\tunsigned long apqi;\n\tunsigned long apid1;\n\tunsigned long apqi1;\n\tunsigned long napm_bits = matrix->apm_max + 1;\n\tunsigned long naqm_bits = matrix->aqm_max + 1;\n\tint nchars = 0;\n\tint n;\n\n\tapid1 = find_first_bit_inv(matrix->apm, napm_bits);\n\tapqi1 = find_first_bit_inv(matrix->aqm, naqm_bits);\n\n\tif ((apid1 < napm_bits) && (apqi1 < naqm_bits)) {\n\t\tfor_each_set_bit_inv(apid, matrix->apm, napm_bits) {\n\t\t\tfor_each_set_bit_inv(apqi, matrix->aqm,\n\t\t\t\t\t     naqm_bits) {\n\t\t\t\tn = sprintf(bufpos, \"%02lx.%04lx\\n\", apid,\n\t\t\t\t\t    apqi);\n\t\t\t\tbufpos += n;\n\t\t\t\tnchars += n;\n\t\t\t}\n\t\t}\n\t} else if (apid1 < napm_bits) {\n\t\tfor_each_set_bit_inv(apid, matrix->apm, napm_bits) {\n\t\t\tn = sprintf(bufpos, \"%02lx.\\n\", apid);\n\t\t\tbufpos += n;\n\t\t\tnchars += n;\n\t\t}\n\t} else if (apqi1 < naqm_bits) {\n\t\tfor_each_set_bit_inv(apqi, matrix->aqm, naqm_bits) {\n\t\t\tn = sprintf(bufpos, \".%04lx\\n\", apqi);\n\t\t\tbufpos += n;\n\t\t\tnchars += n;\n\t\t}\n\t}\n\n\treturn nchars;\n}\n\nstatic ssize_t matrix_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tssize_t nchars;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tnchars = vfio_ap_mdev_matrix_show(&matrix_mdev->matrix, buf);\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\n\treturn nchars;\n}\nstatic DEVICE_ATTR_RO(matrix);\n\nstatic ssize_t guest_matrix_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tssize_t nchars;\n\tstruct ap_matrix_mdev *matrix_mdev = dev_get_drvdata(dev);\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tnchars = vfio_ap_mdev_matrix_show(&matrix_mdev->shadow_apcb, buf);\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\n\treturn nchars;\n}\nstatic DEVICE_ATTR_RO(guest_matrix);\n\nstatic struct attribute *vfio_ap_mdev_attrs[] = {\n\t&dev_attr_assign_adapter.attr,\n\t&dev_attr_unassign_adapter.attr,\n\t&dev_attr_assign_domain.attr,\n\t&dev_attr_unassign_domain.attr,\n\t&dev_attr_assign_control_domain.attr,\n\t&dev_attr_unassign_control_domain.attr,\n\t&dev_attr_control_domains.attr,\n\t&dev_attr_matrix.attr,\n\t&dev_attr_guest_matrix.attr,\n\tNULL,\n};\n\nstatic struct attribute_group vfio_ap_mdev_attr_group = {\n\t.attrs = vfio_ap_mdev_attrs\n};\n\nstatic const struct attribute_group *vfio_ap_mdev_attr_groups[] = {\n\t&vfio_ap_mdev_attr_group,\n\tNULL\n};\n\n \nstatic int vfio_ap_mdev_set_kvm(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\tstruct kvm *kvm)\n{\n\tstruct ap_matrix_mdev *m;\n\n\tif (kvm->arch.crypto.crycbd) {\n\t\tdown_write(&kvm->arch.crypto.pqap_hook_rwsem);\n\t\tkvm->arch.crypto.pqap_hook = &matrix_mdev->pqap_hook;\n\t\tup_write(&kvm->arch.crypto.pqap_hook_rwsem);\n\n\t\tget_update_locks_for_kvm(kvm);\n\n\t\tlist_for_each_entry(m, &matrix_dev->mdev_list, node) {\n\t\t\tif (m != matrix_mdev && m->kvm == kvm) {\n\t\t\t\trelease_update_locks_for_kvm(kvm);\n\t\t\t\treturn -EPERM;\n\t\t\t}\n\t\t}\n\n\t\tkvm_get_kvm(kvm);\n\t\tmatrix_mdev->kvm = kvm;\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\n\t\trelease_update_locks_for_kvm(kvm);\n\t}\n\n\treturn 0;\n}\n\nstatic void unmap_iova(struct ap_matrix_mdev *matrix_mdev, u64 iova, u64 length)\n{\n\tstruct ap_queue_table *qtable = &matrix_mdev->qtable;\n\tstruct vfio_ap_queue *q;\n\tint loop_cursor;\n\n\thash_for_each(qtable->queues, loop_cursor, q, mdev_qnode) {\n\t\tif (q->saved_iova >= iova && q->saved_iova < iova + length)\n\t\t\tvfio_ap_irq_disable(q);\n\t}\n}\n\nstatic void vfio_ap_mdev_dma_unmap(struct vfio_device *vdev, u64 iova,\n\t\t\t\t   u64 length)\n{\n\tstruct ap_matrix_mdev *matrix_mdev =\n\t\tcontainer_of(vdev, struct ap_matrix_mdev, vdev);\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\tunmap_iova(matrix_mdev, iova, length);\n\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n}\n\n \nstatic void vfio_ap_mdev_unset_kvm(struct ap_matrix_mdev *matrix_mdev)\n{\n\tstruct kvm *kvm = matrix_mdev->kvm;\n\n\tif (kvm && kvm->arch.crypto.crycbd) {\n\t\tdown_write(&kvm->arch.crypto.pqap_hook_rwsem);\n\t\tkvm->arch.crypto.pqap_hook = NULL;\n\t\tup_write(&kvm->arch.crypto.pqap_hook_rwsem);\n\n\t\tget_update_locks_for_kvm(kvm);\n\n\t\tkvm_arch_crypto_clear_masks(kvm);\n\t\tvfio_ap_mdev_reset_queues(&matrix_mdev->qtable);\n\t\tkvm_put_kvm(kvm);\n\t\tmatrix_mdev->kvm = NULL;\n\n\t\trelease_update_locks_for_kvm(kvm);\n\t}\n}\n\nstatic struct vfio_ap_queue *vfio_ap_find_queue(int apqn)\n{\n\tstruct ap_queue *queue;\n\tstruct vfio_ap_queue *q = NULL;\n\n\tqueue = ap_get_qdev(apqn);\n\tif (!queue)\n\t\treturn NULL;\n\n\tif (queue->ap_dev.device.driver == &matrix_dev->vfio_ap_drv->driver)\n\t\tq = dev_get_drvdata(&queue->ap_dev.device);\n\n\tput_device(&queue->ap_dev.device);\n\n\treturn q;\n}\n\nstatic int apq_status_check(int apqn, struct ap_queue_status *status)\n{\n\tswitch (status->response_code) {\n\tcase AP_RESPONSE_NORMAL:\n\tcase AP_RESPONSE_DECONFIGURED:\n\t\treturn 0;\n\tcase AP_RESPONSE_RESET_IN_PROGRESS:\n\tcase AP_RESPONSE_BUSY:\n\t\treturn -EBUSY;\n\tcase AP_RESPONSE_ASSOC_SECRET_NOT_UNIQUE:\n\tcase AP_RESPONSE_ASSOC_FAILED:\n\t\t \n\t\treturn -EAGAIN;\n\tdefault:\n\t\tWARN(true,\n\t\t     \"failed to verify reset of queue %02x.%04x: TAPQ rc=%u\\n\",\n\t\t     AP_QID_CARD(apqn), AP_QID_QUEUE(apqn),\n\t\t     status->response_code);\n\t\treturn -EIO;\n\t}\n}\n\n#define WAIT_MSG \"Waited %dms for reset of queue %02x.%04x (%u, %u, %u)\"\n\nstatic void apq_reset_check(struct work_struct *reset_work)\n{\n\tint ret = -EBUSY, elapsed = 0;\n\tstruct ap_queue_status status;\n\tstruct vfio_ap_queue *q;\n\n\tq = container_of(reset_work, struct vfio_ap_queue, reset_work);\n\tmemcpy(&status, &q->reset_status, sizeof(status));\n\twhile (true) {\n\t\tmsleep(AP_RESET_INTERVAL);\n\t\telapsed += AP_RESET_INTERVAL;\n\t\tstatus = ap_tapq(q->apqn, NULL);\n\t\tret = apq_status_check(q->apqn, &status);\n\t\tif (ret == -EIO)\n\t\t\treturn;\n\t\tif (ret == -EBUSY) {\n\t\t\tpr_notice_ratelimited(WAIT_MSG, elapsed,\n\t\t\t\t\t      AP_QID_CARD(q->apqn),\n\t\t\t\t\t      AP_QID_QUEUE(q->apqn),\n\t\t\t\t\t      status.response_code,\n\t\t\t\t\t      status.queue_empty,\n\t\t\t\t\t      status.irq_enabled);\n\t\t} else {\n\t\t\tif (q->reset_status.response_code == AP_RESPONSE_RESET_IN_PROGRESS ||\n\t\t\t    q->reset_status.response_code == AP_RESPONSE_BUSY ||\n\t\t\t    q->reset_status.response_code == AP_RESPONSE_STATE_CHANGE_IN_PROGRESS ||\n\t\t\t    ret == -EAGAIN) {\n\t\t\t\tstatus = ap_zapq(q->apqn, 0);\n\t\t\t\tmemcpy(&q->reset_status, &status, sizeof(status));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tif (status.response_code == AP_RESPONSE_DECONFIGURED)\n\t\t\t\tq->reset_status.response_code = 0;\n\t\t\tif (q->saved_isc != VFIO_AP_ISC_INVALID)\n\t\t\t\tvfio_ap_free_aqic_resources(q);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void vfio_ap_mdev_reset_queue(struct vfio_ap_queue *q)\n{\n\tstruct ap_queue_status status;\n\n\tif (!q)\n\t\treturn;\n\tstatus = ap_zapq(q->apqn, 0);\n\tmemcpy(&q->reset_status, &status, sizeof(status));\n\tswitch (status.response_code) {\n\tcase AP_RESPONSE_NORMAL:\n\tcase AP_RESPONSE_RESET_IN_PROGRESS:\n\tcase AP_RESPONSE_BUSY:\n\tcase AP_RESPONSE_STATE_CHANGE_IN_PROGRESS:\n\t\t \n\t\tqueue_work(system_long_wq, &q->reset_work);\n\t\tbreak;\n\tcase AP_RESPONSE_DECONFIGURED:\n\t\t \n\t\tq->reset_status.response_code = 0;\n\t\tvfio_ap_free_aqic_resources(q);\n\t\tbreak;\n\tdefault:\n\t\tWARN(true,\n\t\t     \"PQAP/ZAPQ for %02x.%04x failed with invalid rc=%u\\n\",\n\t\t     AP_QID_CARD(q->apqn), AP_QID_QUEUE(q->apqn),\n\t\t     status.response_code);\n\t}\n}\n\nstatic int vfio_ap_mdev_reset_queues(struct ap_queue_table *qtable)\n{\n\tint ret = 0, loop_cursor;\n\tstruct vfio_ap_queue *q;\n\n\thash_for_each(qtable->queues, loop_cursor, q, mdev_qnode)\n\t\tvfio_ap_mdev_reset_queue(q);\n\n\thash_for_each(qtable->queues, loop_cursor, q, mdev_qnode) {\n\t\tflush_work(&q->reset_work);\n\n\t\tif (q->reset_status.response_code)\n\t\t\tret = -EIO;\n\t}\n\n\treturn ret;\n}\n\nstatic int vfio_ap_mdev_open_device(struct vfio_device *vdev)\n{\n\tstruct ap_matrix_mdev *matrix_mdev =\n\t\tcontainer_of(vdev, struct ap_matrix_mdev, vdev);\n\n\tif (!vdev->kvm)\n\t\treturn -EINVAL;\n\n\treturn vfio_ap_mdev_set_kvm(matrix_mdev, vdev->kvm);\n}\n\nstatic void vfio_ap_mdev_close_device(struct vfio_device *vdev)\n{\n\tstruct ap_matrix_mdev *matrix_mdev =\n\t\tcontainer_of(vdev, struct ap_matrix_mdev, vdev);\n\n\tvfio_ap_mdev_unset_kvm(matrix_mdev);\n}\n\nstatic void vfio_ap_mdev_request(struct vfio_device *vdev, unsigned int count)\n{\n\tstruct device *dev = vdev->dev;\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tmatrix_mdev = container_of(vdev, struct ap_matrix_mdev, vdev);\n\n\tif (matrix_mdev->req_trigger) {\n\t\tif (!(count % 10))\n\t\t\tdev_notice_ratelimited(dev,\n\t\t\t\t\t       \"Relaying device request to user (#%u)\\n\",\n\t\t\t\t\t       count);\n\n\t\teventfd_signal(matrix_mdev->req_trigger, 1);\n\t} else if (count == 0) {\n\t\tdev_notice(dev,\n\t\t\t   \"No device request registered, blocked until released by user\\n\");\n\t}\n}\n\nstatic int vfio_ap_mdev_get_device_info(unsigned long arg)\n{\n\tunsigned long minsz;\n\tstruct vfio_device_info info;\n\n\tminsz = offsetofend(struct vfio_device_info, num_irqs);\n\n\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tinfo.flags = VFIO_DEVICE_FLAGS_AP | VFIO_DEVICE_FLAGS_RESET;\n\tinfo.num_regions = 0;\n\tinfo.num_irqs = VFIO_AP_NUM_IRQS;\n\n\treturn copy_to_user((void __user *)arg, &info, minsz) ? -EFAULT : 0;\n}\n\nstatic ssize_t vfio_ap_get_irq_info(unsigned long arg)\n{\n\tunsigned long minsz;\n\tstruct vfio_irq_info info;\n\n\tminsz = offsetofend(struct vfio_irq_info, count);\n\n\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz || info.index >= VFIO_AP_NUM_IRQS)\n\t\treturn -EINVAL;\n\n\tswitch (info.index) {\n\tcase VFIO_AP_REQ_IRQ_INDEX:\n\t\tinfo.count = 1;\n\t\tinfo.flags = VFIO_IRQ_INFO_EVENTFD;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn copy_to_user((void __user *)arg, &info, minsz) ? -EFAULT : 0;\n}\n\nstatic int vfio_ap_irq_set_init(struct vfio_irq_set *irq_set, unsigned long arg)\n{\n\tint ret;\n\tsize_t data_size;\n\tunsigned long minsz;\n\n\tminsz = offsetofend(struct vfio_irq_set, count);\n\n\tif (copy_from_user(irq_set, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tret = vfio_set_irqs_validate_and_prepare(irq_set, 1, VFIO_AP_NUM_IRQS,\n\t\t\t\t\t\t &data_size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(irq_set->flags & VFIO_IRQ_SET_ACTION_TRIGGER))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int vfio_ap_set_request_irq(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t   unsigned long arg)\n{\n\ts32 fd;\n\tvoid __user *data;\n\tunsigned long minsz;\n\tstruct eventfd_ctx *req_trigger;\n\n\tminsz = offsetofend(struct vfio_irq_set, count);\n\tdata = (void __user *)(arg + minsz);\n\n\tif (get_user(fd, (s32 __user *)data))\n\t\treturn -EFAULT;\n\n\tif (fd == -1) {\n\t\tif (matrix_mdev->req_trigger)\n\t\t\teventfd_ctx_put(matrix_mdev->req_trigger);\n\t\tmatrix_mdev->req_trigger = NULL;\n\t} else if (fd >= 0) {\n\t\treq_trigger = eventfd_ctx_fdget(fd);\n\t\tif (IS_ERR(req_trigger))\n\t\t\treturn PTR_ERR(req_trigger);\n\n\t\tif (matrix_mdev->req_trigger)\n\t\t\teventfd_ctx_put(matrix_mdev->req_trigger);\n\n\t\tmatrix_mdev->req_trigger = req_trigger;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_ap_set_irqs(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct vfio_irq_set irq_set;\n\n\tret = vfio_ap_irq_set_init(&irq_set, arg);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (irq_set.flags & VFIO_IRQ_SET_DATA_TYPE_MASK) {\n\tcase VFIO_IRQ_SET_DATA_EVENTFD:\n\t\tswitch (irq_set.index) {\n\t\tcase VFIO_AP_REQ_IRQ_INDEX:\n\t\t\treturn vfio_ap_set_request_irq(matrix_mdev, arg);\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic ssize_t vfio_ap_mdev_ioctl(struct vfio_device *vdev,\n\t\t\t\t    unsigned int cmd, unsigned long arg)\n{\n\tstruct ap_matrix_mdev *matrix_mdev =\n\t\tcontainer_of(vdev, struct ap_matrix_mdev, vdev);\n\tint ret;\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tswitch (cmd) {\n\tcase VFIO_DEVICE_GET_INFO:\n\t\tret = vfio_ap_mdev_get_device_info(arg);\n\t\tbreak;\n\tcase VFIO_DEVICE_RESET:\n\t\tret = vfio_ap_mdev_reset_queues(&matrix_mdev->qtable);\n\t\tbreak;\n\tcase VFIO_DEVICE_GET_IRQ_INFO:\n\t\t\tret = vfio_ap_get_irq_info(arg);\n\t\t\tbreak;\n\tcase VFIO_DEVICE_SET_IRQS:\n\t\tret = vfio_ap_set_irqs(matrix_mdev, arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\t}\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\n\treturn ret;\n}\n\nstatic struct ap_matrix_mdev *vfio_ap_mdev_for_queue(struct vfio_ap_queue *q)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\tunsigned long apid = AP_QID_CARD(q->apqn);\n\tunsigned long apqi = AP_QID_QUEUE(q->apqn);\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\tif (test_bit_inv(apid, matrix_mdev->matrix.apm) &&\n\t\t    test_bit_inv(apqi, matrix_mdev->matrix.aqm))\n\t\t\treturn matrix_mdev;\n\t}\n\n\treturn NULL;\n}\n\nstatic ssize_t status_show(struct device *dev,\n\t\t\t   struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tssize_t nchars = 0;\n\tstruct vfio_ap_queue *q;\n\tstruct ap_matrix_mdev *matrix_mdev;\n\tstruct ap_device *apdev = to_ap_dev(dev);\n\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tq = dev_get_drvdata(&apdev->device);\n\tmatrix_mdev = vfio_ap_mdev_for_queue(q);\n\n\tif (matrix_mdev) {\n\t\tif (matrix_mdev->kvm)\n\t\t\tnchars = scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t\t\t   AP_QUEUE_IN_USE);\n\t\telse\n\t\t\tnchars = scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t\t\t   AP_QUEUE_ASSIGNED);\n\t} else {\n\t\tnchars = scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t\t   AP_QUEUE_UNASSIGNED);\n\t}\n\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\n\treturn nchars;\n}\n\nstatic DEVICE_ATTR_RO(status);\n\nstatic struct attribute *vfio_queue_attrs[] = {\n\t&dev_attr_status.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group vfio_queue_attr_group = {\n\t.attrs = vfio_queue_attrs,\n};\n\nstatic const struct vfio_device_ops vfio_ap_matrix_dev_ops = {\n\t.init = vfio_ap_mdev_init_dev,\n\t.open_device = vfio_ap_mdev_open_device,\n\t.close_device = vfio_ap_mdev_close_device,\n\t.ioctl = vfio_ap_mdev_ioctl,\n\t.dma_unmap = vfio_ap_mdev_dma_unmap,\n\t.bind_iommufd = vfio_iommufd_emulated_bind,\n\t.unbind_iommufd = vfio_iommufd_emulated_unbind,\n\t.attach_ioas = vfio_iommufd_emulated_attach_ioas,\n\t.detach_ioas = vfio_iommufd_emulated_detach_ioas,\n\t.request = vfio_ap_mdev_request\n};\n\nstatic struct mdev_driver vfio_ap_matrix_driver = {\n\t.device_api = VFIO_DEVICE_API_AP_STRING,\n\t.max_instances = MAX_ZDEV_ENTRIES_EXT,\n\t.driver = {\n\t\t.name = \"vfio_ap_mdev\",\n\t\t.owner = THIS_MODULE,\n\t\t.mod_name = KBUILD_MODNAME,\n\t\t.dev_groups = vfio_ap_mdev_attr_groups,\n\t},\n\t.probe = vfio_ap_mdev_probe,\n\t.remove = vfio_ap_mdev_remove,\n};\n\nint vfio_ap_mdev_register(void)\n{\n\tint ret;\n\n\tret = mdev_register_driver(&vfio_ap_matrix_driver);\n\tif (ret)\n\t\treturn ret;\n\n\tmatrix_dev->mdev_type.sysfs_name = VFIO_AP_MDEV_TYPE_HWVIRT;\n\tmatrix_dev->mdev_type.pretty_name = VFIO_AP_MDEV_NAME_HWVIRT;\n\tmatrix_dev->mdev_types[0] = &matrix_dev->mdev_type;\n\tret = mdev_register_parent(&matrix_dev->parent, &matrix_dev->device,\n\t\t\t\t   &vfio_ap_matrix_driver,\n\t\t\t\t   matrix_dev->mdev_types, 1);\n\tif (ret)\n\t\tgoto err_driver;\n\treturn 0;\n\nerr_driver:\n\tmdev_unregister_driver(&vfio_ap_matrix_driver);\n\treturn ret;\n}\n\nvoid vfio_ap_mdev_unregister(void)\n{\n\tmdev_unregister_parent(&matrix_dev->parent);\n\tmdev_unregister_driver(&vfio_ap_matrix_driver);\n}\n\nint vfio_ap_mdev_probe_queue(struct ap_device *apdev)\n{\n\tint ret;\n\tstruct vfio_ap_queue *q;\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tret = sysfs_create_group(&apdev->device.kobj, &vfio_queue_attr_group);\n\tif (ret)\n\t\treturn ret;\n\n\tq = kzalloc(sizeof(*q), GFP_KERNEL);\n\tif (!q) {\n\t\tret = -ENOMEM;\n\t\tgoto err_remove_group;\n\t}\n\n\tq->apqn = to_ap_queue(&apdev->device)->qid;\n\tq->saved_isc = VFIO_AP_ISC_INVALID;\n\tmemset(&q->reset_status, 0, sizeof(q->reset_status));\n\tINIT_WORK(&q->reset_work, apq_reset_check);\n\tmatrix_mdev = get_update_locks_by_apqn(q->apqn);\n\n\tif (matrix_mdev) {\n\t\tvfio_ap_mdev_link_queue(matrix_mdev, q);\n\n\t\tif (vfio_ap_mdev_filter_matrix(matrix_mdev->matrix.apm,\n\t\t\t\t\t       matrix_mdev->matrix.aqm,\n\t\t\t\t\t       matrix_mdev))\n\t\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\t}\n\tdev_set_drvdata(&apdev->device, q);\n\trelease_update_locks_for_mdev(matrix_mdev);\n\n\treturn 0;\n\nerr_remove_group:\n\tsysfs_remove_group(&apdev->device.kobj, &vfio_queue_attr_group);\n\treturn ret;\n}\n\nvoid vfio_ap_mdev_remove_queue(struct ap_device *apdev)\n{\n\tunsigned long apid, apqi;\n\tstruct vfio_ap_queue *q;\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tsysfs_remove_group(&apdev->device.kobj, &vfio_queue_attr_group);\n\tq = dev_get_drvdata(&apdev->device);\n\tget_update_locks_for_queue(q);\n\tmatrix_mdev = q->matrix_mdev;\n\n\tif (matrix_mdev) {\n\t\tvfio_ap_unlink_queue_fr_mdev(q);\n\n\t\tapid = AP_QID_CARD(q->apqn);\n\t\tapqi = AP_QID_QUEUE(q->apqn);\n\n\t\t \n\t\tif (test_bit_inv(apid, matrix_mdev->shadow_apcb.apm) &&\n\t\t    test_bit_inv(apqi, matrix_mdev->shadow_apcb.aqm)) {\n\t\t\tclear_bit_inv(apid, matrix_mdev->shadow_apcb.apm);\n\t\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\t\t}\n\t}\n\n\tvfio_ap_mdev_reset_queue(q);\n\tflush_work(&q->reset_work);\n\tdev_set_drvdata(&apdev->device, NULL);\n\tkfree(q);\n\trelease_update_locks_for_mdev(matrix_mdev);\n}\n\n \nint vfio_ap_mdev_resource_in_use(unsigned long *apm, unsigned long *aqm)\n{\n\tint ret;\n\n\tmutex_lock(&matrix_dev->guests_lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\tret = vfio_ap_mdev_verify_no_sharing(apm, aqm);\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\tmutex_unlock(&matrix_dev->guests_lock);\n\n\treturn ret;\n}\n\n \nstatic void vfio_ap_mdev_hot_unplug_cfg(struct ap_matrix_mdev *matrix_mdev,\n\t\t\t\t\tunsigned long *aprem,\n\t\t\t\t\tunsigned long *aqrem,\n\t\t\t\t\tunsigned long *cdrem)\n{\n\tint do_hotplug = 0;\n\n\tif (!bitmap_empty(aprem, AP_DEVICES)) {\n\t\tdo_hotplug |= bitmap_andnot(matrix_mdev->shadow_apcb.apm,\n\t\t\t\t\t    matrix_mdev->shadow_apcb.apm,\n\t\t\t\t\t    aprem, AP_DEVICES);\n\t}\n\n\tif (!bitmap_empty(aqrem, AP_DOMAINS)) {\n\t\tdo_hotplug |= bitmap_andnot(matrix_mdev->shadow_apcb.aqm,\n\t\t\t\t\t    matrix_mdev->shadow_apcb.aqm,\n\t\t\t\t\t    aqrem, AP_DEVICES);\n\t}\n\n\tif (!bitmap_empty(cdrem, AP_DOMAINS))\n\t\tdo_hotplug |= bitmap_andnot(matrix_mdev->shadow_apcb.adm,\n\t\t\t\t\t    matrix_mdev->shadow_apcb.adm,\n\t\t\t\t\t    cdrem, AP_DOMAINS);\n\n\tif (do_hotplug)\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n}\n\n \nstatic void vfio_ap_mdev_cfg_remove(unsigned long *ap_remove,\n\t\t\t\t    unsigned long *aq_remove,\n\t\t\t\t    unsigned long *cd_remove)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\tDECLARE_BITMAP(aprem, AP_DEVICES);\n\tDECLARE_BITMAP(aqrem, AP_DOMAINS);\n\tDECLARE_BITMAP(cdrem, AP_DOMAINS);\n\tint do_remove = 0;\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\tmutex_lock(&matrix_mdev->kvm->lock);\n\t\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\t\tdo_remove |= bitmap_and(aprem, ap_remove,\n\t\t\t\t\t  matrix_mdev->matrix.apm,\n\t\t\t\t\t  AP_DEVICES);\n\t\tdo_remove |= bitmap_and(aqrem, aq_remove,\n\t\t\t\t\t  matrix_mdev->matrix.aqm,\n\t\t\t\t\t  AP_DOMAINS);\n\t\tdo_remove |= bitmap_andnot(cdrem, cd_remove,\n\t\t\t\t\t     matrix_mdev->matrix.adm,\n\t\t\t\t\t     AP_DOMAINS);\n\n\t\tif (do_remove)\n\t\t\tvfio_ap_mdev_hot_unplug_cfg(matrix_mdev, aprem, aqrem,\n\t\t\t\t\t\t    cdrem);\n\n\t\tmutex_unlock(&matrix_dev->mdevs_lock);\n\t\tmutex_unlock(&matrix_mdev->kvm->lock);\n\t}\n}\n\n \nstatic void vfio_ap_mdev_on_cfg_remove(struct ap_config_info *cur_config_info,\n\t\t\t\t       struct ap_config_info *prev_config_info)\n{\n\tint do_remove;\n\tDECLARE_BITMAP(aprem, AP_DEVICES);\n\tDECLARE_BITMAP(aqrem, AP_DOMAINS);\n\tDECLARE_BITMAP(cdrem, AP_DOMAINS);\n\n\tdo_remove = bitmap_andnot(aprem,\n\t\t\t\t  (unsigned long *)prev_config_info->apm,\n\t\t\t\t  (unsigned long *)cur_config_info->apm,\n\t\t\t\t  AP_DEVICES);\n\tdo_remove |= bitmap_andnot(aqrem,\n\t\t\t\t   (unsigned long *)prev_config_info->aqm,\n\t\t\t\t   (unsigned long *)cur_config_info->aqm,\n\t\t\t\t   AP_DEVICES);\n\tdo_remove |= bitmap_andnot(cdrem,\n\t\t\t\t   (unsigned long *)prev_config_info->adm,\n\t\t\t\t   (unsigned long *)cur_config_info->adm,\n\t\t\t\t   AP_DEVICES);\n\n\tif (do_remove)\n\t\tvfio_ap_mdev_cfg_remove(aprem, aqrem, cdrem);\n}\n\n \nstatic void vfio_ap_filter_apid_by_qtype(unsigned long *apm, unsigned long *aqm)\n{\n\tbool apid_cleared;\n\tstruct ap_queue_status status;\n\tunsigned long apid, apqi;\n\tstruct ap_tapq_gr2 info;\n\n\tfor_each_set_bit_inv(apid, apm, AP_DEVICES) {\n\t\tapid_cleared = false;\n\n\t\tfor_each_set_bit_inv(apqi, aqm, AP_DOMAINS) {\n\t\t\tstatus = ap_test_queue(AP_MKQID(apid, apqi), 1, &info);\n\t\t\tswitch (status.response_code) {\n\t\t\t \n\t\t\tcase AP_RESPONSE_NORMAL:\n\t\t\tcase AP_RESPONSE_RESET_IN_PROGRESS:\n\t\t\tcase AP_RESPONSE_DECONFIGURED:\n\t\t\tcase AP_RESPONSE_CHECKSTOPPED:\n\t\t\tcase AP_RESPONSE_BUSY:\n\t\t\t\t \n\t\t\t\tif (info.at < AP_DEVICE_TYPE_CEX4) {\n\t\t\t\t\tclear_bit_inv(apid, apm);\n\t\t\t\t\tapid_cleared = true;\n\t\t\t\t}\n\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tclear_bit_inv(apid, apm);\n\t\t\t\tapid_cleared = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (apid_cleared)\n\t\t\t\tcontinue;\n\t\t}\n\t}\n}\n\n \nstatic void vfio_ap_mdev_cfg_add(unsigned long *apm_add, unsigned long *aqm_add,\n\t\t\t\t unsigned long *adm_add)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tif (list_empty(&matrix_dev->mdev_list))\n\t\treturn;\n\n\tvfio_ap_filter_apid_by_qtype(apm_add, aqm_add);\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\tbitmap_and(matrix_mdev->apm_add,\n\t\t\t   matrix_mdev->matrix.apm, apm_add, AP_DEVICES);\n\t\tbitmap_and(matrix_mdev->aqm_add,\n\t\t\t   matrix_mdev->matrix.aqm, aqm_add, AP_DOMAINS);\n\t\tbitmap_and(matrix_mdev->adm_add,\n\t\t\t   matrix_mdev->matrix.adm, adm_add, AP_DEVICES);\n\t}\n}\n\n \nstatic void vfio_ap_mdev_on_cfg_add(struct ap_config_info *cur_config_info,\n\t\t\t\t    struct ap_config_info *prev_config_info)\n{\n\tbool do_add;\n\tDECLARE_BITMAP(apm_add, AP_DEVICES);\n\tDECLARE_BITMAP(aqm_add, AP_DOMAINS);\n\tDECLARE_BITMAP(adm_add, AP_DOMAINS);\n\n\tdo_add = bitmap_andnot(apm_add,\n\t\t\t       (unsigned long *)cur_config_info->apm,\n\t\t\t       (unsigned long *)prev_config_info->apm,\n\t\t\t       AP_DEVICES);\n\tdo_add |= bitmap_andnot(aqm_add,\n\t\t\t\t(unsigned long *)cur_config_info->aqm,\n\t\t\t\t(unsigned long *)prev_config_info->aqm,\n\t\t\t\tAP_DOMAINS);\n\tdo_add |= bitmap_andnot(adm_add,\n\t\t\t\t(unsigned long *)cur_config_info->adm,\n\t\t\t\t(unsigned long *)prev_config_info->adm,\n\t\t\t\tAP_DOMAINS);\n\n\tif (do_add)\n\t\tvfio_ap_mdev_cfg_add(apm_add, aqm_add, adm_add);\n}\n\n \nvoid vfio_ap_on_cfg_changed(struct ap_config_info *cur_cfg_info,\n\t\t\t    struct ap_config_info *prev_cfg_info)\n{\n\tif (!cur_cfg_info || !prev_cfg_info)\n\t\treturn;\n\n\tmutex_lock(&matrix_dev->guests_lock);\n\n\tvfio_ap_mdev_on_cfg_remove(cur_cfg_info, prev_cfg_info);\n\tvfio_ap_mdev_on_cfg_add(cur_cfg_info, prev_cfg_info);\n\tmemcpy(&matrix_dev->info, cur_cfg_info, sizeof(*cur_cfg_info));\n\n\tmutex_unlock(&matrix_dev->guests_lock);\n}\n\nstatic void vfio_ap_mdev_hot_plug_cfg(struct ap_matrix_mdev *matrix_mdev)\n{\n\tbool do_hotplug = false;\n\tint filter_domains = 0;\n\tint filter_adapters = 0;\n\tDECLARE_BITMAP(apm, AP_DEVICES);\n\tDECLARE_BITMAP(aqm, AP_DOMAINS);\n\n\tmutex_lock(&matrix_mdev->kvm->lock);\n\tmutex_lock(&matrix_dev->mdevs_lock);\n\n\tfilter_adapters = bitmap_and(apm, matrix_mdev->matrix.apm,\n\t\t\t\t     matrix_mdev->apm_add, AP_DEVICES);\n\tfilter_domains = bitmap_and(aqm, matrix_mdev->matrix.aqm,\n\t\t\t\t    matrix_mdev->aqm_add, AP_DOMAINS);\n\n\tif (filter_adapters && filter_domains)\n\t\tdo_hotplug |= vfio_ap_mdev_filter_matrix(apm, aqm, matrix_mdev);\n\telse if (filter_adapters)\n\t\tdo_hotplug |=\n\t\t\tvfio_ap_mdev_filter_matrix(apm,\n\t\t\t\t\t\t   matrix_mdev->shadow_apcb.aqm,\n\t\t\t\t\t\t   matrix_mdev);\n\telse\n\t\tdo_hotplug |=\n\t\t\tvfio_ap_mdev_filter_matrix(matrix_mdev->shadow_apcb.apm,\n\t\t\t\t\t\t   aqm, matrix_mdev);\n\n\tif (bitmap_intersects(matrix_mdev->matrix.adm, matrix_mdev->adm_add,\n\t\t\t      AP_DOMAINS))\n\t\tdo_hotplug |= vfio_ap_mdev_filter_cdoms(matrix_mdev);\n\n\tif (do_hotplug)\n\t\tvfio_ap_mdev_update_guest_apcb(matrix_mdev);\n\n\tmutex_unlock(&matrix_dev->mdevs_lock);\n\tmutex_unlock(&matrix_mdev->kvm->lock);\n}\n\nvoid vfio_ap_on_scan_complete(struct ap_config_info *new_config_info,\n\t\t\t      struct ap_config_info *old_config_info)\n{\n\tstruct ap_matrix_mdev *matrix_mdev;\n\n\tmutex_lock(&matrix_dev->guests_lock);\n\n\tlist_for_each_entry(matrix_mdev, &matrix_dev->mdev_list, node) {\n\t\tif (bitmap_empty(matrix_mdev->apm_add, AP_DEVICES) &&\n\t\t    bitmap_empty(matrix_mdev->aqm_add, AP_DOMAINS) &&\n\t\t    bitmap_empty(matrix_mdev->adm_add, AP_DOMAINS))\n\t\t\tcontinue;\n\n\t\tvfio_ap_mdev_hot_plug_cfg(matrix_mdev);\n\t\tbitmap_clear(matrix_mdev->apm_add, 0, AP_DEVICES);\n\t\tbitmap_clear(matrix_mdev->aqm_add, 0, AP_DOMAINS);\n\t\tbitmap_clear(matrix_mdev->adm_add, 0, AP_DOMAINS);\n\t}\n\n\tmutex_unlock(&matrix_dev->guests_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}