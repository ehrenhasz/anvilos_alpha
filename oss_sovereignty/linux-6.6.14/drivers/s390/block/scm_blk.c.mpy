{
  "module_name": "scm_blk.c",
  "hash_id": "874c8d26d1f72ac073b06d7a6253ead768cfa6c47bca0a1c021e0f2cfddde165",
  "original_prompt": "Ingested from linux-6.6.14/drivers/s390/block/scm_blk.c",
  "human_readable_source": "\n \n\n#define KMSG_COMPONENT \"scm_block\"\n#define pr_fmt(fmt) KMSG_COMPONENT \": \" fmt\n\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/blkdev.h>\n#include <linux/blk-mq.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/io.h>\n#include <asm/eadm.h>\n#include \"scm_blk.h\"\n\ndebug_info_t *scm_debug;\nstatic int scm_major;\nstatic mempool_t *aidaw_pool;\nstatic DEFINE_SPINLOCK(list_lock);\nstatic LIST_HEAD(inactive_requests);\nstatic unsigned int nr_requests = 64;\nstatic unsigned int nr_requests_per_io = 8;\nstatic atomic_t nr_devices = ATOMIC_INIT(0);\nmodule_param(nr_requests, uint, S_IRUGO);\nMODULE_PARM_DESC(nr_requests, \"Number of parallel requests.\");\n\nmodule_param(nr_requests_per_io, uint, S_IRUGO);\nMODULE_PARM_DESC(nr_requests_per_io, \"Number of requests per IO.\");\n\nMODULE_DESCRIPTION(\"Block driver for s390 storage class memory.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"scm:scmdev*\");\n\nstatic void __scm_free_rq(struct scm_request *scmrq)\n{\n\tstruct aob_rq_header *aobrq = to_aobrq(scmrq);\n\n\tfree_page((unsigned long) scmrq->aob);\n\tkfree(scmrq->request);\n\tkfree(aobrq);\n}\n\nstatic void scm_free_rqs(void)\n{\n\tstruct list_head *iter, *safe;\n\tstruct scm_request *scmrq;\n\n\tspin_lock_irq(&list_lock);\n\tlist_for_each_safe(iter, safe, &inactive_requests) {\n\t\tscmrq = list_entry(iter, struct scm_request, list);\n\t\tlist_del(&scmrq->list);\n\t\t__scm_free_rq(scmrq);\n\t}\n\tspin_unlock_irq(&list_lock);\n\n\tmempool_destroy(aidaw_pool);\n}\n\nstatic int __scm_alloc_rq(void)\n{\n\tstruct aob_rq_header *aobrq;\n\tstruct scm_request *scmrq;\n\n\taobrq = kzalloc(sizeof(*aobrq) + sizeof(*scmrq), GFP_KERNEL);\n\tif (!aobrq)\n\t\treturn -ENOMEM;\n\n\tscmrq = (void *) aobrq->data;\n\tscmrq->aob = (void *) get_zeroed_page(GFP_DMA);\n\tif (!scmrq->aob)\n\t\tgoto free;\n\n\tscmrq->request = kcalloc(nr_requests_per_io, sizeof(scmrq->request[0]),\n\t\t\t\t GFP_KERNEL);\n\tif (!scmrq->request)\n\t\tgoto free;\n\n\tINIT_LIST_HEAD(&scmrq->list);\n\tspin_lock_irq(&list_lock);\n\tlist_add(&scmrq->list, &inactive_requests);\n\tspin_unlock_irq(&list_lock);\n\n\treturn 0;\nfree:\n\t__scm_free_rq(scmrq);\n\treturn -ENOMEM;\n}\n\nstatic int scm_alloc_rqs(unsigned int nrqs)\n{\n\tint ret = 0;\n\n\taidaw_pool = mempool_create_page_pool(max(nrqs/8, 1U), 0);\n\tif (!aidaw_pool)\n\t\treturn -ENOMEM;\n\n\twhile (nrqs-- && !ret)\n\t\tret = __scm_alloc_rq();\n\n\treturn ret;\n}\n\nstatic struct scm_request *scm_request_fetch(void)\n{\n\tstruct scm_request *scmrq = NULL;\n\n\tspin_lock_irq(&list_lock);\n\tif (list_empty(&inactive_requests))\n\t\tgoto out;\n\tscmrq = list_first_entry(&inactive_requests, struct scm_request, list);\n\tlist_del(&scmrq->list);\nout:\n\tspin_unlock_irq(&list_lock);\n\treturn scmrq;\n}\n\nstatic void scm_request_done(struct scm_request *scmrq)\n{\n\tunsigned long flags;\n\tstruct msb *msb;\n\tu64 aidaw;\n\tint i;\n\n\tfor (i = 0; i < nr_requests_per_io && scmrq->request[i]; i++) {\n\t\tmsb = &scmrq->aob->msb[i];\n\t\taidaw = (u64)phys_to_virt(msb->data_addr);\n\n\t\tif ((msb->flags & MSB_FLAG_IDA) && aidaw &&\n\t\t    IS_ALIGNED(aidaw, PAGE_SIZE))\n\t\t\tmempool_free(virt_to_page((void *)aidaw), aidaw_pool);\n\t}\n\n\tspin_lock_irqsave(&list_lock, flags);\n\tlist_add(&scmrq->list, &inactive_requests);\n\tspin_unlock_irqrestore(&list_lock, flags);\n}\n\nstatic bool scm_permit_request(struct scm_blk_dev *bdev, struct request *req)\n{\n\treturn rq_data_dir(req) != WRITE || bdev->state != SCM_WR_PROHIBIT;\n}\n\nstatic inline struct aidaw *scm_aidaw_alloc(void)\n{\n\tstruct page *page = mempool_alloc(aidaw_pool, GFP_ATOMIC);\n\n\treturn page ? page_address(page) : NULL;\n}\n\nstatic inline unsigned long scm_aidaw_bytes(struct aidaw *aidaw)\n{\n\tunsigned long _aidaw = (unsigned long) aidaw;\n\tunsigned long bytes = ALIGN(_aidaw, PAGE_SIZE) - _aidaw;\n\n\treturn (bytes / sizeof(*aidaw)) * PAGE_SIZE;\n}\n\nstruct aidaw *scm_aidaw_fetch(struct scm_request *scmrq, unsigned int bytes)\n{\n\tstruct aidaw *aidaw;\n\n\tif (scm_aidaw_bytes(scmrq->next_aidaw) >= bytes)\n\t\treturn scmrq->next_aidaw;\n\n\taidaw = scm_aidaw_alloc();\n\tif (aidaw)\n\t\tmemset(aidaw, 0, PAGE_SIZE);\n\treturn aidaw;\n}\n\nstatic int scm_request_prepare(struct scm_request *scmrq)\n{\n\tstruct scm_blk_dev *bdev = scmrq->bdev;\n\tstruct scm_device *scmdev = bdev->gendisk->private_data;\n\tint pos = scmrq->aob->request.msb_count;\n\tstruct msb *msb = &scmrq->aob->msb[pos];\n\tstruct request *req = scmrq->request[pos];\n\tstruct req_iterator iter;\n\tstruct aidaw *aidaw;\n\tstruct bio_vec bv;\n\n\taidaw = scm_aidaw_fetch(scmrq, blk_rq_bytes(req));\n\tif (!aidaw)\n\t\treturn -ENOMEM;\n\n\tmsb->bs = MSB_BS_4K;\n\tscmrq->aob->request.msb_count++;\n\tmsb->scm_addr = scmdev->address + ((u64) blk_rq_pos(req) << 9);\n\tmsb->oc = (rq_data_dir(req) == READ) ? MSB_OC_READ : MSB_OC_WRITE;\n\tmsb->flags |= MSB_FLAG_IDA;\n\tmsb->data_addr = (u64)virt_to_phys(aidaw);\n\n\trq_for_each_segment(bv, req, iter) {\n\t\tWARN_ON(bv.bv_offset);\n\t\tmsb->blk_count += bv.bv_len >> 12;\n\t\taidaw->data_addr = virt_to_phys(page_address(bv.bv_page));\n\t\taidaw++;\n\t}\n\n\tscmrq->next_aidaw = aidaw;\n\treturn 0;\n}\n\nstatic inline void scm_request_set(struct scm_request *scmrq,\n\t\t\t\t   struct request *req)\n{\n\tscmrq->request[scmrq->aob->request.msb_count] = req;\n}\n\nstatic inline void scm_request_init(struct scm_blk_dev *bdev,\n\t\t\t\t    struct scm_request *scmrq)\n{\n\tstruct aob_rq_header *aobrq = to_aobrq(scmrq);\n\tstruct aob *aob = scmrq->aob;\n\n\tmemset(scmrq->request, 0,\n\t       nr_requests_per_io * sizeof(scmrq->request[0]));\n\tmemset(aob, 0, sizeof(*aob));\n\taobrq->scmdev = bdev->scmdev;\n\taob->request.cmd_code = ARQB_CMD_MOVE;\n\taob->request.data = (u64) aobrq;\n\tscmrq->bdev = bdev;\n\tscmrq->retries = 4;\n\tscmrq->error = BLK_STS_OK;\n\t \n\tscmrq->next_aidaw = (void *) &aob->msb[nr_requests_per_io];\n}\n\nstatic void scm_request_requeue(struct scm_request *scmrq)\n{\n\tstruct scm_blk_dev *bdev = scmrq->bdev;\n\tint i;\n\n\tfor (i = 0; i < nr_requests_per_io && scmrq->request[i]; i++)\n\t\tblk_mq_requeue_request(scmrq->request[i], false);\n\n\tatomic_dec(&bdev->queued_reqs);\n\tscm_request_done(scmrq);\n\tblk_mq_kick_requeue_list(bdev->rq);\n}\n\nstatic void scm_request_finish(struct scm_request *scmrq)\n{\n\tstruct scm_blk_dev *bdev = scmrq->bdev;\n\tblk_status_t *error;\n\tint i;\n\n\tfor (i = 0; i < nr_requests_per_io && scmrq->request[i]; i++) {\n\t\terror = blk_mq_rq_to_pdu(scmrq->request[i]);\n\t\t*error = scmrq->error;\n\t\tif (likely(!blk_should_fake_timeout(scmrq->request[i]->q)))\n\t\t\tblk_mq_complete_request(scmrq->request[i]);\n\t}\n\n\tatomic_dec(&bdev->queued_reqs);\n\tscm_request_done(scmrq);\n}\n\nstatic void scm_request_start(struct scm_request *scmrq)\n{\n\tstruct scm_blk_dev *bdev = scmrq->bdev;\n\n\tatomic_inc(&bdev->queued_reqs);\n\tif (eadm_start_aob(scmrq->aob)) {\n\t\tSCM_LOG(5, \"no subchannel\");\n\t\tscm_request_requeue(scmrq);\n\t}\n}\n\nstruct scm_queue {\n\tstruct scm_request *scmrq;\n\tspinlock_t lock;\n};\n\nstatic blk_status_t scm_blk_request(struct blk_mq_hw_ctx *hctx,\n\t\t\t   const struct blk_mq_queue_data *qd)\n{\n\tstruct scm_device *scmdev = hctx->queue->queuedata;\n\tstruct scm_blk_dev *bdev = dev_get_drvdata(&scmdev->dev);\n\tstruct scm_queue *sq = hctx->driver_data;\n\tstruct request *req = qd->rq;\n\tstruct scm_request *scmrq;\n\n\tspin_lock(&sq->lock);\n\tif (!scm_permit_request(bdev, req)) {\n\t\tspin_unlock(&sq->lock);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\tscmrq = sq->scmrq;\n\tif (!scmrq) {\n\t\tscmrq = scm_request_fetch();\n\t\tif (!scmrq) {\n\t\t\tSCM_LOG(5, \"no request\");\n\t\t\tspin_unlock(&sq->lock);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\t\tscm_request_init(bdev, scmrq);\n\t\tsq->scmrq = scmrq;\n\t}\n\tscm_request_set(scmrq, req);\n\n\tif (scm_request_prepare(scmrq)) {\n\t\tSCM_LOG(5, \"aidaw alloc failed\");\n\t\tscm_request_set(scmrq, NULL);\n\n\t\tif (scmrq->aob->request.msb_count)\n\t\t\tscm_request_start(scmrq);\n\n\t\tsq->scmrq = NULL;\n\t\tspin_unlock(&sq->lock);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\tblk_mq_start_request(req);\n\n\tif (qd->last || scmrq->aob->request.msb_count == nr_requests_per_io) {\n\t\tscm_request_start(scmrq);\n\t\tsq->scmrq = NULL;\n\t}\n\tspin_unlock(&sq->lock);\n\treturn BLK_STS_OK;\n}\n\nstatic int scm_blk_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\t\t     unsigned int idx)\n{\n\tstruct scm_queue *qd = kzalloc(sizeof(*qd), GFP_KERNEL);\n\n\tif (!qd)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&qd->lock);\n\thctx->driver_data = qd;\n\n\treturn 0;\n}\n\nstatic void scm_blk_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int idx)\n{\n\tstruct scm_queue *qd = hctx->driver_data;\n\n\tWARN_ON(qd->scmrq);\n\tkfree(hctx->driver_data);\n\thctx->driver_data = NULL;\n}\n\nstatic void __scmrq_log_error(struct scm_request *scmrq)\n{\n\tstruct aob *aob = scmrq->aob;\n\n\tif (scmrq->error == BLK_STS_TIMEOUT)\n\t\tSCM_LOG(1, \"Request timeout\");\n\telse {\n\t\tSCM_LOG(1, \"Request error\");\n\t\tSCM_LOG_HEX(1, &aob->response, sizeof(aob->response));\n\t}\n\tif (scmrq->retries)\n\t\tSCM_LOG(1, \"Retry request\");\n\telse\n\t\tpr_err(\"An I/O operation to SCM failed with rc=%d\\n\",\n\t\t       scmrq->error);\n}\n\nstatic void scm_blk_handle_error(struct scm_request *scmrq)\n{\n\tstruct scm_blk_dev *bdev = scmrq->bdev;\n\tunsigned long flags;\n\n\tif (scmrq->error != BLK_STS_IOERR)\n\t\tgoto restart;\n\n\t \n\tswitch (scmrq->aob->response.eqc) {\n\tcase EQC_WR_PROHIBIT:\n\t\tspin_lock_irqsave(&bdev->lock, flags);\n\t\tif (bdev->state != SCM_WR_PROHIBIT)\n\t\t\tpr_info(\"%lx: Write access to the SCM increment is suspended\\n\",\n\t\t\t\t(unsigned long) bdev->scmdev->address);\n\t\tbdev->state = SCM_WR_PROHIBIT;\n\t\tspin_unlock_irqrestore(&bdev->lock, flags);\n\t\tgoto requeue;\n\tdefault:\n\t\tbreak;\n\t}\n\nrestart:\n\tif (!eadm_start_aob(scmrq->aob))\n\t\treturn;\n\nrequeue:\n\tscm_request_requeue(scmrq);\n}\n\nvoid scm_blk_irq(struct scm_device *scmdev, void *data, blk_status_t error)\n{\n\tstruct scm_request *scmrq = data;\n\n\tscmrq->error = error;\n\tif (error) {\n\t\t__scmrq_log_error(scmrq);\n\t\tif (scmrq->retries-- > 0) {\n\t\t\tscm_blk_handle_error(scmrq);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tscm_request_finish(scmrq);\n}\n\nstatic void scm_blk_request_done(struct request *req)\n{\n\tblk_status_t *error = blk_mq_rq_to_pdu(req);\n\n\tblk_mq_end_request(req, *error);\n}\n\nstatic const struct block_device_operations scm_blk_devops = {\n\t.owner = THIS_MODULE,\n};\n\nstatic const struct blk_mq_ops scm_mq_ops = {\n\t.queue_rq = scm_blk_request,\n\t.complete = scm_blk_request_done,\n\t.init_hctx = scm_blk_init_hctx,\n\t.exit_hctx = scm_blk_exit_hctx,\n};\n\nint scm_blk_dev_setup(struct scm_blk_dev *bdev, struct scm_device *scmdev)\n{\n\tunsigned int devindex, nr_max_blk;\n\tstruct request_queue *rq;\n\tint len, ret;\n\n\tdevindex = atomic_inc_return(&nr_devices) - 1;\n\t \n\tif (devindex > 701) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tbdev->scmdev = scmdev;\n\tbdev->state = SCM_OPER;\n\tspin_lock_init(&bdev->lock);\n\tatomic_set(&bdev->queued_reqs, 0);\n\n\tbdev->tag_set.ops = &scm_mq_ops;\n\tbdev->tag_set.cmd_size = sizeof(blk_status_t);\n\tbdev->tag_set.nr_hw_queues = nr_requests;\n\tbdev->tag_set.queue_depth = nr_requests_per_io * nr_requests;\n\tbdev->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\n\tbdev->tag_set.numa_node = NUMA_NO_NODE;\n\n\tret = blk_mq_alloc_tag_set(&bdev->tag_set);\n\tif (ret)\n\t\tgoto out;\n\n\tbdev->gendisk = blk_mq_alloc_disk(&bdev->tag_set, scmdev);\n\tif (IS_ERR(bdev->gendisk)) {\n\t\tret = PTR_ERR(bdev->gendisk);\n\t\tgoto out_tag;\n\t}\n\trq = bdev->rq = bdev->gendisk->queue;\n\tnr_max_blk = min(scmdev->nr_max_block,\n\t\t\t (unsigned int) (PAGE_SIZE / sizeof(struct aidaw)));\n\n\tblk_queue_logical_block_size(rq, 1 << 12);\n\tblk_queue_max_hw_sectors(rq, nr_max_blk << 3);  \n\tblk_queue_max_segments(rq, nr_max_blk);\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, rq);\n\tblk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, rq);\n\n\tbdev->gendisk->private_data = scmdev;\n\tbdev->gendisk->fops = &scm_blk_devops;\n\tbdev->gendisk->major = scm_major;\n\tbdev->gendisk->first_minor = devindex * SCM_NR_PARTS;\n\tbdev->gendisk->minors = SCM_NR_PARTS;\n\n\tlen = snprintf(bdev->gendisk->disk_name, DISK_NAME_LEN, \"scm\");\n\tif (devindex > 25) {\n\t\tlen += snprintf(bdev->gendisk->disk_name + len,\n\t\t\t\tDISK_NAME_LEN - len, \"%c\",\n\t\t\t\t'a' + (devindex / 26) - 1);\n\t\tdevindex = devindex % 26;\n\t}\n\tsnprintf(bdev->gendisk->disk_name + len, DISK_NAME_LEN - len, \"%c\",\n\t\t 'a' + devindex);\n\n\t \n\tset_capacity(bdev->gendisk, scmdev->size >> 9);\n\tret = device_add_disk(&scmdev->dev, bdev->gendisk, NULL);\n\tif (ret)\n\t\tgoto out_cleanup_disk;\n\n\treturn 0;\n\nout_cleanup_disk:\n\tput_disk(bdev->gendisk);\nout_tag:\n\tblk_mq_free_tag_set(&bdev->tag_set);\nout:\n\tatomic_dec(&nr_devices);\n\treturn ret;\n}\n\nvoid scm_blk_dev_cleanup(struct scm_blk_dev *bdev)\n{\n\tdel_gendisk(bdev->gendisk);\n\tput_disk(bdev->gendisk);\n\tblk_mq_free_tag_set(&bdev->tag_set);\n}\n\nvoid scm_blk_set_available(struct scm_blk_dev *bdev)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bdev->lock, flags);\n\tif (bdev->state == SCM_WR_PROHIBIT)\n\t\tpr_info(\"%lx: Write access to the SCM increment is restored\\n\",\n\t\t\t(unsigned long) bdev->scmdev->address);\n\tbdev->state = SCM_OPER;\n\tspin_unlock_irqrestore(&bdev->lock, flags);\n}\n\nstatic bool __init scm_blk_params_valid(void)\n{\n\tif (!nr_requests_per_io || nr_requests_per_io > 64)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int __init scm_blk_init(void)\n{\n\tint ret = -EINVAL;\n\n\tif (!scm_blk_params_valid())\n\t\tgoto out;\n\n\tret = register_blkdev(0, \"scm\");\n\tif (ret < 0)\n\t\tgoto out;\n\n\tscm_major = ret;\n\tret = scm_alloc_rqs(nr_requests);\n\tif (ret)\n\t\tgoto out_free;\n\n\tscm_debug = debug_register(\"scm_log\", 16, 1, 16);\n\tif (!scm_debug) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\tdebug_register_view(scm_debug, &debug_hex_ascii_view);\n\tdebug_set_level(scm_debug, 2);\n\n\tret = scm_drv_init();\n\tif (ret)\n\t\tgoto out_dbf;\n\n\treturn ret;\n\nout_dbf:\n\tdebug_unregister(scm_debug);\nout_free:\n\tscm_free_rqs();\n\tunregister_blkdev(scm_major, \"scm\");\nout:\n\treturn ret;\n}\nmodule_init(scm_blk_init);\n\nstatic void __exit scm_blk_cleanup(void)\n{\n\tscm_drv_cleanup();\n\tdebug_unregister(scm_debug);\n\tscm_free_rqs();\n\tunregister_blkdev(scm_major, \"scm\");\n}\nmodule_exit(scm_blk_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}