{
  "module_name": "usnic_uiom.c",
  "hash_id": "967e54e4e3e1766b6a6b1fab0e817d2854083aecfd45cb53cd2be185ac695e9e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/usnic/usnic_uiom.c",
  "human_readable_source": " \n\n#include <linux/mm.h>\n#include <linux/dma-mapping.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/iommu.h>\n#include <linux/workqueue.h>\n#include <linux/list.h>\n#include <rdma/ib_verbs.h>\n\n#include \"usnic_log.h\"\n#include \"usnic_uiom.h\"\n#include \"usnic_uiom_interval_tree.h\"\n\n#define USNIC_UIOM_PAGE_CHUNK\t\t\t\t\t\t\\\n\t((PAGE_SIZE - offsetof(struct usnic_uiom_chunk, page_list))\t/\\\n\t((void *) &((struct usnic_uiom_chunk *) 0)->page_list[1] -\t\\\n\t(void *) &((struct usnic_uiom_chunk *) 0)->page_list[0]))\n\nstatic int usnic_uiom_dma_fault(struct iommu_domain *domain,\n\t\t\t\tstruct device *dev,\n\t\t\t\tunsigned long iova, int flags,\n\t\t\t\tvoid *token)\n{\n\tusnic_err(\"Device %s iommu fault domain 0x%pK va 0x%lx flags 0x%x\\n\",\n\t\tdev_name(dev),\n\t\tdomain, iova, flags);\n\treturn -ENOSYS;\n}\n\nstatic void usnic_uiom_put_pages(struct list_head *chunk_list, int dirty)\n{\n\tstruct usnic_uiom_chunk *chunk, *tmp;\n\tstruct page *page;\n\tstruct scatterlist *sg;\n\tint i;\n\tdma_addr_t pa;\n\n\tlist_for_each_entry_safe(chunk, tmp, chunk_list, list) {\n\t\tfor_each_sg(chunk->page_list, sg, chunk->nents, i) {\n\t\t\tpage = sg_page(sg);\n\t\t\tpa = sg_phys(sg);\n\t\t\tunpin_user_pages_dirty_lock(&page, 1, dirty);\n\t\t\tusnic_dbg(\"pa: %pa\\n\", &pa);\n\t\t}\n\t\tkfree(chunk);\n\t}\n}\n\nstatic int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,\n\t\t\t\tint dmasync, struct usnic_uiom_reg *uiomr)\n{\n\tstruct list_head *chunk_list = &uiomr->chunk_list;\n\tunsigned int gup_flags = FOLL_LONGTERM;\n\tstruct page **page_list;\n\tstruct scatterlist *sg;\n\tstruct usnic_uiom_chunk *chunk;\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tunsigned long cur_base;\n\tunsigned long npages;\n\tint ret;\n\tint off;\n\tint i;\n\tdma_addr_t pa;\n\tstruct mm_struct *mm;\n\n\t \n\tif (((addr + size) < addr) || PAGE_ALIGN(addr + size) < (addr + size))\n\t\treturn -EINVAL;\n\n\tif (!size)\n\t\treturn -EINVAL;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tINIT_LIST_HEAD(chunk_list);\n\n\tpage_list = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!page_list)\n\t\treturn -ENOMEM;\n\n\tnpages = PAGE_ALIGN(size + (addr & ~PAGE_MASK)) >> PAGE_SHIFT;\n\n\tuiomr->owning_mm = mm = current->mm;\n\tmmap_read_lock(mm);\n\n\tlocked = atomic64_add_return(npages, &current->mm->pinned_vm);\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (writable)\n\t\tgup_flags |= FOLL_WRITE;\n\tcur_base = addr & PAGE_MASK;\n\tret = 0;\n\n\twhile (npages) {\n\t\tret = pin_user_pages(cur_base,\n\t\t\t\t     min_t(unsigned long, npages,\n\t\t\t\t     PAGE_SIZE / sizeof(struct page *)),\n\t\t\t\t     gup_flags, page_list);\n\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tnpages -= ret;\n\t\toff = 0;\n\n\t\twhile (ret) {\n\t\t\tchunk = kmalloc(struct_size(chunk, page_list,\n\t\t\t\t\tmin_t(int, ret, USNIC_UIOM_PAGE_CHUNK)),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!chunk) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tchunk->nents = min_t(int, ret, USNIC_UIOM_PAGE_CHUNK);\n\t\t\tsg_init_table(chunk->page_list, chunk->nents);\n\t\t\tfor_each_sg(chunk->page_list, sg, chunk->nents, i) {\n\t\t\t\tsg_set_page(sg, page_list[i + off],\n\t\t\t\t\t\tPAGE_SIZE, 0);\n\t\t\t\tpa = sg_phys(sg);\n\t\t\t\tusnic_dbg(\"va: 0x%lx pa: %pa\\n\",\n\t\t\t\t\t\tcur_base + i*PAGE_SIZE, &pa);\n\t\t\t}\n\t\t\tcur_base += chunk->nents * PAGE_SIZE;\n\t\t\tret -= chunk->nents;\n\t\t\toff += chunk->nents;\n\t\t\tlist_add_tail(&chunk->list, chunk_list);\n\t\t}\n\n\t\tret = 0;\n\t}\n\nout:\n\tif (ret < 0) {\n\t\tusnic_uiom_put_pages(chunk_list, 0);\n\t\tatomic64_sub(npages, &current->mm->pinned_vm);\n\t} else\n\t\tmmgrab(uiomr->owning_mm);\n\n\tmmap_read_unlock(mm);\n\tfree_page((unsigned long) page_list);\n\treturn ret;\n}\n\nstatic void usnic_uiom_unmap_sorted_intervals(struct list_head *intervals,\n\t\t\t\t\t\tstruct usnic_uiom_pd *pd)\n{\n\tstruct usnic_uiom_interval_node *interval, *tmp;\n\tlong unsigned va, size;\n\n\tlist_for_each_entry_safe(interval, tmp, intervals, link) {\n\t\tva = interval->start << PAGE_SHIFT;\n\t\tsize = ((interval->last - interval->start) + 1) << PAGE_SHIFT;\n\t\twhile (size > 0) {\n\t\t\t \n\t\t\tusnic_dbg(\"va 0x%lx size 0x%lx\", va, PAGE_SIZE);\n\t\t\tiommu_unmap(pd->domain, va, PAGE_SIZE);\n\t\t\tva += PAGE_SIZE;\n\t\t\tsize -= PAGE_SIZE;\n\t\t}\n\t}\n}\n\nstatic void __usnic_uiom_reg_release(struct usnic_uiom_pd *pd,\n\t\t\t\t\tstruct usnic_uiom_reg *uiomr,\n\t\t\t\t\tint dirty)\n{\n\tint npages;\n\tunsigned long vpn_start, vpn_last;\n\tstruct usnic_uiom_interval_node *interval, *tmp;\n\tint writable = 0;\n\tLIST_HEAD(rm_intervals);\n\n\tnpages = PAGE_ALIGN(uiomr->length + uiomr->offset) >> PAGE_SHIFT;\n\tvpn_start = (uiomr->va & PAGE_MASK) >> PAGE_SHIFT;\n\tvpn_last = vpn_start + npages - 1;\n\n\tspin_lock(&pd->lock);\n\tusnic_uiom_remove_interval(&pd->root, vpn_start,\n\t\t\t\t\tvpn_last, &rm_intervals);\n\tusnic_uiom_unmap_sorted_intervals(&rm_intervals, pd);\n\n\tlist_for_each_entry_safe(interval, tmp, &rm_intervals, link) {\n\t\tif (interval->flags & IOMMU_WRITE)\n\t\t\twritable = 1;\n\t\tlist_del(&interval->link);\n\t\tkfree(interval);\n\t}\n\n\tusnic_uiom_put_pages(&uiomr->chunk_list, dirty & writable);\n\tspin_unlock(&pd->lock);\n}\n\nstatic int usnic_uiom_map_sorted_intervals(struct list_head *intervals,\n\t\t\t\t\t\tstruct usnic_uiom_reg *uiomr)\n{\n\tint i, err;\n\tsize_t size;\n\tstruct usnic_uiom_chunk *chunk;\n\tstruct usnic_uiom_interval_node *interval_node;\n\tdma_addr_t pa;\n\tdma_addr_t pa_start = 0;\n\tdma_addr_t pa_end = 0;\n\tlong int va_start = -EINVAL;\n\tstruct usnic_uiom_pd *pd = uiomr->pd;\n\tlong int va = uiomr->va & PAGE_MASK;\n\tint flags = IOMMU_READ | IOMMU_CACHE;\n\n\tflags |= (uiomr->writable) ? IOMMU_WRITE : 0;\n\tchunk = list_first_entry(&uiomr->chunk_list, struct usnic_uiom_chunk,\n\t\t\t\t\t\t\t\t\tlist);\n\tlist_for_each_entry(interval_node, intervals, link) {\niter_chunk:\n\t\tfor (i = 0; i < chunk->nents; i++, va += PAGE_SIZE) {\n\t\t\tpa = sg_phys(&chunk->page_list[i]);\n\t\t\tif ((va >> PAGE_SHIFT) < interval_node->start)\n\t\t\t\tcontinue;\n\n\t\t\tif ((va >> PAGE_SHIFT) == interval_node->start) {\n\t\t\t\t \n\t\t\t\tva_start = va;\n\t\t\t\tpa_start = pa;\n\t\t\t\tpa_end = pa;\n\t\t\t}\n\n\t\t\tWARN_ON(va_start == -EINVAL);\n\n\t\t\tif ((pa_end + PAGE_SIZE != pa) &&\n\t\t\t\t\t(pa != pa_start)) {\n\t\t\t\t \n\t\t\t\tsize = pa_end - pa_start + PAGE_SIZE;\n\t\t\t\tusnic_dbg(\"va 0x%lx pa %pa size 0x%zx flags 0x%x\",\n\t\t\t\t\tva_start, &pa_start, size, flags);\n\t\t\t\terr = iommu_map(pd->domain, va_start, pa_start,\n\t\t\t\t\t\tsize, flags, GFP_ATOMIC);\n\t\t\t\tif (err) {\n\t\t\t\t\tusnic_err(\"Failed to map va 0x%lx pa %pa size 0x%zx with err %d\\n\",\n\t\t\t\t\t\tva_start, &pa_start, size, err);\n\t\t\t\t\tgoto err_out;\n\t\t\t\t}\n\t\t\t\tva_start = va;\n\t\t\t\tpa_start = pa;\n\t\t\t\tpa_end = pa;\n\t\t\t}\n\n\t\t\tif ((va >> PAGE_SHIFT) == interval_node->last) {\n\t\t\t\t \n\t\t\t\tsize = pa - pa_start + PAGE_SIZE;\n\t\t\t\tusnic_dbg(\"va 0x%lx pa %pa size 0x%zx flags 0x%x\\n\",\n\t\t\t\t\tva_start, &pa_start, size, flags);\n\t\t\t\terr = iommu_map(pd->domain, va_start, pa_start,\n\t\t\t\t\t\tsize, flags, GFP_ATOMIC);\n\t\t\t\tif (err) {\n\t\t\t\t\tusnic_err(\"Failed to map va 0x%lx pa %pa size 0x%zx with err %d\\n\",\n\t\t\t\t\t\tva_start, &pa_start, size, err);\n\t\t\t\t\tgoto err_out;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (pa != pa_start)\n\t\t\t\tpa_end += PAGE_SIZE;\n\t\t}\n\n\t\tif (i == chunk->nents) {\n\t\t\t \n\t\t\tchunk = list_first_entry(&chunk->list,\n\t\t\t\t\t\t\tstruct usnic_uiom_chunk,\n\t\t\t\t\t\t\tlist);\n\t\t\tgoto iter_chunk;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_out:\n\tusnic_uiom_unmap_sorted_intervals(intervals, pd);\n\treturn err;\n}\n\nstruct usnic_uiom_reg *usnic_uiom_reg_get(struct usnic_uiom_pd *pd,\n\t\t\t\t\t\tunsigned long addr, size_t size,\n\t\t\t\t\t\tint writable, int dmasync)\n{\n\tstruct usnic_uiom_reg *uiomr;\n\tunsigned long va_base, vpn_start, vpn_last;\n\tunsigned long npages;\n\tint offset, err;\n\tLIST_HEAD(sorted_diff_intervals);\n\n\t \n\twritable = 1;\n\n\tva_base = addr & PAGE_MASK;\n\toffset = addr & ~PAGE_MASK;\n\tnpages = PAGE_ALIGN(size + offset) >> PAGE_SHIFT;\n\tvpn_start = (addr & PAGE_MASK) >> PAGE_SHIFT;\n\tvpn_last = vpn_start + npages - 1;\n\n\tuiomr = kmalloc(sizeof(*uiomr), GFP_KERNEL);\n\tif (!uiomr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tuiomr->va = va_base;\n\tuiomr->offset = offset;\n\tuiomr->length = size;\n\tuiomr->writable = writable;\n\tuiomr->pd = pd;\n\n\terr = usnic_uiom_get_pages(addr, size, writable, dmasync,\n\t\t\t\t   uiomr);\n\tif (err) {\n\t\tusnic_err(\"Failed get_pages vpn [0x%lx,0x%lx] err %d\\n\",\n\t\t\t\tvpn_start, vpn_last, err);\n\t\tgoto out_free_uiomr;\n\t}\n\n\tspin_lock(&pd->lock);\n\terr = usnic_uiom_get_intervals_diff(vpn_start, vpn_last,\n\t\t\t\t\t\t(writable) ? IOMMU_WRITE : 0,\n\t\t\t\t\t\tIOMMU_WRITE,\n\t\t\t\t\t\t&pd->root,\n\t\t\t\t\t\t&sorted_diff_intervals);\n\tif (err) {\n\t\tusnic_err(\"Failed disjoint interval vpn [0x%lx,0x%lx] err %d\\n\",\n\t\t\t\t\t\tvpn_start, vpn_last, err);\n\t\tgoto out_put_pages;\n\t}\n\n\terr = usnic_uiom_map_sorted_intervals(&sorted_diff_intervals, uiomr);\n\tif (err) {\n\t\tusnic_err(\"Failed map interval vpn [0x%lx,0x%lx] err %d\\n\",\n\t\t\t\t\t\tvpn_start, vpn_last, err);\n\t\tgoto out_put_intervals;\n\n\t}\n\n\terr = usnic_uiom_insert_interval(&pd->root, vpn_start, vpn_last,\n\t\t\t\t\t(writable) ? IOMMU_WRITE : 0);\n\tif (err) {\n\t\tusnic_err(\"Failed insert interval vpn [0x%lx,0x%lx] err %d\\n\",\n\t\t\t\t\t\tvpn_start, vpn_last, err);\n\t\tgoto out_unmap_intervals;\n\t}\n\n\tusnic_uiom_put_interval_set(&sorted_diff_intervals);\n\tspin_unlock(&pd->lock);\n\n\treturn uiomr;\n\nout_unmap_intervals:\n\tusnic_uiom_unmap_sorted_intervals(&sorted_diff_intervals, pd);\nout_put_intervals:\n\tusnic_uiom_put_interval_set(&sorted_diff_intervals);\nout_put_pages:\n\tusnic_uiom_put_pages(&uiomr->chunk_list, 0);\n\tspin_unlock(&pd->lock);\n\tmmdrop(uiomr->owning_mm);\nout_free_uiomr:\n\tkfree(uiomr);\n\treturn ERR_PTR(err);\n}\n\nstatic void __usnic_uiom_release_tail(struct usnic_uiom_reg *uiomr)\n{\n\tmmdrop(uiomr->owning_mm);\n\tkfree(uiomr);\n}\n\nstatic inline size_t usnic_uiom_num_pages(struct usnic_uiom_reg *uiomr)\n{\n\treturn PAGE_ALIGN(uiomr->length + uiomr->offset) >> PAGE_SHIFT;\n}\n\nvoid usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr)\n{\n\t__usnic_uiom_reg_release(uiomr->pd, uiomr, 1);\n\n\tatomic64_sub(usnic_uiom_num_pages(uiomr), &uiomr->owning_mm->pinned_vm);\n\t__usnic_uiom_release_tail(uiomr);\n}\n\nstruct usnic_uiom_pd *usnic_uiom_alloc_pd(struct device *dev)\n{\n\tstruct usnic_uiom_pd *pd;\n\tvoid *domain;\n\n\tpd = kzalloc(sizeof(*pd), GFP_KERNEL);\n\tif (!pd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpd->domain = domain = iommu_domain_alloc(dev->bus);\n\tif (!domain) {\n\t\tusnic_err(\"Failed to allocate IOMMU domain\");\n\t\tkfree(pd);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tiommu_set_fault_handler(pd->domain, usnic_uiom_dma_fault, NULL);\n\n\tspin_lock_init(&pd->lock);\n\tINIT_LIST_HEAD(&pd->devs);\n\n\treturn pd;\n}\n\nvoid usnic_uiom_dealloc_pd(struct usnic_uiom_pd *pd)\n{\n\tiommu_domain_free(pd->domain);\n\tkfree(pd);\n}\n\nint usnic_uiom_attach_dev_to_pd(struct usnic_uiom_pd *pd, struct device *dev)\n{\n\tstruct usnic_uiom_dev *uiom_dev;\n\tint err;\n\n\tuiom_dev = kzalloc(sizeof(*uiom_dev), GFP_ATOMIC);\n\tif (!uiom_dev)\n\t\treturn -ENOMEM;\n\tuiom_dev->dev = dev;\n\n\terr = iommu_attach_device(pd->domain, dev);\n\tif (err)\n\t\tgoto out_free_dev;\n\n\tif (!device_iommu_capable(dev, IOMMU_CAP_CACHE_COHERENCY)) {\n\t\tusnic_err(\"IOMMU of %s does not support cache coherency\\n\",\n\t\t\t\tdev_name(dev));\n\t\terr = -EINVAL;\n\t\tgoto out_detach_device;\n\t}\n\n\tspin_lock(&pd->lock);\n\tlist_add_tail(&uiom_dev->link, &pd->devs);\n\tpd->dev_cnt++;\n\tspin_unlock(&pd->lock);\n\n\treturn 0;\n\nout_detach_device:\n\tiommu_detach_device(pd->domain, dev);\nout_free_dev:\n\tkfree(uiom_dev);\n\treturn err;\n}\n\nvoid usnic_uiom_detach_dev_from_pd(struct usnic_uiom_pd *pd, struct device *dev)\n{\n\tstruct usnic_uiom_dev *uiom_dev;\n\tint found = 0;\n\n\tspin_lock(&pd->lock);\n\tlist_for_each_entry(uiom_dev, &pd->devs, link) {\n\t\tif (uiom_dev->dev == dev) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found) {\n\t\tusnic_err(\"Unable to free dev %s - not found\\n\",\n\t\t\t\tdev_name(dev));\n\t\tspin_unlock(&pd->lock);\n\t\treturn;\n\t}\n\n\tlist_del(&uiom_dev->link);\n\tpd->dev_cnt--;\n\tspin_unlock(&pd->lock);\n\n\treturn iommu_detach_device(pd->domain, dev);\n}\n\nstruct device **usnic_uiom_get_dev_list(struct usnic_uiom_pd *pd)\n{\n\tstruct usnic_uiom_dev *uiom_dev;\n\tstruct device **devs;\n\tint i = 0;\n\n\tspin_lock(&pd->lock);\n\tdevs = kcalloc(pd->dev_cnt + 1, sizeof(*devs), GFP_ATOMIC);\n\tif (!devs) {\n\t\tdevs = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(uiom_dev, &pd->devs, link) {\n\t\tdevs[i++] = uiom_dev->dev;\n\t}\nout:\n\tspin_unlock(&pd->lock);\n\treturn devs;\n}\n\nvoid usnic_uiom_free_dev_list(struct device **devs)\n{\n\tkfree(devs);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}