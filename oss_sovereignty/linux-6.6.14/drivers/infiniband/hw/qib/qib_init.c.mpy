{
  "module_name": "qib_init.c",
  "hash_id": "6f7fbeda2adafb81ae52a5cab09dbeb58cb069febcff6bfc37c3479fdafe3e35",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qib/qib_init.c",
  "human_readable_source": " \n\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/printk.h>\n#ifdef CONFIG_INFINIBAND_QIB_DCA\n#include <linux/dca.h>\n#endif\n#include <rdma/rdma_vt.h>\n\n#include \"qib.h\"\n#include \"qib_common.h\"\n#include \"qib_mad.h\"\n#ifdef CONFIG_DEBUG_FS\n#include \"qib_debugfs.h\"\n#include \"qib_verbs.h\"\n#endif\n\n#undef pr_fmt\n#define pr_fmt(fmt) QIB_DRV_NAME \": \" fmt\n\n \n#define QIB_MIN_USER_CTXT_BUFCNT 7\n\n#define QLOGIC_IB_R_SOFTWARE_MASK 0xFF\n#define QLOGIC_IB_R_SOFTWARE_SHIFT 24\n#define QLOGIC_IB_R_EMULATOR_MASK (1ULL<<62)\n\n \nushort qib_cfgctxts;\nmodule_param_named(cfgctxts, qib_cfgctxts, ushort, S_IRUGO);\nMODULE_PARM_DESC(cfgctxts, \"Set max number of contexts to use\");\n\nunsigned qib_numa_aware;\nmodule_param_named(numa_aware, qib_numa_aware, uint, S_IRUGO);\nMODULE_PARM_DESC(numa_aware,\n\t\"0 -> PSM allocation close to HCA, 1 -> PSM allocation local to process\");\n\n \nushort qib_mini_init;\nmodule_param_named(mini_init, qib_mini_init, ushort, S_IRUGO);\nMODULE_PARM_DESC(mini_init, \"If set, do minimal diag init\");\n\nunsigned qib_n_krcv_queues;\nmodule_param_named(krcvqs, qib_n_krcv_queues, uint, S_IRUGO);\nMODULE_PARM_DESC(krcvqs, \"number of kernel receive queues per IB port\");\n\nunsigned qib_cc_table_size;\nmodule_param_named(cc_table_size, qib_cc_table_size, uint, S_IRUGO);\nMODULE_PARM_DESC(cc_table_size, \"Congestion control table entries 0 (CCA disabled - default), min = 128, max = 1984\");\n\nstatic void verify_interrupt(struct timer_list *);\n\nDEFINE_XARRAY_FLAGS(qib_dev_table, XA_FLAGS_ALLOC | XA_FLAGS_LOCK_IRQ);\nu32 qib_cpulist_count;\nunsigned long *qib_cpulist;\n\n \nvoid qib_set_ctxtcnt(struct qib_devdata *dd)\n{\n\tif (!qib_cfgctxts) {\n\t\tdd->cfgctxts = dd->first_user_ctxt + num_online_cpus();\n\t\tif (dd->cfgctxts > dd->ctxtcnt)\n\t\t\tdd->cfgctxts = dd->ctxtcnt;\n\t} else if (qib_cfgctxts < dd->num_pports)\n\t\tdd->cfgctxts = dd->ctxtcnt;\n\telse if (qib_cfgctxts <= dd->ctxtcnt)\n\t\tdd->cfgctxts = qib_cfgctxts;\n\telse\n\t\tdd->cfgctxts = dd->ctxtcnt;\n\tdd->freectxts = (dd->first_user_ctxt > dd->cfgctxts) ? 0 :\n\t\tdd->cfgctxts - dd->first_user_ctxt;\n}\n\n \nint qib_create_ctxts(struct qib_devdata *dd)\n{\n\tunsigned i;\n\tint local_node_id = pcibus_to_node(dd->pcidev->bus);\n\n\tif (local_node_id < 0)\n\t\tlocal_node_id = numa_node_id();\n\tdd->assigned_node_id = local_node_id;\n\n\t \n\tdd->rcd = kcalloc(dd->ctxtcnt, sizeof(*dd->rcd), GFP_KERNEL);\n\tif (!dd->rcd)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < dd->first_user_ctxt; ++i) {\n\t\tstruct qib_pportdata *ppd;\n\t\tstruct qib_ctxtdata *rcd;\n\n\t\tif (dd->skip_kctxt_mask & (1 << i))\n\t\t\tcontinue;\n\n\t\tppd = dd->pport + (i % dd->num_pports);\n\n\t\trcd = qib_create_ctxtdata(ppd, i, dd->assigned_node_id);\n\t\tif (!rcd) {\n\t\t\tqib_dev_err(dd,\n\t\t\t\t\"Unable to allocate ctxtdata for Kernel ctxt, failing\\n\");\n\t\t\tkfree(dd->rcd);\n\t\t\tdd->rcd = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\trcd->pkeys[0] = QIB_DEFAULT_P_KEY;\n\t\trcd->seq_cnt = 1;\n\t}\n\treturn 0;\n}\n\n \nstruct qib_ctxtdata *qib_create_ctxtdata(struct qib_pportdata *ppd, u32 ctxt,\n\tint node_id)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\tstruct qib_ctxtdata *rcd;\n\n\trcd = kzalloc_node(sizeof(*rcd), GFP_KERNEL, node_id);\n\tif (rcd) {\n\t\tINIT_LIST_HEAD(&rcd->qp_wait_list);\n\t\trcd->node_id = node_id;\n\t\trcd->ppd = ppd;\n\t\trcd->dd = dd;\n\t\trcd->cnt = 1;\n\t\trcd->ctxt = ctxt;\n\t\tdd->rcd[ctxt] = rcd;\n#ifdef CONFIG_DEBUG_FS\n\t\tif (ctxt < dd->first_user_ctxt) {  \n\t\t\trcd->opstats = kzalloc_node(sizeof(*rcd->opstats),\n\t\t\t\tGFP_KERNEL, node_id);\n\t\t\tif (!rcd->opstats) {\n\t\t\t\tkfree(rcd);\n\t\t\t\tqib_dev_err(dd,\n\t\t\t\t\t\"Unable to allocate per ctxt stats buffer\\n\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n#endif\n\t\tdd->f_init_ctxt(rcd);\n\n\t\t \n\t\trcd->rcvegrbuf_size = 0x8000;\n\t\trcd->rcvegrbufs_perchunk =\n\t\t\trcd->rcvegrbuf_size / dd->rcvegrbufsize;\n\t\trcd->rcvegrbuf_chunks = (rcd->rcvegrcnt +\n\t\t\trcd->rcvegrbufs_perchunk - 1) /\n\t\t\trcd->rcvegrbufs_perchunk;\n\t\trcd->rcvegrbufs_perchunk_shift =\n\t\t\tilog2(rcd->rcvegrbufs_perchunk);\n\t}\n\treturn rcd;\n}\n\n \nint qib_init_pportdata(struct qib_pportdata *ppd, struct qib_devdata *dd,\n\t\t\tu8 hw_pidx, u8 port)\n{\n\tint size;\n\n\tppd->dd = dd;\n\tppd->hw_pidx = hw_pidx;\n\tppd->port = port;  \n\n\tspin_lock_init(&ppd->sdma_lock);\n\tspin_lock_init(&ppd->lflags_lock);\n\tspin_lock_init(&ppd->cc_shadow_lock);\n\tinit_waitqueue_head(&ppd->state_wait);\n\n\ttimer_setup(&ppd->symerr_clear_timer, qib_clear_symerror_on_linkup, 0);\n\n\tppd->qib_wq = NULL;\n\tppd->ibport_data.pmastats =\n\t\talloc_percpu(struct qib_pma_counters);\n\tif (!ppd->ibport_data.pmastats)\n\t\treturn -ENOMEM;\n\tppd->ibport_data.rvp.rc_acks = alloc_percpu(u64);\n\tppd->ibport_data.rvp.rc_qacks = alloc_percpu(u64);\n\tppd->ibport_data.rvp.rc_delayed_comp = alloc_percpu(u64);\n\tif (!(ppd->ibport_data.rvp.rc_acks) ||\n\t    !(ppd->ibport_data.rvp.rc_qacks) ||\n\t    !(ppd->ibport_data.rvp.rc_delayed_comp))\n\t\treturn -ENOMEM;\n\n\tif (qib_cc_table_size < IB_CCT_MIN_ENTRIES)\n\t\tgoto bail;\n\n\tppd->cc_supported_table_entries = min(max_t(int, qib_cc_table_size,\n\t\tIB_CCT_MIN_ENTRIES), IB_CCT_ENTRIES*IB_CC_TABLE_CAP_DEFAULT);\n\n\tppd->cc_max_table_entries =\n\t\tppd->cc_supported_table_entries/IB_CCT_ENTRIES;\n\n\tsize = IB_CC_TABLE_CAP_DEFAULT * sizeof(struct ib_cc_table_entry)\n\t\t* IB_CCT_ENTRIES;\n\tppd->ccti_entries = kzalloc(size, GFP_KERNEL);\n\tif (!ppd->ccti_entries)\n\t\tgoto bail;\n\n\tsize = IB_CC_CCS_ENTRIES * sizeof(struct ib_cc_congestion_entry);\n\tppd->congestion_entries = kzalloc(size, GFP_KERNEL);\n\tif (!ppd->congestion_entries)\n\t\tgoto bail_1;\n\n\tsize = sizeof(struct cc_table_shadow);\n\tppd->ccti_entries_shadow = kzalloc(size, GFP_KERNEL);\n\tif (!ppd->ccti_entries_shadow)\n\t\tgoto bail_2;\n\n\tsize = sizeof(struct ib_cc_congestion_setting_attr);\n\tppd->congestion_entries_shadow = kzalloc(size, GFP_KERNEL);\n\tif (!ppd->congestion_entries_shadow)\n\t\tgoto bail_3;\n\n\treturn 0;\n\nbail_3:\n\tkfree(ppd->ccti_entries_shadow);\n\tppd->ccti_entries_shadow = NULL;\nbail_2:\n\tkfree(ppd->congestion_entries);\n\tppd->congestion_entries = NULL;\nbail_1:\n\tkfree(ppd->ccti_entries);\n\tppd->ccti_entries = NULL;\nbail:\n\t \n\tif (!qib_cc_table_size)\n\t\treturn 0;\n\n\tif (qib_cc_table_size < IB_CCT_MIN_ENTRIES) {\n\t\tqib_cc_table_size = 0;\n\t\tqib_dev_err(dd,\n\t\t \"Congestion Control table size %d less than minimum %d for port %d\\n\",\n\t\t qib_cc_table_size, IB_CCT_MIN_ENTRIES, port);\n\t}\n\n\tqib_dev_err(dd, \"Congestion Control Agent disabled for port %d\\n\",\n\t\tport);\n\treturn 0;\n}\n\nstatic int init_pioavailregs(struct qib_devdata *dd)\n{\n\tint ret, pidx;\n\tu64 *status_page;\n\n\tdd->pioavailregs_dma = dma_alloc_coherent(\n\t\t&dd->pcidev->dev, PAGE_SIZE, &dd->pioavailregs_phys,\n\t\tGFP_KERNEL);\n\tif (!dd->pioavailregs_dma) {\n\t\tqib_dev_err(dd,\n\t\t\t\"failed to allocate PIOavail reg area in memory\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\t \n\tstatus_page = (u64 *)\n\t\t((char *) dd->pioavailregs_dma +\n\t\t ((2 * L1_CACHE_BYTES +\n\t\t   dd->pioavregs * sizeof(u64)) & ~L1_CACHE_BYTES));\n\t \n\tdd->devstatusp = status_page;\n\t*status_page++ = 0;\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tdd->pport[pidx].statusp = status_page;\n\t\t*status_page++ = 0;\n\t}\n\n\t \n\tdd->freezemsg = (char *) status_page;\n\t*dd->freezemsg = 0;\n\t \n\tret = (char *) status_page - (char *) dd->pioavailregs_dma;\n\tdd->freezelen = PAGE_SIZE - ret;\n\n\tret = 0;\n\ndone:\n\treturn ret;\n}\n\n \nstatic void init_shadow_tids(struct qib_devdata *dd)\n{\n\tstruct page **pages;\n\tdma_addr_t *addrs;\n\n\tpages = vzalloc(array_size(sizeof(struct page *),\n\t\t\t\t   dd->cfgctxts * dd->rcvtidcnt));\n\tif (!pages)\n\t\tgoto bail;\n\n\taddrs = vzalloc(array_size(sizeof(dma_addr_t),\n\t\t\t\t   dd->cfgctxts * dd->rcvtidcnt));\n\tif (!addrs)\n\t\tgoto bail_free;\n\n\tdd->pageshadow = pages;\n\tdd->physshadow = addrs;\n\treturn;\n\nbail_free:\n\tvfree(pages);\nbail:\n\tdd->pageshadow = NULL;\n}\n\n \nstatic int loadtime_init(struct qib_devdata *dd)\n{\n\tint ret = 0;\n\n\tif (((dd->revision >> QLOGIC_IB_R_SOFTWARE_SHIFT) &\n\t     QLOGIC_IB_R_SOFTWARE_MASK) != QIB_CHIP_SWVERSION) {\n\t\tqib_dev_err(dd,\n\t\t\t\"Driver only handles version %d, chip swversion is %d (%llx), failing\\n\",\n\t\t\tQIB_CHIP_SWVERSION,\n\t\t\t(int)(dd->revision >>\n\t\t\t\tQLOGIC_IB_R_SOFTWARE_SHIFT) &\n\t\t\t\tQLOGIC_IB_R_SOFTWARE_MASK,\n\t\t\t(unsigned long long) dd->revision);\n\t\tret = -ENOSYS;\n\t\tgoto done;\n\t}\n\n\tif (dd->revision & QLOGIC_IB_R_EMULATOR_MASK)\n\t\tqib_devinfo(dd->pcidev, \"%s\", dd->boardversion);\n\n\tspin_lock_init(&dd->pioavail_lock);\n\tspin_lock_init(&dd->sendctrl_lock);\n\tspin_lock_init(&dd->uctxt_lock);\n\tspin_lock_init(&dd->qib_diag_trans_lock);\n\tspin_lock_init(&dd->eep_st_lock);\n\tmutex_init(&dd->eep_lock);\n\n\tif (qib_mini_init)\n\t\tgoto done;\n\n\tret = init_pioavailregs(dd);\n\tinit_shadow_tids(dd);\n\n\tqib_get_eeprom_info(dd);\n\n\t \n\ttimer_setup(&dd->intrchk_timer, verify_interrupt, 0);\ndone:\n\treturn ret;\n}\n\n \nstatic int init_after_reset(struct qib_devdata *dd)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < dd->num_pports; ++i) {\n\t\t \n\t\tdd->f_rcvctrl(dd->pport + i, QIB_RCVCTRL_CTXT_DIS |\n\t\t\t\t  QIB_RCVCTRL_INTRAVAIL_DIS |\n\t\t\t\t  QIB_RCVCTRL_TAILUPD_DIS, -1);\n\t\t \n\t\tdd->f_sendctrl(dd->pport + i, QIB_SENDCTRL_SEND_DIS |\n\t\t\tQIB_SENDCTRL_AVAIL_DIS);\n\t}\n\n\treturn 0;\n}\n\nstatic void enable_chip(struct qib_devdata *dd)\n{\n\tu64 rcvmask;\n\tint i;\n\n\t \n\tfor (i = 0; i < dd->num_pports; ++i)\n\t\tdd->f_sendctrl(dd->pport + i, QIB_SENDCTRL_SEND_ENB |\n\t\t\tQIB_SENDCTRL_AVAIL_ENB);\n\t \n\trcvmask = QIB_RCVCTRL_CTXT_ENB | QIB_RCVCTRL_INTRAVAIL_ENB;\n\trcvmask |= (dd->flags & QIB_NODMA_RTAIL) ?\n\t\t  QIB_RCVCTRL_TAILUPD_DIS : QIB_RCVCTRL_TAILUPD_ENB;\n\tfor (i = 0; dd->rcd && i < dd->first_user_ctxt; ++i) {\n\t\tstruct qib_ctxtdata *rcd = dd->rcd[i];\n\n\t\tif (rcd)\n\t\t\tdd->f_rcvctrl(rcd->ppd, rcvmask, i);\n\t}\n}\n\nstatic void verify_interrupt(struct timer_list *t)\n{\n\tstruct qib_devdata *dd = from_timer(dd, t, intrchk_timer);\n\tu64 int_counter;\n\n\tif (!dd)\n\t\treturn;  \n\n\t \n\tint_counter = qib_int_counter(dd) - dd->z_int_counter;\n\tif (int_counter == 0) {\n\t\tif (!dd->f_intr_fallback(dd))\n\t\t\tdev_err(&dd->pcidev->dev,\n\t\t\t\t\"No interrupts detected, not usable.\\n\");\n\t\telse  \n\t\t\tmod_timer(&dd->intrchk_timer, jiffies + HZ/2);\n\t}\n}\n\nstatic void init_piobuf_state(struct qib_devdata *dd)\n{\n\tint i, pidx;\n\tu32 uctxts;\n\n\t \n\tdd->f_sendctrl(dd->pport, QIB_SENDCTRL_DISARM_ALL);\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx)\n\t\tdd->f_sendctrl(dd->pport + pidx, QIB_SENDCTRL_FLUSH);\n\n\t \n\tuctxts = dd->cfgctxts - dd->first_user_ctxt;\n\tdd->ctxts_extrabuf = dd->pbufsctxt ?\n\t\tdd->lastctxt_piobuf - (dd->pbufsctxt * uctxts) : 0;\n\n\t \n\tfor (i = 0; i < dd->pioavregs; i++) {\n\t\t__le64 tmp;\n\n\t\ttmp = dd->pioavailregs_dma[i];\n\t\t \n\t\tdd->pioavailshadow[i] = le64_to_cpu(tmp);\n\t}\n\twhile (i < ARRAY_SIZE(dd->pioavailshadow))\n\t\tdd->pioavailshadow[i++] = 0;  \n\n\t \n\tqib_chg_pioavailkernel(dd, 0, dd->piobcnt2k + dd->piobcnt4k,\n\t\t\t       TXCHK_CHG_TYPE_KERN, NULL);\n\tdd->f_initvl15_bufs(dd);\n}\n\n \nstatic int qib_create_workqueues(struct qib_devdata *dd)\n{\n\tint pidx;\n\tstruct qib_pportdata *ppd;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (!ppd->qib_wq) {\n\t\t\tchar wq_name[8];  \n\n\t\t\tsnprintf(wq_name, sizeof(wq_name), \"qib%d_%d\",\n\t\t\t\tdd->unit, pidx);\n\t\t\tppd->qib_wq = alloc_ordered_workqueue(wq_name,\n\t\t\t\t\t\t\t      WQ_MEM_RECLAIM);\n\t\t\tif (!ppd->qib_wq)\n\t\t\t\tgoto wq_error;\n\t\t}\n\t}\n\treturn 0;\nwq_error:\n\tpr_err(\"create_singlethread_workqueue failed for port %d\\n\",\n\t\tpidx + 1);\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (ppd->qib_wq) {\n\t\t\tdestroy_workqueue(ppd->qib_wq);\n\t\t\tppd->qib_wq = NULL;\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\nstatic void qib_free_pportdata(struct qib_pportdata *ppd)\n{\n\tfree_percpu(ppd->ibport_data.pmastats);\n\tfree_percpu(ppd->ibport_data.rvp.rc_acks);\n\tfree_percpu(ppd->ibport_data.rvp.rc_qacks);\n\tfree_percpu(ppd->ibport_data.rvp.rc_delayed_comp);\n\tppd->ibport_data.pmastats = NULL;\n}\n\n \nint qib_init(struct qib_devdata *dd, int reinit)\n{\n\tint ret = 0, pidx, lastfail = 0;\n\tu32 portok = 0;\n\tunsigned i;\n\tstruct qib_ctxtdata *rcd;\n\tstruct qib_pportdata *ppd;\n\tunsigned long flags;\n\n\t \n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tspin_lock_irqsave(&ppd->lflags_lock, flags);\n\t\tppd->lflags &= ~(QIBL_LINKACTIVE | QIBL_LINKARMED |\n\t\t\t\t QIBL_LINKDOWN | QIBL_LINKINIT |\n\t\t\t\t QIBL_LINKV);\n\t\tspin_unlock_irqrestore(&ppd->lflags_lock, flags);\n\t}\n\n\tif (reinit)\n\t\tret = init_after_reset(dd);\n\telse\n\t\tret = loadtime_init(dd);\n\tif (ret)\n\t\tgoto done;\n\n\t \n\tif (qib_mini_init)\n\t\treturn 0;\n\n\tret = dd->f_late_initreg(dd);\n\tif (ret)\n\t\tgoto done;\n\n\t \n\tfor (i = 0; dd->rcd && i < dd->first_user_ctxt; ++i) {\n\t\t \n\t\trcd = dd->rcd[i];\n\t\tif (!rcd)\n\t\t\tcontinue;\n\n\t\tlastfail = qib_create_rcvhdrq(dd, rcd);\n\t\tif (!lastfail)\n\t\t\tlastfail = qib_setup_eagerbufs(rcd);\n\t\tif (lastfail)\n\t\t\tqib_dev_err(dd,\n\t\t\t\t\"failed to allocate kernel ctxt's rcvhdrq and/or egr bufs\\n\");\n\t}\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tint mtu;\n\n\t\tif (lastfail)\n\t\t\tret = lastfail;\n\t\tppd = dd->pport + pidx;\n\t\tmtu = ib_mtu_enum_to_int(qib_ibmtu);\n\t\tif (mtu == -1) {\n\t\t\tmtu = QIB_DEFAULT_MTU;\n\t\t\tqib_ibmtu = 0;  \n\t\t}\n\t\t \n\t\tppd->init_ibmaxlen = min(mtu > 2048 ?\n\t\t\t\t\t dd->piosize4k : dd->piosize2k,\n\t\t\t\t\t dd->rcvegrbufsize +\n\t\t\t\t\t (dd->rcvhdrentsize << 2));\n\t\t \n\t\tppd->ibmaxlen = ppd->init_ibmaxlen;\n\t\tqib_set_mtu(ppd, mtu);\n\n\t\tspin_lock_irqsave(&ppd->lflags_lock, flags);\n\t\tppd->lflags |= QIBL_IB_LINK_DISABLED;\n\t\tspin_unlock_irqrestore(&ppd->lflags_lock, flags);\n\n\t\tlastfail = dd->f_bringup_serdes(ppd);\n\t\tif (lastfail) {\n\t\t\tqib_devinfo(dd->pcidev,\n\t\t\t\t \"Failed to bringup IB port %u\\n\", ppd->port);\n\t\t\tlastfail = -ENETDOWN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tportok++;\n\t}\n\n\tif (!portok) {\n\t\t \n\t\tif (!ret && lastfail)\n\t\t\tret = lastfail;\n\t\telse if (!ret)\n\t\t\tret = -ENETDOWN;\n\t\t \n\t}\n\n\tenable_chip(dd);\n\n\tinit_piobuf_state(dd);\n\ndone:\n\tif (!ret) {\n\t\t \n\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\t\tppd = dd->pport + pidx;\n\t\t\t \n\t\t\t*ppd->statusp |= QIB_STATUS_CHIP_PRESENT |\n\t\t\t\tQIB_STATUS_INITTED;\n\t\t\tif (!ppd->link_speed_enabled)\n\t\t\t\tcontinue;\n\t\t\tif (dd->flags & QIB_HAS_SEND_DMA)\n\t\t\t\tret = qib_setup_sdma(ppd);\n\t\t\ttimer_setup(&ppd->hol_timer, qib_hol_event, 0);\n\t\t\tppd->hol_state = QIB_HOL_UP;\n\t\t}\n\n\t\t \n\t\tdd->f_set_intr_state(dd, 1);\n\n\t\t \n\t\tmod_timer(&dd->intrchk_timer, jiffies + HZ/2);\n\t\t \n\t\tmod_timer(&dd->stats_timer, jiffies + HZ * ACTIVITY_TIMER);\n\t}\n\n\t \n\treturn ret;\n}\n\n \n\nint __attribute__((weak)) qib_enable_wc(struct qib_devdata *dd)\n{\n\treturn -EOPNOTSUPP;\n}\n\nvoid __attribute__((weak)) qib_disable_wc(struct qib_devdata *dd)\n{\n}\n\nstruct qib_devdata *qib_lookup(int unit)\n{\n\treturn xa_load(&qib_dev_table, unit);\n}\n\n \nstatic void qib_stop_timers(struct qib_devdata *dd)\n{\n\tstruct qib_pportdata *ppd;\n\tint pidx;\n\n\tif (dd->stats_timer.function)\n\t\tdel_timer_sync(&dd->stats_timer);\n\tif (dd->intrchk_timer.function)\n\t\tdel_timer_sync(&dd->intrchk_timer);\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (ppd->hol_timer.function)\n\t\t\tdel_timer_sync(&ppd->hol_timer);\n\t\tif (ppd->led_override_timer.function) {\n\t\t\tdel_timer_sync(&ppd->led_override_timer);\n\t\t\tatomic_set(&ppd->led_override_timer_active, 0);\n\t\t}\n\t\tif (ppd->symerr_clear_timer.function)\n\t\t\tdel_timer_sync(&ppd->symerr_clear_timer);\n\t}\n}\n\n \nstatic void qib_shutdown_device(struct qib_devdata *dd)\n{\n\tstruct qib_pportdata *ppd;\n\tunsigned pidx;\n\n\tif (dd->flags & QIB_SHUTDOWN)\n\t\treturn;\n\tdd->flags |= QIB_SHUTDOWN;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\n\t\tspin_lock_irq(&ppd->lflags_lock);\n\t\tppd->lflags &= ~(QIBL_LINKDOWN | QIBL_LINKINIT |\n\t\t\t\t QIBL_LINKARMED | QIBL_LINKACTIVE |\n\t\t\t\t QIBL_LINKV);\n\t\tspin_unlock_irq(&ppd->lflags_lock);\n\t\t*ppd->statusp &= ~(QIB_STATUS_IB_CONF | QIB_STATUS_IB_READY);\n\t}\n\tdd->flags &= ~QIB_INITTED;\n\n\t \n\tdd->f_set_intr_state(dd, 0);\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tdd->f_rcvctrl(ppd, QIB_RCVCTRL_TAILUPD_DIS |\n\t\t\t\t   QIB_RCVCTRL_CTXT_DIS |\n\t\t\t\t   QIB_RCVCTRL_INTRAVAIL_DIS |\n\t\t\t\t   QIB_RCVCTRL_PKEY_ENB, -1);\n\t\t \n\t\tdd->f_sendctrl(ppd, QIB_SENDCTRL_CLEAR);\n\t}\n\n\t \n\tudelay(20);\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tdd->f_setextled(ppd, 0);  \n\n\t\tif (dd->flags & QIB_HAS_SEND_DMA)\n\t\t\tqib_teardown_sdma(ppd);\n\n\t\tdd->f_sendctrl(ppd, QIB_SENDCTRL_AVAIL_DIS |\n\t\t\t\t    QIB_SENDCTRL_SEND_DIS);\n\t\t \n\t\tdd->f_quiet_serdes(ppd);\n\n\t\tif (ppd->qib_wq) {\n\t\t\tdestroy_workqueue(ppd->qib_wq);\n\t\t\tppd->qib_wq = NULL;\n\t\t}\n\t\tqib_free_pportdata(ppd);\n\t}\n\n}\n\n \nvoid qib_free_ctxtdata(struct qib_devdata *dd, struct qib_ctxtdata *rcd)\n{\n\tif (!rcd)\n\t\treturn;\n\n\tif (rcd->rcvhdrq) {\n\t\tdma_free_coherent(&dd->pcidev->dev, rcd->rcvhdrq_size,\n\t\t\t\t  rcd->rcvhdrq, rcd->rcvhdrq_phys);\n\t\trcd->rcvhdrq = NULL;\n\t\tif (rcd->rcvhdrtail_kvaddr) {\n\t\t\tdma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\n\t\t\t\t\t  rcd->rcvhdrtail_kvaddr,\n\t\t\t\t\t  rcd->rcvhdrqtailaddr_phys);\n\t\t\trcd->rcvhdrtail_kvaddr = NULL;\n\t\t}\n\t}\n\tif (rcd->rcvegrbuf) {\n\t\tunsigned e;\n\n\t\tfor (e = 0; e < rcd->rcvegrbuf_chunks; e++) {\n\t\t\tvoid *base = rcd->rcvegrbuf[e];\n\t\t\tsize_t size = rcd->rcvegrbuf_size;\n\n\t\t\tdma_free_coherent(&dd->pcidev->dev, size,\n\t\t\t\t\t  base, rcd->rcvegrbuf_phys[e]);\n\t\t}\n\t\tkfree(rcd->rcvegrbuf);\n\t\trcd->rcvegrbuf = NULL;\n\t\tkfree(rcd->rcvegrbuf_phys);\n\t\trcd->rcvegrbuf_phys = NULL;\n\t\trcd->rcvegrbuf_chunks = 0;\n\t}\n\n\tkfree(rcd->tid_pg_list);\n\tvfree(rcd->user_event_mask);\n\tvfree(rcd->subctxt_uregbase);\n\tvfree(rcd->subctxt_rcvegrbuf);\n\tvfree(rcd->subctxt_rcvhdr_base);\n#ifdef CONFIG_DEBUG_FS\n\tkfree(rcd->opstats);\n\trcd->opstats = NULL;\n#endif\n\tkfree(rcd);\n}\n\n \nstatic void qib_verify_pioperf(struct qib_devdata *dd)\n{\n\tu32 pbnum, cnt, lcnt;\n\tu32 __iomem *piobuf;\n\tu32 *addr;\n\tu64 msecs, emsecs;\n\n\tpiobuf = dd->f_getsendbuf(dd->pport, 0ULL, &pbnum);\n\tif (!piobuf) {\n\t\tqib_devinfo(dd->pcidev,\n\t\t\t \"No PIObufs for checking perf, skipping\\n\");\n\t\treturn;\n\t}\n\n\t \n\tcnt = 1024;\n\n\taddr = vmalloc(cnt);\n\tif (!addr)\n\t\tgoto done;\n\n\tpreempt_disable();   \n\tmsecs = 1 + jiffies_to_msecs(jiffies);\n\tfor (lcnt = 0; lcnt < 10000U; lcnt++) {\n\t\t \n\t\tif (jiffies_to_msecs(jiffies) >= msecs)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tdd->f_set_armlaunch(dd, 0);\n\n\t \n\twriteq(0, piobuf);\n\tqib_flush_wc();\n\n\t \n\tmsecs = jiffies_to_msecs(jiffies);\n\tfor (emsecs = lcnt = 0; emsecs <= 5UL; lcnt++) {\n\t\tqib_pio_copy(piobuf + 64, addr, cnt >> 2);\n\t\temsecs = jiffies_to_msecs(jiffies) - msecs;\n\t}\n\n\t \n\tif (lcnt < (emsecs * 1024U))\n\t\tqib_dev_err(dd,\n\t\t\t    \"Performance problem: bandwidth to PIO buffers is only %u MiB/sec\\n\",\n\t\t\t    lcnt / (u32) emsecs);\n\n\tpreempt_enable();\n\n\tvfree(addr);\n\ndone:\n\t \n\tdd->f_sendctrl(dd->pport, QIB_SENDCTRL_DISARM_BUF(pbnum));\n\tqib_sendbuf_done(dd, pbnum);\n\tdd->f_set_armlaunch(dd, 1);\n}\n\nvoid qib_free_devdata(struct qib_devdata *dd)\n{\n\tunsigned long flags;\n\n\txa_lock_irqsave(&qib_dev_table, flags);\n\t__xa_erase(&qib_dev_table, dd->unit);\n\txa_unlock_irqrestore(&qib_dev_table, flags);\n\n#ifdef CONFIG_DEBUG_FS\n\tqib_dbg_ibdev_exit(&dd->verbs_dev);\n#endif\n\tfree_percpu(dd->int_counter);\n\trvt_dealloc_device(&dd->verbs_dev.rdi);\n}\n\nu64 qib_int_counter(struct qib_devdata *dd)\n{\n\tint cpu;\n\tu64 int_counter = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tint_counter += *per_cpu_ptr(dd->int_counter, cpu);\n\treturn int_counter;\n}\n\nu64 qib_sps_ints(void)\n{\n\tunsigned long index, flags;\n\tstruct qib_devdata *dd;\n\tu64 sps_ints = 0;\n\n\txa_lock_irqsave(&qib_dev_table, flags);\n\txa_for_each(&qib_dev_table, index, dd) {\n\t\tsps_ints += qib_int_counter(dd);\n\t}\n\txa_unlock_irqrestore(&qib_dev_table, flags);\n\treturn sps_ints;\n}\n\n \nstruct qib_devdata *qib_alloc_devdata(struct pci_dev *pdev, size_t extra)\n{\n\tstruct qib_devdata *dd;\n\tint ret, nports;\n\n\t \n\tnports = extra / sizeof(struct qib_pportdata);\n\tdd = (struct qib_devdata *)rvt_alloc_device(sizeof(*dd) + extra,\n\t\t\t\t\t\t    nports);\n\tif (!dd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = xa_alloc_irq(&qib_dev_table, &dd->unit, dd, xa_limit_32b,\n\t\t\tGFP_KERNEL);\n\tif (ret < 0) {\n\t\tqib_early_err(&pdev->dev,\n\t\t\t      \"Could not allocate unit ID: error %d\\n\", -ret);\n\t\tgoto bail;\n\t}\n\trvt_set_ibdev_name(&dd->verbs_dev.rdi, \"%s%d\", \"qib\", dd->unit);\n\n\tdd->int_counter = alloc_percpu(u64);\n\tif (!dd->int_counter) {\n\t\tret = -ENOMEM;\n\t\tqib_early_err(&pdev->dev,\n\t\t\t      \"Could not allocate per-cpu int_counter\\n\");\n\t\tgoto bail;\n\t}\n\n\tif (!qib_cpulist_count) {\n\t\tu32 count = num_online_cpus();\n\n\t\tqib_cpulist = bitmap_zalloc(count, GFP_KERNEL);\n\t\tif (qib_cpulist)\n\t\t\tqib_cpulist_count = count;\n\t}\n#ifdef CONFIG_DEBUG_FS\n\tqib_dbg_ibdev_init(&dd->verbs_dev);\n#endif\n\treturn dd;\nbail:\n\tif (!list_empty(&dd->list))\n\t\tlist_del_init(&dd->list);\n\trvt_dealloc_device(&dd->verbs_dev.rdi);\n\treturn ERR_PTR(ret);\n}\n\n \nvoid qib_disable_after_error(struct qib_devdata *dd)\n{\n\tif (dd->flags & QIB_INITTED) {\n\t\tu32 pidx;\n\n\t\tdd->flags &= ~QIB_INITTED;\n\t\tif (dd->pport)\n\t\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\t\t\tstruct qib_pportdata *ppd;\n\n\t\t\t\tppd = dd->pport + pidx;\n\t\t\t\tif (dd->flags & QIB_PRESENT) {\n\t\t\t\t\tqib_set_linkstate(ppd,\n\t\t\t\t\t\tQIB_IB_LINKDOWN_DISABLE);\n\t\t\t\t\tdd->f_setextled(ppd, 0);\n\t\t\t\t}\n\t\t\t\t*ppd->statusp &= ~QIB_STATUS_IB_READY;\n\t\t\t}\n\t}\n\n\t \n\tif (dd->devstatusp)\n\t\t*dd->devstatusp |= QIB_STATUS_HWERROR;\n}\n\nstatic void qib_remove_one(struct pci_dev *);\nstatic int qib_init_one(struct pci_dev *, const struct pci_device_id *);\nstatic void qib_shutdown_one(struct pci_dev *);\n\n#define DRIVER_LOAD_MSG \"Intel \" QIB_DRV_NAME \" loaded: \"\n#define PFX QIB_DRV_NAME \": \"\n\nstatic const struct pci_device_id qib_pci_tbl[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_PATHSCALE, PCI_DEVICE_ID_QLOGIC_IB_6120) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_QLOGIC, PCI_DEVICE_ID_QLOGIC_IB_7220) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_QLOGIC, PCI_DEVICE_ID_QLOGIC_IB_7322) },\n\t{ 0, }\n};\n\nMODULE_DEVICE_TABLE(pci, qib_pci_tbl);\n\nstatic struct pci_driver qib_driver = {\n\t.name = QIB_DRV_NAME,\n\t.probe = qib_init_one,\n\t.remove = qib_remove_one,\n\t.shutdown = qib_shutdown_one,\n\t.id_table = qib_pci_tbl,\n\t.err_handler = &qib_pci_err_handler,\n};\n\n#ifdef CONFIG_INFINIBAND_QIB_DCA\n\nstatic int qib_notify_dca(struct notifier_block *, unsigned long, void *);\nstatic struct notifier_block dca_notifier = {\n\t.notifier_call  = qib_notify_dca,\n\t.next           = NULL,\n\t.priority       = 0\n};\n\nstatic int qib_notify_dca_device(struct device *device, void *data)\n{\n\tstruct qib_devdata *dd = dev_get_drvdata(device);\n\tunsigned long event = *(unsigned long *)data;\n\n\treturn dd->f_notify_dca(dd, event);\n}\n\nstatic int qib_notify_dca(struct notifier_block *nb, unsigned long event,\n\t\t\t\t\t  void *p)\n{\n\tint rval;\n\n\trval = driver_for_each_device(&qib_driver.driver, NULL,\n\t\t\t\t      &event, qib_notify_dca_device);\n\treturn rval ? NOTIFY_BAD : NOTIFY_DONE;\n}\n\n#endif\n\n \nstatic int __init qib_ib_init(void)\n{\n\tint ret;\n\n\tret = qib_dev_init();\n\tif (ret)\n\t\tgoto bail;\n\n\t \n#ifdef CONFIG_INFINIBAND_QIB_DCA\n\tdca_register_notify(&dca_notifier);\n#endif\n#ifdef CONFIG_DEBUG_FS\n\tqib_dbg_init();\n#endif\n\tret = pci_register_driver(&qib_driver);\n\tif (ret < 0) {\n\t\tpr_err(\"Unable to register driver: error %d\\n\", -ret);\n\t\tgoto bail_dev;\n\t}\n\n\t \n\tif (qib_init_qibfs())\n\t\tpr_err(\"Unable to register ipathfs\\n\");\n\tgoto bail;  \n\nbail_dev:\n#ifdef CONFIG_INFINIBAND_QIB_DCA\n\tdca_unregister_notify(&dca_notifier);\n#endif\n#ifdef CONFIG_DEBUG_FS\n\tqib_dbg_exit();\n#endif\n\tqib_dev_cleanup();\nbail:\n\treturn ret;\n}\n\nmodule_init(qib_ib_init);\n\n \nstatic void __exit qib_ib_cleanup(void)\n{\n\tint ret;\n\n\tret = qib_exit_qibfs();\n\tif (ret)\n\t\tpr_err(\n\t\t\t\"Unable to cleanup counter filesystem: error %d\\n\",\n\t\t\t-ret);\n\n#ifdef CONFIG_INFINIBAND_QIB_DCA\n\tdca_unregister_notify(&dca_notifier);\n#endif\n\tpci_unregister_driver(&qib_driver);\n#ifdef CONFIG_DEBUG_FS\n\tqib_dbg_exit();\n#endif\n\n\tqib_cpulist_count = 0;\n\tbitmap_free(qib_cpulist);\n\n\tWARN_ON(!xa_empty(&qib_dev_table));\n\tqib_dev_cleanup();\n}\n\nmodule_exit(qib_ib_cleanup);\n\n \nstatic void cleanup_device_data(struct qib_devdata *dd)\n{\n\tint ctxt;\n\tint pidx;\n\tstruct qib_ctxtdata **tmp;\n\tunsigned long flags;\n\n\t \n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tif (dd->pport[pidx].statusp)\n\t\t\t*dd->pport[pidx].statusp &= ~QIB_STATUS_CHIP_PRESENT;\n\n\t\tspin_lock(&dd->pport[pidx].cc_shadow_lock);\n\n\t\tkfree(dd->pport[pidx].congestion_entries);\n\t\tdd->pport[pidx].congestion_entries = NULL;\n\t\tkfree(dd->pport[pidx].ccti_entries);\n\t\tdd->pport[pidx].ccti_entries = NULL;\n\t\tkfree(dd->pport[pidx].ccti_entries_shadow);\n\t\tdd->pport[pidx].ccti_entries_shadow = NULL;\n\t\tkfree(dd->pport[pidx].congestion_entries_shadow);\n\t\tdd->pport[pidx].congestion_entries_shadow = NULL;\n\n\t\tspin_unlock(&dd->pport[pidx].cc_shadow_lock);\n\t}\n\n\tqib_disable_wc(dd);\n\n\tif (dd->pioavailregs_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\n\t\t\t\t  (void *) dd->pioavailregs_dma,\n\t\t\t\t  dd->pioavailregs_phys);\n\t\tdd->pioavailregs_dma = NULL;\n\t}\n\n\tif (dd->pageshadow) {\n\t\tstruct page **tmpp = dd->pageshadow;\n\t\tdma_addr_t *tmpd = dd->physshadow;\n\t\tint i;\n\n\t\tfor (ctxt = 0; ctxt < dd->cfgctxts; ctxt++) {\n\t\t\tint ctxt_tidbase = ctxt * dd->rcvtidcnt;\n\t\t\tint maxtid = ctxt_tidbase + dd->rcvtidcnt;\n\n\t\t\tfor (i = ctxt_tidbase; i < maxtid; i++) {\n\t\t\t\tif (!tmpp[i])\n\t\t\t\t\tcontinue;\n\t\t\t\tdma_unmap_page(&dd->pcidev->dev, tmpd[i],\n\t\t\t\t\t       PAGE_SIZE, DMA_FROM_DEVICE);\n\t\t\t\tqib_release_user_pages(&tmpp[i], 1);\n\t\t\t\ttmpp[i] = NULL;\n\t\t\t}\n\t\t}\n\n\t\tdd->pageshadow = NULL;\n\t\tvfree(tmpp);\n\t\tdd->physshadow = NULL;\n\t\tvfree(tmpd);\n\t}\n\n\t \n\tspin_lock_irqsave(&dd->uctxt_lock, flags);\n\ttmp = dd->rcd;\n\tdd->rcd = NULL;\n\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\tfor (ctxt = 0; tmp && ctxt < dd->ctxtcnt; ctxt++) {\n\t\tstruct qib_ctxtdata *rcd = tmp[ctxt];\n\n\t\ttmp[ctxt] = NULL;  \n\t\tqib_free_ctxtdata(dd, rcd);\n\t}\n\tkfree(tmp);\n}\n\n \nstatic void qib_postinit_cleanup(struct qib_devdata *dd)\n{\n\t \n\tif (dd->f_cleanup)\n\t\tdd->f_cleanup(dd);\n\n\tqib_pcie_ddcleanup(dd);\n\n\tcleanup_device_data(dd);\n\n\tqib_free_devdata(dd);\n}\n\nstatic int qib_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tint ret, j, pidx, initfail;\n\tstruct qib_devdata *dd = NULL;\n\n\tret = qib_pcie_init(pdev, ent);\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tswitch (ent->device) {\n\tcase PCI_DEVICE_ID_QLOGIC_IB_6120:\n#ifdef CONFIG_PCI_MSI\n\t\tdd = qib_init_iba6120_funcs(pdev, ent);\n#else\n\t\tqib_early_err(&pdev->dev,\n\t\t\t\"Intel PCIE device 0x%x cannot work if CONFIG_PCI_MSI is not enabled\\n\",\n\t\t\tent->device);\n\t\tdd = ERR_PTR(-ENODEV);\n#endif\n\t\tbreak;\n\n\tcase PCI_DEVICE_ID_QLOGIC_IB_7220:\n\t\tdd = qib_init_iba7220_funcs(pdev, ent);\n\t\tbreak;\n\n\tcase PCI_DEVICE_ID_QLOGIC_IB_7322:\n\t\tdd = qib_init_iba7322_funcs(pdev, ent);\n\t\tbreak;\n\n\tdefault:\n\t\tqib_early_err(&pdev->dev,\n\t\t\t\"Failing on unknown Intel deviceid 0x%x\\n\",\n\t\t\tent->device);\n\t\tret = -ENODEV;\n\t}\n\n\tif (IS_ERR(dd))\n\t\tret = PTR_ERR(dd);\n\tif (ret)\n\t\tgoto bail;  \n\n\tret = qib_create_workqueues(dd);\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tinitfail = qib_init(dd, 0);\n\n\tret = qib_register_ib_device(dd);\n\n\t \n\tif (!qib_mini_init && !initfail && !ret)\n\t\tdd->flags |= QIB_INITTED;\n\n\tj = qib_device_create(dd);\n\tif (j)\n\t\tqib_dev_err(dd, \"Failed to create /dev devices: %d\\n\", -j);\n\tj = qibfs_add(dd);\n\tif (j)\n\t\tqib_dev_err(dd, \"Failed filesystem setup for counters: %d\\n\",\n\t\t\t    -j);\n\n\tif (qib_mini_init || initfail || ret) {\n\t\tqib_stop_timers(dd);\n\t\tflush_workqueue(ib_wq);\n\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx)\n\t\t\tdd->f_quiet_serdes(dd->pport + pidx);\n\t\tif (qib_mini_init)\n\t\t\tgoto bail;\n\t\tif (!j) {\n\t\t\t(void) qibfs_remove(dd);\n\t\t\tqib_device_remove(dd);\n\t\t}\n\t\tif (!ret)\n\t\t\tqib_unregister_ib_device(dd);\n\t\tqib_postinit_cleanup(dd);\n\t\tif (initfail)\n\t\t\tret = initfail;\n\t\tgoto bail;\n\t}\n\n\tret = qib_enable_wc(dd);\n\tif (ret) {\n\t\tqib_dev_err(dd,\n\t\t\t\"Write combining not enabled (err %d): performance may be poor\\n\",\n\t\t\t-ret);\n\t\tret = 0;\n\t}\n\n\tqib_verify_pioperf(dd);\nbail:\n\treturn ret;\n}\n\nstatic void qib_remove_one(struct pci_dev *pdev)\n{\n\tstruct qib_devdata *dd = pci_get_drvdata(pdev);\n\tint ret;\n\n\t \n\tqib_unregister_ib_device(dd);\n\n\t \n\tif (!qib_mini_init)\n\t\tqib_shutdown_device(dd);\n\n\tqib_stop_timers(dd);\n\n\t \n\tflush_workqueue(ib_wq);\n\n\tret = qibfs_remove(dd);\n\tif (ret)\n\t\tqib_dev_err(dd, \"Failed counters filesystem cleanup: %d\\n\",\n\t\t\t    -ret);\n\n\tqib_device_remove(dd);\n\n\tqib_postinit_cleanup(dd);\n}\n\nstatic void qib_shutdown_one(struct pci_dev *pdev)\n{\n\tstruct qib_devdata *dd = pci_get_drvdata(pdev);\n\n\tqib_shutdown_device(dd);\n}\n\n \nint qib_create_rcvhdrq(struct qib_devdata *dd, struct qib_ctxtdata *rcd)\n{\n\tunsigned amt;\n\tint old_node_id;\n\n\tif (!rcd->rcvhdrq) {\n\t\tdma_addr_t phys_hdrqtail;\n\n\t\tamt = ALIGN(dd->rcvhdrcnt * dd->rcvhdrentsize *\n\t\t\t    sizeof(u32), PAGE_SIZE);\n\n\t\told_node_id = dev_to_node(&dd->pcidev->dev);\n\t\tset_dev_node(&dd->pcidev->dev, rcd->node_id);\n\t\trcd->rcvhdrq = dma_alloc_coherent(&dd->pcidev->dev, amt,\n\t\t\t\t&rcd->rcvhdrq_phys, GFP_KERNEL);\n\t\tset_dev_node(&dd->pcidev->dev, old_node_id);\n\n\t\tif (!rcd->rcvhdrq) {\n\t\t\tqib_dev_err(dd,\n\t\t\t\t\"attempt to allocate %d bytes for ctxt %u rcvhdrq failed\\n\",\n\t\t\t\tamt, rcd->ctxt);\n\t\t\tgoto bail;\n\t\t}\n\n\t\tif (rcd->ctxt >= dd->first_user_ctxt) {\n\t\t\trcd->user_event_mask = vmalloc_user(PAGE_SIZE);\n\t\t\tif (!rcd->user_event_mask)\n\t\t\t\tgoto bail_free_hdrq;\n\t\t}\n\n\t\tif (!(dd->flags & QIB_NODMA_RTAIL)) {\n\t\t\tset_dev_node(&dd->pcidev->dev, rcd->node_id);\n\t\t\trcd->rcvhdrtail_kvaddr = dma_alloc_coherent(\n\t\t\t\t&dd->pcidev->dev, PAGE_SIZE, &phys_hdrqtail,\n\t\t\t\tGFP_KERNEL);\n\t\t\tset_dev_node(&dd->pcidev->dev, old_node_id);\n\t\t\tif (!rcd->rcvhdrtail_kvaddr)\n\t\t\t\tgoto bail_free;\n\t\t\trcd->rcvhdrqtailaddr_phys = phys_hdrqtail;\n\t\t}\n\n\t\trcd->rcvhdrq_size = amt;\n\t}\n\n\t \n\tmemset(rcd->rcvhdrq, 0, rcd->rcvhdrq_size);\n\tif (rcd->rcvhdrtail_kvaddr)\n\t\tmemset(rcd->rcvhdrtail_kvaddr, 0, PAGE_SIZE);\n\treturn 0;\n\nbail_free:\n\tqib_dev_err(dd,\n\t\t\"attempt to allocate 1 page for ctxt %u rcvhdrqtailaddr failed\\n\",\n\t\trcd->ctxt);\n\tvfree(rcd->user_event_mask);\n\trcd->user_event_mask = NULL;\nbail_free_hdrq:\n\tdma_free_coherent(&dd->pcidev->dev, amt, rcd->rcvhdrq,\n\t\t\t  rcd->rcvhdrq_phys);\n\trcd->rcvhdrq = NULL;\nbail:\n\treturn -ENOMEM;\n}\n\n \nint qib_setup_eagerbufs(struct qib_ctxtdata *rcd)\n{\n\tstruct qib_devdata *dd = rcd->dd;\n\tunsigned e, egrcnt, egrperchunk, chunk, egrsize, egroff;\n\tsize_t size;\n\tint old_node_id;\n\n\tegrcnt = rcd->rcvegrcnt;\n\tegroff = rcd->rcvegr_tid_base;\n\tegrsize = dd->rcvegrbufsize;\n\n\tchunk = rcd->rcvegrbuf_chunks;\n\tegrperchunk = rcd->rcvegrbufs_perchunk;\n\tsize = rcd->rcvegrbuf_size;\n\tif (!rcd->rcvegrbuf) {\n\t\trcd->rcvegrbuf =\n\t\t\tkcalloc_node(chunk, sizeof(rcd->rcvegrbuf[0]),\n\t\t\t\t     GFP_KERNEL, rcd->node_id);\n\t\tif (!rcd->rcvegrbuf)\n\t\t\tgoto bail;\n\t}\n\tif (!rcd->rcvegrbuf_phys) {\n\t\trcd->rcvegrbuf_phys =\n\t\t\tkmalloc_array_node(chunk,\n\t\t\t\t\t   sizeof(rcd->rcvegrbuf_phys[0]),\n\t\t\t\t\t   GFP_KERNEL, rcd->node_id);\n\t\tif (!rcd->rcvegrbuf_phys)\n\t\t\tgoto bail_rcvegrbuf;\n\t}\n\tfor (e = 0; e < rcd->rcvegrbuf_chunks; e++) {\n\t\tif (rcd->rcvegrbuf[e])\n\t\t\tcontinue;\n\n\t\told_node_id = dev_to_node(&dd->pcidev->dev);\n\t\tset_dev_node(&dd->pcidev->dev, rcd->node_id);\n\t\trcd->rcvegrbuf[e] =\n\t\t\tdma_alloc_coherent(&dd->pcidev->dev, size,\n\t\t\t\t\t   &rcd->rcvegrbuf_phys[e],\n\t\t\t\t\t   GFP_KERNEL);\n\t\tset_dev_node(&dd->pcidev->dev, old_node_id);\n\t\tif (!rcd->rcvegrbuf[e])\n\t\t\tgoto bail_rcvegrbuf_phys;\n\t}\n\n\trcd->rcvegr_phys = rcd->rcvegrbuf_phys[0];\n\n\tfor (e = chunk = 0; chunk < rcd->rcvegrbuf_chunks; chunk++) {\n\t\tdma_addr_t pa = rcd->rcvegrbuf_phys[chunk];\n\t\tunsigned i;\n\n\t\t \n\t\tmemset(rcd->rcvegrbuf[chunk], 0, size);\n\n\t\tfor (i = 0; e < egrcnt && i < egrperchunk; e++, i++) {\n\t\t\tdd->f_put_tid(dd, e + egroff +\n\t\t\t\t\t  (u64 __iomem *)\n\t\t\t\t\t  ((char __iomem *)\n\t\t\t\t\t   dd->kregbase +\n\t\t\t\t\t   dd->rcvegrbase),\n\t\t\t\t\t  RCVHQ_RCV_TYPE_EAGER, pa);\n\t\t\tpa += egrsize;\n\t\t}\n\t\tcond_resched();  \n\t}\n\n\treturn 0;\n\nbail_rcvegrbuf_phys:\n\tfor (e = 0; e < rcd->rcvegrbuf_chunks && rcd->rcvegrbuf[e]; e++)\n\t\tdma_free_coherent(&dd->pcidev->dev, size,\n\t\t\t\t  rcd->rcvegrbuf[e], rcd->rcvegrbuf_phys[e]);\n\tkfree(rcd->rcvegrbuf_phys);\n\trcd->rcvegrbuf_phys = NULL;\nbail_rcvegrbuf:\n\tkfree(rcd->rcvegrbuf);\n\trcd->rcvegrbuf = NULL;\nbail:\n\treturn -ENOMEM;\n}\n\n \nint init_chip_wc_pat(struct qib_devdata *dd, u32 vl15buflen)\n{\n\tu64 __iomem *qib_kregbase = NULL;\n\tvoid __iomem *qib_piobase = NULL;\n\tu64 __iomem *qib_userbase = NULL;\n\tu64 qib_kreglen;\n\tu64 qib_pio2koffset = dd->piobufbase & 0xffffffff;\n\tu64 qib_pio4koffset = dd->piobufbase >> 32;\n\tu64 qib_pio2klen = dd->piobcnt2k * dd->palign;\n\tu64 qib_pio4klen = dd->piobcnt4k * dd->align4k;\n\tu64 qib_physaddr = dd->physaddr;\n\tu64 qib_piolen;\n\tu64 qib_userlen = 0;\n\n\t \n\tiounmap(dd->kregbase);\n\tdd->kregbase = NULL;\n\n\t \n\tif (dd->piobcnt4k == 0) {\n\t\tqib_kreglen = qib_pio2koffset;\n\t\tqib_piolen = qib_pio2klen;\n\t} else if (qib_pio2koffset < qib_pio4koffset) {\n\t\tqib_kreglen = qib_pio2koffset;\n\t\tqib_piolen = qib_pio4koffset + qib_pio4klen - qib_kreglen;\n\t} else {\n\t\tqib_kreglen = qib_pio4koffset;\n\t\tqib_piolen = qib_pio2koffset + qib_pio2klen - qib_kreglen;\n\t}\n\tqib_piolen += vl15buflen;\n\t \n\tif (dd->uregbase > qib_kreglen)\n\t\tqib_userlen = dd->ureg_align * dd->cfgctxts;\n\n\t \n\tqib_kregbase = ioremap(qib_physaddr, qib_kreglen);\n\tif (!qib_kregbase)\n\t\tgoto bail;\n\n\tqib_piobase = ioremap_wc(qib_physaddr + qib_kreglen, qib_piolen);\n\tif (!qib_piobase)\n\t\tgoto bail_kregbase;\n\n\tif (qib_userlen) {\n\t\tqib_userbase = ioremap(qib_physaddr + dd->uregbase,\n\t\t\t\t\t       qib_userlen);\n\t\tif (!qib_userbase)\n\t\t\tgoto bail_piobase;\n\t}\n\n\tdd->kregbase = qib_kregbase;\n\tdd->kregend = (u64 __iomem *)\n\t\t((char __iomem *) qib_kregbase + qib_kreglen);\n\tdd->piobase = qib_piobase;\n\tdd->pio2kbase = (void __iomem *)\n\t\t(((char __iomem *) dd->piobase) +\n\t\t qib_pio2koffset - qib_kreglen);\n\tif (dd->piobcnt4k)\n\t\tdd->pio4kbase = (void __iomem *)\n\t\t\t(((char __iomem *) dd->piobase) +\n\t\t\t qib_pio4koffset - qib_kreglen);\n\tif (qib_userlen)\n\t\t \n\t\tdd->userbase = qib_userbase;\n\treturn 0;\n\nbail_piobase:\n\tiounmap(qib_piobase);\nbail_kregbase:\n\tiounmap(qib_kregbase);\nbail:\n\treturn -ENOMEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}