{
  "module_name": "qib_verbs.c",
  "hash_id": "b0bc544129f78b89ead3019c2cd0cc470b427b1073854edd4fc31dcd357aa1f4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qib/qib_verbs.c",
  "human_readable_source": " \n\n#include <rdma/ib_mad.h>\n#include <rdma/ib_user_verbs.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/utsname.h>\n#include <linux/rculist.h>\n#include <linux/mm.h>\n#include <linux/vmalloc.h>\n#include <rdma/rdma_vt.h>\n\n#include \"qib.h\"\n#include \"qib_common.h\"\n\nstatic unsigned int ib_qib_qp_table_size = 256;\nmodule_param_named(qp_table_size, ib_qib_qp_table_size, uint, S_IRUGO);\nMODULE_PARM_DESC(qp_table_size, \"QP table size\");\n\nstatic unsigned int qib_lkey_table_size = 16;\nmodule_param_named(lkey_table_size, qib_lkey_table_size, uint,\n\t\t   S_IRUGO);\nMODULE_PARM_DESC(lkey_table_size,\n\t\t \"LKEY table size in bits (2^n, 1 <= n <= 23)\");\n\nstatic unsigned int ib_qib_max_pds = 0xFFFF;\nmodule_param_named(max_pds, ib_qib_max_pds, uint, S_IRUGO);\nMODULE_PARM_DESC(max_pds,\n\t\t \"Maximum number of protection domains to support\");\n\nstatic unsigned int ib_qib_max_ahs = 0xFFFF;\nmodule_param_named(max_ahs, ib_qib_max_ahs, uint, S_IRUGO);\nMODULE_PARM_DESC(max_ahs, \"Maximum number of address handles to support\");\n\nunsigned int ib_qib_max_cqes = 0x2FFFF;\nmodule_param_named(max_cqes, ib_qib_max_cqes, uint, S_IRUGO);\nMODULE_PARM_DESC(max_cqes,\n\t\t \"Maximum number of completion queue entries to support\");\n\nunsigned int ib_qib_max_cqs = 0x1FFFF;\nmodule_param_named(max_cqs, ib_qib_max_cqs, uint, S_IRUGO);\nMODULE_PARM_DESC(max_cqs, \"Maximum number of completion queues to support\");\n\nunsigned int ib_qib_max_qp_wrs = 0x3FFF;\nmodule_param_named(max_qp_wrs, ib_qib_max_qp_wrs, uint, S_IRUGO);\nMODULE_PARM_DESC(max_qp_wrs, \"Maximum number of QP WRs to support\");\n\nunsigned int ib_qib_max_qps = 16384;\nmodule_param_named(max_qps, ib_qib_max_qps, uint, S_IRUGO);\nMODULE_PARM_DESC(max_qps, \"Maximum number of QPs to support\");\n\nunsigned int ib_qib_max_sges = 0x60;\nmodule_param_named(max_sges, ib_qib_max_sges, uint, S_IRUGO);\nMODULE_PARM_DESC(max_sges, \"Maximum number of SGEs to support\");\n\nunsigned int ib_qib_max_mcast_grps = 16384;\nmodule_param_named(max_mcast_grps, ib_qib_max_mcast_grps, uint, S_IRUGO);\nMODULE_PARM_DESC(max_mcast_grps,\n\t\t \"Maximum number of multicast groups to support\");\n\nunsigned int ib_qib_max_mcast_qp_attached = 16;\nmodule_param_named(max_mcast_qp_attached, ib_qib_max_mcast_qp_attached,\n\t\t   uint, S_IRUGO);\nMODULE_PARM_DESC(max_mcast_qp_attached,\n\t\t \"Maximum number of attached QPs to support\");\n\nunsigned int ib_qib_max_srqs = 1024;\nmodule_param_named(max_srqs, ib_qib_max_srqs, uint, S_IRUGO);\nMODULE_PARM_DESC(max_srqs, \"Maximum number of SRQs to support\");\n\nunsigned int ib_qib_max_srq_sges = 128;\nmodule_param_named(max_srq_sges, ib_qib_max_srq_sges, uint, S_IRUGO);\nMODULE_PARM_DESC(max_srq_sges, \"Maximum number of SRQ SGEs to support\");\n\nunsigned int ib_qib_max_srq_wrs = 0x1FFFF;\nmodule_param_named(max_srq_wrs, ib_qib_max_srq_wrs, uint, S_IRUGO);\nMODULE_PARM_DESC(max_srq_wrs, \"Maximum number of SRQ WRs support\");\n\nstatic unsigned int ib_qib_disable_sma;\nmodule_param_named(disable_sma, ib_qib_disable_sma, uint, S_IWUSR | S_IRUGO);\nMODULE_PARM_DESC(disable_sma, \"Disable the SMA\");\n\n \nconst enum ib_wc_opcode ib_qib_wc_opcode[] = {\n\t[IB_WR_RDMA_WRITE] = IB_WC_RDMA_WRITE,\n\t[IB_WR_RDMA_WRITE_WITH_IMM] = IB_WC_RDMA_WRITE,\n\t[IB_WR_SEND] = IB_WC_SEND,\n\t[IB_WR_SEND_WITH_IMM] = IB_WC_SEND,\n\t[IB_WR_RDMA_READ] = IB_WC_RDMA_READ,\n\t[IB_WR_ATOMIC_CMP_AND_SWP] = IB_WC_COMP_SWAP,\n\t[IB_WR_ATOMIC_FETCH_AND_ADD] = IB_WC_FETCH_ADD\n};\n\n \n__be64 ib_qib_sys_image_guid;\n\n \nstatic u32 qib_count_sge(struct rvt_sge_state *ss, u32 length)\n{\n\tstruct rvt_sge *sg_list = ss->sg_list;\n\tstruct rvt_sge sge = ss->sge;\n\tu8 num_sge = ss->num_sge;\n\tu32 ndesc = 1;   \n\n\twhile (length) {\n\t\tu32 len = rvt_get_sge_length(&sge, length);\n\n\t\tif (((long) sge.vaddr & (sizeof(u32) - 1)) ||\n\t\t    (len != length && (len & (sizeof(u32) - 1)))) {\n\t\t\tndesc = 0;\n\t\t\tbreak;\n\t\t}\n\t\tndesc++;\n\t\tsge.vaddr += len;\n\t\tsge.length -= len;\n\t\tsge.sge_length -= len;\n\t\tif (sge.sge_length == 0) {\n\t\t\tif (--num_sge)\n\t\t\t\tsge = *sg_list++;\n\t\t} else if (sge.length == 0 && sge.mr->lkey) {\n\t\t\tif (++sge.n >= RVT_SEGSZ) {\n\t\t\t\tif (++sge.m >= sge.mr->mapsz)\n\t\t\t\t\tbreak;\n\t\t\t\tsge.n = 0;\n\t\t\t}\n\t\t\tsge.vaddr =\n\t\t\t\tsge.mr->map[sge.m]->segs[sge.n].vaddr;\n\t\t\tsge.length =\n\t\t\t\tsge.mr->map[sge.m]->segs[sge.n].length;\n\t\t}\n\t\tlength -= len;\n\t}\n\treturn ndesc;\n}\n\n \nstatic void qib_copy_from_sge(void *data, struct rvt_sge_state *ss, u32 length)\n{\n\tstruct rvt_sge *sge = &ss->sge;\n\n\twhile (length) {\n\t\tu32 len = rvt_get_sge_length(sge, length);\n\n\t\tmemcpy(data, sge->vaddr, len);\n\t\tsge->vaddr += len;\n\t\tsge->length -= len;\n\t\tsge->sge_length -= len;\n\t\tif (sge->sge_length == 0) {\n\t\t\tif (--ss->num_sge)\n\t\t\t\t*sge = *ss->sg_list++;\n\t\t} else if (sge->length == 0 && sge->mr->lkey) {\n\t\t\tif (++sge->n >= RVT_SEGSZ) {\n\t\t\t\tif (++sge->m >= sge->mr->mapsz)\n\t\t\t\t\tbreak;\n\t\t\t\tsge->n = 0;\n\t\t\t}\n\t\t\tsge->vaddr =\n\t\t\t\tsge->mr->map[sge->m]->segs[sge->n].vaddr;\n\t\t\tsge->length =\n\t\t\t\tsge->mr->map[sge->m]->segs[sge->n].length;\n\t\t}\n\t\tdata += len;\n\t\tlength -= len;\n\t}\n}\n\n \nstatic void qib_qp_rcv(struct qib_ctxtdata *rcd, struct ib_header *hdr,\n\t\t       int has_grh, void *data, u32 tlen, struct rvt_qp *qp)\n{\n\tstruct qib_ibport *ibp = &rcd->ppd->ibport_data;\n\n\tspin_lock(&qp->r_lock);\n\n\t \n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {\n\t\tibp->rvp.n_pkt_drops++;\n\t\tgoto unlock;\n\t}\n\n\tswitch (qp->ibqp.qp_type) {\n\tcase IB_QPT_SMI:\n\tcase IB_QPT_GSI:\n\t\tif (ib_qib_disable_sma)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase IB_QPT_UD:\n\t\tqib_ud_rcv(ibp, hdr, has_grh, data, tlen, qp);\n\t\tbreak;\n\n\tcase IB_QPT_RC:\n\t\tqib_rc_rcv(rcd, hdr, has_grh, data, tlen, qp);\n\t\tbreak;\n\n\tcase IB_QPT_UC:\n\t\tqib_uc_rcv(ibp, hdr, has_grh, data, tlen, qp);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock(&qp->r_lock);\n}\n\n \nvoid qib_ib_rcv(struct qib_ctxtdata *rcd, void *rhdr, void *data, u32 tlen)\n{\n\tstruct qib_pportdata *ppd = rcd->ppd;\n\tstruct qib_ibport *ibp = &ppd->ibport_data;\n\tstruct ib_header *hdr = rhdr;\n\tstruct qib_devdata *dd = ppd->dd;\n\tstruct rvt_dev_info *rdi = &dd->verbs_dev.rdi;\n\tstruct ib_other_headers *ohdr;\n\tstruct rvt_qp *qp;\n\tu32 qp_num;\n\tint lnh;\n\tu8 opcode;\n\tu16 lid;\n\n\t \n\tif (unlikely(tlen < 24))\n\t\tgoto drop;\n\n\t \n\tlid = be16_to_cpu(hdr->lrh[1]);\n\tif (lid < be16_to_cpu(IB_MULTICAST_LID_BASE)) {\n\t\tlid &= ~((1 << ppd->lmc) - 1);\n\t\tif (unlikely(lid != ppd->lid))\n\t\t\tgoto drop;\n\t}\n\n\t \n\tlnh = be16_to_cpu(hdr->lrh[0]) & 3;\n\tif (lnh == QIB_LRH_BTH)\n\t\tohdr = &hdr->u.oth;\n\telse if (lnh == QIB_LRH_GRH) {\n\t\tu32 vtf;\n\n\t\tohdr = &hdr->u.l.oth;\n\t\tif (hdr->u.l.grh.next_hdr != IB_GRH_NEXT_HDR)\n\t\t\tgoto drop;\n\t\tvtf = be32_to_cpu(hdr->u.l.grh.version_tclass_flow);\n\t\tif ((vtf >> IB_GRH_VERSION_SHIFT) != IB_GRH_VERSION)\n\t\t\tgoto drop;\n\t} else\n\t\tgoto drop;\n\n\topcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0x7f;\n#ifdef CONFIG_DEBUG_FS\n\trcd->opstats->stats[opcode].n_bytes += tlen;\n\trcd->opstats->stats[opcode].n_packets++;\n#endif\n\n\t \n\tqp_num = be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;\n\tif (qp_num == QIB_MULTICAST_QPN) {\n\t\tstruct rvt_mcast *mcast;\n\t\tstruct rvt_mcast_qp *p;\n\n\t\tif (lnh != QIB_LRH_GRH)\n\t\t\tgoto drop;\n\t\tmcast = rvt_mcast_find(&ibp->rvp, &hdr->u.l.grh.dgid, lid);\n\t\tif (mcast == NULL)\n\t\t\tgoto drop;\n\t\tthis_cpu_inc(ibp->pmastats->n_multicast_rcv);\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(p, &mcast->qp_list, list)\n\t\t\tqib_qp_rcv(rcd, hdr, 1, data, tlen, p->qp);\n\t\trcu_read_unlock();\n\t\t \n\t\tif (atomic_dec_return(&mcast->refcount) <= 1)\n\t\t\twake_up(&mcast->wait);\n\t} else {\n\t\trcu_read_lock();\n\t\tqp = rvt_lookup_qpn(rdi, &ibp->rvp, qp_num);\n\t\tif (!qp) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto drop;\n\t\t}\n\t\tthis_cpu_inc(ibp->pmastats->n_unicast_rcv);\n\t\tqib_qp_rcv(rcd, hdr, lnh == QIB_LRH_GRH, data, tlen, qp);\n\t\trcu_read_unlock();\n\t}\n\treturn;\n\ndrop:\n\tibp->rvp.n_pkt_drops++;\n}\n\n \nstatic void mem_timer(struct timer_list *t)\n{\n\tstruct qib_ibdev *dev = from_timer(dev, t, mem_timer);\n\tstruct list_head *list = &dev->memwait;\n\tstruct rvt_qp *qp = NULL;\n\tstruct qib_qp_priv *priv = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->rdi.pending_lock, flags);\n\tif (!list_empty(list)) {\n\t\tpriv = list_entry(list->next, struct qib_qp_priv, iowait);\n\t\tqp = priv->owner;\n\t\tlist_del_init(&priv->iowait);\n\t\trvt_get_qp(qp);\n\t\tif (!list_empty(list))\n\t\t\tmod_timer(&dev->mem_timer, jiffies + 1);\n\t}\n\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n\n\tif (qp) {\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\tif (qp->s_flags & RVT_S_WAIT_KMEM) {\n\t\t\tqp->s_flags &= ~RVT_S_WAIT_KMEM;\n\t\t\tqib_schedule_send(qp);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t\trvt_put_qp(qp);\n\t}\n}\n\n#ifdef __LITTLE_ENDIAN\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\n{\n\treturn data >> shift;\n}\n\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\n{\n\treturn data << shift;\n}\n\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\n{\n\tdata <<= ((sizeof(u32) - n) * BITS_PER_BYTE);\n\tdata >>= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\n\treturn data;\n}\n#else\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\n{\n\treturn data << shift;\n}\n\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\n{\n\treturn data >> shift;\n}\n\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\n{\n\tdata >>= ((sizeof(u32) - n) * BITS_PER_BYTE);\n\tdata <<= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\n\treturn data;\n}\n#endif\n\nstatic void qib_copy_io(u32 __iomem *piobuf, struct rvt_sge_state *ss,\n\t\t    u32 length, unsigned flush_wc)\n{\n\tu32 extra = 0;\n\tu32 data = 0;\n\tu32 last;\n\n\twhile (1) {\n\t\tu32 len = rvt_get_sge_length(&ss->sge, length);\n\t\tu32 off;\n\n\t\t \n\t\toff = (unsigned long)ss->sge.vaddr & (sizeof(u32) - 1);\n\t\tif (off) {\n\t\t\tu32 *addr = (u32 *)((unsigned long)ss->sge.vaddr &\n\t\t\t\t\t    ~(sizeof(u32) - 1));\n\t\t\tu32 v = get_upper_bits(*addr, off * BITS_PER_BYTE);\n\t\t\tu32 y;\n\n\t\t\ty = sizeof(u32) - off;\n\t\t\tif (len > y)\n\t\t\t\tlen = y;\n\t\t\tif (len + extra >= sizeof(u32)) {\n\t\t\t\tdata |= set_upper_bits(v, extra *\n\t\t\t\t\t\t       BITS_PER_BYTE);\n\t\t\t\tlen = sizeof(u32) - extra;\n\t\t\t\tif (len == length) {\n\t\t\t\t\tlast = data;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t__raw_writel(data, piobuf);\n\t\t\t\tpiobuf++;\n\t\t\t\textra = 0;\n\t\t\t\tdata = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tdata |= clear_upper_bytes(v, len, extra);\n\t\t\t\tif (len == length) {\n\t\t\t\t\tlast = data;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\textra += len;\n\t\t\t}\n\t\t} else if (extra) {\n\t\t\t \n\t\t\tu32 *addr = (u32 *) ss->sge.vaddr;\n\t\t\tint shift = extra * BITS_PER_BYTE;\n\t\t\tint ushift = 32 - shift;\n\t\t\tu32 l = len;\n\n\t\t\twhile (l >= sizeof(u32)) {\n\t\t\t\tu32 v = *addr;\n\n\t\t\t\tdata |= set_upper_bits(v, shift);\n\t\t\t\t__raw_writel(data, piobuf);\n\t\t\t\tdata = get_upper_bits(v, ushift);\n\t\t\t\tpiobuf++;\n\t\t\t\taddr++;\n\t\t\t\tl -= sizeof(u32);\n\t\t\t}\n\t\t\t \n\t\t\tif (l) {\n\t\t\t\tu32 v = *addr;\n\n\t\t\t\tif (l + extra >= sizeof(u32)) {\n\t\t\t\t\tdata |= set_upper_bits(v, shift);\n\t\t\t\t\tlen -= l + extra - sizeof(u32);\n\t\t\t\t\tif (len == length) {\n\t\t\t\t\t\tlast = data;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t__raw_writel(data, piobuf);\n\t\t\t\t\tpiobuf++;\n\t\t\t\t\textra = 0;\n\t\t\t\t\tdata = 0;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tdata |= clear_upper_bytes(v, l, extra);\n\t\t\t\t\tif (len == length) {\n\t\t\t\t\t\tlast = data;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\textra += l;\n\t\t\t\t}\n\t\t\t} else if (len == length) {\n\t\t\t\tlast = data;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (len == length) {\n\t\t\tu32 w;\n\n\t\t\t \n\t\t\tw = (len + 3) >> 2;\n\t\t\tqib_pio_copy(piobuf, ss->sge.vaddr, w - 1);\n\t\t\tpiobuf += w - 1;\n\t\t\tlast = ((u32 *) ss->sge.vaddr)[w - 1];\n\t\t\tbreak;\n\t\t} else {\n\t\t\tu32 w = len >> 2;\n\n\t\t\tqib_pio_copy(piobuf, ss->sge.vaddr, w);\n\t\t\tpiobuf += w;\n\n\t\t\textra = len & (sizeof(u32) - 1);\n\t\t\tif (extra) {\n\t\t\t\tu32 v = ((u32 *) ss->sge.vaddr)[w];\n\n\t\t\t\t \n\t\t\t\tdata = clear_upper_bytes(v, extra, 0);\n\t\t\t}\n\t\t}\n\t\trvt_update_sge(ss, len, false);\n\t\tlength -= len;\n\t}\n\t \n\trvt_update_sge(ss, length, false);\n\tif (flush_wc) {\n\t\t \n\t\tqib_flush_wc();\n\t\t__raw_writel(last, piobuf);\n\t\t \n\t\tqib_flush_wc();\n\t} else\n\t\t__raw_writel(last, piobuf);\n}\n\nstatic noinline struct qib_verbs_txreq *__get_txreq(struct qib_ibdev *dev,\n\t\t\t\t\t   struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tstruct qib_verbs_txreq *tx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tspin_lock(&dev->rdi.pending_lock);\n\n\tif (!list_empty(&dev->txreq_free)) {\n\t\tstruct list_head *l = dev->txreq_free.next;\n\n\t\tlist_del(l);\n\t\tspin_unlock(&dev->rdi.pending_lock);\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t\ttx = list_entry(l, struct qib_verbs_txreq, txreq.list);\n\t} else {\n\t\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK &&\n\t\t    list_empty(&priv->iowait)) {\n\t\t\tdev->n_txwait++;\n\t\t\tqp->s_flags |= RVT_S_WAIT_TX;\n\t\t\tlist_add_tail(&priv->iowait, &dev->txwait);\n\t\t}\n\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\tspin_unlock(&dev->rdi.pending_lock);\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t\ttx = ERR_PTR(-EBUSY);\n\t}\n\treturn tx;\n}\n\nstatic inline struct qib_verbs_txreq *get_txreq(struct qib_ibdev *dev,\n\t\t\t\t\t struct rvt_qp *qp)\n{\n\tstruct qib_verbs_txreq *tx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->rdi.pending_lock, flags);\n\t \n\tif (likely(!list_empty(&dev->txreq_free))) {\n\t\tstruct list_head *l = dev->txreq_free.next;\n\n\t\tlist_del(l);\n\t\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n\t\ttx = list_entry(l, struct qib_verbs_txreq, txreq.list);\n\t} else {\n\t\t \n\t\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n\t\ttx =  __get_txreq(dev, qp);\n\t}\n\treturn tx;\n}\n\nvoid qib_put_txreq(struct qib_verbs_txreq *tx)\n{\n\tstruct qib_ibdev *dev;\n\tstruct rvt_qp *qp;\n\tstruct qib_qp_priv *priv;\n\tunsigned long flags;\n\n\tqp = tx->qp;\n\tdev = to_idev(qp->ibqp.device);\n\n\tif (tx->mr) {\n\t\trvt_put_mr(tx->mr);\n\t\ttx->mr = NULL;\n\t}\n\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF) {\n\t\ttx->txreq.flags &= ~QIB_SDMA_TXREQ_F_FREEBUF;\n\t\tdma_unmap_single(&dd_from_dev(dev)->pcidev->dev,\n\t\t\t\t tx->txreq.addr, tx->hdr_dwords << 2,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tkfree(tx->align_buf);\n\t}\n\n\tspin_lock_irqsave(&dev->rdi.pending_lock, flags);\n\n\t \n\tlist_add(&tx->txreq.list, &dev->txreq_free);\n\n\tif (!list_empty(&dev->txwait)) {\n\t\t \n\t\tpriv = list_entry(dev->txwait.next, struct qib_qp_priv,\n\t\t\t\t  iowait);\n\t\tqp = priv->owner;\n\t\tlist_del_init(&priv->iowait);\n\t\trvt_get_qp(qp);\n\t\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\tif (qp->s_flags & RVT_S_WAIT_TX) {\n\t\t\tqp->s_flags &= ~RVT_S_WAIT_TX;\n\t\t\tqib_schedule_send(qp);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\n\t\trvt_put_qp(qp);\n\t} else\n\t\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n}\n\n \nvoid qib_verbs_sdma_desc_avail(struct qib_pportdata *ppd, unsigned avail)\n{\n\tstruct rvt_qp *qp;\n\tstruct qib_qp_priv *qpp, *nqpp;\n\tstruct rvt_qp *qps[20];\n\tstruct qib_ibdev *dev;\n\tunsigned i, n;\n\n\tn = 0;\n\tdev = &ppd->dd->verbs_dev;\n\tspin_lock(&dev->rdi.pending_lock);\n\n\t \n\tlist_for_each_entry_safe(qpp, nqpp, &dev->dmawait, iowait) {\n\t\tqp = qpp->owner;\n\t\tif (qp->port_num != ppd->port)\n\t\t\tcontinue;\n\t\tif (n == ARRAY_SIZE(qps))\n\t\t\tbreak;\n\t\tif (qpp->s_tx->txreq.sg_count > avail)\n\t\t\tbreak;\n\t\tavail -= qpp->s_tx->txreq.sg_count;\n\t\tlist_del_init(&qpp->iowait);\n\t\trvt_get_qp(qp);\n\t\tqps[n++] = qp;\n\t}\n\n\tspin_unlock(&dev->rdi.pending_lock);\n\n\tfor (i = 0; i < n; i++) {\n\t\tqp = qps[i];\n\t\tspin_lock(&qp->s_lock);\n\t\tif (qp->s_flags & RVT_S_WAIT_DMA_DESC) {\n\t\t\tqp->s_flags &= ~RVT_S_WAIT_DMA_DESC;\n\t\t\tqib_schedule_send(qp);\n\t\t}\n\t\tspin_unlock(&qp->s_lock);\n\t\trvt_put_qp(qp);\n\t}\n}\n\n \nstatic void sdma_complete(struct qib_sdma_txreq *cookie, int status)\n{\n\tstruct qib_verbs_txreq *tx =\n\t\tcontainer_of(cookie, struct qib_verbs_txreq, txreq);\n\tstruct rvt_qp *qp = tx->qp;\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\tspin_lock(&qp->s_lock);\n\tif (tx->wqe)\n\t\trvt_send_complete(qp, tx->wqe, IB_WC_SUCCESS);\n\telse if (qp->ibqp.qp_type == IB_QPT_RC) {\n\t\tstruct ib_header *hdr;\n\n\t\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF)\n\t\t\thdr = &tx->align_buf->hdr;\n\t\telse {\n\t\t\tstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\n\n\t\t\thdr = &dev->pio_hdrs[tx->hdr_inx].hdr;\n\t\t}\n\t\tqib_rc_send_complete(qp, hdr);\n\t}\n\tif (atomic_dec_and_test(&priv->s_dma_busy)) {\n\t\tif (qp->state == IB_QPS_RESET)\n\t\t\twake_up(&priv->wait_dma);\n\t\telse if (qp->s_flags & RVT_S_WAIT_DMA) {\n\t\t\tqp->s_flags &= ~RVT_S_WAIT_DMA;\n\t\t\tqib_schedule_send(qp);\n\t\t}\n\t}\n\tspin_unlock(&qp->s_lock);\n\n\tqib_put_txreq(tx);\n}\n\nstatic int wait_kmem(struct qib_ibdev *dev, struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\n\t\tspin_lock(&dev->rdi.pending_lock);\n\t\tif (list_empty(&priv->iowait)) {\n\t\t\tif (list_empty(&dev->memwait))\n\t\t\t\tmod_timer(&dev->mem_timer, jiffies + 1);\n\t\t\tqp->s_flags |= RVT_S_WAIT_KMEM;\n\t\t\tlist_add_tail(&priv->iowait, &dev->memwait);\n\t\t}\n\t\tspin_unlock(&dev->rdi.pending_lock);\n\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\tret = -EBUSY;\n\t}\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\n\treturn ret;\n}\n\nstatic int qib_verbs_send_dma(struct rvt_qp *qp, struct ib_header *hdr,\n\t\t\t      u32 hdrwords, struct rvt_sge_state *ss, u32 len,\n\t\t\t      u32 plen, u32 dwords)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\n\tstruct qib_devdata *dd = dd_from_dev(dev);\n\tstruct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct qib_verbs_txreq *tx;\n\tstruct qib_pio_header *phdr;\n\tu32 control;\n\tu32 ndesc;\n\tint ret;\n\n\ttx = priv->s_tx;\n\tif (tx) {\n\t\tpriv->s_tx = NULL;\n\t\t \n\t\tret = qib_sdma_verbs_send(ppd, tx->ss, tx->dwords, tx);\n\t\tgoto bail;\n\t}\n\n\ttx = get_txreq(dev, qp);\n\tif (IS_ERR(tx))\n\t\tgoto bail_tx;\n\n\tcontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\n\t\t\t\t       be16_to_cpu(hdr->lrh[0]) >> 12);\n\ttx->qp = qp;\n\ttx->wqe = qp->s_wqe;\n\ttx->mr = qp->s_rdma_mr;\n\tif (qp->s_rdma_mr)\n\t\tqp->s_rdma_mr = NULL;\n\ttx->txreq.callback = sdma_complete;\n\tif (dd->flags & QIB_HAS_SDMA_TIMEOUT)\n\t\ttx->txreq.flags = QIB_SDMA_TXREQ_F_HEADTOHOST;\n\telse\n\t\ttx->txreq.flags = QIB_SDMA_TXREQ_F_INTREQ;\n\tif (plen + 1 > dd->piosize2kmax_dwords)\n\t\ttx->txreq.flags |= QIB_SDMA_TXREQ_F_USELARGEBUF;\n\n\tif (len) {\n\t\t \n\t\tndesc = qib_count_sge(ss, len);\n\t\tif (ndesc >= ppd->sdma_descq_cnt)\n\t\t\tndesc = 0;\n\t} else\n\t\tndesc = 1;\n\tif (ndesc) {\n\t\tphdr = &dev->pio_hdrs[tx->hdr_inx];\n\t\tphdr->pbc[0] = cpu_to_le32(plen);\n\t\tphdr->pbc[1] = cpu_to_le32(control);\n\t\tmemcpy(&phdr->hdr, hdr, hdrwords << 2);\n\t\ttx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEDESC;\n\t\ttx->txreq.sg_count = ndesc;\n\t\ttx->txreq.addr = dev->pio_hdrs_phys +\n\t\t\ttx->hdr_inx * sizeof(struct qib_pio_header);\n\t\ttx->hdr_dwords = hdrwords + 2;  \n\t\tret = qib_sdma_verbs_send(ppd, ss, dwords, tx);\n\t\tgoto bail;\n\t}\n\n\t \n\ttx->hdr_dwords = plen + 1;\n\tphdr = kmalloc(tx->hdr_dwords << 2, GFP_ATOMIC);\n\tif (!phdr)\n\t\tgoto err_tx;\n\tphdr->pbc[0] = cpu_to_le32(plen);\n\tphdr->pbc[1] = cpu_to_le32(control);\n\tmemcpy(&phdr->hdr, hdr, hdrwords << 2);\n\tqib_copy_from_sge((u32 *) &phdr->hdr + hdrwords, ss, len);\n\n\ttx->txreq.addr = dma_map_single(&dd->pcidev->dev, phdr,\n\t\t\t\t\ttx->hdr_dwords << 2, DMA_TO_DEVICE);\n\tif (dma_mapping_error(&dd->pcidev->dev, tx->txreq.addr))\n\t\tgoto map_err;\n\ttx->align_buf = phdr;\n\ttx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEBUF;\n\ttx->txreq.sg_count = 1;\n\tret = qib_sdma_verbs_send(ppd, NULL, 0, tx);\n\tgoto unaligned;\n\nmap_err:\n\tkfree(phdr);\nerr_tx:\n\tqib_put_txreq(tx);\n\tret = wait_kmem(dev, qp);\nunaligned:\n\tibp->rvp.n_unaligned++;\nbail:\n\treturn ret;\nbail_tx:\n\tret = PTR_ERR(tx);\n\tgoto bail;\n}\n\n \nstatic int no_bufs_available(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\n\tstruct qib_devdata *dd;\n\tunsigned long flags;\n\tint ret = 0;\n\n\t \n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\n\t\tspin_lock(&dev->rdi.pending_lock);\n\t\tif (list_empty(&priv->iowait)) {\n\t\t\tdev->n_piowait++;\n\t\t\tqp->s_flags |= RVT_S_WAIT_PIO;\n\t\t\tlist_add_tail(&priv->iowait, &dev->piowait);\n\t\t\tdd = dd_from_dev(dev);\n\t\t\tdd->f_wantpiobuf_intr(dd, 1);\n\t\t}\n\t\tspin_unlock(&dev->rdi.pending_lock);\n\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\tret = -EBUSY;\n\t}\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\treturn ret;\n}\n\nstatic int qib_verbs_send_pio(struct rvt_qp *qp, struct ib_header *ibhdr,\n\t\t\t      u32 hdrwords, struct rvt_sge_state *ss, u32 len,\n\t\t\t      u32 plen, u32 dwords)\n{\n\tstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\n\tstruct qib_pportdata *ppd = dd->pport + qp->port_num - 1;\n\tu32 *hdr = (u32 *) ibhdr;\n\tu32 __iomem *piobuf_orig;\n\tu32 __iomem *piobuf;\n\tu64 pbc;\n\tunsigned long flags;\n\tunsigned flush_wc;\n\tu32 control;\n\tu32 pbufn;\n\n\tcontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\n\t\tbe16_to_cpu(ibhdr->lrh[0]) >> 12);\n\tpbc = ((u64) control << 32) | plen;\n\tpiobuf = dd->f_getsendbuf(ppd, pbc, &pbufn);\n\tif (unlikely(piobuf == NULL))\n\t\treturn no_bufs_available(qp);\n\n\t \n\twriteq(pbc, piobuf);\n\tpiobuf_orig = piobuf;\n\tpiobuf += 2;\n\n\tflush_wc = dd->flags & QIB_PIO_FLUSH_WC;\n\tif (len == 0) {\n\t\t \n\t\tif (flush_wc) {\n\t\t\tqib_flush_wc();\n\t\t\tqib_pio_copy(piobuf, hdr, hdrwords - 1);\n\t\t\tqib_flush_wc();\n\t\t\t__raw_writel(hdr[hdrwords - 1], piobuf + hdrwords - 1);\n\t\t\tqib_flush_wc();\n\t\t} else\n\t\t\tqib_pio_copy(piobuf, hdr, hdrwords);\n\t\tgoto done;\n\t}\n\n\tif (flush_wc)\n\t\tqib_flush_wc();\n\tqib_pio_copy(piobuf, hdr, hdrwords);\n\tpiobuf += hdrwords;\n\n\t \n\tif (likely(ss->num_sge == 1 && len <= ss->sge.length &&\n\t\t   !((unsigned long)ss->sge.vaddr & (sizeof(u32) - 1)))) {\n\t\tu32 *addr = (u32 *) ss->sge.vaddr;\n\n\t\t \n\t\trvt_update_sge(ss, len, false);\n\t\tif (flush_wc) {\n\t\t\tqib_pio_copy(piobuf, addr, dwords - 1);\n\t\t\t \n\t\t\tqib_flush_wc();\n\t\t\t__raw_writel(addr[dwords - 1], piobuf + dwords - 1);\n\t\t\t \n\t\t\tqib_flush_wc();\n\t\t} else\n\t\t\tqib_pio_copy(piobuf, addr, dwords);\n\t\tgoto done;\n\t}\n\tqib_copy_io(piobuf, ss, len, flush_wc);\ndone:\n\tif (dd->flags & QIB_USE_SPCL_TRIG) {\n\t\tu32 spcl_off = (pbufn >= dd->piobcnt2k) ? 2047 : 1023;\n\n\t\tqib_flush_wc();\n\t\t__raw_writel(0xaebecede, piobuf_orig + spcl_off);\n\t}\n\tqib_sendbuf_done(dd, pbufn);\n\tif (qp->s_rdma_mr) {\n\t\trvt_put_mr(qp->s_rdma_mr);\n\t\tqp->s_rdma_mr = NULL;\n\t}\n\tif (qp->s_wqe) {\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\trvt_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t} else if (qp->ibqp.qp_type == IB_QPT_RC) {\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\tqib_rc_send_complete(qp, ibhdr);\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t}\n\treturn 0;\n}\n\n \nint qib_verbs_send(struct rvt_qp *qp, struct ib_header *hdr,\n\t\t   u32 hdrwords, struct rvt_sge_state *ss, u32 len)\n{\n\tstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\n\tu32 plen;\n\tint ret;\n\tu32 dwords = (len + 3) >> 2;\n\n\t \n\tplen = hdrwords + dwords + 1;\n\n\t \n\tif (qp->ibqp.qp_type == IB_QPT_SMI ||\n\t    !(dd->flags & QIB_HAS_SEND_DMA))\n\t\tret = qib_verbs_send_pio(qp, hdr, hdrwords, ss, len,\n\t\t\t\t\t plen, dwords);\n\telse\n\t\tret = qib_verbs_send_dma(qp, hdr, hdrwords, ss, len,\n\t\t\t\t\t plen, dwords);\n\n\treturn ret;\n}\n\nint qib_snapshot_counters(struct qib_pportdata *ppd, u64 *swords,\n\t\t\t  u64 *rwords, u64 *spkts, u64 *rpkts,\n\t\t\t  u64 *xmit_wait)\n{\n\tint ret;\n\tstruct qib_devdata *dd = ppd->dd;\n\n\tif (!(dd->flags & QIB_PRESENT)) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto bail;\n\t}\n\t*swords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDSEND);\n\t*rwords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDRCV);\n\t*spkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTSEND);\n\t*rpkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTRCV);\n\t*xmit_wait = dd->f_portcntr(ppd, QIBPORTCNTR_SENDSTALL);\n\n\tret = 0;\n\nbail:\n\treturn ret;\n}\n\n \nint qib_get_counters(struct qib_pportdata *ppd,\n\t\t     struct qib_verbs_counters *cntrs)\n{\n\tint ret;\n\n\tif (!(ppd->dd->flags & QIB_PRESENT)) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto bail;\n\t}\n\tcntrs->symbol_error_counter =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBSYMBOLERR);\n\tcntrs->link_error_recovery_counter =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKERRRECOV);\n\t \n\tcntrs->link_downed_counter =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKDOWN);\n\tcntrs->port_rcv_errors =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXDROPPKT) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVOVFL) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERR_RLEN) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_INVALIDRLEN) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLINK) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRICRC) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRVCRC) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLPCRC) +\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_BADFORMAT);\n\tcntrs->port_rcv_errors +=\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXLOCALPHYERR);\n\tcntrs->port_rcv_errors +=\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXVLERR);\n\tcntrs->port_rcv_remphys_errors =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVEBP);\n\tcntrs->port_xmit_discards =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_UNSUPVL);\n\tcntrs->port_xmit_data = ppd->dd->f_portcntr(ppd,\n\t\t\tQIBPORTCNTR_WORDSEND);\n\tcntrs->port_rcv_data = ppd->dd->f_portcntr(ppd,\n\t\t\tQIBPORTCNTR_WORDRCV);\n\tcntrs->port_xmit_packets = ppd->dd->f_portcntr(ppd,\n\t\t\tQIBPORTCNTR_PKTSEND);\n\tcntrs->port_rcv_packets = ppd->dd->f_portcntr(ppd,\n\t\t\tQIBPORTCNTR_PKTRCV);\n\tcntrs->local_link_integrity_errors =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_LLI);\n\tcntrs->excessive_buffer_overrun_errors =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_EXCESSBUFOVFL);\n\tcntrs->vl15_dropped =\n\t\tppd->dd->f_portcntr(ppd, QIBPORTCNTR_VL15PKTDROP);\n\n\tret = 0;\n\nbail:\n\treturn ret;\n}\n\n \nvoid qib_ib_piobufavail(struct qib_devdata *dd)\n{\n\tstruct qib_ibdev *dev = &dd->verbs_dev;\n\tstruct list_head *list;\n\tstruct rvt_qp *qps[5];\n\tstruct rvt_qp *qp;\n\tunsigned long flags;\n\tunsigned i, n;\n\tstruct qib_qp_priv *priv;\n\n\tlist = &dev->piowait;\n\tn = 0;\n\n\t \n\tspin_lock_irqsave(&dev->rdi.pending_lock, flags);\n\twhile (!list_empty(list)) {\n\t\tif (n == ARRAY_SIZE(qps))\n\t\t\tgoto full;\n\t\tpriv = list_entry(list->next, struct qib_qp_priv, iowait);\n\t\tqp = priv->owner;\n\t\tlist_del_init(&priv->iowait);\n\t\trvt_get_qp(qp);\n\t\tqps[n++] = qp;\n\t}\n\tdd->f_wantpiobuf_intr(dd, 0);\nfull:\n\tspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\n\n\tfor (i = 0; i < n; i++) {\n\t\tqp = qps[i];\n\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\tif (qp->s_flags & RVT_S_WAIT_PIO) {\n\t\t\tqp->s_flags &= ~RVT_S_WAIT_PIO;\n\t\t\tqib_schedule_send(qp);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\n\t\t \n\t\trvt_put_qp(qp);\n\t}\n}\n\nstatic int qib_query_port(struct rvt_dev_info *rdi, u32 port_num,\n\t\t\t  struct ib_port_attr *props)\n{\n\tstruct qib_ibdev *ibdev = container_of(rdi, struct qib_ibdev, rdi);\n\tstruct qib_devdata *dd = dd_from_dev(ibdev);\n\tstruct qib_pportdata *ppd = &dd->pport[port_num - 1];\n\tenum ib_mtu mtu;\n\tu16 lid = ppd->lid;\n\n\t \n\tprops->lid = lid ? lid : be16_to_cpu(IB_LID_PERMISSIVE);\n\tprops->lmc = ppd->lmc;\n\tprops->state = dd->f_iblink_state(ppd->lastibcstat);\n\tprops->phys_state = dd->f_ibphys_portstate(ppd->lastibcstat);\n\tprops->gid_tbl_len = QIB_GUIDS_PER_PORT;\n\tprops->active_width = ppd->link_width_active;\n\t \n\tprops->active_speed = ppd->link_speed_active;\n\tprops->max_vl_num = qib_num_vls(ppd->vls_supported);\n\n\tprops->max_mtu = qib_ibmtu ? qib_ibmtu : IB_MTU_4096;\n\tswitch (ppd->ibmtu) {\n\tcase 4096:\n\t\tmtu = IB_MTU_4096;\n\t\tbreak;\n\tcase 2048:\n\t\tmtu = IB_MTU_2048;\n\t\tbreak;\n\tcase 1024:\n\t\tmtu = IB_MTU_1024;\n\t\tbreak;\n\tcase 512:\n\t\tmtu = IB_MTU_512;\n\t\tbreak;\n\tcase 256:\n\t\tmtu = IB_MTU_256;\n\t\tbreak;\n\tdefault:\n\t\tmtu = IB_MTU_2048;\n\t}\n\tprops->active_mtu = mtu;\n\n\treturn 0;\n}\n\nstatic int qib_modify_device(struct ib_device *device,\n\t\t\t     int device_modify_mask,\n\t\t\t     struct ib_device_modify *device_modify)\n{\n\tstruct qib_devdata *dd = dd_from_ibdev(device);\n\tunsigned i;\n\tint ret;\n\n\tif (device_modify_mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\n\t\t\t\t   IB_DEVICE_MODIFY_NODE_DESC)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto bail;\n\t}\n\n\tif (device_modify_mask & IB_DEVICE_MODIFY_NODE_DESC) {\n\t\tmemcpy(device->node_desc, device_modify->node_desc,\n\t\t       IB_DEVICE_NODE_DESC_MAX);\n\t\tfor (i = 0; i < dd->num_pports; i++) {\n\t\t\tstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\n\n\t\t\tqib_node_desc_chg(ibp);\n\t\t}\n\t}\n\n\tif (device_modify_mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID) {\n\t\tib_qib_sys_image_guid =\n\t\t\tcpu_to_be64(device_modify->sys_image_guid);\n\t\tfor (i = 0; i < dd->num_pports; i++) {\n\t\t\tstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\n\n\t\t\tqib_sys_guid_chg(ibp);\n\t\t}\n\t}\n\n\tret = 0;\n\nbail:\n\treturn ret;\n}\n\nstatic int qib_shut_down_port(struct rvt_dev_info *rdi, u32 port_num)\n{\n\tstruct qib_ibdev *ibdev = container_of(rdi, struct qib_ibdev, rdi);\n\tstruct qib_devdata *dd = dd_from_dev(ibdev);\n\tstruct qib_pportdata *ppd = &dd->pport[port_num - 1];\n\n\tqib_set_linkstate(ppd, QIB_IB_LINKDOWN);\n\n\treturn 0;\n}\n\nstatic int qib_get_guid_be(struct rvt_dev_info *rdi, struct rvt_ibport *rvp,\n\t\t\t   int guid_index, __be64 *guid)\n{\n\tstruct qib_ibport *ibp = container_of(rvp, struct qib_ibport, rvp);\n\tstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\n\n\tif (guid_index == 0)\n\t\t*guid = ppd->guid;\n\telse if (guid_index < QIB_GUIDS_PER_PORT)\n\t\t*guid = ibp->guids[guid_index - 1];\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint qib_check_ah(struct ib_device *ibdev, struct rdma_ah_attr *ah_attr)\n{\n\tif (rdma_ah_get_sl(ah_attr) > 15)\n\t\treturn -EINVAL;\n\n\tif (rdma_ah_get_dlid(ah_attr) == 0)\n\t\treturn -EINVAL;\n\tif (rdma_ah_get_dlid(ah_attr) >=\n\t\tbe16_to_cpu(IB_MULTICAST_LID_BASE) &&\n\t    rdma_ah_get_dlid(ah_attr) !=\n\t\tbe16_to_cpu(IB_LID_PERMISSIVE) &&\n\t    !(rdma_ah_get_ah_flags(ah_attr) & IB_AH_GRH))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void qib_notify_new_ah(struct ib_device *ibdev,\n\t\t\t      struct rdma_ah_attr *ah_attr,\n\t\t\t      struct rvt_ah *ah)\n{\n\tstruct qib_ibport *ibp;\n\tstruct qib_pportdata *ppd;\n\n\t \n\n\tibp = to_iport(ibdev, rdma_ah_get_port_num(ah_attr));\n\tppd = ppd_from_ibp(ibp);\n\tah->vl = ibp->sl_to_vl[rdma_ah_get_sl(&ah->attr)];\n\tah->log_pmtu = ilog2(ppd->ibmtu);\n}\n\nstruct ib_ah *qib_create_qp0_ah(struct qib_ibport *ibp, u16 dlid)\n{\n\tstruct rdma_ah_attr attr;\n\tstruct ib_ah *ah = ERR_PTR(-EINVAL);\n\tstruct rvt_qp *qp0;\n\tstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct qib_devdata *dd = dd_from_ppd(ppd);\n\tu32 port_num = ppd->port;\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.type = rdma_ah_find_type(&dd->verbs_dev.rdi.ibdev, port_num);\n\trdma_ah_set_dlid(&attr, dlid);\n\trdma_ah_set_port_num(&attr, port_num);\n\trcu_read_lock();\n\tqp0 = rcu_dereference(ibp->rvp.qp[0]);\n\tif (qp0)\n\t\tah = rdma_create_ah(qp0->ibqp.pd, &attr, 0);\n\trcu_read_unlock();\n\treturn ah;\n}\n\n \nunsigned qib_get_npkeys(struct qib_devdata *dd)\n{\n\treturn ARRAY_SIZE(dd->rcd[0]->pkeys);\n}\n\n \nunsigned qib_get_pkey(struct qib_ibport *ibp, unsigned index)\n{\n\tstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct qib_devdata *dd = ppd->dd;\n\tunsigned ctxt = ppd->hw_pidx;\n\tunsigned ret;\n\n\t \n\tif (!dd->rcd || index >= ARRAY_SIZE(dd->rcd[ctxt]->pkeys))\n\t\tret = 0;\n\telse\n\t\tret = dd->rcd[ctxt]->pkeys[index];\n\n\treturn ret;\n}\n\nstatic void init_ibport(struct qib_pportdata *ppd)\n{\n\tstruct qib_verbs_counters cntrs;\n\tstruct qib_ibport *ibp = &ppd->ibport_data;\n\n\tspin_lock_init(&ibp->rvp.lock);\n\t \n\tibp->rvp.gid_prefix = IB_DEFAULT_GID_PREFIX;\n\tibp->rvp.sm_lid = be16_to_cpu(IB_LID_PERMISSIVE);\n\tibp->rvp.port_cap_flags = IB_PORT_SYS_IMAGE_GUID_SUP |\n\t\tIB_PORT_CLIENT_REG_SUP | IB_PORT_SL_MAP_SUP |\n\t\tIB_PORT_TRAP_SUP | IB_PORT_AUTO_MIGR_SUP |\n\t\tIB_PORT_DR_NOTICE_SUP | IB_PORT_CAP_MASK_NOTICE_SUP |\n\t\tIB_PORT_OTHER_LOCAL_CHANGES_SUP;\n\tif (ppd->dd->flags & QIB_HAS_LINK_LATENCY)\n\t\tibp->rvp.port_cap_flags |= IB_PORT_LINK_LATENCY_SUP;\n\tibp->rvp.pma_counter_select[0] = IB_PMA_PORT_XMIT_DATA;\n\tibp->rvp.pma_counter_select[1] = IB_PMA_PORT_RCV_DATA;\n\tibp->rvp.pma_counter_select[2] = IB_PMA_PORT_XMIT_PKTS;\n\tibp->rvp.pma_counter_select[3] = IB_PMA_PORT_RCV_PKTS;\n\tibp->rvp.pma_counter_select[4] = IB_PMA_PORT_XMIT_WAIT;\n\n\t \n\tqib_get_counters(ppd, &cntrs);\n\tibp->z_symbol_error_counter = cntrs.symbol_error_counter;\n\tibp->z_link_error_recovery_counter =\n\t\tcntrs.link_error_recovery_counter;\n\tibp->z_link_downed_counter = cntrs.link_downed_counter;\n\tibp->z_port_rcv_errors = cntrs.port_rcv_errors;\n\tibp->z_port_rcv_remphys_errors = cntrs.port_rcv_remphys_errors;\n\tibp->z_port_xmit_discards = cntrs.port_xmit_discards;\n\tibp->z_port_xmit_data = cntrs.port_xmit_data;\n\tibp->z_port_rcv_data = cntrs.port_rcv_data;\n\tibp->z_port_xmit_packets = cntrs.port_xmit_packets;\n\tibp->z_port_rcv_packets = cntrs.port_rcv_packets;\n\tibp->z_local_link_integrity_errors =\n\t\tcntrs.local_link_integrity_errors;\n\tibp->z_excessive_buffer_overrun_errors =\n\t\tcntrs.excessive_buffer_overrun_errors;\n\tibp->z_vl15_dropped = cntrs.vl15_dropped;\n\tRCU_INIT_POINTER(ibp->rvp.qp[0], NULL);\n\tRCU_INIT_POINTER(ibp->rvp.qp[1], NULL);\n}\n\n \nstatic void qib_fill_device_attr(struct qib_devdata *dd)\n{\n\tstruct rvt_dev_info *rdi = &dd->verbs_dev.rdi;\n\n\tmemset(&rdi->dparms.props, 0, sizeof(rdi->dparms.props));\n\n\trdi->dparms.props.max_pd = ib_qib_max_pds;\n\trdi->dparms.props.max_ah = ib_qib_max_ahs;\n\trdi->dparms.props.device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |\n\t\tIB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |\n\t\tIB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |\n\t\tIB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;\n\trdi->dparms.props.page_size_cap = PAGE_SIZE;\n\trdi->dparms.props.vendor_id =\n\t\tQIB_SRC_OUI_1 << 16 | QIB_SRC_OUI_2 << 8 | QIB_SRC_OUI_3;\n\trdi->dparms.props.vendor_part_id = dd->deviceid;\n\trdi->dparms.props.hw_ver = dd->minrev;\n\trdi->dparms.props.sys_image_guid = ib_qib_sys_image_guid;\n\trdi->dparms.props.max_mr_size = ~0ULL;\n\trdi->dparms.props.max_qp = ib_qib_max_qps;\n\trdi->dparms.props.max_qp_wr = ib_qib_max_qp_wrs;\n\trdi->dparms.props.max_send_sge = ib_qib_max_sges;\n\trdi->dparms.props.max_recv_sge = ib_qib_max_sges;\n\trdi->dparms.props.max_sge_rd = ib_qib_max_sges;\n\trdi->dparms.props.max_cq = ib_qib_max_cqs;\n\trdi->dparms.props.max_cqe = ib_qib_max_cqes;\n\trdi->dparms.props.max_ah = ib_qib_max_ahs;\n\trdi->dparms.props.max_qp_rd_atom = QIB_MAX_RDMA_ATOMIC;\n\trdi->dparms.props.max_qp_init_rd_atom = 255;\n\trdi->dparms.props.max_srq = ib_qib_max_srqs;\n\trdi->dparms.props.max_srq_wr = ib_qib_max_srq_wrs;\n\trdi->dparms.props.max_srq_sge = ib_qib_max_srq_sges;\n\trdi->dparms.props.atomic_cap = IB_ATOMIC_GLOB;\n\trdi->dparms.props.max_pkeys = qib_get_npkeys(dd);\n\trdi->dparms.props.max_mcast_grp = ib_qib_max_mcast_grps;\n\trdi->dparms.props.max_mcast_qp_attach = ib_qib_max_mcast_qp_attached;\n\trdi->dparms.props.max_total_mcast_qp_attach =\n\t\t\t\t\trdi->dparms.props.max_mcast_qp_attach *\n\t\t\t\t\trdi->dparms.props.max_mcast_grp;\n\t \n\tdd->verbs_dev.rdi.post_parms = qib_post_parms;\n\n\t \n\tdd->verbs_dev.rdi.wc_opcode = ib_qib_wc_opcode;\n}\n\nstatic const struct ib_device_ops qib_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_QIB,\n\n\t.port_groups = qib_attr_port_groups,\n\t.device_group = &qib_attr_group,\n\t.modify_device = qib_modify_device,\n\t.process_mad = qib_process_mad,\n};\n\n \nint qib_register_ib_device(struct qib_devdata *dd)\n{\n\tstruct qib_ibdev *dev = &dd->verbs_dev;\n\tstruct ib_device *ibdev = &dev->rdi.ibdev;\n\tstruct qib_pportdata *ppd = dd->pport;\n\tunsigned i, ctxt;\n\tint ret;\n\n\tfor (i = 0; i < dd->num_pports; i++)\n\t\tinit_ibport(ppd + i);\n\n\t \n\ttimer_setup(&dev->mem_timer, mem_timer, 0);\n\n\tINIT_LIST_HEAD(&dev->piowait);\n\tINIT_LIST_HEAD(&dev->dmawait);\n\tINIT_LIST_HEAD(&dev->txwait);\n\tINIT_LIST_HEAD(&dev->memwait);\n\tINIT_LIST_HEAD(&dev->txreq_free);\n\n\tif (ppd->sdma_descq_cnt) {\n\t\tdev->pio_hdrs = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tppd->sdma_descq_cnt *\n\t\t\t\t\t\tsizeof(struct qib_pio_header),\n\t\t\t\t\t\t&dev->pio_hdrs_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!dev->pio_hdrs) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_hdrs;\n\t\t}\n\t}\n\n\tfor (i = 0; i < ppd->sdma_descq_cnt; i++) {\n\t\tstruct qib_verbs_txreq *tx;\n\n\t\ttx = kzalloc(sizeof(*tx), GFP_KERNEL);\n\t\tif (!tx) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_tx;\n\t\t}\n\t\ttx->hdr_inx = i;\n\t\tlist_add(&tx->txreq.list, &dev->txreq_free);\n\t}\n\n\t \n\tif (!ib_qib_sys_image_guid)\n\t\tib_qib_sys_image_guid = ppd->guid;\n\n\tibdev->node_guid = ppd->guid;\n\tibdev->phys_port_cnt = dd->num_pports;\n\tibdev->dev.parent = &dd->pcidev->dev;\n\n\tsnprintf(ibdev->node_desc, sizeof(ibdev->node_desc),\n\t\t \"Intel Infiniband HCA %s\", init_utsname()->nodename);\n\n\t \n\tdd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;\n\tdd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;\n\tdd->verbs_dev.rdi.driver_f.setup_wqe = qib_check_send_wqe;\n\tdd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;\n\tdd->verbs_dev.rdi.driver_f.alloc_qpn = qib_alloc_qpn;\n\tdd->verbs_dev.rdi.driver_f.qp_priv_alloc = qib_qp_priv_alloc;\n\tdd->verbs_dev.rdi.driver_f.qp_priv_free = qib_qp_priv_free;\n\tdd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;\n\tdd->verbs_dev.rdi.driver_f.notify_qp_reset = qib_notify_qp_reset;\n\tdd->verbs_dev.rdi.driver_f.do_send = qib_do_send;\n\tdd->verbs_dev.rdi.driver_f.schedule_send = qib_schedule_send;\n\tdd->verbs_dev.rdi.driver_f.quiesce_qp = qib_quiesce_qp;\n\tdd->verbs_dev.rdi.driver_f.stop_send_queue = qib_stop_send_queue;\n\tdd->verbs_dev.rdi.driver_f.flush_qp_waiters = qib_flush_qp_waiters;\n\tdd->verbs_dev.rdi.driver_f.notify_error_qp = qib_notify_error_qp;\n\tdd->verbs_dev.rdi.driver_f.notify_restart_rc = qib_restart_rc;\n\tdd->verbs_dev.rdi.driver_f.mtu_to_path_mtu = qib_mtu_to_path_mtu;\n\tdd->verbs_dev.rdi.driver_f.mtu_from_qp = qib_mtu_from_qp;\n\tdd->verbs_dev.rdi.driver_f.get_pmtu_from_attr = qib_get_pmtu_from_attr;\n\tdd->verbs_dev.rdi.driver_f.schedule_send_no_lock = _qib_schedule_send;\n\tdd->verbs_dev.rdi.driver_f.query_port_state = qib_query_port;\n\tdd->verbs_dev.rdi.driver_f.shut_down_port = qib_shut_down_port;\n\tdd->verbs_dev.rdi.driver_f.cap_mask_chg = qib_cap_mask_chg;\n\tdd->verbs_dev.rdi.driver_f.notify_create_mad_agent =\n\t\t\t\t\t\tqib_notify_create_mad_agent;\n\tdd->verbs_dev.rdi.driver_f.notify_free_mad_agent =\n\t\t\t\t\t\tqib_notify_free_mad_agent;\n\n\tdd->verbs_dev.rdi.dparms.max_rdma_atomic = QIB_MAX_RDMA_ATOMIC;\n\tdd->verbs_dev.rdi.driver_f.get_guid_be = qib_get_guid_be;\n\tdd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;\n\tdd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;\n\tdd->verbs_dev.rdi.dparms.qpn_start = 1;\n\tdd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;\n\tdd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP;  \n\tdd->verbs_dev.rdi.dparms.qpn_inc = 1;\n\tdd->verbs_dev.rdi.dparms.qos_shift = 1;\n\tdd->verbs_dev.rdi.dparms.psn_mask = QIB_PSN_MASK;\n\tdd->verbs_dev.rdi.dparms.psn_shift = QIB_PSN_SHIFT;\n\tdd->verbs_dev.rdi.dparms.psn_modify_mask = QIB_PSN_MASK;\n\tdd->verbs_dev.rdi.dparms.nports = dd->num_pports;\n\tdd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);\n\tdd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;\n\tdd->verbs_dev.rdi.dparms.core_cap_flags = RDMA_CORE_PORT_IBA_IB;\n\tdd->verbs_dev.rdi.dparms.max_mad_size = IB_MGMT_MAD_SIZE;\n\tdd->verbs_dev.rdi.dparms.sge_copy_mode = RVT_SGE_COPY_MEMCPY;\n\n\tqib_fill_device_attr(dd);\n\n\tppd = dd->pport;\n\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\tctxt = ppd->hw_pidx;\n\t\trvt_init_port(&dd->verbs_dev.rdi,\n\t\t\t      &ppd->ibport_data.rvp,\n\t\t\t      i,\n\t\t\t      dd->rcd[ctxt]->pkeys);\n\t}\n\n\tib_set_device_ops(ibdev, &qib_dev_ops);\n\tret = rvt_register_device(&dd->verbs_dev.rdi);\n\tif (ret)\n\t\tgoto err_tx;\n\n\treturn ret;\n\nerr_tx:\n\twhile (!list_empty(&dev->txreq_free)) {\n\t\tstruct list_head *l = dev->txreq_free.next;\n\t\tstruct qib_verbs_txreq *tx;\n\n\t\tlist_del(l);\n\t\ttx = list_entry(l, struct qib_verbs_txreq, txreq.list);\n\t\tkfree(tx);\n\t}\n\tif (ppd->sdma_descq_cnt)\n\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t  ppd->sdma_descq_cnt *\n\t\t\t\t\tsizeof(struct qib_pio_header),\n\t\t\t\t  dev->pio_hdrs, dev->pio_hdrs_phys);\nerr_hdrs:\n\tqib_dev_err(dd, \"cannot register verbs: %d!\\n\", -ret);\n\treturn ret;\n}\n\nvoid qib_unregister_ib_device(struct qib_devdata *dd)\n{\n\tstruct qib_ibdev *dev = &dd->verbs_dev;\n\n\trvt_unregister_device(&dd->verbs_dev.rdi);\n\n\tif (!list_empty(&dev->piowait))\n\t\tqib_dev_err(dd, \"piowait list not empty!\\n\");\n\tif (!list_empty(&dev->dmawait))\n\t\tqib_dev_err(dd, \"dmawait list not empty!\\n\");\n\tif (!list_empty(&dev->txwait))\n\t\tqib_dev_err(dd, \"txwait list not empty!\\n\");\n\tif (!list_empty(&dev->memwait))\n\t\tqib_dev_err(dd, \"memwait list not empty!\\n\");\n\n\tdel_timer_sync(&dev->mem_timer);\n\twhile (!list_empty(&dev->txreq_free)) {\n\t\tstruct list_head *l = dev->txreq_free.next;\n\t\tstruct qib_verbs_txreq *tx;\n\n\t\tlist_del(l);\n\t\ttx = list_entry(l, struct qib_verbs_txreq, txreq.list);\n\t\tkfree(tx);\n\t}\n\tif (dd->pport->sdma_descq_cnt)\n\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t  dd->pport->sdma_descq_cnt *\n\t\t\t\t\tsizeof(struct qib_pio_header),\n\t\t\t\t  dev->pio_hdrs, dev->pio_hdrs_phys);\n}\n\n \nbool _qib_schedule_send(struct rvt_qp *qp)\n{\n\tstruct qib_ibport *ibp =\n\t\tto_iport(qp->ibqp.device, qp->port_num);\n\tstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\treturn queue_work(ppd->qib_wq, &priv->s_work);\n}\n\n \nbool qib_schedule_send(struct rvt_qp *qp)\n{\n\tif (qib_send_ok(qp))\n\t\treturn _qib_schedule_send(qp);\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}