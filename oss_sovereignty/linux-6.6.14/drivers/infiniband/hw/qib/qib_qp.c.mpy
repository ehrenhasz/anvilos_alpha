{
  "module_name": "qib_qp.c",
  "hash_id": "3f9b36743dd7080234eee810858c8963c571941f06fab4f4549208b84d3762d7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qib/qib_qp.c",
  "human_readable_source": " \n\n#include <linux/err.h>\n#include <linux/vmalloc.h>\n#include <rdma/rdma_vt.h>\n#ifdef CONFIG_DEBUG_FS\n#include <linux/seq_file.h>\n#endif\n\n#include \"qib.h\"\n\nstatic inline unsigned mk_qpn(struct rvt_qpn_table *qpt,\n\t\t\t      struct rvt_qpn_map *map, unsigned off)\n{\n\treturn (map - qpt->map) * RVT_BITS_PER_PAGE + off;\n}\n\nstatic inline unsigned find_next_offset(struct rvt_qpn_table *qpt,\n\t\t\t\t\tstruct rvt_qpn_map *map, unsigned off,\n\t\t\t\t\tunsigned n, u16 qpt_mask)\n{\n\tif (qpt_mask) {\n\t\toff++;\n\t\tif (((off & qpt_mask) >> 1) >= n)\n\t\t\toff = (off | qpt_mask) + 2;\n\t} else {\n\t\toff = find_next_zero_bit(map->page, RVT_BITS_PER_PAGE, off);\n\t}\n\treturn off;\n}\n\nconst struct rvt_operation_params qib_post_parms[RVT_OPERATION_MAX] = {\n[IB_WR_RDMA_WRITE] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_RDMA_READ] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC,\n},\n\n[IB_WR_ATOMIC_CMP_AND_SWP] = {\n\t.length = sizeof(struct ib_atomic_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC | RVT_OPERATION_ATOMIC_SGE,\n},\n\n[IB_WR_ATOMIC_FETCH_AND_ADD] = {\n\t.length = sizeof(struct ib_atomic_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC | RVT_OPERATION_ATOMIC_SGE,\n},\n\n[IB_WR_RDMA_WRITE_WITH_IMM] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_SEND] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_UD) | BIT(IB_QPT_SMI) | BIT(IB_QPT_GSI) |\n\t\t       BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_SEND_WITH_IMM] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_UD) | BIT(IB_QPT_SMI) | BIT(IB_QPT_GSI) |\n\t\t       BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n};\n\nstatic void get_map_page(struct rvt_qpn_table *qpt, struct rvt_qpn_map *map)\n{\n\tunsigned long page = get_zeroed_page(GFP_KERNEL);\n\n\t \n\n\tspin_lock(&qpt->lock);\n\tif (map->page)\n\t\tfree_page(page);\n\telse\n\t\tmap->page = (void *)page;\n\tspin_unlock(&qpt->lock);\n}\n\n \nint qib_alloc_qpn(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt,\n\t\t  enum ib_qp_type type, u32 port)\n{\n\tu32 i, offset, max_scan, qpn;\n\tstruct rvt_qpn_map *map;\n\tu32 ret;\n\tstruct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);\n\tstruct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,\n\t\t\t\t\t      verbs_dev);\n\tu16 qpt_mask = dd->qpn_mask;\n\n\tif (type == IB_QPT_SMI || type == IB_QPT_GSI) {\n\t\tu32 n;\n\n\t\tret = type == IB_QPT_GSI;\n\t\tn = 1 << (ret + 2 * (port - 1));\n\t\tspin_lock(&qpt->lock);\n\t\tif (qpt->flags & n)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tqpt->flags |= n;\n\t\tspin_unlock(&qpt->lock);\n\t\tgoto bail;\n\t}\n\n\tqpn = qpt->last + 2;\n\tif (qpn >= RVT_QPN_MAX)\n\t\tqpn = 2;\n\tif (qpt_mask && ((qpn & qpt_mask) >> 1) >= dd->n_krcv_queues)\n\t\tqpn = (qpn | qpt_mask) + 2;\n\toffset = qpn & RVT_BITS_PER_PAGE_MASK;\n\tmap = &qpt->map[qpn / RVT_BITS_PER_PAGE];\n\tmax_scan = qpt->nmaps - !offset;\n\tfor (i = 0;;) {\n\t\tif (unlikely(!map->page)) {\n\t\t\tget_map_page(qpt, map);\n\t\t\tif (unlikely(!map->page))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tif (!test_and_set_bit(offset, map->page)) {\n\t\t\t\tqpt->last = qpn;\n\t\t\t\tret = qpn;\n\t\t\t\tgoto bail;\n\t\t\t}\n\t\t\toffset = find_next_offset(qpt, map, offset,\n\t\t\t\tdd->n_krcv_queues, qpt_mask);\n\t\t\tqpn = mk_qpn(qpt, map, offset);\n\t\t\t \n\t\t} while (offset < RVT_BITS_PER_PAGE && qpn < RVT_QPN_MAX);\n\t\t \n\t\tif (++i > max_scan) {\n\t\t\tif (qpt->nmaps == RVT_QPNMAP_ENTRIES)\n\t\t\t\tbreak;\n\t\t\tmap = &qpt->map[qpt->nmaps++];\n\t\t\toffset = 0;\n\t\t} else if (map < &qpt->map[qpt->nmaps]) {\n\t\t\t++map;\n\t\t\toffset = 0;\n\t\t} else {\n\t\t\tmap = &qpt->map[0];\n\t\t\toffset = 2;\n\t\t}\n\t\tqpn = mk_qpn(qpt, map, offset);\n\t}\n\n\tret = -ENOMEM;\n\nbail:\n\treturn ret;\n}\n\n \nunsigned qib_free_all_qps(struct rvt_dev_info *rdi)\n{\n\tstruct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);\n\tstruct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,\n\t\t\t\t\t      verbs_dev);\n\tunsigned n, qp_inuse = 0;\n\n\tfor (n = 0; n < dd->num_pports; n++) {\n\t\tstruct qib_ibport *ibp = &dd->pport[n].ibport_data;\n\n\t\trcu_read_lock();\n\t\tif (rcu_dereference(ibp->rvp.qp[0]))\n\t\t\tqp_inuse++;\n\t\tif (rcu_dereference(ibp->rvp.qp[1]))\n\t\t\tqp_inuse++;\n\t\trcu_read_unlock();\n\t}\n\treturn qp_inuse;\n}\n\nvoid qib_notify_qp_reset(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\tatomic_set(&priv->s_dma_busy, 0);\n}\n\nvoid qib_notify_error_qp(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\n\n\tspin_lock(&dev->rdi.pending_lock);\n\tif (!list_empty(&priv->iowait) && !(qp->s_flags & RVT_S_BUSY)) {\n\t\tqp->s_flags &= ~RVT_S_ANY_WAIT_IO;\n\t\tlist_del_init(&priv->iowait);\n\t}\n\tspin_unlock(&dev->rdi.pending_lock);\n\n\tif (!(qp->s_flags & RVT_S_BUSY)) {\n\t\tqp->s_hdrwords = 0;\n\t\tif (qp->s_rdma_mr) {\n\t\t\trvt_put_mr(qp->s_rdma_mr);\n\t\t\tqp->s_rdma_mr = NULL;\n\t\t}\n\t\tif (priv->s_tx) {\n\t\t\tqib_put_txreq(priv->s_tx);\n\t\t\tpriv->s_tx = NULL;\n\t\t}\n\t}\n}\n\nstatic int mtu_to_enum(u32 mtu)\n{\n\tint enum_mtu;\n\n\tswitch (mtu) {\n\tcase 4096:\n\t\tenum_mtu = IB_MTU_4096;\n\t\tbreak;\n\tcase 2048:\n\t\tenum_mtu = IB_MTU_2048;\n\t\tbreak;\n\tcase 1024:\n\t\tenum_mtu = IB_MTU_1024;\n\t\tbreak;\n\tcase 512:\n\t\tenum_mtu = IB_MTU_512;\n\t\tbreak;\n\tcase 256:\n\t\tenum_mtu = IB_MTU_256;\n\t\tbreak;\n\tdefault:\n\t\tenum_mtu = IB_MTU_2048;\n\t}\n\treturn enum_mtu;\n}\n\nint qib_get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t\t   struct ib_qp_attr *attr)\n{\n\tint mtu, pmtu, pidx = qp->port_num - 1;\n\tstruct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);\n\tstruct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,\n\t\t\t\t\t      verbs_dev);\n\tmtu = ib_mtu_enum_to_int(attr->path_mtu);\n\tif (mtu == -1)\n\t\treturn -EINVAL;\n\n\tif (mtu > dd->pport[pidx].ibmtu)\n\t\tpmtu = mtu_to_enum(dd->pport[pidx].ibmtu);\n\telse\n\t\tpmtu = attr->path_mtu;\n\treturn pmtu;\n}\n\nint qib_mtu_to_path_mtu(u32 mtu)\n{\n\treturn mtu_to_enum(mtu);\n}\n\nu32 qib_mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu)\n{\n\treturn ib_mtu_enum_to_int(pmtu);\n}\n\nvoid *qib_qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn ERR_PTR(-ENOMEM);\n\tpriv->owner = qp;\n\n\tpriv->s_hdr = kzalloc(sizeof(*priv->s_hdr), GFP_KERNEL);\n\tif (!priv->s_hdr) {\n\t\tkfree(priv);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinit_waitqueue_head(&priv->wait_dma);\n\tINIT_WORK(&priv->s_work, _qib_do_send);\n\tINIT_LIST_HEAD(&priv->iowait);\n\n\treturn priv;\n}\n\nvoid qib_qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\tkfree(priv->s_hdr);\n\tkfree(priv);\n}\n\nvoid qib_stop_send_queue(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\tcancel_work_sync(&priv->s_work);\n}\n\nvoid qib_quiesce_qp(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\twait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));\n\tif (priv->s_tx) {\n\t\tqib_put_txreq(priv->s_tx);\n\t\tpriv->s_tx = NULL;\n\t}\n}\n\nvoid qib_flush_qp_waiters(struct rvt_qp *qp)\n{\n\tstruct qib_qp_priv *priv = qp->priv;\n\tstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\n\n\tspin_lock(&dev->rdi.pending_lock);\n\tif (!list_empty(&priv->iowait))\n\t\tlist_del_init(&priv->iowait);\n\tspin_unlock(&dev->rdi.pending_lock);\n}\n\n \nint qib_check_send_wqe(struct rvt_qp *qp,\n\t\t       struct rvt_swqe *wqe, bool *call_send)\n{\n\tstruct rvt_ah *ah;\n\n\tswitch (qp->ibqp.qp_type) {\n\tcase IB_QPT_RC:\n\tcase IB_QPT_UC:\n\t\tif (wqe->length > 0x80000000U)\n\t\t\treturn -EINVAL;\n\t\tif (wqe->length > qp->pmtu)\n\t\t\t*call_send = false;\n\t\tbreak;\n\tcase IB_QPT_SMI:\n\tcase IB_QPT_GSI:\n\tcase IB_QPT_UD:\n\t\tah = rvt_get_swqe_ah(wqe);\n\t\tif (wqe->length > (1 << ah->log_pmtu))\n\t\t\treturn -EINVAL;\n\t\t \n\t\t*call_send = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic const char * const qp_type_str[] = {\n\t\"SMI\", \"GSI\", \"RC\", \"UC\", \"UD\",\n};\n\n \nvoid qib_qp_iter_print(struct seq_file *s, struct rvt_qp_iter *iter)\n{\n\tstruct rvt_swqe *wqe;\n\tstruct rvt_qp *qp = iter->qp;\n\tstruct qib_qp_priv *priv = qp->priv;\n\n\twqe = rvt_get_swqe_ptr(qp, qp->s_last);\n\tseq_printf(s,\n\t\t   \"N %d QP%u %s %u %u %u f=%x %u %u %u %u %u PSN %x %x %x %x %x (%u %u %u %u %u %u) QP%u LID %x\\n\",\n\t\t   iter->n,\n\t\t   qp->ibqp.qp_num,\n\t\t   qp_type_str[qp->ibqp.qp_type],\n\t\t   qp->state,\n\t\t   wqe->wr.opcode,\n\t\t   qp->s_hdrwords,\n\t\t   qp->s_flags,\n\t\t   atomic_read(&priv->s_dma_busy),\n\t\t   !list_empty(&priv->iowait),\n\t\t   qp->timeout,\n\t\t   wqe->ssn,\n\t\t   qp->s_lsn,\n\t\t   qp->s_last_psn,\n\t\t   qp->s_psn, qp->s_next_psn,\n\t\t   qp->s_sending_psn, qp->s_sending_hpsn,\n\t\t   qp->s_last, qp->s_acked, qp->s_cur,\n\t\t   qp->s_tail, qp->s_head, qp->s_size,\n\t\t   qp->remote_qpn,\n\t\t   rdma_ah_get_dlid(&qp->remote_ah_attr));\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}