{
  "module_name": "qib_user_sdma.c",
  "hash_id": "890d62ef33d30e34dd5f4adb7e99b6330e449bf2d7642ba40d774b9a14a83661",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qib/qib_user_sdma.c",
  "human_readable_source": " \n#include <linux/mm.h>\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/dmapool.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/io.h>\n#include <linux/uio.h>\n#include <linux/rbtree.h>\n#include <linux/spinlock.h>\n#include <linux/delay.h>\n\n#include \"qib.h\"\n#include \"qib_user_sdma.h\"\n\n \n#define QIB_USER_SDMA_MIN_HEADER_LENGTH 64\n \n#define QIB_USER_SDMA_EXP_HEADER_LENGTH 64\n \n#define QIB_USER_SDMA_DRAIN_TIMEOUT 250\n\n \nstatic struct rb_root qib_user_sdma_rb_root = RB_ROOT;\n\nstruct qib_user_sdma_rb_node {\n\tstruct rb_node node;\n\tint refcount;\n\tpid_t pid;\n};\n\nstruct qib_user_sdma_pkt {\n\tstruct list_head list;   \n\n\tu8  tiddma;\t\t \n\tu8  largepkt;\t\t \n\tu16 frag_size;\t\t \n\tu16 index;               \n\tu16 naddr;               \n\tu16 addrlimit;\t\t \n\tu16 tidsmidx;\t\t \n\tu16 tidsmcount;\t\t \n\tu16 payload_size;\t \n\tu32 bytes_togo;\t\t \n\tu32 counter;             \n\tstruct qib_tid_session_member *tidsm;\t \n\tstruct qib_user_sdma_queue *pq;\t \n\tu64 added;               \n\n\tstruct {\n\t\tu16 offset;                      \n\t\tu16 length;                      \n\t\tu16 first_desc;\t\t\t \n\t\tu16 last_desc;\t\t\t \n\t\tu16 put_page;                    \n\t\tu16 dma_mapped;                  \n\t\tu16 dma_length;\t\t\t \n\t\tu16 padding;\n\t\tstruct page *page;               \n\t\tvoid *kvaddr;                    \n\t\tdma_addr_t addr;\n\t} addr[4];    \n};\n\nstruct qib_user_sdma_queue {\n\t \n\tstruct list_head sent;\n\n\t \n\tspinlock_t sent_lock ____cacheline_aligned_in_smp;\n\n\t \n\tchar header_cache_name[64];\n\tstruct dma_pool *header_cache;\n\n\t \n\tchar pkt_slab_name[64];\n\tstruct kmem_cache *pkt_slab;\n\n\t \n\tu32 counter;\n\tu32 sent_counter;\n\t \n\tu32 num_pending;\n\t \n\tu32 num_sending;\n\t \n\tu64 added;\n\n\t \n\tstruct rb_root dma_pages_root;\n\n\tstruct qib_user_sdma_rb_node *sdma_rb_node;\n\n\t \n\tstruct mutex lock;\n};\n\nstatic struct qib_user_sdma_rb_node *\nqib_user_sdma_rb_search(struct rb_root *root, pid_t pid)\n{\n\tstruct qib_user_sdma_rb_node *sdma_rb_node;\n\tstruct rb_node *node = root->rb_node;\n\n\twhile (node) {\n\t\tsdma_rb_node = rb_entry(node, struct qib_user_sdma_rb_node,\n\t\t\t\t\tnode);\n\t\tif (pid < sdma_rb_node->pid)\n\t\t\tnode = node->rb_left;\n\t\telse if (pid > sdma_rb_node->pid)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn sdma_rb_node;\n\t}\n\treturn NULL;\n}\n\nstatic int\nqib_user_sdma_rb_insert(struct rb_root *root, struct qib_user_sdma_rb_node *new)\n{\n\tstruct rb_node **node = &(root->rb_node);\n\tstruct rb_node *parent = NULL;\n\tstruct qib_user_sdma_rb_node *got;\n\n\twhile (*node) {\n\t\tgot = rb_entry(*node, struct qib_user_sdma_rb_node, node);\n\t\tparent = *node;\n\t\tif (new->pid < got->pid)\n\t\t\tnode = &((*node)->rb_left);\n\t\telse if (new->pid > got->pid)\n\t\t\tnode = &((*node)->rb_right);\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\trb_link_node(&new->node, parent, node);\n\trb_insert_color(&new->node, root);\n\treturn 1;\n}\n\nstruct qib_user_sdma_queue *\nqib_user_sdma_queue_create(struct device *dev, int unit, int ctxt, int sctxt)\n{\n\tstruct qib_user_sdma_queue *pq =\n\t\tkmalloc(sizeof(struct qib_user_sdma_queue), GFP_KERNEL);\n\tstruct qib_user_sdma_rb_node *sdma_rb_node;\n\n\tif (!pq)\n\t\tgoto done;\n\n\tpq->counter = 0;\n\tpq->sent_counter = 0;\n\tpq->num_pending = 0;\n\tpq->num_sending = 0;\n\tpq->added = 0;\n\tpq->sdma_rb_node = NULL;\n\n\tINIT_LIST_HEAD(&pq->sent);\n\tspin_lock_init(&pq->sent_lock);\n\tmutex_init(&pq->lock);\n\n\tsnprintf(pq->pkt_slab_name, sizeof(pq->pkt_slab_name),\n\t\t \"qib-user-sdma-pkts-%u-%02u.%02u\", unit, ctxt, sctxt);\n\tpq->pkt_slab = kmem_cache_create(pq->pkt_slab_name,\n\t\t\t\t\t sizeof(struct qib_user_sdma_pkt),\n\t\t\t\t\t 0, 0, NULL);\n\n\tif (!pq->pkt_slab)\n\t\tgoto err_kfree;\n\n\tsnprintf(pq->header_cache_name, sizeof(pq->header_cache_name),\n\t\t \"qib-user-sdma-headers-%u-%02u.%02u\", unit, ctxt, sctxt);\n\tpq->header_cache = dma_pool_create(pq->header_cache_name,\n\t\t\t\t\t   dev,\n\t\t\t\t\t   QIB_USER_SDMA_EXP_HEADER_LENGTH,\n\t\t\t\t\t   4, 0);\n\tif (!pq->header_cache)\n\t\tgoto err_slab;\n\n\tpq->dma_pages_root = RB_ROOT;\n\n\tsdma_rb_node = qib_user_sdma_rb_search(&qib_user_sdma_rb_root,\n\t\t\t\t\tcurrent->pid);\n\tif (sdma_rb_node) {\n\t\tsdma_rb_node->refcount++;\n\t} else {\n\t\tsdma_rb_node = kmalloc(sizeof(\n\t\t\tstruct qib_user_sdma_rb_node), GFP_KERNEL);\n\t\tif (!sdma_rb_node)\n\t\t\tgoto err_rb;\n\n\t\tsdma_rb_node->refcount = 1;\n\t\tsdma_rb_node->pid = current->pid;\n\n\t\tqib_user_sdma_rb_insert(&qib_user_sdma_rb_root, sdma_rb_node);\n\t}\n\tpq->sdma_rb_node = sdma_rb_node;\n\n\tgoto done;\n\nerr_rb:\n\tdma_pool_destroy(pq->header_cache);\nerr_slab:\n\tkmem_cache_destroy(pq->pkt_slab);\nerr_kfree:\n\tkfree(pq);\n\tpq = NULL;\n\ndone:\n\treturn pq;\n}\n\nstatic void qib_user_sdma_init_frag(struct qib_user_sdma_pkt *pkt,\n\t\t\t\t    int i, u16 offset, u16 len,\n\t\t\t\t    u16 first_desc, u16 last_desc,\n\t\t\t\t    u16 put_page, u16 dma_mapped,\n\t\t\t\t    struct page *page, void *kvaddr,\n\t\t\t\t    dma_addr_t dma_addr, u16 dma_length)\n{\n\tpkt->addr[i].offset = offset;\n\tpkt->addr[i].length = len;\n\tpkt->addr[i].first_desc = first_desc;\n\tpkt->addr[i].last_desc = last_desc;\n\tpkt->addr[i].put_page = put_page;\n\tpkt->addr[i].dma_mapped = dma_mapped;\n\tpkt->addr[i].page = page;\n\tpkt->addr[i].kvaddr = kvaddr;\n\tpkt->addr[i].addr = dma_addr;\n\tpkt->addr[i].dma_length = dma_length;\n}\n\nstatic void *qib_user_sdma_alloc_header(struct qib_user_sdma_queue *pq,\n\t\t\t\tsize_t len, dma_addr_t *dma_addr)\n{\n\tvoid *hdr;\n\n\tif (len == QIB_USER_SDMA_EXP_HEADER_LENGTH)\n\t\thdr = dma_pool_alloc(pq->header_cache, GFP_KERNEL,\n\t\t\t\t\t     dma_addr);\n\telse\n\t\thdr = NULL;\n\n\tif (!hdr) {\n\t\thdr = kmalloc(len, GFP_KERNEL);\n\t\tif (!hdr)\n\t\t\treturn NULL;\n\n\t\t*dma_addr = 0;\n\t}\n\n\treturn hdr;\n}\n\nstatic int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,\n\t\t\t\t       struct qib_user_sdma_queue *pq,\n\t\t\t\t       struct qib_user_sdma_pkt *pkt,\n\t\t\t\t       struct page *page, u16 put,\n\t\t\t\t       u16 offset, u16 len, void *kvaddr)\n{\n\t__le16 *pbc16;\n\tvoid *pbcvaddr;\n\tstruct qib_message_header *hdr;\n\tu16 newlen, pbclen, lastdesc, dma_mapped;\n\tu32 vcto;\n\tunion qib_seqnum seqnum;\n\tdma_addr_t pbcdaddr;\n\tdma_addr_t dma_addr =\n\t\tdma_map_page(&dd->pcidev->dev,\n\t\t\tpage, offset, len, DMA_TO_DEVICE);\n\tint ret = 0;\n\n\tif (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {\n\t\t \n\t\tif (put) {\n\t\t\tunpin_user_page(page);\n\t\t} else {\n\t\t\t \n\t\t\t__free_page(page);\n\t\t}\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\toffset = 0;\n\tdma_mapped = 1;\n\n\nnext_fragment:\n\n\t \n\tif (pkt->tiddma && len > pkt->tidsm[pkt->tidsmidx].length)\n\t\tnewlen = pkt->tidsm[pkt->tidsmidx].length;\n\telse\n\t\tnewlen = len;\n\n\t \n\tlastdesc = 0;\n\tif ((pkt->payload_size + newlen) >= pkt->frag_size) {\n\t\tnewlen = pkt->frag_size - pkt->payload_size;\n\t\tlastdesc = 1;\n\t} else if (pkt->tiddma) {\n\t\tif (newlen == pkt->tidsm[pkt->tidsmidx].length)\n\t\t\tlastdesc = 1;\n\t} else {\n\t\tif (newlen == pkt->bytes_togo)\n\t\t\tlastdesc = 1;\n\t}\n\n\t \n\tqib_user_sdma_init_frag(pkt, pkt->naddr,  \n\t\toffset, newlen,\t\t \n\t\t0, lastdesc,\t\t \n\t\tput, dma_mapped,\t \n\t\tpage, kvaddr,\t\t \n\t\tdma_addr, len);\t\t \n\tpkt->bytes_togo -= newlen;\n\tpkt->payload_size += newlen;\n\tpkt->naddr++;\n\tif (pkt->naddr == pkt->addrlimit) {\n\t\tret = -EFAULT;\n\t\tgoto done;\n\t}\n\n\t \n\tif (pkt->bytes_togo == 0) {\n\t\t \n\t\tif (!pkt->addr[pkt->index].addr) {\n\t\t\tpkt->addr[pkt->index].addr =\n\t\t\t\tdma_map_single(&dd->pcidev->dev,\n\t\t\t\t\tpkt->addr[pkt->index].kvaddr,\n\t\t\t\t\tpkt->addr[pkt->index].dma_length,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&dd->pcidev->dev,\n\t\t\t\t\tpkt->addr[pkt->index].addr)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tpkt->addr[pkt->index].dma_mapped = 1;\n\t\t}\n\n\t\tgoto done;\n\t}\n\n\t \n\tif (pkt->tiddma) {\n\t\tpkt->tidsm[pkt->tidsmidx].length -= newlen;\n\t\tif (pkt->tidsm[pkt->tidsmidx].length) {\n\t\t\tpkt->tidsm[pkt->tidsmidx].offset += newlen;\n\t\t} else {\n\t\t\tpkt->tidsmidx++;\n\t\t\tif (pkt->tidsmidx == pkt->tidsmcount) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (lastdesc == 0)\n\t\tgoto done;\n\n\t \n\n\t \n\tpbclen = pkt->addr[pkt->index].length;\n\tpbcvaddr = qib_user_sdma_alloc_header(pq, pbclen, &pbcdaddr);\n\tif (!pbcvaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\t \n\tpbc16 = (__le16 *)pkt->addr[pkt->index].kvaddr;\n\tmemcpy(pbcvaddr, pbc16, pbclen);\n\n\t \n\thdr = (struct qib_message_header *)&pbc16[4];\n\n\t \n\tpbc16[0] = cpu_to_le16(le16_to_cpu(pbc16[0])-(pkt->bytes_togo>>2));\n\n\t \n\thdr->lrh[2] = cpu_to_be16(le16_to_cpu(pbc16[0]));\n\n\tif (pkt->tiddma) {\n\t\t \n\t\thdr->iph.pkt_flags =\n\t\t\tcpu_to_le16(le16_to_cpu(hdr->iph.pkt_flags)|0x2);\n\t\t \n\t\thdr->flags &= ~(0x04|0x20);\n\t} else {\n\t\t \n\t\thdr->bth[0] = cpu_to_be32(be32_to_cpu(hdr->bth[0])&0xFFCFFFFF);\n\t\t \n\t\thdr->flags &= ~(0x04);\n\t}\n\n\t \n\tvcto = le32_to_cpu(hdr->iph.ver_ctxt_tid_offset);\n\thdr->iph.chksum = cpu_to_le16(QIB_LRH_BTH +\n\t\tbe16_to_cpu(hdr->lrh[2]) -\n\t\t((vcto>>16)&0xFFFF) - (vcto&0xFFFF) -\n\t\tle16_to_cpu(hdr->iph.pkt_flags));\n\n\t \n\tif (!pkt->addr[pkt->index].addr) {\n\t\tpkt->addr[pkt->index].addr =\n\t\t\tdma_map_single(&dd->pcidev->dev,\n\t\t\t\tpkt->addr[pkt->index].kvaddr,\n\t\t\t\tpkt->addr[pkt->index].dma_length,\n\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&dd->pcidev->dev,\n\t\t\t\tpkt->addr[pkt->index].addr)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto done;\n\t\t}\n\t\tpkt->addr[pkt->index].dma_mapped = 1;\n\t}\n\n\t \n\tpbc16 = (__le16 *)pbcvaddr;\n\thdr = (struct qib_message_header *)&pbc16[4];\n\n\t \n\tpbc16[0] = cpu_to_le16(le16_to_cpu(pbc16[0])-(pkt->payload_size>>2));\n\n\t \n\thdr->lrh[2] = cpu_to_be16(le16_to_cpu(pbc16[0]));\n\n\tif (pkt->tiddma) {\n\t\t \n\t\thdr->iph.ver_ctxt_tid_offset = cpu_to_le32(\n\t\t\t(le32_to_cpu(hdr->iph.ver_ctxt_tid_offset)&0xFF000000) +\n\t\t\t(pkt->tidsm[pkt->tidsmidx].tid<<QLOGIC_IB_I_TID_SHIFT) +\n\t\t\t(pkt->tidsm[pkt->tidsmidx].offset>>2));\n\t} else {\n\t\t \n\t\thdr->uwords[2] += pkt->payload_size;\n\t}\n\n\t \n\tvcto = le32_to_cpu(hdr->iph.ver_ctxt_tid_offset);\n\thdr->iph.chksum = cpu_to_le16(QIB_LRH_BTH +\n\t\tbe16_to_cpu(hdr->lrh[2]) -\n\t\t((vcto>>16)&0xFFFF) - (vcto&0xFFFF) -\n\t\tle16_to_cpu(hdr->iph.pkt_flags));\n\n\t \n\tseqnum.val = be32_to_cpu(hdr->bth[2]);\n\tif (pkt->tiddma)\n\t\tseqnum.seq++;\n\telse\n\t\tseqnum.pkt++;\n\thdr->bth[2] = cpu_to_be32(seqnum.val);\n\n\t \n\tqib_user_sdma_init_frag(pkt, pkt->naddr,  \n\t\t0, pbclen,\t\t \n\t\t1, 0,\t\t\t \n\t\t0, 0,\t\t\t \n\t\tNULL, pbcvaddr,\t\t \n\t\tpbcdaddr, pbclen);\t \n\tpkt->index = pkt->naddr;\n\tpkt->payload_size = 0;\n\tpkt->naddr++;\n\tif (pkt->naddr == pkt->addrlimit) {\n\t\tret = -EFAULT;\n\t\tgoto done;\n\t}\n\n\t \n\tif (newlen != len) {\n\t\tif (dma_mapped) {\n\t\t\tput = 0;\n\t\t\tdma_mapped = 0;\n\t\t\tpage = NULL;\n\t\t\tkvaddr = NULL;\n\t\t}\n\t\tlen -= newlen;\n\t\toffset += newlen;\n\n\t\tgoto next_fragment;\n\t}\n\ndone:\n\treturn ret;\n}\n\n \nstatic int qib_user_sdma_coalesce(const struct qib_devdata *dd,\n\t\t\t\t  struct qib_user_sdma_queue *pq,\n\t\t\t\t  struct qib_user_sdma_pkt *pkt,\n\t\t\t\t  const struct iovec *iov,\n\t\t\t\t  unsigned long niov)\n{\n\tint ret = 0;\n\tstruct page *page = alloc_page(GFP_KERNEL);\n\tvoid *mpage_save;\n\tchar *mpage;\n\tint i;\n\tint len = 0;\n\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tmpage = page_address(page);\n\tmpage_save = mpage;\n\tfor (i = 0; i < niov; i++) {\n\t\tint cfur;\n\n\t\tcfur = copy_from_user(mpage,\n\t\t\t\t      iov[i].iov_base, iov[i].iov_len);\n\t\tif (cfur) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto page_free;\n\t\t}\n\n\t\tmpage += iov[i].iov_len;\n\t\tlen += iov[i].iov_len;\n\t}\n\n\tret = qib_user_sdma_page_to_frags(dd, pq, pkt,\n\t\t\tpage, 0, 0, len, mpage_save);\n\tgoto done;\n\npage_free:\n\t__free_page(page);\ndone:\n\treturn ret;\n}\n\n \nstatic size_t qib_user_sdma_num_pages(const struct iovec *iov)\n{\n\tconst unsigned long addr  = (unsigned long) iov->iov_base;\n\tconst unsigned long  len  = iov->iov_len;\n\tconst unsigned long spage = addr & PAGE_MASK;\n\tconst unsigned long epage = (addr + len - 1) & PAGE_MASK;\n\n\treturn 1 + ((epage - spage) >> PAGE_SHIFT);\n}\n\nstatic void qib_user_sdma_free_pkt_frag(struct device *dev,\n\t\t\t\t\tstruct qib_user_sdma_queue *pq,\n\t\t\t\t\tstruct qib_user_sdma_pkt *pkt,\n\t\t\t\t\tint frag)\n{\n\tconst int i = frag;\n\n\tif (pkt->addr[i].page) {\n\t\t \n\t\tif (pkt->addr[i].dma_mapped)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       pkt->addr[i].addr,\n\t\t\t\t       pkt->addr[i].dma_length,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\tif (pkt->addr[i].put_page)\n\t\t\tunpin_user_page(pkt->addr[i].page);\n\t\telse\n\t\t\t__free_page(pkt->addr[i].page);\n\t} else if (pkt->addr[i].kvaddr) {\n\t\t \n\t\tif (pkt->addr[i].dma_mapped) {\n\t\t\t \n\t\t\tdma_unmap_single(dev,\n\t\t\t\t       pkt->addr[i].addr,\n\t\t\t\t       pkt->addr[i].dma_length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t\tkfree(pkt->addr[i].kvaddr);\n\t\t} else if (pkt->addr[i].addr) {\n\t\t\t \n\t\t\tdma_pool_free(pq->header_cache,\n\t\t\t      pkt->addr[i].kvaddr, pkt->addr[i].addr);\n\t\t} else {\n\t\t\t \n\t\t\tkfree(pkt->addr[i].kvaddr);\n\t\t}\n\t}\n}\n\n \nstatic int qib_user_sdma_pin_pages(const struct qib_devdata *dd,\n\t\t\t\t   struct qib_user_sdma_queue *pq,\n\t\t\t\t   struct qib_user_sdma_pkt *pkt,\n\t\t\t\t   unsigned long addr, int tlen, size_t npages)\n{\n\tstruct page *pages[8];\n\tint i, j;\n\tint ret = 0;\n\n\twhile (npages) {\n\t\tif (npages > 8)\n\t\t\tj = 8;\n\t\telse\n\t\t\tj = npages;\n\n\t\tret = pin_user_pages_fast(addr, j, FOLL_LONGTERM, pages);\n\t\tif (ret != j) {\n\t\t\ti = 0;\n\t\t\tj = ret;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_pages;\n\t\t}\n\n\t\tfor (i = 0; i < j; i++) {\n\t\t\t \n\t\t\tunsigned long fofs = addr & ~PAGE_MASK;\n\t\t\tint flen = ((fofs + tlen) > PAGE_SIZE) ?\n\t\t\t\t(PAGE_SIZE - fofs) : tlen;\n\n\t\t\tret = qib_user_sdma_page_to_frags(dd, pq, pkt,\n\t\t\t\tpages[i], 1, fofs, flen, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\t \n\t\t\t\ti++;\n\t\t\t\tgoto free_pages;\n\t\t\t}\n\n\t\t\taddr += flen;\n\t\t\ttlen -= flen;\n\t\t}\n\n\t\tnpages -= j;\n\t}\n\n\tgoto done;\n\n\t \nfree_pages:\n\twhile (i < j)\n\t\tunpin_user_page(pages[i++]);\n\ndone:\n\treturn ret;\n}\n\nstatic int qib_user_sdma_pin_pkt(const struct qib_devdata *dd,\n\t\t\t\t struct qib_user_sdma_queue *pq,\n\t\t\t\t struct qib_user_sdma_pkt *pkt,\n\t\t\t\t const struct iovec *iov,\n\t\t\t\t unsigned long niov)\n{\n\tint ret = 0;\n\tunsigned long idx;\n\n\tfor (idx = 0; idx < niov; idx++) {\n\t\tconst size_t npages = qib_user_sdma_num_pages(iov + idx);\n\t\tconst unsigned long addr = (unsigned long) iov[idx].iov_base;\n\n\t\tret = qib_user_sdma_pin_pages(dd, pq, pkt, addr,\n\t\t\t\t\t      iov[idx].iov_len, npages);\n\t\tif (ret < 0)\n\t\t\tgoto free_pkt;\n\t}\n\n\tgoto done;\n\nfree_pkt:\n\t \n\tfor (idx = 1; idx < pkt->naddr; idx++)\n\t\tqib_user_sdma_free_pkt_frag(&dd->pcidev->dev, pq, pkt, idx);\n\n\t \n\tif (pkt->addr[0].dma_mapped) {\n\t\tdma_unmap_single(&dd->pcidev->dev,\n\t\t       pkt->addr[0].addr,\n\t\t       pkt->addr[0].dma_length,\n\t\t       DMA_TO_DEVICE);\n\t\tpkt->addr[0].addr = 0;\n\t\tpkt->addr[0].dma_mapped = 0;\n\t}\n\ndone:\n\treturn ret;\n}\n\nstatic int qib_user_sdma_init_payload(const struct qib_devdata *dd,\n\t\t\t\t      struct qib_user_sdma_queue *pq,\n\t\t\t\t      struct qib_user_sdma_pkt *pkt,\n\t\t\t\t      const struct iovec *iov,\n\t\t\t\t      unsigned long niov, int npages)\n{\n\tint ret = 0;\n\n\tif (pkt->frag_size == pkt->bytes_togo &&\n\t\t\tnpages >= ARRAY_SIZE(pkt->addr))\n\t\tret = qib_user_sdma_coalesce(dd, pq, pkt, iov, niov);\n\telse\n\t\tret = qib_user_sdma_pin_pkt(dd, pq, pkt, iov, niov);\n\n\treturn ret;\n}\n\n \nstatic void qib_user_sdma_free_pkt_list(struct device *dev,\n\t\t\t\t\tstruct qib_user_sdma_queue *pq,\n\t\t\t\t\tstruct list_head *list)\n{\n\tstruct qib_user_sdma_pkt *pkt, *pkt_next;\n\n\tlist_for_each_entry_safe(pkt, pkt_next, list, list) {\n\t\tint i;\n\n\t\tfor (i = 0; i < pkt->naddr; i++)\n\t\t\tqib_user_sdma_free_pkt_frag(dev, pq, pkt, i);\n\n\t\tif (pkt->largepkt)\n\t\t\tkfree(pkt);\n\t\telse\n\t\t\tkmem_cache_free(pq->pkt_slab, pkt);\n\t}\n\tINIT_LIST_HEAD(list);\n}\n\n \nstatic int qib_user_sdma_queue_pkts(const struct qib_devdata *dd,\n\t\t\t\t    struct qib_pportdata *ppd,\n\t\t\t\t    struct qib_user_sdma_queue *pq,\n\t\t\t\t    const struct iovec *iov,\n\t\t\t\t    unsigned long niov,\n\t\t\t\t    struct list_head *list,\n\t\t\t\t    int *maxpkts, int *ndesc)\n{\n\tunsigned long idx = 0;\n\tint ret = 0;\n\tint npkts = 0;\n\t__le32 *pbc;\n\tdma_addr_t dma_addr;\n\tstruct qib_user_sdma_pkt *pkt = NULL;\n\tsize_t len;\n\tsize_t nw;\n\tu32 counter = pq->counter;\n\tu16 frag_size;\n\n\twhile (idx < niov && npkts < *maxpkts) {\n\t\tconst unsigned long addr = (unsigned long) iov[idx].iov_base;\n\t\tconst unsigned long idx_save = idx;\n\t\tunsigned pktnw;\n\t\tunsigned pktnwc;\n\t\tint nfrags = 0;\n\t\tsize_t npages = 0;\n\t\tsize_t bytes_togo = 0;\n\t\tint tiddma = 0;\n\t\tint cfur;\n\n\t\tlen = iov[idx].iov_len;\n\t\tnw = len >> 2;\n\n\t\tif (len < QIB_USER_SDMA_MIN_HEADER_LENGTH ||\n\t\t    len > PAGE_SIZE || len & 3 || addr & 3) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_list;\n\t\t}\n\n\t\tpbc = qib_user_sdma_alloc_header(pq, len, &dma_addr);\n\t\tif (!pbc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_list;\n\t\t}\n\n\t\tcfur = copy_from_user(pbc, iov[idx].iov_base, len);\n\t\tif (cfur) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_pbc;\n\t\t}\n\n\t\t \n\t\tpktnwc = nw - 1;\n\n\t\t \n\t\tpktnw = le32_to_cpu(*pbc) & 0xFFFF;\n\t\tif (pktnw < pktnwc) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_pbc;\n\t\t}\n\n\t\tidx++;\n\t\twhile (pktnwc < pktnw && idx < niov) {\n\t\t\tconst size_t slen = iov[idx].iov_len;\n\t\t\tconst unsigned long faddr =\n\t\t\t\t(unsigned long) iov[idx].iov_base;\n\n\t\t\tif (slen & 3 || faddr & 3 || !slen) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto free_pbc;\n\t\t\t}\n\n\t\t\tnpages += qib_user_sdma_num_pages(&iov[idx]);\n\n\t\t\tif (check_add_overflow(bytes_togo, slen, &bytes_togo) ||\n\t\t\t    bytes_togo > type_max(typeof(pkt->bytes_togo))) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto free_pbc;\n\t\t\t}\n\t\t\tpktnwc += slen >> 2;\n\t\t\tidx++;\n\t\t\tnfrags++;\n\t\t}\n\n\t\tif (pktnwc != pktnw) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_pbc;\n\t\t}\n\n\t\tfrag_size = ((le32_to_cpu(*pbc))>>16) & 0xFFFF;\n\t\tif (((frag_size ? frag_size : bytes_togo) + len) >\n\t\t\t\t\t\tppd->ibmaxlen) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_pbc;\n\t\t}\n\n\t\tif (frag_size) {\n\t\t\tsize_t tidsmsize, n, pktsize, sz, addrlimit;\n\n\t\t\tn = npages*((2*PAGE_SIZE/frag_size)+1);\n\t\t\tpktsize = struct_size(pkt, addr, n);\n\n\t\t\t \n\t\t\ttiddma = (((le32_to_cpu(pbc[7])>>\n\t\t\t\tQLOGIC_IB_I_TID_SHIFT)&\n\t\t\t\tQLOGIC_IB_I_TID_MASK) !=\n\t\t\t\tQLOGIC_IB_I_TID_MASK);\n\n\t\t\tif (tiddma)\n\t\t\t\ttidsmsize = iov[idx].iov_len;\n\t\t\telse\n\t\t\t\ttidsmsize = 0;\n\n\t\t\tif (check_add_overflow(pktsize, tidsmsize, &sz)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto free_pbc;\n\t\t\t}\n\t\t\tpkt = kmalloc(sz, GFP_KERNEL);\n\t\t\tif (!pkt) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto free_pbc;\n\t\t\t}\n\t\t\tpkt->largepkt = 1;\n\t\t\tpkt->frag_size = frag_size;\n\t\t\tif (check_add_overflow(n, ARRAY_SIZE(pkt->addr),\n\t\t\t\t\t       &addrlimit) ||\n\t\t\t    addrlimit > type_max(typeof(pkt->addrlimit))) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto free_pkt;\n\t\t\t}\n\t\t\tpkt->addrlimit = addrlimit;\n\n\t\t\tif (tiddma) {\n\t\t\t\tchar *tidsm = (char *)pkt + pktsize;\n\n\t\t\t\tcfur = copy_from_user(tidsm,\n\t\t\t\t\tiov[idx].iov_base, tidsmsize);\n\t\t\t\tif (cfur) {\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\t\tgoto free_pkt;\n\t\t\t\t}\n\t\t\t\tpkt->tidsm =\n\t\t\t\t\t(struct qib_tid_session_member *)tidsm;\n\t\t\t\tpkt->tidsmcount = tidsmsize/\n\t\t\t\t\tsizeof(struct qib_tid_session_member);\n\t\t\t\tpkt->tidsmidx = 0;\n\t\t\t\tidx++;\n\t\t\t}\n\n\t\t\t \n\t\t\t*pbc = cpu_to_le32(le32_to_cpu(*pbc) & 0x0000FFFF);\n\t\t} else {\n\t\t\tpkt = kmem_cache_alloc(pq->pkt_slab, GFP_KERNEL);\n\t\t\tif (!pkt) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto free_pbc;\n\t\t\t}\n\t\t\tpkt->largepkt = 0;\n\t\t\tpkt->frag_size = bytes_togo;\n\t\t\tpkt->addrlimit = ARRAY_SIZE(pkt->addr);\n\t\t}\n\t\tpkt->bytes_togo = bytes_togo;\n\t\tpkt->payload_size = 0;\n\t\tpkt->counter = counter;\n\t\tpkt->tiddma = tiddma;\n\n\t\t \n\t\tqib_user_sdma_init_frag(pkt, 0,  \n\t\t\t0, len,\t\t \n\t\t\t1, 0,\t\t \n\t\t\t0, 0,\t\t \n\t\t\tNULL, pbc,\t \n\t\t\tdma_addr, len);\t \n\t\tpkt->index = 0;\n\t\tpkt->naddr = 1;\n\n\t\tif (nfrags) {\n\t\t\tret = qib_user_sdma_init_payload(dd, pq, pkt,\n\t\t\t\t\t\t\t iov + idx_save + 1,\n\t\t\t\t\t\t\t nfrags, npages);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto free_pkt;\n\t\t} else {\n\t\t\t \n\t\t\tpkt->addr[0].last_desc = 1;\n\n\t\t\tif (dma_addr == 0) {\n\t\t\t\t \n\t\t\t\tdma_addr = dma_map_single(&dd->pcidev->dev,\n\t\t\t\t\tpbc, len, DMA_TO_DEVICE);\n\t\t\t\tif (dma_mapping_error(&dd->pcidev->dev,\n\t\t\t\t\t\t\t\tdma_addr)) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto free_pkt;\n\t\t\t\t}\n\t\t\t\tpkt->addr[0].addr = dma_addr;\n\t\t\t\tpkt->addr[0].dma_mapped = 1;\n\t\t\t}\n\t\t}\n\n\t\tcounter++;\n\t\tnpkts++;\n\t\tpkt->pq = pq;\n\t\tpkt->index = 0;  \n\t\t*ndesc += pkt->naddr;\n\n\t\tlist_add_tail(&pkt->list, list);\n\t}\n\n\t*maxpkts = npkts;\n\tret = idx;\n\tgoto done;\n\nfree_pkt:\n\tif (pkt->largepkt)\n\t\tkfree(pkt);\n\telse\n\t\tkmem_cache_free(pq->pkt_slab, pkt);\nfree_pbc:\n\tif (dma_addr)\n\t\tdma_pool_free(pq->header_cache, pbc, dma_addr);\n\telse\n\t\tkfree(pbc);\nfree_list:\n\tqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, list);\ndone:\n\treturn ret;\n}\n\nstatic void qib_user_sdma_set_complete_counter(struct qib_user_sdma_queue *pq,\n\t\t\t\t\t       u32 c)\n{\n\tpq->sent_counter = c;\n}\n\n \nstatic int qib_user_sdma_queue_clean(struct qib_pportdata *ppd,\n\t\t\t\t     struct qib_user_sdma_queue *pq)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\tstruct list_head free_list;\n\tstruct qib_user_sdma_pkt *pkt;\n\tstruct qib_user_sdma_pkt *pkt_prev;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!pq->num_sending)\n\t\treturn 0;\n\n\tINIT_LIST_HEAD(&free_list);\n\n\t \n\tspin_lock_irqsave(&pq->sent_lock, flags);\n\tlist_for_each_entry_safe(pkt, pkt_prev, &pq->sent, list) {\n\t\ts64 descd = ppd->sdma_descq_removed - pkt->added;\n\n\t\tif (descd < 0)\n\t\t\tbreak;\n\n\t\tlist_move_tail(&pkt->list, &free_list);\n\n\t\t \n\t\tret++;\n\t\tpq->num_sending--;\n\t}\n\tspin_unlock_irqrestore(&pq->sent_lock, flags);\n\n\tif (!list_empty(&free_list)) {\n\t\tu32 counter;\n\n\t\tpkt = list_entry(free_list.prev,\n\t\t\t\t struct qib_user_sdma_pkt, list);\n\t\tcounter = pkt->counter;\n\n\t\tqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\n\t\tqib_user_sdma_set_complete_counter(pq, counter);\n\t}\n\n\treturn ret;\n}\n\nvoid qib_user_sdma_queue_destroy(struct qib_user_sdma_queue *pq)\n{\n\tif (!pq)\n\t\treturn;\n\n\tpq->sdma_rb_node->refcount--;\n\tif (pq->sdma_rb_node->refcount == 0) {\n\t\trb_erase(&pq->sdma_rb_node->node, &qib_user_sdma_rb_root);\n\t\tkfree(pq->sdma_rb_node);\n\t}\n\tdma_pool_destroy(pq->header_cache);\n\tkmem_cache_destroy(pq->pkt_slab);\n\tkfree(pq);\n}\n\n \nstatic int qib_user_sdma_hwqueue_clean(struct qib_pportdata *ppd)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\tret = qib_sdma_make_progress(ppd);\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\n\treturn ret;\n}\n\n \nvoid qib_user_sdma_queue_drain(struct qib_pportdata *ppd,\n\t\t\t       struct qib_user_sdma_queue *pq)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\tunsigned long flags;\n\tint i;\n\n\tif (!pq)\n\t\treturn;\n\n\tfor (i = 0; i < QIB_USER_SDMA_DRAIN_TIMEOUT; i++) {\n\t\tmutex_lock(&pq->lock);\n\t\tif (!pq->num_pending && !pq->num_sending) {\n\t\t\tmutex_unlock(&pq->lock);\n\t\t\tbreak;\n\t\t}\n\t\tqib_user_sdma_hwqueue_clean(ppd);\n\t\tqib_user_sdma_queue_clean(ppd, pq);\n\t\tmutex_unlock(&pq->lock);\n\t\tmsleep(20);\n\t}\n\n\tif (pq->num_pending || pq->num_sending) {\n\t\tstruct qib_user_sdma_pkt *pkt;\n\t\tstruct qib_user_sdma_pkt *pkt_prev;\n\t\tstruct list_head free_list;\n\n\t\tmutex_lock(&pq->lock);\n\t\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\t\t \n\t\tif (pq->num_pending) {\n\t\t\tlist_for_each_entry_safe(pkt, pkt_prev,\n\t\t\t\t\t&ppd->sdma_userpending, list) {\n\t\t\t\tif (pkt->pq == pq) {\n\t\t\t\t\tlist_move_tail(&pkt->list, &pq->sent);\n\t\t\t\t\tpq->num_pending--;\n\t\t\t\t\tpq->num_sending++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\n\t\tqib_dev_err(dd, \"user sdma lists not empty: forcing!\\n\");\n\t\tINIT_LIST_HEAD(&free_list);\n\t\tlist_splice_init(&pq->sent, &free_list);\n\t\tpq->num_sending = 0;\n\t\tqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\n\t\tmutex_unlock(&pq->lock);\n\t}\n}\n\nstatic inline __le64 qib_sdma_make_desc0(u8 gen,\n\t\t\t\t\t u64 addr, u64 dwlen, u64 dwoffset)\n{\n\treturn cpu_to_le64( \n\t\t\t   ((addr & 0xfffffffcULL) << 32) |\n\t\t\t    \n\t\t\t   ((gen & 3ULL) << 30) |\n\t\t\t    \n\t\t\t   ((dwlen & 0x7ffULL) << 16) |\n\t\t\t    \n\t\t\t   (dwoffset & 0x7ffULL));\n}\n\nstatic inline __le64 qib_sdma_make_first_desc0(__le64 descq)\n{\n\treturn descq | cpu_to_le64(1ULL << 12);\n}\n\nstatic inline __le64 qib_sdma_make_last_desc0(__le64 descq)\n{\n\t\t\t\t\t          \n\treturn descq | cpu_to_le64(1ULL << 11 | 1ULL << 13);\n}\n\nstatic inline __le64 qib_sdma_make_desc1(u64 addr)\n{\n\t \n\treturn cpu_to_le64(addr >> 32);\n}\n\nstatic void qib_user_sdma_send_frag(struct qib_pportdata *ppd,\n\t\t\t\t    struct qib_user_sdma_pkt *pkt, int idx,\n\t\t\t\t    unsigned ofs, u16 tail, u8 gen)\n{\n\tconst u64 addr = (u64) pkt->addr[idx].addr +\n\t\t(u64) pkt->addr[idx].offset;\n\tconst u64 dwlen = (u64) pkt->addr[idx].length / 4;\n\t__le64 *descqp;\n\t__le64 descq0;\n\n\tdescqp = &ppd->sdma_descq[tail].qw[0];\n\n\tdescq0 = qib_sdma_make_desc0(gen, addr, dwlen, ofs);\n\tif (pkt->addr[idx].first_desc)\n\t\tdescq0 = qib_sdma_make_first_desc0(descq0);\n\tif (pkt->addr[idx].last_desc) {\n\t\tdescq0 = qib_sdma_make_last_desc0(descq0);\n\t\tif (ppd->sdma_intrequest) {\n\t\t\tdescq0 |= cpu_to_le64(1ULL << 15);\n\t\t\tppd->sdma_intrequest = 0;\n\t\t}\n\t}\n\n\tdescqp[0] = descq0;\n\tdescqp[1] = qib_sdma_make_desc1(addr);\n}\n\nvoid qib_user_sdma_send_desc(struct qib_pportdata *ppd,\n\t\t\t\tstruct list_head *pktlist)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\tu16 nfree, nsent;\n\tu16 tail, tail_c;\n\tu8 gen, gen_c;\n\n\tnfree = qib_sdma_descq_freecnt(ppd);\n\tif (!nfree)\n\t\treturn;\n\nretry:\n\tnsent = 0;\n\ttail_c = tail = ppd->sdma_descq_tail;\n\tgen_c = gen = ppd->sdma_generation;\n\twhile (!list_empty(pktlist)) {\n\t\tstruct qib_user_sdma_pkt *pkt =\n\t\t\tlist_entry(pktlist->next, struct qib_user_sdma_pkt,\n\t\t\t\t   list);\n\t\tint i, j, c = 0;\n\t\tunsigned ofs = 0;\n\t\tu16 dtail = tail;\n\n\t\tfor (i = pkt->index; i < pkt->naddr && nfree; i++) {\n\t\t\tqib_user_sdma_send_frag(ppd, pkt, i, ofs, tail, gen);\n\t\t\tofs += pkt->addr[i].length >> 2;\n\n\t\t\tif (++tail == ppd->sdma_descq_cnt) {\n\t\t\t\ttail = 0;\n\t\t\t\t++gen;\n\t\t\t\tppd->sdma_intrequest = 1;\n\t\t\t} else if (tail == (ppd->sdma_descq_cnt>>1)) {\n\t\t\t\tppd->sdma_intrequest = 1;\n\t\t\t}\n\t\t\tnfree--;\n\t\t\tif (pkt->addr[i].last_desc == 0)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (ofs > dd->piosize2kmax_dwords) {\n\t\t\t\tfor (j = pkt->index; j <= i; j++) {\n\t\t\t\t\tppd->sdma_descq[dtail].qw[0] |=\n\t\t\t\t\t\tcpu_to_le64(1ULL << 14);\n\t\t\t\t\tif (++dtail == ppd->sdma_descq_cnt)\n\t\t\t\t\t\tdtail = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t\tc += i + 1 - pkt->index;\n\t\t\tpkt->index = i + 1;  \n\t\t\ttail_c = dtail = tail;\n\t\t\tgen_c = gen;\n\t\t\tofs = 0;   \n\t\t}\n\n\t\tppd->sdma_descq_added += c;\n\t\tnsent += c;\n\t\tif (pkt->index == pkt->naddr) {\n\t\t\tpkt->added = ppd->sdma_descq_added;\n\t\t\tpkt->pq->added = pkt->added;\n\t\t\tpkt->pq->num_pending--;\n\t\t\tspin_lock(&pkt->pq->sent_lock);\n\t\t\tpkt->pq->num_sending++;\n\t\t\tlist_move_tail(&pkt->list, &pkt->pq->sent);\n\t\t\tspin_unlock(&pkt->pq->sent_lock);\n\t\t}\n\t\tif (!nfree || (nsent<<2) > ppd->sdma_descq_cnt)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (ppd->sdma_descq_tail != tail_c) {\n\t\tppd->sdma_generation = gen_c;\n\t\tdd->f_sdma_update_tail(ppd, tail_c);\n\t}\n\n\tif (nfree && !list_empty(pktlist))\n\t\tgoto retry;\n}\n\n \nstatic int qib_user_sdma_push_pkts(struct qib_pportdata *ppd,\n\t\t\t\t struct qib_user_sdma_queue *pq,\n\t\t\t\t struct list_head *pktlist, int count)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!(ppd->lflags & QIBL_LINKACTIVE)))\n\t\treturn -ECOMM;\n\n\t \n\tif (pq->sdma_rb_node->refcount > 1) {\n\t\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\t\tif (unlikely(!__qib_sdma_running(ppd))) {\n\t\t\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\t\t\treturn -ECOMM;\n\t\t}\n\t\tpq->num_pending += count;\n\t\tlist_splice_tail_init(pktlist, &ppd->sdma_userpending);\n\t\tqib_user_sdma_send_desc(ppd, &ppd->sdma_userpending);\n\t\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\t\treturn 0;\n\t}\n\n\t \n\n\n\tpq->num_pending += count;\n\t \n\tdo {\n\t\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\t\tif (unlikely(!__qib_sdma_running(ppd))) {\n\t\t\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\t\t\treturn -ECOMM;\n\t\t}\n\t\tqib_user_sdma_send_desc(ppd, pktlist);\n\t\tif (!list_empty(pktlist))\n\t\t\tqib_sdma_make_progress(ppd);\n\t\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\t} while (!list_empty(pktlist));\n\n\treturn 0;\n}\n\nint qib_user_sdma_writev(struct qib_ctxtdata *rcd,\n\t\t\t struct qib_user_sdma_queue *pq,\n\t\t\t const struct iovec *iov,\n\t\t\t unsigned long dim)\n{\n\tstruct qib_devdata *dd = rcd->dd;\n\tstruct qib_pportdata *ppd = rcd->ppd;\n\tint ret = 0;\n\tstruct list_head list;\n\tint npkts = 0;\n\n\tINIT_LIST_HEAD(&list);\n\n\tmutex_lock(&pq->lock);\n\n\t \n\tif (!qib_sdma_running(ppd))\n\t\tgoto done_unlock;\n\n\t \n\tif (pq->added > ppd->sdma_descq_removed)\n\t\tqib_user_sdma_hwqueue_clean(ppd);\n\t \n\tif (pq->num_sending)\n\t\tqib_user_sdma_queue_clean(ppd, pq);\n\n\twhile (dim) {\n\t\tint mxp = 1;\n\t\tint ndesc = 0;\n\n\t\tret = qib_user_sdma_queue_pkts(dd, ppd, pq,\n\t\t\t\tiov, dim, &list, &mxp, &ndesc);\n\t\tif (ret < 0)\n\t\t\tgoto done_unlock;\n\t\telse {\n\t\t\tdim -= ret;\n\t\t\tiov += ret;\n\t\t}\n\n\t\t \n\t\tif (!list_empty(&list)) {\n\t\t\t \n\t\t\tif (qib_sdma_descq_freecnt(ppd) < ndesc) {\n\t\t\t\tqib_user_sdma_hwqueue_clean(ppd);\n\t\t\t\tif (pq->num_sending)\n\t\t\t\t\tqib_user_sdma_queue_clean(ppd, pq);\n\t\t\t}\n\n\t\t\tret = qib_user_sdma_push_pkts(ppd, pq, &list, mxp);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto done_unlock;\n\t\t\telse {\n\t\t\t\tnpkts += mxp;\n\t\t\t\tpq->counter += mxp;\n\t\t\t}\n\t\t}\n\t}\n\ndone_unlock:\n\tif (!list_empty(&list))\n\t\tqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &list);\n\tmutex_unlock(&pq->lock);\n\n\treturn (ret < 0) ? ret : npkts;\n}\n\nint qib_user_sdma_make_progress(struct qib_pportdata *ppd,\n\t\t\t\tstruct qib_user_sdma_queue *pq)\n{\n\tint ret = 0;\n\n\tmutex_lock(&pq->lock);\n\tqib_user_sdma_hwqueue_clean(ppd);\n\tret = qib_user_sdma_queue_clean(ppd, pq);\n\tmutex_unlock(&pq->lock);\n\n\treturn ret;\n}\n\nu32 qib_user_sdma_complete_counter(const struct qib_user_sdma_queue *pq)\n{\n\treturn pq ? pq->sent_counter : 0;\n}\n\nu32 qib_user_sdma_inflight_counter(struct qib_user_sdma_queue *pq)\n{\n\treturn pq ? pq->counter : 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}