{
  "module_name": "qib_sdma.c",
  "hash_id": "e72751bbb606262e981370139d7b986f2fb911b021140f67b7cf716ff7148227",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qib/qib_sdma.c",
  "human_readable_source": " \n\n#include <linux/spinlock.h>\n#include <linux/netdevice.h>\n#include <linux/moduleparam.h>\n\n#include \"qib.h\"\n#include \"qib_common.h\"\n\n \nstatic ushort sdma_descq_cnt = 256;\nmodule_param_named(sdma_descq_cnt, sdma_descq_cnt, ushort, S_IRUGO);\nMODULE_PARM_DESC(sdma_descq_cnt, \"Number of SDMA descq entries\");\n\n \n#define SDMA_DESC_LAST          (1ULL << 11)\n#define SDMA_DESC_FIRST         (1ULL << 12)\n#define SDMA_DESC_DMA_HEAD      (1ULL << 13)\n#define SDMA_DESC_USE_LARGE_BUF (1ULL << 14)\n#define SDMA_DESC_INTR          (1ULL << 15)\n#define SDMA_DESC_COUNT_LSB     16\n#define SDMA_DESC_GEN_LSB       30\n\n \nstatic int alloc_sdma(struct qib_pportdata *);\nstatic void sdma_complete(struct kref *);\nstatic void sdma_finalput(struct qib_sdma_state *);\nstatic void sdma_get(struct qib_sdma_state *);\nstatic void sdma_put(struct qib_sdma_state *);\nstatic void sdma_set_state(struct qib_pportdata *, enum qib_sdma_states);\nstatic void sdma_start_sw_clean_up(struct qib_pportdata *);\nstatic void sdma_sw_clean_up_task(struct tasklet_struct *);\nstatic void unmap_desc(struct qib_pportdata *, unsigned);\n\nstatic void sdma_get(struct qib_sdma_state *ss)\n{\n\tkref_get(&ss->kref);\n}\n\nstatic void sdma_complete(struct kref *kref)\n{\n\tstruct qib_sdma_state *ss =\n\t\tcontainer_of(kref, struct qib_sdma_state, kref);\n\n\tcomplete(&ss->comp);\n}\n\nstatic void sdma_put(struct qib_sdma_state *ss)\n{\n\tkref_put(&ss->kref, sdma_complete);\n}\n\nstatic void sdma_finalput(struct qib_sdma_state *ss)\n{\n\tsdma_put(ss);\n\twait_for_completion(&ss->comp);\n}\n\n \nstatic void clear_sdma_activelist(struct qib_pportdata *ppd)\n{\n\tstruct qib_sdma_txreq *txp, *txp_next;\n\n\tlist_for_each_entry_safe(txp, txp_next, &ppd->sdma_activelist, list) {\n\t\tlist_del_init(&txp->list);\n\t\tif (txp->flags & QIB_SDMA_TXREQ_F_FREEDESC) {\n\t\t\tunsigned idx;\n\n\t\t\tidx = txp->start_idx;\n\t\t\twhile (idx != txp->next_descq_idx) {\n\t\t\t\tunmap_desc(ppd, idx);\n\t\t\t\tif (++idx == ppd->sdma_descq_cnt)\n\t\t\t\t\tidx = 0;\n\t\t\t}\n\t\t}\n\t\tif (txp->callback)\n\t\t\t(*txp->callback)(txp, QIB_SDMA_TXREQ_S_ABORTED);\n\t}\n}\n\nstatic void sdma_sw_clean_up_task(struct tasklet_struct *t)\n{\n\tstruct qib_pportdata *ppd = from_tasklet(ppd, t,\n\t\t\t\t\t\t sdma_sw_clean_up_task);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\n\t \n\n\t \n\tqib_sdma_make_progress(ppd);\n\n\tclear_sdma_activelist(ppd);\n\n\t \n\tppd->sdma_descq_removed = ppd->sdma_descq_added;\n\n\t \n\tppd->sdma_descq_tail = 0;\n\tppd->sdma_descq_head = 0;\n\tppd->sdma_head_dma[0] = 0;\n\tppd->sdma_generation = 0;\n\n\t__qib_sdma_process_event(ppd, qib_sdma_event_e40_sw_cleaned);\n\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n}\n\n \nstatic void sdma_hw_start_up(struct qib_pportdata *ppd)\n{\n\tstruct qib_sdma_state *ss = &ppd->sdma_state;\n\tunsigned bufno;\n\n\tfor (bufno = ss->first_sendbuf; bufno < ss->last_sendbuf; ++bufno)\n\t\tppd->dd->f_sendctrl(ppd, QIB_SENDCTRL_DISARM_BUF(bufno));\n\n\tppd->dd->f_sdma_hw_start_up(ppd);\n}\n\nstatic void sdma_sw_tear_down(struct qib_pportdata *ppd)\n{\n\tstruct qib_sdma_state *ss = &ppd->sdma_state;\n\n\t \n\tsdma_put(ss);\n}\n\nstatic void sdma_start_sw_clean_up(struct qib_pportdata *ppd)\n{\n\ttasklet_hi_schedule(&ppd->sdma_sw_clean_up_task);\n}\n\nstatic void sdma_set_state(struct qib_pportdata *ppd,\n\tenum qib_sdma_states next_state)\n{\n\tstruct qib_sdma_state *ss = &ppd->sdma_state;\n\tstruct sdma_set_state_action *action = ss->set_state_action;\n\tunsigned op = 0;\n\n\t \n\tss->previous_state = ss->current_state;\n\tss->previous_op = ss->current_op;\n\n\tss->current_state = next_state;\n\n\tif (action[next_state].op_enable)\n\t\top |= QIB_SDMA_SENDCTRL_OP_ENABLE;\n\n\tif (action[next_state].op_intenable)\n\t\top |= QIB_SDMA_SENDCTRL_OP_INTENABLE;\n\n\tif (action[next_state].op_halt)\n\t\top |= QIB_SDMA_SENDCTRL_OP_HALT;\n\n\tif (action[next_state].op_drain)\n\t\top |= QIB_SDMA_SENDCTRL_OP_DRAIN;\n\n\tif (action[next_state].go_s99_running_tofalse)\n\t\tss->go_s99_running = 0;\n\n\tif (action[next_state].go_s99_running_totrue)\n\t\tss->go_s99_running = 1;\n\n\tss->current_op = op;\n\n\tppd->dd->f_sdma_sendctrl(ppd, ss->current_op);\n}\n\nstatic void unmap_desc(struct qib_pportdata *ppd, unsigned head)\n{\n\t__le64 *descqp = &ppd->sdma_descq[head].qw[0];\n\tu64 desc[2];\n\tdma_addr_t addr;\n\tsize_t len;\n\n\tdesc[0] = le64_to_cpu(descqp[0]);\n\tdesc[1] = le64_to_cpu(descqp[1]);\n\n\taddr = (desc[1] << 32) | (desc[0] >> 32);\n\tlen = (desc[0] >> 14) & (0x7ffULL << 2);\n\tdma_unmap_single(&ppd->dd->pcidev->dev, addr, len, DMA_TO_DEVICE);\n}\n\nstatic int alloc_sdma(struct qib_pportdata *ppd)\n{\n\tppd->sdma_descq_cnt = sdma_descq_cnt;\n\tif (!ppd->sdma_descq_cnt)\n\t\tppd->sdma_descq_cnt = 256;\n\n\t \n\tppd->sdma_descq = dma_alloc_coherent(&ppd->dd->pcidev->dev,\n\t\tppd->sdma_descq_cnt * sizeof(u64[2]), &ppd->sdma_descq_phys,\n\t\tGFP_KERNEL);\n\n\tif (!ppd->sdma_descq) {\n\t\tqib_dev_err(ppd->dd,\n\t\t\t\"failed to allocate SendDMA descriptor FIFO memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t \n\tppd->sdma_head_dma = dma_alloc_coherent(&ppd->dd->pcidev->dev,\n\t\tPAGE_SIZE, &ppd->sdma_head_phys, GFP_KERNEL);\n\tif (!ppd->sdma_head_dma) {\n\t\tqib_dev_err(ppd->dd,\n\t\t\t\"failed to allocate SendDMA head memory\\n\");\n\t\tgoto cleanup_descq;\n\t}\n\tppd->sdma_head_dma[0] = 0;\n\treturn 0;\n\ncleanup_descq:\n\tdma_free_coherent(&ppd->dd->pcidev->dev,\n\t\tppd->sdma_descq_cnt * sizeof(u64[2]), (void *)ppd->sdma_descq,\n\t\tppd->sdma_descq_phys);\n\tppd->sdma_descq = NULL;\n\tppd->sdma_descq_phys = 0;\nbail:\n\tppd->sdma_descq_cnt = 0;\n\treturn -ENOMEM;\n}\n\nstatic void free_sdma(struct qib_pportdata *ppd)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\n\tif (ppd->sdma_head_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\n\t\t\t\t  (void *)ppd->sdma_head_dma,\n\t\t\t\t  ppd->sdma_head_phys);\n\t\tppd->sdma_head_dma = NULL;\n\t\tppd->sdma_head_phys = 0;\n\t}\n\n\tif (ppd->sdma_descq) {\n\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t  ppd->sdma_descq_cnt * sizeof(u64[2]),\n\t\t\t\t  ppd->sdma_descq, ppd->sdma_descq_phys);\n\t\tppd->sdma_descq = NULL;\n\t\tppd->sdma_descq_phys = 0;\n\t}\n}\n\nstatic inline void make_sdma_desc(struct qib_pportdata *ppd,\n\t\t\t\t  u64 *sdmadesc, u64 addr, u64 dwlen,\n\t\t\t\t  u64 dwoffset)\n{\n\n\tWARN_ON(addr & 3);\n\t \n\tsdmadesc[1] = addr >> 32;\n\t \n\tsdmadesc[0] = (addr & 0xfffffffcULL) << 32;\n\t \n\tsdmadesc[0] |= (ppd->sdma_generation & 3ULL) <<\n\t\tSDMA_DESC_GEN_LSB;\n\t \n\tsdmadesc[0] |= (dwlen & 0x7ffULL) << SDMA_DESC_COUNT_LSB;\n\t \n\tsdmadesc[0] |= dwoffset & 0x7ffULL;\n}\n\n \nint qib_sdma_make_progress(struct qib_pportdata *ppd)\n{\n\tstruct list_head *lp = NULL;\n\tstruct qib_sdma_txreq *txp = NULL;\n\tstruct qib_devdata *dd = ppd->dd;\n\tint progress = 0;\n\tu16 hwhead;\n\tu16 idx = 0;\n\n\thwhead = dd->f_sdma_gethead(ppd);\n\n\t \n\n\tif (!list_empty(&ppd->sdma_activelist)) {\n\t\tlp = ppd->sdma_activelist.next;\n\t\ttxp = list_entry(lp, struct qib_sdma_txreq, list);\n\t\tidx = txp->start_idx;\n\t}\n\n\twhile (ppd->sdma_descq_head != hwhead) {\n\t\t \n\t\tif (txp && (txp->flags & QIB_SDMA_TXREQ_F_FREEDESC) &&\n\t\t    (idx == ppd->sdma_descq_head)) {\n\t\t\tunmap_desc(ppd, ppd->sdma_descq_head);\n\t\t\tif (++idx == ppd->sdma_descq_cnt)\n\t\t\t\tidx = 0;\n\t\t}\n\n\t\t \n\t\tppd->sdma_descq_removed++;\n\n\t\t \n\t\tif (++ppd->sdma_descq_head == ppd->sdma_descq_cnt)\n\t\t\tppd->sdma_descq_head = 0;\n\n\t\t \n\t\tif (txp && txp->next_descq_idx == ppd->sdma_descq_head) {\n\t\t\t \n\t\t\tlist_del_init(&txp->list);\n\t\t\tif (txp->callback)\n\t\t\t\t(*txp->callback)(txp, QIB_SDMA_TXREQ_S_OK);\n\t\t\t \n\t\t\tif (list_empty(&ppd->sdma_activelist))\n\t\t\t\ttxp = NULL;\n\t\t\telse {\n\t\t\t\tlp = ppd->sdma_activelist.next;\n\t\t\t\ttxp = list_entry(lp, struct qib_sdma_txreq,\n\t\t\t\t\tlist);\n\t\t\t\tidx = txp->start_idx;\n\t\t\t}\n\t\t}\n\t\tprogress = 1;\n\t}\n\tif (progress)\n\t\tqib_verbs_sdma_desc_avail(ppd, qib_sdma_descq_freecnt(ppd));\n\treturn progress;\n}\n\n \nvoid qib_sdma_intr(struct qib_pportdata *ppd)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\n\t__qib_sdma_intr(ppd);\n\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n}\n\nvoid __qib_sdma_intr(struct qib_pportdata *ppd)\n{\n\tif (__qib_sdma_running(ppd)) {\n\t\tqib_sdma_make_progress(ppd);\n\t\tif (!list_empty(&ppd->sdma_userpending))\n\t\t\tqib_user_sdma_send_desc(ppd, &ppd->sdma_userpending);\n\t}\n}\n\nint qib_setup_sdma(struct qib_pportdata *ppd)\n{\n\tstruct qib_devdata *dd = ppd->dd;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tret = alloc_sdma(ppd);\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tppd->dd->f_sdma_init_early(ppd);\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\n\t \n\tkref_init(&ppd->sdma_state.kref);\n\tinit_completion(&ppd->sdma_state.comp);\n\n\tppd->sdma_generation = 0;\n\tppd->sdma_descq_head = 0;\n\tppd->sdma_descq_removed = 0;\n\tppd->sdma_descq_added = 0;\n\n\tppd->sdma_intrequest = 0;\n\tINIT_LIST_HEAD(&ppd->sdma_userpending);\n\n\tINIT_LIST_HEAD(&ppd->sdma_activelist);\n\n\ttasklet_setup(&ppd->sdma_sw_clean_up_task, sdma_sw_clean_up_task);\n\n\tret = dd->f_init_sdma_regs(ppd);\n\tif (ret)\n\t\tgoto bail_alloc;\n\n\tqib_sdma_process_event(ppd, qib_sdma_event_e10_go_hw_start);\n\n\treturn 0;\n\nbail_alloc:\n\tqib_teardown_sdma(ppd);\nbail:\n\treturn ret;\n}\n\nvoid qib_teardown_sdma(struct qib_pportdata *ppd)\n{\n\tqib_sdma_process_event(ppd, qib_sdma_event_e00_go_hw_down);\n\n\t \n\tsdma_finalput(&ppd->sdma_state);\n\n\tfree_sdma(ppd);\n}\n\nint qib_sdma_running(struct qib_pportdata *ppd)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\tret = __qib_sdma_running(ppd);\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic void complete_sdma_err_req(struct qib_pportdata *ppd,\n\t\t\t\t  struct qib_verbs_txreq *tx)\n{\n\tstruct qib_qp_priv *priv = tx->qp->priv;\n\n\tatomic_inc(&priv->s_dma_busy);\n\t \n\ttx->txreq.start_idx = 0;\n\ttx->txreq.next_descq_idx = 0;\n\tlist_add_tail(&tx->txreq.list, &ppd->sdma_activelist);\n\tclear_sdma_activelist(ppd);\n}\n\n \nint qib_sdma_verbs_send(struct qib_pportdata *ppd,\n\t\t\tstruct rvt_sge_state *ss, u32 dwords,\n\t\t\tstruct qib_verbs_txreq *tx)\n{\n\tunsigned long flags;\n\tstruct rvt_sge *sge;\n\tstruct rvt_qp *qp;\n\tint ret = 0;\n\tu16 tail;\n\t__le64 *descqp;\n\tu64 sdmadesc[2];\n\tu32 dwoffset;\n\tdma_addr_t addr;\n\tstruct qib_qp_priv *priv;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\nretry:\n\tif (unlikely(!__qib_sdma_running(ppd))) {\n\t\tcomplete_sdma_err_req(ppd, tx);\n\t\tgoto unlock;\n\t}\n\n\tif (tx->txreq.sg_count > qib_sdma_descq_freecnt(ppd)) {\n\t\tif (qib_sdma_make_progress(ppd))\n\t\t\tgoto retry;\n\t\tif (ppd->dd->flags & QIB_HAS_SDMA_TIMEOUT)\n\t\t\tppd->dd->f_sdma_set_desc_cnt(ppd,\n\t\t\t\t\tppd->sdma_descq_cnt / 2);\n\t\tgoto busy;\n\t}\n\n\tdwoffset = tx->hdr_dwords;\n\tmake_sdma_desc(ppd, sdmadesc, (u64) tx->txreq.addr, dwoffset, 0);\n\n\tsdmadesc[0] |= SDMA_DESC_FIRST;\n\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_USELARGEBUF)\n\t\tsdmadesc[0] |= SDMA_DESC_USE_LARGE_BUF;\n\n\t \n\ttail = ppd->sdma_descq_tail;\n\tdescqp = &ppd->sdma_descq[tail].qw[0];\n\t*descqp++ = cpu_to_le64(sdmadesc[0]);\n\t*descqp++ = cpu_to_le64(sdmadesc[1]);\n\n\t \n\tif (++tail == ppd->sdma_descq_cnt) {\n\t\ttail = 0;\n\t\tdescqp = &ppd->sdma_descq[0].qw[0];\n\t\t++ppd->sdma_generation;\n\t}\n\n\ttx->txreq.start_idx = tail;\n\n\tsge = &ss->sge;\n\twhile (dwords) {\n\t\tu32 dw;\n\t\tu32 len = rvt_get_sge_length(sge, dwords << 2);\n\n\t\tdw = (len + 3) >> 2;\n\t\taddr = dma_map_single(&ppd->dd->pcidev->dev, sge->vaddr,\n\t\t\t\t      dw << 2, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&ppd->dd->pcidev->dev, addr)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unmap;\n\t\t}\n\t\tsdmadesc[0] = 0;\n\t\tmake_sdma_desc(ppd, sdmadesc, (u64) addr, dw, dwoffset);\n\t\t \n\t\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_USELARGEBUF)\n\t\t\tsdmadesc[0] |= SDMA_DESC_USE_LARGE_BUF;\n\t\t \n\t\t*descqp++ = cpu_to_le64(sdmadesc[0]);\n\t\t*descqp++ = cpu_to_le64(sdmadesc[1]);\n\n\t\t \n\t\tif (++tail == ppd->sdma_descq_cnt) {\n\t\t\ttail = 0;\n\t\t\tdescqp = &ppd->sdma_descq[0].qw[0];\n\t\t\t++ppd->sdma_generation;\n\t\t}\n\t\trvt_update_sge(ss, len, false);\n\t\tdwoffset += dw;\n\t\tdwords -= dw;\n\t}\n\n\tif (!tail)\n\t\tdescqp = &ppd->sdma_descq[ppd->sdma_descq_cnt].qw[0];\n\tdescqp -= 2;\n\tdescqp[0] |= cpu_to_le64(SDMA_DESC_LAST);\n\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_HEADTOHOST)\n\t\tdescqp[0] |= cpu_to_le64(SDMA_DESC_DMA_HEAD);\n\tif (tx->txreq.flags & QIB_SDMA_TXREQ_F_INTREQ)\n\t\tdescqp[0] |= cpu_to_le64(SDMA_DESC_INTR);\n\tpriv = tx->qp->priv;\n\tatomic_inc(&priv->s_dma_busy);\n\ttx->txreq.next_descq_idx = tail;\n\tppd->dd->f_sdma_update_tail(ppd, tail);\n\tppd->sdma_descq_added += tx->txreq.sg_count;\n\tlist_add_tail(&tx->txreq.list, &ppd->sdma_activelist);\n\tgoto unlock;\n\nunmap:\n\tfor (;;) {\n\t\tif (!tail)\n\t\t\ttail = ppd->sdma_descq_cnt - 1;\n\t\telse\n\t\t\ttail--;\n\t\tif (tail == ppd->sdma_descq_tail)\n\t\t\tbreak;\n\t\tunmap_desc(ppd, tail);\n\t}\n\tqp = tx->qp;\n\tpriv = qp->priv;\n\tqib_put_txreq(tx);\n\tspin_lock(&qp->r_lock);\n\tspin_lock(&qp->s_lock);\n\tif (qp->ibqp.qp_type == IB_QPT_RC) {\n\t\t \n\t\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)\n\t\t\trvt_error_qp(qp, IB_WC_GENERAL_ERR);\n\t} else if (qp->s_wqe)\n\t\trvt_send_complete(qp, qp->s_wqe, IB_WC_GENERAL_ERR);\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->r_lock);\n\t \n\tgoto unlock;\n\nbusy:\n\tqp = tx->qp;\n\tpriv = qp->priv;\n\tspin_lock(&qp->s_lock);\n\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\n\t\tstruct qib_ibdev *dev;\n\n\t\t \n\t\ttx->ss = ss;\n\t\ttx->dwords = dwords;\n\t\tpriv->s_tx = tx;\n\t\tdev = &ppd->dd->verbs_dev;\n\t\tspin_lock(&dev->rdi.pending_lock);\n\t\tif (list_empty(&priv->iowait)) {\n\t\t\tstruct qib_ibport *ibp;\n\n\t\t\tibp = &ppd->ibport_data;\n\t\t\tibp->rvp.n_dmawait++;\n\t\t\tqp->s_flags |= RVT_S_WAIT_DMA_DESC;\n\t\t\tlist_add_tail(&priv->iowait, &dev->dmawait);\n\t\t}\n\t\tspin_unlock(&dev->rdi.pending_lock);\n\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\tspin_unlock(&qp->s_lock);\n\t\tret = -EBUSY;\n\t} else {\n\t\tspin_unlock(&qp->s_lock);\n\t\tqib_put_txreq(tx);\n\t}\nunlock:\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n\treturn ret;\n}\n\n \nvoid dump_sdma_state(struct qib_pportdata *ppd)\n{\n\tstruct qib_sdma_desc *descq;\n\tstruct qib_sdma_txreq *txp, *txpnext;\n\t__le64 *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu16 gen, dwlen, dwoffset;\n\tu16 head, tail, cnt;\n\n\thead = ppd->sdma_descq_head;\n\ttail = ppd->sdma_descq_tail;\n\tcnt = qib_sdma_descq_freecnt(ppd);\n\tdescq = ppd->sdma_descq;\n\n\tqib_dev_porterr(ppd->dd, ppd->port,\n\t\t\"SDMA ppd->sdma_descq_head: %u\\n\", head);\n\tqib_dev_porterr(ppd->dd, ppd->port,\n\t\t\"SDMA ppd->sdma_descq_tail: %u\\n\", tail);\n\tqib_dev_porterr(ppd->dd, ppd->port,\n\t\t\"SDMA sdma_descq_freecnt: %u\\n\", cnt);\n\n\t \n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &descq[head].qw[0];\n\t\tdesc[0] = le64_to_cpu(descqp[0]);\n\t\tdesc[1] = le64_to_cpu(descqp[1]);\n\t\tflags[0] = (desc[0] & 1<<15) ? 'I' : '-';\n\t\tflags[1] = (desc[0] & 1<<14) ? 'L' : 'S';\n\t\tflags[2] = (desc[0] & 1<<13) ? 'H' : '-';\n\t\tflags[3] = (desc[0] & 1<<12) ? 'F' : '-';\n\t\tflags[4] = (desc[0] & 1<<11) ? 'L' : '-';\n\t\taddr = (desc[1] << 32) | ((desc[0] >> 32) & 0xfffffffcULL);\n\t\tgen = (desc[0] >> 30) & 3ULL;\n\t\tdwlen = (desc[0] >> 14) & (0x7ffULL << 2);\n\t\tdwoffset = (desc[0] & 0x7ffULL) << 2;\n\t\tqib_dev_porterr(ppd->dd, ppd->port,\n\t\t\t\"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes offset:%u bytes\\n\",\n\t\t\t head, flags, addr, gen, dwlen, dwoffset);\n\t\tif (++head == ppd->sdma_descq_cnt)\n\t\t\thead = 0;\n\t}\n\n\t \n\tlist_for_each_entry_safe(txp, txpnext, &ppd->sdma_activelist,\n\t\t\t\t list)\n\t\tqib_dev_porterr(ppd->dd, ppd->port,\n\t\t\t\"SDMA txp->start_idx: %u txp->next_descq_idx: %u\\n\",\n\t\t\ttxp->start_idx, txp->next_descq_idx);\n}\n\nvoid qib_sdma_process_event(struct qib_pportdata *ppd,\n\tenum qib_sdma_events event)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ppd->sdma_lock, flags);\n\n\t__qib_sdma_process_event(ppd, event);\n\n\tif (ppd->sdma_state.current_state == qib_sdma_state_s99_running)\n\t\tqib_verbs_sdma_desc_avail(ppd, qib_sdma_descq_freecnt(ppd));\n\n\tspin_unlock_irqrestore(&ppd->sdma_lock, flags);\n}\n\nvoid __qib_sdma_process_event(struct qib_pportdata *ppd,\n\tenum qib_sdma_events event)\n{\n\tstruct qib_sdma_state *ss = &ppd->sdma_state;\n\n\tswitch (ss->current_state) {\n\tcase qib_sdma_state_s00_hw_down:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\t \n\t\t\tss->go_s99_running = 1;\n\t\t\tfallthrough;\t \n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\t \n\t\t\tsdma_get(&ppd->sdma_state);\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s10_hw_start_up_wait);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tsdma_sw_tear_down(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s10_hw_start_up_wait:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tsdma_set_state(ppd, ss->go_s99_running ?\n\t\t\t\t       qib_sdma_state_s99_running :\n\t\t\t\t       qib_sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s20_idle:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s99_running);\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s30_sw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s10_hw_start_up_wait);\n\t\t\tsdma_hw_start_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s40_hw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s30_sw_clean_up_wait);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s50_hw_halt_wait:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s40_hw_clean_up_wait);\n\t\t\tppd->dd->f_sdma_hw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase qib_sdma_state_s99_running:\n\t\tswitch (event) {\n\t\tcase qib_sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e20_hw_started:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e30_go_running:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e60_hw_halted:\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s30_sw_clean_up_wait);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e70_go_idle:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s50_hw_halt_wait);\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7220_err_halted:\n\t\t\tsdma_set_state(ppd,\n\t\t\t\t       qib_sdma_state_s30_sw_clean_up_wait);\n\t\t\tsdma_start_sw_clean_up(ppd);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e7322_err_halted:\n\t\t\tsdma_set_state(ppd, qib_sdma_state_s50_hw_halt_wait);\n\t\t\tbreak;\n\t\tcase qib_sdma_event_e90_timer_tick:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tss->last_event = event;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}