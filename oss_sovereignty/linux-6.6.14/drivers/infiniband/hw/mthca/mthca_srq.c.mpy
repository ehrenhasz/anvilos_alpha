{
  "module_name": "mthca_srq.c",
  "hash_id": "a8ba85eca24fa1b6ab9d63147d16f0598e4ce1b3f8c5495b3f1bbc767c69a931",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mthca/mthca_srq.c",
  "human_readable_source": " \n\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/sched.h>\n\n#include <asm/io.h>\n\n#include <rdma/uverbs_ioctl.h>\n\n#include \"mthca_dev.h\"\n#include \"mthca_cmd.h\"\n#include \"mthca_memfree.h\"\n#include \"mthca_wqe.h\"\n\nenum {\n\tMTHCA_MAX_DIRECT_SRQ_SIZE = 4 * PAGE_SIZE\n};\n\nstruct mthca_tavor_srq_context {\n\t__be64 wqe_base_ds;\t \n\t__be32 state_pd;\n\t__be32 lkey;\n\t__be32 uar;\n\t__be16 limit_watermark;\n\t__be16 wqe_cnt;\n\tu32    reserved[2];\n};\n\nstruct mthca_arbel_srq_context {\n\t__be32 state_logsize_srqn;\n\t__be32 lkey;\n\t__be32 db_index;\n\t__be32 logstride_usrpage;\n\t__be64 wqe_base;\n\t__be32 eq_pd;\n\t__be16 limit_watermark;\n\t__be16 wqe_cnt;\n\tu16    reserved1;\n\t__be16 wqe_counter;\n\tu32    reserved2[3];\n};\n\nstatic void *get_wqe(struct mthca_srq *srq, int n)\n{\n\tif (srq->is_direct)\n\t\treturn srq->queue.direct.buf + (n << srq->wqe_shift);\n\telse\n\t\treturn srq->queue.page_list[(n << srq->wqe_shift) >> PAGE_SHIFT].buf +\n\t\t\t((n << srq->wqe_shift) & (PAGE_SIZE - 1));\n}\n\n \nstatic inline int *wqe_to_link(void *wqe)\n{\n\treturn (int *) (wqe + offsetof(struct mthca_next_seg, imm));\n}\n\nstatic void mthca_tavor_init_srq_context(struct mthca_dev *dev,\n\t\t\t\t\t struct mthca_pd *pd,\n\t\t\t\t\t struct mthca_srq *srq,\n\t\t\t\t\t struct mthca_tavor_srq_context *context,\n\t\t\t\t\t struct ib_udata *udata)\n{\n\tstruct mthca_ucontext *ucontext = rdma_udata_to_drv_context(\n\t\tudata, struct mthca_ucontext, ibucontext);\n\n\tmemset(context, 0, sizeof *context);\n\n\tcontext->wqe_base_ds = cpu_to_be64(1 << (srq->wqe_shift - 4));\n\tcontext->state_pd    = cpu_to_be32(pd->pd_num);\n\tcontext->lkey        = cpu_to_be32(srq->mr.ibmr.lkey);\n\n\tif (udata)\n\t\tcontext->uar = cpu_to_be32(ucontext->uar.index);\n\telse\n\t\tcontext->uar = cpu_to_be32(dev->driver_uar.index);\n}\n\nstatic void mthca_arbel_init_srq_context(struct mthca_dev *dev,\n\t\t\t\t\t struct mthca_pd *pd,\n\t\t\t\t\t struct mthca_srq *srq,\n\t\t\t\t\t struct mthca_arbel_srq_context *context,\n\t\t\t\t\t struct ib_udata *udata)\n{\n\tstruct mthca_ucontext *ucontext = rdma_udata_to_drv_context(\n\t\tudata, struct mthca_ucontext, ibucontext);\n\tint logsize, max;\n\n\tmemset(context, 0, sizeof *context);\n\n\t \n\tmax = srq->max;\n\tlogsize = ilog2(max);\n\tcontext->state_logsize_srqn = cpu_to_be32(logsize << 24 | srq->srqn);\n\tcontext->lkey = cpu_to_be32(srq->mr.ibmr.lkey);\n\tcontext->db_index = cpu_to_be32(srq->db_index);\n\tcontext->logstride_usrpage = cpu_to_be32((srq->wqe_shift - 4) << 29);\n\tif (udata)\n\t\tcontext->logstride_usrpage |= cpu_to_be32(ucontext->uar.index);\n\telse\n\t\tcontext->logstride_usrpage |= cpu_to_be32(dev->driver_uar.index);\n\tcontext->eq_pd = cpu_to_be32(MTHCA_EQ_ASYNC << 24 | pd->pd_num);\n}\n\nstatic void mthca_free_srq_buf(struct mthca_dev *dev, struct mthca_srq *srq)\n{\n\tmthca_buf_free(dev, srq->max << srq->wqe_shift, &srq->queue,\n\t\t       srq->is_direct, &srq->mr);\n\tkfree(srq->wrid);\n}\n\nstatic int mthca_alloc_srq_buf(struct mthca_dev *dev, struct mthca_pd *pd,\n\t\t\t       struct mthca_srq *srq, struct ib_udata *udata)\n{\n\tstruct mthca_data_seg *scatter;\n\tvoid *wqe;\n\tint err;\n\tint i;\n\n\tif (udata)\n\t\treturn 0;\n\n\tsrq->wrid = kmalloc_array(srq->max, sizeof(u64), GFP_KERNEL);\n\tif (!srq->wrid)\n\t\treturn -ENOMEM;\n\n\terr = mthca_buf_alloc(dev, srq->max << srq->wqe_shift,\n\t\t\t      MTHCA_MAX_DIRECT_SRQ_SIZE,\n\t\t\t      &srq->queue, &srq->is_direct, pd, 1, &srq->mr);\n\tif (err) {\n\t\tkfree(srq->wrid);\n\t\treturn err;\n\t}\n\n\t \n\tfor (i = 0; i < srq->max; ++i) {\n\t\tstruct mthca_next_seg *next;\n\n\t\tnext = wqe = get_wqe(srq, i);\n\n\t\tif (i < srq->max - 1) {\n\t\t\t*wqe_to_link(wqe) = i + 1;\n\t\t\tnext->nda_op = htonl(((i + 1) << srq->wqe_shift) | 1);\n\t\t} else {\n\t\t\t*wqe_to_link(wqe) = -1;\n\t\t\tnext->nda_op = 0;\n\t\t}\n\n\t\tfor (scatter = wqe + sizeof (struct mthca_next_seg);\n\t\t     (void *) scatter < wqe + (1 << srq->wqe_shift);\n\t\t     ++scatter)\n\t\t\tscatter->lkey = cpu_to_be32(MTHCA_INVAL_LKEY);\n\t}\n\n\tsrq->last = get_wqe(srq, srq->max - 1);\n\n\treturn 0;\n}\n\nint mthca_alloc_srq(struct mthca_dev *dev, struct mthca_pd *pd,\n\t\t    struct ib_srq_attr *attr, struct mthca_srq *srq,\n\t\t    struct ib_udata *udata)\n{\n\tstruct mthca_mailbox *mailbox;\n\tint ds;\n\tint err;\n\n\t \n\tif (attr->max_wr  > dev->limits.max_srq_wqes ||\n\t    attr->max_sge > dev->limits.max_srq_sge)\n\t\treturn -EINVAL;\n\n\tsrq->max      = attr->max_wr;\n\tsrq->max_gs   = attr->max_sge;\n\tsrq->counter  = 0;\n\n\tif (mthca_is_memfree(dev))\n\t\tsrq->max = roundup_pow_of_two(srq->max + 1);\n\telse\n\t\tsrq->max = srq->max + 1;\n\n\tds = max(64UL,\n\t\t roundup_pow_of_two(sizeof (struct mthca_next_seg) +\n\t\t\t\t    srq->max_gs * sizeof (struct mthca_data_seg)));\n\n\tif (!mthca_is_memfree(dev) && (ds > dev->limits.max_desc_sz))\n\t\treturn -EINVAL;\n\n\tsrq->wqe_shift = ilog2(ds);\n\n\tsrq->srqn = mthca_alloc(&dev->srq_table.alloc);\n\tif (srq->srqn == -1)\n\t\treturn -ENOMEM;\n\n\tif (mthca_is_memfree(dev)) {\n\t\terr = mthca_table_get(dev, dev->srq_table.table, srq->srqn);\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\tif (!udata) {\n\t\t\tsrq->db_index = mthca_alloc_db(dev, MTHCA_DB_TYPE_SRQ,\n\t\t\t\t\t\t       srq->srqn, &srq->db);\n\t\t\tif (srq->db_index < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_out_icm;\n\t\t\t}\n\t\t}\n\t}\n\n\tmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\n\tif (IS_ERR(mailbox)) {\n\t\terr = PTR_ERR(mailbox);\n\t\tgoto err_out_db;\n\t}\n\n\terr = mthca_alloc_srq_buf(dev, pd, srq, udata);\n\tif (err)\n\t\tgoto err_out_mailbox;\n\n\tspin_lock_init(&srq->lock);\n\tsrq->refcount = 1;\n\tinit_waitqueue_head(&srq->wait);\n\tmutex_init(&srq->mutex);\n\n\tif (mthca_is_memfree(dev))\n\t\tmthca_arbel_init_srq_context(dev, pd, srq, mailbox->buf, udata);\n\telse\n\t\tmthca_tavor_init_srq_context(dev, pd, srq, mailbox->buf, udata);\n\n\terr = mthca_SW2HW_SRQ(dev, mailbox, srq->srqn);\n\n\tif (err) {\n\t\tmthca_warn(dev, \"SW2HW_SRQ failed (%d)\\n\", err);\n\t\tgoto err_out_free_buf;\n\t}\n\n\tspin_lock_irq(&dev->srq_table.lock);\n\tif (mthca_array_set(&dev->srq_table.srq,\n\t\t\t    srq->srqn & (dev->limits.num_srqs - 1),\n\t\t\t    srq)) {\n\t\tspin_unlock_irq(&dev->srq_table.lock);\n\t\tgoto err_out_free_srq;\n\t}\n\tspin_unlock_irq(&dev->srq_table.lock);\n\n\tmthca_free_mailbox(dev, mailbox);\n\n\tsrq->first_free = 0;\n\tsrq->last_free  = srq->max - 1;\n\n\tattr->max_wr    = srq->max - 1;\n\tattr->max_sge   = srq->max_gs;\n\n\treturn 0;\n\nerr_out_free_srq:\n\terr = mthca_HW2SW_SRQ(dev, mailbox, srq->srqn);\n\tif (err)\n\t\tmthca_warn(dev, \"HW2SW_SRQ failed (%d)\\n\", err);\n\nerr_out_free_buf:\n\tif (!udata)\n\t\tmthca_free_srq_buf(dev, srq);\n\nerr_out_mailbox:\n\tmthca_free_mailbox(dev, mailbox);\n\nerr_out_db:\n\tif (!udata && mthca_is_memfree(dev))\n\t\tmthca_free_db(dev, MTHCA_DB_TYPE_SRQ, srq->db_index);\n\nerr_out_icm:\n\tmthca_table_put(dev, dev->srq_table.table, srq->srqn);\n\nerr_out:\n\tmthca_free(&dev->srq_table.alloc, srq->srqn);\n\n\treturn err;\n}\n\nstatic inline int get_srq_refcount(struct mthca_dev *dev, struct mthca_srq *srq)\n{\n\tint c;\n\n\tspin_lock_irq(&dev->srq_table.lock);\n\tc = srq->refcount;\n\tspin_unlock_irq(&dev->srq_table.lock);\n\n\treturn c;\n}\n\nvoid mthca_free_srq(struct mthca_dev *dev, struct mthca_srq *srq)\n{\n\tstruct mthca_mailbox *mailbox;\n\tint err;\n\n\tmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\n\tif (IS_ERR(mailbox)) {\n\t\tmthca_warn(dev, \"No memory for mailbox to free SRQ.\\n\");\n\t\treturn;\n\t}\n\n\terr = mthca_HW2SW_SRQ(dev, mailbox, srq->srqn);\n\tif (err)\n\t\tmthca_warn(dev, \"HW2SW_SRQ failed (%d)\\n\", err);\n\n\tspin_lock_irq(&dev->srq_table.lock);\n\tmthca_array_clear(&dev->srq_table.srq,\n\t\t\t  srq->srqn & (dev->limits.num_srqs - 1));\n\t--srq->refcount;\n\tspin_unlock_irq(&dev->srq_table.lock);\n\n\twait_event(srq->wait, !get_srq_refcount(dev, srq));\n\n\tif (!srq->ibsrq.uobject) {\n\t\tmthca_free_srq_buf(dev, srq);\n\t\tif (mthca_is_memfree(dev))\n\t\t\tmthca_free_db(dev, MTHCA_DB_TYPE_SRQ, srq->db_index);\n\t}\n\n\tmthca_table_put(dev, dev->srq_table.table, srq->srqn);\n\tmthca_free(&dev->srq_table.alloc, srq->srqn);\n\tmthca_free_mailbox(dev, mailbox);\n}\n\nint mthca_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,\n\t\t     enum ib_srq_attr_mask attr_mask, struct ib_udata *udata)\n{\n\tstruct mthca_dev *dev = to_mdev(ibsrq->device);\n\tstruct mthca_srq *srq = to_msrq(ibsrq);\n\tint ret = 0;\n\n\t \n\tif (attr_mask & IB_SRQ_MAX_WR)\n\t\treturn -EINVAL;\n\n\tif (attr_mask & IB_SRQ_LIMIT) {\n\t\tu32 max_wr = mthca_is_memfree(dev) ? srq->max - 1 : srq->max;\n\t\tif (attr->srq_limit > max_wr)\n\t\t\treturn -EINVAL;\n\n\t\tmutex_lock(&srq->mutex);\n\t\tret = mthca_ARM_SRQ(dev, srq->srqn, attr->srq_limit);\n\t\tmutex_unlock(&srq->mutex);\n\t}\n\n\treturn ret;\n}\n\nint mthca_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\n{\n\tstruct mthca_dev *dev = to_mdev(ibsrq->device);\n\tstruct mthca_srq *srq = to_msrq(ibsrq);\n\tstruct mthca_mailbox *mailbox;\n\tstruct mthca_arbel_srq_context *arbel_ctx;\n\tstruct mthca_tavor_srq_context *tavor_ctx;\n\tint err;\n\n\tmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\terr = mthca_QUERY_SRQ(dev, srq->srqn, mailbox);\n\tif (err)\n\t\tgoto out;\n\n\tif (mthca_is_memfree(dev)) {\n\t\tarbel_ctx = mailbox->buf;\n\t\tsrq_attr->srq_limit = be16_to_cpu(arbel_ctx->limit_watermark);\n\t} else {\n\t\ttavor_ctx = mailbox->buf;\n\t\tsrq_attr->srq_limit = be16_to_cpu(tavor_ctx->limit_watermark);\n\t}\n\n\tsrq_attr->max_wr  = srq->max - 1;\n\tsrq_attr->max_sge = srq->max_gs;\n\nout:\n\tmthca_free_mailbox(dev, mailbox);\n\n\treturn err;\n}\n\nvoid mthca_srq_event(struct mthca_dev *dev, u32 srqn,\n\t\t     enum ib_event_type event_type)\n{\n\tstruct mthca_srq *srq;\n\tstruct ib_event event;\n\n\tspin_lock(&dev->srq_table.lock);\n\tsrq = mthca_array_get(&dev->srq_table.srq, srqn & (dev->limits.num_srqs - 1));\n\tif (srq)\n\t\t++srq->refcount;\n\tspin_unlock(&dev->srq_table.lock);\n\n\tif (!srq) {\n\t\tmthca_warn(dev, \"Async event for bogus SRQ %08x\\n\", srqn);\n\t\treturn;\n\t}\n\n\tif (!srq->ibsrq.event_handler)\n\t\tgoto out;\n\n\tevent.device      = &dev->ib_dev;\n\tevent.event       = event_type;\n\tevent.element.srq = &srq->ibsrq;\n\tsrq->ibsrq.event_handler(&event, srq->ibsrq.srq_context);\n\nout:\n\tspin_lock(&dev->srq_table.lock);\n\tif (!--srq->refcount)\n\t\twake_up(&srq->wait);\n\tspin_unlock(&dev->srq_table.lock);\n}\n\n \nvoid mthca_free_srq_wqe(struct mthca_srq *srq, u32 wqe_addr)\n{\n\tint ind;\n\tstruct mthca_next_seg *last_free;\n\n\tind = wqe_addr >> srq->wqe_shift;\n\n\tspin_lock(&srq->lock);\n\n\tlast_free = get_wqe(srq, srq->last_free);\n\t*wqe_to_link(last_free) = ind;\n\tlast_free->nda_op = htonl((ind << srq->wqe_shift) | 1);\n\t*wqe_to_link(get_wqe(srq, ind)) = -1;\n\tsrq->last_free = ind;\n\n\tspin_unlock(&srq->lock);\n}\n\nint mthca_tavor_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct mthca_dev *dev = to_mdev(ibsrq->device);\n\tstruct mthca_srq *srq = to_msrq(ibsrq);\n\tunsigned long flags;\n\tint err = 0;\n\tint first_ind;\n\tint ind;\n\tint next_ind;\n\tint nreq;\n\tint i;\n\tvoid *wqe;\n\tvoid *prev_wqe;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\tfirst_ind = srq->first_free;\n\n\tfor (nreq = 0; wr; wr = wr->next) {\n\t\tind       = srq->first_free;\n\t\twqe       = get_wqe(srq, ind);\n\t\tnext_ind  = *wqe_to_link(wqe);\n\n\t\tif (unlikely(next_ind < 0)) {\n\t\t\tmthca_err(dev, \"SRQ %06x full\\n\", srq->srqn);\n\t\t\terr = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tprev_wqe  = srq->last;\n\t\tsrq->last = wqe;\n\n\t\t((struct mthca_next_seg *) wqe)->ee_nds = 0;\n\t\t \n\n\t\twqe += sizeof (struct mthca_next_seg);\n\n\t\tif (unlikely(wr->num_sge > srq->max_gs)) {\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tsrq->last = prev_wqe;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < wr->num_sge; ++i) {\n\t\t\tmthca_set_data_seg(wqe, wr->sg_list + i);\n\t\t\twqe += sizeof (struct mthca_data_seg);\n\t\t}\n\n\t\tif (i < srq->max_gs)\n\t\t\tmthca_set_data_seg_inval(wqe);\n\n\t\t((struct mthca_next_seg *) prev_wqe)->ee_nds =\n\t\t\tcpu_to_be32(MTHCA_NEXT_DBD);\n\n\t\tsrq->wrid[ind]  = wr->wr_id;\n\t\tsrq->first_free = next_ind;\n\n\t\t++nreq;\n\t\tif (unlikely(nreq == MTHCA_TAVOR_MAX_WQES_PER_RECV_DB)) {\n\t\t\tnreq = 0;\n\n\t\t\t \n\t\t\twmb();\n\n\t\t\tmthca_write64(first_ind << srq->wqe_shift, srq->srqn << 8,\n\t\t\t\t      dev->kar + MTHCA_RECEIVE_DOORBELL,\n\t\t\t\t      MTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\n\n\t\t\tfirst_ind = srq->first_free;\n\t\t}\n\t}\n\n\tif (likely(nreq)) {\n\t\t \n\t\twmb();\n\n\t\tmthca_write64(first_ind << srq->wqe_shift, (srq->srqn << 8) | nreq,\n\t\t\t      dev->kar + MTHCA_RECEIVE_DOORBELL,\n\t\t\t      MTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\n\t}\n\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\treturn err;\n}\n\nint mthca_arbel_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct mthca_dev *dev = to_mdev(ibsrq->device);\n\tstruct mthca_srq *srq = to_msrq(ibsrq);\n\tunsigned long flags;\n\tint err = 0;\n\tint ind;\n\tint next_ind;\n\tint nreq;\n\tint i;\n\tvoid *wqe;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\tfor (nreq = 0; wr; ++nreq, wr = wr->next) {\n\t\tind       = srq->first_free;\n\t\twqe       = get_wqe(srq, ind);\n\t\tnext_ind  = *wqe_to_link(wqe);\n\n\t\tif (unlikely(next_ind < 0)) {\n\t\t\tmthca_err(dev, \"SRQ %06x full\\n\", srq->srqn);\n\t\t\terr = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\t((struct mthca_next_seg *) wqe)->ee_nds = 0;\n\t\t \n\n\t\twqe += sizeof (struct mthca_next_seg);\n\n\t\tif (unlikely(wr->num_sge > srq->max_gs)) {\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < wr->num_sge; ++i) {\n\t\t\tmthca_set_data_seg(wqe, wr->sg_list + i);\n\t\t\twqe += sizeof (struct mthca_data_seg);\n\t\t}\n\n\t\tif (i < srq->max_gs)\n\t\t\tmthca_set_data_seg_inval(wqe);\n\n\t\tsrq->wrid[ind]  = wr->wr_id;\n\t\tsrq->first_free = next_ind;\n\t}\n\n\tif (likely(nreq)) {\n\t\tsrq->counter += nreq;\n\n\t\t \n\t\twmb();\n\t\t*srq->db = cpu_to_be32(srq->counter);\n\t}\n\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\treturn err;\n}\n\nint mthca_max_srq_sge(struct mthca_dev *dev)\n{\n\tif (mthca_is_memfree(dev))\n\t\treturn dev->limits.max_sg;\n\n\t \n\treturn min_t(int, dev->limits.max_sg,\n\t\t     ((1 << (fls(dev->limits.max_desc_sz) - 1)) -\n\t\t      sizeof (struct mthca_next_seg)) /\n\t\t     sizeof (struct mthca_data_seg));\n}\n\nint mthca_init_srq_table(struct mthca_dev *dev)\n{\n\tint err;\n\n\tif (!(dev->mthca_flags & MTHCA_FLAG_SRQ))\n\t\treturn 0;\n\n\tspin_lock_init(&dev->srq_table.lock);\n\n\terr = mthca_alloc_init(&dev->srq_table.alloc,\n\t\t\t       dev->limits.num_srqs,\n\t\t\t       dev->limits.num_srqs - 1,\n\t\t\t       dev->limits.reserved_srqs);\n\tif (err)\n\t\treturn err;\n\n\terr = mthca_array_init(&dev->srq_table.srq,\n\t\t\t       dev->limits.num_srqs);\n\tif (err)\n\t\tmthca_alloc_cleanup(&dev->srq_table.alloc);\n\n\treturn err;\n}\n\nvoid mthca_cleanup_srq_table(struct mthca_dev *dev)\n{\n\tif (!(dev->mthca_flags & MTHCA_FLAG_SRQ))\n\t\treturn;\n\n\tmthca_array_cleanup(&dev->srq_table.srq, dev->limits.num_srqs);\n\tmthca_alloc_cleanup(&dev->srq_table.alloc);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}