{
  "module_name": "pvrdma_verbs.c",
  "hash_id": "90187d3a368055d4285524d186da843d2337a1d34df4c71a9fceb97d379676ef",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.c",
  "human_readable_source": " \n\n#include <asm/page.h>\n#include <linux/inet.h>\n#include <linux/io.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/vmw_pvrdma-abi.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"pvrdma.h\"\n\n \nint pvrdma_query_device(struct ib_device *ibdev,\n\t\t\tstruct ib_device_attr *props,\n\t\t\tstruct ib_udata *uhw)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\n\tif (uhw->inlen || uhw->outlen)\n\t\treturn -EINVAL;\n\n\tprops->fw_ver = dev->dsr->caps.fw_ver;\n\tprops->sys_image_guid = dev->dsr->caps.sys_image_guid;\n\tprops->max_mr_size = dev->dsr->caps.max_mr_size;\n\tprops->page_size_cap = dev->dsr->caps.page_size_cap;\n\tprops->vendor_id = dev->dsr->caps.vendor_id;\n\tprops->vendor_part_id = dev->pdev->device;\n\tprops->hw_ver = dev->dsr->caps.hw_ver;\n\tprops->max_qp = dev->dsr->caps.max_qp;\n\tprops->max_qp_wr = dev->dsr->caps.max_qp_wr;\n\tprops->device_cap_flags = dev->dsr->caps.device_cap_flags;\n\tprops->max_send_sge = dev->dsr->caps.max_sge;\n\tprops->max_recv_sge = dev->dsr->caps.max_sge;\n\tprops->max_sge_rd = PVRDMA_GET_CAP(dev, dev->dsr->caps.max_sge,\n\t\t\t\t\t   dev->dsr->caps.max_sge_rd);\n\tprops->max_srq = dev->dsr->caps.max_srq;\n\tprops->max_srq_wr = dev->dsr->caps.max_srq_wr;\n\tprops->max_srq_sge = dev->dsr->caps.max_srq_sge;\n\tprops->max_cq = dev->dsr->caps.max_cq;\n\tprops->max_cqe = dev->dsr->caps.max_cqe;\n\tprops->max_mr = dev->dsr->caps.max_mr;\n\tprops->max_pd = dev->dsr->caps.max_pd;\n\tprops->max_qp_rd_atom = dev->dsr->caps.max_qp_rd_atom;\n\tprops->max_qp_init_rd_atom = dev->dsr->caps.max_qp_init_rd_atom;\n\tprops->atomic_cap =\n\t\tdev->dsr->caps.atomic_ops &\n\t\t(PVRDMA_ATOMIC_OP_COMP_SWAP | PVRDMA_ATOMIC_OP_FETCH_ADD) ?\n\t\tIB_ATOMIC_HCA : IB_ATOMIC_NONE;\n\tprops->masked_atomic_cap = props->atomic_cap;\n\tprops->max_ah = dev->dsr->caps.max_ah;\n\tprops->max_pkeys = dev->dsr->caps.max_pkeys;\n\tprops->local_ca_ack_delay = dev->dsr->caps.local_ca_ack_delay;\n\tif ((dev->dsr->caps.bmme_flags & PVRDMA_BMME_FLAG_LOCAL_INV) &&\n\t    (dev->dsr->caps.bmme_flags & PVRDMA_BMME_FLAG_REMOTE_INV) &&\n\t    (dev->dsr->caps.bmme_flags & PVRDMA_BMME_FLAG_FAST_REG_WR)) {\n\t\tprops->device_cap_flags |= IB_DEVICE_MEM_MGT_EXTENSIONS;\n\t\tprops->max_fast_reg_page_list_len = PVRDMA_GET_CAP(dev,\n\t\t\t\tPVRDMA_MAX_FAST_REG_PAGES,\n\t\t\t\tdev->dsr->caps.max_fast_reg_page_list_len);\n\t}\n\n\tprops->device_cap_flags |= IB_DEVICE_PORT_ACTIVE_EVENT |\n\t\t\t\t   IB_DEVICE_RC_RNR_NAK_GEN;\n\n\treturn 0;\n}\n\n \nint pvrdma_query_port(struct ib_device *ibdev, u32 port,\n\t\t      struct ib_port_attr *props)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_query_port *cmd = &req.query_port;\n\tstruct pvrdma_cmd_query_port_resp *resp = &rsp.query_port_resp;\n\tint err;\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_QUERY_PORT;\n\tcmd->port_num = port;\n\n\terr = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_QUERY_PORT_RESP);\n\tif (err < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not query port, error: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\n\tprops->state = pvrdma_port_state_to_ib(resp->attrs.state);\n\tprops->max_mtu = pvrdma_mtu_to_ib(resp->attrs.max_mtu);\n\tprops->active_mtu = pvrdma_mtu_to_ib(resp->attrs.active_mtu);\n\tprops->gid_tbl_len = resp->attrs.gid_tbl_len;\n\tprops->port_cap_flags =\n\t\tpvrdma_port_cap_flags_to_ib(resp->attrs.port_cap_flags);\n\tprops->port_cap_flags |= IB_PORT_CM_SUP;\n\tprops->ip_gids = true;\n\tprops->max_msg_sz = resp->attrs.max_msg_sz;\n\tprops->bad_pkey_cntr = resp->attrs.bad_pkey_cntr;\n\tprops->qkey_viol_cntr = resp->attrs.qkey_viol_cntr;\n\tprops->pkey_tbl_len = resp->attrs.pkey_tbl_len;\n\tprops->lid = resp->attrs.lid;\n\tprops->sm_lid = resp->attrs.sm_lid;\n\tprops->lmc = resp->attrs.lmc;\n\tprops->max_vl_num = resp->attrs.max_vl_num;\n\tprops->sm_sl = resp->attrs.sm_sl;\n\tprops->subnet_timeout = resp->attrs.subnet_timeout;\n\tprops->init_type_reply = resp->attrs.init_type_reply;\n\tprops->active_width = pvrdma_port_width_to_ib(resp->attrs.active_width);\n\tprops->active_speed = pvrdma_port_speed_to_ib(resp->attrs.active_speed);\n\tprops->phys_state = resp->attrs.phys_state;\n\n\treturn 0;\n}\n\n \nint pvrdma_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t     union ib_gid *gid)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\n\tif (index >= dev->dsr->caps.gid_tbl_len)\n\t\treturn -EINVAL;\n\n\tmemcpy(gid, &dev->sgid_tbl[index], sizeof(union ib_gid));\n\n\treturn 0;\n}\n\n \nint pvrdma_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t      u16 *pkey)\n{\n\tint err = 0;\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_query_pkey *cmd = &req.query_pkey;\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_QUERY_PKEY;\n\tcmd->port_num = port;\n\tcmd->index = index;\n\n\terr = pvrdma_cmd_post(to_vdev(ibdev), &req, &rsp,\n\t\t\t      PVRDMA_CMD_QUERY_PKEY_RESP);\n\tif (err < 0) {\n\t\tdev_warn(&to_vdev(ibdev)->pdev->dev,\n\t\t\t \"could not query pkey, error: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t*pkey = rsp.query_pkey_resp.pkey;\n\n\treturn 0;\n}\n\nenum rdma_link_layer pvrdma_port_link_layer(struct ib_device *ibdev,\n\t\t\t\t\t    u32 port)\n{\n\treturn IB_LINK_LAYER_ETHERNET;\n}\n\nint pvrdma_modify_device(struct ib_device *ibdev, int mask,\n\t\t\t struct ib_device_modify *props)\n{\n\tunsigned long flags;\n\n\tif (mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\n\t\t     IB_DEVICE_MODIFY_NODE_DESC)) {\n\t\tdev_warn(&to_vdev(ibdev)->pdev->dev,\n\t\t\t \"unsupported device modify mask %#x\\n\", mask);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (mask & IB_DEVICE_MODIFY_NODE_DESC) {\n\t\tspin_lock_irqsave(&to_vdev(ibdev)->desc_lock, flags);\n\t\tmemcpy(ibdev->node_desc, props->node_desc, 64);\n\t\tspin_unlock_irqrestore(&to_vdev(ibdev)->desc_lock, flags);\n\t}\n\n\tif (mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID) {\n\t\tmutex_lock(&to_vdev(ibdev)->port_mutex);\n\t\tto_vdev(ibdev)->sys_image_guid =\n\t\t\tcpu_to_be64(props->sys_image_guid);\n\t\tmutex_unlock(&to_vdev(ibdev)->port_mutex);\n\t}\n\n\treturn 0;\n}\n\n \nint pvrdma_modify_port(struct ib_device *ibdev, u32 port, int mask,\n\t\t       struct ib_port_modify *props)\n{\n\tstruct ib_port_attr attr;\n\tstruct pvrdma_dev *vdev = to_vdev(ibdev);\n\tint ret;\n\n\tif (mask & ~IB_PORT_SHUTDOWN) {\n\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t \"unsupported port modify mask %#x\\n\", mask);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&vdev->port_mutex);\n\tret = ib_query_port(ibdev, port, &attr);\n\tif (ret)\n\t\tgoto out;\n\n\tvdev->port_cap_mask |= props->set_port_cap_mask;\n\tvdev->port_cap_mask &= ~props->clr_port_cap_mask;\n\n\tif (mask & IB_PORT_SHUTDOWN)\n\t\tvdev->ib_active = false;\n\nout:\n\tmutex_unlock(&vdev->port_mutex);\n\treturn ret;\n}\n\n \nint pvrdma_alloc_ucontext(struct ib_ucontext *uctx, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tstruct pvrdma_dev *vdev = to_vdev(ibdev);\n\tstruct pvrdma_ucontext *context = to_vucontext(uctx);\n\tunion pvrdma_cmd_req req = {};\n\tunion pvrdma_cmd_resp rsp = {};\n\tstruct pvrdma_cmd_create_uc *cmd = &req.create_uc;\n\tstruct pvrdma_cmd_create_uc_resp *resp = &rsp.create_uc_resp;\n\tstruct pvrdma_alloc_ucontext_resp uresp = {};\n\tint ret;\n\n\tif (!vdev->ib_active)\n\t\treturn -EAGAIN;\n\n\tcontext->dev = vdev;\n\tret = pvrdma_uar_alloc(vdev, &context->uar);\n\tif (ret)\n\t\treturn -ENOMEM;\n\n\t \n\tif (vdev->dsr_version < PVRDMA_PPN64_VERSION)\n\t\tcmd->pfn = context->uar.pfn;\n\telse\n\t\tcmd->pfn64 = context->uar.pfn;\n\n\tcmd->hdr.cmd = PVRDMA_CMD_CREATE_UC;\n\tret = pvrdma_cmd_post(vdev, &req, &rsp, PVRDMA_CMD_CREATE_UC_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t \"could not create ucontext, error: %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tcontext->ctx_handle = resp->ctx_handle;\n\n\t \n\turesp.qp_tab_size = vdev->dsr->caps.max_qp;\n\tret = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (ret) {\n\t\tpvrdma_uar_free(vdev, &context->uar);\n\t\tpvrdma_dealloc_ucontext(&context->ibucontext);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n\nerr:\n\tpvrdma_uar_free(vdev, &context->uar);\n\treturn ret;\n}\n\n \nvoid pvrdma_dealloc_ucontext(struct ib_ucontext *ibcontext)\n{\n\tstruct pvrdma_ucontext *context = to_vucontext(ibcontext);\n\tunion pvrdma_cmd_req req = {};\n\tstruct pvrdma_cmd_destroy_uc *cmd = &req.destroy_uc;\n\tint ret;\n\n\tcmd->hdr.cmd = PVRDMA_CMD_DESTROY_UC;\n\tcmd->ctx_handle = context->ctx_handle;\n\n\tret = pvrdma_cmd_post(context->dev, &req, NULL, 0);\n\tif (ret < 0)\n\t\tdev_warn(&context->dev->pdev->dev,\n\t\t\t \"destroy ucontext failed, error: %d\\n\", ret);\n\n\t \n\tpvrdma_uar_free(to_vdev(ibcontext->device), &context->uar);\n}\n\n \nint pvrdma_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)\n{\n\tstruct pvrdma_ucontext *context = to_vucontext(ibcontext);\n\tunsigned long start = vma->vm_start;\n\tunsigned long size = vma->vm_end - vma->vm_start;\n\tunsigned long offset = vma->vm_pgoff << PAGE_SHIFT;\n\n\tdev_dbg(&context->dev->pdev->dev, \"create mmap region\\n\");\n\n\tif ((size != PAGE_SIZE) || (offset & ~PAGE_MASK)) {\n\t\tdev_warn(&context->dev->pdev->dev,\n\t\t\t \"invalid params for mmap region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tvm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND);\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tif (io_remap_pfn_range(vma, start, context->uar.pfn, size,\n\t\t\t       vma->vm_page_prot))\n\t\treturn -EAGAIN;\n\n\treturn 0;\n}\n\n \nint pvrdma_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct pvrdma_pd *pd = to_vpd(ibpd);\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\tunion pvrdma_cmd_req req = {};\n\tunion pvrdma_cmd_resp rsp = {};\n\tstruct pvrdma_cmd_create_pd *cmd = &req.create_pd;\n\tstruct pvrdma_cmd_create_pd_resp *resp = &rsp.create_pd_resp;\n\tstruct pvrdma_alloc_pd_resp pd_resp = {0};\n\tint ret;\n\tstruct pvrdma_ucontext *context = rdma_udata_to_drv_context(\n\t\tudata, struct pvrdma_ucontext, ibucontext);\n\n\t \n\tif (!atomic_add_unless(&dev->num_pds, 1, dev->dsr->caps.max_pd))\n\t\treturn -ENOMEM;\n\n\tcmd->hdr.cmd = PVRDMA_CMD_CREATE_PD;\n\tcmd->ctx_handle = context ? context->ctx_handle : 0;\n\tret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_CREATE_PD_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"failed to allocate protection domain, error: %d\\n\",\n\t\t\t ret);\n\t\tgoto err;\n\t}\n\n\tpd->privileged = !udata;\n\tpd->pd_handle = resp->pd_handle;\n\tpd->pdn = resp->pd_handle;\n\tpd_resp.pdn = resp->pd_handle;\n\n\tif (udata) {\n\t\tif (ib_copy_to_udata(udata, &pd_resp, sizeof(pd_resp))) {\n\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t \"failed to copy back protection domain\\n\");\n\t\t\tpvrdma_dealloc_pd(&pd->ibpd, udata);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\t \n\treturn 0;\n\nerr:\n\tatomic_dec(&dev->num_pds);\n\treturn ret;\n}\n\n \nint pvrdma_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata)\n{\n\tstruct pvrdma_dev *dev = to_vdev(pd->device);\n\tunion pvrdma_cmd_req req = {};\n\tstruct pvrdma_cmd_destroy_pd *cmd = &req.destroy_pd;\n\tint ret;\n\n\tcmd->hdr.cmd = PVRDMA_CMD_DESTROY_PD;\n\tcmd->pd_handle = to_vpd(pd)->pd_handle;\n\n\tret = pvrdma_cmd_post(dev, &req, NULL, 0);\n\tif (ret)\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not dealloc protection domain, error: %d\\n\",\n\t\t\t ret);\n\n\tatomic_dec(&dev->num_pds);\n\treturn 0;\n}\n\n \nint pvrdma_create_ah(struct ib_ah *ibah, struct rdma_ah_init_attr *init_attr,\n\t\t     struct ib_udata *udata)\n{\n\tstruct rdma_ah_attr *ah_attr = init_attr->ah_attr;\n\tstruct pvrdma_dev *dev = to_vdev(ibah->device);\n\tstruct pvrdma_ah *ah = to_vah(ibah);\n\tconst struct ib_global_route *grh;\n\tu32 port_num = rdma_ah_get_port_num(ah_attr);\n\n\tif (!(rdma_ah_get_ah_flags(ah_attr) & IB_AH_GRH))\n\t\treturn -EINVAL;\n\n\tgrh = rdma_ah_read_grh(ah_attr);\n\tif ((ah_attr->type != RDMA_AH_ATTR_TYPE_ROCE)  ||\n\t    rdma_is_multicast_addr((struct in6_addr *)grh->dgid.raw))\n\t\treturn -EINVAL;\n\n\tif (!atomic_add_unless(&dev->num_ahs, 1, dev->dsr->caps.max_ah))\n\t\treturn -ENOMEM;\n\n\tah->av.port_pd = to_vpd(ibah->pd)->pd_handle | (port_num << 24);\n\tah->av.src_path_bits = rdma_ah_get_path_bits(ah_attr);\n\tah->av.src_path_bits |= 0x80;\n\tah->av.gid_index = grh->sgid_index;\n\tah->av.hop_limit = grh->hop_limit;\n\tah->av.sl_tclass_flowlabel = (grh->traffic_class << 20) |\n\t\t\t\t      grh->flow_label;\n\tmemcpy(ah->av.dgid, grh->dgid.raw, 16);\n\tmemcpy(ah->av.dmac, ah_attr->roce.dmac, ETH_ALEN);\n\n\treturn 0;\n}\n\n \nint pvrdma_destroy_ah(struct ib_ah *ah, u32 flags)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ah->device);\n\n\tatomic_dec(&dev->num_ahs);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}