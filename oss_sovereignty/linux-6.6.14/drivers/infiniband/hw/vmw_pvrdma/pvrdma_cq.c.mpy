{
  "module_name": "pvrdma_cq.c",
  "hash_id": "21656425bd2fd8967984348cdb2ec93084ef5a3c731598d1841aedb54bf1ac9b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/vmw_pvrdma/pvrdma_cq.c",
  "human_readable_source": " \n\n#include <asm/page.h>\n#include <linux/io.h>\n#include <linux/wait.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"pvrdma.h\"\n\n \nint pvrdma_req_notify_cq(struct ib_cq *ibcq,\n\t\t\t enum ib_cq_notify_flags notify_flags)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibcq->device);\n\tstruct pvrdma_cq *cq = to_vcq(ibcq);\n\tu32 val = cq->cq_handle;\n\tunsigned long flags;\n\tint has_data = 0;\n\n\tval |= (notify_flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED ?\n\t\tPVRDMA_UAR_CQ_ARM_SOL : PVRDMA_UAR_CQ_ARM;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\n\tpvrdma_write_uar_cq(dev, val);\n\n\tif (notify_flags & IB_CQ_REPORT_MISSED_EVENTS) {\n\t\tunsigned int head;\n\n\t\thas_data = pvrdma_idx_ring_has_data(&cq->ring_state->rx,\n\t\t\t\t\t\t    cq->ibcq.cqe, &head);\n\t\tif (unlikely(has_data == PVRDMA_INVALID_IDX))\n\t\t\tdev_err(&dev->pdev->dev, \"CQ ring state invalid\\n\");\n\t}\n\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\n\treturn has_data;\n}\n\n \nint pvrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t     struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tint entries = attr->cqe;\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\tstruct pvrdma_cq *cq = to_vcq(ibcq);\n\tint ret;\n\tint npages;\n\tunsigned long flags;\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_create_cq *cmd = &req.create_cq;\n\tstruct pvrdma_cmd_create_cq_resp *resp = &rsp.create_cq_resp;\n\tstruct pvrdma_create_cq_resp cq_resp = {};\n\tstruct pvrdma_create_cq ucmd;\n\tstruct pvrdma_ucontext *context = rdma_udata_to_drv_context(\n\t\tudata, struct pvrdma_ucontext, ibucontext);\n\n\tBUILD_BUG_ON(sizeof(struct pvrdma_cqe) != 64);\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tentries = roundup_pow_of_two(entries);\n\tif (entries < 1 || entries > dev->dsr->caps.max_cqe)\n\t\treturn -EINVAL;\n\n\tif (!atomic_add_unless(&dev->num_cqs, 1, dev->dsr->caps.max_cq))\n\t\treturn -ENOMEM;\n\n\tcq->ibcq.cqe = entries;\n\tcq->is_kernel = !udata;\n\n\tif (!cq->is_kernel) {\n\t\tif (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_cq;\n\t\t}\n\n\t\tcq->umem = ib_umem_get(ibdev, ucmd.buf_addr, ucmd.buf_size,\n\t\t\t\t       IB_ACCESS_LOCAL_WRITE);\n\t\tif (IS_ERR(cq->umem)) {\n\t\t\tret = PTR_ERR(cq->umem);\n\t\t\tgoto err_cq;\n\t\t}\n\n\t\tnpages = ib_umem_num_dma_blocks(cq->umem, PAGE_SIZE);\n\t} else {\n\t\t \n\t\tnpages = 1 + (entries * sizeof(struct pvrdma_cqe) +\n\t\t\t      PAGE_SIZE - 1) / PAGE_SIZE;\n\n\t\t \n\t\tcq->offset = PAGE_SIZE;\n\t}\n\n\tif (npages < 0 || npages > PVRDMA_PAGE_DIR_MAX_PAGES) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"overflow pages in completion queue\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_umem;\n\t}\n\n\tret = pvrdma_page_dir_init(dev, &cq->pdir, npages, cq->is_kernel);\n\tif (ret) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not allocate page directory\\n\");\n\t\tgoto err_umem;\n\t}\n\n\t \n\tif (cq->is_kernel)\n\t\tcq->ring_state = cq->pdir.pages[0];\n\telse\n\t\tpvrdma_page_dir_insert_umem(&cq->pdir, cq->umem, 0);\n\n\trefcount_set(&cq->refcnt, 1);\n\tinit_completion(&cq->free);\n\tspin_lock_init(&cq->cq_lock);\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_CREATE_CQ;\n\tcmd->nchunks = npages;\n\tcmd->ctx_handle = context ? context->ctx_handle : 0;\n\tcmd->cqe = entries;\n\tcmd->pdir_dma = cq->pdir.dir_dma;\n\tret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_CREATE_CQ_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not create completion queue, error: %d\\n\", ret);\n\t\tgoto err_page_dir;\n\t}\n\n\tcq->ibcq.cqe = resp->cqe;\n\tcq->cq_handle = resp->cq_handle;\n\tcq_resp.cqn = resp->cq_handle;\n\tspin_lock_irqsave(&dev->cq_tbl_lock, flags);\n\tdev->cq_tbl[cq->cq_handle % dev->dsr->caps.max_cq] = cq;\n\tspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\n\n\tif (!cq->is_kernel) {\n\t\tcq->uar = &context->uar;\n\n\t\t \n\t\tif (ib_copy_to_udata(udata, &cq_resp, sizeof(cq_resp))) {\n\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t \"failed to copy back udata\\n\");\n\t\t\tpvrdma_destroy_cq(&cq->ibcq, udata);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_page_dir:\n\tpvrdma_page_dir_cleanup(dev, &cq->pdir);\nerr_umem:\n\tib_umem_release(cq->umem);\nerr_cq:\n\tatomic_dec(&dev->num_cqs);\n\treturn ret;\n}\n\nstatic void pvrdma_free_cq(struct pvrdma_dev *dev, struct pvrdma_cq *cq)\n{\n\tif (refcount_dec_and_test(&cq->refcnt))\n\t\tcomplete(&cq->free);\n\twait_for_completion(&cq->free);\n\n\tib_umem_release(cq->umem);\n\n\tpvrdma_page_dir_cleanup(dev, &cq->pdir);\n}\n\n \nint pvrdma_destroy_cq(struct ib_cq *cq, struct ib_udata *udata)\n{\n\tstruct pvrdma_cq *vcq = to_vcq(cq);\n\tunion pvrdma_cmd_req req;\n\tstruct pvrdma_cmd_destroy_cq *cmd = &req.destroy_cq;\n\tstruct pvrdma_dev *dev = to_vdev(cq->device);\n\tunsigned long flags;\n\tint ret;\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_DESTROY_CQ;\n\tcmd->cq_handle = vcq->cq_handle;\n\n\tret = pvrdma_cmd_post(dev, &req, NULL, 0);\n\tif (ret < 0)\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not destroy completion queue, error: %d\\n\",\n\t\t\t ret);\n\n\t \n\tspin_lock_irqsave(&dev->cq_tbl_lock, flags);\n\tdev->cq_tbl[vcq->cq_handle] = NULL;\n\tspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\n\n\tpvrdma_free_cq(dev, vcq);\n\tatomic_dec(&dev->num_cqs);\n\treturn 0;\n}\n\nstatic inline struct pvrdma_cqe *get_cqe(struct pvrdma_cq *cq, int i)\n{\n\treturn (struct pvrdma_cqe *)pvrdma_page_dir_get_ptr(\n\t\t\t\t\t&cq->pdir,\n\t\t\t\t\tcq->offset +\n\t\t\t\t\tsizeof(struct pvrdma_cqe) * i);\n}\n\nvoid _pvrdma_flush_cqe(struct pvrdma_qp *qp, struct pvrdma_cq *cq)\n{\n\tunsigned int head;\n\tint has_data;\n\n\tif (!cq->is_kernel)\n\t\treturn;\n\n\t \n\thas_data = pvrdma_idx_ring_has_data(&cq->ring_state->rx,\n\t\t\t\t\t    cq->ibcq.cqe, &head);\n\tif (unlikely(has_data > 0)) {\n\t\tint items;\n\t\tint curr;\n\t\tint tail = pvrdma_idx(&cq->ring_state->rx.prod_tail,\n\t\t\t\t      cq->ibcq.cqe);\n\t\tstruct pvrdma_cqe *cqe;\n\t\tstruct pvrdma_cqe *curr_cqe;\n\n\t\titems = (tail > head) ? (tail - head) :\n\t\t\t(cq->ibcq.cqe - head + tail);\n\t\tcurr = --tail;\n\t\twhile (items-- > 0) {\n\t\t\tif (curr < 0)\n\t\t\t\tcurr = cq->ibcq.cqe - 1;\n\t\t\tif (tail < 0)\n\t\t\t\ttail = cq->ibcq.cqe - 1;\n\t\t\tcurr_cqe = get_cqe(cq, curr);\n\t\t\tif ((curr_cqe->qp & 0xFFFF) != qp->qp_handle) {\n\t\t\t\tif (curr != tail) {\n\t\t\t\t\tcqe = get_cqe(cq, tail);\n\t\t\t\t\t*cqe = *curr_cqe;\n\t\t\t\t}\n\t\t\t\ttail--;\n\t\t\t} else {\n\t\t\t\tpvrdma_idx_ring_inc(\n\t\t\t\t\t&cq->ring_state->rx.cons_head,\n\t\t\t\t\tcq->ibcq.cqe);\n\t\t\t}\n\t\t\tcurr--;\n\t\t}\n\t}\n}\n\nstatic int pvrdma_poll_one(struct pvrdma_cq *cq, struct pvrdma_qp **cur_qp,\n\t\t\t   struct ib_wc *wc)\n{\n\tstruct pvrdma_dev *dev = to_vdev(cq->ibcq.device);\n\tint has_data;\n\tunsigned int head;\n\tbool tried = false;\n\tstruct pvrdma_cqe *cqe;\n\nretry:\n\thas_data = pvrdma_idx_ring_has_data(&cq->ring_state->rx,\n\t\t\t\t\t    cq->ibcq.cqe, &head);\n\tif (has_data == 0) {\n\t\tif (tried)\n\t\t\treturn -EAGAIN;\n\n\t\tpvrdma_write_uar_cq(dev, cq->cq_handle | PVRDMA_UAR_CQ_POLL);\n\n\t\ttried = true;\n\t\tgoto retry;\n\t} else if (has_data == PVRDMA_INVALID_IDX) {\n\t\tdev_err(&dev->pdev->dev, \"CQ ring state invalid\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\tcqe = get_cqe(cq, head);\n\n\t \n\trmb();\n\tif (dev->qp_tbl[cqe->qp & 0xffff])\n\t\t*cur_qp = (struct pvrdma_qp *)dev->qp_tbl[cqe->qp & 0xffff];\n\telse\n\t\treturn -EAGAIN;\n\n\twc->opcode = pvrdma_wc_opcode_to_ib(cqe->opcode);\n\twc->status = pvrdma_wc_status_to_ib(cqe->status);\n\twc->wr_id = cqe->wr_id;\n\twc->qp = &(*cur_qp)->ibqp;\n\twc->byte_len = cqe->byte_len;\n\twc->ex.imm_data = cqe->imm_data;\n\twc->src_qp = cqe->src_qp;\n\twc->wc_flags = pvrdma_wc_flags_to_ib(cqe->wc_flags);\n\twc->pkey_index = cqe->pkey_index;\n\twc->slid = cqe->slid;\n\twc->sl = cqe->sl;\n\twc->dlid_path_bits = cqe->dlid_path_bits;\n\twc->port_num = cqe->port_num;\n\twc->vendor_err = cqe->vendor_err;\n\twc->network_hdr_type = pvrdma_network_type_to_ib(cqe->network_hdr_type);\n\n\t \n\tpvrdma_idx_ring_inc(&cq->ring_state->rx.cons_head, cq->ibcq.cqe);\n\n\treturn 0;\n}\n\n \nint pvrdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tstruct pvrdma_cq *cq = to_vcq(ibcq);\n\tstruct pvrdma_qp *cur_qp = NULL;\n\tunsigned long flags;\n\tint npolled;\n\n\tif (num_entries < 1 || wc == NULL)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tfor (npolled = 0; npolled < num_entries; ++npolled) {\n\t\tif (pvrdma_poll_one(cq, &cur_qp, wc + npolled))\n\t\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\n\t \n\treturn npolled;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}