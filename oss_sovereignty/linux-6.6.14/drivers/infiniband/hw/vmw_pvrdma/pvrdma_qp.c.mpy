{
  "module_name": "pvrdma_qp.c",
  "hash_id": "a6266df6a6b50ce5d70bb108f983d3b3e53b91512e6e105404cb8f43af40c979",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/vmw_pvrdma/pvrdma_qp.c",
  "human_readable_source": " \n\n#include <asm/page.h>\n#include <linux/io.h>\n#include <linux/wait.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_user_verbs.h>\n\n#include \"pvrdma.h\"\n\nstatic void __pvrdma_destroy_qp(struct pvrdma_dev *dev,\n\t\t\t\tstruct pvrdma_qp *qp);\n\nstatic inline void get_cqs(struct pvrdma_qp *qp, struct pvrdma_cq **send_cq,\n\t\t\t   struct pvrdma_cq **recv_cq)\n{\n\t*send_cq = to_vcq(qp->ibqp.send_cq);\n\t*recv_cq = to_vcq(qp->ibqp.recv_cq);\n}\n\nstatic void pvrdma_lock_cqs(struct pvrdma_cq *scq, struct pvrdma_cq *rcq,\n\t\t\t    unsigned long *scq_flags,\n\t\t\t    unsigned long *rcq_flags)\n\t__acquires(scq->cq_lock) __acquires(rcq->cq_lock)\n{\n\tif (scq == rcq) {\n\t\tspin_lock_irqsave(&scq->cq_lock, *scq_flags);\n\t\t__acquire(rcq->cq_lock);\n\t} else if (scq->cq_handle < rcq->cq_handle) {\n\t\tspin_lock_irqsave(&scq->cq_lock, *scq_flags);\n\t\tspin_lock_irqsave_nested(&rcq->cq_lock, *rcq_flags,\n\t\t\t\t\t SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock_irqsave(&rcq->cq_lock, *rcq_flags);\n\t\tspin_lock_irqsave_nested(&scq->cq_lock, *scq_flags,\n\t\t\t\t\t SINGLE_DEPTH_NESTING);\n\t}\n}\n\nstatic void pvrdma_unlock_cqs(struct pvrdma_cq *scq, struct pvrdma_cq *rcq,\n\t\t\t      unsigned long *scq_flags,\n\t\t\t      unsigned long *rcq_flags)\n\t__releases(scq->cq_lock) __releases(rcq->cq_lock)\n{\n\tif (scq == rcq) {\n\t\t__release(rcq->cq_lock);\n\t\tspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\n\t} else if (scq->cq_handle < rcq->cq_handle) {\n\t\tspin_unlock_irqrestore(&rcq->cq_lock, *rcq_flags);\n\t\tspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\n\t} else {\n\t\tspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\n\t\tspin_unlock_irqrestore(&rcq->cq_lock, *rcq_flags);\n\t}\n}\n\nstatic void pvrdma_reset_qp(struct pvrdma_qp *qp)\n{\n\tstruct pvrdma_cq *scq, *rcq;\n\tunsigned long scq_flags, rcq_flags;\n\n\t \n\tget_cqs(qp, &scq, &rcq);\n\tpvrdma_lock_cqs(scq, rcq, &scq_flags, &rcq_flags);\n\n\t_pvrdma_flush_cqe(qp, scq);\n\tif (scq != rcq)\n\t\t_pvrdma_flush_cqe(qp, rcq);\n\n\tpvrdma_unlock_cqs(scq, rcq, &scq_flags, &rcq_flags);\n\n\t \n\tif (qp->rq.ring) {\n\t\tatomic_set(&qp->rq.ring->cons_head, 0);\n\t\tatomic_set(&qp->rq.ring->prod_tail, 0);\n\t}\n\tif (qp->sq.ring) {\n\t\tatomic_set(&qp->sq.ring->cons_head, 0);\n\t\tatomic_set(&qp->sq.ring->prod_tail, 0);\n\t}\n}\n\nstatic int pvrdma_set_rq_size(struct pvrdma_dev *dev,\n\t\t\t      struct ib_qp_cap *req_cap,\n\t\t\t      struct pvrdma_qp *qp)\n{\n\tif (req_cap->max_recv_wr > dev->dsr->caps.max_qp_wr ||\n\t    req_cap->max_recv_sge > dev->dsr->caps.max_sge) {\n\t\tdev_warn(&dev->pdev->dev, \"recv queue size invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tqp->rq.wqe_cnt = roundup_pow_of_two(max(1U, req_cap->max_recv_wr));\n\tqp->rq.max_sg = roundup_pow_of_two(max(1U, req_cap->max_recv_sge));\n\n\t \n\treq_cap->max_recv_wr = qp->rq.wqe_cnt;\n\treq_cap->max_recv_sge = qp->rq.max_sg;\n\n\tqp->rq.wqe_size = roundup_pow_of_two(sizeof(struct pvrdma_rq_wqe_hdr) +\n\t\t\t\t\t     sizeof(struct pvrdma_sge) *\n\t\t\t\t\t     qp->rq.max_sg);\n\tqp->npages_recv = (qp->rq.wqe_cnt * qp->rq.wqe_size + PAGE_SIZE - 1) /\n\t\t\t  PAGE_SIZE;\n\n\treturn 0;\n}\n\nstatic int pvrdma_set_sq_size(struct pvrdma_dev *dev, struct ib_qp_cap *req_cap,\n\t\t\t      struct pvrdma_qp *qp)\n{\n\tif (req_cap->max_send_wr > dev->dsr->caps.max_qp_wr ||\n\t    req_cap->max_send_sge > dev->dsr->caps.max_sge) {\n\t\tdev_warn(&dev->pdev->dev, \"send queue size invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tqp->sq.wqe_cnt = roundup_pow_of_two(max(1U, req_cap->max_send_wr));\n\tqp->sq.max_sg = roundup_pow_of_two(max(1U, req_cap->max_send_sge));\n\n\t \n\treq_cap->max_send_wr = qp->sq.wqe_cnt;\n\treq_cap->max_send_sge = qp->sq.max_sg;\n\n\tqp->sq.wqe_size = roundup_pow_of_two(sizeof(struct pvrdma_sq_wqe_hdr) +\n\t\t\t\t\t     sizeof(struct pvrdma_sge) *\n\t\t\t\t\t     qp->sq.max_sg);\n\t \n\tqp->npages_send = PVRDMA_QP_NUM_HEADER_PAGES +\n\t\t\t  (qp->sq.wqe_cnt * qp->sq.wqe_size + PAGE_SIZE - 1) /\n\t\t\t\t\t\t\t\tPAGE_SIZE;\n\n\treturn 0;\n}\n\n \nint pvrdma_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *init_attr,\n\t\t     struct ib_udata *udata)\n{\n\tstruct pvrdma_qp *qp = to_vqp(ibqp);\n\tstruct pvrdma_dev *dev = to_vdev(ibqp->device);\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_create_qp *cmd = &req.create_qp;\n\tstruct pvrdma_cmd_create_qp_resp *resp = &rsp.create_qp_resp;\n\tstruct pvrdma_cmd_create_qp_resp_v2 *resp_v2 = &rsp.create_qp_resp_v2;\n\tstruct pvrdma_create_qp ucmd;\n\tstruct pvrdma_create_qp_resp qp_resp = {};\n\tunsigned long flags;\n\tint ret;\n\tbool is_srq = !!init_attr->srq;\n\n\tif (init_attr->create_flags) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"invalid create queuepair flags %#x\\n\",\n\t\t\t init_attr->create_flags);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (init_attr->qp_type != IB_QPT_RC &&\n\t    init_attr->qp_type != IB_QPT_UD &&\n\t    init_attr->qp_type != IB_QPT_GSI) {\n\t\tdev_warn(&dev->pdev->dev, \"queuepair type %d not supported\\n\",\n\t\t\t init_attr->qp_type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (is_srq && !dev->dsr->caps.max_srq) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"SRQs not supported by device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!atomic_add_unless(&dev->num_qps, 1, dev->dsr->caps.max_qp))\n\t\treturn -ENOMEM;\n\n\tswitch (init_attr->qp_type) {\n\tcase IB_QPT_GSI:\n\t\tif (init_attr->port_num == 0 ||\n\t\t    init_attr->port_num > ibqp->device->phys_port_cnt) {\n\t\t\tdev_warn(&dev->pdev->dev, \"invalid queuepair attrs\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_qp;\n\t\t}\n\t\tfallthrough;\n\tcase IB_QPT_RC:\n\tcase IB_QPT_UD:\n\t\tspin_lock_init(&qp->sq.lock);\n\t\tspin_lock_init(&qp->rq.lock);\n\t\tmutex_init(&qp->mutex);\n\t\trefcount_set(&qp->refcnt, 1);\n\t\tinit_completion(&qp->free);\n\n\t\tqp->state = IB_QPS_RESET;\n\t\tqp->is_kernel = !udata;\n\n\t\tif (!qp->is_kernel) {\n\t\t\tdev_dbg(&dev->pdev->dev,\n\t\t\t\t\"create queuepair from user space\\n\");\n\n\t\t\tif (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto err_qp;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dev->dsr_version >= PVRDMA_QPHANDLE_VERSION &&\n\t\t\t    udata->outlen < sizeof(qp_resp)) {\n\t\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t\t \"create queuepair not supported\\n\");\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tgoto err_qp;\n\t\t\t}\n\n\t\t\tif (!is_srq) {\n\t\t\t\t \n\t\t\t\tqp->rumem = ib_umem_get(ibqp->device,\n\t\t\t\t\t\t\tucmd.rbuf_addr,\n\t\t\t\t\t\t\tucmd.rbuf_size, 0);\n\t\t\t\tif (IS_ERR(qp->rumem)) {\n\t\t\t\t\tret = PTR_ERR(qp->rumem);\n\t\t\t\t\tgoto err_qp;\n\t\t\t\t}\n\t\t\t\tqp->srq = NULL;\n\t\t\t} else {\n\t\t\t\tqp->rumem = NULL;\n\t\t\t\tqp->srq = to_vsrq(init_attr->srq);\n\t\t\t}\n\n\t\t\tqp->sumem = ib_umem_get(ibqp->device, ucmd.sbuf_addr,\n\t\t\t\t\t\tucmd.sbuf_size, 0);\n\t\t\tif (IS_ERR(qp->sumem)) {\n\t\t\t\tif (!is_srq)\n\t\t\t\t\tib_umem_release(qp->rumem);\n\t\t\t\tret = PTR_ERR(qp->sumem);\n\t\t\t\tgoto err_qp;\n\t\t\t}\n\n\t\t\tqp->npages_send =\n\t\t\t\tib_umem_num_dma_blocks(qp->sumem, PAGE_SIZE);\n\t\t\tif (!is_srq)\n\t\t\t\tqp->npages_recv = ib_umem_num_dma_blocks(\n\t\t\t\t\tqp->rumem, PAGE_SIZE);\n\t\t\telse\n\t\t\t\tqp->npages_recv = 0;\n\t\t\tqp->npages = qp->npages_send + qp->npages_recv;\n\t\t} else {\n\t\t\tret = pvrdma_set_sq_size(to_vdev(ibqp->device),\n\t\t\t\t\t\t &init_attr->cap, qp);\n\t\t\tif (ret)\n\t\t\t\tgoto err_qp;\n\n\t\t\tret = pvrdma_set_rq_size(to_vdev(ibqp->device),\n\t\t\t\t\t\t &init_attr->cap, qp);\n\t\t\tif (ret)\n\t\t\t\tgoto err_qp;\n\n\t\t\tqp->npages = qp->npages_send + qp->npages_recv;\n\n\t\t\t \n\t\t\tqp->sq.offset = PVRDMA_QP_NUM_HEADER_PAGES * PAGE_SIZE;\n\n\t\t\t \n\t\t\tqp->rq.offset = qp->npages_send * PAGE_SIZE;\n\t\t}\n\n\t\tif (qp->npages < 0 || qp->npages > PVRDMA_PAGE_DIR_MAX_PAGES) {\n\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t \"overflow pages in queuepair\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_umem;\n\t\t}\n\n\t\tret = pvrdma_page_dir_init(dev, &qp->pdir, qp->npages,\n\t\t\t\t\t   qp->is_kernel);\n\t\tif (ret) {\n\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t \"could not allocate page directory\\n\");\n\t\t\tgoto err_umem;\n\t\t}\n\n\t\tif (!qp->is_kernel) {\n\t\t\tpvrdma_page_dir_insert_umem(&qp->pdir, qp->sumem, 0);\n\t\t\tif (!is_srq)\n\t\t\t\tpvrdma_page_dir_insert_umem(&qp->pdir,\n\t\t\t\t\t\t\t    qp->rumem,\n\t\t\t\t\t\t\t    qp->npages_send);\n\t\t} else {\n\t\t\t \n\t\t\tqp->sq.ring = qp->pdir.pages[0];\n\t\t\tqp->rq.ring = is_srq ? NULL : &qp->sq.ring[1];\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err_qp;\n\t}\n\n\t \n\tinit_attr->cap.max_inline_data = 0;\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_CREATE_QP;\n\tcmd->pd_handle = to_vpd(ibqp->pd)->pd_handle;\n\tcmd->send_cq_handle = to_vcq(init_attr->send_cq)->cq_handle;\n\tcmd->recv_cq_handle = to_vcq(init_attr->recv_cq)->cq_handle;\n\tif (is_srq)\n\t\tcmd->srq_handle = to_vsrq(init_attr->srq)->srq_handle;\n\telse\n\t\tcmd->srq_handle = 0;\n\tcmd->max_send_wr = init_attr->cap.max_send_wr;\n\tcmd->max_recv_wr = init_attr->cap.max_recv_wr;\n\tcmd->max_send_sge = init_attr->cap.max_send_sge;\n\tcmd->max_recv_sge = init_attr->cap.max_recv_sge;\n\tcmd->max_inline_data = init_attr->cap.max_inline_data;\n\tcmd->sq_sig_all = (init_attr->sq_sig_type == IB_SIGNAL_ALL_WR) ? 1 : 0;\n\tcmd->qp_type = ib_qp_type_to_pvrdma(init_attr->qp_type);\n\tcmd->is_srq = is_srq;\n\tcmd->lkey = 0;\n\tcmd->access_flags = IB_ACCESS_LOCAL_WRITE;\n\tcmd->total_chunks = qp->npages;\n\tcmd->send_chunks = qp->npages_send - PVRDMA_QP_NUM_HEADER_PAGES;\n\tcmd->pdir_dma = qp->pdir.dir_dma;\n\n\tdev_dbg(&dev->pdev->dev, \"create queuepair with %d, %d, %d, %d\\n\",\n\t\tcmd->max_send_wr, cmd->max_recv_wr, cmd->max_send_sge,\n\t\tcmd->max_recv_sge);\n\n\tret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_CREATE_QP_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not create queuepair, error: %d\\n\", ret);\n\t\tgoto err_pdir;\n\t}\n\n\t \n\tqp->port = init_attr->port_num;\n\n\tif (dev->dsr_version >= PVRDMA_QPHANDLE_VERSION) {\n\t\tqp->ibqp.qp_num = resp_v2->qpn;\n\t\tqp->qp_handle = resp_v2->qp_handle;\n\t} else {\n\t\tqp->ibqp.qp_num = resp->qpn;\n\t\tqp->qp_handle = resp->qpn;\n\t}\n\n\tspin_lock_irqsave(&dev->qp_tbl_lock, flags);\n\tdev->qp_tbl[qp->qp_handle % dev->dsr->caps.max_qp] = qp;\n\tspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\n\n\tif (udata) {\n\t\tqp_resp.qpn = qp->ibqp.qp_num;\n\t\tqp_resp.qp_handle = qp->qp_handle;\n\n\t\tif (ib_copy_to_udata(udata, &qp_resp,\n\t\t\t\t     min(udata->outlen, sizeof(qp_resp)))) {\n\t\t\tdev_warn(&dev->pdev->dev,\n\t\t\t\t \"failed to copy back udata\\n\");\n\t\t\t__pvrdma_destroy_qp(dev, qp);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_pdir:\n\tpvrdma_page_dir_cleanup(dev, &qp->pdir);\nerr_umem:\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\nerr_qp:\n\tatomic_dec(&dev->num_qps);\n\treturn ret;\n}\n\nstatic void _pvrdma_free_qp(struct pvrdma_qp *qp)\n{\n\tunsigned long flags;\n\tstruct pvrdma_dev *dev = to_vdev(qp->ibqp.device);\n\n\tspin_lock_irqsave(&dev->qp_tbl_lock, flags);\n\tdev->qp_tbl[qp->qp_handle] = NULL;\n\tspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\n\n\tif (refcount_dec_and_test(&qp->refcnt))\n\t\tcomplete(&qp->free);\n\twait_for_completion(&qp->free);\n\n\tib_umem_release(qp->rumem);\n\tib_umem_release(qp->sumem);\n\n\tpvrdma_page_dir_cleanup(dev, &qp->pdir);\n\n\tatomic_dec(&dev->num_qps);\n}\n\nstatic void pvrdma_free_qp(struct pvrdma_qp *qp)\n{\n\tstruct pvrdma_cq *scq;\n\tstruct pvrdma_cq *rcq;\n\tunsigned long scq_flags, rcq_flags;\n\n\t \n\tget_cqs(qp, &scq, &rcq);\n\tpvrdma_lock_cqs(scq, rcq, &scq_flags, &rcq_flags);\n\n\t_pvrdma_flush_cqe(qp, scq);\n\tif (scq != rcq)\n\t\t_pvrdma_flush_cqe(qp, rcq);\n\n\t \n\tpvrdma_unlock_cqs(scq, rcq, &scq_flags, &rcq_flags);\n\n\t_pvrdma_free_qp(qp);\n}\n\nstatic inline void _pvrdma_destroy_qp_work(struct pvrdma_dev *dev,\n\t\t\t\t\t   u32 qp_handle)\n{\n\tunion pvrdma_cmd_req req;\n\tstruct pvrdma_cmd_destroy_qp *cmd = &req.destroy_qp;\n\tint ret;\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_DESTROY_QP;\n\tcmd->qp_handle = qp_handle;\n\n\tret = pvrdma_cmd_post(dev, &req, NULL, 0);\n\tif (ret < 0)\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"destroy queuepair failed, error: %d\\n\", ret);\n}\n\n \nint pvrdma_destroy_qp(struct ib_qp *qp, struct ib_udata *udata)\n{\n\tstruct pvrdma_qp *vqp = to_vqp(qp);\n\n\t_pvrdma_destroy_qp_work(to_vdev(qp->device), vqp->qp_handle);\n\tpvrdma_free_qp(vqp);\n\n\treturn 0;\n}\n\nstatic void __pvrdma_destroy_qp(struct pvrdma_dev *dev,\n\t\t\t\tstruct pvrdma_qp *qp)\n{\n\t_pvrdma_destroy_qp_work(dev, qp->qp_handle);\n\t_pvrdma_free_qp(qp);\n}\n\n \nint pvrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t     int attr_mask, struct ib_udata *udata)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibqp->device);\n\tstruct pvrdma_qp *qp = to_vqp(ibqp);\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_modify_qp *cmd = &req.modify_qp;\n\tenum ib_qp_state cur_state, next_state;\n\tint ret;\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tmutex_lock(&qp->mutex);\n\tcur_state = (attr_mask & IB_QP_CUR_STATE) ? attr->cur_qp_state :\n\t\tqp->state;\n\tnext_state = (attr_mask & IB_QP_STATE) ? attr->qp_state : cur_state;\n\n\tif (!ib_modify_qp_is_ok(cur_state, next_state, ibqp->qp_type,\n\t\t\t\tattr_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (attr_mask & IB_QP_PORT) {\n\t\tif (attr->port_num == 0 ||\n\t\t    attr->port_num > ibqp->device->phys_port_cnt) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_MIN_RNR_TIMER) {\n\t\tif (attr->min_rnr_timer > 31) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_PKEY_INDEX) {\n\t\tif (attr->pkey_index >= dev->dsr->caps.max_pkeys) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_QKEY)\n\t\tqp->qkey = attr->qkey;\n\n\tif (cur_state == next_state && cur_state == IB_QPS_RESET) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tqp->state = next_state;\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_MODIFY_QP;\n\tcmd->qp_handle = qp->qp_handle;\n\tcmd->attr_mask = ib_qp_attr_mask_to_pvrdma(attr_mask);\n\tcmd->attrs.qp_state = ib_qp_state_to_pvrdma(attr->qp_state);\n\tcmd->attrs.cur_qp_state =\n\t\tib_qp_state_to_pvrdma(attr->cur_qp_state);\n\tcmd->attrs.path_mtu = ib_mtu_to_pvrdma(attr->path_mtu);\n\tcmd->attrs.path_mig_state =\n\t\tib_mig_state_to_pvrdma(attr->path_mig_state);\n\tcmd->attrs.qkey = attr->qkey;\n\tcmd->attrs.rq_psn = attr->rq_psn;\n\tcmd->attrs.sq_psn = attr->sq_psn;\n\tcmd->attrs.dest_qp_num = attr->dest_qp_num;\n\tcmd->attrs.qp_access_flags =\n\t\tib_access_flags_to_pvrdma(attr->qp_access_flags);\n\tcmd->attrs.pkey_index = attr->pkey_index;\n\tcmd->attrs.alt_pkey_index = attr->alt_pkey_index;\n\tcmd->attrs.en_sqd_async_notify = attr->en_sqd_async_notify;\n\tcmd->attrs.sq_draining = attr->sq_draining;\n\tcmd->attrs.max_rd_atomic = attr->max_rd_atomic;\n\tcmd->attrs.max_dest_rd_atomic = attr->max_dest_rd_atomic;\n\tcmd->attrs.min_rnr_timer = attr->min_rnr_timer;\n\tcmd->attrs.port_num = attr->port_num;\n\tcmd->attrs.timeout = attr->timeout;\n\tcmd->attrs.retry_cnt = attr->retry_cnt;\n\tcmd->attrs.rnr_retry = attr->rnr_retry;\n\tcmd->attrs.alt_port_num = attr->alt_port_num;\n\tcmd->attrs.alt_timeout = attr->alt_timeout;\n\tib_qp_cap_to_pvrdma(&cmd->attrs.cap, &attr->cap);\n\trdma_ah_attr_to_pvrdma(&cmd->attrs.ah_attr, &attr->ah_attr);\n\trdma_ah_attr_to_pvrdma(&cmd->attrs.alt_ah_attr, &attr->alt_ah_attr);\n\n\tret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_MODIFY_QP_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not modify queuepair, error: %d\\n\", ret);\n\t} else if (rsp.hdr.err > 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"cannot modify queuepair, error: %d\\n\", rsp.hdr.err);\n\t\tret = -EINVAL;\n\t}\n\n\tif (ret == 0 && next_state == IB_QPS_RESET)\n\t\tpvrdma_reset_qp(qp);\n\nout:\n\tmutex_unlock(&qp->mutex);\n\n\treturn ret;\n}\n\nstatic inline void *get_sq_wqe(struct pvrdma_qp *qp, unsigned int n)\n{\n\treturn pvrdma_page_dir_get_ptr(&qp->pdir,\n\t\t\t\t       qp->sq.offset + n * qp->sq.wqe_size);\n}\n\nstatic inline void *get_rq_wqe(struct pvrdma_qp *qp, unsigned int n)\n{\n\treturn pvrdma_page_dir_get_ptr(&qp->pdir,\n\t\t\t\t       qp->rq.offset + n * qp->rq.wqe_size);\n}\n\nstatic int set_reg_seg(struct pvrdma_sq_wqe_hdr *wqe_hdr,\n\t\t       const struct ib_reg_wr *wr)\n{\n\tstruct pvrdma_user_mr *mr = to_vmr(wr->mr);\n\n\twqe_hdr->wr.fast_reg.iova_start = mr->ibmr.iova;\n\twqe_hdr->wr.fast_reg.pl_pdir_dma = mr->pdir.dir_dma;\n\twqe_hdr->wr.fast_reg.page_shift = mr->page_shift;\n\twqe_hdr->wr.fast_reg.page_list_len = mr->npages;\n\twqe_hdr->wr.fast_reg.length = mr->ibmr.length;\n\twqe_hdr->wr.fast_reg.access_flags = wr->access;\n\twqe_hdr->wr.fast_reg.rkey = wr->key;\n\n\treturn pvrdma_page_dir_insert_page_list(&mr->pdir, mr->pages,\n\t\t\t\t\t\tmr->npages);\n}\n\n \nint pvrdma_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t     const struct ib_send_wr **bad_wr)\n{\n\tstruct pvrdma_qp *qp = to_vqp(ibqp);\n\tstruct pvrdma_dev *dev = to_vdev(ibqp->device);\n\tunsigned long flags;\n\tstruct pvrdma_sq_wqe_hdr *wqe_hdr;\n\tstruct pvrdma_sge *sge;\n\tint i, ret;\n\n\t \n\tif (qp->state < IB_QPS_RTS) {\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&qp->sq.lock, flags);\n\n\twhile (wr) {\n\t\tunsigned int tail = 0;\n\n\t\tif (unlikely(!pvrdma_idx_ring_has_space(\n\t\t\t\tqp->sq.ring, qp->sq.wqe_cnt, &tail))) {\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"send queue is full\\n\");\n\t\t\t*bad_wr = wr;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (unlikely(wr->num_sge > qp->sq.max_sg || wr->num_sge < 0)) {\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"send SGE overflow\\n\");\n\t\t\t*bad_wr = wr;\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (qp->ibqp.qp_type != IB_QPT_UD &&\n\t\t    qp->ibqp.qp_type != IB_QPT_RC &&\n\t\t\twr->opcode != IB_WR_SEND) {\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"unsupported queuepair type\\n\");\n\t\t\t*bad_wr = wr;\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t} else if (qp->ibqp.qp_type == IB_QPT_UD ||\n\t\t\t   qp->ibqp.qp_type == IB_QPT_GSI) {\n\t\t\tif (wr->opcode != IB_WR_SEND &&\n\t\t\t    wr->opcode != IB_WR_SEND_WITH_IMM) {\n\t\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t\t     \"invalid send opcode\\n\");\n\t\t\t\t*bad_wr = wr;\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\twqe_hdr = (struct pvrdma_sq_wqe_hdr *)get_sq_wqe(qp, tail);\n\t\tmemset(wqe_hdr, 0, sizeof(*wqe_hdr));\n\t\twqe_hdr->wr_id = wr->wr_id;\n\t\twqe_hdr->num_sge = wr->num_sge;\n\t\twqe_hdr->opcode = ib_wr_opcode_to_pvrdma(wr->opcode);\n\t\twqe_hdr->send_flags = ib_send_flags_to_pvrdma(wr->send_flags);\n\t\tif (wr->opcode == IB_WR_SEND_WITH_IMM ||\n\t\t    wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)\n\t\t\twqe_hdr->ex.imm_data = wr->ex.imm_data;\n\n\t\tif (unlikely(wqe_hdr->opcode == PVRDMA_WR_ERROR)) {\n\t\t\t*bad_wr = wr;\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tswitch (qp->ibqp.qp_type) {\n\t\tcase IB_QPT_GSI:\n\t\tcase IB_QPT_UD:\n\t\t\tif (unlikely(!ud_wr(wr)->ah)) {\n\t\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t\t     \"invalid address handle\\n\");\n\t\t\t\t*bad_wr = wr;\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t \n\t\t\twqe_hdr->wr.ud.remote_qpn = ud_wr(wr)->remote_qpn;\n\t\t\twqe_hdr->wr.ud.remote_qkey =\n\t\t\t\tud_wr(wr)->remote_qkey & 0x80000000 ?\n\t\t\t\tqp->qkey : ud_wr(wr)->remote_qkey;\n\t\t\twqe_hdr->wr.ud.av = to_vah(ud_wr(wr)->ah)->av;\n\n\t\t\tbreak;\n\t\tcase IB_QPT_RC:\n\t\t\tswitch (wr->opcode) {\n\t\t\tcase IB_WR_RDMA_READ:\n\t\t\tcase IB_WR_RDMA_WRITE:\n\t\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\t\t\twqe_hdr->wr.rdma.remote_addr =\n\t\t\t\t\trdma_wr(wr)->remote_addr;\n\t\t\t\twqe_hdr->wr.rdma.rkey = rdma_wr(wr)->rkey;\n\t\t\t\tbreak;\n\t\t\tcase IB_WR_LOCAL_INV:\n\t\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\t\twqe_hdr->ex.invalidate_rkey =\n\t\t\t\t\twr->ex.invalidate_rkey;\n\t\t\t\tbreak;\n\t\t\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\t\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\t\t\twqe_hdr->wr.atomic.remote_addr =\n\t\t\t\t\tatomic_wr(wr)->remote_addr;\n\t\t\t\twqe_hdr->wr.atomic.rkey = atomic_wr(wr)->rkey;\n\t\t\t\twqe_hdr->wr.atomic.compare_add =\n\t\t\t\t\tatomic_wr(wr)->compare_add;\n\t\t\t\tif (wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP)\n\t\t\t\t\twqe_hdr->wr.atomic.swap =\n\t\t\t\t\t\tatomic_wr(wr)->swap;\n\t\t\t\tbreak;\n\t\t\tcase IB_WR_REG_MR:\n\t\t\t\tret = set_reg_seg(wqe_hdr, reg_wr(wr));\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t\t\t     \"Failed to set fast register work request\\n\");\n\t\t\t\t\t*bad_wr = wr;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"invalid queuepair type\\n\");\n\t\t\tret = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsge = (struct pvrdma_sge *)(wqe_hdr + 1);\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\t \n\t\t\tsge->addr = wr->sg_list[i].addr;\n\t\t\tsge->length = wr->sg_list[i].length;\n\t\t\tsge->lkey = wr->sg_list[i].lkey;\n\t\t\tsge++;\n\t\t}\n\n\t\t \n\t\tsmp_wmb();\n\n\t\t \n\t\tpvrdma_idx_ring_inc(&qp->sq.ring->prod_tail,\n\t\t\t\t    qp->sq.wqe_cnt);\n\n\t\twr = wr->next;\n\t}\n\n\tret = 0;\n\nout:\n\tspin_unlock_irqrestore(&qp->sq.lock, flags);\n\n\tif (!ret)\n\t\tpvrdma_write_uar_qp(dev, PVRDMA_UAR_QP_SEND | qp->qp_handle);\n\n\treturn ret;\n}\n\n \nint pvrdma_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t     const struct ib_recv_wr **bad_wr)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibqp->device);\n\tunsigned long flags;\n\tstruct pvrdma_qp *qp = to_vqp(ibqp);\n\tstruct pvrdma_rq_wqe_hdr *wqe_hdr;\n\tstruct pvrdma_sge *sge;\n\tint ret = 0;\n\tint i;\n\n\t \n\tif (qp->state == IB_QPS_RESET) {\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\tif (qp->srq) {\n\t\tdev_warn(&dev->pdev->dev, \"QP associated with SRQ\\n\");\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&qp->rq.lock, flags);\n\n\twhile (wr) {\n\t\tunsigned int tail = 0;\n\n\t\tif (unlikely(wr->num_sge > qp->rq.max_sg ||\n\t\t\t     wr->num_sge < 0)) {\n\t\t\tret = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"recv SGE overflow\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (unlikely(!pvrdma_idx_ring_has_space(\n\t\t\t\tqp->rq.ring, qp->rq.wqe_cnt, &tail))) {\n\t\t\tret = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tdev_warn_ratelimited(&dev->pdev->dev,\n\t\t\t\t\t     \"recv queue full\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\twqe_hdr = (struct pvrdma_rq_wqe_hdr *)get_rq_wqe(qp, tail);\n\t\twqe_hdr->wr_id = wr->wr_id;\n\t\twqe_hdr->num_sge = wr->num_sge;\n\t\twqe_hdr->total_len = 0;\n\n\t\tsge = (struct pvrdma_sge *)(wqe_hdr + 1);\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tsge->addr = wr->sg_list[i].addr;\n\t\t\tsge->length = wr->sg_list[i].length;\n\t\t\tsge->lkey = wr->sg_list[i].lkey;\n\t\t\tsge++;\n\t\t}\n\n\t\t \n\t\tsmp_wmb();\n\n\t\t \n\t\tpvrdma_idx_ring_inc(&qp->rq.ring->prod_tail,\n\t\t\t\t    qp->rq.wqe_cnt);\n\n\t\twr = wr->next;\n\t}\n\n\tspin_unlock_irqrestore(&qp->rq.lock, flags);\n\n\tpvrdma_write_uar_qp(dev, PVRDMA_UAR_QP_RECV | qp->qp_handle);\n\n\treturn ret;\n\nout:\n\tspin_unlock_irqrestore(&qp->rq.lock, flags);\n\n\treturn ret;\n}\n\n \nint pvrdma_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t    int attr_mask, struct ib_qp_init_attr *init_attr)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibqp->device);\n\tstruct pvrdma_qp *qp = to_vqp(ibqp);\n\tunion pvrdma_cmd_req req;\n\tunion pvrdma_cmd_resp rsp;\n\tstruct pvrdma_cmd_query_qp *cmd = &req.query_qp;\n\tstruct pvrdma_cmd_query_qp_resp *resp = &rsp.query_qp_resp;\n\tint ret = 0;\n\n\tmutex_lock(&qp->mutex);\n\n\tif (qp->state == IB_QPS_RESET) {\n\t\tattr->qp_state = IB_QPS_RESET;\n\t\tgoto out;\n\t}\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->hdr.cmd = PVRDMA_CMD_QUERY_QP;\n\tcmd->qp_handle = qp->qp_handle;\n\tcmd->attr_mask = ib_qp_attr_mask_to_pvrdma(attr_mask);\n\n\tret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_QUERY_QP_RESP);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not query queuepair, error: %d\\n\", ret);\n\t\tgoto out;\n\t}\n\n\tattr->qp_state = pvrdma_qp_state_to_ib(resp->attrs.qp_state);\n\tattr->cur_qp_state =\n\t\tpvrdma_qp_state_to_ib(resp->attrs.cur_qp_state);\n\tattr->path_mtu = pvrdma_mtu_to_ib(resp->attrs.path_mtu);\n\tattr->path_mig_state =\n\t\tpvrdma_mig_state_to_ib(resp->attrs.path_mig_state);\n\tattr->qkey = resp->attrs.qkey;\n\tattr->rq_psn = resp->attrs.rq_psn;\n\tattr->sq_psn = resp->attrs.sq_psn;\n\tattr->dest_qp_num = resp->attrs.dest_qp_num;\n\tattr->qp_access_flags =\n\t\tpvrdma_access_flags_to_ib(resp->attrs.qp_access_flags);\n\tattr->pkey_index = resp->attrs.pkey_index;\n\tattr->alt_pkey_index = resp->attrs.alt_pkey_index;\n\tattr->en_sqd_async_notify = resp->attrs.en_sqd_async_notify;\n\tattr->sq_draining = resp->attrs.sq_draining;\n\tattr->max_rd_atomic = resp->attrs.max_rd_atomic;\n\tattr->max_dest_rd_atomic = resp->attrs.max_dest_rd_atomic;\n\tattr->min_rnr_timer = resp->attrs.min_rnr_timer;\n\tattr->port_num = resp->attrs.port_num;\n\tattr->timeout = resp->attrs.timeout;\n\tattr->retry_cnt = resp->attrs.retry_cnt;\n\tattr->rnr_retry = resp->attrs.rnr_retry;\n\tattr->alt_port_num = resp->attrs.alt_port_num;\n\tattr->alt_timeout = resp->attrs.alt_timeout;\n\tpvrdma_qp_cap_to_ib(&attr->cap, &resp->attrs.cap);\n\tpvrdma_ah_attr_to_rdma(&attr->ah_attr, &resp->attrs.ah_attr);\n\tpvrdma_ah_attr_to_rdma(&attr->alt_ah_attr, &resp->attrs.alt_ah_attr);\n\n\tqp->state = attr->qp_state;\n\n\tret = 0;\n\nout:\n\tattr->cur_qp_state = attr->qp_state;\n\n\tinit_attr->event_handler = qp->ibqp.event_handler;\n\tinit_attr->qp_context = qp->ibqp.qp_context;\n\tinit_attr->send_cq = qp->ibqp.send_cq;\n\tinit_attr->recv_cq = qp->ibqp.recv_cq;\n\tinit_attr->srq = qp->ibqp.srq;\n\tinit_attr->xrcd = NULL;\n\tinit_attr->cap = attr->cap;\n\tinit_attr->sq_sig_type = 0;\n\tinit_attr->qp_type = qp->ibqp.qp_type;\n\tinit_attr->create_flags = 0;\n\tinit_attr->port_num = qp->port;\n\n\tmutex_unlock(&qp->mutex);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}