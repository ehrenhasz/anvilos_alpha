{
  "module_name": "pvrdma_main.c",
  "hash_id": "c8dcc889a605578ad8a4dad5e80b24bc92884d346fc89e08c225360dcd02a0d0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/vmw_pvrdma/pvrdma_main.c",
  "human_readable_source": " \n\n#include <linux/errno.h>\n#include <linux/inetdevice.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_user_verbs.h>\n#include <net/addrconf.h>\n\n#include \"pvrdma.h\"\n\n#define DRV_NAME\t\"vmw_pvrdma\"\n#define DRV_VERSION\t\"1.0.1.0-k\"\n\nstatic DEFINE_MUTEX(pvrdma_device_list_lock);\nstatic LIST_HEAD(pvrdma_device_list);\nstatic struct workqueue_struct *event_wq;\n\nstatic int pvrdma_add_gid(const struct ib_gid_attr *attr, void **context);\nstatic int pvrdma_del_gid(const struct ib_gid_attr *attr, void **context);\n\nstatic ssize_t hca_type_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"VMW_PVRDMA-%s\\n\", DRV_VERSION);\n}\nstatic DEVICE_ATTR_RO(hca_type);\n\nstatic ssize_t hw_rev_show(struct device *device,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", PVRDMA_REV_ID);\n}\nstatic DEVICE_ATTR_RO(hw_rev);\n\nstatic ssize_t board_id_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", PVRDMA_BOARD_ID);\n}\nstatic DEVICE_ATTR_RO(board_id);\n\nstatic struct attribute *pvrdma_class_attributes[] = {\n\t&dev_attr_hw_rev.attr,\n\t&dev_attr_hca_type.attr,\n\t&dev_attr_board_id.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group pvrdma_attr_group = {\n\t.attrs = pvrdma_class_attributes,\n};\n\nstatic void pvrdma_get_fw_ver_str(struct ib_device *device, char *str)\n{\n\tstruct pvrdma_dev *dev =\n\t\tcontainer_of(device, struct pvrdma_dev, ib_dev);\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%d\\n\",\n\t\t (int) (dev->dsr->caps.fw_ver >> 32),\n\t\t (int) (dev->dsr->caps.fw_ver >> 16) & 0xffff,\n\t\t (int) dev->dsr->caps.fw_ver & 0xffff);\n}\n\nstatic int pvrdma_init_device(struct pvrdma_dev *dev)\n{\n\t \n\tspin_lock_init(&dev->cmd_lock);\n\tsema_init(&dev->cmd_sema, 1);\n\tatomic_set(&dev->num_qps, 0);\n\tatomic_set(&dev->num_srqs, 0);\n\tatomic_set(&dev->num_cqs, 0);\n\tatomic_set(&dev->num_pds, 0);\n\tatomic_set(&dev->num_ahs, 0);\n\n\treturn 0;\n}\n\nstatic int pvrdma_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t\t struct ib_port_immutable *immutable)\n{\n\tstruct pvrdma_dev *dev = to_vdev(ibdev);\n\tstruct ib_port_attr attr;\n\tint err;\n\n\tif (dev->dsr->caps.gid_types == PVRDMA_GID_TYPE_FLAG_ROCE_V1)\n\t\timmutable->core_cap_flags |= RDMA_CORE_PORT_IBA_ROCE;\n\telse if (dev->dsr->caps.gid_types == PVRDMA_GID_TYPE_FLAG_ROCE_V2)\n\t\timmutable->core_cap_flags |= RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\treturn 0;\n}\n\nstatic const struct ib_device_ops pvrdma_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_VMW_PVRDMA,\n\t.uverbs_abi_ver = PVRDMA_UVERBS_ABI_VERSION,\n\n\t.add_gid = pvrdma_add_gid,\n\t.alloc_mr = pvrdma_alloc_mr,\n\t.alloc_pd = pvrdma_alloc_pd,\n\t.alloc_ucontext = pvrdma_alloc_ucontext,\n\t.create_ah = pvrdma_create_ah,\n\t.create_cq = pvrdma_create_cq,\n\t.create_qp = pvrdma_create_qp,\n\t.dealloc_pd = pvrdma_dealloc_pd,\n\t.dealloc_ucontext = pvrdma_dealloc_ucontext,\n\t.del_gid = pvrdma_del_gid,\n\t.dereg_mr = pvrdma_dereg_mr,\n\t.destroy_ah = pvrdma_destroy_ah,\n\t.destroy_cq = pvrdma_destroy_cq,\n\t.destroy_qp = pvrdma_destroy_qp,\n\t.device_group = &pvrdma_attr_group,\n\t.get_dev_fw_str = pvrdma_get_fw_ver_str,\n\t.get_dma_mr = pvrdma_get_dma_mr,\n\t.get_link_layer = pvrdma_port_link_layer,\n\t.get_port_immutable = pvrdma_port_immutable,\n\t.map_mr_sg = pvrdma_map_mr_sg,\n\t.mmap = pvrdma_mmap,\n\t.modify_port = pvrdma_modify_port,\n\t.modify_qp = pvrdma_modify_qp,\n\t.poll_cq = pvrdma_poll_cq,\n\t.post_recv = pvrdma_post_recv,\n\t.post_send = pvrdma_post_send,\n\t.query_device = pvrdma_query_device,\n\t.query_gid = pvrdma_query_gid,\n\t.query_pkey = pvrdma_query_pkey,\n\t.query_port = pvrdma_query_port,\n\t.query_qp = pvrdma_query_qp,\n\t.reg_user_mr = pvrdma_reg_user_mr,\n\t.req_notify_cq = pvrdma_req_notify_cq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_ah, pvrdma_ah, ibah),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, pvrdma_cq, ibcq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, pvrdma_pd, ibpd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, pvrdma_qp, ibqp),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, pvrdma_ucontext, ibucontext),\n};\n\nstatic const struct ib_device_ops pvrdma_dev_srq_ops = {\n\t.create_srq = pvrdma_create_srq,\n\t.destroy_srq = pvrdma_destroy_srq,\n\t.modify_srq = pvrdma_modify_srq,\n\t.query_srq = pvrdma_query_srq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_srq, pvrdma_srq, ibsrq),\n};\n\nstatic int pvrdma_register_device(struct pvrdma_dev *dev)\n{\n\tint ret = -1;\n\n\tdev->ib_dev.node_guid = dev->dsr->caps.node_guid;\n\tdev->sys_image_guid = dev->dsr->caps.sys_image_guid;\n\tdev->flags = 0;\n\tdev->ib_dev.num_comp_vectors = 1;\n\tdev->ib_dev.dev.parent = &dev->pdev->dev;\n\n\tdev->ib_dev.node_type = RDMA_NODE_IB_CA;\n\tdev->ib_dev.phys_port_cnt = dev->dsr->caps.phys_port_cnt;\n\n\tib_set_device_ops(&dev->ib_dev, &pvrdma_dev_ops);\n\n\tmutex_init(&dev->port_mutex);\n\tspin_lock_init(&dev->desc_lock);\n\n\tdev->cq_tbl = kcalloc(dev->dsr->caps.max_cq, sizeof(struct pvrdma_cq *),\n\t\t\t      GFP_KERNEL);\n\tif (!dev->cq_tbl)\n\t\treturn ret;\n\tspin_lock_init(&dev->cq_tbl_lock);\n\n\tdev->qp_tbl = kcalloc(dev->dsr->caps.max_qp, sizeof(struct pvrdma_qp *),\n\t\t\t      GFP_KERNEL);\n\tif (!dev->qp_tbl)\n\t\tgoto err_cq_free;\n\tspin_lock_init(&dev->qp_tbl_lock);\n\n\t \n\tif (dev->dsr->caps.max_srq) {\n\t\tib_set_device_ops(&dev->ib_dev, &pvrdma_dev_srq_ops);\n\n\t\tdev->srq_tbl = kcalloc(dev->dsr->caps.max_srq,\n\t\t\t\t       sizeof(struct pvrdma_srq *),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!dev->srq_tbl)\n\t\t\tgoto err_qp_free;\n\t}\n\tret = ib_device_set_netdev(&dev->ib_dev, dev->netdev, 1);\n\tif (ret)\n\t\tgoto err_srq_free;\n\tspin_lock_init(&dev->srq_tbl_lock);\n\n\tret = ib_register_device(&dev->ib_dev, \"vmw_pvrdma%d\", &dev->pdev->dev);\n\tif (ret)\n\t\tgoto err_srq_free;\n\n\tdev->ib_active = true;\n\n\treturn 0;\n\nerr_srq_free:\n\tkfree(dev->srq_tbl);\nerr_qp_free:\n\tkfree(dev->qp_tbl);\nerr_cq_free:\n\tkfree(dev->cq_tbl);\n\n\treturn ret;\n}\n\nstatic irqreturn_t pvrdma_intr0_handler(int irq, void *dev_id)\n{\n\tu32 icr = PVRDMA_INTR_CAUSE_RESPONSE;\n\tstruct pvrdma_dev *dev = dev_id;\n\n\tdev_dbg(&dev->pdev->dev, \"interrupt 0 (response) handler\\n\");\n\n\tif (!dev->pdev->msix_enabled) {\n\t\t \n\t\ticr = pvrdma_read_reg(dev, PVRDMA_REG_ICR);\n\t\tif (icr == 0)\n\t\t\treturn IRQ_NONE;\n\t}\n\n\tif (icr == PVRDMA_INTR_CAUSE_RESPONSE)\n\t\tcomplete(&dev->cmd_done);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void pvrdma_qp_event(struct pvrdma_dev *dev, u32 qpn, int type)\n{\n\tstruct pvrdma_qp *qp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->qp_tbl_lock, flags);\n\tqp = dev->qp_tbl[qpn % dev->dsr->caps.max_qp];\n\tif (qp)\n\t\trefcount_inc(&qp->refcnt);\n\tspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\n\n\tif (qp && qp->ibqp.event_handler) {\n\t\tstruct ib_qp *ibqp = &qp->ibqp;\n\t\tstruct ib_event e;\n\n\t\te.device = ibqp->device;\n\t\te.element.qp = ibqp;\n\t\te.event = type;  \n\t\tibqp->event_handler(&e, ibqp->qp_context);\n\t}\n\tif (qp) {\n\t\tif (refcount_dec_and_test(&qp->refcnt))\n\t\t\tcomplete(&qp->free);\n\t}\n}\n\nstatic void pvrdma_cq_event(struct pvrdma_dev *dev, u32 cqn, int type)\n{\n\tstruct pvrdma_cq *cq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->cq_tbl_lock, flags);\n\tcq = dev->cq_tbl[cqn % dev->dsr->caps.max_cq];\n\tif (cq)\n\t\trefcount_inc(&cq->refcnt);\n\tspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\n\n\tif (cq && cq->ibcq.event_handler) {\n\t\tstruct ib_cq *ibcq = &cq->ibcq;\n\t\tstruct ib_event e;\n\n\t\te.device = ibcq->device;\n\t\te.element.cq = ibcq;\n\t\te.event = type;  \n\t\tibcq->event_handler(&e, ibcq->cq_context);\n\t}\n\tif (cq) {\n\t\tif (refcount_dec_and_test(&cq->refcnt))\n\t\t\tcomplete(&cq->free);\n\t}\n}\n\nstatic void pvrdma_srq_event(struct pvrdma_dev *dev, u32 srqn, int type)\n{\n\tstruct pvrdma_srq *srq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->srq_tbl_lock, flags);\n\tif (dev->srq_tbl)\n\t\tsrq = dev->srq_tbl[srqn % dev->dsr->caps.max_srq];\n\telse\n\t\tsrq = NULL;\n\tif (srq)\n\t\trefcount_inc(&srq->refcnt);\n\tspin_unlock_irqrestore(&dev->srq_tbl_lock, flags);\n\n\tif (srq && srq->ibsrq.event_handler) {\n\t\tstruct ib_srq *ibsrq = &srq->ibsrq;\n\t\tstruct ib_event e;\n\n\t\te.device = ibsrq->device;\n\t\te.element.srq = ibsrq;\n\t\te.event = type;  \n\t\tibsrq->event_handler(&e, ibsrq->srq_context);\n\t}\n\tif (srq) {\n\t\tif (refcount_dec_and_test(&srq->refcnt))\n\t\t\tcomplete(&srq->free);\n\t}\n}\n\nstatic void pvrdma_dispatch_event(struct pvrdma_dev *dev, int port,\n\t\t\t\t  enum ib_event_type event)\n{\n\tstruct ib_event ib_event;\n\n\tmemset(&ib_event, 0, sizeof(ib_event));\n\tib_event.device = &dev->ib_dev;\n\tib_event.element.port_num = port;\n\tib_event.event = event;\n\tib_dispatch_event(&ib_event);\n}\n\nstatic void pvrdma_dev_event(struct pvrdma_dev *dev, u8 port, int type)\n{\n\tif (port < 1 || port > dev->dsr->caps.phys_port_cnt) {\n\t\tdev_warn(&dev->pdev->dev, \"event on port %d\\n\", port);\n\t\treturn;\n\t}\n\n\tpvrdma_dispatch_event(dev, port, type);\n}\n\nstatic inline struct pvrdma_eqe *get_eqe(struct pvrdma_dev *dev, unsigned int i)\n{\n\treturn (struct pvrdma_eqe *)pvrdma_page_dir_get_ptr(\n\t\t\t\t\t&dev->async_pdir,\n\t\t\t\t\tPAGE_SIZE +\n\t\t\t\t\tsizeof(struct pvrdma_eqe) * i);\n}\n\nstatic irqreturn_t pvrdma_intr1_handler(int irq, void *dev_id)\n{\n\tstruct pvrdma_dev *dev = dev_id;\n\tstruct pvrdma_ring *ring = &dev->async_ring_state->rx;\n\tint ring_slots = (dev->dsr->async_ring_pages.num_pages - 1) *\n\t\t\t PAGE_SIZE / sizeof(struct pvrdma_eqe);\n\tunsigned int head;\n\n\tdev_dbg(&dev->pdev->dev, \"interrupt 1 (async event) handler\\n\");\n\n\t \n\tif (!dev->ib_active)\n\t\treturn IRQ_HANDLED;\n\n\twhile (pvrdma_idx_ring_has_data(ring, ring_slots, &head) > 0) {\n\t\tstruct pvrdma_eqe *eqe;\n\n\t\teqe = get_eqe(dev, head);\n\n\t\tswitch (eqe->type) {\n\t\tcase PVRDMA_EVENT_QP_FATAL:\n\t\tcase PVRDMA_EVENT_QP_REQ_ERR:\n\t\tcase PVRDMA_EVENT_QP_ACCESS_ERR:\n\t\tcase PVRDMA_EVENT_COMM_EST:\n\t\tcase PVRDMA_EVENT_SQ_DRAINED:\n\t\tcase PVRDMA_EVENT_PATH_MIG:\n\t\tcase PVRDMA_EVENT_PATH_MIG_ERR:\n\t\tcase PVRDMA_EVENT_QP_LAST_WQE_REACHED:\n\t\t\tpvrdma_qp_event(dev, eqe->info, eqe->type);\n\t\t\tbreak;\n\n\t\tcase PVRDMA_EVENT_CQ_ERR:\n\t\t\tpvrdma_cq_event(dev, eqe->info, eqe->type);\n\t\t\tbreak;\n\n\t\tcase PVRDMA_EVENT_SRQ_ERR:\n\t\tcase PVRDMA_EVENT_SRQ_LIMIT_REACHED:\n\t\t\tpvrdma_srq_event(dev, eqe->info, eqe->type);\n\t\t\tbreak;\n\n\t\tcase PVRDMA_EVENT_PORT_ACTIVE:\n\t\tcase PVRDMA_EVENT_PORT_ERR:\n\t\tcase PVRDMA_EVENT_LID_CHANGE:\n\t\tcase PVRDMA_EVENT_PKEY_CHANGE:\n\t\tcase PVRDMA_EVENT_SM_CHANGE:\n\t\tcase PVRDMA_EVENT_CLIENT_REREGISTER:\n\t\tcase PVRDMA_EVENT_GID_CHANGE:\n\t\t\tpvrdma_dev_event(dev, eqe->info, eqe->type);\n\t\t\tbreak;\n\n\t\tcase PVRDMA_EVENT_DEVICE_FATAL:\n\t\t\tpvrdma_dev_event(dev, 1, eqe->type);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tpvrdma_idx_ring_inc(&ring->cons_head, ring_slots);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic inline struct pvrdma_cqne *get_cqne(struct pvrdma_dev *dev,\n\t\t\t\t\t   unsigned int i)\n{\n\treturn (struct pvrdma_cqne *)pvrdma_page_dir_get_ptr(\n\t\t\t\t\t&dev->cq_pdir,\n\t\t\t\t\tPAGE_SIZE +\n\t\t\t\t\tsizeof(struct pvrdma_cqne) * i);\n}\n\nstatic irqreturn_t pvrdma_intrx_handler(int irq, void *dev_id)\n{\n\tstruct pvrdma_dev *dev = dev_id;\n\tstruct pvrdma_ring *ring = &dev->cq_ring_state->rx;\n\tint ring_slots = (dev->dsr->cq_ring_pages.num_pages - 1) * PAGE_SIZE /\n\t\t\t sizeof(struct pvrdma_cqne);\n\tunsigned int head;\n\n\tdev_dbg(&dev->pdev->dev, \"interrupt x (completion) handler\\n\");\n\n\twhile (pvrdma_idx_ring_has_data(ring, ring_slots, &head) > 0) {\n\t\tstruct pvrdma_cqne *cqne;\n\t\tstruct pvrdma_cq *cq;\n\n\t\tcqne = get_cqne(dev, head);\n\t\tspin_lock(&dev->cq_tbl_lock);\n\t\tcq = dev->cq_tbl[cqne->info % dev->dsr->caps.max_cq];\n\t\tif (cq)\n\t\t\trefcount_inc(&cq->refcnt);\n\t\tspin_unlock(&dev->cq_tbl_lock);\n\n\t\tif (cq && cq->ibcq.comp_handler)\n\t\t\tcq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\n\t\tif (cq) {\n\t\t\tif (refcount_dec_and_test(&cq->refcnt))\n\t\t\t\tcomplete(&cq->free);\n\t\t}\n\t\tpvrdma_idx_ring_inc(&ring->cons_head, ring_slots);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void pvrdma_free_irq(struct pvrdma_dev *dev)\n{\n\tint i;\n\n\tdev_dbg(&dev->pdev->dev, \"freeing interrupts\\n\");\n\tfor (i = 0; i < dev->nr_vectors; i++)\n\t\tfree_irq(pci_irq_vector(dev->pdev, i), dev);\n}\n\nstatic void pvrdma_enable_intrs(struct pvrdma_dev *dev)\n{\n\tdev_dbg(&dev->pdev->dev, \"enable interrupts\\n\");\n\tpvrdma_write_reg(dev, PVRDMA_REG_IMR, 0);\n}\n\nstatic void pvrdma_disable_intrs(struct pvrdma_dev *dev)\n{\n\tdev_dbg(&dev->pdev->dev, \"disable interrupts\\n\");\n\tpvrdma_write_reg(dev, PVRDMA_REG_IMR, ~0);\n}\n\nstatic int pvrdma_alloc_intrs(struct pvrdma_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\tint ret = 0, i;\n\n\tret = pci_alloc_irq_vectors(pdev, 1, PVRDMA_MAX_INTERRUPTS,\n\t\t\tPCI_IRQ_MSIX);\n\tif (ret < 0) {\n\t\tret = pci_alloc_irq_vectors(pdev, 1, 1,\n\t\t\t\tPCI_IRQ_MSI | PCI_IRQ_LEGACY);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tdev->nr_vectors = ret;\n\n\tret = request_irq(pci_irq_vector(dev->pdev, 0), pvrdma_intr0_handler,\n\t\t\tpdev->msix_enabled ? 0 : IRQF_SHARED, DRV_NAME, dev);\n\tif (ret) {\n\t\tdev_err(&dev->pdev->dev,\n\t\t\t\"failed to request interrupt 0\\n\");\n\t\tgoto out_free_vectors;\n\t}\n\n\tfor (i = 1; i < dev->nr_vectors; i++) {\n\t\tret = request_irq(pci_irq_vector(dev->pdev, i),\n\t\t\t\ti == 1 ? pvrdma_intr1_handler :\n\t\t\t\t\t pvrdma_intrx_handler,\n\t\t\t\t0, DRV_NAME, dev);\n\t\tif (ret) {\n\t\t\tdev_err(&dev->pdev->dev,\n\t\t\t\t\"failed to request interrupt %d\\n\", i);\n\t\t\tgoto free_irqs;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_irqs:\n\twhile (--i >= 0)\n\t\tfree_irq(pci_irq_vector(dev->pdev, i), dev);\nout_free_vectors:\n\tpci_free_irq_vectors(pdev);\n\treturn ret;\n}\n\nstatic void pvrdma_free_slots(struct pvrdma_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\n\tif (dev->resp_slot)\n\t\tdma_free_coherent(&pdev->dev, PAGE_SIZE, dev->resp_slot,\n\t\t\t\t  dev->dsr->resp_slot_dma);\n\tif (dev->cmd_slot)\n\t\tdma_free_coherent(&pdev->dev, PAGE_SIZE, dev->cmd_slot,\n\t\t\t\t  dev->dsr->cmd_slot_dma);\n}\n\nstatic int pvrdma_add_gid_at_index(struct pvrdma_dev *dev,\n\t\t\t\t   const union ib_gid *gid,\n\t\t\t\t   u8 gid_type,\n\t\t\t\t   int index)\n{\n\tint ret;\n\tunion pvrdma_cmd_req req;\n\tstruct pvrdma_cmd_create_bind *cmd_bind = &req.create_bind;\n\n\tif (!dev->sgid_tbl) {\n\t\tdev_warn(&dev->pdev->dev, \"sgid table not initialized\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(cmd_bind, 0, sizeof(*cmd_bind));\n\tcmd_bind->hdr.cmd = PVRDMA_CMD_CREATE_BIND;\n\tmemcpy(cmd_bind->new_gid, gid->raw, 16);\n\tcmd_bind->mtu = ib_mtu_enum_to_int(IB_MTU_1024);\n\tcmd_bind->vlan = 0xfff;\n\tcmd_bind->index = index;\n\tcmd_bind->gid_type = gid_type;\n\n\tret = pvrdma_cmd_post(dev, &req, NULL, 0);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not create binding, error: %d\\n\", ret);\n\t\treturn -EFAULT;\n\t}\n\tmemcpy(&dev->sgid_tbl[index], gid, sizeof(*gid));\n\treturn 0;\n}\n\nstatic int pvrdma_add_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tstruct pvrdma_dev *dev = to_vdev(attr->device);\n\n\treturn pvrdma_add_gid_at_index(dev, &attr->gid,\n\t\t\t\t       ib_gid_type_to_pvrdma(attr->gid_type),\n\t\t\t\t       attr->index);\n}\n\nstatic int pvrdma_del_gid_at_index(struct pvrdma_dev *dev, int index)\n{\n\tint ret;\n\tunion pvrdma_cmd_req req;\n\tstruct pvrdma_cmd_destroy_bind *cmd_dest = &req.destroy_bind;\n\n\t \n\tif (!dev->sgid_tbl) {\n\t\tdev_warn(&dev->pdev->dev, \"sgid table not initialized\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(cmd_dest, 0, sizeof(*cmd_dest));\n\tcmd_dest->hdr.cmd = PVRDMA_CMD_DESTROY_BIND;\n\tmemcpy(cmd_dest->dest_gid, &dev->sgid_tbl[index], 16);\n\tcmd_dest->index = index;\n\n\tret = pvrdma_cmd_post(dev, &req, NULL, 0);\n\tif (ret < 0) {\n\t\tdev_warn(&dev->pdev->dev,\n\t\t\t \"could not destroy binding, error: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tmemset(&dev->sgid_tbl[index], 0, 16);\n\treturn 0;\n}\n\nstatic int pvrdma_del_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tstruct pvrdma_dev *dev = to_vdev(attr->device);\n\n\tdev_dbg(&dev->pdev->dev, \"removing gid at index %u from %s\",\n\t\tattr->index, dev->netdev->name);\n\n\treturn pvrdma_del_gid_at_index(dev, attr->index);\n}\n\nstatic void pvrdma_netdevice_event_handle(struct pvrdma_dev *dev,\n\t\t\t\t\t  struct net_device *ndev,\n\t\t\t\t\t  unsigned long event)\n{\n\tstruct pci_dev *pdev_net;\n\tunsigned int slot;\n\n\tswitch (event) {\n\tcase NETDEV_REBOOT:\n\tcase NETDEV_DOWN:\n\t\tpvrdma_dispatch_event(dev, 1, IB_EVENT_PORT_ERR);\n\t\tbreak;\n\tcase NETDEV_UP:\n\t\tpvrdma_write_reg(dev, PVRDMA_REG_CTL,\n\t\t\t\t PVRDMA_DEVICE_CTL_UNQUIESCE);\n\n\t\tmb();\n\n\t\tif (pvrdma_read_reg(dev, PVRDMA_REG_ERR))\n\t\t\tdev_err(&dev->pdev->dev,\n\t\t\t\t\"failed to activate device during link up\\n\");\n\t\telse\n\t\t\tpvrdma_dispatch_event(dev, 1, IB_EVENT_PORT_ACTIVE);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\t\tib_device_set_netdev(&dev->ib_dev, NULL, 1);\n\t\tdev_put(dev->netdev);\n\t\tdev->netdev = NULL;\n\t\tbreak;\n\tcase NETDEV_REGISTER:\n\t\t \n\t\tslot = PCI_SLOT(dev->pdev->devfn);\n\t\tpdev_net = pci_get_slot(dev->pdev->bus,\n\t\t\t\t\tPCI_DEVFN(slot, 0));\n\t\tif ((dev->netdev == NULL) &&\n\t\t    (pci_get_drvdata(pdev_net) == ndev)) {\n\t\t\t \n\t\t\tib_device_set_netdev(&dev->ib_dev, ndev, 1);\n\t\t\tdev->netdev = ndev;\n\t\t\tdev_hold(ndev);\n\t\t}\n\t\tpci_dev_put(pdev_net);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_dbg(&dev->pdev->dev, \"ignore netdevice event %ld on %s\\n\",\n\t\t\tevent, dev_name(&dev->ib_dev.dev));\n\t\tbreak;\n\t}\n}\n\nstatic void pvrdma_netdevice_event_work(struct work_struct *work)\n{\n\tstruct pvrdma_netdevice_work *netdev_work;\n\tstruct pvrdma_dev *dev;\n\n\tnetdev_work = container_of(work, struct pvrdma_netdevice_work, work);\n\n\tmutex_lock(&pvrdma_device_list_lock);\n\tlist_for_each_entry(dev, &pvrdma_device_list, device_link) {\n\t\tif ((netdev_work->event == NETDEV_REGISTER) ||\n\t\t    (dev->netdev == netdev_work->event_netdev)) {\n\t\t\tpvrdma_netdevice_event_handle(dev,\n\t\t\t\t\t\t      netdev_work->event_netdev,\n\t\t\t\t\t\t      netdev_work->event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&pvrdma_device_list_lock);\n\n\tkfree(netdev_work);\n}\n\nstatic int pvrdma_netdevice_event(struct notifier_block *this,\n\t\t\t\t  unsigned long event, void *ptr)\n{\n\tstruct net_device *event_netdev = netdev_notifier_info_to_dev(ptr);\n\tstruct pvrdma_netdevice_work *netdev_work;\n\n\tnetdev_work = kmalloc(sizeof(*netdev_work), GFP_ATOMIC);\n\tif (!netdev_work)\n\t\treturn NOTIFY_BAD;\n\n\tINIT_WORK(&netdev_work->work, pvrdma_netdevice_event_work);\n\tnetdev_work->event_netdev = event_netdev;\n\tnetdev_work->event = event;\n\tqueue_work(event_wq, &netdev_work->work);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic int pvrdma_pci_probe(struct pci_dev *pdev,\n\t\t\t    const struct pci_device_id *id)\n{\n\tstruct pci_dev *pdev_net;\n\tstruct pvrdma_dev *dev;\n\tint ret;\n\tunsigned long start;\n\tunsigned long len;\n\tdma_addr_t slot_dma = 0;\n\n\tdev_dbg(&pdev->dev, \"initializing driver %s\\n\", pci_name(pdev));\n\n\t \n\tdev = ib_alloc_device(pvrdma_dev, ib_dev);\n\tif (!dev) {\n\t\tdev_err(&pdev->dev, \"failed to allocate IB device\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&pvrdma_device_list_lock);\n\tlist_add(&dev->device_link, &pvrdma_device_list);\n\tmutex_unlock(&pvrdma_device_list_lock);\n\n\tret = pvrdma_init_device(dev);\n\tif (ret)\n\t\tgoto err_free_device;\n\n\tdev->pdev = pdev;\n\tpci_set_drvdata(pdev, dev);\n\n\tret = pci_enable_device(pdev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"cannot enable PCI device\\n\");\n\t\tgoto err_free_device;\n\t}\n\n\tdev_dbg(&pdev->dev, \"PCI resource flags BAR0 %#lx\\n\",\n\t\tpci_resource_flags(pdev, 0));\n\tdev_dbg(&pdev->dev, \"PCI resource len %#llx\\n\",\n\t\t(unsigned long long)pci_resource_len(pdev, 0));\n\tdev_dbg(&pdev->dev, \"PCI resource start %#llx\\n\",\n\t\t(unsigned long long)pci_resource_start(pdev, 0));\n\tdev_dbg(&pdev->dev, \"PCI resource flags BAR1 %#lx\\n\",\n\t\tpci_resource_flags(pdev, 1));\n\tdev_dbg(&pdev->dev, \"PCI resource len %#llx\\n\",\n\t\t(unsigned long long)pci_resource_len(pdev, 1));\n\tdev_dbg(&pdev->dev, \"PCI resource start %#llx\\n\",\n\t\t(unsigned long long)pci_resource_start(pdev, 1));\n\n\tif (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM) ||\n\t    !(pci_resource_flags(pdev, 1) & IORESOURCE_MEM)) {\n\t\tdev_err(&pdev->dev, \"PCI BAR region not MMIO\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_disable_pdev;\n\t}\n\n\tret = pci_request_regions(pdev, DRV_NAME);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"cannot request PCI resources\\n\");\n\t\tgoto err_disable_pdev;\n\t}\n\n\t \n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"dma_set_mask failed\\n\");\n\t\tgoto err_free_resource;\n\t}\n\tdma_set_max_seg_size(&pdev->dev, UINT_MAX);\n\tpci_set_master(pdev);\n\n\t \n\tstart = pci_resource_start(dev->pdev, PVRDMA_PCI_RESOURCE_REG);\n\tlen = pci_resource_len(dev->pdev, PVRDMA_PCI_RESOURCE_REG);\n\tdev->regs = ioremap(start, len);\n\tif (!dev->regs) {\n\t\tdev_err(&pdev->dev, \"register mapping failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_free_resource;\n\t}\n\n\t \n\tdev->driver_uar.index = 0;\n\tdev->driver_uar.pfn =\n\t\tpci_resource_start(dev->pdev, PVRDMA_PCI_RESOURCE_UAR) >>\n\t\tPAGE_SHIFT;\n\tdev->driver_uar.map =\n\t\tioremap(dev->driver_uar.pfn << PAGE_SHIFT, PAGE_SIZE);\n\tif (!dev->driver_uar.map) {\n\t\tdev_err(&pdev->dev, \"failed to remap UAR pages\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_unmap_regs;\n\t}\n\n\tdev->dsr_version = pvrdma_read_reg(dev, PVRDMA_REG_VERSION);\n\tdev_info(&pdev->dev, \"device version %d, driver version %d\\n\",\n\t\t dev->dsr_version, PVRDMA_VERSION);\n\n\tdev->dsr = dma_alloc_coherent(&pdev->dev, sizeof(*dev->dsr),\n\t\t\t\t      &dev->dsrbase, GFP_KERNEL);\n\tif (!dev->dsr) {\n\t\tdev_err(&pdev->dev, \"failed to allocate shared region\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_uar_unmap;\n\t}\n\n\t \n\tdev->dsr->driver_version = PVRDMA_VERSION;\n\tdev->dsr->gos_info.gos_bits = sizeof(void *) == 4 ?\n\t\tPVRDMA_GOS_BITS_32 :\n\t\tPVRDMA_GOS_BITS_64;\n\tdev->dsr->gos_info.gos_type = PVRDMA_GOS_TYPE_LINUX;\n\tdev->dsr->gos_info.gos_ver = 1;\n\n\tif (dev->dsr_version < PVRDMA_PPN64_VERSION)\n\t\tdev->dsr->uar_pfn = dev->driver_uar.pfn;\n\telse\n\t\tdev->dsr->uar_pfn64 = dev->driver_uar.pfn;\n\n\t \n\tdev->cmd_slot = dma_alloc_coherent(&pdev->dev, PAGE_SIZE,\n\t\t\t\t\t   &slot_dma, GFP_KERNEL);\n\tif (!dev->cmd_slot) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_dsr;\n\t}\n\n\tdev->dsr->cmd_slot_dma = (u64)slot_dma;\n\n\t \n\tdev->resp_slot = dma_alloc_coherent(&pdev->dev, PAGE_SIZE,\n\t\t\t\t\t    &slot_dma, GFP_KERNEL);\n\tif (!dev->resp_slot) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_slots;\n\t}\n\n\tdev->dsr->resp_slot_dma = (u64)slot_dma;\n\n\t \n\tdev->dsr->async_ring_pages.num_pages = PVRDMA_NUM_RING_PAGES;\n\tret = pvrdma_page_dir_init(dev, &dev->async_pdir,\n\t\t\t\t   dev->dsr->async_ring_pages.num_pages, true);\n\tif (ret)\n\t\tgoto err_free_slots;\n\tdev->async_ring_state = dev->async_pdir.pages[0];\n\tdev->dsr->async_ring_pages.pdir_dma = dev->async_pdir.dir_dma;\n\n\t \n\tdev->dsr->cq_ring_pages.num_pages = PVRDMA_NUM_RING_PAGES;\n\tret = pvrdma_page_dir_init(dev, &dev->cq_pdir,\n\t\t\t\t   dev->dsr->cq_ring_pages.num_pages, true);\n\tif (ret)\n\t\tgoto err_free_async_ring;\n\tdev->cq_ring_state = dev->cq_pdir.pages[0];\n\tdev->dsr->cq_ring_pages.pdir_dma = dev->cq_pdir.dir_dma;\n\n\t \n\n\tpvrdma_write_reg(dev, PVRDMA_REG_DSRLOW, (u32)dev->dsrbase);\n\tpvrdma_write_reg(dev, PVRDMA_REG_DSRHIGH,\n\t\t\t (u32)((u64)(dev->dsrbase) >> 32));\n\n\t \n\tmb();\n\n\t \n\tif (!PVRDMA_SUPPORTED(dev)) {\n\t\tdev_err(&pdev->dev, \"driver needs RoCE v1 or v2 support\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free_cq_ring;\n\t}\n\n\t \n\tpdev_net = pci_get_slot(pdev->bus, PCI_DEVFN(PCI_SLOT(pdev->devfn), 0));\n\tif (!pdev_net) {\n\t\tdev_err(&pdev->dev, \"failed to find paired net device\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err_free_cq_ring;\n\t}\n\n\tif (pdev_net->vendor != PCI_VENDOR_ID_VMWARE ||\n\t    pdev_net->device != PCI_DEVICE_ID_VMWARE_VMXNET3) {\n\t\tdev_err(&pdev->dev, \"failed to find paired vmxnet3 device\\n\");\n\t\tpci_dev_put(pdev_net);\n\t\tret = -ENODEV;\n\t\tgoto err_free_cq_ring;\n\t}\n\n\tdev->netdev = pci_get_drvdata(pdev_net);\n\tpci_dev_put(pdev_net);\n\tif (!dev->netdev) {\n\t\tdev_err(&pdev->dev, \"failed to get vmxnet3 device\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err_free_cq_ring;\n\t}\n\tdev_hold(dev->netdev);\n\n\tdev_info(&pdev->dev, \"paired device to %s\\n\", dev->netdev->name);\n\n\t \n\tret = pvrdma_alloc_intrs(dev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to allocate interrupts\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_free_cq_ring;\n\t}\n\n\t \n\tret = pvrdma_uar_table_init(dev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to allocate UAR table\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_free_intrs;\n\t}\n\n\t \n\tdev->sgid_tbl = kcalloc(dev->dsr->caps.gid_tbl_len,\n\t\t\t\tsizeof(union ib_gid), GFP_KERNEL);\n\tif (!dev->sgid_tbl) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_uar_table;\n\t}\n\tdev_dbg(&pdev->dev, \"gid table len %d\\n\", dev->dsr->caps.gid_tbl_len);\n\n\tpvrdma_enable_intrs(dev);\n\n\t \n\tpvrdma_write_reg(dev, PVRDMA_REG_CTL, PVRDMA_DEVICE_CTL_ACTIVATE);\n\n\t \n\tmb();\n\n\t \n\tret = pvrdma_read_reg(dev, PVRDMA_REG_ERR);\n\tif (ret != 0) {\n\t\tdev_err(&pdev->dev, \"failed to activate device\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_disable_intr;\n\t}\n\n\t \n\tret = pvrdma_register_device(dev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register IB device\\n\");\n\t\tgoto err_disable_intr;\n\t}\n\n\tdev->nb_netdev.notifier_call = pvrdma_netdevice_event;\n\tret = register_netdevice_notifier(&dev->nb_netdev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register netdevice events\\n\");\n\t\tgoto err_unreg_ibdev;\n\t}\n\n\tdev_info(&pdev->dev, \"attached to device\\n\");\n\treturn 0;\n\nerr_unreg_ibdev:\n\tib_unregister_device(&dev->ib_dev);\nerr_disable_intr:\n\tpvrdma_disable_intrs(dev);\n\tkfree(dev->sgid_tbl);\nerr_free_uar_table:\n\tpvrdma_uar_table_cleanup(dev);\nerr_free_intrs:\n\tpvrdma_free_irq(dev);\n\tpci_free_irq_vectors(pdev);\nerr_free_cq_ring:\n\tif (dev->netdev) {\n\t\tdev_put(dev->netdev);\n\t\tdev->netdev = NULL;\n\t}\n\tpvrdma_page_dir_cleanup(dev, &dev->cq_pdir);\nerr_free_async_ring:\n\tpvrdma_page_dir_cleanup(dev, &dev->async_pdir);\nerr_free_slots:\n\tpvrdma_free_slots(dev);\nerr_free_dsr:\n\tdma_free_coherent(&pdev->dev, sizeof(*dev->dsr), dev->dsr,\n\t\t\t  dev->dsrbase);\nerr_uar_unmap:\n\tiounmap(dev->driver_uar.map);\nerr_unmap_regs:\n\tiounmap(dev->regs);\nerr_free_resource:\n\tpci_release_regions(pdev);\nerr_disable_pdev:\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\nerr_free_device:\n\tmutex_lock(&pvrdma_device_list_lock);\n\tlist_del(&dev->device_link);\n\tmutex_unlock(&pvrdma_device_list_lock);\n\tib_dealloc_device(&dev->ib_dev);\n\treturn ret;\n}\n\nstatic void pvrdma_pci_remove(struct pci_dev *pdev)\n{\n\tstruct pvrdma_dev *dev = pci_get_drvdata(pdev);\n\n\tif (!dev)\n\t\treturn;\n\n\tdev_info(&pdev->dev, \"detaching from device\\n\");\n\n\tunregister_netdevice_notifier(&dev->nb_netdev);\n\tdev->nb_netdev.notifier_call = NULL;\n\n\tflush_workqueue(event_wq);\n\n\tif (dev->netdev) {\n\t\tdev_put(dev->netdev);\n\t\tdev->netdev = NULL;\n\t}\n\n\t \n\tib_unregister_device(&dev->ib_dev);\n\n\tmutex_lock(&pvrdma_device_list_lock);\n\tlist_del(&dev->device_link);\n\tmutex_unlock(&pvrdma_device_list_lock);\n\n\tpvrdma_disable_intrs(dev);\n\tpvrdma_free_irq(dev);\n\tpci_free_irq_vectors(pdev);\n\n\t \n\tpvrdma_write_reg(dev, PVRDMA_REG_CTL, PVRDMA_DEVICE_CTL_RESET);\n\tpvrdma_page_dir_cleanup(dev, &dev->cq_pdir);\n\tpvrdma_page_dir_cleanup(dev, &dev->async_pdir);\n\tpvrdma_free_slots(dev);\n\tdma_free_coherent(&pdev->dev, sizeof(*dev->dsr), dev->dsr,\n\t\t\t  dev->dsrbase);\n\n\tiounmap(dev->regs);\n\tkfree(dev->sgid_tbl);\n\tkfree(dev->cq_tbl);\n\tkfree(dev->srq_tbl);\n\tkfree(dev->qp_tbl);\n\tpvrdma_uar_table_cleanup(dev);\n\tiounmap(dev->driver_uar.map);\n\n\tib_dealloc_device(&dev->ib_dev);\n\n\t \n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n}\n\nstatic const struct pci_device_id pvrdma_pci_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_VMWARE, PCI_DEVICE_ID_VMWARE_PVRDMA), },\n\t{ 0 },\n};\n\nMODULE_DEVICE_TABLE(pci, pvrdma_pci_table);\n\nstatic struct pci_driver pvrdma_driver = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= pvrdma_pci_table,\n\t.probe\t\t= pvrdma_pci_probe,\n\t.remove\t\t= pvrdma_pci_remove,\n};\n\nstatic int __init pvrdma_init(void)\n{\n\tint err;\n\n\tevent_wq = alloc_ordered_workqueue(\"pvrdma_event_wq\", WQ_MEM_RECLAIM);\n\tif (!event_wq)\n\t\treturn -ENOMEM;\n\n\terr = pci_register_driver(&pvrdma_driver);\n\tif (err)\n\t\tdestroy_workqueue(event_wq);\n\n\treturn err;\n}\n\nstatic void __exit pvrdma_cleanup(void)\n{\n\tpci_unregister_driver(&pvrdma_driver);\n\n\tdestroy_workqueue(event_wq);\n}\n\nmodule_init(pvrdma_init);\nmodule_exit(pvrdma_cleanup);\n\nMODULE_AUTHOR(\"VMware, Inc\");\nMODULE_DESCRIPTION(\"VMware Paravirtual RDMA driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}