{
  "module_name": "hw.c",
  "hash_id": "f90c17e8563d529b83deed75e34bb7df292e78de9e0e3a250544b18a29c8dacd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/irdma/hw.c",
  "human_readable_source": "\n \n#include \"main.h\"\n\nstatic struct irdma_rsrc_limits rsrc_limits_table[] = {\n\t[0] = {\n\t\t.qplimit = SZ_128,\n\t},\n\t[1] = {\n\t\t.qplimit = SZ_1K,\n\t},\n\t[2] = {\n\t\t.qplimit = SZ_2K,\n\t},\n\t[3] = {\n\t\t.qplimit = SZ_4K,\n\t},\n\t[4] = {\n\t\t.qplimit = SZ_16K,\n\t},\n\t[5] = {\n\t\t.qplimit = SZ_64K,\n\t},\n\t[6] = {\n\t\t.qplimit = SZ_128K,\n\t},\n\t[7] = {\n\t\t.qplimit = SZ_256K,\n\t},\n};\n\n \nstatic enum irdma_hmc_rsrc_type iw_hmc_obj_types[] = {\n\tIRDMA_HMC_IW_QP,\n\tIRDMA_HMC_IW_CQ,\n\tIRDMA_HMC_IW_HTE,\n\tIRDMA_HMC_IW_ARP,\n\tIRDMA_HMC_IW_APBVT_ENTRY,\n\tIRDMA_HMC_IW_MR,\n\tIRDMA_HMC_IW_XF,\n\tIRDMA_HMC_IW_XFFL,\n\tIRDMA_HMC_IW_Q1,\n\tIRDMA_HMC_IW_Q1FL,\n\tIRDMA_HMC_IW_PBLE,\n\tIRDMA_HMC_IW_TIMER,\n\tIRDMA_HMC_IW_FSIMC,\n\tIRDMA_HMC_IW_FSIAV,\n\tIRDMA_HMC_IW_RRF,\n\tIRDMA_HMC_IW_RRFFL,\n\tIRDMA_HMC_IW_HDR,\n\tIRDMA_HMC_IW_MD,\n\tIRDMA_HMC_IW_OOISC,\n\tIRDMA_HMC_IW_OOISCFFL,\n};\n\n \nstatic void irdma_iwarp_ce_handler(struct irdma_sc_cq *iwcq)\n{\n\tstruct irdma_cq *cq = iwcq->back_cq;\n\n\tif (!cq->user_mode)\n\t\tatomic_set(&cq->armed, 0);\n\tif (cq->ibcq.comp_handler)\n\t\tcq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\n}\n\n \nstatic void irdma_puda_ce_handler(struct irdma_pci_f *rf,\n\t\t\t\t  struct irdma_sc_cq *cq)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tu32 compl_error;\n\tint status;\n\n\tdo {\n\t\tstatus = irdma_puda_poll_cmpl(dev, cq, &compl_error);\n\t\tif (status == -ENOENT)\n\t\t\tbreak;\n\t\tif (status) {\n\t\t\tibdev_dbg(to_ibdev(dev), \"ERR: puda status = %d\\n\", status);\n\t\t\tbreak;\n\t\t}\n\t\tif (compl_error) {\n\t\t\tibdev_dbg(to_ibdev(dev), \"ERR: puda compl_err  =0x%x\\n\",\n\t\t\t\t  compl_error);\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\n\tirdma_sc_ccq_arm(cq);\n}\n\n \nstatic void irdma_process_ceq(struct irdma_pci_f *rf, struct irdma_ceq *ceq)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_sc_ceq *sc_ceq;\n\tstruct irdma_sc_cq *cq;\n\tunsigned long flags;\n\n\tsc_ceq = &ceq->sc_ceq;\n\tdo {\n\t\tspin_lock_irqsave(&ceq->ce_lock, flags);\n\t\tcq = irdma_sc_process_ceq(dev, sc_ceq);\n\t\tif (!cq) {\n\t\t\tspin_unlock_irqrestore(&ceq->ce_lock, flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cq->cq_type == IRDMA_CQ_TYPE_IWARP)\n\t\t\tirdma_iwarp_ce_handler(cq);\n\n\t\tspin_unlock_irqrestore(&ceq->ce_lock, flags);\n\n\t\tif (cq->cq_type == IRDMA_CQ_TYPE_CQP)\n\t\t\tqueue_work(rf->cqp_cmpl_wq, &rf->cqp_cmpl_work);\n\t\telse if (cq->cq_type == IRDMA_CQ_TYPE_ILQ ||\n\t\t\t cq->cq_type == IRDMA_CQ_TYPE_IEQ)\n\t\t\tirdma_puda_ce_handler(rf, cq);\n\t} while (1);\n}\n\nstatic void irdma_set_flush_fields(struct irdma_sc_qp *qp,\n\t\t\t\t   struct irdma_aeqe_info *info)\n{\n\tqp->sq_flush_code = info->sq;\n\tqp->rq_flush_code = info->rq;\n\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\n\tswitch (info->ae_id) {\n\tcase IRDMA_AE_AMP_BOUNDS_VIOLATION:\n\tcase IRDMA_AE_AMP_INVALID_STAG:\n\tcase IRDMA_AE_AMP_RIGHTS_VIOLATION:\n\tcase IRDMA_AE_AMP_UNALLOCATED_STAG:\n\tcase IRDMA_AE_AMP_BAD_PD:\n\tcase IRDMA_AE_AMP_BAD_QP:\n\tcase IRDMA_AE_AMP_BAD_STAG_KEY:\n\tcase IRDMA_AE_AMP_BAD_STAG_INDEX:\n\tcase IRDMA_AE_AMP_TO_WRAP:\n\tcase IRDMA_AE_PRIV_OPERATION_DENIED:\n\t\tqp->flush_code = FLUSH_PROT_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_ACCESS_ERR;\n\t\tbreak;\n\tcase IRDMA_AE_UDA_XMIT_BAD_PD:\n\tcase IRDMA_AE_WQE_UNEXPECTED_OPCODE:\n\t\tqp->flush_code = FLUSH_LOC_QP_OP_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\tcase IRDMA_AE_UDA_XMIT_DGRAM_TOO_LONG:\n\tcase IRDMA_AE_UDA_XMIT_DGRAM_TOO_SHORT:\n\tcase IRDMA_AE_UDA_L4LEN_INVALID:\n\tcase IRDMA_AE_DDP_UBE_INVALID_MO:\n\tcase IRDMA_AE_DDP_UBE_DDP_MESSAGE_TOO_LONG_FOR_AVAILABLE_BUFFER:\n\t\tqp->flush_code = FLUSH_LOC_LEN_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\tcase IRDMA_AE_AMP_INVALIDATE_NO_REMOTE_ACCESS_RIGHTS:\n\tcase IRDMA_AE_IB_REMOTE_ACCESS_ERROR:\n\t\tqp->flush_code = FLUSH_REM_ACCESS_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_ACCESS_ERR;\n\t\tbreak;\n\tcase IRDMA_AE_LLP_SEGMENT_TOO_SMALL:\n\tcase IRDMA_AE_LLP_RECEIVED_MPA_CRC_ERROR:\n\tcase IRDMA_AE_ROCE_RSP_LENGTH_ERROR:\n\tcase IRDMA_AE_IB_REMOTE_OP_ERROR:\n\t\tqp->flush_code = FLUSH_REM_OP_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\tcase IRDMA_AE_LCE_QP_CATASTROPHIC:\n\t\tqp->flush_code = FLUSH_FATAL_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\tcase IRDMA_AE_IB_RREQ_AND_Q1_FULL:\n\t\tqp->flush_code = FLUSH_GENERAL_ERR;\n\t\tbreak;\n\tcase IRDMA_AE_LLP_TOO_MANY_RETRIES:\n\t\tqp->flush_code = FLUSH_RETRY_EXC_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\tcase IRDMA_AE_AMP_MWBIND_INVALID_RIGHTS:\n\tcase IRDMA_AE_AMP_MWBIND_BIND_DISABLED:\n\tcase IRDMA_AE_AMP_MWBIND_INVALID_BOUNDS:\n\tcase IRDMA_AE_AMP_MWBIND_VALID_STAG:\n\t\tqp->flush_code = FLUSH_MW_BIND_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_ACCESS_ERR;\n\t\tbreak;\n\tcase IRDMA_AE_IB_INVALID_REQUEST:\n\t\tqp->flush_code = FLUSH_REM_INV_REQ_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_REQ_ERR;\n\t\tbreak;\n\tdefault:\n\t\tqp->flush_code = FLUSH_GENERAL_ERR;\n\t\tqp->event_type = IRDMA_QP_EVENT_CATASTROPHIC;\n\t\tbreak;\n\t}\n}\n\n \nstatic void irdma_process_aeq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_aeq *aeq = &rf->aeq;\n\tstruct irdma_sc_aeq *sc_aeq = &aeq->sc_aeq;\n\tstruct irdma_aeqe_info aeinfo;\n\tstruct irdma_aeqe_info *info = &aeinfo;\n\tint ret;\n\tstruct irdma_qp *iwqp = NULL;\n\tstruct irdma_cq *iwcq = NULL;\n\tstruct irdma_sc_qp *qp = NULL;\n\tstruct irdma_qp_host_ctx_info *ctx_info = NULL;\n\tstruct irdma_device *iwdev = rf->iwdev;\n\tunsigned long flags;\n\n\tu32 aeqcnt = 0;\n\n\tif (!sc_aeq->size)\n\t\treturn;\n\n\tdo {\n\t\tmemset(info, 0, sizeof(*info));\n\t\tret = irdma_sc_get_next_aeqe(sc_aeq, info);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\taeqcnt++;\n\t\tibdev_dbg(&iwdev->ibdev,\n\t\t\t  \"AEQ: ae_id = 0x%x bool qp=%d qp_id = %d tcp_state=%d iwarp_state=%d ae_src=%d\\n\",\n\t\t\t  info->ae_id, info->qp, info->qp_cq_id, info->tcp_state,\n\t\t\t  info->iwarp_state, info->ae_src);\n\n\t\tif (info->qp) {\n\t\t\tspin_lock_irqsave(&rf->qptable_lock, flags);\n\t\t\tiwqp = rf->qp_table[info->qp_cq_id];\n\t\t\tif (!iwqp) {\n\t\t\t\tspin_unlock_irqrestore(&rf->qptable_lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\tif (info->ae_id == IRDMA_AE_QP_SUSPEND_COMPLETE) {\n\t\t\t\t\tatomic_dec(&iwdev->vsi.qp_suspend_reqs);\n\t\t\t\t\twake_up(&iwdev->suspend_wq);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tibdev_dbg(&iwdev->ibdev, \"AEQ: qp_id %d is already freed\\n\",\n\t\t\t\t\t  info->qp_cq_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tirdma_qp_add_ref(&iwqp->ibqp);\n\t\t\tspin_unlock_irqrestore(&rf->qptable_lock, flags);\n\t\t\tqp = &iwqp->sc_qp;\n\t\t\tspin_lock_irqsave(&iwqp->lock, flags);\n\t\t\tiwqp->hw_tcp_state = info->tcp_state;\n\t\t\tiwqp->hw_iwarp_state = info->iwarp_state;\n\t\t\tif (info->ae_id != IRDMA_AE_QP_SUSPEND_COMPLETE)\n\t\t\t\tiwqp->last_aeq = info->ae_id;\n\t\t\tspin_unlock_irqrestore(&iwqp->lock, flags);\n\t\t\tctx_info = &iwqp->ctx_info;\n\t\t} else {\n\t\t\tif (info->ae_id != IRDMA_AE_CQ_OPERATION_ERROR)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (info->ae_id) {\n\t\t\tstruct irdma_cm_node *cm_node;\n\t\tcase IRDMA_AE_LLP_CONNECTION_ESTABLISHED:\n\t\t\tcm_node = iwqp->cm_node;\n\t\t\tif (cm_node->accept_pend) {\n\t\t\t\tatomic_dec(&cm_node->listener->pend_accepts_cnt);\n\t\t\t\tcm_node->accept_pend = 0;\n\t\t\t}\n\t\t\tiwqp->rts_ae_rcvd = 1;\n\t\t\twake_up_interruptible(&iwqp->waitq);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_LLP_FIN_RECEIVED:\n\t\tcase IRDMA_AE_RDMAP_ROE_BAD_LLP_CLOSE:\n\t\t\tif (qp->term_flags)\n\t\t\t\tbreak;\n\t\t\tif (atomic_inc_return(&iwqp->close_timer_started) == 1) {\n\t\t\t\tiwqp->hw_tcp_state = IRDMA_TCP_STATE_CLOSE_WAIT;\n\t\t\t\tif (iwqp->hw_tcp_state == IRDMA_TCP_STATE_CLOSE_WAIT &&\n\t\t\t\t    iwqp->ibqp_state == IB_QPS_RTS) {\n\t\t\t\t\tirdma_next_iw_state(iwqp,\n\t\t\t\t\t\t\t    IRDMA_QP_STATE_CLOSING,\n\t\t\t\t\t\t\t    0, 0, 0);\n\t\t\t\t\tirdma_cm_disconn(iwqp);\n\t\t\t\t}\n\t\t\t\tirdma_schedule_cm_timer(iwqp->cm_node,\n\t\t\t\t\t\t\t(struct irdma_puda_buf *)iwqp,\n\t\t\t\t\t\t\tIRDMA_TIMER_TYPE_CLOSE,\n\t\t\t\t\t\t\t1, 0);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase IRDMA_AE_LLP_CLOSE_COMPLETE:\n\t\t\tif (qp->term_flags)\n\t\t\t\tirdma_terminate_done(qp, 0);\n\t\t\telse\n\t\t\t\tirdma_cm_disconn(iwqp);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_BAD_CLOSE:\n\t\tcase IRDMA_AE_RESET_SENT:\n\t\t\tirdma_next_iw_state(iwqp, IRDMA_QP_STATE_ERROR, 1, 0,\n\t\t\t\t\t    0);\n\t\t\tirdma_cm_disconn(iwqp);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_LLP_CONNECTION_RESET:\n\t\t\tif (atomic_read(&iwqp->close_timer_started))\n\t\t\t\tbreak;\n\t\t\tirdma_cm_disconn(iwqp);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_QP_SUSPEND_COMPLETE:\n\t\t\tif (iwqp->iwdev->vsi.tc_change_pending) {\n\t\t\t\tif (!atomic_dec_return(&qp->vsi->qp_suspend_reqs))\n\t\t\t\t\twake_up(&iwqp->iwdev->suspend_wq);\n\t\t\t}\n\t\t\tif (iwqp->suspend_pending) {\n\t\t\t\tiwqp->suspend_pending = false;\n\t\t\t\twake_up(&iwqp->iwdev->suspend_wq);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase IRDMA_AE_TERMINATE_SENT:\n\t\t\tirdma_terminate_send_fin(qp);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_LLP_TERMINATE_RECEIVED:\n\t\t\tirdma_terminate_received(qp, info);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_CQ_OPERATION_ERROR:\n\t\t\tibdev_err(&iwdev->ibdev,\n\t\t\t\t  \"Processing an iWARP related AE for CQ misc = 0x%04X\\n\",\n\t\t\t\t  info->ae_id);\n\n\t\t\tspin_lock_irqsave(&rf->cqtable_lock, flags);\n\t\t\tiwcq = rf->cq_table[info->qp_cq_id];\n\t\t\tif (!iwcq) {\n\t\t\t\tspin_unlock_irqrestore(&rf->cqtable_lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t\t\t  \"cq_id %d is already freed\\n\", info->qp_cq_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tirdma_cq_add_ref(&iwcq->ibcq);\n\t\t\tspin_unlock_irqrestore(&rf->cqtable_lock, flags);\n\n\t\t\tif (iwcq->ibcq.event_handler) {\n\t\t\t\tstruct ib_event ibevent;\n\n\t\t\t\tibevent.device = iwcq->ibcq.device;\n\t\t\t\tibevent.event = IB_EVENT_CQ_ERR;\n\t\t\t\tibevent.element.cq = &iwcq->ibcq;\n\t\t\t\tiwcq->ibcq.event_handler(&ibevent,\n\t\t\t\t\t\t\t iwcq->ibcq.cq_context);\n\t\t\t}\n\t\t\tirdma_cq_rem_ref(&iwcq->ibcq);\n\t\t\tbreak;\n\t\tcase IRDMA_AE_RESET_NOT_SENT:\n\t\tcase IRDMA_AE_LLP_DOUBT_REACHABILITY:\n\t\tcase IRDMA_AE_RESOURCE_EXHAUSTION:\n\t\t\tbreak;\n\t\tcase IRDMA_AE_PRIV_OPERATION_DENIED:\n\t\tcase IRDMA_AE_STAG_ZERO_INVALID:\n\t\tcase IRDMA_AE_IB_RREQ_AND_Q1_FULL:\n\t\tcase IRDMA_AE_DDP_UBE_INVALID_DDP_VERSION:\n\t\tcase IRDMA_AE_DDP_UBE_INVALID_MO:\n\t\tcase IRDMA_AE_DDP_UBE_INVALID_QN:\n\t\tcase IRDMA_AE_DDP_NO_L_BIT:\n\t\tcase IRDMA_AE_RDMAP_ROE_INVALID_RDMAP_VERSION:\n\t\tcase IRDMA_AE_RDMAP_ROE_UNEXPECTED_OPCODE:\n\t\tcase IRDMA_AE_ROE_INVALID_RDMA_READ_REQUEST:\n\t\tcase IRDMA_AE_ROE_INVALID_RDMA_WRITE_OR_READ_RESP:\n\t\tcase IRDMA_AE_INVALID_ARP_ENTRY:\n\t\tcase IRDMA_AE_INVALID_TCP_OPTION_RCVD:\n\t\tcase IRDMA_AE_STALE_ARP_ENTRY:\n\t\tcase IRDMA_AE_LLP_RECEIVED_MPA_CRC_ERROR:\n\t\tcase IRDMA_AE_LLP_SEGMENT_TOO_SMALL:\n\t\tcase IRDMA_AE_LLP_SYN_RECEIVED:\n\t\tcase IRDMA_AE_LLP_TOO_MANY_RETRIES:\n\t\tcase IRDMA_AE_LCE_QP_CATASTROPHIC:\n\t\tcase IRDMA_AE_LCE_FUNCTION_CATASTROPHIC:\n\t\tcase IRDMA_AE_LCE_CQ_CATASTROPHIC:\n\t\tcase IRDMA_AE_UDA_XMIT_DGRAM_TOO_LONG:\n\t\tdefault:\n\t\t\tibdev_err(&iwdev->ibdev, \"abnormal ae_id = 0x%x bool qp=%d qp_id = %d, ae_src=%d\\n\",\n\t\t\t\t  info->ae_id, info->qp, info->qp_cq_id, info->ae_src);\n\t\t\tif (rdma_protocol_roce(&iwdev->ibdev, 1)) {\n\t\t\t\tctx_info->roce_info->err_rq_idx_valid = info->rq;\n\t\t\t\tif (info->rq) {\n\t\t\t\t\tctx_info->roce_info->err_rq_idx = info->wqe_idx;\n\t\t\t\t\tirdma_sc_qp_setctx_roce(&iwqp->sc_qp, iwqp->host_ctx.va,\n\t\t\t\t\t\t\t\tctx_info);\n\t\t\t\t}\n\t\t\t\tirdma_set_flush_fields(qp, info);\n\t\t\t\tirdma_cm_disconn(iwqp);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tctx_info->iwarp_info->err_rq_idx_valid = info->rq;\n\t\t\tif (info->rq) {\n\t\t\t\tctx_info->iwarp_info->err_rq_idx = info->wqe_idx;\n\t\t\t\tctx_info->tcp_info_valid = false;\n\t\t\t\tctx_info->iwarp_info_valid = true;\n\t\t\t\tirdma_sc_qp_setctx(&iwqp->sc_qp, iwqp->host_ctx.va,\n\t\t\t\t\t\t   ctx_info);\n\t\t\t}\n\t\t\tif (iwqp->hw_iwarp_state != IRDMA_QP_STATE_RTS &&\n\t\t\t    iwqp->hw_iwarp_state != IRDMA_QP_STATE_TERMINATE) {\n\t\t\t\tirdma_next_iw_state(iwqp, IRDMA_QP_STATE_ERROR, 1, 0, 0);\n\t\t\t\tirdma_cm_disconn(iwqp);\n\t\t\t} else {\n\t\t\t\tirdma_terminate_connection(qp, info);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (info->qp)\n\t\t\tirdma_qp_rem_ref(&iwqp->ibqp);\n\t} while (1);\n\n\tif (aeqcnt)\n\t\tirdma_sc_repost_aeq_entries(dev, aeqcnt);\n}\n\n \nstatic void irdma_ena_intr(struct irdma_sc_dev *dev, u32 msix_id)\n{\n\tdev->irq_ops->irdma_en_irq(dev, msix_id);\n}\n\n \nstatic void irdma_dpc(struct tasklet_struct *t)\n{\n\tstruct irdma_pci_f *rf = from_tasklet(rf, t, dpc_tasklet);\n\n\tif (rf->msix_shared)\n\t\tirdma_process_ceq(rf, rf->ceqlist);\n\tirdma_process_aeq(rf);\n\tirdma_ena_intr(&rf->sc_dev, rf->iw_msixtbl[0].idx);\n}\n\n \nstatic void irdma_ceq_dpc(struct tasklet_struct *t)\n{\n\tstruct irdma_ceq *iwceq = from_tasklet(iwceq, t, dpc_tasklet);\n\tstruct irdma_pci_f *rf = iwceq->rf;\n\n\tirdma_process_ceq(rf, iwceq);\n\tirdma_ena_intr(&rf->sc_dev, iwceq->msix_idx);\n}\n\n \nstatic int irdma_save_msix_info(struct irdma_pci_f *rf)\n{\n\tstruct irdma_qvlist_info *iw_qvlist;\n\tstruct irdma_qv_info *iw_qvinfo;\n\tstruct msix_entry *pmsix;\n\tu32 ceq_idx;\n\tu32 i;\n\tsize_t size;\n\n\tif (!rf->msix_count)\n\t\treturn -EINVAL;\n\n\tsize = sizeof(struct irdma_msix_vector) * rf->msix_count;\n\tsize += struct_size(iw_qvlist, qv_info, rf->msix_count);\n\trf->iw_msixtbl = kzalloc(size, GFP_KERNEL);\n\tif (!rf->iw_msixtbl)\n\t\treturn -ENOMEM;\n\n\trf->iw_qvlist = (struct irdma_qvlist_info *)\n\t\t\t(&rf->iw_msixtbl[rf->msix_count]);\n\tiw_qvlist = rf->iw_qvlist;\n\tiw_qvinfo = iw_qvlist->qv_info;\n\tiw_qvlist->num_vectors = rf->msix_count;\n\tif (rf->msix_count <= num_online_cpus())\n\t\trf->msix_shared = true;\n\telse if (rf->msix_count > num_online_cpus() + 1)\n\t\trf->msix_count = num_online_cpus() + 1;\n\n\tpmsix = rf->msix_entries;\n\tfor (i = 0, ceq_idx = 0; i < rf->msix_count; i++, iw_qvinfo++) {\n\t\trf->iw_msixtbl[i].idx = pmsix->entry;\n\t\trf->iw_msixtbl[i].irq = pmsix->vector;\n\t\trf->iw_msixtbl[i].cpu_affinity = ceq_idx;\n\t\tif (!i) {\n\t\t\tiw_qvinfo->aeq_idx = 0;\n\t\t\tif (rf->msix_shared)\n\t\t\t\tiw_qvinfo->ceq_idx = ceq_idx++;\n\t\t\telse\n\t\t\t\tiw_qvinfo->ceq_idx = IRDMA_Q_INVALID_IDX;\n\t\t} else {\n\t\t\tiw_qvinfo->aeq_idx = IRDMA_Q_INVALID_IDX;\n\t\t\tiw_qvinfo->ceq_idx = ceq_idx++;\n\t\t}\n\t\tiw_qvinfo->itr_idx = 3;\n\t\tiw_qvinfo->v_idx = rf->iw_msixtbl[i].idx;\n\t\tpmsix++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic irqreturn_t irdma_irq_handler(int irq, void *data)\n{\n\tstruct irdma_pci_f *rf = data;\n\n\ttasklet_schedule(&rf->dpc_tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t irdma_ceq_handler(int irq, void *data)\n{\n\tstruct irdma_ceq *iwceq = data;\n\n\tif (iwceq->irq != irq)\n\t\tibdev_err(to_ibdev(&iwceq->rf->sc_dev), \"expected irq = %d received irq = %d\\n\",\n\t\t\t  iwceq->irq, irq);\n\ttasklet_schedule(&iwceq->dpc_tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void irdma_destroy_irq(struct irdma_pci_f *rf,\n\t\t\t      struct irdma_msix_vector *msix_vec, void *dev_id)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\n\tdev->irq_ops->irdma_dis_irq(dev, msix_vec->idx);\n\tirq_update_affinity_hint(msix_vec->irq, NULL);\n\tfree_irq(msix_vec->irq, dev_id);\n}\n\n \nstatic void irdma_destroy_cqp(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_cqp *cqp = &rf->cqp;\n\tint status = 0;\n\n\tstatus = irdma_sc_cqp_destroy(dev->cqp);\n\tif (status)\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: Destroy CQP failed %d\\n\", status);\n\n\tirdma_cleanup_pending_cqp_op(rf);\n\tdma_free_coherent(dev->hw->device, cqp->sq.size, cqp->sq.va,\n\t\t\t  cqp->sq.pa);\n\tcqp->sq.va = NULL;\n\tkfree(cqp->scratch_array);\n\tcqp->scratch_array = NULL;\n\tkfree(cqp->cqp_requests);\n\tcqp->cqp_requests = NULL;\n}\n\nstatic void irdma_destroy_virt_aeq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_aeq *aeq = &rf->aeq;\n\tu32 pg_cnt = DIV_ROUND_UP(aeq->mem.size, PAGE_SIZE);\n\tdma_addr_t *pg_arr = (dma_addr_t *)aeq->palloc.level1.addr;\n\n\tirdma_unmap_vm_page_list(&rf->hw, pg_arr, pg_cnt);\n\tirdma_free_pble(rf->pble_rsrc, &aeq->palloc);\n\tvfree(aeq->mem.va);\n}\n\n \nstatic void irdma_destroy_aeq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_aeq *aeq = &rf->aeq;\n\tint status = -EBUSY;\n\n\tif (!rf->msix_shared) {\n\t\trf->sc_dev.irq_ops->irdma_cfg_aeq(&rf->sc_dev, rf->iw_msixtbl->idx, false);\n\t\tirdma_destroy_irq(rf, rf->iw_msixtbl, rf);\n\t}\n\tif (rf->reset)\n\t\tgoto exit;\n\n\taeq->sc_aeq.size = 0;\n\tstatus = irdma_cqp_aeq_cmd(dev, &aeq->sc_aeq, IRDMA_OP_AEQ_DESTROY);\n\tif (status)\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: Destroy AEQ failed %d\\n\", status);\n\nexit:\n\tif (aeq->virtual_map) {\n\t\tirdma_destroy_virt_aeq(rf);\n\t} else {\n\t\tdma_free_coherent(dev->hw->device, aeq->mem.size, aeq->mem.va,\n\t\t\t\t  aeq->mem.pa);\n\t\taeq->mem.va = NULL;\n\t}\n}\n\n \nstatic void irdma_destroy_ceq(struct irdma_pci_f *rf, struct irdma_ceq *iwceq)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tint status;\n\n\tif (rf->reset)\n\t\tgoto exit;\n\n\tstatus = irdma_sc_ceq_destroy(&iwceq->sc_ceq, 0, 1);\n\tif (status) {\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: CEQ destroy command failed %d\\n\", status);\n\t\tgoto exit;\n\t}\n\n\tstatus = irdma_sc_cceq_destroy_done(&iwceq->sc_ceq);\n\tif (status)\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: CEQ destroy completion failed %d\\n\",\n\t\t\t  status);\nexit:\n\tdma_free_coherent(dev->hw->device, iwceq->mem.size, iwceq->mem.va,\n\t\t\t  iwceq->mem.pa);\n\tiwceq->mem.va = NULL;\n}\n\n \nstatic void irdma_del_ceq_0(struct irdma_pci_f *rf)\n{\n\tstruct irdma_ceq *iwceq = rf->ceqlist;\n\tstruct irdma_msix_vector *msix_vec;\n\n\tif (rf->msix_shared) {\n\t\tmsix_vec = &rf->iw_msixtbl[0];\n\t\trf->sc_dev.irq_ops->irdma_cfg_ceq(&rf->sc_dev,\n\t\t\t\t\t\t  msix_vec->ceq_id,\n\t\t\t\t\t\t  msix_vec->idx, false);\n\t\tirdma_destroy_irq(rf, msix_vec, rf);\n\t} else {\n\t\tmsix_vec = &rf->iw_msixtbl[1];\n\t\tirdma_destroy_irq(rf, msix_vec, iwceq);\n\t}\n\n\tirdma_destroy_ceq(rf, iwceq);\n\trf->sc_dev.ceq_valid = false;\n\trf->ceqs_count = 0;\n}\n\n \nstatic void irdma_del_ceqs(struct irdma_pci_f *rf)\n{\n\tstruct irdma_ceq *iwceq = &rf->ceqlist[1];\n\tstruct irdma_msix_vector *msix_vec;\n\tu32 i = 0;\n\n\tif (rf->msix_shared)\n\t\tmsix_vec = &rf->iw_msixtbl[1];\n\telse\n\t\tmsix_vec = &rf->iw_msixtbl[2];\n\n\tfor (i = 1; i < rf->ceqs_count; i++, msix_vec++, iwceq++) {\n\t\trf->sc_dev.irq_ops->irdma_cfg_ceq(&rf->sc_dev, msix_vec->ceq_id,\n\t\t\t\t\t\t  msix_vec->idx, false);\n\t\tirdma_destroy_irq(rf, msix_vec, iwceq);\n\t\tirdma_cqp_ceq_cmd(&rf->sc_dev, &iwceq->sc_ceq,\n\t\t\t\t  IRDMA_OP_CEQ_DESTROY);\n\t\tdma_free_coherent(rf->sc_dev.hw->device, iwceq->mem.size,\n\t\t\t\t  iwceq->mem.va, iwceq->mem.pa);\n\t\tiwceq->mem.va = NULL;\n\t}\n\trf->ceqs_count = 1;\n}\n\n \nstatic void irdma_destroy_ccq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_ccq *ccq = &rf->ccq;\n\tint status = 0;\n\n\tif (rf->cqp_cmpl_wq)\n\t\tdestroy_workqueue(rf->cqp_cmpl_wq);\n\n\tif (!rf->reset)\n\t\tstatus = irdma_sc_ccq_destroy(dev->ccq, 0, true);\n\tif (status)\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: CCQ destroy failed %d\\n\", status);\n\tdma_free_coherent(dev->hw->device, ccq->mem_cq.size, ccq->mem_cq.va,\n\t\t\t  ccq->mem_cq.pa);\n\tccq->mem_cq.va = NULL;\n}\n\n \nstatic void irdma_close_hmc_objects_type(struct irdma_sc_dev *dev,\n\t\t\t\t\t enum irdma_hmc_rsrc_type obj_type,\n\t\t\t\t\t struct irdma_hmc_info *hmc_info,\n\t\t\t\t\t bool privileged, bool reset)\n{\n\tstruct irdma_hmc_del_obj_info info = {};\n\n\tinfo.hmc_info = hmc_info;\n\tinfo.rsrc_type = obj_type;\n\tinfo.count = hmc_info->hmc_obj[obj_type].cnt;\n\tinfo.privileged = privileged;\n\tif (irdma_sc_del_hmc_obj(dev, &info, reset))\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: del HMC obj of type %d failed\\n\",\n\t\t\t  obj_type);\n}\n\n \nstatic void irdma_del_hmc_objects(struct irdma_sc_dev *dev,\n\t\t\t\t  struct irdma_hmc_info *hmc_info, bool privileged,\n\t\t\t\t  bool reset, enum irdma_vers vers)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < IW_HMC_OBJ_TYPE_NUM; i++) {\n\t\tif (dev->hmc_info->hmc_obj[iw_hmc_obj_types[i]].cnt)\n\t\t\tirdma_close_hmc_objects_type(dev, iw_hmc_obj_types[i],\n\t\t\t\t\t\t     hmc_info, privileged, reset);\n\t\tif (vers == IRDMA_GEN_1 && i == IRDMA_HMC_IW_TIMER)\n\t\t\tbreak;\n\t}\n}\n\n \nstatic int irdma_create_hmc_obj_type(struct irdma_sc_dev *dev,\n\t\t\t\t     struct irdma_hmc_create_obj_info *info)\n{\n\treturn irdma_sc_create_hmc_obj(dev, info);\n}\n\n \nstatic int irdma_create_hmc_objs(struct irdma_pci_f *rf, bool privileged,\n\t\t\t\t enum irdma_vers vers)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_hmc_create_obj_info info = {};\n\tint i, status = 0;\n\n\tinfo.hmc_info = dev->hmc_info;\n\tinfo.privileged = privileged;\n\tinfo.entry_type = rf->sd_type;\n\n\tfor (i = 0; i < IW_HMC_OBJ_TYPE_NUM; i++) {\n\t\tif (iw_hmc_obj_types[i] == IRDMA_HMC_IW_PBLE)\n\t\t\tcontinue;\n\t\tif (dev->hmc_info->hmc_obj[iw_hmc_obj_types[i]].cnt) {\n\t\t\tinfo.rsrc_type = iw_hmc_obj_types[i];\n\t\t\tinfo.count = dev->hmc_info->hmc_obj[info.rsrc_type].cnt;\n\t\t\tinfo.add_sd_cnt = 0;\n\t\t\tstatus = irdma_create_hmc_obj_type(dev, &info);\n\t\t\tif (status) {\n\t\t\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t\t\t  \"ERR: create obj type %d status = %d\\n\",\n\t\t\t\t\t  iw_hmc_obj_types[i], status);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (vers == IRDMA_GEN_1 && i == IRDMA_HMC_IW_TIMER)\n\t\t\tbreak;\n\t}\n\n\tif (!status)\n\t\treturn irdma_sc_static_hmc_pages_allocated(dev->cqp, 0, dev->hmc_fn_id,\n\t\t\t\t\t\t\t   true, true);\n\n\twhile (i) {\n\t\ti--;\n\t\t \n\t\tif (dev->hmc_info->hmc_obj[iw_hmc_obj_types[i]].cnt)\n\t\t\tirdma_close_hmc_objects_type(dev, iw_hmc_obj_types[i],\n\t\t\t\t\t\t     dev->hmc_info, privileged,\n\t\t\t\t\t\t     false);\n\t}\n\n\treturn status;\n}\n\n \nstatic int irdma_obj_aligned_mem(struct irdma_pci_f *rf,\n\t\t\t\t struct irdma_dma_mem *memptr, u32 size,\n\t\t\t\t u32 mask)\n{\n\tunsigned long va, newva;\n\tunsigned long extra;\n\n\tva = (unsigned long)rf->obj_next.va;\n\tnewva = va;\n\tif (mask)\n\t\tnewva = ALIGN(va, (unsigned long)mask + 1ULL);\n\textra = newva - va;\n\tmemptr->va = (u8 *)va + extra;\n\tmemptr->pa = rf->obj_next.pa + extra;\n\tmemptr->size = size;\n\tif (((u8 *)memptr->va + size) > ((u8 *)rf->obj_mem.va + rf->obj_mem.size))\n\t\treturn -ENOMEM;\n\n\trf->obj_next.va = (u8 *)memptr->va + size;\n\trf->obj_next.pa = memptr->pa + size;\n\n\treturn 0;\n}\n\n \nstatic int irdma_create_cqp(struct irdma_pci_f *rf)\n{\n\tu32 sqsize = IRDMA_CQP_SW_SQSIZE_2048;\n\tstruct irdma_dma_mem mem;\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_cqp_init_info cqp_init_info = {};\n\tstruct irdma_cqp *cqp = &rf->cqp;\n\tu16 maj_err, min_err;\n\tint i, status;\n\n\tcqp->cqp_requests = kcalloc(sqsize, sizeof(*cqp->cqp_requests), GFP_KERNEL);\n\tif (!cqp->cqp_requests)\n\t\treturn -ENOMEM;\n\n\tcqp->scratch_array = kcalloc(sqsize, sizeof(*cqp->scratch_array), GFP_KERNEL);\n\tif (!cqp->scratch_array) {\n\t\tstatus = -ENOMEM;\n\t\tgoto err_scratch;\n\t}\n\n\tdev->cqp = &cqp->sc_cqp;\n\tdev->cqp->dev = dev;\n\tcqp->sq.size = ALIGN(sizeof(struct irdma_cqp_sq_wqe) * sqsize,\n\t\t\t     IRDMA_CQP_ALIGNMENT);\n\tcqp->sq.va = dma_alloc_coherent(dev->hw->device, cqp->sq.size,\n\t\t\t\t\t&cqp->sq.pa, GFP_KERNEL);\n\tif (!cqp->sq.va) {\n\t\tstatus = -ENOMEM;\n\t\tgoto err_sq;\n\t}\n\n\tstatus = irdma_obj_aligned_mem(rf, &mem, sizeof(struct irdma_cqp_ctx),\n\t\t\t\t       IRDMA_HOST_CTX_ALIGNMENT_M);\n\tif (status)\n\t\tgoto err_ctx;\n\n\tdev->cqp->host_ctx_pa = mem.pa;\n\tdev->cqp->host_ctx = mem.va;\n\t \n\tcqp_init_info.dev = dev;\n\tcqp_init_info.sq_size = sqsize;\n\tcqp_init_info.sq = cqp->sq.va;\n\tcqp_init_info.sq_pa = cqp->sq.pa;\n\tcqp_init_info.host_ctx_pa = mem.pa;\n\tcqp_init_info.host_ctx = mem.va;\n\tcqp_init_info.hmc_profile = rf->rsrc_profile;\n\tcqp_init_info.scratch_array = cqp->scratch_array;\n\tcqp_init_info.protocol_used = rf->protocol_used;\n\n\tswitch (rf->rdma_ver) {\n\tcase IRDMA_GEN_1:\n\t\tcqp_init_info.hw_maj_ver = IRDMA_CQPHC_HW_MAJVER_GEN_1;\n\t\tbreak;\n\tcase IRDMA_GEN_2:\n\t\tcqp_init_info.hw_maj_ver = IRDMA_CQPHC_HW_MAJVER_GEN_2;\n\t\tbreak;\n\t}\n\tstatus = irdma_sc_cqp_init(dev->cqp, &cqp_init_info);\n\tif (status) {\n\t\tibdev_dbg(to_ibdev(dev), \"ERR: cqp init status %d\\n\", status);\n\t\tgoto err_ctx;\n\t}\n\n\tspin_lock_init(&cqp->req_lock);\n\tspin_lock_init(&cqp->compl_lock);\n\n\tstatus = irdma_sc_cqp_create(dev->cqp, &maj_err, &min_err);\n\tif (status) {\n\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t  \"ERR: cqp create failed - status %d maj_err %d min_err %d\\n\",\n\t\t\t  status, maj_err, min_err);\n\t\tgoto err_ctx;\n\t}\n\n\tINIT_LIST_HEAD(&cqp->cqp_avail_reqs);\n\tINIT_LIST_HEAD(&cqp->cqp_pending_reqs);\n\n\t \n\tfor (i = 0; i < sqsize; i++) {\n\t\tinit_waitqueue_head(&cqp->cqp_requests[i].waitq);\n\t\tlist_add_tail(&cqp->cqp_requests[i].list, &cqp->cqp_avail_reqs);\n\t}\n\tinit_waitqueue_head(&cqp->remove_wq);\n\treturn 0;\n\nerr_ctx:\n\tdma_free_coherent(dev->hw->device, cqp->sq.size,\n\t\t\t  cqp->sq.va, cqp->sq.pa);\n\tcqp->sq.va = NULL;\nerr_sq:\n\tkfree(cqp->scratch_array);\n\tcqp->scratch_array = NULL;\nerr_scratch:\n\tkfree(cqp->cqp_requests);\n\tcqp->cqp_requests = NULL;\n\n\treturn status;\n}\n\n \nstatic int irdma_create_ccq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_ccq_init_info info = {};\n\tstruct irdma_ccq *ccq = &rf->ccq;\n\tint status;\n\n\tdev->ccq = &ccq->sc_cq;\n\tdev->ccq->dev = dev;\n\tinfo.dev = dev;\n\tccq->shadow_area.size = sizeof(struct irdma_cq_shadow_area);\n\tccq->mem_cq.size = ALIGN(sizeof(struct irdma_cqe) * IW_CCQ_SIZE,\n\t\t\t\t IRDMA_CQ0_ALIGNMENT);\n\tccq->mem_cq.va = dma_alloc_coherent(dev->hw->device, ccq->mem_cq.size,\n\t\t\t\t\t    &ccq->mem_cq.pa, GFP_KERNEL);\n\tif (!ccq->mem_cq.va)\n\t\treturn -ENOMEM;\n\n\tstatus = irdma_obj_aligned_mem(rf, &ccq->shadow_area,\n\t\t\t\t       ccq->shadow_area.size,\n\t\t\t\t       IRDMA_SHADOWAREA_M);\n\tif (status)\n\t\tgoto exit;\n\n\tccq->sc_cq.back_cq = ccq;\n\t \n\tinfo.cq_base = ccq->mem_cq.va;\n\tinfo.cq_pa = ccq->mem_cq.pa;\n\tinfo.num_elem = IW_CCQ_SIZE;\n\tinfo.shadow_area = ccq->shadow_area.va;\n\tinfo.shadow_area_pa = ccq->shadow_area.pa;\n\tinfo.ceqe_mask = false;\n\tinfo.ceq_id_valid = true;\n\tinfo.shadow_read_threshold = 16;\n\tinfo.vsi = &rf->default_vsi;\n\tstatus = irdma_sc_ccq_init(dev->ccq, &info);\n\tif (!status)\n\t\tstatus = irdma_sc_ccq_create(dev->ccq, 0, true, true);\nexit:\n\tif (status) {\n\t\tdma_free_coherent(dev->hw->device, ccq->mem_cq.size,\n\t\t\t\t  ccq->mem_cq.va, ccq->mem_cq.pa);\n\t\tccq->mem_cq.va = NULL;\n\t}\n\n\treturn status;\n}\n\n \nstatic int irdma_alloc_set_mac(struct irdma_device *iwdev)\n{\n\tint status;\n\n\tstatus = irdma_alloc_local_mac_entry(iwdev->rf,\n\t\t\t\t\t     &iwdev->mac_ip_table_idx);\n\tif (!status) {\n\t\tstatus = irdma_add_local_mac_entry(iwdev->rf,\n\t\t\t\t\t\t   (const u8 *)iwdev->netdev->dev_addr,\n\t\t\t\t\t\t   (u8)iwdev->mac_ip_table_idx);\n\t\tif (status)\n\t\t\tirdma_del_local_mac_entry(iwdev->rf,\n\t\t\t\t\t\t  (u8)iwdev->mac_ip_table_idx);\n\t}\n\treturn status;\n}\n\n \nstatic int irdma_cfg_ceq_vector(struct irdma_pci_f *rf, struct irdma_ceq *iwceq,\n\t\t\t\tu32 ceq_id, struct irdma_msix_vector *msix_vec)\n{\n\tint status;\n\n\tif (rf->msix_shared && !ceq_id) {\n\t\tsnprintf(msix_vec->name, sizeof(msix_vec->name) - 1,\n\t\t\t \"irdma-%s-AEQCEQ-0\", dev_name(&rf->pcidev->dev));\n\t\ttasklet_setup(&rf->dpc_tasklet, irdma_dpc);\n\t\tstatus = request_irq(msix_vec->irq, irdma_irq_handler, 0,\n\t\t\t\t     msix_vec->name, rf);\n\t} else {\n\t\tsnprintf(msix_vec->name, sizeof(msix_vec->name) - 1,\n\t\t\t \"irdma-%s-CEQ-%d\",\n\t\t\t dev_name(&rf->pcidev->dev), ceq_id);\n\t\ttasklet_setup(&iwceq->dpc_tasklet, irdma_ceq_dpc);\n\n\t\tstatus = request_irq(msix_vec->irq, irdma_ceq_handler, 0,\n\t\t\t\t     msix_vec->name, iwceq);\n\t}\n\tcpumask_clear(&msix_vec->mask);\n\tcpumask_set_cpu(msix_vec->cpu_affinity, &msix_vec->mask);\n\tirq_update_affinity_hint(msix_vec->irq, &msix_vec->mask);\n\tif (status) {\n\t\tibdev_dbg(&rf->iwdev->ibdev, \"ERR: ceq irq config fail\\n\");\n\t\treturn status;\n\t}\n\n\tmsix_vec->ceq_id = ceq_id;\n\trf->sc_dev.irq_ops->irdma_cfg_ceq(&rf->sc_dev, ceq_id, msix_vec->idx, true);\n\n\treturn 0;\n}\n\n \nstatic int irdma_cfg_aeq_vector(struct irdma_pci_f *rf)\n{\n\tstruct irdma_msix_vector *msix_vec = rf->iw_msixtbl;\n\tu32 ret = 0;\n\n\tif (!rf->msix_shared) {\n\t\tsnprintf(msix_vec->name, sizeof(msix_vec->name) - 1,\n\t\t\t \"irdma-%s-AEQ\", dev_name(&rf->pcidev->dev));\n\t\ttasklet_setup(&rf->dpc_tasklet, irdma_dpc);\n\t\tret = request_irq(msix_vec->irq, irdma_irq_handler, 0,\n\t\t\t\t  msix_vec->name, rf);\n\t}\n\tif (ret) {\n\t\tibdev_dbg(&rf->iwdev->ibdev, \"ERR: aeq irq config fail\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trf->sc_dev.irq_ops->irdma_cfg_aeq(&rf->sc_dev, msix_vec->idx, true);\n\n\treturn 0;\n}\n\n \nstatic int irdma_create_ceq(struct irdma_pci_f *rf, struct irdma_ceq *iwceq,\n\t\t\t    u32 ceq_id, struct irdma_sc_vsi *vsi)\n{\n\tint status;\n\tstruct irdma_ceq_init_info info = {};\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tu32 ceq_size;\n\n\tinfo.ceq_id = ceq_id;\n\tiwceq->rf = rf;\n\tceq_size = min(rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_CQ].cnt,\n\t\t       dev->hw_attrs.max_hw_ceq_size);\n\tiwceq->mem.size = ALIGN(sizeof(struct irdma_ceqe) * ceq_size,\n\t\t\t\tIRDMA_CEQ_ALIGNMENT);\n\tiwceq->mem.va = dma_alloc_coherent(dev->hw->device, iwceq->mem.size,\n\t\t\t\t\t   &iwceq->mem.pa, GFP_KERNEL);\n\tif (!iwceq->mem.va)\n\t\treturn -ENOMEM;\n\n\tinfo.ceq_id = ceq_id;\n\tinfo.ceqe_base = iwceq->mem.va;\n\tinfo.ceqe_pa = iwceq->mem.pa;\n\tinfo.elem_cnt = ceq_size;\n\tiwceq->sc_ceq.ceq_id = ceq_id;\n\tinfo.dev = dev;\n\tinfo.vsi = vsi;\n\tstatus = irdma_sc_ceq_init(&iwceq->sc_ceq, &info);\n\tif (!status) {\n\t\tif (dev->ceq_valid)\n\t\t\tstatus = irdma_cqp_ceq_cmd(&rf->sc_dev, &iwceq->sc_ceq,\n\t\t\t\t\t\t   IRDMA_OP_CEQ_CREATE);\n\t\telse\n\t\t\tstatus = irdma_sc_cceq_create(&iwceq->sc_ceq, 0);\n\t}\n\n\tif (status) {\n\t\tdma_free_coherent(dev->hw->device, iwceq->mem.size,\n\t\t\t\t  iwceq->mem.va, iwceq->mem.pa);\n\t\tiwceq->mem.va = NULL;\n\t}\n\n\treturn status;\n}\n\n \nstatic int irdma_setup_ceq_0(struct irdma_pci_f *rf)\n{\n\tstruct irdma_ceq *iwceq;\n\tstruct irdma_msix_vector *msix_vec;\n\tu32 i;\n\tint status = 0;\n\tu32 num_ceqs;\n\n\tnum_ceqs = min(rf->msix_count, rf->sc_dev.hmc_fpm_misc.max_ceqs);\n\trf->ceqlist = kcalloc(num_ceqs, sizeof(*rf->ceqlist), GFP_KERNEL);\n\tif (!rf->ceqlist) {\n\t\tstatus = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tiwceq = &rf->ceqlist[0];\n\tstatus = irdma_create_ceq(rf, iwceq, 0, &rf->default_vsi);\n\tif (status) {\n\t\tibdev_dbg(&rf->iwdev->ibdev, \"ERR: create ceq status = %d\\n\",\n\t\t\t  status);\n\t\tgoto exit;\n\t}\n\n\tspin_lock_init(&iwceq->ce_lock);\n\ti = rf->msix_shared ? 0 : 1;\n\tmsix_vec = &rf->iw_msixtbl[i];\n\tiwceq->irq = msix_vec->irq;\n\tiwceq->msix_idx = msix_vec->idx;\n\tstatus = irdma_cfg_ceq_vector(rf, iwceq, 0, msix_vec);\n\tif (status) {\n\t\tirdma_destroy_ceq(rf, iwceq);\n\t\tgoto exit;\n\t}\n\n\tirdma_ena_intr(&rf->sc_dev, msix_vec->idx);\n\trf->ceqs_count++;\n\nexit:\n\tif (status && !rf->ceqs_count) {\n\t\tkfree(rf->ceqlist);\n\t\trf->ceqlist = NULL;\n\t\treturn status;\n\t}\n\trf->sc_dev.ceq_valid = true;\n\n\treturn 0;\n}\n\n \nstatic int irdma_setup_ceqs(struct irdma_pci_f *rf, struct irdma_sc_vsi *vsi)\n{\n\tu32 i;\n\tu32 ceq_id;\n\tstruct irdma_ceq *iwceq;\n\tstruct irdma_msix_vector *msix_vec;\n\tint status;\n\tu32 num_ceqs;\n\n\tnum_ceqs = min(rf->msix_count, rf->sc_dev.hmc_fpm_misc.max_ceqs);\n\ti = (rf->msix_shared) ? 1 : 2;\n\tfor (ceq_id = 1; i < num_ceqs; i++, ceq_id++) {\n\t\tiwceq = &rf->ceqlist[ceq_id];\n\t\tstatus = irdma_create_ceq(rf, iwceq, ceq_id, vsi);\n\t\tif (status) {\n\t\t\tibdev_dbg(&rf->iwdev->ibdev,\n\t\t\t\t  \"ERR: create ceq status = %d\\n\", status);\n\t\t\tgoto del_ceqs;\n\t\t}\n\t\tspin_lock_init(&iwceq->ce_lock);\n\t\tmsix_vec = &rf->iw_msixtbl[i];\n\t\tiwceq->irq = msix_vec->irq;\n\t\tiwceq->msix_idx = msix_vec->idx;\n\t\tstatus = irdma_cfg_ceq_vector(rf, iwceq, ceq_id, msix_vec);\n\t\tif (status) {\n\t\t\tirdma_destroy_ceq(rf, iwceq);\n\t\t\tgoto del_ceqs;\n\t\t}\n\t\tirdma_ena_intr(&rf->sc_dev, msix_vec->idx);\n\t\trf->ceqs_count++;\n\t}\n\n\treturn 0;\n\ndel_ceqs:\n\tirdma_del_ceqs(rf);\n\n\treturn status;\n}\n\nstatic int irdma_create_virt_aeq(struct irdma_pci_f *rf, u32 size)\n{\n\tstruct irdma_aeq *aeq = &rf->aeq;\n\tdma_addr_t *pg_arr;\n\tu32 pg_cnt;\n\tint status;\n\n\tif (rf->rdma_ver < IRDMA_GEN_2)\n\t\treturn -EOPNOTSUPP;\n\n\taeq->mem.size = sizeof(struct irdma_sc_aeqe) * size;\n\taeq->mem.va = vzalloc(aeq->mem.size);\n\n\tif (!aeq->mem.va)\n\t\treturn -ENOMEM;\n\n\tpg_cnt = DIV_ROUND_UP(aeq->mem.size, PAGE_SIZE);\n\tstatus = irdma_get_pble(rf->pble_rsrc, &aeq->palloc, pg_cnt, true);\n\tif (status) {\n\t\tvfree(aeq->mem.va);\n\t\treturn status;\n\t}\n\n\tpg_arr = (dma_addr_t *)aeq->palloc.level1.addr;\n\tstatus = irdma_map_vm_page_list(&rf->hw, aeq->mem.va, pg_arr, pg_cnt);\n\tif (status) {\n\t\tirdma_free_pble(rf->pble_rsrc, &aeq->palloc);\n\t\tvfree(aeq->mem.va);\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int irdma_create_aeq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_aeq_init_info info = {};\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_aeq *aeq = &rf->aeq;\n\tstruct irdma_hmc_info *hmc_info = rf->sc_dev.hmc_info;\n\tu32 aeq_size;\n\tu8 multiplier = (rf->protocol_used == IRDMA_IWARP_PROTOCOL_ONLY) ? 2 : 1;\n\tint status;\n\n\taeq_size = multiplier * hmc_info->hmc_obj[IRDMA_HMC_IW_QP].cnt +\n\t\t   hmc_info->hmc_obj[IRDMA_HMC_IW_CQ].cnt;\n\taeq_size = min(aeq_size, dev->hw_attrs.max_hw_aeq_size);\n\n\taeq->mem.size = ALIGN(sizeof(struct irdma_sc_aeqe) * aeq_size,\n\t\t\t      IRDMA_AEQ_ALIGNMENT);\n\taeq->mem.va = dma_alloc_coherent(dev->hw->device, aeq->mem.size,\n\t\t\t\t\t &aeq->mem.pa,\n\t\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (aeq->mem.va)\n\t\tgoto skip_virt_aeq;\n\n\t \n\tstatus = irdma_create_virt_aeq(rf, aeq_size);\n\tif (status)\n\t\treturn status;\n\n\tinfo.virtual_map = true;\n\taeq->virtual_map = info.virtual_map;\n\tinfo.pbl_chunk_size = 1;\n\tinfo.first_pm_pbl_idx = aeq->palloc.level1.idx;\n\nskip_virt_aeq:\n\tinfo.aeqe_base = aeq->mem.va;\n\tinfo.aeq_elem_pa = aeq->mem.pa;\n\tinfo.elem_cnt = aeq_size;\n\tinfo.dev = dev;\n\tinfo.msix_idx = rf->iw_msixtbl->idx;\n\tstatus = irdma_sc_aeq_init(&aeq->sc_aeq, &info);\n\tif (status)\n\t\tgoto err;\n\n\tstatus = irdma_cqp_aeq_cmd(dev, &aeq->sc_aeq, IRDMA_OP_AEQ_CREATE);\n\tif (status)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tif (aeq->virtual_map) {\n\t\tirdma_destroy_virt_aeq(rf);\n\t} else {\n\t\tdma_free_coherent(dev->hw->device, aeq->mem.size, aeq->mem.va,\n\t\t\t\t  aeq->mem.pa);\n\t\taeq->mem.va = NULL;\n\t}\n\n\treturn status;\n}\n\n \nstatic int irdma_setup_aeq(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tint status;\n\n\tstatus = irdma_create_aeq(rf);\n\tif (status)\n\t\treturn status;\n\n\tstatus = irdma_cfg_aeq_vector(rf);\n\tif (status) {\n\t\tirdma_destroy_aeq(rf);\n\t\treturn status;\n\t}\n\n\tif (!rf->msix_shared)\n\t\tirdma_ena_intr(dev, rf->iw_msixtbl[0].idx);\n\n\treturn 0;\n}\n\n \nstatic int irdma_initialize_ilq(struct irdma_device *iwdev)\n{\n\tstruct irdma_puda_rsrc_info info = {};\n\tint status;\n\n\tinfo.type = IRDMA_PUDA_RSRC_TYPE_ILQ;\n\tinfo.cq_id = 1;\n\tinfo.qp_id = 1;\n\tinfo.count = 1;\n\tinfo.pd_id = 1;\n\tinfo.abi_ver = IRDMA_ABI_VER;\n\tinfo.sq_size = min(iwdev->rf->max_qp / 2, (u32)32768);\n\tinfo.rq_size = info.sq_size;\n\tinfo.buf_size = 1024;\n\tinfo.tx_buf_cnt = 2 * info.sq_size;\n\tinfo.receive = irdma_receive_ilq;\n\tinfo.xmit_complete = irdma_free_sqbuf;\n\tstatus = irdma_puda_create_rsrc(&iwdev->vsi, &info);\n\tif (status)\n\t\tibdev_dbg(&iwdev->ibdev, \"ERR: ilq create fail\\n\");\n\n\treturn status;\n}\n\n \nstatic int irdma_initialize_ieq(struct irdma_device *iwdev)\n{\n\tstruct irdma_puda_rsrc_info info = {};\n\tint status;\n\n\tinfo.type = IRDMA_PUDA_RSRC_TYPE_IEQ;\n\tinfo.cq_id = 2;\n\tinfo.qp_id = iwdev->vsi.exception_lan_q;\n\tinfo.count = 1;\n\tinfo.pd_id = 2;\n\tinfo.abi_ver = IRDMA_ABI_VER;\n\tinfo.sq_size = min(iwdev->rf->max_qp / 2, (u32)32768);\n\tinfo.rq_size = info.sq_size;\n\tinfo.buf_size = iwdev->vsi.mtu + IRDMA_IPV4_PAD;\n\tinfo.tx_buf_cnt = 4096;\n\tstatus = irdma_puda_create_rsrc(&iwdev->vsi, &info);\n\tif (status)\n\t\tibdev_dbg(&iwdev->ibdev, \"ERR: ieq create fail\\n\");\n\n\treturn status;\n}\n\n \nvoid irdma_reinitialize_ieq(struct irdma_sc_vsi *vsi)\n{\n\tstruct irdma_device *iwdev = vsi->back_vsi;\n\tstruct irdma_pci_f *rf = iwdev->rf;\n\n\tirdma_puda_dele_rsrc(vsi, IRDMA_PUDA_RSRC_TYPE_IEQ, false);\n\tif (irdma_initialize_ieq(iwdev)) {\n\t\tiwdev->rf->reset = true;\n\t\trf->gen_ops.request_reset(rf);\n\t}\n}\n\n \nstatic int irdma_hmc_setup(struct irdma_pci_f *rf)\n{\n\tint status;\n\tu32 qpcnt;\n\n\tqpcnt = rsrc_limits_table[rf->limits_sel].qplimit;\n\n\trf->sd_type = IRDMA_SD_TYPE_DIRECT;\n\tstatus = irdma_cfg_fpm_val(&rf->sc_dev, qpcnt);\n\tif (status)\n\t\treturn status;\n\n\tstatus = irdma_create_hmc_objs(rf, true, rf->rdma_ver);\n\n\treturn status;\n}\n\n \nstatic void irdma_del_init_mem(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\n\tkfree(dev->hmc_info->sd_table.sd_entry);\n\tdev->hmc_info->sd_table.sd_entry = NULL;\n\tvfree(rf->mem_rsrc);\n\trf->mem_rsrc = NULL;\n\tdma_free_coherent(rf->hw.device, rf->obj_mem.size, rf->obj_mem.va,\n\t\t\t  rf->obj_mem.pa);\n\trf->obj_mem.va = NULL;\n\tif (rf->rdma_ver != IRDMA_GEN_1) {\n\t\tbitmap_free(rf->allocated_ws_nodes);\n\t\trf->allocated_ws_nodes = NULL;\n\t}\n\tkfree(rf->ceqlist);\n\trf->ceqlist = NULL;\n\tkfree(rf->iw_msixtbl);\n\trf->iw_msixtbl = NULL;\n\tkfree(rf->hmc_info_mem);\n\trf->hmc_info_mem = NULL;\n}\n\n \nstatic int irdma_initialize_dev(struct irdma_pci_f *rf)\n{\n\tint status;\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_device_init_info info = {};\n\tstruct irdma_dma_mem mem;\n\tu32 size;\n\n\tsize = sizeof(struct irdma_hmc_pble_rsrc) +\n\t       sizeof(struct irdma_hmc_info) +\n\t       (sizeof(struct irdma_hmc_obj_info) * IRDMA_HMC_IW_MAX);\n\n\trf->hmc_info_mem = kzalloc(size, GFP_KERNEL);\n\tif (!rf->hmc_info_mem)\n\t\treturn -ENOMEM;\n\n\trf->pble_rsrc = (struct irdma_hmc_pble_rsrc *)rf->hmc_info_mem;\n\tdev->hmc_info = &rf->hw.hmc;\n\tdev->hmc_info->hmc_obj = (struct irdma_hmc_obj_info *)\n\t\t\t\t (rf->pble_rsrc + 1);\n\n\tstatus = irdma_obj_aligned_mem(rf, &mem, IRDMA_QUERY_FPM_BUF_SIZE,\n\t\t\t\t       IRDMA_FPM_QUERY_BUF_ALIGNMENT_M);\n\tif (status)\n\t\tgoto error;\n\n\tinfo.fpm_query_buf_pa = mem.pa;\n\tinfo.fpm_query_buf = mem.va;\n\n\tstatus = irdma_obj_aligned_mem(rf, &mem, IRDMA_COMMIT_FPM_BUF_SIZE,\n\t\t\t\t       IRDMA_FPM_COMMIT_BUF_ALIGNMENT_M);\n\tif (status)\n\t\tgoto error;\n\n\tinfo.fpm_commit_buf_pa = mem.pa;\n\tinfo.fpm_commit_buf = mem.va;\n\n\tinfo.bar0 = rf->hw.hw_addr;\n\tinfo.hmc_fn_id = rf->pf_id;\n\tinfo.hw = &rf->hw;\n\tstatus = irdma_sc_dev_init(rf->rdma_ver, &rf->sc_dev, &info);\n\tif (status)\n\t\tgoto error;\n\n\treturn status;\nerror:\n\tkfree(rf->hmc_info_mem);\n\trf->hmc_info_mem = NULL;\n\n\treturn status;\n}\n\n \nvoid irdma_rt_deinit_hw(struct irdma_device *iwdev)\n{\n\tibdev_dbg(&iwdev->ibdev, \"INIT: state = %d\\n\", iwdev->init_state);\n\n\tswitch (iwdev->init_state) {\n\tcase IP_ADDR_REGISTERED:\n\t\tif (iwdev->rf->sc_dev.hw_attrs.uk_attrs.hw_rev == IRDMA_GEN_1)\n\t\t\tirdma_del_local_mac_entry(iwdev->rf,\n\t\t\t\t\t\t  (u8)iwdev->mac_ip_table_idx);\n\t\tfallthrough;\n\tcase AEQ_CREATED:\n\tcase PBLE_CHUNK_MEM:\n\tcase CEQS_CREATED:\n\tcase IEQ_CREATED:\n\t\tif (!iwdev->roce_mode)\n\t\t\tirdma_puda_dele_rsrc(&iwdev->vsi, IRDMA_PUDA_RSRC_TYPE_IEQ,\n\t\t\t\t\t     iwdev->rf->reset);\n\t\tfallthrough;\n\tcase ILQ_CREATED:\n\t\tif (!iwdev->roce_mode)\n\t\t\tirdma_puda_dele_rsrc(&iwdev->vsi,\n\t\t\t\t\t     IRDMA_PUDA_RSRC_TYPE_ILQ,\n\t\t\t\t\t     iwdev->rf->reset);\n\t\tbreak;\n\tdefault:\n\t\tibdev_warn(&iwdev->ibdev, \"bad init_state = %d\\n\", iwdev->init_state);\n\t\tbreak;\n\t}\n\n\tirdma_cleanup_cm_core(&iwdev->cm_core);\n\tif (iwdev->vsi.pestat) {\n\t\tirdma_vsi_stats_free(&iwdev->vsi);\n\t\tkfree(iwdev->vsi.pestat);\n\t}\n\tif (iwdev->cleanup_wq)\n\t\tdestroy_workqueue(iwdev->cleanup_wq);\n}\n\nstatic int irdma_setup_init_state(struct irdma_pci_f *rf)\n{\n\tint status;\n\n\tstatus = irdma_save_msix_info(rf);\n\tif (status)\n\t\treturn status;\n\n\trf->hw.device = &rf->pcidev->dev;\n\trf->obj_mem.size = ALIGN(8192, IRDMA_HW_PAGE_SIZE);\n\trf->obj_mem.va = dma_alloc_coherent(rf->hw.device, rf->obj_mem.size,\n\t\t\t\t\t    &rf->obj_mem.pa, GFP_KERNEL);\n\tif (!rf->obj_mem.va) {\n\t\tstatus = -ENOMEM;\n\t\tgoto clean_msixtbl;\n\t}\n\n\trf->obj_next = rf->obj_mem;\n\tstatus = irdma_initialize_dev(rf);\n\tif (status)\n\t\tgoto clean_obj_mem;\n\n\treturn 0;\n\nclean_obj_mem:\n\tdma_free_coherent(rf->hw.device, rf->obj_mem.size, rf->obj_mem.va,\n\t\t\t  rf->obj_mem.pa);\n\trf->obj_mem.va = NULL;\nclean_msixtbl:\n\tkfree(rf->iw_msixtbl);\n\trf->iw_msixtbl = NULL;\n\treturn status;\n}\n\n \nstatic void irdma_get_used_rsrc(struct irdma_device *iwdev)\n{\n\tiwdev->rf->used_pds = find_first_zero_bit(iwdev->rf->allocated_pds,\n\t\t\t\t\t\t iwdev->rf->max_pd);\n\tiwdev->rf->used_qps = find_first_zero_bit(iwdev->rf->allocated_qps,\n\t\t\t\t\t\t iwdev->rf->max_qp);\n\tiwdev->rf->used_cqs = find_first_zero_bit(iwdev->rf->allocated_cqs,\n\t\t\t\t\t\t iwdev->rf->max_cq);\n\tiwdev->rf->used_mrs = find_first_zero_bit(iwdev->rf->allocated_mrs,\n\t\t\t\t\t\t iwdev->rf->max_mr);\n}\n\nvoid irdma_ctrl_deinit_hw(struct irdma_pci_f *rf)\n{\n\tenum init_completion_state state = rf->init_state;\n\n\trf->init_state = INVALID_STATE;\n\tif (rf->rsrc_created) {\n\t\tirdma_destroy_aeq(rf);\n\t\tirdma_destroy_pble_prm(rf->pble_rsrc);\n\t\tirdma_del_ceqs(rf);\n\t\trf->rsrc_created = false;\n\t}\n\tswitch (state) {\n\tcase CEQ0_CREATED:\n\t\tirdma_del_ceq_0(rf);\n\t\tfallthrough;\n\tcase CCQ_CREATED:\n\t\tirdma_destroy_ccq(rf);\n\t\tfallthrough;\n\tcase HW_RSRC_INITIALIZED:\n\tcase HMC_OBJS_CREATED:\n\t\tirdma_del_hmc_objects(&rf->sc_dev, rf->sc_dev.hmc_info, true,\n\t\t\t\t      rf->reset, rf->rdma_ver);\n\t\tfallthrough;\n\tcase CQP_CREATED:\n\t\tirdma_destroy_cqp(rf);\n\t\tfallthrough;\n\tcase INITIAL_STATE:\n\t\tirdma_del_init_mem(rf);\n\t\tbreak;\n\tcase INVALID_STATE:\n\tdefault:\n\t\tibdev_warn(&rf->iwdev->ibdev, \"bad init_state = %d\\n\", rf->init_state);\n\t\tbreak;\n\t}\n}\n\n \nint irdma_rt_init_hw(struct irdma_device *iwdev,\n\t\t     struct irdma_l2params *l2params)\n{\n\tstruct irdma_pci_f *rf = iwdev->rf;\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tstruct irdma_vsi_init_info vsi_info = {};\n\tstruct irdma_vsi_stats_info stats_info = {};\n\tint status;\n\n\tvsi_info.dev = dev;\n\tvsi_info.back_vsi = iwdev;\n\tvsi_info.params = l2params;\n\tvsi_info.pf_data_vsi_num = iwdev->vsi_num;\n\tvsi_info.register_qset = rf->gen_ops.register_qset;\n\tvsi_info.unregister_qset = rf->gen_ops.unregister_qset;\n\tvsi_info.exception_lan_q = 2;\n\tirdma_sc_vsi_init(&iwdev->vsi, &vsi_info);\n\n\tstatus = irdma_setup_cm_core(iwdev, rf->rdma_ver);\n\tif (status)\n\t\treturn status;\n\n\tstats_info.pestat = kzalloc(sizeof(*stats_info.pestat), GFP_KERNEL);\n\tif (!stats_info.pestat) {\n\t\tirdma_cleanup_cm_core(&iwdev->cm_core);\n\t\treturn -ENOMEM;\n\t}\n\tstats_info.fcn_id = dev->hmc_fn_id;\n\tstatus = irdma_vsi_stats_init(&iwdev->vsi, &stats_info);\n\tif (status) {\n\t\tirdma_cleanup_cm_core(&iwdev->cm_core);\n\t\tkfree(stats_info.pestat);\n\t\treturn status;\n\t}\n\n\tdo {\n\t\tif (!iwdev->roce_mode) {\n\t\t\tstatus = irdma_initialize_ilq(iwdev);\n\t\t\tif (status)\n\t\t\t\tbreak;\n\t\t\tiwdev->init_state = ILQ_CREATED;\n\t\t\tstatus = irdma_initialize_ieq(iwdev);\n\t\t\tif (status)\n\t\t\t\tbreak;\n\t\t\tiwdev->init_state = IEQ_CREATED;\n\t\t}\n\t\tif (!rf->rsrc_created) {\n\t\t\tstatus = irdma_setup_ceqs(rf, &iwdev->vsi);\n\t\t\tif (status)\n\t\t\t\tbreak;\n\n\t\t\tiwdev->init_state = CEQS_CREATED;\n\n\t\t\tstatus = irdma_hmc_init_pble(&rf->sc_dev,\n\t\t\t\t\t\t     rf->pble_rsrc);\n\t\t\tif (status) {\n\t\t\t\tirdma_del_ceqs(rf);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tiwdev->init_state = PBLE_CHUNK_MEM;\n\n\t\t\tstatus = irdma_setup_aeq(rf);\n\t\t\tif (status) {\n\t\t\t\tirdma_destroy_pble_prm(rf->pble_rsrc);\n\t\t\t\tirdma_del_ceqs(rf);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tiwdev->init_state = AEQ_CREATED;\n\t\t\trf->rsrc_created = true;\n\t\t}\n\n\t\tif (iwdev->rf->sc_dev.hw_attrs.uk_attrs.hw_rev == IRDMA_GEN_1)\n\t\t\tirdma_alloc_set_mac(iwdev);\n\t\tirdma_add_ip(iwdev);\n\t\tiwdev->init_state = IP_ADDR_REGISTERED;\n\n\t\t \n\t\tiwdev->cleanup_wq = alloc_workqueue(\"irdma-cleanup-wq\",\n\t\t\t\t\tWQ_UNBOUND, WQ_UNBOUND_MAX_ACTIVE);\n\t\tif (!iwdev->cleanup_wq)\n\t\t\treturn -ENOMEM;\n\t\tirdma_get_used_rsrc(iwdev);\n\t\tinit_waitqueue_head(&iwdev->suspend_wq);\n\n\t\treturn 0;\n\t} while (0);\n\n\tdev_err(&rf->pcidev->dev, \"HW runtime init FAIL status = %d last cmpl = %d\\n\",\n\t\tstatus, iwdev->init_state);\n\tirdma_rt_deinit_hw(iwdev);\n\n\treturn status;\n}\n\n \nint irdma_ctrl_init_hw(struct irdma_pci_f *rf)\n{\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tint status;\n\tdo {\n\t\tstatus = irdma_setup_init_state(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = INITIAL_STATE;\n\n\t\tstatus = irdma_create_cqp(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = CQP_CREATED;\n\n\t\tstatus = irdma_hmc_setup(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = HMC_OBJS_CREATED;\n\n\t\tstatus = irdma_initialize_hw_rsrc(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = HW_RSRC_INITIALIZED;\n\n\t\tstatus = irdma_create_ccq(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = CCQ_CREATED;\n\n\t\tdev->feature_info[IRDMA_FEATURE_FW_INFO] = IRDMA_FW_VER_DEFAULT;\n\t\tif (rf->rdma_ver != IRDMA_GEN_1) {\n\t\t\tstatus = irdma_get_rdma_features(dev);\n\t\t\tif (status)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tstatus = irdma_setup_ceq_0(rf);\n\t\tif (status)\n\t\t\tbreak;\n\t\trf->init_state = CEQ0_CREATED;\n\t\t \n\t\trf->cqp_cmpl_wq =\n\t\t\talloc_ordered_workqueue(\"cqp_cmpl_wq\", WQ_HIGHPRI);\n\t\tif (!rf->cqp_cmpl_wq) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tINIT_WORK(&rf->cqp_cmpl_work, cqp_compl_worker);\n\t\tirdma_sc_ccq_arm(dev->ccq);\n\t\treturn 0;\n\t} while (0);\n\n\tdev_err(&rf->pcidev->dev, \"IRDMA hardware initialization FAILED init_state=%d status=%d\\n\",\n\t\trf->init_state, status);\n\tirdma_ctrl_deinit_hw(rf);\n\treturn status;\n}\n\n \nstatic void irdma_set_hw_rsrc(struct irdma_pci_f *rf)\n{\n\trf->allocated_qps = (void *)(rf->mem_rsrc +\n\t\t   (sizeof(struct irdma_arp_entry) * rf->arp_table_size));\n\trf->allocated_cqs = &rf->allocated_qps[BITS_TO_LONGS(rf->max_qp)];\n\trf->allocated_mrs = &rf->allocated_cqs[BITS_TO_LONGS(rf->max_cq)];\n\trf->allocated_pds = &rf->allocated_mrs[BITS_TO_LONGS(rf->max_mr)];\n\trf->allocated_ahs = &rf->allocated_pds[BITS_TO_LONGS(rf->max_pd)];\n\trf->allocated_mcgs = &rf->allocated_ahs[BITS_TO_LONGS(rf->max_ah)];\n\trf->allocated_arps = &rf->allocated_mcgs[BITS_TO_LONGS(rf->max_mcg)];\n\trf->qp_table = (struct irdma_qp **)\n\t\t(&rf->allocated_arps[BITS_TO_LONGS(rf->arp_table_size)]);\n\trf->cq_table = (struct irdma_cq **)(&rf->qp_table[rf->max_qp]);\n\n\tspin_lock_init(&rf->rsrc_lock);\n\tspin_lock_init(&rf->arp_lock);\n\tspin_lock_init(&rf->qptable_lock);\n\tspin_lock_init(&rf->cqtable_lock);\n\tspin_lock_init(&rf->qh_list_lock);\n}\n\n \nstatic u32 irdma_calc_mem_rsrc_size(struct irdma_pci_f *rf)\n{\n\tu32 rsrc_size;\n\n\trsrc_size = sizeof(struct irdma_arp_entry) * rf->arp_table_size;\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_qp);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_mr);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_cq);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_pd);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->arp_table_size);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_ah);\n\trsrc_size += sizeof(unsigned long) * BITS_TO_LONGS(rf->max_mcg);\n\trsrc_size += sizeof(struct irdma_qp **) * rf->max_qp;\n\trsrc_size += sizeof(struct irdma_cq **) * rf->max_cq;\n\n\treturn rsrc_size;\n}\n\n \nu32 irdma_initialize_hw_rsrc(struct irdma_pci_f *rf)\n{\n\tu32 rsrc_size;\n\tu32 mrdrvbits;\n\tu32 ret;\n\n\tif (rf->rdma_ver != IRDMA_GEN_1) {\n\t\trf->allocated_ws_nodes = bitmap_zalloc(IRDMA_MAX_WS_NODES,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!rf->allocated_ws_nodes)\n\t\t\treturn -ENOMEM;\n\n\t\tset_bit(0, rf->allocated_ws_nodes);\n\t\trf->max_ws_node_id = IRDMA_MAX_WS_NODES;\n\t}\n\trf->max_cqe = rf->sc_dev.hw_attrs.uk_attrs.max_hw_cq_size;\n\trf->max_qp = rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_QP].cnt;\n\trf->max_mr = rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_MR].cnt;\n\trf->max_cq = rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_CQ].cnt;\n\trf->max_pd = rf->sc_dev.hw_attrs.max_hw_pds;\n\trf->arp_table_size = rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_ARP].cnt;\n\trf->max_ah = rf->sc_dev.hmc_info->hmc_obj[IRDMA_HMC_IW_FSIAV].cnt;\n\trf->max_mcg = rf->max_qp;\n\n\trsrc_size = irdma_calc_mem_rsrc_size(rf);\n\trf->mem_rsrc = vzalloc(rsrc_size);\n\tif (!rf->mem_rsrc) {\n\t\tret = -ENOMEM;\n\t\tgoto mem_rsrc_vzalloc_fail;\n\t}\n\n\trf->arp_table = (struct irdma_arp_entry *)rf->mem_rsrc;\n\n\tirdma_set_hw_rsrc(rf);\n\n\tset_bit(0, rf->allocated_mrs);\n\tset_bit(0, rf->allocated_qps);\n\tset_bit(0, rf->allocated_cqs);\n\tset_bit(0, rf->allocated_pds);\n\tset_bit(0, rf->allocated_arps);\n\tset_bit(0, rf->allocated_ahs);\n\tset_bit(0, rf->allocated_mcgs);\n\tset_bit(2, rf->allocated_qps);  \n\tset_bit(1, rf->allocated_qps);  \n\tset_bit(1, rf->allocated_cqs);\n\tset_bit(1, rf->allocated_pds);\n\tset_bit(2, rf->allocated_cqs);\n\tset_bit(2, rf->allocated_pds);\n\n\tINIT_LIST_HEAD(&rf->mc_qht_list.list);\n\t \n\tmrdrvbits = 24 - max(get_count_order(rf->max_mr), 14);\n\trf->mr_stagmask = ~(((1 << mrdrvbits) - 1) << (32 - mrdrvbits));\n\n\treturn 0;\n\nmem_rsrc_vzalloc_fail:\n\tbitmap_free(rf->allocated_ws_nodes);\n\trf->allocated_ws_nodes = NULL;\n\n\treturn ret;\n}\n\n \nvoid irdma_cqp_ce_handler(struct irdma_pci_f *rf, struct irdma_sc_cq *cq)\n{\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct irdma_sc_dev *dev = &rf->sc_dev;\n\tu32 cqe_count = 0;\n\tstruct irdma_ccq_cqe_info info;\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tmemset(&info, 0, sizeof(info));\n\t\tspin_lock_irqsave(&rf->cqp.compl_lock, flags);\n\t\tret = irdma_sc_ccq_get_cqe_info(cq, &info);\n\t\tspin_unlock_irqrestore(&rf->cqp.compl_lock, flags);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tcqp_request = (struct irdma_cqp_request *)\n\t\t\t      (unsigned long)info.scratch;\n\t\tif (info.error && irdma_cqp_crit_err(dev, cqp_request->info.cqp_cmd,\n\t\t\t\t\t\t     info.maj_err_code,\n\t\t\t\t\t\t     info.min_err_code))\n\t\t\tibdev_err(&rf->iwdev->ibdev, \"cqp opcode = 0x%x maj_err_code = 0x%x min_err_code = 0x%x\\n\",\n\t\t\t\t  info.op_code, info.maj_err_code, info.min_err_code);\n\t\tif (cqp_request) {\n\t\t\tcqp_request->compl_info.maj_err_code = info.maj_err_code;\n\t\t\tcqp_request->compl_info.min_err_code = info.min_err_code;\n\t\t\tcqp_request->compl_info.op_ret_val = info.op_ret_val;\n\t\t\tcqp_request->compl_info.error = info.error;\n\n\t\t\tif (cqp_request->waiting) {\n\t\t\t\tWRITE_ONCE(cqp_request->request_done, true);\n\t\t\t\twake_up(&cqp_request->waitq);\n\t\t\t\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n\t\t\t} else {\n\t\t\t\tif (cqp_request->callback_fcn)\n\t\t\t\t\tcqp_request->callback_fcn(cqp_request);\n\t\t\t\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n\t\t\t}\n\t\t}\n\n\t\tcqe_count++;\n\t} while (1);\n\n\tif (cqe_count) {\n\t\tirdma_process_bh(dev);\n\t\tirdma_sc_ccq_arm(cq);\n\t}\n}\n\n \nvoid cqp_compl_worker(struct work_struct *work)\n{\n\tstruct irdma_pci_f *rf = container_of(work, struct irdma_pci_f,\n\t\t\t\t\t      cqp_cmpl_work);\n\tstruct irdma_sc_cq *cq = &rf->ccq.sc_cq;\n\n\tirdma_cqp_ce_handler(rf, cq);\n}\n\n \nstatic struct irdma_apbvt_entry *irdma_lookup_apbvt_entry(struct irdma_cm_core *cm_core,\n\t\t\t\t\t\t\t  u16 port)\n{\n\tstruct irdma_apbvt_entry *entry;\n\n\thash_for_each_possible(cm_core->apbvt_hash_tbl, entry, hlist, port) {\n\t\tif (entry->port == port) {\n\t\t\tentry->use_cnt++;\n\t\t\treturn entry;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nvoid irdma_next_iw_state(struct irdma_qp *iwqp, u8 state, u8 del_hash, u8 term,\n\t\t\t u8 termlen)\n{\n\tstruct irdma_modify_qp_info info = {};\n\n\tinfo.next_iwarp_state = state;\n\tinfo.remove_hash_idx = del_hash;\n\tinfo.cq_num_valid = true;\n\tinfo.arp_cache_idx_valid = true;\n\tinfo.dont_send_term = true;\n\tinfo.dont_send_fin = true;\n\tinfo.termlen = termlen;\n\n\tif (term & IRDMAQP_TERM_SEND_TERM_ONLY)\n\t\tinfo.dont_send_term = false;\n\tif (term & IRDMAQP_TERM_SEND_FIN_ONLY)\n\t\tinfo.dont_send_fin = false;\n\tif (iwqp->sc_qp.term_flags && state == IRDMA_QP_STATE_ERROR)\n\t\tinfo.reset_tcp_conn = true;\n\tiwqp->hw_iwarp_state = state;\n\tirdma_hw_modify_qp(iwqp->iwdev, iwqp, &info, 0);\n\tiwqp->iwarp_state = info.next_iwarp_state;\n}\n\n \nvoid irdma_del_local_mac_entry(struct irdma_pci_f *rf, u16 idx)\n{\n\tstruct irdma_cqp *iwcqp = &rf->cqp;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(iwcqp, true);\n\tif (!cqp_request)\n\t\treturn;\n\n\tcqp_info = &cqp_request->info;\n\tcqp_info->cqp_cmd = IRDMA_OP_DELETE_LOCAL_MAC_ENTRY;\n\tcqp_info->post_sq = 1;\n\tcqp_info->in.u.del_local_mac_entry.cqp = &iwcqp->sc_cqp;\n\tcqp_info->in.u.del_local_mac_entry.scratch = (uintptr_t)cqp_request;\n\tcqp_info->in.u.del_local_mac_entry.entry_idx = idx;\n\tcqp_info->in.u.del_local_mac_entry.ignore_ref_count = 0;\n\n\tirdma_handle_cqp_op(rf, cqp_request);\n\tirdma_put_cqp_request(iwcqp, cqp_request);\n}\n\n \nint irdma_add_local_mac_entry(struct irdma_pci_f *rf, const u8 *mac_addr, u16 idx)\n{\n\tstruct irdma_local_mac_entry_info *info;\n\tstruct irdma_cqp *iwcqp = &rf->cqp;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tint status;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(iwcqp, true);\n\tif (!cqp_request)\n\t\treturn -ENOMEM;\n\n\tcqp_info = &cqp_request->info;\n\tcqp_info->post_sq = 1;\n\tinfo = &cqp_info->in.u.add_local_mac_entry.info;\n\tether_addr_copy(info->mac_addr, mac_addr);\n\tinfo->entry_idx = idx;\n\tcqp_info->in.u.add_local_mac_entry.scratch = (uintptr_t)cqp_request;\n\tcqp_info->cqp_cmd = IRDMA_OP_ADD_LOCAL_MAC_ENTRY;\n\tcqp_info->in.u.add_local_mac_entry.cqp = &iwcqp->sc_cqp;\n\tcqp_info->in.u.add_local_mac_entry.scratch = (uintptr_t)cqp_request;\n\n\tstatus = irdma_handle_cqp_op(rf, cqp_request);\n\tirdma_put_cqp_request(iwcqp, cqp_request);\n\n\treturn status;\n}\n\n \nint irdma_alloc_local_mac_entry(struct irdma_pci_f *rf, u16 *mac_tbl_idx)\n{\n\tstruct irdma_cqp *iwcqp = &rf->cqp;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tint status = 0;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(iwcqp, true);\n\tif (!cqp_request)\n\t\treturn -ENOMEM;\n\n\tcqp_info = &cqp_request->info;\n\tcqp_info->cqp_cmd = IRDMA_OP_ALLOC_LOCAL_MAC_ENTRY;\n\tcqp_info->post_sq = 1;\n\tcqp_info->in.u.alloc_local_mac_entry.cqp = &iwcqp->sc_cqp;\n\tcqp_info->in.u.alloc_local_mac_entry.scratch = (uintptr_t)cqp_request;\n\tstatus = irdma_handle_cqp_op(rf, cqp_request);\n\tif (!status)\n\t\t*mac_tbl_idx = (u16)cqp_request->compl_info.op_ret_val;\n\n\tirdma_put_cqp_request(iwcqp, cqp_request);\n\n\treturn status;\n}\n\n \nstatic int irdma_cqp_manage_apbvt_cmd(struct irdma_device *iwdev,\n\t\t\t\t      u16 accel_local_port, bool add_port)\n{\n\tstruct irdma_apbvt_info *info;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tint status;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(&iwdev->rf->cqp, add_port);\n\tif (!cqp_request)\n\t\treturn -ENOMEM;\n\n\tcqp_info = &cqp_request->info;\n\tinfo = &cqp_info->in.u.manage_apbvt_entry.info;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->add = add_port;\n\tinfo->port = accel_local_port;\n\tcqp_info->cqp_cmd = IRDMA_OP_MANAGE_APBVT_ENTRY;\n\tcqp_info->post_sq = 1;\n\tcqp_info->in.u.manage_apbvt_entry.cqp = &iwdev->rf->cqp.sc_cqp;\n\tcqp_info->in.u.manage_apbvt_entry.scratch = (uintptr_t)cqp_request;\n\tibdev_dbg(&iwdev->ibdev, \"DEV: %s: port=0x%04x\\n\",\n\t\t  (!add_port) ? \"DELETE\" : \"ADD\", accel_local_port);\n\n\tstatus = irdma_handle_cqp_op(iwdev->rf, cqp_request);\n\tirdma_put_cqp_request(&iwdev->rf->cqp, cqp_request);\n\n\treturn status;\n}\n\n \nstruct irdma_apbvt_entry *irdma_add_apbvt(struct irdma_device *iwdev, u16 port)\n{\n\tstruct irdma_cm_core *cm_core = &iwdev->cm_core;\n\tstruct irdma_apbvt_entry *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cm_core->apbvt_lock, flags);\n\tentry = irdma_lookup_apbvt_entry(cm_core, port);\n\tif (entry) {\n\t\tspin_unlock_irqrestore(&cm_core->apbvt_lock, flags);\n\t\treturn entry;\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry) {\n\t\tspin_unlock_irqrestore(&cm_core->apbvt_lock, flags);\n\t\treturn NULL;\n\t}\n\n\tentry->port = port;\n\tentry->use_cnt = 1;\n\thash_add(cm_core->apbvt_hash_tbl, &entry->hlist, entry->port);\n\tspin_unlock_irqrestore(&cm_core->apbvt_lock, flags);\n\n\tif (irdma_cqp_manage_apbvt_cmd(iwdev, port, true)) {\n\t\tkfree(entry);\n\t\treturn NULL;\n\t}\n\n\treturn entry;\n}\n\n \nvoid irdma_del_apbvt(struct irdma_device *iwdev,\n\t\t     struct irdma_apbvt_entry *entry)\n{\n\tstruct irdma_cm_core *cm_core = &iwdev->cm_core;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cm_core->apbvt_lock, flags);\n\tif (--entry->use_cnt) {\n\t\tspin_unlock_irqrestore(&cm_core->apbvt_lock, flags);\n\t\treturn;\n\t}\n\n\thash_del(&entry->hlist);\n\t \n\tirdma_cqp_manage_apbvt_cmd(iwdev, entry->port, false);\n\tkfree(entry);\n\tspin_unlock_irqrestore(&cm_core->apbvt_lock, flags);\n}\n\n \nvoid irdma_manage_arp_cache(struct irdma_pci_f *rf,\n\t\t\t    const unsigned char *mac_addr,\n\t\t\t    u32 *ip_addr, bool ipv4, u32 action)\n{\n\tstruct irdma_add_arp_cache_entry_info *info;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tint arp_index;\n\n\tarp_index = irdma_arp_table(rf, ip_addr, ipv4, mac_addr, action);\n\tif (arp_index == -1)\n\t\treturn;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, false);\n\tif (!cqp_request)\n\t\treturn;\n\n\tcqp_info = &cqp_request->info;\n\tif (action == IRDMA_ARP_ADD) {\n\t\tcqp_info->cqp_cmd = IRDMA_OP_ADD_ARP_CACHE_ENTRY;\n\t\tinfo = &cqp_info->in.u.add_arp_cache_entry.info;\n\t\tmemset(info, 0, sizeof(*info));\n\t\tinfo->arp_index = (u16)arp_index;\n\t\tinfo->permanent = true;\n\t\tether_addr_copy(info->mac_addr, mac_addr);\n\t\tcqp_info->in.u.add_arp_cache_entry.scratch =\n\t\t\t(uintptr_t)cqp_request;\n\t\tcqp_info->in.u.add_arp_cache_entry.cqp = &rf->cqp.sc_cqp;\n\t} else {\n\t\tcqp_info->cqp_cmd = IRDMA_OP_DELETE_ARP_CACHE_ENTRY;\n\t\tcqp_info->in.u.del_arp_cache_entry.scratch =\n\t\t\t(uintptr_t)cqp_request;\n\t\tcqp_info->in.u.del_arp_cache_entry.cqp = &rf->cqp.sc_cqp;\n\t\tcqp_info->in.u.del_arp_cache_entry.arp_index = arp_index;\n\t}\n\n\tcqp_info->post_sq = 1;\n\tirdma_handle_cqp_op(rf, cqp_request);\n\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n}\n\n \nstatic void irdma_send_syn_cqp_callback(struct irdma_cqp_request *cqp_request)\n{\n\tstruct irdma_cm_node *cm_node = cqp_request->param;\n\n\tirdma_send_syn(cm_node, 1);\n\tirdma_rem_ref_cm_node(cm_node);\n}\n\n \nint irdma_manage_qhash(struct irdma_device *iwdev, struct irdma_cm_info *cminfo,\n\t\t       enum irdma_quad_entry_type etype,\n\t\t       enum irdma_quad_hash_manage_type mtype, void *cmnode,\n\t\t       bool wait)\n{\n\tstruct irdma_qhash_table_info *info;\n\tstruct irdma_cqp *iwcqp = &iwdev->rf->cqp;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tstruct irdma_cm_node *cm_node = cmnode;\n\tint status;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(iwcqp, wait);\n\tif (!cqp_request)\n\t\treturn -ENOMEM;\n\n\tcqp_info = &cqp_request->info;\n\tinfo = &cqp_info->in.u.manage_qhash_table_entry.info;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->vsi = &iwdev->vsi;\n\tinfo->manage = mtype;\n\tinfo->entry_type = etype;\n\tif (cminfo->vlan_id < VLAN_N_VID) {\n\t\tinfo->vlan_valid = true;\n\t\tinfo->vlan_id = cminfo->vlan_id;\n\t} else {\n\t\tinfo->vlan_valid = false;\n\t}\n\tinfo->ipv4_valid = cminfo->ipv4;\n\tinfo->user_pri = cminfo->user_pri;\n\tether_addr_copy(info->mac_addr, iwdev->netdev->dev_addr);\n\tinfo->qp_num = cminfo->qh_qpid;\n\tinfo->dest_port = cminfo->loc_port;\n\tinfo->dest_ip[0] = cminfo->loc_addr[0];\n\tinfo->dest_ip[1] = cminfo->loc_addr[1];\n\tinfo->dest_ip[2] = cminfo->loc_addr[2];\n\tinfo->dest_ip[3] = cminfo->loc_addr[3];\n\tif (etype == IRDMA_QHASH_TYPE_TCP_ESTABLISHED ||\n\t    etype == IRDMA_QHASH_TYPE_UDP_UNICAST ||\n\t    etype == IRDMA_QHASH_TYPE_UDP_MCAST ||\n\t    etype == IRDMA_QHASH_TYPE_ROCE_MCAST ||\n\t    etype == IRDMA_QHASH_TYPE_ROCEV2_HW) {\n\t\tinfo->src_port = cminfo->rem_port;\n\t\tinfo->src_ip[0] = cminfo->rem_addr[0];\n\t\tinfo->src_ip[1] = cminfo->rem_addr[1];\n\t\tinfo->src_ip[2] = cminfo->rem_addr[2];\n\t\tinfo->src_ip[3] = cminfo->rem_addr[3];\n\t}\n\tif (cmnode) {\n\t\tcqp_request->callback_fcn = irdma_send_syn_cqp_callback;\n\t\tcqp_request->param = cmnode;\n\t\tif (!wait)\n\t\t\trefcount_inc(&cm_node->refcnt);\n\t}\n\tif (info->ipv4_valid)\n\t\tibdev_dbg(&iwdev->ibdev,\n\t\t\t  \"CM: %s caller: %pS loc_port=0x%04x rem_port=0x%04x loc_addr=%pI4 rem_addr=%pI4 mac=%pM, vlan_id=%d cm_node=%p\\n\",\n\t\t\t  (!mtype) ? \"DELETE\" : \"ADD\",\n\t\t\t  __builtin_return_address(0), info->dest_port,\n\t\t\t  info->src_port, info->dest_ip, info->src_ip,\n\t\t\t  info->mac_addr, cminfo->vlan_id,\n\t\t\t  cmnode ? cmnode : NULL);\n\telse\n\t\tibdev_dbg(&iwdev->ibdev,\n\t\t\t  \"CM: %s caller: %pS loc_port=0x%04x rem_port=0x%04x loc_addr=%pI6 rem_addr=%pI6 mac=%pM, vlan_id=%d cm_node=%p\\n\",\n\t\t\t  (!mtype) ? \"DELETE\" : \"ADD\",\n\t\t\t  __builtin_return_address(0), info->dest_port,\n\t\t\t  info->src_port, info->dest_ip, info->src_ip,\n\t\t\t  info->mac_addr, cminfo->vlan_id,\n\t\t\t  cmnode ? cmnode : NULL);\n\n\tcqp_info->in.u.manage_qhash_table_entry.cqp = &iwdev->rf->cqp.sc_cqp;\n\tcqp_info->in.u.manage_qhash_table_entry.scratch = (uintptr_t)cqp_request;\n\tcqp_info->cqp_cmd = IRDMA_OP_MANAGE_QHASH_TABLE_ENTRY;\n\tcqp_info->post_sq = 1;\n\tstatus = irdma_handle_cqp_op(iwdev->rf, cqp_request);\n\tif (status && cm_node && !wait)\n\t\tirdma_rem_ref_cm_node(cm_node);\n\n\tirdma_put_cqp_request(iwcqp, cqp_request);\n\n\treturn status;\n}\n\n \nstatic void irdma_hw_flush_wqes_callback(struct irdma_cqp_request *cqp_request)\n{\n\tstruct irdma_qp_flush_info *hw_info;\n\tstruct irdma_sc_qp *qp;\n\tstruct irdma_qp *iwqp;\n\tstruct cqp_cmds_info *cqp_info;\n\n\tcqp_info = &cqp_request->info;\n\thw_info = &cqp_info->in.u.qp_flush_wqes.info;\n\tqp = cqp_info->in.u.qp_flush_wqes.qp;\n\tiwqp = qp->qp_uk.back_qp;\n\n\tif (cqp_request->compl_info.maj_err_code)\n\t\treturn;\n\n\tif (hw_info->rq &&\n\t    (cqp_request->compl_info.min_err_code == IRDMA_CQP_COMPL_SQ_WQE_FLUSHED ||\n\t     cqp_request->compl_info.min_err_code == 0)) {\n\t\t \n\t\tqp->qp_uk.rq_flush_complete = true;\n\t}\n\tif (hw_info->sq &&\n\t    (cqp_request->compl_info.min_err_code == IRDMA_CQP_COMPL_RQ_WQE_FLUSHED ||\n\t     cqp_request->compl_info.min_err_code == 0)) {\n\t\tif (IRDMA_RING_MORE_WORK(qp->qp_uk.sq_ring)) {\n\t\t\tibdev_err(&iwqp->iwdev->ibdev, \"Flush QP[%d] failed, SQ has more work\",\n\t\t\t\t  qp->qp_uk.qp_id);\n\t\t\tirdma_ib_qp_event(iwqp, IRDMA_QP_EVENT_CATASTROPHIC);\n\t\t}\n\t\tqp->qp_uk.sq_flush_complete = true;\n\t}\n}\n\n \nint irdma_hw_flush_wqes(struct irdma_pci_f *rf, struct irdma_sc_qp *qp,\n\t\t\tstruct irdma_qp_flush_info *info, bool wait)\n{\n\tint status;\n\tstruct irdma_qp_flush_info *hw_info;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\tstruct irdma_qp *iwqp = qp->qp_uk.back_qp;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, wait);\n\tif (!cqp_request)\n\t\treturn -ENOMEM;\n\n\tcqp_info = &cqp_request->info;\n\tif (!wait)\n\t\tcqp_request->callback_fcn = irdma_hw_flush_wqes_callback;\n\thw_info = &cqp_request->info.in.u.qp_flush_wqes.info;\n\tmemcpy(hw_info, info, sizeof(*hw_info));\n\tcqp_info->cqp_cmd = IRDMA_OP_QP_FLUSH_WQES;\n\tcqp_info->post_sq = 1;\n\tcqp_info->in.u.qp_flush_wqes.qp = qp;\n\tcqp_info->in.u.qp_flush_wqes.scratch = (uintptr_t)cqp_request;\n\tstatus = irdma_handle_cqp_op(rf, cqp_request);\n\tif (status) {\n\t\tqp->qp_uk.sq_flush_complete = true;\n\t\tqp->qp_uk.rq_flush_complete = true;\n\t\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n\t\treturn status;\n\t}\n\n\tif (!wait || cqp_request->compl_info.maj_err_code)\n\t\tgoto put_cqp;\n\n\tif (info->rq) {\n\t\tif (cqp_request->compl_info.min_err_code == IRDMA_CQP_COMPL_SQ_WQE_FLUSHED ||\n\t\t    cqp_request->compl_info.min_err_code == 0) {\n\t\t\t \n\t\t\tqp->qp_uk.rq_flush_complete = true;\n\t\t}\n\t}\n\tif (info->sq) {\n\t\tif (cqp_request->compl_info.min_err_code == IRDMA_CQP_COMPL_RQ_WQE_FLUSHED ||\n\t\t    cqp_request->compl_info.min_err_code == 0) {\n\t\t\t \n\t\t\tif (IRDMA_RING_MORE_WORK(qp->qp_uk.sq_ring)) {\n\t\t\t\tstruct irdma_cqp_request *new_req;\n\n\t\t\t\tif (!qp->qp_uk.sq_flush_complete)\n\t\t\t\t\tgoto put_cqp;\n\t\t\t\tqp->qp_uk.sq_flush_complete = false;\n\t\t\t\tqp->flush_sq = false;\n\n\t\t\t\tinfo->rq = false;\n\t\t\t\tinfo->sq = true;\n\t\t\t\tnew_req = irdma_alloc_and_get_cqp_request(&rf->cqp, true);\n\t\t\t\tif (!new_req) {\n\t\t\t\t\tstatus = -ENOMEM;\n\t\t\t\t\tgoto put_cqp;\n\t\t\t\t}\n\t\t\t\tcqp_info = &new_req->info;\n\t\t\t\thw_info = &new_req->info.in.u.qp_flush_wqes.info;\n\t\t\t\tmemcpy(hw_info, info, sizeof(*hw_info));\n\t\t\t\tcqp_info->cqp_cmd = IRDMA_OP_QP_FLUSH_WQES;\n\t\t\t\tcqp_info->post_sq = 1;\n\t\t\t\tcqp_info->in.u.qp_flush_wqes.qp = qp;\n\t\t\t\tcqp_info->in.u.qp_flush_wqes.scratch = (uintptr_t)new_req;\n\n\t\t\t\tstatus = irdma_handle_cqp_op(rf, new_req);\n\t\t\t\tif (new_req->compl_info.maj_err_code ||\n\t\t\t\t    new_req->compl_info.min_err_code != IRDMA_CQP_COMPL_SQ_WQE_FLUSHED ||\n\t\t\t\t    status) {\n\t\t\t\t\tibdev_err(&iwqp->iwdev->ibdev, \"fatal QP event: SQ in error but not flushed, qp: %d\",\n\t\t\t\t\t\t  iwqp->ibqp.qp_num);\n\t\t\t\t\tqp->qp_uk.sq_flush_complete = false;\n\t\t\t\t\tirdma_ib_qp_event(iwqp, IRDMA_QP_EVENT_CATASTROPHIC);\n\t\t\t\t}\n\t\t\t\tirdma_put_cqp_request(&rf->cqp, new_req);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tqp->qp_uk.sq_flush_complete = true;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!IRDMA_RING_MORE_WORK(qp->qp_uk.sq_ring))\n\t\t\t\tqp->qp_uk.sq_flush_complete = true;\n\t\t}\n\t}\n\n\tibdev_dbg(&rf->iwdev->ibdev,\n\t\t  \"VERBS: qp_id=%d qp_type=%d qpstate=%d ibqpstate=%d last_aeq=%d hw_iw_state=%d maj_err_code=%d min_err_code=%d\\n\",\n\t\t  iwqp->ibqp.qp_num, rf->protocol_used, iwqp->iwarp_state,\n\t\t  iwqp->ibqp_state, iwqp->last_aeq, iwqp->hw_iwarp_state,\n\t\t  cqp_request->compl_info.maj_err_code,\n\t\t  cqp_request->compl_info.min_err_code);\nput_cqp:\n\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n\n\treturn status;\n}\n\n \nvoid irdma_gen_ae(struct irdma_pci_f *rf, struct irdma_sc_qp *qp,\n\t\t  struct irdma_gen_ae_info *info, bool wait)\n{\n\tstruct irdma_gen_ae_info *ae_info;\n\tstruct irdma_cqp_request *cqp_request;\n\tstruct cqp_cmds_info *cqp_info;\n\n\tcqp_request = irdma_alloc_and_get_cqp_request(&rf->cqp, wait);\n\tif (!cqp_request)\n\t\treturn;\n\n\tcqp_info = &cqp_request->info;\n\tae_info = &cqp_request->info.in.u.gen_ae.info;\n\tmemcpy(ae_info, info, sizeof(*ae_info));\n\tcqp_info->cqp_cmd = IRDMA_OP_GEN_AE;\n\tcqp_info->post_sq = 1;\n\tcqp_info->in.u.gen_ae.qp = qp;\n\tcqp_info->in.u.gen_ae.scratch = (uintptr_t)cqp_request;\n\n\tirdma_handle_cqp_op(rf, cqp_request);\n\tirdma_put_cqp_request(&rf->cqp, cqp_request);\n}\n\nvoid irdma_flush_wqes(struct irdma_qp *iwqp, u32 flush_mask)\n{\n\tstruct irdma_qp_flush_info info = {};\n\tstruct irdma_pci_f *rf = iwqp->iwdev->rf;\n\tu8 flush_code = iwqp->sc_qp.flush_code;\n\n\tif (!(flush_mask & IRDMA_FLUSH_SQ) && !(flush_mask & IRDMA_FLUSH_RQ))\n\t\treturn;\n\n\t \n\tinfo.sq = flush_mask & IRDMA_FLUSH_SQ;\n\tinfo.rq = flush_mask & IRDMA_FLUSH_RQ;\n\n\t \n\tinfo.sq_major_code = IRDMA_FLUSH_MAJOR_ERR;\n\tinfo.sq_minor_code = FLUSH_GENERAL_ERR;\n\tinfo.rq_major_code = IRDMA_FLUSH_MAJOR_ERR;\n\tinfo.rq_minor_code = FLUSH_GENERAL_ERR;\n\tinfo.userflushcode = true;\n\n\tif (flush_mask & IRDMA_REFLUSH) {\n\t\tif (info.sq)\n\t\t\tiwqp->sc_qp.flush_sq = false;\n\t\tif (info.rq)\n\t\t\tiwqp->sc_qp.flush_rq = false;\n\t} else {\n\t\tif (flush_code) {\n\t\t\tif (info.sq && iwqp->sc_qp.sq_flush_code)\n\t\t\t\tinfo.sq_minor_code = flush_code;\n\t\t\tif (info.rq && iwqp->sc_qp.rq_flush_code)\n\t\t\t\tinfo.rq_minor_code = flush_code;\n\t\t}\n\t\tif (!iwqp->user_mode)\n\t\t\tqueue_delayed_work(iwqp->iwdev->cleanup_wq,\n\t\t\t\t\t   &iwqp->dwork_flush,\n\t\t\t\t\t   msecs_to_jiffies(IRDMA_FLUSH_DELAY_MS));\n\t}\n\n\t \n\t(void)irdma_hw_flush_wqes(rf, &iwqp->sc_qp, &info,\n\t\t\t\t  flush_mask & IRDMA_FLUSH_WAIT);\n\tiwqp->flush_issued = true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}