{
  "module_name": "puda.c",
  "hash_id": "60f3ed112302eaaaefb815b3ebd1afb81b9d04b78710acebd50d6bf2062854d0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/irdma/puda.c",
  "human_readable_source": "\n \n#include \"osdep.h\"\n#include \"hmc.h\"\n#include \"defs.h\"\n#include \"type.h\"\n#include \"protos.h\"\n#include \"puda.h\"\n#include \"ws.h\"\n\nstatic void irdma_ieq_receive(struct irdma_sc_vsi *vsi,\n\t\t\t      struct irdma_puda_buf *buf);\nstatic void irdma_ieq_tx_compl(struct irdma_sc_vsi *vsi, void *sqwrid);\nstatic void irdma_ilq_putback_rcvbuf(struct irdma_sc_qp *qp,\n\t\t\t\t     struct irdma_puda_buf *buf, u32 wqe_idx);\n \nstatic struct irdma_puda_buf *irdma_puda_get_listbuf(struct list_head *list)\n{\n\tstruct irdma_puda_buf *buf = NULL;\n\n\tif (!list_empty(list)) {\n\t\tbuf = (struct irdma_puda_buf *)list->next;\n\t\tlist_del((struct list_head *)&buf->list);\n\t}\n\n\treturn buf;\n}\n\n \nstruct irdma_puda_buf *irdma_puda_get_bufpool(struct irdma_puda_rsrc *rsrc)\n{\n\tstruct irdma_puda_buf *buf = NULL;\n\tstruct list_head *list = &rsrc->bufpool;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rsrc->bufpool_lock, flags);\n\tbuf = irdma_puda_get_listbuf(list);\n\tif (buf) {\n\t\trsrc->avail_buf_count--;\n\t\tbuf->vsi = rsrc->vsi;\n\t} else {\n\t\trsrc->stats_buf_alloc_fail++;\n\t}\n\tspin_unlock_irqrestore(&rsrc->bufpool_lock, flags);\n\n\treturn buf;\n}\n\n \nvoid irdma_puda_ret_bufpool(struct irdma_puda_rsrc *rsrc,\n\t\t\t    struct irdma_puda_buf *buf)\n{\n\tunsigned long flags;\n\n\tbuf->do_lpb = false;\n\tspin_lock_irqsave(&rsrc->bufpool_lock, flags);\n\tlist_add(&buf->list, &rsrc->bufpool);\n\tspin_unlock_irqrestore(&rsrc->bufpool_lock, flags);\n\trsrc->avail_buf_count++;\n}\n\n \nstatic void irdma_puda_post_recvbuf(struct irdma_puda_rsrc *rsrc, u32 wqe_idx,\n\t\t\t\t    struct irdma_puda_buf *buf, bool initial)\n{\n\t__le64 *wqe;\n\tstruct irdma_sc_qp *qp = &rsrc->qp;\n\tu64 offset24 = 0;\n\n\t \n\tdma_sync_single_for_device(rsrc->dev->hw->device, buf->mem.pa,\n\t\t\t\t   buf->mem.size, DMA_BIDIRECTIONAL);\n\tqp->qp_uk.rq_wrid_array[wqe_idx] = (uintptr_t)buf;\n\twqe = qp->qp_uk.rq_base[wqe_idx].elem;\n\tif (!initial)\n\t\tget_64bit_val(wqe, 24, &offset24);\n\n\toffset24 = (offset24) ? 0 : FIELD_PREP(IRDMAQPSQ_VALID, 1);\n\n\tset_64bit_val(wqe, 16, 0);\n\tset_64bit_val(wqe, 0, buf->mem.pa);\n\tif (qp->qp_uk.uk_attrs->hw_rev == IRDMA_GEN_1) {\n\t\tset_64bit_val(wqe, 8,\n\t\t\t      FIELD_PREP(IRDMAQPSQ_GEN1_FRAG_LEN, buf->mem.size));\n\t} else {\n\t\tset_64bit_val(wqe, 8,\n\t\t\t      FIELD_PREP(IRDMAQPSQ_FRAG_LEN, buf->mem.size) |\n\t\t\t      offset24);\n\t}\n\tdma_wmb();  \n\n\tset_64bit_val(wqe, 24, offset24);\n}\n\n \nstatic int irdma_puda_replenish_rq(struct irdma_puda_rsrc *rsrc, bool initial)\n{\n\tu32 i;\n\tu32 invalid_cnt = rsrc->rxq_invalid_cnt;\n\tstruct irdma_puda_buf *buf = NULL;\n\n\tfor (i = 0; i < invalid_cnt; i++) {\n\t\tbuf = irdma_puda_get_bufpool(rsrc);\n\t\tif (!buf)\n\t\t\treturn -ENOBUFS;\n\t\tirdma_puda_post_recvbuf(rsrc, rsrc->rx_wqe_idx, buf, initial);\n\t\trsrc->rx_wqe_idx = ((rsrc->rx_wqe_idx + 1) % rsrc->rq_size);\n\t\trsrc->rxq_invalid_cnt--;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct irdma_puda_buf *irdma_puda_alloc_buf(struct irdma_sc_dev *dev,\n\t\t\t\t\t\t   u32 len)\n{\n\tstruct irdma_puda_buf *buf;\n\tstruct irdma_virt_mem buf_mem;\n\n\tbuf_mem.size = sizeof(struct irdma_puda_buf);\n\tbuf_mem.va = kzalloc(buf_mem.size, GFP_KERNEL);\n\tif (!buf_mem.va)\n\t\treturn NULL;\n\n\tbuf = buf_mem.va;\n\tbuf->mem.size = len;\n\tbuf->mem.va = kzalloc(buf->mem.size, GFP_KERNEL);\n\tif (!buf->mem.va)\n\t\tgoto free_virt;\n\tbuf->mem.pa = dma_map_single(dev->hw->device, buf->mem.va,\n\t\t\t\t     buf->mem.size, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev->hw->device, buf->mem.pa)) {\n\t\tkfree(buf->mem.va);\n\t\tgoto free_virt;\n\t}\n\n\tbuf->buf_mem.va = buf_mem.va;\n\tbuf->buf_mem.size = buf_mem.size;\n\n\treturn buf;\n\nfree_virt:\n\tkfree(buf_mem.va);\n\treturn NULL;\n}\n\n \nstatic void irdma_puda_dele_buf(struct irdma_sc_dev *dev,\n\t\t\t\tstruct irdma_puda_buf *buf)\n{\n\tdma_unmap_single(dev->hw->device, buf->mem.pa, buf->mem.size,\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(buf->mem.va);\n\tkfree(buf->buf_mem.va);\n}\n\n \nstatic __le64 *irdma_puda_get_next_send_wqe(struct irdma_qp_uk *qp,\n\t\t\t\t\t    u32 *wqe_idx)\n{\n\tint ret_code = 0;\n\n\t*wqe_idx = IRDMA_RING_CURRENT_HEAD(qp->sq_ring);\n\tif (!*wqe_idx)\n\t\tqp->swqe_polarity = !qp->swqe_polarity;\n\tIRDMA_RING_MOVE_HEAD(qp->sq_ring, ret_code);\n\tif (ret_code)\n\t\treturn NULL;\n\n\treturn qp->sq_base[*wqe_idx].elem;\n}\n\n \nstatic int irdma_puda_poll_info(struct irdma_sc_cq *cq,\n\t\t\t\tstruct irdma_puda_cmpl_info *info)\n{\n\tstruct irdma_cq_uk *cq_uk = &cq->cq_uk;\n\tu64 qword0, qword2, qword3, qword6;\n\t__le64 *cqe;\n\t__le64 *ext_cqe = NULL;\n\tu64 qword7 = 0;\n\tu64 comp_ctx;\n\tbool valid_bit;\n\tbool ext_valid = 0;\n\tu32 major_err, minor_err;\n\tu32 peek_head;\n\tbool error;\n\tu8 polarity;\n\n\tcqe = IRDMA_GET_CURRENT_CQ_ELEM(&cq->cq_uk);\n\tget_64bit_val(cqe, 24, &qword3);\n\tvalid_bit = (bool)FIELD_GET(IRDMA_CQ_VALID, qword3);\n\tif (valid_bit != cq_uk->polarity)\n\t\treturn -ENOENT;\n\n\t \n\tdma_rmb();\n\n\tif (cq->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2)\n\t\text_valid = (bool)FIELD_GET(IRDMA_CQ_EXTCQE, qword3);\n\n\tif (ext_valid) {\n\t\tpeek_head = (cq_uk->cq_ring.head + 1) % cq_uk->cq_ring.size;\n\t\text_cqe = cq_uk->cq_base[peek_head].buf;\n\t\tget_64bit_val(ext_cqe, 24, &qword7);\n\t\tpolarity = (u8)FIELD_GET(IRDMA_CQ_VALID, qword7);\n\t\tif (!peek_head)\n\t\t\tpolarity ^= 1;\n\t\tif (polarity != cq_uk->polarity)\n\t\t\treturn -ENOENT;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tIRDMA_RING_MOVE_HEAD_NOCHECK(cq_uk->cq_ring);\n\t\tif (!IRDMA_RING_CURRENT_HEAD(cq_uk->cq_ring))\n\t\t\tcq_uk->polarity = !cq_uk->polarity;\n\t\t \n\t\tIRDMA_RING_MOVE_TAIL(cq_uk->cq_ring);\n\t}\n\n\tprint_hex_dump_debug(\"PUDA: PUDA CQE\", DUMP_PREFIX_OFFSET, 16, 8, cqe,\n\t\t\t     32, false);\n\tif (ext_valid)\n\t\tprint_hex_dump_debug(\"PUDA: PUDA EXT-CQE\", DUMP_PREFIX_OFFSET,\n\t\t\t\t     16, 8, ext_cqe, 32, false);\n\n\terror = (bool)FIELD_GET(IRDMA_CQ_ERROR, qword3);\n\tif (error) {\n\t\tibdev_dbg(to_ibdev(cq->dev), \"PUDA: receive error\\n\");\n\t\tmajor_err = (u32)(FIELD_GET(IRDMA_CQ_MAJERR, qword3));\n\t\tminor_err = (u32)(FIELD_GET(IRDMA_CQ_MINERR, qword3));\n\t\tinfo->compl_error = major_err << 16 | minor_err;\n\t\treturn -EIO;\n\t}\n\n\tget_64bit_val(cqe, 0, &qword0);\n\tget_64bit_val(cqe, 16, &qword2);\n\n\tinfo->q_type = (u8)FIELD_GET(IRDMA_CQ_SQ, qword3);\n\tinfo->qp_id = (u32)FIELD_GET(IRDMACQ_QPID, qword2);\n\tif (cq->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2)\n\t\tinfo->ipv4 = (bool)FIELD_GET(IRDMACQ_IPV4, qword3);\n\n\tget_64bit_val(cqe, 8, &comp_ctx);\n\tinfo->qp = (struct irdma_qp_uk *)(unsigned long)comp_ctx;\n\tinfo->wqe_idx = (u32)FIELD_GET(IRDMA_CQ_WQEIDX, qword3);\n\n\tif (info->q_type == IRDMA_CQE_QTYPE_RQ) {\n\t\tif (ext_valid) {\n\t\t\tinfo->vlan_valid = (bool)FIELD_GET(IRDMA_CQ_UDVLANVALID, qword7);\n\t\t\tif (info->vlan_valid) {\n\t\t\t\tget_64bit_val(ext_cqe, 16, &qword6);\n\t\t\t\tinfo->vlan = (u16)FIELD_GET(IRDMA_CQ_UDVLAN, qword6);\n\t\t\t}\n\t\t\tinfo->smac_valid = (bool)FIELD_GET(IRDMA_CQ_UDSMACVALID, qword7);\n\t\t\tif (info->smac_valid) {\n\t\t\t\tget_64bit_val(ext_cqe, 16, &qword6);\n\t\t\t\tinfo->smac[0] = (u8)((qword6 >> 40) & 0xFF);\n\t\t\t\tinfo->smac[1] = (u8)((qword6 >> 32) & 0xFF);\n\t\t\t\tinfo->smac[2] = (u8)((qword6 >> 24) & 0xFF);\n\t\t\t\tinfo->smac[3] = (u8)((qword6 >> 16) & 0xFF);\n\t\t\t\tinfo->smac[4] = (u8)((qword6 >> 8) & 0xFF);\n\t\t\t\tinfo->smac[5] = (u8)(qword6 & 0xFF);\n\t\t\t}\n\t\t}\n\n\t\tif (cq->dev->hw_attrs.uk_attrs.hw_rev == IRDMA_GEN_1) {\n\t\t\tinfo->vlan_valid = (bool)FIELD_GET(IRDMA_VLAN_TAG_VALID, qword3);\n\t\t\tinfo->l4proto = (u8)FIELD_GET(IRDMA_UDA_L4PROTO, qword2);\n\t\t\tinfo->l3proto = (u8)FIELD_GET(IRDMA_UDA_L3PROTO, qword2);\n\t\t}\n\n\t\tinfo->payload_len = (u32)FIELD_GET(IRDMACQ_PAYLDLEN, qword0);\n\t}\n\n\treturn 0;\n}\n\n \nint irdma_puda_poll_cmpl(struct irdma_sc_dev *dev, struct irdma_sc_cq *cq,\n\t\t\t u32 *compl_err)\n{\n\tstruct irdma_qp_uk *qp;\n\tstruct irdma_cq_uk *cq_uk = &cq->cq_uk;\n\tstruct irdma_puda_cmpl_info info = {};\n\tint ret = 0;\n\tstruct irdma_puda_buf *buf;\n\tstruct irdma_puda_rsrc *rsrc;\n\tu8 cq_type = cq->cq_type;\n\tunsigned long flags;\n\n\tif (cq_type == IRDMA_CQ_TYPE_ILQ || cq_type == IRDMA_CQ_TYPE_IEQ) {\n\t\trsrc = (cq_type == IRDMA_CQ_TYPE_ILQ) ? cq->vsi->ilq :\n\t\t\t\t\t\t\tcq->vsi->ieq;\n\t} else {\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: qp_type error\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = irdma_puda_poll_info(cq, &info);\n\t*compl_err = info.compl_error;\n\tif (ret == -ENOENT)\n\t\treturn ret;\n\tif (ret)\n\t\tgoto done;\n\n\tqp = info.qp;\n\tif (!qp || !rsrc) {\n\t\tret = -EFAULT;\n\t\tgoto done;\n\t}\n\n\tif (qp->qp_id != rsrc->qp_id) {\n\t\tret = -EFAULT;\n\t\tgoto done;\n\t}\n\n\tif (info.q_type == IRDMA_CQE_QTYPE_RQ) {\n\t\tbuf = (struct irdma_puda_buf *)(uintptr_t)\n\t\t\t      qp->rq_wrid_array[info.wqe_idx];\n\n\t\t \n\t\tdma_sync_single_for_cpu(dev->hw->device, buf->mem.pa,\n\t\t\t\t\tbuf->mem.size, DMA_BIDIRECTIONAL);\n\t\t \n\t\tret = irdma_puda_get_tcpip_info(&info, buf);\n\t\tif (ret) {\n\t\t\trsrc->stats_rcvd_pkt_err++;\n\t\t\tif (cq_type == IRDMA_CQ_TYPE_ILQ) {\n\t\t\t\tirdma_ilq_putback_rcvbuf(&rsrc->qp, buf,\n\t\t\t\t\t\t\t info.wqe_idx);\n\t\t\t} else {\n\t\t\t\tirdma_puda_ret_bufpool(rsrc, buf);\n\t\t\t\tirdma_puda_replenish_rq(rsrc, false);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\n\t\trsrc->stats_pkt_rcvd++;\n\t\trsrc->compl_rxwqe_idx = info.wqe_idx;\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: RQ completion\\n\");\n\t\trsrc->receive(rsrc->vsi, buf);\n\t\tif (cq_type == IRDMA_CQ_TYPE_ILQ)\n\t\t\tirdma_ilq_putback_rcvbuf(&rsrc->qp, buf, info.wqe_idx);\n\t\telse\n\t\t\tirdma_puda_replenish_rq(rsrc, false);\n\n\t} else {\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: SQ completion\\n\");\n\t\tbuf = (struct irdma_puda_buf *)(uintptr_t)\n\t\t\t\t\tqp->sq_wrtrk_array[info.wqe_idx].wrid;\n\n\t\t \n\t\tdma_sync_single_for_cpu(dev->hw->device, buf->mem.pa,\n\t\t\t\t\tbuf->mem.size, DMA_BIDIRECTIONAL);\n\t\tIRDMA_RING_SET_TAIL(qp->sq_ring, info.wqe_idx);\n\t\trsrc->xmit_complete(rsrc->vsi, buf);\n\t\tspin_lock_irqsave(&rsrc->bufpool_lock, flags);\n\t\trsrc->tx_wqe_avail_cnt++;\n\t\tspin_unlock_irqrestore(&rsrc->bufpool_lock, flags);\n\t\tif (!list_empty(&rsrc->txpend))\n\t\t\tirdma_puda_send_buf(rsrc, NULL);\n\t}\n\ndone:\n\tIRDMA_RING_MOVE_HEAD_NOCHECK(cq_uk->cq_ring);\n\tif (!IRDMA_RING_CURRENT_HEAD(cq_uk->cq_ring))\n\t\tcq_uk->polarity = !cq_uk->polarity;\n\t \n\tIRDMA_RING_MOVE_TAIL(cq_uk->cq_ring);\n\tset_64bit_val(cq_uk->shadow_area, 0,\n\t\t      IRDMA_RING_CURRENT_HEAD(cq_uk->cq_ring));\n\n\treturn ret;\n}\n\n \nint irdma_puda_send(struct irdma_sc_qp *qp, struct irdma_puda_send_info *info)\n{\n\t__le64 *wqe;\n\tu32 iplen, l4len;\n\tu64 hdr[2];\n\tu32 wqe_idx;\n\tu8 iipt;\n\n\t \n\tl4len = info->tcplen >> 2;\n\tif (info->ipv4) {\n\t\tiipt = 3;\n\t\tiplen = 5;\n\t} else {\n\t\tiipt = 1;\n\t\tiplen = 10;\n\t}\n\n\twqe = irdma_puda_get_next_send_wqe(&qp->qp_uk, &wqe_idx);\n\tif (!wqe)\n\t\treturn -ENOMEM;\n\n\tqp->qp_uk.sq_wrtrk_array[wqe_idx].wrid = (uintptr_t)info->scratch;\n\t \n\t \n\n\tif (qp->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\thdr[0] = 0;  \n\t\thdr[1] = FIELD_PREP(IRDMA_UDA_QPSQ_OPCODE, IRDMA_OP_TYPE_SEND) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_L4LEN, l4len) |\n\t\t\t FIELD_PREP(IRDMAQPSQ_AHID, info->ah_id) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_SIGCOMPL, 1) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_VALID,\n\t\t\t\t    qp->qp_uk.swqe_polarity);\n\n\t\t \n\n\t\tset_64bit_val(wqe, 0, info->paddr);\n\t\tset_64bit_val(wqe, 8,\n\t\t\t      FIELD_PREP(IRDMAQPSQ_FRAG_LEN, info->len) |\n\t\t\t      FIELD_PREP(IRDMA_UDA_QPSQ_VALID, qp->qp_uk.swqe_polarity));\n\t} else {\n\t\thdr[0] = FIELD_PREP(IRDMA_UDA_QPSQ_MACLEN, info->maclen >> 1) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_IPLEN, iplen) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_L4T, 1) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_IIPT, iipt) |\n\t\t\t FIELD_PREP(IRDMA_GEN1_UDA_QPSQ_L4LEN, l4len);\n\n\t\thdr[1] = FIELD_PREP(IRDMA_UDA_QPSQ_OPCODE, IRDMA_OP_TYPE_SEND) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_SIGCOMPL, 1) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_DOLOOPBACK, info->do_lpb) |\n\t\t\t FIELD_PREP(IRDMA_UDA_QPSQ_VALID, qp->qp_uk.swqe_polarity);\n\n\t\t \n\n\t\tset_64bit_val(wqe, 0, info->paddr);\n\t\tset_64bit_val(wqe, 8,\n\t\t\t      FIELD_PREP(IRDMAQPSQ_GEN1_FRAG_LEN, info->len));\n\t}\n\n\tset_64bit_val(wqe, 16, hdr[0]);\n\tdma_wmb();  \n\n\tset_64bit_val(wqe, 24, hdr[1]);\n\n\tprint_hex_dump_debug(\"PUDA: PUDA SEND WQE\", DUMP_PREFIX_OFFSET, 16, 8,\n\t\t\t     wqe, 32, false);\n\tirdma_uk_qp_post_wr(&qp->qp_uk);\n\treturn 0;\n}\n\n \nvoid irdma_puda_send_buf(struct irdma_puda_rsrc *rsrc,\n\t\t\t struct irdma_puda_buf *buf)\n{\n\tstruct irdma_puda_send_info info;\n\tint ret = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rsrc->bufpool_lock, flags);\n\t \n\tif (!rsrc->tx_wqe_avail_cnt || (buf && !list_empty(&rsrc->txpend))) {\n\t\tlist_add_tail(&buf->list, &rsrc->txpend);\n\t\tspin_unlock_irqrestore(&rsrc->bufpool_lock, flags);\n\t\trsrc->stats_sent_pkt_q++;\n\t\tif (rsrc->type == IRDMA_PUDA_RSRC_TYPE_ILQ)\n\t\t\tibdev_dbg(to_ibdev(rsrc->dev),\n\t\t\t\t  \"PUDA: adding to txpend\\n\");\n\t\treturn;\n\t}\n\trsrc->tx_wqe_avail_cnt--;\n\t \n\tif (!buf) {\n\t\tbuf = irdma_puda_get_listbuf(&rsrc->txpend);\n\t\tif (!buf)\n\t\t\tgoto done;\n\t}\n\n\tinfo.scratch = buf;\n\tinfo.paddr = buf->mem.pa;\n\tinfo.len = buf->totallen;\n\tinfo.tcplen = buf->tcphlen;\n\tinfo.ipv4 = buf->ipv4;\n\n\tif (rsrc->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\tinfo.ah_id = buf->ah_id;\n\t} else {\n\t\tinfo.maclen = buf->maclen;\n\t\tinfo.do_lpb = buf->do_lpb;\n\t}\n\n\t \n\tdma_sync_single_for_cpu(rsrc->dev->hw->device, buf->mem.pa,\n\t\t\t\tbuf->mem.size, DMA_BIDIRECTIONAL);\n\tret = irdma_puda_send(&rsrc->qp, &info);\n\tif (ret) {\n\t\trsrc->tx_wqe_avail_cnt++;\n\t\trsrc->stats_sent_pkt_q++;\n\t\tlist_add(&buf->list, &rsrc->txpend);\n\t\tif (rsrc->type == IRDMA_PUDA_RSRC_TYPE_ILQ)\n\t\t\tibdev_dbg(to_ibdev(rsrc->dev),\n\t\t\t\t  \"PUDA: adding to puda_send\\n\");\n\t} else {\n\t\trsrc->stats_pkt_sent++;\n\t}\ndone:\n\tspin_unlock_irqrestore(&rsrc->bufpool_lock, flags);\n}\n\n \nstatic void irdma_puda_qp_setctx(struct irdma_puda_rsrc *rsrc)\n{\n\tstruct irdma_sc_qp *qp = &rsrc->qp;\n\t__le64 *qp_ctx = qp->hw_host_ctx;\n\n\tset_64bit_val(qp_ctx, 8, qp->sq_pa);\n\tset_64bit_val(qp_ctx, 16, qp->rq_pa);\n\tset_64bit_val(qp_ctx, 24,\n\t\t      FIELD_PREP(IRDMAQPC_RQSIZE, qp->hw_rq_size) |\n\t\t      FIELD_PREP(IRDMAQPC_SQSIZE, qp->hw_sq_size));\n\tset_64bit_val(qp_ctx, 48,\n\t\t      FIELD_PREP(IRDMAQPC_SNDMSS, rsrc->buf_size));\n\tset_64bit_val(qp_ctx, 56, 0);\n\tif (qp->dev->hw_attrs.uk_attrs.hw_rev == IRDMA_GEN_1)\n\t\tset_64bit_val(qp_ctx, 64, 1);\n\tset_64bit_val(qp_ctx, 136,\n\t\t      FIELD_PREP(IRDMAQPC_TXCQNUM, rsrc->cq_id) |\n\t\t      FIELD_PREP(IRDMAQPC_RXCQNUM, rsrc->cq_id));\n\tset_64bit_val(qp_ctx, 144,\n\t\t      FIELD_PREP(IRDMAQPC_STAT_INDEX, rsrc->stats_idx));\n\tset_64bit_val(qp_ctx, 160,\n\t\t      FIELD_PREP(IRDMAQPC_PRIVEN, 1) |\n\t\t      FIELD_PREP(IRDMAQPC_USESTATSINSTANCE, rsrc->stats_idx_valid));\n\tset_64bit_val(qp_ctx, 168,\n\t\t      FIELD_PREP(IRDMAQPC_QPCOMPCTX, (uintptr_t)qp));\n\tset_64bit_val(qp_ctx, 176,\n\t\t      FIELD_PREP(IRDMAQPC_SQTPHVAL, qp->sq_tph_val) |\n\t\t      FIELD_PREP(IRDMAQPC_RQTPHVAL, qp->rq_tph_val) |\n\t\t      FIELD_PREP(IRDMAQPC_QSHANDLE, qp->qs_handle));\n\n\tprint_hex_dump_debug(\"PUDA: PUDA QP CONTEXT\", DUMP_PREFIX_OFFSET, 16,\n\t\t\t     8, qp_ctx, IRDMA_QP_CTX_SIZE, false);\n}\n\n \nstatic int irdma_puda_qp_wqe(struct irdma_sc_dev *dev, struct irdma_sc_qp *qp)\n{\n\tstruct irdma_sc_cqp *cqp;\n\t__le64 *wqe;\n\tu64 hdr;\n\tstruct irdma_ccq_cqe_info compl_info;\n\tint status = 0;\n\n\tcqp = dev->cqp;\n\twqe = irdma_sc_cqp_get_next_send_wqe(cqp, 0);\n\tif (!wqe)\n\t\treturn -ENOMEM;\n\n\tset_64bit_val(wqe, 16, qp->hw_host_ctx_pa);\n\tset_64bit_val(wqe, 40, qp->shadow_area_pa);\n\n\thdr = qp->qp_uk.qp_id |\n\t      FIELD_PREP(IRDMA_CQPSQ_OPCODE, IRDMA_CQP_OP_CREATE_QP) |\n\t      FIELD_PREP(IRDMA_CQPSQ_QP_QPTYPE, IRDMA_QP_TYPE_UDA) |\n\t      FIELD_PREP(IRDMA_CQPSQ_QP_CQNUMVALID, 1) |\n\t      FIELD_PREP(IRDMA_CQPSQ_QP_NEXTIWSTATE, 2) |\n\t      FIELD_PREP(IRDMA_CQPSQ_WQEVALID, cqp->polarity);\n\tdma_wmb();  \n\n\tset_64bit_val(wqe, 24, hdr);\n\n\tprint_hex_dump_debug(\"PUDA: PUDA QP CREATE\", DUMP_PREFIX_OFFSET, 16,\n\t\t\t     8, wqe, 40, false);\n\tirdma_sc_cqp_post_sq(cqp);\n\tstatus = irdma_sc_poll_for_cqp_op_done(dev->cqp, IRDMA_CQP_OP_CREATE_QP,\n\t\t\t\t\t       &compl_info);\n\n\treturn status;\n}\n\n \nstatic int irdma_puda_qp_create(struct irdma_puda_rsrc *rsrc)\n{\n\tstruct irdma_sc_qp *qp = &rsrc->qp;\n\tstruct irdma_qp_uk *ukqp = &qp->qp_uk;\n\tint ret = 0;\n\tu32 sq_size, rq_size;\n\tstruct irdma_dma_mem *mem;\n\n\tsq_size = rsrc->sq_size * IRDMA_QP_WQE_MIN_SIZE;\n\trq_size = rsrc->rq_size * IRDMA_QP_WQE_MIN_SIZE;\n\trsrc->qpmem.size = ALIGN((sq_size + rq_size + (IRDMA_SHADOW_AREA_SIZE << 3) + IRDMA_QP_CTX_SIZE),\n\t\t\t\t IRDMA_HW_PAGE_SIZE);\n\trsrc->qpmem.va = dma_alloc_coherent(rsrc->dev->hw->device,\n\t\t\t\t\t    rsrc->qpmem.size, &rsrc->qpmem.pa,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!rsrc->qpmem.va)\n\t\treturn -ENOMEM;\n\n\tmem = &rsrc->qpmem;\n\tmemset(mem->va, 0, rsrc->qpmem.size);\n\tqp->hw_sq_size = irdma_get_encoded_wqe_size(rsrc->sq_size, IRDMA_QUEUE_TYPE_SQ_RQ);\n\tqp->hw_rq_size = irdma_get_encoded_wqe_size(rsrc->rq_size, IRDMA_QUEUE_TYPE_SQ_RQ);\n\tqp->pd = &rsrc->sc_pd;\n\tqp->qp_uk.qp_type = IRDMA_QP_TYPE_UDA;\n\tqp->dev = rsrc->dev;\n\tqp->qp_uk.back_qp = rsrc;\n\tqp->sq_pa = mem->pa;\n\tqp->rq_pa = qp->sq_pa + sq_size;\n\tqp->vsi = rsrc->vsi;\n\tukqp->sq_base = mem->va;\n\tukqp->rq_base = &ukqp->sq_base[rsrc->sq_size];\n\tukqp->shadow_area = ukqp->rq_base[rsrc->rq_size].elem;\n\tukqp->uk_attrs = &qp->dev->hw_attrs.uk_attrs;\n\tqp->shadow_area_pa = qp->rq_pa + rq_size;\n\tqp->hw_host_ctx = ukqp->shadow_area + IRDMA_SHADOW_AREA_SIZE;\n\tqp->hw_host_ctx_pa = qp->shadow_area_pa + (IRDMA_SHADOW_AREA_SIZE << 3);\n\tqp->push_idx = IRDMA_INVALID_PUSH_PAGE_INDEX;\n\tukqp->qp_id = rsrc->qp_id;\n\tukqp->sq_wrtrk_array = rsrc->sq_wrtrk_array;\n\tukqp->rq_wrid_array = rsrc->rq_wrid_array;\n\tukqp->sq_size = rsrc->sq_size;\n\tukqp->rq_size = rsrc->rq_size;\n\n\tIRDMA_RING_INIT(ukqp->sq_ring, ukqp->sq_size);\n\tIRDMA_RING_INIT(ukqp->initial_ring, ukqp->sq_size);\n\tIRDMA_RING_INIT(ukqp->rq_ring, ukqp->rq_size);\n\tukqp->wqe_alloc_db = qp->pd->dev->wqe_alloc_db;\n\n\tret = rsrc->dev->ws_add(qp->vsi, qp->user_pri);\n\tif (ret) {\n\t\tdma_free_coherent(rsrc->dev->hw->device, rsrc->qpmem.size,\n\t\t\t\t  rsrc->qpmem.va, rsrc->qpmem.pa);\n\t\trsrc->qpmem.va = NULL;\n\t\treturn ret;\n\t}\n\n\tirdma_qp_add_qos(qp);\n\tirdma_puda_qp_setctx(rsrc);\n\n\tif (rsrc->dev->ceq_valid)\n\t\tret = irdma_cqp_qp_create_cmd(rsrc->dev, qp);\n\telse\n\t\tret = irdma_puda_qp_wqe(rsrc->dev, qp);\n\tif (ret) {\n\t\tirdma_qp_rem_qos(qp);\n\t\trsrc->dev->ws_remove(qp->vsi, qp->user_pri);\n\t\tdma_free_coherent(rsrc->dev->hw->device, rsrc->qpmem.size,\n\t\t\t\t  rsrc->qpmem.va, rsrc->qpmem.pa);\n\t\trsrc->qpmem.va = NULL;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int irdma_puda_cq_wqe(struct irdma_sc_dev *dev, struct irdma_sc_cq *cq)\n{\n\t__le64 *wqe;\n\tstruct irdma_sc_cqp *cqp;\n\tu64 hdr;\n\tstruct irdma_ccq_cqe_info compl_info;\n\tint status = 0;\n\n\tcqp = dev->cqp;\n\twqe = irdma_sc_cqp_get_next_send_wqe(cqp, 0);\n\tif (!wqe)\n\t\treturn -ENOMEM;\n\n\tset_64bit_val(wqe, 0, cq->cq_uk.cq_size);\n\tset_64bit_val(wqe, 8, (uintptr_t)cq >> 1);\n\tset_64bit_val(wqe, 16,\n\t\t      FIELD_PREP(IRDMA_CQPSQ_CQ_SHADOW_READ_THRESHOLD, cq->shadow_read_threshold));\n\tset_64bit_val(wqe, 32, cq->cq_pa);\n\tset_64bit_val(wqe, 40, cq->shadow_area_pa);\n\tset_64bit_val(wqe, 56,\n\t\t      FIELD_PREP(IRDMA_CQPSQ_TPHVAL, cq->tph_val) |\n\t\t      FIELD_PREP(IRDMA_CQPSQ_VSIIDX, cq->vsi->vsi_idx));\n\n\thdr = cq->cq_uk.cq_id |\n\t      FIELD_PREP(IRDMA_CQPSQ_OPCODE, IRDMA_CQP_OP_CREATE_CQ) |\n\t      FIELD_PREP(IRDMA_CQPSQ_CQ_CHKOVERFLOW, 1) |\n\t      FIELD_PREP(IRDMA_CQPSQ_CQ_ENCEQEMASK, 1) |\n\t      FIELD_PREP(IRDMA_CQPSQ_CQ_CEQIDVALID, 1) |\n\t      FIELD_PREP(IRDMA_CQPSQ_WQEVALID, cqp->polarity);\n\tdma_wmb();  \n\n\tset_64bit_val(wqe, 24, hdr);\n\n\tprint_hex_dump_debug(\"PUDA: PUDA CREATE CQ\", DUMP_PREFIX_OFFSET, 16,\n\t\t\t     8, wqe, IRDMA_CQP_WQE_SIZE * 8, false);\n\tirdma_sc_cqp_post_sq(dev->cqp);\n\tstatus = irdma_sc_poll_for_cqp_op_done(dev->cqp, IRDMA_CQP_OP_CREATE_CQ,\n\t\t\t\t\t       &compl_info);\n\tif (!status) {\n\t\tstruct irdma_sc_ceq *ceq = dev->ceq[0];\n\n\t\tif (ceq && ceq->reg_cq)\n\t\t\tstatus = irdma_sc_add_cq_ctx(ceq, cq);\n\t}\n\n\treturn status;\n}\n\n \nstatic int irdma_puda_cq_create(struct irdma_puda_rsrc *rsrc)\n{\n\tstruct irdma_sc_dev *dev = rsrc->dev;\n\tstruct irdma_sc_cq *cq = &rsrc->cq;\n\tint ret = 0;\n\tu32 cqsize;\n\tstruct irdma_dma_mem *mem;\n\tstruct irdma_cq_init_info info = {};\n\tstruct irdma_cq_uk_init_info *init_info = &info.cq_uk_init_info;\n\n\tcq->vsi = rsrc->vsi;\n\tcqsize = rsrc->cq_size * (sizeof(struct irdma_cqe));\n\trsrc->cqmem.size = ALIGN(cqsize + sizeof(struct irdma_cq_shadow_area),\n\t\t\t\t IRDMA_CQ0_ALIGNMENT);\n\trsrc->cqmem.va = dma_alloc_coherent(dev->hw->device, rsrc->cqmem.size,\n\t\t\t\t\t    &rsrc->cqmem.pa, GFP_KERNEL);\n\tif (!rsrc->cqmem.va)\n\t\treturn -ENOMEM;\n\n\tmem = &rsrc->cqmem;\n\tinfo.dev = dev;\n\tinfo.type = (rsrc->type == IRDMA_PUDA_RSRC_TYPE_ILQ) ?\n\t\t    IRDMA_CQ_TYPE_ILQ : IRDMA_CQ_TYPE_IEQ;\n\tinfo.shadow_read_threshold = rsrc->cq_size >> 2;\n\tinfo.cq_base_pa = mem->pa;\n\tinfo.shadow_area_pa = mem->pa + cqsize;\n\tinit_info->cq_base = mem->va;\n\tinit_info->shadow_area = (__le64 *)((u8 *)mem->va + cqsize);\n\tinit_info->cq_size = rsrc->cq_size;\n\tinit_info->cq_id = rsrc->cq_id;\n\tinfo.ceqe_mask = true;\n\tinfo.ceq_id_valid = true;\n\tinfo.vsi = rsrc->vsi;\n\n\tret = irdma_sc_cq_init(cq, &info);\n\tif (ret)\n\t\tgoto error;\n\n\tif (rsrc->dev->ceq_valid)\n\t\tret = irdma_cqp_cq_create_cmd(dev, cq);\n\telse\n\t\tret = irdma_puda_cq_wqe(dev, cq);\nerror:\n\tif (ret) {\n\t\tdma_free_coherent(dev->hw->device, rsrc->cqmem.size,\n\t\t\t\t  rsrc->cqmem.va, rsrc->cqmem.pa);\n\t\trsrc->cqmem.va = NULL;\n\t}\n\n\treturn ret;\n}\n\n \nstatic void irdma_puda_free_qp(struct irdma_puda_rsrc *rsrc)\n{\n\tint ret;\n\tstruct irdma_ccq_cqe_info compl_info;\n\tstruct irdma_sc_dev *dev = rsrc->dev;\n\n\tif (rsrc->dev->ceq_valid) {\n\t\tirdma_cqp_qp_destroy_cmd(dev, &rsrc->qp);\n\t\trsrc->dev->ws_remove(rsrc->qp.vsi, rsrc->qp.user_pri);\n\t\treturn;\n\t}\n\n\tret = irdma_sc_qp_destroy(&rsrc->qp, 0, false, true, true);\n\tif (ret)\n\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t  \"PUDA: error puda qp destroy wqe, status = %d\\n\",\n\t\t\t  ret);\n\tif (!ret) {\n\t\tret = irdma_sc_poll_for_cqp_op_done(dev->cqp, IRDMA_CQP_OP_DESTROY_QP,\n\t\t\t\t\t\t    &compl_info);\n\t\tif (ret)\n\t\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t\t  \"PUDA: error puda qp destroy failed, status = %d\\n\",\n\t\t\t\t  ret);\n\t}\n\trsrc->dev->ws_remove(rsrc->qp.vsi, rsrc->qp.user_pri);\n}\n\n \nstatic void irdma_puda_free_cq(struct irdma_puda_rsrc *rsrc)\n{\n\tint ret;\n\tstruct irdma_ccq_cqe_info compl_info;\n\tstruct irdma_sc_dev *dev = rsrc->dev;\n\n\tif (rsrc->dev->ceq_valid) {\n\t\tirdma_cqp_cq_destroy_cmd(dev, &rsrc->cq);\n\t\treturn;\n\t}\n\n\tret = irdma_sc_cq_destroy(&rsrc->cq, 0, true);\n\tif (ret)\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: error ieq cq destroy\\n\");\n\tif (!ret) {\n\t\tret = irdma_sc_poll_for_cqp_op_done(dev->cqp, IRDMA_CQP_OP_DESTROY_CQ,\n\t\t\t\t\t\t    &compl_info);\n\t\tif (ret)\n\t\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t\t  \"PUDA: error ieq qp destroy done\\n\");\n\t}\n}\n\n \nvoid irdma_puda_dele_rsrc(struct irdma_sc_vsi *vsi, enum puda_rsrc_type type,\n\t\t\t  bool reset)\n{\n\tstruct irdma_sc_dev *dev = vsi->dev;\n\tstruct irdma_puda_rsrc *rsrc;\n\tstruct irdma_puda_buf *buf = NULL;\n\tstruct irdma_puda_buf *nextbuf = NULL;\n\tstruct irdma_virt_mem *vmem;\n\tstruct irdma_sc_ceq *ceq;\n\n\tceq = vsi->dev->ceq[0];\n\tswitch (type) {\n\tcase IRDMA_PUDA_RSRC_TYPE_ILQ:\n\t\trsrc = vsi->ilq;\n\t\tvmem = &vsi->ilq_mem;\n\t\tvsi->ilq = NULL;\n\t\tif (ceq && ceq->reg_cq)\n\t\t\tirdma_sc_remove_cq_ctx(ceq, &rsrc->cq);\n\t\tbreak;\n\tcase IRDMA_PUDA_RSRC_TYPE_IEQ:\n\t\trsrc = vsi->ieq;\n\t\tvmem = &vsi->ieq_mem;\n\t\tvsi->ieq = NULL;\n\t\tif (ceq && ceq->reg_cq)\n\t\t\tirdma_sc_remove_cq_ctx(ceq, &rsrc->cq);\n\t\tbreak;\n\tdefault:\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: error resource type = 0x%x\\n\",\n\t\t\t  type);\n\t\treturn;\n\t}\n\n\tswitch (rsrc->cmpl) {\n\tcase PUDA_HASH_CRC_COMPLETE:\n\t\tirdma_free_hash_desc(rsrc->hash_desc);\n\t\tfallthrough;\n\tcase PUDA_QP_CREATED:\n\t\tirdma_qp_rem_qos(&rsrc->qp);\n\n\t\tif (!reset)\n\t\t\tirdma_puda_free_qp(rsrc);\n\n\t\tdma_free_coherent(dev->hw->device, rsrc->qpmem.size,\n\t\t\t\t  rsrc->qpmem.va, rsrc->qpmem.pa);\n\t\trsrc->qpmem.va = NULL;\n\t\tfallthrough;\n\tcase PUDA_CQ_CREATED:\n\t\tif (!reset)\n\t\t\tirdma_puda_free_cq(rsrc);\n\n\t\tdma_free_coherent(dev->hw->device, rsrc->cqmem.size,\n\t\t\t\t  rsrc->cqmem.va, rsrc->cqmem.pa);\n\t\trsrc->cqmem.va = NULL;\n\t\tbreak;\n\tdefault:\n\t\tibdev_dbg(to_ibdev(rsrc->dev), \"PUDA: error no resources\\n\");\n\t\tbreak;\n\t}\n\t \n\tbuf = rsrc->alloclist;\n\twhile (buf) {\n\t\tnextbuf = buf->next;\n\t\tirdma_puda_dele_buf(dev, buf);\n\t\tbuf = nextbuf;\n\t\trsrc->alloc_buf_count--;\n\t}\n\n\tkfree(vmem->va);\n}\n\n \nstatic int irdma_puda_allocbufs(struct irdma_puda_rsrc *rsrc, u32 count)\n{\n\tu32 i;\n\tstruct irdma_puda_buf *buf;\n\tstruct irdma_puda_buf *nextbuf;\n\n\tfor (i = 0; i < count; i++) {\n\t\tbuf = irdma_puda_alloc_buf(rsrc->dev, rsrc->buf_size);\n\t\tif (!buf) {\n\t\t\trsrc->stats_buf_alloc_fail++;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tirdma_puda_ret_bufpool(rsrc, buf);\n\t\trsrc->alloc_buf_count++;\n\t\tif (!rsrc->alloclist) {\n\t\t\trsrc->alloclist = buf;\n\t\t} else {\n\t\t\tnextbuf = rsrc->alloclist;\n\t\t\trsrc->alloclist = buf;\n\t\t\tbuf->next = nextbuf;\n\t\t}\n\t}\n\n\trsrc->avail_buf_count = rsrc->alloc_buf_count;\n\n\treturn 0;\n}\n\n \nint irdma_puda_create_rsrc(struct irdma_sc_vsi *vsi,\n\t\t\t   struct irdma_puda_rsrc_info *info)\n{\n\tstruct irdma_sc_dev *dev = vsi->dev;\n\tint ret = 0;\n\tstruct irdma_puda_rsrc *rsrc;\n\tu32 pudasize;\n\tu32 sqwridsize, rqwridsize;\n\tstruct irdma_virt_mem *vmem;\n\n\tinfo->count = 1;\n\tpudasize = sizeof(struct irdma_puda_rsrc);\n\tsqwridsize = info->sq_size * sizeof(struct irdma_sq_uk_wr_trk_info);\n\trqwridsize = info->rq_size * 8;\n\tswitch (info->type) {\n\tcase IRDMA_PUDA_RSRC_TYPE_ILQ:\n\t\tvmem = &vsi->ilq_mem;\n\t\tbreak;\n\tcase IRDMA_PUDA_RSRC_TYPE_IEQ:\n\t\tvmem = &vsi->ieq_mem;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\tvmem->size = pudasize + sqwridsize + rqwridsize;\n\tvmem->va = kzalloc(vmem->size, GFP_KERNEL);\n\tif (!vmem->va)\n\t\treturn -ENOMEM;\n\n\trsrc = vmem->va;\n\tspin_lock_init(&rsrc->bufpool_lock);\n\tswitch (info->type) {\n\tcase IRDMA_PUDA_RSRC_TYPE_ILQ:\n\t\tvsi->ilq = vmem->va;\n\t\tvsi->ilq_count = info->count;\n\t\trsrc->receive = info->receive;\n\t\trsrc->xmit_complete = info->xmit_complete;\n\t\tbreak;\n\tcase IRDMA_PUDA_RSRC_TYPE_IEQ:\n\t\tvsi->ieq_count = info->count;\n\t\tvsi->ieq = vmem->va;\n\t\trsrc->receive = irdma_ieq_receive;\n\t\trsrc->xmit_complete = irdma_ieq_tx_compl;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\trsrc->type = info->type;\n\trsrc->sq_wrtrk_array = (struct irdma_sq_uk_wr_trk_info *)\n\t\t\t       ((u8 *)vmem->va + pudasize);\n\trsrc->rq_wrid_array = (u64 *)((u8 *)vmem->va + pudasize + sqwridsize);\n\t \n\tINIT_LIST_HEAD(&rsrc->bufpool);\n\tINIT_LIST_HEAD(&rsrc->txpend);\n\n\trsrc->tx_wqe_avail_cnt = info->sq_size - 1;\n\tirdma_sc_pd_init(dev, &rsrc->sc_pd, info->pd_id, info->abi_ver);\n\trsrc->qp_id = info->qp_id;\n\trsrc->cq_id = info->cq_id;\n\trsrc->sq_size = info->sq_size;\n\trsrc->rq_size = info->rq_size;\n\trsrc->cq_size = info->rq_size + info->sq_size;\n\tif (dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\tif (rsrc->type == IRDMA_PUDA_RSRC_TYPE_ILQ)\n\t\t\trsrc->cq_size += info->rq_size;\n\t}\n\trsrc->buf_size = info->buf_size;\n\trsrc->dev = dev;\n\trsrc->vsi = vsi;\n\trsrc->stats_idx = info->stats_idx;\n\trsrc->stats_idx_valid = info->stats_idx_valid;\n\n\tret = irdma_puda_cq_create(rsrc);\n\tif (!ret) {\n\t\trsrc->cmpl = PUDA_CQ_CREATED;\n\t\tret = irdma_puda_qp_create(rsrc);\n\t}\n\tif (ret) {\n\t\tibdev_dbg(to_ibdev(dev),\n\t\t\t  \"PUDA: error qp_create type=%d, status=%d\\n\",\n\t\t\t  rsrc->type, ret);\n\t\tgoto error;\n\t}\n\trsrc->cmpl = PUDA_QP_CREATED;\n\n\tret = irdma_puda_allocbufs(rsrc, info->tx_buf_cnt + info->rq_size);\n\tif (ret) {\n\t\tibdev_dbg(to_ibdev(dev), \"PUDA: error alloc_buf\\n\");\n\t\tgoto error;\n\t}\n\n\trsrc->rxq_invalid_cnt = info->rq_size;\n\tret = irdma_puda_replenish_rq(rsrc, true);\n\tif (ret)\n\t\tgoto error;\n\n\tif (info->type == IRDMA_PUDA_RSRC_TYPE_IEQ) {\n\t\tif (!irdma_init_hash_desc(&rsrc->hash_desc)) {\n\t\t\trsrc->check_crc = true;\n\t\t\trsrc->cmpl = PUDA_HASH_CRC_COMPLETE;\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\tirdma_sc_ccq_arm(&rsrc->cq);\n\treturn ret;\n\nerror:\n\tirdma_puda_dele_rsrc(vsi, info->type, false);\n\n\treturn ret;\n}\n\n \nstatic void irdma_ilq_putback_rcvbuf(struct irdma_sc_qp *qp,\n\t\t\t\t     struct irdma_puda_buf *buf, u32 wqe_idx)\n{\n\t__le64 *wqe;\n\tu64 offset8, offset24;\n\n\t \n\tdma_sync_single_for_device(qp->dev->hw->device, buf->mem.pa,\n\t\t\t\t   buf->mem.size, DMA_BIDIRECTIONAL);\n\twqe = qp->qp_uk.rq_base[wqe_idx].elem;\n\tget_64bit_val(wqe, 24, &offset24);\n\tif (qp->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\tget_64bit_val(wqe, 8, &offset8);\n\t\tif (offset24)\n\t\t\toffset8 &= ~FIELD_PREP(IRDMAQPSQ_VALID, 1);\n\t\telse\n\t\t\toffset8 |= FIELD_PREP(IRDMAQPSQ_VALID, 1);\n\t\tset_64bit_val(wqe, 8, offset8);\n\t\tdma_wmb();  \n\t}\n\tif (offset24)\n\t\toffset24 = 0;\n\telse\n\t\toffset24 = FIELD_PREP(IRDMAQPSQ_VALID, 1);\n\n\tset_64bit_val(wqe, 24, offset24);\n}\n\n \nstatic u16 irdma_ieq_get_fpdu_len(struct irdma_pfpdu *pfpdu, u8 *datap,\n\t\t\t\t  u32 rcv_seq)\n{\n\tu32 marker_seq, end_seq, blk_start;\n\tu8 marker_len = pfpdu->marker_len;\n\tu16 total_len = 0;\n\tu16 fpdu_len;\n\n\tblk_start = (pfpdu->rcv_start_seq - rcv_seq) & (IRDMA_MRK_BLK_SZ - 1);\n\tif (!blk_start) {\n\t\ttotal_len = marker_len;\n\t\tmarker_seq = rcv_seq + IRDMA_MRK_BLK_SZ;\n\t\tif (marker_len && *(u32 *)datap)\n\t\t\treturn 0;\n\t} else {\n\t\tmarker_seq = rcv_seq + blk_start;\n\t}\n\n\tdatap += total_len;\n\tfpdu_len = ntohs(*(__be16 *)datap);\n\tfpdu_len += IRDMA_IEQ_MPA_FRAMING;\n\tfpdu_len = (fpdu_len + 3) & 0xfffc;\n\n\tif (fpdu_len > pfpdu->max_fpdu_data)\n\t\treturn 0;\n\n\ttotal_len += fpdu_len;\n\tend_seq = rcv_seq + total_len;\n\twhile ((int)(marker_seq - end_seq) < 0) {\n\t\ttotal_len += marker_len;\n\t\tend_seq += marker_len;\n\t\tmarker_seq += IRDMA_MRK_BLK_SZ;\n\t}\n\n\treturn total_len;\n}\n\n \nstatic void irdma_ieq_copy_to_txbuf(struct irdma_puda_buf *buf,\n\t\t\t\t    struct irdma_puda_buf *txbuf,\n\t\t\t\t    u16 buf_offset, u32 txbuf_offset, u32 len)\n{\n\tvoid *mem1 = (u8 *)buf->mem.va + buf_offset;\n\tvoid *mem2 = (u8 *)txbuf->mem.va + txbuf_offset;\n\n\tmemcpy(mem2, mem1, len);\n}\n\n \nstatic void irdma_ieq_setup_tx_buf(struct irdma_puda_buf *buf,\n\t\t\t\t   struct irdma_puda_buf *txbuf)\n{\n\ttxbuf->tcphlen = buf->tcphlen;\n\ttxbuf->ipv4 = buf->ipv4;\n\n\tif (buf->vsi->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\ttxbuf->hdrlen = txbuf->tcphlen;\n\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, IRDMA_TCP_OFFSET, 0,\n\t\t\t\t\ttxbuf->hdrlen);\n\t} else {\n\t\ttxbuf->maclen = buf->maclen;\n\t\ttxbuf->hdrlen = buf->hdrlen;\n\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, 0, 0, buf->hdrlen);\n\t}\n}\n\n \nstatic void irdma_ieq_check_first_buf(struct irdma_puda_buf *buf, u32 fps)\n{\n\tu32 offset;\n\n\tif (buf->seqnum < fps) {\n\t\toffset = fps - buf->seqnum;\n\t\tif (offset > buf->datalen)\n\t\t\treturn;\n\t\tbuf->data += offset;\n\t\tbuf->datalen -= (u16)offset;\n\t\tbuf->seqnum = fps;\n\t}\n}\n\n \nstatic void irdma_ieq_compl_pfpdu(struct irdma_puda_rsrc *ieq,\n\t\t\t\t  struct list_head *rxlist,\n\t\t\t\t  struct list_head *pbufl,\n\t\t\t\t  struct irdma_puda_buf *txbuf, u16 fpdu_len)\n{\n\tstruct irdma_puda_buf *buf;\n\tu32 nextseqnum;\n\tu16 txoffset, bufoffset;\n\n\tbuf = irdma_puda_get_listbuf(pbufl);\n\tif (!buf)\n\t\treturn;\n\n\tnextseqnum = buf->seqnum + fpdu_len;\n\tirdma_ieq_setup_tx_buf(buf, txbuf);\n\tif (buf->vsi->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\ttxoffset = txbuf->hdrlen;\n\t\ttxbuf->totallen = txbuf->hdrlen + fpdu_len;\n\t\ttxbuf->data = (u8 *)txbuf->mem.va + txoffset;\n\t} else {\n\t\ttxoffset = buf->hdrlen;\n\t\ttxbuf->totallen = buf->hdrlen + fpdu_len;\n\t\ttxbuf->data = (u8 *)txbuf->mem.va + buf->hdrlen;\n\t}\n\tbufoffset = (u16)(buf->data - (u8 *)buf->mem.va);\n\n\tdo {\n\t\tif (buf->datalen >= fpdu_len) {\n\t\t\t \n\t\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, bufoffset, txoffset,\n\t\t\t\t\t\tfpdu_len);\n\t\t\tbuf->datalen -= fpdu_len;\n\t\t\tbuf->data += fpdu_len;\n\t\t\tbuf->seqnum = nextseqnum;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, bufoffset, txoffset,\n\t\t\t\t\tbuf->datalen);\n\t\ttxoffset += buf->datalen;\n\t\tfpdu_len -= buf->datalen;\n\t\tirdma_puda_ret_bufpool(ieq, buf);\n\t\tbuf = irdma_puda_get_listbuf(pbufl);\n\t\tif (!buf)\n\t\t\treturn;\n\n\t\tbufoffset = (u16)(buf->data - (u8 *)buf->mem.va);\n\t} while (1);\n\n\t \n\tif (buf->datalen)\n\t\tlist_add(&buf->list, rxlist);\n\telse\n\t\tirdma_puda_ret_bufpool(ieq, buf);\n}\n\n \nstatic int irdma_ieq_create_pbufl(struct irdma_pfpdu *pfpdu,\n\t\t\t\t  struct list_head *rxlist,\n\t\t\t\t  struct list_head *pbufl,\n\t\t\t\t  struct irdma_puda_buf *buf, u16 fpdu_len)\n{\n\tint status = 0;\n\tstruct irdma_puda_buf *nextbuf;\n\tu32 nextseqnum;\n\tu16 plen = fpdu_len - buf->datalen;\n\tbool done = false;\n\n\tnextseqnum = buf->seqnum + buf->datalen;\n\tdo {\n\t\tnextbuf = irdma_puda_get_listbuf(rxlist);\n\t\tif (!nextbuf) {\n\t\t\tstatus = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add_tail(&nextbuf->list, pbufl);\n\t\tif (nextbuf->seqnum != nextseqnum) {\n\t\t\tpfpdu->bad_seq_num++;\n\t\t\tstatus = -ERANGE;\n\t\t\tbreak;\n\t\t}\n\t\tif (nextbuf->datalen >= plen) {\n\t\t\tdone = true;\n\t\t} else {\n\t\t\tplen -= nextbuf->datalen;\n\t\t\tnextseqnum = nextbuf->seqnum + nextbuf->datalen;\n\t\t}\n\n\t} while (!done);\n\n\treturn status;\n}\n\n \nstatic int irdma_ieq_handle_partial(struct irdma_puda_rsrc *ieq,\n\t\t\t\t    struct irdma_pfpdu *pfpdu,\n\t\t\t\t    struct irdma_puda_buf *buf, u16 fpdu_len)\n{\n\tint status = 0;\n\tu8 *crcptr;\n\tu32 mpacrc;\n\tu32 seqnum = buf->seqnum;\n\tstruct list_head pbufl;  \n\tstruct irdma_puda_buf *txbuf = NULL;\n\tstruct list_head *rxlist = &pfpdu->rxlist;\n\n\tieq->partials_handled++;\n\n\tINIT_LIST_HEAD(&pbufl);\n\tlist_add(&buf->list, &pbufl);\n\n\tstatus = irdma_ieq_create_pbufl(pfpdu, rxlist, &pbufl, buf, fpdu_len);\n\tif (status)\n\t\tgoto error;\n\n\ttxbuf = irdma_puda_get_bufpool(ieq);\n\tif (!txbuf) {\n\t\tpfpdu->no_tx_bufs++;\n\t\tstatus = -ENOBUFS;\n\t\tgoto error;\n\t}\n\n\tirdma_ieq_compl_pfpdu(ieq, rxlist, &pbufl, txbuf, fpdu_len);\n\tirdma_ieq_update_tcpip_info(txbuf, fpdu_len, seqnum);\n\n\tcrcptr = txbuf->data + fpdu_len - 4;\n\tmpacrc = *(u32 *)crcptr;\n\tif (ieq->check_crc) {\n\t\tstatus = irdma_ieq_check_mpacrc(ieq->hash_desc, txbuf->data,\n\t\t\t\t\t\t(fpdu_len - 4), mpacrc);\n\t\tif (status) {\n\t\t\tibdev_dbg(to_ibdev(ieq->dev), \"IEQ: error bad crc\\n\");\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tprint_hex_dump_debug(\"IEQ: IEQ TX BUFFER\", DUMP_PREFIX_OFFSET, 16, 8,\n\t\t\t     txbuf->mem.va, txbuf->totallen, false);\n\tif (ieq->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2)\n\t\ttxbuf->ah_id = pfpdu->ah->ah_info.ah_idx;\n\ttxbuf->do_lpb = true;\n\tirdma_puda_send_buf(ieq, txbuf);\n\tpfpdu->rcv_nxt = seqnum + fpdu_len;\n\treturn status;\n\nerror:\n\twhile (!list_empty(&pbufl)) {\n\t\tbuf = list_last_entry(&pbufl, struct irdma_puda_buf, list);\n\t\tlist_move(&buf->list, rxlist);\n\t}\n\tif (txbuf)\n\t\tirdma_puda_ret_bufpool(ieq, txbuf);\n\n\treturn status;\n}\n\n \nstatic int irdma_ieq_process_buf(struct irdma_puda_rsrc *ieq,\n\t\t\t\t struct irdma_pfpdu *pfpdu,\n\t\t\t\t struct irdma_puda_buf *buf)\n{\n\tu16 fpdu_len = 0;\n\tu16 datalen = buf->datalen;\n\tu8 *datap = buf->data;\n\tu8 *crcptr;\n\tu16 ioffset = 0;\n\tu32 mpacrc;\n\tu32 seqnum = buf->seqnum;\n\tu16 len = 0;\n\tu16 full = 0;\n\tbool partial = false;\n\tstruct irdma_puda_buf *txbuf;\n\tstruct list_head *rxlist = &pfpdu->rxlist;\n\tint ret = 0;\n\n\tioffset = (u16)(buf->data - (u8 *)buf->mem.va);\n\twhile (datalen) {\n\t\tfpdu_len = irdma_ieq_get_fpdu_len(pfpdu, datap, buf->seqnum);\n\t\tif (!fpdu_len) {\n\t\t\tibdev_dbg(to_ibdev(ieq->dev),\n\t\t\t\t  \"IEQ: error bad fpdu len\\n\");\n\t\t\tlist_add(&buf->list, rxlist);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (datalen < fpdu_len) {\n\t\t\tpartial = true;\n\t\t\tbreak;\n\t\t}\n\t\tcrcptr = datap + fpdu_len - 4;\n\t\tmpacrc = *(u32 *)crcptr;\n\t\tif (ieq->check_crc)\n\t\t\tret = irdma_ieq_check_mpacrc(ieq->hash_desc, datap,\n\t\t\t\t\t\t     fpdu_len - 4, mpacrc);\n\t\tif (ret) {\n\t\t\tlist_add(&buf->list, rxlist);\n\t\t\tibdev_dbg(to_ibdev(ieq->dev),\n\t\t\t\t  \"ERR: IRDMA_ERR_MPA_CRC\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfull++;\n\t\tpfpdu->fpdu_processed++;\n\t\tieq->fpdu_processed++;\n\t\tdatap += fpdu_len;\n\t\tlen += fpdu_len;\n\t\tdatalen -= fpdu_len;\n\t}\n\tif (full) {\n\t\t \n\t\ttxbuf = irdma_puda_get_bufpool(ieq);\n\t\tif (!txbuf) {\n\t\t\tpfpdu->no_tx_bufs++;\n\t\t\tlist_add(&buf->list, rxlist);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t\t \n\t\tirdma_ieq_setup_tx_buf(buf, txbuf);\n\t\t \n\t\tif (ieq->dev->hw_attrs.uk_attrs.hw_rev >= IRDMA_GEN_2) {\n\t\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, ioffset,\n\t\t\t\t\t\ttxbuf->hdrlen, len);\n\t\t\ttxbuf->totallen = txbuf->hdrlen + len;\n\t\t\ttxbuf->ah_id = pfpdu->ah->ah_info.ah_idx;\n\t\t} else {\n\t\t\tirdma_ieq_copy_to_txbuf(buf, txbuf, ioffset,\n\t\t\t\t\t\tbuf->hdrlen, len);\n\t\t\ttxbuf->totallen = buf->hdrlen + len;\n\t\t}\n\t\tirdma_ieq_update_tcpip_info(txbuf, len, buf->seqnum);\n\t\tprint_hex_dump_debug(\"IEQ: IEQ TX BUFFER\", DUMP_PREFIX_OFFSET,\n\t\t\t\t     16, 8, txbuf->mem.va, txbuf->totallen,\n\t\t\t\t     false);\n\t\ttxbuf->do_lpb = true;\n\t\tirdma_puda_send_buf(ieq, txbuf);\n\n\t\tif (!datalen) {\n\t\t\tpfpdu->rcv_nxt = buf->seqnum + len;\n\t\t\tirdma_puda_ret_bufpool(ieq, buf);\n\t\t\treturn 0;\n\t\t}\n\t\tbuf->data = datap;\n\t\tbuf->seqnum = seqnum + len;\n\t\tbuf->datalen = datalen;\n\t\tpfpdu->rcv_nxt = buf->seqnum;\n\t}\n\tif (partial)\n\t\treturn irdma_ieq_handle_partial(ieq, pfpdu, buf, fpdu_len);\n\n\treturn 0;\n}\n\n \nvoid irdma_ieq_process_fpdus(struct irdma_sc_qp *qp,\n\t\t\t     struct irdma_puda_rsrc *ieq)\n{\n\tstruct irdma_pfpdu *pfpdu = &qp->pfpdu;\n\tstruct list_head *rxlist = &pfpdu->rxlist;\n\tstruct irdma_puda_buf *buf;\n\tint status;\n\n\tdo {\n\t\tif (list_empty(rxlist))\n\t\t\tbreak;\n\t\tbuf = irdma_puda_get_listbuf(rxlist);\n\t\tif (!buf) {\n\t\t\tibdev_dbg(to_ibdev(ieq->dev), \"IEQ: error no buf\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tif (buf->seqnum != pfpdu->rcv_nxt) {\n\t\t\t \n\t\t\tpfpdu->out_of_order++;\n\t\t\tlist_add(&buf->list, rxlist);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tstatus = irdma_ieq_process_buf(ieq, pfpdu, buf);\n\t\tif (status == -EINVAL) {\n\t\t\tpfpdu->mpa_crc_err = true;\n\t\t\twhile (!list_empty(rxlist)) {\n\t\t\t\tbuf = irdma_puda_get_listbuf(rxlist);\n\t\t\t\tirdma_puda_ret_bufpool(ieq, buf);\n\t\t\t\tpfpdu->crc_err++;\n\t\t\t\tieq->crc_err++;\n\t\t\t}\n\t\t\t \n\t\t\tirdma_ieq_mpa_crc_ae(ieq->dev, qp);\n\t\t}\n\t} while (!status);\n}\n\n \nstatic int irdma_ieq_create_ah(struct irdma_sc_qp *qp, struct irdma_puda_buf *buf)\n{\n\tstruct irdma_ah_info ah_info = {};\n\n\tqp->pfpdu.ah_buf = buf;\n\tirdma_puda_ieq_get_ah_info(qp, &ah_info);\n\treturn irdma_puda_create_ah(qp->vsi->dev, &ah_info, false,\n\t\t\t\t    IRDMA_PUDA_RSRC_TYPE_IEQ, qp,\n\t\t\t\t    &qp->pfpdu.ah);\n}\n\n \nstatic void irdma_ieq_handle_exception(struct irdma_puda_rsrc *ieq,\n\t\t\t\t       struct irdma_sc_qp *qp,\n\t\t\t\t       struct irdma_puda_buf *buf)\n{\n\tstruct irdma_pfpdu *pfpdu = &qp->pfpdu;\n\tu32 *hw_host_ctx = (u32 *)qp->hw_host_ctx;\n\tu32 rcv_wnd = hw_host_ctx[23];\n\t \n\tu32 fps = *(u32 *)(qp->q2_buf + Q2_FPSN_OFFSET);\n\tstruct list_head *rxlist = &pfpdu->rxlist;\n\tunsigned long flags = 0;\n\tu8 hw_rev = qp->dev->hw_attrs.uk_attrs.hw_rev;\n\n\tprint_hex_dump_debug(\"IEQ: IEQ RX BUFFER\", DUMP_PREFIX_OFFSET, 16, 8,\n\t\t\t     buf->mem.va, buf->totallen, false);\n\n\tspin_lock_irqsave(&pfpdu->lock, flags);\n\tpfpdu->total_ieq_bufs++;\n\tif (pfpdu->mpa_crc_err) {\n\t\tpfpdu->crc_err++;\n\t\tgoto error;\n\t}\n\tif (pfpdu->mode && fps != pfpdu->fps) {\n\t\t \n\t\tirdma_ieq_cleanup_qp(ieq, qp);\n\t\tibdev_dbg(to_ibdev(ieq->dev), \"IEQ: restarting new partial\\n\");\n\t\tpfpdu->mode = false;\n\t}\n\n\tif (!pfpdu->mode) {\n\t\tprint_hex_dump_debug(\"IEQ: Q2 BUFFER\", DUMP_PREFIX_OFFSET, 16,\n\t\t\t\t     8, (u64 *)qp->q2_buf, 128, false);\n\t\t \n\t\tpfpdu->rcv_nxt = fps;\n\t\tpfpdu->fps = fps;\n\t\tpfpdu->mode = true;\n\t\tpfpdu->max_fpdu_data = (buf->ipv4) ?\n\t\t\t\t       (ieq->vsi->mtu - IRDMA_MTU_TO_MSS_IPV4) :\n\t\t\t\t       (ieq->vsi->mtu - IRDMA_MTU_TO_MSS_IPV6);\n\t\tpfpdu->pmode_count++;\n\t\tieq->pmode_count++;\n\t\tINIT_LIST_HEAD(rxlist);\n\t\tirdma_ieq_check_first_buf(buf, fps);\n\t}\n\n\tif (!(rcv_wnd >= (buf->seqnum - pfpdu->rcv_nxt))) {\n\t\tpfpdu->bad_seq_num++;\n\t\tieq->bad_seq_num++;\n\t\tgoto error;\n\t}\n\n\tif (!list_empty(rxlist)) {\n\t\tif (buf->seqnum != pfpdu->nextseqnum) {\n\t\t\tirdma_send_ieq_ack(qp);\n\t\t\t \n\t\t\tgoto error;\n\t\t}\n\t}\n\t \n\tlist_add_tail(&buf->list, rxlist);\n\tpfpdu->nextseqnum = buf->seqnum + buf->datalen;\n\tpfpdu->lastrcv_buf = buf;\n\tif (hw_rev >= IRDMA_GEN_2 && !pfpdu->ah) {\n\t\tirdma_ieq_create_ah(qp, buf);\n\t\tif (!pfpdu->ah)\n\t\t\tgoto error;\n\t\tgoto exit;\n\t}\n\tif (hw_rev == IRDMA_GEN_1)\n\t\tirdma_ieq_process_fpdus(qp, ieq);\n\telse if (pfpdu->ah && pfpdu->ah->ah_info.ah_valid)\n\t\tirdma_ieq_process_fpdus(qp, ieq);\nexit:\n\tspin_unlock_irqrestore(&pfpdu->lock, flags);\n\n\treturn;\n\nerror:\n\tirdma_puda_ret_bufpool(ieq, buf);\n\tspin_unlock_irqrestore(&pfpdu->lock, flags);\n}\n\n \nstatic void irdma_ieq_receive(struct irdma_sc_vsi *vsi,\n\t\t\t      struct irdma_puda_buf *buf)\n{\n\tstruct irdma_puda_rsrc *ieq = vsi->ieq;\n\tstruct irdma_sc_qp *qp = NULL;\n\tu32 wqe_idx = ieq->compl_rxwqe_idx;\n\n\tqp = irdma_ieq_get_qp(vsi->dev, buf);\n\tif (!qp) {\n\t\tieq->stats_bad_qp_id++;\n\t\tirdma_puda_ret_bufpool(ieq, buf);\n\t} else {\n\t\tirdma_ieq_handle_exception(ieq, qp, buf);\n\t}\n\t \n\tif (!ieq->rxq_invalid_cnt)\n\t\tieq->rx_wqe_idx = wqe_idx;\n\tieq->rxq_invalid_cnt++;\n}\n\n \nstatic void irdma_ieq_tx_compl(struct irdma_sc_vsi *vsi, void *sqwrid)\n{\n\tstruct irdma_puda_rsrc *ieq = vsi->ieq;\n\tstruct irdma_puda_buf *buf = sqwrid;\n\n\tirdma_puda_ret_bufpool(ieq, buf);\n}\n\n \nvoid irdma_ieq_cleanup_qp(struct irdma_puda_rsrc *ieq, struct irdma_sc_qp *qp)\n{\n\tstruct irdma_puda_buf *buf;\n\tstruct irdma_pfpdu *pfpdu = &qp->pfpdu;\n\tstruct list_head *rxlist = &pfpdu->rxlist;\n\n\tif (qp->pfpdu.ah) {\n\t\tirdma_puda_free_ah(ieq->dev, qp->pfpdu.ah);\n\t\tqp->pfpdu.ah = NULL;\n\t\tqp->pfpdu.ah_buf = NULL;\n\t}\n\n\tif (!pfpdu->mode)\n\t\treturn;\n\n\twhile (!list_empty(rxlist)) {\n\t\tbuf = irdma_puda_get_listbuf(rxlist);\n\t\tirdma_puda_ret_bufpool(ieq, buf);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}