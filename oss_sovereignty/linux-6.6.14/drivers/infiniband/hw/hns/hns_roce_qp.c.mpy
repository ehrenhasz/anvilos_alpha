{
  "module_name": "hns_roce_qp.c",
  "hash_id": "8129265e0acfa9ac9296b58da8e4039dacd75230ccf93609dc8228e4840e0ba3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hns/hns_roce_qp.c",
  "human_readable_source": " \n\n#include <linux/pci.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_umem.h>\n#include <rdma/uverbs_ioctl.h>\n#include \"hns_roce_common.h\"\n#include \"hns_roce_device.h\"\n#include \"hns_roce_hem.h\"\n\nstatic void flush_work_handle(struct work_struct *work)\n{\n\tstruct hns_roce_work *flush_work = container_of(work,\n\t\t\t\t\tstruct hns_roce_work, work);\n\tstruct hns_roce_qp *hr_qp = container_of(flush_work,\n\t\t\t\t\tstruct hns_roce_qp, flush_work);\n\tstruct device *dev = flush_work->hr_dev->dev;\n\tstruct ib_qp_attr attr;\n\tint attr_mask;\n\tint ret;\n\n\tattr_mask = IB_QP_STATE;\n\tattr.qp_state = IB_QPS_ERR;\n\n\tif (test_and_clear_bit(HNS_ROCE_FLUSH_FLAG, &hr_qp->flush_flag)) {\n\t\tret = hns_roce_modify_qp(&hr_qp->ibqp, &attr, attr_mask, NULL);\n\t\tif (ret)\n\t\t\tdev_err(dev, \"modify QP to error state failed(%d) during CQE flush\\n\",\n\t\t\t\tret);\n\t}\n\n\t \n\tif (refcount_dec_and_test(&hr_qp->refcount))\n\t\tcomplete(&hr_qp->free);\n}\n\nvoid init_flush_work(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_work *flush_work = &hr_qp->flush_work;\n\n\tflush_work->hr_dev = hr_dev;\n\tINIT_WORK(&flush_work->work, flush_work_handle);\n\trefcount_inc(&hr_qp->refcount);\n\tqueue_work(hr_dev->irq_workq, &flush_work->work);\n}\n\nvoid flush_cqe(struct hns_roce_dev *dev, struct hns_roce_qp *qp)\n{\n\t \n\tif (!test_and_set_bit(HNS_ROCE_FLUSH_FLAG, &qp->flush_flag))\n\t\tinit_flush_work(dev, qp);\n}\n\nvoid hns_roce_qp_event(struct hns_roce_dev *hr_dev, u32 qpn, int event_type)\n{\n\tstruct device *dev = hr_dev->dev;\n\tstruct hns_roce_qp *qp;\n\n\txa_lock(&hr_dev->qp_table_xa);\n\tqp = __hns_roce_qp_lookup(hr_dev, qpn);\n\tif (qp)\n\t\trefcount_inc(&qp->refcount);\n\txa_unlock(&hr_dev->qp_table_xa);\n\n\tif (!qp) {\n\t\tdev_warn(dev, \"async event for bogus QP %08x\\n\", qpn);\n\t\treturn;\n\t}\n\n\tif (event_type == HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR ||\n\t    event_type == HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR ||\n\t    event_type == HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR ||\n\t    event_type == HNS_ROCE_EVENT_TYPE_XRCD_VIOLATION ||\n\t    event_type == HNS_ROCE_EVENT_TYPE_INVALID_XRCETH) {\n\t\tqp->state = IB_QPS_ERR;\n\n\t\tflush_cqe(hr_dev, qp);\n\t}\n\n\tqp->event(qp, (enum hns_roce_event)event_type);\n\n\tif (refcount_dec_and_test(&qp->refcount))\n\t\tcomplete(&qp->free);\n}\n\nstatic void hns_roce_ib_qp_event(struct hns_roce_qp *hr_qp,\n\t\t\t\t enum hns_roce_event type)\n{\n\tstruct ib_qp *ibqp = &hr_qp->ibqp;\n\tstruct ib_event event;\n\n\tif (ibqp->event_handler) {\n\t\tevent.device = ibqp->device;\n\t\tevent.element.qp = ibqp;\n\t\tswitch (type) {\n\t\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG:\n\t\t\tevent.event = IB_EVENT_PATH_MIG;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_COMM_EST:\n\t\t\tevent.event = IB_EVENT_COMM_EST;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_SQ_DRAINED:\n\t\t\tevent.event = IB_EVENT_SQ_DRAINED;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:\n\t\t\tevent.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:\n\t\t\tevent.event = IB_EVENT_QP_FATAL;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG_FAILED:\n\t\t\tevent.event = IB_EVENT_PATH_MIG_ERR;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:\n\t\t\tevent.event = IB_EVENT_QP_REQ_ERR;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:\n\t\tcase HNS_ROCE_EVENT_TYPE_XRCD_VIOLATION:\n\t\tcase HNS_ROCE_EVENT_TYPE_INVALID_XRCETH:\n\t\t\tevent.event = IB_EVENT_QP_ACCESS_ERR;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_dbg(ibqp->device->dev.parent, \"roce_ib: Unexpected event type %d on QP %06lx\\n\",\n\t\t\t\ttype, hr_qp->qpn);\n\t\t\treturn;\n\t\t}\n\t\tibqp->event_handler(&event, ibqp->qp_context);\n\t}\n}\n\nstatic u8 get_affinity_cq_bank(u8 qp_bank)\n{\n\treturn (qp_bank >> 1) & CQ_BANKID_MASK;\n}\n\nstatic u8 get_least_load_bankid_for_qp(struct ib_qp_init_attr *init_attr,\n\t\t\t\t\tstruct hns_roce_bank *bank)\n{\n#define INVALID_LOAD_QPNUM 0xFFFFFFFF\n\tstruct ib_cq *scq = init_attr->send_cq;\n\tu32 least_load = INVALID_LOAD_QPNUM;\n\tunsigned long cqn = 0;\n\tu8 bankid = 0;\n\tu32 bankcnt;\n\tu8 i;\n\n\tif (scq)\n\t\tcqn = to_hr_cq(scq)->cqn;\n\n\tfor (i = 0; i < HNS_ROCE_QP_BANK_NUM; i++) {\n\t\tif (scq && (get_affinity_cq_bank(i) != (cqn & CQ_BANKID_MASK)))\n\t\t\tcontinue;\n\n\t\tbankcnt = bank[i].inuse;\n\t\tif (bankcnt < least_load) {\n\t\t\tleast_load = bankcnt;\n\t\t\tbankid = i;\n\t\t}\n\t}\n\n\treturn bankid;\n}\n\nstatic int alloc_qpn_with_bankid(struct hns_roce_bank *bank, u8 bankid,\n\t\t\t\t unsigned long *qpn)\n{\n\tint id;\n\n\tid = ida_alloc_range(&bank->ida, bank->next, bank->max, GFP_KERNEL);\n\tif (id < 0) {\n\t\tid = ida_alloc_range(&bank->ida, bank->min, bank->max,\n\t\t\t\t     GFP_KERNEL);\n\t\tif (id < 0)\n\t\t\treturn id;\n\t}\n\n\t \n\tbank->next = (id + 1) > bank->max ? bank->min : id + 1;\n\n\t \n\t*qpn = (id << 3) | bankid;\n\n\treturn 0;\n}\nstatic int alloc_qpn(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t     struct ib_qp_init_attr *init_attr)\n{\n\tstruct hns_roce_qp_table *qp_table = &hr_dev->qp_table;\n\tunsigned long num = 0;\n\tu8 bankid;\n\tint ret;\n\n\tif (hr_qp->ibqp.qp_type == IB_QPT_GSI) {\n\t\tnum = 1;\n\t} else {\n\t\tmutex_lock(&qp_table->bank_mutex);\n\t\tbankid = get_least_load_bankid_for_qp(init_attr, qp_table->bank);\n\n\t\tret = alloc_qpn_with_bankid(&qp_table->bank[bankid], bankid,\n\t\t\t\t\t    &num);\n\t\tif (ret) {\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"failed to alloc QPN, ret = %d\\n\", ret);\n\t\t\tmutex_unlock(&qp_table->bank_mutex);\n\t\t\treturn ret;\n\t\t}\n\n\t\tqp_table->bank[bankid].inuse++;\n\t\tmutex_unlock(&qp_table->bank_mutex);\n\t}\n\n\thr_qp->qpn = num;\n\n\treturn 0;\n}\n\nstatic void add_qp_to_list(struct hns_roce_dev *hr_dev,\n\t\t\t   struct hns_roce_qp *hr_qp,\n\t\t\t   struct ib_cq *send_cq, struct ib_cq *recv_cq)\n{\n\tstruct hns_roce_cq *hr_send_cq, *hr_recv_cq;\n\tunsigned long flags;\n\n\thr_send_cq = send_cq ? to_hr_cq(send_cq) : NULL;\n\thr_recv_cq = recv_cq ? to_hr_cq(recv_cq) : NULL;\n\n\tspin_lock_irqsave(&hr_dev->qp_list_lock, flags);\n\thns_roce_lock_cqs(hr_send_cq, hr_recv_cq);\n\n\tlist_add_tail(&hr_qp->node, &hr_dev->qp_list);\n\tif (hr_send_cq)\n\t\tlist_add_tail(&hr_qp->sq_node, &hr_send_cq->sq_list);\n\tif (hr_recv_cq)\n\t\tlist_add_tail(&hr_qp->rq_node, &hr_recv_cq->rq_list);\n\n\thns_roce_unlock_cqs(hr_send_cq, hr_recv_cq);\n\tspin_unlock_irqrestore(&hr_dev->qp_list_lock, flags);\n}\n\nstatic int hns_roce_qp_store(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_qp *hr_qp,\n\t\t\t     struct ib_qp_init_attr *init_attr)\n{\n\tstruct xarray *xa = &hr_dev->qp_table_xa;\n\tint ret;\n\n\tif (!hr_qp->qpn)\n\t\treturn -EINVAL;\n\n\tret = xa_err(xa_store_irq(xa, hr_qp->qpn, hr_qp, GFP_KERNEL));\n\tif (ret)\n\t\tdev_err(hr_dev->dev, \"failed to xa store for QPC\\n\");\n\telse\n\t\t \n\t\tadd_qp_to_list(hr_dev, hr_qp, init_attr->send_cq,\n\t\t\t       init_attr->recv_cq);\n\n\treturn ret;\n}\n\nstatic int alloc_qpc(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_qp_table *qp_table = &hr_dev->qp_table;\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tif (!hr_qp->qpn)\n\t\treturn -EINVAL;\n\n\t \n\tret = hns_roce_table_get(hr_dev, &qp_table->qp_table, hr_qp->qpn);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get QPC table\\n\");\n\t\tgoto err_out;\n\t}\n\n\t \n\tret = hns_roce_table_get(hr_dev, &qp_table->irrl_table, hr_qp->qpn);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get IRRL table\\n\");\n\t\tgoto err_put_qp;\n\t}\n\n\tif (hr_dev->caps.trrl_entry_sz) {\n\t\t \n\t\tret = hns_roce_table_get(hr_dev, &qp_table->trrl_table,\n\t\t\t\t\t hr_qp->qpn);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to get TRRL table\\n\");\n\t\t\tgoto err_put_irrl;\n\t\t}\n\t}\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_QP_FLOW_CTRL) {\n\t\t \n\t\tret = hns_roce_table_get(hr_dev, &qp_table->sccc_table,\n\t\t\t\t\t hr_qp->qpn);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to get SCC CTX table\\n\");\n\t\t\tgoto err_put_trrl;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_put_trrl:\n\tif (hr_dev->caps.trrl_entry_sz)\n\t\thns_roce_table_put(hr_dev, &qp_table->trrl_table, hr_qp->qpn);\n\nerr_put_irrl:\n\thns_roce_table_put(hr_dev, &qp_table->irrl_table, hr_qp->qpn);\n\nerr_put_qp:\n\thns_roce_table_put(hr_dev, &qp_table->qp_table, hr_qp->qpn);\n\nerr_out:\n\treturn ret;\n}\n\nstatic void qp_user_mmap_entry_remove(struct hns_roce_qp *hr_qp)\n{\n\trdma_user_mmap_entry_remove(&hr_qp->dwqe_mmap_entry->rdma_entry);\n}\n\nvoid hns_roce_qp_remove(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\tstruct xarray *xa = &hr_dev->qp_table_xa;\n\tunsigned long flags;\n\n\tlist_del(&hr_qp->node);\n\n\tif (hr_qp->ibqp.qp_type != IB_QPT_XRC_TGT)\n\t\tlist_del(&hr_qp->sq_node);\n\n\tif (hr_qp->ibqp.qp_type != IB_QPT_XRC_INI &&\n\t    hr_qp->ibqp.qp_type != IB_QPT_XRC_TGT)\n\t\tlist_del(&hr_qp->rq_node);\n\n\txa_lock_irqsave(xa, flags);\n\t__xa_erase(xa, hr_qp->qpn);\n\txa_unlock_irqrestore(xa, flags);\n}\n\nstatic void free_qpc(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_qp_table *qp_table = &hr_dev->qp_table;\n\n\tif (hr_dev->caps.trrl_entry_sz)\n\t\thns_roce_table_put(hr_dev, &qp_table->trrl_table, hr_qp->qpn);\n\thns_roce_table_put(hr_dev, &qp_table->irrl_table, hr_qp->qpn);\n}\n\nstatic inline u8 get_qp_bankid(unsigned long qpn)\n{\n\t \n\treturn (u8)(qpn & GENMASK(2, 0));\n}\n\nstatic void free_qpn(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\tu8 bankid;\n\n\tif (hr_qp->ibqp.qp_type == IB_QPT_GSI)\n\t\treturn;\n\n\tif (hr_qp->qpn < hr_dev->caps.reserved_qps)\n\t\treturn;\n\n\tbankid = get_qp_bankid(hr_qp->qpn);\n\n\tida_free(&hr_dev->qp_table.bank[bankid].ida, hr_qp->qpn >> 3);\n\n\tmutex_lock(&hr_dev->qp_table.bank_mutex);\n\thr_dev->qp_table.bank[bankid].inuse--;\n\tmutex_unlock(&hr_dev->qp_table.bank_mutex);\n}\n\nstatic u32 proc_rq_sge(struct hns_roce_dev *dev, struct hns_roce_qp *hr_qp,\n\t\t       bool user)\n{\n\tu32 max_sge = dev->caps.max_rq_sg;\n\n\tif (dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\treturn max_sge;\n\n\t \n\tif (user)\n\t\tmax_sge = roundup_pow_of_two(max_sge + 1);\n\telse\n\t\thr_qp->rq.rsv_sge = 1;\n\n\treturn max_sge;\n}\n\nstatic int set_rq_size(struct hns_roce_dev *hr_dev, struct ib_qp_cap *cap,\n\t\t       struct hns_roce_qp *hr_qp, int has_rq, bool user)\n{\n\tu32 max_sge = proc_rq_sge(hr_dev, hr_qp, user);\n\tu32 cnt;\n\n\t \n\tif (!has_rq) {\n\t\thr_qp->rq.wqe_cnt = 0;\n\t\thr_qp->rq.max_gs = 0;\n\t\tcap->max_recv_wr = 0;\n\t\tcap->max_recv_sge = 0;\n\n\t\treturn 0;\n\t}\n\n\t \n\tif (!cap->max_recv_wr || cap->max_recv_wr > hr_dev->caps.max_wqes ||\n\t    cap->max_recv_sge > max_sge) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"RQ config error, depth = %u, sge = %u\\n\",\n\t\t\t  cap->max_recv_wr, cap->max_recv_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tcnt = roundup_pow_of_two(max(cap->max_recv_wr, hr_dev->caps.min_wqes));\n\tif (cnt > hr_dev->caps.max_wqes) {\n\t\tibdev_err(&hr_dev->ib_dev, \"rq depth %u too large\\n\",\n\t\t\t  cap->max_recv_wr);\n\t\treturn -EINVAL;\n\t}\n\n\thr_qp->rq.max_gs = roundup_pow_of_two(max(1U, cap->max_recv_sge) +\n\t\t\t\t\t      hr_qp->rq.rsv_sge);\n\n\thr_qp->rq.wqe_shift = ilog2(hr_dev->caps.max_rq_desc_sz *\n\t\t\t\t    hr_qp->rq.max_gs);\n\n\thr_qp->rq.wqe_cnt = cnt;\n\n\tcap->max_recv_wr = cnt;\n\tcap->max_recv_sge = hr_qp->rq.max_gs - hr_qp->rq.rsv_sge;\n\n\treturn 0;\n}\n\nstatic u32 get_max_inline_data(struct hns_roce_dev *hr_dev,\n\t\t\t       struct ib_qp_cap *cap)\n{\n\tif (cap->max_inline_data) {\n\t\tcap->max_inline_data = roundup_pow_of_two(cap->max_inline_data);\n\t\treturn min(cap->max_inline_data,\n\t\t\t   hr_dev->caps.max_sq_inline);\n\t}\n\n\treturn 0;\n}\n\nstatic void update_inline_data(struct hns_roce_qp *hr_qp,\n\t\t\t       struct ib_qp_cap *cap)\n{\n\tu32 sge_num = hr_qp->sq.ext_sge_cnt;\n\n\tif (hr_qp->config & HNS_ROCE_EXSGE_FLAGS) {\n\t\tif (!(hr_qp->ibqp.qp_type == IB_QPT_GSI ||\n\t\t      hr_qp->ibqp.qp_type == IB_QPT_UD))\n\t\t\tsge_num = max((u32)HNS_ROCE_SGE_IN_WQE, sge_num);\n\n\t\tcap->max_inline_data = max(cap->max_inline_data,\n\t\t\t\t\t   sge_num * HNS_ROCE_SGE_SIZE);\n\t}\n\n\thr_qp->max_inline_data = cap->max_inline_data;\n}\n\nstatic u32 get_sge_num_from_max_send_sge(bool is_ud_or_gsi,\n\t\t\t\t\t u32 max_send_sge)\n{\n\tunsigned int std_sge_num;\n\tunsigned int min_sge;\n\n\tstd_sge_num = is_ud_or_gsi ? 0 : HNS_ROCE_SGE_IN_WQE;\n\tmin_sge = is_ud_or_gsi ? 1 : 0;\n\treturn max_send_sge > std_sge_num ? (max_send_sge - std_sge_num) :\n\t\t\t\tmin_sge;\n}\n\nstatic unsigned int get_sge_num_from_max_inl_data(bool is_ud_or_gsi,\n\t\t\t\t\t\t  u32 max_inline_data)\n{\n\tunsigned int inline_sge;\n\n\tinline_sge = roundup_pow_of_two(max_inline_data) / HNS_ROCE_SGE_SIZE;\n\n\t \n\tif (!is_ud_or_gsi && inline_sge <= HNS_ROCE_SGE_IN_WQE)\n\t\tinline_sge = 0;\n\n\treturn inline_sge;\n}\n\nstatic void set_ext_sge_param(struct hns_roce_dev *hr_dev, u32 sq_wqe_cnt,\n\t\t\t      struct hns_roce_qp *hr_qp, struct ib_qp_cap *cap)\n{\n\tbool is_ud_or_gsi = (hr_qp->ibqp.qp_type == IB_QPT_GSI ||\n\t\t\t\thr_qp->ibqp.qp_type == IB_QPT_UD);\n\tunsigned int std_sge_num;\n\tu32 inline_ext_sge = 0;\n\tu32 ext_wqe_sge_cnt;\n\tu32 total_sge_cnt;\n\n\tcap->max_inline_data = get_max_inline_data(hr_dev, cap);\n\n\thr_qp->sge.sge_shift = HNS_ROCE_SGE_SHIFT;\n\tstd_sge_num = is_ud_or_gsi ? 0 : HNS_ROCE_SGE_IN_WQE;\n\text_wqe_sge_cnt = get_sge_num_from_max_send_sge(is_ud_or_gsi,\n\t\t\t\t\t\t\tcap->max_send_sge);\n\n\tif (hr_qp->config & HNS_ROCE_EXSGE_FLAGS) {\n\t\tinline_ext_sge = max(ext_wqe_sge_cnt,\n\t\t\t\t     get_sge_num_from_max_inl_data(is_ud_or_gsi,\n\t\t\t\t\t\t\t cap->max_inline_data));\n\t\thr_qp->sq.ext_sge_cnt = inline_ext_sge ?\n\t\t\t\t\troundup_pow_of_two(inline_ext_sge) : 0;\n\n\t\thr_qp->sq.max_gs = max(1U, (hr_qp->sq.ext_sge_cnt + std_sge_num));\n\t\thr_qp->sq.max_gs = min(hr_qp->sq.max_gs, hr_dev->caps.max_sq_sg);\n\n\t\text_wqe_sge_cnt = hr_qp->sq.ext_sge_cnt;\n\t} else {\n\t\thr_qp->sq.max_gs = max(1U, cap->max_send_sge);\n\t\thr_qp->sq.max_gs = min(hr_qp->sq.max_gs, hr_dev->caps.max_sq_sg);\n\t\thr_qp->sq.ext_sge_cnt = hr_qp->sq.max_gs;\n\t}\n\n\t \n\tif (ext_wqe_sge_cnt) {\n\t\ttotal_sge_cnt = roundup_pow_of_two(sq_wqe_cnt * ext_wqe_sge_cnt);\n\t\thr_qp->sge.sge_cnt = max(total_sge_cnt,\n\t\t\t\t(u32)HNS_HW_PAGE_SIZE / HNS_ROCE_SGE_SIZE);\n\t}\n\n\tupdate_inline_data(hr_qp, cap);\n}\n\nstatic int check_sq_size_with_integrity(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tstruct ib_qp_cap *cap,\n\t\t\t\t\tstruct hns_roce_ib_create_qp *ucmd)\n{\n\tu32 roundup_sq_stride = roundup_pow_of_two(hr_dev->caps.max_sq_desc_sz);\n\tu8 max_sq_stride = ilog2(roundup_sq_stride);\n\n\t \n\tif (ucmd->log_sq_stride > max_sq_stride ||\n\t    ucmd->log_sq_stride < HNS_ROCE_IB_MIN_SQ_STRIDE) {\n\t\tibdev_err(&hr_dev->ib_dev, \"failed to check SQ stride size.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cap->max_send_sge > hr_dev->caps.max_sq_sg) {\n\t\tibdev_err(&hr_dev->ib_dev, \"failed to check SQ SGE size %u.\\n\",\n\t\t\t  cap->max_send_sge);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int set_user_sq_size(struct hns_roce_dev *hr_dev,\n\t\t\t    struct ib_qp_cap *cap, struct hns_roce_qp *hr_qp,\n\t\t\t    struct hns_roce_ib_create_qp *ucmd)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tu32 cnt = 0;\n\tint ret;\n\n\tif (check_shl_overflow(1, ucmd->log_sq_bb_count, &cnt) ||\n\t    cnt > hr_dev->caps.max_wqes)\n\t\treturn -EINVAL;\n\n\tret = check_sq_size_with_integrity(hr_dev, cap, ucmd);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to check user SQ size, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\tset_ext_sge_param(hr_dev, cnt, hr_qp, cap);\n\n\thr_qp->sq.wqe_shift = ucmd->log_sq_stride;\n\thr_qp->sq.wqe_cnt = cnt;\n\tcap->max_send_sge = hr_qp->sq.max_gs;\n\n\treturn 0;\n}\n\nstatic int set_wqe_buf_attr(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp,\n\t\t\t    struct hns_roce_buf_attr *buf_attr)\n{\n\tint buf_size;\n\tint idx = 0;\n\n\thr_qp->buff_size = 0;\n\n\t \n\thr_qp->sq.offset = 0;\n\tbuf_size = to_hr_hem_entries_size(hr_qp->sq.wqe_cnt,\n\t\t\t\t\t  hr_qp->sq.wqe_shift);\n\tif (buf_size > 0 && idx < ARRAY_SIZE(buf_attr->region)) {\n\t\tbuf_attr->region[idx].size = buf_size;\n\t\tbuf_attr->region[idx].hopnum = hr_dev->caps.wqe_sq_hop_num;\n\t\tidx++;\n\t\thr_qp->buff_size += buf_size;\n\t}\n\n\t \n\thr_qp->sge.offset = hr_qp->buff_size;\n\tbuf_size = to_hr_hem_entries_size(hr_qp->sge.sge_cnt,\n\t\t\t\t\t  hr_qp->sge.sge_shift);\n\tif (buf_size > 0 && idx < ARRAY_SIZE(buf_attr->region)) {\n\t\tbuf_attr->region[idx].size = buf_size;\n\t\tbuf_attr->region[idx].hopnum = hr_dev->caps.wqe_sge_hop_num;\n\t\tidx++;\n\t\thr_qp->buff_size += buf_size;\n\t}\n\n\t \n\thr_qp->rq.offset = hr_qp->buff_size;\n\tbuf_size = to_hr_hem_entries_size(hr_qp->rq.wqe_cnt,\n\t\t\t\t\t  hr_qp->rq.wqe_shift);\n\tif (buf_size > 0 && idx < ARRAY_SIZE(buf_attr->region)) {\n\t\tbuf_attr->region[idx].size = buf_size;\n\t\tbuf_attr->region[idx].hopnum = hr_dev->caps.wqe_rq_hop_num;\n\t\tidx++;\n\t\thr_qp->buff_size += buf_size;\n\t}\n\n\tif (hr_qp->buff_size < 1)\n\t\treturn -EINVAL;\n\n\tbuf_attr->page_shift = HNS_HW_PAGE_SHIFT + hr_dev->caps.mtt_buf_pg_sz;\n\tbuf_attr->region_count = idx;\n\n\treturn 0;\n}\n\nstatic int set_kernel_sq_size(struct hns_roce_dev *hr_dev,\n\t\t\t      struct ib_qp_cap *cap, struct hns_roce_qp *hr_qp)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tu32 cnt;\n\n\tif (!cap->max_send_wr || cap->max_send_wr > hr_dev->caps.max_wqes ||\n\t    cap->max_send_sge > hr_dev->caps.max_sq_sg) {\n\t\tibdev_err(ibdev, \"failed to check SQ WR or SGE num.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcnt = roundup_pow_of_two(max(cap->max_send_wr, hr_dev->caps.min_wqes));\n\tif (cnt > hr_dev->caps.max_wqes) {\n\t\tibdev_err(ibdev, \"failed to check WQE num, WQE num = %u.\\n\",\n\t\t\t  cnt);\n\t\treturn -EINVAL;\n\t}\n\n\thr_qp->sq.wqe_shift = ilog2(hr_dev->caps.max_sq_desc_sz);\n\thr_qp->sq.wqe_cnt = cnt;\n\n\tset_ext_sge_param(hr_dev, cnt, hr_qp, cap);\n\n\t \n\tcap->max_send_wr = cnt;\n\tcap->max_send_sge = hr_qp->sq.max_gs;\n\n\treturn 0;\n}\n\nstatic int hns_roce_qp_has_sq(struct ib_qp_init_attr *attr)\n{\n\tif (attr->qp_type == IB_QPT_XRC_TGT || !attr->cap.max_send_wr)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int hns_roce_qp_has_rq(struct ib_qp_init_attr *attr)\n{\n\tif (attr->qp_type == IB_QPT_XRC_INI ||\n\t    attr->qp_type == IB_QPT_XRC_TGT || attr->srq ||\n\t    !attr->cap.max_recv_wr)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int alloc_qp_buf(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t\tstruct ib_qp_init_attr *init_attr,\n\t\t\tstruct ib_udata *udata, unsigned long addr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_buf_attr buf_attr = {};\n\tint ret;\n\n\tret = set_wqe_buf_attr(hr_dev, hr_qp, &buf_attr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to split WQE buf, ret = %d.\\n\", ret);\n\t\tgoto err_inline;\n\t}\n\tret = hns_roce_mtr_create(hr_dev, &hr_qp->mtr, &buf_attr,\n\t\t\t\t  PAGE_SHIFT + hr_dev->caps.mtt_ba_pg_sz,\n\t\t\t\t  udata, addr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to create WQE mtr, ret = %d.\\n\", ret);\n\t\tgoto err_inline;\n\t}\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_DIRECT_WQE)\n\t\thr_qp->en_flags |= HNS_ROCE_QP_CAP_DIRECT_WQE;\n\n\treturn 0;\n\nerr_inline:\n\n\treturn ret;\n}\n\nstatic void free_qp_buf(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp)\n{\n\thns_roce_mtr_destroy(hr_dev, &hr_qp->mtr);\n}\n\nstatic inline bool user_qp_has_sdb(struct hns_roce_dev *hr_dev,\n\t\t\t\t   struct ib_qp_init_attr *init_attr,\n\t\t\t\t   struct ib_udata *udata,\n\t\t\t\t   struct hns_roce_ib_create_qp_resp *resp,\n\t\t\t\t   struct hns_roce_ib_create_qp *ucmd)\n{\n\treturn ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_QP_RECORD_DB) &&\n\t\tudata->outlen >= offsetofend(typeof(*resp), cap_flags) &&\n\t\thns_roce_qp_has_sq(init_attr) &&\n\t\tudata->inlen >= offsetofend(typeof(*ucmd), sdb_addr));\n}\n\nstatic inline bool user_qp_has_rdb(struct hns_roce_dev *hr_dev,\n\t\t\t\t   struct ib_qp_init_attr *init_attr,\n\t\t\t\t   struct ib_udata *udata,\n\t\t\t\t   struct hns_roce_ib_create_qp_resp *resp)\n{\n\treturn ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_QP_RECORD_DB) &&\n\t\tudata->outlen >= offsetofend(typeof(*resp), cap_flags) &&\n\t\thns_roce_qp_has_rq(init_attr));\n}\n\nstatic inline bool kernel_qp_has_rdb(struct hns_roce_dev *hr_dev,\n\t\t\t\t     struct ib_qp_init_attr *init_attr)\n{\n\treturn ((hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_QP_RECORD_DB) &&\n\t\thns_roce_qp_has_rq(init_attr));\n}\n\nstatic int qp_mmap_entry(struct hns_roce_qp *hr_qp,\n\t\t\t struct hns_roce_dev *hr_dev,\n\t\t\t struct ib_udata *udata,\n\t\t\t struct hns_roce_ib_create_qp_resp *resp)\n{\n\tstruct hns_roce_ucontext *uctx =\n\t\trdma_udata_to_drv_context(udata,\n\t\t\tstruct hns_roce_ucontext, ibucontext);\n\tstruct rdma_user_mmap_entry *rdma_entry;\n\tu64 address;\n\n\taddress = hr_dev->dwqe_page + hr_qp->qpn * HNS_ROCE_DWQE_SIZE;\n\n\thr_qp->dwqe_mmap_entry =\n\t\thns_roce_user_mmap_entry_insert(&uctx->ibucontext, address,\n\t\t\t\t\t\tHNS_ROCE_DWQE_SIZE,\n\t\t\t\t\t\tHNS_ROCE_MMAP_TYPE_DWQE);\n\n\tif (!hr_qp->dwqe_mmap_entry) {\n\t\tibdev_err(&hr_dev->ib_dev, \"failed to get dwqe mmap entry.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trdma_entry = &hr_qp->dwqe_mmap_entry->rdma_entry;\n\tresp->dwqe_mmap_key = rdma_user_mmap_get_offset(rdma_entry);\n\n\treturn 0;\n}\n\nstatic int alloc_user_qp_db(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp,\n\t\t\t    struct ib_qp_init_attr *init_attr,\n\t\t\t    struct ib_udata *udata,\n\t\t\t    struct hns_roce_ib_create_qp *ucmd,\n\t\t\t    struct hns_roce_ib_create_qp_resp *resp)\n{\n\tstruct hns_roce_ucontext *uctx = rdma_udata_to_drv_context(udata,\n\t\tstruct hns_roce_ucontext, ibucontext);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\tif (user_qp_has_sdb(hr_dev, init_attr, udata, resp, ucmd)) {\n\t\tret = hns_roce_db_map_user(uctx, ucmd->sdb_addr, &hr_qp->sdb);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to map user SQ doorbell, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\tgoto err_out;\n\t\t}\n\t\thr_qp->en_flags |= HNS_ROCE_QP_CAP_SQ_RECORD_DB;\n\t}\n\n\tif (user_qp_has_rdb(hr_dev, init_attr, udata, resp)) {\n\t\tret = hns_roce_db_map_user(uctx, ucmd->db_addr, &hr_qp->rdb);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to map user RQ doorbell, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\tgoto err_sdb;\n\t\t}\n\t\thr_qp->en_flags |= HNS_ROCE_QP_CAP_RQ_RECORD_DB;\n\t}\n\n\treturn 0;\n\nerr_sdb:\n\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_SQ_RECORD_DB)\n\t\thns_roce_db_unmap_user(uctx, &hr_qp->sdb);\nerr_out:\n\treturn ret;\n}\n\nstatic int alloc_kernel_qp_db(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_qp *hr_qp,\n\t\t\t      struct ib_qp_init_attr *init_attr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\thr_qp->sq.db_reg = hr_dev->mem_base +\n\t\t\t\t   HNS_ROCE_DWQE_SIZE * hr_qp->qpn;\n\telse\n\t\thr_qp->sq.db_reg = hr_dev->reg_base + hr_dev->sdb_offset +\n\t\t\t\t   DB_REG_OFFSET * hr_dev->priv_uar.index;\n\n\thr_qp->rq.db_reg = hr_dev->reg_base + hr_dev->odb_offset +\n\t\t\t   DB_REG_OFFSET * hr_dev->priv_uar.index;\n\n\tif (kernel_qp_has_rdb(hr_dev, init_attr)) {\n\t\tret = hns_roce_alloc_db(hr_dev, &hr_qp->rdb, 0);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to alloc kernel RQ doorbell, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\treturn ret;\n\t\t}\n\t\t*hr_qp->rdb.db_record = 0;\n\t\thr_qp->en_flags |= HNS_ROCE_QP_CAP_RQ_RECORD_DB;\n\t}\n\n\treturn 0;\n}\n\nstatic int alloc_qp_db(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t       struct ib_qp_init_attr *init_attr,\n\t\t       struct ib_udata *udata,\n\t\t       struct hns_roce_ib_create_qp *ucmd,\n\t\t       struct hns_roce_ib_create_qp_resp *resp)\n{\n\tint ret;\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_SDI_MODE)\n\t\thr_qp->en_flags |= HNS_ROCE_QP_CAP_OWNER_DB;\n\n\tif (udata) {\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_DIRECT_WQE) {\n\t\t\tret = qp_mmap_entry(hr_qp, hr_dev, udata, resp);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tret = alloc_user_qp_db(hr_dev, hr_qp, init_attr, udata, ucmd,\n\t\t\t\t       resp);\n\t\tif (ret)\n\t\t\tgoto err_remove_qp;\n\t} else {\n\t\tret = alloc_kernel_qp_db(hr_dev, hr_qp, init_attr);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n\nerr_remove_qp:\n\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_DIRECT_WQE)\n\t\tqp_user_mmap_entry_remove(hr_qp);\n\n\treturn ret;\n}\n\nstatic void free_qp_db(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t       struct ib_udata *udata)\n{\n\tstruct hns_roce_ucontext *uctx = rdma_udata_to_drv_context(\n\t\tudata, struct hns_roce_ucontext, ibucontext);\n\n\tif (udata) {\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)\n\t\t\thns_roce_db_unmap_user(uctx, &hr_qp->rdb);\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_SQ_RECORD_DB)\n\t\t\thns_roce_db_unmap_user(uctx, &hr_qp->sdb);\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_DIRECT_WQE)\n\t\t\tqp_user_mmap_entry_remove(hr_qp);\n\t} else {\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)\n\t\t\thns_roce_free_db(hr_dev, &hr_qp->rdb);\n\t}\n}\n\nstatic int alloc_kernel_wrid(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_qp *hr_qp)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tu64 *sq_wrid = NULL;\n\tu64 *rq_wrid = NULL;\n\tint ret;\n\n\tsq_wrid = kcalloc(hr_qp->sq.wqe_cnt, sizeof(u64), GFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(sq_wrid)) {\n\t\tibdev_err(ibdev, \"failed to alloc SQ wrid.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (hr_qp->rq.wqe_cnt) {\n\t\trq_wrid = kcalloc(hr_qp->rq.wqe_cnt, sizeof(u64), GFP_KERNEL);\n\t\tif (ZERO_OR_NULL_PTR(rq_wrid)) {\n\t\t\tibdev_err(ibdev, \"failed to alloc RQ wrid.\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_sq;\n\t\t}\n\t}\n\n\thr_qp->sq.wrid = sq_wrid;\n\thr_qp->rq.wrid = rq_wrid;\n\treturn 0;\nerr_sq:\n\tkfree(sq_wrid);\n\n\treturn ret;\n}\n\nstatic void free_kernel_wrid(struct hns_roce_qp *hr_qp)\n{\n\tkfree(hr_qp->rq.wrid);\n\tkfree(hr_qp->sq.wrid);\n}\n\nstatic int set_qp_param(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t\tstruct ib_qp_init_attr *init_attr,\n\t\t\tstruct ib_udata *udata,\n\t\t\tstruct hns_roce_ib_create_qp *ucmd)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_ucontext *uctx;\n\tint ret;\n\n\tif (init_attr->sq_sig_type == IB_SIGNAL_ALL_WR)\n\t\thr_qp->sq_signal_bits = IB_SIGNAL_ALL_WR;\n\telse\n\t\thr_qp->sq_signal_bits = IB_SIGNAL_REQ_WR;\n\n\tret = set_rq_size(hr_dev, &init_attr->cap, hr_qp,\n\t\t\t  hns_roce_qp_has_rq(init_attr), !!udata);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to set user RQ size, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\tif (udata) {\n\t\tret = ib_copy_from_udata(ucmd, udata,\n\t\t\t\t\t min(udata->inlen, sizeof(*ucmd)));\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to copy QP ucmd, ret = %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tuctx = rdma_udata_to_drv_context(udata, struct hns_roce_ucontext,\n\t\t\t\t\t\t ibucontext);\n\t\thr_qp->config = uctx->config;\n\t\tret = set_user_sq_size(hr_dev, &init_attr->cap, hr_qp, ucmd);\n\t\tif (ret)\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to set user SQ size, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t} else {\n\t\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\t\thr_qp->config = HNS_ROCE_EXSGE_FLAGS;\n\t\tret = set_kernel_sq_size(hr_dev, &init_attr->cap, hr_qp);\n\t\tif (ret)\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to set kernel SQ size, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_roce_create_qp_common(struct hns_roce_dev *hr_dev,\n\t\t\t\t     struct ib_pd *ib_pd,\n\t\t\t\t     struct ib_qp_init_attr *init_attr,\n\t\t\t\t     struct ib_udata *udata,\n\t\t\t\t     struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_ib_create_qp_resp resp = {};\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_ib_create_qp ucmd = {};\n\tint ret;\n\n\tmutex_init(&hr_qp->mutex);\n\tspin_lock_init(&hr_qp->sq.lock);\n\tspin_lock_init(&hr_qp->rq.lock);\n\n\thr_qp->state = IB_QPS_RESET;\n\thr_qp->flush_flag = 0;\n\n\tif (init_attr->create_flags)\n\t\treturn -EOPNOTSUPP;\n\n\tret = set_qp_param(hr_dev, hr_qp, init_attr, udata, &ucmd);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to set QP param, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (!udata) {\n\t\tret = alloc_kernel_wrid(hr_dev, hr_qp);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev, \"failed to alloc wrid, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = alloc_qp_buf(hr_dev, hr_qp, init_attr, udata, ucmd.buf_addr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc QP buffer, ret = %d.\\n\", ret);\n\t\tgoto err_buf;\n\t}\n\n\tret = alloc_qpn(hr_dev, hr_qp, init_attr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc QPN, ret = %d.\\n\", ret);\n\t\tgoto err_qpn;\n\t}\n\n\tret = alloc_qp_db(hr_dev, hr_qp, init_attr, udata, &ucmd, &resp);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc QP doorbell, ret = %d.\\n\",\n\t\t\t  ret);\n\t\tgoto err_db;\n\t}\n\n\tret = alloc_qpc(hr_dev, hr_qp);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc QP context, ret = %d.\\n\",\n\t\t\t  ret);\n\t\tgoto err_qpc;\n\t}\n\n\tret = hns_roce_qp_store(hr_dev, hr_qp, init_attr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to store QP, ret = %d.\\n\", ret);\n\t\tgoto err_store;\n\t}\n\n\tif (udata) {\n\t\tresp.cap_flags = hr_qp->en_flags;\n\t\tret = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(udata->outlen, sizeof(resp)));\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev, \"copy qp resp failed!\\n\");\n\t\t\tgoto err_store;\n\t\t}\n\t}\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_QP_FLOW_CTRL) {\n\t\tret = hr_dev->hw->qp_flow_control_init(hr_dev, hr_qp);\n\t\tif (ret)\n\t\t\tgoto err_flow_ctrl;\n\t}\n\n\thr_qp->ibqp.qp_num = hr_qp->qpn;\n\thr_qp->event = hns_roce_ib_qp_event;\n\trefcount_set(&hr_qp->refcount, 1);\n\tinit_completion(&hr_qp->free);\n\n\treturn 0;\n\nerr_flow_ctrl:\n\thns_roce_qp_remove(hr_dev, hr_qp);\nerr_store:\n\tfree_qpc(hr_dev, hr_qp);\nerr_qpc:\n\tfree_qp_db(hr_dev, hr_qp, udata);\nerr_db:\n\tfree_qpn(hr_dev, hr_qp);\nerr_qpn:\n\tfree_qp_buf(hr_dev, hr_qp);\nerr_buf:\n\tfree_kernel_wrid(hr_qp);\n\treturn ret;\n}\n\nvoid hns_roce_qp_destroy(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp,\n\t\t\t struct ib_udata *udata)\n{\n\tif (refcount_dec_and_test(&hr_qp->refcount))\n\t\tcomplete(&hr_qp->free);\n\twait_for_completion(&hr_qp->free);\n\n\tfree_qpc(hr_dev, hr_qp);\n\tfree_qpn(hr_dev, hr_qp);\n\tfree_qp_buf(hr_dev, hr_qp);\n\tfree_kernel_wrid(hr_qp);\n\tfree_qp_db(hr_dev, hr_qp, udata);\n}\n\nstatic int check_qp_type(struct hns_roce_dev *hr_dev, enum ib_qp_type type,\n\t\t\t bool is_user)\n{\n\tswitch (type) {\n\tcase IB_QPT_XRC_INI:\n\tcase IB_QPT_XRC_TGT:\n\t\tif (!(hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_XRC))\n\t\t\tgoto out;\n\t\tbreak;\n\tcase IB_QPT_UD:\n\t\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08 &&\n\t\t    is_user)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase IB_QPT_RC:\n\tcase IB_QPT_GSI:\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\treturn 0;\n\nout:\n\tibdev_err(&hr_dev->ib_dev, \"not support QP type %d\\n\", type);\n\n\treturn -EOPNOTSUPP;\n}\n\nint hns_roce_create_qp(struct ib_qp *qp, struct ib_qp_init_attr *init_attr,\n\t\t       struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = qp->device;\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibdev);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(qp);\n\tstruct ib_pd *pd = qp->pd;\n\tint ret;\n\n\tret = check_qp_type(hr_dev, init_attr->qp_type, !!udata);\n\tif (ret)\n\t\treturn ret;\n\n\tif (init_attr->qp_type == IB_QPT_XRC_TGT)\n\t\thr_qp->xrcdn = to_hr_xrcd(init_attr->xrcd)->xrcdn;\n\n\tif (init_attr->qp_type == IB_QPT_GSI) {\n\t\thr_qp->port = init_attr->port_num - 1;\n\t\thr_qp->phy_port = hr_dev->iboe.phy_port[hr_qp->port];\n\t}\n\n\tret = hns_roce_create_qp_common(hr_dev, pd, init_attr, udata, hr_qp);\n\tif (ret)\n\t\tibdev_err(ibdev, \"create QP type 0x%x failed(%d)\\n\",\n\t\t\t  init_attr->qp_type, ret);\n\n\treturn ret;\n}\n\nint to_hr_qp_type(int qp_type)\n{\n\tswitch (qp_type) {\n\tcase IB_QPT_RC:\n\t\treturn SERV_TYPE_RC;\n\tcase IB_QPT_UD:\n\tcase IB_QPT_GSI:\n\t\treturn SERV_TYPE_UD;\n\tcase IB_QPT_XRC_INI:\n\tcase IB_QPT_XRC_TGT:\n\t\treturn SERV_TYPE_XRC;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\nstatic int check_mtu_validate(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_qp *hr_qp,\n\t\t\t      struct ib_qp_attr *attr, int attr_mask)\n{\n\tenum ib_mtu active_mtu;\n\tint p;\n\n\tp = attr_mask & IB_QP_PORT ? (attr->port_num - 1) : hr_qp->port;\n\tactive_mtu = iboe_get_mtu(hr_dev->iboe.netdevs[p]->mtu);\n\n\tif ((hr_dev->caps.max_mtu >= IB_MTU_2048 &&\n\t    attr->path_mtu > hr_dev->caps.max_mtu) ||\n\t    attr->path_mtu < IB_MTU_256 || attr->path_mtu > active_mtu) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\"attr path_mtu(%d)invalid while modify qp\",\n\t\t\tattr->path_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_roce_check_qp_attr(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t\t\t  int attr_mask)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tint p;\n\n\tif ((attr_mask & IB_QP_PORT) &&\n\t    (attr->port_num == 0 || attr->port_num > hr_dev->caps.num_ports)) {\n\t\tibdev_err(&hr_dev->ib_dev, \"invalid attr, port_num = %u.\\n\",\n\t\t\t  attr->port_num);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_QP_PKEY_INDEX) {\n\t\tp = attr_mask & IB_QP_PORT ? (attr->port_num - 1) : hr_qp->port;\n\t\tif (attr->pkey_index >= hr_dev->caps.pkey_table_len[p]) {\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"invalid attr, pkey_index = %u.\\n\",\n\t\t\t\t  attr->pkey_index);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC &&\n\t    attr->max_rd_atomic > hr_dev->caps.max_qp_init_rdma) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"invalid attr, max_rd_atomic = %u.\\n\",\n\t\t\t  attr->max_rd_atomic);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC &&\n\t    attr->max_dest_rd_atomic > hr_dev->caps.max_qp_dest_rdma) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"invalid attr, max_dest_rd_atomic = %u.\\n\",\n\t\t\t  attr->max_dest_rd_atomic);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_QP_PATH_MTU)\n\t\treturn check_mtu_validate(hr_dev, hr_qp, attr, attr_mask);\n\n\treturn 0;\n}\n\nint hns_roce_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t       int attr_mask, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tenum ib_qp_state cur_state, new_state;\n\tint ret = -EINVAL;\n\n\tmutex_lock(&hr_qp->mutex);\n\n\tif (attr_mask & IB_QP_CUR_STATE && attr->cur_qp_state != hr_qp->state)\n\t\tgoto out;\n\n\tcur_state = hr_qp->state;\n\tnew_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;\n\n\tif (ibqp->uobject &&\n\t    (attr_mask & IB_QP_STATE) && new_state == IB_QPS_ERR) {\n\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_SQ_RECORD_DB) {\n\t\t\thr_qp->sq.head = *(int *)(hr_qp->sdb.virt_addr);\n\n\t\t\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)\n\t\t\t\thr_qp->rq.head = *(int *)(hr_qp->rdb.virt_addr);\n\t\t} else {\n\t\t\tibdev_warn(&hr_dev->ib_dev,\n\t\t\t\t  \"flush cqe is not supported in userspace!\\n\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,\n\t\t\t\tattr_mask)) {\n\t\tibdev_err(&hr_dev->ib_dev, \"ib_modify_qp_is_ok failed\\n\");\n\t\tgoto out;\n\t}\n\n\tret = hns_roce_check_qp_attr(ibqp, attr, attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tif (cur_state == new_state && cur_state == IB_QPS_RESET)\n\t\tgoto out;\n\n\tret = hr_dev->hw->modify_qp(ibqp, attr, attr_mask, cur_state,\n\t\t\t\t    new_state, udata);\n\nout:\n\tmutex_unlock(&hr_qp->mutex);\n\n\treturn ret;\n}\n\nvoid hns_roce_lock_cqs(struct hns_roce_cq *send_cq, struct hns_roce_cq *recv_cq)\n\t\t       __acquires(&send_cq->lock) __acquires(&recv_cq->lock)\n{\n\tif (unlikely(send_cq == NULL && recv_cq == NULL)) {\n\t\t__acquire(&send_cq->lock);\n\t\t__acquire(&recv_cq->lock);\n\t} else if (unlikely(send_cq != NULL && recv_cq == NULL)) {\n\t\tspin_lock_irq(&send_cq->lock);\n\t\t__acquire(&recv_cq->lock);\n\t} else if (unlikely(send_cq == NULL && recv_cq != NULL)) {\n\t\tspin_lock_irq(&recv_cq->lock);\n\t\t__acquire(&send_cq->lock);\n\t} else if (send_cq == recv_cq) {\n\t\tspin_lock_irq(&send_cq->lock);\n\t\t__acquire(&recv_cq->lock);\n\t} else if (send_cq->cqn < recv_cq->cqn) {\n\t\tspin_lock_irq(&send_cq->lock);\n\t\tspin_lock_nested(&recv_cq->lock, SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock_irq(&recv_cq->lock);\n\t\tspin_lock_nested(&send_cq->lock, SINGLE_DEPTH_NESTING);\n\t}\n}\n\nvoid hns_roce_unlock_cqs(struct hns_roce_cq *send_cq,\n\t\t\t struct hns_roce_cq *recv_cq) __releases(&send_cq->lock)\n\t\t\t __releases(&recv_cq->lock)\n{\n\tif (unlikely(send_cq == NULL && recv_cq == NULL)) {\n\t\t__release(&recv_cq->lock);\n\t\t__release(&send_cq->lock);\n\t} else if (unlikely(send_cq != NULL && recv_cq == NULL)) {\n\t\t__release(&recv_cq->lock);\n\t\tspin_unlock(&send_cq->lock);\n\t} else if (unlikely(send_cq == NULL && recv_cq != NULL)) {\n\t\t__release(&send_cq->lock);\n\t\tspin_unlock(&recv_cq->lock);\n\t} else if (send_cq == recv_cq) {\n\t\t__release(&recv_cq->lock);\n\t\tspin_unlock_irq(&send_cq->lock);\n\t} else if (send_cq->cqn < recv_cq->cqn) {\n\t\tspin_unlock(&recv_cq->lock);\n\t\tspin_unlock_irq(&send_cq->lock);\n\t} else {\n\t\tspin_unlock(&send_cq->lock);\n\t\tspin_unlock_irq(&recv_cq->lock);\n\t}\n}\n\nstatic inline void *get_wqe(struct hns_roce_qp *hr_qp, u32 offset)\n{\n\treturn hns_roce_buf_offset(hr_qp->mtr.kmem, offset);\n}\n\nvoid *hns_roce_get_recv_wqe(struct hns_roce_qp *hr_qp, unsigned int n)\n{\n\treturn get_wqe(hr_qp, hr_qp->rq.offset + (n << hr_qp->rq.wqe_shift));\n}\n\nvoid *hns_roce_get_send_wqe(struct hns_roce_qp *hr_qp, unsigned int n)\n{\n\treturn get_wqe(hr_qp, hr_qp->sq.offset + (n << hr_qp->sq.wqe_shift));\n}\n\nvoid *hns_roce_get_extend_sge(struct hns_roce_qp *hr_qp, unsigned int n)\n{\n\treturn get_wqe(hr_qp, hr_qp->sge.offset + (n << hr_qp->sge.sge_shift));\n}\n\nbool hns_roce_wq_overflow(struct hns_roce_wq *hr_wq, u32 nreq,\n\t\t\t  struct ib_cq *ib_cq)\n{\n\tstruct hns_roce_cq *hr_cq;\n\tu32 cur;\n\n\tcur = hr_wq->head - hr_wq->tail;\n\tif (likely(cur + nreq < hr_wq->wqe_cnt))\n\t\treturn false;\n\n\thr_cq = to_hr_cq(ib_cq);\n\tspin_lock(&hr_cq->lock);\n\tcur = hr_wq->head - hr_wq->tail;\n\tspin_unlock(&hr_cq->lock);\n\n\treturn cur + nreq >= hr_wq->wqe_cnt;\n}\n\nint hns_roce_init_qp_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_qp_table *qp_table = &hr_dev->qp_table;\n\tunsigned int reserved_from_bot;\n\tunsigned int i;\n\n\tqp_table->idx_table.spare_idx = kcalloc(hr_dev->caps.num_qps,\n\t\t\t\t\tsizeof(u32), GFP_KERNEL);\n\tif (!qp_table->idx_table.spare_idx)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&qp_table->scc_mutex);\n\tmutex_init(&qp_table->bank_mutex);\n\txa_init(&hr_dev->qp_table_xa);\n\n\treserved_from_bot = hr_dev->caps.reserved_qps;\n\n\tfor (i = 0; i < reserved_from_bot; i++) {\n\t\thr_dev->qp_table.bank[get_qp_bankid(i)].inuse++;\n\t\thr_dev->qp_table.bank[get_qp_bankid(i)].min++;\n\t}\n\n\tfor (i = 0; i < HNS_ROCE_QP_BANK_NUM; i++) {\n\t\tida_init(&hr_dev->qp_table.bank[i].ida);\n\t\thr_dev->qp_table.bank[i].max = hr_dev->caps.num_qps /\n\t\t\t\t\t       HNS_ROCE_QP_BANK_NUM - 1;\n\t\thr_dev->qp_table.bank[i].next = hr_dev->qp_table.bank[i].min;\n\t}\n\n\treturn 0;\n}\n\nvoid hns_roce_cleanup_qp_table(struct hns_roce_dev *hr_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < HNS_ROCE_QP_BANK_NUM; i++)\n\t\tida_destroy(&hr_dev->qp_table.bank[i].ida);\n\tkfree(hr_dev->qp_table.idx_table.spare_idx);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}