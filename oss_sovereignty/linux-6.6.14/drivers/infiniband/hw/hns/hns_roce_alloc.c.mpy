{
  "module_name": "hns_roce_alloc.c",
  "hash_id": "3941a8196632ae57c27fe90a3ad51640ce4b1bc3831e33b87e61ba4493f7c6fd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hns/hns_roce_alloc.c",
  "human_readable_source": " \n\n#include <linux/vmalloc.h>\n#include <rdma/ib_umem.h>\n#include \"hns_roce_device.h\"\n\nvoid hns_roce_buf_free(struct hns_roce_dev *hr_dev, struct hns_roce_buf *buf)\n{\n\tstruct hns_roce_buf_list *trunks;\n\tu32 i;\n\n\tif (!buf)\n\t\treturn;\n\n\ttrunks = buf->trunk_list;\n\tif (trunks) {\n\t\tbuf->trunk_list = NULL;\n\t\tfor (i = 0; i < buf->ntrunks; i++)\n\t\t\tdma_free_coherent(hr_dev->dev, 1 << buf->trunk_shift,\n\t\t\t\t\t  trunks[i].buf, trunks[i].map);\n\n\t\tkfree(trunks);\n\t}\n\n\tkfree(buf);\n}\n\n \nstruct hns_roce_buf *hns_roce_buf_alloc(struct hns_roce_dev *hr_dev, u32 size,\n\t\t\t\t\tu32 page_shift, u32 flags)\n{\n\tu32 trunk_size, page_size, alloced_size;\n\tstruct hns_roce_buf_list *trunks;\n\tstruct hns_roce_buf *buf;\n\tgfp_t gfp_flags;\n\tu32 ntrunk, i;\n\n\t \n\tif (WARN_ON(page_shift < HNS_HW_PAGE_SHIFT))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tgfp_flags = (flags & HNS_ROCE_BUF_NOSLEEP) ? GFP_ATOMIC : GFP_KERNEL;\n\tbuf = kzalloc(sizeof(*buf), gfp_flags);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf->page_shift = page_shift;\n\tpage_size = 1 << buf->page_shift;\n\n\t \n\tif (flags & HNS_ROCE_BUF_DIRECT) {\n\t\tbuf->trunk_shift = order_base_2(ALIGN(size, PAGE_SIZE));\n\t\tntrunk = 1;\n\t} else {\n\t\tbuf->trunk_shift = order_base_2(ALIGN(page_size, PAGE_SIZE));\n\t\tntrunk = DIV_ROUND_UP(size, 1 << buf->trunk_shift);\n\t}\n\n\ttrunks = kcalloc(ntrunk, sizeof(*trunks), gfp_flags);\n\tif (!trunks) {\n\t\tkfree(buf);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\ttrunk_size = 1 << buf->trunk_shift;\n\talloced_size = 0;\n\tfor (i = 0; i < ntrunk; i++) {\n\t\ttrunks[i].buf = dma_alloc_coherent(hr_dev->dev, trunk_size,\n\t\t\t\t\t\t   &trunks[i].map, gfp_flags);\n\t\tif (!trunks[i].buf)\n\t\t\tbreak;\n\n\t\talloced_size += trunk_size;\n\t}\n\n\tbuf->ntrunks = i;\n\n\t \n\tif ((flags & HNS_ROCE_BUF_NOFAIL) ? i == 0 : i != ntrunk) {\n\t\tfor (i = 0; i < buf->ntrunks; i++)\n\t\t\tdma_free_coherent(hr_dev->dev, trunk_size,\n\t\t\t\t\t  trunks[i].buf, trunks[i].map);\n\n\t\tkfree(trunks);\n\t\tkfree(buf);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tbuf->npages = DIV_ROUND_UP(alloced_size, page_size);\n\tbuf->trunk_list = trunks;\n\n\treturn buf;\n}\n\nint hns_roce_get_kmem_bufs(struct hns_roce_dev *hr_dev, dma_addr_t *bufs,\n\t\t\t   int buf_cnt, struct hns_roce_buf *buf,\n\t\t\t   unsigned int page_shift)\n{\n\tunsigned int offset, max_size;\n\tint total = 0;\n\tint i;\n\n\tif (page_shift > buf->trunk_shift) {\n\t\tdev_err(hr_dev->dev, \"failed to check kmem buf shift %u > %u\\n\",\n\t\t\tpage_shift, buf->trunk_shift);\n\t\treturn -EINVAL;\n\t}\n\n\toffset = 0;\n\tmax_size = buf->ntrunks << buf->trunk_shift;\n\tfor (i = 0; i < buf_cnt && offset < max_size; i++) {\n\t\tbufs[total++] = hns_roce_buf_dma_addr(buf, offset);\n\t\toffset += (1 << page_shift);\n\t}\n\n\treturn total;\n}\n\nint hns_roce_get_umem_bufs(struct hns_roce_dev *hr_dev, dma_addr_t *bufs,\n\t\t\t   int buf_cnt, struct ib_umem *umem,\n\t\t\t   unsigned int page_shift)\n{\n\tstruct ib_block_iter biter;\n\tint total = 0;\n\n\t \n\trdma_umem_for_each_dma_block(umem, &biter, 1 << page_shift) {\n\t\tbufs[total++] = rdma_block_iter_dma_address(&biter);\n\t\tif (total >= buf_cnt)\n\t\t\tgoto done;\n\t}\n\ndone:\n\treturn total;\n}\n\nvoid hns_roce_cleanup_bitmap(struct hns_roce_dev *hr_dev)\n{\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_XRC)\n\t\tida_destroy(&hr_dev->xrcd_ida.ida);\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_SRQ)\n\t\tida_destroy(&hr_dev->srq_table.srq_ida.ida);\n\thns_roce_cleanup_qp_table(hr_dev);\n\thns_roce_cleanup_cq_table(hr_dev);\n\tida_destroy(&hr_dev->mr_table.mtpt_ida.ida);\n\tida_destroy(&hr_dev->pd_ida.ida);\n\tida_destroy(&hr_dev->uar_ida.ida);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}