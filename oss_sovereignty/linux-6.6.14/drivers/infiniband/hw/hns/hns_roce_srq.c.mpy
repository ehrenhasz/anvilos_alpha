{
  "module_name": "hns_roce_srq.c",
  "hash_id": "5fb3be2cb87047d018b64aecf74c84936a6e81969051f509615d2589b0ca3c09",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hns/hns_roce_srq.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <rdma/ib_umem.h>\n#include \"hns_roce_device.h\"\n#include \"hns_roce_cmd.h\"\n#include \"hns_roce_hem.h\"\n\nvoid hns_roce_srq_event(struct hns_roce_dev *hr_dev, u32 srqn, int event_type)\n{\n\tstruct hns_roce_srq_table *srq_table = &hr_dev->srq_table;\n\tstruct hns_roce_srq *srq;\n\n\txa_lock(&srq_table->xa);\n\tsrq = xa_load(&srq_table->xa, srqn & (hr_dev->caps.num_srqs - 1));\n\tif (srq)\n\t\trefcount_inc(&srq->refcount);\n\txa_unlock(&srq_table->xa);\n\n\tif (!srq) {\n\t\tdev_warn(hr_dev->dev, \"Async event for bogus SRQ %08x\\n\", srqn);\n\t\treturn;\n\t}\n\n\tsrq->event(srq, event_type);\n\n\tif (refcount_dec_and_test(&srq->refcount))\n\t\tcomplete(&srq->free);\n}\n\nstatic void hns_roce_ib_srq_event(struct hns_roce_srq *srq,\n\t\t\t\t  enum hns_roce_event event_type)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(srq->ibsrq.device);\n\tstruct ib_srq *ibsrq = &srq->ibsrq;\n\tstruct ib_event event;\n\n\tif (ibsrq->event_handler) {\n\t\tevent.device      = ibsrq->device;\n\t\tevent.element.srq = ibsrq;\n\t\tswitch (event_type) {\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:\n\t\t\tevent.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_CATAS_ERROR:\n\t\t\tevent.event = IB_EVENT_SRQ_ERR;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hr_dev->dev,\n\t\t\t   \"hns_roce:Unexpected event type 0x%x on SRQ %06lx\\n\",\n\t\t\t   event_type, srq->srqn);\n\t\t\treturn;\n\t\t}\n\n\t\tibsrq->event_handler(&event, ibsrq->srq_context);\n\t}\n}\n\nstatic int alloc_srqn(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_ida *srq_ida = &hr_dev->srq_table.srq_ida;\n\tint id;\n\n\tid = ida_alloc_range(&srq_ida->ida, srq_ida->min, srq_ida->max,\n\t\t\t     GFP_KERNEL);\n\tif (id < 0) {\n\t\tibdev_err(&hr_dev->ib_dev, \"failed to alloc srq(%d).\\n\", id);\n\t\treturn -ENOMEM;\n\t}\n\n\tsrq->srqn = id;\n\n\treturn 0;\n}\n\nstatic void free_srqn(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tida_free(&hr_dev->srq_table.srq_ida.ida, (int)srq->srqn);\n}\n\nstatic int hns_roce_create_srqc(struct hns_roce_dev *hr_dev,\n\t\t\t\tstruct hns_roce_srq *srq)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox)) {\n\t\tibdev_err(ibdev, \"failed to alloc mailbox for SRQC.\\n\");\n\t\treturn PTR_ERR(mailbox);\n\t}\n\n\tret = hr_dev->hw->write_srqc(srq, mailbox->buf);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to write SRQC.\\n\");\n\t\tgoto err_mbox;\n\t}\n\n\tret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_SRQ,\n\t\t\t\t     srq->srqn);\n\tif (ret)\n\t\tibdev_err(ibdev, \"failed to config SRQC, ret = %d.\\n\", ret);\n\nerr_mbox:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\treturn ret;\n}\n\nstatic int alloc_srqc(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_srq_table *srq_table = &hr_dev->srq_table;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\tret = hns_roce_table_get(hr_dev, &srq_table->table, srq->srqn);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to get SRQC table, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = xa_err(xa_store(&srq_table->xa, srq->srqn, srq, GFP_KERNEL));\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to store SRQC, ret = %d.\\n\", ret);\n\t\tgoto err_put;\n\t}\n\n\tret = hns_roce_create_srqc(hr_dev, srq);\n\tif (ret)\n\t\tgoto err_xa;\n\n\treturn 0;\n\nerr_xa:\n\txa_erase(&srq_table->xa, srq->srqn);\nerr_put:\n\thns_roce_table_put(hr_dev, &srq_table->table, srq->srqn);\n\n\treturn ret;\n}\n\nstatic void free_srqc(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_srq_table *srq_table = &hr_dev->srq_table;\n\tint ret;\n\n\tret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_SRQ,\n\t\t\t\t      srq->srqn);\n\tif (ret)\n\t\tdev_err(hr_dev->dev, \"DESTROY_SRQ failed (%d) for SRQN %06lx\\n\",\n\t\t\tret, srq->srqn);\n\n\txa_erase(&srq_table->xa, srq->srqn);\n\n\tif (refcount_dec_and_test(&srq->refcount))\n\t\tcomplete(&srq->free);\n\twait_for_completion(&srq->free);\n\n\thns_roce_table_put(hr_dev, &srq_table->table, srq->srqn);\n}\n\nstatic int alloc_srq_idx(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq,\n\t\t\t struct ib_udata *udata, unsigned long addr)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_buf_attr buf_attr = {};\n\tint ret;\n\n\tsrq->idx_que.entry_shift = ilog2(HNS_ROCE_IDX_QUE_ENTRY_SZ);\n\n\tbuf_attr.page_shift = hr_dev->caps.idx_buf_pg_sz + PAGE_SHIFT;\n\tbuf_attr.region[0].size = to_hr_hem_entries_size(srq->wqe_cnt,\n\t\t\t\t\tsrq->idx_que.entry_shift);\n\tbuf_attr.region[0].hopnum = hr_dev->caps.idx_hop_num;\n\tbuf_attr.region_count = 1;\n\n\tret = hns_roce_mtr_create(hr_dev, &idx_que->mtr, &buf_attr,\n\t\t\t\t  hr_dev->caps.idx_ba_pg_sz + PAGE_SHIFT,\n\t\t\t\t  udata, addr);\n\tif (ret) {\n\t\tibdev_err(ibdev,\n\t\t\t  \"failed to alloc SRQ idx mtr, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (!udata) {\n\t\tidx_que->bitmap = bitmap_zalloc(srq->wqe_cnt, GFP_KERNEL);\n\t\tif (!idx_que->bitmap) {\n\t\t\tibdev_err(ibdev, \"failed to alloc SRQ idx bitmap.\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_idx_mtr;\n\t\t}\n\t}\n\n\tidx_que->head = 0;\n\tidx_que->tail = 0;\n\n\treturn 0;\nerr_idx_mtr:\n\thns_roce_mtr_destroy(hr_dev, &idx_que->mtr);\n\n\treturn ret;\n}\n\nstatic void free_srq_idx(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\n\tbitmap_free(idx_que->bitmap);\n\tidx_que->bitmap = NULL;\n\thns_roce_mtr_destroy(hr_dev, &idx_que->mtr);\n}\n\nstatic int alloc_srq_wqe_buf(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_srq *srq,\n\t\t\t     struct ib_udata *udata, unsigned long addr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_buf_attr buf_attr = {};\n\tint ret;\n\n\tsrq->wqe_shift = ilog2(roundup_pow_of_two(max(HNS_ROCE_SGE_SIZE,\n\t\t\t\t\t\t      HNS_ROCE_SGE_SIZE *\n\t\t\t\t\t\t      srq->max_gs)));\n\n\tbuf_attr.page_shift = hr_dev->caps.srqwqe_buf_pg_sz + PAGE_SHIFT;\n\tbuf_attr.region[0].size = to_hr_hem_entries_size(srq->wqe_cnt,\n\t\t\t\t\t\t\t srq->wqe_shift);\n\tbuf_attr.region[0].hopnum = hr_dev->caps.srqwqe_hop_num;\n\tbuf_attr.region_count = 1;\n\n\tret = hns_roce_mtr_create(hr_dev, &srq->buf_mtr, &buf_attr,\n\t\t\t\t  hr_dev->caps.srqwqe_ba_pg_sz + PAGE_SHIFT,\n\t\t\t\t  udata, addr);\n\tif (ret)\n\t\tibdev_err(ibdev,\n\t\t\t  \"failed to alloc SRQ buf mtr, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void free_srq_wqe_buf(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_srq *srq)\n{\n\thns_roce_mtr_destroy(hr_dev, &srq->buf_mtr);\n}\n\nstatic int alloc_srq_wrid(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tsrq->wrid = kvmalloc_array(srq->wqe_cnt, sizeof(u64), GFP_KERNEL);\n\tif (!srq->wrid)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void free_srq_wrid(struct hns_roce_srq *srq)\n{\n\tkvfree(srq->wrid);\n\tsrq->wrid = NULL;\n}\n\nstatic u32 proc_srq_sge(struct hns_roce_dev *dev, struct hns_roce_srq *hr_srq,\n\t\t\tbool user)\n{\n\tu32 max_sge = dev->caps.max_srq_sges;\n\n\tif (dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\treturn max_sge;\n\n\t \n\tif (user)\n\t\tmax_sge = roundup_pow_of_two(max_sge + 1);\n\telse\n\t\thr_srq->rsv_sge = 1;\n\n\treturn max_sge;\n}\n\nstatic int set_srq_basic_param(struct hns_roce_srq *srq,\n\t\t\t       struct ib_srq_init_attr *init_attr,\n\t\t\t       struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(srq->ibsrq.device);\n\tstruct ib_srq_attr *attr = &init_attr->attr;\n\tu32 max_sge;\n\n\tmax_sge = proc_srq_sge(hr_dev, srq, !!udata);\n\tif (attr->max_wr > hr_dev->caps.max_srq_wrs ||\n\t    attr->max_sge > max_sge) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"invalid SRQ attr, depth = %u, sge = %u.\\n\",\n\t\t\t  attr->max_wr, attr->max_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tattr->max_wr = max_t(u32, attr->max_wr, HNS_ROCE_MIN_SRQ_WQE_NUM);\n\tsrq->wqe_cnt = roundup_pow_of_two(attr->max_wr);\n\tsrq->max_gs = roundup_pow_of_two(attr->max_sge + srq->rsv_sge);\n\n\tattr->max_wr = srq->wqe_cnt;\n\tattr->max_sge = srq->max_gs - srq->rsv_sge;\n\tattr->srq_limit = 0;\n\n\treturn 0;\n}\n\nstatic void set_srq_ext_param(struct hns_roce_srq *srq,\n\t\t\t      struct ib_srq_init_attr *init_attr)\n{\n\tsrq->cqn = ib_srq_has_cq(init_attr->srq_type) ?\n\t\t   to_hr_cq(init_attr->ext.cq)->cqn : 0;\n\n\tsrq->xrcdn = (init_attr->srq_type == IB_SRQT_XRC) ?\n\t\t     to_hr_xrcd(init_attr->ext.xrc.xrcd)->xrcdn : 0;\n}\n\nstatic int set_srq_param(struct hns_roce_srq *srq,\n\t\t\t struct ib_srq_init_attr *init_attr,\n\t\t\t struct ib_udata *udata)\n{\n\tint ret;\n\n\tret = set_srq_basic_param(srq, init_attr, udata);\n\tif (ret)\n\t\treturn ret;\n\n\tset_srq_ext_param(srq, init_attr);\n\n\treturn 0;\n}\n\nstatic int alloc_srq_buf(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct hns_roce_ib_create_srq ucmd = {};\n\tint ret;\n\n\tif (udata) {\n\t\tret = ib_copy_from_udata(&ucmd, udata,\n\t\t\t\t\t min(udata->inlen, sizeof(ucmd)));\n\t\tif (ret) {\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"failed to copy SRQ udata, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = alloc_srq_idx(hr_dev, srq, udata, ucmd.que_addr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = alloc_srq_wqe_buf(hr_dev, srq, udata, ucmd.buf_addr);\n\tif (ret)\n\t\tgoto err_idx;\n\n\tif (!udata) {\n\t\tret = alloc_srq_wrid(hr_dev, srq);\n\t\tif (ret)\n\t\t\tgoto err_wqe_buf;\n\t}\n\n\treturn 0;\n\nerr_wqe_buf:\n\tfree_srq_wqe_buf(hr_dev, srq);\nerr_idx:\n\tfree_srq_idx(hr_dev, srq);\n\n\treturn ret;\n}\n\nstatic void free_srq_buf(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq)\n{\n\tfree_srq_wrid(srq);\n\tfree_srq_wqe_buf(hr_dev, srq);\n\tfree_srq_idx(hr_dev, srq);\n}\n\nint hns_roce_create_srq(struct ib_srq *ib_srq,\n\t\t\tstruct ib_srq_init_attr *init_attr,\n\t\t\tstruct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ib_srq->device);\n\tstruct hns_roce_ib_create_srq_resp resp = {};\n\tstruct hns_roce_srq *srq = to_hr_srq(ib_srq);\n\tint ret;\n\n\tmutex_init(&srq->mutex);\n\tspin_lock_init(&srq->lock);\n\n\tret = set_srq_param(srq, init_attr, udata);\n\tif (ret)\n\t\treturn ret;\n\n\tret = alloc_srq_buf(hr_dev, srq, udata);\n\tif (ret)\n\t\treturn ret;\n\n\tret = alloc_srqn(hr_dev, srq);\n\tif (ret)\n\t\tgoto err_srq_buf;\n\n\tret = alloc_srqc(hr_dev, srq);\n\tif (ret)\n\t\tgoto err_srqn;\n\n\tif (udata) {\n\t\tresp.srqn = srq->srqn;\n\t\tif (ib_copy_to_udata(udata, &resp,\n\t\t\t\t     min(udata->outlen, sizeof(resp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_srqc;\n\t\t}\n\t}\n\n\tsrq->db_reg = hr_dev->reg_base + SRQ_DB_REG;\n\tsrq->event = hns_roce_ib_srq_event;\n\trefcount_set(&srq->refcount, 1);\n\tinit_completion(&srq->free);\n\n\treturn 0;\n\nerr_srqc:\n\tfree_srqc(hr_dev, srq);\nerr_srqn:\n\tfree_srqn(hr_dev, srq);\nerr_srq_buf:\n\tfree_srq_buf(hr_dev, srq);\n\n\treturn ret;\n}\n\nint hns_roce_destroy_srq(struct ib_srq *ibsrq, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);\n\tstruct hns_roce_srq *srq = to_hr_srq(ibsrq);\n\n\tfree_srqc(hr_dev, srq);\n\tfree_srqn(hr_dev, srq);\n\tfree_srq_buf(hr_dev, srq);\n\treturn 0;\n}\n\nvoid hns_roce_init_srq_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_srq_table *srq_table = &hr_dev->srq_table;\n\tstruct hns_roce_ida *srq_ida = &srq_table->srq_ida;\n\n\txa_init(&srq_table->xa);\n\n\tida_init(&srq_ida->ida);\n\tsrq_ida->max = hr_dev->caps.num_srqs - 1;\n\tsrq_ida->min = hr_dev->caps.reserved_srqs;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}