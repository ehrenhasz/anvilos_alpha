{
  "module_name": "hns_roce_mr.c",
  "hash_id": "95ff1a7e30e7dd729693630bea6eb2c4a34153e3afb4f646b6a86f5ac591c5a8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hns/hns_roce_mr.c",
  "human_readable_source": " \n\n#include <linux/vmalloc.h>\n#include <rdma/ib_umem.h>\n#include <linux/math.h>\n#include \"hns_roce_device.h\"\n#include \"hns_roce_cmd.h\"\n#include \"hns_roce_hem.h\"\n\nstatic u32 hw_index_to_key(int ind)\n{\n\treturn ((u32)ind >> 24) | ((u32)ind << 8);\n}\n\nunsigned long key_to_hw_index(u32 key)\n{\n\treturn (key << 24) | (key >> 8);\n}\n\nstatic int alloc_mr_key(struct hns_roce_dev *hr_dev, struct hns_roce_mr *mr)\n{\n\tstruct hns_roce_ida *mtpt_ida = &hr_dev->mr_table.mtpt_ida;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint err;\n\tint id;\n\n\t \n\tid = ida_alloc_range(&mtpt_ida->ida, mtpt_ida->min, mtpt_ida->max,\n\t\t\t     GFP_KERNEL);\n\tif (id < 0) {\n\t\tibdev_err(ibdev, \"failed to alloc id for MR key, id(%d)\\n\", id);\n\t\treturn -ENOMEM;\n\t}\n\n\tmr->key = hw_index_to_key(id);  \n\n\terr = hns_roce_table_get(hr_dev, &hr_dev->mr_table.mtpt_table,\n\t\t\t\t (unsigned long)id);\n\tif (err) {\n\t\tibdev_err(ibdev, \"failed to alloc mtpt, ret = %d.\\n\", err);\n\t\tgoto err_free_bitmap;\n\t}\n\n\treturn 0;\nerr_free_bitmap:\n\tida_free(&mtpt_ida->ida, id);\n\treturn err;\n}\n\nstatic void free_mr_key(struct hns_roce_dev *hr_dev, struct hns_roce_mr *mr)\n{\n\tunsigned long obj = key_to_hw_index(mr->key);\n\n\thns_roce_table_put(hr_dev, &hr_dev->mr_table.mtpt_table, obj);\n\tida_free(&hr_dev->mr_table.mtpt_ida.ida, (int)obj);\n}\n\nstatic int alloc_mr_pbl(struct hns_roce_dev *hr_dev, struct hns_roce_mr *mr,\n\t\t\tstruct ib_udata *udata, u64 start)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tbool is_fast = mr->type == MR_TYPE_FRMR;\n\tstruct hns_roce_buf_attr buf_attr = {};\n\tint err;\n\n\tmr->pbl_hop_num = is_fast ? 1 : hr_dev->caps.pbl_hop_num;\n\tbuf_attr.page_shift = is_fast ? PAGE_SHIFT :\n\t\t\t      hr_dev->caps.pbl_buf_pg_sz + PAGE_SHIFT;\n\tbuf_attr.region[0].size = mr->size;\n\tbuf_attr.region[0].hopnum = mr->pbl_hop_num;\n\tbuf_attr.region_count = 1;\n\tbuf_attr.user_access = mr->access;\n\t \n\tbuf_attr.mtt_only = is_fast;\n\n\terr = hns_roce_mtr_create(hr_dev, &mr->pbl_mtr, &buf_attr,\n\t\t\t\t  hr_dev->caps.pbl_ba_pg_sz + PAGE_SHIFT,\n\t\t\t\t  udata, start);\n\tif (err)\n\t\tibdev_err(ibdev, \"failed to alloc pbl mtr, ret = %d.\\n\", err);\n\telse\n\t\tmr->npages = mr->pbl_mtr.hem_cfg.buf_pg_count;\n\n\treturn err;\n}\n\nstatic void free_mr_pbl(struct hns_roce_dev *hr_dev, struct hns_roce_mr *mr)\n{\n\thns_roce_mtr_destroy(hr_dev, &mr->pbl_mtr);\n}\n\nstatic void hns_roce_mr_free(struct hns_roce_dev *hr_dev, struct hns_roce_mr *mr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\tif (mr->enabled) {\n\t\tret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_MPT,\n\t\t\t\t\t      key_to_hw_index(mr->key) &\n\t\t\t\t\t      (hr_dev->caps.num_mtpts - 1));\n\t\tif (ret)\n\t\t\tibdev_warn(ibdev, \"failed to destroy mpt, ret = %d.\\n\",\n\t\t\t\t   ret);\n\t}\n\n\tfree_mr_pbl(hr_dev, mr);\n\tfree_mr_key(hr_dev, mr);\n}\n\nstatic int hns_roce_mr_enable(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_mr *mr)\n{\n\tunsigned long mtpt_idx = key_to_hw_index(mr->key);\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\t \n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tif (mr->type != MR_TYPE_FRMR)\n\t\tret = hr_dev->hw->write_mtpt(hr_dev, mailbox->buf, mr);\n\telse\n\t\tret = hr_dev->hw->frmr_write_mtpt(hr_dev, mailbox->buf, mr);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write mtpt, ret = %d.\\n\", ret);\n\t\tgoto err_page;\n\t}\n\n\tret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_MPT,\n\t\t\t\t     mtpt_idx & (hr_dev->caps.num_mtpts - 1));\n\tif (ret) {\n\t\tdev_err(dev, \"failed to create mpt, ret = %d.\\n\", ret);\n\t\tgoto err_page;\n\t}\n\n\tmr->enabled = 1;\n\nerr_page:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn ret;\n}\n\nvoid hns_roce_init_mr_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_ida *mtpt_ida = &hr_dev->mr_table.mtpt_ida;\n\n\tida_init(&mtpt_ida->ida);\n\tmtpt_ida->max = hr_dev->caps.num_mtpts - 1;\n\tmtpt_ida->min = hr_dev->caps.reserved_mrws;\n}\n\nstruct ib_mr *hns_roce_get_dma_mr(struct ib_pd *pd, int acc)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(pd->device);\n\tstruct hns_roce_mr *mr;\n\tint ret;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn  ERR_PTR(-ENOMEM);\n\n\tmr->type = MR_TYPE_DMA;\n\tmr->pd = to_hr_pd(pd)->pdn;\n\tmr->access = acc;\n\n\t \n\thns_roce_hem_list_init(&mr->pbl_mtr.hem_list);\n\tret = alloc_mr_key(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_free;\n\n\tret = hns_roce_mr_enable(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_mr;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->key;\n\n\treturn &mr->ibmr;\nerr_mr:\n\tfree_mr_key(hr_dev, mr);\n\nerr_free:\n\tkfree(mr);\n\treturn ERR_PTR(ret);\n}\n\nstruct ib_mr *hns_roce_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t\t   u64 virt_addr, int access_flags,\n\t\t\t\t   struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(pd->device);\n\tstruct hns_roce_mr *mr;\n\tint ret;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->iova = virt_addr;\n\tmr->size = length;\n\tmr->pd = to_hr_pd(pd)->pdn;\n\tmr->access = access_flags;\n\tmr->type = MR_TYPE_MR;\n\n\tret = alloc_mr_key(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_alloc_mr;\n\n\tret = alloc_mr_pbl(hr_dev, mr, udata, start);\n\tif (ret)\n\t\tgoto err_alloc_key;\n\n\tret = hns_roce_mr_enable(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_alloc_pbl;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->key;\n\n\treturn &mr->ibmr;\n\nerr_alloc_pbl:\n\tfree_mr_pbl(hr_dev, mr);\nerr_alloc_key:\n\tfree_mr_key(hr_dev, mr);\nerr_alloc_mr:\n\tkfree(mr);\n\treturn ERR_PTR(ret);\n}\n\nstruct ib_mr *hns_roce_rereg_user_mr(struct ib_mr *ibmr, int flags, u64 start,\n\t\t\t\t     u64 length, u64 virt_addr,\n\t\t\t\t     int mr_access_flags, struct ib_pd *pd,\n\t\t\t\t     struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);\n\tstruct ib_device *ib_dev = &hr_dev->ib_dev;\n\tstruct hns_roce_mr *mr = to_hr_mr(ibmr);\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tunsigned long mtpt_idx;\n\tint ret;\n\n\tif (!mr->enabled)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn ERR_CAST(mailbox);\n\n\tmtpt_idx = key_to_hw_index(mr->key) & (hr_dev->caps.num_mtpts - 1);\n\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_MPT,\n\t\t\t\tmtpt_idx);\n\tif (ret)\n\t\tgoto free_cmd_mbox;\n\n\tret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_MPT,\n\t\t\t\t      mtpt_idx);\n\tif (ret)\n\t\tibdev_warn(ib_dev, \"failed to destroy MPT, ret = %d.\\n\", ret);\n\n\tmr->enabled = 0;\n\tmr->iova = virt_addr;\n\tmr->size = length;\n\n\tif (flags & IB_MR_REREG_PD)\n\t\tmr->pd = to_hr_pd(pd)->pdn;\n\n\tif (flags & IB_MR_REREG_ACCESS)\n\t\tmr->access = mr_access_flags;\n\n\tif (flags & IB_MR_REREG_TRANS) {\n\t\tfree_mr_pbl(hr_dev, mr);\n\t\tret = alloc_mr_pbl(hr_dev, mr, udata, start);\n\t\tif (ret) {\n\t\t\tibdev_err(ib_dev, \"failed to alloc mr PBL, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\tgoto free_cmd_mbox;\n\t\t}\n\t}\n\n\tret = hr_dev->hw->rereg_write_mtpt(hr_dev, mr, flags, mailbox->buf);\n\tif (ret) {\n\t\tibdev_err(ib_dev, \"failed to write mtpt, ret = %d.\\n\", ret);\n\t\tgoto free_cmd_mbox;\n\t}\n\n\tret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_MPT,\n\t\t\t\t     mtpt_idx);\n\tif (ret) {\n\t\tibdev_err(ib_dev, \"failed to create MPT, ret = %d.\\n\", ret);\n\t\tgoto free_cmd_mbox;\n\t}\n\n\tmr->enabled = 1;\n\nfree_cmd_mbox:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\treturn NULL;\n}\n\nint hns_roce_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);\n\tstruct hns_roce_mr *mr = to_hr_mr(ibmr);\n\n\tif (hr_dev->hw->dereg_mr)\n\t\thr_dev->hw->dereg_mr(hr_dev);\n\n\thns_roce_mr_free(hr_dev, mr);\n\tkfree(mr);\n\n\treturn 0;\n}\n\nstruct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t\tu32 max_num_sg)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(pd->device);\n\tstruct device *dev = hr_dev->dev;\n\tstruct hns_roce_mr *mr;\n\tint ret;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (max_num_sg > HNS_ROCE_FRMR_MAX_PA) {\n\t\tdev_err(dev, \"max_num_sg larger than %d\\n\",\n\t\t\tHNS_ROCE_FRMR_MAX_PA);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->type = MR_TYPE_FRMR;\n\tmr->pd = to_hr_pd(pd)->pdn;\n\tmr->size = max_num_sg * (1 << PAGE_SHIFT);\n\n\t \n\tret = alloc_mr_key(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_free;\n\n\tret = alloc_mr_pbl(hr_dev, mr, NULL, 0);\n\tif (ret)\n\t\tgoto err_key;\n\n\tret = hns_roce_mr_enable(hr_dev, mr);\n\tif (ret)\n\t\tgoto err_pbl;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->key;\n\tmr->ibmr.length = mr->size;\n\n\treturn &mr->ibmr;\n\nerr_pbl:\n\tfree_mr_pbl(hr_dev, mr);\nerr_key:\n\tfree_mr_key(hr_dev, mr);\nerr_free:\n\tkfree(mr);\n\treturn ERR_PTR(ret);\n}\n\nstatic int hns_roce_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct hns_roce_mr *mr = to_hr_mr(ibmr);\n\n\tif (likely(mr->npages < mr->pbl_mtr.hem_cfg.buf_pg_count)) {\n\t\tmr->page_list[mr->npages++] = addr;\n\t\treturn 0;\n\t}\n\n\treturn -ENOBUFS;\n}\n\nint hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t       unsigned int *sg_offset)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_mr *mr = to_hr_mr(ibmr);\n\tstruct hns_roce_mtr *mtr = &mr->pbl_mtr;\n\tint ret = 0;\n\n\tmr->npages = 0;\n\tmr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count,\n\t\t\t\t sizeof(dma_addr_t), GFP_KERNEL);\n\tif (!mr->page_list)\n\t\treturn ret;\n\n\tret = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page);\n\tif (ret < 1) {\n\t\tibdev_err(ibdev, \"failed to store sg pages %u %u, cnt = %d.\\n\",\n\t\t\t  mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, ret);\n\t\tgoto err_page_list;\n\t}\n\n\tmtr->hem_cfg.region[0].offset = 0;\n\tmtr->hem_cfg.region[0].count = mr->npages;\n\tmtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num;\n\tmtr->hem_cfg.region_count = 1;\n\tret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to map sg mtr, ret = %d.\\n\", ret);\n\t\tret = 0;\n\t} else {\n\t\tmr->pbl_mtr.hem_cfg.buf_pg_shift = (u32)ilog2(ibmr->page_size);\n\t\tret = mr->npages;\n\t}\n\nerr_page_list:\n\tkvfree(mr->page_list);\n\tmr->page_list = NULL;\n\n\treturn ret;\n}\n\nstatic void hns_roce_mw_free(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_mw *mw)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tif (mw->enabled) {\n\t\tret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_MPT,\n\t\t\t\t\t      key_to_hw_index(mw->rkey) &\n\t\t\t\t\t      (hr_dev->caps.num_mtpts - 1));\n\t\tif (ret)\n\t\t\tdev_warn(dev, \"MW DESTROY_MPT failed (%d)\\n\", ret);\n\n\t\thns_roce_table_put(hr_dev, &hr_dev->mr_table.mtpt_table,\n\t\t\t\t   key_to_hw_index(mw->rkey));\n\t}\n\n\tida_free(&hr_dev->mr_table.mtpt_ida.ida,\n\t\t (int)key_to_hw_index(mw->rkey));\n}\n\nstatic int hns_roce_mw_enable(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_mw *mw)\n{\n\tstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tstruct device *dev = hr_dev->dev;\n\tunsigned long mtpt_idx = key_to_hw_index(mw->rkey);\n\tint ret;\n\n\t \n\tret = hns_roce_table_get(hr_dev, &mr_table->mtpt_table, mtpt_idx);\n\tif (ret)\n\t\treturn ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox)) {\n\t\tret = PTR_ERR(mailbox);\n\t\tgoto err_table;\n\t}\n\n\tret = hr_dev->hw->mw_write_mtpt(mailbox->buf, mw);\n\tif (ret) {\n\t\tdev_err(dev, \"MW write mtpt fail!\\n\");\n\t\tgoto err_page;\n\t}\n\n\tret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_MPT,\n\t\t\t\t     mtpt_idx & (hr_dev->caps.num_mtpts - 1));\n\tif (ret) {\n\t\tdev_err(dev, \"MW CREATE_MPT failed (%d)\\n\", ret);\n\t\tgoto err_page;\n\t}\n\n\tmw->enabled = 1;\n\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn 0;\n\nerr_page:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\nerr_table:\n\thns_roce_table_put(hr_dev, &mr_table->mtpt_table, mtpt_idx);\n\n\treturn ret;\n}\n\nint hns_roce_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibmw->device);\n\tstruct hns_roce_ida *mtpt_ida = &hr_dev->mr_table.mtpt_ida;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_mw *mw = to_hr_mw(ibmw);\n\tint ret;\n\tint id;\n\n\t \n\tid = ida_alloc_range(&mtpt_ida->ida, mtpt_ida->min, mtpt_ida->max,\n\t\t\t     GFP_KERNEL);\n\tif (id < 0) {\n\t\tibdev_err(ibdev, \"failed to alloc id for MW key, id(%d)\\n\", id);\n\t\treturn -ENOMEM;\n\t}\n\n\tmw->rkey = hw_index_to_key(id);\n\n\tibmw->rkey = mw->rkey;\n\tmw->pdn = to_hr_pd(ibmw->pd)->pdn;\n\tmw->pbl_hop_num = hr_dev->caps.pbl_hop_num;\n\tmw->pbl_ba_pg_sz = hr_dev->caps.pbl_ba_pg_sz;\n\tmw->pbl_buf_pg_sz = hr_dev->caps.pbl_buf_pg_sz;\n\n\tret = hns_roce_mw_enable(hr_dev, mw);\n\tif (ret)\n\t\tgoto err_mw;\n\n\treturn 0;\n\nerr_mw:\n\thns_roce_mw_free(hr_dev, mw);\n\treturn ret;\n}\n\nint hns_roce_dealloc_mw(struct ib_mw *ibmw)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibmw->device);\n\tstruct hns_roce_mw *mw = to_hr_mw(ibmw);\n\n\thns_roce_mw_free(hr_dev, mw);\n\treturn 0;\n}\n\nstatic int mtr_map_region(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t\t  struct hns_roce_buf_region *region, dma_addr_t *pages,\n\t\t\t  int max_count)\n{\n\tint count, npage;\n\tint offset, end;\n\t__le64 *mtts;\n\tu64 addr;\n\tint i;\n\n\toffset = region->offset;\n\tend = offset + region->count;\n\tnpage = 0;\n\twhile (offset < end && npage < max_count) {\n\t\tcount = 0;\n\t\tmtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,\n\t\t\t\t\t\t  offset, &count);\n\t\tif (!mtts)\n\t\t\treturn -ENOBUFS;\n\n\t\tfor (i = 0; i < count && npage < max_count; i++) {\n\t\t\taddr = pages[npage];\n\n\t\t\tmtts[i] = cpu_to_le64(addr);\n\t\t\tnpage++;\n\t\t}\n\t\toffset += count;\n\t}\n\n\treturn npage;\n}\n\nstatic inline bool mtr_has_mtt(struct hns_roce_buf_attr *attr)\n{\n\tint i;\n\n\tfor (i = 0; i < attr->region_count; i++)\n\t\tif (attr->region[i].hopnum != HNS_ROCE_HOP_NUM_0 &&\n\t\t    attr->region[i].hopnum > 0)\n\t\t\treturn true;\n\n\t \n\treturn false;\n}\n\nstatic inline size_t mtr_bufs_size(struct hns_roce_buf_attr *attr)\n{\n\tsize_t size = 0;\n\tint i;\n\n\tfor (i = 0; i < attr->region_count; i++)\n\t\tsize += attr->region[i].size;\n\n\treturn size;\n}\n\n \nstatic inline int mtr_check_direct_pages(dma_addr_t *pages, int page_count,\n\t\t\t\t\t unsigned int page_shift)\n{\n\tsize_t page_size = 1 << page_shift;\n\tint i;\n\n\tfor (i = 1; i < page_count; i++)\n\t\tif (pages[i] - pages[i - 1] != page_size)\n\t\t\treturn i;\n\n\treturn 0;\n}\n\nstatic void mtr_free_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)\n{\n\t \n\tif (mtr->umem) {\n\t\tib_umem_release(mtr->umem);\n\t\tmtr->umem = NULL;\n\t}\n\n\t \n\tif (mtr->kmem) {\n\t\thns_roce_buf_free(hr_dev, mtr->kmem);\n\t\tmtr->kmem = NULL;\n\t}\n}\n\nstatic int mtr_alloc_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t\t  struct hns_roce_buf_attr *buf_attr,\n\t\t\t  struct ib_udata *udata, unsigned long user_addr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tsize_t total_size;\n\n\ttotal_size = mtr_bufs_size(buf_attr);\n\n\tif (udata) {\n\t\tmtr->kmem = NULL;\n\t\tmtr->umem = ib_umem_get(ibdev, user_addr, total_size,\n\t\t\t\t\tbuf_attr->user_access);\n\t\tif (IS_ERR_OR_NULL(mtr->umem)) {\n\t\t\tibdev_err(ibdev, \"failed to get umem, ret = %ld.\\n\",\n\t\t\t\t  PTR_ERR(mtr->umem));\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else {\n\t\tmtr->umem = NULL;\n\t\tmtr->kmem = hns_roce_buf_alloc(hr_dev, total_size,\n\t\t\t\t\t       buf_attr->page_shift,\n\t\t\t\t\t       mtr->hem_cfg.is_direct ?\n\t\t\t\t\t       HNS_ROCE_BUF_DIRECT : 0);\n\t\tif (IS_ERR(mtr->kmem)) {\n\t\t\tibdev_err(ibdev, \"failed to alloc kmem, ret = %ld.\\n\",\n\t\t\t\t  PTR_ERR(mtr->kmem));\n\t\t\treturn PTR_ERR(mtr->kmem);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mtr_map_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t\tint page_count, unsigned int page_shift)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tdma_addr_t *pages;\n\tint npage;\n\tint ret;\n\n\t \n\tpages = kvcalloc(page_count, sizeof(dma_addr_t), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tif (mtr->umem)\n\t\tnpage = hns_roce_get_umem_bufs(hr_dev, pages, page_count,\n\t\t\t\t\t       mtr->umem, page_shift);\n\telse\n\t\tnpage = hns_roce_get_kmem_bufs(hr_dev, pages, page_count,\n\t\t\t\t\t       mtr->kmem, page_shift);\n\n\tif (npage != page_count) {\n\t\tibdev_err(ibdev, \"failed to get mtr page %d != %d.\\n\", npage,\n\t\t\t  page_count);\n\t\tret = -ENOBUFS;\n\t\tgoto err_alloc_list;\n\t}\n\n\tif (mtr->hem_cfg.is_direct && npage > 1) {\n\t\tret = mtr_check_direct_pages(pages, npage, page_shift);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev, \"failed to check %s page: %d / %d.\\n\",\n\t\t\t\t  mtr->umem ? \"umtr\" : \"kmtr\", ret, npage);\n\t\t\tret = -ENOBUFS;\n\t\t\tgoto err_alloc_list;\n\t\t}\n\t}\n\n\tret = hns_roce_mtr_map(hr_dev, mtr, pages, page_count);\n\tif (ret)\n\t\tibdev_err(ibdev, \"failed to map mtr pages, ret = %d.\\n\", ret);\n\nerr_alloc_list:\n\tkvfree(pages);\n\n\treturn ret;\n}\n\nint hns_roce_mtr_map(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t     dma_addr_t *pages, unsigned int page_cnt)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_buf_region *r;\n\tunsigned int i, mapped_cnt;\n\tint ret = 0;\n\n\t \n\tif (mtr->hem_cfg.is_direct) {\n\t\tmtr->hem_cfg.root_ba = pages[0];\n\t\treturn 0;\n\t}\n\n\tfor (i = 0, mapped_cnt = 0; i < mtr->hem_cfg.region_count &&\n\t     mapped_cnt < page_cnt; i++) {\n\t\tr = &mtr->hem_cfg.region[i];\n\t\t \n\t\tif (!r->hopnum) {\n\t\t\tmapped_cnt += r->count;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (r->offset + r->count > page_cnt) {\n\t\t\tret = -EINVAL;\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to check mtr%u count %u + %u > %u.\\n\",\n\t\t\t\t  i, r->offset, r->count, page_cnt);\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = mtr_map_region(hr_dev, mtr, r, &pages[r->offset],\n\t\t\t\t     page_cnt - mapped_cnt);\n\t\tif (ret < 0) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to map mtr%u offset %u, ret = %d.\\n\",\n\t\t\t\t  i, r->offset, ret);\n\t\t\treturn ret;\n\t\t}\n\t\tmapped_cnt += ret;\n\t\tret = 0;\n\t}\n\n\tif (mapped_cnt < page_cnt) {\n\t\tret = -ENOBUFS;\n\t\tibdev_err(ibdev, \"failed to map mtr pages count: %u < %u.\\n\",\n\t\t\t  mapped_cnt, page_cnt);\n\t}\n\n\treturn ret;\n}\n\nint hns_roce_mtr_find(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t      u32 offset, u64 *mtt_buf, int mtt_max, u64 *base_addr)\n{\n\tstruct hns_roce_hem_cfg *cfg = &mtr->hem_cfg;\n\tint mtt_count, left;\n\tu32 start_index;\n\tint total = 0;\n\t__le64 *mtts;\n\tu32 npage;\n\tu64 addr;\n\n\tif (!mtt_buf || mtt_max < 1)\n\t\tgoto done;\n\n\t \n\tif (cfg->is_direct) {\n\t\tstart_index = offset >> HNS_HW_PAGE_SHIFT;\n\t\tfor (mtt_count = 0; mtt_count < cfg->region_count &&\n\t\t     total < mtt_max; mtt_count++) {\n\t\t\tnpage = cfg->region[mtt_count].offset;\n\t\t\tif (npage < start_index)\n\t\t\t\tcontinue;\n\n\t\t\taddr = cfg->root_ba + (npage << HNS_HW_PAGE_SHIFT);\n\t\t\tmtt_buf[total] = addr;\n\n\t\t\ttotal++;\n\t\t}\n\n\t\tgoto done;\n\t}\n\n\tstart_index = offset >> cfg->buf_pg_shift;\n\tleft = mtt_max;\n\twhile (left > 0) {\n\t\tmtt_count = 0;\n\t\tmtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,\n\t\t\t\t\t\t  start_index + total,\n\t\t\t\t\t\t  &mtt_count);\n\t\tif (!mtts || !mtt_count)\n\t\t\tgoto done;\n\n\t\tnpage = min(mtt_count, left);\n\t\tleft -= npage;\n\t\tfor (mtt_count = 0; mtt_count < npage; mtt_count++)\n\t\t\tmtt_buf[total++] = le64_to_cpu(mtts[mtt_count]);\n\t}\n\ndone:\n\tif (base_addr)\n\t\t*base_addr = cfg->root_ba;\n\n\treturn total;\n}\n\nstatic int mtr_init_buf_cfg(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_buf_attr *attr,\n\t\t\t    struct hns_roce_hem_cfg *cfg,\n\t\t\t    unsigned int *buf_page_shift, u64 unalinged_size)\n{\n\tstruct hns_roce_buf_region *r;\n\tu64 first_region_padding;\n\tint page_cnt, region_cnt;\n\tunsigned int page_shift;\n\tsize_t buf_size;\n\n\t \n\tcfg->is_direct = !mtr_has_mtt(attr);\n\tbuf_size = mtr_bufs_size(attr);\n\tif (cfg->is_direct) {\n\t\t \n\t\tpage_shift = HNS_HW_PAGE_SHIFT;\n\n\t\t \n\t\tcfg->buf_pg_count = 1;\n\t\tcfg->buf_pg_shift = HNS_HW_PAGE_SHIFT +\n\t\t\torder_base_2(DIV_ROUND_UP(buf_size, HNS_HW_PAGE_SIZE));\n\t\tfirst_region_padding = 0;\n\t} else {\n\t\tpage_shift = attr->page_shift;\n\t\tcfg->buf_pg_count = DIV_ROUND_UP(buf_size + unalinged_size,\n\t\t\t\t\t\t 1 << page_shift);\n\t\tcfg->buf_pg_shift = page_shift;\n\t\tfirst_region_padding = unalinged_size;\n\t}\n\n\t \n\tfor (page_cnt = 0, region_cnt = 0; region_cnt < attr->region_count &&\n\t     region_cnt < ARRAY_SIZE(cfg->region); region_cnt++) {\n\t\tr = &cfg->region[region_cnt];\n\t\tr->offset = page_cnt;\n\t\tbuf_size = hr_hw_page_align(attr->region[region_cnt].size +\n\t\t\t\t\t    first_region_padding);\n\t\tr->count = DIV_ROUND_UP(buf_size, 1 << page_shift);\n\t\tfirst_region_padding = 0;\n\t\tpage_cnt += r->count;\n\t\tr->hopnum = to_hr_hem_hopnum(attr->region[region_cnt].hopnum,\n\t\t\t\t\t     r->count);\n\t}\n\n\tcfg->region_count = region_cnt;\n\t*buf_page_shift = page_shift;\n\n\treturn page_cnt;\n}\n\nstatic u64 cal_pages_per_l1ba(unsigned int ba_per_bt, unsigned int hopnum)\n{\n\treturn int_pow(ba_per_bt, hopnum - 1);\n}\n\nstatic unsigned int cal_best_bt_pg_sz(struct hns_roce_dev *hr_dev,\n\t\t\t\t      struct hns_roce_mtr *mtr,\n\t\t\t\t      unsigned int pg_shift)\n{\n\tunsigned long cap = hr_dev->caps.page_size_cap;\n\tstruct hns_roce_buf_region *re;\n\tunsigned int pgs_per_l1ba;\n\tunsigned int ba_per_bt;\n\tunsigned int ba_num;\n\tint i;\n\n\tfor_each_set_bit_from(pg_shift, &cap, sizeof(cap) * BITS_PER_BYTE) {\n\t\tif (!(BIT(pg_shift) & cap))\n\t\t\tcontinue;\n\n\t\tba_per_bt = BIT(pg_shift) / BA_BYTE_LEN;\n\t\tba_num = 0;\n\t\tfor (i = 0; i < mtr->hem_cfg.region_count; i++) {\n\t\t\tre = &mtr->hem_cfg.region[i];\n\t\t\tif (re->hopnum == 0)\n\t\t\t\tcontinue;\n\n\t\t\tpgs_per_l1ba = cal_pages_per_l1ba(ba_per_bt, re->hopnum);\n\t\t\tba_num += DIV_ROUND_UP(re->count, pgs_per_l1ba);\n\t\t}\n\n\t\tif (ba_num <= ba_per_bt)\n\t\t\treturn pg_shift;\n\t}\n\n\treturn 0;\n}\n\nstatic int mtr_alloc_mtt(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t\t unsigned int ba_page_shift)\n{\n\tstruct hns_roce_hem_cfg *cfg = &mtr->hem_cfg;\n\tint ret;\n\n\thns_roce_hem_list_init(&mtr->hem_list);\n\tif (!cfg->is_direct) {\n\t\tba_page_shift = cal_best_bt_pg_sz(hr_dev, mtr, ba_page_shift);\n\t\tif (!ba_page_shift)\n\t\t\treturn -ERANGE;\n\n\t\tret = hns_roce_hem_list_request(hr_dev, &mtr->hem_list,\n\t\t\t\t\t\tcfg->region, cfg->region_count,\n\t\t\t\t\t\tba_page_shift);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tcfg->root_ba = mtr->hem_list.root_ba;\n\t\tcfg->ba_pg_shift = ba_page_shift;\n\t} else {\n\t\tcfg->ba_pg_shift = cfg->buf_pg_shift;\n\t}\n\n\treturn 0;\n}\n\nstatic void mtr_free_mtt(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)\n{\n\thns_roce_hem_list_release(hr_dev, &mtr->hem_list);\n}\n\n \nint hns_roce_mtr_create(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,\n\t\t\tstruct hns_roce_buf_attr *buf_attr,\n\t\t\tunsigned int ba_page_shift, struct ib_udata *udata,\n\t\t\tunsigned long user_addr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tunsigned int buf_page_shift = 0;\n\tint buf_page_cnt;\n\tint ret;\n\n\tbuf_page_cnt = mtr_init_buf_cfg(hr_dev, buf_attr, &mtr->hem_cfg,\n\t\t\t\t\t&buf_page_shift,\n\t\t\t\t\tudata ? user_addr & ~PAGE_MASK : 0);\n\tif (buf_page_cnt < 1 || buf_page_shift < HNS_HW_PAGE_SHIFT) {\n\t\tibdev_err(ibdev, \"failed to init mtr cfg, count %d shift %u.\\n\",\n\t\t\t  buf_page_cnt, buf_page_shift);\n\t\treturn -EINVAL;\n\t}\n\n\tret = mtr_alloc_mtt(hr_dev, mtr, ba_page_shift);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc mtr mtt, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tif (buf_attr->mtt_only) {\n\t\tmtr->umem = NULL;\n\t\tmtr->kmem = NULL;\n\t\treturn 0;\n\t}\n\n\tret = mtr_alloc_bufs(hr_dev, mtr, buf_attr, udata, user_addr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to alloc mtr bufs, ret = %d.\\n\", ret);\n\t\tgoto err_alloc_mtt;\n\t}\n\n\t \n\tret = mtr_map_bufs(hr_dev, mtr, buf_page_cnt, buf_page_shift);\n\tif (ret)\n\t\tibdev_err(ibdev, \"failed to map mtr bufs, ret = %d.\\n\", ret);\n\telse\n\t\treturn 0;\n\n\tmtr_free_bufs(hr_dev, mtr);\nerr_alloc_mtt:\n\tmtr_free_mtt(hr_dev, mtr);\n\treturn ret;\n}\n\nvoid hns_roce_mtr_destroy(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)\n{\n\t \n\thns_roce_hem_list_release(hr_dev, &mtr->hem_list);\n\n\t \n\tmtr_free_bufs(hr_dev, mtr);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}