{
  "module_name": "hns_roce_hw_v2.c",
  "hash_id": "f42d5414fd74673b0ac81bb879685cc12a05503f12e854726ebe7db05272a550",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hns/hns_roce_hw_v2.c",
  "human_readable_source": " \n\n#include <linux/acpi.h>\n#include <linux/etherdevice.h>\n#include <linux/interrupt.h>\n#include <linux/iopoll.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <net/addrconf.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n#include <rdma/ib_umem.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"hnae3.h\"\n#include \"hns_roce_common.h\"\n#include \"hns_roce_device.h\"\n#include \"hns_roce_cmd.h\"\n#include \"hns_roce_hem.h\"\n#include \"hns_roce_hw_v2.h\"\n\nenum {\n\tCMD_RST_PRC_OTHERS,\n\tCMD_RST_PRC_SUCCESS,\n\tCMD_RST_PRC_EBUSY,\n};\n\nenum ecc_resource_type {\n\tECC_RESOURCE_QPC,\n\tECC_RESOURCE_CQC,\n\tECC_RESOURCE_MPT,\n\tECC_RESOURCE_SRQC,\n\tECC_RESOURCE_GMV,\n\tECC_RESOURCE_QPC_TIMER,\n\tECC_RESOURCE_CQC_TIMER,\n\tECC_RESOURCE_SCCC,\n\tECC_RESOURCE_COUNT,\n};\n\nstatic const struct {\n\tconst char *name;\n\tu8 read_bt0_op;\n\tu8 write_bt0_op;\n} fmea_ram_res[] = {\n\t{ \"ECC_RESOURCE_QPC\",\n\t  HNS_ROCE_CMD_READ_QPC_BT0, HNS_ROCE_CMD_WRITE_QPC_BT0 },\n\t{ \"ECC_RESOURCE_CQC\",\n\t  HNS_ROCE_CMD_READ_CQC_BT0, HNS_ROCE_CMD_WRITE_CQC_BT0 },\n\t{ \"ECC_RESOURCE_MPT\",\n\t  HNS_ROCE_CMD_READ_MPT_BT0, HNS_ROCE_CMD_WRITE_MPT_BT0 },\n\t{ \"ECC_RESOURCE_SRQC\",\n\t  HNS_ROCE_CMD_READ_SRQC_BT0, HNS_ROCE_CMD_WRITE_SRQC_BT0 },\n\t \n\t{ \"ECC_RESOURCE_GMV\",\n\t  0, 0 },\n\t{ \"ECC_RESOURCE_QPC_TIMER\",\n\t  HNS_ROCE_CMD_READ_QPC_TIMER_BT0, HNS_ROCE_CMD_WRITE_QPC_TIMER_BT0 },\n\t{ \"ECC_RESOURCE_CQC_TIMER\",\n\t  HNS_ROCE_CMD_READ_CQC_TIMER_BT0, HNS_ROCE_CMD_WRITE_CQC_TIMER_BT0 },\n\t{ \"ECC_RESOURCE_SCCC\",\n\t  HNS_ROCE_CMD_READ_SCCC_BT0, HNS_ROCE_CMD_WRITE_SCCC_BT0 },\n};\n\nstatic inline void set_data_seg_v2(struct hns_roce_v2_wqe_data_seg *dseg,\n\t\t\t\t   struct ib_sge *sg)\n{\n\tdseg->lkey = cpu_to_le32(sg->lkey);\n\tdseg->addr = cpu_to_le64(sg->addr);\n\tdseg->len  = cpu_to_le32(sg->length);\n}\n\n \n#define HR_OPC_MAP(ib_key, hr_key) \\\n\t\t[IB_WR_ ## ib_key] = 1 + HNS_ROCE_V2_WQE_OP_ ## hr_key\n\nstatic const u32 hns_roce_op_code[] = {\n\tHR_OPC_MAP(RDMA_WRITE,\t\t\tRDMA_WRITE),\n\tHR_OPC_MAP(RDMA_WRITE_WITH_IMM,\t\tRDMA_WRITE_WITH_IMM),\n\tHR_OPC_MAP(SEND,\t\t\tSEND),\n\tHR_OPC_MAP(SEND_WITH_IMM,\t\tSEND_WITH_IMM),\n\tHR_OPC_MAP(RDMA_READ,\t\t\tRDMA_READ),\n\tHR_OPC_MAP(ATOMIC_CMP_AND_SWP,\t\tATOM_CMP_AND_SWAP),\n\tHR_OPC_MAP(ATOMIC_FETCH_AND_ADD,\tATOM_FETCH_AND_ADD),\n\tHR_OPC_MAP(SEND_WITH_INV,\t\tSEND_WITH_INV),\n\tHR_OPC_MAP(MASKED_ATOMIC_CMP_AND_SWP,\tATOM_MSK_CMP_AND_SWAP),\n\tHR_OPC_MAP(MASKED_ATOMIC_FETCH_AND_ADD,\tATOM_MSK_FETCH_AND_ADD),\n\tHR_OPC_MAP(REG_MR,\t\t\tFAST_REG_PMR),\n};\n\nstatic u32 to_hr_opcode(u32 ib_opcode)\n{\n\tif (ib_opcode >= ARRAY_SIZE(hns_roce_op_code))\n\t\treturn HNS_ROCE_V2_WQE_OP_MASK;\n\n\treturn hns_roce_op_code[ib_opcode] ? hns_roce_op_code[ib_opcode] - 1 :\n\t\t\t\t\t     HNS_ROCE_V2_WQE_OP_MASK;\n}\n\nstatic void set_frmr_seg(struct hns_roce_v2_rc_send_wqe *rc_sq_wqe,\n\t\t\t const struct ib_reg_wr *wr)\n{\n\tstruct hns_roce_wqe_frmr_seg *fseg =\n\t\t(void *)rc_sq_wqe + sizeof(struct hns_roce_v2_rc_send_wqe);\n\tstruct hns_roce_mr *mr = to_hr_mr(wr->mr);\n\tu64 pbl_ba;\n\n\t \n\thr_reg_write_bool(fseg, FRMR_BIND_EN, wr->access & IB_ACCESS_MW_BIND);\n\thr_reg_write_bool(fseg, FRMR_ATOMIC,\n\t\t\t  wr->access & IB_ACCESS_REMOTE_ATOMIC);\n\thr_reg_write_bool(fseg, FRMR_RR, wr->access & IB_ACCESS_REMOTE_READ);\n\thr_reg_write_bool(fseg, FRMR_RW, wr->access & IB_ACCESS_REMOTE_WRITE);\n\thr_reg_write_bool(fseg, FRMR_LW, wr->access & IB_ACCESS_LOCAL_WRITE);\n\n\t \n\tpbl_ba = mr->pbl_mtr.hem_cfg.root_ba;\n\trc_sq_wqe->msg_len = cpu_to_le32(lower_32_bits(pbl_ba));\n\trc_sq_wqe->inv_key = cpu_to_le32(upper_32_bits(pbl_ba));\n\n\trc_sq_wqe->byte_16 = cpu_to_le32(wr->mr->length & 0xffffffff);\n\trc_sq_wqe->byte_20 = cpu_to_le32(wr->mr->length >> 32);\n\trc_sq_wqe->rkey = cpu_to_le32(wr->key);\n\trc_sq_wqe->va = cpu_to_le64(wr->mr->iova);\n\n\thr_reg_write(fseg, FRMR_PBL_SIZE, mr->npages);\n\thr_reg_write(fseg, FRMR_PBL_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(mr->pbl_mtr.hem_cfg.buf_pg_shift));\n\thr_reg_clear(fseg, FRMR_BLK_MODE);\n}\n\nstatic void set_atomic_seg(const struct ib_send_wr *wr,\n\t\t\t   struct hns_roce_v2_rc_send_wqe *rc_sq_wqe,\n\t\t\t   unsigned int valid_num_sge)\n{\n\tstruct hns_roce_v2_wqe_data_seg *dseg =\n\t\t(void *)rc_sq_wqe + sizeof(struct hns_roce_v2_rc_send_wqe);\n\tstruct hns_roce_wqe_atomic_seg *aseg =\n\t\t(void *)dseg + sizeof(struct hns_roce_v2_wqe_data_seg);\n\n\tset_data_seg_v2(dseg, wr->sg_list);\n\n\tif (wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) {\n\t\taseg->fetchadd_swap_data = cpu_to_le64(atomic_wr(wr)->swap);\n\t\taseg->cmp_data = cpu_to_le64(atomic_wr(wr)->compare_add);\n\t} else {\n\t\taseg->fetchadd_swap_data =\n\t\t\tcpu_to_le64(atomic_wr(wr)->compare_add);\n\t\taseg->cmp_data = 0;\n\t}\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_SGE_NUM, valid_num_sge);\n}\n\nstatic int fill_ext_sge_inl_data(struct hns_roce_qp *qp,\n\t\t\t\t const struct ib_send_wr *wr,\n\t\t\t\t unsigned int *sge_idx, u32 msg_len)\n{\n\tstruct ib_device *ibdev = &(to_hr_dev(qp->ibqp.device))->ib_dev;\n\tunsigned int left_len_in_pg;\n\tunsigned int idx = *sge_idx;\n\tunsigned int i = 0;\n\tunsigned int len;\n\tvoid *addr;\n\tvoid *dseg;\n\n\tif (msg_len > qp->sq.ext_sge_cnt * HNS_ROCE_SGE_SIZE) {\n\t\tibdev_err(ibdev,\n\t\t\t  \"no enough extended sge space for inline data.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdseg = hns_roce_get_extend_sge(qp, idx & (qp->sge.sge_cnt - 1));\n\tleft_len_in_pg = hr_hw_page_align((uintptr_t)dseg) - (uintptr_t)dseg;\n\tlen = wr->sg_list[0].length;\n\taddr = (void *)(unsigned long)(wr->sg_list[0].addr);\n\n\t \n\twhile (1) {\n\t\tif (len <= left_len_in_pg) {\n\t\t\tmemcpy(dseg, addr, len);\n\n\t\t\tidx += len / HNS_ROCE_SGE_SIZE;\n\n\t\t\ti++;\n\t\t\tif (i >= wr->num_sge)\n\t\t\t\tbreak;\n\n\t\t\tleft_len_in_pg -= len;\n\t\t\tlen = wr->sg_list[i].length;\n\t\t\taddr = (void *)(unsigned long)(wr->sg_list[i].addr);\n\t\t\tdseg += len;\n\t\t} else {\n\t\t\tmemcpy(dseg, addr, left_len_in_pg);\n\n\t\t\tlen -= left_len_in_pg;\n\t\t\taddr += left_len_in_pg;\n\t\t\tidx += left_len_in_pg / HNS_ROCE_SGE_SIZE;\n\t\t\tdseg = hns_roce_get_extend_sge(qp,\n\t\t\t\t\t\tidx & (qp->sge.sge_cnt - 1));\n\t\t\tleft_len_in_pg = 1 << HNS_HW_PAGE_SHIFT;\n\t\t}\n\t}\n\n\t*sge_idx = idx;\n\n\treturn 0;\n}\n\nstatic void set_extend_sge(struct hns_roce_qp *qp, struct ib_sge *sge,\n\t\t\t   unsigned int *sge_ind, unsigned int cnt)\n{\n\tstruct hns_roce_v2_wqe_data_seg *dseg;\n\tunsigned int idx = *sge_ind;\n\n\twhile (cnt > 0) {\n\t\tdseg = hns_roce_get_extend_sge(qp, idx & (qp->sge.sge_cnt - 1));\n\t\tif (likely(sge->length)) {\n\t\t\tset_data_seg_v2(dseg, sge);\n\t\t\tidx++;\n\t\t\tcnt--;\n\t\t}\n\t\tsge++;\n\t}\n\n\t*sge_ind = idx;\n}\n\nstatic bool check_inl_data_len(struct hns_roce_qp *qp, unsigned int len)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(qp->ibqp.device);\n\tint mtu = ib_mtu_enum_to_int(qp->path_mtu);\n\n\tif (mtu < 0 || len > qp->max_inline_data || len > mtu) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"invalid length of data, data len = %u, max inline len = %u, path mtu = %d.\\n\",\n\t\t\t  len, qp->max_inline_data, mtu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int set_rc_inl(struct hns_roce_qp *qp, const struct ib_send_wr *wr,\n\t\t      struct hns_roce_v2_rc_send_wqe *rc_sq_wqe,\n\t\t      unsigned int *sge_idx)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(qp->ibqp.device);\n\tu32 msg_len = le32_to_cpu(rc_sq_wqe->msg_len);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tunsigned int curr_idx = *sge_idx;\n\tvoid *dseg = rc_sq_wqe;\n\tunsigned int i;\n\tint ret;\n\n\tif (unlikely(wr->opcode == IB_WR_RDMA_READ)) {\n\t\tibdev_err(ibdev, \"invalid inline parameters!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!check_inl_data_len(qp, msg_len))\n\t\treturn -EINVAL;\n\n\tdseg += sizeof(struct hns_roce_v2_rc_send_wqe);\n\n\tif (msg_len <= HNS_ROCE_V2_MAX_RC_INL_INN_SZ) {\n\t\thr_reg_clear(rc_sq_wqe, RC_SEND_WQE_INL_TYPE);\n\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tmemcpy(dseg, ((void *)wr->sg_list[i].addr),\n\t\t\t       wr->sg_list[i].length);\n\t\t\tdseg += wr->sg_list[i].length;\n\t\t}\n\t} else {\n\t\thr_reg_enable(rc_sq_wqe, RC_SEND_WQE_INL_TYPE);\n\n\t\tret = fill_ext_sge_inl_data(qp, wr, &curr_idx, msg_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_SGE_NUM, curr_idx - *sge_idx);\n\t}\n\n\t*sge_idx = curr_idx;\n\n\treturn 0;\n}\n\nstatic int set_rwqe_data_seg(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t\t     struct hns_roce_v2_rc_send_wqe *rc_sq_wqe,\n\t\t\t     unsigned int *sge_ind,\n\t\t\t     unsigned int valid_num_sge)\n{\n\tstruct hns_roce_v2_wqe_data_seg *dseg =\n\t\t(void *)rc_sq_wqe + sizeof(struct hns_roce_v2_rc_send_wqe);\n\tstruct hns_roce_qp *qp = to_hr_qp(ibqp);\n\tint j = 0;\n\tint i;\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_MSG_START_SGE_IDX,\n\t\t     (*sge_ind) & (qp->sge.sge_cnt - 1));\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_INLINE,\n\t\t     !!(wr->send_flags & IB_SEND_INLINE));\n\tif (wr->send_flags & IB_SEND_INLINE)\n\t\treturn set_rc_inl(qp, wr, rc_sq_wqe, sge_ind);\n\n\tif (valid_num_sge <= HNS_ROCE_SGE_IN_WQE) {\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tif (likely(wr->sg_list[i].length)) {\n\t\t\t\tset_data_seg_v2(dseg, wr->sg_list + i);\n\t\t\t\tdseg++;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < wr->num_sge && j < HNS_ROCE_SGE_IN_WQE; i++) {\n\t\t\tif (likely(wr->sg_list[i].length)) {\n\t\t\t\tset_data_seg_v2(dseg, wr->sg_list + i);\n\t\t\t\tdseg++;\n\t\t\t\tj++;\n\t\t\t}\n\t\t}\n\n\t\tset_extend_sge(qp, wr->sg_list + i, sge_ind,\n\t\t\t       valid_num_sge - HNS_ROCE_SGE_IN_WQE);\n\t}\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_SGE_NUM, valid_num_sge);\n\n\treturn 0;\n}\n\nstatic int check_send_valid(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\n\tif (unlikely(hr_qp->state == IB_QPS_RESET ||\n\t\t     hr_qp->state == IB_QPS_INIT ||\n\t\t     hr_qp->state == IB_QPS_RTR)) {\n\t\tibdev_err(ibdev, \"failed to post WQE, QP state %u!\\n\",\n\t\t\t  hr_qp->state);\n\t\treturn -EINVAL;\n\t} else if (unlikely(hr_dev->state >= HNS_ROCE_DEVICE_STATE_RST_DOWN)) {\n\t\tibdev_err(ibdev, \"failed to post WQE, dev state %d!\\n\",\n\t\t\t  hr_dev->state);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned int calc_wr_sge_num(const struct ib_send_wr *wr,\n\t\t\t\t    unsigned int *sge_len)\n{\n\tunsigned int valid_num = 0;\n\tunsigned int len = 0;\n\tint i;\n\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tif (likely(wr->sg_list[i].length)) {\n\t\t\tlen += wr->sg_list[i].length;\n\t\t\tvalid_num++;\n\t\t}\n\t}\n\n\t*sge_len = len;\n\treturn valid_num;\n}\n\nstatic __le32 get_immtdata(const struct ib_send_wr *wr)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\treturn cpu_to_le32(be32_to_cpu(wr->ex.imm_data));\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int set_ud_opcode(struct hns_roce_v2_ud_send_wqe *ud_sq_wqe,\n\t\t\t const struct ib_send_wr *wr)\n{\n\tu32 ib_op = wr->opcode;\n\n\tif (ib_op != IB_WR_SEND && ib_op != IB_WR_SEND_WITH_IMM)\n\t\treturn -EINVAL;\n\n\tud_sq_wqe->immtdata = get_immtdata(wr);\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_OPCODE, to_hr_opcode(ib_op));\n\n\treturn 0;\n}\n\nstatic int fill_ud_av(struct hns_roce_v2_ud_send_wqe *ud_sq_wqe,\n\t\t      struct hns_roce_ah *ah)\n{\n\tstruct ib_device *ib_dev = ah->ibah.device;\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ib_dev);\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_UDPSPN, ah->av.udp_sport);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_HOPLIMIT, ah->av.hop_limit);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_TCLASS, ah->av.tclass);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_FLOW_LABEL, ah->av.flowlabel);\n\n\tif (WARN_ON(ah->av.sl > MAX_SERVICE_LEVEL))\n\t\treturn -EINVAL;\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_SL, ah->av.sl);\n\n\tud_sq_wqe->sgid_index = ah->av.gid_index;\n\n\tmemcpy(ud_sq_wqe->dmac, ah->av.mac, ETH_ALEN);\n\tmemcpy(ud_sq_wqe->dgid, ah->av.dgid, GID_LEN_V2);\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\treturn 0;\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_VLAN_EN, ah->av.vlan_en);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_VLAN, ah->av.vlan_id);\n\n\treturn 0;\n}\n\nstatic inline int set_ud_wqe(struct hns_roce_qp *qp,\n\t\t\t     const struct ib_send_wr *wr,\n\t\t\t     void *wqe, unsigned int *sge_idx,\n\t\t\t     unsigned int owner_bit)\n{\n\tstruct hns_roce_ah *ah = to_hr_ah(ud_wr(wr)->ah);\n\tstruct hns_roce_v2_ud_send_wqe *ud_sq_wqe = wqe;\n\tunsigned int curr_idx = *sge_idx;\n\tunsigned int valid_num_sge;\n\tu32 msg_len = 0;\n\tint ret;\n\n\tvalid_num_sge = calc_wr_sge_num(wr, &msg_len);\n\n\tret = set_ud_opcode(ud_sq_wqe, wr);\n\tif (WARN_ON(ret))\n\t\treturn ret;\n\n\tud_sq_wqe->msg_len = cpu_to_le32(msg_len);\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_CQE,\n\t\t     !!(wr->send_flags & IB_SEND_SIGNALED));\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_SE,\n\t\t     !!(wr->send_flags & IB_SEND_SOLICITED));\n\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_PD, to_hr_pd(qp->ibqp.pd)->pdn);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_SGE_NUM, valid_num_sge);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_MSG_START_SGE_IDX,\n\t\t     curr_idx & (qp->sge.sge_cnt - 1));\n\n\tud_sq_wqe->qkey = cpu_to_le32(ud_wr(wr)->remote_qkey & 0x80000000 ?\n\t\t\t  qp->qkey : ud_wr(wr)->remote_qkey);\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_DQPN, ud_wr(wr)->remote_qpn);\n\n\tret = fill_ud_av(ud_sq_wqe, ah);\n\tif (ret)\n\t\treturn ret;\n\n\tqp->sl = to_hr_ah(ud_wr(wr)->ah)->av.sl;\n\n\tset_extend_sge(qp, wr->sg_list, &curr_idx, valid_num_sge);\n\n\t \n\tif (qp->en_flags & HNS_ROCE_QP_CAP_OWNER_DB)\n\t\tdma_wmb();\n\n\t*sge_idx = curr_idx;\n\thr_reg_write(ud_sq_wqe, UD_SEND_WQE_OWNER, owner_bit);\n\n\treturn 0;\n}\n\nstatic int set_rc_opcode(struct hns_roce_dev *hr_dev,\n\t\t\t struct hns_roce_v2_rc_send_wqe *rc_sq_wqe,\n\t\t\t const struct ib_send_wr *wr)\n{\n\tu32 ib_op = wr->opcode;\n\tint ret = 0;\n\n\trc_sq_wqe->immtdata = get_immtdata(wr);\n\n\tswitch (ib_op) {\n\tcase IB_WR_RDMA_READ:\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\trc_sq_wqe->rkey = cpu_to_le32(rdma_wr(wr)->rkey);\n\t\trc_sq_wqe->va = cpu_to_le64(rdma_wr(wr)->remote_addr);\n\t\tbreak;\n\tcase IB_WR_SEND:\n\tcase IB_WR_SEND_WITH_IMM:\n\t\tbreak;\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\trc_sq_wqe->rkey = cpu_to_le32(atomic_wr(wr)->rkey);\n\t\trc_sq_wqe->va = cpu_to_le64(atomic_wr(wr)->remote_addr);\n\t\tbreak;\n\tcase IB_WR_REG_MR:\n\t\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\t\tset_frmr_seg(rc_sq_wqe, reg_wr(wr));\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_INV:\n\t\trc_sq_wqe->inv_key = cpu_to_le32(wr->ex.invalidate_rkey);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_OPCODE, to_hr_opcode(ib_op));\n\n\treturn ret;\n}\n\nstatic inline int set_rc_wqe(struct hns_roce_qp *qp,\n\t\t\t     const struct ib_send_wr *wr,\n\t\t\t     void *wqe, unsigned int *sge_idx,\n\t\t\t     unsigned int owner_bit)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(qp->ibqp.device);\n\tstruct hns_roce_v2_rc_send_wqe *rc_sq_wqe = wqe;\n\tunsigned int curr_idx = *sge_idx;\n\tunsigned int valid_num_sge;\n\tu32 msg_len = 0;\n\tint ret;\n\n\tvalid_num_sge = calc_wr_sge_num(wr, &msg_len);\n\n\trc_sq_wqe->msg_len = cpu_to_le32(msg_len);\n\n\tret = set_rc_opcode(hr_dev, rc_sq_wqe, wr);\n\tif (WARN_ON(ret))\n\t\treturn ret;\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_FENCE,\n\t\t     (wr->send_flags & IB_SEND_FENCE) ? 1 : 0);\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_SE,\n\t\t     (wr->send_flags & IB_SEND_SOLICITED) ? 1 : 0);\n\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_CQE,\n\t\t     (wr->send_flags & IB_SEND_SIGNALED) ? 1 : 0);\n\n\tif (wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP ||\n\t    wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD)\n\t\tset_atomic_seg(wr, rc_sq_wqe, valid_num_sge);\n\telse if (wr->opcode != IB_WR_REG_MR)\n\t\tret = set_rwqe_data_seg(&qp->ibqp, wr, rc_sq_wqe,\n\t\t\t\t\t&curr_idx, valid_num_sge);\n\n\t \n\tif (qp->en_flags & HNS_ROCE_QP_CAP_OWNER_DB)\n\t\tdma_wmb();\n\n\t*sge_idx = curr_idx;\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_OWNER, owner_bit);\n\n\treturn ret;\n}\n\nstatic inline void update_sq_db(struct hns_roce_dev *hr_dev,\n\t\t\t\tstruct hns_roce_qp *qp)\n{\n\tif (unlikely(qp->state == IB_QPS_ERR)) {\n\t\tflush_cqe(hr_dev, qp);\n\t} else {\n\t\tstruct hns_roce_v2_db sq_db = {};\n\n\t\thr_reg_write(&sq_db, DB_TAG, qp->qpn);\n\t\thr_reg_write(&sq_db, DB_CMD, HNS_ROCE_V2_SQ_DB);\n\t\thr_reg_write(&sq_db, DB_PI, qp->sq.head);\n\t\thr_reg_write(&sq_db, DB_SL, qp->sl);\n\n\t\thns_roce_write64(hr_dev, (__le32 *)&sq_db, qp->sq.db_reg);\n\t}\n}\n\nstatic inline void update_rq_db(struct hns_roce_dev *hr_dev,\n\t\t\t\tstruct hns_roce_qp *qp)\n{\n\tif (unlikely(qp->state == IB_QPS_ERR)) {\n\t\tflush_cqe(hr_dev, qp);\n\t} else {\n\t\tif (likely(qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)) {\n\t\t\t*qp->rdb.db_record =\n\t\t\t\t\tqp->rq.head & V2_DB_PRODUCER_IDX_M;\n\t\t} else {\n\t\t\tstruct hns_roce_v2_db rq_db = {};\n\n\t\t\thr_reg_write(&rq_db, DB_TAG, qp->qpn);\n\t\t\thr_reg_write(&rq_db, DB_CMD, HNS_ROCE_V2_RQ_DB);\n\t\t\thr_reg_write(&rq_db, DB_PI, qp->rq.head);\n\n\t\t\thns_roce_write64(hr_dev, (__le32 *)&rq_db,\n\t\t\t\t\t qp->rq.db_reg);\n\t\t}\n\t}\n}\n\nstatic void hns_roce_write512(struct hns_roce_dev *hr_dev, u64 *val,\n\t\t\t      u64 __iomem *dest)\n{\n#define HNS_ROCE_WRITE_TIMES 8\n\tstruct hns_roce_v2_priv *priv = (struct hns_roce_v2_priv *)hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tint i;\n\n\tif (!hr_dev->dis_db && !ops->get_hw_reset_stat(handle))\n\t\tfor (i = 0; i < HNS_ROCE_WRITE_TIMES; i++)\n\t\t\twriteq_relaxed(*(val + i), dest + i);\n}\n\nstatic void write_dwqe(struct hns_roce_dev *hr_dev, struct hns_roce_qp *qp,\n\t\t       void *wqe)\n{\n#define HNS_ROCE_SL_SHIFT 2\n\tstruct hns_roce_v2_rc_send_wqe *rc_sq_wqe = wqe;\n\n\t \n\thr_reg_enable(rc_sq_wqe, RC_SEND_WQE_FLAG);\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_DB_SL_L, qp->sl);\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_DB_SL_H,\n\t\t     qp->sl >> HNS_ROCE_SL_SHIFT);\n\thr_reg_write(rc_sq_wqe, RC_SEND_WQE_WQE_INDEX, qp->sq.head);\n\n\thns_roce_write512(hr_dev, wqe, qp->sq.db_reg);\n}\n\nstatic int hns_roce_v2_post_send(struct ib_qp *ibqp,\n\t\t\t\t const struct ib_send_wr *wr,\n\t\t\t\t const struct ib_send_wr **bad_wr)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_qp *qp = to_hr_qp(ibqp);\n\tunsigned long flags = 0;\n\tunsigned int owner_bit;\n\tunsigned int sge_idx;\n\tunsigned int wqe_idx;\n\tvoid *wqe = NULL;\n\tu32 nreq;\n\tint ret;\n\n\tspin_lock_irqsave(&qp->sq.lock, flags);\n\n\tret = check_send_valid(hr_dev, qp);\n\tif (unlikely(ret)) {\n\t\t*bad_wr = wr;\n\t\tnreq = 0;\n\t\tgoto out;\n\t}\n\n\tsge_idx = qp->next_sge;\n\n\tfor (nreq = 0; wr; ++nreq, wr = wr->next) {\n\t\tif (hns_roce_wq_overflow(&qp->sq, nreq, qp->ibqp.send_cq)) {\n\t\t\tret = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\twqe_idx = (qp->sq.head + nreq) & (qp->sq.wqe_cnt - 1);\n\n\t\tif (unlikely(wr->num_sge > qp->sq.max_gs)) {\n\t\t\tibdev_err(ibdev, \"num_sge = %d > qp->sq.max_gs = %u.\\n\",\n\t\t\t\t  wr->num_sge, qp->sq.max_gs);\n\t\t\tret = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\twqe = hns_roce_get_send_wqe(qp, wqe_idx);\n\t\tqp->sq.wrid[wqe_idx] = wr->wr_id;\n\t\towner_bit =\n\t\t       ~(((qp->sq.head + nreq) >> ilog2(qp->sq.wqe_cnt)) & 0x1);\n\n\t\t \n\t\tif (ibqp->qp_type == IB_QPT_RC)\n\t\t\tret = set_rc_wqe(qp, wr, wqe, &sge_idx, owner_bit);\n\t\telse\n\t\t\tret = set_ud_wqe(qp, wr, wqe, &sge_idx, owner_bit);\n\n\t\tif (unlikely(ret)) {\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tif (likely(nreq)) {\n\t\tqp->sq.head += nreq;\n\t\tqp->next_sge = sge_idx;\n\n\t\tif (nreq == 1 && !ret &&\n\t\t    (qp->en_flags & HNS_ROCE_QP_CAP_DIRECT_WQE))\n\t\t\twrite_dwqe(hr_dev, qp, wqe);\n\t\telse\n\t\t\tupdate_sq_db(hr_dev, qp);\n\t}\n\n\tspin_unlock_irqrestore(&qp->sq.lock, flags);\n\n\treturn ret;\n}\n\nstatic int check_recv_valid(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp)\n{\n\tif (unlikely(hr_dev->state >= HNS_ROCE_DEVICE_STATE_RST_DOWN))\n\t\treturn -EIO;\n\n\tif (hr_qp->state == IB_QPS_RESET)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void fill_recv_sge_to_wqe(const struct ib_recv_wr *wr, void *wqe,\n\t\t\t\t u32 max_sge, bool rsv)\n{\n\tstruct hns_roce_v2_wqe_data_seg *dseg = wqe;\n\tu32 i, cnt;\n\n\tfor (i = 0, cnt = 0; i < wr->num_sge; i++) {\n\t\t \n\t\tif (!wr->sg_list[i].length)\n\t\t\tcontinue;\n\t\tset_data_seg_v2(dseg + cnt, wr->sg_list + i);\n\t\tcnt++;\n\t}\n\n\t \n\tif (rsv) {\n\t\tdseg[cnt].lkey = cpu_to_le32(HNS_ROCE_INVALID_LKEY);\n\t\tdseg[cnt].addr = 0;\n\t\tdseg[cnt].len = cpu_to_le32(HNS_ROCE_INVALID_SGE_LENGTH);\n\t} else {\n\t\t \n\t\tif (cnt < max_sge)\n\t\t\tmemset(dseg + cnt, 0,\n\t\t\t       (max_sge - cnt) * HNS_ROCE_SGE_SIZE);\n\t}\n}\n\nstatic void fill_rq_wqe(struct hns_roce_qp *hr_qp, const struct ib_recv_wr *wr,\n\t\t\tu32 wqe_idx, u32 max_sge)\n{\n\tvoid *wqe = NULL;\n\n\twqe = hns_roce_get_recv_wqe(hr_qp, wqe_idx);\n\tfill_recv_sge_to_wqe(wr, wqe, max_sge, hr_qp->rq.rsv_sge);\n}\n\nstatic int hns_roce_v2_post_recv(struct ib_qp *ibqp,\n\t\t\t\t const struct ib_recv_wr *wr,\n\t\t\t\t const struct ib_recv_wr **bad_wr)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tu32 wqe_idx, nreq, max_sge;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&hr_qp->rq.lock, flags);\n\n\tret = check_recv_valid(hr_dev, hr_qp);\n\tif (unlikely(ret)) {\n\t\t*bad_wr = wr;\n\t\tnreq = 0;\n\t\tgoto out;\n\t}\n\n\tmax_sge = hr_qp->rq.max_gs - hr_qp->rq.rsv_sge;\n\tfor (nreq = 0; wr; ++nreq, wr = wr->next) {\n\t\tif (unlikely(hns_roce_wq_overflow(&hr_qp->rq, nreq,\n\t\t\t\t\t\t  hr_qp->ibqp.recv_cq))) {\n\t\t\tret = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (unlikely(wr->num_sge > max_sge)) {\n\t\t\tibdev_err(ibdev, \"num_sge = %d >= max_sge = %u.\\n\",\n\t\t\t\t  wr->num_sge, max_sge);\n\t\t\tret = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\twqe_idx = (hr_qp->rq.head + nreq) & (hr_qp->rq.wqe_cnt - 1);\n\t\tfill_rq_wqe(hr_qp, wr, wqe_idx, max_sge);\n\t\thr_qp->rq.wrid[wqe_idx] = wr->wr_id;\n\t}\n\nout:\n\tif (likely(nreq)) {\n\t\thr_qp->rq.head += nreq;\n\n\t\tupdate_rq_db(hr_dev, hr_qp);\n\t}\n\tspin_unlock_irqrestore(&hr_qp->rq.lock, flags);\n\n\treturn ret;\n}\n\nstatic void *get_srq_wqe_buf(struct hns_roce_srq *srq, u32 n)\n{\n\treturn hns_roce_buf_offset(srq->buf_mtr.kmem, n << srq->wqe_shift);\n}\n\nstatic void *get_idx_buf(struct hns_roce_idx_que *idx_que, u32 n)\n{\n\treturn hns_roce_buf_offset(idx_que->mtr.kmem,\n\t\t\t\t   n << idx_que->entry_shift);\n}\n\nstatic void hns_roce_free_srq_wqe(struct hns_roce_srq *srq, u32 wqe_index)\n{\n\t \n\tspin_lock(&srq->lock);\n\n\tbitmap_clear(srq->idx_que.bitmap, wqe_index, 1);\n\tsrq->idx_que.tail++;\n\n\tspin_unlock(&srq->lock);\n}\n\nstatic int hns_roce_srqwq_overflow(struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\n\treturn idx_que->head - idx_que->tail >= srq->wqe_cnt;\n}\n\nstatic int check_post_srq_valid(struct hns_roce_srq *srq, u32 max_sge,\n\t\t\t\tconst struct ib_recv_wr *wr)\n{\n\tstruct ib_device *ib_dev = srq->ibsrq.device;\n\n\tif (unlikely(wr->num_sge > max_sge)) {\n\t\tibdev_err(ib_dev,\n\t\t\t  \"failed to check sge, wr->num_sge = %d, max_sge = %u.\\n\",\n\t\t\t  wr->num_sge, max_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(hns_roce_srqwq_overflow(srq))) {\n\t\tibdev_err(ib_dev,\n\t\t\t  \"failed to check srqwq status, srqwq is full.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int get_srq_wqe_idx(struct hns_roce_srq *srq, u32 *wqe_idx)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\tu32 pos;\n\n\tpos = find_first_zero_bit(idx_que->bitmap, srq->wqe_cnt);\n\tif (unlikely(pos == srq->wqe_cnt))\n\t\treturn -ENOSPC;\n\n\tbitmap_set(idx_que->bitmap, pos, 1);\n\t*wqe_idx = pos;\n\treturn 0;\n}\n\nstatic void fill_wqe_idx(struct hns_roce_srq *srq, unsigned int wqe_idx)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\tunsigned int head;\n\t__le32 *buf;\n\n\thead = idx_que->head & (srq->wqe_cnt - 1);\n\n\tbuf = get_idx_buf(idx_que, head);\n\t*buf = cpu_to_le32(wqe_idx);\n\n\tidx_que->head++;\n}\n\nstatic void update_srq_db(struct hns_roce_v2_db *db, struct hns_roce_srq *srq)\n{\n\thr_reg_write(db, DB_TAG, srq->srqn);\n\thr_reg_write(db, DB_CMD, HNS_ROCE_V2_SRQ_DB);\n\thr_reg_write(db, DB_PI, srq->idx_que.head);\n}\n\nstatic int hns_roce_v2_post_srq_recv(struct ib_srq *ibsrq,\n\t\t\t\t     const struct ib_recv_wr *wr,\n\t\t\t\t     const struct ib_recv_wr **bad_wr)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);\n\tstruct hns_roce_srq *srq = to_hr_srq(ibsrq);\n\tstruct hns_roce_v2_db srq_db;\n\tunsigned long flags;\n\tint ret = 0;\n\tu32 max_sge;\n\tu32 wqe_idx;\n\tvoid *wqe;\n\tu32 nreq;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\tmax_sge = srq->max_gs - srq->rsv_sge;\n\tfor (nreq = 0; wr; ++nreq, wr = wr->next) {\n\t\tret = check_post_srq_valid(srq, max_sge, wr);\n\t\tif (ret) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = get_srq_wqe_idx(srq, &wqe_idx);\n\t\tif (unlikely(ret)) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\twqe = get_srq_wqe_buf(srq, wqe_idx);\n\t\tfill_recv_sge_to_wqe(wr, wqe, max_sge, srq->rsv_sge);\n\t\tfill_wqe_idx(srq, wqe_idx);\n\t\tsrq->wrid[wqe_idx] = wr->wr_id;\n\t}\n\n\tif (likely(nreq)) {\n\t\tupdate_srq_db(&srq_db, srq);\n\n\t\thns_roce_write64(hr_dev, (__le32 *)&srq_db, srq->db_reg);\n\t}\n\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn ret;\n}\n\nstatic u32 hns_roce_v2_cmd_hw_reseted(struct hns_roce_dev *hr_dev,\n\t\t\t\t      unsigned long instance_stage,\n\t\t\t\t      unsigned long reset_stage)\n{\n\t \n\thr_dev->is_reset = true;\n\thr_dev->dis_db = true;\n\n\tif (reset_stage == HNS_ROCE_STATE_RST_INIT ||\n\t    instance_stage == HNS_ROCE_STATE_INIT)\n\t\treturn CMD_RST_PRC_EBUSY;\n\n\treturn CMD_RST_PRC_SUCCESS;\n}\n\nstatic u32 hns_roce_v2_cmd_hw_resetting(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tunsigned long instance_stage,\n\t\t\t\t\tunsigned long reset_stage)\n{\n#define HW_RESET_TIMEOUT_US 1000000\n#define HW_RESET_SLEEP_US 1000\n\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tunsigned long val;\n\tint ret;\n\n\t \n\thr_dev->dis_db = true;\n\n\tret = read_poll_timeout(ops->ae_dev_reset_cnt, val,\n\t\t\t\tval > hr_dev->reset_cnt, HW_RESET_SLEEP_US,\n\t\t\t\tHW_RESET_TIMEOUT_US, false, handle);\n\tif (!ret)\n\t\thr_dev->is_reset = true;\n\n\tif (!hr_dev->is_reset || reset_stage == HNS_ROCE_STATE_RST_INIT ||\n\t    instance_stage == HNS_ROCE_STATE_INIT)\n\t\treturn CMD_RST_PRC_EBUSY;\n\n\treturn CMD_RST_PRC_SUCCESS;\n}\n\nstatic u32 hns_roce_v2_cmd_sw_resetting(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\n\t \n\thr_dev->dis_db = true;\n\tif (ops->ae_dev_reset_cnt(handle) != hr_dev->reset_cnt)\n\t\thr_dev->is_reset = true;\n\n\treturn CMD_RST_PRC_EBUSY;\n}\n\nstatic u32 check_aedev_reset_status(struct hns_roce_dev *hr_dev,\n\t\t\t\t    struct hnae3_handle *handle)\n{\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tunsigned long instance_stage;  \n\tunsigned long reset_stage;  \n\tunsigned long reset_cnt;\n\tbool sw_resetting;\n\tbool hw_resetting;\n\n\t \n\tinstance_stage = handle->rinfo.instance_state;\n\treset_stage = handle->rinfo.reset_state;\n\treset_cnt = ops->ae_dev_reset_cnt(handle);\n\tif (reset_cnt != hr_dev->reset_cnt)\n\t\treturn hns_roce_v2_cmd_hw_reseted(hr_dev, instance_stage,\n\t\t\t\t\t\t  reset_stage);\n\n\thw_resetting = ops->get_cmdq_stat(handle);\n\tif (hw_resetting)\n\t\treturn hns_roce_v2_cmd_hw_resetting(hr_dev, instance_stage,\n\t\t\t\t\t\t    reset_stage);\n\n\tsw_resetting = ops->ae_dev_resetting(handle);\n\tif (sw_resetting && instance_stage == HNS_ROCE_STATE_INIT)\n\t\treturn hns_roce_v2_cmd_sw_resetting(hr_dev);\n\n\treturn CMD_RST_PRC_OTHERS;\n}\n\nstatic bool check_device_is_in_reset(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\n\tif (hr_dev->reset_cnt != ops->ae_dev_reset_cnt(handle))\n\t\treturn true;\n\n\tif (ops->get_hw_reset_stat(handle))\n\t\treturn true;\n\n\tif (ops->ae_dev_resetting(handle))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool v2_chk_mbox_is_avail(struct hns_roce_dev *hr_dev, bool *busy)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tu32 status;\n\n\tif (hr_dev->is_reset)\n\t\tstatus = CMD_RST_PRC_SUCCESS;\n\telse\n\t\tstatus = check_aedev_reset_status(hr_dev, priv->handle);\n\n\t*busy = (status == CMD_RST_PRC_EBUSY);\n\n\treturn status == CMD_RST_PRC_OTHERS;\n}\n\nstatic int hns_roce_alloc_cmq_desc(struct hns_roce_dev *hr_dev,\n\t\t\t\t   struct hns_roce_v2_cmq_ring *ring)\n{\n\tint size = ring->desc_num * sizeof(struct hns_roce_cmq_desc);\n\n\tring->desc = dma_alloc_coherent(hr_dev->dev, size,\n\t\t\t\t\t&ring->desc_dma_addr, GFP_KERNEL);\n\tif (!ring->desc)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void hns_roce_free_cmq_desc(struct hns_roce_dev *hr_dev,\n\t\t\t\t   struct hns_roce_v2_cmq_ring *ring)\n{\n\tdma_free_coherent(hr_dev->dev,\n\t\t\t  ring->desc_num * sizeof(struct hns_roce_cmq_desc),\n\t\t\t  ring->desc, ring->desc_dma_addr);\n\n\tring->desc_dma_addr = 0;\n}\n\nstatic int init_csq(struct hns_roce_dev *hr_dev,\n\t\t    struct hns_roce_v2_cmq_ring *csq)\n{\n\tdma_addr_t dma;\n\tint ret;\n\n\tcsq->desc_num = CMD_CSQ_DESC_NUM;\n\tspin_lock_init(&csq->lock);\n\tcsq->flag = TYPE_CSQ;\n\tcsq->head = 0;\n\n\tret = hns_roce_alloc_cmq_desc(hr_dev, csq);\n\tif (ret)\n\t\treturn ret;\n\n\tdma = csq->desc_dma_addr;\n\troce_write(hr_dev, ROCEE_TX_CMQ_BASEADDR_L_REG, lower_32_bits(dma));\n\troce_write(hr_dev, ROCEE_TX_CMQ_BASEADDR_H_REG, upper_32_bits(dma));\n\troce_write(hr_dev, ROCEE_TX_CMQ_DEPTH_REG,\n\t\t   (u32)csq->desc_num >> HNS_ROCE_CMQ_DESC_NUM_S);\n\n\t \n\troce_write(hr_dev, ROCEE_TX_CMQ_CI_REG, 0);\n\troce_write(hr_dev, ROCEE_TX_CMQ_PI_REG, 0);\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_cmq_init(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tint ret;\n\n\tpriv->cmq.tx_timeout = HNS_ROCE_CMQ_TX_TIMEOUT;\n\n\tret = init_csq(hr_dev, &priv->cmq.csq);\n\tif (ret)\n\t\tdev_err(hr_dev->dev, \"failed to init CSQ, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void hns_roce_v2_cmq_exit(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\n\thns_roce_free_cmq_desc(hr_dev, &priv->cmq.csq);\n}\n\nstatic void hns_roce_cmq_setup_basic_desc(struct hns_roce_cmq_desc *desc,\n\t\t\t\t\t  enum hns_roce_opcode_type opcode,\n\t\t\t\t\t  bool is_read)\n{\n\tmemset((void *)desc, 0, sizeof(struct hns_roce_cmq_desc));\n\tdesc->opcode = cpu_to_le16(opcode);\n\tdesc->flag = cpu_to_le16(HNS_ROCE_CMD_FLAG_IN);\n\tif (is_read)\n\t\tdesc->flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_WR);\n\telse\n\t\tdesc->flag &= cpu_to_le16(~HNS_ROCE_CMD_FLAG_WR);\n}\n\nstatic int hns_roce_cmq_csq_done(struct hns_roce_dev *hr_dev)\n{\n\tu32 tail = roce_read(hr_dev, ROCEE_TX_CMQ_CI_REG);\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\n\treturn tail == priv->cmq.csq.head;\n}\n\nstatic void update_cmdq_status(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\n\tif (handle->rinfo.reset_state == HNS_ROCE_STATE_RST_INIT ||\n\t    handle->rinfo.instance_state == HNS_ROCE_STATE_INIT)\n\t\thr_dev->cmd.state = HNS_ROCE_CMDQ_STATE_FATAL_ERR;\n}\n\nstatic int hns_roce_cmd_err_convert_errno(u16 desc_ret)\n{\n\tstruct hns_roce_cmd_errcode errcode_table[] = {\n\t\t{CMD_EXEC_SUCCESS, 0},\n\t\t{CMD_NO_AUTH, -EPERM},\n\t\t{CMD_NOT_EXIST, -EOPNOTSUPP},\n\t\t{CMD_CRQ_FULL, -EXFULL},\n\t\t{CMD_NEXT_ERR, -ENOSR},\n\t\t{CMD_NOT_EXEC, -ENOTBLK},\n\t\t{CMD_PARA_ERR, -EINVAL},\n\t\t{CMD_RESULT_ERR, -ERANGE},\n\t\t{CMD_TIMEOUT, -ETIME},\n\t\t{CMD_HILINK_ERR, -ENOLINK},\n\t\t{CMD_INFO_ILLEGAL, -ENXIO},\n\t\t{CMD_INVALID, -EBADR},\n\t};\n\tu16 i;\n\n\tfor (i = 0; i < ARRAY_SIZE(errcode_table); i++)\n\t\tif (desc_ret == errcode_table[i].return_status)\n\t\t\treturn errcode_table[i].errno;\n\treturn -EIO;\n}\n\nstatic int __hns_roce_cmq_send(struct hns_roce_dev *hr_dev,\n\t\t\t       struct hns_roce_cmq_desc *desc, int num)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_cmq_ring *csq = &priv->cmq.csq;\n\tu32 timeout = 0;\n\tu16 desc_ret;\n\tu32 tail;\n\tint ret;\n\tint i;\n\n\tspin_lock_bh(&csq->lock);\n\n\ttail = csq->head;\n\n\tfor (i = 0; i < num; i++) {\n\t\tcsq->desc[csq->head++] = desc[i];\n\t\tif (csq->head == csq->desc_num)\n\t\t\tcsq->head = 0;\n\t}\n\n\t \n\troce_write(hr_dev, ROCEE_TX_CMQ_PI_REG, csq->head);\n\n\tdo {\n\t\tif (hns_roce_cmq_csq_done(hr_dev))\n\t\t\tbreak;\n\t\tudelay(1);\n\t} while (++timeout < priv->cmq.tx_timeout);\n\n\tif (hns_roce_cmq_csq_done(hr_dev)) {\n\t\tret = 0;\n\t\tfor (i = 0; i < num; i++) {\n\t\t\t \n\t\t\tdesc[i] = csq->desc[tail++];\n\t\t\tif (tail == csq->desc_num)\n\t\t\t\ttail = 0;\n\n\t\t\tdesc_ret = le16_to_cpu(desc[i].retval);\n\t\t\tif (likely(desc_ret == CMD_EXEC_SUCCESS))\n\t\t\t\tcontinue;\n\n\t\t\tdev_err_ratelimited(hr_dev->dev,\n\t\t\t\t\t    \"Cmdq IO error, opcode = 0x%x, return = 0x%x.\\n\",\n\t\t\t\t\t    desc->opcode, desc_ret);\n\t\t\tret = hns_roce_cmd_err_convert_errno(desc_ret);\n\t\t}\n\t} else {\n\t\t \n\t\ttail = roce_read(hr_dev, ROCEE_TX_CMQ_CI_REG);\n\t\tdev_warn(hr_dev->dev, \"CMDQ move tail from %u to %u.\\n\",\n\t\t\t csq->head, tail);\n\t\tcsq->head = tail;\n\n\t\tupdate_cmdq_status(hr_dev);\n\n\t\tret = -EAGAIN;\n\t}\n\n\tspin_unlock_bh(&csq->lock);\n\n\treturn ret;\n}\n\nstatic int hns_roce_cmq_send(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_cmq_desc *desc, int num)\n{\n\tbool busy;\n\tint ret;\n\n\tif (hr_dev->cmd.state == HNS_ROCE_CMDQ_STATE_FATAL_ERR)\n\t\treturn -EIO;\n\n\tif (!v2_chk_mbox_is_avail(hr_dev, &busy))\n\t\treturn busy ? -EBUSY : 0;\n\n\tret = __hns_roce_cmq_send(hr_dev, desc, num);\n\tif (ret) {\n\t\tif (!v2_chk_mbox_is_avail(hr_dev, &busy))\n\t\t\treturn busy ? -EBUSY : 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int config_hem_ba_to_hw(struct hns_roce_dev *hr_dev,\n\t\t\t       dma_addr_t base_addr, u8 cmd, unsigned long tag)\n{\n\tstruct hns_roce_cmd_mailbox *mbox;\n\tint ret;\n\n\tmbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mbox))\n\t\treturn PTR_ERR(mbox);\n\n\tret = hns_roce_cmd_mbox(hr_dev, base_addr, mbox->dma, cmd, tag);\n\thns_roce_free_cmd_mailbox(hr_dev, mbox);\n\treturn ret;\n}\n\nstatic int hns_roce_cmq_query_hw_info(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_query_version *resp;\n\tstruct hns_roce_cmq_desc desc;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_QUERY_HW_VER, true);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tresp = (struct hns_roce_query_version *)desc.data;\n\thr_dev->hw_rev = le16_to_cpu(resp->rocee_hw_version);\n\thr_dev->vendor_id = hr_dev->pci_dev->vendor;\n\n\treturn 0;\n}\n\nstatic void func_clr_hw_resetting_state(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tstruct hnae3_handle *handle)\n{\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tunsigned long end;\n\n\thr_dev->dis_db = true;\n\n\tdev_warn(hr_dev->dev,\n\t\t \"func clear is pending, device in resetting state.\\n\");\n\tend = HNS_ROCE_V2_HW_RST_TIMEOUT;\n\twhile (end) {\n\t\tif (!ops->get_hw_reset_stat(handle)) {\n\t\t\thr_dev->is_reset = true;\n\t\t\tdev_info(hr_dev->dev,\n\t\t\t\t \"func clear success after reset.\\n\");\n\t\t\treturn;\n\t\t}\n\t\tmsleep(HNS_ROCE_V2_HW_RST_COMPLETION_WAIT);\n\t\tend -= HNS_ROCE_V2_HW_RST_COMPLETION_WAIT;\n\t}\n\n\tdev_warn(hr_dev->dev, \"func clear failed.\\n\");\n}\n\nstatic void func_clr_sw_resetting_state(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tstruct hnae3_handle *handle)\n{\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tunsigned long end;\n\n\thr_dev->dis_db = true;\n\n\tdev_warn(hr_dev->dev,\n\t\t \"func clear is pending, device in resetting state.\\n\");\n\tend = HNS_ROCE_V2_HW_RST_TIMEOUT;\n\twhile (end) {\n\t\tif (ops->ae_dev_reset_cnt(handle) !=\n\t\t    hr_dev->reset_cnt) {\n\t\t\thr_dev->is_reset = true;\n\t\t\tdev_info(hr_dev->dev,\n\t\t\t\t \"func clear success after sw reset\\n\");\n\t\t\treturn;\n\t\t}\n\t\tmsleep(HNS_ROCE_V2_HW_RST_COMPLETION_WAIT);\n\t\tend -= HNS_ROCE_V2_HW_RST_COMPLETION_WAIT;\n\t}\n\n\tdev_warn(hr_dev->dev, \"func clear failed because of unfinished sw reset\\n\");\n}\n\nstatic void hns_roce_func_clr_rst_proc(struct hns_roce_dev *hr_dev, int retval,\n\t\t\t\t       int flag)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hnae3_handle *handle = priv->handle;\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\n\tif (ops->ae_dev_reset_cnt(handle) != hr_dev->reset_cnt) {\n\t\thr_dev->dis_db = true;\n\t\thr_dev->is_reset = true;\n\t\tdev_info(hr_dev->dev, \"func clear success after reset.\\n\");\n\t\treturn;\n\t}\n\n\tif (ops->get_hw_reset_stat(handle)) {\n\t\tfunc_clr_hw_resetting_state(hr_dev, handle);\n\t\treturn;\n\t}\n\n\tif (ops->ae_dev_resetting(handle) &&\n\t    handle->rinfo.instance_state == HNS_ROCE_STATE_INIT) {\n\t\tfunc_clr_sw_resetting_state(hr_dev, handle);\n\t\treturn;\n\t}\n\n\tif (retval && !flag)\n\t\tdev_warn(hr_dev->dev,\n\t\t\t \"func clear read failed, ret = %d.\\n\", retval);\n\n\tdev_warn(hr_dev->dev, \"func clear failed.\\n\");\n}\n\nstatic void __hns_roce_function_clear(struct hns_roce_dev *hr_dev, int vf_id)\n{\n\tbool fclr_write_fail_flag = false;\n\tstruct hns_roce_func_clear *resp;\n\tstruct hns_roce_cmq_desc desc;\n\tunsigned long end;\n\tint ret = 0;\n\n\tif (check_device_is_in_reset(hr_dev))\n\t\tgoto out;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_FUNC_CLEAR, false);\n\tresp = (struct hns_roce_func_clear *)desc.data;\n\tresp->rst_funcid_en = cpu_to_le32(vf_id);\n\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret) {\n\t\tfclr_write_fail_flag = true;\n\t\tdev_err(hr_dev->dev, \"func clear write failed, ret = %d.\\n\",\n\t\t\t ret);\n\t\tgoto out;\n\t}\n\n\tmsleep(HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_INTERVAL);\n\tend = HNS_ROCE_V2_FUNC_CLEAR_TIMEOUT_MSECS;\n\twhile (end) {\n\t\tif (check_device_is_in_reset(hr_dev))\n\t\t\tgoto out;\n\t\tmsleep(HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_FAIL_WAIT);\n\t\tend -= HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_FAIL_WAIT;\n\n\t\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_FUNC_CLEAR,\n\t\t\t\t\t      true);\n\n\t\tresp->rst_funcid_en = cpu_to_le32(vf_id);\n\t\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\t\tif (ret)\n\t\t\tcontinue;\n\n\t\tif (hr_reg_read(resp, FUNC_CLEAR_RST_FUN_DONE)) {\n\t\t\tif (vf_id == 0)\n\t\t\t\thr_dev->is_reset = true;\n\t\t\treturn;\n\t\t}\n\t}\n\nout:\n\thns_roce_func_clr_rst_proc(hr_dev, ret, fclr_write_fail_flag);\n}\n\nstatic int hns_roce_free_vf_resource(struct hns_roce_dev *hr_dev, int vf_id)\n{\n\tenum hns_roce_opcode_type opcode = HNS_ROCE_OPC_ALLOC_VF_RES;\n\tstruct hns_roce_cmq_desc desc[2];\n\tstruct hns_roce_cmq_req *req_a;\n\n\treq_a = (struct hns_roce_cmq_req *)desc[0].data;\n\thns_roce_cmq_setup_basic_desc(&desc[0], opcode, false);\n\tdesc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\thns_roce_cmq_setup_basic_desc(&desc[1], opcode, false);\n\thr_reg_write(req_a, FUNC_RES_A_VF_ID, vf_id);\n\n\treturn hns_roce_cmq_send(hr_dev, desc, 2);\n}\n\nstatic void hns_roce_function_clear(struct hns_roce_dev *hr_dev)\n{\n\tint ret;\n\tint i;\n\n\tif (hr_dev->cmd.state == HNS_ROCE_CMDQ_STATE_FATAL_ERR)\n\t\treturn;\n\n\tfor (i = hr_dev->func_num - 1; i >= 0; i--) {\n\t\t__hns_roce_function_clear(hr_dev, i);\n\n\t\tif (i == 0)\n\t\t\tcontinue;\n\n\t\tret = hns_roce_free_vf_resource(hr_dev, i);\n\t\tif (ret)\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"failed to free vf resource, vf_id = %d, ret = %d.\\n\",\n\t\t\t\t  i, ret);\n\t}\n}\n\nstatic int hns_roce_clear_extdb_list_info(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CLEAR_EXTDB_LIST_INFO,\n\t\t\t\t      false);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to clear extended doorbell info, ret = %d.\\n\",\n\t\t\t  ret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_query_fw_ver(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_query_fw_info *resp;\n\tstruct hns_roce_cmq_desc desc;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_QUERY_FW_VER, true);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tresp = (struct hns_roce_query_fw_info *)desc.data;\n\thr_dev->caps.fw_ver = (u64)(le32_to_cpu(resp->fw_ver));\n\n\treturn 0;\n}\n\nstatic int hns_roce_query_func_info(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tint ret;\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\thr_dev->func_num = 1;\n\t\treturn 0;\n\t}\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_QUERY_FUNC_INFO,\n\t\t\t\t      true);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret) {\n\t\thr_dev->func_num = 1;\n\t\treturn ret;\n\t}\n\n\thr_dev->func_num = le32_to_cpu(desc.func_info.own_func_num);\n\thr_dev->cong_algo_tmpl_id = le32_to_cpu(desc.func_info.own_mac_id);\n\n\treturn 0;\n}\n\nstatic int hns_roce_hw_v2_query_counter(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tu64 *stats, u32 port, int *num_counters)\n{\n#define CNT_PER_DESC 3\n\tstruct hns_roce_cmq_desc *desc;\n\tint bd_idx, cnt_idx;\n\t__le64 *cnt_data;\n\tint desc_num;\n\tint ret;\n\tint i;\n\n\tif (port > hr_dev->caps.num_ports)\n\t\treturn -EINVAL;\n\n\tdesc_num = DIV_ROUND_UP(HNS_ROCE_HW_CNT_TOTAL, CNT_PER_DESC);\n\tdesc = kcalloc(desc_num, sizeof(*desc), GFP_KERNEL);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < desc_num; i++) {\n\t\thns_roce_cmq_setup_basic_desc(&desc[i],\n\t\t\t\t\t      HNS_ROCE_OPC_QUERY_COUNTER, true);\n\t\tif (i != desc_num - 1)\n\t\t\tdesc[i].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\t}\n\n\tret = hns_roce_cmq_send(hr_dev, desc, desc_num);\n\tif (ret) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to get counter, ret = %d.\\n\", ret);\n\t\tgoto err_out;\n\t}\n\n\tfor (i = 0; i < HNS_ROCE_HW_CNT_TOTAL && i < *num_counters; i++) {\n\t\tbd_idx = i / CNT_PER_DESC;\n\t\tif (!(desc[bd_idx].flag & HNS_ROCE_CMD_FLAG_NEXT) &&\n\t\t    bd_idx != HNS_ROCE_HW_CNT_TOTAL / CNT_PER_DESC)\n\t\t\tbreak;\n\n\t\tcnt_data = (__le64 *)&desc[bd_idx].data[0];\n\t\tcnt_idx = i % CNT_PER_DESC;\n\t\tstats[i] = le64_to_cpu(cnt_data[cnt_idx]);\n\t}\n\t*num_counters = i;\n\nerr_out:\n\tkfree(desc);\n\treturn ret;\n}\n\nstatic int hns_roce_config_global_param(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tu32 clock_cycles_of_1us;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_GLOBAL_PARAM,\n\t\t\t\t      false);\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)\n\t\tclock_cycles_of_1us = HNS_ROCE_1NS_CFG;\n\telse\n\t\tclock_cycles_of_1us = HNS_ROCE_1US_CFG;\n\n\thr_reg_write(req, CFG_GLOBAL_PARAM_1US_CYCLES, clock_cycles_of_1us);\n\thr_reg_write(req, CFG_GLOBAL_PARAM_UDP_PORT, ROCE_V2_UDP_DPORT);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int load_func_res_caps(struct hns_roce_dev *hr_dev, bool is_vf)\n{\n\tstruct hns_roce_cmq_desc desc[2];\n\tstruct hns_roce_cmq_req *r_a = (struct hns_roce_cmq_req *)desc[0].data;\n\tstruct hns_roce_cmq_req *r_b = (struct hns_roce_cmq_req *)desc[1].data;\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\tenum hns_roce_opcode_type opcode;\n\tu32 func_num;\n\tint ret;\n\n\tif (is_vf) {\n\t\topcode = HNS_ROCE_OPC_QUERY_VF_RES;\n\t\tfunc_num = 1;\n\t} else {\n\t\topcode = HNS_ROCE_OPC_QUERY_PF_RES;\n\t\tfunc_num = hr_dev->func_num;\n\t}\n\n\thns_roce_cmq_setup_basic_desc(&desc[0], opcode, true);\n\tdesc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\thns_roce_cmq_setup_basic_desc(&desc[1], opcode, true);\n\n\tret = hns_roce_cmq_send(hr_dev, desc, 2);\n\tif (ret)\n\t\treturn ret;\n\n\tcaps->qpc_bt_num = hr_reg_read(r_a, FUNC_RES_A_QPC_BT_NUM) / func_num;\n\tcaps->srqc_bt_num = hr_reg_read(r_a, FUNC_RES_A_SRQC_BT_NUM) / func_num;\n\tcaps->cqc_bt_num = hr_reg_read(r_a, FUNC_RES_A_CQC_BT_NUM) / func_num;\n\tcaps->mpt_bt_num = hr_reg_read(r_a, FUNC_RES_A_MPT_BT_NUM) / func_num;\n\tcaps->eqc_bt_num = hr_reg_read(r_a, FUNC_RES_A_EQC_BT_NUM) / func_num;\n\tcaps->smac_bt_num = hr_reg_read(r_b, FUNC_RES_B_SMAC_NUM) / func_num;\n\tcaps->sgid_bt_num = hr_reg_read(r_b, FUNC_RES_B_SGID_NUM) / func_num;\n\tcaps->sccc_bt_num = hr_reg_read(r_b, FUNC_RES_B_SCCC_BT_NUM) / func_num;\n\n\tif (is_vf) {\n\t\tcaps->sl_num = hr_reg_read(r_b, FUNC_RES_V_QID_NUM) / func_num;\n\t\tcaps->gmv_bt_num = hr_reg_read(r_b, FUNC_RES_V_GMV_BT_NUM) /\n\t\t\t\t\t       func_num;\n\t} else {\n\t\tcaps->sl_num = hr_reg_read(r_b, FUNC_RES_B_QID_NUM) / func_num;\n\t\tcaps->gmv_bt_num = hr_reg_read(r_b, FUNC_RES_B_GMV_BT_NUM) /\n\t\t\t\t\t       func_num;\n\t}\n\n\treturn 0;\n}\n\nstatic int load_pf_timer_res_caps(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_QUERY_PF_TIMER_RES,\n\t\t\t\t      true);\n\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tcaps->qpc_timer_bt_num = hr_reg_read(req, PF_TIMER_RES_QPC_ITEM_NUM);\n\tcaps->cqc_timer_bt_num = hr_reg_read(req, PF_TIMER_RES_CQC_ITEM_NUM);\n\n\treturn 0;\n}\n\nstatic int hns_roce_query_pf_resource(struct hns_roce_dev *hr_dev)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tret = load_func_res_caps(hr_dev, false);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to load pf res caps, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = load_pf_timer_res_caps(hr_dev);\n\tif (ret)\n\t\tdev_err(dev, \"failed to load pf timer resource, ret = %d.\\n\",\n\t\t\tret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_query_vf_resource(struct hns_roce_dev *hr_dev)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tret = load_func_res_caps(hr_dev, true);\n\tif (ret)\n\t\tdev_err(dev, \"failed to load vf res caps, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int __hns_roce_set_vf_switch_param(struct hns_roce_dev *hr_dev,\n\t\t\t\t\t  u32 vf_id)\n{\n\tstruct hns_roce_vf_switch *swt;\n\tstruct hns_roce_cmq_desc desc;\n\tint ret;\n\n\tswt = (struct hns_roce_vf_switch *)desc.data;\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_SWITCH_PARAMETER_CFG, true);\n\tswt->rocee_sel |= cpu_to_le32(HNS_ICL_SWITCH_CMD_ROCEE_SEL);\n\thr_reg_write(swt, VF_SWITCH_VF_ID, vf_id);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tdesc.flag = cpu_to_le16(HNS_ROCE_CMD_FLAG_IN);\n\tdesc.flag &= cpu_to_le16(~HNS_ROCE_CMD_FLAG_WR);\n\thr_reg_enable(swt, VF_SWITCH_ALW_LPBK);\n\thr_reg_clear(swt, VF_SWITCH_ALW_LCL_LPBK);\n\thr_reg_enable(swt, VF_SWITCH_ALW_DST_OVRD);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int hns_roce_set_vf_switch_param(struct hns_roce_dev *hr_dev)\n{\n\tu32 vf_id;\n\tint ret;\n\n\tfor (vf_id = 0; vf_id < hr_dev->func_num; vf_id++) {\n\t\tret = __hns_roce_set_vf_switch_param(hr_dev, vf_id);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int config_vf_hem_resource(struct hns_roce_dev *hr_dev, int vf_id)\n{\n\tstruct hns_roce_cmq_desc desc[2];\n\tstruct hns_roce_cmq_req *r_a = (struct hns_roce_cmq_req *)desc[0].data;\n\tstruct hns_roce_cmq_req *r_b = (struct hns_roce_cmq_req *)desc[1].data;\n\tenum hns_roce_opcode_type opcode = HNS_ROCE_OPC_ALLOC_VF_RES;\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\n\thns_roce_cmq_setup_basic_desc(&desc[0], opcode, false);\n\tdesc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\thns_roce_cmq_setup_basic_desc(&desc[1], opcode, false);\n\n\thr_reg_write(r_a, FUNC_RES_A_VF_ID, vf_id);\n\n\thr_reg_write(r_a, FUNC_RES_A_QPC_BT_NUM, caps->qpc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_QPC_BT_IDX, vf_id * caps->qpc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_SRQC_BT_NUM, caps->srqc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_SRQC_BT_IDX, vf_id * caps->srqc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_CQC_BT_NUM, caps->cqc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_CQC_BT_IDX, vf_id * caps->cqc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_MPT_BT_NUM, caps->mpt_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_MPT_BT_IDX, vf_id * caps->mpt_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_EQC_BT_NUM, caps->eqc_bt_num);\n\thr_reg_write(r_a, FUNC_RES_A_EQC_BT_IDX, vf_id * caps->eqc_bt_num);\n\thr_reg_write(r_b, FUNC_RES_V_QID_NUM, caps->sl_num);\n\thr_reg_write(r_b, FUNC_RES_B_QID_IDX, vf_id * caps->sl_num);\n\thr_reg_write(r_b, FUNC_RES_B_SCCC_BT_NUM, caps->sccc_bt_num);\n\thr_reg_write(r_b, FUNC_RES_B_SCCC_BT_IDX, vf_id * caps->sccc_bt_num);\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09) {\n\t\thr_reg_write(r_b, FUNC_RES_V_GMV_BT_NUM, caps->gmv_bt_num);\n\t\thr_reg_write(r_b, FUNC_RES_B_GMV_BT_IDX,\n\t\t\t     vf_id * caps->gmv_bt_num);\n\t} else {\n\t\thr_reg_write(r_b, FUNC_RES_B_SGID_NUM, caps->sgid_bt_num);\n\t\thr_reg_write(r_b, FUNC_RES_B_SGID_IDX,\n\t\t\t     vf_id * caps->sgid_bt_num);\n\t\thr_reg_write(r_b, FUNC_RES_B_SMAC_NUM, caps->smac_bt_num);\n\t\thr_reg_write(r_b, FUNC_RES_B_SMAC_IDX,\n\t\t\t     vf_id * caps->smac_bt_num);\n\t}\n\n\treturn hns_roce_cmq_send(hr_dev, desc, 2);\n}\n\nstatic int hns_roce_alloc_vf_resource(struct hns_roce_dev *hr_dev)\n{\n\tu32 func_num = max_t(u32, 1, hr_dev->func_num);\n\tu32 vf_id;\n\tint ret;\n\n\tfor (vf_id = 0; vf_id < func_num; vf_id++) {\n\t\tret = config_vf_hem_resource(hr_dev, vf_id);\n\t\tif (ret) {\n\t\t\tdev_err(hr_dev->dev,\n\t\t\t\t\"failed to config vf-%u hem res, ret = %d.\\n\",\n\t\t\t\tvf_id, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_set_bt(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_BT_ATTR, false);\n\n\thr_reg_write(req, CFG_BT_ATTR_QPC_BA_PGSZ,\n\t\t     caps->qpc_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_QPC_BUF_PGSZ,\n\t\t     caps->qpc_buf_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_QPC_HOPNUM,\n\t\t     to_hr_hem_hopnum(caps->qpc_hop_num, caps->num_qps));\n\n\thr_reg_write(req, CFG_BT_ATTR_SRQC_BA_PGSZ,\n\t\t     caps->srqc_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_SRQC_BUF_PGSZ,\n\t\t     caps->srqc_buf_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_SRQC_HOPNUM,\n\t\t     to_hr_hem_hopnum(caps->srqc_hop_num, caps->num_srqs));\n\n\thr_reg_write(req, CFG_BT_ATTR_CQC_BA_PGSZ,\n\t\t     caps->cqc_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_CQC_BUF_PGSZ,\n\t\t     caps->cqc_buf_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_CQC_HOPNUM,\n\t\t     to_hr_hem_hopnum(caps->cqc_hop_num, caps->num_cqs));\n\n\thr_reg_write(req, CFG_BT_ATTR_MPT_BA_PGSZ,\n\t\t     caps->mpt_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_MPT_BUF_PGSZ,\n\t\t     caps->mpt_buf_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_MPT_HOPNUM,\n\t\t     to_hr_hem_hopnum(caps->mpt_hop_num, caps->num_mtpts));\n\n\thr_reg_write(req, CFG_BT_ATTR_SCCC_BA_PGSZ,\n\t\t     caps->sccc_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_SCCC_BUF_PGSZ,\n\t\t     caps->sccc_buf_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(req, CFG_BT_ATTR_SCCC_HOPNUM,\n\t\t     to_hr_hem_hopnum(caps->sccc_hop_num, caps->num_qps));\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic void calc_pg_sz(u32 obj_num, u32 obj_size, u32 hop_num, u32 ctx_bt_num,\n\t\t       u32 *buf_page_size, u32 *bt_page_size, u32 hem_type)\n{\n\tu64 obj_per_chunk;\n\tu64 bt_chunk_size = PAGE_SIZE;\n\tu64 buf_chunk_size = PAGE_SIZE;\n\tu64 obj_per_chunk_default = buf_chunk_size / obj_size;\n\n\t*buf_page_size = 0;\n\t*bt_page_size = 0;\n\n\tswitch (hop_num) {\n\tcase 3:\n\t\tobj_per_chunk = ctx_bt_num * (bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\t(bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\t(bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\t obj_per_chunk_default;\n\t\tbreak;\n\tcase 2:\n\t\tobj_per_chunk = ctx_bt_num * (bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\t(bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\t obj_per_chunk_default;\n\t\tbreak;\n\tcase 1:\n\t\tobj_per_chunk = ctx_bt_num * (bt_chunk_size / BA_BYTE_LEN) *\n\t\t\t\tobj_per_chunk_default;\n\t\tbreak;\n\tcase HNS_ROCE_HOP_NUM_0:\n\t\tobj_per_chunk = ctx_bt_num * obj_per_chunk_default;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"table %u not support hop_num = %u!\\n\", hem_type,\n\t\t       hop_num);\n\t\treturn;\n\t}\n\n\tif (hem_type >= HEM_TYPE_MTT)\n\t\t*bt_page_size = ilog2(DIV_ROUND_UP(obj_num, obj_per_chunk));\n\telse\n\t\t*buf_page_size = ilog2(DIV_ROUND_UP(obj_num, obj_per_chunk));\n}\n\nstatic void set_hem_page_size(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\n\t \n\tcaps->eqe_ba_pg_sz = 0;\n\tcaps->eqe_buf_pg_sz = 0;\n\n\t \n\tcaps->llm_buf_pg_sz = 0;\n\n\t \n\tcaps->mpt_ba_pg_sz = 0;\n\tcaps->mpt_buf_pg_sz = 0;\n\tcaps->pbl_ba_pg_sz = HNS_ROCE_BA_PG_SZ_SUPPORTED_16K;\n\tcaps->pbl_buf_pg_sz = 0;\n\tcalc_pg_sz(caps->num_mtpts, caps->mtpt_entry_sz, caps->mpt_hop_num,\n\t\t   caps->mpt_bt_num, &caps->mpt_buf_pg_sz, &caps->mpt_ba_pg_sz,\n\t\t   HEM_TYPE_MTPT);\n\n\t \n\tcaps->qpc_ba_pg_sz = 0;\n\tcaps->qpc_buf_pg_sz = 0;\n\tcaps->qpc_timer_ba_pg_sz = 0;\n\tcaps->qpc_timer_buf_pg_sz = 0;\n\tcaps->sccc_ba_pg_sz = 0;\n\tcaps->sccc_buf_pg_sz = 0;\n\tcaps->mtt_ba_pg_sz = 0;\n\tcaps->mtt_buf_pg_sz = 0;\n\tcalc_pg_sz(caps->num_qps, caps->qpc_sz, caps->qpc_hop_num,\n\t\t   caps->qpc_bt_num, &caps->qpc_buf_pg_sz, &caps->qpc_ba_pg_sz,\n\t\t   HEM_TYPE_QPC);\n\n\tif (caps->flags & HNS_ROCE_CAP_FLAG_QP_FLOW_CTRL)\n\t\tcalc_pg_sz(caps->num_qps, caps->sccc_sz, caps->sccc_hop_num,\n\t\t\t   caps->sccc_bt_num, &caps->sccc_buf_pg_sz,\n\t\t\t   &caps->sccc_ba_pg_sz, HEM_TYPE_SCCC);\n\n\t \n\tcaps->cqc_ba_pg_sz = 0;\n\tcaps->cqc_buf_pg_sz = 0;\n\tcaps->cqc_timer_ba_pg_sz = 0;\n\tcaps->cqc_timer_buf_pg_sz = 0;\n\tcaps->cqe_ba_pg_sz = HNS_ROCE_BA_PG_SZ_SUPPORTED_256K;\n\tcaps->cqe_buf_pg_sz = 0;\n\tcalc_pg_sz(caps->num_cqs, caps->cqc_entry_sz, caps->cqc_hop_num,\n\t\t   caps->cqc_bt_num, &caps->cqc_buf_pg_sz, &caps->cqc_ba_pg_sz,\n\t\t   HEM_TYPE_CQC);\n\tcalc_pg_sz(caps->max_cqes, caps->cqe_sz, caps->cqe_hop_num,\n\t\t   1, &caps->cqe_buf_pg_sz, &caps->cqe_ba_pg_sz, HEM_TYPE_CQE);\n\n\t \n\tif (caps->flags & HNS_ROCE_CAP_FLAG_SRQ) {\n\t\tcaps->srqc_ba_pg_sz = 0;\n\t\tcaps->srqc_buf_pg_sz = 0;\n\t\tcaps->srqwqe_ba_pg_sz = 0;\n\t\tcaps->srqwqe_buf_pg_sz = 0;\n\t\tcaps->idx_ba_pg_sz = 0;\n\t\tcaps->idx_buf_pg_sz = 0;\n\t\tcalc_pg_sz(caps->num_srqs, caps->srqc_entry_sz,\n\t\t\t   caps->srqc_hop_num, caps->srqc_bt_num,\n\t\t\t   &caps->srqc_buf_pg_sz, &caps->srqc_ba_pg_sz,\n\t\t\t   HEM_TYPE_SRQC);\n\t\tcalc_pg_sz(caps->num_srqwqe_segs, caps->mtt_entry_sz,\n\t\t\t   caps->srqwqe_hop_num, 1, &caps->srqwqe_buf_pg_sz,\n\t\t\t   &caps->srqwqe_ba_pg_sz, HEM_TYPE_SRQWQE);\n\t\tcalc_pg_sz(caps->num_idx_segs, caps->idx_entry_sz,\n\t\t\t   caps->idx_hop_num, 1, &caps->idx_buf_pg_sz,\n\t\t\t   &caps->idx_ba_pg_sz, HEM_TYPE_IDX);\n\t}\n\n\t \n\tcaps->gmv_ba_pg_sz = 0;\n\tcaps->gmv_buf_pg_sz = 0;\n}\n\n \nstatic void apply_func_caps(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\n\t \n\tcaps->qpc_timer_entry_sz = HNS_ROCE_V2_QPC_TIMER_ENTRY_SZ;\n\tcaps->cqc_timer_entry_sz = HNS_ROCE_V2_CQC_TIMER_ENTRY_SZ;\n\tcaps->mtt_entry_sz = HNS_ROCE_V2_MTT_ENTRY_SZ;\n\n\tcaps->pbl_hop_num = HNS_ROCE_PBL_HOP_NUM;\n\tcaps->qpc_timer_hop_num = HNS_ROCE_HOP_NUM_0;\n\tcaps->cqc_timer_hop_num = HNS_ROCE_HOP_NUM_0;\n\n\tcaps->num_srqwqe_segs = HNS_ROCE_V2_MAX_SRQWQE_SEGS;\n\tcaps->num_idx_segs = HNS_ROCE_V2_MAX_IDX_SEGS;\n\n\tif (!caps->num_comp_vectors)\n\t\tcaps->num_comp_vectors =\n\t\t\tmin_t(u32, caps->eqc_bt_num - HNS_ROCE_V2_AEQE_VEC_NUM,\n\t\t\t\t(u32)priv->handle->rinfo.num_vectors -\n\t\t(HNS_ROCE_V2_AEQE_VEC_NUM + HNS_ROCE_V2_ABNORMAL_VEC_NUM));\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09) {\n\t\tcaps->eqe_hop_num = HNS_ROCE_V3_EQE_HOP_NUM;\n\t\tcaps->ceqe_size = HNS_ROCE_V3_EQE_SIZE;\n\t\tcaps->aeqe_size = HNS_ROCE_V3_EQE_SIZE;\n\n\t\t \n\t\tcaps->qpc_sz = HNS_ROCE_V3_QPC_SZ;\n\t\tcaps->cqe_sz = HNS_ROCE_V3_CQE_SIZE;\n\t\tcaps->sccc_sz = HNS_ROCE_V3_SCCC_SZ;\n\n\t\t \n\t\tcaps->gmv_entry_sz = HNS_ROCE_V3_GMV_ENTRY_SZ;\n\n\t\tcaps->gmv_hop_num = HNS_ROCE_HOP_NUM_0;\n\t\tcaps->gid_table_len[0] = caps->gmv_bt_num *\n\t\t\t\t\t(HNS_HW_PAGE_SIZE / caps->gmv_entry_sz);\n\n\t\tcaps->gmv_entry_num = caps->gmv_bt_num * (PAGE_SIZE /\n\t\t\t\t\t\t\t  caps->gmv_entry_sz);\n\t} else {\n\t\tu32 func_num = max_t(u32, 1, hr_dev->func_num);\n\n\t\tcaps->eqe_hop_num = HNS_ROCE_V2_EQE_HOP_NUM;\n\t\tcaps->ceqe_size = HNS_ROCE_CEQE_SIZE;\n\t\tcaps->aeqe_size = HNS_ROCE_AEQE_SIZE;\n\t\tcaps->gid_table_len[0] /= func_num;\n\t}\n\n\tif (hr_dev->is_vf) {\n\t\tcaps->default_aeq_arm_st = 0x3;\n\t\tcaps->default_ceq_arm_st = 0x3;\n\t\tcaps->default_ceq_max_cnt = 0x1;\n\t\tcaps->default_ceq_period = 0x10;\n\t\tcaps->default_aeq_max_cnt = 0x1;\n\t\tcaps->default_aeq_period = 0x10;\n\t}\n\n\tset_hem_page_size(hr_dev);\n}\n\nstatic int hns_roce_query_caps(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_cmq_desc desc[HNS_ROCE_QUERY_PF_CAPS_CMD_NUM];\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\tstruct hns_roce_query_pf_caps_a *resp_a;\n\tstruct hns_roce_query_pf_caps_b *resp_b;\n\tstruct hns_roce_query_pf_caps_c *resp_c;\n\tstruct hns_roce_query_pf_caps_d *resp_d;\n\tstruct hns_roce_query_pf_caps_e *resp_e;\n\tenum hns_roce_opcode_type cmd;\n\tint ctx_hop_num;\n\tint pbl_hop_num;\n\tint ret;\n\tint i;\n\n\tcmd = hr_dev->is_vf ? HNS_ROCE_OPC_QUERY_VF_CAPS_NUM :\n\t      HNS_ROCE_OPC_QUERY_PF_CAPS_NUM;\n\n\tfor (i = 0; i < HNS_ROCE_QUERY_PF_CAPS_CMD_NUM; i++) {\n\t\thns_roce_cmq_setup_basic_desc(&desc[i], cmd, true);\n\t\tif (i < (HNS_ROCE_QUERY_PF_CAPS_CMD_NUM - 1))\n\t\t\tdesc[i].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\t\telse\n\t\t\tdesc[i].flag &= ~cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\t}\n\n\tret = hns_roce_cmq_send(hr_dev, desc, HNS_ROCE_QUERY_PF_CAPS_CMD_NUM);\n\tif (ret)\n\t\treturn ret;\n\n\tresp_a = (struct hns_roce_query_pf_caps_a *)desc[0].data;\n\tresp_b = (struct hns_roce_query_pf_caps_b *)desc[1].data;\n\tresp_c = (struct hns_roce_query_pf_caps_c *)desc[2].data;\n\tresp_d = (struct hns_roce_query_pf_caps_d *)desc[3].data;\n\tresp_e = (struct hns_roce_query_pf_caps_e *)desc[4].data;\n\n\tcaps->local_ca_ack_delay = resp_a->local_ca_ack_delay;\n\tcaps->max_sq_sg = le16_to_cpu(resp_a->max_sq_sg);\n\tcaps->max_sq_inline = le16_to_cpu(resp_a->max_sq_inline);\n\tcaps->max_rq_sg = le16_to_cpu(resp_a->max_rq_sg);\n\tcaps->max_rq_sg = roundup_pow_of_two(caps->max_rq_sg);\n\tcaps->max_srq_sges = le16_to_cpu(resp_a->max_srq_sges);\n\tcaps->max_srq_sges = roundup_pow_of_two(caps->max_srq_sges);\n\tcaps->num_aeq_vectors = resp_a->num_aeq_vectors;\n\tcaps->num_other_vectors = resp_a->num_other_vectors;\n\tcaps->max_sq_desc_sz = resp_a->max_sq_desc_sz;\n\tcaps->max_rq_desc_sz = resp_a->max_rq_desc_sz;\n\n\tcaps->mtpt_entry_sz = resp_b->mtpt_entry_sz;\n\tcaps->irrl_entry_sz = resp_b->irrl_entry_sz;\n\tcaps->trrl_entry_sz = resp_b->trrl_entry_sz;\n\tcaps->cqc_entry_sz = resp_b->cqc_entry_sz;\n\tcaps->srqc_entry_sz = resp_b->srqc_entry_sz;\n\tcaps->idx_entry_sz = resp_b->idx_entry_sz;\n\tcaps->sccc_sz = resp_b->sccc_sz;\n\tcaps->max_mtu = resp_b->max_mtu;\n\tcaps->min_cqes = resp_b->min_cqes;\n\tcaps->min_wqes = resp_b->min_wqes;\n\tcaps->page_size_cap = le32_to_cpu(resp_b->page_size_cap);\n\tcaps->pkey_table_len[0] = resp_b->pkey_table_len;\n\tcaps->phy_num_uars = resp_b->phy_num_uars;\n\tctx_hop_num = resp_b->ctx_hop_num;\n\tpbl_hop_num = resp_b->pbl_hop_num;\n\n\tcaps->num_pds = 1 << hr_reg_read(resp_c, PF_CAPS_C_NUM_PDS);\n\n\tcaps->flags = hr_reg_read(resp_c, PF_CAPS_C_CAP_FLAGS);\n\tcaps->flags |= le16_to_cpu(resp_d->cap_flags_ex) <<\n\t\t       HNS_ROCE_CAP_FLAGS_EX_SHIFT;\n\n\tcaps->num_cqs = 1 << hr_reg_read(resp_c, PF_CAPS_C_NUM_CQS);\n\tcaps->gid_table_len[0] = hr_reg_read(resp_c, PF_CAPS_C_MAX_GID);\n\tcaps->max_cqes = 1 << hr_reg_read(resp_c, PF_CAPS_C_CQ_DEPTH);\n\tcaps->num_xrcds = 1 << hr_reg_read(resp_c, PF_CAPS_C_NUM_XRCDS);\n\tcaps->num_mtpts = 1 << hr_reg_read(resp_c, PF_CAPS_C_NUM_MRWS);\n\tcaps->num_qps = 1 << hr_reg_read(resp_c, PF_CAPS_C_NUM_QPS);\n\tcaps->max_qp_init_rdma = hr_reg_read(resp_c, PF_CAPS_C_MAX_ORD);\n\tcaps->max_qp_dest_rdma = caps->max_qp_init_rdma;\n\tcaps->max_wqes = 1 << le16_to_cpu(resp_c->sq_depth);\n\n\tcaps->num_srqs = 1 << hr_reg_read(resp_d, PF_CAPS_D_NUM_SRQS);\n\tcaps->cong_type = hr_reg_read(resp_d, PF_CAPS_D_CONG_TYPE);\n\tcaps->max_srq_wrs = 1 << le16_to_cpu(resp_d->srq_depth);\n\tcaps->ceqe_depth = 1 << hr_reg_read(resp_d, PF_CAPS_D_CEQ_DEPTH);\n\tcaps->num_comp_vectors = hr_reg_read(resp_d, PF_CAPS_D_NUM_CEQS);\n\tcaps->aeqe_depth = 1 << hr_reg_read(resp_d, PF_CAPS_D_AEQ_DEPTH);\n\tcaps->reserved_pds = hr_reg_read(resp_d, PF_CAPS_D_RSV_PDS);\n\tcaps->num_uars = 1 << hr_reg_read(resp_d, PF_CAPS_D_NUM_UARS);\n\tcaps->reserved_qps = hr_reg_read(resp_d, PF_CAPS_D_RSV_QPS);\n\tcaps->reserved_uars = hr_reg_read(resp_d, PF_CAPS_D_RSV_UARS);\n\n\tcaps->reserved_mrws = hr_reg_read(resp_e, PF_CAPS_E_RSV_MRWS);\n\tcaps->chunk_sz = 1 << hr_reg_read(resp_e, PF_CAPS_E_CHUNK_SIZE_SHIFT);\n\tcaps->reserved_cqs = hr_reg_read(resp_e, PF_CAPS_E_RSV_CQS);\n\tcaps->reserved_xrcds = hr_reg_read(resp_e, PF_CAPS_E_RSV_XRCDS);\n\tcaps->reserved_srqs = hr_reg_read(resp_e, PF_CAPS_E_RSV_SRQS);\n\tcaps->reserved_lkey = hr_reg_read(resp_e, PF_CAPS_E_RSV_LKEYS);\n\n\tcaps->qpc_hop_num = ctx_hop_num;\n\tcaps->sccc_hop_num = ctx_hop_num;\n\tcaps->srqc_hop_num = ctx_hop_num;\n\tcaps->cqc_hop_num = ctx_hop_num;\n\tcaps->mpt_hop_num = ctx_hop_num;\n\tcaps->mtt_hop_num = pbl_hop_num;\n\tcaps->cqe_hop_num = pbl_hop_num;\n\tcaps->srqwqe_hop_num = pbl_hop_num;\n\tcaps->idx_hop_num = pbl_hop_num;\n\tcaps->wqe_sq_hop_num = hr_reg_read(resp_d, PF_CAPS_D_SQWQE_HOP_NUM);\n\tcaps->wqe_sge_hop_num = hr_reg_read(resp_d, PF_CAPS_D_EX_SGE_HOP_NUM);\n\tcaps->wqe_rq_hop_num = hr_reg_read(resp_d, PF_CAPS_D_RQWQE_HOP_NUM);\n\n\tif (!(caps->page_size_cap & PAGE_SIZE))\n\t\tcaps->page_size_cap = HNS_ROCE_V2_PAGE_SIZE_SUPPORTED;\n\n\tif (!hr_dev->is_vf) {\n\t\tcaps->cqe_sz = resp_a->cqe_sz;\n\t\tcaps->qpc_sz = le16_to_cpu(resp_b->qpc_sz);\n\t\tcaps->default_aeq_arm_st =\n\t\t\t\thr_reg_read(resp_d, PF_CAPS_D_AEQ_ARM_ST);\n\t\tcaps->default_ceq_arm_st =\n\t\t\t\thr_reg_read(resp_d, PF_CAPS_D_CEQ_ARM_ST);\n\t\tcaps->default_ceq_max_cnt = le16_to_cpu(resp_e->ceq_max_cnt);\n\t\tcaps->default_ceq_period = le16_to_cpu(resp_e->ceq_period);\n\t\tcaps->default_aeq_max_cnt = le16_to_cpu(resp_e->aeq_max_cnt);\n\t\tcaps->default_aeq_period = le16_to_cpu(resp_e->aeq_period);\n\t}\n\n\treturn 0;\n}\n\nstatic int config_hem_entry_size(struct hns_roce_dev *hr_dev, u32 type, u32 val)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_ENTRY_SIZE,\n\t\t\t\t      false);\n\n\thr_reg_write(req, CFG_HEM_ENTRY_SIZE_TYPE, type);\n\thr_reg_write(req, CFG_HEM_ENTRY_SIZE_VALUE, val);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int hns_roce_config_entry_size(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_caps *caps = &hr_dev->caps;\n\tint ret;\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)\n\t\treturn 0;\n\n\tret = config_hem_entry_size(hr_dev, HNS_ROCE_CFG_QPC_SIZE,\n\t\t\t\t    caps->qpc_sz);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev, \"failed to cfg qpc sz, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = config_hem_entry_size(hr_dev, HNS_ROCE_CFG_SCCC_SIZE,\n\t\t\t\t    caps->sccc_sz);\n\tif (ret)\n\t\tdev_err(hr_dev->dev, \"failed to cfg sccc sz, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_vf_profile(struct hns_roce_dev *hr_dev)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\thr_dev->func_num = 1;\n\n\tret = hns_roce_query_caps(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query VF caps, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_query_vf_resource(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query VF resource, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tapply_func_caps(hr_dev);\n\n\tret = hns_roce_v2_set_bt(hr_dev);\n\tif (ret)\n\t\tdev_err(dev, \"failed to config VF BA table, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_pf_profile(struct hns_roce_dev *hr_dev)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tret = hns_roce_query_func_info(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query func info, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_config_global_param(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to config global param, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_set_vf_switch_param(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to set switch param, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_query_caps(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query PF caps, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_query_pf_resource(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query pf resource, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tapply_func_caps(hr_dev);\n\n\tret = hns_roce_alloc_vf_resource(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to alloc vf resource, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_v2_set_bt(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to config BA table, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\treturn hns_roce_config_entry_size(hr_dev);\n}\n\nstatic int hns_roce_v2_profile(struct hns_roce_dev *hr_dev)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\n\tret = hns_roce_cmq_query_hw_info(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query hardware info, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_roce_query_fw_ver(hr_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to query firmware info, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\thr_dev->vendor_part_id = hr_dev->pci_dev->device;\n\thr_dev->sys_image_guid = be64_to_cpu(hr_dev->ib_dev.node_guid);\n\n\tif (hr_dev->is_vf)\n\t\treturn hns_roce_v2_vf_profile(hr_dev);\n\telse\n\t\treturn hns_roce_v2_pf_profile(hr_dev);\n}\n\nstatic void config_llm_table(struct hns_roce_buf *data_buf, void *cfg_buf)\n{\n\tu32 i, next_ptr, page_num;\n\t__le64 *entry = cfg_buf;\n\tdma_addr_t addr;\n\tu64 val;\n\n\tpage_num = data_buf->npages;\n\tfor (i = 0; i < page_num; i++) {\n\t\taddr = hns_roce_buf_page(data_buf, i);\n\t\tif (i == (page_num - 1))\n\t\t\tnext_ptr = 0;\n\t\telse\n\t\t\tnext_ptr = i + 1;\n\n\t\tval = HNS_ROCE_EXT_LLM_ENTRY(addr, (u64)next_ptr);\n\t\tentry[i] = cpu_to_le64(val);\n\t}\n}\n\nstatic int set_llm_cfg_to_hw(struct hns_roce_dev *hr_dev,\n\t\t\t     struct hns_roce_link_table *table)\n{\n\tstruct hns_roce_cmq_desc desc[2];\n\tstruct hns_roce_cmq_req *r_a = (struct hns_roce_cmq_req *)desc[0].data;\n\tstruct hns_roce_cmq_req *r_b = (struct hns_roce_cmq_req *)desc[1].data;\n\tstruct hns_roce_buf *buf = table->buf;\n\tenum hns_roce_opcode_type opcode;\n\tdma_addr_t addr;\n\n\topcode = HNS_ROCE_OPC_CFG_EXT_LLM;\n\thns_roce_cmq_setup_basic_desc(&desc[0], opcode, false);\n\tdesc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\thns_roce_cmq_setup_basic_desc(&desc[1], opcode, false);\n\n\thr_reg_write(r_a, CFG_LLM_A_BA_L, lower_32_bits(table->table.map));\n\thr_reg_write(r_a, CFG_LLM_A_BA_H, upper_32_bits(table->table.map));\n\thr_reg_write(r_a, CFG_LLM_A_DEPTH, buf->npages);\n\thr_reg_write(r_a, CFG_LLM_A_PGSZ, to_hr_hw_page_shift(buf->page_shift));\n\thr_reg_enable(r_a, CFG_LLM_A_INIT_EN);\n\n\taddr = to_hr_hw_page_addr(hns_roce_buf_page(buf, 0));\n\thr_reg_write(r_a, CFG_LLM_A_HEAD_BA_L, lower_32_bits(addr));\n\thr_reg_write(r_a, CFG_LLM_A_HEAD_BA_H, upper_32_bits(addr));\n\thr_reg_write(r_a, CFG_LLM_A_HEAD_NXTPTR, 1);\n\thr_reg_write(r_a, CFG_LLM_A_HEAD_PTR, 0);\n\n\taddr = to_hr_hw_page_addr(hns_roce_buf_page(buf, buf->npages - 1));\n\thr_reg_write(r_b, CFG_LLM_B_TAIL_BA_L, lower_32_bits(addr));\n\thr_reg_write(r_b, CFG_LLM_B_TAIL_BA_H, upper_32_bits(addr));\n\thr_reg_write(r_b, CFG_LLM_B_TAIL_PTR, buf->npages - 1);\n\n\treturn hns_roce_cmq_send(hr_dev, desc, 2);\n}\n\nstatic struct hns_roce_link_table *\nalloc_link_table_buf(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_link_table *link_tbl;\n\tu32 pg_shift, size, min_size;\n\n\tlink_tbl = &priv->ext_llm;\n\tpg_shift = hr_dev->caps.llm_buf_pg_sz + PAGE_SHIFT;\n\tsize = hr_dev->caps.num_qps * HNS_ROCE_V2_EXT_LLM_ENTRY_SZ;\n\tmin_size = HNS_ROCE_EXT_LLM_MIN_PAGES(hr_dev->caps.sl_num) << pg_shift;\n\n\t \n\tsize = max(size, min_size);\n\tlink_tbl->buf = hns_roce_buf_alloc(hr_dev, size, pg_shift, 0);\n\tif (IS_ERR(link_tbl->buf))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tsize = link_tbl->buf->npages * sizeof(u64);\n\tlink_tbl->table.buf = dma_alloc_coherent(hr_dev->dev, size,\n\t\t\t\t\t\t &link_tbl->table.map,\n\t\t\t\t\t\t GFP_KERNEL);\n\tif (!link_tbl->table.buf) {\n\t\thns_roce_buf_free(hr_dev, link_tbl->buf);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn link_tbl;\n}\n\nstatic void free_link_table_buf(struct hns_roce_dev *hr_dev,\n\t\t\t\tstruct hns_roce_link_table *tbl)\n{\n\tif (tbl->buf) {\n\t\tu32 size = tbl->buf->npages * sizeof(u64);\n\n\t\tdma_free_coherent(hr_dev->dev, size, tbl->table.buf,\n\t\t\t\t  tbl->table.map);\n\t}\n\n\thns_roce_buf_free(hr_dev, tbl->buf);\n}\n\nstatic int hns_roce_init_link_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_link_table *link_tbl;\n\tint ret;\n\n\tlink_tbl = alloc_link_table_buf(hr_dev);\n\tif (IS_ERR(link_tbl))\n\t\treturn -ENOMEM;\n\n\tif (WARN_ON(link_tbl->buf->npages > HNS_ROCE_V2_EXT_LLM_MAX_DEPTH)) {\n\t\tret = -EINVAL;\n\t\tgoto err_alloc;\n\t}\n\n\tconfig_llm_table(link_tbl->buf, link_tbl->table.buf);\n\tret = set_llm_cfg_to_hw(hr_dev, link_tbl);\n\tif (ret)\n\t\tgoto err_alloc;\n\n\treturn 0;\n\nerr_alloc:\n\tfree_link_table_buf(hr_dev, link_tbl);\n\treturn ret;\n}\n\nstatic void hns_roce_free_link_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\n\tfree_link_table_buf(hr_dev, &priv->ext_llm);\n}\n\nstatic void free_dip_list(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_dip *hr_dip;\n\tstruct hns_roce_dip *tmp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&hr_dev->dip_list_lock, flags);\n\n\tlist_for_each_entry_safe(hr_dip, tmp, &hr_dev->dip_list, node) {\n\t\tlist_del(&hr_dip->node);\n\t\tkfree(hr_dip);\n\t}\n\n\tspin_unlock_irqrestore(&hr_dev->dip_list_lock, flags);\n}\n\nstatic struct ib_pd *free_mr_init_pd(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_pd *hr_pd;\n\tstruct ib_pd *pd;\n\n\thr_pd = kzalloc(sizeof(*hr_pd), GFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(hr_pd))\n\t\treturn NULL;\n\tpd = &hr_pd->ibpd;\n\tpd->device = ibdev;\n\n\tif (hns_roce_alloc_pd(pd, NULL)) {\n\t\tibdev_err(ibdev, \"failed to create pd for free mr.\\n\");\n\t\tkfree(hr_pd);\n\t\treturn NULL;\n\t}\n\tfree_mr->rsv_pd = to_hr_pd(pd);\n\tfree_mr->rsv_pd->ibpd.device = &hr_dev->ib_dev;\n\tfree_mr->rsv_pd->ibpd.uobject = NULL;\n\tfree_mr->rsv_pd->ibpd.__internal_mr = NULL;\n\tatomic_set(&free_mr->rsv_pd->ibpd.usecnt, 0);\n\n\treturn pd;\n}\n\nstatic struct ib_cq *free_mr_init_cq(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct ib_cq_init_attr cq_init_attr = {};\n\tstruct hns_roce_cq *hr_cq;\n\tstruct ib_cq *cq;\n\n\tcq_init_attr.cqe = HNS_ROCE_FREE_MR_USED_CQE_NUM;\n\n\thr_cq = kzalloc(sizeof(*hr_cq), GFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(hr_cq))\n\t\treturn NULL;\n\n\tcq = &hr_cq->ib_cq;\n\tcq->device = ibdev;\n\n\tif (hns_roce_create_cq(cq, &cq_init_attr, NULL)) {\n\t\tibdev_err(ibdev, \"failed to create cq for free mr.\\n\");\n\t\tkfree(hr_cq);\n\t\treturn NULL;\n\t}\n\tfree_mr->rsv_cq = to_hr_cq(cq);\n\tfree_mr->rsv_cq->ib_cq.device = &hr_dev->ib_dev;\n\tfree_mr->rsv_cq->ib_cq.uobject = NULL;\n\tfree_mr->rsv_cq->ib_cq.comp_handler = NULL;\n\tfree_mr->rsv_cq->ib_cq.event_handler = NULL;\n\tfree_mr->rsv_cq->ib_cq.cq_context = NULL;\n\tatomic_set(&free_mr->rsv_cq->ib_cq.usecnt, 0);\n\n\treturn cq;\n}\n\nstatic int free_mr_init_qp(struct hns_roce_dev *hr_dev, struct ib_cq *cq,\n\t\t\t   struct ib_qp_init_attr *init_attr, int i)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_qp *hr_qp;\n\tstruct ib_qp *qp;\n\tint ret;\n\n\thr_qp = kzalloc(sizeof(*hr_qp), GFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(hr_qp))\n\t\treturn -ENOMEM;\n\n\tqp = &hr_qp->ibqp;\n\tqp->device = ibdev;\n\n\tret = hns_roce_create_qp(qp, init_attr, NULL);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to create qp for free mr.\\n\");\n\t\tkfree(hr_qp);\n\t\treturn ret;\n\t}\n\n\tfree_mr->rsv_qp[i] = hr_qp;\n\tfree_mr->rsv_qp[i]->ibqp.recv_cq = cq;\n\tfree_mr->rsv_qp[i]->ibqp.send_cq = cq;\n\n\treturn 0;\n}\n\nstatic void free_mr_exit(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_qp *qp;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) {\n\t\tif (free_mr->rsv_qp[i]) {\n\t\t\tqp = &free_mr->rsv_qp[i]->ibqp;\n\t\t\thns_roce_v2_destroy_qp(qp, NULL);\n\t\t\tkfree(free_mr->rsv_qp[i]);\n\t\t\tfree_mr->rsv_qp[i] = NULL;\n\t\t}\n\t}\n\n\tif (free_mr->rsv_cq) {\n\t\thns_roce_destroy_cq(&free_mr->rsv_cq->ib_cq, NULL);\n\t\tkfree(free_mr->rsv_cq);\n\t\tfree_mr->rsv_cq = NULL;\n\t}\n\n\tif (free_mr->rsv_pd) {\n\t\thns_roce_dealloc_pd(&free_mr->rsv_pd->ibpd, NULL);\n\t\tkfree(free_mr->rsv_pd);\n\t\tfree_mr->rsv_pd = NULL;\n\t}\n}\n\nstatic int free_mr_alloc_res(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_qp_init_attr qp_init_attr = {};\n\tstruct ib_pd *pd;\n\tstruct ib_cq *cq;\n\tint ret;\n\tint i;\n\n\tpd = free_mr_init_pd(hr_dev);\n\tif (!pd)\n\t\treturn -ENOMEM;\n\n\tcq = free_mr_init_cq(hr_dev);\n\tif (!cq) {\n\t\tret = -ENOMEM;\n\t\tgoto create_failed_cq;\n\t}\n\n\tqp_init_attr.qp_type = IB_QPT_RC;\n\tqp_init_attr.sq_sig_type = IB_SIGNAL_ALL_WR;\n\tqp_init_attr.send_cq = cq;\n\tqp_init_attr.recv_cq = cq;\n\tfor (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) {\n\t\tqp_init_attr.cap.max_send_wr = HNS_ROCE_FREE_MR_USED_SQWQE_NUM;\n\t\tqp_init_attr.cap.max_send_sge = HNS_ROCE_FREE_MR_USED_SQSGE_NUM;\n\t\tqp_init_attr.cap.max_recv_wr = HNS_ROCE_FREE_MR_USED_RQWQE_NUM;\n\t\tqp_init_attr.cap.max_recv_sge = HNS_ROCE_FREE_MR_USED_RQSGE_NUM;\n\n\t\tret = free_mr_init_qp(hr_dev, cq, &qp_init_attr, i);\n\t\tif (ret)\n\t\t\tgoto create_failed_qp;\n\t}\n\n\treturn 0;\n\ncreate_failed_qp:\n\tfor (i--; i >= 0; i--) {\n\t\thns_roce_v2_destroy_qp(&free_mr->rsv_qp[i]->ibqp, NULL);\n\t\tkfree(free_mr->rsv_qp[i]);\n\t}\n\thns_roce_destroy_cq(cq, NULL);\n\tkfree(cq);\n\ncreate_failed_cq:\n\thns_roce_dealloc_pd(pd, NULL);\n\tkfree(pd);\n\n\treturn ret;\n}\n\nstatic int free_mr_modify_rsv_qp(struct hns_roce_dev *hr_dev,\n\t\t\t\t struct ib_qp_attr *attr, int sl_num)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_qp *hr_qp;\n\tint loopback;\n\tint mask;\n\tint ret;\n\n\thr_qp = to_hr_qp(&free_mr->rsv_qp[sl_num]->ibqp);\n\thr_qp->free_mr_en = 1;\n\thr_qp->ibqp.device = ibdev;\n\thr_qp->ibqp.qp_type = IB_QPT_RC;\n\n\tmask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT | IB_QP_ACCESS_FLAGS;\n\tattr->qp_state = IB_QPS_INIT;\n\tattr->port_num = 1;\n\tattr->qp_access_flags = IB_ACCESS_REMOTE_WRITE;\n\tret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT,\n\t\t\t\t    IB_QPS_INIT, NULL);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to modify qp to init, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\tloopback = hr_dev->loop_idc;\n\t \n\thr_dev->loop_idc = 1;\n\n\tmask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN |\n\t       IB_QP_RQ_PSN | IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER;\n\tattr->qp_state = IB_QPS_RTR;\n\tattr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\tattr->path_mtu = IB_MTU_256;\n\tattr->dest_qp_num = hr_qp->qpn;\n\tattr->rq_psn = HNS_ROCE_FREE_MR_USED_PSN;\n\n\trdma_ah_set_sl(&attr->ah_attr, (u8)sl_num);\n\n\tret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT,\n\t\t\t\t    IB_QPS_RTR, NULL);\n\thr_dev->loop_idc = loopback;\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to modify qp to rtr, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\tmask = IB_QP_STATE | IB_QP_SQ_PSN | IB_QP_RETRY_CNT | IB_QP_TIMEOUT |\n\t       IB_QP_RNR_RETRY | IB_QP_MAX_QP_RD_ATOMIC;\n\tattr->qp_state = IB_QPS_RTS;\n\tattr->sq_psn = HNS_ROCE_FREE_MR_USED_PSN;\n\tattr->retry_cnt = HNS_ROCE_FREE_MR_USED_QP_RETRY_CNT;\n\tattr->timeout = HNS_ROCE_FREE_MR_USED_QP_TIMEOUT;\n\tret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_RTR,\n\t\t\t\t    IB_QPS_RTS, NULL);\n\tif (ret)\n\t\tibdev_err(ibdev, \"failed to modify qp to rts, ret = %d.\\n\",\n\t\t\t  ret);\n\n\treturn ret;\n}\n\nstatic int free_mr_modify_qp(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_qp_attr attr = {};\n\tint ret;\n\tint i;\n\n\trdma_ah_set_grh(&attr.ah_attr, NULL, 0, 0, 1, 0);\n\trdma_ah_set_static_rate(&attr.ah_attr, 3);\n\trdma_ah_set_port_num(&attr.ah_attr, 1);\n\n\tfor (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) {\n\t\tret = free_mr_modify_rsv_qp(hr_dev, &attr, i);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int free_mr_init(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tint ret;\n\n\tmutex_init(&free_mr->mutex);\n\n\tret = free_mr_alloc_res(hr_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = free_mr_modify_qp(hr_dev);\n\tif (ret)\n\t\tgoto err_modify_qp;\n\n\treturn 0;\n\nerr_modify_qp:\n\tfree_mr_exit(hr_dev);\n\n\treturn ret;\n}\n\nstatic int get_hem_table(struct hns_roce_dev *hr_dev)\n{\n\tunsigned int qpc_count;\n\tunsigned int cqc_count;\n\tunsigned int gmv_count;\n\tint ret;\n\tint i;\n\n\t \n\tfor (gmv_count = 0; gmv_count < hr_dev->caps.gmv_entry_num;\n\t     gmv_count++) {\n\t\tret = hns_roce_table_get(hr_dev, &hr_dev->gmv_table, gmv_count);\n\t\tif (ret)\n\t\t\tgoto err_gmv_failed;\n\t}\n\n\tif (hr_dev->is_vf)\n\t\treturn 0;\n\n\t \n\tfor (qpc_count = 0; qpc_count < hr_dev->caps.qpc_timer_bt_num;\n\t     qpc_count++) {\n\t\tret = hns_roce_table_get(hr_dev, &hr_dev->qpc_timer_table,\n\t\t\t\t\t qpc_count);\n\t\tif (ret) {\n\t\t\tdev_err(hr_dev->dev, \"QPC Timer get failed\\n\");\n\t\t\tgoto err_qpc_timer_failed;\n\t\t}\n\t}\n\n\t \n\tfor (cqc_count = 0; cqc_count < hr_dev->caps.cqc_timer_bt_num;\n\t     cqc_count++) {\n\t\tret = hns_roce_table_get(hr_dev, &hr_dev->cqc_timer_table,\n\t\t\t\t\t cqc_count);\n\t\tif (ret) {\n\t\t\tdev_err(hr_dev->dev, \"CQC Timer get failed\\n\");\n\t\t\tgoto err_cqc_timer_failed;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_cqc_timer_failed:\n\tfor (i = 0; i < cqc_count; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->cqc_timer_table, i);\n\nerr_qpc_timer_failed:\n\tfor (i = 0; i < qpc_count; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->qpc_timer_table, i);\n\nerr_gmv_failed:\n\tfor (i = 0; i < gmv_count; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->gmv_table, i);\n\n\treturn ret;\n}\n\nstatic void put_hem_table(struct hns_roce_dev *hr_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < hr_dev->caps.gmv_entry_num; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->gmv_table, i);\n\n\tif (hr_dev->is_vf)\n\t\treturn;\n\n\tfor (i = 0; i < hr_dev->caps.qpc_timer_bt_num; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->qpc_timer_table, i);\n\n\tfor (i = 0; i < hr_dev->caps.cqc_timer_bt_num; i++)\n\t\thns_roce_table_put(hr_dev, &hr_dev->cqc_timer_table, i);\n}\n\nstatic int hns_roce_v2_init(struct hns_roce_dev *hr_dev)\n{\n\tint ret;\n\n\t \n\tret = hns_roce_clear_extdb_list_info(hr_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = get_hem_table(hr_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (hr_dev->is_vf)\n\t\treturn 0;\n\n\tret = hns_roce_init_link_table(hr_dev);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev, \"failed to init llm, ret = %d.\\n\", ret);\n\t\tgoto err_llm_init_failed;\n\t}\n\n\treturn 0;\n\nerr_llm_init_failed:\n\tput_hem_table(hr_dev);\n\n\treturn ret;\n}\n\nstatic void hns_roce_v2_exit(struct hns_roce_dev *hr_dev)\n{\n\thns_roce_function_clear(hr_dev);\n\n\tif (!hr_dev->is_vf)\n\t\thns_roce_free_link_table(hr_dev);\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP09)\n\t\tfree_dip_list(hr_dev);\n}\n\nstatic int hns_roce_mbox_post(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_mbox_msg *mbox_msg)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_post_mbox *mb = (struct hns_roce_post_mbox *)desc.data;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_POST_MB, false);\n\n\tmb->in_param_l = cpu_to_le32(mbox_msg->in_param);\n\tmb->in_param_h = cpu_to_le32(mbox_msg->in_param >> 32);\n\tmb->out_param_l = cpu_to_le32(mbox_msg->out_param);\n\tmb->out_param_h = cpu_to_le32(mbox_msg->out_param >> 32);\n\tmb->cmd_tag = cpu_to_le32(mbox_msg->tag << 8 | mbox_msg->cmd);\n\tmb->token_event_en = cpu_to_le32(mbox_msg->event_en << 16 |\n\t\t\t\t\t mbox_msg->token);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int v2_wait_mbox_complete(struct hns_roce_dev *hr_dev, u32 timeout,\n\t\t\t\t u8 *complete_status)\n{\n\tstruct hns_roce_mbox_status *mb_st;\n\tstruct hns_roce_cmq_desc desc;\n\tunsigned long end;\n\tint ret = -EBUSY;\n\tu32 status;\n\tbool busy;\n\n\tmb_st = (struct hns_roce_mbox_status *)desc.data;\n\tend = msecs_to_jiffies(timeout) + jiffies;\n\twhile (v2_chk_mbox_is_avail(hr_dev, &busy)) {\n\t\tif (hr_dev->cmd.state == HNS_ROCE_CMDQ_STATE_FATAL_ERR)\n\t\t\treturn -EIO;\n\n\t\tstatus = 0;\n\t\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_QUERY_MB_ST,\n\t\t\t\t\t      true);\n\t\tret = __hns_roce_cmq_send(hr_dev, &desc, 1);\n\t\tif (!ret) {\n\t\t\tstatus = le32_to_cpu(mb_st->mb_status_hw_run);\n\t\t\t \n\t\t\tif (!(status & MB_ST_HW_RUN_M))\n\t\t\t\tbreak;\n\t\t} else if (!v2_chk_mbox_is_avail(hr_dev, &busy)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (time_after(jiffies, end)) {\n\t\t\tdev_err_ratelimited(hr_dev->dev,\n\t\t\t\t\t    \"failed to wait mbox status 0x%x\\n\",\n\t\t\t\t\t    status);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\n\t\tcond_resched();\n\t\tret = -EBUSY;\n\t}\n\n\tif (!ret) {\n\t\t*complete_status = (u8)(status & MB_ST_COMPLETE_M);\n\t} else if (!v2_chk_mbox_is_avail(hr_dev, &busy)) {\n\t\t \n\t\tret = 0;\n\t\t*complete_status = MB_ST_COMPLETE_M;\n\t}\n\n\treturn ret;\n}\n\nstatic int v2_post_mbox(struct hns_roce_dev *hr_dev,\n\t\t\tstruct hns_roce_mbox_msg *mbox_msg)\n{\n\tu8 status = 0;\n\tint ret;\n\n\t \n\tret = v2_wait_mbox_complete(hr_dev, HNS_ROCE_V2_GO_BIT_TIMEOUT_MSECS,\n\t\t\t\t    &status);\n\tif (unlikely(ret)) {\n\t\tdev_err_ratelimited(hr_dev->dev,\n\t\t\t\t    \"failed to check post mbox status = 0x%x, ret = %d.\\n\",\n\t\t\t\t    status, ret);\n\t\treturn ret;\n\t}\n\n\t \n\tret = hns_roce_mbox_post(hr_dev, mbox_msg);\n\tif (ret)\n\t\tdev_err_ratelimited(hr_dev->dev,\n\t\t\t\t    \"failed to post mailbox, ret = %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int v2_poll_mbox_done(struct hns_roce_dev *hr_dev)\n{\n\tu8 status = 0;\n\tint ret;\n\n\tret = v2_wait_mbox_complete(hr_dev, HNS_ROCE_CMD_TIMEOUT_MSECS,\n\t\t\t\t    &status);\n\tif (!ret) {\n\t\tif (status != MB_ST_COMPLETE_SUCC)\n\t\t\treturn -EBUSY;\n\t} else {\n\t\tdev_err_ratelimited(hr_dev->dev,\n\t\t\t\t    \"failed to check mbox status = 0x%x, ret = %d.\\n\",\n\t\t\t\t    status, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic void copy_gid(void *dest, const union ib_gid *gid)\n{\n#define GID_SIZE 4\n\tconst union ib_gid *src = gid;\n\t__le32 (*p)[GID_SIZE] = dest;\n\tint i;\n\n\tif (!gid)\n\t\tsrc = &zgid;\n\n\tfor (i = 0; i < GID_SIZE; i++)\n\t\t(*p)[i] = cpu_to_le32(*(u32 *)&src->raw[i * sizeof(u32)]);\n}\n\nstatic int config_sgid_table(struct hns_roce_dev *hr_dev,\n\t\t\t     int gid_index, const union ib_gid *gid,\n\t\t\t     enum hns_roce_sgid_type sgid_type)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cfg_sgid_tb *sgid_tb =\n\t\t\t\t    (struct hns_roce_cfg_sgid_tb *)desc.data;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_SGID_TB, false);\n\n\thr_reg_write(sgid_tb, CFG_SGID_TB_TABLE_IDX, gid_index);\n\thr_reg_write(sgid_tb, CFG_SGID_TB_VF_SGID_TYPE, sgid_type);\n\n\tcopy_gid(&sgid_tb->vf_sgid_l, gid);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int config_gmv_table(struct hns_roce_dev *hr_dev,\n\t\t\t    int gid_index, const union ib_gid *gid,\n\t\t\t    enum hns_roce_sgid_type sgid_type,\n\t\t\t    const struct ib_gid_attr *attr)\n{\n\tstruct hns_roce_cmq_desc desc[2];\n\tstruct hns_roce_cfg_gmv_tb_a *tb_a =\n\t\t\t\t(struct hns_roce_cfg_gmv_tb_a *)desc[0].data;\n\tstruct hns_roce_cfg_gmv_tb_b *tb_b =\n\t\t\t\t(struct hns_roce_cfg_gmv_tb_b *)desc[1].data;\n\n\tu16 vlan_id = VLAN_CFI_MASK;\n\tu8 mac[ETH_ALEN] = {};\n\tint ret;\n\n\tif (gid) {\n\t\tret = rdma_read_gid_l2_fields(attr, &vlan_id, mac);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\thns_roce_cmq_setup_basic_desc(&desc[0], HNS_ROCE_OPC_CFG_GMV_TBL, false);\n\tdesc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);\n\n\thns_roce_cmq_setup_basic_desc(&desc[1], HNS_ROCE_OPC_CFG_GMV_TBL, false);\n\n\tcopy_gid(&tb_a->vf_sgid_l, gid);\n\n\thr_reg_write(tb_a, GMV_TB_A_VF_SGID_TYPE, sgid_type);\n\thr_reg_write(tb_a, GMV_TB_A_VF_VLAN_EN, vlan_id < VLAN_CFI_MASK);\n\thr_reg_write(tb_a, GMV_TB_A_VF_VLAN_ID, vlan_id);\n\n\ttb_b->vf_smac_l = cpu_to_le32(*(u32 *)mac);\n\n\thr_reg_write(tb_b, GMV_TB_B_SMAC_H, *(u16 *)&mac[4]);\n\thr_reg_write(tb_b, GMV_TB_B_SGID_IDX, gid_index);\n\n\treturn hns_roce_cmq_send(hr_dev, desc, 2);\n}\n\nstatic int hns_roce_v2_set_gid(struct hns_roce_dev *hr_dev, int gid_index,\n\t\t\t       const union ib_gid *gid,\n\t\t\t       const struct ib_gid_attr *attr)\n{\n\tenum hns_roce_sgid_type sgid_type = GID_TYPE_FLAG_ROCE_V1;\n\tint ret;\n\n\tif (gid) {\n\t\tif (attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) {\n\t\t\tif (ipv6_addr_v4mapped((void *)gid))\n\t\t\t\tsgid_type = GID_TYPE_FLAG_ROCE_V2_IPV4;\n\t\t\telse\n\t\t\t\tsgid_type = GID_TYPE_FLAG_ROCE_V2_IPV6;\n\t\t} else if (attr->gid_type == IB_GID_TYPE_ROCE) {\n\t\t\tsgid_type = GID_TYPE_FLAG_ROCE_V1;\n\t\t}\n\t}\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\tret = config_gmv_table(hr_dev, gid_index, gid, sgid_type, attr);\n\telse\n\t\tret = config_sgid_table(hr_dev, gid_index, gid, sgid_type);\n\n\tif (ret)\n\t\tibdev_err(&hr_dev->ib_dev, \"failed to set gid, ret = %d!\\n\",\n\t\t\t  ret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_set_mac(struct hns_roce_dev *hr_dev, u8 phy_port,\n\t\t\t       const u8 *addr)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cfg_smac_tb *smac_tb =\n\t\t\t\t    (struct hns_roce_cfg_smac_tb *)desc.data;\n\tu16 reg_smac_h;\n\tu32 reg_smac_l;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_SMAC_TB, false);\n\n\treg_smac_l = *(u32 *)(&addr[0]);\n\treg_smac_h = *(u16 *)(&addr[4]);\n\n\thr_reg_write(smac_tb, CFG_SMAC_TB_IDX, phy_port);\n\thr_reg_write(smac_tb, CFG_SMAC_TB_VF_SMAC_H, reg_smac_h);\n\tsmac_tb->vf_smac_l = cpu_to_le32(reg_smac_l);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int set_mtpt_pbl(struct hns_roce_dev *hr_dev,\n\t\t\tstruct hns_roce_v2_mpt_entry *mpt_entry,\n\t\t\tstruct hns_roce_mr *mr)\n{\n\tu64 pages[HNS_ROCE_V2_MAX_INNER_MTPT_NUM] = { 0 };\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tdma_addr_t pbl_ba;\n\tint i, count;\n\n\tcount = hns_roce_mtr_find(hr_dev, &mr->pbl_mtr, 0, pages,\n\t\t\t\t  min_t(int, ARRAY_SIZE(pages), mr->npages),\n\t\t\t\t  &pbl_ba);\n\tif (count < 1) {\n\t\tibdev_err(ibdev, \"failed to find PBL mtr, count = %d.\\n\",\n\t\t\t  count);\n\t\treturn -ENOBUFS;\n\t}\n\n\t \n\tfor (i = 0; i < count; i++)\n\t\tpages[i] >>= 6;\n\n\tmpt_entry->pbl_size = cpu_to_le32(mr->npages);\n\tmpt_entry->pbl_ba_l = cpu_to_le32(pbl_ba >> 3);\n\thr_reg_write(mpt_entry, MPT_PBL_BA_H, upper_32_bits(pbl_ba >> 3));\n\n\tmpt_entry->pa0_l = cpu_to_le32(lower_32_bits(pages[0]));\n\thr_reg_write(mpt_entry, MPT_PA0_H, upper_32_bits(pages[0]));\n\n\tmpt_entry->pa1_l = cpu_to_le32(lower_32_bits(pages[1]));\n\thr_reg_write(mpt_entry, MPT_PA1_H, upper_32_bits(pages[1]));\n\thr_reg_write(mpt_entry, MPT_PBL_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(mr->pbl_mtr.hem_cfg.buf_pg_shift));\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_write_mtpt(struct hns_roce_dev *hr_dev,\n\t\t\t\t  void *mb_buf, struct hns_roce_mr *mr)\n{\n\tstruct hns_roce_v2_mpt_entry *mpt_entry;\n\n\tmpt_entry = mb_buf;\n\tmemset(mpt_entry, 0, sizeof(*mpt_entry));\n\n\thr_reg_write(mpt_entry, MPT_ST, V2_MPT_ST_VALID);\n\thr_reg_write(mpt_entry, MPT_PD, mr->pd);\n\n\thr_reg_write_bool(mpt_entry, MPT_BIND_EN,\n\t\t\t  mr->access & IB_ACCESS_MW_BIND);\n\thr_reg_write_bool(mpt_entry, MPT_ATOMIC_EN,\n\t\t\t  mr->access & IB_ACCESS_REMOTE_ATOMIC);\n\thr_reg_write_bool(mpt_entry, MPT_RR_EN,\n\t\t\t  mr->access & IB_ACCESS_REMOTE_READ);\n\thr_reg_write_bool(mpt_entry, MPT_RW_EN,\n\t\t\t  mr->access & IB_ACCESS_REMOTE_WRITE);\n\thr_reg_write_bool(mpt_entry, MPT_LW_EN,\n\t\t\t  mr->access & IB_ACCESS_LOCAL_WRITE);\n\n\tmpt_entry->len_l = cpu_to_le32(lower_32_bits(mr->size));\n\tmpt_entry->len_h = cpu_to_le32(upper_32_bits(mr->size));\n\tmpt_entry->lkey = cpu_to_le32(mr->key);\n\tmpt_entry->va_l = cpu_to_le32(lower_32_bits(mr->iova));\n\tmpt_entry->va_h = cpu_to_le32(upper_32_bits(mr->iova));\n\n\tif (mr->type != MR_TYPE_MR)\n\t\thr_reg_enable(mpt_entry, MPT_PA);\n\n\tif (mr->type == MR_TYPE_DMA)\n\t\treturn 0;\n\n\tif (mr->pbl_hop_num != HNS_ROCE_HOP_NUM_0)\n\t\thr_reg_write(mpt_entry, MPT_PBL_HOP_NUM, mr->pbl_hop_num);\n\n\thr_reg_write(mpt_entry, MPT_PBL_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(mr->pbl_mtr.hem_cfg.ba_pg_shift));\n\thr_reg_enable(mpt_entry, MPT_INNER_PA_VLD);\n\n\treturn set_mtpt_pbl(hr_dev, mpt_entry, mr);\n}\n\nstatic int hns_roce_v2_rereg_write_mtpt(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tstruct hns_roce_mr *mr, int flags,\n\t\t\t\t\tvoid *mb_buf)\n{\n\tstruct hns_roce_v2_mpt_entry *mpt_entry = mb_buf;\n\tu32 mr_access_flags = mr->access;\n\tint ret = 0;\n\n\thr_reg_write(mpt_entry, MPT_ST, V2_MPT_ST_VALID);\n\thr_reg_write(mpt_entry, MPT_PD, mr->pd);\n\n\tif (flags & IB_MR_REREG_ACCESS) {\n\t\thr_reg_write(mpt_entry, MPT_BIND_EN,\n\t\t\t     (mr_access_flags & IB_ACCESS_MW_BIND ? 1 : 0));\n\t\thr_reg_write(mpt_entry, MPT_ATOMIC_EN,\n\t\t\t     mr_access_flags & IB_ACCESS_REMOTE_ATOMIC ? 1 : 0);\n\t\thr_reg_write(mpt_entry, MPT_RR_EN,\n\t\t\t     mr_access_flags & IB_ACCESS_REMOTE_READ ? 1 : 0);\n\t\thr_reg_write(mpt_entry, MPT_RW_EN,\n\t\t\t     mr_access_flags & IB_ACCESS_REMOTE_WRITE ? 1 : 0);\n\t\thr_reg_write(mpt_entry, MPT_LW_EN,\n\t\t\t     mr_access_flags & IB_ACCESS_LOCAL_WRITE ? 1 : 0);\n\t}\n\n\tif (flags & IB_MR_REREG_TRANS) {\n\t\tmpt_entry->va_l = cpu_to_le32(lower_32_bits(mr->iova));\n\t\tmpt_entry->va_h = cpu_to_le32(upper_32_bits(mr->iova));\n\t\tmpt_entry->len_l = cpu_to_le32(lower_32_bits(mr->size));\n\t\tmpt_entry->len_h = cpu_to_le32(upper_32_bits(mr->size));\n\n\t\tret = set_mtpt_pbl(hr_dev, mpt_entry, mr);\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_frmr_write_mtpt(struct hns_roce_dev *hr_dev,\n\t\t\t\t       void *mb_buf, struct hns_roce_mr *mr)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_v2_mpt_entry *mpt_entry;\n\tdma_addr_t pbl_ba = 0;\n\n\tmpt_entry = mb_buf;\n\tmemset(mpt_entry, 0, sizeof(*mpt_entry));\n\n\tif (hns_roce_mtr_find(hr_dev, &mr->pbl_mtr, 0, NULL, 0, &pbl_ba) < 0) {\n\t\tibdev_err(ibdev, \"failed to find frmr mtr.\\n\");\n\t\treturn -ENOBUFS;\n\t}\n\n\thr_reg_write(mpt_entry, MPT_ST, V2_MPT_ST_FREE);\n\thr_reg_write(mpt_entry, MPT_PD, mr->pd);\n\n\thr_reg_enable(mpt_entry, MPT_RA_EN);\n\thr_reg_enable(mpt_entry, MPT_R_INV_EN);\n\n\thr_reg_enable(mpt_entry, MPT_FRE);\n\thr_reg_clear(mpt_entry, MPT_MR_MW);\n\thr_reg_enable(mpt_entry, MPT_BPD);\n\thr_reg_clear(mpt_entry, MPT_PA);\n\n\thr_reg_write(mpt_entry, MPT_PBL_HOP_NUM, 1);\n\thr_reg_write(mpt_entry, MPT_PBL_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(mr->pbl_mtr.hem_cfg.ba_pg_shift));\n\thr_reg_write(mpt_entry, MPT_PBL_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(mr->pbl_mtr.hem_cfg.buf_pg_shift));\n\n\tmpt_entry->pbl_size = cpu_to_le32(mr->npages);\n\n\tmpt_entry->pbl_ba_l = cpu_to_le32(lower_32_bits(pbl_ba >> 3));\n\thr_reg_write(mpt_entry, MPT_PBL_BA_H, upper_32_bits(pbl_ba >> 3));\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_mw_write_mtpt(void *mb_buf, struct hns_roce_mw *mw)\n{\n\tstruct hns_roce_v2_mpt_entry *mpt_entry;\n\n\tmpt_entry = mb_buf;\n\tmemset(mpt_entry, 0, sizeof(*mpt_entry));\n\n\thr_reg_write(mpt_entry, MPT_ST, V2_MPT_ST_FREE);\n\thr_reg_write(mpt_entry, MPT_PD, mw->pdn);\n\n\thr_reg_enable(mpt_entry, MPT_R_INV_EN);\n\thr_reg_enable(mpt_entry, MPT_LW_EN);\n\n\thr_reg_enable(mpt_entry, MPT_MR_MW);\n\thr_reg_enable(mpt_entry, MPT_BPD);\n\thr_reg_clear(mpt_entry, MPT_PA);\n\thr_reg_write(mpt_entry, MPT_BQP,\n\t\t     mw->ibmw.type == IB_MW_TYPE_1 ? 0 : 1);\n\n\tmpt_entry->lkey = cpu_to_le32(mw->rkey);\n\n\thr_reg_write(mpt_entry, MPT_PBL_HOP_NUM,\n\t\t     mw->pbl_hop_num == HNS_ROCE_HOP_NUM_0 ? 0 :\n\t\t\t\t\t\t\t     mw->pbl_hop_num);\n\thr_reg_write(mpt_entry, MPT_PBL_BA_PG_SZ,\n\t\t     mw->pbl_ba_pg_sz + PG_SHIFT_OFFSET);\n\thr_reg_write(mpt_entry, MPT_PBL_BUF_PG_SZ,\n\t\t     mw->pbl_buf_pg_sz + PG_SHIFT_OFFSET);\n\n\treturn 0;\n}\n\nstatic int free_mr_post_send_lp_wqe(struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(hr_qp->ibqp.device);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tconst struct ib_send_wr *bad_wr;\n\tstruct ib_rdma_wr rdma_wr = {};\n\tstruct ib_send_wr *send_wr;\n\tint ret;\n\n\tsend_wr = &rdma_wr.wr;\n\tsend_wr->opcode = IB_WR_RDMA_WRITE;\n\n\tret = hns_roce_v2_post_send(&hr_qp->ibqp, send_wr, &bad_wr);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to post wqe for free mr, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_poll_cq(struct ib_cq *ibcq, int num_entries,\n\t\t\t       struct ib_wc *wc);\n\nstatic void free_mr_send_cmd_to_hw(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tstruct hns_roce_v2_free_mr *free_mr = &priv->free_mr;\n\tstruct ib_wc wc[ARRAY_SIZE(free_mr->rsv_qp)];\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_qp *hr_qp;\n\tunsigned long end;\n\tint cqe_cnt = 0;\n\tint npolled;\n\tint ret;\n\tint i;\n\n\t \n\tif (priv->handle->rinfo.reset_state == HNS_ROCE_STATE_RST_INIT ||\n\t    priv->handle->rinfo.instance_state == HNS_ROCE_STATE_INIT ||\n\t    hr_dev->state == HNS_ROCE_DEVICE_STATE_UNINIT)\n\t\treturn;\n\n\tmutex_lock(&free_mr->mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) {\n\t\thr_qp = free_mr->rsv_qp[i];\n\n\t\tret = free_mr_post_send_lp_wqe(hr_qp);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to send wqe (qp:0x%lx) for free mr, ret = %d.\\n\",\n\t\t\t\t  hr_qp->qpn, ret);\n\t\t\tbreak;\n\t\t}\n\n\t\tcqe_cnt++;\n\t}\n\n\tend = msecs_to_jiffies(HNS_ROCE_V2_FREE_MR_TIMEOUT) + jiffies;\n\twhile (cqe_cnt) {\n\t\tnpolled = hns_roce_v2_poll_cq(&free_mr->rsv_cq->ib_cq, cqe_cnt, wc);\n\t\tif (npolled < 0) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to poll cqe for free mr, remain %d cqe.\\n\",\n\t\t\t\t  cqe_cnt);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after(jiffies, end)) {\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to poll cqe for free mr and timeout, remain %d cqe.\\n\",\n\t\t\t\t  cqe_cnt);\n\t\t\tgoto out;\n\t\t}\n\t\tcqe_cnt -= npolled;\n\t}\n\nout:\n\tmutex_unlock(&free_mr->mutex);\n}\n\nstatic void hns_roce_v2_dereg_mr(struct hns_roce_dev *hr_dev)\n{\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)\n\t\tfree_mr_send_cmd_to_hw(hr_dev);\n}\n\nstatic void *get_cqe_v2(struct hns_roce_cq *hr_cq, int n)\n{\n\treturn hns_roce_buf_offset(hr_cq->mtr.kmem, n * hr_cq->cqe_size);\n}\n\nstatic void *get_sw_cqe_v2(struct hns_roce_cq *hr_cq, unsigned int n)\n{\n\tstruct hns_roce_v2_cqe *cqe = get_cqe_v2(hr_cq, n & hr_cq->ib_cq.cqe);\n\n\t \n\treturn (hr_reg_read(cqe, CQE_OWNER) ^ !!(n & hr_cq->cq_depth)) ? cqe :\n\t\t\t\t\t\t\t\t\t NULL;\n}\n\nstatic inline void update_cq_db(struct hns_roce_dev *hr_dev,\n\t\t\t\tstruct hns_roce_cq *hr_cq)\n{\n\tif (likely(hr_cq->flags & HNS_ROCE_CQ_FLAG_RECORD_DB)) {\n\t\t*hr_cq->set_ci_db = hr_cq->cons_index & V2_CQ_DB_CONS_IDX_M;\n\t} else {\n\t\tstruct hns_roce_v2_db cq_db = {};\n\n\t\thr_reg_write(&cq_db, DB_TAG, hr_cq->cqn);\n\t\thr_reg_write(&cq_db, DB_CMD, HNS_ROCE_V2_CQ_DB);\n\t\thr_reg_write(&cq_db, DB_CQ_CI, hr_cq->cons_index);\n\t\thr_reg_write(&cq_db, DB_CQ_CMD_SN, 1);\n\n\t\thns_roce_write64(hr_dev, (__le32 *)&cq_db, hr_cq->db_reg);\n\t}\n}\n\nstatic void __hns_roce_v2_cq_clean(struct hns_roce_cq *hr_cq, u32 qpn,\n\t\t\t\t   struct hns_roce_srq *srq)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(hr_cq->ib_cq.device);\n\tstruct hns_roce_v2_cqe *cqe, *dest;\n\tu32 prod_index;\n\tint nfreed = 0;\n\tint wqe_index;\n\tu8 owner_bit;\n\n\tfor (prod_index = hr_cq->cons_index; get_sw_cqe_v2(hr_cq, prod_index);\n\t     ++prod_index) {\n\t\tif (prod_index > hr_cq->cons_index + hr_cq->ib_cq.cqe)\n\t\t\tbreak;\n\t}\n\n\t \n\twhile ((int) --prod_index - (int) hr_cq->cons_index >= 0) {\n\t\tcqe = get_cqe_v2(hr_cq, prod_index & hr_cq->ib_cq.cqe);\n\t\tif (hr_reg_read(cqe, CQE_LCL_QPN) == qpn) {\n\t\t\tif (srq && hr_reg_read(cqe, CQE_S_R)) {\n\t\t\t\twqe_index = hr_reg_read(cqe, CQE_WQE_IDX);\n\t\t\t\thns_roce_free_srq_wqe(srq, wqe_index);\n\t\t\t}\n\t\t\t++nfreed;\n\t\t} else if (nfreed) {\n\t\t\tdest = get_cqe_v2(hr_cq, (prod_index + nfreed) &\n\t\t\t\t\t  hr_cq->ib_cq.cqe);\n\t\t\towner_bit = hr_reg_read(dest, CQE_OWNER);\n\t\t\tmemcpy(dest, cqe, hr_cq->cqe_size);\n\t\t\thr_reg_write(dest, CQE_OWNER, owner_bit);\n\t\t}\n\t}\n\n\tif (nfreed) {\n\t\thr_cq->cons_index += nfreed;\n\t\tupdate_cq_db(hr_dev, hr_cq);\n\t}\n}\n\nstatic void hns_roce_v2_cq_clean(struct hns_roce_cq *hr_cq, u32 qpn,\n\t\t\t\t struct hns_roce_srq *srq)\n{\n\tspin_lock_irq(&hr_cq->lock);\n\t__hns_roce_v2_cq_clean(hr_cq, qpn, srq);\n\tspin_unlock_irq(&hr_cq->lock);\n}\n\nstatic void hns_roce_v2_write_cqc(struct hns_roce_dev *hr_dev,\n\t\t\t\t  struct hns_roce_cq *hr_cq, void *mb_buf,\n\t\t\t\t  u64 *mtts, dma_addr_t dma_handle)\n{\n\tstruct hns_roce_v2_cq_context *cq_context;\n\n\tcq_context = mb_buf;\n\tmemset(cq_context, 0, sizeof(*cq_context));\n\n\thr_reg_write(cq_context, CQC_CQ_ST, V2_CQ_STATE_VALID);\n\thr_reg_write(cq_context, CQC_ARM_ST, NO_ARMED);\n\thr_reg_write(cq_context, CQC_SHIFT, ilog2(hr_cq->cq_depth));\n\thr_reg_write(cq_context, CQC_CEQN, hr_cq->vector);\n\thr_reg_write(cq_context, CQC_CQN, hr_cq->cqn);\n\n\tif (hr_cq->cqe_size == HNS_ROCE_V3_CQE_SIZE)\n\t\thr_reg_write(cq_context, CQC_CQE_SIZE, CQE_SIZE_64B);\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_STASH)\n\t\thr_reg_enable(cq_context, CQC_STASH);\n\n\thr_reg_write(cq_context, CQC_CQE_CUR_BLK_ADDR_L,\n\t\t     to_hr_hw_page_addr(mtts[0]));\n\thr_reg_write(cq_context, CQC_CQE_CUR_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts[0])));\n\thr_reg_write(cq_context, CQC_CQE_HOP_NUM, hr_dev->caps.cqe_hop_num ==\n\t\t     HNS_ROCE_HOP_NUM_0 ? 0 : hr_dev->caps.cqe_hop_num);\n\thr_reg_write(cq_context, CQC_CQE_NEX_BLK_ADDR_L,\n\t\t     to_hr_hw_page_addr(mtts[1]));\n\thr_reg_write(cq_context, CQC_CQE_NEX_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts[1])));\n\thr_reg_write(cq_context, CQC_CQE_BAR_PG_SZ,\n\t\t     to_hr_hw_page_shift(hr_cq->mtr.hem_cfg.ba_pg_shift));\n\thr_reg_write(cq_context, CQC_CQE_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(hr_cq->mtr.hem_cfg.buf_pg_shift));\n\thr_reg_write(cq_context, CQC_CQE_BA_L, dma_handle >> 3);\n\thr_reg_write(cq_context, CQC_CQE_BA_H, (dma_handle >> (32 + 3)));\n\thr_reg_write_bool(cq_context, CQC_DB_RECORD_EN,\n\t\t\t  hr_cq->flags & HNS_ROCE_CQ_FLAG_RECORD_DB);\n\thr_reg_write(cq_context, CQC_CQE_DB_RECORD_ADDR_L,\n\t\t     ((u32)hr_cq->db.dma) >> 1);\n\thr_reg_write(cq_context, CQC_CQE_DB_RECORD_ADDR_H,\n\t\t     hr_cq->db.dma >> 32);\n\thr_reg_write(cq_context, CQC_CQ_MAX_CNT,\n\t\t     HNS_ROCE_V2_CQ_DEFAULT_BURST_NUM);\n\thr_reg_write(cq_context, CQC_CQ_PERIOD,\n\t\t     HNS_ROCE_V2_CQ_DEFAULT_INTERVAL);\n}\n\nstatic int hns_roce_v2_req_notify_cq(struct ib_cq *ibcq,\n\t\t\t\t     enum ib_cq_notify_flags flags)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibcq->device);\n\tstruct hns_roce_cq *hr_cq = to_hr_cq(ibcq);\n\tstruct hns_roce_v2_db cq_db = {};\n\tu32 notify_flag;\n\n\t \n\tnotify_flag = (flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED ?\n\t\t      V2_CQ_DB_REQ_NOT : V2_CQ_DB_REQ_NOT_SOL;\n\n\thr_reg_write(&cq_db, DB_TAG, hr_cq->cqn);\n\thr_reg_write(&cq_db, DB_CMD, HNS_ROCE_V2_CQ_DB_NOTIFY);\n\thr_reg_write(&cq_db, DB_CQ_CI, hr_cq->cons_index);\n\thr_reg_write(&cq_db, DB_CQ_CMD_SN, hr_cq->arm_sn);\n\thr_reg_write(&cq_db, DB_CQ_NOTIFY, notify_flag);\n\n\thns_roce_write64(hr_dev, (__le32 *)&cq_db, hr_cq->db_reg);\n\n\treturn 0;\n}\n\nstatic int sw_comp(struct hns_roce_qp *hr_qp, struct hns_roce_wq *wq,\n\t\t   int num_entries, struct ib_wc *wc)\n{\n\tunsigned int left;\n\tint npolled = 0;\n\n\tleft = wq->head - wq->tail;\n\tif (left == 0)\n\t\treturn 0;\n\n\tleft = min_t(unsigned int, (unsigned int)num_entries, left);\n\twhile (npolled < left) {\n\t\twc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\twc->vendor_err = 0;\n\t\twc->qp = &hr_qp->ibqp;\n\n\t\twq->tail++;\n\t\twc++;\n\t\tnpolled++;\n\t}\n\n\treturn npolled;\n}\n\nstatic int hns_roce_v2_sw_poll_cq(struct hns_roce_cq *hr_cq, int num_entries,\n\t\t\t\t  struct ib_wc *wc)\n{\n\tstruct hns_roce_qp *hr_qp;\n\tint npolled = 0;\n\n\tlist_for_each_entry(hr_qp, &hr_cq->sq_list, sq_node) {\n\t\tnpolled += sw_comp(hr_qp, &hr_qp->sq,\n\t\t\t\t   num_entries - npolled, wc + npolled);\n\t\tif (npolled >= num_entries)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(hr_qp, &hr_cq->rq_list, rq_node) {\n\t\tnpolled += sw_comp(hr_qp, &hr_qp->rq,\n\t\t\t\t   num_entries - npolled, wc + npolled);\n\t\tif (npolled >= num_entries)\n\t\t\tgoto out;\n\t}\n\nout:\n\treturn npolled;\n}\n\nstatic void get_cqe_status(struct hns_roce_dev *hr_dev, struct hns_roce_qp *qp,\n\t\t\t   struct hns_roce_cq *cq, struct hns_roce_v2_cqe *cqe,\n\t\t\t   struct ib_wc *wc)\n{\n\tstatic const struct {\n\t\tu32 cqe_status;\n\t\tenum ib_wc_status wc_status;\n\t} map[] = {\n\t\t{ HNS_ROCE_CQE_V2_SUCCESS, IB_WC_SUCCESS },\n\t\t{ HNS_ROCE_CQE_V2_LOCAL_LENGTH_ERR, IB_WC_LOC_LEN_ERR },\n\t\t{ HNS_ROCE_CQE_V2_LOCAL_QP_OP_ERR, IB_WC_LOC_QP_OP_ERR },\n\t\t{ HNS_ROCE_CQE_V2_LOCAL_PROT_ERR, IB_WC_LOC_PROT_ERR },\n\t\t{ HNS_ROCE_CQE_V2_WR_FLUSH_ERR, IB_WC_WR_FLUSH_ERR },\n\t\t{ HNS_ROCE_CQE_V2_MW_BIND_ERR, IB_WC_MW_BIND_ERR },\n\t\t{ HNS_ROCE_CQE_V2_BAD_RESP_ERR, IB_WC_BAD_RESP_ERR },\n\t\t{ HNS_ROCE_CQE_V2_LOCAL_ACCESS_ERR, IB_WC_LOC_ACCESS_ERR },\n\t\t{ HNS_ROCE_CQE_V2_REMOTE_INVAL_REQ_ERR, IB_WC_REM_INV_REQ_ERR },\n\t\t{ HNS_ROCE_CQE_V2_REMOTE_ACCESS_ERR, IB_WC_REM_ACCESS_ERR },\n\t\t{ HNS_ROCE_CQE_V2_REMOTE_OP_ERR, IB_WC_REM_OP_ERR },\n\t\t{ HNS_ROCE_CQE_V2_TRANSPORT_RETRY_EXC_ERR,\n\t\t  IB_WC_RETRY_EXC_ERR },\n\t\t{ HNS_ROCE_CQE_V2_RNR_RETRY_EXC_ERR, IB_WC_RNR_RETRY_EXC_ERR },\n\t\t{ HNS_ROCE_CQE_V2_REMOTE_ABORT_ERR, IB_WC_REM_ABORT_ERR },\n\t\t{ HNS_ROCE_CQE_V2_GENERAL_ERR, IB_WC_GENERAL_ERR}\n\t};\n\n\tu32 cqe_status = hr_reg_read(cqe, CQE_STATUS);\n\tint i;\n\n\twc->status = IB_WC_GENERAL_ERR;\n\tfor (i = 0; i < ARRAY_SIZE(map); i++)\n\t\tif (cqe_status == map[i].cqe_status) {\n\t\t\twc->status = map[i].wc_status;\n\t\t\tbreak;\n\t\t}\n\n\tif (likely(wc->status == IB_WC_SUCCESS ||\n\t\t   wc->status == IB_WC_WR_FLUSH_ERR))\n\t\treturn;\n\n\tibdev_err(&hr_dev->ib_dev, \"error cqe status 0x%x:\\n\", cqe_status);\n\tprint_hex_dump(KERN_ERR, \"\", DUMP_PREFIX_NONE, 16, 4, cqe,\n\t\t       cq->cqe_size, false);\n\twc->vendor_err = hr_reg_read(cqe, CQE_SUB_STATUS);\n\n\t \n\tif (cqe_status == HNS_ROCE_CQE_V2_GENERAL_ERR)\n\t\treturn;\n\n\tflush_cqe(hr_dev, qp);\n}\n\nstatic int get_cur_qp(struct hns_roce_cq *hr_cq, struct hns_roce_v2_cqe *cqe,\n\t\t      struct hns_roce_qp **cur_qp)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(hr_cq->ib_cq.device);\n\tstruct hns_roce_qp *hr_qp = *cur_qp;\n\tu32 qpn;\n\n\tqpn = hr_reg_read(cqe, CQE_LCL_QPN);\n\n\tif (!hr_qp || qpn != hr_qp->qpn) {\n\t\thr_qp = __hns_roce_qp_lookup(hr_dev, qpn);\n\t\tif (unlikely(!hr_qp)) {\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"CQ %06lx with entry for unknown QPN %06x\\n\",\n\t\t\t\t  hr_cq->cqn, qpn);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*cur_qp = hr_qp;\n\t}\n\n\treturn 0;\n}\n\n \n#define HR_WC_OP_MAP(hr_key, ib_key) \\\n\t\t[HNS_ROCE_V2_WQE_OP_ ## hr_key] = 1 + IB_WC_ ## ib_key\n\nstatic const u32 wc_send_op_map[] = {\n\tHR_WC_OP_MAP(SEND,\t\t\tSEND),\n\tHR_WC_OP_MAP(SEND_WITH_INV,\t\tSEND),\n\tHR_WC_OP_MAP(SEND_WITH_IMM,\t\tSEND),\n\tHR_WC_OP_MAP(RDMA_READ,\t\t\tRDMA_READ),\n\tHR_WC_OP_MAP(RDMA_WRITE,\t\tRDMA_WRITE),\n\tHR_WC_OP_MAP(RDMA_WRITE_WITH_IMM,\tRDMA_WRITE),\n\tHR_WC_OP_MAP(ATOM_CMP_AND_SWAP,\t\tCOMP_SWAP),\n\tHR_WC_OP_MAP(ATOM_FETCH_AND_ADD,\tFETCH_ADD),\n\tHR_WC_OP_MAP(ATOM_MSK_CMP_AND_SWAP,\tMASKED_COMP_SWAP),\n\tHR_WC_OP_MAP(ATOM_MSK_FETCH_AND_ADD,\tMASKED_FETCH_ADD),\n\tHR_WC_OP_MAP(FAST_REG_PMR,\t\tREG_MR),\n\tHR_WC_OP_MAP(BIND_MW,\t\t\tREG_MR),\n};\n\nstatic int to_ib_wc_send_op(u32 hr_opcode)\n{\n\tif (hr_opcode >= ARRAY_SIZE(wc_send_op_map))\n\t\treturn -EINVAL;\n\n\treturn wc_send_op_map[hr_opcode] ? wc_send_op_map[hr_opcode] - 1 :\n\t\t\t\t\t   -EINVAL;\n}\n\nstatic const u32 wc_recv_op_map[] = {\n\tHR_WC_OP_MAP(RDMA_WRITE_WITH_IMM,\t\tWITH_IMM),\n\tHR_WC_OP_MAP(SEND,\t\t\t\tRECV),\n\tHR_WC_OP_MAP(SEND_WITH_IMM,\t\t\tWITH_IMM),\n\tHR_WC_OP_MAP(SEND_WITH_INV,\t\t\tRECV),\n};\n\nstatic int to_ib_wc_recv_op(u32 hr_opcode)\n{\n\tif (hr_opcode >= ARRAY_SIZE(wc_recv_op_map))\n\t\treturn -EINVAL;\n\n\treturn wc_recv_op_map[hr_opcode] ? wc_recv_op_map[hr_opcode] - 1 :\n\t\t\t\t\t   -EINVAL;\n}\n\nstatic void fill_send_wc(struct ib_wc *wc, struct hns_roce_v2_cqe *cqe)\n{\n\tu32 hr_opcode;\n\tint ib_opcode;\n\n\twc->wc_flags = 0;\n\n\thr_opcode = hr_reg_read(cqe, CQE_OPCODE);\n\tswitch (hr_opcode) {\n\tcase HNS_ROCE_V2_WQE_OP_RDMA_READ:\n\t\twc->byte_len = le32_to_cpu(cqe->byte_cnt);\n\t\tbreak;\n\tcase HNS_ROCE_V2_WQE_OP_SEND_WITH_IMM:\n\tcase HNS_ROCE_V2_WQE_OP_RDMA_WRITE_WITH_IMM:\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\tbreak;\n\tcase HNS_ROCE_V2_WQE_OP_ATOM_CMP_AND_SWAP:\n\tcase HNS_ROCE_V2_WQE_OP_ATOM_FETCH_AND_ADD:\n\tcase HNS_ROCE_V2_WQE_OP_ATOM_MSK_CMP_AND_SWAP:\n\tcase HNS_ROCE_V2_WQE_OP_ATOM_MSK_FETCH_AND_ADD:\n\t\twc->byte_len  = 8;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tib_opcode = to_ib_wc_send_op(hr_opcode);\n\tif (ib_opcode < 0)\n\t\twc->status = IB_WC_GENERAL_ERR;\n\telse\n\t\twc->opcode = ib_opcode;\n}\n\nstatic int fill_recv_wc(struct ib_wc *wc, struct hns_roce_v2_cqe *cqe)\n{\n\tu32 hr_opcode;\n\tint ib_opcode;\n\n\twc->byte_len = le32_to_cpu(cqe->byte_cnt);\n\n\thr_opcode = hr_reg_read(cqe, CQE_OPCODE);\n\tswitch (hr_opcode) {\n\tcase HNS_ROCE_V2_OPCODE_RDMA_WRITE_IMM:\n\tcase HNS_ROCE_V2_OPCODE_SEND_WITH_IMM:\n\t\twc->wc_flags = IB_WC_WITH_IMM;\n\t\twc->ex.imm_data = cpu_to_be32(le32_to_cpu(cqe->immtdata));\n\t\tbreak;\n\tcase HNS_ROCE_V2_OPCODE_SEND_WITH_INV:\n\t\twc->wc_flags = IB_WC_WITH_INVALIDATE;\n\t\twc->ex.invalidate_rkey = le32_to_cpu(cqe->rkey);\n\t\tbreak;\n\tdefault:\n\t\twc->wc_flags = 0;\n\t}\n\n\tib_opcode = to_ib_wc_recv_op(hr_opcode);\n\tif (ib_opcode < 0)\n\t\twc->status = IB_WC_GENERAL_ERR;\n\telse\n\t\twc->opcode = ib_opcode;\n\n\twc->sl = hr_reg_read(cqe, CQE_SL);\n\twc->src_qp = hr_reg_read(cqe, CQE_RMT_QPN);\n\twc->slid = 0;\n\twc->wc_flags |= hr_reg_read(cqe, CQE_GRH) ? IB_WC_GRH : 0;\n\twc->port_num = hr_reg_read(cqe, CQE_PORTN);\n\twc->pkey_index = 0;\n\n\tif (hr_reg_read(cqe, CQE_VID_VLD)) {\n\t\twc->vlan_id = hr_reg_read(cqe, CQE_VID);\n\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t} else {\n\t\twc->vlan_id = 0xffff;\n\t}\n\n\twc->network_hdr_type = hr_reg_read(cqe, CQE_PORT_TYPE);\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_poll_one(struct hns_roce_cq *hr_cq,\n\t\t\t\tstruct hns_roce_qp **cur_qp, struct ib_wc *wc)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(hr_cq->ib_cq.device);\n\tstruct hns_roce_qp *qp = *cur_qp;\n\tstruct hns_roce_srq *srq = NULL;\n\tstruct hns_roce_v2_cqe *cqe;\n\tstruct hns_roce_wq *wq;\n\tint is_send;\n\tu16 wqe_idx;\n\tint ret;\n\n\tcqe = get_sw_cqe_v2(hr_cq, hr_cq->cons_index);\n\tif (!cqe)\n\t\treturn -EAGAIN;\n\n\t++hr_cq->cons_index;\n\t \n\trmb();\n\n\tret = get_cur_qp(hr_cq, cqe, &qp);\n\tif (ret)\n\t\treturn ret;\n\n\twc->qp = &qp->ibqp;\n\twc->vendor_err = 0;\n\n\twqe_idx = hr_reg_read(cqe, CQE_WQE_IDX);\n\n\tis_send = !hr_reg_read(cqe, CQE_S_R);\n\tif (is_send) {\n\t\twq = &qp->sq;\n\n\t\t \n\t\tif (qp->sq_signal_bits)\n\t\t\twq->tail += (wqe_idx - (u16)wq->tail) &\n\t\t\t\t    (wq->wqe_cnt - 1);\n\n\t\twc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];\n\t\t++wq->tail;\n\n\t\tfill_send_wc(wc, cqe);\n\t} else {\n\t\tif (qp->ibqp.srq) {\n\t\t\tsrq = to_hr_srq(qp->ibqp.srq);\n\t\t\twc->wr_id = srq->wrid[wqe_idx];\n\t\t\thns_roce_free_srq_wqe(srq, wqe_idx);\n\t\t} else {\n\t\t\twq = &qp->rq;\n\t\t\twc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];\n\t\t\t++wq->tail;\n\t\t}\n\n\t\tret = fill_recv_wc(wc, cqe);\n\t}\n\n\tget_cqe_status(hr_dev, qp, hr_cq, cqe, wc);\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\treturn 0;\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_poll_cq(struct ib_cq *ibcq, int num_entries,\n\t\t\t       struct ib_wc *wc)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibcq->device);\n\tstruct hns_roce_cq *hr_cq = to_hr_cq(ibcq);\n\tstruct hns_roce_qp *cur_qp = NULL;\n\tunsigned long flags;\n\tint npolled;\n\n\tspin_lock_irqsave(&hr_cq->lock, flags);\n\n\t \n\tif (hr_dev->state == HNS_ROCE_DEVICE_STATE_UNINIT) {\n\t\tnpolled = hns_roce_v2_sw_poll_cq(hr_cq, num_entries, wc);\n\t\tgoto out;\n\t}\n\n\tfor (npolled = 0; npolled < num_entries; ++npolled) {\n\t\tif (hns_roce_v2_poll_one(hr_cq, &cur_qp, wc + npolled))\n\t\t\tbreak;\n\t}\n\n\tif (npolled)\n\t\tupdate_cq_db(hr_dev, hr_cq);\n\nout:\n\tspin_unlock_irqrestore(&hr_cq->lock, flags);\n\n\treturn npolled;\n}\n\nstatic int get_op_for_set_hem(struct hns_roce_dev *hr_dev, u32 type,\n\t\t\t      u32 step_idx, u8 *mbox_cmd)\n{\n\tu8 cmd;\n\n\tswitch (type) {\n\tcase HEM_TYPE_QPC:\n\t\tcmd = HNS_ROCE_CMD_WRITE_QPC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_MTPT:\n\t\tcmd = HNS_ROCE_CMD_WRITE_MPT_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_CQC:\n\t\tcmd = HNS_ROCE_CMD_WRITE_CQC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_SRQC:\n\t\tcmd = HNS_ROCE_CMD_WRITE_SRQC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_SCCC:\n\t\tcmd = HNS_ROCE_CMD_WRITE_SCCC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_QPC_TIMER:\n\t\tcmd = HNS_ROCE_CMD_WRITE_QPC_TIMER_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_CQC_TIMER:\n\t\tcmd = HNS_ROCE_CMD_WRITE_CQC_TIMER_BT0;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(hr_dev->dev, \"failed to check hem type %u.\\n\", type);\n\t\treturn -EINVAL;\n\t}\n\n\t*mbox_cmd = cmd + step_idx;\n\n\treturn 0;\n}\n\nstatic int config_gmv_ba_to_hw(struct hns_roce_dev *hr_dev, unsigned long obj,\n\t\t\t       dma_addr_t base_addr)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tu32 idx = obj / (HNS_HW_PAGE_SIZE / hr_dev->caps.gmv_entry_sz);\n\tu64 addr = to_hr_hw_page_addr(base_addr);\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_GMV_BT, false);\n\n\thr_reg_write(req, CFG_GMV_BT_BA_L, lower_32_bits(addr));\n\thr_reg_write(req, CFG_GMV_BT_BA_H, upper_32_bits(addr));\n\thr_reg_write(req, CFG_GMV_BT_IDX, idx);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic int set_hem_to_hw(struct hns_roce_dev *hr_dev, int obj,\n\t\t\t dma_addr_t base_addr, u32 hem_type, u32 step_idx)\n{\n\tint ret;\n\tu8 cmd;\n\n\tif (unlikely(hem_type == HEM_TYPE_GMV))\n\t\treturn config_gmv_ba_to_hw(hr_dev, obj, base_addr);\n\n\tif (unlikely(hem_type == HEM_TYPE_SCCC && step_idx))\n\t\treturn 0;\n\n\tret = get_op_for_set_hem(hr_dev, hem_type, step_idx, &cmd);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn config_hem_ba_to_hw(hr_dev, base_addr, cmd, obj);\n}\n\nstatic int hns_roce_v2_set_hem(struct hns_roce_dev *hr_dev,\n\t\t\t       struct hns_roce_hem_table *table, int obj,\n\t\t\t       u32 step_idx)\n{\n\tstruct hns_roce_hem_iter iter;\n\tstruct hns_roce_hem_mhop mhop;\n\tstruct hns_roce_hem *hem;\n\tunsigned long mhop_obj = obj;\n\tint i, j, k;\n\tint ret = 0;\n\tu64 hem_idx = 0;\n\tu64 l1_idx = 0;\n\tu64 bt_ba = 0;\n\tu32 chunk_ba_num;\n\tu32 hop_num;\n\n\tif (!hns_roce_check_whether_mhop(hr_dev, table->type))\n\t\treturn 0;\n\n\thns_roce_calc_hem_mhop(hr_dev, table, &mhop_obj, &mhop);\n\ti = mhop.l0_idx;\n\tj = mhop.l1_idx;\n\tk = mhop.l2_idx;\n\thop_num = mhop.hop_num;\n\tchunk_ba_num = mhop.bt_chunk_size / 8;\n\n\tif (hop_num == 2) {\n\t\them_idx = i * chunk_ba_num * chunk_ba_num + j * chunk_ba_num +\n\t\t\t  k;\n\t\tl1_idx = i * chunk_ba_num + j;\n\t} else if (hop_num == 1) {\n\t\them_idx = i * chunk_ba_num + j;\n\t} else if (hop_num == HNS_ROCE_HOP_NUM_0) {\n\t\them_idx = i;\n\t}\n\n\tif (table->type == HEM_TYPE_SCCC)\n\t\tobj = mhop.l0_idx;\n\n\tif (check_whether_last_step(hop_num, step_idx)) {\n\t\them = table->hem[hem_idx];\n\t\tfor (hns_roce_hem_first(hem, &iter);\n\t\t     !hns_roce_hem_last(&iter); hns_roce_hem_next(&iter)) {\n\t\t\tbt_ba = hns_roce_hem_addr(&iter);\n\t\t\tret = set_hem_to_hw(hr_dev, obj, bt_ba, table->type,\n\t\t\t\t\t    step_idx);\n\t\t}\n\t} else {\n\t\tif (step_idx == 0)\n\t\t\tbt_ba = table->bt_l0_dma_addr[i];\n\t\telse if (step_idx == 1 && hop_num == 2)\n\t\t\tbt_ba = table->bt_l1_dma_addr[l1_idx];\n\n\t\tret = set_hem_to_hw(hr_dev, obj, bt_ba, table->type, step_idx);\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_clear_hem(struct hns_roce_dev *hr_dev,\n\t\t\t\t struct hns_roce_hem_table *table,\n\t\t\t\t int tag, u32 step_idx)\n{\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tstruct device *dev = hr_dev->dev;\n\tu8 cmd = 0xff;\n\tint ret;\n\n\tif (!hns_roce_check_whether_mhop(hr_dev, table->type))\n\t\treturn 0;\n\n\tswitch (table->type) {\n\tcase HEM_TYPE_QPC:\n\t\tcmd = HNS_ROCE_CMD_DESTROY_QPC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_MTPT:\n\t\tcmd = HNS_ROCE_CMD_DESTROY_MPT_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_CQC:\n\t\tcmd = HNS_ROCE_CMD_DESTROY_CQC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_SRQC:\n\t\tcmd = HNS_ROCE_CMD_DESTROY_SRQC_BT0;\n\t\tbreak;\n\tcase HEM_TYPE_SCCC:\n\tcase HEM_TYPE_QPC_TIMER:\n\tcase HEM_TYPE_CQC_TIMER:\n\tcase HEM_TYPE_GMV:\n\t\treturn 0;\n\tdefault:\n\t\tdev_warn(dev, \"table %u not to be destroyed by mailbox!\\n\",\n\t\t\t table->type);\n\t\treturn 0;\n\t}\n\n\tcmd += step_idx;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, cmd, tag);\n\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\treturn ret;\n}\n\nstatic int hns_roce_v2_qp_modify(struct hns_roce_dev *hr_dev,\n\t\t\t\t struct hns_roce_v2_qp_context *context,\n\t\t\t\t struct hns_roce_v2_qp_context *qpc_mask,\n\t\t\t\t struct hns_roce_qp *hr_qp)\n{\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint qpc_size;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\t \n\tqpc_size = hr_dev->caps.qpc_sz;\n\tmemcpy(mailbox->buf, context, qpc_size);\n\tmemcpy(mailbox->buf + qpc_size, qpc_mask, qpc_size);\n\n\tret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0,\n\t\t\t\tHNS_ROCE_CMD_MODIFY_QPC, hr_qp->qpn);\n\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn ret;\n}\n\nstatic void set_access_flags(struct hns_roce_qp *hr_qp,\n\t\t\t     struct hns_roce_v2_qp_context *context,\n\t\t\t     struct hns_roce_v2_qp_context *qpc_mask,\n\t\t\t     const struct ib_qp_attr *attr, int attr_mask)\n{\n\tu8 dest_rd_atomic;\n\tu32 access_flags;\n\n\tdest_rd_atomic = (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) ?\n\t\t\t attr->max_dest_rd_atomic : hr_qp->resp_depth;\n\n\taccess_flags = (attr_mask & IB_QP_ACCESS_FLAGS) ?\n\t\t       attr->qp_access_flags : hr_qp->atomic_rd_en;\n\n\tif (!dest_rd_atomic)\n\t\taccess_flags &= IB_ACCESS_REMOTE_WRITE;\n\n\thr_reg_write_bool(context, QPC_RRE,\n\t\t\t  access_flags & IB_ACCESS_REMOTE_READ);\n\thr_reg_clear(qpc_mask, QPC_RRE);\n\n\thr_reg_write_bool(context, QPC_RWE,\n\t\t\t  access_flags & IB_ACCESS_REMOTE_WRITE);\n\thr_reg_clear(qpc_mask, QPC_RWE);\n\n\thr_reg_write_bool(context, QPC_ATE,\n\t\t\t  access_flags & IB_ACCESS_REMOTE_ATOMIC);\n\thr_reg_clear(qpc_mask, QPC_ATE);\n\thr_reg_write_bool(context, QPC_EXT_ATE,\n\t\t\t  access_flags & IB_ACCESS_REMOTE_ATOMIC);\n\thr_reg_clear(qpc_mask, QPC_EXT_ATE);\n}\n\nstatic void set_qpc_wqe_cnt(struct hns_roce_qp *hr_qp,\n\t\t\t    struct hns_roce_v2_qp_context *context,\n\t\t\t    struct hns_roce_v2_qp_context *qpc_mask)\n{\n\thr_reg_write(context, QPC_SGE_SHIFT,\n\t\t     to_hr_hem_entries_shift(hr_qp->sge.sge_cnt,\n\t\t\t\t\t     hr_qp->sge.sge_shift));\n\n\thr_reg_write(context, QPC_SQ_SHIFT, ilog2(hr_qp->sq.wqe_cnt));\n\n\thr_reg_write(context, QPC_RQ_SHIFT, ilog2(hr_qp->rq.wqe_cnt));\n}\n\nstatic inline int get_cqn(struct ib_cq *ib_cq)\n{\n\treturn ib_cq ? to_hr_cq(ib_cq)->cqn : 0;\n}\n\nstatic inline int get_pdn(struct ib_pd *ib_pd)\n{\n\treturn ib_pd ? to_hr_pd(ib_pd)->pdn : 0;\n}\n\nstatic void modify_qp_reset_to_init(struct ib_qp *ibqp,\n\t\t\t\t    const struct ib_qp_attr *attr,\n\t\t\t\t    struct hns_roce_v2_qp_context *context,\n\t\t\t\t    struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\n\t \n\thr_reg_write(context, QPC_TST, to_hr_qp_type(ibqp->qp_type));\n\n\thr_reg_write(context, QPC_PD, get_pdn(ibqp->pd));\n\n\thr_reg_write(context, QPC_RQWS, ilog2(hr_qp->rq.max_gs));\n\n\tset_qpc_wqe_cnt(hr_qp, context, qpc_mask);\n\n\t \n\thr_reg_write(context, QPC_VLAN_ID, 0xfff);\n\n\tif (ibqp->qp_type == IB_QPT_XRC_TGT) {\n\t\tcontext->qkey_xrcd = cpu_to_le32(hr_qp->xrcdn);\n\n\t\thr_reg_enable(context, QPC_XRC_QP_TYPE);\n\t}\n\n\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)\n\t\thr_reg_enable(context, QPC_RQ_RECORD_EN);\n\n\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_OWNER_DB)\n\t\thr_reg_enable(context, QPC_OWNER_MODE);\n\n\thr_reg_write(context, QPC_RQ_DB_RECORD_ADDR_L,\n\t\t     lower_32_bits(hr_qp->rdb.dma) >> 1);\n\thr_reg_write(context, QPC_RQ_DB_RECORD_ADDR_H,\n\t\t     upper_32_bits(hr_qp->rdb.dma));\n\n\thr_reg_write(context, QPC_RX_CQN, get_cqn(ibqp->recv_cq));\n\n\tif (ibqp->srq) {\n\t\thr_reg_enable(context, QPC_SRQ_EN);\n\t\thr_reg_write(context, QPC_SRQN, to_hr_srq(ibqp->srq)->srqn);\n\t}\n\n\thr_reg_enable(context, QPC_FRE);\n\n\thr_reg_write(context, QPC_TX_CQN, get_cqn(ibqp->send_cq));\n\n\tif (hr_dev->caps.qpc_sz < HNS_ROCE_V3_QPC_SZ)\n\t\treturn;\n\n\tif (hr_dev->caps.flags & HNS_ROCE_CAP_FLAG_STASH)\n\t\thr_reg_enable(&context->ext, QPCEX_STASH);\n}\n\nstatic void modify_qp_init_to_init(struct ib_qp *ibqp,\n\t\t\t\t   const struct ib_qp_attr *attr,\n\t\t\t\t   struct hns_roce_v2_qp_context *context,\n\t\t\t\t   struct hns_roce_v2_qp_context *qpc_mask)\n{\n\t \n\thr_reg_write(context, QPC_TST, to_hr_qp_type(ibqp->qp_type));\n\thr_reg_clear(qpc_mask, QPC_TST);\n\n\thr_reg_write(context, QPC_PD, get_pdn(ibqp->pd));\n\thr_reg_clear(qpc_mask, QPC_PD);\n\n\thr_reg_write(context, QPC_RX_CQN, get_cqn(ibqp->recv_cq));\n\thr_reg_clear(qpc_mask, QPC_RX_CQN);\n\n\thr_reg_write(context, QPC_TX_CQN, get_cqn(ibqp->send_cq));\n\thr_reg_clear(qpc_mask, QPC_TX_CQN);\n\n\tif (ibqp->srq) {\n\t\thr_reg_enable(context, QPC_SRQ_EN);\n\t\thr_reg_clear(qpc_mask, QPC_SRQ_EN);\n\t\thr_reg_write(context, QPC_SRQN, to_hr_srq(ibqp->srq)->srqn);\n\t\thr_reg_clear(qpc_mask, QPC_SRQN);\n\t}\n}\n\nstatic int config_qp_rq_buf(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp,\n\t\t\t    struct hns_roce_v2_qp_context *context,\n\t\t\t    struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tu64 mtts[MTT_MIN_COUNT] = { 0 };\n\tu64 wqe_sge_ba;\n\tint count;\n\n\t \n\tcount = hns_roce_mtr_find(hr_dev, &hr_qp->mtr, hr_qp->rq.offset, mtts,\n\t\t\t\t  MTT_MIN_COUNT, &wqe_sge_ba);\n\tif (hr_qp->rq.wqe_cnt && count < 1) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to find RQ WQE, QPN = 0x%lx.\\n\", hr_qp->qpn);\n\t\treturn -EINVAL;\n\t}\n\n\tcontext->wqe_sge_ba = cpu_to_le32(wqe_sge_ba >> 3);\n\tqpc_mask->wqe_sge_ba = 0;\n\n\t \n\thr_reg_write(context, QPC_WQE_SGE_BA_H, wqe_sge_ba >> (32 + 3));\n\thr_reg_clear(qpc_mask, QPC_WQE_SGE_BA_H);\n\n\thr_reg_write(context, QPC_SQ_HOP_NUM,\n\t\t     to_hr_hem_hopnum(hr_dev->caps.wqe_sq_hop_num,\n\t\t\t\t      hr_qp->sq.wqe_cnt));\n\thr_reg_clear(qpc_mask, QPC_SQ_HOP_NUM);\n\n\thr_reg_write(context, QPC_SGE_HOP_NUM,\n\t\t     to_hr_hem_hopnum(hr_dev->caps.wqe_sge_hop_num,\n\t\t\t\t      hr_qp->sge.sge_cnt));\n\thr_reg_clear(qpc_mask, QPC_SGE_HOP_NUM);\n\n\thr_reg_write(context, QPC_RQ_HOP_NUM,\n\t\t     to_hr_hem_hopnum(hr_dev->caps.wqe_rq_hop_num,\n\t\t\t\t      hr_qp->rq.wqe_cnt));\n\n\thr_reg_clear(qpc_mask, QPC_RQ_HOP_NUM);\n\n\thr_reg_write(context, QPC_WQE_SGE_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(hr_qp->mtr.hem_cfg.ba_pg_shift));\n\thr_reg_clear(qpc_mask, QPC_WQE_SGE_BA_PG_SZ);\n\n\thr_reg_write(context, QPC_WQE_SGE_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(hr_qp->mtr.hem_cfg.buf_pg_shift));\n\thr_reg_clear(qpc_mask, QPC_WQE_SGE_BUF_PG_SZ);\n\n\tcontext->rq_cur_blk_addr = cpu_to_le32(to_hr_hw_page_addr(mtts[0]));\n\tqpc_mask->rq_cur_blk_addr = 0;\n\n\thr_reg_write(context, QPC_RQ_CUR_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts[0])));\n\thr_reg_clear(qpc_mask, QPC_RQ_CUR_BLK_ADDR_H);\n\n\tcontext->rq_nxt_blk_addr = cpu_to_le32(to_hr_hw_page_addr(mtts[1]));\n\tqpc_mask->rq_nxt_blk_addr = 0;\n\n\thr_reg_write(context, QPC_RQ_NXT_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts[1])));\n\thr_reg_clear(qpc_mask, QPC_RQ_NXT_BLK_ADDR_H);\n\n\treturn 0;\n}\n\nstatic int config_qp_sq_buf(struct hns_roce_dev *hr_dev,\n\t\t\t    struct hns_roce_qp *hr_qp,\n\t\t\t    struct hns_roce_v2_qp_context *context,\n\t\t\t    struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tu64 sge_cur_blk = 0;\n\tu64 sq_cur_blk = 0;\n\tint count;\n\n\t \n\tcount = hns_roce_mtr_find(hr_dev, &hr_qp->mtr, 0, &sq_cur_blk, 1, NULL);\n\tif (count < 1) {\n\t\tibdev_err(ibdev, \"failed to find QP(0x%lx) SQ buf.\\n\",\n\t\t\t  hr_qp->qpn);\n\t\treturn -EINVAL;\n\t}\n\tif (hr_qp->sge.sge_cnt > 0) {\n\t\tcount = hns_roce_mtr_find(hr_dev, &hr_qp->mtr,\n\t\t\t\t\t  hr_qp->sge.offset,\n\t\t\t\t\t  &sge_cur_blk, 1, NULL);\n\t\tif (count < 1) {\n\t\t\tibdev_err(ibdev, \"failed to find QP(0x%lx) SGE buf.\\n\",\n\t\t\t\t  hr_qp->qpn);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\thr_reg_write(context, QPC_SQ_CUR_BLK_ADDR_L,\n\t\t     lower_32_bits(to_hr_hw_page_addr(sq_cur_blk)));\n\thr_reg_write(context, QPC_SQ_CUR_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(sq_cur_blk)));\n\thr_reg_clear(qpc_mask, QPC_SQ_CUR_BLK_ADDR_L);\n\thr_reg_clear(qpc_mask, QPC_SQ_CUR_BLK_ADDR_H);\n\n\thr_reg_write(context, QPC_SQ_CUR_SGE_BLK_ADDR_L,\n\t\t     lower_32_bits(to_hr_hw_page_addr(sge_cur_blk)));\n\thr_reg_write(context, QPC_SQ_CUR_SGE_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(sge_cur_blk)));\n\thr_reg_clear(qpc_mask, QPC_SQ_CUR_SGE_BLK_ADDR_L);\n\thr_reg_clear(qpc_mask, QPC_SQ_CUR_SGE_BLK_ADDR_H);\n\n\thr_reg_write(context, QPC_RX_SQ_CUR_BLK_ADDR_L,\n\t\t     lower_32_bits(to_hr_hw_page_addr(sq_cur_blk)));\n\thr_reg_write(context, QPC_RX_SQ_CUR_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(sq_cur_blk)));\n\thr_reg_clear(qpc_mask, QPC_RX_SQ_CUR_BLK_ADDR_L);\n\thr_reg_clear(qpc_mask, QPC_RX_SQ_CUR_BLK_ADDR_H);\n\n\treturn 0;\n}\n\nstatic inline enum ib_mtu get_mtu(struct ib_qp *ibqp,\n\t\t\t\t  const struct ib_qp_attr *attr)\n{\n\tif (ibqp->qp_type == IB_QPT_GSI || ibqp->qp_type == IB_QPT_UD)\n\t\treturn IB_MTU_4096;\n\n\treturn attr->path_mtu;\n}\n\nstatic int modify_qp_init_to_rtr(struct ib_qp *ibqp,\n\t\t\t\t const struct ib_qp_attr *attr, int attr_mask,\n\t\t\t\t struct hns_roce_v2_qp_context *context,\n\t\t\t\t struct hns_roce_v2_qp_context *qpc_mask,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct hns_roce_ucontext *uctx = rdma_udata_to_drv_context(udata,\n\t\t\t\t\t  struct hns_roce_ucontext, ibucontext);\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tdma_addr_t trrl_ba;\n\tdma_addr_t irrl_ba;\n\tenum ib_mtu ib_mtu;\n\tconst u8 *smac;\n\tu8 lp_pktn_ini;\n\tu64 *mtts;\n\tu8 *dmac;\n\tu32 port;\n\tint mtu;\n\tint ret;\n\n\tret = config_qp_rq_buf(hr_dev, hr_qp, context, qpc_mask);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to config rq buf, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tmtts = hns_roce_table_find(hr_dev, &hr_dev->qp_table.irrl_table,\n\t\t\t\t   hr_qp->qpn, &irrl_ba);\n\tif (!mtts) {\n\t\tibdev_err(ibdev, \"failed to find qp irrl_table.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmtts = hns_roce_table_find(hr_dev, &hr_dev->qp_table.trrl_table,\n\t\t\t\t   hr_qp->qpn, &trrl_ba);\n\tif (!mtts) {\n\t\tibdev_err(ibdev, \"failed to find qp trrl_table.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_QP_ALT_PATH) {\n\t\tibdev_err(ibdev, \"INIT2RTR attr_mask (0x%x) error.\\n\",\n\t\t\t  attr_mask);\n\t\treturn -EINVAL;\n\t}\n\n\thr_reg_write(context, QPC_TRRL_BA_L, trrl_ba >> 4);\n\thr_reg_clear(qpc_mask, QPC_TRRL_BA_L);\n\tcontext->trrl_ba = cpu_to_le32(trrl_ba >> (16 + 4));\n\tqpc_mask->trrl_ba = 0;\n\thr_reg_write(context, QPC_TRRL_BA_H, trrl_ba >> (32 + 16 + 4));\n\thr_reg_clear(qpc_mask, QPC_TRRL_BA_H);\n\n\tcontext->irrl_ba = cpu_to_le32(irrl_ba >> 6);\n\tqpc_mask->irrl_ba = 0;\n\thr_reg_write(context, QPC_IRRL_BA_H, irrl_ba >> (32 + 6));\n\thr_reg_clear(qpc_mask, QPC_IRRL_BA_H);\n\n\thr_reg_enable(context, QPC_RMT_E2E);\n\thr_reg_clear(qpc_mask, QPC_RMT_E2E);\n\n\thr_reg_write(context, QPC_SIG_TYPE, hr_qp->sq_signal_bits);\n\thr_reg_clear(qpc_mask, QPC_SIG_TYPE);\n\n\tport = (attr_mask & IB_QP_PORT) ? (attr->port_num - 1) : hr_qp->port;\n\n\tsmac = (const u8 *)hr_dev->dev_addr[port];\n\tdmac = (u8 *)attr->ah_attr.roce.dmac;\n\t \n\tif (ether_addr_equal_unaligned(dmac, smac) ||\n\t    hr_dev->loop_idc == 0x1) {\n\t\thr_reg_write(context, QPC_LBI, hr_dev->loop_idc);\n\t\thr_reg_clear(qpc_mask, QPC_LBI);\n\t}\n\n\tif (attr_mask & IB_QP_DEST_QPN) {\n\t\thr_reg_write(context, QPC_DQPN, attr->dest_qp_num);\n\t\thr_reg_clear(qpc_mask, QPC_DQPN);\n\t}\n\n\tmemcpy(&context->dmac, dmac, sizeof(u32));\n\thr_reg_write(context, QPC_DMAC_H, *((u16 *)(&dmac[4])));\n\tqpc_mask->dmac = 0;\n\thr_reg_clear(qpc_mask, QPC_DMAC_H);\n\n\tib_mtu = get_mtu(ibqp, attr);\n\thr_qp->path_mtu = ib_mtu;\n\n\tmtu = ib_mtu_enum_to_int(ib_mtu);\n\tif (WARN_ON(mtu <= 0))\n\t\treturn -EINVAL;\n#define MIN_LP_MSG_LEN 1024\n\t \n\tlp_pktn_ini = ilog2(max(mtu, MIN_LP_MSG_LEN) / mtu);\n\n\tif (attr_mask & IB_QP_PATH_MTU) {\n\t\thr_reg_write(context, QPC_MTU, ib_mtu);\n\t\thr_reg_clear(qpc_mask, QPC_MTU);\n\t}\n\n\thr_reg_write(context, QPC_LP_PKTN_INI, lp_pktn_ini);\n\thr_reg_clear(qpc_mask, QPC_LP_PKTN_INI);\n\n\t \n\thr_reg_write(context, QPC_ACK_REQ_FREQ, lp_pktn_ini);\n\thr_reg_clear(qpc_mask, QPC_ACK_REQ_FREQ);\n\n\thr_reg_clear(qpc_mask, QPC_RX_REQ_PSN_ERR);\n\thr_reg_clear(qpc_mask, QPC_RX_REQ_MSN);\n\thr_reg_clear(qpc_mask, QPC_RX_REQ_LAST_OPTYPE);\n\n\tcontext->rq_rnr_timer = 0;\n\tqpc_mask->rq_rnr_timer = 0;\n\n\thr_reg_clear(qpc_mask, QPC_TRRL_HEAD_MAX);\n\thr_reg_clear(qpc_mask, QPC_TRRL_TAIL_MAX);\n\n\t \n\thr_reg_write(context, QPC_LP_SGEN_INI, 3);\n\thr_reg_clear(qpc_mask, QPC_LP_SGEN_INI);\n\n\tif (udata && ibqp->qp_type == IB_QPT_RC &&\n\t    (uctx->config & HNS_ROCE_RQ_INLINE_FLAGS)) {\n\t\thr_reg_write_bool(context, QPC_RQIE,\n\t\t\t\t  hr_dev->caps.flags &\n\t\t\t\t  HNS_ROCE_CAP_FLAG_RQ_INLINE);\n\t\thr_reg_clear(qpc_mask, QPC_RQIE);\n\t}\n\n\tif (udata &&\n\t    (ibqp->qp_type == IB_QPT_RC || ibqp->qp_type == IB_QPT_XRC_TGT) &&\n\t    (uctx->config & HNS_ROCE_CQE_INLINE_FLAGS)) {\n\t\thr_reg_write_bool(context, QPC_CQEIE,\n\t\t\t\t  hr_dev->caps.flags &\n\t\t\t\t  HNS_ROCE_CAP_FLAG_CQE_INLINE);\n\t\thr_reg_clear(qpc_mask, QPC_CQEIE);\n\n\t\thr_reg_write(context, QPC_CQEIS, 0);\n\t\thr_reg_clear(qpc_mask, QPC_CQEIS);\n\t}\n\n\treturn 0;\n}\n\nstatic int modify_qp_rtr_to_rts(struct ib_qp *ibqp,\n\t\t\t\tconst struct ib_qp_attr *attr, int attr_mask,\n\t\t\t\tstruct hns_roce_v2_qp_context *context,\n\t\t\t\tstruct hns_roce_v2_qp_context *qpc_mask)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\t \n\tif (attr_mask & (IB_QP_ALT_PATH | IB_QP_PATH_MIG_STATE)) {\n\t\tibdev_err(ibdev, \"RTR2RTS attr_mask (0x%x)error\\n\", attr_mask);\n\t\treturn -EINVAL;\n\t}\n\n\tret = config_qp_sq_buf(hr_dev, hr_qp, context, qpc_mask);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to config sq buf, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\thr_reg_clear(qpc_mask, QPC_IRRL_SGE_IDX);\n\n\thr_reg_clear(qpc_mask, QPC_RX_ACK_MSN);\n\n\thr_reg_clear(qpc_mask, QPC_ACK_LAST_OPTYPE);\n\thr_reg_clear(qpc_mask, QPC_IRRL_PSN_VLD);\n\thr_reg_clear(qpc_mask, QPC_IRRL_PSN);\n\n\thr_reg_clear(qpc_mask, QPC_IRRL_TAIL_REAL);\n\n\thr_reg_clear(qpc_mask, QPC_RETRY_MSG_MSN);\n\n\thr_reg_clear(qpc_mask, QPC_RNR_RETRY_FLAG);\n\n\thr_reg_clear(qpc_mask, QPC_CHECK_FLG);\n\n\thr_reg_clear(qpc_mask, QPC_V2_IRRL_HEAD);\n\n\treturn 0;\n}\n\nstatic int get_dip_ctx_idx(struct ib_qp *ibqp, const struct ib_qp_attr *attr,\n\t\t\t   u32 *dip_idx)\n{\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(&attr->ah_attr);\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tu32 *spare_idx = hr_dev->qp_table.idx_table.spare_idx;\n\tu32 *head =  &hr_dev->qp_table.idx_table.head;\n\tu32 *tail =  &hr_dev->qp_table.idx_table.tail;\n\tstruct hns_roce_dip *hr_dip;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&hr_dev->dip_list_lock, flags);\n\n\tspare_idx[*tail] = ibqp->qp_num;\n\t*tail = (*tail == hr_dev->caps.num_qps - 1) ? 0 : (*tail + 1);\n\n\tlist_for_each_entry(hr_dip, &hr_dev->dip_list, node) {\n\t\tif (!memcmp(grh->dgid.raw, hr_dip->dgid, 16)) {\n\t\t\t*dip_idx = hr_dip->dip_idx;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\thr_dip = kzalloc(sizeof(*hr_dip), GFP_ATOMIC);\n\tif (!hr_dip) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmemcpy(hr_dip->dgid, grh->dgid.raw, sizeof(grh->dgid.raw));\n\thr_dip->dip_idx = *dip_idx = spare_idx[*head];\n\t*head = (*head == hr_dev->caps.num_qps - 1) ? 0 : (*head + 1);\n\tlist_add_tail(&hr_dip->node, &hr_dev->dip_list);\n\nout:\n\tspin_unlock_irqrestore(&hr_dev->dip_list_lock, flags);\n\treturn ret;\n}\n\nenum {\n\tCONG_DCQCN,\n\tCONG_WINDOW,\n};\n\nenum {\n\tUNSUPPORT_CONG_LEVEL,\n\tSUPPORT_CONG_LEVEL,\n};\n\nenum {\n\tCONG_LDCP,\n\tCONG_HC3,\n};\n\nenum {\n\tDIP_INVALID,\n\tDIP_VALID,\n};\n\nenum {\n\tWND_LIMIT,\n\tWND_UNLIMIT,\n};\n\nstatic int check_cong_type(struct ib_qp *ibqp,\n\t\t\t   struct hns_roce_congestion_algorithm *cong_alg)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\n\tif (ibqp->qp_type == IB_QPT_UD)\n\t\thr_dev->caps.cong_type = CONG_TYPE_DCQCN;\n\n\t \n\tswitch (hr_dev->caps.cong_type) {\n\tcase CONG_TYPE_DCQCN:\n\t\tcong_alg->alg_sel = CONG_DCQCN;\n\t\tcong_alg->alg_sub_sel = UNSUPPORT_CONG_LEVEL;\n\t\tcong_alg->dip_vld = DIP_INVALID;\n\t\tcong_alg->wnd_mode_sel = WND_LIMIT;\n\t\tbreak;\n\tcase CONG_TYPE_LDCP:\n\t\tcong_alg->alg_sel = CONG_WINDOW;\n\t\tcong_alg->alg_sub_sel = CONG_LDCP;\n\t\tcong_alg->dip_vld = DIP_INVALID;\n\t\tcong_alg->wnd_mode_sel = WND_UNLIMIT;\n\t\tbreak;\n\tcase CONG_TYPE_HC3:\n\t\tcong_alg->alg_sel = CONG_WINDOW;\n\t\tcong_alg->alg_sub_sel = CONG_HC3;\n\t\tcong_alg->dip_vld = DIP_INVALID;\n\t\tcong_alg->wnd_mode_sel = WND_LIMIT;\n\t\tbreak;\n\tcase CONG_TYPE_DIP:\n\t\tcong_alg->alg_sel = CONG_DCQCN;\n\t\tcong_alg->alg_sub_sel = UNSUPPORT_CONG_LEVEL;\n\t\tcong_alg->dip_vld = DIP_VALID;\n\t\tcong_alg->wnd_mode_sel = WND_LIMIT;\n\t\tbreak;\n\tdefault:\n\t\tibdev_warn(&hr_dev->ib_dev,\n\t\t\t   \"invalid type(%u) for congestion selection.\\n\",\n\t\t\t   hr_dev->caps.cong_type);\n\t\thr_dev->caps.cong_type = CONG_TYPE_DCQCN;\n\t\tcong_alg->alg_sel = CONG_DCQCN;\n\t\tcong_alg->alg_sub_sel = UNSUPPORT_CONG_LEVEL;\n\t\tcong_alg->dip_vld = DIP_INVALID;\n\t\tcong_alg->wnd_mode_sel = WND_LIMIT;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int fill_cong_field(struct ib_qp *ibqp, const struct ib_qp_attr *attr,\n\t\t\t   struct hns_roce_v2_qp_context *context,\n\t\t\t   struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(&attr->ah_attr);\n\tstruct hns_roce_congestion_algorithm cong_field;\n\tstruct ib_device *ibdev = ibqp->device;\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibdev);\n\tu32 dip_idx = 0;\n\tint ret;\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08 ||\n\t    grh->sgid_attr->gid_type == IB_GID_TYPE_ROCE)\n\t\treturn 0;\n\n\tret = check_cong_type(ibqp, &cong_field);\n\tif (ret)\n\t\treturn ret;\n\n\thr_reg_write(context, QPC_CONG_ALGO_TMPL_ID, hr_dev->cong_algo_tmpl_id +\n\t\t     hr_dev->caps.cong_type * HNS_ROCE_CONG_SIZE);\n\thr_reg_clear(qpc_mask, QPC_CONG_ALGO_TMPL_ID);\n\thr_reg_write(&context->ext, QPCEX_CONG_ALG_SEL, cong_field.alg_sel);\n\thr_reg_clear(&qpc_mask->ext, QPCEX_CONG_ALG_SEL);\n\thr_reg_write(&context->ext, QPCEX_CONG_ALG_SUB_SEL,\n\t\t     cong_field.alg_sub_sel);\n\thr_reg_clear(&qpc_mask->ext, QPCEX_CONG_ALG_SUB_SEL);\n\thr_reg_write(&context->ext, QPCEX_DIP_CTX_IDX_VLD, cong_field.dip_vld);\n\thr_reg_clear(&qpc_mask->ext, QPCEX_DIP_CTX_IDX_VLD);\n\thr_reg_write(&context->ext, QPCEX_SQ_RQ_NOT_FORBID_EN,\n\t\t     cong_field.wnd_mode_sel);\n\thr_reg_clear(&qpc_mask->ext, QPCEX_SQ_RQ_NOT_FORBID_EN);\n\n\t \n\tif (cong_field.dip_vld == 0)\n\t\treturn 0;\n\n\tret = get_dip_ctx_idx(ibqp, attr, &dip_idx);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to fill cong field, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\thr_reg_write(&context->ext, QPCEX_DIP_CTX_IDX, dip_idx);\n\thr_reg_write(&qpc_mask->ext, QPCEX_DIP_CTX_IDX, 0);\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_set_path(struct ib_qp *ibqp,\n\t\t\t\tconst struct ib_qp_attr *attr,\n\t\t\t\tint attr_mask,\n\t\t\t\tstruct hns_roce_v2_qp_context *context,\n\t\t\t\tstruct hns_roce_v2_qp_context *qpc_mask)\n{\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(&attr->ah_attr);\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tconst struct ib_gid_attr *gid_attr = NULL;\n\tu8 sl = rdma_ah_get_sl(&attr->ah_attr);\n\tint is_roce_protocol;\n\tu16 vlan_id = 0xffff;\n\tbool is_udp = false;\n\tu32 max_sl;\n\tu8 ib_port;\n\tu8 hr_port;\n\tint ret;\n\n\tmax_sl = min_t(u32, MAX_SERVICE_LEVEL, hr_dev->caps.sl_num - 1);\n\tif (unlikely(sl > max_sl)) {\n\t\tibdev_err_ratelimited(ibdev,\n\t\t\t\t      \"failed to fill QPC, sl (%u) shouldn't be larger than %u.\\n\",\n\t\t\t\t      sl, max_sl);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (hr_qp->free_mr_en) {\n\t\thr_reg_write(context, QPC_SL, sl);\n\t\thr_reg_clear(qpc_mask, QPC_SL);\n\t\thr_qp->sl = sl;\n\t\treturn 0;\n\t}\n\n\tib_port = (attr_mask & IB_QP_PORT) ? attr->port_num : hr_qp->port + 1;\n\thr_port = ib_port - 1;\n\tis_roce_protocol = rdma_cap_eth_ah(&hr_dev->ib_dev, ib_port) &&\n\t\t\t   rdma_ah_get_ah_flags(&attr->ah_attr) & IB_AH_GRH;\n\n\tif (is_roce_protocol) {\n\t\tgid_attr = attr->ah_attr.grh.sgid_attr;\n\t\tret = rdma_read_gid_l2_fields(gid_attr, &vlan_id, NULL);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tis_udp = (gid_attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP);\n\t}\n\n\t \n\tif (vlan_id < VLAN_N_VID &&\n\t    hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\thr_reg_enable(context, QPC_RQ_VLAN_EN);\n\t\thr_reg_clear(qpc_mask, QPC_RQ_VLAN_EN);\n\t\thr_reg_enable(context, QPC_SQ_VLAN_EN);\n\t\thr_reg_clear(qpc_mask, QPC_SQ_VLAN_EN);\n\t}\n\n\thr_reg_write(context, QPC_VLAN_ID, vlan_id);\n\thr_reg_clear(qpc_mask, QPC_VLAN_ID);\n\n\tif (grh->sgid_index >= hr_dev->caps.gid_table_len[hr_port]) {\n\t\tibdev_err(ibdev, \"sgid_index(%u) too large. max is %d\\n\",\n\t\t\t  grh->sgid_index, hr_dev->caps.gid_table_len[hr_port]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr->ah_attr.type != RDMA_AH_ATTR_TYPE_ROCE) {\n\t\tibdev_err(ibdev, \"ah attr is not RDMA roce type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\thr_reg_write(context, QPC_UDPSPN,\n\t\t     is_udp ? rdma_get_udp_sport(grh->flow_label, ibqp->qp_num,\n\t\t\t\t\t\t attr->dest_qp_num) :\n\t\t\t\t    0);\n\n\thr_reg_clear(qpc_mask, QPC_UDPSPN);\n\n\thr_reg_write(context, QPC_GMV_IDX, grh->sgid_index);\n\n\thr_reg_clear(qpc_mask, QPC_GMV_IDX);\n\n\thr_reg_write(context, QPC_HOPLIMIT, grh->hop_limit);\n\thr_reg_clear(qpc_mask, QPC_HOPLIMIT);\n\n\tret = fill_cong_field(ibqp, attr, context, qpc_mask);\n\tif (ret)\n\t\treturn ret;\n\n\thr_reg_write(context, QPC_TC, get_tclass(&attr->ah_attr.grh));\n\thr_reg_clear(qpc_mask, QPC_TC);\n\n\thr_reg_write(context, QPC_FL, grh->flow_label);\n\thr_reg_clear(qpc_mask, QPC_FL);\n\tmemcpy(context->dgid, grh->dgid.raw, sizeof(grh->dgid.raw));\n\tmemset(qpc_mask->dgid, 0, sizeof(grh->dgid.raw));\n\n\thr_qp->sl = sl;\n\thr_reg_write(context, QPC_SL, hr_qp->sl);\n\thr_reg_clear(qpc_mask, QPC_SL);\n\n\treturn 0;\n}\n\nstatic bool check_qp_state(enum ib_qp_state cur_state,\n\t\t\t   enum ib_qp_state new_state)\n{\n\tstatic const bool sm[][IB_QPS_ERR + 1] = {\n\t\t[IB_QPS_RESET] = { [IB_QPS_RESET] = true,\n\t\t\t\t   [IB_QPS_INIT] = true },\n\t\t[IB_QPS_INIT] = { [IB_QPS_RESET] = true,\n\t\t\t\t  [IB_QPS_INIT] = true,\n\t\t\t\t  [IB_QPS_RTR] = true,\n\t\t\t\t  [IB_QPS_ERR] = true },\n\t\t[IB_QPS_RTR] = { [IB_QPS_RESET] = true,\n\t\t\t\t [IB_QPS_RTS] = true,\n\t\t\t\t [IB_QPS_ERR] = true },\n\t\t[IB_QPS_RTS] = { [IB_QPS_RESET] = true,\n\t\t\t\t [IB_QPS_RTS] = true,\n\t\t\t\t [IB_QPS_ERR] = true },\n\t\t[IB_QPS_SQD] = {},\n\t\t[IB_QPS_SQE] = {},\n\t\t[IB_QPS_ERR] = { [IB_QPS_RESET] = true,\n\t\t\t\t [IB_QPS_ERR] = true }\n\t};\n\n\treturn sm[cur_state][new_state];\n}\n\nstatic int hns_roce_v2_set_abs_fields(struct ib_qp *ibqp,\n\t\t\t\t      const struct ib_qp_attr *attr,\n\t\t\t\t      int attr_mask,\n\t\t\t\t      enum ib_qp_state cur_state,\n\t\t\t\t      enum ib_qp_state new_state,\n\t\t\t\t      struct hns_roce_v2_qp_context *context,\n\t\t\t\t      struct hns_roce_v2_qp_context *qpc_mask,\n\t\t\t\t      struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tint ret = 0;\n\n\tif (!check_qp_state(cur_state, new_state)) {\n\t\tibdev_err(&hr_dev->ib_dev, \"Illegal state for QP!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) {\n\t\tmemset(qpc_mask, 0, hr_dev->caps.qpc_sz);\n\t\tmodify_qp_reset_to_init(ibqp, attr, context, qpc_mask);\n\t} else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_INIT) {\n\t\tmodify_qp_init_to_init(ibqp, attr, context, qpc_mask);\n\t} else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_RTR) {\n\t\tret = modify_qp_init_to_rtr(ibqp, attr, attr_mask, context,\n\t\t\t\t\t    qpc_mask, udata);\n\t} else if (cur_state == IB_QPS_RTR && new_state == IB_QPS_RTS) {\n\t\tret = modify_qp_rtr_to_rts(ibqp, attr, attr_mask, context,\n\t\t\t\t\t   qpc_mask);\n\t}\n\n\treturn ret;\n}\n\nstatic bool check_qp_timeout_cfg_range(struct hns_roce_dev *hr_dev, u8 *timeout)\n{\n#define QP_ACK_TIMEOUT_MAX_HIP08 20\n#define QP_ACK_TIMEOUT_MAX 31\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\tif (*timeout > QP_ACK_TIMEOUT_MAX_HIP08) {\n\t\t\tibdev_warn(&hr_dev->ib_dev,\n\t\t\t\t   \"local ACK timeout shall be 0 to 20.\\n\");\n\t\t\treturn false;\n\t\t}\n\t\t*timeout += HNS_ROCE_V2_QP_ACK_TIMEOUT_OFS_HIP08;\n\t} else if (hr_dev->pci_dev->revision > PCI_REVISION_ID_HIP08) {\n\t\tif (*timeout > QP_ACK_TIMEOUT_MAX) {\n\t\t\tibdev_warn(&hr_dev->ib_dev,\n\t\t\t\t   \"local ACK timeout shall be 0 to 31.\\n\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int hns_roce_v2_set_opt_fields(struct ib_qp *ibqp,\n\t\t\t\t      const struct ib_qp_attr *attr,\n\t\t\t\t      int attr_mask,\n\t\t\t\t      struct hns_roce_v2_qp_context *context,\n\t\t\t\t      struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tint ret = 0;\n\tu8 timeout;\n\n\tif (attr_mask & IB_QP_AV) {\n\t\tret = hns_roce_v2_set_path(ibqp, attr, attr_mask, context,\n\t\t\t\t\t   qpc_mask);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (attr_mask & IB_QP_TIMEOUT) {\n\t\ttimeout = attr->timeout;\n\t\tif (check_qp_timeout_cfg_range(hr_dev, &timeout)) {\n\t\t\thr_reg_write(context, QPC_AT, timeout);\n\t\t\thr_reg_clear(qpc_mask, QPC_AT);\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_RETRY_CNT) {\n\t\thr_reg_write(context, QPC_RETRY_NUM_INIT, attr->retry_cnt);\n\t\thr_reg_clear(qpc_mask, QPC_RETRY_NUM_INIT);\n\n\t\thr_reg_write(context, QPC_RETRY_CNT, attr->retry_cnt);\n\t\thr_reg_clear(qpc_mask, QPC_RETRY_CNT);\n\t}\n\n\tif (attr_mask & IB_QP_RNR_RETRY) {\n\t\thr_reg_write(context, QPC_RNR_NUM_INIT, attr->rnr_retry);\n\t\thr_reg_clear(qpc_mask, QPC_RNR_NUM_INIT);\n\n\t\thr_reg_write(context, QPC_RNR_CNT, attr->rnr_retry);\n\t\thr_reg_clear(qpc_mask, QPC_RNR_CNT);\n\t}\n\n\tif (attr_mask & IB_QP_SQ_PSN) {\n\t\thr_reg_write(context, QPC_SQ_CUR_PSN, attr->sq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_SQ_CUR_PSN);\n\n\t\thr_reg_write(context, QPC_SQ_MAX_PSN, attr->sq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_SQ_MAX_PSN);\n\n\t\thr_reg_write(context, QPC_RETRY_MSG_PSN_L, attr->sq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_RETRY_MSG_PSN_L);\n\n\t\thr_reg_write(context, QPC_RETRY_MSG_PSN_H,\n\t\t\t     attr->sq_psn >> RETRY_MSG_PSN_SHIFT);\n\t\thr_reg_clear(qpc_mask, QPC_RETRY_MSG_PSN_H);\n\n\t\thr_reg_write(context, QPC_RETRY_MSG_FPKT_PSN, attr->sq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_RETRY_MSG_FPKT_PSN);\n\n\t\thr_reg_write(context, QPC_RX_ACK_EPSN, attr->sq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_RX_ACK_EPSN);\n\t}\n\n\tif ((attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) &&\n\t     attr->max_dest_rd_atomic) {\n\t\thr_reg_write(context, QPC_RR_MAX,\n\t\t\t     fls(attr->max_dest_rd_atomic - 1));\n\t\thr_reg_clear(qpc_mask, QPC_RR_MAX);\n\t}\n\n\tif ((attr_mask & IB_QP_MAX_QP_RD_ATOMIC) && attr->max_rd_atomic) {\n\t\thr_reg_write(context, QPC_SR_MAX, fls(attr->max_rd_atomic - 1));\n\t\thr_reg_clear(qpc_mask, QPC_SR_MAX);\n\t}\n\n\tif (attr_mask & (IB_QP_ACCESS_FLAGS | IB_QP_MAX_DEST_RD_ATOMIC))\n\t\tset_access_flags(hr_qp, context, qpc_mask, attr, attr_mask);\n\n\tif (attr_mask & IB_QP_MIN_RNR_TIMER) {\n\t\thr_reg_write(context, QPC_MIN_RNR_TIME,\n\t\t\t    hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08 ?\n\t\t\t    HNS_ROCE_RNR_TIMER_10NS : attr->min_rnr_timer);\n\t\thr_reg_clear(qpc_mask, QPC_MIN_RNR_TIME);\n\t}\n\n\tif (attr_mask & IB_QP_RQ_PSN) {\n\t\thr_reg_write(context, QPC_RX_REQ_EPSN, attr->rq_psn);\n\t\thr_reg_clear(qpc_mask, QPC_RX_REQ_EPSN);\n\n\t\thr_reg_write(context, QPC_RAQ_PSN, attr->rq_psn - 1);\n\t\thr_reg_clear(qpc_mask, QPC_RAQ_PSN);\n\t}\n\n\tif (attr_mask & IB_QP_QKEY) {\n\t\tcontext->qkey_xrcd = cpu_to_le32(attr->qkey);\n\t\tqpc_mask->qkey_xrcd = 0;\n\t\thr_qp->qkey = attr->qkey;\n\t}\n\n\treturn ret;\n}\n\nstatic void hns_roce_v2_record_opt_fields(struct ib_qp *ibqp,\n\t\t\t\t\t  const struct ib_qp_attr *attr,\n\t\t\t\t\t  int attr_mask)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\n\tif (attr_mask & IB_QP_ACCESS_FLAGS)\n\t\thr_qp->atomic_rd_en = attr->qp_access_flags;\n\n\tif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\n\t\thr_qp->resp_depth = attr->max_dest_rd_atomic;\n\tif (attr_mask & IB_QP_PORT) {\n\t\thr_qp->port = attr->port_num - 1;\n\t\thr_qp->phy_port = hr_dev->iboe.phy_port[hr_qp->port];\n\t}\n}\n\nstatic void clear_qp(struct hns_roce_qp *hr_qp)\n{\n\tstruct ib_qp *ibqp = &hr_qp->ibqp;\n\n\tif (ibqp->send_cq)\n\t\thns_roce_v2_cq_clean(to_hr_cq(ibqp->send_cq),\n\t\t\t\t     hr_qp->qpn, NULL);\n\n\tif (ibqp->recv_cq  && ibqp->recv_cq != ibqp->send_cq)\n\t\thns_roce_v2_cq_clean(to_hr_cq(ibqp->recv_cq),\n\t\t\t\t     hr_qp->qpn, ibqp->srq ?\n\t\t\t\t     to_hr_srq(ibqp->srq) : NULL);\n\n\tif (hr_qp->en_flags & HNS_ROCE_QP_CAP_RQ_RECORD_DB)\n\t\t*hr_qp->rdb.db_record = 0;\n\n\thr_qp->rq.head = 0;\n\thr_qp->rq.tail = 0;\n\thr_qp->sq.head = 0;\n\thr_qp->sq.tail = 0;\n\thr_qp->next_sge = 0;\n}\n\nstatic void v2_set_flushed_fields(struct ib_qp *ibqp,\n\t\t\t\t  struct hns_roce_v2_qp_context *context,\n\t\t\t\t  struct hns_roce_v2_qp_context *qpc_mask)\n{\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tunsigned long sq_flag = 0;\n\tunsigned long rq_flag = 0;\n\n\tif (ibqp->qp_type == IB_QPT_XRC_TGT)\n\t\treturn;\n\n\tspin_lock_irqsave(&hr_qp->sq.lock, sq_flag);\n\thr_reg_write(context, QPC_SQ_PRODUCER_IDX, hr_qp->sq.head);\n\thr_reg_clear(qpc_mask, QPC_SQ_PRODUCER_IDX);\n\thr_qp->state = IB_QPS_ERR;\n\tspin_unlock_irqrestore(&hr_qp->sq.lock, sq_flag);\n\n\tif (ibqp->srq || ibqp->qp_type == IB_QPT_XRC_INI)  \n\t\treturn;\n\n\tspin_lock_irqsave(&hr_qp->rq.lock, rq_flag);\n\thr_reg_write(context, QPC_RQ_PRODUCER_IDX, hr_qp->rq.head);\n\thr_reg_clear(qpc_mask, QPC_RQ_PRODUCER_IDX);\n\tspin_unlock_irqrestore(&hr_qp->rq.lock, rq_flag);\n}\n\nstatic int hns_roce_v2_modify_qp(struct ib_qp *ibqp,\n\t\t\t\t const struct ib_qp_attr *attr,\n\t\t\t\t int attr_mask, enum ib_qp_state cur_state,\n\t\t\t\t enum ib_qp_state new_state, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct hns_roce_v2_qp_context ctx[2];\n\tstruct hns_roce_v2_qp_context *context = ctx;\n\tstruct hns_roce_v2_qp_context *qpc_mask = ctx + 1;\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint ret;\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tmemset(context, 0, hr_dev->caps.qpc_sz);\n\tmemset(qpc_mask, 0xff, hr_dev->caps.qpc_sz);\n\n\tret = hns_roce_v2_set_abs_fields(ibqp, attr, attr_mask, cur_state,\n\t\t\t\t\t new_state, context, qpc_mask, udata);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (new_state == IB_QPS_ERR)\n\t\tv2_set_flushed_fields(ibqp, context, qpc_mask);\n\n\t \n\tret = hns_roce_v2_set_opt_fields(ibqp, attr, attr_mask, context,\n\t\t\t\t\t qpc_mask);\n\tif (ret)\n\t\tgoto out;\n\n\thr_reg_write_bool(context, QPC_INV_CREDIT,\n\t\t\t  to_hr_qp_type(hr_qp->ibqp.qp_type) == SERV_TYPE_XRC ||\n\t\t\t  ibqp->srq);\n\thr_reg_clear(qpc_mask, QPC_INV_CREDIT);\n\n\t \n\thr_reg_write(context, QPC_QP_ST, new_state);\n\thr_reg_clear(qpc_mask, QPC_QP_ST);\n\n\t \n\tret = hns_roce_v2_qp_modify(hr_dev, context, qpc_mask, hr_qp);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to modify QP, ret = %d.\\n\", ret);\n\t\tgoto out;\n\t}\n\n\thr_qp->state = new_state;\n\n\thns_roce_v2_record_opt_fields(ibqp, attr, attr_mask);\n\n\tif (new_state == IB_QPS_RESET && !ibqp->uobject)\n\t\tclear_qp(hr_qp);\n\nout:\n\treturn ret;\n}\n\nstatic int to_ib_qp_st(enum hns_roce_v2_qp_state state)\n{\n\tstatic const enum ib_qp_state map[] = {\n\t\t[HNS_ROCE_QP_ST_RST] = IB_QPS_RESET,\n\t\t[HNS_ROCE_QP_ST_INIT] = IB_QPS_INIT,\n\t\t[HNS_ROCE_QP_ST_RTR] = IB_QPS_RTR,\n\t\t[HNS_ROCE_QP_ST_RTS] = IB_QPS_RTS,\n\t\t[HNS_ROCE_QP_ST_SQD] = IB_QPS_SQD,\n\t\t[HNS_ROCE_QP_ST_SQER] = IB_QPS_SQE,\n\t\t[HNS_ROCE_QP_ST_ERR] = IB_QPS_ERR,\n\t\t[HNS_ROCE_QP_ST_SQ_DRAINING] = IB_QPS_SQD\n\t};\n\n\treturn (state < ARRAY_SIZE(map)) ? map[state] : -1;\n}\n\nstatic int hns_roce_v2_query_qpc(struct hns_roce_dev *hr_dev, u32 qpn,\n\t\t\t\t void *buffer)\n{\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_QPC,\n\t\t\t\tqpn);\n\tif (ret)\n\t\tgoto out;\n\n\tmemcpy(buffer, mailbox->buf, hr_dev->caps.qpc_sz);\n\nout:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\treturn ret;\n}\n\nstatic u8 get_qp_timeout_attr(struct hns_roce_dev *hr_dev,\n\t\t\t      struct hns_roce_v2_qp_context *context)\n{\n\tu8 timeout;\n\n\ttimeout = (u8)hr_reg_read(context, QPC_AT);\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)\n\t\ttimeout -= HNS_ROCE_V2_QP_ACK_TIMEOUT_OFS_HIP08;\n\n\treturn timeout;\n}\n\nstatic int hns_roce_v2_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr,\n\t\t\t\tint qp_attr_mask,\n\t\t\t\tstruct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tstruct hns_roce_v2_qp_context context = {};\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tint tmp_qp_state;\n\tint state;\n\tint ret;\n\n\tmemset(qp_attr, 0, sizeof(*qp_attr));\n\tmemset(qp_init_attr, 0, sizeof(*qp_init_attr));\n\n\tmutex_lock(&hr_qp->mutex);\n\n\tif (hr_qp->state == IB_QPS_RESET) {\n\t\tqp_attr->qp_state = IB_QPS_RESET;\n\t\tret = 0;\n\t\tgoto done;\n\t}\n\n\tret = hns_roce_v2_query_qpc(hr_dev, hr_qp->qpn, &context);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to query QPC, ret = %d.\\n\", ret);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tstate = hr_reg_read(&context, QPC_QP_ST);\n\ttmp_qp_state = to_ib_qp_st((enum hns_roce_v2_qp_state)state);\n\tif (tmp_qp_state == -1) {\n\t\tibdev_err(ibdev, \"Illegal ib_qp_state\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\thr_qp->state = (u8)tmp_qp_state;\n\tqp_attr->qp_state = (enum ib_qp_state)hr_qp->state;\n\tqp_attr->path_mtu = (enum ib_mtu)hr_reg_read(&context, QPC_MTU);\n\tqp_attr->path_mig_state = IB_MIG_ARMED;\n\tqp_attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\tif (hr_qp->ibqp.qp_type == IB_QPT_UD)\n\t\tqp_attr->qkey = le32_to_cpu(context.qkey_xrcd);\n\n\tqp_attr->rq_psn = hr_reg_read(&context, QPC_RX_REQ_EPSN);\n\tqp_attr->sq_psn = (u32)hr_reg_read(&context, QPC_SQ_CUR_PSN);\n\tqp_attr->dest_qp_num = hr_reg_read(&context, QPC_DQPN);\n\tqp_attr->qp_access_flags =\n\t\t((hr_reg_read(&context, QPC_RRE)) << V2_QP_RRE_S) |\n\t\t((hr_reg_read(&context, QPC_RWE)) << V2_QP_RWE_S) |\n\t\t((hr_reg_read(&context, QPC_ATE)) << V2_QP_ATE_S);\n\n\tif (hr_qp->ibqp.qp_type == IB_QPT_RC ||\n\t    hr_qp->ibqp.qp_type == IB_QPT_XRC_INI ||\n\t    hr_qp->ibqp.qp_type == IB_QPT_XRC_TGT) {\n\t\tstruct ib_global_route *grh =\n\t\t\trdma_ah_retrieve_grh(&qp_attr->ah_attr);\n\n\t\trdma_ah_set_sl(&qp_attr->ah_attr,\n\t\t\t       hr_reg_read(&context, QPC_SL));\n\t\trdma_ah_set_port_num(&qp_attr->ah_attr, hr_qp->port + 1);\n\t\trdma_ah_set_ah_flags(&qp_attr->ah_attr, IB_AH_GRH);\n\t\tgrh->flow_label = hr_reg_read(&context, QPC_FL);\n\t\tgrh->sgid_index = hr_reg_read(&context, QPC_GMV_IDX);\n\t\tgrh->hop_limit = hr_reg_read(&context, QPC_HOPLIMIT);\n\t\tgrh->traffic_class = hr_reg_read(&context, QPC_TC);\n\n\t\tmemcpy(grh->dgid.raw, context.dgid, sizeof(grh->dgid.raw));\n\t}\n\n\tqp_attr->port_num = hr_qp->port + 1;\n\tqp_attr->sq_draining = 0;\n\tqp_attr->max_rd_atomic = 1 << hr_reg_read(&context, QPC_SR_MAX);\n\tqp_attr->max_dest_rd_atomic = 1 << hr_reg_read(&context, QPC_RR_MAX);\n\n\tqp_attr->min_rnr_timer = (u8)hr_reg_read(&context, QPC_MIN_RNR_TIME);\n\tqp_attr->timeout = get_qp_timeout_attr(hr_dev, &context);\n\tqp_attr->retry_cnt = hr_reg_read(&context, QPC_RETRY_NUM_INIT);\n\tqp_attr->rnr_retry = hr_reg_read(&context, QPC_RNR_NUM_INIT);\n\ndone:\n\tqp_attr->cur_qp_state = qp_attr->qp_state;\n\tqp_attr->cap.max_recv_wr = hr_qp->rq.wqe_cnt;\n\tqp_attr->cap.max_recv_sge = hr_qp->rq.max_gs - hr_qp->rq.rsv_sge;\n\tqp_attr->cap.max_inline_data = hr_qp->max_inline_data;\n\n\tqp_attr->cap.max_send_wr = hr_qp->sq.wqe_cnt;\n\tqp_attr->cap.max_send_sge = hr_qp->sq.max_gs;\n\n\tqp_init_attr->qp_context = ibqp->qp_context;\n\tqp_init_attr->qp_type = ibqp->qp_type;\n\tqp_init_attr->recv_cq = ibqp->recv_cq;\n\tqp_init_attr->send_cq = ibqp->send_cq;\n\tqp_init_attr->srq = ibqp->srq;\n\tqp_init_attr->cap = qp_attr->cap;\n\tqp_init_attr->sq_sig_type = hr_qp->sq_signal_bits;\n\nout:\n\tmutex_unlock(&hr_qp->mutex);\n\treturn ret;\n}\n\nstatic inline int modify_qp_is_ok(struct hns_roce_qp *hr_qp)\n{\n\treturn ((hr_qp->ibqp.qp_type == IB_QPT_RC ||\n\t\t hr_qp->ibqp.qp_type == IB_QPT_UD ||\n\t\t hr_qp->ibqp.qp_type == IB_QPT_XRC_INI ||\n\t\t hr_qp->ibqp.qp_type == IB_QPT_XRC_TGT) &&\n\t\thr_qp->state != IB_QPS_RESET);\n}\n\nstatic int hns_roce_v2_destroy_qp_common(struct hns_roce_dev *hr_dev,\n\t\t\t\t\t struct hns_roce_qp *hr_qp,\n\t\t\t\t\t struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_cq *send_cq, *recv_cq;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (modify_qp_is_ok(hr_qp)) {\n\t\t \n\t\tret = hns_roce_v2_modify_qp(&hr_qp->ibqp, NULL, 0,\n\t\t\t\t\t    hr_qp->state, IB_QPS_RESET, udata);\n\t\tif (ret)\n\t\t\tibdev_err(ibdev,\n\t\t\t\t  \"failed to modify QP to RST, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t}\n\n\tsend_cq = hr_qp->ibqp.send_cq ? to_hr_cq(hr_qp->ibqp.send_cq) : NULL;\n\trecv_cq = hr_qp->ibqp.recv_cq ? to_hr_cq(hr_qp->ibqp.recv_cq) : NULL;\n\n\tspin_lock_irqsave(&hr_dev->qp_list_lock, flags);\n\thns_roce_lock_cqs(send_cq, recv_cq);\n\n\tif (!udata) {\n\t\tif (recv_cq)\n\t\t\t__hns_roce_v2_cq_clean(recv_cq, hr_qp->qpn,\n\t\t\t\t\t       (hr_qp->ibqp.srq ?\n\t\t\t\t\t\tto_hr_srq(hr_qp->ibqp.srq) :\n\t\t\t\t\t\tNULL));\n\n\t\tif (send_cq && send_cq != recv_cq)\n\t\t\t__hns_roce_v2_cq_clean(send_cq, hr_qp->qpn, NULL);\n\t}\n\n\thns_roce_qp_remove(hr_dev, hr_qp);\n\n\thns_roce_unlock_cqs(send_cq, recv_cq);\n\tspin_unlock_irqrestore(&hr_dev->qp_list_lock, flags);\n\n\treturn ret;\n}\n\nint hns_roce_v2_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);\n\tstruct hns_roce_qp *hr_qp = to_hr_qp(ibqp);\n\tint ret;\n\n\tret = hns_roce_v2_destroy_qp_common(hr_dev, hr_qp, udata);\n\tif (ret)\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to destroy QP, QPN = 0x%06lx, ret = %d.\\n\",\n\t\t\t  hr_qp->qpn, ret);\n\n\thns_roce_qp_destroy(hr_dev, hr_qp, udata);\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_qp_flow_control_init(struct hns_roce_dev *hr_dev,\n\t\t\t\t\t    struct hns_roce_qp *hr_qp)\n{\n\tstruct ib_device *ibdev = &hr_dev->ib_dev;\n\tstruct hns_roce_sccc_clr_done *resp;\n\tstruct hns_roce_sccc_clr *clr;\n\tstruct hns_roce_cmq_desc desc;\n\tint ret, i;\n\n\tif (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)\n\t\treturn 0;\n\n\tmutex_lock(&hr_dev->qp_table.scc_mutex);\n\n\t \n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_RESET_SCCC, false);\n\tret =  hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to reset SCC ctx, ret = %d.\\n\", ret);\n\t\tgoto out;\n\t}\n\n\t \n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CLR_SCCC, false);\n\tclr = (struct hns_roce_sccc_clr *)desc.data;\n\tclr->qpn = cpu_to_le32(hr_qp->qpn);\n\tret =  hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret) {\n\t\tibdev_err(ibdev, \"failed to clear SCC ctx, ret = %d.\\n\", ret);\n\t\tgoto out;\n\t}\n\n\t \n\tresp = (struct hns_roce_sccc_clr_done *)desc.data;\n\tfor (i = 0; i <= HNS_ROCE_CMQ_SCC_CLR_DONE_CNT; i++) {\n\t\thns_roce_cmq_setup_basic_desc(&desc,\n\t\t\t\t\t      HNS_ROCE_OPC_QUERY_SCCC, true);\n\t\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\t\tif (ret) {\n\t\t\tibdev_err(ibdev, \"failed to query clr cmq, ret = %d\\n\",\n\t\t\t\t  ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (resp->clr_done)\n\t\t\tgoto out;\n\n\t\tmsleep(20);\n\t}\n\n\tibdev_err(ibdev, \"query SCC clr done flag overtime.\\n\");\n\tret = -ETIMEDOUT;\n\nout:\n\tmutex_unlock(&hr_dev->qp_table.scc_mutex);\n\treturn ret;\n}\n\n#define DMA_IDX_SHIFT 3\n#define DMA_WQE_SHIFT 3\n\nstatic int hns_roce_v2_write_srqc_index_queue(struct hns_roce_srq *srq,\n\t\t\t\t\t      struct hns_roce_srq_context *ctx)\n{\n\tstruct hns_roce_idx_que *idx_que = &srq->idx_que;\n\tstruct ib_device *ibdev = srq->ibsrq.device;\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibdev);\n\tu64 mtts_idx[MTT_MIN_COUNT] = {};\n\tdma_addr_t dma_handle_idx = 0;\n\tint ret;\n\n\t \n\tret = hns_roce_mtr_find(hr_dev, &idx_que->mtr, 0, mtts_idx,\n\t\t\t\tARRAY_SIZE(mtts_idx), &dma_handle_idx);\n\tif (ret < 1) {\n\t\tibdev_err(ibdev, \"failed to find mtr for SRQ idx, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn -ENOBUFS;\n\t}\n\n\thr_reg_write(ctx, SRQC_IDX_HOP_NUM,\n\t\t     to_hr_hem_hopnum(hr_dev->caps.idx_hop_num, srq->wqe_cnt));\n\n\thr_reg_write(ctx, SRQC_IDX_BT_BA_L, dma_handle_idx >> DMA_IDX_SHIFT);\n\thr_reg_write(ctx, SRQC_IDX_BT_BA_H,\n\t\t     upper_32_bits(dma_handle_idx >> DMA_IDX_SHIFT));\n\n\thr_reg_write(ctx, SRQC_IDX_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(idx_que->mtr.hem_cfg.ba_pg_shift));\n\thr_reg_write(ctx, SRQC_IDX_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(idx_que->mtr.hem_cfg.buf_pg_shift));\n\n\thr_reg_write(ctx, SRQC_IDX_CUR_BLK_ADDR_L,\n\t\t     to_hr_hw_page_addr(mtts_idx[0]));\n\thr_reg_write(ctx, SRQC_IDX_CUR_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts_idx[0])));\n\n\thr_reg_write(ctx, SRQC_IDX_NXT_BLK_ADDR_L,\n\t\t     to_hr_hw_page_addr(mtts_idx[1]));\n\thr_reg_write(ctx, SRQC_IDX_NXT_BLK_ADDR_H,\n\t\t     upper_32_bits(to_hr_hw_page_addr(mtts_idx[1])));\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_write_srqc(struct hns_roce_srq *srq, void *mb_buf)\n{\n\tstruct ib_device *ibdev = srq->ibsrq.device;\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibdev);\n\tstruct hns_roce_srq_context *ctx = mb_buf;\n\tu64 mtts_wqe[MTT_MIN_COUNT] = {};\n\tdma_addr_t dma_handle_wqe = 0;\n\tint ret;\n\n\tmemset(ctx, 0, sizeof(*ctx));\n\n\t \n\tret = hns_roce_mtr_find(hr_dev, &srq->buf_mtr, 0, mtts_wqe,\n\t\t\t\tARRAY_SIZE(mtts_wqe), &dma_handle_wqe);\n\tif (ret < 1) {\n\t\tibdev_err(ibdev, \"failed to find mtr for SRQ WQE, ret = %d.\\n\",\n\t\t\t  ret);\n\t\treturn -ENOBUFS;\n\t}\n\n\thr_reg_write(ctx, SRQC_SRQ_ST, 1);\n\thr_reg_write_bool(ctx, SRQC_SRQ_TYPE,\n\t\t\t  srq->ibsrq.srq_type == IB_SRQT_XRC);\n\thr_reg_write(ctx, SRQC_PD, to_hr_pd(srq->ibsrq.pd)->pdn);\n\thr_reg_write(ctx, SRQC_SRQN, srq->srqn);\n\thr_reg_write(ctx, SRQC_XRCD, srq->xrcdn);\n\thr_reg_write(ctx, SRQC_XRC_CQN, srq->cqn);\n\thr_reg_write(ctx, SRQC_SHIFT, ilog2(srq->wqe_cnt));\n\thr_reg_write(ctx, SRQC_RQWS,\n\t\t     srq->max_gs <= 0 ? 0 : fls(srq->max_gs - 1));\n\n\thr_reg_write(ctx, SRQC_WQE_HOP_NUM,\n\t\t     to_hr_hem_hopnum(hr_dev->caps.srqwqe_hop_num,\n\t\t\t\t      srq->wqe_cnt));\n\n\thr_reg_write(ctx, SRQC_WQE_BT_BA_L, dma_handle_wqe >> DMA_WQE_SHIFT);\n\thr_reg_write(ctx, SRQC_WQE_BT_BA_H,\n\t\t     upper_32_bits(dma_handle_wqe >> DMA_WQE_SHIFT));\n\n\thr_reg_write(ctx, SRQC_WQE_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(srq->buf_mtr.hem_cfg.ba_pg_shift));\n\thr_reg_write(ctx, SRQC_WQE_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(srq->buf_mtr.hem_cfg.buf_pg_shift));\n\n\treturn hns_roce_v2_write_srqc_index_queue(srq, ctx);\n}\n\nstatic int hns_roce_v2_modify_srq(struct ib_srq *ibsrq,\n\t\t\t\t  struct ib_srq_attr *srq_attr,\n\t\t\t\t  enum ib_srq_attr_mask srq_attr_mask,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);\n\tstruct hns_roce_srq *srq = to_hr_srq(ibsrq);\n\tstruct hns_roce_srq_context *srq_context;\n\tstruct hns_roce_srq_context *srqc_mask;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\t \n\tif (srq_attr_mask & IB_SRQ_MAX_WR)\n\t\treturn -EOPNOTSUPP;\n\n\tif (srq_attr_mask & IB_SRQ_LIMIT) {\n\t\tif (srq_attr->srq_limit > srq->wqe_cnt)\n\t\t\treturn -EINVAL;\n\n\t\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\t\tif (IS_ERR(mailbox))\n\t\t\treturn PTR_ERR(mailbox);\n\n\t\tsrq_context = mailbox->buf;\n\t\tsrqc_mask = (struct hns_roce_srq_context *)mailbox->buf + 1;\n\n\t\tmemset(srqc_mask, 0xff, sizeof(*srqc_mask));\n\n\t\thr_reg_write(srq_context, SRQC_LIMIT_WL, srq_attr->srq_limit);\n\t\thr_reg_clear(srqc_mask, SRQC_LIMIT_WL);\n\n\t\tret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0,\n\t\t\t\t\tHNS_ROCE_CMD_MODIFY_SRQC, srq->srqn);\n\t\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\t\tif (ret) {\n\t\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t\t  \"failed to handle cmd of modifying SRQ, ret = %d.\\n\",\n\t\t\t\t  ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_roce_v2_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(ibsrq->device);\n\tstruct hns_roce_srq *srq = to_hr_srq(ibsrq);\n\tstruct hns_roce_srq_context *srq_context;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tsrq_context = mailbox->buf;\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma,\n\t\t\t\tHNS_ROCE_CMD_QUERY_SRQC, srq->srqn);\n\tif (ret) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to process cmd of querying SRQ, ret = %d.\\n\",\n\t\t\t  ret);\n\t\tgoto out;\n\t}\n\n\tattr->srq_limit = hr_reg_read(srq_context, SRQC_LIMIT_WL);\n\tattr->max_wr = srq->wqe_cnt;\n\tattr->max_sge = srq->max_gs - srq->rsv_sge;\n\nout:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\treturn ret;\n}\n\nstatic int hns_roce_v2_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period)\n{\n\tstruct hns_roce_dev *hr_dev = to_hr_dev(cq->device);\n\tstruct hns_roce_v2_cq_context *cq_context;\n\tstruct hns_roce_cq *hr_cq = to_hr_cq(cq);\n\tstruct hns_roce_v2_cq_context *cqc_mask;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tcq_context = mailbox->buf;\n\tcqc_mask = (struct hns_roce_v2_cq_context *)mailbox->buf + 1;\n\n\tmemset(cqc_mask, 0xff, sizeof(*cqc_mask));\n\n\thr_reg_write(cq_context, CQC_CQ_MAX_CNT, cq_count);\n\thr_reg_clear(cqc_mask, CQC_CQ_MAX_CNT);\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\tif (cq_period * HNS_ROCE_CLOCK_ADJUST > USHRT_MAX) {\n\t\t\tdev_info(hr_dev->dev,\n\t\t\t\t \"cq_period(%u) reached the upper limit, adjusted to 65.\\n\",\n\t\t\t\t cq_period);\n\t\t\tcq_period = HNS_ROCE_MAX_CQ_PERIOD;\n\t\t}\n\t\tcq_period *= HNS_ROCE_CLOCK_ADJUST;\n\t}\n\thr_reg_write(cq_context, CQC_CQ_PERIOD, cq_period);\n\thr_reg_clear(cqc_mask, CQC_CQ_PERIOD);\n\n\tret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0,\n\t\t\t\tHNS_ROCE_CMD_MODIFY_CQC, hr_cq->cqn);\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\tif (ret)\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to process cmd when modifying CQ, ret = %d.\\n\",\n\t\t\t  ret);\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_query_cqc(struct hns_roce_dev *hr_dev, u32 cqn,\n\t\t\t\t void *buffer)\n{\n\tstruct hns_roce_v2_cq_context *context;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tcontext = mailbox->buf;\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma,\n\t\t\t\tHNS_ROCE_CMD_QUERY_CQC, cqn);\n\tif (ret) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to process cmd when querying CQ, ret = %d.\\n\",\n\t\t\t  ret);\n\t\tgoto err_mailbox;\n\t}\n\n\tmemcpy(buffer, context, sizeof(*context));\n\nerr_mailbox:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn ret;\n}\n\nstatic int hns_roce_v2_query_mpt(struct hns_roce_dev *hr_dev, u32 key,\n\t\t\t\t void *buffer)\n{\n\tstruct hns_roce_v2_mpt_entry *context;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tcontext = mailbox->buf;\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_MPT,\n\t\t\t\tkey_to_hw_index(key));\n\tif (ret) {\n\t\tibdev_err(&hr_dev->ib_dev,\n\t\t\t  \"failed to process cmd when querying MPT, ret = %d.\\n\",\n\t\t\t  ret);\n\t\tgoto err_mailbox;\n\t}\n\n\tmemcpy(buffer, context, sizeof(*context));\n\nerr_mailbox:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn ret;\n}\n\nstatic void hns_roce_irq_work_handle(struct work_struct *work)\n{\n\tstruct hns_roce_work *irq_work =\n\t\t\t\tcontainer_of(work, struct hns_roce_work, work);\n\tstruct ib_device *ibdev = &irq_work->hr_dev->ib_dev;\n\n\tswitch (irq_work->event_type) {\n\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG:\n\t\tibdev_info(ibdev, \"path migrated succeeded.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG_FAILED:\n\t\tibdev_warn(ibdev, \"path migration failed.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_COMM_EST:\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_SQ_DRAINED:\n\t\tibdev_dbg(ibdev, \"send queue drained.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:\n\t\tibdev_err(ibdev, \"local work queue 0x%x catast error, sub_event type is: %d\\n\",\n\t\t\t  irq_work->queue_num, irq_work->sub_type);\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:\n\t\tibdev_err(ibdev, \"invalid request local work queue 0x%x error.\\n\",\n\t\t\t  irq_work->queue_num);\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:\n\t\tibdev_err(ibdev, \"local access violation work queue 0x%x error, sub_event type is: %d\\n\",\n\t\t\t  irq_work->queue_num, irq_work->sub_type);\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:\n\t\tibdev_dbg(ibdev, \"SRQ limit reach.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:\n\t\tibdev_dbg(ibdev, \"SRQ last wqe reach.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_SRQ_CATAS_ERROR:\n\t\tibdev_err(ibdev, \"SRQ catas error.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:\n\t\tibdev_err(ibdev, \"CQ 0x%x access err.\\n\", irq_work->queue_num);\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:\n\t\tibdev_warn(ibdev, \"CQ 0x%x overflow\\n\", irq_work->queue_num);\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_DB_OVERFLOW:\n\t\tibdev_warn(ibdev, \"DB overflow.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_FLR:\n\t\tibdev_warn(ibdev, \"function level reset.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_XRCD_VIOLATION:\n\t\tibdev_err(ibdev, \"xrc domain violation error.\\n\");\n\t\tbreak;\n\tcase HNS_ROCE_EVENT_TYPE_INVALID_XRCETH:\n\t\tibdev_err(ibdev, \"invalid xrceth error.\\n\");\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tkfree(irq_work);\n}\n\nstatic void hns_roce_v2_init_irq_work(struct hns_roce_dev *hr_dev,\n\t\t\t\t      struct hns_roce_eq *eq, u32 queue_num)\n{\n\tstruct hns_roce_work *irq_work;\n\n\tirq_work = kzalloc(sizeof(struct hns_roce_work), GFP_ATOMIC);\n\tif (!irq_work)\n\t\treturn;\n\n\tINIT_WORK(&irq_work->work, hns_roce_irq_work_handle);\n\tirq_work->hr_dev = hr_dev;\n\tirq_work->event_type = eq->event_type;\n\tirq_work->sub_type = eq->sub_type;\n\tirq_work->queue_num = queue_num;\n\tqueue_work(hr_dev->irq_workq, &irq_work->work);\n}\n\nstatic void update_eq_db(struct hns_roce_eq *eq)\n{\n\tstruct hns_roce_dev *hr_dev = eq->hr_dev;\n\tstruct hns_roce_v2_db eq_db = {};\n\n\tif (eq->type_flag == HNS_ROCE_AEQ) {\n\t\thr_reg_write(&eq_db, EQ_DB_CMD,\n\t\t\t     eq->arm_st == HNS_ROCE_V2_EQ_ALWAYS_ARMED ?\n\t\t\t     HNS_ROCE_EQ_DB_CMD_AEQ :\n\t\t\t     HNS_ROCE_EQ_DB_CMD_AEQ_ARMED);\n\t} else {\n\t\thr_reg_write(&eq_db, EQ_DB_TAG, eq->eqn);\n\n\t\thr_reg_write(&eq_db, EQ_DB_CMD,\n\t\t\t     eq->arm_st == HNS_ROCE_V2_EQ_ALWAYS_ARMED ?\n\t\t\t     HNS_ROCE_EQ_DB_CMD_CEQ :\n\t\t\t     HNS_ROCE_EQ_DB_CMD_CEQ_ARMED);\n\t}\n\n\thr_reg_write(&eq_db, EQ_DB_CI, eq->cons_index);\n\n\thns_roce_write64(hr_dev, (__le32 *)&eq_db, eq->db_reg);\n}\n\nstatic struct hns_roce_aeqe *next_aeqe_sw_v2(struct hns_roce_eq *eq)\n{\n\tstruct hns_roce_aeqe *aeqe;\n\n\taeqe = hns_roce_buf_offset(eq->mtr.kmem,\n\t\t\t\t   (eq->cons_index & (eq->entries - 1)) *\n\t\t\t\t   eq->eqe_size);\n\n\treturn (hr_reg_read(aeqe, AEQE_OWNER) ^\n\t\t!!(eq->cons_index & eq->entries)) ? aeqe : NULL;\n}\n\nstatic irqreturn_t hns_roce_v2_aeq_int(struct hns_roce_dev *hr_dev,\n\t\t\t\t       struct hns_roce_eq *eq)\n{\n\tstruct device *dev = hr_dev->dev;\n\tstruct hns_roce_aeqe *aeqe = next_aeqe_sw_v2(eq);\n\tirqreturn_t aeqe_found = IRQ_NONE;\n\tint event_type;\n\tu32 queue_num;\n\tint sub_type;\n\n\twhile (aeqe) {\n\t\t \n\t\tdma_rmb();\n\n\t\tevent_type = hr_reg_read(aeqe, AEQE_EVENT_TYPE);\n\t\tsub_type = hr_reg_read(aeqe, AEQE_SUB_TYPE);\n\t\tqueue_num = hr_reg_read(aeqe, AEQE_EVENT_QUEUE_NUM);\n\n\t\tswitch (event_type) {\n\t\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG:\n\t\tcase HNS_ROCE_EVENT_TYPE_PATH_MIG_FAILED:\n\t\tcase HNS_ROCE_EVENT_TYPE_COMM_EST:\n\t\tcase HNS_ROCE_EVENT_TYPE_SQ_DRAINED:\n\t\tcase HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:\n\t\tcase HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:\n\t\tcase HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:\n\t\tcase HNS_ROCE_EVENT_TYPE_XRCD_VIOLATION:\n\t\tcase HNS_ROCE_EVENT_TYPE_INVALID_XRCETH:\n\t\t\thns_roce_qp_event(hr_dev, queue_num, event_type);\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:\n\t\tcase HNS_ROCE_EVENT_TYPE_SRQ_CATAS_ERROR:\n\t\t\thns_roce_srq_event(hr_dev, queue_num, event_type);\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:\n\t\tcase HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:\n\t\t\thns_roce_cq_event(hr_dev, queue_num, event_type);\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_MB:\n\t\t\thns_roce_cmd_event(hr_dev,\n\t\t\t\t\tle16_to_cpu(aeqe->event.cmd.token),\n\t\t\t\t\taeqe->event.cmd.status,\n\t\t\t\t\tle64_to_cpu(aeqe->event.cmd.out_param));\n\t\t\tbreak;\n\t\tcase HNS_ROCE_EVENT_TYPE_DB_OVERFLOW:\n\t\tcase HNS_ROCE_EVENT_TYPE_FLR:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"unhandled event %d on EQ %d at idx %u.\\n\",\n\t\t\t\tevent_type, eq->eqn, eq->cons_index);\n\t\t\tbreak;\n\t\t}\n\n\t\teq->event_type = event_type;\n\t\teq->sub_type = sub_type;\n\t\t++eq->cons_index;\n\t\taeqe_found = IRQ_HANDLED;\n\n\t\thns_roce_v2_init_irq_work(hr_dev, eq, queue_num);\n\n\t\taeqe = next_aeqe_sw_v2(eq);\n\t}\n\n\tupdate_eq_db(eq);\n\n\treturn IRQ_RETVAL(aeqe_found);\n}\n\nstatic struct hns_roce_ceqe *next_ceqe_sw_v2(struct hns_roce_eq *eq)\n{\n\tstruct hns_roce_ceqe *ceqe;\n\n\tceqe = hns_roce_buf_offset(eq->mtr.kmem,\n\t\t\t\t   (eq->cons_index & (eq->entries - 1)) *\n\t\t\t\t   eq->eqe_size);\n\n\treturn (hr_reg_read(ceqe, CEQE_OWNER) ^\n\t\t!!(eq->cons_index & eq->entries)) ? ceqe : NULL;\n}\n\nstatic irqreturn_t hns_roce_v2_ceq_int(struct hns_roce_dev *hr_dev,\n\t\t\t\t       struct hns_roce_eq *eq)\n{\n\tstruct hns_roce_ceqe *ceqe = next_ceqe_sw_v2(eq);\n\tirqreturn_t ceqe_found = IRQ_NONE;\n\tu32 cqn;\n\n\twhile (ceqe) {\n\t\t \n\t\tdma_rmb();\n\n\t\tcqn = hr_reg_read(ceqe, CEQE_CQN);\n\n\t\thns_roce_cq_completion(hr_dev, cqn);\n\n\t\t++eq->cons_index;\n\t\tceqe_found = IRQ_HANDLED;\n\n\t\tceqe = next_ceqe_sw_v2(eq);\n\t}\n\n\tupdate_eq_db(eq);\n\n\treturn IRQ_RETVAL(ceqe_found);\n}\n\nstatic irqreturn_t hns_roce_v2_msix_interrupt_eq(int irq, void *eq_ptr)\n{\n\tstruct hns_roce_eq *eq = eq_ptr;\n\tstruct hns_roce_dev *hr_dev = eq->hr_dev;\n\tirqreturn_t int_work;\n\n\tif (eq->type_flag == HNS_ROCE_CEQ)\n\t\t \n\t\tint_work = hns_roce_v2_ceq_int(hr_dev, eq);\n\telse\n\t\t \n\t\tint_work = hns_roce_v2_aeq_int(hr_dev, eq);\n\n\treturn IRQ_RETVAL(int_work);\n}\n\nstatic irqreturn_t abnormal_interrupt_basic(struct hns_roce_dev *hr_dev,\n\t\t\t\t\t    u32 int_st)\n{\n\tstruct pci_dev *pdev = hr_dev->pci_dev;\n\tstruct hnae3_ae_dev *ae_dev = pci_get_drvdata(pdev);\n\tconst struct hnae3_ae_ops *ops = ae_dev->ops;\n\tirqreturn_t int_work = IRQ_NONE;\n\tu32 int_en;\n\n\tint_en = roce_read(hr_dev, ROCEE_VF_ABN_INT_EN_REG);\n\n\tif (int_st & BIT(HNS_ROCE_V2_VF_INT_ST_AEQ_OVERFLOW_S)) {\n\t\tdev_err(hr_dev->dev, \"AEQ overflow!\\n\");\n\n\t\troce_write(hr_dev, ROCEE_VF_ABN_INT_ST_REG,\n\t\t\t   1 << HNS_ROCE_V2_VF_INT_ST_AEQ_OVERFLOW_S);\n\n\t\t \n\t\tif (ops->set_default_reset_request)\n\t\t\tops->set_default_reset_request(ae_dev,\n\t\t\t\t\t\t       HNAE3_FUNC_RESET);\n\t\tif (ops->reset_event)\n\t\t\tops->reset_event(pdev, NULL);\n\n\t\tint_en |= 1 << HNS_ROCE_V2_VF_ABN_INT_EN_S;\n\t\troce_write(hr_dev, ROCEE_VF_ABN_INT_EN_REG, int_en);\n\n\t\tint_work = IRQ_HANDLED;\n\t} else {\n\t\tdev_err(hr_dev->dev, \"there is no basic abn irq found.\\n\");\n\t}\n\n\treturn IRQ_RETVAL(int_work);\n}\n\nstatic int fmea_ram_ecc_query(struct hns_roce_dev *hr_dev,\n\t\t\t       struct fmea_ram_ecc *ecc_info)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_QUERY_RAM_ECC, true);\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tecc_info->is_ecc_err = hr_reg_read(req, QUERY_RAM_ECC_1BIT_ERR);\n\tecc_info->res_type = hr_reg_read(req, QUERY_RAM_ECC_RES_TYPE);\n\tecc_info->index = hr_reg_read(req, QUERY_RAM_ECC_TAG);\n\n\treturn 0;\n}\n\nstatic int fmea_recover_gmv(struct hns_roce_dev *hr_dev, u32 idx)\n{\n\tstruct hns_roce_cmq_desc desc;\n\tstruct hns_roce_cmq_req *req = (struct hns_roce_cmq_req *)desc.data;\n\tu32 addr_upper;\n\tu32 addr_low;\n\tint ret;\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_GMV_BT, true);\n\thr_reg_write(req, CFG_GMV_BT_IDX, idx);\n\n\tret = hns_roce_cmq_send(hr_dev, &desc, 1);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev,\n\t\t\t\"failed to execute cmd to read gmv, ret = %d.\\n\", ret);\n\t\treturn ret;\n\t}\n\n\taddr_low =  hr_reg_read(req, CFG_GMV_BT_BA_L);\n\taddr_upper = hr_reg_read(req, CFG_GMV_BT_BA_H);\n\n\thns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_GMV_BT, false);\n\thr_reg_write(req, CFG_GMV_BT_BA_L, addr_low);\n\thr_reg_write(req, CFG_GMV_BT_BA_H, addr_upper);\n\thr_reg_write(req, CFG_GMV_BT_IDX, idx);\n\n\treturn hns_roce_cmq_send(hr_dev, &desc, 1);\n}\n\nstatic u64 fmea_get_ram_res_addr(u32 res_type, __le64 *data)\n{\n\tif (res_type == ECC_RESOURCE_QPC_TIMER ||\n\t    res_type == ECC_RESOURCE_CQC_TIMER ||\n\t    res_type == ECC_RESOURCE_SCCC)\n\t\treturn le64_to_cpu(*data);\n\n\treturn le64_to_cpu(*data) << PAGE_SHIFT;\n}\n\nstatic int fmea_recover_others(struct hns_roce_dev *hr_dev, u32 res_type,\n\t\t\t       u32 index)\n{\n\tu8 write_bt0_op = fmea_ram_res[res_type].write_bt0_op;\n\tu8 read_bt0_op = fmea_ram_res[res_type].read_bt0_op;\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tu64 addr;\n\tint ret;\n\n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, read_bt0_op, index);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev,\n\t\t\t\"failed to execute cmd to read fmea ram, ret = %d.\\n\",\n\t\t\tret);\n\t\tgoto out;\n\t}\n\n\taddr = fmea_get_ram_res_addr(res_type, mailbox->buf);\n\n\tret = hns_roce_cmd_mbox(hr_dev, addr, 0, write_bt0_op, index);\n\tif (ret)\n\t\tdev_err(hr_dev->dev,\n\t\t\t\"failed to execute cmd to write fmea ram, ret = %d.\\n\",\n\t\t\tret);\n\nout:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\treturn ret;\n}\n\nstatic void fmea_ram_ecc_recover(struct hns_roce_dev *hr_dev,\n\t\t\t\t struct fmea_ram_ecc *ecc_info)\n{\n\tu32 res_type = ecc_info->res_type;\n\tu32 index = ecc_info->index;\n\tint ret;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(fmea_ram_res) != ECC_RESOURCE_COUNT);\n\n\tif (res_type >= ECC_RESOURCE_COUNT) {\n\t\tdev_err(hr_dev->dev, \"unsupported fmea ram ecc type %u.\\n\",\n\t\t\tres_type);\n\t\treturn;\n\t}\n\n\tif (res_type == ECC_RESOURCE_GMV)\n\t\tret = fmea_recover_gmv(hr_dev, index);\n\telse\n\t\tret = fmea_recover_others(hr_dev, res_type, index);\n\tif (ret)\n\t\tdev_err(hr_dev->dev,\n\t\t\t\"failed to recover %s, index = %u, ret = %d.\\n\",\n\t\t\tfmea_ram_res[res_type].name, index, ret);\n}\n\nstatic void fmea_ram_ecc_work(struct work_struct *ecc_work)\n{\n\tstruct hns_roce_dev *hr_dev =\n\t\tcontainer_of(ecc_work, struct hns_roce_dev, ecc_work);\n\tstruct fmea_ram_ecc ecc_info = {};\n\n\tif (fmea_ram_ecc_query(hr_dev, &ecc_info)) {\n\t\tdev_err(hr_dev->dev, \"failed to query fmea ram ecc.\\n\");\n\t\treturn;\n\t}\n\n\tif (!ecc_info.is_ecc_err) {\n\t\tdev_err(hr_dev->dev, \"there is no fmea ram ecc err found.\\n\");\n\t\treturn;\n\t}\n\n\tfmea_ram_ecc_recover(hr_dev, &ecc_info);\n}\n\nstatic irqreturn_t hns_roce_v2_msix_interrupt_abn(int irq, void *dev_id)\n{\n\tstruct hns_roce_dev *hr_dev = dev_id;\n\tirqreturn_t int_work = IRQ_NONE;\n\tu32 int_st;\n\n\tint_st = roce_read(hr_dev, ROCEE_VF_ABN_INT_ST_REG);\n\n\tif (int_st) {\n\t\tint_work = abnormal_interrupt_basic(hr_dev, int_st);\n\t} else if (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09) {\n\t\tqueue_work(hr_dev->irq_workq, &hr_dev->ecc_work);\n\t\tint_work = IRQ_HANDLED;\n\t} else {\n\t\tdev_err(hr_dev->dev, \"there is no abnormal irq found.\\n\");\n\t}\n\n\treturn IRQ_RETVAL(int_work);\n}\n\nstatic void hns_roce_v2_int_mask_enable(struct hns_roce_dev *hr_dev,\n\t\t\t\t\tint eq_num, u32 enable_flag)\n{\n\tint i;\n\n\tfor (i = 0; i < eq_num; i++)\n\t\troce_write(hr_dev, ROCEE_VF_EVENT_INT_EN_REG +\n\t\t\t   i * EQ_REG_OFFSET, enable_flag);\n\n\troce_write(hr_dev, ROCEE_VF_ABN_INT_EN_REG, enable_flag);\n\troce_write(hr_dev, ROCEE_VF_ABN_INT_CFG_REG, enable_flag);\n}\n\nstatic void hns_roce_v2_destroy_eqc(struct hns_roce_dev *hr_dev, u32 eqn)\n{\n\tstruct device *dev = hr_dev->dev;\n\tint ret;\n\tu8 cmd;\n\n\tif (eqn < hr_dev->caps.num_comp_vectors)\n\t\tcmd = HNS_ROCE_CMD_DESTROY_CEQC;\n\telse\n\t\tcmd = HNS_ROCE_CMD_DESTROY_AEQC;\n\n\tret = hns_roce_destroy_hw_ctx(hr_dev, cmd, eqn & HNS_ROCE_V2_EQN_M);\n\tif (ret)\n\t\tdev_err(dev, \"[mailbox cmd] destroy eqc(%u) failed.\\n\", eqn);\n}\n\nstatic void free_eq_buf(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\n{\n\thns_roce_mtr_destroy(hr_dev, &eq->mtr);\n}\n\nstatic void init_eq_config(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\n{\n\teq->db_reg = hr_dev->reg_base + ROCEE_VF_EQ_DB_CFG0_REG;\n\teq->cons_index = 0;\n\teq->over_ignore = HNS_ROCE_V2_EQ_OVER_IGNORE_0;\n\teq->coalesce = HNS_ROCE_V2_EQ_COALESCE_0;\n\teq->arm_st = HNS_ROCE_V2_EQ_ALWAYS_ARMED;\n\teq->shift = ilog2((unsigned int)eq->entries);\n}\n\nstatic int config_eqc(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq,\n\t\t      void *mb_buf)\n{\n\tu64 eqe_ba[MTT_MIN_COUNT] = { 0 };\n\tstruct hns_roce_eq_context *eqc;\n\tu64 bt_ba = 0;\n\tint count;\n\n\teqc = mb_buf;\n\tmemset(eqc, 0, sizeof(struct hns_roce_eq_context));\n\n\tinit_eq_config(hr_dev, eq);\n\n\t \n\tcount = hns_roce_mtr_find(hr_dev, &eq->mtr, 0, eqe_ba, MTT_MIN_COUNT,\n\t\t\t\t  &bt_ba);\n\tif (count < 1) {\n\t\tdev_err(hr_dev->dev, \"failed to find EQE mtr\\n\");\n\t\treturn -ENOBUFS;\n\t}\n\n\thr_reg_write(eqc, EQC_EQ_ST, HNS_ROCE_V2_EQ_STATE_VALID);\n\thr_reg_write(eqc, EQC_EQE_HOP_NUM, eq->hop_num);\n\thr_reg_write(eqc, EQC_OVER_IGNORE, eq->over_ignore);\n\thr_reg_write(eqc, EQC_COALESCE, eq->coalesce);\n\thr_reg_write(eqc, EQC_ARM_ST, eq->arm_st);\n\thr_reg_write(eqc, EQC_EQN, eq->eqn);\n\thr_reg_write(eqc, EQC_EQE_CNT, HNS_ROCE_EQ_INIT_EQE_CNT);\n\thr_reg_write(eqc, EQC_EQE_BA_PG_SZ,\n\t\t     to_hr_hw_page_shift(eq->mtr.hem_cfg.ba_pg_shift));\n\thr_reg_write(eqc, EQC_EQE_BUF_PG_SZ,\n\t\t     to_hr_hw_page_shift(eq->mtr.hem_cfg.buf_pg_shift));\n\thr_reg_write(eqc, EQC_EQ_PROD_INDX, HNS_ROCE_EQ_INIT_PROD_IDX);\n\thr_reg_write(eqc, EQC_EQ_MAX_CNT, eq->eq_max_cnt);\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\tif (eq->eq_period * HNS_ROCE_CLOCK_ADJUST > USHRT_MAX) {\n\t\t\tdev_info(hr_dev->dev, \"eq_period(%u) reached the upper limit, adjusted to 65.\\n\",\n\t\t\t\t eq->eq_period);\n\t\t\teq->eq_period = HNS_ROCE_MAX_EQ_PERIOD;\n\t\t}\n\t\teq->eq_period *= HNS_ROCE_CLOCK_ADJUST;\n\t}\n\n\thr_reg_write(eqc, EQC_EQ_PERIOD, eq->eq_period);\n\thr_reg_write(eqc, EQC_EQE_REPORT_TIMER, HNS_ROCE_EQ_INIT_REPORT_TIMER);\n\thr_reg_write(eqc, EQC_EQE_BA_L, bt_ba >> 3);\n\thr_reg_write(eqc, EQC_EQE_BA_H, bt_ba >> 35);\n\thr_reg_write(eqc, EQC_SHIFT, eq->shift);\n\thr_reg_write(eqc, EQC_MSI_INDX, HNS_ROCE_EQ_INIT_MSI_IDX);\n\thr_reg_write(eqc, EQC_CUR_EQE_BA_L, eqe_ba[0] >> 12);\n\thr_reg_write(eqc, EQC_CUR_EQE_BA_M, eqe_ba[0] >> 28);\n\thr_reg_write(eqc, EQC_CUR_EQE_BA_H, eqe_ba[0] >> 60);\n\thr_reg_write(eqc, EQC_EQ_CONS_INDX, HNS_ROCE_EQ_INIT_CONS_IDX);\n\thr_reg_write(eqc, EQC_NEX_EQE_BA_L, eqe_ba[1] >> 12);\n\thr_reg_write(eqc, EQC_NEX_EQE_BA_H, eqe_ba[1] >> 44);\n\thr_reg_write(eqc, EQC_EQE_SIZE, eq->eqe_size == HNS_ROCE_V3_EQE_SIZE);\n\n\treturn 0;\n}\n\nstatic int alloc_eq_buf(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\n{\n\tstruct hns_roce_buf_attr buf_attr = {};\n\tint err;\n\n\tif (hr_dev->caps.eqe_hop_num == HNS_ROCE_HOP_NUM_0)\n\t\teq->hop_num = 0;\n\telse\n\t\teq->hop_num = hr_dev->caps.eqe_hop_num;\n\n\tbuf_attr.page_shift = hr_dev->caps.eqe_buf_pg_sz + PAGE_SHIFT;\n\tbuf_attr.region[0].size = eq->entries * eq->eqe_size;\n\tbuf_attr.region[0].hopnum = eq->hop_num;\n\tbuf_attr.region_count = 1;\n\n\terr = hns_roce_mtr_create(hr_dev, &eq->mtr, &buf_attr,\n\t\t\t\t  hr_dev->caps.eqe_ba_pg_sz + PAGE_SHIFT, NULL,\n\t\t\t\t  0);\n\tif (err)\n\t\tdev_err(hr_dev->dev, \"failed to alloc EQE mtr, err %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int hns_roce_v2_create_eq(struct hns_roce_dev *hr_dev,\n\t\t\t\t struct hns_roce_eq *eq, u8 eq_cmd)\n{\n\tstruct hns_roce_cmd_mailbox *mailbox;\n\tint ret;\n\n\t \n\tmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tret = alloc_eq_buf(hr_dev, eq);\n\tif (ret)\n\t\tgoto free_cmd_mbox;\n\n\tret = config_eqc(hr_dev, eq, mailbox->buf);\n\tif (ret)\n\t\tgoto err_cmd_mbox;\n\n\tret = hns_roce_create_hw_ctx(hr_dev, mailbox, eq_cmd, eq->eqn);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev, \"[mailbox cmd] create eqc failed.\\n\");\n\t\tgoto err_cmd_mbox;\n\t}\n\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn 0;\n\nerr_cmd_mbox:\n\tfree_eq_buf(hr_dev, eq);\n\nfree_cmd_mbox:\n\thns_roce_free_cmd_mailbox(hr_dev, mailbox);\n\n\treturn ret;\n}\n\nstatic int __hns_roce_request_irq(struct hns_roce_dev *hr_dev, int irq_num,\n\t\t\t\t  int comp_num, int aeq_num, int other_num)\n{\n\tstruct hns_roce_eq_table *eq_table = &hr_dev->eq_table;\n\tint i, j;\n\tint ret;\n\n\tfor (i = 0; i < irq_num; i++) {\n\t\thr_dev->irq_names[i] = kzalloc(HNS_ROCE_INT_NAME_LEN,\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!hr_dev->irq_names[i]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_kzalloc_failed;\n\t\t}\n\t}\n\n\t \n\tfor (j = 0; j < other_num; j++)\n\t\tsnprintf((char *)hr_dev->irq_names[j], HNS_ROCE_INT_NAME_LEN,\n\t\t\t \"hns-abn-%d\", j);\n\n\tfor (j = other_num; j < (other_num + aeq_num); j++)\n\t\tsnprintf((char *)hr_dev->irq_names[j], HNS_ROCE_INT_NAME_LEN,\n\t\t\t \"hns-aeq-%d\", j - other_num);\n\n\tfor (j = (other_num + aeq_num); j < irq_num; j++)\n\t\tsnprintf((char *)hr_dev->irq_names[j], HNS_ROCE_INT_NAME_LEN,\n\t\t\t \"hns-ceq-%d\", j - other_num - aeq_num);\n\n\tfor (j = 0; j < irq_num; j++) {\n\t\tif (j < other_num)\n\t\t\tret = request_irq(hr_dev->irq[j],\n\t\t\t\t\t  hns_roce_v2_msix_interrupt_abn,\n\t\t\t\t\t  0, hr_dev->irq_names[j], hr_dev);\n\n\t\telse if (j < (other_num + comp_num))\n\t\t\tret = request_irq(eq_table->eq[j - other_num].irq,\n\t\t\t\t\t  hns_roce_v2_msix_interrupt_eq,\n\t\t\t\t\t  0, hr_dev->irq_names[j + aeq_num],\n\t\t\t\t\t  &eq_table->eq[j - other_num]);\n\t\telse\n\t\t\tret = request_irq(eq_table->eq[j - other_num].irq,\n\t\t\t\t\t  hns_roce_v2_msix_interrupt_eq,\n\t\t\t\t\t  0, hr_dev->irq_names[j - comp_num],\n\t\t\t\t\t  &eq_table->eq[j - other_num]);\n\t\tif (ret) {\n\t\t\tdev_err(hr_dev->dev, \"request irq error!\\n\");\n\t\t\tgoto err_request_failed;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_request_failed:\n\tfor (j -= 1; j >= 0; j--)\n\t\tif (j < other_num)\n\t\t\tfree_irq(hr_dev->irq[j], hr_dev);\n\t\telse\n\t\t\tfree_irq(eq_table->eq[j - other_num].irq,\n\t\t\t\t &eq_table->eq[j - other_num]);\n\nerr_kzalloc_failed:\n\tfor (i -= 1; i >= 0; i--)\n\t\tkfree(hr_dev->irq_names[i]);\n\n\treturn ret;\n}\n\nstatic void __hns_roce_free_irq(struct hns_roce_dev *hr_dev)\n{\n\tint irq_num;\n\tint eq_num;\n\tint i;\n\n\teq_num = hr_dev->caps.num_comp_vectors + hr_dev->caps.num_aeq_vectors;\n\tirq_num = eq_num + hr_dev->caps.num_other_vectors;\n\n\tfor (i = 0; i < hr_dev->caps.num_other_vectors; i++)\n\t\tfree_irq(hr_dev->irq[i], hr_dev);\n\n\tfor (i = 0; i < eq_num; i++)\n\t\tfree_irq(hr_dev->eq_table.eq[i].irq, &hr_dev->eq_table.eq[i]);\n\n\tfor (i = 0; i < irq_num; i++)\n\t\tkfree(hr_dev->irq_names[i]);\n}\n\nstatic int hns_roce_v2_init_eq_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_eq_table *eq_table = &hr_dev->eq_table;\n\tstruct device *dev = hr_dev->dev;\n\tstruct hns_roce_eq *eq;\n\tint other_num;\n\tint comp_num;\n\tint aeq_num;\n\tint irq_num;\n\tint eq_num;\n\tu8 eq_cmd;\n\tint ret;\n\tint i;\n\n\tother_num = hr_dev->caps.num_other_vectors;\n\tcomp_num = hr_dev->caps.num_comp_vectors;\n\taeq_num = hr_dev->caps.num_aeq_vectors;\n\n\teq_num = comp_num + aeq_num;\n\tirq_num = eq_num + other_num;\n\n\teq_table->eq = kcalloc(eq_num, sizeof(*eq_table->eq), GFP_KERNEL);\n\tif (!eq_table->eq)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < eq_num; i++) {\n\t\teq = &eq_table->eq[i];\n\t\teq->hr_dev = hr_dev;\n\t\teq->eqn = i;\n\t\tif (i < comp_num) {\n\t\t\t \n\t\t\teq_cmd = HNS_ROCE_CMD_CREATE_CEQC;\n\t\t\teq->type_flag = HNS_ROCE_CEQ;\n\t\t\teq->entries = hr_dev->caps.ceqe_depth;\n\t\t\teq->eqe_size = hr_dev->caps.ceqe_size;\n\t\t\teq->irq = hr_dev->irq[i + other_num + aeq_num];\n\t\t\teq->eq_max_cnt = HNS_ROCE_CEQ_DEFAULT_BURST_NUM;\n\t\t\teq->eq_period = HNS_ROCE_CEQ_DEFAULT_INTERVAL;\n\t\t} else {\n\t\t\t \n\t\t\teq_cmd = HNS_ROCE_CMD_CREATE_AEQC;\n\t\t\teq->type_flag = HNS_ROCE_AEQ;\n\t\t\teq->entries = hr_dev->caps.aeqe_depth;\n\t\t\teq->eqe_size = hr_dev->caps.aeqe_size;\n\t\t\teq->irq = hr_dev->irq[i - comp_num + other_num];\n\t\t\teq->eq_max_cnt = HNS_ROCE_AEQ_DEFAULT_BURST_NUM;\n\t\t\teq->eq_period = HNS_ROCE_AEQ_DEFAULT_INTERVAL;\n\t\t}\n\n\t\tret = hns_roce_v2_create_eq(hr_dev, eq, eq_cmd);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to create eq.\\n\");\n\t\t\tgoto err_create_eq_fail;\n\t\t}\n\t}\n\n\tINIT_WORK(&hr_dev->ecc_work, fmea_ram_ecc_work);\n\n\thr_dev->irq_workq = alloc_ordered_workqueue(\"hns_roce_irq_workq\", 0);\n\tif (!hr_dev->irq_workq) {\n\t\tdev_err(dev, \"failed to create irq workqueue.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_create_eq_fail;\n\t}\n\n\tret = __hns_roce_request_irq(hr_dev, irq_num, comp_num, aeq_num,\n\t\t\t\t     other_num);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to request irq.\\n\");\n\t\tgoto err_request_irq_fail;\n\t}\n\n\t \n\thns_roce_v2_int_mask_enable(hr_dev, eq_num, EQ_ENABLE);\n\n\treturn 0;\n\nerr_request_irq_fail:\n\tdestroy_workqueue(hr_dev->irq_workq);\n\nerr_create_eq_fail:\n\tfor (i -= 1; i >= 0; i--)\n\t\tfree_eq_buf(hr_dev, &eq_table->eq[i]);\n\tkfree(eq_table->eq);\n\n\treturn ret;\n}\n\nstatic void hns_roce_v2_cleanup_eq_table(struct hns_roce_dev *hr_dev)\n{\n\tstruct hns_roce_eq_table *eq_table = &hr_dev->eq_table;\n\tint eq_num;\n\tint i;\n\n\teq_num = hr_dev->caps.num_comp_vectors + hr_dev->caps.num_aeq_vectors;\n\n\t \n\thns_roce_v2_int_mask_enable(hr_dev, eq_num, EQ_DISABLE);\n\n\t__hns_roce_free_irq(hr_dev);\n\tdestroy_workqueue(hr_dev->irq_workq);\n\n\tfor (i = 0; i < eq_num; i++) {\n\t\thns_roce_v2_destroy_eqc(hr_dev, i);\n\n\t\tfree_eq_buf(hr_dev, &eq_table->eq[i]);\n\t}\n\n\tkfree(eq_table->eq);\n}\n\nstatic const struct ib_device_ops hns_roce_v2_dev_ops = {\n\t.destroy_qp = hns_roce_v2_destroy_qp,\n\t.modify_cq = hns_roce_v2_modify_cq,\n\t.poll_cq = hns_roce_v2_poll_cq,\n\t.post_recv = hns_roce_v2_post_recv,\n\t.post_send = hns_roce_v2_post_send,\n\t.query_qp = hns_roce_v2_query_qp,\n\t.req_notify_cq = hns_roce_v2_req_notify_cq,\n};\n\nstatic const struct ib_device_ops hns_roce_v2_dev_srq_ops = {\n\t.modify_srq = hns_roce_v2_modify_srq,\n\t.post_srq_recv = hns_roce_v2_post_srq_recv,\n\t.query_srq = hns_roce_v2_query_srq,\n};\n\nstatic const struct hns_roce_hw hns_roce_hw_v2 = {\n\t.cmq_init = hns_roce_v2_cmq_init,\n\t.cmq_exit = hns_roce_v2_cmq_exit,\n\t.hw_profile = hns_roce_v2_profile,\n\t.hw_init = hns_roce_v2_init,\n\t.hw_exit = hns_roce_v2_exit,\n\t.post_mbox = v2_post_mbox,\n\t.poll_mbox_done = v2_poll_mbox_done,\n\t.chk_mbox_avail = v2_chk_mbox_is_avail,\n\t.set_gid = hns_roce_v2_set_gid,\n\t.set_mac = hns_roce_v2_set_mac,\n\t.write_mtpt = hns_roce_v2_write_mtpt,\n\t.rereg_write_mtpt = hns_roce_v2_rereg_write_mtpt,\n\t.frmr_write_mtpt = hns_roce_v2_frmr_write_mtpt,\n\t.mw_write_mtpt = hns_roce_v2_mw_write_mtpt,\n\t.write_cqc = hns_roce_v2_write_cqc,\n\t.set_hem = hns_roce_v2_set_hem,\n\t.clear_hem = hns_roce_v2_clear_hem,\n\t.modify_qp = hns_roce_v2_modify_qp,\n\t.dereg_mr = hns_roce_v2_dereg_mr,\n\t.qp_flow_control_init = hns_roce_v2_qp_flow_control_init,\n\t.init_eq = hns_roce_v2_init_eq_table,\n\t.cleanup_eq = hns_roce_v2_cleanup_eq_table,\n\t.write_srqc = hns_roce_v2_write_srqc,\n\t.query_cqc = hns_roce_v2_query_cqc,\n\t.query_qpc = hns_roce_v2_query_qpc,\n\t.query_mpt = hns_roce_v2_query_mpt,\n\t.query_hw_counter = hns_roce_hw_v2_query_counter,\n\t.hns_roce_dev_ops = &hns_roce_v2_dev_ops,\n\t.hns_roce_dev_srq_ops = &hns_roce_v2_dev_srq_ops,\n};\n\nstatic const struct pci_device_id hns_roce_hw_v2_pci_tbl[] = {\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_25GE_RDMA), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_25GE_RDMA_MACSEC), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_50GE_RDMA), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_50GE_RDMA_MACSEC), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_100G_RDMA_MACSEC), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_200G_RDMA), 0},\n\t{PCI_VDEVICE(HUAWEI, HNAE3_DEV_ID_RDMA_DCB_PFC_VF),\n\t HNAE3_DEV_SUPPORT_ROCE_DCB_BITS},\n\t \n\t{0, }\n};\n\nMODULE_DEVICE_TABLE(pci, hns_roce_hw_v2_pci_tbl);\n\nstatic void hns_roce_hw_v2_get_cfg(struct hns_roce_dev *hr_dev,\n\t\t\t\t  struct hnae3_handle *handle)\n{\n\tstruct hns_roce_v2_priv *priv = hr_dev->priv;\n\tconst struct pci_device_id *id;\n\tint i;\n\n\thr_dev->pci_dev = handle->pdev;\n\tid = pci_match_id(hns_roce_hw_v2_pci_tbl, hr_dev->pci_dev);\n\thr_dev->is_vf = id->driver_data;\n\thr_dev->dev = &handle->pdev->dev;\n\thr_dev->hw = &hns_roce_hw_v2;\n\thr_dev->sdb_offset = ROCEE_DB_SQ_L_0_REG;\n\thr_dev->odb_offset = hr_dev->sdb_offset;\n\n\t \n\thr_dev->reg_base = handle->rinfo.roce_io_base;\n\thr_dev->mem_base = handle->rinfo.roce_mem_base;\n\thr_dev->caps.num_ports = 1;\n\thr_dev->iboe.netdevs[0] = handle->rinfo.netdev;\n\thr_dev->iboe.phy_port[0] = 0;\n\n\taddrconf_addr_eui48((u8 *)&hr_dev->ib_dev.node_guid,\n\t\t\t    hr_dev->iboe.netdevs[0]->dev_addr);\n\n\tfor (i = 0; i < handle->rinfo.num_vectors; i++)\n\t\thr_dev->irq[i] = pci_irq_vector(handle->pdev,\n\t\t\t\t\t\ti + handle->rinfo.base_vector);\n\n\t \n\thr_dev->cmd_mod = 1;\n\thr_dev->loop_idc = 0;\n\n\thr_dev->reset_cnt = handle->ae_algo->ops->ae_dev_reset_cnt(handle);\n\tpriv->handle = handle;\n}\n\nstatic int __hns_roce_hw_v2_init_instance(struct hnae3_handle *handle)\n{\n\tstruct hns_roce_dev *hr_dev;\n\tint ret;\n\n\thr_dev = ib_alloc_device(hns_roce_dev, ib_dev);\n\tif (!hr_dev)\n\t\treturn -ENOMEM;\n\n\thr_dev->priv = kzalloc(sizeof(struct hns_roce_v2_priv), GFP_KERNEL);\n\tif (!hr_dev->priv) {\n\t\tret = -ENOMEM;\n\t\tgoto error_failed_kzalloc;\n\t}\n\n\thns_roce_hw_v2_get_cfg(hr_dev, handle);\n\n\tret = hns_roce_init(hr_dev);\n\tif (ret) {\n\t\tdev_err(hr_dev->dev, \"RoCE Engine init failed!\\n\");\n\t\tgoto error_failed_roce_init;\n\t}\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) {\n\t\tret = free_mr_init(hr_dev);\n\t\tif (ret) {\n\t\t\tdev_err(hr_dev->dev, \"failed to init free mr!\\n\");\n\t\t\tgoto error_failed_free_mr_init;\n\t\t}\n\t}\n\n\thandle->priv = hr_dev;\n\n\treturn 0;\n\nerror_failed_free_mr_init:\n\thns_roce_exit(hr_dev);\n\nerror_failed_roce_init:\n\tkfree(hr_dev->priv);\n\nerror_failed_kzalloc:\n\tib_dealloc_device(&hr_dev->ib_dev);\n\n\treturn ret;\n}\n\nstatic void __hns_roce_hw_v2_uninit_instance(struct hnae3_handle *handle,\n\t\t\t\t\t   bool reset)\n{\n\tstruct hns_roce_dev *hr_dev = handle->priv;\n\n\tif (!hr_dev)\n\t\treturn;\n\n\thandle->priv = NULL;\n\n\thr_dev->state = HNS_ROCE_DEVICE_STATE_UNINIT;\n\thns_roce_handle_device_err(hr_dev);\n\n\tif (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)\n\t\tfree_mr_exit(hr_dev);\n\n\thns_roce_exit(hr_dev);\n\tkfree(hr_dev->priv);\n\tib_dealloc_device(&hr_dev->ib_dev);\n}\n\nstatic int hns_roce_hw_v2_init_instance(struct hnae3_handle *handle)\n{\n\tconst struct hnae3_ae_ops *ops = handle->ae_algo->ops;\n\tconst struct pci_device_id *id;\n\tstruct device *dev = &handle->pdev->dev;\n\tint ret;\n\n\thandle->rinfo.instance_state = HNS_ROCE_STATE_INIT;\n\n\tif (ops->ae_dev_resetting(handle) || ops->get_hw_reset_stat(handle)) {\n\t\thandle->rinfo.instance_state = HNS_ROCE_STATE_NON_INIT;\n\t\tgoto reset_chk_err;\n\t}\n\n\tid = pci_match_id(hns_roce_hw_v2_pci_tbl, handle->pdev);\n\tif (!id)\n\t\treturn 0;\n\n\tif (id->driver_data && handle->pdev->revision == PCI_REVISION_ID_HIP08)\n\t\treturn 0;\n\n\tret = __hns_roce_hw_v2_init_instance(handle);\n\tif (ret) {\n\t\thandle->rinfo.instance_state = HNS_ROCE_STATE_NON_INIT;\n\t\tdev_err(dev, \"RoCE instance init failed! ret = %d\\n\", ret);\n\t\tif (ops->ae_dev_resetting(handle) ||\n\t\t    ops->get_hw_reset_stat(handle))\n\t\t\tgoto reset_chk_err;\n\t\telse\n\t\t\treturn ret;\n\t}\n\n\thandle->rinfo.instance_state = HNS_ROCE_STATE_INITED;\n\n\treturn 0;\n\nreset_chk_err:\n\tdev_err(dev, \"Device is busy in resetting state.\\n\"\n\t\t     \"please retry later.\\n\");\n\n\treturn -EBUSY;\n}\n\nstatic void hns_roce_hw_v2_uninit_instance(struct hnae3_handle *handle,\n\t\t\t\t\t   bool reset)\n{\n\tif (handle->rinfo.instance_state != HNS_ROCE_STATE_INITED)\n\t\treturn;\n\n\thandle->rinfo.instance_state = HNS_ROCE_STATE_UNINIT;\n\n\t__hns_roce_hw_v2_uninit_instance(handle, reset);\n\n\thandle->rinfo.instance_state = HNS_ROCE_STATE_NON_INIT;\n}\nstatic int hns_roce_hw_v2_reset_notify_down(struct hnae3_handle *handle)\n{\n\tstruct hns_roce_dev *hr_dev;\n\n\tif (handle->rinfo.instance_state != HNS_ROCE_STATE_INITED) {\n\t\tset_bit(HNS_ROCE_RST_DIRECT_RETURN, &handle->rinfo.state);\n\t\treturn 0;\n\t}\n\n\thandle->rinfo.reset_state = HNS_ROCE_STATE_RST_DOWN;\n\tclear_bit(HNS_ROCE_RST_DIRECT_RETURN, &handle->rinfo.state);\n\n\thr_dev = handle->priv;\n\tif (!hr_dev)\n\t\treturn 0;\n\n\thr_dev->active = false;\n\thr_dev->dis_db = true;\n\thr_dev->state = HNS_ROCE_DEVICE_STATE_RST_DOWN;\n\n\treturn 0;\n}\n\nstatic int hns_roce_hw_v2_reset_notify_init(struct hnae3_handle *handle)\n{\n\tstruct device *dev = &handle->pdev->dev;\n\tint ret;\n\n\tif (test_and_clear_bit(HNS_ROCE_RST_DIRECT_RETURN,\n\t\t\t       &handle->rinfo.state)) {\n\t\thandle->rinfo.reset_state = HNS_ROCE_STATE_RST_INITED;\n\t\treturn 0;\n\t}\n\n\thandle->rinfo.reset_state = HNS_ROCE_STATE_RST_INIT;\n\n\tdev_info(&handle->pdev->dev, \"In reset process RoCE client reinit.\\n\");\n\tret = __hns_roce_hw_v2_init_instance(handle);\n\tif (ret) {\n\t\t \n\t\thandle->priv = NULL;\n\t\tdev_err(dev, \"In reset process RoCE reinit failed %d.\\n\", ret);\n\t} else {\n\t\thandle->rinfo.reset_state = HNS_ROCE_STATE_RST_INITED;\n\t\tdev_info(dev, \"reset done, RoCE client reinit finished.\\n\");\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_roce_hw_v2_reset_notify_uninit(struct hnae3_handle *handle)\n{\n\tif (test_bit(HNS_ROCE_RST_DIRECT_RETURN, &handle->rinfo.state))\n\t\treturn 0;\n\n\thandle->rinfo.reset_state = HNS_ROCE_STATE_RST_UNINIT;\n\tdev_info(&handle->pdev->dev, \"In reset process RoCE client uninit.\\n\");\n\tmsleep(HNS_ROCE_V2_HW_RST_UNINT_DELAY);\n\t__hns_roce_hw_v2_uninit_instance(handle, false);\n\n\treturn 0;\n}\n\nstatic int hns_roce_hw_v2_reset_notify(struct hnae3_handle *handle,\n\t\t\t\t       enum hnae3_reset_notify_type type)\n{\n\tint ret = 0;\n\n\tswitch (type) {\n\tcase HNAE3_DOWN_CLIENT:\n\t\tret = hns_roce_hw_v2_reset_notify_down(handle);\n\t\tbreak;\n\tcase HNAE3_INIT_CLIENT:\n\t\tret = hns_roce_hw_v2_reset_notify_init(handle);\n\t\tbreak;\n\tcase HNAE3_UNINIT_CLIENT:\n\t\tret = hns_roce_hw_v2_reset_notify_uninit(handle);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct hnae3_client_ops hns_roce_hw_v2_ops = {\n\t.init_instance = hns_roce_hw_v2_init_instance,\n\t.uninit_instance = hns_roce_hw_v2_uninit_instance,\n\t.reset_notify = hns_roce_hw_v2_reset_notify,\n};\n\nstatic struct hnae3_client hns_roce_hw_v2_client = {\n\t.name = \"hns_roce_hw_v2\",\n\t.type = HNAE3_CLIENT_ROCE,\n\t.ops = &hns_roce_hw_v2_ops,\n};\n\nstatic int __init hns_roce_hw_v2_init(void)\n{\n\treturn hnae3_register_client(&hns_roce_hw_v2_client);\n}\n\nstatic void __exit hns_roce_hw_v2_exit(void)\n{\n\thnae3_unregister_client(&hns_roce_hw_v2_client);\n}\n\nmodule_init(hns_roce_hw_v2_init);\nmodule_exit(hns_roce_hw_v2_exit);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Wei Hu <xavier.huwei@huawei.com>\");\nMODULE_AUTHOR(\"Lijun Ou <oulijun@huawei.com>\");\nMODULE_AUTHOR(\"Shaobo Xu <xushaobo2@huawei.com>\");\nMODULE_DESCRIPTION(\"Hisilicon Hip08 Family RoCE Driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}