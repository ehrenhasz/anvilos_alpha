{
  "module_name": "qp.c",
  "hash_id": "31d3e9536edd67d9338a280c6131175dc9189a3d287967c185b2fae207196fc9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mana/qp.c",
  "human_readable_source": "\n \n\n#include \"mana_ib.h\"\n\nstatic int mana_ib_cfg_vport_steering(struct mana_ib_dev *dev,\n\t\t\t\t      struct net_device *ndev,\n\t\t\t\t      mana_handle_t default_rxobj,\n\t\t\t\t      mana_handle_t ind_table[],\n\t\t\t\t      u32 log_ind_tbl_size, u32 rx_hash_key_len,\n\t\t\t\t      u8 *rx_hash_key)\n{\n\tstruct mana_port_context *mpc = netdev_priv(ndev);\n\tstruct mana_cfg_rx_steer_req_v2 *req;\n\tstruct mana_cfg_rx_steer_resp resp = {};\n\tmana_handle_t *req_indir_tab;\n\tstruct gdma_context *gc;\n\tstruct gdma_dev *mdev;\n\tu32 req_buf_size;\n\tint i, err;\n\n\tmdev = dev->gdma_dev;\n\tgc = mdev->gdma_context;\n\n\treq_buf_size =\n\t\tsizeof(*req) + sizeof(mana_handle_t) * MANA_INDIRECT_TABLE_SIZE;\n\treq = kzalloc(req_buf_size, GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tmana_gd_init_req_hdr(&req->hdr, MANA_CONFIG_VPORT_RX, req_buf_size,\n\t\t\t     sizeof(resp));\n\n\treq->hdr.req.msg_version = GDMA_MESSAGE_V2;\n\n\treq->vport = mpc->port_handle;\n\treq->rx_enable = 1;\n\treq->update_default_rxobj = 1;\n\treq->default_rxobj = default_rxobj;\n\treq->hdr.dev_id = mdev->dev_id;\n\n\t \n\tif (log_ind_tbl_size)\n\t\treq->rss_enable = true;\n\n\treq->num_indir_entries = MANA_INDIRECT_TABLE_SIZE;\n\treq->indir_tab_offset = sizeof(*req);\n\treq->update_indir_tab = true;\n\treq->cqe_coalescing_enable = 1;\n\n\treq_indir_tab = (mana_handle_t *)(req + 1);\n\t \n\tibdev_dbg(&dev->ib_dev, \"ind table size %u\\n\", 1 << log_ind_tbl_size);\n\tfor (i = 0; i < MANA_INDIRECT_TABLE_SIZE; i++) {\n\t\treq_indir_tab[i] = ind_table[i % (1 << log_ind_tbl_size)];\n\t\tibdev_dbg(&dev->ib_dev, \"index %u handle 0x%llx\\n\", i,\n\t\t\t  req_indir_tab[i]);\n\t}\n\n\treq->update_hashkey = true;\n\tif (rx_hash_key_len)\n\t\tmemcpy(req->hashkey, rx_hash_key, rx_hash_key_len);\n\telse\n\t\tnetdev_rss_key_fill(req->hashkey, MANA_HASH_KEY_SIZE);\n\n\tibdev_dbg(&dev->ib_dev, \"vport handle %llu default_rxobj 0x%llx\\n\",\n\t\t  req->vport, default_rxobj);\n\n\terr = mana_gd_send_request(gc, req_buf_size, req, sizeof(resp), &resp);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to configure vPort RX: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tif (resp.hdr.status) {\n\t\tnetdev_err(ndev, \"vPort RX configuration failed: 0x%x\\n\",\n\t\t\t   resp.hdr.status);\n\t\terr = -EPROTO;\n\t\tgoto out;\n\t}\n\n\tnetdev_info(ndev, \"Configured steering vPort %llu log_entries %u\\n\",\n\t\t    mpc->port_handle, log_ind_tbl_size);\n\nout:\n\tkfree(req);\n\treturn err;\n}\n\nstatic int mana_ib_create_qp_rss(struct ib_qp *ibqp, struct ib_pd *pd,\n\t\t\t\t struct ib_qp_init_attr *attr,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct mana_ib_qp *qp = container_of(ibqp, struct mana_ib_qp, ibqp);\n\tstruct mana_ib_dev *mdev =\n\t\tcontainer_of(pd->device, struct mana_ib_dev, ib_dev);\n\tstruct ib_rwq_ind_table *ind_tbl = attr->rwq_ind_tbl;\n\tstruct mana_ib_create_qp_rss_resp resp = {};\n\tstruct mana_ib_create_qp_rss ucmd = {};\n\tstruct gdma_dev *gd = mdev->gdma_dev;\n\tmana_handle_t *mana_ind_table;\n\tstruct mana_port_context *mpc;\n\tstruct mana_context *mc;\n\tstruct net_device *ndev;\n\tstruct mana_ib_cq *cq;\n\tstruct mana_ib_wq *wq;\n\tunsigned int ind_tbl_size;\n\tstruct ib_cq *ibcq;\n\tstruct ib_wq *ibwq;\n\tint i = 0;\n\tu32 port;\n\tint ret;\n\n\tmc = gd->driver_data;\n\n\tif (!udata || udata->inlen < sizeof(ucmd))\n\t\treturn -EINVAL;\n\n\tret = ib_copy_from_udata(&ucmd, udata, min(sizeof(ucmd), udata->inlen));\n\tif (ret) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed copy from udata for create rss-qp, err %d\\n\",\n\t\t\t  ret);\n\t\treturn ret;\n\t}\n\n\tif (attr->cap.max_recv_wr > MAX_SEND_BUFFERS_PER_QUEUE) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Requested max_recv_wr %d exceeding limit\\n\",\n\t\t\t  attr->cap.max_recv_wr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr->cap.max_recv_sge > MAX_RX_WQE_SGL_ENTRIES) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Requested max_recv_sge %d exceeding limit\\n\",\n\t\t\t  attr->cap.max_recv_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tind_tbl_size = 1 << ind_tbl->log_ind_tbl_size;\n\tif (ind_tbl_size > MANA_INDIRECT_TABLE_SIZE) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Indirect table size %d exceeding limit\\n\",\n\t\t\t  ind_tbl_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ucmd.rx_hash_function != MANA_IB_RX_HASH_FUNC_TOEPLITZ) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"RX Hash function is not supported, %d\\n\",\n\t\t\t  ucmd.rx_hash_function);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tport = ucmd.port;\n\tif (port < 1 || port > mc->num_ports) {\n\t\tibdev_dbg(&mdev->ib_dev, \"Invalid port %u in creating qp\\n\",\n\t\t\t  port);\n\t\treturn -EINVAL;\n\t}\n\tndev = mc->ports[port - 1];\n\tmpc = netdev_priv(ndev);\n\n\tibdev_dbg(&mdev->ib_dev, \"rx_hash_function %d port %d\\n\",\n\t\t  ucmd.rx_hash_function, port);\n\n\tmana_ind_table = kcalloc(ind_tbl_size, sizeof(mana_handle_t),\n\t\t\t\t GFP_KERNEL);\n\tif (!mana_ind_table) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tqp->port = port;\n\n\tfor (i = 0; i < ind_tbl_size; i++) {\n\t\tstruct mana_obj_spec wq_spec = {};\n\t\tstruct mana_obj_spec cq_spec = {};\n\n\t\tibwq = ind_tbl->ind_tbl[i];\n\t\twq = container_of(ibwq, struct mana_ib_wq, ibwq);\n\n\t\tibcq = ibwq->cq;\n\t\tcq = container_of(ibcq, struct mana_ib_cq, ibcq);\n\n\t\twq_spec.gdma_region = wq->gdma_region;\n\t\twq_spec.queue_size = wq->wq_buf_size;\n\n\t\tcq_spec.gdma_region = cq->gdma_region;\n\t\tcq_spec.queue_size = cq->cqe * COMP_ENTRY_SIZE;\n\t\tcq_spec.modr_ctx_id = 0;\n\t\tcq_spec.attached_eq = GDMA_CQ_NO_EQ;\n\n\t\tret = mana_create_wq_obj(mpc, mpc->port_handle, GDMA_RQ,\n\t\t\t\t\t &wq_spec, &cq_spec, &wq->rx_object);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t\t \n\t\twq->gdma_region = GDMA_INVALID_DMA_REGION;\n\t\tcq->gdma_region = GDMA_INVALID_DMA_REGION;\n\n\t\twq->id = wq_spec.queue_index;\n\t\tcq->id = cq_spec.queue_index;\n\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"ret %d rx_object 0x%llx wq id %llu cq id %llu\\n\",\n\t\t\t  ret, wq->rx_object, wq->id, cq->id);\n\n\t\tresp.entries[i].cqid = cq->id;\n\t\tresp.entries[i].wqid = wq->id;\n\n\t\tmana_ind_table[i] = wq->rx_object;\n\t}\n\tresp.num_entries = i;\n\n\tret = mana_ib_cfg_vport_steering(mdev, ndev, wq->rx_object,\n\t\t\t\t\t mana_ind_table,\n\t\t\t\t\t ind_tbl->log_ind_tbl_size,\n\t\t\t\t\t ucmd.rx_hash_key_len,\n\t\t\t\t\t ucmd.rx_hash_key);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\tif (ret) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed to copy to udata create rss-qp, %d\\n\",\n\t\t\t  ret);\n\t\tgoto fail;\n\t}\n\n\tkfree(mana_ind_table);\n\n\treturn 0;\n\nfail:\n\twhile (i-- > 0) {\n\t\tibwq = ind_tbl->ind_tbl[i];\n\t\twq = container_of(ibwq, struct mana_ib_wq, ibwq);\n\t\tmana_destroy_wq_obj(mpc, GDMA_RQ, wq->rx_object);\n\t}\n\n\tkfree(mana_ind_table);\n\n\treturn ret;\n}\n\nstatic int mana_ib_create_qp_raw(struct ib_qp *ibqp, struct ib_pd *ibpd,\n\t\t\t\t struct ib_qp_init_attr *attr,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct mana_ib_pd *pd = container_of(ibpd, struct mana_ib_pd, ibpd);\n\tstruct mana_ib_qp *qp = container_of(ibqp, struct mana_ib_qp, ibqp);\n\tstruct mana_ib_dev *mdev =\n\t\tcontainer_of(ibpd->device, struct mana_ib_dev, ib_dev);\n\tstruct mana_ib_cq *send_cq =\n\t\tcontainer_of(attr->send_cq, struct mana_ib_cq, ibcq);\n\tstruct mana_ib_ucontext *mana_ucontext =\n\t\trdma_udata_to_drv_context(udata, struct mana_ib_ucontext,\n\t\t\t\t\t  ibucontext);\n\tstruct mana_ib_create_qp_resp resp = {};\n\tstruct gdma_dev *gd = mdev->gdma_dev;\n\tstruct mana_ib_create_qp ucmd = {};\n\tstruct mana_obj_spec wq_spec = {};\n\tstruct mana_obj_spec cq_spec = {};\n\tstruct mana_port_context *mpc;\n\tstruct mana_context *mc;\n\tstruct net_device *ndev;\n\tstruct ib_umem *umem;\n\tint err;\n\tu32 port;\n\n\tmc = gd->driver_data;\n\n\tif (!mana_ucontext || udata->inlen < sizeof(ucmd))\n\t\treturn -EINVAL;\n\n\terr = ib_copy_from_udata(&ucmd, udata, min(sizeof(ucmd), udata->inlen));\n\tif (err) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed to copy from udata create qp-raw, %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\tport = ucmd.port;\n\tif (port < 1 || port > mc->num_ports)\n\t\treturn -EINVAL;\n\n\tif (attr->cap.max_send_wr > MAX_SEND_BUFFERS_PER_QUEUE) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Requested max_send_wr %d exceeding limit\\n\",\n\t\t\t  attr->cap.max_send_wr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr->cap.max_send_sge > MAX_TX_WQE_SGL_ENTRIES) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Requested max_send_sge %d exceeding limit\\n\",\n\t\t\t  attr->cap.max_send_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tndev = mc->ports[port - 1];\n\tmpc = netdev_priv(ndev);\n\tibdev_dbg(&mdev->ib_dev, \"port %u ndev %p mpc %p\\n\", port, ndev, mpc);\n\n\terr = mana_ib_cfg_vport(mdev, port - 1, pd, mana_ucontext->doorbell);\n\tif (err)\n\t\treturn -ENODEV;\n\n\tqp->port = port;\n\n\tibdev_dbg(&mdev->ib_dev, \"ucmd sq_buf_addr 0x%llx port %u\\n\",\n\t\t  ucmd.sq_buf_addr, ucmd.port);\n\n\tumem = ib_umem_get(ibpd->device, ucmd.sq_buf_addr, ucmd.sq_buf_size,\n\t\t\t   IB_ACCESS_LOCAL_WRITE);\n\tif (IS_ERR(umem)) {\n\t\terr = PTR_ERR(umem);\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed to get umem for create qp-raw, err %d\\n\",\n\t\t\t  err);\n\t\tgoto err_free_vport;\n\t}\n\tqp->sq_umem = umem;\n\n\terr = mana_ib_gd_create_dma_region(mdev, qp->sq_umem,\n\t\t\t\t\t   &qp->sq_gdma_region);\n\tif (err) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed to create dma region for create qp-raw, %d\\n\",\n\t\t\t  err);\n\t\tgoto err_release_umem;\n\t}\n\n\tibdev_dbg(&mdev->ib_dev,\n\t\t  \"mana_ib_gd_create_dma_region ret %d gdma_region 0x%llx\\n\",\n\t\t  err, qp->sq_gdma_region);\n\n\t \n\twq_spec.gdma_region = qp->sq_gdma_region;\n\twq_spec.queue_size = ucmd.sq_buf_size;\n\n\tcq_spec.gdma_region = send_cq->gdma_region;\n\tcq_spec.queue_size = send_cq->cqe * COMP_ENTRY_SIZE;\n\tcq_spec.modr_ctx_id = 0;\n\tcq_spec.attached_eq = GDMA_CQ_NO_EQ;\n\n\terr = mana_create_wq_obj(mpc, mpc->port_handle, GDMA_SQ, &wq_spec,\n\t\t\t\t &cq_spec, &qp->tx_object);\n\tif (err) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed to create wq for create raw-qp, err %d\\n\",\n\t\t\t  err);\n\t\tgoto err_destroy_dma_region;\n\t}\n\n\t \n\tqp->sq_gdma_region = GDMA_INVALID_DMA_REGION;\n\tsend_cq->gdma_region = GDMA_INVALID_DMA_REGION;\n\n\tqp->sq_id = wq_spec.queue_index;\n\tsend_cq->id = cq_spec.queue_index;\n\n\tibdev_dbg(&mdev->ib_dev,\n\t\t  \"ret %d qp->tx_object 0x%llx sq id %llu cq id %llu\\n\", err,\n\t\t  qp->tx_object, qp->sq_id, send_cq->id);\n\n\tresp.sqid = qp->sq_id;\n\tresp.cqid = send_cq->id;\n\tresp.tx_vp_offset = pd->tx_vp_offset;\n\n\terr = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\tif (err) {\n\t\tibdev_dbg(&mdev->ib_dev,\n\t\t\t  \"Failed copy udata for create qp-raw, %d\\n\",\n\t\t\t  err);\n\t\tgoto err_destroy_wq_obj;\n\t}\n\n\treturn 0;\n\nerr_destroy_wq_obj:\n\tmana_destroy_wq_obj(mpc, GDMA_SQ, qp->tx_object);\n\nerr_destroy_dma_region:\n\tmana_ib_gd_destroy_dma_region(mdev, qp->sq_gdma_region);\n\nerr_release_umem:\n\tib_umem_release(umem);\n\nerr_free_vport:\n\tmana_ib_uncfg_vport(mdev, pd, port - 1);\n\n\treturn err;\n}\n\nint mana_ib_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *attr,\n\t\t      struct ib_udata *udata)\n{\n\tswitch (attr->qp_type) {\n\tcase IB_QPT_RAW_PACKET:\n\t\t \n\t\tif (attr->rwq_ind_tbl)\n\t\t\treturn mana_ib_create_qp_rss(ibqp, ibqp->pd, attr,\n\t\t\t\t\t\t     udata);\n\n\t\treturn mana_ib_create_qp_raw(ibqp, ibqp->pd, attr, udata);\n\tdefault:\n\t\t \n\t\tibdev_dbg(ibqp->device, \"Creating QP type %u not supported\\n\",\n\t\t\t  attr->qp_type);\n\t}\n\n\treturn -EINVAL;\n}\n\nint mana_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t      int attr_mask, struct ib_udata *udata)\n{\n\t \n\treturn -EOPNOTSUPP;\n}\n\nstatic int mana_ib_destroy_qp_rss(struct mana_ib_qp *qp,\n\t\t\t\t  struct ib_rwq_ind_table *ind_tbl,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct mana_ib_dev *mdev =\n\t\tcontainer_of(qp->ibqp.device, struct mana_ib_dev, ib_dev);\n\tstruct gdma_dev *gd = mdev->gdma_dev;\n\tstruct mana_port_context *mpc;\n\tstruct mana_context *mc;\n\tstruct net_device *ndev;\n\tstruct mana_ib_wq *wq;\n\tstruct ib_wq *ibwq;\n\tint i;\n\n\tmc = gd->driver_data;\n\tndev = mc->ports[qp->port - 1];\n\tmpc = netdev_priv(ndev);\n\n\tfor (i = 0; i < (1 << ind_tbl->log_ind_tbl_size); i++) {\n\t\tibwq = ind_tbl->ind_tbl[i];\n\t\twq = container_of(ibwq, struct mana_ib_wq, ibwq);\n\t\tibdev_dbg(&mdev->ib_dev, \"destroying wq->rx_object %llu\\n\",\n\t\t\t  wq->rx_object);\n\t\tmana_destroy_wq_obj(mpc, GDMA_RQ, wq->rx_object);\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_ib_destroy_qp_raw(struct mana_ib_qp *qp, struct ib_udata *udata)\n{\n\tstruct mana_ib_dev *mdev =\n\t\tcontainer_of(qp->ibqp.device, struct mana_ib_dev, ib_dev);\n\tstruct gdma_dev *gd = mdev->gdma_dev;\n\tstruct ib_pd *ibpd = qp->ibqp.pd;\n\tstruct mana_port_context *mpc;\n\tstruct mana_context *mc;\n\tstruct net_device *ndev;\n\tstruct mana_ib_pd *pd;\n\n\tmc = gd->driver_data;\n\tndev = mc->ports[qp->port - 1];\n\tmpc = netdev_priv(ndev);\n\tpd = container_of(ibpd, struct mana_ib_pd, ibpd);\n\n\tmana_destroy_wq_obj(mpc, GDMA_SQ, qp->tx_object);\n\n\tif (qp->sq_umem) {\n\t\tmana_ib_gd_destroy_dma_region(mdev, qp->sq_gdma_region);\n\t\tib_umem_release(qp->sq_umem);\n\t}\n\n\tmana_ib_uncfg_vport(mdev, pd, qp->port - 1);\n\n\treturn 0;\n}\n\nint mana_ib_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct mana_ib_qp *qp = container_of(ibqp, struct mana_ib_qp, ibqp);\n\n\tswitch (ibqp->qp_type) {\n\tcase IB_QPT_RAW_PACKET:\n\t\tif (ibqp->rwq_ind_tbl)\n\t\t\treturn mana_ib_destroy_qp_rss(qp, ibqp->rwq_ind_tbl,\n\t\t\t\t\t\t      udata);\n\n\t\treturn mana_ib_destroy_qp_raw(qp, udata);\n\n\tdefault:\n\t\tibdev_dbg(ibqp->device, \"Unexpected QP type %u\\n\",\n\t\t\t  ibqp->qp_type);\n\t}\n\n\treturn -ENOENT;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}