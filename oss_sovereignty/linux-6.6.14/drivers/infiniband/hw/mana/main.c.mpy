{
  "module_name": "main.c",
  "hash_id": "45865b228aff845d44e30d42adc1886979646bcdd8457013725860bfd954e693",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mana/main.c",
  "human_readable_source": "\n \n\n#include \"mana_ib.h\"\n\nvoid mana_ib_uncfg_vport(struct mana_ib_dev *dev, struct mana_ib_pd *pd,\n\t\t\t u32 port)\n{\n\tstruct gdma_dev *gd = dev->gdma_dev;\n\tstruct mana_port_context *mpc;\n\tstruct net_device *ndev;\n\tstruct mana_context *mc;\n\n\tmc = gd->driver_data;\n\tndev = mc->ports[port];\n\tmpc = netdev_priv(ndev);\n\n\tmutex_lock(&pd->vport_mutex);\n\n\tpd->vport_use_count--;\n\tWARN_ON(pd->vport_use_count < 0);\n\n\tif (!pd->vport_use_count)\n\t\tmana_uncfg_vport(mpc);\n\n\tmutex_unlock(&pd->vport_mutex);\n}\n\nint mana_ib_cfg_vport(struct mana_ib_dev *dev, u32 port, struct mana_ib_pd *pd,\n\t\t      u32 doorbell_id)\n{\n\tstruct gdma_dev *mdev = dev->gdma_dev;\n\tstruct mana_port_context *mpc;\n\tstruct mana_context *mc;\n\tstruct net_device *ndev;\n\tint err;\n\n\tmc = mdev->driver_data;\n\tndev = mc->ports[port];\n\tmpc = netdev_priv(ndev);\n\n\tmutex_lock(&pd->vport_mutex);\n\n\tpd->vport_use_count++;\n\tif (pd->vport_use_count > 1) {\n\t\tibdev_dbg(&dev->ib_dev,\n\t\t\t  \"Skip as this PD is already configured vport\\n\");\n\t\tmutex_unlock(&pd->vport_mutex);\n\t\treturn 0;\n\t}\n\n\terr = mana_cfg_vport(mpc, pd->pdn, doorbell_id);\n\tif (err) {\n\t\tpd->vport_use_count--;\n\t\tmutex_unlock(&pd->vport_mutex);\n\n\t\tibdev_dbg(&dev->ib_dev, \"Failed to configure vPort %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tmutex_unlock(&pd->vport_mutex);\n\n\tpd->tx_shortform_allowed = mpc->tx_shortform_allowed;\n\tpd->tx_vp_offset = mpc->tx_vp_offset;\n\n\tibdev_dbg(&dev->ib_dev, \"vport handle %llx pdid %x doorbell_id %x\\n\",\n\t\t  mpc->port_handle, pd->pdn, doorbell_id);\n\n\treturn 0;\n}\n\nint mana_ib_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct mana_ib_pd *pd = container_of(ibpd, struct mana_ib_pd, ibpd);\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct gdma_create_pd_resp resp = {};\n\tstruct gdma_create_pd_req req = {};\n\tenum gdma_pd_flags flags = 0;\n\tstruct mana_ib_dev *dev;\n\tstruct gdma_dev *mdev;\n\tint err;\n\n\tdev = container_of(ibdev, struct mana_ib_dev, ib_dev);\n\tmdev = dev->gdma_dev;\n\n\tmana_gd_init_req_hdr(&req.hdr, GDMA_CREATE_PD, sizeof(req),\n\t\t\t     sizeof(resp));\n\n\treq.flags = flags;\n\terr = mana_gd_send_request(mdev->gdma_context, sizeof(req), &req,\n\t\t\t\t   sizeof(resp), &resp);\n\n\tif (err || resp.hdr.status) {\n\t\tibdev_dbg(&dev->ib_dev,\n\t\t\t  \"Failed to get pd_id err %d status %u\\n\", err,\n\t\t\t  resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\n\t\treturn err;\n\t}\n\n\tpd->pd_handle = resp.pd_handle;\n\tpd->pdn = resp.pd_id;\n\tibdev_dbg(&dev->ib_dev, \"pd_handle 0x%llx pd_id %d\\n\",\n\t\t  pd->pd_handle, pd->pdn);\n\n\tmutex_init(&pd->vport_mutex);\n\tpd->vport_use_count = 0;\n\treturn 0;\n}\n\nint mana_ib_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct mana_ib_pd *pd = container_of(ibpd, struct mana_ib_pd, ibpd);\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct gdma_destory_pd_resp resp = {};\n\tstruct gdma_destroy_pd_req req = {};\n\tstruct mana_ib_dev *dev;\n\tstruct gdma_dev *mdev;\n\tint err;\n\n\tdev = container_of(ibdev, struct mana_ib_dev, ib_dev);\n\tmdev = dev->gdma_dev;\n\n\tmana_gd_init_req_hdr(&req.hdr, GDMA_DESTROY_PD, sizeof(req),\n\t\t\t     sizeof(resp));\n\n\treq.pd_handle = pd->pd_handle;\n\terr = mana_gd_send_request(mdev->gdma_context, sizeof(req), &req,\n\t\t\t\t   sizeof(resp), &resp);\n\n\tif (err || resp.hdr.status) {\n\t\tibdev_dbg(&dev->ib_dev,\n\t\t\t  \"Failed to destroy pd_handle 0x%llx err %d status %u\",\n\t\t\t  pd->pd_handle, err, resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\t}\n\n\treturn err;\n}\n\nstatic int mana_gd_destroy_doorbell_page(struct gdma_context *gc,\n\t\t\t\t\t int doorbell_page)\n{\n\tstruct gdma_destroy_resource_range_req req = {};\n\tstruct gdma_resp_hdr resp = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, GDMA_DESTROY_RESOURCE_RANGE,\n\t\t\t     sizeof(req), sizeof(resp));\n\n\treq.resource_type = GDMA_RESOURCE_DOORBELL_PAGE;\n\treq.num_resources = 1;\n\treq.allocated_resources = doorbell_page;\n\n\terr = mana_gd_send_request(gc, sizeof(req), &req, sizeof(resp), &resp);\n\tif (err || resp.status) {\n\t\tdev_err(gc->dev,\n\t\t\t\"Failed to destroy doorbell page: ret %d, 0x%x\\n\",\n\t\t\terr, resp.status);\n\t\treturn err ?: -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_gd_allocate_doorbell_page(struct gdma_context *gc,\n\t\t\t\t\t  int *doorbell_page)\n{\n\tstruct gdma_allocate_resource_range_req req = {};\n\tstruct gdma_allocate_resource_range_resp resp = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, GDMA_ALLOCATE_RESOURCE_RANGE,\n\t\t\t     sizeof(req), sizeof(resp));\n\n\treq.resource_type = GDMA_RESOURCE_DOORBELL_PAGE;\n\treq.num_resources = 1;\n\treq.alignment = 1;\n\n\t \n\treq.allocated_resources = 0;\n\n\terr = mana_gd_send_request(gc, sizeof(req), &req, sizeof(resp), &resp);\n\tif (err || resp.hdr.status) {\n\t\tdev_err(gc->dev,\n\t\t\t\"Failed to allocate doorbell page: ret %d, 0x%x\\n\",\n\t\t\terr, resp.hdr.status);\n\t\treturn err ?: -EPROTO;\n\t}\n\n\t*doorbell_page = resp.allocated_resources;\n\n\treturn 0;\n}\n\nint mana_ib_alloc_ucontext(struct ib_ucontext *ibcontext,\n\t\t\t   struct ib_udata *udata)\n{\n\tstruct mana_ib_ucontext *ucontext =\n\t\tcontainer_of(ibcontext, struct mana_ib_ucontext, ibucontext);\n\tstruct ib_device *ibdev = ibcontext->device;\n\tstruct mana_ib_dev *mdev;\n\tstruct gdma_context *gc;\n\tstruct gdma_dev *dev;\n\tint doorbell_page;\n\tint ret;\n\n\tmdev = container_of(ibdev, struct mana_ib_dev, ib_dev);\n\tdev = mdev->gdma_dev;\n\tgc = dev->gdma_context;\n\n\t \n\tret = mana_gd_allocate_doorbell_page(gc, &doorbell_page);\n\tif (ret) {\n\t\tibdev_dbg(ibdev, \"Failed to allocate doorbell page %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tibdev_dbg(ibdev, \"Doorbell page allocated %d\\n\", doorbell_page);\n\n\tucontext->doorbell = doorbell_page;\n\n\treturn 0;\n}\n\nvoid mana_ib_dealloc_ucontext(struct ib_ucontext *ibcontext)\n{\n\tstruct mana_ib_ucontext *mana_ucontext =\n\t\tcontainer_of(ibcontext, struct mana_ib_ucontext, ibucontext);\n\tstruct ib_device *ibdev = ibcontext->device;\n\tstruct mana_ib_dev *mdev;\n\tstruct gdma_context *gc;\n\tint ret;\n\n\tmdev = container_of(ibdev, struct mana_ib_dev, ib_dev);\n\tgc = mdev->gdma_dev->gdma_context;\n\n\tret = mana_gd_destroy_doorbell_page(gc, mana_ucontext->doorbell);\n\tif (ret)\n\t\tibdev_dbg(ibdev, \"Failed to destroy doorbell page %d\\n\", ret);\n}\n\nstatic int\nmana_ib_gd_first_dma_region(struct mana_ib_dev *dev,\n\t\t\t    struct gdma_context *gc,\n\t\t\t    struct gdma_create_dma_region_req *create_req,\n\t\t\t    size_t num_pages, mana_handle_t *gdma_region,\n\t\t\t    u32 expected_status)\n{\n\tstruct gdma_create_dma_region_resp create_resp = {};\n\tunsigned int create_req_msg_size;\n\tint err;\n\n\tcreate_req_msg_size =\n\t\tstruct_size(create_req, page_addr_list, num_pages);\n\tcreate_req->page_addr_list_len = num_pages;\n\n\terr = mana_gd_send_request(gc, create_req_msg_size, create_req,\n\t\t\t\t   sizeof(create_resp), &create_resp);\n\tif (err || create_resp.hdr.status != expected_status) {\n\t\tibdev_dbg(&dev->ib_dev,\n\t\t\t  \"Failed to create DMA region: %d, 0x%x\\n\",\n\t\t\t  err, create_resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\n\t\treturn err;\n\t}\n\n\t*gdma_region = create_resp.dma_region_handle;\n\tibdev_dbg(&dev->ib_dev, \"Created DMA region handle 0x%llx\\n\",\n\t\t  *gdma_region);\n\n\treturn 0;\n}\n\nstatic int\nmana_ib_gd_add_dma_region(struct mana_ib_dev *dev, struct gdma_context *gc,\n\t\t\t  struct gdma_dma_region_add_pages_req *add_req,\n\t\t\t  unsigned int num_pages, u32 expected_status)\n{\n\tunsigned int add_req_msg_size =\n\t\tstruct_size(add_req, page_addr_list, num_pages);\n\tstruct gdma_general_resp add_resp = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&add_req->hdr, GDMA_DMA_REGION_ADD_PAGES,\n\t\t\t     add_req_msg_size, sizeof(add_resp));\n\tadd_req->page_addr_list_len = num_pages;\n\n\terr = mana_gd_send_request(gc, add_req_msg_size, add_req,\n\t\t\t\t   sizeof(add_resp), &add_resp);\n\tif (err || add_resp.hdr.status != expected_status) {\n\t\tibdev_dbg(&dev->ib_dev,\n\t\t\t  \"Failed to create DMA region: %d, 0x%x\\n\",\n\t\t\t  err, add_resp.hdr.status);\n\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint mana_ib_gd_create_dma_region(struct mana_ib_dev *dev, struct ib_umem *umem,\n\t\t\t\t mana_handle_t *gdma_region)\n{\n\tstruct gdma_dma_region_add_pages_req *add_req = NULL;\n\tsize_t num_pages_processed = 0, num_pages_to_handle;\n\tstruct gdma_create_dma_region_req *create_req;\n\tunsigned int create_req_msg_size;\n\tstruct hw_channel_context *hwc;\n\tstruct ib_block_iter biter;\n\tsize_t max_pgs_add_cmd = 0;\n\tsize_t max_pgs_create_cmd;\n\tstruct gdma_context *gc;\n\tsize_t num_pages_total;\n\tstruct gdma_dev *mdev;\n\tunsigned long page_sz;\n\tunsigned int tail = 0;\n\tu64 *page_addr_list;\n\tvoid *request_buf;\n\tint err;\n\n\tmdev = dev->gdma_dev;\n\tgc = mdev->gdma_context;\n\thwc = gc->hwc.driver_data;\n\n\t \n\tpage_sz = ib_umem_find_best_pgsz(umem, PAGE_SZ_BM, 0);\n\tif (!page_sz) {\n\t\tibdev_dbg(&dev->ib_dev, \"failed to find page size.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tnum_pages_total = ib_umem_num_dma_blocks(umem, page_sz);\n\n\tmax_pgs_create_cmd =\n\t\t(hwc->max_req_msg_size - sizeof(*create_req)) / sizeof(u64);\n\tnum_pages_to_handle =\n\t\tmin_t(size_t, num_pages_total, max_pgs_create_cmd);\n\tcreate_req_msg_size =\n\t\tstruct_size(create_req, page_addr_list, num_pages_to_handle);\n\n\trequest_buf = kzalloc(hwc->max_req_msg_size, GFP_KERNEL);\n\tif (!request_buf)\n\t\treturn -ENOMEM;\n\n\tcreate_req = request_buf;\n\tmana_gd_init_req_hdr(&create_req->hdr, GDMA_CREATE_DMA_REGION,\n\t\t\t     create_req_msg_size,\n\t\t\t     sizeof(struct gdma_create_dma_region_resp));\n\n\tcreate_req->length = umem->length;\n\tcreate_req->offset_in_page = umem->address & (page_sz - 1);\n\tcreate_req->gdma_page_type = order_base_2(page_sz) - PAGE_SHIFT;\n\tcreate_req->page_count = num_pages_total;\n\n\tibdev_dbg(&dev->ib_dev, \"size_dma_region %lu num_pages_total %lu\\n\",\n\t\t  umem->length, num_pages_total);\n\n\tibdev_dbg(&dev->ib_dev, \"page_sz %lu offset_in_page %u\\n\",\n\t\t  page_sz, create_req->offset_in_page);\n\n\tibdev_dbg(&dev->ib_dev, \"num_pages_to_handle %lu, gdma_page_type %u\",\n\t\t  num_pages_to_handle, create_req->gdma_page_type);\n\n\tpage_addr_list = create_req->page_addr_list;\n\trdma_umem_for_each_dma_block(umem, &biter, page_sz) {\n\t\tu32 expected_status = 0;\n\n\t\tpage_addr_list[tail++] = rdma_block_iter_dma_address(&biter);\n\t\tif (tail < num_pages_to_handle)\n\t\t\tcontinue;\n\n\t\tif (num_pages_processed + num_pages_to_handle <\n\t\t    num_pages_total)\n\t\t\texpected_status = GDMA_STATUS_MORE_ENTRIES;\n\n\t\tif (!num_pages_processed) {\n\t\t\t \n\t\t\terr = mana_ib_gd_first_dma_region(dev, gc, create_req,\n\t\t\t\t\t\t\t  tail, gdma_region,\n\t\t\t\t\t\t\t  expected_status);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tmax_pgs_add_cmd = (hwc->max_req_msg_size -\n\t\t\t\tsizeof(*add_req)) / sizeof(u64);\n\n\t\t\tadd_req = request_buf;\n\t\t\tadd_req->dma_region_handle = *gdma_region;\n\t\t\tadd_req->reserved3 = 0;\n\t\t\tpage_addr_list = add_req->page_addr_list;\n\t\t} else {\n\t\t\t \n\t\t\terr = mana_ib_gd_add_dma_region(dev, gc, add_req, tail,\n\t\t\t\t\t\t\texpected_status);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tnum_pages_processed += tail;\n\t\ttail = 0;\n\n\t\t \n\t\tnum_pages_to_handle =\n\t\t\tmin_t(size_t,\n\t\t\t      num_pages_total - num_pages_processed,\n\t\t\t      max_pgs_add_cmd);\n\t}\n\n\tif (err)\n\t\tmana_ib_gd_destroy_dma_region(dev, *gdma_region);\n\nout:\n\tkfree(request_buf);\n\treturn err;\n}\n\nint mana_ib_gd_destroy_dma_region(struct mana_ib_dev *dev, u64 gdma_region)\n{\n\tstruct gdma_dev *mdev = dev->gdma_dev;\n\tstruct gdma_context *gc;\n\n\tgc = mdev->gdma_context;\n\tibdev_dbg(&dev->ib_dev, \"destroy dma region 0x%llx\\n\", gdma_region);\n\n\treturn mana_gd_destroy_dma_region(gc, gdma_region);\n}\n\nint mana_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)\n{\n\tstruct mana_ib_ucontext *mana_ucontext =\n\t\tcontainer_of(ibcontext, struct mana_ib_ucontext, ibucontext);\n\tstruct ib_device *ibdev = ibcontext->device;\n\tstruct mana_ib_dev *mdev;\n\tstruct gdma_context *gc;\n\tphys_addr_t pfn;\n\tpgprot_t prot;\n\tint ret;\n\n\tmdev = container_of(ibdev, struct mana_ib_dev, ib_dev);\n\tgc = mdev->gdma_dev->gdma_context;\n\n\tif (vma->vm_pgoff != 0) {\n\t\tibdev_dbg(ibdev, \"Unexpected vm_pgoff %lu\\n\", vma->vm_pgoff);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tpfn = (gc->phys_db_page_base +\n\t       gc->db_page_size * mana_ucontext->doorbell) >>\n\t      PAGE_SHIFT;\n\tprot = pgprot_writecombine(vma->vm_page_prot);\n\n\tret = rdma_user_mmap_io(ibcontext, vma, pfn, gc->db_page_size, prot,\n\t\t\t\tNULL);\n\tif (ret)\n\t\tibdev_dbg(ibdev, \"can't rdma_user_mmap_io ret %d\\n\", ret);\n\telse\n\t\tibdev_dbg(ibdev, \"mapped I/O pfn 0x%llx page_size %u, ret %d\\n\",\n\t\t\t  pfn, gc->db_page_size, ret);\n\n\treturn ret;\n}\n\nint mana_ib_get_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t       struct ib_port_immutable *immutable)\n{\n\t \n\timmutable->core_cap_flags = RDMA_CORE_PORT_RAW_PACKET;\n\n\treturn 0;\n}\n\nint mana_ib_query_device(struct ib_device *ibdev, struct ib_device_attr *props,\n\t\t\t struct ib_udata *uhw)\n{\n\tprops->max_qp = MANA_MAX_NUM_QUEUES;\n\tprops->max_qp_wr = MAX_SEND_BUFFERS_PER_QUEUE;\n\n\t \n\tprops->max_cqe = MAX_SEND_BUFFERS_PER_QUEUE;\n\n\tprops->max_mr_size = MANA_IB_MAX_MR_SIZE;\n\tprops->max_mr = MANA_IB_MAX_MR;\n\tprops->max_send_sge = MAX_TX_WQE_SGL_ENTRIES;\n\tprops->max_recv_sge = MAX_RX_WQE_SGL_ENTRIES;\n\n\treturn 0;\n}\n\nint mana_ib_query_port(struct ib_device *ibdev, u32 port,\n\t\t       struct ib_port_attr *props)\n{\n\t \n\treturn 0;\n}\n\nint mana_ib_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t      union ib_gid *gid)\n{\n\t \n\treturn 0;\n}\n\nvoid mana_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)\n{\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}