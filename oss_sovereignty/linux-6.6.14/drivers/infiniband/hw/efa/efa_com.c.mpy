{
  "module_name": "efa_com.c",
  "hash_id": "17d8a6c2ac42479eb8b59e102a440d4159e8a162a1bf1530d3fc304481f967e7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/efa/efa_com.c",
  "human_readable_source": "\n \n\n#include \"efa_com.h\"\n#include \"efa_regs_defs.h\"\n\n#define ADMIN_CMD_TIMEOUT_US 30000000  \n\n#define EFA_REG_READ_TIMEOUT_US 50000  \n#define EFA_MMIO_READ_INVALID 0xffffffff\n\n#define EFA_POLL_INTERVAL_MS 100  \n\n#define EFA_ASYNC_QUEUE_DEPTH 16\n#define EFA_ADMIN_QUEUE_DEPTH 32\n\n#define EFA_CTRL_MAJOR          0\n#define EFA_CTRL_MINOR          0\n#define EFA_CTRL_SUB_MINOR      1\n\nenum efa_cmd_status {\n\tEFA_CMD_SUBMITTED,\n\tEFA_CMD_COMPLETED,\n};\n\nstruct efa_comp_ctx {\n\tstruct completion wait_event;\n\tstruct efa_admin_acq_entry *user_cqe;\n\tu32 comp_size;\n\tenum efa_cmd_status status;\n\tu8 cmd_opcode;\n\tu8 occupied;\n};\n\nstatic const char *efa_com_cmd_str(u8 cmd)\n{\n#define EFA_CMD_STR_CASE(_cmd) case EFA_ADMIN_##_cmd: return #_cmd\n\n\tswitch (cmd) {\n\tEFA_CMD_STR_CASE(CREATE_QP);\n\tEFA_CMD_STR_CASE(MODIFY_QP);\n\tEFA_CMD_STR_CASE(QUERY_QP);\n\tEFA_CMD_STR_CASE(DESTROY_QP);\n\tEFA_CMD_STR_CASE(CREATE_AH);\n\tEFA_CMD_STR_CASE(DESTROY_AH);\n\tEFA_CMD_STR_CASE(REG_MR);\n\tEFA_CMD_STR_CASE(DEREG_MR);\n\tEFA_CMD_STR_CASE(CREATE_CQ);\n\tEFA_CMD_STR_CASE(DESTROY_CQ);\n\tEFA_CMD_STR_CASE(GET_FEATURE);\n\tEFA_CMD_STR_CASE(SET_FEATURE);\n\tEFA_CMD_STR_CASE(GET_STATS);\n\tEFA_CMD_STR_CASE(ALLOC_PD);\n\tEFA_CMD_STR_CASE(DEALLOC_PD);\n\tEFA_CMD_STR_CASE(ALLOC_UAR);\n\tEFA_CMD_STR_CASE(DEALLOC_UAR);\n\tEFA_CMD_STR_CASE(CREATE_EQ);\n\tEFA_CMD_STR_CASE(DESTROY_EQ);\n\tdefault: return \"unknown command opcode\";\n\t}\n#undef EFA_CMD_STR_CASE\n}\n\nvoid efa_com_set_dma_addr(dma_addr_t addr, u32 *addr_high, u32 *addr_low)\n{\n\t*addr_low = lower_32_bits(addr);\n\t*addr_high = upper_32_bits(addr);\n}\n\nstatic u32 efa_com_reg_read32(struct efa_com_dev *edev, u16 offset)\n{\n\tstruct efa_com_mmio_read *mmio_read = &edev->mmio_read;\n\tstruct efa_admin_mmio_req_read_less_resp *read_resp;\n\tunsigned long exp_time;\n\tu32 mmio_read_reg = 0;\n\tu32 err;\n\n\tread_resp = mmio_read->read_resp;\n\n\tspin_lock(&mmio_read->lock);\n\tmmio_read->seq_num++;\n\n\t \n\tread_resp->req_id = mmio_read->seq_num + 0x9aL;\n\tEFA_SET(&mmio_read_reg, EFA_REGS_MMIO_REG_READ_REG_OFF, offset);\n\tEFA_SET(&mmio_read_reg, EFA_REGS_MMIO_REG_READ_REQ_ID,\n\t\tmmio_read->seq_num);\n\n\twritel(mmio_read_reg, edev->reg_bar + EFA_REGS_MMIO_REG_READ_OFF);\n\n\texp_time = jiffies + usecs_to_jiffies(mmio_read->mmio_read_timeout);\n\tdo {\n\t\tif (READ_ONCE(read_resp->req_id) == mmio_read->seq_num)\n\t\t\tbreak;\n\t\tudelay(1);\n\t} while (time_is_after_jiffies(exp_time));\n\n\tif (read_resp->req_id != mmio_read->seq_num) {\n\t\tibdev_err_ratelimited(\n\t\t\tedev->efa_dev,\n\t\t\t\"Reading register timed out. expected: req id[%u] offset[%#x] actual: req id[%u] offset[%#x]\\n\",\n\t\t\tmmio_read->seq_num, offset, read_resp->req_id,\n\t\t\tread_resp->reg_off);\n\t\terr = EFA_MMIO_READ_INVALID;\n\t\tgoto out;\n\t}\n\n\tif (read_resp->reg_off != offset) {\n\t\tibdev_err_ratelimited(\n\t\t\tedev->efa_dev,\n\t\t\t\"Reading register failed: wrong offset provided\\n\");\n\t\terr = EFA_MMIO_READ_INVALID;\n\t\tgoto out;\n\t}\n\n\terr = read_resp->reg_val;\nout:\n\tspin_unlock(&mmio_read->lock);\n\treturn err;\n}\n\nstatic int efa_com_admin_init_sq(struct efa_com_dev *edev)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tstruct efa_com_admin_sq *sq = &aq->sq;\n\tu16 size = aq->depth * sizeof(*sq->entries);\n\tu32 aq_caps = 0;\n\tu32 addr_high;\n\tu32 addr_low;\n\n\tsq->entries =\n\t\tdma_alloc_coherent(aq->dmadev, size, &sq->dma_addr, GFP_KERNEL);\n\tif (!sq->entries)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&sq->lock);\n\n\tsq->cc = 0;\n\tsq->pc = 0;\n\tsq->phase = 1;\n\n\tsq->db_addr = (u32 __iomem *)(edev->reg_bar + EFA_REGS_AQ_PROD_DB_OFF);\n\n\taddr_high = upper_32_bits(sq->dma_addr);\n\taddr_low = lower_32_bits(sq->dma_addr);\n\n\twritel(addr_low, edev->reg_bar + EFA_REGS_AQ_BASE_LO_OFF);\n\twritel(addr_high, edev->reg_bar + EFA_REGS_AQ_BASE_HI_OFF);\n\n\tEFA_SET(&aq_caps, EFA_REGS_AQ_CAPS_AQ_DEPTH, aq->depth);\n\tEFA_SET(&aq_caps, EFA_REGS_AQ_CAPS_AQ_ENTRY_SIZE,\n\t\tsizeof(struct efa_admin_aq_entry));\n\n\twritel(aq_caps, edev->reg_bar + EFA_REGS_AQ_CAPS_OFF);\n\n\treturn 0;\n}\n\nstatic int efa_com_admin_init_cq(struct efa_com_dev *edev)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tstruct efa_com_admin_cq *cq = &aq->cq;\n\tu16 size = aq->depth * sizeof(*cq->entries);\n\tu32 acq_caps = 0;\n\tu32 addr_high;\n\tu32 addr_low;\n\n\tcq->entries =\n\t\tdma_alloc_coherent(aq->dmadev, size, &cq->dma_addr, GFP_KERNEL);\n\tif (!cq->entries)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&cq->lock);\n\n\tcq->cc = 0;\n\tcq->phase = 1;\n\n\taddr_high = upper_32_bits(cq->dma_addr);\n\taddr_low = lower_32_bits(cq->dma_addr);\n\n\twritel(addr_low, edev->reg_bar + EFA_REGS_ACQ_BASE_LO_OFF);\n\twritel(addr_high, edev->reg_bar + EFA_REGS_ACQ_BASE_HI_OFF);\n\n\tEFA_SET(&acq_caps, EFA_REGS_ACQ_CAPS_ACQ_DEPTH, aq->depth);\n\tEFA_SET(&acq_caps, EFA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE,\n\t\tsizeof(struct efa_admin_acq_entry));\n\tEFA_SET(&acq_caps, EFA_REGS_ACQ_CAPS_ACQ_MSIX_VECTOR,\n\t\taq->msix_vector_idx);\n\n\twritel(acq_caps, edev->reg_bar + EFA_REGS_ACQ_CAPS_OFF);\n\n\treturn 0;\n}\n\nstatic int efa_com_admin_init_aenq(struct efa_com_dev *edev,\n\t\t\t\t   struct efa_aenq_handlers *aenq_handlers)\n{\n\tstruct efa_com_aenq *aenq = &edev->aenq;\n\tu32 addr_low, addr_high;\n\tu32 aenq_caps = 0;\n\tu16 size;\n\n\tif (!aenq_handlers) {\n\t\tibdev_err(edev->efa_dev, \"aenq handlers pointer is NULL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsize = EFA_ASYNC_QUEUE_DEPTH * sizeof(*aenq->entries);\n\taenq->entries = dma_alloc_coherent(edev->dmadev, size, &aenq->dma_addr,\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!aenq->entries)\n\t\treturn -ENOMEM;\n\n\taenq->aenq_handlers = aenq_handlers;\n\taenq->depth = EFA_ASYNC_QUEUE_DEPTH;\n\taenq->cc = 0;\n\taenq->phase = 1;\n\n\taddr_low = lower_32_bits(aenq->dma_addr);\n\taddr_high = upper_32_bits(aenq->dma_addr);\n\n\twritel(addr_low, edev->reg_bar + EFA_REGS_AENQ_BASE_LO_OFF);\n\twritel(addr_high, edev->reg_bar + EFA_REGS_AENQ_BASE_HI_OFF);\n\n\tEFA_SET(&aenq_caps, EFA_REGS_AENQ_CAPS_AENQ_DEPTH, aenq->depth);\n\tEFA_SET(&aenq_caps, EFA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE,\n\t\tsizeof(struct efa_admin_aenq_entry));\n\tEFA_SET(&aenq_caps, EFA_REGS_AENQ_CAPS_AENQ_MSIX_VECTOR,\n\t\taenq->msix_vector_idx);\n\twritel(aenq_caps, edev->reg_bar + EFA_REGS_AENQ_CAPS_OFF);\n\n\t \n\twritel(edev->aenq.cc, edev->reg_bar + EFA_REGS_AENQ_CONS_DB_OFF);\n\n\treturn 0;\n}\n\n \nstatic u16 efa_com_alloc_ctx_id(struct efa_com_admin_queue *aq)\n{\n\tu16 ctx_id;\n\n\tspin_lock(&aq->comp_ctx_lock);\n\tctx_id = aq->comp_ctx_pool[aq->comp_ctx_pool_next];\n\taq->comp_ctx_pool_next++;\n\tspin_unlock(&aq->comp_ctx_lock);\n\n\treturn ctx_id;\n}\n\nstatic void efa_com_dealloc_ctx_id(struct efa_com_admin_queue *aq,\n\t\t\t\t   u16 ctx_id)\n{\n\tspin_lock(&aq->comp_ctx_lock);\n\taq->comp_ctx_pool_next--;\n\taq->comp_ctx_pool[aq->comp_ctx_pool_next] = ctx_id;\n\tspin_unlock(&aq->comp_ctx_lock);\n}\n\nstatic inline void efa_com_put_comp_ctx(struct efa_com_admin_queue *aq,\n\t\t\t\t\tstruct efa_comp_ctx *comp_ctx)\n{\n\tu16 cmd_id = EFA_GET(&comp_ctx->user_cqe->acq_common_descriptor.command,\n\t\t\t     EFA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID);\n\tu16 ctx_id = cmd_id & (aq->depth - 1);\n\n\tibdev_dbg(aq->efa_dev, \"Put completion command_id %#x\\n\", cmd_id);\n\tcomp_ctx->occupied = 0;\n\tefa_com_dealloc_ctx_id(aq, ctx_id);\n}\n\nstatic struct efa_comp_ctx *efa_com_get_comp_ctx(struct efa_com_admin_queue *aq,\n\t\t\t\t\t\t u16 cmd_id, bool capture)\n{\n\tu16 ctx_id = cmd_id & (aq->depth - 1);\n\n\tif (aq->comp_ctx[ctx_id].occupied && capture) {\n\t\tibdev_err_ratelimited(\n\t\t\taq->efa_dev,\n\t\t\t\"Completion context for command_id %#x is occupied\\n\",\n\t\t\tcmd_id);\n\t\treturn NULL;\n\t}\n\n\tif (capture) {\n\t\taq->comp_ctx[ctx_id].occupied = 1;\n\t\tibdev_dbg(aq->efa_dev,\n\t\t\t  \"Take completion ctxt for command_id %#x\\n\", cmd_id);\n\t}\n\n\treturn &aq->comp_ctx[ctx_id];\n}\n\nstatic struct efa_comp_ctx *__efa_com_submit_admin_cmd(struct efa_com_admin_queue *aq,\n\t\t\t\t\t\t       struct efa_admin_aq_entry *cmd,\n\t\t\t\t\t\t       size_t cmd_size_in_bytes,\n\t\t\t\t\t\t       struct efa_admin_acq_entry *comp,\n\t\t\t\t\t\t       size_t comp_size_in_bytes)\n{\n\tstruct efa_admin_aq_entry *aqe;\n\tstruct efa_comp_ctx *comp_ctx;\n\tu16 queue_size_mask;\n\tu16 cmd_id;\n\tu16 ctx_id;\n\tu16 pi;\n\n\tqueue_size_mask = aq->depth - 1;\n\tpi = aq->sq.pc & queue_size_mask;\n\n\tctx_id = efa_com_alloc_ctx_id(aq);\n\n\t \n\tcmd_id = ctx_id & queue_size_mask;\n\tcmd_id |= aq->sq.pc & ~queue_size_mask;\n\tcmd_id &= EFA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK;\n\n\tcmd->aq_common_descriptor.command_id = cmd_id;\n\tEFA_SET(&cmd->aq_common_descriptor.flags,\n\t\tEFA_ADMIN_AQ_COMMON_DESC_PHASE, aq->sq.phase);\n\n\tcomp_ctx = efa_com_get_comp_ctx(aq, cmd_id, true);\n\tif (!comp_ctx) {\n\t\tefa_com_dealloc_ctx_id(aq, ctx_id);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tcomp_ctx->status = EFA_CMD_SUBMITTED;\n\tcomp_ctx->comp_size = comp_size_in_bytes;\n\tcomp_ctx->user_cqe = comp;\n\tcomp_ctx->cmd_opcode = cmd->aq_common_descriptor.opcode;\n\n\treinit_completion(&comp_ctx->wait_event);\n\n\taqe = &aq->sq.entries[pi];\n\tmemset(aqe, 0, sizeof(*aqe));\n\tmemcpy(aqe, cmd, cmd_size_in_bytes);\n\n\taq->sq.pc++;\n\tatomic64_inc(&aq->stats.submitted_cmd);\n\n\tif ((aq->sq.pc & queue_size_mask) == 0)\n\t\taq->sq.phase = !aq->sq.phase;\n\n\t \n\twritel(aq->sq.pc, aq->sq.db_addr);\n\n\treturn comp_ctx;\n}\n\nstatic inline int efa_com_init_comp_ctxt(struct efa_com_admin_queue *aq)\n{\n\tsize_t pool_size = aq->depth * sizeof(*aq->comp_ctx_pool);\n\tsize_t size = aq->depth * sizeof(struct efa_comp_ctx);\n\tstruct efa_comp_ctx *comp_ctx;\n\tu16 i;\n\n\taq->comp_ctx = devm_kzalloc(aq->dmadev, size, GFP_KERNEL);\n\taq->comp_ctx_pool = devm_kzalloc(aq->dmadev, pool_size, GFP_KERNEL);\n\tif (!aq->comp_ctx || !aq->comp_ctx_pool) {\n\t\tdevm_kfree(aq->dmadev, aq->comp_ctx_pool);\n\t\tdevm_kfree(aq->dmadev, aq->comp_ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < aq->depth; i++) {\n\t\tcomp_ctx = efa_com_get_comp_ctx(aq, i, false);\n\t\tif (comp_ctx)\n\t\t\tinit_completion(&comp_ctx->wait_event);\n\n\t\taq->comp_ctx_pool[i] = i;\n\t}\n\n\tspin_lock_init(&aq->comp_ctx_lock);\n\n\taq->comp_ctx_pool_next = 0;\n\n\treturn 0;\n}\n\nstatic struct efa_comp_ctx *efa_com_submit_admin_cmd(struct efa_com_admin_queue *aq,\n\t\t\t\t\t\t     struct efa_admin_aq_entry *cmd,\n\t\t\t\t\t\t     size_t cmd_size_in_bytes,\n\t\t\t\t\t\t     struct efa_admin_acq_entry *comp,\n\t\t\t\t\t\t     size_t comp_size_in_bytes)\n{\n\tstruct efa_comp_ctx *comp_ctx;\n\n\tspin_lock(&aq->sq.lock);\n\tif (!test_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state)) {\n\t\tibdev_err_ratelimited(aq->efa_dev, \"Admin queue is closed\\n\");\n\t\tspin_unlock(&aq->sq.lock);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tcomp_ctx = __efa_com_submit_admin_cmd(aq, cmd, cmd_size_in_bytes, comp,\n\t\t\t\t\t      comp_size_in_bytes);\n\tspin_unlock(&aq->sq.lock);\n\tif (IS_ERR(comp_ctx))\n\t\tclear_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\n\treturn comp_ctx;\n}\n\nstatic void efa_com_handle_single_admin_completion(struct efa_com_admin_queue *aq,\n\t\t\t\t\t\t   struct efa_admin_acq_entry *cqe)\n{\n\tstruct efa_comp_ctx *comp_ctx;\n\tu16 cmd_id;\n\n\tcmd_id = EFA_GET(&cqe->acq_common_descriptor.command,\n\t\t\t EFA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID);\n\n\tcomp_ctx = efa_com_get_comp_ctx(aq, cmd_id, false);\n\tif (!comp_ctx) {\n\t\tibdev_err(aq->efa_dev,\n\t\t\t  \"comp_ctx is NULL. Changing the admin queue running state\\n\");\n\t\tclear_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\t\treturn;\n\t}\n\n\tcomp_ctx->status = EFA_CMD_COMPLETED;\n\tmemcpy(comp_ctx->user_cqe, cqe, comp_ctx->comp_size);\n\n\tif (!test_bit(EFA_AQ_STATE_POLLING_BIT, &aq->state))\n\t\tcomplete(&comp_ctx->wait_event);\n}\n\nstatic void efa_com_handle_admin_completion(struct efa_com_admin_queue *aq)\n{\n\tstruct efa_admin_acq_entry *cqe;\n\tu16 queue_size_mask;\n\tu16 comp_num = 0;\n\tu8 phase;\n\tu16 ci;\n\n\tqueue_size_mask = aq->depth - 1;\n\n\tci = aq->cq.cc & queue_size_mask;\n\tphase = aq->cq.phase;\n\n\tcqe = &aq->cq.entries[ci];\n\n\t \n\twhile ((READ_ONCE(cqe->acq_common_descriptor.flags) &\n\t\tEFA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK) == phase) {\n\t\t \n\t\tdma_rmb();\n\t\tefa_com_handle_single_admin_completion(aq, cqe);\n\n\t\tci++;\n\t\tcomp_num++;\n\t\tif (ci == aq->depth) {\n\t\t\tci = 0;\n\t\t\tphase = !phase;\n\t\t}\n\n\t\tcqe = &aq->cq.entries[ci];\n\t}\n\n\taq->cq.cc += comp_num;\n\taq->cq.phase = phase;\n\taq->sq.cc += comp_num;\n\tatomic64_add(comp_num, &aq->stats.completed_cmd);\n}\n\nstatic int efa_com_comp_status_to_errno(u8 comp_status)\n{\n\tswitch (comp_status) {\n\tcase EFA_ADMIN_SUCCESS:\n\t\treturn 0;\n\tcase EFA_ADMIN_RESOURCE_ALLOCATION_FAILURE:\n\t\treturn -ENOMEM;\n\tcase EFA_ADMIN_UNSUPPORTED_OPCODE:\n\t\treturn -EOPNOTSUPP;\n\tcase EFA_ADMIN_BAD_OPCODE:\n\tcase EFA_ADMIN_MALFORMED_REQUEST:\n\tcase EFA_ADMIN_ILLEGAL_PARAMETER:\n\tcase EFA_ADMIN_UNKNOWN_ERROR:\n\t\treturn -EINVAL;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int efa_com_wait_and_process_admin_cq_polling(struct efa_comp_ctx *comp_ctx,\n\t\t\t\t\t\t     struct efa_com_admin_queue *aq)\n{\n\tunsigned long timeout;\n\tunsigned long flags;\n\tint err;\n\n\ttimeout = jiffies + usecs_to_jiffies(aq->completion_timeout);\n\n\twhile (1) {\n\t\tspin_lock_irqsave(&aq->cq.lock, flags);\n\t\tefa_com_handle_admin_completion(aq);\n\t\tspin_unlock_irqrestore(&aq->cq.lock, flags);\n\n\t\tif (comp_ctx->status != EFA_CMD_SUBMITTED)\n\t\t\tbreak;\n\n\t\tif (time_is_before_jiffies(timeout)) {\n\t\t\tibdev_err_ratelimited(\n\t\t\t\taq->efa_dev,\n\t\t\t\t\"Wait for completion (polling) timeout\\n\");\n\t\t\t \n\t\t\tatomic64_inc(&aq->stats.no_completion);\n\n\t\t\tclear_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\t\t\terr = -ETIME;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmsleep(aq->poll_interval);\n\t}\n\n\terr = efa_com_comp_status_to_errno(comp_ctx->user_cqe->acq_common_descriptor.status);\nout:\n\tefa_com_put_comp_ctx(aq, comp_ctx);\n\treturn err;\n}\n\nstatic int efa_com_wait_and_process_admin_cq_interrupts(struct efa_comp_ctx *comp_ctx,\n\t\t\t\t\t\t\tstruct efa_com_admin_queue *aq)\n{\n\tunsigned long flags;\n\tint err;\n\n\twait_for_completion_timeout(&comp_ctx->wait_event,\n\t\t\t\t    usecs_to_jiffies(aq->completion_timeout));\n\n\t \n\tif (comp_ctx->status == EFA_CMD_SUBMITTED) {\n\t\tspin_lock_irqsave(&aq->cq.lock, flags);\n\t\tefa_com_handle_admin_completion(aq);\n\t\tspin_unlock_irqrestore(&aq->cq.lock, flags);\n\n\t\tatomic64_inc(&aq->stats.no_completion);\n\n\t\tif (comp_ctx->status == EFA_CMD_COMPLETED)\n\t\t\tibdev_err_ratelimited(\n\t\t\t\taq->efa_dev,\n\t\t\t\t\"The device sent a completion but the driver didn't receive any MSI-X interrupt for admin cmd %s(%d) status %d (ctx: 0x%p, sq producer: %d, sq consumer: %d, cq consumer: %d)\\n\",\n\t\t\t\tefa_com_cmd_str(comp_ctx->cmd_opcode),\n\t\t\t\tcomp_ctx->cmd_opcode, comp_ctx->status,\n\t\t\t\tcomp_ctx, aq->sq.pc, aq->sq.cc, aq->cq.cc);\n\t\telse\n\t\t\tibdev_err_ratelimited(\n\t\t\t\taq->efa_dev,\n\t\t\t\t\"The device didn't send any completion for admin cmd %s(%d) status %d (ctx 0x%p, sq producer: %d, sq consumer: %d, cq consumer: %d)\\n\",\n\t\t\t\tefa_com_cmd_str(comp_ctx->cmd_opcode),\n\t\t\t\tcomp_ctx->cmd_opcode, comp_ctx->status,\n\t\t\t\tcomp_ctx, aq->sq.pc, aq->sq.cc, aq->cq.cc);\n\n\t\tclear_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\t\terr = -ETIME;\n\t\tgoto out;\n\t}\n\n\terr = efa_com_comp_status_to_errno(comp_ctx->user_cqe->acq_common_descriptor.status);\nout:\n\tefa_com_put_comp_ctx(aq, comp_ctx);\n\treturn err;\n}\n\n \nstatic int efa_com_wait_and_process_admin_cq(struct efa_comp_ctx *comp_ctx,\n\t\t\t\t\t     struct efa_com_admin_queue *aq)\n{\n\tif (test_bit(EFA_AQ_STATE_POLLING_BIT, &aq->state))\n\t\treturn efa_com_wait_and_process_admin_cq_polling(comp_ctx, aq);\n\n\treturn efa_com_wait_and_process_admin_cq_interrupts(comp_ctx, aq);\n}\n\n \nint efa_com_cmd_exec(struct efa_com_admin_queue *aq,\n\t\t     struct efa_admin_aq_entry *cmd,\n\t\t     size_t cmd_size,\n\t\t     struct efa_admin_acq_entry *comp,\n\t\t     size_t comp_size)\n{\n\tstruct efa_comp_ctx *comp_ctx;\n\tint err;\n\n\tmight_sleep();\n\n\t \n\tdown(&aq->avail_cmds);\n\n\tibdev_dbg(aq->efa_dev, \"%s (opcode %d)\\n\",\n\t\t  efa_com_cmd_str(cmd->aq_common_descriptor.opcode),\n\t\t  cmd->aq_common_descriptor.opcode);\n\tcomp_ctx = efa_com_submit_admin_cmd(aq, cmd, cmd_size, comp, comp_size);\n\tif (IS_ERR(comp_ctx)) {\n\t\tibdev_err_ratelimited(\n\t\t\taq->efa_dev,\n\t\t\t\"Failed to submit command %s (opcode %u) err %ld\\n\",\n\t\t\tefa_com_cmd_str(cmd->aq_common_descriptor.opcode),\n\t\t\tcmd->aq_common_descriptor.opcode, PTR_ERR(comp_ctx));\n\n\t\tup(&aq->avail_cmds);\n\t\tatomic64_inc(&aq->stats.cmd_err);\n\t\treturn PTR_ERR(comp_ctx);\n\t}\n\n\terr = efa_com_wait_and_process_admin_cq(comp_ctx, aq);\n\tif (err) {\n\t\tibdev_err_ratelimited(\n\t\t\taq->efa_dev,\n\t\t\t\"Failed to process command %s (opcode %u) comp_status %d err %d\\n\",\n\t\t\tefa_com_cmd_str(cmd->aq_common_descriptor.opcode),\n\t\t\tcmd->aq_common_descriptor.opcode,\n\t\t\tcomp_ctx->user_cqe->acq_common_descriptor.status, err);\n\t\tatomic64_inc(&aq->stats.cmd_err);\n\t}\n\n\tup(&aq->avail_cmds);\n\n\treturn err;\n}\n\n \nvoid efa_com_admin_destroy(struct efa_com_dev *edev)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tstruct efa_com_aenq *aenq = &edev->aenq;\n\tstruct efa_com_admin_cq *cq = &aq->cq;\n\tstruct efa_com_admin_sq *sq = &aq->sq;\n\tu16 size;\n\n\tclear_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\n\tdevm_kfree(edev->dmadev, aq->comp_ctx_pool);\n\tdevm_kfree(edev->dmadev, aq->comp_ctx);\n\n\tsize = aq->depth * sizeof(*sq->entries);\n\tdma_free_coherent(edev->dmadev, size, sq->entries, sq->dma_addr);\n\n\tsize = aq->depth * sizeof(*cq->entries);\n\tdma_free_coherent(edev->dmadev, size, cq->entries, cq->dma_addr);\n\n\tsize = aenq->depth * sizeof(*aenq->entries);\n\tdma_free_coherent(edev->dmadev, size, aenq->entries, aenq->dma_addr);\n}\n\n \nvoid efa_com_set_admin_polling_mode(struct efa_com_dev *edev, bool polling)\n{\n\tu32 mask_value = 0;\n\n\tif (polling)\n\t\tEFA_SET(&mask_value, EFA_REGS_INTR_MASK_EN, 1);\n\n\twritel(mask_value, edev->reg_bar + EFA_REGS_INTR_MASK_OFF);\n\tif (polling)\n\t\tset_bit(EFA_AQ_STATE_POLLING_BIT, &edev->aq.state);\n\telse\n\t\tclear_bit(EFA_AQ_STATE_POLLING_BIT, &edev->aq.state);\n}\n\nstatic void efa_com_stats_init(struct efa_com_dev *edev)\n{\n\tatomic64_t *s = (atomic64_t *)&edev->aq.stats;\n\tint i;\n\n\tfor (i = 0; i < sizeof(edev->aq.stats) / sizeof(*s); i++, s++)\n\t\tatomic64_set(s, 0);\n}\n\n \nint efa_com_admin_init(struct efa_com_dev *edev,\n\t\t       struct efa_aenq_handlers *aenq_handlers)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tu32 timeout;\n\tu32 dev_sts;\n\tu32 cap;\n\tint err;\n\n\tdev_sts = efa_com_reg_read32(edev, EFA_REGS_DEV_STS_OFF);\n\tif (!EFA_GET(&dev_sts, EFA_REGS_DEV_STS_READY)) {\n\t\tibdev_err(edev->efa_dev,\n\t\t\t  \"Device isn't ready, abort com init %#x\\n\", dev_sts);\n\t\treturn -ENODEV;\n\t}\n\n\taq->depth = EFA_ADMIN_QUEUE_DEPTH;\n\n\taq->dmadev = edev->dmadev;\n\taq->efa_dev = edev->efa_dev;\n\tset_bit(EFA_AQ_STATE_POLLING_BIT, &aq->state);\n\n\tsema_init(&aq->avail_cmds, aq->depth);\n\n\tefa_com_stats_init(edev);\n\n\terr = efa_com_init_comp_ctxt(aq);\n\tif (err)\n\t\treturn err;\n\n\terr = efa_com_admin_init_sq(edev);\n\tif (err)\n\t\tgoto err_destroy_comp_ctxt;\n\n\terr = efa_com_admin_init_cq(edev);\n\tif (err)\n\t\tgoto err_destroy_sq;\n\n\tefa_com_set_admin_polling_mode(edev, false);\n\n\terr = efa_com_admin_init_aenq(edev, aenq_handlers);\n\tif (err)\n\t\tgoto err_destroy_cq;\n\n\tcap = efa_com_reg_read32(edev, EFA_REGS_CAPS_OFF);\n\ttimeout = EFA_GET(&cap, EFA_REGS_CAPS_ADMIN_CMD_TO);\n\tif (timeout)\n\t\t \n\t\taq->completion_timeout = timeout * 100000;\n\telse\n\t\taq->completion_timeout = ADMIN_CMD_TIMEOUT_US;\n\n\taq->poll_interval = EFA_POLL_INTERVAL_MS;\n\n\tset_bit(EFA_AQ_STATE_RUNNING_BIT, &aq->state);\n\n\treturn 0;\n\nerr_destroy_cq:\n\tdma_free_coherent(edev->dmadev, aq->depth * sizeof(*aq->cq.entries),\n\t\t\t  aq->cq.entries, aq->cq.dma_addr);\nerr_destroy_sq:\n\tdma_free_coherent(edev->dmadev, aq->depth * sizeof(*aq->sq.entries),\n\t\t\t  aq->sq.entries, aq->sq.dma_addr);\nerr_destroy_comp_ctxt:\n\tdevm_kfree(edev->dmadev, aq->comp_ctx);\n\n\treturn err;\n}\n\n \nvoid efa_com_admin_q_comp_intr_handler(struct efa_com_dev *edev)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&edev->aq.cq.lock, flags);\n\tefa_com_handle_admin_completion(&edev->aq);\n\tspin_unlock_irqrestore(&edev->aq.cq.lock, flags);\n}\n\n \nstatic efa_aenq_handler efa_com_get_specific_aenq_cb(struct efa_com_dev *edev,\n\t\t\t\t\t\t     u16 group)\n{\n\tstruct efa_aenq_handlers *aenq_handlers = edev->aenq.aenq_handlers;\n\n\tif (group < EFA_MAX_HANDLERS && aenq_handlers->handlers[group])\n\t\treturn aenq_handlers->handlers[group];\n\n\treturn aenq_handlers->unimplemented_handler;\n}\n\n \nvoid efa_com_aenq_intr_handler(struct efa_com_dev *edev, void *data)\n{\n\tstruct efa_admin_aenq_common_desc *aenq_common;\n\tstruct efa_com_aenq *aenq = &edev->aenq;\n\tstruct efa_admin_aenq_entry *aenq_e;\n\tefa_aenq_handler handler_cb;\n\tu32 processed = 0;\n\tu8 phase;\n\tu32 ci;\n\n\tci = aenq->cc & (aenq->depth - 1);\n\tphase = aenq->phase;\n\taenq_e = &aenq->entries[ci];  \n\taenq_common = &aenq_e->aenq_common_desc;\n\n\t \n\twhile ((READ_ONCE(aenq_common->flags) &\n\t\tEFA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK) == phase) {\n\t\t \n\t\tdma_rmb();\n\n\t\t \n\t\thandler_cb = efa_com_get_specific_aenq_cb(edev,\n\t\t\t\t\t\t\t  aenq_common->group);\n\t\thandler_cb(data, aenq_e);  \n\n\t\t \n\t\tci++;\n\t\tprocessed++;\n\n\t\tif (ci == aenq->depth) {\n\t\t\tci = 0;\n\t\t\tphase = !phase;\n\t\t}\n\t\taenq_e = &aenq->entries[ci];\n\t\taenq_common = &aenq_e->aenq_common_desc;\n\t}\n\n\taenq->cc += processed;\n\taenq->phase = phase;\n\n\t \n\tif (!processed)\n\t\treturn;\n\n\t \n\twritel(aenq->cc, edev->reg_bar + EFA_REGS_AENQ_CONS_DB_OFF);\n}\n\nstatic void efa_com_mmio_reg_read_resp_addr_init(struct efa_com_dev *edev)\n{\n\tstruct efa_com_mmio_read *mmio_read = &edev->mmio_read;\n\tu32 addr_high;\n\tu32 addr_low;\n\n\t \n\taddr_high = (mmio_read->read_resp_dma_addr >> 32) & GENMASK(31, 0);\n\taddr_low = mmio_read->read_resp_dma_addr & GENMASK(31, 0);\n\n\twritel(addr_high, edev->reg_bar + EFA_REGS_MMIO_RESP_HI_OFF);\n\twritel(addr_low, edev->reg_bar + EFA_REGS_MMIO_RESP_LO_OFF);\n}\n\nint efa_com_mmio_reg_read_init(struct efa_com_dev *edev)\n{\n\tstruct efa_com_mmio_read *mmio_read = &edev->mmio_read;\n\n\tspin_lock_init(&mmio_read->lock);\n\tmmio_read->read_resp =\n\t\tdma_alloc_coherent(edev->dmadev, sizeof(*mmio_read->read_resp),\n\t\t\t\t   &mmio_read->read_resp_dma_addr, GFP_KERNEL);\n\tif (!mmio_read->read_resp)\n\t\treturn -ENOMEM;\n\n\tefa_com_mmio_reg_read_resp_addr_init(edev);\n\n\tmmio_read->read_resp->req_id = 0;\n\tmmio_read->seq_num = 0;\n\tmmio_read->mmio_read_timeout = EFA_REG_READ_TIMEOUT_US;\n\n\treturn 0;\n}\n\nvoid efa_com_mmio_reg_read_destroy(struct efa_com_dev *edev)\n{\n\tstruct efa_com_mmio_read *mmio_read = &edev->mmio_read;\n\n\tdma_free_coherent(edev->dmadev, sizeof(*mmio_read->read_resp),\n\t\t\t  mmio_read->read_resp, mmio_read->read_resp_dma_addr);\n}\n\nint efa_com_validate_version(struct efa_com_dev *edev)\n{\n\tu32 min_ctrl_ver = 0;\n\tu32 ctrl_ver_masked;\n\tu32 min_ver = 0;\n\tu32 ctrl_ver;\n\tu32 ver;\n\n\t \n\tver = efa_com_reg_read32(edev, EFA_REGS_VERSION_OFF);\n\tctrl_ver = efa_com_reg_read32(edev,\n\t\t\t\t      EFA_REGS_CONTROLLER_VERSION_OFF);\n\n\tibdev_dbg(edev->efa_dev, \"efa device version: %d.%d\\n\",\n\t\t  EFA_GET(&ver, EFA_REGS_VERSION_MAJOR_VERSION),\n\t\t  EFA_GET(&ver, EFA_REGS_VERSION_MINOR_VERSION));\n\n\tEFA_SET(&min_ver, EFA_REGS_VERSION_MAJOR_VERSION,\n\t\tEFA_ADMIN_API_VERSION_MAJOR);\n\tEFA_SET(&min_ver, EFA_REGS_VERSION_MINOR_VERSION,\n\t\tEFA_ADMIN_API_VERSION_MINOR);\n\tif (ver < min_ver) {\n\t\tibdev_err(edev->efa_dev,\n\t\t\t  \"EFA version is lower than the minimal version the driver supports\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tibdev_dbg(\n\t\tedev->efa_dev,\n\t\t\"efa controller version: %d.%d.%d implementation version %d\\n\",\n\t\tEFA_GET(&ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MAJOR_VERSION),\n\t\tEFA_GET(&ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MINOR_VERSION),\n\t\tEFA_GET(&ctrl_ver,\n\t\t\tEFA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION),\n\t\tEFA_GET(&ctrl_ver, EFA_REGS_CONTROLLER_VERSION_IMPL_ID));\n\n\tctrl_ver_masked =\n\t\tEFA_GET(&ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MAJOR_VERSION) |\n\t\tEFA_GET(&ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MINOR_VERSION) |\n\t\tEFA_GET(&ctrl_ver,\n\t\t\tEFA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION);\n\n\tEFA_SET(&min_ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MAJOR_VERSION,\n\t\tEFA_CTRL_MAJOR);\n\tEFA_SET(&min_ctrl_ver, EFA_REGS_CONTROLLER_VERSION_MINOR_VERSION,\n\t\tEFA_CTRL_MINOR);\n\tEFA_SET(&min_ctrl_ver, EFA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION,\n\t\tEFA_CTRL_SUB_MINOR);\n\t \n\tif (ctrl_ver_masked < min_ctrl_ver) {\n\t\tibdev_err(edev->efa_dev,\n\t\t\t  \"EFA ctrl version is lower than the minimal ctrl version the driver supports\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\n \nint efa_com_get_dma_width(struct efa_com_dev *edev)\n{\n\tu32 caps = efa_com_reg_read32(edev, EFA_REGS_CAPS_OFF);\n\tint width;\n\n\twidth = EFA_GET(&caps, EFA_REGS_CAPS_DMA_ADDR_WIDTH);\n\n\tibdev_dbg(edev->efa_dev, \"DMA width: %d\\n\", width);\n\n\tif (width < 32 || width > 64) {\n\t\tibdev_err(edev->efa_dev, \"DMA width illegal value: %d\\n\", width);\n\t\treturn -EINVAL;\n\t}\n\n\tedev->dma_addr_bits = width;\n\n\treturn width;\n}\n\nstatic int wait_for_reset_state(struct efa_com_dev *edev, u32 timeout, int on)\n{\n\tu32 val, i;\n\n\tfor (i = 0; i < timeout; i++) {\n\t\tval = efa_com_reg_read32(edev, EFA_REGS_DEV_STS_OFF);\n\n\t\tif (EFA_GET(&val, EFA_REGS_DEV_STS_RESET_IN_PROGRESS) == on)\n\t\t\treturn 0;\n\n\t\tibdev_dbg(edev->efa_dev, \"Reset indication val %d\\n\", val);\n\t\tmsleep(EFA_POLL_INTERVAL_MS);\n\t}\n\n\treturn -ETIME;\n}\n\n \nint efa_com_dev_reset(struct efa_com_dev *edev,\n\t\t      enum efa_regs_reset_reason_types reset_reason)\n{\n\tu32 stat, timeout, cap;\n\tu32 reset_val = 0;\n\tint err;\n\n\tstat = efa_com_reg_read32(edev, EFA_REGS_DEV_STS_OFF);\n\tcap = efa_com_reg_read32(edev, EFA_REGS_CAPS_OFF);\n\n\tif (!EFA_GET(&stat, EFA_REGS_DEV_STS_READY)) {\n\t\tibdev_err(edev->efa_dev,\n\t\t\t  \"Device isn't ready, can't reset device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ttimeout = EFA_GET(&cap, EFA_REGS_CAPS_RESET_TIMEOUT);\n\tif (!timeout) {\n\t\tibdev_err(edev->efa_dev, \"Invalid timeout value\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tEFA_SET(&reset_val, EFA_REGS_DEV_CTL_DEV_RESET, 1);\n\tEFA_SET(&reset_val, EFA_REGS_DEV_CTL_RESET_REASON, reset_reason);\n\twritel(reset_val, edev->reg_bar + EFA_REGS_DEV_CTL_OFF);\n\n\t \n\tefa_com_mmio_reg_read_resp_addr_init(edev);\n\n\terr = wait_for_reset_state(edev, timeout, 1);\n\tif (err) {\n\t\tibdev_err(edev->efa_dev, \"Reset indication didn't turn on\\n\");\n\t\treturn err;\n\t}\n\n\t \n\twritel(0, edev->reg_bar + EFA_REGS_DEV_CTL_OFF);\n\terr = wait_for_reset_state(edev, timeout, 0);\n\tif (err) {\n\t\tibdev_err(edev->efa_dev, \"Reset indication didn't turn off\\n\");\n\t\treturn err;\n\t}\n\n\ttimeout = EFA_GET(&cap, EFA_REGS_CAPS_ADMIN_CMD_TO);\n\tif (timeout)\n\t\t \n\t\tedev->aq.completion_timeout = timeout * 100000;\n\telse\n\t\tedev->aq.completion_timeout = ADMIN_CMD_TIMEOUT_US;\n\n\treturn 0;\n}\n\nstatic int efa_com_create_eq(struct efa_com_dev *edev,\n\t\t\t     struct efa_com_create_eq_params *params,\n\t\t\t     struct efa_com_create_eq_result *result)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tstruct efa_admin_create_eq_resp resp = {};\n\tstruct efa_admin_create_eq_cmd cmd = {};\n\tint err;\n\n\tcmd.aq_common_descriptor.opcode = EFA_ADMIN_CREATE_EQ;\n\tEFA_SET(&cmd.caps, EFA_ADMIN_CREATE_EQ_CMD_ENTRY_SIZE_WORDS,\n\t\tparams->entry_size_in_bytes / 4);\n\tcmd.depth = params->depth;\n\tcmd.event_bitmask = params->event_bitmask;\n\tcmd.msix_vec = params->msix_vec;\n\n\tefa_com_set_dma_addr(params->dma_addr, &cmd.ba.mem_addr_high,\n\t\t\t     &cmd.ba.mem_addr_low);\n\n\terr = efa_com_cmd_exec(aq,\n\t\t\t       (struct efa_admin_aq_entry *)&cmd,\n\t\t\t       sizeof(cmd),\n\t\t\t       (struct efa_admin_acq_entry *)&resp,\n\t\t\t       sizeof(resp));\n\tif (err) {\n\t\tibdev_err_ratelimited(edev->efa_dev,\n\t\t\t\t      \"Failed to create eq[%d]\\n\", err);\n\t\treturn err;\n\t}\n\n\tresult->eqn = resp.eqn;\n\n\treturn 0;\n}\n\nstatic void efa_com_destroy_eq(struct efa_com_dev *edev,\n\t\t\t       struct efa_com_destroy_eq_params *params)\n{\n\tstruct efa_com_admin_queue *aq = &edev->aq;\n\tstruct efa_admin_destroy_eq_resp resp = {};\n\tstruct efa_admin_destroy_eq_cmd cmd = {};\n\tint err;\n\n\tcmd.aq_common_descriptor.opcode = EFA_ADMIN_DESTROY_EQ;\n\tcmd.eqn = params->eqn;\n\n\terr = efa_com_cmd_exec(aq,\n\t\t\t       (struct efa_admin_aq_entry *)&cmd,\n\t\t\t       sizeof(cmd),\n\t\t\t       (struct efa_admin_acq_entry *)&resp,\n\t\t\t       sizeof(resp));\n\tif (err)\n\t\tibdev_err_ratelimited(edev->efa_dev,\n\t\t\t\t      \"Failed to destroy EQ-%u [%d]\\n\", cmd.eqn,\n\t\t\t\t      err);\n}\n\nstatic void efa_com_arm_eq(struct efa_com_dev *edev, struct efa_com_eq *eeq)\n{\n\tu32 val = 0;\n\n\tEFA_SET(&val, EFA_REGS_EQ_DB_EQN, eeq->eqn);\n\tEFA_SET(&val, EFA_REGS_EQ_DB_ARM, 1);\n\n\twritel(val, edev->reg_bar + EFA_REGS_EQ_DB_OFF);\n}\n\nvoid efa_com_eq_comp_intr_handler(struct efa_com_dev *edev,\n\t\t\t\t  struct efa_com_eq *eeq)\n{\n\tstruct efa_admin_eqe *eqe;\n\tu32 processed = 0;\n\tu8 phase;\n\tu32 ci;\n\n\tci = eeq->cc & (eeq->depth - 1);\n\tphase = eeq->phase;\n\teqe = &eeq->eqes[ci];\n\n\t \n\twhile ((READ_ONCE(eqe->common) & EFA_ADMIN_EQE_PHASE_MASK) == phase) {\n\t\t \n\t\tdma_rmb();\n\n\t\teeq->cb(eeq, eqe);\n\n\t\t \n\t\tci++;\n\t\tprocessed++;\n\n\t\tif (ci == eeq->depth) {\n\t\t\tci = 0;\n\t\t\tphase = !phase;\n\t\t}\n\n\t\teqe = &eeq->eqes[ci];\n\t}\n\n\teeq->cc += processed;\n\teeq->phase = phase;\n\tefa_com_arm_eq(eeq->edev, eeq);\n}\n\nvoid efa_com_eq_destroy(struct efa_com_dev *edev, struct efa_com_eq *eeq)\n{\n\tstruct efa_com_destroy_eq_params params = {\n\t\t.eqn = eeq->eqn,\n\t};\n\n\tefa_com_destroy_eq(edev, &params);\n\tdma_free_coherent(edev->dmadev, eeq->depth * sizeof(*eeq->eqes),\n\t\t\t  eeq->eqes, eeq->dma_addr);\n}\n\nint efa_com_eq_init(struct efa_com_dev *edev, struct efa_com_eq *eeq,\n\t\t    efa_eqe_handler cb, u16 depth, u8 msix_vec)\n{\n\tstruct efa_com_create_eq_params params = {};\n\tstruct efa_com_create_eq_result result = {};\n\tint err;\n\n\tparams.depth = depth;\n\tparams.entry_size_in_bytes = sizeof(*eeq->eqes);\n\tEFA_SET(&params.event_bitmask,\n\t\tEFA_ADMIN_CREATE_EQ_CMD_COMPLETION_EVENTS, 1);\n\tparams.msix_vec = msix_vec;\n\n\teeq->eqes = dma_alloc_coherent(edev->dmadev,\n\t\t\t\t       params.depth * sizeof(*eeq->eqes),\n\t\t\t\t       &params.dma_addr, GFP_KERNEL);\n\tif (!eeq->eqes)\n\t\treturn -ENOMEM;\n\n\terr = efa_com_create_eq(edev, &params, &result);\n\tif (err)\n\t\tgoto err_free_coherent;\n\n\teeq->eqn = result.eqn;\n\teeq->edev = edev;\n\teeq->dma_addr = params.dma_addr;\n\teeq->phase = 1;\n\teeq->depth = params.depth;\n\teeq->cb = cb;\n\tefa_com_arm_eq(edev, eeq);\n\n\treturn 0;\n\nerr_free_coherent:\n\tdma_free_coherent(edev->dmadev, params.depth * sizeof(*eeq->eqes),\n\t\t\t  eeq->eqes, params.dma_addr);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}