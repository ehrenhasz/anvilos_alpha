{
  "module_name": "efa_verbs.c",
  "hash_id": "90247cefd41e576166bd97ed2c11b397a52ad7293f14abcca4bb579b8f4c3971",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/efa/efa_verbs.c",
  "human_readable_source": "\n \n\n#include <linux/dma-buf.h>\n#include <linux/dma-resv.h>\n#include <linux/vmalloc.h>\n#include <linux/log2.h>\n\n#include <rdma/ib_addr.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"efa.h\"\n#include \"efa_io_defs.h\"\n\nenum {\n\tEFA_MMAP_DMA_PAGE = 0,\n\tEFA_MMAP_IO_WC,\n\tEFA_MMAP_IO_NC,\n};\n\n#define EFA_AENQ_ENABLED_GROUPS \\\n\t(BIT(EFA_ADMIN_FATAL_ERROR) | BIT(EFA_ADMIN_WARNING) | \\\n\t BIT(EFA_ADMIN_NOTIFICATION) | BIT(EFA_ADMIN_KEEP_ALIVE))\n\nstruct efa_user_mmap_entry {\n\tstruct rdma_user_mmap_entry rdma_entry;\n\tu64 address;\n\tu8 mmap_flag;\n};\n\n#define EFA_DEFINE_DEVICE_STATS(op) \\\n\top(EFA_SUBMITTED_CMDS, \"submitted_cmds\") \\\n\top(EFA_COMPLETED_CMDS, \"completed_cmds\") \\\n\top(EFA_CMDS_ERR, \"cmds_err\") \\\n\top(EFA_NO_COMPLETION_CMDS, \"no_completion_cmds\") \\\n\top(EFA_KEEP_ALIVE_RCVD, \"keep_alive_rcvd\") \\\n\top(EFA_ALLOC_PD_ERR, \"alloc_pd_err\") \\\n\top(EFA_CREATE_QP_ERR, \"create_qp_err\") \\\n\top(EFA_CREATE_CQ_ERR, \"create_cq_err\") \\\n\top(EFA_REG_MR_ERR, \"reg_mr_err\") \\\n\top(EFA_ALLOC_UCONTEXT_ERR, \"alloc_ucontext_err\") \\\n\top(EFA_CREATE_AH_ERR, \"create_ah_err\") \\\n\top(EFA_MMAP_ERR, \"mmap_err\")\n\n#define EFA_DEFINE_PORT_STATS(op) \\\n\top(EFA_TX_BYTES, \"tx_bytes\") \\\n\top(EFA_TX_PKTS, \"tx_pkts\") \\\n\top(EFA_RX_BYTES, \"rx_bytes\") \\\n\top(EFA_RX_PKTS, \"rx_pkts\") \\\n\top(EFA_RX_DROPS, \"rx_drops\") \\\n\top(EFA_SEND_BYTES, \"send_bytes\") \\\n\top(EFA_SEND_WRS, \"send_wrs\") \\\n\top(EFA_RECV_BYTES, \"recv_bytes\") \\\n\top(EFA_RECV_WRS, \"recv_wrs\") \\\n\top(EFA_RDMA_READ_WRS, \"rdma_read_wrs\") \\\n\top(EFA_RDMA_READ_BYTES, \"rdma_read_bytes\") \\\n\top(EFA_RDMA_READ_WR_ERR, \"rdma_read_wr_err\") \\\n\top(EFA_RDMA_READ_RESP_BYTES, \"rdma_read_resp_bytes\") \\\n\top(EFA_RDMA_WRITE_WRS, \"rdma_write_wrs\") \\\n\top(EFA_RDMA_WRITE_BYTES, \"rdma_write_bytes\") \\\n\top(EFA_RDMA_WRITE_WR_ERR, \"rdma_write_wr_err\") \\\n\top(EFA_RDMA_WRITE_RECV_BYTES, \"rdma_write_recv_bytes\") \\\n\n#define EFA_STATS_ENUM(ename, name) ename,\n#define EFA_STATS_STR(ename, nam) \\\n\t[ename].name = nam,\n\nenum efa_hw_device_stats {\n\tEFA_DEFINE_DEVICE_STATS(EFA_STATS_ENUM)\n};\n\nstatic const struct rdma_stat_desc efa_device_stats_descs[] = {\n\tEFA_DEFINE_DEVICE_STATS(EFA_STATS_STR)\n};\n\nenum efa_hw_port_stats {\n\tEFA_DEFINE_PORT_STATS(EFA_STATS_ENUM)\n};\n\nstatic const struct rdma_stat_desc efa_port_stats_descs[] = {\n\tEFA_DEFINE_PORT_STATS(EFA_STATS_STR)\n};\n\n#define EFA_CHUNK_PAYLOAD_SHIFT       12\n#define EFA_CHUNK_PAYLOAD_SIZE        BIT(EFA_CHUNK_PAYLOAD_SHIFT)\n#define EFA_CHUNK_PAYLOAD_PTR_SIZE    8\n\n#define EFA_CHUNK_SHIFT               12\n#define EFA_CHUNK_SIZE                BIT(EFA_CHUNK_SHIFT)\n#define EFA_CHUNK_PTR_SIZE            sizeof(struct efa_com_ctrl_buff_info)\n\n#define EFA_PTRS_PER_CHUNK \\\n\t((EFA_CHUNK_SIZE - EFA_CHUNK_PTR_SIZE) / EFA_CHUNK_PAYLOAD_PTR_SIZE)\n\n#define EFA_CHUNK_USED_SIZE \\\n\t((EFA_PTRS_PER_CHUNK * EFA_CHUNK_PAYLOAD_PTR_SIZE) + EFA_CHUNK_PTR_SIZE)\n\nstruct pbl_chunk {\n\tdma_addr_t dma_addr;\n\tu64 *buf;\n\tu32 length;\n};\n\nstruct pbl_chunk_list {\n\tstruct pbl_chunk *chunks;\n\tunsigned int size;\n};\n\nstruct pbl_context {\n\tunion {\n\t\tstruct {\n\t\t\tdma_addr_t dma_addr;\n\t\t} continuous;\n\t\tstruct {\n\t\t\tu32 pbl_buf_size_in_pages;\n\t\t\tstruct scatterlist *sgl;\n\t\t\tint sg_dma_cnt;\n\t\t\tstruct pbl_chunk_list chunk_list;\n\t\t} indirect;\n\t} phys;\n\tu64 *pbl_buf;\n\tu32 pbl_buf_size_in_bytes;\n\tu8 physically_continuous;\n};\n\nstatic inline struct efa_dev *to_edev(struct ib_device *ibdev)\n{\n\treturn container_of(ibdev, struct efa_dev, ibdev);\n}\n\nstatic inline struct efa_ucontext *to_eucontext(struct ib_ucontext *ibucontext)\n{\n\treturn container_of(ibucontext, struct efa_ucontext, ibucontext);\n}\n\nstatic inline struct efa_pd *to_epd(struct ib_pd *ibpd)\n{\n\treturn container_of(ibpd, struct efa_pd, ibpd);\n}\n\nstatic inline struct efa_mr *to_emr(struct ib_mr *ibmr)\n{\n\treturn container_of(ibmr, struct efa_mr, ibmr);\n}\n\nstatic inline struct efa_qp *to_eqp(struct ib_qp *ibqp)\n{\n\treturn container_of(ibqp, struct efa_qp, ibqp);\n}\n\nstatic inline struct efa_cq *to_ecq(struct ib_cq *ibcq)\n{\n\treturn container_of(ibcq, struct efa_cq, ibcq);\n}\n\nstatic inline struct efa_ah *to_eah(struct ib_ah *ibah)\n{\n\treturn container_of(ibah, struct efa_ah, ibah);\n}\n\nstatic inline struct efa_user_mmap_entry *\nto_emmap(struct rdma_user_mmap_entry *rdma_entry)\n{\n\treturn container_of(rdma_entry, struct efa_user_mmap_entry, rdma_entry);\n}\n\n#define EFA_DEV_CAP(dev, cap) \\\n\t((dev)->dev_attr.device_caps & \\\n\t EFA_ADMIN_FEATURE_DEVICE_ATTR_DESC_##cap##_MASK)\n\n#define is_reserved_cleared(reserved) \\\n\t!memchr_inv(reserved, 0, sizeof(reserved))\n\nstatic void *efa_zalloc_mapped(struct efa_dev *dev, dma_addr_t *dma_addr,\n\t\t\t       size_t size, enum dma_data_direction dir)\n{\n\tvoid *addr;\n\n\taddr = alloc_pages_exact(size, GFP_KERNEL | __GFP_ZERO);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*dma_addr = dma_map_single(&dev->pdev->dev, addr, size, dir);\n\tif (dma_mapping_error(&dev->pdev->dev, *dma_addr)) {\n\t\tibdev_err(&dev->ibdev, \"Failed to map DMA address\\n\");\n\t\tfree_pages_exact(addr, size);\n\t\treturn NULL;\n\t}\n\n\treturn addr;\n}\n\nstatic void efa_free_mapped(struct efa_dev *dev, void *cpu_addr,\n\t\t\t    dma_addr_t dma_addr,\n\t\t\t    size_t size, enum dma_data_direction dir)\n{\n\tdma_unmap_single(&dev->pdev->dev, dma_addr, size, dir);\n\tfree_pages_exact(cpu_addr, size);\n}\n\nint efa_query_device(struct ib_device *ibdev,\n\t\t     struct ib_device_attr *props,\n\t\t     struct ib_udata *udata)\n{\n\tstruct efa_com_get_device_attr_result *dev_attr;\n\tstruct efa_ibv_ex_query_device_resp resp = {};\n\tstruct efa_dev *dev = to_edev(ibdev);\n\tint err;\n\n\tif (udata && udata->inlen &&\n\t    !ib_is_udata_cleared(udata, 0, udata->inlen)) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Incompatible ABI params, udata not cleared\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev_attr = &dev->dev_attr;\n\n\tmemset(props, 0, sizeof(*props));\n\tprops->max_mr_size = dev_attr->max_mr_pages * PAGE_SIZE;\n\tprops->page_size_cap = dev_attr->page_size_cap;\n\tprops->vendor_id = dev->pdev->vendor;\n\tprops->vendor_part_id = dev->pdev->device;\n\tprops->hw_ver = dev->pdev->subsystem_device;\n\tprops->max_qp = dev_attr->max_qp;\n\tprops->max_cq = dev_attr->max_cq;\n\tprops->max_pd = dev_attr->max_pd;\n\tprops->max_mr = dev_attr->max_mr;\n\tprops->max_ah = dev_attr->max_ah;\n\tprops->max_cqe = dev_attr->max_cq_depth;\n\tprops->max_qp_wr = min_t(u32, dev_attr->max_sq_depth,\n\t\t\t\t dev_attr->max_rq_depth);\n\tprops->max_send_sge = dev_attr->max_sq_sge;\n\tprops->max_recv_sge = dev_attr->max_rq_sge;\n\tprops->max_sge_rd = dev_attr->max_wr_rdma_sge;\n\tprops->max_pkeys = 1;\n\n\tif (udata && udata->outlen) {\n\t\tresp.max_sq_sge = dev_attr->max_sq_sge;\n\t\tresp.max_rq_sge = dev_attr->max_rq_sge;\n\t\tresp.max_sq_wr = dev_attr->max_sq_depth;\n\t\tresp.max_rq_wr = dev_attr->max_rq_depth;\n\t\tresp.max_rdma_size = dev_attr->max_rdma_size;\n\n\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_CQ_WITH_SGID;\n\t\tif (EFA_DEV_CAP(dev, RDMA_READ))\n\t\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_RDMA_READ;\n\n\t\tif (EFA_DEV_CAP(dev, RNR_RETRY))\n\t\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_RNR_RETRY;\n\n\t\tif (EFA_DEV_CAP(dev, DATA_POLLING_128))\n\t\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_DATA_POLLING_128;\n\n\t\tif (EFA_DEV_CAP(dev, RDMA_WRITE))\n\t\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_RDMA_WRITE;\n\n\t\tif (dev->neqs)\n\t\t\tresp.device_caps |= EFA_QUERY_DEVICE_CAPS_CQ_NOTIFICATIONS;\n\n\t\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(sizeof(resp), udata->outlen));\n\t\tif (err) {\n\t\t\tibdev_dbg(ibdev,\n\t\t\t\t  \"Failed to copy udata for query_device\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint efa_query_port(struct ib_device *ibdev, u32 port,\n\t\t   struct ib_port_attr *props)\n{\n\tstruct efa_dev *dev = to_edev(ibdev);\n\n\tprops->lmc = 1;\n\n\tprops->state = IB_PORT_ACTIVE;\n\tprops->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\tprops->gid_tbl_len = 1;\n\tprops->pkey_tbl_len = 1;\n\tprops->active_speed = IB_SPEED_EDR;\n\tprops->active_width = IB_WIDTH_4X;\n\tprops->max_mtu = ib_mtu_int_to_enum(dev->dev_attr.mtu);\n\tprops->active_mtu = ib_mtu_int_to_enum(dev->dev_attr.mtu);\n\tprops->max_msg_sz = dev->dev_attr.mtu;\n\tprops->max_vl_num = 1;\n\n\treturn 0;\n}\n\nint efa_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr,\n\t\t int qp_attr_mask,\n\t\t struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct efa_dev *dev = to_edev(ibqp->device);\n\tstruct efa_com_query_qp_params params = {};\n\tstruct efa_com_query_qp_result result;\n\tstruct efa_qp *qp = to_eqp(ibqp);\n\tint err;\n\n#define EFA_QUERY_QP_SUPP_MASK \\\n\t(IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT | \\\n\t IB_QP_QKEY | IB_QP_SQ_PSN | IB_QP_CAP | IB_QP_RNR_RETRY)\n\n\tif (qp_attr_mask & ~EFA_QUERY_QP_SUPP_MASK) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Unsupported qp_attr_mask[%#x] supported[%#x]\\n\",\n\t\t\t  qp_attr_mask, EFA_QUERY_QP_SUPP_MASK);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(qp_attr, 0, sizeof(*qp_attr));\n\tmemset(qp_init_attr, 0, sizeof(*qp_init_attr));\n\n\tparams.qp_handle = qp->qp_handle;\n\terr = efa_com_query_qp(&dev->edev, &params, &result);\n\tif (err)\n\t\treturn err;\n\n\tqp_attr->qp_state = result.qp_state;\n\tqp_attr->qkey = result.qkey;\n\tqp_attr->sq_psn = result.sq_psn;\n\tqp_attr->sq_draining = result.sq_draining;\n\tqp_attr->port_num = 1;\n\tqp_attr->rnr_retry = result.rnr_retry;\n\n\tqp_attr->cap.max_send_wr = qp->max_send_wr;\n\tqp_attr->cap.max_recv_wr = qp->max_recv_wr;\n\tqp_attr->cap.max_send_sge = qp->max_send_sge;\n\tqp_attr->cap.max_recv_sge = qp->max_recv_sge;\n\tqp_attr->cap.max_inline_data = qp->max_inline_data;\n\n\tqp_init_attr->qp_type = ibqp->qp_type;\n\tqp_init_attr->recv_cq = ibqp->recv_cq;\n\tqp_init_attr->send_cq = ibqp->send_cq;\n\tqp_init_attr->qp_context = ibqp->qp_context;\n\tqp_init_attr->cap = qp_attr->cap;\n\n\treturn 0;\n}\n\nint efa_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t  union ib_gid *gid)\n{\n\tstruct efa_dev *dev = to_edev(ibdev);\n\n\tmemcpy(gid->raw, dev->dev_attr.addr, sizeof(dev->dev_attr.addr));\n\n\treturn 0;\n}\n\nint efa_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t   u16 *pkey)\n{\n\tif (index > 0)\n\t\treturn -EINVAL;\n\n\t*pkey = 0xffff;\n\treturn 0;\n}\n\nstatic int efa_pd_dealloc(struct efa_dev *dev, u16 pdn)\n{\n\tstruct efa_com_dealloc_pd_params params = {\n\t\t.pdn = pdn,\n\t};\n\n\treturn efa_com_dealloc_pd(&dev->edev, &params);\n}\n\nint efa_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tstruct efa_ibv_alloc_pd_resp resp = {};\n\tstruct efa_com_alloc_pd_result result;\n\tstruct efa_pd *pd = to_epd(ibpd);\n\tint err;\n\n\tif (udata->inlen &&\n\t    !ib_is_udata_cleared(udata, 0, udata->inlen)) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, udata not cleared\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\terr = efa_com_alloc_pd(&dev->edev, &result);\n\tif (err)\n\t\tgoto err_out;\n\n\tpd->pdn = result.pdn;\n\tresp.pdn = result.pdn;\n\n\tif (udata->outlen) {\n\t\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(sizeof(resp), udata->outlen));\n\t\tif (err) {\n\t\t\tibdev_dbg(&dev->ibdev,\n\t\t\t\t  \"Failed to copy udata for alloc_pd\\n\");\n\t\t\tgoto err_dealloc_pd;\n\t\t}\n\t}\n\n\tibdev_dbg(&dev->ibdev, \"Allocated pd[%d]\\n\", pd->pdn);\n\n\treturn 0;\n\nerr_dealloc_pd:\n\tefa_pd_dealloc(dev, result.pdn);\nerr_out:\n\tatomic64_inc(&dev->stats.alloc_pd_err);\n\treturn err;\n}\n\nint efa_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tstruct efa_pd *pd = to_epd(ibpd);\n\n\tibdev_dbg(&dev->ibdev, \"Dealloc pd[%d]\\n\", pd->pdn);\n\tefa_pd_dealloc(dev, pd->pdn);\n\treturn 0;\n}\n\nstatic int efa_destroy_qp_handle(struct efa_dev *dev, u32 qp_handle)\n{\n\tstruct efa_com_destroy_qp_params params = { .qp_handle = qp_handle };\n\n\treturn efa_com_destroy_qp(&dev->edev, &params);\n}\n\nstatic void efa_qp_user_mmap_entries_remove(struct efa_qp *qp)\n{\n\trdma_user_mmap_entry_remove(qp->rq_mmap_entry);\n\trdma_user_mmap_entry_remove(qp->rq_db_mmap_entry);\n\trdma_user_mmap_entry_remove(qp->llq_desc_mmap_entry);\n\trdma_user_mmap_entry_remove(qp->sq_db_mmap_entry);\n}\n\nint efa_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibqp->pd->device);\n\tstruct efa_qp *qp = to_eqp(ibqp);\n\tint err;\n\n\tibdev_dbg(&dev->ibdev, \"Destroy qp[%u]\\n\", ibqp->qp_num);\n\n\terr = efa_destroy_qp_handle(dev, qp->qp_handle);\n\tif (err)\n\t\treturn err;\n\n\tefa_qp_user_mmap_entries_remove(qp);\n\n\tif (qp->rq_cpu_addr) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp->cpu_addr[0x%p] freed: size[%lu], dma[%pad]\\n\",\n\t\t\t  qp->rq_cpu_addr, qp->rq_size,\n\t\t\t  &qp->rq_dma_addr);\n\t\tefa_free_mapped(dev, qp->rq_cpu_addr, qp->rq_dma_addr,\n\t\t\t\tqp->rq_size, DMA_TO_DEVICE);\n\t}\n\n\treturn 0;\n}\n\nstatic struct rdma_user_mmap_entry*\nefa_user_mmap_entry_insert(struct ib_ucontext *ucontext,\n\t\t\t   u64 address, size_t length,\n\t\t\t   u8 mmap_flag, u64 *offset)\n{\n\tstruct efa_user_mmap_entry *entry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tint err;\n\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->address = address;\n\tentry->mmap_flag = mmap_flag;\n\n\terr = rdma_user_mmap_entry_insert(ucontext, &entry->rdma_entry,\n\t\t\t\t\t  length);\n\tif (err) {\n\t\tkfree(entry);\n\t\treturn NULL;\n\t}\n\t*offset = rdma_user_mmap_get_offset(&entry->rdma_entry);\n\n\treturn &entry->rdma_entry;\n}\n\nstatic int qp_mmap_entries_setup(struct efa_qp *qp,\n\t\t\t\t struct efa_dev *dev,\n\t\t\t\t struct efa_ucontext *ucontext,\n\t\t\t\t struct efa_com_create_qp_params *params,\n\t\t\t\t struct efa_ibv_create_qp_resp *resp)\n{\n\tsize_t length;\n\tu64 address;\n\n\taddress = dev->db_bar_addr + resp->sq_db_offset;\n\tqp->sq_db_mmap_entry =\n\t\tefa_user_mmap_entry_insert(&ucontext->ibucontext,\n\t\t\t\t\t   address,\n\t\t\t\t\t   PAGE_SIZE, EFA_MMAP_IO_NC,\n\t\t\t\t\t   &resp->sq_db_mmap_key);\n\tif (!qp->sq_db_mmap_entry)\n\t\treturn -ENOMEM;\n\n\tresp->sq_db_offset &= ~PAGE_MASK;\n\n\taddress = dev->mem_bar_addr + resp->llq_desc_offset;\n\tlength = PAGE_ALIGN(params->sq_ring_size_in_bytes +\n\t\t\t    (resp->llq_desc_offset & ~PAGE_MASK));\n\n\tqp->llq_desc_mmap_entry =\n\t\tefa_user_mmap_entry_insert(&ucontext->ibucontext,\n\t\t\t\t\t   address, length,\n\t\t\t\t\t   EFA_MMAP_IO_WC,\n\t\t\t\t\t   &resp->llq_desc_mmap_key);\n\tif (!qp->llq_desc_mmap_entry)\n\t\tgoto err_remove_mmap;\n\n\tresp->llq_desc_offset &= ~PAGE_MASK;\n\n\tif (qp->rq_size) {\n\t\taddress = dev->db_bar_addr + resp->rq_db_offset;\n\n\t\tqp->rq_db_mmap_entry =\n\t\t\tefa_user_mmap_entry_insert(&ucontext->ibucontext,\n\t\t\t\t\t\t   address, PAGE_SIZE,\n\t\t\t\t\t\t   EFA_MMAP_IO_NC,\n\t\t\t\t\t\t   &resp->rq_db_mmap_key);\n\t\tif (!qp->rq_db_mmap_entry)\n\t\t\tgoto err_remove_mmap;\n\n\t\tresp->rq_db_offset &= ~PAGE_MASK;\n\n\t\taddress = virt_to_phys(qp->rq_cpu_addr);\n\t\tqp->rq_mmap_entry =\n\t\t\tefa_user_mmap_entry_insert(&ucontext->ibucontext,\n\t\t\t\t\t\t   address, qp->rq_size,\n\t\t\t\t\t\t   EFA_MMAP_DMA_PAGE,\n\t\t\t\t\t\t   &resp->rq_mmap_key);\n\t\tif (!qp->rq_mmap_entry)\n\t\t\tgoto err_remove_mmap;\n\n\t\tresp->rq_mmap_size = qp->rq_size;\n\t}\n\n\treturn 0;\n\nerr_remove_mmap:\n\tefa_qp_user_mmap_entries_remove(qp);\n\n\treturn -ENOMEM;\n}\n\nstatic int efa_qp_validate_cap(struct efa_dev *dev,\n\t\t\t       struct ib_qp_init_attr *init_attr)\n{\n\tif (init_attr->cap.max_send_wr > dev->dev_attr.max_sq_depth) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp: requested send wr[%u] exceeds the max[%u]\\n\",\n\t\t\t  init_attr->cap.max_send_wr,\n\t\t\t  dev->dev_attr.max_sq_depth);\n\t\treturn -EINVAL;\n\t}\n\tif (init_attr->cap.max_recv_wr > dev->dev_attr.max_rq_depth) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp: requested receive wr[%u] exceeds the max[%u]\\n\",\n\t\t\t  init_attr->cap.max_recv_wr,\n\t\t\t  dev->dev_attr.max_rq_depth);\n\t\treturn -EINVAL;\n\t}\n\tif (init_attr->cap.max_send_sge > dev->dev_attr.max_sq_sge) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp: requested sge send[%u] exceeds the max[%u]\\n\",\n\t\t\t  init_attr->cap.max_send_sge, dev->dev_attr.max_sq_sge);\n\t\treturn -EINVAL;\n\t}\n\tif (init_attr->cap.max_recv_sge > dev->dev_attr.max_rq_sge) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp: requested sge recv[%u] exceeds the max[%u]\\n\",\n\t\t\t  init_attr->cap.max_recv_sge, dev->dev_attr.max_rq_sge);\n\t\treturn -EINVAL;\n\t}\n\tif (init_attr->cap.max_inline_data > dev->dev_attr.inline_buf_size) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp: requested inline data[%u] exceeds the max[%u]\\n\",\n\t\t\t  init_attr->cap.max_inline_data,\n\t\t\t  dev->dev_attr.inline_buf_size);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int efa_qp_validate_attr(struct efa_dev *dev,\n\t\t\t\tstruct ib_qp_init_attr *init_attr)\n{\n\tif (init_attr->qp_type != IB_QPT_DRIVER &&\n\t    init_attr->qp_type != IB_QPT_UD) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Unsupported qp type %d\\n\", init_attr->qp_type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (init_attr->srq) {\n\t\tibdev_dbg(&dev->ibdev, \"SRQ is not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (init_attr->create_flags) {\n\t\tibdev_dbg(&dev->ibdev, \"Unsupported create flags\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nint efa_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *init_attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct efa_com_create_qp_params create_qp_params = {};\n\tstruct efa_com_create_qp_result create_qp_resp;\n\tstruct efa_dev *dev = to_edev(ibqp->device);\n\tstruct efa_ibv_create_qp_resp resp = {};\n\tstruct efa_ibv_create_qp cmd = {};\n\tstruct efa_qp *qp = to_eqp(ibqp);\n\tstruct efa_ucontext *ucontext;\n\tint err;\n\n\tucontext = rdma_udata_to_drv_context(udata, struct efa_ucontext,\n\t\t\t\t\t     ibucontext);\n\n\terr = efa_qp_validate_cap(dev, init_attr);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = efa_qp_validate_attr(dev, init_attr);\n\tif (err)\n\t\tgoto err_out;\n\n\tif (offsetofend(typeof(cmd), driver_qp_type) > udata->inlen) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, no input udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (udata->inlen > sizeof(cmd) &&\n\t    !ib_is_udata_cleared(udata, sizeof(cmd),\n\t\t\t\t udata->inlen - sizeof(cmd))) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, unknown fields in udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\terr = ib_copy_from_udata(&cmd, udata,\n\t\t\t\t min(sizeof(cmd), udata->inlen));\n\tif (err) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Cannot copy udata for create_qp\\n\");\n\t\tgoto err_out;\n\t}\n\n\tif (cmd.comp_mask) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, unknown fields in udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tcreate_qp_params.uarn = ucontext->uarn;\n\tcreate_qp_params.pd = to_epd(ibqp->pd)->pdn;\n\n\tif (init_attr->qp_type == IB_QPT_UD) {\n\t\tcreate_qp_params.qp_type = EFA_ADMIN_QP_TYPE_UD;\n\t} else if (cmd.driver_qp_type == EFA_QP_DRIVER_TYPE_SRD) {\n\t\tcreate_qp_params.qp_type = EFA_ADMIN_QP_TYPE_SRD;\n\t} else {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Unsupported qp type %d driver qp type %d\\n\",\n\t\t\t  init_attr->qp_type, cmd.driver_qp_type);\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_out;\n\t}\n\n\tibdev_dbg(&dev->ibdev, \"Create QP: qp type %d driver qp type %#x\\n\",\n\t\t  init_attr->qp_type, cmd.driver_qp_type);\n\tcreate_qp_params.send_cq_idx = to_ecq(init_attr->send_cq)->cq_idx;\n\tcreate_qp_params.recv_cq_idx = to_ecq(init_attr->recv_cq)->cq_idx;\n\tcreate_qp_params.sq_depth = init_attr->cap.max_send_wr;\n\tcreate_qp_params.sq_ring_size_in_bytes = cmd.sq_ring_size;\n\n\tcreate_qp_params.rq_depth = init_attr->cap.max_recv_wr;\n\tcreate_qp_params.rq_ring_size_in_bytes = cmd.rq_ring_size;\n\tqp->rq_size = PAGE_ALIGN(create_qp_params.rq_ring_size_in_bytes);\n\tif (qp->rq_size) {\n\t\tqp->rq_cpu_addr = efa_zalloc_mapped(dev, &qp->rq_dma_addr,\n\t\t\t\t\t\t    qp->rq_size, DMA_TO_DEVICE);\n\t\tif (!qp->rq_cpu_addr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"qp->cpu_addr[0x%p] allocated: size[%lu], dma[%pad]\\n\",\n\t\t\t  qp->rq_cpu_addr, qp->rq_size, &qp->rq_dma_addr);\n\t\tcreate_qp_params.rq_base_addr = qp->rq_dma_addr;\n\t}\n\n\terr = efa_com_create_qp(&dev->edev, &create_qp_params,\n\t\t\t\t&create_qp_resp);\n\tif (err)\n\t\tgoto err_free_mapped;\n\n\tresp.sq_db_offset = create_qp_resp.sq_db_offset;\n\tresp.rq_db_offset = create_qp_resp.rq_db_offset;\n\tresp.llq_desc_offset = create_qp_resp.llq_descriptors_offset;\n\tresp.send_sub_cq_idx = create_qp_resp.send_sub_cq_idx;\n\tresp.recv_sub_cq_idx = create_qp_resp.recv_sub_cq_idx;\n\n\terr = qp_mmap_entries_setup(qp, dev, ucontext, &create_qp_params,\n\t\t\t\t    &resp);\n\tif (err)\n\t\tgoto err_destroy_qp;\n\n\tqp->qp_handle = create_qp_resp.qp_handle;\n\tqp->ibqp.qp_num = create_qp_resp.qp_num;\n\tqp->max_send_wr = init_attr->cap.max_send_wr;\n\tqp->max_recv_wr = init_attr->cap.max_recv_wr;\n\tqp->max_send_sge = init_attr->cap.max_send_sge;\n\tqp->max_recv_sge = init_attr->cap.max_recv_sge;\n\tqp->max_inline_data = init_attr->cap.max_inline_data;\n\n\tif (udata->outlen) {\n\t\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(sizeof(resp), udata->outlen));\n\t\tif (err) {\n\t\t\tibdev_dbg(&dev->ibdev,\n\t\t\t\t  \"Failed to copy udata for qp[%u]\\n\",\n\t\t\t\t  create_qp_resp.qp_num);\n\t\t\tgoto err_remove_mmap_entries;\n\t\t}\n\t}\n\n\tibdev_dbg(&dev->ibdev, \"Created qp[%d]\\n\", qp->ibqp.qp_num);\n\n\treturn 0;\n\nerr_remove_mmap_entries:\n\tefa_qp_user_mmap_entries_remove(qp);\nerr_destroy_qp:\n\tefa_destroy_qp_handle(dev, create_qp_resp.qp_handle);\nerr_free_mapped:\n\tif (qp->rq_size)\n\t\tefa_free_mapped(dev, qp->rq_cpu_addr, qp->rq_dma_addr,\n\t\t\t\tqp->rq_size, DMA_TO_DEVICE);\nerr_out:\n\tatomic64_inc(&dev->stats.create_qp_err);\n\treturn err;\n}\n\nstatic const struct {\n\tint\t\t\tvalid;\n\tenum ib_qp_attr_mask\treq_param;\n\tenum ib_qp_attr_mask\topt_param;\n} srd_qp_state_table[IB_QPS_ERR + 1][IB_QPS_ERR + 1] = {\n\t[IB_QPS_RESET] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_INIT]  = {\n\t\t\t.valid = 1,\n\t\t\t.req_param = IB_QP_PKEY_INDEX |\n\t\t\t\t     IB_QP_PORT |\n\t\t\t\t     IB_QP_QKEY,\n\t\t},\n\t},\n\t[IB_QPS_INIT] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t\t[IB_QPS_INIT]  = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_PKEY_INDEX |\n\t\t\t\t     IB_QP_PORT |\n\t\t\t\t     IB_QP_QKEY,\n\t\t},\n\t\t[IB_QPS_RTR]   = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_PKEY_INDEX |\n\t\t\t\t     IB_QP_QKEY,\n\t\t},\n\t},\n\t[IB_QPS_RTR] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t\t[IB_QPS_RTS]   = {\n\t\t\t.valid = 1,\n\t\t\t.req_param = IB_QP_SQ_PSN,\n\t\t\t.opt_param = IB_QP_CUR_STATE |\n\t\t\t\t     IB_QP_QKEY |\n\t\t\t\t     IB_QP_RNR_RETRY,\n\n\t\t}\n\t},\n\t[IB_QPS_RTS] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t\t[IB_QPS_RTS]   = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_CUR_STATE |\n\t\t\t\t     IB_QP_QKEY,\n\t\t},\n\t\t[IB_QPS_SQD] = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_EN_SQD_ASYNC_NOTIFY,\n\t\t},\n\t},\n\t[IB_QPS_SQD] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t\t[IB_QPS_RTS]   = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_CUR_STATE |\n\t\t\t\t     IB_QP_QKEY,\n\t\t},\n\t\t[IB_QPS_SQD] = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_PKEY_INDEX |\n\t\t\t\t     IB_QP_QKEY,\n\t\t}\n\t},\n\t[IB_QPS_SQE] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t\t[IB_QPS_RTS]   = {\n\t\t\t.valid = 1,\n\t\t\t.opt_param = IB_QP_CUR_STATE |\n\t\t\t\t     IB_QP_QKEY,\n\t\t}\n\t},\n\t[IB_QPS_ERR] = {\n\t\t[IB_QPS_RESET] = { .valid = 1 },\n\t\t[IB_QPS_ERR]   = { .valid = 1 },\n\t}\n};\n\nstatic bool efa_modify_srd_qp_is_ok(enum ib_qp_state cur_state,\n\t\t\t\t    enum ib_qp_state next_state,\n\t\t\t\t    enum ib_qp_attr_mask mask)\n{\n\tenum ib_qp_attr_mask req_param, opt_param;\n\n\tif (mask & IB_QP_CUR_STATE  &&\n\t    cur_state != IB_QPS_RTR && cur_state != IB_QPS_RTS &&\n\t    cur_state != IB_QPS_SQD && cur_state != IB_QPS_SQE)\n\t\treturn false;\n\n\tif (!srd_qp_state_table[cur_state][next_state].valid)\n\t\treturn false;\n\n\treq_param = srd_qp_state_table[cur_state][next_state].req_param;\n\topt_param = srd_qp_state_table[cur_state][next_state].opt_param;\n\n\tif ((mask & req_param) != req_param)\n\t\treturn false;\n\n\tif (mask & ~(req_param | opt_param | IB_QP_STATE))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int efa_modify_qp_validate(struct efa_dev *dev, struct efa_qp *qp,\n\t\t\t\t  struct ib_qp_attr *qp_attr, int qp_attr_mask,\n\t\t\t\t  enum ib_qp_state cur_state,\n\t\t\t\t  enum ib_qp_state new_state)\n{\n\tint err;\n\n#define EFA_MODIFY_QP_SUPP_MASK \\\n\t(IB_QP_STATE | IB_QP_CUR_STATE | IB_QP_EN_SQD_ASYNC_NOTIFY | \\\n\t IB_QP_PKEY_INDEX | IB_QP_PORT | IB_QP_QKEY | IB_QP_SQ_PSN | \\\n\t IB_QP_RNR_RETRY)\n\n\tif (qp_attr_mask & ~EFA_MODIFY_QP_SUPP_MASK) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Unsupported qp_attr_mask[%#x] supported[%#x]\\n\",\n\t\t\t  qp_attr_mask, EFA_MODIFY_QP_SUPP_MASK);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (qp->ibqp.qp_type == IB_QPT_DRIVER)\n\t\terr = !efa_modify_srd_qp_is_ok(cur_state, new_state,\n\t\t\t\t\t       qp_attr_mask);\n\telse\n\t\terr = !ib_modify_qp_is_ok(cur_state, new_state, IB_QPT_UD,\n\t\t\t\t\t  qp_attr_mask);\n\n\tif (err) {\n\t\tibdev_dbg(&dev->ibdev, \"Invalid modify QP parameters\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((qp_attr_mask & IB_QP_PORT) && qp_attr->port_num != 1) {\n\t\tibdev_dbg(&dev->ibdev, \"Can't change port num\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif ((qp_attr_mask & IB_QP_PKEY_INDEX) && qp_attr->pkey_index) {\n\t\tibdev_dbg(&dev->ibdev, \"Can't change pkey index\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nint efa_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr,\n\t\t  int qp_attr_mask, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibqp->device);\n\tstruct efa_com_modify_qp_params params = {};\n\tstruct efa_qp *qp = to_eqp(ibqp);\n\tenum ib_qp_state cur_state;\n\tenum ib_qp_state new_state;\n\tint err;\n\n\tif (qp_attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\tif (udata->inlen &&\n\t    !ib_is_udata_cleared(udata, 0, udata->inlen)) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, udata not cleared\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcur_state = qp_attr_mask & IB_QP_CUR_STATE ? qp_attr->cur_qp_state :\n\t\t\t\t\t\t     qp->state;\n\tnew_state = qp_attr_mask & IB_QP_STATE ? qp_attr->qp_state : cur_state;\n\n\terr = efa_modify_qp_validate(dev, qp, qp_attr, qp_attr_mask, cur_state,\n\t\t\t\t     new_state);\n\tif (err)\n\t\treturn err;\n\n\tparams.qp_handle = qp->qp_handle;\n\n\tif (qp_attr_mask & IB_QP_STATE) {\n\t\tEFA_SET(&params.modify_mask, EFA_ADMIN_MODIFY_QP_CMD_QP_STATE,\n\t\t\t1);\n\t\tEFA_SET(&params.modify_mask,\n\t\t\tEFA_ADMIN_MODIFY_QP_CMD_CUR_QP_STATE, 1);\n\t\tparams.cur_qp_state = cur_state;\n\t\tparams.qp_state = new_state;\n\t}\n\n\tif (qp_attr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY) {\n\t\tEFA_SET(&params.modify_mask,\n\t\t\tEFA_ADMIN_MODIFY_QP_CMD_SQ_DRAINED_ASYNC_NOTIFY, 1);\n\t\tparams.sq_drained_async_notify = qp_attr->en_sqd_async_notify;\n\t}\n\n\tif (qp_attr_mask & IB_QP_QKEY) {\n\t\tEFA_SET(&params.modify_mask, EFA_ADMIN_MODIFY_QP_CMD_QKEY, 1);\n\t\tparams.qkey = qp_attr->qkey;\n\t}\n\n\tif (qp_attr_mask & IB_QP_SQ_PSN) {\n\t\tEFA_SET(&params.modify_mask, EFA_ADMIN_MODIFY_QP_CMD_SQ_PSN, 1);\n\t\tparams.sq_psn = qp_attr->sq_psn;\n\t}\n\n\tif (qp_attr_mask & IB_QP_RNR_RETRY) {\n\t\tEFA_SET(&params.modify_mask, EFA_ADMIN_MODIFY_QP_CMD_RNR_RETRY,\n\t\t\t1);\n\t\tparams.rnr_retry = qp_attr->rnr_retry;\n\t}\n\n\terr = efa_com_modify_qp(&dev->edev, &params);\n\tif (err)\n\t\treturn err;\n\n\tqp->state = new_state;\n\n\treturn 0;\n}\n\nstatic int efa_destroy_cq_idx(struct efa_dev *dev, int cq_idx)\n{\n\tstruct efa_com_destroy_cq_params params = { .cq_idx = cq_idx };\n\n\treturn efa_com_destroy_cq(&dev->edev, &params);\n}\n\nstatic void efa_cq_user_mmap_entries_remove(struct efa_cq *cq)\n{\n\trdma_user_mmap_entry_remove(cq->db_mmap_entry);\n\trdma_user_mmap_entry_remove(cq->mmap_entry);\n}\n\nint efa_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibcq->device);\n\tstruct efa_cq *cq = to_ecq(ibcq);\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"Destroy cq[%d] virt[0x%p] freed: size[%lu], dma[%pad]\\n\",\n\t\t  cq->cq_idx, cq->cpu_addr, cq->size, &cq->dma_addr);\n\n\tefa_destroy_cq_idx(dev, cq->cq_idx);\n\tefa_cq_user_mmap_entries_remove(cq);\n\tif (cq->eq) {\n\t\txa_erase(&dev->cqs_xa, cq->cq_idx);\n\t\tsynchronize_irq(cq->eq->irq.irqn);\n\t}\n\tefa_free_mapped(dev, cq->cpu_addr, cq->dma_addr, cq->size,\n\t\t\tDMA_FROM_DEVICE);\n\treturn 0;\n}\n\nstatic struct efa_eq *efa_vec2eq(struct efa_dev *dev, int vec)\n{\n\treturn &dev->eqs[vec];\n}\n\nstatic int cq_mmap_entries_setup(struct efa_dev *dev, struct efa_cq *cq,\n\t\t\t\t struct efa_ibv_create_cq_resp *resp,\n\t\t\t\t bool db_valid)\n{\n\tresp->q_mmap_size = cq->size;\n\tcq->mmap_entry = efa_user_mmap_entry_insert(&cq->ucontext->ibucontext,\n\t\t\t\t\t\t    virt_to_phys(cq->cpu_addr),\n\t\t\t\t\t\t    cq->size, EFA_MMAP_DMA_PAGE,\n\t\t\t\t\t\t    &resp->q_mmap_key);\n\tif (!cq->mmap_entry)\n\t\treturn -ENOMEM;\n\n\tif (db_valid) {\n\t\tcq->db_mmap_entry =\n\t\t\tefa_user_mmap_entry_insert(&cq->ucontext->ibucontext,\n\t\t\t\t\t\t   dev->db_bar_addr + resp->db_off,\n\t\t\t\t\t\t   PAGE_SIZE, EFA_MMAP_IO_NC,\n\t\t\t\t\t\t   &resp->db_mmap_key);\n\t\tif (!cq->db_mmap_entry) {\n\t\t\trdma_user_mmap_entry_remove(cq->mmap_entry);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tresp->db_off &= ~PAGE_MASK;\n\t\tresp->comp_mask |= EFA_CREATE_CQ_RESP_DB_OFF;\n\t}\n\n\treturn 0;\n}\n\nint efa_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct efa_ucontext *ucontext = rdma_udata_to_drv_context(\n\t\tudata, struct efa_ucontext, ibucontext);\n\tstruct efa_com_create_cq_params params = {};\n\tstruct efa_ibv_create_cq_resp resp = {};\n\tstruct efa_com_create_cq_result result;\n\tstruct ib_device *ibdev = ibcq->device;\n\tstruct efa_dev *dev = to_edev(ibdev);\n\tstruct efa_ibv_create_cq cmd = {};\n\tstruct efa_cq *cq = to_ecq(ibcq);\n\tint entries = attr->cqe;\n\tbool set_src_addr;\n\tint err;\n\n\tibdev_dbg(ibdev, \"create_cq entries %d\\n\", entries);\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (entries < 1 || entries > dev->dev_attr.max_cq_depth) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"cq: requested entries[%u] non-positive or greater than max[%u]\\n\",\n\t\t\t  entries, dev->dev_attr.max_cq_depth);\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (offsetofend(typeof(cmd), num_sub_cqs) > udata->inlen) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Incompatible ABI params, no input udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (udata->inlen > sizeof(cmd) &&\n\t    !ib_is_udata_cleared(udata, sizeof(cmd),\n\t\t\t\t udata->inlen - sizeof(cmd))) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Incompatible ABI params, unknown fields in udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\terr = ib_copy_from_udata(&cmd, udata,\n\t\t\t\t min(sizeof(cmd), udata->inlen));\n\tif (err) {\n\t\tibdev_dbg(ibdev, \"Cannot copy udata for create_cq\\n\");\n\t\tgoto err_out;\n\t}\n\n\tif (cmd.comp_mask || !is_reserved_cleared(cmd.reserved_58)) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Incompatible ABI params, unknown fields in udata\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tset_src_addr = !!(cmd.flags & EFA_CREATE_CQ_WITH_SGID);\n\tif ((cmd.cq_entry_size != sizeof(struct efa_io_rx_cdesc_ex)) &&\n\t    (set_src_addr ||\n\t     cmd.cq_entry_size != sizeof(struct efa_io_rx_cdesc))) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Invalid entry size [%u]\\n\", cmd.cq_entry_size);\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (cmd.num_sub_cqs != dev->dev_attr.sub_cqs_per_cq) {\n\t\tibdev_dbg(ibdev,\n\t\t\t  \"Invalid number of sub cqs[%u] expected[%u]\\n\",\n\t\t\t  cmd.num_sub_cqs, dev->dev_attr.sub_cqs_per_cq);\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tcq->ucontext = ucontext;\n\tcq->size = PAGE_ALIGN(cmd.cq_entry_size * entries * cmd.num_sub_cqs);\n\tcq->cpu_addr = efa_zalloc_mapped(dev, &cq->dma_addr, cq->size,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\tif (!cq->cpu_addr) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\tparams.uarn = cq->ucontext->uarn;\n\tparams.cq_depth = entries;\n\tparams.dma_addr = cq->dma_addr;\n\tparams.entry_size_in_bytes = cmd.cq_entry_size;\n\tparams.num_sub_cqs = cmd.num_sub_cqs;\n\tparams.set_src_addr = set_src_addr;\n\tif (cmd.flags & EFA_CREATE_CQ_WITH_COMPLETION_CHANNEL) {\n\t\tcq->eq = efa_vec2eq(dev, attr->comp_vector);\n\t\tparams.eqn = cq->eq->eeq.eqn;\n\t\tparams.interrupt_mode_enabled = true;\n\t}\n\n\terr = efa_com_create_cq(&dev->edev, &params, &result);\n\tif (err)\n\t\tgoto err_free_mapped;\n\n\tresp.db_off = result.db_off;\n\tresp.cq_idx = result.cq_idx;\n\tcq->cq_idx = result.cq_idx;\n\tcq->ibcq.cqe = result.actual_depth;\n\tWARN_ON_ONCE(entries != result.actual_depth);\n\n\terr = cq_mmap_entries_setup(dev, cq, &resp, result.db_valid);\n\tif (err) {\n\t\tibdev_dbg(ibdev, \"Could not setup cq[%u] mmap entries\\n\",\n\t\t\t  cq->cq_idx);\n\t\tgoto err_destroy_cq;\n\t}\n\n\tif (cq->eq) {\n\t\terr = xa_err(xa_store(&dev->cqs_xa, cq->cq_idx, cq, GFP_KERNEL));\n\t\tif (err) {\n\t\t\tibdev_dbg(ibdev, \"Failed to store cq[%u] in xarray\\n\",\n\t\t\t\t  cq->cq_idx);\n\t\t\tgoto err_remove_mmap;\n\t\t}\n\t}\n\n\tif (udata->outlen) {\n\t\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(sizeof(resp), udata->outlen));\n\t\tif (err) {\n\t\t\tibdev_dbg(ibdev,\n\t\t\t\t  \"Failed to copy udata for create_cq\\n\");\n\t\t\tgoto err_xa_erase;\n\t\t}\n\t}\n\n\tibdev_dbg(ibdev, \"Created cq[%d], cq depth[%u]. dma[%pad] virt[0x%p]\\n\",\n\t\t  cq->cq_idx, result.actual_depth, &cq->dma_addr, cq->cpu_addr);\n\n\treturn 0;\n\nerr_xa_erase:\n\tif (cq->eq)\n\t\txa_erase(&dev->cqs_xa, cq->cq_idx);\nerr_remove_mmap:\n\tefa_cq_user_mmap_entries_remove(cq);\nerr_destroy_cq:\n\tefa_destroy_cq_idx(dev, cq->cq_idx);\nerr_free_mapped:\n\tefa_free_mapped(dev, cq->cpu_addr, cq->dma_addr, cq->size,\n\t\t\tDMA_FROM_DEVICE);\n\nerr_out:\n\tatomic64_inc(&dev->stats.create_cq_err);\n\treturn err;\n}\n\nstatic int umem_to_page_list(struct efa_dev *dev,\n\t\t\t     struct ib_umem *umem,\n\t\t\t     u64 *page_list,\n\t\t\t     u32 hp_cnt,\n\t\t\t     u8 hp_shift)\n{\n\tu32 pages_in_hp = BIT(hp_shift - PAGE_SHIFT);\n\tstruct ib_block_iter biter;\n\tunsigned int hp_idx = 0;\n\n\tibdev_dbg(&dev->ibdev, \"hp_cnt[%u], pages_in_hp[%u]\\n\",\n\t\t  hp_cnt, pages_in_hp);\n\n\trdma_umem_for_each_dma_block(umem, &biter, BIT(hp_shift))\n\t\tpage_list[hp_idx++] = rdma_block_iter_dma_address(&biter);\n\n\treturn 0;\n}\n\nstatic struct scatterlist *efa_vmalloc_buf_to_sg(u64 *buf, int page_cnt)\n{\n\tstruct scatterlist *sglist;\n\tstruct page *pg;\n\tint i;\n\n\tsglist = kmalloc_array(page_cnt, sizeof(*sglist), GFP_KERNEL);\n\tif (!sglist)\n\t\treturn NULL;\n\tsg_init_table(sglist, page_cnt);\n\tfor (i = 0; i < page_cnt; i++) {\n\t\tpg = vmalloc_to_page(buf);\n\t\tif (!pg)\n\t\t\tgoto err;\n\t\tsg_set_page(&sglist[i], pg, PAGE_SIZE, 0);\n\t\tbuf += PAGE_SIZE / sizeof(*buf);\n\t}\n\treturn sglist;\n\nerr:\n\tkfree(sglist);\n\treturn NULL;\n}\n\n \nstatic int pbl_chunk_list_create(struct efa_dev *dev, struct pbl_context *pbl)\n{\n\tstruct pbl_chunk_list *chunk_list = &pbl->phys.indirect.chunk_list;\n\tint page_cnt = pbl->phys.indirect.pbl_buf_size_in_pages;\n\tstruct scatterlist *pages_sgl = pbl->phys.indirect.sgl;\n\tunsigned int chunk_list_size, chunk_idx, payload_idx;\n\tint sg_dma_cnt = pbl->phys.indirect.sg_dma_cnt;\n\tstruct efa_com_ctrl_buff_info *ctrl_buf;\n\tu64 *cur_chunk_buf, *prev_chunk_buf;\n\tstruct ib_block_iter biter;\n\tdma_addr_t dma_addr;\n\tint i;\n\n\t \n\tchunk_list_size = DIV_ROUND_UP(page_cnt, EFA_PTRS_PER_CHUNK);\n\n\tchunk_list->size = chunk_list_size;\n\tchunk_list->chunks = kcalloc(chunk_list_size,\n\t\t\t\t     sizeof(*chunk_list->chunks),\n\t\t\t\t     GFP_KERNEL);\n\tif (!chunk_list->chunks)\n\t\treturn -ENOMEM;\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"chunk_list_size[%u] - pages[%u]\\n\", chunk_list_size,\n\t\t  page_cnt);\n\n\t \n\tfor (i = 0; i < chunk_list_size; i++) {\n\t\tchunk_list->chunks[i].buf = kzalloc(EFA_CHUNK_SIZE, GFP_KERNEL);\n\t\tif (!chunk_list->chunks[i].buf)\n\t\t\tgoto chunk_list_dealloc;\n\n\t\tchunk_list->chunks[i].length = EFA_CHUNK_USED_SIZE;\n\t}\n\tchunk_list->chunks[chunk_list_size - 1].length =\n\t\t((page_cnt % EFA_PTRS_PER_CHUNK) * EFA_CHUNK_PAYLOAD_PTR_SIZE) +\n\t\t\tEFA_CHUNK_PTR_SIZE;\n\n\t \n\tchunk_idx = 0;\n\tpayload_idx = 0;\n\tcur_chunk_buf = chunk_list->chunks[0].buf;\n\trdma_for_each_block(pages_sgl, &biter, sg_dma_cnt,\n\t\t\t    EFA_CHUNK_PAYLOAD_SIZE) {\n\t\tcur_chunk_buf[payload_idx++] =\n\t\t\trdma_block_iter_dma_address(&biter);\n\n\t\tif (payload_idx == EFA_PTRS_PER_CHUNK) {\n\t\t\tchunk_idx++;\n\t\t\tcur_chunk_buf = chunk_list->chunks[chunk_idx].buf;\n\t\t\tpayload_idx = 0;\n\t\t}\n\t}\n\n\t \n\tfor (i = chunk_list_size - 1; i >= 0; i--) {\n\t\tdma_addr = dma_map_single(&dev->pdev->dev,\n\t\t\t\t\t  chunk_list->chunks[i].buf,\n\t\t\t\t\t  chunk_list->chunks[i].length,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&dev->pdev->dev, dma_addr)) {\n\t\t\tibdev_err(&dev->ibdev,\n\t\t\t\t  \"chunk[%u] dma_map_failed\\n\", i);\n\t\t\tgoto chunk_list_unmap;\n\t\t}\n\n\t\tchunk_list->chunks[i].dma_addr = dma_addr;\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"chunk[%u] mapped at [%pad]\\n\", i, &dma_addr);\n\n\t\tif (!i)\n\t\t\tbreak;\n\n\t\tprev_chunk_buf = chunk_list->chunks[i - 1].buf;\n\n\t\tctrl_buf = (struct efa_com_ctrl_buff_info *)\n\t\t\t\t&prev_chunk_buf[EFA_PTRS_PER_CHUNK];\n\t\tctrl_buf->length = chunk_list->chunks[i].length;\n\n\t\tefa_com_set_dma_addr(dma_addr,\n\t\t\t\t     &ctrl_buf->address.mem_addr_high,\n\t\t\t\t     &ctrl_buf->address.mem_addr_low);\n\t}\n\n\treturn 0;\n\nchunk_list_unmap:\n\tfor (; i < chunk_list_size; i++) {\n\t\tdma_unmap_single(&dev->pdev->dev, chunk_list->chunks[i].dma_addr,\n\t\t\t\t chunk_list->chunks[i].length, DMA_TO_DEVICE);\n\t}\nchunk_list_dealloc:\n\tfor (i = 0; i < chunk_list_size; i++)\n\t\tkfree(chunk_list->chunks[i].buf);\n\n\tkfree(chunk_list->chunks);\n\treturn -ENOMEM;\n}\n\nstatic void pbl_chunk_list_destroy(struct efa_dev *dev, struct pbl_context *pbl)\n{\n\tstruct pbl_chunk_list *chunk_list = &pbl->phys.indirect.chunk_list;\n\tint i;\n\n\tfor (i = 0; i < chunk_list->size; i++) {\n\t\tdma_unmap_single(&dev->pdev->dev, chunk_list->chunks[i].dma_addr,\n\t\t\t\t chunk_list->chunks[i].length, DMA_TO_DEVICE);\n\t\tkfree(chunk_list->chunks[i].buf);\n\t}\n\n\tkfree(chunk_list->chunks);\n}\n\n \nstatic int pbl_continuous_initialize(struct efa_dev *dev,\n\t\t\t\t     struct pbl_context *pbl)\n{\n\tdma_addr_t dma_addr;\n\n\tdma_addr = dma_map_single(&dev->pdev->dev, pbl->pbl_buf,\n\t\t\t\t  pbl->pbl_buf_size_in_bytes, DMA_TO_DEVICE);\n\tif (dma_mapping_error(&dev->pdev->dev, dma_addr)) {\n\t\tibdev_err(&dev->ibdev, \"Unable to map pbl to DMA address\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpbl->phys.continuous.dma_addr = dma_addr;\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"pbl continuous - dma_addr = %pad, size[%u]\\n\",\n\t\t  &dma_addr, pbl->pbl_buf_size_in_bytes);\n\n\treturn 0;\n}\n\n \nstatic int pbl_indirect_initialize(struct efa_dev *dev, struct pbl_context *pbl)\n{\n\tu32 size_in_pages = DIV_ROUND_UP(pbl->pbl_buf_size_in_bytes, EFA_CHUNK_PAYLOAD_SIZE);\n\tstruct scatterlist *sgl;\n\tint sg_dma_cnt, err;\n\n\tBUILD_BUG_ON(EFA_CHUNK_PAYLOAD_SIZE > PAGE_SIZE);\n\tsgl = efa_vmalloc_buf_to_sg(pbl->pbl_buf, size_in_pages);\n\tif (!sgl)\n\t\treturn -ENOMEM;\n\n\tsg_dma_cnt = dma_map_sg(&dev->pdev->dev, sgl, size_in_pages, DMA_TO_DEVICE);\n\tif (!sg_dma_cnt) {\n\t\terr = -EINVAL;\n\t\tgoto err_map;\n\t}\n\n\tpbl->phys.indirect.pbl_buf_size_in_pages = size_in_pages;\n\tpbl->phys.indirect.sgl = sgl;\n\tpbl->phys.indirect.sg_dma_cnt = sg_dma_cnt;\n\terr = pbl_chunk_list_create(dev, pbl);\n\tif (err) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"chunk_list creation failed[%d]\\n\", err);\n\t\tgoto err_chunk;\n\t}\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"pbl indirect - size[%u], chunks[%u]\\n\",\n\t\t  pbl->pbl_buf_size_in_bytes,\n\t\t  pbl->phys.indirect.chunk_list.size);\n\n\treturn 0;\n\nerr_chunk:\n\tdma_unmap_sg(&dev->pdev->dev, sgl, size_in_pages, DMA_TO_DEVICE);\nerr_map:\n\tkfree(sgl);\n\treturn err;\n}\n\nstatic void pbl_indirect_terminate(struct efa_dev *dev, struct pbl_context *pbl)\n{\n\tpbl_chunk_list_destroy(dev, pbl);\n\tdma_unmap_sg(&dev->pdev->dev, pbl->phys.indirect.sgl,\n\t\t     pbl->phys.indirect.pbl_buf_size_in_pages, DMA_TO_DEVICE);\n\tkfree(pbl->phys.indirect.sgl);\n}\n\n \nstatic int pbl_create(struct efa_dev *dev,\n\t\t      struct pbl_context *pbl,\n\t\t      struct ib_umem *umem,\n\t\t      int hp_cnt,\n\t\t      u8 hp_shift)\n{\n\tint err;\n\n\tpbl->pbl_buf_size_in_bytes = hp_cnt * EFA_CHUNK_PAYLOAD_PTR_SIZE;\n\tpbl->pbl_buf = kvzalloc(pbl->pbl_buf_size_in_bytes, GFP_KERNEL);\n\tif (!pbl->pbl_buf)\n\t\treturn -ENOMEM;\n\n\tif (is_vmalloc_addr(pbl->pbl_buf)) {\n\t\tpbl->physically_continuous = 0;\n\t\terr = umem_to_page_list(dev, umem, pbl->pbl_buf, hp_cnt,\n\t\t\t\t\thp_shift);\n\t\tif (err)\n\t\t\tgoto err_free;\n\n\t\terr = pbl_indirect_initialize(dev, pbl);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t} else {\n\t\tpbl->physically_continuous = 1;\n\t\terr = umem_to_page_list(dev, umem, pbl->pbl_buf, hp_cnt,\n\t\t\t\t\thp_shift);\n\t\tif (err)\n\t\t\tgoto err_free;\n\n\t\terr = pbl_continuous_initialize(dev, pbl);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t}\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"user_pbl_created: user_pages[%u], continuous[%u]\\n\",\n\t\t  hp_cnt, pbl->physically_continuous);\n\n\treturn 0;\n\nerr_free:\n\tkvfree(pbl->pbl_buf);\n\treturn err;\n}\n\nstatic void pbl_destroy(struct efa_dev *dev, struct pbl_context *pbl)\n{\n\tif (pbl->physically_continuous)\n\t\tdma_unmap_single(&dev->pdev->dev, pbl->phys.continuous.dma_addr,\n\t\t\t\t pbl->pbl_buf_size_in_bytes, DMA_TO_DEVICE);\n\telse\n\t\tpbl_indirect_terminate(dev, pbl);\n\n\tkvfree(pbl->pbl_buf);\n}\n\nstatic int efa_create_inline_pbl(struct efa_dev *dev, struct efa_mr *mr,\n\t\t\t\t struct efa_com_reg_mr_params *params)\n{\n\tint err;\n\n\tparams->inline_pbl = 1;\n\terr = umem_to_page_list(dev, mr->umem, params->pbl.inline_pbl_array,\n\t\t\t\tparams->page_num, params->page_shift);\n\tif (err)\n\t\treturn err;\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"inline_pbl_array - pages[%u]\\n\", params->page_num);\n\n\treturn 0;\n}\n\nstatic int efa_create_pbl(struct efa_dev *dev,\n\t\t\t  struct pbl_context *pbl,\n\t\t\t  struct efa_mr *mr,\n\t\t\t  struct efa_com_reg_mr_params *params)\n{\n\tint err;\n\n\terr = pbl_create(dev, pbl, mr->umem, params->page_num,\n\t\t\t params->page_shift);\n\tif (err) {\n\t\tibdev_dbg(&dev->ibdev, \"Failed to create pbl[%d]\\n\", err);\n\t\treturn err;\n\t}\n\n\tparams->inline_pbl = 0;\n\tparams->indirect = !pbl->physically_continuous;\n\tif (pbl->physically_continuous) {\n\t\tparams->pbl.pbl.length = pbl->pbl_buf_size_in_bytes;\n\n\t\tefa_com_set_dma_addr(pbl->phys.continuous.dma_addr,\n\t\t\t\t     &params->pbl.pbl.address.mem_addr_high,\n\t\t\t\t     &params->pbl.pbl.address.mem_addr_low);\n\t} else {\n\t\tparams->pbl.pbl.length =\n\t\t\tpbl->phys.indirect.chunk_list.chunks[0].length;\n\n\t\tefa_com_set_dma_addr(pbl->phys.indirect.chunk_list.chunks[0].dma_addr,\n\t\t\t\t     &params->pbl.pbl.address.mem_addr_high,\n\t\t\t\t     &params->pbl.pbl.address.mem_addr_low);\n\t}\n\n\treturn 0;\n}\n\nstatic struct efa_mr *efa_alloc_mr(struct ib_pd *ibpd, int access_flags,\n\t\t\t\t   struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tint supp_access_flags;\n\tstruct efa_mr *mr;\n\n\tif (udata && udata->inlen &&\n\t    !ib_is_udata_cleared(udata, 0, sizeof(udata->inlen))) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Incompatible ABI params, udata not cleared\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tsupp_access_flags =\n\t\tIB_ACCESS_LOCAL_WRITE |\n\t\t(EFA_DEV_CAP(dev, RDMA_READ) ? IB_ACCESS_REMOTE_READ : 0) |\n\t\t(EFA_DEV_CAP(dev, RDMA_WRITE) ? IB_ACCESS_REMOTE_WRITE : 0);\n\n\taccess_flags &= ~IB_ACCESS_OPTIONAL;\n\tif (access_flags & ~supp_access_flags) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Unsupported access flags[%#x], supported[%#x]\\n\",\n\t\t\t  access_flags, supp_access_flags);\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn mr;\n}\n\nstatic int efa_register_mr(struct ib_pd *ibpd, struct efa_mr *mr, u64 start,\n\t\t\t   u64 length, u64 virt_addr, int access_flags)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tstruct efa_com_reg_mr_params params = {};\n\tstruct efa_com_reg_mr_result result = {};\n\tstruct pbl_context pbl;\n\tunsigned int pg_sz;\n\tint inline_size;\n\tint err;\n\n\tparams.pd = to_epd(ibpd)->pdn;\n\tparams.iova = virt_addr;\n\tparams.mr_length_in_bytes = length;\n\tparams.permissions = access_flags;\n\n\tpg_sz = ib_umem_find_best_pgsz(mr->umem,\n\t\t\t\t       dev->dev_attr.page_size_cap,\n\t\t\t\t       virt_addr);\n\tif (!pg_sz) {\n\t\tibdev_dbg(&dev->ibdev, \"Failed to find a suitable page size in page_size_cap %#llx\\n\",\n\t\t\t  dev->dev_attr.page_size_cap);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tparams.page_shift = order_base_2(pg_sz);\n\tparams.page_num = ib_umem_num_dma_blocks(mr->umem, pg_sz);\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"start %#llx length %#llx params.page_shift %u params.page_num %u\\n\",\n\t\t  start, length, params.page_shift, params.page_num);\n\n\tinline_size = ARRAY_SIZE(params.pbl.inline_pbl_array);\n\tif (params.page_num <= inline_size) {\n\t\terr = efa_create_inline_pbl(dev, mr, &params);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = efa_com_register_mr(&dev->edev, &params, &result);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\terr = efa_create_pbl(dev, &pbl, mr, &params);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = efa_com_register_mr(&dev->edev, &params, &result);\n\t\tpbl_destroy(dev, &pbl);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tmr->ibmr.lkey = result.l_key;\n\tmr->ibmr.rkey = result.r_key;\n\tmr->ibmr.length = length;\n\tibdev_dbg(&dev->ibdev, \"Registered mr[%d]\\n\", mr->ibmr.lkey);\n\n\treturn 0;\n}\n\nstruct ib_mr *efa_reg_user_mr_dmabuf(struct ib_pd *ibpd, u64 start,\n\t\t\t\t     u64 length, u64 virt_addr,\n\t\t\t\t     int fd, int access_flags,\n\t\t\t\t     struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tstruct ib_umem_dmabuf *umem_dmabuf;\n\tstruct efa_mr *mr;\n\tint err;\n\n\tmr = efa_alloc_mr(ibpd, access_flags, udata);\n\tif (IS_ERR(mr)) {\n\t\terr = PTR_ERR(mr);\n\t\tgoto err_out;\n\t}\n\n\tumem_dmabuf = ib_umem_dmabuf_get_pinned(ibpd->device, start, length, fd,\n\t\t\t\t\t\taccess_flags);\n\tif (IS_ERR(umem_dmabuf)) {\n\t\terr = PTR_ERR(umem_dmabuf);\n\t\tibdev_dbg(&dev->ibdev, \"Failed to get dmabuf umem[%d]\\n\", err);\n\t\tgoto err_free;\n\t}\n\n\tmr->umem = &umem_dmabuf->umem;\n\terr = efa_register_mr(ibpd, mr, start, length, virt_addr, access_flags);\n\tif (err)\n\t\tgoto err_release;\n\n\treturn &mr->ibmr;\n\nerr_release:\n\tib_umem_release(mr->umem);\nerr_free:\n\tkfree(mr);\nerr_out:\n\tatomic64_inc(&dev->stats.reg_mr_err);\n\treturn ERR_PTR(err);\n}\n\nstruct ib_mr *efa_reg_mr(struct ib_pd *ibpd, u64 start, u64 length,\n\t\t\t u64 virt_addr, int access_flags,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibpd->device);\n\tstruct efa_mr *mr;\n\tint err;\n\n\tmr = efa_alloc_mr(ibpd, access_flags, udata);\n\tif (IS_ERR(mr)) {\n\t\terr = PTR_ERR(mr);\n\t\tgoto err_out;\n\t}\n\n\tmr->umem = ib_umem_get(ibpd->device, start, length, access_flags);\n\tif (IS_ERR(mr->umem)) {\n\t\terr = PTR_ERR(mr->umem);\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Failed to pin and map user space memory[%d]\\n\", err);\n\t\tgoto err_free;\n\t}\n\n\terr = efa_register_mr(ibpd, mr, start, length, virt_addr, access_flags);\n\tif (err)\n\t\tgoto err_release;\n\n\treturn &mr->ibmr;\n\nerr_release:\n\tib_umem_release(mr->umem);\nerr_free:\n\tkfree(mr);\nerr_out:\n\tatomic64_inc(&dev->stats.reg_mr_err);\n\treturn ERR_PTR(err);\n}\n\nint efa_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct efa_dev *dev = to_edev(ibmr->device);\n\tstruct efa_com_dereg_mr_params params;\n\tstruct efa_mr *mr = to_emr(ibmr);\n\tint err;\n\n\tibdev_dbg(&dev->ibdev, \"Deregister mr[%d]\\n\", ibmr->lkey);\n\n\tparams.l_key = mr->ibmr.lkey;\n\terr = efa_com_dereg_mr(&dev->edev, &params);\n\tif (err)\n\t\treturn err;\n\n\tib_umem_release(mr->umem);\n\tkfree(mr);\n\n\treturn 0;\n}\n\nint efa_get_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t   struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tint err;\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err) {\n\t\tibdev_dbg(ibdev, \"Couldn't query port err[%d]\\n\", err);\n\t\treturn err;\n\t}\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\n\treturn 0;\n}\n\nstatic int efa_dealloc_uar(struct efa_dev *dev, u16 uarn)\n{\n\tstruct efa_com_dealloc_uar_params params = {\n\t\t.uarn = uarn,\n\t};\n\n\treturn efa_com_dealloc_uar(&dev->edev, &params);\n}\n\n#define EFA_CHECK_USER_COMP(_dev, _comp_mask, _attr, _mask, _attr_str) \\\n\t(_attr_str = (!(_dev)->dev_attr._attr || ((_comp_mask) & (_mask))) ? \\\n\t\t     NULL : #_attr)\n\nstatic int efa_user_comp_handshake(const struct ib_ucontext *ibucontext,\n\t\t\t\t   const struct efa_ibv_alloc_ucontext_cmd *cmd)\n{\n\tstruct efa_dev *dev = to_edev(ibucontext->device);\n\tchar *attr_str;\n\n\tif (EFA_CHECK_USER_COMP(dev, cmd->comp_mask, max_tx_batch,\n\t\t\t\tEFA_ALLOC_UCONTEXT_CMD_COMP_TX_BATCH, attr_str))\n\t\tgoto err;\n\n\tif (EFA_CHECK_USER_COMP(dev, cmd->comp_mask, min_sq_depth,\n\t\t\t\tEFA_ALLOC_UCONTEXT_CMD_COMP_MIN_SQ_WR,\n\t\t\t\tattr_str))\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tibdev_dbg(&dev->ibdev, \"Userspace handshake failed for %s attribute\\n\",\n\t\t  attr_str);\n\treturn -EOPNOTSUPP;\n}\n\nint efa_alloc_ucontext(struct ib_ucontext *ibucontext, struct ib_udata *udata)\n{\n\tstruct efa_ucontext *ucontext = to_eucontext(ibucontext);\n\tstruct efa_dev *dev = to_edev(ibucontext->device);\n\tstruct efa_ibv_alloc_ucontext_resp resp = {};\n\tstruct efa_ibv_alloc_ucontext_cmd cmd = {};\n\tstruct efa_com_alloc_uar_result result;\n\tint err;\n\n\t \n\n\terr = ib_copy_from_udata(&cmd, udata,\n\t\t\t\t min(sizeof(cmd), udata->inlen));\n\tif (err) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Cannot copy udata for alloc_ucontext\\n\");\n\t\tgoto err_out;\n\t}\n\n\terr = efa_user_comp_handshake(ibucontext, &cmd);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = efa_com_alloc_uar(&dev->edev, &result);\n\tif (err)\n\t\tgoto err_out;\n\n\tucontext->uarn = result.uarn;\n\n\tresp.cmds_supp_udata_mask |= EFA_USER_CMDS_SUPP_UDATA_QUERY_DEVICE;\n\tresp.cmds_supp_udata_mask |= EFA_USER_CMDS_SUPP_UDATA_CREATE_AH;\n\tresp.sub_cqs_per_cq = dev->dev_attr.sub_cqs_per_cq;\n\tresp.inline_buf_size = dev->dev_attr.inline_buf_size;\n\tresp.max_llq_size = dev->dev_attr.max_llq_size;\n\tresp.max_tx_batch = dev->dev_attr.max_tx_batch;\n\tresp.min_sq_wr = dev->dev_attr.min_sq_depth;\n\n\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t       min(sizeof(resp), udata->outlen));\n\tif (err)\n\t\tgoto err_dealloc_uar;\n\n\treturn 0;\n\nerr_dealloc_uar:\n\tefa_dealloc_uar(dev, result.uarn);\nerr_out:\n\tatomic64_inc(&dev->stats.alloc_ucontext_err);\n\treturn err;\n}\n\nvoid efa_dealloc_ucontext(struct ib_ucontext *ibucontext)\n{\n\tstruct efa_ucontext *ucontext = to_eucontext(ibucontext);\n\tstruct efa_dev *dev = to_edev(ibucontext->device);\n\n\tefa_dealloc_uar(dev, ucontext->uarn);\n}\n\nvoid efa_mmap_free(struct rdma_user_mmap_entry *rdma_entry)\n{\n\tstruct efa_user_mmap_entry *entry = to_emmap(rdma_entry);\n\n\tkfree(entry);\n}\n\nstatic int __efa_mmap(struct efa_dev *dev, struct efa_ucontext *ucontext,\n\t\t      struct vm_area_struct *vma)\n{\n\tstruct rdma_user_mmap_entry *rdma_entry;\n\tstruct efa_user_mmap_entry *entry;\n\tunsigned long va;\n\tint err = 0;\n\tu64 pfn;\n\n\trdma_entry = rdma_user_mmap_entry_get(&ucontext->ibucontext, vma);\n\tif (!rdma_entry) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"pgoff[%#lx] does not have valid entry\\n\",\n\t\t\t  vma->vm_pgoff);\n\t\tatomic64_inc(&dev->stats.mmap_err);\n\t\treturn -EINVAL;\n\t}\n\tentry = to_emmap(rdma_entry);\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"Mapping address[%#llx], length[%#zx], mmap_flag[%d]\\n\",\n\t\t  entry->address, rdma_entry->npages * PAGE_SIZE,\n\t\t  entry->mmap_flag);\n\n\tpfn = entry->address >> PAGE_SHIFT;\n\tswitch (entry->mmap_flag) {\n\tcase EFA_MMAP_IO_NC:\n\t\terr = rdma_user_mmap_io(&ucontext->ibucontext, vma, pfn,\n\t\t\t\t\tentry->rdma_entry.npages * PAGE_SIZE,\n\t\t\t\t\tpgprot_noncached(vma->vm_page_prot),\n\t\t\t\t\trdma_entry);\n\t\tbreak;\n\tcase EFA_MMAP_IO_WC:\n\t\terr = rdma_user_mmap_io(&ucontext->ibucontext, vma, pfn,\n\t\t\t\t\tentry->rdma_entry.npages * PAGE_SIZE,\n\t\t\t\t\tpgprot_writecombine(vma->vm_page_prot),\n\t\t\t\t\trdma_entry);\n\t\tbreak;\n\tcase EFA_MMAP_DMA_PAGE:\n\t\tfor (va = vma->vm_start; va < vma->vm_end;\n\t\t     va += PAGE_SIZE, pfn++) {\n\t\t\terr = vm_insert_page(vma, va, pfn_to_page(pfn));\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\n\tif (err) {\n\t\tibdev_dbg(\n\t\t\t&dev->ibdev,\n\t\t\t\"Couldn't mmap address[%#llx] length[%#zx] mmap_flag[%d] err[%d]\\n\",\n\t\t\tentry->address, rdma_entry->npages * PAGE_SIZE,\n\t\t\tentry->mmap_flag, err);\n\t\tatomic64_inc(&dev->stats.mmap_err);\n\t}\n\n\trdma_user_mmap_entry_put(rdma_entry);\n\treturn err;\n}\n\nint efa_mmap(struct ib_ucontext *ibucontext,\n\t     struct vm_area_struct *vma)\n{\n\tstruct efa_ucontext *ucontext = to_eucontext(ibucontext);\n\tstruct efa_dev *dev = to_edev(ibucontext->device);\n\tsize_t length = vma->vm_end - vma->vm_start;\n\n\tibdev_dbg(&dev->ibdev,\n\t\t  \"start %#lx, end %#lx, length = %#zx, pgoff = %#lx\\n\",\n\t\t  vma->vm_start, vma->vm_end, length, vma->vm_pgoff);\n\n\treturn __efa_mmap(dev, ucontext, vma);\n}\n\nstatic int efa_ah_destroy(struct efa_dev *dev, struct efa_ah *ah)\n{\n\tstruct efa_com_destroy_ah_params params = {\n\t\t.ah = ah->ah,\n\t\t.pdn = to_epd(ah->ibah.pd)->pdn,\n\t};\n\n\treturn efa_com_destroy_ah(&dev->edev, &params);\n}\n\nint efa_create_ah(struct ib_ah *ibah,\n\t\t  struct rdma_ah_init_attr *init_attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct rdma_ah_attr *ah_attr = init_attr->ah_attr;\n\tstruct efa_dev *dev = to_edev(ibah->device);\n\tstruct efa_com_create_ah_params params = {};\n\tstruct efa_ibv_create_ah_resp resp = {};\n\tstruct efa_com_create_ah_result result;\n\tstruct efa_ah *ah = to_eah(ibah);\n\tint err;\n\n\tif (!(init_attr->flags & RDMA_CREATE_AH_SLEEPABLE)) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Create address handle is not supported in atomic context\\n\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_out;\n\t}\n\n\tif (udata->inlen &&\n\t    !ib_is_udata_cleared(udata, 0, udata->inlen)) {\n\t\tibdev_dbg(&dev->ibdev, \"Incompatible ABI params\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tmemcpy(params.dest_addr, ah_attr->grh.dgid.raw,\n\t       sizeof(params.dest_addr));\n\tparams.pdn = to_epd(ibah->pd)->pdn;\n\terr = efa_com_create_ah(&dev->edev, &params, &result);\n\tif (err)\n\t\tgoto err_out;\n\n\tmemcpy(ah->id, ah_attr->grh.dgid.raw, sizeof(ah->id));\n\tah->ah = result.ah;\n\n\tresp.efa_address_handle = result.ah;\n\n\tif (udata->outlen) {\n\t\terr = ib_copy_to_udata(udata, &resp,\n\t\t\t\t       min(sizeof(resp), udata->outlen));\n\t\tif (err) {\n\t\t\tibdev_dbg(&dev->ibdev,\n\t\t\t\t  \"Failed to copy udata for create_ah response\\n\");\n\t\t\tgoto err_destroy_ah;\n\t\t}\n\t}\n\tibdev_dbg(&dev->ibdev, \"Created ah[%d]\\n\", ah->ah);\n\n\treturn 0;\n\nerr_destroy_ah:\n\tefa_ah_destroy(dev, ah);\nerr_out:\n\tatomic64_inc(&dev->stats.create_ah_err);\n\treturn err;\n}\n\nint efa_destroy_ah(struct ib_ah *ibah, u32 flags)\n{\n\tstruct efa_dev *dev = to_edev(ibah->pd->device);\n\tstruct efa_ah *ah = to_eah(ibah);\n\n\tibdev_dbg(&dev->ibdev, \"Destroy ah[%d]\\n\", ah->ah);\n\n\tif (!(flags & RDMA_DESTROY_AH_SLEEPABLE)) {\n\t\tibdev_dbg(&dev->ibdev,\n\t\t\t  \"Destroy address handle is not supported in atomic context\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tefa_ah_destroy(dev, ah);\n\treturn 0;\n}\n\nstruct rdma_hw_stats *efa_alloc_hw_port_stats(struct ib_device *ibdev,\n\t\t\t\t\t      u32 port_num)\n{\n\treturn rdma_alloc_hw_stats_struct(efa_port_stats_descs,\n\t\t\t\t\t  ARRAY_SIZE(efa_port_stats_descs),\n\t\t\t\t\t  RDMA_HW_STATS_DEFAULT_LIFESPAN);\n}\n\nstruct rdma_hw_stats *efa_alloc_hw_device_stats(struct ib_device *ibdev)\n{\n\treturn rdma_alloc_hw_stats_struct(efa_device_stats_descs,\n\t\t\t\t\t  ARRAY_SIZE(efa_device_stats_descs),\n\t\t\t\t\t  RDMA_HW_STATS_DEFAULT_LIFESPAN);\n}\n\nstatic int efa_fill_device_stats(struct efa_dev *dev,\n\t\t\t\t struct rdma_hw_stats *stats)\n{\n\tstruct efa_com_stats_admin *as = &dev->edev.aq.stats;\n\tstruct efa_stats *s = &dev->stats;\n\n\tstats->value[EFA_SUBMITTED_CMDS] = atomic64_read(&as->submitted_cmd);\n\tstats->value[EFA_COMPLETED_CMDS] = atomic64_read(&as->completed_cmd);\n\tstats->value[EFA_CMDS_ERR] = atomic64_read(&as->cmd_err);\n\tstats->value[EFA_NO_COMPLETION_CMDS] = atomic64_read(&as->no_completion);\n\n\tstats->value[EFA_KEEP_ALIVE_RCVD] = atomic64_read(&s->keep_alive_rcvd);\n\tstats->value[EFA_ALLOC_PD_ERR] = atomic64_read(&s->alloc_pd_err);\n\tstats->value[EFA_CREATE_QP_ERR] = atomic64_read(&s->create_qp_err);\n\tstats->value[EFA_CREATE_CQ_ERR] = atomic64_read(&s->create_cq_err);\n\tstats->value[EFA_REG_MR_ERR] = atomic64_read(&s->reg_mr_err);\n\tstats->value[EFA_ALLOC_UCONTEXT_ERR] =\n\t\tatomic64_read(&s->alloc_ucontext_err);\n\tstats->value[EFA_CREATE_AH_ERR] = atomic64_read(&s->create_ah_err);\n\tstats->value[EFA_MMAP_ERR] = atomic64_read(&s->mmap_err);\n\n\treturn ARRAY_SIZE(efa_device_stats_descs);\n}\n\nstatic int efa_fill_port_stats(struct efa_dev *dev, struct rdma_hw_stats *stats,\n\t\t\t       u32 port_num)\n{\n\tstruct efa_com_get_stats_params params = {};\n\tunion efa_com_get_stats_result result;\n\tstruct efa_com_rdma_write_stats *rws;\n\tstruct efa_com_rdma_read_stats *rrs;\n\tstruct efa_com_messages_stats *ms;\n\tstruct efa_com_basic_stats *bs;\n\tint err;\n\n\tparams.scope = EFA_ADMIN_GET_STATS_SCOPE_ALL;\n\tparams.type = EFA_ADMIN_GET_STATS_TYPE_BASIC;\n\n\terr = efa_com_get_stats(&dev->edev, &params, &result);\n\tif (err)\n\t\treturn err;\n\n\tbs = &result.basic_stats;\n\tstats->value[EFA_TX_BYTES] = bs->tx_bytes;\n\tstats->value[EFA_TX_PKTS] = bs->tx_pkts;\n\tstats->value[EFA_RX_BYTES] = bs->rx_bytes;\n\tstats->value[EFA_RX_PKTS] = bs->rx_pkts;\n\tstats->value[EFA_RX_DROPS] = bs->rx_drops;\n\n\tparams.type = EFA_ADMIN_GET_STATS_TYPE_MESSAGES;\n\terr = efa_com_get_stats(&dev->edev, &params, &result);\n\tif (err)\n\t\treturn err;\n\n\tms = &result.messages_stats;\n\tstats->value[EFA_SEND_BYTES] = ms->send_bytes;\n\tstats->value[EFA_SEND_WRS] = ms->send_wrs;\n\tstats->value[EFA_RECV_BYTES] = ms->recv_bytes;\n\tstats->value[EFA_RECV_WRS] = ms->recv_wrs;\n\n\tparams.type = EFA_ADMIN_GET_STATS_TYPE_RDMA_READ;\n\terr = efa_com_get_stats(&dev->edev, &params, &result);\n\tif (err)\n\t\treturn err;\n\n\trrs = &result.rdma_read_stats;\n\tstats->value[EFA_RDMA_READ_WRS] = rrs->read_wrs;\n\tstats->value[EFA_RDMA_READ_BYTES] = rrs->read_bytes;\n\tstats->value[EFA_RDMA_READ_WR_ERR] = rrs->read_wr_err;\n\tstats->value[EFA_RDMA_READ_RESP_BYTES] = rrs->read_resp_bytes;\n\n\tif (EFA_DEV_CAP(dev, RDMA_WRITE)) {\n\t\tparams.type = EFA_ADMIN_GET_STATS_TYPE_RDMA_WRITE;\n\t\terr = efa_com_get_stats(&dev->edev, &params, &result);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\trws = &result.rdma_write_stats;\n\t\tstats->value[EFA_RDMA_WRITE_WRS] = rws->write_wrs;\n\t\tstats->value[EFA_RDMA_WRITE_BYTES] = rws->write_bytes;\n\t\tstats->value[EFA_RDMA_WRITE_WR_ERR] = rws->write_wr_err;\n\t\tstats->value[EFA_RDMA_WRITE_RECV_BYTES] = rws->write_recv_bytes;\n\t}\n\n\treturn ARRAY_SIZE(efa_port_stats_descs);\n}\n\nint efa_get_hw_stats(struct ib_device *ibdev, struct rdma_hw_stats *stats,\n\t\t     u32 port_num, int index)\n{\n\tif (port_num)\n\t\treturn efa_fill_port_stats(to_edev(ibdev), stats, port_num);\n\telse\n\t\treturn efa_fill_device_stats(to_edev(ibdev), stats);\n}\n\nenum rdma_link_layer efa_port_link_layer(struct ib_device *ibdev,\n\t\t\t\t\t u32 port_num)\n{\n\treturn IB_LINK_LAYER_UNSPECIFIED;\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}