{
  "module_name": "pin_system.c",
  "hash_id": "4fd242f194db85faa191c3cb9f5ac5f87d6bfce0e13952c00a673086a9eb4eb3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/pin_system.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n\n#include \"hfi.h\"\n#include \"common.h\"\n#include \"device.h\"\n#include \"pinning.h\"\n#include \"mmu_rb.h\"\n#include \"user_sdma.h\"\n#include \"trace.h\"\n\nstruct sdma_mmu_node {\n\tstruct mmu_rb_node rb;\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\tstruct page **pages;\n\tunsigned int npages;\n};\n\nstatic bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,\n\t\t\t   unsigned long len);\nstatic int sdma_rb_evict(void *arg, struct mmu_rb_node *mnode, void *arg2,\n\t\t\t bool *stop);\nstatic void sdma_rb_remove(void *arg, struct mmu_rb_node *mnode);\n\nstatic struct mmu_rb_ops sdma_rb_ops = {\n\t.filter = sdma_rb_filter,\n\t.evict = sdma_rb_evict,\n\t.remove = sdma_rb_remove,\n};\n\nint hfi1_init_system_pinning(struct hfi1_user_sdma_pkt_q *pq)\n{\n\tstruct hfi1_devdata *dd = pq->dd;\n\tint ret;\n\n\tret = hfi1_mmu_rb_register(pq, &sdma_rb_ops, dd->pport->hfi1_wq,\n\t\t\t\t   &pq->handler);\n\tif (ret)\n\t\tdd_dev_err(dd,\n\t\t\t   \"[%u:%u] Failed to register system memory DMA support with MMU: %d\\n\",\n\t\t\t   pq->ctxt, pq->subctxt, ret);\n\treturn ret;\n}\n\nvoid hfi1_free_system_pinning(struct hfi1_user_sdma_pkt_q *pq)\n{\n\tif (pq->handler)\n\t\thfi1_mmu_rb_unregister(pq->handler);\n}\n\nstatic u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)\n{\n\tstruct evict_data evict_data;\n\n\tevict_data.cleared = 0;\n\tevict_data.target = npages;\n\thfi1_mmu_rb_evict(pq->handler, &evict_data);\n\treturn evict_data.cleared;\n}\n\nstatic void unpin_vector_pages(struct mm_struct *mm, struct page **pages,\n\t\t\t       unsigned int start, unsigned int npages)\n{\n\thfi1_release_user_pages(mm, pages + start, npages, false);\n\tkfree(pages);\n}\n\nstatic inline struct mm_struct *mm_from_sdma_node(struct sdma_mmu_node *node)\n{\n\treturn node->rb.handler->mn.mm;\n}\n\nstatic void free_system_node(struct sdma_mmu_node *node)\n{\n\tif (node->npages) {\n\t\tunpin_vector_pages(mm_from_sdma_node(node), node->pages, 0,\n\t\t\t\t   node->npages);\n\t\tatomic_sub(node->npages, &node->pq->n_locked);\n\t}\n\tkfree(node);\n}\n\n \nstatic struct sdma_mmu_node *find_system_node(struct mmu_rb_handler *handler,\n\t\t\t\t\t      unsigned long start,\n\t\t\t\t\t      unsigned long end)\n{\n\tstruct mmu_rb_node *rb_node;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\trb_node = hfi1_mmu_rb_get_first(handler, start, (end - start));\n\tif (!rb_node) {\n\t\tspin_unlock_irqrestore(&handler->lock, flags);\n\t\treturn NULL;\n\t}\n\n\t \n\tkref_get(&rb_node->refcount);\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\treturn container_of(rb_node, struct sdma_mmu_node, rb);\n}\n\nstatic int pin_system_pages(struct user_sdma_request *req,\n\t\t\t    uintptr_t start_address, size_t length,\n\t\t\t    struct sdma_mmu_node *node, int npages)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tint pinned, cleared;\n\tstruct page **pages;\n\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\nretry:\n\tif (!hfi1_can_pin_pages(pq->dd, current->mm, atomic_read(&pq->n_locked),\n\t\t\t\tnpages)) {\n\t\tSDMA_DBG(req, \"Evicting: nlocked %u npages %u\",\n\t\t\t atomic_read(&pq->n_locked), npages);\n\t\tcleared = sdma_cache_evict(pq, npages);\n\t\tif (cleared >= npages)\n\t\t\tgoto retry;\n\t}\n\n\tSDMA_DBG(req, \"Acquire user pages start_address %lx node->npages %u npages %u\",\n\t\t start_address, node->npages, npages);\n\tpinned = hfi1_acquire_user_pages(current->mm, start_address, npages, 0,\n\t\t\t\t\t pages);\n\n\tif (pinned < 0) {\n\t\tkfree(pages);\n\t\tSDMA_DBG(req, \"pinned %d\", pinned);\n\t\treturn pinned;\n\t}\n\tif (pinned != npages) {\n\t\tunpin_vector_pages(current->mm, pages, node->npages, pinned);\n\t\tSDMA_DBG(req, \"npages %u pinned %d\", npages, pinned);\n\t\treturn -EFAULT;\n\t}\n\tnode->rb.addr = start_address;\n\tnode->rb.len = length;\n\tnode->pages = pages;\n\tnode->npages = npages;\n\tatomic_add(pinned, &pq->n_locked);\n\tSDMA_DBG(req, \"done. pinned %d\", pinned);\n\treturn 0;\n}\n\n \nstatic int add_system_pinning(struct user_sdma_request *req,\n\t\t\t      struct sdma_mmu_node **node_p,\n\t\t\t      unsigned long start, unsigned long len)\n\n{\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tstruct sdma_mmu_node *node;\n\tint ret;\n\n\tnode = kzalloc(sizeof(*node), GFP_KERNEL);\n\tif (!node)\n\t\treturn -ENOMEM;\n\n\t \n\tkref_init(&node->rb.refcount);\n\n\t \n\tkref_get(&node->rb.refcount);\n\n\tnode->pq = pq;\n\tret = pin_system_pages(req, start, len, node, PFN_DOWN(len));\n\tif (ret == 0) {\n\t\tret = hfi1_mmu_rb_insert(pq->handler, &node->rb);\n\t\tif (ret)\n\t\t\tfree_system_node(node);\n\t\telse\n\t\t\t*node_p = node;\n\n\t\treturn ret;\n\t}\n\n\tkfree(node);\n\treturn ret;\n}\n\nstatic int get_system_cache_entry(struct user_sdma_request *req,\n\t\t\t\t  struct sdma_mmu_node **node_p,\n\t\t\t\t  size_t req_start, size_t req_len)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tu64 start = ALIGN_DOWN(req_start, PAGE_SIZE);\n\tu64 end = PFN_ALIGN(req_start + req_len);\n\tint ret;\n\n\tif ((end - start) == 0) {\n\t\tSDMA_DBG(req,\n\t\t\t \"Request for empty cache entry req_start %lx req_len %lx start %llx end %llx\",\n\t\t\t req_start, req_len, start, end);\n\t\treturn -EINVAL;\n\t}\n\n\tSDMA_DBG(req, \"req_start %lx req_len %lu\", req_start, req_len);\n\n\twhile (1) {\n\t\tstruct sdma_mmu_node *node =\n\t\t\tfind_system_node(pq->handler, start, end);\n\t\tu64 prepend_len = 0;\n\n\t\tSDMA_DBG(req, \"node %p start %llx end %llu\", node, start, end);\n\t\tif (!node) {\n\t\t\tret = add_system_pinning(req, node_p, start,\n\t\t\t\t\t\t end - start);\n\t\t\tif (ret == -EEXIST) {\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (node->rb.addr <= start) {\n\t\t\t \n\t\t\t*node_p = node;\n\t\t\treturn 0;\n\t\t}\n\n\t\tSDMA_DBG(req, \"prepend: node->rb.addr %lx, node->rb.refcount %d\",\n\t\t\t node->rb.addr, kref_read(&node->rb.refcount));\n\t\tprepend_len = node->rb.addr - start;\n\n\t\t \n\t\tkref_put(&node->rb.refcount, hfi1_mmu_rb_release);\n\n\t\t \n\t\tret = add_system_pinning(req, node_p, start, prepend_len);\n\t\tif (ret == -EEXIST) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\treturn ret;\n\t}\n}\n\nstatic void sdma_mmu_rb_node_get(void *ctx)\n{\n\tstruct mmu_rb_node *node = ctx;\n\n\tkref_get(&node->refcount);\n}\n\nstatic void sdma_mmu_rb_node_put(void *ctx)\n{\n\tstruct sdma_mmu_node *node = ctx;\n\n\tkref_put(&node->rb.refcount, hfi1_mmu_rb_release);\n}\n\nstatic int add_mapping_to_sdma_packet(struct user_sdma_request *req,\n\t\t\t\t      struct user_sdma_txreq *tx,\n\t\t\t\t      struct sdma_mmu_node *cache_entry,\n\t\t\t\t      size_t start,\n\t\t\t\t      size_t from_this_cache_entry)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tunsigned int page_offset;\n\tunsigned int from_this_page;\n\tsize_t page_index;\n\tvoid *ctx;\n\tint ret;\n\n\t \n\n\twhile (from_this_cache_entry) {\n\t\tpage_index = PFN_DOWN(start - cache_entry->rb.addr);\n\n\t\tif (page_index >= cache_entry->npages) {\n\t\t\tSDMA_DBG(req,\n\t\t\t\t \"Request for page_index %zu >= cache_entry->npages %u\",\n\t\t\t\t page_index, cache_entry->npages);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpage_offset = start - ALIGN_DOWN(start, PAGE_SIZE);\n\t\tfrom_this_page = PAGE_SIZE - page_offset;\n\n\t\tif (from_this_page < from_this_cache_entry) {\n\t\t\tctx = NULL;\n\t\t} else {\n\t\t\t \n\t\t\tfrom_this_page = from_this_cache_entry;\n\t\t\tctx = cache_entry;\n\t\t}\n\n\t\tret = sdma_txadd_page(pq->dd, &tx->txreq,\n\t\t\t\t      cache_entry->pages[page_index],\n\t\t\t\t      page_offset, from_this_page,\n\t\t\t\t      ctx,\n\t\t\t\t      sdma_mmu_rb_node_get,\n\t\t\t\t      sdma_mmu_rb_node_put);\n\t\tif (ret) {\n\t\t\t \n\t\t\tSDMA_DBG(req,\n\t\t\t\t \"sdma_txadd_page failed %d page_index %lu page_offset %u from_this_page %u\",\n\t\t\t\t ret, page_index, page_offset, from_this_page);\n\t\t\treturn ret;\n\t\t}\n\t\tstart += from_this_page;\n\t\tfrom_this_cache_entry -= from_this_page;\n\t}\n\treturn 0;\n}\n\nstatic int add_system_iovec_to_sdma_packet(struct user_sdma_request *req,\n\t\t\t\t\t   struct user_sdma_txreq *tx,\n\t\t\t\t\t   struct user_sdma_iovec *iovec,\n\t\t\t\t\t   size_t from_this_iovec)\n{\n\twhile (from_this_iovec > 0) {\n\t\tstruct sdma_mmu_node *cache_entry;\n\t\tsize_t from_this_cache_entry;\n\t\tsize_t start;\n\t\tint ret;\n\n\t\tstart = (uintptr_t)iovec->iov.iov_base + iovec->offset;\n\t\tret = get_system_cache_entry(req, &cache_entry, start,\n\t\t\t\t\t     from_this_iovec);\n\t\tif (ret) {\n\t\t\tSDMA_DBG(req, \"pin system segment failed %d\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tfrom_this_cache_entry = cache_entry->rb.len - (start - cache_entry->rb.addr);\n\t\tif (from_this_cache_entry > from_this_iovec)\n\t\t\tfrom_this_cache_entry = from_this_iovec;\n\n\t\tret = add_mapping_to_sdma_packet(req, tx, cache_entry, start,\n\t\t\t\t\t\t from_this_cache_entry);\n\n\t\t \n\t\tkref_put(&cache_entry->rb.refcount, hfi1_mmu_rb_release);\n\n\t\tif (ret) {\n\t\t\tSDMA_DBG(req, \"add system segment failed %d\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tiovec->offset += from_this_cache_entry;\n\t\tfrom_this_iovec -= from_this_cache_entry;\n\t}\n\n\treturn 0;\n}\n\n \nint hfi1_add_pages_to_sdma_packet(struct user_sdma_request *req,\n\t\t\t\t  struct user_sdma_txreq *tx,\n\t\t\t\t  struct user_sdma_iovec *iovec,\n\t\t\t\t  u32 *pkt_data_remaining)\n{\n\tsize_t remaining_to_add = *pkt_data_remaining;\n\t \n\twhile (remaining_to_add > 0) {\n\t\tstruct user_sdma_iovec *cur_iovec;\n\t\tsize_t from_this_iovec;\n\t\tint ret;\n\n\t\tcur_iovec = iovec;\n\t\tfrom_this_iovec = iovec->iov.iov_len - iovec->offset;\n\n\t\tif (from_this_iovec > remaining_to_add) {\n\t\t\tfrom_this_iovec = remaining_to_add;\n\t\t} else {\n\t\t\t \n\t\t\treq->iov_idx++;\n\t\t\tiovec++;\n\t\t}\n\n\t\tret = add_system_iovec_to_sdma_packet(req, tx, cur_iovec,\n\t\t\t\t\t\t      from_this_iovec);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tremaining_to_add -= from_this_iovec;\n\t}\n\t*pkt_data_remaining = remaining_to_add;\n\n\treturn 0;\n}\n\nstatic bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,\n\t\t\t   unsigned long len)\n{\n\treturn (bool)(node->addr == addr);\n}\n\n \nstatic int sdma_rb_evict(void *arg, struct mmu_rb_node *mnode,\n\t\t\t void *evict_arg, bool *stop)\n{\n\tstruct sdma_mmu_node *node =\n\t\tcontainer_of(mnode, struct sdma_mmu_node, rb);\n\tstruct evict_data *evict_data = evict_arg;\n\n\t \n\tevict_data->cleared += node->npages;\n\n\t \n\tif (evict_data->cleared >= evict_data->target)\n\t\t*stop = true;\n\n\treturn 1;  \n}\n\nstatic void sdma_rb_remove(void *arg, struct mmu_rb_node *mnode)\n{\n\tstruct sdma_mmu_node *node =\n\t\tcontainer_of(mnode, struct sdma_mmu_node, rb);\n\n\tfree_system_node(node);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}