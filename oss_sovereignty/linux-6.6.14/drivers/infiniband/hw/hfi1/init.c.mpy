{
  "module_name": "init.c",
  "hash_id": "f8f2acdb63f1c4498321b53b9537e31b38db1facb78c4234ffcd19c24550d2f2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/init.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/delay.h>\n#include <linux/xarray.h>\n#include <linux/module.h>\n#include <linux/printk.h>\n#include <linux/hrtimer.h>\n#include <linux/bitmap.h>\n#include <linux/numa.h>\n#include <rdma/rdma_vt.h>\n\n#include \"hfi.h\"\n#include \"device.h\"\n#include \"common.h\"\n#include \"trace.h\"\n#include \"mad.h\"\n#include \"sdma.h\"\n#include \"debugfs.h\"\n#include \"verbs.h\"\n#include \"aspm.h\"\n#include \"affinity.h\"\n#include \"vnic.h\"\n#include \"exp_rcv.h\"\n#include \"netdev.h\"\n\n#undef pr_fmt\n#define pr_fmt(fmt) DRIVER_NAME \": \" fmt\n\n \n#define HFI1_MIN_USER_CTXT_BUFCNT 7\n\n#define HFI1_MIN_EAGER_BUFFER_SIZE (4 * 1024)  \n#define HFI1_MAX_EAGER_BUFFER_SIZE (256 * 1024)  \n\n#define NUM_IB_PORTS 1\n\n \nint num_user_contexts = -1;\nmodule_param_named(num_user_contexts, num_user_contexts, int, 0444);\nMODULE_PARM_DESC(\n\tnum_user_contexts, \"Set max number of user contexts to use (default: -1 will use the real (non-HT) CPU count)\");\n\nuint krcvqs[RXE_NUM_DATA_VL];\nint krcvqsset;\nmodule_param_array(krcvqs, uint, &krcvqsset, S_IRUGO);\nMODULE_PARM_DESC(krcvqs, \"Array of the number of non-control kernel receive queues by VL\");\n\n \nunsigned long n_krcvqs;\n\nstatic unsigned hfi1_rcvarr_split = 25;\nmodule_param_named(rcvarr_split, hfi1_rcvarr_split, uint, S_IRUGO);\nMODULE_PARM_DESC(rcvarr_split, \"Percent of context's RcvArray entries used for Eager buffers\");\n\nstatic uint eager_buffer_size = (8 << 20);  \nmodule_param(eager_buffer_size, uint, S_IRUGO);\nMODULE_PARM_DESC(eager_buffer_size, \"Size of the eager buffers, default: 8MB\");\n\nstatic uint rcvhdrcnt = 2048;  \nmodule_param_named(rcvhdrcnt, rcvhdrcnt, uint, S_IRUGO);\nMODULE_PARM_DESC(rcvhdrcnt, \"Receive header queue count (default 2048)\");\n\nstatic uint hfi1_hdrq_entsize = 32;\nmodule_param_named(hdrq_entsize, hfi1_hdrq_entsize, uint, 0444);\nMODULE_PARM_DESC(hdrq_entsize, \"Size of header queue entries: 2 - 8B, 16 - 64B, 32 - 128B (default)\");\n\nunsigned int user_credit_return_threshold = 33;\t \nmodule_param(user_credit_return_threshold, uint, S_IRUGO);\nMODULE_PARM_DESC(user_credit_return_threshold, \"Credit return threshold for user send contexts, return when unreturned credits passes this many blocks (in percent of allocated blocks, 0 is off)\");\n\nDEFINE_XARRAY_FLAGS(hfi1_dev_table, XA_FLAGS_ALLOC | XA_FLAGS_LOCK_IRQ);\n\nstatic int hfi1_create_kctxt(struct hfi1_devdata *dd,\n\t\t\t     struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_ctxtdata *rcd;\n\tint ret;\n\n\t \n\tBUILD_BUG_ON(HFI1_CTRL_CTXT != 0);\n\n\tret = hfi1_create_ctxtdata(ppd, dd->node, &rcd);\n\tif (ret < 0) {\n\t\tdd_dev_err(dd, \"Kernel receive context allocation failed\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\trcd->flags = HFI1_CAP_KGET(MULTI_PKT_EGR) |\n\t\tHFI1_CAP_KGET(NODROP_RHQ_FULL) |\n\t\tHFI1_CAP_KGET(NODROP_EGR_FULL) |\n\t\tHFI1_CAP_KGET(DMA_RTAIL);\n\n\t \n\tif (rcd->ctxt == HFI1_CTRL_CTXT)\n\t\trcd->flags |= HFI1_CAP_DMA_RTAIL;\n\trcd->fast_handler = get_dma_rtail_setting(rcd) ?\n\t\t\t\thandle_receive_interrupt_dma_rtail :\n\t\t\t\thandle_receive_interrupt_nodma_rtail;\n\n\thfi1_set_seq_cnt(rcd, 1);\n\n\trcd->sc = sc_alloc(dd, SC_ACK, rcd->rcvhdrqentsize, dd->node);\n\tif (!rcd->sc) {\n\t\tdd_dev_err(dd, \"Kernel send context allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\thfi1_init_ctxt(rcd->sc);\n\n\treturn 0;\n}\n\n \nint hfi1_create_kctxts(struct hfi1_devdata *dd)\n{\n\tu16 i;\n\tint ret;\n\n\tdd->rcd = kcalloc_node(dd->num_rcv_contexts, sizeof(*dd->rcd),\n\t\t\t       GFP_KERNEL, dd->node);\n\tif (!dd->rcd)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {\n\t\tret = hfi1_create_kctxt(dd, dd->pport);\n\t\tif (ret)\n\t\t\tgoto bail;\n\t}\n\n\treturn 0;\nbail:\n\tfor (i = 0; dd->rcd && i < dd->first_dyn_alloc_ctxt; ++i)\n\t\thfi1_free_ctxt(dd->rcd[i]);\n\n\t \n\tkfree(dd->rcd);\n\tdd->rcd = NULL;\n\treturn ret;\n}\n\n \nstatic void hfi1_rcd_init(struct hfi1_ctxtdata *rcd)\n{\n\tkref_init(&rcd->kref);\n}\n\n \nstatic void hfi1_rcd_free(struct kref *kref)\n{\n\tunsigned long flags;\n\tstruct hfi1_ctxtdata *rcd =\n\t\tcontainer_of(kref, struct hfi1_ctxtdata, kref);\n\n\tspin_lock_irqsave(&rcd->dd->uctxt_lock, flags);\n\trcd->dd->rcd[rcd->ctxt] = NULL;\n\tspin_unlock_irqrestore(&rcd->dd->uctxt_lock, flags);\n\n\thfi1_free_ctxtdata(rcd->dd, rcd);\n\n\tkfree(rcd);\n}\n\n \nint hfi1_rcd_put(struct hfi1_ctxtdata *rcd)\n{\n\tif (rcd)\n\t\treturn kref_put(&rcd->kref, hfi1_rcd_free);\n\n\treturn 0;\n}\n\n \nint hfi1_rcd_get(struct hfi1_ctxtdata *rcd)\n{\n\treturn kref_get_unless_zero(&rcd->kref);\n}\n\n \nstatic int allocate_rcd_index(struct hfi1_devdata *dd,\n\t\t\t      struct hfi1_ctxtdata *rcd, u16 *index)\n{\n\tunsigned long flags;\n\tu16 ctxt;\n\n\tspin_lock_irqsave(&dd->uctxt_lock, flags);\n\tfor (ctxt = 0; ctxt < dd->num_rcv_contexts; ctxt++)\n\t\tif (!dd->rcd[ctxt])\n\t\t\tbreak;\n\n\tif (ctxt < dd->num_rcv_contexts) {\n\t\trcd->ctxt = ctxt;\n\t\tdd->rcd[ctxt] = rcd;\n\t\thfi1_rcd_init(rcd);\n\t}\n\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\n\tif (ctxt >= dd->num_rcv_contexts)\n\t\treturn -EBUSY;\n\n\t*index = ctxt;\n\n\treturn 0;\n}\n\n \nstruct hfi1_ctxtdata *hfi1_rcd_get_by_index_safe(struct hfi1_devdata *dd,\n\t\t\t\t\t\t u16 ctxt)\n{\n\tif (ctxt < dd->num_rcv_contexts)\n\t\treturn hfi1_rcd_get_by_index(dd, ctxt);\n\n\treturn NULL;\n}\n\n \nstruct hfi1_ctxtdata *hfi1_rcd_get_by_index(struct hfi1_devdata *dd, u16 ctxt)\n{\n\tunsigned long flags;\n\tstruct hfi1_ctxtdata *rcd = NULL;\n\n\tspin_lock_irqsave(&dd->uctxt_lock, flags);\n\tif (dd->rcd[ctxt]) {\n\t\trcd = dd->rcd[ctxt];\n\t\tif (!hfi1_rcd_get(rcd))\n\t\t\trcd = NULL;\n\t}\n\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\n\treturn rcd;\n}\n\n \nint hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, int numa,\n\t\t\t struct hfi1_ctxtdata **context)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct hfi1_ctxtdata *rcd;\n\tunsigned kctxt_ngroups = 0;\n\tu32 base;\n\n\tif (dd->rcv_entries.nctxt_extra >\n\t    dd->num_rcv_contexts - dd->first_dyn_alloc_ctxt)\n\t\tkctxt_ngroups = (dd->rcv_entries.nctxt_extra -\n\t\t\t (dd->num_rcv_contexts - dd->first_dyn_alloc_ctxt));\n\trcd = kzalloc_node(sizeof(*rcd), GFP_KERNEL, numa);\n\tif (rcd) {\n\t\tu32 rcvtids, max_entries;\n\t\tu16 ctxt;\n\t\tint ret;\n\n\t\tret = allocate_rcd_index(dd, rcd, &ctxt);\n\t\tif (ret) {\n\t\t\t*context = NULL;\n\t\t\tkfree(rcd);\n\t\t\treturn ret;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&rcd->qp_wait_list);\n\t\thfi1_exp_tid_group_init(rcd);\n\t\trcd->ppd = ppd;\n\t\trcd->dd = dd;\n\t\trcd->numa_id = numa;\n\t\trcd->rcv_array_groups = dd->rcv_entries.ngroups;\n\t\trcd->rhf_rcv_function_map = normal_rhf_rcv_functions;\n\t\trcd->slow_handler = handle_receive_interrupt;\n\t\trcd->do_interrupt = rcd->slow_handler;\n\t\trcd->msix_intr = CCE_NUM_MSIX_VECTORS;\n\n\t\tmutex_init(&rcd->exp_mutex);\n\t\tspin_lock_init(&rcd->exp_lock);\n\t\tINIT_LIST_HEAD(&rcd->flow_queue.queue_head);\n\t\tINIT_LIST_HEAD(&rcd->rarr_queue.queue_head);\n\n\t\thfi1_cdbg(PROC, \"setting up context %u\", rcd->ctxt);\n\n\t\t \n\t\tif (ctxt < dd->first_dyn_alloc_ctxt) {\n\t\t\tif (ctxt < kctxt_ngroups) {\n\t\t\t\tbase = ctxt * (dd->rcv_entries.ngroups + 1);\n\t\t\t\trcd->rcv_array_groups++;\n\t\t\t} else {\n\t\t\t\tbase = kctxt_ngroups +\n\t\t\t\t\t(ctxt * dd->rcv_entries.ngroups);\n\t\t\t}\n\t\t} else {\n\t\t\tu16 ct = ctxt - dd->first_dyn_alloc_ctxt;\n\n\t\t\tbase = ((dd->n_krcv_queues * dd->rcv_entries.ngroups) +\n\t\t\t\tkctxt_ngroups);\n\t\t\tif (ct < dd->rcv_entries.nctxt_extra) {\n\t\t\t\tbase += ct * (dd->rcv_entries.ngroups + 1);\n\t\t\t\trcd->rcv_array_groups++;\n\t\t\t} else {\n\t\t\t\tbase += dd->rcv_entries.nctxt_extra +\n\t\t\t\t\t(ct * dd->rcv_entries.ngroups);\n\t\t\t}\n\t\t}\n\t\trcd->eager_base = base * dd->rcv_entries.group_size;\n\n\t\trcd->rcvhdrq_cnt = rcvhdrcnt;\n\t\trcd->rcvhdrqentsize = hfi1_hdrq_entsize;\n\t\trcd->rhf_offset =\n\t\t\trcd->rcvhdrqentsize - sizeof(u64) / sizeof(u32);\n\t\t \n\t\tmax_entries = rcd->rcv_array_groups *\n\t\t\tdd->rcv_entries.group_size;\n\t\trcvtids = ((max_entries * hfi1_rcvarr_split) / 100);\n\t\trcd->egrbufs.count = round_down(rcvtids,\n\t\t\t\t\t\tdd->rcv_entries.group_size);\n\t\tif (rcd->egrbufs.count > MAX_EAGER_ENTRIES) {\n\t\t\tdd_dev_err(dd, \"ctxt%u: requested too many RcvArray entries.\\n\",\n\t\t\t\t   rcd->ctxt);\n\t\t\trcd->egrbufs.count = MAX_EAGER_ENTRIES;\n\t\t}\n\t\thfi1_cdbg(PROC,\n\t\t\t  \"ctxt%u: max Eager buffer RcvArray entries: %u\",\n\t\t\t  rcd->ctxt, rcd->egrbufs.count);\n\n\t\t \n\t\trcd->egrbufs.buffers =\n\t\t\tkcalloc_node(rcd->egrbufs.count,\n\t\t\t\t     sizeof(*rcd->egrbufs.buffers),\n\t\t\t\t     GFP_KERNEL, numa);\n\t\tif (!rcd->egrbufs.buffers)\n\t\t\tgoto bail;\n\t\trcd->egrbufs.rcvtids =\n\t\t\tkcalloc_node(rcd->egrbufs.count,\n\t\t\t\t     sizeof(*rcd->egrbufs.rcvtids),\n\t\t\t\t     GFP_KERNEL, numa);\n\t\tif (!rcd->egrbufs.rcvtids)\n\t\t\tgoto bail;\n\t\trcd->egrbufs.size = eager_buffer_size;\n\t\t \n\t\tif (rcd->egrbufs.size < hfi1_max_mtu) {\n\t\t\trcd->egrbufs.size = __roundup_pow_of_two(hfi1_max_mtu);\n\t\t\thfi1_cdbg(PROC,\n\t\t\t\t  \"ctxt%u: eager bufs size too small. Adjusting to %u\",\n\t\t\t\t    rcd->ctxt, rcd->egrbufs.size);\n\t\t}\n\t\trcd->egrbufs.rcvtid_size = HFI1_MAX_EAGER_BUFFER_SIZE;\n\n\t\t \n\t\tif (ctxt < dd->first_dyn_alloc_ctxt) {\n\t\t\trcd->opstats = kzalloc_node(sizeof(*rcd->opstats),\n\t\t\t\t\t\t    GFP_KERNEL, numa);\n\t\t\tif (!rcd->opstats)\n\t\t\t\tgoto bail;\n\n\t\t\t \n\t\t\thfi1_kern_init_ctxt_generations(rcd);\n\t\t}\n\n\t\t*context = rcd;\n\t\treturn 0;\n\t}\n\nbail:\n\t*context = NULL;\n\thfi1_free_ctxt(rcd);\n\treturn -ENOMEM;\n}\n\n \nvoid hfi1_free_ctxt(struct hfi1_ctxtdata *rcd)\n{\n\thfi1_rcd_put(rcd);\n}\n\n \nvoid set_link_ipg(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct cc_state *cc_state;\n\tint i;\n\tu16 cce, ccti_limit, max_ccti = 0;\n\tu16 shift, mult;\n\tu64 src;\n\tu32 current_egress_rate;  \n\tu64 max_pkt_time;\n\t \n\n\tcc_state = get_cc_state(ppd);\n\n\tif (!cc_state)\n\t\t \n\t\treturn;\n\n\tfor (i = 0; i < OPA_MAX_SLS; i++) {\n\t\tu16 ccti = ppd->cca_timer[i].ccti;\n\n\t\tif (ccti > max_ccti)\n\t\t\tmax_ccti = ccti;\n\t}\n\n\tccti_limit = cc_state->cct.ccti_limit;\n\tif (max_ccti > ccti_limit)\n\t\tmax_ccti = ccti_limit;\n\n\tcce = cc_state->cct.entries[max_ccti].entry;\n\tshift = (cce & 0xc000) >> 14;\n\tmult = (cce & 0x3fff);\n\n\tcurrent_egress_rate = active_egress_rate(ppd);\n\n\tmax_pkt_time = egress_cycles(ppd->ibmaxlen, current_egress_rate);\n\n\tsrc = (max_pkt_time >> shift) * mult;\n\n\tsrc &= SEND_STATIC_RATE_CONTROL_CSR_SRC_RELOAD_SMASK;\n\tsrc <<= SEND_STATIC_RATE_CONTROL_CSR_SRC_RELOAD_SHIFT;\n\n\twrite_csr(dd, SEND_STATIC_RATE_CONTROL, src);\n}\n\nstatic enum hrtimer_restart cca_timer_fn(struct hrtimer *t)\n{\n\tstruct cca_timer *cca_timer;\n\tstruct hfi1_pportdata *ppd;\n\tint sl;\n\tu16 ccti_timer, ccti_min;\n\tstruct cc_state *cc_state;\n\tunsigned long flags;\n\tenum hrtimer_restart ret = HRTIMER_NORESTART;\n\n\tcca_timer = container_of(t, struct cca_timer, hrtimer);\n\tppd = cca_timer->ppd;\n\tsl = cca_timer->sl;\n\n\trcu_read_lock();\n\n\tcc_state = get_cc_state(ppd);\n\n\tif (!cc_state) {\n\t\trcu_read_unlock();\n\t\treturn HRTIMER_NORESTART;\n\t}\n\n\t \n\n\tccti_min = cc_state->cong_setting.entries[sl].ccti_min;\n\tccti_timer = cc_state->cong_setting.entries[sl].ccti_timer;\n\n\tspin_lock_irqsave(&ppd->cca_timer_lock, flags);\n\n\tif (cca_timer->ccti > ccti_min) {\n\t\tcca_timer->ccti--;\n\t\tset_link_ipg(ppd);\n\t}\n\n\tif (cca_timer->ccti > ccti_min) {\n\t\tunsigned long nsec = 1024 * ccti_timer;\n\t\t \n\t\thrtimer_forward_now(t, ns_to_ktime(nsec));\n\t\tret = HRTIMER_RESTART;\n\t}\n\n\tspin_unlock_irqrestore(&ppd->cca_timer_lock, flags);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nvoid hfi1_init_pportdata(struct pci_dev *pdev, struct hfi1_pportdata *ppd,\n\t\t\t struct hfi1_devdata *dd, u8 hw_pidx, u32 port)\n{\n\tint i;\n\tuint default_pkey_idx;\n\tstruct cc_state *cc_state;\n\n\tppd->dd = dd;\n\tppd->hw_pidx = hw_pidx;\n\tppd->port = port;  \n\tppd->prev_link_width = LINK_WIDTH_DEFAULT;\n\t \n\tfor (i = 0; i < C_VL_COUNT + 1; i++) {\n\t\tppd->port_vl_xmit_wait_last[i] = 0;\n\t\tppd->vl_xmit_flit_cnt[i] = 0;\n\t}\n\n\tdefault_pkey_idx = 1;\n\n\tppd->pkeys[default_pkey_idx] = DEFAULT_P_KEY;\n\tppd->part_enforce |= HFI1_PART_ENFORCE_IN;\n\tppd->pkeys[0] = 0x8001;\n\n\tINIT_WORK(&ppd->link_vc_work, handle_verify_cap);\n\tINIT_WORK(&ppd->link_up_work, handle_link_up);\n\tINIT_WORK(&ppd->link_down_work, handle_link_down);\n\tINIT_WORK(&ppd->freeze_work, handle_freeze);\n\tINIT_WORK(&ppd->link_downgrade_work, handle_link_downgrade);\n\tINIT_WORK(&ppd->sma_message_work, handle_sma_message);\n\tINIT_WORK(&ppd->link_bounce_work, handle_link_bounce);\n\tINIT_DELAYED_WORK(&ppd->start_link_work, handle_start_link);\n\tINIT_WORK(&ppd->linkstate_active_work, receive_interrupt_work);\n\tINIT_WORK(&ppd->qsfp_info.qsfp_work, qsfp_event);\n\n\tmutex_init(&ppd->hls_lock);\n\tspin_lock_init(&ppd->qsfp_info.qsfp_lock);\n\n\tppd->qsfp_info.ppd = ppd;\n\tppd->sm_trap_qp = 0x0;\n\tppd->sa_qp = 0x1;\n\n\tppd->hfi1_wq = NULL;\n\n\tspin_lock_init(&ppd->cca_timer_lock);\n\n\tfor (i = 0; i < OPA_MAX_SLS; i++) {\n\t\thrtimer_init(&ppd->cca_timer[i].hrtimer, CLOCK_MONOTONIC,\n\t\t\t     HRTIMER_MODE_REL);\n\t\tppd->cca_timer[i].ppd = ppd;\n\t\tppd->cca_timer[i].sl = i;\n\t\tppd->cca_timer[i].ccti = 0;\n\t\tppd->cca_timer[i].hrtimer.function = cca_timer_fn;\n\t}\n\n\tppd->cc_max_table_entries = IB_CC_TABLE_CAP_DEFAULT;\n\n\tspin_lock_init(&ppd->cc_state_lock);\n\tspin_lock_init(&ppd->cc_log_lock);\n\tcc_state = kzalloc(sizeof(*cc_state), GFP_KERNEL);\n\tRCU_INIT_POINTER(ppd->cc_state, cc_state);\n\tif (!cc_state)\n\t\tgoto bail;\n\treturn;\n\nbail:\n\tdd_dev_err(dd, \"Congestion Control Agent disabled for port %d\\n\", port);\n}\n\n \nstatic int loadtime_init(struct hfi1_devdata *dd)\n{\n\treturn 0;\n}\n\n \nstatic int init_after_reset(struct hfi1_devdata *dd)\n{\n\tint i;\n\tstruct hfi1_ctxtdata *rcd;\n\t \n\tfor (i = 0; i < dd->num_rcv_contexts; i++) {\n\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\t\thfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS |\n\t\t\t     HFI1_RCVCTRL_INTRAVAIL_DIS |\n\t\t\t     HFI1_RCVCTRL_TAILUPD_DIS, rcd);\n\t\thfi1_rcd_put(rcd);\n\t}\n\tpio_send_control(dd, PSC_GLOBAL_DISABLE);\n\tfor (i = 0; i < dd->num_send_contexts; i++)\n\t\tsc_disable(dd->send_contexts[i].sc);\n\n\treturn 0;\n}\n\nstatic void enable_chip(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_ctxtdata *rcd;\n\tu32 rcvmask;\n\tu16 i;\n\n\t \n\tpio_send_control(dd, PSC_GLOBAL_ENABLE);\n\n\t \n\tfor (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {\n\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\t\tif (!rcd)\n\t\t\tcontinue;\n\t\trcvmask = HFI1_RCVCTRL_CTXT_ENB | HFI1_RCVCTRL_INTRAVAIL_ENB;\n\t\trcvmask |= HFI1_CAP_KGET_MASK(rcd->flags, DMA_RTAIL) ?\n\t\t\tHFI1_RCVCTRL_TAILUPD_ENB : HFI1_RCVCTRL_TAILUPD_DIS;\n\t\tif (!HFI1_CAP_KGET_MASK(rcd->flags, MULTI_PKT_EGR))\n\t\t\trcvmask |= HFI1_RCVCTRL_ONE_PKT_EGR_ENB;\n\t\tif (HFI1_CAP_KGET_MASK(rcd->flags, NODROP_RHQ_FULL))\n\t\t\trcvmask |= HFI1_RCVCTRL_NO_RHQ_DROP_ENB;\n\t\tif (HFI1_CAP_KGET_MASK(rcd->flags, NODROP_EGR_FULL))\n\t\t\trcvmask |= HFI1_RCVCTRL_NO_EGR_DROP_ENB;\n\t\tif (HFI1_CAP_IS_KSET(TID_RDMA))\n\t\t\trcvmask |= HFI1_RCVCTRL_TIDFLOW_ENB;\n\t\thfi1_rcvctrl(dd, rcvmask, rcd);\n\t\tsc_enable(rcd->sc);\n\t\thfi1_rcd_put(rcd);\n\t}\n}\n\n \nstatic int create_workqueues(struct hfi1_devdata *dd)\n{\n\tint pidx;\n\tstruct hfi1_pportdata *ppd;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (!ppd->hfi1_wq) {\n\t\t\tppd->hfi1_wq =\n\t\t\t\talloc_workqueue(\n\t\t\t\t    \"hfi%d_%d\",\n\t\t\t\t    WQ_SYSFS | WQ_HIGHPRI | WQ_CPU_INTENSIVE |\n\t\t\t\t    WQ_MEM_RECLAIM,\n\t\t\t\t    HFI1_MAX_ACTIVE_WORKQUEUE_ENTRIES,\n\t\t\t\t    dd->unit, pidx);\n\t\t\tif (!ppd->hfi1_wq)\n\t\t\t\tgoto wq_error;\n\t\t}\n\t\tif (!ppd->link_wq) {\n\t\t\t \n\t\t\tppd->link_wq =\n\t\t\t\talloc_workqueue(\n\t\t\t\t    \"hfi_link_%d_%d\",\n\t\t\t\t    WQ_SYSFS | WQ_MEM_RECLAIM | WQ_UNBOUND,\n\t\t\t\t    1,  \n\t\t\t\t    dd->unit, pidx);\n\t\t\tif (!ppd->link_wq)\n\t\t\t\tgoto wq_error;\n\t\t}\n\t}\n\treturn 0;\nwq_error:\n\tpr_err(\"alloc_workqueue failed for port %d\\n\", pidx + 1);\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (ppd->hfi1_wq) {\n\t\t\tdestroy_workqueue(ppd->hfi1_wq);\n\t\t\tppd->hfi1_wq = NULL;\n\t\t}\n\t\tif (ppd->link_wq) {\n\t\t\tdestroy_workqueue(ppd->link_wq);\n\t\t\tppd->link_wq = NULL;\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic void destroy_workqueues(struct hfi1_devdata *dd)\n{\n\tint pidx;\n\tstruct hfi1_pportdata *ppd;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\n\t\tif (ppd->hfi1_wq) {\n\t\t\tdestroy_workqueue(ppd->hfi1_wq);\n\t\t\tppd->hfi1_wq = NULL;\n\t\t}\n\t\tif (ppd->link_wq) {\n\t\t\tdestroy_workqueue(ppd->link_wq);\n\t\t\tppd->link_wq = NULL;\n\t\t}\n\t}\n}\n\n \nstatic void enable_general_intr(struct hfi1_devdata *dd)\n{\n\tset_intr_bits(dd, CCE_ERR_INT, MISC_ERR_INT, true);\n\tset_intr_bits(dd, PIO_ERR_INT, TXE_ERR_INT, true);\n\tset_intr_bits(dd, IS_SENDCTXT_ERR_START, IS_SENDCTXT_ERR_END, true);\n\tset_intr_bits(dd, PBC_INT, GPIO_ASSERT_INT, true);\n\tset_intr_bits(dd, TCRIT_INT, TCRIT_INT, true);\n\tset_intr_bits(dd, IS_DC_START, IS_DC_END, true);\n\tset_intr_bits(dd, IS_SENDCREDIT_START, IS_SENDCREDIT_END, true);\n}\n\n \nint hfi1_init(struct hfi1_devdata *dd, int reinit)\n{\n\tint ret = 0, pidx, lastfail = 0;\n\tunsigned long len;\n\tu16 i;\n\tstruct hfi1_ctxtdata *rcd;\n\tstruct hfi1_pportdata *ppd;\n\n\t \n\tdd->process_pio_send = hfi1_verbs_send_pio;\n\tdd->process_dma_send = hfi1_verbs_send_dma;\n\tdd->pio_inline_send = pio_copy;\n\tdd->process_vnic_dma_send = hfi1_vnic_send_dma;\n\n\tif (is_ax(dd)) {\n\t\tatomic_set(&dd->drop_packet, DROP_PACKET_ON);\n\t\tdd->do_drop = true;\n\t} else {\n\t\tatomic_set(&dd->drop_packet, DROP_PACKET_OFF);\n\t\tdd->do_drop = false;\n\t}\n\n\t \n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tppd->linkup = 0;\n\t}\n\n\tif (reinit)\n\t\tret = init_after_reset(dd);\n\telse\n\t\tret = loadtime_init(dd);\n\tif (ret)\n\t\tgoto done;\n\n\t \n\tfor (i = 0; dd->rcd && i < dd->first_dyn_alloc_ctxt; ++i) {\n\t\t \n\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\t\tif (!rcd)\n\t\t\tcontinue;\n\n\t\tlastfail = hfi1_create_rcvhdrq(dd, rcd);\n\t\tif (!lastfail)\n\t\t\tlastfail = hfi1_setup_eagerbufs(rcd);\n\t\tif (!lastfail)\n\t\t\tlastfail = hfi1_kern_exp_rcv_init(rcd, reinit);\n\t\tif (lastfail) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"failed to allocate kernel ctxt's rcvhdrq and/or egr bufs\\n\");\n\t\t\tret = lastfail;\n\t\t}\n\t\t \n\t\thfi1_rcd_put(rcd);\n\t}\n\n\t \n\tlen = PAGE_ALIGN(chip_rcv_contexts(dd) * HFI1_MAX_SHARED_CTXTS *\n\t\t\t sizeof(*dd->events));\n\tdd->events = vmalloc_user(len);\n\tif (!dd->events)\n\t\tdd_dev_err(dd, \"Failed to allocate user events page\\n\");\n\t \n\tdd->status = vmalloc_user(PAGE_SIZE);\n\tif (!dd->status)\n\t\tdd_dev_err(dd, \"Failed to allocate dev status page\\n\");\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (dd->status)\n\t\t\t \n\t\t\tppd->statusp = &dd->status->port;\n\n\t\tset_mtu(ppd);\n\t}\n\n\t \n\tenable_chip(dd);\n\ndone:\n\t \n\tif (dd->status)\n\t\tdd->status->dev |= HFI1_STATUS_CHIP_PRESENT |\n\t\t\tHFI1_STATUS_INITTED;\n\tif (!ret) {\n\t\t \n\t\tenable_general_intr(dd);\n\t\tinit_qsfp_int(dd);\n\n\t\t \n\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\t\tppd = dd->pport + pidx;\n\n\t\t\t \n\t\t\tlastfail = bringup_serdes(ppd);\n\t\t\tif (lastfail)\n\t\t\t\tdd_dev_info(dd,\n\t\t\t\t\t    \"Failed to bring up port %u\\n\",\n\t\t\t\t\t    ppd->port);\n\n\t\t\t \n\t\t\tif (ppd->statusp)\n\t\t\t\t*ppd->statusp |= HFI1_STATUS_CHIP_PRESENT |\n\t\t\t\t\t\t\tHFI1_STATUS_INITTED;\n\t\t\tif (!ppd->link_speed_enabled)\n\t\t\t\tcontinue;\n\t\t}\n\t}\n\n\t \n\treturn ret;\n}\n\nstruct hfi1_devdata *hfi1_lookup(int unit)\n{\n\treturn xa_load(&hfi1_dev_table, unit);\n}\n\n \nstatic void stop_timers(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd;\n\tint pidx;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tif (ppd->led_override_timer.function) {\n\t\t\tdel_timer_sync(&ppd->led_override_timer);\n\t\t\tatomic_set(&ppd->led_override_timer_active, 0);\n\t\t}\n\t}\n}\n\n \nstatic void shutdown_device(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd;\n\tstruct hfi1_ctxtdata *rcd;\n\tunsigned pidx;\n\tint i;\n\n\tif (dd->flags & HFI1_SHUTDOWN)\n\t\treturn;\n\tdd->flags |= HFI1_SHUTDOWN;\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\n\t\tppd->linkup = 0;\n\t\tif (ppd->statusp)\n\t\t\t*ppd->statusp &= ~(HFI1_STATUS_IB_CONF |\n\t\t\t\t\t   HFI1_STATUS_IB_READY);\n\t}\n\tdd->flags &= ~HFI1_INITTED;\n\n\t \n\tset_intr_bits(dd, IS_FIRST_SOURCE, IS_LAST_SOURCE, false);\n\tmsix_clean_up_interrupts(dd);\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\t\tfor (i = 0; i < dd->num_rcv_contexts; i++) {\n\t\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\t\t\thfi1_rcvctrl(dd, HFI1_RCVCTRL_TAILUPD_DIS |\n\t\t\t\t     HFI1_RCVCTRL_CTXT_DIS |\n\t\t\t\t     HFI1_RCVCTRL_INTRAVAIL_DIS |\n\t\t\t\t     HFI1_RCVCTRL_PKEY_DIS |\n\t\t\t\t     HFI1_RCVCTRL_ONE_PKT_EGR_DIS, rcd);\n\t\t\thfi1_rcd_put(rcd);\n\t\t}\n\t\t \n\t\tfor (i = 0; i < dd->num_send_contexts; i++)\n\t\t\tsc_flush(dd->send_contexts[i].sc);\n\t}\n\n\t \n\tudelay(20);\n\n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tppd = dd->pport + pidx;\n\n\t\t \n\t\tfor (i = 0; i < dd->num_send_contexts; i++)\n\t\t\tsc_disable(dd->send_contexts[i].sc);\n\t\t \n\t\tpio_send_control(dd, PSC_GLOBAL_DISABLE);\n\n\t\tshutdown_led_override(ppd);\n\n\t\t \n\t\thfi1_quiet_serdes(ppd);\n\t\tif (ppd->hfi1_wq)\n\t\t\tflush_workqueue(ppd->hfi1_wq);\n\t\tif (ppd->link_wq)\n\t\t\tflush_workqueue(ppd->link_wq);\n\t}\n\tsdma_exit(dd);\n}\n\n \nvoid hfi1_free_ctxtdata(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)\n{\n\tu32 e;\n\n\tif (!rcd)\n\t\treturn;\n\n\tif (rcd->rcvhdrq) {\n\t\tdma_free_coherent(&dd->pcidev->dev, rcvhdrq_size(rcd),\n\t\t\t\t  rcd->rcvhdrq, rcd->rcvhdrq_dma);\n\t\trcd->rcvhdrq = NULL;\n\t\tif (hfi1_rcvhdrtail_kvaddr(rcd)) {\n\t\t\tdma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\n\t\t\t\t\t  (void *)hfi1_rcvhdrtail_kvaddr(rcd),\n\t\t\t\t\t  rcd->rcvhdrqtailaddr_dma);\n\t\t\trcd->rcvhdrtail_kvaddr = NULL;\n\t\t}\n\t}\n\n\t \n\tkfree(rcd->egrbufs.rcvtids);\n\trcd->egrbufs.rcvtids = NULL;\n\n\tfor (e = 0; e < rcd->egrbufs.alloced; e++) {\n\t\tif (rcd->egrbufs.buffers[e].addr)\n\t\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t\t  rcd->egrbufs.buffers[e].len,\n\t\t\t\t\t  rcd->egrbufs.buffers[e].addr,\n\t\t\t\t\t  rcd->egrbufs.buffers[e].dma);\n\t}\n\tkfree(rcd->egrbufs.buffers);\n\trcd->egrbufs.alloced = 0;\n\trcd->egrbufs.buffers = NULL;\n\n\tsc_free(rcd->sc);\n\trcd->sc = NULL;\n\n\tvfree(rcd->subctxt_uregbase);\n\tvfree(rcd->subctxt_rcvegrbuf);\n\tvfree(rcd->subctxt_rcvhdr_base);\n\tkfree(rcd->opstats);\n\n\trcd->subctxt_uregbase = NULL;\n\trcd->subctxt_rcvegrbuf = NULL;\n\trcd->subctxt_rcvhdr_base = NULL;\n\trcd->opstats = NULL;\n}\n\n \nstatic struct hfi1_asic_data *release_asic_data(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_asic_data *ad;\n\tint other;\n\n\tif (!dd->asic_data)\n\t\treturn NULL;\n\tdd->asic_data->dds[dd->hfi1_id] = NULL;\n\tother = dd->hfi1_id ? 0 : 1;\n\tad = dd->asic_data;\n\tdd->asic_data = NULL;\n\t \n\treturn ad->dds[other] ? NULL : ad;\n}\n\nstatic void finalize_asic_data(struct hfi1_devdata *dd,\n\t\t\t       struct hfi1_asic_data *ad)\n{\n\tclean_up_i2c(dd, ad);\n\tkfree(ad);\n}\n\n \nvoid hfi1_free_devdata(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_asic_data *ad;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&hfi1_dev_table, flags);\n\t__xa_erase(&hfi1_dev_table, dd->unit);\n\tad = release_asic_data(dd);\n\txa_unlock_irqrestore(&hfi1_dev_table, flags);\n\n\tfinalize_asic_data(dd, ad);\n\tfree_platform_config(dd);\n\trcu_barrier();  \n\tfree_percpu(dd->int_counter);\n\tfree_percpu(dd->rcv_limit);\n\tfree_percpu(dd->send_schedule);\n\tfree_percpu(dd->tx_opstats);\n\tdd->int_counter   = NULL;\n\tdd->rcv_limit     = NULL;\n\tdd->send_schedule = NULL;\n\tdd->tx_opstats    = NULL;\n\tkfree(dd->comp_vect);\n\tdd->comp_vect = NULL;\n\tif (dd->rcvhdrtail_dummy_kvaddr)\n\t\tdma_free_coherent(&dd->pcidev->dev, sizeof(u64),\n\t\t\t\t  (void *)dd->rcvhdrtail_dummy_kvaddr,\n\t\t\t\t  dd->rcvhdrtail_dummy_dma);\n\tdd->rcvhdrtail_dummy_kvaddr = NULL;\n\tsdma_clean(dd, dd->num_sdma);\n\trvt_dealloc_device(&dd->verbs_dev.rdi);\n}\n\n \nstatic struct hfi1_devdata *hfi1_alloc_devdata(struct pci_dev *pdev,\n\t\t\t\t\t       size_t extra)\n{\n\tstruct hfi1_devdata *dd;\n\tint ret, nports;\n\n\t \n\tnports = extra / sizeof(struct hfi1_pportdata);\n\n\tdd = (struct hfi1_devdata *)rvt_alloc_device(sizeof(*dd) + extra,\n\t\t\t\t\t\t     nports);\n\tif (!dd)\n\t\treturn ERR_PTR(-ENOMEM);\n\tdd->num_pports = nports;\n\tdd->pport = (struct hfi1_pportdata *)(dd + 1);\n\tdd->pcidev = pdev;\n\tpci_set_drvdata(pdev, dd);\n\n\tret = xa_alloc_irq(&hfi1_dev_table, &dd->unit, dd, xa_limit_32b,\n\t\t\tGFP_KERNEL);\n\tif (ret < 0) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Could not allocate unit ID: error %d\\n\", -ret);\n\t\tgoto bail;\n\t}\n\trvt_set_ibdev_name(&dd->verbs_dev.rdi, \"%s_%d\", class_name(), dd->unit);\n\t \n\tdd->node = pcibus_to_node(pdev->bus);\n\tif (dd->node == NUMA_NO_NODE) {\n\t\tdd_dev_err(dd, \"Invalid PCI NUMA node. Performance may be affected\\n\");\n\t\tdd->node = 0;\n\t}\n\n\t \n\tspin_lock_init(&dd->sc_lock);\n\tspin_lock_init(&dd->sendctrl_lock);\n\tspin_lock_init(&dd->rcvctrl_lock);\n\tspin_lock_init(&dd->uctxt_lock);\n\tspin_lock_init(&dd->hfi1_diag_trans_lock);\n\tspin_lock_init(&dd->sc_init_lock);\n\tspin_lock_init(&dd->dc8051_memlock);\n\tseqlock_init(&dd->sc2vl_lock);\n\tspin_lock_init(&dd->sde_map_lock);\n\tspin_lock_init(&dd->pio_map_lock);\n\tmutex_init(&dd->dc8051_lock);\n\tinit_waitqueue_head(&dd->event_queue);\n\tspin_lock_init(&dd->irq_src_lock);\n\n\tdd->int_counter = alloc_percpu(u64);\n\tif (!dd->int_counter) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tdd->rcv_limit = alloc_percpu(u64);\n\tif (!dd->rcv_limit) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tdd->send_schedule = alloc_percpu(u64);\n\tif (!dd->send_schedule) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tdd->tx_opstats = alloc_percpu(struct hfi1_opcode_stats_perctx);\n\tif (!dd->tx_opstats) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tdd->comp_vect = kzalloc(sizeof(*dd->comp_vect), GFP_KERNEL);\n\tif (!dd->comp_vect) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\t \n\tdd->rcvhdrtail_dummy_kvaddr =\n\t\tdma_alloc_coherent(&dd->pcidev->dev, sizeof(u64),\n\t\t\t\t   &dd->rcvhdrtail_dummy_dma, GFP_KERNEL);\n\tif (!dd->rcvhdrtail_dummy_kvaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tatomic_set(&dd->ipoib_rsm_usr_num, 0);\n\treturn dd;\n\nbail:\n\thfi1_free_devdata(dd);\n\treturn ERR_PTR(ret);\n}\n\n \nvoid hfi1_disable_after_error(struct hfi1_devdata *dd)\n{\n\tif (dd->flags & HFI1_INITTED) {\n\t\tu32 pidx;\n\n\t\tdd->flags &= ~HFI1_INITTED;\n\t\tif (dd->pport)\n\t\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\t\t\tstruct hfi1_pportdata *ppd;\n\n\t\t\t\tppd = dd->pport + pidx;\n\t\t\t\tif (dd->flags & HFI1_PRESENT)\n\t\t\t\t\tset_link_state(ppd, HLS_DN_DISABLE);\n\n\t\t\t\tif (ppd->statusp)\n\t\t\t\t\t*ppd->statusp &= ~HFI1_STATUS_IB_READY;\n\t\t\t}\n\t}\n\n\t \n\tif (dd->status)\n\t\tdd->status->dev |= HFI1_STATUS_HWERROR;\n}\n\nstatic void remove_one(struct pci_dev *);\nstatic int init_one(struct pci_dev *, const struct pci_device_id *);\nstatic void shutdown_one(struct pci_dev *);\n\n#define DRIVER_LOAD_MSG \"Cornelis \" DRIVER_NAME \" loaded: \"\n#define PFX DRIVER_NAME \": \"\n\nconst struct pci_device_id hfi1_pci_tbl[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL0) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL1) },\n\t{ 0, }\n};\n\nMODULE_DEVICE_TABLE(pci, hfi1_pci_tbl);\n\nstatic struct pci_driver hfi1_pci_driver = {\n\t.name = DRIVER_NAME,\n\t.probe = init_one,\n\t.remove = remove_one,\n\t.shutdown = shutdown_one,\n\t.id_table = hfi1_pci_tbl,\n\t.err_handler = &hfi1_pci_err_handler,\n};\n\nstatic void __init compute_krcvqs(void)\n{\n\tint i;\n\n\tfor (i = 0; i < krcvqsset; i++)\n\t\tn_krcvqs += krcvqs[i];\n}\n\n \nstatic int __init hfi1_mod_init(void)\n{\n\tint ret;\n\n\tret = dev_init();\n\tif (ret)\n\t\tgoto bail;\n\n\tret = node_affinity_init();\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tif (!valid_opa_max_mtu(hfi1_max_mtu)) {\n\t\tpr_err(\"Invalid max_mtu 0x%x, using 0x%x instead\\n\",\n\t\t       hfi1_max_mtu, HFI1_DEFAULT_MAX_MTU);\n\t\thfi1_max_mtu = HFI1_DEFAULT_MAX_MTU;\n\t}\n\t \n\tif (hfi1_cu > 128 || !is_power_of_2(hfi1_cu))\n\t\thfi1_cu = 1;\n\t \n\tif (user_credit_return_threshold > 100)\n\t\tuser_credit_return_threshold = 100;\n\n\tcompute_krcvqs();\n\t \n\tif (rcv_intr_count > RCV_HDR_HEAD_COUNTER_MASK)\n\t\trcv_intr_count = RCV_HDR_HEAD_COUNTER_MASK;\n\t \n\tif (rcv_intr_count == 0 && rcv_intr_timeout == 0) {\n\t\tpr_err(\"Invalid mode: both receive interrupt count and available timeout are zero - setting interrupt count to 1\\n\");\n\t\trcv_intr_count = 1;\n\t}\n\tif (rcv_intr_count > 1 && rcv_intr_timeout == 0) {\n\t\t \n\t\tpr_err(\"Invalid mode: receive interrupt count greater than 1 and available timeout is zero - setting available timeout to 1\\n\");\n\t\trcv_intr_timeout = 1;\n\t}\n\tif (rcv_intr_dynamic && !(rcv_intr_count > 1 && rcv_intr_timeout > 0)) {\n\t\t \n\t\tpr_err(\"Invalid mode: dynamic receive interrupt mitigation with invalid count and timeout - turning dynamic off\\n\");\n\t\trcv_intr_dynamic = 0;\n\t}\n\n\t \n\tlink_crc_mask &= SUPPORTED_CRCS;\n\n\tret = opfn_init();\n\tif (ret < 0) {\n\t\tpr_err(\"Failed to allocate opfn_wq\");\n\t\tgoto bail_dev;\n\t}\n\n\t \n\thfi1_dbg_init();\n\tret = pci_register_driver(&hfi1_pci_driver);\n\tif (ret < 0) {\n\t\tpr_err(\"Unable to register driver: error %d\\n\", -ret);\n\t\tgoto bail_dev;\n\t}\n\tgoto bail;  \n\nbail_dev:\n\thfi1_dbg_exit();\n\tdev_cleanup();\nbail:\n\treturn ret;\n}\n\nmodule_init(hfi1_mod_init);\n\n \nstatic void __exit hfi1_mod_cleanup(void)\n{\n\tpci_unregister_driver(&hfi1_pci_driver);\n\topfn_exit();\n\tnode_affinity_destroy_all();\n\thfi1_dbg_exit();\n\n\tWARN_ON(!xa_empty(&hfi1_dev_table));\n\tdispose_firmware();\t \n\tdev_cleanup();\n}\n\nmodule_exit(hfi1_mod_cleanup);\n\n \nstatic void cleanup_device_data(struct hfi1_devdata *dd)\n{\n\tint ctxt;\n\tint pidx;\n\n\t \n\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\tstruct hfi1_pportdata *ppd = &dd->pport[pidx];\n\t\tstruct cc_state *cc_state;\n\t\tint i;\n\n\t\tif (ppd->statusp)\n\t\t\t*ppd->statusp &= ~HFI1_STATUS_CHIP_PRESENT;\n\n\t\tfor (i = 0; i < OPA_MAX_SLS; i++)\n\t\t\thrtimer_cancel(&ppd->cca_timer[i].hrtimer);\n\n\t\tspin_lock(&ppd->cc_state_lock);\n\t\tcc_state = get_cc_state_protected(ppd);\n\t\tRCU_INIT_POINTER(ppd->cc_state, NULL);\n\t\tspin_unlock(&ppd->cc_state_lock);\n\n\t\tif (cc_state)\n\t\t\tkfree_rcu(cc_state, rcu);\n\t}\n\n\tfree_credit_return(dd);\n\n\t \n\tfor (ctxt = 0; dd->rcd && ctxt < dd->num_rcv_contexts; ctxt++) {\n\t\tstruct hfi1_ctxtdata *rcd = dd->rcd[ctxt];\n\n\t\tif (rcd) {\n\t\t\thfi1_free_ctxt_rcv_groups(rcd);\n\t\t\thfi1_free_ctxt(rcd);\n\t\t}\n\t}\n\n\tkfree(dd->rcd);\n\tdd->rcd = NULL;\n\n\tfree_pio_map(dd);\n\t \n\tfor (ctxt = 0; ctxt < dd->num_send_contexts; ctxt++)\n\t\tsc_free(dd->send_contexts[ctxt].sc);\n\tdd->num_send_contexts = 0;\n\tkfree(dd->send_contexts);\n\tdd->send_contexts = NULL;\n\tkfree(dd->hw_to_sw);\n\tdd->hw_to_sw = NULL;\n\tkfree(dd->boardname);\n\tvfree(dd->events);\n\tvfree(dd->status);\n}\n\n \nstatic void postinit_cleanup(struct hfi1_devdata *dd)\n{\n\thfi1_start_cleanup(dd);\n\thfi1_comp_vectors_clean_up(dd);\n\thfi1_dev_affinity_clean_up(dd);\n\n\thfi1_pcie_ddcleanup(dd);\n\thfi1_pcie_cleanup(dd->pcidev);\n\n\tcleanup_device_data(dd);\n\n\thfi1_free_devdata(dd);\n}\n\nstatic int init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tint ret = 0, j, pidx, initfail;\n\tstruct hfi1_devdata *dd;\n\tstruct hfi1_pportdata *ppd;\n\n\t \n\tHFI1_CAP_LOCK();\n\n\t \n\tif (!(ent->device == PCI_DEVICE_ID_INTEL0 ||\n\t      ent->device == PCI_DEVICE_ID_INTEL1)) {\n\t\tdev_err(&pdev->dev, \"Failing on unknown Intel deviceid 0x%x\\n\",\n\t\t\tent->device);\n\t\tret = -ENODEV;\n\t\tgoto bail;\n\t}\n\n\t \n\tdd = hfi1_alloc_devdata(pdev, NUM_IB_PORTS *\n\t\t\t\tsizeof(struct hfi1_pportdata));\n\tif (IS_ERR(dd)) {\n\t\tret = PTR_ERR(dd);\n\t\tgoto bail;\n\t}\n\n\t \n\tret = hfi1_validate_rcvhdrcnt(dd, rcvhdrcnt);\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tif (!encode_rcv_header_entry_size(hfi1_hdrq_entsize)) {\n\t\tdd_dev_err(dd, \"Invalid HdrQ Entry size %u\\n\",\n\t\t\t   hfi1_hdrq_entsize);\n\t\tret = -EINVAL;\n\t\tgoto bail;\n\t}\n\n\t \n\tif (eager_buffer_size) {\n\t\tif (!is_power_of_2(eager_buffer_size))\n\t\t\teager_buffer_size =\n\t\t\t\troundup_pow_of_two(eager_buffer_size);\n\t\teager_buffer_size =\n\t\t\tclamp_val(eager_buffer_size,\n\t\t\t\t  MIN_EAGER_BUFFER * 8,\n\t\t\t\t  MAX_EAGER_BUFFER_TOTAL);\n\t\tdd_dev_info(dd, \"Eager buffer size %u\\n\",\n\t\t\t    eager_buffer_size);\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid Eager buffer size of 0\\n\");\n\t\tret = -EINVAL;\n\t\tgoto bail;\n\t}\n\n\t \n\thfi1_rcvarr_split = clamp_val(hfi1_rcvarr_split, 0, 100);\n\n\tret = hfi1_pcie_init(dd);\n\tif (ret)\n\t\tgoto bail;\n\n\t \n\tret = hfi1_init_dd(dd);\n\tif (ret)\n\t\tgoto clean_bail;  \n\n\tret = create_workqueues(dd);\n\tif (ret)\n\t\tgoto clean_bail;\n\n\t \n\tinitfail = hfi1_init(dd, 0);\n\n\tret = hfi1_register_ib_device(dd);\n\n\t \n\tif (!initfail && !ret) {\n\t\tdd->flags |= HFI1_INITTED;\n\t\t \n\t\thfi1_dbg_ibdev_init(&dd->verbs_dev);\n\t}\n\n\tj = hfi1_device_create(dd);\n\tif (j)\n\t\tdd_dev_err(dd, \"Failed to create /dev devices: %d\\n\", -j);\n\n\tif (initfail || ret) {\n\t\tmsix_clean_up_interrupts(dd);\n\t\tstop_timers(dd);\n\t\tflush_workqueue(ib_wq);\n\t\tfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\n\t\t\thfi1_quiet_serdes(dd->pport + pidx);\n\t\t\tppd = dd->pport + pidx;\n\t\t\tif (ppd->hfi1_wq) {\n\t\t\t\tdestroy_workqueue(ppd->hfi1_wq);\n\t\t\t\tppd->hfi1_wq = NULL;\n\t\t\t}\n\t\t\tif (ppd->link_wq) {\n\t\t\t\tdestroy_workqueue(ppd->link_wq);\n\t\t\t\tppd->link_wq = NULL;\n\t\t\t}\n\t\t}\n\t\tif (!j)\n\t\t\thfi1_device_remove(dd);\n\t\tif (!ret)\n\t\t\thfi1_unregister_ib_device(dd);\n\t\tpostinit_cleanup(dd);\n\t\tif (initfail)\n\t\t\tret = initfail;\n\t\tgoto bail;\t \n\t}\n\n\tsdma_start(dd);\n\n\treturn 0;\n\nclean_bail:\n\thfi1_pcie_cleanup(pdev);\nbail:\n\treturn ret;\n}\n\nstatic void wait_for_clients(struct hfi1_devdata *dd)\n{\n\t \n\tif (refcount_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\n\twait_for_completion(&dd->user_comp);\n}\n\nstatic void remove_one(struct pci_dev *pdev)\n{\n\tstruct hfi1_devdata *dd = pci_get_drvdata(pdev);\n\n\t \n\thfi1_dbg_ibdev_exit(&dd->verbs_dev);\n\n\t \n\thfi1_device_remove(dd);\n\n\t \n\twait_for_clients(dd);\n\n\t \n\thfi1_unregister_ib_device(dd);\n\n\t \n\thfi1_free_rx(dd);\n\n\t \n\tshutdown_device(dd);\n\tdestroy_workqueues(dd);\n\n\tstop_timers(dd);\n\n\t \n\tflush_workqueue(ib_wq);\n\n\tpostinit_cleanup(dd);\n}\n\nstatic void shutdown_one(struct pci_dev *pdev)\n{\n\tstruct hfi1_devdata *dd = pci_get_drvdata(pdev);\n\n\tshutdown_device(dd);\n}\n\n \nint hfi1_create_rcvhdrq(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)\n{\n\tunsigned amt;\n\n\tif (!rcd->rcvhdrq) {\n\t\tamt = rcvhdrq_size(rcd);\n\n\t\trcd->rcvhdrq = dma_alloc_coherent(&dd->pcidev->dev, amt,\n\t\t\t\t\t\t  &rcd->rcvhdrq_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!rcd->rcvhdrq) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"attempt to allocate %d bytes for ctxt %u rcvhdrq failed\\n\",\n\t\t\t\t   amt, rcd->ctxt);\n\t\t\tgoto bail;\n\t\t}\n\n\t\tif (HFI1_CAP_KGET_MASK(rcd->flags, DMA_RTAIL) ||\n\t\t    HFI1_CAP_UGET_MASK(rcd->flags, DMA_RTAIL)) {\n\t\t\trcd->rcvhdrtail_kvaddr = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\t\t\t    PAGE_SIZE,\n\t\t\t\t\t\t\t\t    &rcd->rcvhdrqtailaddr_dma,\n\t\t\t\t\t\t\t\t    GFP_KERNEL);\n\t\t\tif (!rcd->rcvhdrtail_kvaddr)\n\t\t\t\tgoto bail_free;\n\t\t}\n\t}\n\n\tset_hdrq_regs(rcd->dd, rcd->ctxt, rcd->rcvhdrqentsize,\n\t\t      rcd->rcvhdrq_cnt);\n\n\treturn 0;\n\nbail_free:\n\tdd_dev_err(dd,\n\t\t   \"attempt to allocate 1 page for ctxt %u rcvhdrqtailaddr failed\\n\",\n\t\t   rcd->ctxt);\n\tdma_free_coherent(&dd->pcidev->dev, amt, rcd->rcvhdrq,\n\t\t\t  rcd->rcvhdrq_dma);\n\trcd->rcvhdrq = NULL;\nbail:\n\treturn -ENOMEM;\n}\n\n \nint hfi1_setup_eagerbufs(struct hfi1_ctxtdata *rcd)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu32 max_entries, egrtop, alloced_bytes = 0;\n\tu16 order, idx = 0;\n\tint ret = 0;\n\tu16 round_mtu = roundup_pow_of_two(hfi1_max_mtu);\n\n\t \n\tif (rcd->egrbufs.size < (round_mtu * dd->rcv_entries.group_size))\n\t\trcd->egrbufs.size = round_mtu * dd->rcv_entries.group_size;\n\t \n\tif (!HFI1_CAP_KGET_MASK(rcd->flags, MULTI_PKT_EGR))\n\t\trcd->egrbufs.rcvtid_size = round_mtu;\n\n\t \n\tif (rcd->egrbufs.size <= (1 << 20))\n\t\trcd->egrbufs.rcvtid_size = max((unsigned long)round_mtu,\n\t\t\trounddown_pow_of_two(rcd->egrbufs.size / 8));\n\n\twhile (alloced_bytes < rcd->egrbufs.size &&\n\t       rcd->egrbufs.alloced < rcd->egrbufs.count) {\n\t\trcd->egrbufs.buffers[idx].addr =\n\t\t\tdma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t   rcd->egrbufs.rcvtid_size,\n\t\t\t\t\t   &rcd->egrbufs.buffers[idx].dma,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (rcd->egrbufs.buffers[idx].addr) {\n\t\t\trcd->egrbufs.buffers[idx].len =\n\t\t\t\trcd->egrbufs.rcvtid_size;\n\t\t\trcd->egrbufs.rcvtids[rcd->egrbufs.alloced].addr =\n\t\t\t\trcd->egrbufs.buffers[idx].addr;\n\t\t\trcd->egrbufs.rcvtids[rcd->egrbufs.alloced].dma =\n\t\t\t\trcd->egrbufs.buffers[idx].dma;\n\t\t\trcd->egrbufs.alloced++;\n\t\t\talloced_bytes += rcd->egrbufs.rcvtid_size;\n\t\t\tidx++;\n\t\t} else {\n\t\t\tu32 new_size, i, j;\n\t\t\tu64 offset = 0;\n\n\t\t\t \n\t\t\tif (rcd->egrbufs.rcvtid_size == round_mtu ||\n\t\t\t    !HFI1_CAP_KGET_MASK(rcd->flags, MULTI_PKT_EGR)) {\n\t\t\t\tdd_dev_err(dd, \"ctxt%u: Failed to allocate eager buffers\\n\",\n\t\t\t\t\t   rcd->ctxt);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto bail_rcvegrbuf_phys;\n\t\t\t}\n\n\t\t\tnew_size = rcd->egrbufs.rcvtid_size / 2;\n\n\t\t\t \n\t\t\tif (idx == 0) {\n\t\t\t\trcd->egrbufs.rcvtid_size = new_size;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\trcd->egrbufs.alloced = 0;\n\t\t\tfor (i = 0, j = 0, offset = 0; j < idx; i++) {\n\t\t\t\tif (i >= rcd->egrbufs.count)\n\t\t\t\t\tbreak;\n\t\t\t\trcd->egrbufs.rcvtids[i].dma =\n\t\t\t\t\trcd->egrbufs.buffers[j].dma + offset;\n\t\t\t\trcd->egrbufs.rcvtids[i].addr =\n\t\t\t\t\trcd->egrbufs.buffers[j].addr + offset;\n\t\t\t\trcd->egrbufs.alloced++;\n\t\t\t\tif ((rcd->egrbufs.buffers[j].dma + offset +\n\t\t\t\t     new_size) ==\n\t\t\t\t    (rcd->egrbufs.buffers[j].dma +\n\t\t\t\t     rcd->egrbufs.buffers[j].len)) {\n\t\t\t\t\tj++;\n\t\t\t\t\toffset = 0;\n\t\t\t\t} else {\n\t\t\t\t\toffset += new_size;\n\t\t\t\t}\n\t\t\t}\n\t\t\trcd->egrbufs.rcvtid_size = new_size;\n\t\t}\n\t}\n\trcd->egrbufs.numbufs = idx;\n\trcd->egrbufs.size = alloced_bytes;\n\n\thfi1_cdbg(PROC,\n\t\t  \"ctxt%u: Alloced %u rcv tid entries @ %uKB, total %uKB\",\n\t\t  rcd->ctxt, rcd->egrbufs.alloced,\n\t\t  rcd->egrbufs.rcvtid_size / 1024, rcd->egrbufs.size / 1024);\n\n\t \n\trcd->egrbufs.threshold =\n\t\trounddown_pow_of_two(rcd->egrbufs.alloced / 2);\n\t \n\tmax_entries = rcd->rcv_array_groups * dd->rcv_entries.group_size;\n\tegrtop = roundup(rcd->egrbufs.alloced, dd->rcv_entries.group_size);\n\trcd->expected_count = max_entries - egrtop;\n\tif (rcd->expected_count > MAX_TID_PAIR_ENTRIES * 2)\n\t\trcd->expected_count = MAX_TID_PAIR_ENTRIES * 2;\n\n\trcd->expected_base = rcd->eager_base + egrtop;\n\thfi1_cdbg(PROC, \"ctxt%u: eager:%u, exp:%u, egrbase:%u, expbase:%u\",\n\t\t  rcd->ctxt, rcd->egrbufs.alloced, rcd->expected_count,\n\t\t  rcd->eager_base, rcd->expected_base);\n\n\tif (!hfi1_rcvbuf_validate(rcd->egrbufs.rcvtid_size, PT_EAGER, &order)) {\n\t\thfi1_cdbg(PROC,\n\t\t\t  \"ctxt%u: current Eager buffer size is invalid %u\",\n\t\t\t  rcd->ctxt, rcd->egrbufs.rcvtid_size);\n\t\tret = -EINVAL;\n\t\tgoto bail_rcvegrbuf_phys;\n\t}\n\n\tfor (idx = 0; idx < rcd->egrbufs.alloced; idx++) {\n\t\thfi1_put_tid(dd, rcd->eager_base + idx, PT_EAGER,\n\t\t\t     rcd->egrbufs.rcvtids[idx].dma, order);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n\nbail_rcvegrbuf_phys:\n\tfor (idx = 0; idx < rcd->egrbufs.alloced &&\n\t     rcd->egrbufs.buffers[idx].addr;\n\t     idx++) {\n\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t  rcd->egrbufs.buffers[idx].len,\n\t\t\t\t  rcd->egrbufs.buffers[idx].addr,\n\t\t\t\t  rcd->egrbufs.buffers[idx].dma);\n\t\trcd->egrbufs.buffers[idx].addr = NULL;\n\t\trcd->egrbufs.buffers[idx].dma = 0;\n\t\trcd->egrbufs.buffers[idx].len = 0;\n\t}\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}