{
  "module_name": "tid_rdma.c",
  "hash_id": "4bb441855f5103810d3c495e9215e0db3d8dec2c7ea16dece28190ec8bb1b4f8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/tid_rdma.c",
  "human_readable_source": "\n \n\n#include \"hfi.h\"\n#include \"qp.h\"\n#include \"rc.h\"\n#include \"verbs.h\"\n#include \"tid_rdma.h\"\n#include \"exp_rcv.h\"\n#include \"trace.h\"\n\n \n\n#define RCV_TID_FLOW_TABLE_CTRL_FLOW_VALID_SMASK BIT_ULL(32)\n#define RCV_TID_FLOW_TABLE_CTRL_HDR_SUPP_EN_SMASK BIT_ULL(33)\n#define RCV_TID_FLOW_TABLE_CTRL_KEEP_AFTER_SEQ_ERR_SMASK BIT_ULL(34)\n#define RCV_TID_FLOW_TABLE_CTRL_KEEP_ON_GEN_ERR_SMASK BIT_ULL(35)\n#define RCV_TID_FLOW_TABLE_STATUS_SEQ_MISMATCH_SMASK BIT_ULL(37)\n#define RCV_TID_FLOW_TABLE_STATUS_GEN_MISMATCH_SMASK BIT_ULL(38)\n\n \n#define MAX_TID_FLOW_PSN BIT(HFI1_KDETH_BTH_SEQ_SHIFT)\n\n#define GENERATION_MASK 0xFFFFF\n\nstatic u32 mask_generation(u32 a)\n{\n\treturn a & GENERATION_MASK;\n}\n\n \n#define KERN_GENERATION_RESERVED mask_generation(U32_MAX)\n\n \n#define TID_RDMA_JKEY                   32\n#define HFI1_KERNEL_MIN_JKEY HFI1_ADMIN_JKEY_RANGE\n#define HFI1_KERNEL_MAX_JKEY (2 * HFI1_ADMIN_JKEY_RANGE - 1)\n\n \n#define TID_RDMA_MAX_READ_SEGS_PER_REQ  6\n#define TID_RDMA_MAX_WRITE_SEGS_PER_REQ 4\n#define MAX_REQ max_t(u16, TID_RDMA_MAX_READ_SEGS_PER_REQ, \\\n\t\t\tTID_RDMA_MAX_WRITE_SEGS_PER_REQ)\n#define MAX_FLOWS roundup_pow_of_two(MAX_REQ + 1)\n\n#define MAX_EXPECTED_PAGES     (MAX_EXPECTED_BUFFER / PAGE_SIZE)\n\n#define TID_RDMA_DESTQP_FLOW_SHIFT      11\n#define TID_RDMA_DESTQP_FLOW_MASK       0x1f\n\n#define TID_OPFN_QP_CTXT_MASK 0xff\n#define TID_OPFN_QP_CTXT_SHIFT 56\n#define TID_OPFN_QP_KDETH_MASK 0xff\n#define TID_OPFN_QP_KDETH_SHIFT 48\n#define TID_OPFN_MAX_LEN_MASK 0x7ff\n#define TID_OPFN_MAX_LEN_SHIFT 37\n#define TID_OPFN_TIMEOUT_MASK 0x1f\n#define TID_OPFN_TIMEOUT_SHIFT 32\n#define TID_OPFN_RESERVED_MASK 0x3f\n#define TID_OPFN_RESERVED_SHIFT 26\n#define TID_OPFN_URG_MASK 0x1\n#define TID_OPFN_URG_SHIFT 25\n#define TID_OPFN_VER_MASK 0x7\n#define TID_OPFN_VER_SHIFT 22\n#define TID_OPFN_JKEY_MASK 0x3f\n#define TID_OPFN_JKEY_SHIFT 16\n#define TID_OPFN_MAX_READ_MASK 0x3f\n#define TID_OPFN_MAX_READ_SHIFT 10\n#define TID_OPFN_MAX_WRITE_MASK 0x3f\n#define TID_OPFN_MAX_WRITE_SHIFT 4\n\n \n\nstatic void tid_rdma_trigger_resume(struct work_struct *work);\nstatic void hfi1_kern_exp_rcv_free_flows(struct tid_rdma_request *req);\nstatic int hfi1_kern_exp_rcv_alloc_flows(struct tid_rdma_request *req,\n\t\t\t\t\t gfp_t gfp);\nstatic void hfi1_init_trdma_req(struct rvt_qp *qp,\n\t\t\t\tstruct tid_rdma_request *req);\nstatic void hfi1_tid_write_alloc_resources(struct rvt_qp *qp, bool intr_ctx);\nstatic void hfi1_tid_timeout(struct timer_list *t);\nstatic void hfi1_add_tid_reap_timer(struct rvt_qp *qp);\nstatic void hfi1_mod_tid_reap_timer(struct rvt_qp *qp);\nstatic void hfi1_mod_tid_retry_timer(struct rvt_qp *qp);\nstatic int hfi1_stop_tid_retry_timer(struct rvt_qp *qp);\nstatic void hfi1_tid_retry_timeout(struct timer_list *t);\nstatic int make_tid_rdma_ack(struct rvt_qp *qp,\n\t\t\t     struct ib_other_headers *ohdr,\n\t\t\t     struct hfi1_pkt_state *ps);\nstatic void hfi1_do_tid_send(struct rvt_qp *qp);\nstatic u32 read_r_next_psn(struct hfi1_devdata *dd, u8 ctxt, u8 fidx);\nstatic void tid_rdma_rcv_err(struct hfi1_packet *packet,\n\t\t\t     struct ib_other_headers *ohdr,\n\t\t\t     struct rvt_qp *qp, u32 psn, int diff, bool fecn);\nstatic void update_r_next_psn_fecn(struct hfi1_packet *packet,\n\t\t\t\t   struct hfi1_qp_priv *priv,\n\t\t\t\t   struct hfi1_ctxtdata *rcd,\n\t\t\t\t   struct tid_rdma_flow *flow,\n\t\t\t\t   bool fecn);\n\nstatic void validate_r_tid_ack(struct hfi1_qp_priv *priv)\n{\n\tif (priv->r_tid_ack == HFI1_QP_WQE_INVALID)\n\t\tpriv->r_tid_ack = priv->r_tid_tail;\n}\n\nstatic void tid_rdma_schedule_ack(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tpriv->s_flags |= RVT_S_ACK_PENDING;\n\thfi1_schedule_tid_send(qp);\n}\n\nstatic void tid_rdma_trigger_ack(struct rvt_qp *qp)\n{\n\tvalidate_r_tid_ack(qp->priv);\n\ttid_rdma_schedule_ack(qp);\n}\n\nstatic u64 tid_rdma_opfn_encode(struct tid_rdma_params *p)\n{\n\treturn\n\t\t(((u64)p->qp & TID_OPFN_QP_CTXT_MASK) <<\n\t\t\tTID_OPFN_QP_CTXT_SHIFT) |\n\t\t((((u64)p->qp >> 16) & TID_OPFN_QP_KDETH_MASK) <<\n\t\t\tTID_OPFN_QP_KDETH_SHIFT) |\n\t\t(((u64)((p->max_len >> PAGE_SHIFT) - 1) &\n\t\t\tTID_OPFN_MAX_LEN_MASK) << TID_OPFN_MAX_LEN_SHIFT) |\n\t\t(((u64)p->timeout & TID_OPFN_TIMEOUT_MASK) <<\n\t\t\tTID_OPFN_TIMEOUT_SHIFT) |\n\t\t(((u64)p->urg & TID_OPFN_URG_MASK) << TID_OPFN_URG_SHIFT) |\n\t\t(((u64)p->jkey & TID_OPFN_JKEY_MASK) << TID_OPFN_JKEY_SHIFT) |\n\t\t(((u64)p->max_read & TID_OPFN_MAX_READ_MASK) <<\n\t\t\tTID_OPFN_MAX_READ_SHIFT) |\n\t\t(((u64)p->max_write & TID_OPFN_MAX_WRITE_MASK) <<\n\t\t\tTID_OPFN_MAX_WRITE_SHIFT);\n}\n\nstatic void tid_rdma_opfn_decode(struct tid_rdma_params *p, u64 data)\n{\n\tp->max_len = (((data >> TID_OPFN_MAX_LEN_SHIFT) &\n\t\tTID_OPFN_MAX_LEN_MASK) + 1) << PAGE_SHIFT;\n\tp->jkey = (data >> TID_OPFN_JKEY_SHIFT) & TID_OPFN_JKEY_MASK;\n\tp->max_write = (data >> TID_OPFN_MAX_WRITE_SHIFT) &\n\t\tTID_OPFN_MAX_WRITE_MASK;\n\tp->max_read = (data >> TID_OPFN_MAX_READ_SHIFT) &\n\t\tTID_OPFN_MAX_READ_MASK;\n\tp->qp =\n\t\t((((data >> TID_OPFN_QP_KDETH_SHIFT) & TID_OPFN_QP_KDETH_MASK)\n\t\t\t<< 16) |\n\t\t((data >> TID_OPFN_QP_CTXT_SHIFT) & TID_OPFN_QP_CTXT_MASK));\n\tp->urg = (data >> TID_OPFN_URG_SHIFT) & TID_OPFN_URG_MASK;\n\tp->timeout = (data >> TID_OPFN_TIMEOUT_SHIFT) & TID_OPFN_TIMEOUT_MASK;\n}\n\nvoid tid_rdma_opfn_init(struct rvt_qp *qp, struct tid_rdma_params *p)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tp->qp = (RVT_KDETH_QP_PREFIX << 16) | priv->rcd->ctxt;\n\tp->max_len = TID_RDMA_MAX_SEGMENT_SIZE;\n\tp->jkey = priv->rcd->jkey;\n\tp->max_read = TID_RDMA_MAX_READ_SEGS_PER_REQ;\n\tp->max_write = TID_RDMA_MAX_WRITE_SEGS_PER_REQ;\n\tp->timeout = qp->timeout;\n\tp->urg = is_urg_masked(priv->rcd);\n}\n\nbool tid_rdma_conn_req(struct rvt_qp *qp, u64 *data)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\t*data = tid_rdma_opfn_encode(&priv->tid_rdma.local);\n\treturn true;\n}\n\nbool tid_rdma_conn_reply(struct rvt_qp *qp, u64 data)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct tid_rdma_params *remote, *old;\n\tbool ret = true;\n\n\told = rcu_dereference_protected(priv->tid_rdma.remote,\n\t\t\t\t\tlockdep_is_held(&priv->opfn.lock));\n\tdata &= ~0xfULL;\n\t \n\tif (!data || !HFI1_CAP_IS_KSET(TID_RDMA))\n\t\tgoto null;\n\t \n\tremote = kzalloc(sizeof(*remote), GFP_ATOMIC);\n\tif (!remote) {\n\t\tret = false;\n\t\tgoto null;\n\t}\n\n\ttid_rdma_opfn_decode(remote, data);\n\tpriv->tid_timer_timeout_jiffies =\n\t\tusecs_to_jiffies((((4096UL * (1UL << remote->timeout)) /\n\t\t\t\t   1000UL) << 3) * 7);\n\ttrace_hfi1_opfn_param(qp, 0, &priv->tid_rdma.local);\n\ttrace_hfi1_opfn_param(qp, 1, remote);\n\trcu_assign_pointer(priv->tid_rdma.remote, remote);\n\t \n\tpriv->pkts_ps = (u16)rvt_div_mtu(qp, remote->max_len);\n\tpriv->timeout_shift = ilog2(priv->pkts_ps - 1) + 1;\n\tgoto free;\nnull:\n\tRCU_INIT_POINTER(priv->tid_rdma.remote, NULL);\n\tpriv->timeout_shift = 0;\nfree:\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n\treturn ret;\n}\n\nbool tid_rdma_conn_resp(struct rvt_qp *qp, u64 *data)\n{\n\tbool ret;\n\n\tret = tid_rdma_conn_reply(qp, *data);\n\t*data = 0;\n\t \n\tif (ret)\n\t\t(void)tid_rdma_conn_req(qp, data);\n\treturn ret;\n}\n\nvoid tid_rdma_conn_error(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct tid_rdma_params *old;\n\n\told = rcu_dereference_protected(priv->tid_rdma.remote,\n\t\t\t\t\tlockdep_is_held(&priv->opfn.lock));\n\tRCU_INIT_POINTER(priv->tid_rdma.remote, NULL);\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n}\n\n \nint hfi1_kern_exp_rcv_init(struct hfi1_ctxtdata *rcd, int reinit)\n{\n\tif (reinit)\n\t\treturn 0;\n\n\tBUILD_BUG_ON(TID_RDMA_JKEY < HFI1_KERNEL_MIN_JKEY);\n\tBUILD_BUG_ON(TID_RDMA_JKEY > HFI1_KERNEL_MAX_JKEY);\n\trcd->jkey = TID_RDMA_JKEY;\n\thfi1_set_ctxt_jkey(rcd->dd, rcd, rcd->jkey);\n\treturn hfi1_alloc_ctxt_rcv_groups(rcd);\n}\n\n \nstatic struct hfi1_ctxtdata *qp_to_rcd(struct rvt_dev_info *rdi,\n\t\t\t\t       struct rvt_qp *qp)\n{\n\tstruct hfi1_ibdev *verbs_dev = container_of(rdi,\n\t\t\t\t\t\t    struct hfi1_ibdev,\n\t\t\t\t\t\t    rdi);\n\tstruct hfi1_devdata *dd = container_of(verbs_dev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       verbs_dev);\n\tunsigned int ctxt;\n\n\tif (qp->ibqp.qp_num == 0)\n\t\tctxt = 0;\n\telse\n\t\tctxt = hfi1_get_qp_map(dd, qp->ibqp.qp_num >> dd->qos_shift);\n\treturn dd->rcd[ctxt];\n}\n\nint hfi1_qp_priv_init(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t      struct ib_qp_init_attr *init_attr)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tint i, ret;\n\n\tqpriv->rcd = qp_to_rcd(rdi, qp);\n\n\tspin_lock_init(&qpriv->opfn.lock);\n\tINIT_WORK(&qpriv->opfn.opfn_work, opfn_send_conn_request);\n\tINIT_WORK(&qpriv->tid_rdma.trigger_work, tid_rdma_trigger_resume);\n\tqpriv->flow_state.psn = 0;\n\tqpriv->flow_state.index = RXE_NUM_TID_FLOWS;\n\tqpriv->flow_state.last_index = RXE_NUM_TID_FLOWS;\n\tqpriv->flow_state.generation = KERN_GENERATION_RESERVED;\n\tqpriv->s_state = TID_OP(WRITE_RESP);\n\tqpriv->s_tid_cur = HFI1_QP_WQE_INVALID;\n\tqpriv->s_tid_head = HFI1_QP_WQE_INVALID;\n\tqpriv->s_tid_tail = HFI1_QP_WQE_INVALID;\n\tqpriv->rnr_nak_state = TID_RNR_NAK_INIT;\n\tqpriv->r_tid_head = HFI1_QP_WQE_INVALID;\n\tqpriv->r_tid_tail = HFI1_QP_WQE_INVALID;\n\tqpriv->r_tid_ack = HFI1_QP_WQE_INVALID;\n\tqpriv->r_tid_alloc = HFI1_QP_WQE_INVALID;\n\tatomic_set(&qpriv->n_requests, 0);\n\tatomic_set(&qpriv->n_tid_requests, 0);\n\ttimer_setup(&qpriv->s_tid_timer, hfi1_tid_timeout, 0);\n\ttimer_setup(&qpriv->s_tid_retry_timer, hfi1_tid_retry_timeout, 0);\n\tINIT_LIST_HEAD(&qpriv->tid_wait);\n\n\tif (init_attr->qp_type == IB_QPT_RC && HFI1_CAP_IS_KSET(TID_RDMA)) {\n\t\tstruct hfi1_devdata *dd = qpriv->rcd->dd;\n\n\t\tqpriv->pages = kzalloc_node(TID_RDMA_MAX_PAGES *\n\t\t\t\t\t\tsizeof(*qpriv->pages),\n\t\t\t\t\t    GFP_KERNEL, dd->node);\n\t\tif (!qpriv->pages)\n\t\t\treturn -ENOMEM;\n\t\tfor (i = 0; i < qp->s_size; i++) {\n\t\t\tstruct hfi1_swqe_priv *priv;\n\t\t\tstruct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, i);\n\n\t\t\tpriv = kzalloc_node(sizeof(*priv), GFP_KERNEL,\n\t\t\t\t\t    dd->node);\n\t\t\tif (!priv)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\thfi1_init_trdma_req(qp, &priv->tid_req);\n\t\t\tpriv->tid_req.e.swqe = wqe;\n\t\t\twqe->priv = priv;\n\t\t}\n\t\tfor (i = 0; i < rvt_max_atomic(rdi); i++) {\n\t\t\tstruct hfi1_ack_priv *priv;\n\n\t\t\tpriv = kzalloc_node(sizeof(*priv), GFP_KERNEL,\n\t\t\t\t\t    dd->node);\n\t\t\tif (!priv)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\thfi1_init_trdma_req(qp, &priv->tid_req);\n\t\t\tpriv->tid_req.e.ack = &qp->s_ack_queue[i];\n\n\t\t\tret = hfi1_kern_exp_rcv_alloc_flows(&priv->tid_req,\n\t\t\t\t\t\t\t    GFP_KERNEL);\n\t\t\tif (ret) {\n\t\t\t\tkfree(priv);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tqp->s_ack_queue[i].priv = priv;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nvoid hfi1_qp_priv_tid_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct rvt_swqe *wqe;\n\tu32 i;\n\n\tif (qp->ibqp.qp_type == IB_QPT_RC && HFI1_CAP_IS_KSET(TID_RDMA)) {\n\t\tfor (i = 0; i < qp->s_size; i++) {\n\t\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\t\tkfree(wqe->priv);\n\t\t\twqe->priv = NULL;\n\t\t}\n\t\tfor (i = 0; i < rvt_max_atomic(rdi); i++) {\n\t\t\tstruct hfi1_ack_priv *priv = qp->s_ack_queue[i].priv;\n\n\t\t\tif (priv)\n\t\t\t\thfi1_kern_exp_rcv_free_flows(&priv->tid_req);\n\t\t\tkfree(priv);\n\t\t\tqp->s_ack_queue[i].priv = NULL;\n\t\t}\n\t\tcancel_work_sync(&qpriv->opfn.opfn_work);\n\t\tkfree(qpriv->pages);\n\t\tqpriv->pages = NULL;\n\t}\n}\n\n \n \n\n \nstatic struct rvt_qp *first_qp(struct hfi1_ctxtdata *rcd,\n\t\t\t       struct tid_queue *queue)\n\t__must_hold(&rcd->exp_lock)\n{\n\tstruct hfi1_qp_priv *priv;\n\n\tlockdep_assert_held(&rcd->exp_lock);\n\tpriv = list_first_entry_or_null(&queue->queue_head,\n\t\t\t\t\tstruct hfi1_qp_priv,\n\t\t\t\t\ttid_wait);\n\tif (!priv)\n\t\treturn NULL;\n\trvt_get_qp(priv->owner);\n\treturn priv->owner;\n}\n\n \nstatic bool kernel_tid_waiters(struct hfi1_ctxtdata *rcd,\n\t\t\t       struct tid_queue *queue, struct rvt_qp *qp)\n\t__must_hold(&rcd->exp_lock) __must_hold(&qp->s_lock)\n{\n\tstruct rvt_qp *fqp;\n\tbool ret = true;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tlockdep_assert_held(&rcd->exp_lock);\n\tfqp = first_qp(rcd, queue);\n\tif (!fqp || (fqp == qp && (qp->s_flags & HFI1_S_WAIT_TID_SPACE)))\n\t\tret = false;\n\trvt_put_qp(fqp);\n\treturn ret;\n}\n\n \nstatic void dequeue_tid_waiter(struct hfi1_ctxtdata *rcd,\n\t\t\t       struct tid_queue *queue, struct rvt_qp *qp)\n\t__must_hold(&rcd->exp_lock) __must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tlockdep_assert_held(&rcd->exp_lock);\n\tif (list_empty(&priv->tid_wait))\n\t\treturn;\n\tlist_del_init(&priv->tid_wait);\n\tqp->s_flags &= ~HFI1_S_WAIT_TID_SPACE;\n\tqueue->dequeue++;\n\trvt_put_qp(qp);\n}\n\n \nstatic void queue_qp_for_tid_wait(struct hfi1_ctxtdata *rcd,\n\t\t\t\t  struct tid_queue *queue, struct rvt_qp *qp)\n\t__must_hold(&rcd->exp_lock) __must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tlockdep_assert_held(&rcd->exp_lock);\n\tif (list_empty(&priv->tid_wait)) {\n\t\tqp->s_flags |= HFI1_S_WAIT_TID_SPACE;\n\t\tlist_add_tail(&priv->tid_wait, &queue->queue_head);\n\t\tpriv->tid_enqueue = ++queue->enqueue;\n\t\trcd->dd->verbs_dev.n_tidwait++;\n\t\ttrace_hfi1_qpsleep(qp, HFI1_S_WAIT_TID_SPACE);\n\t\trvt_get_qp(qp);\n\t}\n}\n\n \nstatic void __trigger_tid_waiter(struct rvt_qp *qp)\n\t__must_hold(&qp->s_lock)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\tif (!(qp->s_flags & HFI1_S_WAIT_TID_SPACE))\n\t\treturn;\n\ttrace_hfi1_qpwakeup(qp, HFI1_S_WAIT_TID_SPACE);\n\thfi1_schedule_send(qp);\n}\n\n \nstatic void tid_rdma_schedule_tid_wakeup(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv;\n\tstruct hfi1_ibport *ibp;\n\tstruct hfi1_pportdata *ppd;\n\tstruct hfi1_devdata *dd;\n\tbool rval;\n\n\tif (!qp)\n\t\treturn;\n\n\tpriv = qp->priv;\n\tibp = to_iport(qp->ibqp.device, qp->port_num);\n\tppd = ppd_from_ibp(ibp);\n\tdd = dd_from_ibdev(qp->ibqp.device);\n\n\trval = queue_work_on(priv->s_sde ?\n\t\t\t     priv->s_sde->cpu :\n\t\t\t     cpumask_first(cpumask_of_node(dd->node)),\n\t\t\t     ppd->hfi1_wq,\n\t\t\t     &priv->tid_rdma.trigger_work);\n\tif (!rval)\n\t\trvt_put_qp(qp);\n}\n\n \nstatic void tid_rdma_trigger_resume(struct work_struct *work)\n{\n\tstruct tid_rdma_qp_params *tr;\n\tstruct hfi1_qp_priv *priv;\n\tstruct rvt_qp *qp;\n\n\ttr = container_of(work, struct tid_rdma_qp_params, trigger_work);\n\tpriv = container_of(tr, struct hfi1_qp_priv, tid_rdma);\n\tqp = priv->owner;\n\tspin_lock_irq(&qp->s_lock);\n\tif (qp->s_flags & HFI1_S_WAIT_TID_SPACE) {\n\t\tspin_unlock_irq(&qp->s_lock);\n\t\thfi1_do_send(priv->owner, true);\n\t} else {\n\t\tspin_unlock_irq(&qp->s_lock);\n\t}\n\trvt_put_qp(qp);\n}\n\n \nstatic void _tid_rdma_flush_wait(struct rvt_qp *qp, struct tid_queue *queue)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv;\n\n\tif (!qp)\n\t\treturn;\n\tlockdep_assert_held(&qp->s_lock);\n\tpriv = qp->priv;\n\tqp->s_flags &= ~HFI1_S_WAIT_TID_SPACE;\n\tspin_lock(&priv->rcd->exp_lock);\n\tif (!list_empty(&priv->tid_wait)) {\n\t\tlist_del_init(&priv->tid_wait);\n\t\tqp->s_flags &= ~HFI1_S_WAIT_TID_SPACE;\n\t\tqueue->dequeue++;\n\t\trvt_put_qp(qp);\n\t}\n\tspin_unlock(&priv->rcd->exp_lock);\n}\n\nvoid hfi1_tid_rdma_flush_wait(struct rvt_qp *qp)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\t_tid_rdma_flush_wait(qp, &priv->rcd->flow_queue);\n\t_tid_rdma_flush_wait(qp, &priv->rcd->rarr_queue);\n}\n\n \n \nstatic int kern_reserve_flow(struct hfi1_ctxtdata *rcd, int last)\n\t__must_hold(&rcd->exp_lock)\n{\n\tint nr;\n\n\t \n\tif (last >= 0 && last < RXE_NUM_TID_FLOWS &&\n\t    !test_and_set_bit(last, &rcd->flow_mask))\n\t\treturn last;\n\n\tnr = ffz(rcd->flow_mask);\n\tBUILD_BUG_ON(RXE_NUM_TID_FLOWS >=\n\t\t     (sizeof(rcd->flow_mask) * BITS_PER_BYTE));\n\tif (nr > (RXE_NUM_TID_FLOWS - 1))\n\t\treturn -EAGAIN;\n\tset_bit(nr, &rcd->flow_mask);\n\treturn nr;\n}\n\nstatic void kern_set_hw_flow(struct hfi1_ctxtdata *rcd, u32 generation,\n\t\t\t     u32 flow_idx)\n{\n\tu64 reg;\n\n\treg = ((u64)generation << HFI1_KDETH_BTH_SEQ_SHIFT) |\n\t\tRCV_TID_FLOW_TABLE_CTRL_FLOW_VALID_SMASK |\n\t\tRCV_TID_FLOW_TABLE_CTRL_KEEP_AFTER_SEQ_ERR_SMASK |\n\t\tRCV_TID_FLOW_TABLE_CTRL_KEEP_ON_GEN_ERR_SMASK |\n\t\tRCV_TID_FLOW_TABLE_STATUS_SEQ_MISMATCH_SMASK |\n\t\tRCV_TID_FLOW_TABLE_STATUS_GEN_MISMATCH_SMASK;\n\n\tif (generation != KERN_GENERATION_RESERVED)\n\t\treg |= RCV_TID_FLOW_TABLE_CTRL_HDR_SUPP_EN_SMASK;\n\n\twrite_uctxt_csr(rcd->dd, rcd->ctxt,\n\t\t\tRCV_TID_FLOW_TABLE + 8 * flow_idx, reg);\n}\n\nstatic u32 kern_setup_hw_flow(struct hfi1_ctxtdata *rcd, u32 flow_idx)\n\t__must_hold(&rcd->exp_lock)\n{\n\tu32 generation = rcd->flows[flow_idx].generation;\n\n\tkern_set_hw_flow(rcd, generation, flow_idx);\n\treturn generation;\n}\n\nstatic u32 kern_flow_generation_next(u32 gen)\n{\n\tu32 generation = mask_generation(gen + 1);\n\n\tif (generation == KERN_GENERATION_RESERVED)\n\t\tgeneration = mask_generation(generation + 1);\n\treturn generation;\n}\n\nstatic void kern_clear_hw_flow(struct hfi1_ctxtdata *rcd, u32 flow_idx)\n\t__must_hold(&rcd->exp_lock)\n{\n\trcd->flows[flow_idx].generation =\n\t\tkern_flow_generation_next(rcd->flows[flow_idx].generation);\n\tkern_set_hw_flow(rcd, KERN_GENERATION_RESERVED, flow_idx);\n}\n\nint hfi1_kern_setup_hw_flow(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = (struct hfi1_qp_priv *)qp->priv;\n\tstruct tid_flow_state *fs = &qpriv->flow_state;\n\tstruct rvt_qp *fqp;\n\tunsigned long flags;\n\tint ret = 0;\n\n\t \n\tif (fs->index != RXE_NUM_TID_FLOWS)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&rcd->exp_lock, flags);\n\tif (kernel_tid_waiters(rcd, &rcd->flow_queue, qp))\n\t\tgoto queue;\n\n\tret = kern_reserve_flow(rcd, fs->last_index);\n\tif (ret < 0)\n\t\tgoto queue;\n\tfs->index = ret;\n\tfs->last_index = fs->index;\n\n\t \n\tif (fs->generation != KERN_GENERATION_RESERVED)\n\t\trcd->flows[fs->index].generation = fs->generation;\n\tfs->generation = kern_setup_hw_flow(rcd, fs->index);\n\tfs->psn = 0;\n\tdequeue_tid_waiter(rcd, &rcd->flow_queue, qp);\n\t \n\tfqp = first_qp(rcd, &rcd->flow_queue);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\n\ttid_rdma_schedule_tid_wakeup(fqp);\n\treturn 0;\nqueue:\n\tqueue_qp_for_tid_wait(rcd, &rcd->flow_queue, qp);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\treturn -EAGAIN;\n}\n\nvoid hfi1_kern_clear_hw_flow(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = (struct hfi1_qp_priv *)qp->priv;\n\tstruct tid_flow_state *fs = &qpriv->flow_state;\n\tstruct rvt_qp *fqp;\n\tunsigned long flags;\n\n\tif (fs->index >= RXE_NUM_TID_FLOWS)\n\t\treturn;\n\tspin_lock_irqsave(&rcd->exp_lock, flags);\n\tkern_clear_hw_flow(rcd, fs->index);\n\tclear_bit(fs->index, &rcd->flow_mask);\n\tfs->index = RXE_NUM_TID_FLOWS;\n\tfs->psn = 0;\n\tfs->generation = KERN_GENERATION_RESERVED;\n\n\t \n\tfqp = first_qp(rcd, &rcd->flow_queue);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\n\tif (fqp == qp) {\n\t\t__trigger_tid_waiter(fqp);\n\t\trvt_put_qp(fqp);\n\t} else {\n\t\ttid_rdma_schedule_tid_wakeup(fqp);\n\t}\n}\n\nvoid hfi1_kern_init_ctxt_generations(struct hfi1_ctxtdata *rcd)\n{\n\tint i;\n\n\tfor (i = 0; i < RXE_NUM_TID_FLOWS; i++) {\n\t\trcd->flows[i].generation = mask_generation(get_random_u32());\n\t\tkern_set_hw_flow(rcd, KERN_GENERATION_RESERVED, i);\n\t}\n}\n\n \nstatic u8 trdma_pset_order(struct tid_rdma_pageset *s)\n{\n\tu8 count = s->count;\n\n\treturn ilog2(count) + 1;\n}\n\n \nstatic u32 tid_rdma_find_phys_blocks_4k(struct tid_rdma_flow *flow,\n\t\t\t\t\tstruct page **pages,\n\t\t\t\t\tu32 npages,\n\t\t\t\t\tstruct tid_rdma_pageset *list)\n{\n\tu32 pagecount, pageidx, setcount = 0, i;\n\tvoid *vaddr, *this_vaddr;\n\n\tif (!npages)\n\t\treturn 0;\n\n\t \n\tvaddr = page_address(pages[0]);\n\ttrace_hfi1_tid_flow_page(flow->req->qp, flow, 0, 0, 0, vaddr);\n\tfor (pageidx = 0, pagecount = 1, i = 1; i <= npages; i++) {\n\t\tthis_vaddr = i < npages ? page_address(pages[i]) : NULL;\n\t\ttrace_hfi1_tid_flow_page(flow->req->qp, flow, i, 0, 0,\n\t\t\t\t\t this_vaddr);\n\t\t \n\t\tif (this_vaddr != (vaddr + PAGE_SIZE)) {\n\t\t\t \n\t\t\twhile (pagecount) {\n\t\t\t\tint maxpages = pagecount;\n\t\t\t\tu32 bufsize = pagecount * PAGE_SIZE;\n\n\t\t\t\tif (bufsize > MAX_EXPECTED_BUFFER)\n\t\t\t\t\tmaxpages =\n\t\t\t\t\t\tMAX_EXPECTED_BUFFER >>\n\t\t\t\t\t\tPAGE_SHIFT;\n\t\t\t\telse if (!is_power_of_2(bufsize))\n\t\t\t\t\tmaxpages =\n\t\t\t\t\t\trounddown_pow_of_two(bufsize) >>\n\t\t\t\t\t\tPAGE_SHIFT;\n\n\t\t\t\tlist[setcount].idx = pageidx;\n\t\t\t\tlist[setcount].count = maxpages;\n\t\t\t\ttrace_hfi1_tid_pageset(flow->req->qp, setcount,\n\t\t\t\t\t\t       list[setcount].idx,\n\t\t\t\t\t\t       list[setcount].count);\n\t\t\t\tpagecount -= maxpages;\n\t\t\t\tpageidx += maxpages;\n\t\t\t\tsetcount++;\n\t\t\t}\n\t\t\tpageidx = i;\n\t\t\tpagecount = 1;\n\t\t\tvaddr = this_vaddr;\n\t\t} else {\n\t\t\tvaddr += PAGE_SIZE;\n\t\t\tpagecount++;\n\t\t}\n\t}\n\t \n\tif (setcount & 1)\n\t\tlist[setcount++].count = 0;\n\treturn setcount;\n}\n\n \n\nstatic u32 tid_flush_pages(struct tid_rdma_pageset *list,\n\t\t\t   u32 *idx, u32 pages, u32 sets)\n{\n\twhile (pages) {\n\t\tu32 maxpages = pages;\n\n\t\tif (maxpages > MAX_EXPECTED_PAGES)\n\t\t\tmaxpages = MAX_EXPECTED_PAGES;\n\t\telse if (!is_power_of_2(maxpages))\n\t\t\tmaxpages = rounddown_pow_of_two(maxpages);\n\t\tlist[sets].idx = *idx;\n\t\tlist[sets++].count = maxpages;\n\t\t*idx += maxpages;\n\t\tpages -= maxpages;\n\t}\n\t \n\tif (sets & 1)\n\t\tlist[sets++].count = 0;\n\treturn sets;\n}\n\n \nstatic u32 tid_rdma_find_phys_blocks_8k(struct tid_rdma_flow *flow,\n\t\t\t\t\tstruct page **pages,\n\t\t\t\t\tu32 npages,\n\t\t\t\t\tstruct tid_rdma_pageset *list)\n{\n\tu32 idx, sets = 0, i;\n\tu32 pagecnt = 0;\n\tvoid *v0, *v1, *vm1;\n\n\tif (!npages)\n\t\treturn 0;\n\tfor (idx = 0, i = 0, vm1 = NULL; i < npages; i += 2) {\n\t\t \n\t\tv0 = page_address(pages[i]);\n\t\ttrace_hfi1_tid_flow_page(flow->req->qp, flow, i, 1, 0, v0);\n\t\tv1 = i + 1 < npages ?\n\t\t\t\tpage_address(pages[i + 1]) : NULL;\n\t\ttrace_hfi1_tid_flow_page(flow->req->qp, flow, i, 1, 1, v1);\n\t\t \n\t\tif (v1 != (v0 + PAGE_SIZE)) {\n\t\t\t \n\t\t\tsets = tid_flush_pages(list, &idx, pagecnt, sets);\n\t\t\t \n\t\t\tlist[sets].idx = idx++;\n\t\t\tlist[sets++].count = 1;\n\t\t\tif (v1) {\n\t\t\t\tlist[sets].count = 1;\n\t\t\t\tlist[sets++].idx = idx++;\n\t\t\t} else {\n\t\t\t\tlist[sets++].count = 0;\n\t\t\t}\n\t\t\tvm1 = NULL;\n\t\t\tpagecnt = 0;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (vm1 && v0 != (vm1 + PAGE_SIZE)) {\n\t\t\t \n\t\t\tsets = tid_flush_pages(list, &idx, pagecnt, sets);\n\t\t\tpagecnt = 0;\n\t\t}\n\t\t \n\t\tpagecnt += 2;\n\t\t \n\t\tvm1 = v1;\n\t\t \n\t}\n\t \n\tsets = tid_flush_pages(list, &idx, npages - idx, sets);\n\t \n\tWARN_ON(sets & 1);\n\treturn sets;\n}\n\n \nstatic u32 kern_find_pages(struct tid_rdma_flow *flow,\n\t\t\t   struct page **pages,\n\t\t\t   struct rvt_sge_state *ss, bool *last)\n{\n\tstruct tid_rdma_request *req = flow->req;\n\tstruct rvt_sge *sge = &ss->sge;\n\tu32 length = flow->req->seg_len;\n\tu32 len = PAGE_SIZE;\n\tu32 i = 0;\n\n\twhile (length && req->isge < ss->num_sge) {\n\t\tpages[i++] = virt_to_page(sge->vaddr);\n\n\t\tsge->vaddr += len;\n\t\tsge->length -= len;\n\t\tsge->sge_length -= len;\n\t\tif (!sge->sge_length) {\n\t\t\tif (++req->isge < ss->num_sge)\n\t\t\t\t*sge = ss->sg_list[req->isge - 1];\n\t\t} else if (sge->length == 0 && sge->mr->lkey) {\n\t\t\tif (++sge->n >= RVT_SEGSZ) {\n\t\t\t\t++sge->m;\n\t\t\t\tsge->n = 0;\n\t\t\t}\n\t\t\tsge->vaddr = sge->mr->map[sge->m]->segs[sge->n].vaddr;\n\t\t\tsge->length = sge->mr->map[sge->m]->segs[sge->n].length;\n\t\t}\n\t\tlength -= len;\n\t}\n\n\tflow->length = flow->req->seg_len - length;\n\t*last = req->isge != ss->num_sge;\n\treturn i;\n}\n\nstatic void dma_unmap_flow(struct tid_rdma_flow *flow)\n{\n\tstruct hfi1_devdata *dd;\n\tint i;\n\tstruct tid_rdma_pageset *pset;\n\n\tdd = flow->req->rcd->dd;\n\tfor (i = 0, pset = &flow->pagesets[0]; i < flow->npagesets;\n\t\t\ti++, pset++) {\n\t\tif (pset->count && pset->addr) {\n\t\t\tdma_unmap_page(&dd->pcidev->dev,\n\t\t\t\t       pset->addr,\n\t\t\t\t       PAGE_SIZE * pset->count,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\tpset->mapped = 0;\n\t\t}\n\t}\n}\n\nstatic int dma_map_flow(struct tid_rdma_flow *flow, struct page **pages)\n{\n\tint i;\n\tstruct hfi1_devdata *dd = flow->req->rcd->dd;\n\tstruct tid_rdma_pageset *pset;\n\n\tfor (i = 0, pset = &flow->pagesets[0]; i < flow->npagesets;\n\t\t\ti++, pset++) {\n\t\tif (pset->count) {\n\t\t\tpset->addr = dma_map_page(&dd->pcidev->dev,\n\t\t\t\t\t\t  pages[pset->idx],\n\t\t\t\t\t\t  0,\n\t\t\t\t\t\t  PAGE_SIZE * pset->count,\n\t\t\t\t\t\t  DMA_FROM_DEVICE);\n\n\t\t\tif (dma_mapping_error(&dd->pcidev->dev, pset->addr)) {\n\t\t\t\tdma_unmap_flow(flow);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tpset->mapped = 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic inline bool dma_mapped(struct tid_rdma_flow *flow)\n{\n\treturn !!flow->pagesets[0].mapped;\n}\n\n \nstatic int kern_get_phys_blocks(struct tid_rdma_flow *flow,\n\t\t\t\tstruct page **pages,\n\t\t\t\tstruct rvt_sge_state *ss, bool *last)\n{\n\tu8 npages;\n\n\t \n\tif (flow->npagesets) {\n\t\ttrace_hfi1_tid_flow_alloc(flow->req->qp, flow->req->setup_head,\n\t\t\t\t\t  flow);\n\t\tif (!dma_mapped(flow))\n\t\t\treturn dma_map_flow(flow, pages);\n\t\treturn 0;\n\t}\n\n\tnpages = kern_find_pages(flow, pages, ss, last);\n\n\tif (flow->req->qp->pmtu == enum_to_mtu(OPA_MTU_4096))\n\t\tflow->npagesets =\n\t\t\ttid_rdma_find_phys_blocks_4k(flow, pages, npages,\n\t\t\t\t\t\t     flow->pagesets);\n\telse\n\t\tflow->npagesets =\n\t\t\ttid_rdma_find_phys_blocks_8k(flow, pages, npages,\n\t\t\t\t\t\t     flow->pagesets);\n\n\treturn dma_map_flow(flow, pages);\n}\n\nstatic inline void kern_add_tid_node(struct tid_rdma_flow *flow,\n\t\t\t\t     struct hfi1_ctxtdata *rcd, char *s,\n\t\t\t\t     struct tid_group *grp, u8 cnt)\n{\n\tstruct kern_tid_node *node = &flow->tnode[flow->tnode_cnt++];\n\n\tWARN_ON_ONCE(flow->tnode_cnt >=\n\t\t     (TID_RDMA_MAX_SEGMENT_SIZE >> PAGE_SHIFT));\n\tif (WARN_ON_ONCE(cnt & 1))\n\t\tdd_dev_err(rcd->dd,\n\t\t\t   \"unexpected odd allocation cnt %u map 0x%x used %u\",\n\t\t\t   cnt, grp->map, grp->used);\n\n\tnode->grp = grp;\n\tnode->map = grp->map;\n\tnode->cnt = cnt;\n\ttrace_hfi1_tid_node_add(flow->req->qp, s, flow->tnode_cnt - 1,\n\t\t\t\tgrp->base, grp->map, grp->used, cnt);\n}\n\n \nstatic int kern_alloc_tids(struct tid_rdma_flow *flow)\n{\n\tstruct hfi1_ctxtdata *rcd = flow->req->rcd;\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu32 ngroups, pageidx = 0;\n\tstruct tid_group *group = NULL, *used;\n\tu8 use;\n\n\tflow->tnode_cnt = 0;\n\tngroups = flow->npagesets / dd->rcv_entries.group_size;\n\tif (!ngroups)\n\t\tgoto used_list;\n\n\t \n\tlist_for_each_entry(group,  &rcd->tid_group_list.list, list) {\n\t\tkern_add_tid_node(flow, rcd, \"complete groups\", group,\n\t\t\t\t  group->size);\n\n\t\tpageidx += group->size;\n\t\tif (!--ngroups)\n\t\t\tbreak;\n\t}\n\n\tif (pageidx >= flow->npagesets)\n\t\tgoto ok;\n\nused_list:\n\t \n\tlist_for_each_entry(used, &rcd->tid_used_list.list, list) {\n\t\tuse = min_t(u32, flow->npagesets - pageidx,\n\t\t\t    used->size - used->used);\n\t\tkern_add_tid_node(flow, rcd, \"used groups\", used, use);\n\n\t\tpageidx += use;\n\t\tif (pageidx >= flow->npagesets)\n\t\t\tgoto ok;\n\t}\n\n\t \n\tif (group && &group->list == &rcd->tid_group_list.list)\n\t\tgoto bail_eagain;\n\tgroup = list_prepare_entry(group, &rcd->tid_group_list.list,\n\t\t\t\t   list);\n\tif (list_is_last(&group->list, &rcd->tid_group_list.list))\n\t\tgoto bail_eagain;\n\tgroup = list_next_entry(group, list);\n\tuse = min_t(u32, flow->npagesets - pageidx, group->size);\n\tkern_add_tid_node(flow, rcd, \"complete continue\", group, use);\n\tpageidx += use;\n\tif (pageidx >= flow->npagesets)\n\t\tgoto ok;\nbail_eagain:\n\ttrace_hfi1_msg_alloc_tids(flow->req->qp, \" insufficient tids: needed \",\n\t\t\t\t  (u64)flow->npagesets);\n\treturn -EAGAIN;\nok:\n\treturn 0;\n}\n\nstatic void kern_program_rcv_group(struct tid_rdma_flow *flow, int grp_num,\n\t\t\t\t   u32 *pset_idx)\n{\n\tstruct hfi1_ctxtdata *rcd = flow->req->rcd;\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tstruct kern_tid_node *node = &flow->tnode[grp_num];\n\tstruct tid_group *grp = node->grp;\n\tstruct tid_rdma_pageset *pset;\n\tu32 pmtu_pg = flow->req->qp->pmtu >> PAGE_SHIFT;\n\tu32 rcventry, npages = 0, pair = 0, tidctrl;\n\tu8 i, cnt = 0;\n\n\tfor (i = 0; i < grp->size; i++) {\n\t\trcventry = grp->base + i;\n\n\t\tif (node->map & BIT(i) || cnt >= node->cnt) {\n\t\t\trcv_array_wc_fill(dd, rcventry);\n\t\t\tcontinue;\n\t\t}\n\t\tpset = &flow->pagesets[(*pset_idx)++];\n\t\tif (pset->count) {\n\t\t\thfi1_put_tid(dd, rcventry, PT_EXPECTED,\n\t\t\t\t     pset->addr, trdma_pset_order(pset));\n\t\t} else {\n\t\t\thfi1_put_tid(dd, rcventry, PT_INVALID, 0, 0);\n\t\t}\n\t\tnpages += pset->count;\n\n\t\trcventry -= rcd->expected_base;\n\t\ttidctrl = pair ? 0x3 : rcventry & 0x1 ? 0x2 : 0x1;\n\t\t \n\t\tpair = !(i & 0x1) && !((node->map >> i) & 0x3) &&\n\t\t\tnode->cnt >= cnt + 2;\n\t\tif (!pair) {\n\t\t\tif (!pset->count)\n\t\t\t\ttidctrl = 0x1;\n\t\t\tflow->tid_entry[flow->tidcnt++] =\n\t\t\t\tEXP_TID_SET(IDX, rcventry >> 1) |\n\t\t\t\tEXP_TID_SET(CTRL, tidctrl) |\n\t\t\t\tEXP_TID_SET(LEN, npages);\n\t\t\ttrace_hfi1_tid_entry_alloc( \n\t\t\t   flow->req->qp, flow->tidcnt - 1,\n\t\t\t   flow->tid_entry[flow->tidcnt - 1]);\n\n\t\t\t \n\t\t\tflow->npkts += (npages + pmtu_pg - 1) >> ilog2(pmtu_pg);\n\t\t\tnpages = 0;\n\t\t}\n\n\t\tif (grp->used == grp->size - 1)\n\t\t\ttid_group_move(grp, &rcd->tid_used_list,\n\t\t\t\t       &rcd->tid_full_list);\n\t\telse if (!grp->used)\n\t\t\ttid_group_move(grp, &rcd->tid_group_list,\n\t\t\t\t       &rcd->tid_used_list);\n\n\t\tgrp->used++;\n\t\tgrp->map |= BIT(i);\n\t\tcnt++;\n\t}\n}\n\nstatic void kern_unprogram_rcv_group(struct tid_rdma_flow *flow, int grp_num)\n{\n\tstruct hfi1_ctxtdata *rcd = flow->req->rcd;\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tstruct kern_tid_node *node = &flow->tnode[grp_num];\n\tstruct tid_group *grp = node->grp;\n\tu32 rcventry;\n\tu8 i, cnt = 0;\n\n\tfor (i = 0; i < grp->size; i++) {\n\t\trcventry = grp->base + i;\n\n\t\tif (node->map & BIT(i) || cnt >= node->cnt) {\n\t\t\trcv_array_wc_fill(dd, rcventry);\n\t\t\tcontinue;\n\t\t}\n\n\t\thfi1_put_tid(dd, rcventry, PT_INVALID, 0, 0);\n\n\t\tgrp->used--;\n\t\tgrp->map &= ~BIT(i);\n\t\tcnt++;\n\n\t\tif (grp->used == grp->size - 1)\n\t\t\ttid_group_move(grp, &rcd->tid_full_list,\n\t\t\t\t       &rcd->tid_used_list);\n\t\telse if (!grp->used)\n\t\t\ttid_group_move(grp, &rcd->tid_used_list,\n\t\t\t\t       &rcd->tid_group_list);\n\t}\n\tif (WARN_ON_ONCE(cnt & 1)) {\n\t\tstruct hfi1_ctxtdata *rcd = flow->req->rcd;\n\t\tstruct hfi1_devdata *dd = rcd->dd;\n\n\t\tdd_dev_err(dd, \"unexpected odd free cnt %u map 0x%x used %u\",\n\t\t\t   cnt, grp->map, grp->used);\n\t}\n}\n\nstatic void kern_program_rcvarray(struct tid_rdma_flow *flow)\n{\n\tu32 pset_idx = 0;\n\tint i;\n\n\tflow->npkts = 0;\n\tflow->tidcnt = 0;\n\tfor (i = 0; i < flow->tnode_cnt; i++)\n\t\tkern_program_rcv_group(flow, i, &pset_idx);\n\ttrace_hfi1_tid_flow_alloc(flow->req->qp, flow->req->setup_head, flow);\n}\n\n \nint hfi1_kern_exp_rcv_setup(struct tid_rdma_request *req,\n\t\t\t    struct rvt_sge_state *ss, bool *last)\n\t__must_hold(&req->qp->s_lock)\n{\n\tstruct tid_rdma_flow *flow = &req->flows[req->setup_head];\n\tstruct hfi1_ctxtdata *rcd = req->rcd;\n\tstruct hfi1_qp_priv *qpriv = req->qp->priv;\n\tunsigned long flags;\n\tstruct rvt_qp *fqp;\n\tu16 clear_tail = req->clear_tail;\n\n\tlockdep_assert_held(&req->qp->s_lock);\n\t \n\tif (!CIRC_SPACE(req->setup_head, clear_tail, MAX_FLOWS) ||\n\t    CIRC_CNT(req->setup_head, clear_tail, MAX_FLOWS) >=\n\t    req->n_flows)\n\t\treturn -EINVAL;\n\n\t \n\tif (kern_get_phys_blocks(flow, qpriv->pages, ss, last)) {\n\t\thfi1_wait_kmem(flow->req->qp);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock_irqsave(&rcd->exp_lock, flags);\n\tif (kernel_tid_waiters(rcd, &rcd->rarr_queue, flow->req->qp))\n\t\tgoto queue;\n\n\t \n\tif (kern_alloc_tids(flow))\n\t\tgoto queue;\n\t \n\tkern_program_rcvarray(flow);\n\n\t \n\tmemset(&flow->flow_state, 0x0, sizeof(flow->flow_state));\n\tflow->idx = qpriv->flow_state.index;\n\tflow->flow_state.generation = qpriv->flow_state.generation;\n\tflow->flow_state.spsn = qpriv->flow_state.psn;\n\tflow->flow_state.lpsn = flow->flow_state.spsn + flow->npkts - 1;\n\tflow->flow_state.r_next_psn =\n\t\tfull_flow_psn(flow, flow->flow_state.spsn);\n\tqpriv->flow_state.psn += flow->npkts;\n\n\tdequeue_tid_waiter(rcd, &rcd->rarr_queue, flow->req->qp);\n\t \n\tfqp = first_qp(rcd, &rcd->rarr_queue);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\ttid_rdma_schedule_tid_wakeup(fqp);\n\n\treq->setup_head = (req->setup_head + 1) & (MAX_FLOWS - 1);\n\treturn 0;\nqueue:\n\tqueue_qp_for_tid_wait(rcd, &rcd->rarr_queue, flow->req->qp);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\treturn -EAGAIN;\n}\n\nstatic void hfi1_tid_rdma_reset_flow(struct tid_rdma_flow *flow)\n{\n\tflow->npagesets = 0;\n}\n\n \nint hfi1_kern_exp_rcv_clear(struct tid_rdma_request *req)\n\t__must_hold(&req->qp->s_lock)\n{\n\tstruct tid_rdma_flow *flow = &req->flows[req->clear_tail];\n\tstruct hfi1_ctxtdata *rcd = req->rcd;\n\tunsigned long flags;\n\tint i;\n\tstruct rvt_qp *fqp;\n\n\tlockdep_assert_held(&req->qp->s_lock);\n\t \n\tif (!CIRC_CNT(req->setup_head, req->clear_tail, MAX_FLOWS))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&rcd->exp_lock, flags);\n\n\tfor (i = 0; i < flow->tnode_cnt; i++)\n\t\tkern_unprogram_rcv_group(flow, i);\n\t \n\tflow->tnode_cnt = 0;\n\t \n\tfqp = first_qp(rcd, &rcd->rarr_queue);\n\tspin_unlock_irqrestore(&rcd->exp_lock, flags);\n\n\tdma_unmap_flow(flow);\n\n\thfi1_tid_rdma_reset_flow(flow);\n\treq->clear_tail = (req->clear_tail + 1) & (MAX_FLOWS - 1);\n\n\tif (fqp == req->qp) {\n\t\t__trigger_tid_waiter(fqp);\n\t\trvt_put_qp(fqp);\n\t} else {\n\t\ttid_rdma_schedule_tid_wakeup(fqp);\n\t}\n\n\treturn 0;\n}\n\n \nvoid hfi1_kern_exp_rcv_clear_all(struct tid_rdma_request *req)\n\t__must_hold(&req->qp->s_lock)\n{\n\t \n\twhile (CIRC_CNT(req->setup_head, req->clear_tail, MAX_FLOWS)) {\n\t\tif (hfi1_kern_exp_rcv_clear(req))\n\t\t\tbreak;\n\t}\n}\n\n \nstatic void hfi1_kern_exp_rcv_free_flows(struct tid_rdma_request *req)\n{\n\tkfree(req->flows);\n\treq->flows = NULL;\n}\n\n \nvoid __trdma_clean_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tstruct hfi1_swqe_priv *p = wqe->priv;\n\n\thfi1_kern_exp_rcv_free_flows(&p->tid_req);\n}\n\n \nstatic int hfi1_kern_exp_rcv_alloc_flows(struct tid_rdma_request *req,\n\t\t\t\t\t gfp_t gfp)\n{\n\tstruct tid_rdma_flow *flows;\n\tint i;\n\n\tif (likely(req->flows))\n\t\treturn 0;\n\tflows = kmalloc_node(MAX_FLOWS * sizeof(*flows), gfp,\n\t\t\t     req->rcd->numa_id);\n\tif (!flows)\n\t\treturn -ENOMEM;\n\t \n\tfor (i = 0; i < MAX_FLOWS; i++) {\n\t\tflows[i].req = req;\n\t\tflows[i].npagesets = 0;\n\t\tflows[i].pagesets[0].mapped =  0;\n\t\tflows[i].resync_npkts = 0;\n\t}\n\treq->flows = flows;\n\treturn 0;\n}\n\nstatic void hfi1_init_trdma_req(struct rvt_qp *qp,\n\t\t\t\tstruct tid_rdma_request *req)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\n\t \n\treq->qp = qp;\n\treq->rcd = qpriv->rcd;\n}\n\nu64 hfi1_access_sw_tid_wait(const struct cntr_entry *entry,\n\t\t\t    void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn dd->verbs_dev.n_tidwait;\n}\n\nstatic struct tid_rdma_flow *find_flow_ib(struct tid_rdma_request *req,\n\t\t\t\t\t  u32 psn, u16 *fidx)\n{\n\tu16 head, tail;\n\tstruct tid_rdma_flow *flow;\n\n\thead = req->setup_head;\n\ttail = req->clear_tail;\n\tfor ( ; CIRC_CNT(head, tail, MAX_FLOWS);\n\t     tail = CIRC_NEXT(tail, MAX_FLOWS)) {\n\t\tflow = &req->flows[tail];\n\t\tif (cmp_psn(psn, flow->flow_state.ib_spsn) >= 0 &&\n\t\t    cmp_psn(psn, flow->flow_state.ib_lpsn) <= 0) {\n\t\t\tif (fidx)\n\t\t\t\t*fidx = tail;\n\t\t\treturn flow;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nu32 hfi1_build_tid_rdma_read_packet(struct rvt_swqe *wqe,\n\t\t\t\t    struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t\t    u32 *bth2, u32 *len)\n{\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_flow *flow = &req->flows[req->flow_idx];\n\tstruct rvt_qp *qp = req->qp;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct hfi1_swqe_priv *wpriv = wqe->priv;\n\tstruct tid_rdma_read_req *rreq = &ohdr->u.tid_rdma.r_req;\n\tstruct tid_rdma_params *remote;\n\tu32 req_len = 0;\n\tvoid *req_addr = NULL;\n\n\t \n\t*bth2 = mask_psn(flow->flow_state.ib_spsn + flow->pkt);\n\ttrace_hfi1_tid_flow_build_read_pkt(qp, req->flow_idx, flow);\n\n\t \n\treq_addr = &flow->tid_entry[flow->tid_idx];\n\treq_len = sizeof(*flow->tid_entry) *\n\t\t\t(flow->tidcnt - flow->tid_idx);\n\n\tmemset(&ohdr->u.tid_rdma.r_req, 0, sizeof(ohdr->u.tid_rdma.r_req));\n\twpriv->ss.sge.vaddr = req_addr;\n\twpriv->ss.sge.sge_length = req_len;\n\twpriv->ss.sge.length = wpriv->ss.sge.sge_length;\n\t \n\twpriv->ss.sge.mr = NULL;\n\twpriv->ss.sge.m = 0;\n\twpriv->ss.sge.n = 0;\n\n\twpriv->ss.sg_list = NULL;\n\twpriv->ss.total_len = wpriv->ss.sge.sge_length;\n\twpriv->ss.num_sge = 1;\n\n\t \n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\n\tKDETH_RESET(rreq->kdeth0, KVER, 0x1);\n\tKDETH_RESET(rreq->kdeth1, JKEY, remote->jkey);\n\trreq->reth.vaddr = cpu_to_be64(wqe->rdma_wr.remote_addr +\n\t\t\t   req->cur_seg * req->seg_len + flow->sent);\n\trreq->reth.rkey = cpu_to_be32(wqe->rdma_wr.rkey);\n\trreq->reth.length = cpu_to_be32(*len);\n\trreq->tid_flow_psn =\n\t\tcpu_to_be32((flow->flow_state.generation <<\n\t\t\t     HFI1_KDETH_BTH_SEQ_SHIFT) |\n\t\t\t    ((flow->flow_state.spsn + flow->pkt) &\n\t\t\t     HFI1_KDETH_BTH_SEQ_MASK));\n\trreq->tid_flow_qp =\n\t\tcpu_to_be32(qpriv->tid_rdma.local.qp |\n\t\t\t    ((flow->idx & TID_RDMA_DESTQP_FLOW_MASK) <<\n\t\t\t     TID_RDMA_DESTQP_FLOW_SHIFT) |\n\t\t\t    qpriv->rcd->ctxt);\n\trreq->verbs_qp = cpu_to_be32(qp->remote_qpn);\n\t*bth1 &= ~RVT_QPN_MASK;\n\t*bth1 |= remote->qp;\n\t*bth2 |= IB_BTH_REQ_ACK;\n\trcu_read_unlock();\n\n\t \n\tflow->sent += *len;\n\treq->cur_seg++;\n\tqp->s_state = TID_OP(READ_REQ);\n\treq->ack_pending++;\n\treq->flow_idx = (req->flow_idx + 1) & (MAX_FLOWS - 1);\n\tqpriv->pending_tid_r_segs++;\n\tqp->s_num_rd_atomic++;\n\n\t \n\t*len = req_len;\n\n\treturn sizeof(ohdr->u.tid_rdma.r_req) / sizeof(u32);\n}\n\n \nu32 hfi1_build_tid_rdma_read_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t\t struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t\t u32 *bth2, u32 *len)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_flow *flow = NULL;\n\tu32 hdwords = 0;\n\tbool last;\n\tbool retry = true;\n\tu32 npkts = rvt_div_round_up_mtu(qp, *len);\n\n\ttrace_hfi1_tid_req_build_read_req(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t\t  wqe->lpsn, req);\n\t \nsync_check:\n\tif (req->state == TID_REQUEST_SYNC) {\n\t\tif (qpriv->pending_tid_r_segs)\n\t\t\tgoto done;\n\n\t\thfi1_kern_clear_hw_flow(req->rcd, qp);\n\t\tqpriv->s_flags &= ~HFI1_R_TID_SW_PSN;\n\t\treq->state = TID_REQUEST_ACTIVE;\n\t}\n\n\t \n\tif (req->flow_idx == req->setup_head) {\n\t\tretry = false;\n\t\tif (req->state == TID_REQUEST_RESEND) {\n\t\t\t \n\t\t\trestart_sge(&qp->s_sge, wqe, req->s_next_psn,\n\t\t\t\t    qp->pmtu);\n\t\t\treq->isge = 0;\n\t\t\treq->state = TID_REQUEST_ACTIVE;\n\t\t}\n\n\t\t \n\t\tif ((qpriv->flow_state.psn + npkts) > MAX_TID_FLOW_PSN - 1) {\n\t\t\treq->state = TID_REQUEST_SYNC;\n\t\t\tgoto sync_check;\n\t\t}\n\n\t\t \n\t\tif (hfi1_kern_setup_hw_flow(qpriv->rcd, qp))\n\t\t\tgoto done;\n\n\t\t \n\t\tif (hfi1_kern_exp_rcv_setup(req, &qp->s_sge, &last)) {\n\t\t\treq->state = TID_REQUEST_QUEUED;\n\n\t\t\t \n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tflow = &req->flows[req->flow_idx];\n\tflow->pkt = 0;\n\tflow->tid_idx = 0;\n\tflow->sent = 0;\n\tif (!retry) {\n\t\t \n\t\tflow->flow_state.ib_spsn = req->s_next_psn;\n\t\tflow->flow_state.ib_lpsn =\n\t\t\tflow->flow_state.ib_spsn + flow->npkts - 1;\n\t}\n\n\t \n\treq->s_next_psn += flow->npkts;\n\n\t \n\thdwords = hfi1_build_tid_rdma_read_packet(wqe, ohdr, bth1, bth2, len);\ndone:\n\treturn hdwords;\n}\n\n \nstatic int tid_rdma_rcv_read_request(struct rvt_qp *qp,\n\t\t\t\t     struct rvt_ack_entry *e,\n\t\t\t\t     struct hfi1_packet *packet,\n\t\t\t\t     struct ib_other_headers *ohdr,\n\t\t\t\t     u32 bth0, u32 psn, u64 vaddr, u32 len)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tu32 flow_psn, i, tidlen = 0, pktlen, tlen;\n\n\treq = ack_to_tid_req(e);\n\n\t \n\tflow = &req->flows[req->setup_head];\n\n\t \n\tpktlen = packet->tlen - (packet->hlen + 4);\n\tif (pktlen > sizeof(flow->tid_entry))\n\t\treturn 1;\n\tmemcpy(flow->tid_entry, packet->ebuf, pktlen);\n\tflow->tidcnt = pktlen / sizeof(*flow->tid_entry);\n\n\t \n\tflow->npkts = rvt_div_round_up_mtu(qp, len);\n\tfor (i = 0; i < flow->tidcnt; i++) {\n\t\ttrace_hfi1_tid_entry_rcv_read_req(qp, i,\n\t\t\t\t\t\t  flow->tid_entry[i]);\n\t\ttlen = EXP_TID_GET(flow->tid_entry[i], LEN);\n\t\tif (!tlen)\n\t\t\treturn 1;\n\n\t\t \n\t\ttidlen += tlen;\n\t}\n\tif (tidlen * PAGE_SIZE < len)\n\t\treturn 1;\n\n\t \n\treq->clear_tail = req->setup_head;\n\tflow->pkt = 0;\n\tflow->tid_idx = 0;\n\tflow->tid_offset = 0;\n\tflow->sent = 0;\n\tflow->tid_qpn = be32_to_cpu(ohdr->u.tid_rdma.r_req.tid_flow_qp);\n\tflow->idx = (flow->tid_qpn >> TID_RDMA_DESTQP_FLOW_SHIFT) &\n\t\t    TID_RDMA_DESTQP_FLOW_MASK;\n\tflow_psn = mask_psn(be32_to_cpu(ohdr->u.tid_rdma.r_req.tid_flow_psn));\n\tflow->flow_state.generation = flow_psn >> HFI1_KDETH_BTH_SEQ_SHIFT;\n\tflow->flow_state.spsn = flow_psn & HFI1_KDETH_BTH_SEQ_MASK;\n\tflow->length = len;\n\n\tflow->flow_state.lpsn = flow->flow_state.spsn +\n\t\tflow->npkts - 1;\n\tflow->flow_state.ib_spsn = psn;\n\tflow->flow_state.ib_lpsn = flow->flow_state.ib_spsn + flow->npkts - 1;\n\n\ttrace_hfi1_tid_flow_rcv_read_req(qp, req->setup_head, flow);\n\t \n\treq->flow_idx = req->setup_head;\n\n\t \n\treq->setup_head = (req->setup_head + 1) & (MAX_FLOWS - 1);\n\n\t \n\te->opcode = (bth0 >> 24) & 0xff;\n\te->psn = psn;\n\te->lpsn = psn + flow->npkts - 1;\n\te->sent = 0;\n\n\treq->n_flows = qpriv->tid_rdma.local.max_read;\n\treq->state = TID_REQUEST_ACTIVE;\n\treq->cur_seg = 0;\n\treq->comp_seg = 0;\n\treq->ack_seg = 0;\n\treq->isge = 0;\n\treq->seg_len = qpriv->tid_rdma.local.max_len;\n\treq->total_len = len;\n\treq->total_segs = 1;\n\treq->r_flow_psn = e->psn;\n\n\ttrace_hfi1_tid_req_rcv_read_req(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\treq);\n\treturn 0;\n}\n\nstatic int tid_rdma_rcv_error(struct hfi1_packet *packet,\n\t\t\t      struct ib_other_headers *ohdr,\n\t\t\t      struct rvt_qp *qp, u32 psn, int diff)\n{\n\tstruct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tstruct hfi1_ctxtdata *rcd = ((struct hfi1_qp_priv *)qp->priv)->rcd;\n\tstruct hfi1_ibdev *dev = to_idev(qp->ibqp.device);\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct rvt_ack_entry *e;\n\tstruct tid_rdma_request *req;\n\tunsigned long flags;\n\tu8 prev;\n\tbool old_req;\n\n\ttrace_hfi1_rsp_tid_rcv_error(qp, psn);\n\ttrace_hfi1_tid_rdma_rcv_err(qp, 0, psn, diff);\n\tif (diff > 0) {\n\t\t \n\t\tif (!qp->r_nak_state) {\n\t\t\tibp->rvp.n_rc_seqnak++;\n\t\t\tqp->r_nak_state = IB_NAK_PSN_ERROR;\n\t\t\tqp->r_ack_psn = qp->r_psn;\n\t\t\trc_defered_ack(rcd, qp);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tibp->rvp.n_rc_dupreq++;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\te = find_prev_entry(qp, psn, &prev, NULL, &old_req);\n\tif (!e || (e->opcode != TID_OP(READ_REQ) &&\n\t\t   e->opcode != TID_OP(WRITE_REQ)))\n\t\tgoto unlock;\n\n\treq = ack_to_tid_req(e);\n\treq->r_flow_psn = psn;\n\ttrace_hfi1_tid_req_rcv_err(qp, 0, e->opcode, e->psn, e->lpsn, req);\n\tif (e->opcode == TID_OP(READ_REQ)) {\n\t\tstruct ib_reth *reth;\n\t\tu32 len;\n\t\tu32 rkey;\n\t\tu64 vaddr;\n\t\tint ok;\n\t\tu32 bth0;\n\n\t\treth = &ohdr->u.tid_rdma.r_req.reth;\n\t\t \n\t\tlen = be32_to_cpu(reth->length);\n\t\tif (psn != e->psn || len != req->total_len)\n\t\t\tgoto unlock;\n\n\t\trelease_rdma_sge_mr(e);\n\n\t\trkey = be32_to_cpu(reth->rkey);\n\t\tvaddr = get_ib_reth_vaddr(reth);\n\n\t\tqp->r_len = len;\n\t\tok = rvt_rkey_ok(qp, &e->rdma_sge, len, vaddr, rkey,\n\t\t\t\t IB_ACCESS_REMOTE_READ);\n\t\tif (unlikely(!ok))\n\t\t\tgoto unlock;\n\n\t\t \n\t\tbth0 = be32_to_cpu(ohdr->bth[0]);\n\t\tif (tid_rdma_rcv_read_request(qp, e, packet, ohdr, bth0, psn,\n\t\t\t\t\t      vaddr, len))\n\t\t\tgoto unlock;\n\n\t\t \n\t\tif (old_req)\n\t\t\tgoto unlock;\n\t} else {\n\t\tstruct flow_state *fstate;\n\t\tbool schedule = false;\n\t\tu8 i;\n\n\t\tif (req->state == TID_REQUEST_RESEND) {\n\t\t\treq->state = TID_REQUEST_RESEND_ACTIVE;\n\t\t} else if (req->state == TID_REQUEST_INIT_RESEND) {\n\t\t\treq->state = TID_REQUEST_INIT;\n\t\t\tschedule = true;\n\t\t}\n\n\t\t \n\t\tif (old_req || req->state == TID_REQUEST_INIT ||\n\t\t    (req->state == TID_REQUEST_SYNC && !req->cur_seg)) {\n\t\t\tfor (i = prev + 1; ; i++) {\n\t\t\t\tif (i > rvt_size_atomic(&dev->rdi))\n\t\t\t\t\ti = 0;\n\t\t\t\tif (i == qp->r_head_ack_queue)\n\t\t\t\t\tbreak;\n\t\t\t\te = &qp->s_ack_queue[i];\n\t\t\t\treq = ack_to_tid_req(e);\n\t\t\t\tif (e->opcode == TID_OP(WRITE_REQ) &&\n\t\t\t\t    req->state == TID_REQUEST_INIT)\n\t\t\t\t\treq->state = TID_REQUEST_INIT_RESEND;\n\t\t\t}\n\t\t\t \n\t\t\tif (!schedule)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\t \n\t\tif (req->clear_tail == req->setup_head)\n\t\t\tgoto schedule;\n\t\t \n\t\tif (CIRC_CNT(req->flow_idx, req->clear_tail, MAX_FLOWS)) {\n\t\t\tfstate = &req->flows[req->clear_tail].flow_state;\n\t\t\tqpriv->pending_tid_w_segs -=\n\t\t\t\tCIRC_CNT(req->flow_idx, req->clear_tail,\n\t\t\t\t\t MAX_FLOWS);\n\t\t\treq->flow_idx =\n\t\t\t\tCIRC_ADD(req->clear_tail,\n\t\t\t\t\t delta_psn(psn, fstate->resp_ib_psn),\n\t\t\t\t\t MAX_FLOWS);\n\t\t\tqpriv->pending_tid_w_segs +=\n\t\t\t\tdelta_psn(psn, fstate->resp_ib_psn);\n\t\t\t \n\t\t\tif (CIRC_CNT(req->setup_head, req->flow_idx,\n\t\t\t\t     MAX_FLOWS)) {\n\t\t\t\treq->cur_seg = delta_psn(psn, e->psn);\n\t\t\t\treq->state = TID_REQUEST_RESEND_ACTIVE;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = prev + 1; ; i++) {\n\t\t\t \n\t\t\tif (i > rvt_size_atomic(&dev->rdi))\n\t\t\t\ti = 0;\n\t\t\tif (i == qp->r_head_ack_queue)\n\t\t\t\tbreak;\n\t\t\te = &qp->s_ack_queue[i];\n\t\t\treq = ack_to_tid_req(e);\n\t\t\ttrace_hfi1_tid_req_rcv_err(qp, 0, e->opcode, e->psn,\n\t\t\t\t\t\t   e->lpsn, req);\n\t\t\tif (e->opcode != TID_OP(WRITE_REQ) ||\n\t\t\t    req->cur_seg == req->comp_seg ||\n\t\t\t    req->state == TID_REQUEST_INIT ||\n\t\t\t    req->state == TID_REQUEST_INIT_RESEND) {\n\t\t\t\tif (req->state == TID_REQUEST_INIT)\n\t\t\t\t\treq->state = TID_REQUEST_INIT_RESEND;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tqpriv->pending_tid_w_segs -=\n\t\t\t\tCIRC_CNT(req->flow_idx,\n\t\t\t\t\t req->clear_tail,\n\t\t\t\t\t MAX_FLOWS);\n\t\t\treq->flow_idx = req->clear_tail;\n\t\t\treq->state = TID_REQUEST_RESEND;\n\t\t\treq->cur_seg = req->comp_seg;\n\t\t}\n\t\tqpriv->s_flags &= ~HFI1_R_TID_WAIT_INTERLCK;\n\t}\n\t \n\tif (qp->s_acked_ack_queue == qp->s_tail_ack_queue)\n\t\tqp->s_acked_ack_queue = prev;\n\tqp->s_tail_ack_queue = prev;\n\t \n\tqp->s_ack_state = OP(ACKNOWLEDGE);\nschedule:\n\t \n\tif (qpriv->rnr_nak_state) {\n\t\tqp->s_nak_state = 0;\n\t\tqpriv->rnr_nak_state = TID_RNR_NAK_INIT;\n\t\tqp->r_psn = e->lpsn + 1;\n\t\thfi1_tid_write_alloc_resources(qp, true);\n\t}\n\n\tqp->r_state = e->opcode;\n\tqp->r_nak_state = 0;\n\tqp->s_flags |= RVT_S_RESP_PENDING;\n\thfi1_schedule_send(qp);\nunlock:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\ndone:\n\treturn 1;\n}\n\nvoid hfi1_rc_rcv_tid_rdma_read_req(struct hfi1_packet *packet)\n{\n\t \n\n\t \n\tstruct hfi1_ctxtdata *rcd = packet->rcd;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_ack_entry *e;\n\tunsigned long flags;\n\tstruct ib_reth *reth;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tu32 bth0, psn, len, rkey;\n\tbool fecn;\n\tu8 next;\n\tu64 vaddr;\n\tint diff;\n\tu8 nack_state = IB_NAK_INVALID_REQUEST;\n\n\tbth0 = be32_to_cpu(ohdr->bth[0]);\n\tif (hfi1_ruc_check_hdr(ibp, packet))\n\t\treturn;\n\n\tfecn = process_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\ttrace_hfi1_rsp_rcv_tid_read_req(qp, psn);\n\n\tif (qp->state == IB_QPS_RTR && !(qp->r_flags & RVT_R_COMM_EST))\n\t\trvt_comm_est(qp);\n\n\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))\n\t\tgoto nack_inv;\n\n\treth = &ohdr->u.tid_rdma.r_req.reth;\n\tvaddr = be64_to_cpu(reth->vaddr);\n\tlen = be32_to_cpu(reth->length);\n\t \n\tif (!len || len & ~PAGE_MASK || len > qpriv->tid_rdma.local.max_len)\n\t\tgoto nack_inv;\n\n\tdiff = delta_psn(psn, qp->r_psn);\n\tif (unlikely(diff)) {\n\t\ttid_rdma_rcv_err(packet, ohdr, qp, psn, diff, fecn);\n\t\treturn;\n\t}\n\n\t \n\tnext = qp->r_head_ack_queue + 1;\n\tif (next > rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))\n\t\tnext = 0;\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (unlikely(next == qp->s_tail_ack_queue)) {\n\t\tif (!qp->s_ack_queue[next].sent) {\n\t\t\tnack_state = IB_NAK_REMOTE_OPERATIONAL_ERROR;\n\t\t\tgoto nack_inv_unlock;\n\t\t}\n\t\tupdate_ack_queue(qp, next);\n\t}\n\te = &qp->s_ack_queue[qp->r_head_ack_queue];\n\trelease_rdma_sge_mr(e);\n\n\trkey = be32_to_cpu(reth->rkey);\n\tqp->r_len = len;\n\n\tif (unlikely(!rvt_rkey_ok(qp, &e->rdma_sge, qp->r_len, vaddr,\n\t\t\t\t  rkey, IB_ACCESS_REMOTE_READ)))\n\t\tgoto nack_acc;\n\n\t \n\tif (tid_rdma_rcv_read_request(qp, e, packet, ohdr, bth0, psn, vaddr,\n\t\t\t\t      len))\n\t\tgoto nack_inv_unlock;\n\n\tqp->r_state = e->opcode;\n\tqp->r_nak_state = 0;\n\t \n\tqp->r_msn++;\n\tqp->r_psn += e->lpsn - e->psn + 1;\n\n\tqp->r_head_ack_queue = next;\n\n\t \n\tqpriv->r_tid_alloc = qp->r_head_ack_queue;\n\n\t \n\tqp->s_flags |= RVT_S_RESP_PENDING;\n\tif (fecn)\n\t\tqp->s_flags |= RVT_S_ECN;\n\thfi1_schedule_send(qp);\n\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\treturn;\n\nnack_inv_unlock:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\nnack_inv:\n\trvt_rc_error(qp, IB_WC_LOC_QP_OP_ERR);\n\tqp->r_nak_state = nack_state;\n\tqp->r_ack_psn = qp->r_psn;\n\t \n\trc_defered_ack(rcd, qp);\n\treturn;\nnack_acc:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\trvt_rc_error(qp, IB_WC_LOC_PROT_ERR);\n\tqp->r_nak_state = IB_NAK_REMOTE_ACCESS_ERROR;\n\tqp->r_ack_psn = qp->r_psn;\n}\n\nu32 hfi1_build_tid_rdma_read_resp(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t  struct ib_other_headers *ohdr, u32 *bth0,\n\t\t\t\t  u32 *bth1, u32 *bth2, u32 *len, bool *last)\n{\n\tstruct hfi1_ack_priv *epriv = e->priv;\n\tstruct tid_rdma_request *req = &epriv->tid_req;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_flow *flow = &req->flows[req->clear_tail];\n\tu32 tidentry = flow->tid_entry[flow->tid_idx];\n\tu32 tidlen = EXP_TID_GET(tidentry, LEN) << PAGE_SHIFT;\n\tstruct tid_rdma_read_resp *resp = &ohdr->u.tid_rdma.r_rsp;\n\tu32 next_offset, om = KDETH_OM_LARGE;\n\tbool last_pkt;\n\tu32 hdwords = 0;\n\tstruct tid_rdma_params *remote;\n\n\t*len = min_t(u32, qp->pmtu, tidlen - flow->tid_offset);\n\tflow->sent += *len;\n\tnext_offset = flow->tid_offset + *len;\n\tlast_pkt = (flow->sent >= flow->length);\n\n\ttrace_hfi1_tid_entry_build_read_resp(qp, flow->tid_idx, tidentry);\n\ttrace_hfi1_tid_flow_build_read_resp(qp, req->clear_tail, flow);\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\tif (!remote) {\n\t\trcu_read_unlock();\n\t\tgoto done;\n\t}\n\tKDETH_RESET(resp->kdeth0, KVER, 0x1);\n\tKDETH_SET(resp->kdeth0, SH, !last_pkt);\n\tKDETH_SET(resp->kdeth0, INTR, !!(!last_pkt && remote->urg));\n\tKDETH_SET(resp->kdeth0, TIDCTRL, EXP_TID_GET(tidentry, CTRL));\n\tKDETH_SET(resp->kdeth0, TID, EXP_TID_GET(tidentry, IDX));\n\tKDETH_SET(resp->kdeth0, OM, om == KDETH_OM_LARGE);\n\tKDETH_SET(resp->kdeth0, OFFSET, flow->tid_offset / om);\n\tKDETH_RESET(resp->kdeth1, JKEY, remote->jkey);\n\tresp->verbs_qp = cpu_to_be32(qp->remote_qpn);\n\trcu_read_unlock();\n\n\tresp->aeth = rvt_compute_aeth(qp);\n\tresp->verbs_psn = cpu_to_be32(mask_psn(flow->flow_state.ib_spsn +\n\t\t\t\t\t       flow->pkt));\n\n\t*bth0 = TID_OP(READ_RESP) << 24;\n\t*bth1 = flow->tid_qpn;\n\t*bth2 = mask_psn(((flow->flow_state.spsn + flow->pkt++) &\n\t\t\t  HFI1_KDETH_BTH_SEQ_MASK) |\n\t\t\t (flow->flow_state.generation <<\n\t\t\t  HFI1_KDETH_BTH_SEQ_SHIFT));\n\t*last = last_pkt;\n\tif (last_pkt)\n\t\t \n\t\treq->clear_tail = (req->clear_tail + 1) &\n\t\t\t\t  (MAX_FLOWS - 1);\n\n\tif (next_offset >= tidlen) {\n\t\tflow->tid_offset = 0;\n\t\tflow->tid_idx++;\n\t} else {\n\t\tflow->tid_offset = next_offset;\n\t}\n\n\thdwords = sizeof(ohdr->u.tid_rdma.r_rsp) / sizeof(u32);\n\ndone:\n\treturn hdwords;\n}\n\nstatic inline struct tid_rdma_request *\nfind_tid_request(struct rvt_qp *qp, u32 psn, enum ib_wr_opcode opcode)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct rvt_swqe *wqe;\n\tstruct tid_rdma_request *req = NULL;\n\tu32 i, end;\n\n\tend = qp->s_cur + 1;\n\tif (end == qp->s_size)\n\t\tend = 0;\n\tfor (i = qp->s_acked; i != end;) {\n\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\tif (cmp_psn(psn, wqe->psn) >= 0 &&\n\t\t    cmp_psn(psn, wqe->lpsn) <= 0) {\n\t\t\tif (wqe->wr.opcode == opcode)\n\t\t\t\treq = wqe_to_tid_req(wqe);\n\t\t\tbreak;\n\t\t}\n\t\tif (++i == qp->s_size)\n\t\t\ti = 0;\n\t}\n\n\treturn req;\n}\n\nvoid hfi1_rc_rcv_tid_rdma_read_resp(struct hfi1_packet *packet)\n{\n\t \n\n\t \n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct hfi1_ctxtdata *rcd = packet->rcd;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tu32 opcode, aeth;\n\tbool fecn;\n\tunsigned long flags;\n\tu32 kpsn, ipsn;\n\n\ttrace_hfi1_sender_rcv_tid_read_resp(qp);\n\tfecn = process_ecn(qp, packet);\n\tkpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\taeth = be32_to_cpu(ohdr->u.tid_rdma.r_rsp.aeth);\n\topcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0xff;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tipsn = mask_psn(be32_to_cpu(ohdr->u.tid_rdma.r_rsp.verbs_psn));\n\treq = find_tid_request(qp, ipsn, IB_WR_TID_RDMA_READ);\n\tif (unlikely(!req))\n\t\tgoto ack_op_err;\n\n\tflow = &req->flows[req->clear_tail];\n\t \n\tif (cmp_psn(ipsn, flow->flow_state.ib_lpsn)) {\n\t\tupdate_r_next_psn_fecn(packet, priv, rcd, flow, fecn);\n\n\t\tif (cmp_psn(kpsn, flow->flow_state.r_next_psn))\n\t\t\tgoto ack_done;\n\t\tflow->flow_state.r_next_psn = mask_psn(kpsn + 1);\n\t\t \n\t\tif (fecn && packet->etype == RHF_RCV_TYPE_EAGER) {\n\t\t\tstruct rvt_sge_state ss;\n\t\t\tu32 len;\n\t\t\tu32 tlen = packet->tlen;\n\t\t\tu16 hdrsize = packet->hlen;\n\t\t\tu8 pad = packet->pad;\n\t\t\tu8 extra_bytes = pad + packet->extra_byte +\n\t\t\t\t(SIZE_OF_CRC << 2);\n\t\t\tu32 pmtu = qp->pmtu;\n\n\t\t\tif (unlikely(tlen != (hdrsize + pmtu + extra_bytes)))\n\t\t\t\tgoto ack_op_err;\n\t\t\tlen = restart_sge(&ss, req->e.swqe, ipsn, pmtu);\n\t\t\tif (unlikely(len < pmtu))\n\t\t\t\tgoto ack_op_err;\n\t\t\trvt_copy_sge(qp, &ss, packet->payload, pmtu, false,\n\t\t\t\t     false);\n\t\t\t \n\t\t\tpriv->s_flags |= HFI1_R_TID_SW_PSN;\n\t\t}\n\n\t\tgoto ack_done;\n\t}\n\tflow->flow_state.r_next_psn = mask_psn(kpsn + 1);\n\treq->ack_pending--;\n\tpriv->pending_tid_r_segs--;\n\tqp->s_num_rd_atomic--;\n\tif ((qp->s_flags & RVT_S_WAIT_FENCE) &&\n\t    !qp->s_num_rd_atomic) {\n\t\tqp->s_flags &= ~(RVT_S_WAIT_FENCE |\n\t\t\t\t RVT_S_WAIT_ACK);\n\t\thfi1_schedule_send(qp);\n\t}\n\tif (qp->s_flags & RVT_S_WAIT_RDMAR) {\n\t\tqp->s_flags &= ~(RVT_S_WAIT_RDMAR | RVT_S_WAIT_ACK);\n\t\thfi1_schedule_send(qp);\n\t}\n\n\ttrace_hfi1_ack(qp, ipsn);\n\ttrace_hfi1_tid_req_rcv_read_resp(qp, 0, req->e.swqe->wr.opcode,\n\t\t\t\t\t req->e.swqe->psn, req->e.swqe->lpsn,\n\t\t\t\t\t req);\n\ttrace_hfi1_tid_flow_rcv_read_resp(qp, req->clear_tail, flow);\n\n\t \n\thfi1_kern_exp_rcv_clear(req);\n\n\tif (!do_rc_ack(qp, aeth, ipsn, opcode, 0, rcd))\n\t\tgoto ack_done;\n\n\t \n\tif (++req->comp_seg >= req->total_segs) {\n\t\tpriv->tid_r_comp++;\n\t\treq->state = TID_REQUEST_COMPLETE;\n\t}\n\n\t \n\tif ((req->state == TID_REQUEST_SYNC &&\n\t     req->comp_seg == req->cur_seg) ||\n\t    priv->tid_r_comp == priv->tid_r_reqs) {\n\t\thfi1_kern_clear_hw_flow(priv->rcd, qp);\n\t\tpriv->s_flags &= ~HFI1_R_TID_SW_PSN;\n\t\tif (req->state == TID_REQUEST_SYNC)\n\t\t\treq->state = TID_REQUEST_ACTIVE;\n\t}\n\n\thfi1_schedule_send(qp);\n\tgoto ack_done;\n\nack_op_err:\n\t \n\tif (qp->s_last == qp->s_acked)\n\t\trvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);\n\nack_done:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n}\n\nvoid hfi1_kern_read_tid_flow_free(struct rvt_qp *qp)\n\t__must_hold(&qp->s_lock)\n{\n\tu32 n = qp->s_acked;\n\tstruct rvt_swqe *wqe;\n\tstruct tid_rdma_request *req;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tlockdep_assert_held(&qp->s_lock);\n\t \n\twhile (n != qp->s_tail) {\n\t\twqe = rvt_get_swqe_ptr(qp, n);\n\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ) {\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t\thfi1_kern_exp_rcv_clear_all(req);\n\t\t}\n\n\t\tif (++n == qp->s_size)\n\t\t\tn = 0;\n\t}\n\t \n\thfi1_kern_clear_hw_flow(priv->rcd, qp);\n}\n\nstatic bool tid_rdma_tid_err(struct hfi1_packet *packet, u8 rcv_type)\n{\n\tstruct rvt_qp *qp = packet->qp;\n\n\tif (rcv_type >= RHF_RCV_TYPE_IB)\n\t\tgoto done;\n\n\tspin_lock(&qp->s_lock);\n\n\t \n\tif (rcv_type == RHF_RCV_TYPE_EAGER) {\n\t\thfi1_restart_rc(qp, qp->s_last_psn + 1, 1);\n\t\thfi1_schedule_send(qp);\n\t}\n\n\t \n\tspin_unlock(&qp->s_lock);\ndone:\n\treturn true;\n}\n\nstatic void restart_tid_rdma_read_req(struct hfi1_ctxtdata *rcd,\n\t\t\t\t      struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\n\t \n\tqp->r_flags |= RVT_R_RDMAR_SEQ;\n\treq = wqe_to_tid_req(wqe);\n\tflow = &req->flows[req->clear_tail];\n\thfi1_restart_rc(qp, flow->flow_state.ib_spsn, 0);\n\tif (list_empty(&qp->rspwait)) {\n\t\tqp->r_flags |= RVT_R_RSP_SEND;\n\t\trvt_get_qp(qp);\n\t\tlist_add_tail(&qp->rspwait, &rcd->qp_wait_list);\n\t}\n}\n\n \nstatic bool handle_read_kdeth_eflags(struct hfi1_ctxtdata *rcd,\n\t\t\t\t     struct hfi1_packet *packet, u8 rcv_type,\n\t\t\t\t     u8 rte, u32 psn, u32 ibpsn)\n\t__must_hold(&packet->qp->r_lock) __must_hold(RCU)\n{\n\tstruct hfi1_pportdata *ppd = rcd->ppd;\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct hfi1_ibport *ibp;\n\tstruct rvt_swqe *wqe;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tu32 ack_psn;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tbool ret = true;\n\tint diff = 0;\n\tu32 fpsn;\n\n\tlockdep_assert_held(&qp->r_lock);\n\ttrace_hfi1_rsp_read_kdeth_eflags(qp, ibpsn);\n\ttrace_hfi1_sender_read_kdeth_eflags(qp);\n\ttrace_hfi1_tid_read_sender_kdeth_eflags(qp, 0);\n\tspin_lock(&qp->s_lock);\n\t \n\tif (cmp_psn(ibpsn, qp->s_last_psn) < 0 ||\n\t    cmp_psn(ibpsn, qp->s_psn) > 0)\n\t\tgoto s_unlock;\n\n\t \n\tack_psn = ibpsn - 1;\n\twqe = rvt_get_swqe_ptr(qp, qp->s_acked);\n\tibp = to_iport(qp->ibqp.device, qp->port_num);\n\n\t \n\twhile ((int)delta_psn(ack_psn, wqe->lpsn) >= 0) {\n\t\t \n\t\tif (wqe->wr.opcode == IB_WR_RDMA_READ ||\n\t\t    wqe->wr.opcode == IB_WR_TID_RDMA_READ ||\n\t\t    wqe->wr.opcode == IB_WR_ATOMIC_CMP_AND_SWP ||\n\t\t    wqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) {\n\t\t\t \n\t\t\tif (!(qp->r_flags & RVT_R_RDMAR_SEQ)) {\n\t\t\t\tqp->r_flags |= RVT_R_RDMAR_SEQ;\n\t\t\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ) {\n\t\t\t\t\trestart_tid_rdma_read_req(rcd, qp,\n\t\t\t\t\t\t\t\t  wqe);\n\t\t\t\t} else {\n\t\t\t\t\thfi1_restart_rc(qp, qp->s_last_psn + 1,\n\t\t\t\t\t\t\t0);\n\t\t\t\t\tif (list_empty(&qp->rspwait)) {\n\t\t\t\t\t\tqp->r_flags |= RVT_R_RSP_SEND;\n\t\t\t\t\t\trvt_get_qp(qp);\n\t\t\t\t\t\tlist_add_tail( \n\t\t\t\t\t\t   &qp->rspwait,\n\t\t\t\t\t\t   &rcd->qp_wait_list);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\twqe = do_rc_completion(qp, wqe, ibp);\n\t\tif (qp->s_acked == qp->s_tail)\n\t\t\tgoto s_unlock;\n\t}\n\n\tif (qp->s_acked == qp->s_tail)\n\t\tgoto s_unlock;\n\n\t \n\tif (wqe->wr.opcode != IB_WR_TID_RDMA_READ)\n\t\tgoto s_unlock;\n\n\treq = wqe_to_tid_req(wqe);\n\ttrace_hfi1_tid_req_read_kdeth_eflags(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t\t     wqe->lpsn, req);\n\tswitch (rcv_type) {\n\tcase RHF_RCV_TYPE_EXPECTED:\n\t\tswitch (rte) {\n\t\tcase RHF_RTE_EXPECTED_FLOW_SEQ_ERR:\n\t\t\t \n\t\t\tflow = &req->flows[req->clear_tail];\n\t\t\ttrace_hfi1_tid_flow_read_kdeth_eflags(qp,\n\t\t\t\t\t\t\t      req->clear_tail,\n\t\t\t\t\t\t\t      flow);\n\t\t\tif (priv->s_flags & HFI1_R_TID_SW_PSN) {\n\t\t\t\tdiff = cmp_psn(psn,\n\t\t\t\t\t       flow->flow_state.r_next_psn);\n\t\t\t\tif (diff > 0) {\n\t\t\t\t\t \n\t\t\t\t\tgoto s_unlock;\n\t\t\t\t} else if (diff < 0) {\n\t\t\t\t\t \n\t\t\t\t\tif (qp->r_flags & RVT_R_RDMAR_SEQ)\n\t\t\t\t\t\tqp->r_flags &=\n\t\t\t\t\t\t\t~RVT_R_RDMAR_SEQ;\n\n\t\t\t\t\t \n\t\t\t\t\tgoto s_unlock;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tfpsn = full_flow_psn(flow,\n\t\t\t\t\t\t     flow->flow_state.lpsn);\n\t\t\t\tif (cmp_psn(fpsn, psn) == 0) {\n\t\t\t\t\tret = false;\n\t\t\t\t\tif (qp->r_flags & RVT_R_RDMAR_SEQ)\n\t\t\t\t\t\tqp->r_flags &=\n\t\t\t\t\t\t\t~RVT_R_RDMAR_SEQ;\n\t\t\t\t}\n\t\t\t\tflow->flow_state.r_next_psn =\n\t\t\t\t\tmask_psn(psn + 1);\n\t\t\t} else {\n\t\t\t\tu32 last_psn;\n\n\t\t\t\tlast_psn = read_r_next_psn(dd, rcd->ctxt,\n\t\t\t\t\t\t\t   flow->idx);\n\t\t\t\tflow->flow_state.r_next_psn = last_psn;\n\t\t\t\tpriv->s_flags |= HFI1_R_TID_SW_PSN;\n\t\t\t\t \n\t\t\t\tif (!(qp->r_flags & RVT_R_RDMAR_SEQ))\n\t\t\t\t\trestart_tid_rdma_read_req(rcd, qp,\n\t\t\t\t\t\t\t\t  wqe);\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase RHF_RTE_EXPECTED_FLOW_GEN_ERR:\n\t\t\t \n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase RHF_RCV_TYPE_ERROR:\n\t\tswitch (rte) {\n\t\tcase RHF_RTE_ERROR_OP_CODE_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_MIN_LEN_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_HCRC_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_KVER_ERR:\n\t\tcase RHF_RTE_ERROR_CONTEXT_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_TID_ERR:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\ns_unlock:\n\tspin_unlock(&qp->s_lock);\n\treturn ret;\n}\n\nbool hfi1_handle_kdeth_eflags(struct hfi1_ctxtdata *rcd,\n\t\t\t      struct hfi1_pportdata *ppd,\n\t\t\t      struct hfi1_packet *packet)\n{\n\tstruct hfi1_ibport *ibp = &ppd->ibport_data;\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct rvt_dev_info *rdi = &dd->verbs_dev.rdi;\n\tu8 rcv_type = rhf_rcv_type(packet->rhf);\n\tu8 rte = rhf_rcv_type_err(packet->rhf);\n\tstruct ib_header *hdr = packet->hdr;\n\tstruct ib_other_headers *ohdr = NULL;\n\tint lnh = be16_to_cpu(hdr->lrh[0]) & 3;\n\tu16 lid  = be16_to_cpu(hdr->lrh[1]);\n\tu8 opcode;\n\tu32 qp_num, psn, ibpsn;\n\tstruct rvt_qp *qp;\n\tstruct hfi1_qp_priv *qpriv;\n\tunsigned long flags;\n\tbool ret = true;\n\tstruct rvt_ack_entry *e;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tint diff = 0;\n\n\ttrace_hfi1_msg_handle_kdeth_eflags(NULL, \"Kdeth error: rhf \",\n\t\t\t\t\t   packet->rhf);\n\tif (packet->rhf & RHF_ICRC_ERR)\n\t\treturn ret;\n\n\tpacket->ohdr = &hdr->u.oth;\n\tohdr = packet->ohdr;\n\ttrace_input_ibhdr(rcd->dd, packet, !!(rhf_dc_info(packet->rhf)));\n\n\t \n\tqp_num = be32_to_cpu(ohdr->u.tid_rdma.r_rsp.verbs_qp) &\n\t\tRVT_QPN_MASK;\n\tif (lid >= be16_to_cpu(IB_MULTICAST_LID_BASE))\n\t\tgoto drop;\n\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\topcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0xff;\n\n\trcu_read_lock();\n\tqp = rvt_lookup_qpn(rdi, &ibp->rvp, qp_num);\n\tif (!qp)\n\t\tgoto rcu_unlock;\n\n\tpacket->qp = qp;\n\n\t \n\tspin_lock_irqsave(&qp->r_lock, flags);\n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {\n\t\tibp->rvp.n_pkt_drops++;\n\t\tgoto r_unlock;\n\t}\n\n\tif (packet->rhf & RHF_TID_ERR) {\n\t\t \n\t\tu32 tlen = rhf_pkt_len(packet->rhf);  \n\n\t\t \n\t\tif (tlen < 24)\n\t\t\tgoto r_unlock;\n\n\t\t \n\t\tif (lnh == HFI1_LRH_GRH)\n\t\t\tgoto r_unlock;\n\n\t\tif (tid_rdma_tid_err(packet, rcv_type))\n\t\t\tgoto r_unlock;\n\t}\n\n\t \n\tif (opcode == TID_OP(READ_RESP)) {\n\t\tibpsn = be32_to_cpu(ohdr->u.tid_rdma.r_rsp.verbs_psn);\n\t\tibpsn = mask_psn(ibpsn);\n\t\tret = handle_read_kdeth_eflags(rcd, packet, rcv_type, rte, psn,\n\t\t\t\t\t       ibpsn);\n\t\tgoto r_unlock;\n\t}\n\n\t \n\tspin_lock(&qp->s_lock);\n\tqpriv = qp->priv;\n\tif (qpriv->r_tid_tail == HFI1_QP_WQE_INVALID ||\n\t    qpriv->r_tid_tail == qpriv->r_tid_head)\n\t\tgoto unlock;\n\te = &qp->s_ack_queue[qpriv->r_tid_tail];\n\tif (e->opcode != TID_OP(WRITE_REQ))\n\t\tgoto unlock;\n\treq = ack_to_tid_req(e);\n\tif (req->comp_seg == req->cur_seg)\n\t\tgoto unlock;\n\tflow = &req->flows[req->clear_tail];\n\ttrace_hfi1_eflags_err_write(qp, rcv_type, rte, psn);\n\ttrace_hfi1_rsp_handle_kdeth_eflags(qp, psn);\n\ttrace_hfi1_tid_write_rsp_handle_kdeth_eflags(qp);\n\ttrace_hfi1_tid_req_handle_kdeth_eflags(qp, 0, e->opcode, e->psn,\n\t\t\t\t\t       e->lpsn, req);\n\ttrace_hfi1_tid_flow_handle_kdeth_eflags(qp, req->clear_tail, flow);\n\n\tswitch (rcv_type) {\n\tcase RHF_RCV_TYPE_EXPECTED:\n\t\tswitch (rte) {\n\t\tcase RHF_RTE_EXPECTED_FLOW_SEQ_ERR:\n\t\t\tif (!(qpriv->s_flags & HFI1_R_TID_SW_PSN)) {\n\t\t\t\tqpriv->s_flags |= HFI1_R_TID_SW_PSN;\n\t\t\t\tflow->flow_state.r_next_psn =\n\t\t\t\t\tread_r_next_psn(dd, rcd->ctxt,\n\t\t\t\t\t\t\tflow->idx);\n\t\t\t\tqpriv->r_next_psn_kdeth =\n\t\t\t\t\tflow->flow_state.r_next_psn;\n\t\t\t\tgoto nak_psn;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tdiff = cmp_psn(psn,\n\t\t\t\t\t       flow->flow_state.r_next_psn);\n\t\t\t\tif (diff > 0)\n\t\t\t\t\tgoto nak_psn;\n\t\t\t\telse if (diff < 0)\n\t\t\t\t\tbreak;\n\n\t\t\t\tqpriv->s_nak_state = 0;\n\t\t\t\t \n\t\t\t\tif (psn == full_flow_psn(flow,\n\t\t\t\t\t\t\t flow->flow_state.lpsn))\n\t\t\t\t\tret = false;\n\t\t\t\tflow->flow_state.r_next_psn =\n\t\t\t\t\tmask_psn(psn + 1);\n\t\t\t\tqpriv->r_next_psn_kdeth =\n\t\t\t\t\tflow->flow_state.r_next_psn;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase RHF_RTE_EXPECTED_FLOW_GEN_ERR:\n\t\t\tgoto nak_psn;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase RHF_RCV_TYPE_ERROR:\n\t\tswitch (rte) {\n\t\tcase RHF_RTE_ERROR_OP_CODE_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_MIN_LEN_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_HCRC_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_KVER_ERR:\n\t\tcase RHF_RTE_ERROR_CONTEXT_ERR:\n\t\tcase RHF_RTE_ERROR_KHDR_TID_ERR:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock(&qp->s_lock);\nr_unlock:\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\nrcu_unlock:\n\trcu_read_unlock();\ndrop:\n\treturn ret;\nnak_psn:\n\tibp->rvp.n_rc_seqnak++;\n\tif (!qpriv->s_nak_state) {\n\t\tqpriv->s_nak_state = IB_NAK_PSN_ERROR;\n\t\t \n\t\tqpriv->s_nak_psn = mask_psn(flow->flow_state.r_next_psn);\n\t\ttid_rdma_trigger_ack(qp);\n\t}\n\tgoto unlock;\n}\n\n \nvoid hfi1_tid_rdma_restart_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t       u32 *bth2)\n{\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_flow *flow;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tint diff, delta_pkts;\n\tu32 tididx = 0, i;\n\tu16 fidx;\n\n\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ) {\n\t\t*bth2 = mask_psn(qp->s_psn);\n\t\tflow = find_flow_ib(req, *bth2, &fidx);\n\t\tif (!flow) {\n\t\t\ttrace_hfi1_msg_tid_restart_req( \n\t\t\t   qp, \"!!!!!! Could not find flow to restart: bth2 \",\n\t\t\t   (u64)*bth2);\n\t\t\ttrace_hfi1_tid_req_restart_req(qp, 0, wqe->wr.opcode,\n\t\t\t\t\t\t       wqe->psn, wqe->lpsn,\n\t\t\t\t\t\t       req);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tfidx = req->acked_tail;\n\t\tflow = &req->flows[fidx];\n\t\t*bth2 = mask_psn(req->r_ack_psn);\n\t}\n\n\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ)\n\t\tdelta_pkts = delta_psn(*bth2, flow->flow_state.ib_spsn);\n\telse\n\t\tdelta_pkts = delta_psn(*bth2,\n\t\t\t\t       full_flow_psn(flow,\n\t\t\t\t\t\t     flow->flow_state.spsn));\n\n\ttrace_hfi1_tid_flow_restart_req(qp, fidx, flow);\n\tdiff = delta_pkts + flow->resync_npkts;\n\n\tflow->sent = 0;\n\tflow->pkt = 0;\n\tflow->tid_idx = 0;\n\tflow->tid_offset = 0;\n\tif (diff) {\n\t\tfor (tididx = 0; tididx < flow->tidcnt; tididx++) {\n\t\t\tu32 tidentry = flow->tid_entry[tididx], tidlen,\n\t\t\t\ttidnpkts, npkts;\n\n\t\t\tflow->tid_offset = 0;\n\t\t\ttidlen = EXP_TID_GET(tidentry, LEN) * PAGE_SIZE;\n\t\t\ttidnpkts = rvt_div_round_up_mtu(qp, tidlen);\n\t\t\tnpkts = min_t(u32, diff, tidnpkts);\n\t\t\tflow->pkt += npkts;\n\t\t\tflow->sent += (npkts == tidnpkts ? tidlen :\n\t\t\t\t       npkts * qp->pmtu);\n\t\t\tflow->tid_offset += npkts * qp->pmtu;\n\t\t\tdiff -= npkts;\n\t\t\tif (!diff)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (wqe->wr.opcode == IB_WR_TID_RDMA_WRITE) {\n\t\trvt_skip_sge(&qpriv->tid_ss, (req->cur_seg * req->seg_len) +\n\t\t\t     flow->sent, 0);\n\t\t \n\t\tflow->pkt -= flow->resync_npkts;\n\t}\n\n\tif (flow->tid_offset ==\n\t    EXP_TID_GET(flow->tid_entry[tididx], LEN) * PAGE_SIZE) {\n\t\ttididx++;\n\t\tflow->tid_offset = 0;\n\t}\n\tflow->tid_idx = tididx;\n\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ)\n\t\t \n\t\treq->flow_idx = fidx;\n\telse\n\t\treq->clear_tail = fidx;\n\n\ttrace_hfi1_tid_flow_restart_req(qp, fidx, flow);\n\ttrace_hfi1_tid_req_restart_req(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t       wqe->lpsn, req);\n\treq->state = TID_REQUEST_ACTIVE;\n\tif (wqe->wr.opcode == IB_WR_TID_RDMA_WRITE) {\n\t\t \n\t\tfidx = CIRC_NEXT(fidx, MAX_FLOWS);\n\t\ti = qpriv->s_tid_tail;\n\t\tdo {\n\t\t\tfor (; CIRC_CNT(req->setup_head, fidx, MAX_FLOWS);\n\t\t\t      fidx = CIRC_NEXT(fidx, MAX_FLOWS)) {\n\t\t\t\treq->flows[fidx].sent = 0;\n\t\t\t\treq->flows[fidx].pkt = 0;\n\t\t\t\treq->flows[fidx].tid_idx = 0;\n\t\t\t\treq->flows[fidx].tid_offset = 0;\n\t\t\t\treq->flows[fidx].resync_npkts = 0;\n\t\t\t}\n\t\t\tif (i == qpriv->s_tid_cur)\n\t\t\t\tbreak;\n\t\t\tdo {\n\t\t\t\ti = (++i == qp->s_size ? 0 : i);\n\t\t\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\t\t} while (wqe->wr.opcode != IB_WR_TID_RDMA_WRITE);\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t\treq->cur_seg = req->ack_seg;\n\t\t\tfidx = req->acked_tail;\n\t\t\t \n\t\t\treq->clear_tail = fidx;\n\t\t} while (1);\n\t}\n}\n\nvoid hfi1_qp_kern_exp_rcv_clear_all(struct rvt_qp *qp)\n{\n\tint i, ret;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_flow_state *fs;\n\n\tif (qp->ibqp.qp_type != IB_QPT_RC || !HFI1_CAP_IS_KSET(TID_RDMA))\n\t\treturn;\n\n\t \n\tfs = &qpriv->flow_state;\n\tif (fs->index != RXE_NUM_TID_FLOWS)\n\t\thfi1_kern_clear_hw_flow(qpriv->rcd, qp);\n\n\tfor (i = qp->s_acked; i != qp->s_head;) {\n\t\tstruct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, i);\n\n\t\tif (++i == qp->s_size)\n\t\t\ti = 0;\n\t\t \n\t\tif (wqe->wr.opcode != IB_WR_TID_RDMA_READ)\n\t\t\tcontinue;\n\t\tdo {\n\t\t\tstruct hfi1_swqe_priv *priv = wqe->priv;\n\n\t\t\tret = hfi1_kern_exp_rcv_clear(&priv->tid_req);\n\t\t} while (!ret);\n\t}\n\tfor (i = qp->s_acked_ack_queue; i != qp->r_head_ack_queue;) {\n\t\tstruct rvt_ack_entry *e = &qp->s_ack_queue[i];\n\n\t\tif (++i == rvt_max_atomic(ib_to_rvt(qp->ibqp.device)))\n\t\t\ti = 0;\n\t\t \n\t\tif (e->opcode != TID_OP(WRITE_REQ))\n\t\t\tcontinue;\n\t\tdo {\n\t\t\tstruct hfi1_ack_priv *priv = e->priv;\n\n\t\t\tret = hfi1_kern_exp_rcv_clear(&priv->tid_req);\n\t\t} while (!ret);\n\t}\n}\n\nbool hfi1_tid_rdma_wqe_interlock(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tstruct rvt_swqe *prev;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tu32 s_prev;\n\tstruct tid_rdma_request *req;\n\n\ts_prev = (qp->s_cur == 0 ? qp->s_size : qp->s_cur) - 1;\n\tprev = rvt_get_swqe_ptr(qp, s_prev);\n\n\tswitch (wqe->wr.opcode) {\n\tcase IB_WR_SEND:\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_SEND_WITH_INV:\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tswitch (prev->wr.opcode) {\n\t\tcase IB_WR_TID_RDMA_WRITE:\n\t\t\treq = wqe_to_tid_req(prev);\n\t\t\tif (req->ack_seg != req->total_segs)\n\t\t\t\tgoto interlock;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase IB_WR_RDMA_READ:\n\t\tif (prev->wr.opcode != IB_WR_TID_RDMA_WRITE)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase IB_WR_TID_RDMA_READ:\n\t\tswitch (prev->wr.opcode) {\n\t\tcase IB_WR_RDMA_READ:\n\t\t\tif (qp->s_acked != qp->s_cur)\n\t\t\t\tgoto interlock;\n\t\t\tbreak;\n\t\tcase IB_WR_TID_RDMA_WRITE:\n\t\t\treq = wqe_to_tid_req(prev);\n\t\t\tif (req->ack_seg != req->total_segs)\n\t\t\t\tgoto interlock;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n\ninterlock:\n\tpriv->s_flags |= HFI1_S_TID_WAIT_INTERLCK;\n\treturn true;\n}\n\n \nstatic inline bool hfi1_check_sge_align(struct rvt_qp *qp,\n\t\t\t\t\tstruct rvt_sge *sge, int num_sge)\n{\n\tint i;\n\n\tfor (i = 0; i < num_sge; i++, sge++) {\n\t\ttrace_hfi1_sge_check_align(qp, i, sge);\n\t\tif ((u64)sge->vaddr & ~PAGE_MASK ||\n\t\t    sge->sge_length & ~PAGE_MASK)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nvoid setup_tid_rdma_wqe(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tstruct hfi1_qp_priv *qpriv = (struct hfi1_qp_priv *)qp->priv;\n\tstruct hfi1_swqe_priv *priv = wqe->priv;\n\tstruct tid_rdma_params *remote;\n\tenum ib_wr_opcode new_opcode;\n\tbool do_tid_rdma = false;\n\tstruct hfi1_pportdata *ppd = qpriv->rcd->ppd;\n\n\tif ((rdma_ah_get_dlid(&qp->remote_ah_attr) & ~((1 << ppd->lmc) - 1)) ==\n\t\t\t\tppd->lid)\n\t\treturn;\n\tif (qpriv->hdr_type != HFI1_PKT_TYPE_9B)\n\t\treturn;\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\t \n\tif (!remote)\n\t\tgoto exit;\n\n\tif (wqe->wr.opcode == IB_WR_RDMA_READ) {\n\t\tif (hfi1_check_sge_align(qp, &wqe->sg_list[0],\n\t\t\t\t\t wqe->wr.num_sge)) {\n\t\t\tnew_opcode = IB_WR_TID_RDMA_READ;\n\t\t\tdo_tid_rdma = true;\n\t\t}\n\t} else if (wqe->wr.opcode == IB_WR_RDMA_WRITE) {\n\t\t \n\t\tif (!(wqe->rdma_wr.remote_addr & ~PAGE_MASK) &&\n\t\t    !(wqe->length & ~PAGE_MASK)) {\n\t\t\tnew_opcode = IB_WR_TID_RDMA_WRITE;\n\t\t\tdo_tid_rdma = true;\n\t\t}\n\t}\n\n\tif (do_tid_rdma) {\n\t\tif (hfi1_kern_exp_rcv_alloc_flows(&priv->tid_req, GFP_ATOMIC))\n\t\t\tgoto exit;\n\t\twqe->wr.opcode = new_opcode;\n\t\tpriv->tid_req.seg_len =\n\t\t\tmin_t(u32, remote->max_len, wqe->length);\n\t\tpriv->tid_req.total_segs =\n\t\t\tDIV_ROUND_UP(wqe->length, priv->tid_req.seg_len);\n\t\t \n\t\twqe->lpsn = wqe->psn;\n\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_READ) {\n\t\t\tpriv->tid_req.n_flows = remote->max_read;\n\t\t\tqpriv->tid_r_reqs++;\n\t\t\twqe->lpsn += rvt_div_round_up_mtu(qp, wqe->length) - 1;\n\t\t} else {\n\t\t\twqe->lpsn += priv->tid_req.total_segs - 1;\n\t\t\tatomic_inc(&qpriv->n_requests);\n\t\t}\n\n\t\tpriv->tid_req.cur_seg = 0;\n\t\tpriv->tid_req.comp_seg = 0;\n\t\tpriv->tid_req.ack_seg = 0;\n\t\tpriv->tid_req.state = TID_REQUEST_INACTIVE;\n\t\t \n\t\tpriv->tid_req.acked_tail = priv->tid_req.setup_head;\n\t\ttrace_hfi1_tid_req_setup_tid_wqe(qp, 1, wqe->wr.opcode,\n\t\t\t\t\t\t wqe->psn, wqe->lpsn,\n\t\t\t\t\t\t &priv->tid_req);\n\t}\nexit:\n\trcu_read_unlock();\n}\n\n \n\nu32 hfi1_build_tid_rdma_write_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t\t  struct ib_other_headers *ohdr,\n\t\t\t\t  u32 *bth1, u32 *bth2, u32 *len)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_params *remote;\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\t \n\treq->n_flows = remote->max_write;\n\treq->state = TID_REQUEST_ACTIVE;\n\n\tKDETH_RESET(ohdr->u.tid_rdma.w_req.kdeth0, KVER, 0x1);\n\tKDETH_RESET(ohdr->u.tid_rdma.w_req.kdeth1, JKEY, remote->jkey);\n\tohdr->u.tid_rdma.w_req.reth.vaddr =\n\t\tcpu_to_be64(wqe->rdma_wr.remote_addr + (wqe->length - *len));\n\tohdr->u.tid_rdma.w_req.reth.rkey =\n\t\tcpu_to_be32(wqe->rdma_wr.rkey);\n\tohdr->u.tid_rdma.w_req.reth.length = cpu_to_be32(*len);\n\tohdr->u.tid_rdma.w_req.verbs_qp = cpu_to_be32(qp->remote_qpn);\n\t*bth1 &= ~RVT_QPN_MASK;\n\t*bth1 |= remote->qp;\n\tqp->s_state = TID_OP(WRITE_REQ);\n\tqp->s_flags |= HFI1_S_WAIT_TID_RESP;\n\t*bth2 |= IB_BTH_REQ_ACK;\n\t*len = 0;\n\n\trcu_read_unlock();\n\treturn sizeof(ohdr->u.tid_rdma.w_req) / sizeof(u32);\n}\n\nstatic u32 hfi1_compute_tid_rdma_flow_wt(struct rvt_qp *qp)\n{\n\t \n\treturn (MAX_TID_FLOW_PSN * qp->pmtu) >> TID_RDMA_SEGMENT_SHIFT;\n}\n\nstatic u32 position_in_queue(struct hfi1_qp_priv *qpriv,\n\t\t\t     struct tid_queue *queue)\n{\n\treturn qpriv->tid_enqueue - queue->dequeue;\n}\n\n \nstatic u32 hfi1_compute_tid_rnr_timeout(struct rvt_qp *qp, u32 to_seg)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tu64 timeout;\n\tu32 bytes_per_us;\n\tu8 i;\n\n\tbytes_per_us = active_egress_rate(qpriv->rcd->ppd) / 8;\n\ttimeout = (to_seg * TID_RDMA_MAX_SEGMENT_SIZE) / bytes_per_us;\n\t \n\tfor (i = 1; i <= IB_AETH_CREDIT_MASK; i++)\n\t\tif (rvt_rnr_tbl_to_usec(i) >= timeout)\n\t\t\treturn i;\n\treturn 0;\n}\n\n \nstatic void hfi1_tid_write_alloc_resources(struct rvt_qp *qp, bool intr_ctx)\n{\n\tstruct tid_rdma_request *req;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct hfi1_ctxtdata *rcd = qpriv->rcd;\n\tstruct tid_rdma_params *local = &qpriv->tid_rdma.local;\n\tstruct rvt_ack_entry *e;\n\tu32 npkts, to_seg;\n\tbool last;\n\tint ret = 0;\n\n\tlockdep_assert_held(&qp->s_lock);\n\n\twhile (1) {\n\t\ttrace_hfi1_rsp_tid_write_alloc_res(qp, 0);\n\t\ttrace_hfi1_tid_write_rsp_alloc_res(qp);\n\t\t \n\t\tif (qpriv->rnr_nak_state == TID_RNR_NAK_SEND)\n\t\t\tbreak;\n\n\t\t \n\t\tif (qpriv->r_tid_alloc == qpriv->r_tid_head) {\n\t\t\t \n\t\t\tif (qpriv->flow_state.index < RXE_NUM_TID_FLOWS &&\n\t\t\t    !qpriv->alloc_w_segs) {\n\t\t\t\thfi1_kern_clear_hw_flow(rcd, qp);\n\t\t\t\tqpriv->s_flags &= ~HFI1_R_TID_SW_PSN;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\te = &qp->s_ack_queue[qpriv->r_tid_alloc];\n\t\tif (e->opcode != TID_OP(WRITE_REQ))\n\t\t\tgoto next_req;\n\t\treq = ack_to_tid_req(e);\n\t\ttrace_hfi1_tid_req_write_alloc_res(qp, 0, e->opcode, e->psn,\n\t\t\t\t\t\t   e->lpsn, req);\n\t\t \n\t\tif (req->alloc_seg >= req->total_segs)\n\t\t\tgoto next_req;\n\n\t\t \n\t\tif (qpriv->alloc_w_segs >= local->max_write)\n\t\t\tbreak;\n\n\t\t \n\t\tif (qpriv->sync_pt && qpriv->alloc_w_segs)\n\t\t\tbreak;\n\n\t\t \n\t\tif (qpriv->sync_pt && !qpriv->alloc_w_segs) {\n\t\t\thfi1_kern_clear_hw_flow(rcd, qp);\n\t\t\tqpriv->sync_pt = false;\n\t\t\tqpriv->s_flags &= ~HFI1_R_TID_SW_PSN;\n\t\t}\n\n\t\t \n\t\tif (qpriv->flow_state.index >= RXE_NUM_TID_FLOWS) {\n\t\t\tret = hfi1_kern_setup_hw_flow(qpriv->rcd, qp);\n\t\t\tif (ret) {\n\t\t\t\tto_seg = hfi1_compute_tid_rdma_flow_wt(qp) *\n\t\t\t\t\tposition_in_queue(qpriv,\n\t\t\t\t\t\t\t  &rcd->flow_queue);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tnpkts = rvt_div_round_up_mtu(qp, req->seg_len);\n\n\t\t \n\t\tif (qpriv->flow_state.psn + npkts > MAX_TID_FLOW_PSN - 1) {\n\t\t\tqpriv->sync_pt = true;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!CIRC_SPACE(req->setup_head, req->acked_tail,\n\t\t\t\tMAX_FLOWS)) {\n\t\t\tret = -EAGAIN;\n\t\t\tto_seg = MAX_FLOWS >> 1;\n\t\t\ttid_rdma_trigger_ack(qp);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tret = hfi1_kern_exp_rcv_setup(req, &req->ss, &last);\n\t\tif (ret == -EAGAIN)\n\t\t\tto_seg = position_in_queue(qpriv, &rcd->rarr_queue);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tqpriv->alloc_w_segs++;\n\t\treq->alloc_seg++;\n\t\tcontinue;\nnext_req:\n\t\t \n\t\tif (++qpriv->r_tid_alloc >\n\t\t    rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))\n\t\t\tqpriv->r_tid_alloc = 0;\n\t}\n\n\t \n\tif (ret == -EAGAIN && intr_ctx && !qp->r_nak_state)\n\t\tgoto send_rnr_nak;\n\n\treturn;\n\nsend_rnr_nak:\n\tlockdep_assert_held(&qp->r_lock);\n\n\t \n\tqp->r_nak_state = hfi1_compute_tid_rnr_timeout(qp, to_seg) | IB_RNR_NAK;\n\n\t \n\tqp->r_psn = e->psn + req->alloc_seg;\n\tqp->r_ack_psn = qp->r_psn;\n\t \n\tqp->r_head_ack_queue = qpriv->r_tid_alloc + 1;\n\tif (qp->r_head_ack_queue > rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))\n\t\tqp->r_head_ack_queue = 0;\n\tqpriv->r_tid_head = qp->r_head_ack_queue;\n\t \n\tqp->s_nak_state = qp->r_nak_state;\n\tqp->s_ack_psn = qp->r_ack_psn;\n\t \n\tqp->s_flags &= ~(RVT_S_ACK_PENDING);\n\n\ttrace_hfi1_rsp_tid_write_alloc_res(qp, qp->r_psn);\n\t \n\tqpriv->rnr_nak_state = TID_RNR_NAK_SEND;\n\n\t \n\trc_defered_ack(rcd, qp);\n}\n\nvoid hfi1_rc_rcv_tid_rdma_write_req(struct hfi1_packet *packet)\n{\n\t \n\n\t \n\tstruct hfi1_ctxtdata *rcd = packet->rcd;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_ack_entry *e;\n\tunsigned long flags;\n\tstruct ib_reth *reth;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_request *req;\n\tu32 bth0, psn, len, rkey, num_segs;\n\tbool fecn;\n\tu8 next;\n\tu64 vaddr;\n\tint diff;\n\n\tbth0 = be32_to_cpu(ohdr->bth[0]);\n\tif (hfi1_ruc_check_hdr(ibp, packet))\n\t\treturn;\n\n\tfecn = process_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\ttrace_hfi1_rsp_rcv_tid_write_req(qp, psn);\n\n\tif (qp->state == IB_QPS_RTR && !(qp->r_flags & RVT_R_COMM_EST))\n\t\trvt_comm_est(qp);\n\n\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))\n\t\tgoto nack_inv;\n\n\treth = &ohdr->u.tid_rdma.w_req.reth;\n\tvaddr = be64_to_cpu(reth->vaddr);\n\tlen = be32_to_cpu(reth->length);\n\n\tnum_segs = DIV_ROUND_UP(len, qpriv->tid_rdma.local.max_len);\n\tdiff = delta_psn(psn, qp->r_psn);\n\tif (unlikely(diff)) {\n\t\ttid_rdma_rcv_err(packet, ohdr, qp, psn, diff, fecn);\n\t\treturn;\n\t}\n\n\t \n\tif (qpriv->rnr_nak_state)\n\t\tqp->r_head_ack_queue = qp->r_head_ack_queue ?\n\t\t\tqp->r_head_ack_queue - 1 :\n\t\t\trvt_size_atomic(ib_to_rvt(qp->ibqp.device));\n\n\t \n\tnext = qp->r_head_ack_queue + 1;\n\tif (next > rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))\n\t\tnext = 0;\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (unlikely(next == qp->s_acked_ack_queue)) {\n\t\tif (!qp->s_ack_queue[next].sent)\n\t\t\tgoto nack_inv_unlock;\n\t\tupdate_ack_queue(qp, next);\n\t}\n\te = &qp->s_ack_queue[qp->r_head_ack_queue];\n\treq = ack_to_tid_req(e);\n\n\t \n\tif (qpriv->rnr_nak_state) {\n\t\tqp->r_nak_state = 0;\n\t\tqp->s_nak_state = 0;\n\t\tqpriv->rnr_nak_state = TID_RNR_NAK_INIT;\n\t\tqp->r_psn = e->lpsn + 1;\n\t\treq->state = TID_REQUEST_INIT;\n\t\tgoto update_head;\n\t}\n\n\trelease_rdma_sge_mr(e);\n\n\t \n\tif (!len || len & ~PAGE_MASK)\n\t\tgoto nack_inv_unlock;\n\n\trkey = be32_to_cpu(reth->rkey);\n\tqp->r_len = len;\n\n\tif (e->opcode == TID_OP(WRITE_REQ) &&\n\t    (req->setup_head != req->clear_tail ||\n\t     req->clear_tail != req->acked_tail))\n\t\tgoto nack_inv_unlock;\n\n\tif (unlikely(!rvt_rkey_ok(qp, &e->rdma_sge, qp->r_len, vaddr,\n\t\t\t\t  rkey, IB_ACCESS_REMOTE_WRITE)))\n\t\tgoto nack_acc;\n\n\tqp->r_psn += num_segs - 1;\n\n\te->opcode = (bth0 >> 24) & 0xff;\n\te->psn = psn;\n\te->lpsn = qp->r_psn;\n\te->sent = 0;\n\n\treq->n_flows = min_t(u16, num_segs, qpriv->tid_rdma.local.max_write);\n\treq->state = TID_REQUEST_INIT;\n\treq->cur_seg = 0;\n\treq->comp_seg = 0;\n\treq->ack_seg = 0;\n\treq->alloc_seg = 0;\n\treq->isge = 0;\n\treq->seg_len = qpriv->tid_rdma.local.max_len;\n\treq->total_len = len;\n\treq->total_segs = num_segs;\n\treq->r_flow_psn = e->psn;\n\treq->ss.sge = e->rdma_sge;\n\treq->ss.num_sge = 1;\n\n\treq->flow_idx = req->setup_head;\n\treq->clear_tail = req->setup_head;\n\treq->acked_tail = req->setup_head;\n\n\tqp->r_state = e->opcode;\n\tqp->r_nak_state = 0;\n\t \n\tqp->r_msn++;\n\tqp->r_psn++;\n\n\ttrace_hfi1_tid_req_rcv_write_req(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\t req);\n\n\tif (qpriv->r_tid_tail == HFI1_QP_WQE_INVALID) {\n\t\tqpriv->r_tid_tail = qp->r_head_ack_queue;\n\t} else if (qpriv->r_tid_tail == qpriv->r_tid_head) {\n\t\tstruct tid_rdma_request *ptr;\n\n\t\te = &qp->s_ack_queue[qpriv->r_tid_tail];\n\t\tptr = ack_to_tid_req(e);\n\n\t\tif (e->opcode != TID_OP(WRITE_REQ) ||\n\t\t    ptr->comp_seg == ptr->total_segs) {\n\t\t\tif (qpriv->r_tid_tail == qpriv->r_tid_ack)\n\t\t\t\tqpriv->r_tid_ack = qp->r_head_ack_queue;\n\t\t\tqpriv->r_tid_tail = qp->r_head_ack_queue;\n\t\t}\n\t}\nupdate_head:\n\tqp->r_head_ack_queue = next;\n\tqpriv->r_tid_head = qp->r_head_ack_queue;\n\n\thfi1_tid_write_alloc_resources(qp, true);\n\ttrace_hfi1_tid_write_rsp_rcv_req(qp);\n\n\t \n\tqp->s_flags |= RVT_S_RESP_PENDING;\n\tif (fecn)\n\t\tqp->s_flags |= RVT_S_ECN;\n\thfi1_schedule_send(qp);\n\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\treturn;\n\nnack_inv_unlock:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\nnack_inv:\n\trvt_rc_error(qp, IB_WC_LOC_QP_OP_ERR);\n\tqp->r_nak_state = IB_NAK_INVALID_REQUEST;\n\tqp->r_ack_psn = qp->r_psn;\n\t \n\trc_defered_ack(rcd, qp);\n\treturn;\nnack_acc:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\trvt_rc_error(qp, IB_WC_LOC_PROT_ERR);\n\tqp->r_nak_state = IB_NAK_REMOTE_ACCESS_ERROR;\n\tqp->r_ack_psn = qp->r_psn;\n}\n\nu32 hfi1_build_tid_rdma_write_resp(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t   struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t\t   u32 bth2, u32 *len,\n\t\t\t\t   struct rvt_sge_state **ss)\n{\n\tstruct hfi1_ack_priv *epriv = e->priv;\n\tstruct tid_rdma_request *req = &epriv->tid_req;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_flow *flow = NULL;\n\tu32 resp_len = 0, hdwords = 0;\n\tvoid *resp_addr = NULL;\n\tstruct tid_rdma_params *remote;\n\n\ttrace_hfi1_tid_req_build_write_resp(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\t    req);\n\ttrace_hfi1_tid_write_rsp_build_resp(qp);\n\ttrace_hfi1_rsp_build_tid_write_resp(qp, bth2);\n\tflow = &req->flows[req->flow_idx];\n\tswitch (req->state) {\n\tdefault:\n\t\t \n\t\thfi1_tid_write_alloc_resources(qp, false);\n\n\t\t \n\t\tif (req->cur_seg >= req->alloc_seg)\n\t\t\tgoto done;\n\n\t\t \n\t\tif (qpriv->rnr_nak_state == TID_RNR_NAK_SENT)\n\t\t\tgoto done;\n\n\t\treq->state = TID_REQUEST_ACTIVE;\n\t\ttrace_hfi1_tid_flow_build_write_resp(qp, req->flow_idx, flow);\n\t\treq->flow_idx = CIRC_NEXT(req->flow_idx, MAX_FLOWS);\n\t\thfi1_add_tid_reap_timer(qp);\n\t\tbreak;\n\n\tcase TID_REQUEST_RESEND_ACTIVE:\n\tcase TID_REQUEST_RESEND:\n\t\ttrace_hfi1_tid_flow_build_write_resp(qp, req->flow_idx, flow);\n\t\treq->flow_idx = CIRC_NEXT(req->flow_idx, MAX_FLOWS);\n\t\tif (!CIRC_CNT(req->setup_head, req->flow_idx, MAX_FLOWS))\n\t\t\treq->state = TID_REQUEST_ACTIVE;\n\n\t\thfi1_mod_tid_reap_timer(qp);\n\t\tbreak;\n\t}\n\tflow->flow_state.resp_ib_psn = bth2;\n\tresp_addr = (void *)flow->tid_entry;\n\tresp_len = sizeof(*flow->tid_entry) * flow->tidcnt;\n\treq->cur_seg++;\n\n\tmemset(&ohdr->u.tid_rdma.w_rsp, 0, sizeof(ohdr->u.tid_rdma.w_rsp));\n\tepriv->ss.sge.vaddr = resp_addr;\n\tepriv->ss.sge.sge_length = resp_len;\n\tepriv->ss.sge.length = epriv->ss.sge.sge_length;\n\t \n\tepriv->ss.sge.mr = NULL;\n\tepriv->ss.sge.m = 0;\n\tepriv->ss.sge.n = 0;\n\n\tepriv->ss.sg_list = NULL;\n\tepriv->ss.total_len = epriv->ss.sge.sge_length;\n\tepriv->ss.num_sge = 1;\n\n\t*ss = &epriv->ss;\n\t*len = epriv->ss.total_len;\n\n\t \n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\n\tKDETH_RESET(ohdr->u.tid_rdma.w_rsp.kdeth0, KVER, 0x1);\n\tKDETH_RESET(ohdr->u.tid_rdma.w_rsp.kdeth1, JKEY, remote->jkey);\n\tohdr->u.tid_rdma.w_rsp.aeth = rvt_compute_aeth(qp);\n\tohdr->u.tid_rdma.w_rsp.tid_flow_psn =\n\t\tcpu_to_be32((flow->flow_state.generation <<\n\t\t\t     HFI1_KDETH_BTH_SEQ_SHIFT) |\n\t\t\t    (flow->flow_state.spsn &\n\t\t\t     HFI1_KDETH_BTH_SEQ_MASK));\n\tohdr->u.tid_rdma.w_rsp.tid_flow_qp =\n\t\tcpu_to_be32(qpriv->tid_rdma.local.qp |\n\t\t\t    ((flow->idx & TID_RDMA_DESTQP_FLOW_MASK) <<\n\t\t\t     TID_RDMA_DESTQP_FLOW_SHIFT) |\n\t\t\t    qpriv->rcd->ctxt);\n\tohdr->u.tid_rdma.w_rsp.verbs_qp = cpu_to_be32(qp->remote_qpn);\n\t*bth1 = remote->qp;\n\trcu_read_unlock();\n\thdwords = sizeof(ohdr->u.tid_rdma.w_rsp) / sizeof(u32);\n\tqpriv->pending_tid_w_segs++;\ndone:\n\treturn hdwords;\n}\n\nstatic void hfi1_add_tid_reap_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tif (!(qpriv->s_flags & HFI1_R_TID_RSC_TIMER)) {\n\t\tqpriv->s_flags |= HFI1_R_TID_RSC_TIMER;\n\t\tqpriv->s_tid_timer.expires = jiffies +\n\t\t\tqpriv->tid_timer_timeout_jiffies;\n\t\tadd_timer(&qpriv->s_tid_timer);\n\t}\n}\n\nstatic void hfi1_mod_tid_reap_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tqpriv->s_flags |= HFI1_R_TID_RSC_TIMER;\n\tmod_timer(&qpriv->s_tid_timer, jiffies +\n\t\t  qpriv->tid_timer_timeout_jiffies);\n}\n\nstatic int hfi1_stop_tid_reap_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tint rval = 0;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tif (qpriv->s_flags & HFI1_R_TID_RSC_TIMER) {\n\t\trval = del_timer(&qpriv->s_tid_timer);\n\t\tqpriv->s_flags &= ~HFI1_R_TID_RSC_TIMER;\n\t}\n\treturn rval;\n}\n\nvoid hfi1_del_tid_reap_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\n\tdel_timer_sync(&qpriv->s_tid_timer);\n\tqpriv->s_flags &= ~HFI1_R_TID_RSC_TIMER;\n}\n\nstatic void hfi1_tid_timeout(struct timer_list *t)\n{\n\tstruct hfi1_qp_priv *qpriv = from_timer(qpriv, t, s_tid_timer);\n\tstruct rvt_qp *qp = qpriv->owner;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\tunsigned long flags;\n\tu32 i;\n\n\tspin_lock_irqsave(&qp->r_lock, flags);\n\tspin_lock(&qp->s_lock);\n\tif (qpriv->s_flags & HFI1_R_TID_RSC_TIMER) {\n\t\tdd_dev_warn(dd_from_ibdev(qp->ibqp.device), \"[QP%u] %s %d\\n\",\n\t\t\t    qp->ibqp.qp_num, __func__, __LINE__);\n\t\ttrace_hfi1_msg_tid_timeout( \n\t\t\tqp, \"resource timeout = \",\n\t\t\t(u64)qpriv->tid_timer_timeout_jiffies);\n\t\thfi1_stop_tid_reap_timer(qp);\n\t\t \n\t\thfi1_kern_clear_hw_flow(qpriv->rcd, qp);\n\t\tfor (i = 0; i < rvt_max_atomic(rdi); i++) {\n\t\t\tstruct tid_rdma_request *req =\n\t\t\t\tack_to_tid_req(&qp->s_ack_queue[i]);\n\n\t\t\thfi1_kern_exp_rcv_clear_all(req);\n\t\t}\n\t\tspin_unlock(&qp->s_lock);\n\t\tif (qp->ibqp.event_handler) {\n\t\t\tstruct ib_event ev;\n\n\t\t\tev.device = qp->ibqp.device;\n\t\t\tev.element.qp = &qp->ibqp;\n\t\t\tev.event = IB_EVENT_QP_FATAL;\n\t\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t\t}\n\t\trvt_rc_error(qp, IB_WC_RESP_TIMEOUT_ERR);\n\t\tgoto unlock_r_lock;\n\t}\n\tspin_unlock(&qp->s_lock);\nunlock_r_lock:\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\n}\n\nvoid hfi1_rc_rcv_tid_rdma_write_resp(struct hfi1_packet *packet)\n{\n\t \n\n\t \n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct hfi1_ctxtdata *rcd = packet->rcd;\n\tstruct rvt_swqe *wqe;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tenum ib_wc_status status;\n\tu32 opcode, aeth, psn, flow_psn, i, tidlen = 0, pktlen;\n\tbool fecn;\n\tunsigned long flags;\n\n\tfecn = process_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\taeth = be32_to_cpu(ohdr->u.tid_rdma.w_rsp.aeth);\n\topcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0xff;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\n\t \n\tif (cmp_psn(psn, qp->s_next_psn) >= 0)\n\t\tgoto ack_done;\n\n\t \n\tif (unlikely(cmp_psn(psn, qp->s_last_psn) <= 0))\n\t\tgoto ack_done;\n\n\tif (unlikely(qp->s_acked == qp->s_tail))\n\t\tgoto ack_done;\n\n\t \n\tif (qp->r_flags & RVT_R_RDMAR_SEQ) {\n\t\tif (cmp_psn(psn, qp->s_last_psn + 1) != 0)\n\t\t\tgoto ack_done;\n\t\tqp->r_flags &= ~RVT_R_RDMAR_SEQ;\n\t}\n\n\twqe = rvt_get_swqe_ptr(qp, qpriv->s_tid_cur);\n\tif (unlikely(wqe->wr.opcode != IB_WR_TID_RDMA_WRITE))\n\t\tgoto ack_op_err;\n\n\treq = wqe_to_tid_req(wqe);\n\t \n\tif (!CIRC_SPACE(req->setup_head, req->acked_tail, MAX_FLOWS))\n\t\tgoto ack_done;\n\n\t \n\tif (!do_rc_ack(qp, aeth, psn, opcode, 0, rcd))\n\t\tgoto ack_done;\n\n\ttrace_hfi1_ack(qp, psn);\n\n\tflow = &req->flows[req->setup_head];\n\tflow->pkt = 0;\n\tflow->tid_idx = 0;\n\tflow->tid_offset = 0;\n\tflow->sent = 0;\n\tflow->resync_npkts = 0;\n\tflow->tid_qpn = be32_to_cpu(ohdr->u.tid_rdma.w_rsp.tid_flow_qp);\n\tflow->idx = (flow->tid_qpn >> TID_RDMA_DESTQP_FLOW_SHIFT) &\n\t\tTID_RDMA_DESTQP_FLOW_MASK;\n\tflow_psn = mask_psn(be32_to_cpu(ohdr->u.tid_rdma.w_rsp.tid_flow_psn));\n\tflow->flow_state.generation = flow_psn >> HFI1_KDETH_BTH_SEQ_SHIFT;\n\tflow->flow_state.spsn = flow_psn & HFI1_KDETH_BTH_SEQ_MASK;\n\tflow->flow_state.resp_ib_psn = psn;\n\tflow->length = min_t(u32, req->seg_len,\n\t\t\t     (wqe->length - (req->comp_seg * req->seg_len)));\n\n\tflow->npkts = rvt_div_round_up_mtu(qp, flow->length);\n\tflow->flow_state.lpsn = flow->flow_state.spsn +\n\t\tflow->npkts - 1;\n\t \n\tpktlen = packet->tlen - (packet->hlen + 4);\n\tif (pktlen > sizeof(flow->tid_entry)) {\n\t\tstatus = IB_WC_LOC_LEN_ERR;\n\t\tgoto ack_err;\n\t}\n\tmemcpy(flow->tid_entry, packet->ebuf, pktlen);\n\tflow->tidcnt = pktlen / sizeof(*flow->tid_entry);\n\ttrace_hfi1_tid_flow_rcv_write_resp(qp, req->setup_head, flow);\n\n\treq->comp_seg++;\n\ttrace_hfi1_tid_write_sender_rcv_resp(qp, 0);\n\t \n\tfor (i = 0; i < flow->tidcnt; i++) {\n\t\ttrace_hfi1_tid_entry_rcv_write_resp( \n\t\t\tqp, i, flow->tid_entry[i]);\n\t\tif (!EXP_TID_GET(flow->tid_entry[i], LEN)) {\n\t\t\tstatus = IB_WC_LOC_LEN_ERR;\n\t\t\tgoto ack_err;\n\t\t}\n\t\ttidlen += EXP_TID_GET(flow->tid_entry[i], LEN);\n\t}\n\tif (tidlen * PAGE_SIZE < flow->length) {\n\t\tstatus = IB_WC_LOC_LEN_ERR;\n\t\tgoto ack_err;\n\t}\n\n\ttrace_hfi1_tid_req_rcv_write_resp(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t\t  wqe->lpsn, req);\n\t \n\tif (!cmp_psn(psn, wqe->psn)) {\n\t\treq->r_last_acked = mask_psn(wqe->psn - 1);\n\t\t \n\t\treq->acked_tail = req->setup_head;\n\t}\n\n\t \n\treq->setup_head = CIRC_NEXT(req->setup_head, MAX_FLOWS);\n\treq->state = TID_REQUEST_ACTIVE;\n\n\t \n\tif (qpriv->s_tid_cur != qpriv->s_tid_head &&\n\t    req->comp_seg == req->total_segs) {\n\t\tfor (i = qpriv->s_tid_cur + 1; ; i++) {\n\t\t\tif (i == qp->s_size)\n\t\t\t\ti = 0;\n\t\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\t\tif (i == qpriv->s_tid_head)\n\t\t\t\tbreak;\n\t\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_WRITE)\n\t\t\t\tbreak;\n\t\t}\n\t\tqpriv->s_tid_cur = i;\n\t}\n\tqp->s_flags &= ~HFI1_S_WAIT_TID_RESP;\n\thfi1_schedule_tid_send(qp);\n\tgoto ack_done;\n\nack_op_err:\n\tstatus = IB_WC_LOC_QP_OP_ERR;\nack_err:\n\trvt_error_qp(qp, status);\nack_done:\n\tif (fecn)\n\t\tqp->s_flags |= RVT_S_ECN;\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n}\n\nbool hfi1_build_tid_rdma_packet(struct rvt_swqe *wqe,\n\t\t\t\tstruct ib_other_headers *ohdr,\n\t\t\t\tu32 *bth1, u32 *bth2, u32 *len)\n{\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_flow *flow = &req->flows[req->clear_tail];\n\tstruct tid_rdma_params *remote;\n\tstruct rvt_qp *qp = req->qp;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tu32 tidentry = flow->tid_entry[flow->tid_idx];\n\tu32 tidlen = EXP_TID_GET(tidentry, LEN) << PAGE_SHIFT;\n\tstruct tid_rdma_write_data *wd = &ohdr->u.tid_rdma.w_data;\n\tu32 next_offset, om = KDETH_OM_LARGE;\n\tbool last_pkt;\n\n\tif (!tidlen) {\n\t\thfi1_trdma_send_complete(qp, wqe, IB_WC_REM_INV_RD_REQ_ERR);\n\t\trvt_error_qp(qp, IB_WC_REM_INV_RD_REQ_ERR);\n\t}\n\n\t*len = min_t(u32, qp->pmtu, tidlen - flow->tid_offset);\n\tflow->sent += *len;\n\tnext_offset = flow->tid_offset + *len;\n\tlast_pkt = (flow->tid_idx == (flow->tidcnt - 1) &&\n\t\t    next_offset >= tidlen) || (flow->sent >= flow->length);\n\ttrace_hfi1_tid_entry_build_write_data(qp, flow->tid_idx, tidentry);\n\ttrace_hfi1_tid_flow_build_write_data(qp, req->clear_tail, flow);\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\tKDETH_RESET(wd->kdeth0, KVER, 0x1);\n\tKDETH_SET(wd->kdeth0, SH, !last_pkt);\n\tKDETH_SET(wd->kdeth0, INTR, !!(!last_pkt && remote->urg));\n\tKDETH_SET(wd->kdeth0, TIDCTRL, EXP_TID_GET(tidentry, CTRL));\n\tKDETH_SET(wd->kdeth0, TID, EXP_TID_GET(tidentry, IDX));\n\tKDETH_SET(wd->kdeth0, OM, om == KDETH_OM_LARGE);\n\tKDETH_SET(wd->kdeth0, OFFSET, flow->tid_offset / om);\n\tKDETH_RESET(wd->kdeth1, JKEY, remote->jkey);\n\twd->verbs_qp = cpu_to_be32(qp->remote_qpn);\n\trcu_read_unlock();\n\n\t*bth1 = flow->tid_qpn;\n\t*bth2 = mask_psn(((flow->flow_state.spsn + flow->pkt++) &\n\t\t\t HFI1_KDETH_BTH_SEQ_MASK) |\n\t\t\t (flow->flow_state.generation <<\n\t\t\t  HFI1_KDETH_BTH_SEQ_SHIFT));\n\tif (last_pkt) {\n\t\t \n\t\tif (flow->flow_state.lpsn + 1 +\n\t\t    rvt_div_round_up_mtu(qp, req->seg_len) >\n\t\t    MAX_TID_FLOW_PSN)\n\t\t\treq->state = TID_REQUEST_SYNC;\n\t\t*bth2 |= IB_BTH_REQ_ACK;\n\t}\n\n\tif (next_offset >= tidlen) {\n\t\tflow->tid_offset = 0;\n\t\tflow->tid_idx++;\n\t} else {\n\t\tflow->tid_offset = next_offset;\n\t}\n\treturn last_pkt;\n}\n\nvoid hfi1_rc_rcv_tid_rdma_write_data(struct hfi1_packet *packet)\n{\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct hfi1_ctxtdata *rcd = priv->rcd;\n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_ack_entry *e;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tstruct hfi1_ibdev *dev = to_idev(qp->ibqp.device);\n\tunsigned long flags;\n\tu32 psn, next;\n\tu8 opcode;\n\tbool fecn;\n\n\tfecn = process_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\topcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0xff;\n\n\t \n\tspin_lock_irqsave(&qp->s_lock, flags);\n\te = &qp->s_ack_queue[priv->r_tid_tail];\n\treq = ack_to_tid_req(e);\n\tflow = &req->flows[req->clear_tail];\n\tif (cmp_psn(psn, full_flow_psn(flow, flow->flow_state.lpsn))) {\n\t\tupdate_r_next_psn_fecn(packet, priv, rcd, flow, fecn);\n\n\t\tif (cmp_psn(psn, flow->flow_state.r_next_psn))\n\t\t\tgoto send_nak;\n\n\t\tflow->flow_state.r_next_psn = mask_psn(psn + 1);\n\t\t \n\t\tif (fecn && packet->etype == RHF_RCV_TYPE_EAGER) {\n\t\t\tstruct rvt_sge_state ss;\n\t\t\tu32 len;\n\t\t\tu32 tlen = packet->tlen;\n\t\t\tu16 hdrsize = packet->hlen;\n\t\t\tu8 pad = packet->pad;\n\t\t\tu8 extra_bytes = pad + packet->extra_byte +\n\t\t\t\t(SIZE_OF_CRC << 2);\n\t\t\tu32 pmtu = qp->pmtu;\n\n\t\t\tif (unlikely(tlen != (hdrsize + pmtu + extra_bytes)))\n\t\t\t\tgoto send_nak;\n\t\t\tlen = req->comp_seg * req->seg_len;\n\t\t\tlen += delta_psn(psn,\n\t\t\t\tfull_flow_psn(flow, flow->flow_state.spsn)) *\n\t\t\t\tpmtu;\n\t\t\tif (unlikely(req->total_len - len < pmtu))\n\t\t\t\tgoto send_nak;\n\n\t\t\t \n\t\t\tss.sge = e->rdma_sge;\n\t\t\tss.sg_list = NULL;\n\t\t\tss.num_sge = 1;\n\t\t\tss.total_len = req->total_len;\n\t\t\trvt_skip_sge(&ss, len, false);\n\t\t\trvt_copy_sge(qp, &ss, packet->payload, pmtu, false,\n\t\t\t\t     false);\n\t\t\t \n\t\t\tpriv->r_next_psn_kdeth = mask_psn(psn + 1);\n\t\t\tpriv->s_flags |= HFI1_R_TID_SW_PSN;\n\t\t}\n\t\tgoto exit;\n\t}\n\tflow->flow_state.r_next_psn = mask_psn(psn + 1);\n\thfi1_kern_exp_rcv_clear(req);\n\tpriv->alloc_w_segs--;\n\trcd->flows[flow->idx].psn = psn & HFI1_KDETH_BTH_SEQ_MASK;\n\treq->comp_seg++;\n\tpriv->s_nak_state = 0;\n\n\t \n\ttrace_hfi1_rsp_rcv_tid_write_data(qp, psn);\n\ttrace_hfi1_tid_req_rcv_write_data(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\t  req);\n\ttrace_hfi1_tid_write_rsp_rcv_data(qp);\n\tvalidate_r_tid_ack(priv);\n\n\tif (opcode == TID_OP(WRITE_DATA_LAST)) {\n\t\trelease_rdma_sge_mr(e);\n\t\tfor (next = priv->r_tid_tail + 1; ; next++) {\n\t\t\tif (next > rvt_size_atomic(&dev->rdi))\n\t\t\t\tnext = 0;\n\t\t\tif (next == priv->r_tid_head)\n\t\t\t\tbreak;\n\t\t\te = &qp->s_ack_queue[next];\n\t\t\tif (e->opcode == TID_OP(WRITE_REQ))\n\t\t\t\tbreak;\n\t\t}\n\t\tpriv->r_tid_tail = next;\n\t\tif (++qp->s_acked_ack_queue > rvt_size_atomic(&dev->rdi))\n\t\t\tqp->s_acked_ack_queue = 0;\n\t}\n\n\thfi1_tid_write_alloc_resources(qp, true);\n\n\t \n\tif (req->cur_seg < req->total_segs ||\n\t    qp->s_tail_ack_queue != qp->r_head_ack_queue) {\n\t\tqp->s_flags |= RVT_S_RESP_PENDING;\n\t\thfi1_schedule_send(qp);\n\t}\n\n\tpriv->pending_tid_w_segs--;\n\tif (priv->s_flags & HFI1_R_TID_RSC_TIMER) {\n\t\tif (priv->pending_tid_w_segs)\n\t\t\thfi1_mod_tid_reap_timer(req->qp);\n\t\telse\n\t\t\thfi1_stop_tid_reap_timer(req->qp);\n\t}\n\ndone:\n\ttid_rdma_schedule_ack(qp);\nexit:\n\tpriv->r_next_psn_kdeth = flow->flow_state.r_next_psn;\n\tif (fecn)\n\t\tqp->s_flags |= RVT_S_ECN;\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\treturn;\n\nsend_nak:\n\tif (!priv->s_nak_state) {\n\t\tpriv->s_nak_state = IB_NAK_PSN_ERROR;\n\t\tpriv->s_nak_psn = flow->flow_state.r_next_psn;\n\t\ttid_rdma_trigger_ack(qp);\n\t}\n\tgoto done;\n}\n\nstatic bool hfi1_tid_rdma_is_resync_psn(u32 psn)\n{\n\treturn (bool)((psn & HFI1_KDETH_BTH_SEQ_MASK) ==\n\t\t      HFI1_KDETH_BTH_SEQ_MASK);\n}\n\nu32 hfi1_build_tid_rdma_write_ack(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t  struct ib_other_headers *ohdr, u16 iflow,\n\t\t\t\t  u32 *bth1, u32 *bth2)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_flow_state *fs = &qpriv->flow_state;\n\tstruct tid_rdma_request *req = ack_to_tid_req(e);\n\tstruct tid_rdma_flow *flow = &req->flows[iflow];\n\tstruct tid_rdma_params *remote;\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\tKDETH_RESET(ohdr->u.tid_rdma.ack.kdeth1, JKEY, remote->jkey);\n\tohdr->u.tid_rdma.ack.verbs_qp = cpu_to_be32(qp->remote_qpn);\n\t*bth1 = remote->qp;\n\trcu_read_unlock();\n\n\tif (qpriv->resync) {\n\t\t*bth2 = mask_psn((fs->generation <<\n\t\t\t\t  HFI1_KDETH_BTH_SEQ_SHIFT) - 1);\n\t\tohdr->u.tid_rdma.ack.aeth = rvt_compute_aeth(qp);\n\t} else if (qpriv->s_nak_state) {\n\t\t*bth2 = mask_psn(qpriv->s_nak_psn);\n\t\tohdr->u.tid_rdma.ack.aeth =\n\t\t\tcpu_to_be32((qp->r_msn & IB_MSN_MASK) |\n\t\t\t\t    (qpriv->s_nak_state <<\n\t\t\t\t     IB_AETH_CREDIT_SHIFT));\n\t} else {\n\t\t*bth2 = full_flow_psn(flow, flow->flow_state.lpsn);\n\t\tohdr->u.tid_rdma.ack.aeth = rvt_compute_aeth(qp);\n\t}\n\tKDETH_RESET(ohdr->u.tid_rdma.ack.kdeth0, KVER, 0x1);\n\tohdr->u.tid_rdma.ack.tid_flow_qp =\n\t\tcpu_to_be32(qpriv->tid_rdma.local.qp |\n\t\t\t    ((flow->idx & TID_RDMA_DESTQP_FLOW_MASK) <<\n\t\t\t     TID_RDMA_DESTQP_FLOW_SHIFT) |\n\t\t\t    qpriv->rcd->ctxt);\n\n\tohdr->u.tid_rdma.ack.tid_flow_psn = 0;\n\tohdr->u.tid_rdma.ack.verbs_psn =\n\t\tcpu_to_be32(flow->flow_state.resp_ib_psn);\n\n\tif (qpriv->resync) {\n\t\t \n\t\tif (hfi1_tid_rdma_is_resync_psn(qpriv->r_next_psn_kdeth - 1)) {\n\t\t\tohdr->u.tid_rdma.ack.tid_flow_psn =\n\t\t\t\tcpu_to_be32(qpriv->r_next_psn_kdeth_save);\n\t\t} else {\n\t\t\t \n\t\t\tqpriv->r_next_psn_kdeth_save =\n\t\t\t\tqpriv->r_next_psn_kdeth - 1;\n\t\t\tohdr->u.tid_rdma.ack.tid_flow_psn =\n\t\t\t\tcpu_to_be32(qpriv->r_next_psn_kdeth_save);\n\t\t\tqpriv->r_next_psn_kdeth = mask_psn(*bth2 + 1);\n\t\t}\n\t\tqpriv->resync = false;\n\t}\n\n\treturn sizeof(ohdr->u.tid_rdma.ack) / sizeof(u32);\n}\n\nvoid hfi1_rc_rcv_tid_rdma_ack(struct hfi1_packet *packet)\n{\n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct rvt_swqe *wqe;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tu32 aeth, psn, req_psn, ack_psn, flpsn, resync_psn, ack_kpsn;\n\tunsigned long flags;\n\tu16 fidx;\n\n\ttrace_hfi1_tid_write_sender_rcv_tid_ack(qp, 0);\n\tprocess_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\taeth = be32_to_cpu(ohdr->u.tid_rdma.ack.aeth);\n\treq_psn = mask_psn(be32_to_cpu(ohdr->u.tid_rdma.ack.verbs_psn));\n\tresync_psn = mask_psn(be32_to_cpu(ohdr->u.tid_rdma.ack.tid_flow_psn));\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\ttrace_hfi1_rcv_tid_ack(qp, aeth, psn, req_psn, resync_psn);\n\n\t \n\tif ((qp->s_flags & HFI1_S_WAIT_HALT) &&\n\t    cmp_psn(psn, qpriv->s_resync_psn))\n\t\tgoto ack_op_err;\n\n\tack_psn = req_psn;\n\tif (hfi1_tid_rdma_is_resync_psn(psn))\n\t\tack_kpsn = resync_psn;\n\telse\n\t\tack_kpsn = psn;\n\tif (aeth >> 29) {\n\t\tack_psn--;\n\t\tack_kpsn--;\n\t}\n\n\tif (unlikely(qp->s_acked == qp->s_tail))\n\t\tgoto ack_op_err;\n\n\twqe = rvt_get_swqe_ptr(qp, qp->s_acked);\n\n\tif (wqe->wr.opcode != IB_WR_TID_RDMA_WRITE)\n\t\tgoto ack_op_err;\n\n\treq = wqe_to_tid_req(wqe);\n\ttrace_hfi1_tid_req_rcv_tid_ack(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t       wqe->lpsn, req);\n\tflow = &req->flows[req->acked_tail];\n\ttrace_hfi1_tid_flow_rcv_tid_ack(qp, req->acked_tail, flow);\n\n\t \n\tif (cmp_psn(psn, full_flow_psn(flow, flow->flow_state.spsn)) < 0 ||\n\t    cmp_psn(req_psn, flow->flow_state.resp_ib_psn) < 0)\n\t\tgoto ack_op_err;\n\n\twhile (cmp_psn(ack_kpsn,\n\t\t       full_flow_psn(flow, flow->flow_state.lpsn)) >= 0 &&\n\t       req->ack_seg < req->cur_seg) {\n\t\treq->ack_seg++;\n\t\t \n\t\treq->acked_tail = CIRC_NEXT(req->acked_tail, MAX_FLOWS);\n\t\treq->r_last_acked = flow->flow_state.resp_ib_psn;\n\t\ttrace_hfi1_tid_req_rcv_tid_ack(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t\t       wqe->lpsn, req);\n\t\tif (req->ack_seg == req->total_segs) {\n\t\t\treq->state = TID_REQUEST_COMPLETE;\n\t\t\twqe = do_rc_completion(qp, wqe,\n\t\t\t\t\t       to_iport(qp->ibqp.device,\n\t\t\t\t\t\t\tqp->port_num));\n\t\t\ttrace_hfi1_sender_rcv_tid_ack(qp);\n\t\t\tatomic_dec(&qpriv->n_tid_requests);\n\t\t\tif (qp->s_acked == qp->s_tail)\n\t\t\t\tbreak;\n\t\t\tif (wqe->wr.opcode != IB_WR_TID_RDMA_WRITE)\n\t\t\t\tbreak;\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t}\n\t\tflow = &req->flows[req->acked_tail];\n\t\ttrace_hfi1_tid_flow_rcv_tid_ack(qp, req->acked_tail, flow);\n\t}\n\n\ttrace_hfi1_tid_req_rcv_tid_ack(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t       wqe->lpsn, req);\n\tswitch (aeth >> 29) {\n\tcase 0:          \n\t\tif (qpriv->s_flags & RVT_S_WAIT_ACK)\n\t\t\tqpriv->s_flags &= ~RVT_S_WAIT_ACK;\n\t\tif (!hfi1_tid_rdma_is_resync_psn(psn)) {\n\t\t\t \n\t\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_WRITE &&\n\t\t\t    req->ack_seg < req->cur_seg)\n\t\t\t\thfi1_mod_tid_retry_timer(qp);\n\t\t\telse\n\t\t\t\thfi1_stop_tid_retry_timer(qp);\n\t\t\thfi1_schedule_send(qp);\n\t\t} else {\n\t\t\tu32 spsn, fpsn, last_acked, generation;\n\t\t\tstruct tid_rdma_request *rptr;\n\n\t\t\t \n\t\t\thfi1_stop_tid_retry_timer(qp);\n\t\t\t \n\t\t\tqp->s_flags &= ~HFI1_S_WAIT_HALT;\n\t\t\t \n\t\t\tqpriv->s_flags &= ~RVT_S_SEND_ONE;\n\t\t\thfi1_schedule_send(qp);\n\n\t\t\tif ((qp->s_acked == qpriv->s_tid_tail &&\n\t\t\t     req->ack_seg == req->total_segs) ||\n\t\t\t    qp->s_acked == qp->s_tail) {\n\t\t\t\tqpriv->s_state = TID_OP(WRITE_DATA_LAST);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (req->ack_seg == req->comp_seg) {\n\t\t\t\tqpriv->s_state = TID_OP(WRITE_DATA);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t \n\t\t\tpsn = mask_psn(psn + 1);\n\t\t\tgeneration = psn >> HFI1_KDETH_BTH_SEQ_SHIFT;\n\t\t\tspsn = 0;\n\n\t\t\t \n\t\t\tif (delta_psn(ack_psn, wqe->lpsn))\n\t\t\t\twqe = rvt_get_swqe_ptr(qp, qp->s_acked);\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t\tflow = &req->flows[req->acked_tail];\n\t\t\t \n\t\t\tfpsn = full_flow_psn(flow, flow->flow_state.spsn);\n\t\t\treq->r_ack_psn = psn;\n\t\t\t \n\t\t\tif (flow->flow_state.generation !=\n\t\t\t    (resync_psn >> HFI1_KDETH_BTH_SEQ_SHIFT))\n\t\t\t\tresync_psn = mask_psn(fpsn - 1);\n\t\t\tflow->resync_npkts +=\n\t\t\t\tdelta_psn(mask_psn(resync_psn + 1), fpsn);\n\t\t\t \n\t\t\tlast_acked = qp->s_acked;\n\t\t\trptr = req;\n\t\t\twhile (1) {\n\t\t\t\t \n\t\t\t\tfor (fidx = rptr->acked_tail;\n\t\t\t\t     CIRC_CNT(rptr->setup_head, fidx,\n\t\t\t\t\t      MAX_FLOWS);\n\t\t\t\t     fidx = CIRC_NEXT(fidx, MAX_FLOWS)) {\n\t\t\t\t\tu32 lpsn;\n\t\t\t\t\tu32 gen;\n\n\t\t\t\t\tflow = &rptr->flows[fidx];\n\t\t\t\t\tgen = flow->flow_state.generation;\n\t\t\t\t\tif (WARN_ON(gen == generation &&\n\t\t\t\t\t\t    flow->flow_state.spsn !=\n\t\t\t\t\t\t     spsn))\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tlpsn = flow->flow_state.lpsn;\n\t\t\t\t\tlpsn = full_flow_psn(flow, lpsn);\n\t\t\t\t\tflow->npkts =\n\t\t\t\t\t\tdelta_psn(lpsn,\n\t\t\t\t\t\t\t  mask_psn(resync_psn)\n\t\t\t\t\t\t\t  );\n\t\t\t\t\tflow->flow_state.generation =\n\t\t\t\t\t\tgeneration;\n\t\t\t\t\tflow->flow_state.spsn = spsn;\n\t\t\t\t\tflow->flow_state.lpsn =\n\t\t\t\t\t\tflow->flow_state.spsn +\n\t\t\t\t\t\tflow->npkts - 1;\n\t\t\t\t\tflow->pkt = 0;\n\t\t\t\t\tspsn += flow->npkts;\n\t\t\t\t\tresync_psn += flow->npkts;\n\t\t\t\t\ttrace_hfi1_tid_flow_rcv_tid_ack(qp,\n\t\t\t\t\t\t\t\t\tfidx,\n\t\t\t\t\t\t\t\t\tflow);\n\t\t\t\t}\n\t\t\t\tif (++last_acked == qpriv->s_tid_cur + 1)\n\t\t\t\t\tbreak;\n\t\t\t\tif (last_acked == qp->s_size)\n\t\t\t\t\tlast_acked = 0;\n\t\t\t\twqe = rvt_get_swqe_ptr(qp, last_acked);\n\t\t\t\trptr = wqe_to_tid_req(wqe);\n\t\t\t}\n\t\t\treq->cur_seg = req->ack_seg;\n\t\t\tqpriv->s_tid_tail = qp->s_acked;\n\t\t\tqpriv->s_state = TID_OP(WRITE_REQ);\n\t\t\thfi1_schedule_tid_send(qp);\n\t\t}\ndone:\n\t\tqpriv->s_retry = qp->s_retry_cnt;\n\t\tbreak;\n\n\tcase 3:          \n\t\thfi1_stop_tid_retry_timer(qp);\n\t\tswitch ((aeth >> IB_AETH_CREDIT_SHIFT) &\n\t\t\tIB_AETH_CREDIT_MASK) {\n\t\tcase 0:  \n\t\t\tif (!req->flows)\n\t\t\t\tbreak;\n\t\t\tflow = &req->flows[req->acked_tail];\n\t\t\tflpsn = full_flow_psn(flow, flow->flow_state.lpsn);\n\t\t\tif (cmp_psn(psn, flpsn) > 0)\n\t\t\t\tbreak;\n\t\t\ttrace_hfi1_tid_flow_rcv_tid_ack(qp, req->acked_tail,\n\t\t\t\t\t\t\tflow);\n\t\t\treq->r_ack_psn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\t\t\treq->cur_seg = req->ack_seg;\n\t\t\tqpriv->s_tid_tail = qp->s_acked;\n\t\t\tqpriv->s_state = TID_OP(WRITE_REQ);\n\t\t\tqpriv->s_retry = qp->s_retry_cnt;\n\t\t\thfi1_schedule_tid_send(qp);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\nack_op_err:\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n}\n\nvoid hfi1_add_tid_retry_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\tlockdep_assert_held(&qp->s_lock);\n\tif (!(priv->s_flags & HFI1_S_TID_RETRY_TIMER)) {\n\t\tpriv->s_flags |= HFI1_S_TID_RETRY_TIMER;\n\t\tpriv->s_tid_retry_timer.expires = jiffies +\n\t\t\tpriv->tid_retry_timeout_jiffies + rdi->busy_jiffies;\n\t\tadd_timer(&priv->s_tid_retry_timer);\n\t}\n}\n\nstatic void hfi1_mod_tid_retry_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\tlockdep_assert_held(&qp->s_lock);\n\tpriv->s_flags |= HFI1_S_TID_RETRY_TIMER;\n\tmod_timer(&priv->s_tid_retry_timer, jiffies +\n\t\t  priv->tid_retry_timeout_jiffies + rdi->busy_jiffies);\n}\n\nstatic int hfi1_stop_tid_retry_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tint rval = 0;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tif (priv->s_flags & HFI1_S_TID_RETRY_TIMER) {\n\t\trval = del_timer(&priv->s_tid_retry_timer);\n\t\tpriv->s_flags &= ~HFI1_S_TID_RETRY_TIMER;\n\t}\n\treturn rval;\n}\n\nvoid hfi1_del_tid_retry_timer(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tdel_timer_sync(&priv->s_tid_retry_timer);\n\tpriv->s_flags &= ~HFI1_S_TID_RETRY_TIMER;\n}\n\nstatic void hfi1_tid_retry_timeout(struct timer_list *t)\n{\n\tstruct hfi1_qp_priv *priv = from_timer(priv, t, s_tid_retry_timer);\n\tstruct rvt_qp *qp = priv->owner;\n\tstruct rvt_swqe *wqe;\n\tunsigned long flags;\n\tstruct tid_rdma_request *req;\n\n\tspin_lock_irqsave(&qp->r_lock, flags);\n\tspin_lock(&qp->s_lock);\n\ttrace_hfi1_tid_write_sender_retry_timeout(qp, 0);\n\tif (priv->s_flags & HFI1_S_TID_RETRY_TIMER) {\n\t\thfi1_stop_tid_retry_timer(qp);\n\t\tif (!priv->s_retry) {\n\t\t\ttrace_hfi1_msg_tid_retry_timeout( \n\t\t\t\tqp,\n\t\t\t\t\"Exhausted retries. Tid retry timeout = \",\n\t\t\t\t(u64)priv->tid_retry_timeout_jiffies);\n\n\t\t\twqe = rvt_get_swqe_ptr(qp, qp->s_acked);\n\t\t\thfi1_trdma_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);\n\t\t\trvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);\n\t\t} else {\n\t\t\twqe = rvt_get_swqe_ptr(qp, qp->s_acked);\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t\ttrace_hfi1_tid_req_tid_retry_timeout( \n\t\t\t   qp, 0, wqe->wr.opcode, wqe->psn, wqe->lpsn, req);\n\n\t\t\tpriv->s_flags &= ~RVT_S_WAIT_ACK;\n\t\t\t \n\t\t\tpriv->s_flags |= RVT_S_SEND_ONE;\n\t\t\t \n\t\t\tqp->s_flags |= HFI1_S_WAIT_HALT;\n\t\t\tpriv->s_state = TID_OP(RESYNC);\n\t\t\tpriv->s_retry--;\n\t\t\thfi1_schedule_tid_send(qp);\n\t\t}\n\t}\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\n}\n\nu32 hfi1_build_tid_rdma_resync(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t       struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t       u32 *bth2, u16 fidx)\n{\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct tid_rdma_params *remote;\n\tstruct tid_rdma_request *req = wqe_to_tid_req(wqe);\n\tstruct tid_rdma_flow *flow = &req->flows[fidx];\n\tu32 generation;\n\n\trcu_read_lock();\n\tremote = rcu_dereference(qpriv->tid_rdma.remote);\n\tKDETH_RESET(ohdr->u.tid_rdma.ack.kdeth1, JKEY, remote->jkey);\n\tohdr->u.tid_rdma.ack.verbs_qp = cpu_to_be32(qp->remote_qpn);\n\t*bth1 = remote->qp;\n\trcu_read_unlock();\n\n\tgeneration = kern_flow_generation_next(flow->flow_state.generation);\n\t*bth2 = mask_psn((generation << HFI1_KDETH_BTH_SEQ_SHIFT) - 1);\n\tqpriv->s_resync_psn = *bth2;\n\t*bth2 |= IB_BTH_REQ_ACK;\n\tKDETH_RESET(ohdr->u.tid_rdma.ack.kdeth0, KVER, 0x1);\n\n\treturn sizeof(ohdr->u.tid_rdma.resync) / sizeof(u32);\n}\n\nvoid hfi1_rc_rcv_tid_rdma_resync(struct hfi1_packet *packet)\n{\n\tstruct ib_other_headers *ohdr = packet->ohdr;\n\tstruct rvt_qp *qp = packet->qp;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct hfi1_ctxtdata *rcd = qpriv->rcd;\n\tstruct hfi1_ibdev *dev = to_idev(qp->ibqp.device);\n\tstruct rvt_ack_entry *e;\n\tstruct tid_rdma_request *req;\n\tstruct tid_rdma_flow *flow;\n\tstruct tid_flow_state *fs = &qpriv->flow_state;\n\tu32 psn, generation, idx, gen_next;\n\tbool fecn;\n\tunsigned long flags;\n\n\tfecn = process_ecn(qp, packet);\n\tpsn = mask_psn(be32_to_cpu(ohdr->bth[2]));\n\n\tgeneration = mask_psn(psn + 1) >> HFI1_KDETH_BTH_SEQ_SHIFT;\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\n\tgen_next = (fs->generation == KERN_GENERATION_RESERVED) ?\n\t\tgeneration : kern_flow_generation_next(fs->generation);\n\t \n\tif (generation != mask_generation(gen_next - 1) &&\n\t    generation != gen_next)\n\t\tgoto bail;\n\t \n\tif (qpriv->resync)\n\t\tgoto bail;\n\n\tspin_lock(&rcd->exp_lock);\n\tif (fs->index >= RXE_NUM_TID_FLOWS) {\n\t\t \n\t\tfs->generation = generation;\n\t} else {\n\t\t \n\t\trcd->flows[fs->index].generation = generation;\n\t\tfs->generation = kern_setup_hw_flow(rcd, fs->index);\n\t}\n\tfs->psn = 0;\n\t \n\tqpriv->s_flags &= ~HFI1_R_TID_SW_PSN;\n\ttrace_hfi1_tid_write_rsp_rcv_resync(qp);\n\n\t \n\tfor (idx = qpriv->r_tid_tail; ; idx++) {\n\t\tu16 flow_idx;\n\n\t\tif (idx > rvt_size_atomic(&dev->rdi))\n\t\t\tidx = 0;\n\t\te = &qp->s_ack_queue[idx];\n\t\tif (e->opcode == TID_OP(WRITE_REQ)) {\n\t\t\treq = ack_to_tid_req(e);\n\t\t\ttrace_hfi1_tid_req_rcv_resync(qp, 0, e->opcode, e->psn,\n\t\t\t\t\t\t      e->lpsn, req);\n\n\t\t\t \n\t\t\tfor (flow_idx = req->clear_tail;\n\t\t\t     CIRC_CNT(req->setup_head, flow_idx,\n\t\t\t\t      MAX_FLOWS);\n\t\t\t     flow_idx = CIRC_NEXT(flow_idx, MAX_FLOWS)) {\n\t\t\t\tu32 lpsn;\n\t\t\t\tu32 next;\n\n\t\t\t\tflow = &req->flows[flow_idx];\n\t\t\t\tlpsn = full_flow_psn(flow,\n\t\t\t\t\t\t     flow->flow_state.lpsn);\n\t\t\t\tnext = flow->flow_state.r_next_psn;\n\t\t\t\tflow->npkts = delta_psn(lpsn, next - 1);\n\t\t\t\tflow->flow_state.generation = fs->generation;\n\t\t\t\tflow->flow_state.spsn = fs->psn;\n\t\t\t\tflow->flow_state.lpsn =\n\t\t\t\t\tflow->flow_state.spsn + flow->npkts - 1;\n\t\t\t\tflow->flow_state.r_next_psn =\n\t\t\t\t\tfull_flow_psn(flow,\n\t\t\t\t\t\t      flow->flow_state.spsn);\n\t\t\t\tfs->psn += flow->npkts;\n\t\t\t\ttrace_hfi1_tid_flow_rcv_resync(qp, flow_idx,\n\t\t\t\t\t\t\t       flow);\n\t\t\t}\n\t\t}\n\t\tif (idx == qp->s_tail_ack_queue)\n\t\t\tbreak;\n\t}\n\n\tspin_unlock(&rcd->exp_lock);\n\tqpriv->resync = true;\n\t \n\tqpriv->s_nak_state = 0;\n\ttid_rdma_trigger_ack(qp);\nbail:\n\tif (fecn)\n\t\tqp->s_flags |= RVT_S_ECN;\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n}\n\n \nstatic void update_tid_tail(struct rvt_qp *qp)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tu32 i;\n\tstruct rvt_swqe *wqe;\n\n\tlockdep_assert_held(&qp->s_lock);\n\t \n\tif (priv->s_tid_tail == priv->s_tid_cur)\n\t\treturn;\n\tfor (i = priv->s_tid_tail + 1; ; i++) {\n\t\tif (i == qp->s_size)\n\t\t\ti = 0;\n\n\t\tif (i == priv->s_tid_cur)\n\t\t\tbreak;\n\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\tif (wqe->wr.opcode == IB_WR_TID_RDMA_WRITE)\n\t\t\tbreak;\n\t}\n\tpriv->s_tid_tail = i;\n\tpriv->s_state = TID_OP(WRITE_RESP);\n}\n\nint hfi1_make_tid_rdma_pkt(struct rvt_qp *qp, struct hfi1_pkt_state *ps)\n\t__must_hold(&qp->s_lock)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct rvt_swqe *wqe;\n\tu32 bth1 = 0, bth2 = 0, hwords = 5, len, middle = 0;\n\tstruct ib_other_headers *ohdr;\n\tstruct rvt_sge_state *ss = &qp->s_sge;\n\tstruct rvt_ack_entry *e = &qp->s_ack_queue[qp->s_tail_ack_queue];\n\tstruct tid_rdma_request *req = ack_to_tid_req(e);\n\tbool last = false;\n\tu8 opcode = TID_OP(WRITE_DATA);\n\n\tlockdep_assert_held(&qp->s_lock);\n\ttrace_hfi1_tid_write_sender_make_tid_pkt(qp, 0);\n\t \n\tif (((atomic_read(&priv->n_tid_requests) < HFI1_TID_RDMA_WRITE_CNT) &&\n\t     atomic_read(&priv->n_requests) &&\n\t     !(qp->s_flags & (RVT_S_BUSY | RVT_S_WAIT_ACK |\n\t\t\t     HFI1_S_ANY_WAIT_IO))) ||\n\t    (e->opcode == TID_OP(WRITE_REQ) && req->cur_seg < req->alloc_seg &&\n\t     !(qp->s_flags & (RVT_S_BUSY | HFI1_S_ANY_WAIT_IO)))) {\n\t\tstruct iowait_work *iowork;\n\n\t\tiowork = iowait_get_ib_work(&priv->s_iowait);\n\t\tps->s_txreq = get_waiting_verbs_txreq(iowork);\n\t\tif (ps->s_txreq || hfi1_make_rc_req(qp, ps)) {\n\t\t\tpriv->s_flags |= HFI1_S_TID_BUSY_SET;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tps->s_txreq = get_txreq(ps->dev, qp);\n\tif (!ps->s_txreq)\n\t\tgoto bail_no_tx;\n\n\tohdr = &ps->s_txreq->phdr.hdr.ibh.u.oth;\n\n\tif ((priv->s_flags & RVT_S_ACK_PENDING) &&\n\t    make_tid_rdma_ack(qp, ohdr, ps))\n\t\treturn 1;\n\n\t \n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_SEND_OK))\n\t\tgoto bail;\n\n\tif (priv->s_flags & RVT_S_WAIT_ACK)\n\t\tgoto bail;\n\n\t \n\tif (priv->s_tid_tail == HFI1_QP_WQE_INVALID)\n\t\tgoto bail;\n\twqe = rvt_get_swqe_ptr(qp, priv->s_tid_tail);\n\treq = wqe_to_tid_req(wqe);\n\ttrace_hfi1_tid_req_make_tid_pkt(qp, 0, wqe->wr.opcode, wqe->psn,\n\t\t\t\t\twqe->lpsn, req);\n\tswitch (priv->s_state) {\n\tcase TID_OP(WRITE_REQ):\n\tcase TID_OP(WRITE_RESP):\n\t\tpriv->tid_ss.sge = wqe->sg_list[0];\n\t\tpriv->tid_ss.sg_list = wqe->sg_list + 1;\n\t\tpriv->tid_ss.num_sge = wqe->wr.num_sge;\n\t\tpriv->tid_ss.total_len = wqe->length;\n\n\t\tif (priv->s_state == TID_OP(WRITE_REQ))\n\t\t\thfi1_tid_rdma_restart_req(qp, wqe, &bth2);\n\t\tpriv->s_state = TID_OP(WRITE_DATA);\n\t\tfallthrough;\n\n\tcase TID_OP(WRITE_DATA):\n\t\t \n\t\ttrace_hfi1_sender_make_tid_pkt(qp);\n\t\ttrace_hfi1_tid_write_sender_make_tid_pkt(qp, 0);\n\t\twqe = rvt_get_swqe_ptr(qp, priv->s_tid_tail);\n\t\treq = wqe_to_tid_req(wqe);\n\t\tlen = wqe->length;\n\n\t\tif (!req->comp_seg || req->cur_seg == req->comp_seg)\n\t\t\tgoto bail;\n\n\t\ttrace_hfi1_tid_req_make_tid_pkt(qp, 0, wqe->wr.opcode,\n\t\t\t\t\t\twqe->psn, wqe->lpsn, req);\n\t\tlast = hfi1_build_tid_rdma_packet(wqe, ohdr, &bth1, &bth2,\n\t\t\t\t\t\t  &len);\n\n\t\tif (last) {\n\t\t\t \n\t\t\treq->clear_tail = CIRC_NEXT(req->clear_tail,\n\t\t\t\t\t\t    MAX_FLOWS);\n\t\t\tif (++req->cur_seg < req->total_segs) {\n\t\t\t\tif (!CIRC_CNT(req->setup_head, req->clear_tail,\n\t\t\t\t\t      MAX_FLOWS))\n\t\t\t\t\tqp->s_flags |= HFI1_S_WAIT_TID_RESP;\n\t\t\t} else {\n\t\t\t\tpriv->s_state = TID_OP(WRITE_DATA_LAST);\n\t\t\t\topcode = TID_OP(WRITE_DATA_LAST);\n\n\t\t\t\t \n\t\t\t\tupdate_tid_tail(qp);\n\t\t\t}\n\t\t}\n\t\thwords += sizeof(ohdr->u.tid_rdma.w_data) / sizeof(u32);\n\t\tss = &priv->tid_ss;\n\t\tbreak;\n\n\tcase TID_OP(RESYNC):\n\t\ttrace_hfi1_sender_make_tid_pkt(qp);\n\t\t \n\t\twqe = rvt_get_swqe_ptr(qp, priv->s_tid_cur);\n\t\treq = wqe_to_tid_req(wqe);\n\t\t \n\t\tif (!req->comp_seg) {\n\t\t\twqe = rvt_get_swqe_ptr(qp,\n\t\t\t\t\t       (!priv->s_tid_cur ? qp->s_size :\n\t\t\t\t\t\tpriv->s_tid_cur) - 1);\n\t\t\treq = wqe_to_tid_req(wqe);\n\t\t}\n\t\thwords += hfi1_build_tid_rdma_resync(qp, wqe, ohdr, &bth1,\n\t\t\t\t\t\t     &bth2,\n\t\t\t\t\t\t     CIRC_PREV(req->setup_head,\n\t\t\t\t\t\t\t       MAX_FLOWS));\n\t\tss = NULL;\n\t\tlen = 0;\n\t\topcode = TID_OP(RESYNC);\n\t\tbreak;\n\n\tdefault:\n\t\tgoto bail;\n\t}\n\tif (priv->s_flags & RVT_S_SEND_ONE) {\n\t\tpriv->s_flags &= ~RVT_S_SEND_ONE;\n\t\tpriv->s_flags |= RVT_S_WAIT_ACK;\n\t\tbth2 |= IB_BTH_REQ_ACK;\n\t}\n\tqp->s_len -= len;\n\tps->s_txreq->hdr_dwords = hwords;\n\tps->s_txreq->sde = priv->s_sde;\n\tps->s_txreq->ss = ss;\n\tps->s_txreq->s_cur_size = len;\n\thfi1_make_ruc_header(qp, ohdr, (opcode << 24), bth1, bth2,\n\t\t\t     middle, ps);\n\treturn 1;\nbail:\n\thfi1_put_txreq(ps->s_txreq);\nbail_no_tx:\n\tps->s_txreq = NULL;\n\tpriv->s_flags &= ~RVT_S_BUSY;\n\t \n\tiowait_set_flag(&priv->s_iowait, IOWAIT_PENDING_TID);\n\treturn 0;\n}\n\nstatic int make_tid_rdma_ack(struct rvt_qp *qp,\n\t\t\t     struct ib_other_headers *ohdr,\n\t\t\t     struct hfi1_pkt_state *ps)\n{\n\tstruct rvt_ack_entry *e;\n\tstruct hfi1_qp_priv *qpriv = qp->priv;\n\tstruct hfi1_ibdev *dev = to_idev(qp->ibqp.device);\n\tu32 hwords, next;\n\tu32 len = 0;\n\tu32 bth1 = 0, bth2 = 0;\n\tint middle = 0;\n\tu16 flow;\n\tstruct tid_rdma_request *req, *nreq;\n\n\ttrace_hfi1_tid_write_rsp_make_tid_ack(qp);\n\t \n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK))\n\t\tgoto bail;\n\n\t \n\thwords = 5;\n\n\te = &qp->s_ack_queue[qpriv->r_tid_ack];\n\treq = ack_to_tid_req(e);\n\t \n\tif (qpriv->resync) {\n\t\tif (!req->ack_seg || req->ack_seg == req->total_segs)\n\t\t\tqpriv->r_tid_ack = !qpriv->r_tid_ack ?\n\t\t\t\trvt_size_atomic(&dev->rdi) :\n\t\t\t\tqpriv->r_tid_ack - 1;\n\t\te = &qp->s_ack_queue[qpriv->r_tid_ack];\n\t\treq = ack_to_tid_req(e);\n\t}\n\n\ttrace_hfi1_rsp_make_tid_ack(qp, e->psn);\n\ttrace_hfi1_tid_req_make_tid_ack(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\treq);\n\t \n\tif (!qpriv->s_nak_state && !qpriv->resync &&\n\t    req->ack_seg == req->comp_seg)\n\t\tgoto bail;\n\n\tdo {\n\t\t \n\t\treq->ack_seg +=\n\t\t\t \n\t\t\tCIRC_CNT(req->clear_tail, req->acked_tail,\n\t\t\t\t MAX_FLOWS);\n\t\t \n\t\treq->acked_tail = req->clear_tail;\n\n\t\t \n\t\tflow = CIRC_PREV(req->acked_tail, MAX_FLOWS);\n\t\tif (req->ack_seg != req->total_segs)\n\t\t\tbreak;\n\t\treq->state = TID_REQUEST_COMPLETE;\n\n\t\tnext = qpriv->r_tid_ack + 1;\n\t\tif (next > rvt_size_atomic(&dev->rdi))\n\t\t\tnext = 0;\n\t\tqpriv->r_tid_ack = next;\n\t\tif (qp->s_ack_queue[next].opcode != TID_OP(WRITE_REQ))\n\t\t\tbreak;\n\t\tnreq = ack_to_tid_req(&qp->s_ack_queue[next]);\n\t\tif (!nreq->comp_seg || nreq->ack_seg == nreq->comp_seg)\n\t\t\tbreak;\n\n\t\t \n\t\te = &qp->s_ack_queue[qpriv->r_tid_ack];\n\t\treq = ack_to_tid_req(e);\n\t} while (1);\n\n\t \n\tif (qpriv->s_nak_state ||\n\t    (qpriv->resync &&\n\t     !hfi1_tid_rdma_is_resync_psn(qpriv->r_next_psn_kdeth - 1) &&\n\t     (cmp_psn(qpriv->r_next_psn_kdeth - 1,\n\t\t      full_flow_psn(&req->flows[flow],\n\t\t\t\t    req->flows[flow].flow_state.lpsn)) > 0))) {\n\t\t \n\t\te = &qp->s_ack_queue[qpriv->r_tid_ack];\n\t\treq = ack_to_tid_req(e);\n\t\tflow = req->acked_tail;\n\t} else if (req->ack_seg == req->total_segs &&\n\t\t   qpriv->s_flags & HFI1_R_TID_WAIT_INTERLCK)\n\t\tqpriv->s_flags &= ~HFI1_R_TID_WAIT_INTERLCK;\n\n\ttrace_hfi1_tid_write_rsp_make_tid_ack(qp);\n\ttrace_hfi1_tid_req_make_tid_ack(qp, 0, e->opcode, e->psn, e->lpsn,\n\t\t\t\t\treq);\n\thwords += hfi1_build_tid_rdma_write_ack(qp, e, ohdr, flow, &bth1,\n\t\t\t\t\t\t&bth2);\n\tlen = 0;\n\tqpriv->s_flags &= ~RVT_S_ACK_PENDING;\n\tps->s_txreq->hdr_dwords = hwords;\n\tps->s_txreq->sde = qpriv->s_sde;\n\tps->s_txreq->s_cur_size = len;\n\tps->s_txreq->ss = NULL;\n\thfi1_make_ruc_header(qp, ohdr, (TID_OP(ACK) << 24), bth1, bth2, middle,\n\t\t\t     ps);\n\tps->s_txreq->txreq.flags |= SDMA_TXREQ_F_VIP;\n\treturn 1;\nbail:\n\t \n\tsmp_wmb();\n\tqpriv->s_flags &= ~RVT_S_ACK_PENDING;\n\treturn 0;\n}\n\nstatic int hfi1_send_tid_ok(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\treturn !(priv->s_flags & RVT_S_BUSY ||\n\t\t qp->s_flags & HFI1_S_ANY_WAIT_IO) &&\n\t\t(verbs_txreq_queued(iowait_get_tid_work(&priv->s_iowait)) ||\n\t\t (priv->s_flags & RVT_S_RESP_PENDING) ||\n\t\t !(qp->s_flags & HFI1_S_ANY_TID_WAIT_SEND));\n}\n\nvoid _hfi1_do_tid_send(struct work_struct *work)\n{\n\tstruct iowait_work *w = container_of(work, struct iowait_work, iowork);\n\tstruct rvt_qp *qp = iowait_to_qp(w->iow);\n\n\thfi1_do_tid_send(qp);\n}\n\nstatic void hfi1_do_tid_send(struct rvt_qp *qp)\n{\n\tstruct hfi1_pkt_state ps;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tps.dev = to_idev(qp->ibqp.device);\n\tps.ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tps.ppd = ppd_from_ibp(ps.ibp);\n\tps.wait = iowait_get_tid_work(&priv->s_iowait);\n\tps.in_thread = false;\n\tps.timeout_int = qp->timeout_jiffies / 8;\n\n\ttrace_hfi1_rc_do_tid_send(qp, false);\n\tspin_lock_irqsave(&qp->s_lock, ps.flags);\n\n\t \n\tif (!hfi1_send_tid_ok(qp)) {\n\t\tif (qp->s_flags & HFI1_S_ANY_WAIT_IO)\n\t\t\tiowait_set_flag(&priv->s_iowait, IOWAIT_PENDING_TID);\n\t\tspin_unlock_irqrestore(&qp->s_lock, ps.flags);\n\t\treturn;\n\t}\n\n\tpriv->s_flags |= RVT_S_BUSY;\n\n\tps.timeout = jiffies + ps.timeout_int;\n\tps.cpu = priv->s_sde ? priv->s_sde->cpu :\n\t\tcpumask_first(cpumask_of_node(ps.ppd->dd->node));\n\tps.pkts_sent = false;\n\n\t \n\tps.s_txreq = get_waiting_verbs_txreq(ps.wait);\n\tdo {\n\t\t \n\t\tif (ps.s_txreq) {\n\t\t\tif (priv->s_flags & HFI1_S_TID_BUSY_SET) {\n\t\t\t\tqp->s_flags |= RVT_S_BUSY;\n\t\t\t\tps.wait = iowait_get_ib_work(&priv->s_iowait);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&qp->s_lock, ps.flags);\n\n\t\t\t \n\t\t\tif (hfi1_verbs_send(qp, &ps))\n\t\t\t\treturn;\n\n\t\t\t \n\t\t\tif (hfi1_schedule_send_yield(qp, &ps, true))\n\t\t\t\treturn;\n\n\t\t\tspin_lock_irqsave(&qp->s_lock, ps.flags);\n\t\t\tif (priv->s_flags & HFI1_S_TID_BUSY_SET) {\n\t\t\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\t\t\tpriv->s_flags &= ~HFI1_S_TID_BUSY_SET;\n\t\t\t\tps.wait = iowait_get_tid_work(&priv->s_iowait);\n\t\t\t\tif (iowait_flag_set(&priv->s_iowait,\n\t\t\t\t\t\t    IOWAIT_PENDING_IB))\n\t\t\t\t\thfi1_schedule_send(qp);\n\t\t\t}\n\t\t}\n\t} while (hfi1_make_tid_rdma_pkt(qp, &ps));\n\tiowait_starve_clear(ps.pkts_sent, &priv->s_iowait);\n\tspin_unlock_irqrestore(&qp->s_lock, ps.flags);\n}\n\nstatic bool _hfi1_schedule_tid_send(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct hfi1_ibport *ibp =\n\t\tto_iport(qp->ibqp.device, qp->port_num);\n\tstruct hfi1_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\tif ((dd->flags & HFI1_SHUTDOWN))\n\t\treturn true;\n\n\treturn iowait_tid_schedule(&priv->s_iowait, ppd->hfi1_wq,\n\t\t\t\t   priv->s_sde ?\n\t\t\t\t   priv->s_sde->cpu :\n\t\t\t\t   cpumask_first(cpumask_of_node(dd->node)));\n}\n\n \nbool hfi1_schedule_tid_send(struct rvt_qp *qp)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\tif (hfi1_send_tid_ok(qp)) {\n\t\t \n\t\t_hfi1_schedule_tid_send(qp);\n\t\treturn true;\n\t}\n\tif (qp->s_flags & HFI1_S_ANY_WAIT_IO)\n\t\tiowait_set_flag(&((struct hfi1_qp_priv *)qp->priv)->s_iowait,\n\t\t\t\tIOWAIT_PENDING_TID);\n\treturn false;\n}\n\nbool hfi1_tid_rdma_ack_interlock(struct rvt_qp *qp, struct rvt_ack_entry *e)\n{\n\tstruct rvt_ack_entry *prev;\n\tstruct tid_rdma_request *req;\n\tstruct hfi1_ibdev *dev = to_idev(qp->ibqp.device);\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tu32 s_prev;\n\n\ts_prev = qp->s_tail_ack_queue == 0 ? rvt_size_atomic(&dev->rdi) :\n\t\t(qp->s_tail_ack_queue - 1);\n\tprev = &qp->s_ack_queue[s_prev];\n\n\tif ((e->opcode == TID_OP(READ_REQ) ||\n\t     e->opcode == OP(RDMA_READ_REQUEST)) &&\n\t    prev->opcode == TID_OP(WRITE_REQ)) {\n\t\treq = ack_to_tid_req(prev);\n\t\tif (req->ack_seg != req->total_segs) {\n\t\t\tpriv->s_flags |= HFI1_R_TID_WAIT_INTERLCK;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic u32 read_r_next_psn(struct hfi1_devdata *dd, u8 ctxt, u8 fidx)\n{\n\tu64 reg;\n\n\t \n\treg = read_uctxt_csr(dd, ctxt, RCV_TID_FLOW_TABLE + (8 * fidx));\n\treturn mask_psn(reg);\n}\n\nstatic void tid_rdma_rcv_err(struct hfi1_packet *packet,\n\t\t\t     struct ib_other_headers *ohdr,\n\t\t\t     struct rvt_qp *qp, u32 psn, int diff, bool fecn)\n{\n\tunsigned long flags;\n\n\ttid_rdma_rcv_error(packet, ohdr, qp, psn, diff);\n\tif (fecn) {\n\t\tspin_lock_irqsave(&qp->s_lock, flags);\n\t\tqp->s_flags |= RVT_S_ECN;\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t}\n}\n\nstatic void update_r_next_psn_fecn(struct hfi1_packet *packet,\n\t\t\t\t   struct hfi1_qp_priv *priv,\n\t\t\t\t   struct hfi1_ctxtdata *rcd,\n\t\t\t\t   struct tid_rdma_flow *flow,\n\t\t\t\t   bool fecn)\n{\n\t \n\tif (fecn && packet->etype == RHF_RCV_TYPE_EAGER &&\n\t    !(priv->s_flags & HFI1_R_TID_SW_PSN)) {\n\t\tstruct hfi1_devdata *dd = rcd->dd;\n\n\t\tflow->flow_state.r_next_psn =\n\t\t\tread_r_next_psn(dd, rcd->ctxt, flow->idx);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}