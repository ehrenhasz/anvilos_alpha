{
  "module_name": "chip.c",
  "hash_id": "4f233d174898fce8ee1e627af20a33e2cadcc7140fd7b5643e23e3c2ba81c9fb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/chip.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n\n#include \"hfi.h\"\n#include \"trace.h\"\n#include \"mad.h\"\n#include \"pio.h\"\n#include \"sdma.h\"\n#include \"eprom.h\"\n#include \"efivar.h\"\n#include \"platform.h\"\n#include \"aspm.h\"\n#include \"affinity.h\"\n#include \"debugfs.h\"\n#include \"fault.h\"\n#include \"netdev.h\"\n\nuint num_vls = HFI1_MAX_VLS_SUPPORTED;\nmodule_param(num_vls, uint, S_IRUGO);\nMODULE_PARM_DESC(num_vls, \"Set number of Virtual Lanes to use (1-8)\");\n\n \nuint rcv_intr_timeout = (824 + 16);  \nmodule_param(rcv_intr_timeout, uint, S_IRUGO);\nMODULE_PARM_DESC(rcv_intr_timeout, \"Receive interrupt mitigation timeout in ns\");\n\nuint rcv_intr_count = 16;  \nmodule_param(rcv_intr_count, uint, S_IRUGO);\nMODULE_PARM_DESC(rcv_intr_count, \"Receive interrupt mitigation count\");\n\nushort link_crc_mask = SUPPORTED_CRCS;\nmodule_param(link_crc_mask, ushort, S_IRUGO);\nMODULE_PARM_DESC(link_crc_mask, \"CRCs to use on the link\");\n\nuint loopback;\nmodule_param_named(loopback, loopback, uint, S_IRUGO);\nMODULE_PARM_DESC(loopback, \"Put into loopback mode (1 = serdes, 3 = external cable\");\n\n \nuint rcv_intr_dynamic = 1;  \nstatic ushort crc_14b_sideband = 1;\nstatic uint use_flr = 1;\nuint quick_linkup;  \n\nstruct flag_table {\n\tu64 flag;\t \n\tchar *str;\t \n\tu16 extra;\t \n\tu16 unused0;\n\tu32 unused1;\n};\n\n \n#define FLAG_ENTRY(str, extra, flag) {flag, str, extra}\n#define FLAG_ENTRY0(str, flag) {flag, str, 0}\n\n \n#define SEC_WRITE_DROPPED\t0x1\n#define SEC_PACKET_DROPPED\t0x2\n#define SEC_SC_HALTED\t\t0x4\t \n#define SEC_SPC_FREEZE\t\t0x8\t \n\n#define DEFAULT_KRCVQS\t\t  2\n#define MIN_KERNEL_KCTXTS         2\n#define FIRST_KERNEL_KCTXT        1\n\n \n#define RSM_INS_FECN              0\n#define RSM_INS_VNIC              1\n#define RSM_INS_AIP               2\n#define RSM_INS_VERBS             3\n\n \n#define GUID_HFI_INDEX_SHIFT     39\n\n \n#define emulator_rev(dd) ((dd)->irev >> 8)\n \n#define is_emulator_p(dd) ((((dd)->irev) & 0xf) == 3)\n#define is_emulator_s(dd) ((((dd)->irev) & 0xf) == 4)\n\n \n \n#define IB_PACKET_TYPE         2ull\n#define QW_SHIFT               6ull\n \n#define QPN_WIDTH              7ull\n\n \n#define LRH_BTH_QW             0ull\n#define LRH_BTH_BIT_OFFSET     48ull\n#define LRH_BTH_OFFSET(off)    ((LRH_BTH_QW << QW_SHIFT) | (off))\n#define LRH_BTH_MATCH_OFFSET   LRH_BTH_OFFSET(LRH_BTH_BIT_OFFSET)\n#define LRH_BTH_SELECT\n#define LRH_BTH_MASK           3ull\n#define LRH_BTH_VALUE          2ull\n\n \n#define LRH_SC_QW              0ull\n#define LRH_SC_BIT_OFFSET      56ull\n#define LRH_SC_OFFSET(off)     ((LRH_SC_QW << QW_SHIFT) | (off))\n#define LRH_SC_MATCH_OFFSET    LRH_SC_OFFSET(LRH_SC_BIT_OFFSET)\n#define LRH_SC_MASK            128ull\n#define LRH_SC_VALUE           0ull\n\n \n#define LRH_SC_SELECT_OFFSET  ((LRH_SC_QW << QW_SHIFT) | (60ull))\n\n \n#define QPN_SELECT_OFFSET      ((1ull << QW_SHIFT) | (1ull))\n\n \n \n\n \n#define BTH_DESTQP_QW           1ull\n#define BTH_DESTQP_BIT_OFFSET   16ull\n#define BTH_DESTQP_OFFSET(off) ((BTH_DESTQP_QW << QW_SHIFT) | (off))\n#define BTH_DESTQP_MATCH_OFFSET BTH_DESTQP_OFFSET(BTH_DESTQP_BIT_OFFSET)\n#define BTH_DESTQP_MASK         0xFFull\n#define BTH_DESTQP_VALUE        0x81ull\n\n \n \n#define DETH_AIP_SQPN_QW 3ull\n#define DETH_AIP_SQPN_BIT_OFFSET 56ull\n#define DETH_AIP_SQPN_OFFSET(off) ((DETH_AIP_SQPN_QW << QW_SHIFT) | (off))\n#define DETH_AIP_SQPN_SELECT_OFFSET \\\n\tDETH_AIP_SQPN_OFFSET(DETH_AIP_SQPN_BIT_OFFSET)\n\n \n \n#define L2_TYPE_QW             0ull\n#define L2_TYPE_BIT_OFFSET     61ull\n#define L2_TYPE_OFFSET(off)    ((L2_TYPE_QW << QW_SHIFT) | (off))\n#define L2_TYPE_MATCH_OFFSET   L2_TYPE_OFFSET(L2_TYPE_BIT_OFFSET)\n#define L2_TYPE_MASK           3ull\n#define L2_16B_VALUE           2ull\n\n \n#define L4_TYPE_QW              1ull\n#define L4_TYPE_BIT_OFFSET      0ull\n#define L4_TYPE_OFFSET(off)     ((L4_TYPE_QW << QW_SHIFT) | (off))\n#define L4_TYPE_MATCH_OFFSET    L4_TYPE_OFFSET(L4_TYPE_BIT_OFFSET)\n#define L4_16B_TYPE_MASK        0xFFull\n#define L4_16B_ETH_VALUE        0x78ull\n\n \n#define L4_16B_HDR_VESWID_OFFSET  ((2 << QW_SHIFT) | (16ull))\n \n#define L2_16B_ENTROPY_OFFSET     ((1 << QW_SHIFT) | (32ull))\n\n \n#define SC2VL_VAL( \\\n\tnum, \\\n\tsc0, sc0val, \\\n\tsc1, sc1val, \\\n\tsc2, sc2val, \\\n\tsc3, sc3val, \\\n\tsc4, sc4val, \\\n\tsc5, sc5val, \\\n\tsc6, sc6val, \\\n\tsc7, sc7val) \\\n( \\\n\t((u64)(sc0val) << SEND_SC2VLT##num##_SC##sc0##_SHIFT) | \\\n\t((u64)(sc1val) << SEND_SC2VLT##num##_SC##sc1##_SHIFT) | \\\n\t((u64)(sc2val) << SEND_SC2VLT##num##_SC##sc2##_SHIFT) | \\\n\t((u64)(sc3val) << SEND_SC2VLT##num##_SC##sc3##_SHIFT) | \\\n\t((u64)(sc4val) << SEND_SC2VLT##num##_SC##sc4##_SHIFT) | \\\n\t((u64)(sc5val) << SEND_SC2VLT##num##_SC##sc5##_SHIFT) | \\\n\t((u64)(sc6val) << SEND_SC2VLT##num##_SC##sc6##_SHIFT) | \\\n\t((u64)(sc7val) << SEND_SC2VLT##num##_SC##sc7##_SHIFT)   \\\n)\n\n#define DC_SC_VL_VAL( \\\n\trange, \\\n\te0, e0val, \\\n\te1, e1val, \\\n\te2, e2val, \\\n\te3, e3val, \\\n\te4, e4val, \\\n\te5, e5val, \\\n\te6, e6val, \\\n\te7, e7val, \\\n\te8, e8val, \\\n\te9, e9val, \\\n\te10, e10val, \\\n\te11, e11val, \\\n\te12, e12val, \\\n\te13, e13val, \\\n\te14, e14val, \\\n\te15, e15val) \\\n( \\\n\t((u64)(e0val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e0##_SHIFT) | \\\n\t((u64)(e1val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e1##_SHIFT) | \\\n\t((u64)(e2val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e2##_SHIFT) | \\\n\t((u64)(e3val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e3##_SHIFT) | \\\n\t((u64)(e4val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e4##_SHIFT) | \\\n\t((u64)(e5val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e5##_SHIFT) | \\\n\t((u64)(e6val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e6##_SHIFT) | \\\n\t((u64)(e7val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e7##_SHIFT) | \\\n\t((u64)(e8val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e8##_SHIFT) | \\\n\t((u64)(e9val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e9##_SHIFT) | \\\n\t((u64)(e10val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e10##_SHIFT) | \\\n\t((u64)(e11val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e11##_SHIFT) | \\\n\t((u64)(e12val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e12##_SHIFT) | \\\n\t((u64)(e13val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e13##_SHIFT) | \\\n\t((u64)(e14val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e14##_SHIFT) | \\\n\t((u64)(e15val) << DCC_CFG_SC_VL_TABLE_##range##_ENTRY##e15##_SHIFT) \\\n)\n\n \n#define ALL_FROZE (CCE_STATUS_SDMA_FROZE_SMASK \\\n\t\t\t| CCE_STATUS_RXE_FROZE_SMASK \\\n\t\t\t| CCE_STATUS_TXE_FROZE_SMASK \\\n\t\t\t| CCE_STATUS_TXE_PIO_FROZE_SMASK)\n \n#define ALL_TXE_PAUSE (CCE_STATUS_TXE_PIO_PAUSED_SMASK \\\n\t\t\t| CCE_STATUS_TXE_PAUSED_SMASK \\\n\t\t\t| CCE_STATUS_SDMA_PAUSED_SMASK)\n \n#define ALL_RXE_PAUSE CCE_STATUS_RXE_PAUSED_SMASK\n\n#define CNTR_MAX 0xFFFFFFFFFFFFFFFFULL\n#define CNTR_32BIT_MAX 0x00000000FFFFFFFF\n\n \nstatic struct flag_table cce_err_status_flags[] = {\n \tFLAG_ENTRY0(\"CceCsrParityErr\",\n\t\tCCE_ERR_STATUS_CCE_CSR_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCsrReadBadAddrErr\",\n\t\tCCE_ERR_STATUS_CCE_CSR_READ_BAD_ADDR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCsrWriteBadAddrErr\",\n\t\tCCE_ERR_STATUS_CCE_CSR_WRITE_BAD_ADDR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceTrgtAsyncFifoParityErr\",\n\t\tCCE_ERR_STATUS_CCE_TRGT_ASYNC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceTrgtAccessErr\",\n\t\tCCE_ERR_STATUS_CCE_TRGT_ACCESS_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceRspdDataParityErr\",\n\t\tCCE_ERR_STATUS_CCE_RSPD_DATA_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCli0AsyncFifoParityErr\",\n\t\tCCE_ERR_STATUS_CCE_CLI0_ASYNC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCsrCfgBusParityErr\",\n\t\tCCE_ERR_STATUS_CCE_CSR_CFG_BUS_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCli2AsyncFifoParityErr\",\n\t\tCCE_ERR_STATUS_CCE_CLI2_ASYNC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCli1AsyncFifoPioCrdtParityErr\",\n\t    CCE_ERR_STATUS_CCE_CLI1_ASYNC_FIFO_PIO_CRDT_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCli1AsyncFifoPioCrdtParityErr\",\n\t    CCE_ERR_STATUS_CCE_CLI1_ASYNC_FIFO_SDMA_HD_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceCli1AsyncFifoRxdmaParityError\",\n\t    CCE_ERR_STATUS_CCE_CLI1_ASYNC_FIFO_RXDMA_PARITY_ERROR_SMASK),\n \tFLAG_ENTRY0(\"CceCli1AsyncFifoDbgParityError\",\n\t\tCCE_ERR_STATUS_CCE_CLI1_ASYNC_FIFO_DBG_PARITY_ERROR_SMASK),\n \tFLAG_ENTRY0(\"PcicRetryMemCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_RETRY_MEM_COR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicRetryMemCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_RETRY_SOT_MEM_COR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicPostHdQCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_POST_HD_QCOR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicPostHdQCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_POST_DAT_QCOR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicPostHdQCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_CPL_HD_QCOR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicCplDatQCorErr\",\n\t\tCCE_ERR_STATUS_PCIC_CPL_DAT_QCOR_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicNPostHQParityErr\",\n\t\tCCE_ERR_STATUS_PCIC_NPOST_HQ_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicNPostDatQParityErr\",\n\t\tCCE_ERR_STATUS_PCIC_NPOST_DAT_QPARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicRetryMemUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_RETRY_MEM_UNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicRetrySotMemUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_RETRY_SOT_MEM_UNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicPostHdQUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_POST_HD_QUNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicPostDatQUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_POST_DAT_QUNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicCplHdQUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_CPL_HD_QUNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicCplDatQUncErr\",\n\t\tCCE_ERR_STATUS_PCIC_CPL_DAT_QUNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicTransmitFrontParityErr\",\n\t\tCCE_ERR_STATUS_PCIC_TRANSMIT_FRONT_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicTransmitBackParityErr\",\n\t\tCCE_ERR_STATUS_PCIC_TRANSMIT_BACK_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"PcicReceiveParityErr\",\n\t\tCCE_ERR_STATUS_PCIC_RECEIVE_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceTrgtCplTimeoutErr\",\n\t\tCCE_ERR_STATUS_CCE_TRGT_CPL_TIMEOUT_ERR_SMASK),\n \tFLAG_ENTRY0(\"LATriggered\",\n\t\tCCE_ERR_STATUS_LA_TRIGGERED_SMASK),\n \tFLAG_ENTRY0(\"CceSegReadBadAddrErr\",\n\t\tCCE_ERR_STATUS_CCE_SEG_READ_BAD_ADDR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceSegWriteBadAddrErr\",\n\t\tCCE_ERR_STATUS_CCE_SEG_WRITE_BAD_ADDR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceRcplAsyncFifoParityErr\",\n\t\tCCE_ERR_STATUS_CCE_RCPL_ASYNC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceRxdmaConvFifoParityErr\",\n\t\tCCE_ERR_STATUS_CCE_RXDMA_CONV_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceMsixTableCorErr\",\n\t\tCCE_ERR_STATUS_CCE_MSIX_TABLE_COR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceMsixTableUncErr\",\n\t\tCCE_ERR_STATUS_CCE_MSIX_TABLE_UNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceIntMapCorErr\",\n\t\tCCE_ERR_STATUS_CCE_INT_MAP_COR_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceIntMapUncErr\",\n\t\tCCE_ERR_STATUS_CCE_INT_MAP_UNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"CceMsixCsrParityErr\",\n\t\tCCE_ERR_STATUS_CCE_MSIX_CSR_PARITY_ERR_SMASK),\n \n};\n\n \n#define MES(text) MISC_ERR_STATUS_MISC_##text##_ERR_SMASK\nstatic struct flag_table misc_err_status_flags[] = {\n \tFLAG_ENTRY0(\"CSR_PARITY\", MES(CSR_PARITY)),\n \tFLAG_ENTRY0(\"CSR_READ_BAD_ADDR\", MES(CSR_READ_BAD_ADDR)),\n \tFLAG_ENTRY0(\"CSR_WRITE_BAD_ADDR\", MES(CSR_WRITE_BAD_ADDR)),\n \tFLAG_ENTRY0(\"SBUS_WRITE_FAILED\", MES(SBUS_WRITE_FAILED)),\n \tFLAG_ENTRY0(\"KEY_MISMATCH\", MES(KEY_MISMATCH)),\n \tFLAG_ENTRY0(\"FW_AUTH_FAILED\", MES(FW_AUTH_FAILED)),\n \tFLAG_ENTRY0(\"EFUSE_CSR_PARITY\", MES(EFUSE_CSR_PARITY)),\n \tFLAG_ENTRY0(\"EFUSE_READ_BAD_ADDR\", MES(EFUSE_READ_BAD_ADDR)),\n \tFLAG_ENTRY0(\"EFUSE_WRITE\", MES(EFUSE_WRITE)),\n \tFLAG_ENTRY0(\"EFUSE_DONE_PARITY\", MES(EFUSE_DONE_PARITY)),\n \tFLAG_ENTRY0(\"INVALID_EEP_CMD\", MES(INVALID_EEP_CMD)),\n \tFLAG_ENTRY0(\"MBIST_FAIL\", MES(MBIST_FAIL)),\n \tFLAG_ENTRY0(\"PLL_LOCK_FAIL\", MES(PLL_LOCK_FAIL))\n};\n\n \nstatic struct flag_table pio_err_status_flags[] = {\n \tFLAG_ENTRY(\"PioWriteBadCtxt\",\n\tSEC_WRITE_DROPPED,\n\tSEND_PIO_ERR_STATUS_PIO_WRITE_BAD_CTXT_ERR_SMASK),\n \tFLAG_ENTRY(\"PioWriteAddrParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_WRITE_ADDR_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioCsrParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_CSR_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioSbMemFifo0\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_SB_MEM_FIFO0_ERR_SMASK),\n \tFLAG_ENTRY(\"PioSbMemFifo1\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_SB_MEM_FIFO1_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPccFifoParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PCC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPecFifoParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PEC_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioSbrdctlCrrelParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_SBRDCTL_CRREL_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioSbrdctrlCrrelFifoParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_SBRDCTRL_CRREL_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPktEvictFifoParityErr\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PKT_EVICT_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioSmPktResetParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_SM_PKT_RESET_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlLenMemBank0Unc\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK0_UNC_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlLenMemBank1Unc\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK1_UNC_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlLenMemBank0Cor\",\n\t0,\n\tSEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK0_COR_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlLenMemBank1Cor\",\n\t0,\n\tSEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK1_COR_ERR_SMASK),\n \tFLAG_ENTRY(\"PioCreditRetFifoParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_CREDIT_RET_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPpmcPblFifo\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PPMC_PBL_FIFO_ERR_SMASK),\n \tFLAG_ENTRY(\"PioInitSmIn\",\n\t0,\n\tSEND_PIO_ERR_STATUS_PIO_INIT_SM_IN_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPktEvictSmOrArbSm\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PKT_EVICT_SM_OR_ARB_SM_ERR_SMASK),\n \tFLAG_ENTRY(\"PioHostAddrMemUnc\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_HOST_ADDR_MEM_UNC_ERR_SMASK),\n \tFLAG_ENTRY(\"PioHostAddrMemCor\",\n\t0,\n\tSEND_PIO_ERR_STATUS_PIO_HOST_ADDR_MEM_COR_ERR_SMASK),\n \tFLAG_ENTRY(\"PioWriteDataParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_WRITE_DATA_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioStateMachine\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_STATE_MACHINE_ERR_SMASK),\n \tFLAG_ENTRY(\"PioWriteQwValidParity\",\n\tSEC_WRITE_DROPPED | SEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_WRITE_QW_VALID_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioBlockQwCountParity\",\n\tSEC_WRITE_DROPPED | SEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_BLOCK_QW_COUNT_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlfVlLenParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_VLF_VL_LEN_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlfSopParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_VLF_SOP_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioVlFifoParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_VL_FIFO_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPpmcBqcMemParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PPMC_BQC_MEM_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPpmcSopLen\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PPMC_SOP_LEN_ERR_SMASK),\n \n \tFLAG_ENTRY(\"PioCurrentFreeCntParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_CURRENT_FREE_CNT_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioLastReturnedCntParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_LAST_RETURNED_CNT_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPccSopHeadParity\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PCC_SOP_HEAD_PARITY_ERR_SMASK),\n \tFLAG_ENTRY(\"PioPecSopHeadParityErr\",\n\tSEC_SPC_FREEZE,\n\tSEND_PIO_ERR_STATUS_PIO_PEC_SOP_HEAD_PARITY_ERR_SMASK),\n \n};\n\n \n#define ALL_PIO_FREEZE_ERR \\\n\t(SEND_PIO_ERR_STATUS_PIO_WRITE_ADDR_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_CSR_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_SB_MEM_FIFO0_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_SB_MEM_FIFO1_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PCC_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PEC_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_SBRDCTL_CRREL_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_SBRDCTRL_CRREL_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PKT_EVICT_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_SM_PKT_RESET_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK0_UNC_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_VL_LEN_MEM_BANK1_UNC_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_CREDIT_RET_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PPMC_PBL_FIFO_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PKT_EVICT_SM_OR_ARB_SM_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_HOST_ADDR_MEM_UNC_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_WRITE_DATA_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_STATE_MACHINE_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_WRITE_QW_VALID_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_BLOCK_QW_COUNT_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_VLF_VL_LEN_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_VLF_SOP_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_VL_FIFO_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PPMC_BQC_MEM_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PPMC_SOP_LEN_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_CURRENT_FREE_CNT_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_LAST_RETURNED_CNT_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PCC_SOP_HEAD_PARITY_ERR_SMASK \\\n\t| SEND_PIO_ERR_STATUS_PIO_PEC_SOP_HEAD_PARITY_ERR_SMASK)\n\n \nstatic struct flag_table sdma_err_status_flags[] = {\n \tFLAG_ENTRY0(\"SDmaRpyTagErr\",\n\t\tSEND_DMA_ERR_STATUS_SDMA_RPY_TAG_ERR_SMASK),\n \tFLAG_ENTRY0(\"SDmaCsrParityErr\",\n\t\tSEND_DMA_ERR_STATUS_SDMA_CSR_PARITY_ERR_SMASK),\n \tFLAG_ENTRY0(\"SDmaPcieReqTrackingUncErr\",\n\t\tSEND_DMA_ERR_STATUS_SDMA_PCIE_REQ_TRACKING_UNC_ERR_SMASK),\n \tFLAG_ENTRY0(\"SDmaPcieReqTrackingCorErr\",\n\t\tSEND_DMA_ERR_STATUS_SDMA_PCIE_REQ_TRACKING_COR_ERR_SMASK),\n \n};\n\n \n#define ALL_SDMA_FREEZE_ERR  \\\n\t\t(SEND_DMA_ERR_STATUS_SDMA_RPY_TAG_ERR_SMASK \\\n\t\t| SEND_DMA_ERR_STATUS_SDMA_CSR_PARITY_ERR_SMASK \\\n\t\t| SEND_DMA_ERR_STATUS_SDMA_PCIE_REQ_TRACKING_UNC_ERR_SMASK)\n\n \n#define PORT_DISCARD_EGRESS_ERRS \\\n\t(SEND_EGRESS_ERR_INFO_TOO_LONG_IB_PACKET_ERR_SMASK \\\n\t| SEND_EGRESS_ERR_INFO_VL_MAPPING_ERR_SMASK \\\n\t| SEND_EGRESS_ERR_INFO_VL_ERR_SMASK)\n\n \n#define SEES(text) SEND_EGRESS_ERR_STATUS_##text##_ERR_SMASK\nstatic struct flag_table egress_err_status_flags[] = {\n \tFLAG_ENTRY0(\"TxPktIntegrityMemCorErr\", SEES(TX_PKT_INTEGRITY_MEM_COR)),\n \tFLAG_ENTRY0(\"TxPktIntegrityMemUncErr\", SEES(TX_PKT_INTEGRITY_MEM_UNC)),\n \n \tFLAG_ENTRY0(\"TxEgressFifoUnderrunOrParityErr\",\n\t\tSEES(TX_EGRESS_FIFO_UNDERRUN_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLinkdownErr\", SEES(TX_LINKDOWN)),\n \tFLAG_ENTRY0(\"TxIncorrectLinkStateErr\", SEES(TX_INCORRECT_LINK_STATE)),\n \n \tFLAG_ENTRY0(\"TxPioLaunchIntfParityErr\",\n\t\tSEES(TX_PIO_LAUNCH_INTF_PARITY)),\n \tFLAG_ENTRY0(\"TxSdmaLaunchIntfParityErr\",\n\t\tSEES(TX_SDMA_LAUNCH_INTF_PARITY)),\n \n \tFLAG_ENTRY0(\"TxSbrdCtlStateMachineParityErr\",\n\t\tSEES(TX_SBRD_CTL_STATE_MACHINE_PARITY)),\n \tFLAG_ENTRY0(\"TxIllegalVLErr\", SEES(TX_ILLEGAL_VL)),\n \tFLAG_ENTRY0(\"TxLaunchCsrParityErr\", SEES(TX_LAUNCH_CSR_PARITY)),\n \tFLAG_ENTRY0(\"TxSbrdCtlCsrParityErr\", SEES(TX_SBRD_CTL_CSR_PARITY)),\n \tFLAG_ENTRY0(\"TxConfigParityErr\", SEES(TX_CONFIG_PARITY)),\n \tFLAG_ENTRY0(\"TxSdma0DisallowedPacketErr\",\n\t\tSEES(TX_SDMA0_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma1DisallowedPacketErr\",\n\t\tSEES(TX_SDMA1_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma2DisallowedPacketErr\",\n\t\tSEES(TX_SDMA2_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma3DisallowedPacketErr\",\n\t\tSEES(TX_SDMA3_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma4DisallowedPacketErr\",\n\t\tSEES(TX_SDMA4_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma5DisallowedPacketErr\",\n\t\tSEES(TX_SDMA5_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma6DisallowedPacketErr\",\n\t\tSEES(TX_SDMA6_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma7DisallowedPacketErr\",\n\t\tSEES(TX_SDMA7_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma8DisallowedPacketErr\",\n\t\tSEES(TX_SDMA8_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma9DisallowedPacketErr\",\n\t\tSEES(TX_SDMA9_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma10DisallowedPacketErr\",\n\t\tSEES(TX_SDMA10_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma11DisallowedPacketErr\",\n\t\tSEES(TX_SDMA11_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma12DisallowedPacketErr\",\n\t\tSEES(TX_SDMA12_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma13DisallowedPacketErr\",\n\t\tSEES(TX_SDMA13_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma14DisallowedPacketErr\",\n\t\tSEES(TX_SDMA14_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxSdma15DisallowedPacketErr\",\n\t\tSEES(TX_SDMA15_DISALLOWED_PACKET)),\n \tFLAG_ENTRY0(\"TxLaunchFifo0UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO0_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo1UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO1_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo2UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO2_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo3UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO3_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo4UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO4_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo5UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO5_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo6UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO6_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo7UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO7_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxLaunchFifo8UncOrParityErr\",\n\t\tSEES(TX_LAUNCH_FIFO8_UNC_OR_PARITY)),\n \tFLAG_ENTRY0(\"TxCreditReturnParityErr\", SEES(TX_CREDIT_RETURN_PARITY)),\n \tFLAG_ENTRY0(\"TxSbHdrUncErr\", SEES(TX_SB_HDR_UNC)),\n \tFLAG_ENTRY0(\"TxReadSdmaMemoryUncErr\", SEES(TX_READ_SDMA_MEMORY_UNC)),\n \tFLAG_ENTRY0(\"TxReadPioMemoryUncErr\", SEES(TX_READ_PIO_MEMORY_UNC)),\n \tFLAG_ENTRY0(\"TxEgressFifoUncErr\", SEES(TX_EGRESS_FIFO_UNC)),\n \tFLAG_ENTRY0(\"TxHcrcInsertionErr\", SEES(TX_HCRC_INSERTION)),\n \tFLAG_ENTRY0(\"TxCreditReturnVLErr\", SEES(TX_CREDIT_RETURN_VL)),\n \tFLAG_ENTRY0(\"TxLaunchFifo0CorErr\", SEES(TX_LAUNCH_FIFO0_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo1CorErr\", SEES(TX_LAUNCH_FIFO1_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo2CorErr\", SEES(TX_LAUNCH_FIFO2_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo3CorErr\", SEES(TX_LAUNCH_FIFO3_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo4CorErr\", SEES(TX_LAUNCH_FIFO4_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo5CorErr\", SEES(TX_LAUNCH_FIFO5_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo6CorErr\", SEES(TX_LAUNCH_FIFO6_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo7CorErr\", SEES(TX_LAUNCH_FIFO7_COR)),\n \tFLAG_ENTRY0(\"TxLaunchFifo8CorErr\", SEES(TX_LAUNCH_FIFO8_COR)),\n \tFLAG_ENTRY0(\"TxCreditOverrunErr\", SEES(TX_CREDIT_OVERRUN)),\n \tFLAG_ENTRY0(\"TxSbHdrCorErr\", SEES(TX_SB_HDR_COR)),\n \tFLAG_ENTRY0(\"TxReadSdmaMemoryCorErr\", SEES(TX_READ_SDMA_MEMORY_COR)),\n \tFLAG_ENTRY0(\"TxReadPioMemoryCorErr\", SEES(TX_READ_PIO_MEMORY_COR)),\n \tFLAG_ENTRY0(\"TxEgressFifoCorErr\", SEES(TX_EGRESS_FIFO_COR)),\n \tFLAG_ENTRY0(\"TxReadSdmaMemoryCsrUncErr\",\n\t\tSEES(TX_READ_SDMA_MEMORY_CSR_UNC)),\n \tFLAG_ENTRY0(\"TxReadPioMemoryCsrUncErr\",\n\t\tSEES(TX_READ_PIO_MEMORY_CSR_UNC)),\n};\n\n \n#define SEEI(text) SEND_EGRESS_ERR_INFO_##text##_ERR_SMASK\nstatic struct flag_table egress_err_info_flags[] = {\n \tFLAG_ENTRY0(\"Reserved\", 0ull),\n \tFLAG_ENTRY0(\"VLErr\", SEEI(VL)),\n \tFLAG_ENTRY0(\"JobKeyErr\", SEEI(JOB_KEY)),\n \tFLAG_ENTRY0(\"JobKeyErr\", SEEI(JOB_KEY)),\n \tFLAG_ENTRY0(\"PartitionKeyErr\", SEEI(PARTITION_KEY)),\n \tFLAG_ENTRY0(\"SLIDErr\", SEEI(SLID)),\n \tFLAG_ENTRY0(\"OpcodeErr\", SEEI(OPCODE)),\n \tFLAG_ENTRY0(\"VLMappingErr\", SEEI(VL_MAPPING)),\n \tFLAG_ENTRY0(\"RawErr\", SEEI(RAW)),\n \tFLAG_ENTRY0(\"RawIPv6Err\", SEEI(RAW_IPV6)),\n \tFLAG_ENTRY0(\"GRHErr\", SEEI(GRH)),\n \tFLAG_ENTRY0(\"BypassErr\", SEEI(BYPASS)),\n \tFLAG_ENTRY0(\"KDETHPacketsErr\", SEEI(KDETH_PACKETS)),\n \tFLAG_ENTRY0(\"NonKDETHPacketsErr\", SEEI(NON_KDETH_PACKETS)),\n \tFLAG_ENTRY0(\"TooSmallIBPacketsErr\", SEEI(TOO_SMALL_IB_PACKETS)),\n \tFLAG_ENTRY0(\"TooSmallBypassPacketsErr\", SEEI(TOO_SMALL_BYPASS_PACKETS)),\n \tFLAG_ENTRY0(\"PbcTestErr\", SEEI(PBC_TEST)),\n \tFLAG_ENTRY0(\"BadPktLenErr\", SEEI(BAD_PKT_LEN)),\n \tFLAG_ENTRY0(\"TooLongIBPacketErr\", SEEI(TOO_LONG_IB_PACKET)),\n \tFLAG_ENTRY0(\"TooLongBypassPacketsErr\", SEEI(TOO_LONG_BYPASS_PACKETS)),\n \tFLAG_ENTRY0(\"PbcStaticRateControlErr\", SEEI(PBC_STATIC_RATE_CONTROL)),\n \tFLAG_ENTRY0(\"BypassBadPktLenErr\", SEEI(BAD_PKT_LEN)),\n};\n\n \n#define ALL_TXE_EGRESS_FREEZE_ERR \\\n\t(SEES(TX_EGRESS_FIFO_UNDERRUN_OR_PARITY) \\\n\t| SEES(TX_PIO_LAUNCH_INTF_PARITY) \\\n\t| SEES(TX_SDMA_LAUNCH_INTF_PARITY) \\\n\t| SEES(TX_SBRD_CTL_STATE_MACHINE_PARITY) \\\n\t| SEES(TX_LAUNCH_CSR_PARITY) \\\n\t| SEES(TX_SBRD_CTL_CSR_PARITY) \\\n\t| SEES(TX_CONFIG_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO0_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO1_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO2_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO3_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO4_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO5_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO6_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO7_UNC_OR_PARITY) \\\n\t| SEES(TX_LAUNCH_FIFO8_UNC_OR_PARITY) \\\n\t| SEES(TX_CREDIT_RETURN_PARITY))\n\n \n#define SES(name) SEND_ERR_STATUS_SEND_##name##_ERR_SMASK\nstatic struct flag_table send_err_status_flags[] = {\n \tFLAG_ENTRY0(\"SendCsrParityErr\", SES(CSR_PARITY)),\n \tFLAG_ENTRY0(\"SendCsrReadBadAddrErr\", SES(CSR_READ_BAD_ADDR)),\n \tFLAG_ENTRY0(\"SendCsrWriteBadAddrErr\", SES(CSR_WRITE_BAD_ADDR))\n};\n\n \nstatic struct flag_table sc_err_status_flags[] = {\n \tFLAG_ENTRY(\"InconsistentSop\",\n\t\tSEC_PACKET_DROPPED | SEC_SC_HALTED,\n\t\tSEND_CTXT_ERR_STATUS_PIO_INCONSISTENT_SOP_ERR_SMASK),\n \tFLAG_ENTRY(\"DisallowedPacket\",\n\t\tSEC_PACKET_DROPPED | SEC_SC_HALTED,\n\t\tSEND_CTXT_ERR_STATUS_PIO_DISALLOWED_PACKET_ERR_SMASK),\n \tFLAG_ENTRY(\"WriteCrossesBoundary\",\n\t\tSEC_WRITE_DROPPED | SEC_SC_HALTED,\n\t\tSEND_CTXT_ERR_STATUS_PIO_WRITE_CROSSES_BOUNDARY_ERR_SMASK),\n \tFLAG_ENTRY(\"WriteOverflow\",\n\t\tSEC_WRITE_DROPPED | SEC_SC_HALTED,\n\t\tSEND_CTXT_ERR_STATUS_PIO_WRITE_OVERFLOW_ERR_SMASK),\n \tFLAG_ENTRY(\"WriteOutOfBounds\",\n\t\tSEC_WRITE_DROPPED | SEC_SC_HALTED,\n\t\tSEND_CTXT_ERR_STATUS_PIO_WRITE_OUT_OF_BOUNDS_ERR_SMASK),\n \n};\n\n \n#define RXES(name) RCV_ERR_STATUS_RX_##name##_ERR_SMASK\nstatic struct flag_table rxe_err_status_flags[] = {\n \tFLAG_ENTRY0(\"RxDmaCsrCorErr\", RXES(DMA_CSR_COR)),\n \tFLAG_ENTRY0(\"RxDcIntfParityErr\", RXES(DC_INTF_PARITY)),\n \tFLAG_ENTRY0(\"RxRcvHdrUncErr\", RXES(RCV_HDR_UNC)),\n \tFLAG_ENTRY0(\"RxRcvHdrCorErr\", RXES(RCV_HDR_COR)),\n \tFLAG_ENTRY0(\"RxRcvDataUncErr\", RXES(RCV_DATA_UNC)),\n \tFLAG_ENTRY0(\"RxRcvDataCorErr\", RXES(RCV_DATA_COR)),\n \tFLAG_ENTRY0(\"RxRcvQpMapTableUncErr\", RXES(RCV_QP_MAP_TABLE_UNC)),\n \tFLAG_ENTRY0(\"RxRcvQpMapTableCorErr\", RXES(RCV_QP_MAP_TABLE_COR)),\n \tFLAG_ENTRY0(\"RxRcvCsrParityErr\", RXES(RCV_CSR_PARITY)),\n \tFLAG_ENTRY0(\"RxDcSopEopParityErr\", RXES(DC_SOP_EOP_PARITY)),\n \tFLAG_ENTRY0(\"RxDmaFlagUncErr\", RXES(DMA_FLAG_UNC)),\n \tFLAG_ENTRY0(\"RxDmaFlagCorErr\", RXES(DMA_FLAG_COR)),\n \tFLAG_ENTRY0(\"RxRcvFsmEncodingErr\", RXES(RCV_FSM_ENCODING)),\n \tFLAG_ENTRY0(\"RxRbufFreeListUncErr\", RXES(RBUF_FREE_LIST_UNC)),\n \tFLAG_ENTRY0(\"RxRbufFreeListCorErr\", RXES(RBUF_FREE_LIST_COR)),\n \tFLAG_ENTRY0(\"RxRbufLookupDesRegUncErr\", RXES(RBUF_LOOKUP_DES_REG_UNC)),\n \tFLAG_ENTRY0(\"RxRbufLookupDesRegUncCorErr\",\n\t\tRXES(RBUF_LOOKUP_DES_REG_UNC_COR)),\n \tFLAG_ENTRY0(\"RxRbufLookupDesUncErr\", RXES(RBUF_LOOKUP_DES_UNC)),\n \tFLAG_ENTRY0(\"RxRbufLookupDesCorErr\", RXES(RBUF_LOOKUP_DES_COR)),\n \tFLAG_ENTRY0(\"RxRbufBlockListReadUncErr\",\n\t\tRXES(RBUF_BLOCK_LIST_READ_UNC)),\n \tFLAG_ENTRY0(\"RxRbufBlockListReadCorErr\",\n\t\tRXES(RBUF_BLOCK_LIST_READ_COR)),\n \tFLAG_ENTRY0(\"RxRbufCsrQHeadBufNumParityErr\",\n\t\tRXES(RBUF_CSR_QHEAD_BUF_NUM_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQEntCntParityErr\",\n\t\tRXES(RBUF_CSR_QENT_CNT_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQNextBufParityErr\",\n\t\tRXES(RBUF_CSR_QNEXT_BUF_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQVldBitParityErr\",\n\t\tRXES(RBUF_CSR_QVLD_BIT_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQHdPtrParityErr\", RXES(RBUF_CSR_QHD_PTR_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQTlPtrParityErr\", RXES(RBUF_CSR_QTL_PTR_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQNumOfPktParityErr\",\n\t\tRXES(RBUF_CSR_QNUM_OF_PKT_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCsrQEOPDWParityErr\", RXES(RBUF_CSR_QEOPDW_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufCtxIdParityErr\", RXES(RBUF_CTX_ID_PARITY)),\n \tFLAG_ENTRY0(\"RxRBufBadLookupErr\", RXES(RBUF_BAD_LOOKUP)),\n \tFLAG_ENTRY0(\"RxRbufFullErr\", RXES(RBUF_FULL)),\n \tFLAG_ENTRY0(\"RxRbufEmptyErr\", RXES(RBUF_EMPTY)),\n \tFLAG_ENTRY0(\"RxRbufFlRdAddrParityErr\", RXES(RBUF_FL_RD_ADDR_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufFlWrAddrParityErr\", RXES(RBUF_FL_WR_ADDR_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufFlInitdoneParityErr\",\n\t\tRXES(RBUF_FL_INITDONE_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufFlInitWrAddrParityErr\",\n\t\tRXES(RBUF_FL_INIT_WR_ADDR_PARITY)),\n \tFLAG_ENTRY0(\"RxRbufNextFreeBufUncErr\", RXES(RBUF_NEXT_FREE_BUF_UNC)),\n \tFLAG_ENTRY0(\"RxRbufNextFreeBufCorErr\", RXES(RBUF_NEXT_FREE_BUF_COR)),\n \tFLAG_ENTRY0(\"RxLookupDesPart1UncErr\", RXES(LOOKUP_DES_PART1_UNC)),\n \tFLAG_ENTRY0(\"RxLookupDesPart1UncCorErr\",\n\t\tRXES(LOOKUP_DES_PART1_UNC_COR)),\n \tFLAG_ENTRY0(\"RxLookupDesPart2ParityErr\",\n\t\tRXES(LOOKUP_DES_PART2_PARITY)),\n \tFLAG_ENTRY0(\"RxLookupRcvArrayUncErr\", RXES(LOOKUP_RCV_ARRAY_UNC)),\n \tFLAG_ENTRY0(\"RxLookupRcvArrayCorErr\", RXES(LOOKUP_RCV_ARRAY_COR)),\n \tFLAG_ENTRY0(\"RxLookupCsrParityErr\", RXES(LOOKUP_CSR_PARITY)),\n \tFLAG_ENTRY0(\"RxHqIntrCsrParityErr\", RXES(HQ_INTR_CSR_PARITY)),\n \tFLAG_ENTRY0(\"RxHqIntrFsmErr\", RXES(HQ_INTR_FSM)),\n \tFLAG_ENTRY0(\"RxRbufDescPart1UncErr\", RXES(RBUF_DESC_PART1_UNC)),\n \tFLAG_ENTRY0(\"RxRbufDescPart1CorErr\", RXES(RBUF_DESC_PART1_COR)),\n \tFLAG_ENTRY0(\"RxRbufDescPart2UncErr\", RXES(RBUF_DESC_PART2_UNC)),\n \tFLAG_ENTRY0(\"RxRbufDescPart2CorErr\", RXES(RBUF_DESC_PART2_COR)),\n \tFLAG_ENTRY0(\"RxDmaHdrFifoRdUncErr\", RXES(DMA_HDR_FIFO_RD_UNC)),\n \tFLAG_ENTRY0(\"RxDmaHdrFifoRdCorErr\", RXES(DMA_HDR_FIFO_RD_COR)),\n \tFLAG_ENTRY0(\"RxDmaDataFifoRdUncErr\", RXES(DMA_DATA_FIFO_RD_UNC)),\n \tFLAG_ENTRY0(\"RxDmaDataFifoRdCorErr\", RXES(DMA_DATA_FIFO_RD_COR)),\n \tFLAG_ENTRY0(\"RxRbufDataUncErr\", RXES(RBUF_DATA_UNC)),\n \tFLAG_ENTRY0(\"RxRbufDataCorErr\", RXES(RBUF_DATA_COR)),\n \tFLAG_ENTRY0(\"RxDmaCsrParityErr\", RXES(DMA_CSR_PARITY)),\n \tFLAG_ENTRY0(\"RxDmaEqFsmEncodingErr\", RXES(DMA_EQ_FSM_ENCODING)),\n \tFLAG_ENTRY0(\"RxDmaDqFsmEncodingErr\", RXES(DMA_DQ_FSM_ENCODING)),\n \tFLAG_ENTRY0(\"RxDmaCsrUncErr\", RXES(DMA_CSR_UNC)),\n \tFLAG_ENTRY0(\"RxCsrReadBadAddrErr\", RXES(CSR_READ_BAD_ADDR)),\n \tFLAG_ENTRY0(\"RxCsrWriteBadAddrErr\", RXES(CSR_WRITE_BAD_ADDR)),\n \tFLAG_ENTRY0(\"RxCsrParityErr\", RXES(CSR_PARITY))\n};\n\n \n#define ALL_RXE_FREEZE_ERR  \\\n\t(RCV_ERR_STATUS_RX_RCV_QP_MAP_TABLE_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RCV_CSR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_FLAG_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RCV_FSM_ENCODING_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FREE_LIST_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_LOOKUP_DES_REG_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_LOOKUP_DES_REG_UNC_COR_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_LOOKUP_DES_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_BLOCK_LIST_READ_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QHEAD_BUF_NUM_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QENT_CNT_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QNEXT_BUF_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QVLD_BIT_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QHD_PTR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QTL_PTR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QNUM_OF_PKT_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CSR_QEOPDW_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_CTX_ID_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_BAD_LOOKUP_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FULL_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_EMPTY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FL_RD_ADDR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FL_WR_ADDR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FL_INITDONE_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_FL_INIT_WR_ADDR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_NEXT_FREE_BUF_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_LOOKUP_DES_PART1_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_LOOKUP_DES_PART1_UNC_COR_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_LOOKUP_DES_PART2_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_LOOKUP_RCV_ARRAY_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_LOOKUP_CSR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_HQ_INTR_CSR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_HQ_INTR_FSM_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_DESC_PART1_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_DESC_PART1_COR_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_DESC_PART2_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_HDR_FIFO_RD_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_DATA_FIFO_RD_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_RBUF_DATA_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_CSR_PARITY_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_EQ_FSM_ENCODING_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_DQ_FSM_ENCODING_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_DMA_CSR_UNC_ERR_SMASK \\\n\t| RCV_ERR_STATUS_RX_CSR_PARITY_ERR_SMASK)\n\n#define RXE_FREEZE_ABORT_MASK \\\n\t(RCV_ERR_STATUS_RX_DMA_CSR_UNC_ERR_SMASK | \\\n\tRCV_ERR_STATUS_RX_DMA_HDR_FIFO_RD_UNC_ERR_SMASK | \\\n\tRCV_ERR_STATUS_RX_DMA_DATA_FIFO_RD_UNC_ERR_SMASK)\n\n \n#define DCCE(name) DCC_ERR_FLG_##name##_SMASK\nstatic struct flag_table dcc_err_flags[] = {\n\tFLAG_ENTRY0(\"bad_l2_err\", DCCE(BAD_L2_ERR)),\n\tFLAG_ENTRY0(\"bad_sc_err\", DCCE(BAD_SC_ERR)),\n\tFLAG_ENTRY0(\"bad_mid_tail_err\", DCCE(BAD_MID_TAIL_ERR)),\n\tFLAG_ENTRY0(\"bad_preemption_err\", DCCE(BAD_PREEMPTION_ERR)),\n\tFLAG_ENTRY0(\"preemption_err\", DCCE(PREEMPTION_ERR)),\n\tFLAG_ENTRY0(\"preemptionvl15_err\", DCCE(PREEMPTIONVL15_ERR)),\n\tFLAG_ENTRY0(\"bad_vl_marker_err\", DCCE(BAD_VL_MARKER_ERR)),\n\tFLAG_ENTRY0(\"bad_dlid_target_err\", DCCE(BAD_DLID_TARGET_ERR)),\n\tFLAG_ENTRY0(\"bad_lver_err\", DCCE(BAD_LVER_ERR)),\n\tFLAG_ENTRY0(\"uncorrectable_err\", DCCE(UNCORRECTABLE_ERR)),\n\tFLAG_ENTRY0(\"bad_crdt_ack_err\", DCCE(BAD_CRDT_ACK_ERR)),\n\tFLAG_ENTRY0(\"unsup_pkt_type\", DCCE(UNSUP_PKT_TYPE)),\n\tFLAG_ENTRY0(\"bad_ctrl_flit_err\", DCCE(BAD_CTRL_FLIT_ERR)),\n\tFLAG_ENTRY0(\"event_cntr_parity_err\", DCCE(EVENT_CNTR_PARITY_ERR)),\n\tFLAG_ENTRY0(\"event_cntr_rollover_err\", DCCE(EVENT_CNTR_ROLLOVER_ERR)),\n\tFLAG_ENTRY0(\"link_err\", DCCE(LINK_ERR)),\n\tFLAG_ENTRY0(\"misc_cntr_rollover_err\", DCCE(MISC_CNTR_ROLLOVER_ERR)),\n\tFLAG_ENTRY0(\"bad_ctrl_dist_err\", DCCE(BAD_CTRL_DIST_ERR)),\n\tFLAG_ENTRY0(\"bad_tail_dist_err\", DCCE(BAD_TAIL_DIST_ERR)),\n\tFLAG_ENTRY0(\"bad_head_dist_err\", DCCE(BAD_HEAD_DIST_ERR)),\n\tFLAG_ENTRY0(\"nonvl15_state_err\", DCCE(NONVL15_STATE_ERR)),\n\tFLAG_ENTRY0(\"vl15_multi_err\", DCCE(VL15_MULTI_ERR)),\n\tFLAG_ENTRY0(\"bad_pkt_length_err\", DCCE(BAD_PKT_LENGTH_ERR)),\n\tFLAG_ENTRY0(\"unsup_vl_err\", DCCE(UNSUP_VL_ERR)),\n\tFLAG_ENTRY0(\"perm_nvl15_err\", DCCE(PERM_NVL15_ERR)),\n\tFLAG_ENTRY0(\"slid_zero_err\", DCCE(SLID_ZERO_ERR)),\n\tFLAG_ENTRY0(\"dlid_zero_err\", DCCE(DLID_ZERO_ERR)),\n\tFLAG_ENTRY0(\"length_mtu_err\", DCCE(LENGTH_MTU_ERR)),\n\tFLAG_ENTRY0(\"rx_early_drop_err\", DCCE(RX_EARLY_DROP_ERR)),\n\tFLAG_ENTRY0(\"late_short_err\", DCCE(LATE_SHORT_ERR)),\n\tFLAG_ENTRY0(\"late_long_err\", DCCE(LATE_LONG_ERR)),\n\tFLAG_ENTRY0(\"late_ebp_err\", DCCE(LATE_EBP_ERR)),\n\tFLAG_ENTRY0(\"fpe_tx_fifo_ovflw_err\", DCCE(FPE_TX_FIFO_OVFLW_ERR)),\n\tFLAG_ENTRY0(\"fpe_tx_fifo_unflw_err\", DCCE(FPE_TX_FIFO_UNFLW_ERR)),\n\tFLAG_ENTRY0(\"csr_access_blocked_host\", DCCE(CSR_ACCESS_BLOCKED_HOST)),\n\tFLAG_ENTRY0(\"csr_access_blocked_uc\", DCCE(CSR_ACCESS_BLOCKED_UC)),\n\tFLAG_ENTRY0(\"tx_ctrl_parity_err\", DCCE(TX_CTRL_PARITY_ERR)),\n\tFLAG_ENTRY0(\"tx_ctrl_parity_mbe_err\", DCCE(TX_CTRL_PARITY_MBE_ERR)),\n\tFLAG_ENTRY0(\"tx_sc_parity_err\", DCCE(TX_SC_PARITY_ERR)),\n\tFLAG_ENTRY0(\"rx_ctrl_parity_mbe_err\", DCCE(RX_CTRL_PARITY_MBE_ERR)),\n\tFLAG_ENTRY0(\"csr_parity_err\", DCCE(CSR_PARITY_ERR)),\n\tFLAG_ENTRY0(\"csr_inval_addr\", DCCE(CSR_INVAL_ADDR)),\n\tFLAG_ENTRY0(\"tx_byte_shft_parity_err\", DCCE(TX_BYTE_SHFT_PARITY_ERR)),\n\tFLAG_ENTRY0(\"rx_byte_shft_parity_err\", DCCE(RX_BYTE_SHFT_PARITY_ERR)),\n\tFLAG_ENTRY0(\"fmconfig_err\", DCCE(FMCONFIG_ERR)),\n\tFLAG_ENTRY0(\"rcvport_err\", DCCE(RCVPORT_ERR)),\n};\n\n \n#define LCBE(name) DC_LCB_ERR_FLG_##name##_SMASK\nstatic struct flag_table lcb_err_flags[] = {\n \tFLAG_ENTRY0(\"CSR_PARITY_ERR\", LCBE(CSR_PARITY_ERR)),\n \tFLAG_ENTRY0(\"INVALID_CSR_ADDR\", LCBE(INVALID_CSR_ADDR)),\n \tFLAG_ENTRY0(\"RST_FOR_FAILED_DESKEW\", LCBE(RST_FOR_FAILED_DESKEW)),\n \tFLAG_ENTRY0(\"ALL_LNS_FAILED_REINIT_TEST\",\n\t\tLCBE(ALL_LNS_FAILED_REINIT_TEST)),\n \tFLAG_ENTRY0(\"LOST_REINIT_STALL_OR_TOS\", LCBE(LOST_REINIT_STALL_OR_TOS)),\n \tFLAG_ENTRY0(\"TX_LESS_THAN_FOUR_LNS\", LCBE(TX_LESS_THAN_FOUR_LNS)),\n \tFLAG_ENTRY0(\"RX_LESS_THAN_FOUR_LNS\", LCBE(RX_LESS_THAN_FOUR_LNS)),\n \tFLAG_ENTRY0(\"SEQ_CRC_ERR\", LCBE(SEQ_CRC_ERR)),\n \tFLAG_ENTRY0(\"REINIT_FROM_PEER\", LCBE(REINIT_FROM_PEER)),\n \tFLAG_ENTRY0(\"REINIT_FOR_LN_DEGRADE\", LCBE(REINIT_FOR_LN_DEGRADE)),\n \tFLAG_ENTRY0(\"CRC_ERR_CNT_HIT_LIMIT\", LCBE(CRC_ERR_CNT_HIT_LIMIT)),\n \tFLAG_ENTRY0(\"RCLK_STOPPED\", LCBE(RCLK_STOPPED)),\n \tFLAG_ENTRY0(\"UNEXPECTED_REPLAY_MARKER\", LCBE(UNEXPECTED_REPLAY_MARKER)),\n \tFLAG_ENTRY0(\"UNEXPECTED_ROUND_TRIP_MARKER\",\n\t\tLCBE(UNEXPECTED_ROUND_TRIP_MARKER)),\n \tFLAG_ENTRY0(\"ILLEGAL_NULL_LTP\", LCBE(ILLEGAL_NULL_LTP)),\n \tFLAG_ENTRY0(\"ILLEGAL_FLIT_ENCODING\", LCBE(ILLEGAL_FLIT_ENCODING)),\n \tFLAG_ENTRY0(\"FLIT_INPUT_BUF_OFLW\", LCBE(FLIT_INPUT_BUF_OFLW)),\n \tFLAG_ENTRY0(\"VL_ACK_INPUT_BUF_OFLW\", LCBE(VL_ACK_INPUT_BUF_OFLW)),\n \tFLAG_ENTRY0(\"VL_ACK_INPUT_PARITY_ERR\", LCBE(VL_ACK_INPUT_PARITY_ERR)),\n \tFLAG_ENTRY0(\"VL_ACK_INPUT_WRONG_CRC_MODE\",\n\t\tLCBE(VL_ACK_INPUT_WRONG_CRC_MODE)),\n \tFLAG_ENTRY0(\"FLIT_INPUT_BUF_MBE\", LCBE(FLIT_INPUT_BUF_MBE)),\n \tFLAG_ENTRY0(\"FLIT_INPUT_BUF_SBE\", LCBE(FLIT_INPUT_BUF_SBE)),\n \tFLAG_ENTRY0(\"REPLAY_BUF_MBE\", LCBE(REPLAY_BUF_MBE)),\n \tFLAG_ENTRY0(\"REPLAY_BUF_SBE\", LCBE(REPLAY_BUF_SBE)),\n \tFLAG_ENTRY0(\"CREDIT_RETURN_FLIT_MBE\", LCBE(CREDIT_RETURN_FLIT_MBE)),\n \tFLAG_ENTRY0(\"RST_FOR_LINK_TIMEOUT\", LCBE(RST_FOR_LINK_TIMEOUT)),\n \tFLAG_ENTRY0(\"RST_FOR_INCOMPLT_RND_TRIP\",\n\t\tLCBE(RST_FOR_INCOMPLT_RND_TRIP)),\n \tFLAG_ENTRY0(\"HOLD_REINIT\", LCBE(HOLD_REINIT)),\n \tFLAG_ENTRY0(\"NEG_EDGE_LINK_TRANSFER_ACTIVE\",\n\t\tLCBE(NEG_EDGE_LINK_TRANSFER_ACTIVE)),\n \tFLAG_ENTRY0(\"REDUNDANT_FLIT_PARITY_ERR\",\n\t\tLCBE(REDUNDANT_FLIT_PARITY_ERR))\n};\n\n \n#define D8E(name) DC_DC8051_ERR_FLG_##name##_SMASK\nstatic struct flag_table dc8051_err_flags[] = {\n\tFLAG_ENTRY0(\"SET_BY_8051\", D8E(SET_BY_8051)),\n\tFLAG_ENTRY0(\"LOST_8051_HEART_BEAT\", D8E(LOST_8051_HEART_BEAT)),\n\tFLAG_ENTRY0(\"CRAM_MBE\", D8E(CRAM_MBE)),\n\tFLAG_ENTRY0(\"CRAM_SBE\", D8E(CRAM_SBE)),\n\tFLAG_ENTRY0(\"DRAM_MBE\", D8E(DRAM_MBE)),\n\tFLAG_ENTRY0(\"DRAM_SBE\", D8E(DRAM_SBE)),\n\tFLAG_ENTRY0(\"IRAM_MBE\", D8E(IRAM_MBE)),\n\tFLAG_ENTRY0(\"IRAM_SBE\", D8E(IRAM_SBE)),\n\tFLAG_ENTRY0(\"UNMATCHED_SECURE_MSG_ACROSS_BCC_LANES\",\n\t\t    D8E(UNMATCHED_SECURE_MSG_ACROSS_BCC_LANES)),\n\tFLAG_ENTRY0(\"INVALID_CSR_ADDR\", D8E(INVALID_CSR_ADDR)),\n};\n\n \nstatic struct flag_table dc8051_info_err_flags[] = {\n\tFLAG_ENTRY0(\"Spico ROM check failed\",  SPICO_ROM_FAILED),\n\tFLAG_ENTRY0(\"Unknown frame received\",  UNKNOWN_FRAME),\n\tFLAG_ENTRY0(\"Target BER not met\",      TARGET_BER_NOT_MET),\n\tFLAG_ENTRY0(\"Serdes internal loopback failure\",\n\t\t    FAILED_SERDES_INTERNAL_LOOPBACK),\n\tFLAG_ENTRY0(\"Failed SerDes init\",      FAILED_SERDES_INIT),\n\tFLAG_ENTRY0(\"Failed LNI(Polling)\",     FAILED_LNI_POLLING),\n\tFLAG_ENTRY0(\"Failed LNI(Debounce)\",    FAILED_LNI_DEBOUNCE),\n\tFLAG_ENTRY0(\"Failed LNI(EstbComm)\",    FAILED_LNI_ESTBCOMM),\n\tFLAG_ENTRY0(\"Failed LNI(OptEq)\",       FAILED_LNI_OPTEQ),\n\tFLAG_ENTRY0(\"Failed LNI(VerifyCap_1)\", FAILED_LNI_VERIFY_CAP1),\n\tFLAG_ENTRY0(\"Failed LNI(VerifyCap_2)\", FAILED_LNI_VERIFY_CAP2),\n\tFLAG_ENTRY0(\"Failed LNI(ConfigLT)\",    FAILED_LNI_CONFIGLT),\n\tFLAG_ENTRY0(\"Host Handshake Timeout\",  HOST_HANDSHAKE_TIMEOUT),\n\tFLAG_ENTRY0(\"External Device Request Timeout\",\n\t\t    EXTERNAL_DEVICE_REQ_TIMEOUT),\n};\n\n \nstatic struct flag_table dc8051_info_host_msg_flags[] = {\n\tFLAG_ENTRY0(\"Host request done\", 0x0001),\n\tFLAG_ENTRY0(\"BC PWR_MGM message\", 0x0002),\n\tFLAG_ENTRY0(\"BC SMA message\", 0x0004),\n\tFLAG_ENTRY0(\"BC Unknown message (BCC)\", 0x0008),\n\tFLAG_ENTRY0(\"BC Unknown message (LCB)\", 0x0010),\n\tFLAG_ENTRY0(\"External device config request\", 0x0020),\n\tFLAG_ENTRY0(\"VerifyCap all frames received\", 0x0040),\n\tFLAG_ENTRY0(\"LinkUp achieved\", 0x0080),\n\tFLAG_ENTRY0(\"Link going down\", 0x0100),\n\tFLAG_ENTRY0(\"Link width downgraded\", 0x0200),\n};\n\nstatic u32 encoded_size(u32 size);\nstatic u32 chip_to_opa_lstate(struct hfi1_devdata *dd, u32 chip_lstate);\nstatic int set_physical_link_state(struct hfi1_devdata *dd, u64 state);\nstatic void read_vc_remote_phy(struct hfi1_devdata *dd, u8 *power_management,\n\t\t\t       u8 *continuous);\nstatic void read_vc_remote_fabric(struct hfi1_devdata *dd, u8 *vau, u8 *z,\n\t\t\t\t  u8 *vcu, u16 *vl15buf, u8 *crc_sizes);\nstatic void read_vc_remote_link_width(struct hfi1_devdata *dd,\n\t\t\t\t      u8 *remote_tx_rate, u16 *link_widths);\nstatic void read_vc_local_link_mode(struct hfi1_devdata *dd, u8 *misc_bits,\n\t\t\t\t    u8 *flag_bits, u16 *link_widths);\nstatic void read_remote_device_id(struct hfi1_devdata *dd, u16 *device_id,\n\t\t\t\t  u8 *device_rev);\nstatic void read_local_lni(struct hfi1_devdata *dd, u8 *enable_lane_rx);\nstatic int read_tx_settings(struct hfi1_devdata *dd, u8 *enable_lane_tx,\n\t\t\t    u8 *tx_polarity_inversion,\n\t\t\t    u8 *rx_polarity_inversion, u8 *max_rate);\nstatic void handle_sdma_eng_err(struct hfi1_devdata *dd,\n\t\t\t\tunsigned int context, u64 err_status);\nstatic void handle_qsfp_int(struct hfi1_devdata *dd, u32 source, u64 reg);\nstatic void handle_dcc_err(struct hfi1_devdata *dd,\n\t\t\t   unsigned int context, u64 err_status);\nstatic void handle_lcb_err(struct hfi1_devdata *dd,\n\t\t\t   unsigned int context, u64 err_status);\nstatic void handle_8051_interrupt(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_cce_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_rxe_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_misc_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_pio_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_sdma_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_egress_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void handle_txe_err(struct hfi1_devdata *dd, u32 unused, u64 reg);\nstatic void set_partition_keys(struct hfi1_pportdata *ppd);\nstatic const char *link_state_name(u32 state);\nstatic const char *link_state_reason_name(struct hfi1_pportdata *ppd,\n\t\t\t\t\t  u32 state);\nstatic int do_8051_command(struct hfi1_devdata *dd, u32 type, u64 in_data,\n\t\t\t   u64 *out_data);\nstatic int read_idle_sma(struct hfi1_devdata *dd, u64 *data);\nstatic int thermal_init(struct hfi1_devdata *dd);\n\nstatic void update_statusp(struct hfi1_pportdata *ppd, u32 state);\nstatic int wait_phys_link_offline_substates(struct hfi1_pportdata *ppd,\n\t\t\t\t\t    int msecs);\nstatic int wait_logical_linkstate(struct hfi1_pportdata *ppd, u32 state,\n\t\t\t\t  int msecs);\nstatic void log_state_transition(struct hfi1_pportdata *ppd, u32 state);\nstatic void log_physical_state(struct hfi1_pportdata *ppd, u32 state);\nstatic int wait_physical_linkstate(struct hfi1_pportdata *ppd, u32 state,\n\t\t\t\t   int msecs);\nstatic int wait_phys_link_out_of_offline(struct hfi1_pportdata *ppd,\n\t\t\t\t\t int msecs);\nstatic void read_planned_down_reason_code(struct hfi1_devdata *dd, u8 *pdrrc);\nstatic void read_link_down_reason(struct hfi1_devdata *dd, u8 *ldr);\nstatic void handle_temp_err(struct hfi1_devdata *dd);\nstatic void dc_shutdown(struct hfi1_devdata *dd);\nstatic void dc_start(struct hfi1_devdata *dd);\nstatic int qos_rmt_entries(unsigned int n_krcv_queues, unsigned int *mp,\n\t\t\t   unsigned int *np);\nstatic void clear_full_mgmt_pkey(struct hfi1_pportdata *ppd);\nstatic int wait_link_transfer_active(struct hfi1_devdata *dd, int wait_ms);\nstatic void clear_rsm_rule(struct hfi1_devdata *dd, u8 rule_index);\nstatic void update_xmit_counters(struct hfi1_pportdata *ppd, u16 link_width);\n\n \nstruct err_reg_info {\n\tu32 status;\t\t \n\tu32 clear;\t\t \n\tu32 mask;\t\t \n\tvoid (*handler)(struct hfi1_devdata *dd, u32 source, u64 reg);\n\tconst char *desc;\n};\n\n#define NUM_MISC_ERRS (IS_GENERAL_ERR_END + 1 - IS_GENERAL_ERR_START)\n#define NUM_DC_ERRS (IS_DC_END + 1 - IS_DC_START)\n#define NUM_VARIOUS (IS_VARIOUS_END + 1 - IS_VARIOUS_START)\n\n \n#define EE(reg, handler, desc) \\\n\t{ reg##_STATUS, reg##_CLEAR, reg##_MASK, \\\n\t\thandler, desc }\n#define DC_EE1(reg, handler, desc) \\\n\t{ reg##_FLG, reg##_FLG_CLR, reg##_FLG_EN, handler, desc }\n#define DC_EE2(reg, handler, desc) \\\n\t{ reg##_FLG, reg##_CLR, reg##_EN, handler, desc }\n\n \nstatic const struct err_reg_info misc_errs[NUM_MISC_ERRS] = {\n \tEE(CCE_ERR,\t\thandle_cce_err,    \"CceErr\"),\n \tEE(RCV_ERR,\t\thandle_rxe_err,    \"RxeErr\"),\n \tEE(MISC_ERR,\thandle_misc_err,   \"MiscErr\"),\n \t{ 0, 0, 0, NULL },  \n \tEE(SEND_PIO_ERR,    handle_pio_err,    \"PioErr\"),\n \tEE(SEND_DMA_ERR,    handle_sdma_err,   \"SDmaErr\"),\n \tEE(SEND_EGRESS_ERR, handle_egress_err, \"EgressErr\"),\n \tEE(SEND_ERR,\thandle_txe_err,    \"TxeErr\")\n\t \n};\n\n \n#define TCRIT_INT_SOURCE 4\n\n \nstatic const struct err_reg_info sdma_eng_err =\n\tEE(SEND_DMA_ENG_ERR, handle_sdma_eng_err, \"SDmaEngErr\");\n\nstatic const struct err_reg_info various_err[NUM_VARIOUS] = {\n \t{ 0, 0, 0, NULL },  \n \t{ 0, 0, 0, NULL },  \n \tEE(ASIC_QSFP1,\thandle_qsfp_int,\t\"QSFP1\"),\n \tEE(ASIC_QSFP2,\thandle_qsfp_int,\t\"QSFP2\"),\n \t{ 0, 0, 0, NULL },  \n\t \n};\n\n \n#define DCC_CFG_PORT_MTU_CAP_10240 7\n\n \nstatic const struct err_reg_info dc_errs[NUM_DC_ERRS] = {\n \tDC_EE1(DCC_ERR,\t\thandle_dcc_err,\t       \"DCC Err\"),\n \tDC_EE2(DC_LCB_ERR,\thandle_lcb_err,\t       \"LCB Err\"),\n \tDC_EE2(DC_DC8051_ERR,\thandle_8051_interrupt, \"DC8051 Interrupt\"),\n \t \n\t \n};\n\nstruct cntr_entry {\n\t \n\tchar *name;\n\n\t \n\tu64 csr;\n\n\t \n\tint offset;\n\n\t \n\tu8 flags;\n\n\t \n\tu64 (*rw_cntr)(const struct cntr_entry *, void *context, int vl,\n\t\t       int mode, u64 data);\n};\n\n#define C_RCV_HDR_OVF_FIRST C_RCV_HDR_OVF_0\n#define C_RCV_HDR_OVF_LAST C_RCV_HDR_OVF_159\n\n#define CNTR_ELEM(name, csr, offset, flags, accessor) \\\n{ \\\n\tname, \\\n\tcsr, \\\n\toffset, \\\n\tflags, \\\n\taccessor \\\n}\n\n \n#define RXE32_PORT_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + RCV_COUNTER_ARRAY32), \\\n\t  0, flags | CNTR_32BIT, \\\n\t  port_access_u32_csr)\n\n#define RXE32_DEV_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + RCV_COUNTER_ARRAY32), \\\n\t  0, flags | CNTR_32BIT, \\\n\t  dev_access_u32_csr)\n\n \n#define RXE64_PORT_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + RCV_COUNTER_ARRAY64), \\\n\t  0, flags, \\\n\t  port_access_u64_csr)\n\n#define RXE64_DEV_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + RCV_COUNTER_ARRAY64), \\\n\t  0, flags, \\\n\t  dev_access_u64_csr)\n\n#define OVR_LBL(ctx) C_RCV_HDR_OVF_ ## ctx\n#define OVR_ELM(ctx) \\\nCNTR_ELEM(\"RcvHdrOvr\" #ctx, \\\n\t  (RCV_HDR_OVFL_CNT + ctx * 0x100), \\\n\t  0, CNTR_NORMAL, port_access_u64_csr)\n\n \n#define TXE32_PORT_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + SEND_COUNTER_ARRAY32), \\\n\t  0, flags | CNTR_32BIT, \\\n\t  port_access_u32_csr)\n\n \n#define TXE64_PORT_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + SEND_COUNTER_ARRAY64), \\\n\t  0, flags, \\\n\t  port_access_u64_csr)\n\n# define TX64_DEV_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name,\\\n\t  counter * 8 + SEND_COUNTER_ARRAY64, \\\n\t  0, \\\n\t  flags, \\\n\t  dev_access_u64_csr)\n\n \n#define CCE_PERF_DEV_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + CCE_COUNTER_ARRAY32), \\\n\t  0, flags | CNTR_32BIT, \\\n\t  dev_access_u32_csr)\n\n#define CCE_INT_DEV_CNTR_ELEM(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  (counter * 8 + CCE_INT_COUNTER_ARRAY32), \\\n\t  0, flags | CNTR_32BIT, \\\n\t  dev_access_u32_csr)\n\n \n#define DC_PERF_CNTR(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  counter, \\\n\t  0, \\\n\t  flags, \\\n\t  dev_access_u64_csr)\n\n#define DC_PERF_CNTR_LCB(name, counter, flags) \\\nCNTR_ELEM(#name, \\\n\t  counter, \\\n\t  0, \\\n\t  flags, \\\n\t  dc_access_lcb_cntr)\n\n \n#define SW_IBP_CNTR(name, cntr) \\\nCNTR_ELEM(#name, \\\n\t  0, \\\n\t  0, \\\n\t  CNTR_SYNTH, \\\n\t  access_ibp_##cntr)\n\n \nstatic inline void __iomem *hfi1_addr_from_offset(\n\tconst struct hfi1_devdata *dd,\n\tu32 offset)\n{\n\tif (offset >= dd->base2_start)\n\t\treturn dd->kregbase2 + (offset - dd->base2_start);\n\treturn dd->kregbase1 + offset;\n}\n\n \nu64 read_csr(const struct hfi1_devdata *dd, u32 offset)\n{\n\tif (dd->flags & HFI1_PRESENT)\n\t\treturn readq(hfi1_addr_from_offset(dd, offset));\n\treturn -1;\n}\n\n \nvoid write_csr(const struct hfi1_devdata *dd, u32 offset, u64 value)\n{\n\tif (dd->flags & HFI1_PRESENT) {\n\t\tvoid __iomem *base = hfi1_addr_from_offset(dd, offset);\n\n\t\t \n\t\tif (WARN_ON(offset >= RCV_ARRAY && offset < dd->base2_start))\n\t\t\treturn;\n\t\twriteq(value, base);\n\t}\n}\n\n \nvoid __iomem *get_csr_addr(\n\tconst struct hfi1_devdata *dd,\n\tu32 offset)\n{\n\tif (dd->flags & HFI1_PRESENT)\n\t\treturn hfi1_addr_from_offset(dd, offset);\n\treturn NULL;\n}\n\nstatic inline u64 read_write_csr(const struct hfi1_devdata *dd, u32 csr,\n\t\t\t\t int mode, u64 value)\n{\n\tu64 ret;\n\n\tif (mode == CNTR_MODE_R) {\n\t\tret = read_csr(dd, csr);\n\t} else if (mode == CNTR_MODE_W) {\n\t\twrite_csr(dd, csr, value);\n\t\tret = value;\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid cntr register access mode\");\n\t\treturn 0;\n\t}\n\n\thfi1_cdbg(CNTR, \"csr 0x%x val 0x%llx mode %d\", csr, ret, mode);\n\treturn ret;\n}\n\n \nstatic u64 dev_access_u32_csr(const struct cntr_entry *entry,\n\t\t\t      void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\tu64 csr = entry->csr;\n\n\tif (entry->flags & CNTR_SDMA) {\n\t\tif (vl == CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t\tcsr += 0x100 * vl;\n\t} else {\n\t\tif (vl != CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t}\n\treturn read_write_csr(dd, csr, mode, data);\n}\n\nstatic u64 access_sde_err_cnt(const struct cntr_entry *entry,\n\t\t\t      void *context, int idx, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\tif (dd->per_sdma && idx < dd->num_sdma)\n\t\treturn dd->per_sdma[idx].err_cnt;\n\treturn 0;\n}\n\nstatic u64 access_sde_int_cnt(const struct cntr_entry *entry,\n\t\t\t      void *context, int idx, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\tif (dd->per_sdma && idx < dd->num_sdma)\n\t\treturn dd->per_sdma[idx].sdma_int_cnt;\n\treturn 0;\n}\n\nstatic u64 access_sde_idle_int_cnt(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int idx, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\tif (dd->per_sdma && idx < dd->num_sdma)\n\t\treturn dd->per_sdma[idx].idle_int_cnt;\n\treturn 0;\n}\n\nstatic u64 access_sde_progress_int_cnt(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int idx, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\tif (dd->per_sdma && idx < dd->num_sdma)\n\t\treturn dd->per_sdma[idx].progress_int_cnt;\n\treturn 0;\n}\n\nstatic u64 dev_access_u64_csr(const struct cntr_entry *entry, void *context,\n\t\t\t      int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\tu64 val = 0;\n\tu64 csr = entry->csr;\n\n\tif (entry->flags & CNTR_VL) {\n\t\tif (vl == CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t\tcsr += 8 * vl;\n\t} else {\n\t\tif (vl != CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t}\n\n\tval = read_write_csr(dd, csr, mode, data);\n\treturn val;\n}\n\nstatic u64 dc_access_lcb_cntr(const struct cntr_entry *entry, void *context,\n\t\t\t      int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\tu32 csr = entry->csr;\n\tint ret = 0;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\tif (mode == CNTR_MODE_R)\n\t\tret = read_lcb_csr(dd, csr, &data);\n\telse if (mode == CNTR_MODE_W)\n\t\tret = write_lcb_csr(dd, csr, data);\n\n\tif (ret) {\n\t\tif (!(dd->flags & HFI1_SHUTDOWN))\n\t\t\tdd_dev_err(dd, \"Could not acquire LCB for counter 0x%x\", csr);\n\t\treturn 0;\n\t}\n\n\thfi1_cdbg(CNTR, \"csr 0x%x val 0x%llx mode %d\", csr, data, mode);\n\treturn data;\n}\n\n \nstatic u64 port_access_u32_csr(const struct cntr_entry *entry, void *context,\n\t\t\t       int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\treturn read_write_csr(ppd->dd, entry->csr, mode, data);\n}\n\nstatic u64 port_access_u64_csr(const struct cntr_entry *entry,\n\t\t\t       void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\tu64 val;\n\tu64 csr = entry->csr;\n\n\tif (entry->flags & CNTR_VL) {\n\t\tif (vl == CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t\tcsr += 8 * vl;\n\t} else {\n\t\tif (vl != CNTR_INVALID_VL)\n\t\t\treturn 0;\n\t}\n\tval = read_write_csr(ppd->dd, csr, mode, data);\n\treturn val;\n}\n\n \nstatic inline u64 read_write_sw(struct hfi1_devdata *dd, u64 *cntr, int mode,\n\t\t\t\tu64 data)\n{\n\tu64 ret;\n\n\tif (mode == CNTR_MODE_R) {\n\t\tret = *cntr;\n\t} else if (mode == CNTR_MODE_W) {\n\t\t*cntr = data;\n\t\tret = data;\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid cntr sw access mode\");\n\t\treturn 0;\n\t}\n\n\thfi1_cdbg(CNTR, \"val 0x%llx mode %d\", ret, mode);\n\n\treturn ret;\n}\n\nstatic u64 access_sw_link_dn_cnt(const struct cntr_entry *entry, void *context,\n\t\t\t\t int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\treturn read_write_sw(ppd->dd, &ppd->link_downed, mode, data);\n}\n\nstatic u64 access_sw_link_up_cnt(const struct cntr_entry *entry, void *context,\n\t\t\t\t int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\treturn read_write_sw(ppd->dd, &ppd->link_up, mode, data);\n}\n\nstatic u64 access_sw_unknown_frame_cnt(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_pportdata *ppd = (struct hfi1_pportdata *)context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\treturn read_write_sw(ppd->dd, &ppd->unknown_frame_count, mode, data);\n}\n\nstatic u64 access_sw_xmit_discards(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = (struct hfi1_pportdata *)context;\n\tu64 zero = 0;\n\tu64 *counter;\n\n\tif (vl == CNTR_INVALID_VL)\n\t\tcounter = &ppd->port_xmit_discards;\n\telse if (vl >= 0 && vl < C_VL_COUNT)\n\t\tcounter = &ppd->port_xmit_discards_vl[vl];\n\telse\n\t\tcounter = &zero;\n\n\treturn read_write_sw(ppd->dd, counter, mode, data);\n}\n\nstatic u64 access_xmit_constraint_errs(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\n\treturn read_write_sw(ppd->dd, &ppd->port_xmit_constraint_errors,\n\t\t\t     mode, data);\n}\n\nstatic u64 access_rcv_constraint_errs(const struct cntr_entry *entry,\n\t\t\t\t      void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = context;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\n\treturn read_write_sw(ppd->dd, &ppd->port_rcv_constraint_errors,\n\t\t\t     mode, data);\n}\n\nu64 get_all_cpu_total(u64 __percpu *cntr)\n{\n\tint cpu;\n\tu64 counter = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tcounter += *per_cpu_ptr(cntr, cpu);\n\treturn counter;\n}\n\nstatic u64 read_write_cpu(struct hfi1_devdata *dd, u64 *z_val,\n\t\t\t  u64 __percpu *cntr,\n\t\t\t  int vl, int mode, u64 data)\n{\n\tu64 ret = 0;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\treturn 0;\n\n\tif (mode == CNTR_MODE_R) {\n\t\tret = get_all_cpu_total(cntr) - *z_val;\n\t} else if (mode == CNTR_MODE_W) {\n\t\t \n\t\tif (data == 0)\n\t\t\t*z_val = get_all_cpu_total(cntr);\n\t\telse\n\t\t\tdd_dev_err(dd, \"Per CPU cntrs can only be zeroed\");\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid cntr sw cpu access mode\");\n\t\treturn 0;\n\t}\n\n\treturn ret;\n}\n\nstatic u64 access_sw_cpu_intr(const struct cntr_entry *entry,\n\t\t\t      void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn read_write_cpu(dd, &dd->z_int_counter, dd->int_counter, vl,\n\t\t\t      mode, data);\n}\n\nstatic u64 access_sw_cpu_rcv_limit(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn read_write_cpu(dd, &dd->z_rcv_limit, dd->rcv_limit, vl,\n\t\t\t      mode, data);\n}\n\nstatic u64 access_sw_pio_wait(const struct cntr_entry *entry,\n\t\t\t      void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn dd->verbs_dev.n_piowait;\n}\n\nstatic u64 access_sw_pio_drain(const struct cntr_entry *entry,\n\t\t\t       void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->verbs_dev.n_piodrain;\n}\n\nstatic u64 access_sw_ctx0_seq_drop(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn dd->ctx0_seq_drop;\n}\n\nstatic u64 access_sw_vtx_wait(const struct cntr_entry *entry,\n\t\t\t      void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn dd->verbs_dev.n_txwait;\n}\n\nstatic u64 access_sw_kmem_wait(const struct cntr_entry *entry,\n\t\t\t       void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = context;\n\n\treturn dd->verbs_dev.n_kmem_wait;\n}\n\nstatic u64 access_sw_send_schedule(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn read_write_cpu(dd, &dd->z_send_schedule, dd->send_schedule, vl,\n\t\t\t      mode, data);\n}\n\n \nstatic u64 access_misc_pll_lock_fail_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[12];\n}\n\nstatic u64 access_misc_mbist_fail_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[11];\n}\n\nstatic u64 access_misc_invalid_eep_cmd_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[10];\n}\n\nstatic u64 access_misc_efuse_done_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[9];\n}\n\nstatic u64 access_misc_efuse_write_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[8];\n}\n\nstatic u64 access_misc_efuse_read_bad_addr_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[7];\n}\n\nstatic u64 access_misc_efuse_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[6];\n}\n\nstatic u64 access_misc_fw_auth_failed_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[5];\n}\n\nstatic u64 access_misc_key_mismatch_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[4];\n}\n\nstatic u64 access_misc_sbus_write_failed_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[3];\n}\n\nstatic u64 access_misc_csr_write_bad_addr_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[2];\n}\n\nstatic u64 access_misc_csr_read_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[1];\n}\n\nstatic u64 access_misc_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->misc_err_status_cnt[0];\n}\n\n \nstatic u64 access_sw_cce_err_status_aggregated_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_cce_err_status_aggregate;\n}\n\n \nstatic u64 access_cce_msix_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[40];\n}\n\nstatic u64 access_cce_int_map_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[39];\n}\n\nstatic u64 access_cce_int_map_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[38];\n}\n\nstatic u64 access_cce_msix_table_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[37];\n}\n\nstatic u64 access_cce_msix_table_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[36];\n}\n\nstatic u64 access_cce_rxdma_conv_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[35];\n}\n\nstatic u64 access_cce_rcpl_async_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[34];\n}\n\nstatic u64 access_cce_seg_write_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[33];\n}\n\nstatic u64 access_cce_seg_read_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[32];\n}\n\nstatic u64 access_la_triggered_cnt(const struct cntr_entry *entry,\n\t\t\t\t   void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[31];\n}\n\nstatic u64 access_cce_trgt_cpl_timeout_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[30];\n}\n\nstatic u64 access_pcic_receive_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[29];\n}\n\nstatic u64 access_pcic_transmit_back_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[28];\n}\n\nstatic u64 access_pcic_transmit_front_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[27];\n}\n\nstatic u64 access_pcic_cpl_dat_q_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[26];\n}\n\nstatic u64 access_pcic_cpl_hd_q_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[25];\n}\n\nstatic u64 access_pcic_post_dat_q_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[24];\n}\n\nstatic u64 access_pcic_post_hd_q_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[23];\n}\n\nstatic u64 access_pcic_retry_sot_mem_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[22];\n}\n\nstatic u64 access_pcic_retry_mem_unc_err(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[21];\n}\n\nstatic u64 access_pcic_n_post_dat_q_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[20];\n}\n\nstatic u64 access_pcic_n_post_h_q_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[19];\n}\n\nstatic u64 access_pcic_cpl_dat_q_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[18];\n}\n\nstatic u64 access_pcic_cpl_hd_q_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[17];\n}\n\nstatic u64 access_pcic_post_dat_q_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[16];\n}\n\nstatic u64 access_pcic_post_hd_q_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[15];\n}\n\nstatic u64 access_pcic_retry_sot_mem_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[14];\n}\n\nstatic u64 access_pcic_retry_mem_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[13];\n}\n\nstatic u64 access_cce_cli1_async_fifo_dbg_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[12];\n}\n\nstatic u64 access_cce_cli1_async_fifo_rxdma_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[11];\n}\n\nstatic u64 access_cce_cli1_async_fifo_sdma_hd_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[10];\n}\n\nstatic u64 access_cce_cl1_async_fifo_pio_crdt_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[9];\n}\n\nstatic u64 access_cce_cli2_async_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[8];\n}\n\nstatic u64 access_cce_csr_cfg_bus_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[7];\n}\n\nstatic u64 access_cce_cli0_async_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[6];\n}\n\nstatic u64 access_cce_rspd_data_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[5];\n}\n\nstatic u64 access_cce_trgt_access_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[4];\n}\n\nstatic u64 access_cce_trgt_async_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[3];\n}\n\nstatic u64 access_cce_csr_write_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[2];\n}\n\nstatic u64 access_cce_csr_read_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[1];\n}\n\nstatic u64 access_ccs_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->cce_err_status_cnt[0];\n}\n\n \nstatic u64 access_rx_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[63];\n}\n\nstatic u64 access_rx_csr_write_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[62];\n}\n\nstatic u64 access_rx_csr_read_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[61];\n}\n\nstatic u64 access_rx_dma_csr_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[60];\n}\n\nstatic u64 access_rx_dma_dq_fsm_encoding_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[59];\n}\n\nstatic u64 access_rx_dma_eq_fsm_encoding_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[58];\n}\n\nstatic u64 access_rx_dma_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[57];\n}\n\nstatic u64 access_rx_rbuf_data_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[56];\n}\n\nstatic u64 access_rx_rbuf_data_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[55];\n}\n\nstatic u64 access_rx_dma_data_fifo_rd_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[54];\n}\n\nstatic u64 access_rx_dma_data_fifo_rd_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[53];\n}\n\nstatic u64 access_rx_dma_hdr_fifo_rd_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[52];\n}\n\nstatic u64 access_rx_dma_hdr_fifo_rd_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[51];\n}\n\nstatic u64 access_rx_rbuf_desc_part2_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[50];\n}\n\nstatic u64 access_rx_rbuf_desc_part2_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[49];\n}\n\nstatic u64 access_rx_rbuf_desc_part1_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[48];\n}\n\nstatic u64 access_rx_rbuf_desc_part1_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[47];\n}\n\nstatic u64 access_rx_hq_intr_fsm_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[46];\n}\n\nstatic u64 access_rx_hq_intr_csr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[45];\n}\n\nstatic u64 access_rx_lookup_csr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[44];\n}\n\nstatic u64 access_rx_lookup_rcv_array_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[43];\n}\n\nstatic u64 access_rx_lookup_rcv_array_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[42];\n}\n\nstatic u64 access_rx_lookup_des_part2_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[41];\n}\n\nstatic u64 access_rx_lookup_des_part1_unc_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[40];\n}\n\nstatic u64 access_rx_lookup_des_part1_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[39];\n}\n\nstatic u64 access_rx_rbuf_next_free_buf_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[38];\n}\n\nstatic u64 access_rx_rbuf_next_free_buf_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[37];\n}\n\nstatic u64 access_rbuf_fl_init_wr_addr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[36];\n}\n\nstatic u64 access_rx_rbuf_fl_initdone_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[35];\n}\n\nstatic u64 access_rx_rbuf_fl_write_addr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[34];\n}\n\nstatic u64 access_rx_rbuf_fl_rd_addr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[33];\n}\n\nstatic u64 access_rx_rbuf_empty_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[32];\n}\n\nstatic u64 access_rx_rbuf_full_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[31];\n}\n\nstatic u64 access_rbuf_bad_lookup_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[30];\n}\n\nstatic u64 access_rbuf_ctx_id_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[29];\n}\n\nstatic u64 access_rbuf_csr_qeopdw_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[28];\n}\n\nstatic u64 access_rx_rbuf_csr_q_num_of_pkt_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[27];\n}\n\nstatic u64 access_rx_rbuf_csr_q_t1_ptr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[26];\n}\n\nstatic u64 access_rx_rbuf_csr_q_hd_ptr_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[25];\n}\n\nstatic u64 access_rx_rbuf_csr_q_vld_bit_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[24];\n}\n\nstatic u64 access_rx_rbuf_csr_q_next_buf_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[23];\n}\n\nstatic u64 access_rx_rbuf_csr_q_ent_cnt_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[22];\n}\n\nstatic u64 access_rx_rbuf_csr_q_head_buf_num_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[21];\n}\n\nstatic u64 access_rx_rbuf_block_list_read_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[20];\n}\n\nstatic u64 access_rx_rbuf_block_list_read_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[19];\n}\n\nstatic u64 access_rx_rbuf_lookup_des_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[18];\n}\n\nstatic u64 access_rx_rbuf_lookup_des_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[17];\n}\n\nstatic u64 access_rx_rbuf_lookup_des_reg_unc_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[16];\n}\n\nstatic u64 access_rx_rbuf_lookup_des_reg_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[15];\n}\n\nstatic u64 access_rx_rbuf_free_list_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[14];\n}\n\nstatic u64 access_rx_rbuf_free_list_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[13];\n}\n\nstatic u64 access_rx_rcv_fsm_encoding_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[12];\n}\n\nstatic u64 access_rx_dma_flag_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[11];\n}\n\nstatic u64 access_rx_dma_flag_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[10];\n}\n\nstatic u64 access_rx_dc_sop_eop_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[9];\n}\n\nstatic u64 access_rx_rcv_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[8];\n}\n\nstatic u64 access_rx_rcv_qp_map_table_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[7];\n}\n\nstatic u64 access_rx_rcv_qp_map_table_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[6];\n}\n\nstatic u64 access_rx_rcv_data_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[5];\n}\n\nstatic u64 access_rx_rcv_data_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[4];\n}\n\nstatic u64 access_rx_rcv_hdr_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[3];\n}\n\nstatic u64 access_rx_rcv_hdr_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[2];\n}\n\nstatic u64 access_rx_dc_intf_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[1];\n}\n\nstatic u64 access_rx_dma_csr_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->rcv_err_status_cnt[0];\n}\n\n \nstatic u64 access_pio_pec_sop_head_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[35];\n}\n\nstatic u64 access_pio_pcc_sop_head_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[34];\n}\n\nstatic u64 access_pio_last_returned_cnt_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[33];\n}\n\nstatic u64 access_pio_current_free_cnt_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[32];\n}\n\nstatic u64 access_pio_reserved_31_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[31];\n}\n\nstatic u64 access_pio_reserved_30_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[30];\n}\n\nstatic u64 access_pio_ppmc_sop_len_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[29];\n}\n\nstatic u64 access_pio_ppmc_bqc_mem_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[28];\n}\n\nstatic u64 access_pio_vl_fifo_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[27];\n}\n\nstatic u64 access_pio_vlf_sop_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[26];\n}\n\nstatic u64 access_pio_vlf_v1_len_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[25];\n}\n\nstatic u64 access_pio_block_qw_count_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[24];\n}\n\nstatic u64 access_pio_write_qw_valid_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[23];\n}\n\nstatic u64 access_pio_state_machine_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[22];\n}\n\nstatic u64 access_pio_write_data_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[21];\n}\n\nstatic u64 access_pio_host_addr_mem_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[20];\n}\n\nstatic u64 access_pio_host_addr_mem_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[19];\n}\n\nstatic u64 access_pio_pkt_evict_sm_or_arb_sm_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[18];\n}\n\nstatic u64 access_pio_init_sm_in_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[17];\n}\n\nstatic u64 access_pio_ppmc_pbl_fifo_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[16];\n}\n\nstatic u64 access_pio_credit_ret_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[15];\n}\n\nstatic u64 access_pio_v1_len_mem_bank1_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[14];\n}\n\nstatic u64 access_pio_v1_len_mem_bank0_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[13];\n}\n\nstatic u64 access_pio_v1_len_mem_bank1_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[12];\n}\n\nstatic u64 access_pio_v1_len_mem_bank0_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[11];\n}\n\nstatic u64 access_pio_sm_pkt_reset_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[10];\n}\n\nstatic u64 access_pio_pkt_evict_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[9];\n}\n\nstatic u64 access_pio_sbrdctrl_crrel_fifo_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[8];\n}\n\nstatic u64 access_pio_sbrdctl_crrel_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[7];\n}\n\nstatic u64 access_pio_pec_fifo_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[6];\n}\n\nstatic u64 access_pio_pcc_fifo_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[5];\n}\n\nstatic u64 access_pio_sb_mem_fifo1_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[4];\n}\n\nstatic u64 access_pio_sb_mem_fifo0_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[3];\n}\n\nstatic u64 access_pio_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[2];\n}\n\nstatic u64 access_pio_write_addr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[1];\n}\n\nstatic u64 access_pio_write_bad_ctxt_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_pio_err_status_cnt[0];\n}\n\n \nstatic u64 access_sdma_pcie_req_tracking_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_dma_err_status_cnt[3];\n}\n\nstatic u64 access_sdma_pcie_req_tracking_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_dma_err_status_cnt[2];\n}\n\nstatic u64 access_sdma_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_dma_err_status_cnt[1];\n}\n\nstatic u64 access_sdma_rpy_tag_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_dma_err_status_cnt[0];\n}\n\n \nstatic u64 access_tx_read_pio_memory_csr_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[63];\n}\n\nstatic u64 access_tx_read_sdma_memory_csr_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[62];\n}\n\nstatic u64 access_tx_egress_fifo_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[61];\n}\n\nstatic u64 access_tx_read_pio_memory_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[60];\n}\n\nstatic u64 access_tx_read_sdma_memory_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[59];\n}\n\nstatic u64 access_tx_sb_hdr_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[58];\n}\n\nstatic u64 access_tx_credit_overrun_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[57];\n}\n\nstatic u64 access_tx_launch_fifo8_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[56];\n}\n\nstatic u64 access_tx_launch_fifo7_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[55];\n}\n\nstatic u64 access_tx_launch_fifo6_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[54];\n}\n\nstatic u64 access_tx_launch_fifo5_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[53];\n}\n\nstatic u64 access_tx_launch_fifo4_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[52];\n}\n\nstatic u64 access_tx_launch_fifo3_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[51];\n}\n\nstatic u64 access_tx_launch_fifo2_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[50];\n}\n\nstatic u64 access_tx_launch_fifo1_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[49];\n}\n\nstatic u64 access_tx_launch_fifo0_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[48];\n}\n\nstatic u64 access_tx_credit_return_vl_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[47];\n}\n\nstatic u64 access_tx_hcrc_insertion_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[46];\n}\n\nstatic u64 access_tx_egress_fifo_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[45];\n}\n\nstatic u64 access_tx_read_pio_memory_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[44];\n}\n\nstatic u64 access_tx_read_sdma_memory_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[43];\n}\n\nstatic u64 access_tx_sb_hdr_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[42];\n}\n\nstatic u64 access_tx_credit_return_partiy_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[41];\n}\n\nstatic u64 access_tx_launch_fifo8_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[40];\n}\n\nstatic u64 access_tx_launch_fifo7_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[39];\n}\n\nstatic u64 access_tx_launch_fifo6_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[38];\n}\n\nstatic u64 access_tx_launch_fifo5_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[37];\n}\n\nstatic u64 access_tx_launch_fifo4_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[36];\n}\n\nstatic u64 access_tx_launch_fifo3_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[35];\n}\n\nstatic u64 access_tx_launch_fifo2_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[34];\n}\n\nstatic u64 access_tx_launch_fifo1_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[33];\n}\n\nstatic u64 access_tx_launch_fifo0_unc_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[32];\n}\n\nstatic u64 access_tx_sdma15_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[31];\n}\n\nstatic u64 access_tx_sdma14_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[30];\n}\n\nstatic u64 access_tx_sdma13_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[29];\n}\n\nstatic u64 access_tx_sdma12_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[28];\n}\n\nstatic u64 access_tx_sdma11_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[27];\n}\n\nstatic u64 access_tx_sdma10_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[26];\n}\n\nstatic u64 access_tx_sdma9_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[25];\n}\n\nstatic u64 access_tx_sdma8_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[24];\n}\n\nstatic u64 access_tx_sdma7_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[23];\n}\n\nstatic u64 access_tx_sdma6_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[22];\n}\n\nstatic u64 access_tx_sdma5_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[21];\n}\n\nstatic u64 access_tx_sdma4_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[20];\n}\n\nstatic u64 access_tx_sdma3_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[19];\n}\n\nstatic u64 access_tx_sdma2_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[18];\n}\n\nstatic u64 access_tx_sdma1_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[17];\n}\n\nstatic u64 access_tx_sdma0_disallowed_packet_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[16];\n}\n\nstatic u64 access_tx_config_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t   void *context, int vl, int mode,\n\t\t\t\t\t   u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[15];\n}\n\nstatic u64 access_tx_sbrd_ctl_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[14];\n}\n\nstatic u64 access_tx_launch_csr_parity_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[13];\n}\n\nstatic u64 access_tx_illegal_vl_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[12];\n}\n\nstatic u64 access_tx_sbrd_ctl_state_machine_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[11];\n}\n\nstatic u64 access_egress_reserved_10_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[10];\n}\n\nstatic u64 access_egress_reserved_9_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[9];\n}\n\nstatic u64 access_tx_sdma_launch_intf_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[8];\n}\n\nstatic u64 access_tx_pio_launch_intf_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[7];\n}\n\nstatic u64 access_egress_reserved_6_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[6];\n}\n\nstatic u64 access_tx_incorrect_link_state_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[5];\n}\n\nstatic u64 access_tx_linkdown_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[4];\n}\n\nstatic u64 access_tx_egress_fifi_underrun_or_parity_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[3];\n}\n\nstatic u64 access_egress_reserved_2_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[2];\n}\n\nstatic u64 access_tx_pkt_integrity_mem_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[1];\n}\n\nstatic u64 access_tx_pkt_integrity_mem_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_egress_err_status_cnt[0];\n}\n\n \nstatic u64 access_send_csr_write_bad_addr_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_err_status_cnt[2];\n}\n\nstatic u64 access_send_csr_read_bad_addr_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\t void *context, int vl,\n\t\t\t\t\t\t int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_err_status_cnt[1];\n}\n\nstatic u64 access_send_csr_parity_cnt(const struct cntr_entry *entry,\n\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->send_err_status_cnt[0];\n}\n\n \nstatic u64 access_pio_write_out_of_bounds_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_ctxt_err_status_cnt[4];\n}\n\nstatic u64 access_pio_write_overflow_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_ctxt_err_status_cnt[3];\n}\n\nstatic u64 access_pio_write_crosses_boundary_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_ctxt_err_status_cnt[2];\n}\n\nstatic u64 access_pio_disallowed_packet_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t\tvoid *context, int vl,\n\t\t\t\t\t\tint mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_ctxt_err_status_cnt[1];\n}\n\nstatic u64 access_pio_inconsistent_sop_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_ctxt_err_status_cnt[0];\n}\n\n \nstatic u64 access_sdma_header_request_fifo_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[23];\n}\n\nstatic u64 access_sdma_header_storage_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[22];\n}\n\nstatic u64 access_sdma_packet_tracking_cor_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[21];\n}\n\nstatic u64 access_sdma_assembly_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[20];\n}\n\nstatic u64 access_sdma_desc_table_cor_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[19];\n}\n\nstatic u64 access_sdma_header_request_fifo_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[18];\n}\n\nstatic u64 access_sdma_header_storage_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[17];\n}\n\nstatic u64 access_sdma_packet_tracking_unc_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[16];\n}\n\nstatic u64 access_sdma_assembly_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[15];\n}\n\nstatic u64 access_sdma_desc_table_unc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[14];\n}\n\nstatic u64 access_sdma_timeout_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t       void *context, int vl, int mode,\n\t\t\t\t       u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[13];\n}\n\nstatic u64 access_sdma_header_length_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[12];\n}\n\nstatic u64 access_sdma_header_address_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t      void *context, int vl, int mode,\n\t\t\t\t\t      u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[11];\n}\n\nstatic u64 access_sdma_header_select_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t     void *context, int vl, int mode,\n\t\t\t\t\t     u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[10];\n}\n\nstatic u64 access_sdma_reserved_9_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[9];\n}\n\nstatic u64 access_sdma_packet_desc_overflow_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[8];\n}\n\nstatic u64 access_sdma_length_mismatch_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t       void *context, int vl,\n\t\t\t\t\t       int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[7];\n}\n\nstatic u64 access_sdma_halt_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t    void *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[6];\n}\n\nstatic u64 access_sdma_mem_read_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[5];\n}\n\nstatic u64 access_sdma_first_desc_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t  void *context, int vl, int mode,\n\t\t\t\t\t  u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[4];\n}\n\nstatic u64 access_sdma_tail_out_of_bounds_err_cnt(\n\t\t\t\tconst struct cntr_entry *entry,\n\t\t\t\tvoid *context, int vl, int mode, u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[3];\n}\n\nstatic u64 access_sdma_too_long_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[2];\n}\n\nstatic u64 access_sdma_gen_mismatch_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\t    void *context, int vl, int mode,\n\t\t\t\t\t    u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[1];\n}\n\nstatic u64 access_sdma_wrong_dw_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t\tvoid *context, int vl, int mode,\n\t\t\t\t\tu64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\treturn dd->sw_send_dma_eng_err_status_cnt[0];\n}\n\nstatic u64 access_dc_rcv_err_cnt(const struct cntr_entry *entry,\n\t\t\t\t void *context, int vl, int mode,\n\t\t\t\t u64 data)\n{\n\tstruct hfi1_devdata *dd = (struct hfi1_devdata *)context;\n\n\tu64 val = 0;\n\tu64 csr = entry->csr;\n\n\tval = read_write_csr(dd, csr, mode, data);\n\tif (mode == CNTR_MODE_R) {\n\t\tval = val > CNTR_MAX - dd->sw_rcv_bypass_packet_errors ?\n\t\t\tCNTR_MAX : val + dd->sw_rcv_bypass_packet_errors;\n\t} else if (mode == CNTR_MODE_W) {\n\t\tdd->sw_rcv_bypass_packet_errors = 0;\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid cntr register access mode\");\n\t\treturn 0;\n\t}\n\treturn val;\n}\n\n#define def_access_sw_cpu(cntr) \\\nstatic u64 access_sw_cpu_##cntr(const struct cntr_entry *entry,\t\t      \\\n\t\t\t      void *context, int vl, int mode, u64 data)      \\\n{\t\t\t\t\t\t\t\t\t      \\\n\tstruct hfi1_pportdata *ppd = (struct hfi1_pportdata *)context;\t      \\\n\treturn read_write_cpu(ppd->dd, &ppd->ibport_data.rvp.z_ ##cntr,\t      \\\n\t\t\t      ppd->ibport_data.rvp.cntr, vl,\t\t      \\\n\t\t\t      mode, data);\t\t\t\t      \\\n}\n\ndef_access_sw_cpu(rc_acks);\ndef_access_sw_cpu(rc_qacks);\ndef_access_sw_cpu(rc_delayed_comp);\n\n#define def_access_ibp_counter(cntr) \\\nstatic u64 access_ibp_##cntr(const struct cntr_entry *entry,\t\t      \\\n\t\t\t\tvoid *context, int vl, int mode, u64 data)    \\\n{\t\t\t\t\t\t\t\t\t      \\\n\tstruct hfi1_pportdata *ppd = (struct hfi1_pportdata *)context;\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\tif (vl != CNTR_INVALID_VL)\t\t\t\t\t      \\\n\t\treturn 0;\t\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\treturn read_write_sw(ppd->dd, &ppd->ibport_data.rvp.n_ ##cntr,\t      \\\n\t\t\t     mode, data);\t\t\t\t      \\\n}\n\ndef_access_ibp_counter(loop_pkts);\ndef_access_ibp_counter(rc_resends);\ndef_access_ibp_counter(rnr_naks);\ndef_access_ibp_counter(other_naks);\ndef_access_ibp_counter(rc_timeouts);\ndef_access_ibp_counter(pkt_drops);\ndef_access_ibp_counter(dmawait);\ndef_access_ibp_counter(rc_seqnak);\ndef_access_ibp_counter(rc_dupreq);\ndef_access_ibp_counter(rdma_seq);\ndef_access_ibp_counter(unaligned);\ndef_access_ibp_counter(seq_naks);\ndef_access_ibp_counter(rc_crwaits);\n\nstatic struct cntr_entry dev_cntrs[DEV_CNTR_LAST] = {\n[C_RCV_OVF] = RXE32_DEV_CNTR_ELEM(RcvOverflow, RCV_BUF_OVFL_CNT, CNTR_SYNTH),\n[C_RX_LEN_ERR] = RXE32_DEV_CNTR_ELEM(RxLenErr, RCV_LENGTH_ERR_CNT, CNTR_SYNTH),\n[C_RX_SHORT_ERR] = RXE32_DEV_CNTR_ELEM(RxShrErr, RCV_SHORT_ERR_CNT, CNTR_SYNTH),\n[C_RX_ICRC_ERR] = RXE32_DEV_CNTR_ELEM(RxICrcErr, RCV_ICRC_ERR_CNT, CNTR_SYNTH),\n[C_RX_EBP] = RXE32_DEV_CNTR_ELEM(RxEbpCnt, RCV_EBP_CNT, CNTR_SYNTH),\n[C_RX_TID_FULL] = RXE32_DEV_CNTR_ELEM(RxTIDFullEr, RCV_TID_FULL_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_RX_TID_INVALID] = RXE32_DEV_CNTR_ELEM(RxTIDInvalid, RCV_TID_VALID_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_RX_TID_FLGMS] = RXE32_DEV_CNTR_ELEM(RxTidFLGMs,\n\t\t\tRCV_TID_FLOW_GEN_MISMATCH_CNT,\n\t\t\tCNTR_NORMAL),\n[C_RX_CTX_EGRS] = RXE32_DEV_CNTR_ELEM(RxCtxEgrS, RCV_CONTEXT_EGR_STALL,\n\t\t\tCNTR_NORMAL),\n[C_RCV_TID_FLSMS] = RXE32_DEV_CNTR_ELEM(RxTidFLSMs,\n\t\t\tRCV_TID_FLOW_SEQ_MISMATCH_CNT, CNTR_NORMAL),\n[C_CCE_PCI_CR_ST] = CCE_PERF_DEV_CNTR_ELEM(CcePciCrSt,\n\t\t\tCCE_PCIE_POSTED_CRDT_STALL_CNT, CNTR_NORMAL),\n[C_CCE_PCI_TR_ST] = CCE_PERF_DEV_CNTR_ELEM(CcePciTrSt, CCE_PCIE_TRGT_STALL_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_PIO_WR_ST] = CCE_PERF_DEV_CNTR_ELEM(CcePioWrSt, CCE_PIO_WR_STALL_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_ERR_INT] = CCE_INT_DEV_CNTR_ELEM(CceErrInt, CCE_ERR_INT_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_SDMA_INT] = CCE_INT_DEV_CNTR_ELEM(CceSdmaInt, CCE_SDMA_INT_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_MISC_INT] = CCE_INT_DEV_CNTR_ELEM(CceMiscInt, CCE_MISC_INT_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_RCV_AV_INT] = CCE_INT_DEV_CNTR_ELEM(CceRcvAvInt, CCE_RCV_AVAIL_INT_CNT,\n\t\t\tCNTR_NORMAL),\n[C_CCE_RCV_URG_INT] = CCE_INT_DEV_CNTR_ELEM(CceRcvUrgInt,\n\t\t\tCCE_RCV_URGENT_INT_CNT,\tCNTR_NORMAL),\n[C_CCE_SEND_CR_INT] = CCE_INT_DEV_CNTR_ELEM(CceSndCrInt,\n\t\t\tCCE_SEND_CREDIT_INT_CNT, CNTR_NORMAL),\n[C_DC_UNC_ERR] = DC_PERF_CNTR(DcUnctblErr, DCC_ERR_UNCORRECTABLE_CNT,\n\t\t\t      CNTR_SYNTH),\n[C_DC_RCV_ERR] = CNTR_ELEM(\"DcRecvErr\", DCC_ERR_PORTRCV_ERR_CNT, 0, CNTR_SYNTH,\n\t\t\t    access_dc_rcv_err_cnt),\n[C_DC_FM_CFG_ERR] = DC_PERF_CNTR(DcFmCfgErr, DCC_ERR_FMCONFIG_ERR_CNT,\n\t\t\t\t CNTR_SYNTH),\n[C_DC_RMT_PHY_ERR] = DC_PERF_CNTR(DcRmtPhyErr, DCC_ERR_RCVREMOTE_PHY_ERR_CNT,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_DROPPED_PKT] = DC_PERF_CNTR(DcDroppedPkt, DCC_ERR_DROPPED_PKT_CNT,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_MC_XMIT_PKTS] = DC_PERF_CNTR(DcMcXmitPkts,\n\t\t\t\t   DCC_PRF_PORT_XMIT_MULTICAST_CNT, CNTR_SYNTH),\n[C_DC_MC_RCV_PKTS] = DC_PERF_CNTR(DcMcRcvPkts,\n\t\t\t\t  DCC_PRF_PORT_RCV_MULTICAST_PKT_CNT,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_XMIT_CERR] = DC_PERF_CNTR(DcXmitCorr,\n\t\t\t\tDCC_PRF_PORT_XMIT_CORRECTABLE_CNT, CNTR_SYNTH),\n[C_DC_RCV_CERR] = DC_PERF_CNTR(DcRcvCorrCnt, DCC_PRF_PORT_RCV_CORRECTABLE_CNT,\n\t\t\t       CNTR_SYNTH),\n[C_DC_RCV_FCC] = DC_PERF_CNTR(DcRxFCntl, DCC_PRF_RX_FLOW_CRTL_CNT,\n\t\t\t      CNTR_SYNTH),\n[C_DC_XMIT_FCC] = DC_PERF_CNTR(DcXmitFCntl, DCC_PRF_TX_FLOW_CRTL_CNT,\n\t\t\t       CNTR_SYNTH),\n[C_DC_XMIT_FLITS] = DC_PERF_CNTR(DcXmitFlits, DCC_PRF_PORT_XMIT_DATA_CNT,\n\t\t\t\t CNTR_SYNTH),\n[C_DC_RCV_FLITS] = DC_PERF_CNTR(DcRcvFlits, DCC_PRF_PORT_RCV_DATA_CNT,\n\t\t\t\tCNTR_SYNTH),\n[C_DC_XMIT_PKTS] = DC_PERF_CNTR(DcXmitPkts, DCC_PRF_PORT_XMIT_PKTS_CNT,\n\t\t\t\tCNTR_SYNTH),\n[C_DC_RCV_PKTS] = DC_PERF_CNTR(DcRcvPkts, DCC_PRF_PORT_RCV_PKTS_CNT,\n\t\t\t       CNTR_SYNTH),\n[C_DC_RX_FLIT_VL] = DC_PERF_CNTR(DcRxFlitVl, DCC_PRF_PORT_VL_RCV_DATA_CNT,\n\t\t\t\t CNTR_SYNTH | CNTR_VL),\n[C_DC_RX_PKT_VL] = DC_PERF_CNTR(DcRxPktVl, DCC_PRF_PORT_VL_RCV_PKTS_CNT,\n\t\t\t\tCNTR_SYNTH | CNTR_VL),\n[C_DC_RCV_FCN] = DC_PERF_CNTR(DcRcvFcn, DCC_PRF_PORT_RCV_FECN_CNT, CNTR_SYNTH),\n[C_DC_RCV_FCN_VL] = DC_PERF_CNTR(DcRcvFcnVl, DCC_PRF_PORT_VL_RCV_FECN_CNT,\n\t\t\t\t CNTR_SYNTH | CNTR_VL),\n[C_DC_RCV_BCN] = DC_PERF_CNTR(DcRcvBcn, DCC_PRF_PORT_RCV_BECN_CNT, CNTR_SYNTH),\n[C_DC_RCV_BCN_VL] = DC_PERF_CNTR(DcRcvBcnVl, DCC_PRF_PORT_VL_RCV_BECN_CNT,\n\t\t\t\t CNTR_SYNTH | CNTR_VL),\n[C_DC_RCV_BBL] = DC_PERF_CNTR(DcRcvBbl, DCC_PRF_PORT_RCV_BUBBLE_CNT,\n\t\t\t      CNTR_SYNTH),\n[C_DC_RCV_BBL_VL] = DC_PERF_CNTR(DcRcvBblVl, DCC_PRF_PORT_VL_RCV_BUBBLE_CNT,\n\t\t\t\t CNTR_SYNTH | CNTR_VL),\n[C_DC_MARK_FECN] = DC_PERF_CNTR(DcMarkFcn, DCC_PRF_PORT_MARK_FECN_CNT,\n\t\t\t\tCNTR_SYNTH),\n[C_DC_MARK_FECN_VL] = DC_PERF_CNTR(DcMarkFcnVl, DCC_PRF_PORT_VL_MARK_FECN_CNT,\n\t\t\t\t   CNTR_SYNTH | CNTR_VL),\n[C_DC_TOTAL_CRC] =\n\tDC_PERF_CNTR_LCB(DcTotCrc, DC_LCB_ERR_INFO_TOTAL_CRC_ERR,\n\t\t\t CNTR_SYNTH),\n[C_DC_CRC_LN0] = DC_PERF_CNTR_LCB(DcCrcLn0, DC_LCB_ERR_INFO_CRC_ERR_LN0,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_CRC_LN1] = DC_PERF_CNTR_LCB(DcCrcLn1, DC_LCB_ERR_INFO_CRC_ERR_LN1,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_CRC_LN2] = DC_PERF_CNTR_LCB(DcCrcLn2, DC_LCB_ERR_INFO_CRC_ERR_LN2,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_CRC_LN3] = DC_PERF_CNTR_LCB(DcCrcLn3, DC_LCB_ERR_INFO_CRC_ERR_LN3,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_CRC_MULT_LN] =\n\tDC_PERF_CNTR_LCB(DcMultLn, DC_LCB_ERR_INFO_CRC_ERR_MULTI_LN,\n\t\t\t CNTR_SYNTH),\n[C_DC_TX_REPLAY] = DC_PERF_CNTR_LCB(DcTxReplay, DC_LCB_ERR_INFO_TX_REPLAY_CNT,\n\t\t\t\t    CNTR_SYNTH),\n[C_DC_RX_REPLAY] = DC_PERF_CNTR_LCB(DcRxReplay, DC_LCB_ERR_INFO_RX_REPLAY_CNT,\n\t\t\t\t    CNTR_SYNTH),\n[C_DC_SEQ_CRC_CNT] =\n\tDC_PERF_CNTR_LCB(DcLinkSeqCrc, DC_LCB_ERR_INFO_SEQ_CRC_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_ESC0_ONLY_CNT] =\n\tDC_PERF_CNTR_LCB(DcEsc0, DC_LCB_ERR_INFO_ESCAPE_0_ONLY_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_ESC0_PLUS1_CNT] =\n\tDC_PERF_CNTR_LCB(DcEsc1, DC_LCB_ERR_INFO_ESCAPE_0_PLUS1_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_ESC0_PLUS2_CNT] =\n\tDC_PERF_CNTR_LCB(DcEsc0Plus2, DC_LCB_ERR_INFO_ESCAPE_0_PLUS2_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_REINIT_FROM_PEER_CNT] =\n\tDC_PERF_CNTR_LCB(DcReinitPeer, DC_LCB_ERR_INFO_REINIT_FROM_PEER_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_SBE_CNT] = DC_PERF_CNTR_LCB(DcSbe, DC_LCB_ERR_INFO_SBE_CNT,\n\t\t\t\t  CNTR_SYNTH),\n[C_DC_MISC_FLG_CNT] =\n\tDC_PERF_CNTR_LCB(DcMiscFlg, DC_LCB_ERR_INFO_MISC_FLG_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_PRF_GOOD_LTP_CNT] =\n\tDC_PERF_CNTR_LCB(DcGoodLTP, DC_LCB_PRF_GOOD_LTP_CNT, CNTR_SYNTH),\n[C_DC_PRF_ACCEPTED_LTP_CNT] =\n\tDC_PERF_CNTR_LCB(DcAccLTP, DC_LCB_PRF_ACCEPTED_LTP_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_PRF_RX_FLIT_CNT] =\n\tDC_PERF_CNTR_LCB(DcPrfRxFlit, DC_LCB_PRF_RX_FLIT_CNT, CNTR_SYNTH),\n[C_DC_PRF_TX_FLIT_CNT] =\n\tDC_PERF_CNTR_LCB(DcPrfTxFlit, DC_LCB_PRF_TX_FLIT_CNT, CNTR_SYNTH),\n[C_DC_PRF_CLK_CNTR] =\n\tDC_PERF_CNTR_LCB(DcPrfClk, DC_LCB_PRF_CLK_CNTR, CNTR_SYNTH),\n[C_DC_PG_DBG_FLIT_CRDTS_CNT] =\n\tDC_PERF_CNTR_LCB(DcFltCrdts, DC_LCB_PG_DBG_FLIT_CRDTS_CNT, CNTR_SYNTH),\n[C_DC_PG_STS_PAUSE_COMPLETE_CNT] =\n\tDC_PERF_CNTR_LCB(DcPauseComp, DC_LCB_PG_STS_PAUSE_COMPLETE_CNT,\n\t\t\t CNTR_SYNTH),\n[C_DC_PG_STS_TX_SBE_CNT] =\n\tDC_PERF_CNTR_LCB(DcStsTxSbe, DC_LCB_PG_STS_TX_SBE_CNT, CNTR_SYNTH),\n[C_DC_PG_STS_TX_MBE_CNT] =\n\tDC_PERF_CNTR_LCB(DcStsTxMbe, DC_LCB_PG_STS_TX_MBE_CNT,\n\t\t\t CNTR_SYNTH),\n[C_SW_CPU_INTR] = CNTR_ELEM(\"Intr\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_cpu_intr),\n[C_SW_CPU_RCV_LIM] = CNTR_ELEM(\"RcvLimit\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_cpu_rcv_limit),\n[C_SW_CTX0_SEQ_DROP] = CNTR_ELEM(\"SeqDrop0\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_ctx0_seq_drop),\n[C_SW_VTX_WAIT] = CNTR_ELEM(\"vTxWait\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_vtx_wait),\n[C_SW_PIO_WAIT] = CNTR_ELEM(\"PioWait\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_pio_wait),\n[C_SW_PIO_DRAIN] = CNTR_ELEM(\"PioDrain\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_pio_drain),\n[C_SW_KMEM_WAIT] = CNTR_ELEM(\"KmemWait\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_kmem_wait),\n[C_SW_TID_WAIT] = CNTR_ELEM(\"TidWait\", 0, 0, CNTR_NORMAL,\n\t\t\t    hfi1_access_sw_tid_wait),\n[C_SW_SEND_SCHED] = CNTR_ELEM(\"SendSched\", 0, 0, CNTR_NORMAL,\n\t\t\t    access_sw_send_schedule),\n[C_SDMA_DESC_FETCHED_CNT] = CNTR_ELEM(\"SDEDscFdCn\",\n\t\t\t\t      SEND_DMA_DESC_FETCHED_CNT, 0,\n\t\t\t\t      CNTR_NORMAL | CNTR_32BIT | CNTR_SDMA,\n\t\t\t\t      dev_access_u32_csr),\n[C_SDMA_INT_CNT] = CNTR_ELEM(\"SDMAInt\", 0, 0,\n\t\t\t     CNTR_NORMAL | CNTR_32BIT | CNTR_SDMA,\n\t\t\t     access_sde_int_cnt),\n[C_SDMA_ERR_CNT] = CNTR_ELEM(\"SDMAErrCt\", 0, 0,\n\t\t\t     CNTR_NORMAL | CNTR_32BIT | CNTR_SDMA,\n\t\t\t     access_sde_err_cnt),\n[C_SDMA_IDLE_INT_CNT] = CNTR_ELEM(\"SDMAIdInt\", 0, 0,\n\t\t\t\t  CNTR_NORMAL | CNTR_32BIT | CNTR_SDMA,\n\t\t\t\t  access_sde_idle_int_cnt),\n[C_SDMA_PROGRESS_INT_CNT] = CNTR_ELEM(\"SDMAPrIntCn\", 0, 0,\n\t\t\t\t      CNTR_NORMAL | CNTR_32BIT | CNTR_SDMA,\n\t\t\t\t      access_sde_progress_int_cnt),\n \n[C_MISC_PLL_LOCK_FAIL_ERR] = CNTR_ELEM(\"MISC_PLL_LOCK_FAIL_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_pll_lock_fail_err_cnt),\n[C_MISC_MBIST_FAIL_ERR] = CNTR_ELEM(\"MISC_MBIST_FAIL_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_mbist_fail_err_cnt),\n[C_MISC_INVALID_EEP_CMD_ERR] = CNTR_ELEM(\"MISC_INVALID_EEP_CMD_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_invalid_eep_cmd_err_cnt),\n[C_MISC_EFUSE_DONE_PARITY_ERR] = CNTR_ELEM(\"MISC_EFUSE_DONE_PARITY_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_efuse_done_parity_err_cnt),\n[C_MISC_EFUSE_WRITE_ERR] = CNTR_ELEM(\"MISC_EFUSE_WRITE_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_efuse_write_err_cnt),\n[C_MISC_EFUSE_READ_BAD_ADDR_ERR] = CNTR_ELEM(\"MISC_EFUSE_READ_BAD_ADDR_ERR\", 0,\n\t\t\t\t0, CNTR_NORMAL,\n\t\t\t\taccess_misc_efuse_read_bad_addr_err_cnt),\n[C_MISC_EFUSE_CSR_PARITY_ERR] = CNTR_ELEM(\"MISC_EFUSE_CSR_PARITY_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_efuse_csr_parity_err_cnt),\n[C_MISC_FW_AUTH_FAILED_ERR] = CNTR_ELEM(\"MISC_FW_AUTH_FAILED_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_fw_auth_failed_err_cnt),\n[C_MISC_KEY_MISMATCH_ERR] = CNTR_ELEM(\"MISC_KEY_MISMATCH_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_key_mismatch_err_cnt),\n[C_MISC_SBUS_WRITE_FAILED_ERR] = CNTR_ELEM(\"MISC_SBUS_WRITE_FAILED_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_sbus_write_failed_err_cnt),\n[C_MISC_CSR_WRITE_BAD_ADDR_ERR] = CNTR_ELEM(\"MISC_CSR_WRITE_BAD_ADDR_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_csr_write_bad_addr_err_cnt),\n[C_MISC_CSR_READ_BAD_ADDR_ERR] = CNTR_ELEM(\"MISC_CSR_READ_BAD_ADDR_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_csr_read_bad_addr_err_cnt),\n[C_MISC_CSR_PARITY_ERR] = CNTR_ELEM(\"MISC_CSR_PARITY_ERR\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_misc_csr_parity_err_cnt),\n \n[C_CCE_ERR_STATUS_AGGREGATED_CNT] = CNTR_ELEM(\"CceErrStatusAggregatedCnt\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_sw_cce_err_status_aggregated_cnt),\n[C_CCE_MSIX_CSR_PARITY_ERR] = CNTR_ELEM(\"CceMsixCsrParityErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_msix_csr_parity_err_cnt),\n[C_CCE_INT_MAP_UNC_ERR] = CNTR_ELEM(\"CceIntMapUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_int_map_unc_err_cnt),\n[C_CCE_INT_MAP_COR_ERR] = CNTR_ELEM(\"CceIntMapCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_int_map_cor_err_cnt),\n[C_CCE_MSIX_TABLE_UNC_ERR] = CNTR_ELEM(\"CceMsixTableUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_msix_table_unc_err_cnt),\n[C_CCE_MSIX_TABLE_COR_ERR] = CNTR_ELEM(\"CceMsixTableCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_msix_table_cor_err_cnt),\n[C_CCE_RXDMA_CONV_FIFO_PARITY_ERR] = CNTR_ELEM(\"CceRxdmaConvFifoParityErr\", 0,\n\t\t\t\t0, CNTR_NORMAL,\n\t\t\t\taccess_cce_rxdma_conv_fifo_parity_err_cnt),\n[C_CCE_RCPL_ASYNC_FIFO_PARITY_ERR] = CNTR_ELEM(\"CceRcplAsyncFifoParityErr\", 0,\n\t\t\t\t0, CNTR_NORMAL,\n\t\t\t\taccess_cce_rcpl_async_fifo_parity_err_cnt),\n[C_CCE_SEG_WRITE_BAD_ADDR_ERR] = CNTR_ELEM(\"CceSegWriteBadAddrErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_seg_write_bad_addr_err_cnt),\n[C_CCE_SEG_READ_BAD_ADDR_ERR] = CNTR_ELEM(\"CceSegReadBadAddrErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_seg_read_bad_addr_err_cnt),\n[C_LA_TRIGGERED] = CNTR_ELEM(\"Cce LATriggered\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_la_triggered_cnt),\n[C_CCE_TRGT_CPL_TIMEOUT_ERR] = CNTR_ELEM(\"CceTrgtCplTimeoutErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_trgt_cpl_timeout_err_cnt),\n[C_PCIC_RECEIVE_PARITY_ERR] = CNTR_ELEM(\"PcicReceiveParityErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_receive_parity_err_cnt),\n[C_PCIC_TRANSMIT_BACK_PARITY_ERR] = CNTR_ELEM(\"PcicTransmitBackParityErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_transmit_back_parity_err_cnt),\n[C_PCIC_TRANSMIT_FRONT_PARITY_ERR] = CNTR_ELEM(\"PcicTransmitFrontParityErr\", 0,\n\t\t\t\t0, CNTR_NORMAL,\n\t\t\t\taccess_pcic_transmit_front_parity_err_cnt),\n[C_PCIC_CPL_DAT_Q_UNC_ERR] = CNTR_ELEM(\"PcicCplDatQUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_cpl_dat_q_unc_err_cnt),\n[C_PCIC_CPL_HD_Q_UNC_ERR] = CNTR_ELEM(\"PcicCplHdQUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_cpl_hd_q_unc_err_cnt),\n[C_PCIC_POST_DAT_Q_UNC_ERR] = CNTR_ELEM(\"PcicPostDatQUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_post_dat_q_unc_err_cnt),\n[C_PCIC_POST_HD_Q_UNC_ERR] = CNTR_ELEM(\"PcicPostHdQUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_post_hd_q_unc_err_cnt),\n[C_PCIC_RETRY_SOT_MEM_UNC_ERR] = CNTR_ELEM(\"PcicRetrySotMemUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_retry_sot_mem_unc_err_cnt),\n[C_PCIC_RETRY_MEM_UNC_ERR] = CNTR_ELEM(\"PcicRetryMemUncErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_retry_mem_unc_err),\n[C_PCIC_N_POST_DAT_Q_PARITY_ERR] = CNTR_ELEM(\"PcicNPostDatQParityErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_n_post_dat_q_parity_err_cnt),\n[C_PCIC_N_POST_H_Q_PARITY_ERR] = CNTR_ELEM(\"PcicNPostHQParityErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_n_post_h_q_parity_err_cnt),\n[C_PCIC_CPL_DAT_Q_COR_ERR] = CNTR_ELEM(\"PcicCplDatQCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_cpl_dat_q_cor_err_cnt),\n[C_PCIC_CPL_HD_Q_COR_ERR] = CNTR_ELEM(\"PcicCplHdQCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_cpl_hd_q_cor_err_cnt),\n[C_PCIC_POST_DAT_Q_COR_ERR] = CNTR_ELEM(\"PcicPostDatQCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_post_dat_q_cor_err_cnt),\n[C_PCIC_POST_HD_Q_COR_ERR] = CNTR_ELEM(\"PcicPostHdQCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_post_hd_q_cor_err_cnt),\n[C_PCIC_RETRY_SOT_MEM_COR_ERR] = CNTR_ELEM(\"PcicRetrySotMemCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_retry_sot_mem_cor_err_cnt),\n[C_PCIC_RETRY_MEM_COR_ERR] = CNTR_ELEM(\"PcicRetryMemCorErr\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_pcic_retry_mem_cor_err_cnt),\n[C_CCE_CLI1_ASYNC_FIFO_DBG_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\t\"CceCli1AsyncFifoDbgParityError\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_cli1_async_fifo_dbg_parity_err_cnt),\n[C_CCE_CLI1_ASYNC_FIFO_RXDMA_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\t\"CceCli1AsyncFifoRxdmaParityError\", 0, 0,\n\t\t\t\tCNTR_NORMAL,\n\t\t\t\taccess_cce_cli1_async_fifo_rxdma_parity_err_cnt\n\t\t\t\t),\n[C_CCE_CLI1_ASYNC_FIFO_SDMA_HD_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"CceCli1AsyncFifoSdmaHdParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_cli1_async_fifo_sdma_hd_parity_err_cnt),\n[C_CCE_CLI1_ASYNC_FIFO_PIO_CRDT_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"CceCli1AsyncFifoPioCrdtParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_cl1_async_fifo_pio_crdt_parity_err_cnt),\n[C_CCE_CLI2_ASYNC_FIFO_PARITY_ERR] = CNTR_ELEM(\"CceCli2AsyncFifoParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_cce_cli2_async_fifo_parity_err_cnt),\n[C_CCE_CSR_CFG_BUS_PARITY_ERR] = CNTR_ELEM(\"CceCsrCfgBusParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_csr_cfg_bus_parity_err_cnt),\n[C_CCE_CLI0_ASYNC_FIFO_PARTIY_ERR] = CNTR_ELEM(\"CceCli0AsyncFifoParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_cce_cli0_async_fifo_parity_err_cnt),\n[C_CCE_RSPD_DATA_PARITY_ERR] = CNTR_ELEM(\"CceRspdDataParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_rspd_data_parity_err_cnt),\n[C_CCE_TRGT_ACCESS_ERR] = CNTR_ELEM(\"CceTrgtAccessErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_trgt_access_err_cnt),\n[C_CCE_TRGT_ASYNC_FIFO_PARITY_ERR] = CNTR_ELEM(\"CceTrgtAsyncFifoParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_cce_trgt_async_fifo_parity_err_cnt),\n[C_CCE_CSR_WRITE_BAD_ADDR_ERR] = CNTR_ELEM(\"CceCsrWriteBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_csr_write_bad_addr_err_cnt),\n[C_CCE_CSR_READ_BAD_ADDR_ERR] = CNTR_ELEM(\"CceCsrReadBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_cce_csr_read_bad_addr_err_cnt),\n[C_CCE_CSR_PARITY_ERR] = CNTR_ELEM(\"CceCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_ccs_csr_parity_err_cnt),\n\n \n[C_RX_CSR_PARITY_ERR] = CNTR_ELEM(\"RxCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_csr_parity_err_cnt),\n[C_RX_CSR_WRITE_BAD_ADDR_ERR] = CNTR_ELEM(\"RxCsrWriteBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_csr_write_bad_addr_err_cnt),\n[C_RX_CSR_READ_BAD_ADDR_ERR] = CNTR_ELEM(\"RxCsrReadBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_csr_read_bad_addr_err_cnt),\n[C_RX_DMA_CSR_UNC_ERR] = CNTR_ELEM(\"RxDmaCsrUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_csr_unc_err_cnt),\n[C_RX_DMA_DQ_FSM_ENCODING_ERR] = CNTR_ELEM(\"RxDmaDqFsmEncodingErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_dq_fsm_encoding_err_cnt),\n[C_RX_DMA_EQ_FSM_ENCODING_ERR] = CNTR_ELEM(\"RxDmaEqFsmEncodingErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_eq_fsm_encoding_err_cnt),\n[C_RX_DMA_CSR_PARITY_ERR] = CNTR_ELEM(\"RxDmaCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_csr_parity_err_cnt),\n[C_RX_RBUF_DATA_COR_ERR] = CNTR_ELEM(\"RxRbufDataCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_data_cor_err_cnt),\n[C_RX_RBUF_DATA_UNC_ERR] = CNTR_ELEM(\"RxRbufDataUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_data_unc_err_cnt),\n[C_RX_DMA_DATA_FIFO_RD_COR_ERR] = CNTR_ELEM(\"RxDmaDataFifoRdCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_data_fifo_rd_cor_err_cnt),\n[C_RX_DMA_DATA_FIFO_RD_UNC_ERR] = CNTR_ELEM(\"RxDmaDataFifoRdUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_data_fifo_rd_unc_err_cnt),\n[C_RX_DMA_HDR_FIFO_RD_COR_ERR] = CNTR_ELEM(\"RxDmaHdrFifoRdCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_hdr_fifo_rd_cor_err_cnt),\n[C_RX_DMA_HDR_FIFO_RD_UNC_ERR] = CNTR_ELEM(\"RxDmaHdrFifoRdUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_hdr_fifo_rd_unc_err_cnt),\n[C_RX_RBUF_DESC_PART2_COR_ERR] = CNTR_ELEM(\"RxRbufDescPart2CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_desc_part2_cor_err_cnt),\n[C_RX_RBUF_DESC_PART2_UNC_ERR] = CNTR_ELEM(\"RxRbufDescPart2UncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_desc_part2_unc_err_cnt),\n[C_RX_RBUF_DESC_PART1_COR_ERR] = CNTR_ELEM(\"RxRbufDescPart1CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_desc_part1_cor_err_cnt),\n[C_RX_RBUF_DESC_PART1_UNC_ERR] = CNTR_ELEM(\"RxRbufDescPart1UncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_desc_part1_unc_err_cnt),\n[C_RX_HQ_INTR_FSM_ERR] = CNTR_ELEM(\"RxHqIntrFsmErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_hq_intr_fsm_err_cnt),\n[C_RX_HQ_INTR_CSR_PARITY_ERR] = CNTR_ELEM(\"RxHqIntrCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_hq_intr_csr_parity_err_cnt),\n[C_RX_LOOKUP_CSR_PARITY_ERR] = CNTR_ELEM(\"RxLookupCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_lookup_csr_parity_err_cnt),\n[C_RX_LOOKUP_RCV_ARRAY_COR_ERR] = CNTR_ELEM(\"RxLookupRcvArrayCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_lookup_rcv_array_cor_err_cnt),\n[C_RX_LOOKUP_RCV_ARRAY_UNC_ERR] = CNTR_ELEM(\"RxLookupRcvArrayUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_lookup_rcv_array_unc_err_cnt),\n[C_RX_LOOKUP_DES_PART2_PARITY_ERR] = CNTR_ELEM(\"RxLookupDesPart2ParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_lookup_des_part2_parity_err_cnt),\n[C_RX_LOOKUP_DES_PART1_UNC_COR_ERR] = CNTR_ELEM(\"RxLookupDesPart1UncCorErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_lookup_des_part1_unc_cor_err_cnt),\n[C_RX_LOOKUP_DES_PART1_UNC_ERR] = CNTR_ELEM(\"RxLookupDesPart1UncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_lookup_des_part1_unc_err_cnt),\n[C_RX_RBUF_NEXT_FREE_BUF_COR_ERR] = CNTR_ELEM(\"RxRbufNextFreeBufCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_next_free_buf_cor_err_cnt),\n[C_RX_RBUF_NEXT_FREE_BUF_UNC_ERR] = CNTR_ELEM(\"RxRbufNextFreeBufUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_next_free_buf_unc_err_cnt),\n[C_RX_RBUF_FL_INIT_WR_ADDR_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"RxRbufFlInitWrAddrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rbuf_fl_init_wr_addr_parity_err_cnt),\n[C_RX_RBUF_FL_INITDONE_PARITY_ERR] = CNTR_ELEM(\"RxRbufFlInitdoneParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_fl_initdone_parity_err_cnt),\n[C_RX_RBUF_FL_WRITE_ADDR_PARITY_ERR] = CNTR_ELEM(\"RxRbufFlWrAddrParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_fl_write_addr_parity_err_cnt),\n[C_RX_RBUF_FL_RD_ADDR_PARITY_ERR] = CNTR_ELEM(\"RxRbufFlRdAddrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_fl_rd_addr_parity_err_cnt),\n[C_RX_RBUF_EMPTY_ERR] = CNTR_ELEM(\"RxRbufEmptyErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_empty_err_cnt),\n[C_RX_RBUF_FULL_ERR] = CNTR_ELEM(\"RxRbufFullErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_full_err_cnt),\n[C_RX_RBUF_BAD_LOOKUP_ERR] = CNTR_ELEM(\"RxRBufBadLookupErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rbuf_bad_lookup_err_cnt),\n[C_RX_RBUF_CTX_ID_PARITY_ERR] = CNTR_ELEM(\"RxRbufCtxIdParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rbuf_ctx_id_parity_err_cnt),\n[C_RX_RBUF_CSR_QEOPDW_PARITY_ERR] = CNTR_ELEM(\"RxRbufCsrQEOPDWParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rbuf_csr_qeopdw_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_NUM_OF_PKT_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"RxRbufCsrQNumOfPktParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_num_of_pkt_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_T1_PTR_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"RxRbufCsrQTlPtrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_t1_ptr_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_HD_PTR_PARITY_ERR] = CNTR_ELEM(\"RxRbufCsrQHdPtrParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_hd_ptr_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_VLD_BIT_PARITY_ERR] = CNTR_ELEM(\"RxRbufCsrQVldBitParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_vld_bit_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_NEXT_BUF_PARITY_ERR] = CNTR_ELEM(\"RxRbufCsrQNextBufParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_next_buf_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_ENT_CNT_PARITY_ERR] = CNTR_ELEM(\"RxRbufCsrQEntCntParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_ent_cnt_parity_err_cnt),\n[C_RX_RBUF_CSR_Q_HEAD_BUF_NUM_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"RxRbufCsrQHeadBufNumParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_csr_q_head_buf_num_parity_err_cnt),\n[C_RX_RBUF_BLOCK_LIST_READ_COR_ERR] = CNTR_ELEM(\"RxRbufBlockListReadCorErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_block_list_read_cor_err_cnt),\n[C_RX_RBUF_BLOCK_LIST_READ_UNC_ERR] = CNTR_ELEM(\"RxRbufBlockListReadUncErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_rx_rbuf_block_list_read_unc_err_cnt),\n[C_RX_RBUF_LOOKUP_DES_COR_ERR] = CNTR_ELEM(\"RxRbufLookupDesCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_lookup_des_cor_err_cnt),\n[C_RX_RBUF_LOOKUP_DES_UNC_ERR] = CNTR_ELEM(\"RxRbufLookupDesUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_lookup_des_unc_err_cnt),\n[C_RX_RBUF_LOOKUP_DES_REG_UNC_COR_ERR] = CNTR_ELEM(\n\t\t\t\"RxRbufLookupDesRegUncCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_lookup_des_reg_unc_cor_err_cnt),\n[C_RX_RBUF_LOOKUP_DES_REG_UNC_ERR] = CNTR_ELEM(\"RxRbufLookupDesRegUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_lookup_des_reg_unc_err_cnt),\n[C_RX_RBUF_FREE_LIST_COR_ERR] = CNTR_ELEM(\"RxRbufFreeListCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_free_list_cor_err_cnt),\n[C_RX_RBUF_FREE_LIST_UNC_ERR] = CNTR_ELEM(\"RxRbufFreeListUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rbuf_free_list_unc_err_cnt),\n[C_RX_RCV_FSM_ENCODING_ERR] = CNTR_ELEM(\"RxRcvFsmEncodingErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_fsm_encoding_err_cnt),\n[C_RX_DMA_FLAG_COR_ERR] = CNTR_ELEM(\"RxDmaFlagCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_flag_cor_err_cnt),\n[C_RX_DMA_FLAG_UNC_ERR] = CNTR_ELEM(\"RxDmaFlagUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_flag_unc_err_cnt),\n[C_RX_DC_SOP_EOP_PARITY_ERR] = CNTR_ELEM(\"RxDcSopEopParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dc_sop_eop_parity_err_cnt),\n[C_RX_RCV_CSR_PARITY_ERR] = CNTR_ELEM(\"RxRcvCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_csr_parity_err_cnt),\n[C_RX_RCV_QP_MAP_TABLE_COR_ERR] = CNTR_ELEM(\"RxRcvQpMapTableCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_qp_map_table_cor_err_cnt),\n[C_RX_RCV_QP_MAP_TABLE_UNC_ERR] = CNTR_ELEM(\"RxRcvQpMapTableUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_qp_map_table_unc_err_cnt),\n[C_RX_RCV_DATA_COR_ERR] = CNTR_ELEM(\"RxRcvDataCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_data_cor_err_cnt),\n[C_RX_RCV_DATA_UNC_ERR] = CNTR_ELEM(\"RxRcvDataUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_data_unc_err_cnt),\n[C_RX_RCV_HDR_COR_ERR] = CNTR_ELEM(\"RxRcvHdrCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_hdr_cor_err_cnt),\n[C_RX_RCV_HDR_UNC_ERR] = CNTR_ELEM(\"RxRcvHdrUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_rcv_hdr_unc_err_cnt),\n[C_RX_DC_INTF_PARITY_ERR] = CNTR_ELEM(\"RxDcIntfParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dc_intf_parity_err_cnt),\n[C_RX_DMA_CSR_COR_ERR] = CNTR_ELEM(\"RxDmaCsrCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_rx_dma_csr_cor_err_cnt),\n \n[C_PIO_PEC_SOP_HEAD_PARITY_ERR] = CNTR_ELEM(\"PioPecSopHeadParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pec_sop_head_parity_err_cnt),\n[C_PIO_PCC_SOP_HEAD_PARITY_ERR] = CNTR_ELEM(\"PioPccSopHeadParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pcc_sop_head_parity_err_cnt),\n[C_PIO_LAST_RETURNED_CNT_PARITY_ERR] = CNTR_ELEM(\"PioLastReturnedCntParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_pio_last_returned_cnt_parity_err_cnt),\n[C_PIO_CURRENT_FREE_CNT_PARITY_ERR] = CNTR_ELEM(\"PioCurrentFreeCntParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_pio_current_free_cnt_parity_err_cnt),\n[C_PIO_RSVD_31_ERR] = CNTR_ELEM(\"Pio Reserved 31\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_reserved_31_err_cnt),\n[C_PIO_RSVD_30_ERR] = CNTR_ELEM(\"Pio Reserved 30\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_reserved_30_err_cnt),\n[C_PIO_PPMC_SOP_LEN_ERR] = CNTR_ELEM(\"PioPpmcSopLenErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_ppmc_sop_len_err_cnt),\n[C_PIO_PPMC_BQC_MEM_PARITY_ERR] = CNTR_ELEM(\"PioPpmcBqcMemParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_ppmc_bqc_mem_parity_err_cnt),\n[C_PIO_VL_FIFO_PARITY_ERR] = CNTR_ELEM(\"PioVlFifoParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_vl_fifo_parity_err_cnt),\n[C_PIO_VLF_SOP_PARITY_ERR] = CNTR_ELEM(\"PioVlfSopParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_vlf_sop_parity_err_cnt),\n[C_PIO_VLF_V1_LEN_PARITY_ERR] = CNTR_ELEM(\"PioVlfVlLenParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_vlf_v1_len_parity_err_cnt),\n[C_PIO_BLOCK_QW_COUNT_PARITY_ERR] = CNTR_ELEM(\"PioBlockQwCountParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_block_qw_count_parity_err_cnt),\n[C_PIO_WRITE_QW_VALID_PARITY_ERR] = CNTR_ELEM(\"PioWriteQwValidParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_qw_valid_parity_err_cnt),\n[C_PIO_STATE_MACHINE_ERR] = CNTR_ELEM(\"PioStateMachineErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_state_machine_err_cnt),\n[C_PIO_WRITE_DATA_PARITY_ERR] = CNTR_ELEM(\"PioWriteDataParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_data_parity_err_cnt),\n[C_PIO_HOST_ADDR_MEM_COR_ERR] = CNTR_ELEM(\"PioHostAddrMemCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_host_addr_mem_cor_err_cnt),\n[C_PIO_HOST_ADDR_MEM_UNC_ERR] = CNTR_ELEM(\"PioHostAddrMemUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_host_addr_mem_unc_err_cnt),\n[C_PIO_PKT_EVICT_SM_OR_ARM_SM_ERR] = CNTR_ELEM(\"PioPktEvictSmOrArbSmErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pkt_evict_sm_or_arb_sm_err_cnt),\n[C_PIO_INIT_SM_IN_ERR] = CNTR_ELEM(\"PioInitSmInErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_init_sm_in_err_cnt),\n[C_PIO_PPMC_PBL_FIFO_ERR] = CNTR_ELEM(\"PioPpmcPblFifoErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_ppmc_pbl_fifo_err_cnt),\n[C_PIO_CREDIT_RET_FIFO_PARITY_ERR] = CNTR_ELEM(\"PioCreditRetFifoParityErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_pio_credit_ret_fifo_parity_err_cnt),\n[C_PIO_V1_LEN_MEM_BANK1_COR_ERR] = CNTR_ELEM(\"PioVlLenMemBank1CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_v1_len_mem_bank1_cor_err_cnt),\n[C_PIO_V1_LEN_MEM_BANK0_COR_ERR] = CNTR_ELEM(\"PioVlLenMemBank0CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_v1_len_mem_bank0_cor_err_cnt),\n[C_PIO_V1_LEN_MEM_BANK1_UNC_ERR] = CNTR_ELEM(\"PioVlLenMemBank1UncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_v1_len_mem_bank1_unc_err_cnt),\n[C_PIO_V1_LEN_MEM_BANK0_UNC_ERR] = CNTR_ELEM(\"PioVlLenMemBank0UncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_v1_len_mem_bank0_unc_err_cnt),\n[C_PIO_SM_PKT_RESET_PARITY_ERR] = CNTR_ELEM(\"PioSmPktResetParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_sm_pkt_reset_parity_err_cnt),\n[C_PIO_PKT_EVICT_FIFO_PARITY_ERR] = CNTR_ELEM(\"PioPktEvictFifoParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pkt_evict_fifo_parity_err_cnt),\n[C_PIO_SBRDCTRL_CRREL_FIFO_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"PioSbrdctrlCrrelFifoParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_sbrdctrl_crrel_fifo_parity_err_cnt),\n[C_PIO_SBRDCTL_CRREL_PARITY_ERR] = CNTR_ELEM(\"PioSbrdctlCrrelParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_sbrdctl_crrel_parity_err_cnt),\n[C_PIO_PEC_FIFO_PARITY_ERR] = CNTR_ELEM(\"PioPecFifoParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pec_fifo_parity_err_cnt),\n[C_PIO_PCC_FIFO_PARITY_ERR] = CNTR_ELEM(\"PioPccFifoParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_pcc_fifo_parity_err_cnt),\n[C_PIO_SB_MEM_FIFO1_ERR] = CNTR_ELEM(\"PioSbMemFifo1Err\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_sb_mem_fifo1_err_cnt),\n[C_PIO_SB_MEM_FIFO0_ERR] = CNTR_ELEM(\"PioSbMemFifo0Err\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_sb_mem_fifo0_err_cnt),\n[C_PIO_CSR_PARITY_ERR] = CNTR_ELEM(\"PioCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_csr_parity_err_cnt),\n[C_PIO_WRITE_ADDR_PARITY_ERR] = CNTR_ELEM(\"PioWriteAddrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_addr_parity_err_cnt),\n[C_PIO_WRITE_BAD_CTXT_ERR] = CNTR_ELEM(\"PioWriteBadCtxtErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_bad_ctxt_err_cnt),\n \n[C_SDMA_PCIE_REQ_TRACKING_COR_ERR] = CNTR_ELEM(\"SDmaPcieReqTrackingCorErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_sdma_pcie_req_tracking_cor_err_cnt),\n[C_SDMA_PCIE_REQ_TRACKING_UNC_ERR] = CNTR_ELEM(\"SDmaPcieReqTrackingUncErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_sdma_pcie_req_tracking_unc_err_cnt),\n[C_SDMA_CSR_PARITY_ERR] = CNTR_ELEM(\"SDmaCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_csr_parity_err_cnt),\n[C_SDMA_RPY_TAG_ERR] = CNTR_ELEM(\"SDmaRpyTagErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_rpy_tag_err_cnt),\n \n[C_TX_READ_PIO_MEMORY_CSR_UNC_ERR] = CNTR_ELEM(\"TxReadPioMemoryCsrUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_read_pio_memory_csr_unc_err_cnt),\n[C_TX_READ_SDMA_MEMORY_CSR_UNC_ERR] = CNTR_ELEM(\"TxReadSdmaMemoryCsrUncErr\", 0,\n\t\t\t0, CNTR_NORMAL,\n\t\t\taccess_tx_read_sdma_memory_csr_err_cnt),\n[C_TX_EGRESS_FIFO_COR_ERR] = CNTR_ELEM(\"TxEgressFifoCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_egress_fifo_cor_err_cnt),\n[C_TX_READ_PIO_MEMORY_COR_ERR] = CNTR_ELEM(\"TxReadPioMemoryCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_read_pio_memory_cor_err_cnt),\n[C_TX_READ_SDMA_MEMORY_COR_ERR] = CNTR_ELEM(\"TxReadSdmaMemoryCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_read_sdma_memory_cor_err_cnt),\n[C_TX_SB_HDR_COR_ERR] = CNTR_ELEM(\"TxSbHdrCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_sb_hdr_cor_err_cnt),\n[C_TX_CREDIT_OVERRUN_ERR] = CNTR_ELEM(\"TxCreditOverrunErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_credit_overrun_err_cnt),\n[C_TX_LAUNCH_FIFO8_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo8CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo8_cor_err_cnt),\n[C_TX_LAUNCH_FIFO7_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo7CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo7_cor_err_cnt),\n[C_TX_LAUNCH_FIFO6_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo6CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo6_cor_err_cnt),\n[C_TX_LAUNCH_FIFO5_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo5CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo5_cor_err_cnt),\n[C_TX_LAUNCH_FIFO4_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo4CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo4_cor_err_cnt),\n[C_TX_LAUNCH_FIFO3_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo3CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo3_cor_err_cnt),\n[C_TX_LAUNCH_FIFO2_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo2CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo2_cor_err_cnt),\n[C_TX_LAUNCH_FIFO1_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo1CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo1_cor_err_cnt),\n[C_TX_LAUNCH_FIFO0_COR_ERR] = CNTR_ELEM(\"TxLaunchFifo0CorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo0_cor_err_cnt),\n[C_TX_CREDIT_RETURN_VL_ERR] = CNTR_ELEM(\"TxCreditReturnVLErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_credit_return_vl_err_cnt),\n[C_TX_HCRC_INSERTION_ERR] = CNTR_ELEM(\"TxHcrcInsertionErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_hcrc_insertion_err_cnt),\n[C_TX_EGRESS_FIFI_UNC_ERR] = CNTR_ELEM(\"TxEgressFifoUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_egress_fifo_unc_err_cnt),\n[C_TX_READ_PIO_MEMORY_UNC_ERR] = CNTR_ELEM(\"TxReadPioMemoryUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_read_pio_memory_unc_err_cnt),\n[C_TX_READ_SDMA_MEMORY_UNC_ERR] = CNTR_ELEM(\"TxReadSdmaMemoryUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_read_sdma_memory_unc_err_cnt),\n[C_TX_SB_HDR_UNC_ERR] = CNTR_ELEM(\"TxSbHdrUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_sb_hdr_unc_err_cnt),\n[C_TX_CREDIT_RETURN_PARITY_ERR] = CNTR_ELEM(\"TxCreditReturnParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_credit_return_partiy_err_cnt),\n[C_TX_LAUNCH_FIFO8_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo8UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo8_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO7_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo7UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo7_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO6_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo6UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo6_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO5_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo5UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo5_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO4_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo4UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo4_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO3_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo3UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo3_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO2_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo2UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo2_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO1_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo1UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo1_unc_or_parity_err_cnt),\n[C_TX_LAUNCH_FIFO0_UNC_OR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchFifo0UncOrParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_launch_fifo0_unc_or_parity_err_cnt),\n[C_TX_SDMA15_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma15DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma15_disallowed_packet_err_cnt),\n[C_TX_SDMA14_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma14DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma14_disallowed_packet_err_cnt),\n[C_TX_SDMA13_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma13DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma13_disallowed_packet_err_cnt),\n[C_TX_SDMA12_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma12DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma12_disallowed_packet_err_cnt),\n[C_TX_SDMA11_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma11DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma11_disallowed_packet_err_cnt),\n[C_TX_SDMA10_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma10DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma10_disallowed_packet_err_cnt),\n[C_TX_SDMA9_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma9DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma9_disallowed_packet_err_cnt),\n[C_TX_SDMA8_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma8DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma8_disallowed_packet_err_cnt),\n[C_TX_SDMA7_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma7DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma7_disallowed_packet_err_cnt),\n[C_TX_SDMA6_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma6DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma6_disallowed_packet_err_cnt),\n[C_TX_SDMA5_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma5DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma5_disallowed_packet_err_cnt),\n[C_TX_SDMA4_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma4DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma4_disallowed_packet_err_cnt),\n[C_TX_SDMA3_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma3DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma3_disallowed_packet_err_cnt),\n[C_TX_SDMA2_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma2DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma2_disallowed_packet_err_cnt),\n[C_TX_SDMA1_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma1DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma1_disallowed_packet_err_cnt),\n[C_TX_SDMA0_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"TxSdma0DisallowedPacketErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma0_disallowed_packet_err_cnt),\n[C_TX_CONFIG_PARITY_ERR] = CNTR_ELEM(\"TxConfigParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_config_parity_err_cnt),\n[C_TX_SBRD_CTL_CSR_PARITY_ERR] = CNTR_ELEM(\"TxSbrdCtlCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_sbrd_ctl_csr_parity_err_cnt),\n[C_TX_LAUNCH_CSR_PARITY_ERR] = CNTR_ELEM(\"TxLaunchCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_launch_csr_parity_err_cnt),\n[C_TX_ILLEGAL_CL_ERR] = CNTR_ELEM(\"TxIllegalVLErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_illegal_vl_err_cnt),\n[C_TX_SBRD_CTL_STATE_MACHINE_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"TxSbrdCtlStateMachineParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_sbrd_ctl_state_machine_parity_err_cnt),\n[C_TX_RESERVED_10] = CNTR_ELEM(\"Tx Egress Reserved 10\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_egress_reserved_10_err_cnt),\n[C_TX_RESERVED_9] = CNTR_ELEM(\"Tx Egress Reserved 9\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_egress_reserved_9_err_cnt),\n[C_TX_SDMA_LAUNCH_INTF_PARITY_ERR] = CNTR_ELEM(\"TxSdmaLaunchIntfParityErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_tx_sdma_launch_intf_parity_err_cnt),\n[C_TX_PIO_LAUNCH_INTF_PARITY_ERR] = CNTR_ELEM(\"TxPioLaunchIntfParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_pio_launch_intf_parity_err_cnt),\n[C_TX_RESERVED_6] = CNTR_ELEM(\"Tx Egress Reserved 6\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_egress_reserved_6_err_cnt),\n[C_TX_INCORRECT_LINK_STATE_ERR] = CNTR_ELEM(\"TxIncorrectLinkStateErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_incorrect_link_state_err_cnt),\n[C_TX_LINK_DOWN_ERR] = CNTR_ELEM(\"TxLinkdownErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_linkdown_err_cnt),\n[C_TX_EGRESS_FIFO_UNDERRUN_OR_PARITY_ERR] = CNTR_ELEM(\n\t\t\t\"EgressFifoUnderrunOrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_egress_fifi_underrun_or_parity_err_cnt),\n[C_TX_RESERVED_2] = CNTR_ELEM(\"Tx Egress Reserved 2\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_egress_reserved_2_err_cnt),\n[C_TX_PKT_INTEGRITY_MEM_UNC_ERR] = CNTR_ELEM(\"TxPktIntegrityMemUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_pkt_integrity_mem_unc_err_cnt),\n[C_TX_PKT_INTEGRITY_MEM_COR_ERR] = CNTR_ELEM(\"TxPktIntegrityMemCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_tx_pkt_integrity_mem_cor_err_cnt),\n \n[C_SEND_CSR_WRITE_BAD_ADDR_ERR] = CNTR_ELEM(\"SendCsrWriteBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_send_csr_write_bad_addr_err_cnt),\n[C_SEND_CSR_READ_BAD_ADD_ERR] = CNTR_ELEM(\"SendCsrReadBadAddrErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_send_csr_read_bad_addr_err_cnt),\n[C_SEND_CSR_PARITY_ERR] = CNTR_ELEM(\"SendCsrParityErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_send_csr_parity_cnt),\n \n[C_PIO_WRITE_OUT_OF_BOUNDS_ERR] = CNTR_ELEM(\"PioWriteOutOfBoundsErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_out_of_bounds_err_cnt),\n[C_PIO_WRITE_OVERFLOW_ERR] = CNTR_ELEM(\"PioWriteOverflowErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_write_overflow_err_cnt),\n[C_PIO_WRITE_CROSSES_BOUNDARY_ERR] = CNTR_ELEM(\"PioWriteCrossesBoundaryErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_pio_write_crosses_boundary_err_cnt),\n[C_PIO_DISALLOWED_PACKET_ERR] = CNTR_ELEM(\"PioDisallowedPacketErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_disallowed_packet_err_cnt),\n[C_PIO_INCONSISTENT_SOP_ERR] = CNTR_ELEM(\"PioInconsistentSopErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_pio_inconsistent_sop_err_cnt),\n \n[C_SDMA_HEADER_REQUEST_FIFO_COR_ERR] = CNTR_ELEM(\"SDmaHeaderRequestFifoCorErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_sdma_header_request_fifo_cor_err_cnt),\n[C_SDMA_HEADER_STORAGE_COR_ERR] = CNTR_ELEM(\"SDmaHeaderStorageCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_header_storage_cor_err_cnt),\n[C_SDMA_PACKET_TRACKING_COR_ERR] = CNTR_ELEM(\"SDmaPacketTrackingCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_packet_tracking_cor_err_cnt),\n[C_SDMA_ASSEMBLY_COR_ERR] = CNTR_ELEM(\"SDmaAssemblyCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_assembly_cor_err_cnt),\n[C_SDMA_DESC_TABLE_COR_ERR] = CNTR_ELEM(\"SDmaDescTableCorErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_desc_table_cor_err_cnt),\n[C_SDMA_HEADER_REQUEST_FIFO_UNC_ERR] = CNTR_ELEM(\"SDmaHeaderRequestFifoUncErr\",\n\t\t\t0, 0, CNTR_NORMAL,\n\t\t\taccess_sdma_header_request_fifo_unc_err_cnt),\n[C_SDMA_HEADER_STORAGE_UNC_ERR] = CNTR_ELEM(\"SDmaHeaderStorageUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_header_storage_unc_err_cnt),\n[C_SDMA_PACKET_TRACKING_UNC_ERR] = CNTR_ELEM(\"SDmaPacketTrackingUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_packet_tracking_unc_err_cnt),\n[C_SDMA_ASSEMBLY_UNC_ERR] = CNTR_ELEM(\"SDmaAssemblyUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_assembly_unc_err_cnt),\n[C_SDMA_DESC_TABLE_UNC_ERR] = CNTR_ELEM(\"SDmaDescTableUncErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_desc_table_unc_err_cnt),\n[C_SDMA_TIMEOUT_ERR] = CNTR_ELEM(\"SDmaTimeoutErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_timeout_err_cnt),\n[C_SDMA_HEADER_LENGTH_ERR] = CNTR_ELEM(\"SDmaHeaderLengthErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_header_length_err_cnt),\n[C_SDMA_HEADER_ADDRESS_ERR] = CNTR_ELEM(\"SDmaHeaderAddressErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_header_address_err_cnt),\n[C_SDMA_HEADER_SELECT_ERR] = CNTR_ELEM(\"SDmaHeaderSelectErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_header_select_err_cnt),\n[C_SMDA_RESERVED_9] = CNTR_ELEM(\"SDma Reserved 9\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_reserved_9_err_cnt),\n[C_SDMA_PACKET_DESC_OVERFLOW_ERR] = CNTR_ELEM(\"SDmaPacketDescOverflowErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_packet_desc_overflow_err_cnt),\n[C_SDMA_LENGTH_MISMATCH_ERR] = CNTR_ELEM(\"SDmaLengthMismatchErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_length_mismatch_err_cnt),\n[C_SDMA_HALT_ERR] = CNTR_ELEM(\"SDmaHaltErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_halt_err_cnt),\n[C_SDMA_MEM_READ_ERR] = CNTR_ELEM(\"SDmaMemReadErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_mem_read_err_cnt),\n[C_SDMA_FIRST_DESC_ERR] = CNTR_ELEM(\"SDmaFirstDescErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_first_desc_err_cnt),\n[C_SDMA_TAIL_OUT_OF_BOUNDS_ERR] = CNTR_ELEM(\"SDmaTailOutOfBoundsErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_tail_out_of_bounds_err_cnt),\n[C_SDMA_TOO_LONG_ERR] = CNTR_ELEM(\"SDmaTooLongErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_too_long_err_cnt),\n[C_SDMA_GEN_MISMATCH_ERR] = CNTR_ELEM(\"SDmaGenMismatchErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_gen_mismatch_err_cnt),\n[C_SDMA_WRONG_DW_ERR] = CNTR_ELEM(\"SDmaWrongDwErr\", 0, 0,\n\t\t\tCNTR_NORMAL,\n\t\t\taccess_sdma_wrong_dw_err_cnt),\n};\n\nstatic struct cntr_entry port_cntrs[PORT_CNTR_LAST] = {\n[C_TX_UNSUP_VL] = TXE32_PORT_CNTR_ELEM(TxUnVLErr, SEND_UNSUP_VL_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_INVAL_LEN] = TXE32_PORT_CNTR_ELEM(TxInvalLen, SEND_LEN_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_MM_LEN_ERR] = TXE32_PORT_CNTR_ELEM(TxMMLenErr, SEND_MAX_MIN_LEN_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_UNDERRUN] = TXE32_PORT_CNTR_ELEM(TxUnderrun, SEND_UNDERRUN_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_FLOW_STALL] = TXE32_PORT_CNTR_ELEM(TxFlowStall, SEND_FLOW_STALL_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_DROPPED] = TXE32_PORT_CNTR_ELEM(TxDropped, SEND_DROPPED_PKT_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_HDR_ERR] = TXE32_PORT_CNTR_ELEM(TxHdrErr, SEND_HEADERS_ERR_CNT,\n\t\t\tCNTR_NORMAL),\n[C_TX_PKT] = TXE64_PORT_CNTR_ELEM(TxPkt, SEND_DATA_PKT_CNT, CNTR_NORMAL),\n[C_TX_WORDS] = TXE64_PORT_CNTR_ELEM(TxWords, SEND_DWORD_CNT, CNTR_NORMAL),\n[C_TX_WAIT] = TXE64_PORT_CNTR_ELEM(TxWait, SEND_WAIT_CNT, CNTR_SYNTH),\n[C_TX_FLIT_VL] = TXE64_PORT_CNTR_ELEM(TxFlitVL, SEND_DATA_VL0_CNT,\n\t\t\t\t      CNTR_SYNTH | CNTR_VL),\n[C_TX_PKT_VL] = TXE64_PORT_CNTR_ELEM(TxPktVL, SEND_DATA_PKT_VL0_CNT,\n\t\t\t\t     CNTR_SYNTH | CNTR_VL),\n[C_TX_WAIT_VL] = TXE64_PORT_CNTR_ELEM(TxWaitVL, SEND_WAIT_VL0_CNT,\n\t\t\t\t      CNTR_SYNTH | CNTR_VL),\n[C_RX_PKT] = RXE64_PORT_CNTR_ELEM(RxPkt, RCV_DATA_PKT_CNT, CNTR_NORMAL),\n[C_RX_WORDS] = RXE64_PORT_CNTR_ELEM(RxWords, RCV_DWORD_CNT, CNTR_NORMAL),\n[C_SW_LINK_DOWN] = CNTR_ELEM(\"SwLinkDown\", 0, 0, CNTR_SYNTH | CNTR_32BIT,\n\t\t\t     access_sw_link_dn_cnt),\n[C_SW_LINK_UP] = CNTR_ELEM(\"SwLinkUp\", 0, 0, CNTR_SYNTH | CNTR_32BIT,\n\t\t\t   access_sw_link_up_cnt),\n[C_SW_UNKNOWN_FRAME] = CNTR_ELEM(\"UnknownFrame\", 0, 0, CNTR_NORMAL,\n\t\t\t\t access_sw_unknown_frame_cnt),\n[C_SW_XMIT_DSCD] = CNTR_ELEM(\"XmitDscd\", 0, 0, CNTR_SYNTH | CNTR_32BIT,\n\t\t\t     access_sw_xmit_discards),\n[C_SW_XMIT_DSCD_VL] = CNTR_ELEM(\"XmitDscdVl\", 0, 0,\n\t\t\t\tCNTR_SYNTH | CNTR_32BIT | CNTR_VL,\n\t\t\t\taccess_sw_xmit_discards),\n[C_SW_XMIT_CSTR_ERR] = CNTR_ELEM(\"XmitCstrErr\", 0, 0, CNTR_SYNTH,\n\t\t\t\t access_xmit_constraint_errs),\n[C_SW_RCV_CSTR_ERR] = CNTR_ELEM(\"RcvCstrErr\", 0, 0, CNTR_SYNTH,\n\t\t\t\taccess_rcv_constraint_errs),\n[C_SW_IBP_LOOP_PKTS] = SW_IBP_CNTR(LoopPkts, loop_pkts),\n[C_SW_IBP_RC_RESENDS] = SW_IBP_CNTR(RcResend, rc_resends),\n[C_SW_IBP_RNR_NAKS] = SW_IBP_CNTR(RnrNak, rnr_naks),\n[C_SW_IBP_OTHER_NAKS] = SW_IBP_CNTR(OtherNak, other_naks),\n[C_SW_IBP_RC_TIMEOUTS] = SW_IBP_CNTR(RcTimeOut, rc_timeouts),\n[C_SW_IBP_PKT_DROPS] = SW_IBP_CNTR(PktDrop, pkt_drops),\n[C_SW_IBP_DMA_WAIT] = SW_IBP_CNTR(DmaWait, dmawait),\n[C_SW_IBP_RC_SEQNAK] = SW_IBP_CNTR(RcSeqNak, rc_seqnak),\n[C_SW_IBP_RC_DUPREQ] = SW_IBP_CNTR(RcDupRew, rc_dupreq),\n[C_SW_IBP_RDMA_SEQ] = SW_IBP_CNTR(RdmaSeq, rdma_seq),\n[C_SW_IBP_UNALIGNED] = SW_IBP_CNTR(Unaligned, unaligned),\n[C_SW_IBP_SEQ_NAK] = SW_IBP_CNTR(SeqNak, seq_naks),\n[C_SW_IBP_RC_CRWAITS] = SW_IBP_CNTR(RcCrWait, rc_crwaits),\n[C_SW_CPU_RC_ACKS] = CNTR_ELEM(\"RcAcks\", 0, 0, CNTR_NORMAL,\n\t\t\t       access_sw_cpu_rc_acks),\n[C_SW_CPU_RC_QACKS] = CNTR_ELEM(\"RcQacks\", 0, 0, CNTR_NORMAL,\n\t\t\t\taccess_sw_cpu_rc_qacks),\n[C_SW_CPU_RC_DELAYED_COMP] = CNTR_ELEM(\"RcDelayComp\", 0, 0, CNTR_NORMAL,\n\t\t\t\t       access_sw_cpu_rc_delayed_comp),\n[OVR_LBL(0)] = OVR_ELM(0), [OVR_LBL(1)] = OVR_ELM(1),\n[OVR_LBL(2)] = OVR_ELM(2), [OVR_LBL(3)] = OVR_ELM(3),\n[OVR_LBL(4)] = OVR_ELM(4), [OVR_LBL(5)] = OVR_ELM(5),\n[OVR_LBL(6)] = OVR_ELM(6), [OVR_LBL(7)] = OVR_ELM(7),\n[OVR_LBL(8)] = OVR_ELM(8), [OVR_LBL(9)] = OVR_ELM(9),\n[OVR_LBL(10)] = OVR_ELM(10), [OVR_LBL(11)] = OVR_ELM(11),\n[OVR_LBL(12)] = OVR_ELM(12), [OVR_LBL(13)] = OVR_ELM(13),\n[OVR_LBL(14)] = OVR_ELM(14), [OVR_LBL(15)] = OVR_ELM(15),\n[OVR_LBL(16)] = OVR_ELM(16), [OVR_LBL(17)] = OVR_ELM(17),\n[OVR_LBL(18)] = OVR_ELM(18), [OVR_LBL(19)] = OVR_ELM(19),\n[OVR_LBL(20)] = OVR_ELM(20), [OVR_LBL(21)] = OVR_ELM(21),\n[OVR_LBL(22)] = OVR_ELM(22), [OVR_LBL(23)] = OVR_ELM(23),\n[OVR_LBL(24)] = OVR_ELM(24), [OVR_LBL(25)] = OVR_ELM(25),\n[OVR_LBL(26)] = OVR_ELM(26), [OVR_LBL(27)] = OVR_ELM(27),\n[OVR_LBL(28)] = OVR_ELM(28), [OVR_LBL(29)] = OVR_ELM(29),\n[OVR_LBL(30)] = OVR_ELM(30), [OVR_LBL(31)] = OVR_ELM(31),\n[OVR_LBL(32)] = OVR_ELM(32), [OVR_LBL(33)] = OVR_ELM(33),\n[OVR_LBL(34)] = OVR_ELM(34), [OVR_LBL(35)] = OVR_ELM(35),\n[OVR_LBL(36)] = OVR_ELM(36), [OVR_LBL(37)] = OVR_ELM(37),\n[OVR_LBL(38)] = OVR_ELM(38), [OVR_LBL(39)] = OVR_ELM(39),\n[OVR_LBL(40)] = OVR_ELM(40), [OVR_LBL(41)] = OVR_ELM(41),\n[OVR_LBL(42)] = OVR_ELM(42), [OVR_LBL(43)] = OVR_ELM(43),\n[OVR_LBL(44)] = OVR_ELM(44), [OVR_LBL(45)] = OVR_ELM(45),\n[OVR_LBL(46)] = OVR_ELM(46), [OVR_LBL(47)] = OVR_ELM(47),\n[OVR_LBL(48)] = OVR_ELM(48), [OVR_LBL(49)] = OVR_ELM(49),\n[OVR_LBL(50)] = OVR_ELM(50), [OVR_LBL(51)] = OVR_ELM(51),\n[OVR_LBL(52)] = OVR_ELM(52), [OVR_LBL(53)] = OVR_ELM(53),\n[OVR_LBL(54)] = OVR_ELM(54), [OVR_LBL(55)] = OVR_ELM(55),\n[OVR_LBL(56)] = OVR_ELM(56), [OVR_LBL(57)] = OVR_ELM(57),\n[OVR_LBL(58)] = OVR_ELM(58), [OVR_LBL(59)] = OVR_ELM(59),\n[OVR_LBL(60)] = OVR_ELM(60), [OVR_LBL(61)] = OVR_ELM(61),\n[OVR_LBL(62)] = OVR_ELM(62), [OVR_LBL(63)] = OVR_ELM(63),\n[OVR_LBL(64)] = OVR_ELM(64), [OVR_LBL(65)] = OVR_ELM(65),\n[OVR_LBL(66)] = OVR_ELM(66), [OVR_LBL(67)] = OVR_ELM(67),\n[OVR_LBL(68)] = OVR_ELM(68), [OVR_LBL(69)] = OVR_ELM(69),\n[OVR_LBL(70)] = OVR_ELM(70), [OVR_LBL(71)] = OVR_ELM(71),\n[OVR_LBL(72)] = OVR_ELM(72), [OVR_LBL(73)] = OVR_ELM(73),\n[OVR_LBL(74)] = OVR_ELM(74), [OVR_LBL(75)] = OVR_ELM(75),\n[OVR_LBL(76)] = OVR_ELM(76), [OVR_LBL(77)] = OVR_ELM(77),\n[OVR_LBL(78)] = OVR_ELM(78), [OVR_LBL(79)] = OVR_ELM(79),\n[OVR_LBL(80)] = OVR_ELM(80), [OVR_LBL(81)] = OVR_ELM(81),\n[OVR_LBL(82)] = OVR_ELM(82), [OVR_LBL(83)] = OVR_ELM(83),\n[OVR_LBL(84)] = OVR_ELM(84), [OVR_LBL(85)] = OVR_ELM(85),\n[OVR_LBL(86)] = OVR_ELM(86), [OVR_LBL(87)] = OVR_ELM(87),\n[OVR_LBL(88)] = OVR_ELM(88), [OVR_LBL(89)] = OVR_ELM(89),\n[OVR_LBL(90)] = OVR_ELM(90), [OVR_LBL(91)] = OVR_ELM(91),\n[OVR_LBL(92)] = OVR_ELM(92), [OVR_LBL(93)] = OVR_ELM(93),\n[OVR_LBL(94)] = OVR_ELM(94), [OVR_LBL(95)] = OVR_ELM(95),\n[OVR_LBL(96)] = OVR_ELM(96), [OVR_LBL(97)] = OVR_ELM(97),\n[OVR_LBL(98)] = OVR_ELM(98), [OVR_LBL(99)] = OVR_ELM(99),\n[OVR_LBL(100)] = OVR_ELM(100), [OVR_LBL(101)] = OVR_ELM(101),\n[OVR_LBL(102)] = OVR_ELM(102), [OVR_LBL(103)] = OVR_ELM(103),\n[OVR_LBL(104)] = OVR_ELM(104), [OVR_LBL(105)] = OVR_ELM(105),\n[OVR_LBL(106)] = OVR_ELM(106), [OVR_LBL(107)] = OVR_ELM(107),\n[OVR_LBL(108)] = OVR_ELM(108), [OVR_LBL(109)] = OVR_ELM(109),\n[OVR_LBL(110)] = OVR_ELM(110), [OVR_LBL(111)] = OVR_ELM(111),\n[OVR_LBL(112)] = OVR_ELM(112), [OVR_LBL(113)] = OVR_ELM(113),\n[OVR_LBL(114)] = OVR_ELM(114), [OVR_LBL(115)] = OVR_ELM(115),\n[OVR_LBL(116)] = OVR_ELM(116), [OVR_LBL(117)] = OVR_ELM(117),\n[OVR_LBL(118)] = OVR_ELM(118), [OVR_LBL(119)] = OVR_ELM(119),\n[OVR_LBL(120)] = OVR_ELM(120), [OVR_LBL(121)] = OVR_ELM(121),\n[OVR_LBL(122)] = OVR_ELM(122), [OVR_LBL(123)] = OVR_ELM(123),\n[OVR_LBL(124)] = OVR_ELM(124), [OVR_LBL(125)] = OVR_ELM(125),\n[OVR_LBL(126)] = OVR_ELM(126), [OVR_LBL(127)] = OVR_ELM(127),\n[OVR_LBL(128)] = OVR_ELM(128), [OVR_LBL(129)] = OVR_ELM(129),\n[OVR_LBL(130)] = OVR_ELM(130), [OVR_LBL(131)] = OVR_ELM(131),\n[OVR_LBL(132)] = OVR_ELM(132), [OVR_LBL(133)] = OVR_ELM(133),\n[OVR_LBL(134)] = OVR_ELM(134), [OVR_LBL(135)] = OVR_ELM(135),\n[OVR_LBL(136)] = OVR_ELM(136), [OVR_LBL(137)] = OVR_ELM(137),\n[OVR_LBL(138)] = OVR_ELM(138), [OVR_LBL(139)] = OVR_ELM(139),\n[OVR_LBL(140)] = OVR_ELM(140), [OVR_LBL(141)] = OVR_ELM(141),\n[OVR_LBL(142)] = OVR_ELM(142), [OVR_LBL(143)] = OVR_ELM(143),\n[OVR_LBL(144)] = OVR_ELM(144), [OVR_LBL(145)] = OVR_ELM(145),\n[OVR_LBL(146)] = OVR_ELM(146), [OVR_LBL(147)] = OVR_ELM(147),\n[OVR_LBL(148)] = OVR_ELM(148), [OVR_LBL(149)] = OVR_ELM(149),\n[OVR_LBL(150)] = OVR_ELM(150), [OVR_LBL(151)] = OVR_ELM(151),\n[OVR_LBL(152)] = OVR_ELM(152), [OVR_LBL(153)] = OVR_ELM(153),\n[OVR_LBL(154)] = OVR_ELM(154), [OVR_LBL(155)] = OVR_ELM(155),\n[OVR_LBL(156)] = OVR_ELM(156), [OVR_LBL(157)] = OVR_ELM(157),\n[OVR_LBL(158)] = OVR_ELM(158), [OVR_LBL(159)] = OVR_ELM(159),\n};\n\n \n\n \nint is_ax(struct hfi1_devdata *dd)\n{\n\tu8 chip_rev_minor =\n\t\tdd->revision >> CCE_REVISION_CHIP_REV_MINOR_SHIFT\n\t\t\t& CCE_REVISION_CHIP_REV_MINOR_MASK;\n\treturn (chip_rev_minor & 0xf0) == 0;\n}\n\n \nint is_bx(struct hfi1_devdata *dd)\n{\n\tu8 chip_rev_minor =\n\t\tdd->revision >> CCE_REVISION_CHIP_REV_MINOR_SHIFT\n\t\t\t& CCE_REVISION_CHIP_REV_MINOR_MASK;\n\treturn (chip_rev_minor & 0xF0) == 0x10;\n}\n\n \nbool is_urg_masked(struct hfi1_ctxtdata *rcd)\n{\n\tu64 mask;\n\tu32 is = IS_RCVURGENT_START + rcd->ctxt;\n\tu8 bit = is % 64;\n\n\tmask = read_csr(rcd->dd, CCE_INT_MASK + (8 * (is / 64)));\n\treturn !(mask & BIT_ULL(bit));\n}\n\n \nstatic int append_str(char *buf, char **curp, int *lenp, const char *s)\n{\n\tchar *p = *curp;\n\tint len = *lenp;\n\tint result = 0;  \n\tchar c;\n\n\t \n\tif (p != buf) {\n\t\tif (len == 0) {\n\t\t\tresult = 1;  \n\t\t\tgoto done;\n\t\t}\n\t\t*p++ = ',';\n\t\tlen--;\n\t}\n\n\t \n\twhile ((c = *s++) != 0) {\n\t\tif (len == 0) {\n\t\t\tresult = 1;  \n\t\t\tgoto done;\n\t\t}\n\t\t*p++ = c;\n\t\tlen--;\n\t}\n\ndone:\n\t \n\t*curp = p;\n\t*lenp = len;\n\n\treturn result;\n}\n\n \nstatic char *flag_string(char *buf, int buf_len, u64 flags,\n\t\t\t struct flag_table *table, int table_size)\n{\n\tchar extra[32];\n\tchar *p = buf;\n\tint len = buf_len;\n\tint no_room = 0;\n\tint i;\n\n\t \n\tif (len < 2)\n\t\treturn \"\";\n\n\tlen--;\t \n\tfor (i = 0; i < table_size; i++) {\n\t\tif (flags & table[i].flag) {\n\t\t\tno_room = append_str(buf, &p, &len, table[i].str);\n\t\t\tif (no_room)\n\t\t\t\tbreak;\n\t\t\tflags &= ~table[i].flag;\n\t\t}\n\t}\n\n\t \n\tif (!no_room && flags) {\n\t\tsnprintf(extra, sizeof(extra), \"bits 0x%llx\", flags);\n\t\tno_room = append_str(buf, &p, &len, extra);\n\t}\n\n\t \n\tif (no_room) {\n\t\t \n\t\tif (len == 0)\n\t\t\t--p;\n\t\t*p++ = '*';\n\t}\n\n\t \n\t*p = 0;\n\treturn buf;\n}\n\n \nstatic const char * const cce_misc_names[] = {\n\t\"CceErrInt\",\t\t \n\t\"RxeErrInt\",\t\t \n\t\"MiscErrInt\",\t\t \n\t\"Reserved3\",\t\t \n\t\"PioErrInt\",\t\t \n\t\"SDmaErrInt\",\t\t \n\t\"EgressErrInt\",\t\t \n\t\"TxeErrInt\"\t\t \n};\n\n \nstatic char *is_misc_err_name(char *buf, size_t bsize, unsigned int source)\n{\n\tif (source < ARRAY_SIZE(cce_misc_names))\n\t\tstrncpy(buf, cce_misc_names[source], bsize);\n\telse\n\t\tsnprintf(buf, bsize, \"Reserved%u\",\n\t\t\t source + IS_GENERAL_ERR_START);\n\n\treturn buf;\n}\n\n \nstatic char *is_sdma_eng_err_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"SDmaEngErrInt%u\", source);\n\treturn buf;\n}\n\n \nstatic char *is_sendctxt_err_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"SendCtxtErrInt%u\", source);\n\treturn buf;\n}\n\nstatic const char * const various_names[] = {\n\t\"PbcInt\",\n\t\"GpioAssertInt\",\n\t\"Qsfp1Int\",\n\t\"Qsfp2Int\",\n\t\"TCritInt\"\n};\n\n \nstatic char *is_various_name(char *buf, size_t bsize, unsigned int source)\n{\n\tif (source < ARRAY_SIZE(various_names))\n\t\tstrncpy(buf, various_names[source], bsize);\n\telse\n\t\tsnprintf(buf, bsize, \"Reserved%u\", source + IS_VARIOUS_START);\n\treturn buf;\n}\n\n \nstatic char *is_dc_name(char *buf, size_t bsize, unsigned int source)\n{\n\tstatic const char * const dc_int_names[] = {\n\t\t\"common\",\n\t\t\"lcb\",\n\t\t\"8051\",\n\t\t\"lbm\"\t \n\t};\n\n\tif (source < ARRAY_SIZE(dc_int_names))\n\t\tsnprintf(buf, bsize, \"dc_%s_int\", dc_int_names[source]);\n\telse\n\t\tsnprintf(buf, bsize, \"DCInt%u\", source);\n\treturn buf;\n}\n\nstatic const char * const sdma_int_names[] = {\n\t\"SDmaInt\",\n\t\"SdmaIdleInt\",\n\t\"SdmaProgressInt\",\n};\n\n \nstatic char *is_sdma_eng_name(char *buf, size_t bsize, unsigned int source)\n{\n\t \n\tunsigned int what  = source / TXE_NUM_SDMA_ENGINES;\n\t \n\tunsigned int which = source % TXE_NUM_SDMA_ENGINES;\n\n\tif (likely(what < 3))\n\t\tsnprintf(buf, bsize, \"%s%u\", sdma_int_names[what], which);\n\telse\n\t\tsnprintf(buf, bsize, \"Invalid SDMA interrupt %u\", source);\n\treturn buf;\n}\n\n \nstatic char *is_rcv_avail_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"RcvAvailInt%u\", source);\n\treturn buf;\n}\n\n \nstatic char *is_rcv_urgent_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"RcvUrgentInt%u\", source);\n\treturn buf;\n}\n\n \nstatic char *is_send_credit_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"SendCreditInt%u\", source);\n\treturn buf;\n}\n\n \nstatic char *is_reserved_name(char *buf, size_t bsize, unsigned int source)\n{\n\tsnprintf(buf, bsize, \"Reserved%u\", source + IS_RESERVED_START);\n\treturn buf;\n}\n\nstatic char *cce_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   cce_err_status_flags,\n\t\t\t   ARRAY_SIZE(cce_err_status_flags));\n}\n\nstatic char *rxe_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   rxe_err_status_flags,\n\t\t\t   ARRAY_SIZE(rxe_err_status_flags));\n}\n\nstatic char *misc_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, misc_err_status_flags,\n\t\t\t   ARRAY_SIZE(misc_err_status_flags));\n}\n\nstatic char *pio_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   pio_err_status_flags,\n\t\t\t   ARRAY_SIZE(pio_err_status_flags));\n}\n\nstatic char *sdma_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   sdma_err_status_flags,\n\t\t\t   ARRAY_SIZE(sdma_err_status_flags));\n}\n\nstatic char *egress_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   egress_err_status_flags,\n\t\t\t   ARRAY_SIZE(egress_err_status_flags));\n}\n\nstatic char *egress_err_info_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   egress_err_info_flags,\n\t\t\t   ARRAY_SIZE(egress_err_info_flags));\n}\n\nstatic char *send_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   send_err_status_flags,\n\t\t\t   ARRAY_SIZE(send_err_status_flags));\n}\n\nstatic void handle_cce_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\t \n\tdd_dev_info(dd, \"CCE Error: %s\\n\",\n\t\t    cce_err_status_string(buf, sizeof(buf), reg));\n\n\tif ((reg & CCE_ERR_STATUS_CCE_CLI2_ASYNC_FIFO_PARITY_ERR_SMASK) &&\n\t    is_ax(dd) && (dd->icode != ICODE_FUNCTIONAL_SIMULATOR)) {\n\t\t \n\t\t \n\t\tstart_freeze_handling(dd->pport, FREEZE_SELF);\n\t}\n\n\tfor (i = 0; i < NUM_CCE_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i)) {\n\t\t\tincr_cntr64(&dd->cce_err_status_cnt[i]);\n\t\t\t \n\t\t\tincr_cntr64(&dd->sw_cce_err_status_aggregate);\n\t\t}\n\t}\n}\n\n \n#define RCVERR_CHECK_TIME 10\nstatic void update_rcverr_timer(struct timer_list *t)\n{\n\tstruct hfi1_devdata *dd = from_timer(dd, t, rcverr_timer);\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tu32 cur_ovfl_cnt = read_dev_cntr(dd, C_RCV_OVF, CNTR_INVALID_VL);\n\n\tif (dd->rcv_ovfl_cnt < cur_ovfl_cnt &&\n\t    ppd->port_error_action & OPA_PI_MASK_EX_BUFFER_OVERRUN) {\n\t\tdd_dev_info(dd, \"%s: PortErrorAction bounce\\n\", __func__);\n\t\tset_link_down_reason(\n\t\tppd, OPA_LINKDOWN_REASON_EXCESSIVE_BUFFER_OVERRUN, 0,\n\t\tOPA_LINKDOWN_REASON_EXCESSIVE_BUFFER_OVERRUN);\n\t\tqueue_work(ppd->link_wq, &ppd->link_bounce_work);\n\t}\n\tdd->rcv_ovfl_cnt = (u32)cur_ovfl_cnt;\n\n\tmod_timer(&dd->rcverr_timer, jiffies + HZ * RCVERR_CHECK_TIME);\n}\n\nstatic int init_rcverr(struct hfi1_devdata *dd)\n{\n\ttimer_setup(&dd->rcverr_timer, update_rcverr_timer, 0);\n\t \n\tdd->rcv_ovfl_cnt = 0;\n\treturn mod_timer(&dd->rcverr_timer, jiffies + HZ * RCVERR_CHECK_TIME);\n}\n\nstatic void free_rcverr(struct hfi1_devdata *dd)\n{\n\tif (dd->rcverr_timer.function)\n\t\tdel_timer_sync(&dd->rcverr_timer);\n}\n\nstatic void handle_rxe_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\tdd_dev_info(dd, \"Receive Error: %s\\n\",\n\t\t    rxe_err_status_string(buf, sizeof(buf), reg));\n\n\tif (reg & ALL_RXE_FREEZE_ERR) {\n\t\tint flags = 0;\n\n\t\t \n\t\tif (is_ax(dd) && (reg & RXE_FREEZE_ABORT_MASK))\n\t\t\tflags = FREEZE_ABORT;\n\n\t\tstart_freeze_handling(dd->pport, flags);\n\t}\n\n\tfor (i = 0; i < NUM_RCV_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->rcv_err_status_cnt[i]);\n\t}\n}\n\nstatic void handle_misc_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\tdd_dev_info(dd, \"Misc Error: %s\",\n\t\t    misc_err_status_string(buf, sizeof(buf), reg));\n\tfor (i = 0; i < NUM_MISC_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->misc_err_status_cnt[i]);\n\t}\n}\n\nstatic void handle_pio_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\tdd_dev_info(dd, \"PIO Error: %s\\n\",\n\t\t    pio_err_status_string(buf, sizeof(buf), reg));\n\n\tif (reg & ALL_PIO_FREEZE_ERR)\n\t\tstart_freeze_handling(dd->pport, 0);\n\n\tfor (i = 0; i < NUM_SEND_PIO_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->send_pio_err_status_cnt[i]);\n\t}\n}\n\nstatic void handle_sdma_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\tdd_dev_info(dd, \"SDMA Error: %s\\n\",\n\t\t    sdma_err_status_string(buf, sizeof(buf), reg));\n\n\tif (reg & ALL_SDMA_FREEZE_ERR)\n\t\tstart_freeze_handling(dd->pport, 0);\n\n\tfor (i = 0; i < NUM_SEND_DMA_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->send_dma_err_status_cnt[i]);\n\t}\n}\n\nstatic inline void __count_port_discards(struct hfi1_pportdata *ppd)\n{\n\tincr_cntr64(&ppd->port_xmit_discards);\n}\n\nstatic void count_port_inactive(struct hfi1_devdata *dd)\n{\n\t__count_port_discards(dd->pport);\n}\n\n \nstatic void handle_send_egress_err_info(struct hfi1_devdata *dd,\n\t\t\t\t\tint vl)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tu64 src = read_csr(dd, SEND_EGRESS_ERR_SOURCE);  \n\tu64 info = read_csr(dd, SEND_EGRESS_ERR_INFO);\n\tchar buf[96];\n\n\t \n\twrite_csr(dd, SEND_EGRESS_ERR_INFO, info);\n\n\tdd_dev_info(dd,\n\t\t    \"Egress Error Info: 0x%llx, %s Egress Error Src 0x%llx\\n\",\n\t\t    info, egress_err_info_string(buf, sizeof(buf), info), src);\n\n\t \n\tif (info & PORT_DISCARD_EGRESS_ERRS) {\n\t\tint weight, i;\n\n\t\t \n\t\tweight = hweight64(info & PORT_DISCARD_EGRESS_ERRS);\n\t\tfor (i = 0; i < weight; i++) {\n\t\t\t__count_port_discards(ppd);\n\t\t\tif (vl >= 0 && vl < TXE_NUM_DATA_VL)\n\t\t\t\tincr_cntr64(&ppd->port_xmit_discards_vl[vl]);\n\t\t\telse if (vl == 15)\n\t\t\t\tincr_cntr64(&ppd->port_xmit_discards_vl\n\t\t\t\t\t    [C_VL_15]);\n\t\t}\n\t}\n}\n\n \nstatic inline int port_inactive_err(u64 posn)\n{\n\treturn (posn >= SEES(TX_LINKDOWN) &&\n\t\tposn <= SEES(TX_INCORRECT_LINK_STATE));\n}\n\n \nstatic inline int disallowed_pkt_err(int posn)\n{\n\treturn (posn >= SEES(TX_SDMA0_DISALLOWED_PACKET) &&\n\t\tposn <= SEES(TX_SDMA15_DISALLOWED_PACKET));\n}\n\n \nstatic inline int disallowed_pkt_engine(int posn)\n{\n\treturn posn - SEES(TX_SDMA0_DISALLOWED_PACKET);\n}\n\n \nstatic int engine_to_vl(struct hfi1_devdata *dd, int engine)\n{\n\tstruct sdma_vl_map *m;\n\tint vl;\n\n\t \n\tif (engine < 0 || engine >= TXE_NUM_SDMA_ENGINES)\n\t\treturn -1;\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tvl = m->engine_to_vl[engine];\n\trcu_read_unlock();\n\n\treturn vl;\n}\n\n \nstatic int sc_to_vl(struct hfi1_devdata *dd, int sw_index)\n{\n\tstruct send_context_info *sci;\n\tstruct send_context *sc;\n\tint i;\n\n\tsci = &dd->send_contexts[sw_index];\n\n\t \n\tif ((sci->type != SC_KERNEL) && (sci->type != SC_VL15))\n\t\treturn -1;\n\n\tsc = sci->sc;\n\tif (!sc)\n\t\treturn -1;\n\tif (dd->vld[15].sc == sc)\n\t\treturn 15;\n\tfor (i = 0; i < num_vls; i++)\n\t\tif (dd->vld[i].sc == sc)\n\t\t\treturn i;\n\n\treturn -1;\n}\n\nstatic void handle_egress_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tu64 reg_copy = reg, handled = 0;\n\tchar buf[96];\n\tint i = 0;\n\n\tif (reg & ALL_TXE_EGRESS_FREEZE_ERR)\n\t\tstart_freeze_handling(dd->pport, 0);\n\telse if (is_ax(dd) &&\n\t\t (reg & SEND_EGRESS_ERR_STATUS_TX_CREDIT_RETURN_VL_ERR_SMASK) &&\n\t\t (dd->icode != ICODE_FUNCTIONAL_SIMULATOR))\n\t\tstart_freeze_handling(dd->pport, 0);\n\n\twhile (reg_copy) {\n\t\tint posn = fls64(reg_copy);\n\t\t \n\t\tint shift = posn - 1;\n\t\tu64 mask = 1ULL << shift;\n\n\t\tif (port_inactive_err(shift)) {\n\t\t\tcount_port_inactive(dd);\n\t\t\thandled |= mask;\n\t\t} else if (disallowed_pkt_err(shift)) {\n\t\t\tint vl = engine_to_vl(dd, disallowed_pkt_engine(shift));\n\n\t\t\thandle_send_egress_err_info(dd, vl);\n\t\t\thandled |= mask;\n\t\t}\n\t\treg_copy &= ~mask;\n\t}\n\n\treg &= ~handled;\n\n\tif (reg)\n\t\tdd_dev_info(dd, \"Egress Error: %s\\n\",\n\t\t\t    egress_err_status_string(buf, sizeof(buf), reg));\n\n\tfor (i = 0; i < NUM_SEND_EGRESS_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->send_egress_err_status_cnt[i]);\n\t}\n}\n\nstatic void handle_txe_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\tint i = 0;\n\n\tdd_dev_info(dd, \"Send Error: %s\\n\",\n\t\t    send_err_status_string(buf, sizeof(buf), reg));\n\n\tfor (i = 0; i < NUM_SEND_ERR_STATUS_COUNTERS; i++) {\n\t\tif (reg & (1ull << i))\n\t\t\tincr_cntr64(&dd->send_err_status_cnt[i]);\n\t}\n}\n\n \n#define MAX_CLEAR_COUNT 20\n\n \nstatic void interrupt_clear_down(struct hfi1_devdata *dd,\n\t\t\t\t u32 context,\n\t\t\t\t const struct err_reg_info *eri)\n{\n\tu64 reg;\n\tu32 count;\n\n\t \n\tcount = 0;\n\twhile (1) {\n\t\treg = read_kctxt_csr(dd, context, eri->status);\n\t\tif (reg == 0)\n\t\t\tbreak;\n\t\twrite_kctxt_csr(dd, context, eri->clear, reg);\n\t\tif (likely(eri->handler))\n\t\t\teri->handler(dd, context, reg);\n\t\tcount++;\n\t\tif (count > MAX_CLEAR_COUNT) {\n\t\t\tu64 mask;\n\n\t\t\tdd_dev_err(dd, \"Repeating %s bits 0x%llx - masking\\n\",\n\t\t\t\t   eri->desc, reg);\n\t\t\t \n\t\t\tmask = read_kctxt_csr(dd, context, eri->mask);\n\t\t\tmask &= ~reg;\n\t\t\twrite_kctxt_csr(dd, context, eri->mask, mask);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void is_misc_err_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tconst struct err_reg_info *eri = &misc_errs[source];\n\n\tif (eri->handler) {\n\t\tinterrupt_clear_down(dd, 0, eri);\n\t} else {\n\t\tdd_dev_err(dd, \"Unexpected misc interrupt (%u) - reserved\\n\",\n\t\t\t   source);\n\t}\n}\n\nstatic char *send_context_err_status_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags,\n\t\t\t   sc_err_status_flags,\n\t\t\t   ARRAY_SIZE(sc_err_status_flags));\n}\n\n \nstatic void is_sendctxt_err_int(struct hfi1_devdata *dd,\n\t\t\t\tunsigned int hw_context)\n{\n\tstruct send_context_info *sci;\n\tstruct send_context *sc;\n\tchar flags[96];\n\tu64 status;\n\tu32 sw_index;\n\tint i = 0;\n\tunsigned long irq_flags;\n\n\tsw_index = dd->hw_to_sw[hw_context];\n\tif (sw_index >= dd->num_send_contexts) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"out of range sw index %u for send context %u\\n\",\n\t\t\t   sw_index, hw_context);\n\t\treturn;\n\t}\n\tsci = &dd->send_contexts[sw_index];\n\tspin_lock_irqsave(&dd->sc_lock, irq_flags);\n\tsc = sci->sc;\n\tif (!sc) {\n\t\tdd_dev_err(dd, \"%s: context %u(%u): no sc?\\n\", __func__,\n\t\t\t   sw_index, hw_context);\n\t\tspin_unlock_irqrestore(&dd->sc_lock, irq_flags);\n\t\treturn;\n\t}\n\n\t \n\tsc_stop(sc, SCF_HALTED);\n\n\tstatus = read_kctxt_csr(dd, hw_context, SEND_CTXT_ERR_STATUS);\n\n\tdd_dev_info(dd, \"Send Context %u(%u) Error: %s\\n\", sw_index, hw_context,\n\t\t    send_context_err_status_string(flags, sizeof(flags),\n\t\t\t\t\t\t   status));\n\n\tif (status & SEND_CTXT_ERR_STATUS_PIO_DISALLOWED_PACKET_ERR_SMASK)\n\t\thandle_send_egress_err_info(dd, sc_to_vl(dd, sw_index));\n\n\t \n\tif (sc->type != SC_USER)\n\t\tqueue_work(dd->pport->hfi1_wq, &sc->halt_work);\n\tspin_unlock_irqrestore(&dd->sc_lock, irq_flags);\n\n\t \n\tfor (i = 0; i < NUM_SEND_CTXT_ERR_STATUS_COUNTERS; i++) {\n\t\tif (status & (1ull << i))\n\t\t\tincr_cntr64(&dd->sw_ctxt_err_status_cnt[i]);\n\t}\n}\n\nstatic void handle_sdma_eng_err(struct hfi1_devdata *dd,\n\t\t\t\tunsigned int source, u64 status)\n{\n\tstruct sdma_engine *sde;\n\tint i = 0;\n\n\tsde = &dd->per_sdma[source];\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) source: %u status 0x%llx\\n\",\n\t\t   sde->this_idx, source, (unsigned long long)status);\n#endif\n\tsde->err_cnt++;\n\tsdma_engine_error(sde, status);\n\n\t \n\tfor (i = 0; i < NUM_SEND_DMA_ENG_ERR_STATUS_COUNTERS; i++) {\n\t\tif (status & (1ull << i))\n\t\t\tincr_cntr64(&dd->sw_send_dma_eng_err_status_cnt[i]);\n\t}\n}\n\n \nstatic void is_sdma_eng_err_int(struct hfi1_devdata *dd, unsigned int source)\n{\n#ifdef CONFIG_SDMA_VERBOSITY\n\tstruct sdma_engine *sde = &dd->per_sdma[source];\n\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) source: %u\\n\", sde->this_idx,\n\t\t   source);\n\tsdma_dumpstate(sde);\n#endif\n\tinterrupt_clear_down(dd, source, &sdma_eng_err);\n}\n\n \nstatic void is_various_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tconst struct err_reg_info *eri = &various_err[source];\n\n\t \n\tif (source == TCRIT_INT_SOURCE)\n\t\thandle_temp_err(dd);\n\telse if (eri->handler)\n\t\tinterrupt_clear_down(dd, 0, eri);\n\telse\n\t\tdd_dev_info(dd,\n\t\t\t    \"%s: Unimplemented/reserved interrupt %d\\n\",\n\t\t\t    __func__, source);\n}\n\nstatic void handle_qsfp_int(struct hfi1_devdata *dd, u32 src_ctx, u64 reg)\n{\n\t \n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tunsigned long flags;\n\tu64 qsfp_int_mgmt = (u64)(QSFP_HFI0_INT_N | QSFP_HFI0_MODPRST_N);\n\n\tif (reg & QSFP_HFI0_MODPRST_N) {\n\t\tif (!qsfp_mod_present(ppd)) {\n\t\t\tdd_dev_info(dd, \"%s: QSFP module removed\\n\",\n\t\t\t\t    __func__);\n\n\t\t\tppd->driver_link_ready = 0;\n\t\t\t \n\n\t\t\tspin_lock_irqsave(&ppd->qsfp_info.qsfp_lock, flags);\n\t\t\t \n\t\t\tppd->qsfp_info.cache_valid = 0;\n\t\t\tppd->qsfp_info.reset_needed = 0;\n\t\t\tppd->qsfp_info.limiting_active = 0;\n\t\t\tspin_unlock_irqrestore(&ppd->qsfp_info.qsfp_lock,\n\t\t\t\t\t       flags);\n\t\t\t \n\t\t\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_INVERT :\n\t\t\t\t  ASIC_QSFP1_INVERT, qsfp_int_mgmt);\n\n\t\t\tif ((ppd->offline_disabled_reason >\n\t\t\t  HFI1_ODR_MASK(\n\t\t\t  OPA_LINKDOWN_REASON_LOCAL_MEDIA_NOT_INSTALLED)) ||\n\t\t\t  (ppd->offline_disabled_reason ==\n\t\t\t  HFI1_ODR_MASK(OPA_LINKDOWN_REASON_NONE)))\n\t\t\t\tppd->offline_disabled_reason =\n\t\t\t\tHFI1_ODR_MASK(\n\t\t\t\tOPA_LINKDOWN_REASON_LOCAL_MEDIA_NOT_INSTALLED);\n\n\t\t\tif (ppd->host_link_state == HLS_DN_POLL) {\n\t\t\t\t \n\t\t\t\tqueue_work(ppd->link_wq, &ppd->link_down_work);\n\t\t\t}\n\t\t} else {\n\t\t\tdd_dev_info(dd, \"%s: QSFP module inserted\\n\",\n\t\t\t\t    __func__);\n\n\t\t\tspin_lock_irqsave(&ppd->qsfp_info.qsfp_lock, flags);\n\t\t\tppd->qsfp_info.cache_valid = 0;\n\t\t\tppd->qsfp_info.cache_refresh_required = 1;\n\t\t\tspin_unlock_irqrestore(&ppd->qsfp_info.qsfp_lock,\n\t\t\t\t\t       flags);\n\n\t\t\t \n\t\t\tqsfp_int_mgmt &= ~(u64)QSFP_HFI0_MODPRST_N;\n\t\t\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_INVERT :\n\t\t\t\t  ASIC_QSFP1_INVERT, qsfp_int_mgmt);\n\n\t\t\tppd->offline_disabled_reason =\n\t\t\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_TRANSIENT);\n\t\t}\n\t}\n\n\tif (reg & QSFP_HFI0_INT_N) {\n\t\tdd_dev_info(dd, \"%s: Interrupt received from QSFP module\\n\",\n\t\t\t    __func__);\n\t\tspin_lock_irqsave(&ppd->qsfp_info.qsfp_lock, flags);\n\t\tppd->qsfp_info.check_interrupt_flags = 1;\n\t\tspin_unlock_irqrestore(&ppd->qsfp_info.qsfp_lock, flags);\n\t}\n\n\t \n\tif (qsfp_mod_present(ppd))\n\t\tqueue_work(ppd->link_wq, &ppd->qsfp_info.qsfp_work);\n}\n\nstatic int request_host_lcb_access(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\tret = do_8051_command(dd, HCMD_MISC,\n\t\t\t      (u64)HCMD_MISC_REQUEST_LCB_ACCESS <<\n\t\t\t      LOAD_DATA_FIELD_ID_SHIFT, NULL);\n\tif (ret != HCMD_SUCCESS && !(dd->flags & HFI1_SHUTDOWN)) {\n\t\tdd_dev_err(dd, \"%s: command failed with error %d\\n\",\n\t\t\t   __func__, ret);\n\t}\n\treturn ret == HCMD_SUCCESS ? 0 : -EBUSY;\n}\n\nstatic int request_8051_lcb_access(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\tret = do_8051_command(dd, HCMD_MISC,\n\t\t\t      (u64)HCMD_MISC_GRANT_LCB_ACCESS <<\n\t\t\t      LOAD_DATA_FIELD_ID_SHIFT, NULL);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd, \"%s: command failed with error %d\\n\",\n\t\t\t   __func__, ret);\n\t}\n\treturn ret == HCMD_SUCCESS ? 0 : -EBUSY;\n}\n\n \nstatic inline void set_host_lcb_access(struct hfi1_devdata *dd)\n{\n\twrite_csr(dd, DC_DC8051_CFG_CSR_ACCESS_SEL,\n\t\t  DC_DC8051_CFG_CSR_ACCESS_SEL_DCC_SMASK |\n\t\t  DC_DC8051_CFG_CSR_ACCESS_SEL_LCB_SMASK);\n}\n\n \nstatic inline void set_8051_lcb_access(struct hfi1_devdata *dd)\n{\n\twrite_csr(dd, DC_DC8051_CFG_CSR_ACCESS_SEL,\n\t\t  DC_DC8051_CFG_CSR_ACCESS_SEL_DCC_SMASK);\n}\n\n \nint acquire_lcb_access(struct hfi1_devdata *dd, int sleep_ok)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tint ret = 0;\n\n\t \n\tif (sleep_ok) {\n\t\tmutex_lock(&ppd->hls_lock);\n\t} else {\n\t\twhile (!mutex_trylock(&ppd->hls_lock))\n\t\t\tudelay(1);\n\t}\n\n\t \n\tif (ppd->host_link_state & HLS_DOWN) {\n\t\tdd_dev_info(dd, \"%s: link state %s not up\\n\",\n\t\t\t    __func__, link_state_name(ppd->host_link_state));\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\tif (dd->lcb_access_count == 0) {\n\t\tret = request_host_lcb_access(dd);\n\t\tif (ret) {\n\t\t\tif (!(dd->flags & HFI1_SHUTDOWN))\n\t\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: unable to acquire LCB access, err %d\\n\",\n\t\t\t\t   __func__, ret);\n\t\t\tgoto done;\n\t\t}\n\t\tset_host_lcb_access(dd);\n\t}\n\tdd->lcb_access_count++;\ndone:\n\tmutex_unlock(&ppd->hls_lock);\n\treturn ret;\n}\n\n \nint release_lcb_access(struct hfi1_devdata *dd, int sleep_ok)\n{\n\tint ret = 0;\n\n\t \n\tif (sleep_ok) {\n\t\tmutex_lock(&dd->pport->hls_lock);\n\t} else {\n\t\twhile (!mutex_trylock(&dd->pport->hls_lock))\n\t\t\tudelay(1);\n\t}\n\n\tif (dd->lcb_access_count == 0) {\n\t\tdd_dev_err(dd, \"%s: LCB access count is zero.  Skipping.\\n\",\n\t\t\t   __func__);\n\t\tgoto done;\n\t}\n\n\tif (dd->lcb_access_count == 1) {\n\t\tset_8051_lcb_access(dd);\n\t\tret = request_8051_lcb_access(dd);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: unable to release LCB access, err %d\\n\",\n\t\t\t\t   __func__, ret);\n\t\t\t \n\t\t\tset_host_lcb_access(dd);\n\t\t\tgoto done;\n\t\t}\n\t}\n\tdd->lcb_access_count--;\ndone:\n\tmutex_unlock(&dd->pport->hls_lock);\n\treturn ret;\n}\n\n \nstatic void init_lcb_access(struct hfi1_devdata *dd)\n{\n\tdd->lcb_access_count = 0;\n}\n\n \nstatic void hreq_response(struct hfi1_devdata *dd, u8 return_code, u16 rsp_data)\n{\n\twrite_csr(dd, DC_DC8051_CFG_EXT_DEV_0,\n\t\t  DC_DC8051_CFG_EXT_DEV_0_COMPLETED_SMASK |\n\t\t  (u64)return_code <<\n\t\t  DC_DC8051_CFG_EXT_DEV_0_RETURN_CODE_SHIFT |\n\t\t  (u64)rsp_data << DC_DC8051_CFG_EXT_DEV_0_RSP_DATA_SHIFT);\n}\n\n \nstatic void handle_8051_request(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 reg;\n\tu16 data = 0;\n\tu8 type;\n\n\treg = read_csr(dd, DC_DC8051_CFG_EXT_DEV_1);\n\tif ((reg & DC_DC8051_CFG_EXT_DEV_1_REQ_NEW_SMASK) == 0)\n\t\treturn;\t \n\n\t \n\twrite_csr(dd, DC_DC8051_CFG_EXT_DEV_0, 0);\n\n\t \n\ttype = (reg >> DC_DC8051_CFG_EXT_DEV_1_REQ_TYPE_SHIFT)\n\t\t\t& DC_DC8051_CFG_EXT_DEV_1_REQ_TYPE_MASK;\n\tdata = (reg >> DC_DC8051_CFG_EXT_DEV_1_REQ_DATA_SHIFT)\n\t\t\t& DC_DC8051_CFG_EXT_DEV_1_REQ_DATA_MASK;\n\n\tswitch (type) {\n\tcase HREQ_LOAD_CONFIG:\n\tcase HREQ_SAVE_CONFIG:\n\tcase HREQ_READ_CONFIG:\n\tcase HREQ_SET_TX_EQ_ABS:\n\tcase HREQ_SET_TX_EQ_REL:\n\tcase HREQ_ENABLE:\n\t\tdd_dev_info(dd, \"8051 request: request 0x%x not supported\\n\",\n\t\t\t    type);\n\t\threq_response(dd, HREQ_NOT_SUPPORTED, 0);\n\t\tbreak;\n\tcase HREQ_LCB_RESET:\n\t\t \n\t\twrite_csr(dd, DCC_CFG_RESET, LCB_RX_FPE_TX_FPE_INTO_RESET);\n\t\t \n\t\t(void)read_csr(dd, DCC_CFG_RESET);\n\t\t \n\t\tudelay(1);\n\t\t \n\t\twrite_csr(dd, DCC_CFG_RESET, LCB_RX_FPE_TX_FPE_OUT_OF_RESET);\n\t\threq_response(dd, HREQ_SUCCESS, 0);\n\n\t\tbreak;\n\tcase HREQ_CONFIG_DONE:\n\t\threq_response(dd, HREQ_SUCCESS, 0);\n\t\tbreak;\n\n\tcase HREQ_INTERFACE_TEST:\n\t\threq_response(dd, HREQ_SUCCESS, data);\n\t\tbreak;\n\tdefault:\n\t\tdd_dev_err(dd, \"8051 request: unknown request 0x%x\\n\", type);\n\t\threq_response(dd, HREQ_NOT_SUPPORTED, 0);\n\t\tbreak;\n\t}\n}\n\n \nvoid set_up_vau(struct hfi1_devdata *dd, u8 vau)\n{\n\tu64 reg = read_csr(dd, SEND_CM_GLOBAL_CREDIT);\n\n\t \n\treg &= ~SEND_CM_GLOBAL_CREDIT_AU_SMASK;\n\treg |= (u64)vau << SEND_CM_GLOBAL_CREDIT_AU_SHIFT;\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, reg);\n}\n\n \nvoid set_up_vl15(struct hfi1_devdata *dd, u16 vl15buf)\n{\n\tu64 reg = read_csr(dd, SEND_CM_GLOBAL_CREDIT);\n\n\t \n\treg &= ~(SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_SMASK |\n\t\t SEND_CM_GLOBAL_CREDIT_SHARED_LIMIT_SMASK);\n\n\t \n\treg |= (u64)vl15buf << SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_SHIFT;\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, reg);\n\n\twrite_csr(dd, SEND_CM_CREDIT_VL15, (u64)vl15buf\n\t\t  << SEND_CM_CREDIT_VL15_DEDICATED_LIMIT_VL_SHIFT);\n}\n\n \nvoid reset_link_credits(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < TXE_NUM_DATA_VL; i++)\n\t\twrite_csr(dd, SEND_CM_CREDIT_VL + (8 * i), 0);\n\twrite_csr(dd, SEND_CM_CREDIT_VL15, 0);\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, 0);\n\t \n\tpio_send_control(dd, PSC_CM_RESET);\n\t \n\tdd->vl15buf_cached = 0;\n}\n\n \nstatic u32 vcu_to_cu(u8 vcu)\n{\n\treturn 1 << vcu;\n}\n\n \nstatic u8 cu_to_vcu(u32 cu)\n{\n\treturn ilog2(cu);\n}\n\n \nstatic u32 vau_to_au(u8 vau)\n{\n\treturn 8 * (1 << vau);\n}\n\nstatic void set_linkup_defaults(struct hfi1_pportdata *ppd)\n{\n\tppd->sm_trap_qp = 0x0;\n\tppd->sa_qp = 0x1;\n}\n\n \nstatic void lcb_shutdown(struct hfi1_devdata *dd, int abort)\n{\n\tu64 reg;\n\n\t \n\twrite_csr(dd, DC_LCB_CFG_RUN, 0);\n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET,\n\t\t  1ull << DC_LCB_CFG_TX_FIFOS_RESET_VAL_SHIFT);\n\t \n\tdd->lcb_err_en = read_csr(dd, DC_LCB_ERR_EN);\n\treg = read_csr(dd, DCC_CFG_RESET);\n\twrite_csr(dd, DCC_CFG_RESET, reg |\n\t\t  DCC_CFG_RESET_RESET_LCB | DCC_CFG_RESET_RESET_RX_FPE);\n\t(void)read_csr(dd, DCC_CFG_RESET);  \n\tif (!abort) {\n\t\tudelay(1);     \n\t\twrite_csr(dd, DCC_CFG_RESET, reg);\n\t\twrite_csr(dd, DC_LCB_ERR_EN, dd->lcb_err_en);\n\t}\n}\n\n \nstatic void _dc_shutdown(struct hfi1_devdata *dd)\n{\n\tlockdep_assert_held(&dd->dc8051_lock);\n\n\tif (dd->dc_shutdown)\n\t\treturn;\n\n\tdd->dc_shutdown = 1;\n\t \n\tlcb_shutdown(dd, 1);\n\t \n\twrite_csr(dd, DC_DC8051_CFG_RST, 0x1);\n}\n\nstatic void dc_shutdown(struct hfi1_devdata *dd)\n{\n\tmutex_lock(&dd->dc8051_lock);\n\t_dc_shutdown(dd);\n\tmutex_unlock(&dd->dc8051_lock);\n}\n\n \nstatic void _dc_start(struct hfi1_devdata *dd)\n{\n\tlockdep_assert_held(&dd->dc8051_lock);\n\n\tif (!dd->dc_shutdown)\n\t\treturn;\n\n\t \n\twrite_csr(dd, DC_DC8051_CFG_RST, 0ull);\n\t \n\tif (wait_fm_ready(dd, TIMEOUT_8051_START))\n\t\tdd_dev_err(dd, \"%s: timeout starting 8051 firmware\\n\",\n\t\t\t   __func__);\n\n\t \n\twrite_csr(dd, DCC_CFG_RESET, LCB_RX_FPE_TX_FPE_OUT_OF_RESET);\n\t \n\twrite_csr(dd, DC_LCB_ERR_EN, dd->lcb_err_en);\n\tdd->dc_shutdown = 0;\n}\n\nstatic void dc_start(struct hfi1_devdata *dd)\n{\n\tmutex_lock(&dd->dc8051_lock);\n\t_dc_start(dd);\n\tmutex_unlock(&dd->dc8051_lock);\n}\n\n \nstatic void adjust_lcb_for_fpga_serdes(struct hfi1_devdata *dd)\n{\n\tu64 rx_radr, tx_radr;\n\tu32 version;\n\n\tif (dd->icode != ICODE_FPGA_EMULATION)\n\t\treturn;\n\n\t \n\tif (is_emulator_s(dd))\n\t\treturn;\n\t \n\n\tversion = emulator_rev(dd);\n\tif (!is_ax(dd))\n\t\tversion = 0x2d;\t \n\n\tif (version <= 0x12) {\n\t\t \n\n\t\t \n\t\trx_radr =\n\t\t      0xaull << DC_LCB_CFG_RX_FIFOS_RADR_DO_NOT_JUMP_VAL_SHIFT\n\t\t    | 0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_OK_TO_JUMP_VAL_SHIFT\n\t\t    | 0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\t \n\t\ttx_radr = 6ull << DC_LCB_CFG_TX_FIFOS_RADR_RST_VAL_SHIFT;\n\t} else if (version <= 0x18) {\n\t\t \n\t\t \n\t\trx_radr =\n\t\t      0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_DO_NOT_JUMP_VAL_SHIFT\n\t\t    | 0x8ull << DC_LCB_CFG_RX_FIFOS_RADR_OK_TO_JUMP_VAL_SHIFT\n\t\t    | 0x8ull << DC_LCB_CFG_RX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\ttx_radr = 7ull << DC_LCB_CFG_TX_FIFOS_RADR_RST_VAL_SHIFT;\n\t} else if (version == 0x19) {\n\t\t \n\t\t \n\t\trx_radr =\n\t\t      0xAull << DC_LCB_CFG_RX_FIFOS_RADR_DO_NOT_JUMP_VAL_SHIFT\n\t\t    | 0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_OK_TO_JUMP_VAL_SHIFT\n\t\t    | 0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\ttx_radr = 3ull << DC_LCB_CFG_TX_FIFOS_RADR_RST_VAL_SHIFT;\n\t} else if (version == 0x1a) {\n\t\t \n\t\t \n\t\trx_radr =\n\t\t      0x9ull << DC_LCB_CFG_RX_FIFOS_RADR_DO_NOT_JUMP_VAL_SHIFT\n\t\t    | 0x8ull << DC_LCB_CFG_RX_FIFOS_RADR_OK_TO_JUMP_VAL_SHIFT\n\t\t    | 0x8ull << DC_LCB_CFG_RX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\ttx_radr = 7ull << DC_LCB_CFG_TX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\twrite_csr(dd, DC_LCB_CFG_LN_DCLK, 1ull);\n\t} else {\n\t\t \n\t\t \n\t\trx_radr =\n\t\t      0x8ull << DC_LCB_CFG_RX_FIFOS_RADR_DO_NOT_JUMP_VAL_SHIFT\n\t\t    | 0x7ull << DC_LCB_CFG_RX_FIFOS_RADR_OK_TO_JUMP_VAL_SHIFT\n\t\t    | 0x7ull << DC_LCB_CFG_RX_FIFOS_RADR_RST_VAL_SHIFT;\n\t\ttx_radr = 3ull << DC_LCB_CFG_TX_FIFOS_RADR_RST_VAL_SHIFT;\n\t}\n\n\twrite_csr(dd, DC_LCB_CFG_RX_FIFOS_RADR, rx_radr);\n\t \n\twrite_csr(dd, DC_LCB_CFG_IGNORE_LOST_RCLK,\n\t\t  DC_LCB_CFG_IGNORE_LOST_RCLK_EN_SMASK);\n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RADR, tx_radr);\n}\n\n \nvoid handle_sma_message(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t\tsma_message_work);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 msg;\n\tint ret;\n\n\t \n\tret = read_idle_sma(dd, &msg);\n\tif (ret)\n\t\treturn;\n\tdd_dev_info(dd, \"%s: SMA message 0x%llx\\n\", __func__, msg);\n\t \n\tswitch (msg & 0xff) {\n\tcase SMA_IDLE_ARM:\n\t\t \n\t\tif (ppd->host_link_state & (HLS_UP_INIT | HLS_UP_ARMED))\n\t\t\tppd->neighbor_normal = 1;\n\t\tbreak;\n\tcase SMA_IDLE_ACTIVE:\n\t\t \n\t\tif (ppd->host_link_state == HLS_UP_ARMED &&\n\t\t    ppd->is_active_optimize_enabled) {\n\t\t\tppd->neighbor_normal = 1;\n\t\t\tret = set_link_state(ppd, HLS_UP_ACTIVE);\n\t\t\tif (ret)\n\t\t\t\tdd_dev_err(\n\t\t\t\t\tdd,\n\t\t\t\t\t\"%s: received Active SMA idle message, couldn't set link to Active\\n\",\n\t\t\t\t\t__func__);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdd_dev_err(dd,\n\t\t\t   \"%s: received unexpected SMA idle message 0x%llx\\n\",\n\t\t\t   __func__, msg);\n\t\tbreak;\n\t}\n}\n\nstatic void adjust_rcvctrl(struct hfi1_devdata *dd, u64 add, u64 clear)\n{\n\tu64 rcvctrl;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dd->rcvctrl_lock, flags);\n\trcvctrl = read_csr(dd, RCV_CTRL);\n\trcvctrl |= add;\n\trcvctrl &= ~clear;\n\twrite_csr(dd, RCV_CTRL, rcvctrl);\n\tspin_unlock_irqrestore(&dd->rcvctrl_lock, flags);\n}\n\nstatic inline void add_rcvctrl(struct hfi1_devdata *dd, u64 add)\n{\n\tadjust_rcvctrl(dd, add, 0);\n}\n\nstatic inline void clear_rcvctrl(struct hfi1_devdata *dd, u64 clear)\n{\n\tadjust_rcvctrl(dd, 0, clear);\n}\n\n \nvoid start_freeze_handling(struct hfi1_pportdata *ppd, int flags)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct send_context *sc;\n\tint i;\n\tint sc_flags;\n\n\tif (flags & FREEZE_SELF)\n\t\twrite_csr(dd, CCE_CTRL, CCE_CTRL_SPC_FREEZE_SMASK);\n\n\t \n\tdd->flags |= HFI1_FROZEN;\n\n\t \n\tsdma_freeze_notify(dd, !!(flags & FREEZE_LINK_DOWN));\n\n\tsc_flags = SCF_FROZEN | SCF_HALTED | (flags & FREEZE_LINK_DOWN ?\n\t\t\t\t\t      SCF_LINK_DOWN : 0);\n\t \n\tfor (i = 0; i < dd->num_send_contexts; i++) {\n\t\tsc = dd->send_contexts[i].sc;\n\t\tif (sc && (sc->flags & SCF_ENABLED))\n\t\t\tsc_stop(sc, sc_flags);\n\t}\n\n\t \n\thfi1_set_uevent_bits(ppd, _HFI1_EVENT_FROZEN_BIT);\n\n\tif (flags & FREEZE_ABORT) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Aborted freeze recovery. Please REBOOT system\\n\");\n\t\treturn;\n\t}\n\t \n\tqueue_work(ppd->hfi1_wq, &ppd->freeze_work);\n}\n\n \nstatic void wait_for_freeze_status(struct hfi1_devdata *dd, int freeze)\n{\n\tunsigned long timeout;\n\tu64 reg;\n\n\ttimeout = jiffies + msecs_to_jiffies(FREEZE_STATUS_TIMEOUT);\n\twhile (1) {\n\t\treg = read_csr(dd, CCE_STATUS);\n\t\tif (freeze) {\n\t\t\t \n\t\t\tif ((reg & ALL_FROZE) == ALL_FROZE)\n\t\t\t\treturn;\t \n\t\t} else {\n\t\t\t \n\t\t\tif ((reg & ALL_FROZE) == 0)\n\t\t\t\treturn;  \n\t\t}\n\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Time out waiting for SPC %sfreeze, bits 0x%llx, expecting 0x%llx, continuing\",\n\t\t\t\t   freeze ? \"\" : \"un\", reg & ALL_FROZE,\n\t\t\t\t   freeze ? ALL_FROZE : 0ull);\n\t\t\treturn;\n\t\t}\n\t\tusleep_range(80, 120);\n\t}\n}\n\n \nstatic void rxe_freeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\tstruct hfi1_ctxtdata *rcd;\n\n\t \n\tclear_rcvctrl(dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);\n\n\t \n\tfor (i = 0; i < dd->num_rcv_contexts; i++) {\n\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\t\thfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS, rcd);\n\t\thfi1_rcd_put(rcd);\n\t}\n}\n\n \nstatic void rxe_kernel_unfreeze(struct hfi1_devdata *dd)\n{\n\tu32 rcvmask;\n\tu16 i;\n\tstruct hfi1_ctxtdata *rcd;\n\n\t \n\tfor (i = 0; i < dd->num_rcv_contexts; i++) {\n\t\trcd = hfi1_rcd_get_by_index(dd, i);\n\n\t\t \n\t\tif (!rcd ||\n\t\t    (i >= dd->first_dyn_alloc_ctxt && !rcd->is_vnic)) {\n\t\t\thfi1_rcd_put(rcd);\n\t\t\tcontinue;\n\t\t}\n\t\trcvmask = HFI1_RCVCTRL_CTXT_ENB;\n\t\t \n\t\trcvmask |= hfi1_rcvhdrtail_kvaddr(rcd) ?\n\t\t\tHFI1_RCVCTRL_TAILUPD_ENB : HFI1_RCVCTRL_TAILUPD_DIS;\n\t\thfi1_rcvctrl(dd, rcvmask, rcd);\n\t\thfi1_rcd_put(rcd);\n\t}\n\n\t \n\tadd_rcvctrl(dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);\n}\n\n \nvoid handle_freeze(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t\t\tfreeze_work);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\t \n\twait_for_freeze_status(dd, 1);\n\n\t \n\n\t \n\tpio_freeze(dd);\n\n\t \n\tsdma_freeze(dd);\n\n\t \n\n\t \n\trxe_freeze(dd);\n\n\t \n\twrite_csr(dd, CCE_CTRL, CCE_CTRL_SPC_UNFREEZE_SMASK);\n\twait_for_freeze_status(dd, 0);\n\n\tif (is_ax(dd)) {\n\t\twrite_csr(dd, CCE_CTRL, CCE_CTRL_SPC_FREEZE_SMASK);\n\t\twait_for_freeze_status(dd, 1);\n\t\twrite_csr(dd, CCE_CTRL, CCE_CTRL_SPC_UNFREEZE_SMASK);\n\t\twait_for_freeze_status(dd, 0);\n\t}\n\n\t \n\tpio_kernel_unfreeze(dd);\n\n\t \n\tsdma_unfreeze(dd);\n\n\t \n\n\t \n\trxe_kernel_unfreeze(dd);\n\n\t \n\tdd->flags &= ~HFI1_FROZEN;\n\twake_up(&dd->event_queue);\n\n\t \n}\n\n \nstatic void update_xmit_counters(struct hfi1_pportdata *ppd, u16 link_width)\n{\n\tint i;\n\tu16 tx_width;\n\tu16 link_speed;\n\n\ttx_width = tx_link_width(link_width);\n\tlink_speed = get_link_speed(ppd->link_speed_active);\n\n\t \n\tfor (i = 0; i < C_VL_COUNT + 1; i++)\n\t\tget_xmit_wait_counters(ppd, tx_width, link_speed, i);\n}\n\n \nvoid handle_link_up(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t  link_up_work);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\tset_link_state(ppd, HLS_UP_INIT);\n\n\t \n\tread_ltp_rtt(dd);\n\t \n\tclear_linkup_counters(dd);\n\t \n\tset_linkup_defaults(ppd);\n\n\t \n\tif (!(quick_linkup || dd->icode == ICODE_FUNCTIONAL_SIMULATOR))\n\t\tset_up_vl15(dd, dd->vl15buf_cached);\n\n\t \n\tif ((ppd->link_speed_active & ppd->link_speed_enabled) == 0) {\n\t\t \n\t\tdd_dev_err(dd,\n\t\t\t   \"Link speed active 0x%x is outside enabled 0x%x, downing link\\n\",\n\t\t\t   ppd->link_speed_active, ppd->link_speed_enabled);\n\t\tset_link_down_reason(ppd, OPA_LINKDOWN_REASON_SPEED_POLICY, 0,\n\t\t\t\t     OPA_LINKDOWN_REASON_SPEED_POLICY);\n\t\tset_link_state(ppd, HLS_DN_OFFLINE);\n\t\tstart_link(ppd);\n\t}\n}\n\n \nstatic void reset_neighbor_info(struct hfi1_pportdata *ppd)\n{\n\tppd->neighbor_guid = 0;\n\tppd->neighbor_port_number = 0;\n\tppd->neighbor_type = 0;\n\tppd->neighbor_fm_security = 0;\n}\n\nstatic const char * const link_down_reason_strs[] = {\n\t[OPA_LINKDOWN_REASON_NONE] = \"None\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_0] = \"Receive error 0\",\n\t[OPA_LINKDOWN_REASON_BAD_PKT_LEN] = \"Bad packet length\",\n\t[OPA_LINKDOWN_REASON_PKT_TOO_LONG] = \"Packet too long\",\n\t[OPA_LINKDOWN_REASON_PKT_TOO_SHORT] = \"Packet too short\",\n\t[OPA_LINKDOWN_REASON_BAD_SLID] = \"Bad SLID\",\n\t[OPA_LINKDOWN_REASON_BAD_DLID] = \"Bad DLID\",\n\t[OPA_LINKDOWN_REASON_BAD_L2] = \"Bad L2\",\n\t[OPA_LINKDOWN_REASON_BAD_SC] = \"Bad SC\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_8] = \"Receive error 8\",\n\t[OPA_LINKDOWN_REASON_BAD_MID_TAIL] = \"Bad mid tail\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_10] = \"Receive error 10\",\n\t[OPA_LINKDOWN_REASON_PREEMPT_ERROR] = \"Preempt error\",\n\t[OPA_LINKDOWN_REASON_PREEMPT_VL15] = \"Preempt vl15\",\n\t[OPA_LINKDOWN_REASON_BAD_VL_MARKER] = \"Bad VL marker\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_14] = \"Receive error 14\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_15] = \"Receive error 15\",\n\t[OPA_LINKDOWN_REASON_BAD_HEAD_DIST] = \"Bad head distance\",\n\t[OPA_LINKDOWN_REASON_BAD_TAIL_DIST] = \"Bad tail distance\",\n\t[OPA_LINKDOWN_REASON_BAD_CTRL_DIST] = \"Bad control distance\",\n\t[OPA_LINKDOWN_REASON_BAD_CREDIT_ACK] = \"Bad credit ack\",\n\t[OPA_LINKDOWN_REASON_UNSUPPORTED_VL_MARKER] = \"Unsupported VL marker\",\n\t[OPA_LINKDOWN_REASON_BAD_PREEMPT] = \"Bad preempt\",\n\t[OPA_LINKDOWN_REASON_BAD_CONTROL_FLIT] = \"Bad control flit\",\n\t[OPA_LINKDOWN_REASON_EXCEED_MULTICAST_LIMIT] = \"Exceed multicast limit\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_24] = \"Receive error 24\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_25] = \"Receive error 25\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_26] = \"Receive error 26\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_27] = \"Receive error 27\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_28] = \"Receive error 28\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_29] = \"Receive error 29\",\n\t[OPA_LINKDOWN_REASON_RCV_ERROR_30] = \"Receive error 30\",\n\t[OPA_LINKDOWN_REASON_EXCESSIVE_BUFFER_OVERRUN] =\n\t\t\t\t\t\"Excessive buffer overrun\",\n\t[OPA_LINKDOWN_REASON_UNKNOWN] = \"Unknown\",\n\t[OPA_LINKDOWN_REASON_REBOOT] = \"Reboot\",\n\t[OPA_LINKDOWN_REASON_NEIGHBOR_UNKNOWN] = \"Neighbor unknown\",\n\t[OPA_LINKDOWN_REASON_FM_BOUNCE] = \"FM bounce\",\n\t[OPA_LINKDOWN_REASON_SPEED_POLICY] = \"Speed policy\",\n\t[OPA_LINKDOWN_REASON_WIDTH_POLICY] = \"Width policy\",\n\t[OPA_LINKDOWN_REASON_DISCONNECTED] = \"Disconnected\",\n\t[OPA_LINKDOWN_REASON_LOCAL_MEDIA_NOT_INSTALLED] =\n\t\t\t\t\t\"Local media not installed\",\n\t[OPA_LINKDOWN_REASON_NOT_INSTALLED] = \"Not installed\",\n\t[OPA_LINKDOWN_REASON_CHASSIS_CONFIG] = \"Chassis config\",\n\t[OPA_LINKDOWN_REASON_END_TO_END_NOT_INSTALLED] =\n\t\t\t\t\t\"End to end not installed\",\n\t[OPA_LINKDOWN_REASON_POWER_POLICY] = \"Power policy\",\n\t[OPA_LINKDOWN_REASON_LINKSPEED_POLICY] = \"Link speed policy\",\n\t[OPA_LINKDOWN_REASON_LINKWIDTH_POLICY] = \"Link width policy\",\n\t[OPA_LINKDOWN_REASON_SWITCH_MGMT] = \"Switch management\",\n\t[OPA_LINKDOWN_REASON_SMA_DISABLED] = \"SMA disabled\",\n\t[OPA_LINKDOWN_REASON_TRANSIENT] = \"Transient\"\n};\n\n \nstatic const char *link_down_reason_str(u8 reason)\n{\n\tconst char *str = NULL;\n\n\tif (reason < ARRAY_SIZE(link_down_reason_strs))\n\t\tstr = link_down_reason_strs[reason];\n\tif (!str)\n\t\tstr = \"(invalid)\";\n\n\treturn str;\n}\n\n \nvoid handle_link_down(struct work_struct *work)\n{\n\tu8 lcl_reason, neigh_reason = 0;\n\tu8 link_down_reason;\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t  link_down_work);\n\tint was_up;\n\tstatic const char ldr_str[] = \"Link down reason: \";\n\n\tif ((ppd->host_link_state &\n\t     (HLS_DN_POLL | HLS_VERIFY_CAP | HLS_GOING_UP)) &&\n\t     ppd->port_type == PORT_TYPE_FIXED)\n\t\tppd->offline_disabled_reason =\n\t\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_NOT_INSTALLED);\n\n\t \n\twas_up = !!(ppd->host_link_state & HLS_UP);\n\tset_link_state(ppd, HLS_DN_OFFLINE);\n\txchg(&ppd->is_link_down_queued, 0);\n\n\tif (was_up) {\n\t\tlcl_reason = 0;\n\t\t \n\t\tread_link_down_reason(ppd->dd, &link_down_reason);\n\t\tswitch (link_down_reason) {\n\t\tcase LDR_LINK_TRANSFER_ACTIVE_LOW:\n\t\t\t \n\t\t\tdd_dev_info(ppd->dd, \"%sUnexpected link down\\n\",\n\t\t\t\t    ldr_str);\n\t\t\tbreak;\n\t\tcase LDR_RECEIVED_LINKDOWN_IDLE_MSG:\n\t\t\t \n\t\t\tread_planned_down_reason_code(ppd->dd, &neigh_reason);\n\t\t\tdd_dev_info(ppd->dd,\n\t\t\t\t    \"%sNeighbor link down message %d, %s\\n\",\n\t\t\t\t    ldr_str, neigh_reason,\n\t\t\t\t    link_down_reason_str(neigh_reason));\n\t\t\tbreak;\n\t\tcase LDR_RECEIVED_HOST_OFFLINE_REQ:\n\t\t\tdd_dev_info(ppd->dd,\n\t\t\t\t    \"%sHost requested link to go offline\\n\",\n\t\t\t\t    ldr_str);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdd_dev_info(ppd->dd, \"%sUnknown reason 0x%x\\n\",\n\t\t\t\t    ldr_str, link_down_reason);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (neigh_reason == 0)\n\t\t\tlcl_reason = OPA_LINKDOWN_REASON_NEIGHBOR_UNKNOWN;\n\t} else {\n\t\t \n\t\tlcl_reason = OPA_LINKDOWN_REASON_TRANSIENT;\n\t}\n\n\tset_link_down_reason(ppd, lcl_reason, neigh_reason, 0);\n\n\t \n\tif (was_up && ppd->local_link_down_reason.sma == 0 &&\n\t    ppd->neigh_link_down_reason.sma == 0) {\n\t\tppd->local_link_down_reason.sma =\n\t\t\t\t\tppd->local_link_down_reason.latest;\n\t\tppd->neigh_link_down_reason.sma =\n\t\t\t\t\tppd->neigh_link_down_reason.latest;\n\t}\n\n\treset_neighbor_info(ppd);\n\n\t \n\tclear_rcvctrl(ppd->dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);\n\n\t \n\tif (ppd->port_type == PORT_TYPE_QSFP && !qsfp_mod_present(ppd))\n\t\tdc_shutdown(ppd->dd);\n\telse\n\t\tstart_link(ppd);\n}\n\nvoid handle_link_bounce(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t\tlink_bounce_work);\n\n\t \n\tif (ppd->host_link_state & HLS_UP) {\n\t\tset_link_state(ppd, HLS_DN_OFFLINE);\n\t\tstart_link(ppd);\n\t} else {\n\t\tdd_dev_info(ppd->dd, \"%s: link not up (%s), nothing to do\\n\",\n\t\t\t    __func__, link_state_name(ppd->host_link_state));\n\t}\n}\n\n \nstatic int cap_to_port_ltp(int cap)\n{\n\tint port_ltp = PORT_LTP_CRC_MODE_16;  \n\n\tif (cap & CAP_CRC_14B)\n\t\tport_ltp |= PORT_LTP_CRC_MODE_14;\n\tif (cap & CAP_CRC_48B)\n\t\tport_ltp |= PORT_LTP_CRC_MODE_48;\n\tif (cap & CAP_CRC_12B_16B_PER_LANE)\n\t\tport_ltp |= PORT_LTP_CRC_MODE_PER_LANE;\n\n\treturn port_ltp;\n}\n\n \nint port_ltp_to_cap(int port_ltp)\n{\n\tint cap_mask = 0;\n\n\tif (port_ltp & PORT_LTP_CRC_MODE_14)\n\t\tcap_mask |= CAP_CRC_14B;\n\tif (port_ltp & PORT_LTP_CRC_MODE_48)\n\t\tcap_mask |= CAP_CRC_48B;\n\tif (port_ltp & PORT_LTP_CRC_MODE_PER_LANE)\n\t\tcap_mask |= CAP_CRC_12B_16B_PER_LANE;\n\n\treturn cap_mask;\n}\n\n \nstatic int lcb_to_port_ltp(int lcb_crc)\n{\n\tint port_ltp = 0;\n\n\tif (lcb_crc == LCB_CRC_12B_16B_PER_LANE)\n\t\tport_ltp = PORT_LTP_CRC_MODE_PER_LANE;\n\telse if (lcb_crc == LCB_CRC_48B)\n\t\tport_ltp = PORT_LTP_CRC_MODE_48;\n\telse if (lcb_crc == LCB_CRC_14B)\n\t\tport_ltp = PORT_LTP_CRC_MODE_14;\n\telse\n\t\tport_ltp = PORT_LTP_CRC_MODE_16;\n\n\treturn port_ltp;\n}\n\nstatic void clear_full_mgmt_pkey(struct hfi1_pportdata *ppd)\n{\n\tif (ppd->pkeys[2] != 0) {\n\t\tppd->pkeys[2] = 0;\n\t\t(void)hfi1_set_ib_cfg(ppd, HFI1_IB_CFG_PKEYS, 0);\n\t\thfi1_event_pkey_change(ppd->dd, ppd->port);\n\t}\n}\n\n \nstatic u16 link_width_to_bits(struct hfi1_devdata *dd, u16 width)\n{\n\tswitch (width) {\n\tcase 0:\n\t\t \n\t\tif (dd->icode == ICODE_FUNCTIONAL_SIMULATOR || quick_linkup)\n\t\t\treturn OPA_LINK_WIDTH_4X;\n\t\treturn 0;  \n\tcase 1: return OPA_LINK_WIDTH_1X;\n\tcase 2: return OPA_LINK_WIDTH_2X;\n\tcase 3: return OPA_LINK_WIDTH_3X;\n\tcase 4: return OPA_LINK_WIDTH_4X;\n\tdefault:\n\t\tdd_dev_info(dd, \"%s: invalid width %d, using 4\\n\",\n\t\t\t    __func__, width);\n\t\treturn OPA_LINK_WIDTH_4X;\n\t}\n}\n\n \nstatic const u8 bit_counts[16] = {\n\t0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n};\n\nstatic inline u8 nibble_to_count(u8 nibble)\n{\n\treturn bit_counts[nibble & 0xf];\n}\n\n \nstatic void get_link_widths(struct hfi1_devdata *dd, u16 *tx_width,\n\t\t\t    u16 *rx_width)\n{\n\tu16 tx, rx;\n\tu8 enable_lane_rx;\n\tu8 enable_lane_tx;\n\tu8 tx_polarity_inversion;\n\tu8 rx_polarity_inversion;\n\tu8 max_rate;\n\n\t \n\tread_tx_settings(dd, &enable_lane_tx, &tx_polarity_inversion,\n\t\t\t &rx_polarity_inversion, &max_rate);\n\tread_local_lni(dd, &enable_lane_rx);\n\n\t \n\ttx = nibble_to_count(enable_lane_tx);\n\trx = nibble_to_count(enable_lane_rx);\n\n\t \n\tif ((dd->icode == ICODE_RTL_SILICON) &&\n\t    (dd->dc8051_ver < dc8051_ver(0, 19, 0))) {\n\t\t \n\t\tswitch (max_rate) {\n\t\tcase 0:\n\t\t\tdd->pport[0].link_speed_active = OPA_LINK_SPEED_12_5G;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tdd->pport[0].link_speed_active = OPA_LINK_SPEED_25G;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: unexpected max rate %d, using 25Gb\\n\",\n\t\t\t\t   __func__, (int)max_rate);\n\t\t\tdd->pport[0].link_speed_active = OPA_LINK_SPEED_25G;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdd_dev_info(dd,\n\t\t    \"Fabric active lanes (width): tx 0x%x (%d), rx 0x%x (%d)\\n\",\n\t\t    enable_lane_tx, tx, enable_lane_rx, rx);\n\t*tx_width = link_width_to_bits(dd, tx);\n\t*rx_width = link_width_to_bits(dd, rx);\n}\n\n \nstatic void get_linkup_widths(struct hfi1_devdata *dd, u16 *tx_width,\n\t\t\t      u16 *rx_width)\n{\n\tu16 widths, tx, rx;\n\tu8 misc_bits, local_flags;\n\tu16 active_tx, active_rx;\n\n\tread_vc_local_link_mode(dd, &misc_bits, &local_flags, &widths);\n\ttx = widths >> 12;\n\trx = (widths >> 8) & 0xf;\n\n\t*tx_width = link_width_to_bits(dd, tx);\n\t*rx_width = link_width_to_bits(dd, rx);\n\n\t \n\tget_link_widths(dd, &active_tx, &active_rx);\n}\n\n \nvoid get_linkup_link_widths(struct hfi1_pportdata *ppd)\n{\n\tu16 tx_width, rx_width;\n\n\t \n\tget_linkup_widths(ppd->dd, &tx_width, &rx_width);\n\n\t \n\tppd->link_width_active = tx_width;\n\t \n\tppd->link_width_downgrade_tx_active = ppd->link_width_active;\n\tppd->link_width_downgrade_rx_active = ppd->link_width_active;\n\t \n\tppd->link_width_downgrade_enabled = ppd->link_width_downgrade_supported;\n\t \n\tppd->current_egress_rate = active_egress_rate(ppd);\n}\n\n \nvoid handle_verify_cap(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t\t\tlink_vc_work);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 reg;\n\tu8 power_management;\n\tu8 continuous;\n\tu8 vcu;\n\tu8 vau;\n\tu8 z;\n\tu16 vl15buf;\n\tu16 link_widths;\n\tu16 crc_mask;\n\tu16 crc_val;\n\tu16 device_id;\n\tu16 active_tx, active_rx;\n\tu8 partner_supported_crc;\n\tu8 remote_tx_rate;\n\tu8 device_rev;\n\n\tset_link_state(ppd, HLS_VERIFY_CAP);\n\n\tlcb_shutdown(dd, 0);\n\tadjust_lcb_for_fpga_serdes(dd);\n\n\tread_vc_remote_phy(dd, &power_management, &continuous);\n\tread_vc_remote_fabric(dd, &vau, &z, &vcu, &vl15buf,\n\t\t\t      &partner_supported_crc);\n\tread_vc_remote_link_width(dd, &remote_tx_rate, &link_widths);\n\tread_remote_device_id(dd, &device_id, &device_rev);\n\n\t \n\tget_link_widths(dd, &active_tx, &active_rx);\n\tdd_dev_info(dd,\n\t\t    \"Peer PHY: power management 0x%x, continuous updates 0x%x\\n\",\n\t\t    (int)power_management, (int)continuous);\n\tdd_dev_info(dd,\n\t\t    \"Peer Fabric: vAU %d, Z %d, vCU %d, vl15 credits 0x%x, CRC sizes 0x%x\\n\",\n\t\t    (int)vau, (int)z, (int)vcu, (int)vl15buf,\n\t\t    (int)partner_supported_crc);\n\tdd_dev_info(dd, \"Peer Link Width: tx rate 0x%x, widths 0x%x\\n\",\n\t\t    (u32)remote_tx_rate, (u32)link_widths);\n\tdd_dev_info(dd, \"Peer Device ID: 0x%04x, Revision 0x%02x\\n\",\n\t\t    (u32)device_id, (u32)device_rev);\n\t \n\tif (vau == 0)\n\t\tvau = 1;\n\tset_up_vau(dd, vau);\n\n\t \n\tset_up_vl15(dd, 0);\n\tdd->vl15buf_cached = vl15buf;\n\n\t \n\tcrc_mask = ppd->port_crc_mode_enabled & partner_supported_crc;\n\n\t \n\tif (crc_mask & CAP_CRC_14B)\n\t\tcrc_val = LCB_CRC_14B;\n\telse if (crc_mask & CAP_CRC_48B)\n\t\tcrc_val = LCB_CRC_48B;\n\telse if (crc_mask & CAP_CRC_12B_16B_PER_LANE)\n\t\tcrc_val = LCB_CRC_12B_16B_PER_LANE;\n\telse\n\t\tcrc_val = LCB_CRC_16B;\n\n\tdd_dev_info(dd, \"Final LCB CRC mode: %d\\n\", (int)crc_val);\n\twrite_csr(dd, DC_LCB_CFG_CRC_MODE,\n\t\t  (u64)crc_val << DC_LCB_CFG_CRC_MODE_TX_VAL_SHIFT);\n\n\t \n\treg = read_csr(dd, SEND_CM_CTRL);\n\tif (crc_val == LCB_CRC_14B && crc_14b_sideband) {\n\t\twrite_csr(dd, SEND_CM_CTRL,\n\t\t\t  reg | SEND_CM_CTRL_FORCE_CREDIT_MODE_SMASK);\n\t} else {\n\t\twrite_csr(dd, SEND_CM_CTRL,\n\t\t\t  reg & ~SEND_CM_CTRL_FORCE_CREDIT_MODE_SMASK);\n\t}\n\n\tppd->link_speed_active = 0;\t \n\tif (dd->dc8051_ver < dc8051_ver(0, 20, 0)) {\n\t\t \n\t\tswitch (remote_tx_rate) {\n\t\tcase 0:\n\t\t\tppd->link_speed_active = OPA_LINK_SPEED_12_5G;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tppd->link_speed_active = OPA_LINK_SPEED_25G;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t \n\t\tu8 rate = remote_tx_rate & ppd->local_tx_rate;\n\n\t\tif (rate & 2)\n\t\t\tppd->link_speed_active = OPA_LINK_SPEED_25G;\n\t\telse if (rate & 1)\n\t\t\tppd->link_speed_active = OPA_LINK_SPEED_12_5G;\n\t}\n\tif (ppd->link_speed_active == 0) {\n\t\tdd_dev_err(dd, \"%s: unexpected remote tx rate %d, using 25Gb\\n\",\n\t\t\t   __func__, (int)remote_tx_rate);\n\t\tppd->link_speed_active = OPA_LINK_SPEED_25G;\n\t}\n\n\t \n\tppd->port_ltp_crc_mode = cap_to_port_ltp(link_crc_mask) << 8;\n\t\t \n\tppd->port_ltp_crc_mode |=\n\t\tcap_to_port_ltp(ppd->port_crc_mode_enabled) << 4;\n\t\t \n\tppd->port_ltp_crc_mode |= lcb_to_port_ltp(crc_val);\n\t\t \n\n\t \n\tassign_remote_cm_au_table(dd, vcu);\n\n\t \n\tif (is_ax(dd)) {\t\t\t \n\t\treg = read_csr(dd, DC_LCB_CFG_LINK_KILL_EN);\n\t\treg |= DC_LCB_CFG_LINK_KILL_EN_REPLAY_BUF_MBE_SMASK\n\t\t\t| DC_LCB_CFG_LINK_KILL_EN_FLIT_INPUT_BUF_MBE_SMASK;\n\t\twrite_csr(dd, DC_LCB_CFG_LINK_KILL_EN, reg);\n\t}\n\n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 0);\n\n\t \n\twrite_csr(dd, DC_LCB_ERR_EN, 0);  \n\tset_8051_lcb_access(dd);\n\n\t \n\tset_link_state(ppd, HLS_GOING_UP);\n}\n\n \nbool apply_link_downgrade_policy(struct hfi1_pportdata *ppd,\n\t\t\t\t bool refresh_widths)\n{\n\tint do_bounce = 0;\n\tint tries;\n\tu16 lwde;\n\tu16 tx, rx;\n\tbool link_downgraded = refresh_widths;\n\n\t \n\ttries = 0;\nretry:\n\tmutex_lock(&ppd->hls_lock);\n\t \n\tif (ppd->host_link_state & HLS_DOWN) {\n\t\t \n\t\tif (ppd->host_link_state & HLS_GOING_UP) {\n\t\t\tif (++tries < 1000) {\n\t\t\t\tmutex_unlock(&ppd->hls_lock);\n\t\t\t\tusleep_range(100, 120);  \n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"%s: giving up waiting for link state change\\n\",\n\t\t\t\t   __func__);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tlwde = ppd->link_width_downgrade_enabled;\n\n\tif (refresh_widths) {\n\t\tget_link_widths(ppd->dd, &tx, &rx);\n\t\tppd->link_width_downgrade_tx_active = tx;\n\t\tppd->link_width_downgrade_rx_active = rx;\n\t}\n\n\tif (ppd->link_width_downgrade_tx_active == 0 ||\n\t    ppd->link_width_downgrade_rx_active == 0) {\n\t\t \n\t\tdd_dev_err(ppd->dd, \"Link downgrade is really a link down, ignoring\\n\");\n\t\tlink_downgraded = false;\n\t} else if (lwde == 0) {\n\t\t \n\n\t\t \n\t\tif ((ppd->link_width_active !=\n\t\t     ppd->link_width_downgrade_tx_active) ||\n\t\t    (ppd->link_width_active !=\n\t\t     ppd->link_width_downgrade_rx_active)) {\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"Link downgrade is disabled and link has downgraded, downing link\\n\");\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"  original 0x%x, tx active 0x%x, rx active 0x%x\\n\",\n\t\t\t\t   ppd->link_width_active,\n\t\t\t\t   ppd->link_width_downgrade_tx_active,\n\t\t\t\t   ppd->link_width_downgrade_rx_active);\n\t\t\tdo_bounce = 1;\n\t\t\tlink_downgraded = false;\n\t\t}\n\t} else if ((lwde & ppd->link_width_downgrade_tx_active) == 0 ||\n\t\t   (lwde & ppd->link_width_downgrade_rx_active) == 0) {\n\t\t \n\t\tdd_dev_err(ppd->dd,\n\t\t\t   \"Link is outside of downgrade allowed, downing link\\n\");\n\t\tdd_dev_err(ppd->dd,\n\t\t\t   \"  enabled 0x%x, tx active 0x%x, rx active 0x%x\\n\",\n\t\t\t   lwde, ppd->link_width_downgrade_tx_active,\n\t\t\t   ppd->link_width_downgrade_rx_active);\n\t\tdo_bounce = 1;\n\t\tlink_downgraded = false;\n\t}\n\ndone:\n\tmutex_unlock(&ppd->hls_lock);\n\n\tif (do_bounce) {\n\t\tset_link_down_reason(ppd, OPA_LINKDOWN_REASON_WIDTH_POLICY, 0,\n\t\t\t\t     OPA_LINKDOWN_REASON_WIDTH_POLICY);\n\t\tset_link_state(ppd, HLS_DN_OFFLINE);\n\t\tstart_link(ppd);\n\t}\n\n\treturn link_downgraded;\n}\n\n \nvoid handle_link_downgrade(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t\tlink_downgrade_work);\n\n\tdd_dev_info(ppd->dd, \"8051: Link width downgrade\\n\");\n\tif (apply_link_downgrade_policy(ppd, true))\n\t\tupdate_xmit_counters(ppd, ppd->link_width_downgrade_tx_active);\n}\n\nstatic char *dcc_err_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, dcc_err_flags,\n\t\tARRAY_SIZE(dcc_err_flags));\n}\n\nstatic char *lcb_err_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, lcb_err_flags,\n\t\tARRAY_SIZE(lcb_err_flags));\n}\n\nstatic char *dc8051_err_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, dc8051_err_flags,\n\t\tARRAY_SIZE(dc8051_err_flags));\n}\n\nstatic char *dc8051_info_err_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, dc8051_info_err_flags,\n\t\tARRAY_SIZE(dc8051_info_err_flags));\n}\n\nstatic char *dc8051_info_host_msg_string(char *buf, int buf_len, u64 flags)\n{\n\treturn flag_string(buf, buf_len, flags, dc8051_info_host_msg_flags,\n\t\tARRAY_SIZE(dc8051_info_host_msg_flags));\n}\n\nstatic void handle_8051_interrupt(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tu64 info, err, host_msg;\n\tint queue_link_down = 0;\n\tchar buf[96];\n\n\t \n\tif (reg & DC_DC8051_ERR_FLG_SET_BY_8051_SMASK) {\n\t\t \n\t\t \n\t\tinfo = read_csr(dd, DC_DC8051_DBG_ERR_INFO_SET_BY_8051);\n\t\terr = (info >> DC_DC8051_DBG_ERR_INFO_SET_BY_8051_ERROR_SHIFT)\n\t\t\t& DC_DC8051_DBG_ERR_INFO_SET_BY_8051_ERROR_MASK;\n\t\thost_msg = (info >>\n\t\t\tDC_DC8051_DBG_ERR_INFO_SET_BY_8051_HOST_MSG_SHIFT)\n\t\t\t& DC_DC8051_DBG_ERR_INFO_SET_BY_8051_HOST_MSG_MASK;\n\n\t\t \n\t\tif (err & FAILED_LNI) {\n\t\t\t \n\t\t\tif (ppd->host_link_state\n\t\t\t    & (HLS_DN_POLL | HLS_VERIFY_CAP | HLS_GOING_UP)) {\n\t\t\t\tqueue_link_down = 1;\n\t\t\t\tdd_dev_info(dd, \"Link error: %s\\n\",\n\t\t\t\t\t    dc8051_info_err_string(buf,\n\t\t\t\t\t\t\t\t   sizeof(buf),\n\t\t\t\t\t\t\t\t   err &\n\t\t\t\t\t\t\t\t   FAILED_LNI));\n\t\t\t}\n\t\t\terr &= ~(u64)FAILED_LNI;\n\t\t}\n\t\t \n\t\tif (err & UNKNOWN_FRAME) {\n\t\t\tppd->unknown_frame_count++;\n\t\t\terr &= ~(u64)UNKNOWN_FRAME;\n\t\t}\n\t\tif (err) {\n\t\t\t \n\t\t\tdd_dev_err(dd, \"8051 info error: %s\\n\",\n\t\t\t\t   dc8051_info_err_string(buf, sizeof(buf),\n\t\t\t\t\t\t\t  err));\n\t\t}\n\n\t\t \n\t\tif (host_msg & HOST_REQ_DONE) {\n\t\t\t \n\t\t\thost_msg &= ~(u64)HOST_REQ_DONE;\n\t\t}\n\t\tif (host_msg & BC_SMA_MSG) {\n\t\t\tqueue_work(ppd->link_wq, &ppd->sma_message_work);\n\t\t\thost_msg &= ~(u64)BC_SMA_MSG;\n\t\t}\n\t\tif (host_msg & LINKUP_ACHIEVED) {\n\t\t\tdd_dev_info(dd, \"8051: Link up\\n\");\n\t\t\tqueue_work(ppd->link_wq, &ppd->link_up_work);\n\t\t\thost_msg &= ~(u64)LINKUP_ACHIEVED;\n\t\t}\n\t\tif (host_msg & EXT_DEVICE_CFG_REQ) {\n\t\t\thandle_8051_request(ppd);\n\t\t\thost_msg &= ~(u64)EXT_DEVICE_CFG_REQ;\n\t\t}\n\t\tif (host_msg & VERIFY_CAP_FRAME) {\n\t\t\tqueue_work(ppd->link_wq, &ppd->link_vc_work);\n\t\t\thost_msg &= ~(u64)VERIFY_CAP_FRAME;\n\t\t}\n\t\tif (host_msg & LINK_GOING_DOWN) {\n\t\t\tconst char *extra = \"\";\n\t\t\t \n\t\t\tif (host_msg & LINK_WIDTH_DOWNGRADED) {\n\t\t\t\thost_msg &= ~(u64)LINK_WIDTH_DOWNGRADED;\n\t\t\t\textra = \" (ignoring downgrade)\";\n\t\t\t}\n\t\t\tdd_dev_info(dd, \"8051: Link down%s\\n\", extra);\n\t\t\tqueue_link_down = 1;\n\t\t\thost_msg &= ~(u64)LINK_GOING_DOWN;\n\t\t}\n\t\tif (host_msg & LINK_WIDTH_DOWNGRADED) {\n\t\t\tqueue_work(ppd->link_wq, &ppd->link_downgrade_work);\n\t\t\thost_msg &= ~(u64)LINK_WIDTH_DOWNGRADED;\n\t\t}\n\t\tif (host_msg) {\n\t\t\t \n\t\t\tdd_dev_info(dd, \"8051 info host message: %s\\n\",\n\t\t\t\t    dc8051_info_host_msg_string(buf,\n\t\t\t\t\t\t\t\tsizeof(buf),\n\t\t\t\t\t\t\t\thost_msg));\n\t\t}\n\n\t\treg &= ~DC_DC8051_ERR_FLG_SET_BY_8051_SMASK;\n\t}\n\tif (reg & DC_DC8051_ERR_FLG_LOST_8051_HEART_BEAT_SMASK) {\n\t\t \n\t\tdd_dev_err(dd, \"Lost 8051 heartbeat\\n\");\n\t\twrite_csr(dd, DC_DC8051_ERR_EN,\n\t\t\t  read_csr(dd, DC_DC8051_ERR_EN) &\n\t\t\t  ~DC_DC8051_ERR_EN_LOST_8051_HEART_BEAT_SMASK);\n\n\t\treg &= ~DC_DC8051_ERR_FLG_LOST_8051_HEART_BEAT_SMASK;\n\t}\n\tif (reg) {\n\t\t \n\t\tdd_dev_err(dd, \"8051 error: %s\\n\",\n\t\t\t   dc8051_err_string(buf, sizeof(buf), reg));\n\t}\n\n\tif (queue_link_down) {\n\t\t \n\t\tif ((ppd->host_link_state &\n\t\t    (HLS_GOING_OFFLINE | HLS_LINK_COOLDOWN)) ||\n\t\t    ppd->link_enabled == 0) {\n\t\t\tdd_dev_info(dd, \"%s: not queuing link down. host_link_state %x, link_enabled %x\\n\",\n\t\t\t\t    __func__, ppd->host_link_state,\n\t\t\t\t    ppd->link_enabled);\n\t\t} else {\n\t\t\tif (xchg(&ppd->is_link_down_queued, 1) == 1)\n\t\t\t\tdd_dev_info(dd,\n\t\t\t\t\t    \"%s: link down request already queued\\n\",\n\t\t\t\t\t    __func__);\n\t\t\telse\n\t\t\t\tqueue_work(ppd->link_wq, &ppd->link_down_work);\n\t\t}\n\t}\n}\n\nstatic const char * const fm_config_txt[] = {\n[0] =\n\t\"BadHeadDist: Distance violation between two head flits\",\n[1] =\n\t\"BadTailDist: Distance violation between two tail flits\",\n[2] =\n\t\"BadCtrlDist: Distance violation between two credit control flits\",\n[3] =\n\t\"BadCrdAck: Credits return for unsupported VL\",\n[4] =\n\t\"UnsupportedVLMarker: Received VL Marker\",\n[5] =\n\t\"BadPreempt: Exceeded the preemption nesting level\",\n[6] =\n\t\"BadControlFlit: Received unsupported control flit\",\n \n[8] =\n\t\"UnsupportedVLMarker: Received VL Marker for unconfigured or disabled VL\",\n};\n\nstatic const char * const port_rcv_txt[] = {\n[1] =\n\t\"BadPktLen: Illegal PktLen\",\n[2] =\n\t\"PktLenTooLong: Packet longer than PktLen\",\n[3] =\n\t\"PktLenTooShort: Packet shorter than PktLen\",\n[4] =\n\t\"BadSLID: Illegal SLID (0, using multicast as SLID, does not include security validation of SLID)\",\n[5] =\n\t\"BadDLID: Illegal DLID (0, doesn't match HFI)\",\n[6] =\n\t\"BadL2: Illegal L2 opcode\",\n[7] =\n\t\"BadSC: Unsupported SC\",\n[9] =\n\t\"BadRC: Illegal RC\",\n[11] =\n\t\"PreemptError: Preempting with same VL\",\n[12] =\n\t\"PreemptVL15: Preempting a VL15 packet\",\n};\n\n#define OPA_LDR_FMCONFIG_OFFSET 16\n#define OPA_LDR_PORTRCV_OFFSET 0\nstatic void handle_dcc_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tu64 info, hdr0, hdr1;\n\tconst char *extra;\n\tchar buf[96];\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tu8 lcl_reason = 0;\n\tint do_bounce = 0;\n\n\tif (reg & DCC_ERR_FLG_UNCORRECTABLE_ERR_SMASK) {\n\t\tif (!(dd->err_info_uncorrectable & OPA_EI_STATUS_SMASK)) {\n\t\t\tinfo = read_csr(dd, DCC_ERR_INFO_UNCORRECTABLE);\n\t\t\tdd->err_info_uncorrectable = info & OPA_EI_CODE_SMASK;\n\t\t\t \n\t\t\tdd->err_info_uncorrectable |= OPA_EI_STATUS_SMASK;\n\t\t}\n\t\treg &= ~DCC_ERR_FLG_UNCORRECTABLE_ERR_SMASK;\n\t}\n\n\tif (reg & DCC_ERR_FLG_LINK_ERR_SMASK) {\n\t\tstruct hfi1_pportdata *ppd = dd->pport;\n\t\t \n\t\tif (ppd->link_downed < (u32)UINT_MAX)\n\t\t\tppd->link_downed++;\n\t\treg &= ~DCC_ERR_FLG_LINK_ERR_SMASK;\n\t}\n\n\tif (reg & DCC_ERR_FLG_FMCONFIG_ERR_SMASK) {\n\t\tu8 reason_valid = 1;\n\n\t\tinfo = read_csr(dd, DCC_ERR_INFO_FMCONFIG);\n\t\tif (!(dd->err_info_fmconfig & OPA_EI_STATUS_SMASK)) {\n\t\t\tdd->err_info_fmconfig = info & OPA_EI_CODE_SMASK;\n\t\t\t \n\t\t\tdd->err_info_fmconfig |= OPA_EI_STATUS_SMASK;\n\t\t}\n\t\tswitch (info) {\n\t\tcase 0:\n\t\tcase 1:\n\t\tcase 2:\n\t\tcase 3:\n\t\tcase 4:\n\t\tcase 5:\n\t\tcase 6:\n\t\t\textra = fm_config_txt[info];\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\textra = fm_config_txt[info];\n\t\t\tif (ppd->port_error_action &\n\t\t\t    OPA_PI_MASK_FM_CFG_UNSUPPORTED_VL_MARKER) {\n\t\t\t\tdo_bounce = 1;\n\t\t\t\t \n\t\t\t\tlcl_reason =\n\t\t\t\t  OPA_LINKDOWN_REASON_UNSUPPORTED_VL_MARKER;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treason_valid = 0;\n\t\t\tsnprintf(buf, sizeof(buf), \"reserved%lld\", info);\n\t\t\textra = buf;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (reason_valid && !do_bounce) {\n\t\t\tdo_bounce = ppd->port_error_action &\n\t\t\t\t\t(1 << (OPA_LDR_FMCONFIG_OFFSET + info));\n\t\t\tlcl_reason = info + OPA_LINKDOWN_REASON_BAD_HEAD_DIST;\n\t\t}\n\n\t\t \n\t\tdd_dev_info_ratelimited(dd, \"DCC Error: fmconfig error: %s\\n\",\n\t\t\t\t\textra);\n\t\treg &= ~DCC_ERR_FLG_FMCONFIG_ERR_SMASK;\n\t}\n\n\tif (reg & DCC_ERR_FLG_RCVPORT_ERR_SMASK) {\n\t\tu8 reason_valid = 1;\n\n\t\tinfo = read_csr(dd, DCC_ERR_INFO_PORTRCV);\n\t\thdr0 = read_csr(dd, DCC_ERR_INFO_PORTRCV_HDR0);\n\t\thdr1 = read_csr(dd, DCC_ERR_INFO_PORTRCV_HDR1);\n\t\tif (!(dd->err_info_rcvport.status_and_code &\n\t\t      OPA_EI_STATUS_SMASK)) {\n\t\t\tdd->err_info_rcvport.status_and_code =\n\t\t\t\tinfo & OPA_EI_CODE_SMASK;\n\t\t\t \n\t\t\tdd->err_info_rcvport.status_and_code |=\n\t\t\t\tOPA_EI_STATUS_SMASK;\n\t\t\t \n\t\t\tdd->err_info_rcvport.packet_flit1 = hdr0;\n\t\t\tdd->err_info_rcvport.packet_flit2 = hdr1;\n\t\t}\n\t\tswitch (info) {\n\t\tcase 1:\n\t\tcase 2:\n\t\tcase 3:\n\t\tcase 4:\n\t\tcase 5:\n\t\tcase 6:\n\t\tcase 7:\n\t\tcase 9:\n\t\tcase 11:\n\t\tcase 12:\n\t\t\textra = port_rcv_txt[info];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treason_valid = 0;\n\t\t\tsnprintf(buf, sizeof(buf), \"reserved%lld\", info);\n\t\t\textra = buf;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (reason_valid && !do_bounce) {\n\t\t\tdo_bounce = ppd->port_error_action &\n\t\t\t\t\t(1 << (OPA_LDR_PORTRCV_OFFSET + info));\n\t\t\tlcl_reason = info + OPA_LINKDOWN_REASON_RCV_ERROR_0;\n\t\t}\n\n\t\t \n\t\tdd_dev_info_ratelimited(dd, \"DCC Error: PortRcv error: %s\\n\"\n\t\t\t\t\t\"               hdr0 0x%llx, hdr1 0x%llx\\n\",\n\t\t\t\t\textra, hdr0, hdr1);\n\n\t\treg &= ~DCC_ERR_FLG_RCVPORT_ERR_SMASK;\n\t}\n\n\tif (reg & DCC_ERR_FLG_EN_CSR_ACCESS_BLOCKED_UC_SMASK) {\n\t\t \n\t\tdd_dev_info_ratelimited(dd, \"8051 access to LCB blocked\\n\");\n\t\treg &= ~DCC_ERR_FLG_EN_CSR_ACCESS_BLOCKED_UC_SMASK;\n\t}\n\tif (reg & DCC_ERR_FLG_EN_CSR_ACCESS_BLOCKED_HOST_SMASK) {\n\t\t \n\t\tdd_dev_info_ratelimited(dd, \"host access to LCB blocked\\n\");\n\t\treg &= ~DCC_ERR_FLG_EN_CSR_ACCESS_BLOCKED_HOST_SMASK;\n\t}\n\n\tif (unlikely(hfi1_dbg_fault_suppress_err(&dd->verbs_dev)))\n\t\treg &= ~DCC_ERR_FLG_LATE_EBP_ERR_SMASK;\n\n\t \n\tif (reg)\n\t\tdd_dev_info_ratelimited(dd, \"DCC Error: %s\\n\",\n\t\t\t\t\tdcc_err_string(buf, sizeof(buf), reg));\n\n\tif (lcl_reason == 0)\n\t\tlcl_reason = OPA_LINKDOWN_REASON_UNKNOWN;\n\n\tif (do_bounce) {\n\t\tdd_dev_info_ratelimited(dd, \"%s: PortErrorAction bounce\\n\",\n\t\t\t\t\t__func__);\n\t\tset_link_down_reason(ppd, lcl_reason, 0, lcl_reason);\n\t\tqueue_work(ppd->link_wq, &ppd->link_bounce_work);\n\t}\n}\n\nstatic void handle_lcb_err(struct hfi1_devdata *dd, u32 unused, u64 reg)\n{\n\tchar buf[96];\n\n\tdd_dev_info(dd, \"LCB Error: %s\\n\",\n\t\t    lcb_err_string(buf, sizeof(buf), reg));\n}\n\n \nstatic void is_dc_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tconst struct err_reg_info *eri = &dc_errs[source];\n\n\tif (eri->handler) {\n\t\tinterrupt_clear_down(dd, 0, eri);\n\t} else if (source == 3  ) {\n\t\t \n\t\tdd_dev_err(dd, \"Parity error in DC LBM block\\n\");\n\t} else {\n\t\tdd_dev_err(dd, \"Invalid DC interrupt %u\\n\", source);\n\t}\n}\n\n \nstatic void is_send_credit_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tsc_group_release_update(dd, source);\n}\n\n \nstatic void is_sdma_eng_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\t \n\tunsigned int what  = source / TXE_NUM_SDMA_ENGINES;\n\t \n\tunsigned int which = source % TXE_NUM_SDMA_ENGINES;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", which,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tsdma_dumpstate(&dd->per_sdma[which]);\n#endif\n\n\tif (likely(what < 3 && which < dd->num_sdma)) {\n\t\tsdma_engine_interrupt(&dd->per_sdma[which], 1ull << source);\n\t} else {\n\t\t \n\t\tdd_dev_err(dd, \"Invalid SDMA interrupt 0x%x\\n\", source);\n\t}\n}\n\n \nstatic void is_rcv_avail_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tstruct hfi1_ctxtdata *rcd;\n\tchar *err_detail;\n\n\tif (likely(source < dd->num_rcv_contexts)) {\n\t\trcd = hfi1_rcd_get_by_index(dd, source);\n\t\tif (rcd) {\n\t\t\thandle_user_interrupt(rcd);\n\t\t\thfi1_rcd_put(rcd);\n\t\t\treturn;\t \n\t\t}\n\t\t \n\t\terr_detail = \"dataless\";\n\t} else {\n\t\t \n\t\terr_detail = \"out of range\";\n\t}\n\tdd_dev_err(dd, \"unexpected %s receive available context interrupt %u\\n\",\n\t\t   err_detail, source);\n}\n\n \nstatic void is_rcv_urgent_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tstruct hfi1_ctxtdata *rcd;\n\tchar *err_detail;\n\n\tif (likely(source < dd->num_rcv_contexts)) {\n\t\trcd = hfi1_rcd_get_by_index(dd, source);\n\t\tif (rcd) {\n\t\t\thandle_user_interrupt(rcd);\n\t\t\thfi1_rcd_put(rcd);\n\t\t\treturn;\t \n\t\t}\n\t\t \n\t\terr_detail = \"dataless\";\n\t} else {\n\t\t \n\t\terr_detail = \"out of range\";\n\t}\n\tdd_dev_err(dd, \"unexpected %s receive urgent context interrupt %u\\n\",\n\t\t   err_detail, source);\n}\n\n \nstatic void is_reserved_int(struct hfi1_devdata *dd, unsigned int source)\n{\n\tchar name[64];\n\n\tdd_dev_err(dd, \"unexpected %s interrupt\\n\",\n\t\t   is_reserved_name(name, sizeof(name), source));\n}\n\nstatic const struct is_table is_table[] = {\n \n{ IS_GENERAL_ERR_START,  IS_GENERAL_ERR_END,\n\t\t\t\tis_misc_err_name,\tis_misc_err_int },\n{ IS_SDMAENG_ERR_START,  IS_SDMAENG_ERR_END,\n\t\t\t\tis_sdma_eng_err_name,\tis_sdma_eng_err_int },\n{ IS_SENDCTXT_ERR_START, IS_SENDCTXT_ERR_END,\n\t\t\t\tis_sendctxt_err_name,\tis_sendctxt_err_int },\n{ IS_SDMA_START,\t     IS_SDMA_IDLE_END,\n\t\t\t\tis_sdma_eng_name,\tis_sdma_eng_int },\n{ IS_VARIOUS_START,\t     IS_VARIOUS_END,\n\t\t\t\tis_various_name,\tis_various_int },\n{ IS_DC_START,\t     IS_DC_END,\n\t\t\t\tis_dc_name,\t\tis_dc_int },\n{ IS_RCVAVAIL_START,     IS_RCVAVAIL_END,\n\t\t\t\tis_rcv_avail_name,\tis_rcv_avail_int },\n{ IS_RCVURGENT_START,    IS_RCVURGENT_END,\n\t\t\t\tis_rcv_urgent_name,\tis_rcv_urgent_int },\n{ IS_SENDCREDIT_START,   IS_SENDCREDIT_END,\n\t\t\t\tis_send_credit_name,\tis_send_credit_int},\n{ IS_RESERVED_START,     IS_RESERVED_END,\n\t\t\t\tis_reserved_name,\tis_reserved_int},\n};\n\n \nstatic void is_interrupt(struct hfi1_devdata *dd, unsigned int source)\n{\n\tconst struct is_table *entry;\n\n\t \n\tfor (entry = &is_table[0]; entry->is_name; entry++) {\n\t\tif (source <= entry->end) {\n\t\t\ttrace_hfi1_interrupt(dd, entry, source);\n\t\t\tentry->is_int(dd, source - entry->start);\n\t\t\treturn;\n\t\t}\n\t}\n\t \n\tdd_dev_err(dd, \"invalid interrupt source %u\\n\", source);\n}\n\n \nirqreturn_t general_interrupt(int irq, void *data)\n{\n\tstruct hfi1_devdata *dd = data;\n\tu64 regs[CCE_NUM_INT_CSRS];\n\tu32 bit;\n\tint i;\n\tirqreturn_t handled = IRQ_NONE;\n\n\tthis_cpu_inc(*dd->int_counter);\n\n\t \n\tfor (i = 0; i < CCE_NUM_INT_CSRS; i++) {\n\t\tif (dd->gi_mask[i] == 0) {\n\t\t\tregs[i] = 0;\t \n\t\t\tcontinue;\n\t\t}\n\t\tregs[i] = read_csr(dd, CCE_INT_STATUS + (8 * i)) &\n\t\t\t\tdd->gi_mask[i];\n\t\t \n\t\tif (regs[i])\n\t\t\twrite_csr(dd, CCE_INT_CLEAR + (8 * i), regs[i]);\n\t}\n\n\t \n\tfor_each_set_bit(bit, (unsigned long *)&regs[0],\n\t\t\t CCE_NUM_INT_CSRS * 64) {\n\t\tis_interrupt(dd, bit);\n\t\thandled = IRQ_HANDLED;\n\t}\n\n\treturn handled;\n}\n\nirqreturn_t sdma_interrupt(int irq, void *data)\n{\n\tstruct sdma_engine *sde = data;\n\tstruct hfi1_devdata *dd = sde->dd;\n\tu64 status;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tsdma_dumpstate(sde);\n#endif\n\n\tthis_cpu_inc(*dd->int_counter);\n\n\t \n\tstatus = read_csr(dd,\n\t\t\t  CCE_INT_STATUS + (8 * (IS_SDMA_START / 64)))\n\t\t\t  & sde->imask;\n\tif (likely(status)) {\n\t\t \n\t\twrite_csr(dd,\n\t\t\t  CCE_INT_CLEAR + (8 * (IS_SDMA_START / 64)),\n\t\t\t  status);\n\n\t\t \n\t\tsdma_engine_interrupt(sde, status);\n\t} else {\n\t\tdd_dev_info_ratelimited(dd, \"SDMA engine %u interrupt, but no status bits set\\n\",\n\t\t\t\t\tsde->this_idx);\n\t}\n\treturn IRQ_HANDLED;\n}\n\n \nstatic inline void clear_recv_intr(struct hfi1_ctxtdata *rcd)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu32 addr = CCE_INT_CLEAR + (8 * rcd->ireg);\n\n\twrite_csr(dd, addr, rcd->imask);\n\t \n\t(void)read_csr(dd, addr);\n}\n\n \nvoid force_recv_intr(struct hfi1_ctxtdata *rcd)\n{\n\twrite_csr(rcd->dd, CCE_INT_FORCE + (8 * rcd->ireg), rcd->imask);\n}\n\n \nstatic inline int check_packet_present(struct hfi1_ctxtdata *rcd)\n{\n\tu32 tail;\n\n\tif (hfi1_packet_present(rcd))\n\t\treturn 1;\n\n\t \n\ttail = (u32)read_uctxt_csr(rcd->dd, rcd->ctxt, RCV_HDR_TAIL);\n\treturn hfi1_rcd_head(rcd) != tail;\n}\n\n \nstatic void receive_interrupt_common(struct hfi1_ctxtdata *rcd)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\n\ttrace_hfi1_receive_interrupt(dd, rcd);\n\tthis_cpu_inc(*dd->int_counter);\n\taspm_ctx_disable(rcd);\n}\n\n \nstatic void __hfi1_rcd_eoi_intr(struct hfi1_ctxtdata *rcd)\n{\n\tif (!rcd->rcvhdrq)\n\t\treturn;\n\tclear_recv_intr(rcd);\n\tif (check_packet_present(rcd))\n\t\tforce_recv_intr(rcd);\n}\n\n \nstatic void hfi1_rcd_eoi_intr(struct hfi1_ctxtdata *rcd)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__hfi1_rcd_eoi_intr(rcd);\n\tlocal_irq_restore(flags);\n}\n\n \nint hfi1_netdev_rx_napi(struct napi_struct *napi, int budget)\n{\n\tstruct hfi1_netdev_rxq *rxq = container_of(napi,\n\t\t\tstruct hfi1_netdev_rxq, napi);\n\tstruct hfi1_ctxtdata *rcd = rxq->rcd;\n\tint work_done = 0;\n\n\twork_done = rcd->do_interrupt(rcd, budget);\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\thfi1_rcd_eoi_intr(rcd);\n\t}\n\n\treturn work_done;\n}\n\n \nirqreturn_t receive_context_interrupt_napi(int irq, void *data)\n{\n\tstruct hfi1_ctxtdata *rcd = data;\n\n\treceive_interrupt_common(rcd);\n\n\tif (likely(rcd->napi)) {\n\t\tif (likely(napi_schedule_prep(rcd->napi)))\n\t\t\t__napi_schedule_irqoff(rcd->napi);\n\t\telse\n\t\t\t__hfi1_rcd_eoi_intr(rcd);\n\t} else {\n\t\tWARN_ONCE(1, \"Napi IRQ handler without napi set up ctxt=%d\\n\",\n\t\t\t  rcd->ctxt);\n\t\t__hfi1_rcd_eoi_intr(rcd);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nirqreturn_t receive_context_interrupt(int irq, void *data)\n{\n\tstruct hfi1_ctxtdata *rcd = data;\n\tint disposition;\n\n\treceive_interrupt_common(rcd);\n\n\t \n\tdisposition = rcd->do_interrupt(rcd, 0);\n\n\t \n\tif (disposition == RCV_PKT_LIMIT)\n\t\treturn IRQ_WAKE_THREAD;\n\n\t__hfi1_rcd_eoi_intr(rcd);\n\treturn IRQ_HANDLED;\n}\n\n \nirqreturn_t receive_context_thread(int irq, void *data)\n{\n\tstruct hfi1_ctxtdata *rcd = data;\n\n\t \n\t(void)rcd->do_interrupt(rcd, 1);\n\n\thfi1_rcd_eoi_intr(rcd);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nu32 read_physical_state(struct hfi1_devdata *dd)\n{\n\tu64 reg;\n\n\treg = read_csr(dd, DC_DC8051_STS_CUR_STATE);\n\treturn (reg >> DC_DC8051_STS_CUR_STATE_PORT_SHIFT)\n\t\t\t\t& DC_DC8051_STS_CUR_STATE_PORT_MASK;\n}\n\nu32 read_logical_state(struct hfi1_devdata *dd)\n{\n\tu64 reg;\n\n\treg = read_csr(dd, DCC_CFG_PORT_CONFIG);\n\treturn (reg >> DCC_CFG_PORT_CONFIG_LINK_STATE_SHIFT)\n\t\t\t\t& DCC_CFG_PORT_CONFIG_LINK_STATE_MASK;\n}\n\nstatic void set_logical_state(struct hfi1_devdata *dd, u32 chip_lstate)\n{\n\tu64 reg;\n\n\treg = read_csr(dd, DCC_CFG_PORT_CONFIG);\n\t \n\treg &= ~DCC_CFG_PORT_CONFIG_LINK_STATE_SMASK;\n\treg |= (u64)chip_lstate << DCC_CFG_PORT_CONFIG_LINK_STATE_SHIFT;\n\twrite_csr(dd, DCC_CFG_PORT_CONFIG, reg);\n}\n\n \nstatic int read_lcb_via_8051(struct hfi1_devdata *dd, u32 addr, u64 *data)\n{\n\tu32 regno;\n\tint ret;\n\n\tif (dd->icode == ICODE_FUNCTIONAL_SIMULATOR) {\n\t\tif (acquire_lcb_access(dd, 0) == 0) {\n\t\t\t*data = read_csr(dd, addr);\n\t\t\trelease_lcb_access(dd, 0);\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tregno = (addr - DC_LCB_CFG_RUN) >> 3;\n\tret = do_8051_command(dd, HCMD_READ_LCB_CSR, regno, data);\n\tif (ret != HCMD_SUCCESS)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n \nstruct lcb_datum {\n\tu32 off;\n\tu64 val;\n};\n\nstatic struct lcb_datum lcb_cache[] = {\n\t{ DC_LCB_ERR_INFO_RX_REPLAY_CNT, 0},\n\t{ DC_LCB_ERR_INFO_SEQ_CRC_CNT, 0 },\n\t{ DC_LCB_ERR_INFO_REINIT_FROM_PEER_CNT, 0 },\n};\n\nstatic void update_lcb_cache(struct hfi1_devdata *dd)\n{\n\tint i;\n\tint ret;\n\tu64 val;\n\n\tfor (i = 0; i < ARRAY_SIZE(lcb_cache); i++) {\n\t\tret = read_lcb_csr(dd, lcb_cache[i].off, &val);\n\n\t\t \n\t\tif (likely(ret != -EBUSY))\n\t\t\tlcb_cache[i].val = val;\n\t}\n}\n\nstatic int read_lcb_cache(u32 off, u64 *val)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(lcb_cache); i++) {\n\t\tif (lcb_cache[i].off == off) {\n\t\t\t*val = lcb_cache[i].val;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpr_warn(\"%s bad offset 0x%x\\n\", __func__, off);\n\treturn -1;\n}\n\n \nint read_lcb_csr(struct hfi1_devdata *dd, u32 addr, u64 *data)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\n\t \n\tif (ppd->host_link_state & HLS_UP)\n\t\treturn read_lcb_via_8051(dd, addr, data);\n\t \n\tif (ppd->host_link_state & (HLS_GOING_UP | HLS_GOING_OFFLINE)) {\n\t\tif (read_lcb_cache(addr, data))\n\t\t\treturn -EBUSY;\n\t\treturn 0;\n\t}\n\n\t \n\t*data = read_csr(dd, addr);\n\treturn 0;\n}\n\n \nstatic int write_lcb_via_8051(struct hfi1_devdata *dd, u32 addr, u64 data)\n{\n\tu32 regno;\n\tint ret;\n\n\tif (dd->icode == ICODE_FUNCTIONAL_SIMULATOR ||\n\t    (dd->dc8051_ver < dc8051_ver(0, 20, 0))) {\n\t\tif (acquire_lcb_access(dd, 0) == 0) {\n\t\t\twrite_csr(dd, addr, data);\n\t\t\trelease_lcb_access(dd, 0);\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tregno = (addr - DC_LCB_CFG_RUN) >> 3;\n\tret = do_8051_command(dd, HCMD_WRITE_LCB_CSR, regno, &data);\n\tif (ret != HCMD_SUCCESS)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n \nint write_lcb_csr(struct hfi1_devdata *dd, u32 addr, u64 data)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\n\t \n\tif (ppd->host_link_state & HLS_UP)\n\t\treturn write_lcb_via_8051(dd, addr, data);\n\t \n\tif (ppd->host_link_state & (HLS_GOING_UP | HLS_GOING_OFFLINE))\n\t\treturn -EBUSY;\n\t \n\twrite_csr(dd, addr, data);\n\treturn 0;\n}\n\n \nstatic int do_8051_command(struct hfi1_devdata *dd, u32 type, u64 in_data,\n\t\t\t   u64 *out_data)\n{\n\tu64 reg, completed;\n\tint return_code;\n\tunsigned long timeout;\n\n\thfi1_cdbg(DC8051, \"type %d, data 0x%012llx\", type, in_data);\n\n\tmutex_lock(&dd->dc8051_lock);\n\n\t \n\tif (dd->dc_shutdown) {\n\t\treturn_code = -ENODEV;\n\t\tgoto fail;\n\t}\n\n\t \n\tif (dd->dc8051_timed_out) {\n\t\tif (dd->dc8051_timed_out > 1) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Previous 8051 host command timed out, skipping command %u\\n\",\n\t\t\t\t   type);\n\t\t\treturn_code = -ENXIO;\n\t\t\tgoto fail;\n\t\t}\n\t\t_dc_shutdown(dd);\n\t\t_dc_start(dd);\n\t}\n\n\t \n\n\t \n\tif (type == HCMD_WRITE_LCB_CSR) {\n\t\tin_data |= ((*out_data) & 0xffffffffffull) << 8;\n\t\t \n\t\treg = read_csr(dd, DC_DC8051_CFG_EXT_DEV_0);\n\t\treg &= DC_DC8051_CFG_EXT_DEV_0_COMPLETED_SMASK;\n\t\treg |= ((((*out_data) >> 40) & 0xff) <<\n\t\t\t\tDC_DC8051_CFG_EXT_DEV_0_RETURN_CODE_SHIFT)\n\t\t      | ((((*out_data) >> 48) & 0xffff) <<\n\t\t\t\tDC_DC8051_CFG_EXT_DEV_0_RSP_DATA_SHIFT);\n\t\twrite_csr(dd, DC_DC8051_CFG_EXT_DEV_0, reg);\n\t}\n\n\t \n\treg = ((u64)type & DC_DC8051_CFG_HOST_CMD_0_REQ_TYPE_MASK)\n\t\t\t<< DC_DC8051_CFG_HOST_CMD_0_REQ_TYPE_SHIFT\n\t\t| (in_data & DC_DC8051_CFG_HOST_CMD_0_REQ_DATA_MASK)\n\t\t\t<< DC_DC8051_CFG_HOST_CMD_0_REQ_DATA_SHIFT;\n\twrite_csr(dd, DC_DC8051_CFG_HOST_CMD_0, reg);\n\treg |= DC_DC8051_CFG_HOST_CMD_0_REQ_NEW_SMASK;\n\twrite_csr(dd, DC_DC8051_CFG_HOST_CMD_0, reg);\n\n\t \n\ttimeout = jiffies + msecs_to_jiffies(DC8051_COMMAND_TIMEOUT);\n\twhile (1) {\n\t\treg = read_csr(dd, DC_DC8051_CFG_HOST_CMD_1);\n\t\tcompleted = reg & DC_DC8051_CFG_HOST_CMD_1_COMPLETED_SMASK;\n\t\tif (completed)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd->dc8051_timed_out++;\n\t\t\tdd_dev_err(dd, \"8051 host command %u timeout\\n\", type);\n\t\t\tif (out_data)\n\t\t\t\t*out_data = 0;\n\t\t\treturn_code = -ETIMEDOUT;\n\t\t\tgoto fail;\n\t\t}\n\t\tudelay(2);\n\t}\n\n\tif (out_data) {\n\t\t*out_data = (reg >> DC_DC8051_CFG_HOST_CMD_1_RSP_DATA_SHIFT)\n\t\t\t\t& DC_DC8051_CFG_HOST_CMD_1_RSP_DATA_MASK;\n\t\tif (type == HCMD_READ_LCB_CSR) {\n\t\t\t \n\t\t\t*out_data |= (read_csr(dd, DC_DC8051_CFG_EXT_DEV_1)\n\t\t\t\t& DC_DC8051_CFG_EXT_DEV_1_REQ_DATA_SMASK)\n\t\t\t\t<< (48\n\t\t\t\t    - DC_DC8051_CFG_EXT_DEV_1_REQ_DATA_SHIFT);\n\t\t}\n\t}\n\treturn_code = (reg >> DC_DC8051_CFG_HOST_CMD_1_RETURN_CODE_SHIFT)\n\t\t\t\t& DC_DC8051_CFG_HOST_CMD_1_RETURN_CODE_MASK;\n\tdd->dc8051_timed_out = 0;\n\t \n\twrite_csr(dd, DC_DC8051_CFG_HOST_CMD_0, 0);\n\nfail:\n\tmutex_unlock(&dd->dc8051_lock);\n\treturn return_code;\n}\n\nstatic int set_physical_link_state(struct hfi1_devdata *dd, u64 state)\n{\n\treturn do_8051_command(dd, HCMD_CHANGE_PHY_STATE, state, NULL);\n}\n\nint load_8051_config(struct hfi1_devdata *dd, u8 field_id,\n\t\t     u8 lane_id, u32 config_data)\n{\n\tu64 data;\n\tint ret;\n\n\tdata = (u64)field_id << LOAD_DATA_FIELD_ID_SHIFT\n\t\t| (u64)lane_id << LOAD_DATA_LANE_ID_SHIFT\n\t\t| (u64)config_data << LOAD_DATA_DATA_SHIFT;\n\tret = do_8051_command(dd, HCMD_LOAD_CONFIG_DATA, data, NULL);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"load 8051 config: field id %d, lane %d, err %d\\n\",\n\t\t\t   (int)field_id, (int)lane_id, ret);\n\t}\n\treturn ret;\n}\n\n \nint read_8051_config(struct hfi1_devdata *dd, u8 field_id, u8 lane_id,\n\t\t     u32 *result)\n{\n\tu64 big_data;\n\tu32 addr;\n\tint ret;\n\n\t \n\tif (lane_id < 4)\n\t\taddr = (4 * NUM_GENERAL_FIELDS)\n\t\t\t+ (lane_id * 4 * NUM_LANE_FIELDS);\n\telse\n\t\taddr = 0;\n\taddr += field_id * 4;\n\n\t \n\tret = read_8051_data(dd, addr, 8, &big_data);\n\n\tif (ret == 0) {\n\t\t \n\t\tif (addr & 0x4)\n\t\t\t*result = (u32)(big_data >> 32);\n\t\telse\n\t\t\t*result = (u32)big_data;\n\t} else {\n\t\t*result = 0;\n\t\tdd_dev_err(dd, \"%s: direct read failed, lane %d, field %d!\\n\",\n\t\t\t   __func__, lane_id, field_id);\n\t}\n\n\treturn ret;\n}\n\nstatic int write_vc_local_phy(struct hfi1_devdata *dd, u8 power_management,\n\t\t\t      u8 continuous)\n{\n\tu32 frame;\n\n\tframe = continuous << CONTINIOUS_REMOTE_UPDATE_SUPPORT_SHIFT\n\t\t| power_management << POWER_MANAGEMENT_SHIFT;\n\treturn load_8051_config(dd, VERIFY_CAP_LOCAL_PHY,\n\t\t\t\tGENERAL_CONFIG, frame);\n}\n\nstatic int write_vc_local_fabric(struct hfi1_devdata *dd, u8 vau, u8 z, u8 vcu,\n\t\t\t\t u16 vl15buf, u8 crc_sizes)\n{\n\tu32 frame;\n\n\tframe = (u32)vau << VAU_SHIFT\n\t\t| (u32)z << Z_SHIFT\n\t\t| (u32)vcu << VCU_SHIFT\n\t\t| (u32)vl15buf << VL15BUF_SHIFT\n\t\t| (u32)crc_sizes << CRC_SIZES_SHIFT;\n\treturn load_8051_config(dd, VERIFY_CAP_LOCAL_FABRIC,\n\t\t\t\tGENERAL_CONFIG, frame);\n}\n\nstatic void read_vc_local_link_mode(struct hfi1_devdata *dd, u8 *misc_bits,\n\t\t\t\t    u8 *flag_bits, u16 *link_widths)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, VERIFY_CAP_LOCAL_LINK_MODE, GENERAL_CONFIG,\n\t\t\t &frame);\n\t*misc_bits = (frame >> MISC_CONFIG_BITS_SHIFT) & MISC_CONFIG_BITS_MASK;\n\t*flag_bits = (frame >> LOCAL_FLAG_BITS_SHIFT) & LOCAL_FLAG_BITS_MASK;\n\t*link_widths = (frame >> LINK_WIDTH_SHIFT) & LINK_WIDTH_MASK;\n}\n\nstatic int write_vc_local_link_mode(struct hfi1_devdata *dd,\n\t\t\t\t    u8 misc_bits,\n\t\t\t\t    u8 flag_bits,\n\t\t\t\t    u16 link_widths)\n{\n\tu32 frame;\n\n\tframe = (u32)misc_bits << MISC_CONFIG_BITS_SHIFT\n\t\t| (u32)flag_bits << LOCAL_FLAG_BITS_SHIFT\n\t\t| (u32)link_widths << LINK_WIDTH_SHIFT;\n\treturn load_8051_config(dd, VERIFY_CAP_LOCAL_LINK_MODE, GENERAL_CONFIG,\n\t\t     frame);\n}\n\nstatic int write_local_device_id(struct hfi1_devdata *dd, u16 device_id,\n\t\t\t\t u8 device_rev)\n{\n\tu32 frame;\n\n\tframe = ((u32)device_id << LOCAL_DEVICE_ID_SHIFT)\n\t\t| ((u32)device_rev << LOCAL_DEVICE_REV_SHIFT);\n\treturn load_8051_config(dd, LOCAL_DEVICE_ID, GENERAL_CONFIG, frame);\n}\n\nstatic void read_remote_device_id(struct hfi1_devdata *dd, u16 *device_id,\n\t\t\t\t  u8 *device_rev)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, REMOTE_DEVICE_ID, GENERAL_CONFIG, &frame);\n\t*device_id = (frame >> REMOTE_DEVICE_ID_SHIFT) & REMOTE_DEVICE_ID_MASK;\n\t*device_rev = (frame >> REMOTE_DEVICE_REV_SHIFT)\n\t\t\t& REMOTE_DEVICE_REV_MASK;\n}\n\nint write_host_interface_version(struct hfi1_devdata *dd, u8 version)\n{\n\tu32 frame;\n\tu32 mask;\n\n\tmask = (HOST_INTERFACE_VERSION_MASK << HOST_INTERFACE_VERSION_SHIFT);\n\tread_8051_config(dd, RESERVED_REGISTERS, GENERAL_CONFIG, &frame);\n\t \n\tframe &= ~mask;\n\tframe |= ((u32)version << HOST_INTERFACE_VERSION_SHIFT);\n\treturn load_8051_config(dd, RESERVED_REGISTERS, GENERAL_CONFIG,\n\t\t\t\tframe);\n}\n\nvoid read_misc_status(struct hfi1_devdata *dd, u8 *ver_major, u8 *ver_minor,\n\t\t      u8 *ver_patch)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, MISC_STATUS, GENERAL_CONFIG, &frame);\n\t*ver_major = (frame >> STS_FM_VERSION_MAJOR_SHIFT) &\n\t\tSTS_FM_VERSION_MAJOR_MASK;\n\t*ver_minor = (frame >> STS_FM_VERSION_MINOR_SHIFT) &\n\t\tSTS_FM_VERSION_MINOR_MASK;\n\n\tread_8051_config(dd, VERSION_PATCH, GENERAL_CONFIG, &frame);\n\t*ver_patch = (frame >> STS_FM_VERSION_PATCH_SHIFT) &\n\t\tSTS_FM_VERSION_PATCH_MASK;\n}\n\nstatic void read_vc_remote_phy(struct hfi1_devdata *dd, u8 *power_management,\n\t\t\t       u8 *continuous)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, VERIFY_CAP_REMOTE_PHY, GENERAL_CONFIG, &frame);\n\t*power_management = (frame >> POWER_MANAGEMENT_SHIFT)\n\t\t\t\t\t& POWER_MANAGEMENT_MASK;\n\t*continuous = (frame >> CONTINIOUS_REMOTE_UPDATE_SUPPORT_SHIFT)\n\t\t\t\t\t& CONTINIOUS_REMOTE_UPDATE_SUPPORT_MASK;\n}\n\nstatic void read_vc_remote_fabric(struct hfi1_devdata *dd, u8 *vau, u8 *z,\n\t\t\t\t  u8 *vcu, u16 *vl15buf, u8 *crc_sizes)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, VERIFY_CAP_REMOTE_FABRIC, GENERAL_CONFIG, &frame);\n\t*vau = (frame >> VAU_SHIFT) & VAU_MASK;\n\t*z = (frame >> Z_SHIFT) & Z_MASK;\n\t*vcu = (frame >> VCU_SHIFT) & VCU_MASK;\n\t*vl15buf = (frame >> VL15BUF_SHIFT) & VL15BUF_MASK;\n\t*crc_sizes = (frame >> CRC_SIZES_SHIFT) & CRC_SIZES_MASK;\n}\n\nstatic void read_vc_remote_link_width(struct hfi1_devdata *dd,\n\t\t\t\t      u8 *remote_tx_rate,\n\t\t\t\t      u16 *link_widths)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, VERIFY_CAP_REMOTE_LINK_WIDTH, GENERAL_CONFIG,\n\t\t\t &frame);\n\t*remote_tx_rate = (frame >> REMOTE_TX_RATE_SHIFT)\n\t\t\t\t& REMOTE_TX_RATE_MASK;\n\t*link_widths = (frame >> LINK_WIDTH_SHIFT) & LINK_WIDTH_MASK;\n}\n\nstatic void read_local_lni(struct hfi1_devdata *dd, u8 *enable_lane_rx)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, LOCAL_LNI_INFO, GENERAL_CONFIG, &frame);\n\t*enable_lane_rx = (frame >> ENABLE_LANE_RX_SHIFT) & ENABLE_LANE_RX_MASK;\n}\n\nstatic void read_last_local_state(struct hfi1_devdata *dd, u32 *lls)\n{\n\tread_8051_config(dd, LAST_LOCAL_STATE_COMPLETE, GENERAL_CONFIG, lls);\n}\n\nstatic void read_last_remote_state(struct hfi1_devdata *dd, u32 *lrs)\n{\n\tread_8051_config(dd, LAST_REMOTE_STATE_COMPLETE, GENERAL_CONFIG, lrs);\n}\n\nvoid hfi1_read_link_quality(struct hfi1_devdata *dd, u8 *link_quality)\n{\n\tu32 frame;\n\tint ret;\n\n\t*link_quality = 0;\n\tif (dd->pport->host_link_state & HLS_UP) {\n\t\tret = read_8051_config(dd, LINK_QUALITY_INFO, GENERAL_CONFIG,\n\t\t\t\t       &frame);\n\t\tif (ret == 0)\n\t\t\t*link_quality = (frame >> LINK_QUALITY_SHIFT)\n\t\t\t\t\t\t& LINK_QUALITY_MASK;\n\t}\n}\n\nstatic void read_planned_down_reason_code(struct hfi1_devdata *dd, u8 *pdrrc)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, LINK_QUALITY_INFO, GENERAL_CONFIG, &frame);\n\t*pdrrc = (frame >> DOWN_REMOTE_REASON_SHIFT) & DOWN_REMOTE_REASON_MASK;\n}\n\nstatic void read_link_down_reason(struct hfi1_devdata *dd, u8 *ldr)\n{\n\tu32 frame;\n\n\tread_8051_config(dd, LINK_DOWN_REASON, GENERAL_CONFIG, &frame);\n\t*ldr = (frame & 0xff);\n}\n\nstatic int read_tx_settings(struct hfi1_devdata *dd,\n\t\t\t    u8 *enable_lane_tx,\n\t\t\t    u8 *tx_polarity_inversion,\n\t\t\t    u8 *rx_polarity_inversion,\n\t\t\t    u8 *max_rate)\n{\n\tu32 frame;\n\tint ret;\n\n\tret = read_8051_config(dd, TX_SETTINGS, GENERAL_CONFIG, &frame);\n\t*enable_lane_tx = (frame >> ENABLE_LANE_TX_SHIFT)\n\t\t\t\t& ENABLE_LANE_TX_MASK;\n\t*tx_polarity_inversion = (frame >> TX_POLARITY_INVERSION_SHIFT)\n\t\t\t\t& TX_POLARITY_INVERSION_MASK;\n\t*rx_polarity_inversion = (frame >> RX_POLARITY_INVERSION_SHIFT)\n\t\t\t\t& RX_POLARITY_INVERSION_MASK;\n\t*max_rate = (frame >> MAX_RATE_SHIFT) & MAX_RATE_MASK;\n\treturn ret;\n}\n\nstatic int write_tx_settings(struct hfi1_devdata *dd,\n\t\t\t     u8 enable_lane_tx,\n\t\t\t     u8 tx_polarity_inversion,\n\t\t\t     u8 rx_polarity_inversion,\n\t\t\t     u8 max_rate)\n{\n\tu32 frame;\n\n\t \n\tframe = enable_lane_tx << ENABLE_LANE_TX_SHIFT\n\t\t| tx_polarity_inversion << TX_POLARITY_INVERSION_SHIFT\n\t\t| rx_polarity_inversion << RX_POLARITY_INVERSION_SHIFT\n\t\t| max_rate << MAX_RATE_SHIFT;\n\treturn load_8051_config(dd, TX_SETTINGS, GENERAL_CONFIG, frame);\n}\n\n \nstatic int read_idle_message(struct hfi1_devdata *dd, u64 type, u64 *data_out)\n{\n\tint ret;\n\n\tret = do_8051_command(dd, HCMD_READ_LCB_IDLE_MSG, type, data_out);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd, \"read idle message: type %d, err %d\\n\",\n\t\t\t   (u32)type, ret);\n\t\treturn -EINVAL;\n\t}\n\tdd_dev_info(dd, \"%s: read idle message 0x%llx\\n\", __func__, *data_out);\n\t \n\t*data_out >>= IDLE_PAYLOAD_SHIFT;\n\treturn 0;\n}\n\n \nstatic int read_idle_sma(struct hfi1_devdata *dd, u64 *data)\n{\n\treturn read_idle_message(dd, (u64)IDLE_SMA << IDLE_MSG_TYPE_SHIFT,\n\t\t\t\t data);\n}\n\n \nstatic int send_idle_message(struct hfi1_devdata *dd, u64 data)\n{\n\tint ret;\n\n\tdd_dev_info(dd, \"%s: sending idle message 0x%llx\\n\", __func__, data);\n\tret = do_8051_command(dd, HCMD_SEND_LCB_IDLE_MSG, data, NULL);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd, \"send idle message: data 0x%llx, err %d\\n\",\n\t\t\t   data, ret);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nint send_idle_sma(struct hfi1_devdata *dd, u64 message)\n{\n\tu64 data;\n\n\tdata = ((message & IDLE_PAYLOAD_MASK) << IDLE_PAYLOAD_SHIFT) |\n\t\t((u64)IDLE_SMA << IDLE_MSG_TYPE_SHIFT);\n\treturn send_idle_message(dd, data);\n}\n\n \nstatic int do_quick_linkup(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\tlcb_shutdown(dd, 0);\n\n\tif (loopback) {\n\t\t \n\t\t \n\t\twrite_csr(dd, DC_LCB_CFG_LOOPBACK,\n\t\t\t  IB_PACKET_TYPE << DC_LCB_CFG_LOOPBACK_VAL_SHIFT);\n\t\twrite_csr(dd, DC_LCB_CFG_LANE_WIDTH, 0);\n\t}\n\n\t \n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 0);\n\n\t \n\tif (loopback && dd->icode == ICODE_FUNCTIONAL_SIMULATOR) {\n\t\t \n\t\twrite_csr(dd, DC_LCB_CFG_RUN,\n\t\t\t  1ull << DC_LCB_CFG_RUN_EN_SHIFT);\n\n\t\tret = wait_link_transfer_active(dd, 10);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\twrite_csr(dd, DC_LCB_CFG_ALLOW_LINK_UP,\n\t\t\t  1ull << DC_LCB_CFG_ALLOW_LINK_UP_VAL_SHIFT);\n\t}\n\n\tif (!loopback) {\n\t\t \n\t\tdd_dev_err(dd,\n\t\t\t   \"Pausing for peer to be finished with LCB set up\\n\");\n\t\tmsleep(5000);\n\t\tdd_dev_err(dd, \"Continuing with quick linkup\\n\");\n\t}\n\n\twrite_csr(dd, DC_LCB_ERR_EN, 0);  \n\tset_8051_lcb_access(dd);\n\n\t \n\tret = set_physical_link_state(dd, PLS_QUICK_LINKUP);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"%s: set physical link state to quick LinkUp failed with return %d\\n\",\n\t\t\t   __func__, ret);\n\n\t\tset_host_lcb_access(dd);\n\t\twrite_csr(dd, DC_LCB_ERR_EN, ~0ull);  \n\n\t\tif (ret >= 0)\n\t\t\tret = -EINVAL;\n\t\treturn ret;\n\t}\n\n\treturn 0;  \n}\n\n \nstatic int init_loopback(struct hfi1_devdata *dd)\n{\n\tdd_dev_info(dd, \"Entering loopback mode\\n\");\n\n\t \n\twrite_csr(dd, DC_DC8051_CFG_MODE,\n\t\t  (read_csr(dd, DC_DC8051_CFG_MODE) | DISABLE_SELF_GUID_CHECK));\n\n\t \n\tif ((dd->icode == ICODE_FUNCTIONAL_SIMULATOR) &&\n\t    (loopback == LOOPBACK_SERDES || loopback == LOOPBACK_LCB ||\n\t     loopback == LOOPBACK_CABLE)) {\n\t\tloopback = LOOPBACK_LCB;\n\t\tquick_linkup = 1;\n\t\treturn 0;\n\t}\n\n\t \n\tif (loopback == LOOPBACK_SERDES)\n\t\treturn 0;\n\n\t \n\tif (loopback == LOOPBACK_LCB) {\n\t\tquick_linkup = 1;  \n\n\t\t \n\t\tif (dd->icode == ICODE_FPGA_EMULATION) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"LCB loopback not supported in emulation\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tif (loopback == LOOPBACK_CABLE)\n\t\treturn 0;\n\n\tdd_dev_err(dd, \"Invalid loopback mode %d\\n\", loopback);\n\treturn -EINVAL;\n}\n\n \nstatic u16 opa_to_vc_link_widths(u16 opa_widths)\n{\n\tint i;\n\tu16 result = 0;\n\n\tstatic const struct link_bits {\n\t\tu16 from;\n\t\tu16 to;\n\t} opa_link_xlate[] = {\n\t\t{ OPA_LINK_WIDTH_1X, 1 << (1 - 1)  },\n\t\t{ OPA_LINK_WIDTH_2X, 1 << (2 - 1)  },\n\t\t{ OPA_LINK_WIDTH_3X, 1 << (3 - 1)  },\n\t\t{ OPA_LINK_WIDTH_4X, 1 << (4 - 1)  },\n\t};\n\n\tfor (i = 0; i < ARRAY_SIZE(opa_link_xlate); i++) {\n\t\tif (opa_widths & opa_link_xlate[i].from)\n\t\t\tresult |= opa_link_xlate[i].to;\n\t}\n\treturn result;\n}\n\n \nstatic int set_local_link_attributes(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu8 enable_lane_tx;\n\tu8 tx_polarity_inversion;\n\tu8 rx_polarity_inversion;\n\tint ret;\n\tu32 misc_bits = 0;\n\t \n\tfabric_serdes_reset(dd);\n\n\t \n\tret = read_tx_settings(dd, &enable_lane_tx, &tx_polarity_inversion,\n\t\t\t       &rx_polarity_inversion, &ppd->local_tx_rate);\n\tif (ret)\n\t\tgoto set_local_link_attributes_fail;\n\n\tif (dd->dc8051_ver < dc8051_ver(0, 20, 0)) {\n\t\t \n\t\tif (ppd->link_speed_enabled & OPA_LINK_SPEED_25G)\n\t\t\tppd->local_tx_rate = 1;\n\t\telse\n\t\t\tppd->local_tx_rate = 0;\n\t} else {\n\t\t \n\t\tppd->local_tx_rate = 0;\n\t\tif (ppd->link_speed_enabled & OPA_LINK_SPEED_25G)\n\t\t\tppd->local_tx_rate |= 2;\n\t\tif (ppd->link_speed_enabled & OPA_LINK_SPEED_12_5G)\n\t\t\tppd->local_tx_rate |= 1;\n\t}\n\n\tenable_lane_tx = 0xF;  \n\tret = write_tx_settings(dd, enable_lane_tx, tx_polarity_inversion,\n\t\t\t\trx_polarity_inversion, ppd->local_tx_rate);\n\tif (ret != HCMD_SUCCESS)\n\t\tgoto set_local_link_attributes_fail;\n\n\tret = write_host_interface_version(dd, HOST_INTERFACE_VERSION);\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Failed to set host interface version, return 0x%x\\n\",\n\t\t\t   ret);\n\t\tgoto set_local_link_attributes_fail;\n\t}\n\n\t \n\tret = write_vc_local_phy(dd,\n\t\t\t\t 0  ,\n\t\t\t\t 1  );\n\tif (ret != HCMD_SUCCESS)\n\t\tgoto set_local_link_attributes_fail;\n\n\t \n\tret = write_vc_local_fabric(dd, dd->vau, 1, dd->vcu, dd->vl15_init,\n\t\t\t\t    ppd->port_crc_mode_enabled);\n\tif (ret != HCMD_SUCCESS)\n\t\tgoto set_local_link_attributes_fail;\n\n\t \n\tif (loopback == LOOPBACK_SERDES)\n\t\tmisc_bits |= 1 << LOOPBACK_SERDES_CONFIG_BIT_MASK_SHIFT;\n\n\t \n\tif (dd->dc8051_ver >= dc8051_ver(1, 25, 0))\n\t\tmisc_bits |= 1 << EXT_CFG_LCB_RESET_SUPPORTED_SHIFT;\n\n\tret = write_vc_local_link_mode(dd, misc_bits, 0,\n\t\t\t\t       opa_to_vc_link_widths(\n\t\t\t\t\t\tppd->link_width_enabled));\n\tif (ret != HCMD_SUCCESS)\n\t\tgoto set_local_link_attributes_fail;\n\n\t \n\tret = write_local_device_id(dd, dd->pcidev->device, dd->minrev);\n\tif (ret == HCMD_SUCCESS)\n\t\treturn 0;\n\nset_local_link_attributes_fail:\n\tdd_dev_err(dd,\n\t\t   \"Failed to set local link attributes, return 0x%x\\n\",\n\t\t   ret);\n\treturn ret;\n}\n\n \nint start_link(struct hfi1_pportdata *ppd)\n{\n\t \n\ttune_serdes(ppd);\n\n\tif (!ppd->driver_link_ready) {\n\t\tdd_dev_info(ppd->dd,\n\t\t\t    \"%s: stopping link start because driver is not ready\\n\",\n\t\t\t    __func__);\n\t\treturn 0;\n\t}\n\n\t \n\tclear_full_mgmt_pkey(ppd);\n\n\treturn set_link_state(ppd, HLS_DN_POLL);\n}\n\nstatic void wait_for_qsfp_init(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 mask;\n\tunsigned long timeout;\n\n\t \n\tmsleep(500);\n\n\t \n\ttimeout = jiffies + msecs_to_jiffies(2000);\n\twhile (1) {\n\t\tmask = read_csr(dd, dd->hfi1_id ?\n\t\t\t\tASIC_QSFP2_IN : ASIC_QSFP1_IN);\n\t\tif (!(mask & QSFP_HFI0_INT_N))\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_info(dd, \"%s: No IntN detected, reset complete\\n\",\n\t\t\t\t    __func__);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(2);\n\t}\n}\n\nstatic void set_qsfp_int_n(struct hfi1_pportdata *ppd, u8 enable)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 mask;\n\n\tmask = read_csr(dd, dd->hfi1_id ? ASIC_QSFP2_MASK : ASIC_QSFP1_MASK);\n\tif (enable) {\n\t\t \n\t\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_CLEAR : ASIC_QSFP1_CLEAR,\n\t\t\t  QSFP_HFI0_INT_N);\n\t\tmask |= (u64)QSFP_HFI0_INT_N;\n\t} else {\n\t\tmask &= ~(u64)QSFP_HFI0_INT_N;\n\t}\n\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_MASK : ASIC_QSFP1_MASK, mask);\n}\n\nint reset_qsfp(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 mask, qsfp_mask;\n\n\t \n\tset_qsfp_int_n(ppd, 0);\n\n\t \n\tmask = (u64)QSFP_HFI0_RESET_N;\n\n\tqsfp_mask = read_csr(dd,\n\t\t\t     dd->hfi1_id ? ASIC_QSFP2_OUT : ASIC_QSFP1_OUT);\n\tqsfp_mask &= ~mask;\n\twrite_csr(dd,\n\t\t  dd->hfi1_id ? ASIC_QSFP2_OUT : ASIC_QSFP1_OUT, qsfp_mask);\n\n\tudelay(10);\n\n\tqsfp_mask |= mask;\n\twrite_csr(dd,\n\t\t  dd->hfi1_id ? ASIC_QSFP2_OUT : ASIC_QSFP1_OUT, qsfp_mask);\n\n\twait_for_qsfp_init(ppd);\n\n\t \n\tset_qsfp_int_n(ppd, 1);\n\n\t \n\treturn set_qsfp_tx(ppd, 0);\n}\n\nstatic int handle_qsfp_error_conditions(struct hfi1_pportdata *ppd,\n\t\t\t\t\tu8 *qsfp_interrupt_status)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\tif ((qsfp_interrupt_status[0] & QSFP_HIGH_TEMP_ALARM) ||\n\t    (qsfp_interrupt_status[0] & QSFP_HIGH_TEMP_WARNING))\n\t\tdd_dev_err(dd, \"%s: QSFP cable temperature too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[0] & QSFP_LOW_TEMP_ALARM) ||\n\t    (qsfp_interrupt_status[0] & QSFP_LOW_TEMP_WARNING))\n\t\tdd_dev_err(dd, \"%s: QSFP cable temperature too low\\n\",\n\t\t\t   __func__);\n\n\t \n\tif (ppd->host_link_state & HLS_DOWN)\n\t\treturn 0;\n\n\tif ((qsfp_interrupt_status[1] & QSFP_HIGH_VCC_ALARM) ||\n\t    (qsfp_interrupt_status[1] & QSFP_HIGH_VCC_WARNING))\n\t\tdd_dev_err(dd, \"%s: QSFP supply voltage too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[1] & QSFP_LOW_VCC_ALARM) ||\n\t    (qsfp_interrupt_status[1] & QSFP_LOW_VCC_WARNING))\n\t\tdd_dev_err(dd, \"%s: QSFP supply voltage too low\\n\",\n\t\t\t   __func__);\n\n\t \n\n\tif ((qsfp_interrupt_status[3] & QSFP_HIGH_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[3] & QSFP_HIGH_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable RX channel 1/2 power too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[3] & QSFP_LOW_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[3] & QSFP_LOW_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable RX channel 1/2 power too low\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[4] & QSFP_HIGH_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[4] & QSFP_HIGH_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable RX channel 3/4 power too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[4] & QSFP_LOW_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[4] & QSFP_LOW_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable RX channel 3/4 power too low\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[5] & QSFP_HIGH_BIAS_ALARM) ||\n\t    (qsfp_interrupt_status[5] & QSFP_HIGH_BIAS_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 1/2 bias too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[5] & QSFP_LOW_BIAS_ALARM) ||\n\t    (qsfp_interrupt_status[5] & QSFP_LOW_BIAS_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 1/2 bias too low\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[6] & QSFP_HIGH_BIAS_ALARM) ||\n\t    (qsfp_interrupt_status[6] & QSFP_HIGH_BIAS_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 3/4 bias too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[6] & QSFP_LOW_BIAS_ALARM) ||\n\t    (qsfp_interrupt_status[6] & QSFP_LOW_BIAS_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 3/4 bias too low\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[7] & QSFP_HIGH_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[7] & QSFP_HIGH_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 1/2 power too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[7] & QSFP_LOW_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[7] & QSFP_LOW_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 1/2 power too low\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[8] & QSFP_HIGH_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[8] & QSFP_HIGH_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 3/4 power too high\\n\",\n\t\t\t   __func__);\n\n\tif ((qsfp_interrupt_status[8] & QSFP_LOW_POWER_ALARM) ||\n\t    (qsfp_interrupt_status[8] & QSFP_LOW_POWER_WARNING))\n\t\tdd_dev_err(dd, \"%s: Cable TX channel 3/4 power too low\\n\",\n\t\t\t   __func__);\n\n\t \n\t \n\n\treturn 0;\n}\n\n \nvoid qsfp_event(struct work_struct *work)\n{\n\tstruct qsfp_data *qd;\n\tstruct hfi1_pportdata *ppd;\n\tstruct hfi1_devdata *dd;\n\n\tqd = container_of(work, struct qsfp_data, qsfp_work);\n\tppd = qd->ppd;\n\tdd = ppd->dd;\n\n\t \n\tif (!qsfp_mod_present(ppd))\n\t\treturn;\n\n\tif (ppd->host_link_state == HLS_DN_DISABLE) {\n\t\tdd_dev_info(ppd->dd,\n\t\t\t    \"%s: stopping link start because link is disabled\\n\",\n\t\t\t    __func__);\n\t\treturn;\n\t}\n\n\t \n\tdc_start(dd);\n\n\tif (qd->cache_refresh_required) {\n\t\tset_qsfp_int_n(ppd, 0);\n\n\t\twait_for_qsfp_init(ppd);\n\n\t\t \n\t\tset_qsfp_int_n(ppd, 1);\n\n\t\tstart_link(ppd);\n\t}\n\n\tif (qd->check_interrupt_flags) {\n\t\tu8 qsfp_interrupt_status[16] = {0,};\n\n\t\tif (one_qsfp_read(ppd, dd->hfi1_id, 6,\n\t\t\t\t  &qsfp_interrupt_status[0], 16) != 16) {\n\t\t\tdd_dev_info(dd,\n\t\t\t\t    \"%s: Failed to read status of QSFP module\\n\",\n\t\t\t\t    __func__);\n\t\t} else {\n\t\t\tunsigned long flags;\n\n\t\t\thandle_qsfp_error_conditions(\n\t\t\t\t\tppd, qsfp_interrupt_status);\n\t\t\tspin_lock_irqsave(&ppd->qsfp_info.qsfp_lock, flags);\n\t\t\tppd->qsfp_info.check_interrupt_flags = 0;\n\t\t\tspin_unlock_irqrestore(&ppd->qsfp_info.qsfp_lock,\n\t\t\t\t\t       flags);\n\t\t}\n\t}\n}\n\nvoid init_qsfp_int(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\tu64 qsfp_mask;\n\n\tqsfp_mask = (u64)(QSFP_HFI0_INT_N | QSFP_HFI0_MODPRST_N);\n\t \n\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_CLEAR : ASIC_QSFP1_CLEAR,\n\t\t  qsfp_mask);\n\twrite_csr(dd, dd->hfi1_id ? ASIC_QSFP2_MASK : ASIC_QSFP1_MASK,\n\t\t  qsfp_mask);\n\n\tset_qsfp_int_n(ppd, 0);\n\n\t \n\tif (qsfp_mod_present(ppd))\n\t\tqsfp_mask &= ~(u64)QSFP_HFI0_MODPRST_N;\n\twrite_csr(dd,\n\t\t  dd->hfi1_id ? ASIC_QSFP2_INVERT : ASIC_QSFP1_INVERT,\n\t\t  qsfp_mask);\n\n\t \n\tif (!dd->hfi1_id)\n\t\tset_intr_bits(dd, QSFP1_INT, QSFP1_INT, true);\n\telse\n\t\tset_intr_bits(dd, QSFP2_INT, QSFP2_INT, true);\n}\n\n \nstatic void init_lcb(struct hfi1_devdata *dd)\n{\n\t \n\tif (dd->icode == ICODE_FUNCTIONAL_SIMULATOR)\n\t\treturn;\n\n\t \n\n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 0x01);\n\twrite_csr(dd, DC_LCB_CFG_LANE_WIDTH, 0x00);\n\twrite_csr(dd, DC_LCB_CFG_REINIT_AS_SLAVE, 0x00);\n\twrite_csr(dd, DC_LCB_CFG_CNT_FOR_SKIP_STALL, 0x110);\n\twrite_csr(dd, DC_LCB_CFG_CLK_CNTR, 0x08);\n\twrite_csr(dd, DC_LCB_CFG_LOOPBACK, 0x02);\n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 0x00);\n}\n\n \nstatic int test_qsfp_read(struct hfi1_pportdata *ppd)\n{\n\tint ret;\n\tu8 status;\n\n\t \n\tif (ppd->port_type != PORT_TYPE_QSFP || !qsfp_mod_present(ppd))\n\t\treturn 0;\n\n\t \n\tret = one_qsfp_read(ppd, ppd->dd->hfi1_id, 2, &status, 1);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret != 1)\n\t\treturn -EIO;\n\n\treturn 0;  \n}\n\n \n#define MAX_QSFP_RETRIES 20\n#define QSFP_RETRY_WAIT 500  \n\n \nstatic void try_start_link(struct hfi1_pportdata *ppd)\n{\n\tif (test_qsfp_read(ppd)) {\n\t\t \n\t\tif (ppd->qsfp_retry_count >= MAX_QSFP_RETRIES) {\n\t\t\tdd_dev_err(ppd->dd, \"QSFP not responding, giving up\\n\");\n\t\t\treturn;\n\t\t}\n\t\tdd_dev_info(ppd->dd,\n\t\t\t    \"QSFP not responding, waiting and retrying %d\\n\",\n\t\t\t    (int)ppd->qsfp_retry_count);\n\t\tppd->qsfp_retry_count++;\n\t\tqueue_delayed_work(ppd->link_wq, &ppd->start_link_work,\n\t\t\t\t   msecs_to_jiffies(QSFP_RETRY_WAIT));\n\t\treturn;\n\t}\n\tppd->qsfp_retry_count = 0;\n\n\tstart_link(ppd);\n}\n\n \nvoid handle_start_link(struct work_struct *work)\n{\n\tstruct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,\n\t\t\t\t\t\t  start_link_work.work);\n\ttry_start_link(ppd);\n}\n\nint bringup_serdes(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 guid;\n\tint ret;\n\n\tif (HFI1_CAP_IS_KSET(EXTENDED_PSN))\n\t\tadd_rcvctrl(dd, RCV_CTRL_RCV_EXTENDED_PSN_ENABLE_SMASK);\n\n\tguid = ppd->guids[HFI1_PORT_GUID_INDEX];\n\tif (!guid) {\n\t\tif (dd->base_guid)\n\t\t\tguid = dd->base_guid + ppd->port - 1;\n\t\tppd->guids[HFI1_PORT_GUID_INDEX] = guid;\n\t}\n\n\t \n\tppd->linkinit_reason = OPA_LINKINIT_REASON_LINKUP;\n\n\t \n\tinit_lcb(dd);\n\n\tif (loopback) {\n\t\tret = init_loopback(dd);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tget_port_type(ppd);\n\tif (ppd->port_type == PORT_TYPE_QSFP) {\n\t\tset_qsfp_int_n(ppd, 0);\n\t\twait_for_qsfp_init(ppd);\n\t\tset_qsfp_int_n(ppd, 1);\n\t}\n\n\ttry_start_link(ppd);\n\treturn 0;\n}\n\nvoid hfi1_quiet_serdes(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\t \n\tppd->driver_link_ready = 0;\n\tppd->link_enabled = 0;\n\n\tppd->qsfp_retry_count = MAX_QSFP_RETRIES;  \n\tflush_delayed_work(&ppd->start_link_work);\n\tcancel_delayed_work_sync(&ppd->start_link_work);\n\n\tppd->offline_disabled_reason =\n\t\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_REBOOT);\n\tset_link_down_reason(ppd, OPA_LINKDOWN_REASON_REBOOT, 0,\n\t\t\t     OPA_LINKDOWN_REASON_REBOOT);\n\tset_link_state(ppd, HLS_DN_OFFLINE);\n\n\t \n\tclear_rcvctrl(dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);\n\tcancel_work_sync(&ppd->freeze_work);\n}\n\nstatic inline int init_cpu_counters(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd;\n\tint i;\n\n\tppd = (struct hfi1_pportdata *)(dd + 1);\n\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\tppd->ibport_data.rvp.rc_acks = NULL;\n\t\tppd->ibport_data.rvp.rc_qacks = NULL;\n\t\tppd->ibport_data.rvp.rc_acks = alloc_percpu(u64);\n\t\tppd->ibport_data.rvp.rc_qacks = alloc_percpu(u64);\n\t\tppd->ibport_data.rvp.rc_delayed_comp = alloc_percpu(u64);\n\t\tif (!ppd->ibport_data.rvp.rc_acks ||\n\t\t    !ppd->ibport_data.rvp.rc_delayed_comp ||\n\t\t    !ppd->ibport_data.rvp.rc_qacks)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nvoid hfi1_put_tid(struct hfi1_devdata *dd, u32 index,\n\t\t  u32 type, unsigned long pa, u16 order)\n{\n\tu64 reg;\n\n\tif (!(dd->flags & HFI1_PRESENT))\n\t\tgoto done;\n\n\tif (type == PT_INVALID || type == PT_INVALID_FLUSH) {\n\t\tpa = 0;\n\t\torder = 0;\n\t} else if (type > PT_INVALID) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"unexpected receive array type %u for index %u, not handled\\n\",\n\t\t\t   type, index);\n\t\tgoto done;\n\t}\n\ttrace_hfi1_put_tid(dd, index, type, pa, order);\n\n#define RT_ADDR_SHIFT 12\t \n\treg = RCV_ARRAY_RT_WRITE_ENABLE_SMASK\n\t\t| (u64)order << RCV_ARRAY_RT_BUF_SIZE_SHIFT\n\t\t| ((pa >> RT_ADDR_SHIFT) & RCV_ARRAY_RT_ADDR_MASK)\n\t\t\t\t\t<< RCV_ARRAY_RT_ADDR_SHIFT;\n\ttrace_hfi1_write_rcvarray(dd->rcvarray_wc + (index * 8), reg);\n\twriteq(reg, dd->rcvarray_wc + (index * 8));\n\n\tif (type == PT_EAGER || type == PT_INVALID_FLUSH || (index & 3) == 3)\n\t\t \n\t\tflush_wc();\ndone:\n\treturn;\n}\n\nvoid hfi1_clear_tids(struct hfi1_ctxtdata *rcd)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu32 i;\n\n\t \n\tfor (i = rcd->eager_base; i < rcd->eager_base +\n\t\t     rcd->egrbufs.alloced; i++)\n\t\thfi1_put_tid(dd, i, PT_INVALID, 0, 0);\n\n\tfor (i = rcd->expected_base;\n\t\t\ti < rcd->expected_base + rcd->expected_count; i++)\n\t\thfi1_put_tid(dd, i, PT_INVALID, 0, 0);\n}\n\nstatic const char * const ib_cfg_name_strings[] = {\n\t\"HFI1_IB_CFG_LIDLMC\",\n\t\"HFI1_IB_CFG_LWID_DG_ENB\",\n\t\"HFI1_IB_CFG_LWID_ENB\",\n\t\"HFI1_IB_CFG_LWID\",\n\t\"HFI1_IB_CFG_SPD_ENB\",\n\t\"HFI1_IB_CFG_SPD\",\n\t\"HFI1_IB_CFG_RXPOL_ENB\",\n\t\"HFI1_IB_CFG_LREV_ENB\",\n\t\"HFI1_IB_CFG_LINKLATENCY\",\n\t\"HFI1_IB_CFG_HRTBT\",\n\t\"HFI1_IB_CFG_OP_VLS\",\n\t\"HFI1_IB_CFG_VL_HIGH_CAP\",\n\t\"HFI1_IB_CFG_VL_LOW_CAP\",\n\t\"HFI1_IB_CFG_OVERRUN_THRESH\",\n\t\"HFI1_IB_CFG_PHYERR_THRESH\",\n\t\"HFI1_IB_CFG_LINKDEFAULT\",\n\t\"HFI1_IB_CFG_PKEYS\",\n\t\"HFI1_IB_CFG_MTU\",\n\t\"HFI1_IB_CFG_LSTATE\",\n\t\"HFI1_IB_CFG_VL_HIGH_LIMIT\",\n\t\"HFI1_IB_CFG_PMA_TICKS\",\n\t\"HFI1_IB_CFG_PORT\"\n};\n\nstatic const char *ib_cfg_name(int which)\n{\n\tif (which < 0 || which >= ARRAY_SIZE(ib_cfg_name_strings))\n\t\treturn \"invalid\";\n\treturn ib_cfg_name_strings[which];\n}\n\nint hfi1_get_ib_cfg(struct hfi1_pportdata *ppd, int which)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tint val = 0;\n\n\tswitch (which) {\n\tcase HFI1_IB_CFG_LWID_ENB:  \n\t\tval = ppd->link_width_enabled;\n\t\tbreak;\n\tcase HFI1_IB_CFG_LWID:  \n\t\tval = ppd->link_width_active;\n\t\tbreak;\n\tcase HFI1_IB_CFG_SPD_ENB:  \n\t\tval = ppd->link_speed_enabled;\n\t\tbreak;\n\tcase HFI1_IB_CFG_SPD:  \n\t\tval = ppd->link_speed_active;\n\t\tbreak;\n\n\tcase HFI1_IB_CFG_RXPOL_ENB:  \n\tcase HFI1_IB_CFG_LREV_ENB:  \n\tcase HFI1_IB_CFG_LINKLATENCY:\n\t\tgoto unimplemented;\n\n\tcase HFI1_IB_CFG_OP_VLS:\n\t\tval = ppd->actual_vls_operational;\n\t\tbreak;\n\tcase HFI1_IB_CFG_VL_HIGH_CAP:  \n\t\tval = VL_ARB_HIGH_PRIO_TABLE_SIZE;\n\t\tbreak;\n\tcase HFI1_IB_CFG_VL_LOW_CAP:  \n\t\tval = VL_ARB_LOW_PRIO_TABLE_SIZE;\n\t\tbreak;\n\tcase HFI1_IB_CFG_OVERRUN_THRESH:  \n\t\tval = ppd->overrun_threshold;\n\t\tbreak;\n\tcase HFI1_IB_CFG_PHYERR_THRESH:  \n\t\tval = ppd->phy_error_threshold;\n\t\tbreak;\n\tcase HFI1_IB_CFG_LINKDEFAULT:  \n\t\tval = HLS_DEFAULT;\n\t\tbreak;\n\n\tcase HFI1_IB_CFG_HRTBT:  \n\tcase HFI1_IB_CFG_PMA_TICKS:\n\tdefault:\nunimplemented:\n\t\tif (HFI1_CAP_IS_KSET(PRINT_UNIMPL))\n\t\t\tdd_dev_info(\n\t\t\t\tdd,\n\t\t\t\t\"%s: which %s: not implemented\\n\",\n\t\t\t\t__func__,\n\t\t\t\tib_cfg_name(which));\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\n \n#define MAX_MAD_PACKET 2048\n\n \nu32 lrh_max_header_bytes(struct hfi1_devdata *dd)\n{\n\t \n\treturn (get_hdrqentsize(dd->rcd[0]) - 2  + 1 ) << 2;\n}\n\n \nstatic void set_send_length(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu32 max_hb = lrh_max_header_bytes(dd), dcmtu;\n\tu32 maxvlmtu = dd->vld[15].mtu;\n\tu64 len1 = 0, len2 = (((dd->vld[15].mtu + max_hb) >> 2)\n\t\t\t      & SEND_LEN_CHECK1_LEN_VL15_MASK) <<\n\t\tSEND_LEN_CHECK1_LEN_VL15_SHIFT;\n\tint i, j;\n\tu32 thres;\n\n\tfor (i = 0; i < ppd->vls_supported; i++) {\n\t\tif (dd->vld[i].mtu > maxvlmtu)\n\t\t\tmaxvlmtu = dd->vld[i].mtu;\n\t\tif (i <= 3)\n\t\t\tlen1 |= (((dd->vld[i].mtu + max_hb) >> 2)\n\t\t\t\t & SEND_LEN_CHECK0_LEN_VL0_MASK) <<\n\t\t\t\t((i % 4) * SEND_LEN_CHECK0_LEN_VL1_SHIFT);\n\t\telse\n\t\t\tlen2 |= (((dd->vld[i].mtu + max_hb) >> 2)\n\t\t\t\t & SEND_LEN_CHECK1_LEN_VL4_MASK) <<\n\t\t\t\t((i % 4) * SEND_LEN_CHECK1_LEN_VL5_SHIFT);\n\t}\n\twrite_csr(dd, SEND_LEN_CHECK0, len1);\n\twrite_csr(dd, SEND_LEN_CHECK1, len2);\n\t \n\t \n\tfor (i = 0; i < ppd->vls_supported; i++) {\n\t\tthres = min(sc_percent_to_threshold(dd->vld[i].sc, 50),\n\t\t\t    sc_mtu_to_threshold(dd->vld[i].sc,\n\t\t\t\t\t\tdd->vld[i].mtu,\n\t\t\t\t\t\tget_hdrqentsize(dd->rcd[0])));\n\t\tfor (j = 0; j < INIT_SC_PER_VL; j++)\n\t\t\tsc_set_cr_threshold(\n\t\t\t\t\tpio_select_send_context_vl(dd, j, i),\n\t\t\t\t\t    thres);\n\t}\n\tthres = min(sc_percent_to_threshold(dd->vld[15].sc, 50),\n\t\t    sc_mtu_to_threshold(dd->vld[15].sc,\n\t\t\t\t\tdd->vld[15].mtu,\n\t\t\t\t\tdd->rcd[0]->rcvhdrqentsize));\n\tsc_set_cr_threshold(dd->vld[15].sc, thres);\n\n\t \n\tdcmtu = maxvlmtu == 10240 ? DCC_CFG_PORT_MTU_CAP_10240 :\n\t\t(ilog2(maxvlmtu >> 8) + 1);\n\tlen1 = read_csr(ppd->dd, DCC_CFG_PORT_CONFIG);\n\tlen1 &= ~DCC_CFG_PORT_CONFIG_MTU_CAP_SMASK;\n\tlen1 |= ((u64)dcmtu & DCC_CFG_PORT_CONFIG_MTU_CAP_MASK) <<\n\t\tDCC_CFG_PORT_CONFIG_MTU_CAP_SHIFT;\n\twrite_csr(ppd->dd, DCC_CFG_PORT_CONFIG, len1);\n}\n\nstatic void set_lidlmc(struct hfi1_pportdata *ppd)\n{\n\tint i;\n\tu64 sreg = 0;\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu32 mask = ~((1U << ppd->lmc) - 1);\n\tu64 c1 = read_csr(ppd->dd, DCC_CFG_PORT_CONFIG1);\n\tu32 lid;\n\n\t \n\tlid = (ppd->lid >= be16_to_cpu(IB_MULTICAST_LID_BASE)) ? 0 : ppd->lid;\n\tc1 &= ~(DCC_CFG_PORT_CONFIG1_TARGET_DLID_SMASK\n\t\t| DCC_CFG_PORT_CONFIG1_DLID_MASK_SMASK);\n\tc1 |= ((lid & DCC_CFG_PORT_CONFIG1_TARGET_DLID_MASK)\n\t\t\t<< DCC_CFG_PORT_CONFIG1_TARGET_DLID_SHIFT) |\n\t      ((mask & DCC_CFG_PORT_CONFIG1_DLID_MASK_MASK)\n\t\t\t<< DCC_CFG_PORT_CONFIG1_DLID_MASK_SHIFT);\n\twrite_csr(ppd->dd, DCC_CFG_PORT_CONFIG1, c1);\n\n\t \n\tsreg = ((mask & SEND_CTXT_CHECK_SLID_MASK_MASK) <<\n\t\t\tSEND_CTXT_CHECK_SLID_MASK_SHIFT) |\n\t       (((lid & mask) & SEND_CTXT_CHECK_SLID_VALUE_MASK) <<\n\t\t\tSEND_CTXT_CHECK_SLID_VALUE_SHIFT);\n\n\tfor (i = 0; i < chip_send_contexts(dd); i++) {\n\t\thfi1_cdbg(LINKVERB, \"SendContext[%d].SLID_CHECK = 0x%x\",\n\t\t\t  i, (u32)sreg);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_SLID, sreg);\n\t}\n\n\t \n\tsdma_update_lmc(dd, mask, lid);\n}\n\nstatic const char *state_completed_string(u32 completed)\n{\n\tstatic const char * const state_completed[] = {\n\t\t\"EstablishComm\",\n\t\t\"OptimizeEQ\",\n\t\t\"VerifyCap\"\n\t};\n\n\tif (completed < ARRAY_SIZE(state_completed))\n\t\treturn state_completed[completed];\n\n\treturn \"unknown\";\n}\n\nstatic const char all_lanes_dead_timeout_expired[] =\n\t\"All lanes were inactive \u2013 was the interconnect media removed?\";\nstatic const char tx_out_of_policy[] =\n\t\"Passing lanes on local port do not meet the local link width policy\";\nstatic const char no_state_complete[] =\n\t\"State timeout occurred before link partner completed the state\";\nstatic const char * const state_complete_reasons[] = {\n\t[0x00] = \"Reason unknown\",\n\t[0x01] = \"Link was halted by driver, refer to LinkDownReason\",\n\t[0x02] = \"Link partner reported failure\",\n\t[0x10] = \"Unable to achieve frame sync on any lane\",\n\t[0x11] =\n\t  \"Unable to find a common bit rate with the link partner\",\n\t[0x12] =\n\t  \"Unable to achieve frame sync on sufficient lanes to meet the local link width policy\",\n\t[0x13] =\n\t  \"Unable to identify preset equalization on sufficient lanes to meet the local link width policy\",\n\t[0x14] = no_state_complete,\n\t[0x15] =\n\t  \"State timeout occurred before link partner identified equalization presets\",\n\t[0x16] =\n\t  \"Link partner completed the EstablishComm state, but the passing lanes do not meet the local link width policy\",\n\t[0x17] = tx_out_of_policy,\n\t[0x20] = all_lanes_dead_timeout_expired,\n\t[0x21] =\n\t  \"Unable to achieve acceptable BER on sufficient lanes to meet the local link width policy\",\n\t[0x22] = no_state_complete,\n\t[0x23] =\n\t  \"Link partner completed the OptimizeEq state, but the passing lanes do not meet the local link width policy\",\n\t[0x24] = tx_out_of_policy,\n\t[0x30] = all_lanes_dead_timeout_expired,\n\t[0x31] =\n\t  \"State timeout occurred waiting for host to process received frames\",\n\t[0x32] = no_state_complete,\n\t[0x33] =\n\t  \"Link partner completed the VerifyCap state, but the passing lanes do not meet the local link width policy\",\n\t[0x34] = tx_out_of_policy,\n\t[0x35] = \"Negotiated link width is mutually exclusive\",\n\t[0x36] =\n\t  \"Timed out before receiving verifycap frames in VerifyCap.Exchange\",\n\t[0x37] = \"Unable to resolve secure data exchange\",\n};\n\nstatic const char *state_complete_reason_code_string(struct hfi1_pportdata *ppd,\n\t\t\t\t\t\t     u32 code)\n{\n\tconst char *str = NULL;\n\n\tif (code < ARRAY_SIZE(state_complete_reasons))\n\t\tstr = state_complete_reasons[code];\n\n\tif (str)\n\t\treturn str;\n\treturn \"Reserved\";\n}\n\n \nstatic void decode_state_complete(struct hfi1_pportdata *ppd, u32 frame,\n\t\t\t\t  const char *prefix)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu32 success;\n\tu32 state;\n\tu32 reason;\n\tu32 lanes;\n\n\t \n\tsuccess = frame & 0x1;\n\tstate = (frame >> 1) & 0x7;\n\treason = (frame >> 8) & 0xff;\n\tlanes = (frame >> 16) & 0xffff;\n\n\tdd_dev_err(dd, \"Last %s LNI state complete frame 0x%08x:\\n\",\n\t\t   prefix, frame);\n\tdd_dev_err(dd, \"    last reported state state: %s (0x%x)\\n\",\n\t\t   state_completed_string(state), state);\n\tdd_dev_err(dd, \"    state successfully completed: %s\\n\",\n\t\t   success ? \"yes\" : \"no\");\n\tdd_dev_err(dd, \"    fail reason 0x%x: %s\\n\",\n\t\t   reason, state_complete_reason_code_string(ppd, reason));\n\tdd_dev_err(dd, \"    passing lane mask: 0x%x\", lanes);\n}\n\n \nstatic void check_lni_states(struct hfi1_pportdata *ppd)\n{\n\tu32 last_local_state;\n\tu32 last_remote_state;\n\n\tread_last_local_state(ppd->dd, &last_local_state);\n\tread_last_remote_state(ppd->dd, &last_remote_state);\n\n\t \n\tif (last_local_state == 0 && last_remote_state == 0)\n\t\treturn;\n\n\tdecode_state_complete(ppd, last_local_state, \"transmitted\");\n\tdecode_state_complete(ppd, last_remote_state, \"received\");\n}\n\n \nstatic int wait_link_transfer_active(struct hfi1_devdata *dd, int wait_ms)\n{\n\tu64 reg;\n\tunsigned long timeout;\n\n\t \n\ttimeout = jiffies + msecs_to_jiffies(wait_ms);\n\twhile (1) {\n\t\treg = read_csr(dd, DC_LCB_STS_LINK_TRANSFER_ACTIVE);\n\t\tif (reg)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"timeout waiting for LINK_TRANSFER_ACTIVE\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tudelay(2);\n\t}\n\treturn 0;\n}\n\n \nstatic void force_logical_link_state_down(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 1);\n\twrite_csr(dd, DC_LCB_CFG_IGNORE_LOST_RCLK,\n\t\t  DC_LCB_CFG_IGNORE_LOST_RCLK_EN_SMASK);\n\n\twrite_csr(dd, DC_LCB_CFG_LANE_WIDTH, 0);\n\twrite_csr(dd, DC_LCB_CFG_REINIT_AS_SLAVE, 0);\n\twrite_csr(dd, DC_LCB_CFG_CNT_FOR_SKIP_STALL, 0x110);\n\twrite_csr(dd, DC_LCB_CFG_LOOPBACK, 0x2);\n\n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 0);\n\t(void)read_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET);\n\tudelay(3);\n\twrite_csr(dd, DC_LCB_CFG_ALLOW_LINK_UP, 1);\n\twrite_csr(dd, DC_LCB_CFG_RUN, 1ull << DC_LCB_CFG_RUN_EN_SHIFT);\n\n\twait_link_transfer_active(dd, 100);\n\n\t \n\twrite_csr(dd, DC_LCB_CFG_TX_FIFOS_RESET, 1);\n\twrite_csr(dd, DC_LCB_CFG_ALLOW_LINK_UP, 0);\n\twrite_csr(dd, DC_LCB_CFG_IGNORE_LOST_RCLK, 0);\n\n\tdd_dev_info(ppd->dd, \"logical state forced to LINK_DOWN\\n\");\n}\n\n \nstatic int goto_offline(struct hfi1_pportdata *ppd, u8 rem_reason)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu32 previous_state;\n\tint offline_state_ret;\n\tint ret;\n\n\tupdate_lcb_cache(dd);\n\n\tprevious_state = ppd->host_link_state;\n\tppd->host_link_state = HLS_GOING_OFFLINE;\n\n\t \n\tret = set_physical_link_state(dd, (rem_reason << 8) | PLS_OFFLINE);\n\n\tif (ret != HCMD_SUCCESS) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Failed to transition to Offline link state, return %d\\n\",\n\t\t\t   ret);\n\t\treturn -EINVAL;\n\t}\n\tif (ppd->offline_disabled_reason ==\n\t\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_NONE))\n\t\tppd->offline_disabled_reason =\n\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_TRANSIENT);\n\n\toffline_state_ret = wait_phys_link_offline_substates(ppd, 10000);\n\tif (offline_state_ret < 0)\n\t\treturn offline_state_ret;\n\n\t \n\tif (ppd->port_type == PORT_TYPE_QSFP &&\n\t    ppd->qsfp_info.limiting_active &&\n\t    qsfp_mod_present(ppd)) {\n\t\tint ret;\n\n\t\tret = acquire_chip_resource(dd, qsfp_resource(dd), QSFP_WAIT);\n\t\tif (ret == 0) {\n\t\t\tset_qsfp_tx(ppd, 0);\n\t\t\trelease_chip_resource(dd, qsfp_resource(dd));\n\t\t} else {\n\t\t\t \n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Unable to acquire lock to turn off QSFP TX\\n\");\n\t\t}\n\t}\n\n\t \n\tif (offline_state_ret != PLS_OFFLINE_QUIET) {\n\t\tret = wait_physical_linkstate(ppd, PLS_OFFLINE, 30000);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\tset_host_lcb_access(dd);\n\twrite_csr(dd, DC_LCB_ERR_EN, ~0ull);  \n\n\t \n\tret = wait_logical_linkstate(ppd, IB_PORT_DOWN, 1000);\n\tif (ret)\n\t\tforce_logical_link_state_down(ppd);\n\n\tppd->host_link_state = HLS_LINK_COOLDOWN;  \n\tupdate_statusp(ppd, IB_PORT_DOWN);\n\n\t \n\tret = wait_fm_ready(dd, 7000);\n\tif (ret) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"After going offline, timed out waiting for the 8051 to become ready to accept host requests\\n\");\n\t\t \n\t\tppd->host_link_state = HLS_DN_OFFLINE;\n\t\treturn ret;\n\t}\n\n\t \n\tppd->host_link_state = HLS_DN_OFFLINE;\n\tif (previous_state & HLS_UP) {\n\t\t \n\t\thandle_linkup_change(dd, 0);\n\t} else if (previous_state\n\t\t\t& (HLS_DN_POLL | HLS_VERIFY_CAP | HLS_GOING_UP)) {\n\t\t \n\t\tcheck_lni_states(ppd);\n\n\t\t \n\t\tppd->qsfp_info.reset_needed = 0;\n\t}\n\n\t \n\tppd->link_width_active = 0;\n\tppd->link_width_downgrade_tx_active = 0;\n\tppd->link_width_downgrade_rx_active = 0;\n\tppd->current_egress_rate = 0;\n\treturn 0;\n}\n\n \nstatic const char *link_state_name(u32 state)\n{\n\tconst char *name;\n\tint n = ilog2(state);\n\tstatic const char * const names[] = {\n\t\t[__HLS_UP_INIT_BP]\t = \"INIT\",\n\t\t[__HLS_UP_ARMED_BP]\t = \"ARMED\",\n\t\t[__HLS_UP_ACTIVE_BP]\t = \"ACTIVE\",\n\t\t[__HLS_DN_DOWNDEF_BP]\t = \"DOWNDEF\",\n\t\t[__HLS_DN_POLL_BP]\t = \"POLL\",\n\t\t[__HLS_DN_DISABLE_BP]\t = \"DISABLE\",\n\t\t[__HLS_DN_OFFLINE_BP]\t = \"OFFLINE\",\n\t\t[__HLS_VERIFY_CAP_BP]\t = \"VERIFY_CAP\",\n\t\t[__HLS_GOING_UP_BP]\t = \"GOING_UP\",\n\t\t[__HLS_GOING_OFFLINE_BP] = \"GOING_OFFLINE\",\n\t\t[__HLS_LINK_COOLDOWN_BP] = \"LINK_COOLDOWN\"\n\t};\n\n\tname = n < ARRAY_SIZE(names) ? names[n] : NULL;\n\treturn name ? name : \"unknown\";\n}\n\n \nstatic const char *link_state_reason_name(struct hfi1_pportdata *ppd, u32 state)\n{\n\tif (state == HLS_UP_INIT) {\n\t\tswitch (ppd->linkinit_reason) {\n\t\tcase OPA_LINKINIT_REASON_LINKUP:\n\t\t\treturn \"(LINKUP)\";\n\t\tcase OPA_LINKINIT_REASON_FLAPPING:\n\t\t\treturn \"(FLAPPING)\";\n\t\tcase OPA_LINKINIT_OUTSIDE_POLICY:\n\t\t\treturn \"(OUTSIDE_POLICY)\";\n\t\tcase OPA_LINKINIT_QUARANTINED:\n\t\t\treturn \"(QUARANTINED)\";\n\t\tcase OPA_LINKINIT_INSUFIC_CAPABILITY:\n\t\t\treturn \"(INSUFIC_CAPABILITY)\";\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn \"\";\n}\n\n \nu32 driver_pstate(struct hfi1_pportdata *ppd)\n{\n\tswitch (ppd->host_link_state) {\n\tcase HLS_UP_INIT:\n\tcase HLS_UP_ARMED:\n\tcase HLS_UP_ACTIVE:\n\t\treturn IB_PORTPHYSSTATE_LINKUP;\n\tcase HLS_DN_POLL:\n\t\treturn IB_PORTPHYSSTATE_POLLING;\n\tcase HLS_DN_DISABLE:\n\t\treturn IB_PORTPHYSSTATE_DISABLED;\n\tcase HLS_DN_OFFLINE:\n\t\treturn OPA_PORTPHYSSTATE_OFFLINE;\n\tcase HLS_VERIFY_CAP:\n\t\treturn IB_PORTPHYSSTATE_TRAINING;\n\tcase HLS_GOING_UP:\n\t\treturn IB_PORTPHYSSTATE_TRAINING;\n\tcase HLS_GOING_OFFLINE:\n\t\treturn OPA_PORTPHYSSTATE_OFFLINE;\n\tcase HLS_LINK_COOLDOWN:\n\t\treturn OPA_PORTPHYSSTATE_OFFLINE;\n\tcase HLS_DN_DOWNDEF:\n\tdefault:\n\t\tdd_dev_err(ppd->dd, \"invalid host_link_state 0x%x\\n\",\n\t\t\t   ppd->host_link_state);\n\t\treturn  -1;\n\t}\n}\n\n \nu32 driver_lstate(struct hfi1_pportdata *ppd)\n{\n\tif (ppd->host_link_state && (ppd->host_link_state & HLS_DOWN))\n\t\treturn IB_PORT_DOWN;\n\n\tswitch (ppd->host_link_state & HLS_UP) {\n\tcase HLS_UP_INIT:\n\t\treturn IB_PORT_INIT;\n\tcase HLS_UP_ARMED:\n\t\treturn IB_PORT_ARMED;\n\tcase HLS_UP_ACTIVE:\n\t\treturn IB_PORT_ACTIVE;\n\tdefault:\n\t\tdd_dev_err(ppd->dd, \"invalid host_link_state 0x%x\\n\",\n\t\t\t   ppd->host_link_state);\n\treturn -1;\n\t}\n}\n\nvoid set_link_down_reason(struct hfi1_pportdata *ppd, u8 lcl_reason,\n\t\t\t  u8 neigh_reason, u8 rem_reason)\n{\n\tif (ppd->local_link_down_reason.latest == 0 &&\n\t    ppd->neigh_link_down_reason.latest == 0) {\n\t\tppd->local_link_down_reason.latest = lcl_reason;\n\t\tppd->neigh_link_down_reason.latest = neigh_reason;\n\t\tppd->remote_link_down_reason = rem_reason;\n\t}\n}\n\n \nstatic inline bool data_vls_operational(struct hfi1_pportdata *ppd)\n{\n\tint i;\n\tu64 reg;\n\n\tif (!ppd->actual_vls_operational)\n\t\treturn false;\n\n\tfor (i = 0; i < ppd->vls_supported; i++) {\n\t\treg = read_csr(ppd->dd, SEND_CM_CREDIT_VL + (8 * i));\n\t\tif ((reg && !ppd->dd->vld[i].mtu) ||\n\t\t    (!reg && ppd->dd->vld[i].mtu))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nint set_link_state(struct hfi1_pportdata *ppd, u32 state)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tstruct ib_event event = {.device = NULL};\n\tint ret1, ret = 0;\n\tint orig_new_state, poll_bounce;\n\n\tmutex_lock(&ppd->hls_lock);\n\n\torig_new_state = state;\n\tif (state == HLS_DN_DOWNDEF)\n\t\tstate = HLS_DEFAULT;\n\n\t \n\tpoll_bounce = ppd->host_link_state == HLS_DN_POLL &&\n\t\t      state == HLS_DN_POLL;\n\n\tdd_dev_info(dd, \"%s: current %s, new %s %s%s\\n\", __func__,\n\t\t    link_state_name(ppd->host_link_state),\n\t\t    link_state_name(orig_new_state),\n\t\t    poll_bounce ? \"(bounce) \" : \"\",\n\t\t    link_state_reason_name(ppd, state));\n\n\t \n\tif (!(state & (HLS_UP_ARMED | HLS_UP_ACTIVE)))\n\t\tppd->is_sm_config_started = 0;\n\n\t \n\tif (ppd->host_link_state == state && !poll_bounce)\n\t\tgoto done;\n\n\tswitch (state) {\n\tcase HLS_UP_INIT:\n\t\tif (ppd->host_link_state == HLS_DN_POLL &&\n\t\t    (quick_linkup || dd->icode == ICODE_FUNCTIONAL_SIMULATOR)) {\n\t\t\t \n\t\t\t \n\t\t} else if (ppd->host_link_state != HLS_GOING_UP) {\n\t\t\tgoto unexpected;\n\t\t}\n\n\t\t \n\t\tret = wait_physical_linkstate(ppd, PLS_LINKUP, 1000);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: physical state did not change to LINK-UP\\n\",\n\t\t\t\t   __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = wait_logical_linkstate(ppd, IB_PORT_INIT, 1000);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: logical state did not change to INIT\\n\",\n\t\t\t\t   __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (ppd->linkinit_reason >= OPA_LINKINIT_REASON_CLEAR)\n\t\t\tppd->linkinit_reason =\n\t\t\t\tOPA_LINKINIT_REASON_LINKUP;\n\n\t\t \n\t\tadd_rcvctrl(dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);\n\n\t\thandle_linkup_change(dd, 1);\n\t\tpio_kernel_linkup(dd);\n\n\t\t \n\t\tupdate_xmit_counters(ppd, ppd->link_width_active);\n\n\t\tppd->host_link_state = HLS_UP_INIT;\n\t\tupdate_statusp(ppd, IB_PORT_INIT);\n\t\tbreak;\n\tcase HLS_UP_ARMED:\n\t\tif (ppd->host_link_state != HLS_UP_INIT)\n\t\t\tgoto unexpected;\n\n\t\tif (!data_vls_operational(ppd)) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: Invalid data VL credits or mtu\\n\",\n\t\t\t\t   __func__);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tset_logical_state(dd, LSTATE_ARMED);\n\t\tret = wait_logical_linkstate(ppd, IB_PORT_ARMED, 1000);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: logical state did not change to ARMED\\n\",\n\t\t\t\t   __func__);\n\t\t\tbreak;\n\t\t}\n\t\tppd->host_link_state = HLS_UP_ARMED;\n\t\tupdate_statusp(ppd, IB_PORT_ARMED);\n\t\t \n\t\tif (dd->icode == ICODE_FUNCTIONAL_SIMULATOR)\n\t\t\tppd->neighbor_normal = 1;\n\t\tbreak;\n\tcase HLS_UP_ACTIVE:\n\t\tif (ppd->host_link_state != HLS_UP_ARMED)\n\t\t\tgoto unexpected;\n\n\t\tset_logical_state(dd, LSTATE_ACTIVE);\n\t\tret = wait_logical_linkstate(ppd, IB_PORT_ACTIVE, 1000);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: logical state did not change to ACTIVE\\n\",\n\t\t\t\t   __func__);\n\t\t} else {\n\t\t\t \n\t\t\tsdma_all_running(dd);\n\t\t\tppd->host_link_state = HLS_UP_ACTIVE;\n\t\t\tupdate_statusp(ppd, IB_PORT_ACTIVE);\n\n\t\t\t \n\t\t\tevent.device = &dd->verbs_dev.rdi.ibdev;\n\t\t\tevent.element.port_num = ppd->port;\n\t\t\tevent.event = IB_EVENT_PORT_ACTIVE;\n\t\t}\n\t\tbreak;\n\tcase HLS_DN_POLL:\n\t\tif ((ppd->host_link_state == HLS_DN_DISABLE ||\n\t\t     ppd->host_link_state == HLS_DN_OFFLINE) &&\n\t\t    dd->dc_shutdown)\n\t\t\tdc_start(dd);\n\t\t \n\t\twrite_csr(dd, DCC_CFG_LED_CNTRL, 0);\n\n\t\tif (ppd->host_link_state != HLS_DN_OFFLINE) {\n\t\t\tu8 tmp = ppd->link_enabled;\n\n\t\t\tret = goto_offline(ppd, ppd->remote_link_down_reason);\n\t\t\tif (ret) {\n\t\t\t\tppd->link_enabled = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tppd->remote_link_down_reason = 0;\n\n\t\t\tif (ppd->driver_link_ready)\n\t\t\t\tppd->link_enabled = 1;\n\t\t}\n\n\t\tset_all_slowpath(ppd->dd);\n\t\tret = set_local_link_attributes(ppd);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tppd->port_error_action = 0;\n\n\t\tif (quick_linkup) {\n\t\t\t \n\t\t\tret = do_quick_linkup(dd);\n\t\t} else {\n\t\t\tret1 = set_physical_link_state(dd, PLS_POLLING);\n\t\t\tif (!ret1)\n\t\t\t\tret1 = wait_phys_link_out_of_offline(ppd,\n\t\t\t\t\t\t\t\t     3000);\n\t\t\tif (ret1 != HCMD_SUCCESS) {\n\t\t\t\tdd_dev_err(dd,\n\t\t\t\t\t   \"Failed to transition to Polling link state, return 0x%x\\n\",\n\t\t\t\t\t   ret1);\n\t\t\t\tret = -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tppd->host_link_state = HLS_DN_POLL;\n\t\tppd->offline_disabled_reason =\n\t\t\tHFI1_ODR_MASK(OPA_LINKDOWN_REASON_NONE);\n\t\t \n\t\tif (ret)\n\t\t\tgoto_offline(ppd, 0);\n\t\telse\n\t\t\tlog_physical_state(ppd, PLS_POLLING);\n\t\tbreak;\n\tcase HLS_DN_DISABLE:\n\t\t \n\t\tppd->link_enabled = 0;\n\n\t\t \n\n\t\t \n\t\tif (ppd->host_link_state != HLS_DN_OFFLINE) {\n\t\t\tret = goto_offline(ppd, ppd->remote_link_down_reason);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tppd->remote_link_down_reason = 0;\n\t\t}\n\n\t\tif (!dd->dc_shutdown) {\n\t\t\tret1 = set_physical_link_state(dd, PLS_DISABLED);\n\t\t\tif (ret1 != HCMD_SUCCESS) {\n\t\t\t\tdd_dev_err(dd,\n\t\t\t\t\t   \"Failed to transition to Disabled link state, return 0x%x\\n\",\n\t\t\t\t\t   ret1);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret = wait_physical_linkstate(ppd, PLS_DISABLED, 10000);\n\t\t\tif (ret) {\n\t\t\t\tdd_dev_err(dd,\n\t\t\t\t\t   \"%s: physical state did not change to DISABLED\\n\",\n\t\t\t\t\t   __func__);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdc_shutdown(dd);\n\t\t}\n\t\tppd->host_link_state = HLS_DN_DISABLE;\n\t\tbreak;\n\tcase HLS_DN_OFFLINE:\n\t\tif (ppd->host_link_state == HLS_DN_DISABLE)\n\t\t\tdc_start(dd);\n\n\t\t \n\t\tret = goto_offline(ppd, ppd->remote_link_down_reason);\n\t\tif (!ret)\n\t\t\tppd->remote_link_down_reason = 0;\n\t\tbreak;\n\tcase HLS_VERIFY_CAP:\n\t\tif (ppd->host_link_state != HLS_DN_POLL)\n\t\t\tgoto unexpected;\n\t\tppd->host_link_state = HLS_VERIFY_CAP;\n\t\tlog_physical_state(ppd, PLS_CONFIGPHY_VERIFYCAP);\n\t\tbreak;\n\tcase HLS_GOING_UP:\n\t\tif (ppd->host_link_state != HLS_VERIFY_CAP)\n\t\t\tgoto unexpected;\n\n\t\tret1 = set_physical_link_state(dd, PLS_LINKUP);\n\t\tif (ret1 != HCMD_SUCCESS) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Failed to transition to link up state, return 0x%x\\n\",\n\t\t\t\t   ret1);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tppd->host_link_state = HLS_GOING_UP;\n\t\tbreak;\n\n\tcase HLS_GOING_OFFLINE:\t\t \n\tcase HLS_LINK_COOLDOWN:\t\t \n\tdefault:\n\t\tdd_dev_info(dd, \"%s: state 0x%x: not supported\\n\",\n\t\t\t    __func__, state);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tgoto done;\n\nunexpected:\n\tdd_dev_err(dd, \"%s: unexpected state transition from %s to %s\\n\",\n\t\t   __func__, link_state_name(ppd->host_link_state),\n\t\t   link_state_name(state));\n\tret = -EINVAL;\n\ndone:\n\tmutex_unlock(&ppd->hls_lock);\n\n\tif (event.device)\n\t\tib_dispatch_event(&event);\n\n\treturn ret;\n}\n\nint hfi1_set_ib_cfg(struct hfi1_pportdata *ppd, int which, u32 val)\n{\n\tu64 reg;\n\tint ret = 0;\n\n\tswitch (which) {\n\tcase HFI1_IB_CFG_LIDLMC:\n\t\tset_lidlmc(ppd);\n\t\tbreak;\n\tcase HFI1_IB_CFG_VL_HIGH_LIMIT:\n\t\t \n\t\tval *= 4096 / 64;\n\t\treg = ((u64)val & SEND_HIGH_PRIORITY_LIMIT_LIMIT_MASK)\n\t\t\t<< SEND_HIGH_PRIORITY_LIMIT_LIMIT_SHIFT;\n\t\twrite_csr(ppd->dd, SEND_HIGH_PRIORITY_LIMIT, reg);\n\t\tbreak;\n\tcase HFI1_IB_CFG_LINKDEFAULT:  \n\t\t \n\t\tif (val != HLS_DN_POLL)\n\t\t\tret = -EINVAL;\n\t\tbreak;\n\tcase HFI1_IB_CFG_OP_VLS:\n\t\tif (ppd->vls_operational != val) {\n\t\t\tppd->vls_operational = val;\n\t\t\tif (!ppd->port)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\t \n\tcase HFI1_IB_CFG_LWID_ENB:  \n\t\tppd->link_width_enabled = val & ppd->link_width_supported;\n\t\tbreak;\n\tcase HFI1_IB_CFG_LWID_DG_ENB:  \n\t\tppd->link_width_downgrade_enabled =\n\t\t\t\tval & ppd->link_width_downgrade_supported;\n\t\tbreak;\n\tcase HFI1_IB_CFG_SPD_ENB:  \n\t\tppd->link_speed_enabled = val & ppd->link_speed_supported;\n\t\tbreak;\n\tcase HFI1_IB_CFG_OVERRUN_THRESH:  \n\t\t \n\t\tppd->overrun_threshold = val;\n\t\tbreak;\n\tcase HFI1_IB_CFG_PHYERR_THRESH:  \n\t\t \n\t\tppd->phy_error_threshold = val;\n\t\tbreak;\n\n\tcase HFI1_IB_CFG_MTU:\n\t\tset_send_length(ppd);\n\t\tbreak;\n\n\tcase HFI1_IB_CFG_PKEYS:\n\t\tif (HFI1_CAP_IS_KSET(PKEY_CHECK))\n\t\t\tset_partition_keys(ppd);\n\t\tbreak;\n\n\tdefault:\n\t\tif (HFI1_CAP_IS_KSET(PRINT_UNIMPL))\n\t\t\tdd_dev_info(ppd->dd,\n\t\t\t\t    \"%s: which %s, val 0x%x: not implemented\\n\",\n\t\t\t\t    __func__, ib_cfg_name(which), val);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n \nstatic void init_vl_arb_caches(struct hfi1_pportdata *ppd)\n{\n\tint i;\n\n\tBUILD_BUG_ON(VL_ARB_TABLE_SIZE !=\n\t\t\tVL_ARB_LOW_PRIO_TABLE_SIZE);\n\tBUILD_BUG_ON(VL_ARB_TABLE_SIZE !=\n\t\t\tVL_ARB_HIGH_PRIO_TABLE_SIZE);\n\n\t \n\n\tfor (i = 0; i < MAX_PRIO_TABLE; i++)\n\t\tspin_lock_init(&ppd->vl_arb_cache[i].lock);\n}\n\n \nstatic inline struct vl_arb_cache *\nvl_arb_lock_cache(struct hfi1_pportdata *ppd, int idx)\n{\n\tif (idx != LO_PRIO_TABLE && idx != HI_PRIO_TABLE)\n\t\treturn NULL;\n\tspin_lock(&ppd->vl_arb_cache[idx].lock);\n\treturn &ppd->vl_arb_cache[idx];\n}\n\nstatic inline void vl_arb_unlock_cache(struct hfi1_pportdata *ppd, int idx)\n{\n\tspin_unlock(&ppd->vl_arb_cache[idx].lock);\n}\n\nstatic void vl_arb_get_cache(struct vl_arb_cache *cache,\n\t\t\t     struct ib_vl_weight_elem *vl)\n{\n\tmemcpy(vl, cache->table, VL_ARB_TABLE_SIZE * sizeof(*vl));\n}\n\nstatic void vl_arb_set_cache(struct vl_arb_cache *cache,\n\t\t\t     struct ib_vl_weight_elem *vl)\n{\n\tmemcpy(cache->table, vl, VL_ARB_TABLE_SIZE * sizeof(*vl));\n}\n\nstatic int vl_arb_match_cache(struct vl_arb_cache *cache,\n\t\t\t      struct ib_vl_weight_elem *vl)\n{\n\treturn !memcmp(cache->table, vl, VL_ARB_TABLE_SIZE * sizeof(*vl));\n}\n\n \n\nstatic int set_vl_weights(struct hfi1_pportdata *ppd, u32 target,\n\t\t\t  u32 size, struct ib_vl_weight_elem *vl)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 reg;\n\tunsigned int i, is_up = 0;\n\tint drain, ret = 0;\n\n\tmutex_lock(&ppd->hls_lock);\n\n\tif (ppd->host_link_state & HLS_UP)\n\t\tis_up = 1;\n\n\tdrain = !is_ax(dd) && is_up;\n\n\tif (drain)\n\t\t \n\t\tret = stop_drain_data_vls(dd);\n\n\tif (ret) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"%s: cannot stop/drain VLs - refusing to change VL arbitration weights\\n\",\n\t\t\t__func__);\n\t\tgoto err;\n\t}\n\n\tfor (i = 0; i < size; i++, vl++) {\n\t\t \n\t\treg = (((u64)vl->vl & SEND_LOW_PRIORITY_LIST_VL_MASK)\n\t\t\t\t<< SEND_LOW_PRIORITY_LIST_VL_SHIFT)\n\t\t      | (((u64)vl->weight\n\t\t\t\t& SEND_LOW_PRIORITY_LIST_WEIGHT_MASK)\n\t\t\t\t<< SEND_LOW_PRIORITY_LIST_WEIGHT_SHIFT);\n\t\twrite_csr(dd, target + (i * 8), reg);\n\t}\n\tpio_send_control(dd, PSC_GLOBAL_VLARB_ENABLE);\n\n\tif (drain)\n\t\topen_fill_data_vls(dd);  \n\nerr:\n\tmutex_unlock(&ppd->hls_lock);\n\n\treturn ret;\n}\n\n \nstatic void read_one_cm_vl(struct hfi1_devdata *dd, u32 csr,\n\t\t\t   struct vl_limit *vll)\n{\n\tu64 reg = read_csr(dd, csr);\n\n\tvll->dedicated = cpu_to_be16(\n\t\t(reg >> SEND_CM_CREDIT_VL_DEDICATED_LIMIT_VL_SHIFT)\n\t\t& SEND_CM_CREDIT_VL_DEDICATED_LIMIT_VL_MASK);\n\tvll->shared = cpu_to_be16(\n\t\t(reg >> SEND_CM_CREDIT_VL_SHARED_LIMIT_VL_SHIFT)\n\t\t& SEND_CM_CREDIT_VL_SHARED_LIMIT_VL_MASK);\n}\n\n \nstatic int get_buffer_control(struct hfi1_devdata *dd,\n\t\t\t      struct buffer_control *bc, u16 *overall_limit)\n{\n\tu64 reg;\n\tint i;\n\n\t \n\tmemset(bc, 0, sizeof(*bc));\n\n\t \n\tfor (i = 0; i < TXE_NUM_DATA_VL; i++)\n\t\tread_one_cm_vl(dd, SEND_CM_CREDIT_VL + (8 * i), &bc->vl[i]);\n\n\t \n\tread_one_cm_vl(dd, SEND_CM_CREDIT_VL15, &bc->vl[15]);\n\n\treg = read_csr(dd, SEND_CM_GLOBAL_CREDIT);\n\tbc->overall_shared_limit = cpu_to_be16(\n\t\t(reg >> SEND_CM_GLOBAL_CREDIT_SHARED_LIMIT_SHIFT)\n\t\t& SEND_CM_GLOBAL_CREDIT_SHARED_LIMIT_MASK);\n\tif (overall_limit)\n\t\t*overall_limit = (reg\n\t\t\t>> SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_SHIFT)\n\t\t\t& SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_MASK;\n\treturn sizeof(struct buffer_control);\n}\n\nstatic int get_sc2vlnt(struct hfi1_devdata *dd, struct sc2vlnt *dp)\n{\n\tu64 reg;\n\tint i;\n\n\t \n\treg = read_csr(dd, DCC_CFG_SC_VL_TABLE_15_0);\n\tfor (i = 0; i < sizeof(u64); i++) {\n\t\tu8 byte = *(((u8 *)&reg) + i);\n\n\t\tdp->vlnt[2 * i] = byte & 0xf;\n\t\tdp->vlnt[(2 * i) + 1] = (byte & 0xf0) >> 4;\n\t}\n\n\treg = read_csr(dd, DCC_CFG_SC_VL_TABLE_31_16);\n\tfor (i = 0; i < sizeof(u64); i++) {\n\t\tu8 byte = *(((u8 *)&reg) + i);\n\n\t\tdp->vlnt[16 + (2 * i)] = byte & 0xf;\n\t\tdp->vlnt[16 + (2 * i) + 1] = (byte & 0xf0) >> 4;\n\t}\n\treturn sizeof(struct sc2vlnt);\n}\n\nstatic void get_vlarb_preempt(struct hfi1_devdata *dd, u32 nelems,\n\t\t\t      struct ib_vl_weight_elem *vl)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < nelems; i++, vl++) {\n\t\tvl->vl = 0xf;\n\t\tvl->weight = 0;\n\t}\n}\n\nstatic void set_sc2vlnt(struct hfi1_devdata *dd, struct sc2vlnt *dp)\n{\n\twrite_csr(dd, DCC_CFG_SC_VL_TABLE_15_0,\n\t\t  DC_SC_VL_VAL(15_0,\n\t\t\t       0, dp->vlnt[0] & 0xf,\n\t\t\t       1, dp->vlnt[1] & 0xf,\n\t\t\t       2, dp->vlnt[2] & 0xf,\n\t\t\t       3, dp->vlnt[3] & 0xf,\n\t\t\t       4, dp->vlnt[4] & 0xf,\n\t\t\t       5, dp->vlnt[5] & 0xf,\n\t\t\t       6, dp->vlnt[6] & 0xf,\n\t\t\t       7, dp->vlnt[7] & 0xf,\n\t\t\t       8, dp->vlnt[8] & 0xf,\n\t\t\t       9, dp->vlnt[9] & 0xf,\n\t\t\t       10, dp->vlnt[10] & 0xf,\n\t\t\t       11, dp->vlnt[11] & 0xf,\n\t\t\t       12, dp->vlnt[12] & 0xf,\n\t\t\t       13, dp->vlnt[13] & 0xf,\n\t\t\t       14, dp->vlnt[14] & 0xf,\n\t\t\t       15, dp->vlnt[15] & 0xf));\n\twrite_csr(dd, DCC_CFG_SC_VL_TABLE_31_16,\n\t\t  DC_SC_VL_VAL(31_16,\n\t\t\t       16, dp->vlnt[16] & 0xf,\n\t\t\t       17, dp->vlnt[17] & 0xf,\n\t\t\t       18, dp->vlnt[18] & 0xf,\n\t\t\t       19, dp->vlnt[19] & 0xf,\n\t\t\t       20, dp->vlnt[20] & 0xf,\n\t\t\t       21, dp->vlnt[21] & 0xf,\n\t\t\t       22, dp->vlnt[22] & 0xf,\n\t\t\t       23, dp->vlnt[23] & 0xf,\n\t\t\t       24, dp->vlnt[24] & 0xf,\n\t\t\t       25, dp->vlnt[25] & 0xf,\n\t\t\t       26, dp->vlnt[26] & 0xf,\n\t\t\t       27, dp->vlnt[27] & 0xf,\n\t\t\t       28, dp->vlnt[28] & 0xf,\n\t\t\t       29, dp->vlnt[29] & 0xf,\n\t\t\t       30, dp->vlnt[30] & 0xf,\n\t\t\t       31, dp->vlnt[31] & 0xf));\n}\n\nstatic void nonzero_msg(struct hfi1_devdata *dd, int idx, const char *what,\n\t\t\tu16 limit)\n{\n\tif (limit != 0)\n\t\tdd_dev_info(dd, \"Invalid %s limit %d on VL %d, ignoring\\n\",\n\t\t\t    what, (int)limit, idx);\n}\n\n \nstatic void set_global_shared(struct hfi1_devdata *dd, u16 limit)\n{\n\tu64 reg;\n\n\treg = read_csr(dd, SEND_CM_GLOBAL_CREDIT);\n\treg &= ~SEND_CM_GLOBAL_CREDIT_SHARED_LIMIT_SMASK;\n\treg |= (u64)limit << SEND_CM_GLOBAL_CREDIT_SHARED_LIMIT_SHIFT;\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, reg);\n}\n\n \nstatic void set_global_limit(struct hfi1_devdata *dd, u16 limit)\n{\n\tu64 reg;\n\n\treg = read_csr(dd, SEND_CM_GLOBAL_CREDIT);\n\treg &= ~SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_SMASK;\n\treg |= (u64)limit << SEND_CM_GLOBAL_CREDIT_TOTAL_CREDIT_LIMIT_SHIFT;\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, reg);\n}\n\n \nstatic void set_vl_shared(struct hfi1_devdata *dd, int vl, u16 limit)\n{\n\tu64 reg;\n\tu32 addr;\n\n\tif (vl < TXE_NUM_DATA_VL)\n\t\taddr = SEND_CM_CREDIT_VL + (8 * vl);\n\telse\n\t\taddr = SEND_CM_CREDIT_VL15;\n\n\treg = read_csr(dd, addr);\n\treg &= ~SEND_CM_CREDIT_VL_SHARED_LIMIT_VL_SMASK;\n\treg |= (u64)limit << SEND_CM_CREDIT_VL_SHARED_LIMIT_VL_SHIFT;\n\twrite_csr(dd, addr, reg);\n}\n\n \nstatic void set_vl_dedicated(struct hfi1_devdata *dd, int vl, u16 limit)\n{\n\tu64 reg;\n\tu32 addr;\n\n\tif (vl < TXE_NUM_DATA_VL)\n\t\taddr = SEND_CM_CREDIT_VL + (8 * vl);\n\telse\n\t\taddr = SEND_CM_CREDIT_VL15;\n\n\treg = read_csr(dd, addr);\n\treg &= ~SEND_CM_CREDIT_VL_DEDICATED_LIMIT_VL_SMASK;\n\treg |= (u64)limit << SEND_CM_CREDIT_VL_DEDICATED_LIMIT_VL_SHIFT;\n\twrite_csr(dd, addr, reg);\n}\n\n \nstatic void wait_for_vl_status_clear(struct hfi1_devdata *dd, u64 mask,\n\t\t\t\t     const char *which)\n{\n\tunsigned long timeout;\n\tu64 reg;\n\n\ttimeout = jiffies + msecs_to_jiffies(VL_STATUS_CLEAR_TIMEOUT);\n\twhile (1) {\n\t\treg = read_csr(dd, SEND_CM_CREDIT_USED_STATUS) & mask;\n\n\t\tif (reg == 0)\n\t\t\treturn;\t \n\t\tif (time_after(jiffies, timeout))\n\t\t\tbreak;\t\t \n\t\tudelay(1);\n\t}\n\n\tdd_dev_err(dd,\n\t\t   \"%s credit change status not clearing after %dms, mask 0x%llx, not clear 0x%llx\\n\",\n\t\t   which, VL_STATUS_CLEAR_TIMEOUT, mask, reg);\n\t \n\tdd_dev_err(dd,\n\t\t   \"Continuing anyway.  A credit loss may occur.  Suggest a link bounce\\n\");\n}\n\n \nint set_buffer_control(struct hfi1_pportdata *ppd,\n\t\t       struct buffer_control *new_bc)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 changing_mask, ld_mask, stat_mask;\n\tint change_count;\n\tint i, use_all_mask;\n\tint this_shared_changing;\n\tint vl_count = 0, ret;\n\t \n\tint any_shared_limit_changing;\n\tstruct buffer_control cur_bc;\n\tu8 changing[OPA_MAX_VLS];\n\tu8 lowering_dedicated[OPA_MAX_VLS];\n\tu16 cur_total;\n\tu32 new_total = 0;\n\tconst u64 all_mask =\n\tSEND_CM_CREDIT_USED_STATUS_VL0_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL1_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL2_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL3_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL4_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL5_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL6_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL7_RETURN_CREDIT_STATUS_SMASK\n\t | SEND_CM_CREDIT_USED_STATUS_VL15_RETURN_CREDIT_STATUS_SMASK;\n\n#define valid_vl(idx) ((idx) < TXE_NUM_DATA_VL || (idx) == 15)\n#define NUM_USABLE_VLS 16\t \n\n\t \n\tfor (i = 0; i < OPA_MAX_VLS; i++) {\n\t\tif (valid_vl(i)) {\n\t\t\tnew_total += be16_to_cpu(new_bc->vl[i].dedicated);\n\t\t\tcontinue;\n\t\t}\n\t\tnonzero_msg(dd, i, \"dedicated\",\n\t\t\t    be16_to_cpu(new_bc->vl[i].dedicated));\n\t\tnonzero_msg(dd, i, \"shared\",\n\t\t\t    be16_to_cpu(new_bc->vl[i].shared));\n\t\tnew_bc->vl[i].dedicated = 0;\n\t\tnew_bc->vl[i].shared = 0;\n\t}\n\tnew_total += be16_to_cpu(new_bc->overall_shared_limit);\n\n\t \n\tget_buffer_control(dd, &cur_bc, &cur_total);\n\n\t \n\tmemset(changing, 0, sizeof(changing));\n\tmemset(lowering_dedicated, 0, sizeof(lowering_dedicated));\n\t \n\tstat_mask =\n\t\tSEND_CM_CREDIT_USED_STATUS_VL0_RETURN_CREDIT_STATUS_SMASK;\n\tchanging_mask = 0;\n\tld_mask = 0;\n\tchange_count = 0;\n\tany_shared_limit_changing = 0;\n\tfor (i = 0; i < NUM_USABLE_VLS; i++, stat_mask <<= 1) {\n\t\tif (!valid_vl(i))\n\t\t\tcontinue;\n\t\tthis_shared_changing = new_bc->vl[i].shared\n\t\t\t\t\t\t!= cur_bc.vl[i].shared;\n\t\tif (this_shared_changing)\n\t\t\tany_shared_limit_changing = 1;\n\t\tif (new_bc->vl[i].dedicated != cur_bc.vl[i].dedicated ||\n\t\t    this_shared_changing) {\n\t\t\tchanging[i] = 1;\n\t\t\tchanging_mask |= stat_mask;\n\t\t\tchange_count++;\n\t\t}\n\t\tif (be16_to_cpu(new_bc->vl[i].dedicated) <\n\t\t\t\t\tbe16_to_cpu(cur_bc.vl[i].dedicated)) {\n\t\t\tlowering_dedicated[i] = 1;\n\t\t\tld_mask |= stat_mask;\n\t\t}\n\t}\n\n\t \n\tif (new_total > cur_total)\n\t\tset_global_limit(dd, new_total);\n\n\t \n\tuse_all_mask = 0;\n\tif ((be16_to_cpu(new_bc->overall_shared_limit) <\n\t     be16_to_cpu(cur_bc.overall_shared_limit)) ||\n\t    (is_ax(dd) && any_shared_limit_changing)) {\n\t\tset_global_shared(dd, 0);\n\t\tcur_bc.overall_shared_limit = 0;\n\t\tuse_all_mask = 1;\n\t}\n\n\tfor (i = 0; i < NUM_USABLE_VLS; i++) {\n\t\tif (!valid_vl(i))\n\t\t\tcontinue;\n\n\t\tif (changing[i]) {\n\t\t\tset_vl_shared(dd, i, 0);\n\t\t\tcur_bc.vl[i].shared = 0;\n\t\t}\n\t}\n\n\twait_for_vl_status_clear(dd, use_all_mask ? all_mask : changing_mask,\n\t\t\t\t \"shared\");\n\n\tif (change_count > 0) {\n\t\tfor (i = 0; i < NUM_USABLE_VLS; i++) {\n\t\t\tif (!valid_vl(i))\n\t\t\t\tcontinue;\n\n\t\t\tif (lowering_dedicated[i]) {\n\t\t\t\tset_vl_dedicated(dd, i,\n\t\t\t\t\t\t be16_to_cpu(new_bc->\n\t\t\t\t\t\t\t     vl[i].dedicated));\n\t\t\t\tcur_bc.vl[i].dedicated =\n\t\t\t\t\t\tnew_bc->vl[i].dedicated;\n\t\t\t}\n\t\t}\n\n\t\twait_for_vl_status_clear(dd, ld_mask, \"dedicated\");\n\n\t\t \n\t\tfor (i = 0; i < NUM_USABLE_VLS; i++) {\n\t\t\tif (!valid_vl(i))\n\t\t\t\tcontinue;\n\n\t\t\tif (be16_to_cpu(new_bc->vl[i].dedicated) >\n\t\t\t\t\tbe16_to_cpu(cur_bc.vl[i].dedicated))\n\t\t\t\tset_vl_dedicated(dd, i,\n\t\t\t\t\t\t be16_to_cpu(new_bc->\n\t\t\t\t\t\t\t     vl[i].dedicated));\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < NUM_USABLE_VLS; i++) {\n\t\tif (!valid_vl(i))\n\t\t\tcontinue;\n\n\t\tif (be16_to_cpu(new_bc->vl[i].shared) >\n\t\t\t\tbe16_to_cpu(cur_bc.vl[i].shared))\n\t\t\tset_vl_shared(dd, i, be16_to_cpu(new_bc->vl[i].shared));\n\t}\n\n\t \n\tif (be16_to_cpu(new_bc->overall_shared_limit) >\n\t    be16_to_cpu(cur_bc.overall_shared_limit))\n\t\tset_global_shared(dd,\n\t\t\t\t  be16_to_cpu(new_bc->overall_shared_limit));\n\n\t \n\tif (new_total < cur_total)\n\t\tset_global_limit(dd, new_total);\n\n\t \n\tif (change_count > 0) {\n\t\tfor (i = 0; i < TXE_NUM_DATA_VL; i++)\n\t\t\tif (be16_to_cpu(new_bc->vl[i].dedicated) > 0 ||\n\t\t\t    be16_to_cpu(new_bc->vl[i].shared) > 0)\n\t\t\t\tvl_count++;\n\t\tppd->actual_vls_operational = vl_count;\n\t\tret = sdma_map_init(dd, ppd->port - 1, vl_count ?\n\t\t\t\t    ppd->actual_vls_operational :\n\t\t\t\t    ppd->vls_operational,\n\t\t\t\t    NULL);\n\t\tif (ret == 0)\n\t\t\tret = pio_map_init(dd, ppd->port - 1, vl_count ?\n\t\t\t\t\t   ppd->actual_vls_operational :\n\t\t\t\t\t   ppd->vls_operational, NULL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nint fm_get_table(struct hfi1_pportdata *ppd, int which, void *t)\n\n{\n\tint size;\n\tstruct vl_arb_cache *vlc;\n\n\tswitch (which) {\n\tcase FM_TBL_VL_HIGH_ARB:\n\t\tsize = 256;\n\t\t \n\t\tvlc = vl_arb_lock_cache(ppd, HI_PRIO_TABLE);\n\t\tvl_arb_get_cache(vlc, t);\n\t\tvl_arb_unlock_cache(ppd, HI_PRIO_TABLE);\n\t\tbreak;\n\tcase FM_TBL_VL_LOW_ARB:\n\t\tsize = 256;\n\t\t \n\t\tvlc = vl_arb_lock_cache(ppd, LO_PRIO_TABLE);\n\t\tvl_arb_get_cache(vlc, t);\n\t\tvl_arb_unlock_cache(ppd, LO_PRIO_TABLE);\n\t\tbreak;\n\tcase FM_TBL_BUFFER_CONTROL:\n\t\tsize = get_buffer_control(ppd->dd, t, NULL);\n\t\tbreak;\n\tcase FM_TBL_SC2VLNT:\n\t\tsize = get_sc2vlnt(ppd->dd, t);\n\t\tbreak;\n\tcase FM_TBL_VL_PREEMPT_ELEMS:\n\t\tsize = 256;\n\t\t \n\t\tget_vlarb_preempt(ppd->dd, OPA_MAX_VLS, t);\n\t\tbreak;\n\tcase FM_TBL_VL_PREEMPT_MATRIX:\n\t\tsize = 256;\n\t\t \n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn size;\n}\n\n \nint fm_set_table(struct hfi1_pportdata *ppd, int which, void *t)\n{\n\tint ret = 0;\n\tstruct vl_arb_cache *vlc;\n\n\tswitch (which) {\n\tcase FM_TBL_VL_HIGH_ARB:\n\t\tvlc = vl_arb_lock_cache(ppd, HI_PRIO_TABLE);\n\t\tif (vl_arb_match_cache(vlc, t)) {\n\t\t\tvl_arb_unlock_cache(ppd, HI_PRIO_TABLE);\n\t\t\tbreak;\n\t\t}\n\t\tvl_arb_set_cache(vlc, t);\n\t\tvl_arb_unlock_cache(ppd, HI_PRIO_TABLE);\n\t\tret = set_vl_weights(ppd, SEND_HIGH_PRIORITY_LIST,\n\t\t\t\t     VL_ARB_HIGH_PRIO_TABLE_SIZE, t);\n\t\tbreak;\n\tcase FM_TBL_VL_LOW_ARB:\n\t\tvlc = vl_arb_lock_cache(ppd, LO_PRIO_TABLE);\n\t\tif (vl_arb_match_cache(vlc, t)) {\n\t\t\tvl_arb_unlock_cache(ppd, LO_PRIO_TABLE);\n\t\t\tbreak;\n\t\t}\n\t\tvl_arb_set_cache(vlc, t);\n\t\tvl_arb_unlock_cache(ppd, LO_PRIO_TABLE);\n\t\tret = set_vl_weights(ppd, SEND_LOW_PRIORITY_LIST,\n\t\t\t\t     VL_ARB_LOW_PRIO_TABLE_SIZE, t);\n\t\tbreak;\n\tcase FM_TBL_BUFFER_CONTROL:\n\t\tret = set_buffer_control(ppd, t);\n\t\tbreak;\n\tcase FM_TBL_SC2VLNT:\n\t\tset_sc2vlnt(ppd->dd, t);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\treturn ret;\n}\n\n \nstatic int disable_data_vls(struct hfi1_devdata *dd)\n{\n\tif (is_ax(dd))\n\t\treturn 1;\n\n\tpio_send_control(dd, PSC_DATA_VL_DISABLE);\n\n\treturn 0;\n}\n\n \nint open_fill_data_vls(struct hfi1_devdata *dd)\n{\n\tif (is_ax(dd))\n\t\treturn 1;\n\n\tpio_send_control(dd, PSC_DATA_VL_ENABLE);\n\n\treturn 0;\n}\n\n \nstatic void drain_data_vls(struct hfi1_devdata *dd)\n{\n\tsc_wait(dd);\n\tsdma_wait(dd);\n\tpause_for_credit_return(dd);\n}\n\n \nint stop_drain_data_vls(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\tret = disable_data_vls(dd);\n\tif (ret == 0)\n\t\tdrain_data_vls(dd);\n\n\treturn ret;\n}\n\n \nu32 ns_to_cclock(struct hfi1_devdata *dd, u32 ns)\n{\n\tu32 cclocks;\n\n\tif (dd->icode == ICODE_FPGA_EMULATION)\n\t\tcclocks = (ns * 1000) / FPGA_CCLOCK_PS;\n\telse   \n\t\tcclocks = (ns * 1000) / ASIC_CCLOCK_PS;\n\tif (ns && !cclocks)\t \n\t\tcclocks = 1;\n\treturn cclocks;\n}\n\n \nu32 cclock_to_ns(struct hfi1_devdata *dd, u32 cclocks)\n{\n\tu32 ns;\n\n\tif (dd->icode == ICODE_FPGA_EMULATION)\n\t\tns = (cclocks * FPGA_CCLOCK_PS) / 1000;\n\telse   \n\t\tns = (cclocks * ASIC_CCLOCK_PS) / 1000;\n\tif (cclocks && !ns)\n\t\tns = 1;\n\treturn ns;\n}\n\n \nstatic void adjust_rcv_timeout(struct hfi1_ctxtdata *rcd, u32 npkts)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu32 timeout = rcd->rcvavail_timeout;\n\n\t \n\tif (npkts < rcv_intr_count) {\n\t\t \n\t\tif (timeout < 2)  \n\t\t\treturn;\n\t\ttimeout >>= 1;\n\t} else {\n\t\t \n\t\tif (timeout >= dd->rcv_intr_timeout_csr)  \n\t\t\treturn;\n\t\ttimeout = min(timeout << 1, dd->rcv_intr_timeout_csr);\n\t}\n\n\trcd->rcvavail_timeout = timeout;\n\t \n\twrite_kctxt_csr(dd, rcd->ctxt, RCV_AVAIL_TIME_OUT,\n\t\t\t(u64)timeout <<\n\t\t\tRCV_AVAIL_TIME_OUT_TIME_OUT_RELOAD_SHIFT);\n}\n\nvoid update_usrhead(struct hfi1_ctxtdata *rcd, u32 hd, u32 updegr, u32 egrhd,\n\t\t    u32 intr_adjust, u32 npkts)\n{\n\tstruct hfi1_devdata *dd = rcd->dd;\n\tu64 reg;\n\tu32 ctxt = rcd->ctxt;\n\n\t \n\tif (intr_adjust)\n\t\tadjust_rcv_timeout(rcd, npkts);\n\tif (updegr) {\n\t\treg = (egrhd & RCV_EGR_INDEX_HEAD_HEAD_MASK)\n\t\t\t<< RCV_EGR_INDEX_HEAD_HEAD_SHIFT;\n\t\twrite_uctxt_csr(dd, ctxt, RCV_EGR_INDEX_HEAD, reg);\n\t}\n\treg = ((u64)rcv_intr_count << RCV_HDR_HEAD_COUNTER_SHIFT) |\n\t\t(((u64)hd & RCV_HDR_HEAD_HEAD_MASK)\n\t\t\t<< RCV_HDR_HEAD_HEAD_SHIFT);\n\twrite_uctxt_csr(dd, ctxt, RCV_HDR_HEAD, reg);\n}\n\nu32 hdrqempty(struct hfi1_ctxtdata *rcd)\n{\n\tu32 head, tail;\n\n\thead = (read_uctxt_csr(rcd->dd, rcd->ctxt, RCV_HDR_HEAD)\n\t\t& RCV_HDR_HEAD_HEAD_SMASK) >> RCV_HDR_HEAD_HEAD_SHIFT;\n\n\tif (hfi1_rcvhdrtail_kvaddr(rcd))\n\t\ttail = get_rcvhdrtail(rcd);\n\telse\n\t\ttail = read_uctxt_csr(rcd->dd, rcd->ctxt, RCV_HDR_TAIL);\n\n\treturn head == tail;\n}\n\n \nstatic u32 encoded_size(u32 size)\n{\n\tswitch (size) {\n\tcase   4 * 1024: return 0x1;\n\tcase   8 * 1024: return 0x2;\n\tcase  16 * 1024: return 0x3;\n\tcase  32 * 1024: return 0x4;\n\tcase  64 * 1024: return 0x5;\n\tcase 128 * 1024: return 0x6;\n\tcase 256 * 1024: return 0x7;\n\tcase 512 * 1024: return 0x8;\n\tcase   1 * 1024 * 1024: return 0x9;\n\tcase   2 * 1024 * 1024: return 0xa;\n\t}\n\treturn 0x1;\t \n}\n\n \nu8 encode_rcv_header_entry_size(u8 size)\n{\n\t \n\tif (size == 2)\n\t\treturn 1;\n\tif (size == 16)\n\t\treturn 2;\n\tif (size == 32)\n\t\treturn 4;\n\treturn 0;  \n}\n\n \nint hfi1_validate_rcvhdrcnt(struct hfi1_devdata *dd, uint thecnt)\n{\n\tif (thecnt <= HFI1_MIN_HDRQ_EGRBUF_CNT) {\n\t\tdd_dev_err(dd, \"Receive header queue count too small\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (thecnt > HFI1_MAX_HDRQ_EGRBUF_CNT) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Receive header queue count cannot be greater than %u\\n\",\n\t\t\t   HFI1_MAX_HDRQ_EGRBUF_CNT);\n\t\treturn -EINVAL;\n\t}\n\n\tif (thecnt % HDRQ_INCREMENT) {\n\t\tdd_dev_err(dd, \"Receive header queue count %d must be divisible by %lu\\n\",\n\t\t\t   thecnt, HDRQ_INCREMENT);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nvoid set_hdrq_regs(struct hfi1_devdata *dd, u8 ctxt, u8 entsize, u16 hdrcnt)\n{\n\tu64 reg;\n\n\treg = (((u64)hdrcnt >> HDRQ_SIZE_SHIFT) & RCV_HDR_CNT_CNT_MASK) <<\n\t      RCV_HDR_CNT_CNT_SHIFT;\n\twrite_kctxt_csr(dd, ctxt, RCV_HDR_CNT, reg);\n\treg = ((u64)encode_rcv_header_entry_size(entsize) &\n\t       RCV_HDR_ENT_SIZE_ENT_SIZE_MASK) <<\n\t      RCV_HDR_ENT_SIZE_ENT_SIZE_SHIFT;\n\twrite_kctxt_csr(dd, ctxt, RCV_HDR_ENT_SIZE, reg);\n\treg = ((u64)DEFAULT_RCVHDRSIZE & RCV_HDR_SIZE_HDR_SIZE_MASK) <<\n\t      RCV_HDR_SIZE_HDR_SIZE_SHIFT;\n\twrite_kctxt_csr(dd, ctxt, RCV_HDR_SIZE, reg);\n\n\t \n\twrite_kctxt_csr(dd, ctxt, RCV_HDR_TAIL_ADDR,\n\t\t\tdd->rcvhdrtail_dummy_dma);\n}\n\nvoid hfi1_rcvctrl(struct hfi1_devdata *dd, unsigned int op,\n\t\t  struct hfi1_ctxtdata *rcd)\n{\n\tu64 rcvctrl, reg;\n\tint did_enable = 0;\n\tu16 ctxt;\n\n\tif (!rcd)\n\t\treturn;\n\n\tctxt = rcd->ctxt;\n\n\thfi1_cdbg(RCVCTRL, \"ctxt %d op 0x%x\", ctxt, op);\n\n\trcvctrl = read_kctxt_csr(dd, ctxt, RCV_CTXT_CTRL);\n\t \n\tif ((op & HFI1_RCVCTRL_CTXT_ENB) &&\n\t    !(rcvctrl & RCV_CTXT_CTRL_ENABLE_SMASK)) {\n\t\t \n\t\twrite_kctxt_csr(dd, ctxt, RCV_HDR_ADDR,\n\t\t\t\trcd->rcvhdrq_dma);\n\t\tif (hfi1_rcvhdrtail_kvaddr(rcd))\n\t\t\twrite_kctxt_csr(dd, ctxt, RCV_HDR_TAIL_ADDR,\n\t\t\t\t\trcd->rcvhdrqtailaddr_dma);\n\t\thfi1_set_seq_cnt(rcd, 1);\n\n\t\t \n\t\thfi1_set_rcd_head(rcd, 0);\n\n\t\t \n\t\tmemset(rcd->rcvhdrq, 0, rcvhdrq_size(rcd));\n\n\t\t \n\t\trcd->rcvavail_timeout = dd->rcv_intr_timeout_csr;\n\n\t\t \n\t\trcvctrl |= RCV_CTXT_CTRL_ENABLE_SMASK;\n\n\t\t \n\t\trcvctrl &= ~RCV_CTXT_CTRL_EGR_BUF_SIZE_SMASK;\n\t\trcvctrl |= ((u64)encoded_size(rcd->egrbufs.rcvtid_size)\n\t\t\t\t& RCV_CTXT_CTRL_EGR_BUF_SIZE_MASK)\n\t\t\t\t\t<< RCV_CTXT_CTRL_EGR_BUF_SIZE_SHIFT;\n\n\t\t \n\t\twrite_uctxt_csr(dd, ctxt, RCV_HDR_HEAD, 0);\n\t\tdid_enable = 1;\n\n\t\t \n\t\twrite_uctxt_csr(dd, ctxt, RCV_EGR_INDEX_HEAD, 0);\n\n\t\t \n\t\treg = (((u64)(rcd->egrbufs.alloced >> RCV_SHIFT)\n\t\t\t& RCV_EGR_CTRL_EGR_CNT_MASK)\n\t\t       << RCV_EGR_CTRL_EGR_CNT_SHIFT) |\n\t\t\t(((rcd->eager_base >> RCV_SHIFT)\n\t\t\t  & RCV_EGR_CTRL_EGR_BASE_INDEX_MASK)\n\t\t\t << RCV_EGR_CTRL_EGR_BASE_INDEX_SHIFT);\n\t\twrite_kctxt_csr(dd, ctxt, RCV_EGR_CTRL, reg);\n\n\t\t \n\t\treg = (((rcd->expected_count >> RCV_SHIFT)\n\t\t\t\t\t& RCV_TID_CTRL_TID_PAIR_CNT_MASK)\n\t\t\t\t<< RCV_TID_CTRL_TID_PAIR_CNT_SHIFT) |\n\t\t      (((rcd->expected_base >> RCV_SHIFT)\n\t\t\t\t\t& RCV_TID_CTRL_TID_BASE_INDEX_MASK)\n\t\t\t\t<< RCV_TID_CTRL_TID_BASE_INDEX_SHIFT);\n\t\twrite_kctxt_csr(dd, ctxt, RCV_TID_CTRL, reg);\n\t\tif (ctxt == HFI1_CTRL_CTXT)\n\t\t\twrite_csr(dd, RCV_VL15, HFI1_CTRL_CTXT);\n\t}\n\tif (op & HFI1_RCVCTRL_CTXT_DIS) {\n\t\twrite_csr(dd, RCV_VL15, 0);\n\t\t \n\t\tif (dd->rcvhdrtail_dummy_dma) {\n\t\t\twrite_kctxt_csr(dd, ctxt, RCV_HDR_TAIL_ADDR,\n\t\t\t\t\tdd->rcvhdrtail_dummy_dma);\n\t\t\t \n\t\t\trcvctrl |= RCV_CTXT_CTRL_TAIL_UPD_SMASK;\n\t\t}\n\n\t\trcvctrl &= ~RCV_CTXT_CTRL_ENABLE_SMASK;\n\t}\n\tif (op & HFI1_RCVCTRL_INTRAVAIL_ENB) {\n\t\tset_intr_bits(dd, IS_RCVAVAIL_START + rcd->ctxt,\n\t\t\t      IS_RCVAVAIL_START + rcd->ctxt, true);\n\t\trcvctrl |= RCV_CTXT_CTRL_INTR_AVAIL_SMASK;\n\t}\n\tif (op & HFI1_RCVCTRL_INTRAVAIL_DIS) {\n\t\tset_intr_bits(dd, IS_RCVAVAIL_START + rcd->ctxt,\n\t\t\t      IS_RCVAVAIL_START + rcd->ctxt, false);\n\t\trcvctrl &= ~RCV_CTXT_CTRL_INTR_AVAIL_SMASK;\n\t}\n\tif ((op & HFI1_RCVCTRL_TAILUPD_ENB) && hfi1_rcvhdrtail_kvaddr(rcd))\n\t\trcvctrl |= RCV_CTXT_CTRL_TAIL_UPD_SMASK;\n\tif (op & HFI1_RCVCTRL_TAILUPD_DIS) {\n\t\t \n\t\tif (!(op & HFI1_RCVCTRL_CTXT_DIS))\n\t\t\trcvctrl &= ~RCV_CTXT_CTRL_TAIL_UPD_SMASK;\n\t}\n\tif (op & HFI1_RCVCTRL_TIDFLOW_ENB)\n\t\trcvctrl |= RCV_CTXT_CTRL_TID_FLOW_ENABLE_SMASK;\n\tif (op & HFI1_RCVCTRL_TIDFLOW_DIS)\n\t\trcvctrl &= ~RCV_CTXT_CTRL_TID_FLOW_ENABLE_SMASK;\n\tif (op & HFI1_RCVCTRL_ONE_PKT_EGR_ENB) {\n\t\t \n\t\trcvctrl &= ~RCV_CTXT_CTRL_EGR_BUF_SIZE_SMASK;\n\t\trcvctrl |= RCV_CTXT_CTRL_ONE_PACKET_PER_EGR_BUFFER_SMASK;\n\t}\n\tif (op & HFI1_RCVCTRL_ONE_PKT_EGR_DIS)\n\t\trcvctrl &= ~RCV_CTXT_CTRL_ONE_PACKET_PER_EGR_BUFFER_SMASK;\n\tif (op & HFI1_RCVCTRL_NO_RHQ_DROP_ENB)\n\t\trcvctrl |= RCV_CTXT_CTRL_DONT_DROP_RHQ_FULL_SMASK;\n\tif (op & HFI1_RCVCTRL_NO_RHQ_DROP_DIS)\n\t\trcvctrl &= ~RCV_CTXT_CTRL_DONT_DROP_RHQ_FULL_SMASK;\n\tif (op & HFI1_RCVCTRL_NO_EGR_DROP_ENB)\n\t\trcvctrl |= RCV_CTXT_CTRL_DONT_DROP_EGR_FULL_SMASK;\n\tif (op & HFI1_RCVCTRL_NO_EGR_DROP_DIS)\n\t\trcvctrl &= ~RCV_CTXT_CTRL_DONT_DROP_EGR_FULL_SMASK;\n\tif (op & HFI1_RCVCTRL_URGENT_ENB)\n\t\tset_intr_bits(dd, IS_RCVURGENT_START + rcd->ctxt,\n\t\t\t      IS_RCVURGENT_START + rcd->ctxt, true);\n\tif (op & HFI1_RCVCTRL_URGENT_DIS)\n\t\tset_intr_bits(dd, IS_RCVURGENT_START + rcd->ctxt,\n\t\t\t      IS_RCVURGENT_START + rcd->ctxt, false);\n\n\thfi1_cdbg(RCVCTRL, \"ctxt %d rcvctrl 0x%llx\", ctxt, rcvctrl);\n\twrite_kctxt_csr(dd, ctxt, RCV_CTXT_CTRL, rcvctrl);\n\n\t \n\tif (did_enable &&\n\t    (rcvctrl & RCV_CTXT_CTRL_DONT_DROP_RHQ_FULL_SMASK)) {\n\t\treg = read_kctxt_csr(dd, ctxt, RCV_CTXT_STATUS);\n\t\tif (reg != 0) {\n\t\t\tdd_dev_info(dd, \"ctxt %d status %lld (blocked)\\n\",\n\t\t\t\t    ctxt, reg);\n\t\t\tread_uctxt_csr(dd, ctxt, RCV_HDR_HEAD);\n\t\t\twrite_uctxt_csr(dd, ctxt, RCV_HDR_HEAD, 0x10);\n\t\t\twrite_uctxt_csr(dd, ctxt, RCV_HDR_HEAD, 0x00);\n\t\t\tread_uctxt_csr(dd, ctxt, RCV_HDR_HEAD);\n\t\t\treg = read_kctxt_csr(dd, ctxt, RCV_CTXT_STATUS);\n\t\t\tdd_dev_info(dd, \"ctxt %d status %lld (%s blocked)\\n\",\n\t\t\t\t    ctxt, reg, reg == 0 ? \"not\" : \"still\");\n\t\t}\n\t}\n\n\tif (did_enable) {\n\t\t \n\t\t \n\t\twrite_kctxt_csr(dd, ctxt, RCV_AVAIL_TIME_OUT,\n\t\t\t\t(u64)rcd->rcvavail_timeout <<\n\t\t\t\tRCV_AVAIL_TIME_OUT_TIME_OUT_RELOAD_SHIFT);\n\n\t\t \n\t\treg = (u64)rcv_intr_count << RCV_HDR_HEAD_COUNTER_SHIFT;\n\t\twrite_uctxt_csr(dd, ctxt, RCV_HDR_HEAD, reg);\n\t}\n\n\tif (op & (HFI1_RCVCTRL_TAILUPD_DIS | HFI1_RCVCTRL_CTXT_DIS))\n\t\t \n\t\twrite_kctxt_csr(dd, ctxt, RCV_HDR_TAIL_ADDR,\n\t\t\t\tdd->rcvhdrtail_dummy_dma);\n}\n\nu32 hfi1_read_cntrs(struct hfi1_devdata *dd, char **namep, u64 **cntrp)\n{\n\tint ret;\n\tu64 val = 0;\n\n\tif (namep) {\n\t\tret = dd->cntrnameslen;\n\t\t*namep = dd->cntrnames;\n\t} else {\n\t\tconst struct cntr_entry *entry;\n\t\tint i, j;\n\n\t\tret = (dd->ndevcntrs) * sizeof(u64);\n\n\t\t \n\t\t*cntrp = dd->cntrs;\n\n\t\t \n\t\tfor (i = 0; i < DEV_CNTR_LAST; i++) {\n\t\t\tentry = &dev_cntrs[i];\n\t\t\thfi1_cdbg(CNTR, \"reading %s\", entry->name);\n\t\t\tif (entry->flags & CNTR_DISABLED) {\n\t\t\t\t \n\t\t\t\thfi1_cdbg(CNTR, \"\\tDisabled\");\n\t\t\t} else {\n\t\t\t\tif (entry->flags & CNTR_VL) {\n\t\t\t\t\thfi1_cdbg(CNTR, \"\\tPer VL\");\n\t\t\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\t\t\tval = entry->rw_cntr(entry,\n\t\t\t\t\t\t\t\t  dd, j,\n\t\t\t\t\t\t\t\t  CNTR_MODE_R,\n\t\t\t\t\t\t\t\t  0);\n\t\t\t\t\t\thfi1_cdbg(\n\t\t\t\t\t\t   CNTR,\n\t\t\t\t\t\t   \"\\t\\tRead 0x%llx for %d\",\n\t\t\t\t\t\t   val, j);\n\t\t\t\t\t\tdd->cntrs[entry->offset + j] =\n\t\t\t\t\t\t\t\t\t    val;\n\t\t\t\t\t}\n\t\t\t\t} else if (entry->flags & CNTR_SDMA) {\n\t\t\t\t\thfi1_cdbg(CNTR,\n\t\t\t\t\t\t  \"\\t Per SDMA Engine\");\n\t\t\t\t\tfor (j = 0; j < chip_sdma_engines(dd);\n\t\t\t\t\t     j++) {\n\t\t\t\t\t\tval =\n\t\t\t\t\t\tentry->rw_cntr(entry, dd, j,\n\t\t\t\t\t\t\t       CNTR_MODE_R, 0);\n\t\t\t\t\t\thfi1_cdbg(CNTR,\n\t\t\t\t\t\t\t  \"\\t\\tRead 0x%llx for %d\",\n\t\t\t\t\t\t\t  val, j);\n\t\t\t\t\t\tdd->cntrs[entry->offset + j] =\n\t\t\t\t\t\t\t\t\tval;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tval = entry->rw_cntr(entry, dd,\n\t\t\t\t\t\t\tCNTR_INVALID_VL,\n\t\t\t\t\t\t\tCNTR_MODE_R, 0);\n\t\t\t\t\tdd->cntrs[entry->offset] = val;\n\t\t\t\t\thfi1_cdbg(CNTR, \"\\tRead 0x%llx\", val);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nu32 hfi1_read_portcntrs(struct hfi1_pportdata *ppd, char **namep, u64 **cntrp)\n{\n\tint ret;\n\tu64 val = 0;\n\n\tif (namep) {\n\t\tret = ppd->dd->portcntrnameslen;\n\t\t*namep = ppd->dd->portcntrnames;\n\t} else {\n\t\tconst struct cntr_entry *entry;\n\t\tint i, j;\n\n\t\tret = ppd->dd->nportcntrs * sizeof(u64);\n\t\t*cntrp = ppd->cntrs;\n\n\t\tfor (i = 0; i < PORT_CNTR_LAST; i++) {\n\t\t\tentry = &port_cntrs[i];\n\t\t\thfi1_cdbg(CNTR, \"reading %s\", entry->name);\n\t\t\tif (entry->flags & CNTR_DISABLED) {\n\t\t\t\t \n\t\t\t\thfi1_cdbg(CNTR, \"\\tDisabled\");\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (entry->flags & CNTR_VL) {\n\t\t\t\thfi1_cdbg(CNTR, \"\\tPer VL\");\n\t\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\t\tval = entry->rw_cntr(entry, ppd, j,\n\t\t\t\t\t\t\t       CNTR_MODE_R,\n\t\t\t\t\t\t\t       0);\n\t\t\t\t\thfi1_cdbg(\n\t\t\t\t\t   CNTR,\n\t\t\t\t\t   \"\\t\\tRead 0x%llx for %d\",\n\t\t\t\t\t   val, j);\n\t\t\t\t\tppd->cntrs[entry->offset + j] = val;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tval = entry->rw_cntr(entry, ppd,\n\t\t\t\t\t\t       CNTR_INVALID_VL,\n\t\t\t\t\t\t       CNTR_MODE_R,\n\t\t\t\t\t\t       0);\n\t\t\t\tppd->cntrs[entry->offset] = val;\n\t\t\t\thfi1_cdbg(CNTR, \"\\tRead 0x%llx\", val);\n\t\t\t}\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void free_cntrs(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd;\n\tint i;\n\n\tif (dd->synth_stats_timer.function)\n\t\tdel_timer_sync(&dd->synth_stats_timer);\n\tcancel_work_sync(&dd->update_cntr_work);\n\tppd = (struct hfi1_pportdata *)(dd + 1);\n\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\tkfree(ppd->cntrs);\n\t\tkfree(ppd->scntrs);\n\t\tfree_percpu(ppd->ibport_data.rvp.rc_acks);\n\t\tfree_percpu(ppd->ibport_data.rvp.rc_qacks);\n\t\tfree_percpu(ppd->ibport_data.rvp.rc_delayed_comp);\n\t\tppd->cntrs = NULL;\n\t\tppd->scntrs = NULL;\n\t\tppd->ibport_data.rvp.rc_acks = NULL;\n\t\tppd->ibport_data.rvp.rc_qacks = NULL;\n\t\tppd->ibport_data.rvp.rc_delayed_comp = NULL;\n\t}\n\tkfree(dd->portcntrnames);\n\tdd->portcntrnames = NULL;\n\tkfree(dd->cntrs);\n\tdd->cntrs = NULL;\n\tkfree(dd->scntrs);\n\tdd->scntrs = NULL;\n\tkfree(dd->cntrnames);\n\tdd->cntrnames = NULL;\n\tif (dd->update_cntr_wq) {\n\t\tdestroy_workqueue(dd->update_cntr_wq);\n\t\tdd->update_cntr_wq = NULL;\n\t}\n}\n\nstatic u64 read_dev_port_cntr(struct hfi1_devdata *dd, struct cntr_entry *entry,\n\t\t\t      u64 *psval, void *context, int vl)\n{\n\tu64 val;\n\tu64 sval = *psval;\n\n\tif (entry->flags & CNTR_DISABLED) {\n\t\tdd_dev_err(dd, \"Counter %s not enabled\", entry->name);\n\t\treturn 0;\n\t}\n\n\thfi1_cdbg(CNTR, \"cntr: %s vl %d psval 0x%llx\", entry->name, vl, *psval);\n\n\tval = entry->rw_cntr(entry, context, vl, CNTR_MODE_R, 0);\n\n\t \n\tif (entry->flags & CNTR_SYNTH) {\n\t\tif (sval == CNTR_MAX) {\n\t\t\t \n\t\t\treturn CNTR_MAX;\n\t\t}\n\n\t\tif (entry->flags & CNTR_32BIT) {\n\t\t\t \n\t\t\tu64 upper = sval >> 32;\n\t\t\tu64 lower = (sval << 32) >> 32;\n\n\t\t\tif (lower > val) {  \n\t\t\t\tif (upper == CNTR_32BIT_MAX)\n\t\t\t\t\tval = CNTR_MAX;\n\t\t\t\telse\n\t\t\t\t\tupper++;\n\t\t\t}\n\n\t\t\tif (val != CNTR_MAX)\n\t\t\t\tval = (upper << 32) | val;\n\n\t\t} else {\n\t\t\t \n\t\t\tif ((val < sval) || (val > CNTR_MAX))\n\t\t\t\tval = CNTR_MAX;\n\t\t}\n\t}\n\n\t*psval = val;\n\n\thfi1_cdbg(CNTR, \"\\tNew val=0x%llx\", val);\n\n\treturn val;\n}\n\nstatic u64 write_dev_port_cntr(struct hfi1_devdata *dd,\n\t\t\t       struct cntr_entry *entry,\n\t\t\t       u64 *psval, void *context, int vl, u64 data)\n{\n\tu64 val;\n\n\tif (entry->flags & CNTR_DISABLED) {\n\t\tdd_dev_err(dd, \"Counter %s not enabled\", entry->name);\n\t\treturn 0;\n\t}\n\n\thfi1_cdbg(CNTR, \"cntr: %s vl %d psval 0x%llx\", entry->name, vl, *psval);\n\n\tif (entry->flags & CNTR_SYNTH) {\n\t\t*psval = data;\n\t\tif (entry->flags & CNTR_32BIT) {\n\t\t\tval = entry->rw_cntr(entry, context, vl, CNTR_MODE_W,\n\t\t\t\t\t     (data << 32) >> 32);\n\t\t\tval = data;  \n\t\t} else {\n\t\t\tval = entry->rw_cntr(entry, context, vl, CNTR_MODE_W,\n\t\t\t\t\t     data);\n\t\t}\n\t} else {\n\t\tval = entry->rw_cntr(entry, context, vl, CNTR_MODE_W, data);\n\t}\n\n\t*psval = val;\n\n\thfi1_cdbg(CNTR, \"\\tNew val=0x%llx\", val);\n\n\treturn val;\n}\n\nu64 read_dev_cntr(struct hfi1_devdata *dd, int index, int vl)\n{\n\tstruct cntr_entry *entry;\n\tu64 *sval;\n\n\tentry = &dev_cntrs[index];\n\tsval = dd->scntrs + entry->offset;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\tsval += vl;\n\n\treturn read_dev_port_cntr(dd, entry, sval, dd, vl);\n}\n\nu64 write_dev_cntr(struct hfi1_devdata *dd, int index, int vl, u64 data)\n{\n\tstruct cntr_entry *entry;\n\tu64 *sval;\n\n\tentry = &dev_cntrs[index];\n\tsval = dd->scntrs + entry->offset;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\tsval += vl;\n\n\treturn write_dev_port_cntr(dd, entry, sval, dd, vl, data);\n}\n\nu64 read_port_cntr(struct hfi1_pportdata *ppd, int index, int vl)\n{\n\tstruct cntr_entry *entry;\n\tu64 *sval;\n\n\tentry = &port_cntrs[index];\n\tsval = ppd->scntrs + entry->offset;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\tsval += vl;\n\n\tif ((index >= C_RCV_HDR_OVF_FIRST + ppd->dd->num_rcv_contexts) &&\n\t    (index <= C_RCV_HDR_OVF_LAST)) {\n\t\t \n\t\treturn 0;\n\t}\n\n\treturn read_dev_port_cntr(ppd->dd, entry, sval, ppd, vl);\n}\n\nu64 write_port_cntr(struct hfi1_pportdata *ppd, int index, int vl, u64 data)\n{\n\tstruct cntr_entry *entry;\n\tu64 *sval;\n\n\tentry = &port_cntrs[index];\n\tsval = ppd->scntrs + entry->offset;\n\n\tif (vl != CNTR_INVALID_VL)\n\t\tsval += vl;\n\n\tif ((index >= C_RCV_HDR_OVF_FIRST + ppd->dd->num_rcv_contexts) &&\n\t    (index <= C_RCV_HDR_OVF_LAST)) {\n\t\t \n\t\treturn 0;\n\t}\n\n\treturn write_dev_port_cntr(ppd->dd, entry, sval, ppd, vl, data);\n}\n\nstatic void do_update_synth_timer(struct work_struct *work)\n{\n\tu64 cur_tx;\n\tu64 cur_rx;\n\tu64 total_flits;\n\tu8 update = 0;\n\tint i, j, vl;\n\tstruct hfi1_pportdata *ppd;\n\tstruct cntr_entry *entry;\n\tstruct hfi1_devdata *dd = container_of(work, struct hfi1_devdata,\n\t\t\t\t\t       update_cntr_work);\n\n\t \n\tentry = &dev_cntrs[C_DC_RCV_FLITS];\n\tcur_rx = entry->rw_cntr(entry, dd, CNTR_INVALID_VL, CNTR_MODE_R, 0);\n\n\tentry = &dev_cntrs[C_DC_XMIT_FLITS];\n\tcur_tx = entry->rw_cntr(entry, dd, CNTR_INVALID_VL, CNTR_MODE_R, 0);\n\n\thfi1_cdbg(\n\t    CNTR,\n\t    \"[%d] curr tx=0x%llx rx=0x%llx :: last tx=0x%llx rx=0x%llx\",\n\t    dd->unit, cur_tx, cur_rx, dd->last_tx, dd->last_rx);\n\n\tif ((cur_tx < dd->last_tx) || (cur_rx < dd->last_rx)) {\n\t\t \n\t\tupdate = 1;\n\t\thfi1_cdbg(CNTR, \"[%d] Tripwire counter rolled, updating\",\n\t\t\t  dd->unit);\n\t} else {\n\t\ttotal_flits = (cur_tx - dd->last_tx) + (cur_rx - dd->last_rx);\n\t\thfi1_cdbg(CNTR,\n\t\t\t  \"[%d] total flits 0x%llx limit 0x%llx\", dd->unit,\n\t\t\t  total_flits, (u64)CNTR_32BIT_MAX);\n\t\tif (total_flits >= CNTR_32BIT_MAX) {\n\t\t\thfi1_cdbg(CNTR, \"[%d] 32bit limit hit, updating\",\n\t\t\t\t  dd->unit);\n\t\t\tupdate = 1;\n\t\t}\n\t}\n\n\tif (update) {\n\t\thfi1_cdbg(CNTR, \"[%d] Updating dd and ppd counters\", dd->unit);\n\t\tfor (i = 0; i < DEV_CNTR_LAST; i++) {\n\t\t\tentry = &dev_cntrs[i];\n\t\t\tif (entry->flags & CNTR_VL) {\n\t\t\t\tfor (vl = 0; vl < C_VL_COUNT; vl++)\n\t\t\t\t\tread_dev_cntr(dd, i, vl);\n\t\t\t} else {\n\t\t\t\tread_dev_cntr(dd, i, CNTR_INVALID_VL);\n\t\t\t}\n\t\t}\n\t\tppd = (struct hfi1_pportdata *)(dd + 1);\n\t\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\t\tfor (j = 0; j < PORT_CNTR_LAST; j++) {\n\t\t\t\tentry = &port_cntrs[j];\n\t\t\t\tif (entry->flags & CNTR_VL) {\n\t\t\t\t\tfor (vl = 0; vl < C_VL_COUNT; vl++)\n\t\t\t\t\t\tread_port_cntr(ppd, j, vl);\n\t\t\t\t} else {\n\t\t\t\t\tread_port_cntr(ppd, j, CNTR_INVALID_VL);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tentry = &dev_cntrs[C_DC_XMIT_FLITS];\n\t\tdd->last_tx = entry->rw_cntr(entry, dd, CNTR_INVALID_VL,\n\t\t\t\t\t\tCNTR_MODE_R, 0);\n\n\t\tentry = &dev_cntrs[C_DC_RCV_FLITS];\n\t\tdd->last_rx = entry->rw_cntr(entry, dd, CNTR_INVALID_VL,\n\t\t\t\t\t\tCNTR_MODE_R, 0);\n\n\t\thfi1_cdbg(CNTR, \"[%d] setting last tx/rx to 0x%llx 0x%llx\",\n\t\t\t  dd->unit, dd->last_tx, dd->last_rx);\n\n\t} else {\n\t\thfi1_cdbg(CNTR, \"[%d] No update necessary\", dd->unit);\n\t}\n}\n\nstatic void update_synth_timer(struct timer_list *t)\n{\n\tstruct hfi1_devdata *dd = from_timer(dd, t, synth_stats_timer);\n\n\tqueue_work(dd->update_cntr_wq, &dd->update_cntr_work);\n\tmod_timer(&dd->synth_stats_timer, jiffies + HZ * SYNTH_CNT_TIME);\n}\n\n#define C_MAX_NAME 16  \nstatic int init_cntrs(struct hfi1_devdata *dd)\n{\n\tint i, rcv_ctxts, j;\n\tsize_t sz;\n\tchar *p;\n\tchar name[C_MAX_NAME];\n\tstruct hfi1_pportdata *ppd;\n\tconst char *bit_type_32 = \",32\";\n\tconst int bit_type_32_sz = strlen(bit_type_32);\n\tu32 sdma_engines = chip_sdma_engines(dd);\n\n\t \n\ttimer_setup(&dd->synth_stats_timer, update_synth_timer, 0);\n\n\t \n\t \n\t \n\n\t \n\tdd->ndevcntrs = 0;\n\tsz = 0;\n\n\tfor (i = 0; i < DEV_CNTR_LAST; i++) {\n\t\tif (dev_cntrs[i].flags & CNTR_DISABLED) {\n\t\t\thfi1_dbg_early(\"\\tSkipping %s\\n\", dev_cntrs[i].name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dev_cntrs[i].flags & CNTR_VL) {\n\t\t\tdev_cntrs[i].offset = dd->ndevcntrs;\n\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t dev_cntrs[i].name, vl_from_idx(j));\n\t\t\t\tsz += strlen(name);\n\t\t\t\t \n\t\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT)\n\t\t\t\t\tsz += bit_type_32_sz;\n\t\t\t\tsz++;\n\t\t\t\tdd->ndevcntrs++;\n\t\t\t}\n\t\t} else if (dev_cntrs[i].flags & CNTR_SDMA) {\n\t\t\tdev_cntrs[i].offset = dd->ndevcntrs;\n\t\t\tfor (j = 0; j < sdma_engines; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t dev_cntrs[i].name, j);\n\t\t\t\tsz += strlen(name);\n\t\t\t\t \n\t\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT)\n\t\t\t\t\tsz += bit_type_32_sz;\n\t\t\t\tsz++;\n\t\t\t\tdd->ndevcntrs++;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tsz += strlen(dev_cntrs[i].name) + 1;\n\t\t\t \n\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT)\n\t\t\t\tsz += bit_type_32_sz;\n\t\t\tdev_cntrs[i].offset = dd->ndevcntrs;\n\t\t\tdd->ndevcntrs++;\n\t\t}\n\t}\n\n\t \n\tdd->cntrs = kcalloc(dd->ndevcntrs + num_driver_cntrs, sizeof(u64),\n\t\t\t    GFP_KERNEL);\n\tif (!dd->cntrs)\n\t\tgoto bail;\n\n\tdd->scntrs = kcalloc(dd->ndevcntrs, sizeof(u64), GFP_KERNEL);\n\tif (!dd->scntrs)\n\t\tgoto bail;\n\n\t \n\tdd->cntrnameslen = sz;\n\tdd->cntrnames = kmalloc(sz, GFP_KERNEL);\n\tif (!dd->cntrnames)\n\t\tgoto bail;\n\n\t \n\tfor (p = dd->cntrnames, i = 0; i < DEV_CNTR_LAST; i++) {\n\t\tif (dev_cntrs[i].flags & CNTR_DISABLED) {\n\t\t\t \n\t\t} else if (dev_cntrs[i].flags & CNTR_VL) {\n\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t dev_cntrs[i].name,\n\t\t\t\t\t vl_from_idx(j));\n\t\t\t\tmemcpy(p, name, strlen(name));\n\t\t\t\tp += strlen(name);\n\n\t\t\t\t \n\t\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT) {\n\t\t\t\t\tmemcpy(p, bit_type_32, bit_type_32_sz);\n\t\t\t\t\tp += bit_type_32_sz;\n\t\t\t\t}\n\n\t\t\t\t*p++ = '\\n';\n\t\t\t}\n\t\t} else if (dev_cntrs[i].flags & CNTR_SDMA) {\n\t\t\tfor (j = 0; j < sdma_engines; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t dev_cntrs[i].name, j);\n\t\t\t\tmemcpy(p, name, strlen(name));\n\t\t\t\tp += strlen(name);\n\n\t\t\t\t \n\t\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT) {\n\t\t\t\t\tmemcpy(p, bit_type_32, bit_type_32_sz);\n\t\t\t\t\tp += bit_type_32_sz;\n\t\t\t\t}\n\n\t\t\t\t*p++ = '\\n';\n\t\t\t}\n\t\t} else {\n\t\t\tmemcpy(p, dev_cntrs[i].name, strlen(dev_cntrs[i].name));\n\t\t\tp += strlen(dev_cntrs[i].name);\n\n\t\t\t \n\t\t\tif (dev_cntrs[i].flags & CNTR_32BIT) {\n\t\t\t\tmemcpy(p, bit_type_32, bit_type_32_sz);\n\t\t\t\tp += bit_type_32_sz;\n\t\t\t}\n\n\t\t\t*p++ = '\\n';\n\t\t}\n\t}\n\n\t \n\t \n\t \n\n\t \n\trcv_ctxts = dd->num_rcv_contexts;\n\tfor (i = C_RCV_HDR_OVF_FIRST + rcv_ctxts;\n\t     i <= C_RCV_HDR_OVF_LAST; i++) {\n\t\tport_cntrs[i].flags |= CNTR_DISABLED;\n\t}\n\n\t \n\tsz = 0;\n\tdd->nportcntrs = 0;\n\tfor (i = 0; i < PORT_CNTR_LAST; i++) {\n\t\tif (port_cntrs[i].flags & CNTR_DISABLED) {\n\t\t\thfi1_dbg_early(\"\\tSkipping %s\\n\", port_cntrs[i].name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (port_cntrs[i].flags & CNTR_VL) {\n\t\t\tport_cntrs[i].offset = dd->nportcntrs;\n\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t port_cntrs[i].name, vl_from_idx(j));\n\t\t\t\tsz += strlen(name);\n\t\t\t\t \n\t\t\t\tif (port_cntrs[i].flags & CNTR_32BIT)\n\t\t\t\t\tsz += bit_type_32_sz;\n\t\t\t\tsz++;\n\t\t\t\tdd->nportcntrs++;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tsz += strlen(port_cntrs[i].name) + 1;\n\t\t\t \n\t\t\tif (port_cntrs[i].flags & CNTR_32BIT)\n\t\t\t\tsz += bit_type_32_sz;\n\t\t\tport_cntrs[i].offset = dd->nportcntrs;\n\t\t\tdd->nportcntrs++;\n\t\t}\n\t}\n\n\t \n\tdd->portcntrnameslen = sz;\n\tdd->portcntrnames = kmalloc(sz, GFP_KERNEL);\n\tif (!dd->portcntrnames)\n\t\tgoto bail;\n\n\t \n\tfor (p = dd->portcntrnames, i = 0; i < PORT_CNTR_LAST; i++) {\n\t\tif (port_cntrs[i].flags & CNTR_DISABLED)\n\t\t\tcontinue;\n\n\t\tif (port_cntrs[i].flags & CNTR_VL) {\n\t\t\tfor (j = 0; j < C_VL_COUNT; j++) {\n\t\t\t\tsnprintf(name, C_MAX_NAME, \"%s%d\",\n\t\t\t\t\t port_cntrs[i].name, vl_from_idx(j));\n\t\t\t\tmemcpy(p, name, strlen(name));\n\t\t\t\tp += strlen(name);\n\n\t\t\t\t \n\t\t\t\tif (port_cntrs[i].flags & CNTR_32BIT) {\n\t\t\t\t\tmemcpy(p, bit_type_32, bit_type_32_sz);\n\t\t\t\t\tp += bit_type_32_sz;\n\t\t\t\t}\n\n\t\t\t\t*p++ = '\\n';\n\t\t\t}\n\t\t} else {\n\t\t\tmemcpy(p, port_cntrs[i].name,\n\t\t\t       strlen(port_cntrs[i].name));\n\t\t\tp += strlen(port_cntrs[i].name);\n\n\t\t\t \n\t\t\tif (port_cntrs[i].flags & CNTR_32BIT) {\n\t\t\t\tmemcpy(p, bit_type_32, bit_type_32_sz);\n\t\t\t\tp += bit_type_32_sz;\n\t\t\t}\n\n\t\t\t*p++ = '\\n';\n\t\t}\n\t}\n\n\t \n\tppd = (struct hfi1_pportdata *)(dd + 1);\n\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\tppd->cntrs = kcalloc(dd->nportcntrs, sizeof(u64), GFP_KERNEL);\n\t\tif (!ppd->cntrs)\n\t\t\tgoto bail;\n\n\t\tppd->scntrs = kcalloc(dd->nportcntrs, sizeof(u64), GFP_KERNEL);\n\t\tif (!ppd->scntrs)\n\t\t\tgoto bail;\n\t}\n\n\t \n\tif (init_cpu_counters(dd))\n\t\tgoto bail;\n\n\tdd->update_cntr_wq = alloc_ordered_workqueue(\"hfi1_update_cntr_%d\",\n\t\t\t\t\t\t     WQ_MEM_RECLAIM, dd->unit);\n\tif (!dd->update_cntr_wq)\n\t\tgoto bail;\n\n\tINIT_WORK(&dd->update_cntr_work, do_update_synth_timer);\n\n\tmod_timer(&dd->synth_stats_timer, jiffies + HZ * SYNTH_CNT_TIME);\n\treturn 0;\nbail:\n\tfree_cntrs(dd);\n\treturn -ENOMEM;\n}\n\nstatic u32 chip_to_opa_lstate(struct hfi1_devdata *dd, u32 chip_lstate)\n{\n\tswitch (chip_lstate) {\n\tcase LSTATE_DOWN:\n\t\treturn IB_PORT_DOWN;\n\tcase LSTATE_INIT:\n\t\treturn IB_PORT_INIT;\n\tcase LSTATE_ARMED:\n\t\treturn IB_PORT_ARMED;\n\tcase LSTATE_ACTIVE:\n\t\treturn IB_PORT_ACTIVE;\n\tdefault:\n\t\tdd_dev_err(dd,\n\t\t\t   \"Unknown logical state 0x%x, reporting IB_PORT_DOWN\\n\",\n\t\t\t   chip_lstate);\n\t\treturn IB_PORT_DOWN;\n\t}\n}\n\nu32 chip_to_opa_pstate(struct hfi1_devdata *dd, u32 chip_pstate)\n{\n\t \n\tswitch (chip_pstate & 0xf0) {\n\tcase PLS_DISABLED:\n\t\treturn IB_PORTPHYSSTATE_DISABLED;\n\tcase PLS_OFFLINE:\n\t\treturn OPA_PORTPHYSSTATE_OFFLINE;\n\tcase PLS_POLLING:\n\t\treturn IB_PORTPHYSSTATE_POLLING;\n\tcase PLS_CONFIGPHY:\n\t\treturn IB_PORTPHYSSTATE_TRAINING;\n\tcase PLS_LINKUP:\n\t\treturn IB_PORTPHYSSTATE_LINKUP;\n\tcase PLS_PHYTEST:\n\t\treturn IB_PORTPHYSSTATE_PHY_TEST;\n\tdefault:\n\t\tdd_dev_err(dd, \"Unexpected chip physical state of 0x%x\\n\",\n\t\t\t   chip_pstate);\n\t\treturn IB_PORTPHYSSTATE_DISABLED;\n\t}\n}\n\n \nconst char *opa_lstate_name(u32 lstate)\n{\n\tstatic const char * const port_logical_names[] = {\n\t\t\"PORT_NOP\",\n\t\t\"PORT_DOWN\",\n\t\t\"PORT_INIT\",\n\t\t\"PORT_ARMED\",\n\t\t\"PORT_ACTIVE\",\n\t\t\"PORT_ACTIVE_DEFER\",\n\t};\n\tif (lstate < ARRAY_SIZE(port_logical_names))\n\t\treturn port_logical_names[lstate];\n\treturn \"unknown\";\n}\n\n \nconst char *opa_pstate_name(u32 pstate)\n{\n\tstatic const char * const port_physical_names[] = {\n\t\t\"PHYS_NOP\",\n\t\t\"reserved1\",\n\t\t\"PHYS_POLL\",\n\t\t\"PHYS_DISABLED\",\n\t\t\"PHYS_TRAINING\",\n\t\t\"PHYS_LINKUP\",\n\t\t\"PHYS_LINK_ERR_RECOVER\",\n\t\t\"PHYS_PHY_TEST\",\n\t\t\"reserved8\",\n\t\t\"PHYS_OFFLINE\",\n\t\t\"PHYS_GANGED\",\n\t\t\"PHYS_TEST\",\n\t};\n\tif (pstate < ARRAY_SIZE(port_physical_names))\n\t\treturn port_physical_names[pstate];\n\treturn \"unknown\";\n}\n\n \nstatic void update_statusp(struct hfi1_pportdata *ppd, u32 state)\n{\n\t \n\tif (ppd->statusp) {\n\t\tswitch (state) {\n\t\tcase IB_PORT_DOWN:\n\t\tcase IB_PORT_INIT:\n\t\t\t*ppd->statusp &= ~(HFI1_STATUS_IB_CONF |\n\t\t\t\t\t   HFI1_STATUS_IB_READY);\n\t\t\tbreak;\n\t\tcase IB_PORT_ARMED:\n\t\t\t*ppd->statusp |= HFI1_STATUS_IB_CONF;\n\t\t\tbreak;\n\t\tcase IB_PORT_ACTIVE:\n\t\t\t*ppd->statusp |= HFI1_STATUS_IB_READY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tdd_dev_info(ppd->dd, \"logical state changed to %s (0x%x)\\n\",\n\t\t    opa_lstate_name(state), state);\n}\n\n \nstatic int wait_logical_linkstate(struct hfi1_pportdata *ppd, u32 state,\n\t\t\t\t  int msecs)\n{\n\tunsigned long timeout;\n\tu32 new_state;\n\n\ttimeout = jiffies + msecs_to_jiffies(msecs);\n\twhile (1) {\n\t\tnew_state = chip_to_opa_lstate(ppd->dd,\n\t\t\t\t\t       read_logical_state(ppd->dd));\n\t\tif (new_state == state)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"timeout waiting for link state 0x%x\\n\",\n\t\t\t\t   state);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tmsleep(20);\n\t}\n\n\treturn 0;\n}\n\nstatic void log_state_transition(struct hfi1_pportdata *ppd, u32 state)\n{\n\tu32 ib_pstate = chip_to_opa_pstate(ppd->dd, state);\n\n\tdd_dev_info(ppd->dd,\n\t\t    \"physical state changed to %s (0x%x), phy 0x%x\\n\",\n\t\t    opa_pstate_name(ib_pstate), ib_pstate, state);\n}\n\n \nstatic void log_physical_state(struct hfi1_pportdata *ppd, u32 state)\n{\n\tu32 read_state = read_physical_state(ppd->dd);\n\n\tif (read_state == state) {\n\t\tlog_state_transition(ppd, state);\n\t} else {\n\t\tdd_dev_err(ppd->dd,\n\t\t\t   \"anticipated phy link state 0x%x, read 0x%x\\n\",\n\t\t\t   state, read_state);\n\t}\n}\n\n \nstatic int wait_physical_linkstate(struct hfi1_pportdata *ppd, u32 state,\n\t\t\t\t   int msecs)\n{\n\tu32 read_state;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(msecs);\n\twhile (1) {\n\t\tread_state = read_physical_state(ppd->dd);\n\t\tif (read_state == state)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"timeout waiting for phy link state 0x%x\\n\",\n\t\t\t\t   state);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tusleep_range(1950, 2050);  \n\t}\n\n\tlog_state_transition(ppd, state);\n\treturn 0;\n}\n\n \nstatic int wait_phys_link_offline_substates(struct hfi1_pportdata *ppd,\n\t\t\t\t\t    int msecs)\n{\n\tu32 read_state;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(msecs);\n\twhile (1) {\n\t\tread_state = read_physical_state(ppd->dd);\n\t\tif ((read_state & 0xF0) == PLS_OFFLINE)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"timeout waiting for phy link offline.quiet substates. Read state 0x%x, %dms\\n\",\n\t\t\t\t   read_state, msecs);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tusleep_range(1950, 2050);  \n\t}\n\n\tlog_state_transition(ppd, read_state);\n\treturn read_state;\n}\n\n \nstatic int wait_phys_link_out_of_offline(struct hfi1_pportdata *ppd,\n\t\t\t\t\t int msecs)\n{\n\tu32 read_state;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(msecs);\n\twhile (1) {\n\t\tread_state = read_physical_state(ppd->dd);\n\t\tif ((read_state & 0xF0) != PLS_OFFLINE)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(ppd->dd,\n\t\t\t\t   \"timeout waiting for phy link out of offline. Read state 0x%x, %dms\\n\",\n\t\t\t\t   read_state, msecs);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tusleep_range(1950, 2050);  \n\t}\n\n\tlog_state_transition(ppd, read_state);\n\treturn read_state;\n}\n\n#define CLEAR_STATIC_RATE_CONTROL_SMASK(r) \\\n(r &= ~SEND_CTXT_CHECK_ENABLE_DISALLOW_PBC_STATIC_RATE_CONTROL_SMASK)\n\n#define SET_STATIC_RATE_CONTROL_SMASK(r) \\\n(r |= SEND_CTXT_CHECK_ENABLE_DISALLOW_PBC_STATIC_RATE_CONTROL_SMASK)\n\nvoid hfi1_init_ctxt(struct send_context *sc)\n{\n\tif (sc) {\n\t\tstruct hfi1_devdata *dd = sc->dd;\n\t\tu64 reg;\n\t\tu8 set = (sc->type == SC_USER ?\n\t\t\t  HFI1_CAP_IS_USET(STATIC_RATE_CTRL) :\n\t\t\t  HFI1_CAP_IS_KSET(STATIC_RATE_CTRL));\n\t\treg = read_kctxt_csr(dd, sc->hw_context,\n\t\t\t\t     SEND_CTXT_CHECK_ENABLE);\n\t\tif (set)\n\t\t\tCLEAR_STATIC_RATE_CONTROL_SMASK(reg);\n\t\telse\n\t\t\tSET_STATIC_RATE_CONTROL_SMASK(reg);\n\t\twrite_kctxt_csr(dd, sc->hw_context,\n\t\t\t\tSEND_CTXT_CHECK_ENABLE, reg);\n\t}\n}\n\nint hfi1_tempsense_rd(struct hfi1_devdata *dd, struct hfi1_temp *temp)\n{\n\tint ret = 0;\n\tu64 reg;\n\n\tif (dd->icode != ICODE_RTL_SILICON) {\n\t\tif (HFI1_CAP_IS_KSET(PRINT_UNIMPL))\n\t\t\tdd_dev_info(dd, \"%s: tempsense not supported by HW\\n\",\n\t\t\t\t    __func__);\n\t\treturn -EINVAL;\n\t}\n\treg = read_csr(dd, ASIC_STS_THERM);\n\ttemp->curr = ((reg >> ASIC_STS_THERM_CURR_TEMP_SHIFT) &\n\t\t      ASIC_STS_THERM_CURR_TEMP_MASK);\n\ttemp->lo_lim = ((reg >> ASIC_STS_THERM_LO_TEMP_SHIFT) &\n\t\t\tASIC_STS_THERM_LO_TEMP_MASK);\n\ttemp->hi_lim = ((reg >> ASIC_STS_THERM_HI_TEMP_SHIFT) &\n\t\t\tASIC_STS_THERM_HI_TEMP_MASK);\n\ttemp->crit_lim = ((reg >> ASIC_STS_THERM_CRIT_TEMP_SHIFT) &\n\t\t\t  ASIC_STS_THERM_CRIT_TEMP_MASK);\n\t \n\ttemp->triggers = (u8)((reg >> ASIC_STS_THERM_LOW_SHIFT) & 0x7);\n\n\treturn ret;\n}\n\n \n\n \nstatic void read_mod_write(struct hfi1_devdata *dd, u16 src, u64 bits,\n\t\t\t   bool set)\n{\n\tu64 reg;\n\tu16 idx = src / BITS_PER_REGISTER;\n\n\tspin_lock(&dd->irq_src_lock);\n\treg = read_csr(dd, CCE_INT_MASK + (8 * idx));\n\tif (set)\n\t\treg |= bits;\n\telse\n\t\treg &= ~bits;\n\twrite_csr(dd, CCE_INT_MASK + (8 * idx), reg);\n\tspin_unlock(&dd->irq_src_lock);\n}\n\n \nint set_intr_bits(struct hfi1_devdata *dd, u16 first, u16 last, bool set)\n{\n\tu64 bits = 0;\n\tu64 bit;\n\tu16 src;\n\n\tif (first > NUM_INTERRUPT_SOURCES || last > NUM_INTERRUPT_SOURCES)\n\t\treturn -EINVAL;\n\n\tif (last < first)\n\t\treturn -ERANGE;\n\n\tfor (src = first; src <= last; src++) {\n\t\tbit = src % BITS_PER_REGISTER;\n\t\t \n\t\tif (!bit && bits) {\n\t\t\tread_mod_write(dd, src - 1, bits, set);\n\t\t\tbits = 0;\n\t\t}\n\t\tbits |= BIT_ULL(bit);\n\t}\n\tread_mod_write(dd, last, bits, set);\n\n\treturn 0;\n}\n\n \nvoid clear_all_interrupts(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < CCE_NUM_INT_CSRS; i++)\n\t\twrite_csr(dd, CCE_INT_CLEAR + (8 * i), ~(u64)0);\n\n\twrite_csr(dd, CCE_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, MISC_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, RCV_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, SEND_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, SEND_PIO_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, SEND_DMA_ERR_CLEAR, ~(u64)0);\n\twrite_csr(dd, SEND_EGRESS_ERR_CLEAR, ~(u64)0);\n\tfor (i = 0; i < chip_send_contexts(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_ERR_CLEAR, ~(u64)0);\n\tfor (i = 0; i < chip_sdma_engines(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_ENG_ERR_CLEAR, ~(u64)0);\n\n\twrite_csr(dd, DCC_ERR_FLG_CLR, ~(u64)0);\n\twrite_csr(dd, DC_LCB_ERR_CLR, ~(u64)0);\n\twrite_csr(dd, DC_DC8051_ERR_CLR, ~(u64)0);\n}\n\n \nvoid remap_intr(struct hfi1_devdata *dd, int isrc, int msix_intr)\n{\n\tu64 reg;\n\tint m, n;\n\n\t \n\tm = isrc / 64;\n\tn = isrc % 64;\n\tif (likely(m < CCE_NUM_INT_CSRS)) {\n\t\tdd->gi_mask[m] &= ~((u64)1 << n);\n\t} else {\n\t\tdd_dev_err(dd, \"remap interrupt err\\n\");\n\t\treturn;\n\t}\n\n\t \n\tm = isrc / 8;\n\tn = isrc % 8;\n\treg = read_csr(dd, CCE_INT_MAP + (8 * m));\n\treg &= ~((u64)0xff << (8 * n));\n\treg |= ((u64)msix_intr & 0xff) << (8 * n);\n\twrite_csr(dd, CCE_INT_MAP + (8 * m), reg);\n}\n\nvoid remap_sdma_interrupts(struct hfi1_devdata *dd, int engine, int msix_intr)\n{\n\t \n\tremap_intr(dd, IS_SDMA_START + engine, msix_intr);\n\tremap_intr(dd, IS_SDMA_PROGRESS_START + engine, msix_intr);\n\tremap_intr(dd, IS_SDMA_IDLE_START + engine, msix_intr);\n}\n\n \nvoid reset_interrupts(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < CCE_NUM_INT_CSRS; i++)\n\t\tdd->gi_mask[i] = ~(u64)0;\n\n\t \n\tfor (i = 0; i < CCE_NUM_INT_MAP_CSRS; i++)\n\t\twrite_csr(dd, CCE_INT_MAP + (8 * i), 0);\n}\n\n \nstatic int set_up_interrupts(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\t \n\tset_intr_bits(dd, IS_FIRST_SOURCE, IS_LAST_SOURCE, false);\n\n\t \n\tclear_all_interrupts(dd);\n\n\t \n\treset_interrupts(dd);\n\n\t \n\tret = msix_initialize(dd);\n\tif (ret)\n\t\treturn ret;\n\n\tret = msix_request_irqs(dd);\n\tif (ret)\n\t\tmsix_clean_up_interrupts(dd);\n\n\treturn ret;\n}\n\n \nstatic int set_up_context_variables(struct hfi1_devdata *dd)\n{\n\tunsigned long num_kernel_contexts;\n\tu16 num_netdev_contexts;\n\tint ret;\n\tunsigned ngroups;\n\tint rmt_count;\n\tu32 n_usr_ctxts;\n\tu32 send_contexts = chip_send_contexts(dd);\n\tu32 rcv_contexts = chip_rcv_contexts(dd);\n\n\t \n\tif (n_krcvqs)\n\t\t \n\t\tnum_kernel_contexts = n_krcvqs + 1;\n\telse\n\t\tnum_kernel_contexts = DEFAULT_KRCVQS + 1;\n\t \n\tif (num_kernel_contexts > (send_contexts - num_vls - 1)) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Reducing # kernel rcv contexts to: %d, from %lu\\n\",\n\t\t\t   send_contexts - num_vls - 1,\n\t\t\t   num_kernel_contexts);\n\t\tnum_kernel_contexts = send_contexts - num_vls - 1;\n\t}\n\n\t \n\tif (num_user_contexts < 0)\n\t\tn_usr_ctxts = cpumask_weight(&node_affinity.real_cpu_mask);\n\telse\n\t\tn_usr_ctxts = num_user_contexts;\n\t \n\tif (num_kernel_contexts + n_usr_ctxts > rcv_contexts) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"Reducing # user receive contexts to: %u, from %u\\n\",\n\t\t\t   (u32)(rcv_contexts - num_kernel_contexts),\n\t\t\t   n_usr_ctxts);\n\t\t \n\t\tn_usr_ctxts = rcv_contexts - num_kernel_contexts;\n\t}\n\n\tnum_netdev_contexts =\n\t\thfi1_num_netdev_contexts(dd, rcv_contexts -\n\t\t\t\t\t (num_kernel_contexts + n_usr_ctxts),\n\t\t\t\t\t &node_affinity.real_cpu_mask);\n\t \n\trmt_count = qos_rmt_entries(num_kernel_contexts - 1, NULL, NULL)\n\t\t    + (HFI1_CAP_IS_KSET(TID_RDMA) ? num_kernel_contexts - 1\n\t\t\t\t\t\t  : 0)\n\t\t    + n_usr_ctxts\n\t\t    + num_netdev_contexts\n\t\t    + NUM_NETDEV_MAP_ENTRIES;\n\tif (rmt_count > NUM_MAP_ENTRIES) {\n\t\tint over = rmt_count - NUM_MAP_ENTRIES;\n\t\t \n\t\tif (over >= n_usr_ctxts) {\n\t\t\tdd_dev_err(dd, \"RMT overflow: reduce the requested number of contexts\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdd_dev_err(dd, \"RMT overflow: reducing # user contexts from %u to %u\\n\",\n\t\t\t   n_usr_ctxts, n_usr_ctxts - over);\n\t\tn_usr_ctxts -= over;\n\t}\n\n\t \n\tdd->num_rcv_contexts =\n\t\tnum_kernel_contexts + n_usr_ctxts + num_netdev_contexts;\n\tdd->n_krcv_queues = num_kernel_contexts;\n\tdd->first_dyn_alloc_ctxt = num_kernel_contexts;\n\tdd->num_netdev_contexts = num_netdev_contexts;\n\tdd->num_user_contexts = n_usr_ctxts;\n\tdd->freectxts = n_usr_ctxts;\n\tdd_dev_info(dd,\n\t\t    \"rcv contexts: chip %d, used %d (kernel %d, netdev %u, user %u)\\n\",\n\t\t    rcv_contexts,\n\t\t    (int)dd->num_rcv_contexts,\n\t\t    (int)dd->n_krcv_queues,\n\t\t    dd->num_netdev_contexts,\n\t\t    dd->num_user_contexts);\n\n\t \n\tdd->rcv_entries.group_size = RCV_INCREMENT;\n\tngroups = chip_rcv_array_count(dd) / dd->rcv_entries.group_size;\n\tdd->rcv_entries.ngroups = ngroups / dd->num_rcv_contexts;\n\tdd->rcv_entries.nctxt_extra = ngroups -\n\t\t(dd->num_rcv_contexts * dd->rcv_entries.ngroups);\n\tdd_dev_info(dd, \"RcvArray groups %u, ctxts extra %u\\n\",\n\t\t    dd->rcv_entries.ngroups,\n\t\t    dd->rcv_entries.nctxt_extra);\n\tif (dd->rcv_entries.ngroups * dd->rcv_entries.group_size >\n\t    MAX_EAGER_ENTRIES * 2) {\n\t\tdd->rcv_entries.ngroups = (MAX_EAGER_ENTRIES * 2) /\n\t\t\tdd->rcv_entries.group_size;\n\t\tdd_dev_info(dd,\n\t\t\t    \"RcvArray group count too high, change to %u\\n\",\n\t\t\t    dd->rcv_entries.ngroups);\n\t\tdd->rcv_entries.nctxt_extra = 0;\n\t}\n\t \n\tret = init_sc_pools_and_sizes(dd);\n\tif (ret >= 0) {\t \n\t\tdd->num_send_contexts = ret;\n\t\tdd_dev_info(\n\t\t\tdd,\n\t\t\t\"send contexts: chip %d, used %d (kernel %d, ack %d, user %d, vl15 %d)\\n\",\n\t\t\tsend_contexts,\n\t\t\tdd->num_send_contexts,\n\t\t\tdd->sc_sizes[SC_KERNEL].count,\n\t\t\tdd->sc_sizes[SC_ACK].count,\n\t\t\tdd->sc_sizes[SC_USER].count,\n\t\t\tdd->sc_sizes[SC_VL15].count);\n\t\tret = 0;\t \n\t}\n\n\treturn ret;\n}\n\n \nstatic void set_partition_keys(struct hfi1_pportdata *ppd)\n{\n\tstruct hfi1_devdata *dd = ppd->dd;\n\tu64 reg = 0;\n\tint i;\n\n\tdd_dev_info(dd, \"Setting partition keys\\n\");\n\tfor (i = 0; i < hfi1_get_npkeys(dd); i++) {\n\t\treg |= (ppd->pkeys[i] &\n\t\t\tRCV_PARTITION_KEY_PARTITION_KEY_A_MASK) <<\n\t\t\t((i % 4) *\n\t\t\t RCV_PARTITION_KEY_PARTITION_KEY_B_SHIFT);\n\t\t \n\t\tif ((i % 4) == 3) {\n\t\t\twrite_csr(dd, RCV_PARTITION_KEY +\n\t\t\t\t  ((i - 3) * 2), reg);\n\t\t\treg = 0;\n\t\t}\n\t}\n\n\t \n\tadd_rcvctrl(dd, RCV_CTRL_RCV_PARTITION_KEY_ENABLE_SMASK);\n}\n\n \nstatic void write_uninitialized_csrs_and_memories(struct hfi1_devdata *dd)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0; i < CCE_NUM_INT_MAP_CSRS; i++)\n\t\twrite_csr(dd, CCE_INT_MAP + (8 * i), 0);\n\n\t \n\tfor (i = 0; i < chip_send_contexts(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CREDIT_RETURN_ADDR, 0);\n\n\t \n\t \n\t \n\n\t \n\t \n\t \n\tfor (i = 0; i < chip_rcv_contexts(dd); i++) {\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_TAIL_ADDR, 0);\n\t\tfor (j = 0; j < RXE_NUM_TID_FLOWS; j++)\n\t\t\twrite_uctxt_csr(dd, i, RCV_TID_FLOW_TABLE + (8 * j), 0);\n\t}\n\n\t \n\tfor (i = 0; i < chip_rcv_array_count(dd); i++)\n\t\thfi1_put_tid(dd, i, PT_INVALID_FLUSH, 0, 0);\n\n\t \n\tfor (i = 0; i < 32; i++)\n\t\twrite_csr(dd, RCV_QP_MAP_TABLE + (8 * i), 0);\n}\n\n \nstatic void clear_cce_status(struct hfi1_devdata *dd, u64 status_bits,\n\t\t\t     u64 ctrl_bits)\n{\n\tunsigned long timeout;\n\tu64 reg;\n\n\t \n\treg = read_csr(dd, CCE_STATUS);\n\tif ((reg & status_bits) == 0)\n\t\treturn;\n\n\t \n\twrite_csr(dd, CCE_CTRL, ctrl_bits);\n\n\t \n\ttimeout = jiffies + msecs_to_jiffies(CCE_STATUS_TIMEOUT);\n\twhile (1) {\n\t\treg = read_csr(dd, CCE_STATUS);\n\t\tif ((reg & status_bits) == 0)\n\t\t\treturn;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Timeout waiting for CceStatus to clear bits 0x%llx, remaining 0x%llx\\n\",\n\t\t\t\t   status_bits, reg & status_bits);\n\t\t\treturn;\n\t\t}\n\t\tudelay(1);\n\t}\n}\n\n \nstatic void reset_cce_csrs(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\t \n\t \n\t \n\tclear_cce_status(dd, ALL_FROZE, CCE_CTRL_SPC_UNFREEZE_SMASK);\n\tclear_cce_status(dd, ALL_TXE_PAUSE, CCE_CTRL_TXE_RESUME_SMASK);\n\tclear_cce_status(dd, ALL_RXE_PAUSE, CCE_CTRL_RXE_RESUME_SMASK);\n\tfor (i = 0; i < CCE_NUM_SCRATCH; i++)\n\t\twrite_csr(dd, CCE_SCRATCH + (8 * i), 0);\n\t \n\twrite_csr(dd, CCE_ERR_MASK, 0);\n\twrite_csr(dd, CCE_ERR_CLEAR, ~0ull);\n\t \n\tfor (i = 0; i < CCE_NUM_32_BIT_COUNTERS; i++)\n\t\twrite_csr(dd, CCE_COUNTER_ARRAY32 + (8 * i), 0);\n\twrite_csr(dd, CCE_DC_CTRL, CCE_DC_CTRL_RESETCSR);\n\t \n\tfor (i = 0; i < CCE_NUM_MSIX_VECTORS; i++) {\n\t\twrite_csr(dd, CCE_MSIX_TABLE_LOWER + (8 * i), 0);\n\t\twrite_csr(dd, CCE_MSIX_TABLE_UPPER + (8 * i),\n\t\t\t  CCE_MSIX_TABLE_UPPER_RESETCSR);\n\t}\n\tfor (i = 0; i < CCE_NUM_MSIX_PBAS; i++) {\n\t\t \n\t\twrite_csr(dd, CCE_MSIX_INT_GRANTED, ~0ull);\n\t\twrite_csr(dd, CCE_MSIX_VEC_CLR_WITHOUT_INT, ~0ull);\n\t}\n\tfor (i = 0; i < CCE_NUM_INT_MAP_CSRS; i++)\n\t\twrite_csr(dd, CCE_INT_MAP, 0);\n\tfor (i = 0; i < CCE_NUM_INT_CSRS; i++) {\n\t\t \n\t\twrite_csr(dd, CCE_INT_MASK + (8 * i), 0);\n\t\twrite_csr(dd, CCE_INT_CLEAR + (8 * i), ~0ull);\n\t\t \n\t\t \n\t}\n\tfor (i = 0; i < CCE_NUM_32_BIT_INT_COUNTERS; i++)\n\t\twrite_csr(dd, CCE_INT_COUNTER_ARRAY32 + (8 * i), 0);\n}\n\n \nstatic void reset_misc_csrs(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < 32; i++) {\n\t\twrite_csr(dd, MISC_CFG_RSA_R2 + (8 * i), 0);\n\t\twrite_csr(dd, MISC_CFG_RSA_SIGNATURE + (8 * i), 0);\n\t\twrite_csr(dd, MISC_CFG_RSA_MODULUS + (8 * i), 0);\n\t}\n\t \n\t \n\twrite_csr(dd, MISC_CFG_RSA_CMD, 1);\n\twrite_csr(dd, MISC_CFG_RSA_MU, 0);\n\twrite_csr(dd, MISC_CFG_FW_CTRL, 0);\n\t \n\t \n\t \n\t \n\t \n\twrite_csr(dd, MISC_ERR_MASK, 0);\n\twrite_csr(dd, MISC_ERR_CLEAR, ~0ull);\n\t \n}\n\n \nstatic void reset_txe_csrs(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\twrite_csr(dd, SEND_CTRL, 0);\n\t__cm_reset(dd, 0);\t \n\t \n\t \n\t \n\t \n\twrite_csr(dd, SEND_HIGH_PRIORITY_LIMIT, 0);\n\tpio_reset_all(dd);\t \n\t \n\twrite_csr(dd, SEND_PIO_ERR_MASK, 0);\n\twrite_csr(dd, SEND_PIO_ERR_CLEAR, ~0ull);\n\t \n\t \n\twrite_csr(dd, SEND_DMA_ERR_MASK, 0);\n\twrite_csr(dd, SEND_DMA_ERR_CLEAR, ~0ull);\n\t \n\t \n\twrite_csr(dd, SEND_EGRESS_ERR_MASK, 0);\n\twrite_csr(dd, SEND_EGRESS_ERR_CLEAR, ~0ull);\n\t \n\twrite_csr(dd, SEND_BTH_QP, 0);\n\twrite_csr(dd, SEND_STATIC_RATE_CONTROL, 0);\n\twrite_csr(dd, SEND_SC2VLT0, 0);\n\twrite_csr(dd, SEND_SC2VLT1, 0);\n\twrite_csr(dd, SEND_SC2VLT2, 0);\n\twrite_csr(dd, SEND_SC2VLT3, 0);\n\twrite_csr(dd, SEND_LEN_CHECK0, 0);\n\twrite_csr(dd, SEND_LEN_CHECK1, 0);\n\t \n\twrite_csr(dd, SEND_ERR_MASK, 0);\n\twrite_csr(dd, SEND_ERR_CLEAR, ~0ull);\n\t \n\tfor (i = 0; i < VL_ARB_LOW_PRIO_TABLE_SIZE; i++)\n\t\twrite_csr(dd, SEND_LOW_PRIORITY_LIST + (8 * i), 0);\n\tfor (i = 0; i < VL_ARB_HIGH_PRIO_TABLE_SIZE; i++)\n\t\twrite_csr(dd, SEND_HIGH_PRIORITY_LIST + (8 * i), 0);\n\tfor (i = 0; i < chip_send_contexts(dd) / NUM_CONTEXTS_PER_SET; i++)\n\t\twrite_csr(dd, SEND_CONTEXT_SET_CTRL + (8 * i), 0);\n\tfor (i = 0; i < TXE_NUM_32_BIT_COUNTER; i++)\n\t\twrite_csr(dd, SEND_COUNTER_ARRAY32 + (8 * i), 0);\n\tfor (i = 0; i < TXE_NUM_64_BIT_COUNTER; i++)\n\t\twrite_csr(dd, SEND_COUNTER_ARRAY64 + (8 * i), 0);\n\twrite_csr(dd, SEND_CM_CTRL, SEND_CM_CTRL_RESETCSR);\n\twrite_csr(dd, SEND_CM_GLOBAL_CREDIT, SEND_CM_GLOBAL_CREDIT_RESETCSR);\n\t \n\twrite_csr(dd, SEND_CM_TIMER_CTRL, 0);\n\twrite_csr(dd, SEND_CM_LOCAL_AU_TABLE0_TO3, 0);\n\twrite_csr(dd, SEND_CM_LOCAL_AU_TABLE4_TO7, 0);\n\twrite_csr(dd, SEND_CM_REMOTE_AU_TABLE0_TO3, 0);\n\twrite_csr(dd, SEND_CM_REMOTE_AU_TABLE4_TO7, 0);\n\tfor (i = 0; i < TXE_NUM_DATA_VL; i++)\n\t\twrite_csr(dd, SEND_CM_CREDIT_VL + (8 * i), 0);\n\twrite_csr(dd, SEND_CM_CREDIT_VL15, 0);\n\t \n\t \n\t \n\t \n\twrite_csr(dd, SEND_EGRESS_ERR_INFO, ~0ull);\n\t \n\t \n\n\t \n\tfor (i = 0; i < chip_send_contexts(dd); i++) {\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CTRL, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CREDIT_CTRL, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CREDIT_RETURN_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CREDIT_FORCE, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_ERR_MASK, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_ERR_CLEAR, ~0ull);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_ENABLE, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_VL, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_JOB_KEY, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_PARTITION_KEY, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_SLID, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CHECK_OPCODE, 0);\n\t}\n\n\t \n\tfor (i = 0; i < chip_sdma_engines(dd); i++) {\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CTRL, 0);\n\t\t \n\t\twrite_kctxt_csr(dd, i, SEND_DMA_BASE_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_LEN_GEN, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_TAIL, 0);\n\t\t \n\t\twrite_kctxt_csr(dd, i, SEND_DMA_HEAD_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_PRIORITY_THLD, 0);\n\t\t \n\t\twrite_kctxt_csr(dd, i, SEND_DMA_RELOAD_CNT, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_DESC_CNT, 0);\n\t\t \n\t\t \n\t\twrite_kctxt_csr(dd, i, SEND_DMA_ENG_ERR_MASK, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_ENG_ERR_CLEAR, ~0ull);\n\t\t \n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_ENABLE, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_VL, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_JOB_KEY, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_PARTITION_KEY, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_SLID, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CHECK_OPCODE, 0);\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_MEMORY, 0);\n\t}\n}\n\n \nstatic void init_rbufs(struct hfi1_devdata *dd)\n{\n\tu64 reg;\n\tint count;\n\n\t \n\tcount = 0;\n\twhile (1) {\n\t\treg = read_csr(dd, RCV_STATUS);\n\t\tif ((reg & (RCV_STATUS_RX_RBUF_PKT_PENDING_SMASK\n\t\t\t    | RCV_STATUS_RX_PKT_IN_PROGRESS_SMASK)) == 0)\n\t\t\tbreak;\n\t\t \n\t\tif (count++ > 500) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: in-progress DMA not clearing: RcvStatus 0x%llx, continuing\\n\",\n\t\t\t\t   __func__, reg);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(2);  \n\t}\n\n\t \n\twrite_csr(dd, RCV_CTRL, RCV_CTRL_RX_RBUF_INIT_SMASK);\n\n\t \n\tread_csr(dd, RCV_CTRL);\n\n\t \n\tcount = 0;\n\twhile (1) {\n\t\t \n\t\tudelay(2);  \n\t\treg = read_csr(dd, RCV_STATUS);\n\t\tif (reg & (RCV_STATUS_RX_RBUF_INIT_DONE_SMASK))\n\t\t\tbreak;\n\n\t\t \n\t\tif (count++ > 50) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: RcvStatus.RxRbufInit not set, continuing\\n\",\n\t\t\t\t   __func__);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void reset_rxe_csrs(struct hfi1_devdata *dd)\n{\n\tint i, j;\n\n\t \n\twrite_csr(dd, RCV_CTRL, 0);\n\tinit_rbufs(dd);\n\t \n\t \n\t \n\t \n\twrite_csr(dd, RCV_BTH_QP, 0);\n\twrite_csr(dd, RCV_MULTICAST, 0);\n\twrite_csr(dd, RCV_BYPASS, 0);\n\twrite_csr(dd, RCV_VL15, 0);\n\t \n\twrite_csr(dd, RCV_ERR_INFO,\n\t\t  RCV_ERR_INFO_RCV_EXCESS_BUFFER_OVERRUN_SMASK);\n\t \n\twrite_csr(dd, RCV_ERR_MASK, 0);\n\twrite_csr(dd, RCV_ERR_CLEAR, ~0ull);\n\t \n\tfor (i = 0; i < 32; i++)\n\t\twrite_csr(dd, RCV_QP_MAP_TABLE + (8 * i), 0);\n\tfor (i = 0; i < 4; i++)\n\t\twrite_csr(dd, RCV_PARTITION_KEY + (8 * i), 0);\n\tfor (i = 0; i < RXE_NUM_32_BIT_COUNTERS; i++)\n\t\twrite_csr(dd, RCV_COUNTER_ARRAY32 + (8 * i), 0);\n\tfor (i = 0; i < RXE_NUM_64_BIT_COUNTERS; i++)\n\t\twrite_csr(dd, RCV_COUNTER_ARRAY64 + (8 * i), 0);\n\tfor (i = 0; i < RXE_NUM_RSM_INSTANCES; i++)\n\t\tclear_rsm_rule(dd, i);\n\tfor (i = 0; i < 32; i++)\n\t\twrite_csr(dd, RCV_RSM_MAP_TABLE + (8 * i), 0);\n\n\t \n\tfor (i = 0; i < chip_rcv_contexts(dd); i++) {\n\t\t \n\t\twrite_kctxt_csr(dd, i, RCV_CTXT_CTRL, 0);\n\t\t \n\t\twrite_kctxt_csr(dd, i, RCV_EGR_CTRL, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_TID_CTRL, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_KEY_CTRL, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_CNT, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_ENT_SIZE, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_SIZE, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_TAIL_ADDR, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_AVAIL_TIME_OUT, 0);\n\t\twrite_kctxt_csr(dd, i, RCV_HDR_OVFL_CNT, 0);\n\n\t\t \n\t\t \n\t\twrite_uctxt_csr(dd, i, RCV_HDR_HEAD, 0);\n\t\t \n\t\twrite_uctxt_csr(dd, i, RCV_EGR_INDEX_HEAD, 0);\n\t\t \n\t\tfor (j = 0; j < RXE_NUM_TID_FLOWS; j++) {\n\t\t\twrite_uctxt_csr(dd, i,\n\t\t\t\t\tRCV_TID_FLOW_TABLE + (8 * j), 0);\n\t\t}\n\t}\n}\n\n \nstatic void init_sc2vl_tables(struct hfi1_devdata *dd)\n{\n\tint i;\n\t \n\n\t \n\twrite_csr(dd, SEND_SC2VLT0, SC2VL_VAL(\n\t\t0,\n\t\t0, 0, 1, 1,\n\t\t2, 2, 3, 3,\n\t\t4, 4, 5, 5,\n\t\t6, 6, 7, 7));\n\twrite_csr(dd, SEND_SC2VLT1, SC2VL_VAL(\n\t\t1,\n\t\t8, 0, 9, 0,\n\t\t10, 0, 11, 0,\n\t\t12, 0, 13, 0,\n\t\t14, 0, 15, 15));\n\twrite_csr(dd, SEND_SC2VLT2, SC2VL_VAL(\n\t\t2,\n\t\t16, 0, 17, 0,\n\t\t18, 0, 19, 0,\n\t\t20, 0, 21, 0,\n\t\t22, 0, 23, 0));\n\twrite_csr(dd, SEND_SC2VLT3, SC2VL_VAL(\n\t\t3,\n\t\t24, 0, 25, 0,\n\t\t26, 0, 27, 0,\n\t\t28, 0, 29, 0,\n\t\t30, 0, 31, 0));\n\n\t \n\twrite_csr(dd, DCC_CFG_SC_VL_TABLE_15_0, DC_SC_VL_VAL(\n\t\t15_0,\n\t\t0, 0, 1, 1,  2, 2,  3, 3,  4, 4,  5, 5,  6, 6,  7,  7,\n\t\t8, 0, 9, 0, 10, 0, 11, 0, 12, 0, 13, 0, 14, 0, 15, 15));\n\twrite_csr(dd, DCC_CFG_SC_VL_TABLE_31_16, DC_SC_VL_VAL(\n\t\t31_16,\n\t\t16, 0, 17, 0, 18, 0, 19, 0, 20, 0, 21, 0, 22, 0, 23, 0,\n\t\t24, 0, 25, 0, 26, 0, 27, 0, 28, 0, 29, 0, 30, 0, 31, 0));\n\n\t \n\tfor (i = 0; i < 32; i++) {\n\t\tif (i < 8 || i == 15)\n\t\t\t*((u8 *)(dd->sc2vl) + i) = (u8)i;\n\t\telse\n\t\t\t*((u8 *)(dd->sc2vl) + i) = 0;\n\t}\n}\n\n \nstatic int init_chip(struct hfi1_devdata *dd)\n{\n\tint i;\n\tint ret = 0;\n\n\t \n\n\t \n\twrite_csr(dd, SEND_CTRL, 0);\n\tfor (i = 0; i < chip_send_contexts(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_CTRL, 0);\n\tfor (i = 0; i < chip_sdma_engines(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_CTRL, 0);\n\t \n\twrite_csr(dd, RCV_CTRL, 0);\n\tfor (i = 0; i < chip_rcv_contexts(dd); i++)\n\t\twrite_csr(dd, RCV_CTXT_CTRL, 0);\n\t \n\tfor (i = 0; i < CCE_NUM_INT_CSRS; i++)\n\t\twrite_csr(dd, CCE_INT_MASK + (8 * i), 0ull);\n\n\t \n\twrite_csr(dd, CCE_DC_CTRL, CCE_DC_CTRL_DC_RESET_SMASK);\n\t(void)read_csr(dd, CCE_DC_CTRL);\n\n\tif (use_flr) {\n\t\t \n\t\tdd_dev_info(dd, \"Resetting CSRs with FLR\\n\");\n\n\t\t \n\t\tpcie_flr(dd->pcidev);\n\n\t\t \n\t\tret = restore_pci_variables(dd);\n\t\tif (ret) {\n\t\t\tdd_dev_err(dd, \"%s: Could not restore PCI variables\\n\",\n\t\t\t\t   __func__);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (is_ax(dd)) {\n\t\t\tdd_dev_info(dd, \"Resetting CSRs with FLR\\n\");\n\t\t\tpcie_flr(dd->pcidev);\n\t\t\tret = restore_pci_variables(dd);\n\t\t\tif (ret) {\n\t\t\t\tdd_dev_err(dd, \"%s: Could not restore PCI variables\\n\",\n\t\t\t\t\t   __func__);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdd_dev_info(dd, \"Resetting CSRs with writes\\n\");\n\t\treset_cce_csrs(dd);\n\t\treset_txe_csrs(dd);\n\t\treset_rxe_csrs(dd);\n\t\treset_misc_csrs(dd);\n\t}\n\t \n\twrite_csr(dd, CCE_DC_CTRL, 0);\n\n\t \n\tsetextled(dd, 0);\n\n\t \n\twrite_csr(dd, ASIC_QSFP1_OUT, 0x1f);\n\twrite_csr(dd, ASIC_QSFP2_OUT, 0x1f);\n\tinit_chip_resources(dd);\n\treturn ret;\n}\n\nstatic void init_early_variables(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\tdd->vau = CM_VAU;\n\tdd->link_credits = CM_GLOBAL_CREDITS;\n\tif (is_ax(dd))\n\t\tdd->link_credits--;\n\tdd->vcu = cu_to_vcu(hfi1_cu);\n\t \n\tdd->vl15_init = (8 * (2048 + 128)) / vau_to_au(dd->vau);\n\tif (dd->vl15_init > dd->link_credits)\n\t\tdd->vl15_init = dd->link_credits;\n\n\twrite_uninitialized_csrs_and_memories(dd);\n\n\tif (HFI1_CAP_IS_KSET(PKEY_CHECK))\n\t\tfor (i = 0; i < dd->num_pports; i++) {\n\t\t\tstruct hfi1_pportdata *ppd = &dd->pport[i];\n\n\t\t\tset_partition_keys(ppd);\n\t\t}\n\tinit_sc2vl_tables(dd);\n}\n\nstatic void init_kdeth_qp(struct hfi1_devdata *dd)\n{\n\twrite_csr(dd, SEND_BTH_QP,\n\t\t  (RVT_KDETH_QP_PREFIX & SEND_BTH_QP_KDETH_QP_MASK) <<\n\t\t  SEND_BTH_QP_KDETH_QP_SHIFT);\n\n\twrite_csr(dd, RCV_BTH_QP,\n\t\t  (RVT_KDETH_QP_PREFIX & RCV_BTH_QP_KDETH_QP_MASK) <<\n\t\t  RCV_BTH_QP_KDETH_QP_SHIFT);\n}\n\n \nu8 hfi1_get_qp_map(struct hfi1_devdata *dd, u8 idx)\n{\n\tu64 reg = read_csr(dd, RCV_QP_MAP_TABLE + (idx / 8) * 8);\n\n\treg >>= (idx % 8) * 8;\n\treturn reg;\n}\n\n \nstatic void init_qpmap_table(struct hfi1_devdata *dd,\n\t\t\t     u32 first_ctxt,\n\t\t\t     u32 last_ctxt)\n{\n\tu64 reg = 0;\n\tu64 regno = RCV_QP_MAP_TABLE;\n\tint i;\n\tu64 ctxt = first_ctxt;\n\n\tfor (i = 0; i < 256; i++) {\n\t\treg |= ctxt << (8 * (i % 8));\n\t\tctxt++;\n\t\tif (ctxt > last_ctxt)\n\t\t\tctxt = first_ctxt;\n\t\tif (i % 8 == 7) {\n\t\t\twrite_csr(dd, regno, reg);\n\t\t\treg = 0;\n\t\t\tregno += 8;\n\t\t}\n\t}\n\n\tadd_rcvctrl(dd, RCV_CTRL_RCV_QP_MAP_ENABLE_SMASK\n\t\t\t| RCV_CTRL_RCV_BYPASS_ENABLE_SMASK);\n}\n\nstruct rsm_map_table {\n\tu64 map[NUM_MAP_REGS];\n\tunsigned int used;\n};\n\nstruct rsm_rule_data {\n\tu8 offset;\n\tu8 pkt_type;\n\tu32 field1_off;\n\tu32 field2_off;\n\tu32 index1_off;\n\tu32 index1_width;\n\tu32 index2_off;\n\tu32 index2_width;\n\tu32 mask1;\n\tu32 value1;\n\tu32 mask2;\n\tu32 value2;\n};\n\n \nstatic struct rsm_map_table *alloc_rsm_map_table(struct hfi1_devdata *dd)\n{\n\tstruct rsm_map_table *rmt;\n\tu8 rxcontext = is_ax(dd) ? 0 : 0xff;   \n\n\trmt = kmalloc(sizeof(*rmt), GFP_KERNEL);\n\tif (rmt) {\n\t\tmemset(rmt->map, rxcontext, sizeof(rmt->map));\n\t\trmt->used = 0;\n\t}\n\n\treturn rmt;\n}\n\n \nstatic void complete_rsm_map_table(struct hfi1_devdata *dd,\n\t\t\t\t   struct rsm_map_table *rmt)\n{\n\tint i;\n\n\tif (rmt) {\n\t\t \n\t\tfor (i = 0; i < NUM_MAP_REGS; i++)\n\t\t\twrite_csr(dd, RCV_RSM_MAP_TABLE + (8 * i), rmt->map[i]);\n\n\t\t \n\t\tadd_rcvctrl(dd, RCV_CTRL_RCV_RSM_ENABLE_SMASK);\n\t}\n}\n\n \nstatic bool has_rsm_rule(struct hfi1_devdata *dd, u8 rule_index)\n{\n\treturn read_csr(dd, RCV_RSM_CFG + (8 * rule_index)) != 0;\n}\n\n \nstatic void add_rsm_rule(struct hfi1_devdata *dd, u8 rule_index,\n\t\t\t struct rsm_rule_data *rrd)\n{\n\twrite_csr(dd, RCV_RSM_CFG + (8 * rule_index),\n\t\t  (u64)rrd->offset << RCV_RSM_CFG_OFFSET_SHIFT |\n\t\t  1ull << rule_index |  \n\t\t  (u64)rrd->pkt_type << RCV_RSM_CFG_PACKET_TYPE_SHIFT);\n\twrite_csr(dd, RCV_RSM_SELECT + (8 * rule_index),\n\t\t  (u64)rrd->field1_off << RCV_RSM_SELECT_FIELD1_OFFSET_SHIFT |\n\t\t  (u64)rrd->field2_off << RCV_RSM_SELECT_FIELD2_OFFSET_SHIFT |\n\t\t  (u64)rrd->index1_off << RCV_RSM_SELECT_INDEX1_OFFSET_SHIFT |\n\t\t  (u64)rrd->index1_width << RCV_RSM_SELECT_INDEX1_WIDTH_SHIFT |\n\t\t  (u64)rrd->index2_off << RCV_RSM_SELECT_INDEX2_OFFSET_SHIFT |\n\t\t  (u64)rrd->index2_width << RCV_RSM_SELECT_INDEX2_WIDTH_SHIFT);\n\twrite_csr(dd, RCV_RSM_MATCH + (8 * rule_index),\n\t\t  (u64)rrd->mask1 << RCV_RSM_MATCH_MASK1_SHIFT |\n\t\t  (u64)rrd->value1 << RCV_RSM_MATCH_VALUE1_SHIFT |\n\t\t  (u64)rrd->mask2 << RCV_RSM_MATCH_MASK2_SHIFT |\n\t\t  (u64)rrd->value2 << RCV_RSM_MATCH_VALUE2_SHIFT);\n}\n\n \nstatic void clear_rsm_rule(struct hfi1_devdata *dd, u8 rule_index)\n{\n\twrite_csr(dd, RCV_RSM_CFG + (8 * rule_index), 0);\n\twrite_csr(dd, RCV_RSM_SELECT + (8 * rule_index), 0);\n\twrite_csr(dd, RCV_RSM_MATCH + (8 * rule_index), 0);\n}\n\n \nstatic int qos_rmt_entries(unsigned int n_krcv_queues, unsigned int *mp,\n\t\t\t   unsigned int *np)\n{\n\tint i;\n\tunsigned int m, n;\n\tuint max_by_vl = 0;\n\n\t \n\tif (n_krcv_queues < MIN_KERNEL_KCTXTS ||\n\t    num_vls == 1 ||\n\t    krcvqsset <= 1)\n\t\tgoto no_qos;\n\n\t \n\tfor (i = 0; i < min_t(unsigned int, num_vls, krcvqsset); i++)\n\t\tif (krcvqs[i] > max_by_vl)\n\t\t\tmax_by_vl = krcvqs[i];\n\tif (max_by_vl > 32)\n\t\tgoto no_qos;\n\tm = ilog2(__roundup_pow_of_two(max_by_vl));\n\n\t \n\tn = ilog2(__roundup_pow_of_two(num_vls));\n\n\t \n\tif ((m + n) > 7)\n\t\tgoto no_qos;\n\n\tif (mp)\n\t\t*mp = m;\n\tif (np)\n\t\t*np = n;\n\n\treturn 1 << (m + n);\n\nno_qos:\n\tif (mp)\n\t\t*mp = 0;\n\tif (np)\n\t\t*np = 0;\n\treturn 0;\n}\n\n \nstatic void init_qos(struct hfi1_devdata *dd, struct rsm_map_table *rmt)\n{\n\tstruct rsm_rule_data rrd;\n\tunsigned qpns_per_vl, ctxt, i, qpn, n = 1, m;\n\tunsigned int rmt_entries;\n\tu64 reg;\n\n\tif (!rmt)\n\t\tgoto bail;\n\trmt_entries = qos_rmt_entries(dd->n_krcv_queues - 1, &m, &n);\n\tif (rmt_entries == 0)\n\t\tgoto bail;\n\tqpns_per_vl = 1 << m;\n\n\t \n\trmt_entries = 1 << (m + n);\n\tif (rmt->used + rmt_entries >= NUM_MAP_ENTRIES)\n\t\tgoto bail;\n\n\t \n\tfor (i = 0, ctxt = FIRST_KERNEL_KCTXT; i < num_vls; i++) {\n\t\tunsigned tctxt;\n\n\t\tfor (qpn = 0, tctxt = ctxt;\n\t\t     krcvqs[i] && qpn < qpns_per_vl; qpn++) {\n\t\t\tunsigned idx, regoff, regidx;\n\n\t\t\t \n\t\t\tidx = rmt->used + ((qpn << n) ^ i);\n\t\t\tregoff = (idx % 8) * 8;\n\t\t\tregidx = idx / 8;\n\t\t\t \n\t\t\treg = rmt->map[regidx];\n\t\t\treg &= ~(RCV_RSM_MAP_TABLE_RCV_CONTEXT_A_MASK\n\t\t\t\t<< regoff);\n\t\t\treg |= (u64)(tctxt++) << regoff;\n\t\t\trmt->map[regidx] = reg;\n\t\t\tif (tctxt == ctxt + krcvqs[i])\n\t\t\t\ttctxt = ctxt;\n\t\t}\n\t\tctxt += krcvqs[i];\n\t}\n\n\trrd.offset = rmt->used;\n\trrd.pkt_type = 2;\n\trrd.field1_off = LRH_BTH_MATCH_OFFSET;\n\trrd.field2_off = LRH_SC_MATCH_OFFSET;\n\trrd.index1_off = LRH_SC_SELECT_OFFSET;\n\trrd.index1_width = n;\n\trrd.index2_off = QPN_SELECT_OFFSET;\n\trrd.index2_width = m + n;\n\trrd.mask1 = LRH_BTH_MASK;\n\trrd.value1 = LRH_BTH_VALUE;\n\trrd.mask2 = LRH_SC_MASK;\n\trrd.value2 = LRH_SC_VALUE;\n\n\t \n\tadd_rsm_rule(dd, RSM_INS_VERBS, &rrd);\n\n\t \n\trmt->used += rmt_entries;\n\t \n\tinit_qpmap_table(dd, HFI1_CTRL_CTXT, HFI1_CTRL_CTXT);\n\tdd->qos_shift = n + 1;\n\treturn;\nbail:\n\tdd->qos_shift = 1;\n\tinit_qpmap_table(dd, FIRST_KERNEL_KCTXT, dd->n_krcv_queues - 1);\n}\n\nstatic void init_fecn_handling(struct hfi1_devdata *dd,\n\t\t\t       struct rsm_map_table *rmt)\n{\n\tstruct rsm_rule_data rrd;\n\tu64 reg;\n\tint i, idx, regoff, regidx, start;\n\tu8 offset;\n\tu32 total_cnt;\n\n\tif (HFI1_CAP_IS_KSET(TID_RDMA))\n\t\t \n\t\tstart = 1;\n\telse\n\t\tstart = dd->first_dyn_alloc_ctxt;\n\n\ttotal_cnt = dd->num_rcv_contexts - start;\n\n\t \n\tif (rmt->used + total_cnt >= NUM_MAP_ENTRIES) {\n\t\tdd_dev_err(dd, \"FECN handling disabled - too many contexts allocated\\n\");\n\t\treturn;\n\t}\n\n\t \n\toffset = (u8)(NUM_MAP_ENTRIES + rmt->used - start);\n\n\tfor (i = start, idx = rmt->used; i < dd->num_rcv_contexts;\n\t     i++, idx++) {\n\t\t \n\t\tregoff = (idx % 8) * 8;\n\t\tregidx = idx / 8;\n\t\treg = rmt->map[regidx];\n\t\treg &= ~(RCV_RSM_MAP_TABLE_RCV_CONTEXT_A_MASK << regoff);\n\t\treg |= (u64)i << regoff;\n\t\trmt->map[regidx] = reg;\n\t}\n\n\t \n\trrd.offset = offset;\n\trrd.pkt_type = 0;\n\trrd.field1_off = 95;\n\trrd.field2_off = 133;\n\trrd.index1_off = 64;\n\trrd.index1_width = 8;\n\trrd.index2_off = 0;\n\trrd.index2_width = 0;\n\trrd.mask1 = 1;\n\trrd.value1 = 1;\n\trrd.mask2 = 1;\n\trrd.value2 = 1;\n\n\t \n\tadd_rsm_rule(dd, RSM_INS_FECN, &rrd);\n\n\trmt->used += total_cnt;\n}\n\nstatic inline bool hfi1_is_rmt_full(int start, int spare)\n{\n\treturn (start + spare) > NUM_MAP_ENTRIES;\n}\n\nstatic bool hfi1_netdev_update_rmt(struct hfi1_devdata *dd)\n{\n\tu8 i, j;\n\tu8 ctx_id = 0;\n\tu64 reg;\n\tu32 regoff;\n\tint rmt_start = hfi1_netdev_get_free_rmt_idx(dd);\n\tint ctxt_count = hfi1_netdev_ctxt_count(dd);\n\n\t \n\tif (has_rsm_rule(dd, RSM_INS_VNIC) || has_rsm_rule(dd, RSM_INS_AIP)) {\n\t\tdd_dev_info(dd, \"Contexts are already mapped in RMT\\n\");\n\t\treturn true;\n\t}\n\n\tif (hfi1_is_rmt_full(rmt_start, NUM_NETDEV_MAP_ENTRIES)) {\n\t\tdd_dev_err(dd, \"Not enough RMT entries used = %d\\n\",\n\t\t\t   rmt_start);\n\t\treturn false;\n\t}\n\n\tdev_dbg(&(dd)->pcidev->dev, \"RMT start = %d, end %d\\n\",\n\t\trmt_start,\n\t\trmt_start + NUM_NETDEV_MAP_ENTRIES);\n\n\t \n\tregoff = RCV_RSM_MAP_TABLE + (rmt_start / 8) * 8;\n\treg = read_csr(dd, regoff);\n\tfor (i = 0; i < NUM_NETDEV_MAP_ENTRIES; i++) {\n\t\t \n\t\tj = (rmt_start + i) % 8;\n\t\treg &= ~(0xffllu << (j * 8));\n\t\treg |= (u64)hfi1_netdev_get_ctxt(dd, ctx_id++)->ctxt << (j * 8);\n\t\t \n\t\tctx_id %= ctxt_count;\n\t\t \n\t\tif (j == 7 || ((i + 1) == NUM_NETDEV_MAP_ENTRIES)) {\n\t\t\tdev_dbg(&(dd)->pcidev->dev,\n\t\t\t\t\"RMT[%d] =0x%llx\\n\",\n\t\t\t\tregoff - RCV_RSM_MAP_TABLE, reg);\n\n\t\t\twrite_csr(dd, regoff, reg);\n\t\t\tregoff += 8;\n\t\t\tif (i < (NUM_NETDEV_MAP_ENTRIES - 1))\n\t\t\t\treg = read_csr(dd, regoff);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic void hfi1_enable_rsm_rule(struct hfi1_devdata *dd,\n\t\t\t\t int rule, struct rsm_rule_data *rrd)\n{\n\tif (!hfi1_netdev_update_rmt(dd)) {\n\t\tdd_dev_err(dd, \"Failed to update RMT for RSM%d rule\\n\", rule);\n\t\treturn;\n\t}\n\n\tadd_rsm_rule(dd, rule, rrd);\n\tadd_rcvctrl(dd, RCV_CTRL_RCV_RSM_ENABLE_SMASK);\n}\n\nvoid hfi1_init_aip_rsm(struct hfi1_devdata *dd)\n{\n\t \n\tif (atomic_fetch_inc(&dd->ipoib_rsm_usr_num) == 0) {\n\t\tint rmt_start = hfi1_netdev_get_free_rmt_idx(dd);\n\t\tstruct rsm_rule_data rrd = {\n\t\t\t.offset = rmt_start,\n\t\t\t.pkt_type = IB_PACKET_TYPE,\n\t\t\t.field1_off = LRH_BTH_MATCH_OFFSET,\n\t\t\t.mask1 = LRH_BTH_MASK,\n\t\t\t.value1 = LRH_BTH_VALUE,\n\t\t\t.field2_off = BTH_DESTQP_MATCH_OFFSET,\n\t\t\t.mask2 = BTH_DESTQP_MASK,\n\t\t\t.value2 = BTH_DESTQP_VALUE,\n\t\t\t.index1_off = DETH_AIP_SQPN_SELECT_OFFSET +\n\t\t\t\t\tilog2(NUM_NETDEV_MAP_ENTRIES),\n\t\t\t.index1_width = ilog2(NUM_NETDEV_MAP_ENTRIES),\n\t\t\t.index2_off = DETH_AIP_SQPN_SELECT_OFFSET,\n\t\t\t.index2_width = ilog2(NUM_NETDEV_MAP_ENTRIES)\n\t\t};\n\n\t\thfi1_enable_rsm_rule(dd, RSM_INS_AIP, &rrd);\n\t}\n}\n\n \nvoid hfi1_init_vnic_rsm(struct hfi1_devdata *dd)\n{\n\tint rmt_start = hfi1_netdev_get_free_rmt_idx(dd);\n\tstruct rsm_rule_data rrd = {\n\t\t \n\t\t.offset = rmt_start,\n\t\t.pkt_type = 4,\n\t\t \n\t\t.field1_off = L2_TYPE_MATCH_OFFSET,\n\t\t.mask1 = L2_TYPE_MASK,\n\t\t.value1 = L2_16B_VALUE,\n\t\t \n\t\t.field2_off = L4_TYPE_MATCH_OFFSET,\n\t\t.mask2 = L4_16B_TYPE_MASK,\n\t\t.value2 = L4_16B_ETH_VALUE,\n\t\t \n\t\t.index1_off = L4_16B_HDR_VESWID_OFFSET,\n\t\t.index1_width = ilog2(NUM_NETDEV_MAP_ENTRIES),\n\t\t.index2_off = L2_16B_ENTROPY_OFFSET,\n\t\t.index2_width = ilog2(NUM_NETDEV_MAP_ENTRIES)\n\t};\n\n\thfi1_enable_rsm_rule(dd, RSM_INS_VNIC, &rrd);\n}\n\nvoid hfi1_deinit_vnic_rsm(struct hfi1_devdata *dd)\n{\n\tclear_rsm_rule(dd, RSM_INS_VNIC);\n}\n\nvoid hfi1_deinit_aip_rsm(struct hfi1_devdata *dd)\n{\n\t \n\tif (atomic_fetch_add_unless(&dd->ipoib_rsm_usr_num, -1, 0) == 1)\n\t\tclear_rsm_rule(dd, RSM_INS_AIP);\n}\n\nstatic int init_rxe(struct hfi1_devdata *dd)\n{\n\tstruct rsm_map_table *rmt;\n\tu64 val;\n\n\t \n\twrite_csr(dd, RCV_ERR_MASK, ~0ull);\n\n\trmt = alloc_rsm_map_table(dd);\n\tif (!rmt)\n\t\treturn -ENOMEM;\n\n\t \n\tinit_qos(dd, rmt);\n\tinit_fecn_handling(dd, rmt);\n\tcomplete_rsm_map_table(dd, rmt);\n\t \n\thfi1_netdev_set_free_rmt_idx(dd, rmt->used);\n\tkfree(rmt);\n\n\t \n\n\t \n\tval = read_csr(dd, RCV_BYPASS);\n\tval &= ~RCV_BYPASS_HDR_SIZE_SMASK;\n\tval |= ((4ull & RCV_BYPASS_HDR_SIZE_MASK) <<\n\t\tRCV_BYPASS_HDR_SIZE_SHIFT);\n\twrite_csr(dd, RCV_BYPASS, val);\n\treturn 0;\n}\n\nstatic void init_other(struct hfi1_devdata *dd)\n{\n\t \n\twrite_csr(dd, CCE_ERR_MASK, ~0ull);\n\t \n\twrite_csr(dd, MISC_ERR_MASK, DRIVER_MISC_MASK);\n\t \n\twrite_csr(dd, DCC_ERR_FLG_EN, ~0ull);\n\twrite_csr(dd, DC_DC8051_ERR_EN, ~0ull);\n}\n\n \nstatic void assign_cm_au_table(struct hfi1_devdata *dd, u32 cu,\n\t\t\t       u32 csr0to3, u32 csr4to7)\n{\n\twrite_csr(dd, csr0to3,\n\t\t  0ull << SEND_CM_LOCAL_AU_TABLE0_TO3_LOCAL_AU_TABLE0_SHIFT |\n\t\t  1ull << SEND_CM_LOCAL_AU_TABLE0_TO3_LOCAL_AU_TABLE1_SHIFT |\n\t\t  2ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE0_TO3_LOCAL_AU_TABLE2_SHIFT |\n\t\t  4ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE0_TO3_LOCAL_AU_TABLE3_SHIFT);\n\twrite_csr(dd, csr4to7,\n\t\t  8ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE4_TO7_LOCAL_AU_TABLE4_SHIFT |\n\t\t  16ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE4_TO7_LOCAL_AU_TABLE5_SHIFT |\n\t\t  32ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE4_TO7_LOCAL_AU_TABLE6_SHIFT |\n\t\t  64ull * cu <<\n\t\t  SEND_CM_LOCAL_AU_TABLE4_TO7_LOCAL_AU_TABLE7_SHIFT);\n}\n\nstatic void assign_local_cm_au_table(struct hfi1_devdata *dd, u8 vcu)\n{\n\tassign_cm_au_table(dd, vcu_to_cu(vcu), SEND_CM_LOCAL_AU_TABLE0_TO3,\n\t\t\t   SEND_CM_LOCAL_AU_TABLE4_TO7);\n}\n\nvoid assign_remote_cm_au_table(struct hfi1_devdata *dd, u8 vcu)\n{\n\tassign_cm_au_table(dd, vcu_to_cu(vcu), SEND_CM_REMOTE_AU_TABLE0_TO3,\n\t\t\t   SEND_CM_REMOTE_AU_TABLE4_TO7);\n}\n\nstatic void init_txe(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\twrite_csr(dd, SEND_PIO_ERR_MASK, ~0ull);\n\twrite_csr(dd, SEND_DMA_ERR_MASK, ~0ull);\n\twrite_csr(dd, SEND_ERR_MASK, ~0ull);\n\twrite_csr(dd, SEND_EGRESS_ERR_MASK, ~0ull);\n\n\t \n\tfor (i = 0; i < chip_send_contexts(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_CTXT_ERR_MASK, ~0ull);\n\tfor (i = 0; i < chip_sdma_engines(dd); i++)\n\t\twrite_kctxt_csr(dd, i, SEND_DMA_ENG_ERR_MASK, ~0ull);\n\n\t \n\tassign_local_cm_au_table(dd, dd->vcu);\n\n\t \n\tif (dd->icode != ICODE_FUNCTIONAL_SIMULATOR)\n\t\twrite_csr(dd, SEND_CM_TIMER_CTRL, HFI1_CREDIT_RETURN_RATE);\n}\n\nint hfi1_set_ctxt_jkey(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd,\n\t\t       u16 jkey)\n{\n\tu8 hw_ctxt;\n\tu64 reg;\n\n\tif (!rcd || !rcd->sc)\n\t\treturn -EINVAL;\n\n\thw_ctxt = rcd->sc->hw_context;\n\treg = SEND_CTXT_CHECK_JOB_KEY_MASK_SMASK |  \n\t\t((jkey & SEND_CTXT_CHECK_JOB_KEY_VALUE_MASK) <<\n\t\t SEND_CTXT_CHECK_JOB_KEY_VALUE_SHIFT);\n\t \n\tif (HFI1_CAP_KGET_MASK(rcd->flags, ALLOW_PERM_JKEY))\n\t\treg |= SEND_CTXT_CHECK_JOB_KEY_ALLOW_PERMISSIVE_SMASK;\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_JOB_KEY, reg);\n\t \n\tif (!is_ax(dd)) {\n\t\treg = read_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE);\n\t\treg |= SEND_CTXT_CHECK_ENABLE_CHECK_JOB_KEY_SMASK;\n\t\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE, reg);\n\t}\n\n\t \n\treg = RCV_KEY_CTRL_JOB_KEY_ENABLE_SMASK |\n\t\t((jkey & RCV_KEY_CTRL_JOB_KEY_VALUE_MASK) <<\n\t\t RCV_KEY_CTRL_JOB_KEY_VALUE_SHIFT);\n\twrite_kctxt_csr(dd, rcd->ctxt, RCV_KEY_CTRL, reg);\n\n\treturn 0;\n}\n\nint hfi1_clear_ctxt_jkey(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)\n{\n\tu8 hw_ctxt;\n\tu64 reg;\n\n\tif (!rcd || !rcd->sc)\n\t\treturn -EINVAL;\n\n\thw_ctxt = rcd->sc->hw_context;\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_JOB_KEY, 0);\n\t \n\tif (!is_ax(dd)) {\n\t\treg = read_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE);\n\t\treg &= ~SEND_CTXT_CHECK_ENABLE_CHECK_JOB_KEY_SMASK;\n\t\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE, reg);\n\t}\n\t \n\twrite_kctxt_csr(dd, rcd->ctxt, RCV_KEY_CTRL, 0);\n\n\treturn 0;\n}\n\nint hfi1_set_ctxt_pkey(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd,\n\t\t       u16 pkey)\n{\n\tu8 hw_ctxt;\n\tu64 reg;\n\n\tif (!rcd || !rcd->sc)\n\t\treturn -EINVAL;\n\n\thw_ctxt = rcd->sc->hw_context;\n\treg = ((u64)pkey & SEND_CTXT_CHECK_PARTITION_KEY_VALUE_MASK) <<\n\t\tSEND_CTXT_CHECK_PARTITION_KEY_VALUE_SHIFT;\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_PARTITION_KEY, reg);\n\treg = read_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE);\n\treg |= SEND_CTXT_CHECK_ENABLE_CHECK_PARTITION_KEY_SMASK;\n\treg &= ~SEND_CTXT_CHECK_ENABLE_DISALLOW_KDETH_PACKETS_SMASK;\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE, reg);\n\n\treturn 0;\n}\n\nint hfi1_clear_ctxt_pkey(struct hfi1_devdata *dd, struct hfi1_ctxtdata *ctxt)\n{\n\tu8 hw_ctxt;\n\tu64 reg;\n\n\tif (!ctxt || !ctxt->sc)\n\t\treturn -EINVAL;\n\n\thw_ctxt = ctxt->sc->hw_context;\n\treg = read_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE);\n\treg &= ~SEND_CTXT_CHECK_ENABLE_CHECK_PARTITION_KEY_SMASK;\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_ENABLE, reg);\n\twrite_kctxt_csr(dd, hw_ctxt, SEND_CTXT_CHECK_PARTITION_KEY, 0);\n\n\treturn 0;\n}\n\n \nvoid hfi1_start_cleanup(struct hfi1_devdata *dd)\n{\n\taspm_exit(dd);\n\tfree_cntrs(dd);\n\tfree_rcverr(dd);\n\tfinish_chip_resources(dd);\n}\n\n#define HFI_BASE_GUID(dev) \\\n\t((dev)->base_guid & ~(1ULL << GUID_HFI_INDEX_SHIFT))\n\n \nstatic int init_asic_data(struct hfi1_devdata *dd)\n{\n\tunsigned long index;\n\tstruct hfi1_devdata *peer;\n\tstruct hfi1_asic_data *asic_data;\n\tint ret = 0;\n\n\t \n\tasic_data = kzalloc(sizeof(*dd->asic_data), GFP_KERNEL);\n\tif (!asic_data)\n\t\treturn -ENOMEM;\n\n\txa_lock_irq(&hfi1_dev_table);\n\t \n\txa_for_each(&hfi1_dev_table, index, peer) {\n\t\tif ((HFI_BASE_GUID(dd) == HFI_BASE_GUID(peer)) &&\n\t\t    dd->unit != peer->unit)\n\t\t\tbreak;\n\t}\n\n\tif (peer) {\n\t\t \n\t\tdd->asic_data = peer->asic_data;\n\t\tkfree(asic_data);\n\t} else {\n\t\tdd->asic_data = asic_data;\n\t\tmutex_init(&dd->asic_data->asic_resource_mutex);\n\t}\n\tdd->asic_data->dds[dd->hfi1_id] = dd;  \n\txa_unlock_irq(&hfi1_dev_table);\n\n\t \n\tif (!peer)\n\t\tret = set_up_i2c(dd, dd->asic_data);\n\n\treturn ret;\n}\n\n \nstatic int obtain_boardname(struct hfi1_devdata *dd)\n{\n\t \n\tconst char generic[] =\n\t\t\"Cornelis Omni-Path Host Fabric Interface Adapter 100 Series\";\n\tunsigned long size;\n\tint ret;\n\n\tret = read_hfi1_efi_var(dd, \"description\", &size,\n\t\t\t\t(void **)&dd->boardname);\n\tif (ret) {\n\t\tdd_dev_info(dd, \"Board description not found\\n\");\n\t\t \n\t\tdd->boardname = kstrdup(generic, GFP_KERNEL);\n\t\tif (!dd->boardname)\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n \nstatic int check_int_registers(struct hfi1_devdata *dd)\n{\n\tu64 reg;\n\tu64 all_bits = ~(u64)0;\n\tu64 mask;\n\n\t \n\tmask = read_csr(dd, CCE_INT_MASK);\n\twrite_csr(dd, CCE_INT_MASK, 0ull);\n\treg = read_csr(dd, CCE_INT_MASK);\n\tif (reg)\n\t\tgoto err_exit;\n\n\t \n\twrite_csr(dd, CCE_INT_CLEAR, all_bits);\n\treg = read_csr(dd, CCE_INT_STATUS);\n\tif (reg)\n\t\tgoto err_exit;\n\n\t \n\twrite_csr(dd, CCE_INT_FORCE, all_bits);\n\treg = read_csr(dd, CCE_INT_STATUS);\n\tif (reg != all_bits)\n\t\tgoto err_exit;\n\n\t \n\twrite_csr(dd, CCE_INT_CLEAR, all_bits);\n\twrite_csr(dd, CCE_INT_MASK, mask);\n\n\treturn 0;\nerr_exit:\n\twrite_csr(dd, CCE_INT_MASK, mask);\n\tdd_dev_err(dd, \"Interrupt registers not properly mapped by VMM\\n\");\n\treturn -EINVAL;\n}\n\n \nint hfi1_init_dd(struct hfi1_devdata *dd)\n{\n\tstruct pci_dev *pdev = dd->pcidev;\n\tstruct hfi1_pportdata *ppd;\n\tu64 reg;\n\tint i, ret;\n\tstatic const char * const inames[] = {  \n\t\t\"RTL silicon\",\n\t\t\"RTL VCS simulation\",\n\t\t\"RTL FPGA emulation\",\n\t\t\"Functional simulator\"\n\t};\n\tstruct pci_dev *parent = pdev->bus->self;\n\tu32 sdma_engines = chip_sdma_engines(dd);\n\n\tppd = dd->pport;\n\tfor (i = 0; i < dd->num_pports; i++, ppd++) {\n\t\tint vl;\n\t\t \n\t\thfi1_init_pportdata(pdev, ppd, dd, 0, 1);\n\t\t \n\t\tppd->link_width_supported =\n\t\t\tOPA_LINK_WIDTH_1X | OPA_LINK_WIDTH_2X |\n\t\t\tOPA_LINK_WIDTH_3X | OPA_LINK_WIDTH_4X;\n\t\tppd->link_width_downgrade_supported =\n\t\t\tppd->link_width_supported;\n\t\t \n\t\tppd->link_width_enabled = OPA_LINK_WIDTH_4X;\n\t\tppd->link_width_downgrade_enabled =\n\t\t\t\t\tppd->link_width_downgrade_supported;\n\t\t \n\t\t \n\n\t\tif (num_vls < HFI1_MIN_VLS_SUPPORTED ||\n\t\t    num_vls > HFI1_MAX_VLS_SUPPORTED) {\n\t\t\tdd_dev_err(dd, \"Invalid num_vls %u, using %u VLs\\n\",\n\t\t\t\t   num_vls, HFI1_MAX_VLS_SUPPORTED);\n\t\t\tnum_vls = HFI1_MAX_VLS_SUPPORTED;\n\t\t}\n\t\tppd->vls_supported = num_vls;\n\t\tppd->vls_operational = ppd->vls_supported;\n\t\t \n\t\tfor (vl = 0; vl < num_vls; vl++)\n\t\t\tdd->vld[vl].mtu = hfi1_max_mtu;\n\t\tdd->vld[15].mtu = MAX_MAD_PACKET;\n\t\t \n\t\tppd->overrun_threshold = 0x4;\n\t\tppd->phy_error_threshold = 0xf;\n\t\tppd->port_crc_mode_enabled = link_crc_mask;\n\t\t \n\t\tppd->port_ltp_crc_mode = cap_to_port_ltp(link_crc_mask) << 8;\n\t\t \n\t\tppd->port_ltp_crc_mode |= cap_to_port_ltp(link_crc_mask) << 4;\n\t\t \n\t\tppd->host_link_state = HLS_DN_OFFLINE;\n\t\tinit_vl_arb_caches(ppd);\n\t}\n\n\t \n\tret = hfi1_pcie_ddinit(dd, pdev);\n\tif (ret < 0)\n\t\tgoto bail_free;\n\n\t \n\tret = save_pci_variables(dd);\n\tif (ret < 0)\n\t\tgoto bail_cleanup;\n\n\tdd->majrev = (dd->revision >> CCE_REVISION_CHIP_REV_MAJOR_SHIFT)\n\t\t\t& CCE_REVISION_CHIP_REV_MAJOR_MASK;\n\tdd->minrev = (dd->revision >> CCE_REVISION_CHIP_REV_MINOR_SHIFT)\n\t\t\t& CCE_REVISION_CHIP_REV_MINOR_MASK;\n\n\t \n\tif (!parent) {\n\t\tret = check_int_registers(dd);\n\t\tif (ret)\n\t\t\tgoto bail_cleanup;\n\t}\n\n\t \n\treg = read_csr(dd, CCE_REVISION2);\n\tdd->hfi1_id = (reg >> CCE_REVISION2_HFI_ID_SHIFT)\n\t\t\t\t\t& CCE_REVISION2_HFI_ID_MASK;\n\t \n\tdd->icode = reg >> CCE_REVISION2_IMPL_CODE_SHIFT;\n\tdd->irev = reg >> CCE_REVISION2_IMPL_REVISION_SHIFT;\n\tdd_dev_info(dd, \"Implementation: %s, revision 0x%x\\n\",\n\t\t    dd->icode < ARRAY_SIZE(inames) ?\n\t\t    inames[dd->icode] : \"unknown\", (int)dd->irev);\n\n\t \n\tdd->pport->link_speed_supported = OPA_LINK_SPEED_25G;\n\t \n\tdd->pport->link_speed_enabled = dd->pport->link_speed_supported;\n\t \n\tdd->pport->link_speed_active = OPA_LINK_SPEED_25G;\n\n\t \n\tppd = dd->pport;\n\tif (dd->icode == ICODE_FPGA_EMULATION && is_emulator_p(dd)) {\n\t\tppd->link_width_supported =\n\t\t\tppd->link_width_enabled =\n\t\t\tppd->link_width_downgrade_supported =\n\t\t\tppd->link_width_downgrade_enabled =\n\t\t\t\tOPA_LINK_WIDTH_1X;\n\t}\n\t \n\tif (HFI1_CAP_IS_KSET(SDMA) && num_vls > sdma_engines) {\n\t\tdd_dev_err(dd, \"num_vls %u too large, using %u VLs\\n\",\n\t\t\t   num_vls, sdma_engines);\n\t\tnum_vls = sdma_engines;\n\t\tppd->vls_supported = sdma_engines;\n\t\tppd->vls_operational = ppd->vls_supported;\n\t}\n\n\t \n\tdd->rcv_intr_timeout_csr = ns_to_cclock(dd, rcv_intr_timeout) / 64;\n\tif (dd->rcv_intr_timeout_csr >\n\t\t\tRCV_AVAIL_TIME_OUT_TIME_OUT_RELOAD_MASK)\n\t\tdd->rcv_intr_timeout_csr =\n\t\t\tRCV_AVAIL_TIME_OUT_TIME_OUT_RELOAD_MASK;\n\telse if (dd->rcv_intr_timeout_csr == 0 && rcv_intr_timeout)\n\t\tdd->rcv_intr_timeout_csr = 1;\n\n\t \n\tread_guid(dd);\n\n\t \n\tret = init_asic_data(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = init_chip(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = pcie_speeds(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = eprom_init(dd);\n\tif (ret)\n\t\tgoto bail_free_rcverr;\n\n\t \n\tget_platform_config(dd);\n\n\t \n\tret = hfi1_firmware_init(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = do_pcie_gen3_transition(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\ttune_pcie_caps(dd);\n\n\t \n\tinit_early_variables(dd);\n\n\tparse_platform_config(dd);\n\n\tret = obtain_boardname(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\tsnprintf(dd->boardversion, BOARD_VERS_MAX,\n\t\t \"ChipABI %u.%u, ChipRev %u.%u, SW Compat %llu\\n\",\n\t\t HFI1_CHIP_VERS_MAJ, HFI1_CHIP_VERS_MIN,\n\t\t (u32)dd->majrev,\n\t\t (u32)dd->minrev,\n\t\t (dd->revision >> CCE_REVISION_SW_SHIFT)\n\t\t    & CCE_REVISION_SW_MASK);\n\n\t \n\tret = hfi1_alloc_rx(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\tret = set_up_context_variables(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = init_rxe(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tinit_txe(dd);\n\t \n\tinit_other(dd);\n\t \n\tinit_kdeth_qp(dd);\n\n\tret = hfi1_dev_affinity_init(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tret = init_send_contexts(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\tret = hfi1_create_kctxts(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\taspm_init(dd);\n\n\tret = init_pervl_scs(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\t \n\tfor (i = 0; i < dd->num_pports; ++i) {\n\t\tret = sdma_init(dd, i);\n\t\tif (ret)\n\t\t\tgoto bail_cleanup;\n\t}\n\n\t \n\tret = set_up_interrupts(dd);\n\tif (ret)\n\t\tgoto bail_cleanup;\n\n\tret = hfi1_comp_vectors_set_up(dd);\n\tif (ret)\n\t\tgoto bail_clear_intr;\n\n\t \n\tinit_lcb_access(dd);\n\n\t \n\tsnprintf(dd->serial, SERIAL_MAX, \"0x%08llx\\n\",\n\t\t (dd->base_guid & 0xFFFFFF) |\n\t\t     ((dd->base_guid >> 11) & 0xF000000));\n\n\tdd->oui1 = dd->base_guid >> 56 & 0xFF;\n\tdd->oui2 = dd->base_guid >> 48 & 0xFF;\n\tdd->oui3 = dd->base_guid >> 40 & 0xFF;\n\n\tret = load_firmware(dd);  \n\tif (ret)\n\t\tgoto bail_clear_intr;\n\n\tthermal_init(dd);\n\n\tret = init_cntrs(dd);\n\tif (ret)\n\t\tgoto bail_clear_intr;\n\n\tret = init_rcverr(dd);\n\tif (ret)\n\t\tgoto bail_free_cntrs;\n\n\tinit_completion(&dd->user_comp);\n\n\t \n\trefcount_set(&dd->user_refcount, 1);\n\n\tgoto bail;\n\nbail_free_rcverr:\n\tfree_rcverr(dd);\nbail_free_cntrs:\n\tfree_cntrs(dd);\nbail_clear_intr:\n\thfi1_comp_vectors_clean_up(dd);\n\tmsix_clean_up_interrupts(dd);\nbail_cleanup:\n\thfi1_free_rx(dd);\n\thfi1_pcie_ddcleanup(dd);\nbail_free:\n\thfi1_free_devdata(dd);\nbail:\n\treturn ret;\n}\n\nstatic u16 delay_cycles(struct hfi1_pportdata *ppd, u32 desired_egress_rate,\n\t\t\tu32 dw_len)\n{\n\tu32 delta_cycles;\n\tu32 current_egress_rate = ppd->current_egress_rate;\n\t \n\n\tif (desired_egress_rate == -1)\n\t\treturn 0;  \n\n\tif (desired_egress_rate >= current_egress_rate)\n\t\treturn 0;  \n\n\tdelta_cycles = egress_cycles(dw_len * 4, desired_egress_rate) -\n\t\t\tegress_cycles(dw_len * 4, current_egress_rate);\n\n\treturn (u16)delta_cycles;\n}\n\n \nu64 create_pbc(struct hfi1_pportdata *ppd, u64 flags, int srate_mbs, u32 vl,\n\t       u32 dw_len)\n{\n\tu64 pbc, delay = 0;\n\n\tif (unlikely(srate_mbs))\n\t\tdelay = delay_cycles(ppd, srate_mbs, dw_len);\n\n\tpbc = flags\n\t\t| (delay << PBC_STATIC_RATE_CONTROL_COUNT_SHIFT)\n\t\t| ((u64)PBC_IHCRC_NONE << PBC_INSERT_HCRC_SHIFT)\n\t\t| (vl & PBC_VL_MASK) << PBC_VL_SHIFT\n\t\t| (dw_len & PBC_LENGTH_DWS_MASK)\n\t\t\t<< PBC_LENGTH_DWS_SHIFT;\n\n\treturn pbc;\n}\n\n#define SBUS_THERMAL    0x4f\n#define SBUS_THERM_MONITOR_MODE 0x1\n\n#define THERM_FAILURE(dev, ret, reason) \\\n\tdd_dev_err((dd),\t\t\t\t\t\t\\\n\t\t   \"Thermal sensor initialization failed: %s (%d)\\n\",\t\\\n\t\t   (reason), (ret))\n\n \nstatic int thermal_init(struct hfi1_devdata *dd)\n{\n\tint ret = 0;\n\n\tif (dd->icode != ICODE_RTL_SILICON ||\n\t    check_chip_resource(dd, CR_THERM_INIT, NULL))\n\t\treturn ret;\n\n\tret = acquire_chip_resource(dd, CR_SBUS, SBUS_TIMEOUT);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Acquire SBus\");\n\t\treturn ret;\n\t}\n\n\tdd_dev_info(dd, \"Initializing thermal sensor\\n\");\n\t \n\twrite_csr(dd, ASIC_CFG_THERM_POLL_EN, 0x0);\n\tmsleep(100);\n\t \n\t \n\tret = sbus_request_slow(dd, SBUS_THERMAL, 0x0,\n\t\t\t\tRESET_SBUS_RECEIVER, 0);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Bus Reset\");\n\t\tgoto done;\n\t}\n\t \n\tret = sbus_request_slow(dd, SBUS_THERMAL, 0x0,\n\t\t\t\tWRITE_SBUS_RECEIVER, 0x1);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Therm Block Reset\");\n\t\tgoto done;\n\t}\n\t \n\tret = sbus_request_slow(dd, SBUS_THERMAL, 0x1,\n\t\t\t\tWRITE_SBUS_RECEIVER, 0x32);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Write Clock Div\");\n\t\tgoto done;\n\t}\n\t \n\tret = sbus_request_slow(dd, SBUS_THERMAL, 0x3,\n\t\t\t\tWRITE_SBUS_RECEIVER,\n\t\t\t\tSBUS_THERM_MONITOR_MODE);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Write Mode Sel\");\n\t\tgoto done;\n\t}\n\t \n\tret = sbus_request_slow(dd, SBUS_THERMAL, 0x0,\n\t\t\t\tWRITE_SBUS_RECEIVER, 0x2);\n\tif (ret) {\n\t\tTHERM_FAILURE(dd, ret, \"Write Reset Deassert\");\n\t\tgoto done;\n\t}\n\t \n\tmsleep(22);\n\n\t \n\twrite_csr(dd, ASIC_CFG_THERM_POLL_EN, 0x1);\n\n\t \n\tret = acquire_chip_resource(dd, CR_THERM_INIT, 0);\n\tif (ret)\n\t\tTHERM_FAILURE(dd, ret, \"Unable to set thermal init flag\");\n\ndone:\n\trelease_chip_resource(dd, CR_SBUS);\n\treturn ret;\n}\n\nstatic void handle_temp_err(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_pportdata *ppd = &dd->pport[0];\n\t \n\tdd_dev_emerg(dd,\n\t\t     \"Critical temperature reached! Forcing device into freeze mode!\\n\");\n\tdd->flags |= HFI1_FORCED_FREEZE;\n\tstart_freeze_handling(ppd, FREEZE_SELF | FREEZE_ABORT);\n\t \n\tppd->driver_link_ready = 0;\n\tppd->link_enabled = 0;\n\tset_physical_link_state(dd, (OPA_LINKDOWN_REASON_SMA_DISABLED << 8) |\n\t\t\t\tPLS_OFFLINE);\n\t \n\tdc_shutdown(dd);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}