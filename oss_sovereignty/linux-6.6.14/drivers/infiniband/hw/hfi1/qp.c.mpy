{
  "module_name": "qp.c",
  "hash_id": "3b2fad49fe690e692a65f1f0a72243b475e8f95b91ffdda7549ecdb0a02034d5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/qp.c",
  "human_readable_source": "\n \n\n#include <linux/err.h>\n#include <linux/vmalloc.h>\n#include <linux/hash.h>\n#include <linux/module.h>\n#include <linux/seq_file.h>\n#include <rdma/rdma_vt.h>\n#include <rdma/rdmavt_qp.h>\n#include <rdma/ib_verbs.h>\n\n#include \"hfi.h\"\n#include \"qp.h\"\n#include \"trace.h\"\n#include \"verbs_txreq.h\"\n\nunsigned int hfi1_qp_table_size = 256;\nmodule_param_named(qp_table_size, hfi1_qp_table_size, uint, S_IRUGO);\nMODULE_PARM_DESC(qp_table_size, \"QP table size\");\n\nstatic void flush_tx_list(struct rvt_qp *qp);\nstatic int iowait_sleep(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *stx,\n\tunsigned int seq,\n\tbool pkts_sent);\nstatic void iowait_wakeup(struct iowait *wait, int reason);\nstatic void iowait_sdma_drained(struct iowait *wait);\nstatic void qp_pio_drain(struct rvt_qp *qp);\n\nconst struct rvt_operation_params hfi1_post_parms[RVT_OPERATION_MAX] = {\n[IB_WR_RDMA_WRITE] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_RDMA_READ] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC,\n},\n\n[IB_WR_ATOMIC_CMP_AND_SWP] = {\n\t.length = sizeof(struct ib_atomic_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC | RVT_OPERATION_ATOMIC_SGE,\n},\n\n[IB_WR_ATOMIC_FETCH_AND_ADD] = {\n\t.length = sizeof(struct ib_atomic_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_ATOMIC | RVT_OPERATION_ATOMIC_SGE,\n},\n\n[IB_WR_RDMA_WRITE_WITH_IMM] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_SEND] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_UD) | BIT(IB_QPT_SMI) | BIT(IB_QPT_GSI) |\n\t\t       BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_SEND_WITH_IMM] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_UD) | BIT(IB_QPT_SMI) | BIT(IB_QPT_GSI) |\n\t\t       BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n},\n\n[IB_WR_REG_MR] = {\n\t.length = sizeof(struct ib_reg_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_LOCAL,\n},\n\n[IB_WR_LOCAL_INV] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_UC) | BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_LOCAL,\n},\n\n[IB_WR_SEND_WITH_INV] = {\n\t.length = sizeof(struct ib_send_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n},\n\n[IB_WR_OPFN] = {\n\t.length = sizeof(struct ib_atomic_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_USE_RESERVE,\n},\n\n[IB_WR_TID_RDMA_WRITE] = {\n\t.length = sizeof(struct ib_rdma_wr),\n\t.qpt_support = BIT(IB_QPT_RC),\n\t.flags = RVT_OPERATION_IGN_RNR_CNT,\n},\n\n};\n\nstatic void flush_list_head(struct list_head *l)\n{\n\twhile (!list_empty(l)) {\n\t\tstruct sdma_txreq *tx;\n\n\t\ttx = list_first_entry(\n\t\t\tl,\n\t\t\tstruct sdma_txreq,\n\t\t\tlist);\n\t\tlist_del_init(&tx->list);\n\t\thfi1_put_txreq(\n\t\t\tcontainer_of(tx, struct verbs_txreq, txreq));\n\t}\n}\n\nstatic void flush_tx_list(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tflush_list_head(&iowait_get_ib_work(&priv->s_iowait)->tx_head);\n\tflush_list_head(&iowait_get_tid_work(&priv->s_iowait)->tx_head);\n}\n\nstatic void flush_iowait(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tunsigned long flags;\n\tseqlock_t *lock = priv->s_iowait.lock;\n\n\tif (!lock)\n\t\treturn;\n\twrite_seqlock_irqsave(lock, flags);\n\tif (!list_empty(&priv->s_iowait.list)) {\n\t\tlist_del_init(&priv->s_iowait.list);\n\t\tpriv->s_iowait.lock = NULL;\n\t\trvt_put_qp(qp);\n\t}\n\twrite_sequnlock_irqrestore(lock, flags);\n}\n\n \nstatic inline int verbs_mtu_enum_to_int(struct ib_device *dev, enum ib_mtu mtu)\n{\n\t \n\tif (mtu == (enum ib_mtu)OPA_MTU_10240)\n\t\tmtu = (enum ib_mtu)OPA_MTU_8192;\n\treturn opa_mtu_enum_to_int((enum opa_mtu)mtu);\n}\n\nint hfi1_check_modify_qp(struct rvt_qp *qp, struct ib_qp_attr *attr,\n\t\t\t int attr_mask, struct ib_udata *udata)\n{\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct hfi1_ibdev *dev = to_idev(ibqp->device);\n\tstruct hfi1_devdata *dd = dd_from_dev(dev);\n\tu8 sc;\n\n\tif (attr_mask & IB_QP_AV) {\n\t\tsc = ah_to_sc(ibqp->device, &attr->ah_attr);\n\t\tif (sc == 0xf)\n\t\t\treturn -EINVAL;\n\n\t\tif (!qp_to_sdma_engine(qp, sc) &&\n\t\t    dd->flags & HFI1_HAS_SEND_DMA)\n\t\t\treturn -EINVAL;\n\n\t\tif (!qp_to_send_context(qp, sc))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_QP_ALT_PATH) {\n\t\tsc = ah_to_sc(ibqp->device, &attr->alt_ah_attr);\n\t\tif (sc == 0xf)\n\t\t\treturn -EINVAL;\n\n\t\tif (!qp_to_sdma_engine(qp, sc) &&\n\t\t    dd->flags & HFI1_HAS_SEND_DMA)\n\t\t\treturn -EINVAL;\n\n\t\tif (!qp_to_send_context(qp, sc))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline void qp_set_16b(struct rvt_qp *qp)\n{\n\tstruct hfi1_pportdata *ppd;\n\tstruct hfi1_ibport *ibp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\t \n\thfi1_update_ah_attr(qp->ibqp.device, &qp->remote_ah_attr);\n\n\t \n\thfi1_make_opa_lid(&qp->remote_ah_attr);\n\n\tif (!(rdma_ah_get_ah_flags(&qp->remote_ah_attr) & IB_AH_GRH))\n\t\treturn;\n\n\tibp = to_iport(qp->ibqp.device, qp->port_num);\n\tppd = ppd_from_ibp(ibp);\n\tpriv->hdr_type = hfi1_get_hdr_type(ppd->lid, &qp->remote_ah_attr);\n}\n\nvoid hfi1_modify_qp(struct rvt_qp *qp, struct ib_qp_attr *attr,\n\t\t    int attr_mask, struct ib_udata *udata)\n{\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tif (attr_mask & IB_QP_AV) {\n\t\tpriv->s_sc = ah_to_sc(ibqp->device, &qp->remote_ah_attr);\n\t\tpriv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);\n\t\tpriv->s_sendcontext = qp_to_send_context(qp, priv->s_sc);\n\t\tqp_set_16b(qp);\n\t}\n\n\tif (attr_mask & IB_QP_PATH_MIG_STATE &&\n\t    attr->path_mig_state == IB_MIG_MIGRATED &&\n\t    qp->s_mig_state == IB_MIG_ARMED) {\n\t\tqp->s_flags |= HFI1_S_AHG_CLEAR;\n\t\tpriv->s_sc = ah_to_sc(ibqp->device, &qp->remote_ah_attr);\n\t\tpriv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);\n\t\tpriv->s_sendcontext = qp_to_send_context(qp, priv->s_sc);\n\t\tqp_set_16b(qp);\n\t}\n\n\topfn_qp_init(qp, attr, attr_mask);\n}\n\n \nint hfi1_setup_wqe(struct rvt_qp *qp, struct rvt_swqe *wqe, bool *call_send)\n{\n\tstruct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\n\tstruct rvt_ah *ah;\n\tstruct hfi1_pportdata *ppd;\n\tstruct hfi1_devdata *dd;\n\n\tswitch (qp->ibqp.qp_type) {\n\tcase IB_QPT_RC:\n\t\thfi1_setup_tid_rdma_wqe(qp, wqe);\n\t\tfallthrough;\n\tcase IB_QPT_UC:\n\t\tif (wqe->length > 0x80000000U)\n\t\t\treturn -EINVAL;\n\t\tif (wqe->length > qp->pmtu)\n\t\t\t*call_send = false;\n\t\tbreak;\n\tcase IB_QPT_SMI:\n\t\t \n\t\tppd = ppd_from_ibp(ibp);\n\t\tdd = dd_from_ppd(ppd);\n\t\tif (wqe->length > dd->vld[15].mtu)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase IB_QPT_GSI:\n\tcase IB_QPT_UD:\n\t\tah = rvt_get_swqe_ah(wqe);\n\t\tif (wqe->length > (1 << ah->log_pmtu))\n\t\t\treturn -EINVAL;\n\t\tif (ibp->sl_to_sc[rdma_ah_get_sl(&ah->attr)] == 0xf)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (wqe->length <= piothreshold)\n\t\t*call_send = true;\n\treturn 0;\n}\n\n \nbool _hfi1_schedule_send(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct hfi1_ibport *ibp =\n\t\tto_iport(qp->ibqp.device, qp->port_num);\n\tstruct hfi1_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct hfi1_devdata *dd = ppd->dd;\n\n\tif (dd->flags & HFI1_SHUTDOWN)\n\t\treturn true;\n\n\treturn iowait_schedule(&priv->s_iowait, ppd->hfi1_wq,\n\t\t\t       priv->s_sde ?\n\t\t\t       priv->s_sde->cpu :\n\t\t\t       cpumask_first(cpumask_of_node(dd->node)));\n}\n\nstatic void qp_pio_drain(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tif (!priv->s_sendcontext)\n\t\treturn;\n\twhile (iowait_pio_pending(&priv->s_iowait)) {\n\t\twrite_seqlock_irq(&priv->s_sendcontext->waitlock);\n\t\thfi1_sc_wantpiobuf_intr(priv->s_sendcontext, 1);\n\t\twrite_sequnlock_irq(&priv->s_sendcontext->waitlock);\n\t\tiowait_pio_drain(&priv->s_iowait);\n\t\twrite_seqlock_irq(&priv->s_sendcontext->waitlock);\n\t\thfi1_sc_wantpiobuf_intr(priv->s_sendcontext, 0);\n\t\twrite_sequnlock_irq(&priv->s_sendcontext->waitlock);\n\t}\n}\n\n \nbool hfi1_schedule_send(struct rvt_qp *qp)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\tif (hfi1_send_ok(qp)) {\n\t\t_hfi1_schedule_send(qp);\n\t\treturn true;\n\t}\n\tif (qp->s_flags & HFI1_S_ANY_WAIT_IO)\n\t\tiowait_set_flag(&((struct hfi1_qp_priv *)qp->priv)->s_iowait,\n\t\t\t\tIOWAIT_PENDING_IB);\n\treturn false;\n}\n\nstatic void hfi1_qp_schedule(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tbool ret;\n\n\tif (iowait_flag_set(&priv->s_iowait, IOWAIT_PENDING_IB)) {\n\t\tret = hfi1_schedule_send(qp);\n\t\tif (ret)\n\t\t\tiowait_clear_flag(&priv->s_iowait, IOWAIT_PENDING_IB);\n\t}\n\tif (iowait_flag_set(&priv->s_iowait, IOWAIT_PENDING_TID)) {\n\t\tret = hfi1_schedule_tid_send(qp);\n\t\tif (ret)\n\t\t\tiowait_clear_flag(&priv->s_iowait, IOWAIT_PENDING_TID);\n\t}\n}\n\nvoid hfi1_qp_wakeup(struct rvt_qp *qp, u32 flag)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (qp->s_flags & flag) {\n\t\tqp->s_flags &= ~flag;\n\t\ttrace_hfi1_qpwakeup(qp, flag);\n\t\thfi1_qp_schedule(qp);\n\t}\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t \n\trvt_put_qp(qp);\n}\n\nvoid hfi1_qp_unbusy(struct rvt_qp *qp, struct iowait_work *wait)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tif (iowait_set_work_flag(wait) == IOWAIT_IB_SE) {\n\t\tqp->s_flags &= ~RVT_S_BUSY;\n\t\t \n\t\tif (priv->s_flags & HFI1_S_TID_BUSY_SET) {\n\t\t\tpriv->s_flags &= ~(HFI1_S_TID_BUSY_SET |\n\t\t\t\t\t   RVT_S_BUSY);\n\t\t\tiowait_set_flag(&priv->s_iowait, IOWAIT_PENDING_TID);\n\t\t}\n\t} else {\n\t\tpriv->s_flags &= ~RVT_S_BUSY;\n\t}\n}\n\nstatic int iowait_sleep(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *stx,\n\tuint seq,\n\tbool pkts_sent)\n{\n\tstruct verbs_txreq *tx = container_of(stx, struct verbs_txreq, txreq);\n\tstruct rvt_qp *qp;\n\tstruct hfi1_qp_priv *priv;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tqp = tx->qp;\n\tpriv = qp->priv;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\n\t\t \n\t\t \n\t\tlist_add_tail(&stx->list, &wait->tx_head);\n\t\twrite_seqlock(&sde->waitlock);\n\t\tif (sdma_progress(sde, seq, stx))\n\t\t\tgoto eagain;\n\t\tif (list_empty(&priv->s_iowait.list)) {\n\t\t\tstruct hfi1_ibport *ibp =\n\t\t\t\tto_iport(qp->ibqp.device, qp->port_num);\n\n\t\t\tibp->rvp.n_dmawait++;\n\t\t\tqp->s_flags |= RVT_S_WAIT_DMA_DESC;\n\t\t\tiowait_get_priority(&priv->s_iowait);\n\t\t\tiowait_queue(pkts_sent, &priv->s_iowait,\n\t\t\t\t     &sde->dmawait);\n\t\t\tpriv->s_iowait.lock = &sde->waitlock;\n\t\t\ttrace_hfi1_qpsleep(qp, RVT_S_WAIT_DMA_DESC);\n\t\t\trvt_get_qp(qp);\n\t\t}\n\t\twrite_sequnlock(&sde->waitlock);\n\t\thfi1_qp_unbusy(qp, wait);\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t\tret = -EBUSY;\n\t} else {\n\t\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\t\thfi1_put_txreq(tx);\n\t}\n\treturn ret;\neagain:\n\twrite_sequnlock(&sde->waitlock);\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\tlist_del_init(&stx->list);\n\treturn -EAGAIN;\n}\n\nstatic void iowait_wakeup(struct iowait *wait, int reason)\n{\n\tstruct rvt_qp *qp = iowait_to_qp(wait);\n\n\tWARN_ON(reason != SDMA_AVAIL_REASON);\n\thfi1_qp_wakeup(qp, RVT_S_WAIT_DMA_DESC);\n}\n\nstatic void iowait_sdma_drained(struct iowait *wait)\n{\n\tstruct rvt_qp *qp = iowait_to_qp(wait);\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tif (qp->s_flags & RVT_S_WAIT_DMA) {\n\t\tqp->s_flags &= ~RVT_S_WAIT_DMA;\n\t\thfi1_schedule_send(qp);\n\t}\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n}\n\nstatic void hfi1_init_priority(struct iowait *w)\n{\n\tstruct rvt_qp *qp = iowait_to_qp(w);\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tif (qp->s_flags & RVT_S_ACK_PENDING)\n\t\tw->priority++;\n\tif (priv->s_flags & RVT_S_ACK_PENDING)\n\t\tw->priority++;\n}\n\n \nstruct sdma_engine *qp_to_sdma_engine(struct rvt_qp *qp, u8 sc5)\n{\n\tstruct hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device);\n\tstruct sdma_engine *sde;\n\n\tif (!(dd->flags & HFI1_HAS_SEND_DMA))\n\t\treturn NULL;\n\tswitch (qp->ibqp.qp_type) {\n\tcase IB_QPT_SMI:\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\tsde = sdma_select_engine_sc(dd, qp->ibqp.qp_num >> dd->qos_shift, sc5);\n\treturn sde;\n}\n\n \nstruct send_context *qp_to_send_context(struct rvt_qp *qp, u8 sc5)\n{\n\tstruct hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device);\n\n\tswitch (qp->ibqp.qp_type) {\n\tcase IB_QPT_SMI:\n\t\t \n\t\treturn dd->vld[15].sc;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn pio_select_send_context_sc(dd, qp->ibqp.qp_num >> dd->qos_shift,\n\t\t\t\t\t  sc5);\n}\n\nstatic const char * const qp_type_str[] = {\n\t\"SMI\", \"GSI\", \"RC\", \"UC\", \"UD\",\n};\n\nstatic int qp_idle(struct rvt_qp *qp)\n{\n\treturn\n\t\tqp->s_last == qp->s_acked &&\n\t\tqp->s_acked == qp->s_cur &&\n\t\tqp->s_cur == qp->s_tail &&\n\t\tqp->s_tail == qp->s_head;\n}\n\n \nvoid qp_iter_print(struct seq_file *s, struct rvt_qp_iter *iter)\n{\n\tstruct rvt_swqe *wqe;\n\tstruct rvt_qp *qp = iter->qp;\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct sdma_engine *sde;\n\tstruct send_context *send_context;\n\tstruct rvt_ack_entry *e = NULL;\n\tstruct rvt_srq *srq = qp->ibqp.srq ?\n\t\tibsrq_to_rvtsrq(qp->ibqp.srq) : NULL;\n\n\tsde = qp_to_sdma_engine(qp, priv->s_sc);\n\twqe = rvt_get_swqe_ptr(qp, qp->s_last);\n\tsend_context = qp_to_send_context(qp, priv->s_sc);\n\tif (qp->s_ack_queue)\n\t\te = &qp->s_ack_queue[qp->s_tail_ack_queue];\n\tseq_printf(s,\n\t\t   \"N %d %s QP %x R %u %s %u %u f=%x %u %u %u %u %u %u SPSN %x %x %x %x %x RPSN %x S(%u %u %u %u %u %u %u) R(%u %u %u) RQP %x LID %x SL %u MTU %u %u %u %u %u SDE %p,%u SC %p,%u SCQ %u %u PID %d OS %x %x E %x %x %x RNR %d %s %d\\n\",\n\t\t   iter->n,\n\t\t   qp_idle(qp) ? \"I\" : \"B\",\n\t\t   qp->ibqp.qp_num,\n\t\t   atomic_read(&qp->refcount),\n\t\t   qp_type_str[qp->ibqp.qp_type],\n\t\t   qp->state,\n\t\t   wqe ? wqe->wr.opcode : 0,\n\t\t   qp->s_flags,\n\t\t   iowait_sdma_pending(&priv->s_iowait),\n\t\t   iowait_pio_pending(&priv->s_iowait),\n\t\t   !list_empty(&priv->s_iowait.list),\n\t\t   qp->timeout,\n\t\t   wqe ? wqe->ssn : 0,\n\t\t   qp->s_lsn,\n\t\t   qp->s_last_psn,\n\t\t   qp->s_psn, qp->s_next_psn,\n\t\t   qp->s_sending_psn, qp->s_sending_hpsn,\n\t\t   qp->r_psn,\n\t\t   qp->s_last, qp->s_acked, qp->s_cur,\n\t\t   qp->s_tail, qp->s_head, qp->s_size,\n\t\t   qp->s_avail,\n\t\t    \n\t\t   qp->s_tail_ack_queue, qp->r_head_ack_queue,\n\t\t   rvt_max_atomic(&to_idev(qp->ibqp.device)->rdi),\n\t\t    \n\t\t   qp->remote_qpn,\n\t\t   rdma_ah_get_dlid(&qp->remote_ah_attr),\n\t\t   rdma_ah_get_sl(&qp->remote_ah_attr),\n\t\t   qp->pmtu,\n\t\t   qp->s_retry,\n\t\t   qp->s_retry_cnt,\n\t\t   qp->s_rnr_retry_cnt,\n\t\t   qp->s_rnr_retry,\n\t\t   sde,\n\t\t   sde ? sde->this_idx : 0,\n\t\t   send_context,\n\t\t   send_context ? send_context->sw_index : 0,\n\t\t   ib_cq_head(qp->ibqp.send_cq),\n\t\t   ib_cq_tail(qp->ibqp.send_cq),\n\t\t   qp->pid,\n\t\t   qp->s_state,\n\t\t   qp->s_ack_state,\n\t\t    \n\t\t   e ? e->opcode : 0,\n\t\t   e ? e->psn : 0,\n\t\t   e ? e->lpsn : 0,\n\t\t   qp->r_min_rnr_timer,\n\t\t   srq ? \"SRQ\" : \"RQ\",\n\t\t   srq ? srq->rq.size : qp->r_rq.size\n\t\t);\n}\n\nvoid *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv;\n\n\tpriv = kzalloc_node(sizeof(*priv), GFP_KERNEL, rdi->dparms.node);\n\tif (!priv)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpriv->owner = qp;\n\n\tpriv->s_ahg = kzalloc_node(sizeof(*priv->s_ahg), GFP_KERNEL,\n\t\t\t\t   rdi->dparms.node);\n\tif (!priv->s_ahg) {\n\t\tkfree(priv);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tiowait_init(\n\t\t&priv->s_iowait,\n\t\t1,\n\t\t_hfi1_do_send,\n\t\t_hfi1_do_tid_send,\n\t\tiowait_sleep,\n\t\tiowait_wakeup,\n\t\tiowait_sdma_drained,\n\t\thfi1_init_priority);\n\t \n\tpriv->s_running_pkt_size = piothreshold / 2;\n\treturn priv;\n}\n\nvoid qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\thfi1_qp_priv_tid_free(rdi, qp);\n\tkfree(priv->s_ahg);\n\tkfree(priv);\n}\n\nunsigned free_all_qps(struct rvt_dev_info *rdi)\n{\n\tstruct hfi1_ibdev *verbs_dev = container_of(rdi,\n\t\t\t\t\t\t    struct hfi1_ibdev,\n\t\t\t\t\t\t    rdi);\n\tstruct hfi1_devdata *dd = container_of(verbs_dev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       verbs_dev);\n\tint n;\n\tunsigned qp_inuse = 0;\n\n\tfor (n = 0; n < dd->num_pports; n++) {\n\t\tstruct hfi1_ibport *ibp = &dd->pport[n].ibport_data;\n\n\t\trcu_read_lock();\n\t\tif (rcu_dereference(ibp->rvp.qp[0]))\n\t\t\tqp_inuse++;\n\t\tif (rcu_dereference(ibp->rvp.qp[1]))\n\t\t\tqp_inuse++;\n\t\trcu_read_unlock();\n\t}\n\n\treturn qp_inuse;\n}\n\nvoid flush_qp_waiters(struct rvt_qp *qp)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\tflush_iowait(qp);\n\thfi1_tid_rdma_flush_wait(qp);\n}\n\nvoid stop_send_queue(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\tiowait_cancel_work(&priv->s_iowait);\n\tif (cancel_work_sync(&priv->tid_rdma.trigger_work))\n\t\trvt_put_qp(qp);\n}\n\nvoid quiesce_qp(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\n\thfi1_del_tid_reap_timer(qp);\n\thfi1_del_tid_retry_timer(qp);\n\tiowait_sdma_drain(&priv->s_iowait);\n\tqp_pio_drain(qp);\n\tflush_tx_list(qp);\n}\n\nvoid notify_qp_reset(struct rvt_qp *qp)\n{\n\thfi1_qp_kern_exp_rcv_clear_all(qp);\n\tqp->r_adefered = 0;\n\tclear_ahg(qp);\n\n\t \n\tif (qp->ibqp.qp_type == IB_QPT_RC)\n\t\topfn_conn_error(qp);\n}\n\n \nvoid hfi1_migrate_qp(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tstruct ib_event ev;\n\n\tqp->s_mig_state = IB_MIG_MIGRATED;\n\tqp->remote_ah_attr = qp->alt_ah_attr;\n\tqp->port_num = rdma_ah_get_port_num(&qp->alt_ah_attr);\n\tqp->s_pkey_index = qp->s_alt_pkey_index;\n\tqp->s_flags |= HFI1_S_AHG_CLEAR;\n\tpriv->s_sc = ah_to_sc(qp->ibqp.device, &qp->remote_ah_attr);\n\tpriv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);\n\tqp_set_16b(qp);\n\n\tev.device = qp->ibqp.device;\n\tev.element.qp = &qp->ibqp;\n\tev.event = IB_EVENT_PATH_MIG;\n\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n}\n\nint mtu_to_path_mtu(u32 mtu)\n{\n\treturn mtu_to_enum(mtu, OPA_MTU_8192);\n}\n\nu32 mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu)\n{\n\tu32 mtu;\n\tstruct hfi1_ibdev *verbs_dev = container_of(rdi,\n\t\t\t\t\t\t    struct hfi1_ibdev,\n\t\t\t\t\t\t    rdi);\n\tstruct hfi1_devdata *dd = container_of(verbs_dev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       verbs_dev);\n\tstruct hfi1_ibport *ibp;\n\tu8 sc, vl;\n\n\tibp = &dd->pport[qp->port_num - 1].ibport_data;\n\tsc = ibp->sl_to_sc[rdma_ah_get_sl(&qp->remote_ah_attr)];\n\tvl = sc_to_vlt(dd, sc);\n\n\tmtu = verbs_mtu_enum_to_int(qp->ibqp.device, pmtu);\n\tif (vl < PER_VL_SEND_CONTEXTS)\n\t\tmtu = min_t(u32, mtu, dd->vld[vl].mtu);\n\treturn mtu;\n}\n\nint get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t       struct ib_qp_attr *attr)\n{\n\tint mtu, pidx = qp->port_num - 1;\n\tstruct hfi1_ibdev *verbs_dev = container_of(rdi,\n\t\t\t\t\t\t    struct hfi1_ibdev,\n\t\t\t\t\t\t    rdi);\n\tstruct hfi1_devdata *dd = container_of(verbs_dev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       verbs_dev);\n\tmtu = verbs_mtu_enum_to_int(qp->ibqp.device, attr->path_mtu);\n\tif (mtu == -1)\n\t\treturn -1;  \n\n\tif (mtu > dd->pport[pidx].ibmtu)\n\t\treturn mtu_to_enum(dd->pport[pidx].ibmtu, IB_MTU_2048);\n\telse\n\t\treturn attr->path_mtu;\n}\n\nvoid notify_error_qp(struct rvt_qp *qp)\n{\n\tstruct hfi1_qp_priv *priv = qp->priv;\n\tseqlock_t *lock = priv->s_iowait.lock;\n\n\tif (lock) {\n\t\twrite_seqlock(lock);\n\t\tif (!list_empty(&priv->s_iowait.list) &&\n\t\t    !(qp->s_flags & RVT_S_BUSY) &&\n\t\t    !(priv->s_flags & RVT_S_BUSY)) {\n\t\t\tqp->s_flags &= ~HFI1_S_ANY_WAIT_IO;\n\t\t\tiowait_clear_flag(&priv->s_iowait, IOWAIT_PENDING_IB);\n\t\t\tiowait_clear_flag(&priv->s_iowait, IOWAIT_PENDING_TID);\n\t\t\tlist_del_init(&priv->s_iowait.list);\n\t\t\tpriv->s_iowait.lock = NULL;\n\t\t\trvt_put_qp(qp);\n\t\t}\n\t\twrite_sequnlock(lock);\n\t}\n\n\tif (!(qp->s_flags & RVT_S_BUSY) && !(priv->s_flags & RVT_S_BUSY)) {\n\t\tqp->s_hdrwords = 0;\n\t\tif (qp->s_rdma_mr) {\n\t\t\trvt_put_mr(qp->s_rdma_mr);\n\t\t\tqp->s_rdma_mr = NULL;\n\t\t}\n\t\tflush_tx_list(qp);\n\t}\n}\n\n \nstatic void hfi1_qp_iter_cb(struct rvt_qp *qp, u64 v)\n{\n\tint lastwqe;\n\tstruct ib_event ev;\n\tstruct hfi1_ibport *ibp =\n\t\tto_iport(qp->ibqp.device, qp->port_num);\n\tstruct hfi1_pportdata *ppd = ppd_from_ibp(ibp);\n\tu8 sl = (u8)v;\n\n\tif (qp->port_num != ppd->port ||\n\t    (qp->ibqp.qp_type != IB_QPT_UC &&\n\t     qp->ibqp.qp_type != IB_QPT_RC) ||\n\t    rdma_ah_get_sl(&qp->remote_ah_attr) != sl ||\n\t    !(ib_rvt_state_ops[qp->state] & RVT_POST_SEND_OK))\n\t\treturn;\n\n\tspin_lock_irq(&qp->r_lock);\n\tspin_lock(&qp->s_hlock);\n\tspin_lock(&qp->s_lock);\n\tlastwqe = rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->s_hlock);\n\tspin_unlock_irq(&qp->r_lock);\n\tif (lastwqe) {\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n}\n\n \nvoid hfi1_error_port_qps(struct hfi1_ibport *ibp, u8 sl)\n{\n\tstruct hfi1_pportdata *ppd = ppd_from_ibp(ibp);\n\tstruct hfi1_ibdev *dev = &ppd->dd->verbs_dev;\n\n\trvt_qp_iter(&dev->rdi, sl, hfi1_qp_iter_cb);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}