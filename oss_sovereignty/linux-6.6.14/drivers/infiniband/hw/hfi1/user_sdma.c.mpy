{
  "module_name": "user_sdma.c",
  "hash_id": "023f8cde92b4e55247bf5676d04ecaf5c8621049027f19d9f4b7b7b499d62b9f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/user_sdma.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/dmapool.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/io.h>\n#include <linux/uio.h>\n#include <linux/rbtree.h>\n#include <linux/spinlock.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/mmu_context.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/string.h>\n\n#include \"hfi.h\"\n#include \"sdma.h\"\n#include \"user_sdma.h\"\n#include \"verbs.h\"   \n#include \"common.h\"  \n#include \"trace.h\"\n\nstatic uint hfi1_sdma_comp_ring_size = 128;\nmodule_param_named(sdma_comp_size, hfi1_sdma_comp_ring_size, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_comp_size, \"Size of User SDMA completion ring. Default: 128\");\n\nstatic unsigned initial_pkt_count = 8;\n\nstatic int user_sdma_send_pkts(struct user_sdma_request *req, u16 maxpkts);\nstatic void user_sdma_txreq_cb(struct sdma_txreq *txreq, int status);\nstatic inline void pq_update(struct hfi1_user_sdma_pkt_q *pq);\nstatic void user_sdma_free_request(struct user_sdma_request *req);\nstatic int check_header_template(struct user_sdma_request *req,\n\t\t\t\t struct hfi1_pkt_header *hdr, u32 lrhlen,\n\t\t\t\t u32 datalen);\nstatic int set_txreq_header(struct user_sdma_request *req,\n\t\t\t    struct user_sdma_txreq *tx, u32 datalen);\nstatic int set_txreq_header_ahg(struct user_sdma_request *req,\n\t\t\t\tstruct user_sdma_txreq *tx, u32 len);\nstatic inline void set_comp_state(struct hfi1_user_sdma_pkt_q *pq,\n\t\t\t\t  struct hfi1_user_sdma_comp_q *cq,\n\t\t\t\t  u16 idx, enum hfi1_sdma_comp_state state,\n\t\t\t\t  int ret);\nstatic inline u32 set_pkt_bth_psn(__be32 bthpsn, u8 expct, u32 frags);\nstatic inline u32 get_lrh_len(struct hfi1_pkt_header, u32 len);\n\nstatic int defer_packet_queue(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *txreq,\n\tuint seq,\n\tbool pkts_sent);\nstatic void activate_packet_queue(struct iowait *wait, int reason);\n\nstatic int defer_packet_queue(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *txreq,\n\tuint seq,\n\tbool pkts_sent)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq =\n\t\tcontainer_of(wait->iow, struct hfi1_user_sdma_pkt_q, busy);\n\n\twrite_seqlock(&sde->waitlock);\n\ttrace_hfi1_usdma_defer(pq, sde, &pq->busy);\n\tif (sdma_progress(sde, seq, txreq))\n\t\tgoto eagain;\n\t \n\txchg(&pq->state, SDMA_PKT_Q_DEFERRED);\n\tif (list_empty(&pq->busy.list)) {\n\t\tpq->busy.lock = &sde->waitlock;\n\t\tiowait_get_priority(&pq->busy);\n\t\tiowait_queue(pkts_sent, &pq->busy, &sde->dmawait);\n\t}\n\twrite_sequnlock(&sde->waitlock);\n\treturn -EBUSY;\neagain:\n\twrite_sequnlock(&sde->waitlock);\n\treturn -EAGAIN;\n}\n\nstatic void activate_packet_queue(struct iowait *wait, int reason)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq =\n\t\tcontainer_of(wait, struct hfi1_user_sdma_pkt_q, busy);\n\n\ttrace_hfi1_usdma_activate(pq, wait, reason);\n\txchg(&pq->state, SDMA_PKT_Q_ACTIVE);\n\twake_up(&wait->wait_dma);\n};\n\nint hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *uctxt,\n\t\t\t\tstruct hfi1_filedata *fd)\n{\n\tint ret = -ENOMEM;\n\tchar buf[64];\n\tstruct hfi1_devdata *dd;\n\tstruct hfi1_user_sdma_comp_q *cq;\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\n\tif (!uctxt || !fd)\n\t\treturn -EBADF;\n\n\tif (!hfi1_sdma_comp_ring_size)\n\t\treturn -EINVAL;\n\n\tdd = uctxt->dd;\n\n\tpq = kzalloc(sizeof(*pq), GFP_KERNEL);\n\tif (!pq)\n\t\treturn -ENOMEM;\n\tpq->dd = dd;\n\tpq->ctxt = uctxt->ctxt;\n\tpq->subctxt = fd->subctxt;\n\tpq->n_max_reqs = hfi1_sdma_comp_ring_size;\n\tatomic_set(&pq->n_reqs, 0);\n\tinit_waitqueue_head(&pq->wait);\n\tatomic_set(&pq->n_locked, 0);\n\n\tiowait_init(&pq->busy, 0, NULL, NULL, defer_packet_queue,\n\t\t    activate_packet_queue, NULL, NULL);\n\tpq->reqidx = 0;\n\n\tpq->reqs = kcalloc(hfi1_sdma_comp_ring_size,\n\t\t\t   sizeof(*pq->reqs),\n\t\t\t   GFP_KERNEL);\n\tif (!pq->reqs)\n\t\tgoto pq_reqs_nomem;\n\n\tpq->req_in_use = bitmap_zalloc(hfi1_sdma_comp_ring_size, GFP_KERNEL);\n\tif (!pq->req_in_use)\n\t\tgoto pq_reqs_no_in_use;\n\n\tsnprintf(buf, 64, \"txreq-kmem-cache-%u-%u-%u\", dd->unit, uctxt->ctxt,\n\t\t fd->subctxt);\n\tpq->txreq_cache = kmem_cache_create(buf,\n\t\t\t\t\t    sizeof(struct user_sdma_txreq),\n\t\t\t\t\t    L1_CACHE_BYTES,\n\t\t\t\t\t    SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t    NULL);\n\tif (!pq->txreq_cache) {\n\t\tdd_dev_err(dd, \"[%u] Failed to allocate TxReq cache\\n\",\n\t\t\t   uctxt->ctxt);\n\t\tgoto pq_txreq_nomem;\n\t}\n\n\tcq = kzalloc(sizeof(*cq), GFP_KERNEL);\n\tif (!cq)\n\t\tgoto cq_nomem;\n\n\tcq->comps = vmalloc_user(PAGE_ALIGN(sizeof(*cq->comps)\n\t\t\t\t * hfi1_sdma_comp_ring_size));\n\tif (!cq->comps)\n\t\tgoto cq_comps_nomem;\n\n\tcq->nentries = hfi1_sdma_comp_ring_size;\n\n\tret = hfi1_init_system_pinning(pq);\n\tif (ret)\n\t\tgoto pq_mmu_fail;\n\n\trcu_assign_pointer(fd->pq, pq);\n\tfd->cq = cq;\n\n\treturn 0;\n\npq_mmu_fail:\n\tvfree(cq->comps);\ncq_comps_nomem:\n\tkfree(cq);\ncq_nomem:\n\tkmem_cache_destroy(pq->txreq_cache);\npq_txreq_nomem:\n\tbitmap_free(pq->req_in_use);\npq_reqs_no_in_use:\n\tkfree(pq->reqs);\npq_reqs_nomem:\n\tkfree(pq);\n\n\treturn ret;\n}\n\nstatic void flush_pq_iowait(struct hfi1_user_sdma_pkt_q *pq)\n{\n\tunsigned long flags;\n\tseqlock_t *lock = pq->busy.lock;\n\n\tif (!lock)\n\t\treturn;\n\twrite_seqlock_irqsave(lock, flags);\n\tif (!list_empty(&pq->busy.list)) {\n\t\tlist_del_init(&pq->busy.list);\n\t\tpq->busy.lock = NULL;\n\t}\n\twrite_sequnlock_irqrestore(lock, flags);\n}\n\nint hfi1_user_sdma_free_queues(struct hfi1_filedata *fd,\n\t\t\t       struct hfi1_ctxtdata *uctxt)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\n\ttrace_hfi1_sdma_user_free_queues(uctxt->dd, uctxt->ctxt, fd->subctxt);\n\n\tspin_lock(&fd->pq_rcu_lock);\n\tpq = srcu_dereference_check(fd->pq, &fd->pq_srcu,\n\t\t\t\t    lockdep_is_held(&fd->pq_rcu_lock));\n\tif (pq) {\n\t\trcu_assign_pointer(fd->pq, NULL);\n\t\tspin_unlock(&fd->pq_rcu_lock);\n\t\tsynchronize_srcu(&fd->pq_srcu);\n\t\t \n\t\tiowait_sdma_drain(&pq->busy);\n\t\t \n\t\twait_event_interruptible(\n\t\t\tpq->wait,\n\t\t\t!atomic_read(&pq->n_reqs));\n\t\tkfree(pq->reqs);\n\t\thfi1_free_system_pinning(pq);\n\t\tbitmap_free(pq->req_in_use);\n\t\tkmem_cache_destroy(pq->txreq_cache);\n\t\tflush_pq_iowait(pq);\n\t\tkfree(pq);\n\t} else {\n\t\tspin_unlock(&fd->pq_rcu_lock);\n\t}\n\tif (fd->cq) {\n\t\tvfree(fd->cq->comps);\n\t\tkfree(fd->cq);\n\t\tfd->cq = NULL;\n\t}\n\treturn 0;\n}\n\nstatic u8 dlid_to_selector(u16 dlid)\n{\n\tstatic u8 mapping[256];\n\tstatic int initialized;\n\tstatic u8 next;\n\tint hash;\n\n\tif (!initialized) {\n\t\tmemset(mapping, 0xFF, 256);\n\t\tinitialized = 1;\n\t}\n\n\thash = ((dlid >> 8) ^ dlid) & 0xFF;\n\tif (mapping[hash] == 0xFF) {\n\t\tmapping[hash] = next;\n\t\tnext = (next + 1) & 0x7F;\n\t}\n\n\treturn mapping[hash];\n}\n\n \nint hfi1_user_sdma_process_request(struct hfi1_filedata *fd,\n\t\t\t\t   struct iovec *iovec, unsigned long dim,\n\t\t\t\t   unsigned long *count)\n{\n\tint ret = 0, i;\n\tstruct hfi1_ctxtdata *uctxt = fd->uctxt;\n\tstruct hfi1_user_sdma_pkt_q *pq =\n\t\tsrcu_dereference(fd->pq, &fd->pq_srcu);\n\tstruct hfi1_user_sdma_comp_q *cq = fd->cq;\n\tstruct hfi1_devdata *dd = pq->dd;\n\tunsigned long idx = 0;\n\tu8 pcount = initial_pkt_count;\n\tstruct sdma_req_info info;\n\tstruct user_sdma_request *req;\n\tu8 opcode, sc, vl;\n\tu16 pkey;\n\tu32 slid;\n\tu16 dlid;\n\tu32 selector;\n\n\tif (iovec[idx].iov_len < sizeof(info) + sizeof(req->hdr)) {\n\t\thfi1_cdbg(\n\t\t   SDMA,\n\t\t   \"[%u:%u:%u] First vector not big enough for header %lu/%lu\",\n\t\t   dd->unit, uctxt->ctxt, fd->subctxt,\n\t\t   iovec[idx].iov_len, sizeof(info) + sizeof(req->hdr));\n\t\treturn -EINVAL;\n\t}\n\tret = copy_from_user(&info, iovec[idx].iov_base, sizeof(info));\n\tif (ret) {\n\t\thfi1_cdbg(SDMA, \"[%u:%u:%u] Failed to copy info QW (%d)\",\n\t\t\t  dd->unit, uctxt->ctxt, fd->subctxt, ret);\n\t\treturn -EFAULT;\n\t}\n\n\ttrace_hfi1_sdma_user_reqinfo(dd, uctxt->ctxt, fd->subctxt,\n\t\t\t\t     (u16 *)&info);\n\tif (info.comp_idx >= hfi1_sdma_comp_ring_size) {\n\t\thfi1_cdbg(SDMA,\n\t\t\t  \"[%u:%u:%u:%u] Invalid comp index\",\n\t\t\t  dd->unit, uctxt->ctxt, fd->subctxt, info.comp_idx);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (req_iovcnt(info.ctrl) < 1 || req_iovcnt(info.ctrl) > dim) {\n\t\thfi1_cdbg(SDMA,\n\t\t\t  \"[%u:%u:%u:%u] Invalid iov count %d, dim %ld\",\n\t\t\t  dd->unit, uctxt->ctxt, fd->subctxt, info.comp_idx,\n\t\t\t  req_iovcnt(info.ctrl), dim);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!info.fragsize) {\n\t\thfi1_cdbg(SDMA,\n\t\t\t  \"[%u:%u:%u:%u] Request does not specify fragsize\",\n\t\t\t  dd->unit, uctxt->ctxt, fd->subctxt, info.comp_idx);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (test_and_set_bit(info.comp_idx, pq->req_in_use)) {\n\t\thfi1_cdbg(SDMA, \"[%u:%u:%u] Entry %u is in use\",\n\t\t\t  dd->unit, uctxt->ctxt, fd->subctxt,\n\t\t\t  info.comp_idx);\n\t\treturn -EBADSLT;\n\t}\n\t \n\ttrace_hfi1_sdma_user_process_request(dd, uctxt->ctxt, fd->subctxt,\n\t\t\t\t\t     info.comp_idx);\n\treq = pq->reqs + info.comp_idx;\n\treq->data_iovs = req_iovcnt(info.ctrl) - 1;  \n\treq->data_len  = 0;\n\treq->pq = pq;\n\treq->cq = cq;\n\treq->ahg_idx = -1;\n\treq->iov_idx = 0;\n\treq->sent = 0;\n\treq->seqnum = 0;\n\treq->seqcomp = 0;\n\treq->seqsubmitted = 0;\n\treq->tids = NULL;\n\treq->has_error = 0;\n\tINIT_LIST_HEAD(&req->txps);\n\n\tmemcpy(&req->info, &info, sizeof(info));\n\n\t \n\tatomic_inc(&pq->n_reqs);\n\n\tif (req_opcode(info.ctrl) == EXPECTED) {\n\t\t \n\t\tif (req->data_iovs < 2) {\n\t\t\tSDMA_DBG(req,\n\t\t\t\t \"Not enough vectors for expected request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_req;\n\t\t}\n\t\treq->data_iovs--;\n\t}\n\n\tif (!info.npkts || req->data_iovs > MAX_VECTORS_PER_REQ) {\n\t\tSDMA_DBG(req, \"Too many vectors (%u/%u)\", req->data_iovs,\n\t\t\t MAX_VECTORS_PER_REQ);\n\t\tret = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\t \n\tret = copy_from_user(&req->hdr, iovec[idx].iov_base + sizeof(info),\n\t\t\t     sizeof(req->hdr));\n\tif (ret) {\n\t\tSDMA_DBG(req, \"Failed to copy header template (%d)\", ret);\n\t\tret = -EFAULT;\n\t\tgoto free_req;\n\t}\n\n\t \n\tif (!HFI1_CAP_IS_USET(STATIC_RATE_CTRL))\n\t\treq->hdr.pbc[2] = 0;\n\n\t \n\topcode = (be32_to_cpu(req->hdr.bth[0]) >> 24) & 0xff;\n\tif ((opcode & USER_OPCODE_CHECK_MASK) !=\n\t     USER_OPCODE_CHECK_VAL) {\n\t\tSDMA_DBG(req, \"Invalid opcode (%d)\", opcode);\n\t\tret = -EINVAL;\n\t\tgoto free_req;\n\t}\n\t \n\tvl = (le16_to_cpu(req->hdr.pbc[0]) >> 12) & 0xF;\n\tsc = (((be16_to_cpu(req->hdr.lrh[0]) >> 12) & 0xF) |\n\t      (((le16_to_cpu(req->hdr.pbc[1]) >> 14) & 0x1) << 4));\n\tif (vl >= dd->pport->vls_operational ||\n\t    vl != sc_to_vlt(dd, sc)) {\n\t\tSDMA_DBG(req, \"Invalid SC(%u)/VL(%u)\", sc, vl);\n\t\tret = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\t \n\tpkey = (u16)be32_to_cpu(req->hdr.bth[0]);\n\tslid = be16_to_cpu(req->hdr.lrh[3]);\n\tif (egress_pkey_check(dd->pport, slid, pkey, sc, PKEY_CHECK_INVALID)) {\n\t\tret = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\t \n\tif ((be16_to_cpu(req->hdr.lrh[0]) & 0x3) == HFI1_LRH_GRH) {\n\t\tSDMA_DBG(req, \"User tried to pass in a GRH\");\n\t\tret = -EINVAL;\n\t\tgoto free_req;\n\t}\n\n\treq->koffset = le32_to_cpu(req->hdr.kdeth.swdata[6]);\n\t \n\treq->tidoffset = KDETH_GET(req->hdr.kdeth.ver_tid_offset, OFFSET) *\n\t\t(KDETH_GET(req->hdr.kdeth.ver_tid_offset, OM) ?\n\t\t KDETH_OM_LARGE : KDETH_OM_SMALL);\n\ttrace_hfi1_sdma_user_initial_tidoffset(dd, uctxt->ctxt, fd->subctxt,\n\t\t\t\t\t       info.comp_idx, req->tidoffset);\n\tidx++;\n\n\t \n\tfor (i = 0; i < req->data_iovs; i++) {\n\t\treq->iovs[i].offset = 0;\n\t\tINIT_LIST_HEAD(&req->iovs[i].list);\n\t\tmemcpy(&req->iovs[i].iov,\n\t\t       iovec + idx++,\n\t\t       sizeof(req->iovs[i].iov));\n\t\tif (req->iovs[i].iov.iov_len == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_req;\n\t\t}\n\t\treq->data_len += req->iovs[i].iov.iov_len;\n\t}\n\ttrace_hfi1_sdma_user_data_length(dd, uctxt->ctxt, fd->subctxt,\n\t\t\t\t\t info.comp_idx, req->data_len);\n\tif (pcount > req->info.npkts)\n\t\tpcount = req->info.npkts;\n\t \n\tif (req_opcode(req->info.ctrl) == EXPECTED) {\n\t\tu16 ntids = iovec[idx].iov_len / sizeof(*req->tids);\n\t\tu32 *tmp;\n\n\t\tif (!ntids || ntids > MAX_TID_PAIR_ENTRIES) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_req;\n\t\t}\n\n\t\t \n\t\ttmp = memdup_user(iovec[idx].iov_base,\n\t\t\t\t  ntids * sizeof(*req->tids));\n\t\tif (IS_ERR(tmp)) {\n\t\t\tret = PTR_ERR(tmp);\n\t\t\tSDMA_DBG(req, \"Failed to copy %d TIDs (%d)\",\n\t\t\t\t ntids, ret);\n\t\t\tgoto free_req;\n\t\t}\n\t\treq->tids = tmp;\n\t\treq->n_tids = ntids;\n\t\treq->tididx = 0;\n\t\tidx++;\n\t}\n\n\tdlid = be16_to_cpu(req->hdr.lrh[1]);\n\tselector = dlid_to_selector(dlid);\n\tselector += uctxt->ctxt + fd->subctxt;\n\treq->sde = sdma_select_user_engine(dd, selector, vl);\n\n\tif (!req->sde || !sdma_running(req->sde)) {\n\t\tret = -ECOMM;\n\t\tgoto free_req;\n\t}\n\n\t \n\tif (req->info.npkts > 1 && HFI1_CAP_IS_USET(SDMA_AHG))\n\t\treq->ahg_idx = sdma_ahg_alloc(req->sde);\n\n\tset_comp_state(pq, cq, info.comp_idx, QUEUED, 0);\n\tpq->state = SDMA_PKT_Q_ACTIVE;\n\n\t \n\twhile (req->seqsubmitted != req->info.npkts) {\n\t\tret = user_sdma_send_pkts(req, pcount);\n\t\tif (ret < 0) {\n\t\t\tint we_ret;\n\n\t\t\tif (ret != -EBUSY)\n\t\t\t\tgoto free_req;\n\t\t\twe_ret = wait_event_interruptible_timeout(\n\t\t\t\tpq->busy.wait_dma,\n\t\t\t\tpq->state == SDMA_PKT_Q_ACTIVE,\n\t\t\t\tmsecs_to_jiffies(\n\t\t\t\t\tSDMA_IOWAIT_TIMEOUT));\n\t\t\ttrace_hfi1_usdma_we(pq, we_ret);\n\t\t\tif (we_ret <= 0)\n\t\t\t\tflush_pq_iowait(pq);\n\t\t}\n\t}\n\t*count += idx;\n\treturn 0;\nfree_req:\n\t \n\tif (req->seqsubmitted < req->info.npkts) {\n\t\tif (req->seqsubmitted)\n\t\t\twait_event(pq->busy.wait_dma,\n\t\t\t\t   (req->seqcomp == req->seqsubmitted - 1));\n\t\tuser_sdma_free_request(req);\n\t\tpq_update(pq);\n\t\tset_comp_state(pq, cq, info.comp_idx, ERROR, ret);\n\t}\n\treturn ret;\n}\n\nstatic inline u32 compute_data_length(struct user_sdma_request *req,\n\t\t\t\t      struct user_sdma_txreq *tx)\n{\n\t \n\tu32 len;\n\n\tif (!req->seqnum) {\n\t\tif (req->data_len < sizeof(u32))\n\t\t\tlen = req->data_len;\n\t\telse\n\t\t\tlen = ((be16_to_cpu(req->hdr.lrh[2]) << 2) -\n\t\t\t       (sizeof(tx->hdr) - 4));\n\t} else if (req_opcode(req->info.ctrl) == EXPECTED) {\n\t\tu32 tidlen = EXP_TID_GET(req->tids[req->tididx], LEN) *\n\t\t\tPAGE_SIZE;\n\t\t \n\t\tlen = min(tidlen - req->tidoffset, (u32)req->info.fragsize);\n\t\t \n\t\tif (unlikely(!len) && ++req->tididx < req->n_tids &&\n\t\t    req->tids[req->tididx]) {\n\t\t\ttidlen = EXP_TID_GET(req->tids[req->tididx],\n\t\t\t\t\t     LEN) * PAGE_SIZE;\n\t\t\treq->tidoffset = 0;\n\t\t\tlen = min_t(u32, tidlen, req->info.fragsize);\n\t\t}\n\t\t \n\t\tlen = min(len, req->data_len - req->sent);\n\t} else {\n\t\tlen = min(req->data_len - req->sent, (u32)req->info.fragsize);\n\t}\n\ttrace_hfi1_sdma_user_compute_length(req->pq->dd,\n\t\t\t\t\t    req->pq->ctxt,\n\t\t\t\t\t    req->pq->subctxt,\n\t\t\t\t\t    req->info.comp_idx,\n\t\t\t\t\t    len);\n\treturn len;\n}\n\nstatic inline u32 pad_len(u32 len)\n{\n\tif (len & (sizeof(u32) - 1))\n\t\tlen += sizeof(u32) - (len & (sizeof(u32) - 1));\n\treturn len;\n}\n\nstatic inline u32 get_lrh_len(struct hfi1_pkt_header hdr, u32 len)\n{\n\t \n\treturn ((sizeof(hdr) - sizeof(hdr.pbc)) + 4 + len);\n}\n\nstatic int user_sdma_txadd_ahg(struct user_sdma_request *req,\n\t\t\t       struct user_sdma_txreq *tx,\n\t\t\t       u32 datalen)\n{\n\tint ret;\n\tu16 pbclen = le16_to_cpu(req->hdr.pbc[0]);\n\tu32 lrhlen = get_lrh_len(req->hdr, pad_len(datalen));\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\n\t \n\tmemcpy(&tx->hdr, &req->hdr, sizeof(tx->hdr));\n\tif (PBC2LRH(pbclen) != lrhlen) {\n\t\tpbclen = (pbclen & 0xf000) | LRH2PBC(lrhlen);\n\t\ttx->hdr.pbc[0] = cpu_to_le16(pbclen);\n\t}\n\tret = check_header_template(req, &tx->hdr, lrhlen, datalen);\n\tif (ret)\n\t\treturn ret;\n\tret = sdma_txinit_ahg(&tx->txreq, SDMA_TXREQ_F_AHG_COPY,\n\t\t\t      sizeof(tx->hdr) + datalen, req->ahg_idx,\n\t\t\t      0, NULL, 0, user_sdma_txreq_cb);\n\tif (ret)\n\t\treturn ret;\n\tret = sdma_txadd_kvaddr(pq->dd, &tx->txreq, &tx->hdr, sizeof(tx->hdr));\n\tif (ret)\n\t\tsdma_txclean(pq->dd, &tx->txreq);\n\treturn ret;\n}\n\nstatic int user_sdma_send_pkts(struct user_sdma_request *req, u16 maxpkts)\n{\n\tint ret = 0;\n\tu16 count;\n\tunsigned npkts = 0;\n\tstruct user_sdma_txreq *tx = NULL;\n\tstruct hfi1_user_sdma_pkt_q *pq = NULL;\n\tstruct user_sdma_iovec *iovec = NULL;\n\n\tif (!req->pq)\n\t\treturn -EINVAL;\n\n\tpq = req->pq;\n\n\t \n\tif (READ_ONCE(req->has_error))\n\t\treturn -EFAULT;\n\n\t \n\tif (unlikely(req->seqnum == req->info.npkts)) {\n\t\tif (!list_empty(&req->txps))\n\t\t\tgoto dosend;\n\t\treturn ret;\n\t}\n\n\tif (!maxpkts || maxpkts > req->info.npkts - req->seqnum)\n\t\tmaxpkts = req->info.npkts - req->seqnum;\n\n\twhile (npkts < maxpkts) {\n\t\tu32 datalen = 0;\n\n\t\t \n\t\tif (READ_ONCE(req->has_error))\n\t\t\treturn -EFAULT;\n\n\t\ttx = kmem_cache_alloc(pq->txreq_cache, GFP_KERNEL);\n\t\tif (!tx)\n\t\t\treturn -ENOMEM;\n\n\t\ttx->flags = 0;\n\t\ttx->req = req;\n\t\tINIT_LIST_HEAD(&tx->list);\n\n\t\t \n\t\tif (req->seqnum == req->info.npkts - 1)\n\t\t\ttx->flags |= (TXREQ_FLAGS_REQ_ACK |\n\t\t\t\t      TXREQ_FLAGS_REQ_DISABLE_SH);\n\n\t\t \n\t\tif (req->data_len) {\n\t\t\tiovec = &req->iovs[req->iov_idx];\n\t\t\tif (READ_ONCE(iovec->offset) == iovec->iov.iov_len) {\n\t\t\t\tif (++req->iov_idx == req->data_iovs) {\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\t\tgoto free_tx;\n\t\t\t\t}\n\t\t\t\tiovec = &req->iovs[req->iov_idx];\n\t\t\t\tWARN_ON(iovec->offset);\n\t\t\t}\n\n\t\t\tdatalen = compute_data_length(req, tx);\n\n\t\t\t \n\t\t\tif (!datalen) {\n\t\t\t\tSDMA_DBG(req,\n\t\t\t\t\t \"Request has data but pkt len is 0\");\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_tx;\n\t\t\t} else if (datalen <= 32) {\n\t\t\t\ttx->flags |= TXREQ_FLAGS_REQ_DISABLE_SH;\n\t\t\t}\n\t\t}\n\n\t\tif (req->ahg_idx >= 0) {\n\t\t\tif (!req->seqnum) {\n\t\t\t\tret = user_sdma_txadd_ahg(req, tx, datalen);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto free_tx;\n\t\t\t} else {\n\t\t\t\tint changes;\n\n\t\t\t\tchanges = set_txreq_header_ahg(req, tx,\n\t\t\t\t\t\t\t       datalen);\n\t\t\t\tif (changes < 0) {\n\t\t\t\t\tret = changes;\n\t\t\t\t\tgoto free_tx;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tret = sdma_txinit(&tx->txreq, 0, sizeof(req->hdr) +\n\t\t\t\t\t  datalen, user_sdma_txreq_cb);\n\t\t\tif (ret)\n\t\t\t\tgoto free_tx;\n\t\t\t \n\t\t\tret = set_txreq_header(req, tx, datalen);\n\t\t\tif (ret)\n\t\t\t\tgoto free_txreq;\n\t\t}\n\n\t\treq->koffset += datalen;\n\t\tif (req_opcode(req->info.ctrl) == EXPECTED)\n\t\t\treq->tidoffset += datalen;\n\t\treq->sent += datalen;\n\t\twhile (datalen) {\n\t\t\tret = hfi1_add_pages_to_sdma_packet(req, tx, iovec,\n\t\t\t\t\t\t\t    &datalen);\n\t\t\tif (ret)\n\t\t\t\tgoto free_txreq;\n\t\t\tiovec = &req->iovs[req->iov_idx];\n\t\t}\n\t\tlist_add_tail(&tx->txreq.list, &req->txps);\n\t\t \n\t\ttx->seqnum = req->seqnum++;\n\t\tnpkts++;\n\t}\ndosend:\n\tret = sdma_send_txlist(req->sde,\n\t\t\t       iowait_get_ib_work(&pq->busy),\n\t\t\t       &req->txps, &count);\n\treq->seqsubmitted += count;\n\tif (req->seqsubmitted == req->info.npkts) {\n\t\t \n\t\tif (req->ahg_idx >= 0)\n\t\t\tsdma_ahg_free(req->sde, req->ahg_idx);\n\t}\n\treturn ret;\n\nfree_txreq:\n\tsdma_txclean(pq->dd, &tx->txreq);\nfree_tx:\n\tkmem_cache_free(pq->txreq_cache, tx);\n\treturn ret;\n}\n\nstatic int check_header_template(struct user_sdma_request *req,\n\t\t\t\t struct hfi1_pkt_header *hdr, u32 lrhlen,\n\t\t\t\t u32 datalen)\n{\n\t \n\tif (req->info.fragsize % PIO_BLOCK_SIZE || lrhlen & 0x3 ||\n\t    lrhlen > get_lrh_len(*hdr, req->info.fragsize))\n\t\treturn -EINVAL;\n\n\tif (req_opcode(req->info.ctrl) == EXPECTED) {\n\t\t \n\t\tu32 tidval = req->tids[req->tididx],\n\t\t\ttidlen = EXP_TID_GET(tidval, LEN) * PAGE_SIZE,\n\t\t\ttididx = EXP_TID_GET(tidval, IDX),\n\t\t\ttidctrl = EXP_TID_GET(tidval, CTRL),\n\t\t\ttidoff;\n\t\t__le32 kval = hdr->kdeth.ver_tid_offset;\n\n\t\ttidoff = KDETH_GET(kval, OFFSET) *\n\t\t\t  (KDETH_GET(req->hdr.kdeth.ver_tid_offset, OM) ?\n\t\t\t   KDETH_OM_LARGE : KDETH_OM_SMALL);\n\t\t \n\t\tif ((tidoff + datalen > tidlen) ||\n\t\t    KDETH_GET(kval, TIDCTRL) != tidctrl ||\n\t\t    KDETH_GET(kval, TID) != tididx)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic inline u32 set_pkt_bth_psn(__be32 bthpsn, u8 expct, u32 frags)\n{\n\tu32 val = be32_to_cpu(bthpsn),\n\t\tmask = (HFI1_CAP_IS_KSET(EXTENDED_PSN) ? 0x7fffffffull :\n\t\t\t0xffffffull),\n\t\tpsn = val & mask;\n\tif (expct)\n\t\tpsn = (psn & ~HFI1_KDETH_BTH_SEQ_MASK) |\n\t\t\t((psn + frags) & HFI1_KDETH_BTH_SEQ_MASK);\n\telse\n\t\tpsn = psn + frags;\n\treturn psn & mask;\n}\n\nstatic int set_txreq_header(struct user_sdma_request *req,\n\t\t\t    struct user_sdma_txreq *tx, u32 datalen)\n{\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tstruct hfi1_pkt_header *hdr = &tx->hdr;\n\tu8 omfactor;  \n\tu16 pbclen;\n\tint ret;\n\tu32 tidval = 0, lrhlen = get_lrh_len(*hdr, pad_len(datalen));\n\n\t \n\tmemcpy(hdr, &req->hdr, sizeof(*hdr));\n\n\t \n\tpbclen = le16_to_cpu(hdr->pbc[0]);\n\tif (PBC2LRH(pbclen) != lrhlen) {\n\t\tpbclen = (pbclen & 0xf000) | LRH2PBC(lrhlen);\n\t\thdr->pbc[0] = cpu_to_le16(pbclen);\n\t\thdr->lrh[2] = cpu_to_be16(lrhlen >> 2);\n\t\t \n\t\tif (unlikely(req->seqnum == 2)) {\n\t\t\t \n\t\t\treq->hdr.pbc[0] = hdr->pbc[0];\n\t\t\treq->hdr.lrh[2] = hdr->lrh[2];\n\t\t}\n\t}\n\t \n\tif (unlikely(!req->seqnum)) {\n\t\tret = check_header_template(req, hdr, lrhlen, datalen);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto done;\n\t}\n\n\thdr->bth[2] = cpu_to_be32(\n\t\tset_pkt_bth_psn(hdr->bth[2],\n\t\t\t\t(req_opcode(req->info.ctrl) == EXPECTED),\n\t\t\t\treq->seqnum));\n\n\t \n\tif (unlikely(tx->flags & TXREQ_FLAGS_REQ_ACK))\n\t\thdr->bth[2] |= cpu_to_be32(1UL << 31);\n\n\t \n\thdr->kdeth.swdata[6] = cpu_to_le32(req->koffset);\n\t \n\tif (req_opcode(req->info.ctrl) == EXPECTED) {\n\t\ttidval = req->tids[req->tididx];\n\t\t \n\t\tif ((req->tidoffset) == (EXP_TID_GET(tidval, LEN) *\n\t\t\t\t\t PAGE_SIZE)) {\n\t\t\treq->tidoffset = 0;\n\t\t\t \n\t\t\tif (++req->tididx > req->n_tids - 1 ||\n\t\t\t    !req->tids[req->tididx]) {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\ttidval = req->tids[req->tididx];\n\t\t}\n\t\tomfactor = EXP_TID_GET(tidval, LEN) * PAGE_SIZE >=\n\t\t\tKDETH_OM_MAX_SIZE ? KDETH_OM_LARGE_SHIFT :\n\t\t\tKDETH_OM_SMALL_SHIFT;\n\t\t \n\t\tKDETH_SET(hdr->kdeth.ver_tid_offset, TIDCTRL,\n\t\t\t  EXP_TID_GET(tidval, CTRL));\n\t\t \n\t\tKDETH_SET(hdr->kdeth.ver_tid_offset, TID,\n\t\t\t  EXP_TID_GET(tidval, IDX));\n\t\t \n\t\tif (unlikely(tx->flags & TXREQ_FLAGS_REQ_DISABLE_SH))\n\t\t\tKDETH_SET(hdr->kdeth.ver_tid_offset, SH, 0);\n\t\t \n\t\ttrace_hfi1_sdma_user_tid_info(\n\t\t\tpq->dd, pq->ctxt, pq->subctxt, req->info.comp_idx,\n\t\t\treq->tidoffset, req->tidoffset >> omfactor,\n\t\t\tomfactor != KDETH_OM_SMALL_SHIFT);\n\t\tKDETH_SET(hdr->kdeth.ver_tid_offset, OFFSET,\n\t\t\t  req->tidoffset >> omfactor);\n\t\tKDETH_SET(hdr->kdeth.ver_tid_offset, OM,\n\t\t\t  omfactor != KDETH_OM_SMALL_SHIFT);\n\t}\ndone:\n\ttrace_hfi1_sdma_user_header(pq->dd, pq->ctxt, pq->subctxt,\n\t\t\t\t    req->info.comp_idx, hdr, tidval);\n\treturn sdma_txadd_kvaddr(pq->dd, &tx->txreq, hdr, sizeof(*hdr));\n}\n\nstatic int set_txreq_header_ahg(struct user_sdma_request *req,\n\t\t\t\tstruct user_sdma_txreq *tx, u32 datalen)\n{\n\tu32 ahg[AHG_KDETH_ARRAY_SIZE];\n\tint idx = 0;\n\tu8 omfactor;  \n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\tstruct hfi1_pkt_header *hdr = &req->hdr;\n\tu16 pbclen = le16_to_cpu(hdr->pbc[0]);\n\tu32 val32, tidval = 0, lrhlen = get_lrh_len(*hdr, pad_len(datalen));\n\tsize_t array_size = ARRAY_SIZE(ahg);\n\n\tif (PBC2LRH(pbclen) != lrhlen) {\n\t\t \n\t\tidx = ahg_header_set(ahg, idx, array_size, 0, 0, 12,\n\t\t\t\t     (__force u16)cpu_to_le16(LRH2PBC(lrhlen)));\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\t \n\t\tidx = ahg_header_set(ahg, idx, array_size, 3, 0, 16,\n\t\t\t\t     (__force u16)cpu_to_be16(lrhlen >> 2));\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t}\n\n\t \n\t \n\tval32 = (be32_to_cpu(hdr->bth[2]) + req->seqnum) &\n\t\t(HFI1_CAP_IS_KSET(EXTENDED_PSN) ? 0x7fffffff : 0xffffff);\n\tif (unlikely(tx->flags & TXREQ_FLAGS_REQ_ACK))\n\t\tval32 |= 1UL << 31;\n\tidx = ahg_header_set(ahg, idx, array_size, 6, 0, 16,\n\t\t\t     (__force u16)cpu_to_be16(val32 >> 16));\n\tif (idx < 0)\n\t\treturn idx;\n\tidx = ahg_header_set(ahg, idx, array_size, 6, 16, 16,\n\t\t\t     (__force u16)cpu_to_be16(val32 & 0xffff));\n\tif (idx < 0)\n\t\treturn idx;\n\t \n\tidx = ahg_header_set(ahg, idx, array_size, 15, 0, 16,\n\t\t\t     (__force u16)cpu_to_le16(req->koffset & 0xffff));\n\tif (idx < 0)\n\t\treturn idx;\n\tidx = ahg_header_set(ahg, idx, array_size, 15, 16, 16,\n\t\t\t     (__force u16)cpu_to_le16(req->koffset >> 16));\n\tif (idx < 0)\n\t\treturn idx;\n\tif (req_opcode(req->info.ctrl) == EXPECTED) {\n\t\t__le16 val;\n\n\t\ttidval = req->tids[req->tididx];\n\n\t\t \n\t\tif ((req->tidoffset) == (EXP_TID_GET(tidval, LEN) *\n\t\t\t\t\t PAGE_SIZE)) {\n\t\t\treq->tidoffset = 0;\n\t\t\t \n\t\t\tif (++req->tididx > req->n_tids - 1 ||\n\t\t\t    !req->tids[req->tididx])\n\t\t\t\treturn -EINVAL;\n\t\t\ttidval = req->tids[req->tididx];\n\t\t}\n\t\tomfactor = ((EXP_TID_GET(tidval, LEN) *\n\t\t\t\t  PAGE_SIZE) >=\n\t\t\t\t KDETH_OM_MAX_SIZE) ? KDETH_OM_LARGE_SHIFT :\n\t\t\t\t KDETH_OM_SMALL_SHIFT;\n\t\t \n\t\tidx = ahg_header_set(\n\t\t\t\tahg, idx, array_size, 7, 0, 16,\n\t\t\t\t((!!(omfactor - KDETH_OM_SMALL_SHIFT)) << 15 |\n\t\t\t\t((req->tidoffset >> omfactor)\n\t\t\t\t& 0x7fff)));\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\t \n\t\tval = cpu_to_le16(((EXP_TID_GET(tidval, CTRL) & 0x3) << 10) |\n\t\t\t\t   (EXP_TID_GET(tidval, IDX) & 0x3ff));\n\n\t\tif (unlikely(tx->flags & TXREQ_FLAGS_REQ_DISABLE_SH)) {\n\t\t\tval |= cpu_to_le16((KDETH_GET(hdr->kdeth.ver_tid_offset,\n\t\t\t\t\t\t      INTR) <<\n\t\t\t\t\t    AHG_KDETH_INTR_SHIFT));\n\t\t} else {\n\t\t\tval |= KDETH_GET(hdr->kdeth.ver_tid_offset, SH) ?\n\t\t\t       cpu_to_le16(0x1 << AHG_KDETH_SH_SHIFT) :\n\t\t\t       cpu_to_le16((KDETH_GET(hdr->kdeth.ver_tid_offset,\n\t\t\t\t\t\t      INTR) <<\n\t\t\t\t\t     AHG_KDETH_INTR_SHIFT));\n\t\t}\n\n\t\tidx = ahg_header_set(ahg, idx, array_size,\n\t\t\t\t     7, 16, 14, (__force u16)val);\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t}\n\n\ttrace_hfi1_sdma_user_header_ahg(pq->dd, pq->ctxt, pq->subctxt,\n\t\t\t\t\treq->info.comp_idx, req->sde->this_idx,\n\t\t\t\t\treq->ahg_idx, ahg, idx, tidval);\n\tsdma_txinit_ahg(&tx->txreq,\n\t\t\tSDMA_TXREQ_F_USE_AHG,\n\t\t\tdatalen, req->ahg_idx, idx,\n\t\t\tahg, sizeof(req->hdr),\n\t\t\tuser_sdma_txreq_cb);\n\n\treturn idx;\n}\n\n \nstatic void user_sdma_txreq_cb(struct sdma_txreq *txreq, int status)\n{\n\tstruct user_sdma_txreq *tx =\n\t\tcontainer_of(txreq, struct user_sdma_txreq, txreq);\n\tstruct user_sdma_request *req;\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\tstruct hfi1_user_sdma_comp_q *cq;\n\tenum hfi1_sdma_comp_state state = COMPLETE;\n\n\tif (!tx->req)\n\t\treturn;\n\n\treq = tx->req;\n\tpq = req->pq;\n\tcq = req->cq;\n\n\tif (status != SDMA_TXREQ_S_OK) {\n\t\tSDMA_DBG(req, \"SDMA completion with error %d\",\n\t\t\t status);\n\t\tWRITE_ONCE(req->has_error, 1);\n\t\tstate = ERROR;\n\t}\n\n\treq->seqcomp = tx->seqnum;\n\tkmem_cache_free(pq->txreq_cache, tx);\n\n\t \n\tif (req->seqcomp != req->info.npkts - 1)\n\t\treturn;\n\n\tuser_sdma_free_request(req);\n\tset_comp_state(pq, cq, req->info.comp_idx, state, status);\n\tpq_update(pq);\n}\n\nstatic inline void pq_update(struct hfi1_user_sdma_pkt_q *pq)\n{\n\tif (atomic_dec_and_test(&pq->n_reqs))\n\t\twake_up(&pq->wait);\n}\n\nstatic void user_sdma_free_request(struct user_sdma_request *req)\n{\n\tif (!list_empty(&req->txps)) {\n\t\tstruct sdma_txreq *t, *p;\n\n\t\tlist_for_each_entry_safe(t, p, &req->txps, list) {\n\t\t\tstruct user_sdma_txreq *tx =\n\t\t\t\tcontainer_of(t, struct user_sdma_txreq, txreq);\n\t\t\tlist_del_init(&t->list);\n\t\t\tsdma_txclean(req->pq->dd, t);\n\t\t\tkmem_cache_free(req->pq->txreq_cache, tx);\n\t\t}\n\t}\n\n\tkfree(req->tids);\n\tclear_bit(req->info.comp_idx, req->pq->req_in_use);\n}\n\nstatic inline void set_comp_state(struct hfi1_user_sdma_pkt_q *pq,\n\t\t\t\t  struct hfi1_user_sdma_comp_q *cq,\n\t\t\t\t  u16 idx, enum hfi1_sdma_comp_state state,\n\t\t\t\t  int ret)\n{\n\tif (state == ERROR)\n\t\tcq->comps[idx].errcode = -ret;\n\tsmp_wmb();  \n\tcq->comps[idx].status = state;\n\ttrace_hfi1_sdma_user_completion(pq->dd, pq->ctxt, pq->subctxt,\n\t\t\t\t\tidx, state, ret);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}