{
  "module_name": "tid_rdma.h",
  "hash_id": "fc6f14ccb0b016990cea7418e09aa77a2a58690173f518dee539680f16c7338a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/tid_rdma.h",
  "human_readable_source": " \n \n#ifndef HFI1_TID_RDMA_H\n#define HFI1_TID_RDMA_H\n\n#include <linux/circ_buf.h>\n#include \"common.h\"\n\n \n#define CIRC_ADD(val, add, size) (((val) + (add)) & ((size) - 1))\n#define CIRC_NEXT(val, size) CIRC_ADD(val, 1, size)\n#define CIRC_PREV(val, size) CIRC_ADD(val, -1, size)\n\n#define TID_RDMA_MIN_SEGMENT_SIZE       BIT(18)    \n#define TID_RDMA_MAX_SEGMENT_SIZE       BIT(18)    \n#define TID_RDMA_MAX_PAGES              (BIT(18) >> PAGE_SHIFT)\n#define TID_RDMA_SEGMENT_SHIFT\t\t18\n\n \n#define HFI1_S_TID_BUSY_SET       BIT(0)\n \n#define HFI1_R_TID_RSC_TIMER      BIT(2)\n \n \n#define HFI1_S_TID_WAIT_INTERLCK  BIT(5)\n#define HFI1_R_TID_WAIT_INTERLCK  BIT(6)\n \n \n#define HFI1_S_TID_RETRY_TIMER    BIT(17)\n \n#define HFI1_R_TID_SW_PSN         BIT(19)\n \n \n \n\n \n#define HFI1_TID_RDMA_WRITE_CNT 8\n\nstruct tid_rdma_params {\n\tstruct rcu_head rcu_head;\n\tu32 qp;\n\tu32 max_len;\n\tu16 jkey;\n\tu8 max_read;\n\tu8 max_write;\n\tu8 timeout;\n\tu8 urg;\n\tu8 version;\n};\n\nstruct tid_rdma_qp_params {\n\tstruct work_struct trigger_work;\n\tstruct tid_rdma_params local;\n\tstruct tid_rdma_params __rcu *remote;\n};\n\n \nstruct tid_flow_state {\n\tu32 generation;\n\tu32 psn;\n\tu8 index;\n\tu8 last_index;\n};\n\nenum tid_rdma_req_state {\n\tTID_REQUEST_INACTIVE = 0,\n\tTID_REQUEST_INIT,\n\tTID_REQUEST_INIT_RESEND,\n\tTID_REQUEST_ACTIVE,\n\tTID_REQUEST_RESEND,\n\tTID_REQUEST_RESEND_ACTIVE,\n\tTID_REQUEST_QUEUED,\n\tTID_REQUEST_SYNC,\n\tTID_REQUEST_RNR_NAK,\n\tTID_REQUEST_COMPLETE,\n};\n\nstruct tid_rdma_request {\n\tstruct rvt_qp *qp;\n\tstruct hfi1_ctxtdata *rcd;\n\tunion {\n\t\tstruct rvt_swqe *swqe;\n\t\tstruct rvt_ack_entry *ack;\n\t} e;\n\n\tstruct tid_rdma_flow *flows;\t \n\tstruct rvt_sge_state ss;  \n\tu16 n_flows;\t\t \n\tu16 setup_head;\t\t \n\tu16 clear_tail;\t\t \n\tu16 flow_idx;\t\t \n\tu16 acked_tail;\n\n\tu32 seg_len;\n\tu32 total_len;\n\tu32 r_ack_psn;           \n\tu32 r_flow_psn;          \n\tu32 r_last_acked;        \n\tu32 s_next_psn;\t\t \n\n\tu32 total_segs;\t\t \n\tu32 cur_seg;\t\t \n\tu32 comp_seg;            \n\tu32 ack_seg;             \n\tu32 alloc_seg;           \n\tu32 isge;\t\t \n\tu32 ack_pending;         \n\n\tenum tid_rdma_req_state state;\n};\n\n \nstruct flow_state {\n\tu32 flags;\n\tu32 resp_ib_psn;      \n\tu32 generation;       \n\tu32 spsn;             \n\tu32 lpsn;             \n\tu32 r_next_psn;       \n\n\t \n\tu32 ib_spsn;          \n\tu32 ib_lpsn;          \n};\n\nstruct tid_rdma_pageset {\n\tdma_addr_t addr : 48;  \n\tu8 idx: 8;\n\tu8 count : 7;\n\tu8 mapped: 1;\n};\n\n \nstruct kern_tid_node {\n\tstruct tid_group *grp;\n\tu8 map;\n\tu8 cnt;\n};\n\n \nstruct tid_rdma_flow {\n\t \n\tstruct flow_state flow_state;\n\tstruct tid_rdma_request *req;\n\tu32 tid_qpn;\n\tu32 tid_offset;\n\tu32 length;\n\tu32 sent;\n\tu8 tnode_cnt;\n\tu8 tidcnt;\n\tu8 tid_idx;\n\tu8 idx;\n\tu8 npagesets;\n\tu8 npkts;\n\tu8 pkt;\n\tu8 resync_npkts;\n\tstruct kern_tid_node tnode[TID_RDMA_MAX_PAGES];\n\tstruct tid_rdma_pageset pagesets[TID_RDMA_MAX_PAGES];\n\tu32 tid_entry[TID_RDMA_MAX_PAGES];\n};\n\nenum tid_rnr_nak_state {\n\tTID_RNR_NAK_INIT = 0,\n\tTID_RNR_NAK_SEND,\n\tTID_RNR_NAK_SENT,\n};\n\nbool tid_rdma_conn_req(struct rvt_qp *qp, u64 *data);\nbool tid_rdma_conn_reply(struct rvt_qp *qp, u64 data);\nbool tid_rdma_conn_resp(struct rvt_qp *qp, u64 *data);\nvoid tid_rdma_conn_error(struct rvt_qp *qp);\nvoid tid_rdma_opfn_init(struct rvt_qp *qp, struct tid_rdma_params *p);\n\nint hfi1_kern_exp_rcv_init(struct hfi1_ctxtdata *rcd, int reinit);\nint hfi1_kern_exp_rcv_setup(struct tid_rdma_request *req,\n\t\t\t    struct rvt_sge_state *ss, bool *last);\nint hfi1_kern_exp_rcv_clear(struct tid_rdma_request *req);\nvoid hfi1_kern_exp_rcv_clear_all(struct tid_rdma_request *req);\nvoid __trdma_clean_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe);\n\n \nstatic inline void trdma_clean_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tif (!wqe->priv)\n\t\treturn;\n\t__trdma_clean_swqe(qp, wqe);\n}\n\nvoid hfi1_kern_read_tid_flow_free(struct rvt_qp *qp);\n\nint hfi1_qp_priv_init(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t      struct ib_qp_init_attr *init_attr);\nvoid hfi1_qp_priv_tid_free(struct rvt_dev_info *rdi, struct rvt_qp *qp);\n\nvoid hfi1_tid_rdma_flush_wait(struct rvt_qp *qp);\n\nint hfi1_kern_setup_hw_flow(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp);\nvoid hfi1_kern_clear_hw_flow(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp);\nvoid hfi1_kern_init_ctxt_generations(struct hfi1_ctxtdata *rcd);\n\nstruct cntr_entry;\nu64 hfi1_access_sw_tid_wait(const struct cntr_entry *entry,\n\t\t\t    void *context, int vl, int mode, u64 data);\n\nu32 hfi1_build_tid_rdma_read_packet(struct rvt_swqe *wqe,\n\t\t\t\t    struct ib_other_headers *ohdr,\n\t\t\t\t    u32 *bth1, u32 *bth2, u32 *len);\nu32 hfi1_build_tid_rdma_read_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t\t struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t\t u32 *bth2, u32 *len);\nvoid hfi1_rc_rcv_tid_rdma_read_req(struct hfi1_packet *packet);\nu32 hfi1_build_tid_rdma_read_resp(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t  struct ib_other_headers *ohdr, u32 *bth0,\n\t\t\t\t  u32 *bth1, u32 *bth2, u32 *len, bool *last);\nvoid hfi1_rc_rcv_tid_rdma_read_resp(struct hfi1_packet *packet);\nbool hfi1_handle_kdeth_eflags(struct hfi1_ctxtdata *rcd,\n\t\t\t      struct hfi1_pportdata *ppd,\n\t\t\t      struct hfi1_packet *packet);\nvoid hfi1_tid_rdma_restart_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t       u32 *bth2);\nvoid hfi1_qp_kern_exp_rcv_clear_all(struct rvt_qp *qp);\nbool hfi1_tid_rdma_wqe_interlock(struct rvt_qp *qp, struct rvt_swqe *wqe);\n\nvoid setup_tid_rdma_wqe(struct rvt_qp *qp, struct rvt_swqe *wqe);\nstatic inline void hfi1_setup_tid_rdma_wqe(struct rvt_qp *qp,\n\t\t\t\t\t   struct rvt_swqe *wqe)\n{\n\tif (wqe->priv &&\n\t    (wqe->wr.opcode == IB_WR_RDMA_READ ||\n\t     wqe->wr.opcode == IB_WR_RDMA_WRITE) &&\n\t    wqe->length >= TID_RDMA_MIN_SEGMENT_SIZE)\n\t\tsetup_tid_rdma_wqe(qp, wqe);\n}\n\nu32 hfi1_build_tid_rdma_write_req(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t\t  struct ib_other_headers *ohdr,\n\t\t\t\t  u32 *bth1, u32 *bth2, u32 *len);\n\nvoid hfi1_rc_rcv_tid_rdma_write_req(struct hfi1_packet *packet);\n\nu32 hfi1_build_tid_rdma_write_resp(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t   struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t\t   u32 bth2, u32 *len,\n\t\t\t\t   struct rvt_sge_state **ss);\n\nvoid hfi1_del_tid_reap_timer(struct rvt_qp *qp);\n\nvoid hfi1_rc_rcv_tid_rdma_write_resp(struct hfi1_packet *packet);\n\nbool hfi1_build_tid_rdma_packet(struct rvt_swqe *wqe,\n\t\t\t\tstruct ib_other_headers *ohdr,\n\t\t\t\tu32 *bth1, u32 *bth2, u32 *len);\n\nvoid hfi1_rc_rcv_tid_rdma_write_data(struct hfi1_packet *packet);\n\nu32 hfi1_build_tid_rdma_write_ack(struct rvt_qp *qp, struct rvt_ack_entry *e,\n\t\t\t\t  struct ib_other_headers *ohdr, u16 iflow,\n\t\t\t\t  u32 *bth1, u32 *bth2);\n\nvoid hfi1_rc_rcv_tid_rdma_ack(struct hfi1_packet *packet);\n\nvoid hfi1_add_tid_retry_timer(struct rvt_qp *qp);\nvoid hfi1_del_tid_retry_timer(struct rvt_qp *qp);\n\nu32 hfi1_build_tid_rdma_resync(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t\t       struct ib_other_headers *ohdr, u32 *bth1,\n\t\t\t       u32 *bth2, u16 fidx);\n\nvoid hfi1_rc_rcv_tid_rdma_resync(struct hfi1_packet *packet);\n\nstruct hfi1_pkt_state;\nint hfi1_make_tid_rdma_pkt(struct rvt_qp *qp, struct hfi1_pkt_state *ps);\n\nvoid _hfi1_do_tid_send(struct work_struct *work);\n\nbool hfi1_schedule_tid_send(struct rvt_qp *qp);\n\nbool hfi1_tid_rdma_ack_interlock(struct rvt_qp *qp, struct rvt_ack_entry *e);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}