{
  "module_name": "sdma.h",
  "hash_id": "4b465b20c2b2f61e7f770778f93df5b0427816c1c697b1752bb1fe8669b763a1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/sdma.h",
  "human_readable_source": " \n \n\n#ifndef _HFI1_SDMA_H\n#define _HFI1_SDMA_H\n\n#include <linux/types.h>\n#include <linux/list.h>\n#include <asm/byteorder.h>\n#include <linux/workqueue.h>\n#include <linux/rculist.h>\n\n#include \"hfi.h\"\n#include \"verbs.h\"\n#include \"sdma_txreq.h\"\n\n \n#define MAX_DESC 64\n \n#define MAX_SDMA_PKT_SIZE ((16 * 1024) - 1)\n\n#define SDMA_MAP_NONE          0\n#define SDMA_MAP_SINGLE        1\n#define SDMA_MAP_PAGE          2\n\n#define SDMA_AHG_VALUE_MASK          0xffff\n#define SDMA_AHG_VALUE_SHIFT         0\n#define SDMA_AHG_INDEX_MASK          0xf\n#define SDMA_AHG_INDEX_SHIFT         16\n#define SDMA_AHG_FIELD_LEN_MASK      0xf\n#define SDMA_AHG_FIELD_LEN_SHIFT     20\n#define SDMA_AHG_FIELD_START_MASK    0x1f\n#define SDMA_AHG_FIELD_START_SHIFT   24\n#define SDMA_AHG_UPDATE_ENABLE_MASK  0x1\n#define SDMA_AHG_UPDATE_ENABLE_SHIFT 31\n\n \n\n \n#define SDMA_AHG_NO_AHG              0\n#define SDMA_AHG_COPY                1\n#define SDMA_AHG_APPLY_UPDATE1       2\n#define SDMA_AHG_APPLY_UPDATE2       3\n#define SDMA_AHG_APPLY_UPDATE3       4\n\n \n#define SDMA_DESC0_FIRST_DESC_FLAG      BIT_ULL(63)\n#define SDMA_DESC0_LAST_DESC_FLAG       BIT_ULL(62)\n#define SDMA_DESC0_BYTE_COUNT_SHIFT     48\n#define SDMA_DESC0_BYTE_COUNT_WIDTH     14\n#define SDMA_DESC0_BYTE_COUNT_MASK \\\n\t((1ULL << SDMA_DESC0_BYTE_COUNT_WIDTH) - 1)\n#define SDMA_DESC0_BYTE_COUNT_SMASK \\\n\t(SDMA_DESC0_BYTE_COUNT_MASK << SDMA_DESC0_BYTE_COUNT_SHIFT)\n#define SDMA_DESC0_PHY_ADDR_SHIFT       0\n#define SDMA_DESC0_PHY_ADDR_WIDTH       48\n#define SDMA_DESC0_PHY_ADDR_MASK \\\n\t((1ULL << SDMA_DESC0_PHY_ADDR_WIDTH) - 1)\n#define SDMA_DESC0_PHY_ADDR_SMASK \\\n\t(SDMA_DESC0_PHY_ADDR_MASK << SDMA_DESC0_PHY_ADDR_SHIFT)\n\n#define SDMA_DESC1_HEADER_UPDATE1_SHIFT 32\n#define SDMA_DESC1_HEADER_UPDATE1_WIDTH 32\n#define SDMA_DESC1_HEADER_UPDATE1_MASK \\\n\t((1ULL << SDMA_DESC1_HEADER_UPDATE1_WIDTH) - 1)\n#define SDMA_DESC1_HEADER_UPDATE1_SMASK \\\n\t(SDMA_DESC1_HEADER_UPDATE1_MASK << SDMA_DESC1_HEADER_UPDATE1_SHIFT)\n#define SDMA_DESC1_HEADER_MODE_SHIFT    13\n#define SDMA_DESC1_HEADER_MODE_WIDTH    3\n#define SDMA_DESC1_HEADER_MODE_MASK \\\n\t((1ULL << SDMA_DESC1_HEADER_MODE_WIDTH) - 1)\n#define SDMA_DESC1_HEADER_MODE_SMASK \\\n\t(SDMA_DESC1_HEADER_MODE_MASK << SDMA_DESC1_HEADER_MODE_SHIFT)\n#define SDMA_DESC1_HEADER_INDEX_SHIFT   8\n#define SDMA_DESC1_HEADER_INDEX_WIDTH   5\n#define SDMA_DESC1_HEADER_INDEX_MASK \\\n\t((1ULL << SDMA_DESC1_HEADER_INDEX_WIDTH) - 1)\n#define SDMA_DESC1_HEADER_INDEX_SMASK \\\n\t(SDMA_DESC1_HEADER_INDEX_MASK << SDMA_DESC1_HEADER_INDEX_SHIFT)\n#define SDMA_DESC1_HEADER_DWS_SHIFT     4\n#define SDMA_DESC1_HEADER_DWS_WIDTH     4\n#define SDMA_DESC1_HEADER_DWS_MASK \\\n\t((1ULL << SDMA_DESC1_HEADER_DWS_WIDTH) - 1)\n#define SDMA_DESC1_HEADER_DWS_SMASK \\\n\t(SDMA_DESC1_HEADER_DWS_MASK << SDMA_DESC1_HEADER_DWS_SHIFT)\n#define SDMA_DESC1_GENERATION_SHIFT     2\n#define SDMA_DESC1_GENERATION_WIDTH     2\n#define SDMA_DESC1_GENERATION_MASK \\\n\t((1ULL << SDMA_DESC1_GENERATION_WIDTH) - 1)\n#define SDMA_DESC1_GENERATION_SMASK \\\n\t(SDMA_DESC1_GENERATION_MASK << SDMA_DESC1_GENERATION_SHIFT)\n#define SDMA_DESC1_INT_REQ_FLAG         BIT_ULL(1)\n#define SDMA_DESC1_HEAD_TO_HOST_FLAG    BIT_ULL(0)\n\nenum sdma_states {\n\tsdma_state_s00_hw_down,\n\tsdma_state_s10_hw_start_up_halt_wait,\n\tsdma_state_s15_hw_start_up_clean_wait,\n\tsdma_state_s20_idle,\n\tsdma_state_s30_sw_clean_up_wait,\n\tsdma_state_s40_hw_clean_up_wait,\n\tsdma_state_s50_hw_halt_wait,\n\tsdma_state_s60_idle_halt_wait,\n\tsdma_state_s80_hw_freeze,\n\tsdma_state_s82_freeze_sw_clean,\n\tsdma_state_s99_running,\n};\n\nenum sdma_events {\n\tsdma_event_e00_go_hw_down,\n\tsdma_event_e10_go_hw_start,\n\tsdma_event_e15_hw_halt_done,\n\tsdma_event_e25_hw_clean_up_done,\n\tsdma_event_e30_go_running,\n\tsdma_event_e40_sw_cleaned,\n\tsdma_event_e50_hw_cleaned,\n\tsdma_event_e60_hw_halted,\n\tsdma_event_e70_go_idle,\n\tsdma_event_e80_hw_freeze,\n\tsdma_event_e81_hw_frozen,\n\tsdma_event_e82_hw_unfreeze,\n\tsdma_event_e85_link_down,\n\tsdma_event_e90_sw_halted,\n};\n\nstruct sdma_set_state_action {\n\tunsigned op_enable:1;\n\tunsigned op_intenable:1;\n\tunsigned op_halt:1;\n\tunsigned op_cleanup:1;\n\tunsigned go_s99_running_tofalse:1;\n\tunsigned go_s99_running_totrue:1;\n};\n\nstruct sdma_state {\n\tstruct kref          kref;\n\tstruct completion    comp;\n\tenum sdma_states current_state;\n\tunsigned             current_op;\n\tunsigned             go_s99_running;\n\t \n\tenum sdma_states previous_state;\n\tunsigned             previous_op;\n\tenum sdma_events last_event;\n};\n\n \n\n \n\n \n\n \nstruct hw_sdma_desc {\n\t \n\t__le64 qw[2];\n};\n\n \nstruct sdma_engine {\n\t \n\tstruct hfi1_devdata *dd;\n\tstruct hfi1_pportdata *ppd;\n\t \n\tvoid __iomem *tail_csr;\n\tu64 imask;\t\t\t \n\tu64 idle_mask;\n\tu64 progress_mask;\n\tu64 int_mask;\n\t \n\tvolatile __le64      *head_dma;  \n\t \n\tdma_addr_t            head_phys;\n\t \n\tstruct hw_sdma_desc *descq;\n\t \n\tunsigned descq_full_count;\n\tstruct sdma_txreq **tx_ring;\n\t \n\tdma_addr_t            descq_phys;\n\t \n\tu32 sdma_mask;\n\t \n\tstruct sdma_state state;\n\t \n\tint cpu;\n\t \n\tu8 sdma_shift;\n\t \n\tu8 this_idx;  \n\t \n\tspinlock_t senddmactrl_lock;\n\t \n\tu64 p_senddmactrl;\t\t \n\n\t \n\tspinlock_t            tail_lock ____cacheline_aligned_in_smp;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\t \n\tu64                   tail_sn;\n#endif\n\t \n\tu32                   descq_tail;\n\t \n\tunsigned long         ahg_bits;\n\t \n\tu16                   desc_avail;\n\t \n\tu16                   tx_tail;\n\t \n\tu16 descq_cnt;\n\n\t \n\t \n\tseqlock_t            head_lock ____cacheline_aligned_in_smp;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\t \n\tu64                   head_sn;\n#endif\n\t \n\tu32                   descq_head;\n\t \n\tu16                   tx_head;\n\t \n\tu64                   last_status;\n\t \n\tu64                     err_cnt;\n\t \n\tu64                     sdma_int_cnt;\n\tu64                     idle_int_cnt;\n\tu64                     progress_int_cnt;\n\n\t \n\tseqlock_t            waitlock;\n\tstruct list_head      dmawait;\n\n\t \n\t \n\tstruct tasklet_struct sdma_hw_clean_up_task\n\t\t____cacheline_aligned_in_smp;\n\n\t \n\tstruct tasklet_struct sdma_sw_clean_up_task\n\t\t____cacheline_aligned_in_smp;\n\t \n\tstruct work_struct err_halt_worker;\n\t \n\tstruct timer_list     err_progress_check_timer;\n\tu32                   progress_check_head;\n\t \n\tstruct work_struct flush_worker;\n\t \n\tspinlock_t flushlist_lock;\n\t \n\tstruct list_head flushlist;\n\tstruct cpumask cpu_mask;\n\tstruct kobject kobj;\n\tu32 msix_intr;\n};\n\nint sdma_init(struct hfi1_devdata *dd, u8 port);\nvoid sdma_start(struct hfi1_devdata *dd);\nvoid sdma_exit(struct hfi1_devdata *dd);\nvoid sdma_clean(struct hfi1_devdata *dd, size_t num_engines);\nvoid sdma_all_running(struct hfi1_devdata *dd);\nvoid sdma_all_idle(struct hfi1_devdata *dd);\nvoid sdma_freeze_notify(struct hfi1_devdata *dd, int go_idle);\nvoid sdma_freeze(struct hfi1_devdata *dd);\nvoid sdma_unfreeze(struct hfi1_devdata *dd);\nvoid sdma_wait(struct hfi1_devdata *dd);\n\n \nstatic inline int sdma_empty(struct sdma_engine *sde)\n{\n\treturn sde->descq_tail == sde->descq_head;\n}\n\nstatic inline u16 sdma_descq_freecnt(struct sdma_engine *sde)\n{\n\treturn sde->descq_cnt -\n\t\t(sde->descq_tail -\n\t\t READ_ONCE(sde->descq_head)) - 1;\n}\n\nstatic inline u16 sdma_descq_inprocess(struct sdma_engine *sde)\n{\n\treturn sde->descq_cnt - sdma_descq_freecnt(sde);\n}\n\n \nstatic inline int __sdma_running(struct sdma_engine *engine)\n{\n\treturn engine->state.current_state == sdma_state_s99_running;\n}\n\n \nstatic inline int sdma_running(struct sdma_engine *engine)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&engine->tail_lock, flags);\n\tret = __sdma_running(engine);\n\tspin_unlock_irqrestore(&engine->tail_lock, flags);\n\treturn ret;\n}\n\nvoid _sdma_txreq_ahgadd(\n\tstruct sdma_txreq *tx,\n\tu8 num_ahg,\n\tu8 ahg_entry,\n\tu32 *ahg,\n\tu8 ahg_hlen);\n\n \nstatic inline int sdma_txinit_ahg(\n\tstruct sdma_txreq *tx,\n\tu16 flags,\n\tu16 tlen,\n\tu8 ahg_entry,\n\tu8 num_ahg,\n\tu32 *ahg,\n\tu8 ahg_hlen,\n\tvoid (*cb)(struct sdma_txreq *, int))\n{\n\tif (tlen == 0)\n\t\treturn -ENODATA;\n\tif (tlen > MAX_SDMA_PKT_SIZE)\n\t\treturn -EMSGSIZE;\n\ttx->desc_limit = ARRAY_SIZE(tx->descs);\n\ttx->descp = &tx->descs[0];\n\tINIT_LIST_HEAD(&tx->list);\n\ttx->num_desc = 0;\n\ttx->flags = flags;\n\ttx->complete = cb;\n\ttx->coalesce_buf = NULL;\n\ttx->wait = NULL;\n\ttx->packet_len = tlen;\n\ttx->tlen = tx->packet_len;\n\ttx->descs[0].qw[0] = SDMA_DESC0_FIRST_DESC_FLAG;\n\ttx->descs[0].qw[1] = 0;\n\tif (flags & SDMA_TXREQ_F_AHG_COPY)\n\t\ttx->descs[0].qw[1] |=\n\t\t\t(((u64)ahg_entry & SDMA_DESC1_HEADER_INDEX_MASK)\n\t\t\t\t<< SDMA_DESC1_HEADER_INDEX_SHIFT) |\n\t\t\t(((u64)SDMA_AHG_COPY & SDMA_DESC1_HEADER_MODE_MASK)\n\t\t\t\t<< SDMA_DESC1_HEADER_MODE_SHIFT);\n\telse if (flags & SDMA_TXREQ_F_USE_AHG && num_ahg)\n\t\t_sdma_txreq_ahgadd(tx, num_ahg, ahg_entry, ahg, ahg_hlen);\n\treturn 0;\n}\n\n \nstatic inline int sdma_txinit(\n\tstruct sdma_txreq *tx,\n\tu16 flags,\n\tu16 tlen,\n\tvoid (*cb)(struct sdma_txreq *, int))\n{\n\treturn sdma_txinit_ahg(tx, flags, tlen, 0, 0, NULL, 0, cb);\n}\n\n \nstatic inline int sdma_mapping_type(struct sdma_desc *d)\n{\n\treturn (d->qw[1] & SDMA_DESC1_GENERATION_SMASK)\n\t\t>> SDMA_DESC1_GENERATION_SHIFT;\n}\n\nstatic inline size_t sdma_mapping_len(struct sdma_desc *d)\n{\n\treturn (d->qw[0] & SDMA_DESC0_BYTE_COUNT_SMASK)\n\t\t>> SDMA_DESC0_BYTE_COUNT_SHIFT;\n}\n\nstatic inline dma_addr_t sdma_mapping_addr(struct sdma_desc *d)\n{\n\treturn (d->qw[0] & SDMA_DESC0_PHY_ADDR_SMASK)\n\t\t>> SDMA_DESC0_PHY_ADDR_SHIFT;\n}\n\nstatic inline void make_tx_sdma_desc(\n\tstruct sdma_txreq *tx,\n\tint type,\n\tdma_addr_t addr,\n\tsize_t len,\n\tvoid *pinning_ctx,\n\tvoid (*ctx_get)(void *),\n\tvoid (*ctx_put)(void *))\n{\n\tstruct sdma_desc *desc = &tx->descp[tx->num_desc];\n\n\tif (!tx->num_desc) {\n\t\t \n\t\tdesc->qw[1] |= ((u64)type & SDMA_DESC1_GENERATION_MASK)\n\t\t\t\t<< SDMA_DESC1_GENERATION_SHIFT;\n\t} else {\n\t\tdesc->qw[0] = 0;\n\t\tdesc->qw[1] = ((u64)type & SDMA_DESC1_GENERATION_MASK)\n\t\t\t\t<< SDMA_DESC1_GENERATION_SHIFT;\n\t}\n\tdesc->qw[0] |= (((u64)addr & SDMA_DESC0_PHY_ADDR_MASK)\n\t\t\t\t<< SDMA_DESC0_PHY_ADDR_SHIFT) |\n\t\t\t(((u64)len & SDMA_DESC0_BYTE_COUNT_MASK)\n\t\t\t\t<< SDMA_DESC0_BYTE_COUNT_SHIFT);\n\n\tdesc->pinning_ctx = pinning_ctx;\n\tdesc->ctx_put = ctx_put;\n\tif (pinning_ctx && ctx_get)\n\t\tctx_get(pinning_ctx);\n}\n\n \nint ext_coal_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx,\n\t\t\t   int type, void *kvaddr, struct page *page,\n\t\t\t   unsigned long offset, u16 len);\nint _pad_sdma_tx_descs(struct hfi1_devdata *, struct sdma_txreq *);\nvoid __sdma_txclean(struct hfi1_devdata *, struct sdma_txreq *);\n\nstatic inline void sdma_txclean(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tif (tx->num_desc)\n\t\t__sdma_txclean(dd, tx);\n}\n\n \nstatic inline void _sdma_close_tx(struct hfi1_devdata *dd,\n\t\t\t\t  struct sdma_txreq *tx)\n{\n\tu16 last_desc = tx->num_desc - 1;\n\n\ttx->descp[last_desc].qw[0] |= SDMA_DESC0_LAST_DESC_FLAG;\n\ttx->descp[last_desc].qw[1] |= dd->default_desc1;\n\tif (tx->flags & SDMA_TXREQ_F_URGENT)\n\t\ttx->descp[last_desc].qw[1] |= (SDMA_DESC1_HEAD_TO_HOST_FLAG |\n\t\t\t\t\t       SDMA_DESC1_INT_REQ_FLAG);\n}\n\nstatic inline int _sdma_txadd_daddr(\n\tstruct hfi1_devdata *dd,\n\tint type,\n\tstruct sdma_txreq *tx,\n\tdma_addr_t addr,\n\tu16 len,\n\tvoid *pinning_ctx,\n\tvoid (*ctx_get)(void *),\n\tvoid (*ctx_put)(void *))\n{\n\tint rval = 0;\n\n\tmake_tx_sdma_desc(\n\t\ttx,\n\t\ttype,\n\t\taddr, len,\n\t\tpinning_ctx, ctx_get, ctx_put);\n\tWARN_ON(len > tx->tlen);\n\ttx->num_desc++;\n\ttx->tlen -= len;\n\t \n\tif (!tx->tlen) {\n\t\tif (tx->packet_len & (sizeof(u32) - 1)) {\n\t\t\trval = _pad_sdma_tx_descs(dd, tx);\n\t\t\tif (rval)\n\t\t\t\treturn rval;\n\t\t} else {\n\t\t\t_sdma_close_tx(dd, tx);\n\t\t}\n\t}\n\treturn rval;\n}\n\n \nstatic inline int sdma_txadd_page(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx,\n\tstruct page *page,\n\tunsigned long offset,\n\tu16 len,\n\tvoid *pinning_ctx,\n\tvoid (*ctx_get)(void *),\n\tvoid (*ctx_put)(void *))\n{\n\tdma_addr_t addr;\n\tint rval;\n\n\tif ((unlikely(tx->num_desc == tx->desc_limit))) {\n\t\trval = ext_coal_sdma_tx_descs(dd, tx, SDMA_MAP_PAGE,\n\t\t\t\t\t      NULL, page, offset, len);\n\t\tif (rval <= 0)\n\t\t\treturn rval;\n\t}\n\n\taddr = dma_map_page(\n\t\t       &dd->pcidev->dev,\n\t\t       page,\n\t\t       offset,\n\t\t       len,\n\t\t       DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\n\t\t__sdma_txclean(dd, tx);\n\t\treturn -ENOSPC;\n\t}\n\n\treturn _sdma_txadd_daddr(dd, SDMA_MAP_PAGE, tx, addr, len,\n\t\t\t\t pinning_ctx, ctx_get, ctx_put);\n}\n\n \n\nstatic inline int sdma_txadd_daddr(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx,\n\tdma_addr_t addr,\n\tu16 len)\n{\n\tint rval;\n\n\tif ((unlikely(tx->num_desc == tx->desc_limit))) {\n\t\trval = ext_coal_sdma_tx_descs(dd, tx, SDMA_MAP_NONE,\n\t\t\t\t\t      NULL, NULL, 0, 0);\n\t\tif (rval <= 0)\n\t\t\treturn rval;\n\t}\n\n\treturn _sdma_txadd_daddr(dd, SDMA_MAP_NONE, tx, addr, len,\n\t\t\t\t NULL, NULL, NULL);\n}\n\n \nstatic inline int sdma_txadd_kvaddr(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx,\n\tvoid *kvaddr,\n\tu16 len)\n{\n\tdma_addr_t addr;\n\tint rval;\n\n\tif ((unlikely(tx->num_desc == tx->desc_limit))) {\n\t\trval = ext_coal_sdma_tx_descs(dd, tx, SDMA_MAP_SINGLE,\n\t\t\t\t\t      kvaddr, NULL, 0, len);\n\t\tif (rval <= 0)\n\t\t\treturn rval;\n\t}\n\n\taddr = dma_map_single(\n\t\t       &dd->pcidev->dev,\n\t\t       kvaddr,\n\t\t       len,\n\t\t       DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\n\t\t__sdma_txclean(dd, tx);\n\t\treturn -ENOSPC;\n\t}\n\n\treturn _sdma_txadd_daddr(dd, SDMA_MAP_SINGLE, tx, addr, len,\n\t\t\t\t NULL, NULL, NULL);\n}\n\nstruct iowait_work;\n\nint sdma_send_txreq(struct sdma_engine *sde,\n\t\t    struct iowait_work *wait,\n\t\t    struct sdma_txreq *tx,\n\t\t    bool pkts_sent);\nint sdma_send_txlist(struct sdma_engine *sde,\n\t\t     struct iowait_work *wait,\n\t\t     struct list_head *tx_list,\n\t\t     u16 *count_out);\n\nint sdma_ahg_alloc(struct sdma_engine *sde);\nvoid sdma_ahg_free(struct sdma_engine *sde, int ahg_index);\n\n \nstatic inline u32 sdma_build_ahg_descriptor(\n\tu16 data,\n\tu8 dwindex,\n\tu8 startbit,\n\tu8 bits)\n{\n\treturn (u32)(1UL << SDMA_AHG_UPDATE_ENABLE_SHIFT |\n\t\t((startbit & SDMA_AHG_FIELD_START_MASK) <<\n\t\tSDMA_AHG_FIELD_START_SHIFT) |\n\t\t((bits & SDMA_AHG_FIELD_LEN_MASK) <<\n\t\tSDMA_AHG_FIELD_LEN_SHIFT) |\n\t\t((dwindex & SDMA_AHG_INDEX_MASK) <<\n\t\tSDMA_AHG_INDEX_SHIFT) |\n\t\t((data & SDMA_AHG_VALUE_MASK) <<\n\t\tSDMA_AHG_VALUE_SHIFT));\n}\n\n \nstatic inline unsigned sdma_progress(struct sdma_engine *sde, unsigned seq,\n\t\t\t\t     struct sdma_txreq *tx)\n{\n\tif (read_seqretry(&sde->head_lock, seq)) {\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tif (tx->num_desc > sde->desc_avail)\n\t\t\treturn 0;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nvoid sdma_engine_error(struct sdma_engine *sde, u64 status);\nvoid sdma_engine_interrupt(struct sdma_engine *sde, u64 status);\n\n \n\n \nstruct sdma_map_elem {\n\tu32 mask;\n\tstruct sdma_engine *sde[];\n};\n\n \nstruct sdma_vl_map {\n\ts8 engine_to_vl[TXE_NUM_SDMA_ENGINES];\n\tstruct rcu_head list;\n\tu32 mask;\n\tu8 actual_vls;\n\tu8 vls;\n\tstruct sdma_map_elem *map[];\n};\n\nint sdma_map_init(\n\tstruct hfi1_devdata *dd,\n\tu8 port,\n\tu8 num_vls,\n\tu8 *vl_engines);\n\n \nvoid _sdma_engine_progress_schedule(struct sdma_engine *sde);\n\n \nstatic inline void sdma_engine_progress_schedule(\n\tstruct sdma_engine *sde)\n{\n\tif (!sde || sdma_descq_inprocess(sde) < (sde->descq_cnt / 8))\n\t\treturn;\n\t_sdma_engine_progress_schedule(sde);\n}\n\nstruct sdma_engine *sdma_select_engine_sc(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 sc5);\n\nstruct sdma_engine *sdma_select_engine_vl(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 vl);\n\nstruct sdma_engine *sdma_select_user_engine(struct hfi1_devdata *dd,\n\t\t\t\t\t    u32 selector, u8 vl);\nssize_t sdma_get_cpu_to_sde_map(struct sdma_engine *sde, char *buf);\nssize_t sdma_set_cpu_to_sde_map(struct sdma_engine *sde, const char *buf,\n\t\t\t\tsize_t count);\nint sdma_engine_get_vl(struct sdma_engine *sde);\nvoid sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *);\nvoid sdma_seqfile_dump_cpu_list(struct seq_file *s, struct hfi1_devdata *dd,\n\t\t\t\tunsigned long cpuid);\n\n#ifdef CONFIG_SDMA_VERBOSITY\nvoid sdma_dumpstate(struct sdma_engine *);\n#endif\nstatic inline char *slashstrip(char *s)\n{\n\tchar *r = s;\n\n\twhile (*s)\n\t\tif (*s++ == '/')\n\t\t\tr = s;\n\treturn r;\n}\n\nu16 sdma_get_descq_cnt(void);\n\nextern uint mod_num_sdma;\n\nvoid sdma_update_lmc(struct hfi1_devdata *dd, u64 mask, u32 lid);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}