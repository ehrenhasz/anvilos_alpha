{
  "module_name": "affinity.c",
  "hash_id": "abeef5efaa9aa0298cf47a6bd282e1178f9a4ae198f02be1a783d37c681e985c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/affinity.c",
  "human_readable_source": "\n \n\n#include <linux/topology.h>\n#include <linux/cpumask.h>\n#include <linux/interrupt.h>\n#include <linux/numa.h>\n\n#include \"hfi.h\"\n#include \"affinity.h\"\n#include \"sdma.h\"\n#include \"trace.h\"\n\nstruct hfi1_affinity_node_list node_affinity = {\n\t.list = LIST_HEAD_INIT(node_affinity.list),\n\t.lock = __MUTEX_INITIALIZER(node_affinity.lock)\n};\n\n \nstatic const char * const irq_type_names[] = {\n\t\"SDMA\",\n\t\"RCVCTXT\",\n\t\"NETDEVCTXT\",\n\t\"GENERAL\",\n\t\"OTHER\",\n};\n\n \nstatic unsigned int *hfi1_per_node_cntr;\n\nstatic inline void init_cpu_mask_set(struct cpu_mask_set *set)\n{\n\tcpumask_clear(&set->mask);\n\tcpumask_clear(&set->used);\n\tset->gen = 0;\n}\n\n \nstatic void _cpu_mask_set_gen_inc(struct cpu_mask_set *set)\n{\n\tif (cpumask_equal(&set->mask, &set->used)) {\n\t\t \n\t\tset->gen++;\n\t\tcpumask_clear(&set->used);\n\t}\n}\n\nstatic void _cpu_mask_set_gen_dec(struct cpu_mask_set *set)\n{\n\tif (cpumask_empty(&set->used) && set->gen) {\n\t\tset->gen--;\n\t\tcpumask_copy(&set->used, &set->mask);\n\t}\n}\n\n \nstatic int cpu_mask_set_get_first(struct cpu_mask_set *set, cpumask_var_t diff)\n{\n\tint cpu;\n\n\tif (!diff || !set)\n\t\treturn -EINVAL;\n\n\t_cpu_mask_set_gen_inc(set);\n\n\t \n\tcpumask_andnot(diff, &set->mask, &set->used);\n\n\tcpu = cpumask_first(diff);\n\tif (cpu >= nr_cpu_ids)  \n\t\tcpu = -EINVAL;\n\telse\n\t\tcpumask_set_cpu(cpu, &set->used);\n\n\treturn cpu;\n}\n\nstatic void cpu_mask_set_put(struct cpu_mask_set *set, int cpu)\n{\n\tif (!set)\n\t\treturn;\n\n\tcpumask_clear_cpu(cpu, &set->used);\n\t_cpu_mask_set_gen_dec(set);\n}\n\n \nvoid init_real_cpu_mask(void)\n{\n\tint possible, curr_cpu, i, ht;\n\n\tcpumask_clear(&node_affinity.real_cpu_mask);\n\n\t \n\tcpumask_copy(&node_affinity.real_cpu_mask, cpu_online_mask);\n\n\t \n\tpossible = cpumask_weight(&node_affinity.real_cpu_mask);\n\tht = cpumask_weight(topology_sibling_cpumask(\n\t\t\t\tcpumask_first(&node_affinity.real_cpu_mask)));\n\t \n\tcurr_cpu = cpumask_first(&node_affinity.real_cpu_mask);\n\tfor (i = 0; i < possible / ht; i++)\n\t\tcurr_cpu = cpumask_next(curr_cpu, &node_affinity.real_cpu_mask);\n\t \n\tfor (; i < possible; i++) {\n\t\tcpumask_clear_cpu(curr_cpu, &node_affinity.real_cpu_mask);\n\t\tcurr_cpu = cpumask_next(curr_cpu, &node_affinity.real_cpu_mask);\n\t}\n}\n\nint node_affinity_init(void)\n{\n\tint node;\n\tstruct pci_dev *dev = NULL;\n\tconst struct pci_device_id *ids = hfi1_pci_tbl;\n\n\tcpumask_clear(&node_affinity.proc.used);\n\tcpumask_copy(&node_affinity.proc.mask, cpu_online_mask);\n\n\tnode_affinity.proc.gen = 0;\n\tnode_affinity.num_core_siblings =\n\t\t\t\tcpumask_weight(topology_sibling_cpumask(\n\t\t\t\t\tcpumask_first(&node_affinity.proc.mask)\n\t\t\t\t\t));\n\tnode_affinity.num_possible_nodes = num_possible_nodes();\n\tnode_affinity.num_online_nodes = num_online_nodes();\n\tnode_affinity.num_online_cpus = num_online_cpus();\n\n\t \n\tinit_real_cpu_mask();\n\n\thfi1_per_node_cntr = kcalloc(node_affinity.num_possible_nodes,\n\t\t\t\t     sizeof(*hfi1_per_node_cntr), GFP_KERNEL);\n\tif (!hfi1_per_node_cntr)\n\t\treturn -ENOMEM;\n\n\twhile (ids->vendor) {\n\t\tdev = NULL;\n\t\twhile ((dev = pci_get_device(ids->vendor, ids->device, dev))) {\n\t\t\tnode = pcibus_to_node(dev->bus);\n\t\t\tif (node < 0)\n\t\t\t\tgoto out;\n\n\t\t\thfi1_per_node_cntr[node]++;\n\t\t}\n\t\tids++;\n\t}\n\n\treturn 0;\n\nout:\n\t \n\tpr_err(\"HFI: Invalid PCI NUMA node. Performance may be affected\\n\");\n\tpr_err(\"HFI: System BIOS may need to be upgraded\\n\");\n\tfor (node = 0; node < node_affinity.num_possible_nodes; node++)\n\t\thfi1_per_node_cntr[node] = 1;\n\n\tpci_dev_put(dev);\n\n\treturn 0;\n}\n\nstatic void node_affinity_destroy(struct hfi1_affinity_node *entry)\n{\n\tfree_percpu(entry->comp_vect_affinity);\n\tkfree(entry);\n}\n\nvoid node_affinity_destroy_all(void)\n{\n\tstruct list_head *pos, *q;\n\tstruct hfi1_affinity_node *entry;\n\n\tmutex_lock(&node_affinity.lock);\n\tlist_for_each_safe(pos, q, &node_affinity.list) {\n\t\tentry = list_entry(pos, struct hfi1_affinity_node,\n\t\t\t\t   list);\n\t\tlist_del(pos);\n\t\tnode_affinity_destroy(entry);\n\t}\n\tmutex_unlock(&node_affinity.lock);\n\tkfree(hfi1_per_node_cntr);\n}\n\nstatic struct hfi1_affinity_node *node_affinity_allocate(int node)\n{\n\tstruct hfi1_affinity_node *entry;\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn NULL;\n\tentry->node = node;\n\tentry->comp_vect_affinity = alloc_percpu(u16);\n\tINIT_LIST_HEAD(&entry->list);\n\n\treturn entry;\n}\n\n \nstatic void node_affinity_add_tail(struct hfi1_affinity_node *entry)\n{\n\tlist_add_tail(&entry->list, &node_affinity.list);\n}\n\n \nstatic struct hfi1_affinity_node *node_affinity_lookup(int node)\n{\n\tstruct hfi1_affinity_node *entry;\n\n\tlist_for_each_entry(entry, &node_affinity.list, list) {\n\t\tif (entry->node == node)\n\t\t\treturn entry;\n\t}\n\n\treturn NULL;\n}\n\nstatic int per_cpu_affinity_get(cpumask_var_t possible_cpumask,\n\t\t\t\tu16 __percpu *comp_vect_affinity)\n{\n\tint curr_cpu;\n\tu16 cntr;\n\tu16 prev_cntr;\n\tint ret_cpu;\n\n\tif (!possible_cpumask) {\n\t\tret_cpu = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\tif (!comp_vect_affinity) {\n\t\tret_cpu = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\tret_cpu = cpumask_first(possible_cpumask);\n\tif (ret_cpu >= nr_cpu_ids) {\n\t\tret_cpu = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\tprev_cntr = *per_cpu_ptr(comp_vect_affinity, ret_cpu);\n\tfor_each_cpu(curr_cpu, possible_cpumask) {\n\t\tcntr = *per_cpu_ptr(comp_vect_affinity, curr_cpu);\n\n\t\tif (cntr < prev_cntr) {\n\t\t\tret_cpu = curr_cpu;\n\t\t\tprev_cntr = cntr;\n\t\t}\n\t}\n\n\t*per_cpu_ptr(comp_vect_affinity, ret_cpu) += 1;\n\nfail:\n\treturn ret_cpu;\n}\n\nstatic int per_cpu_affinity_put_max(cpumask_var_t possible_cpumask,\n\t\t\t\t    u16 __percpu *comp_vect_affinity)\n{\n\tint curr_cpu;\n\tint max_cpu;\n\tu16 cntr;\n\tu16 prev_cntr;\n\n\tif (!possible_cpumask)\n\t\treturn -EINVAL;\n\n\tif (!comp_vect_affinity)\n\t\treturn -EINVAL;\n\n\tmax_cpu = cpumask_first(possible_cpumask);\n\tif (max_cpu >= nr_cpu_ids)\n\t\treturn -EINVAL;\n\n\tprev_cntr = *per_cpu_ptr(comp_vect_affinity, max_cpu);\n\tfor_each_cpu(curr_cpu, possible_cpumask) {\n\t\tcntr = *per_cpu_ptr(comp_vect_affinity, curr_cpu);\n\n\t\tif (cntr > prev_cntr) {\n\t\t\tmax_cpu = curr_cpu;\n\t\t\tprev_cntr = cntr;\n\t\t}\n\t}\n\n\t*per_cpu_ptr(comp_vect_affinity, max_cpu) -= 1;\n\n\treturn max_cpu;\n}\n\n \nstatic int _dev_comp_vect_cpu_get(struct hfi1_devdata *dd,\n\t\t\t\t  struct hfi1_affinity_node *entry,\n\t\t\t\t  cpumask_var_t non_intr_cpus,\n\t\t\t\t  cpumask_var_t available_cpus)\n\t__must_hold(&node_affinity.lock)\n{\n\tint cpu;\n\tstruct cpu_mask_set *set = dd->comp_vect;\n\n\tlockdep_assert_held(&node_affinity.lock);\n\tif (!non_intr_cpus) {\n\t\tcpu = -1;\n\t\tgoto fail;\n\t}\n\n\tif (!available_cpus) {\n\t\tcpu = -1;\n\t\tgoto fail;\n\t}\n\n\t \n\t_cpu_mask_set_gen_inc(set);\n\tcpumask_andnot(available_cpus, &set->mask, &set->used);\n\n\t \n\tcpumask_andnot(non_intr_cpus, available_cpus,\n\t\t       &entry->def_intr.used);\n\n\t \n\tif (!cpumask_empty(non_intr_cpus))\n\t\tcpu = cpumask_first(non_intr_cpus);\n\telse  \n\t\tcpu = cpumask_first(available_cpus);\n\n\tif (cpu >= nr_cpu_ids) {  \n\t\tcpu = -1;\n\t\tgoto fail;\n\t}\n\tcpumask_set_cpu(cpu, &set->used);\n\nfail:\n\treturn cpu;\n}\n\nstatic void _dev_comp_vect_cpu_put(struct hfi1_devdata *dd, int cpu)\n{\n\tstruct cpu_mask_set *set = dd->comp_vect;\n\n\tif (cpu < 0)\n\t\treturn;\n\n\tcpu_mask_set_put(set, cpu);\n}\n\n \nstatic void _dev_comp_vect_mappings_destroy(struct hfi1_devdata *dd)\n{\n\tint i, cpu;\n\n\tif (!dd->comp_vect_mappings)\n\t\treturn;\n\n\tfor (i = 0; i < dd->comp_vect_possible_cpus; i++) {\n\t\tcpu = dd->comp_vect_mappings[i];\n\t\t_dev_comp_vect_cpu_put(dd, cpu);\n\t\tdd->comp_vect_mappings[i] = -1;\n\t\thfi1_cdbg(AFFINITY,\n\t\t\t  \"[%s] Release CPU %d from completion vector %d\",\n\t\t\t  rvt_get_ibdev_name(&(dd)->verbs_dev.rdi), cpu, i);\n\t}\n\n\tkfree(dd->comp_vect_mappings);\n\tdd->comp_vect_mappings = NULL;\n}\n\n \nstatic int _dev_comp_vect_mappings_create(struct hfi1_devdata *dd,\n\t\t\t\t\t  struct hfi1_affinity_node *entry)\n\t__must_hold(&node_affinity.lock)\n{\n\tint i, cpu, ret;\n\tcpumask_var_t non_intr_cpus;\n\tcpumask_var_t available_cpus;\n\n\tlockdep_assert_held(&node_affinity.lock);\n\n\tif (!zalloc_cpumask_var(&non_intr_cpus, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tif (!zalloc_cpumask_var(&available_cpus, GFP_KERNEL)) {\n\t\tfree_cpumask_var(non_intr_cpus);\n\t\treturn -ENOMEM;\n\t}\n\n\tdd->comp_vect_mappings = kcalloc(dd->comp_vect_possible_cpus,\n\t\t\t\t\t sizeof(*dd->comp_vect_mappings),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!dd->comp_vect_mappings) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tfor (i = 0; i < dd->comp_vect_possible_cpus; i++)\n\t\tdd->comp_vect_mappings[i] = -1;\n\n\tfor (i = 0; i < dd->comp_vect_possible_cpus; i++) {\n\t\tcpu = _dev_comp_vect_cpu_get(dd, entry, non_intr_cpus,\n\t\t\t\t\t     available_cpus);\n\t\tif (cpu < 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tdd->comp_vect_mappings[i] = cpu;\n\t\thfi1_cdbg(AFFINITY,\n\t\t\t  \"[%s] Completion Vector %d -> CPU %d\",\n\t\t\t  rvt_get_ibdev_name(&(dd)->verbs_dev.rdi), i, cpu);\n\t}\n\n\tfree_cpumask_var(available_cpus);\n\tfree_cpumask_var(non_intr_cpus);\n\treturn 0;\n\nfail:\n\tfree_cpumask_var(available_cpus);\n\tfree_cpumask_var(non_intr_cpus);\n\t_dev_comp_vect_mappings_destroy(dd);\n\n\treturn ret;\n}\n\nint hfi1_comp_vectors_set_up(struct hfi1_devdata *dd)\n{\n\tint ret;\n\tstruct hfi1_affinity_node *entry;\n\n\tmutex_lock(&node_affinity.lock);\n\tentry = node_affinity_lookup(dd->node);\n\tif (!entry) {\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\tret = _dev_comp_vect_mappings_create(dd, entry);\nunlock:\n\tmutex_unlock(&node_affinity.lock);\n\n\treturn ret;\n}\n\nvoid hfi1_comp_vectors_clean_up(struct hfi1_devdata *dd)\n{\n\t_dev_comp_vect_mappings_destroy(dd);\n}\n\nint hfi1_comp_vect_mappings_lookup(struct rvt_dev_info *rdi, int comp_vect)\n{\n\tstruct hfi1_ibdev *verbs_dev = dev_from_rdi(rdi);\n\tstruct hfi1_devdata *dd = dd_from_dev(verbs_dev);\n\n\tif (!dd->comp_vect_mappings)\n\t\treturn -EINVAL;\n\tif (comp_vect >= dd->comp_vect_possible_cpus)\n\t\treturn -EINVAL;\n\n\treturn dd->comp_vect_mappings[comp_vect];\n}\n\n \nstatic int _dev_comp_vect_cpu_mask_init(struct hfi1_devdata *dd,\n\t\t\t\t\tstruct hfi1_affinity_node *entry,\n\t\t\t\t\tbool first_dev_init)\n\t__must_hold(&node_affinity.lock)\n{\n\tint i, j, curr_cpu;\n\tint possible_cpus_comp_vect = 0;\n\tstruct cpumask *dev_comp_vect_mask = &dd->comp_vect->mask;\n\n\tlockdep_assert_held(&node_affinity.lock);\n\t \n\tif (cpumask_weight(&entry->comp_vect_mask) == 1) {\n\t\tpossible_cpus_comp_vect = 1;\n\t\tdd_dev_warn(dd,\n\t\t\t    \"Number of kernel receive queues is too large for completion vector affinity to be effective\\n\");\n\t} else {\n\t\tpossible_cpus_comp_vect +=\n\t\t\tcpumask_weight(&entry->comp_vect_mask) /\n\t\t\t\t       hfi1_per_node_cntr[dd->node];\n\n\t\t \n\t\tif (first_dev_init &&\n\t\t    cpumask_weight(&entry->comp_vect_mask) %\n\t\t    hfi1_per_node_cntr[dd->node] != 0)\n\t\t\tpossible_cpus_comp_vect++;\n\t}\n\n\tdd->comp_vect_possible_cpus = possible_cpus_comp_vect;\n\n\t \n\tfor (i = 0; i < dd->comp_vect_possible_cpus; i++) {\n\t\tcurr_cpu = per_cpu_affinity_get(&entry->comp_vect_mask,\n\t\t\t\t\t\tentry->comp_vect_affinity);\n\t\tif (curr_cpu < 0)\n\t\t\tgoto fail;\n\n\t\tcpumask_set_cpu(curr_cpu, dev_comp_vect_mask);\n\t}\n\n\thfi1_cdbg(AFFINITY,\n\t\t  \"[%s] Completion vector affinity CPU set(s) %*pbl\",\n\t\t  rvt_get_ibdev_name(&(dd)->verbs_dev.rdi),\n\t\t  cpumask_pr_args(dev_comp_vect_mask));\n\n\treturn 0;\n\nfail:\n\tfor (j = 0; j < i; j++)\n\t\tper_cpu_affinity_put_max(&entry->comp_vect_mask,\n\t\t\t\t\t entry->comp_vect_affinity);\n\n\treturn curr_cpu;\n}\n\n \nstatic void _dev_comp_vect_cpu_mask_clean_up(struct hfi1_devdata *dd,\n\t\t\t\t\t     struct hfi1_affinity_node *entry)\n\t__must_hold(&node_affinity.lock)\n{\n\tint i, cpu;\n\n\tlockdep_assert_held(&node_affinity.lock);\n\tif (!dd->comp_vect_possible_cpus)\n\t\treturn;\n\n\tfor (i = 0; i < dd->comp_vect_possible_cpus; i++) {\n\t\tcpu = per_cpu_affinity_put_max(&dd->comp_vect->mask,\n\t\t\t\t\t       entry->comp_vect_affinity);\n\t\t \n\t\tif (cpu >= 0)\n\t\t\tcpumask_clear_cpu(cpu, &dd->comp_vect->mask);\n\t}\n\n\tdd->comp_vect_possible_cpus = 0;\n}\n\n \nint hfi1_dev_affinity_init(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_affinity_node *entry;\n\tconst struct cpumask *local_mask;\n\tint curr_cpu, possible, i, ret;\n\tbool new_entry = false;\n\n\tlocal_mask = cpumask_of_node(dd->node);\n\tif (cpumask_first(local_mask) >= nr_cpu_ids)\n\t\tlocal_mask = topology_core_cpumask(0);\n\n\tmutex_lock(&node_affinity.lock);\n\tentry = node_affinity_lookup(dd->node);\n\n\t \n\tif (!entry) {\n\t\tentry = node_affinity_allocate(dd->node);\n\t\tif (!entry) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Unable to allocate global affinity node\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t\tnew_entry = true;\n\n\t\tinit_cpu_mask_set(&entry->def_intr);\n\t\tinit_cpu_mask_set(&entry->rcv_intr);\n\t\tcpumask_clear(&entry->comp_vect_mask);\n\t\tcpumask_clear(&entry->general_intr_mask);\n\t\t \n\t\tcpumask_and(&entry->def_intr.mask, &node_affinity.real_cpu_mask,\n\t\t\t    local_mask);\n\n\t\t \n\t\tpossible = cpumask_weight(&entry->def_intr.mask);\n\t\tcurr_cpu = cpumask_first(&entry->def_intr.mask);\n\n\t\tif (possible == 1) {\n\t\t\t \n\t\t\tcpumask_set_cpu(curr_cpu, &entry->rcv_intr.mask);\n\t\t\tcpumask_set_cpu(curr_cpu, &entry->general_intr_mask);\n\t\t} else {\n\t\t\t \n\t\t\tcpumask_clear_cpu(curr_cpu, &entry->def_intr.mask);\n\t\t\tcpumask_set_cpu(curr_cpu, &entry->general_intr_mask);\n\t\t\tcurr_cpu = cpumask_next(curr_cpu,\n\t\t\t\t\t\t&entry->def_intr.mask);\n\n\t\t\t \n\t\t\tfor (i = 0;\n\t\t\t     i < (dd->n_krcv_queues - 1) *\n\t\t\t\t  hfi1_per_node_cntr[dd->node];\n\t\t\t     i++) {\n\t\t\t\tcpumask_clear_cpu(curr_cpu,\n\t\t\t\t\t\t  &entry->def_intr.mask);\n\t\t\t\tcpumask_set_cpu(curr_cpu,\n\t\t\t\t\t\t&entry->rcv_intr.mask);\n\t\t\t\tcurr_cpu = cpumask_next(curr_cpu,\n\t\t\t\t\t\t\t&entry->def_intr.mask);\n\t\t\t\tif (curr_cpu >= nr_cpu_ids)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (cpumask_empty(&entry->def_intr.mask))\n\t\t\t\tcpumask_copy(&entry->def_intr.mask,\n\t\t\t\t\t     &entry->general_intr_mask);\n\t\t}\n\n\t\t \n\t\tcpumask_and(&entry->comp_vect_mask,\n\t\t\t    &node_affinity.real_cpu_mask, local_mask);\n\t\tcpumask_andnot(&entry->comp_vect_mask,\n\t\t\t       &entry->comp_vect_mask,\n\t\t\t       &entry->rcv_intr.mask);\n\t\tcpumask_andnot(&entry->comp_vect_mask,\n\t\t\t       &entry->comp_vect_mask,\n\t\t\t       &entry->general_intr_mask);\n\n\t\t \n\t\tif (cpumask_empty(&entry->comp_vect_mask))\n\t\t\tcpumask_copy(&entry->comp_vect_mask,\n\t\t\t\t     &entry->general_intr_mask);\n\t}\n\n\tret = _dev_comp_vect_cpu_mask_init(dd, entry, new_entry);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\tif (new_entry)\n\t\tnode_affinity_add_tail(entry);\n\n\tdd->affinity_entry = entry;\n\tmutex_unlock(&node_affinity.lock);\n\n\treturn 0;\n\nfail:\n\tif (new_entry)\n\t\tnode_affinity_destroy(entry);\n\tmutex_unlock(&node_affinity.lock);\n\treturn ret;\n}\n\nvoid hfi1_dev_affinity_clean_up(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_affinity_node *entry;\n\n\tmutex_lock(&node_affinity.lock);\n\tif (!dd->affinity_entry)\n\t\tgoto unlock;\n\tentry = node_affinity_lookup(dd->node);\n\tif (!entry)\n\t\tgoto unlock;\n\n\t \n\t_dev_comp_vect_cpu_mask_clean_up(dd, entry);\nunlock:\n\tdd->affinity_entry = NULL;\n\tmutex_unlock(&node_affinity.lock);\n}\n\n \nstatic void hfi1_update_sdma_affinity(struct hfi1_msix_entry *msix, int cpu)\n{\n\tstruct sdma_engine *sde = msix->arg;\n\tstruct hfi1_devdata *dd = sde->dd;\n\tstruct hfi1_affinity_node *entry;\n\tstruct cpu_mask_set *set;\n\tint i, old_cpu;\n\n\tif (cpu > num_online_cpus() || cpu == sde->cpu)\n\t\treturn;\n\n\tmutex_lock(&node_affinity.lock);\n\tentry = node_affinity_lookup(dd->node);\n\tif (!entry)\n\t\tgoto unlock;\n\n\told_cpu = sde->cpu;\n\tsde->cpu = cpu;\n\tcpumask_clear(&msix->mask);\n\tcpumask_set_cpu(cpu, &msix->mask);\n\tdd_dev_dbg(dd, \"IRQ: %u, type %s engine %u -> cpu: %d\\n\",\n\t\t   msix->irq, irq_type_names[msix->type],\n\t\t   sde->this_idx, cpu);\n\tirq_set_affinity_hint(msix->irq, &msix->mask);\n\n\t \n\tset = &entry->def_intr;\n\tcpumask_set_cpu(cpu, &set->mask);\n\tcpumask_set_cpu(cpu, &set->used);\n\tfor (i = 0; i < dd->msix_info.max_requested; i++) {\n\t\tstruct hfi1_msix_entry *other_msix;\n\n\t\tother_msix = &dd->msix_info.msix_entries[i];\n\t\tif (other_msix->type != IRQ_SDMA || other_msix == msix)\n\t\t\tcontinue;\n\n\t\tif (cpumask_test_cpu(old_cpu, &other_msix->mask))\n\t\t\tgoto unlock;\n\t}\n\tcpumask_clear_cpu(old_cpu, &set->mask);\n\tcpumask_clear_cpu(old_cpu, &set->used);\nunlock:\n\tmutex_unlock(&node_affinity.lock);\n}\n\nstatic void hfi1_irq_notifier_notify(struct irq_affinity_notify *notify,\n\t\t\t\t     const cpumask_t *mask)\n{\n\tint cpu = cpumask_first(mask);\n\tstruct hfi1_msix_entry *msix = container_of(notify,\n\t\t\t\t\t\t    struct hfi1_msix_entry,\n\t\t\t\t\t\t    notify);\n\n\t \n\thfi1_update_sdma_affinity(msix, cpu);\n}\n\nstatic void hfi1_irq_notifier_release(struct kref *ref)\n{\n\t \n}\n\nstatic void hfi1_setup_sdma_notifier(struct hfi1_msix_entry *msix)\n{\n\tstruct irq_affinity_notify *notify = &msix->notify;\n\n\tnotify->irq = msix->irq;\n\tnotify->notify = hfi1_irq_notifier_notify;\n\tnotify->release = hfi1_irq_notifier_release;\n\n\tif (irq_set_affinity_notifier(notify->irq, notify))\n\t\tpr_err(\"Failed to register sdma irq affinity notifier for irq %d\\n\",\n\t\t       notify->irq);\n}\n\nstatic void hfi1_cleanup_sdma_notifier(struct hfi1_msix_entry *msix)\n{\n\tstruct irq_affinity_notify *notify = &msix->notify;\n\n\tif (irq_set_affinity_notifier(notify->irq, NULL))\n\t\tpr_err(\"Failed to cleanup sdma irq affinity notifier for irq %d\\n\",\n\t\t       notify->irq);\n}\n\n \nstatic int get_irq_affinity(struct hfi1_devdata *dd,\n\t\t\t    struct hfi1_msix_entry *msix)\n{\n\tcpumask_var_t diff;\n\tstruct hfi1_affinity_node *entry;\n\tstruct cpu_mask_set *set = NULL;\n\tstruct sdma_engine *sde = NULL;\n\tstruct hfi1_ctxtdata *rcd = NULL;\n\tchar extra[64];\n\tint cpu = -1;\n\n\textra[0] = '\\0';\n\tcpumask_clear(&msix->mask);\n\n\tentry = node_affinity_lookup(dd->node);\n\n\tswitch (msix->type) {\n\tcase IRQ_SDMA:\n\t\tsde = (struct sdma_engine *)msix->arg;\n\t\tscnprintf(extra, 64, \"engine %u\", sde->this_idx);\n\t\tset = &entry->def_intr;\n\t\tbreak;\n\tcase IRQ_GENERAL:\n\t\tcpu = cpumask_first(&entry->general_intr_mask);\n\t\tbreak;\n\tcase IRQ_RCVCTXT:\n\t\trcd = (struct hfi1_ctxtdata *)msix->arg;\n\t\tif (rcd->ctxt == HFI1_CTRL_CTXT)\n\t\t\tcpu = cpumask_first(&entry->general_intr_mask);\n\t\telse\n\t\t\tset = &entry->rcv_intr;\n\t\tscnprintf(extra, 64, \"ctxt %u\", rcd->ctxt);\n\t\tbreak;\n\tcase IRQ_NETDEVCTXT:\n\t\trcd = (struct hfi1_ctxtdata *)msix->arg;\n\t\tset = &entry->def_intr;\n\t\tscnprintf(extra, 64, \"ctxt %u\", rcd->ctxt);\n\t\tbreak;\n\tdefault:\n\t\tdd_dev_err(dd, \"Invalid IRQ type %d\\n\", msix->type);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (cpu == -1 && set) {\n\t\tif (!zalloc_cpumask_var(&diff, GFP_KERNEL))\n\t\t\treturn -ENOMEM;\n\n\t\tcpu = cpu_mask_set_get_first(set, diff);\n\t\tif (cpu < 0) {\n\t\t\tfree_cpumask_var(diff);\n\t\t\tdd_dev_err(dd, \"Failure to obtain CPU for IRQ\\n\");\n\t\t\treturn cpu;\n\t\t}\n\n\t\tfree_cpumask_var(diff);\n\t}\n\n\tcpumask_set_cpu(cpu, &msix->mask);\n\tdd_dev_info(dd, \"IRQ: %u, type %s %s -> cpu: %d\\n\",\n\t\t    msix->irq, irq_type_names[msix->type],\n\t\t    extra, cpu);\n\tirq_set_affinity_hint(msix->irq, &msix->mask);\n\n\tif (msix->type == IRQ_SDMA) {\n\t\tsde->cpu = cpu;\n\t\thfi1_setup_sdma_notifier(msix);\n\t}\n\n\treturn 0;\n}\n\nint hfi1_get_irq_affinity(struct hfi1_devdata *dd, struct hfi1_msix_entry *msix)\n{\n\tint ret;\n\n\tmutex_lock(&node_affinity.lock);\n\tret = get_irq_affinity(dd, msix);\n\tmutex_unlock(&node_affinity.lock);\n\treturn ret;\n}\n\nvoid hfi1_put_irq_affinity(struct hfi1_devdata *dd,\n\t\t\t   struct hfi1_msix_entry *msix)\n{\n\tstruct cpu_mask_set *set = NULL;\n\tstruct hfi1_affinity_node *entry;\n\n\tmutex_lock(&node_affinity.lock);\n\tentry = node_affinity_lookup(dd->node);\n\n\tswitch (msix->type) {\n\tcase IRQ_SDMA:\n\t\tset = &entry->def_intr;\n\t\thfi1_cleanup_sdma_notifier(msix);\n\t\tbreak;\n\tcase IRQ_GENERAL:\n\t\t \n\t\tbreak;\n\tcase IRQ_RCVCTXT: {\n\t\tstruct hfi1_ctxtdata *rcd = msix->arg;\n\n\t\t \n\t\tif (rcd->ctxt != HFI1_CTRL_CTXT)\n\t\t\tset = &entry->rcv_intr;\n\t\tbreak;\n\t}\n\tcase IRQ_NETDEVCTXT:\n\t\tset = &entry->def_intr;\n\t\tbreak;\n\tdefault:\n\t\tmutex_unlock(&node_affinity.lock);\n\t\treturn;\n\t}\n\n\tif (set) {\n\t\tcpumask_andnot(&set->used, &set->used, &msix->mask);\n\t\t_cpu_mask_set_gen_dec(set);\n\t}\n\n\tirq_set_affinity_hint(msix->irq, NULL);\n\tcpumask_clear(&msix->mask);\n\tmutex_unlock(&node_affinity.lock);\n}\n\n \nstatic void find_hw_thread_mask(uint hw_thread_no, cpumask_var_t hw_thread_mask,\n\t\t\t\tstruct hfi1_affinity_node_list *affinity)\n{\n\tint possible, curr_cpu, i;\n\tuint num_cores_per_socket = node_affinity.num_online_cpus /\n\t\t\t\t\taffinity->num_core_siblings /\n\t\t\t\t\t\tnode_affinity.num_online_nodes;\n\n\tcpumask_copy(hw_thread_mask, &affinity->proc.mask);\n\tif (affinity->num_core_siblings > 0) {\n\t\t \n\t\tpossible = cpumask_weight(hw_thread_mask);\n\t\tcurr_cpu = cpumask_first(hw_thread_mask);\n\t\tfor (i = 0;\n\t\t     i < num_cores_per_socket * node_affinity.num_online_nodes;\n\t\t     i++)\n\t\t\tcurr_cpu = cpumask_next(curr_cpu, hw_thread_mask);\n\n\t\tfor (; i < possible; i++) {\n\t\t\tcpumask_clear_cpu(curr_cpu, hw_thread_mask);\n\t\t\tcurr_cpu = cpumask_next(curr_cpu, hw_thread_mask);\n\t\t}\n\n\t\t \n\t\tcpumask_shift_left(hw_thread_mask, hw_thread_mask,\n\t\t\t\t   num_cores_per_socket *\n\t\t\t\t   node_affinity.num_online_nodes *\n\t\t\t\t   hw_thread_no);\n\t}\n}\n\nint hfi1_get_proc_affinity(int node)\n{\n\tint cpu = -1, ret, i;\n\tstruct hfi1_affinity_node *entry;\n\tcpumask_var_t diff, hw_thread_mask, available_mask, intrs_mask;\n\tconst struct cpumask *node_mask,\n\t\t*proc_mask = current->cpus_ptr;\n\tstruct hfi1_affinity_node_list *affinity = &node_affinity;\n\tstruct cpu_mask_set *set = &affinity->proc;\n\n\t \n\tif (current->nr_cpus_allowed == 1) {\n\t\thfi1_cdbg(PROC, \"PID %u %s affinity set to CPU %*pbl\",\n\t\t\t  current->pid, current->comm,\n\t\t\t  cpumask_pr_args(proc_mask));\n\t\t \n\t\tcpu = cpumask_first(proc_mask);\n\t\tcpumask_set_cpu(cpu, &set->used);\n\t\tgoto done;\n\t} else if (current->nr_cpus_allowed < cpumask_weight(&set->mask)) {\n\t\thfi1_cdbg(PROC, \"PID %u %s affinity set to CPU set(s) %*pbl\",\n\t\t\t  current->pid, current->comm,\n\t\t\t  cpumask_pr_args(proc_mask));\n\t\tgoto done;\n\t}\n\n\t \n\n\tret = zalloc_cpumask_var(&diff, GFP_KERNEL);\n\tif (!ret)\n\t\tgoto done;\n\tret = zalloc_cpumask_var(&hw_thread_mask, GFP_KERNEL);\n\tif (!ret)\n\t\tgoto free_diff;\n\tret = zalloc_cpumask_var(&available_mask, GFP_KERNEL);\n\tif (!ret)\n\t\tgoto free_hw_thread_mask;\n\tret = zalloc_cpumask_var(&intrs_mask, GFP_KERNEL);\n\tif (!ret)\n\t\tgoto free_available_mask;\n\n\tmutex_lock(&affinity->lock);\n\t \n\t_cpu_mask_set_gen_inc(set);\n\n\t \n\tentry = node_affinity_lookup(node);\n\tif (entry) {\n\t\tcpumask_copy(intrs_mask, (entry->def_intr.gen ?\n\t\t\t\t\t  &entry->def_intr.mask :\n\t\t\t\t\t  &entry->def_intr.used));\n\t\tcpumask_or(intrs_mask, intrs_mask, (entry->rcv_intr.gen ?\n\t\t\t\t\t\t    &entry->rcv_intr.mask :\n\t\t\t\t\t\t    &entry->rcv_intr.used));\n\t\tcpumask_or(intrs_mask, intrs_mask, &entry->general_intr_mask);\n\t}\n\thfi1_cdbg(PROC, \"CPUs used by interrupts: %*pbl\",\n\t\t  cpumask_pr_args(intrs_mask));\n\n\tcpumask_copy(hw_thread_mask, &set->mask);\n\n\t \n\tif (affinity->num_core_siblings > 0) {\n\t\tfor (i = 0; i < affinity->num_core_siblings; i++) {\n\t\t\tfind_hw_thread_mask(i, hw_thread_mask, affinity);\n\n\t\t\t \n\t\t\tcpumask_andnot(diff, hw_thread_mask, &set->used);\n\t\t\tif (!cpumask_empty(diff))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\thfi1_cdbg(PROC, \"Same available HW thread on all physical CPUs: %*pbl\",\n\t\t  cpumask_pr_args(hw_thread_mask));\n\n\tnode_mask = cpumask_of_node(node);\n\thfi1_cdbg(PROC, \"Device on NUMA %u, CPUs %*pbl\", node,\n\t\t  cpumask_pr_args(node_mask));\n\n\t \n\tcpumask_and(available_mask, hw_thread_mask, node_mask);\n\tcpumask_andnot(available_mask, available_mask, &set->used);\n\thfi1_cdbg(PROC, \"Available CPUs on NUMA %u: %*pbl\", node,\n\t\t  cpumask_pr_args(available_mask));\n\n\t \n\tcpumask_andnot(diff, available_mask, intrs_mask);\n\tif (!cpumask_empty(diff))\n\t\tcpumask_copy(available_mask, diff);\n\n\t \n\tif (cpumask_empty(available_mask)) {\n\t\tcpumask_andnot(available_mask, hw_thread_mask, &set->used);\n\t\t \n\t\tcpumask_andnot(available_mask, available_mask, node_mask);\n\t\thfi1_cdbg(PROC,\n\t\t\t  \"Preferred NUMA node cores are taken, cores available in other NUMA nodes: %*pbl\",\n\t\t\t  cpumask_pr_args(available_mask));\n\n\t\t \n\t\tcpumask_andnot(diff, available_mask, intrs_mask);\n\t\tif (!cpumask_empty(diff))\n\t\t\tcpumask_copy(available_mask, diff);\n\t}\n\thfi1_cdbg(PROC, \"Possible CPUs for process: %*pbl\",\n\t\t  cpumask_pr_args(available_mask));\n\n\tcpu = cpumask_first(available_mask);\n\tif (cpu >= nr_cpu_ids)  \n\t\tcpu = -1;\n\telse\n\t\tcpumask_set_cpu(cpu, &set->used);\n\n\tmutex_unlock(&affinity->lock);\n\thfi1_cdbg(PROC, \"Process assigned to CPU %d\", cpu);\n\n\tfree_cpumask_var(intrs_mask);\nfree_available_mask:\n\tfree_cpumask_var(available_mask);\nfree_hw_thread_mask:\n\tfree_cpumask_var(hw_thread_mask);\nfree_diff:\n\tfree_cpumask_var(diff);\ndone:\n\treturn cpu;\n}\n\nvoid hfi1_put_proc_affinity(int cpu)\n{\n\tstruct hfi1_affinity_node_list *affinity = &node_affinity;\n\tstruct cpu_mask_set *set = &affinity->proc;\n\n\tif (cpu < 0)\n\t\treturn;\n\n\tmutex_lock(&affinity->lock);\n\tcpu_mask_set_put(set, cpu);\n\thfi1_cdbg(PROC, \"Returning CPU %d for future process assignment\", cpu);\n\tmutex_unlock(&affinity->lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}