{
  "module_name": "ipoib_tx.c",
  "hash_id": "e9ac6ca6d9546a4807c28de7d810097d5554541e5a350e02d569b6ea60611845",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/ipoib_tx.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/log2.h>\n#include <linux/circ_buf.h>\n\n#include \"sdma.h\"\n#include \"verbs.h\"\n#include \"trace_ibhdrs.h\"\n#include \"ipoib.h\"\n#include \"trace_tx.h\"\n\n \n#define CIRC_ADD(val, add, size) (((val) + (add)) & ((size) - 1))\n#define CIRC_NEXT(val, size) CIRC_ADD(val, 1, size)\n#define CIRC_PREV(val, size) CIRC_ADD(val, -1, size)\n\nstruct ipoib_txparms {\n\tstruct hfi1_devdata        *dd;\n\tstruct rdma_ah_attr        *ah_attr;\n\tstruct hfi1_ibport         *ibp;\n\tstruct hfi1_ipoib_txq      *txq;\n\tunion hfi1_ipoib_flow       flow;\n\tu32                         dqpn;\n\tu8                          hdr_dwords;\n\tu8                          entropy;\n};\n\nstatic struct ipoib_txreq *\nhfi1_txreq_from_idx(struct hfi1_ipoib_circ_buf *r, u32 idx)\n{\n\treturn (struct ipoib_txreq *)(r->items + (idx << r->shift));\n}\n\nstatic u32 hfi1_ipoib_txreqs(const u64 sent, const u64 completed)\n{\n\treturn sent - completed;\n}\n\nstatic u64 hfi1_ipoib_used(struct hfi1_ipoib_txq *txq)\n{\n\treturn hfi1_ipoib_txreqs(txq->tx_ring.sent_txreqs,\n\t\t\t\t txq->tx_ring.complete_txreqs);\n}\n\nstatic void hfi1_ipoib_stop_txq(struct hfi1_ipoib_txq *txq)\n{\n\ttrace_hfi1_txq_stop(txq);\n\tif (atomic_inc_return(&txq->tx_ring.stops) == 1)\n\t\tnetif_stop_subqueue(txq->priv->netdev, txq->q_idx);\n}\n\nstatic void hfi1_ipoib_wake_txq(struct hfi1_ipoib_txq *txq)\n{\n\ttrace_hfi1_txq_wake(txq);\n\tif (atomic_dec_and_test(&txq->tx_ring.stops))\n\t\tnetif_wake_subqueue(txq->priv->netdev, txq->q_idx);\n}\n\nstatic uint hfi1_ipoib_ring_hwat(struct hfi1_ipoib_txq *txq)\n{\n\treturn min_t(uint, txq->priv->netdev->tx_queue_len,\n\t\t     txq->tx_ring.max_items - 1);\n}\n\nstatic uint hfi1_ipoib_ring_lwat(struct hfi1_ipoib_txq *txq)\n{\n\treturn min_t(uint, txq->priv->netdev->tx_queue_len,\n\t\t     txq->tx_ring.max_items) >> 1;\n}\n\nstatic void hfi1_ipoib_check_queue_depth(struct hfi1_ipoib_txq *txq)\n{\n\t++txq->tx_ring.sent_txreqs;\n\tif (hfi1_ipoib_used(txq) >= hfi1_ipoib_ring_hwat(txq) &&\n\t    !atomic_xchg(&txq->tx_ring.ring_full, 1)) {\n\t\ttrace_hfi1_txq_full(txq);\n\t\thfi1_ipoib_stop_txq(txq);\n\t}\n}\n\nstatic void hfi1_ipoib_check_queue_stopped(struct hfi1_ipoib_txq *txq)\n{\n\tstruct net_device *dev = txq->priv->netdev;\n\n\t \n\tif (unlikely(dev->reg_state != NETREG_REGISTERED))\n\t\treturn;\n\n\t \n\tif (hfi1_ipoib_used(txq) < hfi1_ipoib_ring_lwat(txq) &&\n\t    atomic_xchg(&txq->tx_ring.ring_full, 0)) {\n\t\ttrace_hfi1_txq_xmit_unstopped(txq);\n\t\thfi1_ipoib_wake_txq(txq);\n\t}\n}\n\nstatic void hfi1_ipoib_free_tx(struct ipoib_txreq *tx, int budget)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = tx->txq->priv;\n\n\tif (likely(!tx->sdma_status)) {\n\t\tdev_sw_netstats_tx_add(priv->netdev, 1, tx->skb->len);\n\t} else {\n\t\t++priv->netdev->stats.tx_errors;\n\t\tdd_dev_warn(priv->dd,\n\t\t\t    \"%s: Status = 0x%x pbc 0x%llx txq = %d sde = %d\\n\",\n\t\t\t    __func__, tx->sdma_status,\n\t\t\t    le64_to_cpu(tx->sdma_hdr->pbc), tx->txq->q_idx,\n\t\t\t    tx->txq->sde->this_idx);\n\t}\n\n\tnapi_consume_skb(tx->skb, budget);\n\ttx->skb = NULL;\n\tsdma_txclean(priv->dd, &tx->txreq);\n}\n\nstatic void hfi1_ipoib_drain_tx_ring(struct hfi1_ipoib_txq *txq)\n{\n\tstruct hfi1_ipoib_circ_buf *tx_ring = &txq->tx_ring;\n\tint i;\n\tstruct ipoib_txreq *tx;\n\n\tfor (i = 0; i < tx_ring->max_items; i++) {\n\t\ttx = hfi1_txreq_from_idx(tx_ring, i);\n\t\ttx->complete = 0;\n\t\tdev_kfree_skb_any(tx->skb);\n\t\ttx->skb = NULL;\n\t\tsdma_txclean(txq->priv->dd, &tx->txreq);\n\t}\n\ttx_ring->head = 0;\n\ttx_ring->tail = 0;\n\ttx_ring->complete_txreqs = 0;\n\ttx_ring->sent_txreqs = 0;\n\ttx_ring->avail = hfi1_ipoib_ring_hwat(txq);\n}\n\nstatic int hfi1_ipoib_poll_tx_ring(struct napi_struct *napi, int budget)\n{\n\tstruct hfi1_ipoib_txq *txq =\n\t\tcontainer_of(napi, struct hfi1_ipoib_txq, napi);\n\tstruct hfi1_ipoib_circ_buf *tx_ring = &txq->tx_ring;\n\tu32 head = tx_ring->head;\n\tu32 max_tx = tx_ring->max_items;\n\tint work_done;\n\tstruct ipoib_txreq *tx =  hfi1_txreq_from_idx(tx_ring, head);\n\n\ttrace_hfi1_txq_poll(txq);\n\tfor (work_done = 0; work_done < budget; work_done++) {\n\t\t \n\t\tif (!smp_load_acquire(&tx->complete))\n\t\t\tbreak;\n\t\ttx->complete = 0;\n\t\ttrace_hfi1_tx_produce(tx, head);\n\t\thfi1_ipoib_free_tx(tx, budget);\n\t\thead = CIRC_NEXT(head, max_tx);\n\t\ttx =  hfi1_txreq_from_idx(tx_ring, head);\n\t}\n\ttx_ring->complete_txreqs += work_done;\n\n\t \n\tsmp_store_release(&tx_ring->head, head);\n\n\thfi1_ipoib_check_queue_stopped(txq);\n\n\tif (work_done < budget)\n\t\tnapi_complete_done(napi, work_done);\n\n\treturn work_done;\n}\n\nstatic void hfi1_ipoib_sdma_complete(struct sdma_txreq *txreq, int status)\n{\n\tstruct ipoib_txreq *tx = container_of(txreq, struct ipoib_txreq, txreq);\n\n\ttrace_hfi1_txq_complete(tx->txq);\n\ttx->sdma_status = status;\n\t \n\tsmp_store_release(&tx->complete, 1);\n\tnapi_schedule_irqoff(&tx->txq->napi);\n}\n\nstatic int hfi1_ipoib_build_ulp_payload(struct ipoib_txreq *tx,\n\t\t\t\t\tstruct ipoib_txparms *txp)\n{\n\tstruct hfi1_devdata *dd = txp->dd;\n\tstruct sdma_txreq *txreq = &tx->txreq;\n\tstruct sk_buff *skb = tx->skb;\n\tint ret = 0;\n\tint i;\n\n\tif (skb_headlen(skb)) {\n\t\tret = sdma_txadd_kvaddr(dd, txreq, skb->data, skb_headlen(skb));\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tret = sdma_txadd_page(dd,\n\t\t\t\t      txreq,\n\t\t\t\t      skb_frag_page(frag),\n\t\t\t\t      frag->bv_offset,\n\t\t\t\t      skb_frag_size(frag),\n\t\t\t\t      NULL, NULL, NULL);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int hfi1_ipoib_build_tx_desc(struct ipoib_txreq *tx,\n\t\t\t\t    struct ipoib_txparms *txp)\n{\n\tstruct hfi1_devdata *dd = txp->dd;\n\tstruct sdma_txreq *txreq = &tx->txreq;\n\tstruct hfi1_sdma_header *sdma_hdr = tx->sdma_hdr;\n\tu16 pkt_bytes =\n\t\tsizeof(sdma_hdr->pbc) + (txp->hdr_dwords << 2) + tx->skb->len;\n\tint ret;\n\n\tret = sdma_txinit(txreq, 0, pkt_bytes, hfi1_ipoib_sdma_complete);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t \n\tret = sdma_txadd_kvaddr(dd,\n\t\t\t\ttxreq,\n\t\t\t\tsdma_hdr,\n\t\t\t\tsizeof(sdma_hdr->pbc) + (txp->hdr_dwords << 2));\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t \n\treturn hfi1_ipoib_build_ulp_payload(tx, txp);\n}\n\nstatic void hfi1_ipoib_build_ib_tx_headers(struct ipoib_txreq *tx,\n\t\t\t\t\t   struct ipoib_txparms *txp)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = tx->txq->priv;\n\tstruct hfi1_sdma_header *sdma_hdr = tx->sdma_hdr;\n\tstruct sk_buff *skb = tx->skb;\n\tstruct hfi1_pportdata *ppd = ppd_from_ibp(txp->ibp);\n\tstruct rdma_ah_attr *ah_attr = txp->ah_attr;\n\tstruct ib_other_headers *ohdr;\n\tstruct ib_grh *grh;\n\tu16 dwords;\n\tu16 slid;\n\tu16 dlid;\n\tu16 lrh0;\n\tu32 bth0;\n\tu32 sqpn = (u32)(priv->netdev->dev_addr[1] << 16 |\n\t\t\t priv->netdev->dev_addr[2] << 8 |\n\t\t\t priv->netdev->dev_addr[3]);\n\tu16 payload_dwords;\n\tu8 pad_cnt;\n\n\tpad_cnt = -skb->len & 3;\n\n\t \n\tpayload_dwords = ((skb->len + pad_cnt) >> 2) + SIZE_OF_CRC;\n\n\t \n\ttxp->hdr_dwords = 7;\n\n\tif (rdma_ah_get_ah_flags(ah_attr) & IB_AH_GRH) {\n\t\tgrh = &sdma_hdr->hdr.ibh.u.l.grh;\n\t\ttxp->hdr_dwords +=\n\t\t\thfi1_make_grh(txp->ibp,\n\t\t\t\t      grh,\n\t\t\t\t      rdma_ah_read_grh(ah_attr),\n\t\t\t\t      txp->hdr_dwords - LRH_9B_DWORDS,\n\t\t\t\t      payload_dwords);\n\t\tlrh0 = HFI1_LRH_GRH;\n\t\tohdr = &sdma_hdr->hdr.ibh.u.l.oth;\n\t} else {\n\t\tlrh0 = HFI1_LRH_BTH;\n\t\tohdr = &sdma_hdr->hdr.ibh.u.oth;\n\t}\n\n\tlrh0 |= (rdma_ah_get_sl(ah_attr) & 0xf) << 4;\n\tlrh0 |= (txp->flow.sc5 & 0xf) << 12;\n\n\tdlid = opa_get_lid(rdma_ah_get_dlid(ah_attr), 9B);\n\tif (dlid == be16_to_cpu(IB_LID_PERMISSIVE)) {\n\t\tslid = be16_to_cpu(IB_LID_PERMISSIVE);\n\t} else {\n\t\tu16 lid = (u16)ppd->lid;\n\n\t\tif (lid) {\n\t\t\tlid |= rdma_ah_get_path_bits(ah_attr) &\n\t\t\t\t((1 << ppd->lmc) - 1);\n\t\t\tslid = lid;\n\t\t} else {\n\t\t\tslid = be16_to_cpu(IB_LID_PERMISSIVE);\n\t\t}\n\t}\n\n\t \n\tdwords = txp->hdr_dwords + payload_dwords;\n\n\t \n\tsdma_hdr->hdr.hdr_type = HFI1_PKT_TYPE_9B;\n\thfi1_make_ib_hdr(&sdma_hdr->hdr.ibh, lrh0, dwords, dlid, slid);\n\n\t \n\tbth0 = (IB_OPCODE_UD_SEND_ONLY << 24) | (pad_cnt << 20) | priv->pkey;\n\n\tohdr->bth[0] = cpu_to_be32(bth0);\n\tohdr->bth[1] = cpu_to_be32(txp->dqpn);\n\tohdr->bth[2] = cpu_to_be32(mask_psn((u32)txp->txq->tx_ring.sent_txreqs));\n\n\t \n\tohdr->u.ud.deth[0] = cpu_to_be32(priv->qkey);\n\tohdr->u.ud.deth[1] = cpu_to_be32((txp->entropy <<\n\t\t\t\t\t  HFI1_IPOIB_ENTROPY_SHIFT) | sqpn);\n\n\t \n\tsdma_hdr->pbc =\n\t\tcpu_to_le64(create_pbc(ppd,\n\t\t\t\t       ib_is_sc5(txp->flow.sc5) <<\n\t\t\t\t\t\t\t      PBC_DC_INFO_SHIFT,\n\t\t\t\t       0,\n\t\t\t\t       sc_to_vlt(priv->dd, txp->flow.sc5),\n\t\t\t\t       dwords - SIZE_OF_CRC +\n\t\t\t\t\t\t(sizeof(sdma_hdr->pbc) >> 2)));\n}\n\nstatic struct ipoib_txreq *hfi1_ipoib_send_dma_common(struct net_device *dev,\n\t\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t\t      struct ipoib_txparms *txp)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = hfi1_ipoib_priv(dev);\n\tstruct hfi1_ipoib_txq *txq = txp->txq;\n\tstruct ipoib_txreq *tx;\n\tstruct hfi1_ipoib_circ_buf *tx_ring = &txq->tx_ring;\n\tu32 tail = tx_ring->tail;\n\tint ret;\n\n\tif (unlikely(!tx_ring->avail)) {\n\t\tu32 head;\n\n\t\tif (hfi1_ipoib_used(txq) >= hfi1_ipoib_ring_hwat(txq))\n\t\t\t \n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t \n\t\thead = smp_load_acquire(&tx_ring->head);\n\t\ttx_ring->avail =\n\t\t\tmin_t(u32, hfi1_ipoib_ring_hwat(txq),\n\t\t\t      CIRC_CNT(head, tail, tx_ring->max_items));\n\t} else {\n\t\ttx_ring->avail--;\n\t}\n\ttx = hfi1_txreq_from_idx(tx_ring, tail);\n\ttrace_hfi1_txq_alloc_tx(txq);\n\n\t \n\ttx->txreq.num_desc = 0;\n\ttx->txq = txq;\n\ttx->skb = skb;\n\tINIT_LIST_HEAD(&tx->txreq.list);\n\n\thfi1_ipoib_build_ib_tx_headers(tx, txp);\n\n\tret = hfi1_ipoib_build_tx_desc(tx, txp);\n\tif (likely(!ret)) {\n\t\tif (txq->flow.as_int != txp->flow.as_int) {\n\t\t\ttxq->flow.tx_queue = txp->flow.tx_queue;\n\t\t\ttxq->flow.sc5 = txp->flow.sc5;\n\t\t\ttxq->sde =\n\t\t\t\tsdma_select_engine_sc(priv->dd,\n\t\t\t\t\t\t      txp->flow.tx_queue,\n\t\t\t\t\t\t      txp->flow.sc5);\n\t\t\ttrace_hfi1_flow_switch(txq);\n\t\t}\n\n\t\treturn tx;\n\t}\n\n\tsdma_txclean(priv->dd, &tx->txreq);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic int hfi1_ipoib_submit_tx_list(struct net_device *dev,\n\t\t\t\t     struct hfi1_ipoib_txq *txq)\n{\n\tint ret;\n\tu16 count_out;\n\n\tret = sdma_send_txlist(txq->sde,\n\t\t\t       iowait_get_ib_work(&txq->wait),\n\t\t\t       &txq->tx_list,\n\t\t\t       &count_out);\n\tif (likely(!ret) || ret == -EBUSY || ret == -ECOMM)\n\t\treturn ret;\n\n\tdd_dev_warn(txq->priv->dd, \"cannot send skb tx list, err %d.\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int hfi1_ipoib_flush_tx_list(struct net_device *dev,\n\t\t\t\t    struct hfi1_ipoib_txq *txq)\n{\n\tint ret = 0;\n\n\tif (!list_empty(&txq->tx_list)) {\n\t\t \n\t\tret = hfi1_ipoib_submit_tx_list(dev, txq);\n\n\t\tif (unlikely(ret))\n\t\t\tif (ret != -EBUSY)\n\t\t\t\t++dev->stats.tx_carrier_errors;\n\t}\n\n\treturn ret;\n}\n\nstatic int hfi1_ipoib_submit_tx(struct hfi1_ipoib_txq *txq,\n\t\t\t\tstruct ipoib_txreq *tx)\n{\n\tint ret;\n\n\tret = sdma_send_txreq(txq->sde,\n\t\t\t      iowait_get_ib_work(&txq->wait),\n\t\t\t      &tx->txreq,\n\t\t\t      txq->pkts_sent);\n\tif (likely(!ret)) {\n\t\ttxq->pkts_sent = true;\n\t\tiowait_starve_clear(txq->pkts_sent, &txq->wait);\n\t}\n\n\treturn ret;\n}\n\nstatic int hfi1_ipoib_send_dma_single(struct net_device *dev,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      struct ipoib_txparms *txp)\n{\n\tstruct hfi1_ipoib_txq *txq = txp->txq;\n\tstruct hfi1_ipoib_circ_buf *tx_ring;\n\tstruct ipoib_txreq *tx;\n\tint ret;\n\n\ttx = hfi1_ipoib_send_dma_common(dev, skb, txp);\n\tif (IS_ERR(tx)) {\n\t\tint ret = PTR_ERR(tx);\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\tif (ret == -ENOMEM)\n\t\t\t++dev->stats.tx_errors;\n\t\telse\n\t\t\t++dev->stats.tx_carrier_errors;\n\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttx_ring = &txq->tx_ring;\n\ttrace_hfi1_tx_consume(tx, tx_ring->tail);\n\t \n\tsmp_store_release(&tx_ring->tail, CIRC_NEXT(tx_ring->tail, tx_ring->max_items));\n\tret = hfi1_ipoib_submit_tx(txq, tx);\n\tif (likely(!ret)) {\ntx_ok:\n\t\ttrace_sdma_output_ibhdr(txq->priv->dd,\n\t\t\t\t\t&tx->sdma_hdr->hdr,\n\t\t\t\t\tib_is_sc5(txp->flow.sc5));\n\t\thfi1_ipoib_check_queue_depth(txq);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttxq->pkts_sent = false;\n\n\tif (ret == -EBUSY || ret == -ECOMM)\n\t\tgoto tx_ok;\n\n\t \n\tsmp_store_release(&tx->complete, 1);\n\tnapi_schedule(&tx->txq->napi);\n\n\t++dev->stats.tx_carrier_errors;\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic int hfi1_ipoib_send_dma_list(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    struct ipoib_txparms *txp)\n{\n\tstruct hfi1_ipoib_txq *txq = txp->txq;\n\tstruct hfi1_ipoib_circ_buf *tx_ring;\n\tstruct ipoib_txreq *tx;\n\n\t \n\tif (txq->flow.as_int != txp->flow.as_int) {\n\t\tint ret;\n\n\t\ttrace_hfi1_flow_flush(txq);\n\t\tret = hfi1_ipoib_flush_tx_list(dev, txq);\n\t\tif (unlikely(ret)) {\n\t\t\tif (ret == -EBUSY)\n\t\t\t\t++dev->stats.tx_dropped;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\ttx = hfi1_ipoib_send_dma_common(dev, skb, txp);\n\tif (IS_ERR(tx)) {\n\t\tint ret = PTR_ERR(tx);\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\tif (ret == -ENOMEM)\n\t\t\t++dev->stats.tx_errors;\n\t\telse\n\t\t\t++dev->stats.tx_carrier_errors;\n\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttx_ring = &txq->tx_ring;\n\ttrace_hfi1_tx_consume(tx, tx_ring->tail);\n\t \n\tsmp_store_release(&tx_ring->tail, CIRC_NEXT(tx_ring->tail, tx_ring->max_items));\n\tlist_add_tail(&tx->txreq.list, &txq->tx_list);\n\n\thfi1_ipoib_check_queue_depth(txq);\n\n\ttrace_sdma_output_ibhdr(txq->priv->dd,\n\t\t\t\t&tx->sdma_hdr->hdr,\n\t\t\t\tib_is_sc5(txp->flow.sc5));\n\n\tif (!netdev_xmit_more())\n\t\t(void)hfi1_ipoib_flush_tx_list(dev, txq);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic u8 hfi1_ipoib_calc_entropy(struct sk_buff *skb)\n{\n\tif (skb_transport_header_was_set(skb)) {\n\t\tu8 *hdr = (u8 *)skb_transport_header(skb);\n\n\t\treturn (hdr[0] ^ hdr[1] ^ hdr[2] ^ hdr[3]);\n\t}\n\n\treturn (u8)skb_get_queue_mapping(skb);\n}\n\nint hfi1_ipoib_send(struct net_device *dev,\n\t\t    struct sk_buff *skb,\n\t\t    struct ib_ah *address,\n\t\t    u32 dqpn)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = hfi1_ipoib_priv(dev);\n\tstruct ipoib_txparms txp;\n\tstruct rdma_netdev *rn = netdev_priv(dev);\n\n\tif (unlikely(skb->len > rn->mtu + HFI1_IPOIB_ENCAP_LEN)) {\n\t\tdd_dev_warn(priv->dd, \"packet len %d (> %d) too long to send, dropping\\n\",\n\t\t\t    skb->len,\n\t\t\t    rn->mtu + HFI1_IPOIB_ENCAP_LEN);\n\t\t++dev->stats.tx_dropped;\n\t\t++dev->stats.tx_errors;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttxp.dd = priv->dd;\n\ttxp.ah_attr = &ibah_to_rvtah(address)->attr;\n\ttxp.ibp = to_iport(priv->device, priv->port_num);\n\ttxp.txq = &priv->txqs[skb_get_queue_mapping(skb)];\n\ttxp.dqpn = dqpn;\n\ttxp.flow.sc5 = txp.ibp->sl_to_sc[rdma_ah_get_sl(txp.ah_attr)];\n\ttxp.flow.tx_queue = (u8)skb_get_queue_mapping(skb);\n\ttxp.entropy = hfi1_ipoib_calc_entropy(skb);\n\n\tif (netdev_xmit_more() || !list_empty(&txp.txq->tx_list))\n\t\treturn hfi1_ipoib_send_dma_list(dev, skb, &txp);\n\n\treturn hfi1_ipoib_send_dma_single(dev, skb,  &txp);\n}\n\n \nstatic int hfi1_ipoib_sdma_sleep(struct sdma_engine *sde,\n\t\t\t\t struct iowait_work *wait,\n\t\t\t\t struct sdma_txreq *txreq,\n\t\t\t\t uint seq,\n\t\t\t\t bool pkts_sent)\n{\n\tstruct hfi1_ipoib_txq *txq =\n\t\tcontainer_of(wait->iow, struct hfi1_ipoib_txq, wait);\n\n\twrite_seqlock(&sde->waitlock);\n\n\tif (likely(txq->priv->netdev->reg_state == NETREG_REGISTERED)) {\n\t\tif (sdma_progress(sde, seq, txreq)) {\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\tif (list_empty(&txreq->list))\n\t\t\t \n\t\t\tlist_add_tail(&txreq->list, &txq->tx_list);\n\t\tif (list_empty(&txq->wait.list)) {\n\t\t\tstruct hfi1_ibport *ibp = &sde->ppd->ibport_data;\n\n\t\t\tif (!atomic_xchg(&txq->tx_ring.no_desc, 1)) {\n\t\t\t\ttrace_hfi1_txq_queued(txq);\n\t\t\t\thfi1_ipoib_stop_txq(txq);\n\t\t\t}\n\t\t\tibp->rvp.n_dmawait++;\n\t\t\tiowait_queue(pkts_sent, wait->iow, &sde->dmawait);\n\t\t}\n\n\t\twrite_sequnlock(&sde->waitlock);\n\t\treturn -EBUSY;\n\t}\n\n\twrite_sequnlock(&sde->waitlock);\n\treturn -EINVAL;\n}\n\n \nstatic void hfi1_ipoib_sdma_wakeup(struct iowait *wait, int reason)\n{\n\tstruct hfi1_ipoib_txq *txq =\n\t\tcontainer_of(wait, struct hfi1_ipoib_txq, wait);\n\n\ttrace_hfi1_txq_wakeup(txq);\n\tif (likely(txq->priv->netdev->reg_state == NETREG_REGISTERED))\n\t\tiowait_schedule(wait, system_highpri_wq, WORK_CPU_UNBOUND);\n}\n\nstatic void hfi1_ipoib_flush_txq(struct work_struct *work)\n{\n\tstruct iowait_work *ioww =\n\t\tcontainer_of(work, struct iowait_work, iowork);\n\tstruct iowait *wait = iowait_ioww_to_iow(ioww);\n\tstruct hfi1_ipoib_txq *txq =\n\t\tcontainer_of(wait, struct hfi1_ipoib_txq, wait);\n\tstruct net_device *dev = txq->priv->netdev;\n\n\tif (likely(dev->reg_state == NETREG_REGISTERED) &&\n\t    likely(!hfi1_ipoib_flush_tx_list(dev, txq)))\n\t\tif (atomic_xchg(&txq->tx_ring.no_desc, 0))\n\t\t\thfi1_ipoib_wake_txq(txq);\n}\n\nint hfi1_ipoib_txreq_init(struct hfi1_ipoib_dev_priv *priv)\n{\n\tstruct net_device *dev = priv->netdev;\n\tu32 tx_ring_size, tx_item_size;\n\tstruct hfi1_ipoib_circ_buf *tx_ring;\n\tint i, j;\n\n\t \n\ttx_ring_size = roundup_pow_of_two(dev->tx_queue_len + 1);\n\ttx_item_size = roundup_pow_of_two(sizeof(struct ipoib_txreq));\n\n\tpriv->txqs = kcalloc_node(dev->num_tx_queues,\n\t\t\t\t  sizeof(struct hfi1_ipoib_txq),\n\t\t\t\t  GFP_KERNEL,\n\t\t\t\t  priv->dd->node);\n\tif (!priv->txqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct hfi1_ipoib_txq *txq = &priv->txqs[i];\n\t\tstruct ipoib_txreq *tx;\n\n\t\ttx_ring = &txq->tx_ring;\n\t\tiowait_init(&txq->wait,\n\t\t\t    0,\n\t\t\t    hfi1_ipoib_flush_txq,\n\t\t\t    NULL,\n\t\t\t    hfi1_ipoib_sdma_sleep,\n\t\t\t    hfi1_ipoib_sdma_wakeup,\n\t\t\t    NULL,\n\t\t\t    NULL);\n\t\ttxq->priv = priv;\n\t\ttxq->sde = NULL;\n\t\tINIT_LIST_HEAD(&txq->tx_list);\n\t\tatomic_set(&txq->tx_ring.stops, 0);\n\t\tatomic_set(&txq->tx_ring.ring_full, 0);\n\t\tatomic_set(&txq->tx_ring.no_desc, 0);\n\t\ttxq->q_idx = i;\n\t\ttxq->flow.tx_queue = 0xff;\n\t\ttxq->flow.sc5 = 0xff;\n\t\ttxq->pkts_sent = false;\n\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, i),\n\t\t\t\t\t     priv->dd->node);\n\n\t\ttxq->tx_ring.items =\n\t\t\tkvzalloc_node(array_size(tx_ring_size, tx_item_size),\n\t\t\t\t      GFP_KERNEL, priv->dd->node);\n\t\tif (!txq->tx_ring.items)\n\t\t\tgoto free_txqs;\n\n\t\ttxq->tx_ring.max_items = tx_ring_size;\n\t\ttxq->tx_ring.shift = ilog2(tx_item_size);\n\t\ttxq->tx_ring.avail = hfi1_ipoib_ring_hwat(txq);\n\t\ttx_ring = &txq->tx_ring;\n\t\tfor (j = 0; j < tx_ring_size; j++) {\n\t\t\thfi1_txreq_from_idx(tx_ring, j)->sdma_hdr =\n\t\t\t\tkzalloc_node(sizeof(*tx->sdma_hdr),\n\t\t\t\t\t     GFP_KERNEL, priv->dd->node);\n\t\t\tif (!hfi1_txreq_from_idx(tx_ring, j)->sdma_hdr)\n\t\t\t\tgoto free_txqs;\n\t\t}\n\n\t\tnetif_napi_add_tx(dev, &txq->napi, hfi1_ipoib_poll_tx_ring);\n\t}\n\n\treturn 0;\n\nfree_txqs:\n\tfor (i--; i >= 0; i--) {\n\t\tstruct hfi1_ipoib_txq *txq = &priv->txqs[i];\n\n\t\tnetif_napi_del(&txq->napi);\n\t\ttx_ring = &txq->tx_ring;\n\t\tfor (j = 0; j < tx_ring_size; j++)\n\t\t\tkfree(hfi1_txreq_from_idx(tx_ring, j)->sdma_hdr);\n\t\tkvfree(tx_ring->items);\n\t}\n\n\tkfree(priv->txqs);\n\tpriv->txqs = NULL;\n\treturn -ENOMEM;\n}\n\nstatic void hfi1_ipoib_drain_tx_list(struct hfi1_ipoib_txq *txq)\n{\n\tstruct sdma_txreq *txreq;\n\tstruct sdma_txreq *txreq_tmp;\n\n\tlist_for_each_entry_safe(txreq, txreq_tmp, &txq->tx_list, list) {\n\t\tstruct ipoib_txreq *tx =\n\t\t\tcontainer_of(txreq, struct ipoib_txreq, txreq);\n\n\t\tlist_del(&txreq->list);\n\t\tsdma_txclean(txq->priv->dd, &tx->txreq);\n\t\tdev_kfree_skb_any(tx->skb);\n\t\ttx->skb = NULL;\n\t\ttxq->tx_ring.complete_txreqs++;\n\t}\n\n\tif (hfi1_ipoib_used(txq))\n\t\tdd_dev_warn(txq->priv->dd,\n\t\t\t    \"txq %d not empty found %u requests\\n\",\n\t\t\t    txq->q_idx,\n\t\t\t    hfi1_ipoib_txreqs(txq->tx_ring.sent_txreqs,\n\t\t\t\t\t      txq->tx_ring.complete_txreqs));\n}\n\nvoid hfi1_ipoib_txreq_deinit(struct hfi1_ipoib_dev_priv *priv)\n{\n\tint i, j;\n\n\tfor (i = 0; i < priv->netdev->num_tx_queues; i++) {\n\t\tstruct hfi1_ipoib_txq *txq = &priv->txqs[i];\n\t\tstruct hfi1_ipoib_circ_buf *tx_ring = &txq->tx_ring;\n\n\t\tiowait_cancel_work(&txq->wait);\n\t\tiowait_sdma_drain(&txq->wait);\n\t\thfi1_ipoib_drain_tx_list(txq);\n\t\tnetif_napi_del(&txq->napi);\n\t\thfi1_ipoib_drain_tx_ring(txq);\n\t\tfor (j = 0; j < tx_ring->max_items; j++)\n\t\t\tkfree(hfi1_txreq_from_idx(tx_ring, j)->sdma_hdr);\n\t\tkvfree(tx_ring->items);\n\t}\n\n\tkfree(priv->txqs);\n\tpriv->txqs = NULL;\n}\n\nvoid hfi1_ipoib_napi_tx_enable(struct net_device *dev)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = hfi1_ipoib_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct hfi1_ipoib_txq *txq = &priv->txqs[i];\n\n\t\tnapi_enable(&txq->napi);\n\t}\n}\n\nvoid hfi1_ipoib_napi_tx_disable(struct net_device *dev)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = hfi1_ipoib_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct hfi1_ipoib_txq *txq = &priv->txqs[i];\n\n\t\tnapi_disable(&txq->napi);\n\t\thfi1_ipoib_drain_tx_ring(txq);\n\t}\n}\n\nvoid hfi1_ipoib_tx_timeout(struct net_device *dev, unsigned int q)\n{\n\tstruct hfi1_ipoib_dev_priv *priv = hfi1_ipoib_priv(dev);\n\tstruct hfi1_ipoib_txq *txq = &priv->txqs[q];\n\n\tdd_dev_info(priv->dd, \"timeout txq %p q %u stopped %u stops %d no_desc %d ring_full %d\\n\",\n\t\t    txq, q,\n\t\t    __netif_subqueue_stopped(dev, txq->q_idx),\n\t\t    atomic_read(&txq->tx_ring.stops),\n\t\t    atomic_read(&txq->tx_ring.no_desc),\n\t\t    atomic_read(&txq->tx_ring.ring_full));\n\tdd_dev_info(priv->dd, \"sde %p engine %u\\n\",\n\t\t    txq->sde,\n\t\t    txq->sde ? txq->sde->this_idx : 0);\n\tdd_dev_info(priv->dd, \"flow %x\\n\", txq->flow.as_int);\n\tdd_dev_info(priv->dd, \"sent %llu completed %llu used %llu\\n\",\n\t\t    txq->tx_ring.sent_txreqs, txq->tx_ring.complete_txreqs,\n\t\t    hfi1_ipoib_used(txq));\n\tdd_dev_info(priv->dd, \"tx_queue_len %u max_items %u\\n\",\n\t\t    dev->tx_queue_len, txq->tx_ring.max_items);\n\tdd_dev_info(priv->dd, \"head %u tail %u\\n\",\n\t\t    txq->tx_ring.head, txq->tx_ring.tail);\n\tdd_dev_info(priv->dd, \"wait queued %u\\n\",\n\t\t    !list_empty(&txq->wait.list));\n\tdd_dev_info(priv->dd, \"tx_list empty %u\\n\",\n\t\t    list_empty(&txq->tx_list));\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}