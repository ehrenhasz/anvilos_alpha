{
  "module_name": "pio.c",
  "hash_id": "1fda0eab476f75e1cb6c703610b44eada2910e7ef4beba251c79e44514e43b64",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/pio.c",
  "human_readable_source": "\n \n\n#include <linux/delay.h>\n#include \"hfi.h\"\n#include \"qp.h\"\n#include \"trace.h\"\n\n#define SC(name) SEND_CTXT_##name\n \nstatic void sc_wait_for_packet_egress(struct send_context *sc, int pause);\n\n \nvoid __cm_reset(struct hfi1_devdata *dd, u64 sendctrl)\n{\n\twrite_csr(dd, SEND_CTRL, sendctrl | SEND_CTRL_CM_RESET_SMASK);\n\twhile (1) {\n\t\tudelay(1);\n\t\tsendctrl = read_csr(dd, SEND_CTRL);\n\t\tif ((sendctrl & SEND_CTRL_CM_RESET_SMASK) == 0)\n\t\t\tbreak;\n\t}\n}\n\n \nvoid pio_send_control(struct hfi1_devdata *dd, int op)\n{\n\tu64 reg, mask;\n\tunsigned long flags;\n\tint write = 1;\t \n\tint flush = 0;\t \n\tint i;\n\n\tspin_lock_irqsave(&dd->sendctrl_lock, flags);\n\n\treg = read_csr(dd, SEND_CTRL);\n\tswitch (op) {\n\tcase PSC_GLOBAL_ENABLE:\n\t\treg |= SEND_CTRL_SEND_ENABLE_SMASK;\n\t\tfallthrough;\n\tcase PSC_DATA_VL_ENABLE:\n\t\tmask = 0;\n\t\tfor (i = 0; i < ARRAY_SIZE(dd->vld); i++)\n\t\t\tif (!dd->vld[i].mtu)\n\t\t\t\tmask |= BIT_ULL(i);\n\t\t \n\t\tmask = (mask & SEND_CTRL_UNSUPPORTED_VL_MASK) <<\n\t\t\tSEND_CTRL_UNSUPPORTED_VL_SHIFT;\n\t\treg = (reg & ~SEND_CTRL_UNSUPPORTED_VL_SMASK) | mask;\n\t\tbreak;\n\tcase PSC_GLOBAL_DISABLE:\n\t\treg &= ~SEND_CTRL_SEND_ENABLE_SMASK;\n\t\tbreak;\n\tcase PSC_GLOBAL_VLARB_ENABLE:\n\t\treg |= SEND_CTRL_VL_ARBITER_ENABLE_SMASK;\n\t\tbreak;\n\tcase PSC_GLOBAL_VLARB_DISABLE:\n\t\treg &= ~SEND_CTRL_VL_ARBITER_ENABLE_SMASK;\n\t\tbreak;\n\tcase PSC_CM_RESET:\n\t\t__cm_reset(dd, reg);\n\t\twrite = 0;  \n\t\tbreak;\n\tcase PSC_DATA_VL_DISABLE:\n\t\treg |= SEND_CTRL_UNSUPPORTED_VL_SMASK;\n\t\tflush = 1;\n\t\tbreak;\n\tdefault:\n\t\tdd_dev_err(dd, \"%s: invalid control %d\\n\", __func__, op);\n\t\tbreak;\n\t}\n\n\tif (write) {\n\t\twrite_csr(dd, SEND_CTRL, reg);\n\t\tif (flush)\n\t\t\t(void)read_csr(dd, SEND_CTRL);  \n\t}\n\n\tspin_unlock_irqrestore(&dd->sendctrl_lock, flags);\n}\n\n \n#define NUM_SC_POOLS 2\n\n \n#define SCS_POOL_0 -1\n#define SCS_POOL_1 -2\n\n \n#define SCC_PER_VL -1\n#define SCC_PER_CPU  -2\n#define SCC_PER_KRCVQ  -3\n\n \n#define SCS_ACK_CREDITS  32\n#define SCS_VL15_CREDITS 102\t \n\n#define PIO_THRESHOLD_CEILING 4096\n\n#define PIO_WAIT_BATCH_SIZE 5\n\n \nstatic struct sc_config_sizes sc_config_sizes[SC_MAX] = {\n\t[SC_KERNEL] = { .size  = SCS_POOL_0,\t \n\t\t\t.count = SCC_PER_VL },\t \n\t[SC_ACK]    = { .size  = SCS_ACK_CREDITS,\n\t\t\t.count = SCC_PER_KRCVQ },\n\t[SC_USER]   = { .size  = SCS_POOL_0,\t \n\t\t\t.count = SCC_PER_CPU },\t \n\t[SC_VL15]   = { .size  = SCS_VL15_CREDITS,\n\t\t\t.count = 1 },\n\n};\n\n \nstruct mem_pool_config {\n\tint centipercent;\t \n\tint absolute_blocks;\t \n};\n\n \nstatic struct mem_pool_config sc_mem_pool_config[NUM_SC_POOLS] = {\n\t \n\t{  10000,     -1 },\t\t \n\t{      0,     -1 },\t\t \n};\n\n \nstruct mem_pool_info {\n\tint centipercent;\t \n\tint count;\t\t \n\tint blocks;\t\t \n\tint size;\t\t \n};\n\n \nstatic int wildcard_to_pool(int wc)\n{\n\tif (wc >= 0)\n\t\treturn -1;\t \n\treturn -wc - 1;\n}\n\nstatic const char *sc_type_names[SC_MAX] = {\n\t\"kernel\",\n\t\"ack\",\n\t\"user\",\n\t\"vl15\"\n};\n\nstatic const char *sc_type_name(int index)\n{\n\tif (index < 0 || index >= SC_MAX)\n\t\treturn \"unknown\";\n\treturn sc_type_names[index];\n}\n\n \nint init_sc_pools_and_sizes(struct hfi1_devdata *dd)\n{\n\tstruct mem_pool_info mem_pool_info[NUM_SC_POOLS] = { { 0 } };\n\tint total_blocks = (chip_pio_mem_size(dd) / PIO_BLOCK_SIZE) - 1;\n\tint total_contexts = 0;\n\tint fixed_blocks;\n\tint pool_blocks;\n\tint used_blocks;\n\tint cp_total;\t\t \n\tint ab_total;\t\t \n\tint extra;\n\tint i;\n\n\t \n\tif (HFI1_CAP_IS_KSET(SDMA)) {\n\t\tu16 max_pkt_size = (piothreshold < PIO_THRESHOLD_CEILING) ?\n\t\t\t\t\t piothreshold : PIO_THRESHOLD_CEILING;\n\t\tsc_config_sizes[SC_KERNEL].size =\n\t\t\t3 * (max_pkt_size + 128) / PIO_BLOCK_SIZE;\n\t}\n\n\t \n\tcp_total = 0;\n\tab_total = 0;\n\tfor (i = 0; i < NUM_SC_POOLS; i++) {\n\t\tint cp = sc_mem_pool_config[i].centipercent;\n\t\tint ab = sc_mem_pool_config[i].absolute_blocks;\n\n\t\t \n\t\tif (cp >= 0) {\t\t\t \n\t\t\tcp_total += cp;\n\t\t} else if (ab >= 0) {\t\t \n\t\t\tab_total += ab;\n\t\t} else {\t\t\t \n\t\t\tdd_dev_err(\n\t\t\t\tdd,\n\t\t\t\t\"Send context memory pool %d: both the block count and centipercent are invalid\\n\",\n\t\t\t\ti);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmem_pool_info[i].centipercent = cp;\n\t\tmem_pool_info[i].blocks = ab;\n\t}\n\n\t \n\tif (cp_total != 0 && ab_total != 0) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"All send context memory pools must be described as either centipercent or blocks, no mixing between pools\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (cp_total != 0 && cp_total != 10000) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"Send context memory pool centipercent is %d, expecting 10000\\n\",\n\t\t\tcp_total);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (ab_total > total_blocks) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"Send context memory pool absolute block count %d is larger than the memory size %d\\n\",\n\t\t\tab_total, total_blocks);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfixed_blocks = 0;\n\tfor (i = 0; i < SC_MAX; i++) {\n\t\tint count = sc_config_sizes[i].count;\n\t\tint size = sc_config_sizes[i].size;\n\t\tint pool;\n\n\t\t \n\t\tif (i == SC_ACK) {\n\t\t\tcount = dd->n_krcv_queues;\n\t\t} else if (i == SC_KERNEL) {\n\t\t\tcount = INIT_SC_PER_VL * num_vls;\n\t\t} else if (count == SCC_PER_CPU) {\n\t\t\tcount = dd->num_rcv_contexts - dd->n_krcv_queues;\n\t\t} else if (count < 0) {\n\t\t\tdd_dev_err(\n\t\t\t\tdd,\n\t\t\t\t\"%s send context invalid count wildcard %d\\n\",\n\t\t\t\tsc_type_name(i), count);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (total_contexts + count > chip_send_contexts(dd))\n\t\t\tcount = chip_send_contexts(dd) - total_contexts;\n\n\t\ttotal_contexts += count;\n\n\t\t \n\t\tpool = wildcard_to_pool(size);\n\t\tif (pool == -1) {\t\t\t \n\t\t\tfixed_blocks += size * count;\n\t\t} else if (pool < NUM_SC_POOLS) {\t \n\t\t\tmem_pool_info[pool].count += count;\n\t\t} else {\t\t\t\t \n\t\t\tdd_dev_err(\n\t\t\t\tdd,\n\t\t\t\t\"%s send context invalid pool wildcard %d\\n\",\n\t\t\t\tsc_type_name(i), size);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdd->sc_sizes[i].count = count;\n\t\tdd->sc_sizes[i].size = size;\n\t}\n\tif (fixed_blocks > total_blocks) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"Send context fixed block count, %u, larger than total block count %u\\n\",\n\t\t\tfixed_blocks, total_blocks);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tpool_blocks = total_blocks - fixed_blocks;\n\tif (ab_total > pool_blocks) {\n\t\tdd_dev_err(\n\t\t\tdd,\n\t\t\t\"Send context fixed pool sizes, %u, larger than pool block count %u\\n\",\n\t\t\tab_total, pool_blocks);\n\t\treturn -EINVAL;\n\t}\n\t \n\tpool_blocks -= ab_total;\n\n\tfor (i = 0; i < NUM_SC_POOLS; i++) {\n\t\tstruct mem_pool_info *pi = &mem_pool_info[i];\n\n\t\t \n\t\tif (pi->centipercent >= 0)\n\t\t\tpi->blocks = (pool_blocks * pi->centipercent) / 10000;\n\n\t\tif (pi->blocks == 0 && pi->count != 0) {\n\t\t\tdd_dev_err(\n\t\t\t\tdd,\n\t\t\t\t\"Send context memory pool %d has %u contexts, but no blocks\\n\",\n\t\t\t\ti, pi->count);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (pi->count == 0) {\n\t\t\t \n\t\t\tif (pi->blocks != 0)\n\t\t\t\tdd_dev_err(\n\t\t\t\t\tdd,\n\t\t\t\t\t\"Send context memory pool %d has %u blocks, but zero contexts\\n\",\n\t\t\t\t\ti, pi->blocks);\n\t\t\tpi->size = 0;\n\t\t} else {\n\t\t\tpi->size = pi->blocks / pi->count;\n\t\t}\n\t}\n\n\t \n\tused_blocks = 0;\n\tfor (i = 0; i < SC_MAX; i++) {\n\t\tif (dd->sc_sizes[i].size < 0) {\n\t\t\tunsigned pool = wildcard_to_pool(dd->sc_sizes[i].size);\n\n\t\t\tWARN_ON_ONCE(pool >= NUM_SC_POOLS);\n\t\t\tdd->sc_sizes[i].size = mem_pool_info[pool].size;\n\t\t}\n\t\t \n#define PIO_MAX_BLOCKS 1024\n\t\tif (dd->sc_sizes[i].size > PIO_MAX_BLOCKS)\n\t\t\tdd->sc_sizes[i].size = PIO_MAX_BLOCKS;\n\n\t\t \n\t\tused_blocks += dd->sc_sizes[i].size * dd->sc_sizes[i].count;\n\t}\n\textra = total_blocks - used_blocks;\n\tif (extra != 0)\n\t\tdd_dev_info(dd, \"unused send context blocks: %d\\n\", extra);\n\n\treturn total_contexts;\n}\n\nint init_send_contexts(struct hfi1_devdata *dd)\n{\n\tu16 base;\n\tint ret, i, j, context;\n\n\tret = init_credit_return(dd);\n\tif (ret)\n\t\treturn ret;\n\n\tdd->hw_to_sw = kmalloc_array(TXE_NUM_CONTEXTS, sizeof(u8),\n\t\t\t\t\tGFP_KERNEL);\n\tdd->send_contexts = kcalloc(dd->num_send_contexts,\n\t\t\t\t    sizeof(struct send_context_info),\n\t\t\t\t    GFP_KERNEL);\n\tif (!dd->send_contexts || !dd->hw_to_sw) {\n\t\tkfree(dd->hw_to_sw);\n\t\tkfree(dd->send_contexts);\n\t\tfree_credit_return(dd);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (i = 0; i < TXE_NUM_CONTEXTS; i++)\n\t\tdd->hw_to_sw[i] = INVALID_SCI;\n\n\t \n\tcontext = 0;\n\tbase = 1;\n\tfor (i = 0; i < SC_MAX; i++) {\n\t\tstruct sc_config_sizes *scs = &dd->sc_sizes[i];\n\n\t\tfor (j = 0; j < scs->count; j++) {\n\t\t\tstruct send_context_info *sci =\n\t\t\t\t\t\t&dd->send_contexts[context];\n\t\t\tsci->type = i;\n\t\t\tsci->base = base;\n\t\t\tsci->credits = scs->size;\n\n\t\t\tcontext++;\n\t\t\tbase += scs->size;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sc_hw_alloc(struct hfi1_devdata *dd, int type, u32 *sw_index,\n\t\t       u32 *hw_context)\n{\n\tstruct send_context_info *sci;\n\tu32 index;\n\tu32 context;\n\n\tfor (index = 0, sci = &dd->send_contexts[0];\n\t\t\tindex < dd->num_send_contexts; index++, sci++) {\n\t\tif (sci->type == type && sci->allocated == 0) {\n\t\t\tsci->allocated = 1;\n\t\t\t \n\t\t\tcontext = chip_send_contexts(dd) - index - 1;\n\t\t\tdd->hw_to_sw[context] = index;\n\t\t\t*sw_index = index;\n\t\t\t*hw_context = context;\n\t\t\treturn 0;  \n\t\t}\n\t}\n\tdd_dev_err(dd, \"Unable to locate a free type %d send context\\n\", type);\n\treturn -ENOSPC;\n}\n\n \nstatic void sc_hw_free(struct hfi1_devdata *dd, u32 sw_index, u32 hw_context)\n{\n\tstruct send_context_info *sci;\n\n\tsci = &dd->send_contexts[sw_index];\n\tif (!sci->allocated) {\n\t\tdd_dev_err(dd, \"%s: sw_index %u not allocated? hw_context %u\\n\",\n\t\t\t   __func__, sw_index, hw_context);\n\t}\n\tsci->allocated = 0;\n\tdd->hw_to_sw[hw_context] = INVALID_SCI;\n}\n\n \nstatic inline u32 group_context(u32 context, u32 group)\n{\n\treturn (context >> group) << group;\n}\n\n \nstatic inline u32 group_size(u32 group)\n{\n\treturn 1 << group;\n}\n\n \nstatic void cr_group_addresses(struct send_context *sc, dma_addr_t *dma)\n{\n\tu32 gc = group_context(sc->hw_context, sc->group);\n\tu32 index = sc->hw_context & 0x7;\n\n\tsc->hw_free = &sc->dd->cr_base[sc->node].va[gc].cr[index];\n\t*dma = (unsigned long)\n\t       &((struct credit_return *)sc->dd->cr_base[sc->node].dma)[gc];\n}\n\n \nstatic void sc_halted(struct work_struct *work)\n{\n\tstruct send_context *sc;\n\n\tsc = container_of(work, struct send_context, halt_work);\n\tsc_restart(sc);\n}\n\n \nu32 sc_mtu_to_threshold(struct send_context *sc, u32 mtu, u32 hdrqentsize)\n{\n\tu32 release_credits;\n\tu32 threshold;\n\n\t \n\tmtu += hdrqentsize << 2;\n\trelease_credits = DIV_ROUND_UP(mtu, PIO_BLOCK_SIZE);\n\n\t \n\tif (sc->credits <= release_credits)\n\t\tthreshold = 1;\n\telse\n\t\tthreshold = sc->credits - release_credits;\n\n\treturn threshold;\n}\n\n \nu32 sc_percent_to_threshold(struct send_context *sc, u32 percent)\n{\n\treturn (sc->credits * percent) / 100;\n}\n\n \nvoid sc_set_cr_threshold(struct send_context *sc, u32 new_threshold)\n{\n\tunsigned long flags;\n\tu32 old_threshold;\n\tint force_return = 0;\n\n\tspin_lock_irqsave(&sc->credit_ctrl_lock, flags);\n\n\told_threshold = (sc->credit_ctrl >>\n\t\t\t\tSC(CREDIT_CTRL_THRESHOLD_SHIFT))\n\t\t\t & SC(CREDIT_CTRL_THRESHOLD_MASK);\n\n\tif (new_threshold != old_threshold) {\n\t\tsc->credit_ctrl =\n\t\t\t(sc->credit_ctrl\n\t\t\t\t& ~SC(CREDIT_CTRL_THRESHOLD_SMASK))\n\t\t\t| ((new_threshold\n\t\t\t\t& SC(CREDIT_CTRL_THRESHOLD_MASK))\n\t\t\t   << SC(CREDIT_CTRL_THRESHOLD_SHIFT));\n\t\twrite_kctxt_csr(sc->dd, sc->hw_context,\n\t\t\t\tSC(CREDIT_CTRL), sc->credit_ctrl);\n\n\t\t \n\t\tforce_return = 1;\n\t}\n\n\tspin_unlock_irqrestore(&sc->credit_ctrl_lock, flags);\n\n\tif (force_return)\n\t\tsc_return_credits(sc);\n}\n\n \nvoid set_pio_integrity(struct send_context *sc)\n{\n\tstruct hfi1_devdata *dd = sc->dd;\n\tu32 hw_context = sc->hw_context;\n\tint type = sc->type;\n\n\twrite_kctxt_csr(dd, hw_context,\n\t\t\tSC(CHECK_ENABLE),\n\t\t\thfi1_pkt_default_send_ctxt_mask(dd, type));\n}\n\nstatic u32 get_buffers_allocated(struct send_context *sc)\n{\n\tint cpu;\n\tu32 ret = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tret += *per_cpu_ptr(sc->buffers_allocated, cpu);\n\treturn ret;\n}\n\nstatic void reset_buffers_allocated(struct send_context *sc)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\t(*per_cpu_ptr(sc->buffers_allocated, cpu)) = 0;\n}\n\n \nstruct send_context *sc_alloc(struct hfi1_devdata *dd, int type,\n\t\t\t      uint hdrqentsize, int numa)\n{\n\tstruct send_context_info *sci;\n\tstruct send_context *sc = NULL;\n\tdma_addr_t dma;\n\tunsigned long flags;\n\tu64 reg;\n\tu32 thresh;\n\tu32 sw_index;\n\tu32 hw_context;\n\tint ret;\n\tu8 opval, opmask;\n\n\t \n\tif (dd->flags & HFI1_FROZEN)\n\t\treturn NULL;\n\n\tsc = kzalloc_node(sizeof(*sc), GFP_KERNEL, numa);\n\tif (!sc)\n\t\treturn NULL;\n\n\tsc->buffers_allocated = alloc_percpu(u32);\n\tif (!sc->buffers_allocated) {\n\t\tkfree(sc);\n\t\tdd_dev_err(dd,\n\t\t\t   \"Cannot allocate buffers_allocated per cpu counters\\n\"\n\t\t\t  );\n\t\treturn NULL;\n\t}\n\n\tspin_lock_irqsave(&dd->sc_lock, flags);\n\tret = sc_hw_alloc(dd, type, &sw_index, &hw_context);\n\tif (ret) {\n\t\tspin_unlock_irqrestore(&dd->sc_lock, flags);\n\t\tfree_percpu(sc->buffers_allocated);\n\t\tkfree(sc);\n\t\treturn NULL;\n\t}\n\n\tsci = &dd->send_contexts[sw_index];\n\tsci->sc = sc;\n\n\tsc->dd = dd;\n\tsc->node = numa;\n\tsc->type = type;\n\tspin_lock_init(&sc->alloc_lock);\n\tspin_lock_init(&sc->release_lock);\n\tspin_lock_init(&sc->credit_ctrl_lock);\n\tseqlock_init(&sc->waitlock);\n\tINIT_LIST_HEAD(&sc->piowait);\n\tINIT_WORK(&sc->halt_work, sc_halted);\n\tinit_waitqueue_head(&sc->halt_wait);\n\n\t \n\tsc->group = 0;\n\n\tsc->sw_index = sw_index;\n\tsc->hw_context = hw_context;\n\tcr_group_addresses(sc, &dma);\n\tsc->credits = sci->credits;\n\tsc->size = sc->credits * PIO_BLOCK_SIZE;\n\n \n#define PIO_ADDR_CONTEXT_MASK 0xfful\n#define PIO_ADDR_CONTEXT_SHIFT 16\n\tsc->base_addr = dd->piobase + ((hw_context & PIO_ADDR_CONTEXT_MASK)\n\t\t\t\t\t<< PIO_ADDR_CONTEXT_SHIFT);\n\n\t \n\treg = ((sci->credits & SC(CTRL_CTXT_DEPTH_MASK))\n\t\t\t\t\t<< SC(CTRL_CTXT_DEPTH_SHIFT))\n\t\t| ((sci->base & SC(CTRL_CTXT_BASE_MASK))\n\t\t\t\t\t<< SC(CTRL_CTXT_BASE_SHIFT));\n\twrite_kctxt_csr(dd, hw_context, SC(CTRL), reg);\n\n\tset_pio_integrity(sc);\n\n\t \n\twrite_kctxt_csr(dd, hw_context, SC(ERR_MASK), (u64)-1);\n\n\t \n\twrite_kctxt_csr(dd, hw_context, SC(CHECK_PARTITION_KEY),\n\t\t\t(SC(CHECK_PARTITION_KEY_VALUE_MASK) &\n\t\t\t DEFAULT_PKEY) <<\n\t\t\tSC(CHECK_PARTITION_KEY_VALUE_SHIFT));\n\n\t \n\tif (type == SC_USER) {\n\t\topval = USER_OPCODE_CHECK_VAL;\n\t\topmask = USER_OPCODE_CHECK_MASK;\n\t} else {\n\t\topval = OPCODE_CHECK_VAL_DISABLED;\n\t\topmask = OPCODE_CHECK_MASK_DISABLED;\n\t}\n\n\t \n\twrite_kctxt_csr(dd, hw_context, SC(CHECK_OPCODE),\n\t\t\t((u64)opmask << SC(CHECK_OPCODE_MASK_SHIFT)) |\n\t\t\t((u64)opval << SC(CHECK_OPCODE_VALUE_SHIFT)));\n\n\t \n\treg = dma & SC(CREDIT_RETURN_ADDR_ADDRESS_SMASK);\n\twrite_kctxt_csr(dd, hw_context, SC(CREDIT_RETURN_ADDR), reg);\n\n\t \n\tif (type == SC_ACK) {\n\t\tthresh = sc_percent_to_threshold(sc, 50);\n\t} else if (type == SC_USER) {\n\t\tthresh = sc_percent_to_threshold(sc,\n\t\t\t\t\t\t user_credit_return_threshold);\n\t} else {  \n\t\tthresh = min(sc_percent_to_threshold(sc, 50),\n\t\t\t     sc_mtu_to_threshold(sc, hfi1_max_mtu,\n\t\t\t\t\t\t hdrqentsize));\n\t}\n\treg = thresh << SC(CREDIT_CTRL_THRESHOLD_SHIFT);\n\t \n\tif (type == SC_USER && HFI1_CAP_IS_USET(EARLY_CREDIT_RETURN))\n\t\treg |= SC(CREDIT_CTRL_EARLY_RETURN_SMASK);\n\telse if (HFI1_CAP_IS_KSET(EARLY_CREDIT_RETURN))  \n\t\treg |= SC(CREDIT_CTRL_EARLY_RETURN_SMASK);\n\n\t \n\tsc->credit_ctrl = reg;\n\twrite_kctxt_csr(dd, hw_context, SC(CREDIT_CTRL), reg);\n\n\t \n\tif (type == SC_USER) {\n\t\treg = 1ULL << 15;\n\t\twrite_kctxt_csr(dd, hw_context, SC(CHECK_VL), reg);\n\t}\n\n\tspin_unlock_irqrestore(&dd->sc_lock, flags);\n\n\t \n\tif (type != SC_USER) {\n\t\t \n\t\tsc->sr_size = sci->credits + 1;\n\t\tsc->sr = kcalloc_node(sc->sr_size,\n\t\t\t\t      sizeof(union pio_shadow_ring),\n\t\t\t\t      GFP_KERNEL, numa);\n\t\tif (!sc->sr) {\n\t\t\tsc_free(sc);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\thfi1_cdbg(PIO,\n\t\t  \"Send context %u(%u) %s group %u credits %u credit_ctrl 0x%llx threshold %u\",\n\t\t  sw_index,\n\t\t  hw_context,\n\t\t  sc_type_name(type),\n\t\t  sc->group,\n\t\t  sc->credits,\n\t\t  sc->credit_ctrl,\n\t\t  thresh);\n\n\treturn sc;\n}\n\n \nvoid sc_free(struct send_context *sc)\n{\n\tstruct hfi1_devdata *dd;\n\tunsigned long flags;\n\tu32 sw_index;\n\tu32 hw_context;\n\n\tif (!sc)\n\t\treturn;\n\n\tsc->flags |= SCF_IN_FREE;\t \n\tdd = sc->dd;\n\tif (!list_empty(&sc->piowait))\n\t\tdd_dev_err(dd, \"piowait list not empty!\\n\");\n\tsw_index = sc->sw_index;\n\thw_context = sc->hw_context;\n\tsc_disable(sc);\t \n\tflush_work(&sc->halt_work);\n\n\tspin_lock_irqsave(&dd->sc_lock, flags);\n\tdd->send_contexts[sw_index].sc = NULL;\n\n\t \n\twrite_kctxt_csr(dd, hw_context, SC(CTRL), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(CHECK_ENABLE), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(ERR_MASK), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(CHECK_PARTITION_KEY), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(CHECK_OPCODE), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(CREDIT_RETURN_ADDR), 0);\n\twrite_kctxt_csr(dd, hw_context, SC(CREDIT_CTRL), 0);\n\n\t \n\tsc_hw_free(dd, sw_index, hw_context);\n\tspin_unlock_irqrestore(&dd->sc_lock, flags);\n\n\tkfree(sc->sr);\n\tfree_percpu(sc->buffers_allocated);\n\tkfree(sc);\n}\n\n \nvoid sc_disable(struct send_context *sc)\n{\n\tu64 reg;\n\tstruct pio_buf *pbuf;\n\tLIST_HEAD(wake_list);\n\n\tif (!sc)\n\t\treturn;\n\n\t \n\tspin_lock_irq(&sc->alloc_lock);\n\treg = read_kctxt_csr(sc->dd, sc->hw_context, SC(CTRL));\n\treg &= ~SC(CTRL_CTXT_ENABLE_SMASK);\n\tsc->flags &= ~SCF_ENABLED;\n\tsc_wait_for_packet_egress(sc, 1);\n\twrite_kctxt_csr(sc->dd, sc->hw_context, SC(CTRL), reg);\n\n\t \n\tudelay(1);\n\tspin_lock(&sc->release_lock);\n\tif (sc->sr) {\t \n\t\twhile (sc->sr_tail != sc->sr_head) {\n\t\t\tpbuf = &sc->sr[sc->sr_tail].pbuf;\n\t\t\tif (pbuf->cb)\n\t\t\t\t(*pbuf->cb)(pbuf->arg, PRC_SC_DISABLE);\n\t\t\tsc->sr_tail++;\n\t\t\tif (sc->sr_tail >= sc->sr_size)\n\t\t\t\tsc->sr_tail = 0;\n\t\t}\n\t}\n\tspin_unlock(&sc->release_lock);\n\n\twrite_seqlock(&sc->waitlock);\n\tlist_splice_init(&sc->piowait, &wake_list);\n\twrite_sequnlock(&sc->waitlock);\n\twhile (!list_empty(&wake_list)) {\n\t\tstruct iowait *wait;\n\t\tstruct rvt_qp *qp;\n\t\tstruct hfi1_qp_priv *priv;\n\n\t\twait = list_first_entry(&wake_list, struct iowait, list);\n\t\tqp = iowait_to_qp(wait);\n\t\tpriv = qp->priv;\n\t\tlist_del_init(&priv->s_iowait.list);\n\t\tpriv->s_iowait.lock = NULL;\n\t\thfi1_qp_wakeup(qp, RVT_S_WAIT_PIO | HFI1_S_WAIT_PIO_DRAIN);\n\t}\n\n\tspin_unlock_irq(&sc->alloc_lock);\n}\n\n \nstatic u64 packet_occupancy(u64 reg)\n{\n\treturn (reg &\n\t\tSEND_EGRESS_CTXT_STATUS_CTXT_EGRESS_PACKET_OCCUPANCY_SMASK)\n\t\t>> SEND_EGRESS_CTXT_STATUS_CTXT_EGRESS_PACKET_OCCUPANCY_SHIFT;\n}\n\n \nstatic bool egress_halted(u64 reg)\n{\n\treturn !!(reg & SEND_EGRESS_CTXT_STATUS_CTXT_EGRESS_HALT_STATUS_SMASK);\n}\n\n \nstatic bool is_sc_halted(struct hfi1_devdata *dd, u32 hw_context)\n{\n\treturn !!(read_kctxt_csr(dd, hw_context, SC(STATUS)) &\n\t\t  SC(STATUS_CTXT_HALTED_SMASK));\n}\n\n \nstatic void sc_wait_for_packet_egress(struct send_context *sc, int pause)\n{\n\tstruct hfi1_devdata *dd = sc->dd;\n\tu64 reg = 0;\n\tu64 reg_prev;\n\tu32 loop = 0;\n\n\twhile (1) {\n\t\treg_prev = reg;\n\t\treg = read_csr(dd, sc->hw_context * 8 +\n\t\t\t       SEND_EGRESS_CTXT_STATUS);\n\t\t \n\t\tif (sc->flags & SCF_HALTED ||\n\t\t    is_sc_halted(dd, sc->hw_context) || egress_halted(reg))\n\t\t\tbreak;\n\t\treg = packet_occupancy(reg);\n\t\tif (reg == 0)\n\t\t\tbreak;\n\t\t \n\t\tif (reg != reg_prev)\n\t\t\tloop = 0;\n\t\tif (loop > 50000) {\n\t\t\t \n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: context %u(%u) timeout waiting for packets to egress, remaining count %u, bouncing link\\n\",\n\t\t\t\t   __func__, sc->sw_index,\n\t\t\t\t   sc->hw_context, (u32)reg);\n\t\t\tqueue_work(dd->pport->link_wq,\n\t\t\t\t   &dd->pport->link_bounce_work);\n\t\t\tbreak;\n\t\t}\n\t\tloop++;\n\t\tudelay(1);\n\t}\n\n\tif (pause)\n\t\t \n\t\tpause_for_credit_return(dd);\n}\n\nvoid sc_wait(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < dd->num_send_contexts; i++) {\n\t\tstruct send_context *sc = dd->send_contexts[i].sc;\n\n\t\tif (!sc)\n\t\t\tcontinue;\n\t\tsc_wait_for_packet_egress(sc, 0);\n\t}\n}\n\n \nint sc_restart(struct send_context *sc)\n{\n\tstruct hfi1_devdata *dd = sc->dd;\n\tu64 reg;\n\tu32 loop;\n\tint count;\n\n\t \n\tif (!(sc->flags & SCF_HALTED) || (sc->flags & SCF_IN_FREE))\n\t\treturn -EINVAL;\n\n\tdd_dev_info(dd, \"restarting send context %u(%u)\\n\", sc->sw_index,\n\t\t    sc->hw_context);\n\n\t \n\tloop = 0;\n\twhile (1) {\n\t\treg = read_kctxt_csr(dd, sc->hw_context, SC(STATUS));\n\t\tif (reg & SC(STATUS_CTXT_HALTED_SMASK))\n\t\t\tbreak;\n\t\tif (loop > 100) {\n\t\t\tdd_dev_err(dd, \"%s: context %u(%u) not halting, skipping\\n\",\n\t\t\t\t   __func__, sc->sw_index, sc->hw_context);\n\t\t\treturn -ETIME;\n\t\t}\n\t\tloop++;\n\t\tudelay(1);\n\t}\n\n\t \n\tif (sc->type != SC_USER) {\n\t\t \n\t\tloop = 0;\n\t\twhile (1) {\n\t\t\tcount = get_buffers_allocated(sc);\n\t\t\tif (count == 0)\n\t\t\t\tbreak;\n\t\t\tif (loop > 100) {\n\t\t\t\tdd_dev_err(dd,\n\t\t\t\t\t   \"%s: context %u(%u) timeout waiting for PIO buffers to zero, remaining %d\\n\",\n\t\t\t\t\t   __func__, sc->sw_index,\n\t\t\t\t\t   sc->hw_context, count);\n\t\t\t}\n\t\t\tloop++;\n\t\t\tudelay(1);\n\t\t}\n\t}\n\n\t \n\tsc_disable(sc);\n\n\t \n\treturn sc_enable(sc);\n}\n\n \nvoid pio_freeze(struct hfi1_devdata *dd)\n{\n\tstruct send_context *sc;\n\tint i;\n\n\tfor (i = 0; i < dd->num_send_contexts; i++) {\n\t\tsc = dd->send_contexts[i].sc;\n\t\t \n\t\tif (!sc || !(sc->flags & SCF_FROZEN) || sc->type == SC_USER)\n\t\t\tcontinue;\n\n\t\t \n\t\tsc_disable(sc);\n\t}\n}\n\n \nvoid pio_kernel_unfreeze(struct hfi1_devdata *dd)\n{\n\tstruct send_context *sc;\n\tint i;\n\n\tfor (i = 0; i < dd->num_send_contexts; i++) {\n\t\tsc = dd->send_contexts[i].sc;\n\t\tif (!sc || !(sc->flags & SCF_FROZEN) || sc->type == SC_USER)\n\t\t\tcontinue;\n\t\tif (sc->flags & SCF_LINK_DOWN)\n\t\t\tcontinue;\n\n\t\tsc_enable(sc);\t \n\t}\n}\n\n \nvoid pio_kernel_linkup(struct hfi1_devdata *dd)\n{\n\tstruct send_context *sc;\n\tint i;\n\n\tfor (i = 0; i < dd->num_send_contexts; i++) {\n\t\tsc = dd->send_contexts[i].sc;\n\t\tif (!sc || !(sc->flags & SCF_LINK_DOWN) || sc->type == SC_USER)\n\t\t\tcontinue;\n\n\t\tsc_enable(sc);\t \n\t}\n}\n\n \nstatic int pio_init_wait_progress(struct hfi1_devdata *dd)\n{\n\tu64 reg;\n\tint max, count = 0;\n\n\t \n\tmax = (dd->icode == ICODE_FPGA_EMULATION) ? 120 : 5;\n\twhile (1) {\n\t\treg = read_csr(dd, SEND_PIO_INIT_CTXT);\n\t\tif (!(reg & SEND_PIO_INIT_CTXT_PIO_INIT_IN_PROGRESS_SMASK))\n\t\t\tbreak;\n\t\tif (count >= max)\n\t\t\treturn -ETIMEDOUT;\n\t\tudelay(5);\n\t\tcount++;\n\t}\n\n\treturn reg & SEND_PIO_INIT_CTXT_PIO_INIT_ERR_SMASK ? -EIO : 0;\n}\n\n \nvoid pio_reset_all(struct hfi1_devdata *dd)\n{\n\tint ret;\n\n\t \n\tret = pio_init_wait_progress(dd);\n\t \n\tif (ret == -EIO) {\n\t\t \n\t\twrite_csr(dd, SEND_PIO_ERR_CLEAR,\n\t\t\t  SEND_PIO_ERR_CLEAR_PIO_INIT_SM_IN_ERR_SMASK);\n\t}\n\n\t \n\twrite_csr(dd, SEND_PIO_INIT_CTXT,\n\t\t  SEND_PIO_INIT_CTXT_PIO_ALL_CTXT_INIT_SMASK);\n\tudelay(2);\n\tret = pio_init_wait_progress(dd);\n\tif (ret < 0) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"PIO send context init %s while initializing all PIO blocks\\n\",\n\t\t\t   ret == -ETIMEDOUT ? \"is stuck\" : \"had an error\");\n\t}\n}\n\n \nint sc_enable(struct send_context *sc)\n{\n\tu64 sc_ctrl, reg, pio;\n\tstruct hfi1_devdata *dd;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!sc)\n\t\treturn -EINVAL;\n\tdd = sc->dd;\n\n\t \n\tspin_lock_irqsave(&sc->alloc_lock, flags);\n\tsc_ctrl = read_kctxt_csr(dd, sc->hw_context, SC(CTRL));\n\tif ((sc_ctrl & SC(CTRL_CTXT_ENABLE_SMASK)))\n\t\tgoto unlock;  \n\n\t \n\n\t*sc->hw_free = 0;\n\tsc->free = 0;\n\tsc->alloc_free = 0;\n\tsc->fill = 0;\n\tsc->fill_wrap = 0;\n\tsc->sr_head = 0;\n\tsc->sr_tail = 0;\n\tsc->flags = 0;\n\t \n\treset_buffers_allocated(sc);\n\n\t \n\treg = read_kctxt_csr(dd, sc->hw_context, SC(ERR_STATUS));\n\tif (reg)\n\t\twrite_kctxt_csr(dd, sc->hw_context, SC(ERR_CLEAR), reg);\n\n\t \n\tspin_lock(&dd->sc_init_lock);\n\t \n\tpio = ((sc->hw_context & SEND_PIO_INIT_CTXT_PIO_CTXT_NUM_MASK) <<\n\t       SEND_PIO_INIT_CTXT_PIO_CTXT_NUM_SHIFT) |\n\t\tSEND_PIO_INIT_CTXT_PIO_SINGLE_CTXT_INIT_SMASK;\n\twrite_csr(dd, SEND_PIO_INIT_CTXT, pio);\n\t \n\tudelay(2);\n\tret = pio_init_wait_progress(dd);\n\tspin_unlock(&dd->sc_init_lock);\n\tif (ret) {\n\t\tdd_dev_err(dd,\n\t\t\t   \"sctxt%u(%u): Context not enabled due to init failure %d\\n\",\n\t\t\t   sc->sw_index, sc->hw_context, ret);\n\t\tgoto unlock;\n\t}\n\n\t \n\tsc_ctrl |= SC(CTRL_CTXT_ENABLE_SMASK);\n\twrite_kctxt_csr(dd, sc->hw_context, SC(CTRL), sc_ctrl);\n\t \n\tread_kctxt_csr(dd, sc->hw_context, SC(CTRL));\n\tsc->flags |= SCF_ENABLED;\n\nunlock:\n\tspin_unlock_irqrestore(&sc->alloc_lock, flags);\n\n\treturn ret;\n}\n\n \nvoid sc_return_credits(struct send_context *sc)\n{\n\tif (!sc)\n\t\treturn;\n\n\t \n\twrite_kctxt_csr(sc->dd, sc->hw_context, SC(CREDIT_FORCE),\n\t\t\tSC(CREDIT_FORCE_FORCE_RETURN_SMASK));\n\t \n\tread_kctxt_csr(sc->dd, sc->hw_context, SC(CREDIT_FORCE));\n\t \n\twrite_kctxt_csr(sc->dd, sc->hw_context, SC(CREDIT_FORCE), 0);\n}\n\n \nvoid sc_flush(struct send_context *sc)\n{\n\tif (!sc)\n\t\treturn;\n\n\tsc_wait_for_packet_egress(sc, 1);\n}\n\n \nvoid sc_drop(struct send_context *sc)\n{\n\tif (!sc)\n\t\treturn;\n\n\tdd_dev_info(sc->dd, \"%s: context %u(%u) - not implemented\\n\",\n\t\t    __func__, sc->sw_index, sc->hw_context);\n}\n\n \nvoid sc_stop(struct send_context *sc, int flag)\n{\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&sc->alloc_lock, flags);\n\t \n\tsc->flags |= flag;\n\tsc->flags &= ~SCF_ENABLED;\n\tspin_unlock_irqrestore(&sc->alloc_lock, flags);\n\twake_up(&sc->halt_wait);\n}\n\n#define BLOCK_DWORDS (PIO_BLOCK_SIZE / sizeof(u32))\n#define dwords_to_blocks(x) DIV_ROUND_UP(x, BLOCK_DWORDS)\n\n \nstruct pio_buf *sc_buffer_alloc(struct send_context *sc, u32 dw_len,\n\t\t\t\tpio_release_cb cb, void *arg)\n{\n\tstruct pio_buf *pbuf = NULL;\n\tunsigned long flags;\n\tunsigned long avail;\n\tunsigned long blocks = dwords_to_blocks(dw_len);\n\tu32 fill_wrap;\n\tint trycount = 0;\n\tu32 head, next;\n\n\tspin_lock_irqsave(&sc->alloc_lock, flags);\n\tif (!(sc->flags & SCF_ENABLED)) {\n\t\tspin_unlock_irqrestore(&sc->alloc_lock, flags);\n\t\treturn ERR_PTR(-ECOMM);\n\t}\n\nretry:\n\tavail = (unsigned long)sc->credits - (sc->fill - sc->alloc_free);\n\tif (blocks > avail) {\n\t\t \n\t\tif (unlikely(trycount))\t{  \n\t\t\tspin_unlock_irqrestore(&sc->alloc_lock, flags);\n\t\t\tgoto done;\n\t\t}\n\t\t \n\t\tsc->alloc_free = READ_ONCE(sc->free);\n\t\tavail =\n\t\t\t(unsigned long)sc->credits -\n\t\t\t(sc->fill - sc->alloc_free);\n\t\tif (blocks > avail) {\n\t\t\t \n\t\t\tsc_release_update(sc);\n\t\t\tsc->alloc_free = READ_ONCE(sc->free);\n\t\t\ttrycount++;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\t \n\n\tpreempt_disable();\n\tthis_cpu_inc(*sc->buffers_allocated);\n\n\t \n\thead = sc->sr_head;\n\n\t \n\tsc->fill += blocks;\n\tfill_wrap = sc->fill_wrap;\n\tsc->fill_wrap += blocks;\n\tif (sc->fill_wrap >= sc->credits)\n\t\tsc->fill_wrap = sc->fill_wrap - sc->credits;\n\n\t \n\tpbuf = &sc->sr[head].pbuf;\n\tpbuf->sent_at = sc->fill;\n\tpbuf->cb = cb;\n\tpbuf->arg = arg;\n\tpbuf->sc = sc;\t \n\t \n\n\t \n\tnext = head + 1;\n\tif (next >= sc->sr_size)\n\t\tnext = 0;\n\t \n\tsmp_wmb();\n\tsc->sr_head = next;\n\tspin_unlock_irqrestore(&sc->alloc_lock, flags);\n\n\t \n\tpbuf->start = sc->base_addr + fill_wrap * PIO_BLOCK_SIZE;\n\tpbuf->end = sc->base_addr + sc->size;\n\tpbuf->qw_written = 0;\n\tpbuf->carry_bytes = 0;\n\tpbuf->carry.val64 = 0;\ndone:\n\treturn pbuf;\n}\n\n \n\n \nvoid sc_add_credit_return_intr(struct send_context *sc)\n{\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&sc->credit_ctrl_lock, flags);\n\tif (sc->credit_intr_count == 0) {\n\t\tsc->credit_ctrl |= SC(CREDIT_CTRL_CREDIT_INTR_SMASK);\n\t\twrite_kctxt_csr(sc->dd, sc->hw_context,\n\t\t\t\tSC(CREDIT_CTRL), sc->credit_ctrl);\n\t}\n\tsc->credit_intr_count++;\n\tspin_unlock_irqrestore(&sc->credit_ctrl_lock, flags);\n}\n\n \nvoid sc_del_credit_return_intr(struct send_context *sc)\n{\n\tunsigned long flags;\n\n\tWARN_ON(sc->credit_intr_count == 0);\n\n\t \n\tspin_lock_irqsave(&sc->credit_ctrl_lock, flags);\n\tsc->credit_intr_count--;\n\tif (sc->credit_intr_count == 0) {\n\t\tsc->credit_ctrl &= ~SC(CREDIT_CTRL_CREDIT_INTR_SMASK);\n\t\twrite_kctxt_csr(sc->dd, sc->hw_context,\n\t\t\t\tSC(CREDIT_CTRL), sc->credit_ctrl);\n\t}\n\tspin_unlock_irqrestore(&sc->credit_ctrl_lock, flags);\n}\n\n \nvoid hfi1_sc_wantpiobuf_intr(struct send_context *sc, u32 needint)\n{\n\tif (needint)\n\t\tsc_add_credit_return_intr(sc);\n\telse\n\t\tsc_del_credit_return_intr(sc);\n\ttrace_hfi1_wantpiointr(sc, needint, sc->credit_ctrl);\n\tif (needint)\n\t\tsc_return_credits(sc);\n}\n\n \nstatic void sc_piobufavail(struct send_context *sc)\n{\n\tstruct hfi1_devdata *dd = sc->dd;\n\tstruct list_head *list;\n\tstruct rvt_qp *qps[PIO_WAIT_BATCH_SIZE];\n\tstruct rvt_qp *qp;\n\tstruct hfi1_qp_priv *priv;\n\tunsigned long flags;\n\tuint i, n = 0, top_idx = 0;\n\n\tif (dd->send_contexts[sc->sw_index].type != SC_KERNEL &&\n\t    dd->send_contexts[sc->sw_index].type != SC_VL15)\n\t\treturn;\n\tlist = &sc->piowait;\n\t \n\twrite_seqlock_irqsave(&sc->waitlock, flags);\n\twhile (!list_empty(list)) {\n\t\tstruct iowait *wait;\n\n\t\tif (n == ARRAY_SIZE(qps))\n\t\t\tbreak;\n\t\twait = list_first_entry(list, struct iowait, list);\n\t\tiowait_get_priority(wait);\n\t\tqp = iowait_to_qp(wait);\n\t\tpriv = qp->priv;\n\t\tlist_del_init(&priv->s_iowait.list);\n\t\tpriv->s_iowait.lock = NULL;\n\t\tif (n) {\n\t\t\tpriv = qps[top_idx]->priv;\n\t\t\ttop_idx = iowait_priority_update_top(wait,\n\t\t\t\t\t\t\t     &priv->s_iowait,\n\t\t\t\t\t\t\t     n, top_idx);\n\t\t}\n\n\t\t \n\t\tqps[n++] = qp;\n\t}\n\t \n\tif (n) {\n\t\thfi1_sc_wantpiobuf_intr(sc, 0);\n\t\tif (!list_empty(list))\n\t\t\thfi1_sc_wantpiobuf_intr(sc, 1);\n\t}\n\twrite_sequnlock_irqrestore(&sc->waitlock, flags);\n\n\t \n\tif (n)\n\t\thfi1_qp_wakeup(qps[top_idx],\n\t\t\t       RVT_S_WAIT_PIO | HFI1_S_WAIT_PIO_DRAIN);\n\tfor (i = 0; i < n; i++)\n\t\tif (i != top_idx)\n\t\t\thfi1_qp_wakeup(qps[i],\n\t\t\t\t       RVT_S_WAIT_PIO | HFI1_S_WAIT_PIO_DRAIN);\n}\n\n \nstatic inline int fill_code(u64 hw_free)\n{\n\tint code = 0;\n\n\tif (hw_free & CR_STATUS_SMASK)\n\t\tcode |= PRC_STATUS_ERR;\n\tif (hw_free & CR_CREDIT_RETURN_DUE_TO_PBC_SMASK)\n\t\tcode |= PRC_PBC;\n\tif (hw_free & CR_CREDIT_RETURN_DUE_TO_THRESHOLD_SMASK)\n\t\tcode |= PRC_THRESHOLD;\n\tif (hw_free & CR_CREDIT_RETURN_DUE_TO_ERR_SMASK)\n\t\tcode |= PRC_FILL_ERR;\n\tif (hw_free & CR_CREDIT_RETURN_DUE_TO_FORCE_SMASK)\n\t\tcode |= PRC_SC_DISABLE;\n\treturn code;\n}\n\n \n#define sent_before(a, b) time_before(a, b)\t \n\n \nvoid sc_release_update(struct send_context *sc)\n{\n\tstruct pio_buf *pbuf;\n\tu64 hw_free;\n\tu32 head, tail;\n\tunsigned long old_free;\n\tunsigned long free;\n\tunsigned long extra;\n\tunsigned long flags;\n\tint code;\n\n\tif (!sc)\n\t\treturn;\n\n\tspin_lock_irqsave(&sc->release_lock, flags);\n\t \n\thw_free = le64_to_cpu(*sc->hw_free);\t\t \n\told_free = sc->free;\n\textra = (((hw_free & CR_COUNTER_SMASK) >> CR_COUNTER_SHIFT)\n\t\t\t- (old_free & CR_COUNTER_MASK))\n\t\t\t\t& CR_COUNTER_MASK;\n\tfree = old_free + extra;\n\ttrace_hfi1_piofree(sc, extra);\n\n\t \n\tcode = -1;\t\t\t\t \n\thead = READ_ONCE(sc->sr_head);\t \n\ttail = sc->sr_tail;\n\twhile (head != tail) {\n\t\tpbuf = &sc->sr[tail].pbuf;\n\n\t\tif (sent_before(free, pbuf->sent_at)) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tif (pbuf->cb) {\n\t\t\tif (code < 0)  \n\t\t\t\tcode = fill_code(hw_free);\n\t\t\t(*pbuf->cb)(pbuf->arg, code);\n\t\t}\n\n\t\ttail++;\n\t\tif (tail >= sc->sr_size)\n\t\t\ttail = 0;\n\t}\n\tsc->sr_tail = tail;\n\t \n\tsmp_wmb();\n\tsc->free = free;\n\tspin_unlock_irqrestore(&sc->release_lock, flags);\n\tsc_piobufavail(sc);\n}\n\n \nvoid sc_group_release_update(struct hfi1_devdata *dd, u32 hw_context)\n{\n\tstruct send_context *sc;\n\tu32 sw_index;\n\tu32 gc, gc_end;\n\n\tspin_lock(&dd->sc_lock);\n\tsw_index = dd->hw_to_sw[hw_context];\n\tif (unlikely(sw_index >= dd->num_send_contexts)) {\n\t\tdd_dev_err(dd, \"%s: invalid hw (%u) to sw (%u) mapping\\n\",\n\t\t\t   __func__, hw_context, sw_index);\n\t\tgoto done;\n\t}\n\tsc = dd->send_contexts[sw_index].sc;\n\tif (unlikely(!sc))\n\t\tgoto done;\n\n\tgc = group_context(hw_context, sc->group);\n\tgc_end = gc + group_size(sc->group);\n\tfor (; gc < gc_end; gc++) {\n\t\tsw_index = dd->hw_to_sw[gc];\n\t\tif (unlikely(sw_index >= dd->num_send_contexts)) {\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"%s: invalid hw (%u) to sw (%u) mapping\\n\",\n\t\t\t\t   __func__, hw_context, sw_index);\n\t\t\tcontinue;\n\t\t}\n\t\tsc_release_update(dd->send_contexts[sw_index].sc);\n\t}\ndone:\n\tspin_unlock(&dd->sc_lock);\n}\n\n \nstruct send_context *pio_select_send_context_vl(struct hfi1_devdata *dd,\n\t\t\t\t\t\tu32 selector, u8 vl)\n{\n\tstruct pio_vl_map *m;\n\tstruct pio_map_elem *e;\n\tstruct send_context *rval;\n\n\t \n\tif (unlikely(vl >= num_vls)) {\n\t\trval = NULL;\n\t\tgoto done;\n\t}\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->pio_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn dd->vld[0].sc;\n\t}\n\te = m->map[vl & m->mask];\n\trval = e->ksc[selector & e->mask];\n\trcu_read_unlock();\n\ndone:\n\trval = !rval ? dd->vld[0].sc : rval;\n\treturn rval;\n}\n\n \nstruct send_context *pio_select_send_context_sc(struct hfi1_devdata *dd,\n\t\t\t\t\t\tu32 selector, u8 sc5)\n{\n\tu8 vl = sc_to_vlt(dd, sc5);\n\n\treturn pio_select_send_context_vl(dd, selector, vl);\n}\n\n \nstatic void pio_map_free(struct pio_vl_map *m)\n{\n\tint i;\n\n\tfor (i = 0; m && i < m->actual_vls; i++)\n\t\tkfree(m->map[i]);\n\tkfree(m);\n}\n\n \nstatic void pio_map_rcu_callback(struct rcu_head *list)\n{\n\tstruct pio_vl_map *m = container_of(list, struct pio_vl_map, list);\n\n\tpio_map_free(m);\n}\n\n \nstatic void set_threshold(struct hfi1_devdata *dd, int scontext, int i)\n{\n\tu32 thres;\n\n\tthres = min(sc_percent_to_threshold(dd->kernel_send_context[scontext],\n\t\t\t\t\t    50),\n\t\t    sc_mtu_to_threshold(dd->kernel_send_context[scontext],\n\t\t\t\t\tdd->vld[i].mtu,\n\t\t\t\t\tdd->rcd[0]->rcvhdrqentsize));\n\tsc_set_cr_threshold(dd->kernel_send_context[scontext], thres);\n}\n\n \nint pio_map_init(struct hfi1_devdata *dd, u8 port, u8 num_vls, u8 *vl_scontexts)\n{\n\tint i, j;\n\tint extra, sc_per_vl;\n\tint scontext = 1;\n\tint num_kernel_send_contexts = 0;\n\tu8 lvl_scontexts[OPA_MAX_VLS];\n\tstruct pio_vl_map *oldmap, *newmap;\n\n\tif (!vl_scontexts) {\n\t\tfor (i = 0; i < dd->num_send_contexts; i++)\n\t\t\tif (dd->send_contexts[i].type == SC_KERNEL)\n\t\t\t\tnum_kernel_send_contexts++;\n\t\t \n\t\tsc_per_vl = num_kernel_send_contexts / num_vls;\n\t\t \n\t\textra = num_kernel_send_contexts % num_vls;\n\t\tvl_scontexts = lvl_scontexts;\n\t\t \n\t\tfor (i = num_vls - 1; i >= 0; i--, extra--)\n\t\t\tvl_scontexts[i] = sc_per_vl + (extra > 0 ? 1 : 0);\n\t}\n\t \n\tnewmap = kzalloc(struct_size(newmap, map, roundup_pow_of_two(num_vls)),\n\t\t\t GFP_KERNEL);\n\tif (!newmap)\n\t\tgoto bail;\n\tnewmap->actual_vls = num_vls;\n\tnewmap->vls = roundup_pow_of_two(num_vls);\n\tnewmap->mask = (1 << ilog2(newmap->vls)) - 1;\n\tfor (i = 0; i < newmap->vls; i++) {\n\t\t \n\t\tint first_scontext = scontext;\n\n\t\tif (i < newmap->actual_vls) {\n\t\t\tint sz = roundup_pow_of_two(vl_scontexts[i]);\n\n\t\t\t \n\t\t\tnewmap->map[i] = kzalloc(struct_size(newmap->map[i],\n\t\t\t\t\t\t\t     ksc, sz),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (!newmap->map[i])\n\t\t\t\tgoto bail;\n\t\t\tnewmap->map[i]->mask = (1 << ilog2(sz)) - 1;\n\t\t\t \n\t\t\tfor (j = 0; j < sz; j++) {\n\t\t\t\tif (dd->kernel_send_context[scontext]) {\n\t\t\t\t\tnewmap->map[i]->ksc[j] =\n\t\t\t\t\tdd->kernel_send_context[scontext];\n\t\t\t\t\tset_threshold(dd, scontext, i);\n\t\t\t\t}\n\t\t\t\tif (++scontext >= first_scontext +\n\t\t\t\t\t\t  vl_scontexts[i])\n\t\t\t\t\t \n\t\t\t\t\tscontext = first_scontext;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tnewmap->map[i] = newmap->map[i % num_vls];\n\t\t}\n\t\tscontext = first_scontext + vl_scontexts[i];\n\t}\n\t \n\tspin_lock_irq(&dd->pio_map_lock);\n\toldmap = rcu_dereference_protected(dd->pio_map,\n\t\t\t\t\t   lockdep_is_held(&dd->pio_map_lock));\n\n\t \n\trcu_assign_pointer(dd->pio_map, newmap);\n\n\tspin_unlock_irq(&dd->pio_map_lock);\n\t \n\tif (oldmap)\n\t\tcall_rcu(&oldmap->list, pio_map_rcu_callback);\n\treturn 0;\nbail:\n\t \n\tpio_map_free(newmap);\n\treturn -ENOMEM;\n}\n\nvoid free_pio_map(struct hfi1_devdata *dd)\n{\n\t \n\tif (rcu_access_pointer(dd->pio_map)) {\n\t\tspin_lock_irq(&dd->pio_map_lock);\n\t\tpio_map_free(rcu_access_pointer(dd->pio_map));\n\t\tRCU_INIT_POINTER(dd->pio_map, NULL);\n\t\tspin_unlock_irq(&dd->pio_map_lock);\n\t\tsynchronize_rcu();\n\t}\n\tkfree(dd->kernel_send_context);\n\tdd->kernel_send_context = NULL;\n}\n\nint init_pervl_scs(struct hfi1_devdata *dd)\n{\n\tint i;\n\tu64 mask, all_vl_mask = (u64)0x80ff;  \n\tu64 data_vls_mask = (u64)0x00ff;  \n\tu32 ctxt;\n\tstruct hfi1_pportdata *ppd = dd->pport;\n\n\tdd->vld[15].sc = sc_alloc(dd, SC_VL15,\n\t\t\t\t  dd->rcd[0]->rcvhdrqentsize, dd->node);\n\tif (!dd->vld[15].sc)\n\t\treturn -ENOMEM;\n\n\thfi1_init_ctxt(dd->vld[15].sc);\n\tdd->vld[15].mtu = enum_to_mtu(OPA_MTU_2048);\n\n\tdd->kernel_send_context = kcalloc_node(dd->num_send_contexts,\n\t\t\t\t\t       sizeof(struct send_context *),\n\t\t\t\t\t       GFP_KERNEL, dd->node);\n\tif (!dd->kernel_send_context)\n\t\tgoto freesc15;\n\n\tdd->kernel_send_context[0] = dd->vld[15].sc;\n\n\tfor (i = 0; i < num_vls; i++) {\n\t\t \n\t\tdd->vld[i].sc = sc_alloc(dd, SC_KERNEL,\n\t\t\t\t\t dd->rcd[0]->rcvhdrqentsize, dd->node);\n\t\tif (!dd->vld[i].sc)\n\t\t\tgoto nomem;\n\t\tdd->kernel_send_context[i + 1] = dd->vld[i].sc;\n\t\thfi1_init_ctxt(dd->vld[i].sc);\n\t\t \n\t\tdd->vld[i].mtu = hfi1_max_mtu;\n\t}\n\tfor (i = num_vls; i < INIT_SC_PER_VL * num_vls; i++) {\n\t\tdd->kernel_send_context[i + 1] =\n\t\tsc_alloc(dd, SC_KERNEL, dd->rcd[0]->rcvhdrqentsize, dd->node);\n\t\tif (!dd->kernel_send_context[i + 1])\n\t\t\tgoto nomem;\n\t\thfi1_init_ctxt(dd->kernel_send_context[i + 1]);\n\t}\n\n\tsc_enable(dd->vld[15].sc);\n\tctxt = dd->vld[15].sc->hw_context;\n\tmask = all_vl_mask & ~(1LL << 15);\n\twrite_kctxt_csr(dd, ctxt, SC(CHECK_VL), mask);\n\tdd_dev_info(dd,\n\t\t    \"Using send context %u(%u) for VL15\\n\",\n\t\t    dd->vld[15].sc->sw_index, ctxt);\n\n\tfor (i = 0; i < num_vls; i++) {\n\t\tsc_enable(dd->vld[i].sc);\n\t\tctxt = dd->vld[i].sc->hw_context;\n\t\tmask = all_vl_mask & ~(data_vls_mask);\n\t\twrite_kctxt_csr(dd, ctxt, SC(CHECK_VL), mask);\n\t}\n\tfor (i = num_vls; i < INIT_SC_PER_VL * num_vls; i++) {\n\t\tsc_enable(dd->kernel_send_context[i + 1]);\n\t\tctxt = dd->kernel_send_context[i + 1]->hw_context;\n\t\tmask = all_vl_mask & ~(data_vls_mask);\n\t\twrite_kctxt_csr(dd, ctxt, SC(CHECK_VL), mask);\n\t}\n\n\tif (pio_map_init(dd, ppd->port - 1, num_vls, NULL))\n\t\tgoto nomem;\n\treturn 0;\n\nnomem:\n\tfor (i = 0; i < num_vls; i++) {\n\t\tsc_free(dd->vld[i].sc);\n\t\tdd->vld[i].sc = NULL;\n\t}\n\n\tfor (i = num_vls; i < INIT_SC_PER_VL * num_vls; i++)\n\t\tsc_free(dd->kernel_send_context[i + 1]);\n\n\tkfree(dd->kernel_send_context);\n\tdd->kernel_send_context = NULL;\n\nfreesc15:\n\tsc_free(dd->vld[15].sc);\n\treturn -ENOMEM;\n}\n\nint init_credit_return(struct hfi1_devdata *dd)\n{\n\tint ret;\n\tint i;\n\n\tdd->cr_base = kcalloc(\n\t\tnode_affinity.num_possible_nodes,\n\t\tsizeof(struct credit_return_base),\n\t\tGFP_KERNEL);\n\tif (!dd->cr_base) {\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\tfor_each_node_with_cpus(i) {\n\t\tint bytes = TXE_NUM_CONTEXTS * sizeof(struct credit_return);\n\n\t\tset_dev_node(&dd->pcidev->dev, i);\n\t\tdd->cr_base[i].va = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\t       bytes,\n\t\t\t\t\t\t       &dd->cr_base[i].dma,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!dd->cr_base[i].va) {\n\t\t\tset_dev_node(&dd->pcidev->dev, dd->node);\n\t\t\tdd_dev_err(dd,\n\t\t\t\t   \"Unable to allocate credit return DMA range for NUMA %d\\n\",\n\t\t\t\t   i);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto done;\n\t\t}\n\t}\n\tset_dev_node(&dd->pcidev->dev, dd->node);\n\n\tret = 0;\ndone:\n\treturn ret;\n}\n\nvoid free_credit_return(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tif (!dd->cr_base)\n\t\treturn;\n\tfor (i = 0; i < node_affinity.num_possible_nodes; i++) {\n\t\tif (dd->cr_base[i].va) {\n\t\t\tdma_free_coherent(&dd->pcidev->dev,\n\t\t\t\t\t  TXE_NUM_CONTEXTS *\n\t\t\t\t\t  sizeof(struct credit_return),\n\t\t\t\t\t  dd->cr_base[i].va,\n\t\t\t\t\t  dd->cr_base[i].dma);\n\t\t}\n\t}\n\tkfree(dd->cr_base);\n\tdd->cr_base = NULL;\n}\n\nvoid seqfile_dump_sci(struct seq_file *s, u32 i,\n\t\t      struct send_context_info *sci)\n{\n\tstruct send_context *sc = sci->sc;\n\tu64 reg;\n\n\tseq_printf(s, \"SCI %u: type %u base %u credits %u\\n\",\n\t\t   i, sci->type, sci->base, sci->credits);\n\tseq_printf(s, \"  flags 0x%x sw_inx %u hw_ctxt %u grp %u\\n\",\n\t\t   sc->flags,  sc->sw_index, sc->hw_context, sc->group);\n\tseq_printf(s, \"  sr_size %u credits %u sr_head %u sr_tail %u\\n\",\n\t\t   sc->sr_size, sc->credits, sc->sr_head, sc->sr_tail);\n\tseq_printf(s, \"  fill %lu free %lu fill_wrap %u alloc_free %lu\\n\",\n\t\t   sc->fill, sc->free, sc->fill_wrap, sc->alloc_free);\n\tseq_printf(s, \"  credit_intr_count %u credit_ctrl 0x%llx\\n\",\n\t\t   sc->credit_intr_count, sc->credit_ctrl);\n\treg = read_kctxt_csr(sc->dd, sc->hw_context, SC(CREDIT_STATUS));\n\tseq_printf(s, \"  *hw_free %llu CurrentFree %llu LastReturned %llu\\n\",\n\t\t   (le64_to_cpu(*sc->hw_free) & CR_COUNTER_SMASK) >>\n\t\t    CR_COUNTER_SHIFT,\n\t\t   (reg >> SC(CREDIT_STATUS_CURRENT_FREE_COUNTER_SHIFT)) &\n\t\t    SC(CREDIT_STATUS_CURRENT_FREE_COUNTER_MASK),\n\t\t   reg & SC(CREDIT_STATUS_LAST_RETURNED_COUNTER_SMASK));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}