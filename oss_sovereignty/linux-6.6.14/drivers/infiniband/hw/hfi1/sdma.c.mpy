{
  "module_name": "sdma.c",
  "hash_id": "53675fa67deec681f13e6421f2bc8b93fadf46d0e5cfe7e80c2f3825ee9fc386",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/sdma.c",
  "human_readable_source": "\n \n\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/netdevice.h>\n#include <linux/moduleparam.h>\n#include <linux/bitops.h>\n#include <linux/timer.h>\n#include <linux/vmalloc.h>\n#include <linux/highmem.h>\n\n#include \"hfi.h\"\n#include \"common.h\"\n#include \"qp.h\"\n#include \"sdma.h\"\n#include \"iowait.h\"\n#include \"trace.h\"\n\n \n#define SDMA_DESCQ_CNT 2048\n#define SDMA_DESC_INTR 64\n#define INVALID_TAIL 0xffff\n#define SDMA_PAD max_t(size_t, MAX_16B_PADDING, sizeof(u32))\n\nstatic uint sdma_descq_cnt = SDMA_DESCQ_CNT;\nmodule_param(sdma_descq_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_descq_cnt, \"Number of SDMA descq entries\");\n\nstatic uint sdma_idle_cnt = 250;\nmodule_param(sdma_idle_cnt, uint, S_IRUGO);\nMODULE_PARM_DESC(sdma_idle_cnt, \"sdma interrupt idle delay (ns,default 250)\");\n\nuint mod_num_sdma;\nmodule_param_named(num_sdma, mod_num_sdma, uint, S_IRUGO);\nMODULE_PARM_DESC(num_sdma, \"Set max number SDMA engines to use\");\n\nstatic uint sdma_desct_intr = SDMA_DESC_INTR;\nmodule_param_named(desct_intr, sdma_desct_intr, uint, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(desct_intr, \"Number of SDMA descriptor before interrupt\");\n\n#define SDMA_WAIT_BATCH_SIZE 20\n \n#define SDMA_ERR_HALT_TIMEOUT 10  \n \n\n#define SD(name) SEND_DMA_##name\n#define ALL_SDMA_ENG_HALT_ERRS \\\n\t(SD(ENG_ERR_STATUS_SDMA_WRONG_DW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_GEN_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TOO_LONG_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TAIL_OUT_OF_BOUNDS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_FIRST_DESC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_MEM_READ_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_LENGTH_MISMATCH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_DESC_OVERFLOW_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_SELECT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_ADDRESS_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_LENGTH_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_TIMEOUT_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_DESC_TABLE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_ASSEMBLY_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_PACKET_TRACKING_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_STORAGE_UNC_ERR_SMASK) \\\n\t| SD(ENG_ERR_STATUS_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SMASK))\n\n \n#define SDMA_SENDCTRL_OP_ENABLE    BIT(0)\n#define SDMA_SENDCTRL_OP_INTENABLE BIT(1)\n#define SDMA_SENDCTRL_OP_HALT      BIT(2)\n#define SDMA_SENDCTRL_OP_CLEANUP   BIT(3)\n\n \n#define SDMA_EGRESS_PACKET_OCCUPANCY_SMASK \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SMASK\n#define SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT \\\nSEND_EGRESS_SEND_DMA_STATUS_SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT\n\nstatic const char * const sdma_state_names[] = {\n\t[sdma_state_s00_hw_down]                = \"s00_HwDown\",\n\t[sdma_state_s10_hw_start_up_halt_wait]  = \"s10_HwStartUpHaltWait\",\n\t[sdma_state_s15_hw_start_up_clean_wait] = \"s15_HwStartUpCleanWait\",\n\t[sdma_state_s20_idle]                   = \"s20_Idle\",\n\t[sdma_state_s30_sw_clean_up_wait]       = \"s30_SwCleanUpWait\",\n\t[sdma_state_s40_hw_clean_up_wait]       = \"s40_HwCleanUpWait\",\n\t[sdma_state_s50_hw_halt_wait]           = \"s50_HwHaltWait\",\n\t[sdma_state_s60_idle_halt_wait]         = \"s60_IdleHaltWait\",\n\t[sdma_state_s80_hw_freeze]\t\t= \"s80_HwFreeze\",\n\t[sdma_state_s82_freeze_sw_clean]\t= \"s82_FreezeSwClean\",\n\t[sdma_state_s99_running]                = \"s99_Running\",\n};\n\n#ifdef CONFIG_SDMA_VERBOSITY\nstatic const char * const sdma_event_names[] = {\n\t[sdma_event_e00_go_hw_down]   = \"e00_GoHwDown\",\n\t[sdma_event_e10_go_hw_start]  = \"e10_GoHwStart\",\n\t[sdma_event_e15_hw_halt_done] = \"e15_HwHaltDone\",\n\t[sdma_event_e25_hw_clean_up_done] = \"e25_HwCleanUpDone\",\n\t[sdma_event_e30_go_running]   = \"e30_GoRunning\",\n\t[sdma_event_e40_sw_cleaned]   = \"e40_SwCleaned\",\n\t[sdma_event_e50_hw_cleaned]   = \"e50_HwCleaned\",\n\t[sdma_event_e60_hw_halted]    = \"e60_HwHalted\",\n\t[sdma_event_e70_go_idle]      = \"e70_GoIdle\",\n\t[sdma_event_e80_hw_freeze]    = \"e80_HwFreeze\",\n\t[sdma_event_e81_hw_frozen]    = \"e81_HwFrozen\",\n\t[sdma_event_e82_hw_unfreeze]  = \"e82_HwUnfreeze\",\n\t[sdma_event_e85_link_down]    = \"e85_LinkDown\",\n\t[sdma_event_e90_sw_halted]    = \"e90_SwHalted\",\n};\n#endif\n\nstatic const struct sdma_set_state_action sdma_action_table[] = {\n\t[sdma_state_s00_hw_down] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s10_hw_start_up_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s15_hw_start_up_clean_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s20_idle] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s30_sw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s40_hw_clean_up_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 1,\n\t},\n\t[sdma_state_s50_hw_halt_wait] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s60_idle_halt_wait] = {\n\t\t.go_s99_running_tofalse = 1,\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 1,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s80_hw_freeze] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s82_freeze_sw_clean] = {\n\t\t.op_enable = 0,\n\t\t.op_intenable = 0,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t},\n\t[sdma_state_s99_running] = {\n\t\t.op_enable = 1,\n\t\t.op_intenable = 1,\n\t\t.op_halt = 0,\n\t\t.op_cleanup = 0,\n\t\t.go_s99_running_totrue = 1,\n\t},\n};\n\n#define SDMA_TAIL_UPDATE_THRESH 0x1F\n\n \nstatic void sdma_complete(struct kref *);\nstatic void sdma_finalput(struct sdma_state *);\nstatic void sdma_get(struct sdma_state *);\nstatic void sdma_hw_clean_up_task(struct tasklet_struct *);\nstatic void sdma_put(struct sdma_state *);\nstatic void sdma_set_state(struct sdma_engine *, enum sdma_states);\nstatic void sdma_start_hw_clean_up(struct sdma_engine *);\nstatic void sdma_sw_clean_up_task(struct tasklet_struct *);\nstatic void sdma_sendctrl(struct sdma_engine *, unsigned);\nstatic void init_sdma_regs(struct sdma_engine *, u32, uint);\nstatic void sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void __sdma_process_event(\n\tstruct sdma_engine *sde,\n\tenum sdma_events event);\nstatic void dump_sdma_state(struct sdma_engine *sde);\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status);\nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail);\nstatic void sdma_flush_descq(struct sdma_engine *sde);\n\n \nstatic const char *sdma_state_name(enum sdma_states state)\n{\n\treturn sdma_state_names[state];\n}\n\nstatic void sdma_get(struct sdma_state *ss)\n{\n\tkref_get(&ss->kref);\n}\n\nstatic void sdma_complete(struct kref *kref)\n{\n\tstruct sdma_state *ss =\n\t\tcontainer_of(kref, struct sdma_state, kref);\n\n\tcomplete(&ss->comp);\n}\n\nstatic void sdma_put(struct sdma_state *ss)\n{\n\tkref_put(&ss->kref, sdma_complete);\n}\n\nstatic void sdma_finalput(struct sdma_state *ss)\n{\n\tsdma_put(ss);\n\twait_for_completion(&ss->comp);\n}\n\nstatic inline void write_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0,\n\tu64 value)\n{\n\twrite_kctxt_csr(sde->dd, sde->this_idx, offset0, value);\n}\n\nstatic inline u64 read_sde_csr(\n\tstruct sdma_engine *sde,\n\tu32 offset0)\n{\n\treturn read_kctxt_csr(sde->dd, sde->this_idx, offset0);\n}\n\n \nstatic void sdma_wait_for_packet_egress(struct sdma_engine *sde,\n\t\t\t\t\tint pause)\n{\n\tu64 off = 8 * sde->this_idx;\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint lcnt = 0;\n\tu64 reg_prev;\n\tu64 reg = 0;\n\n\twhile (1) {\n\t\treg_prev = reg;\n\t\treg = read_csr(dd, off + SEND_EGRESS_SEND_DMA_STATUS);\n\n\t\treg &= SDMA_EGRESS_PACKET_OCCUPANCY_SMASK;\n\t\treg >>= SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT;\n\t\tif (reg == 0)\n\t\t\tbreak;\n\t\t \n\t\tif (reg != reg_prev)\n\t\t\tlcnt = 0;\n\t\tif (lcnt++ > 500) {\n\t\t\t \n\t\t\tdd_dev_err(dd, \"%s: engine %u timeout waiting for packets to egress, remaining count %u, bouncing link\\n\",\n\t\t\t\t   __func__, sde->this_idx, (u32)reg);\n\t\t\tqueue_work(dd->pport->link_wq,\n\t\t\t\t   &dd->pport->link_bounce_work);\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n}\n\n \nvoid sdma_wait(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\tstruct sdma_engine *sde = &dd->per_sdma[i];\n\n\t\tsdma_wait_for_packet_egress(sde, 0);\n\t}\n}\n\nstatic inline void sdma_set_desc_cnt(struct sdma_engine *sde, unsigned cnt)\n{\n\tu64 reg;\n\n\tif (!(sde->dd->flags & HFI1_HAS_SDMA_TIMEOUT))\n\t\treturn;\n\treg = cnt;\n\treg &= SD(DESC_CNT_CNT_MASK);\n\treg <<= SD(DESC_CNT_CNT_SHIFT);\n\twrite_sde_csr(sde, SD(DESC_CNT), reg);\n}\n\nstatic inline void complete_tx(struct sdma_engine *sde,\n\t\t\t       struct sdma_txreq *tx,\n\t\t\t       int res)\n{\n\t \n\tstruct iowait *wait = tx->wait;\n\tcallback_t complete = tx->complete;\n\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttrace_hfi1_sdma_out_sn(sde, tx->sn);\n\tif (WARN_ON_ONCE(sde->head_sn != tx->sn))\n\t\tdd_dev_err(sde->dd, \"expected %llu got %llu\\n\",\n\t\t\t   sde->head_sn, tx->sn);\n\tsde->head_sn++;\n#endif\n\t__sdma_txclean(sde->dd, tx);\n\tif (complete)\n\t\t(*complete)(tx, res);\n\tif (iowait_sdma_dec(wait))\n\t\tiowait_drain_wakeup(wait);\n}\n\n \nstatic void sdma_flush(struct sdma_engine *sde)\n{\n\tstruct sdma_txreq *txp, *txp_next;\n\tLIST_HEAD(flushlist);\n\tunsigned long flags;\n\tuint seq;\n\n\t \n\tsdma_flush_descq(sde);\n\tspin_lock_irqsave(&sde->flushlist_lock, flags);\n\t \n\tlist_splice_init(&sde->flushlist, &flushlist);\n\tspin_unlock_irqrestore(&sde->flushlist_lock, flags);\n\t \n\tlist_for_each_entry_safe(txp, txp_next, &flushlist, list)\n\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t \n\tdo {\n\t\tstruct iowait *w, *nw;\n\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\tlist_for_each_entry_safe(w, nw, &sde->dmawait, list) {\n\t\t\t\tif (w->wakeup) {\n\t\t\t\t\tw->wakeup(w, SDMA_AVAIL_REASON);\n\t\t\t\t\tlist_del_init(&w->list);\n\t\t\t\t}\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n}\n\n \nstatic void sdma_field_flush(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct sdma_engine *sde =\n\t\tcontainer_of(work, struct sdma_engine, flush_worker);\n\n\twrite_seqlock_irqsave(&sde->head_lock, flags);\n\tif (!__sdma_running(sde))\n\t\tsdma_flush(sde);\n\twrite_sequnlock_irqrestore(&sde->head_lock, flags);\n}\n\nstatic void sdma_err_halt_wait(struct work_struct *work)\n{\n\tstruct sdma_engine *sde = container_of(work, struct sdma_engine,\n\t\t\t\t\t\terr_halt_worker);\n\tu64 statuscsr;\n\tunsigned long timeout;\n\n\ttimeout = jiffies + msecs_to_jiffies(SDMA_ERR_HALT_TIMEOUT);\n\twhile (1) {\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_HALTED_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"SDMA engine %d - timeout waiting for engine to halt\\n\",\n\t\t\t\t   sde->this_idx);\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tusleep_range(80, 120);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e15_hw_halt_done);\n}\n\nstatic void sdma_err_progress_check_schedule(struct sdma_engine *sde)\n{\n\tif (!is_bx(sde->dd) && HFI1_CAP_IS_KSET(SDMA_AHG)) {\n\t\tunsigned index;\n\t\tstruct hfi1_devdata *dd = sde->dd;\n\n\t\tfor (index = 0; index < dd->num_sdma; index++) {\n\t\t\tstruct sdma_engine *curr_sdma = &dd->per_sdma[index];\n\n\t\t\tif (curr_sdma != sde)\n\t\t\t\tcurr_sdma->progress_check_head =\n\t\t\t\t\t\t\tcurr_sdma->descq_head;\n\t\t}\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA engine %d - check scheduled\\n\",\n\t\t\t\tsde->this_idx);\n\t\tmod_timer(&sde->err_progress_check_timer, jiffies + 10);\n\t}\n}\n\nstatic void sdma_err_progress_check(struct timer_list *t)\n{\n\tunsigned index;\n\tstruct sdma_engine *sde = from_timer(sde, t, err_progress_check_timer);\n\n\tdd_dev_err(sde->dd, \"SDE progress check event\\n\");\n\tfor (index = 0; index < sde->dd->num_sdma; index++) {\n\t\tstruct sdma_engine *curr_sde = &sde->dd->per_sdma[index];\n\t\tunsigned long flags;\n\n\t\t \n\t\tif (curr_sde == sde)\n\t\t\tcontinue;\n\t\t \n\t\tspin_lock_irqsave(&curr_sde->tail_lock, flags);\n\t\twrite_seqlock(&curr_sde->head_lock);\n\n\t\t \n\t\tif (curr_sde->state.current_state != sdma_state_s99_running) {\n\t\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((curr_sde->descq_head != curr_sde->descq_tail) &&\n\t\t    (curr_sde->descq_head ==\n\t\t\t\tcurr_sde->progress_check_head))\n\t\t\t__sdma_process_event(curr_sde,\n\t\t\t\t\t     sdma_event_e90_sw_halted);\n\t\twrite_sequnlock(&curr_sde->head_lock);\n\t\tspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\n\t}\n\tschedule_work(&sde->err_halt_worker);\n}\n\nstatic void sdma_hw_clean_up_task(struct tasklet_struct *t)\n{\n\tstruct sdma_engine *sde = from_tasklet(sde, t,\n\t\t\t\t\t       sdma_hw_clean_up_task);\n\tu64 statuscsr;\n\n\twhile (1) {\n#ifdef CONFIG_SDMA_VERBOSITY\n\t\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__,\n\t\t\t__func__);\n#endif\n\t\tstatuscsr = read_sde_csr(sde, SD(STATUS));\n\t\tstatuscsr &= SD(STATUS_ENG_CLEANED_UP_SMASK);\n\t\tif (statuscsr)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\tsdma_process_event(sde, sdma_event_e25_hw_clean_up_done);\n}\n\nstatic inline struct sdma_txreq *get_txhead(struct sdma_engine *sde)\n{\n\treturn sde->tx_ring[sde->tx_head & sde->sdma_mask];\n}\n\n \nstatic void sdma_flush_descq(struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tint progress = 0;\n\tstruct sdma_txreq *txp = get_txhead(sde);\n\n\t \n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\twhile (head != tail) {\n\t\t \n\t\thead = ++sde->descq_head & sde->sdma_mask;\n\t\t \n\t\tif (txp && txp->next_descq_idx == head) {\n\t\t\t \n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\n\t\t\ttrace_hfi1_sdma_progress(sde, head, tail, txp);\n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\tprogress++;\n\t}\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\nstatic void sdma_sw_clean_up_task(struct tasklet_struct *t)\n{\n\tstruct sdma_engine *sde = from_tasklet(sde, t, sdma_sw_clean_up_task);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t \n\n\t \n\tsdma_make_progress(sde, 0);\n\n\tsdma_flush(sde);\n\n\t \n\tsde->descq_tail = 0;\n\tsde->descq_head = 0;\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t*sde->head_dma = 0;\n\n\t__sdma_process_event(sde, sdma_event_e40_sw_cleaned);\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sw_tear_down(struct sdma_engine *sde)\n{\n\tstruct sdma_state *ss = &sde->state;\n\n\t \n\tsdma_put(ss);\n\n\t \n\tatomic_set(&sde->dd->sdma_unfreeze_count, -1);\n\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n}\n\nstatic void sdma_start_hw_clean_up(struct sdma_engine *sde)\n{\n\ttasklet_hi_schedule(&sde->sdma_hw_clean_up_task);\n}\n\nstatic void sdma_set_state(struct sdma_engine *sde,\n\t\t\t   enum sdma_states next_state)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tconst struct sdma_set_state_action *action = sdma_action_table;\n\tunsigned op = 0;\n\n\ttrace_hfi1_sdma_state(\n\t\tsde,\n\t\tsdma_state_names[ss->current_state],\n\t\tsdma_state_names[next_state]);\n\n\t \n\tss->previous_state = ss->current_state;\n\tss->previous_op = ss->current_op;\n\tss->current_state = next_state;\n\n\tif (ss->previous_state != sdma_state_s99_running &&\n\t    next_state == sdma_state_s99_running)\n\t\tsdma_flush(sde);\n\n\tif (action[next_state].op_enable)\n\t\top |= SDMA_SENDCTRL_OP_ENABLE;\n\n\tif (action[next_state].op_intenable)\n\t\top |= SDMA_SENDCTRL_OP_INTENABLE;\n\n\tif (action[next_state].op_halt)\n\t\top |= SDMA_SENDCTRL_OP_HALT;\n\n\tif (action[next_state].op_cleanup)\n\t\top |= SDMA_SENDCTRL_OP_CLEANUP;\n\n\tif (action[next_state].go_s99_running_tofalse)\n\t\tss->go_s99_running = 0;\n\n\tif (action[next_state].go_s99_running_totrue)\n\t\tss->go_s99_running = 1;\n\n\tss->current_op = op;\n\tsdma_sendctrl(sde, ss->current_op);\n}\n\n \nu16 sdma_get_descq_cnt(void)\n{\n\tu16 count = sdma_descq_cnt;\n\n\tif (!count)\n\t\treturn SDMA_DESCQ_CNT;\n\t \n\tif (!is_power_of_2(count))\n\t\treturn SDMA_DESCQ_CNT;\n\tif (count < 64 || count > 32768)\n\t\treturn SDMA_DESCQ_CNT;\n\treturn count;\n}\n\n \nint sdma_engine_get_vl(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tstruct sdma_vl_map *m;\n\tu8 vl;\n\n\tif (sde->this_idx >= TXE_NUM_SDMA_ENGINES)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\tvl = m->engine_to_vl[sde->this_idx];\n\trcu_read_unlock();\n\n\treturn vl;\n}\n\n \nstruct sdma_engine *sdma_select_engine_vl(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 vl)\n{\n\tstruct sdma_vl_map *m;\n\tstruct sdma_map_elem *e;\n\tstruct sdma_engine *rval;\n\n\t \n\tif (vl >= num_vls) {\n\t\trval = NULL;\n\t\tgoto done;\n\t}\n\n\trcu_read_lock();\n\tm = rcu_dereference(dd->sdma_map);\n\tif (unlikely(!m)) {\n\t\trcu_read_unlock();\n\t\treturn &dd->per_sdma[0];\n\t}\n\te = m->map[vl & m->mask];\n\trval = e->sde[selector & e->mask];\n\trcu_read_unlock();\n\ndone:\n\trval =  !rval ? &dd->per_sdma[0] : rval;\n\ttrace_hfi1_sdma_engine_select(dd, selector, vl, rval->this_idx);\n\treturn rval;\n}\n\n \nstruct sdma_engine *sdma_select_engine_sc(\n\tstruct hfi1_devdata *dd,\n\tu32 selector,\n\tu8 sc5)\n{\n\tu8 vl = sc_to_vlt(dd, sc5);\n\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstruct sdma_rht_map_elem {\n\tu32 mask;\n\tu8 ctr;\n\tstruct sdma_engine *sde[];\n};\n\nstruct sdma_rht_node {\n\tunsigned long cpu_id;\n\tstruct sdma_rht_map_elem *map[HFI1_MAX_VLS_SUPPORTED];\n\tstruct rhash_head node;\n};\n\n#define NR_CPUS_HINT 192\n\nstatic const struct rhashtable_params sdma_rht_params = {\n\t.nelem_hint = NR_CPUS_HINT,\n\t.head_offset = offsetof(struct sdma_rht_node, node),\n\t.key_offset = offsetof(struct sdma_rht_node, cpu_id),\n\t.key_len = sizeof_field(struct sdma_rht_node, cpu_id),\n\t.max_size = NR_CPUS,\n\t.min_size = 8,\n\t.automatic_shrinking = true,\n};\n\n \nstruct sdma_engine *sdma_select_user_engine(struct hfi1_devdata *dd,\n\t\t\t\t\t    u32 selector, u8 vl)\n{\n\tstruct sdma_rht_node *rht_node;\n\tstruct sdma_engine *sde = NULL;\n\tunsigned long cpu_id;\n\n\t \n\tif (current->nr_cpus_allowed != 1)\n\t\tgoto out;\n\n\trcu_read_lock();\n\tcpu_id = smp_processor_id();\n\trht_node = rhashtable_lookup(dd->sdma_rht, &cpu_id,\n\t\t\t\t     sdma_rht_params);\n\n\tif (rht_node && rht_node->map[vl]) {\n\t\tstruct sdma_rht_map_elem *map = rht_node->map[vl];\n\n\t\tsde = map->sde[selector & map->mask];\n\t}\n\trcu_read_unlock();\n\n\tif (sde)\n\t\treturn sde;\n\nout:\n\treturn sdma_select_engine_vl(dd, selector, vl);\n}\n\nstatic void sdma_populate_sde_map(struct sdma_rht_map_elem *map)\n{\n\tint i;\n\n\tfor (i = 0; i < roundup_pow_of_two(map->ctr ? : 1) - map->ctr; i++)\n\t\tmap->sde[map->ctr + i] = map->sde[i];\n}\n\nstatic void sdma_cleanup_sde_map(struct sdma_rht_map_elem *map,\n\t\t\t\t struct sdma_engine *sde)\n{\n\tunsigned int i, pow;\n\n\t \n\tfor (i = 0; i < map->ctr; i++) {\n\t\tif (map->sde[i] == sde) {\n\t\t\tmemmove(&map->sde[i], &map->sde[i + 1],\n\t\t\t\t(map->ctr - i - 1) * sizeof(map->sde[0]));\n\t\t\tmap->ctr--;\n\t\t\tpow = roundup_pow_of_two(map->ctr ? : 1);\n\t\t\tmap->mask = pow - 1;\n\t\t\tsdma_populate_sde_map(map);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic DEFINE_MUTEX(process_to_sde_mutex);\n\nssize_t sdma_set_cpu_to_sde_map(struct sdma_engine *sde, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tcpumask_var_t mask, new_mask;\n\tunsigned long cpu;\n\tint ret, vl, sz;\n\tstruct sdma_rht_node *rht_node;\n\n\tvl = sdma_engine_get_vl(sde);\n\tif (unlikely(vl < 0 || vl >= ARRAY_SIZE(rht_node->map)))\n\t\treturn -EINVAL;\n\n\tret = zalloc_cpumask_var(&mask, GFP_KERNEL);\n\tif (!ret)\n\t\treturn -ENOMEM;\n\n\tret = zalloc_cpumask_var(&new_mask, GFP_KERNEL);\n\tif (!ret) {\n\t\tfree_cpumask_var(mask);\n\t\treturn -ENOMEM;\n\t}\n\tret = cpulist_parse(buf, mask);\n\tif (ret)\n\t\tgoto out_free;\n\n\tif (!cpumask_subset(mask, cpu_online_mask)) {\n\t\tdd_dev_warn(sde->dd, \"Invalid CPU mask\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tsz = sizeof(struct sdma_rht_map_elem) +\n\t\t\t(TXE_NUM_SDMA_ENGINES * sizeof(struct sdma_engine *));\n\n\tmutex_lock(&process_to_sde_mutex);\n\n\tfor_each_cpu(cpu, mask) {\n\t\t \n\t\tif (cpumask_test_cpu(cpu, &sde->cpu_mask)) {\n\t\t\tcpumask_set_cpu(cpu, new_mask);\n\t\t\tcontinue;\n\t\t}\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (!rht_node) {\n\t\t\trht_node = kzalloc(sizeof(*rht_node), GFP_KERNEL);\n\t\t\tif (!rht_node) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tkfree(rht_node);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\trht_node->cpu_id = cpu;\n\t\t\trht_node->map[vl]->mask = 0;\n\t\t\trht_node->map[vl]->ctr = 1;\n\t\t\trht_node->map[vl]->sde[0] = sde;\n\n\t\t\tret = rhashtable_insert_fast(dd->sdma_rht,\n\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t     sdma_rht_params);\n\t\t\tif (ret) {\n\t\t\t\tkfree(rht_node->map[vl]);\n\t\t\t\tkfree(rht_node);\n\t\t\t\tdd_dev_err(sde->dd, \"Failed to set process to sde affinity for cpu %lu\\n\",\n\t\t\t\t\t   cpu);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t} else {\n\t\t\tint ctr, pow;\n\n\t\t\t \n\t\t\tif (!rht_node->map[vl])\n\t\t\t\trht_node->map[vl] = kzalloc(sz, GFP_KERNEL);\n\n\t\t\tif (!rht_node->map[vl]) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trht_node->map[vl]->ctr++;\n\t\t\tctr = rht_node->map[vl]->ctr;\n\t\t\trht_node->map[vl]->sde[ctr - 1] = sde;\n\t\t\tpow = roundup_pow_of_two(ctr);\n\t\t\trht_node->map[vl]->mask = pow - 1;\n\n\t\t\t \n\t\t\tsdma_populate_sde_map(rht_node->map[vl]);\n\t\t}\n\t\tcpumask_set_cpu(cpu, new_mask);\n\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_online_mask) {\n\t\tstruct sdma_rht_node *rht_node;\n\n\t\t \n\t\tif (cpumask_test_cpu(cpu, mask))\n\t\t\tcontinue;\n\n\t\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpu,\n\t\t\t\t\t\t  sdma_rht_params);\n\t\tif (rht_node) {\n\t\t\tbool empty = true;\n\t\t\tint i;\n\n\t\t\t \n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\tif (rht_node->map[i])\n\t\t\t\t\tsdma_cleanup_sde_map(rht_node->map[i],\n\t\t\t\t\t\t\t     sde);\n\n\t\t\t \n\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\t\t\tif (!rht_node->map[i])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (rht_node->map[i]->ctr) {\n\t\t\t\t\tempty = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (empty) {\n\t\t\t\tret = rhashtable_remove_fast(dd->sdma_rht,\n\t\t\t\t\t\t\t     &rht_node->node,\n\t\t\t\t\t\t\t     sdma_rht_params);\n\t\t\t\tWARN_ON(ret);\n\n\t\t\t\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\t\t\t\tkfree(rht_node->map[i]);\n\n\t\t\t\tkfree(rht_node);\n\t\t\t}\n\t\t}\n\t}\n\n\tcpumask_copy(&sde->cpu_mask, new_mask);\nout:\n\tmutex_unlock(&process_to_sde_mutex);\nout_free:\n\tfree_cpumask_var(mask);\n\tfree_cpumask_var(new_mask);\n\treturn ret ? : strnlen(buf, PAGE_SIZE);\n}\n\nssize_t sdma_get_cpu_to_sde_map(struct sdma_engine *sde, char *buf)\n{\n\tmutex_lock(&process_to_sde_mutex);\n\tif (cpumask_empty(&sde->cpu_mask))\n\t\tsnprintf(buf, PAGE_SIZE, \"%s\\n\", \"empty\");\n\telse\n\t\tcpumap_print_to_pagebuf(true, buf, &sde->cpu_mask);\n\tmutex_unlock(&process_to_sde_mutex);\n\treturn strnlen(buf, PAGE_SIZE);\n}\n\nstatic void sdma_rht_free(void *ptr, void *arg)\n{\n\tstruct sdma_rht_node *rht_node = ptr;\n\tint i;\n\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++)\n\t\tkfree(rht_node->map[i]);\n\n\tkfree(rht_node);\n}\n\n \nvoid sdma_seqfile_dump_cpu_list(struct seq_file *s,\n\t\t\t\tstruct hfi1_devdata *dd,\n\t\t\t\tunsigned long cpuid)\n{\n\tstruct sdma_rht_node *rht_node;\n\tint i, j;\n\n\trht_node = rhashtable_lookup_fast(dd->sdma_rht, &cpuid,\n\t\t\t\t\t  sdma_rht_params);\n\tif (!rht_node)\n\t\treturn;\n\n\tseq_printf(s, \"cpu%3lu: \", cpuid);\n\tfor (i = 0; i < HFI1_MAX_VLS_SUPPORTED; i++) {\n\t\tif (!rht_node->map[i] || !rht_node->map[i]->ctr)\n\t\t\tcontinue;\n\n\t\tseq_printf(s, \" vl%d: [\", i);\n\n\t\tfor (j = 0; j < rht_node->map[i]->ctr; j++) {\n\t\t\tif (!rht_node->map[i]->sde[j])\n\t\t\t\tcontinue;\n\n\t\t\tif (j > 0)\n\t\t\t\tseq_puts(s, \",\");\n\n\t\t\tseq_printf(s, \" sdma%2d\",\n\t\t\t\t   rht_node->map[i]->sde[j]->this_idx);\n\t\t}\n\t\tseq_puts(s, \" ]\");\n\t}\n\n\tseq_puts(s, \"\\n\");\n}\n\n \nstatic void sdma_map_free(struct sdma_vl_map *m)\n{\n\tint i;\n\n\tfor (i = 0; m && i < m->actual_vls; i++)\n\t\tkfree(m->map[i]);\n\tkfree(m);\n}\n\n \nstatic void sdma_map_rcu_callback(struct rcu_head *list)\n{\n\tstruct sdma_vl_map *m = container_of(list, struct sdma_vl_map, list);\n\n\tsdma_map_free(m);\n}\n\n \nint sdma_map_init(struct hfi1_devdata *dd, u8 port, u8 num_vls, u8 *vl_engines)\n{\n\tint i, j;\n\tint extra, sde_per_vl;\n\tint engine = 0;\n\tu8 lvl_engines[OPA_MAX_VLS];\n\tstruct sdma_vl_map *oldmap, *newmap;\n\n\tif (!(dd->flags & HFI1_HAS_SEND_DMA))\n\t\treturn 0;\n\n\tif (!vl_engines) {\n\t\t \n\t\tsde_per_vl = dd->num_sdma / num_vls;\n\t\t \n\t\textra = dd->num_sdma % num_vls;\n\t\tvl_engines = lvl_engines;\n\t\t \n\t\tfor (i = num_vls - 1; i >= 0; i--, extra--)\n\t\t\tvl_engines[i] = sde_per_vl + (extra > 0 ? 1 : 0);\n\t}\n\t \n\tnewmap = kzalloc(\n\t\tsizeof(struct sdma_vl_map) +\n\t\t\troundup_pow_of_two(num_vls) *\n\t\t\tsizeof(struct sdma_map_elem *),\n\t\tGFP_KERNEL);\n\tif (!newmap)\n\t\tgoto bail;\n\tnewmap->actual_vls = num_vls;\n\tnewmap->vls = roundup_pow_of_two(num_vls);\n\tnewmap->mask = (1 << ilog2(newmap->vls)) - 1;\n\t \n\tfor (i = 0; i < TXE_NUM_SDMA_ENGINES; i++)\n\t\tnewmap->engine_to_vl[i] = -1;\n\tfor (i = 0; i < newmap->vls; i++) {\n\t\t \n\t\tint first_engine = engine;\n\n\t\tif (i < newmap->actual_vls) {\n\t\t\tint sz = roundup_pow_of_two(vl_engines[i]);\n\n\t\t\t \n\t\t\tnewmap->map[i] = kzalloc(\n\t\t\t\tsizeof(struct sdma_map_elem) +\n\t\t\t\t\tsz * sizeof(struct sdma_engine *),\n\t\t\t\tGFP_KERNEL);\n\t\t\tif (!newmap->map[i])\n\t\t\t\tgoto bail;\n\t\t\tnewmap->map[i]->mask = (1 << ilog2(sz)) - 1;\n\t\t\t \n\t\t\tfor (j = 0; j < sz; j++) {\n\t\t\t\tnewmap->map[i]->sde[j] =\n\t\t\t\t\t&dd->per_sdma[engine];\n\t\t\t\tif (++engine >= first_engine + vl_engines[i])\n\t\t\t\t\t \n\t\t\t\t\tengine = first_engine;\n\t\t\t}\n\t\t\t \n\t\t\tfor (j = 0; j < vl_engines[i]; j++)\n\t\t\t\tnewmap->engine_to_vl[first_engine + j] = i;\n\t\t} else {\n\t\t\t \n\t\t\tnewmap->map[i] = newmap->map[i % num_vls];\n\t\t}\n\t\tengine = first_engine + vl_engines[i];\n\t}\n\t \n\tspin_lock_irq(&dd->sde_map_lock);\n\toldmap = rcu_dereference_protected(dd->sdma_map,\n\t\t\t\t\t   lockdep_is_held(&dd->sde_map_lock));\n\n\t \n\trcu_assign_pointer(dd->sdma_map, newmap);\n\n\tspin_unlock_irq(&dd->sde_map_lock);\n\t \n\tif (oldmap)\n\t\tcall_rcu(&oldmap->list, sdma_map_rcu_callback);\n\treturn 0;\nbail:\n\t \n\tsdma_map_free(newmap);\n\treturn -ENOMEM;\n}\n\n \nvoid sdma_clean(struct hfi1_devdata *dd, size_t num_engines)\n{\n\tsize_t i;\n\tstruct sdma_engine *sde;\n\n\tif (dd->sdma_pad_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, SDMA_PAD,\n\t\t\t\t  (void *)dd->sdma_pad_dma,\n\t\t\t\t  dd->sdma_pad_phys);\n\t\tdd->sdma_pad_dma = NULL;\n\t\tdd->sdma_pad_phys = 0;\n\t}\n\tif (dd->sdma_heads_dma) {\n\t\tdma_free_coherent(&dd->pcidev->dev, dd->sdma_heads_size,\n\t\t\t\t  (void *)dd->sdma_heads_dma,\n\t\t\t\t  dd->sdma_heads_phys);\n\t\tdd->sdma_heads_dma = NULL;\n\t\tdd->sdma_heads_phys = 0;\n\t}\n\tfor (i = 0; dd->per_sdma && i < num_engines; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\n\t\tsde->head_dma = NULL;\n\t\tsde->head_phys = 0;\n\n\t\tif (sde->descq) {\n\t\t\tdma_free_coherent(\n\t\t\t\t&dd->pcidev->dev,\n\t\t\t\tsde->descq_cnt * sizeof(u64[2]),\n\t\t\t\tsde->descq,\n\t\t\t\tsde->descq_phys\n\t\t\t);\n\t\t\tsde->descq = NULL;\n\t\t\tsde->descq_phys = 0;\n\t\t}\n\t\tkvfree(sde->tx_ring);\n\t\tsde->tx_ring = NULL;\n\t}\n\tif (rcu_access_pointer(dd->sdma_map)) {\n\t\tspin_lock_irq(&dd->sde_map_lock);\n\t\tsdma_map_free(rcu_access_pointer(dd->sdma_map));\n\t\tRCU_INIT_POINTER(dd->sdma_map, NULL);\n\t\tspin_unlock_irq(&dd->sde_map_lock);\n\t\tsynchronize_rcu();\n\t}\n\tkfree(dd->per_sdma);\n\tdd->per_sdma = NULL;\n\n\tif (dd->sdma_rht) {\n\t\trhashtable_free_and_destroy(dd->sdma_rht, sdma_rht_free, NULL);\n\t\tkfree(dd->sdma_rht);\n\t\tdd->sdma_rht = NULL;\n\t}\n}\n\n \nint sdma_init(struct hfi1_devdata *dd, u8 port)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\tstruct rhashtable *tmp_sdma_rht;\n\tu16 descq_cnt;\n\tvoid *curr_head;\n\tstruct hfi1_pportdata *ppd = dd->pport + port;\n\tu32 per_sdma_credits;\n\tuint idle_cnt = sdma_idle_cnt;\n\tsize_t num_engines = chip_sdma_engines(dd);\n\tint ret = -ENOMEM;\n\n\tif (!HFI1_CAP_IS_KSET(SDMA)) {\n\t\tHFI1_CAP_CLEAR(SDMA_AHG);\n\t\treturn 0;\n\t}\n\tif (mod_num_sdma &&\n\t     \n\t    mod_num_sdma <= chip_sdma_engines(dd) &&\n\t     \n\t    mod_num_sdma >= num_vls)\n\t\tnum_engines = mod_num_sdma;\n\n\tdd_dev_info(dd, \"SDMA mod_num_sdma: %u\\n\", mod_num_sdma);\n\tdd_dev_info(dd, \"SDMA chip_sdma_engines: %u\\n\", chip_sdma_engines(dd));\n\tdd_dev_info(dd, \"SDMA chip_sdma_mem_size: %u\\n\",\n\t\t    chip_sdma_mem_size(dd));\n\n\tper_sdma_credits =\n\t\tchip_sdma_mem_size(dd) / (num_engines * SDMA_BLOCK_SIZE);\n\n\t \n\tinit_waitqueue_head(&dd->sdma_unfreeze_wq);\n\tatomic_set(&dd->sdma_unfreeze_count, 0);\n\n\tdescq_cnt = sdma_get_descq_cnt();\n\tdd_dev_info(dd, \"SDMA engines %zu descq_cnt %u\\n\",\n\t\t    num_engines, descq_cnt);\n\n\t \n\tdd->per_sdma = kcalloc_node(num_engines, sizeof(*dd->per_sdma),\n\t\t\t\t    GFP_KERNEL, dd->node);\n\tif (!dd->per_sdma)\n\t\treturn ret;\n\n\tidle_cnt = ns_to_cclock(dd, idle_cnt);\n\tif (idle_cnt)\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_HEAD_TO_HOST_FLAG;\n\telse\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_INT_REQ_FLAG;\n\n\tif (!sdma_desct_intr)\n\t\tsdma_desct_intr = SDMA_DESC_INTR;\n\n\t \n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tsde->dd = dd;\n\t\tsde->ppd = ppd;\n\t\tsde->this_idx = this_idx;\n\t\tsde->descq_cnt = descq_cnt;\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tsde->sdma_shift = ilog2(descq_cnt);\n\t\tsde->sdma_mask = (1 << sde->sdma_shift) - 1;\n\n\t\t \n\t\tsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t   this_idx);\n\t\tsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t\tthis_idx);\n\t\tsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t    this_idx);\n\t\t \n\t\tsde->imask = sde->int_mask | sde->progress_mask |\n\t\t\t     sde->idle_mask;\n\n\t\tspin_lock_init(&sde->tail_lock);\n\t\tseqlock_init(&sde->head_lock);\n\t\tspin_lock_init(&sde->senddmactrl_lock);\n\t\tspin_lock_init(&sde->flushlist_lock);\n\t\tseqlock_init(&sde->waitlock);\n\t\t \n\t\tsde->ahg_bits = 0xfffffffe00000000ULL;\n\n\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\n\t\t \n\t\tkref_init(&sde->state.kref);\n\t\tinit_completion(&sde->state.comp);\n\n\t\tINIT_LIST_HEAD(&sde->flushlist);\n\t\tINIT_LIST_HEAD(&sde->dmawait);\n\n\t\tsde->tail_csr =\n\t\t\tget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\n\n\t\ttasklet_setup(&sde->sdma_hw_clean_up_task,\n\t\t\t      sdma_hw_clean_up_task);\n\t\ttasklet_setup(&sde->sdma_sw_clean_up_task,\n\t\t\t      sdma_sw_clean_up_task);\n\t\tINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\n\t\tINIT_WORK(&sde->flush_worker, sdma_field_flush);\n\n\t\tsde->progress_check_head = 0;\n\n\t\ttimer_setup(&sde->err_progress_check_timer,\n\t\t\t    sdma_err_progress_check, 0);\n\n\t\tsde->descq = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdescq_cnt * sizeof(u64[2]),\n\t\t\t\t\t\t&sde->descq_phys, GFP_KERNEL);\n\t\tif (!sde->descq)\n\t\t\tgoto bail;\n\t\tsde->tx_ring =\n\t\t\tkvzalloc_node(array_size(descq_cnt,\n\t\t\t\t\t\t sizeof(struct sdma_txreq *)),\n\t\t\t\t      GFP_KERNEL, dd->node);\n\t\tif (!sde->tx_ring)\n\t\t\tgoto bail;\n\t}\n\n\tdd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\n\t \n\tdd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdd->sdma_heads_size,\n\t\t\t\t\t\t&dd->sdma_heads_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dd->sdma_heads_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA head memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t \n\tdd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, SDMA_PAD,\n\t\t\t\t\t      &dd->sdma_pad_phys, GFP_KERNEL);\n\tif (!dd->sdma_pad_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA pad memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t \n\tcurr_head = (void *)dd->sdma_heads_dma;\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tunsigned long phys_offset;\n\n\t\tsde = &dd->per_sdma[this_idx];\n\n\t\tsde->head_dma = curr_head;\n\t\tcurr_head += L1_CACHE_BYTES;\n\t\tphys_offset = (unsigned long)sde->head_dma -\n\t\t\t      (unsigned long)dd->sdma_heads_dma;\n\t\tsde->head_phys = dd->sdma_heads_phys + phys_offset;\n\t\tinit_sdma_regs(sde, per_sdma_credits, idle_cnt);\n\t}\n\tdd->flags |= HFI1_HAS_SEND_DMA;\n\tdd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\n\tdd->num_sdma = num_engines;\n\tret = sdma_map_init(dd, port, ppd->vls_operational, NULL);\n\tif (ret < 0)\n\t\tgoto bail;\n\n\ttmp_sdma_rht = kzalloc(sizeof(*tmp_sdma_rht), GFP_KERNEL);\n\tif (!tmp_sdma_rht) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n\tif (ret < 0) {\n\t\tkfree(tmp_sdma_rht);\n\t\tgoto bail;\n\t}\n\n\tdd->sdma_rht = tmp_sdma_rht;\n\n\tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);\n\treturn 0;\n\nbail:\n\tsdma_clean(dd, num_engines);\n\treturn ret;\n}\n\n \nvoid sdma_all_running(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e30_go_running);\n\t}\n}\n\n \nvoid sdma_all_idle(struct hfi1_devdata *dd)\n{\n\tstruct sdma_engine *sde;\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e70_go_idle);\n\t}\n}\n\n \nvoid sdma_start(struct hfi1_devdata *dd)\n{\n\tunsigned i;\n\tstruct sdma_engine *sde;\n\n\t \n\tfor (i = 0; i < dd->num_sdma; ++i) {\n\t\tsde = &dd->per_sdma[i];\n\t\tsdma_process_event(sde, sdma_event_e10_go_hw_start);\n\t}\n}\n\n \nvoid sdma_exit(struct hfi1_devdata *dd)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\n\tfor (this_idx = 0; dd->per_sdma && this_idx < dd->num_sdma;\n\t\t\t++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tif (!list_empty(&sde->dmawait))\n\t\t\tdd_dev_err(dd, \"sde %u: dmawait list not empty!\\n\",\n\t\t\t\t   sde->this_idx);\n\t\tsdma_process_event(sde, sdma_event_e00_go_hw_down);\n\n\t\tdel_timer_sync(&sde->err_progress_check_timer);\n\n\t\t \n\t\tsdma_finalput(&sde->state);\n\t}\n}\n\n \nstatic inline void sdma_unmap_desc(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_desc *descp)\n{\n\tswitch (sdma_mapping_type(descp)) {\n\tcase SDMA_MAP_SINGLE:\n\t\tdma_unmap_single(&dd->pcidev->dev, sdma_mapping_addr(descp),\n\t\t\t\t sdma_mapping_len(descp), DMA_TO_DEVICE);\n\t\tbreak;\n\tcase SDMA_MAP_PAGE:\n\t\tdma_unmap_page(&dd->pcidev->dev, sdma_mapping_addr(descp),\n\t\t\t       sdma_mapping_len(descp), DMA_TO_DEVICE);\n\t\tbreak;\n\t}\n\n\tif (descp->pinning_ctx && descp->ctx_put)\n\t\tdescp->ctx_put(descp->pinning_ctx);\n\tdescp->pinning_ctx = NULL;\n}\n\n \nstatic inline u8 ahg_mode(struct sdma_txreq *tx)\n{\n\treturn (tx->descp[0].qw[1] & SDMA_DESC1_HEADER_MODE_SMASK)\n\t\t>> SDMA_DESC1_HEADER_MODE_SHIFT;\n}\n\n \nvoid __sdma_txclean(\n\tstruct hfi1_devdata *dd,\n\tstruct sdma_txreq *tx)\n{\n\tu16 i;\n\n\tif (tx->num_desc) {\n\t\tu8 skip = 0, mode = ahg_mode(tx);\n\n\t\t \n\t\tsdma_unmap_desc(dd, &tx->descp[0]);\n\t\t \n\t\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\t\tskip = mode >> 1;\n\t\tfor (i = 1 + skip; i < tx->num_desc; i++)\n\t\t\tsdma_unmap_desc(dd, &tx->descp[i]);\n\t\ttx->num_desc = 0;\n\t}\n\tkfree(tx->coalesce_buf);\n\ttx->coalesce_buf = NULL;\n\t \n\tif (unlikely(tx->desc_limit > ARRAY_SIZE(tx->descs))) {\n\t\ttx->desc_limit = ARRAY_SIZE(tx->descs);\n\t\tkfree(tx->descp);\n\t}\n}\n\nstatic inline u16 sdma_gethead(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\tint use_dmahead;\n\tu16 hwhead;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\nretry:\n\tuse_dmahead = HFI1_CAP_IS_KSET(USE_SDMA_HEAD) && __sdma_running(sde) &&\n\t\t\t\t\t(dd->flags & HFI1_HAS_SDMA_TIMEOUT);\n\thwhead = use_dmahead ?\n\t\t(u16)le64_to_cpu(*sde->head_dma) :\n\t\t(u16)read_sde_csr(sde, SD(HEAD));\n\n\tif (unlikely(HFI1_CAP_IS_KSET(SDMA_HEAD_CHECK))) {\n\t\tu16 cnt;\n\t\tu16 swtail;\n\t\tu16 swhead;\n\t\tint sane;\n\n\t\tswhead = sde->descq_head & sde->sdma_mask;\n\t\t \n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tcnt = sde->descq_cnt;\n\n\t\tif (swhead < swtail)\n\t\t\t \n\t\t\tsane = (hwhead >= swhead) & (hwhead <= swtail);\n\t\telse if (swhead > swtail)\n\t\t\t \n\t\t\tsane = ((hwhead >= swhead) && (hwhead < cnt)) ||\n\t\t\t\t(hwhead <= swtail);\n\t\telse\n\t\t\t \n\t\t\tsane = (hwhead == swhead);\n\n\t\tif (unlikely(!sane)) {\n\t\t\tdd_dev_err(dd, \"SDMA(%u) bad head (%s) hwhd=%u swhd=%u swtl=%u cnt=%u\\n\",\n\t\t\t\t   sde->this_idx,\n\t\t\t\t   use_dmahead ? \"dma\" : \"kreg\",\n\t\t\t\t   hwhead, swhead, swtail, cnt);\n\t\t\tif (use_dmahead) {\n\t\t\t\t \n\t\t\t\tuse_dmahead = 0;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\t \n\t\t\thwhead = swhead;\n\t\t}\n\t}\n\treturn hwhead;\n}\n\n \nstatic void sdma_desc_avail(struct sdma_engine *sde, uint avail)\n{\n\tstruct iowait *wait, *nw, *twait;\n\tstruct iowait *waits[SDMA_WAIT_BATCH_SIZE];\n\tuint i, n = 0, seq, tidx = 0;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\", sde->this_idx,\n\t\t   slashstrip(__FILE__), __LINE__, __func__);\n\tdd_dev_err(sde->dd, \"avail: %u\\n\", avail);\n#endif\n\n\tdo {\n\t\tseq = read_seqbegin(&sde->waitlock);\n\t\tif (!list_empty(&sde->dmawait)) {\n\t\t\t \n\t\t\twrite_seqlock(&sde->waitlock);\n\t\t\t \n\t\t\tlist_for_each_entry_safe(\n\t\t\t\t\twait,\n\t\t\t\t\tnw,\n\t\t\t\t\t&sde->dmawait,\n\t\t\t\t\tlist) {\n\t\t\t\tu32 num_desc;\n\n\t\t\t\tif (!wait->wakeup)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (n == ARRAY_SIZE(waits))\n\t\t\t\t\tbreak;\n\t\t\t\tiowait_init_priority(wait);\n\t\t\t\tnum_desc = iowait_get_all_desc(wait);\n\t\t\t\tif (num_desc > avail)\n\t\t\t\t\tbreak;\n\t\t\t\tavail -= num_desc;\n\t\t\t\t \n\t\t\t\tif (n) {\n\t\t\t\t\ttwait = waits[tidx];\n\t\t\t\t\ttidx =\n\t\t\t\t\t    iowait_priority_update_top(wait,\n\t\t\t\t\t\t\t\t       twait,\n\t\t\t\t\t\t\t\t       n,\n\t\t\t\t\t\t\t\t       tidx);\n\t\t\t\t}\n\t\t\t\tlist_del_init(&wait->list);\n\t\t\t\twaits[n++] = wait;\n\t\t\t}\n\t\t\twrite_sequnlock(&sde->waitlock);\n\t\t\tbreak;\n\t\t}\n\t} while (read_seqretry(&sde->waitlock, seq));\n\n\t \n\tif (n)\n\t\twaits[tidx]->wakeup(waits[tidx], SDMA_AVAIL_REASON);\n\n\tfor (i = 0; i < n; i++)\n\t\tif (i != tidx)\n\t\t\twaits[i]->wakeup(waits[i], SDMA_AVAIL_REASON);\n}\n\n \nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status)\n{\n\tstruct sdma_txreq *txp = NULL;\n\tint progress = 0;\n\tu16 hwhead, swhead;\n\tint idle_check_done = 0;\n\n\thwhead = sdma_gethead(sde);\n\n\t \n\nretry:\n\ttxp = get_txhead(sde);\n\tswhead = sde->descq_head & sde->sdma_mask;\n\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\twhile (swhead != hwhead) {\n\t\t \n\t\tswhead = ++sde->descq_head & sde->sdma_mask;\n\n\t\t \n\t\tif (txp && txp->next_descq_idx == swhead) {\n\t\t\t \n\t\t\tsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\n\t\t\tcomplete_tx(sde, txp, SDMA_TXREQ_S_OK);\n\t\t\t \n\t\t\ttxp = get_txhead(sde);\n\t\t}\n\t\ttrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\n\t\tprogress++;\n\t}\n\n\t \n\tif ((status & sde->idle_mask) && !idle_check_done) {\n\t\tu16 swtail;\n\n\t\tswtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\t\tif (swtail != hwhead) {\n\t\t\thwhead = (u16)read_sde_csr(sde, SD(HEAD));\n\t\t\tidle_check_done = 1;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tsde->last_status = status;\n\tif (progress)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n}\n\n \nvoid sdma_engine_interrupt(struct sdma_engine *sde, u64 status)\n{\n\ttrace_hfi1_sdma_engine_interrupt(sde, status);\n\twrite_seqlock(&sde->head_lock);\n\tsdma_set_desc_cnt(sde, sdma_desct_intr);\n\tif (status & sde->idle_mask)\n\t\tsde->idle_int_cnt++;\n\telse if (status & sde->progress_mask)\n\t\tsde->progress_int_cnt++;\n\telse if (status & sde->int_mask)\n\t\tsde->sdma_int_cnt++;\n\tsdma_make_progress(sde, status);\n\twrite_sequnlock(&sde->head_lock);\n}\n\n \nvoid sdma_engine_error(struct sdma_engine *sde, u64 status)\n{\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) error status 0x%llx state %s\\n\",\n\t\t   sde->this_idx,\n\t\t   (unsigned long long)status,\n\t\t   sdma_state_names[sde->state.current_state]);\n#endif\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\tif (status & ALL_SDMA_ENG_HALT_ERRS)\n\t\t__sdma_process_event(sde, sdma_event_e60_hw_halted);\n\tif (status & ~SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK)) {\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA (%u) engine error: 0x%llx state %s\\n\",\n\t\t\t   sde->this_idx,\n\t\t\t   (unsigned long long)status,\n\t\t\t   sdma_state_names[sde->state.current_state]);\n\t\tdump_sdma_state(sde);\n\t}\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void sdma_sendctrl(struct sdma_engine *sde, unsigned op)\n{\n\tu64 set_senddmactrl = 0;\n\tu64 clr_senddmactrl = 0;\n\tunsigned long flags;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) senddmactrl E=%d I=%d H=%d C=%d\\n\",\n\t\t   sde->this_idx,\n\t\t   (op & SDMA_SENDCTRL_OP_ENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_INTENABLE) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_HALT) ? 1 : 0,\n\t\t   (op & SDMA_SENDCTRL_OP_CLEANUP) ? 1 : 0);\n#endif\n\n\tif (op & SDMA_SENDCTRL_OP_ENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_INTENABLE)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\n\n\tif (op & SDMA_SENDCTRL_OP_HALT)\n\t\tset_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\telse\n\t\tclr_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\n\n\tspin_lock_irqsave(&sde->senddmactrl_lock, flags);\n\n\tsde->p_senddmactrl |= set_senddmactrl;\n\tsde->p_senddmactrl &= ~clr_senddmactrl;\n\n\tif (op & SDMA_SENDCTRL_OP_CLEANUP)\n\t\twrite_sde_csr(sde, SD(CTRL),\n\t\t\t      sde->p_senddmactrl |\n\t\t\t      SD(CTRL_SDMA_CLEANUP_SMASK));\n\telse\n\t\twrite_sde_csr(sde, SD(CTRL), sde->p_senddmactrl);\n\n\tspin_unlock_irqrestore(&sde->senddmactrl_lock, flags);\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tsdma_dumpstate(sde);\n#endif\n}\n\nstatic void sdma_setlengen(struct sdma_engine *sde)\n{\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\t \n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      (sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT));\n\twrite_sde_csr(sde, SD(LEN_GEN),\n\t\t      ((sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT)) |\n\t\t      (4ULL << SD(LEN_GEN_GENERATION_SHIFT)));\n}\n\nstatic inline void sdma_update_tail(struct sdma_engine *sde, u16 tail)\n{\n\t \n\tsmp_wmb();  \n\twriteq(tail, sde->tail_csr);\n}\n\n \nstatic void sdma_hw_start_up(struct sdma_engine *sde)\n{\n\tu64 reg;\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0);  \n\t*sde->head_dma = 0;\n\n\treg = SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_MASK) <<\n\t      SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SHIFT);\n\twrite_sde_csr(sde, SD(ENG_ERR_CLEAR), reg);\n}\n\n \nstatic void set_sdma_integrity(struct sdma_engine *sde)\n{\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\twrite_sde_csr(sde, SD(CHECK_ENABLE),\n\t\t      hfi1_pkt_base_sdma_integrity(dd));\n}\n\nstatic void init_sdma_regs(\n\tstruct sdma_engine *sde,\n\tu32 credits,\n\tuint idle_cnt)\n{\n\tu8 opval, opmask;\n#ifdef CONFIG_SDMA_VERBOSITY\n\tstruct hfi1_devdata *dd = sde->dd;\n\n\tdd_dev_err(dd, \"CONFIG SDMA(%u) %s:%d %s()\\n\",\n\t\t   sde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\n#endif\n\n\twrite_sde_csr(sde, SD(BASE_ADDR), sde->descq_phys);\n\tsdma_setlengen(sde);\n\tsdma_update_tail(sde, 0);  \n\twrite_sde_csr(sde, SD(RELOAD_CNT), idle_cnt);\n\twrite_sde_csr(sde, SD(DESC_CNT), 0);\n\twrite_sde_csr(sde, SD(HEAD_ADDR), sde->head_phys);\n\twrite_sde_csr(sde, SD(MEMORY),\n\t\t      ((u64)credits << SD(MEMORY_SDMA_MEMORY_CNT_SHIFT)) |\n\t\t      ((u64)(credits * sde->this_idx) <<\n\t\t       SD(MEMORY_SDMA_MEMORY_INDEX_SHIFT)));\n\twrite_sde_csr(sde, SD(ENG_ERR_MASK), ~0ull);\n\tset_sdma_integrity(sde);\n\topmask = OPCODE_CHECK_MASK_DISABLED;\n\topval = OPCODE_CHECK_VAL_DISABLED;\n\twrite_sde_csr(sde, SD(CHECK_OPCODE),\n\t\t      (opmask << SEND_CTXT_CHECK_OPCODE_MASK_SHIFT) |\n\t\t      (opval << SEND_CTXT_CHECK_OPCODE_VALUE_SHIFT));\n}\n\n#ifdef CONFIG_SDMA_VERBOSITY\n\n#define sdma_dumpstate_helper0(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s     0x%016llx\\n\", #reg, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper(reg) do { \\\n\t\tcsr = read_sde_csr(sde, reg); \\\n\t\tdd_dev_err(sde->dd, \"%36s[%02u] 0x%016llx\\n\", \\\n\t\t\t#reg, sde->this_idx, csr); \\\n\t} while (0)\n\n#define sdma_dumpstate_helper2(reg) do { \\\n\t\tcsr = read_csr(sde->dd, reg + (8 * i)); \\\n\t\tdd_dev_err(sde->dd, \"%33s_%02u     0x%016llx\\n\", \\\n\t\t\t\t#reg, i, csr); \\\n\t} while (0)\n\nvoid sdma_dumpstate(struct sdma_engine *sde)\n{\n\tu64 csr;\n\tunsigned i;\n\n\tsdma_dumpstate_helper(SD(CTRL));\n\tsdma_dumpstate_helper(SD(STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_STATUS));\n\tsdma_dumpstate_helper0(SD(ERR_MASK));\n\tsdma_dumpstate_helper(SD(ENG_ERR_STATUS));\n\tsdma_dumpstate_helper(SD(ENG_ERR_MASK));\n\n\tfor (i = 0; i < CCE_NUM_INT_CSRS; ++i) {\n\t\tsdma_dumpstate_helper2(CCE_INT_STATUS);\n\t\tsdma_dumpstate_helper2(CCE_INT_MASK);\n\t\tsdma_dumpstate_helper2(CCE_INT_BLOCKED);\n\t}\n\n\tsdma_dumpstate_helper(SD(TAIL));\n\tsdma_dumpstate_helper(SD(HEAD));\n\tsdma_dumpstate_helper(SD(PRIORITY_THLD));\n\tsdma_dumpstate_helper(SD(IDLE_CNT));\n\tsdma_dumpstate_helper(SD(RELOAD_CNT));\n\tsdma_dumpstate_helper(SD(DESC_CNT));\n\tsdma_dumpstate_helper(SD(DESC_FETCHED_CNT));\n\tsdma_dumpstate_helper(SD(MEMORY));\n\tsdma_dumpstate_helper0(SD(ENGINES));\n\tsdma_dumpstate_helper0(SD(MEM_SIZE));\n\t \n\tsdma_dumpstate_helper(SD(BASE_ADDR));\n\tsdma_dumpstate_helper(SD(LEN_GEN));\n\tsdma_dumpstate_helper(SD(HEAD_ADDR));\n\tsdma_dumpstate_helper(SD(CHECK_ENABLE));\n\tsdma_dumpstate_helper(SD(CHECK_VL));\n\tsdma_dumpstate_helper(SD(CHECK_JOB_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_PARTITION_KEY));\n\tsdma_dumpstate_helper(SD(CHECK_SLID));\n\tsdma_dumpstate_helper(SD(CHECK_OPCODE));\n}\n#endif\n\nstatic void dump_sdma_state(struct sdma_engine *sde)\n{\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\tu16 head, tail, cnt;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tcnt = sdma_descq_freecnt(sde);\n\n\tdd_dev_err(sde->dd,\n\t\t   \"SDMA (%u) descq_head: %u descq_tail: %u freecnt: %u FLE %d\\n\",\n\t\t   sde->this_idx, head, tail, cnt,\n\t\t   !list_empty(&sde->flushlist));\n\n\t \n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tdd_dev_err(sde->dd,\n\t\t\t   \"\\tdesc0:0x%016llx desc1 0x%016llx\\n\",\n\t\t\t   desc[0], desc[1]);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tdd_dev_err(sde->dd,\n\t\t\t\t   \"\\taidx: %u amode: %u alen: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_DWS_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_DWS_SHIFT));\n\t\thead++;\n\t\thead &= sde->sdma_mask;\n\t}\n}\n\n#define SDE_FMT \\\n\t\"SDE %u CPU %d STE %s C 0x%llx S 0x%016llx E 0x%llx T(HW) 0x%llx T(SW) 0x%x H(HW) 0x%llx H(SW) 0x%x H(D) 0x%llx DM 0x%llx GL 0x%llx R 0x%llx LIS 0x%llx AHGI 0x%llx TXT %u TXH %u DT %u DH %u FLNE %d DQF %u SLC 0x%llx\\n\"\n \nvoid sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *sde)\n{\n\tu16 head, tail;\n\tstruct hw_sdma_desc *descqp;\n\tu64 desc[2];\n\tu64 addr;\n\tu8 gen;\n\tu16 len;\n\n\thead = sde->descq_head & sde->sdma_mask;\n\ttail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;\n\tseq_printf(s, SDE_FMT, sde->this_idx,\n\t\t   sde->cpu,\n\t\t   sdma_state_name(sde->state.current_state),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(CTRL)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(ENG_ERR_STATUS)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(TAIL)), tail,\n\t\t   (unsigned long long)read_sde_csr(sde, SD(HEAD)), head,\n\t\t   (unsigned long long)le64_to_cpu(*sde->head_dma),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(MEMORY)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(LEN_GEN)),\n\t\t   (unsigned long long)read_sde_csr(sde, SD(RELOAD_CNT)),\n\t\t   (unsigned long long)sde->last_status,\n\t\t   (unsigned long long)sde->ahg_bits,\n\t\t   sde->tx_tail,\n\t\t   sde->tx_head,\n\t\t   sde->descq_tail,\n\t\t   sde->descq_head,\n\t\t   !list_empty(&sde->flushlist),\n\t\t   sde->descq_full_count,\n\t\t   (unsigned long long)read_sde_csr(sde, SEND_DMA_CHECK_SLID));\n\n\t \n\twhile (head != tail) {\n\t\tchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\n\n\t\tdescqp = &sde->descq[head];\n\t\tdesc[0] = le64_to_cpu(descqp->qw[0]);\n\t\tdesc[1] = le64_to_cpu(descqp->qw[1]);\n\t\tflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\n\t\tflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\n\t\t\t\t'H' : '-';\n\t\tflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\n\t\tflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\n\t\taddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\n\t\t\t& SDMA_DESC0_PHY_ADDR_MASK;\n\t\tgen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\n\t\t\t& SDMA_DESC1_GENERATION_MASK;\n\t\tlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\n\t\t\t& SDMA_DESC0_BYTE_COUNT_MASK;\n\t\tseq_printf(s,\n\t\t\t   \"\\tdesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\\n\",\n\t\t\t   head, flags, addr, gen, len);\n\t\tif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\n\t\t\tseq_printf(s, \"\\t\\tahgidx: %u ahgmode: %u\\n\",\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_INDEX_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_INDEX_SHIFT),\n\t\t\t\t   (u8)((desc[1] &\n\t\t\t\t\t SDMA_DESC1_HEADER_MODE_SMASK) >>\n\t\t\t\t\tSDMA_DESC1_HEADER_MODE_SHIFT));\n\t\thead = (head + 1) & sde->sdma_mask;\n\t}\n}\n\n \nstatic inline u64 add_gen(struct sdma_engine *sde, u64 qw1)\n{\n\tu8 generation = (sde->descq_tail >> sde->sdma_shift) & 3;\n\n\tqw1 &= ~SDMA_DESC1_GENERATION_SMASK;\n\tqw1 |= ((u64)generation & SDMA_DESC1_GENERATION_MASK)\n\t\t\t<< SDMA_DESC1_GENERATION_SHIFT;\n\treturn qw1;\n}\n\n \nstatic inline u16 submit_tx(struct sdma_engine *sde, struct sdma_txreq *tx)\n{\n\tint i;\n\tu16 tail;\n\tstruct sdma_desc *descp = tx->descp;\n\tu8 skip = 0, mode = ahg_mode(tx);\n\n\ttail = sde->descq_tail & sde->sdma_mask;\n\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\tsde->descq[tail].qw[1] = cpu_to_le64(add_gen(sde, descp->qw[1]));\n\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], descp->qw[1],\n\t\t\t\t   tail, &sde->descq[tail]);\n\ttail = ++sde->descq_tail & sde->sdma_mask;\n\tdescp++;\n\tif (mode > SDMA_AHG_APPLY_UPDATE1)\n\t\tskip = mode >> 1;\n\tfor (i = 1; i < tx->num_desc; i++, descp++) {\n\t\tu64 qw1;\n\n\t\tsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\n\t\tif (skip) {\n\t\t\t \n\t\t\tqw1 = descp->qw[1];\n\t\t\tskip--;\n\t\t} else {\n\t\t\t \n\t\t\tqw1 = add_gen(sde, descp->qw[1]);\n\t\t}\n\t\tsde->descq[tail].qw[1] = cpu_to_le64(qw1);\n\t\ttrace_hfi1_sdma_descriptor(sde, descp->qw[0], qw1,\n\t\t\t\t\t   tail, &sde->descq[tail]);\n\t\ttail = ++sde->descq_tail & sde->sdma_mask;\n\t}\n\ttx->next_descq_idx = tail;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n\tWARN_ON_ONCE(sde->tx_ring[sde->tx_tail & sde->sdma_mask]);\n#endif\n\tsde->tx_ring[sde->tx_tail++ & sde->sdma_mask] = tx;\n\tsde->desc_avail -= tx->num_desc;\n\treturn tail;\n}\n\n \nstatic int sdma_check_progress(\n\tstruct sdma_engine *sde,\n\tstruct iowait_work *wait,\n\tstruct sdma_txreq *tx,\n\tbool pkts_sent)\n{\n\tint ret;\n\n\tsde->desc_avail = sdma_descq_freecnt(sde);\n\tif (tx->num_desc <= sde->desc_avail)\n\t\treturn -EAGAIN;\n\t \n\tif (wait && iowait_ioww_to_iow(wait)->sleep) {\n\t\tunsigned seq;\n\n\t\tseq = raw_seqcount_begin(\n\t\t\t(const seqcount_t *)&sde->head_lock.seqcount);\n\t\tret = wait->iow->sleep(sde, wait, tx, seq, pkts_sent);\n\t\tif (ret == -EAGAIN)\n\t\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t} else {\n\t\tret = -EBUSY;\n\t}\n\treturn ret;\n}\n\n \nint sdma_send_txreq(struct sdma_engine *sde,\n\t\t    struct iowait_work *wait,\n\t\t    struct sdma_txreq *tx,\n\t\t    bool pkts_sent)\n{\n\tint ret = 0;\n\tu16 tail;\n\tunsigned long flags;\n\n\t \n\tif (unlikely(tx->tlen))\n\t\treturn -EINVAL;\n\ttx->wait = iowait_ioww_to_iow(wait);\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tif (unlikely(!__sdma_running(sde)))\n\t\tgoto unlock_noconn;\n\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\tgoto nodesc;\n\ttail = submit_tx(sde, tx);\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\tsdma_update_tail(sde, tail);\nunlock:\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\treturn ret;\nunlock_noconn:\n\tif (wait)\n\t\tiowait_sdma_inc(iowait_ioww_to_iow(wait));\n\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\ttx->sn = sde->tail_sn++;\n\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\tspin_lock(&sde->flushlist_lock);\n\tlist_add_tail(&tx->list, &sde->flushlist);\n\tspin_unlock(&sde->flushlist_lock);\n\tiowait_inc_wait_count(wait, tx->num_desc);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto unlock;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, pkts_sent);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto unlock;\n}\n\n \nint sdma_send_txlist(struct sdma_engine *sde, struct iowait_work *wait,\n\t\t     struct list_head *tx_list, u16 *count_out)\n{\n\tstruct sdma_txreq *tx, *tx_next;\n\tint ret = 0;\n\tunsigned long flags;\n\tu16 tail = INVALID_TAIL;\n\tu32 submit_count = 0, flush_count = 0, total_count;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\nretry:\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tif (unlikely(!__sdma_running(sde)))\n\t\t\tgoto unlock_noconn;\n\t\tif (unlikely(tx->num_desc > sde->desc_avail))\n\t\t\tgoto nodesc;\n\t\tif (unlikely(tx->tlen)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto update_tail;\n\t\t}\n\t\tlist_del_init(&tx->list);\n\t\ttail = submit_tx(sde, tx);\n\t\tsubmit_count++;\n\t\tif (tail != INVALID_TAIL &&\n\t\t    (submit_count & SDMA_TAIL_UPDATE_THRESH) == 0) {\n\t\t\tsdma_update_tail(sde, tail);\n\t\t\ttail = INVALID_TAIL;\n\t\t}\n\t}\nupdate_tail:\n\ttotal_count = submit_count + flush_count;\n\tif (wait) {\n\t\tiowait_sdma_add(iowait_ioww_to_iow(wait), total_count);\n\t\tiowait_starve_clear(submit_count > 0,\n\t\t\t\t    iowait_ioww_to_iow(wait));\n\t}\n\tif (tail != INVALID_TAIL)\n\t\tsdma_update_tail(sde, tail);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n\t*count_out = total_count;\n\treturn ret;\nunlock_noconn:\n\tspin_lock(&sde->flushlist_lock);\n\tlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\n\t\ttx->wait = iowait_ioww_to_iow(wait);\n\t\tlist_del_init(&tx->list);\n\t\ttx->next_descq_idx = 0;\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\n\t\ttx->sn = sde->tail_sn++;\n\t\ttrace_hfi1_sdma_in_sn(sde, tx->sn);\n#endif\n\t\tlist_add_tail(&tx->list, &sde->flushlist);\n\t\tflush_count++;\n\t\tiowait_inc_wait_count(wait, tx->num_desc);\n\t}\n\tspin_unlock(&sde->flushlist_lock);\n\tqueue_work_on(sde->cpu, system_highpri_wq, &sde->flush_worker);\n\tret = -ECOMM;\n\tgoto update_tail;\nnodesc:\n\tret = sdma_check_progress(sde, wait, tx, submit_count > 0);\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tsde->descq_full_count++;\n\tgoto update_tail;\n}\n\nstatic void sdma_process_event(struct sdma_engine *sde, enum sdma_events event)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sde->tail_lock, flags);\n\twrite_seqlock(&sde->head_lock);\n\n\t__sdma_process_event(sde, event);\n\n\tif (sde->state.current_state == sdma_state_s99_running)\n\t\tsdma_desc_avail(sde, sdma_descq_freecnt(sde));\n\n\twrite_sequnlock(&sde->head_lock);\n\tspin_unlock_irqrestore(&sde->tail_lock, flags);\n}\n\nstatic void __sdma_process_event(struct sdma_engine *sde,\n\t\t\t\t enum sdma_events event)\n{\n\tstruct sdma_state *ss = &sde->state;\n\tint need_progress = 0;\n\n\t \n#ifdef CONFIG_SDMA_VERBOSITY\n\tdd_dev_err(sde->dd, \"CONFIG SDMA(%u) [%s] %s\\n\", sde->this_idx,\n\t\t   sdma_state_names[ss->current_state],\n\t\t   sdma_event_names[event]);\n#endif\n\n\tswitch (ss->current_state) {\n\tcase sdma_state_s00_hw_down:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\t \n\t\t\tss->go_s99_running = 1;\n\t\t\tfallthrough;\t \n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\t \n\t\t\tsdma_get(&sde->state);\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s10_hw_start_up_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s10_hw_start_up_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde,\n\t\t\t\t       sdma_state_s15_hw_start_up_clean_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s15_hw_start_up_clean_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s20_idle:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tsdma_sw_tear_down(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tsdma_set_state(sde, sdma_state_s99_running);\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s30_sw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tsdma_set_state(sde, sdma_state_s40_hw_clean_up_wait);\n\t\t\tsdma_start_hw_clean_up(sde);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s40_hw_clean_up_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s50_hw_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s60_idle_halt_wait:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s80_hw_freeze:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tsdma_set_state(sde, sdma_state_s82_freeze_sw_clean);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s82_freeze_sw_clean:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tss->go_s99_running = 1;\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\t \n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tss->go_s99_running = 0;\n\t\t\tbreak;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tsdma_hw_start_up(sde);\n\t\t\tsdma_set_state(sde, ss->go_s99_running ?\n\t\t\t\t       sdma_state_s99_running :\n\t\t\t\t       sdma_state_s20_idle);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tbreak;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase sdma_state_s99_running:\n\t\tswitch (event) {\n\t\tcase sdma_event_e00_go_hw_down:\n\t\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\t\t\ttasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\n\t\t\tbreak;\n\t\tcase sdma_event_e10_go_hw_start:\n\t\t\tbreak;\n\t\tcase sdma_event_e15_hw_halt_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e25_hw_clean_up_done:\n\t\t\tbreak;\n\t\tcase sdma_event_e30_go_running:\n\t\t\tbreak;\n\t\tcase sdma_event_e40_sw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e50_hw_cleaned:\n\t\t\tbreak;\n\t\tcase sdma_event_e60_hw_halted:\n\t\t\tneed_progress = 1;\n\t\t\tsdma_err_progress_check_schedule(sde);\n\t\t\tfallthrough;\n\t\tcase sdma_event_e90_sw_halted:\n\t\t\t \n\t\t\tsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\n\t\t\tschedule_work(&sde->err_halt_worker);\n\t\t\tbreak;\n\t\tcase sdma_event_e70_go_idle:\n\t\t\tsdma_set_state(sde, sdma_state_s60_idle_halt_wait);\n\t\t\tbreak;\n\t\tcase sdma_event_e85_link_down:\n\t\t\tss->go_s99_running = 0;\n\t\t\tfallthrough;\n\t\tcase sdma_event_e80_hw_freeze:\n\t\t\tsdma_set_state(sde, sdma_state_s80_hw_freeze);\n\t\t\tatomic_dec(&sde->dd->sdma_unfreeze_count);\n\t\t\twake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\n\t\t\tbreak;\n\t\tcase sdma_event_e81_hw_frozen:\n\t\t\tbreak;\n\t\tcase sdma_event_e82_hw_unfreeze:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tss->last_event = event;\n\tif (need_progress)\n\t\tsdma_make_progress(sde, 0);\n}\n\n \nstatic int _extend_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint i;\n\tstruct sdma_desc *descp;\n\n\t \n\tif (unlikely((tx->num_desc == (MAX_DESC - 1)))) {\n\t\t \n\t\tif (!tx->tlen) {\n\t\t\ttx->desc_limit = MAX_DESC;\n\t\t} else if (!tx->coalesce_buf) {\n\t\t\t \n\t\t\ttx->coalesce_buf = kmalloc(tx->tlen + sizeof(u32),\n\t\t\t\t\t\t   GFP_ATOMIC);\n\t\t\tif (!tx->coalesce_buf)\n\t\t\t\tgoto enomem;\n\t\t\ttx->coalesce_idx = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(tx->num_desc == MAX_DESC))\n\t\tgoto enomem;\n\n\tdescp = kmalloc_array(MAX_DESC, sizeof(struct sdma_desc), GFP_ATOMIC);\n\tif (!descp)\n\t\tgoto enomem;\n\ttx->descp = descp;\n\n\t \n\ttx->desc_limit = MAX_DESC - 1;\n\t \n\tfor (i = 0; i < tx->num_desc; i++)\n\t\ttx->descp[i] = tx->descs[i];\n\treturn 0;\nenomem:\n\t__sdma_txclean(dd, tx);\n\treturn -ENOMEM;\n}\n\n \nint ext_coal_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx,\n\t\t\t   int type, void *kvaddr, struct page *page,\n\t\t\t   unsigned long offset, u16 len)\n{\n\tint pad_len, rval;\n\tdma_addr_t addr;\n\n\trval = _extend_sdma_tx_descs(dd, tx);\n\tif (rval) {\n\t\t__sdma_txclean(dd, tx);\n\t\treturn rval;\n\t}\n\n\t \n\tif (tx->coalesce_buf) {\n\t\tif (type == SDMA_MAP_NONE) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (type == SDMA_MAP_PAGE) {\n\t\t\tkvaddr = kmap_local_page(page);\n\t\t\tkvaddr += offset;\n\t\t} else if (WARN_ON(!kvaddr)) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(tx->coalesce_buf + tx->coalesce_idx, kvaddr, len);\n\t\ttx->coalesce_idx += len;\n\t\tif (type == SDMA_MAP_PAGE)\n\t\t\tkunmap_local(kvaddr);\n\n\t\t \n\t\tif (tx->tlen - tx->coalesce_idx)\n\t\t\treturn 0;\n\n\t\t \n\t\tpad_len = tx->packet_len & (sizeof(u32) - 1);\n\t\tif (pad_len) {\n\t\t\tpad_len = sizeof(u32) - pad_len;\n\t\t\tmemset(tx->coalesce_buf + tx->coalesce_idx, 0, pad_len);\n\t\t\t \n\t\t\ttx->packet_len += pad_len;\n\t\t\ttx->tlen += pad_len;\n\t\t}\n\n\t\t \n\t\taddr = dma_map_single(&dd->pcidev->dev,\n\t\t\t\t      tx->coalesce_buf,\n\t\t\t\t      tx->tlen,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\t\tif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\t \n\t\ttx->desc_limit = MAX_DESC;\n\t\treturn _sdma_txadd_daddr(dd, SDMA_MAP_SINGLE, tx,\n\t\t\t\t\t addr, tx->tlen, NULL, NULL, NULL);\n\t}\n\n\treturn 1;\n}\n\n \nvoid sdma_update_lmc(struct hfi1_devdata *dd, u64 mask, u32 lid)\n{\n\tstruct sdma_engine *sde;\n\tint i;\n\tu64 sreg;\n\n\tsreg = ((mask & SD(CHECK_SLID_MASK_MASK)) <<\n\t\tSD(CHECK_SLID_MASK_SHIFT)) |\n\t\t(((lid & mask) & SD(CHECK_SLID_VALUE_MASK)) <<\n\t\tSD(CHECK_SLID_VALUE_SHIFT));\n\n\tfor (i = 0; i < dd->num_sdma; i++) {\n\t\thfi1_cdbg(LINKVERB, \"SendDmaEngine[%d].SLID_CHECK = 0x%x\",\n\t\t\t  i, (u32)sreg);\n\t\tsde = &dd->per_sdma[i];\n\t\twrite_sde_csr(sde, SD(CHECK_SLID), sreg);\n\t}\n}\n\n \nint _pad_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\n{\n\tint rval = 0;\n\n\tif ((unlikely(tx->num_desc + 1 == tx->desc_limit))) {\n\t\trval = _extend_sdma_tx_descs(dd, tx);\n\t\tif (rval) {\n\t\t\t__sdma_txclean(dd, tx);\n\t\t\treturn rval;\n\t\t}\n\t}\n\n\t \n\tmake_tx_sdma_desc(\n\t\ttx,\n\t\tSDMA_MAP_NONE,\n\t\tdd->sdma_pad_phys,\n\t\tsizeof(u32) - (tx->packet_len & (sizeof(u32) - 1)),\n\t\tNULL, NULL, NULL);\n\ttx->num_desc++;\n\t_sdma_close_tx(dd, tx);\n\treturn rval;\n}\n\n \nvoid _sdma_txreq_ahgadd(\n\tstruct sdma_txreq *tx,\n\tu8 num_ahg,\n\tu8 ahg_entry,\n\tu32 *ahg,\n\tu8 ahg_hlen)\n{\n\tu32 i, shift = 0, desc = 0;\n\tu8 mode;\n\n\tWARN_ON_ONCE(num_ahg > 9 || (ahg_hlen & 3) || ahg_hlen == 4);\n\t \n\tif (num_ahg == 1)\n\t\tmode = SDMA_AHG_APPLY_UPDATE1;\n\telse if (num_ahg <= 5)\n\t\tmode = SDMA_AHG_APPLY_UPDATE2;\n\telse\n\t\tmode = SDMA_AHG_APPLY_UPDATE3;\n\ttx->num_desc++;\n\t \n\tswitch (mode) {\n\tcase SDMA_AHG_APPLY_UPDATE3:\n\t\ttx->num_desc++;\n\t\ttx->descs[2].qw[0] = 0;\n\t\ttx->descs[2].qw[1] = 0;\n\t\tfallthrough;\n\tcase SDMA_AHG_APPLY_UPDATE2:\n\t\ttx->num_desc++;\n\t\ttx->descs[1].qw[0] = 0;\n\t\ttx->descs[1].qw[1] = 0;\n\t\tbreak;\n\t}\n\tahg_hlen >>= 2;\n\ttx->descs[0].qw[1] |=\n\t\t(((u64)ahg_entry & SDMA_DESC1_HEADER_INDEX_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_INDEX_SHIFT) |\n\t\t(((u64)ahg_hlen & SDMA_DESC1_HEADER_DWS_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_DWS_SHIFT) |\n\t\t(((u64)mode & SDMA_DESC1_HEADER_MODE_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_MODE_SHIFT) |\n\t\t(((u64)ahg[0] & SDMA_DESC1_HEADER_UPDATE1_MASK)\n\t\t\t<< SDMA_DESC1_HEADER_UPDATE1_SHIFT);\n\tfor (i = 0; i < (num_ahg - 1); i++) {\n\t\tif (!shift && !(i & 2))\n\t\t\tdesc++;\n\t\ttx->descs[desc].qw[!!(i & 2)] |=\n\t\t\t(((u64)ahg[i + 1])\n\t\t\t\t<< shift);\n\t\tshift = (shift + 32) & 63;\n\t}\n}\n\n \nint sdma_ahg_alloc(struct sdma_engine *sde)\n{\n\tint nr;\n\tint oldbit;\n\n\tif (!sde) {\n\t\ttrace_hfi1_ahg_allocate(sde, -EINVAL);\n\t\treturn -EINVAL;\n\t}\n\twhile (1) {\n\t\tnr = ffz(READ_ONCE(sde->ahg_bits));\n\t\tif (nr > 31) {\n\t\t\ttrace_hfi1_ahg_allocate(sde, -ENOSPC);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\toldbit = test_and_set_bit(nr, &sde->ahg_bits);\n\t\tif (!oldbit)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\ttrace_hfi1_ahg_allocate(sde, nr);\n\treturn nr;\n}\n\n \nvoid sdma_ahg_free(struct sdma_engine *sde, int ahg_index)\n{\n\tif (!sde)\n\t\treturn;\n\ttrace_hfi1_ahg_deallocate(sde, ahg_index);\n\tif (ahg_index < 0 || ahg_index > 31)\n\t\treturn;\n\tclear_bit(ahg_index, &sde->ahg_bits);\n}\n\n \nvoid sdma_freeze_notify(struct hfi1_devdata *dd, int link_down)\n{\n\tint i;\n\tenum sdma_events event = link_down ? sdma_event_e85_link_down :\n\t\t\t\t\t     sdma_event_e80_hw_freeze;\n\n\t \n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t \n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], event);\n\n\t \n}\n\n \nvoid sdma_freeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\tint ret;\n\n\t \n\tret = wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\t       atomic_read(&dd->sdma_unfreeze_count) <=\n\t\t\t\t       0);\n\t \n\tif (ret || atomic_read(&dd->sdma_unfreeze_count) < 0)\n\t\treturn;\n\n\t \n\tatomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\n\n\t \n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i], sdma_event_e81_hw_frozen);\n\n\t \n\t(void)wait_event_interruptible(dd->sdma_unfreeze_wq,\n\t\t\t\tatomic_read(&dd->sdma_unfreeze_count) <= 0);\n\t \n}\n\n \nvoid sdma_unfreeze(struct hfi1_devdata *dd)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < dd->num_sdma; i++)\n\t\tsdma_process_event(&dd->per_sdma[i],\n\t\t\t\t   sdma_event_e82_hw_unfreeze);\n}\n\n \nvoid _sdma_engine_progress_schedule(\n\tstruct sdma_engine *sde)\n{\n\ttrace_hfi1_sdma_engine_progress(sde, sde->progress_mask);\n\t \n\twrite_csr(sde->dd,\n\t\t  CCE_INT_FORCE + (8 * (IS_SDMA_START / 64)),\n\t\t  sde->progress_mask);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}