{
  "module_name": "netdev_rx.c",
  "hash_id": "56712fe745cbfce2f02ede115ec56c31bc3972fa6b390b327594960a06fa5c7a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/hfi1/netdev_rx.c",
  "human_readable_source": "\n \n\n \n\n#include \"sdma.h\"\n#include \"verbs.h\"\n#include \"netdev.h\"\n#include \"hfi.h\"\n\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <rdma/ib_verbs.h>\n\nstatic int hfi1_netdev_setup_ctxt(struct hfi1_netdev_rx *rx,\n\t\t\t\t  struct hfi1_ctxtdata *uctxt)\n{\n\tunsigned int rcvctrl_ops;\n\tstruct hfi1_devdata *dd = rx->dd;\n\tint ret;\n\n\tuctxt->rhf_rcv_function_map = netdev_rhf_rcv_functions;\n\tuctxt->do_interrupt = &handle_receive_interrupt_napi_sp;\n\n\t \n\tret = hfi1_create_rcvhdrq(dd, uctxt);\n\tif (ret)\n\t\tgoto done;\n\n\tret = hfi1_setup_eagerbufs(uctxt);\n\tif (ret)\n\t\tgoto done;\n\n\tclear_rcvhdrtail(uctxt);\n\n\trcvctrl_ops = HFI1_RCVCTRL_CTXT_DIS;\n\trcvctrl_ops |= HFI1_RCVCTRL_INTRAVAIL_DIS;\n\n\tif (!HFI1_CAP_KGET_MASK(uctxt->flags, MULTI_PKT_EGR))\n\t\trcvctrl_ops |= HFI1_RCVCTRL_ONE_PKT_EGR_ENB;\n\tif (HFI1_CAP_KGET_MASK(uctxt->flags, NODROP_EGR_FULL))\n\t\trcvctrl_ops |= HFI1_RCVCTRL_NO_EGR_DROP_ENB;\n\tif (HFI1_CAP_KGET_MASK(uctxt->flags, NODROP_RHQ_FULL))\n\t\trcvctrl_ops |= HFI1_RCVCTRL_NO_RHQ_DROP_ENB;\n\tif (HFI1_CAP_KGET_MASK(uctxt->flags, DMA_RTAIL))\n\t\trcvctrl_ops |= HFI1_RCVCTRL_TAILUPD_ENB;\n\n\thfi1_rcvctrl(uctxt->dd, rcvctrl_ops, uctxt);\ndone:\n\treturn ret;\n}\n\nstatic int hfi1_netdev_allocate_ctxt(struct hfi1_devdata *dd,\n\t\t\t\t     struct hfi1_ctxtdata **ctxt)\n{\n\tstruct hfi1_ctxtdata *uctxt;\n\tint ret;\n\n\tif (dd->flags & HFI1_FROZEN)\n\t\treturn -EIO;\n\n\tret = hfi1_create_ctxtdata(dd->pport, dd->node, &uctxt);\n\tif (ret < 0) {\n\t\tdd_dev_err(dd, \"Unable to create ctxtdata, failing open\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tuctxt->flags = HFI1_CAP_KGET(MULTI_PKT_EGR) |\n\t\tHFI1_CAP_KGET(NODROP_RHQ_FULL) |\n\t\tHFI1_CAP_KGET(NODROP_EGR_FULL) |\n\t\tHFI1_CAP_KGET(DMA_RTAIL);\n\t \n\tuctxt->fast_handler = handle_receive_interrupt_napi_fp;\n\tuctxt->slow_handler = handle_receive_interrupt_napi_sp;\n\thfi1_set_seq_cnt(uctxt, 1);\n\tuctxt->is_vnic = true;\n\n\thfi1_stats.sps_ctxts++;\n\n\tdd_dev_info(dd, \"created netdev context %d\\n\", uctxt->ctxt);\n\t*ctxt = uctxt;\n\n\treturn 0;\n}\n\nstatic void hfi1_netdev_deallocate_ctxt(struct hfi1_devdata *dd,\n\t\t\t\t\tstruct hfi1_ctxtdata *uctxt)\n{\n\tflush_wc();\n\n\t \n\thfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS |\n\t\t     HFI1_RCVCTRL_TIDFLOW_DIS |\n\t\t     HFI1_RCVCTRL_INTRAVAIL_DIS |\n\t\t     HFI1_RCVCTRL_ONE_PKT_EGR_DIS |\n\t\t     HFI1_RCVCTRL_NO_RHQ_DROP_DIS |\n\t\t     HFI1_RCVCTRL_NO_EGR_DROP_DIS, uctxt);\n\n\tif (uctxt->msix_intr != CCE_NUM_MSIX_VECTORS)\n\t\tmsix_free_irq(dd, uctxt->msix_intr);\n\n\tuctxt->msix_intr = CCE_NUM_MSIX_VECTORS;\n\tuctxt->event_flags = 0;\n\n\thfi1_clear_tids(uctxt);\n\thfi1_clear_ctxt_pkey(dd, uctxt);\n\n\thfi1_stats.sps_ctxts--;\n\n\thfi1_free_ctxt(uctxt);\n}\n\nstatic int hfi1_netdev_allot_ctxt(struct hfi1_netdev_rx *rx,\n\t\t\t\t  struct hfi1_ctxtdata **ctxt)\n{\n\tint rc;\n\tstruct hfi1_devdata *dd = rx->dd;\n\n\trc = hfi1_netdev_allocate_ctxt(dd, ctxt);\n\tif (rc) {\n\t\tdd_dev_err(dd, \"netdev ctxt alloc failed %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\trc = hfi1_netdev_setup_ctxt(rx, *ctxt);\n\tif (rc) {\n\t\tdd_dev_err(dd, \"netdev ctxt setup failed %d\\n\", rc);\n\t\thfi1_netdev_deallocate_ctxt(dd, *ctxt);\n\t\t*ctxt = NULL;\n\t}\n\n\treturn rc;\n}\n\n \nu32 hfi1_num_netdev_contexts(struct hfi1_devdata *dd, u32 available_contexts,\n\t\t\t     struct cpumask *cpu_mask)\n{\n\tcpumask_var_t node_cpu_mask;\n\tunsigned int available_cpus;\n\n\tif (!HFI1_CAP_IS_KSET(AIP))\n\t\treturn 0;\n\n\t \n\tif (available_contexts == 0) {\n\t\tdd_dev_info(dd, \"No receive contexts available for netdevs.\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!zalloc_cpumask_var(&node_cpu_mask, GFP_KERNEL)) {\n\t\tdd_dev_err(dd, \"Unable to allocate cpu_mask for netdevs.\\n\");\n\t\treturn 0;\n\t}\n\n\tcpumask_and(node_cpu_mask, cpu_mask, cpumask_of_node(dd->node));\n\n\tavailable_cpus = cpumask_weight(node_cpu_mask);\n\n\tfree_cpumask_var(node_cpu_mask);\n\n\treturn min3(available_cpus, available_contexts,\n\t\t    (u32)HFI1_MAX_NETDEV_CTXTS);\n}\n\nstatic int hfi1_netdev_rxq_init(struct hfi1_netdev_rx *rx)\n{\n\tint i;\n\tint rc;\n\tstruct hfi1_devdata *dd = rx->dd;\n\tstruct net_device *dev = &rx->rx_napi;\n\n\trx->num_rx_q = dd->num_netdev_contexts;\n\trx->rxq = kcalloc_node(rx->num_rx_q, sizeof(*rx->rxq),\n\t\t\t       GFP_KERNEL, dd->node);\n\n\tif (!rx->rxq) {\n\t\tdd_dev_err(dd, \"Unable to allocate netdev queue data\\n\");\n\t\treturn (-ENOMEM);\n\t}\n\n\tfor (i = 0; i < rx->num_rx_q; i++) {\n\t\tstruct hfi1_netdev_rxq *rxq = &rx->rxq[i];\n\n\t\trc = hfi1_netdev_allot_ctxt(rx, &rxq->rcd);\n\t\tif (rc)\n\t\t\tgoto bail_context_irq_failure;\n\n\t\thfi1_rcd_get(rxq->rcd);\n\t\trxq->rx = rx;\n\t\trxq->rcd->napi = &rxq->napi;\n\t\tdd_dev_info(dd, \"Setting rcv queue %d napi to context %d\\n\",\n\t\t\t    i, rxq->rcd->ctxt);\n\t\t \n\t\tset_bit(NAPI_STATE_NO_BUSY_POLL, &rxq->napi.state);\n\t\tnetif_napi_add(dev, &rxq->napi, hfi1_netdev_rx_napi);\n\t\trc = msix_netdev_request_rcd_irq(rxq->rcd);\n\t\tif (rc)\n\t\t\tgoto bail_context_irq_failure;\n\t}\n\n\treturn 0;\n\nbail_context_irq_failure:\n\tdd_dev_err(dd, \"Unable to allot receive context\\n\");\n\tfor (; i >= 0; i--) {\n\t\tstruct hfi1_netdev_rxq *rxq = &rx->rxq[i];\n\n\t\tif (rxq->rcd) {\n\t\t\thfi1_netdev_deallocate_ctxt(dd, rxq->rcd);\n\t\t\thfi1_rcd_put(rxq->rcd);\n\t\t\trxq->rcd = NULL;\n\t\t}\n\t}\n\tkfree(rx->rxq);\n\trx->rxq = NULL;\n\n\treturn rc;\n}\n\nstatic void hfi1_netdev_rxq_deinit(struct hfi1_netdev_rx *rx)\n{\n\tint i;\n\tstruct hfi1_devdata *dd = rx->dd;\n\n\tfor (i = 0; i < rx->num_rx_q; i++) {\n\t\tstruct hfi1_netdev_rxq *rxq = &rx->rxq[i];\n\n\t\tnetif_napi_del(&rxq->napi);\n\t\thfi1_netdev_deallocate_ctxt(dd, rxq->rcd);\n\t\thfi1_rcd_put(rxq->rcd);\n\t\trxq->rcd = NULL;\n\t}\n\n\tkfree(rx->rxq);\n\trx->rxq = NULL;\n\trx->num_rx_q = 0;\n}\n\nstatic void enable_queues(struct hfi1_netdev_rx *rx)\n{\n\tint i;\n\n\tfor (i = 0; i < rx->num_rx_q; i++) {\n\t\tstruct hfi1_netdev_rxq *rxq = &rx->rxq[i];\n\n\t\tdd_dev_info(rx->dd, \"enabling queue %d on context %d\\n\", i,\n\t\t\t    rxq->rcd->ctxt);\n\t\tnapi_enable(&rxq->napi);\n\t\thfi1_rcvctrl(rx->dd,\n\t\t\t     HFI1_RCVCTRL_CTXT_ENB | HFI1_RCVCTRL_INTRAVAIL_ENB,\n\t\t\t     rxq->rcd);\n\t}\n}\n\nstatic void disable_queues(struct hfi1_netdev_rx *rx)\n{\n\tint i;\n\n\tmsix_netdev_synchronize_irq(rx->dd);\n\n\tfor (i = 0; i < rx->num_rx_q; i++) {\n\t\tstruct hfi1_netdev_rxq *rxq = &rx->rxq[i];\n\n\t\tdd_dev_info(rx->dd, \"disabling queue %d on context %d\\n\", i,\n\t\t\t    rxq->rcd->ctxt);\n\n\t\t \n\t\thfi1_rcvctrl(rx->dd,\n\t\t\t     HFI1_RCVCTRL_CTXT_DIS | HFI1_RCVCTRL_INTRAVAIL_DIS,\n\t\t\t     rxq->rcd);\n\t\tnapi_synchronize(&rxq->napi);\n\t\tnapi_disable(&rxq->napi);\n\t}\n}\n\n \nint hfi1_netdev_rx_init(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\tint res;\n\n\tif (atomic_fetch_inc(&rx->netdevs))\n\t\treturn 0;\n\n\tmutex_lock(&hfi1_mutex);\n\tres = hfi1_netdev_rxq_init(rx);\n\tmutex_unlock(&hfi1_mutex);\n\treturn res;\n}\n\n \nint hfi1_netdev_rx_destroy(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\n\t \n\tif (atomic_fetch_add_unless(&rx->netdevs, -1, 0) == 1) {\n\t\tmutex_lock(&hfi1_mutex);\n\t\thfi1_netdev_rxq_deinit(rx);\n\t\tmutex_unlock(&hfi1_mutex);\n\t}\n\n\treturn 0;\n}\n\n \nint hfi1_alloc_rx(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_netdev_rx *rx;\n\n\tdd_dev_info(dd, \"allocating rx size %ld\\n\", sizeof(*rx));\n\trx = kzalloc_node(sizeof(*rx), GFP_KERNEL, dd->node);\n\n\tif (!rx)\n\t\treturn -ENOMEM;\n\trx->dd = dd;\n\tinit_dummy_netdev(&rx->rx_napi);\n\n\txa_init(&rx->dev_tbl);\n\tatomic_set(&rx->enabled, 0);\n\tatomic_set(&rx->netdevs, 0);\n\tdd->netdev_rx = rx;\n\n\treturn 0;\n}\n\nvoid hfi1_free_rx(struct hfi1_devdata *dd)\n{\n\tif (dd->netdev_rx) {\n\t\tdd_dev_info(dd, \"hfi1 rx freed\\n\");\n\t\tkfree(dd->netdev_rx);\n\t\tdd->netdev_rx = NULL;\n\t}\n}\n\n \nvoid hfi1_netdev_enable_queues(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_netdev_rx *rx;\n\n\tif (!dd->netdev_rx)\n\t\treturn;\n\n\trx = dd->netdev_rx;\n\tif (atomic_fetch_inc(&rx->enabled))\n\t\treturn;\n\n\tmutex_lock(&hfi1_mutex);\n\tenable_queues(rx);\n\tmutex_unlock(&hfi1_mutex);\n}\n\nvoid hfi1_netdev_disable_queues(struct hfi1_devdata *dd)\n{\n\tstruct hfi1_netdev_rx *rx;\n\n\tif (!dd->netdev_rx)\n\t\treturn;\n\n\trx = dd->netdev_rx;\n\tif (atomic_dec_if_positive(&rx->enabled))\n\t\treturn;\n\n\tmutex_lock(&hfi1_mutex);\n\tdisable_queues(rx);\n\tmutex_unlock(&hfi1_mutex);\n}\n\n \nint hfi1_netdev_add_data(struct hfi1_devdata *dd, int id, void *data)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\n\treturn xa_insert(&rx->dev_tbl, id, data, GFP_NOWAIT);\n}\n\n \nvoid *hfi1_netdev_remove_data(struct hfi1_devdata *dd, int id)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\n\treturn xa_erase(&rx->dev_tbl, id);\n}\n\n \nvoid *hfi1_netdev_get_data(struct hfi1_devdata *dd, int id)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\n\treturn xa_load(&rx->dev_tbl, id);\n}\n\n \nvoid *hfi1_netdev_get_first_data(struct hfi1_devdata *dd, int *start_id)\n{\n\tstruct hfi1_netdev_rx *rx = dd->netdev_rx;\n\tunsigned long index = *start_id;\n\tvoid *ret;\n\n\tret = xa_find(&rx->dev_tbl, &index, UINT_MAX, XA_PRESENT);\n\t*start_id = (int)index;\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}