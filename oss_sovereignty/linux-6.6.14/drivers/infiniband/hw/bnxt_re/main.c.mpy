{
  "module_name": "main.c",
  "hash_id": "b08f0f0d52df998a995fc1fac9b7c1002250444b16d5c4e3dd16a8ce839db7b3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/bnxt_re/main.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/ethtool.h>\n#include <linux/mutex.h>\n#include <linux/list.h>\n#include <linux/rculist.h>\n#include <linux/spinlock.h>\n#include <linux/pci.h>\n#include <net/dcbnl.h>\n#include <net/ipv6.h>\n#include <net/addrconf.h>\n#include <linux/if_ether.h>\n#include <linux/auxiliary_bus.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_addr.h>\n\n#include \"bnxt_ulp.h\"\n#include \"roce_hsi.h\"\n#include \"qplib_res.h\"\n#include \"qplib_sp.h\"\n#include \"qplib_fp.h\"\n#include \"qplib_rcfw.h\"\n#include \"bnxt_re.h\"\n#include \"ib_verbs.h\"\n#include <rdma/bnxt_re-abi.h>\n#include \"bnxt.h\"\n#include \"hw_counters.h\"\n\nstatic char version[] =\n\t\tBNXT_RE_DESC \"\\n\";\n\nMODULE_AUTHOR(\"Eddie Wai <eddie.wai@broadcom.com>\");\nMODULE_DESCRIPTION(BNXT_RE_DESC);\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n \nstatic DEFINE_MUTEX(bnxt_re_mutex);\n\nstatic void bnxt_re_stop_irq(void *handle);\nstatic void bnxt_re_dev_stop(struct bnxt_re_dev *rdev);\nstatic int bnxt_re_netdev_event(struct notifier_block *notifier,\n\t\t\t\tunsigned long event, void *ptr);\nstatic struct bnxt_re_dev *bnxt_re_from_netdev(struct net_device *netdev);\nstatic void bnxt_re_dev_uninit(struct bnxt_re_dev *rdev);\nstatic int bnxt_re_hwrm_qcaps(struct bnxt_re_dev *rdev);\n\nstatic int bnxt_re_hwrm_qcfg(struct bnxt_re_dev *rdev, u32 *db_len,\n\t\t\t     u32 *offset);\nstatic void bnxt_re_set_db_offset(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_chip_ctx *cctx;\n\tstruct bnxt_en_dev *en_dev;\n\tstruct bnxt_qplib_res *res;\n\tu32 l2db_len = 0;\n\tu32 offset = 0;\n\tu32 barlen;\n\tint rc;\n\n\tres = &rdev->qplib_res;\n\ten_dev = rdev->en_dev;\n\tcctx = rdev->chip_ctx;\n\n\t \n\trc = bnxt_re_hwrm_qcfg(rdev, &l2db_len, &offset);\n\tif (rc)\n\t\tdev_info(rdev_to_dev(rdev),\n\t\t\t \"Couldn't get DB bar size, Low latency framework is disabled\\n\");\n\t \n\tres->dpi_tbl.ucreg.offset = res->is_vf ? BNXT_QPLIB_DBR_VF_DB_OFFSET :\n\t\t\t\t\t\t BNXT_QPLIB_DBR_PF_DB_OFFSET;\n\tres->dpi_tbl.wcreg.offset = res->dpi_tbl.ucreg.offset;\n\n\t \n\tbarlen = pci_resource_len(res->pdev, RCFW_DBR_PCI_BAR_REGION);\n\tif (cctx->modes.db_push && l2db_len && en_dev->l2_db_size != barlen) {\n\t\tres->dpi_tbl.wcreg.offset = en_dev->l2_db_size;\n\t\tdev_info(rdev_to_dev(rdev),  \"Low latency framework is enabled\\n\");\n\t}\n}\n\nstatic void bnxt_re_set_drv_mode(struct bnxt_re_dev *rdev, u8 mode)\n{\n\tstruct bnxt_qplib_chip_ctx *cctx;\n\n\tcctx = rdev->chip_ctx;\n\tcctx->modes.wqe_mode = bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx) ?\n\t\t\t       mode : BNXT_QPLIB_WQE_MODE_STATIC;\n\tif (bnxt_re_hwrm_qcaps(rdev))\n\t\tdev_err(rdev_to_dev(rdev),\n\t\t\t\"Failed to query hwrm qcaps\\n\");\n}\n\nstatic void bnxt_re_destroy_chip_ctx(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_chip_ctx *chip_ctx;\n\n\tif (!rdev->chip_ctx)\n\t\treturn;\n\tchip_ctx = rdev->chip_ctx;\n\trdev->chip_ctx = NULL;\n\trdev->rcfw.res = NULL;\n\trdev->qplib_res.cctx = NULL;\n\trdev->qplib_res.pdev = NULL;\n\trdev->qplib_res.netdev = NULL;\n\tkfree(chip_ctx);\n}\n\nstatic int bnxt_re_setup_chip_ctx(struct bnxt_re_dev *rdev, u8 wqe_mode)\n{\n\tstruct bnxt_qplib_chip_ctx *chip_ctx;\n\tstruct bnxt_en_dev *en_dev;\n\tint rc;\n\n\ten_dev = rdev->en_dev;\n\n\tchip_ctx = kzalloc(sizeof(*chip_ctx), GFP_KERNEL);\n\tif (!chip_ctx)\n\t\treturn -ENOMEM;\n\tchip_ctx->chip_num = en_dev->chip_num;\n\tchip_ctx->hw_stats_size = en_dev->hw_ring_stats_size;\n\n\trdev->chip_ctx = chip_ctx;\n\t \n\n\trdev->qplib_res.cctx = rdev->chip_ctx;\n\trdev->rcfw.res = &rdev->qplib_res;\n\trdev->qplib_res.dattr = &rdev->dev_attr;\n\trdev->qplib_res.is_vf = BNXT_EN_VF(en_dev);\n\n\tbnxt_re_set_drv_mode(rdev, wqe_mode);\n\n\tbnxt_re_set_db_offset(rdev);\n\trc = bnxt_qplib_map_db_bar(&rdev->qplib_res);\n\tif (rc)\n\t\treturn rc;\n\n\tif (bnxt_qplib_determine_atomics(en_dev->pdev))\n\t\tibdev_info(&rdev->ibdev,\n\t\t\t   \"platform doesn't support global atomics.\");\n\treturn 0;\n}\n\n \n\nstatic void bnxt_re_get_sriov_func_type(struct bnxt_re_dev *rdev)\n{\n\tif (BNXT_EN_VF(rdev->en_dev))\n\t\trdev->is_virtfn = 1;\n}\n\n \nstatic void bnxt_re_limit_pf_res(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_dev_attr *attr;\n\tstruct bnxt_qplib_ctx *ctx;\n\tint i;\n\n\tattr = &rdev->dev_attr;\n\tctx = &rdev->qplib_ctx;\n\n\tctx->qpc_count = min_t(u32, BNXT_RE_MAX_QPC_COUNT,\n\t\t\t       attr->max_qp);\n\tctx->mrw_count = BNXT_RE_MAX_MRW_COUNT_256K;\n\t \n\tctx->mrw_count = min_t(u32, ctx->mrw_count, attr->max_mr);\n\tctx->srqc_count = min_t(u32, BNXT_RE_MAX_SRQC_COUNT,\n\t\t\t\tattr->max_srq);\n\tctx->cq_count = min_t(u32, BNXT_RE_MAX_CQ_COUNT, attr->max_cq);\n\tif (!bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx))\n\t\tfor (i = 0; i < MAX_TQM_ALLOC_REQ; i++)\n\t\t\trdev->qplib_ctx.tqm_ctx.qcount[i] =\n\t\t\trdev->dev_attr.tqm_alloc_reqs[i];\n}\n\nstatic void bnxt_re_limit_vf_res(struct bnxt_qplib_ctx *qplib_ctx, u32 num_vf)\n{\n\tstruct bnxt_qplib_vf_res *vf_res;\n\tu32 mrws = 0;\n\tu32 vf_pct;\n\tu32 nvfs;\n\n\tvf_res = &qplib_ctx->vf_res;\n\t \n\tvf_pct = 100 - BNXT_RE_PCT_RSVD_FOR_PF;\n\tnvfs = num_vf;\n\tnum_vf = 100 * num_vf;\n\tvf_res->max_qp_per_vf = (qplib_ctx->qpc_count * vf_pct) / num_vf;\n\tvf_res->max_srq_per_vf = (qplib_ctx->srqc_count * vf_pct) / num_vf;\n\tvf_res->max_cq_per_vf = (qplib_ctx->cq_count * vf_pct) / num_vf;\n\t \n\tif (qplib_ctx->mrw_count < BNXT_RE_MAX_MRW_COUNT_64K) {\n\t\tmrws = qplib_ctx->mrw_count * vf_pct;\n\t\tnvfs = num_vf;\n\t} else {\n\t\tmrws = qplib_ctx->mrw_count - BNXT_RE_RESVD_MR_FOR_PF;\n\t}\n\tvf_res->max_mrw_per_vf = (mrws / nvfs);\n\tvf_res->max_gid_per_vf = BNXT_RE_MAX_GID_PER_VF;\n}\n\nstatic void bnxt_re_set_resource_limits(struct bnxt_re_dev *rdev)\n{\n\tu32 num_vfs;\n\n\tmemset(&rdev->qplib_ctx.vf_res, 0, sizeof(struct bnxt_qplib_vf_res));\n\tbnxt_re_limit_pf_res(rdev);\n\n\tnum_vfs =  bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx) ?\n\t\t\tBNXT_RE_GEN_P5_MAX_VF : rdev->num_vfs;\n\tif (num_vfs)\n\t\tbnxt_re_limit_vf_res(&rdev->qplib_ctx, num_vfs);\n}\n\nstatic void bnxt_re_vf_res_config(struct bnxt_re_dev *rdev)\n{\n\n\tif (test_bit(BNXT_RE_FLAG_ERR_DEVICE_DETACHED, &rdev->flags))\n\t\treturn;\n\trdev->num_vfs = pci_sriov_get_totalvfs(rdev->en_dev->pdev);\n\tif (!bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx)) {\n\t\tbnxt_re_set_resource_limits(rdev);\n\t\tbnxt_qplib_set_func_resources(&rdev->qplib_res, &rdev->rcfw,\n\t\t\t\t\t      &rdev->qplib_ctx);\n\t}\n}\n\nstatic void bnxt_re_shutdown(struct auxiliary_device *adev)\n{\n\tstruct bnxt_re_dev *rdev = auxiliary_get_drvdata(adev);\n\n\tif (!rdev)\n\t\treturn;\n\tib_unregister_device(&rdev->ibdev);\n\tbnxt_re_dev_uninit(rdev);\n}\n\nstatic void bnxt_re_stop_irq(void *handle)\n{\n\tstruct bnxt_re_dev *rdev = (struct bnxt_re_dev *)handle;\n\tstruct bnxt_qplib_rcfw *rcfw = &rdev->rcfw;\n\tstruct bnxt_qplib_nq *nq;\n\tint indx;\n\n\tfor (indx = BNXT_RE_NQ_IDX; indx < rdev->num_msix; indx++) {\n\t\tnq = &rdev->nq[indx - 1];\n\t\tbnxt_qplib_nq_stop_irq(nq, false);\n\t}\n\n\tbnxt_qplib_rcfw_stop_irq(rcfw, false);\n}\n\nstatic void bnxt_re_start_irq(void *handle, struct bnxt_msix_entry *ent)\n{\n\tstruct bnxt_re_dev *rdev = (struct bnxt_re_dev *)handle;\n\tstruct bnxt_msix_entry *msix_ent = rdev->en_dev->msix_entries;\n\tstruct bnxt_qplib_rcfw *rcfw = &rdev->rcfw;\n\tstruct bnxt_qplib_nq *nq;\n\tint indx, rc;\n\n\tif (!ent) {\n\t\t \n\t\tibdev_err(&rdev->ibdev, \"Failed to re-start IRQs\\n\");\n\t\treturn;\n\t}\n\n\t \n\tfor (indx = 0; indx < rdev->num_msix; indx++)\n\t\trdev->en_dev->msix_entries[indx].vector = ent[indx].vector;\n\n\trc = bnxt_qplib_rcfw_start_irq(rcfw, msix_ent[BNXT_RE_AEQ_IDX].vector,\n\t\t\t\t       false);\n\tif (rc) {\n\t\tibdev_warn(&rdev->ibdev, \"Failed to reinit CREQ\\n\");\n\t\treturn;\n\t}\n\tfor (indx = BNXT_RE_NQ_IDX ; indx < rdev->num_msix; indx++) {\n\t\tnq = &rdev->nq[indx - 1];\n\t\trc = bnxt_qplib_nq_start_irq(nq, indx - 1,\n\t\t\t\t\t     msix_ent[indx].vector, false);\n\t\tif (rc) {\n\t\t\tibdev_warn(&rdev->ibdev, \"Failed to reinit NQ index %d\\n\",\n\t\t\t\t   indx - 1);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic struct bnxt_ulp_ops bnxt_re_ulp_ops = {\n\t.ulp_irq_stop = bnxt_re_stop_irq,\n\t.ulp_irq_restart = bnxt_re_start_irq\n};\n\n \n\nstatic int bnxt_re_register_netdev(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_en_dev *en_dev;\n\tint rc;\n\n\ten_dev = rdev->en_dev;\n\n\trc = bnxt_register_dev(en_dev, &bnxt_re_ulp_ops, rdev);\n\tif (!rc)\n\t\trdev->qplib_res.pdev = rdev->en_dev->pdev;\n\treturn rc;\n}\n\nstatic void bnxt_re_init_hwrm_hdr(struct input *hdr, u16 opcd)\n{\n\thdr->req_type = cpu_to_le16(opcd);\n\thdr->cmpl_ring = cpu_to_le16(-1);\n\thdr->target_id = cpu_to_le16(-1);\n}\n\nstatic void bnxt_re_fill_fw_msg(struct bnxt_fw_msg *fw_msg, void *msg,\n\t\t\t\tint msg_len, void *resp, int resp_max_len,\n\t\t\t\tint timeout)\n{\n\tfw_msg->msg = msg;\n\tfw_msg->msg_len = msg_len;\n\tfw_msg->resp = resp;\n\tfw_msg->resp_max_len = resp_max_len;\n\tfw_msg->timeout = timeout;\n}\n\n \nstatic int bnxt_re_hwrm_qcfg(struct bnxt_re_dev *rdev, u32 *db_len,\n\t\t\t     u32 *offset)\n{\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct hwrm_func_qcfg_output resp = {0};\n\tstruct hwrm_func_qcfg_input req = {0};\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_FUNC_QCFG);\n\treq.fid = cpu_to_le16(0xffff);\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (!rc) {\n\t\t*db_len = PAGE_ALIGN(le16_to_cpu(resp.l2_doorbell_bar_size_kb) * 1024);\n\t\t*offset = PAGE_ALIGN(le16_to_cpu(resp.legacy_l2_db_size_kb) * 1024);\n\t}\n\treturn rc;\n}\n\n \nint bnxt_re_hwrm_qcaps(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct hwrm_func_qcaps_output resp = {};\n\tstruct hwrm_func_qcaps_input req = {};\n\tstruct bnxt_qplib_chip_ctx *cctx;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc;\n\n\tcctx = rdev->chip_ctx;\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_FUNC_QCAPS);\n\treq.fid = cpu_to_le16(0xffff);\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (rc)\n\t\treturn rc;\n\tcctx->modes.db_push = le32_to_cpu(resp.flags) & FUNC_QCAPS_RESP_FLAGS_WCB_PUSH_MODE;\n\n\tcctx->modes.dbr_pacing =\n\t\tle32_to_cpu(resp.flags_ext2) &\n\t\tFUNC_QCAPS_RESP_FLAGS_EXT2_DBR_PACING_EXT_SUPPORTED;\n\treturn 0;\n}\n\nstatic int bnxt_re_hwrm_dbr_pacing_qcfg(struct bnxt_re_dev *rdev)\n{\n\tstruct hwrm_func_dbr_pacing_qcfg_output resp = {};\n\tstruct hwrm_func_dbr_pacing_qcfg_input req = {};\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct bnxt_qplib_chip_ctx *cctx;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc;\n\n\tcctx = rdev->chip_ctx;\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_FUNC_DBR_PACING_QCFG);\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (rc)\n\t\treturn rc;\n\n\tif ((le32_to_cpu(resp.dbr_stat_db_fifo_reg) &\n\t    FUNC_DBR_PACING_QCFG_RESP_DBR_STAT_DB_FIFO_REG_ADDR_SPACE_MASK) ==\n\t\tFUNC_DBR_PACING_QCFG_RESP_DBR_STAT_DB_FIFO_REG_ADDR_SPACE_GRC)\n\t\tcctx->dbr_stat_db_fifo =\n\t\t\tle32_to_cpu(resp.dbr_stat_db_fifo_reg) &\n\t\t\t~FUNC_DBR_PACING_QCFG_RESP_DBR_STAT_DB_FIFO_REG_ADDR_SPACE_MASK;\n\treturn 0;\n}\n\n \nstatic void bnxt_re_set_default_pacing_data(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_db_pacing_data *pacing_data = rdev->qplib_res.pacing_data;\n\n\tpacing_data->do_pacing = rdev->pacing.dbr_def_do_pacing;\n\tpacing_data->pacing_th = rdev->pacing.pacing_algo_th;\n\tpacing_data->alarm_th =\n\t\tpacing_data->pacing_th * BNXT_RE_PACING_ALARM_TH_MULTIPLE;\n}\n\nstatic void __wait_for_fifo_occupancy_below_th(struct bnxt_re_dev *rdev)\n{\n\tu32 read_val, fifo_occup;\n\n\t \n\twhile (1) {\n\t\tread_val = readl(rdev->en_dev->bar0 + rdev->pacing.dbr_db_fifo_reg_off);\n\t\tfifo_occup = BNXT_RE_MAX_FIFO_DEPTH -\n\t\t\t((read_val & BNXT_RE_DB_FIFO_ROOM_MASK) >>\n\t\t\t BNXT_RE_DB_FIFO_ROOM_SHIFT);\n\t\t \n\t\tif (fifo_occup > BNXT_RE_MAX_FIFO_DEPTH)\n\t\t\tbreak;\n\n\t\tif (fifo_occup < rdev->qplib_res.pacing_data->pacing_th)\n\t\t\tbreak;\n\t}\n}\n\nstatic void bnxt_re_db_fifo_check(struct work_struct *work)\n{\n\tstruct bnxt_re_dev *rdev = container_of(work, struct bnxt_re_dev,\n\t\t\tdbq_fifo_check_work);\n\tstruct bnxt_qplib_db_pacing_data *pacing_data;\n\tu32 pacing_save;\n\n\tif (!mutex_trylock(&rdev->pacing.dbq_lock))\n\t\treturn;\n\tpacing_data = rdev->qplib_res.pacing_data;\n\tpacing_save = rdev->pacing.do_pacing_save;\n\t__wait_for_fifo_occupancy_below_th(rdev);\n\tcancel_delayed_work_sync(&rdev->dbq_pacing_work);\n\tif (pacing_save > rdev->pacing.dbr_def_do_pacing) {\n\t\t \n\t\tpacing_save = pacing_save << 1;\n\t} else {\n\t\t \n\t\tpacing_save = pacing_save << 3;\n\t\tpacing_data->pacing_th = rdev->pacing.pacing_algo_th * 4;\n\t}\n\n\tif (pacing_save > BNXT_RE_MAX_DBR_DO_PACING)\n\t\tpacing_save = BNXT_RE_MAX_DBR_DO_PACING;\n\n\tpacing_data->do_pacing = pacing_save;\n\trdev->pacing.do_pacing_save = pacing_data->do_pacing;\n\tpacing_data->alarm_th =\n\t\tpacing_data->pacing_th * BNXT_RE_PACING_ALARM_TH_MULTIPLE;\n\tschedule_delayed_work(&rdev->dbq_pacing_work,\n\t\t\t      msecs_to_jiffies(rdev->pacing.dbq_pacing_time));\n\trdev->stats.pacing.alerts++;\n\tmutex_unlock(&rdev->pacing.dbq_lock);\n}\n\nstatic void bnxt_re_pacing_timer_exp(struct work_struct *work)\n{\n\tstruct bnxt_re_dev *rdev = container_of(work, struct bnxt_re_dev,\n\t\t\tdbq_pacing_work.work);\n\tstruct bnxt_qplib_db_pacing_data *pacing_data;\n\tu32 read_val, fifo_occup;\n\n\tif (!mutex_trylock(&rdev->pacing.dbq_lock))\n\t\treturn;\n\n\tpacing_data = rdev->qplib_res.pacing_data;\n\tread_val = readl(rdev->en_dev->bar0 + rdev->pacing.dbr_db_fifo_reg_off);\n\tfifo_occup = BNXT_RE_MAX_FIFO_DEPTH -\n\t\t((read_val & BNXT_RE_DB_FIFO_ROOM_MASK) >>\n\t\t BNXT_RE_DB_FIFO_ROOM_SHIFT);\n\n\tif (fifo_occup > pacing_data->pacing_th)\n\t\tgoto restart_timer;\n\n\t \n\tpacing_data->do_pacing = pacing_data->do_pacing - (pacing_data->do_pacing >> 3);\n\tpacing_data->do_pacing = max_t(u32, rdev->pacing.dbr_def_do_pacing, pacing_data->do_pacing);\n\tif (pacing_data->do_pacing <= rdev->pacing.dbr_def_do_pacing) {\n\t\tbnxt_re_set_default_pacing_data(rdev);\n\t\trdev->stats.pacing.complete++;\n\t\tgoto dbq_unlock;\n\t}\n\nrestart_timer:\n\tschedule_delayed_work(&rdev->dbq_pacing_work,\n\t\t\t      msecs_to_jiffies(rdev->pacing.dbq_pacing_time));\n\trdev->stats.pacing.resched++;\ndbq_unlock:\n\trdev->pacing.do_pacing_save = pacing_data->do_pacing;\n\tmutex_unlock(&rdev->pacing.dbq_lock);\n}\n\nvoid bnxt_re_pacing_alert(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_db_pacing_data *pacing_data;\n\n\tif (!rdev->pacing.dbr_pacing)\n\t\treturn;\n\tmutex_lock(&rdev->pacing.dbq_lock);\n\tpacing_data = rdev->qplib_res.pacing_data;\n\n\t \n\tpacing_data->alarm_th = BNXT_RE_MAX_FIFO_DEPTH;\n\tpacing_data->do_pacing = BNXT_RE_MAX_DBR_DO_PACING;\n\tcancel_work_sync(&rdev->dbq_fifo_check_work);\n\tschedule_work(&rdev->dbq_fifo_check_work);\n\tmutex_unlock(&rdev->pacing.dbq_lock);\n}\n\nstatic int bnxt_re_initialize_dbr_pacing(struct bnxt_re_dev *rdev)\n{\n\tif (bnxt_re_hwrm_dbr_pacing_qcfg(rdev))\n\t\treturn -EIO;\n\n\t \n\trdev->pacing.dbr_page = (void *)__get_free_page(GFP_KERNEL);\n\tif (!rdev->pacing.dbr_page)\n\t\treturn -ENOMEM;\n\n\tmemset((u8 *)rdev->pacing.dbr_page, 0, PAGE_SIZE);\n\trdev->qplib_res.pacing_data = (struct bnxt_qplib_db_pacing_data *)rdev->pacing.dbr_page;\n\n\t \n\twritel(rdev->chip_ctx->dbr_stat_db_fifo & BNXT_GRC_BASE_MASK,\n\t       rdev->en_dev->bar0 + BNXT_GRCPF_REG_WINDOW_BASE_OUT + 4);\n\trdev->pacing.dbr_db_fifo_reg_off =\n\t\t(rdev->chip_ctx->dbr_stat_db_fifo & BNXT_GRC_OFFSET_MASK) +\n\t\t BNXT_RE_GRC_FIFO_REG_BASE;\n\trdev->pacing.dbr_bar_addr =\n\t\tpci_resource_start(rdev->qplib_res.pdev, 0) + rdev->pacing.dbr_db_fifo_reg_off;\n\n\trdev->pacing.pacing_algo_th = BNXT_RE_PACING_ALGO_THRESHOLD;\n\trdev->pacing.dbq_pacing_time = BNXT_RE_DBR_PACING_TIME;\n\trdev->pacing.dbr_def_do_pacing = BNXT_RE_DBR_DO_PACING_NO_CONGESTION;\n\trdev->pacing.do_pacing_save = rdev->pacing.dbr_def_do_pacing;\n\trdev->qplib_res.pacing_data->fifo_max_depth = BNXT_RE_MAX_FIFO_DEPTH;\n\trdev->qplib_res.pacing_data->fifo_room_mask = BNXT_RE_DB_FIFO_ROOM_MASK;\n\trdev->qplib_res.pacing_data->fifo_room_shift = BNXT_RE_DB_FIFO_ROOM_SHIFT;\n\trdev->qplib_res.pacing_data->grc_reg_offset = rdev->pacing.dbr_db_fifo_reg_off;\n\tbnxt_re_set_default_pacing_data(rdev);\n\t \n\tINIT_WORK(&rdev->dbq_fifo_check_work, bnxt_re_db_fifo_check);\n\tINIT_DELAYED_WORK(&rdev->dbq_pacing_work, bnxt_re_pacing_timer_exp);\n\treturn 0;\n}\n\nstatic void bnxt_re_deinitialize_dbr_pacing(struct bnxt_re_dev *rdev)\n{\n\tcancel_work_sync(&rdev->dbq_fifo_check_work);\n\tcancel_delayed_work_sync(&rdev->dbq_pacing_work);\n\tif (rdev->pacing.dbr_page)\n\t\tfree_page((u64)rdev->pacing.dbr_page);\n\n\trdev->pacing.dbr_page = NULL;\n\trdev->pacing.dbr_pacing = false;\n}\n\nstatic int bnxt_re_net_ring_free(struct bnxt_re_dev *rdev,\n\t\t\t\t u16 fw_ring_id, int type)\n{\n\tstruct bnxt_en_dev *en_dev;\n\tstruct hwrm_ring_free_input req = {};\n\tstruct hwrm_ring_free_output resp;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc = -EINVAL;\n\n\tif (!rdev)\n\t\treturn rc;\n\n\ten_dev = rdev->en_dev;\n\n\tif (!en_dev)\n\t\treturn rc;\n\n\tif (test_bit(BNXT_RE_FLAG_ERR_DEVICE_DETACHED, &rdev->flags))\n\t\treturn 0;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_RING_FREE);\n\treq.ring_type = type;\n\treq.ring_id = cpu_to_le16(fw_ring_id);\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (rc)\n\t\tibdev_err(&rdev->ibdev, \"Failed to free HW ring:%d :%#x\",\n\t\t\t  req.ring_id, rc);\n\treturn rc;\n}\n\nstatic int bnxt_re_net_ring_alloc(struct bnxt_re_dev *rdev,\n\t\t\t\t  struct bnxt_re_ring_attr *ring_attr,\n\t\t\t\t  u16 *fw_ring_id)\n{\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct hwrm_ring_alloc_input req = {};\n\tstruct hwrm_ring_alloc_output resp;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc = -EINVAL;\n\n\tif (!en_dev)\n\t\treturn rc;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_RING_ALLOC);\n\treq.enables = 0;\n\treq.page_tbl_addr =  cpu_to_le64(ring_attr->dma_arr[0]);\n\tif (ring_attr->pages > 1) {\n\t\t \n\t\treq.page_size = BNXT_PAGE_SHIFT;\n\t\treq.page_tbl_depth = 1;\n\t}\n\treq.fbo = 0;\n\t \n\treq.logical_id = cpu_to_le16(ring_attr->lrid);\n\treq.length = cpu_to_le32(ring_attr->depth + 1);\n\treq.ring_type = ring_attr->type;\n\treq.int_mode = ring_attr->mode;\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (!rc)\n\t\t*fw_ring_id = le16_to_cpu(resp.ring_id);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_net_stats_ctx_free(struct bnxt_re_dev *rdev,\n\t\t\t\t      u32 fw_stats_ctx_id)\n{\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct hwrm_stat_ctx_free_input req = {};\n\tstruct hwrm_stat_ctx_free_output resp = {};\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc = -EINVAL;\n\n\tif (!en_dev)\n\t\treturn rc;\n\n\tif (test_bit(BNXT_RE_FLAG_ERR_DEVICE_DETACHED, &rdev->flags))\n\t\treturn 0;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_STAT_CTX_FREE);\n\treq.stat_ctx_id = cpu_to_le32(fw_stats_ctx_id);\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (rc)\n\t\tibdev_err(&rdev->ibdev, \"Failed to free HW stats context %#x\",\n\t\t\t  rc);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_net_stats_ctx_alloc(struct bnxt_re_dev *rdev,\n\t\t\t\t       dma_addr_t dma_map,\n\t\t\t\t       u32 *fw_stats_ctx_id)\n{\n\tstruct bnxt_qplib_chip_ctx *chip_ctx = rdev->chip_ctx;\n\tstruct hwrm_stat_ctx_alloc_output resp = {};\n\tstruct hwrm_stat_ctx_alloc_input req = {};\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc = -EINVAL;\n\n\t*fw_stats_ctx_id = INVALID_STATS_CTX_ID;\n\n\tif (!en_dev)\n\t\treturn rc;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_STAT_CTX_ALLOC);\n\treq.update_period_ms = cpu_to_le32(1000);\n\treq.stats_dma_addr = cpu_to_le64(dma_map);\n\treq.stats_dma_length = cpu_to_le16(chip_ctx->hw_stats_size);\n\treq.stat_ctx_flags = STAT_CTX_ALLOC_REQ_STAT_CTX_FLAGS_ROCE;\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (!rc)\n\t\t*fw_stats_ctx_id = le32_to_cpu(resp.stat_ctx_id);\n\n\treturn rc;\n}\n\nstatic void bnxt_re_disassociate_ucontext(struct ib_ucontext *ibcontext)\n{\n}\n\n \n\nstatic struct bnxt_re_dev *bnxt_re_from_netdev(struct net_device *netdev)\n{\n\tstruct ib_device *ibdev =\n\t\tib_device_get_by_netdev(netdev, RDMA_DRIVER_BNXT_RE);\n\tif (!ibdev)\n\t\treturn NULL;\n\n\treturn container_of(ibdev, struct bnxt_re_dev, ibdev);\n}\n\nstatic ssize_t hw_rev_show(struct device *device, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct bnxt_re_dev *rdev =\n\t\trdma_device_to_drv_device(device, struct bnxt_re_dev, ibdev);\n\n\treturn sysfs_emit(buf, \"0x%x\\n\", rdev->en_dev->pdev->vendor);\n}\nstatic DEVICE_ATTR_RO(hw_rev);\n\nstatic ssize_t hca_type_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct bnxt_re_dev *rdev =\n\t\trdma_device_to_drv_device(device, struct bnxt_re_dev, ibdev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", rdev->ibdev.node_desc);\n}\nstatic DEVICE_ATTR_RO(hca_type);\n\nstatic struct attribute *bnxt_re_attributes[] = {\n\t&dev_attr_hw_rev.attr,\n\t&dev_attr_hca_type.attr,\n\tNULL\n};\n\nstatic const struct attribute_group bnxt_re_dev_attr_group = {\n\t.attrs = bnxt_re_attributes,\n};\n\nstatic const struct ib_device_ops bnxt_re_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_BNXT_RE,\n\t.uverbs_abi_ver = BNXT_RE_ABI_VERSION,\n\n\t.add_gid = bnxt_re_add_gid,\n\t.alloc_hw_port_stats = bnxt_re_ib_alloc_hw_port_stats,\n\t.alloc_mr = bnxt_re_alloc_mr,\n\t.alloc_pd = bnxt_re_alloc_pd,\n\t.alloc_ucontext = bnxt_re_alloc_ucontext,\n\t.create_ah = bnxt_re_create_ah,\n\t.create_cq = bnxt_re_create_cq,\n\t.create_qp = bnxt_re_create_qp,\n\t.create_srq = bnxt_re_create_srq,\n\t.create_user_ah = bnxt_re_create_ah,\n\t.dealloc_pd = bnxt_re_dealloc_pd,\n\t.dealloc_ucontext = bnxt_re_dealloc_ucontext,\n\t.del_gid = bnxt_re_del_gid,\n\t.dereg_mr = bnxt_re_dereg_mr,\n\t.destroy_ah = bnxt_re_destroy_ah,\n\t.destroy_cq = bnxt_re_destroy_cq,\n\t.destroy_qp = bnxt_re_destroy_qp,\n\t.destroy_srq = bnxt_re_destroy_srq,\n\t.device_group = &bnxt_re_dev_attr_group,\n\t.disassociate_ucontext = bnxt_re_disassociate_ucontext,\n\t.get_dev_fw_str = bnxt_re_query_fw_str,\n\t.get_dma_mr = bnxt_re_get_dma_mr,\n\t.get_hw_stats = bnxt_re_ib_get_hw_stats,\n\t.get_link_layer = bnxt_re_get_link_layer,\n\t.get_port_immutable = bnxt_re_get_port_immutable,\n\t.map_mr_sg = bnxt_re_map_mr_sg,\n\t.mmap = bnxt_re_mmap,\n\t.mmap_free = bnxt_re_mmap_free,\n\t.modify_qp = bnxt_re_modify_qp,\n\t.modify_srq = bnxt_re_modify_srq,\n\t.poll_cq = bnxt_re_poll_cq,\n\t.post_recv = bnxt_re_post_recv,\n\t.post_send = bnxt_re_post_send,\n\t.post_srq_recv = bnxt_re_post_srq_recv,\n\t.query_ah = bnxt_re_query_ah,\n\t.query_device = bnxt_re_query_device,\n\t.query_pkey = bnxt_re_query_pkey,\n\t.query_port = bnxt_re_query_port,\n\t.query_qp = bnxt_re_query_qp,\n\t.query_srq = bnxt_re_query_srq,\n\t.reg_user_mr = bnxt_re_reg_user_mr,\n\t.reg_user_mr_dmabuf = bnxt_re_reg_user_mr_dmabuf,\n\t.req_notify_cq = bnxt_re_req_notify_cq,\n\t.resize_cq = bnxt_re_resize_cq,\n\tINIT_RDMA_OBJ_SIZE(ib_ah, bnxt_re_ah, ib_ah),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, bnxt_re_cq, ib_cq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, bnxt_re_pd, ib_pd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, bnxt_re_qp, ib_qp),\n\tINIT_RDMA_OBJ_SIZE(ib_srq, bnxt_re_srq, ib_srq),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, bnxt_re_ucontext, ib_uctx),\n};\n\nstatic int bnxt_re_register_ib(struct bnxt_re_dev *rdev)\n{\n\tstruct ib_device *ibdev = &rdev->ibdev;\n\tint ret;\n\n\t \n\tibdev->node_type = RDMA_NODE_IB_CA;\n\tstrscpy(ibdev->node_desc, BNXT_RE_DESC \" HCA\",\n\t\tstrlen(BNXT_RE_DESC) + 5);\n\tibdev->phys_port_cnt = 1;\n\n\taddrconf_addr_eui48((u8 *)&ibdev->node_guid, rdev->netdev->dev_addr);\n\n\tibdev->num_comp_vectors\t= rdev->num_msix - 1;\n\tibdev->dev.parent = &rdev->en_dev->pdev->dev;\n\tibdev->local_dma_lkey = BNXT_QPLIB_RSVD_LKEY;\n\n\tif (IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS))\n\t\tibdev->driver_def = bnxt_re_uapi_defs;\n\n\tib_set_device_ops(ibdev, &bnxt_re_dev_ops);\n\tret = ib_device_set_netdev(&rdev->ibdev, rdev->netdev, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tdma_set_max_seg_size(&rdev->en_dev->pdev->dev, UINT_MAX);\n\tibdev->uverbs_cmd_mask |= BIT_ULL(IB_USER_VERBS_CMD_POLL_CQ);\n\treturn ib_register_device(ibdev, \"bnxt_re%d\", &rdev->en_dev->pdev->dev);\n}\n\nstatic struct bnxt_re_dev *bnxt_re_dev_add(struct bnxt_aux_priv *aux_priv,\n\t\t\t\t\t   struct bnxt_en_dev *en_dev)\n{\n\tstruct bnxt_re_dev *rdev;\n\n\t \n\trdev = ib_alloc_device(bnxt_re_dev, ibdev);\n\tif (!rdev) {\n\t\tibdev_err(NULL, \"%s: bnxt_re_dev allocation failure!\",\n\t\t\t  ROCE_DRV_MODULE_NAME);\n\t\treturn NULL;\n\t}\n\t \n\trdev->nb.notifier_call = NULL;\n\trdev->netdev = en_dev->net;\n\trdev->en_dev = en_dev;\n\trdev->id = rdev->en_dev->pdev->devfn;\n\tINIT_LIST_HEAD(&rdev->qp_list);\n\tmutex_init(&rdev->qp_lock);\n\tmutex_init(&rdev->pacing.dbq_lock);\n\tatomic_set(&rdev->stats.res.qp_count, 0);\n\tatomic_set(&rdev->stats.res.cq_count, 0);\n\tatomic_set(&rdev->stats.res.srq_count, 0);\n\tatomic_set(&rdev->stats.res.mr_count, 0);\n\tatomic_set(&rdev->stats.res.mw_count, 0);\n\tatomic_set(&rdev->stats.res.ah_count, 0);\n\tatomic_set(&rdev->stats.res.pd_count, 0);\n\trdev->cosq[0] = 0xFFFF;\n\trdev->cosq[1] = 0xFFFF;\n\n\treturn rdev;\n}\n\nstatic int bnxt_re_handle_unaffi_async_event(struct creq_func_event\n\t\t\t\t\t     *unaffi_async)\n{\n\tswitch (unaffi_async->event) {\n\tcase CREQ_FUNC_EVENT_EVENT_TX_WQE_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_TX_DATA_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_RX_WQE_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_RX_DATA_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_CQ_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_TQM_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_CFCQ_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_CFCS_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_CFCC_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_CFCM_ERROR:\n\t\tbreak;\n\tcase CREQ_FUNC_EVENT_EVENT_TIM_ERROR:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int bnxt_re_handle_qp_async_event(struct creq_qp_event *qp_event,\n\t\t\t\t\t struct bnxt_re_qp *qp)\n{\n\tstruct ib_event event = {};\n\tunsigned int flags;\n\n\tif (qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_ERR &&\n\t    rdma_is_kernel_res(&qp->ib_qp.res)) {\n\t\tflags = bnxt_re_lock_cqs(qp);\n\t\tbnxt_qplib_add_flush_qp(&qp->qplib_qp);\n\t\tbnxt_re_unlock_cqs(qp, flags);\n\t}\n\n\tif (qp->qplib_qp.srq) {\n\t\tevent.device = &qp->rdev->ibdev;\n\t\tevent.element.qp = &qp->ib_qp;\n\t\tevent.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t}\n\n\tif (event.device && qp->ib_qp.event_handler)\n\t\tqp->ib_qp.event_handler(&event, qp->ib_qp.qp_context);\n\n\treturn 0;\n}\n\nstatic int bnxt_re_handle_affi_async_event(struct creq_qp_event *affi_async,\n\t\t\t\t\t   void *obj)\n{\n\tint rc = 0;\n\tu8 event;\n\n\tif (!obj)\n\t\treturn rc;  \n\n\tevent = affi_async->event;\n\tif (event == CREQ_QP_EVENT_EVENT_QP_ERROR_NOTIFICATION) {\n\t\tstruct bnxt_qplib_qp *lib_qp = obj;\n\t\tstruct bnxt_re_qp *qp = container_of(lib_qp, struct bnxt_re_qp,\n\t\t\t\t\t\t     qplib_qp);\n\t\trc = bnxt_re_handle_qp_async_event(affi_async, qp);\n\t}\n\treturn rc;\n}\n\nstatic int bnxt_re_aeq_handler(struct bnxt_qplib_rcfw *rcfw,\n\t\t\t       void *aeqe, void *obj)\n{\n\tstruct creq_qp_event *affi_async;\n\tstruct creq_func_event *unaffi_async;\n\tu8 type;\n\tint rc;\n\n\ttype = ((struct creq_base *)aeqe)->type;\n\tif (type == CREQ_BASE_TYPE_FUNC_EVENT) {\n\t\tunaffi_async = aeqe;\n\t\trc = bnxt_re_handle_unaffi_async_event(unaffi_async);\n\t} else {\n\t\taffi_async = aeqe;\n\t\trc = bnxt_re_handle_affi_async_event(affi_async, obj);\n\t}\n\n\treturn rc;\n}\n\nstatic int bnxt_re_srqn_handler(struct bnxt_qplib_nq *nq,\n\t\t\t\tstruct bnxt_qplib_srq *handle, u8 event)\n{\n\tstruct bnxt_re_srq *srq = container_of(handle, struct bnxt_re_srq,\n\t\t\t\t\t       qplib_srq);\n\tstruct ib_event ib_event;\n\n\tib_event.device = &srq->rdev->ibdev;\n\tib_event.element.srq = &srq->ib_srq;\n\tif (event == NQ_SRQ_EVENT_EVENT_SRQ_THRESHOLD_EVENT)\n\t\tib_event.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\telse\n\t\tib_event.event = IB_EVENT_SRQ_ERR;\n\n\tif (srq->ib_srq.event_handler) {\n\t\t \n\t\t(*srq->ib_srq.event_handler)(&ib_event,\n\t\t\t\t\t     srq->ib_srq.srq_context);\n\t}\n\treturn 0;\n}\n\nstatic int bnxt_re_cqn_handler(struct bnxt_qplib_nq *nq,\n\t\t\t       struct bnxt_qplib_cq *handle)\n{\n\tstruct bnxt_re_cq *cq = container_of(handle, struct bnxt_re_cq,\n\t\t\t\t\t     qplib_cq);\n\n\tif (cq->ib_cq.comp_handler) {\n\t\t \n\t\t(*cq->ib_cq.comp_handler)(&cq->ib_cq, cq->ib_cq.cq_context);\n\t}\n\n\treturn 0;\n}\n\n#define BNXT_RE_GEN_P5_PF_NQ_DB\t\t0x10000\n#define BNXT_RE_GEN_P5_VF_NQ_DB\t\t0x4000\nstatic u32 bnxt_re_get_nqdb_offset(struct bnxt_re_dev *rdev, u16 indx)\n{\n\treturn bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx) ?\n\t\t(rdev->is_virtfn ? BNXT_RE_GEN_P5_VF_NQ_DB :\n\t\t\t\t   BNXT_RE_GEN_P5_PF_NQ_DB) :\n\t\t\t\t   rdev->en_dev->msix_entries[indx].db_offset;\n}\n\nstatic void bnxt_re_cleanup_res(struct bnxt_re_dev *rdev)\n{\n\tint i;\n\n\tfor (i = 1; i < rdev->num_msix; i++)\n\t\tbnxt_qplib_disable_nq(&rdev->nq[i - 1]);\n\n\tif (rdev->qplib_res.rcfw)\n\t\tbnxt_qplib_cleanup_res(&rdev->qplib_res);\n}\n\nstatic int bnxt_re_init_res(struct bnxt_re_dev *rdev)\n{\n\tint num_vec_enabled = 0;\n\tint rc = 0, i;\n\tu32 db_offt;\n\n\tbnxt_qplib_init_res(&rdev->qplib_res);\n\n\tfor (i = 1; i < rdev->num_msix ; i++) {\n\t\tdb_offt = bnxt_re_get_nqdb_offset(rdev, i);\n\t\trc = bnxt_qplib_enable_nq(rdev->en_dev->pdev, &rdev->nq[i - 1],\n\t\t\t\t\t  i - 1, rdev->en_dev->msix_entries[i].vector,\n\t\t\t\t\t  db_offt, &bnxt_re_cqn_handler,\n\t\t\t\t\t  &bnxt_re_srqn_handler);\n\t\tif (rc) {\n\t\t\tibdev_err(&rdev->ibdev,\n\t\t\t\t  \"Failed to enable NQ with rc = 0x%x\", rc);\n\t\t\tgoto fail;\n\t\t}\n\t\tnum_vec_enabled++;\n\t}\n\treturn 0;\nfail:\n\tfor (i = num_vec_enabled; i >= 0; i--)\n\t\tbnxt_qplib_disable_nq(&rdev->nq[i]);\n\treturn rc;\n}\n\nstatic void bnxt_re_free_nq_res(struct bnxt_re_dev *rdev)\n{\n\tu8 type;\n\tint i;\n\n\tfor (i = 0; i < rdev->num_msix - 1; i++) {\n\t\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\t\tbnxt_re_net_ring_free(rdev, rdev->nq[i].ring_id, type);\n\t\tbnxt_qplib_free_nq(&rdev->nq[i]);\n\t\trdev->nq[i].res = NULL;\n\t}\n}\n\nstatic void bnxt_re_free_res(struct bnxt_re_dev *rdev)\n{\n\tbnxt_re_free_nq_res(rdev);\n\n\tif (rdev->qplib_res.dpi_tbl.max) {\n\t\tbnxt_qplib_dealloc_dpi(&rdev->qplib_res,\n\t\t\t\t       &rdev->dpi_privileged);\n\t}\n\tif (rdev->qplib_res.rcfw) {\n\t\tbnxt_qplib_free_res(&rdev->qplib_res);\n\t\trdev->qplib_res.rcfw = NULL;\n\t}\n}\n\nstatic int bnxt_re_alloc_res(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_re_ring_attr rattr = {};\n\tint num_vec_created = 0;\n\tint rc, i;\n\tu8 type;\n\n\t \n\trdev->qplib_res.rcfw = &rdev->rcfw;\n\trc = bnxt_qplib_get_dev_attr(&rdev->rcfw, &rdev->dev_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = bnxt_qplib_alloc_res(&rdev->qplib_res, rdev->en_dev->pdev,\n\t\t\t\t  rdev->netdev, &rdev->dev_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = bnxt_qplib_alloc_dpi(&rdev->qplib_res,\n\t\t\t\t  &rdev->dpi_privileged,\n\t\t\t\t  rdev, BNXT_QPLIB_DPI_TYPE_KERNEL);\n\tif (rc)\n\t\tgoto dealloc_res;\n\n\tfor (i = 0; i < rdev->num_msix - 1; i++) {\n\t\tstruct bnxt_qplib_nq *nq;\n\n\t\tnq = &rdev->nq[i];\n\t\tnq->hwq.max_elements = BNXT_QPLIB_NQE_MAX_CNT;\n\t\trc = bnxt_qplib_alloc_nq(&rdev->qplib_res, &rdev->nq[i]);\n\t\tif (rc) {\n\t\t\tibdev_err(&rdev->ibdev, \"Alloc Failed NQ%d rc:%#x\",\n\t\t\t\t  i, rc);\n\t\t\tgoto free_nq;\n\t\t}\n\t\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\t\trattr.dma_arr = nq->hwq.pbl[PBL_LVL_0].pg_map_arr;\n\t\trattr.pages = nq->hwq.pbl[rdev->nq[i].hwq.level].pg_count;\n\t\trattr.type = type;\n\t\trattr.mode = RING_ALLOC_REQ_INT_MODE_MSIX;\n\t\trattr.depth = BNXT_QPLIB_NQE_MAX_CNT - 1;\n\t\trattr.lrid = rdev->en_dev->msix_entries[i + 1].ring_idx;\n\t\trc = bnxt_re_net_ring_alloc(rdev, &rattr, &nq->ring_id);\n\t\tif (rc) {\n\t\t\tibdev_err(&rdev->ibdev,\n\t\t\t\t  \"Failed to allocate NQ fw id with rc = 0x%x\",\n\t\t\t\t  rc);\n\t\t\tbnxt_qplib_free_nq(&rdev->nq[i]);\n\t\t\tgoto free_nq;\n\t\t}\n\t\tnum_vec_created++;\n\t}\n\treturn 0;\nfree_nq:\n\tfor (i = num_vec_created - 1; i >= 0; i--) {\n\t\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\t\tbnxt_re_net_ring_free(rdev, rdev->nq[i].ring_id, type);\n\t\tbnxt_qplib_free_nq(&rdev->nq[i]);\n\t}\n\tbnxt_qplib_dealloc_dpi(&rdev->qplib_res,\n\t\t\t       &rdev->dpi_privileged);\ndealloc_res:\n\tbnxt_qplib_free_res(&rdev->qplib_res);\n\nfail:\n\trdev->qplib_res.rcfw = NULL;\n\treturn rc;\n}\n\nstatic void bnxt_re_dispatch_event(struct ib_device *ibdev, struct ib_qp *qp,\n\t\t\t\t   u8 port_num, enum ib_event_type event)\n{\n\tstruct ib_event ib_event;\n\n\tib_event.device = ibdev;\n\tif (qp) {\n\t\tib_event.element.qp = qp;\n\t\tib_event.event = event;\n\t\tif (qp->event_handler)\n\t\t\tqp->event_handler(&ib_event, qp->qp_context);\n\n\t} else {\n\t\tib_event.element.port_num = port_num;\n\t\tib_event.event = event;\n\t\tib_dispatch_event(&ib_event);\n\t}\n}\n\nstatic bool bnxt_re_is_qp1_or_shadow_qp(struct bnxt_re_dev *rdev,\n\t\t\t\t\tstruct bnxt_re_qp *qp)\n{\n\treturn (qp->ib_qp.qp_type == IB_QPT_GSI) ||\n\t       (qp == rdev->gsi_ctx.gsi_sqp);\n}\n\nstatic void bnxt_re_dev_stop(struct bnxt_re_dev *rdev)\n{\n\tint mask = IB_QP_STATE;\n\tstruct ib_qp_attr qp_attr;\n\tstruct bnxt_re_qp *qp;\n\n\tqp_attr.qp_state = IB_QPS_ERR;\n\tmutex_lock(&rdev->qp_lock);\n\tlist_for_each_entry(qp, &rdev->qp_list, list) {\n\t\t \n\t\tif (!bnxt_re_is_qp1_or_shadow_qp(rdev, qp)) {\n\t\t\tif (qp->qplib_qp.state !=\n\t\t\t    CMDQ_MODIFY_QP_NEW_STATE_RESET &&\n\t\t\t    qp->qplib_qp.state !=\n\t\t\t    CMDQ_MODIFY_QP_NEW_STATE_ERR) {\n\t\t\t\tbnxt_re_dispatch_event(&rdev->ibdev, &qp->ib_qp,\n\t\t\t\t\t\t       1, IB_EVENT_QP_FATAL);\n\t\t\t\tbnxt_re_modify_qp(&qp->ib_qp, &qp_attr, mask,\n\t\t\t\t\t\t  NULL);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&rdev->qp_lock);\n}\n\nstatic int bnxt_re_update_gid(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_qplib_sgid_tbl *sgid_tbl = &rdev->qplib_res.sgid_tbl;\n\tstruct bnxt_qplib_gid gid;\n\tu16 gid_idx, index;\n\tint rc = 0;\n\n\tif (!ib_device_try_get(&rdev->ibdev))\n\t\treturn 0;\n\n\tfor (index = 0; index < sgid_tbl->active; index++) {\n\t\tgid_idx = sgid_tbl->hw_id[index];\n\n\t\tif (!memcmp(&sgid_tbl->tbl[index], &bnxt_qplib_gid_zero,\n\t\t\t    sizeof(bnxt_qplib_gid_zero)))\n\t\t\tcontinue;\n\t\t \n\t\tif (sgid_tbl->vlan[index])\n\t\t\tcontinue;\n\n\t\tmemcpy(&gid, &sgid_tbl->tbl[index], sizeof(gid));\n\n\t\trc = bnxt_qplib_update_sgid(sgid_tbl, &gid, gid_idx,\n\t\t\t\t\t    rdev->qplib_res.netdev->dev_addr);\n\t}\n\n\tib_device_put(&rdev->ibdev);\n\treturn rc;\n}\n\nstatic u32 bnxt_re_get_priority_mask(struct bnxt_re_dev *rdev)\n{\n\tu32 prio_map = 0, tmp_map = 0;\n\tstruct net_device *netdev;\n\tstruct dcb_app app = {};\n\n\tnetdev = rdev->netdev;\n\n\tapp.selector = IEEE_8021QAZ_APP_SEL_ETHERTYPE;\n\tapp.protocol = ETH_P_IBOE;\n\ttmp_map = dcb_ieee_getapp_mask(netdev, &app);\n\tprio_map = tmp_map;\n\n\tapp.selector = IEEE_8021QAZ_APP_SEL_DGRAM;\n\tapp.protocol = ROCE_V2_UDP_DPORT;\n\ttmp_map = dcb_ieee_getapp_mask(netdev, &app);\n\tprio_map |= tmp_map;\n\n\treturn prio_map;\n}\n\nstatic int bnxt_re_setup_qos(struct bnxt_re_dev *rdev)\n{\n\tu8 prio_map = 0;\n\n\t \n\tprio_map = bnxt_re_get_priority_mask(rdev);\n\n\tif (prio_map == rdev->cur_prio_map)\n\t\treturn 0;\n\trdev->cur_prio_map = prio_map;\n\t \n\tif ((prio_map == 0 && rdev->qplib_res.prio) ||\n\t    (prio_map != 0 && !rdev->qplib_res.prio)) {\n\t\trdev->qplib_res.prio = prio_map;\n\t\tbnxt_re_update_gid(rdev);\n\t}\n\n\treturn 0;\n}\n\nstatic void bnxt_re_query_hwrm_intf_version(struct bnxt_re_dev *rdev)\n{\n\tstruct bnxt_en_dev *en_dev = rdev->en_dev;\n\tstruct hwrm_ver_get_output resp = {};\n\tstruct hwrm_ver_get_input req = {};\n\tstruct bnxt_qplib_chip_ctx *cctx;\n\tstruct bnxt_fw_msg fw_msg = {};\n\tint rc;\n\n\tbnxt_re_init_hwrm_hdr((void *)&req, HWRM_VER_GET);\n\treq.hwrm_intf_maj = HWRM_VERSION_MAJOR;\n\treq.hwrm_intf_min = HWRM_VERSION_MINOR;\n\treq.hwrm_intf_upd = HWRM_VERSION_UPDATE;\n\tbnxt_re_fill_fw_msg(&fw_msg, (void *)&req, sizeof(req), (void *)&resp,\n\t\t\t    sizeof(resp), DFLT_HWRM_CMD_TIMEOUT);\n\trc = bnxt_send_msg(en_dev, &fw_msg);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev, \"Failed to query HW version, rc = 0x%x\",\n\t\t\t  rc);\n\t\treturn;\n\t}\n\n\tcctx = rdev->chip_ctx;\n\tcctx->hwrm_intf_ver =\n\t\t(u64)le16_to_cpu(resp.hwrm_intf_major) << 48 |\n\t\t(u64)le16_to_cpu(resp.hwrm_intf_minor) << 32 |\n\t\t(u64)le16_to_cpu(resp.hwrm_intf_build) << 16 |\n\t\tle16_to_cpu(resp.hwrm_intf_patch);\n\n\tcctx->hwrm_cmd_max_timeout = le16_to_cpu(resp.max_req_timeout);\n\n\tif (!cctx->hwrm_cmd_max_timeout)\n\t\tcctx->hwrm_cmd_max_timeout = RCFW_FW_STALL_MAX_TIMEOUT;\n}\n\nstatic int bnxt_re_ib_init(struct bnxt_re_dev *rdev)\n{\n\tint rc;\n\tu32 event;\n\n\t \n\trc = bnxt_re_register_ib(rdev);\n\tif (rc) {\n\t\tpr_err(\"Failed to register with IB: %#x\\n\", rc);\n\t\treturn rc;\n\t}\n\tdev_info(rdev_to_dev(rdev), \"Device registered with IB successfully\");\n\tset_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags);\n\n\tevent = netif_running(rdev->netdev) && netif_carrier_ok(rdev->netdev) ?\n\t\tIB_EVENT_PORT_ACTIVE : IB_EVENT_PORT_ERR;\n\n\tbnxt_re_dispatch_event(&rdev->ibdev, NULL, 1, event);\n\n\treturn rc;\n}\n\nstatic void bnxt_re_dev_uninit(struct bnxt_re_dev *rdev)\n{\n\tu8 type;\n\tint rc;\n\n\tif (test_and_clear_bit(BNXT_RE_FLAG_QOS_WORK_REG, &rdev->flags))\n\t\tcancel_delayed_work_sync(&rdev->worker);\n\n\tif (test_and_clear_bit(BNXT_RE_FLAG_RESOURCES_INITIALIZED,\n\t\t\t       &rdev->flags))\n\t\tbnxt_re_cleanup_res(rdev);\n\tif (test_and_clear_bit(BNXT_RE_FLAG_RESOURCES_ALLOCATED, &rdev->flags))\n\t\tbnxt_re_free_res(rdev);\n\n\tif (test_and_clear_bit(BNXT_RE_FLAG_RCFW_CHANNEL_EN, &rdev->flags)) {\n\t\trc = bnxt_qplib_deinit_rcfw(&rdev->rcfw);\n\t\tif (rc)\n\t\t\tibdev_warn(&rdev->ibdev,\n\t\t\t\t   \"Failed to deinitialize RCFW: %#x\", rc);\n\t\tbnxt_re_net_stats_ctx_free(rdev, rdev->qplib_ctx.stats.fw_id);\n\t\tbnxt_qplib_free_ctx(&rdev->qplib_res, &rdev->qplib_ctx);\n\t\tbnxt_qplib_disable_rcfw_channel(&rdev->rcfw);\n\t\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\t\tbnxt_re_net_ring_free(rdev, rdev->rcfw.creq.ring_id, type);\n\t\tbnxt_qplib_free_rcfw_channel(&rdev->rcfw);\n\t}\n\n\trdev->num_msix = 0;\n\n\tif (rdev->pacing.dbr_pacing)\n\t\tbnxt_re_deinitialize_dbr_pacing(rdev);\n\n\tbnxt_re_destroy_chip_ctx(rdev);\n\tif (test_and_clear_bit(BNXT_RE_FLAG_NETDEV_REGISTERED, &rdev->flags))\n\t\tbnxt_unregister_dev(rdev->en_dev);\n}\n\n \nstatic void bnxt_re_worker(struct work_struct *work)\n{\n\tstruct bnxt_re_dev *rdev = container_of(work, struct bnxt_re_dev,\n\t\t\t\t\t\tworker.work);\n\n\tbnxt_re_setup_qos(rdev);\n\tschedule_delayed_work(&rdev->worker, msecs_to_jiffies(30000));\n}\n\nstatic int bnxt_re_dev_init(struct bnxt_re_dev *rdev, u8 wqe_mode)\n{\n\tstruct bnxt_re_ring_attr rattr = {};\n\tstruct bnxt_qplib_creq_ctx *creq;\n\tu32 db_offt;\n\tint vid;\n\tu8 type;\n\tint rc;\n\n\t \n\trc = bnxt_re_register_netdev(rdev);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to register with netedev: %#x\\n\", rc);\n\t\treturn -EINVAL;\n\t}\n\tset_bit(BNXT_RE_FLAG_NETDEV_REGISTERED, &rdev->flags);\n\n\trc = bnxt_re_setup_chip_ctx(rdev, wqe_mode);\n\tif (rc) {\n\t\tbnxt_unregister_dev(rdev->en_dev);\n\t\tclear_bit(BNXT_RE_FLAG_NETDEV_REGISTERED, &rdev->flags);\n\t\tibdev_err(&rdev->ibdev, \"Failed to get chip context\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tbnxt_re_get_sriov_func_type(rdev);\n\n\tif (!rdev->en_dev->ulp_tbl->msix_requested) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to get MSI-X vectors: %#x\\n\", rc);\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\tibdev_dbg(&rdev->ibdev, \"Got %d MSI-X vectors\\n\",\n\t\t  rdev->en_dev->ulp_tbl->msix_requested);\n\trdev->num_msix = rdev->en_dev->ulp_tbl->msix_requested;\n\n\tbnxt_re_query_hwrm_intf_version(rdev);\n\n\t \n\trc = bnxt_qplib_alloc_rcfw_channel(&rdev->qplib_res, &rdev->rcfw,\n\t\t\t\t\t   &rdev->qplib_ctx,\n\t\t\t\t\t   BNXT_RE_MAX_QPC_COUNT);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to allocate RCFW Channel: %#x\\n\", rc);\n\t\tgoto fail;\n\t}\n\n\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\tcreq = &rdev->rcfw.creq;\n\trattr.dma_arr = creq->hwq.pbl[PBL_LVL_0].pg_map_arr;\n\trattr.pages = creq->hwq.pbl[creq->hwq.level].pg_count;\n\trattr.type = type;\n\trattr.mode = RING_ALLOC_REQ_INT_MODE_MSIX;\n\trattr.depth = BNXT_QPLIB_CREQE_MAX_CNT - 1;\n\trattr.lrid = rdev->en_dev->msix_entries[BNXT_RE_AEQ_IDX].ring_idx;\n\trc = bnxt_re_net_ring_alloc(rdev, &rattr, &creq->ring_id);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev, \"Failed to allocate CREQ: %#x\\n\", rc);\n\t\tgoto free_rcfw;\n\t}\n\tdb_offt = bnxt_re_get_nqdb_offset(rdev, BNXT_RE_AEQ_IDX);\n\tvid = rdev->en_dev->msix_entries[BNXT_RE_AEQ_IDX].vector;\n\trc = bnxt_qplib_enable_rcfw_channel(&rdev->rcfw,\n\t\t\t\t\t    vid, db_offt,\n\t\t\t\t\t    &bnxt_re_aeq_handler);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev, \"Failed to enable RCFW channel: %#x\\n\",\n\t\t\t  rc);\n\t\tgoto free_ring;\n\t}\n\n\tif (bnxt_qplib_dbr_pacing_en(rdev->chip_ctx)) {\n\t\trc = bnxt_re_initialize_dbr_pacing(rdev);\n\t\tif (!rc) {\n\t\t\trdev->pacing.dbr_pacing = true;\n\t\t} else {\n\t\t\tibdev_err(&rdev->ibdev,\n\t\t\t\t  \"DBR pacing disabled with error : %d\\n\", rc);\n\t\t\trdev->pacing.dbr_pacing = false;\n\t\t}\n\t}\n\trc = bnxt_qplib_get_dev_attr(&rdev->rcfw, &rdev->dev_attr);\n\tif (rc)\n\t\tgoto disable_rcfw;\n\n\tbnxt_re_set_resource_limits(rdev);\n\n\trc = bnxt_qplib_alloc_ctx(&rdev->qplib_res, &rdev->qplib_ctx, 0,\n\t\t\t\t  bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx));\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to allocate QPLIB context: %#x\\n\", rc);\n\t\tgoto disable_rcfw;\n\t}\n\trc = bnxt_re_net_stats_ctx_alloc(rdev,\n\t\t\t\t\t rdev->qplib_ctx.stats.dma_map,\n\t\t\t\t\t &rdev->qplib_ctx.stats.fw_id);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to allocate stats context: %#x\\n\", rc);\n\t\tgoto free_ctx;\n\t}\n\n\trc = bnxt_qplib_init_rcfw(&rdev->rcfw, &rdev->qplib_ctx,\n\t\t\t\t  rdev->is_virtfn);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to initialize RCFW: %#x\\n\", rc);\n\t\tgoto free_sctx;\n\t}\n\tset_bit(BNXT_RE_FLAG_RCFW_CHANNEL_EN, &rdev->flags);\n\n\t \n\trc = bnxt_re_alloc_res(rdev);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to allocate resources: %#x\\n\", rc);\n\t\tgoto fail;\n\t}\n\tset_bit(BNXT_RE_FLAG_RESOURCES_ALLOCATED, &rdev->flags);\n\trc = bnxt_re_init_res(rdev);\n\tif (rc) {\n\t\tibdev_err(&rdev->ibdev,\n\t\t\t  \"Failed to initialize resources: %#x\\n\", rc);\n\t\tgoto fail;\n\t}\n\n\tset_bit(BNXT_RE_FLAG_RESOURCES_INITIALIZED, &rdev->flags);\n\n\tif (!rdev->is_virtfn) {\n\t\trc = bnxt_re_setup_qos(rdev);\n\t\tif (rc)\n\t\t\tibdev_info(&rdev->ibdev,\n\t\t\t\t   \"RoCE priority not yet configured\\n\");\n\n\t\tINIT_DELAYED_WORK(&rdev->worker, bnxt_re_worker);\n\t\tset_bit(BNXT_RE_FLAG_QOS_WORK_REG, &rdev->flags);\n\t\tschedule_delayed_work(&rdev->worker, msecs_to_jiffies(30000));\n\t\t \n\t\tbnxt_re_vf_res_config(rdev);\n\t}\n\n\treturn 0;\nfree_sctx:\n\tbnxt_re_net_stats_ctx_free(rdev, rdev->qplib_ctx.stats.fw_id);\nfree_ctx:\n\tbnxt_qplib_free_ctx(&rdev->qplib_res, &rdev->qplib_ctx);\ndisable_rcfw:\n\tbnxt_qplib_disable_rcfw_channel(&rdev->rcfw);\nfree_ring:\n\ttype = bnxt_qplib_get_ring_type(rdev->chip_ctx);\n\tbnxt_re_net_ring_free(rdev, rdev->rcfw.creq.ring_id, type);\nfree_rcfw:\n\tbnxt_qplib_free_rcfw_channel(&rdev->rcfw);\nfail:\n\tbnxt_re_dev_uninit(rdev);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_add_device(struct auxiliary_device *adev, u8 wqe_mode)\n{\n\tstruct bnxt_aux_priv *aux_priv =\n\t\tcontainer_of(adev, struct bnxt_aux_priv, aux_dev);\n\tstruct bnxt_en_dev *en_dev;\n\tstruct bnxt_re_dev *rdev;\n\tint rc;\n\n\t \n\ten_dev = aux_priv->edev;\n\n\trdev = bnxt_re_dev_add(aux_priv, en_dev);\n\tif (!rdev || !rdev_to_dev(rdev)) {\n\t\trc = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\trc = bnxt_re_dev_init(rdev, wqe_mode);\n\tif (rc)\n\t\tgoto re_dev_dealloc;\n\n\trc = bnxt_re_ib_init(rdev);\n\tif (rc) {\n\t\tpr_err(\"Failed to register with IB: %s\",\n\t\t\taux_priv->aux_dev.name);\n\t\tgoto re_dev_uninit;\n\t}\n\tauxiliary_set_drvdata(adev, rdev);\n\n\treturn 0;\n\nre_dev_uninit:\n\tbnxt_re_dev_uninit(rdev);\nre_dev_dealloc:\n\tib_dealloc_device(&rdev->ibdev);\nexit:\n\treturn rc;\n}\n\nstatic void bnxt_re_setup_cc(struct bnxt_re_dev *rdev, bool enable)\n{\n\tstruct bnxt_qplib_cc_param cc_param = {};\n\n\t \n\tif (rdev->is_virtfn)\n\t\treturn;\n\n\t \n\tif (!bnxt_qplib_is_chip_gen_p5(rdev->chip_ctx))\n\t\treturn;\n\n\tif (enable) {\n\t\tcc_param.enable  = 1;\n\t\tcc_param.cc_mode = CMDQ_MODIFY_ROCE_CC_CC_MODE_PROBABILISTIC_CC_MODE;\n\t}\n\n\tcc_param.mask = (CMDQ_MODIFY_ROCE_CC_MODIFY_MASK_CC_MODE |\n\t\t\t CMDQ_MODIFY_ROCE_CC_MODIFY_MASK_ENABLE_CC |\n\t\t\t CMDQ_MODIFY_ROCE_CC_MODIFY_MASK_TOS_ECN);\n\n\tif (bnxt_qplib_modify_cc(&rdev->qplib_res, &cc_param))\n\t\tibdev_err(&rdev->ibdev, \"Failed to setup CC enable = %d\\n\", enable);\n}\n\n \nstatic int bnxt_re_netdev_event(struct notifier_block *notifier,\n\t\t\t\tunsigned long event, void *ptr)\n{\n\tstruct net_device *real_dev, *netdev = netdev_notifier_info_to_dev(ptr);\n\tstruct bnxt_re_dev *rdev;\n\n\treal_dev = rdma_vlan_dev_real_dev(netdev);\n\tif (!real_dev)\n\t\treal_dev = netdev;\n\n\tif (real_dev != netdev)\n\t\tgoto exit;\n\n\trdev = bnxt_re_from_netdev(real_dev);\n\tif (!rdev)\n\t\treturn NOTIFY_DONE;\n\n\n\tswitch (event) {\n\tcase NETDEV_UP:\n\tcase NETDEV_DOWN:\n\tcase NETDEV_CHANGE:\n\t\tbnxt_re_dispatch_event(&rdev->ibdev, NULL, 1,\n\t\t\t\t\tnetif_carrier_ok(real_dev) ?\n\t\t\t\t\tIB_EVENT_PORT_ACTIVE :\n\t\t\t\t\tIB_EVENT_PORT_ERR);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tib_device_put(&rdev->ibdev);\nexit:\n\treturn NOTIFY_DONE;\n}\n\n#define BNXT_ADEV_NAME \"bnxt_en\"\n\nstatic void bnxt_re_remove(struct auxiliary_device *adev)\n{\n\tstruct bnxt_re_dev *rdev = auxiliary_get_drvdata(adev);\n\n\tif (!rdev)\n\t\treturn;\n\n\tmutex_lock(&bnxt_re_mutex);\n\tif (rdev->nb.notifier_call) {\n\t\tunregister_netdevice_notifier(&rdev->nb);\n\t\trdev->nb.notifier_call = NULL;\n\t} else {\n\t\t \n\t\tgoto skip_remove;\n\t}\n\tbnxt_re_setup_cc(rdev, false);\n\tib_unregister_device(&rdev->ibdev);\n\tbnxt_re_dev_uninit(rdev);\n\tib_dealloc_device(&rdev->ibdev);\nskip_remove:\n\tmutex_unlock(&bnxt_re_mutex);\n}\n\nstatic int bnxt_re_probe(struct auxiliary_device *adev,\n\t\t\t const struct auxiliary_device_id *id)\n{\n\tstruct bnxt_re_dev *rdev;\n\tint rc;\n\n\tmutex_lock(&bnxt_re_mutex);\n\trc = bnxt_re_add_device(adev, BNXT_QPLIB_WQE_MODE_STATIC);\n\tif (rc) {\n\t\tmutex_unlock(&bnxt_re_mutex);\n\t\treturn rc;\n\t}\n\n\trdev = auxiliary_get_drvdata(adev);\n\n\trdev->nb.notifier_call = bnxt_re_netdev_event;\n\trc = register_netdevice_notifier(&rdev->nb);\n\tif (rc) {\n\t\trdev->nb.notifier_call = NULL;\n\t\tpr_err(\"%s: Cannot register to netdevice_notifier\",\n\t\t       ROCE_DRV_MODULE_NAME);\n\t\tgoto err;\n\t}\n\n\tbnxt_re_setup_cc(rdev, true);\n\tmutex_unlock(&bnxt_re_mutex);\n\treturn 0;\n\nerr:\n\tmutex_unlock(&bnxt_re_mutex);\n\tbnxt_re_remove(adev);\n\n\treturn rc;\n}\n\nstatic int bnxt_re_suspend(struct auxiliary_device *adev, pm_message_t state)\n{\n\tstruct bnxt_re_dev *rdev = auxiliary_get_drvdata(adev);\n\n\tif (!rdev)\n\t\treturn 0;\n\n\tmutex_lock(&bnxt_re_mutex);\n\t \n\n\tibdev_info(&rdev->ibdev, \"Handle device suspend call\");\n\t \n\tif (test_bit(BNXT_STATE_FW_FATAL_COND, &rdev->en_dev->en_state))\n\t\tset_bit(ERR_DEVICE_DETACHED, &rdev->rcfw.cmdq.flags);\n\n\tbnxt_re_dev_stop(rdev);\n\tbnxt_re_stop_irq(rdev);\n\t \n\tset_bit(BNXT_RE_FLAG_ERR_DEVICE_DETACHED, &rdev->flags);\n\tset_bit(ERR_DEVICE_DETACHED, &rdev->rcfw.cmdq.flags);\n\twake_up_all(&rdev->rcfw.cmdq.waitq);\n\tmutex_unlock(&bnxt_re_mutex);\n\n\treturn 0;\n}\n\nstatic int bnxt_re_resume(struct auxiliary_device *adev)\n{\n\tstruct bnxt_re_dev *rdev = auxiliary_get_drvdata(adev);\n\n\tif (!rdev)\n\t\treturn 0;\n\n\tmutex_lock(&bnxt_re_mutex);\n\t \n\n\tibdev_info(&rdev->ibdev, \"Handle device resume call\");\n\tmutex_unlock(&bnxt_re_mutex);\n\n\treturn 0;\n}\n\nstatic const struct auxiliary_device_id bnxt_re_id_table[] = {\n\t{ .name = BNXT_ADEV_NAME \".rdma\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, bnxt_re_id_table);\n\nstatic struct auxiliary_driver bnxt_re_driver = {\n\t.name = \"rdma\",\n\t.probe = bnxt_re_probe,\n\t.remove = bnxt_re_remove,\n\t.shutdown = bnxt_re_shutdown,\n\t.suspend = bnxt_re_suspend,\n\t.resume = bnxt_re_resume,\n\t.id_table = bnxt_re_id_table,\n};\n\nstatic int __init bnxt_re_mod_init(void)\n{\n\tint rc;\n\n\tpr_info(\"%s: %s\", ROCE_DRV_MODULE_NAME, version);\n\trc = auxiliary_driver_register(&bnxt_re_driver);\n\tif (rc) {\n\t\tpr_err(\"%s: Failed to register auxiliary driver\\n\",\n\t\t\tROCE_DRV_MODULE_NAME);\n\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nstatic void __exit bnxt_re_mod_exit(void)\n{\n\tauxiliary_driver_unregister(&bnxt_re_driver);\n}\n\nmodule_init(bnxt_re_mod_init);\nmodule_exit(bnxt_re_mod_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}