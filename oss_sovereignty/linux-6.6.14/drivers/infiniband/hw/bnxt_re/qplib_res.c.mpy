{
  "module_name": "qplib_res.c",
  "hash_id": "e6d7fb934871fac36172ad493a9588ff2c53da6c8973e025d4c15115bfa71d86",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/bnxt_re/qplib_res.c",
  "human_readable_source": " \n\n#define dev_fmt(fmt) \"QPLIB: \" fmt\n\n#include <linux/spinlock.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/inetdevice.h>\n#include <linux/dma-mapping.h>\n#include <linux/if_vlan.h>\n#include <linux/vmalloc.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_umem.h>\n\n#include \"roce_hsi.h\"\n#include \"qplib_res.h\"\n#include \"qplib_sp.h\"\n#include \"qplib_rcfw.h\"\n\nstatic void bnxt_qplib_free_stats_ctx(struct pci_dev *pdev,\n\t\t\t\t      struct bnxt_qplib_stats *stats);\nstatic int bnxt_qplib_alloc_stats_ctx(struct pci_dev *pdev,\n\t\t\t\t      struct bnxt_qplib_chip_ctx *cctx,\n\t\t\t\t      struct bnxt_qplib_stats *stats);\n\n \nstatic void __free_pbl(struct bnxt_qplib_res *res, struct bnxt_qplib_pbl *pbl,\n\t\t       bool is_umem)\n{\n\tstruct pci_dev *pdev = res->pdev;\n\tint i;\n\n\tif (!is_umem) {\n\t\tfor (i = 0; i < pbl->pg_count; i++) {\n\t\t\tif (pbl->pg_arr[i])\n\t\t\t\tdma_free_coherent(&pdev->dev, pbl->pg_size,\n\t\t\t\t\t\t  (void *)((unsigned long)\n\t\t\t\t\t\t   pbl->pg_arr[i] &\n\t\t\t\t\t\t  PAGE_MASK),\n\t\t\t\t\t\t  pbl->pg_map_arr[i]);\n\t\t\telse\n\t\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t\t \"PBL free pg_arr[%d] empty?!\\n\", i);\n\t\t\tpbl->pg_arr[i] = NULL;\n\t\t}\n\t}\n\tvfree(pbl->pg_arr);\n\tpbl->pg_arr = NULL;\n\tvfree(pbl->pg_map_arr);\n\tpbl->pg_map_arr = NULL;\n\tpbl->pg_count = 0;\n\tpbl->pg_size = 0;\n}\n\nstatic void bnxt_qplib_fill_user_dma_pages(struct bnxt_qplib_pbl *pbl,\n\t\t\t\t\t   struct bnxt_qplib_sg_info *sginfo)\n{\n\tstruct ib_block_iter biter;\n\tint i = 0;\n\n\trdma_umem_for_each_dma_block(sginfo->umem, &biter, sginfo->pgsize) {\n\t\tpbl->pg_map_arr[i] = rdma_block_iter_dma_address(&biter);\n\t\tpbl->pg_arr[i] = NULL;\n\t\tpbl->pg_count++;\n\t\ti++;\n\t}\n}\n\nstatic int __alloc_pbl(struct bnxt_qplib_res *res,\n\t\t       struct bnxt_qplib_pbl *pbl,\n\t\t       struct bnxt_qplib_sg_info *sginfo)\n{\n\tstruct pci_dev *pdev = res->pdev;\n\tbool is_umem = false;\n\tu32 pages;\n\tint i;\n\n\tif (sginfo->nopte)\n\t\treturn 0;\n\tif (sginfo->umem)\n\t\tpages = ib_umem_num_dma_blocks(sginfo->umem, sginfo->pgsize);\n\telse\n\t\tpages = sginfo->npages;\n\t \n\tpbl->pg_arr = vmalloc_array(pages, sizeof(void *));\n\tif (!pbl->pg_arr)\n\t\treturn -ENOMEM;\n\n\tpbl->pg_map_arr = vmalloc_array(pages, sizeof(dma_addr_t));\n\tif (!pbl->pg_map_arr) {\n\t\tvfree(pbl->pg_arr);\n\t\tpbl->pg_arr = NULL;\n\t\treturn -ENOMEM;\n\t}\n\tpbl->pg_count = 0;\n\tpbl->pg_size = sginfo->pgsize;\n\n\tif (!sginfo->umem) {\n\t\tfor (i = 0; i < pages; i++) {\n\t\t\tpbl->pg_arr[i] = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\t\t    pbl->pg_size,\n\t\t\t\t\t\t\t    &pbl->pg_map_arr[i],\n\t\t\t\t\t\t\t    GFP_KERNEL);\n\t\t\tif (!pbl->pg_arr[i])\n\t\t\t\tgoto fail;\n\t\t\tpbl->pg_count++;\n\t\t}\n\t} else {\n\t\tis_umem = true;\n\t\tbnxt_qplib_fill_user_dma_pages(pbl, sginfo);\n\t}\n\n\treturn 0;\nfail:\n\t__free_pbl(res, pbl, is_umem);\n\treturn -ENOMEM;\n}\n\n \nvoid bnxt_qplib_free_hwq(struct bnxt_qplib_res *res,\n\t\t\t struct bnxt_qplib_hwq *hwq)\n{\n\tint i;\n\n\tif (!hwq->max_elements)\n\t\treturn;\n\tif (hwq->level >= PBL_LVL_MAX)\n\t\treturn;\n\n\tfor (i = 0; i < hwq->level + 1; i++) {\n\t\tif (i == hwq->level)\n\t\t\t__free_pbl(res, &hwq->pbl[i], hwq->is_user);\n\t\telse\n\t\t\t__free_pbl(res, &hwq->pbl[i], false);\n\t}\n\n\thwq->level = PBL_LVL_MAX;\n\thwq->max_elements = 0;\n\thwq->element_size = 0;\n\thwq->prod = 0;\n\thwq->cons = 0;\n\thwq->cp_bit = 0;\n}\n\n \n\nint bnxt_qplib_alloc_init_hwq(struct bnxt_qplib_hwq *hwq,\n\t\t\t      struct bnxt_qplib_hwq_attr *hwq_attr)\n{\n\tu32 npages, aux_slots, pg_size, aux_pages = 0, aux_size = 0;\n\tstruct bnxt_qplib_sg_info sginfo = {};\n\tu32 depth, stride, npbl, npde;\n\tdma_addr_t *src_phys_ptr, **dst_virt_ptr;\n\tstruct bnxt_qplib_res *res;\n\tstruct pci_dev *pdev;\n\tint i, rc, lvl;\n\n\tres = hwq_attr->res;\n\tpdev = res->pdev;\n\tpg_size = hwq_attr->sginfo->pgsize;\n\thwq->level = PBL_LVL_MAX;\n\n\tdepth = roundup_pow_of_two(hwq_attr->depth);\n\tstride = roundup_pow_of_two(hwq_attr->stride);\n\tif (hwq_attr->aux_depth) {\n\t\taux_slots = hwq_attr->aux_depth;\n\t\taux_size = roundup_pow_of_two(hwq_attr->aux_stride);\n\t\taux_pages = (aux_slots * aux_size) / pg_size;\n\t\tif ((aux_slots * aux_size) % pg_size)\n\t\t\taux_pages++;\n\t}\n\n\tif (!hwq_attr->sginfo->umem) {\n\t\thwq->is_user = false;\n\t\tnpages = (depth * stride) / pg_size + aux_pages;\n\t\tif ((depth * stride) % pg_size)\n\t\t\tnpages++;\n\t\tif (!npages)\n\t\t\treturn -EINVAL;\n\t\thwq_attr->sginfo->npages = npages;\n\t} else {\n\t\tnpages = ib_umem_num_dma_blocks(hwq_attr->sginfo->umem,\n\t\t\t\t\t\thwq_attr->sginfo->pgsize);\n\t\thwq->is_user = true;\n\t}\n\n\tif (npages == MAX_PBL_LVL_0_PGS && !hwq_attr->sginfo->nopte) {\n\t\t \n\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_0], hwq_attr->sginfo);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t\thwq->level = PBL_LVL_0;\n\t\tgoto done;\n\t}\n\n\tif (npages >= MAX_PBL_LVL_0_PGS) {\n\t\tif (npages > MAX_PBL_LVL_1_PGS) {\n\t\t\tu32 flag = (hwq_attr->type == HWQ_TYPE_L2_CMPL) ?\n\t\t\t\t    0 : PTU_PTE_VALID;\n\t\t\t \n\t\t\tnpbl = npages >> MAX_PBL_LVL_1_PGS_SHIFT;\n\t\t\tif (npages % BIT(MAX_PBL_LVL_1_PGS_SHIFT))\n\t\t\t\tnpbl++;\n\t\t\tnpde = npbl >> MAX_PDL_LVL_SHIFT;\n\t\t\tif (npbl % BIT(MAX_PDL_LVL_SHIFT))\n\t\t\t\tnpde++;\n\t\t\t \n\t\t\tsginfo.pgsize = npde * pg_size;\n\t\t\tsginfo.npages = 1;\n\t\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_0], &sginfo);\n\n\t\t\t \n\t\t\tsginfo.npages = npbl;\n\t\t\tsginfo.pgsize = PAGE_SIZE;\n\t\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_1], &sginfo);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t\t \n\t\t\tdst_virt_ptr =\n\t\t\t\t(dma_addr_t **)hwq->pbl[PBL_LVL_0].pg_arr;\n\t\t\tsrc_phys_ptr = hwq->pbl[PBL_LVL_1].pg_map_arr;\n\t\t\tif (hwq_attr->type == HWQ_TYPE_MR) {\n\t\t\t \n\t\t\t\tfor (i = 0; i < hwq->pbl[PBL_LVL_1].pg_count;\n\t\t\t\t     i++)\n\t\t\t\t\tdst_virt_ptr[0][i] = src_phys_ptr[i] |\n\t\t\t\t\t\tflag;\n\t\t\t} else {\n\t\t\t\tfor (i = 0; i < hwq->pbl[PBL_LVL_1].pg_count;\n\t\t\t\t     i++)\n\t\t\t\t\tdst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\n\t\t\t\t\t\tsrc_phys_ptr[i] |\n\t\t\t\t\t\tPTU_PDE_VALID;\n\t\t\t}\n\t\t\t \n\t\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_2],\n\t\t\t\t\t hwq_attr->sginfo);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t\thwq->level = PBL_LVL_2;\n\t\t\tif (hwq_attr->sginfo->nopte)\n\t\t\t\tgoto done;\n\t\t\t \n\t\t\tdst_virt_ptr =\n\t\t\t\t(dma_addr_t **)hwq->pbl[PBL_LVL_1].pg_arr;\n\t\t\tsrc_phys_ptr = hwq->pbl[PBL_LVL_2].pg_map_arr;\n\t\t\tfor (i = 0; i < hwq->pbl[PBL_LVL_2].pg_count; i++) {\n\t\t\t\tdst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\n\t\t\t\t\tsrc_phys_ptr[i] | PTU_PTE_VALID;\n\t\t\t}\n\t\t\tif (hwq_attr->type == HWQ_TYPE_QUEUE) {\n\t\t\t\t \n\t\t\t\ti = hwq->pbl[PBL_LVL_2].pg_count;\n\t\t\t\tdst_virt_ptr[PTR_PG(i - 1)][PTR_IDX(i - 1)] |=\n\t\t\t\t\t\t\t\t  PTU_PTE_LAST;\n\t\t\t\tif (i > 1)\n\t\t\t\t\tdst_virt_ptr[PTR_PG(i - 2)]\n\t\t\t\t\t\t    [PTR_IDX(i - 2)] |=\n\t\t\t\t\t\t    PTU_PTE_NEXT_TO_LAST;\n\t\t\t}\n\t\t} else {  \n\t\t\tu32 flag = (hwq_attr->type == HWQ_TYPE_L2_CMPL) ?\n\t\t\t\t    0 : PTU_PTE_VALID;\n\n\t\t\t \n\t\t\tnpbl = npages >> MAX_PBL_LVL_1_PGS_SHIFT;\n\t\t\tif (npages % BIT(MAX_PBL_LVL_1_PGS_SHIFT))\n\t\t\t\tnpbl++;\n\t\t\tsginfo.npages = npbl;\n\t\t\tsginfo.pgsize = PAGE_SIZE;\n\t\t\t \n\t\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_0], &sginfo);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t\t \n\t\t\trc = __alloc_pbl(res, &hwq->pbl[PBL_LVL_1],\n\t\t\t\t\t hwq_attr->sginfo);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t\thwq->level = PBL_LVL_1;\n\t\t\tif (hwq_attr->sginfo->nopte)\n\t\t\t\tgoto done;\n\t\t\t \n\t\t\tdst_virt_ptr =\n\t\t\t\t(dma_addr_t **)hwq->pbl[PBL_LVL_0].pg_arr;\n\t\t\tsrc_phys_ptr = hwq->pbl[PBL_LVL_1].pg_map_arr;\n\t\t\tfor (i = 0; i < hwq->pbl[PBL_LVL_1].pg_count; i++)\n\t\t\t\tdst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\n\t\t\t\t\tsrc_phys_ptr[i] | flag;\n\t\t\tif (hwq_attr->type == HWQ_TYPE_QUEUE) {\n\t\t\t\t \n\t\t\t\ti = hwq->pbl[PBL_LVL_1].pg_count;\n\t\t\t\tdst_virt_ptr[PTR_PG(i - 1)][PTR_IDX(i - 1)] |=\n\t\t\t\t\t\t\t\t  PTU_PTE_LAST;\n\t\t\t\tif (i > 1)\n\t\t\t\t\tdst_virt_ptr[PTR_PG(i - 2)]\n\t\t\t\t\t\t    [PTR_IDX(i - 2)] |=\n\t\t\t\t\t\t    PTU_PTE_NEXT_TO_LAST;\n\t\t\t}\n\t\t}\n\t}\ndone:\n\thwq->prod = 0;\n\thwq->cons = 0;\n\thwq->pdev = pdev;\n\thwq->depth = hwq_attr->depth;\n\thwq->max_elements = depth;\n\thwq->element_size = stride;\n\thwq->qe_ppg = pg_size / stride;\n\t \n\tlvl = hwq->level;\n\tif (hwq_attr->sginfo->nopte && hwq->level)\n\t\tlvl = hwq->level - 1;\n\thwq->pbl_ptr = hwq->pbl[lvl].pg_arr;\n\thwq->pbl_dma_ptr = hwq->pbl[lvl].pg_map_arr;\n\tspin_lock_init(&hwq->lock);\n\n\treturn 0;\nfail:\n\tbnxt_qplib_free_hwq(res, hwq);\n\treturn -ENOMEM;\n}\n\n \nvoid bnxt_qplib_free_ctx(struct bnxt_qplib_res *res,\n\t\t\t struct bnxt_qplib_ctx *ctx)\n{\n\tint i;\n\n\tbnxt_qplib_free_hwq(res, &ctx->qpc_tbl);\n\tbnxt_qplib_free_hwq(res, &ctx->mrw_tbl);\n\tbnxt_qplib_free_hwq(res, &ctx->srqc_tbl);\n\tbnxt_qplib_free_hwq(res, &ctx->cq_tbl);\n\tbnxt_qplib_free_hwq(res, &ctx->tim_tbl);\n\tfor (i = 0; i < MAX_TQM_ALLOC_REQ; i++)\n\t\tbnxt_qplib_free_hwq(res, &ctx->tqm_ctx.qtbl[i]);\n\t \n\tctx->tqm_ctx.pde.level = ctx->tqm_ctx.pde_level;\n\tbnxt_qplib_free_hwq(res, &ctx->tqm_ctx.pde);\n\tbnxt_qplib_free_stats_ctx(res->pdev, &ctx->stats);\n}\n\nstatic int bnxt_qplib_alloc_tqm_rings(struct bnxt_qplib_res *res,\n\t\t\t\t      struct bnxt_qplib_ctx *ctx)\n{\n\tstruct bnxt_qplib_hwq_attr hwq_attr = {};\n\tstruct bnxt_qplib_sg_info sginfo = {};\n\tstruct bnxt_qplib_tqm_ctx *tqmctx;\n\tint rc;\n\tint i;\n\n\ttqmctx = &ctx->tqm_ctx;\n\n\tsginfo.pgsize = PAGE_SIZE;\n\tsginfo.pgshft = PAGE_SHIFT;\n\thwq_attr.sginfo = &sginfo;\n\thwq_attr.res = res;\n\thwq_attr.type = HWQ_TYPE_CTX;\n\thwq_attr.depth = 512;\n\thwq_attr.stride = sizeof(u64);\n\t \n\trc = bnxt_qplib_alloc_init_hwq(&tqmctx->pde, &hwq_attr);\n\tif (rc)\n\t\tgoto out;\n\t \n\ttqmctx->pde_level = tqmctx->pde.level;\n\n\thwq_attr.stride = 1;\n\tfor (i = 0; i < MAX_TQM_ALLOC_REQ; i++) {\n\t\tif (!tqmctx->qcount[i])\n\t\t\tcontinue;\n\t\thwq_attr.depth = ctx->qpc_count * tqmctx->qcount[i];\n\t\trc = bnxt_qplib_alloc_init_hwq(&tqmctx->qtbl[i], &hwq_attr);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\nout:\n\treturn rc;\n}\n\nstatic void bnxt_qplib_map_tqm_pgtbl(struct bnxt_qplib_tqm_ctx *ctx)\n{\n\tstruct bnxt_qplib_hwq *tbl;\n\tdma_addr_t *dma_ptr;\n\t__le64 **pbl_ptr, *ptr;\n\tint i, j, k;\n\tint fnz_idx = -1;\n\tint pg_count;\n\n\tpbl_ptr = (__le64 **)ctx->pde.pbl_ptr;\n\n\tfor (i = 0, j = 0; i < MAX_TQM_ALLOC_REQ;\n\t     i++, j += MAX_TQM_ALLOC_BLK_SIZE) {\n\t\ttbl = &ctx->qtbl[i];\n\t\tif (!tbl->max_elements)\n\t\t\tcontinue;\n\t\tif (fnz_idx == -1)\n\t\t\tfnz_idx = i;  \n\t\tswitch (tbl->level) {\n\t\tcase PBL_LVL_2:\n\t\t\tpg_count = tbl->pbl[PBL_LVL_1].pg_count;\n\t\t\tfor (k = 0; k < pg_count; k++) {\n\t\t\t\tptr = &pbl_ptr[PTR_PG(j + k)][PTR_IDX(j + k)];\n\t\t\t\tdma_ptr = &tbl->pbl[PBL_LVL_1].pg_map_arr[k];\n\t\t\t\t*ptr = cpu_to_le64(*dma_ptr | PTU_PTE_VALID);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase PBL_LVL_1:\n\t\tcase PBL_LVL_0:\n\t\tdefault:\n\t\t\tptr = &pbl_ptr[PTR_PG(j)][PTR_IDX(j)];\n\t\t\t*ptr = cpu_to_le64(tbl->pbl[PBL_LVL_0].pg_map_arr[0] |\n\t\t\t\t\t   PTU_PTE_VALID);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (fnz_idx == -1)\n\t\tfnz_idx = 0;\n\t \n\tctx->pde.level = (ctx->qtbl[fnz_idx].level == PBL_LVL_2) ? PBL_LVL_2 :\n\t\t\t  ctx->qtbl[fnz_idx].level + 1;\n}\n\nstatic int bnxt_qplib_setup_tqm_rings(struct bnxt_qplib_res *res,\n\t\t\t\t      struct bnxt_qplib_ctx *ctx)\n{\n\tint rc;\n\n\trc = bnxt_qplib_alloc_tqm_rings(res, ctx);\n\tif (rc)\n\t\tgoto fail;\n\n\tbnxt_qplib_map_tqm_pgtbl(&ctx->tqm_ctx);\nfail:\n\treturn rc;\n}\n\n \nint bnxt_qplib_alloc_ctx(struct bnxt_qplib_res *res,\n\t\t\t struct bnxt_qplib_ctx *ctx,\n\t\t\t bool virt_fn, bool is_p5)\n{\n\tstruct bnxt_qplib_hwq_attr hwq_attr = {};\n\tstruct bnxt_qplib_sg_info sginfo = {};\n\tint rc;\n\n\tif (virt_fn || is_p5)\n\t\tgoto stats_alloc;\n\n\t \n\tsginfo.pgsize = PAGE_SIZE;\n\tsginfo.pgshft = PAGE_SHIFT;\n\thwq_attr.sginfo = &sginfo;\n\n\thwq_attr.res = res;\n\thwq_attr.depth = ctx->qpc_count;\n\thwq_attr.stride = BNXT_QPLIB_MAX_QP_CTX_ENTRY_SIZE;\n\thwq_attr.type = HWQ_TYPE_CTX;\n\trc = bnxt_qplib_alloc_init_hwq(&ctx->qpc_tbl, &hwq_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\t \n\thwq_attr.depth = ctx->mrw_count;\n\thwq_attr.stride = BNXT_QPLIB_MAX_MRW_CTX_ENTRY_SIZE;\n\trc = bnxt_qplib_alloc_init_hwq(&ctx->mrw_tbl, &hwq_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\t \n\thwq_attr.depth = ctx->srqc_count;\n\thwq_attr.stride = BNXT_QPLIB_MAX_SRQ_CTX_ENTRY_SIZE;\n\trc = bnxt_qplib_alloc_init_hwq(&ctx->srqc_tbl, &hwq_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\t \n\thwq_attr.depth = ctx->cq_count;\n\thwq_attr.stride = BNXT_QPLIB_MAX_CQ_CTX_ENTRY_SIZE;\n\trc = bnxt_qplib_alloc_init_hwq(&ctx->cq_tbl, &hwq_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\t \n\trc = bnxt_qplib_setup_tqm_rings(res, ctx);\n\tif (rc)\n\t\tgoto fail;\n\t \n\tctx->tim_tbl.max_elements = ctx->qpc_count * 16;\n\thwq_attr.depth = ctx->qpc_count * 16;\n\thwq_attr.stride = 1;\n\trc = bnxt_qplib_alloc_init_hwq(&ctx->tim_tbl, &hwq_attr);\n\tif (rc)\n\t\tgoto fail;\nstats_alloc:\n\t \n\trc = bnxt_qplib_alloc_stats_ctx(res->pdev, res->cctx, &ctx->stats);\n\tif (rc)\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tbnxt_qplib_free_ctx(res, ctx);\n\treturn rc;\n}\n\nstatic void bnxt_qplib_free_sgid_tbl(struct bnxt_qplib_res *res,\n\t\t\t\t     struct bnxt_qplib_sgid_tbl *sgid_tbl)\n{\n\tkfree(sgid_tbl->tbl);\n\tkfree(sgid_tbl->hw_id);\n\tkfree(sgid_tbl->ctx);\n\tkfree(sgid_tbl->vlan);\n\tsgid_tbl->tbl = NULL;\n\tsgid_tbl->hw_id = NULL;\n\tsgid_tbl->ctx = NULL;\n\tsgid_tbl->vlan = NULL;\n\tsgid_tbl->max = 0;\n\tsgid_tbl->active = 0;\n}\n\nstatic int bnxt_qplib_alloc_sgid_tbl(struct bnxt_qplib_res *res,\n\t\t\t\t     struct bnxt_qplib_sgid_tbl *sgid_tbl,\n\t\t\t\t     u16 max)\n{\n\tsgid_tbl->tbl = kcalloc(max, sizeof(*sgid_tbl->tbl), GFP_KERNEL);\n\tif (!sgid_tbl->tbl)\n\t\treturn -ENOMEM;\n\n\tsgid_tbl->hw_id = kcalloc(max, sizeof(u16), GFP_KERNEL);\n\tif (!sgid_tbl->hw_id)\n\t\tgoto out_free1;\n\n\tsgid_tbl->ctx = kcalloc(max, sizeof(void *), GFP_KERNEL);\n\tif (!sgid_tbl->ctx)\n\t\tgoto out_free2;\n\n\tsgid_tbl->vlan = kcalloc(max, sizeof(u8), GFP_KERNEL);\n\tif (!sgid_tbl->vlan)\n\t\tgoto out_free3;\n\n\tsgid_tbl->max = max;\n\treturn 0;\nout_free3:\n\tkfree(sgid_tbl->ctx);\n\tsgid_tbl->ctx = NULL;\nout_free2:\n\tkfree(sgid_tbl->hw_id);\n\tsgid_tbl->hw_id = NULL;\nout_free1:\n\tkfree(sgid_tbl->tbl);\n\tsgid_tbl->tbl = NULL;\n\treturn -ENOMEM;\n};\n\nstatic void bnxt_qplib_cleanup_sgid_tbl(struct bnxt_qplib_res *res,\n\t\t\t\t\tstruct bnxt_qplib_sgid_tbl *sgid_tbl)\n{\n\tint i;\n\n\tfor (i = 0; i < sgid_tbl->max; i++) {\n\t\tif (memcmp(&sgid_tbl->tbl[i], &bnxt_qplib_gid_zero,\n\t\t\t   sizeof(bnxt_qplib_gid_zero)))\n\t\t\tbnxt_qplib_del_sgid(sgid_tbl, &sgid_tbl->tbl[i].gid,\n\t\t\t\t\t    sgid_tbl->tbl[i].vlan_id, true);\n\t}\n\tmemset(sgid_tbl->tbl, 0, sizeof(*sgid_tbl->tbl) * sgid_tbl->max);\n\tmemset(sgid_tbl->hw_id, -1, sizeof(u16) * sgid_tbl->max);\n\tmemset(sgid_tbl->vlan, 0, sizeof(u8) * sgid_tbl->max);\n\tsgid_tbl->active = 0;\n}\n\nstatic void bnxt_qplib_init_sgid_tbl(struct bnxt_qplib_sgid_tbl *sgid_tbl,\n\t\t\t\t     struct net_device *netdev)\n{\n\tu32 i;\n\n\tfor (i = 0; i < sgid_tbl->max; i++)\n\t\tsgid_tbl->tbl[i].vlan_id = 0xffff;\n\n\tmemset(sgid_tbl->hw_id, -1, sizeof(u16) * sgid_tbl->max);\n}\n\n \nint bnxt_qplib_alloc_pd(struct bnxt_qplib_res  *res, struct bnxt_qplib_pd *pd)\n{\n\tstruct bnxt_qplib_pd_tbl *pdt = &res->pd_tbl;\n\tu32 bit_num;\n\tint rc = 0;\n\n\tmutex_lock(&res->pd_tbl_lock);\n\tbit_num = find_first_bit(pdt->tbl, pdt->max);\n\tif (bit_num == pdt->max) {\n\t\trc = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\t \n\tclear_bit(bit_num, pdt->tbl);\n\tpd->id = bit_num;\nexit:\n\tmutex_unlock(&res->pd_tbl_lock);\n\treturn rc;\n}\n\nint bnxt_qplib_dealloc_pd(struct bnxt_qplib_res *res,\n\t\t\t  struct bnxt_qplib_pd_tbl *pdt,\n\t\t\t  struct bnxt_qplib_pd *pd)\n{\n\tint rc = 0;\n\n\tmutex_lock(&res->pd_tbl_lock);\n\tif (test_and_set_bit(pd->id, pdt->tbl)) {\n\t\tdev_warn(&res->pdev->dev, \"Freeing an unused PD? pdn = %d\\n\",\n\t\t\t pd->id);\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\tpd->id = 0;\nexit:\n\tmutex_unlock(&res->pd_tbl_lock);\n\treturn rc;\n}\n\nstatic void bnxt_qplib_free_pd_tbl(struct bnxt_qplib_pd_tbl *pdt)\n{\n\tkfree(pdt->tbl);\n\tpdt->tbl = NULL;\n\tpdt->max = 0;\n}\n\nstatic int bnxt_qplib_alloc_pd_tbl(struct bnxt_qplib_res *res,\n\t\t\t\t   struct bnxt_qplib_pd_tbl *pdt,\n\t\t\t\t   u32 max)\n{\n\tu32 bytes;\n\n\tbytes = max >> 3;\n\tif (!bytes)\n\t\tbytes = 1;\n\tpdt->tbl = kmalloc(bytes, GFP_KERNEL);\n\tif (!pdt->tbl)\n\t\treturn -ENOMEM;\n\n\tpdt->max = max;\n\tmemset((u8 *)pdt->tbl, 0xFF, bytes);\n\tmutex_init(&res->pd_tbl_lock);\n\n\treturn 0;\n}\n\n \nint bnxt_qplib_alloc_dpi(struct bnxt_qplib_res *res,\n\t\t\t struct bnxt_qplib_dpi *dpi,\n\t\t\t void *app, u8 type)\n{\n\tstruct bnxt_qplib_dpi_tbl *dpit = &res->dpi_tbl;\n\tstruct bnxt_qplib_reg_desc *reg;\n\tu32 bit_num;\n\tu64 umaddr;\n\n\treg = &dpit->wcreg;\n\tmutex_lock(&res->dpi_tbl_lock);\n\n\tbit_num = find_first_bit(dpit->tbl, dpit->max);\n\tif (bit_num == dpit->max) {\n\t\tmutex_unlock(&res->dpi_tbl_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tclear_bit(bit_num, dpit->tbl);\n\tdpit->app_tbl[bit_num] = app;\n\n\tdpi->bit = bit_num;\n\tdpi->dpi = bit_num + (reg->offset - dpit->ucreg.offset) / PAGE_SIZE;\n\n\tumaddr = reg->bar_base + reg->offset + bit_num * PAGE_SIZE;\n\tdpi->umdbr = umaddr;\n\n\tswitch (type) {\n\tcase BNXT_QPLIB_DPI_TYPE_KERNEL:\n\t\t \n\t\tdpi->umdbr = dpit->ucreg.bar_base +\n\t\t\t     dpit->ucreg.offset + bit_num * PAGE_SIZE;\n\t\tdpi->dbr = dpit->priv_db;\n\t\tdpi->dpi = dpi->bit;\n\t\tbreak;\n\tcase BNXT_QPLIB_DPI_TYPE_WC:\n\t\tdpi->dbr = ioremap_wc(umaddr, PAGE_SIZE);\n\t\tbreak;\n\tdefault:\n\t\tdpi->dbr = ioremap(umaddr, PAGE_SIZE);\n\t\tbreak;\n\t}\n\n\tdpi->type = type;\n\tmutex_unlock(&res->dpi_tbl_lock);\n\treturn 0;\n\n}\n\nint bnxt_qplib_dealloc_dpi(struct bnxt_qplib_res *res,\n\t\t\t   struct bnxt_qplib_dpi *dpi)\n{\n\tstruct bnxt_qplib_dpi_tbl *dpit = &res->dpi_tbl;\n\n\tmutex_lock(&res->dpi_tbl_lock);\n\tif (dpi->dpi && dpi->type != BNXT_QPLIB_DPI_TYPE_KERNEL)\n\t\tpci_iounmap(res->pdev, dpi->dbr);\n\n\tif (test_and_set_bit(dpi->bit, dpit->tbl)) {\n\t\tdev_warn(&res->pdev->dev,\n\t\t\t \"Freeing an unused DPI? dpi = %d, bit = %d\\n\",\n\t\t\t\tdpi->dpi, dpi->bit);\n\t\tmutex_unlock(&res->dpi_tbl_lock);\n\t\treturn -EINVAL;\n\t}\n\tif (dpit->app_tbl)\n\t\tdpit->app_tbl[dpi->bit] = NULL;\n\tmemset(dpi, 0, sizeof(*dpi));\n\tmutex_unlock(&res->dpi_tbl_lock);\n\treturn 0;\n}\n\nstatic void bnxt_qplib_free_dpi_tbl(struct bnxt_qplib_res     *res,\n\t\t\t\t    struct bnxt_qplib_dpi_tbl *dpit)\n{\n\tkfree(dpit->tbl);\n\tkfree(dpit->app_tbl);\n\tdpit->tbl = NULL;\n\tdpit->app_tbl = NULL;\n\tdpit->max = 0;\n}\n\nstatic int bnxt_qplib_alloc_dpi_tbl(struct bnxt_qplib_res *res,\n\t\t\t\t    struct bnxt_qplib_dev_attr *dev_attr)\n{\n\tstruct bnxt_qplib_dpi_tbl *dpit;\n\tstruct bnxt_qplib_reg_desc *reg;\n\tunsigned long bar_len;\n\tu32 dbr_offset;\n\tu32 bytes;\n\n\tdpit = &res->dpi_tbl;\n\treg = &dpit->wcreg;\n\n\tif (!bnxt_qplib_is_chip_gen_p5(res->cctx)) {\n\t\t \n\t\tdbr_offset = dev_attr->l2_db_size;\n\t\tdpit->ucreg.offset = dbr_offset;\n\t\tdpit->wcreg.offset = dbr_offset;\n\t}\n\n\tbar_len = pci_resource_len(res->pdev, reg->bar_id);\n\tdpit->max = (bar_len - reg->offset) / PAGE_SIZE;\n\tif (dev_attr->max_dpi)\n\t\tdpit->max = min_t(u32, dpit->max, dev_attr->max_dpi);\n\n\tdpit->app_tbl = kcalloc(dpit->max,  sizeof(void *), GFP_KERNEL);\n\tif (!dpit->app_tbl)\n\t\treturn -ENOMEM;\n\n\tbytes = dpit->max >> 3;\n\tif (!bytes)\n\t\tbytes = 1;\n\n\tdpit->tbl = kmalloc(bytes, GFP_KERNEL);\n\tif (!dpit->tbl) {\n\t\tkfree(dpit->app_tbl);\n\t\tdpit->app_tbl = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset((u8 *)dpit->tbl, 0xFF, bytes);\n\tmutex_init(&res->dpi_tbl_lock);\n\tdpit->priv_db = dpit->ucreg.bar_reg + dpit->ucreg.offset;\n\n\treturn 0;\n\n}\n\n \nstatic void bnxt_qplib_free_stats_ctx(struct pci_dev *pdev,\n\t\t\t\t      struct bnxt_qplib_stats *stats)\n{\n\tif (stats->dma) {\n\t\tdma_free_coherent(&pdev->dev, stats->size,\n\t\t\t\t  stats->dma, stats->dma_map);\n\t}\n\tmemset(stats, 0, sizeof(*stats));\n\tstats->fw_id = -1;\n}\n\nstatic int bnxt_qplib_alloc_stats_ctx(struct pci_dev *pdev,\n\t\t\t\t      struct bnxt_qplib_chip_ctx *cctx,\n\t\t\t\t      struct bnxt_qplib_stats *stats)\n{\n\tmemset(stats, 0, sizeof(*stats));\n\tstats->fw_id = -1;\n\tstats->size = cctx->hw_stats_size;\n\tstats->dma = dma_alloc_coherent(&pdev->dev, stats->size,\n\t\t\t\t\t&stats->dma_map, GFP_KERNEL);\n\tif (!stats->dma) {\n\t\tdev_err(&pdev->dev, \"Stats DMA allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid bnxt_qplib_cleanup_res(struct bnxt_qplib_res *res)\n{\n\tbnxt_qplib_cleanup_sgid_tbl(res, &res->sgid_tbl);\n}\n\nint bnxt_qplib_init_res(struct bnxt_qplib_res *res)\n{\n\tbnxt_qplib_init_sgid_tbl(&res->sgid_tbl, res->netdev);\n\n\treturn 0;\n}\n\nvoid bnxt_qplib_free_res(struct bnxt_qplib_res *res)\n{\n\tbnxt_qplib_free_sgid_tbl(res, &res->sgid_tbl);\n\tbnxt_qplib_free_pd_tbl(&res->pd_tbl);\n\tbnxt_qplib_free_dpi_tbl(res, &res->dpi_tbl);\n}\n\nint bnxt_qplib_alloc_res(struct bnxt_qplib_res *res, struct pci_dev *pdev,\n\t\t\t struct net_device *netdev,\n\t\t\t struct bnxt_qplib_dev_attr *dev_attr)\n{\n\tint rc;\n\n\tres->pdev = pdev;\n\tres->netdev = netdev;\n\n\trc = bnxt_qplib_alloc_sgid_tbl(res, &res->sgid_tbl, dev_attr->max_sgid);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = bnxt_qplib_alloc_pd_tbl(res, &res->pd_tbl, dev_attr->max_pd);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = bnxt_qplib_alloc_dpi_tbl(res, dev_attr);\n\tif (rc)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\tbnxt_qplib_free_res(res);\n\treturn rc;\n}\n\nvoid bnxt_qplib_unmap_db_bar(struct bnxt_qplib_res *res)\n{\n\tstruct bnxt_qplib_reg_desc *reg;\n\n\treg = &res->dpi_tbl.ucreg;\n\tif (reg->bar_reg)\n\t\tpci_iounmap(res->pdev, reg->bar_reg);\n\treg->bar_reg = NULL;\n\treg->bar_base = 0;\n\treg->len = 0;\n\treg->bar_id = 0;\n}\n\nint bnxt_qplib_map_db_bar(struct bnxt_qplib_res *res)\n{\n\tstruct bnxt_qplib_reg_desc *ucreg;\n\tstruct bnxt_qplib_reg_desc *wcreg;\n\n\twcreg = &res->dpi_tbl.wcreg;\n\twcreg->bar_id = RCFW_DBR_PCI_BAR_REGION;\n\twcreg->bar_base = pci_resource_start(res->pdev, wcreg->bar_id);\n\n\tucreg = &res->dpi_tbl.ucreg;\n\tucreg->bar_id = RCFW_DBR_PCI_BAR_REGION;\n\tucreg->bar_base = pci_resource_start(res->pdev, ucreg->bar_id);\n\tucreg->len = ucreg->offset + PAGE_SIZE;\n\tif (!ucreg->len || ((ucreg->len & (PAGE_SIZE - 1)) != 0)) {\n\t\tdev_err(&res->pdev->dev, \"QPLIB: invalid dbr length %d\",\n\t\t\t(int)ucreg->len);\n\t\treturn -EINVAL;\n\t}\n\tucreg->bar_reg = ioremap(ucreg->bar_base, ucreg->len);\n\tif (!ucreg->bar_reg) {\n\t\tdev_err(&res->pdev->dev, \"privileged dpi map failed!\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint bnxt_qplib_determine_atomics(struct pci_dev *dev)\n{\n\tint comp;\n\tu16 ctl2;\n\n\tcomp = pci_enable_atomic_ops_to_root(dev,\n\t\t\t\t\t     PCI_EXP_DEVCAP2_ATOMIC_COMP32);\n\tif (comp)\n\t\treturn -EOPNOTSUPP;\n\tcomp = pci_enable_atomic_ops_to_root(dev,\n\t\t\t\t\t     PCI_EXP_DEVCAP2_ATOMIC_COMP64);\n\tif (comp)\n\t\treturn -EOPNOTSUPP;\n\tpcie_capability_read_word(dev, PCI_EXP_DEVCTL2, &ctl2);\n\treturn !(ctl2 & PCI_EXP_DEVCTL2_ATOMIC_REQ);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}