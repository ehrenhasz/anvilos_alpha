{
  "module_name": "main.c",
  "hash_id": "278aceecd33fc93219b3ef692a1c29f0882f805be1f12108ec5ad3832f725d30",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/main.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/netdevice.h>\n#include <linux/inetdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/if_vlan.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n\n#include <net/ipv6.h>\n#include <net/addrconf.h>\n#include <net/devlink.h>\n\n#include <rdma/ib_smi.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n\n#include <net/bonding.h>\n\n#include <linux/mlx4/driver.h>\n#include <linux/mlx4/cmd.h>\n#include <linux/mlx4/qp.h>\n\n#include \"mlx4_ib.h\"\n#include <rdma/mlx4-abi.h>\n\n#define DRV_NAME\tMLX4_IB_DRV_NAME\n#define DRV_VERSION\t\"4.0-0\"\n\n#define MLX4_IB_FLOW_MAX_PRIO 0xFFF\n#define MLX4_IB_FLOW_QPN_MASK 0xFFFFFF\n#define MLX4_IB_CARD_REV_A0   0xA0\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"Mellanox ConnectX HCA InfiniBand driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nint mlx4_ib_sm_guid_assign = 0;\nmodule_param_named(sm_guid_assign, mlx4_ib_sm_guid_assign, int, 0444);\nMODULE_PARM_DESC(sm_guid_assign, \"Enable SM alias_GUID assignment if sm_guid_assign > 0 (Default: 0)\");\n\nstatic const char mlx4_ib_version[] =\n\tDRV_NAME \": Mellanox ConnectX InfiniBand driver v\"\n\tDRV_VERSION \"\\n\";\n\nstatic void do_slave_init(struct mlx4_ib_dev *ibdev, int slave, int do_init);\nstatic enum rdma_link_layer mlx4_ib_port_link_layer(struct ib_device *device,\n\t\t\t\t\t\t    u32 port_num);\nstatic int mlx4_ib_event(struct notifier_block *this, unsigned long event,\n\t\t\t void *param);\n\nstatic struct workqueue_struct *wq;\n\nstatic int check_flow_steering_support(struct mlx4_dev *dev)\n{\n\tint eth_num_ports = 0;\n\tint ib_num_ports = 0;\n\n\tint dmfs = dev->caps.steering_mode == MLX4_STEERING_MODE_DEVICE_MANAGED;\n\n\tif (dmfs) {\n\t\tint i;\n\t\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH)\n\t\t\teth_num_ports++;\n\t\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)\n\t\t\tib_num_ports++;\n\t\tdmfs &= (!ib_num_ports ||\n\t\t\t (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DMFS_IPOIB)) &&\n\t\t\t(!eth_num_ports ||\n\t\t\t (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_FS_EN));\n\t\tif (ib_num_ports && mlx4_is_mfunc(dev)) {\n\t\t\tpr_warn(\"Device managed flow steering is unavailable for IB port in multifunction env.\\n\");\n\t\t\tdmfs = 0;\n\t\t}\n\t}\n\treturn dmfs;\n}\n\nstatic int num_ib_ports(struct mlx4_dev *dev)\n{\n\tint ib_ports = 0;\n\tint i;\n\n\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)\n\t\tib_ports++;\n\n\treturn ib_ports;\n}\n\nstatic struct net_device *mlx4_ib_get_netdev(struct ib_device *device,\n\t\t\t\t\t     u32 port_num)\n{\n\tstruct mlx4_ib_dev *ibdev = to_mdev(device);\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(&init_net, dev) {\n\t\tif (dev->dev.parent != ibdev->ib_dev.dev.parent ||\n\t\t    dev->dev_port + 1 != port_num)\n\t\t\tcontinue;\n\n\t\tif (mlx4_is_bonded(ibdev->dev)) {\n\t\t\tstruct net_device *upper;\n\n\t\t\tupper = netdev_master_upper_dev_get_rcu(dev);\n\t\t\tif (upper) {\n\t\t\t\tstruct net_device *active;\n\n\t\t\t\tactive = bond_option_active_slave_get_rcu(netdev_priv(upper));\n\t\t\t\tif (active)\n\t\t\t\t\tdev = active;\n\t\t\t}\n\t\t}\n\n\t\tdev_hold(dev);\n\t\tret = dev;\n\t\tbreak;\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic int mlx4_ib_update_gids_v1(struct gid_entry *gids,\n\t\t\t\t  struct mlx4_ib_dev *ibdev,\n\t\t\t\t  u32 port_num)\n{\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tint err;\n\tstruct mlx4_dev *dev = ibdev->dev;\n\tint i;\n\tunion ib_gid *gid_tbl;\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(mailbox))\n\t\treturn -ENOMEM;\n\n\tgid_tbl = mailbox->buf;\n\n\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; ++i)\n\t\tmemcpy(&gid_tbl[i], &gids[i].gid, sizeof(union ib_gid));\n\n\terr = mlx4_cmd(dev, mailbox->dma,\n\t\t       MLX4_SET_PORT_GID_TABLE << 8 | port_num,\n\t\t       1, MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t       MLX4_CMD_WRAPPED);\n\tif (mlx4_is_bonded(dev))\n\t\terr += mlx4_cmd(dev, mailbox->dma,\n\t\t\t\tMLX4_SET_PORT_GID_TABLE << 8 | 2,\n\t\t\t\t1, MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t\t\tMLX4_CMD_WRAPPED);\n\n\tmlx4_free_cmd_mailbox(dev, mailbox);\n\treturn err;\n}\n\nstatic int mlx4_ib_update_gids_v1_v2(struct gid_entry *gids,\n\t\t\t\t     struct mlx4_ib_dev *ibdev,\n\t\t\t\t     u32 port_num)\n{\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tint err;\n\tstruct mlx4_dev *dev = ibdev->dev;\n\tint i;\n\tstruct {\n\t\tunion ib_gid\tgid;\n\t\t__be32\t\trsrvd1[2];\n\t\t__be16\t\trsrvd2;\n\t\tu8\t\ttype;\n\t\tu8\t\tversion;\n\t\t__be32\t\trsrvd3;\n\t} *gid_tbl;\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(mailbox))\n\t\treturn -ENOMEM;\n\n\tgid_tbl = mailbox->buf;\n\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; ++i) {\n\t\tmemcpy(&gid_tbl[i].gid, &gids[i].gid, sizeof(union ib_gid));\n\t\tif (gids[i].gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) {\n\t\t\tgid_tbl[i].version = 2;\n\t\t\tif (!ipv6_addr_v4mapped((struct in6_addr *)&gids[i].gid))\n\t\t\t\tgid_tbl[i].type = 1;\n\t\t}\n\t}\n\n\terr = mlx4_cmd(dev, mailbox->dma,\n\t\t       MLX4_SET_PORT_ROCE_ADDR << 8 | port_num,\n\t\t       1, MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t       MLX4_CMD_WRAPPED);\n\tif (mlx4_is_bonded(dev))\n\t\terr += mlx4_cmd(dev, mailbox->dma,\n\t\t\t\tMLX4_SET_PORT_ROCE_ADDR << 8 | 2,\n\t\t\t\t1, MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t\t\tMLX4_CMD_WRAPPED);\n\n\tmlx4_free_cmd_mailbox(dev, mailbox);\n\treturn err;\n}\n\nstatic int mlx4_ib_update_gids(struct gid_entry *gids,\n\t\t\t       struct mlx4_ib_dev *ibdev,\n\t\t\t       u32 port_num)\n{\n\tif (ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ROCE_V1_V2)\n\t\treturn mlx4_ib_update_gids_v1_v2(gids, ibdev, port_num);\n\n\treturn mlx4_ib_update_gids_v1(gids, ibdev, port_num);\n}\n\nstatic void free_gid_entry(struct gid_entry *entry)\n{\n\tmemset(&entry->gid, 0, sizeof(entry->gid));\n\tkfree(entry->ctx);\n\tentry->ctx = NULL;\n}\n\nstatic int mlx4_ib_add_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tstruct mlx4_ib_dev *ibdev = to_mdev(attr->device);\n\tstruct mlx4_ib_iboe *iboe = &ibdev->iboe;\n\tstruct mlx4_port_gid_table   *port_gid_table;\n\tint free = -1, found = -1;\n\tint ret = 0;\n\tint hw_update = 0;\n\tint i;\n\tstruct gid_entry *gids;\n\tu16 vlan_id = 0xffff;\n\tu8 mac[ETH_ALEN];\n\n\tif (!rdma_cap_roce_gid_table(attr->device, attr->port_num))\n\t\treturn -EINVAL;\n\n\tif (attr->port_num > MLX4_MAX_PORTS)\n\t\treturn -EINVAL;\n\n\tif (!context)\n\t\treturn -EINVAL;\n\n\tret = rdma_read_gid_l2_fields(attr, &vlan_id, &mac[0]);\n\tif (ret)\n\t\treturn ret;\n\tport_gid_table = &iboe->gids[attr->port_num - 1];\n\tspin_lock_bh(&iboe->lock);\n\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; ++i) {\n\t\tif (!memcmp(&port_gid_table->gids[i].gid,\n\t\t\t    &attr->gid, sizeof(attr->gid)) &&\n\t\t    port_gid_table->gids[i].gid_type == attr->gid_type &&\n\t\t    port_gid_table->gids[i].vlan_id == vlan_id)  {\n\t\t\tfound = i;\n\t\t\tbreak;\n\t\t}\n\t\tif (free < 0 && rdma_is_zero_gid(&port_gid_table->gids[i].gid))\n\t\t\tfree = i;  \n\t}\n\n\tif (found < 0) {\n\t\tif (free < 0) {\n\t\t\tret = -ENOSPC;\n\t\t} else {\n\t\t\tport_gid_table->gids[free].ctx = kmalloc(sizeof(*port_gid_table->gids[free].ctx), GFP_ATOMIC);\n\t\t\tif (!port_gid_table->gids[free].ctx) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t} else {\n\t\t\t\t*context = port_gid_table->gids[free].ctx;\n\t\t\t\tport_gid_table->gids[free].gid = attr->gid;\n\t\t\t\tport_gid_table->gids[free].gid_type = attr->gid_type;\n\t\t\t\tport_gid_table->gids[free].vlan_id = vlan_id;\n\t\t\t\tport_gid_table->gids[free].ctx->real_index = free;\n\t\t\t\tport_gid_table->gids[free].ctx->refcount = 1;\n\t\t\t\thw_update = 1;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstruct gid_cache_context *ctx = port_gid_table->gids[found].ctx;\n\t\t*context = ctx;\n\t\tctx->refcount++;\n\t}\n\tif (!ret && hw_update) {\n\t\tgids = kmalloc_array(MLX4_MAX_PORT_GIDS, sizeof(*gids),\n\t\t\t\t     GFP_ATOMIC);\n\t\tif (!gids) {\n\t\t\tret = -ENOMEM;\n\t\t\t*context = NULL;\n\t\t\tfree_gid_entry(&port_gid_table->gids[free]);\n\t\t} else {\n\t\t\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; i++) {\n\t\t\t\tmemcpy(&gids[i].gid, &port_gid_table->gids[i].gid, sizeof(union ib_gid));\n\t\t\t\tgids[i].gid_type = port_gid_table->gids[i].gid_type;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_bh(&iboe->lock);\n\n\tif (!ret && hw_update) {\n\t\tret = mlx4_ib_update_gids(gids, ibdev, attr->port_num);\n\t\tif (ret) {\n\t\t\tspin_lock_bh(&iboe->lock);\n\t\t\t*context = NULL;\n\t\t\tfree_gid_entry(&port_gid_table->gids[free]);\n\t\t\tspin_unlock_bh(&iboe->lock);\n\t\t}\n\t\tkfree(gids);\n\t}\n\n\treturn ret;\n}\n\nstatic int mlx4_ib_del_gid(const struct ib_gid_attr *attr, void **context)\n{\n\tstruct gid_cache_context *ctx = *context;\n\tstruct mlx4_ib_dev *ibdev = to_mdev(attr->device);\n\tstruct mlx4_ib_iboe *iboe = &ibdev->iboe;\n\tstruct mlx4_port_gid_table   *port_gid_table;\n\tint ret = 0;\n\tint hw_update = 0;\n\tstruct gid_entry *gids;\n\n\tif (!rdma_cap_roce_gid_table(attr->device, attr->port_num))\n\t\treturn -EINVAL;\n\n\tif (attr->port_num > MLX4_MAX_PORTS)\n\t\treturn -EINVAL;\n\n\tport_gid_table = &iboe->gids[attr->port_num - 1];\n\tspin_lock_bh(&iboe->lock);\n\tif (ctx) {\n\t\tctx->refcount--;\n\t\tif (!ctx->refcount) {\n\t\t\tunsigned int real_index = ctx->real_index;\n\n\t\t\tfree_gid_entry(&port_gid_table->gids[real_index]);\n\t\t\thw_update = 1;\n\t\t}\n\t}\n\tif (!ret && hw_update) {\n\t\tint i;\n\n\t\tgids = kmalloc_array(MLX4_MAX_PORT_GIDS, sizeof(*gids),\n\t\t\t\t     GFP_ATOMIC);\n\t\tif (!gids) {\n\t\t\tret = -ENOMEM;\n\t\t} else {\n\t\t\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; i++) {\n\t\t\t\tmemcpy(&gids[i].gid,\n\t\t\t\t       &port_gid_table->gids[i].gid,\n\t\t\t\t       sizeof(union ib_gid));\n\t\t\t\tgids[i].gid_type =\n\t\t\t\t    port_gid_table->gids[i].gid_type;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_bh(&iboe->lock);\n\n\tif (!ret && hw_update) {\n\t\tret = mlx4_ib_update_gids(gids, ibdev, attr->port_num);\n\t\tkfree(gids);\n\t}\n\treturn ret;\n}\n\nint mlx4_ib_gid_index_to_real_index(struct mlx4_ib_dev *ibdev,\n\t\t\t\t    const struct ib_gid_attr *attr)\n{\n\tstruct mlx4_ib_iboe *iboe = &ibdev->iboe;\n\tstruct gid_cache_context *ctx = NULL;\n\tstruct mlx4_port_gid_table   *port_gid_table;\n\tint real_index = -EINVAL;\n\tint i;\n\tunsigned long flags;\n\tu32 port_num = attr->port_num;\n\n\tif (port_num > MLX4_MAX_PORTS)\n\t\treturn -EINVAL;\n\n\tif (mlx4_is_bonded(ibdev->dev))\n\t\tport_num = 1;\n\n\tif (!rdma_cap_roce_gid_table(&ibdev->ib_dev, port_num))\n\t\treturn attr->index;\n\n\tspin_lock_irqsave(&iboe->lock, flags);\n\tport_gid_table = &iboe->gids[port_num - 1];\n\n\tfor (i = 0; i < MLX4_MAX_PORT_GIDS; ++i)\n\t\tif (!memcmp(&port_gid_table->gids[i].gid,\n\t\t\t    &attr->gid, sizeof(attr->gid)) &&\n\t\t    attr->gid_type == port_gid_table->gids[i].gid_type) {\n\t\t\tctx = port_gid_table->gids[i].ctx;\n\t\t\tbreak;\n\t\t}\n\tif (ctx)\n\t\treal_index = ctx->real_index;\n\tspin_unlock_irqrestore(&iboe->lock, flags);\n\treturn real_index;\n}\n\nstatic int mlx4_ib_query_device(struct ib_device *ibdev,\n\t\t\t\tstruct ib_device_attr *props,\n\t\t\t\tstruct ib_udata *uhw)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint err;\n\tint have_ib_ports;\n\tstruct mlx4_uverbs_ex_query_device cmd;\n\tstruct mlx4_uverbs_ex_query_device_resp resp = {};\n\tstruct mlx4_clock_params clock_params;\n\n\tif (uhw->inlen) {\n\t\tif (uhw->inlen < sizeof(cmd))\n\t\t\treturn -EINVAL;\n\n\t\terr = ib_copy_from_udata(&cmd, uhw, sizeof(cmd));\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (cmd.comp_mask)\n\t\t\treturn -EINVAL;\n\n\t\tif (cmd.reserved)\n\t\t\treturn -EINVAL;\n\t}\n\n\tresp.response_length = offsetof(typeof(resp), response_length) +\n\t\tsizeof(resp.response_length);\n\tin_mad  = kzalloc(sizeof *in_mad, GFP_KERNEL);\n\tout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id = IB_SMP_ATTR_NODE_INFO;\n\n\terr = mlx4_MAD_IFC(to_mdev(ibdev), MLX4_MAD_IFC_IGNORE_KEYS,\n\t\t\t   1, NULL, NULL, in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tmemset(props, 0, sizeof *props);\n\n\thave_ib_ports = num_ib_ports(dev->dev);\n\n\tprops->fw_ver = dev->dev->caps.fw_ver;\n\tprops->device_cap_flags    = IB_DEVICE_CHANGE_PHY_PORT |\n\t\tIB_DEVICE_PORT_ACTIVE_EVENT\t\t|\n\t\tIB_DEVICE_SYS_IMAGE_GUID\t\t|\n\t\tIB_DEVICE_RC_RNR_NAK_GEN;\n\tprops->kernel_cap_flags = IBK_BLOCK_MULTICAST_LOOPBACK;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_BAD_PKEY_CNTR)\n\t\tprops->device_cap_flags |= IB_DEVICE_BAD_PKEY_CNTR;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_BAD_QKEY_CNTR)\n\t\tprops->device_cap_flags |= IB_DEVICE_BAD_QKEY_CNTR;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_APM && have_ib_ports)\n\t\tprops->device_cap_flags |= IB_DEVICE_AUTO_PATH_MIG;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_UD_AV_PORT)\n\t\tprops->device_cap_flags |= IB_DEVICE_UD_AV_PORT_ENFORCE;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_IPOIB_CSUM)\n\t\tprops->device_cap_flags |= IB_DEVICE_UD_IP_CSUM;\n\tif (dev->dev->caps.max_gso_sz &&\n\t    (dev->dev->rev_id != MLX4_IB_CARD_REV_A0) &&\n\t    (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_BLH))\n\t\tprops->kernel_cap_flags |= IBK_UD_TSO;\n\tif (dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_RESERVED_LKEY)\n\t\tprops->kernel_cap_flags |= IBK_LOCAL_DMA_LKEY;\n\tif ((dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_LOCAL_INV) &&\n\t    (dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_REMOTE_INV) &&\n\t    (dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_FAST_REG_WR))\n\t\tprops->device_cap_flags |= IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC)\n\t\tprops->device_cap_flags |= IB_DEVICE_XRC;\n\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_MEM_WINDOW)\n\t\tprops->device_cap_flags |= IB_DEVICE_MEM_WINDOW;\n\tif (dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_TYPE_2_WIN) {\n\t\tif (dev->dev->caps.bmme_flags & MLX4_BMME_FLAG_WIN_TYPE_2B)\n\t\t\tprops->device_cap_flags |= IB_DEVICE_MEM_WINDOW_TYPE_2B;\n\t\telse\n\t\t\tprops->device_cap_flags |= IB_DEVICE_MEM_WINDOW_TYPE_2A;\n\t}\n\tif (dev->steering_support == MLX4_STEERING_MODE_DEVICE_MANAGED)\n\t\tprops->device_cap_flags |= IB_DEVICE_MANAGED_FLOW_STEERING;\n\n\tprops->device_cap_flags |= IB_DEVICE_RAW_IP_CSUM;\n\n\tprops->vendor_id\t   = be32_to_cpup((__be32 *) (out_mad->data + 36)) &\n\t\t0xffffff;\n\tprops->vendor_part_id\t   = dev->dev->persist->pdev->device;\n\tprops->hw_ver\t\t   = be32_to_cpup((__be32 *) (out_mad->data + 32));\n\tmemcpy(&props->sys_image_guid, out_mad->data +\t4, 8);\n\n\tprops->max_mr_size\t   = ~0ull;\n\tprops->page_size_cap\t   = dev->dev->caps.page_size_cap;\n\tprops->max_qp\t\t   = dev->dev->quotas.qp;\n\tprops->max_qp_wr\t   = dev->dev->caps.max_wqes - MLX4_IB_SQ_MAX_SPARE;\n\tprops->max_send_sge =\n\t\tmin(dev->dev->caps.max_sq_sg, dev->dev->caps.max_rq_sg);\n\tprops->max_recv_sge =\n\t\tmin(dev->dev->caps.max_sq_sg, dev->dev->caps.max_rq_sg);\n\tprops->max_sge_rd = MLX4_MAX_SGE_RD;\n\tprops->max_cq\t\t   = dev->dev->quotas.cq;\n\tprops->max_cqe\t\t   = dev->dev->caps.max_cqes;\n\tprops->max_mr\t\t   = dev->dev->quotas.mpt;\n\tprops->max_pd\t\t   = dev->dev->caps.num_pds - dev->dev->caps.reserved_pds;\n\tprops->max_qp_rd_atom\t   = dev->dev->caps.max_qp_dest_rdma;\n\tprops->max_qp_init_rd_atom = dev->dev->caps.max_qp_init_rdma;\n\tprops->max_res_rd_atom\t   = props->max_qp_rd_atom * props->max_qp;\n\tprops->max_srq\t\t   = dev->dev->quotas.srq;\n\tprops->max_srq_wr\t   = dev->dev->caps.max_srq_wqes - 1;\n\tprops->max_srq_sge\t   = dev->dev->caps.max_srq_sge;\n\tprops->max_fast_reg_page_list_len = MLX4_MAX_FAST_REG_PAGES;\n\tprops->local_ca_ack_delay  = dev->dev->caps.local_ca_ack_delay;\n\tprops->atomic_cap\t   = dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_ATOMIC ?\n\t\tIB_ATOMIC_HCA : IB_ATOMIC_NONE;\n\tprops->masked_atomic_cap   = props->atomic_cap;\n\tprops->max_pkeys\t   = dev->dev->caps.pkey_table_len[1];\n\tprops->max_mcast_grp\t   = dev->dev->caps.num_mgms + dev->dev->caps.num_amgms;\n\tprops->max_mcast_qp_attach = dev->dev->caps.num_qp_per_mgm;\n\tprops->max_total_mcast_qp_attach = props->max_mcast_qp_attach *\n\t\t\t\t\t   props->max_mcast_grp;\n\tprops->hca_core_clock = dev->dev->caps.hca_core_clock * 1000UL;\n\tprops->timestamp_mask = 0xFFFFFFFFFFFFULL;\n\tprops->max_ah = INT_MAX;\n\n\tif (mlx4_ib_port_link_layer(ibdev, 1) == IB_LINK_LAYER_ETHERNET ||\n\t    mlx4_ib_port_link_layer(ibdev, 2) == IB_LINK_LAYER_ETHERNET) {\n\t\tif (dev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS) {\n\t\t\tprops->rss_caps.max_rwq_indirection_tables =\n\t\t\t\tprops->max_qp;\n\t\t\tprops->rss_caps.max_rwq_indirection_table_size =\n\t\t\t\tdev->dev->caps.max_rss_tbl_sz;\n\t\t\tprops->rss_caps.supported_qpts = 1 << IB_QPT_RAW_PACKET;\n\t\t\tprops->max_wq_type_rq = props->max_qp;\n\t\t}\n\n\t\tif (dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP)\n\t\t\tprops->raw_packet_caps |= IB_RAW_PACKET_CAP_SCATTER_FCS;\n\t}\n\n\tprops->cq_caps.max_cq_moderation_count = MLX4_MAX_CQ_COUNT;\n\tprops->cq_caps.max_cq_moderation_period = MLX4_MAX_CQ_PERIOD;\n\n\tif (uhw->outlen >= resp.response_length + sizeof(resp.hca_core_clock_offset)) {\n\t\tresp.response_length += sizeof(resp.hca_core_clock_offset);\n\t\tif (!mlx4_get_internal_clock_params(dev->dev, &clock_params)) {\n\t\t\tresp.comp_mask |= MLX4_IB_QUERY_DEV_RESP_MASK_CORE_CLOCK_OFFSET;\n\t\t\tresp.hca_core_clock_offset = clock_params.offset % PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (uhw->outlen >= resp.response_length +\n\t    sizeof(resp.max_inl_recv_sz)) {\n\t\tresp.response_length += sizeof(resp.max_inl_recv_sz);\n\t\tresp.max_inl_recv_sz  = dev->dev->caps.max_rq_sg *\n\t\t\tsizeof(struct mlx4_wqe_data_seg);\n\t}\n\n\tif (offsetofend(typeof(resp), rss_caps) <= uhw->outlen) {\n\t\tif (props->rss_caps.supported_qpts) {\n\t\t\tresp.rss_caps.rx_hash_function =\n\t\t\t\tMLX4_IB_RX_HASH_FUNC_TOEPLITZ;\n\n\t\t\tresp.rss_caps.rx_hash_fields_mask =\n\t\t\t\tMLX4_IB_RX_HASH_SRC_IPV4 |\n\t\t\t\tMLX4_IB_RX_HASH_DST_IPV4 |\n\t\t\t\tMLX4_IB_RX_HASH_SRC_IPV6 |\n\t\t\t\tMLX4_IB_RX_HASH_DST_IPV6 |\n\t\t\t\tMLX4_IB_RX_HASH_SRC_PORT_TCP |\n\t\t\t\tMLX4_IB_RX_HASH_DST_PORT_TCP |\n\t\t\t\tMLX4_IB_RX_HASH_SRC_PORT_UDP |\n\t\t\t\tMLX4_IB_RX_HASH_DST_PORT_UDP;\n\n\t\t\tif (dev->dev->caps.tunnel_offload_mode ==\n\t\t\t    MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)\n\t\t\t\tresp.rss_caps.rx_hash_fields_mask |=\n\t\t\t\t\tMLX4_IB_RX_HASH_INNER;\n\t\t}\n\t\tresp.response_length = offsetof(typeof(resp), rss_caps) +\n\t\t\t\t       sizeof(resp.rss_caps);\n\t}\n\n\tif (offsetofend(typeof(resp), tso_caps) <= uhw->outlen) {\n\t\tif (dev->dev->caps.max_gso_sz &&\n\t\t    ((mlx4_ib_port_link_layer(ibdev, 1) ==\n\t\t    IB_LINK_LAYER_ETHERNET) ||\n\t\t    (mlx4_ib_port_link_layer(ibdev, 2) ==\n\t\t    IB_LINK_LAYER_ETHERNET))) {\n\t\t\tresp.tso_caps.max_tso = dev->dev->caps.max_gso_sz;\n\t\t\tresp.tso_caps.supported_qpts |=\n\t\t\t\t1 << IB_QPT_RAW_PACKET;\n\t\t}\n\t\tresp.response_length = offsetof(typeof(resp), tso_caps) +\n\t\t\t\t       sizeof(resp.tso_caps);\n\t}\n\n\tif (uhw->outlen) {\n\t\terr = ib_copy_to_udata(uhw, &resp, resp.response_length);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\nout:\n\tkfree(in_mad);\n\tkfree(out_mad);\n\n\treturn err;\n}\n\nstatic enum rdma_link_layer\nmlx4_ib_port_link_layer(struct ib_device *device, u32 port_num)\n{\n\tstruct mlx4_dev *dev = to_mdev(device)->dev;\n\n\treturn dev->caps.port_mask[port_num] == MLX4_PORT_TYPE_IB ?\n\t\tIB_LINK_LAYER_INFINIBAND : IB_LINK_LAYER_ETHERNET;\n}\n\nstatic int ib_link_query_port(struct ib_device *ibdev, u32 port,\n\t\t\t      struct ib_port_attr *props, int netw_view)\n{\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint ext_active_speed;\n\tint mad_ifc_flags = MLX4_MAD_IFC_IGNORE_KEYS;\n\tint err = -ENOMEM;\n\n\tin_mad  = kzalloc(sizeof *in_mad, GFP_KERNEL);\n\tout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id  = IB_SMP_ATTR_PORT_INFO;\n\tin_mad->attr_mod = cpu_to_be32(port);\n\n\tif (mlx4_is_mfunc(to_mdev(ibdev)->dev) && netw_view)\n\t\tmad_ifc_flags |= MLX4_MAD_IFC_NET_VIEW;\n\n\terr = mlx4_MAD_IFC(to_mdev(ibdev), mad_ifc_flags, port, NULL, NULL,\n\t\t\t\tin_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\n\tprops->lid\t\t= be16_to_cpup((__be16 *) (out_mad->data + 16));\n\tprops->lmc\t\t= out_mad->data[34] & 0x7;\n\tprops->sm_lid\t\t= be16_to_cpup((__be16 *) (out_mad->data + 18));\n\tprops->sm_sl\t\t= out_mad->data[36] & 0xf;\n\tprops->state\t\t= out_mad->data[32] & 0xf;\n\tprops->phys_state\t= out_mad->data[33] >> 4;\n\tprops->port_cap_flags\t= be32_to_cpup((__be32 *) (out_mad->data + 20));\n\tif (netw_view)\n\t\tprops->gid_tbl_len = out_mad->data[50];\n\telse\n\t\tprops->gid_tbl_len = to_mdev(ibdev)->dev->caps.gid_table_len[port];\n\tprops->max_msg_sz\t= to_mdev(ibdev)->dev->caps.max_msg_sz;\n\tprops->pkey_tbl_len\t= to_mdev(ibdev)->dev->caps.pkey_table_len[port];\n\tprops->bad_pkey_cntr\t= be16_to_cpup((__be16 *) (out_mad->data + 46));\n\tprops->qkey_viol_cntr\t= be16_to_cpup((__be16 *) (out_mad->data + 48));\n\tprops->active_width\t= out_mad->data[31] & 0xf;\n\tprops->active_speed\t= out_mad->data[35] >> 4;\n\tprops->max_mtu\t\t= out_mad->data[41] & 0xf;\n\tprops->active_mtu\t= out_mad->data[36] >> 4;\n\tprops->subnet_timeout\t= out_mad->data[51] & 0x1f;\n\tprops->max_vl_num\t= out_mad->data[37] >> 4;\n\tprops->init_type_reply\t= out_mad->data[41] >> 4;\n\n\t \n\tif (props->port_cap_flags & IB_PORT_EXTENDED_SPEEDS_SUP) {\n\t\text_active_speed = out_mad->data[62] >> 4;\n\n\t\tswitch (ext_active_speed) {\n\t\tcase 1:\n\t\t\tprops->active_speed = IB_SPEED_FDR;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tprops->active_speed = IB_SPEED_EDR;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (props->active_speed == IB_SPEED_QDR) {\n\t\tib_init_query_mad(in_mad);\n\t\tin_mad->attr_id = MLX4_ATTR_EXTENDED_PORT_INFO;\n\t\tin_mad->attr_mod = cpu_to_be32(port);\n\n\t\terr = mlx4_MAD_IFC(to_mdev(ibdev), mad_ifc_flags, port,\n\t\t\t\t   NULL, NULL, in_mad, out_mad);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (out_mad->data[15] & 0x1)\n\t\t\tprops->active_speed = IB_SPEED_FDR10;\n\t}\n\n\t \n\tif (props->state == IB_PORT_DOWN)\n\t\t props->active_speed = IB_SPEED_SDR;\n\nout:\n\tkfree(in_mad);\n\tkfree(out_mad);\n\treturn err;\n}\n\nstatic u8 state_to_phys_state(enum ib_port_state state)\n{\n\treturn state == IB_PORT_ACTIVE ?\n\t\tIB_PORT_PHYS_STATE_LINK_UP : IB_PORT_PHYS_STATE_DISABLED;\n}\n\nstatic int eth_link_query_port(struct ib_device *ibdev, u32 port,\n\t\t\t       struct ib_port_attr *props)\n{\n\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibdev);\n\tstruct mlx4_ib_iboe *iboe = &mdev->iboe;\n\tstruct net_device *ndev;\n\tenum ib_mtu tmp;\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tint err = 0;\n\tint is_bonded = mlx4_is_bonded(mdev->dev);\n\n\tmailbox = mlx4_alloc_cmd_mailbox(mdev->dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\terr = mlx4_cmd_box(mdev->dev, 0, mailbox->dma, port, 0,\n\t\t\t   MLX4_CMD_QUERY_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t\t   MLX4_CMD_WRAPPED);\n\tif (err)\n\t\tgoto out;\n\n\tprops->active_width\t=  (((u8 *)mailbox->buf)[5] == 0x40) ||\n\t\t\t\t   (((u8 *)mailbox->buf)[5] == 0x20  ) ?\n\t\t\t\t\t   IB_WIDTH_4X : IB_WIDTH_1X;\n\tprops->active_speed\t=  (((u8 *)mailbox->buf)[5] == 0x20  ) ?\n\t\t\t\t\t   IB_SPEED_FDR : IB_SPEED_QDR;\n\tprops->port_cap_flags\t= IB_PORT_CM_SUP;\n\tprops->ip_gids = true;\n\tprops->gid_tbl_len\t= mdev->dev->caps.gid_table_len[port];\n\tprops->max_msg_sz\t= mdev->dev->caps.max_msg_sz;\n\tif (mdev->dev->caps.pkey_table_len[port])\n\t\tprops->pkey_tbl_len = 1;\n\tprops->max_mtu\t\t= IB_MTU_4096;\n\tprops->max_vl_num\t= 2;\n\tprops->state\t\t= IB_PORT_DOWN;\n\tprops->phys_state\t= state_to_phys_state(props->state);\n\tprops->active_mtu\t= IB_MTU_256;\n\tspin_lock_bh(&iboe->lock);\n\tndev = iboe->netdevs[port - 1];\n\tif (ndev && is_bonded) {\n\t\trcu_read_lock();  \n\t\tndev = netdev_master_upper_dev_get_rcu(ndev);\n\t\trcu_read_unlock();\n\t}\n\tif (!ndev)\n\t\tgoto out_unlock;\n\n\ttmp = iboe_get_mtu(ndev->mtu);\n\tprops->active_mtu = tmp ? min(props->max_mtu, tmp) : IB_MTU_256;\n\n\tprops->state\t\t= (netif_running(ndev) && netif_carrier_ok(ndev)) ?\n\t\t\t\t\tIB_PORT_ACTIVE : IB_PORT_DOWN;\n\tprops->phys_state\t= state_to_phys_state(props->state);\nout_unlock:\n\tspin_unlock_bh(&iboe->lock);\nout:\n\tmlx4_free_cmd_mailbox(mdev->dev, mailbox);\n\treturn err;\n}\n\nint __mlx4_ib_query_port(struct ib_device *ibdev, u32 port,\n\t\t\t struct ib_port_attr *props, int netw_view)\n{\n\tint err;\n\n\t \n\n\terr = mlx4_ib_port_link_layer(ibdev, port) == IB_LINK_LAYER_INFINIBAND ?\n\t\tib_link_query_port(ibdev, port, props, netw_view) :\n\t\t\t\teth_link_query_port(ibdev, port, props);\n\n\treturn err;\n}\n\nstatic int mlx4_ib_query_port(struct ib_device *ibdev, u32 port,\n\t\t\t      struct ib_port_attr *props)\n{\n\t \n\treturn __mlx4_ib_query_port(ibdev, port, props, 0);\n}\n\nint __mlx4_ib_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t\tunion ib_gid *gid, int netw_view)\n{\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint err = -ENOMEM;\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tint clear = 0;\n\tint mad_ifc_flags = MLX4_MAD_IFC_IGNORE_KEYS;\n\n\tin_mad  = kzalloc(sizeof *in_mad, GFP_KERNEL);\n\tout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id  = IB_SMP_ATTR_PORT_INFO;\n\tin_mad->attr_mod = cpu_to_be32(port);\n\n\tif (mlx4_is_mfunc(dev->dev) && netw_view)\n\t\tmad_ifc_flags |= MLX4_MAD_IFC_NET_VIEW;\n\n\terr = mlx4_MAD_IFC(dev, mad_ifc_flags, port, NULL, NULL, in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tmemcpy(gid->raw, out_mad->data + 8, 8);\n\n\tif (mlx4_is_mfunc(dev->dev) && !netw_view) {\n\t\tif (index) {\n\t\t\t \n\t\t\terr = 0;\n\t\t\tclear = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id  = IB_SMP_ATTR_GUID_INFO;\n\tin_mad->attr_mod = cpu_to_be32(index / 8);\n\n\terr = mlx4_MAD_IFC(dev, mad_ifc_flags, port,\n\t\t\t   NULL, NULL, in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tmemcpy(gid->raw + 8, out_mad->data + (index % 8) * 8, 8);\n\nout:\n\tif (clear)\n\t\tmemset(gid->raw + 8, 0, 8);\n\tkfree(in_mad);\n\tkfree(out_mad);\n\treturn err;\n}\n\nstatic int mlx4_ib_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t\t     union ib_gid *gid)\n{\n\tif (rdma_protocol_ib(ibdev, port))\n\t\treturn __mlx4_ib_query_gid(ibdev, port, index, gid, 0);\n\treturn 0;\n}\n\nstatic int mlx4_ib_query_sl2vl(struct ib_device *ibdev, u32 port,\n\t\t\t       u64 *sl2vl_tbl)\n{\n\tunion sl2vl_tbl_to_u64 sl2vl64;\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint mad_ifc_flags = MLX4_MAD_IFC_IGNORE_KEYS;\n\tint err = -ENOMEM;\n\tint jj;\n\n\tif (mlx4_is_slave(to_mdev(ibdev)->dev)) {\n\t\t*sl2vl_tbl = 0;\n\t\treturn 0;\n\t}\n\n\tin_mad  = kzalloc(sizeof(*in_mad), GFP_KERNEL);\n\tout_mad = kmalloc(sizeof(*out_mad), GFP_KERNEL);\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id  = IB_SMP_ATTR_SL_TO_VL_TABLE;\n\tin_mad->attr_mod = 0;\n\n\tif (mlx4_is_mfunc(to_mdev(ibdev)->dev))\n\t\tmad_ifc_flags |= MLX4_MAD_IFC_NET_VIEW;\n\n\terr = mlx4_MAD_IFC(to_mdev(ibdev), mad_ifc_flags, port, NULL, NULL,\n\t\t\t   in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tfor (jj = 0; jj < 8; jj++)\n\t\tsl2vl64.sl8[jj] = ((struct ib_smp *)out_mad)->data[jj];\n\t*sl2vl_tbl = sl2vl64.sl64;\n\nout:\n\tkfree(in_mad);\n\tkfree(out_mad);\n\treturn err;\n}\n\nstatic void mlx4_init_sl2vl_tbl(struct mlx4_ib_dev *mdev)\n{\n\tu64 sl2vl;\n\tint i;\n\tint err;\n\n\tfor (i = 1; i <= mdev->dev->caps.num_ports; i++) {\n\t\tif (mdev->dev->caps.port_type[i] == MLX4_PORT_TYPE_ETH)\n\t\t\tcontinue;\n\t\terr = mlx4_ib_query_sl2vl(&mdev->ib_dev, i, &sl2vl);\n\t\tif (err) {\n\t\t\tpr_err(\"Unable to get default sl to vl mapping for port %d.  Using all zeroes (%d)\\n\",\n\t\t\t       i, err);\n\t\t\tsl2vl = 0;\n\t\t}\n\t\tatomic64_set(&mdev->sl2vl[i - 1], sl2vl);\n\t}\n}\n\nint __mlx4_ib_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t\t u16 *pkey, int netw_view)\n{\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint mad_ifc_flags = MLX4_MAD_IFC_IGNORE_KEYS;\n\tint err = -ENOMEM;\n\n\tin_mad  = kzalloc(sizeof *in_mad, GFP_KERNEL);\n\tout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id  = IB_SMP_ATTR_PKEY_TABLE;\n\tin_mad->attr_mod = cpu_to_be32(index / 32);\n\n\tif (mlx4_is_mfunc(to_mdev(ibdev)->dev) && netw_view)\n\t\tmad_ifc_flags |= MLX4_MAD_IFC_NET_VIEW;\n\n\terr = mlx4_MAD_IFC(to_mdev(ibdev), mad_ifc_flags, port, NULL, NULL,\n\t\t\t   in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\t*pkey = be16_to_cpu(((__be16 *) out_mad->data)[index % 32]);\n\nout:\n\tkfree(in_mad);\n\tkfree(out_mad);\n\treturn err;\n}\n\nstatic int mlx4_ib_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t\t      u16 *pkey)\n{\n\treturn __mlx4_ib_query_pkey(ibdev, port, index, pkey, 0);\n}\n\nstatic int mlx4_ib_modify_device(struct ib_device *ibdev, int mask,\n\t\t\t\t struct ib_device_modify *props)\n{\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tunsigned long flags;\n\n\tif (mask & ~IB_DEVICE_MODIFY_NODE_DESC)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!(mask & IB_DEVICE_MODIFY_NODE_DESC))\n\t\treturn 0;\n\n\tif (mlx4_is_slave(to_mdev(ibdev)->dev))\n\t\treturn -EOPNOTSUPP;\n\n\tspin_lock_irqsave(&to_mdev(ibdev)->sm_lock, flags);\n\tmemcpy(ibdev->node_desc, props->node_desc, IB_DEVICE_NODE_DESC_MAX);\n\tspin_unlock_irqrestore(&to_mdev(ibdev)->sm_lock, flags);\n\n\t \n\tmailbox = mlx4_alloc_cmd_mailbox(to_mdev(ibdev)->dev);\n\tif (IS_ERR(mailbox))\n\t\treturn 0;\n\n\tmemcpy(mailbox->buf, props->node_desc, IB_DEVICE_NODE_DESC_MAX);\n\tmlx4_cmd(to_mdev(ibdev)->dev, mailbox->dma, 1, 0,\n\t\t MLX4_CMD_SET_NODE, MLX4_CMD_TIME_CLASS_A, MLX4_CMD_NATIVE);\n\n\tmlx4_free_cmd_mailbox(to_mdev(ibdev)->dev, mailbox);\n\n\treturn 0;\n}\n\nstatic int mlx4_ib_SET_PORT(struct mlx4_ib_dev *dev, u32 port,\n\t\t\t    int reset_qkey_viols, u32 cap_mask)\n{\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tint err;\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev->dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tif (dev->dev->flags & MLX4_FLAG_OLD_PORT_CMDS) {\n\t\t*(u8 *) mailbox->buf\t     = !!reset_qkey_viols << 6;\n\t\t((__be32 *) mailbox->buf)[2] = cpu_to_be32(cap_mask);\n\t} else {\n\t\t((u8 *) mailbox->buf)[3]     = !!reset_qkey_viols;\n\t\t((__be32 *) mailbox->buf)[1] = cpu_to_be32(cap_mask);\n\t}\n\n\terr = mlx4_cmd(dev->dev, mailbox->dma, port, MLX4_SET_PORT_IB_OPCODE,\n\t\t       MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B,\n\t\t       MLX4_CMD_WRAPPED);\n\n\tmlx4_free_cmd_mailbox(dev->dev, mailbox);\n\treturn err;\n}\n\nstatic int mlx4_ib_modify_port(struct ib_device *ibdev, u32 port, int mask,\n\t\t\t       struct ib_port_modify *props)\n{\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibdev);\n\tu8 is_eth = mdev->dev->caps.port_type[port] == MLX4_PORT_TYPE_ETH;\n\tstruct ib_port_attr attr;\n\tu32 cap_mask;\n\tint err;\n\n\t \n\tif (is_eth)\n\t\treturn 0;\n\n\tmutex_lock(&mdev->cap_mask_mutex);\n\n\terr = ib_query_port(ibdev, port, &attr);\n\tif (err)\n\t\tgoto out;\n\n\tcap_mask = (attr.port_cap_flags | props->set_port_cap_mask) &\n\t\t~props->clr_port_cap_mask;\n\n\terr = mlx4_ib_SET_PORT(mdev, port,\n\t\t\t       !!(mask & IB_PORT_RESET_QKEY_CNTR),\n\t\t\t       cap_mask);\n\nout:\n\tmutex_unlock(&to_mdev(ibdev)->cap_mask_mutex);\n\treturn err;\n}\n\nstatic int mlx4_ib_alloc_ucontext(struct ib_ucontext *uctx,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx4_ib_ucontext *context = to_mucontext(uctx);\n\tstruct mlx4_ib_alloc_ucontext_resp_v3 resp_v3;\n\tstruct mlx4_ib_alloc_ucontext_resp resp;\n\tint err;\n\n\tif (!dev->ib_active)\n\t\treturn -EAGAIN;\n\n\tif (ibdev->ops.uverbs_abi_ver ==\n\t    MLX4_IB_UVERBS_NO_DEV_CAPS_ABI_VERSION) {\n\t\tresp_v3.qp_tab_size      = dev->dev->caps.num_qps;\n\t\tresp_v3.bf_reg_size      = dev->dev->caps.bf_reg_size;\n\t\tresp_v3.bf_regs_per_page = dev->dev->caps.bf_regs_per_page;\n\t} else {\n\t\tresp.dev_caps\t      = dev->dev->caps.userspace_caps;\n\t\tresp.qp_tab_size      = dev->dev->caps.num_qps;\n\t\tresp.bf_reg_size      = dev->dev->caps.bf_reg_size;\n\t\tresp.bf_regs_per_page = dev->dev->caps.bf_regs_per_page;\n\t\tresp.cqe_size\t      = dev->dev->caps.cqe_size;\n\t}\n\n\terr = mlx4_uar_alloc(to_mdev(ibdev)->dev, &context->uar);\n\tif (err)\n\t\treturn err;\n\n\tINIT_LIST_HEAD(&context->db_page_list);\n\tmutex_init(&context->db_page_mutex);\n\n\tINIT_LIST_HEAD(&context->wqn_ranges_list);\n\tmutex_init(&context->wqn_ranges_mutex);\n\n\tif (ibdev->ops.uverbs_abi_ver == MLX4_IB_UVERBS_NO_DEV_CAPS_ABI_VERSION)\n\t\terr = ib_copy_to_udata(udata, &resp_v3, sizeof(resp_v3));\n\telse\n\t\terr = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\n\tif (err) {\n\t\tmlx4_uar_free(to_mdev(ibdev)->dev, &context->uar);\n\t\treturn -EFAULT;\n\t}\n\n\treturn err;\n}\n\nstatic void mlx4_ib_dealloc_ucontext(struct ib_ucontext *ibcontext)\n{\n\tstruct mlx4_ib_ucontext *context = to_mucontext(ibcontext);\n\n\tmlx4_uar_free(to_mdev(ibcontext->device)->dev, &context->uar);\n}\n\nstatic void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)\n{\n}\n\nstatic int mlx4_ib_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(context->device);\n\n\tswitch (vma->vm_pgoff) {\n\tcase 0:\n\t\treturn rdma_user_mmap_io(context, vma,\n\t\t\t\t\t to_mucontext(context)->uar.pfn,\n\t\t\t\t\t PAGE_SIZE,\n\t\t\t\t\t pgprot_noncached(vma->vm_page_prot),\n\t\t\t\t\t NULL);\n\n\tcase 1:\n\t\tif (dev->dev->caps.bf_reg_size == 0)\n\t\t\treturn -EINVAL;\n\t\treturn rdma_user_mmap_io(\n\t\t\tcontext, vma,\n\t\t\tto_mucontext(context)->uar.pfn +\n\t\t\t\tdev->dev->caps.num_uars,\n\t\t\tPAGE_SIZE, pgprot_writecombine(vma->vm_page_prot),\n\t\t\tNULL);\n\n\tcase 3: {\n\t\tstruct mlx4_clock_params params;\n\t\tint ret;\n\n\t\tret = mlx4_get_internal_clock_params(dev->dev, &params);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\treturn rdma_user_mmap_io(\n\t\t\tcontext, vma,\n\t\t\t(pci_resource_start(dev->dev->persist->pdev,\n\t\t\t\t\t    params.bar) +\n\t\t\t params.offset) >>\n\t\t\t\tPAGE_SHIFT,\n\t\t\tPAGE_SIZE, pgprot_noncached(vma->vm_page_prot),\n\t\t\tNULL);\n\t}\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int mlx4_ib_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_pd *pd = to_mpd(ibpd);\n\tstruct ib_device *ibdev = ibpd->device;\n\tint err;\n\n\terr = mlx4_pd_alloc(to_mdev(ibdev)->dev, &pd->pdn);\n\tif (err)\n\t\treturn err;\n\n\tif (udata && ib_copy_to_udata(udata, &pd->pdn, sizeof(__u32))) {\n\t\tmlx4_pd_free(to_mdev(ibdev)->dev, pd->pdn);\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nstatic int mlx4_ib_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata)\n{\n\tmlx4_pd_free(to_mdev(pd->device)->dev, to_mpd(pd)->pdn);\n\treturn 0;\n}\n\nstatic int mlx4_ib_alloc_xrcd(struct ib_xrcd *ibxrcd, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibxrcd->device);\n\tstruct mlx4_ib_xrcd *xrcd = to_mxrcd(ibxrcd);\n\tstruct ib_cq_init_attr cq_attr = {};\n\tint err;\n\n\tif (!(dev->dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC))\n\t\treturn -EOPNOTSUPP;\n\n\terr = mlx4_xrcd_alloc(dev->dev, &xrcd->xrcdn);\n\tif (err)\n\t\treturn err;\n\n\txrcd->pd = ib_alloc_pd(ibxrcd->device, 0);\n\tif (IS_ERR(xrcd->pd)) {\n\t\terr = PTR_ERR(xrcd->pd);\n\t\tgoto err2;\n\t}\n\n\tcq_attr.cqe = 1;\n\txrcd->cq = ib_create_cq(ibxrcd->device, NULL, NULL, xrcd, &cq_attr);\n\tif (IS_ERR(xrcd->cq)) {\n\t\terr = PTR_ERR(xrcd->cq);\n\t\tgoto err3;\n\t}\n\n\treturn 0;\n\nerr3:\n\tib_dealloc_pd(xrcd->pd);\nerr2:\n\tmlx4_xrcd_free(dev->dev, xrcd->xrcdn);\n\treturn err;\n}\n\nstatic int mlx4_ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata)\n{\n\tib_destroy_cq(to_mxrcd(xrcd)->cq);\n\tib_dealloc_pd(to_mxrcd(xrcd)->pd);\n\tmlx4_xrcd_free(to_mdev(xrcd->device)->dev, to_mxrcd(xrcd)->xrcdn);\n\treturn 0;\n}\n\nstatic int add_gid_entry(struct ib_qp *ibqp, union ib_gid *gid)\n{\n\tstruct mlx4_ib_qp *mqp = to_mqp(ibqp);\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibqp->device);\n\tstruct mlx4_ib_gid_entry *ge;\n\n\tge = kzalloc(sizeof *ge, GFP_KERNEL);\n\tif (!ge)\n\t\treturn -ENOMEM;\n\n\tge->gid = *gid;\n\tif (mlx4_ib_add_mc(mdev, mqp, gid)) {\n\t\tge->port = mqp->port;\n\t\tge->added = 1;\n\t}\n\n\tmutex_lock(&mqp->mutex);\n\tlist_add_tail(&ge->list, &mqp->gid_list);\n\tmutex_unlock(&mqp->mutex);\n\n\treturn 0;\n}\n\nstatic void mlx4_ib_delete_counters_table(struct mlx4_ib_dev *ibdev,\n\t\t\t\t\t  struct mlx4_ib_counters *ctr_table)\n{\n\tstruct counter_index *counter, *tmp_count;\n\n\tmutex_lock(&ctr_table->mutex);\n\tlist_for_each_entry_safe(counter, tmp_count, &ctr_table->counters_list,\n\t\t\t\t list) {\n\t\tif (counter->allocated)\n\t\t\tmlx4_counter_free(ibdev->dev, counter->index);\n\t\tlist_del(&counter->list);\n\t\tkfree(counter);\n\t}\n\tmutex_unlock(&ctr_table->mutex);\n}\n\nint mlx4_ib_add_mc(struct mlx4_ib_dev *mdev, struct mlx4_ib_qp *mqp,\n\t\t   union ib_gid *gid)\n{\n\tstruct net_device *ndev;\n\tint ret = 0;\n\n\tif (!mqp->port)\n\t\treturn 0;\n\n\tspin_lock_bh(&mdev->iboe.lock);\n\tndev = mdev->iboe.netdevs[mqp->port - 1];\n\tdev_hold(ndev);\n\tspin_unlock_bh(&mdev->iboe.lock);\n\n\tif (ndev) {\n\t\tret = 1;\n\t\tdev_put(ndev);\n\t}\n\n\treturn ret;\n}\n\nstruct mlx4_ib_steering {\n\tstruct list_head list;\n\tstruct mlx4_flow_reg_id reg_id;\n\tunion ib_gid gid;\n};\n\n#define LAST_ETH_FIELD vlan_tag\n#define LAST_IB_FIELD sl\n#define LAST_IPV4_FIELD dst_ip\n#define LAST_TCP_UDP_FIELD src_port\n\n \n#define FIELDS_NOT_SUPPORTED(filter, field)\\\n\tmemchr_inv((void *)&filter.field  +\\\n\t\t   sizeof(filter.field), 0,\\\n\t\t   sizeof(filter) -\\\n\t\t   offsetof(typeof(filter), field) -\\\n\t\t   sizeof(filter.field))\n\nstatic int parse_flow_attr(struct mlx4_dev *dev,\n\t\t\t   u32 qp_num,\n\t\t\t   union ib_flow_spec *ib_spec,\n\t\t\t   struct _rule_hw *mlx4_spec)\n{\n\tenum mlx4_net_trans_rule_id type;\n\n\tswitch (ib_spec->type) {\n\tcase IB_FLOW_SPEC_ETH:\n\t\tif (FIELDS_NOT_SUPPORTED(ib_spec->eth.mask, LAST_ETH_FIELD))\n\t\t\treturn -ENOTSUPP;\n\n\t\ttype = MLX4_NET_TRANS_RULE_ID_ETH;\n\t\tmemcpy(mlx4_spec->eth.dst_mac, ib_spec->eth.val.dst_mac,\n\t\t       ETH_ALEN);\n\t\tmemcpy(mlx4_spec->eth.dst_mac_msk, ib_spec->eth.mask.dst_mac,\n\t\t       ETH_ALEN);\n\t\tmlx4_spec->eth.vlan_tag = ib_spec->eth.val.vlan_tag;\n\t\tmlx4_spec->eth.vlan_tag_msk = ib_spec->eth.mask.vlan_tag;\n\t\tbreak;\n\tcase IB_FLOW_SPEC_IB:\n\t\tif (FIELDS_NOT_SUPPORTED(ib_spec->ib.mask, LAST_IB_FIELD))\n\t\t\treturn -ENOTSUPP;\n\n\t\ttype = MLX4_NET_TRANS_RULE_ID_IB;\n\t\tmlx4_spec->ib.l3_qpn =\n\t\t\tcpu_to_be32(qp_num);\n\t\tmlx4_spec->ib.qpn_mask =\n\t\t\tcpu_to_be32(MLX4_IB_FLOW_QPN_MASK);\n\t\tbreak;\n\n\n\tcase IB_FLOW_SPEC_IPV4:\n\t\tif (FIELDS_NOT_SUPPORTED(ib_spec->ipv4.mask, LAST_IPV4_FIELD))\n\t\t\treturn -ENOTSUPP;\n\n\t\ttype = MLX4_NET_TRANS_RULE_ID_IPV4;\n\t\tmlx4_spec->ipv4.src_ip = ib_spec->ipv4.val.src_ip;\n\t\tmlx4_spec->ipv4.src_ip_msk = ib_spec->ipv4.mask.src_ip;\n\t\tmlx4_spec->ipv4.dst_ip = ib_spec->ipv4.val.dst_ip;\n\t\tmlx4_spec->ipv4.dst_ip_msk = ib_spec->ipv4.mask.dst_ip;\n\t\tbreak;\n\n\tcase IB_FLOW_SPEC_TCP:\n\tcase IB_FLOW_SPEC_UDP:\n\t\tif (FIELDS_NOT_SUPPORTED(ib_spec->tcp_udp.mask, LAST_TCP_UDP_FIELD))\n\t\t\treturn -ENOTSUPP;\n\n\t\ttype = ib_spec->type == IB_FLOW_SPEC_TCP ?\n\t\t\t\t\tMLX4_NET_TRANS_RULE_ID_TCP :\n\t\t\t\t\tMLX4_NET_TRANS_RULE_ID_UDP;\n\t\tmlx4_spec->tcp_udp.dst_port = ib_spec->tcp_udp.val.dst_port;\n\t\tmlx4_spec->tcp_udp.dst_port_msk = ib_spec->tcp_udp.mask.dst_port;\n\t\tmlx4_spec->tcp_udp.src_port = ib_spec->tcp_udp.val.src_port;\n\t\tmlx4_spec->tcp_udp.src_port_msk = ib_spec->tcp_udp.mask.src_port;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (mlx4_map_sw_to_hw_steering_id(dev, type) < 0 ||\n\t    mlx4_hw_rule_sz(dev, type) < 0)\n\t\treturn -EINVAL;\n\tmlx4_spec->id = cpu_to_be16(mlx4_map_sw_to_hw_steering_id(dev, type));\n\tmlx4_spec->size = mlx4_hw_rule_sz(dev, type) >> 2;\n\treturn mlx4_hw_rule_sz(dev, type);\n}\n\nstruct default_rules {\n\t__u32 mandatory_fields[IB_FLOW_SPEC_SUPPORT_LAYERS];\n\t__u32 mandatory_not_fields[IB_FLOW_SPEC_SUPPORT_LAYERS];\n\t__u32 rules_create_list[IB_FLOW_SPEC_SUPPORT_LAYERS];\n\t__u8  link_layer;\n};\nstatic const struct default_rules default_table[] = {\n\t{\n\t\t.mandatory_fields = {IB_FLOW_SPEC_IPV4},\n\t\t.mandatory_not_fields = {IB_FLOW_SPEC_ETH},\n\t\t.rules_create_list = {IB_FLOW_SPEC_IB},\n\t\t.link_layer = IB_LINK_LAYER_INFINIBAND\n\t}\n};\n\nstatic int __mlx4_ib_default_rules_match(struct ib_qp *qp,\n\t\t\t\t\t struct ib_flow_attr *flow_attr)\n{\n\tint i, j, k;\n\tvoid *ib_flow;\n\tconst struct default_rules *pdefault_rules = default_table;\n\tu8 link_layer = rdma_port_get_link_layer(qp->device, flow_attr->port);\n\n\tfor (i = 0; i < ARRAY_SIZE(default_table); i++, pdefault_rules++) {\n\t\t__u32 field_types[IB_FLOW_SPEC_SUPPORT_LAYERS];\n\t\tmemset(&field_types, 0, sizeof(field_types));\n\n\t\tif (link_layer != pdefault_rules->link_layer)\n\t\t\tcontinue;\n\n\t\tib_flow = flow_attr + 1;\n\t\t \n\t\tfor (j = 0, k = 0; k < IB_FLOW_SPEC_SUPPORT_LAYERS &&\n\t\t     j < flow_attr->num_of_specs; k++) {\n\t\t\tunion ib_flow_spec *current_flow =\n\t\t\t\t(union ib_flow_spec *)ib_flow;\n\n\t\t\t \n\t\t\tif (((current_flow->type & IB_FLOW_SPEC_LAYER_MASK) ==\n\t\t\t     (pdefault_rules->mandatory_fields[k] &\n\t\t\t      IB_FLOW_SPEC_LAYER_MASK)) &&\n\t\t\t    (current_flow->type !=\n\t\t\t     pdefault_rules->mandatory_fields[k]))\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tif (current_flow->type ==\n\t\t\t    pdefault_rules->mandatory_fields[k]) {\n\t\t\t\tj++;\n\t\t\t\tib_flow +=\n\t\t\t\t\t((union ib_flow_spec *)ib_flow)->size;\n\t\t\t}\n\t\t}\n\n\t\tib_flow = flow_attr + 1;\n\t\tfor (j = 0; j < flow_attr->num_of_specs;\n\t\t     j++, ib_flow += ((union ib_flow_spec *)ib_flow)->size)\n\t\t\tfor (k = 0; k < IB_FLOW_SPEC_SUPPORT_LAYERS; k++)\n\t\t\t\t \n\t\t\t\tif (((union ib_flow_spec *)ib_flow)->type ==\n\t\t\t\t    pdefault_rules->mandatory_not_fields[k])\n\t\t\t\t\tgoto out;\n\n\t\treturn i;\n\t}\nout:\n\treturn -1;\n}\n\nstatic int __mlx4_ib_create_default_rules(\n\t\tstruct mlx4_ib_dev *mdev,\n\t\tstruct ib_qp *qp,\n\t\tconst struct default_rules *pdefault_rules,\n\t\tstruct _rule_hw *mlx4_spec) {\n\tint size = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pdefault_rules->rules_create_list); i++) {\n\t\tunion ib_flow_spec ib_spec = {};\n\t\tint ret;\n\n\t\tswitch (pdefault_rules->rules_create_list[i]) {\n\t\tcase 0:\n\t\t\t \n\t\t\tcontinue;\n\t\tcase IB_FLOW_SPEC_IB:\n\t\t\tib_spec.type = IB_FLOW_SPEC_IB;\n\t\t\tib_spec.size = sizeof(struct ib_flow_spec_ib);\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t\tret = parse_flow_attr(mdev->dev, 0, &ib_spec,\n\t\t\t\t      mlx4_spec);\n\t\tif (ret < 0) {\n\t\t\tpr_info(\"invalid parsing\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmlx4_spec = (void *)mlx4_spec + ret;\n\t\tsize += ret;\n\t}\n\treturn size;\n}\n\nstatic int __mlx4_ib_create_flow(struct ib_qp *qp, struct ib_flow_attr *flow_attr,\n\t\t\t  int domain,\n\t\t\t  enum mlx4_net_trans_promisc_mode flow_type,\n\t\t\t  u64 *reg_id)\n{\n\tint ret, i;\n\tint size = 0;\n\tvoid *ib_flow;\n\tstruct mlx4_ib_dev *mdev = to_mdev(qp->device);\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tstruct mlx4_net_trans_rule_hw_ctrl *ctrl;\n\tint default_flow;\n\n\tif (flow_attr->priority > MLX4_IB_FLOW_MAX_PRIO) {\n\t\tpr_err(\"Invalid priority value %d\\n\", flow_attr->priority);\n\t\treturn -EINVAL;\n\t}\n\n\tif (mlx4_map_sw_to_hw_steering_mode(mdev->dev, flow_type) < 0)\n\t\treturn -EINVAL;\n\n\tmailbox = mlx4_alloc_cmd_mailbox(mdev->dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\tctrl = mailbox->buf;\n\n\tctrl->prio = cpu_to_be16(domain | flow_attr->priority);\n\tctrl->type = mlx4_map_sw_to_hw_steering_mode(mdev->dev, flow_type);\n\tctrl->port = flow_attr->port;\n\tctrl->qpn = cpu_to_be32(qp->qp_num);\n\n\tib_flow = flow_attr + 1;\n\tsize += sizeof(struct mlx4_net_trans_rule_hw_ctrl);\n\t \n\tdefault_flow = __mlx4_ib_default_rules_match(qp, flow_attr);\n\tif (default_flow >= 0) {\n\t\tret = __mlx4_ib_create_default_rules(\n\t\t\t\tmdev, qp, default_table + default_flow,\n\t\t\t\tmailbox->buf + size);\n\t\tif (ret < 0) {\n\t\t\tmlx4_free_cmd_mailbox(mdev->dev, mailbox);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize += ret;\n\t}\n\tfor (i = 0; i < flow_attr->num_of_specs; i++) {\n\t\tret = parse_flow_attr(mdev->dev, qp->qp_num, ib_flow,\n\t\t\t\t      mailbox->buf + size);\n\t\tif (ret < 0) {\n\t\t\tmlx4_free_cmd_mailbox(mdev->dev, mailbox);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tib_flow += ((union ib_flow_spec *) ib_flow)->size;\n\t\tsize += ret;\n\t}\n\n\tif (mlx4_is_master(mdev->dev) && flow_type == MLX4_FS_REGULAR &&\n\t    flow_attr->num_of_specs == 1) {\n\t\tstruct _rule_hw *rule_header = (struct _rule_hw *)(ctrl + 1);\n\t\tenum ib_flow_spec_type header_spec =\n\t\t\t((union ib_flow_spec *)(flow_attr + 1))->type;\n\n\t\tif (header_spec == IB_FLOW_SPEC_ETH)\n\t\t\tmlx4_handle_eth_header_mcast_prio(ctrl, rule_header);\n\t}\n\n\tret = mlx4_cmd_imm(mdev->dev, mailbox->dma, reg_id, size >> 2, 0,\n\t\t\t   MLX4_QP_FLOW_STEERING_ATTACH, MLX4_CMD_TIME_CLASS_A,\n\t\t\t   MLX4_CMD_NATIVE);\n\tif (ret == -ENOMEM)\n\t\tpr_err(\"mcg table is full. Fail to register network rule.\\n\");\n\telse if (ret == -ENXIO)\n\t\tpr_err(\"Device managed flow steering is disabled. Fail to register network rule.\\n\");\n\telse if (ret)\n\t\tpr_err(\"Invalid argument. Fail to register network rule.\\n\");\n\n\tmlx4_free_cmd_mailbox(mdev->dev, mailbox);\n\treturn ret;\n}\n\nstatic int __mlx4_ib_destroy_flow(struct mlx4_dev *dev, u64 reg_id)\n{\n\tint err;\n\terr = mlx4_cmd(dev, reg_id, 0, 0,\n\t\t       MLX4_QP_FLOW_STEERING_DETACH, MLX4_CMD_TIME_CLASS_A,\n\t\t       MLX4_CMD_NATIVE);\n\tif (err)\n\t\tpr_err(\"Fail to detach network rule. registration id = 0x%llx\\n\",\n\t\t       reg_id);\n\treturn err;\n}\n\nstatic int mlx4_ib_tunnel_steer_add(struct ib_qp *qp, struct ib_flow_attr *flow_attr,\n\t\t\t\t    u64 *reg_id)\n{\n\tvoid *ib_flow;\n\tunion ib_flow_spec *ib_spec;\n\tstruct mlx4_dev\t*dev = to_mdev(qp->device)->dev;\n\tint err = 0;\n\n\tif (dev->caps.tunnel_offload_mode != MLX4_TUNNEL_OFFLOAD_MODE_VXLAN ||\n\t    dev->caps.dmfs_high_steer_mode == MLX4_STEERING_DMFS_A0_STATIC)\n\t\treturn 0;  \n\n\tib_flow = flow_attr + 1;\n\tib_spec = (union ib_flow_spec *)ib_flow;\n\n\tif (ib_spec->type !=  IB_FLOW_SPEC_ETH || flow_attr->num_of_specs != 1)\n\t\treturn 0;  \n\n\terr = mlx4_tunnel_steer_add(to_mdev(qp->device)->dev, ib_spec->eth.val.dst_mac,\n\t\t\t\t    flow_attr->port, qp->qp_num,\n\t\t\t\t    MLX4_DOMAIN_UVERBS | (flow_attr->priority & 0xff),\n\t\t\t\t    reg_id);\n\treturn err;\n}\n\nstatic int mlx4_ib_add_dont_trap_rule(struct mlx4_dev *dev,\n\t\t\t\t      struct ib_flow_attr *flow_attr,\n\t\t\t\t      enum mlx4_net_trans_promisc_mode *type)\n{\n\tint err = 0;\n\n\tif (!(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DMFS_UC_MC_SNIFFER) ||\n\t    (dev->caps.dmfs_high_steer_mode == MLX4_STEERING_DMFS_A0_STATIC) ||\n\t    (flow_attr->num_of_specs > 1) || (flow_attr->priority != 0)) {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (flow_attr->num_of_specs == 0) {\n\t\ttype[0] = MLX4_FS_MC_SNIFFER;\n\t\ttype[1] = MLX4_FS_UC_SNIFFER;\n\t} else {\n\t\tunion ib_flow_spec *ib_spec;\n\n\t\tib_spec = (union ib_flow_spec *)(flow_attr + 1);\n\t\tif (ib_spec->type !=  IB_FLOW_SPEC_ETH)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (is_zero_ether_addr(ib_spec->eth.mask.dst_mac)) {\n\t\t\ttype[0] = MLX4_FS_MC_SNIFFER;\n\t\t\ttype[1] = MLX4_FS_UC_SNIFFER;\n\t\t} else {\n\t\t\tu8 mac[ETH_ALEN] = {ib_spec->eth.mask.dst_mac[0] ^ 0x01,\n\t\t\t\t\t    ib_spec->eth.mask.dst_mac[1],\n\t\t\t\t\t    ib_spec->eth.mask.dst_mac[2],\n\t\t\t\t\t    ib_spec->eth.mask.dst_mac[3],\n\t\t\t\t\t    ib_spec->eth.mask.dst_mac[4],\n\t\t\t\t\t    ib_spec->eth.mask.dst_mac[5]};\n\n\t\t\t \n\t\t\tif (!is_zero_ether_addr(&mac[0]))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (is_multicast_ether_addr(ib_spec->eth.val.dst_mac))\n\t\t\t\ttype[0] = MLX4_FS_MC_SNIFFER;\n\t\t\telse\n\t\t\t\ttype[0] = MLX4_FS_UC_SNIFFER;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic struct ib_flow *mlx4_ib_create_flow(struct ib_qp *qp,\n\t\t\t\t\t   struct ib_flow_attr *flow_attr,\n\t\t\t\t\t   struct ib_udata *udata)\n{\n\tint err = 0, i = 0, j = 0;\n\tstruct mlx4_ib_flow *mflow;\n\tenum mlx4_net_trans_promisc_mode type[2];\n\tstruct mlx4_dev *dev = (to_mdev(qp->device))->dev;\n\tint is_bonded = mlx4_is_bonded(dev);\n\n\tif (flow_attr->flags & ~IB_FLOW_ATTR_FLAGS_DONT_TRAP)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif ((flow_attr->flags & IB_FLOW_ATTR_FLAGS_DONT_TRAP) &&\n\t    (flow_attr->type != IB_FLOW_ATTR_NORMAL))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif (udata &&\n\t    udata->inlen && !ib_is_udata_cleared(udata, 0, udata->inlen))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tmemset(type, 0, sizeof(type));\n\n\tmflow = kzalloc(sizeof(*mflow), GFP_KERNEL);\n\tif (!mflow) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tswitch (flow_attr->type) {\n\tcase IB_FLOW_ATTR_NORMAL:\n\t\t \n\t\tif (unlikely(flow_attr->flags & IB_FLOW_ATTR_FLAGS_DONT_TRAP)) {\n\t\t\terr = mlx4_ib_add_dont_trap_rule(dev,\n\t\t\t\t\t\t\t flow_attr,\n\t\t\t\t\t\t\t type);\n\t\t\tif (err)\n\t\t\t\tgoto err_free;\n\t\t} else {\n\t\t\ttype[0] = MLX4_FS_REGULAR;\n\t\t}\n\t\tbreak;\n\n\tcase IB_FLOW_ATTR_ALL_DEFAULT:\n\t\ttype[0] = MLX4_FS_ALL_DEFAULT;\n\t\tbreak;\n\n\tcase IB_FLOW_ATTR_MC_DEFAULT:\n\t\ttype[0] = MLX4_FS_MC_DEFAULT;\n\t\tbreak;\n\n\tcase IB_FLOW_ATTR_SNIFFER:\n\t\ttype[0] = MLX4_FS_MIRROR_RX_PORT;\n\t\ttype[1] = MLX4_FS_MIRROR_SX_PORT;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\twhile (i < ARRAY_SIZE(type) && type[i]) {\n\t\terr = __mlx4_ib_create_flow(qp, flow_attr, MLX4_DOMAIN_UVERBS,\n\t\t\t\t\t    type[i], &mflow->reg_id[i].id);\n\t\tif (err)\n\t\t\tgoto err_create_flow;\n\t\tif (is_bonded) {\n\t\t\t \n\t\t\tflow_attr->port = 2;\n\t\t\terr = __mlx4_ib_create_flow(qp, flow_attr,\n\t\t\t\t\t\t    MLX4_DOMAIN_UVERBS, type[j],\n\t\t\t\t\t\t    &mflow->reg_id[j].mirror);\n\t\t\tflow_attr->port = 1;\n\t\t\tif (err)\n\t\t\t\tgoto err_create_flow;\n\t\t\tj++;\n\t\t}\n\n\t\ti++;\n\t}\n\n\tif (i < ARRAY_SIZE(type) && flow_attr->type == IB_FLOW_ATTR_NORMAL) {\n\t\terr = mlx4_ib_tunnel_steer_add(qp, flow_attr,\n\t\t\t\t\t       &mflow->reg_id[i].id);\n\t\tif (err)\n\t\t\tgoto err_create_flow;\n\n\t\tif (is_bonded) {\n\t\t\tflow_attr->port = 2;\n\t\t\terr = mlx4_ib_tunnel_steer_add(qp, flow_attr,\n\t\t\t\t\t\t       &mflow->reg_id[j].mirror);\n\t\t\tflow_attr->port = 1;\n\t\t\tif (err)\n\t\t\t\tgoto err_create_flow;\n\t\t\tj++;\n\t\t}\n\t\t \n\t\ti++;\n\t}\n\n\treturn &mflow->ibflow;\n\nerr_create_flow:\n\twhile (i) {\n\t\t(void)__mlx4_ib_destroy_flow(to_mdev(qp->device)->dev,\n\t\t\t\t\t     mflow->reg_id[i].id);\n\t\ti--;\n\t}\n\n\twhile (j) {\n\t\t(void)__mlx4_ib_destroy_flow(to_mdev(qp->device)->dev,\n\t\t\t\t\t     mflow->reg_id[j].mirror);\n\t\tj--;\n\t}\nerr_free:\n\tkfree(mflow);\n\treturn ERR_PTR(err);\n}\n\nstatic int mlx4_ib_destroy_flow(struct ib_flow *flow_id)\n{\n\tint err, ret = 0;\n\tint i = 0;\n\tstruct mlx4_ib_dev *mdev = to_mdev(flow_id->qp->device);\n\tstruct mlx4_ib_flow *mflow = to_mflow(flow_id);\n\n\twhile (i < ARRAY_SIZE(mflow->reg_id) && mflow->reg_id[i].id) {\n\t\terr = __mlx4_ib_destroy_flow(mdev->dev, mflow->reg_id[i].id);\n\t\tif (err)\n\t\t\tret = err;\n\t\tif (mflow->reg_id[i].mirror) {\n\t\t\terr = __mlx4_ib_destroy_flow(mdev->dev,\n\t\t\t\t\t\t     mflow->reg_id[i].mirror);\n\t\t\tif (err)\n\t\t\t\tret = err;\n\t\t}\n\t\ti++;\n\t}\n\n\tkfree(mflow);\n\treturn ret;\n}\n\nstatic int mlx4_ib_mcg_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\n{\n\tint err;\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibqp->device);\n\tstruct mlx4_dev\t*dev = mdev->dev;\n\tstruct mlx4_ib_qp *mqp = to_mqp(ibqp);\n\tstruct mlx4_ib_steering *ib_steering = NULL;\n\tenum mlx4_protocol prot = MLX4_PROT_IB_IPV6;\n\tstruct mlx4_flow_reg_id\treg_id;\n\n\tif (mdev->dev->caps.steering_mode ==\n\t    MLX4_STEERING_MODE_DEVICE_MANAGED) {\n\t\tib_steering = kmalloc(sizeof(*ib_steering), GFP_KERNEL);\n\t\tif (!ib_steering)\n\t\t\treturn -ENOMEM;\n\t}\n\n\terr = mlx4_multicast_attach(mdev->dev, &mqp->mqp, gid->raw, mqp->port,\n\t\t\t\t    !!(mqp->flags &\n\t\t\t\t       MLX4_IB_QP_BLOCK_MULTICAST_LOOPBACK),\n\t\t\t\t    prot, &reg_id.id);\n\tif (err) {\n\t\tpr_err(\"multicast attach op failed, err %d\\n\", err);\n\t\tgoto err_malloc;\n\t}\n\n\treg_id.mirror = 0;\n\tif (mlx4_is_bonded(dev)) {\n\t\terr = mlx4_multicast_attach(mdev->dev, &mqp->mqp, gid->raw,\n\t\t\t\t\t    (mqp->port == 1) ? 2 : 1,\n\t\t\t\t\t    !!(mqp->flags &\n\t\t\t\t\t    MLX4_IB_QP_BLOCK_MULTICAST_LOOPBACK),\n\t\t\t\t\t    prot, &reg_id.mirror);\n\t\tif (err)\n\t\t\tgoto err_add;\n\t}\n\n\terr = add_gid_entry(ibqp, gid);\n\tif (err)\n\t\tgoto err_add;\n\n\tif (ib_steering) {\n\t\tmemcpy(ib_steering->gid.raw, gid->raw, 16);\n\t\tib_steering->reg_id = reg_id;\n\t\tmutex_lock(&mqp->mutex);\n\t\tlist_add(&ib_steering->list, &mqp->steering_rules);\n\t\tmutex_unlock(&mqp->mutex);\n\t}\n\treturn 0;\n\nerr_add:\n\tmlx4_multicast_detach(mdev->dev, &mqp->mqp, gid->raw,\n\t\t\t      prot, reg_id.id);\n\tif (reg_id.mirror)\n\t\tmlx4_multicast_detach(mdev->dev, &mqp->mqp, gid->raw,\n\t\t\t\t      prot, reg_id.mirror);\nerr_malloc:\n\tkfree(ib_steering);\n\n\treturn err;\n}\n\nstatic struct mlx4_ib_gid_entry *find_gid_entry(struct mlx4_ib_qp *qp, u8 *raw)\n{\n\tstruct mlx4_ib_gid_entry *ge;\n\tstruct mlx4_ib_gid_entry *tmp;\n\tstruct mlx4_ib_gid_entry *ret = NULL;\n\n\tlist_for_each_entry_safe(ge, tmp, &qp->gid_list, list) {\n\t\tif (!memcmp(raw, ge->gid.raw, 16)) {\n\t\t\tret = ge;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int mlx4_ib_mcg_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\n{\n\tint err;\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibqp->device);\n\tstruct mlx4_dev *dev = mdev->dev;\n\tstruct mlx4_ib_qp *mqp = to_mqp(ibqp);\n\tstruct net_device *ndev;\n\tstruct mlx4_ib_gid_entry *ge;\n\tstruct mlx4_flow_reg_id reg_id = {0, 0};\n\tenum mlx4_protocol prot =  MLX4_PROT_IB_IPV6;\n\n\tif (mdev->dev->caps.steering_mode ==\n\t    MLX4_STEERING_MODE_DEVICE_MANAGED) {\n\t\tstruct mlx4_ib_steering *ib_steering;\n\n\t\tmutex_lock(&mqp->mutex);\n\t\tlist_for_each_entry(ib_steering, &mqp->steering_rules, list) {\n\t\t\tif (!memcmp(ib_steering->gid.raw, gid->raw, 16)) {\n\t\t\t\tlist_del(&ib_steering->list);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&mqp->mutex);\n\t\tif (&ib_steering->list == &mqp->steering_rules) {\n\t\t\tpr_err(\"Couldn't find reg_id for mgid. Steering rule is left attached\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treg_id = ib_steering->reg_id;\n\t\tkfree(ib_steering);\n\t}\n\n\terr = mlx4_multicast_detach(mdev->dev, &mqp->mqp, gid->raw,\n\t\t\t\t    prot, reg_id.id);\n\tif (err)\n\t\treturn err;\n\n\tif (mlx4_is_bonded(dev)) {\n\t\terr = mlx4_multicast_detach(mdev->dev, &mqp->mqp, gid->raw,\n\t\t\t\t\t    prot, reg_id.mirror);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tmutex_lock(&mqp->mutex);\n\tge = find_gid_entry(mqp, gid->raw);\n\tif (ge) {\n\t\tspin_lock_bh(&mdev->iboe.lock);\n\t\tndev = ge->added ? mdev->iboe.netdevs[ge->port - 1] : NULL;\n\t\tdev_hold(ndev);\n\t\tspin_unlock_bh(&mdev->iboe.lock);\n\t\tdev_put(ndev);\n\t\tlist_del(&ge->list);\n\t\tkfree(ge);\n\t} else\n\t\tpr_warn(\"could not find mgid entry\\n\");\n\n\tmutex_unlock(&mqp->mutex);\n\n\treturn 0;\n}\n\nstatic int init_node_data(struct mlx4_ib_dev *dev)\n{\n\tstruct ib_smp *in_mad;\n\tstruct ib_smp *out_mad;\n\tint mad_ifc_flags = MLX4_MAD_IFC_IGNORE_KEYS;\n\tint err = -ENOMEM;\n\n\tin_mad  = kzalloc(sizeof *in_mad, GFP_KERNEL);\n\tout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\n\tif (!in_mad || !out_mad)\n\t\tgoto out;\n\n\tib_init_query_mad(in_mad);\n\tin_mad->attr_id = IB_SMP_ATTR_NODE_DESC;\n\tif (mlx4_is_master(dev->dev))\n\t\tmad_ifc_flags |= MLX4_MAD_IFC_NET_VIEW;\n\n\terr = mlx4_MAD_IFC(dev, mad_ifc_flags, 1, NULL, NULL, in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tmemcpy(dev->ib_dev.node_desc, out_mad->data, IB_DEVICE_NODE_DESC_MAX);\n\n\tin_mad->attr_id = IB_SMP_ATTR_NODE_INFO;\n\n\terr = mlx4_MAD_IFC(dev, mad_ifc_flags, 1, NULL, NULL, in_mad, out_mad);\n\tif (err)\n\t\tgoto out;\n\n\tdev->dev->rev_id = be32_to_cpup((__be32 *) (out_mad->data + 32));\n\tmemcpy(&dev->ib_dev.node_guid, out_mad->data + 12, 8);\n\nout:\n\tkfree(in_mad);\n\tkfree(out_mad);\n\treturn err;\n}\n\nstatic ssize_t hca_type_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct mlx4_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx4_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"MT%d\\n\", dev->dev->persist->pdev->device);\n}\nstatic DEVICE_ATTR_RO(hca_type);\n\nstatic ssize_t hw_rev_show(struct device *device,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct mlx4_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx4_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%x\\n\", dev->dev->rev_id);\n}\nstatic DEVICE_ATTR_RO(hw_rev);\n\nstatic ssize_t board_id_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct mlx4_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx4_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%.*s\\n\", MLX4_BOARD_ID_LEN, dev->dev->board_id);\n}\nstatic DEVICE_ATTR_RO(board_id);\n\nstatic struct attribute *mlx4_class_attributes[] = {\n\t&dev_attr_hw_rev.attr,\n\t&dev_attr_hca_type.attr,\n\t&dev_attr_board_id.attr,\n\tNULL\n};\n\nstatic const struct attribute_group mlx4_attr_group = {\n\t.attrs = mlx4_class_attributes,\n};\n\nstruct diag_counter {\n\tconst char *name;\n\tu32 offset;\n};\n\n#define DIAG_COUNTER(_name, _offset)\t\t\t\\\n\t{ .name = #_name, .offset = _offset }\n\nstatic const struct diag_counter diag_basic[] = {\n\tDIAG_COUNTER(rq_num_lle, 0x00),\n\tDIAG_COUNTER(sq_num_lle, 0x04),\n\tDIAG_COUNTER(rq_num_lqpoe, 0x08),\n\tDIAG_COUNTER(sq_num_lqpoe, 0x0C),\n\tDIAG_COUNTER(rq_num_lpe, 0x18),\n\tDIAG_COUNTER(sq_num_lpe, 0x1C),\n\tDIAG_COUNTER(rq_num_wrfe, 0x20),\n\tDIAG_COUNTER(sq_num_wrfe, 0x24),\n\tDIAG_COUNTER(sq_num_mwbe, 0x2C),\n\tDIAG_COUNTER(sq_num_bre, 0x34),\n\tDIAG_COUNTER(sq_num_rire, 0x44),\n\tDIAG_COUNTER(rq_num_rire, 0x48),\n\tDIAG_COUNTER(sq_num_rae, 0x4C),\n\tDIAG_COUNTER(rq_num_rae, 0x50),\n\tDIAG_COUNTER(sq_num_roe, 0x54),\n\tDIAG_COUNTER(sq_num_tree, 0x5C),\n\tDIAG_COUNTER(sq_num_rree, 0x64),\n\tDIAG_COUNTER(rq_num_rnr, 0x68),\n\tDIAG_COUNTER(sq_num_rnr, 0x6C),\n\tDIAG_COUNTER(rq_num_oos, 0x100),\n\tDIAG_COUNTER(sq_num_oos, 0x104),\n};\n\nstatic const struct diag_counter diag_ext[] = {\n\tDIAG_COUNTER(rq_num_dup, 0x130),\n\tDIAG_COUNTER(sq_num_to, 0x134),\n};\n\nstatic const struct diag_counter diag_device_only[] = {\n\tDIAG_COUNTER(num_cqovf, 0x1A0),\n\tDIAG_COUNTER(rq_num_udsdprd, 0x118),\n};\n\nstatic struct rdma_hw_stats *\nmlx4_ib_alloc_hw_device_stats(struct ib_device *ibdev)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx4_ib_diag_counters *diag = dev->diag_counters;\n\n\tif (!diag[0].descs)\n\t\treturn NULL;\n\n\treturn rdma_alloc_hw_stats_struct(diag[0].descs, diag[0].num_counters,\n\t\t\t\t\t  RDMA_HW_STATS_DEFAULT_LIFESPAN);\n}\n\nstatic struct rdma_hw_stats *\nmlx4_ib_alloc_hw_port_stats(struct ib_device *ibdev, u32 port_num)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx4_ib_diag_counters *diag = dev->diag_counters;\n\n\tif (!diag[1].descs)\n\t\treturn NULL;\n\n\treturn rdma_alloc_hw_stats_struct(diag[1].descs, diag[1].num_counters,\n\t\t\t\t\t  RDMA_HW_STATS_DEFAULT_LIFESPAN);\n}\n\nstatic int mlx4_ib_get_hw_stats(struct ib_device *ibdev,\n\t\t\t\tstruct rdma_hw_stats *stats,\n\t\t\t\tu32 port, int index)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx4_ib_diag_counters *diag = dev->diag_counters;\n\tu32 hw_value[ARRAY_SIZE(diag_device_only) +\n\t\tARRAY_SIZE(diag_ext) + ARRAY_SIZE(diag_basic)] = {};\n\tint ret;\n\tint i;\n\n\tret = mlx4_query_diag_counters(dev->dev,\n\t\t\t\t       MLX4_OP_MOD_QUERY_TRANSPORT_CI_ERRORS,\n\t\t\t\t       diag[!!port].offset, hw_value,\n\t\t\t\t       diag[!!port].num_counters, port);\n\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < diag[!!port].num_counters; i++)\n\t\tstats->value[i] = hw_value[i];\n\n\treturn diag[!!port].num_counters;\n}\n\nstatic int __mlx4_ib_alloc_diag_counters(struct mlx4_ib_dev *ibdev,\n\t\t\t\t\t struct rdma_stat_desc **pdescs,\n\t\t\t\t\t u32 **offset, u32 *num, bool port)\n{\n\tu32 num_counters;\n\n\tnum_counters = ARRAY_SIZE(diag_basic);\n\n\tif (ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DIAG_PER_PORT)\n\t\tnum_counters += ARRAY_SIZE(diag_ext);\n\n\tif (!port)\n\t\tnum_counters += ARRAY_SIZE(diag_device_only);\n\n\t*pdescs = kcalloc(num_counters, sizeof(struct rdma_stat_desc),\n\t\t\t  GFP_KERNEL);\n\tif (!*pdescs)\n\t\treturn -ENOMEM;\n\n\t*offset = kcalloc(num_counters, sizeof(**offset), GFP_KERNEL);\n\tif (!*offset)\n\t\tgoto err;\n\n\t*num = num_counters;\n\n\treturn 0;\n\nerr:\n\tkfree(*pdescs);\n\treturn -ENOMEM;\n}\n\nstatic void mlx4_ib_fill_diag_counters(struct mlx4_ib_dev *ibdev,\n\t\t\t\t       struct rdma_stat_desc *descs,\n\t\t\t\t       u32 *offset, bool port)\n{\n\tint i;\n\tint j;\n\n\tfor (i = 0, j = 0; i < ARRAY_SIZE(diag_basic); i++, j++) {\n\t\tdescs[i].name = diag_basic[i].name;\n\t\toffset[i] = diag_basic[i].offset;\n\t}\n\n\tif (ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DIAG_PER_PORT) {\n\t\tfor (i = 0; i < ARRAY_SIZE(diag_ext); i++, j++) {\n\t\t\tdescs[j].name = diag_ext[i].name;\n\t\t\toffset[j] = diag_ext[i].offset;\n\t\t}\n\t}\n\n\tif (!port) {\n\t\tfor (i = 0; i < ARRAY_SIZE(diag_device_only); i++, j++) {\n\t\t\tdescs[j].name = diag_device_only[i].name;\n\t\t\toffset[j] = diag_device_only[i].offset;\n\t\t}\n\t}\n}\n\nstatic const struct ib_device_ops mlx4_ib_hw_stats_ops = {\n\t.alloc_hw_device_stats = mlx4_ib_alloc_hw_device_stats,\n\t.alloc_hw_port_stats = mlx4_ib_alloc_hw_port_stats,\n\t.get_hw_stats = mlx4_ib_get_hw_stats,\n};\n\nstatic const struct ib_device_ops mlx4_ib_hw_stats_ops1 = {\n\t.alloc_hw_device_stats = mlx4_ib_alloc_hw_device_stats,\n\t.get_hw_stats = mlx4_ib_get_hw_stats,\n};\n\nstatic int mlx4_ib_alloc_diag_counters(struct mlx4_ib_dev *ibdev)\n{\n\tstruct mlx4_ib_diag_counters *diag = ibdev->diag_counters;\n\tint i;\n\tint ret;\n\tbool per_port = !!(ibdev->dev->caps.flags2 &\n\t\tMLX4_DEV_CAP_FLAG2_DIAG_PER_PORT);\n\n\tif (mlx4_is_slave(ibdev->dev))\n\t\treturn 0;\n\n\tfor (i = 0; i < MLX4_DIAG_COUNTERS_TYPES; i++) {\n\t\t \n\t\tif (i && !per_port) {\n\t\t\tib_set_device_ops(&ibdev->ib_dev,\n\t\t\t\t\t  &mlx4_ib_hw_stats_ops1);\n\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = __mlx4_ib_alloc_diag_counters(ibdev, &diag[i].descs,\n\t\t\t\t\t\t    &diag[i].offset,\n\t\t\t\t\t\t    &diag[i].num_counters, i);\n\t\tif (ret)\n\t\t\tgoto err_alloc;\n\n\t\tmlx4_ib_fill_diag_counters(ibdev, diag[i].descs,\n\t\t\t\t\t   diag[i].offset, i);\n\t}\n\n\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_hw_stats_ops);\n\n\treturn 0;\n\nerr_alloc:\n\tif (i) {\n\t\tkfree(diag[i - 1].descs);\n\t\tkfree(diag[i - 1].offset);\n\t}\n\n\treturn ret;\n}\n\nstatic void mlx4_ib_diag_cleanup(struct mlx4_ib_dev *ibdev)\n{\n\tint i;\n\n\tfor (i = 0; i < MLX4_DIAG_COUNTERS_TYPES; i++) {\n\t\tkfree(ibdev->diag_counters[i].offset);\n\t\tkfree(ibdev->diag_counters[i].descs);\n\t}\n}\n\n#define MLX4_IB_INVALID_MAC\t((u64)-1)\nstatic void mlx4_ib_update_qps(struct mlx4_ib_dev *ibdev,\n\t\t\t       struct net_device *dev,\n\t\t\t       int port)\n{\n\tu64 new_smac = 0;\n\tu64 release_mac = MLX4_IB_INVALID_MAC;\n\tstruct mlx4_ib_qp *qp;\n\n\tnew_smac = ether_addr_to_u64(dev->dev_addr);\n\tatomic64_set(&ibdev->iboe.mac[port - 1], new_smac);\n\n\t \n\tif (!mlx4_is_mfunc(ibdev->dev))\n\t\treturn;\n\n\tmutex_lock(&ibdev->qp1_proxy_lock[port - 1]);\n\tqp = ibdev->qp1_proxy[port - 1];\n\tif (qp) {\n\t\tint new_smac_index;\n\t\tu64 old_smac;\n\t\tstruct mlx4_update_qp_params update_params;\n\n\t\tmutex_lock(&qp->mutex);\n\t\told_smac = qp->pri.smac;\n\t\tif (new_smac == old_smac)\n\t\t\tgoto unlock;\n\n\t\tnew_smac_index = mlx4_register_mac(ibdev->dev, port, new_smac);\n\n\t\tif (new_smac_index < 0)\n\t\t\tgoto unlock;\n\n\t\tupdate_params.smac_index = new_smac_index;\n\t\tif (mlx4_update_qp(ibdev->dev, qp->mqp.qpn, MLX4_UPDATE_QP_SMAC,\n\t\t\t\t   &update_params)) {\n\t\t\trelease_mac = new_smac;\n\t\t\tgoto unlock;\n\t\t}\n\t\t \n\t\tif (qp->pri.smac_port)\n\t\t\trelease_mac = old_smac;\n\t\tqp->pri.smac = new_smac;\n\t\tqp->pri.smac_port = port;\n\t\tqp->pri.smac_index = new_smac_index;\n\t}\n\nunlock:\n\tif (release_mac != MLX4_IB_INVALID_MAC)\n\t\tmlx4_unregister_mac(ibdev->dev, port, release_mac);\n\tif (qp)\n\t\tmutex_unlock(&qp->mutex);\n\tmutex_unlock(&ibdev->qp1_proxy_lock[port - 1]);\n}\n\nstatic void mlx4_ib_scan_netdev(struct mlx4_ib_dev *ibdev,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tunsigned long event)\n\n{\n\tstruct mlx4_ib_iboe *iboe = &ibdev->iboe;\n\n\tASSERT_RTNL();\n\n\tif (dev->dev.parent != ibdev->ib_dev.dev.parent)\n\t\treturn;\n\n\tspin_lock_bh(&iboe->lock);\n\n\tiboe->netdevs[dev->dev_port] = event != NETDEV_UNREGISTER ? dev : NULL;\n\n\tif (event == NETDEV_UP || event == NETDEV_DOWN) {\n\t\tenum ib_port_state port_state;\n\t\tstruct ib_event ibev = { };\n\n\t\tif (ib_get_cached_port_state(&ibdev->ib_dev, dev->dev_port + 1,\n\t\t\t\t\t     &port_state))\n\t\t\tgoto iboe_out;\n\n\t\tif (event == NETDEV_UP &&\n\t\t    (port_state != IB_PORT_ACTIVE ||\n\t\t     iboe->last_port_state[dev->dev_port] != IB_PORT_DOWN))\n\t\t\tgoto iboe_out;\n\t\tif (event == NETDEV_DOWN &&\n\t\t    (port_state != IB_PORT_DOWN ||\n\t\t     iboe->last_port_state[dev->dev_port] != IB_PORT_ACTIVE))\n\t\t\tgoto iboe_out;\n\t\tiboe->last_port_state[dev->dev_port] = port_state;\n\n\t\tibev.device = &ibdev->ib_dev;\n\t\tibev.element.port_num = dev->dev_port + 1;\n\t\tibev.event = event == NETDEV_UP ? IB_EVENT_PORT_ACTIVE :\n\t\t\t\t\t\t  IB_EVENT_PORT_ERR;\n\t\tib_dispatch_event(&ibev);\n\t}\n\niboe_out:\n\tspin_unlock_bh(&iboe->lock);\n\n\tif (event == NETDEV_CHANGEADDR || event == NETDEV_REGISTER ||\n\t    event == NETDEV_UP || event == NETDEV_CHANGE)\n\t\tmlx4_ib_update_qps(ibdev, dev, dev->dev_port + 1);\n}\n\nstatic int mlx4_ib_netdev_event(struct notifier_block *this,\n\t\t\t\tunsigned long event, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct mlx4_ib_dev *ibdev;\n\n\tif (!net_eq(dev_net(dev), &init_net))\n\t\treturn NOTIFY_DONE;\n\n\tibdev = container_of(this, struct mlx4_ib_dev, iboe.nb);\n\tmlx4_ib_scan_netdev(ibdev, dev, event);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic void init_pkeys(struct mlx4_ib_dev *ibdev)\n{\n\tint port;\n\tint slave;\n\tint i;\n\n\tif (mlx4_is_master(ibdev->dev)) {\n\t\tfor (slave = 0; slave <= ibdev->dev->persist->num_vfs;\n\t\t     ++slave) {\n\t\t\tfor (port = 1; port <= ibdev->dev->caps.num_ports; ++port) {\n\t\t\t\tfor (i = 0;\n\t\t\t\t     i < ibdev->dev->phys_caps.pkey_phys_table_len[port];\n\t\t\t\t     ++i) {\n\t\t\t\t\tibdev->pkeys.virt2phys_pkey[slave][port - 1][i] =\n\t\t\t\t\t \n\t\t\t\t\t\t(slave == mlx4_master_func_num(ibdev->dev) || !i) ? i :\n\t\t\t\t\t\t\tibdev->dev->phys_caps.pkey_phys_table_len[port] - 1;\n\t\t\t\t\tmlx4_sync_pkey_table(ibdev->dev, slave, port, i,\n\t\t\t\t\t\t\t     ibdev->pkeys.virt2phys_pkey[slave][port - 1][i]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t \n\t\tfor (port = 1; port <= ibdev->dev->caps.num_ports; ++port) {\n\t\t\tfor (i = 0;\n\t\t\t     i < ibdev->dev->phys_caps.pkey_phys_table_len[port];\n\t\t\t     ++i)\n\t\t\t\tibdev->pkeys.phys_pkey_cache[port-1][i] =\n\t\t\t\t\t(i) ? 0 : 0xFFFF;\n\t\t}\n\t}\n}\n\nstatic void mlx4_ib_alloc_eqs(struct mlx4_dev *dev, struct mlx4_ib_dev *ibdev)\n{\n\tint i, j, eq = 0, total_eqs = 0;\n\n\tibdev->eq_table = kcalloc(dev->caps.num_comp_vectors,\n\t\t\t\t  sizeof(ibdev->eq_table[0]), GFP_KERNEL);\n\tif (!ibdev->eq_table)\n\t\treturn;\n\n\tfor (i = 1; i <= dev->caps.num_ports; i++) {\n\t\tfor (j = 0; j < mlx4_get_eqs_per_port(dev, i);\n\t\t     j++, total_eqs++) {\n\t\t\tif (i > 1 &&  mlx4_is_eq_shared(dev, total_eqs))\n\t\t\t\tcontinue;\n\t\t\tibdev->eq_table[eq] = total_eqs;\n\t\t\tif (!mlx4_assign_eq(dev, i,\n\t\t\t\t\t    &ibdev->eq_table[eq]))\n\t\t\t\teq++;\n\t\t\telse\n\t\t\t\tibdev->eq_table[eq] = -1;\n\t\t}\n\t}\n\n\tfor (i = eq; i < dev->caps.num_comp_vectors;\n\t     ibdev->eq_table[i++] = -1)\n\t\t;\n\n\t \n\tibdev->ib_dev.num_comp_vectors = eq;\n}\n\nstatic void mlx4_ib_free_eqs(struct mlx4_dev *dev, struct mlx4_ib_dev *ibdev)\n{\n\tint i;\n\tint total_eqs = ibdev->ib_dev.num_comp_vectors;\n\n\t \n\tif (!ibdev->eq_table)\n\t\treturn;\n\n\t \n\tibdev->ib_dev.num_comp_vectors = 0;\n\n\tfor (i = 0; i < total_eqs; i++)\n\t\tmlx4_release_eq(dev, ibdev->eq_table[i]);\n\n\tkfree(ibdev->eq_table);\n\tibdev->eq_table = NULL;\n}\n\nstatic int mlx4_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t       struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tstruct mlx4_ib_dev *mdev = to_mdev(ibdev);\n\tint err;\n\n\tif (mlx4_ib_port_link_layer(ibdev, port_num) == IB_LINK_LAYER_INFINIBAND) {\n\t\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_IB;\n\t\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\t} else {\n\t\tif (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_IBOE)\n\t\t\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE;\n\t\tif (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ROCE_V1_V2)\n\t\t\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE |\n\t\t\t\tRDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;\n\t\timmutable->core_cap_flags |= RDMA_CORE_PORT_RAW_PACKET;\n\t\tif (immutable->core_cap_flags & (RDMA_CORE_PORT_IBA_ROCE |\n\t\t    RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP))\n\t\t\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\t}\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\n\treturn 0;\n}\n\nstatic void get_fw_ver_str(struct ib_device *device, char *str)\n{\n\tstruct mlx4_ib_dev *dev =\n\t\tcontainer_of(device, struct mlx4_ib_dev, ib_dev);\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%d\",\n\t\t (int) (dev->dev->caps.fw_ver >> 32),\n\t\t (int) (dev->dev->caps.fw_ver >> 16) & 0xffff,\n\t\t (int) dev->dev->caps.fw_ver & 0xffff);\n}\n\nstatic const struct ib_device_ops mlx4_ib_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_MLX4,\n\t.uverbs_abi_ver = MLX4_IB_UVERBS_ABI_VERSION,\n\n\t.add_gid = mlx4_ib_add_gid,\n\t.alloc_mr = mlx4_ib_alloc_mr,\n\t.alloc_pd = mlx4_ib_alloc_pd,\n\t.alloc_ucontext = mlx4_ib_alloc_ucontext,\n\t.attach_mcast = mlx4_ib_mcg_attach,\n\t.create_ah = mlx4_ib_create_ah,\n\t.create_cq = mlx4_ib_create_cq,\n\t.create_qp = mlx4_ib_create_qp,\n\t.create_srq = mlx4_ib_create_srq,\n\t.dealloc_pd = mlx4_ib_dealloc_pd,\n\t.dealloc_ucontext = mlx4_ib_dealloc_ucontext,\n\t.del_gid = mlx4_ib_del_gid,\n\t.dereg_mr = mlx4_ib_dereg_mr,\n\t.destroy_ah = mlx4_ib_destroy_ah,\n\t.destroy_cq = mlx4_ib_destroy_cq,\n\t.destroy_qp = mlx4_ib_destroy_qp,\n\t.destroy_srq = mlx4_ib_destroy_srq,\n\t.detach_mcast = mlx4_ib_mcg_detach,\n\t.device_group = &mlx4_attr_group,\n\t.disassociate_ucontext = mlx4_ib_disassociate_ucontext,\n\t.drain_rq = mlx4_ib_drain_rq,\n\t.drain_sq = mlx4_ib_drain_sq,\n\t.get_dev_fw_str = get_fw_ver_str,\n\t.get_dma_mr = mlx4_ib_get_dma_mr,\n\t.get_link_layer = mlx4_ib_port_link_layer,\n\t.get_netdev = mlx4_ib_get_netdev,\n\t.get_port_immutable = mlx4_port_immutable,\n\t.map_mr_sg = mlx4_ib_map_mr_sg,\n\t.mmap = mlx4_ib_mmap,\n\t.modify_cq = mlx4_ib_modify_cq,\n\t.modify_device = mlx4_ib_modify_device,\n\t.modify_port = mlx4_ib_modify_port,\n\t.modify_qp = mlx4_ib_modify_qp,\n\t.modify_srq = mlx4_ib_modify_srq,\n\t.poll_cq = mlx4_ib_poll_cq,\n\t.post_recv = mlx4_ib_post_recv,\n\t.post_send = mlx4_ib_post_send,\n\t.post_srq_recv = mlx4_ib_post_srq_recv,\n\t.process_mad = mlx4_ib_process_mad,\n\t.query_ah = mlx4_ib_query_ah,\n\t.query_device = mlx4_ib_query_device,\n\t.query_gid = mlx4_ib_query_gid,\n\t.query_pkey = mlx4_ib_query_pkey,\n\t.query_port = mlx4_ib_query_port,\n\t.query_qp = mlx4_ib_query_qp,\n\t.query_srq = mlx4_ib_query_srq,\n\t.reg_user_mr = mlx4_ib_reg_user_mr,\n\t.req_notify_cq = mlx4_ib_arm_cq,\n\t.rereg_user_mr = mlx4_ib_rereg_user_mr,\n\t.resize_cq = mlx4_ib_resize_cq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_ah, mlx4_ib_ah, ibah),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, mlx4_ib_cq, ibcq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, mlx4_ib_pd, ibpd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, mlx4_ib_qp, ibqp),\n\tINIT_RDMA_OBJ_SIZE(ib_srq, mlx4_ib_srq, ibsrq),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, mlx4_ib_ucontext, ibucontext),\n};\n\nstatic const struct ib_device_ops mlx4_ib_dev_wq_ops = {\n\t.create_rwq_ind_table = mlx4_ib_create_rwq_ind_table,\n\t.create_wq = mlx4_ib_create_wq,\n\t.destroy_rwq_ind_table = mlx4_ib_destroy_rwq_ind_table,\n\t.destroy_wq = mlx4_ib_destroy_wq,\n\t.modify_wq = mlx4_ib_modify_wq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_rwq_ind_table, mlx4_ib_rwq_ind_table,\n\t\t\t   ib_rwq_ind_tbl),\n};\n\nstatic const struct ib_device_ops mlx4_ib_dev_mw_ops = {\n\t.alloc_mw = mlx4_ib_alloc_mw,\n\t.dealloc_mw = mlx4_ib_dealloc_mw,\n\n\tINIT_RDMA_OBJ_SIZE(ib_mw, mlx4_ib_mw, ibmw),\n};\n\nstatic const struct ib_device_ops mlx4_ib_dev_xrc_ops = {\n\t.alloc_xrcd = mlx4_ib_alloc_xrcd,\n\t.dealloc_xrcd = mlx4_ib_dealloc_xrcd,\n\n\tINIT_RDMA_OBJ_SIZE(ib_xrcd, mlx4_ib_xrcd, ibxrcd),\n};\n\nstatic const struct ib_device_ops mlx4_ib_dev_fs_ops = {\n\t.create_flow = mlx4_ib_create_flow,\n\t.destroy_flow = mlx4_ib_destroy_flow,\n};\n\nstatic int mlx4_ib_probe(struct auxiliary_device *adev,\n\t\t\t const struct auxiliary_device_id *id)\n{\n\tstruct mlx4_adev *madev = container_of(adev, struct mlx4_adev, adev);\n\tstruct mlx4_dev *dev = madev->mdev;\n\tstruct mlx4_ib_dev *ibdev;\n\tint num_ports = 0;\n\tint i, j;\n\tint err;\n\tstruct mlx4_ib_iboe *iboe;\n\tint ib_num_ports = 0;\n\tint num_req_counters;\n\tint allocated;\n\tu32 counter_index;\n\tstruct counter_index *new_counter_index;\n\n\tpr_info_once(\"%s\", mlx4_ib_version);\n\n\tnum_ports = 0;\n\tmlx4_foreach_ib_transport_port(i, dev)\n\t\tnum_ports++;\n\n\t \n\tif (num_ports == 0)\n\t\treturn -ENODEV;\n\n\tibdev = ib_alloc_device(mlx4_ib_dev, ib_dev);\n\tif (!ibdev) {\n\t\tdev_err(&dev->persist->pdev->dev,\n\t\t\t\"Device struct alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tiboe = &ibdev->iboe;\n\n\terr = mlx4_pd_alloc(dev, &ibdev->priv_pdn);\n\tif (err)\n\t\tgoto err_dealloc;\n\n\terr = mlx4_uar_alloc(dev, &ibdev->priv_uar);\n\tif (err)\n\t\tgoto err_pd;\n\n\tibdev->uar_map = ioremap((phys_addr_t) ibdev->priv_uar.pfn << PAGE_SHIFT,\n\t\t\t\t PAGE_SIZE);\n\tif (!ibdev->uar_map) {\n\t\terr = -ENOMEM;\n\t\tgoto err_uar;\n\t}\n\tMLX4_INIT_DOORBELL_LOCK(&ibdev->uar_lock);\n\n\tibdev->dev = dev;\n\tibdev->bond_next_port\t= 0;\n\n\tibdev->ib_dev.node_type\t\t= RDMA_NODE_IB_CA;\n\tibdev->ib_dev.local_dma_lkey\t= dev->caps.reserved_lkey;\n\tibdev->num_ports\t\t= num_ports;\n\tibdev->ib_dev.phys_port_cnt     = mlx4_is_bonded(dev) ?\n\t\t\t\t\t\t1 : ibdev->num_ports;\n\tibdev->ib_dev.num_comp_vectors\t= dev->caps.num_comp_vectors;\n\tibdev->ib_dev.dev.parent\t= &dev->persist->pdev->dev;\n\n\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_ops);\n\n\tif ((dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS) &&\n\t    ((mlx4_ib_port_link_layer(&ibdev->ib_dev, 1) ==\n\t    IB_LINK_LAYER_ETHERNET) ||\n\t    (mlx4_ib_port_link_layer(&ibdev->ib_dev, 2) ==\n\t    IB_LINK_LAYER_ETHERNET)))\n\t\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_wq_ops);\n\n\tif (dev->caps.flags & MLX4_DEV_CAP_FLAG_MEM_WINDOW ||\n\t    dev->caps.bmme_flags & MLX4_BMME_FLAG_TYPE_2_WIN)\n\t\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_mw_ops);\n\n\tif (dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC) {\n\t\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_xrc_ops);\n\t}\n\n\tif (check_flow_steering_support(dev)) {\n\t\tibdev->steering_support = MLX4_STEERING_MODE_DEVICE_MANAGED;\n\t\tib_set_device_ops(&ibdev->ib_dev, &mlx4_ib_dev_fs_ops);\n\t}\n\n\tif (!dev->caps.userspace_caps)\n\t\tibdev->ib_dev.ops.uverbs_abi_ver =\n\t\t\tMLX4_IB_UVERBS_NO_DEV_CAPS_ABI_VERSION;\n\n\tmlx4_ib_alloc_eqs(dev, ibdev);\n\n\tspin_lock_init(&iboe->lock);\n\n\terr = init_node_data(ibdev);\n\tif (err)\n\t\tgoto err_map;\n\tmlx4_init_sl2vl_tbl(ibdev);\n\n\tfor (i = 0; i < ibdev->num_ports; ++i) {\n\t\tmutex_init(&ibdev->counters_table[i].mutex);\n\t\tINIT_LIST_HEAD(&ibdev->counters_table[i].counters_list);\n\t\tiboe->last_port_state[i] = IB_PORT_DOWN;\n\t}\n\n\tnum_req_counters = mlx4_is_bonded(dev) ? 1 : ibdev->num_ports;\n\tfor (i = 0; i < num_req_counters; ++i) {\n\t\tmutex_init(&ibdev->qp1_proxy_lock[i]);\n\t\tallocated = 0;\n\t\tif (mlx4_ib_port_link_layer(&ibdev->ib_dev, i + 1) ==\n\t\t\t\t\t\tIB_LINK_LAYER_ETHERNET) {\n\t\t\terr = mlx4_counter_alloc(ibdev->dev, &counter_index,\n\t\t\t\t\t\t MLX4_RES_USAGE_DRIVER);\n\t\t\t \n\t\t\tif (err)\n\t\t\t\tcounter_index =\n\t\t\t\t\tmlx4_get_default_counter_index(dev,\n\t\t\t\t\t\t\t\t       i + 1);\n\t\t\telse\n\t\t\t\tallocated = 1;\n\t\t} else {  \n\t\t\tcounter_index = mlx4_get_default_counter_index(dev,\n\t\t\t\t\t\t\t\t       i + 1);\n\t\t}\n\t\tnew_counter_index = kmalloc(sizeof(*new_counter_index),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!new_counter_index) {\n\t\t\terr = -ENOMEM;\n\t\t\tif (allocated)\n\t\t\t\tmlx4_counter_free(ibdev->dev, counter_index);\n\t\t\tgoto err_counter;\n\t\t}\n\t\tnew_counter_index->index = counter_index;\n\t\tnew_counter_index->allocated = allocated;\n\t\tlist_add_tail(&new_counter_index->list,\n\t\t\t      &ibdev->counters_table[i].counters_list);\n\t\tibdev->counters_table[i].default_counter = counter_index;\n\t\tpr_info(\"counter index %d for port %d allocated %d\\n\",\n\t\t\tcounter_index, i + 1, allocated);\n\t}\n\tif (mlx4_is_bonded(dev))\n\t\tfor (i = 1; i < ibdev->num_ports ; ++i) {\n\t\t\tnew_counter_index =\n\t\t\t\t\tkmalloc(sizeof(struct counter_index),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!new_counter_index) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_counter;\n\t\t\t}\n\t\t\tnew_counter_index->index = counter_index;\n\t\t\tnew_counter_index->allocated = 0;\n\t\t\tlist_add_tail(&new_counter_index->list,\n\t\t\t\t      &ibdev->counters_table[i].counters_list);\n\t\t\tibdev->counters_table[i].default_counter =\n\t\t\t\t\t\t\t\tcounter_index;\n\t\t}\n\n\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)\n\t\tib_num_ports++;\n\n\tspin_lock_init(&ibdev->sm_lock);\n\tmutex_init(&ibdev->cap_mask_mutex);\n\tINIT_LIST_HEAD(&ibdev->qp_list);\n\tspin_lock_init(&ibdev->reset_flow_resource_lock);\n\n\tif (ibdev->steering_support == MLX4_STEERING_MODE_DEVICE_MANAGED &&\n\t    ib_num_ports) {\n\t\tibdev->steer_qpn_count = MLX4_IB_UC_MAX_NUM_QPS;\n\t\terr = mlx4_qp_reserve_range(dev, ibdev->steer_qpn_count,\n\t\t\t\t\t    MLX4_IB_UC_STEER_QPN_ALIGN,\n\t\t\t\t\t    &ibdev->steer_qpn_base, 0,\n\t\t\t\t\t    MLX4_RES_USAGE_DRIVER);\n\t\tif (err)\n\t\t\tgoto err_counter;\n\n\t\tibdev->ib_uc_qpns_bitmap = bitmap_alloc(ibdev->steer_qpn_count,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!ibdev->ib_uc_qpns_bitmap) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_steer_qp_release;\n\t\t}\n\n\t\tif (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DMFS_IPOIB) {\n\t\t\tbitmap_zero(ibdev->ib_uc_qpns_bitmap,\n\t\t\t\t    ibdev->steer_qpn_count);\n\t\t\terr = mlx4_FLOW_STEERING_IB_UC_QP_RANGE(\n\t\t\t\t\tdev, ibdev->steer_qpn_base,\n\t\t\t\t\tibdev->steer_qpn_base +\n\t\t\t\t\tibdev->steer_qpn_count - 1);\n\t\t\tif (err)\n\t\t\t\tgoto err_steer_free_bitmap;\n\t\t} else {\n\t\t\tbitmap_fill(ibdev->ib_uc_qpns_bitmap,\n\t\t\t\t    ibdev->steer_qpn_count);\n\t\t}\n\t}\n\n\tfor (j = 1; j <= ibdev->dev->caps.num_ports; j++)\n\t\tatomic64_set(&iboe->mac[j - 1], ibdev->dev->caps.def_mac[j]);\n\n\terr = mlx4_ib_alloc_diag_counters(ibdev);\n\tif (err)\n\t\tgoto err_steer_free_bitmap;\n\n\terr = ib_register_device(&ibdev->ib_dev, \"mlx4_%d\",\n\t\t\t\t &dev->persist->pdev->dev);\n\tif (err)\n\t\tgoto err_diag_counters;\n\n\terr = mlx4_ib_mad_init(ibdev);\n\tif (err)\n\t\tgoto err_reg;\n\n\terr = mlx4_ib_init_sriov(ibdev);\n\tif (err)\n\t\tgoto err_mad;\n\n\tif (!iboe->nb.notifier_call) {\n\t\tiboe->nb.notifier_call = mlx4_ib_netdev_event;\n\t\terr = register_netdevice_notifier(&iboe->nb);\n\t\tif (err) {\n\t\t\tiboe->nb.notifier_call = NULL;\n\t\t\tgoto err_notif;\n\t\t}\n\t}\n\tif (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ROCE_V1_V2) {\n\t\terr = mlx4_config_roce_v2_port(dev, ROCE_V2_UDP_DPORT);\n\t\tif (err)\n\t\t\tgoto err_notif;\n\t}\n\n\tibdev->ib_active = true;\n\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)\n\t\tdevlink_port_type_ib_set(mlx4_get_devlink_port(dev, i),\n\t\t\t\t\t &ibdev->ib_dev);\n\n\tif (mlx4_is_mfunc(ibdev->dev))\n\t\tinit_pkeys(ibdev);\n\n\t \n\tif (mlx4_is_master(ibdev->dev)) {\n\t\tfor (j = 0; j < MLX4_MFUNC_MAX; j++) {\n\t\t\tif (j == mlx4_master_func_num(ibdev->dev))\n\t\t\t\tcontinue;\n\t\t\tif (mlx4_is_slave_active(ibdev->dev, j))\n\t\t\t\tdo_slave_init(ibdev, j, 1);\n\t\t}\n\t}\n\n\t \n\tibdev->mlx_nb.notifier_call = mlx4_ib_event;\n\terr = mlx4_register_event_notifier(dev, &ibdev->mlx_nb);\n\tWARN(err, \"failed to register mlx4 event notifier (%d)\", err);\n\n\tauxiliary_set_drvdata(adev, ibdev);\n\treturn 0;\n\nerr_notif:\n\tif (ibdev->iboe.nb.notifier_call) {\n\t\tif (unregister_netdevice_notifier(&ibdev->iboe.nb))\n\t\t\tpr_warn(\"failure unregistering notifier\\n\");\n\t\tibdev->iboe.nb.notifier_call = NULL;\n\t}\n\tflush_workqueue(wq);\n\n\tmlx4_ib_close_sriov(ibdev);\n\nerr_mad:\n\tmlx4_ib_mad_cleanup(ibdev);\n\nerr_reg:\n\tib_unregister_device(&ibdev->ib_dev);\n\nerr_diag_counters:\n\tmlx4_ib_diag_cleanup(ibdev);\n\nerr_steer_free_bitmap:\n\tbitmap_free(ibdev->ib_uc_qpns_bitmap);\n\nerr_steer_qp_release:\n\tmlx4_qp_release_range(dev, ibdev->steer_qpn_base,\n\t\t\t      ibdev->steer_qpn_count);\nerr_counter:\n\tfor (i = 0; i < ibdev->num_ports; ++i)\n\t\tmlx4_ib_delete_counters_table(ibdev, &ibdev->counters_table[i]);\n\nerr_map:\n\tmlx4_ib_free_eqs(dev, ibdev);\n\tiounmap(ibdev->uar_map);\n\nerr_uar:\n\tmlx4_uar_free(dev, &ibdev->priv_uar);\n\nerr_pd:\n\tmlx4_pd_free(dev, ibdev->priv_pdn);\n\nerr_dealloc:\n\tib_dealloc_device(&ibdev->ib_dev);\n\n\treturn err;\n}\n\nint mlx4_ib_steer_qp_alloc(struct mlx4_ib_dev *dev, int count, int *qpn)\n{\n\tint offset;\n\n\tWARN_ON(!dev->ib_uc_qpns_bitmap);\n\n\toffset = bitmap_find_free_region(dev->ib_uc_qpns_bitmap,\n\t\t\t\t\t dev->steer_qpn_count,\n\t\t\t\t\t get_count_order(count));\n\tif (offset < 0)\n\t\treturn offset;\n\n\t*qpn = dev->steer_qpn_base + offset;\n\treturn 0;\n}\n\nvoid mlx4_ib_steer_qp_free(struct mlx4_ib_dev *dev, u32 qpn, int count)\n{\n\tif (!qpn ||\n\t    dev->steering_support != MLX4_STEERING_MODE_DEVICE_MANAGED)\n\t\treturn;\n\n\tif (WARN(qpn < dev->steer_qpn_base, \"qpn = %u, steer_qpn_base = %u\\n\",\n\t\t qpn, dev->steer_qpn_base))\n\t\t \n\t\treturn;\n\n\tbitmap_release_region(dev->ib_uc_qpns_bitmap,\n\t\t\t      qpn - dev->steer_qpn_base,\n\t\t\t      get_count_order(count));\n}\n\nint mlx4_ib_steer_qp_reg(struct mlx4_ib_dev *mdev, struct mlx4_ib_qp *mqp,\n\t\t\t int is_attach)\n{\n\tint err;\n\tsize_t flow_size;\n\tstruct ib_flow_attr *flow;\n\tstruct ib_flow_spec_ib *ib_spec;\n\n\tif (is_attach) {\n\t\tflow_size = sizeof(struct ib_flow_attr) +\n\t\t\t    sizeof(struct ib_flow_spec_ib);\n\t\tflow = kzalloc(flow_size, GFP_KERNEL);\n\t\tif (!flow)\n\t\t\treturn -ENOMEM;\n\t\tflow->port = mqp->port;\n\t\tflow->num_of_specs = 1;\n\t\tflow->size = flow_size;\n\t\tib_spec = (struct ib_flow_spec_ib *)(flow + 1);\n\t\tib_spec->type = IB_FLOW_SPEC_IB;\n\t\tib_spec->size = sizeof(struct ib_flow_spec_ib);\n\t\t \n\t\tmemset(&ib_spec->mask, 0, sizeof(ib_spec->mask));\n\n\t\terr = __mlx4_ib_create_flow(&mqp->ibqp, flow, MLX4_DOMAIN_NIC,\n\t\t\t\t\t    MLX4_FS_REGULAR, &mqp->reg_id);\n\t\tkfree(flow);\n\t\treturn err;\n\t}\n\t\n\treturn __mlx4_ib_destroy_flow(mdev->dev, mqp->reg_id);\n}\n\nstatic void mlx4_ib_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx4_adev *madev = container_of(adev, struct mlx4_adev, adev);\n\tstruct mlx4_dev *dev = madev->mdev;\n\tstruct mlx4_ib_dev *ibdev = auxiliary_get_drvdata(adev);\n\tint p;\n\tint i;\n\n\tmlx4_unregister_event_notifier(dev, &ibdev->mlx_nb);\n\n\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)\n\t\tdevlink_port_type_clear(mlx4_get_devlink_port(dev, i));\n\tibdev->ib_active = false;\n\tflush_workqueue(wq);\n\n\tif (ibdev->iboe.nb.notifier_call) {\n\t\tif (unregister_netdevice_notifier(&ibdev->iboe.nb))\n\t\t\tpr_warn(\"failure unregistering notifier\\n\");\n\t\tibdev->iboe.nb.notifier_call = NULL;\n\t}\n\n\tmlx4_ib_close_sriov(ibdev);\n\tmlx4_ib_mad_cleanup(ibdev);\n\tib_unregister_device(&ibdev->ib_dev);\n\tmlx4_ib_diag_cleanup(ibdev);\n\n\tmlx4_qp_release_range(dev, ibdev->steer_qpn_base,\n\t\t\t      ibdev->steer_qpn_count);\n\tbitmap_free(ibdev->ib_uc_qpns_bitmap);\n\n\tiounmap(ibdev->uar_map);\n\tfor (p = 0; p < ibdev->num_ports; ++p)\n\t\tmlx4_ib_delete_counters_table(ibdev, &ibdev->counters_table[p]);\n\n\tmlx4_foreach_port(p, dev, MLX4_PORT_TYPE_IB)\n\t\tmlx4_CLOSE_PORT(dev, p);\n\n\tmlx4_ib_free_eqs(dev, ibdev);\n\n\tmlx4_uar_free(dev, &ibdev->priv_uar);\n\tmlx4_pd_free(dev, ibdev->priv_pdn);\n\tib_dealloc_device(&ibdev->ib_dev);\n}\n\nstatic void do_slave_init(struct mlx4_ib_dev *ibdev, int slave, int do_init)\n{\n\tstruct mlx4_ib_demux_work **dm;\n\tstruct mlx4_dev *dev = ibdev->dev;\n\tint i;\n\tunsigned long flags;\n\tstruct mlx4_active_ports actv_ports;\n\tunsigned int ports;\n\tunsigned int first_port;\n\n\tif (!mlx4_is_master(dev))\n\t\treturn;\n\n\tactv_ports = mlx4_get_active_ports(dev, slave);\n\tports = bitmap_weight(actv_ports.ports, dev->caps.num_ports);\n\tfirst_port = find_first_bit(actv_ports.ports, dev->caps.num_ports);\n\n\tdm = kcalloc(ports, sizeof(*dm), GFP_ATOMIC);\n\tif (!dm)\n\t\treturn;\n\n\tfor (i = 0; i < ports; i++) {\n\t\tdm[i] = kmalloc(sizeof (struct mlx4_ib_demux_work), GFP_ATOMIC);\n\t\tif (!dm[i]) {\n\t\t\twhile (--i >= 0)\n\t\t\t\tkfree(dm[i]);\n\t\t\tgoto out;\n\t\t}\n\t\tINIT_WORK(&dm[i]->work, mlx4_ib_tunnels_update_work);\n\t\tdm[i]->port = first_port + i + 1;\n\t\tdm[i]->slave = slave;\n\t\tdm[i]->do_init = do_init;\n\t\tdm[i]->dev = ibdev;\n\t}\n\t \n\tspin_lock_irqsave(&ibdev->sriov.going_down_lock, flags);\n\tif (!ibdev->sriov.is_going_down) {\n\t\tfor (i = 0; i < ports; i++)\n\t\t\tqueue_work(ibdev->sriov.demux[i].ud_wq, &dm[i]->work);\n\t\tspin_unlock_irqrestore(&ibdev->sriov.going_down_lock, flags);\n\t} else {\n\t\tspin_unlock_irqrestore(&ibdev->sriov.going_down_lock, flags);\n\t\tfor (i = 0; i < ports; i++)\n\t\t\tkfree(dm[i]);\n\t}\nout:\n\tkfree(dm);\n\treturn;\n}\n\nstatic void mlx4_ib_handle_catas_error(struct mlx4_ib_dev *ibdev)\n{\n\tstruct mlx4_ib_qp *mqp;\n\tunsigned long flags_qp;\n\tunsigned long flags_cq;\n\tstruct mlx4_ib_cq *send_mcq, *recv_mcq;\n\tstruct list_head    cq_notify_list;\n\tstruct mlx4_cq *mcq;\n\tunsigned long flags;\n\n\tpr_warn(\"mlx4_ib_handle_catas_error was started\\n\");\n\tINIT_LIST_HEAD(&cq_notify_list);\n\n\t \n\tspin_lock_irqsave(&ibdev->reset_flow_resource_lock, flags);\n\n\tlist_for_each_entry(mqp, &ibdev->qp_list, qps_list) {\n\t\tspin_lock_irqsave(&mqp->sq.lock, flags_qp);\n\t\tif (mqp->sq.tail != mqp->sq.head) {\n\t\t\tsend_mcq = to_mcq(mqp->ibqp.send_cq);\n\t\t\tspin_lock_irqsave(&send_mcq->lock, flags_cq);\n\t\t\tif (send_mcq->mcq.comp &&\n\t\t\t    mqp->ibqp.send_cq->comp_handler) {\n\t\t\t\tif (!send_mcq->mcq.reset_notify_added) {\n\t\t\t\t\tsend_mcq->mcq.reset_notify_added = 1;\n\t\t\t\t\tlist_add_tail(&send_mcq->mcq.reset_notify,\n\t\t\t\t\t\t      &cq_notify_list);\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&send_mcq->lock, flags_cq);\n\t\t}\n\t\tspin_unlock_irqrestore(&mqp->sq.lock, flags_qp);\n\t\t \n\t\tspin_lock_irqsave(&mqp->rq.lock, flags_qp);\n\t\t \n\t\tif (!mqp->ibqp.srq) {\n\t\t\tif (mqp->rq.tail != mqp->rq.head) {\n\t\t\t\trecv_mcq = to_mcq(mqp->ibqp.recv_cq);\n\t\t\t\tspin_lock_irqsave(&recv_mcq->lock, flags_cq);\n\t\t\t\tif (recv_mcq->mcq.comp &&\n\t\t\t\t    mqp->ibqp.recv_cq->comp_handler) {\n\t\t\t\t\tif (!recv_mcq->mcq.reset_notify_added) {\n\t\t\t\t\t\trecv_mcq->mcq.reset_notify_added = 1;\n\t\t\t\t\t\tlist_add_tail(&recv_mcq->mcq.reset_notify,\n\t\t\t\t\t\t\t      &cq_notify_list);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tspin_unlock_irqrestore(&recv_mcq->lock,\n\t\t\t\t\t\t       flags_cq);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&mqp->rq.lock, flags_qp);\n\t}\n\n\tlist_for_each_entry(mcq, &cq_notify_list, reset_notify) {\n\t\tmcq->comp(mcq);\n\t}\n\tspin_unlock_irqrestore(&ibdev->reset_flow_resource_lock, flags);\n\tpr_warn(\"mlx4_ib_handle_catas_error ended\\n\");\n}\n\nstatic void handle_bonded_port_state_event(struct work_struct *work)\n{\n\tstruct ib_event_work *ew =\n\t\tcontainer_of(work, struct ib_event_work, work);\n\tstruct mlx4_ib_dev *ibdev = ew->ib_dev;\n\tenum ib_port_state bonded_port_state = IB_PORT_NOP;\n\tint i;\n\tstruct ib_event ibev;\n\n\tkfree(ew);\n\tspin_lock_bh(&ibdev->iboe.lock);\n\tfor (i = 0; i < MLX4_MAX_PORTS; ++i) {\n\t\tstruct net_device *curr_netdev = ibdev->iboe.netdevs[i];\n\t\tenum ib_port_state curr_port_state;\n\n\t\tif (!curr_netdev)\n\t\t\tcontinue;\n\n\t\tcurr_port_state =\n\t\t\t(netif_running(curr_netdev) &&\n\t\t\t netif_carrier_ok(curr_netdev)) ?\n\t\t\tIB_PORT_ACTIVE : IB_PORT_DOWN;\n\n\t\tbonded_port_state = (bonded_port_state != IB_PORT_ACTIVE) ?\n\t\t\tcurr_port_state : IB_PORT_ACTIVE;\n\t}\n\tspin_unlock_bh(&ibdev->iboe.lock);\n\n\tibev.device = &ibdev->ib_dev;\n\tibev.element.port_num = 1;\n\tibev.event = (bonded_port_state == IB_PORT_ACTIVE) ?\n\t\tIB_EVENT_PORT_ACTIVE : IB_EVENT_PORT_ERR;\n\n\tib_dispatch_event(&ibev);\n}\n\nvoid mlx4_ib_sl2vl_update(struct mlx4_ib_dev *mdev, int port)\n{\n\tu64 sl2vl;\n\tint err;\n\n\terr = mlx4_ib_query_sl2vl(&mdev->ib_dev, port, &sl2vl);\n\tif (err) {\n\t\tpr_err(\"Unable to get current sl to vl mapping for port %d.  Using all zeroes (%d)\\n\",\n\t\t       port, err);\n\t\tsl2vl = 0;\n\t}\n\tatomic64_set(&mdev->sl2vl[port - 1], sl2vl);\n}\n\nstatic void ib_sl2vl_update_work(struct work_struct *work)\n{\n\tstruct ib_event_work *ew = container_of(work, struct ib_event_work, work);\n\tstruct mlx4_ib_dev *mdev = ew->ib_dev;\n\tint port = ew->port;\n\n\tmlx4_ib_sl2vl_update(mdev, port);\n\n\tkfree(ew);\n}\n\nvoid mlx4_sched_ib_sl2vl_update_work(struct mlx4_ib_dev *ibdev,\n\t\t\t\t     int port)\n{\n\tstruct ib_event_work *ew;\n\n\tew = kmalloc(sizeof(*ew), GFP_ATOMIC);\n\tif (ew) {\n\t\tINIT_WORK(&ew->work, ib_sl2vl_update_work);\n\t\tew->port = port;\n\t\tew->ib_dev = ibdev;\n\t\tqueue_work(wq, &ew->work);\n\t}\n}\n\nstatic int mlx4_ib_event(struct notifier_block *this, unsigned long event,\n\t\t\t void *param)\n{\n\tstruct mlx4_ib_dev *ibdev =\n\t\tcontainer_of(this, struct mlx4_ib_dev, mlx_nb);\n\tstruct mlx4_dev *dev = ibdev->dev;\n\tstruct ib_event ibev;\n\tstruct mlx4_eqe *eqe = NULL;\n\tstruct ib_event_work *ew;\n\tint p = 0;\n\n\tif (mlx4_is_bonded(dev) &&\n\t    ((event == MLX4_DEV_EVENT_PORT_UP) ||\n\t    (event == MLX4_DEV_EVENT_PORT_DOWN))) {\n\t\tew = kmalloc(sizeof(*ew), GFP_ATOMIC);\n\t\tif (!ew)\n\t\t\treturn NOTIFY_DONE;\n\t\tINIT_WORK(&ew->work, handle_bonded_port_state_event);\n\t\tew->ib_dev = ibdev;\n\t\tqueue_work(wq, &ew->work);\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tswitch (event) {\n\tcase MLX4_DEV_EVENT_CATASTROPHIC_ERROR:\n\t\tbreak;\n\tcase MLX4_DEV_EVENT_PORT_MGMT_CHANGE:\n\t\teqe = (struct mlx4_eqe *)param;\n\t\tbreak;\n\tdefault:\n\t\tp = *(int *)param;\n\t\tbreak;\n\t}\n\n\tswitch (event) {\n\tcase MLX4_DEV_EVENT_PORT_UP:\n\t\tif (p > ibdev->num_ports)\n\t\t\treturn NOTIFY_DONE;\n\t\tif (!mlx4_is_slave(dev) &&\n\t\t    rdma_port_get_link_layer(&ibdev->ib_dev, p) ==\n\t\t\tIB_LINK_LAYER_INFINIBAND) {\n\t\t\tif (mlx4_is_master(dev))\n\t\t\t\tmlx4_ib_invalidate_all_guid_record(ibdev, p);\n\t\t\tif (ibdev->dev->flags & MLX4_FLAG_SECURE_HOST &&\n\t\t\t    !(ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SL_TO_VL_CHANGE_EVENT))\n\t\t\t\tmlx4_sched_ib_sl2vl_update_work(ibdev, p);\n\t\t}\n\t\tibev.event = IB_EVENT_PORT_ACTIVE;\n\t\tbreak;\n\n\tcase MLX4_DEV_EVENT_PORT_DOWN:\n\t\tif (p > ibdev->num_ports)\n\t\t\treturn NOTIFY_DONE;\n\t\tibev.event = IB_EVENT_PORT_ERR;\n\t\tbreak;\n\n\tcase MLX4_DEV_EVENT_CATASTROPHIC_ERROR:\n\t\tibdev->ib_active = false;\n\t\tibev.event = IB_EVENT_DEVICE_FATAL;\n\t\tmlx4_ib_handle_catas_error(ibdev);\n\t\tbreak;\n\n\tcase MLX4_DEV_EVENT_PORT_MGMT_CHANGE:\n\t\tew = kmalloc(sizeof *ew, GFP_ATOMIC);\n\t\tif (!ew)\n\t\t\treturn NOTIFY_DONE;\n\n\t\tINIT_WORK(&ew->work, handle_port_mgmt_change_event);\n\t\tmemcpy(&ew->ib_eqe, eqe, sizeof *eqe);\n\t\tew->ib_dev = ibdev;\n\t\t \n\t\tif (mlx4_is_master(dev))\n\t\t\tqueue_work(wq, &ew->work);\n\t\telse\n\t\t\thandle_port_mgmt_change_event(&ew->work);\n\t\treturn NOTIFY_DONE;\n\n\tcase MLX4_DEV_EVENT_SLAVE_INIT:\n\t\t \n\t\tdo_slave_init(ibdev, p, 1);\n\t\tif (mlx4_is_master(dev)) {\n\t\t\tint i;\n\n\t\t\tfor (i = 1; i <= ibdev->num_ports; i++) {\n\t\t\t\tif (rdma_port_get_link_layer(&ibdev->ib_dev, i)\n\t\t\t\t\t== IB_LINK_LAYER_INFINIBAND)\n\t\t\t\t\tmlx4_ib_slave_alias_guid_event(ibdev,\n\t\t\t\t\t\t\t\t       p, i,\n\t\t\t\t\t\t\t\t       1);\n\t\t\t}\n\t\t}\n\t\treturn NOTIFY_DONE;\n\n\tcase MLX4_DEV_EVENT_SLAVE_SHUTDOWN:\n\t\tif (mlx4_is_master(dev)) {\n\t\t\tint i;\n\n\t\t\tfor (i = 1; i <= ibdev->num_ports; i++) {\n\t\t\t\tif (rdma_port_get_link_layer(&ibdev->ib_dev, i)\n\t\t\t\t\t== IB_LINK_LAYER_INFINIBAND)\n\t\t\t\t\tmlx4_ib_slave_alias_guid_event(ibdev,\n\t\t\t\t\t\t\t\t       p, i,\n\t\t\t\t\t\t\t\t       0);\n\t\t\t}\n\t\t}\n\t\t \n\t\tdo_slave_init(ibdev, p, 0);\n\t\treturn NOTIFY_DONE;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tibev.device\t      = &ibdev->ib_dev;\n\tibev.element.port_num = mlx4_is_bonded(ibdev->dev) ? 1 : (u8)p;\n\n\tib_dispatch_event(&ibev);\n\treturn NOTIFY_DONE;\n}\n\nstatic const struct auxiliary_device_id mlx4_ib_id_table[] = {\n\t{ .name = MLX4_ADEV_NAME \".ib\" },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, mlx4_ib_id_table);\n\nstatic struct mlx4_adrv mlx4_ib_adrv = {\n\t.adrv = {\n\t\t.name\t= \"ib\",\n\t\t.probe\t= mlx4_ib_probe,\n\t\t.remove\t= mlx4_ib_remove,\n\t\t.id_table = mlx4_ib_id_table,\n\t},\n\t.protocol\t= MLX4_PROT_IB_IPV6,\n\t.flags\t\t= MLX4_INTFF_BONDING\n};\n\nstatic int __init mlx4_ib_init(void)\n{\n\tint err;\n\n\twq = alloc_ordered_workqueue(\"mlx4_ib\", WQ_MEM_RECLAIM);\n\tif (!wq)\n\t\treturn -ENOMEM;\n\n\terr = mlx4_ib_qp_event_init();\n\tif (err)\n\t\tgoto clean_qp_event;\n\n\terr = mlx4_ib_cm_init();\n\tif (err)\n\t\tgoto clean_wq;\n\n\terr = mlx4_ib_mcg_init();\n\tif (err)\n\t\tgoto clean_cm;\n\n\terr = mlx4_register_auxiliary_driver(&mlx4_ib_adrv);\n\tif (err)\n\t\tgoto clean_mcg;\n\n\treturn 0;\n\nclean_mcg:\n\tmlx4_ib_mcg_destroy();\n\nclean_cm:\n\tmlx4_ib_cm_destroy();\n\nclean_wq:\n\tmlx4_ib_qp_event_cleanup();\n\nclean_qp_event:\n\tdestroy_workqueue(wq);\n\treturn err;\n}\n\nstatic void __exit mlx4_ib_cleanup(void)\n{\n\tmlx4_unregister_auxiliary_driver(&mlx4_ib_adrv);\n\tmlx4_ib_mcg_destroy();\n\tmlx4_ib_cm_destroy();\n\tmlx4_ib_qp_event_cleanup();\n\tdestroy_workqueue(wq);\n}\n\nmodule_init(mlx4_ib_init);\nmodule_exit(mlx4_ib_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}