{
  "module_name": "mr.c",
  "hash_id": "4f0411d5f161151b4f3422b46a9c624f69a342102297e6d1cf04e77519b707fd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/mr.c",
  "human_readable_source": " \n\n#include <linux/slab.h>\n#include <rdma/ib_user_verbs.h>\n\n#include \"mlx4_ib.h\"\n\nstatic u32 convert_access(int acc)\n{\n\treturn (acc & IB_ACCESS_REMOTE_ATOMIC ? MLX4_PERM_ATOMIC       : 0) |\n\t       (acc & IB_ACCESS_REMOTE_WRITE  ? MLX4_PERM_REMOTE_WRITE : 0) |\n\t       (acc & IB_ACCESS_REMOTE_READ   ? MLX4_PERM_REMOTE_READ  : 0) |\n\t       (acc & IB_ACCESS_LOCAL_WRITE   ? MLX4_PERM_LOCAL_WRITE  : 0) |\n\t       (acc & IB_ACCESS_MW_BIND\t      ? MLX4_PERM_BIND_MW      : 0) |\n\t       MLX4_PERM_LOCAL_READ;\n}\n\nstatic enum mlx4_mw_type to_mlx4_type(enum ib_mw_type type)\n{\n\tswitch (type) {\n\tcase IB_MW_TYPE_1:\treturn MLX4_MW_TYPE_1;\n\tcase IB_MW_TYPE_2:\treturn MLX4_MW_TYPE_2;\n\tdefault:\t\treturn -1;\n\t}\n}\n\nstruct ib_mr *mlx4_ib_get_dma_mr(struct ib_pd *pd, int acc)\n{\n\tstruct mlx4_ib_mr *mr;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mlx4_mr_alloc(to_mdev(pd->device)->dev, to_mpd(pd)->pdn, 0,\n\t\t\t    ~0ull, convert_access(acc), 0, 0, &mr->mmr);\n\tif (err)\n\t\tgoto err_free;\n\n\terr = mlx4_mr_enable(to_mdev(pd->device)->dev, &mr->mmr);\n\tif (err)\n\t\tgoto err_mr;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->mmr.key;\n\tmr->umem = NULL;\n\n\treturn &mr->ibmr;\n\nerr_mr:\n\t(void) mlx4_mr_free(to_mdev(pd->device)->dev, &mr->mmr);\n\nerr_free:\n\tkfree(mr);\n\n\treturn ERR_PTR(err);\n}\n\nenum {\n\tMLX4_MAX_MTT_SHIFT = 31\n};\n\nstatic int mlx4_ib_umem_write_mtt_block(struct mlx4_ib_dev *dev,\n\t\t\t\t\tstruct mlx4_mtt *mtt,\n\t\t\t\t\tu64 mtt_size, u64 mtt_shift, u64 len,\n\t\t\t\t\tu64 cur_start_addr, u64 *pages,\n\t\t\t\t\tint *start_index, int *npages)\n{\n\tu64 cur_end_addr = cur_start_addr + len;\n\tu64 cur_end_addr_aligned = 0;\n\tu64 mtt_entries;\n\tint err = 0;\n\tint k;\n\n\tlen += (cur_start_addr & (mtt_size - 1ULL));\n\tcur_end_addr_aligned = round_up(cur_end_addr, mtt_size);\n\tlen += (cur_end_addr_aligned - cur_end_addr);\n\tif (len & (mtt_size - 1ULL)) {\n\t\tpr_warn(\"write_block: len %llx is not aligned to mtt_size %llx\\n\",\n\t\t\tlen, mtt_size);\n\t\treturn -EINVAL;\n\t}\n\n\tmtt_entries = (len >> mtt_shift);\n\n\t \n\tcur_start_addr = round_down(cur_start_addr, mtt_size);\n\t \n\tfor (k = 0; k < mtt_entries; ++k) {\n\t\tpages[*npages] = cur_start_addr + (mtt_size * k);\n\t\t(*npages)++;\n\t\t \n\t\tif (*npages == PAGE_SIZE / sizeof(u64)) {\n\t\t\terr = mlx4_write_mtt(dev->dev, mtt, *start_index,\n\t\t\t\t\t     *npages, pages);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t(*start_index) += *npages;\n\t\t\t*npages = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic inline u64 alignment_of(u64 ptr)\n{\n\treturn ilog2(ptr & (~(ptr - 1)));\n}\n\nstatic int mlx4_ib_umem_calc_block_mtt(u64 next_block_start,\n\t\t\t\t       u64 current_block_end,\n\t\t\t\t       u64 block_shift)\n{\n\t \n\tif ((next_block_start & ((1ULL << block_shift) - 1ULL)) != 0)\n\t\t \n\t\tblock_shift = alignment_of(next_block_start);\n\n\t \n\tif (((current_block_end) & ((1ULL << block_shift) - 1ULL)) != 0)\n\t\t \n\t\tblock_shift = alignment_of(current_block_end);\n\n\treturn block_shift;\n}\n\nint mlx4_ib_umem_write_mtt(struct mlx4_ib_dev *dev, struct mlx4_mtt *mtt,\n\t\t\t   struct ib_umem *umem)\n{\n\tu64 *pages;\n\tu64 len = 0;\n\tint err = 0;\n\tu64 mtt_size;\n\tu64 cur_start_addr = 0;\n\tu64 mtt_shift;\n\tint start_index = 0;\n\tint npages = 0;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tpages = (u64 *) __get_free_page(GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tmtt_shift = mtt->page_shift;\n\tmtt_size = 1ULL << mtt_shift;\n\n\tfor_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i) {\n\t\tif (cur_start_addr + len == sg_dma_address(sg)) {\n\t\t\t \n\t\t\tlen += sg_dma_len(sg);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\terr = mlx4_ib_umem_write_mtt_block(dev, mtt, mtt_size,\n\t\t\t\t\t\t   mtt_shift, len,\n\t\t\t\t\t\t   cur_start_addr,\n\t\t\t\t\t\t   pages, &start_index,\n\t\t\t\t\t\t   &npages);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tcur_start_addr = sg_dma_address(sg);\n\t\tlen = sg_dma_len(sg);\n\t}\n\n\t \n\tif (len > 0) {\n\t\t \n\t\terr = mlx4_ib_umem_write_mtt_block(dev, mtt, mtt_size,\n\t\t\t\t\t\t   mtt_shift, len,\n\t\t\t\t\t\t   cur_start_addr, pages,\n\t\t\t\t\t\t   &start_index, &npages);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (npages)\n\t\terr = mlx4_write_mtt(dev->dev, mtt, start_index, npages, pages);\n\nout:\n\tfree_page((unsigned long) pages);\n\treturn err;\n}\n\n \nint mlx4_ib_umem_calc_optimal_mtt_size(struct ib_umem *umem, u64 start_va,\n\t\t\t\t       int *num_of_mtts)\n{\n\tu64 block_shift = MLX4_MAX_MTT_SHIFT;\n\tu64 min_shift = PAGE_SHIFT;\n\tu64 last_block_aligned_end = 0;\n\tu64 current_block_start = 0;\n\tu64 first_block_start = 0;\n\tu64 current_block_len = 0;\n\tu64 last_block_end = 0;\n\tstruct scatterlist *sg;\n\tu64 current_block_end;\n\tu64 misalignment_bits;\n\tu64 next_block_start;\n\tu64 total_len = 0;\n\tint i;\n\n\t*num_of_mtts = ib_umem_num_dma_blocks(umem, PAGE_SIZE);\n\n\tfor_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i) {\n\t\t \n\t\tif (current_block_len == 0 && current_block_start == 0) {\n\t\t\tcurrent_block_start = sg_dma_address(sg);\n\t\t\tfirst_block_start = current_block_start;\n\t\t\t \n\t\t\tmisalignment_bits =\n\t\t\t\t(start_va & (~(((u64)(PAGE_SIZE)) - 1ULL))) ^\n\t\t\t\tcurrent_block_start;\n\t\t\tblock_shift = min(alignment_of(misalignment_bits),\n\t\t\t\t\t  block_shift);\n\t\t}\n\n\t\t \n\t\tnext_block_start = sg_dma_address(sg);\n\t\tcurrent_block_end = current_block_start\t+ current_block_len;\n\t\t \n\t\tif (current_block_end != next_block_start) {\n\t\t\tblock_shift = mlx4_ib_umem_calc_block_mtt\n\t\t\t\t\t(next_block_start,\n\t\t\t\t\t current_block_end,\n\t\t\t\t\t block_shift);\n\n\t\t\t \n\t\t\tif (block_shift <= min_shift)\n\t\t\t\tgoto end;\n\n\t\t\t \n\t\t\ttotal_len += current_block_len;\n\n\t\t\t \n\t\t\tcurrent_block_start = next_block_start;\n\t\t\tcurrent_block_len = sg_dma_len(sg);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tcurrent_block_len += sg_dma_len(sg);\n\t}\n\n\t \n\ttotal_len += current_block_len;\n\t \n\ttotal_len += (first_block_start & ((1ULL << block_shift) - 1ULL));\n\tlast_block_end = current_block_start + current_block_len;\n\tlast_block_aligned_end = round_up(last_block_end, 1ULL << block_shift);\n\ttotal_len += (last_block_aligned_end - last_block_end);\n\n\tif (total_len & ((1ULL << block_shift) - 1ULL))\n\t\tpr_warn(\"misaligned total length detected (%llu, %llu)!\",\n\t\t\ttotal_len, block_shift);\n\n\t*num_of_mtts = total_len >> block_shift;\nend:\n\tif (block_shift < min_shift) {\n\t\t \n\t\tpr_warn(\"umem_calc_optimal_mtt_size - unexpected shift %lld\\n\", block_shift);\n\n\t\tblock_shift = min_shift;\n\t}\n\treturn block_shift;\n}\n\nstatic struct ib_umem *mlx4_get_umem_mr(struct ib_device *device, u64 start,\n\t\t\t\t\tu64 length, int access_flags)\n{\n\t \n\tif (!ib_access_writable(access_flags)) {\n\t\tunsigned long untagged_start = untagged_addr(start);\n\t\tstruct vm_area_struct *vma;\n\n\t\tmmap_read_lock(current->mm);\n\t\t \n\t\tvma = find_vma(current->mm, untagged_start);\n\t\tif (vma && vma->vm_end >= untagged_start + length &&\n\t\t    vma->vm_start <= untagged_start) {\n\t\t\tif (vma->vm_flags & VM_WRITE)\n\t\t\t\taccess_flags |= IB_ACCESS_LOCAL_WRITE;\n\t\t} else {\n\t\t\taccess_flags |= IB_ACCESS_LOCAL_WRITE;\n\t\t}\n\n\t\tmmap_read_unlock(current->mm);\n\t}\n\n\treturn ib_umem_get(device, start, length, access_flags);\n}\n\nstruct ib_mr *mlx4_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t\t  u64 virt_addr, int access_flags,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx4_ib_mr *mr;\n\tint shift;\n\tint err;\n\tint n;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->umem = mlx4_get_umem_mr(pd->device, start, length, access_flags);\n\tif (IS_ERR(mr->umem)) {\n\t\terr = PTR_ERR(mr->umem);\n\t\tgoto err_free;\n\t}\n\n\tshift = mlx4_ib_umem_calc_optimal_mtt_size(mr->umem, start, &n);\n\n\terr = mlx4_mr_alloc(dev->dev, to_mpd(pd)->pdn, virt_addr, length,\n\t\t\t    convert_access(access_flags), n, shift, &mr->mmr);\n\tif (err)\n\t\tgoto err_umem;\n\n\terr = mlx4_ib_umem_write_mtt(dev, &mr->mmr.mtt, mr->umem);\n\tif (err)\n\t\tgoto err_mr;\n\n\terr = mlx4_mr_enable(dev->dev, &mr->mmr);\n\tif (err)\n\t\tgoto err_mr;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->mmr.key;\n\tmr->ibmr.page_size = 1U << shift;\n\n\treturn &mr->ibmr;\n\nerr_mr:\n\t(void) mlx4_mr_free(to_mdev(pd->device)->dev, &mr->mmr);\n\nerr_umem:\n\tib_umem_release(mr->umem);\n\nerr_free:\n\tkfree(mr);\n\n\treturn ERR_PTR(err);\n}\n\nstruct ib_mr *mlx4_ib_rereg_user_mr(struct ib_mr *mr, int flags, u64 start,\n\t\t\t\t    u64 length, u64 virt_addr,\n\t\t\t\t    int mr_access_flags, struct ib_pd *pd,\n\t\t\t\t    struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(mr->device);\n\tstruct mlx4_ib_mr *mmr = to_mmr(mr);\n\tstruct mlx4_mpt_entry *mpt_entry;\n\tstruct mlx4_mpt_entry **pmpt_entry = &mpt_entry;\n\tint err;\n\n\t \n\terr =  mlx4_mr_hw_get_mpt(dev->dev, &mmr->mmr, &pmpt_entry);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tif (flags & IB_MR_REREG_PD) {\n\t\terr = mlx4_mr_hw_change_pd(dev->dev, *pmpt_entry,\n\t\t\t\t\t   to_mpd(pd)->pdn);\n\n\t\tif (err)\n\t\t\tgoto release_mpt_entry;\n\t}\n\n\tif (flags & IB_MR_REREG_ACCESS) {\n\t\tif (ib_access_writable(mr_access_flags) &&\n\t\t    !mmr->umem->writable) {\n\t\t\terr = -EPERM;\n\t\t\tgoto release_mpt_entry;\n\t\t}\n\n\t\terr = mlx4_mr_hw_change_access(dev->dev, *pmpt_entry,\n\t\t\t\t\t       convert_access(mr_access_flags));\n\n\t\tif (err)\n\t\t\tgoto release_mpt_entry;\n\t}\n\n\tif (flags & IB_MR_REREG_TRANS) {\n\t\tint shift;\n\t\tint n;\n\n\t\tmlx4_mr_rereg_mem_cleanup(dev->dev, &mmr->mmr);\n\t\tib_umem_release(mmr->umem);\n\t\tmmr->umem = mlx4_get_umem_mr(mr->device, start, length,\n\t\t\t\t\t     mr_access_flags);\n\t\tif (IS_ERR(mmr->umem)) {\n\t\t\terr = PTR_ERR(mmr->umem);\n\t\t\t \n\t\t\tmmr->umem = NULL;\n\t\t\tgoto release_mpt_entry;\n\t\t}\n\t\tn = ib_umem_num_dma_blocks(mmr->umem, PAGE_SIZE);\n\t\tshift = PAGE_SHIFT;\n\n\t\terr = mlx4_mr_rereg_mem_write(dev->dev, &mmr->mmr,\n\t\t\t\t\t      virt_addr, length, n, shift,\n\t\t\t\t\t      *pmpt_entry);\n\t\tif (err) {\n\t\t\tib_umem_release(mmr->umem);\n\t\t\tgoto release_mpt_entry;\n\t\t}\n\t\tmmr->mmr.iova       = virt_addr;\n\t\tmmr->mmr.size       = length;\n\n\t\terr = mlx4_ib_umem_write_mtt(dev, &mmr->mmr.mtt, mmr->umem);\n\t\tif (err) {\n\t\t\tmlx4_mr_rereg_mem_cleanup(dev->dev, &mmr->mmr);\n\t\t\tib_umem_release(mmr->umem);\n\t\t\tgoto release_mpt_entry;\n\t\t}\n\t}\n\n\t \n\terr = mlx4_mr_hw_write_mpt(dev->dev, &mmr->mmr, pmpt_entry);\n\tif (!err && flags & IB_MR_REREG_ACCESS)\n\t\tmmr->mmr.access = mr_access_flags;\n\nrelease_mpt_entry:\n\tmlx4_mr_hw_put_mpt(dev->dev, pmpt_entry);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\treturn NULL;\n}\n\nstatic int\nmlx4_alloc_priv_pages(struct ib_device *device,\n\t\t      struct mlx4_ib_mr *mr,\n\t\t      int max_pages)\n{\n\tint ret;\n\n\t \n\tmr->page_map_size = roundup(max_pages * sizeof(u64),\n\t\t\t\t    MLX4_MR_PAGES_ALIGN);\n\n\t \n\tmr->pages = (__be64 *)get_zeroed_page(GFP_KERNEL);\n\tif (!mr->pages)\n\t\treturn -ENOMEM;\n\n\tmr->page_map = dma_map_single(device->dev.parent, mr->pages,\n\t\t\t\t      mr->page_map_size, DMA_TO_DEVICE);\n\n\tif (dma_mapping_error(device->dev.parent, mr->page_map)) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfree_page((unsigned long)mr->pages);\n\treturn ret;\n}\n\nstatic void\nmlx4_free_priv_pages(struct mlx4_ib_mr *mr)\n{\n\tif (mr->pages) {\n\t\tstruct ib_device *device = mr->ibmr.device;\n\n\t\tdma_unmap_single(device->dev.parent, mr->page_map,\n\t\t\t\t mr->page_map_size, DMA_TO_DEVICE);\n\t\tfree_page((unsigned long)mr->pages);\n\t\tmr->pages = NULL;\n\t}\n}\n\nint mlx4_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_mr *mr = to_mmr(ibmr);\n\tint ret;\n\n\tmlx4_free_priv_pages(mr);\n\n\tret = mlx4_mr_free(to_mdev(ibmr->device)->dev, &mr->mmr);\n\tif (ret)\n\t\treturn ret;\n\tif (mr->umem)\n\t\tib_umem_release(mr->umem);\n\tkfree(mr);\n\n\treturn 0;\n}\n\nint mlx4_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibmw->device);\n\tstruct mlx4_ib_mw *mw = to_mmw(ibmw);\n\tint err;\n\n\terr = mlx4_mw_alloc(dev->dev, to_mpd(ibmw->pd)->pdn,\n\t\t\t    to_mlx4_type(ibmw->type), &mw->mmw);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx4_mw_enable(dev->dev, &mw->mmw);\n\tif (err)\n\t\tgoto err_mw;\n\n\tibmw->rkey = mw->mmw.key;\n\treturn 0;\n\nerr_mw:\n\tmlx4_mw_free(dev->dev, &mw->mmw);\n\treturn err;\n}\n\nint mlx4_ib_dealloc_mw(struct ib_mw *ibmw)\n{\n\tstruct mlx4_ib_mw *mw = to_mmw(ibmw);\n\n\tmlx4_mw_free(to_mdev(ibmw->device)->dev, &mw->mmw);\n\treturn 0;\n}\n\nstruct ib_mr *mlx4_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t       u32 max_num_sg)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx4_ib_mr *mr;\n\tint err;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG ||\n\t    max_num_sg > MLX4_MAX_FAST_REG_PAGES)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mlx4_mr_alloc(dev->dev, to_mpd(pd)->pdn, 0, 0, 0,\n\t\t\t    max_num_sg, 0, &mr->mmr);\n\tif (err)\n\t\tgoto err_free;\n\n\terr = mlx4_alloc_priv_pages(pd->device, mr, max_num_sg);\n\tif (err)\n\t\tgoto err_free_mr;\n\n\tmr->max_pages = max_num_sg;\n\terr = mlx4_mr_enable(dev->dev, &mr->mmr);\n\tif (err)\n\t\tgoto err_free_pl;\n\n\tmr->ibmr.rkey = mr->ibmr.lkey = mr->mmr.key;\n\tmr->umem = NULL;\n\n\treturn &mr->ibmr;\n\nerr_free_pl:\n\tmr->ibmr.device = pd->device;\n\tmlx4_free_priv_pages(mr);\nerr_free_mr:\n\t(void) mlx4_mr_free(dev->dev, &mr->mmr);\nerr_free:\n\tkfree(mr);\n\treturn ERR_PTR(err);\n}\n\nstatic int mlx4_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct mlx4_ib_mr *mr = to_mmr(ibmr);\n\n\tif (unlikely(mr->npages == mr->max_pages))\n\t\treturn -ENOMEM;\n\n\tmr->pages[mr->npages++] = cpu_to_be64(addr | MLX4_MTT_FLAG_PRESENT);\n\n\treturn 0;\n}\n\nint mlx4_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t      unsigned int *sg_offset)\n{\n\tstruct mlx4_ib_mr *mr = to_mmr(ibmr);\n\tint rc;\n\n\tmr->npages = 0;\n\n\tib_dma_sync_single_for_cpu(ibmr->device, mr->page_map,\n\t\t\t\t   mr->page_map_size, DMA_TO_DEVICE);\n\n\trc = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, mlx4_set_page);\n\n\tib_dma_sync_single_for_device(ibmr->device, mr->page_map,\n\t\t\t\t      mr->page_map_size, DMA_TO_DEVICE);\n\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}