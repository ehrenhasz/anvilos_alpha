{
  "module_name": "mcg.c",
  "hash_id": "721b6bd4fe800fd451aec22ed74ff1e9af0b6bec650f33926d5ca02f33cccc8f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/mcg.c",
  "human_readable_source": " \n\n#include <rdma/ib_mad.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_cache.h>\n#include <rdma/ib_sa.h>\n\n#include <linux/mlx4/cmd.h>\n#include <linux/rbtree.h>\n#include <linux/delay.h>\n\n#include \"mlx4_ib.h\"\n\n#define MAX_VFS\t\t80\n#define MAX_PEND_REQS_PER_FUNC 4\n#define MAD_TIMEOUT_MS\t2000\n\n#define mcg_warn(fmt, arg...)\tpr_warn(\"MCG WARNING: \" fmt, ##arg)\n#define mcg_error(fmt, arg...)\tpr_err(fmt, ##arg)\n#define mcg_warn_group(group, format, arg...) \\\n\tpr_warn(\"%s-%d: %16s (port %d): WARNING: \" format, __func__, __LINE__,\\\n\t(group)->name, group->demux->port, ## arg)\n\n#define mcg_debug_group(group, format, arg...) \\\n\tpr_debug(\"%s-%d: %16s (port %d): WARNING: \" format, __func__, __LINE__,\\\n\t\t (group)->name, (group)->demux->port, ## arg)\n\n#define mcg_error_group(group, format, arg...) \\\n\tpr_err(\"  %16s: \" format, (group)->name, ## arg)\n\n\nstatic union ib_gid mgid0;\n\nstatic struct workqueue_struct *clean_wq;\n\nenum mcast_state {\n\tMCAST_NOT_MEMBER = 0,\n\tMCAST_MEMBER,\n};\n\nenum mcast_group_state {\n\tMCAST_IDLE,\n\tMCAST_JOIN_SENT,\n\tMCAST_LEAVE_SENT,\n\tMCAST_RESP_READY\n};\n\nstruct mcast_member {\n\tenum mcast_state state;\n\tuint8_t\t\t\tjoin_state;\n\tint\t\t\tnum_pend_reqs;\n\tstruct list_head\tpending;\n};\n\nstruct ib_sa_mcmember_data {\n\tunion ib_gid\tmgid;\n\tunion ib_gid\tport_gid;\n\t__be32\t\tqkey;\n\t__be16\t\tmlid;\n\tu8\t\tmtusel_mtu;\n\tu8\t\ttclass;\n\t__be16\t\tpkey;\n\tu8\t\tratesel_rate;\n\tu8\t\tlifetmsel_lifetm;\n\t__be32\t\tsl_flowlabel_hoplimit;\n\tu8\t\tscope_join_state;\n\tu8\t\tproxy_join;\n\tu8\t\treserved[2];\n} __packed __aligned(4);\n\nstruct mcast_group {\n\tstruct ib_sa_mcmember_data rec;\n\tstruct rb_node\t\tnode;\n\tstruct list_head\tmgid0_list;\n\tstruct mlx4_ib_demux_ctx *demux;\n\tstruct mcast_member\tfunc[MAX_VFS];\n\tstruct mutex\t\tlock;\n\tstruct work_struct\twork;\n\tstruct list_head\tpending_list;\n\tint\t\t\tmembers[3];\n\tenum mcast_group_state\tstate;\n\tenum mcast_group_state\tprev_state;\n\tstruct ib_sa_mad\tresponse_sa_mad;\n\t__be64\t\t\tlast_req_tid;\n\n\tchar\t\t\tname[33];  \n\tstruct device_attribute\tdentry;\n\n\t \n\tatomic_t\t\trefcount;\n\n\t \n\tstruct delayed_work\ttimeout_work;\n\tstruct list_head\tcleanup_list;\n};\n\nstruct mcast_req {\n\tint\t\t\tfunc;\n\tstruct ib_sa_mad\tsa_mad;\n\tstruct list_head\tgroup_list;\n\tstruct list_head\tfunc_list;\n\tstruct mcast_group\t*group;\n\tint\t\t\tclean;\n};\n\n\n#define safe_atomic_dec(ref) \\\n\tdo {\\\n\t\tif (atomic_dec_and_test(ref)) \\\n\t\t\tmcg_warn_group(group, \"did not expect to reach zero\\n\"); \\\n\t} while (0)\n\nstatic const char *get_state_string(enum mcast_group_state state)\n{\n\tswitch (state) {\n\tcase MCAST_IDLE:\n\t\treturn \"MCAST_IDLE\";\n\tcase MCAST_JOIN_SENT:\n\t\treturn \"MCAST_JOIN_SENT\";\n\tcase MCAST_LEAVE_SENT:\n\t\treturn \"MCAST_LEAVE_SENT\";\n\tcase MCAST_RESP_READY:\n\t\treturn \"MCAST_RESP_READY\";\n\t}\n\treturn \"Invalid State\";\n}\n\nstatic struct mcast_group *mcast_find(struct mlx4_ib_demux_ctx *ctx,\n\t\t\t\t      union ib_gid *mgid)\n{\n\tstruct rb_node *node = ctx->mcg_table.rb_node;\n\tstruct mcast_group *group;\n\tint ret;\n\n\twhile (node) {\n\t\tgroup = rb_entry(node, struct mcast_group, node);\n\t\tret = memcmp(mgid->raw, group->rec.mgid.raw, sizeof *mgid);\n\t\tif (!ret)\n\t\t\treturn group;\n\n\t\tif (ret < 0)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\t\tnode = node->rb_right;\n\t}\n\treturn NULL;\n}\n\nstatic struct mcast_group *mcast_insert(struct mlx4_ib_demux_ctx *ctx,\n\t\t\t\t\tstruct mcast_group *group)\n{\n\tstruct rb_node **link = &ctx->mcg_table.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct mcast_group *cur_group;\n\tint ret;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tcur_group = rb_entry(parent, struct mcast_group, node);\n\n\t\tret = memcmp(group->rec.mgid.raw, cur_group->rec.mgid.raw,\n\t\t\t     sizeof group->rec.mgid);\n\t\tif (ret < 0)\n\t\t\tlink = &(*link)->rb_left;\n\t\telse if (ret > 0)\n\t\t\tlink = &(*link)->rb_right;\n\t\telse\n\t\t\treturn cur_group;\n\t}\n\trb_link_node(&group->node, parent, link);\n\trb_insert_color(&group->node, &ctx->mcg_table);\n\treturn NULL;\n}\n\nstatic int send_mad_to_wire(struct mlx4_ib_demux_ctx *ctx, struct ib_mad *mad)\n{\n\tstruct mlx4_ib_dev *dev = ctx->dev;\n\tstruct rdma_ah_attr\tah_attr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->sm_lock, flags);\n\tif (!dev->sm_ah[ctx->port - 1]) {\n\t\t \n\t\tspin_unlock_irqrestore(&dev->sm_lock, flags);\n\t\treturn -EAGAIN;\n\t}\n\tmlx4_ib_query_ah(dev->sm_ah[ctx->port - 1], &ah_attr);\n\tspin_unlock_irqrestore(&dev->sm_lock, flags);\n\treturn mlx4_ib_send_to_wire(dev, mlx4_master_func_num(dev->dev),\n\t\t\t\t    ctx->port, IB_QPT_GSI, 0, 1, IB_QP1_QKEY,\n\t\t\t\t    &ah_attr, NULL, 0xffff, mad);\n}\n\nstatic int send_mad_to_slave(int slave, struct mlx4_ib_demux_ctx *ctx,\n\t\t\t     struct ib_mad *mad)\n{\n\tstruct mlx4_ib_dev *dev = ctx->dev;\n\tstruct ib_mad_agent *agent = dev->send_agent[ctx->port - 1][1];\n\tstruct ib_wc wc;\n\tstruct rdma_ah_attr ah_attr;\n\n\t \n\tif (!agent)\n\t\treturn -EAGAIN;\n\n\trdma_query_ah(dev->sm_ah[ctx->port - 1], &ah_attr);\n\n\tif (ib_find_cached_pkey(&dev->ib_dev, ctx->port, IB_DEFAULT_PKEY_FULL, &wc.pkey_index))\n\t\treturn -EINVAL;\n\twc.sl = 0;\n\twc.dlid_path_bits = 0;\n\twc.port_num = ctx->port;\n\twc.slid = rdma_ah_get_dlid(&ah_attr);   \n\twc.src_qp = 1;\n\treturn mlx4_ib_send_to_slave(dev, slave, ctx->port, IB_QPT_GSI, &wc, NULL, mad);\n}\n\nstatic int send_join_to_wire(struct mcast_group *group, struct ib_sa_mad *sa_mad)\n{\n\tstruct ib_sa_mad mad;\n\tstruct ib_sa_mcmember_data *sa_mad_data = (struct ib_sa_mcmember_data *)&mad.data;\n\tint ret;\n\n\t \n\tmemcpy(&mad, sa_mad, sizeof mad);\n\n\t \n\tsa_mad_data->port_gid.global.interface_id = group->demux->guid_cache[0];\n\n\t \n\tmad.mad_hdr.tid = mlx4_ib_get_new_demux_tid(group->demux);\n\tgroup->last_req_tid = mad.mad_hdr.tid;  \n\n\tret = send_mad_to_wire(group->demux, (struct ib_mad *)&mad);\n\t \n\tif (!ret) {\n\t\t \n\t\tqueue_delayed_work(group->demux->mcg_wq, &group->timeout_work,\n\t\t\t\tmsecs_to_jiffies(MAD_TIMEOUT_MS));\n\t}\n\n\treturn ret;\n}\n\nstatic int send_leave_to_wire(struct mcast_group *group, u8 join_state)\n{\n\tstruct ib_sa_mad mad;\n\tstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)&mad.data;\n\tint ret;\n\n\tmemset(&mad, 0, sizeof mad);\n\tmad.mad_hdr.base_version = 1;\n\tmad.mad_hdr.mgmt_class = IB_MGMT_CLASS_SUBN_ADM;\n\tmad.mad_hdr.class_version = 2;\n\tmad.mad_hdr.method = IB_SA_METHOD_DELETE;\n\tmad.mad_hdr.status = cpu_to_be16(0);\n\tmad.mad_hdr.class_specific = cpu_to_be16(0);\n\tmad.mad_hdr.tid = mlx4_ib_get_new_demux_tid(group->demux);\n\tgroup->last_req_tid = mad.mad_hdr.tid;  \n\tmad.mad_hdr.attr_id = cpu_to_be16(IB_SA_ATTR_MC_MEMBER_REC);\n\tmad.mad_hdr.attr_mod = cpu_to_be32(0);\n\tmad.sa_hdr.sm_key = 0x0;\n\tmad.sa_hdr.attr_offset = cpu_to_be16(7);\n\tmad.sa_hdr.comp_mask = IB_SA_MCMEMBER_REC_MGID |\n\t\tIB_SA_MCMEMBER_REC_PORT_GID | IB_SA_MCMEMBER_REC_JOIN_STATE;\n\n\t*sa_data = group->rec;\n\tsa_data->scope_join_state = join_state;\n\n\tret = send_mad_to_wire(group->demux, (struct ib_mad *)&mad);\n\tif (ret)\n\t\tgroup->state = MCAST_IDLE;\n\n\t \n\tif (!ret) {\n\t\t \n\t\tqueue_delayed_work(group->demux->mcg_wq, &group->timeout_work,\n\t\t\t\tmsecs_to_jiffies(MAD_TIMEOUT_MS));\n\t}\n\n\treturn ret;\n}\n\nstatic int send_reply_to_slave(int slave, struct mcast_group *group,\n\t\tstruct ib_sa_mad *req_sa_mad, u16 status)\n{\n\tstruct ib_sa_mad mad;\n\tstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)&mad.data;\n\tstruct ib_sa_mcmember_data *req_sa_data = (struct ib_sa_mcmember_data *)&req_sa_mad->data;\n\tint ret;\n\n\tmemset(&mad, 0, sizeof mad);\n\tmad.mad_hdr.base_version = 1;\n\tmad.mad_hdr.mgmt_class = IB_MGMT_CLASS_SUBN_ADM;\n\tmad.mad_hdr.class_version = 2;\n\tmad.mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\n\tmad.mad_hdr.status = cpu_to_be16(status);\n\tmad.mad_hdr.class_specific = cpu_to_be16(0);\n\tmad.mad_hdr.tid = req_sa_mad->mad_hdr.tid;\n\t*(u8 *)&mad.mad_hdr.tid = 0;  \n\tmad.mad_hdr.attr_id = cpu_to_be16(IB_SA_ATTR_MC_MEMBER_REC);\n\tmad.mad_hdr.attr_mod = cpu_to_be32(0);\n\tmad.sa_hdr.sm_key = req_sa_mad->sa_hdr.sm_key;\n\tmad.sa_hdr.attr_offset = cpu_to_be16(7);\n\tmad.sa_hdr.comp_mask = 0;  \n\n\t*sa_data = group->rec;\n\n\t \n\tsa_data->scope_join_state &= 0xf0;\n\tsa_data->scope_join_state |= (group->func[slave].join_state & 0x0f);\n\tmemcpy(&sa_data->port_gid, &req_sa_data->port_gid, sizeof req_sa_data->port_gid);\n\n\tret = send_mad_to_slave(slave, group->demux, (struct ib_mad *)&mad);\n\treturn ret;\n}\n\nstatic int check_selector(ib_sa_comp_mask comp_mask,\n\t\t\t  ib_sa_comp_mask selector_mask,\n\t\t\t  ib_sa_comp_mask value_mask,\n\t\t\t  u8 src_value, u8 dst_value)\n{\n\tint err;\n\tu8 selector = dst_value >> 6;\n\tdst_value &= 0x3f;\n\tsrc_value &= 0x3f;\n\n\tif (!(comp_mask & selector_mask) || !(comp_mask & value_mask))\n\t\treturn 0;\n\n\tswitch (selector) {\n\tcase IB_SA_GT:\n\t\terr = (src_value <= dst_value);\n\t\tbreak;\n\tcase IB_SA_LT:\n\t\terr = (src_value >= dst_value);\n\t\tbreak;\n\tcase IB_SA_EQ:\n\t\terr = (src_value != dst_value);\n\t\tbreak;\n\tdefault:\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic u16 cmp_rec(struct ib_sa_mcmember_data *src,\n\t\t   struct ib_sa_mcmember_data *dst, ib_sa_comp_mask comp_mask)\n{\n\t \n\t \n\t \n\n#define MAD_STATUS_REQ_INVALID 0x0200\n\tif (comp_mask & IB_SA_MCMEMBER_REC_QKEY && src->qkey != dst->qkey)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_MLID && src->mlid != dst->mlid)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (check_selector(comp_mask, IB_SA_MCMEMBER_REC_MTU_SELECTOR,\n\t\t\t\t IB_SA_MCMEMBER_REC_MTU,\n\t\t\t\t src->mtusel_mtu, dst->mtusel_mtu))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_TRAFFIC_CLASS &&\n\t    src->tclass != dst->tclass)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_PKEY && src->pkey != dst->pkey)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (check_selector(comp_mask, IB_SA_MCMEMBER_REC_RATE_SELECTOR,\n\t\t\t\t IB_SA_MCMEMBER_REC_RATE,\n\t\t\t\t src->ratesel_rate, dst->ratesel_rate))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (check_selector(comp_mask,\n\t\t\t\t IB_SA_MCMEMBER_REC_PACKET_LIFE_TIME_SELECTOR,\n\t\t\t\t IB_SA_MCMEMBER_REC_PACKET_LIFE_TIME,\n\t\t\t\t src->lifetmsel_lifetm, dst->lifetmsel_lifetm))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_SL &&\n\t\t\t(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0xf0000000) !=\n\t\t\t(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0xf0000000))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_FLOW_LABEL &&\n\t\t\t(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0x0fffff00) !=\n\t\t\t(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0x0fffff00))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_HOP_LIMIT &&\n\t\t\t(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0x000000ff) !=\n\t\t\t(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0x000000ff))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\tif (comp_mask & IB_SA_MCMEMBER_REC_SCOPE &&\n\t\t\t(src->scope_join_state & 0xf0) !=\n\t\t\t(dst->scope_join_state & 0xf0))\n\t\treturn MAD_STATUS_REQ_INVALID;\n\n\t \n\n\treturn 0;\n}\n\n \nstatic int release_group(struct mcast_group *group, int from_timeout_handler)\n{\n\tstruct mlx4_ib_demux_ctx *ctx = group->demux;\n\tint nzgroup;\n\n\tmutex_lock(&ctx->mcg_table_lock);\n\tmutex_lock(&group->lock);\n\tif (atomic_dec_and_test(&group->refcount)) {\n\t\tif (!from_timeout_handler) {\n\t\t\tif (group->state != MCAST_IDLE &&\n\t\t\t    !cancel_delayed_work(&group->timeout_work)) {\n\t\t\t\tatomic_inc(&group->refcount);\n\t\t\t\tmutex_unlock(&group->lock);\n\t\t\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tnzgroup = memcmp(&group->rec.mgid, &mgid0, sizeof mgid0);\n\t\tif (nzgroup)\n\t\t\tdel_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\n\t\tif (!list_empty(&group->pending_list))\n\t\t\tmcg_warn_group(group, \"releasing a group with non empty pending list\\n\");\n\t\tif (nzgroup)\n\t\t\trb_erase(&group->node, &ctx->mcg_table);\n\t\tlist_del_init(&group->mgid0_list);\n\t\tmutex_unlock(&group->lock);\n\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\tkfree(group);\n\t\treturn 1;\n\t} else {\n\t\tmutex_unlock(&group->lock);\n\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t}\n\treturn 0;\n}\n\nstatic void adjust_membership(struct mcast_group *group, u8 join_state, int inc)\n{\n\tint i;\n\n\tfor (i = 0; i < 3; i++, join_state >>= 1)\n\t\tif (join_state & 0x1)\n\t\t\tgroup->members[i] += inc;\n}\n\nstatic u8 get_leave_state(struct mcast_group *group)\n{\n\tu8 leave_state = 0;\n\tint i;\n\n\tfor (i = 0; i < 3; i++)\n\t\tif (!group->members[i])\n\t\t\tleave_state |= (1 << i);\n\n\treturn leave_state & (group->rec.scope_join_state & 0xf);\n}\n\nstatic int join_group(struct mcast_group *group, int slave, u8 join_mask)\n{\n\tint ret = 0;\n\tu8 join_state;\n\n\t \n\tjoin_state = join_mask & (~group->func[slave].join_state);\n\tadjust_membership(group, join_state, 1);\n\tgroup->func[slave].join_state |= join_state;\n\tif (group->func[slave].state != MCAST_MEMBER && join_state) {\n\t\tgroup->func[slave].state = MCAST_MEMBER;\n\t\tret = 1;\n\t}\n\treturn ret;\n}\n\nstatic int leave_group(struct mcast_group *group, int slave, u8 leave_state)\n{\n\tint ret = 0;\n\n\tadjust_membership(group, leave_state, -1);\n\tgroup->func[slave].join_state &= ~leave_state;\n\tif (!group->func[slave].join_state) {\n\t\tgroup->func[slave].state = MCAST_NOT_MEMBER;\n\t\tret = 1;\n\t}\n\treturn ret;\n}\n\nstatic int check_leave(struct mcast_group *group, int slave, u8 leave_mask)\n{\n\tif (group->func[slave].state != MCAST_MEMBER)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\n\t \n\tif (~group->func[slave].join_state & leave_mask)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\n\tif (!leave_mask)\n\t\treturn MAD_STATUS_REQ_INVALID;\n\n\treturn 0;\n}\n\nstatic void mlx4_ib_mcg_timeout_handler(struct work_struct *work)\n{\n\tstruct delayed_work *delay = to_delayed_work(work);\n\tstruct mcast_group *group;\n\tstruct mcast_req *req = NULL;\n\n\tgroup = container_of(delay, typeof(*group), timeout_work);\n\n\tmutex_lock(&group->lock);\n\tif (group->state == MCAST_JOIN_SENT) {\n\t\tif (!list_empty(&group->pending_list)) {\n\t\t\treq = list_first_entry(&group->pending_list, struct mcast_req, group_list);\n\t\t\tlist_del(&req->group_list);\n\t\t\tlist_del(&req->func_list);\n\t\t\t--group->func[req->func].num_pend_reqs;\n\t\t\tmutex_unlock(&group->lock);\n\t\t\tkfree(req);\n\t\t\tif (memcmp(&group->rec.mgid, &mgid0, sizeof mgid0)) {\n\t\t\t\tif (release_group(group, 1))\n\t\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tkfree(group);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tmutex_lock(&group->lock);\n\t\t} else\n\t\t\tmcg_warn_group(group, \"DRIVER BUG\\n\");\n\t} else if (group->state == MCAST_LEAVE_SENT) {\n\t\tif (group->rec.scope_join_state & 0xf)\n\t\t\tgroup->rec.scope_join_state &= 0xf0;\n\t\tgroup->state = MCAST_IDLE;\n\t\tmutex_unlock(&group->lock);\n\t\tif (release_group(group, 1))\n\t\t\treturn;\n\t\tmutex_lock(&group->lock);\n\t} else\n\t\tmcg_warn_group(group, \"invalid state %s\\n\", get_state_string(group->state));\n\tgroup->state = MCAST_IDLE;\n\tatomic_inc(&group->refcount);\n\tif (!queue_work(group->demux->mcg_wq, &group->work))\n\t\tsafe_atomic_dec(&group->refcount);\n\n\tmutex_unlock(&group->lock);\n}\n\nstatic int handle_leave_req(struct mcast_group *group, u8 leave_mask,\n\t\t\t    struct mcast_req *req)\n{\n\tu16 status;\n\n\tif (req->clean)\n\t\tleave_mask = group->func[req->func].join_state;\n\n\tstatus = check_leave(group, req->func, leave_mask);\n\tif (!status)\n\t\tleave_group(group, req->func, leave_mask);\n\n\tif (!req->clean)\n\t\tsend_reply_to_slave(req->func, group, &req->sa_mad, status);\n\t--group->func[req->func].num_pend_reqs;\n\tlist_del(&req->group_list);\n\tlist_del(&req->func_list);\n\tkfree(req);\n\treturn 1;\n}\n\nstatic int handle_join_req(struct mcast_group *group, u8 join_mask,\n\t\t\t   struct mcast_req *req)\n{\n\tu8 group_join_state = group->rec.scope_join_state & 0xf;\n\tint ref = 0;\n\tu16 status;\n\tstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)req->sa_mad.data;\n\n\tif (join_mask == (group_join_state & join_mask)) {\n\t\t \n\t\tstatus = cmp_rec(&group->rec, sa_data, req->sa_mad.sa_hdr.comp_mask);\n\t\tif (!status)\n\t\t\tjoin_group(group, req->func, join_mask);\n\n\t\t--group->func[req->func].num_pend_reqs;\n\t\tsend_reply_to_slave(req->func, group, &req->sa_mad, status);\n\t\tlist_del(&req->group_list);\n\t\tlist_del(&req->func_list);\n\t\tkfree(req);\n\t\t++ref;\n\t} else {\n\t\t \n\t\tgroup->prev_state = group->state;\n\t\tif (send_join_to_wire(group, &req->sa_mad)) {\n\t\t\t--group->func[req->func].num_pend_reqs;\n\t\t\tlist_del(&req->group_list);\n\t\t\tlist_del(&req->func_list);\n\t\t\tkfree(req);\n\t\t\tref = 1;\n\t\t\tgroup->state = group->prev_state;\n\t\t} else\n\t\t\tgroup->state = MCAST_JOIN_SENT;\n\t}\n\n\treturn ref;\n}\n\nstatic void mlx4_ib_mcg_work_handler(struct work_struct *work)\n{\n\tstruct mcast_group *group;\n\tstruct mcast_req *req = NULL;\n\tstruct ib_sa_mcmember_data *sa_data;\n\tu8 req_join_state;\n\tint rc = 1;  \n\tu16 status;\n\tu8 method;\n\n\tgroup = container_of(work, typeof(*group), work);\n\n\tmutex_lock(&group->lock);\n\n\t \n\tif (group->state == MCAST_RESP_READY) {\n\t\t \n\t\tcancel_delayed_work(&group->timeout_work);\n\t\tstatus = be16_to_cpu(group->response_sa_mad.mad_hdr.status);\n\t\tmethod = group->response_sa_mad.mad_hdr.method;\n\t\tif (group->last_req_tid != group->response_sa_mad.mad_hdr.tid) {\n\t\t\tmcg_warn_group(group, \"Got MAD response to existing MGID but wrong TID, dropping. Resp TID=%llx, group TID=%llx\\n\",\n\t\t\t\tbe64_to_cpu(group->response_sa_mad.mad_hdr.tid),\n\t\t\t\tbe64_to_cpu(group->last_req_tid));\n\t\t\tgroup->state = group->prev_state;\n\t\t\tgoto process_requests;\n\t\t}\n\t\tif (status) {\n\t\t\tif (!list_empty(&group->pending_list))\n\t\t\t\treq = list_first_entry(&group->pending_list,\n\t\t\t\t\t\tstruct mcast_req, group_list);\n\t\t\tif (method == IB_MGMT_METHOD_GET_RESP) {\n\t\t\t\t\tif (req) {\n\t\t\t\t\t\tsend_reply_to_slave(req->func, group, &req->sa_mad, status);\n\t\t\t\t\t\t--group->func[req->func].num_pend_reqs;\n\t\t\t\t\t\tlist_del(&req->group_list);\n\t\t\t\t\t\tlist_del(&req->func_list);\n\t\t\t\t\t\tkfree(req);\n\t\t\t\t\t\t++rc;\n\t\t\t\t\t} else\n\t\t\t\t\t\tmcg_warn_group(group, \"no request for failed join\\n\");\n\t\t\t} else if (method == IB_SA_METHOD_DELETE_RESP && group->demux->flushing)\n\t\t\t\t++rc;\n\t\t} else {\n\t\t\tu8 resp_join_state;\n\t\t\tu8 cur_join_state;\n\n\t\t\tresp_join_state = ((struct ib_sa_mcmember_data *)\n\t\t\t\t\t\tgroup->response_sa_mad.data)->scope_join_state & 0xf;\n\t\t\tcur_join_state = group->rec.scope_join_state & 0xf;\n\n\t\t\tif (method == IB_MGMT_METHOD_GET_RESP) {\n\t\t\t\t \n\t\t\t\tif (!cur_join_state && resp_join_state)\n\t\t\t\t\t--rc;\n\t\t\t} else if (!resp_join_state)\n\t\t\t\t\t++rc;\n\t\t\tmemcpy(&group->rec, group->response_sa_mad.data, sizeof group->rec);\n\t\t}\n\t\tgroup->state = MCAST_IDLE;\n\t}\n\nprocess_requests:\n\t \n\twhile (!list_empty(&group->pending_list) && group->state == MCAST_IDLE) {\n\t\treq = list_first_entry(&group->pending_list, struct mcast_req,\n\t\t\t\t       group_list);\n\t\tsa_data = (struct ib_sa_mcmember_data *)req->sa_mad.data;\n\t\treq_join_state = sa_data->scope_join_state & 0xf;\n\n\t\t \n\t\tif (req->sa_mad.mad_hdr.method == IB_SA_METHOD_DELETE)\n\t\t\trc += handle_leave_req(group, req_join_state, req);\n\t\telse\n\t\t\trc += handle_join_req(group, req_join_state, req);\n\t}\n\n\t \n\tif (group->state == MCAST_IDLE) {\n\t\treq_join_state = get_leave_state(group);\n\t\tif (req_join_state) {\n\t\t\tgroup->rec.scope_join_state &= ~req_join_state;\n\t\t\tgroup->prev_state = group->state;\n\t\t\tif (send_leave_to_wire(group, req_join_state)) {\n\t\t\t\tgroup->state = group->prev_state;\n\t\t\t\t++rc;\n\t\t\t} else\n\t\t\t\tgroup->state = MCAST_LEAVE_SENT;\n\t\t}\n\t}\n\n\tif (!list_empty(&group->pending_list) && group->state == MCAST_IDLE)\n\t\tgoto process_requests;\n\tmutex_unlock(&group->lock);\n\n\twhile (rc--)\n\t\trelease_group(group, 0);\n}\n\nstatic struct mcast_group *search_relocate_mgid0_group(struct mlx4_ib_demux_ctx *ctx,\n\t\t\t\t\t\t       __be64 tid,\n\t\t\t\t\t\t       union ib_gid *new_mgid)\n{\n\tstruct mcast_group *group = NULL, *cur_group, *n;\n\tstruct mcast_req *req;\n\n\tmutex_lock(&ctx->mcg_table_lock);\n\tlist_for_each_entry_safe(group, n, &ctx->mcg_mgid0_list, mgid0_list) {\n\t\tmutex_lock(&group->lock);\n\t\tif (group->last_req_tid == tid) {\n\t\t\tif (memcmp(new_mgid, &mgid0, sizeof mgid0)) {\n\t\t\t\tgroup->rec.mgid = *new_mgid;\n\t\t\t\tsprintf(group->name, \"%016llx%016llx\",\n\t\t\t\t\t\tbe64_to_cpu(group->rec.mgid.global.subnet_prefix),\n\t\t\t\t\t\tbe64_to_cpu(group->rec.mgid.global.interface_id));\n\t\t\t\tlist_del_init(&group->mgid0_list);\n\t\t\t\tcur_group = mcast_insert(ctx, group);\n\t\t\t\tif (cur_group) {\n\t\t\t\t\t \n\t\t\t\t\treq = list_first_entry(&group->pending_list,\n\t\t\t\t\t\t\t       struct mcast_req, group_list);\n\t\t\t\t\t--group->func[req->func].num_pend_reqs;\n\t\t\t\t\tlist_del(&req->group_list);\n\t\t\t\t\tlist_del(&req->func_list);\n\t\t\t\t\tkfree(req);\n\t\t\t\t\tmutex_unlock(&group->lock);\n\t\t\t\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\t\t\t\trelease_group(group, 0);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\n\t\t\t\tatomic_inc(&group->refcount);\n\t\t\t\tadd_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\n\t\t\t\tmutex_unlock(&group->lock);\n\t\t\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\t\t\treturn group;\n\t\t\t} else {\n\t\t\t\tstruct mcast_req *tmp1, *tmp2;\n\n\t\t\t\tlist_del(&group->mgid0_list);\n\t\t\t\tif (!list_empty(&group->pending_list) && group->state != MCAST_IDLE)\n\t\t\t\t\tcancel_delayed_work_sync(&group->timeout_work);\n\n\t\t\t\tlist_for_each_entry_safe(tmp1, tmp2, &group->pending_list, group_list) {\n\t\t\t\t\tlist_del(&tmp1->group_list);\n\t\t\t\t\tkfree(tmp1);\n\t\t\t\t}\n\t\t\t\tmutex_unlock(&group->lock);\n\t\t\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\t\t\tkfree(group);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&group->lock);\n\t}\n\tmutex_unlock(&ctx->mcg_table_lock);\n\n\treturn NULL;\n}\n\nstatic ssize_t sysfs_show_group(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf);\n\nstatic struct mcast_group *acquire_group(struct mlx4_ib_demux_ctx *ctx,\n\t\t\t\t\t union ib_gid *mgid, int create)\n{\n\tstruct mcast_group *group, *cur_group;\n\tint is_mgid0;\n\tint i;\n\n\tis_mgid0 = !memcmp(&mgid0, mgid, sizeof mgid0);\n\tif (!is_mgid0) {\n\t\tgroup = mcast_find(ctx, mgid);\n\t\tif (group)\n\t\t\tgoto found;\n\t}\n\n\tif (!create)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tgroup = kzalloc(sizeof(*group), GFP_KERNEL);\n\tif (!group)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tgroup->demux = ctx;\n\tgroup->rec.mgid = *mgid;\n\tINIT_LIST_HEAD(&group->pending_list);\n\tINIT_LIST_HEAD(&group->mgid0_list);\n\tfor (i = 0; i < MAX_VFS; ++i)\n\t\tINIT_LIST_HEAD(&group->func[i].pending);\n\tINIT_WORK(&group->work, mlx4_ib_mcg_work_handler);\n\tINIT_DELAYED_WORK(&group->timeout_work, mlx4_ib_mcg_timeout_handler);\n\tmutex_init(&group->lock);\n\tsprintf(group->name, \"%016llx%016llx\",\n\t\t\tbe64_to_cpu(group->rec.mgid.global.subnet_prefix),\n\t\t\tbe64_to_cpu(group->rec.mgid.global.interface_id));\n\tsysfs_attr_init(&group->dentry.attr);\n\tgroup->dentry.show = sysfs_show_group;\n\tgroup->dentry.store = NULL;\n\tgroup->dentry.attr.name = group->name;\n\tgroup->dentry.attr.mode = 0400;\n\tgroup->state = MCAST_IDLE;\n\n\tif (is_mgid0) {\n\t\tlist_add(&group->mgid0_list, &ctx->mcg_mgid0_list);\n\t\tgoto found;\n\t}\n\n\tcur_group = mcast_insert(ctx, group);\n\tif (cur_group) {\n\t\tmcg_warn(\"group just showed up %s - confused\\n\", cur_group->name);\n\t\tkfree(group);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tadd_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\n\nfound:\n\tatomic_inc(&group->refcount);\n\treturn group;\n}\n\nstatic void queue_req(struct mcast_req *req)\n{\n\tstruct mcast_group *group = req->group;\n\n\tatomic_inc(&group->refcount);  \n\tatomic_inc(&group->refcount);  \n\tlist_add_tail(&req->group_list, &group->pending_list);\n\tlist_add_tail(&req->func_list, &group->func[req->func].pending);\n\t \n\tif (!queue_work(group->demux->mcg_wq, &group->work))\n\t\tsafe_atomic_dec(&group->refcount);\n}\n\nint mlx4_ib_mcg_demux_handler(struct ib_device *ibdev, int port, int slave,\n\t\t\t      struct ib_sa_mad *mad)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct ib_sa_mcmember_data *rec = (struct ib_sa_mcmember_data *)mad->data;\n\tstruct mlx4_ib_demux_ctx *ctx = &dev->sriov.demux[port - 1];\n\tstruct mcast_group *group;\n\n\tswitch (mad->mad_hdr.method) {\n\tcase IB_MGMT_METHOD_GET_RESP:\n\tcase IB_SA_METHOD_DELETE_RESP:\n\t\tmutex_lock(&ctx->mcg_table_lock);\n\t\tgroup = acquire_group(ctx, &rec->mgid, 0);\n\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\tif (IS_ERR(group)) {\n\t\t\tif (mad->mad_hdr.method == IB_MGMT_METHOD_GET_RESP) {\n\t\t\t\t__be64 tid = mad->mad_hdr.tid;\n\t\t\t\t*(u8 *)(&tid) = (u8)slave;  \n\t\t\t\tgroup = search_relocate_mgid0_group(ctx, tid, &rec->mgid);\n\t\t\t} else\n\t\t\t\tgroup = NULL;\n\t\t}\n\n\t\tif (!group)\n\t\t\treturn 1;\n\n\t\tmutex_lock(&group->lock);\n\t\tgroup->response_sa_mad = *mad;\n\t\tgroup->prev_state = group->state;\n\t\tgroup->state = MCAST_RESP_READY;\n\t\t \n\t\tatomic_inc(&group->refcount);\n\t\tif (!queue_work(ctx->mcg_wq, &group->work))\n\t\t\tsafe_atomic_dec(&group->refcount);\n\t\tmutex_unlock(&group->lock);\n\t\trelease_group(group, 0);\n\t\treturn 1;  \n\tcase IB_MGMT_METHOD_SET:\n\tcase IB_SA_METHOD_GET_TABLE:\n\tcase IB_SA_METHOD_GET_TABLE_RESP:\n\tcase IB_SA_METHOD_DELETE:\n\t\treturn 0;  \n\tdefault:\n\t\tmcg_warn(\"In demux, port %d: unexpected MCMember method: 0x%x, dropping\\n\",\n\t\t\tport, mad->mad_hdr.method);\n\t\treturn 1;  \n\t}\n}\n\nint mlx4_ib_mcg_multiplex_handler(struct ib_device *ibdev, int port,\n\t\t\t\t  int slave, struct ib_sa_mad *sa_mad)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct ib_sa_mcmember_data *rec = (struct ib_sa_mcmember_data *)sa_mad->data;\n\tstruct mlx4_ib_demux_ctx *ctx = &dev->sriov.demux[port - 1];\n\tstruct mcast_group *group;\n\tstruct mcast_req *req;\n\tint may_create = 0;\n\n\tif (ctx->flushing)\n\t\treturn -EAGAIN;\n\n\tswitch (sa_mad->mad_hdr.method) {\n\tcase IB_MGMT_METHOD_SET:\n\t\tmay_create = 1;\n\t\tfallthrough;\n\tcase IB_SA_METHOD_DELETE:\n\t\treq = kzalloc(sizeof *req, GFP_KERNEL);\n\t\tif (!req)\n\t\t\treturn -ENOMEM;\n\n\t\treq->func = slave;\n\t\treq->sa_mad = *sa_mad;\n\n\t\tmutex_lock(&ctx->mcg_table_lock);\n\t\tgroup = acquire_group(ctx, &rec->mgid, may_create);\n\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\tif (IS_ERR(group)) {\n\t\t\tkfree(req);\n\t\t\treturn PTR_ERR(group);\n\t\t}\n\t\tmutex_lock(&group->lock);\n\t\tif (group->func[slave].num_pend_reqs > MAX_PEND_REQS_PER_FUNC) {\n\t\t\tmutex_unlock(&group->lock);\n\t\t\tmcg_debug_group(group, \"Port %d, Func %d has too many pending requests (%d), dropping\\n\",\n\t\t\t\t\tport, slave, MAX_PEND_REQS_PER_FUNC);\n\t\t\trelease_group(group, 0);\n\t\t\tkfree(req);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\t++group->func[slave].num_pend_reqs;\n\t\treq->group = group;\n\t\tqueue_req(req);\n\t\tmutex_unlock(&group->lock);\n\t\trelease_group(group, 0);\n\t\treturn 1;  \n\tcase IB_SA_METHOD_GET_TABLE:\n\tcase IB_MGMT_METHOD_GET_RESP:\n\tcase IB_SA_METHOD_GET_TABLE_RESP:\n\tcase IB_SA_METHOD_DELETE_RESP:\n\t\treturn 0;  \n\tdefault:\n\t\tmcg_warn(\"In multiplex, port %d, func %d: unexpected MCMember method: 0x%x, dropping\\n\",\n\t\t\tport, slave, sa_mad->mad_hdr.method);\n\t\treturn 1;  \n\t}\n}\n\nstatic ssize_t sysfs_show_group(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct mcast_group *group =\n\t\tcontainer_of(attr, struct mcast_group, dentry);\n\tstruct mcast_req *req = NULL;\n\tchar state_str[40];\n\tchar pending_str[40];\n\tint len;\n\tint i;\n\tu32 hoplimit;\n\n\tif (group->state == MCAST_IDLE)\n\t\tscnprintf(state_str, sizeof(state_str), \"%s\",\n\t\t\t  get_state_string(group->state));\n\telse\n\t\tscnprintf(state_str, sizeof(state_str), \"%s(TID=0x%llx)\",\n\t\t\t  get_state_string(group->state),\n\t\t\t  be64_to_cpu(group->last_req_tid));\n\n\tif (list_empty(&group->pending_list)) {\n\t\tscnprintf(pending_str, sizeof(pending_str), \"No\");\n\t} else {\n\t\treq = list_first_entry(&group->pending_list, struct mcast_req,\n\t\t\t\t       group_list);\n\t\tscnprintf(pending_str, sizeof(pending_str), \"Yes(TID=0x%llx)\",\n\t\t\t  be64_to_cpu(req->sa_mad.mad_hdr.tid));\n\t}\n\n\tlen = sysfs_emit(buf, \"%1d [%02d,%02d,%02d] %4d %4s %5s     \",\n\t\t\t group->rec.scope_join_state & 0xf,\n\t\t\t group->members[2],\n\t\t\t group->members[1],\n\t\t\t group->members[0],\n\t\t\t atomic_read(&group->refcount),\n\t\t\t pending_str,\n\t\t\t state_str);\n\n\tfor (i = 0; i < MAX_VFS; i++) {\n\t\tif (group->func[i].state == MCAST_MEMBER)\n\t\t\tlen += sysfs_emit_at(buf, len, \"%d[%1x] \", i,\n\t\t\t\t\t     group->func[i].join_state);\n\t}\n\n\thoplimit = be32_to_cpu(group->rec.sl_flowlabel_hoplimit);\n\tlen += sysfs_emit_at(buf, len,\n\t\t\t     \"\\t\\t(%4hx %4x %2x %2x %2x %2x %2x %4x %4x %2x %2x)\\n\",\n\t\t\t     be16_to_cpu(group->rec.pkey),\n\t\t\t     be32_to_cpu(group->rec.qkey),\n\t\t\t     (group->rec.mtusel_mtu & 0xc0) >> 6,\n\t\t\t     (group->rec.mtusel_mtu & 0x3f),\n\t\t\t     group->rec.tclass,\n\t\t\t     (group->rec.ratesel_rate & 0xc0) >> 6,\n\t\t\t     (group->rec.ratesel_rate & 0x3f),\n\t\t\t     (hoplimit & 0xf0000000) >> 28,\n\t\t\t     (hoplimit & 0x0fffff00) >> 8,\n\t\t\t     (hoplimit & 0x000000ff),\n\t\t\t     group->rec.proxy_join);\n\n\treturn len;\n}\n\nint mlx4_ib_mcg_port_init(struct mlx4_ib_demux_ctx *ctx)\n{\n\tchar name[20];\n\n\tatomic_set(&ctx->tid, 0);\n\tsprintf(name, \"mlx4_ib_mcg%d\", ctx->port);\n\tctx->mcg_wq = alloc_ordered_workqueue(name, WQ_MEM_RECLAIM);\n\tif (!ctx->mcg_wq)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&ctx->mcg_table_lock);\n\tctx->mcg_table = RB_ROOT;\n\tINIT_LIST_HEAD(&ctx->mcg_mgid0_list);\n\tctx->flushing = 0;\n\n\treturn 0;\n}\n\nstatic void force_clean_group(struct mcast_group *group)\n{\n\tstruct mcast_req *req, *tmp\n\t\t;\n\tlist_for_each_entry_safe(req, tmp, &group->pending_list, group_list) {\n\t\tlist_del(&req->group_list);\n\t\tkfree(req);\n\t}\n\tdel_sysfs_port_mcg_attr(group->demux->dev, group->demux->port, &group->dentry.attr);\n\trb_erase(&group->node, &group->demux->mcg_table);\n\tkfree(group);\n}\n\nstatic void _mlx4_ib_mcg_port_cleanup(struct mlx4_ib_demux_ctx *ctx, int destroy_wq)\n{\n\tint i;\n\tstruct rb_node *p;\n\tstruct mcast_group *group;\n\tunsigned long end;\n\tint count;\n\n\tfor (i = 0; i < MAX_VFS; ++i)\n\t\tclean_vf_mcast(ctx, i);\n\n\tend = jiffies + msecs_to_jiffies(MAD_TIMEOUT_MS + 3000);\n\tdo {\n\t\tcount = 0;\n\t\tmutex_lock(&ctx->mcg_table_lock);\n\t\tfor (p = rb_first(&ctx->mcg_table); p; p = rb_next(p))\n\t\t\t++count;\n\t\tmutex_unlock(&ctx->mcg_table_lock);\n\t\tif (!count)\n\t\t\tbreak;\n\n\t\tusleep_range(1000, 2000);\n\t} while (time_after(end, jiffies));\n\n\tflush_workqueue(ctx->mcg_wq);\n\tif (destroy_wq)\n\t\tdestroy_workqueue(ctx->mcg_wq);\n\n\tmutex_lock(&ctx->mcg_table_lock);\n\twhile ((p = rb_first(&ctx->mcg_table)) != NULL) {\n\t\tgroup = rb_entry(p, struct mcast_group, node);\n\t\tif (atomic_read(&group->refcount))\n\t\t\tmcg_debug_group(group, \"group refcount %d!!! (pointer %p)\\n\",\n\t\t\t\t\tatomic_read(&group->refcount), group);\n\n\t\tforce_clean_group(group);\n\t}\n\tmutex_unlock(&ctx->mcg_table_lock);\n}\n\nstruct clean_work {\n\tstruct work_struct work;\n\tstruct mlx4_ib_demux_ctx *ctx;\n\tint destroy_wq;\n};\n\nstatic void mcg_clean_task(struct work_struct *work)\n{\n\tstruct clean_work *cw = container_of(work, struct clean_work, work);\n\n\t_mlx4_ib_mcg_port_cleanup(cw->ctx, cw->destroy_wq);\n\tcw->ctx->flushing = 0;\n\tkfree(cw);\n}\n\nvoid mlx4_ib_mcg_port_cleanup(struct mlx4_ib_demux_ctx *ctx, int destroy_wq)\n{\n\tstruct clean_work *work;\n\n\tif (ctx->flushing)\n\t\treturn;\n\n\tctx->flushing = 1;\n\n\tif (destroy_wq) {\n\t\t_mlx4_ib_mcg_port_cleanup(ctx, destroy_wq);\n\t\tctx->flushing = 0;\n\t\treturn;\n\t}\n\n\twork = kmalloc(sizeof *work, GFP_KERNEL);\n\tif (!work) {\n\t\tctx->flushing = 0;\n\t\treturn;\n\t}\n\n\twork->ctx = ctx;\n\twork->destroy_wq = destroy_wq;\n\tINIT_WORK(&work->work, mcg_clean_task);\n\tqueue_work(clean_wq, &work->work);\n}\n\nstatic void build_leave_mad(struct mcast_req *req)\n{\n\tstruct ib_sa_mad *mad = &req->sa_mad;\n\n\tmad->mad_hdr.method = IB_SA_METHOD_DELETE;\n}\n\n\nstatic void clear_pending_reqs(struct mcast_group *group, int vf)\n{\n\tstruct mcast_req *req, *tmp, *group_first = NULL;\n\tint clear;\n\tint pend = 0;\n\n\tif (!list_empty(&group->pending_list))\n\t\tgroup_first = list_first_entry(&group->pending_list, struct mcast_req, group_list);\n\n\tlist_for_each_entry_safe(req, tmp, &group->func[vf].pending, func_list) {\n\t\tclear = 1;\n\t\tif (group_first == req &&\n\t\t    (group->state == MCAST_JOIN_SENT ||\n\t\t     group->state == MCAST_LEAVE_SENT)) {\n\t\t\tclear = cancel_delayed_work(&group->timeout_work);\n\t\t\tpend = !clear;\n\t\t\tgroup->state = MCAST_IDLE;\n\t\t}\n\t\tif (clear) {\n\t\t\t--group->func[vf].num_pend_reqs;\n\t\t\tlist_del(&req->group_list);\n\t\t\tlist_del(&req->func_list);\n\t\t\tkfree(req);\n\t\t\tatomic_dec(&group->refcount);\n\t\t}\n\t}\n\n\tif (!pend && (!list_empty(&group->func[vf].pending) || group->func[vf].num_pend_reqs)) {\n\t\tmcg_warn_group(group, \"DRIVER BUG: list_empty %d, num_pend_reqs %d\\n\",\n\t\t\t       list_empty(&group->func[vf].pending), group->func[vf].num_pend_reqs);\n\t}\n}\n\nstatic int push_deleteing_req(struct mcast_group *group, int slave)\n{\n\tstruct mcast_req *req;\n\tstruct mcast_req *pend_req;\n\n\tif (!group->func[slave].join_state)\n\t\treturn 0;\n\n\treq = kzalloc(sizeof *req, GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tif (!list_empty(&group->func[slave].pending)) {\n\t\tpend_req = list_entry(group->func[slave].pending.prev, struct mcast_req, group_list);\n\t\tif (pend_req->clean) {\n\t\t\tkfree(req);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treq->clean = 1;\n\treq->func = slave;\n\treq->group = group;\n\t++group->func[slave].num_pend_reqs;\n\tbuild_leave_mad(req);\n\tqueue_req(req);\n\treturn 0;\n}\n\nvoid clean_vf_mcast(struct mlx4_ib_demux_ctx *ctx, int slave)\n{\n\tstruct mcast_group *group;\n\tstruct rb_node *p;\n\n\tmutex_lock(&ctx->mcg_table_lock);\n\tfor (p = rb_first(&ctx->mcg_table); p; p = rb_next(p)) {\n\t\tgroup = rb_entry(p, struct mcast_group, node);\n\t\tmutex_lock(&group->lock);\n\t\tif (atomic_read(&group->refcount)) {\n\t\t\t \n\t\t\tclear_pending_reqs(group, slave);\n\t\t\tpush_deleteing_req(group, slave);\n\t\t}\n\t\tmutex_unlock(&group->lock);\n\t}\n\tmutex_unlock(&ctx->mcg_table_lock);\n}\n\n\nint mlx4_ib_mcg_init(void)\n{\n\tclean_wq = alloc_ordered_workqueue(\"mlx4_ib_mcg\", WQ_MEM_RECLAIM);\n\tif (!clean_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid mlx4_ib_mcg_destroy(void)\n{\n\tdestroy_workqueue(clean_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}