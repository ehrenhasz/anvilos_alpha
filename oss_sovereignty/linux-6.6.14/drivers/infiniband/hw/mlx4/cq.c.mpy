{
  "module_name": "cq.c",
  "hash_id": "ef5e1afce5515ebdb3c9470eb4db057ad50a1120ebcbfef7f0af5d289fa9a807",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/cq.c",
  "human_readable_source": " \n\n#include <linux/mlx4/cq.h>\n#include <linux/mlx4/qp.h>\n#include <linux/mlx4/srq.h>\n#include <linux/slab.h>\n\n#include \"mlx4_ib.h\"\n#include <rdma/mlx4-abi.h>\n#include <rdma/uverbs_ioctl.h>\n\nstatic void mlx4_ib_cq_comp(struct mlx4_cq *cq)\n{\n\tstruct ib_cq *ibcq = &to_mibcq(cq)->ibcq;\n\tibcq->comp_handler(ibcq, ibcq->cq_context);\n}\n\nstatic void mlx4_ib_cq_event(struct mlx4_cq *cq, enum mlx4_event type)\n{\n\tstruct ib_event event;\n\tstruct ib_cq *ibcq;\n\n\tif (type != MLX4_EVENT_TYPE_CQ_ERROR) {\n\t\tpr_warn(\"Unexpected event type %d \"\n\t\t       \"on CQ %06x\\n\", type, cq->cqn);\n\t\treturn;\n\t}\n\n\tibcq = &to_mibcq(cq)->ibcq;\n\tif (ibcq->event_handler) {\n\t\tevent.device     = ibcq->device;\n\t\tevent.event      = IB_EVENT_CQ_ERR;\n\t\tevent.element.cq = ibcq;\n\t\tibcq->event_handler(&event, ibcq->cq_context);\n\t}\n}\n\nstatic void *get_cqe_from_buf(struct mlx4_ib_cq_buf *buf, int n)\n{\n\treturn mlx4_buf_offset(&buf->buf, n * buf->entry_size);\n}\n\nstatic void *get_cqe(struct mlx4_ib_cq *cq, int n)\n{\n\treturn get_cqe_from_buf(&cq->buf, n);\n}\n\nstatic void *get_sw_cqe(struct mlx4_ib_cq *cq, int n)\n{\n\tstruct mlx4_cqe *cqe = get_cqe(cq, n & cq->ibcq.cqe);\n\tstruct mlx4_cqe *tcqe = ((cq->buf.entry_size == 64) ? (cqe + 1) : cqe);\n\n\treturn (!!(tcqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK) ^\n\t\t!!(n & (cq->ibcq.cqe + 1))) ? NULL : cqe;\n}\n\nstatic struct mlx4_cqe *next_cqe_sw(struct mlx4_ib_cq *cq)\n{\n\treturn get_sw_cqe(cq, cq->mcq.cons_index);\n}\n\nint mlx4_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period)\n{\n\tstruct mlx4_ib_cq *mcq = to_mcq(cq);\n\tstruct mlx4_ib_dev *dev = to_mdev(cq->device);\n\n\treturn mlx4_cq_modify(dev->dev, &mcq->mcq, cq_count, cq_period);\n}\n\nstatic int mlx4_ib_alloc_cq_buf(struct mlx4_ib_dev *dev, struct mlx4_ib_cq_buf *buf, int nent)\n{\n\tint err;\n\n\terr = mlx4_buf_alloc(dev->dev, nent * dev->dev->caps.cqe_size,\n\t\t\t     PAGE_SIZE * 2, &buf->buf);\n\n\tif (err)\n\t\tgoto out;\n\n\tbuf->entry_size = dev->dev->caps.cqe_size;\n\terr = mlx4_mtt_init(dev->dev, buf->buf.npages, buf->buf.page_shift,\n\t\t\t\t    &buf->mtt);\n\tif (err)\n\t\tgoto err_buf;\n\n\terr = mlx4_buf_write_mtt(dev->dev, &buf->mtt, &buf->buf);\n\tif (err)\n\t\tgoto err_mtt;\n\n\treturn 0;\n\nerr_mtt:\n\tmlx4_mtt_cleanup(dev->dev, &buf->mtt);\n\nerr_buf:\n\tmlx4_buf_free(dev->dev, nent * buf->entry_size, &buf->buf);\n\nout:\n\treturn err;\n}\n\nstatic void mlx4_ib_free_cq_buf(struct mlx4_ib_dev *dev, struct mlx4_ib_cq_buf *buf, int cqe)\n{\n\tmlx4_buf_free(dev->dev, (cqe + 1) * buf->entry_size, &buf->buf);\n}\n\nstatic int mlx4_ib_get_cq_umem(struct mlx4_ib_dev *dev,\n\t\t\t       struct mlx4_ib_cq_buf *buf,\n\t\t\t       struct ib_umem **umem, u64 buf_addr, int cqe)\n{\n\tint err;\n\tint cqe_size = dev->dev->caps.cqe_size;\n\tint shift;\n\tint n;\n\n\t*umem = ib_umem_get(&dev->ib_dev, buf_addr, cqe * cqe_size,\n\t\t\t    IB_ACCESS_LOCAL_WRITE);\n\tif (IS_ERR(*umem))\n\t\treturn PTR_ERR(*umem);\n\n\tshift = mlx4_ib_umem_calc_optimal_mtt_size(*umem, 0, &n);\n\terr = mlx4_mtt_init(dev->dev, n, shift, &buf->mtt);\n\n\tif (err)\n\t\tgoto err_buf;\n\n\terr = mlx4_ib_umem_write_mtt(dev, &buf->mtt, *umem);\n\tif (err)\n\t\tgoto err_mtt;\n\n\treturn 0;\n\nerr_mtt:\n\tmlx4_mtt_cleanup(dev->dev, &buf->mtt);\n\nerr_buf:\n\tib_umem_release(*umem);\n\n\treturn err;\n}\n\n#define CQ_CREATE_FLAGS_SUPPORTED IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION\nint mlx4_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t      struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tint entries = attr->cqe;\n\tint vector = attr->comp_vector;\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx4_ib_cq *cq = to_mcq(ibcq);\n\tstruct mlx4_uar *uar;\n\tvoid *buf_addr;\n\tint err;\n\tstruct mlx4_ib_ucontext *context = rdma_udata_to_drv_context(\n\t\tudata, struct mlx4_ib_ucontext, ibucontext);\n\n\tif (entries < 1 || entries > dev->dev->caps.max_cqes)\n\t\treturn -EINVAL;\n\n\tif (attr->flags & ~CQ_CREATE_FLAGS_SUPPORTED)\n\t\treturn -EINVAL;\n\n\tentries      = roundup_pow_of_two(entries + 1);\n\tcq->ibcq.cqe = entries - 1;\n\tmutex_init(&cq->resize_mutex);\n\tspin_lock_init(&cq->lock);\n\tcq->resize_buf = NULL;\n\tcq->resize_umem = NULL;\n\tcq->create_flags = attr->flags;\n\tINIT_LIST_HEAD(&cq->send_qp_list);\n\tINIT_LIST_HEAD(&cq->recv_qp_list);\n\n\tif (udata) {\n\t\tstruct mlx4_ib_create_cq ucmd;\n\n\t\tif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_cq;\n\t\t}\n\n\t\tbuf_addr = (void *)(unsigned long)ucmd.buf_addr;\n\t\terr = mlx4_ib_get_cq_umem(dev, &cq->buf, &cq->umem,\n\t\t\t\t\t  ucmd.buf_addr, entries);\n\t\tif (err)\n\t\t\tgoto err_cq;\n\n\t\terr = mlx4_ib_db_map_user(udata, ucmd.db_addr, &cq->db);\n\t\tif (err)\n\t\t\tgoto err_mtt;\n\n\t\tuar = &context->uar;\n\t\tcq->mcq.usage = MLX4_RES_USAGE_USER_VERBS;\n\t} else {\n\t\terr = mlx4_db_alloc(dev->dev, &cq->db, 1);\n\t\tif (err)\n\t\t\tgoto err_cq;\n\n\t\tcq->mcq.set_ci_db  = cq->db.db;\n\t\tcq->mcq.arm_db     = cq->db.db + 1;\n\t\t*cq->mcq.set_ci_db = 0;\n\t\t*cq->mcq.arm_db    = 0;\n\n\t\terr = mlx4_ib_alloc_cq_buf(dev, &cq->buf, entries);\n\t\tif (err)\n\t\t\tgoto err_db;\n\n\t\tbuf_addr = &cq->buf.buf;\n\n\t\tuar = &dev->priv_uar;\n\t\tcq->mcq.usage = MLX4_RES_USAGE_DRIVER;\n\t}\n\n\tif (dev->eq_table)\n\t\tvector = dev->eq_table[vector % ibdev->num_comp_vectors];\n\n\terr = mlx4_cq_alloc(dev->dev, entries, &cq->buf.mtt, uar, cq->db.dma,\n\t\t\t    &cq->mcq, vector, 0,\n\t\t\t    !!(cq->create_flags &\n\t\t\t       IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION),\n\t\t\t    buf_addr, !!udata);\n\tif (err)\n\t\tgoto err_dbmap;\n\n\tif (udata)\n\t\tcq->mcq.tasklet_ctx.comp = mlx4_ib_cq_comp;\n\telse\n\t\tcq->mcq.comp = mlx4_ib_cq_comp;\n\tcq->mcq.event = mlx4_ib_cq_event;\n\n\tif (udata)\n\t\tif (ib_copy_to_udata(udata, &cq->mcq.cqn, sizeof (__u32))) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_cq_free;\n\t\t}\n\n\treturn 0;\n\nerr_cq_free:\n\tmlx4_cq_free(dev->dev, &cq->mcq);\n\nerr_dbmap:\n\tif (udata)\n\t\tmlx4_ib_db_unmap_user(context, &cq->db);\n\nerr_mtt:\n\tmlx4_mtt_cleanup(dev->dev, &cq->buf.mtt);\n\n\tib_umem_release(cq->umem);\n\tif (!udata)\n\t\tmlx4_ib_free_cq_buf(dev, &cq->buf, cq->ibcq.cqe);\n\nerr_db:\n\tif (!udata)\n\t\tmlx4_db_free(dev->dev, &cq->db);\nerr_cq:\n\treturn err;\n}\n\nstatic int mlx4_alloc_resize_buf(struct mlx4_ib_dev *dev, struct mlx4_ib_cq *cq,\n\t\t\t\t  int entries)\n{\n\tint err;\n\n\tif (cq->resize_buf)\n\t\treturn -EBUSY;\n\n\tcq->resize_buf = kmalloc(sizeof *cq->resize_buf, GFP_KERNEL);\n\tif (!cq->resize_buf)\n\t\treturn -ENOMEM;\n\n\terr = mlx4_ib_alloc_cq_buf(dev, &cq->resize_buf->buf, entries);\n\tif (err) {\n\t\tkfree(cq->resize_buf);\n\t\tcq->resize_buf = NULL;\n\t\treturn err;\n\t}\n\n\tcq->resize_buf->cqe = entries - 1;\n\n\treturn 0;\n}\n\nstatic int mlx4_alloc_resize_umem(struct mlx4_ib_dev *dev, struct mlx4_ib_cq *cq,\n\t\t\t\t   int entries, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_resize_cq ucmd;\n\tint err;\n\n\tif (cq->resize_umem)\n\t\treturn -EBUSY;\n\n\tif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd))\n\t\treturn -EFAULT;\n\n\tcq->resize_buf = kmalloc(sizeof *cq->resize_buf, GFP_KERNEL);\n\tif (!cq->resize_buf)\n\t\treturn -ENOMEM;\n\n\terr = mlx4_ib_get_cq_umem(dev, &cq->resize_buf->buf, &cq->resize_umem,\n\t\t\t\t  ucmd.buf_addr, entries);\n\tif (err) {\n\t\tkfree(cq->resize_buf);\n\t\tcq->resize_buf = NULL;\n\t\treturn err;\n\t}\n\n\tcq->resize_buf->cqe = entries - 1;\n\n\treturn 0;\n}\n\nstatic int mlx4_ib_get_outstanding_cqes(struct mlx4_ib_cq *cq)\n{\n\tu32 i;\n\n\ti = cq->mcq.cons_index;\n\twhile (get_sw_cqe(cq, i))\n\t\t++i;\n\n\treturn i - cq->mcq.cons_index;\n}\n\nstatic void mlx4_ib_cq_resize_copy_cqes(struct mlx4_ib_cq *cq)\n{\n\tstruct mlx4_cqe *cqe, *new_cqe;\n\tint i;\n\tint cqe_size = cq->buf.entry_size;\n\tint cqe_inc = cqe_size == 64 ? 1 : 0;\n\n\ti = cq->mcq.cons_index;\n\tcqe = get_cqe(cq, i & cq->ibcq.cqe);\n\tcqe += cqe_inc;\n\n\twhile ((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) != MLX4_CQE_OPCODE_RESIZE) {\n\t\tnew_cqe = get_cqe_from_buf(&cq->resize_buf->buf,\n\t\t\t\t\t   (i + 1) & cq->resize_buf->cqe);\n\t\tmemcpy(new_cqe, get_cqe(cq, i & cq->ibcq.cqe), cqe_size);\n\t\tnew_cqe += cqe_inc;\n\n\t\tnew_cqe->owner_sr_opcode = (cqe->owner_sr_opcode & ~MLX4_CQE_OWNER_MASK) |\n\t\t\t(((i + 1) & (cq->resize_buf->cqe + 1)) ? MLX4_CQE_OWNER_MASK : 0);\n\t\tcqe = get_cqe(cq, ++i & cq->ibcq.cqe);\n\t\tcqe += cqe_inc;\n\t}\n\t++cq->mcq.cons_index;\n}\n\nint mlx4_ib_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(ibcq->device);\n\tstruct mlx4_ib_cq *cq = to_mcq(ibcq);\n\tstruct mlx4_mtt mtt;\n\tint outst_cqe;\n\tint err;\n\n\tmutex_lock(&cq->resize_mutex);\n\tif (entries < 1 || entries > dev->dev->caps.max_cqes) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tentries = roundup_pow_of_two(entries + 1);\n\tif (entries == ibcq->cqe + 1) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tif (entries > dev->dev->caps.max_cqes + 1) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (ibcq->uobject) {\n\t\terr = mlx4_alloc_resize_umem(dev, cq, entries, udata);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t \n\t\toutst_cqe = mlx4_ib_get_outstanding_cqes(cq);\n\t\tif (entries < outst_cqe + 1) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = mlx4_alloc_resize_buf(dev, cq, entries);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tmtt = cq->buf.mtt;\n\n\terr = mlx4_cq_resize(dev->dev, &cq->mcq, entries, &cq->resize_buf->buf.mtt);\n\tif (err)\n\t\tgoto err_buf;\n\n\tmlx4_mtt_cleanup(dev->dev, &mtt);\n\tif (ibcq->uobject) {\n\t\tcq->buf      = cq->resize_buf->buf;\n\t\tcq->ibcq.cqe = cq->resize_buf->cqe;\n\t\tib_umem_release(cq->umem);\n\t\tcq->umem     = cq->resize_umem;\n\n\t\tkfree(cq->resize_buf);\n\t\tcq->resize_buf = NULL;\n\t\tcq->resize_umem = NULL;\n\t} else {\n\t\tstruct mlx4_ib_cq_buf tmp_buf;\n\t\tint tmp_cqe = 0;\n\n\t\tspin_lock_irq(&cq->lock);\n\t\tif (cq->resize_buf) {\n\t\t\tmlx4_ib_cq_resize_copy_cqes(cq);\n\t\t\ttmp_buf = cq->buf;\n\t\t\ttmp_cqe = cq->ibcq.cqe;\n\t\t\tcq->buf      = cq->resize_buf->buf;\n\t\t\tcq->ibcq.cqe = cq->resize_buf->cqe;\n\n\t\t\tkfree(cq->resize_buf);\n\t\t\tcq->resize_buf = NULL;\n\t\t}\n\t\tspin_unlock_irq(&cq->lock);\n\n\t\tif (tmp_cqe)\n\t\t\tmlx4_ib_free_cq_buf(dev, &tmp_buf, tmp_cqe);\n\t}\n\n\tgoto out;\n\nerr_buf:\n\tmlx4_mtt_cleanup(dev->dev, &cq->resize_buf->buf.mtt);\n\tif (!ibcq->uobject)\n\t\tmlx4_ib_free_cq_buf(dev, &cq->resize_buf->buf,\n\t\t\t\t    cq->resize_buf->cqe);\n\n\tkfree(cq->resize_buf);\n\tcq->resize_buf = NULL;\n\n\tib_umem_release(cq->resize_umem);\n\tcq->resize_umem = NULL;\nout:\n\tmutex_unlock(&cq->resize_mutex);\n\n\treturn err;\n}\n\nint mlx4_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata)\n{\n\tstruct mlx4_ib_dev *dev = to_mdev(cq->device);\n\tstruct mlx4_ib_cq *mcq = to_mcq(cq);\n\n\tmlx4_cq_free(dev->dev, &mcq->mcq);\n\tmlx4_mtt_cleanup(dev->dev, &mcq->buf.mtt);\n\n\tif (udata) {\n\t\tmlx4_ib_db_unmap_user(\n\t\t\trdma_udata_to_drv_context(\n\t\t\t\tudata,\n\t\t\t\tstruct mlx4_ib_ucontext,\n\t\t\t\tibucontext),\n\t\t\t&mcq->db);\n\t} else {\n\t\tmlx4_ib_free_cq_buf(dev, &mcq->buf, cq->cqe);\n\t\tmlx4_db_free(dev->dev, &mcq->db);\n\t}\n\tib_umem_release(mcq->umem);\n\treturn 0;\n}\n\nstatic void dump_cqe(void *cqe)\n{\n\t__be32 *buf = cqe;\n\n\tpr_debug(\"CQE contents %08x %08x %08x %08x %08x %08x %08x %08x\\n\",\n\t       be32_to_cpu(buf[0]), be32_to_cpu(buf[1]), be32_to_cpu(buf[2]),\n\t       be32_to_cpu(buf[3]), be32_to_cpu(buf[4]), be32_to_cpu(buf[5]),\n\t       be32_to_cpu(buf[6]), be32_to_cpu(buf[7]));\n}\n\nstatic void mlx4_ib_handle_error_cqe(struct mlx4_err_cqe *cqe,\n\t\t\t\t     struct ib_wc *wc)\n{\n\tif (cqe->syndrome == MLX4_CQE_SYNDROME_LOCAL_QP_OP_ERR) {\n\t\tpr_debug(\"local QP operation err \"\n\t\t       \"(QPN %06x, WQE index %x, vendor syndrome %02x, \"\n\t\t       \"opcode = %02x)\\n\",\n\t\t       be32_to_cpu(cqe->my_qpn), be16_to_cpu(cqe->wqe_index),\n\t\t       cqe->vendor_err_syndrome,\n\t\t       cqe->owner_sr_opcode & ~MLX4_CQE_OWNER_MASK);\n\t\tdump_cqe(cqe);\n\t}\n\n\tswitch (cqe->syndrome) {\n\tcase MLX4_CQE_SYNDROME_LOCAL_LENGTH_ERR:\n\t\twc->status = IB_WC_LOC_LEN_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_LOCAL_QP_OP_ERR:\n\t\twc->status = IB_WC_LOC_QP_OP_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_LOCAL_PROT_ERR:\n\t\twc->status = IB_WC_LOC_PROT_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_WR_FLUSH_ERR:\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_MW_BIND_ERR:\n\t\twc->status = IB_WC_MW_BIND_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_BAD_RESP_ERR:\n\t\twc->status = IB_WC_BAD_RESP_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_LOCAL_ACCESS_ERR:\n\t\twc->status = IB_WC_LOC_ACCESS_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_REMOTE_INVAL_REQ_ERR:\n\t\twc->status = IB_WC_REM_INV_REQ_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_REMOTE_ACCESS_ERR:\n\t\twc->status = IB_WC_REM_ACCESS_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_REMOTE_OP_ERR:\n\t\twc->status = IB_WC_REM_OP_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_TRANSPORT_RETRY_EXC_ERR:\n\t\twc->status = IB_WC_RETRY_EXC_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_RNR_RETRY_EXC_ERR:\n\t\twc->status = IB_WC_RNR_RETRY_EXC_ERR;\n\t\tbreak;\n\tcase MLX4_CQE_SYNDROME_REMOTE_ABORTED_ERR:\n\t\twc->status = IB_WC_REM_ABORT_ERR;\n\t\tbreak;\n\tdefault:\n\t\twc->status = IB_WC_GENERAL_ERR;\n\t\tbreak;\n\t}\n\n\twc->vendor_err = cqe->vendor_err_syndrome;\n}\n\nstatic int mlx4_ib_ipoib_csum_ok(__be16 status, u8 badfcs_enc, __be16 checksum)\n{\n\treturn ((badfcs_enc & MLX4_CQE_STATUS_L4_CSUM) ||\n\t\t((status & cpu_to_be16(MLX4_CQE_STATUS_IPOK)) &&\n\t\t (status & cpu_to_be16(MLX4_CQE_STATUS_TCP |\n\t\t\t\t       MLX4_CQE_STATUS_UDP)) &&\n\t\t (checksum == cpu_to_be16(0xffff))));\n}\n\nstatic void use_tunnel_data(struct mlx4_ib_qp *qp, struct mlx4_ib_cq *cq, struct ib_wc *wc,\n\t\t\t    unsigned tail, struct mlx4_cqe *cqe, int is_eth)\n{\n\tstruct mlx4_ib_proxy_sqp_hdr *hdr;\n\n\tib_dma_sync_single_for_cpu(qp->ibqp.device,\n\t\t\t\t   qp->sqp_proxy_rcv[tail].map,\n\t\t\t\t   sizeof (struct mlx4_ib_proxy_sqp_hdr),\n\t\t\t\t   DMA_FROM_DEVICE);\n\thdr = (struct mlx4_ib_proxy_sqp_hdr *) (qp->sqp_proxy_rcv[tail].addr);\n\twc->pkey_index\t= be16_to_cpu(hdr->tun.pkey_index);\n\twc->src_qp\t= be32_to_cpu(hdr->tun.flags_src_qp) & 0xFFFFFF;\n\twc->wc_flags   |= (hdr->tun.g_ml_path & 0x80) ? (IB_WC_GRH) : 0;\n\twc->dlid_path_bits = 0;\n\n\tif (is_eth) {\n\t\twc->slid = 0;\n\t\twc->vlan_id = be16_to_cpu(hdr->tun.sl_vid);\n\t\tmemcpy(&(wc->smac[0]), (char *)&hdr->tun.mac_31_0, 4);\n\t\tmemcpy(&(wc->smac[4]), (char *)&hdr->tun.slid_mac_47_32, 2);\n\t\twc->wc_flags |= (IB_WC_WITH_VLAN | IB_WC_WITH_SMAC);\n\t} else {\n\t\twc->slid        = be16_to_cpu(hdr->tun.slid_mac_47_32);\n\t\twc->sl          = (u8) (be16_to_cpu(hdr->tun.sl_vid) >> 12);\n\t}\n}\n\nstatic void mlx4_ib_qp_sw_comp(struct mlx4_ib_qp *qp, int num_entries,\n\t\t\t       struct ib_wc *wc, int *npolled, int is_send)\n{\n\tstruct mlx4_ib_wq *wq;\n\tunsigned cur;\n\tint i;\n\n\twq = is_send ? &qp->sq : &qp->rq;\n\tcur = wq->head - wq->tail;\n\n\tif (cur == 0)\n\t\treturn;\n\n\tfor (i = 0;  i < cur && *npolled < num_entries; i++) {\n\t\twc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\twc->vendor_err = MLX4_CQE_SYNDROME_WR_FLUSH_ERR;\n\t\twq->tail++;\n\t\t(*npolled)++;\n\t\twc->qp = &qp->ibqp;\n\t\twc++;\n\t}\n}\n\nstatic void mlx4_ib_poll_sw_comp(struct mlx4_ib_cq *cq, int num_entries,\n\t\t\t\t struct ib_wc *wc, int *npolled)\n{\n\tstruct mlx4_ib_qp *qp;\n\n\t*npolled = 0;\n\t \n\tlist_for_each_entry(qp, &cq->send_qp_list, cq_send_list) {\n\t\tmlx4_ib_qp_sw_comp(qp, num_entries, wc + *npolled, npolled, 1);\n\t\tif (*npolled >= num_entries)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(qp, &cq->recv_qp_list, cq_recv_list) {\n\t\tmlx4_ib_qp_sw_comp(qp, num_entries, wc + *npolled, npolled, 0);\n\t\tif (*npolled >= num_entries)\n\t\t\tgoto out;\n\t}\n\nout:\n\treturn;\n}\n\nstatic int mlx4_ib_poll_one(struct mlx4_ib_cq *cq,\n\t\t\t    struct mlx4_ib_qp **cur_qp,\n\t\t\t    struct ib_wc *wc)\n{\n\tstruct mlx4_cqe *cqe;\n\tstruct mlx4_qp *mqp;\n\tstruct mlx4_ib_wq *wq;\n\tstruct mlx4_ib_srq *srq;\n\tstruct mlx4_srq *msrq = NULL;\n\tint is_send;\n\tint is_error;\n\tint is_eth;\n\tu32 g_mlpath_rqpn;\n\tu16 wqe_ctr;\n\tunsigned tail = 0;\n\nrepoll:\n\tcqe = next_cqe_sw(cq);\n\tif (!cqe)\n\t\treturn -EAGAIN;\n\n\tif (cq->buf.entry_size == 64)\n\t\tcqe++;\n\n\t++cq->mcq.cons_index;\n\n\t \n\trmb();\n\n\tis_send  = cqe->owner_sr_opcode & MLX4_CQE_IS_SEND_MASK;\n\tis_error = (cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==\n\t\tMLX4_CQE_OPCODE_ERROR;\n\n\t \n\tif (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) == MLX4_CQE_OPCODE_RESIZE)) {\n\t\tif (cq->resize_buf) {\n\t\t\tstruct mlx4_ib_dev *dev = to_mdev(cq->ibcq.device);\n\n\t\t\tmlx4_ib_free_cq_buf(dev, &cq->buf, cq->ibcq.cqe);\n\t\t\tcq->buf      = cq->resize_buf->buf;\n\t\t\tcq->ibcq.cqe = cq->resize_buf->cqe;\n\n\t\t\tkfree(cq->resize_buf);\n\t\t\tcq->resize_buf = NULL;\n\t\t}\n\n\t\tgoto repoll;\n\t}\n\n\tif (!*cur_qp ||\n\t    (be32_to_cpu(cqe->vlan_my_qpn) & MLX4_CQE_QPN_MASK) != (*cur_qp)->mqp.qpn) {\n\t\t \n\t\tmqp = __mlx4_qp_lookup(to_mdev(cq->ibcq.device)->dev,\n\t\t\t\t       be32_to_cpu(cqe->vlan_my_qpn));\n\t\t*cur_qp = to_mibqp(mqp);\n\t}\n\n\twc->qp = &(*cur_qp)->ibqp;\n\n\tif (wc->qp->qp_type == IB_QPT_XRC_TGT) {\n\t\tu32 srq_num;\n\t\tg_mlpath_rqpn = be32_to_cpu(cqe->g_mlpath_rqpn);\n\t\tsrq_num       = g_mlpath_rqpn & 0xffffff;\n\t\t \n\t\tmsrq = mlx4_srq_lookup(to_mdev(cq->ibcq.device)->dev,\n\t\t\t\t       srq_num);\n\t}\n\n\tif (is_send) {\n\t\twq = &(*cur_qp)->sq;\n\t\tif (!(*cur_qp)->sq_signal_bits) {\n\t\t\twqe_ctr = be16_to_cpu(cqe->wqe_index);\n\t\t\twq->tail += (u16) (wqe_ctr - (u16) wq->tail);\n\t\t}\n\t\twc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];\n\t\t++wq->tail;\n\t} else if ((*cur_qp)->ibqp.srq) {\n\t\tsrq = to_msrq((*cur_qp)->ibqp.srq);\n\t\twqe_ctr = be16_to_cpu(cqe->wqe_index);\n\t\twc->wr_id = srq->wrid[wqe_ctr];\n\t\tmlx4_ib_free_srq_wqe(srq, wqe_ctr);\n\t} else if (msrq) {\n\t\tsrq = to_mibsrq(msrq);\n\t\twqe_ctr = be16_to_cpu(cqe->wqe_index);\n\t\twc->wr_id = srq->wrid[wqe_ctr];\n\t\tmlx4_ib_free_srq_wqe(srq, wqe_ctr);\n\t} else {\n\t\twq\t  = &(*cur_qp)->rq;\n\t\ttail\t  = wq->tail & (wq->wqe_cnt - 1);\n\t\twc->wr_id = wq->wrid[tail];\n\t\t++wq->tail;\n\t}\n\n\tif (unlikely(is_error)) {\n\t\tmlx4_ib_handle_error_cqe((struct mlx4_err_cqe *) cqe, wc);\n\t\treturn 0;\n\t}\n\n\twc->status = IB_WC_SUCCESS;\n\n\tif (is_send) {\n\t\twc->wc_flags = 0;\n\t\tswitch (cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) {\n\t\tcase MLX4_OPCODE_RDMA_WRITE_IMM:\n\t\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\t\tfallthrough;\n\t\tcase MLX4_OPCODE_RDMA_WRITE:\n\t\t\twc->opcode    = IB_WC_RDMA_WRITE;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_SEND_IMM:\n\t\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\t\tfallthrough;\n\t\tcase MLX4_OPCODE_SEND:\n\t\tcase MLX4_OPCODE_SEND_INVAL:\n\t\t\twc->opcode    = IB_WC_SEND;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_RDMA_READ:\n\t\t\twc->opcode    = IB_WC_RDMA_READ;\n\t\t\twc->byte_len  = be32_to_cpu(cqe->byte_cnt);\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_ATOMIC_CS:\n\t\t\twc->opcode    = IB_WC_COMP_SWAP;\n\t\t\twc->byte_len  = 8;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_ATOMIC_FA:\n\t\t\twc->opcode    = IB_WC_FETCH_ADD;\n\t\t\twc->byte_len  = 8;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_MASKED_ATOMIC_CS:\n\t\t\twc->opcode    = IB_WC_MASKED_COMP_SWAP;\n\t\t\twc->byte_len  = 8;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_MASKED_ATOMIC_FA:\n\t\t\twc->opcode    = IB_WC_MASKED_FETCH_ADD;\n\t\t\twc->byte_len  = 8;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_LSO:\n\t\t\twc->opcode    = IB_WC_LSO;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_FMR:\n\t\t\twc->opcode    = IB_WC_REG_MR;\n\t\t\tbreak;\n\t\tcase MLX4_OPCODE_LOCAL_INVAL:\n\t\t\twc->opcode    = IB_WC_LOCAL_INV;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\twc->byte_len = be32_to_cpu(cqe->byte_cnt);\n\n\t\tswitch (cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) {\n\t\tcase MLX4_RECV_OPCODE_RDMA_WRITE_IMM:\n\t\t\twc->opcode\t= IB_WC_RECV_RDMA_WITH_IMM;\n\t\t\twc->wc_flags\t= IB_WC_WITH_IMM;\n\t\t\twc->ex.imm_data = cqe->immed_rss_invalid;\n\t\t\tbreak;\n\t\tcase MLX4_RECV_OPCODE_SEND_INVAL:\n\t\t\twc->opcode\t= IB_WC_RECV;\n\t\t\twc->wc_flags\t= IB_WC_WITH_INVALIDATE;\n\t\t\twc->ex.invalidate_rkey = be32_to_cpu(cqe->immed_rss_invalid);\n\t\t\tbreak;\n\t\tcase MLX4_RECV_OPCODE_SEND:\n\t\t\twc->opcode   = IB_WC_RECV;\n\t\t\twc->wc_flags = 0;\n\t\t\tbreak;\n\t\tcase MLX4_RECV_OPCODE_SEND_IMM:\n\t\t\twc->opcode\t= IB_WC_RECV;\n\t\t\twc->wc_flags\t= IB_WC_WITH_IMM;\n\t\t\twc->ex.imm_data = cqe->immed_rss_invalid;\n\t\t\tbreak;\n\t\t}\n\n\t\tis_eth = (rdma_port_get_link_layer(wc->qp->device,\n\t\t\t\t\t\t  (*cur_qp)->port) ==\n\t\t\t  IB_LINK_LAYER_ETHERNET);\n\t\tif (mlx4_is_mfunc(to_mdev(cq->ibcq.device)->dev)) {\n\t\t\tif ((*cur_qp)->mlx4_ib_qp_type &\n\t\t\t    (MLX4_IB_QPT_PROXY_SMI_OWNER |\n\t\t\t     MLX4_IB_QPT_PROXY_SMI | MLX4_IB_QPT_PROXY_GSI)) {\n\t\t\t\tuse_tunnel_data(*cur_qp, cq, wc, tail, cqe,\n\t\t\t\t\t\tis_eth);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tg_mlpath_rqpn\t   = be32_to_cpu(cqe->g_mlpath_rqpn);\n\t\twc->src_qp\t   = g_mlpath_rqpn & 0xffffff;\n\t\twc->dlid_path_bits = (g_mlpath_rqpn >> 24) & 0x7f;\n\t\twc->wc_flags\t  |= g_mlpath_rqpn & 0x80000000 ? IB_WC_GRH : 0;\n\t\twc->pkey_index     = be32_to_cpu(cqe->immed_rss_invalid) & 0x7f;\n\t\twc->wc_flags\t  |= mlx4_ib_ipoib_csum_ok(cqe->status,\n\t\t\t\t\tcqe->badfcs_enc,\n\t\t\t\t\tcqe->checksum) ? IB_WC_IP_CSUM_OK : 0;\n\t\tif (is_eth) {\n\t\t\twc->slid = 0;\n\t\t\twc->sl  = be16_to_cpu(cqe->sl_vid) >> 13;\n\t\t\tif (be32_to_cpu(cqe->vlan_my_qpn) &\n\t\t\t\t\tMLX4_CQE_CVLAN_PRESENT_MASK) {\n\t\t\t\twc->vlan_id = be16_to_cpu(cqe->sl_vid) &\n\t\t\t\t\tMLX4_CQE_VID_MASK;\n\t\t\t} else {\n\t\t\t\twc->vlan_id = 0xffff;\n\t\t\t}\n\t\t\tmemcpy(wc->smac, cqe->smac, ETH_ALEN);\n\t\t\twc->wc_flags |= (IB_WC_WITH_VLAN | IB_WC_WITH_SMAC);\n\t\t} else {\n\t\t\twc->slid = be16_to_cpu(cqe->rlid);\n\t\t\twc->sl  = be16_to_cpu(cqe->sl_vid) >> 12;\n\t\t\twc->vlan_id = 0xffff;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint mlx4_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tstruct mlx4_ib_cq *cq = to_mcq(ibcq);\n\tstruct mlx4_ib_qp *cur_qp = NULL;\n\tunsigned long flags;\n\tint npolled;\n\tstruct mlx4_ib_dev *mdev = to_mdev(cq->ibcq.device);\n\n\tspin_lock_irqsave(&cq->lock, flags);\n\tif (mdev->dev->persist->state & MLX4_DEVICE_STATE_INTERNAL_ERROR) {\n\t\tmlx4_ib_poll_sw_comp(cq, num_entries, wc, &npolled);\n\t\tgoto out;\n\t}\n\n\tfor (npolled = 0; npolled < num_entries; ++npolled) {\n\t\tif (mlx4_ib_poll_one(cq, &cur_qp, wc + npolled))\n\t\t\tbreak;\n\t}\n\n\tmlx4_cq_set_ci(&cq->mcq);\n\nout:\n\tspin_unlock_irqrestore(&cq->lock, flags);\n\n\treturn npolled;\n}\n\nint mlx4_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags)\n{\n\tmlx4_cq_arm(&to_mcq(ibcq)->mcq,\n\t\t    (flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED ?\n\t\t    MLX4_CQ_DB_REQ_NOT_SOL : MLX4_CQ_DB_REQ_NOT,\n\t\t    to_mdev(ibcq->device)->uar_map,\n\t\t    MLX4_GET_DOORBELL_LOCK(&to_mdev(ibcq->device)->uar_lock));\n\n\treturn 0;\n}\n\nvoid __mlx4_ib_cq_clean(struct mlx4_ib_cq *cq, u32 qpn, struct mlx4_ib_srq *srq)\n{\n\tu32 prod_index;\n\tint nfreed = 0;\n\tstruct mlx4_cqe *cqe, *dest;\n\tu8 owner_bit;\n\tint cqe_inc = cq->buf.entry_size == 64 ? 1 : 0;\n\n\t \n\tfor (prod_index = cq->mcq.cons_index; get_sw_cqe(cq, prod_index); ++prod_index)\n\t\tif (prod_index == cq->mcq.cons_index + cq->ibcq.cqe)\n\t\t\tbreak;\n\n\t \n\twhile ((int) --prod_index - (int) cq->mcq.cons_index >= 0) {\n\t\tcqe = get_cqe(cq, prod_index & cq->ibcq.cqe);\n\t\tcqe += cqe_inc;\n\n\t\tif ((be32_to_cpu(cqe->vlan_my_qpn) & MLX4_CQE_QPN_MASK) == qpn) {\n\t\t\tif (srq && !(cqe->owner_sr_opcode & MLX4_CQE_IS_SEND_MASK))\n\t\t\t\tmlx4_ib_free_srq_wqe(srq, be16_to_cpu(cqe->wqe_index));\n\t\t\t++nfreed;\n\t\t} else if (nfreed) {\n\t\t\tdest = get_cqe(cq, (prod_index + nfreed) & cq->ibcq.cqe);\n\t\t\tdest += cqe_inc;\n\n\t\t\towner_bit = dest->owner_sr_opcode & MLX4_CQE_OWNER_MASK;\n\t\t\tmemcpy(dest, cqe, sizeof *cqe);\n\t\t\tdest->owner_sr_opcode = owner_bit |\n\t\t\t\t(dest->owner_sr_opcode & ~MLX4_CQE_OWNER_MASK);\n\t\t}\n\t}\n\n\tif (nfreed) {\n\t\tcq->mcq.cons_index += nfreed;\n\t\t \n\t\twmb();\n\t\tmlx4_cq_set_ci(&cq->mcq);\n\t}\n}\n\nvoid mlx4_ib_cq_clean(struct mlx4_ib_cq *cq, u32 qpn, struct mlx4_ib_srq *srq)\n{\n\tspin_lock_irq(&cq->lock);\n\t__mlx4_ib_cq_clean(cq, qpn, srq);\n\tspin_unlock_irq(&cq->lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}