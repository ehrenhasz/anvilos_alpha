{
  "module_name": "cm.c",
  "hash_id": "6d6facc3d46f23ff0504d3e652d1489d4db183ff4506c978814f326f0685cbf2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/cm.c",
  "human_readable_source": " \n\n#include <rdma/ib_mad.h>\n\n#include <linux/mlx4/cmd.h>\n#include <linux/rbtree.h>\n#include <linux/idr.h>\n#include <rdma/ib_cm.h>\n\n#include \"mlx4_ib.h\"\n\n#define CM_CLEANUP_CACHE_TIMEOUT  (30 * HZ)\n\nstruct id_map_entry {\n\tstruct rb_node node;\n\n\tu32 sl_cm_id;\n\tu32 pv_cm_id;\n\tint slave_id;\n\tint scheduled_delete;\n\tstruct mlx4_ib_dev *dev;\n\n\tstruct list_head list;\n\tstruct delayed_work timeout;\n};\n\nstruct rej_tmout_entry {\n\tint slave;\n\tu32 rem_pv_cm_id;\n\tstruct delayed_work timeout;\n\tstruct xarray *xa_rej_tmout;\n};\n\nstruct cm_generic_msg {\n\tstruct ib_mad_hdr hdr;\n\n\t__be32 local_comm_id;\n\t__be32 remote_comm_id;\n\tunsigned char unused[2];\n\t__be16 rej_reason;\n};\n\nstruct cm_sidr_generic_msg {\n\tstruct ib_mad_hdr hdr;\n\t__be32 request_id;\n};\n\nstruct cm_req_msg {\n\tunsigned char unused[0x60];\n\tunion ib_gid primary_path_sgid;\n};\n\nstatic struct workqueue_struct *cm_wq;\n\nstatic void set_local_comm_id(struct ib_mad *mad, u32 cm_id)\n{\n\tif (mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID) {\n\t\tstruct cm_sidr_generic_msg *msg =\n\t\t\t(struct cm_sidr_generic_msg *)mad;\n\t\tmsg->request_id = cpu_to_be32(cm_id);\n\t} else if (mad->mad_hdr.attr_id == CM_SIDR_REP_ATTR_ID) {\n\t\tpr_err(\"trying to set local_comm_id in SIDR_REP\\n\");\n\t\treturn;\n\t} else {\n\t\tstruct cm_generic_msg *msg = (struct cm_generic_msg *)mad;\n\t\tmsg->local_comm_id = cpu_to_be32(cm_id);\n\t}\n}\n\nstatic u32 get_local_comm_id(struct ib_mad *mad)\n{\n\tif (mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID) {\n\t\tstruct cm_sidr_generic_msg *msg =\n\t\t\t(struct cm_sidr_generic_msg *)mad;\n\t\treturn be32_to_cpu(msg->request_id);\n\t} else if (mad->mad_hdr.attr_id == CM_SIDR_REP_ATTR_ID) {\n\t\tpr_err(\"trying to set local_comm_id in SIDR_REP\\n\");\n\t\treturn -1;\n\t} else {\n\t\tstruct cm_generic_msg *msg = (struct cm_generic_msg *)mad;\n\t\treturn be32_to_cpu(msg->local_comm_id);\n\t}\n}\n\nstatic void set_remote_comm_id(struct ib_mad *mad, u32 cm_id)\n{\n\tif (mad->mad_hdr.attr_id == CM_SIDR_REP_ATTR_ID) {\n\t\tstruct cm_sidr_generic_msg *msg =\n\t\t\t(struct cm_sidr_generic_msg *)mad;\n\t\tmsg->request_id = cpu_to_be32(cm_id);\n\t} else if (mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID) {\n\t\tpr_err(\"trying to set remote_comm_id in SIDR_REQ\\n\");\n\t\treturn;\n\t} else {\n\t\tstruct cm_generic_msg *msg = (struct cm_generic_msg *)mad;\n\t\tmsg->remote_comm_id = cpu_to_be32(cm_id);\n\t}\n}\n\nstatic u32 get_remote_comm_id(struct ib_mad *mad)\n{\n\tif (mad->mad_hdr.attr_id == CM_SIDR_REP_ATTR_ID) {\n\t\tstruct cm_sidr_generic_msg *msg =\n\t\t\t(struct cm_sidr_generic_msg *)mad;\n\t\treturn be32_to_cpu(msg->request_id);\n\t} else if (mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID) {\n\t\tpr_err(\"trying to set remote_comm_id in SIDR_REQ\\n\");\n\t\treturn -1;\n\t} else {\n\t\tstruct cm_generic_msg *msg = (struct cm_generic_msg *)mad;\n\t\treturn be32_to_cpu(msg->remote_comm_id);\n\t}\n}\n\nstatic union ib_gid gid_from_req_msg(struct ib_device *ibdev, struct ib_mad *mad)\n{\n\tstruct cm_req_msg *msg = (struct cm_req_msg *)mad;\n\n\treturn msg->primary_path_sgid;\n}\n\n \nstatic struct id_map_entry *\nid_map_find_by_sl_id(struct ib_device *ibdev, u32 slave_id, u32 sl_cm_id)\n{\n\tstruct rb_root *sl_id_map = &to_mdev(ibdev)->sriov.sl_id_map;\n\tstruct rb_node *node = sl_id_map->rb_node;\n\n\twhile (node) {\n\t\tstruct id_map_entry *id_map_entry =\n\t\t\trb_entry(node, struct id_map_entry, node);\n\n\t\tif (id_map_entry->sl_cm_id > sl_cm_id)\n\t\t\tnode = node->rb_left;\n\t\telse if (id_map_entry->sl_cm_id < sl_cm_id)\n\t\t\tnode = node->rb_right;\n\t\telse if (id_map_entry->slave_id > slave_id)\n\t\t\tnode = node->rb_left;\n\t\telse if (id_map_entry->slave_id < slave_id)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn id_map_entry;\n\t}\n\treturn NULL;\n}\n\nstatic void id_map_ent_timeout(struct work_struct *work)\n{\n\tstruct delayed_work *delay = to_delayed_work(work);\n\tstruct id_map_entry *ent = container_of(delay, struct id_map_entry, timeout);\n\tstruct id_map_entry *found_ent;\n\tstruct mlx4_ib_dev *dev = ent->dev;\n\tstruct mlx4_ib_sriov *sriov = &dev->sriov;\n\tstruct rb_root *sl_id_map = &sriov->sl_id_map;\n\n\tspin_lock(&sriov->id_map_lock);\n\tif (!xa_erase(&sriov->pv_id_table, ent->pv_cm_id))\n\t\tgoto out;\n\tfound_ent = id_map_find_by_sl_id(&dev->ib_dev, ent->slave_id, ent->sl_cm_id);\n\tif (found_ent && found_ent == ent)\n\t\trb_erase(&found_ent->node, sl_id_map);\n\nout:\n\tlist_del(&ent->list);\n\tspin_unlock(&sriov->id_map_lock);\n\tkfree(ent);\n}\n\nstatic void sl_id_map_add(struct ib_device *ibdev, struct id_map_entry *new)\n{\n\tstruct rb_root *sl_id_map = &to_mdev(ibdev)->sriov.sl_id_map;\n\tstruct rb_node **link = &sl_id_map->rb_node, *parent = NULL;\n\tstruct id_map_entry *ent;\n\tint slave_id = new->slave_id;\n\tint sl_cm_id = new->sl_cm_id;\n\n\tent = id_map_find_by_sl_id(ibdev, slave_id, sl_cm_id);\n\tif (ent) {\n\t\tpr_debug(\"overriding existing sl_id_map entry (cm_id = %x)\\n\",\n\t\t\t sl_cm_id);\n\n\t\trb_replace_node(&ent->node, &new->node, sl_id_map);\n\t\treturn;\n\t}\n\n\t \n\twhile (*link) {\n\t\tparent = *link;\n\t\tent = rb_entry(parent, struct id_map_entry, node);\n\n\t\tif (ent->sl_cm_id > sl_cm_id || (ent->sl_cm_id == sl_cm_id && ent->slave_id > slave_id))\n\t\t\tlink = &(*link)->rb_left;\n\t\telse\n\t\t\tlink = &(*link)->rb_right;\n\t}\n\n\trb_link_node(&new->node, parent, link);\n\trb_insert_color(&new->node, sl_id_map);\n}\n\nstatic struct id_map_entry *\nid_map_alloc(struct ib_device *ibdev, int slave_id, u32 sl_cm_id)\n{\n\tint ret;\n\tstruct id_map_entry *ent;\n\tstruct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;\n\n\tent = kmalloc(sizeof (struct id_map_entry), GFP_KERNEL);\n\tif (!ent)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tent->sl_cm_id = sl_cm_id;\n\tent->slave_id = slave_id;\n\tent->scheduled_delete = 0;\n\tent->dev = to_mdev(ibdev);\n\tINIT_DELAYED_WORK(&ent->timeout, id_map_ent_timeout);\n\n\tret = xa_alloc_cyclic(&sriov->pv_id_table, &ent->pv_cm_id, ent,\n\t\t\txa_limit_32b, &sriov->pv_id_next, GFP_KERNEL);\n\tif (ret >= 0) {\n\t\tspin_lock(&sriov->id_map_lock);\n\t\tsl_id_map_add(ibdev, ent);\n\t\tlist_add_tail(&ent->list, &sriov->cm_list);\n\t\tspin_unlock(&sriov->id_map_lock);\n\t\treturn ent;\n\t}\n\n\t \n\tkfree(ent);\n\tmlx4_ib_warn(ibdev, \"Allocation failed (err:0x%x)\\n\", ret);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic struct id_map_entry *\nid_map_get(struct ib_device *ibdev, int *pv_cm_id, int slave_id, int sl_cm_id)\n{\n\tstruct id_map_entry *ent;\n\tstruct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;\n\n\tspin_lock(&sriov->id_map_lock);\n\tif (*pv_cm_id == -1) {\n\t\tent = id_map_find_by_sl_id(ibdev, slave_id, sl_cm_id);\n\t\tif (ent)\n\t\t\t*pv_cm_id = (int) ent->pv_cm_id;\n\t} else\n\t\tent = xa_load(&sriov->pv_id_table, *pv_cm_id);\n\tspin_unlock(&sriov->id_map_lock);\n\n\treturn ent;\n}\n\nstatic void schedule_delayed(struct ib_device *ibdev, struct id_map_entry *id)\n{\n\tstruct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;\n\tunsigned long flags;\n\n\tspin_lock(&sriov->id_map_lock);\n\tspin_lock_irqsave(&sriov->going_down_lock, flags);\n\t \n\tif (!sriov->is_going_down && !id->scheduled_delete) {\n\t\tid->scheduled_delete = 1;\n\t\tqueue_delayed_work(cm_wq, &id->timeout, CM_CLEANUP_CACHE_TIMEOUT);\n\t} else if (id->scheduled_delete) {\n\t\t \n\t\tmod_delayed_work(cm_wq, &id->timeout, CM_CLEANUP_CACHE_TIMEOUT);\n\t}\n\tspin_unlock_irqrestore(&sriov->going_down_lock, flags);\n\tspin_unlock(&sriov->id_map_lock);\n}\n\n#define REJ_REASON(m) be16_to_cpu(((struct cm_generic_msg *)(m))->rej_reason)\nint mlx4_ib_multiplex_cm_handler(struct ib_device *ibdev, int port, int slave_id,\n\t\tstruct ib_mad *mad)\n{\n\tstruct id_map_entry *id;\n\tu32 sl_cm_id;\n\tint pv_cm_id = -1;\n\n\tif (mad->mad_hdr.attr_id == CM_REQ_ATTR_ID ||\n\t    mad->mad_hdr.attr_id == CM_REP_ATTR_ID ||\n\t    mad->mad_hdr.attr_id == CM_MRA_ATTR_ID ||\n\t    mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID ||\n\t    (mad->mad_hdr.attr_id == CM_REJ_ATTR_ID && REJ_REASON(mad) == IB_CM_REJ_TIMEOUT)) {\n\t\tsl_cm_id = get_local_comm_id(mad);\n\t\tid = id_map_get(ibdev, &pv_cm_id, slave_id, sl_cm_id);\n\t\tif (id)\n\t\t\tgoto cont;\n\t\tid = id_map_alloc(ibdev, slave_id, sl_cm_id);\n\t\tif (IS_ERR(id)) {\n\t\t\tmlx4_ib_warn(ibdev, \"%s: id{slave: %d, sl_cm_id: 0x%x} Failed to id_map_alloc\\n\",\n\t\t\t\t__func__, slave_id, sl_cm_id);\n\t\t\treturn PTR_ERR(id);\n\t\t}\n\t} else if (mad->mad_hdr.attr_id == CM_REJ_ATTR_ID ||\n\t\t   mad->mad_hdr.attr_id == CM_SIDR_REP_ATTR_ID) {\n\t\treturn 0;\n\t} else {\n\t\tsl_cm_id = get_local_comm_id(mad);\n\t\tid = id_map_get(ibdev, &pv_cm_id, slave_id, sl_cm_id);\n\t}\n\n\tif (!id) {\n\t\tpr_debug(\"id{slave: %d, sl_cm_id: 0x%x} is NULL! attr_id: 0x%x\\n\",\n\t\t\t slave_id, sl_cm_id, be16_to_cpu(mad->mad_hdr.attr_id));\n\t\treturn -EINVAL;\n\t}\n\ncont:\n\tset_local_comm_id(mad, id->pv_cm_id);\n\n\tif (mad->mad_hdr.attr_id == CM_DREQ_ATTR_ID)\n\t\tschedule_delayed(ibdev, id);\n\treturn 0;\n}\n\nstatic void rej_tmout_timeout(struct work_struct *work)\n{\n\tstruct delayed_work *delay = to_delayed_work(work);\n\tstruct rej_tmout_entry *item = container_of(delay, struct rej_tmout_entry, timeout);\n\tstruct rej_tmout_entry *deleted;\n\n\tdeleted = xa_cmpxchg(item->xa_rej_tmout, item->rem_pv_cm_id, item, NULL, 0);\n\n\tif (deleted != item)\n\t\tpr_debug(\"deleted(%p) != item(%p)\\n\", deleted, item);\n\n\tkfree(item);\n}\n\nstatic int alloc_rej_tmout(struct mlx4_ib_sriov *sriov, u32 rem_pv_cm_id, int slave)\n{\n\tstruct rej_tmout_entry *item;\n\tstruct rej_tmout_entry *old;\n\tint ret = 0;\n\n\txa_lock(&sriov->xa_rej_tmout);\n\titem = xa_load(&sriov->xa_rej_tmout, (unsigned long)rem_pv_cm_id);\n\n\tif (item) {\n\t\tif (xa_err(item))\n\t\t\tret =  xa_err(item);\n\t\telse\n\t\t\t \n\t\t\tmod_delayed_work(cm_wq, &item->timeout, CM_CLEANUP_CACHE_TIMEOUT);\n\t\tgoto err_or_exists;\n\t}\n\txa_unlock(&sriov->xa_rej_tmout);\n\n\titem = kmalloc(sizeof(*item), GFP_KERNEL);\n\tif (!item)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&item->timeout, rej_tmout_timeout);\n\titem->slave = slave;\n\titem->rem_pv_cm_id = rem_pv_cm_id;\n\titem->xa_rej_tmout = &sriov->xa_rej_tmout;\n\n\told = xa_cmpxchg(&sriov->xa_rej_tmout, (unsigned long)rem_pv_cm_id, NULL, item, GFP_KERNEL);\n\tif (old) {\n\t\tpr_debug(\n\t\t\t\"Non-null old entry (%p) or error (%d) when inserting\\n\",\n\t\t\told, xa_err(old));\n\t\tkfree(item);\n\t\treturn xa_err(old);\n\t}\n\n\tqueue_delayed_work(cm_wq, &item->timeout, CM_CLEANUP_CACHE_TIMEOUT);\n\n\treturn 0;\n\nerr_or_exists:\n\txa_unlock(&sriov->xa_rej_tmout);\n\treturn ret;\n}\n\nstatic int lookup_rej_tmout_slave(struct mlx4_ib_sriov *sriov, u32 rem_pv_cm_id)\n{\n\tstruct rej_tmout_entry *item;\n\tint slave;\n\n\txa_lock(&sriov->xa_rej_tmout);\n\titem = xa_load(&sriov->xa_rej_tmout, (unsigned long)rem_pv_cm_id);\n\n\tif (!item || xa_err(item)) {\n\t\tpr_debug(\"Could not find slave. rem_pv_cm_id 0x%x error: %d\\n\",\n\t\t\t rem_pv_cm_id, xa_err(item));\n\t\tslave = !item ? -ENOENT : xa_err(item);\n\t} else {\n\t\tslave = item->slave;\n\t}\n\txa_unlock(&sriov->xa_rej_tmout);\n\n\treturn slave;\n}\n\nint mlx4_ib_demux_cm_handler(struct ib_device *ibdev, int port, int *slave,\n\t\t\t     struct ib_mad *mad)\n{\n\tstruct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;\n\tu32 rem_pv_cm_id = get_local_comm_id(mad);\n\tu32 pv_cm_id;\n\tstruct id_map_entry *id;\n\tint sts;\n\n\tif (mad->mad_hdr.attr_id == CM_REQ_ATTR_ID ||\n\t    mad->mad_hdr.attr_id == CM_SIDR_REQ_ATTR_ID) {\n\t\tunion ib_gid gid;\n\n\t\tif (!slave)\n\t\t\treturn 0;\n\n\t\tgid = gid_from_req_msg(ibdev, mad);\n\t\t*slave = mlx4_ib_find_real_gid(ibdev, port, gid.global.interface_id);\n\t\tif (*slave < 0) {\n\t\t\tmlx4_ib_warn(ibdev, \"failed matching slave_id by gid (0x%llx)\\n\",\n\t\t\t\t     be64_to_cpu(gid.global.interface_id));\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tsts = alloc_rej_tmout(sriov, rem_pv_cm_id, *slave);\n\t\tif (sts)\n\t\t\t \n\t\t\tpr_debug(\"Could not allocate rej_tmout entry. rem_pv_cm_id 0x%x slave %d status %d\\n\",\n\t\t\t\t rem_pv_cm_id, *slave, sts);\n\n\t\treturn 0;\n\t}\n\n\tpv_cm_id = get_remote_comm_id(mad);\n\tid = id_map_get(ibdev, (int *)&pv_cm_id, -1, -1);\n\n\tif (!id) {\n\t\tif (mad->mad_hdr.attr_id == CM_REJ_ATTR_ID &&\n\t\t    REJ_REASON(mad) == IB_CM_REJ_TIMEOUT && slave) {\n\t\t\t*slave = lookup_rej_tmout_slave(sriov, rem_pv_cm_id);\n\n\t\t\treturn (*slave < 0) ? *slave : 0;\n\t\t}\n\t\tpr_debug(\"Couldn't find an entry for pv_cm_id 0x%x, attr_id 0x%x\\n\",\n\t\t\t pv_cm_id, be16_to_cpu(mad->mad_hdr.attr_id));\n\t\treturn -ENOENT;\n\t}\n\n\tif (slave)\n\t\t*slave = id->slave_id;\n\tset_remote_comm_id(mad, id->sl_cm_id);\n\n\tif (mad->mad_hdr.attr_id == CM_DREQ_ATTR_ID ||\n\t    mad->mad_hdr.attr_id == CM_REJ_ATTR_ID)\n\t\tschedule_delayed(ibdev, id);\n\n\treturn 0;\n}\n\nvoid mlx4_ib_cm_paravirt_init(struct mlx4_ib_dev *dev)\n{\n\tspin_lock_init(&dev->sriov.id_map_lock);\n\tINIT_LIST_HEAD(&dev->sriov.cm_list);\n\tdev->sriov.sl_id_map = RB_ROOT;\n\txa_init_flags(&dev->sriov.pv_id_table, XA_FLAGS_ALLOC);\n\txa_init(&dev->sriov.xa_rej_tmout);\n}\n\nstatic void rej_tmout_xa_cleanup(struct mlx4_ib_sriov *sriov, int slave)\n{\n\tstruct rej_tmout_entry *item;\n\tbool flush_needed = false;\n\tunsigned long id;\n\tint cnt = 0;\n\n\txa_lock(&sriov->xa_rej_tmout);\n\txa_for_each(&sriov->xa_rej_tmout, id, item) {\n\t\tif (slave < 0 || slave == item->slave) {\n\t\t\tmod_delayed_work(cm_wq, &item->timeout, 0);\n\t\t\tflush_needed = true;\n\t\t\t++cnt;\n\t\t}\n\t}\n\txa_unlock(&sriov->xa_rej_tmout);\n\n\tif (flush_needed) {\n\t\tflush_workqueue(cm_wq);\n\t\tpr_debug(\"Deleted %d entries in xarray for slave %d during cleanup\\n\",\n\t\t\t cnt, slave);\n\t}\n\n\tif (slave < 0)\n\t\tWARN_ON(!xa_empty(&sriov->xa_rej_tmout));\n}\n\n \n \nvoid mlx4_ib_cm_paravirt_clean(struct mlx4_ib_dev *dev, int slave)\n{\n\tstruct mlx4_ib_sriov *sriov = &dev->sriov;\n\tstruct rb_root *sl_id_map = &sriov->sl_id_map;\n\tstruct list_head lh;\n\tstruct rb_node *nd;\n\tint need_flush = 0;\n\tstruct id_map_entry *map, *tmp_map;\n\t \n\tINIT_LIST_HEAD(&lh);\n\tspin_lock(&sriov->id_map_lock);\n\tlist_for_each_entry_safe(map, tmp_map, &dev->sriov.cm_list, list) {\n\t\tif (slave < 0 || slave == map->slave_id) {\n\t\t\tif (map->scheduled_delete)\n\t\t\t\tneed_flush |= !cancel_delayed_work(&map->timeout);\n\t\t}\n\t}\n\n\tspin_unlock(&sriov->id_map_lock);\n\n\tif (need_flush)\n\t\tflush_workqueue(cm_wq);  \n\n\t \n\tspin_lock(&sriov->id_map_lock);\n\tif (slave < 0) {\n\t\twhile (rb_first(sl_id_map)) {\n\t\t\tstruct id_map_entry *ent =\n\t\t\t\trb_entry(rb_first(sl_id_map),\n\t\t\t\t\t struct id_map_entry, node);\n\n\t\t\trb_erase(&ent->node, sl_id_map);\n\t\t\txa_erase(&sriov->pv_id_table, ent->pv_cm_id);\n\t\t}\n\t\tlist_splice_init(&dev->sriov.cm_list, &lh);\n\t} else {\n\t\t \n\t\tnd = rb_first(sl_id_map);\n\t\twhile (nd) {\n\t\t\tstruct id_map_entry *ent =\n\t\t\t\trb_entry(nd, struct id_map_entry, node);\n\t\t\tnd = rb_next(nd);\n\t\t\tif (ent->slave_id == slave)\n\t\t\t\tlist_move_tail(&ent->list, &lh);\n\t\t}\n\t\t \n\t\tlist_for_each_entry_safe(map, tmp_map, &lh, list) {\n\t\t\trb_erase(&map->node, sl_id_map);\n\t\t\txa_erase(&sriov->pv_id_table, map->pv_cm_id);\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry_safe(map, tmp_map, &dev->sriov.cm_list, list) {\n\t\t\tif (slave == map->slave_id)\n\t\t\t\tlist_move_tail(&map->list, &lh);\n\t\t}\n\t}\n\n\tspin_unlock(&sriov->id_map_lock);\n\n\t \n\tlist_for_each_entry_safe(map, tmp_map, &lh, list) {\n\t\tlist_del(&map->list);\n\t\tkfree(map);\n\t}\n\n\trej_tmout_xa_cleanup(sriov, slave);\n}\n\nint mlx4_ib_cm_init(void)\n{\n\tcm_wq = alloc_workqueue(\"mlx4_ib_cm\", 0, 0);\n\tif (!cm_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid mlx4_ib_cm_destroy(void)\n{\n\tdestroy_workqueue(cm_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}