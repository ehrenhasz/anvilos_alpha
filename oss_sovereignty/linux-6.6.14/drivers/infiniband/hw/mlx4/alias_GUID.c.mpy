{
  "module_name": "alias_GUID.c",
  "hash_id": "65647836fe5b7909b2cb0181eb0df836b985b22489575e88f4780266490cf164",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx4/alias_GUID.c",
  "human_readable_source": " \n  \n \n \n#include <rdma/ib_mad.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_cache.h>\n#include <rdma/ib_sa.h>\n#include <rdma/ib_pack.h>\n#include <linux/mlx4/cmd.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <rdma/ib_user_verbs.h>\n#include <linux/delay.h>\n#include \"mlx4_ib.h\"\n\n \n\nstruct mlx4_alias_guid_work_context {\n\tu8 port;\n\tstruct mlx4_ib_dev     *dev ;\n\tstruct ib_sa_query     *sa_query;\n\tstruct completion\tdone;\n\tint\t\t\tquery_id;\n\tstruct list_head\tlist;\n\tint\t\t\tblock_num;\n\tib_sa_comp_mask\t\tguid_indexes;\n\tu8\t\t\tmethod;\n};\n\nstruct mlx4_next_alias_guid_work {\n\tu8 port;\n\tu8 block_num;\n\tu8 method;\n\tstruct mlx4_sriov_alias_guid_info_rec_det rec_det;\n};\n\nstatic int get_low_record_time_index(struct mlx4_ib_dev *dev, u8 port,\n\t\t\t\t     int *resched_delay_sec);\n\nvoid mlx4_ib_update_cache_on_guid_change(struct mlx4_ib_dev *dev, int block_num,\n\t\t\t\t\t u32 port_num, u8 *p_data)\n{\n\tint i;\n\tu64 guid_indexes;\n\tint slave_id;\n\tu32 port_index = port_num - 1;\n\n\tif (!mlx4_is_master(dev->dev))\n\t\treturn;\n\n\tguid_indexes = be64_to_cpu((__force __be64) dev->sriov.alias_guid.\n\t\t\t\t   ports_guid[port_num - 1].\n\t\t\t\t   all_rec_per_port[block_num].guid_indexes);\n\tpr_debug(\"port: %u, guid_indexes: 0x%llx\\n\", port_num, guid_indexes);\n\n\tfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\n\t\t \n\t\tif (test_bit(i + 4, (unsigned long *)&guid_indexes)) {\n\t\t\tslave_id = (block_num * NUM_ALIAS_GUID_IN_REC) + i ;\n\t\t\tif (slave_id >= dev->dev->num_slaves) {\n\t\t\t\tpr_debug(\"The last slave: %d\\n\", slave_id);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t \n\t\t\tmemcpy(&dev->sriov.demux[port_index].guid_cache[slave_id],\n\t\t\t       &p_data[i * GUID_REC_SIZE],\n\t\t\t       GUID_REC_SIZE);\n\t\t} else\n\t\t\tpr_debug(\"Guid number: %d in block: %d\"\n\t\t\t\t \" was not updated\\n\", i, block_num);\n\t}\n}\n\nstatic __be64 get_cached_alias_guid(struct mlx4_ib_dev *dev, int port, int index)\n{\n\tif (index >= NUM_ALIAS_GUID_PER_PORT) {\n\t\tpr_err(\"%s: ERROR: asked for index:%d\\n\", __func__, index);\n\t\treturn (__force __be64) -1;\n\t}\n\treturn *(__be64 *)&dev->sriov.demux[port - 1].guid_cache[index];\n}\n\n\nib_sa_comp_mask mlx4_ib_get_aguid_comp_mask_from_ix(int index)\n{\n\treturn IB_SA_COMP_MASK(4 + index);\n}\n\nvoid mlx4_ib_slave_alias_guid_event(struct mlx4_ib_dev *dev, int slave,\n\t\t\t\t    int port,  int slave_init)\n{\n\t__be64 curr_guid, required_guid;\n\tint record_num = slave / 8;\n\tint index = slave % 8;\n\tint port_index = port - 1;\n\tunsigned long flags;\n\tint do_work = 0;\n\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\n\tif (dev->sriov.alias_guid.ports_guid[port_index].state_flags &\n\t    GUID_STATE_NEED_PORT_INIT)\n\t\tgoto unlock;\n\tif (!slave_init) {\n\t\tcurr_guid = *(__be64 *)&dev->sriov.\n\t\t\talias_guid.ports_guid[port_index].\n\t\t\tall_rec_per_port[record_num].\n\t\t\tall_recs[GUID_REC_SIZE * index];\n\t\tif (curr_guid == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL) ||\n\t\t    !curr_guid)\n\t\t\tgoto unlock;\n\t\trequired_guid = cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL);\n\t} else {\n\t\trequired_guid = mlx4_get_admin_guid(dev->dev, slave, port);\n\t\tif (required_guid == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\n\t\t\tgoto unlock;\n\t}\n\t*(__be64 *)&dev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[record_num].\n\t\tall_recs[GUID_REC_SIZE * index] = required_guid;\n\tdev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[record_num].guid_indexes\n\t\t|= mlx4_ib_get_aguid_comp_mask_from_ix(index);\n\tdev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[record_num].status\n\t\t= MLX4_GUID_INFO_STATUS_IDLE;\n\t \n\tdev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[record_num].time_to_run = 0;\n\tdev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[record_num].\n\t\tguids_retry_schedule[index] = 0;\n\tdo_work = 1;\nunlock:\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\n\n\tif (do_work)\n\t\tmlx4_ib_init_alias_guid_work(dev, port_index);\n}\n\n \nvoid mlx4_ib_notify_slaves_on_guid_change(struct mlx4_ib_dev *dev,\n\t\t\t\t\t  int block_num, u32 port_num,\n\t\t\t\t\t  u8 *p_data)\n{\n\tint i;\n\tu64 guid_indexes;\n\tint slave_id, slave_port;\n\tenum slave_port_state new_state;\n\tenum slave_port_state prev_state;\n\t__be64 tmp_cur_ag, form_cache_ag;\n\tenum slave_port_gen_event gen_event;\n\tstruct mlx4_sriov_alias_guid_info_rec_det *rec;\n\tunsigned long flags;\n\t__be64 required_value;\n\n\tif (!mlx4_is_master(dev->dev))\n\t\treturn;\n\n\trec = &dev->sriov.alias_guid.ports_guid[port_num - 1].\n\t\t\tall_rec_per_port[block_num];\n\tguid_indexes = be64_to_cpu((__force __be64) dev->sriov.alias_guid.\n\t\t\t\t   ports_guid[port_num - 1].\n\t\t\t\t   all_rec_per_port[block_num].guid_indexes);\n\tpr_debug(\"port: %u, guid_indexes: 0x%llx\\n\", port_num, guid_indexes);\n\n\t \n\tfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\n\t\t \n\t\tif (!(test_bit(i + 4, (unsigned long *)&guid_indexes)))\n\t\t\tcontinue;\n\n\t\tslave_id = (block_num * NUM_ALIAS_GUID_IN_REC) + i ;\n\t\tif (slave_id >= dev->dev->persist->num_vfs + 1)\n\t\t\treturn;\n\n\t\tslave_port = mlx4_phys_to_slave_port(dev->dev, slave_id, port_num);\n\t\tif (slave_port < 0)  \n\t\t\tcontinue;\n\n\t\ttmp_cur_ag = *(__be64 *)&p_data[i * GUID_REC_SIZE];\n\t\tform_cache_ag = get_cached_alias_guid(dev, port_num,\n\t\t\t\t\t(NUM_ALIAS_GUID_IN_REC * block_num) + i);\n\t\t \n\t\tif (tmp_cur_ag != form_cache_ag)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\n\t\trequired_value = *(__be64 *)&rec->all_recs[i * GUID_REC_SIZE];\n\n\t\tif (required_value == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\n\t\t\trequired_value = 0;\n\n\t\tif (tmp_cur_ag == required_value) {\n\t\t\trec->guid_indexes = rec->guid_indexes &\n\t\t\t       ~mlx4_ib_get_aguid_comp_mask_from_ix(i);\n\t\t} else {\n\t\t\t \n\t\t\tif (tmp_cur_ag != MLX4_NOT_SET_GUID) {\n\t\t\t\tspin_unlock_irqrestore(&dev->sriov.\n\t\t\t\t\talias_guid.ag_work_lock, flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock,\n\t\t\t\t       flags);\n\t\tmlx4_gen_guid_change_eqe(dev->dev, slave_id, port_num);\n\t\t \n\n\t\tif (tmp_cur_ag != MLX4_NOT_SET_GUID) {  \n\t\t\tprev_state = mlx4_get_slave_port_state(dev->dev, slave_id, port_num);\n\t\t\tnew_state = set_and_calc_slave_port_state(dev->dev, slave_id, port_num,\n\t\t\t\t\t\t\t\t  MLX4_PORT_STATE_IB_PORT_STATE_EVENT_GID_VALID,\n\t\t\t\t\t\t\t\t  &gen_event);\n\t\t\tpr_debug(\"slave: %d, port: %u prev_port_state: %d,\"\n\t\t\t\t \" new_port_state: %d, gen_event: %d\\n\",\n\t\t\t\t slave_id, port_num, prev_state, new_state, gen_event);\n\t\t\tif (gen_event == SLAVE_PORT_GEN_EVENT_UP) {\n\t\t\t\tpr_debug(\"sending PORT_UP event to slave: %d, port: %u\\n\",\n\t\t\t\t\t slave_id, port_num);\n\t\t\t\tmlx4_gen_port_state_change_eqe(dev->dev, slave_id,\n\t\t\t\t\t\t\t       port_num, MLX4_PORT_CHANGE_SUBTYPE_ACTIVE);\n\t\t\t}\n\t\t} else {  \n\t\t\tset_and_calc_slave_port_state(dev->dev, slave_id, port_num,\n\t\t\t\t\t\t      MLX4_PORT_STATE_IB_EVENT_GID_INVALID,\n\t\t\t\t\t\t      &gen_event);\n\t\t\tif (gen_event == SLAVE_PORT_GEN_EVENT_DOWN) {\n\t\t\t\tpr_debug(\"sending PORT DOWN event to slave: %d, port: %u\\n\",\n\t\t\t\t\t slave_id, port_num);\n\t\t\t\tmlx4_gen_port_state_change_eqe(dev->dev,\n\t\t\t\t\t\t\t       slave_id,\n\t\t\t\t\t\t\t       port_num,\n\t\t\t\t\t\t\t       MLX4_PORT_CHANGE_SUBTYPE_DOWN);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void aliasguid_query_handler(int status,\n\t\t\t\t    struct ib_sa_guidinfo_rec *guid_rec,\n\t\t\t\t    void *context)\n{\n\tstruct mlx4_ib_dev *dev;\n\tstruct mlx4_alias_guid_work_context *cb_ctx = context;\n\tu8 port_index ;\n\tint i;\n\tstruct mlx4_sriov_alias_guid_info_rec_det *rec;\n\tunsigned long flags, flags1;\n\tib_sa_comp_mask declined_guid_indexes = 0;\n\tib_sa_comp_mask applied_guid_indexes = 0;\n\tunsigned int resched_delay_sec = 0;\n\n\tif (!context)\n\t\treturn;\n\n\tdev = cb_ctx->dev;\n\tport_index = cb_ctx->port - 1;\n\trec = &dev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[cb_ctx->block_num];\n\n\tif (status) {\n\t\tpr_debug(\"(port: %d) failed: status = %d\\n\",\n\t\t\t cb_ctx->port, status);\n\t\trec->time_to_run = ktime_get_boottime_ns() + 1 * NSEC_PER_SEC;\n\t\tgoto out;\n\t}\n\n\tif (guid_rec->block_num != cb_ctx->block_num) {\n\t\tpr_err(\"block num mismatch: %d != %d\\n\",\n\t\t       cb_ctx->block_num, guid_rec->block_num);\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"lid/port: %d/%d, block_num: %d\\n\",\n\t\t be16_to_cpu(guid_rec->lid), cb_ctx->port,\n\t\t guid_rec->block_num);\n\n\trec = &dev->sriov.alias_guid.ports_guid[port_index].\n\t\tall_rec_per_port[guid_rec->block_num];\n\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\n\tfor (i = 0 ; i < NUM_ALIAS_GUID_IN_REC; i++) {\n\t\t__be64 sm_response, required_val;\n\n\t\tif (!(cb_ctx->guid_indexes &\n\t\t\tmlx4_ib_get_aguid_comp_mask_from_ix(i)))\n\t\t\tcontinue;\n\t\tsm_response = *(__be64 *)&guid_rec->guid_info_list\n\t\t\t\t[i * GUID_REC_SIZE];\n\t\trequired_val = *(__be64 *)&rec->all_recs[i * GUID_REC_SIZE];\n\t\tif (cb_ctx->method == MLX4_GUID_INFO_RECORD_DELETE) {\n\t\t\tif (required_val ==\n\t\t\t    cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\n\t\t\t\tgoto next_entry;\n\n\t\t\t \n\t\t\tpr_debug(\"need to set new value %llx, record num %d, block_num:%d\\n\",\n\t\t\t\t be64_to_cpu(required_val),\n\t\t\t\t i, guid_rec->block_num);\n\t\t\tgoto entry_declined;\n\t\t}\n\n\t\t \n\t\tif (sm_response == MLX4_NOT_SET_GUID) {\n\t\t\tif (rec->guids_retry_schedule[i] == 0)\n\t\t\t\tmlx4_ib_warn(&dev->ib_dev,\n\t\t\t\t\t     \"%s:Record num %d in  block_num: %d was declined by SM\\n\",\n\t\t\t\t\t     __func__, i,\n\t\t\t\t\t     guid_rec->block_num);\n\t\t\tgoto entry_declined;\n\t\t} else {\n\t\t        \n\t\t        \n\t\t\tif (required_val &&\n\t\t\t    sm_response != required_val) {\n\t\t\t\t \n\t\t\t\tif (rec->guids_retry_schedule[i] == 0)\n\t\t\t\t\tmlx4_ib_warn(&dev->ib_dev, \"%s: Failed to set\"\n\t\t\t\t\t\t     \" admin guid after SysAdmin \"\n\t\t\t\t\t\t     \"configuration. \"\n\t\t\t\t\t\t     \"Record num %d in block_num:%d \"\n\t\t\t\t\t\t     \"was declined by SM, \"\n\t\t\t\t\t\t     \"new val(0x%llx) was kept, SM returned (0x%llx)\\n\",\n\t\t\t\t\t\t      __func__, i,\n\t\t\t\t\t\t     guid_rec->block_num,\n\t\t\t\t\t\t     be64_to_cpu(required_val),\n\t\t\t\t\t\t     be64_to_cpu(sm_response));\n\t\t\t\tgoto entry_declined;\n\t\t\t} else {\n\t\t\t\t*(__be64 *)&rec->all_recs[i * GUID_REC_SIZE] =\n\t\t\t\t\tsm_response;\n\t\t\t\tif (required_val == 0)\n\t\t\t\t\tmlx4_set_admin_guid(dev->dev,\n\t\t\t\t\t\t\t    sm_response,\n\t\t\t\t\t\t\t    (guid_rec->block_num\n\t\t\t\t\t\t\t    * NUM_ALIAS_GUID_IN_REC) + i,\n\t\t\t\t\t\t\t    cb_ctx->port);\n\t\t\t\tgoto next_entry;\n\t\t\t}\n\t\t}\nentry_declined:\n\t\tdeclined_guid_indexes |= mlx4_ib_get_aguid_comp_mask_from_ix(i);\n\t\trec->guids_retry_schedule[i] =\n\t\t\t(rec->guids_retry_schedule[i] == 0) ?  1 :\n\t\t\tmin((unsigned int)60,\n\t\t\t    rec->guids_retry_schedule[i] * 2);\n\t\t \n\t\tresched_delay_sec = (resched_delay_sec == 0) ?\n\t\t\t\trec->guids_retry_schedule[i] :\n\t\t\t\tmin(resched_delay_sec,\n\t\t\t\t    rec->guids_retry_schedule[i]);\n\t\tcontinue;\n\nnext_entry:\n\t\trec->guids_retry_schedule[i] = 0;\n\t}\n\n\tapplied_guid_indexes =  cb_ctx->guid_indexes & ~declined_guid_indexes;\n\tif (declined_guid_indexes ||\n\t    rec->guid_indexes & ~(applied_guid_indexes)) {\n\t\tpr_debug(\"record=%d wasn't fully set, guid_indexes=0x%llx applied_indexes=0x%llx, declined_indexes=0x%llx\\n\",\n\t\t\t guid_rec->block_num,\n\t\t\t be64_to_cpu((__force __be64)rec->guid_indexes),\n\t\t\t be64_to_cpu((__force __be64)applied_guid_indexes),\n\t\t\t be64_to_cpu((__force __be64)declined_guid_indexes));\n\t\trec->time_to_run = ktime_get_boottime_ns() +\n\t\t\tresched_delay_sec * NSEC_PER_SEC;\n\t} else {\n\t\trec->status = MLX4_GUID_INFO_STATUS_SET;\n\t}\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\n\t \n\tmlx4_ib_notify_slaves_on_guid_change(dev, guid_rec->block_num,\n\t\t\t\t\t     cb_ctx->port,\n\t\t\t\t\t     guid_rec->guid_info_list);\nout:\n\tspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tif (!dev->sriov.is_going_down) {\n\t\tget_low_record_time_index(dev, port_index, &resched_delay_sec);\n\t\tqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port_index].wq,\n\t\t\t\t   &dev->sriov.alias_guid.ports_guid[port_index].\n\t\t\t\t   alias_guid_work,\n\t\t\t\t   msecs_to_jiffies(resched_delay_sec * 1000));\n\t}\n\tif (cb_ctx->sa_query) {\n\t\tlist_del(&cb_ctx->list);\n\t\tkfree(cb_ctx);\n\t} else\n\t\tcomplete(&cb_ctx->done);\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\n}\n\nstatic void invalidate_guid_record(struct mlx4_ib_dev *dev, u8 port, int index)\n{\n\tint i;\n\tu64 cur_admin_val;\n\tib_sa_comp_mask comp_mask = 0;\n\n\tdev->sriov.alias_guid.ports_guid[port - 1].all_rec_per_port[index].status\n\t\t= MLX4_GUID_INFO_STATUS_SET;\n\n\t \n\tfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\n\t\tcur_admin_val =\n\t\t\t*(u64 *)&dev->sriov.alias_guid.ports_guid[port - 1].\n\t\t\tall_rec_per_port[index].all_recs[GUID_REC_SIZE * i];\n\t\t \n\t\tif (MLX4_GUID_FOR_DELETE_VAL == cur_admin_val ||\n\t\t    (!index && !i))\n\t\t\tcontinue;\n\t\tcomp_mask |= mlx4_ib_get_aguid_comp_mask_from_ix(i);\n\t}\n\tdev->sriov.alias_guid.ports_guid[port - 1].\n\t\tall_rec_per_port[index].guid_indexes |= comp_mask;\n\tif (dev->sriov.alias_guid.ports_guid[port - 1].\n\t    all_rec_per_port[index].guid_indexes)\n\t\tdev->sriov.alias_guid.ports_guid[port - 1].\n\t\tall_rec_per_port[index].status = MLX4_GUID_INFO_STATUS_IDLE;\n\n}\n\nstatic int set_guid_rec(struct ib_device *ibdev,\n\t\t\tstruct mlx4_next_alias_guid_work *rec)\n{\n\tint err;\n\tstruct mlx4_ib_dev *dev = to_mdev(ibdev);\n\tstruct ib_sa_guidinfo_rec guid_info_rec;\n\tib_sa_comp_mask comp_mask;\n\tstruct ib_port_attr attr;\n\tstruct mlx4_alias_guid_work_context *callback_context;\n\tunsigned long resched_delay, flags, flags1;\n\tu8 port = rec->port + 1;\n\tint index = rec->block_num;\n\tstruct mlx4_sriov_alias_guid_info_rec_det *rec_det = &rec->rec_det;\n\tstruct list_head *head =\n\t\t&dev->sriov.alias_guid.ports_guid[port - 1].cb_list;\n\n\tmemset(&attr, 0, sizeof(attr));\n\terr = __mlx4_ib_query_port(ibdev, port, &attr, 1);\n\tif (err) {\n\t\tpr_debug(\"mlx4_ib_query_port failed (err: %d), port: %d\\n\",\n\t\t\t err, port);\n\t\treturn err;\n\t}\n\t \n\tif (attr.state != IB_PORT_ACTIVE) {\n\t\tpr_debug(\"port %d not active...rescheduling\\n\", port);\n\t\tresched_delay = 5 * HZ;\n\t\terr = -EAGAIN;\n\t\tgoto new_schedule;\n\t}\n\n\tcallback_context = kmalloc(sizeof *callback_context, GFP_KERNEL);\n\tif (!callback_context) {\n\t\terr = -ENOMEM;\n\t\tresched_delay = HZ * 5;\n\t\tgoto new_schedule;\n\t}\n\tcallback_context->port = port;\n\tcallback_context->dev = dev;\n\tcallback_context->block_num = index;\n\tcallback_context->guid_indexes = rec_det->guid_indexes;\n\tcallback_context->method = rec->method;\n\n\tmemset(&guid_info_rec, 0, sizeof (struct ib_sa_guidinfo_rec));\n\n\tguid_info_rec.lid = ib_lid_be16(attr.lid);\n\tguid_info_rec.block_num = index;\n\n\tmemcpy(guid_info_rec.guid_info_list, rec_det->all_recs,\n\t       GUID_REC_SIZE * NUM_ALIAS_GUID_IN_REC);\n\tcomp_mask = IB_SA_GUIDINFO_REC_LID | IB_SA_GUIDINFO_REC_BLOCK_NUM |\n\t\trec_det->guid_indexes;\n\n\tinit_completion(&callback_context->done);\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tlist_add_tail(&callback_context->list, head);\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\n\tcallback_context->query_id =\n\t\tib_sa_guid_info_rec_query(dev->sriov.alias_guid.sa_client,\n\t\t\t\t\t  ibdev, port, &guid_info_rec,\n\t\t\t\t\t  comp_mask, rec->method, 1000,\n\t\t\t\t\t  GFP_KERNEL, aliasguid_query_handler,\n\t\t\t\t\t  callback_context,\n\t\t\t\t\t  &callback_context->sa_query);\n\tif (callback_context->query_id < 0) {\n\t\tpr_debug(\"ib_sa_guid_info_rec_query failed, query_id: \"\n\t\t\t \"%d. will reschedule to the next 1 sec.\\n\",\n\t\t\t callback_context->query_id);\n\t\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\t\tlist_del(&callback_context->list);\n\t\tkfree(callback_context);\n\t\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\t\tresched_delay = 1 * HZ;\n\t\terr = -EAGAIN;\n\t\tgoto new_schedule;\n\t}\n\terr = 0;\n\tgoto out;\n\nnew_schedule:\n\tspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tinvalidate_guid_record(dev, port, index);\n\tif (!dev->sriov.is_going_down) {\n\t\tqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,\n\t\t\t\t   &dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,\n\t\t\t\t   resched_delay);\n\t}\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\n\nout:\n\treturn err;\n}\n\nstatic void mlx4_ib_guid_port_init(struct mlx4_ib_dev *dev, int port)\n{\n\tint j, k, entry;\n\t__be64 guid;\n\n\t \n\tfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\n\t\tfor (k = 0; k < NUM_ALIAS_GUID_IN_REC; k++) {\n\t\t\tentry = j * NUM_ALIAS_GUID_IN_REC + k;\n\t\t\t \n\t\t\tif (!entry || entry > dev->dev->persist->num_vfs ||\n\t\t\t    !mlx4_is_slave_active(dev->dev, entry))\n\t\t\t\tcontinue;\n\t\t\tguid = mlx4_get_admin_guid(dev->dev, entry, port);\n\t\t\t*(__be64 *)&dev->sriov.alias_guid.ports_guid[port - 1].\n\t\t\t\tall_rec_per_port[j].all_recs\n\t\t\t\t[GUID_REC_SIZE * k] = guid;\n\t\t\tpr_debug(\"guid was set, entry=%d, val=0x%llx, port=%d\\n\",\n\t\t\t\t entry,\n\t\t\t\t be64_to_cpu(guid),\n\t\t\t\t port);\n\t\t}\n\t}\n}\nvoid mlx4_ib_invalidate_all_guid_record(struct mlx4_ib_dev *dev, int port)\n{\n\tint i;\n\tunsigned long flags, flags1;\n\n\tpr_debug(\"port %d\\n\", port);\n\n\tspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\n\tif (dev->sriov.alias_guid.ports_guid[port - 1].state_flags &\n\t\tGUID_STATE_NEED_PORT_INIT) {\n\t\tmlx4_ib_guid_port_init(dev, port);\n\t\tdev->sriov.alias_guid.ports_guid[port - 1].state_flags &=\n\t\t\t(~GUID_STATE_NEED_PORT_INIT);\n\t}\n\tfor (i = 0; i < NUM_ALIAS_GUID_REC_IN_PORT; i++)\n\t\tinvalidate_guid_record(dev, port, i);\n\n\tif (mlx4_is_master(dev->dev) && !dev->sriov.is_going_down) {\n\t\t \n\t\tcancel_delayed_work(&dev->sriov.alias_guid.\n\t\t\t\t      ports_guid[port - 1].alias_guid_work);\n\t\tqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,\n\t\t\t\t   &dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,\n\t\t\t\t   0);\n\t}\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\n}\n\nstatic void set_required_record(struct mlx4_ib_dev *dev, u8 port,\n\t\t\t\tstruct mlx4_next_alias_guid_work *next_rec,\n\t\t\t\tint record_index)\n{\n\tint i;\n\tint lowset_time_entry = -1;\n\tint lowest_time = 0;\n\tib_sa_comp_mask delete_guid_indexes = 0;\n\tib_sa_comp_mask set_guid_indexes = 0;\n\tstruct mlx4_sriov_alias_guid_info_rec_det *rec =\n\t\t\t&dev->sriov.alias_guid.ports_guid[port].\n\t\t\tall_rec_per_port[record_index];\n\n\tfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\n\t\tif (!(rec->guid_indexes &\n\t\t\tmlx4_ib_get_aguid_comp_mask_from_ix(i)))\n\t\t\tcontinue;\n\n\t\tif (*(__be64 *)&rec->all_recs[i * GUID_REC_SIZE] ==\n\t\t\t\tcpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\n\t\t\tdelete_guid_indexes |=\n\t\t\t\tmlx4_ib_get_aguid_comp_mask_from_ix(i);\n\t\telse\n\t\t\tset_guid_indexes |=\n\t\t\t\tmlx4_ib_get_aguid_comp_mask_from_ix(i);\n\n\t\tif (lowset_time_entry == -1 || rec->guids_retry_schedule[i] <=\n\t\t\tlowest_time) {\n\t\t\tlowset_time_entry = i;\n\t\t\tlowest_time = rec->guids_retry_schedule[i];\n\t\t}\n\t}\n\n\tmemcpy(&next_rec->rec_det, rec, sizeof(*rec));\n\tnext_rec->port = port;\n\tnext_rec->block_num = record_index;\n\n\tif (*(__be64 *)&rec->all_recs[lowset_time_entry * GUID_REC_SIZE] ==\n\t\t\t\tcpu_to_be64(MLX4_GUID_FOR_DELETE_VAL)) {\n\t\tnext_rec->rec_det.guid_indexes = delete_guid_indexes;\n\t\tnext_rec->method = MLX4_GUID_INFO_RECORD_DELETE;\n\t} else {\n\t\tnext_rec->rec_det.guid_indexes = set_guid_indexes;\n\t\tnext_rec->method = MLX4_GUID_INFO_RECORD_SET;\n\t}\n}\n\n \nstatic int get_low_record_time_index(struct mlx4_ib_dev *dev, u8 port,\n\t\t\t\t     int *resched_delay_sec)\n{\n\tint record_index = -1;\n\tu64 low_record_time = 0;\n\tstruct mlx4_sriov_alias_guid_info_rec_det rec;\n\tint j;\n\n\tfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\n\t\trec = dev->sriov.alias_guid.ports_guid[port].\n\t\t\tall_rec_per_port[j];\n\t\tif (rec.status == MLX4_GUID_INFO_STATUS_IDLE &&\n\t\t    rec.guid_indexes) {\n\t\t\tif (record_index == -1 ||\n\t\t\t    rec.time_to_run < low_record_time) {\n\t\t\t\trecord_index = j;\n\t\t\t\tlow_record_time = rec.time_to_run;\n\t\t\t}\n\t\t}\n\t}\n\tif (resched_delay_sec) {\n\t\tu64 curr_time = ktime_get_boottime_ns();\n\n\t\t*resched_delay_sec = (low_record_time < curr_time) ? 0 :\n\t\t\tdiv_u64((low_record_time - curr_time), NSEC_PER_SEC);\n\t}\n\n\treturn record_index;\n}\n\n \nstatic int get_next_record_to_update(struct mlx4_ib_dev *dev, u8 port,\n\t\t\t\t     struct mlx4_next_alias_guid_work *rec)\n{\n\tunsigned long flags;\n\tint record_index;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\n\trecord_index = get_low_record_time_index(dev, port, NULL);\n\n\tif (record_index < 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tset_required_record(dev, port, rec, record_index);\nout:\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\n\treturn ret;\n}\n\nstatic void alias_guid_work(struct work_struct *work)\n{\n\tstruct delayed_work *delay = to_delayed_work(work);\n\tint ret = 0;\n\tstruct mlx4_next_alias_guid_work *rec;\n\tstruct mlx4_sriov_alias_guid_port_rec_det *sriov_alias_port =\n\t\tcontainer_of(delay, struct mlx4_sriov_alias_guid_port_rec_det,\n\t\t\t     alias_guid_work);\n\tstruct mlx4_sriov_alias_guid *sriov_alias_guid = sriov_alias_port->parent;\n\tstruct mlx4_ib_sriov *ib_sriov = container_of(sriov_alias_guid,\n\t\t\t\t\t\tstruct mlx4_ib_sriov,\n\t\t\t\t\t\talias_guid);\n\tstruct mlx4_ib_dev *dev = container_of(ib_sriov, struct mlx4_ib_dev, sriov);\n\n\trec = kzalloc(sizeof *rec, GFP_KERNEL);\n\tif (!rec)\n\t\treturn;\n\n\tpr_debug(\"starting [port: %d]...\\n\", sriov_alias_port->port + 1);\n\tret = get_next_record_to_update(dev, sriov_alias_port->port, rec);\n\tif (ret) {\n\t\tpr_debug(\"No more records to update.\\n\");\n\t\tgoto out;\n\t}\n\n\tset_guid_rec(&dev->ib_dev, rec);\nout:\n\tkfree(rec);\n}\n\n\nvoid mlx4_ib_init_alias_guid_work(struct mlx4_ib_dev *dev, int port)\n{\n\tunsigned long flags, flags1;\n\n\tif (!mlx4_is_master(dev->dev))\n\t\treturn;\n\tspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\n\tspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tif (!dev->sriov.is_going_down) {\n\t\t \n\t\tcancel_delayed_work(&dev->sriov.alias_guid.ports_guid[port].\n\t\t\t\t    alias_guid_work);\n\t\tqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port].wq,\n\t\t\t   &dev->sriov.alias_guid.ports_guid[port].alias_guid_work, 0);\n\t}\n\tspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\n\tspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\n}\n\nvoid mlx4_ib_destroy_alias_guid_service(struct mlx4_ib_dev *dev)\n{\n\tint i;\n\tstruct mlx4_ib_sriov *sriov = &dev->sriov;\n\tstruct mlx4_alias_guid_work_context *cb_ctx;\n\tstruct mlx4_sriov_alias_guid_port_rec_det *det;\n\tstruct ib_sa_query *sa_query;\n\tunsigned long flags;\n\n\tfor (i = 0 ; i < dev->num_ports; i++) {\n\t\tdet = &sriov->alias_guid.ports_guid[i];\n\t\tcancel_delayed_work_sync(&det->alias_guid_work);\n\t\tspin_lock_irqsave(&sriov->alias_guid.ag_work_lock, flags);\n\t\twhile (!list_empty(&det->cb_list)) {\n\t\t\tcb_ctx = list_entry(det->cb_list.next,\n\t\t\t\t\t    struct mlx4_alias_guid_work_context,\n\t\t\t\t\t    list);\n\t\t\tsa_query = cb_ctx->sa_query;\n\t\t\tcb_ctx->sa_query = NULL;\n\t\t\tlist_del(&cb_ctx->list);\n\t\t\tspin_unlock_irqrestore(&sriov->alias_guid.ag_work_lock, flags);\n\t\t\tib_sa_cancel_query(cb_ctx->query_id, sa_query);\n\t\t\twait_for_completion(&cb_ctx->done);\n\t\t\tkfree(cb_ctx);\n\t\t\tspin_lock_irqsave(&sriov->alias_guid.ag_work_lock, flags);\n\t\t}\n\t\tspin_unlock_irqrestore(&sriov->alias_guid.ag_work_lock, flags);\n\t}\n\tfor (i = 0 ; i < dev->num_ports; i++)\n\t\tdestroy_workqueue(dev->sriov.alias_guid.ports_guid[i].wq);\n\tib_sa_unregister_client(dev->sriov.alias_guid.sa_client);\n\tkfree(dev->sriov.alias_guid.sa_client);\n}\n\nint mlx4_ib_init_alias_guid_service(struct mlx4_ib_dev *dev)\n{\n\tchar alias_wq_name[15];\n\tint ret = 0;\n\tint i, j;\n\tunion ib_gid gid;\n\n\tif (!mlx4_is_master(dev->dev))\n\t\treturn 0;\n\tdev->sriov.alias_guid.sa_client =\n\t\tkzalloc(sizeof *dev->sriov.alias_guid.sa_client, GFP_KERNEL);\n\tif (!dev->sriov.alias_guid.sa_client)\n\t\treturn -ENOMEM;\n\n\tib_sa_register_client(dev->sriov.alias_guid.sa_client);\n\n\tspin_lock_init(&dev->sriov.alias_guid.ag_work_lock);\n\n\tfor (i = 1; i <= dev->num_ports; ++i) {\n\t\tif (dev->ib_dev.ops.query_gid(&dev->ib_dev, i, 0, &gid)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_unregister;\n\t\t}\n\t}\n\n\tfor (i = 0 ; i < dev->num_ports; i++) {\n\t\tmemset(&dev->sriov.alias_guid.ports_guid[i], 0,\n\t\t       sizeof (struct mlx4_sriov_alias_guid_port_rec_det));\n\t\tdev->sriov.alias_guid.ports_guid[i].state_flags |=\n\t\t\t\tGUID_STATE_NEED_PORT_INIT;\n\t\tfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\n\t\t\t \n\t\t\tmemset(dev->sriov.alias_guid.ports_guid[i].\n\t\t\t\tall_rec_per_port[j].all_recs, 0xFF,\n\t\t\t\tsizeof(dev->sriov.alias_guid.ports_guid[i].\n\t\t\t\tall_rec_per_port[j].all_recs));\n\t\t}\n\t\tINIT_LIST_HEAD(&dev->sriov.alias_guid.ports_guid[i].cb_list);\n\t\t \n\t\tif (mlx4_ib_sm_guid_assign)\n\t\t\tfor (j = 1; j < NUM_ALIAS_GUID_PER_PORT; j++)\n\t\t\t\tmlx4_set_admin_guid(dev->dev, 0, j, i + 1);\n\t\tfor (j = 0 ; j < NUM_ALIAS_GUID_REC_IN_PORT; j++)\n\t\t\tinvalidate_guid_record(dev, i + 1, j);\n\n\t\tdev->sriov.alias_guid.ports_guid[i].parent = &dev->sriov.alias_guid;\n\t\tdev->sriov.alias_guid.ports_guid[i].port  = i;\n\n\t\tsnprintf(alias_wq_name, sizeof alias_wq_name, \"alias_guid%d\", i);\n\t\tdev->sriov.alias_guid.ports_guid[i].wq =\n\t\t\talloc_ordered_workqueue(alias_wq_name, WQ_MEM_RECLAIM);\n\t\tif (!dev->sriov.alias_guid.ports_guid[i].wq) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_thread;\n\t\t}\n\t\tINIT_DELAYED_WORK(&dev->sriov.alias_guid.ports_guid[i].alias_guid_work,\n\t\t\t  alias_guid_work);\n\t}\n\treturn 0;\n\nerr_thread:\n\tfor (--i; i >= 0; i--) {\n\t\tdestroy_workqueue(dev->sriov.alias_guid.ports_guid[i].wq);\n\t\tdev->sriov.alias_guid.ports_guid[i].wq = NULL;\n\t}\n\nerr_unregister:\n\tib_sa_unregister_client(dev->sriov.alias_guid.sa_client);\n\tkfree(dev->sriov.alias_guid.sa_client);\n\tdev->sriov.alias_guid.sa_client = NULL;\n\tpr_err(\"init_alias_guid_service: Failed. (ret:%d)\\n\", ret);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}