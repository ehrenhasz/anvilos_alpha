{
  "module_name": "ocrdma_verbs.c",
  "hash_id": "d5bbe834e546513ebd52232a2df183af78976e9f9f5628c9cd4a66d913257eb3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c",
  "human_readable_source": " \n\n#include <linux/dma-mapping.h>\n#include <net/addrconf.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/iw_cm.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"ocrdma.h\"\n#include \"ocrdma_hw.h\"\n#include \"ocrdma_verbs.h\"\n#include <rdma/ocrdma-abi.h>\n\nint ocrdma_query_pkey(struct ib_device *ibdev, u32 port, u16 index, u16 *pkey)\n{\n\tif (index > 0)\n\t\treturn -EINVAL;\n\n\t*pkey = 0xffff;\n\treturn 0;\n}\n\nint ocrdma_query_device(struct ib_device *ibdev, struct ib_device_attr *attr,\n\t\t\tstruct ib_udata *uhw)\n{\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\n\n\tif (uhw->inlen || uhw->outlen)\n\t\treturn -EINVAL;\n\n\tmemset(attr, 0, sizeof *attr);\n\tmemcpy(&attr->fw_ver, &dev->attr.fw_ver[0],\n\t       min(sizeof(dev->attr.fw_ver), sizeof(attr->fw_ver)));\n\taddrconf_addr_eui48((u8 *)&attr->sys_image_guid,\n\t\t\t    dev->nic_info.mac_addr);\n\tattr->max_mr_size = dev->attr.max_mr_size;\n\tattr->page_size_cap = 0xffff000;\n\tattr->vendor_id = dev->nic_info.pdev->vendor;\n\tattr->vendor_part_id = dev->nic_info.pdev->device;\n\tattr->hw_ver = dev->asic_id;\n\tattr->max_qp = dev->attr.max_qp;\n\tattr->max_ah = OCRDMA_MAX_AH;\n\tattr->max_qp_wr = dev->attr.max_wqe;\n\n\tattr->device_cap_flags = IB_DEVICE_CURR_QP_STATE_MOD |\n\t\t\t\t\tIB_DEVICE_RC_RNR_NAK_GEN |\n\t\t\t\t\tIB_DEVICE_SHUTDOWN_PORT |\n\t\t\t\t\tIB_DEVICE_SYS_IMAGE_GUID |\n\t\t\t\t\tIB_DEVICE_MEM_MGT_EXTENSIONS;\n\tattr->kernel_cap_flags = IBK_LOCAL_DMA_LKEY;\n\tattr->max_send_sge = dev->attr.max_send_sge;\n\tattr->max_recv_sge = dev->attr.max_recv_sge;\n\tattr->max_sge_rd = dev->attr.max_rdma_sge;\n\tattr->max_cq = dev->attr.max_cq;\n\tattr->max_cqe = dev->attr.max_cqe;\n\tattr->max_mr = dev->attr.max_mr;\n\tattr->max_mw = dev->attr.max_mw;\n\tattr->max_pd = dev->attr.max_pd;\n\tattr->atomic_cap = 0;\n\tattr->max_qp_rd_atom =\n\t    min(dev->attr.max_ord_per_qp, dev->attr.max_ird_per_qp);\n\tattr->max_qp_init_rd_atom = dev->attr.max_ord_per_qp;\n\tattr->max_srq = dev->attr.max_srq;\n\tattr->max_srq_sge = dev->attr.max_srq_sge;\n\tattr->max_srq_wr = dev->attr.max_rqe;\n\tattr->local_ca_ack_delay = dev->attr.local_ca_ack_delay;\n\tattr->max_fast_reg_page_list_len = dev->attr.max_pages_per_frmr;\n\tattr->max_pkeys = 1;\n\treturn 0;\n}\n\nstatic inline void get_link_speed_and_width(struct ocrdma_dev *dev,\n\t\t\t\t\t    u16 *ib_speed, u8 *ib_width)\n{\n\tint status;\n\tu8 speed;\n\n\tstatus = ocrdma_mbx_get_link_speed(dev, &speed, NULL);\n\tif (status)\n\t\tspeed = OCRDMA_PHYS_LINK_SPEED_ZERO;\n\n\tswitch (speed) {\n\tcase OCRDMA_PHYS_LINK_SPEED_1GBPS:\n\t\t*ib_speed = IB_SPEED_SDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\n\tcase OCRDMA_PHYS_LINK_SPEED_10GBPS:\n\t\t*ib_speed = IB_SPEED_QDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\n\tcase OCRDMA_PHYS_LINK_SPEED_20GBPS:\n\t\t*ib_speed = IB_SPEED_DDR;\n\t\t*ib_width = IB_WIDTH_4X;\n\t\tbreak;\n\n\tcase OCRDMA_PHYS_LINK_SPEED_40GBPS:\n\t\t*ib_speed = IB_SPEED_QDR;\n\t\t*ib_width = IB_WIDTH_4X;\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\t*ib_speed = IB_SPEED_SDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t}\n}\n\nint ocrdma_query_port(struct ib_device *ibdev,\n\t\t      u32 port, struct ib_port_attr *props)\n{\n\tenum ib_port_state port_state;\n\tstruct ocrdma_dev *dev;\n\tstruct net_device *netdev;\n\n\t \n\tdev = get_ocrdma_dev(ibdev);\n\tnetdev = dev->nic_info.netdev;\n\tif (netif_running(netdev) && netif_oper_up(netdev)) {\n\t\tport_state = IB_PORT_ACTIVE;\n\t\tprops->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t} else {\n\t\tport_state = IB_PORT_DOWN;\n\t\tprops->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\t}\n\tprops->max_mtu = IB_MTU_4096;\n\tprops->active_mtu = iboe_get_mtu(netdev->mtu);\n\tprops->lid = 0;\n\tprops->lmc = 0;\n\tprops->sm_lid = 0;\n\tprops->sm_sl = 0;\n\tprops->state = port_state;\n\tprops->port_cap_flags = IB_PORT_CM_SUP | IB_PORT_REINIT_SUP |\n\t\t\t\tIB_PORT_DEVICE_MGMT_SUP |\n\t\t\t\tIB_PORT_VENDOR_CLASS_SUP;\n\tprops->ip_gids = true;\n\tprops->gid_tbl_len = OCRDMA_MAX_SGID;\n\tprops->pkey_tbl_len = 1;\n\tprops->bad_pkey_cntr = 0;\n\tprops->qkey_viol_cntr = 0;\n\tget_link_speed_and_width(dev, &props->active_speed,\n\t\t\t\t &props->active_width);\n\tprops->max_msg_sz = 0x80000000;\n\tprops->max_vl_num = 4;\n\treturn 0;\n}\n\nstatic int ocrdma_add_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\n\t\t\t   unsigned long len)\n{\n\tstruct ocrdma_mm *mm;\n\n\tmm = kzalloc(sizeof(*mm), GFP_KERNEL);\n\tif (mm == NULL)\n\t\treturn -ENOMEM;\n\tmm->key.phy_addr = phy_addr;\n\tmm->key.len = len;\n\tINIT_LIST_HEAD(&mm->entry);\n\n\tmutex_lock(&uctx->mm_list_lock);\n\tlist_add_tail(&mm->entry, &uctx->mm_head);\n\tmutex_unlock(&uctx->mm_list_lock);\n\treturn 0;\n}\n\nstatic void ocrdma_del_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\n\t\t\t    unsigned long len)\n{\n\tstruct ocrdma_mm *mm, *tmp;\n\n\tmutex_lock(&uctx->mm_list_lock);\n\tlist_for_each_entry_safe(mm, tmp, &uctx->mm_head, entry) {\n\t\tif (len != mm->key.len && phy_addr != mm->key.phy_addr)\n\t\t\tcontinue;\n\n\t\tlist_del(&mm->entry);\n\t\tkfree(mm);\n\t\tbreak;\n\t}\n\tmutex_unlock(&uctx->mm_list_lock);\n}\n\nstatic bool ocrdma_search_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\n\t\t\t      unsigned long len)\n{\n\tbool found = false;\n\tstruct ocrdma_mm *mm;\n\n\tmutex_lock(&uctx->mm_list_lock);\n\tlist_for_each_entry(mm, &uctx->mm_head, entry) {\n\t\tif (len != mm->key.len && phy_addr != mm->key.phy_addr)\n\t\t\tcontinue;\n\n\t\tfound = true;\n\t\tbreak;\n\t}\n\tmutex_unlock(&uctx->mm_list_lock);\n\treturn found;\n}\n\n\nstatic u16 _ocrdma_pd_mgr_get_bitmap(struct ocrdma_dev *dev, bool dpp_pool)\n{\n\tu16 pd_bitmap_idx = 0;\n\tunsigned long *pd_bitmap;\n\n\tif (dpp_pool) {\n\t\tpd_bitmap = dev->pd_mgr->pd_dpp_bitmap;\n\t\tpd_bitmap_idx = find_first_zero_bit(pd_bitmap,\n\t\t\t\t\t\t    dev->pd_mgr->max_dpp_pd);\n\t\t__set_bit(pd_bitmap_idx, pd_bitmap);\n\t\tdev->pd_mgr->pd_dpp_count++;\n\t\tif (dev->pd_mgr->pd_dpp_count > dev->pd_mgr->pd_dpp_thrsh)\n\t\t\tdev->pd_mgr->pd_dpp_thrsh = dev->pd_mgr->pd_dpp_count;\n\t} else {\n\t\tpd_bitmap = dev->pd_mgr->pd_norm_bitmap;\n\t\tpd_bitmap_idx = find_first_zero_bit(pd_bitmap,\n\t\t\t\t\t\t    dev->pd_mgr->max_normal_pd);\n\t\t__set_bit(pd_bitmap_idx, pd_bitmap);\n\t\tdev->pd_mgr->pd_norm_count++;\n\t\tif (dev->pd_mgr->pd_norm_count > dev->pd_mgr->pd_norm_thrsh)\n\t\t\tdev->pd_mgr->pd_norm_thrsh = dev->pd_mgr->pd_norm_count;\n\t}\n\treturn pd_bitmap_idx;\n}\n\nstatic int _ocrdma_pd_mgr_put_bitmap(struct ocrdma_dev *dev, u16 pd_id,\n\t\t\t\t\tbool dpp_pool)\n{\n\tu16 pd_count;\n\tu16 pd_bit_index;\n\n\tpd_count = dpp_pool ? dev->pd_mgr->pd_dpp_count :\n\t\t\t      dev->pd_mgr->pd_norm_count;\n\tif (pd_count == 0)\n\t\treturn -EINVAL;\n\n\tif (dpp_pool) {\n\t\tpd_bit_index = pd_id - dev->pd_mgr->pd_dpp_start;\n\t\tif (pd_bit_index >= dev->pd_mgr->max_dpp_pd) {\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\t__clear_bit(pd_bit_index, dev->pd_mgr->pd_dpp_bitmap);\n\t\t\tdev->pd_mgr->pd_dpp_count--;\n\t\t}\n\t} else {\n\t\tpd_bit_index = pd_id - dev->pd_mgr->pd_norm_start;\n\t\tif (pd_bit_index >= dev->pd_mgr->max_normal_pd) {\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\t__clear_bit(pd_bit_index, dev->pd_mgr->pd_norm_bitmap);\n\t\t\tdev->pd_mgr->pd_norm_count--;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ocrdma_put_pd_num(struct ocrdma_dev *dev, u16 pd_id,\n\t\t\t\t   bool dpp_pool)\n{\n\tint status;\n\n\tmutex_lock(&dev->dev_lock);\n\tstatus = _ocrdma_pd_mgr_put_bitmap(dev, pd_id, dpp_pool);\n\tmutex_unlock(&dev->dev_lock);\n\treturn status;\n}\n\nstatic int ocrdma_get_pd_num(struct ocrdma_dev *dev, struct ocrdma_pd *pd)\n{\n\tu16 pd_idx = 0;\n\tint status = 0;\n\n\tmutex_lock(&dev->dev_lock);\n\tif (pd->dpp_enabled) {\n\t\t \n\t\tif (dev->pd_mgr->pd_dpp_count < dev->pd_mgr->max_dpp_pd) {\n\t\t\tpd_idx = _ocrdma_pd_mgr_get_bitmap(dev, true);\n\t\t\tpd->id = dev->pd_mgr->pd_dpp_start + pd_idx;\n\t\t\tpd->dpp_page = dev->pd_mgr->dpp_page_index + pd_idx;\n\t\t} else if (dev->pd_mgr->pd_norm_count <\n\t\t\t   dev->pd_mgr->max_normal_pd) {\n\t\t\tpd_idx = _ocrdma_pd_mgr_get_bitmap(dev, false);\n\t\t\tpd->id = dev->pd_mgr->pd_norm_start + pd_idx;\n\t\t\tpd->dpp_enabled = false;\n\t\t} else {\n\t\t\tstatus = -EINVAL;\n\t\t}\n\t} else {\n\t\tif (dev->pd_mgr->pd_norm_count < dev->pd_mgr->max_normal_pd) {\n\t\t\tpd_idx = _ocrdma_pd_mgr_get_bitmap(dev, false);\n\t\t\tpd->id = dev->pd_mgr->pd_norm_start + pd_idx;\n\t\t} else {\n\t\t\tstatus = -EINVAL;\n\t\t}\n\t}\n\tmutex_unlock(&dev->dev_lock);\n\treturn status;\n}\n\n \nstatic int _ocrdma_alloc_pd(struct ocrdma_dev *dev, struct ocrdma_pd *pd,\n\t\t\t    struct ocrdma_ucontext *uctx,\n\t\t\t    struct ib_udata *udata)\n{\n\tint status;\n\n\tif (udata && uctx && dev->attr.max_dpp_pds) {\n\t\tpd->dpp_enabled =\n\t\t\tocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R;\n\t\tpd->num_dpp_qp =\n\t\t\tpd->dpp_enabled ? (dev->nic_info.db_page_size /\n\t\t\t\t\t   dev->attr.wqe_size) : 0;\n\t}\n\n\tif (dev->pd_mgr->pd_prealloc_valid)\n\t\treturn ocrdma_get_pd_num(dev, pd);\n\nretry:\n\tstatus = ocrdma_mbx_alloc_pd(dev, pd);\n\tif (status) {\n\t\tif (pd->dpp_enabled) {\n\t\t\tpd->dpp_enabled = false;\n\t\t\tpd->num_dpp_qp = 0;\n\t\t\tgoto retry;\n\t\t}\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int is_ucontext_pd(struct ocrdma_ucontext *uctx,\n\t\t\t\t struct ocrdma_pd *pd)\n{\n\treturn (uctx->cntxt_pd == pd);\n}\n\nstatic void _ocrdma_dealloc_pd(struct ocrdma_dev *dev,\n\t\t\t      struct ocrdma_pd *pd)\n{\n\tif (dev->pd_mgr->pd_prealloc_valid)\n\t\tocrdma_put_pd_num(dev, pd->id, pd->dpp_enabled);\n\telse\n\t\tocrdma_mbx_dealloc_pd(dev, pd);\n}\n\nstatic int ocrdma_alloc_ucontext_pd(struct ocrdma_dev *dev,\n\t\t\t\t    struct ocrdma_ucontext *uctx,\n\t\t\t\t    struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = &dev->ibdev;\n\tstruct ib_pd *pd;\n\tint status;\n\n\tpd = rdma_zalloc_drv_obj(ibdev, ib_pd);\n\tif (!pd)\n\t\treturn -ENOMEM;\n\n\tpd->device  = ibdev;\n\tuctx->cntxt_pd = get_ocrdma_pd(pd);\n\n\tstatus = _ocrdma_alloc_pd(dev, uctx->cntxt_pd, uctx, udata);\n\tif (status) {\n\t\tkfree(uctx->cntxt_pd);\n\t\tgoto err;\n\t}\n\n\tuctx->cntxt_pd->uctx = uctx;\n\tuctx->cntxt_pd->ibpd.device = &dev->ibdev;\nerr:\n\treturn status;\n}\n\nstatic void ocrdma_dealloc_ucontext_pd(struct ocrdma_ucontext *uctx)\n{\n\tstruct ocrdma_pd *pd = uctx->cntxt_pd;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(pd->ibpd.device);\n\n\tif (uctx->pd_in_use) {\n\t\tpr_err(\"%s(%d) Freeing in use pdid=0x%x.\\n\",\n\t\t       __func__, dev->id, pd->id);\n\t}\n\tuctx->cntxt_pd = NULL;\n\t_ocrdma_dealloc_pd(dev, pd);\n\tkfree(pd);\n}\n\nstatic struct ocrdma_pd *ocrdma_get_ucontext_pd(struct ocrdma_ucontext *uctx)\n{\n\tstruct ocrdma_pd *pd = NULL;\n\n\tmutex_lock(&uctx->mm_list_lock);\n\tif (!uctx->pd_in_use) {\n\t\tuctx->pd_in_use = true;\n\t\tpd = uctx->cntxt_pd;\n\t}\n\tmutex_unlock(&uctx->mm_list_lock);\n\n\treturn pd;\n}\n\nstatic void ocrdma_release_ucontext_pd(struct ocrdma_ucontext *uctx)\n{\n\tmutex_lock(&uctx->mm_list_lock);\n\tuctx->pd_in_use = false;\n\tmutex_unlock(&uctx->mm_list_lock);\n}\n\nint ocrdma_alloc_ucontext(struct ib_ucontext *uctx, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tint status;\n\tstruct ocrdma_ucontext *ctx = get_ocrdma_ucontext(uctx);\n\tstruct ocrdma_alloc_ucontext_resp resp = {};\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\n\tstruct pci_dev *pdev = dev->nic_info.pdev;\n\tu32 map_len = roundup(sizeof(u32) * 2048, PAGE_SIZE);\n\n\tif (!udata)\n\t\treturn -EFAULT;\n\tINIT_LIST_HEAD(&ctx->mm_head);\n\tmutex_init(&ctx->mm_list_lock);\n\n\tctx->ah_tbl.va = dma_alloc_coherent(&pdev->dev, map_len,\n\t\t\t\t\t    &ctx->ah_tbl.pa, GFP_KERNEL);\n\tif (!ctx->ah_tbl.va)\n\t\treturn -ENOMEM;\n\n\tctx->ah_tbl.len = map_len;\n\n\tresp.ah_tbl_len = ctx->ah_tbl.len;\n\tresp.ah_tbl_page = virt_to_phys(ctx->ah_tbl.va);\n\n\tstatus = ocrdma_add_mmap(ctx, resp.ah_tbl_page, resp.ah_tbl_len);\n\tif (status)\n\t\tgoto map_err;\n\n\tstatus = ocrdma_alloc_ucontext_pd(dev, ctx, udata);\n\tif (status)\n\t\tgoto pd_err;\n\n\tresp.dev_id = dev->id;\n\tresp.max_inline_data = dev->attr.max_inline_data;\n\tresp.wqe_size = dev->attr.wqe_size;\n\tresp.rqe_size = dev->attr.rqe_size;\n\tresp.dpp_wqe_size = dev->attr.wqe_size;\n\n\tmemcpy(resp.fw_ver, dev->attr.fw_ver, sizeof(resp.fw_ver));\n\tstatus = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\tif (status)\n\t\tgoto cpy_err;\n\treturn 0;\n\ncpy_err:\n\tocrdma_dealloc_ucontext_pd(ctx);\npd_err:\n\tocrdma_del_mmap(ctx, ctx->ah_tbl.pa, ctx->ah_tbl.len);\nmap_err:\n\tdma_free_coherent(&pdev->dev, ctx->ah_tbl.len, ctx->ah_tbl.va,\n\t\t\t  ctx->ah_tbl.pa);\n\treturn status;\n}\n\nvoid ocrdma_dealloc_ucontext(struct ib_ucontext *ibctx)\n{\n\tstruct ocrdma_mm *mm, *tmp;\n\tstruct ocrdma_ucontext *uctx = get_ocrdma_ucontext(ibctx);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibctx->device);\n\tstruct pci_dev *pdev = dev->nic_info.pdev;\n\n\tocrdma_dealloc_ucontext_pd(uctx);\n\n\tocrdma_del_mmap(uctx, uctx->ah_tbl.pa, uctx->ah_tbl.len);\n\tdma_free_coherent(&pdev->dev, uctx->ah_tbl.len, uctx->ah_tbl.va,\n\t\t\t  uctx->ah_tbl.pa);\n\n\tlist_for_each_entry_safe(mm, tmp, &uctx->mm_head, entry) {\n\t\tlist_del(&mm->entry);\n\t\tkfree(mm);\n\t}\n}\n\nint ocrdma_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)\n{\n\tstruct ocrdma_ucontext *ucontext = get_ocrdma_ucontext(context);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(context->device);\n\tunsigned long vm_page = vma->vm_pgoff << PAGE_SHIFT;\n\tu64 unmapped_db = (u64) dev->nic_info.unmapped_db;\n\tunsigned long len = (vma->vm_end - vma->vm_start);\n\tint status;\n\tbool found;\n\n\tif (vma->vm_start & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\tfound = ocrdma_search_mmap(ucontext, vma->vm_pgoff << PAGE_SHIFT, len);\n\tif (!found)\n\t\treturn -EINVAL;\n\n\tif ((vm_page >= unmapped_db) && (vm_page <= (unmapped_db +\n\t\tdev->nic_info.db_total_size)) &&\n\t\t(len <=\tdev->nic_info.db_page_size)) {\n\t\tif (vma->vm_flags & VM_READ)\n\t\t\treturn -EPERM;\n\n\t\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\t\tstatus = io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t\t\t    len, vma->vm_page_prot);\n\t} else if (dev->nic_info.dpp_unmapped_len &&\n\t\t(vm_page >= (u64) dev->nic_info.dpp_unmapped_addr) &&\n\t\t(vm_page <= (u64) (dev->nic_info.dpp_unmapped_addr +\n\t\t\tdev->nic_info.dpp_unmapped_len)) &&\n\t\t(len <= dev->nic_info.dpp_unmapped_len)) {\n\t\tif (vma->vm_flags & VM_READ)\n\t\t\treturn -EPERM;\n\n\t\tvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\n\t\tstatus = io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t\t\t    len, vma->vm_page_prot);\n\t} else {\n\t\tstatus = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t\t vma->vm_pgoff, len, vma->vm_page_prot);\n\t}\n\treturn status;\n}\n\nstatic int ocrdma_copy_pd_uresp(struct ocrdma_dev *dev, struct ocrdma_pd *pd,\n\t\t\t\tstruct ib_udata *udata)\n{\n\tint status;\n\tu64 db_page_addr;\n\tu64 dpp_page_addr = 0;\n\tu32 db_page_size;\n\tstruct ocrdma_alloc_pd_uresp rsp;\n\tstruct ocrdma_ucontext *uctx = rdma_udata_to_drv_context(\n\t\tudata, struct ocrdma_ucontext, ibucontext);\n\n\tmemset(&rsp, 0, sizeof(rsp));\n\trsp.id = pd->id;\n\trsp.dpp_enabled = pd->dpp_enabled;\n\tdb_page_addr = ocrdma_get_db_addr(dev, pd->id);\n\tdb_page_size = dev->nic_info.db_page_size;\n\n\tstatus = ocrdma_add_mmap(uctx, db_page_addr, db_page_size);\n\tif (status)\n\t\treturn status;\n\n\tif (pd->dpp_enabled) {\n\t\tdpp_page_addr = dev->nic_info.dpp_unmapped_addr +\n\t\t\t\t(pd->id * PAGE_SIZE);\n\t\tstatus = ocrdma_add_mmap(uctx, dpp_page_addr,\n\t\t\t\t PAGE_SIZE);\n\t\tif (status)\n\t\t\tgoto dpp_map_err;\n\t\trsp.dpp_page_addr_hi = upper_32_bits(dpp_page_addr);\n\t\trsp.dpp_page_addr_lo = dpp_page_addr;\n\t}\n\n\tstatus = ib_copy_to_udata(udata, &rsp, sizeof(rsp));\n\tif (status)\n\t\tgoto ucopy_err;\n\n\tpd->uctx = uctx;\n\treturn 0;\n\nucopy_err:\n\tif (pd->dpp_enabled)\n\t\tocrdma_del_mmap(pd->uctx, dpp_page_addr, PAGE_SIZE);\ndpp_map_err:\n\tocrdma_del_mmap(pd->uctx, db_page_addr, db_page_size);\n\treturn status;\n}\n\nint ocrdma_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\n\tstruct ocrdma_pd *pd;\n\tint status;\n\tu8 is_uctx_pd = false;\n\tstruct ocrdma_ucontext *uctx = rdma_udata_to_drv_context(\n\t\tudata, struct ocrdma_ucontext, ibucontext);\n\n\tif (udata) {\n\t\tpd = ocrdma_get_ucontext_pd(uctx);\n\t\tif (pd) {\n\t\t\tis_uctx_pd = true;\n\t\t\tgoto pd_mapping;\n\t\t}\n\t}\n\n\tpd = get_ocrdma_pd(ibpd);\n\tstatus = _ocrdma_alloc_pd(dev, pd, uctx, udata);\n\tif (status)\n\t\tgoto exit;\n\npd_mapping:\n\tif (udata) {\n\t\tstatus = ocrdma_copy_pd_uresp(dev, pd, udata);\n\t\tif (status)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tif (is_uctx_pd)\n\t\tocrdma_release_ucontext_pd(uctx);\n\telse\n\t\t_ocrdma_dealloc_pd(dev, pd);\nexit:\n\treturn status;\n}\n\nint ocrdma_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\n\tstruct ocrdma_ucontext *uctx = NULL;\n\tu64 usr_db;\n\n\tuctx = pd->uctx;\n\tif (uctx) {\n\t\tu64 dpp_db = dev->nic_info.dpp_unmapped_addr +\n\t\t\t(pd->id * PAGE_SIZE);\n\t\tif (pd->dpp_enabled)\n\t\t\tocrdma_del_mmap(pd->uctx, dpp_db, PAGE_SIZE);\n\t\tusr_db = ocrdma_get_db_addr(dev, pd->id);\n\t\tocrdma_del_mmap(pd->uctx, usr_db, dev->nic_info.db_page_size);\n\n\t\tif (is_ucontext_pd(uctx, pd)) {\n\t\t\tocrdma_release_ucontext_pd(uctx);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t_ocrdma_dealloc_pd(dev, pd);\n\treturn 0;\n}\n\nstatic int ocrdma_alloc_lkey(struct ocrdma_dev *dev, struct ocrdma_mr *mr,\n\t\t\t    u32 pdid, int acc, u32 num_pbls, u32 addr_check)\n{\n\tint status;\n\n\tmr->hwmr.fr_mr = 0;\n\tmr->hwmr.local_rd = 1;\n\tmr->hwmr.remote_rd = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\n\tmr->hwmr.remote_wr = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\n\tmr->hwmr.local_wr = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\n\tmr->hwmr.mw_bind = (acc & IB_ACCESS_MW_BIND) ? 1 : 0;\n\tmr->hwmr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\n\tmr->hwmr.num_pbls = num_pbls;\n\n\tstatus = ocrdma_mbx_alloc_lkey(dev, &mr->hwmr, pdid, addr_check);\n\tif (status)\n\t\treturn status;\n\n\tmr->ibmr.lkey = mr->hwmr.lkey;\n\tif (mr->hwmr.remote_wr || mr->hwmr.remote_rd)\n\t\tmr->ibmr.rkey = mr->hwmr.lkey;\n\treturn 0;\n}\n\nstruct ib_mr *ocrdma_get_dma_mr(struct ib_pd *ibpd, int acc)\n{\n\tint status;\n\tstruct ocrdma_mr *mr;\n\tstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\n\n\tif (acc & IB_ACCESS_REMOTE_WRITE && !(acc & IB_ACCESS_LOCAL_WRITE)) {\n\t\tpr_err(\"%s err, invalid access rights\\n\", __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tstatus = ocrdma_alloc_lkey(dev, mr, pd->id, acc, 0,\n\t\t\t\t   OCRDMA_ADDR_CHECK_DISABLE);\n\tif (status) {\n\t\tkfree(mr);\n\t\treturn ERR_PTR(status);\n\t}\n\n\treturn &mr->ibmr;\n}\n\nstatic void ocrdma_free_mr_pbl_tbl(struct ocrdma_dev *dev,\n\t\t\t\t   struct ocrdma_hw_mr *mr)\n{\n\tstruct pci_dev *pdev = dev->nic_info.pdev;\n\tint i = 0;\n\n\tif (mr->pbl_table) {\n\t\tfor (i = 0; i < mr->num_pbls; i++) {\n\t\t\tif (!mr->pbl_table[i].va)\n\t\t\t\tcontinue;\n\t\t\tdma_free_coherent(&pdev->dev, mr->pbl_size,\n\t\t\t\t\t  mr->pbl_table[i].va,\n\t\t\t\t\t  mr->pbl_table[i].pa);\n\t\t}\n\t\tkfree(mr->pbl_table);\n\t\tmr->pbl_table = NULL;\n\t}\n}\n\nstatic int ocrdma_get_pbl_info(struct ocrdma_dev *dev, struct ocrdma_mr *mr,\n\t\t\t      u32 num_pbes)\n{\n\tu32 num_pbls = 0;\n\tu32 idx = 0;\n\tint status = 0;\n\tu32 pbl_size;\n\n\tdo {\n\t\tpbl_size = OCRDMA_MIN_HPAGE_SIZE * (1 << idx);\n\t\tif (pbl_size > MAX_OCRDMA_PBL_SIZE) {\n\t\t\tstatus = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tnum_pbls = roundup(num_pbes, (pbl_size / sizeof(u64)));\n\t\tnum_pbls = num_pbls / (pbl_size / sizeof(u64));\n\t\tidx++;\n\t} while (num_pbls >= dev->attr.max_num_mr_pbl);\n\n\tmr->hwmr.num_pbes = num_pbes;\n\tmr->hwmr.num_pbls = num_pbls;\n\tmr->hwmr.pbl_size = pbl_size;\n\treturn status;\n}\n\nstatic int ocrdma_build_pbl_tbl(struct ocrdma_dev *dev, struct ocrdma_hw_mr *mr)\n{\n\tint status = 0;\n\tint i;\n\tu32 dma_len = mr->pbl_size;\n\tstruct pci_dev *pdev = dev->nic_info.pdev;\n\tvoid *va;\n\tdma_addr_t pa;\n\n\tmr->pbl_table = kcalloc(mr->num_pbls, sizeof(struct ocrdma_pbl),\n\t\t\t\tGFP_KERNEL);\n\n\tif (!mr->pbl_table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < mr->num_pbls; i++) {\n\t\tva = dma_alloc_coherent(&pdev->dev, dma_len, &pa, GFP_KERNEL);\n\t\tif (!va) {\n\t\t\tocrdma_free_mr_pbl_tbl(dev, mr);\n\t\t\tstatus = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tmr->pbl_table[i].va = va;\n\t\tmr->pbl_table[i].pa = pa;\n\t}\n\treturn status;\n}\n\nstatic void build_user_pbes(struct ocrdma_dev *dev, struct ocrdma_mr *mr)\n{\n\tstruct ocrdma_pbe *pbe;\n\tstruct ib_block_iter biter;\n\tstruct ocrdma_pbl *pbl_tbl = mr->hwmr.pbl_table;\n\tint pbe_cnt;\n\tu64 pg_addr;\n\n\tif (!mr->hwmr.num_pbes)\n\t\treturn;\n\n\tpbe = (struct ocrdma_pbe *)pbl_tbl->va;\n\tpbe_cnt = 0;\n\n\trdma_umem_for_each_dma_block (mr->umem, &biter, PAGE_SIZE) {\n\t\t \n\t\tpg_addr = rdma_block_iter_dma_address(&biter);\n\t\tpbe->pa_lo = cpu_to_le32(pg_addr);\n\t\tpbe->pa_hi = cpu_to_le32(upper_32_bits(pg_addr));\n\t\tpbe_cnt += 1;\n\t\tpbe++;\n\n\t\t \n\t\tif (pbe_cnt == (mr->hwmr.pbl_size / sizeof(u64))) {\n\t\t\tpbl_tbl++;\n\t\t\tpbe = (struct ocrdma_pbe *)pbl_tbl->va;\n\t\t\tpbe_cnt = 0;\n\t\t}\n\t}\n}\n\nstruct ib_mr *ocrdma_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 len,\n\t\t\t\t u64 usr_addr, int acc, struct ib_udata *udata)\n{\n\tint status = -ENOMEM;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\n\tstruct ocrdma_mr *mr;\n\tstruct ocrdma_pd *pd;\n\n\tpd = get_ocrdma_pd(ibpd);\n\n\tif (acc & IB_ACCESS_REMOTE_WRITE && !(acc & IB_ACCESS_LOCAL_WRITE))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(status);\n\tmr->umem = ib_umem_get(ibpd->device, start, len, acc);\n\tif (IS_ERR(mr->umem)) {\n\t\tstatus = -EFAULT;\n\t\tgoto umem_err;\n\t}\n\tstatus = ocrdma_get_pbl_info(\n\t\tdev, mr, ib_umem_num_dma_blocks(mr->umem, PAGE_SIZE));\n\tif (status)\n\t\tgoto umem_err;\n\n\tmr->hwmr.pbe_size = PAGE_SIZE;\n\tmr->hwmr.va = usr_addr;\n\tmr->hwmr.len = len;\n\tmr->hwmr.remote_wr = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\n\tmr->hwmr.remote_rd = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\n\tmr->hwmr.local_wr = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\n\tmr->hwmr.local_rd = 1;\n\tmr->hwmr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\n\tstatus = ocrdma_build_pbl_tbl(dev, &mr->hwmr);\n\tif (status)\n\t\tgoto umem_err;\n\tbuild_user_pbes(dev, mr);\n\tstatus = ocrdma_reg_mr(dev, &mr->hwmr, pd->id, acc);\n\tif (status)\n\t\tgoto mbx_err;\n\tmr->ibmr.lkey = mr->hwmr.lkey;\n\tif (mr->hwmr.remote_wr || mr->hwmr.remote_rd)\n\t\tmr->ibmr.rkey = mr->hwmr.lkey;\n\n\treturn &mr->ibmr;\n\nmbx_err:\n\tocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\numem_err:\n\tkfree(mr);\n\treturn ERR_PTR(status);\n}\n\nint ocrdma_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata)\n{\n\tstruct ocrdma_mr *mr = get_ocrdma_mr(ib_mr);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ib_mr->device);\n\n\t(void) ocrdma_mbx_dealloc_lkey(dev, mr->hwmr.fr_mr, mr->hwmr.lkey);\n\n\tkfree(mr->pages);\n\tocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\n\n\t \n\tib_umem_release(mr->umem);\n\tkfree(mr);\n\n\t \n\tif (dev->mqe_ctx.fw_error_state) {\n\t\tpr_err(\"%s(%d) fw not responding.\\n\",\n\t\t       __func__, dev->id);\n\t}\n\treturn 0;\n}\n\nstatic int ocrdma_copy_cq_uresp(struct ocrdma_dev *dev, struct ocrdma_cq *cq,\n\t\t\t\tstruct ib_udata *udata)\n{\n\tint status;\n\tstruct ocrdma_ucontext *uctx = rdma_udata_to_drv_context(\n\t\tudata, struct ocrdma_ucontext, ibucontext);\n\tstruct ocrdma_create_cq_uresp uresp;\n\n\t \n\tif (!udata)\n\t\treturn -EINVAL;\n\n\tmemset(&uresp, 0, sizeof(uresp));\n\turesp.cq_id = cq->id;\n\turesp.page_size = PAGE_ALIGN(cq->len);\n\turesp.num_pages = 1;\n\turesp.max_hw_cqe = cq->max_hw_cqe;\n\turesp.page_addr[0] = virt_to_phys(cq->va);\n\turesp.db_page_addr =  ocrdma_get_db_addr(dev, uctx->cntxt_pd->id);\n\turesp.db_page_size = dev->nic_info.db_page_size;\n\turesp.phase_change = cq->phase_change ? 1 : 0;\n\tstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (status) {\n\t\tpr_err(\"%s(%d) copy error cqid=0x%x.\\n\",\n\t\t       __func__, dev->id, cq->id);\n\t\tgoto err;\n\t}\n\tstatus = ocrdma_add_mmap(uctx, uresp.db_page_addr, uresp.db_page_size);\n\tif (status)\n\t\tgoto err;\n\tstatus = ocrdma_add_mmap(uctx, uresp.page_addr[0], uresp.page_size);\n\tif (status) {\n\t\tocrdma_del_mmap(uctx, uresp.db_page_addr, uresp.db_page_size);\n\t\tgoto err;\n\t}\n\tcq->ucontext = uctx;\nerr:\n\treturn status;\n}\n\nint ocrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t     struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tint entries = attr->cqe;\n\tstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\n\tstruct ocrdma_ucontext *uctx = rdma_udata_to_drv_context(\n\t\tudata, struct ocrdma_ucontext, ibucontext);\n\tu16 pd_id = 0;\n\tint status;\n\tstruct ocrdma_create_cq_ureq ureq;\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (udata) {\n\t\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\t\treturn -EFAULT;\n\t} else\n\t\tureq.dpp_cq = 0;\n\n\tspin_lock_init(&cq->cq_lock);\n\tspin_lock_init(&cq->comp_handler_lock);\n\tINIT_LIST_HEAD(&cq->sq_head);\n\tINIT_LIST_HEAD(&cq->rq_head);\n\n\tif (udata)\n\t\tpd_id = uctx->cntxt_pd->id;\n\n\tstatus = ocrdma_mbx_create_cq(dev, cq, entries, ureq.dpp_cq, pd_id);\n\tif (status)\n\t\treturn status;\n\n\tif (udata) {\n\t\tstatus = ocrdma_copy_cq_uresp(dev, cq, udata);\n\t\tif (status)\n\t\t\tgoto ctx_err;\n\t}\n\tcq->phase = OCRDMA_CQE_VALID;\n\tdev->cq_tbl[cq->id] = cq;\n\treturn 0;\n\nctx_err:\n\tocrdma_mbx_destroy_cq(dev, cq);\n\treturn status;\n}\n\nint ocrdma_resize_cq(struct ib_cq *ibcq, int new_cnt,\n\t\t     struct ib_udata *udata)\n{\n\tint status = 0;\n\tstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\n\n\tif (new_cnt < 1 || new_cnt > cq->max_hw_cqe) {\n\t\tstatus = -EINVAL;\n\t\treturn status;\n\t}\n\tibcq->cqe = new_cnt;\n\treturn status;\n}\n\nstatic void ocrdma_flush_cq(struct ocrdma_cq *cq)\n{\n\tint cqe_cnt;\n\tint valid_count = 0;\n\tunsigned long flags;\n\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(cq->ibcq.device);\n\tstruct ocrdma_cqe *cqe = NULL;\n\n\tcqe = cq->va;\n\tcqe_cnt = cq->cqe_cnt;\n\n\t \n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\twhile (cqe_cnt) {\n\t\tif (is_cqe_valid(cq, cqe))\n\t\t\tvalid_count++;\n\t\tcqe++;\n\t\tcqe_cnt--;\n\t}\n\tocrdma_ring_cq_db(dev, cq->id, false, false, valid_count);\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n}\n\nint ocrdma_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)\n{\n\tstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\n\tstruct ocrdma_eq *eq = NULL;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\n\tint pdid = 0;\n\tu32 irq, indx;\n\n\tdev->cq_tbl[cq->id] = NULL;\n\tindx = ocrdma_get_eq_table_index(dev, cq->eqn);\n\n\teq = &dev->eq_tbl[indx];\n\tirq = ocrdma_get_irq(dev, eq);\n\tsynchronize_irq(irq);\n\tocrdma_flush_cq(cq);\n\n\tocrdma_mbx_destroy_cq(dev, cq);\n\tif (cq->ucontext) {\n\t\tpdid = cq->ucontext->cntxt_pd->id;\n\t\tocrdma_del_mmap(cq->ucontext, (u64) cq->pa,\n\t\t\t\tPAGE_ALIGN(cq->len));\n\t\tocrdma_del_mmap(cq->ucontext,\n\t\t\t\tocrdma_get_db_addr(dev, pdid),\n\t\t\t\tdev->nic_info.db_page_size);\n\t}\n\treturn 0;\n}\n\nstatic int ocrdma_add_qpn_map(struct ocrdma_dev *dev, struct ocrdma_qp *qp)\n{\n\tint status = -EINVAL;\n\n\tif (qp->id < OCRDMA_MAX_QP && dev->qp_tbl[qp->id] == NULL) {\n\t\tdev->qp_tbl[qp->id] = qp;\n\t\tstatus = 0;\n\t}\n\treturn status;\n}\n\nstatic void ocrdma_del_qpn_map(struct ocrdma_dev *dev, struct ocrdma_qp *qp)\n{\n\tdev->qp_tbl[qp->id] = NULL;\n}\n\nstatic int ocrdma_check_qp_params(struct ib_pd *ibpd, struct ocrdma_dev *dev,\n\t\t\t\t  struct ib_qp_init_attr *attrs,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tif ((attrs->qp_type != IB_QPT_GSI) &&\n\t    (attrs->qp_type != IB_QPT_RC) &&\n\t    (attrs->qp_type != IB_QPT_UC) &&\n\t    (attrs->qp_type != IB_QPT_UD)) {\n\t\tpr_err(\"%s(%d) unsupported qp type=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->qp_type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\t \n\tif ((attrs->qp_type != IB_QPT_GSI) &&\n\t    (attrs->cap.max_send_wr > dev->attr.max_wqe)) {\n\t\tpr_err(\"%s(%d) unsupported send_wr=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->cap.max_send_wr);\n\t\tpr_err(\"%s(%d) supported send_wr=0x%x\\n\",\n\t\t       __func__, dev->id, dev->attr.max_wqe);\n\t\treturn -EINVAL;\n\t}\n\tif (!attrs->srq && (attrs->cap.max_recv_wr > dev->attr.max_rqe)) {\n\t\tpr_err(\"%s(%d) unsupported recv_wr=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->cap.max_recv_wr);\n\t\tpr_err(\"%s(%d) supported recv_wr=0x%x\\n\",\n\t\t       __func__, dev->id, dev->attr.max_rqe);\n\t\treturn -EINVAL;\n\t}\n\tif (attrs->cap.max_inline_data > dev->attr.max_inline_data) {\n\t\tpr_err(\"%s(%d) unsupported inline data size=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->cap.max_inline_data);\n\t\tpr_err(\"%s(%d) supported inline data size=0x%x\\n\",\n\t\t       __func__, dev->id, dev->attr.max_inline_data);\n\t\treturn -EINVAL;\n\t}\n\tif (attrs->cap.max_send_sge > dev->attr.max_send_sge) {\n\t\tpr_err(\"%s(%d) unsupported send_sge=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->cap.max_send_sge);\n\t\tpr_err(\"%s(%d) supported send_sge=0x%x\\n\",\n\t\t       __func__, dev->id, dev->attr.max_send_sge);\n\t\treturn -EINVAL;\n\t}\n\tif (attrs->cap.max_recv_sge > dev->attr.max_recv_sge) {\n\t\tpr_err(\"%s(%d) unsupported recv_sge=0x%x requested\\n\",\n\t\t       __func__, dev->id, attrs->cap.max_recv_sge);\n\t\tpr_err(\"%s(%d) supported recv_sge=0x%x\\n\",\n\t\t       __func__, dev->id, dev->attr.max_recv_sge);\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (udata && attrs->qp_type == IB_QPT_GSI) {\n\t\tpr_err\n\t\t    (\"%s(%d) Userspace can't create special QPs of type=0x%x\\n\",\n\t\t     __func__, dev->id, attrs->qp_type);\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (attrs->qp_type == IB_QPT_GSI && dev->gsi_qp_created) {\n\t\tpr_err(\"%s(%d) GSI special QPs already created.\\n\",\n\t\t       __func__, dev->id);\n\t\treturn -EINVAL;\n\t}\n\t \n\tif ((attrs->qp_type != IB_QPT_GSI) && (dev->gsi_qp_created)) {\n\t\tif ((dev->gsi_sqcq == get_ocrdma_cq(attrs->send_cq)) ||\n\t\t\t(dev->gsi_rqcq == get_ocrdma_cq(attrs->recv_cq))) {\n\t\t\tpr_err(\"%s(%d) Consumer QP cannot use GSI CQs.\\n\",\n\t\t\t\t__func__, dev->id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int ocrdma_copy_qp_uresp(struct ocrdma_qp *qp,\n\t\t\t\tstruct ib_udata *udata, int dpp_offset,\n\t\t\t\tint dpp_credit_lmt, int srq)\n{\n\tint status;\n\tu64 usr_db;\n\tstruct ocrdma_create_qp_uresp uresp;\n\tstruct ocrdma_pd *pd = qp->pd;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(pd->ibpd.device);\n\n\tmemset(&uresp, 0, sizeof(uresp));\n\tusr_db = dev->nic_info.unmapped_db +\n\t\t\t(pd->id * dev->nic_info.db_page_size);\n\turesp.qp_id = qp->id;\n\turesp.sq_dbid = qp->sq.dbid;\n\turesp.num_sq_pages = 1;\n\turesp.sq_page_size = PAGE_ALIGN(qp->sq.len);\n\turesp.sq_page_addr[0] = virt_to_phys(qp->sq.va);\n\turesp.num_wqe_allocated = qp->sq.max_cnt;\n\tif (!srq) {\n\t\turesp.rq_dbid = qp->rq.dbid;\n\t\turesp.num_rq_pages = 1;\n\t\turesp.rq_page_size = PAGE_ALIGN(qp->rq.len);\n\t\turesp.rq_page_addr[0] = virt_to_phys(qp->rq.va);\n\t\turesp.num_rqe_allocated = qp->rq.max_cnt;\n\t}\n\turesp.db_page_addr = usr_db;\n\turesp.db_page_size = dev->nic_info.db_page_size;\n\turesp.db_sq_offset = OCRDMA_DB_GEN2_SQ_OFFSET;\n\turesp.db_rq_offset = OCRDMA_DB_GEN2_RQ_OFFSET;\n\turesp.db_shift = OCRDMA_DB_RQ_SHIFT;\n\n\tif (qp->dpp_enabled) {\n\t\turesp.dpp_credit = dpp_credit_lmt;\n\t\turesp.dpp_offset = dpp_offset;\n\t}\n\tstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (status) {\n\t\tpr_err(\"%s(%d) user copy error.\\n\", __func__, dev->id);\n\t\tgoto err;\n\t}\n\tstatus = ocrdma_add_mmap(pd->uctx, uresp.sq_page_addr[0],\n\t\t\t\t uresp.sq_page_size);\n\tif (status)\n\t\tgoto err;\n\n\tif (!srq) {\n\t\tstatus = ocrdma_add_mmap(pd->uctx, uresp.rq_page_addr[0],\n\t\t\t\t\t uresp.rq_page_size);\n\t\tif (status)\n\t\t\tgoto rq_map_err;\n\t}\n\treturn status;\nrq_map_err:\n\tocrdma_del_mmap(pd->uctx, uresp.sq_page_addr[0], uresp.sq_page_size);\nerr:\n\treturn status;\n}\n\nstatic void ocrdma_set_qp_db(struct ocrdma_dev *dev, struct ocrdma_qp *qp,\n\t\t\t     struct ocrdma_pd *pd)\n{\n\tif (ocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R) {\n\t\tqp->sq_db = dev->nic_info.db +\n\t\t\t(pd->id * dev->nic_info.db_page_size) +\n\t\t\tOCRDMA_DB_GEN2_SQ_OFFSET;\n\t\tqp->rq_db = dev->nic_info.db +\n\t\t\t(pd->id * dev->nic_info.db_page_size) +\n\t\t\tOCRDMA_DB_GEN2_RQ_OFFSET;\n\t} else {\n\t\tqp->sq_db = dev->nic_info.db +\n\t\t\t(pd->id * dev->nic_info.db_page_size) +\n\t\t\tOCRDMA_DB_SQ_OFFSET;\n\t\tqp->rq_db = dev->nic_info.db +\n\t\t\t(pd->id * dev->nic_info.db_page_size) +\n\t\t\tOCRDMA_DB_RQ_OFFSET;\n\t}\n}\n\nstatic int ocrdma_alloc_wr_id_tbl(struct ocrdma_qp *qp)\n{\n\tqp->wqe_wr_id_tbl =\n\t    kcalloc(qp->sq.max_cnt, sizeof(*(qp->wqe_wr_id_tbl)),\n\t\t    GFP_KERNEL);\n\tif (qp->wqe_wr_id_tbl == NULL)\n\t\treturn -ENOMEM;\n\tqp->rqe_wr_id_tbl =\n\t    kcalloc(qp->rq.max_cnt, sizeof(u64), GFP_KERNEL);\n\tif (qp->rqe_wr_id_tbl == NULL)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ocrdma_set_qp_init_params(struct ocrdma_qp *qp,\n\t\t\t\t      struct ocrdma_pd *pd,\n\t\t\t\t      struct ib_qp_init_attr *attrs)\n{\n\tqp->pd = pd;\n\tspin_lock_init(&qp->q_lock);\n\tINIT_LIST_HEAD(&qp->sq_entry);\n\tINIT_LIST_HEAD(&qp->rq_entry);\n\n\tqp->qp_type = attrs->qp_type;\n\tqp->cap_flags = OCRDMA_QP_INB_RD | OCRDMA_QP_INB_WR;\n\tqp->max_inline_data = attrs->cap.max_inline_data;\n\tqp->sq.max_sges = attrs->cap.max_send_sge;\n\tqp->rq.max_sges = attrs->cap.max_recv_sge;\n\tqp->state = OCRDMA_QPS_RST;\n\tqp->signaled = attrs->sq_sig_type == IB_SIGNAL_ALL_WR;\n}\n\nstatic void ocrdma_store_gsi_qp_cq(struct ocrdma_dev *dev,\n\t\t\t\t   struct ib_qp_init_attr *attrs)\n{\n\tif (attrs->qp_type == IB_QPT_GSI) {\n\t\tdev->gsi_qp_created = 1;\n\t\tdev->gsi_sqcq = get_ocrdma_cq(attrs->send_cq);\n\t\tdev->gsi_rqcq = get_ocrdma_cq(attrs->recv_cq);\n\t}\n}\n\nint ocrdma_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *attrs,\n\t\t     struct ib_udata *udata)\n{\n\tint status;\n\tstruct ib_pd *ibpd = ibqp->pd;\n\tstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\n\tstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibqp->device);\n\tstruct ocrdma_create_qp_ureq ureq;\n\tu16 dpp_credit_lmt, dpp_offset;\n\n\tif (attrs->create_flags)\n\t\treturn -EOPNOTSUPP;\n\n\tstatus = ocrdma_check_qp_params(ibpd, dev, attrs, udata);\n\tif (status)\n\t\tgoto gen_err;\n\n\tmemset(&ureq, 0, sizeof(ureq));\n\tif (udata) {\n\t\tif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\n\t\t\treturn -EFAULT;\n\t}\n\tocrdma_set_qp_init_params(qp, pd, attrs);\n\tif (udata == NULL)\n\t\tqp->cap_flags |= (OCRDMA_QP_MW_BIND | OCRDMA_QP_LKEY0 |\n\t\t\t\t\tOCRDMA_QP_FAST_REG);\n\n\tmutex_lock(&dev->dev_lock);\n\tstatus = ocrdma_mbx_create_qp(qp, attrs, ureq.enable_dpp_cq,\n\t\t\t\t\tureq.dpp_cq_id,\n\t\t\t\t\t&dpp_offset, &dpp_credit_lmt);\n\tif (status)\n\t\tgoto mbx_err;\n\n\t \n\tif (udata == NULL) {\n\t\tstatus = ocrdma_alloc_wr_id_tbl(qp);\n\t\tif (status)\n\t\t\tgoto map_err;\n\t}\n\n\tstatus = ocrdma_add_qpn_map(dev, qp);\n\tif (status)\n\t\tgoto map_err;\n\tocrdma_set_qp_db(dev, qp, pd);\n\tif (udata) {\n\t\tstatus = ocrdma_copy_qp_uresp(qp, udata, dpp_offset,\n\t\t\t\t\t      dpp_credit_lmt,\n\t\t\t\t\t      (attrs->srq != NULL));\n\t\tif (status)\n\t\t\tgoto cpy_err;\n\t}\n\tocrdma_store_gsi_qp_cq(dev, attrs);\n\tqp->ibqp.qp_num = qp->id;\n\tmutex_unlock(&dev->dev_lock);\n\treturn 0;\n\ncpy_err:\n\tocrdma_del_qpn_map(dev, qp);\nmap_err:\n\tocrdma_mbx_destroy_qp(dev, qp);\nmbx_err:\n\tmutex_unlock(&dev->dev_lock);\n\tkfree(qp->wqe_wr_id_tbl);\n\tkfree(qp->rqe_wr_id_tbl);\n\tpr_err(\"%s(%d) error=%d\\n\", __func__, dev->id, status);\ngen_err:\n\treturn status;\n}\n\nint _ocrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t      int attr_mask)\n{\n\tint status = 0;\n\tstruct ocrdma_qp *qp;\n\tstruct ocrdma_dev *dev;\n\tenum ib_qp_state old_qps;\n\n\tqp = get_ocrdma_qp(ibqp);\n\tdev = get_ocrdma_dev(ibqp->device);\n\tif (attr_mask & IB_QP_STATE)\n\t\tstatus = ocrdma_qp_state_change(qp, attr->qp_state, &old_qps);\n\t \n\tif (status < 0)\n\t\treturn status;\n\treturn ocrdma_mbx_modify_qp(dev, qp, attr, attr_mask);\n}\n\nint ocrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t     int attr_mask, struct ib_udata *udata)\n{\n\tunsigned long flags;\n\tint status = -EINVAL;\n\tstruct ocrdma_qp *qp;\n\tstruct ocrdma_dev *dev;\n\tenum ib_qp_state old_qps, new_qps;\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\tqp = get_ocrdma_qp(ibqp);\n\tdev = get_ocrdma_dev(ibqp->device);\n\n\t \n\tmutex_lock(&dev->dev_lock);\n\t \n\tspin_lock_irqsave(&qp->q_lock, flags);\n\told_qps = get_ibqp_state(qp->state);\n\tif (attr_mask & IB_QP_STATE)\n\t\tnew_qps = attr->qp_state;\n\telse\n\t\tnew_qps = old_qps;\n\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\n\tif (!ib_modify_qp_is_ok(old_qps, new_qps, ibqp->qp_type, attr_mask)) {\n\t\tpr_err(\"%s(%d) invalid attribute mask=0x%x specified for\\n\"\n\t\t       \"qpn=0x%x of type=0x%x old_qps=0x%x, new_qps=0x%x\\n\",\n\t\t       __func__, dev->id, attr_mask, qp->id, ibqp->qp_type,\n\t\t       old_qps, new_qps);\n\t\tgoto param_err;\n\t}\n\n\tstatus = _ocrdma_modify_qp(ibqp, attr, attr_mask);\n\tif (status > 0)\n\t\tstatus = 0;\nparam_err:\n\tmutex_unlock(&dev->dev_lock);\n\treturn status;\n}\n\nstatic enum ib_mtu ocrdma_mtu_int_to_enum(u16 mtu)\n{\n\tswitch (mtu) {\n\tcase 256:\n\t\treturn IB_MTU_256;\n\tcase 512:\n\t\treturn IB_MTU_512;\n\tcase 1024:\n\t\treturn IB_MTU_1024;\n\tcase 2048:\n\t\treturn IB_MTU_2048;\n\tcase 4096:\n\t\treturn IB_MTU_4096;\n\tdefault:\n\t\treturn IB_MTU_1024;\n\t}\n}\n\nstatic int ocrdma_to_ib_qp_acc_flags(int qp_cap_flags)\n{\n\tint ib_qp_acc_flags = 0;\n\n\tif (qp_cap_flags & OCRDMA_QP_INB_WR)\n\t\tib_qp_acc_flags |= IB_ACCESS_REMOTE_WRITE;\n\tif (qp_cap_flags & OCRDMA_QP_INB_RD)\n\t\tib_qp_acc_flags |= IB_ACCESS_LOCAL_WRITE;\n\treturn ib_qp_acc_flags;\n}\n\nint ocrdma_query_qp(struct ib_qp *ibqp,\n\t\t    struct ib_qp_attr *qp_attr,\n\t\t    int attr_mask, struct ib_qp_init_attr *qp_init_attr)\n{\n\tint status;\n\tu32 qp_state;\n\tstruct ocrdma_qp_params params;\n\tstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibqp->device);\n\n\tmemset(&params, 0, sizeof(params));\n\tmutex_lock(&dev->dev_lock);\n\tstatus = ocrdma_mbx_query_qp(dev, qp, &params);\n\tmutex_unlock(&dev->dev_lock);\n\tif (status)\n\t\tgoto mbx_err;\n\tif (qp->qp_type == IB_QPT_UD)\n\t\tqp_attr->qkey = params.qkey;\n\tqp_attr->path_mtu =\n\t\tocrdma_mtu_int_to_enum(params.path_mtu_pkey_indx &\n\t\t\t\tOCRDMA_QP_PARAMS_PATH_MTU_MASK) >>\n\t\t\t\tOCRDMA_QP_PARAMS_PATH_MTU_SHIFT;\n\tqp_attr->path_mig_state = IB_MIG_MIGRATED;\n\tqp_attr->rq_psn = params.hop_lmt_rq_psn & OCRDMA_QP_PARAMS_RQ_PSN_MASK;\n\tqp_attr->sq_psn = params.tclass_sq_psn & OCRDMA_QP_PARAMS_SQ_PSN_MASK;\n\tqp_attr->dest_qp_num =\n\t    params.ack_to_rnr_rtc_dest_qpn & OCRDMA_QP_PARAMS_DEST_QPN_MASK;\n\n\tqp_attr->qp_access_flags = ocrdma_to_ib_qp_acc_flags(qp->cap_flags);\n\tqp_attr->cap.max_send_wr = qp->sq.max_cnt - 1;\n\tqp_attr->cap.max_recv_wr = qp->rq.max_cnt - 1;\n\tqp_attr->cap.max_send_sge = qp->sq.max_sges;\n\tqp_attr->cap.max_recv_sge = qp->rq.max_sges;\n\tqp_attr->cap.max_inline_data = qp->max_inline_data;\n\tqp_init_attr->cap = qp_attr->cap;\n\tqp_attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\n\trdma_ah_set_grh(&qp_attr->ah_attr, NULL,\n\t\t\tparams.rnt_rc_sl_fl &\n\t\t\t  OCRDMA_QP_PARAMS_FLOW_LABEL_MASK,\n\t\t\tqp->sgid_idx,\n\t\t\t(params.hop_lmt_rq_psn &\n\t\t\t OCRDMA_QP_PARAMS_HOP_LMT_MASK) >>\n\t\t\t OCRDMA_QP_PARAMS_HOP_LMT_SHIFT,\n\t\t\t(params.tclass_sq_psn &\n\t\t\t OCRDMA_QP_PARAMS_TCLASS_MASK) >>\n\t\t\t OCRDMA_QP_PARAMS_TCLASS_SHIFT);\n\trdma_ah_set_dgid_raw(&qp_attr->ah_attr, &params.dgid[0]);\n\n\trdma_ah_set_port_num(&qp_attr->ah_attr, 1);\n\trdma_ah_set_sl(&qp_attr->ah_attr, (params.rnt_rc_sl_fl &\n\t\t\t\t\t   OCRDMA_QP_PARAMS_SL_MASK) >>\n\t\t\t\t\t   OCRDMA_QP_PARAMS_SL_SHIFT);\n\tqp_attr->timeout = (params.ack_to_rnr_rtc_dest_qpn &\n\t\t\t    OCRDMA_QP_PARAMS_ACK_TIMEOUT_MASK) >>\n\t\t\t\tOCRDMA_QP_PARAMS_ACK_TIMEOUT_SHIFT;\n\tqp_attr->rnr_retry = (params.ack_to_rnr_rtc_dest_qpn &\n\t\t\t      OCRDMA_QP_PARAMS_RNR_RETRY_CNT_MASK) >>\n\t\t\t\tOCRDMA_QP_PARAMS_RNR_RETRY_CNT_SHIFT;\n\tqp_attr->retry_cnt =\n\t    (params.rnt_rc_sl_fl & OCRDMA_QP_PARAMS_RETRY_CNT_MASK) >>\n\t\tOCRDMA_QP_PARAMS_RETRY_CNT_SHIFT;\n\tqp_attr->min_rnr_timer = 0;\n\tqp_attr->pkey_index = 0;\n\tqp_attr->port_num = 1;\n\trdma_ah_set_path_bits(&qp_attr->ah_attr, 0);\n\trdma_ah_set_static_rate(&qp_attr->ah_attr, 0);\n\tqp_attr->alt_pkey_index = 0;\n\tqp_attr->alt_port_num = 0;\n\tqp_attr->alt_timeout = 0;\n\tmemset(&qp_attr->alt_ah_attr, 0, sizeof(qp_attr->alt_ah_attr));\n\tqp_state = (params.max_sge_recv_flags & OCRDMA_QP_PARAMS_STATE_MASK) >>\n\t\t    OCRDMA_QP_PARAMS_STATE_SHIFT;\n\tqp_attr->qp_state = get_ibqp_state(qp_state);\n\tqp_attr->cur_qp_state = qp_attr->qp_state;\n\tqp_attr->sq_draining = (qp_state == OCRDMA_QPS_SQ_DRAINING) ? 1 : 0;\n\tqp_attr->max_dest_rd_atomic =\n\t    params.max_ord_ird >> OCRDMA_QP_PARAMS_MAX_ORD_SHIFT;\n\tqp_attr->max_rd_atomic =\n\t    params.max_ord_ird & OCRDMA_QP_PARAMS_MAX_IRD_MASK;\n\tqp_attr->en_sqd_async_notify = (params.max_sge_recv_flags &\n\t\t\t\tOCRDMA_QP_PARAMS_FLAGS_SQD_ASYNC) ? 1 : 0;\n\t \n\tocrdma_qp_state_change(qp, qp_attr->qp_state, NULL);\nmbx_err:\n\treturn status;\n}\n\nstatic void ocrdma_srq_toggle_bit(struct ocrdma_srq *srq, unsigned int idx)\n{\n\tunsigned int i = idx / 32;\n\tu32 mask = (1U << (idx % 32));\n\n\tsrq->idx_bit_fields[i] ^= mask;\n}\n\nstatic int ocrdma_hwq_free_cnt(struct ocrdma_qp_hwq_info *q)\n{\n\treturn ((q->max_wqe_idx - q->head) + q->tail) % q->max_cnt;\n}\n\nstatic int is_hw_sq_empty(struct ocrdma_qp *qp)\n{\n\treturn (qp->sq.tail == qp->sq.head);\n}\n\nstatic int is_hw_rq_empty(struct ocrdma_qp *qp)\n{\n\treturn (qp->rq.tail == qp->rq.head);\n}\n\nstatic void *ocrdma_hwq_head(struct ocrdma_qp_hwq_info *q)\n{\n\treturn q->va + (q->head * q->entry_size);\n}\n\nstatic void *ocrdma_hwq_head_from_idx(struct ocrdma_qp_hwq_info *q,\n\t\t\t\t      u32 idx)\n{\n\treturn q->va + (idx * q->entry_size);\n}\n\nstatic void ocrdma_hwq_inc_head(struct ocrdma_qp_hwq_info *q)\n{\n\tq->head = (q->head + 1) & q->max_wqe_idx;\n}\n\nstatic void ocrdma_hwq_inc_tail(struct ocrdma_qp_hwq_info *q)\n{\n\tq->tail = (q->tail + 1) & q->max_wqe_idx;\n}\n\n \nstatic void ocrdma_discard_cqes(struct ocrdma_qp *qp, struct ocrdma_cq *cq)\n{\n\tunsigned long cq_flags;\n\tunsigned long flags;\n\tu32 cur_getp, stop_getp;\n\tstruct ocrdma_cqe *cqe;\n\tu32 qpn = 0, wqe_idx = 0;\n\n\tspin_lock_irqsave(&cq->cq_lock, cq_flags);\n\n\t \n\n\tcur_getp = cq->getp;\n\t \n\tstop_getp = cur_getp;\n\tdo {\n\t\tif (is_hw_sq_empty(qp) && (!qp->srq && is_hw_rq_empty(qp)))\n\t\t\tbreak;\n\n\t\tcqe = cq->va + cur_getp;\n\t\t \n\t\tqpn = cqe->cmn.qpn & OCRDMA_CQE_QPN_MASK;\n\t\t \n\t\t \n\t\tif (qpn == 0 || qpn != qp->id)\n\t\t\tgoto skip_cqe;\n\n\t\tif (is_cqe_for_sq(cqe)) {\n\t\t\tocrdma_hwq_inc_tail(&qp->sq);\n\t\t} else {\n\t\t\tif (qp->srq) {\n\t\t\t\twqe_idx = (le32_to_cpu(cqe->rq.buftag_qpn) >>\n\t\t\t\t\tOCRDMA_CQE_BUFTAG_SHIFT) &\n\t\t\t\t\tqp->srq->rq.max_wqe_idx;\n\t\t\t\tBUG_ON(wqe_idx < 1);\n\t\t\t\tspin_lock_irqsave(&qp->srq->q_lock, flags);\n\t\t\t\tocrdma_hwq_inc_tail(&qp->srq->rq);\n\t\t\t\tocrdma_srq_toggle_bit(qp->srq, wqe_idx - 1);\n\t\t\t\tspin_unlock_irqrestore(&qp->srq->q_lock, flags);\n\n\t\t\t} else {\n\t\t\t\tocrdma_hwq_inc_tail(&qp->rq);\n\t\t\t}\n\t\t}\n\t\t \n\t\tcqe->cmn.qpn = 0;\nskip_cqe:\n\t\tcur_getp = (cur_getp + 1) % cq->max_hw_cqe;\n\t} while (cur_getp != stop_getp);\n\tspin_unlock_irqrestore(&cq->cq_lock, cq_flags);\n}\n\nvoid ocrdma_del_flush_qp(struct ocrdma_qp *qp)\n{\n\tint found = false;\n\tunsigned long flags;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(qp->ibqp.device);\n\t \n\n\tspin_lock_irqsave(&dev->flush_q_lock, flags);\n\tfound = ocrdma_is_qp_in_sq_flushlist(qp->sq_cq, qp);\n\tif (found)\n\t\tlist_del(&qp->sq_entry);\n\tif (!qp->srq) {\n\t\tfound = ocrdma_is_qp_in_rq_flushlist(qp->rq_cq, qp);\n\t\tif (found)\n\t\t\tlist_del(&qp->rq_entry);\n\t}\n\tspin_unlock_irqrestore(&dev->flush_q_lock, flags);\n}\n\nint ocrdma_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct ocrdma_pd *pd;\n\tstruct ocrdma_qp *qp;\n\tstruct ocrdma_dev *dev;\n\tstruct ib_qp_attr attrs;\n\tint attr_mask;\n\tunsigned long flags;\n\n\tqp = get_ocrdma_qp(ibqp);\n\tdev = get_ocrdma_dev(ibqp->device);\n\n\tpd = qp->pd;\n\n\t \n\tif (qp->state != OCRDMA_QPS_RST) {\n\t\tattrs.qp_state = IB_QPS_ERR;\n\t\tattr_mask = IB_QP_STATE;\n\t\t_ocrdma_modify_qp(ibqp, &attrs, attr_mask);\n\t}\n\t \n\tmutex_lock(&dev->dev_lock);\n\t(void) ocrdma_mbx_destroy_qp(dev, qp);\n\n\t \n\tspin_lock_irqsave(&qp->sq_cq->cq_lock, flags);\n\tif (qp->rq_cq && (qp->rq_cq != qp->sq_cq)) {\n\t\tspin_lock(&qp->rq_cq->cq_lock);\n\t\tocrdma_del_qpn_map(dev, qp);\n\t\tspin_unlock(&qp->rq_cq->cq_lock);\n\t} else {\n\t\tocrdma_del_qpn_map(dev, qp);\n\t}\n\tspin_unlock_irqrestore(&qp->sq_cq->cq_lock, flags);\n\n\tif (!pd->uctx) {\n\t\tocrdma_discard_cqes(qp, qp->sq_cq);\n\t\tocrdma_discard_cqes(qp, qp->rq_cq);\n\t}\n\tmutex_unlock(&dev->dev_lock);\n\n\tif (pd->uctx) {\n\t\tocrdma_del_mmap(pd->uctx, (u64) qp->sq.pa,\n\t\t\t\tPAGE_ALIGN(qp->sq.len));\n\t\tif (!qp->srq)\n\t\t\tocrdma_del_mmap(pd->uctx, (u64) qp->rq.pa,\n\t\t\t\t\tPAGE_ALIGN(qp->rq.len));\n\t}\n\n\tocrdma_del_flush_qp(qp);\n\n\tkfree(qp->wqe_wr_id_tbl);\n\tkfree(qp->rqe_wr_id_tbl);\n\treturn 0;\n}\n\nstatic int ocrdma_copy_srq_uresp(struct ocrdma_dev *dev, struct ocrdma_srq *srq,\n\t\t\t\tstruct ib_udata *udata)\n{\n\tint status;\n\tstruct ocrdma_create_srq_uresp uresp;\n\n\tmemset(&uresp, 0, sizeof(uresp));\n\turesp.rq_dbid = srq->rq.dbid;\n\turesp.num_rq_pages = 1;\n\turesp.rq_page_addr[0] = virt_to_phys(srq->rq.va);\n\turesp.rq_page_size = srq->rq.len;\n\turesp.db_page_addr = dev->nic_info.unmapped_db +\n\t    (srq->pd->id * dev->nic_info.db_page_size);\n\turesp.db_page_size = dev->nic_info.db_page_size;\n\turesp.num_rqe_allocated = srq->rq.max_cnt;\n\tif (ocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R) {\n\t\turesp.db_rq_offset = OCRDMA_DB_GEN2_RQ_OFFSET;\n\t\turesp.db_shift = 24;\n\t} else {\n\t\turesp.db_rq_offset = OCRDMA_DB_RQ_OFFSET;\n\t\turesp.db_shift = 16;\n\t}\n\n\tstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (status)\n\t\treturn status;\n\tstatus = ocrdma_add_mmap(srq->pd->uctx, uresp.rq_page_addr[0],\n\t\t\t\t uresp.rq_page_size);\n\tif (status)\n\t\treturn status;\n\treturn status;\n}\n\nint ocrdma_create_srq(struct ib_srq *ibsrq, struct ib_srq_init_attr *init_attr,\n\t\t      struct ib_udata *udata)\n{\n\tint status;\n\tstruct ocrdma_pd *pd = get_ocrdma_pd(ibsrq->pd);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibsrq->device);\n\tstruct ocrdma_srq *srq = get_ocrdma_srq(ibsrq);\n\n\tif (init_attr->srq_type != IB_SRQT_BASIC)\n\t\treturn -EOPNOTSUPP;\n\n\tif (init_attr->attr.max_sge > dev->attr.max_recv_sge)\n\t\treturn -EINVAL;\n\tif (init_attr->attr.max_wr > dev->attr.max_rqe)\n\t\treturn -EINVAL;\n\n\tspin_lock_init(&srq->q_lock);\n\tsrq->pd = pd;\n\tsrq->db = dev->nic_info.db + (pd->id * dev->nic_info.db_page_size);\n\tstatus = ocrdma_mbx_create_srq(dev, srq, init_attr, pd);\n\tif (status)\n\t\treturn status;\n\n\tif (!udata) {\n\t\tsrq->rqe_wr_id_tbl = kcalloc(srq->rq.max_cnt, sizeof(u64),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!srq->rqe_wr_id_tbl) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto arm_err;\n\t\t}\n\n\t\tsrq->bit_fields_len = (srq->rq.max_cnt / 32) +\n\t\t    (srq->rq.max_cnt % 32 ? 1 : 0);\n\t\tsrq->idx_bit_fields =\n\t\t    kmalloc_array(srq->bit_fields_len, sizeof(u32),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!srq->idx_bit_fields) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto arm_err;\n\t\t}\n\t\tmemset(srq->idx_bit_fields, 0xff,\n\t\t       srq->bit_fields_len * sizeof(u32));\n\t}\n\n\tif (init_attr->attr.srq_limit) {\n\t\tstatus = ocrdma_mbx_modify_srq(srq, &init_attr->attr);\n\t\tif (status)\n\t\t\tgoto arm_err;\n\t}\n\n\tif (udata) {\n\t\tstatus = ocrdma_copy_srq_uresp(dev, srq, udata);\n\t\tif (status)\n\t\t\tgoto arm_err;\n\t}\n\n\treturn 0;\n\narm_err:\n\tocrdma_mbx_destroy_srq(dev, srq);\n\tkfree(srq->rqe_wr_id_tbl);\n\tkfree(srq->idx_bit_fields);\n\treturn status;\n}\n\nint ocrdma_modify_srq(struct ib_srq *ibsrq,\n\t\t      struct ib_srq_attr *srq_attr,\n\t\t      enum ib_srq_attr_mask srq_attr_mask,\n\t\t      struct ib_udata *udata)\n{\n\tint status;\n\tstruct ocrdma_srq *srq;\n\n\tsrq = get_ocrdma_srq(ibsrq);\n\tif (srq_attr_mask & IB_SRQ_MAX_WR)\n\t\tstatus = -EINVAL;\n\telse\n\t\tstatus = ocrdma_mbx_modify_srq(srq, srq_attr);\n\treturn status;\n}\n\nint ocrdma_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\n{\n\tstruct ocrdma_srq *srq;\n\n\tsrq = get_ocrdma_srq(ibsrq);\n\treturn ocrdma_mbx_query_srq(srq, srq_attr);\n}\n\nint ocrdma_destroy_srq(struct ib_srq *ibsrq, struct ib_udata *udata)\n{\n\tstruct ocrdma_srq *srq;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibsrq->device);\n\n\tsrq = get_ocrdma_srq(ibsrq);\n\n\tocrdma_mbx_destroy_srq(dev, srq);\n\n\tif (srq->pd->uctx)\n\t\tocrdma_del_mmap(srq->pd->uctx, (u64) srq->rq.pa,\n\t\t\t\tPAGE_ALIGN(srq->rq.len));\n\n\tkfree(srq->idx_bit_fields);\n\tkfree(srq->rqe_wr_id_tbl);\n\treturn 0;\n}\n\n \nstatic void ocrdma_build_ud_hdr(struct ocrdma_qp *qp,\n\t\t\t\tstruct ocrdma_hdr_wqe *hdr,\n\t\t\t\tconst struct ib_send_wr *wr)\n{\n\tstruct ocrdma_ewqe_ud_hdr *ud_hdr =\n\t\t(struct ocrdma_ewqe_ud_hdr *)(hdr + 1);\n\tstruct ocrdma_ah *ah = get_ocrdma_ah(ud_wr(wr)->ah);\n\n\tud_hdr->rsvd_dest_qpn = ud_wr(wr)->remote_qpn;\n\tif (qp->qp_type == IB_QPT_GSI)\n\t\tud_hdr->qkey = qp->qkey;\n\telse\n\t\tud_hdr->qkey = ud_wr(wr)->remote_qkey;\n\tud_hdr->rsvd_ahid = ah->id;\n\tud_hdr->hdr_type = ah->hdr_type;\n\tif (ah->av->valid & OCRDMA_AV_VLAN_VALID)\n\t\thdr->cw |= (OCRDMA_FLAG_AH_VLAN_PR << OCRDMA_WQE_FLAGS_SHIFT);\n}\n\nstatic void ocrdma_build_sges(struct ocrdma_hdr_wqe *hdr,\n\t\t\t      struct ocrdma_sge *sge, int num_sge,\n\t\t\t      struct ib_sge *sg_list)\n{\n\tint i;\n\n\tfor (i = 0; i < num_sge; i++) {\n\t\tsge[i].lrkey = sg_list[i].lkey;\n\t\tsge[i].addr_lo = sg_list[i].addr;\n\t\tsge[i].addr_hi = upper_32_bits(sg_list[i].addr);\n\t\tsge[i].len = sg_list[i].length;\n\t\thdr->total_len += sg_list[i].length;\n\t}\n\tif (num_sge == 0)\n\t\tmemset(sge, 0, sizeof(*sge));\n}\n\nstatic inline uint32_t ocrdma_sglist_len(struct ib_sge *sg_list, int num_sge)\n{\n\tuint32_t total_len = 0, i;\n\n\tfor (i = 0; i < num_sge; i++)\n\t\ttotal_len += sg_list[i].length;\n\treturn total_len;\n}\n\n\nstatic int ocrdma_build_inline_sges(struct ocrdma_qp *qp,\n\t\t\t\t    struct ocrdma_hdr_wqe *hdr,\n\t\t\t\t    struct ocrdma_sge *sge,\n\t\t\t\t    const struct ib_send_wr *wr, u32 wqe_size)\n{\n\tint i;\n\tchar *dpp_addr;\n\n\tif (wr->send_flags & IB_SEND_INLINE && qp->qp_type != IB_QPT_UD) {\n\t\thdr->total_len = ocrdma_sglist_len(wr->sg_list, wr->num_sge);\n\t\tif (unlikely(hdr->total_len > qp->max_inline_data)) {\n\t\t\tpr_err(\"%s() supported_len=0x%x,\\n\"\n\t\t\t       \" unsupported len req=0x%x\\n\", __func__,\n\t\t\t\tqp->max_inline_data, hdr->total_len);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdpp_addr = (char *)sge;\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tmemcpy(dpp_addr,\n\t\t\t       (void *)(unsigned long)wr->sg_list[i].addr,\n\t\t\t       wr->sg_list[i].length);\n\t\t\tdpp_addr += wr->sg_list[i].length;\n\t\t}\n\n\t\twqe_size += roundup(hdr->total_len, OCRDMA_WQE_ALIGN_BYTES);\n\t\tif (0 == hdr->total_len)\n\t\t\twqe_size += sizeof(struct ocrdma_sge);\n\t\thdr->cw |= (OCRDMA_TYPE_INLINE << OCRDMA_WQE_TYPE_SHIFT);\n\t} else {\n\t\tocrdma_build_sges(hdr, sge, wr->num_sge, wr->sg_list);\n\t\tif (wr->num_sge)\n\t\t\twqe_size += (wr->num_sge * sizeof(struct ocrdma_sge));\n\t\telse\n\t\t\twqe_size += sizeof(struct ocrdma_sge);\n\t\thdr->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\n\t}\n\thdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\n\treturn 0;\n}\n\nstatic int ocrdma_build_send(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\n\t\t\t     const struct ib_send_wr *wr)\n{\n\tstruct ocrdma_sge *sge;\n\tu32 wqe_size = sizeof(*hdr);\n\n\tif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\n\t\tocrdma_build_ud_hdr(qp, hdr, wr);\n\t\tsge = (struct ocrdma_sge *)(hdr + 2);\n\t\twqe_size += sizeof(struct ocrdma_ewqe_ud_hdr);\n\t} else {\n\t\tsge = (struct ocrdma_sge *)(hdr + 1);\n\t}\n\n\treturn ocrdma_build_inline_sges(qp, hdr, sge, wr, wqe_size);\n}\n\nstatic int ocrdma_build_write(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\n\t\t\t      const struct ib_send_wr *wr)\n{\n\tint status;\n\tstruct ocrdma_sge *ext_rw = (struct ocrdma_sge *)(hdr + 1);\n\tstruct ocrdma_sge *sge = ext_rw + 1;\n\tu32 wqe_size = sizeof(*hdr) + sizeof(*ext_rw);\n\n\tstatus = ocrdma_build_inline_sges(qp, hdr, sge, wr, wqe_size);\n\tif (status)\n\t\treturn status;\n\text_rw->addr_lo = rdma_wr(wr)->remote_addr;\n\text_rw->addr_hi = upper_32_bits(rdma_wr(wr)->remote_addr);\n\text_rw->lrkey = rdma_wr(wr)->rkey;\n\text_rw->len = hdr->total_len;\n\treturn 0;\n}\n\nstatic void ocrdma_build_read(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\n\t\t\t      const struct ib_send_wr *wr)\n{\n\tstruct ocrdma_sge *ext_rw = (struct ocrdma_sge *)(hdr + 1);\n\tstruct ocrdma_sge *sge = ext_rw + 1;\n\tu32 wqe_size = ((wr->num_sge + 1) * sizeof(struct ocrdma_sge)) +\n\t    sizeof(struct ocrdma_hdr_wqe);\n\n\tocrdma_build_sges(hdr, sge, wr->num_sge, wr->sg_list);\n\thdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\n\thdr->cw |= (OCRDMA_READ << OCRDMA_WQE_OPCODE_SHIFT);\n\thdr->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\n\n\text_rw->addr_lo = rdma_wr(wr)->remote_addr;\n\text_rw->addr_hi = upper_32_bits(rdma_wr(wr)->remote_addr);\n\text_rw->lrkey = rdma_wr(wr)->rkey;\n\text_rw->len = hdr->total_len;\n}\n\nstatic int get_encoded_page_size(int pg_sz)\n{\n\t \n\tint i = 0;\n\tfor (; i < 17; i++)\n\t\tif (pg_sz == (4096 << i))\n\t\t\tbreak;\n\treturn i;\n}\n\nstatic int ocrdma_build_reg(struct ocrdma_qp *qp,\n\t\t\t    struct ocrdma_hdr_wqe *hdr,\n\t\t\t    const struct ib_reg_wr *wr)\n{\n\tu64 fbo;\n\tstruct ocrdma_ewqe_fr *fast_reg = (struct ocrdma_ewqe_fr *)(hdr + 1);\n\tstruct ocrdma_mr *mr = get_ocrdma_mr(wr->mr);\n\tstruct ocrdma_pbl *pbl_tbl = mr->hwmr.pbl_table;\n\tstruct ocrdma_pbe *pbe;\n\tu32 wqe_size = sizeof(*fast_reg) + sizeof(*hdr);\n\tint num_pbes = 0, i;\n\n\twqe_size = roundup(wqe_size, OCRDMA_WQE_ALIGN_BYTES);\n\n\thdr->cw |= (OCRDMA_FR_MR << OCRDMA_WQE_OPCODE_SHIFT);\n\thdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\n\n\tif (wr->access & IB_ACCESS_LOCAL_WRITE)\n\t\thdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_LOCAL_WR;\n\tif (wr->access & IB_ACCESS_REMOTE_WRITE)\n\t\thdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_REMOTE_WR;\n\tif (wr->access & IB_ACCESS_REMOTE_READ)\n\t\thdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_REMOTE_RD;\n\thdr->lkey = wr->key;\n\thdr->total_len = mr->ibmr.length;\n\n\tfbo = mr->ibmr.iova - mr->pages[0];\n\n\tfast_reg->va_hi = upper_32_bits(mr->ibmr.iova);\n\tfast_reg->va_lo = (u32) (mr->ibmr.iova & 0xffffffff);\n\tfast_reg->fbo_hi = upper_32_bits(fbo);\n\tfast_reg->fbo_lo = (u32) fbo & 0xffffffff;\n\tfast_reg->num_sges = mr->npages;\n\tfast_reg->size_sge = get_encoded_page_size(mr->ibmr.page_size);\n\n\tpbe = pbl_tbl->va;\n\tfor (i = 0; i < mr->npages; i++) {\n\t\tu64 buf_addr = mr->pages[i];\n\n\t\tpbe->pa_lo = cpu_to_le32((u32) (buf_addr & PAGE_MASK));\n\t\tpbe->pa_hi = cpu_to_le32((u32) upper_32_bits(buf_addr));\n\t\tnum_pbes += 1;\n\t\tpbe++;\n\n\t\t \n\t\tif (num_pbes == (mr->hwmr.pbl_size/sizeof(u64))) {\n\t\t\tpbl_tbl++;\n\t\t\tpbe = (struct ocrdma_pbe *)pbl_tbl->va;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void ocrdma_ring_sq_db(struct ocrdma_qp *qp)\n{\n\tu32 val = qp->sq.dbid | (1 << OCRDMA_DB_SQ_SHIFT);\n\n\tiowrite32(val, qp->sq_db);\n}\n\nint ocrdma_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t     const struct ib_send_wr **bad_wr)\n{\n\tint status = 0;\n\tstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\n\tstruct ocrdma_hdr_wqe *hdr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->q_lock, flags);\n\tif (qp->state != OCRDMA_QPS_RTS && qp->state != OCRDMA_QPS_SQD) {\n\t\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\twhile (wr) {\n\t\tif (qp->qp_type == IB_QPT_UD &&\n\t\t    (wr->opcode != IB_WR_SEND &&\n\t\t     wr->opcode != IB_WR_SEND_WITH_IMM)) {\n\t\t\t*bad_wr = wr;\n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (ocrdma_hwq_free_cnt(&qp->sq) == 0 ||\n\t\t    wr->num_sge > qp->sq.max_sges) {\n\t\t\t*bad_wr = wr;\n\t\t\tstatus = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\thdr = ocrdma_hwq_head(&qp->sq);\n\t\thdr->cw = 0;\n\t\tif (wr->send_flags & IB_SEND_SIGNALED || qp->signaled)\n\t\t\thdr->cw |= (OCRDMA_FLAG_SIG << OCRDMA_WQE_FLAGS_SHIFT);\n\t\tif (wr->send_flags & IB_SEND_FENCE)\n\t\t\thdr->cw |=\n\t\t\t    (OCRDMA_FLAG_FENCE_L << OCRDMA_WQE_FLAGS_SHIFT);\n\t\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\t\thdr->cw |=\n\t\t\t    (OCRDMA_FLAG_SOLICIT << OCRDMA_WQE_FLAGS_SHIFT);\n\t\thdr->total_len = 0;\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\thdr->cw |= (OCRDMA_FLAG_IMM << OCRDMA_WQE_FLAGS_SHIFT);\n\t\t\thdr->immdt = ntohl(wr->ex.imm_data);\n\t\t\tfallthrough;\n\t\tcase IB_WR_SEND:\n\t\t\thdr->cw |= (OCRDMA_SEND << OCRDMA_WQE_OPCODE_SHIFT);\n\t\t\tocrdma_build_send(qp, hdr, wr);\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\thdr->cw |= (OCRDMA_FLAG_INV << OCRDMA_WQE_FLAGS_SHIFT);\n\t\t\thdr->cw |= (OCRDMA_SEND << OCRDMA_WQE_OPCODE_SHIFT);\n\t\t\thdr->lkey = wr->ex.invalidate_rkey;\n\t\t\tstatus = ocrdma_build_send(qp, hdr, wr);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\t\thdr->cw |= (OCRDMA_FLAG_IMM << OCRDMA_WQE_FLAGS_SHIFT);\n\t\t\thdr->immdt = ntohl(wr->ex.imm_data);\n\t\t\tfallthrough;\n\t\tcase IB_WR_RDMA_WRITE:\n\t\t\thdr->cw |= (OCRDMA_WRITE << OCRDMA_WQE_OPCODE_SHIFT);\n\t\t\tstatus = ocrdma_build_write(qp, hdr, wr);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ:\n\t\t\tocrdma_build_read(qp, hdr, wr);\n\t\t\tbreak;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\thdr->cw |=\n\t\t\t    (OCRDMA_LKEY_INV << OCRDMA_WQE_OPCODE_SHIFT);\n\t\t\thdr->cw |= ((sizeof(struct ocrdma_hdr_wqe) +\n\t\t\t\t\tsizeof(struct ocrdma_sge)) /\n\t\t\t\tOCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT;\n\t\t\thdr->lkey = wr->ex.invalidate_rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR:\n\t\t\tstatus = ocrdma_build_reg(qp, hdr, reg_wr(wr));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (status) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\tif (wr->send_flags & IB_SEND_SIGNALED || qp->signaled)\n\t\t\tqp->wqe_wr_id_tbl[qp->sq.head].signaled = 1;\n\t\telse\n\t\t\tqp->wqe_wr_id_tbl[qp->sq.head].signaled = 0;\n\t\tqp->wqe_wr_id_tbl[qp->sq.head].wrid = wr->wr_id;\n\t\tocrdma_cpu_to_le32(hdr, ((hdr->cw >> OCRDMA_WQE_SIZE_SHIFT) &\n\t\t\t\t   OCRDMA_WQE_SIZE_MASK) * OCRDMA_WQE_STRIDE);\n\t\t \n\t\twmb();\n\t\t \n\t\tocrdma_ring_sq_db(qp);\n\n\t\t \n\t\tocrdma_hwq_inc_head(&qp->sq);\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\treturn status;\n}\n\nstatic void ocrdma_ring_rq_db(struct ocrdma_qp *qp)\n{\n\tu32 val = qp->rq.dbid | (1 << OCRDMA_DB_RQ_SHIFT);\n\n\tiowrite32(val, qp->rq_db);\n}\n\nstatic void ocrdma_build_rqe(struct ocrdma_hdr_wqe *rqe,\n\t\t\t     const struct ib_recv_wr *wr, u16 tag)\n{\n\tu32 wqe_size = 0;\n\tstruct ocrdma_sge *sge;\n\tif (wr->num_sge)\n\t\twqe_size = (wr->num_sge * sizeof(*sge)) + sizeof(*rqe);\n\telse\n\t\twqe_size = sizeof(*sge) + sizeof(*rqe);\n\n\trqe->cw = ((wqe_size / OCRDMA_WQE_STRIDE) <<\n\t\t\t\tOCRDMA_WQE_SIZE_SHIFT);\n\trqe->cw |= (OCRDMA_FLAG_SIG << OCRDMA_WQE_FLAGS_SHIFT);\n\trqe->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\n\trqe->total_len = 0;\n\trqe->rsvd_tag = tag;\n\tsge = (struct ocrdma_sge *)(rqe + 1);\n\tocrdma_build_sges(rqe, sge, wr->num_sge, wr->sg_list);\n\tocrdma_cpu_to_le32(rqe, wqe_size);\n}\n\nint ocrdma_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t     const struct ib_recv_wr **bad_wr)\n{\n\tint status = 0;\n\tunsigned long flags;\n\tstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\n\tstruct ocrdma_hdr_wqe *rqe;\n\n\tspin_lock_irqsave(&qp->q_lock, flags);\n\tif (qp->state == OCRDMA_QPS_RST || qp->state == OCRDMA_QPS_ERR) {\n\t\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\twhile (wr) {\n\t\tif (ocrdma_hwq_free_cnt(&qp->rq) == 0 ||\n\t\t    wr->num_sge > qp->rq.max_sges) {\n\t\t\t*bad_wr = wr;\n\t\t\tstatus = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\trqe = ocrdma_hwq_head(&qp->rq);\n\t\tocrdma_build_rqe(rqe, wr, 0);\n\n\t\tqp->rqe_wr_id_tbl[qp->rq.head] = wr->wr_id;\n\t\t \n\t\twmb();\n\n\t\t \n\t\tocrdma_ring_rq_db(qp);\n\n\t\t \n\t\tocrdma_hwq_inc_head(&qp->rq);\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\treturn status;\n}\n\n \nstatic int ocrdma_srq_get_idx(struct ocrdma_srq *srq)\n{\n\tint row = 0;\n\tint indx = 0;\n\n\tfor (row = 0; row < srq->bit_fields_len; row++) {\n\t\tif (srq->idx_bit_fields[row]) {\n\t\t\tindx = ffs(srq->idx_bit_fields[row]);\n\t\t\tindx = (row * 32) + (indx - 1);\n\t\t\tBUG_ON(indx >= srq->rq.max_cnt);\n\t\t\tocrdma_srq_toggle_bit(srq, indx);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tBUG_ON(row == srq->bit_fields_len);\n\treturn indx + 1;  \n}\n\nstatic void ocrdma_ring_srq_db(struct ocrdma_srq *srq)\n{\n\tu32 val = srq->rq.dbid | (1 << 16);\n\n\tiowrite32(val, srq->db + OCRDMA_DB_GEN2_SRQ_OFFSET);\n}\n\nint ocrdma_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t\t const struct ib_recv_wr **bad_wr)\n{\n\tint status = 0;\n\tunsigned long flags;\n\tstruct ocrdma_srq *srq;\n\tstruct ocrdma_hdr_wqe *rqe;\n\tu16 tag;\n\n\tsrq = get_ocrdma_srq(ibsrq);\n\n\tspin_lock_irqsave(&srq->q_lock, flags);\n\twhile (wr) {\n\t\tif (ocrdma_hwq_free_cnt(&srq->rq) == 0 ||\n\t\t    wr->num_sge > srq->rq.max_sges) {\n\t\t\tstatus = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\ttag = ocrdma_srq_get_idx(srq);\n\t\trqe = ocrdma_hwq_head(&srq->rq);\n\t\tocrdma_build_rqe(rqe, wr, tag);\n\n\t\tsrq->rqe_wr_id_tbl[tag] = wr->wr_id;\n\t\t \n\t\twmb();\n\t\t \n\t\tocrdma_ring_srq_db(srq);\n\t\t \n\t\tocrdma_hwq_inc_head(&srq->rq);\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&srq->q_lock, flags);\n\treturn status;\n}\n\nstatic enum ib_wc_status ocrdma_to_ibwc_err(u16 status)\n{\n\tenum ib_wc_status ibwc_status;\n\n\tswitch (status) {\n\tcase OCRDMA_CQE_GENERAL_ERR:\n\t\tibwc_status = IB_WC_GENERAL_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_LEN_ERR:\n\t\tibwc_status = IB_WC_LOC_LEN_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_QP_OP_ERR:\n\t\tibwc_status = IB_WC_LOC_QP_OP_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_EEC_OP_ERR:\n\t\tibwc_status = IB_WC_LOC_EEC_OP_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_PROT_ERR:\n\t\tibwc_status = IB_WC_LOC_PROT_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_WR_FLUSH_ERR:\n\t\tibwc_status = IB_WC_WR_FLUSH_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_MW_BIND_ERR:\n\t\tibwc_status = IB_WC_MW_BIND_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_BAD_RESP_ERR:\n\t\tibwc_status = IB_WC_BAD_RESP_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_ACCESS_ERR:\n\t\tibwc_status = IB_WC_LOC_ACCESS_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_REM_INV_REQ_ERR:\n\t\tibwc_status = IB_WC_REM_INV_REQ_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_REM_ACCESS_ERR:\n\t\tibwc_status = IB_WC_REM_ACCESS_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_REM_OP_ERR:\n\t\tibwc_status = IB_WC_REM_OP_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_RETRY_EXC_ERR:\n\t\tibwc_status = IB_WC_RETRY_EXC_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_RNR_RETRY_EXC_ERR:\n\t\tibwc_status = IB_WC_RNR_RETRY_EXC_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_LOC_RDD_VIOL_ERR:\n\t\tibwc_status = IB_WC_LOC_RDD_VIOL_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_REM_INV_RD_REQ_ERR:\n\t\tibwc_status = IB_WC_REM_INV_RD_REQ_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_REM_ABORT_ERR:\n\t\tibwc_status = IB_WC_REM_ABORT_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_INV_EECN_ERR:\n\t\tibwc_status = IB_WC_INV_EECN_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_INV_EEC_STATE_ERR:\n\t\tibwc_status = IB_WC_INV_EEC_STATE_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_FATAL_ERR:\n\t\tibwc_status = IB_WC_FATAL_ERR;\n\t\tbreak;\n\tcase OCRDMA_CQE_RESP_TIMEOUT_ERR:\n\t\tibwc_status = IB_WC_RESP_TIMEOUT_ERR;\n\t\tbreak;\n\tdefault:\n\t\tibwc_status = IB_WC_GENERAL_ERR;\n\t\tbreak;\n\t}\n\treturn ibwc_status;\n}\n\nstatic void ocrdma_update_wc(struct ocrdma_qp *qp, struct ib_wc *ibwc,\n\t\t      u32 wqe_idx)\n{\n\tstruct ocrdma_hdr_wqe *hdr;\n\tstruct ocrdma_sge *rw;\n\tint opcode;\n\n\thdr = ocrdma_hwq_head_from_idx(&qp->sq, wqe_idx);\n\n\tibwc->wr_id = qp->wqe_wr_id_tbl[wqe_idx].wrid;\n\t \n\topcode = le32_to_cpu(hdr->cw) & OCRDMA_WQE_OPCODE_MASK;\n\tswitch (opcode) {\n\tcase OCRDMA_WRITE:\n\t\tibwc->opcode = IB_WC_RDMA_WRITE;\n\t\tbreak;\n\tcase OCRDMA_READ:\n\t\trw = (struct ocrdma_sge *)(hdr + 1);\n\t\tibwc->opcode = IB_WC_RDMA_READ;\n\t\tibwc->byte_len = rw->len;\n\t\tbreak;\n\tcase OCRDMA_SEND:\n\t\tibwc->opcode = IB_WC_SEND;\n\t\tbreak;\n\tcase OCRDMA_FR_MR:\n\t\tibwc->opcode = IB_WC_REG_MR;\n\t\tbreak;\n\tcase OCRDMA_LKEY_INV:\n\t\tibwc->opcode = IB_WC_LOCAL_INV;\n\t\tbreak;\n\tdefault:\n\t\tibwc->status = IB_WC_GENERAL_ERR;\n\t\tpr_err(\"%s() invalid opcode received = 0x%x\\n\",\n\t\t       __func__, hdr->cw & OCRDMA_WQE_OPCODE_MASK);\n\t\tbreak;\n\t}\n}\n\nstatic void ocrdma_set_cqe_status_flushed(struct ocrdma_qp *qp,\n\t\t\t\t\t\tstruct ocrdma_cqe *cqe)\n{\n\tif (is_cqe_for_sq(cqe)) {\n\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\tcqe->flags_status_srcqpn) &\n\t\t\t\t\t~OCRDMA_CQE_STATUS_MASK);\n\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\tcqe->flags_status_srcqpn) |\n\t\t\t\t(OCRDMA_CQE_WR_FLUSH_ERR <<\n\t\t\t\t\tOCRDMA_CQE_STATUS_SHIFT));\n\t} else {\n\t\tif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\n\t\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\t\tcqe->flags_status_srcqpn) &\n\t\t\t\t\t\t~OCRDMA_CQE_UD_STATUS_MASK);\n\t\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\t\tcqe->flags_status_srcqpn) |\n\t\t\t\t\t(OCRDMA_CQE_WR_FLUSH_ERR <<\n\t\t\t\t\t\tOCRDMA_CQE_UD_STATUS_SHIFT));\n\t\t} else {\n\t\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\t\tcqe->flags_status_srcqpn) &\n\t\t\t\t\t\t~OCRDMA_CQE_STATUS_MASK);\n\t\t\tcqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\n\t\t\t\t\tcqe->flags_status_srcqpn) |\n\t\t\t\t\t(OCRDMA_CQE_WR_FLUSH_ERR <<\n\t\t\t\t\t\tOCRDMA_CQE_STATUS_SHIFT));\n\t\t}\n\t}\n}\n\nstatic bool ocrdma_update_err_cqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\n\t\t\t\t  struct ocrdma_qp *qp, int status)\n{\n\tbool expand = false;\n\n\tibwc->byte_len = 0;\n\tibwc->qp = &qp->ibqp;\n\tibwc->status = ocrdma_to_ibwc_err(status);\n\n\tocrdma_flush_qp(qp);\n\tocrdma_qp_state_change(qp, IB_QPS_ERR, NULL);\n\n\t \n\tif (!is_hw_rq_empty(qp) || !is_hw_sq_empty(qp)) {\n\t\texpand = true;\n\t\tocrdma_set_cqe_status_flushed(qp, cqe);\n\t}\n\treturn expand;\n}\n\nstatic int ocrdma_update_err_rcqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\n\t\t\t\t  struct ocrdma_qp *qp, int status)\n{\n\tibwc->opcode = IB_WC_RECV;\n\tibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\n\tocrdma_hwq_inc_tail(&qp->rq);\n\n\treturn ocrdma_update_err_cqe(ibwc, cqe, qp, status);\n}\n\nstatic int ocrdma_update_err_scqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\n\t\t\t\t  struct ocrdma_qp *qp, int status)\n{\n\tocrdma_update_wc(qp, ibwc, qp->sq.tail);\n\tocrdma_hwq_inc_tail(&qp->sq);\n\n\treturn ocrdma_update_err_cqe(ibwc, cqe, qp, status);\n}\n\n\nstatic bool ocrdma_poll_err_scqe(struct ocrdma_qp *qp,\n\t\t\t\t struct ocrdma_cqe *cqe, struct ib_wc *ibwc,\n\t\t\t\t bool *polled, bool *stop)\n{\n\tbool expand;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(qp->ibqp.device);\n\tint status = (le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\tOCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\n\tif (status < OCRDMA_MAX_CQE_ERR)\n\t\tatomic_inc(&dev->cqe_err_stats[status]);\n\n\t \n\tif (is_hw_sq_empty(qp) && !is_hw_rq_empty(qp)) {\n\t\t \n\t\tif (!qp->srq && (qp->sq_cq == qp->rq_cq)) {\n\t\t\t*polled = true;\n\t\t\tstatus = OCRDMA_CQE_WR_FLUSH_ERR;\n\t\t\texpand = ocrdma_update_err_rcqe(ibwc, cqe, qp, status);\n\t\t} else {\n\t\t\t \n\t\t\t*polled = false;\n\t\t\t*stop = true;\n\t\t\texpand = false;\n\t\t}\n\t} else if (is_hw_sq_empty(qp)) {\n\t\t \n\t\texpand = false;\n\t\t*polled = false;\n\t\t*stop = false;\n\t} else {\n\t\t*polled = true;\n\t\texpand = ocrdma_update_err_scqe(ibwc, cqe, qp, status);\n\t}\n\treturn expand;\n}\n\nstatic bool ocrdma_poll_success_scqe(struct ocrdma_qp *qp,\n\t\t\t\t     struct ocrdma_cqe *cqe,\n\t\t\t\t     struct ib_wc *ibwc, bool *polled)\n{\n\tbool expand = false;\n\tint tail = qp->sq.tail;\n\tu32 wqe_idx;\n\n\tif (!qp->wqe_wr_id_tbl[tail].signaled) {\n\t\t*polled = false;     \n\t} else {\n\t\tibwc->status = IB_WC_SUCCESS;\n\t\tibwc->wc_flags = 0;\n\t\tibwc->qp = &qp->ibqp;\n\t\tocrdma_update_wc(qp, ibwc, tail);\n\t\t*polled = true;\n\t}\n\twqe_idx = (le32_to_cpu(cqe->wq.wqeidx) &\n\t\t\tOCRDMA_CQE_WQEIDX_MASK) & qp->sq.max_wqe_idx;\n\tif (tail != wqe_idx)\n\t\texpand = true;  \n\n\tocrdma_hwq_inc_tail(&qp->sq);\n\treturn expand;\n}\n\nstatic bool ocrdma_poll_scqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\n\t\t\t     struct ib_wc *ibwc, bool *polled, bool *stop)\n{\n\tint status;\n\tbool expand;\n\n\tstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\tOCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\n\n\tif (status == OCRDMA_CQE_SUCCESS)\n\t\texpand = ocrdma_poll_success_scqe(qp, cqe, ibwc, polled);\n\telse\n\t\texpand = ocrdma_poll_err_scqe(qp, cqe, ibwc, polled, stop);\n\treturn expand;\n}\n\nstatic int ocrdma_update_ud_rcqe(struct ocrdma_dev *dev, struct ib_wc *ibwc,\n\t\t\t\t struct ocrdma_cqe *cqe)\n{\n\tint status;\n\tu16 hdr_type = 0;\n\n\tstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\tOCRDMA_CQE_UD_STATUS_MASK) >> OCRDMA_CQE_UD_STATUS_SHIFT;\n\tibwc->src_qp = le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\t\t\t\t\tOCRDMA_CQE_SRCQP_MASK;\n\tibwc->pkey_index = 0;\n\tibwc->wc_flags = IB_WC_GRH;\n\tibwc->byte_len = (le32_to_cpu(cqe->ud.rxlen_pkey) >>\n\t\t\t  OCRDMA_CQE_UD_XFER_LEN_SHIFT) &\n\t\t\t  OCRDMA_CQE_UD_XFER_LEN_MASK;\n\n\tif (ocrdma_is_udp_encap_supported(dev)) {\n\t\thdr_type = (le32_to_cpu(cqe->ud.rxlen_pkey) >>\n\t\t\t    OCRDMA_CQE_UD_L3TYPE_SHIFT) &\n\t\t\t    OCRDMA_CQE_UD_L3TYPE_MASK;\n\t\tibwc->wc_flags |= IB_WC_WITH_NETWORK_HDR_TYPE;\n\t\tibwc->network_hdr_type = hdr_type;\n\t}\n\n\treturn status;\n}\n\nstatic void ocrdma_update_free_srq_cqe(struct ib_wc *ibwc,\n\t\t\t\t       struct ocrdma_cqe *cqe,\n\t\t\t\t       struct ocrdma_qp *qp)\n{\n\tunsigned long flags;\n\tstruct ocrdma_srq *srq;\n\tu32 wqe_idx;\n\n\tsrq = get_ocrdma_srq(qp->ibqp.srq);\n\twqe_idx = (le32_to_cpu(cqe->rq.buftag_qpn) >>\n\t\tOCRDMA_CQE_BUFTAG_SHIFT) & srq->rq.max_wqe_idx;\n\tBUG_ON(wqe_idx < 1);\n\n\tibwc->wr_id = srq->rqe_wr_id_tbl[wqe_idx];\n\tspin_lock_irqsave(&srq->q_lock, flags);\n\tocrdma_srq_toggle_bit(srq, wqe_idx - 1);\n\tspin_unlock_irqrestore(&srq->q_lock, flags);\n\tocrdma_hwq_inc_tail(&srq->rq);\n}\n\nstatic bool ocrdma_poll_err_rcqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\n\t\t\t\tstruct ib_wc *ibwc, bool *polled, bool *stop,\n\t\t\t\tint status)\n{\n\tbool expand;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(qp->ibqp.device);\n\n\tif (status < OCRDMA_MAX_CQE_ERR)\n\t\tatomic_inc(&dev->cqe_err_stats[status]);\n\n\t \n\tif (is_hw_rq_empty(qp) && !is_hw_sq_empty(qp)) {\n\t\tif (!qp->srq && (qp->sq_cq == qp->rq_cq)) {\n\t\t\t*polled = true;\n\t\t\tstatus = OCRDMA_CQE_WR_FLUSH_ERR;\n\t\t\texpand = ocrdma_update_err_scqe(ibwc, cqe, qp, status);\n\t\t} else {\n\t\t\t*polled = false;\n\t\t\t*stop = true;\n\t\t\texpand = false;\n\t\t}\n\t} else if (is_hw_rq_empty(qp)) {\n\t\t \n\t\texpand = false;\n\t\t*polled = false;\n\t\t*stop = false;\n\t} else {\n\t\t*polled = true;\n\t\texpand = ocrdma_update_err_rcqe(ibwc, cqe, qp, status);\n\t}\n\treturn expand;\n}\n\nstatic void ocrdma_poll_success_rcqe(struct ocrdma_qp *qp,\n\t\t\t\t     struct ocrdma_cqe *cqe, struct ib_wc *ibwc)\n{\n\tstruct ocrdma_dev *dev;\n\n\tdev = get_ocrdma_dev(qp->ibqp.device);\n\tibwc->opcode = IB_WC_RECV;\n\tibwc->qp = &qp->ibqp;\n\tibwc->status = IB_WC_SUCCESS;\n\n\tif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI)\n\t\tocrdma_update_ud_rcqe(dev, ibwc, cqe);\n\telse\n\t\tibwc->byte_len = le32_to_cpu(cqe->rq.rxlen);\n\n\tif (is_cqe_imm(cqe)) {\n\t\tibwc->ex.imm_data = htonl(le32_to_cpu(cqe->rq.lkey_immdt));\n\t\tibwc->wc_flags |= IB_WC_WITH_IMM;\n\t} else if (is_cqe_wr_imm(cqe)) {\n\t\tibwc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\n\t\tibwc->ex.imm_data = htonl(le32_to_cpu(cqe->rq.lkey_immdt));\n\t\tibwc->wc_flags |= IB_WC_WITH_IMM;\n\t} else if (is_cqe_invalidated(cqe)) {\n\t\tibwc->ex.invalidate_rkey = le32_to_cpu(cqe->rq.lkey_immdt);\n\t\tibwc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t}\n\tif (qp->ibqp.srq) {\n\t\tocrdma_update_free_srq_cqe(ibwc, cqe, qp);\n\t} else {\n\t\tibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\n\t\tocrdma_hwq_inc_tail(&qp->rq);\n\t}\n}\n\nstatic bool ocrdma_poll_rcqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\n\t\t\t     struct ib_wc *ibwc, bool *polled, bool *stop)\n{\n\tint status;\n\tbool expand = false;\n\n\tibwc->wc_flags = 0;\n\tif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\n\t\tstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\t\t\t\tOCRDMA_CQE_UD_STATUS_MASK) >>\n\t\t\t\t\tOCRDMA_CQE_UD_STATUS_SHIFT;\n\t} else {\n\t\tstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\n\t\t\t     OCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\n\t}\n\n\tif (status == OCRDMA_CQE_SUCCESS) {\n\t\t*polled = true;\n\t\tocrdma_poll_success_rcqe(qp, cqe, ibwc);\n\t} else {\n\t\texpand = ocrdma_poll_err_rcqe(qp, cqe, ibwc, polled, stop,\n\t\t\t\t\t      status);\n\t}\n\treturn expand;\n}\n\nstatic void ocrdma_change_cq_phase(struct ocrdma_cq *cq, struct ocrdma_cqe *cqe,\n\t\t\t\t   u16 cur_getp)\n{\n\tif (cq->phase_change) {\n\t\tif (cur_getp == 0)\n\t\t\tcq->phase = (~cq->phase & OCRDMA_CQE_VALID);\n\t} else {\n\t\t \n\t\tcqe->flags_status_srcqpn = 0;\n\t}\n}\n\nstatic int ocrdma_poll_hwcq(struct ocrdma_cq *cq, int num_entries,\n\t\t\t    struct ib_wc *ibwc)\n{\n\tu16 qpn = 0;\n\tint i = 0;\n\tbool expand = false;\n\tint polled_hw_cqes = 0;\n\tstruct ocrdma_qp *qp = NULL;\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(cq->ibcq.device);\n\tstruct ocrdma_cqe *cqe;\n\tu16 cur_getp; bool polled = false; bool stop = false;\n\n\tcur_getp = cq->getp;\n\twhile (num_entries) {\n\t\tcqe = cq->va + cur_getp;\n\t\t \n\t\tif (!is_cqe_valid(cq, cqe))\n\t\t\tbreak;\n\t\tqpn = (le32_to_cpu(cqe->cmn.qpn) & OCRDMA_CQE_QPN_MASK);\n\t\t \n\t\tif (qpn == 0)\n\t\t\tgoto skip_cqe;\n\t\tqp = dev->qp_tbl[qpn];\n\t\tBUG_ON(qp == NULL);\n\n\t\tif (is_cqe_for_sq(cqe)) {\n\t\t\texpand = ocrdma_poll_scqe(qp, cqe, ibwc, &polled,\n\t\t\t\t\t\t  &stop);\n\t\t} else {\n\t\t\texpand = ocrdma_poll_rcqe(qp, cqe, ibwc, &polled,\n\t\t\t\t\t\t  &stop);\n\t\t}\n\t\tif (expand)\n\t\t\tgoto expand_cqe;\n\t\tif (stop)\n\t\t\tgoto stop_cqe;\n\t\t \n\t\tcqe->cmn.qpn = 0;\nskip_cqe:\n\t\tpolled_hw_cqes += 1;\n\t\tcur_getp = (cur_getp + 1) % cq->max_hw_cqe;\n\t\tocrdma_change_cq_phase(cq, cqe, cur_getp);\nexpand_cqe:\n\t\tif (polled) {\n\t\t\tnum_entries -= 1;\n\t\t\ti += 1;\n\t\t\tibwc = ibwc + 1;\n\t\t\tpolled = false;\n\t\t}\n\t}\nstop_cqe:\n\tcq->getp = cur_getp;\n\n\tif (polled_hw_cqes)\n\t\tocrdma_ring_cq_db(dev, cq->id, false, false, polled_hw_cqes);\n\n\treturn i;\n}\n\n \nstatic int ocrdma_add_err_cqe(struct ocrdma_cq *cq, int num_entries,\n\t\t\t      struct ocrdma_qp *qp, struct ib_wc *ibwc)\n{\n\tint err_cqes = 0;\n\n\twhile (num_entries) {\n\t\tif (is_hw_sq_empty(qp) && is_hw_rq_empty(qp))\n\t\t\tbreak;\n\t\tif (!is_hw_sq_empty(qp) && qp->sq_cq == cq) {\n\t\t\tocrdma_update_wc(qp, ibwc, qp->sq.tail);\n\t\t\tocrdma_hwq_inc_tail(&qp->sq);\n\t\t} else if (!is_hw_rq_empty(qp) && qp->rq_cq == cq) {\n\t\t\tibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\n\t\t\tocrdma_hwq_inc_tail(&qp->rq);\n\t\t} else {\n\t\t\treturn err_cqes;\n\t\t}\n\t\tibwc->byte_len = 0;\n\t\tibwc->status = IB_WC_WR_FLUSH_ERR;\n\t\tibwc = ibwc + 1;\n\t\terr_cqes += 1;\n\t\tnum_entries -= 1;\n\t}\n\treturn err_cqes;\n}\n\nint ocrdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tint cqes_to_poll = num_entries;\n\tstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\n\tint num_os_cqe = 0, err_cqes = 0;\n\tstruct ocrdma_qp *qp;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tnum_os_cqe = ocrdma_poll_hwcq(cq, cqes_to_poll, wc);\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\tcqes_to_poll -= num_os_cqe;\n\n\tif (cqes_to_poll) {\n\t\twc = wc + num_os_cqe;\n\t\t \n\t\tspin_lock_irqsave(&dev->flush_q_lock, flags);\n\t\tlist_for_each_entry(qp, &cq->sq_head, sq_entry) {\n\t\t\tif (cqes_to_poll == 0)\n\t\t\t\tbreak;\n\t\t\terr_cqes = ocrdma_add_err_cqe(cq, cqes_to_poll, qp, wc);\n\t\t\tcqes_to_poll -= err_cqes;\n\t\t\tnum_os_cqe += err_cqes;\n\t\t\twc = wc + err_cqes;\n\t\t}\n\t\tspin_unlock_irqrestore(&dev->flush_q_lock, flags);\n\t}\n\treturn num_os_cqe;\n}\n\nint ocrdma_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags cq_flags)\n{\n\tstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\n\tu16 cq_id;\n\tunsigned long flags;\n\tbool arm_needed = false, sol_needed = false;\n\n\tcq_id = cq->id;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tif (cq_flags & IB_CQ_NEXT_COMP || cq_flags & IB_CQ_SOLICITED)\n\t\tarm_needed = true;\n\tif (cq_flags & IB_CQ_SOLICITED)\n\t\tsol_needed = true;\n\n\tocrdma_ring_cq_db(dev, cq_id, arm_needed, sol_needed, 0);\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\n\treturn 0;\n}\n\nstruct ib_mr *ocrdma_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,\n\t\t\t      u32 max_num_sg)\n{\n\tint status;\n\tstruct ocrdma_mr *mr;\n\tstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\n\tstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (max_num_sg > dev->attr.max_pages_per_frmr)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->pages = kcalloc(max_num_sg, sizeof(u64), GFP_KERNEL);\n\tif (!mr->pages) {\n\t\tstatus = -ENOMEM;\n\t\tgoto pl_err;\n\t}\n\n\tstatus = ocrdma_get_pbl_info(dev, mr, max_num_sg);\n\tif (status)\n\t\tgoto pbl_err;\n\tmr->hwmr.fr_mr = 1;\n\tmr->hwmr.remote_rd = 0;\n\tmr->hwmr.remote_wr = 0;\n\tmr->hwmr.local_rd = 0;\n\tmr->hwmr.local_wr = 0;\n\tmr->hwmr.mw_bind = 0;\n\tstatus = ocrdma_build_pbl_tbl(dev, &mr->hwmr);\n\tif (status)\n\t\tgoto pbl_err;\n\tstatus = ocrdma_reg_mr(dev, &mr->hwmr, pd->id, 0);\n\tif (status)\n\t\tgoto mbx_err;\n\tmr->ibmr.rkey = mr->hwmr.lkey;\n\tmr->ibmr.lkey = mr->hwmr.lkey;\n\tdev->stag_arr[(mr->hwmr.lkey >> 8) & (OCRDMA_MAX_STAG - 1)] =\n\t\t(unsigned long) mr;\n\treturn &mr->ibmr;\nmbx_err:\n\tocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\npbl_err:\n\tkfree(mr->pages);\npl_err:\n\tkfree(mr);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int ocrdma_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct ocrdma_mr *mr = get_ocrdma_mr(ibmr);\n\n\tif (unlikely(mr->npages == mr->hwmr.num_pbes))\n\t\treturn -ENOMEM;\n\n\tmr->pages[mr->npages++] = addr;\n\n\treturn 0;\n}\n\nint ocrdma_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t     unsigned int *sg_offset)\n{\n\tstruct ocrdma_mr *mr = get_ocrdma_mr(ibmr);\n\n\tmr->npages = 0;\n\n\treturn ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, ocrdma_set_page);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}