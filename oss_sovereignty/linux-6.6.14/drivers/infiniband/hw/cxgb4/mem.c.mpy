{
  "module_name": "mem.c",
  "hash_id": "1d40cb5f6bbde69e98000362aa8bbb47eefe5d3f7baae175736c10e9bd47657d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/mem.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <rdma/ib_umem.h>\n#include <linux/atomic.h>\n#include <rdma/ib_user_verbs.h>\n\n#include \"iw_cxgb4.h\"\n\nint use_dsgl = 1;\nmodule_param(use_dsgl, int, 0644);\nMODULE_PARM_DESC(use_dsgl, \"Use DSGL for PBL/FastReg (default=1) (DEPRECATED)\");\n\n#define T4_ULPTX_MIN_IO 32\n#define C4IW_MAX_INLINE_SIZE 96\n#define T4_ULPTX_MAX_DMA 1024\n#define C4IW_INLINE_THRESHOLD 128\n\nstatic int inline_threshold = C4IW_INLINE_THRESHOLD;\nmodule_param(inline_threshold, int, 0644);\nMODULE_PARM_DESC(inline_threshold, \"inline vs dsgl threshold (default=128)\");\n\nstatic int mr_exceeds_hw_limits(struct c4iw_dev *dev, u64 length)\n{\n\treturn (is_t4(dev->rdev.lldi.adapter_type) ||\n\t\tis_t5(dev->rdev.lldi.adapter_type)) &&\n\t\tlength >= 8*1024*1024*1024ULL;\n}\n\nstatic int _c4iw_write_mem_dma_aligned(struct c4iw_rdev *rdev, u32 addr,\n\t\t\t\t       u32 len, dma_addr_t data,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct ulp_mem_io *req;\n\tstruct ulptx_sgl *sgl;\n\tu8 wr_len;\n\tint ret = 0;\n\n\taddr &= 0x7FFFFFF;\n\n\tif (wr_waitp)\n\t\tc4iw_init_wr_wait(wr_waitp);\n\twr_len = roundup(sizeof(*req) + sizeof(*sgl), 16);\n\n\tif (!skb) {\n\t\tskb = alloc_skb(wr_len, GFP_KERNEL | __GFP_NOFAIL);\n\t\tif (!skb)\n\t\t\treturn -ENOMEM;\n\t}\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\treq = __skb_put_zero(skb, wr_len);\n\tINIT_ULPTX_WR(req, wr_len, 0, 0);\n\treq->wr.wr_hi = cpu_to_be32(FW_WR_OP_V(FW_ULPTX_WR) |\n\t\t\t(wr_waitp ? FW_WR_COMPL_F : 0));\n\treq->wr.wr_lo = wr_waitp ? (__force __be64)(unsigned long)wr_waitp : 0L;\n\treq->wr.wr_mid = cpu_to_be32(FW_WR_LEN16_V(DIV_ROUND_UP(wr_len, 16)));\n\treq->cmd = cpu_to_be32(ULPTX_CMD_V(ULP_TX_MEM_WRITE) |\n\t\t\t       T5_ULP_MEMIO_ORDER_V(1) |\n\t\t\t       T5_ULP_MEMIO_FID_V(rdev->lldi.rxq_ids[0]));\n\treq->dlen = cpu_to_be32(ULP_MEMIO_DATA_LEN_V(len>>5));\n\treq->len16 = cpu_to_be32(DIV_ROUND_UP(wr_len-sizeof(req->wr), 16));\n\treq->lock_addr = cpu_to_be32(ULP_MEMIO_ADDR_V(addr));\n\n\tsgl = (struct ulptx_sgl *)(req + 1);\n\tsgl->cmd_nsge = cpu_to_be32(ULPTX_CMD_V(ULP_TX_SC_DSGL) |\n\t\t\t\t    ULPTX_NSGE_V(1));\n\tsgl->len0 = cpu_to_be32(len);\n\tsgl->addr0 = cpu_to_be64(data);\n\n\tif (wr_waitp)\n\t\tret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);\n\telse\n\t\tret = c4iw_ofld_send(rdev, skb);\n\treturn ret;\n}\n\nstatic int _c4iw_write_mem_inline(struct c4iw_rdev *rdev, u32 addr, u32 len,\n\t\t\t\t  void *data, struct sk_buff *skb,\n\t\t\t\t  struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct ulp_mem_io *req;\n\tstruct ulptx_idata *sc;\n\tu8 wr_len, *to_dp, *from_dp;\n\tint copy_len, num_wqe, i, ret = 0;\n\t__be32 cmd = cpu_to_be32(ULPTX_CMD_V(ULP_TX_MEM_WRITE));\n\n\tif (is_t4(rdev->lldi.adapter_type))\n\t\tcmd |= cpu_to_be32(ULP_MEMIO_ORDER_F);\n\telse\n\t\tcmd |= cpu_to_be32(T5_ULP_MEMIO_IMM_F);\n\n\taddr &= 0x7FFFFFF;\n\tpr_debug(\"addr 0x%x len %u\\n\", addr, len);\n\tnum_wqe = DIV_ROUND_UP(len, C4IW_MAX_INLINE_SIZE);\n\tc4iw_init_wr_wait(wr_waitp);\n\tfor (i = 0; i < num_wqe; i++) {\n\n\t\tcopy_len = len > C4IW_MAX_INLINE_SIZE ? C4IW_MAX_INLINE_SIZE :\n\t\t\t   len;\n\t\twr_len = roundup(sizeof(*req) + sizeof(*sc) +\n\t\t\t\t\t roundup(copy_len, T4_ULPTX_MIN_IO),\n\t\t\t\t 16);\n\n\t\tif (!skb) {\n\t\t\tskb = alloc_skb(wr_len, GFP_KERNEL | __GFP_NOFAIL);\n\t\t\tif (!skb)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\t\treq = __skb_put_zero(skb, wr_len);\n\t\tINIT_ULPTX_WR(req, wr_len, 0, 0);\n\n\t\tif (i == (num_wqe-1)) {\n\t\t\treq->wr.wr_hi = cpu_to_be32(FW_WR_OP_V(FW_ULPTX_WR) |\n\t\t\t\t\t\t    FW_WR_COMPL_F);\n\t\t\treq->wr.wr_lo = (__force __be64)(unsigned long)wr_waitp;\n\t\t} else\n\t\t\treq->wr.wr_hi = cpu_to_be32(FW_WR_OP_V(FW_ULPTX_WR));\n\t\treq->wr.wr_mid = cpu_to_be32(\n\t\t\t\t       FW_WR_LEN16_V(DIV_ROUND_UP(wr_len, 16)));\n\n\t\treq->cmd = cmd;\n\t\treq->dlen = cpu_to_be32(ULP_MEMIO_DATA_LEN_V(\n\t\t\t\tDIV_ROUND_UP(copy_len, T4_ULPTX_MIN_IO)));\n\t\treq->len16 = cpu_to_be32(DIV_ROUND_UP(wr_len-sizeof(req->wr),\n\t\t\t\t\t\t      16));\n\t\treq->lock_addr = cpu_to_be32(ULP_MEMIO_ADDR_V(addr + i * 3));\n\n\t\tsc = (struct ulptx_idata *)(req + 1);\n\t\tsc->cmd_more = cpu_to_be32(ULPTX_CMD_V(ULP_TX_SC_IMM));\n\t\tsc->len = cpu_to_be32(roundup(copy_len, T4_ULPTX_MIN_IO));\n\n\t\tto_dp = (u8 *)(sc + 1);\n\t\tfrom_dp = (u8 *)data + i * C4IW_MAX_INLINE_SIZE;\n\t\tif (data)\n\t\t\tmemcpy(to_dp, from_dp, copy_len);\n\t\telse\n\t\t\tmemset(to_dp, 0, copy_len);\n\t\tif (copy_len % T4_ULPTX_MIN_IO)\n\t\t\tmemset(to_dp + copy_len, 0, T4_ULPTX_MIN_IO -\n\t\t\t       (copy_len % T4_ULPTX_MIN_IO));\n\t\tif (i == (num_wqe-1))\n\t\t\tret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0,\n\t\t\t\t\t\t __func__);\n\t\telse\n\t\t\tret = c4iw_ofld_send(rdev, skb);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tskb = NULL;\n\t\tlen -= C4IW_MAX_INLINE_SIZE;\n\t}\n\n\treturn ret;\n}\n\nstatic int _c4iw_write_mem_dma(struct c4iw_rdev *rdev, u32 addr, u32 len,\n\t\t\t       void *data, struct sk_buff *skb,\n\t\t\t       struct c4iw_wr_wait *wr_waitp)\n{\n\tu32 remain = len;\n\tu32 dmalen;\n\tint ret = 0;\n\tdma_addr_t daddr;\n\tdma_addr_t save;\n\n\tdaddr = dma_map_single(&rdev->lldi.pdev->dev, data, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(&rdev->lldi.pdev->dev, daddr))\n\t\treturn -1;\n\tsave = daddr;\n\n\twhile (remain > inline_threshold) {\n\t\tif (remain < T4_ULPTX_MAX_DMA) {\n\t\t\tif (remain & ~T4_ULPTX_MIN_IO)\n\t\t\t\tdmalen = remain & ~(T4_ULPTX_MIN_IO-1);\n\t\t\telse\n\t\t\t\tdmalen = remain;\n\t\t} else\n\t\t\tdmalen = T4_ULPTX_MAX_DMA;\n\t\tremain -= dmalen;\n\t\tret = _c4iw_write_mem_dma_aligned(rdev, addr, dmalen, daddr,\n\t\t\t\t\t\t skb, remain ? NULL : wr_waitp);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\taddr += dmalen >> 5;\n\t\tdata += dmalen;\n\t\tdaddr += dmalen;\n\t}\n\tif (remain)\n\t\tret = _c4iw_write_mem_inline(rdev, addr, remain, data, skb,\n\t\t\t\t\t     wr_waitp);\nout:\n\tdma_unmap_single(&rdev->lldi.pdev->dev, save, len, DMA_TO_DEVICE);\n\treturn ret;\n}\n\n \nstatic int write_adapter_mem(struct c4iw_rdev *rdev, u32 addr, u32 len,\n\t\t\t     void *data, struct sk_buff *skb,\n\t\t\t     struct c4iw_wr_wait *wr_waitp)\n{\n\tint ret;\n\n\tif (!rdev->lldi.ulptx_memwrite_dsgl || !use_dsgl) {\n\t\tret = _c4iw_write_mem_inline(rdev, addr, len, data, skb,\n\t\t\t\t\t      wr_waitp);\n\t\tgoto out;\n\t}\n\n\tif (len <= inline_threshold) {\n\t\tret = _c4iw_write_mem_inline(rdev, addr, len, data, skb,\n\t\t\t\t\t      wr_waitp);\n\t\tgoto out;\n\t}\n\n\tret = _c4iw_write_mem_dma(rdev, addr, len, data, skb, wr_waitp);\n\tif (ret) {\n\t\tpr_warn_ratelimited(\"%s: dma map failure (non fatal)\\n\",\n\t\t\t\t    pci_name(rdev->lldi.pdev));\n\t\tret = _c4iw_write_mem_inline(rdev, addr, len, data, skb,\n\t\t\t\t\t      wr_waitp);\n\t}\nout:\n\treturn ret;\n\n}\n\n \nstatic int write_tpt_entry(struct c4iw_rdev *rdev, u32 reset_tpt_entry,\n\t\t\t   u32 *stag, u8 stag_state, u32 pdid,\n\t\t\t   enum fw_ri_stag_type type, enum fw_ri_mem_perms perm,\n\t\t\t   int bind_enabled, u32 zbva, u64 to,\n\t\t\t   u64 len, u8 page_size, u32 pbl_size, u32 pbl_addr,\n\t\t\t   struct sk_buff *skb, struct c4iw_wr_wait *wr_waitp)\n{\n\tint err;\n\tstruct fw_ri_tpte *tpt;\n\tu32 stag_idx;\n\tstatic atomic_t key;\n\n\tif (c4iw_fatal_error(rdev))\n\t\treturn -EIO;\n\n\ttpt = kmalloc(sizeof(*tpt), GFP_KERNEL);\n\tif (!tpt)\n\t\treturn -ENOMEM;\n\n\tstag_state = stag_state > 0;\n\tstag_idx = (*stag) >> 8;\n\n\tif ((!reset_tpt_entry) && (*stag == T4_STAG_UNSET)) {\n\t\tstag_idx = c4iw_get_resource(&rdev->resource.tpt_table);\n\t\tif (!stag_idx) {\n\t\t\tmutex_lock(&rdev->stats.lock);\n\t\t\trdev->stats.stag.fail++;\n\t\t\tmutex_unlock(&rdev->stats.lock);\n\t\t\tkfree(tpt);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmutex_lock(&rdev->stats.lock);\n\t\trdev->stats.stag.cur += 32;\n\t\tif (rdev->stats.stag.cur > rdev->stats.stag.max)\n\t\t\trdev->stats.stag.max = rdev->stats.stag.cur;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t\t*stag = (stag_idx << 8) | (atomic_inc_return(&key) & 0xff);\n\t}\n\tpr_debug(\"stag_state 0x%0x type 0x%0x pdid 0x%0x, stag_idx 0x%x\\n\",\n\t\t stag_state, type, pdid, stag_idx);\n\n\t \n\tif (reset_tpt_entry)\n\t\tmemset(tpt, 0, sizeof(*tpt));\n\telse {\n\t\ttpt->valid_to_pdid = cpu_to_be32(FW_RI_TPTE_VALID_F |\n\t\t\tFW_RI_TPTE_STAGKEY_V((*stag & FW_RI_TPTE_STAGKEY_M)) |\n\t\t\tFW_RI_TPTE_STAGSTATE_V(stag_state) |\n\t\t\tFW_RI_TPTE_STAGTYPE_V(type) | FW_RI_TPTE_PDID_V(pdid));\n\t\ttpt->locread_to_qpid = cpu_to_be32(FW_RI_TPTE_PERM_V(perm) |\n\t\t\t(bind_enabled ? FW_RI_TPTE_MWBINDEN_F : 0) |\n\t\t\tFW_RI_TPTE_ADDRTYPE_V((zbva ? FW_RI_ZERO_BASED_TO :\n\t\t\t\t\t\t      FW_RI_VA_BASED_TO))|\n\t\t\tFW_RI_TPTE_PS_V(page_size));\n\t\ttpt->nosnoop_pbladdr = !pbl_size ? 0 : cpu_to_be32(\n\t\t\tFW_RI_TPTE_PBLADDR_V(PBL_OFF(rdev, pbl_addr)>>3));\n\t\ttpt->len_lo = cpu_to_be32((u32)(len & 0xffffffffUL));\n\t\ttpt->va_hi = cpu_to_be32((u32)(to >> 32));\n\t\ttpt->va_lo_fbo = cpu_to_be32((u32)(to & 0xffffffffUL));\n\t\ttpt->dca_mwbcnt_pstag = cpu_to_be32(0);\n\t\ttpt->len_hi = cpu_to_be32((u32)(len >> 32));\n\t}\n\terr = write_adapter_mem(rdev, stag_idx +\n\t\t\t\t(rdev->lldi.vr->stag.start >> 5),\n\t\t\t\tsizeof(*tpt), tpt, skb, wr_waitp);\n\n\tif (reset_tpt_entry) {\n\t\tc4iw_put_resource(&rdev->resource.tpt_table, stag_idx);\n\t\tmutex_lock(&rdev->stats.lock);\n\t\trdev->stats.stag.cur -= 32;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t}\n\tkfree(tpt);\n\treturn err;\n}\n\nstatic int write_pbl(struct c4iw_rdev *rdev, __be64 *pbl,\n\t\t     u32 pbl_addr, u32 pbl_size, struct c4iw_wr_wait *wr_waitp)\n{\n\tint err;\n\n\tpr_debug(\"*pdb_addr 0x%x, pbl_base 0x%x, pbl_size %d\\n\",\n\t\t pbl_addr, rdev->lldi.vr->pbl.start,\n\t\t pbl_size);\n\n\terr = write_adapter_mem(rdev, pbl_addr >> 5, pbl_size << 3, pbl, NULL,\n\t\t\t\twr_waitp);\n\treturn err;\n}\n\nstatic int dereg_mem(struct c4iw_rdev *rdev, u32 stag, u32 pbl_size,\n\t\t     u32 pbl_addr, struct sk_buff *skb,\n\t\t     struct c4iw_wr_wait *wr_waitp)\n{\n\treturn write_tpt_entry(rdev, 1, &stag, 0, 0, 0, 0, 0, 0, 0UL, 0, 0,\n\t\t\t       pbl_size, pbl_addr, skb, wr_waitp);\n}\n\nstatic int allocate_stag(struct c4iw_rdev *rdev, u32 *stag, u32 pdid,\n\t\t\t u32 pbl_size, u32 pbl_addr,\n\t\t\t struct c4iw_wr_wait *wr_waitp)\n{\n\t*stag = T4_STAG_UNSET;\n\treturn write_tpt_entry(rdev, 0, stag, 0, pdid, FW_RI_STAG_NSMR, 0, 0, 0,\n\t\t\t       0UL, 0, 0, pbl_size, pbl_addr, NULL, wr_waitp);\n}\n\nstatic int finish_mem_reg(struct c4iw_mr *mhp, u32 stag)\n{\n\tu32 mmid;\n\n\tmhp->attr.state = 1;\n\tmhp->attr.stag = stag;\n\tmmid = stag >> 8;\n\tmhp->ibmr.rkey = mhp->ibmr.lkey = stag;\n\tmhp->ibmr.length = mhp->attr.len;\n\tmhp->ibmr.page_size = 1U << (mhp->attr.page_size + 12);\n\tpr_debug(\"mmid 0x%x mhp %p\\n\", mmid, mhp);\n\treturn xa_insert_irq(&mhp->rhp->mrs, mmid, mhp, GFP_KERNEL);\n}\n\nstatic int register_mem(struct c4iw_dev *rhp, struct c4iw_pd *php,\n\t\t      struct c4iw_mr *mhp, int shift)\n{\n\tu32 stag = T4_STAG_UNSET;\n\tint ret;\n\n\tret = write_tpt_entry(&rhp->rdev, 0, &stag, 1, mhp->attr.pdid,\n\t\t\t      FW_RI_STAG_NSMR, mhp->attr.len ?\n\t\t\t      mhp->attr.perms : 0,\n\t\t\t      mhp->attr.mw_bind_enable, mhp->attr.zbva,\n\t\t\t      mhp->attr.va_fbo, mhp->attr.len ?\n\t\t\t      mhp->attr.len : -1, shift - 12,\n\t\t\t      mhp->attr.pbl_size, mhp->attr.pbl_addr, NULL,\n\t\t\t      mhp->wr_waitp);\n\tif (ret)\n\t\treturn ret;\n\n\tret = finish_mem_reg(mhp, stag);\n\tif (ret) {\n\t\tdereg_mem(&rhp->rdev, mhp->attr.stag, mhp->attr.pbl_size,\n\t\t\t  mhp->attr.pbl_addr, mhp->dereg_skb, mhp->wr_waitp);\n\t\tmhp->dereg_skb = NULL;\n\t}\n\treturn ret;\n}\n\nstatic int alloc_pbl(struct c4iw_mr *mhp, int npages)\n{\n\tmhp->attr.pbl_addr = c4iw_pblpool_alloc(&mhp->rhp->rdev,\n\t\t\t\t\t\t    npages << 3);\n\n\tif (!mhp->attr.pbl_addr)\n\t\treturn -ENOMEM;\n\n\tmhp->attr.pbl_size = npages;\n\n\treturn 0;\n}\n\nstruct ib_mr *c4iw_get_dma_mr(struct ib_pd *pd, int acc)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_pd *php;\n\tstruct c4iw_mr *mhp;\n\tint ret;\n\tu32 stag = T4_STAG_UNSET;\n\n\tpr_debug(\"ib_pd %p\\n\", pd);\n\tphp = to_c4iw_pd(pd);\n\trhp = php->rhp;\n\n\tmhp = kzalloc(sizeof(*mhp), GFP_KERNEL);\n\tif (!mhp)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmhp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!mhp->wr_waitp) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_mhp;\n\t}\n\tc4iw_init_wr_wait(mhp->wr_waitp);\n\n\tmhp->dereg_skb = alloc_skb(SGE_MAX_WR_LEN, GFP_KERNEL);\n\tif (!mhp->dereg_skb) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_wr_wait;\n\t}\n\n\tmhp->rhp = rhp;\n\tmhp->attr.pdid = php->pdid;\n\tmhp->attr.perms = c4iw_ib_to_tpt_access(acc);\n\tmhp->attr.mw_bind_enable = (acc&IB_ACCESS_MW_BIND) == IB_ACCESS_MW_BIND;\n\tmhp->attr.zbva = 0;\n\tmhp->attr.va_fbo = 0;\n\tmhp->attr.page_size = 0;\n\tmhp->attr.len = ~0ULL;\n\tmhp->attr.pbl_size = 0;\n\n\tret = write_tpt_entry(&rhp->rdev, 0, &stag, 1, php->pdid,\n\t\t\t      FW_RI_STAG_NSMR, mhp->attr.perms,\n\t\t\t      mhp->attr.mw_bind_enable, 0, 0, ~0ULL, 0, 0, 0,\n\t\t\t      NULL, mhp->wr_waitp);\n\tif (ret)\n\t\tgoto err_free_skb;\n\n\tret = finish_mem_reg(mhp, stag);\n\tif (ret)\n\t\tgoto err_dereg_mem;\n\treturn &mhp->ibmr;\nerr_dereg_mem:\n\tdereg_mem(&rhp->rdev, mhp->attr.stag, mhp->attr.pbl_size,\n\t\t  mhp->attr.pbl_addr, mhp->dereg_skb, mhp->wr_waitp);\nerr_free_skb:\n\tkfree_skb(mhp->dereg_skb);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(mhp->wr_waitp);\nerr_free_mhp:\n\tkfree(mhp);\n\treturn ERR_PTR(ret);\n}\n\nstruct ib_mr *c4iw_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t       u64 virt, int acc, struct ib_udata *udata)\n{\n\t__be64 *pages;\n\tint shift, n, i;\n\tint err = -ENOMEM;\n\tstruct ib_block_iter biter;\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_pd *php;\n\tstruct c4iw_mr *mhp;\n\n\tpr_debug(\"ib_pd %p\\n\", pd);\n\n\tif (length == ~0ULL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((length + start) < start)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tphp = to_c4iw_pd(pd);\n\trhp = php->rhp;\n\n\tif (mr_exceeds_hw_limits(rhp, length))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmhp = kzalloc(sizeof(*mhp), GFP_KERNEL);\n\tif (!mhp)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmhp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!mhp->wr_waitp)\n\t\tgoto err_free_mhp;\n\n\tmhp->dereg_skb = alloc_skb(SGE_MAX_WR_LEN, GFP_KERNEL);\n\tif (!mhp->dereg_skb)\n\t\tgoto err_free_wr_wait;\n\n\tmhp->rhp = rhp;\n\n\tmhp->umem = ib_umem_get(pd->device, start, length, acc);\n\tif (IS_ERR(mhp->umem))\n\t\tgoto err_free_skb;\n\n\tshift = PAGE_SHIFT;\n\n\tn = ib_umem_num_dma_blocks(mhp->umem, 1 << shift);\n\terr = alloc_pbl(mhp, n);\n\tif (err)\n\t\tgoto err_umem_release;\n\n\tpages = (__be64 *) __get_free_page(GFP_KERNEL);\n\tif (!pages) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pbl_free;\n\t}\n\n\ti = n = 0;\n\n\trdma_umem_for_each_dma_block(mhp->umem, &biter, 1 << shift) {\n\t\tpages[i++] = cpu_to_be64(rdma_block_iter_dma_address(&biter));\n\t\tif (i == PAGE_SIZE / sizeof(*pages)) {\n\t\t\terr = write_pbl(&mhp->rhp->rdev, pages,\n\t\t\t\t\tmhp->attr.pbl_addr + (n << 3), i,\n\t\t\t\t\tmhp->wr_waitp);\n\t\t\tif (err)\n\t\t\t\tgoto pbl_done;\n\t\t\tn += i;\n\t\t\ti = 0;\n\t\t}\n\t}\n\n\tif (i)\n\t\terr = write_pbl(&mhp->rhp->rdev, pages,\n\t\t\t\tmhp->attr.pbl_addr + (n << 3), i,\n\t\t\t\tmhp->wr_waitp);\n\npbl_done:\n\tfree_page((unsigned long) pages);\n\tif (err)\n\t\tgoto err_pbl_free;\n\n\tmhp->attr.pdid = php->pdid;\n\tmhp->attr.zbva = 0;\n\tmhp->attr.perms = c4iw_ib_to_tpt_access(acc);\n\tmhp->attr.va_fbo = virt;\n\tmhp->attr.page_size = shift - 12;\n\tmhp->attr.len = length;\n\n\terr = register_mem(rhp, php, mhp, shift);\n\tif (err)\n\t\tgoto err_pbl_free;\n\n\treturn &mhp->ibmr;\n\nerr_pbl_free:\n\tc4iw_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,\n\t\t\t      mhp->attr.pbl_size << 3);\nerr_umem_release:\n\tib_umem_release(mhp->umem);\nerr_free_skb:\n\tkfree_skb(mhp->dereg_skb);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(mhp->wr_waitp);\nerr_free_mhp:\n\tkfree(mhp);\n\treturn ERR_PTR(err);\n}\n\nstruct ib_mr *c4iw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t    u32 max_num_sg)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_pd *php;\n\tstruct c4iw_mr *mhp;\n\tu32 mmid;\n\tu32 stag = 0;\n\tint ret = 0;\n\tint length = roundup(max_num_sg * sizeof(u64), 32);\n\n\tphp = to_c4iw_pd(pd);\n\trhp = php->rhp;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG ||\n\t    max_num_sg > t4_max_fr_depth(rhp->rdev.lldi.ulptx_memwrite_dsgl &&\n\t\t\t\t\t use_dsgl))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmhp = kzalloc(sizeof(*mhp), GFP_KERNEL);\n\tif (!mhp) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tmhp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!mhp->wr_waitp) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_mhp;\n\t}\n\tc4iw_init_wr_wait(mhp->wr_waitp);\n\n\tmhp->mpl = dma_alloc_coherent(&rhp->rdev.lldi.pdev->dev,\n\t\t\t\t      length, &mhp->mpl_addr, GFP_KERNEL);\n\tif (!mhp->mpl) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_wr_wait;\n\t}\n\tmhp->max_mpl_len = length;\n\n\tmhp->rhp = rhp;\n\tret = alloc_pbl(mhp, max_num_sg);\n\tif (ret)\n\t\tgoto err_free_dma;\n\tmhp->attr.pbl_size = max_num_sg;\n\tret = allocate_stag(&rhp->rdev, &stag, php->pdid,\n\t\t\t    mhp->attr.pbl_size, mhp->attr.pbl_addr,\n\t\t\t    mhp->wr_waitp);\n\tif (ret)\n\t\tgoto err_free_pbl;\n\tmhp->attr.pdid = php->pdid;\n\tmhp->attr.type = FW_RI_STAG_NSMR;\n\tmhp->attr.stag = stag;\n\tmhp->attr.state = 0;\n\tmmid = (stag) >> 8;\n\tmhp->ibmr.rkey = mhp->ibmr.lkey = stag;\n\tif (xa_insert_irq(&rhp->mrs, mmid, mhp, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto err_dereg;\n\t}\n\n\tpr_debug(\"mmid 0x%x mhp %p stag 0x%x\\n\", mmid, mhp, stag);\n\treturn &(mhp->ibmr);\nerr_dereg:\n\tdereg_mem(&rhp->rdev, stag, mhp->attr.pbl_size,\n\t\t  mhp->attr.pbl_addr, mhp->dereg_skb, mhp->wr_waitp);\nerr_free_pbl:\n\tc4iw_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,\n\t\t\t      mhp->attr.pbl_size << 3);\nerr_free_dma:\n\tdma_free_coherent(&mhp->rhp->rdev.lldi.pdev->dev,\n\t\t\t  mhp->max_mpl_len, mhp->mpl, mhp->mpl_addr);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(mhp->wr_waitp);\nerr_free_mhp:\n\tkfree(mhp);\nerr:\n\treturn ERR_PTR(ret);\n}\n\nstatic int c4iw_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct c4iw_mr *mhp = to_c4iw_mr(ibmr);\n\n\tif (unlikely(mhp->mpl_len == mhp->attr.pbl_size))\n\t\treturn -ENOMEM;\n\n\tmhp->mpl[mhp->mpl_len++] = addr;\n\n\treturn 0;\n}\n\nint c4iw_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t   unsigned int *sg_offset)\n{\n\tstruct c4iw_mr *mhp = to_c4iw_mr(ibmr);\n\n\tmhp->mpl_len = 0;\n\n\treturn ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, c4iw_set_page);\n}\n\nint c4iw_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_mr *mhp;\n\tu32 mmid;\n\n\tpr_debug(\"ib_mr %p\\n\", ib_mr);\n\n\tmhp = to_c4iw_mr(ib_mr);\n\trhp = mhp->rhp;\n\tmmid = mhp->attr.stag >> 8;\n\txa_erase_irq(&rhp->mrs, mmid);\n\tif (mhp->mpl)\n\t\tdma_free_coherent(&mhp->rhp->rdev.lldi.pdev->dev,\n\t\t\t\t  mhp->max_mpl_len, mhp->mpl, mhp->mpl_addr);\n\tdereg_mem(&rhp->rdev, mhp->attr.stag, mhp->attr.pbl_size,\n\t\t  mhp->attr.pbl_addr, mhp->dereg_skb, mhp->wr_waitp);\n\tif (mhp->attr.pbl_size)\n\t\tc4iw_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,\n\t\t\t\t  mhp->attr.pbl_size << 3);\n\tif (mhp->kva)\n\t\tkfree((void *) (unsigned long) mhp->kva);\n\tib_umem_release(mhp->umem);\n\tpr_debug(\"mmid 0x%x ptr %p\\n\", mmid, mhp);\n\tc4iw_put_wr_wait(mhp->wr_waitp);\n\tkfree(mhp);\n\treturn 0;\n}\n\nvoid c4iw_invalidate_mr(struct c4iw_dev *rhp, u32 rkey)\n{\n\tstruct c4iw_mr *mhp;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&rhp->mrs, flags);\n\tmhp = xa_load(&rhp->mrs, rkey >> 8);\n\tif (mhp)\n\t\tmhp->attr.state = 0;\n\txa_unlock_irqrestore(&rhp->mrs, flags);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}