{
  "module_name": "t4.h",
  "hash_id": "eb4d6c5fae46364865693e78a03447cba843964143784064f8bef6d67bf47003",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/t4.h",
  "human_readable_source": " \n#ifndef __T4_H__\n#define __T4_H__\n\n#include \"t4_hw.h\"\n#include \"t4_regs.h\"\n#include \"t4_values.h\"\n#include \"t4_msg.h\"\n#include \"t4_tcb.h\"\n#include \"t4fw_ri_api.h\"\n\n#define T4_MAX_NUM_PD 65536\n#define T4_MAX_MR_SIZE (~0ULL)\n#define T4_PAGESIZE_MASK 0xffff000   \n#define T4_STAG_UNSET 0xffffffff\n#define T4_FW_MAJ 0\n#define PCIE_MA_SYNC_A 0x30b4\n\nstruct t4_status_page {\n\t__be32 rsvd1;\t \n\t__be16 rsvd2;\n\t__be16 qid;\n\t__be16 cidx;\n\t__be16 pidx;\n\tu8 qp_err;\t \n\tu8 db_off;\n\tu8 pad[2];\n\tu16 host_wq_pidx;\n\tu16 host_cidx;\n\tu16 host_pidx;\n\tu16 pad2;\n\tu32 srqidx;\n};\n\n#define T4_RQT_ENTRY_SHIFT 6\n#define T4_RQT_ENTRY_SIZE  BIT(T4_RQT_ENTRY_SHIFT)\n#define T4_EQ_ENTRY_SIZE 64\n\n#define T4_SQ_NUM_SLOTS 5\n#define T4_SQ_NUM_BYTES (T4_EQ_ENTRY_SIZE * T4_SQ_NUM_SLOTS)\n#define T4_MAX_SEND_SGE ((T4_SQ_NUM_BYTES - sizeof(struct fw_ri_send_wr) - \\\n\t\t\tsizeof(struct fw_ri_isgl)) / sizeof(struct fw_ri_sge))\n#define T4_MAX_SEND_INLINE ((T4_SQ_NUM_BYTES - sizeof(struct fw_ri_send_wr) - \\\n\t\t\tsizeof(struct fw_ri_immd)))\n#define T4_MAX_WRITE_INLINE ((T4_SQ_NUM_BYTES - \\\n\t\t\tsizeof(struct fw_ri_rdma_write_wr) - \\\n\t\t\tsizeof(struct fw_ri_immd)))\n#define T4_MAX_WRITE_SGE ((T4_SQ_NUM_BYTES - \\\n\t\t\tsizeof(struct fw_ri_rdma_write_wr) - \\\n\t\t\tsizeof(struct fw_ri_isgl)) / sizeof(struct fw_ri_sge))\n#define T4_MAX_FR_IMMD ((T4_SQ_NUM_BYTES - sizeof(struct fw_ri_fr_nsmr_wr) - \\\n\t\t\tsizeof(struct fw_ri_immd)) & ~31UL)\n#define T4_MAX_FR_IMMD_DEPTH (T4_MAX_FR_IMMD / sizeof(u64))\n#define T4_MAX_FR_DSGL 1024\n#define T4_MAX_FR_DSGL_DEPTH (T4_MAX_FR_DSGL / sizeof(u64))\n\nstatic inline int t4_max_fr_depth(int use_dsgl)\n{\n\treturn use_dsgl ? T4_MAX_FR_DSGL_DEPTH : T4_MAX_FR_IMMD_DEPTH;\n}\n\n#define T4_RQ_NUM_SLOTS 2\n#define T4_RQ_NUM_BYTES (T4_EQ_ENTRY_SIZE * T4_RQ_NUM_SLOTS)\n#define T4_MAX_RECV_SGE 4\n\n#define T4_WRITE_CMPL_MAX_SGL 4\n#define T4_WRITE_CMPL_MAX_CQE 16\n\nunion t4_wr {\n\tstruct fw_ri_res_wr res;\n\tstruct fw_ri_wr ri;\n\tstruct fw_ri_rdma_write_wr write;\n\tstruct fw_ri_send_wr send;\n\tstruct fw_ri_rdma_read_wr read;\n\tstruct fw_ri_bind_mw_wr bind;\n\tstruct fw_ri_fr_nsmr_wr fr;\n\tstruct fw_ri_fr_nsmr_tpte_wr fr_tpte;\n\tstruct fw_ri_inv_lstag_wr inv;\n\tstruct fw_ri_rdma_write_cmpl_wr write_cmpl;\n\tstruct t4_status_page status;\n\t__be64 flits[T4_EQ_ENTRY_SIZE / sizeof(__be64) * T4_SQ_NUM_SLOTS];\n};\n\nunion t4_recv_wr {\n\tstruct fw_ri_recv_wr recv;\n\tstruct t4_status_page status;\n\t__be64 flits[T4_EQ_ENTRY_SIZE / sizeof(__be64) * T4_RQ_NUM_SLOTS];\n};\n\nstatic inline void init_wr_hdr(union t4_wr *wqe, u16 wrid,\n\t\t\t       enum fw_wr_opcodes opcode, u8 flags, u8 len16)\n{\n\twqe->send.opcode = (u8)opcode;\n\twqe->send.flags = flags;\n\twqe->send.wrid = wrid;\n\twqe->send.r1[0] = 0;\n\twqe->send.r1[1] = 0;\n\twqe->send.r1[2] = 0;\n\twqe->send.len16 = len16;\n}\n\n \n#define T4_ERR_SUCCESS                     0x0\n#define T4_ERR_STAG                        0x1\t \n\t\t\t\t\t\t \n\t\t\t\t\t\t \n#define T4_ERR_PDID                        0x2\t \n#define T4_ERR_QPID                        0x3\t \n#define T4_ERR_ACCESS                      0x4\t \n#define T4_ERR_WRAP                        0x5\t \n#define T4_ERR_BOUND                       0x6\t \n#define T4_ERR_INVALIDATE_SHARED_MR        0x7\t \n\t\t\t\t\t\t \n#define T4_ERR_INVALIDATE_MR_WITH_MW_BOUND 0x8\t \n\t\t\t\t\t\t \n#define T4_ERR_ECC                         0x9\t \n#define T4_ERR_ECC_PSTAG                   0xA\t \n\t\t\t\t\t\t \n\t\t\t\t\t\t \n#define T4_ERR_PBL_ADDR_BOUND              0xB\t \n\t\t\t\t\t\t \n#define T4_ERR_SWFLUSH\t\t\t   0xC\t \n#define T4_ERR_CRC                         0x10  \n#define T4_ERR_MARKER                      0x11  \n#define T4_ERR_PDU_LEN_ERR                 0x12  \n#define T4_ERR_OUT_OF_RQE                  0x13  \n#define T4_ERR_DDP_VERSION                 0x14  \n#define T4_ERR_RDMA_VERSION                0x15  \n#define T4_ERR_OPCODE                      0x16  \n#define T4_ERR_DDP_QUEUE_NUM               0x17  \n#define T4_ERR_MSN                         0x18  \n#define T4_ERR_TBIT                        0x19  \n#define T4_ERR_MO                          0x1A  \n\t\t\t\t\t\t \n#define T4_ERR_MSN_GAP                     0x1B\n#define T4_ERR_MSN_RANGE                   0x1C\n#define T4_ERR_IRD_OVERFLOW                0x1D\n#define T4_ERR_RQE_ADDR_BOUND              0x1E  \n\t\t\t\t\t\t \n#define T4_ERR_INTERNAL_ERR                0x1F  \n\t\t\t\t\t\t \n \nstruct t4_cqe {\n\t__be32 header;\n\t__be32 len;\n\tunion {\n\t\tstruct {\n\t\t\t__be32 stag;\n\t\t\t__be32 msn;\n\t\t} rcqe;\n\t\tstruct {\n\t\t\t__be32 stag;\n\t\t\tu16 nada2;\n\t\t\tu16 cidx;\n\t\t} scqe;\n\t\tstruct {\n\t\t\t__be32 wrid_hi;\n\t\t\t__be32 wrid_low;\n\t\t} gen;\n\t\tstruct {\n\t\t\t__be32 stag;\n\t\t\t__be32 msn;\n\t\t\t__be32 reserved;\n\t\t\t__be32 abs_rqe_idx;\n\t\t} srcqe;\n\t\tstruct {\n\t\t\t__be32 mo;\n\t\t\t__be32 msn;\n\t\t\t \n\t\t\tunion {\n\t\t\t\tstruct {\n\t\t\t\t\t__be32 imm_data32;\n\t\t\t\t\tu32 reserved;\n\t\t\t\t} ib_imm_data;\n\t\t\t\t__be64 imm_data64;\n\t\t\t} iw_imm_data;\n\t\t} imm_data_rcqe;\n\n\t\tu64 drain_cookie;\n\t\t__be64 flits[3];\n\t} u;\n\t__be64 reserved[3];\n\t__be64 bits_type_ts;\n};\n\n \n\n#define CQE_QPID_S        12\n#define CQE_QPID_M        0xFFFFF\n#define CQE_QPID_G(x)     ((((x) >> CQE_QPID_S)) & CQE_QPID_M)\n#define CQE_QPID_V(x)\t  ((x)<<CQE_QPID_S)\n\n#define CQE_SWCQE_S       11\n#define CQE_SWCQE_M       0x1\n#define CQE_SWCQE_G(x)    ((((x) >> CQE_SWCQE_S)) & CQE_SWCQE_M)\n#define CQE_SWCQE_V(x)\t  ((x)<<CQE_SWCQE_S)\n\n#define CQE_DRAIN_S       10\n#define CQE_DRAIN_M       0x1\n#define CQE_DRAIN_G(x)    ((((x) >> CQE_DRAIN_S)) & CQE_DRAIN_M)\n#define CQE_DRAIN_V(x)\t  ((x)<<CQE_DRAIN_S)\n\n#define CQE_STATUS_S      5\n#define CQE_STATUS_M      0x1F\n#define CQE_STATUS_G(x)   ((((x) >> CQE_STATUS_S)) & CQE_STATUS_M)\n#define CQE_STATUS_V(x)   ((x)<<CQE_STATUS_S)\n\n#define CQE_TYPE_S        4\n#define CQE_TYPE_M        0x1\n#define CQE_TYPE_G(x)     ((((x) >> CQE_TYPE_S)) & CQE_TYPE_M)\n#define CQE_TYPE_V(x)     ((x)<<CQE_TYPE_S)\n\n#define CQE_OPCODE_S      0\n#define CQE_OPCODE_M      0xF\n#define CQE_OPCODE_G(x)   ((((x) >> CQE_OPCODE_S)) & CQE_OPCODE_M)\n#define CQE_OPCODE_V(x)   ((x)<<CQE_OPCODE_S)\n\n#define SW_CQE(x)         (CQE_SWCQE_G(be32_to_cpu((x)->header)))\n#define DRAIN_CQE(x)      (CQE_DRAIN_G(be32_to_cpu((x)->header)))\n#define CQE_QPID(x)       (CQE_QPID_G(be32_to_cpu((x)->header)))\n#define CQE_TYPE(x)       (CQE_TYPE_G(be32_to_cpu((x)->header)))\n#define SQ_TYPE(x)\t  (CQE_TYPE((x)))\n#define RQ_TYPE(x)\t  (!CQE_TYPE((x)))\n#define CQE_STATUS(x)     (CQE_STATUS_G(be32_to_cpu((x)->header)))\n#define CQE_OPCODE(x)     (CQE_OPCODE_G(be32_to_cpu((x)->header)))\n\n#define CQE_SEND_OPCODE(x)( \\\n\t(CQE_OPCODE_G(be32_to_cpu((x)->header)) == FW_RI_SEND) || \\\n\t(CQE_OPCODE_G(be32_to_cpu((x)->header)) == FW_RI_SEND_WITH_SE) || \\\n\t(CQE_OPCODE_G(be32_to_cpu((x)->header)) == FW_RI_SEND_WITH_INV) || \\\n\t(CQE_OPCODE_G(be32_to_cpu((x)->header)) == FW_RI_SEND_WITH_SE_INV))\n\n#define CQE_LEN(x)        (be32_to_cpu((x)->len))\n\n \n#define CQE_WRID_STAG(x)  (be32_to_cpu((x)->u.rcqe.stag))\n#define CQE_WRID_MSN(x)   (be32_to_cpu((x)->u.rcqe.msn))\n#define CQE_ABS_RQE_IDX(x) (be32_to_cpu((x)->u.srcqe.abs_rqe_idx))\n#define CQE_IMM_DATA(x)( \\\n\t(x)->u.imm_data_rcqe.iw_imm_data.ib_imm_data.imm_data32)\n\n \n#define CQE_WRID_SQ_IDX(x)\t((x)->u.scqe.cidx)\n#define CQE_WRID_FR_STAG(x)     (be32_to_cpu((x)->u.scqe.stag))\n\n \n#define CQE_WRID_HI(x)\t\t(be32_to_cpu((x)->u.gen.wrid_hi))\n#define CQE_WRID_LOW(x)\t\t(be32_to_cpu((x)->u.gen.wrid_low))\n#define CQE_DRAIN_COOKIE(x)\t((x)->u.drain_cookie)\n\n \n#define CQE_GENBIT_S\t63\n#define CQE_GENBIT_M\t0x1\n#define CQE_GENBIT_G(x)\t(((x) >> CQE_GENBIT_S) & CQE_GENBIT_M)\n#define CQE_GENBIT_V(x) ((x)<<CQE_GENBIT_S)\n\n#define CQE_OVFBIT_S\t62\n#define CQE_OVFBIT_M\t0x1\n#define CQE_OVFBIT_G(x)\t((((x) >> CQE_OVFBIT_S)) & CQE_OVFBIT_M)\n\n#define CQE_IQTYPE_S\t60\n#define CQE_IQTYPE_M\t0x3\n#define CQE_IQTYPE_G(x)\t((((x) >> CQE_IQTYPE_S)) & CQE_IQTYPE_M)\n\n#define CQE_TS_M\t0x0fffffffffffffffULL\n#define CQE_TS_G(x)\t((x) & CQE_TS_M)\n\n#define CQE_OVFBIT(x)\t((unsigned)CQE_OVFBIT_G(be64_to_cpu((x)->bits_type_ts)))\n#define CQE_GENBIT(x)\t((unsigned)CQE_GENBIT_G(be64_to_cpu((x)->bits_type_ts)))\n#define CQE_TS(x)\t(CQE_TS_G(be64_to_cpu((x)->bits_type_ts)))\n\nstruct t4_swsqe {\n\tu64\t\t\twr_id;\n\tstruct t4_cqe\t\tcqe;\n\tint\t\t\tread_len;\n\tint\t\t\topcode;\n\tint\t\t\tcomplete;\n\tint\t\t\tsignaled;\n\tu16\t\t\tidx;\n\tint                     flushed;\n\tktime_t\t\t\thost_time;\n\tu64                     sge_ts;\n};\n\nstatic inline pgprot_t t4_pgprot_wc(pgprot_t prot)\n{\n#if defined(__i386__) || defined(__x86_64__) || defined(CONFIG_PPC64)\n\treturn pgprot_writecombine(prot);\n#else\n\treturn pgprot_noncached(prot);\n#endif\n}\n\nenum {\n\tT4_SQ_ONCHIP = (1<<0),\n};\n\nstruct t4_sq {\n\tunion t4_wr *queue;\n\tdma_addr_t dma_addr;\n\tDEFINE_DMA_UNMAP_ADDR(mapping);\n\tunsigned long phys_addr;\n\tstruct t4_swsqe *sw_sq;\n\tstruct t4_swsqe *oldest_read;\n\tvoid __iomem *bar2_va;\n\tu64 bar2_pa;\n\tsize_t memsize;\n\tu32 bar2_qid;\n\tu32 qid;\n\tu16 in_use;\n\tu16 size;\n\tu16 cidx;\n\tu16 pidx;\n\tu16 wq_pidx;\n\tu16 wq_pidx_inc;\n\tu16 flags;\n\tshort flush_cidx;\n};\n\nstruct t4_swrqe {\n\tu64 wr_id;\n\tktime_t\thost_time;\n\tu64 sge_ts;\n\tint valid;\n};\n\nstruct t4_rq {\n\tunion  t4_recv_wr *queue;\n\tdma_addr_t dma_addr;\n\tDEFINE_DMA_UNMAP_ADDR(mapping);\n\tstruct t4_swrqe *sw_rq;\n\tvoid __iomem *bar2_va;\n\tu64 bar2_pa;\n\tsize_t memsize;\n\tu32 bar2_qid;\n\tu32 qid;\n\tu32 msn;\n\tu32 rqt_hwaddr;\n\tu16 rqt_size;\n\tu16 in_use;\n\tu16 size;\n\tu16 cidx;\n\tu16 pidx;\n\tu16 wq_pidx;\n\tu16 wq_pidx_inc;\n};\n\nstruct t4_wq {\n\tstruct t4_sq sq;\n\tstruct t4_rq rq;\n\tvoid __iomem *db;\n\tstruct c4iw_rdev *rdev;\n\tint flushed;\n\tu8 *qp_errp;\n\tu32 *srqidxp;\n};\n\nstruct t4_srq_pending_wr {\n\tu64 wr_id;\n\tunion t4_recv_wr wqe;\n\tu8 len16;\n};\n\nstruct t4_srq {\n\tunion t4_recv_wr *queue;\n\tdma_addr_t dma_addr;\n\tDEFINE_DMA_UNMAP_ADDR(mapping);\n\tstruct t4_swrqe *sw_rq;\n\tvoid __iomem *bar2_va;\n\tu64 bar2_pa;\n\tsize_t memsize;\n\tu32 bar2_qid;\n\tu32 qid;\n\tu32 msn;\n\tu32 rqt_hwaddr;\n\tu32 rqt_abs_idx;\n\tu16 rqt_size;\n\tu16 size;\n\tu16 cidx;\n\tu16 pidx;\n\tu16 wq_pidx;\n\tu16 wq_pidx_inc;\n\tu16 in_use;\n\tstruct t4_srq_pending_wr *pending_wrs;\n\tu16 pending_cidx;\n\tu16 pending_pidx;\n\tu16 pending_in_use;\n\tu16 ooo_count;\n};\n\nstatic inline u32 t4_srq_avail(struct t4_srq *srq)\n{\n\treturn srq->size - 1 - srq->in_use;\n}\n\nstatic inline void t4_srq_produce(struct t4_srq *srq, u8 len16)\n{\n\tsrq->in_use++;\n\tif (++srq->pidx == srq->size)\n\t\tsrq->pidx = 0;\n\tsrq->wq_pidx += DIV_ROUND_UP(len16 * 16, T4_EQ_ENTRY_SIZE);\n\tif (srq->wq_pidx >= srq->size * T4_RQ_NUM_SLOTS)\n\t\tsrq->wq_pidx %= srq->size * T4_RQ_NUM_SLOTS;\n\tsrq->queue[srq->size].status.host_pidx = srq->pidx;\n}\n\nstatic inline void t4_srq_produce_pending_wr(struct t4_srq *srq)\n{\n\tsrq->pending_in_use++;\n\tsrq->in_use++;\n\tif (++srq->pending_pidx == srq->size)\n\t\tsrq->pending_pidx = 0;\n}\n\nstatic inline void t4_srq_consume_pending_wr(struct t4_srq *srq)\n{\n\tsrq->pending_in_use--;\n\tsrq->in_use--;\n\tif (++srq->pending_cidx == srq->size)\n\t\tsrq->pending_cidx = 0;\n}\n\nstatic inline void t4_srq_produce_ooo(struct t4_srq *srq)\n{\n\tsrq->in_use--;\n\tsrq->ooo_count++;\n}\n\nstatic inline void t4_srq_consume_ooo(struct t4_srq *srq)\n{\n\tsrq->cidx++;\n\tif (srq->cidx == srq->size)\n\t\tsrq->cidx  = 0;\n\tsrq->queue[srq->size].status.host_cidx = srq->cidx;\n\tsrq->ooo_count--;\n}\n\nstatic inline void t4_srq_consume(struct t4_srq *srq)\n{\n\tsrq->in_use--;\n\tif (++srq->cidx == srq->size)\n\t\tsrq->cidx = 0;\n\tsrq->queue[srq->size].status.host_cidx = srq->cidx;\n}\n\nstatic inline int t4_rqes_posted(struct t4_wq *wq)\n{\n\treturn wq->rq.in_use;\n}\n\nstatic inline int t4_rq_empty(struct t4_wq *wq)\n{\n\treturn wq->rq.in_use == 0;\n}\n\nstatic inline u32 t4_rq_avail(struct t4_wq *wq)\n{\n\treturn wq->rq.size - 1 - wq->rq.in_use;\n}\n\nstatic inline void t4_rq_produce(struct t4_wq *wq, u8 len16)\n{\n\twq->rq.in_use++;\n\tif (++wq->rq.pidx == wq->rq.size)\n\t\twq->rq.pidx = 0;\n\twq->rq.wq_pidx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);\n\tif (wq->rq.wq_pidx >= wq->rq.size * T4_RQ_NUM_SLOTS)\n\t\twq->rq.wq_pidx %= wq->rq.size * T4_RQ_NUM_SLOTS;\n}\n\nstatic inline void t4_rq_consume(struct t4_wq *wq)\n{\n\twq->rq.in_use--;\n\tif (++wq->rq.cidx == wq->rq.size)\n\t\twq->rq.cidx = 0;\n}\n\nstatic inline u16 t4_rq_host_wq_pidx(struct t4_wq *wq)\n{\n\treturn wq->rq.queue[wq->rq.size].status.host_wq_pidx;\n}\n\nstatic inline u16 t4_rq_wq_size(struct t4_wq *wq)\n{\n\t\treturn wq->rq.size * T4_RQ_NUM_SLOTS;\n}\n\nstatic inline int t4_sq_onchip(struct t4_sq *sq)\n{\n\treturn sq->flags & T4_SQ_ONCHIP;\n}\n\nstatic inline int t4_sq_empty(struct t4_wq *wq)\n{\n\treturn wq->sq.in_use == 0;\n}\n\nstatic inline u32 t4_sq_avail(struct t4_wq *wq)\n{\n\treturn wq->sq.size - 1 - wq->sq.in_use;\n}\n\nstatic inline void t4_sq_produce(struct t4_wq *wq, u8 len16)\n{\n\twq->sq.in_use++;\n\tif (++wq->sq.pidx == wq->sq.size)\n\t\twq->sq.pidx = 0;\n\twq->sq.wq_pidx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);\n\tif (wq->sq.wq_pidx >= wq->sq.size * T4_SQ_NUM_SLOTS)\n\t\twq->sq.wq_pidx %= wq->sq.size * T4_SQ_NUM_SLOTS;\n}\n\nstatic inline void t4_sq_consume(struct t4_wq *wq)\n{\n\tif (wq->sq.cidx == wq->sq.flush_cidx)\n\t\twq->sq.flush_cidx = -1;\n\twq->sq.in_use--;\n\tif (++wq->sq.cidx == wq->sq.size)\n\t\twq->sq.cidx = 0;\n}\n\nstatic inline u16 t4_sq_host_wq_pidx(struct t4_wq *wq)\n{\n\treturn wq->sq.queue[wq->sq.size].status.host_wq_pidx;\n}\n\nstatic inline u16 t4_sq_wq_size(struct t4_wq *wq)\n{\n\t\treturn wq->sq.size * T4_SQ_NUM_SLOTS;\n}\n\n \nstatic inline void pio_copy(u64 __iomem *dst, u64 *src)\n{\n\tint count = 8;\n\n\twhile (count) {\n\t\twriteq(*src, dst);\n\t\tsrc++;\n\t\tdst++;\n\t\tcount--;\n\t}\n}\n\nstatic inline void t4_ring_srq_db(struct t4_srq *srq, u16 inc, u8 len16,\n\t\t\t\t  union t4_recv_wr *wqe)\n{\n\t \n\twmb();\n\tif (inc == 1 && srq->bar2_qid == 0 && wqe) {\n\t\tpr_debug(\"%s : WC srq->pidx = %d; len16=%d\\n\",\n\t\t\t __func__, srq->pidx, len16);\n\t\tpio_copy(srq->bar2_va + SGE_UDB_WCDOORBELL, (u64 *)wqe);\n\t} else {\n\t\tpr_debug(\"%s: DB srq->pidx = %d; len16=%d\\n\",\n\t\t\t __func__, srq->pidx, len16);\n\t\twritel(PIDX_T5_V(inc) | QID_V(srq->bar2_qid),\n\t\t       srq->bar2_va + SGE_UDB_KDOORBELL);\n\t}\n\t \n\twmb();\n}\n\nstatic inline void t4_ring_sq_db(struct t4_wq *wq, u16 inc, union t4_wr *wqe)\n{\n\n\t \n\twmb();\n\tif (wq->sq.bar2_va) {\n\t\tif (inc == 1 && wq->sq.bar2_qid == 0 && wqe) {\n\t\t\tpr_debug(\"WC wq->sq.pidx = %d\\n\", wq->sq.pidx);\n\t\t\tpio_copy((u64 __iomem *)\n\t\t\t\t (wq->sq.bar2_va + SGE_UDB_WCDOORBELL),\n\t\t\t\t (u64 *)wqe);\n\t\t} else {\n\t\t\tpr_debug(\"DB wq->sq.pidx = %d\\n\", wq->sq.pidx);\n\t\t\twritel(PIDX_T5_V(inc) | QID_V(wq->sq.bar2_qid),\n\t\t\t       wq->sq.bar2_va + SGE_UDB_KDOORBELL);\n\t\t}\n\n\t\t \n\t\twmb();\n\t\treturn;\n\t}\n\twritel(QID_V(wq->sq.qid) | PIDX_V(inc), wq->db);\n}\n\nstatic inline void t4_ring_rq_db(struct t4_wq *wq, u16 inc,\n\t\t\t\t union t4_recv_wr *wqe)\n{\n\n\t \n\twmb();\n\tif (wq->rq.bar2_va) {\n\t\tif (inc == 1 && wq->rq.bar2_qid == 0 && wqe) {\n\t\t\tpr_debug(\"WC wq->rq.pidx = %d\\n\", wq->rq.pidx);\n\t\t\tpio_copy((u64 __iomem *)\n\t\t\t\t (wq->rq.bar2_va + SGE_UDB_WCDOORBELL),\n\t\t\t\t (void *)wqe);\n\t\t} else {\n\t\t\tpr_debug(\"DB wq->rq.pidx = %d\\n\", wq->rq.pidx);\n\t\t\twritel(PIDX_T5_V(inc) | QID_V(wq->rq.bar2_qid),\n\t\t\t       wq->rq.bar2_va + SGE_UDB_KDOORBELL);\n\t\t}\n\n\t\t \n\t\twmb();\n\t\treturn;\n\t}\n\twritel(QID_V(wq->rq.qid) | PIDX_V(inc), wq->db);\n}\n\nstatic inline int t4_wq_in_error(struct t4_wq *wq)\n{\n\treturn *wq->qp_errp;\n}\n\nstatic inline void t4_set_wq_in_error(struct t4_wq *wq, u32 srqidx)\n{\n\tif (srqidx)\n\t\t*wq->srqidxp = srqidx;\n\t*wq->qp_errp = 1;\n}\n\nstatic inline void t4_disable_wq_db(struct t4_wq *wq)\n{\n\twq->rq.queue[wq->rq.size].status.db_off = 1;\n}\n\nstatic inline void t4_enable_wq_db(struct t4_wq *wq)\n{\n\twq->rq.queue[wq->rq.size].status.db_off = 0;\n}\n\nenum t4_cq_flags {\n\tCQ_ARMED\t= 1,\n};\n\nstruct t4_cq {\n\tstruct t4_cqe *queue;\n\tdma_addr_t dma_addr;\n\tDEFINE_DMA_UNMAP_ADDR(mapping);\n\tstruct t4_cqe *sw_queue;\n\tvoid __iomem *gts;\n\tvoid __iomem *bar2_va;\n\tu64 bar2_pa;\n\tu32 bar2_qid;\n\tstruct c4iw_rdev *rdev;\n\tsize_t memsize;\n\t__be64 bits_type_ts;\n\tu32 cqid;\n\tu32 qid_mask;\n\tint vector;\n\tu16 size;  \n\tu16 cidx;\n\tu16 sw_pidx;\n\tu16 sw_cidx;\n\tu16 sw_in_use;\n\tu16 cidx_inc;\n\tu8 gen;\n\tu8 error;\n\tu8 *qp_errp;\n\tunsigned long flags;\n};\n\nstatic inline void write_gts(struct t4_cq *cq, u32 val)\n{\n\tif (cq->bar2_va)\n\t\twritel(val | INGRESSQID_V(cq->bar2_qid),\n\t\t       cq->bar2_va + SGE_UDB_GTS);\n\telse\n\t\twritel(val | INGRESSQID_V(cq->cqid), cq->gts);\n}\n\nstatic inline int t4_clear_cq_armed(struct t4_cq *cq)\n{\n\treturn test_and_clear_bit(CQ_ARMED, &cq->flags);\n}\n\nstatic inline int t4_arm_cq(struct t4_cq *cq, int se)\n{\n\tu32 val;\n\n\tset_bit(CQ_ARMED, &cq->flags);\n\twhile (cq->cidx_inc > CIDXINC_M) {\n\t\tval = SEINTARM_V(0) | CIDXINC_V(CIDXINC_M) | TIMERREG_V(7);\n\t\twrite_gts(cq, val);\n\t\tcq->cidx_inc -= CIDXINC_M;\n\t}\n\tval = SEINTARM_V(se) | CIDXINC_V(cq->cidx_inc) | TIMERREG_V(6);\n\twrite_gts(cq, val);\n\tcq->cidx_inc = 0;\n\treturn 0;\n}\n\nstatic inline void t4_swcq_produce(struct t4_cq *cq)\n{\n\tcq->sw_in_use++;\n\tif (cq->sw_in_use == cq->size) {\n\t\tpr_warn(\"%s cxgb4 sw cq overflow cqid %u\\n\",\n\t\t\t__func__, cq->cqid);\n\t\tcq->error = 1;\n\t\tcq->sw_in_use--;\n\t\treturn;\n\t}\n\tif (++cq->sw_pidx == cq->size)\n\t\tcq->sw_pidx = 0;\n}\n\nstatic inline void t4_swcq_consume(struct t4_cq *cq)\n{\n\tcq->sw_in_use--;\n\tif (++cq->sw_cidx == cq->size)\n\t\tcq->sw_cidx = 0;\n}\n\nstatic inline void t4_hwcq_consume(struct t4_cq *cq)\n{\n\tcq->bits_type_ts = cq->queue[cq->cidx].bits_type_ts;\n\tif (++cq->cidx_inc == (cq->size >> 4) || cq->cidx_inc == CIDXINC_M) {\n\t\tu32 val;\n\n\t\tval = SEINTARM_V(0) | CIDXINC_V(cq->cidx_inc) | TIMERREG_V(7);\n\t\twrite_gts(cq, val);\n\t\tcq->cidx_inc = 0;\n\t}\n\tif (++cq->cidx == cq->size) {\n\t\tcq->cidx = 0;\n\t\tcq->gen ^= 1;\n\t}\n}\n\nstatic inline int t4_valid_cqe(struct t4_cq *cq, struct t4_cqe *cqe)\n{\n\treturn (CQE_GENBIT(cqe) == cq->gen);\n}\n\nstatic inline int t4_cq_notempty(struct t4_cq *cq)\n{\n\treturn cq->sw_in_use || t4_valid_cqe(cq, &cq->queue[cq->cidx]);\n}\n\nstatic inline int t4_next_hw_cqe(struct t4_cq *cq, struct t4_cqe **cqe)\n{\n\tint ret;\n\tu16 prev_cidx;\n\n\tif (cq->cidx == 0)\n\t\tprev_cidx = cq->size - 1;\n\telse\n\t\tprev_cidx = cq->cidx - 1;\n\n\tif (cq->queue[prev_cidx].bits_type_ts != cq->bits_type_ts) {\n\t\tret = -EOVERFLOW;\n\t\tcq->error = 1;\n\t\tpr_err(\"cq overflow cqid %u\\n\", cq->cqid);\n\t} else if (t4_valid_cqe(cq, &cq->queue[cq->cidx])) {\n\n\t\t \n\t\trmb();\n\t\t*cqe = &cq->queue[cq->cidx];\n\t\tret = 0;\n\t} else\n\t\tret = -ENODATA;\n\treturn ret;\n}\n\nstatic inline int t4_next_cqe(struct t4_cq *cq, struct t4_cqe **cqe)\n{\n\tint ret = 0;\n\n\tif (cq->error)\n\t\tret = -ENODATA;\n\telse if (cq->sw_in_use)\n\t\t*cqe = &cq->sw_queue[cq->sw_cidx];\n\telse\n\t\tret = t4_next_hw_cqe(cq, cqe);\n\treturn ret;\n}\n\nstatic inline void t4_set_cq_in_error(struct t4_cq *cq)\n{\n\t*cq->qp_errp = 1;\n}\n#endif\n\nstruct t4_dev_status_page {\n\tu8 db_off;\n\tu8 write_cmpl_supported;\n\tu16 pad2;\n\tu32 pad3;\n\tu64 qp_start;\n\tu64 qp_size;\n\tu64 cq_start;\n\tu64 cq_size;\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}