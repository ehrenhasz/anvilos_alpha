{
  "module_name": "iw_cxgb4.h",
  "hash_id": "07785363a5a43322cc162ff8e2bb5d30048b4039a72f28c9693bf1612bb3b337",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/iw_cxgb4.h",
  "human_readable_source": " \n#ifndef __IW_CXGB4_H__\n#define __IW_CXGB4_H__\n\n#include <linux/mutex.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/xarray.h>\n#include <linux/completion.h>\n#include <linux/netdevice.h>\n#include <linux/sched/mm.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/inet.h>\n#include <linux/wait.h>\n#include <linux/kref.h>\n#include <linux/timer.h>\n#include <linux/io.h>\n#include <linux/workqueue.h>\n\n#include <asm/byteorder.h>\n\n#include <net/net_namespace.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/iw_cm.h>\n#include <rdma/rdma_netlink.h>\n#include <rdma/iw_portmap.h>\n#include <rdma/restrack.h>\n\n#include \"cxgb4.h\"\n#include \"cxgb4_uld.h\"\n#include \"l2t.h\"\n#include <rdma/cxgb4-abi.h>\n\n#define DRV_NAME \"iw_cxgb4\"\n#define MOD DRV_NAME \":\"\n\n#ifdef pr_fmt\n#undef pr_fmt\n#endif\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"t4.h\"\n\n#define PBL_OFF(rdev_p, a) ((a) - (rdev_p)->lldi.vr->pbl.start)\n#define RQT_OFF(rdev_p, a) ((a) - (rdev_p)->lldi.vr->rq.start)\n\nstatic inline void *cplhdr(struct sk_buff *skb)\n{\n\treturn skb->data;\n}\n\n#define C4IW_ID_TABLE_F_RANDOM 1        \n#define C4IW_ID_TABLE_F_EMPTY  2        \n\nstruct c4iw_id_table {\n\tu32 flags;\n\tu32 start;               \n\tu32 last;                \n\tu32 max;\n\tspinlock_t lock;\n\tunsigned long *table;\n};\n\nstruct c4iw_resource {\n\tstruct c4iw_id_table tpt_table;\n\tstruct c4iw_id_table qid_table;\n\tstruct c4iw_id_table pdid_table;\n\tstruct c4iw_id_table srq_table;\n};\n\nstruct c4iw_qid_list {\n\tstruct list_head entry;\n\tu32 qid;\n};\n\nstruct c4iw_dev_ucontext {\n\tstruct list_head qpids;\n\tstruct list_head cqids;\n\tstruct mutex lock;\n\tstruct kref kref;\n};\n\nenum c4iw_rdev_flags {\n\tT4_FATAL_ERROR = (1<<0),\n\tT4_STATUS_PAGE_DISABLED = (1<<1),\n};\n\nstruct c4iw_stat {\n\tu64 total;\n\tu64 cur;\n\tu64 max;\n\tu64 fail;\n};\n\nstruct c4iw_stats {\n\tstruct mutex lock;\n\tstruct c4iw_stat qid;\n\tstruct c4iw_stat pd;\n\tstruct c4iw_stat stag;\n\tstruct c4iw_stat pbl;\n\tstruct c4iw_stat rqt;\n\tstruct c4iw_stat srqt;\n\tstruct c4iw_stat srq;\n\tstruct c4iw_stat ocqp;\n\tu64  db_full;\n\tu64  db_empty;\n\tu64  db_drop;\n\tu64  db_state_transitions;\n\tu64  db_fc_interruptions;\n\tu64  tcam_full;\n\tu64  act_ofld_conn_fails;\n\tu64  pas_ofld_conn_fails;\n\tu64  neg_adv;\n};\n\nstruct c4iw_hw_queue {\n\tint t4_eq_status_entries;\n\tint t4_max_eq_size;\n\tint t4_max_iq_size;\n\tint t4_max_rq_size;\n\tint t4_max_sq_size;\n\tint t4_max_qp_depth;\n\tint t4_max_cq_depth;\n\tint t4_stat_len;\n};\n\nstruct wr_log_entry {\n\tktime_t post_host_time;\n\tktime_t poll_host_time;\n\tu64 post_sge_ts;\n\tu64 cqe_sge_ts;\n\tu64 poll_sge_ts;\n\tu16 qid;\n\tu16 wr_id;\n\tu8 opcode;\n\tu8 valid;\n};\n\nstruct c4iw_rdev {\n\tstruct c4iw_resource resource;\n\tu32 qpmask;\n\tu32 cqmask;\n\tstruct c4iw_dev_ucontext uctx;\n\tstruct gen_pool *pbl_pool;\n\tstruct gen_pool *rqt_pool;\n\tstruct gen_pool *ocqp_pool;\n\tu32 flags;\n\tstruct cxgb4_lld_info lldi;\n\tunsigned long bar2_pa;\n\tvoid __iomem *bar2_kva;\n\tunsigned long oc_mw_pa;\n\tvoid __iomem *oc_mw_kva;\n\tstruct c4iw_stats stats;\n\tstruct c4iw_hw_queue hw_queue;\n\tstruct t4_dev_status_page *status_page;\n\tatomic_t wr_log_idx;\n\tstruct wr_log_entry *wr_log;\n\tint wr_log_size;\n\tstruct workqueue_struct *free_workq;\n\tstruct completion rqt_compl;\n\tstruct completion pbl_compl;\n\tstruct kref rqt_kref;\n\tstruct kref pbl_kref;\n};\n\nstatic inline int c4iw_fatal_error(struct c4iw_rdev *rdev)\n{\n\treturn rdev->flags & T4_FATAL_ERROR;\n}\n\nstatic inline int c4iw_num_stags(struct c4iw_rdev *rdev)\n{\n\treturn (int)(rdev->lldi.vr->stag.size >> 5);\n}\n\n#define C4IW_WR_TO (60*HZ)\n\nstruct c4iw_wr_wait {\n\tstruct completion completion;\n\tint ret;\n\tstruct kref kref;\n};\n\nvoid _c4iw_free_wr_wait(struct kref *kref);\n\nstatic inline void c4iw_put_wr_wait(struct c4iw_wr_wait *wr_waitp)\n{\n\tpr_debug(\"wr_wait %p ref before put %u\\n\", wr_waitp,\n\t\t kref_read(&wr_waitp->kref));\n\tWARN_ON(kref_read(&wr_waitp->kref) == 0);\n\tkref_put(&wr_waitp->kref, _c4iw_free_wr_wait);\n}\n\nstatic inline void c4iw_get_wr_wait(struct c4iw_wr_wait *wr_waitp)\n{\n\tpr_debug(\"wr_wait %p ref before get %u\\n\", wr_waitp,\n\t\t kref_read(&wr_waitp->kref));\n\tWARN_ON(kref_read(&wr_waitp->kref) == 0);\n\tkref_get(&wr_waitp->kref);\n}\n\nstatic inline void c4iw_init_wr_wait(struct c4iw_wr_wait *wr_waitp)\n{\n\twr_waitp->ret = 0;\n\tinit_completion(&wr_waitp->completion);\n}\n\nstatic inline void _c4iw_wake_up(struct c4iw_wr_wait *wr_waitp, int ret,\n\t\t\t\t bool deref)\n{\n\twr_waitp->ret = ret;\n\tcomplete(&wr_waitp->completion);\n\tif (deref)\n\t\tc4iw_put_wr_wait(wr_waitp);\n}\n\nstatic inline void c4iw_wake_up_noref(struct c4iw_wr_wait *wr_waitp, int ret)\n{\n\t_c4iw_wake_up(wr_waitp, ret, false);\n}\n\nstatic inline void c4iw_wake_up_deref(struct c4iw_wr_wait *wr_waitp, int ret)\n{\n\t_c4iw_wake_up(wr_waitp, ret, true);\n}\n\nstatic inline int c4iw_wait_for_reply(struct c4iw_rdev *rdev,\n\t\t\t\t struct c4iw_wr_wait *wr_waitp,\n\t\t\t\t u32 hwtid, u32 qpid,\n\t\t\t\t const char *func)\n{\n\tint ret;\n\n\tif (c4iw_fatal_error(rdev)) {\n\t\twr_waitp->ret = -EIO;\n\t\tgoto out;\n\t}\n\n\tret = wait_for_completion_timeout(&wr_waitp->completion, C4IW_WR_TO);\n\tif (!ret) {\n\t\tpr_err(\"%s - Device %s not responding (disabling device) - tid %u qpid %u\\n\",\n\t\t       func, pci_name(rdev->lldi.pdev), hwtid, qpid);\n\t\trdev->flags |= T4_FATAL_ERROR;\n\t\twr_waitp->ret = -EIO;\n\t\tgoto out;\n\t}\n\tif (wr_waitp->ret)\n\t\tpr_debug(\"%s: FW reply %d tid %u qpid %u\\n\",\n\t\t\t pci_name(rdev->lldi.pdev), wr_waitp->ret, hwtid, qpid);\nout:\n\treturn wr_waitp->ret;\n}\n\nint c4iw_ofld_send(struct c4iw_rdev *rdev, struct sk_buff *skb);\n\nstatic inline int c4iw_ref_send_wait(struct c4iw_rdev *rdev,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     struct c4iw_wr_wait *wr_waitp,\n\t\t\t\t     u32 hwtid, u32 qpid,\n\t\t\t\t     const char *func)\n{\n\tint ret;\n\n\tpr_debug(\"%s wr_wait %p hwtid %u qpid %u\\n\", func, wr_waitp, hwtid,\n\t\t qpid);\n\tc4iw_get_wr_wait(wr_waitp);\n\tret = c4iw_ofld_send(rdev, skb);\n\tif (ret) {\n\t\tc4iw_put_wr_wait(wr_waitp);\n\t\treturn ret;\n\t}\n\treturn c4iw_wait_for_reply(rdev, wr_waitp, hwtid, qpid, func);\n}\n\nenum db_state {\n\tNORMAL = 0,\n\tFLOW_CONTROL = 1,\n\tRECOVERY = 2,\n\tSTOPPED = 3\n};\n\nstruct c4iw_dev {\n\tstruct ib_device ibdev;\n\tstruct c4iw_rdev rdev;\n\tstruct xarray cqs;\n\tstruct xarray qps;\n\tstruct xarray mrs;\n\tstruct mutex db_mutex;\n\tstruct dentry *debugfs_root;\n\tenum db_state db_state;\n\tstruct xarray hwtids;\n\tstruct xarray atids;\n\tstruct xarray stids;\n\tstruct list_head db_fc_list;\n\tu32 avail_ird;\n\twait_queue_head_t wait;\n};\n\nstruct uld_ctx {\n\tstruct list_head entry;\n\tstruct cxgb4_lld_info lldi;\n\tstruct c4iw_dev *dev;\n\tstruct work_struct reg_work;\n};\n\nstatic inline struct c4iw_dev *to_c4iw_dev(struct ib_device *ibdev)\n{\n\treturn container_of(ibdev, struct c4iw_dev, ibdev);\n}\n\nstatic inline struct c4iw_cq *get_chp(struct c4iw_dev *rhp, u32 cqid)\n{\n\treturn xa_load(&rhp->cqs, cqid);\n}\n\nstatic inline struct c4iw_qp *get_qhp(struct c4iw_dev *rhp, u32 qpid)\n{\n\treturn xa_load(&rhp->qps, qpid);\n}\n\nextern uint c4iw_max_read_depth;\n\nstatic inline int cur_max_read_depth(struct c4iw_dev *dev)\n{\n\treturn min(dev->rdev.lldi.max_ordird_qp, c4iw_max_read_depth);\n}\n\nstruct c4iw_pd {\n\tstruct ib_pd ibpd;\n\tu32 pdid;\n\tstruct c4iw_dev *rhp;\n};\n\nstatic inline struct c4iw_pd *to_c4iw_pd(struct ib_pd *ibpd)\n{\n\treturn container_of(ibpd, struct c4iw_pd, ibpd);\n}\n\nstruct tpt_attributes {\n\tu64 len;\n\tu64 va_fbo;\n\tenum fw_ri_mem_perms perms;\n\tu32 stag;\n\tu32 pdid;\n\tu32 qpid;\n\tu32 pbl_addr;\n\tu32 pbl_size;\n\tu32 state:1;\n\tu32 type:2;\n\tu32 rsvd:1;\n\tu32 remote_invaliate_disable:1;\n\tu32 zbva:1;\n\tu32 mw_bind_enable:1;\n\tu32 page_size:5;\n};\n\nstruct c4iw_mr {\n\tstruct ib_mr ibmr;\n\tstruct ib_umem *umem;\n\tstruct c4iw_dev *rhp;\n\tstruct sk_buff *dereg_skb;\n\tu64 kva;\n\tstruct tpt_attributes attr;\n\tu64 *mpl;\n\tdma_addr_t mpl_addr;\n\tu32 max_mpl_len;\n\tu32 mpl_len;\n\tstruct c4iw_wr_wait *wr_waitp;\n};\n\nstatic inline struct c4iw_mr *to_c4iw_mr(struct ib_mr *ibmr)\n{\n\treturn container_of(ibmr, struct c4iw_mr, ibmr);\n}\n\nstruct c4iw_mw {\n\tstruct ib_mw ibmw;\n\tstruct c4iw_dev *rhp;\n\tstruct sk_buff *dereg_skb;\n\tu64 kva;\n\tstruct tpt_attributes attr;\n\tstruct c4iw_wr_wait *wr_waitp;\n};\n\nstatic inline struct c4iw_mw *to_c4iw_mw(struct ib_mw *ibmw)\n{\n\treturn container_of(ibmw, struct c4iw_mw, ibmw);\n}\n\nstruct c4iw_cq {\n\tstruct ib_cq ibcq;\n\tstruct c4iw_dev *rhp;\n\tstruct sk_buff *destroy_skb;\n\tstruct t4_cq cq;\n\tspinlock_t lock;\n\tspinlock_t comp_handler_lock;\n\trefcount_t refcnt;\n\tstruct completion cq_rel_comp;\n\tstruct c4iw_wr_wait *wr_waitp;\n};\n\nstatic inline struct c4iw_cq *to_c4iw_cq(struct ib_cq *ibcq)\n{\n\treturn container_of(ibcq, struct c4iw_cq, ibcq);\n}\n\nstruct c4iw_mpa_attributes {\n\tu8 initiator;\n\tu8 recv_marker_enabled;\n\tu8 xmit_marker_enabled;\n\tu8 crc_enabled;\n\tu8 enhanced_rdma_conn;\n\tu8 version;\n\tu8 p2p_type;\n};\n\nstruct c4iw_qp_attributes {\n\tu32 scq;\n\tu32 rcq;\n\tu32 sq_num_entries;\n\tu32 rq_num_entries;\n\tu32 sq_max_sges;\n\tu32 sq_max_sges_rdma_write;\n\tu32 rq_max_sges;\n\tu32 state;\n\tu8 enable_rdma_read;\n\tu8 enable_rdma_write;\n\tu8 enable_bind;\n\tu8 enable_mmid0_fastreg;\n\tu32 max_ord;\n\tu32 max_ird;\n\tu32 pd;\n\tu32 next_state;\n\tchar terminate_buffer[52];\n\tu32 terminate_msg_len;\n\tu8 is_terminate_local;\n\tstruct c4iw_mpa_attributes mpa_attr;\n\tstruct c4iw_ep *llp_stream_handle;\n\tu8 layer_etype;\n\tu8 ecode;\n\tu16 sq_db_inc;\n\tu16 rq_db_inc;\n\tu8 send_term;\n};\n\nstruct c4iw_qp {\n\tstruct ib_qp ibqp;\n\tstruct list_head db_fc_entry;\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_ep *ep;\n\tstruct c4iw_qp_attributes attr;\n\tstruct t4_wq wq;\n\tspinlock_t lock;\n\tstruct mutex mutex;\n\twait_queue_head_t wait;\n\tint sq_sig_all;\n\tstruct c4iw_srq *srq;\n\tstruct c4iw_ucontext *ucontext;\n\tstruct c4iw_wr_wait *wr_waitp;\n\tstruct completion qp_rel_comp;\n\trefcount_t qp_refcnt;\n};\n\nstatic inline struct c4iw_qp *to_c4iw_qp(struct ib_qp *ibqp)\n{\n\treturn container_of(ibqp, struct c4iw_qp, ibqp);\n}\n\nstruct c4iw_srq {\n\tstruct ib_srq ibsrq;\n\tstruct list_head db_fc_entry;\n\tstruct c4iw_dev *rhp;\n\tstruct t4_srq wq;\n\tstruct sk_buff *destroy_skb;\n\tu32 srq_limit;\n\tu32 pdid;\n\tint idx;\n\tu32 flags;\n\tspinlock_t lock;  \n\tstruct c4iw_wr_wait *wr_waitp;\n\tbool armed;\n};\n\nstatic inline struct c4iw_srq *to_c4iw_srq(struct ib_srq *ibsrq)\n{\n\treturn container_of(ibsrq, struct c4iw_srq, ibsrq);\n}\n\nstruct c4iw_ucontext {\n\tstruct ib_ucontext ibucontext;\n\tstruct c4iw_dev_ucontext uctx;\n\tu32 key;\n\tspinlock_t mmap_lock;\n\tstruct list_head mmaps;\n\tbool is_32b_cqe;\n};\n\nstatic inline struct c4iw_ucontext *to_c4iw_ucontext(struct ib_ucontext *c)\n{\n\treturn container_of(c, struct c4iw_ucontext, ibucontext);\n}\n\nstruct c4iw_mm_entry {\n\tstruct list_head entry;\n\tu64 addr;\n\tu32 key;\n\tunsigned len;\n};\n\nstatic inline struct c4iw_mm_entry *remove_mmap(struct c4iw_ucontext *ucontext,\n\t\t\t\t\t\tu32 key, unsigned len)\n{\n\tstruct list_head *pos, *nxt;\n\tstruct c4iw_mm_entry *mm;\n\n\tspin_lock(&ucontext->mmap_lock);\n\tlist_for_each_safe(pos, nxt, &ucontext->mmaps) {\n\n\t\tmm = list_entry(pos, struct c4iw_mm_entry, entry);\n\t\tif (mm->key == key && mm->len == len) {\n\t\t\tlist_del_init(&mm->entry);\n\t\t\tspin_unlock(&ucontext->mmap_lock);\n\t\t\tpr_debug(\"key 0x%x addr 0x%llx len %d\\n\", key,\n\t\t\t\t (unsigned long long)mm->addr, mm->len);\n\t\t\treturn mm;\n\t\t}\n\t}\n\tspin_unlock(&ucontext->mmap_lock);\n\treturn NULL;\n}\n\nstatic inline void insert_mmap(struct c4iw_ucontext *ucontext,\n\t\t\t       struct c4iw_mm_entry *mm)\n{\n\tspin_lock(&ucontext->mmap_lock);\n\tpr_debug(\"key 0x%x addr 0x%llx len %d\\n\",\n\t\t mm->key, (unsigned long long)mm->addr, mm->len);\n\tlist_add_tail(&mm->entry, &ucontext->mmaps);\n\tspin_unlock(&ucontext->mmap_lock);\n}\n\nenum c4iw_qp_attr_mask {\n\tC4IW_QP_ATTR_NEXT_STATE = 1 << 0,\n\tC4IW_QP_ATTR_SQ_DB = 1<<1,\n\tC4IW_QP_ATTR_RQ_DB = 1<<2,\n\tC4IW_QP_ATTR_ENABLE_RDMA_READ = 1 << 7,\n\tC4IW_QP_ATTR_ENABLE_RDMA_WRITE = 1 << 8,\n\tC4IW_QP_ATTR_ENABLE_RDMA_BIND = 1 << 9,\n\tC4IW_QP_ATTR_MAX_ORD = 1 << 11,\n\tC4IW_QP_ATTR_MAX_IRD = 1 << 12,\n\tC4IW_QP_ATTR_LLP_STREAM_HANDLE = 1 << 22,\n\tC4IW_QP_ATTR_STREAM_MSG_BUFFER = 1 << 23,\n\tC4IW_QP_ATTR_MPA_ATTR = 1 << 24,\n\tC4IW_QP_ATTR_QP_CONTEXT_ACTIVATE = 1 << 25,\n\tC4IW_QP_ATTR_VALID_MODIFY = (C4IW_QP_ATTR_ENABLE_RDMA_READ |\n\t\t\t\t     C4IW_QP_ATTR_ENABLE_RDMA_WRITE |\n\t\t\t\t     C4IW_QP_ATTR_MAX_ORD |\n\t\t\t\t     C4IW_QP_ATTR_MAX_IRD |\n\t\t\t\t     C4IW_QP_ATTR_LLP_STREAM_HANDLE |\n\t\t\t\t     C4IW_QP_ATTR_STREAM_MSG_BUFFER |\n\t\t\t\t     C4IW_QP_ATTR_MPA_ATTR |\n\t\t\t\t     C4IW_QP_ATTR_QP_CONTEXT_ACTIVATE)\n};\n\nint c4iw_modify_qp(struct c4iw_dev *rhp,\n\t\t\t\tstruct c4iw_qp *qhp,\n\t\t\t\tenum c4iw_qp_attr_mask mask,\n\t\t\t\tstruct c4iw_qp_attributes *attrs,\n\t\t\t\tint internal);\n\nenum c4iw_qp_state {\n\tC4IW_QP_STATE_IDLE,\n\tC4IW_QP_STATE_RTS,\n\tC4IW_QP_STATE_ERROR,\n\tC4IW_QP_STATE_TERMINATE,\n\tC4IW_QP_STATE_CLOSING,\n\tC4IW_QP_STATE_TOT\n};\n\nstatic inline int c4iw_convert_state(enum ib_qp_state ib_state)\n{\n\tswitch (ib_state) {\n\tcase IB_QPS_RESET:\n\tcase IB_QPS_INIT:\n\t\treturn C4IW_QP_STATE_IDLE;\n\tcase IB_QPS_RTS:\n\t\treturn C4IW_QP_STATE_RTS;\n\tcase IB_QPS_SQD:\n\t\treturn C4IW_QP_STATE_CLOSING;\n\tcase IB_QPS_SQE:\n\t\treturn C4IW_QP_STATE_TERMINATE;\n\tcase IB_QPS_ERR:\n\t\treturn C4IW_QP_STATE_ERROR;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\nstatic inline int to_ib_qp_state(int c4iw_qp_state)\n{\n\tswitch (c4iw_qp_state) {\n\tcase C4IW_QP_STATE_IDLE:\n\t\treturn IB_QPS_INIT;\n\tcase C4IW_QP_STATE_RTS:\n\t\treturn IB_QPS_RTS;\n\tcase C4IW_QP_STATE_CLOSING:\n\t\treturn IB_QPS_SQD;\n\tcase C4IW_QP_STATE_TERMINATE:\n\t\treturn IB_QPS_SQE;\n\tcase C4IW_QP_STATE_ERROR:\n\t\treturn IB_QPS_ERR;\n\t}\n\treturn IB_QPS_ERR;\n}\n\nstatic inline u32 c4iw_ib_to_tpt_access(int a)\n{\n\treturn (a & IB_ACCESS_REMOTE_WRITE ? FW_RI_MEM_ACCESS_REM_WRITE : 0) |\n\t       (a & IB_ACCESS_REMOTE_READ ? FW_RI_MEM_ACCESS_REM_READ : 0) |\n\t       (a & IB_ACCESS_LOCAL_WRITE ? FW_RI_MEM_ACCESS_LOCAL_WRITE : 0) |\n\t       FW_RI_MEM_ACCESS_LOCAL_READ;\n}\n\nenum c4iw_mmid_state {\n\tC4IW_STAG_STATE_VALID,\n\tC4IW_STAG_STATE_INVALID\n};\n\n#define C4IW_NODE_DESC \"cxgb4 Chelsio Communications\"\n\n#define MPA_KEY_REQ \"MPA ID Req Frame\"\n#define MPA_KEY_REP \"MPA ID Rep Frame\"\n\n#define MPA_MAX_PRIVATE_DATA\t256\n#define MPA_ENHANCED_RDMA_CONN\t0x10\n#define MPA_REJECT\t\t0x20\n#define MPA_CRC\t\t\t0x40\n#define MPA_MARKERS\t\t0x80\n#define MPA_FLAGS_MASK\t\t0xE0\n\n#define MPA_V2_PEER2PEER_MODEL          0x8000\n#define MPA_V2_ZERO_LEN_FPDU_RTR        0x4000\n#define MPA_V2_RDMA_WRITE_RTR           0x8000\n#define MPA_V2_RDMA_READ_RTR            0x4000\n#define MPA_V2_IRD_ORD_MASK             0x3FFF\n\n#define c4iw_put_ep(ep) {\t\t\t\t\t\t\\\n\tpr_debug(\"put_ep ep %p refcnt %d\\n\",\t\t\\\n\t\t ep, kref_read(&((ep)->kref)));\t\t\t\t\\\n\tWARN_ON(kref_read(&((ep)->kref)) < 1);\t\t\t\t\\\n\tkref_put(&((ep)->kref), _c4iw_free_ep);\t\t\t\t\\\n}\n\n#define c4iw_get_ep(ep) {\t\t\t\t\t\t\\\n\tpr_debug(\"get_ep ep %p, refcnt %d\\n\",\t\t\\\n\t\t ep, kref_read(&((ep)->kref)));\t\t\t\t\\\n\tkref_get(&((ep)->kref));\t\t\t\t\t\\\n}\nvoid _c4iw_free_ep(struct kref *kref);\n\nstruct mpa_message {\n\tu8 key[16];\n\tu8 flags;\n\tu8 revision;\n\t__be16 private_data_size;\n\tu8 private_data[];\n};\n\nstruct mpa_v2_conn_params {\n\t__be16 ird;\n\t__be16 ord;\n};\n\nstruct terminate_message {\n\tu8 layer_etype;\n\tu8 ecode;\n\t__be16 hdrct_rsvd;\n\tu8 len_hdrs[];\n};\n\n#define TERM_MAX_LENGTH (sizeof(struct terminate_message) + 2 + 18 + 28)\n\nenum c4iw_layers_types {\n\tLAYER_RDMAP\t\t= 0x00,\n\tLAYER_DDP\t\t= 0x10,\n\tLAYER_MPA\t\t= 0x20,\n\tRDMAP_LOCAL_CATA\t= 0x00,\n\tRDMAP_REMOTE_PROT\t= 0x01,\n\tRDMAP_REMOTE_OP\t\t= 0x02,\n\tDDP_LOCAL_CATA\t\t= 0x00,\n\tDDP_TAGGED_ERR\t\t= 0x01,\n\tDDP_UNTAGGED_ERR\t= 0x02,\n\tDDP_LLP\t\t\t= 0x03\n};\n\nenum c4iw_rdma_ecodes {\n\tRDMAP_INV_STAG\t\t= 0x00,\n\tRDMAP_BASE_BOUNDS\t= 0x01,\n\tRDMAP_ACC_VIOL\t\t= 0x02,\n\tRDMAP_STAG_NOT_ASSOC\t= 0x03,\n\tRDMAP_TO_WRAP\t\t= 0x04,\n\tRDMAP_INV_VERS\t\t= 0x05,\n\tRDMAP_INV_OPCODE\t= 0x06,\n\tRDMAP_STREAM_CATA\t= 0x07,\n\tRDMAP_GLOBAL_CATA\t= 0x08,\n\tRDMAP_CANT_INV_STAG\t= 0x09,\n\tRDMAP_UNSPECIFIED\t= 0xff\n};\n\nenum c4iw_ddp_ecodes {\n\tDDPT_INV_STAG\t\t= 0x00,\n\tDDPT_BASE_BOUNDS\t= 0x01,\n\tDDPT_STAG_NOT_ASSOC\t= 0x02,\n\tDDPT_TO_WRAP\t\t= 0x03,\n\tDDPT_INV_VERS\t\t= 0x04,\n\tDDPU_INV_QN\t\t= 0x01,\n\tDDPU_INV_MSN_NOBUF\t= 0x02,\n\tDDPU_INV_MSN_RANGE\t= 0x03,\n\tDDPU_INV_MO\t\t= 0x04,\n\tDDPU_MSG_TOOBIG\t\t= 0x05,\n\tDDPU_INV_VERS\t\t= 0x06\n};\n\nenum c4iw_mpa_ecodes {\n\tMPA_CRC_ERR\t\t= 0x02,\n\tMPA_MARKER_ERR          = 0x03,\n\tMPA_LOCAL_CATA          = 0x05,\n\tMPA_INSUFF_IRD          = 0x06,\n\tMPA_NOMATCH_RTR         = 0x07,\n};\n\nenum c4iw_ep_state {\n\tIDLE = 0,\n\tLISTEN,\n\tCONNECTING,\n\tMPA_REQ_WAIT,\n\tMPA_REQ_SENT,\n\tMPA_REQ_RCVD,\n\tMPA_REP_SENT,\n\tFPDU_MODE,\n\tABORTING,\n\tCLOSING,\n\tMORIBUND,\n\tDEAD,\n};\n\nenum c4iw_ep_flags {\n\tPEER_ABORT_IN_PROGRESS\t= 0,\n\tABORT_REQ_IN_PROGRESS\t= 1,\n\tRELEASE_RESOURCES\t= 2,\n\tCLOSE_SENT\t\t= 3,\n\tTIMEOUT                 = 4,\n\tQP_REFERENCED           = 5,\n\tSTOP_MPA_TIMER\t\t= 7,\n};\n\nenum c4iw_ep_history {\n\tACT_OPEN_REQ            = 0,\n\tACT_OFLD_CONN           = 1,\n\tACT_OPEN_RPL            = 2,\n\tACT_ESTAB               = 3,\n\tPASS_ACCEPT_REQ         = 4,\n\tPASS_ESTAB              = 5,\n\tABORT_UPCALL            = 6,\n\tESTAB_UPCALL            = 7,\n\tCLOSE_UPCALL            = 8,\n\tULP_ACCEPT              = 9,\n\tULP_REJECT              = 10,\n\tTIMEDOUT                = 11,\n\tPEER_ABORT              = 12,\n\tPEER_CLOSE              = 13,\n\tCONNREQ_UPCALL          = 14,\n\tABORT_CONN              = 15,\n\tDISCONN_UPCALL          = 16,\n\tEP_DISC_CLOSE           = 17,\n\tEP_DISC_ABORT           = 18,\n\tCONN_RPL_UPCALL         = 19,\n\tACT_RETRY_NOMEM         = 20,\n\tACT_RETRY_INUSE         = 21,\n\tCLOSE_CON_RPL\t\t= 22,\n\tEP_DISC_FAIL\t\t= 24,\n\tQP_REFED\t\t= 25,\n\tQP_DEREFED\t\t= 26,\n\tCM_ID_REFED\t\t= 27,\n\tCM_ID_DEREFED\t\t= 28,\n};\n\nenum conn_pre_alloc_buffers {\n\tCN_ABORT_REQ_BUF,\n\tCN_ABORT_RPL_BUF,\n\tCN_CLOSE_CON_REQ_BUF,\n\tCN_DESTROY_BUF,\n\tCN_FLOWC_BUF,\n\tCN_MAX_CON_BUF\n};\n\nenum {\n\tFLOWC_LEN = offsetof(struct fw_flowc_wr, mnemval[FW_FLOWC_MNEM_MAX])\n};\n\nunion cpl_wr_size {\n\tstruct cpl_abort_req abrt_req;\n\tstruct cpl_abort_rpl abrt_rpl;\n\tstruct fw_ri_wr ri_req;\n\tstruct cpl_close_con_req close_req;\n\tchar flowc_buf[FLOWC_LEN];\n};\n\nstruct c4iw_ep_common {\n\tstruct iw_cm_id *cm_id;\n\tstruct c4iw_qp *qp;\n\tstruct c4iw_dev *dev;\n\tstruct sk_buff_head ep_skb_list;\n\tenum c4iw_ep_state state;\n\tstruct kref kref;\n\tstruct mutex mutex;\n\tstruct sockaddr_storage local_addr;\n\tstruct sockaddr_storage remote_addr;\n\tstruct c4iw_wr_wait *wr_waitp;\n\tunsigned long flags;\n\tunsigned long history;\n};\n\nstruct c4iw_listen_ep {\n\tstruct c4iw_ep_common com;\n\tunsigned int stid;\n\tint backlog;\n};\n\nstruct c4iw_ep_stats {\n\tunsigned connect_neg_adv;\n\tunsigned abort_neg_adv;\n};\n\nstruct c4iw_ep {\n\tstruct c4iw_ep_common com;\n\tstruct c4iw_ep *parent_ep;\n\tstruct timer_list timer;\n\tstruct list_head entry;\n\tunsigned int atid;\n\tu32 hwtid;\n\tu32 snd_seq;\n\tu32 rcv_seq;\n\tstruct l2t_entry *l2t;\n\tstruct dst_entry *dst;\n\tstruct sk_buff *mpa_skb;\n\tstruct c4iw_mpa_attributes mpa_attr;\n\tu8 mpa_pkt[sizeof(struct mpa_message) + MPA_MAX_PRIVATE_DATA];\n\tunsigned int mpa_pkt_len;\n\tu32 ird;\n\tu32 ord;\n\tu32 smac_idx;\n\tu32 tx_chan;\n\tu32 mtu;\n\tu16 mss;\n\tu16 emss;\n\tu16 plen;\n\tu16 rss_qid;\n\tu16 txq_idx;\n\tu16 ctrlq_idx;\n\tu8 tos;\n\tu8 retry_with_mpa_v1;\n\tu8 tried_with_mpa_v1;\n\tunsigned int retry_count;\n\tint snd_win;\n\tint rcv_win;\n\tu32 snd_wscale;\n\tstruct c4iw_ep_stats stats;\n\tu32 srqe_idx;\n\tu32 rx_pdu_out_cnt;\n\tstruct sk_buff *peer_abort_skb;\n};\n\nstatic inline struct c4iw_ep *to_ep(struct iw_cm_id *cm_id)\n{\n\treturn cm_id->provider_data;\n}\n\nstatic inline struct c4iw_listen_ep *to_listen_ep(struct iw_cm_id *cm_id)\n{\n\treturn cm_id->provider_data;\n}\n\nstatic inline int ocqp_supported(const struct cxgb4_lld_info *infop)\n{\n#if defined(__i386__) || defined(__x86_64__) || defined(CONFIG_PPC64)\n\treturn infop->vr->ocq.size > 0;\n#else\n\treturn 0;\n#endif\n}\n\nu32 c4iw_id_alloc(struct c4iw_id_table *alloc);\nvoid c4iw_id_free(struct c4iw_id_table *alloc, u32 obj);\nint c4iw_id_table_alloc(struct c4iw_id_table *alloc, u32 start, u32 num,\n\t\t\tu32 reserved, u32 flags);\nvoid c4iw_id_table_free(struct c4iw_id_table *alloc);\n\ntypedef int (*c4iw_handler_func)(struct c4iw_dev *dev, struct sk_buff *skb);\n\nint c4iw_ep_redirect(void *ctx, struct dst_entry *old, struct dst_entry *new,\n\t\t     struct l2t_entry *l2t);\nvoid c4iw_put_qpid(struct c4iw_rdev *rdev, u32 qpid,\n\t\t   struct c4iw_dev_ucontext *uctx);\nu32 c4iw_get_resource(struct c4iw_id_table *id_table);\nvoid c4iw_put_resource(struct c4iw_id_table *id_table, u32 entry);\nint c4iw_init_resource(struct c4iw_rdev *rdev, u32 nr_tpt,\n\t\t       u32 nr_pdid, u32 nr_srqt);\nint c4iw_init_ctrl_qp(struct c4iw_rdev *rdev);\nint c4iw_pblpool_create(struct c4iw_rdev *rdev);\nint c4iw_rqtpool_create(struct c4iw_rdev *rdev);\nint c4iw_ocqp_pool_create(struct c4iw_rdev *rdev);\nvoid c4iw_pblpool_destroy(struct c4iw_rdev *rdev);\nvoid c4iw_rqtpool_destroy(struct c4iw_rdev *rdev);\nvoid c4iw_ocqp_pool_destroy(struct c4iw_rdev *rdev);\nvoid c4iw_destroy_resource(struct c4iw_resource *rscp);\nint c4iw_destroy_ctrl_qp(struct c4iw_rdev *rdev);\nvoid c4iw_register_device(struct work_struct *work);\nvoid c4iw_unregister_device(struct c4iw_dev *dev);\nint __init c4iw_cm_init(void);\nvoid c4iw_cm_term(void);\nvoid c4iw_release_dev_ucontext(struct c4iw_rdev *rdev,\n\t\t\t       struct c4iw_dev_ucontext *uctx);\nvoid c4iw_init_dev_ucontext(struct c4iw_rdev *rdev,\n\t\t\t    struct c4iw_dev_ucontext *uctx);\nint c4iw_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);\nint c4iw_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t   const struct ib_send_wr **bad_wr);\nint c4iw_post_receive(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr);\nint c4iw_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);\nint c4iw_create_listen(struct iw_cm_id *cm_id, int backlog);\nint c4iw_destroy_listen(struct iw_cm_id *cm_id);\nint c4iw_accept_cr(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);\nint c4iw_reject_cr(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len);\nvoid c4iw_qp_add_ref(struct ib_qp *qp);\nvoid c4iw_qp_rem_ref(struct ib_qp *qp);\nstruct ib_mr *c4iw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t    u32 max_num_sg);\nint c4iw_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t   unsigned int *sg_offset);\nvoid c4iw_dealloc(struct uld_ctx *ctx);\nstruct ib_mr *c4iw_reg_user_mr(struct ib_pd *pd, u64 start,\n\t\t\t\t\t   u64 length, u64 virt, int acc,\n\t\t\t\t\t   struct ib_udata *udata);\nstruct ib_mr *c4iw_get_dma_mr(struct ib_pd *pd, int acc);\nint c4iw_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata);\nint c4iw_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata);\nvoid c4iw_cq_rem_ref(struct c4iw_cq *chp);\nint c4iw_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t   struct ib_udata *udata);\nint c4iw_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);\nint c4iw_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *attr,\n\t\t    enum ib_srq_attr_mask srq_attr_mask,\n\t\t    struct ib_udata *udata);\nint c4iw_destroy_srq(struct ib_srq *ib_srq, struct ib_udata *udata);\nint c4iw_create_srq(struct ib_srq *srq, struct ib_srq_init_attr *attrs,\n\t\t    struct ib_udata *udata);\nint c4iw_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata);\nint c4iw_create_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,\n\t\t   struct ib_udata *udata);\nint c4iw_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t\t\t int attr_mask, struct ib_udata *udata);\nint c4iw_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t     int attr_mask, struct ib_qp_init_attr *init_attr);\nstruct ib_qp *c4iw_get_qp(struct ib_device *dev, int qpn);\nu32 c4iw_rqtpool_alloc(struct c4iw_rdev *rdev, int size);\nvoid c4iw_rqtpool_free(struct c4iw_rdev *rdev, u32 addr, int size);\nu32 c4iw_pblpool_alloc(struct c4iw_rdev *rdev, int size);\nvoid c4iw_pblpool_free(struct c4iw_rdev *rdev, u32 addr, int size);\nu32 c4iw_ocqp_pool_alloc(struct c4iw_rdev *rdev, int size);\nvoid c4iw_ocqp_pool_free(struct c4iw_rdev *rdev, u32 addr, int size);\nvoid c4iw_flush_hw_cq(struct c4iw_cq *chp, struct c4iw_qp *flush_qhp);\nvoid c4iw_count_rcqes(struct t4_cq *cq, struct t4_wq *wq, int *count);\nint c4iw_ep_disconnect(struct c4iw_ep *ep, int abrupt, gfp_t gfp);\nint c4iw_flush_rq(struct t4_wq *wq, struct t4_cq *cq, int count);\nint c4iw_flush_sq(struct c4iw_qp *qhp);\nint c4iw_ev_handler(struct c4iw_dev *rnicp, u32 qid);\nu16 c4iw_rqes_posted(struct c4iw_qp *qhp);\nint c4iw_post_terminate(struct c4iw_qp *qhp, struct t4_cqe *err_cqe);\nu32 c4iw_get_cqid(struct c4iw_rdev *rdev, struct c4iw_dev_ucontext *uctx);\nvoid c4iw_put_cqid(struct c4iw_rdev *rdev, u32 qid,\n\t\tstruct c4iw_dev_ucontext *uctx);\nu32 c4iw_get_qpid(struct c4iw_rdev *rdev, struct c4iw_dev_ucontext *uctx);\nvoid c4iw_put_qpid(struct c4iw_rdev *rdev, u32 qid,\n\t\tstruct c4iw_dev_ucontext *uctx);\nvoid c4iw_ev_dispatch(struct c4iw_dev *dev, struct t4_cqe *err_cqe);\n\nextern struct cxgb4_client t4c_client;\nextern c4iw_handler_func c4iw_handlers[NUM_CPL_CMDS];\nvoid __iomem *c4iw_bar2_addrs(struct c4iw_rdev *rdev, unsigned int qid,\n\t\t\t      enum cxgb4_bar2_qtype qtype,\n\t\t\t      unsigned int *pbar2_qid, u64 *pbar2_pa);\nint c4iw_alloc_srq_idx(struct c4iw_rdev *rdev);\nvoid c4iw_free_srq_idx(struct c4iw_rdev *rdev, int idx);\nextern void c4iw_log_wr_stats(struct t4_wq *wq, struct t4_cqe *cqe);\nextern int c4iw_wr_log;\nextern int db_fc_threshold;\nextern int db_coalescing_threshold;\nextern int use_dsgl;\nvoid c4iw_invalidate_mr(struct c4iw_dev *rhp, u32 rkey);\nvoid c4iw_dispatch_srq_limit_reached_event(struct c4iw_srq *srq);\nvoid c4iw_copy_wr_to_srq(struct t4_srq *srq, union t4_recv_wr *wqe, u8 len16);\nvoid c4iw_flush_srqidx(struct c4iw_qp *qhp, u32 srqidx);\nint c4iw_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t       const struct ib_recv_wr **bad_wr);\nstruct c4iw_wr_wait *c4iw_alloc_wr_wait(gfp_t gfp);\n\nint c4iw_fill_res_mr_entry(struct sk_buff *msg, struct ib_mr *ibmr);\nint c4iw_fill_res_cq_entry(struct sk_buff *msg, struct ib_cq *ibcq);\nint c4iw_fill_res_qp_entry(struct sk_buff *msg, struct ib_qp *ibqp);\nint c4iw_fill_res_cm_id_entry(struct sk_buff *msg, struct rdma_cm_id *cm_id);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}