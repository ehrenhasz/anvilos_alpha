{
  "module_name": "qp.c",
  "hash_id": "b03f8878983beeb2b4f65b265ed39b30fd5011099e28228a2a8b0439c3734bd6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/qp.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"iw_cxgb4.h\"\n\nstatic int db_delay_usecs = 1;\nmodule_param(db_delay_usecs, int, 0644);\nMODULE_PARM_DESC(db_delay_usecs, \"Usecs to delay awaiting db fifo to drain\");\n\nstatic int ocqp_support = 1;\nmodule_param(ocqp_support, int, 0644);\nMODULE_PARM_DESC(ocqp_support, \"Support on-chip SQs (default=1)\");\n\nint db_fc_threshold = 1000;\nmodule_param(db_fc_threshold, int, 0644);\nMODULE_PARM_DESC(db_fc_threshold,\n\t\t \"QP count/threshold that triggers\"\n\t\t \" automatic db flow control mode (default = 1000)\");\n\nint db_coalescing_threshold;\nmodule_param(db_coalescing_threshold, int, 0644);\nMODULE_PARM_DESC(db_coalescing_threshold,\n\t\t \"QP count/threshold that triggers\"\n\t\t \" disabling db coalescing (default = 0)\");\n\nstatic int max_fr_immd = T4_MAX_FR_IMMD;\nmodule_param(max_fr_immd, int, 0644);\nMODULE_PARM_DESC(max_fr_immd, \"fastreg threshold for using DSGL instead of immediate\");\n\nstatic int alloc_ird(struct c4iw_dev *dev, u32 ird)\n{\n\tint ret = 0;\n\n\txa_lock_irq(&dev->qps);\n\tif (ird <= dev->avail_ird)\n\t\tdev->avail_ird -= ird;\n\telse\n\t\tret = -ENOMEM;\n\txa_unlock_irq(&dev->qps);\n\n\tif (ret)\n\t\tdev_warn(&dev->rdev.lldi.pdev->dev,\n\t\t\t \"device IRD resources exhausted\\n\");\n\n\treturn ret;\n}\n\nstatic void free_ird(struct c4iw_dev *dev, int ird)\n{\n\txa_lock_irq(&dev->qps);\n\tdev->avail_ird += ird;\n\txa_unlock_irq(&dev->qps);\n}\n\nstatic void set_state(struct c4iw_qp *qhp, enum c4iw_qp_state state)\n{\n\tunsigned long flag;\n\tspin_lock_irqsave(&qhp->lock, flag);\n\tqhp->attr.state = state;\n\tspin_unlock_irqrestore(&qhp->lock, flag);\n}\n\nstatic void dealloc_oc_sq(struct c4iw_rdev *rdev, struct t4_sq *sq)\n{\n\tc4iw_ocqp_pool_free(rdev, sq->dma_addr, sq->memsize);\n}\n\nstatic void dealloc_host_sq(struct c4iw_rdev *rdev, struct t4_sq *sq)\n{\n\tdma_free_coherent(&(rdev->lldi.pdev->dev), sq->memsize, sq->queue,\n\t\t\t  dma_unmap_addr(sq, mapping));\n}\n\nstatic void dealloc_sq(struct c4iw_rdev *rdev, struct t4_sq *sq)\n{\n\tif (t4_sq_onchip(sq))\n\t\tdealloc_oc_sq(rdev, sq);\n\telse\n\t\tdealloc_host_sq(rdev, sq);\n}\n\nstatic int alloc_oc_sq(struct c4iw_rdev *rdev, struct t4_sq *sq)\n{\n\tif (!ocqp_support || !ocqp_supported(&rdev->lldi))\n\t\treturn -ENOSYS;\n\tsq->dma_addr = c4iw_ocqp_pool_alloc(rdev, sq->memsize);\n\tif (!sq->dma_addr)\n\t\treturn -ENOMEM;\n\tsq->phys_addr = rdev->oc_mw_pa + sq->dma_addr -\n\t\t\trdev->lldi.vr->ocq.start;\n\tsq->queue = (__force union t4_wr *)(rdev->oc_mw_kva + sq->dma_addr -\n\t\t\t\t\t    rdev->lldi.vr->ocq.start);\n\tsq->flags |= T4_SQ_ONCHIP;\n\treturn 0;\n}\n\nstatic int alloc_host_sq(struct c4iw_rdev *rdev, struct t4_sq *sq)\n{\n\tsq->queue = dma_alloc_coherent(&(rdev->lldi.pdev->dev), sq->memsize,\n\t\t\t\t       &(sq->dma_addr), GFP_KERNEL);\n\tif (!sq->queue)\n\t\treturn -ENOMEM;\n\tsq->phys_addr = virt_to_phys(sq->queue);\n\tdma_unmap_addr_set(sq, mapping, sq->dma_addr);\n\treturn 0;\n}\n\nstatic int alloc_sq(struct c4iw_rdev *rdev, struct t4_sq *sq, int user)\n{\n\tint ret = -ENOSYS;\n\tif (user)\n\t\tret = alloc_oc_sq(rdev, sq);\n\tif (ret)\n\t\tret = alloc_host_sq(rdev, sq);\n\treturn ret;\n}\n\nstatic int destroy_qp(struct c4iw_rdev *rdev, struct t4_wq *wq,\n\t\t      struct c4iw_dev_ucontext *uctx, int has_rq)\n{\n\t \n\tdealloc_sq(rdev, &wq->sq);\n\tkfree(wq->sq.sw_sq);\n\tc4iw_put_qpid(rdev, wq->sq.qid, uctx);\n\n\tif (has_rq) {\n\t\tdma_free_coherent(&rdev->lldi.pdev->dev,\n\t\t\t\t  wq->rq.memsize, wq->rq.queue,\n\t\t\t\t  dma_unmap_addr(&wq->rq, mapping));\n\t\tc4iw_rqtpool_free(rdev, wq->rq.rqt_hwaddr, wq->rq.rqt_size);\n\t\tkfree(wq->rq.sw_rq);\n\t\tc4iw_put_qpid(rdev, wq->rq.qid, uctx);\n\t}\n\treturn 0;\n}\n\n \nvoid __iomem *c4iw_bar2_addrs(struct c4iw_rdev *rdev, unsigned int qid,\n\t\t\t      enum cxgb4_bar2_qtype qtype,\n\t\t\t      unsigned int *pbar2_qid, u64 *pbar2_pa)\n{\n\tu64 bar2_qoffset;\n\tint ret;\n\n\tret = cxgb4_bar2_sge_qregs(rdev->lldi.ports[0], qid, qtype,\n\t\t\t\t   pbar2_pa ? 1 : 0,\n\t\t\t\t   &bar2_qoffset, pbar2_qid);\n\tif (ret)\n\t\treturn NULL;\n\n\tif (pbar2_pa)\n\t\t*pbar2_pa = (rdev->bar2_pa + bar2_qoffset) & PAGE_MASK;\n\n\tif (is_t4(rdev->lldi.adapter_type))\n\t\treturn NULL;\n\n\treturn rdev->bar2_kva + bar2_qoffset;\n}\n\nstatic int create_qp(struct c4iw_rdev *rdev, struct t4_wq *wq,\n\t\t     struct t4_cq *rcq, struct t4_cq *scq,\n\t\t     struct c4iw_dev_ucontext *uctx,\n\t\t     struct c4iw_wr_wait *wr_waitp,\n\t\t     int need_rq)\n{\n\tint user = (uctx != &rdev->uctx);\n\tstruct fw_ri_res_wr *res_wr;\n\tstruct fw_ri_res *res;\n\tint wr_len;\n\tstruct sk_buff *skb;\n\tint ret = 0;\n\tint eqsize;\n\n\twq->sq.qid = c4iw_get_qpid(rdev, uctx);\n\tif (!wq->sq.qid)\n\t\treturn -ENOMEM;\n\n\tif (need_rq) {\n\t\twq->rq.qid = c4iw_get_qpid(rdev, uctx);\n\t\tif (!wq->rq.qid) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_sq_qid;\n\t\t}\n\t}\n\n\tif (!user) {\n\t\twq->sq.sw_sq = kcalloc(wq->sq.size, sizeof(*wq->sq.sw_sq),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!wq->sq.sw_sq) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_rq_qid;\n\t\t}\n\n\t\tif (need_rq) {\n\t\t\twq->rq.sw_rq = kcalloc(wq->rq.size,\n\t\t\t\t\t       sizeof(*wq->rq.sw_rq),\n\t\t\t\t\t       GFP_KERNEL);\n\t\t\tif (!wq->rq.sw_rq) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto free_sw_sq;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (need_rq) {\n\t\t \n\t\twq->rq.rqt_size =\n\t\t\troundup_pow_of_two(max_t(u16, wq->rq.size, 16));\n\t\twq->rq.rqt_hwaddr = c4iw_rqtpool_alloc(rdev, wq->rq.rqt_size);\n\t\tif (!wq->rq.rqt_hwaddr) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_sw_rq;\n\t\t}\n\t}\n\n\tret = alloc_sq(rdev, &wq->sq, user);\n\tif (ret)\n\t\tgoto free_hwaddr;\n\tmemset(wq->sq.queue, 0, wq->sq.memsize);\n\tdma_unmap_addr_set(&wq->sq, mapping, wq->sq.dma_addr);\n\n\tif (need_rq) {\n\t\twq->rq.queue = dma_alloc_coherent(&rdev->lldi.pdev->dev,\n\t\t\t\t\t\t  wq->rq.memsize,\n\t\t\t\t\t\t  &wq->rq.dma_addr,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!wq->rq.queue) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_sq;\n\t\t}\n\t\tpr_debug(\"sq base va 0x%p pa 0x%llx rq base va 0x%p pa 0x%llx\\n\",\n\t\t\t wq->sq.queue,\n\t\t\t (unsigned long long)virt_to_phys(wq->sq.queue),\n\t\t\t wq->rq.queue,\n\t\t\t (unsigned long long)virt_to_phys(wq->rq.queue));\n\t\tdma_unmap_addr_set(&wq->rq, mapping, wq->rq.dma_addr);\n\t}\n\n\twq->db = rdev->lldi.db_reg;\n\n\twq->sq.bar2_va = c4iw_bar2_addrs(rdev, wq->sq.qid,\n\t\t\t\t\t CXGB4_BAR2_QTYPE_EGRESS,\n\t\t\t\t\t &wq->sq.bar2_qid,\n\t\t\t\t\t user ? &wq->sq.bar2_pa : NULL);\n\tif (need_rq)\n\t\twq->rq.bar2_va = c4iw_bar2_addrs(rdev, wq->rq.qid,\n\t\t\t\t\t\t CXGB4_BAR2_QTYPE_EGRESS,\n\t\t\t\t\t\t &wq->rq.bar2_qid,\n\t\t\t\t\t\t user ? &wq->rq.bar2_pa : NULL);\n\n\t \n\tif (user && (!wq->sq.bar2_pa || (need_rq && !wq->rq.bar2_pa))) {\n\t\tpr_warn(\"%s: sqid %u or rqid %u not in BAR2 range\\n\",\n\t\t\tpci_name(rdev->lldi.pdev), wq->sq.qid, wq->rq.qid);\n\t\tret = -EINVAL;\n\t\tgoto free_dma;\n\t}\n\n\twq->rdev = rdev;\n\twq->rq.msn = 1;\n\n\t \n\twr_len = sizeof(*res_wr) + 2 * sizeof(*res);\n\tif (need_rq)\n\t\twr_len += sizeof(*res);\n\tskb = alloc_skb(wr_len, GFP_KERNEL);\n\tif (!skb) {\n\t\tret = -ENOMEM;\n\t\tgoto free_dma;\n\t}\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\tres_wr = __skb_put_zero(skb, wr_len);\n\tres_wr->op_nres = cpu_to_be32(\n\t\t\tFW_WR_OP_V(FW_RI_RES_WR) |\n\t\t\tFW_RI_RES_WR_NRES_V(need_rq ? 2 : 1) |\n\t\t\tFW_WR_COMPL_F);\n\tres_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));\n\tres_wr->cookie = (uintptr_t)wr_waitp;\n\tres = res_wr->res;\n\tres->u.sqrq.restype = FW_RI_RES_TYPE_SQ;\n\tres->u.sqrq.op = FW_RI_RES_OP_WRITE;\n\n\t \n\teqsize = wq->sq.size * T4_SQ_NUM_SLOTS +\n\t\trdev->hw_queue.t4_eq_status_entries;\n\n\tres->u.sqrq.fetchszm_to_iqid = cpu_to_be32(\n\t\tFW_RI_RES_WR_HOSTFCMODE_V(0) |\t \n\t\tFW_RI_RES_WR_CPRIO_V(0) |\t \n\t\tFW_RI_RES_WR_PCIECHN_V(0) |\t \n\t\t(t4_sq_onchip(&wq->sq) ? FW_RI_RES_WR_ONCHIP_F : 0) |\n\t\tFW_RI_RES_WR_IQID_V(scq->cqid));\n\tres->u.sqrq.dcaen_to_eqsize = cpu_to_be32(\n\t\tFW_RI_RES_WR_DCAEN_V(0) |\n\t\tFW_RI_RES_WR_DCACPU_V(0) |\n\t\tFW_RI_RES_WR_FBMIN_V(2) |\n\t\t(t4_sq_onchip(&wq->sq) ? FW_RI_RES_WR_FBMAX_V(2) :\n\t\t\t\t\t FW_RI_RES_WR_FBMAX_V(3)) |\n\t\tFW_RI_RES_WR_CIDXFTHRESHO_V(0) |\n\t\tFW_RI_RES_WR_CIDXFTHRESH_V(0) |\n\t\tFW_RI_RES_WR_EQSIZE_V(eqsize));\n\tres->u.sqrq.eqid = cpu_to_be32(wq->sq.qid);\n\tres->u.sqrq.eqaddr = cpu_to_be64(wq->sq.dma_addr);\n\n\tif (need_rq) {\n\t\tres++;\n\t\tres->u.sqrq.restype = FW_RI_RES_TYPE_RQ;\n\t\tres->u.sqrq.op = FW_RI_RES_OP_WRITE;\n\n\t\t \n\t\teqsize = wq->rq.size * T4_RQ_NUM_SLOTS +\n\t\t\trdev->hw_queue.t4_eq_status_entries;\n\t\tres->u.sqrq.fetchszm_to_iqid =\n\t\t\t \n\t\t\tcpu_to_be32(FW_RI_RES_WR_HOSTFCMODE_V(0) |\n\t\t\t \n\t\t\tFW_RI_RES_WR_CPRIO_V(0) |\n\t\t\t \n\t\t\tFW_RI_RES_WR_PCIECHN_V(0) |\n\t\t\tFW_RI_RES_WR_IQID_V(rcq->cqid));\n\t\tres->u.sqrq.dcaen_to_eqsize =\n\t\t\tcpu_to_be32(FW_RI_RES_WR_DCAEN_V(0) |\n\t\t\tFW_RI_RES_WR_DCACPU_V(0) |\n\t\t\tFW_RI_RES_WR_FBMIN_V(2) |\n\t\t\tFW_RI_RES_WR_FBMAX_V(3) |\n\t\t\tFW_RI_RES_WR_CIDXFTHRESHO_V(0) |\n\t\t\tFW_RI_RES_WR_CIDXFTHRESH_V(0) |\n\t\t\tFW_RI_RES_WR_EQSIZE_V(eqsize));\n\t\tres->u.sqrq.eqid = cpu_to_be32(wq->rq.qid);\n\t\tres->u.sqrq.eqaddr = cpu_to_be64(wq->rq.dma_addr);\n\t}\n\n\tc4iw_init_wr_wait(wr_waitp);\n\tret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, wq->sq.qid, __func__);\n\tif (ret)\n\t\tgoto free_dma;\n\n\tpr_debug(\"sqid 0x%x rqid 0x%x kdb 0x%p sq_bar2_addr %p rq_bar2_addr %p\\n\",\n\t\t wq->sq.qid, wq->rq.qid, wq->db,\n\t\t wq->sq.bar2_va, wq->rq.bar2_va);\n\n\treturn 0;\nfree_dma:\n\tif (need_rq)\n\t\tdma_free_coherent(&rdev->lldi.pdev->dev,\n\t\t\t\t  wq->rq.memsize, wq->rq.queue,\n\t\t\t\t  dma_unmap_addr(&wq->rq, mapping));\nfree_sq:\n\tdealloc_sq(rdev, &wq->sq);\nfree_hwaddr:\n\tif (need_rq)\n\t\tc4iw_rqtpool_free(rdev, wq->rq.rqt_hwaddr, wq->rq.rqt_size);\nfree_sw_rq:\n\tif (need_rq)\n\t\tkfree(wq->rq.sw_rq);\nfree_sw_sq:\n\tkfree(wq->sq.sw_sq);\nfree_rq_qid:\n\tif (need_rq)\n\t\tc4iw_put_qpid(rdev, wq->rq.qid, uctx);\nfree_sq_qid:\n\tc4iw_put_qpid(rdev, wq->sq.qid, uctx);\n\treturn ret;\n}\n\nstatic int build_immd(struct t4_sq *sq, struct fw_ri_immd *immdp,\n\t\t      const struct ib_send_wr *wr, int max, u32 *plenp)\n{\n\tu8 *dstp, *srcp;\n\tu32 plen = 0;\n\tint i;\n\tint rem, len;\n\n\tdstp = (u8 *)immdp->data;\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tif ((plen + wr->sg_list[i].length) > max)\n\t\t\treturn -EMSGSIZE;\n\t\tsrcp = (u8 *)(unsigned long)wr->sg_list[i].addr;\n\t\tplen += wr->sg_list[i].length;\n\t\trem = wr->sg_list[i].length;\n\t\twhile (rem) {\n\t\t\tif (dstp == (u8 *)&sq->queue[sq->size])\n\t\t\t\tdstp = (u8 *)sq->queue;\n\t\t\tif (rem <= (u8 *)&sq->queue[sq->size] - dstp)\n\t\t\t\tlen = rem;\n\t\t\telse\n\t\t\t\tlen = (u8 *)&sq->queue[sq->size] - dstp;\n\t\t\tmemcpy(dstp, srcp, len);\n\t\t\tdstp += len;\n\t\t\tsrcp += len;\n\t\t\trem -= len;\n\t\t}\n\t}\n\tlen = roundup(plen + sizeof(*immdp), 16) - (plen + sizeof(*immdp));\n\tif (len)\n\t\tmemset(dstp, 0, len);\n\timmdp->op = FW_RI_DATA_IMMD;\n\timmdp->r1 = 0;\n\timmdp->r2 = 0;\n\timmdp->immdlen = cpu_to_be32(plen);\n\t*plenp = plen;\n\treturn 0;\n}\n\nstatic int build_isgl(__be64 *queue_start, __be64 *queue_end,\n\t\t      struct fw_ri_isgl *isglp, struct ib_sge *sg_list,\n\t\t      int num_sge, u32 *plenp)\n\n{\n\tint i;\n\tu32 plen = 0;\n\t__be64 *flitp;\n\n\tif ((__be64 *)isglp == queue_end)\n\t\tisglp = (struct fw_ri_isgl *)queue_start;\n\n\tflitp = (__be64 *)isglp->sge;\n\n\tfor (i = 0; i < num_sge; i++) {\n\t\tif ((plen + sg_list[i].length) < plen)\n\t\t\treturn -EMSGSIZE;\n\t\tplen += sg_list[i].length;\n\t\t*flitp = cpu_to_be64(((u64)sg_list[i].lkey << 32) |\n\t\t\t\t     sg_list[i].length);\n\t\tif (++flitp == queue_end)\n\t\t\tflitp = queue_start;\n\t\t*flitp = cpu_to_be64(sg_list[i].addr);\n\t\tif (++flitp == queue_end)\n\t\t\tflitp = queue_start;\n\t}\n\t*flitp = (__force __be64)0;\n\tisglp->op = FW_RI_DATA_ISGL;\n\tisglp->r1 = 0;\n\tisglp->nsge = cpu_to_be16(num_sge);\n\tisglp->r2 = 0;\n\tif (plenp)\n\t\t*plenp = plen;\n\treturn 0;\n}\n\nstatic int build_rdma_send(struct t4_sq *sq, union t4_wr *wqe,\n\t\t\t   const struct ib_send_wr *wr, u8 *len16)\n{\n\tu32 plen;\n\tint size;\n\tint ret;\n\n\tif (wr->num_sge > T4_MAX_SEND_SGE)\n\t\treturn -EINVAL;\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND:\n\t\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\t\twqe->send.sendop_pkd = cpu_to_be32(\n\t\t\t\tFW_RI_SEND_WR_SENDOP_V(FW_RI_SEND_WITH_SE));\n\t\telse\n\t\t\twqe->send.sendop_pkd = cpu_to_be32(\n\t\t\t\tFW_RI_SEND_WR_SENDOP_V(FW_RI_SEND));\n\t\twqe->send.stag_inv = 0;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_INV:\n\t\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\t\twqe->send.sendop_pkd = cpu_to_be32(\n\t\t\t\tFW_RI_SEND_WR_SENDOP_V(FW_RI_SEND_WITH_SE_INV));\n\t\telse\n\t\t\twqe->send.sendop_pkd = cpu_to_be32(\n\t\t\t\tFW_RI_SEND_WR_SENDOP_V(FW_RI_SEND_WITH_INV));\n\t\twqe->send.stag_inv = cpu_to_be32(wr->ex.invalidate_rkey);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\twqe->send.r3 = 0;\n\twqe->send.r4 = 0;\n\n\tplen = 0;\n\tif (wr->num_sge) {\n\t\tif (wr->send_flags & IB_SEND_INLINE) {\n\t\t\tret = build_immd(sq, wqe->send.u.immd_src, wr,\n\t\t\t\t\t T4_MAX_SEND_INLINE, &plen);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tsize = sizeof(wqe->send) + sizeof(struct fw_ri_immd) +\n\t\t\t       plen;\n\t\t} else {\n\t\t\tret = build_isgl((__be64 *)sq->queue,\n\t\t\t\t\t (__be64 *)&sq->queue[sq->size],\n\t\t\t\t\t wqe->send.u.isgl_src,\n\t\t\t\t\t wr->sg_list, wr->num_sge, &plen);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tsize = sizeof(wqe->send) + sizeof(struct fw_ri_isgl) +\n\t\t\t       wr->num_sge * sizeof(struct fw_ri_sge);\n\t\t}\n\t} else {\n\t\twqe->send.u.immd_src[0].op = FW_RI_DATA_IMMD;\n\t\twqe->send.u.immd_src[0].r1 = 0;\n\t\twqe->send.u.immd_src[0].r2 = 0;\n\t\twqe->send.u.immd_src[0].immdlen = 0;\n\t\tsize = sizeof(wqe->send) + sizeof(struct fw_ri_immd);\n\t\tplen = 0;\n\t}\n\t*len16 = DIV_ROUND_UP(size, 16);\n\twqe->send.plen = cpu_to_be32(plen);\n\treturn 0;\n}\n\nstatic int build_rdma_write(struct t4_sq *sq, union t4_wr *wqe,\n\t\t\t    const struct ib_send_wr *wr, u8 *len16)\n{\n\tu32 plen;\n\tint size;\n\tint ret;\n\n\tif (wr->num_sge > T4_MAX_SEND_SGE)\n\t\treturn -EINVAL;\n\n\t \n\tif (wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)\n\t\twqe->write.iw_imm_data.ib_imm_data.imm_data32 = wr->ex.imm_data;\n\telse\n\t\twqe->write.iw_imm_data.ib_imm_data.imm_data32 = 0;\n\twqe->write.stag_sink = cpu_to_be32(rdma_wr(wr)->rkey);\n\twqe->write.to_sink = cpu_to_be64(rdma_wr(wr)->remote_addr);\n\tif (wr->num_sge) {\n\t\tif (wr->send_flags & IB_SEND_INLINE) {\n\t\t\tret = build_immd(sq, wqe->write.u.immd_src, wr,\n\t\t\t\t\t T4_MAX_WRITE_INLINE, &plen);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tsize = sizeof(wqe->write) + sizeof(struct fw_ri_immd) +\n\t\t\t       plen;\n\t\t} else {\n\t\t\tret = build_isgl((__be64 *)sq->queue,\n\t\t\t\t\t (__be64 *)&sq->queue[sq->size],\n\t\t\t\t\t wqe->write.u.isgl_src,\n\t\t\t\t\t wr->sg_list, wr->num_sge, &plen);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tsize = sizeof(wqe->write) + sizeof(struct fw_ri_isgl) +\n\t\t\t       wr->num_sge * sizeof(struct fw_ri_sge);\n\t\t}\n\t} else {\n\t\twqe->write.u.immd_src[0].op = FW_RI_DATA_IMMD;\n\t\twqe->write.u.immd_src[0].r1 = 0;\n\t\twqe->write.u.immd_src[0].r2 = 0;\n\t\twqe->write.u.immd_src[0].immdlen = 0;\n\t\tsize = sizeof(wqe->write) + sizeof(struct fw_ri_immd);\n\t\tplen = 0;\n\t}\n\t*len16 = DIV_ROUND_UP(size, 16);\n\twqe->write.plen = cpu_to_be32(plen);\n\treturn 0;\n}\n\nstatic void build_immd_cmpl(struct t4_sq *sq, struct fw_ri_immd_cmpl *immdp,\n\t\t\t    struct ib_send_wr *wr)\n{\n\tmemcpy((u8 *)immdp->data, (u8 *)(uintptr_t)wr->sg_list->addr, 16);\n\tmemset(immdp->r1, 0, 6);\n\timmdp->op = FW_RI_DATA_IMMD;\n\timmdp->immdlen = 16;\n}\n\nstatic void build_rdma_write_cmpl(struct t4_sq *sq,\n\t\t\t\t  struct fw_ri_rdma_write_cmpl_wr *wcwr,\n\t\t\t\t  const struct ib_send_wr *wr, u8 *len16)\n{\n\tu32 plen;\n\tint size;\n\n\t \n\tBUILD_BUG_ON(offsetof(struct fw_ri_rdma_write_cmpl_wr, u) > 64);\n\n\twcwr->stag_sink = cpu_to_be32(rdma_wr(wr)->rkey);\n\twcwr->to_sink = cpu_to_be64(rdma_wr(wr)->remote_addr);\n\tif (wr->next->opcode == IB_WR_SEND)\n\t\twcwr->stag_inv = 0;\n\telse\n\t\twcwr->stag_inv = cpu_to_be32(wr->next->ex.invalidate_rkey);\n\twcwr->r2 = 0;\n\twcwr->r3 = 0;\n\n\t \n\tif (wr->next->send_flags & IB_SEND_INLINE)\n\t\tbuild_immd_cmpl(sq, &wcwr->u_cmpl.immd_src, wr->next);\n\telse\n\t\tbuild_isgl((__be64 *)sq->queue, (__be64 *)&sq->queue[sq->size],\n\t\t\t   &wcwr->u_cmpl.isgl_src, wr->next->sg_list, 1, NULL);\n\n\t \n\tbuild_isgl((__be64 *)sq->queue, (__be64 *)&sq->queue[sq->size],\n\t\t   wcwr->u.isgl_src, wr->sg_list, wr->num_sge, &plen);\n\n\tsize = sizeof(*wcwr) + sizeof(struct fw_ri_isgl) +\n\t\twr->num_sge * sizeof(struct fw_ri_sge);\n\twcwr->plen = cpu_to_be32(plen);\n\t*len16 = DIV_ROUND_UP(size, 16);\n}\n\nstatic int build_rdma_read(union t4_wr *wqe, const struct ib_send_wr *wr,\n\t\t\t   u8 *len16)\n{\n\tif (wr->num_sge > 1)\n\t\treturn -EINVAL;\n\tif (wr->num_sge && wr->sg_list[0].length) {\n\t\twqe->read.stag_src = cpu_to_be32(rdma_wr(wr)->rkey);\n\t\twqe->read.to_src_hi = cpu_to_be32((u32)(rdma_wr(wr)->remote_addr\n\t\t\t\t\t\t\t>> 32));\n\t\twqe->read.to_src_lo = cpu_to_be32((u32)rdma_wr(wr)->remote_addr);\n\t\twqe->read.stag_sink = cpu_to_be32(wr->sg_list[0].lkey);\n\t\twqe->read.plen = cpu_to_be32(wr->sg_list[0].length);\n\t\twqe->read.to_sink_hi = cpu_to_be32((u32)(wr->sg_list[0].addr\n\t\t\t\t\t\t\t >> 32));\n\t\twqe->read.to_sink_lo = cpu_to_be32((u32)(wr->sg_list[0].addr));\n\t} else {\n\t\twqe->read.stag_src = cpu_to_be32(2);\n\t\twqe->read.to_src_hi = 0;\n\t\twqe->read.to_src_lo = 0;\n\t\twqe->read.stag_sink = cpu_to_be32(2);\n\t\twqe->read.plen = 0;\n\t\twqe->read.to_sink_hi = 0;\n\t\twqe->read.to_sink_lo = 0;\n\t}\n\twqe->read.r2 = 0;\n\twqe->read.r5 = 0;\n\t*len16 = DIV_ROUND_UP(sizeof(wqe->read), 16);\n\treturn 0;\n}\n\nstatic void post_write_cmpl(struct c4iw_qp *qhp, const struct ib_send_wr *wr)\n{\n\tbool send_signaled = (wr->next->send_flags & IB_SEND_SIGNALED) ||\n\t\t\t     qhp->sq_sig_all;\n\tbool write_signaled = (wr->send_flags & IB_SEND_SIGNALED) ||\n\t\t\t      qhp->sq_sig_all;\n\tstruct t4_swsqe *swsqe;\n\tunion t4_wr *wqe;\n\tu16 write_wrid;\n\tu8 len16;\n\tu16 idx;\n\n\t \n\twqe = (union t4_wr *)((u8 *)qhp->wq.sq.queue +\n\t       qhp->wq.sq.wq_pidx * T4_EQ_ENTRY_SIZE);\n\tbuild_rdma_write_cmpl(&qhp->wq.sq, &wqe->write_cmpl, wr, &len16);\n\n\t \n\tswsqe = &qhp->wq.sq.sw_sq[qhp->wq.sq.pidx];\n\tswsqe->opcode = FW_RI_RDMA_WRITE;\n\tswsqe->idx = qhp->wq.sq.pidx;\n\tswsqe->complete = 0;\n\tswsqe->signaled = write_signaled;\n\tswsqe->flushed = 0;\n\tswsqe->wr_id = wr->wr_id;\n\tif (c4iw_wr_log) {\n\t\tswsqe->sge_ts =\n\t\t\tcxgb4_read_sge_timestamp(qhp->rhp->rdev.lldi.ports[0]);\n\t\tswsqe->host_time = ktime_get();\n\t}\n\n\twrite_wrid = qhp->wq.sq.pidx;\n\n\t \n\tqhp->wq.sq.in_use++;\n\tif (++qhp->wq.sq.pidx == qhp->wq.sq.size)\n\t\tqhp->wq.sq.pidx = 0;\n\n\t \n\tswsqe = &qhp->wq.sq.sw_sq[qhp->wq.sq.pidx];\n\tif (wr->next->opcode == IB_WR_SEND)\n\t\tswsqe->opcode = FW_RI_SEND;\n\telse\n\t\tswsqe->opcode = FW_RI_SEND_WITH_INV;\n\tswsqe->idx = qhp->wq.sq.pidx;\n\tswsqe->complete = 0;\n\tswsqe->signaled = send_signaled;\n\tswsqe->flushed = 0;\n\tswsqe->wr_id = wr->next->wr_id;\n\tif (c4iw_wr_log) {\n\t\tswsqe->sge_ts =\n\t\t\tcxgb4_read_sge_timestamp(qhp->rhp->rdev.lldi.ports[0]);\n\t\tswsqe->host_time = ktime_get();\n\t}\n\n\twqe->write_cmpl.flags_send = send_signaled ? FW_RI_COMPLETION_FLAG : 0;\n\twqe->write_cmpl.wrid_send = qhp->wq.sq.pidx;\n\n\tinit_wr_hdr(wqe, write_wrid, FW_RI_RDMA_WRITE_CMPL_WR,\n\t\t    write_signaled ? FW_RI_COMPLETION_FLAG : 0, len16);\n\tt4_sq_produce(&qhp->wq, len16);\n\tidx = DIV_ROUND_UP(len16 * 16, T4_EQ_ENTRY_SIZE);\n\n\tt4_ring_sq_db(&qhp->wq, idx, wqe);\n}\n\nstatic int build_rdma_recv(struct c4iw_qp *qhp, union t4_recv_wr *wqe,\n\t\t\t   const struct ib_recv_wr *wr, u8 *len16)\n{\n\tint ret;\n\n\tret = build_isgl((__be64 *)qhp->wq.rq.queue,\n\t\t\t (__be64 *)&qhp->wq.rq.queue[qhp->wq.rq.size],\n\t\t\t &wqe->recv.isgl, wr->sg_list, wr->num_sge, NULL);\n\tif (ret)\n\t\treturn ret;\n\t*len16 = DIV_ROUND_UP(\n\t\tsizeof(wqe->recv) + wr->num_sge * sizeof(struct fw_ri_sge), 16);\n\treturn 0;\n}\n\nstatic int build_srq_recv(union t4_recv_wr *wqe, const struct ib_recv_wr *wr,\n\t\t\t  u8 *len16)\n{\n\tint ret;\n\n\tret = build_isgl((__be64 *)wqe, (__be64 *)(wqe + 1),\n\t\t\t &wqe->recv.isgl, wr->sg_list, wr->num_sge, NULL);\n\tif (ret)\n\t\treturn ret;\n\t*len16 = DIV_ROUND_UP(sizeof(wqe->recv) +\n\t\t\t      wr->num_sge * sizeof(struct fw_ri_sge), 16);\n\treturn 0;\n}\n\nstatic void build_tpte_memreg(struct fw_ri_fr_nsmr_tpte_wr *fr,\n\t\t\t      const struct ib_reg_wr *wr, struct c4iw_mr *mhp,\n\t\t\t      u8 *len16)\n{\n\t__be64 *p = (__be64 *)fr->pbl;\n\n\tfr->r2 = cpu_to_be32(0);\n\tfr->stag = cpu_to_be32(mhp->ibmr.rkey);\n\n\tfr->tpte.valid_to_pdid = cpu_to_be32(FW_RI_TPTE_VALID_F |\n\t\tFW_RI_TPTE_STAGKEY_V((mhp->ibmr.rkey & FW_RI_TPTE_STAGKEY_M)) |\n\t\tFW_RI_TPTE_STAGSTATE_V(1) |\n\t\tFW_RI_TPTE_STAGTYPE_V(FW_RI_STAG_NSMR) |\n\t\tFW_RI_TPTE_PDID_V(mhp->attr.pdid));\n\tfr->tpte.locread_to_qpid = cpu_to_be32(\n\t\tFW_RI_TPTE_PERM_V(c4iw_ib_to_tpt_access(wr->access)) |\n\t\tFW_RI_TPTE_ADDRTYPE_V(FW_RI_VA_BASED_TO) |\n\t\tFW_RI_TPTE_PS_V(ilog2(wr->mr->page_size) - 12));\n\tfr->tpte.nosnoop_pbladdr = cpu_to_be32(FW_RI_TPTE_PBLADDR_V(\n\t\tPBL_OFF(&mhp->rhp->rdev, mhp->attr.pbl_addr)>>3));\n\tfr->tpte.dca_mwbcnt_pstag = cpu_to_be32(0);\n\tfr->tpte.len_hi = cpu_to_be32(0);\n\tfr->tpte.len_lo = cpu_to_be32(mhp->ibmr.length);\n\tfr->tpte.va_hi = cpu_to_be32(mhp->ibmr.iova >> 32);\n\tfr->tpte.va_lo_fbo = cpu_to_be32(mhp->ibmr.iova & 0xffffffff);\n\n\tp[0] = cpu_to_be64((u64)mhp->mpl[0]);\n\tp[1] = cpu_to_be64((u64)mhp->mpl[1]);\n\n\t*len16 = DIV_ROUND_UP(sizeof(*fr), 16);\n}\n\nstatic int build_memreg(struct t4_sq *sq, union t4_wr *wqe,\n\t\t\tconst struct ib_reg_wr *wr, struct c4iw_mr *mhp,\n\t\t\tu8 *len16, bool dsgl_supported)\n{\n\tstruct fw_ri_immd *imdp;\n\t__be64 *p;\n\tint i;\n\tint pbllen = roundup(mhp->mpl_len * sizeof(u64), 32);\n\tint rem;\n\n\tif (mhp->mpl_len > t4_max_fr_depth(dsgl_supported && use_dsgl))\n\t\treturn -EINVAL;\n\n\twqe->fr.qpbinde_to_dcacpu = 0;\n\twqe->fr.pgsz_shift = ilog2(wr->mr->page_size) - 12;\n\twqe->fr.addr_type = FW_RI_VA_BASED_TO;\n\twqe->fr.mem_perms = c4iw_ib_to_tpt_access(wr->access);\n\twqe->fr.len_hi = 0;\n\twqe->fr.len_lo = cpu_to_be32(mhp->ibmr.length);\n\twqe->fr.stag = cpu_to_be32(wr->key);\n\twqe->fr.va_hi = cpu_to_be32(mhp->ibmr.iova >> 32);\n\twqe->fr.va_lo_fbo = cpu_to_be32(mhp->ibmr.iova &\n\t\t\t\t\t0xffffffff);\n\n\tif (dsgl_supported && use_dsgl && (pbllen > max_fr_immd)) {\n\t\tstruct fw_ri_dsgl *sglp;\n\n\t\tfor (i = 0; i < mhp->mpl_len; i++)\n\t\t\tmhp->mpl[i] = (__force u64)cpu_to_be64((u64)mhp->mpl[i]);\n\n\t\tsglp = (struct fw_ri_dsgl *)(&wqe->fr + 1);\n\t\tsglp->op = FW_RI_DATA_DSGL;\n\t\tsglp->r1 = 0;\n\t\tsglp->nsge = cpu_to_be16(1);\n\t\tsglp->addr0 = cpu_to_be64(mhp->mpl_addr);\n\t\tsglp->len0 = cpu_to_be32(pbllen);\n\n\t\t*len16 = DIV_ROUND_UP(sizeof(wqe->fr) + sizeof(*sglp), 16);\n\t} else {\n\t\timdp = (struct fw_ri_immd *)(&wqe->fr + 1);\n\t\timdp->op = FW_RI_DATA_IMMD;\n\t\timdp->r1 = 0;\n\t\timdp->r2 = 0;\n\t\timdp->immdlen = cpu_to_be32(pbllen);\n\t\tp = (__be64 *)(imdp + 1);\n\t\trem = pbllen;\n\t\tfor (i = 0; i < mhp->mpl_len; i++) {\n\t\t\t*p = cpu_to_be64((u64)mhp->mpl[i]);\n\t\t\trem -= sizeof(*p);\n\t\t\tif (++p == (__be64 *)&sq->queue[sq->size])\n\t\t\t\tp = (__be64 *)sq->queue;\n\t\t}\n\t\twhile (rem) {\n\t\t\t*p = 0;\n\t\t\trem -= sizeof(*p);\n\t\t\tif (++p == (__be64 *)&sq->queue[sq->size])\n\t\t\t\tp = (__be64 *)sq->queue;\n\t\t}\n\t\t*len16 = DIV_ROUND_UP(sizeof(wqe->fr) + sizeof(*imdp)\n\t\t\t\t      + pbllen, 16);\n\t}\n\treturn 0;\n}\n\nstatic int build_inv_stag(union t4_wr *wqe, const struct ib_send_wr *wr,\n\t\t\t  u8 *len16)\n{\n\twqe->inv.stag_inv = cpu_to_be32(wr->ex.invalidate_rkey);\n\twqe->inv.r2 = 0;\n\t*len16 = DIV_ROUND_UP(sizeof(wqe->inv), 16);\n\treturn 0;\n}\n\nvoid c4iw_qp_add_ref(struct ib_qp *qp)\n{\n\tpr_debug(\"ib_qp %p\\n\", qp);\n\trefcount_inc(&to_c4iw_qp(qp)->qp_refcnt);\n}\n\nvoid c4iw_qp_rem_ref(struct ib_qp *qp)\n{\n\tpr_debug(\"ib_qp %p\\n\", qp);\n\tif (refcount_dec_and_test(&to_c4iw_qp(qp)->qp_refcnt))\n\t\tcomplete(&to_c4iw_qp(qp)->qp_rel_comp);\n}\n\nstatic void add_to_fc_list(struct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(entry))\n\t\tlist_add_tail(entry, head);\n}\n\nstatic int ring_kernel_sq_db(struct c4iw_qp *qhp, u16 inc)\n{\n\tunsigned long flags;\n\n\txa_lock_irqsave(&qhp->rhp->qps, flags);\n\tspin_lock(&qhp->lock);\n\tif (qhp->rhp->db_state == NORMAL)\n\t\tt4_ring_sq_db(&qhp->wq, inc, NULL);\n\telse {\n\t\tadd_to_fc_list(&qhp->rhp->db_fc_list, &qhp->db_fc_entry);\n\t\tqhp->wq.sq.wq_pidx_inc += inc;\n\t}\n\tspin_unlock(&qhp->lock);\n\txa_unlock_irqrestore(&qhp->rhp->qps, flags);\n\treturn 0;\n}\n\nstatic int ring_kernel_rq_db(struct c4iw_qp *qhp, u16 inc)\n{\n\tunsigned long flags;\n\n\txa_lock_irqsave(&qhp->rhp->qps, flags);\n\tspin_lock(&qhp->lock);\n\tif (qhp->rhp->db_state == NORMAL)\n\t\tt4_ring_rq_db(&qhp->wq, inc, NULL);\n\telse {\n\t\tadd_to_fc_list(&qhp->rhp->db_fc_list, &qhp->db_fc_entry);\n\t\tqhp->wq.rq.wq_pidx_inc += inc;\n\t}\n\tspin_unlock(&qhp->lock);\n\txa_unlock_irqrestore(&qhp->rhp->qps, flags);\n\treturn 0;\n}\n\nstatic int ib_to_fw_opcode(int ib_opcode)\n{\n\tint opcode;\n\n\tswitch (ib_opcode) {\n\tcase IB_WR_SEND_WITH_INV:\n\t\topcode = FW_RI_SEND_WITH_INV;\n\t\tbreak;\n\tcase IB_WR_SEND:\n\t\topcode = FW_RI_SEND;\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE:\n\t\topcode = FW_RI_RDMA_WRITE;\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\topcode = FW_RI_WRITE_IMMEDIATE;\n\t\tbreak;\n\tcase IB_WR_RDMA_READ:\n\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\topcode = FW_RI_READ_REQ;\n\t\tbreak;\n\tcase IB_WR_REG_MR:\n\t\topcode = FW_RI_FAST_REGISTER;\n\t\tbreak;\n\tcase IB_WR_LOCAL_INV:\n\t\topcode = FW_RI_LOCAL_INV;\n\t\tbreak;\n\tdefault:\n\t\topcode = -EINVAL;\n\t}\n\treturn opcode;\n}\n\nstatic int complete_sq_drain_wr(struct c4iw_qp *qhp,\n\t\t\t\tconst struct ib_send_wr *wr)\n{\n\tstruct t4_cqe cqe = {};\n\tstruct c4iw_cq *schp;\n\tunsigned long flag;\n\tstruct t4_cq *cq;\n\tint opcode;\n\n\tschp = to_c4iw_cq(qhp->ibqp.send_cq);\n\tcq = &schp->cq;\n\n\topcode = ib_to_fw_opcode(wr->opcode);\n\tif (opcode < 0)\n\t\treturn opcode;\n\n\tcqe.u.drain_cookie = wr->wr_id;\n\tcqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |\n\t\t\t\t CQE_OPCODE_V(opcode) |\n\t\t\t\t CQE_TYPE_V(1) |\n\t\t\t\t CQE_SWCQE_V(1) |\n\t\t\t\t CQE_DRAIN_V(1) |\n\t\t\t\t CQE_QPID_V(qhp->wq.sq.qid));\n\n\tspin_lock_irqsave(&schp->lock, flag);\n\tcqe.bits_type_ts = cpu_to_be64(CQE_GENBIT_V((u64)cq->gen));\n\tcq->sw_queue[cq->sw_pidx] = cqe;\n\tt4_swcq_produce(cq);\n\tspin_unlock_irqrestore(&schp->lock, flag);\n\n\tif (t4_clear_cq_armed(&schp->cq)) {\n\t\tspin_lock_irqsave(&schp->comp_handler_lock, flag);\n\t\t(*schp->ibcq.comp_handler)(&schp->ibcq,\n\t\t\t\t\t   schp->ibcq.cq_context);\n\t\tspin_unlock_irqrestore(&schp->comp_handler_lock, flag);\n\t}\n\treturn 0;\n}\n\nstatic int complete_sq_drain_wrs(struct c4iw_qp *qhp,\n\t\t\t\t const struct ib_send_wr *wr,\n\t\t\t\t const struct ib_send_wr **bad_wr)\n{\n\tint ret = 0;\n\n\twhile (wr) {\n\t\tret = complete_sq_drain_wr(qhp, wr);\n\t\tif (ret) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\treturn ret;\n}\n\nstatic void complete_rq_drain_wr(struct c4iw_qp *qhp,\n\t\t\t\t const struct ib_recv_wr *wr)\n{\n\tstruct t4_cqe cqe = {};\n\tstruct c4iw_cq *rchp;\n\tunsigned long flag;\n\tstruct t4_cq *cq;\n\n\trchp = to_c4iw_cq(qhp->ibqp.recv_cq);\n\tcq = &rchp->cq;\n\n\tcqe.u.drain_cookie = wr->wr_id;\n\tcqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |\n\t\t\t\t CQE_OPCODE_V(FW_RI_SEND) |\n\t\t\t\t CQE_TYPE_V(0) |\n\t\t\t\t CQE_SWCQE_V(1) |\n\t\t\t\t CQE_DRAIN_V(1) |\n\t\t\t\t CQE_QPID_V(qhp->wq.sq.qid));\n\n\tspin_lock_irqsave(&rchp->lock, flag);\n\tcqe.bits_type_ts = cpu_to_be64(CQE_GENBIT_V((u64)cq->gen));\n\tcq->sw_queue[cq->sw_pidx] = cqe;\n\tt4_swcq_produce(cq);\n\tspin_unlock_irqrestore(&rchp->lock, flag);\n\n\tif (t4_clear_cq_armed(&rchp->cq)) {\n\t\tspin_lock_irqsave(&rchp->comp_handler_lock, flag);\n\t\t(*rchp->ibcq.comp_handler)(&rchp->ibcq,\n\t\t\t\t\t   rchp->ibcq.cq_context);\n\t\tspin_unlock_irqrestore(&rchp->comp_handler_lock, flag);\n\t}\n}\n\nstatic void complete_rq_drain_wrs(struct c4iw_qp *qhp,\n\t\t\t\t  const struct ib_recv_wr *wr)\n{\n\twhile (wr) {\n\t\tcomplete_rq_drain_wr(qhp, wr);\n\t\twr = wr->next;\n\t}\n}\n\nint c4iw_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t   const struct ib_send_wr **bad_wr)\n{\n\tint err = 0;\n\tu8 len16 = 0;\n\tenum fw_wr_opcodes fw_opcode = 0;\n\tenum fw_ri_wr_flags fw_flags;\n\tstruct c4iw_qp *qhp;\n\tstruct c4iw_dev *rhp;\n\tunion t4_wr *wqe = NULL;\n\tu32 num_wrs;\n\tstruct t4_swsqe *swsqe;\n\tunsigned long flag;\n\tu16 idx = 0;\n\n\tqhp = to_c4iw_qp(ibqp);\n\trhp = qhp->rhp;\n\tspin_lock_irqsave(&qhp->lock, flag);\n\n\t \n\tif (qhp->wq.flushed) {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\terr = complete_sq_drain_wrs(qhp, wr, bad_wr);\n\t\treturn err;\n\t}\n\tnum_wrs = t4_sq_avail(&qhp->wq);\n\tif (num_wrs == 0) {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\t*bad_wr = wr;\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (qhp->rhp->rdev.lldi.write_cmpl_support &&\n\t    CHELSIO_CHIP_VERSION(qhp->rhp->rdev.lldi.adapter_type) >=\n\t    CHELSIO_T5 &&\n\t    wr && wr->next && !wr->next->next &&\n\t    wr->opcode == IB_WR_RDMA_WRITE &&\n\t    wr->sg_list[0].length && wr->num_sge <= T4_WRITE_CMPL_MAX_SGL &&\n\t    (wr->next->opcode == IB_WR_SEND ||\n\t    wr->next->opcode == IB_WR_SEND_WITH_INV) &&\n\t    wr->next->sg_list[0].length == T4_WRITE_CMPL_MAX_CQE &&\n\t    wr->next->num_sge == 1 && num_wrs >= 2) {\n\t\tpost_write_cmpl(qhp, wr);\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\treturn 0;\n\t}\n\n\twhile (wr) {\n\t\tif (num_wrs == 0) {\n\t\t\terr = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twqe = (union t4_wr *)((u8 *)qhp->wq.sq.queue +\n\t\t      qhp->wq.sq.wq_pidx * T4_EQ_ENTRY_SIZE);\n\n\t\tfw_flags = 0;\n\t\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\t\tfw_flags |= FW_RI_SOLICITED_EVENT_FLAG;\n\t\tif (wr->send_flags & IB_SEND_SIGNALED || qhp->sq_sig_all)\n\t\t\tfw_flags |= FW_RI_COMPLETION_FLAG;\n\t\tswsqe = &qhp->wq.sq.sw_sq[qhp->wq.sq.pidx];\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\tcase IB_WR_SEND:\n\t\t\tif (wr->send_flags & IB_SEND_FENCE)\n\t\t\t\tfw_flags |= FW_RI_READ_FENCE_FLAG;\n\t\t\tfw_opcode = FW_RI_SEND_WR;\n\t\t\tif (wr->opcode == IB_WR_SEND)\n\t\t\t\tswsqe->opcode = FW_RI_SEND;\n\t\t\telse\n\t\t\t\tswsqe->opcode = FW_RI_SEND_WITH_INV;\n\t\t\terr = build_rdma_send(&qhp->wq.sq, wqe, wr, &len16);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\t\tif (unlikely(!rhp->rdev.lldi.write_w_imm_support)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfw_flags |= FW_RI_RDMA_WRITE_WITH_IMMEDIATE;\n\t\t\tfallthrough;\n\t\tcase IB_WR_RDMA_WRITE:\n\t\t\tfw_opcode = FW_RI_RDMA_WRITE_WR;\n\t\t\tswsqe->opcode = FW_RI_RDMA_WRITE;\n\t\t\terr = build_rdma_write(&qhp->wq.sq, wqe, wr, &len16);\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ:\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\t\tfw_opcode = FW_RI_RDMA_READ_WR;\n\t\t\tswsqe->opcode = FW_RI_READ_REQ;\n\t\t\tif (wr->opcode == IB_WR_RDMA_READ_WITH_INV) {\n\t\t\t\tc4iw_invalidate_mr(rhp, wr->sg_list[0].lkey);\n\t\t\t\tfw_flags = FW_RI_RDMA_READ_INVALIDATE;\n\t\t\t} else {\n\t\t\t\tfw_flags = 0;\n\t\t\t}\n\t\t\terr = build_rdma_read(wqe, wr, &len16);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tswsqe->read_len = wr->sg_list[0].length;\n\t\t\tif (!qhp->wq.sq.oldest_read)\n\t\t\t\tqhp->wq.sq.oldest_read = swsqe;\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR: {\n\t\t\tstruct c4iw_mr *mhp = to_c4iw_mr(reg_wr(wr)->mr);\n\n\t\t\tswsqe->opcode = FW_RI_FAST_REGISTER;\n\t\t\tif (rhp->rdev.lldi.fr_nsmr_tpte_wr_support &&\n\t\t\t    !mhp->attr.state && mhp->mpl_len <= 2) {\n\t\t\t\tfw_opcode = FW_RI_FR_NSMR_TPTE_WR;\n\t\t\t\tbuild_tpte_memreg(&wqe->fr_tpte, reg_wr(wr),\n\t\t\t\t\t\t  mhp, &len16);\n\t\t\t} else {\n\t\t\t\tfw_opcode = FW_RI_FR_NSMR_WR;\n\t\t\t\terr = build_memreg(&qhp->wq.sq, wqe, reg_wr(wr),\n\t\t\t\t       mhp, &len16,\n\t\t\t\t       rhp->rdev.lldi.ulptx_memwrite_dsgl);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmhp->attr.state = 1;\n\t\t\tbreak;\n\t\t}\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\tif (wr->send_flags & IB_SEND_FENCE)\n\t\t\t\tfw_flags |= FW_RI_LOCAL_FENCE_FLAG;\n\t\t\tfw_opcode = FW_RI_INV_LSTAG_WR;\n\t\t\tswsqe->opcode = FW_RI_LOCAL_INV;\n\t\t\terr = build_inv_stag(wqe, wr, &len16);\n\t\t\tc4iw_invalidate_mr(rhp, wr->ex.invalidate_rkey);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_warn(\"%s post of type=%d TBD!\\n\", __func__,\n\t\t\t\twr->opcode);\n\t\t\terr = -EINVAL;\n\t\t}\n\t\tif (err) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\tswsqe->idx = qhp->wq.sq.pidx;\n\t\tswsqe->complete = 0;\n\t\tswsqe->signaled = (wr->send_flags & IB_SEND_SIGNALED) ||\n\t\t\t\t  qhp->sq_sig_all;\n\t\tswsqe->flushed = 0;\n\t\tswsqe->wr_id = wr->wr_id;\n\t\tif (c4iw_wr_log) {\n\t\t\tswsqe->sge_ts = cxgb4_read_sge_timestamp(\n\t\t\t\t\trhp->rdev.lldi.ports[0]);\n\t\t\tswsqe->host_time = ktime_get();\n\t\t}\n\n\t\tinit_wr_hdr(wqe, qhp->wq.sq.pidx, fw_opcode, fw_flags, len16);\n\n\t\tpr_debug(\"cookie 0x%llx pidx 0x%x opcode 0x%x read_len %u\\n\",\n\t\t\t (unsigned long long)wr->wr_id, qhp->wq.sq.pidx,\n\t\t\t swsqe->opcode, swsqe->read_len);\n\t\twr = wr->next;\n\t\tnum_wrs--;\n\t\tt4_sq_produce(&qhp->wq, len16);\n\t\tidx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);\n\t}\n\tif (!rhp->rdev.status_page->db_off) {\n\t\tt4_ring_sq_db(&qhp->wq, idx, wqe);\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t} else {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\tring_kernel_sq_db(qhp, idx);\n\t}\n\treturn err;\n}\n\nint c4iw_post_receive(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tint err = 0;\n\tstruct c4iw_qp *qhp;\n\tunion t4_recv_wr *wqe = NULL;\n\tu32 num_wrs;\n\tu8 len16 = 0;\n\tunsigned long flag;\n\tu16 idx = 0;\n\n\tqhp = to_c4iw_qp(ibqp);\n\tspin_lock_irqsave(&qhp->lock, flag);\n\n\t \n\tif (qhp->wq.flushed) {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\tcomplete_rq_drain_wrs(qhp, wr);\n\t\treturn err;\n\t}\n\tnum_wrs = t4_rq_avail(&qhp->wq);\n\tif (num_wrs == 0) {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\t*bad_wr = wr;\n\t\treturn -ENOMEM;\n\t}\n\twhile (wr) {\n\t\tif (wr->num_sge > T4_MAX_RECV_SGE) {\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twqe = (union t4_recv_wr *)((u8 *)qhp->wq.rq.queue +\n\t\t\t\t\t   qhp->wq.rq.wq_pidx *\n\t\t\t\t\t   T4_EQ_ENTRY_SIZE);\n\t\tif (num_wrs)\n\t\t\terr = build_rdma_recv(qhp, wqe, wr, &len16);\n\t\telse\n\t\t\terr = -ENOMEM;\n\t\tif (err) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tqhp->wq.rq.sw_rq[qhp->wq.rq.pidx].wr_id = wr->wr_id;\n\t\tif (c4iw_wr_log) {\n\t\t\tqhp->wq.rq.sw_rq[qhp->wq.rq.pidx].sge_ts =\n\t\t\t\tcxgb4_read_sge_timestamp(\n\t\t\t\t\t\tqhp->rhp->rdev.lldi.ports[0]);\n\t\t\tqhp->wq.rq.sw_rq[qhp->wq.rq.pidx].host_time =\n\t\t\t\tktime_get();\n\t\t}\n\n\t\twqe->recv.opcode = FW_RI_RECV_WR;\n\t\twqe->recv.r1 = 0;\n\t\twqe->recv.wrid = qhp->wq.rq.pidx;\n\t\twqe->recv.r2[0] = 0;\n\t\twqe->recv.r2[1] = 0;\n\t\twqe->recv.r2[2] = 0;\n\t\twqe->recv.len16 = len16;\n\t\tpr_debug(\"cookie 0x%llx pidx %u\\n\",\n\t\t\t (unsigned long long)wr->wr_id, qhp->wq.rq.pidx);\n\t\tt4_rq_produce(&qhp->wq, len16);\n\t\tidx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);\n\t\twr = wr->next;\n\t\tnum_wrs--;\n\t}\n\tif (!qhp->rhp->rdev.status_page->db_off) {\n\t\tt4_ring_rq_db(&qhp->wq, idx, wqe);\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t} else {\n\t\tspin_unlock_irqrestore(&qhp->lock, flag);\n\t\tring_kernel_rq_db(qhp, idx);\n\t}\n\treturn err;\n}\n\nstatic void defer_srq_wr(struct t4_srq *srq, union t4_recv_wr *wqe,\n\t\t\t u64 wr_id, u8 len16)\n{\n\tstruct t4_srq_pending_wr *pwr = &srq->pending_wrs[srq->pending_pidx];\n\n\tpr_debug(\"%s cidx %u pidx %u wq_pidx %u in_use %u ooo_count %u wr_id 0x%llx pending_cidx %u pending_pidx %u pending_in_use %u\\n\",\n\t\t __func__, srq->cidx, srq->pidx, srq->wq_pidx,\n\t\t srq->in_use, srq->ooo_count,\n\t\t (unsigned long long)wr_id, srq->pending_cidx,\n\t\t srq->pending_pidx, srq->pending_in_use);\n\tpwr->wr_id = wr_id;\n\tpwr->len16 = len16;\n\tmemcpy(&pwr->wqe, wqe, len16 * 16);\n\tt4_srq_produce_pending_wr(srq);\n}\n\nint c4iw_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t       const struct ib_recv_wr **bad_wr)\n{\n\tunion t4_recv_wr *wqe, lwqe;\n\tstruct c4iw_srq *srq;\n\tunsigned long flag;\n\tu8 len16 = 0;\n\tu16 idx = 0;\n\tint err = 0;\n\tu32 num_wrs;\n\n\tsrq = to_c4iw_srq(ibsrq);\n\tspin_lock_irqsave(&srq->lock, flag);\n\tnum_wrs = t4_srq_avail(&srq->wq);\n\tif (num_wrs == 0) {\n\t\tspin_unlock_irqrestore(&srq->lock, flag);\n\t\treturn -ENOMEM;\n\t}\n\twhile (wr) {\n\t\tif (wr->num_sge > T4_MAX_RECV_SGE) {\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twqe = &lwqe;\n\t\tif (num_wrs)\n\t\t\terr = build_srq_recv(wqe, wr, &len16);\n\t\telse\n\t\t\terr = -ENOMEM;\n\t\tif (err) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\twqe->recv.opcode = FW_RI_RECV_WR;\n\t\twqe->recv.r1 = 0;\n\t\twqe->recv.wrid = srq->wq.pidx;\n\t\twqe->recv.r2[0] = 0;\n\t\twqe->recv.r2[1] = 0;\n\t\twqe->recv.r2[2] = 0;\n\t\twqe->recv.len16 = len16;\n\n\t\tif (srq->wq.ooo_count ||\n\t\t    srq->wq.pending_in_use ||\n\t\t    srq->wq.sw_rq[srq->wq.pidx].valid) {\n\t\t\tdefer_srq_wr(&srq->wq, wqe, wr->wr_id, len16);\n\t\t} else {\n\t\t\tsrq->wq.sw_rq[srq->wq.pidx].wr_id = wr->wr_id;\n\t\t\tsrq->wq.sw_rq[srq->wq.pidx].valid = 1;\n\t\t\tc4iw_copy_wr_to_srq(&srq->wq, wqe, len16);\n\t\t\tpr_debug(\"%s cidx %u pidx %u wq_pidx %u in_use %u wr_id 0x%llx\\n\",\n\t\t\t\t __func__, srq->wq.cidx,\n\t\t\t\t srq->wq.pidx, srq->wq.wq_pidx,\n\t\t\t\t srq->wq.in_use,\n\t\t\t\t (unsigned long long)wr->wr_id);\n\t\t\tt4_srq_produce(&srq->wq, len16);\n\t\t\tidx += DIV_ROUND_UP(len16 * 16, T4_EQ_ENTRY_SIZE);\n\t\t}\n\t\twr = wr->next;\n\t\tnum_wrs--;\n\t}\n\tif (idx)\n\t\tt4_ring_srq_db(&srq->wq, idx, len16, wqe);\n\tspin_unlock_irqrestore(&srq->lock, flag);\n\treturn err;\n}\n\nstatic inline void build_term_codes(struct t4_cqe *err_cqe, u8 *layer_type,\n\t\t\t\t    u8 *ecode)\n{\n\tint status;\n\tint tagged;\n\tint opcode;\n\tint rqtype;\n\tint send_inv;\n\n\tif (!err_cqe) {\n\t\t*layer_type = LAYER_RDMAP|DDP_LOCAL_CATA;\n\t\t*ecode = 0;\n\t\treturn;\n\t}\n\n\tstatus = CQE_STATUS(err_cqe);\n\topcode = CQE_OPCODE(err_cqe);\n\trqtype = RQ_TYPE(err_cqe);\n\tsend_inv = (opcode == FW_RI_SEND_WITH_INV) ||\n\t\t   (opcode == FW_RI_SEND_WITH_SE_INV);\n\ttagged = (opcode == FW_RI_RDMA_WRITE) ||\n\t\t (rqtype && (opcode == FW_RI_READ_RESP));\n\n\tswitch (status) {\n\tcase T4_ERR_STAG:\n\t\tif (send_inv) {\n\t\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_OP;\n\t\t\t*ecode = RDMAP_CANT_INV_STAG;\n\t\t} else {\n\t\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\t\t*ecode = RDMAP_INV_STAG;\n\t\t}\n\t\tbreak;\n\tcase T4_ERR_PDID:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\tif ((opcode == FW_RI_SEND_WITH_INV) ||\n\t\t    (opcode == FW_RI_SEND_WITH_SE_INV))\n\t\t\t*ecode = RDMAP_CANT_INV_STAG;\n\t\telse\n\t\t\t*ecode = RDMAP_STAG_NOT_ASSOC;\n\t\tbreak;\n\tcase T4_ERR_QPID:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\t*ecode = RDMAP_STAG_NOT_ASSOC;\n\t\tbreak;\n\tcase T4_ERR_ACCESS:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\t*ecode = RDMAP_ACC_VIOL;\n\t\tbreak;\n\tcase T4_ERR_WRAP:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\t*ecode = RDMAP_TO_WRAP;\n\t\tbreak;\n\tcase T4_ERR_BOUND:\n\t\tif (tagged) {\n\t\t\t*layer_type = LAYER_DDP|DDP_TAGGED_ERR;\n\t\t\t*ecode = DDPT_BASE_BOUNDS;\n\t\t} else {\n\t\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_PROT;\n\t\t\t*ecode = RDMAP_BASE_BOUNDS;\n\t\t}\n\t\tbreak;\n\tcase T4_ERR_INVALIDATE_SHARED_MR:\n\tcase T4_ERR_INVALIDATE_MR_WITH_MW_BOUND:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_OP;\n\t\t*ecode = RDMAP_CANT_INV_STAG;\n\t\tbreak;\n\tcase T4_ERR_ECC:\n\tcase T4_ERR_ECC_PSTAG:\n\tcase T4_ERR_INTERNAL_ERR:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_LOCAL_CATA;\n\t\t*ecode = 0;\n\t\tbreak;\n\tcase T4_ERR_OUT_OF_RQE:\n\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t*ecode = DDPU_INV_MSN_NOBUF;\n\t\tbreak;\n\tcase T4_ERR_PBL_ADDR_BOUND:\n\t\t*layer_type = LAYER_DDP|DDP_TAGGED_ERR;\n\t\t*ecode = DDPT_BASE_BOUNDS;\n\t\tbreak;\n\tcase T4_ERR_CRC:\n\t\t*layer_type = LAYER_MPA|DDP_LLP;\n\t\t*ecode = MPA_CRC_ERR;\n\t\tbreak;\n\tcase T4_ERR_MARKER:\n\t\t*layer_type = LAYER_MPA|DDP_LLP;\n\t\t*ecode = MPA_MARKER_ERR;\n\t\tbreak;\n\tcase T4_ERR_PDU_LEN_ERR:\n\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t*ecode = DDPU_MSG_TOOBIG;\n\t\tbreak;\n\tcase T4_ERR_DDP_VERSION:\n\t\tif (tagged) {\n\t\t\t*layer_type = LAYER_DDP|DDP_TAGGED_ERR;\n\t\t\t*ecode = DDPT_INV_VERS;\n\t\t} else {\n\t\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t\t*ecode = DDPU_INV_VERS;\n\t\t}\n\t\tbreak;\n\tcase T4_ERR_RDMA_VERSION:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_OP;\n\t\t*ecode = RDMAP_INV_VERS;\n\t\tbreak;\n\tcase T4_ERR_OPCODE:\n\t\t*layer_type = LAYER_RDMAP|RDMAP_REMOTE_OP;\n\t\t*ecode = RDMAP_INV_OPCODE;\n\t\tbreak;\n\tcase T4_ERR_DDP_QUEUE_NUM:\n\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t*ecode = DDPU_INV_QN;\n\t\tbreak;\n\tcase T4_ERR_MSN:\n\tcase T4_ERR_MSN_GAP:\n\tcase T4_ERR_MSN_RANGE:\n\tcase T4_ERR_IRD_OVERFLOW:\n\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t*ecode = DDPU_INV_MSN_RANGE;\n\t\tbreak;\n\tcase T4_ERR_TBIT:\n\t\t*layer_type = LAYER_DDP|DDP_LOCAL_CATA;\n\t\t*ecode = 0;\n\t\tbreak;\n\tcase T4_ERR_MO:\n\t\t*layer_type = LAYER_DDP|DDP_UNTAGGED_ERR;\n\t\t*ecode = DDPU_INV_MO;\n\t\tbreak;\n\tdefault:\n\t\t*layer_type = LAYER_RDMAP|DDP_LOCAL_CATA;\n\t\t*ecode = 0;\n\t\tbreak;\n\t}\n}\n\nstatic void post_terminate(struct c4iw_qp *qhp, struct t4_cqe *err_cqe,\n\t\t\t   gfp_t gfp)\n{\n\tstruct fw_ri_wr *wqe;\n\tstruct sk_buff *skb;\n\tstruct terminate_message *term;\n\n\tpr_debug(\"qhp %p qid 0x%x tid %u\\n\", qhp, qhp->wq.sq.qid,\n\t\t qhp->ep->hwtid);\n\n\tskb = skb_dequeue(&qhp->ep->com.ep_skb_list);\n\tif (WARN_ON(!skb))\n\t\treturn;\n\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, qhp->ep->txq_idx);\n\n\twqe = __skb_put_zero(skb, sizeof(*wqe));\n\twqe->op_compl = cpu_to_be32(FW_WR_OP_V(FW_RI_INIT_WR));\n\twqe->flowid_len16 = cpu_to_be32(\n\t\tFW_WR_FLOWID_V(qhp->ep->hwtid) |\n\t\tFW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*wqe), 16)));\n\n\twqe->u.terminate.type = FW_RI_TYPE_TERMINATE;\n\twqe->u.terminate.immdlen = cpu_to_be32(sizeof(*term));\n\tterm = (struct terminate_message *)wqe->u.terminate.termmsg;\n\tif (qhp->attr.layer_etype == (LAYER_MPA|DDP_LLP)) {\n\t\tterm->layer_etype = qhp->attr.layer_etype;\n\t\tterm->ecode = qhp->attr.ecode;\n\t} else\n\t\tbuild_term_codes(err_cqe, &term->layer_etype, &term->ecode);\n\tc4iw_ofld_send(&qhp->rhp->rdev, skb);\n}\n\n \nstatic void __flush_qp(struct c4iw_qp *qhp, struct c4iw_cq *rchp,\n\t\t       struct c4iw_cq *schp)\n{\n\tint count;\n\tint rq_flushed = 0, sq_flushed;\n\tunsigned long flag;\n\n\tpr_debug(\"qhp %p rchp %p schp %p\\n\", qhp, rchp, schp);\n\n\t \n\tspin_lock_irqsave(&rchp->lock, flag);\n\tif (schp != rchp)\n\t\tspin_lock(&schp->lock);\n\tspin_lock(&qhp->lock);\n\n\tif (qhp->wq.flushed) {\n\t\tspin_unlock(&qhp->lock);\n\t\tif (schp != rchp)\n\t\t\tspin_unlock(&schp->lock);\n\t\tspin_unlock_irqrestore(&rchp->lock, flag);\n\t\treturn;\n\t}\n\tqhp->wq.flushed = 1;\n\tt4_set_wq_in_error(&qhp->wq, 0);\n\n\tc4iw_flush_hw_cq(rchp, qhp);\n\tif (!qhp->srq) {\n\t\tc4iw_count_rcqes(&rchp->cq, &qhp->wq, &count);\n\t\trq_flushed = c4iw_flush_rq(&qhp->wq, &rchp->cq, count);\n\t}\n\n\tif (schp != rchp)\n\t\tc4iw_flush_hw_cq(schp, qhp);\n\tsq_flushed = c4iw_flush_sq(qhp);\n\n\tspin_unlock(&qhp->lock);\n\tif (schp != rchp)\n\t\tspin_unlock(&schp->lock);\n\tspin_unlock_irqrestore(&rchp->lock, flag);\n\n\tif (schp == rchp) {\n\t\tif ((rq_flushed || sq_flushed) &&\n\t\t    t4_clear_cq_armed(&rchp->cq)) {\n\t\t\tspin_lock_irqsave(&rchp->comp_handler_lock, flag);\n\t\t\t(*rchp->ibcq.comp_handler)(&rchp->ibcq,\n\t\t\t\t\t\t   rchp->ibcq.cq_context);\n\t\t\tspin_unlock_irqrestore(&rchp->comp_handler_lock, flag);\n\t\t}\n\t} else {\n\t\tif (rq_flushed && t4_clear_cq_armed(&rchp->cq)) {\n\t\t\tspin_lock_irqsave(&rchp->comp_handler_lock, flag);\n\t\t\t(*rchp->ibcq.comp_handler)(&rchp->ibcq,\n\t\t\t\t\t\t   rchp->ibcq.cq_context);\n\t\t\tspin_unlock_irqrestore(&rchp->comp_handler_lock, flag);\n\t\t}\n\t\tif (sq_flushed && t4_clear_cq_armed(&schp->cq)) {\n\t\t\tspin_lock_irqsave(&schp->comp_handler_lock, flag);\n\t\t\t(*schp->ibcq.comp_handler)(&schp->ibcq,\n\t\t\t\t\t\t   schp->ibcq.cq_context);\n\t\t\tspin_unlock_irqrestore(&schp->comp_handler_lock, flag);\n\t\t}\n\t}\n}\n\nstatic void flush_qp(struct c4iw_qp *qhp)\n{\n\tstruct c4iw_cq *rchp, *schp;\n\tunsigned long flag;\n\n\trchp = to_c4iw_cq(qhp->ibqp.recv_cq);\n\tschp = to_c4iw_cq(qhp->ibqp.send_cq);\n\n\tif (qhp->ibqp.uobject) {\n\n\t\t \n\t\tif (qhp->wq.flushed)\n\t\t\treturn;\n\n\t\tqhp->wq.flushed = 1;\n\t\tt4_set_wq_in_error(&qhp->wq, 0);\n\t\tt4_set_cq_in_error(&rchp->cq);\n\t\tspin_lock_irqsave(&rchp->comp_handler_lock, flag);\n\t\t(*rchp->ibcq.comp_handler)(&rchp->ibcq, rchp->ibcq.cq_context);\n\t\tspin_unlock_irqrestore(&rchp->comp_handler_lock, flag);\n\t\tif (schp != rchp) {\n\t\t\tt4_set_cq_in_error(&schp->cq);\n\t\t\tspin_lock_irqsave(&schp->comp_handler_lock, flag);\n\t\t\t(*schp->ibcq.comp_handler)(&schp->ibcq,\n\t\t\t\t\tschp->ibcq.cq_context);\n\t\t\tspin_unlock_irqrestore(&schp->comp_handler_lock, flag);\n\t\t}\n\t\treturn;\n\t}\n\t__flush_qp(qhp, rchp, schp);\n}\n\nstatic int rdma_fini(struct c4iw_dev *rhp, struct c4iw_qp *qhp,\n\t\t     struct c4iw_ep *ep)\n{\n\tstruct fw_ri_wr *wqe;\n\tint ret;\n\tstruct sk_buff *skb;\n\n\tpr_debug(\"qhp %p qid 0x%x tid %u\\n\", qhp, qhp->wq.sq.qid, ep->hwtid);\n\n\tskb = skb_dequeue(&ep->com.ep_skb_list);\n\tif (WARN_ON(!skb))\n\t\treturn -ENOMEM;\n\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, ep->txq_idx);\n\n\twqe = __skb_put_zero(skb, sizeof(*wqe));\n\twqe->op_compl = cpu_to_be32(\n\t\tFW_WR_OP_V(FW_RI_INIT_WR) |\n\t\tFW_WR_COMPL_F);\n\twqe->flowid_len16 = cpu_to_be32(\n\t\tFW_WR_FLOWID_V(ep->hwtid) |\n\t\tFW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*wqe), 16)));\n\twqe->cookie = (uintptr_t)ep->com.wr_waitp;\n\n\twqe->u.fini.type = FW_RI_TYPE_FINI;\n\n\tret = c4iw_ref_send_wait(&rhp->rdev, skb, ep->com.wr_waitp,\n\t\t\t\t qhp->ep->hwtid, qhp->wq.sq.qid, __func__);\n\n\tpr_debug(\"ret %d\\n\", ret);\n\treturn ret;\n}\n\nstatic void build_rtr_msg(u8 p2p_type, struct fw_ri_init *init)\n{\n\tpr_debug(\"p2p_type = %d\\n\", p2p_type);\n\tmemset(&init->u, 0, sizeof(init->u));\n\tswitch (p2p_type) {\n\tcase FW_RI_INIT_P2PTYPE_RDMA_WRITE:\n\t\tinit->u.write.opcode = FW_RI_RDMA_WRITE_WR;\n\t\tinit->u.write.stag_sink = cpu_to_be32(1);\n\t\tinit->u.write.to_sink = cpu_to_be64(1);\n\t\tinit->u.write.u.immd_src[0].op = FW_RI_DATA_IMMD;\n\t\tinit->u.write.len16 = DIV_ROUND_UP(\n\t\t\tsizeof(init->u.write) + sizeof(struct fw_ri_immd), 16);\n\t\tbreak;\n\tcase FW_RI_INIT_P2PTYPE_READ_REQ:\n\t\tinit->u.write.opcode = FW_RI_RDMA_READ_WR;\n\t\tinit->u.read.stag_src = cpu_to_be32(1);\n\t\tinit->u.read.to_src_lo = cpu_to_be32(1);\n\t\tinit->u.read.stag_sink = cpu_to_be32(1);\n\t\tinit->u.read.to_sink_lo = cpu_to_be32(1);\n\t\tinit->u.read.len16 = DIV_ROUND_UP(sizeof(init->u.read), 16);\n\t\tbreak;\n\t}\n}\n\nstatic int rdma_init(struct c4iw_dev *rhp, struct c4iw_qp *qhp)\n{\n\tstruct fw_ri_wr *wqe;\n\tint ret;\n\tstruct sk_buff *skb;\n\n\tpr_debug(\"qhp %p qid 0x%x tid %u ird %u ord %u\\n\", qhp,\n\t\t qhp->wq.sq.qid, qhp->ep->hwtid, qhp->ep->ird, qhp->ep->ord);\n\n\tskb = alloc_skb(sizeof(*wqe), GFP_KERNEL);\n\tif (!skb) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tret = alloc_ird(rhp, qhp->attr.max_ird);\n\tif (ret) {\n\t\tqhp->attr.max_ird = 0;\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, qhp->ep->txq_idx);\n\n\twqe = __skb_put_zero(skb, sizeof(*wqe));\n\twqe->op_compl = cpu_to_be32(\n\t\tFW_WR_OP_V(FW_RI_INIT_WR) |\n\t\tFW_WR_COMPL_F);\n\twqe->flowid_len16 = cpu_to_be32(\n\t\tFW_WR_FLOWID_V(qhp->ep->hwtid) |\n\t\tFW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*wqe), 16)));\n\n\twqe->cookie = (uintptr_t)qhp->ep->com.wr_waitp;\n\n\twqe->u.init.type = FW_RI_TYPE_INIT;\n\twqe->u.init.mpareqbit_p2ptype =\n\t\tFW_RI_WR_MPAREQBIT_V(qhp->attr.mpa_attr.initiator) |\n\t\tFW_RI_WR_P2PTYPE_V(qhp->attr.mpa_attr.p2p_type);\n\twqe->u.init.mpa_attrs = FW_RI_MPA_IETF_ENABLE;\n\tif (qhp->attr.mpa_attr.recv_marker_enabled)\n\t\twqe->u.init.mpa_attrs |= FW_RI_MPA_RX_MARKER_ENABLE;\n\tif (qhp->attr.mpa_attr.xmit_marker_enabled)\n\t\twqe->u.init.mpa_attrs |= FW_RI_MPA_TX_MARKER_ENABLE;\n\tif (qhp->attr.mpa_attr.crc_enabled)\n\t\twqe->u.init.mpa_attrs |= FW_RI_MPA_CRC_ENABLE;\n\n\twqe->u.init.qp_caps = FW_RI_QP_RDMA_READ_ENABLE |\n\t\t\t    FW_RI_QP_RDMA_WRITE_ENABLE |\n\t\t\t    FW_RI_QP_BIND_ENABLE;\n\tif (!qhp->ibqp.uobject)\n\t\twqe->u.init.qp_caps |= FW_RI_QP_FAST_REGISTER_ENABLE |\n\t\t\t\t     FW_RI_QP_STAG0_ENABLE;\n\twqe->u.init.nrqe = cpu_to_be16(t4_rqes_posted(&qhp->wq));\n\twqe->u.init.pdid = cpu_to_be32(qhp->attr.pd);\n\twqe->u.init.qpid = cpu_to_be32(qhp->wq.sq.qid);\n\twqe->u.init.sq_eqid = cpu_to_be32(qhp->wq.sq.qid);\n\tif (qhp->srq) {\n\t\twqe->u.init.rq_eqid = cpu_to_be32(FW_RI_INIT_RQEQID_SRQ |\n\t\t\t\t\t\t  qhp->srq->idx);\n\t} else {\n\t\twqe->u.init.rq_eqid = cpu_to_be32(qhp->wq.rq.qid);\n\t\twqe->u.init.hwrqsize = cpu_to_be32(qhp->wq.rq.rqt_size);\n\t\twqe->u.init.hwrqaddr = cpu_to_be32(qhp->wq.rq.rqt_hwaddr -\n\t\t\t\t\t\t   rhp->rdev.lldi.vr->rq.start);\n\t}\n\twqe->u.init.scqid = cpu_to_be32(qhp->attr.scq);\n\twqe->u.init.rcqid = cpu_to_be32(qhp->attr.rcq);\n\twqe->u.init.ord_max = cpu_to_be32(qhp->attr.max_ord);\n\twqe->u.init.ird_max = cpu_to_be32(qhp->attr.max_ird);\n\twqe->u.init.iss = cpu_to_be32(qhp->ep->snd_seq);\n\twqe->u.init.irs = cpu_to_be32(qhp->ep->rcv_seq);\n\tif (qhp->attr.mpa_attr.initiator)\n\t\tbuild_rtr_msg(qhp->attr.mpa_attr.p2p_type, &wqe->u.init);\n\n\tret = c4iw_ref_send_wait(&rhp->rdev, skb, qhp->ep->com.wr_waitp,\n\t\t\t\t qhp->ep->hwtid, qhp->wq.sq.qid, __func__);\n\tif (!ret)\n\t\tgoto out;\n\n\tfree_ird(rhp, qhp->attr.max_ird);\nout:\n\tpr_debug(\"ret %d\\n\", ret);\n\treturn ret;\n}\n\nint c4iw_modify_qp(struct c4iw_dev *rhp, struct c4iw_qp *qhp,\n\t\t   enum c4iw_qp_attr_mask mask,\n\t\t   struct c4iw_qp_attributes *attrs,\n\t\t   int internal)\n{\n\tint ret = 0;\n\tstruct c4iw_qp_attributes newattr = qhp->attr;\n\tint disconnect = 0;\n\tint terminate = 0;\n\tint abort = 0;\n\tint free = 0;\n\tstruct c4iw_ep *ep = NULL;\n\n\tpr_debug(\"qhp %p sqid 0x%x rqid 0x%x ep %p state %d -> %d\\n\",\n\t\t qhp, qhp->wq.sq.qid, qhp->wq.rq.qid, qhp->ep, qhp->attr.state,\n\t\t (mask & C4IW_QP_ATTR_NEXT_STATE) ? attrs->next_state : -1);\n\n\tmutex_lock(&qhp->mutex);\n\n\t \n\tif (mask & C4IW_QP_ATTR_VALID_MODIFY) {\n\t\tif (qhp->attr.state != C4IW_QP_STATE_IDLE) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (mask & C4IW_QP_ATTR_ENABLE_RDMA_READ)\n\t\t\tnewattr.enable_rdma_read = attrs->enable_rdma_read;\n\t\tif (mask & C4IW_QP_ATTR_ENABLE_RDMA_WRITE)\n\t\t\tnewattr.enable_rdma_write = attrs->enable_rdma_write;\n\t\tif (mask & C4IW_QP_ATTR_ENABLE_RDMA_BIND)\n\t\t\tnewattr.enable_bind = attrs->enable_bind;\n\t\tif (mask & C4IW_QP_ATTR_MAX_ORD) {\n\t\t\tif (attrs->max_ord > c4iw_max_read_depth) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tnewattr.max_ord = attrs->max_ord;\n\t\t}\n\t\tif (mask & C4IW_QP_ATTR_MAX_IRD) {\n\t\t\tif (attrs->max_ird > cur_max_read_depth(rhp)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tnewattr.max_ird = attrs->max_ird;\n\t\t}\n\t\tqhp->attr = newattr;\n\t}\n\n\tif (mask & C4IW_QP_ATTR_SQ_DB) {\n\t\tret = ring_kernel_sq_db(qhp, attrs->sq_db_inc);\n\t\tgoto out;\n\t}\n\tif (mask & C4IW_QP_ATTR_RQ_DB) {\n\t\tret = ring_kernel_rq_db(qhp, attrs->rq_db_inc);\n\t\tgoto out;\n\t}\n\n\tif (!(mask & C4IW_QP_ATTR_NEXT_STATE))\n\t\tgoto out;\n\tif (qhp->attr.state == attrs->next_state)\n\t\tgoto out;\n\n\tswitch (qhp->attr.state) {\n\tcase C4IW_QP_STATE_IDLE:\n\t\tswitch (attrs->next_state) {\n\t\tcase C4IW_QP_STATE_RTS:\n\t\t\tif (!(mask & C4IW_QP_ATTR_LLP_STREAM_HANDLE)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!(mask & C4IW_QP_ATTR_MPA_ATTR)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tqhp->attr.mpa_attr = attrs->mpa_attr;\n\t\t\tqhp->attr.llp_stream_handle = attrs->llp_stream_handle;\n\t\t\tqhp->ep = qhp->attr.llp_stream_handle;\n\t\t\tset_state(qhp, C4IW_QP_STATE_RTS);\n\n\t\t\t \n\t\t\tc4iw_get_ep(&qhp->ep->com);\n\t\t\tret = rdma_init(rhp, qhp);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t\tbreak;\n\t\tcase C4IW_QP_STATE_ERROR:\n\t\t\tset_state(qhp, C4IW_QP_STATE_ERROR);\n\t\t\tflush_qp(qhp);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase C4IW_QP_STATE_RTS:\n\t\tswitch (attrs->next_state) {\n\t\tcase C4IW_QP_STATE_CLOSING:\n\t\t\tt4_set_wq_in_error(&qhp->wq, 0);\n\t\t\tset_state(qhp, C4IW_QP_STATE_CLOSING);\n\t\t\tep = qhp->ep;\n\t\t\tif (!internal) {\n\t\t\t\tabort = 0;\n\t\t\t\tdisconnect = 1;\n\t\t\t\tc4iw_get_ep(&qhp->ep->com);\n\t\t\t}\n\t\t\tret = rdma_fini(rhp, qhp, ep);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t\tbreak;\n\t\tcase C4IW_QP_STATE_TERMINATE:\n\t\t\tt4_set_wq_in_error(&qhp->wq, 0);\n\t\t\tset_state(qhp, C4IW_QP_STATE_TERMINATE);\n\t\t\tqhp->attr.layer_etype = attrs->layer_etype;\n\t\t\tqhp->attr.ecode = attrs->ecode;\n\t\t\tep = qhp->ep;\n\t\t\tif (!internal) {\n\t\t\t\tc4iw_get_ep(&ep->com);\n\t\t\t\tterminate = 1;\n\t\t\t\tdisconnect = 1;\n\t\t\t} else {\n\t\t\t\tterminate = qhp->attr.send_term;\n\t\t\t\tret = rdma_fini(rhp, qhp, ep);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase C4IW_QP_STATE_ERROR:\n\t\t\tt4_set_wq_in_error(&qhp->wq, 0);\n\t\t\tset_state(qhp, C4IW_QP_STATE_ERROR);\n\t\t\tif (!internal) {\n\t\t\t\tdisconnect = 1;\n\t\t\t\tep = qhp->ep;\n\t\t\t\tc4iw_get_ep(&qhp->ep->com);\n\t\t\t}\n\t\t\tgoto err;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase C4IW_QP_STATE_CLOSING:\n\n\t\t \n\t\tif (!internal && (qhp->ibqp.uobject || attrs->next_state !=\n\t\t\t\t  C4IW_QP_STATE_ERROR)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tswitch (attrs->next_state) {\n\t\tcase C4IW_QP_STATE_IDLE:\n\t\t\tflush_qp(qhp);\n\t\t\tset_state(qhp, C4IW_QP_STATE_IDLE);\n\t\t\tqhp->attr.llp_stream_handle = NULL;\n\t\t\tc4iw_put_ep(&qhp->ep->com);\n\t\t\tqhp->ep = NULL;\n\t\t\twake_up(&qhp->wait);\n\t\t\tbreak;\n\t\tcase C4IW_QP_STATE_ERROR:\n\t\t\tgoto err;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tbreak;\n\tcase C4IW_QP_STATE_ERROR:\n\t\tif (attrs->next_state != C4IW_QP_STATE_IDLE) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!t4_sq_empty(&qhp->wq) || !t4_rq_empty(&qhp->wq)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tset_state(qhp, C4IW_QP_STATE_IDLE);\n\t\tbreak;\n\tcase C4IW_QP_STATE_TERMINATE:\n\t\tif (!internal) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tgoto err;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s in a bad state %d\\n\", __func__, qhp->attr.state);\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t\tbreak;\n\t}\n\tgoto out;\nerr:\n\tpr_debug(\"disassociating ep %p qpid 0x%x\\n\", qhp->ep,\n\t\t qhp->wq.sq.qid);\n\n\t \n\tqhp->attr.llp_stream_handle = NULL;\n\tif (!ep)\n\t\tep = qhp->ep;\n\tqhp->ep = NULL;\n\tset_state(qhp, C4IW_QP_STATE_ERROR);\n\tfree = 1;\n\tabort = 1;\n\tflush_qp(qhp);\n\twake_up(&qhp->wait);\nout:\n\tmutex_unlock(&qhp->mutex);\n\n\tif (terminate)\n\t\tpost_terminate(qhp, NULL, internal ? GFP_ATOMIC : GFP_KERNEL);\n\n\t \n\tif (disconnect) {\n\t\tc4iw_ep_disconnect(ep, abort, internal ? GFP_ATOMIC :\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\tc4iw_put_ep(&ep->com);\n\t}\n\n\t \n\tif (free)\n\t\tc4iw_put_ep(&ep->com);\n\tpr_debug(\"exit state %d\\n\", qhp->attr.state);\n\treturn ret;\n}\n\nint c4iw_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_qp *qhp;\n\tstruct c4iw_ucontext *ucontext;\n\tstruct c4iw_qp_attributes attrs;\n\n\tqhp = to_c4iw_qp(ib_qp);\n\trhp = qhp->rhp;\n\tucontext = qhp->ucontext;\n\n\tattrs.next_state = C4IW_QP_STATE_ERROR;\n\tif (qhp->attr.state == C4IW_QP_STATE_TERMINATE)\n\t\tc4iw_modify_qp(rhp, qhp, C4IW_QP_ATTR_NEXT_STATE, &attrs, 1);\n\telse\n\t\tc4iw_modify_qp(rhp, qhp, C4IW_QP_ATTR_NEXT_STATE, &attrs, 0);\n\twait_event(qhp->wait, !qhp->ep);\n\n\txa_lock_irq(&rhp->qps);\n\t__xa_erase(&rhp->qps, qhp->wq.sq.qid);\n\tif (!list_empty(&qhp->db_fc_entry))\n\t\tlist_del_init(&qhp->db_fc_entry);\n\txa_unlock_irq(&rhp->qps);\n\tfree_ird(rhp, qhp->attr.max_ird);\n\n\tc4iw_qp_rem_ref(ib_qp);\n\n\twait_for_completion(&qhp->qp_rel_comp);\n\n\tpr_debug(\"ib_qp %p qpid 0x%0x\\n\", ib_qp, qhp->wq.sq.qid);\n\tpr_debug(\"qhp %p ucontext %p\\n\", qhp, ucontext);\n\n\tdestroy_qp(&rhp->rdev, &qhp->wq,\n\t\t   ucontext ? &ucontext->uctx : &rhp->rdev.uctx, !qhp->srq);\n\n\tc4iw_put_wr_wait(qhp->wr_waitp);\n\treturn 0;\n}\n\nint c4iw_create_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,\n\t\t   struct ib_udata *udata)\n{\n\tstruct ib_pd *pd = qp->pd;\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_qp *qhp = to_c4iw_qp(qp);\n\tstruct c4iw_pd *php;\n\tstruct c4iw_cq *schp;\n\tstruct c4iw_cq *rchp;\n\tstruct c4iw_create_qp_resp uresp;\n\tunsigned int sqsize, rqsize = 0;\n\tstruct c4iw_ucontext *ucontext = rdma_udata_to_drv_context(\n\t\tudata, struct c4iw_ucontext, ibucontext);\n\tint ret;\n\tstruct c4iw_mm_entry *sq_key_mm, *rq_key_mm = NULL, *sq_db_key_mm;\n\tstruct c4iw_mm_entry *rq_db_key_mm = NULL, *ma_sync_key_mm = NULL;\n\n\tif (attrs->qp_type != IB_QPT_RC || attrs->create_flags)\n\t\treturn -EOPNOTSUPP;\n\n\tphp = to_c4iw_pd(pd);\n\trhp = php->rhp;\n\tschp = get_chp(rhp, ((struct c4iw_cq *)attrs->send_cq)->cq.cqid);\n\trchp = get_chp(rhp, ((struct c4iw_cq *)attrs->recv_cq)->cq.cqid);\n\tif (!schp || !rchp)\n\t\treturn -EINVAL;\n\n\tif (attrs->cap.max_inline_data > T4_MAX_SEND_INLINE)\n\t\treturn -EINVAL;\n\n\tif (!attrs->srq) {\n\t\tif (attrs->cap.max_recv_wr > rhp->rdev.hw_queue.t4_max_rq_size)\n\t\t\treturn -E2BIG;\n\t\trqsize = attrs->cap.max_recv_wr + 1;\n\t\tif (rqsize < 8)\n\t\t\trqsize = 8;\n\t}\n\n\tif (attrs->cap.max_send_wr > rhp->rdev.hw_queue.t4_max_sq_size)\n\t\treturn -E2BIG;\n\tsqsize = attrs->cap.max_send_wr + 1;\n\tif (sqsize < 8)\n\t\tsqsize = 8;\n\n\tqhp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!qhp->wr_waitp)\n\t\treturn -ENOMEM;\n\n\tqhp->wq.sq.size = sqsize;\n\tqhp->wq.sq.memsize =\n\t\t(sqsize + rhp->rdev.hw_queue.t4_eq_status_entries) *\n\t\tsizeof(*qhp->wq.sq.queue) + 16 * sizeof(__be64);\n\tqhp->wq.sq.flush_cidx = -1;\n\tif (!attrs->srq) {\n\t\tqhp->wq.rq.size = rqsize;\n\t\tqhp->wq.rq.memsize =\n\t\t\t(rqsize + rhp->rdev.hw_queue.t4_eq_status_entries) *\n\t\t\tsizeof(*qhp->wq.rq.queue);\n\t}\n\n\tif (ucontext) {\n\t\tqhp->wq.sq.memsize = roundup(qhp->wq.sq.memsize, PAGE_SIZE);\n\t\tif (!attrs->srq)\n\t\t\tqhp->wq.rq.memsize =\n\t\t\t\troundup(qhp->wq.rq.memsize, PAGE_SIZE);\n\t}\n\n\tret = create_qp(&rhp->rdev, &qhp->wq, &schp->cq, &rchp->cq,\n\t\t\tucontext ? &ucontext->uctx : &rhp->rdev.uctx,\n\t\t\tqhp->wr_waitp, !attrs->srq);\n\tif (ret)\n\t\tgoto err_free_wr_wait;\n\n\tattrs->cap.max_recv_wr = rqsize - 1;\n\tattrs->cap.max_send_wr = sqsize - 1;\n\tattrs->cap.max_inline_data = T4_MAX_SEND_INLINE;\n\n\tqhp->rhp = rhp;\n\tqhp->attr.pd = php->pdid;\n\tqhp->attr.scq = ((struct c4iw_cq *) attrs->send_cq)->cq.cqid;\n\tqhp->attr.rcq = ((struct c4iw_cq *) attrs->recv_cq)->cq.cqid;\n\tqhp->attr.sq_num_entries = attrs->cap.max_send_wr;\n\tqhp->attr.sq_max_sges = attrs->cap.max_send_sge;\n\tqhp->attr.sq_max_sges_rdma_write = attrs->cap.max_send_sge;\n\tif (!attrs->srq) {\n\t\tqhp->attr.rq_num_entries = attrs->cap.max_recv_wr;\n\t\tqhp->attr.rq_max_sges = attrs->cap.max_recv_sge;\n\t}\n\tqhp->attr.state = C4IW_QP_STATE_IDLE;\n\tqhp->attr.next_state = C4IW_QP_STATE_IDLE;\n\tqhp->attr.enable_rdma_read = 1;\n\tqhp->attr.enable_rdma_write = 1;\n\tqhp->attr.enable_bind = 1;\n\tqhp->attr.max_ord = 0;\n\tqhp->attr.max_ird = 0;\n\tqhp->sq_sig_all = attrs->sq_sig_type == IB_SIGNAL_ALL_WR;\n\tspin_lock_init(&qhp->lock);\n\tmutex_init(&qhp->mutex);\n\tinit_waitqueue_head(&qhp->wait);\n\tinit_completion(&qhp->qp_rel_comp);\n\trefcount_set(&qhp->qp_refcnt, 1);\n\n\tret = xa_insert_irq(&rhp->qps, qhp->wq.sq.qid, qhp, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err_destroy_qp;\n\n\tif (udata && ucontext) {\n\t\tsq_key_mm = kmalloc(sizeof(*sq_key_mm), GFP_KERNEL);\n\t\tif (!sq_key_mm) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_remove_handle;\n\t\t}\n\t\tif (!attrs->srq) {\n\t\t\trq_key_mm = kmalloc(sizeof(*rq_key_mm), GFP_KERNEL);\n\t\t\tif (!rq_key_mm) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err_free_sq_key;\n\t\t\t}\n\t\t}\n\t\tsq_db_key_mm = kmalloc(sizeof(*sq_db_key_mm), GFP_KERNEL);\n\t\tif (!sq_db_key_mm) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_rq_key;\n\t\t}\n\t\tif (!attrs->srq) {\n\t\t\trq_db_key_mm =\n\t\t\t\tkmalloc(sizeof(*rq_db_key_mm), GFP_KERNEL);\n\t\t\tif (!rq_db_key_mm) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err_free_sq_db_key;\n\t\t\t}\n\t\t}\n\t\tmemset(&uresp, 0, sizeof(uresp));\n\t\tif (t4_sq_onchip(&qhp->wq.sq)) {\n\t\t\tma_sync_key_mm = kmalloc(sizeof(*ma_sync_key_mm),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (!ma_sync_key_mm) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err_free_rq_db_key;\n\t\t\t}\n\t\t\turesp.flags = C4IW_QPF_ONCHIP;\n\t\t}\n\t\tif (rhp->rdev.lldi.write_w_imm_support)\n\t\t\turesp.flags |= C4IW_QPF_WRITE_W_IMM;\n\t\turesp.qid_mask = rhp->rdev.qpmask;\n\t\turesp.sqid = qhp->wq.sq.qid;\n\t\turesp.sq_size = qhp->wq.sq.size;\n\t\turesp.sq_memsize = qhp->wq.sq.memsize;\n\t\tif (!attrs->srq) {\n\t\t\turesp.rqid = qhp->wq.rq.qid;\n\t\t\turesp.rq_size = qhp->wq.rq.size;\n\t\t\turesp.rq_memsize = qhp->wq.rq.memsize;\n\t\t}\n\t\tspin_lock(&ucontext->mmap_lock);\n\t\tif (ma_sync_key_mm) {\n\t\t\turesp.ma_sync_key = ucontext->key;\n\t\t\tucontext->key += PAGE_SIZE;\n\t\t}\n\t\turesp.sq_key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\tif (!attrs->srq) {\n\t\t\turesp.rq_key = ucontext->key;\n\t\t\tucontext->key += PAGE_SIZE;\n\t\t}\n\t\turesp.sq_db_gts_key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\tif (!attrs->srq) {\n\t\t\turesp.rq_db_gts_key = ucontext->key;\n\t\t\tucontext->key += PAGE_SIZE;\n\t\t}\n\t\tspin_unlock(&ucontext->mmap_lock);\n\t\tret = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (ret)\n\t\t\tgoto err_free_ma_sync_key;\n\t\tsq_key_mm->key = uresp.sq_key;\n\t\tsq_key_mm->addr = qhp->wq.sq.phys_addr;\n\t\tsq_key_mm->len = PAGE_ALIGN(qhp->wq.sq.memsize);\n\t\tinsert_mmap(ucontext, sq_key_mm);\n\t\tif (!attrs->srq) {\n\t\t\trq_key_mm->key = uresp.rq_key;\n\t\t\trq_key_mm->addr = virt_to_phys(qhp->wq.rq.queue);\n\t\t\trq_key_mm->len = PAGE_ALIGN(qhp->wq.rq.memsize);\n\t\t\tinsert_mmap(ucontext, rq_key_mm);\n\t\t}\n\t\tsq_db_key_mm->key = uresp.sq_db_gts_key;\n\t\tsq_db_key_mm->addr = (u64)(unsigned long)qhp->wq.sq.bar2_pa;\n\t\tsq_db_key_mm->len = PAGE_SIZE;\n\t\tinsert_mmap(ucontext, sq_db_key_mm);\n\t\tif (!attrs->srq) {\n\t\t\trq_db_key_mm->key = uresp.rq_db_gts_key;\n\t\t\trq_db_key_mm->addr =\n\t\t\t\t(u64)(unsigned long)qhp->wq.rq.bar2_pa;\n\t\t\trq_db_key_mm->len = PAGE_SIZE;\n\t\t\tinsert_mmap(ucontext, rq_db_key_mm);\n\t\t}\n\t\tif (ma_sync_key_mm) {\n\t\t\tma_sync_key_mm->key = uresp.ma_sync_key;\n\t\t\tma_sync_key_mm->addr =\n\t\t\t\t(pci_resource_start(rhp->rdev.lldi.pdev, 0) +\n\t\t\t\tPCIE_MA_SYNC_A) & PAGE_MASK;\n\t\t\tma_sync_key_mm->len = PAGE_SIZE;\n\t\t\tinsert_mmap(ucontext, ma_sync_key_mm);\n\t\t}\n\n\t\tqhp->ucontext = ucontext;\n\t}\n\tif (!attrs->srq) {\n\t\tqhp->wq.qp_errp =\n\t\t\t&qhp->wq.rq.queue[qhp->wq.rq.size].status.qp_err;\n\t} else {\n\t\tqhp->wq.qp_errp =\n\t\t\t&qhp->wq.sq.queue[qhp->wq.sq.size].status.qp_err;\n\t\tqhp->wq.srqidxp =\n\t\t\t&qhp->wq.sq.queue[qhp->wq.sq.size].status.srqidx;\n\t}\n\n\tqhp->ibqp.qp_num = qhp->wq.sq.qid;\n\tif (attrs->srq)\n\t\tqhp->srq = to_c4iw_srq(attrs->srq);\n\tINIT_LIST_HEAD(&qhp->db_fc_entry);\n\tpr_debug(\"sq id %u size %u memsize %zu num_entries %u rq id %u size %u memsize %zu num_entries %u\\n\",\n\t\t qhp->wq.sq.qid, qhp->wq.sq.size, qhp->wq.sq.memsize,\n\t\t attrs->cap.max_send_wr, qhp->wq.rq.qid, qhp->wq.rq.size,\n\t\t qhp->wq.rq.memsize, attrs->cap.max_recv_wr);\n\treturn 0;\nerr_free_ma_sync_key:\n\tkfree(ma_sync_key_mm);\nerr_free_rq_db_key:\n\tif (!attrs->srq)\n\t\tkfree(rq_db_key_mm);\nerr_free_sq_db_key:\n\tkfree(sq_db_key_mm);\nerr_free_rq_key:\n\tif (!attrs->srq)\n\t\tkfree(rq_key_mm);\nerr_free_sq_key:\n\tkfree(sq_key_mm);\nerr_remove_handle:\n\txa_erase_irq(&rhp->qps, qhp->wq.sq.qid);\nerr_destroy_qp:\n\tdestroy_qp(&rhp->rdev, &qhp->wq,\n\t\t   ucontext ? &ucontext->uctx : &rhp->rdev.uctx, !attrs->srq);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(qhp->wr_waitp);\n\treturn ret;\n}\n\nint c4iw_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t      int attr_mask, struct ib_udata *udata)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_qp *qhp;\n\tenum c4iw_qp_attr_mask mask = 0;\n\tstruct c4iw_qp_attributes attrs = {};\n\n\tpr_debug(\"ib_qp %p\\n\", ibqp);\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif ((attr_mask & IB_QP_STATE) && (attr->qp_state == IB_QPS_RTR))\n\t\tattr_mask &= ~IB_QP_STATE;\n\n\t \n\tif (!attr_mask)\n\t\treturn 0;\n\n\tqhp = to_c4iw_qp(ibqp);\n\trhp = qhp->rhp;\n\n\tattrs.next_state = c4iw_convert_state(attr->qp_state);\n\tattrs.enable_rdma_read = (attr->qp_access_flags &\n\t\t\t       IB_ACCESS_REMOTE_READ) ?  1 : 0;\n\tattrs.enable_rdma_write = (attr->qp_access_flags &\n\t\t\t\tIB_ACCESS_REMOTE_WRITE) ? 1 : 0;\n\tattrs.enable_bind = (attr->qp_access_flags & IB_ACCESS_MW_BIND) ? 1 : 0;\n\n\n\tmask |= (attr_mask & IB_QP_STATE) ? C4IW_QP_ATTR_NEXT_STATE : 0;\n\tmask |= (attr_mask & IB_QP_ACCESS_FLAGS) ?\n\t\t\t(C4IW_QP_ATTR_ENABLE_RDMA_READ |\n\t\t\t C4IW_QP_ATTR_ENABLE_RDMA_WRITE |\n\t\t\t C4IW_QP_ATTR_ENABLE_RDMA_BIND) : 0;\n\n\t \n\tattrs.sq_db_inc = attr->sq_psn;\n\tattrs.rq_db_inc = attr->rq_psn;\n\tmask |= (attr_mask & IB_QP_SQ_PSN) ? C4IW_QP_ATTR_SQ_DB : 0;\n\tmask |= (attr_mask & IB_QP_RQ_PSN) ? C4IW_QP_ATTR_RQ_DB : 0;\n\tif (!is_t4(to_c4iw_qp(ibqp)->rhp->rdev.lldi.adapter_type) &&\n\t    (mask & (C4IW_QP_ATTR_SQ_DB|C4IW_QP_ATTR_RQ_DB)))\n\t\treturn -EINVAL;\n\n\treturn c4iw_modify_qp(rhp, qhp, mask, &attrs, 0);\n}\n\nstruct ib_qp *c4iw_get_qp(struct ib_device *dev, int qpn)\n{\n\tpr_debug(\"ib_dev %p qpn 0x%x\\n\", dev, qpn);\n\treturn (struct ib_qp *)get_qhp(to_c4iw_dev(dev), qpn);\n}\n\nvoid c4iw_dispatch_srq_limit_reached_event(struct c4iw_srq *srq)\n{\n\tstruct ib_event event = {};\n\n\tevent.device = &srq->rhp->ibdev;\n\tevent.element.srq = &srq->ibsrq;\n\tevent.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\tib_dispatch_event(&event);\n}\n\nint c4iw_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *attr,\n\t\t    enum ib_srq_attr_mask srq_attr_mask,\n\t\t    struct ib_udata *udata)\n{\n\tstruct c4iw_srq *srq = to_c4iw_srq(ib_srq);\n\tint ret = 0;\n\n\t \n\tif (udata && !srq_attr_mask) {\n\t\tc4iw_dispatch_srq_limit_reached_event(srq);\n\t\tgoto out;\n\t}\n\n\t \n\tif (srq_attr_mask & IB_SRQ_MAX_WR) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!udata && (srq_attr_mask & IB_SRQ_LIMIT)) {\n\t\tsrq->armed = true;\n\t\tsrq->srq_limit = attr->srq_limit;\n\t}\nout:\n\treturn ret;\n}\n\nint c4iw_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t     int attr_mask, struct ib_qp_init_attr *init_attr)\n{\n\tstruct c4iw_qp *qhp = to_c4iw_qp(ibqp);\n\n\tmemset(attr, 0, sizeof(*attr));\n\tmemset(init_attr, 0, sizeof(*init_attr));\n\tattr->qp_state = to_ib_qp_state(qhp->attr.state);\n\tattr->cur_qp_state = to_ib_qp_state(qhp->attr.state);\n\tinit_attr->cap.max_send_wr = qhp->attr.sq_num_entries;\n\tinit_attr->cap.max_recv_wr = qhp->attr.rq_num_entries;\n\tinit_attr->cap.max_send_sge = qhp->attr.sq_max_sges;\n\tinit_attr->cap.max_recv_sge = qhp->attr.rq_max_sges;\n\tinit_attr->cap.max_inline_data = T4_MAX_SEND_INLINE;\n\tinit_attr->sq_sig_type = qhp->sq_sig_all ? IB_SIGNAL_ALL_WR : IB_SIGNAL_REQ_WR;\n\treturn 0;\n}\n\nstatic void free_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,\n\t\t\t   struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct c4iw_rdev *rdev = &srq->rhp->rdev;\n\tstruct sk_buff *skb = srq->destroy_skb;\n\tstruct t4_srq *wq = &srq->wq;\n\tstruct fw_ri_res_wr *res_wr;\n\tstruct fw_ri_res *res;\n\tint wr_len;\n\n\twr_len = sizeof(*res_wr) + sizeof(*res);\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\tres_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);\n\tmemset(res_wr, 0, wr_len);\n\tres_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |\n\t\t\tFW_RI_RES_WR_NRES_V(1) |\n\t\t\tFW_WR_COMPL_F);\n\tres_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));\n\tres_wr->cookie = (uintptr_t)wr_waitp;\n\tres = res_wr->res;\n\tres->u.srq.restype = FW_RI_RES_TYPE_SRQ;\n\tres->u.srq.op = FW_RI_RES_OP_RESET;\n\tres->u.srq.srqid = cpu_to_be32(srq->idx);\n\tres->u.srq.eqid = cpu_to_be32(wq->qid);\n\n\tc4iw_init_wr_wait(wr_waitp);\n\tc4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);\n\n\tdma_free_coherent(&rdev->lldi.pdev->dev,\n\t\t\t  wq->memsize, wq->queue,\n\t\t\tdma_unmap_addr(wq, mapping));\n\tc4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);\n\tkfree(wq->sw_rq);\n\tc4iw_put_qpid(rdev, wq->qid, uctx);\n}\n\nstatic int alloc_srq_queue(struct c4iw_srq *srq, struct c4iw_dev_ucontext *uctx,\n\t\t\t   struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct c4iw_rdev *rdev = &srq->rhp->rdev;\n\tint user = (uctx != &rdev->uctx);\n\tstruct t4_srq *wq = &srq->wq;\n\tstruct fw_ri_res_wr *res_wr;\n\tstruct fw_ri_res *res;\n\tstruct sk_buff *skb;\n\tint wr_len;\n\tint eqsize;\n\tint ret = -ENOMEM;\n\n\twq->qid = c4iw_get_qpid(rdev, uctx);\n\tif (!wq->qid)\n\t\tgoto err;\n\n\tif (!user) {\n\t\twq->sw_rq = kcalloc(wq->size, sizeof(*wq->sw_rq),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (!wq->sw_rq)\n\t\t\tgoto err_put_qpid;\n\t\twq->pending_wrs = kcalloc(srq->wq.size,\n\t\t\t\t\t  sizeof(*srq->wq.pending_wrs),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!wq->pending_wrs)\n\t\t\tgoto err_free_sw_rq;\n\t}\n\n\twq->rqt_size = wq->size;\n\twq->rqt_hwaddr = c4iw_rqtpool_alloc(rdev, wq->rqt_size);\n\tif (!wq->rqt_hwaddr)\n\t\tgoto err_free_pending_wrs;\n\twq->rqt_abs_idx = (wq->rqt_hwaddr - rdev->lldi.vr->rq.start) >>\n\t\tT4_RQT_ENTRY_SHIFT;\n\n\twq->queue = dma_alloc_coherent(&rdev->lldi.pdev->dev, wq->memsize,\n\t\t\t\t       &wq->dma_addr, GFP_KERNEL);\n\tif (!wq->queue)\n\t\tgoto err_free_rqtpool;\n\n\tdma_unmap_addr_set(wq, mapping, wq->dma_addr);\n\n\twq->bar2_va = c4iw_bar2_addrs(rdev, wq->qid, CXGB4_BAR2_QTYPE_EGRESS,\n\t\t\t\t      &wq->bar2_qid,\n\t\t\tuser ? &wq->bar2_pa : NULL);\n\n\t \n\n\tif (user && !wq->bar2_va) {\n\t\tpr_warn(MOD \"%s: srqid %u not in BAR2 range.\\n\",\n\t\t\tpci_name(rdev->lldi.pdev), wq->qid);\n\t\tret = -EINVAL;\n\t\tgoto err_free_queue;\n\t}\n\n\t \n\twr_len = sizeof(*res_wr) + sizeof(*res);\n\n\tskb = alloc_skb(wr_len, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto err_free_queue;\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\tres_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);\n\tmemset(res_wr, 0, wr_len);\n\tres_wr->op_nres = cpu_to_be32(FW_WR_OP_V(FW_RI_RES_WR) |\n\t\t\tFW_RI_RES_WR_NRES_V(1) |\n\t\t\tFW_WR_COMPL_F);\n\tres_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));\n\tres_wr->cookie = (uintptr_t)wr_waitp;\n\tres = res_wr->res;\n\tres->u.srq.restype = FW_RI_RES_TYPE_SRQ;\n\tres->u.srq.op = FW_RI_RES_OP_WRITE;\n\n\t \n\teqsize = wq->size * T4_RQ_NUM_SLOTS +\n\t\trdev->hw_queue.t4_eq_status_entries;\n\tres->u.srq.eqid = cpu_to_be32(wq->qid);\n\tres->u.srq.fetchszm_to_iqid =\n\t\t\t\t\t\t \n\t\tcpu_to_be32(FW_RI_RES_WR_HOSTFCMODE_V(0) |\n\t\tFW_RI_RES_WR_CPRIO_V(0) |        \n\t\tFW_RI_RES_WR_PCIECHN_V(0) |      \n\t\tFW_RI_RES_WR_FETCHRO_V(0));      \n\tres->u.srq.dcaen_to_eqsize =\n\t\tcpu_to_be32(FW_RI_RES_WR_DCAEN_V(0) |\n\t\tFW_RI_RES_WR_DCACPU_V(0) |\n\t\tFW_RI_RES_WR_FBMIN_V(2) |\n\t\tFW_RI_RES_WR_FBMAX_V(3) |\n\t\tFW_RI_RES_WR_CIDXFTHRESHO_V(0) |\n\t\tFW_RI_RES_WR_CIDXFTHRESH_V(0) |\n\t\tFW_RI_RES_WR_EQSIZE_V(eqsize));\n\tres->u.srq.eqaddr = cpu_to_be64(wq->dma_addr);\n\tres->u.srq.srqid = cpu_to_be32(srq->idx);\n\tres->u.srq.pdid = cpu_to_be32(srq->pdid);\n\tres->u.srq.hwsrqsize = cpu_to_be32(wq->rqt_size);\n\tres->u.srq.hwsrqaddr = cpu_to_be32(wq->rqt_hwaddr -\n\t\t\trdev->lldi.vr->rq.start);\n\n\tc4iw_init_wr_wait(wr_waitp);\n\n\tret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, wq->qid, __func__);\n\tif (ret)\n\t\tgoto err_free_queue;\n\n\tpr_debug(\"%s srq %u eqid %u pdid %u queue va %p pa 0x%llx\\n\"\n\t\t\t\" bar2_addr %p rqt addr 0x%x size %d\\n\",\n\t\t\t__func__, srq->idx, wq->qid, srq->pdid, wq->queue,\n\t\t\t(u64)virt_to_phys(wq->queue), wq->bar2_va,\n\t\t\twq->rqt_hwaddr, wq->rqt_size);\n\n\treturn 0;\nerr_free_queue:\n\tdma_free_coherent(&rdev->lldi.pdev->dev,\n\t\t\t  wq->memsize, wq->queue,\n\t\t\tdma_unmap_addr(wq, mapping));\nerr_free_rqtpool:\n\tc4iw_rqtpool_free(rdev, wq->rqt_hwaddr, wq->rqt_size);\nerr_free_pending_wrs:\n\tif (!user)\n\t\tkfree(wq->pending_wrs);\nerr_free_sw_rq:\n\tif (!user)\n\t\tkfree(wq->sw_rq);\nerr_put_qpid:\n\tc4iw_put_qpid(rdev, wq->qid, uctx);\nerr:\n\treturn ret;\n}\n\nvoid c4iw_copy_wr_to_srq(struct t4_srq *srq, union t4_recv_wr *wqe, u8 len16)\n{\n\tu64 *src, *dst;\n\n\tsrc = (u64 *)wqe;\n\tdst = (u64 *)((u8 *)srq->queue + srq->wq_pidx * T4_EQ_ENTRY_SIZE);\n\twhile (len16) {\n\t\t*dst++ = *src++;\n\t\tif (dst >= (u64 *)&srq->queue[srq->size])\n\t\t\tdst = (u64 *)srq->queue;\n\t\t*dst++ = *src++;\n\t\tif (dst >= (u64 *)&srq->queue[srq->size])\n\t\t\tdst = (u64 *)srq->queue;\n\t\tlen16--;\n\t}\n}\n\nint c4iw_create_srq(struct ib_srq *ib_srq, struct ib_srq_init_attr *attrs,\n\t\t\t       struct ib_udata *udata)\n{\n\tstruct ib_pd *pd = ib_srq->pd;\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_srq *srq = to_c4iw_srq(ib_srq);\n\tstruct c4iw_pd *php;\n\tstruct c4iw_create_srq_resp uresp;\n\tstruct c4iw_ucontext *ucontext;\n\tstruct c4iw_mm_entry *srq_key_mm, *srq_db_key_mm;\n\tint rqsize;\n\tint ret;\n\tint wr_len;\n\n\tif (attrs->srq_type != IB_SRQT_BASIC)\n\t\treturn -EOPNOTSUPP;\n\n\tpr_debug(\"%s ib_pd %p\\n\", __func__, pd);\n\n\tphp = to_c4iw_pd(pd);\n\trhp = php->rhp;\n\n\tif (!rhp->rdev.lldi.vr->srq.size)\n\t\treturn -EINVAL;\n\tif (attrs->attr.max_wr > rhp->rdev.hw_queue.t4_max_rq_size)\n\t\treturn -E2BIG;\n\tif (attrs->attr.max_sge > T4_MAX_RECV_SGE)\n\t\treturn -E2BIG;\n\n\t \n\trqsize = attrs->attr.max_wr + 1;\n\trqsize = roundup_pow_of_two(max_t(u16, rqsize, 16));\n\n\tucontext = rdma_udata_to_drv_context(udata, struct c4iw_ucontext,\n\t\t\t\t\t     ibucontext);\n\n\tsrq->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!srq->wr_waitp)\n\t\treturn -ENOMEM;\n\n\tsrq->idx = c4iw_alloc_srq_idx(&rhp->rdev);\n\tif (srq->idx < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_wr_wait;\n\t}\n\n\twr_len = sizeof(struct fw_ri_res_wr) + sizeof(struct fw_ri_res);\n\tsrq->destroy_skb = alloc_skb(wr_len, GFP_KERNEL);\n\tif (!srq->destroy_skb) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_srq_idx;\n\t}\n\n\tsrq->rhp = rhp;\n\tsrq->pdid = php->pdid;\n\n\tsrq->wq.size = rqsize;\n\tsrq->wq.memsize =\n\t\t(rqsize + rhp->rdev.hw_queue.t4_eq_status_entries) *\n\t\tsizeof(*srq->wq.queue);\n\tif (ucontext)\n\t\tsrq->wq.memsize = roundup(srq->wq.memsize, PAGE_SIZE);\n\n\tret = alloc_srq_queue(srq, ucontext ? &ucontext->uctx :\n\t\t\t&rhp->rdev.uctx, srq->wr_waitp);\n\tif (ret)\n\t\tgoto err_free_skb;\n\tattrs->attr.max_wr = rqsize - 1;\n\n\tif (CHELSIO_CHIP_VERSION(rhp->rdev.lldi.adapter_type) > CHELSIO_T6)\n\t\tsrq->flags = T4_SRQ_LIMIT_SUPPORT;\n\n\tif (udata) {\n\t\tsrq_key_mm = kmalloc(sizeof(*srq_key_mm), GFP_KERNEL);\n\t\tif (!srq_key_mm) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_queue;\n\t\t}\n\t\tsrq_db_key_mm = kmalloc(sizeof(*srq_db_key_mm), GFP_KERNEL);\n\t\tif (!srq_db_key_mm) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_srq_key_mm;\n\t\t}\n\t\tmemset(&uresp, 0, sizeof(uresp));\n\t\turesp.flags = srq->flags;\n\t\turesp.qid_mask = rhp->rdev.qpmask;\n\t\turesp.srqid = srq->wq.qid;\n\t\turesp.srq_size = srq->wq.size;\n\t\turesp.srq_memsize = srq->wq.memsize;\n\t\turesp.rqt_abs_idx = srq->wq.rqt_abs_idx;\n\t\tspin_lock(&ucontext->mmap_lock);\n\t\turesp.srq_key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\turesp.srq_db_gts_key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\tspin_unlock(&ucontext->mmap_lock);\n\t\tret = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (ret)\n\t\t\tgoto err_free_srq_db_key_mm;\n\t\tsrq_key_mm->key = uresp.srq_key;\n\t\tsrq_key_mm->addr = virt_to_phys(srq->wq.queue);\n\t\tsrq_key_mm->len = PAGE_ALIGN(srq->wq.memsize);\n\t\tinsert_mmap(ucontext, srq_key_mm);\n\t\tsrq_db_key_mm->key = uresp.srq_db_gts_key;\n\t\tsrq_db_key_mm->addr = (u64)(unsigned long)srq->wq.bar2_pa;\n\t\tsrq_db_key_mm->len = PAGE_SIZE;\n\t\tinsert_mmap(ucontext, srq_db_key_mm);\n\t}\n\n\tpr_debug(\"%s srq qid %u idx %u size %u memsize %lu num_entries %u\\n\",\n\t\t __func__, srq->wq.qid, srq->idx, srq->wq.size,\n\t\t\t(unsigned long)srq->wq.memsize, attrs->attr.max_wr);\n\n\tspin_lock_init(&srq->lock);\n\treturn 0;\n\nerr_free_srq_db_key_mm:\n\tkfree(srq_db_key_mm);\nerr_free_srq_key_mm:\n\tkfree(srq_key_mm);\nerr_free_queue:\n\tfree_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,\n\t\t       srq->wr_waitp);\nerr_free_skb:\n\tkfree_skb(srq->destroy_skb);\nerr_free_srq_idx:\n\tc4iw_free_srq_idx(&rhp->rdev, srq->idx);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(srq->wr_waitp);\n\treturn ret;\n}\n\nint c4iw_destroy_srq(struct ib_srq *ibsrq, struct ib_udata *udata)\n{\n\tstruct c4iw_dev *rhp;\n\tstruct c4iw_srq *srq;\n\tstruct c4iw_ucontext *ucontext;\n\n\tsrq = to_c4iw_srq(ibsrq);\n\trhp = srq->rhp;\n\n\tpr_debug(\"%s id %d\\n\", __func__, srq->wq.qid);\n\tucontext = rdma_udata_to_drv_context(udata, struct c4iw_ucontext,\n\t\t\t\t\t     ibucontext);\n\tfree_srq_queue(srq, ucontext ? &ucontext->uctx : &rhp->rdev.uctx,\n\t\t       srq->wr_waitp);\n\tc4iw_free_srq_idx(&rhp->rdev, srq->idx);\n\tc4iw_put_wr_wait(srq->wr_waitp);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}