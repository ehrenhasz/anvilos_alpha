{
  "module_name": "resource.c",
  "hash_id": "7840d2613e9192550c6fe9aad209c38609a4ea7d17ebfe515f4b44ee020efb60",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/resource.c",
  "human_readable_source": " \n \n#include <linux/spinlock.h>\n#include <linux/genalloc.h>\n#include <linux/ratelimit.h>\n#include \"iw_cxgb4.h\"\n\nstatic int c4iw_init_qid_table(struct c4iw_rdev *rdev)\n{\n\tu32 i;\n\n\tif (c4iw_id_table_alloc(&rdev->resource.qid_table,\n\t\t\t\trdev->lldi.vr->qp.start,\n\t\t\t\trdev->lldi.vr->qp.size,\n\t\t\t\trdev->lldi.vr->qp.size, 0))\n\t\treturn -ENOMEM;\n\n\tfor (i = rdev->lldi.vr->qp.start;\n\t\ti < rdev->lldi.vr->qp.start + rdev->lldi.vr->qp.size; i++)\n\t\tif (!(i & rdev->qpmask))\n\t\t\tc4iw_id_free(&rdev->resource.qid_table, i);\n\treturn 0;\n}\n\n \nint c4iw_init_resource(struct c4iw_rdev *rdev, u32 nr_tpt,\n\t\t       u32 nr_pdid, u32 nr_srqt)\n{\n\tint err = 0;\n\terr = c4iw_id_table_alloc(&rdev->resource.tpt_table, 0, nr_tpt, 1,\n\t\t\t\t\tC4IW_ID_TABLE_F_RANDOM);\n\tif (err)\n\t\tgoto tpt_err;\n\terr = c4iw_init_qid_table(rdev);\n\tif (err)\n\t\tgoto qid_err;\n\terr = c4iw_id_table_alloc(&rdev->resource.pdid_table, 0,\n\t\t\t\t\tnr_pdid, 1, 0);\n\tif (err)\n\t\tgoto pdid_err;\n\tif (!nr_srqt)\n\t\terr = c4iw_id_table_alloc(&rdev->resource.srq_table, 0,\n\t\t\t\t\t  1, 1, 0);\n\telse\n\t\terr = c4iw_id_table_alloc(&rdev->resource.srq_table, 0,\n\t\t\t\t\t  nr_srqt, 0, 0);\n\tif (err)\n\t\tgoto srq_err;\n\treturn 0;\n srq_err:\n\tc4iw_id_table_free(&rdev->resource.pdid_table);\n pdid_err:\n\tc4iw_id_table_free(&rdev->resource.qid_table);\n qid_err:\n\tc4iw_id_table_free(&rdev->resource.tpt_table);\n tpt_err:\n\treturn -ENOMEM;\n}\n\n \nu32 c4iw_get_resource(struct c4iw_id_table *id_table)\n{\n\tu32 entry;\n\tentry = c4iw_id_alloc(id_table);\n\tif (entry == (u32)(-1))\n\t\treturn 0;\n\treturn entry;\n}\n\nvoid c4iw_put_resource(struct c4iw_id_table *id_table, u32 entry)\n{\n\tpr_debug(\"entry 0x%x\\n\", entry);\n\tc4iw_id_free(id_table, entry);\n}\n\nu32 c4iw_get_cqid(struct c4iw_rdev *rdev, struct c4iw_dev_ucontext *uctx)\n{\n\tstruct c4iw_qid_list *entry;\n\tu32 qid;\n\tint i;\n\n\tmutex_lock(&uctx->lock);\n\tif (!list_empty(&uctx->cqids)) {\n\t\tentry = list_entry(uctx->cqids.next, struct c4iw_qid_list,\n\t\t\t\t   entry);\n\t\tlist_del(&entry->entry);\n\t\tqid = entry->qid;\n\t\tkfree(entry);\n\t} else {\n\t\tqid = c4iw_get_resource(&rdev->resource.qid_table);\n\t\tif (!qid)\n\t\t\tgoto out;\n\t\tmutex_lock(&rdev->stats.lock);\n\t\trdev->stats.qid.cur += rdev->qpmask + 1;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t\tfor (i = qid+1; i & rdev->qpmask; i++) {\n\t\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\t\t\tentry->qid = i;\n\t\t\tlist_add_tail(&entry->entry, &uctx->cqids);\n\t\t}\n\n\t\t \n\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\tif (!entry)\n\t\t\tgoto out;\n\t\tentry->qid = qid;\n\t\tlist_add_tail(&entry->entry, &uctx->qpids);\n\t\tfor (i = qid+1; i & rdev->qpmask; i++) {\n\t\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\t\t\tentry->qid = i;\n\t\t\tlist_add_tail(&entry->entry, &uctx->qpids);\n\t\t}\n\t}\nout:\n\tmutex_unlock(&uctx->lock);\n\tpr_debug(\"qid 0x%x\\n\", qid);\n\tmutex_lock(&rdev->stats.lock);\n\tif (rdev->stats.qid.cur > rdev->stats.qid.max)\n\t\trdev->stats.qid.max = rdev->stats.qid.cur;\n\tmutex_unlock(&rdev->stats.lock);\n\treturn qid;\n}\n\nvoid c4iw_put_cqid(struct c4iw_rdev *rdev, u32 qid,\n\t\t   struct c4iw_dev_ucontext *uctx)\n{\n\tstruct c4iw_qid_list *entry;\n\n\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn;\n\tpr_debug(\"qid 0x%x\\n\", qid);\n\tentry->qid = qid;\n\tmutex_lock(&uctx->lock);\n\tlist_add_tail(&entry->entry, &uctx->cqids);\n\tmutex_unlock(&uctx->lock);\n}\n\nu32 c4iw_get_qpid(struct c4iw_rdev *rdev, struct c4iw_dev_ucontext *uctx)\n{\n\tstruct c4iw_qid_list *entry;\n\tu32 qid;\n\tint i;\n\n\tmutex_lock(&uctx->lock);\n\tif (!list_empty(&uctx->qpids)) {\n\t\tentry = list_entry(uctx->qpids.next, struct c4iw_qid_list,\n\t\t\t\t   entry);\n\t\tlist_del(&entry->entry);\n\t\tqid = entry->qid;\n\t\tkfree(entry);\n\t} else {\n\t\tqid = c4iw_get_resource(&rdev->resource.qid_table);\n\t\tif (!qid) {\n\t\t\tmutex_lock(&rdev->stats.lock);\n\t\t\trdev->stats.qid.fail++;\n\t\t\tmutex_unlock(&rdev->stats.lock);\n\t\t\tgoto out;\n\t\t}\n\t\tmutex_lock(&rdev->stats.lock);\n\t\trdev->stats.qid.cur += rdev->qpmask + 1;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t\tfor (i = qid+1; i & rdev->qpmask; i++) {\n\t\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\t\t\tentry->qid = i;\n\t\t\tlist_add_tail(&entry->entry, &uctx->qpids);\n\t\t}\n\n\t\t \n\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\tif (!entry)\n\t\t\tgoto out;\n\t\tentry->qid = qid;\n\t\tlist_add_tail(&entry->entry, &uctx->cqids);\n\t\tfor (i = qid + 1; i & rdev->qpmask; i++) {\n\t\t\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\t\t\tentry->qid = i;\n\t\t\tlist_add_tail(&entry->entry, &uctx->cqids);\n\t\t}\n\t}\nout:\n\tmutex_unlock(&uctx->lock);\n\tpr_debug(\"qid 0x%x\\n\", qid);\n\tmutex_lock(&rdev->stats.lock);\n\tif (rdev->stats.qid.cur > rdev->stats.qid.max)\n\t\trdev->stats.qid.max = rdev->stats.qid.cur;\n\tmutex_unlock(&rdev->stats.lock);\n\treturn qid;\n}\n\nvoid c4iw_put_qpid(struct c4iw_rdev *rdev, u32 qid,\n\t\t   struct c4iw_dev_ucontext *uctx)\n{\n\tstruct c4iw_qid_list *entry;\n\n\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn;\n\tpr_debug(\"qid 0x%x\\n\", qid);\n\tentry->qid = qid;\n\tmutex_lock(&uctx->lock);\n\tlist_add_tail(&entry->entry, &uctx->qpids);\n\tmutex_unlock(&uctx->lock);\n}\n\nvoid c4iw_destroy_resource(struct c4iw_resource *rscp)\n{\n\tc4iw_id_table_free(&rscp->tpt_table);\n\tc4iw_id_table_free(&rscp->qid_table);\n\tc4iw_id_table_free(&rscp->pdid_table);\n}\n\n \n\n#define MIN_PBL_SHIFT 8\t\t\t \n\nu32 c4iw_pblpool_alloc(struct c4iw_rdev *rdev, int size)\n{\n\tunsigned long addr = gen_pool_alloc(rdev->pbl_pool, size);\n\tpr_debug(\"addr 0x%x size %d\\n\", (u32)addr, size);\n\tmutex_lock(&rdev->stats.lock);\n\tif (addr) {\n\t\trdev->stats.pbl.cur += roundup(size, 1 << MIN_PBL_SHIFT);\n\t\tif (rdev->stats.pbl.cur > rdev->stats.pbl.max)\n\t\t\trdev->stats.pbl.max = rdev->stats.pbl.cur;\n\t\tkref_get(&rdev->pbl_kref);\n\t} else\n\t\trdev->stats.pbl.fail++;\n\tmutex_unlock(&rdev->stats.lock);\n\treturn (u32)addr;\n}\n\nstatic void destroy_pblpool(struct kref *kref)\n{\n\tstruct c4iw_rdev *rdev;\n\n\trdev = container_of(kref, struct c4iw_rdev, pbl_kref);\n\tgen_pool_destroy(rdev->pbl_pool);\n\tcomplete(&rdev->pbl_compl);\n}\n\nvoid c4iw_pblpool_free(struct c4iw_rdev *rdev, u32 addr, int size)\n{\n\tpr_debug(\"addr 0x%x size %d\\n\", addr, size);\n\tmutex_lock(&rdev->stats.lock);\n\trdev->stats.pbl.cur -= roundup(size, 1 << MIN_PBL_SHIFT);\n\tmutex_unlock(&rdev->stats.lock);\n\tgen_pool_free(rdev->pbl_pool, (unsigned long)addr, size);\n\tkref_put(&rdev->pbl_kref, destroy_pblpool);\n}\n\nint c4iw_pblpool_create(struct c4iw_rdev *rdev)\n{\n\tunsigned pbl_start, pbl_chunk, pbl_top;\n\n\trdev->pbl_pool = gen_pool_create(MIN_PBL_SHIFT, -1);\n\tif (!rdev->pbl_pool)\n\t\treturn -ENOMEM;\n\n\tpbl_start = rdev->lldi.vr->pbl.start;\n\tpbl_chunk = rdev->lldi.vr->pbl.size;\n\tpbl_top = pbl_start + pbl_chunk;\n\n\twhile (pbl_start < pbl_top) {\n\t\tpbl_chunk = min(pbl_top - pbl_start + 1, pbl_chunk);\n\t\tif (gen_pool_add(rdev->pbl_pool, pbl_start, pbl_chunk, -1)) {\n\t\t\tpr_debug(\"failed to add PBL chunk (%x/%x)\\n\",\n\t\t\t\t pbl_start, pbl_chunk);\n\t\t\tif (pbl_chunk <= 1024 << MIN_PBL_SHIFT) {\n\t\t\t\tpr_warn(\"Failed to add all PBL chunks (%x/%x)\\n\",\n\t\t\t\t\tpbl_start, pbl_top - pbl_start);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tpbl_chunk >>= 1;\n\t\t} else {\n\t\t\tpr_debug(\"added PBL chunk (%x/%x)\\n\",\n\t\t\t\t pbl_start, pbl_chunk);\n\t\t\tpbl_start += pbl_chunk;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nvoid c4iw_pblpool_destroy(struct c4iw_rdev *rdev)\n{\n\tkref_put(&rdev->pbl_kref, destroy_pblpool);\n}\n\n \n\n#define MIN_RQT_SHIFT 10\t \n\nu32 c4iw_rqtpool_alloc(struct c4iw_rdev *rdev, int size)\n{\n\tunsigned long addr = gen_pool_alloc(rdev->rqt_pool, size << 6);\n\tpr_debug(\"addr 0x%x size %d\\n\", (u32)addr, size << 6);\n\tif (!addr)\n\t\tpr_warn_ratelimited(\"%s: Out of RQT memory\\n\",\n\t\t\t\t    pci_name(rdev->lldi.pdev));\n\tmutex_lock(&rdev->stats.lock);\n\tif (addr) {\n\t\trdev->stats.rqt.cur += roundup(size << 6, 1 << MIN_RQT_SHIFT);\n\t\tif (rdev->stats.rqt.cur > rdev->stats.rqt.max)\n\t\t\trdev->stats.rqt.max = rdev->stats.rqt.cur;\n\t\tkref_get(&rdev->rqt_kref);\n\t} else\n\t\trdev->stats.rqt.fail++;\n\tmutex_unlock(&rdev->stats.lock);\n\treturn (u32)addr;\n}\n\nstatic void destroy_rqtpool(struct kref *kref)\n{\n\tstruct c4iw_rdev *rdev;\n\n\trdev = container_of(kref, struct c4iw_rdev, rqt_kref);\n\tgen_pool_destroy(rdev->rqt_pool);\n\tcomplete(&rdev->rqt_compl);\n}\n\nvoid c4iw_rqtpool_free(struct c4iw_rdev *rdev, u32 addr, int size)\n{\n\tpr_debug(\"addr 0x%x size %d\\n\", addr, size << 6);\n\tmutex_lock(&rdev->stats.lock);\n\trdev->stats.rqt.cur -= roundup(size << 6, 1 << MIN_RQT_SHIFT);\n\tmutex_unlock(&rdev->stats.lock);\n\tgen_pool_free(rdev->rqt_pool, (unsigned long)addr, size << 6);\n\tkref_put(&rdev->rqt_kref, destroy_rqtpool);\n}\n\nint c4iw_rqtpool_create(struct c4iw_rdev *rdev)\n{\n\tunsigned rqt_start, rqt_chunk, rqt_top;\n\tint skip = 0;\n\n\trdev->rqt_pool = gen_pool_create(MIN_RQT_SHIFT, -1);\n\tif (!rdev->rqt_pool)\n\t\treturn -ENOMEM;\n\n\t \n\tif (rdev->lldi.vr->srq.size)\n\t\tskip = T4_RQT_ENTRY_SIZE;\n\n\trqt_start = rdev->lldi.vr->rq.start + skip;\n\trqt_chunk = rdev->lldi.vr->rq.size - skip;\n\trqt_top = rqt_start + rqt_chunk;\n\n\twhile (rqt_start < rqt_top) {\n\t\trqt_chunk = min(rqt_top - rqt_start + 1, rqt_chunk);\n\t\tif (gen_pool_add(rdev->rqt_pool, rqt_start, rqt_chunk, -1)) {\n\t\t\tpr_debug(\"failed to add RQT chunk (%x/%x)\\n\",\n\t\t\t\t rqt_start, rqt_chunk);\n\t\t\tif (rqt_chunk <= 1024 << MIN_RQT_SHIFT) {\n\t\t\t\tpr_warn(\"Failed to add all RQT chunks (%x/%x)\\n\",\n\t\t\t\t\trqt_start, rqt_top - rqt_start);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\trqt_chunk >>= 1;\n\t\t} else {\n\t\t\tpr_debug(\"added RQT chunk (%x/%x)\\n\",\n\t\t\t\t rqt_start, rqt_chunk);\n\t\t\trqt_start += rqt_chunk;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid c4iw_rqtpool_destroy(struct c4iw_rdev *rdev)\n{\n\tkref_put(&rdev->rqt_kref, destroy_rqtpool);\n}\n\nint c4iw_alloc_srq_idx(struct c4iw_rdev *rdev)\n{\n\tint idx;\n\n\tidx = c4iw_id_alloc(&rdev->resource.srq_table);\n\tmutex_lock(&rdev->stats.lock);\n\tif (idx == -1) {\n\t\trdev->stats.srqt.fail++;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t\treturn -ENOMEM;\n\t}\n\trdev->stats.srqt.cur++;\n\tif (rdev->stats.srqt.cur > rdev->stats.srqt.max)\n\t\trdev->stats.srqt.max = rdev->stats.srqt.cur;\n\tmutex_unlock(&rdev->stats.lock);\n\treturn idx;\n}\n\nvoid c4iw_free_srq_idx(struct c4iw_rdev *rdev, int idx)\n{\n\tc4iw_id_free(&rdev->resource.srq_table, idx);\n\tmutex_lock(&rdev->stats.lock);\n\trdev->stats.srqt.cur--;\n\tmutex_unlock(&rdev->stats.lock);\n}\n\n \n#define MIN_OCQP_SHIFT 12\t \n\nu32 c4iw_ocqp_pool_alloc(struct c4iw_rdev *rdev, int size)\n{\n\tunsigned long addr = gen_pool_alloc(rdev->ocqp_pool, size);\n\tpr_debug(\"addr 0x%x size %d\\n\", (u32)addr, size);\n\tif (addr) {\n\t\tmutex_lock(&rdev->stats.lock);\n\t\trdev->stats.ocqp.cur += roundup(size, 1 << MIN_OCQP_SHIFT);\n\t\tif (rdev->stats.ocqp.cur > rdev->stats.ocqp.max)\n\t\t\trdev->stats.ocqp.max = rdev->stats.ocqp.cur;\n\t\tmutex_unlock(&rdev->stats.lock);\n\t}\n\treturn (u32)addr;\n}\n\nvoid c4iw_ocqp_pool_free(struct c4iw_rdev *rdev, u32 addr, int size)\n{\n\tpr_debug(\"addr 0x%x size %d\\n\", addr, size);\n\tmutex_lock(&rdev->stats.lock);\n\trdev->stats.ocqp.cur -= roundup(size, 1 << MIN_OCQP_SHIFT);\n\tmutex_unlock(&rdev->stats.lock);\n\tgen_pool_free(rdev->ocqp_pool, (unsigned long)addr, size);\n}\n\nint c4iw_ocqp_pool_create(struct c4iw_rdev *rdev)\n{\n\tunsigned start, chunk, top;\n\n\trdev->ocqp_pool = gen_pool_create(MIN_OCQP_SHIFT, -1);\n\tif (!rdev->ocqp_pool)\n\t\treturn -ENOMEM;\n\n\tstart = rdev->lldi.vr->ocq.start;\n\tchunk = rdev->lldi.vr->ocq.size;\n\ttop = start + chunk;\n\n\twhile (start < top) {\n\t\tchunk = min(top - start + 1, chunk);\n\t\tif (gen_pool_add(rdev->ocqp_pool, start, chunk, -1)) {\n\t\t\tpr_debug(\"failed to add OCQP chunk (%x/%x)\\n\",\n\t\t\t\t start, chunk);\n\t\t\tif (chunk <= 1024 << MIN_OCQP_SHIFT) {\n\t\t\t\tpr_warn(\"Failed to add all OCQP chunks (%x/%x)\\n\",\n\t\t\t\t\tstart, top - start);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tchunk >>= 1;\n\t\t} else {\n\t\t\tpr_debug(\"added OCQP chunk (%x/%x)\\n\",\n\t\t\t\t start, chunk);\n\t\t\tstart += chunk;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid c4iw_ocqp_pool_destroy(struct c4iw_rdev *rdev)\n{\n\tgen_pool_destroy(rdev->ocqp_pool);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}