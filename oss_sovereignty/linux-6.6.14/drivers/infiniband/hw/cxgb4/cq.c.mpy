{
  "module_name": "cq.c",
  "hash_id": "55570f0bff17f0fd2e2ae260748993ff88b300c0c3c5fa28e3a56ce45d0ea269",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/cxgb4/cq.c",
  "human_readable_source": " \n\n#include <rdma/uverbs_ioctl.h>\n\n#include \"iw_cxgb4.h\"\n\nstatic void destroy_cq(struct c4iw_rdev *rdev, struct t4_cq *cq,\n\t\t       struct c4iw_dev_ucontext *uctx, struct sk_buff *skb,\n\t\t       struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct fw_ri_res_wr *res_wr;\n\tstruct fw_ri_res *res;\n\tint wr_len;\n\n\twr_len = sizeof(*res_wr) + sizeof(*res);\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\tres_wr = __skb_put_zero(skb, wr_len);\n\tres_wr->op_nres = cpu_to_be32(\n\t\t\tFW_WR_OP_V(FW_RI_RES_WR) |\n\t\t\tFW_RI_RES_WR_NRES_V(1) |\n\t\t\tFW_WR_COMPL_F);\n\tres_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));\n\tres_wr->cookie = (uintptr_t)wr_waitp;\n\tres = res_wr->res;\n\tres->u.cq.restype = FW_RI_RES_TYPE_CQ;\n\tres->u.cq.op = FW_RI_RES_OP_RESET;\n\tres->u.cq.iqid = cpu_to_be32(cq->cqid);\n\n\tc4iw_init_wr_wait(wr_waitp);\n\tc4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);\n\n\tkfree(cq->sw_queue);\n\tdma_free_coherent(&(rdev->lldi.pdev->dev),\n\t\t\t  cq->memsize, cq->queue,\n\t\t\t  dma_unmap_addr(cq, mapping));\n\tc4iw_put_cqid(rdev, cq->cqid, uctx);\n}\n\nstatic int create_cq(struct c4iw_rdev *rdev, struct t4_cq *cq,\n\t\t     struct c4iw_dev_ucontext *uctx,\n\t\t     struct c4iw_wr_wait *wr_waitp)\n{\n\tstruct fw_ri_res_wr *res_wr;\n\tstruct fw_ri_res *res;\n\tint wr_len;\n\tint user = (uctx != &rdev->uctx);\n\tint ret;\n\tstruct sk_buff *skb;\n\tstruct c4iw_ucontext *ucontext = NULL;\n\n\tif (user)\n\t\tucontext = container_of(uctx, struct c4iw_ucontext, uctx);\n\n\tcq->cqid = c4iw_get_cqid(rdev, uctx);\n\tif (!cq->cqid) {\n\t\tret = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tif (!user) {\n\t\tcq->sw_queue = kzalloc(cq->memsize, GFP_KERNEL);\n\t\tif (!cq->sw_queue) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err2;\n\t\t}\n\t}\n\tcq->queue = dma_alloc_coherent(&rdev->lldi.pdev->dev, cq->memsize,\n\t\t\t\t       &cq->dma_addr, GFP_KERNEL);\n\tif (!cq->queue) {\n\t\tret = -ENOMEM;\n\t\tgoto err3;\n\t}\n\tdma_unmap_addr_set(cq, mapping, cq->dma_addr);\n\n\tif (user && ucontext->is_32b_cqe) {\n\t\tcq->qp_errp = &((struct t4_status_page *)\n\t\t((u8 *)cq->queue + (cq->size - 1) *\n\t\t (sizeof(*cq->queue) / 2)))->qp_err;\n\t} else {\n\t\tcq->qp_errp = &((struct t4_status_page *)\n\t\t((u8 *)cq->queue + (cq->size - 1) *\n\t\t sizeof(*cq->queue)))->qp_err;\n\t}\n\n\t \n\twr_len = sizeof(*res_wr) + sizeof(*res);\n\n\tskb = alloc_skb(wr_len, GFP_KERNEL);\n\tif (!skb) {\n\t\tret = -ENOMEM;\n\t\tgoto err4;\n\t}\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);\n\n\tres_wr = __skb_put_zero(skb, wr_len);\n\tres_wr->op_nres = cpu_to_be32(\n\t\t\tFW_WR_OP_V(FW_RI_RES_WR) |\n\t\t\tFW_RI_RES_WR_NRES_V(1) |\n\t\t\tFW_WR_COMPL_F);\n\tres_wr->len16_pkd = cpu_to_be32(DIV_ROUND_UP(wr_len, 16));\n\tres_wr->cookie = (uintptr_t)wr_waitp;\n\tres = res_wr->res;\n\tres->u.cq.restype = FW_RI_RES_TYPE_CQ;\n\tres->u.cq.op = FW_RI_RES_OP_WRITE;\n\tres->u.cq.iqid = cpu_to_be32(cq->cqid);\n\tres->u.cq.iqandst_to_iqandstindex = cpu_to_be32(\n\t\t\tFW_RI_RES_WR_IQANUS_V(0) |\n\t\t\tFW_RI_RES_WR_IQANUD_V(1) |\n\t\t\tFW_RI_RES_WR_IQANDST_F |\n\t\t\tFW_RI_RES_WR_IQANDSTINDEX_V(\n\t\t\t\trdev->lldi.ciq_ids[cq->vector]));\n\tres->u.cq.iqdroprss_to_iqesize = cpu_to_be16(\n\t\t\tFW_RI_RES_WR_IQDROPRSS_F |\n\t\t\tFW_RI_RES_WR_IQPCIECH_V(2) |\n\t\t\tFW_RI_RES_WR_IQINTCNTTHRESH_V(0) |\n\t\t\tFW_RI_RES_WR_IQO_F |\n\t\t\t((user && ucontext->is_32b_cqe) ?\n\t\t\t FW_RI_RES_WR_IQESIZE_V(1) :\n\t\t\t FW_RI_RES_WR_IQESIZE_V(2)));\n\tres->u.cq.iqsize = cpu_to_be16(cq->size);\n\tres->u.cq.iqaddr = cpu_to_be64(cq->dma_addr);\n\n\tc4iw_init_wr_wait(wr_waitp);\n\tret = c4iw_ref_send_wait(rdev, skb, wr_waitp, 0, 0, __func__);\n\tif (ret)\n\t\tgoto err4;\n\n\tcq->gen = 1;\n\tcq->gts = rdev->lldi.gts_reg;\n\tcq->rdev = rdev;\n\n\tcq->bar2_va = c4iw_bar2_addrs(rdev, cq->cqid, CXGB4_BAR2_QTYPE_INGRESS,\n\t\t\t\t      &cq->bar2_qid,\n\t\t\t\t      user ? &cq->bar2_pa : NULL);\n\tif (user && !cq->bar2_pa) {\n\t\tpr_warn(\"%s: cqid %u not in BAR2 range\\n\",\n\t\t\tpci_name(rdev->lldi.pdev), cq->cqid);\n\t\tret = -EINVAL;\n\t\tgoto err4;\n\t}\n\treturn 0;\nerr4:\n\tdma_free_coherent(&rdev->lldi.pdev->dev, cq->memsize, cq->queue,\n\t\t\t  dma_unmap_addr(cq, mapping));\nerr3:\n\tkfree(cq->sw_queue);\nerr2:\n\tc4iw_put_cqid(rdev, cq->cqid, uctx);\nerr1:\n\treturn ret;\n}\n\nstatic void insert_recv_cqe(struct t4_wq *wq, struct t4_cq *cq, u32 srqidx)\n{\n\tstruct t4_cqe cqe;\n\n\tpr_debug(\"wq %p cq %p sw_cidx %u sw_pidx %u\\n\",\n\t\t wq, cq, cq->sw_cidx, cq->sw_pidx);\n\tmemset(&cqe, 0, sizeof(cqe));\n\tcqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |\n\t\t\t\t CQE_OPCODE_V(FW_RI_SEND) |\n\t\t\t\t CQE_TYPE_V(0) |\n\t\t\t\t CQE_SWCQE_V(1) |\n\t\t\t\t CQE_QPID_V(wq->sq.qid));\n\tcqe.bits_type_ts = cpu_to_be64(CQE_GENBIT_V((u64)cq->gen));\n\tif (srqidx)\n\t\tcqe.u.srcqe.abs_rqe_idx = cpu_to_be32(srqidx);\n\tcq->sw_queue[cq->sw_pidx] = cqe;\n\tt4_swcq_produce(cq);\n}\n\nint c4iw_flush_rq(struct t4_wq *wq, struct t4_cq *cq, int count)\n{\n\tint flushed = 0;\n\tint in_use = wq->rq.in_use - count;\n\n\tpr_debug(\"wq %p cq %p rq.in_use %u skip count %u\\n\",\n\t\t wq, cq, wq->rq.in_use, count);\n\twhile (in_use--) {\n\t\tinsert_recv_cqe(wq, cq, 0);\n\t\tflushed++;\n\t}\n\treturn flushed;\n}\n\nstatic void insert_sq_cqe(struct t4_wq *wq, struct t4_cq *cq,\n\t\t\t  struct t4_swsqe *swcqe)\n{\n\tstruct t4_cqe cqe;\n\n\tpr_debug(\"wq %p cq %p sw_cidx %u sw_pidx %u\\n\",\n\t\t wq, cq, cq->sw_cidx, cq->sw_pidx);\n\tmemset(&cqe, 0, sizeof(cqe));\n\tcqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |\n\t\t\t\t CQE_OPCODE_V(swcqe->opcode) |\n\t\t\t\t CQE_TYPE_V(1) |\n\t\t\t\t CQE_SWCQE_V(1) |\n\t\t\t\t CQE_QPID_V(wq->sq.qid));\n\tCQE_WRID_SQ_IDX(&cqe) = swcqe->idx;\n\tcqe.bits_type_ts = cpu_to_be64(CQE_GENBIT_V((u64)cq->gen));\n\tcq->sw_queue[cq->sw_pidx] = cqe;\n\tt4_swcq_produce(cq);\n}\n\nstatic void advance_oldest_read(struct t4_wq *wq);\n\nint c4iw_flush_sq(struct c4iw_qp *qhp)\n{\n\tint flushed = 0;\n\tstruct t4_wq *wq = &qhp->wq;\n\tstruct c4iw_cq *chp = to_c4iw_cq(qhp->ibqp.send_cq);\n\tstruct t4_cq *cq = &chp->cq;\n\tint idx;\n\tstruct t4_swsqe *swsqe;\n\n\tif (wq->sq.flush_cidx == -1)\n\t\twq->sq.flush_cidx = wq->sq.cidx;\n\tidx = wq->sq.flush_cidx;\n\twhile (idx != wq->sq.pidx) {\n\t\tswsqe = &wq->sq.sw_sq[idx];\n\t\tswsqe->flushed = 1;\n\t\tinsert_sq_cqe(wq, cq, swsqe);\n\t\tif (wq->sq.oldest_read == swsqe) {\n\t\t\tadvance_oldest_read(wq);\n\t\t}\n\t\tflushed++;\n\t\tif (++idx == wq->sq.size)\n\t\t\tidx = 0;\n\t}\n\twq->sq.flush_cidx += flushed;\n\tif (wq->sq.flush_cidx >= wq->sq.size)\n\t\twq->sq.flush_cidx -= wq->sq.size;\n\treturn flushed;\n}\n\nstatic void flush_completed_wrs(struct t4_wq *wq, struct t4_cq *cq)\n{\n\tstruct t4_swsqe *swsqe;\n\tint cidx;\n\n\tif (wq->sq.flush_cidx == -1)\n\t\twq->sq.flush_cidx = wq->sq.cidx;\n\tcidx = wq->sq.flush_cidx;\n\n\twhile (cidx != wq->sq.pidx) {\n\t\tswsqe = &wq->sq.sw_sq[cidx];\n\t\tif (!swsqe->signaled) {\n\t\t\tif (++cidx == wq->sq.size)\n\t\t\t\tcidx = 0;\n\t\t} else if (swsqe->complete) {\n\n\t\t\t \n\t\t\tpr_debug(\"moving cqe into swcq sq idx %u cq idx %u\\n\",\n\t\t\t\t cidx, cq->sw_pidx);\n\t\t\tswsqe->cqe.header |= htonl(CQE_SWCQE_V(1));\n\t\t\tcq->sw_queue[cq->sw_pidx] = swsqe->cqe;\n\t\t\tt4_swcq_produce(cq);\n\t\t\tswsqe->flushed = 1;\n\t\t\tif (++cidx == wq->sq.size)\n\t\t\t\tcidx = 0;\n\t\t\twq->sq.flush_cidx = cidx;\n\t\t} else\n\t\t\tbreak;\n\t}\n}\n\nstatic void create_read_req_cqe(struct t4_wq *wq, struct t4_cqe *hw_cqe,\n\t\tstruct t4_cqe *read_cqe)\n{\n\tread_cqe->u.scqe.cidx = wq->sq.oldest_read->idx;\n\tread_cqe->len = htonl(wq->sq.oldest_read->read_len);\n\tread_cqe->header = htonl(CQE_QPID_V(CQE_QPID(hw_cqe)) |\n\t\t\tCQE_SWCQE_V(SW_CQE(hw_cqe)) |\n\t\t\tCQE_OPCODE_V(FW_RI_READ_REQ) |\n\t\t\tCQE_TYPE_V(1));\n\tread_cqe->bits_type_ts = hw_cqe->bits_type_ts;\n}\n\nstatic void advance_oldest_read(struct t4_wq *wq)\n{\n\n\tu32 rptr = wq->sq.oldest_read - wq->sq.sw_sq + 1;\n\n\tif (rptr == wq->sq.size)\n\t\trptr = 0;\n\twhile (rptr != wq->sq.pidx) {\n\t\twq->sq.oldest_read = &wq->sq.sw_sq[rptr];\n\n\t\tif (wq->sq.oldest_read->opcode == FW_RI_READ_REQ)\n\t\t\treturn;\n\t\tif (++rptr == wq->sq.size)\n\t\t\trptr = 0;\n\t}\n\twq->sq.oldest_read = NULL;\n}\n\n \nvoid c4iw_flush_hw_cq(struct c4iw_cq *chp, struct c4iw_qp *flush_qhp)\n{\n\tstruct t4_cqe *hw_cqe, *swcqe, read_cqe;\n\tstruct c4iw_qp *qhp;\n\tstruct t4_swsqe *swsqe;\n\tint ret;\n\n\tpr_debug(\"cqid 0x%x\\n\", chp->cq.cqid);\n\tret = t4_next_hw_cqe(&chp->cq, &hw_cqe);\n\n\t \n\twhile (!ret) {\n\t\tqhp = get_qhp(chp->rhp, CQE_QPID(hw_cqe));\n\n\t\t \n\t\tif (qhp == NULL)\n\t\t\tgoto next_cqe;\n\n\t\tif (flush_qhp != qhp) {\n\t\t\tspin_lock(&qhp->lock);\n\n\t\t\tif (qhp->wq.flushed == 1)\n\t\t\t\tgoto next_cqe;\n\t\t}\n\n\t\tif (CQE_OPCODE(hw_cqe) == FW_RI_TERMINATE)\n\t\t\tgoto next_cqe;\n\n\t\tif (CQE_OPCODE(hw_cqe) == FW_RI_READ_RESP) {\n\n\t\t\t \n\t\t\tif (CQE_TYPE(hw_cqe) == 1)\n\t\t\t\tgoto next_cqe;\n\n\t\t\t \n\t\t\tif (CQE_WRID_STAG(hw_cqe) == 1)\n\t\t\t\tgoto next_cqe;\n\n\t\t\t \n\t\t\tif (!qhp->wq.sq.oldest_read->signaled) {\n\t\t\t\tadvance_oldest_read(&qhp->wq);\n\t\t\t\tgoto next_cqe;\n\t\t\t}\n\n\t\t\t \n\t\t\tcreate_read_req_cqe(&qhp->wq, hw_cqe, &read_cqe);\n\t\t\thw_cqe = &read_cqe;\n\t\t\tadvance_oldest_read(&qhp->wq);\n\t\t}\n\n\t\t \n\t\tif (SQ_TYPE(hw_cqe)) {\n\t\t\tswsqe = &qhp->wq.sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)];\n\t\t\tswsqe->cqe = *hw_cqe;\n\t\t\tswsqe->complete = 1;\n\t\t\tflush_completed_wrs(&qhp->wq, &chp->cq);\n\t\t} else {\n\t\t\tswcqe = &chp->cq.sw_queue[chp->cq.sw_pidx];\n\t\t\t*swcqe = *hw_cqe;\n\t\t\tswcqe->header |= cpu_to_be32(CQE_SWCQE_V(1));\n\t\t\tt4_swcq_produce(&chp->cq);\n\t\t}\nnext_cqe:\n\t\tt4_hwcq_consume(&chp->cq);\n\t\tret = t4_next_hw_cqe(&chp->cq, &hw_cqe);\n\t\tif (qhp && flush_qhp != qhp)\n\t\t\tspin_unlock(&qhp->lock);\n\t}\n}\n\nstatic int cqe_completes_wr(struct t4_cqe *cqe, struct t4_wq *wq)\n{\n\tif (DRAIN_CQE(cqe)) {\n\t\tWARN_ONCE(1, \"Unexpected DRAIN CQE qp id %u!\\n\", wq->sq.qid);\n\t\treturn 0;\n\t}\n\n\tif (CQE_OPCODE(cqe) == FW_RI_TERMINATE)\n\t\treturn 0;\n\n\tif ((CQE_OPCODE(cqe) == FW_RI_RDMA_WRITE) && RQ_TYPE(cqe))\n\t\treturn 0;\n\n\tif ((CQE_OPCODE(cqe) == FW_RI_READ_RESP) && SQ_TYPE(cqe))\n\t\treturn 0;\n\n\tif (CQE_SEND_OPCODE(cqe) && RQ_TYPE(cqe) && t4_rq_empty(wq))\n\t\treturn 0;\n\treturn 1;\n}\n\nvoid c4iw_count_rcqes(struct t4_cq *cq, struct t4_wq *wq, int *count)\n{\n\tstruct t4_cqe *cqe;\n\tu32 ptr;\n\n\t*count = 0;\n\tpr_debug(\"count zero %d\\n\", *count);\n\tptr = cq->sw_cidx;\n\twhile (ptr != cq->sw_pidx) {\n\t\tcqe = &cq->sw_queue[ptr];\n\t\tif (RQ_TYPE(cqe) && (CQE_OPCODE(cqe) != FW_RI_READ_RESP) &&\n\t\t    (CQE_QPID(cqe) == wq->sq.qid) && cqe_completes_wr(cqe, wq))\n\t\t\t(*count)++;\n\t\tif (++ptr == cq->size)\n\t\t\tptr = 0;\n\t}\n\tpr_debug(\"cq %p count %d\\n\", cq, *count);\n}\n\nstatic void post_pending_srq_wrs(struct t4_srq *srq)\n{\n\tstruct t4_srq_pending_wr *pwr;\n\tu16 idx = 0;\n\n\twhile (srq->pending_in_use) {\n\t\tpwr = &srq->pending_wrs[srq->pending_cidx];\n\t\tsrq->sw_rq[srq->pidx].wr_id = pwr->wr_id;\n\t\tsrq->sw_rq[srq->pidx].valid = 1;\n\n\t\tpr_debug(\"%s posting pending cidx %u pidx %u wq_pidx %u in_use %u rq_size %u wr_id %llx\\n\",\n\t\t\t __func__,\n\t\t\t srq->cidx, srq->pidx, srq->wq_pidx,\n\t\t\t srq->in_use, srq->size,\n\t\t\t (unsigned long long)pwr->wr_id);\n\n\t\tc4iw_copy_wr_to_srq(srq, &pwr->wqe, pwr->len16);\n\t\tt4_srq_consume_pending_wr(srq);\n\t\tt4_srq_produce(srq, pwr->len16);\n\t\tidx += DIV_ROUND_UP(pwr->len16 * 16, T4_EQ_ENTRY_SIZE);\n\t}\n\n\tif (idx) {\n\t\tt4_ring_srq_db(srq, idx, pwr->len16, &pwr->wqe);\n\t\tsrq->queue[srq->size].status.host_wq_pidx =\n\t\t\tsrq->wq_pidx;\n\t}\n}\n\nstatic u64 reap_srq_cqe(struct t4_cqe *hw_cqe, struct t4_srq *srq)\n{\n\tint rel_idx = CQE_ABS_RQE_IDX(hw_cqe) - srq->rqt_abs_idx;\n\tu64 wr_id;\n\n\tsrq->sw_rq[rel_idx].valid = 0;\n\twr_id = srq->sw_rq[rel_idx].wr_id;\n\n\tif (rel_idx == srq->cidx) {\n\t\tpr_debug(\"%s in order cqe rel_idx %u cidx %u pidx %u wq_pidx %u in_use %u rq_size %u wr_id %llx\\n\",\n\t\t\t __func__, rel_idx, srq->cidx, srq->pidx,\n\t\t\t srq->wq_pidx, srq->in_use, srq->size,\n\t\t\t (unsigned long long)srq->sw_rq[rel_idx].wr_id);\n\t\tt4_srq_consume(srq);\n\t\twhile (srq->ooo_count && !srq->sw_rq[srq->cidx].valid) {\n\t\t\tpr_debug(\"%s eat ooo cidx %u pidx %u wq_pidx %u in_use %u rq_size %u ooo_count %u wr_id %llx\\n\",\n\t\t\t\t __func__, srq->cidx, srq->pidx,\n\t\t\t\t srq->wq_pidx, srq->in_use,\n\t\t\t\t srq->size, srq->ooo_count,\n\t\t\t\t (unsigned long long)\n\t\t\t\t srq->sw_rq[srq->cidx].wr_id);\n\t\t\tt4_srq_consume_ooo(srq);\n\t\t}\n\t\tif (srq->ooo_count == 0 && srq->pending_in_use)\n\t\t\tpost_pending_srq_wrs(srq);\n\t} else {\n\t\tpr_debug(\"%s ooo cqe rel_idx %u cidx %u pidx %u wq_pidx %u in_use %u rq_size %u ooo_count %u wr_id %llx\\n\",\n\t\t\t __func__, rel_idx, srq->cidx,\n\t\t\t srq->pidx, srq->wq_pidx,\n\t\t\t srq->in_use, srq->size,\n\t\t\t srq->ooo_count,\n\t\t\t (unsigned long long)srq->sw_rq[rel_idx].wr_id);\n\t\tt4_srq_produce_ooo(srq);\n\t}\n\treturn wr_id;\n}\n\n \nstatic int poll_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,\n\t\t   u8 *cqe_flushed, u64 *cookie, u32 *credit,\n\t\t   struct t4_srq *srq)\n{\n\tint ret = 0;\n\tstruct t4_cqe *hw_cqe, read_cqe;\n\n\t*cqe_flushed = 0;\n\t*credit = 0;\n\tret = t4_next_cqe(cq, &hw_cqe);\n\tif (ret)\n\t\treturn ret;\n\n\tpr_debug(\"CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x\\n\",\n\t\t CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),\n\t\t CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),\n\t\t CQE_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),\n\t\t CQE_WRID_LOW(hw_cqe));\n\n\t \n\tif (wq == NULL) {\n\t\tret = -EAGAIN;\n\t\tgoto skip_cqe;\n\t}\n\n\t \n\tif (wq->flushed && !SW_CQE(hw_cqe)) {\n\t\tret = -EAGAIN;\n\t\tgoto skip_cqe;\n\t}\n\n\t \n\tif (CQE_OPCODE(hw_cqe) == FW_RI_TERMINATE) {\n\t\tret = -EAGAIN;\n\t\tgoto skip_cqe;\n\t}\n\n\t \n\tif (DRAIN_CQE(hw_cqe)) {\n\t\t*cookie = CQE_DRAIN_COOKIE(hw_cqe);\n\t\t*cqe = *hw_cqe;\n\t\tgoto skip_cqe;\n\t}\n\n\t \n\tif (RQ_TYPE(hw_cqe) && (CQE_OPCODE(hw_cqe) == FW_RI_READ_RESP)) {\n\n\t\t \n\t\tif (CQE_TYPE(hw_cqe) == 1) {\n\t\t\tif (CQE_STATUS(hw_cqe))\n\t\t\t\tt4_set_wq_in_error(wq, 0);\n\t\t\tret = -EAGAIN;\n\t\t\tgoto skip_cqe;\n\t\t}\n\n\t\t \n\t\tif (CQE_WRID_STAG(hw_cqe) == 1) {\n\t\t\tif (CQE_STATUS(hw_cqe))\n\t\t\t\tt4_set_wq_in_error(wq, 0);\n\t\t\tret = -EAGAIN;\n\t\t\tgoto skip_cqe;\n\t\t}\n\n\t\t \n\t\tif (!wq->sq.oldest_read->signaled) {\n\t\t\tadvance_oldest_read(wq);\n\t\t\tret = -EAGAIN;\n\t\t\tgoto skip_cqe;\n\t\t}\n\n\t\t \n\t\tcreate_read_req_cqe(wq, hw_cqe, &read_cqe);\n\t\thw_cqe = &read_cqe;\n\t\tadvance_oldest_read(wq);\n\t}\n\n\tif (CQE_STATUS(hw_cqe) || t4_wq_in_error(wq)) {\n\t\t*cqe_flushed = (CQE_STATUS(hw_cqe) == T4_ERR_SWFLUSH);\n\t\tt4_set_wq_in_error(wq, 0);\n\t}\n\n\t \n\tif (RQ_TYPE(hw_cqe)) {\n\n\t\t \n\t\tif (unlikely(!CQE_STATUS(hw_cqe) &&\n\t\t\t     CQE_WRID_MSN(hw_cqe) != wq->rq.msn)) {\n\t\t\tt4_set_wq_in_error(wq, 0);\n\t\t\thw_cqe->header |= cpu_to_be32(CQE_STATUS_V(T4_ERR_MSN));\n\t\t}\n\t\tgoto proc_cqe;\n\t}\n\n\t \n\tif (!SW_CQE(hw_cqe) && (CQE_WRID_SQ_IDX(hw_cqe) != wq->sq.cidx)) {\n\t\tstruct t4_swsqe *swsqe;\n\n\t\tpr_debug(\"out of order completion going in sw_sq at idx %u\\n\",\n\t\t\t CQE_WRID_SQ_IDX(hw_cqe));\n\t\tswsqe = &wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)];\n\t\tswsqe->cqe = *hw_cqe;\n\t\tswsqe->complete = 1;\n\t\tret = -EAGAIN;\n\t\tgoto flush_wq;\n\t}\n\nproc_cqe:\n\t*cqe = *hw_cqe;\n\n\t \n\tif (SQ_TYPE(hw_cqe)) {\n\t\tint idx = CQE_WRID_SQ_IDX(hw_cqe);\n\n\t\t \n\t\tif (idx < wq->sq.cidx)\n\t\t\twq->sq.in_use -= wq->sq.size + idx - wq->sq.cidx;\n\t\telse\n\t\t\twq->sq.in_use -= idx - wq->sq.cidx;\n\n\t\twq->sq.cidx = (uint16_t)idx;\n\t\tpr_debug(\"completing sq idx %u\\n\", wq->sq.cidx);\n\t\t*cookie = wq->sq.sw_sq[wq->sq.cidx].wr_id;\n\t\tif (c4iw_wr_log)\n\t\t\tc4iw_log_wr_stats(wq, hw_cqe);\n\t\tt4_sq_consume(wq);\n\t} else {\n\t\tif (!srq) {\n\t\t\tpr_debug(\"completing rq idx %u\\n\", wq->rq.cidx);\n\t\t\t*cookie = wq->rq.sw_rq[wq->rq.cidx].wr_id;\n\t\t\tif (c4iw_wr_log)\n\t\t\t\tc4iw_log_wr_stats(wq, hw_cqe);\n\t\t\tt4_rq_consume(wq);\n\t\t} else {\n\t\t\t*cookie = reap_srq_cqe(hw_cqe, srq);\n\t\t}\n\t\twq->rq.msn++;\n\t\tgoto skip_cqe;\n\t}\n\nflush_wq:\n\t \n\tflush_completed_wrs(wq, cq);\n\nskip_cqe:\n\tif (SW_CQE(hw_cqe)) {\n\t\tpr_debug(\"cq %p cqid 0x%x skip sw cqe cidx %u\\n\",\n\t\t\t cq, cq->cqid, cq->sw_cidx);\n\t\tt4_swcq_consume(cq);\n\t} else {\n\t\tpr_debug(\"cq %p cqid 0x%x skip hw cqe cidx %u\\n\",\n\t\t\t cq, cq->cqid, cq->cidx);\n\t\tt4_hwcq_consume(cq);\n\t}\n\treturn ret;\n}\n\nstatic int __c4iw_poll_cq_one(struct c4iw_cq *chp, struct c4iw_qp *qhp,\n\t\t\t      struct ib_wc *wc, struct c4iw_srq *srq)\n{\n\tstruct t4_cqe cqe;\n\tstruct t4_wq *wq = qhp ? &qhp->wq : NULL;\n\tu32 credit = 0;\n\tu8 cqe_flushed;\n\tu64 cookie = 0;\n\tint ret;\n\n\tret = poll_cq(wq, &(chp->cq), &cqe, &cqe_flushed, &cookie, &credit,\n\t\t      srq ? &srq->wq : NULL);\n\tif (ret)\n\t\tgoto out;\n\n\twc->wr_id = cookie;\n\twc->qp = &qhp->ibqp;\n\twc->vendor_err = CQE_STATUS(&cqe);\n\twc->wc_flags = 0;\n\n\t \n\tif (srq && !(srq->flags & T4_SRQ_LIMIT_SUPPORT) && srq->armed &&\n\t    srq->wq.in_use < srq->srq_limit)\n\t\tc4iw_dispatch_srq_limit_reached_event(srq);\n\n\tpr_debug(\"qpid 0x%x type %d opcode %d status 0x%x len %u wrid hi 0x%x lo 0x%x cookie 0x%llx\\n\",\n\t\t CQE_QPID(&cqe),\n\t\t CQE_TYPE(&cqe), CQE_OPCODE(&cqe),\n\t\t CQE_STATUS(&cqe), CQE_LEN(&cqe),\n\t\t CQE_WRID_HI(&cqe), CQE_WRID_LOW(&cqe),\n\t\t (unsigned long long)cookie);\n\n\tif (CQE_TYPE(&cqe) == 0) {\n\t\tif (!CQE_STATUS(&cqe))\n\t\t\twc->byte_len = CQE_LEN(&cqe);\n\t\telse\n\t\t\twc->byte_len = 0;\n\n\t\tswitch (CQE_OPCODE(&cqe)) {\n\t\tcase FW_RI_SEND:\n\t\t\twc->opcode = IB_WC_RECV;\n\t\t\tbreak;\n\t\tcase FW_RI_SEND_WITH_INV:\n\t\tcase FW_RI_SEND_WITH_SE_INV:\n\t\t\twc->opcode = IB_WC_RECV;\n\t\t\twc->ex.invalidate_rkey = CQE_WRID_STAG(&cqe);\n\t\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\t\tc4iw_invalidate_mr(qhp->rhp, wc->ex.invalidate_rkey);\n\t\t\tbreak;\n\t\tcase FW_RI_WRITE_IMMEDIATE:\n\t\t\twc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\n\t\t\twc->ex.imm_data = CQE_IMM_DATA(&cqe);\n\t\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"Unexpected opcode %d in the CQE received for QPID=0x%0x\\n\",\n\t\t\t       CQE_OPCODE(&cqe), CQE_QPID(&cqe));\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tswitch (CQE_OPCODE(&cqe)) {\n\t\tcase FW_RI_WRITE_IMMEDIATE:\n\t\tcase FW_RI_RDMA_WRITE:\n\t\t\twc->opcode = IB_WC_RDMA_WRITE;\n\t\t\tbreak;\n\t\tcase FW_RI_READ_REQ:\n\t\t\twc->opcode = IB_WC_RDMA_READ;\n\t\t\twc->byte_len = CQE_LEN(&cqe);\n\t\t\tbreak;\n\t\tcase FW_RI_SEND_WITH_INV:\n\t\tcase FW_RI_SEND_WITH_SE_INV:\n\t\t\twc->opcode = IB_WC_SEND;\n\t\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\t\tbreak;\n\t\tcase FW_RI_SEND:\n\t\tcase FW_RI_SEND_WITH_SE:\n\t\t\twc->opcode = IB_WC_SEND;\n\t\t\tbreak;\n\n\t\tcase FW_RI_LOCAL_INV:\n\t\t\twc->opcode = IB_WC_LOCAL_INV;\n\t\t\tbreak;\n\t\tcase FW_RI_FAST_REGISTER:\n\t\t\twc->opcode = IB_WC_REG_MR;\n\n\t\t\t \n\t\t\tif (CQE_STATUS(&cqe) != T4_ERR_SUCCESS)\n\t\t\t\tc4iw_invalidate_mr(qhp->rhp,\n\t\t\t\t\t\t   CQE_WRID_FR_STAG(&cqe));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"Unexpected opcode %d in the CQE received for QPID=0x%0x\\n\",\n\t\t\t       CQE_OPCODE(&cqe), CQE_QPID(&cqe));\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (cqe_flushed)\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\telse {\n\n\t\tswitch (CQE_STATUS(&cqe)) {\n\t\tcase T4_ERR_SUCCESS:\n\t\t\twc->status = IB_WC_SUCCESS;\n\t\t\tbreak;\n\t\tcase T4_ERR_STAG:\n\t\t\twc->status = IB_WC_LOC_ACCESS_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_PDID:\n\t\t\twc->status = IB_WC_LOC_PROT_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_QPID:\n\t\tcase T4_ERR_ACCESS:\n\t\t\twc->status = IB_WC_LOC_ACCESS_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_WRAP:\n\t\t\twc->status = IB_WC_GENERAL_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_BOUND:\n\t\t\twc->status = IB_WC_LOC_LEN_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_INVALIDATE_SHARED_MR:\n\t\tcase T4_ERR_INVALIDATE_MR_WITH_MW_BOUND:\n\t\t\twc->status = IB_WC_MW_BIND_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_CRC:\n\t\tcase T4_ERR_MARKER:\n\t\tcase T4_ERR_PDU_LEN_ERR:\n\t\tcase T4_ERR_OUT_OF_RQE:\n\t\tcase T4_ERR_DDP_VERSION:\n\t\tcase T4_ERR_RDMA_VERSION:\n\t\tcase T4_ERR_DDP_QUEUE_NUM:\n\t\tcase T4_ERR_MSN:\n\t\tcase T4_ERR_TBIT:\n\t\tcase T4_ERR_MO:\n\t\tcase T4_ERR_MSN_RANGE:\n\t\tcase T4_ERR_IRD_OVERFLOW:\n\t\tcase T4_ERR_OPCODE:\n\t\tcase T4_ERR_INTERNAL_ERR:\n\t\t\twc->status = IB_WC_FATAL_ERR;\n\t\t\tbreak;\n\t\tcase T4_ERR_SWFLUSH:\n\t\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"Unexpected cqe_status 0x%x for QPID=0x%0x\\n\",\n\t\t\t       CQE_STATUS(&cqe), CQE_QPID(&cqe));\n\t\t\twc->status = IB_WC_FATAL_ERR;\n\t\t}\n\t}\nout:\n\treturn ret;\n}\n\n \nstatic int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)\n{\n\tstruct c4iw_srq *srq = NULL;\n\tstruct c4iw_qp *qhp = NULL;\n\tstruct t4_cqe *rd_cqe;\n\tint ret;\n\n\tret = t4_next_cqe(&chp->cq, &rd_cqe);\n\n\tif (ret)\n\t\treturn ret;\n\n\tqhp = get_qhp(chp->rhp, CQE_QPID(rd_cqe));\n\tif (qhp) {\n\t\tspin_lock(&qhp->lock);\n\t\tsrq = qhp->srq;\n\t\tif (srq)\n\t\t\tspin_lock(&srq->lock);\n\t\tret = __c4iw_poll_cq_one(chp, qhp, wc, srq);\n\t\tspin_unlock(&qhp->lock);\n\t\tif (srq)\n\t\t\tspin_unlock(&srq->lock);\n\t} else {\n\t\tret = __c4iw_poll_cq_one(chp, NULL, wc, NULL);\n\t}\n\treturn ret;\n}\n\nint c4iw_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tstruct c4iw_cq *chp;\n\tunsigned long flags;\n\tint npolled;\n\tint err = 0;\n\n\tchp = to_c4iw_cq(ibcq);\n\n\tspin_lock_irqsave(&chp->lock, flags);\n\tfor (npolled = 0; npolled < num_entries; ++npolled) {\n\t\tdo {\n\t\t\terr = c4iw_poll_cq_one(chp, wc + npolled);\n\t\t} while (err == -EAGAIN);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&chp->lock, flags);\n\treturn !err || err == -ENODATA ? npolled : err;\n}\n\nvoid c4iw_cq_rem_ref(struct c4iw_cq *chp)\n{\n\tif (refcount_dec_and_test(&chp->refcnt))\n\t\tcomplete(&chp->cq_rel_comp);\n}\n\nint c4iw_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata)\n{\n\tstruct c4iw_cq *chp;\n\tstruct c4iw_ucontext *ucontext;\n\n\tpr_debug(\"ib_cq %p\\n\", ib_cq);\n\tchp = to_c4iw_cq(ib_cq);\n\n\txa_erase_irq(&chp->rhp->cqs, chp->cq.cqid);\n\tc4iw_cq_rem_ref(chp);\n\twait_for_completion(&chp->cq_rel_comp);\n\n\tucontext = rdma_udata_to_drv_context(udata, struct c4iw_ucontext,\n\t\t\t\t\t     ibucontext);\n\tdestroy_cq(&chp->rhp->rdev, &chp->cq,\n\t\t   ucontext ? &ucontext->uctx : &chp->cq.rdev->uctx,\n\t\t   chp->destroy_skb, chp->wr_waitp);\n\tc4iw_put_wr_wait(chp->wr_waitp);\n\treturn 0;\n}\n\nint c4iw_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t   struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tint entries = attr->cqe;\n\tint vector = attr->comp_vector;\n\tstruct c4iw_dev *rhp = to_c4iw_dev(ibcq->device);\n\tstruct c4iw_cq *chp = to_c4iw_cq(ibcq);\n\tstruct c4iw_create_cq ucmd;\n\tstruct c4iw_create_cq_resp uresp;\n\tint ret, wr_len;\n\tsize_t memsize, hwentries;\n\tstruct c4iw_mm_entry *mm, *mm2;\n\tstruct c4iw_ucontext *ucontext = rdma_udata_to_drv_context(\n\t\tudata, struct c4iw_ucontext, ibucontext);\n\n\tpr_debug(\"ib_dev %p entries %d\\n\", ibdev, entries);\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (entries < 1 || entries > ibdev->attrs.max_cqe)\n\t\treturn -EINVAL;\n\n\tif (vector >= rhp->rdev.lldi.nciq)\n\t\treturn -EINVAL;\n\n\tif (udata) {\n\t\tif (udata->inlen < sizeof(ucmd))\n\t\t\tucontext->is_32b_cqe = 1;\n\t}\n\n\tchp->wr_waitp = c4iw_alloc_wr_wait(GFP_KERNEL);\n\tif (!chp->wr_waitp) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_chp;\n\t}\n\tc4iw_init_wr_wait(chp->wr_waitp);\n\n\twr_len = sizeof(struct fw_ri_res_wr) + sizeof(struct fw_ri_res);\n\tchp->destroy_skb = alloc_skb(wr_len, GFP_KERNEL);\n\tif (!chp->destroy_skb) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_wr_wait;\n\t}\n\n\t \n\tentries++;\n\n\t \n\tentries++;\n\n\t \n\tentries = roundup(entries, 16);\n\n\t \n\thwentries = min(entries * 2, rhp->rdev.hw_queue.t4_max_iq_size);\n\n\t \n\tif (hwentries < 64)\n\t\thwentries = 64;\n\n\tmemsize = hwentries * ((ucontext && ucontext->is_32b_cqe) ?\n\t\t\t(sizeof(*chp->cq.queue) / 2) : sizeof(*chp->cq.queue));\n\n\t \n\tif (udata)\n\t\tmemsize = roundup(memsize, PAGE_SIZE);\n\n\tchp->cq.size = hwentries;\n\tchp->cq.memsize = memsize;\n\tchp->cq.vector = vector;\n\n\tret = create_cq(&rhp->rdev, &chp->cq,\n\t\t\tucontext ? &ucontext->uctx : &rhp->rdev.uctx,\n\t\t\tchp->wr_waitp);\n\tif (ret)\n\t\tgoto err_free_skb;\n\n\tchp->rhp = rhp;\n\tchp->cq.size--;\t\t\t\t \n\tchp->ibcq.cqe = entries - 2;\n\tspin_lock_init(&chp->lock);\n\tspin_lock_init(&chp->comp_handler_lock);\n\trefcount_set(&chp->refcnt, 1);\n\tinit_completion(&chp->cq_rel_comp);\n\tret = xa_insert_irq(&rhp->cqs, chp->cq.cqid, chp, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err_destroy_cq;\n\n\tif (ucontext) {\n\t\tret = -ENOMEM;\n\t\tmm = kmalloc(sizeof(*mm), GFP_KERNEL);\n\t\tif (!mm)\n\t\t\tgoto err_remove_handle;\n\t\tmm2 = kmalloc(sizeof(*mm2), GFP_KERNEL);\n\t\tif (!mm2)\n\t\t\tgoto err_free_mm;\n\n\t\tmemset(&uresp, 0, sizeof(uresp));\n\t\turesp.qid_mask = rhp->rdev.cqmask;\n\t\turesp.cqid = chp->cq.cqid;\n\t\turesp.size = chp->cq.size;\n\t\turesp.memsize = chp->cq.memsize;\n\t\tspin_lock(&ucontext->mmap_lock);\n\t\turesp.key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\turesp.gts_key = ucontext->key;\n\t\tucontext->key += PAGE_SIZE;\n\t\t \n\t\turesp.flags |= C4IW_64B_CQE;\n\n\t\tspin_unlock(&ucontext->mmap_lock);\n\t\tret = ib_copy_to_udata(udata, &uresp,\n\t\t\t\t       ucontext->is_32b_cqe ?\n\t\t\t\t       sizeof(uresp) - sizeof(uresp.flags) :\n\t\t\t\t       sizeof(uresp));\n\t\tif (ret)\n\t\t\tgoto err_free_mm2;\n\n\t\tmm->key = uresp.key;\n\t\tmm->addr = virt_to_phys(chp->cq.queue);\n\t\tmm->len = chp->cq.memsize;\n\t\tinsert_mmap(ucontext, mm);\n\n\t\tmm2->key = uresp.gts_key;\n\t\tmm2->addr = chp->cq.bar2_pa;\n\t\tmm2->len = PAGE_SIZE;\n\t\tinsert_mmap(ucontext, mm2);\n\t}\n\n\tpr_debug(\"cqid 0x%0x chp %p size %u memsize %zu, dma_addr %pad\\n\",\n\t\t chp->cq.cqid, chp, chp->cq.size, chp->cq.memsize,\n\t\t &chp->cq.dma_addr);\n\treturn 0;\nerr_free_mm2:\n\tkfree(mm2);\nerr_free_mm:\n\tkfree(mm);\nerr_remove_handle:\n\txa_erase_irq(&rhp->cqs, chp->cq.cqid);\nerr_destroy_cq:\n\tdestroy_cq(&chp->rhp->rdev, &chp->cq,\n\t\t   ucontext ? &ucontext->uctx : &rhp->rdev.uctx,\n\t\t   chp->destroy_skb, chp->wr_waitp);\nerr_free_skb:\n\tkfree_skb(chp->destroy_skb);\nerr_free_wr_wait:\n\tc4iw_put_wr_wait(chp->wr_waitp);\nerr_free_chp:\n\treturn ret;\n}\n\nint c4iw_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags)\n{\n\tstruct c4iw_cq *chp;\n\tint ret = 0;\n\tunsigned long flag;\n\n\tchp = to_c4iw_cq(ibcq);\n\tspin_lock_irqsave(&chp->lock, flag);\n\tt4_arm_cq(&chp->cq,\n\t\t  (flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED);\n\tif (flags & IB_CQ_REPORT_MISSED_EVENTS)\n\t\tret = t4_cq_notempty(&chp->cq);\n\tspin_unlock_irqrestore(&chp->lock, flag);\n\treturn ret;\n}\n\nvoid c4iw_flush_srqidx(struct c4iw_qp *qhp, u32 srqidx)\n{\n\tstruct c4iw_cq *rchp = to_c4iw_cq(qhp->ibqp.recv_cq);\n\tunsigned long flag;\n\n\t \n\tspin_lock_irqsave(&rchp->lock, flag);\n\tspin_lock(&qhp->lock);\n\n\t \n\tinsert_recv_cqe(&qhp->wq, &rchp->cq, srqidx);\n\n\tspin_unlock(&qhp->lock);\n\tspin_unlock_irqrestore(&rchp->lock, flag);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}