{
  "module_name": "umr.c",
  "hash_id": "c0153adbda15cb90156d2a783ad75b9113103408f3a396cfbf1b4b439a0a74f3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/umr.c",
  "human_readable_source": "\n \n\n#include <rdma/ib_umem_odp.h>\n#include \"mlx5_ib.h\"\n#include \"umr.h\"\n#include \"wr.h\"\n\n \nvoid *xlt_emergency_page;\nstatic DEFINE_MUTEX(xlt_emergency_page_mutex);\n\nstatic __be64 get_umr_enable_mr_mask(void)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_KEY |\n\t\t MLX5_MKEY_MASK_FREE;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic __be64 get_umr_disable_mr_mask(void)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_FREE;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic __be64 get_umr_update_translation_mask(void)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_LEN |\n\t\t MLX5_MKEY_MASK_PAGE_SIZE |\n\t\t MLX5_MKEY_MASK_START_ADDR;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic __be64 get_umr_update_access_mask(struct mlx5_ib_dev *dev)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_LR |\n\t\t MLX5_MKEY_MASK_LW |\n\t\t MLX5_MKEY_MASK_RR |\n\t\t MLX5_MKEY_MASK_RW;\n\n\tif (MLX5_CAP_GEN(dev->mdev, atomic))\n\t\tresult |= MLX5_MKEY_MASK_A;\n\n\tif (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))\n\t\tresult |= MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE;\n\n\tif (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))\n\t\tresult |= MLX5_MKEY_MASK_RELAXED_ORDERING_READ;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic __be64 get_umr_update_pd_mask(void)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_PD;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic int umr_check_mkey_mask(struct mlx5_ib_dev *dev, u64 mask)\n{\n\tif (mask & MLX5_MKEY_MASK_PAGE_SIZE &&\n\t    MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))\n\t\treturn -EPERM;\n\n\tif (mask & MLX5_MKEY_MASK_A &&\n\t    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))\n\t\treturn -EPERM;\n\n\tif (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE &&\n\t    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))\n\t\treturn -EPERM;\n\n\tif (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_READ &&\n\t    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nenum {\n\tMAX_UMR_WR = 128,\n};\n\nstatic int mlx5r_umr_qp_rst2rts(struct mlx5_ib_dev *dev, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr attr = {};\n\tint ret;\n\n\tattr.qp_state = IB_QPS_INIT;\n\tattr.port_num = 1;\n\tret = ib_modify_qp(qp, &attr,\n\t\t\t   IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT);\n\tif (ret) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't modify UMR QP\\n\");\n\t\treturn ret;\n\t}\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.qp_state = IB_QPS_RTR;\n\n\tret = ib_modify_qp(qp, &attr, IB_QP_STATE);\n\tif (ret) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't modify umr QP to rtr\\n\");\n\t\treturn ret;\n\t}\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.qp_state = IB_QPS_RTS;\n\tret = ib_modify_qp(qp, &attr, IB_QP_STATE);\n\tif (ret) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't modify umr QP to rts\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nint mlx5r_umr_resource_init(struct mlx5_ib_dev *dev)\n{\n\tstruct ib_qp_init_attr init_attr = {};\n\tstruct ib_pd *pd;\n\tstruct ib_cq *cq;\n\tstruct ib_qp *qp;\n\tint ret;\n\n\tpd = ib_alloc_pd(&dev->ib_dev, 0);\n\tif (IS_ERR(pd)) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't create PD for sync UMR QP\\n\");\n\t\treturn PTR_ERR(pd);\n\t}\n\n\tcq = ib_alloc_cq(&dev->ib_dev, NULL, 128, 0, IB_POLL_SOFTIRQ);\n\tif (IS_ERR(cq)) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't create CQ for sync UMR QP\\n\");\n\t\tret = PTR_ERR(cq);\n\t\tgoto destroy_pd;\n\t}\n\n\tinit_attr.send_cq = cq;\n\tinit_attr.recv_cq = cq;\n\tinit_attr.sq_sig_type = IB_SIGNAL_ALL_WR;\n\tinit_attr.cap.max_send_wr = MAX_UMR_WR;\n\tinit_attr.cap.max_send_sge = 1;\n\tinit_attr.qp_type = MLX5_IB_QPT_REG_UMR;\n\tinit_attr.port_num = 1;\n\tqp = ib_create_qp(pd, &init_attr);\n\tif (IS_ERR(qp)) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't create sync UMR QP\\n\");\n\t\tret = PTR_ERR(qp);\n\t\tgoto destroy_cq;\n\t}\n\n\tret = mlx5r_umr_qp_rst2rts(dev, qp);\n\tif (ret)\n\t\tgoto destroy_qp;\n\n\tdev->umrc.qp = qp;\n\tdev->umrc.cq = cq;\n\tdev->umrc.pd = pd;\n\n\tsema_init(&dev->umrc.sem, MAX_UMR_WR);\n\tmutex_init(&dev->umrc.lock);\n\tdev->umrc.state = MLX5_UMR_STATE_ACTIVE;\n\n\treturn 0;\n\ndestroy_qp:\n\tib_destroy_qp(qp);\ndestroy_cq:\n\tib_free_cq(cq);\ndestroy_pd:\n\tib_dealloc_pd(pd);\n\treturn ret;\n}\n\nvoid mlx5r_umr_resource_cleanup(struct mlx5_ib_dev *dev)\n{\n\tif (dev->umrc.state == MLX5_UMR_STATE_UNINIT)\n\t\treturn;\n\tib_destroy_qp(dev->umrc.qp);\n\tib_free_cq(dev->umrc.cq);\n\tib_dealloc_pd(dev->umrc.pd);\n}\n\nstatic int mlx5r_umr_recover(struct mlx5_ib_dev *dev)\n{\n\tstruct umr_common *umrc = &dev->umrc;\n\tstruct ib_qp_attr attr;\n\tint err;\n\n\tattr.qp_state = IB_QPS_RESET;\n\terr = ib_modify_qp(umrc->qp, &attr, IB_QP_STATE);\n\tif (err) {\n\t\tmlx5_ib_dbg(dev, \"Couldn't modify UMR QP\\n\");\n\t\tgoto err;\n\t}\n\n\terr = mlx5r_umr_qp_rst2rts(dev, umrc->qp);\n\tif (err)\n\t\tgoto err;\n\n\tumrc->state = MLX5_UMR_STATE_ACTIVE;\n\treturn 0;\n\nerr:\n\tumrc->state = MLX5_UMR_STATE_ERR;\n\treturn err;\n}\n\nstatic int mlx5r_umr_post_send(struct ib_qp *ibqp, u32 mkey, struct ib_cqe *cqe,\n\t\t\t       struct mlx5r_umr_wqe *wqe, bool with_data)\n{\n\tunsigned int wqe_size =\n\t\twith_data ? sizeof(struct mlx5r_umr_wqe) :\n\t\t\t    sizeof(struct mlx5r_umr_wqe) -\n\t\t\t\t    sizeof(struct mlx5_wqe_data_seg);\n\tstruct mlx5_ib_dev *dev = to_mdev(ibqp->device);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tstruct mlx5_ib_qp *qp = to_mqp(ibqp);\n\tstruct mlx5_wqe_ctrl_seg *ctrl;\n\tunion {\n\t\tstruct ib_cqe *ib_cqe;\n\t\tu64 wr_id;\n\t} id;\n\tvoid *cur_edge, *seg;\n\tunsigned long flags;\n\tunsigned int idx;\n\tint size, err;\n\n\tif (unlikely(mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR))\n\t\treturn -EIO;\n\n\tspin_lock_irqsave(&qp->sq.lock, flags);\n\n\terr = mlx5r_begin_wqe(qp, &seg, &ctrl, &idx, &size, &cur_edge, 0,\n\t\t\t      cpu_to_be32(mkey), false, false);\n\tif (WARN_ON(err))\n\t\tgoto out;\n\n\tqp->sq.wr_data[idx] = MLX5_IB_WR_UMR;\n\n\tmlx5r_memcpy_send_wqe(&qp->sq, &cur_edge, &seg, &size, wqe, wqe_size);\n\n\tid.ib_cqe = cqe;\n\tmlx5r_finish_wqe(qp, ctrl, seg, size, cur_edge, idx, id.wr_id, 0,\n\t\t\t MLX5_FENCE_MODE_INITIATOR_SMALL, MLX5_OPCODE_UMR);\n\n\tmlx5r_ring_db(qp, 1, ctrl);\n\nout:\n\tspin_unlock_irqrestore(&qp->sq.lock, flags);\n\n\treturn err;\n}\n\nstatic void mlx5r_umr_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct mlx5_ib_umr_context *context =\n\t\tcontainer_of(wc->wr_cqe, struct mlx5_ib_umr_context, cqe);\n\n\tcontext->status = wc->status;\n\tcomplete(&context->done);\n}\n\nstatic inline void mlx5r_umr_init_context(struct mlx5r_umr_context *context)\n{\n\tcontext->cqe.done = mlx5r_umr_done;\n\tinit_completion(&context->done);\n}\n\nstatic int mlx5r_umr_post_send_wait(struct mlx5_ib_dev *dev, u32 mkey,\n\t\t\t\t   struct mlx5r_umr_wqe *wqe, bool with_data)\n{\n\tstruct umr_common *umrc = &dev->umrc;\n\tstruct mlx5r_umr_context umr_context;\n\tint err;\n\n\terr = umr_check_mkey_mask(dev, be64_to_cpu(wqe->ctrl_seg.mkey_mask));\n\tif (WARN_ON(err))\n\t\treturn err;\n\n\tmlx5r_umr_init_context(&umr_context);\n\n\tdown(&umrc->sem);\n\twhile (true) {\n\t\tmutex_lock(&umrc->lock);\n\t\tif (umrc->state == MLX5_UMR_STATE_ERR) {\n\t\t\tmutex_unlock(&umrc->lock);\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (umrc->state == MLX5_UMR_STATE_RECOVER) {\n\t\t\tmutex_unlock(&umrc->lock);\n\t\t\tusleep_range(3000, 5000);\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = mlx5r_umr_post_send(umrc->qp, mkey, &umr_context.cqe, wqe,\n\t\t\t\t\t  with_data);\n\t\tmutex_unlock(&umrc->lock);\n\t\tif (err) {\n\t\t\tmlx5_ib_warn(dev, \"UMR post send failed, err %d\\n\",\n\t\t\t\t     err);\n\t\t\tbreak;\n\t\t}\n\n\t\twait_for_completion(&umr_context.done);\n\n\t\tif (umr_context.status == IB_WC_SUCCESS)\n\t\t\tbreak;\n\n\t\tif (umr_context.status == IB_WC_WR_FLUSH_ERR)\n\t\t\tcontinue;\n\n\t\tWARN_ON_ONCE(1);\n\t\tmlx5_ib_warn(dev,\n\t\t\t\"reg umr failed (%u). Trying to recover and resubmit the flushed WQEs\\n\",\n\t\t\tumr_context.status);\n\t\tmutex_lock(&umrc->lock);\n\t\terr = mlx5r_umr_recover(dev);\n\t\tmutex_unlock(&umrc->lock);\n\t\tif (err)\n\t\t\tmlx5_ib_warn(dev, \"couldn't recover UMR, err %d\\n\",\n\t\t\t\t     err);\n\t\terr = -EFAULT;\n\t\tbreak;\n\t}\n\tup(&umrc->sem);\n\treturn err;\n}\n\n \nint mlx5r_umr_revoke_mr(struct mlx5_ib_mr *mr)\n{\n\tstruct mlx5_ib_dev *dev = mr_to_mdev(mr);\n\tstruct mlx5r_umr_wqe wqe = {};\n\n\tif (dev->mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR)\n\t\treturn 0;\n\n\twqe.ctrl_seg.mkey_mask |= get_umr_update_pd_mask();\n\twqe.ctrl_seg.mkey_mask |= get_umr_disable_mr_mask();\n\twqe.ctrl_seg.flags |= MLX5_UMR_INLINE;\n\n\tMLX5_SET(mkc, &wqe.mkey_seg, free, 1);\n\tMLX5_SET(mkc, &wqe.mkey_seg, pd, to_mpd(dev->umrc.pd)->pdn);\n\tMLX5_SET(mkc, &wqe.mkey_seg, qpn, 0xffffff);\n\tMLX5_SET(mkc, &wqe.mkey_seg, mkey_7_0,\n\t\t mlx5_mkey_variant(mr->mmkey.key));\n\n\treturn mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, false);\n}\n\nstatic void mlx5r_umr_set_access_flags(struct mlx5_ib_dev *dev,\n\t\t\t\t       struct mlx5_mkey_seg *seg,\n\t\t\t\t       unsigned int access_flags)\n{\n\tbool ro_read = (access_flags & IB_ACCESS_RELAXED_ORDERING) &&\n\t\t       (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) ||\n\t\t\tpcie_relaxed_ordering_enabled(dev->mdev->pdev));\n\n\tMLX5_SET(mkc, seg, a, !!(access_flags & IB_ACCESS_REMOTE_ATOMIC));\n\tMLX5_SET(mkc, seg, rw, !!(access_flags & IB_ACCESS_REMOTE_WRITE));\n\tMLX5_SET(mkc, seg, rr, !!(access_flags & IB_ACCESS_REMOTE_READ));\n\tMLX5_SET(mkc, seg, lw, !!(access_flags & IB_ACCESS_LOCAL_WRITE));\n\tMLX5_SET(mkc, seg, lr, 1);\n\tMLX5_SET(mkc, seg, relaxed_ordering_write,\n\t\t !!(access_flags & IB_ACCESS_RELAXED_ORDERING));\n\tMLX5_SET(mkc, seg, relaxed_ordering_read, ro_read);\n}\n\nint mlx5r_umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,\n\t\t\t      int access_flags)\n{\n\tstruct mlx5_ib_dev *dev = mr_to_mdev(mr);\n\tstruct mlx5r_umr_wqe wqe = {};\n\tint err;\n\n\twqe.ctrl_seg.mkey_mask = get_umr_update_access_mask(dev);\n\twqe.ctrl_seg.mkey_mask |= get_umr_update_pd_mask();\n\twqe.ctrl_seg.flags = MLX5_UMR_CHECK_FREE;\n\twqe.ctrl_seg.flags |= MLX5_UMR_INLINE;\n\n\tmlx5r_umr_set_access_flags(dev, &wqe.mkey_seg, access_flags);\n\tMLX5_SET(mkc, &wqe.mkey_seg, pd, to_mpd(pd)->pdn);\n\tMLX5_SET(mkc, &wqe.mkey_seg, qpn, 0xffffff);\n\tMLX5_SET(mkc, &wqe.mkey_seg, mkey_7_0,\n\t\t mlx5_mkey_variant(mr->mmkey.key));\n\n\terr = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, false);\n\tif (err)\n\t\treturn err;\n\n\tmr->access_flags = access_flags;\n\treturn 0;\n}\n\n#define MLX5_MAX_UMR_CHUNK                                                     \\\n\t((1 << (MLX5_MAX_UMR_SHIFT + 4)) - MLX5_UMR_FLEX_ALIGNMENT)\n#define MLX5_SPARE_UMR_CHUNK 0x10000\n\n \nstatic void *mlx5r_umr_alloc_xlt(size_t *nents, size_t ent_size, gfp_t gfp_mask)\n{\n\tconst size_t xlt_chunk_align = MLX5_UMR_FLEX_ALIGNMENT / ent_size;\n\tsize_t size;\n\tvoid *res = NULL;\n\n\tstatic_assert(PAGE_SIZE % MLX5_UMR_FLEX_ALIGNMENT == 0);\n\n\t \n\tmight_sleep();\n\n\tgfp_mask |= __GFP_ZERO | __GFP_NORETRY;\n\n\t \n\tsize = min_t(size_t, ent_size * ALIGN(*nents, xlt_chunk_align),\n\t\t     MLX5_MAX_UMR_CHUNK);\n\t*nents = size / ent_size;\n\tres = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,\n\t\t\t\t       get_order(size));\n\tif (res)\n\t\treturn res;\n\n\tif (size > MLX5_SPARE_UMR_CHUNK) {\n\t\tsize = MLX5_SPARE_UMR_CHUNK;\n\t\t*nents = size / ent_size;\n\t\tres = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,\n\t\t\t\t\t       get_order(size));\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\t*nents = PAGE_SIZE / ent_size;\n\tres = (void *)__get_free_page(gfp_mask);\n\tif (res)\n\t\treturn res;\n\n\tmutex_lock(&xlt_emergency_page_mutex);\n\tmemset(xlt_emergency_page, 0, PAGE_SIZE);\n\treturn xlt_emergency_page;\n}\n\nstatic void mlx5r_umr_free_xlt(void *xlt, size_t length)\n{\n\tif (xlt == xlt_emergency_page) {\n\t\tmutex_unlock(&xlt_emergency_page_mutex);\n\t\treturn;\n\t}\n\n\tfree_pages((unsigned long)xlt, get_order(length));\n}\n\nstatic void mlx5r_umr_unmap_free_xlt(struct mlx5_ib_dev *dev, void *xlt,\n\t\t\t\t     struct ib_sge *sg)\n{\n\tstruct device *ddev = &dev->mdev->pdev->dev;\n\n\tdma_unmap_single(ddev, sg->addr, sg->length, DMA_TO_DEVICE);\n\tmlx5r_umr_free_xlt(xlt, sg->length);\n}\n\n \nstatic void *mlx5r_umr_create_xlt(struct mlx5_ib_dev *dev, struct ib_sge *sg,\n\t\t\t\t  size_t nents, size_t ent_size,\n\t\t\t\t  unsigned int flags)\n{\n\tstruct device *ddev = &dev->mdev->pdev->dev;\n\tdma_addr_t dma;\n\tvoid *xlt;\n\n\txlt = mlx5r_umr_alloc_xlt(&nents, ent_size,\n\t\t\t\t flags & MLX5_IB_UPD_XLT_ATOMIC ? GFP_ATOMIC :\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\tsg->length = nents * ent_size;\n\tdma = dma_map_single(ddev, xlt, sg->length, DMA_TO_DEVICE);\n\tif (dma_mapping_error(ddev, dma)) {\n\t\tmlx5_ib_err(dev, \"unable to map DMA during XLT update.\\n\");\n\t\tmlx5r_umr_free_xlt(xlt, sg->length);\n\t\treturn NULL;\n\t}\n\tsg->addr = dma;\n\tsg->lkey = dev->umrc.pd->local_dma_lkey;\n\n\treturn xlt;\n}\n\nstatic void\nmlx5r_umr_set_update_xlt_ctrl_seg(struct mlx5_wqe_umr_ctrl_seg *ctrl_seg,\n\t\t\t\t  unsigned int flags, struct ib_sge *sg)\n{\n\tif (!(flags & MLX5_IB_UPD_XLT_ENABLE))\n\t\t \n\t\tctrl_seg->flags = MLX5_UMR_CHECK_FREE;\n\telse\n\t\t \n\t\tctrl_seg->flags = MLX5_UMR_CHECK_NOT_FREE;\n\tctrl_seg->xlt_octowords =\n\t\tcpu_to_be16(mlx5r_umr_get_xlt_octo(sg->length));\n}\n\nstatic void mlx5r_umr_set_update_xlt_mkey_seg(struct mlx5_ib_dev *dev,\n\t\t\t\t\t      struct mlx5_mkey_seg *mkey_seg,\n\t\t\t\t\t      struct mlx5_ib_mr *mr,\n\t\t\t\t\t      unsigned int page_shift)\n{\n\tmlx5r_umr_set_access_flags(dev, mkey_seg, mr->access_flags);\n\tMLX5_SET(mkc, mkey_seg, pd, to_mpd(mr->ibmr.pd)->pdn);\n\tMLX5_SET64(mkc, mkey_seg, start_addr, mr->ibmr.iova);\n\tMLX5_SET64(mkc, mkey_seg, len, mr->ibmr.length);\n\tMLX5_SET(mkc, mkey_seg, log_page_size, page_shift);\n\tMLX5_SET(mkc, mkey_seg, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkey_seg, mkey_7_0, mlx5_mkey_variant(mr->mmkey.key));\n}\n\nstatic void\nmlx5r_umr_set_update_xlt_data_seg(struct mlx5_wqe_data_seg *data_seg,\n\t\t\t\t  struct ib_sge *sg)\n{\n\tdata_seg->byte_count = cpu_to_be32(sg->length);\n\tdata_seg->lkey = cpu_to_be32(sg->lkey);\n\tdata_seg->addr = cpu_to_be64(sg->addr);\n}\n\nstatic void mlx5r_umr_update_offset(struct mlx5_wqe_umr_ctrl_seg *ctrl_seg,\n\t\t\t\t    u64 offset)\n{\n\tu64 octo_offset = mlx5r_umr_get_xlt_octo(offset);\n\n\tctrl_seg->xlt_offset = cpu_to_be16(octo_offset & 0xffff);\n\tctrl_seg->xlt_offset_47_16 = cpu_to_be32(octo_offset >> 16);\n\tctrl_seg->flags |= MLX5_UMR_TRANSLATION_OFFSET_EN;\n}\n\nstatic void mlx5r_umr_final_update_xlt(struct mlx5_ib_dev *dev,\n\t\t\t\t       struct mlx5r_umr_wqe *wqe,\n\t\t\t\t       struct mlx5_ib_mr *mr, struct ib_sge *sg,\n\t\t\t\t       unsigned int flags)\n{\n\tbool update_pd_access, update_translation;\n\n\tif (flags & MLX5_IB_UPD_XLT_ENABLE)\n\t\twqe->ctrl_seg.mkey_mask |= get_umr_enable_mr_mask();\n\n\tupdate_pd_access = flags & MLX5_IB_UPD_XLT_ENABLE ||\n\t\t\t   flags & MLX5_IB_UPD_XLT_PD ||\n\t\t\t   flags & MLX5_IB_UPD_XLT_ACCESS;\n\n\tif (update_pd_access) {\n\t\twqe->ctrl_seg.mkey_mask |= get_umr_update_access_mask(dev);\n\t\twqe->ctrl_seg.mkey_mask |= get_umr_update_pd_mask();\n\t}\n\n\tupdate_translation =\n\t\tflags & MLX5_IB_UPD_XLT_ENABLE || flags & MLX5_IB_UPD_XLT_ADDR;\n\n\tif (update_translation) {\n\t\twqe->ctrl_seg.mkey_mask |= get_umr_update_translation_mask();\n\t\tif (!mr->ibmr.length)\n\t\t\tMLX5_SET(mkc, &wqe->mkey_seg, length64, 1);\n\t}\n\n\twqe->ctrl_seg.xlt_octowords =\n\t\tcpu_to_be16(mlx5r_umr_get_xlt_octo(sg->length));\n\twqe->data_seg.byte_count = cpu_to_be32(sg->length);\n}\n\n \nint mlx5r_umr_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags)\n{\n\tstruct mlx5_ib_dev *dev = mr_to_mdev(mr);\n\tstruct device *ddev = &dev->mdev->pdev->dev;\n\tstruct mlx5r_umr_wqe wqe = {};\n\tstruct ib_block_iter biter;\n\tstruct mlx5_mtt *cur_mtt;\n\tsize_t orig_sg_length;\n\tstruct mlx5_mtt *mtt;\n\tsize_t final_size;\n\tstruct ib_sge sg;\n\tu64 offset = 0;\n\tint err = 0;\n\n\tif (WARN_ON(mr->umem->is_odp))\n\t\treturn -EINVAL;\n\n\tmtt = mlx5r_umr_create_xlt(\n\t\tdev, &sg, ib_umem_num_dma_blocks(mr->umem, 1 << mr->page_shift),\n\t\tsizeof(*mtt), flags);\n\tif (!mtt)\n\t\treturn -ENOMEM;\n\n\torig_sg_length = sg.length;\n\n\tmlx5r_umr_set_update_xlt_ctrl_seg(&wqe.ctrl_seg, flags, &sg);\n\tmlx5r_umr_set_update_xlt_mkey_seg(dev, &wqe.mkey_seg, mr,\n\t\t\t\t\t  mr->page_shift);\n\tmlx5r_umr_set_update_xlt_data_seg(&wqe.data_seg, &sg);\n\n\tcur_mtt = mtt;\n\trdma_umem_for_each_dma_block(mr->umem, &biter, BIT(mr->page_shift)) {\n\t\tif (cur_mtt == (void *)mtt + sg.length) {\n\t\t\tdma_sync_single_for_device(ddev, sg.addr, sg.length,\n\t\t\t\t\t\t   DMA_TO_DEVICE);\n\n\t\t\terr = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe,\n\t\t\t\t\t\t       true);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t\tdma_sync_single_for_cpu(ddev, sg.addr, sg.length,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\toffset += sg.length;\n\t\t\tmlx5r_umr_update_offset(&wqe.ctrl_seg, offset);\n\n\t\t\tcur_mtt = mtt;\n\t\t}\n\n\t\tcur_mtt->ptag =\n\t\t\tcpu_to_be64(rdma_block_iter_dma_address(&biter) |\n\t\t\t\t    MLX5_IB_MTT_PRESENT);\n\n\t\tif (mr->umem->is_dmabuf && (flags & MLX5_IB_UPD_XLT_ZAP))\n\t\t\tcur_mtt->ptag = 0;\n\n\t\tcur_mtt++;\n\t}\n\n\tfinal_size = (void *)cur_mtt - (void *)mtt;\n\tsg.length = ALIGN(final_size, MLX5_UMR_FLEX_ALIGNMENT);\n\tmemset(cur_mtt, 0, sg.length - final_size);\n\tmlx5r_umr_final_update_xlt(dev, &wqe, mr, &sg, flags);\n\n\tdma_sync_single_for_device(ddev, sg.addr, sg.length, DMA_TO_DEVICE);\n\terr = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, true);\n\nerr:\n\tsg.length = orig_sg_length;\n\tmlx5r_umr_unmap_free_xlt(dev, mtt, &sg);\n\treturn err;\n}\n\nstatic bool umr_can_use_indirect_mkey(struct mlx5_ib_dev *dev)\n{\n\treturn !MLX5_CAP_GEN(dev->mdev, umr_indirect_mkey_disabled);\n}\n\nint mlx5r_umr_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,\n\t\t\t int page_shift, int flags)\n{\n\tint desc_size = (flags & MLX5_IB_UPD_XLT_INDIRECT)\n\t\t\t       ? sizeof(struct mlx5_klm)\n\t\t\t       : sizeof(struct mlx5_mtt);\n\tconst int page_align = MLX5_UMR_FLEX_ALIGNMENT / desc_size;\n\tstruct mlx5_ib_dev *dev = mr_to_mdev(mr);\n\tstruct device *ddev = &dev->mdev->pdev->dev;\n\tconst int page_mask = page_align - 1;\n\tstruct mlx5r_umr_wqe wqe = {};\n\tsize_t pages_mapped = 0;\n\tsize_t pages_to_map = 0;\n\tsize_t size_to_map = 0;\n\tsize_t orig_sg_length;\n\tsize_t pages_iter;\n\tstruct ib_sge sg;\n\tint err = 0;\n\tvoid *xlt;\n\n\tif ((flags & MLX5_IB_UPD_XLT_INDIRECT) &&\n\t    !umr_can_use_indirect_mkey(dev))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(!mr->umem->is_odp))\n\t\treturn -EINVAL;\n\n\t \n\tif (idx & page_mask) {\n\t\tnpages += idx & page_mask;\n\t\tidx &= ~page_mask;\n\t}\n\tpages_to_map = ALIGN(npages, page_align);\n\n\txlt = mlx5r_umr_create_xlt(dev, &sg, npages, desc_size, flags);\n\tif (!xlt)\n\t\treturn -ENOMEM;\n\n\tpages_iter = sg.length / desc_size;\n\torig_sg_length = sg.length;\n\n\tif (!(flags & MLX5_IB_UPD_XLT_INDIRECT)) {\n\t\tstruct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);\n\t\tsize_t max_pages = ib_umem_odp_num_pages(odp) - idx;\n\n\t\tpages_to_map = min_t(size_t, pages_to_map, max_pages);\n\t}\n\n\tmlx5r_umr_set_update_xlt_ctrl_seg(&wqe.ctrl_seg, flags, &sg);\n\tmlx5r_umr_set_update_xlt_mkey_seg(dev, &wqe.mkey_seg, mr, page_shift);\n\tmlx5r_umr_set_update_xlt_data_seg(&wqe.data_seg, &sg);\n\n\tfor (pages_mapped = 0;\n\t     pages_mapped < pages_to_map && !err;\n\t     pages_mapped += pages_iter, idx += pages_iter) {\n\t\tnpages = min_t(int, pages_iter, pages_to_map - pages_mapped);\n\t\tsize_to_map = npages * desc_size;\n\t\tdma_sync_single_for_cpu(ddev, sg.addr, sg.length,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\tmlx5_odp_populate_xlt(xlt, idx, npages, mr, flags);\n\t\tdma_sync_single_for_device(ddev, sg.addr, sg.length,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tsg.length = ALIGN(size_to_map, MLX5_UMR_FLEX_ALIGNMENT);\n\n\t\tif (pages_mapped + pages_iter >= pages_to_map)\n\t\t\tmlx5r_umr_final_update_xlt(dev, &wqe, mr, &sg, flags);\n\t\tmlx5r_umr_update_offset(&wqe.ctrl_seg, idx * desc_size);\n\t\terr = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, true);\n\t}\n\tsg.length = orig_sg_length;\n\tmlx5r_umr_unmap_free_xlt(dev, xlt, &sg);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}