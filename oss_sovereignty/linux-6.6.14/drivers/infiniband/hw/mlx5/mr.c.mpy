{
  "module_name": "mr.c",
  "hash_id": "a5f9dd9b64dc8183980782a91c045cc458e4ab6716c4eb25613086048f2b033e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/mr.c",
  "human_readable_source": " \n\n\n#include <linux/kref.h>\n#include <linux/random.h>\n#include <linux/debugfs.h>\n#include <linux/export.h>\n#include <linux/delay.h>\n#include <linux/dma-buf.h>\n#include <linux/dma-resv.h>\n#include <rdma/ib_umem_odp.h>\n#include \"dm.h\"\n#include \"mlx5_ib.h\"\n#include \"umr.h\"\n\nenum {\n\tMAX_PENDING_REG_MR = 8,\n};\n\n#define MLX5_UMR_ALIGN 2048\n\nstatic void\ncreate_mkey_callback(int status, struct mlx5_async_work *context);\nstatic struct mlx5_ib_mr *reg_create(struct ib_pd *pd, struct ib_umem *umem,\n\t\t\t\t     u64 iova, int access_flags,\n\t\t\t\t     unsigned int page_size, bool populate);\n\nstatic void set_mkc_access_pd_addr_fields(void *mkc, int acc, u64 start_addr,\n\t\t\t\t\t  struct ib_pd *pd)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\n\tMLX5_SET(mkc, mkc, a, !!(acc & IB_ACCESS_REMOTE_ATOMIC));\n\tMLX5_SET(mkc, mkc, rw, !!(acc & IB_ACCESS_REMOTE_WRITE));\n\tMLX5_SET(mkc, mkc, rr, !!(acc & IB_ACCESS_REMOTE_READ));\n\tMLX5_SET(mkc, mkc, lw, !!(acc & IB_ACCESS_LOCAL_WRITE));\n\tMLX5_SET(mkc, mkc, lr, 1);\n\n\tif (acc & IB_ACCESS_RELAXED_ORDERING) {\n\t\tif (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write))\n\t\t\tMLX5_SET(mkc, mkc, relaxed_ordering_write, 1);\n\n\t\tif (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) ||\n\t\t    (MLX5_CAP_GEN(dev->mdev,\n\t\t\t\t  relaxed_ordering_read_pci_enabled) &&\n\t\t     pcie_relaxed_ordering_enabled(dev->mdev->pdev)))\n\t\t\tMLX5_SET(mkc, mkc, relaxed_ordering_read, 1);\n\t}\n\n\tMLX5_SET(mkc, mkc, pd, to_mpd(pd)->pdn);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET64(mkc, mkc, start_addr, start_addr);\n}\n\nstatic void assign_mkey_variant(struct mlx5_ib_dev *dev, u32 *mkey, u32 *in)\n{\n\tu8 key = atomic_inc_return(&dev->mkey_var);\n\tvoid *mkc;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, mkey_7_0, key);\n\t*mkey = key;\n}\n\nstatic int mlx5_ib_create_mkey(struct mlx5_ib_dev *dev,\n\t\t\t       struct mlx5_ib_mkey *mkey, u32 *in, int inlen)\n{\n\tint ret;\n\n\tassign_mkey_variant(dev, &mkey->key, in);\n\tret = mlx5_core_create_mkey(dev->mdev, &mkey->key, in, inlen);\n\tif (!ret)\n\t\tinit_waitqueue_head(&mkey->wait);\n\n\treturn ret;\n}\n\nstatic int mlx5_ib_create_mkey_cb(struct mlx5r_async_create_mkey *async_create)\n{\n\tstruct mlx5_ib_dev *dev = async_create->ent->dev;\n\tsize_t inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tsize_t outlen = MLX5_ST_SZ_BYTES(create_mkey_out);\n\n\tMLX5_SET(create_mkey_in, async_create->in, opcode,\n\t\t MLX5_CMD_OP_CREATE_MKEY);\n\tassign_mkey_variant(dev, &async_create->mkey, async_create->in);\n\treturn mlx5_cmd_exec_cb(&dev->async_ctx, async_create->in, inlen,\n\t\t\t\tasync_create->out, outlen, create_mkey_callback,\n\t\t\t\t&async_create->cb_work);\n}\n\nstatic int mkey_cache_max_order(struct mlx5_ib_dev *dev);\nstatic void queue_adjust_cache_locked(struct mlx5_cache_ent *ent);\n\nstatic int destroy_mkey(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)\n{\n\tWARN_ON(xa_load(&dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key)));\n\n\treturn mlx5_core_destroy_mkey(dev->mdev, mr->mmkey.key);\n}\n\nstatic void create_mkey_warn(struct mlx5_ib_dev *dev, int status, void *out)\n{\n\tif (status == -ENXIO)  \n\t\treturn;\n\n\tmlx5_ib_warn(dev, \"async reg mr failed. status %d\\n\", status);\n\tif (status != -EREMOTEIO)  \n\t\treturn;\n\n\t \n\tmlx5_cmd_out_err(dev->mdev, MLX5_CMD_OP_CREATE_MKEY, 0, out);\n}\n\nstatic int push_mkey_locked(struct mlx5_cache_ent *ent, bool limit_pendings,\n\t\t\t    void *to_store)\n{\n\tXA_STATE(xas, &ent->mkeys, 0);\n\tvoid *curr;\n\n\tif (limit_pendings &&\n\t    (ent->reserved - ent->stored) > MAX_PENDING_REG_MR)\n\t\treturn -EAGAIN;\n\n\twhile (1) {\n\t\t \n\t\txas_set(&xas, ent->reserved);\n\t\tcurr = xas_load(&xas);\n\t\tif (!curr) {\n\t\t\tif (to_store && ent->stored == ent->reserved)\n\t\t\t\txas_store(&xas, to_store);\n\t\t\telse\n\t\t\t\txas_store(&xas, XA_ZERO_ENTRY);\n\t\t\tif (xas_valid(&xas)) {\n\t\t\t\tent->reserved++;\n\t\t\t\tif (to_store) {\n\t\t\t\t\tif (ent->stored != ent->reserved)\n\t\t\t\t\t\t__xa_store(&ent->mkeys,\n\t\t\t\t\t\t\t   ent->stored,\n\t\t\t\t\t\t\t   to_store,\n\t\t\t\t\t\t\t   GFP_KERNEL);\n\t\t\t\t\tent->stored++;\n\t\t\t\t\tqueue_adjust_cache_locked(ent);\n\t\t\t\t\tWRITE_ONCE(ent->dev->cache.last_add,\n\t\t\t\t\t\t   jiffies);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\txa_unlock_irq(&ent->mkeys);\n\n\t\t \n\t\tif (!xas_nomem(&xas, GFP_KERNEL))\n\t\t\tbreak;\n\t\txa_lock_irq(&ent->mkeys);\n\t}\n\txa_lock_irq(&ent->mkeys);\n\tif (xas_error(&xas))\n\t\treturn xas_error(&xas);\n\tif (WARN_ON(curr))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int push_mkey(struct mlx5_cache_ent *ent, bool limit_pendings,\n\t\t     void *to_store)\n{\n\tint ret;\n\n\txa_lock_irq(&ent->mkeys);\n\tret = push_mkey_locked(ent, limit_pendings, to_store);\n\txa_unlock_irq(&ent->mkeys);\n\treturn ret;\n}\n\nstatic void undo_push_reserve_mkey(struct mlx5_cache_ent *ent)\n{\n\tvoid *old;\n\n\tent->reserved--;\n\told = __xa_erase(&ent->mkeys, ent->reserved);\n\tWARN_ON(old);\n}\n\nstatic void push_to_reserved(struct mlx5_cache_ent *ent, u32 mkey)\n{\n\tvoid *old;\n\n\told = __xa_store(&ent->mkeys, ent->stored, xa_mk_value(mkey), 0);\n\tWARN_ON(old);\n\tent->stored++;\n}\n\nstatic u32 pop_stored_mkey(struct mlx5_cache_ent *ent)\n{\n\tvoid *old, *xa_mkey;\n\n\tent->stored--;\n\tent->reserved--;\n\n\tif (ent->stored == ent->reserved) {\n\t\txa_mkey = __xa_erase(&ent->mkeys, ent->stored);\n\t\tWARN_ON(!xa_mkey);\n\t\treturn (u32)xa_to_value(xa_mkey);\n\t}\n\n\txa_mkey = __xa_store(&ent->mkeys, ent->stored, XA_ZERO_ENTRY,\n\t\t\t     GFP_KERNEL);\n\tWARN_ON(!xa_mkey || xa_is_err(xa_mkey));\n\told = __xa_erase(&ent->mkeys, ent->reserved);\n\tWARN_ON(old);\n\treturn (u32)xa_to_value(xa_mkey);\n}\n\nstatic void create_mkey_callback(int status, struct mlx5_async_work *context)\n{\n\tstruct mlx5r_async_create_mkey *mkey_out =\n\t\tcontainer_of(context, struct mlx5r_async_create_mkey, cb_work);\n\tstruct mlx5_cache_ent *ent = mkey_out->ent;\n\tstruct mlx5_ib_dev *dev = ent->dev;\n\tunsigned long flags;\n\n\tif (status) {\n\t\tcreate_mkey_warn(dev, status, mkey_out->out);\n\t\tkfree(mkey_out);\n\t\txa_lock_irqsave(&ent->mkeys, flags);\n\t\tundo_push_reserve_mkey(ent);\n\t\tWRITE_ONCE(dev->fill_delay, 1);\n\t\txa_unlock_irqrestore(&ent->mkeys, flags);\n\t\tmod_timer(&dev->delay_timer, jiffies + HZ);\n\t\treturn;\n\t}\n\n\tmkey_out->mkey |= mlx5_idx_to_mkey(\n\t\tMLX5_GET(create_mkey_out, mkey_out->out, mkey_index));\n\tWRITE_ONCE(dev->cache.last_add, jiffies);\n\n\txa_lock_irqsave(&ent->mkeys, flags);\n\tpush_to_reserved(ent, mkey_out->mkey);\n\t \n\tqueue_adjust_cache_locked(ent);\n\txa_unlock_irqrestore(&ent->mkeys, flags);\n\tkfree(mkey_out);\n}\n\nstatic int get_mkc_octo_size(unsigned int access_mode, unsigned int ndescs)\n{\n\tint ret = 0;\n\n\tswitch (access_mode) {\n\tcase MLX5_MKC_ACCESS_MODE_MTT:\n\t\tret = DIV_ROUND_UP(ndescs, MLX5_IB_UMR_OCTOWORD /\n\t\t\t\t\t\t   sizeof(struct mlx5_mtt));\n\t\tbreak;\n\tcase MLX5_MKC_ACCESS_MODE_KSM:\n\t\tret = DIV_ROUND_UP(ndescs, MLX5_IB_UMR_OCTOWORD /\n\t\t\t\t\t\t   sizeof(struct mlx5_klm));\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t}\n\treturn ret;\n}\n\nstatic void set_cache_mkc(struct mlx5_cache_ent *ent, void *mkc)\n{\n\tset_mkc_access_pd_addr_fields(mkc, ent->rb_key.access_flags, 0,\n\t\t\t\t      ent->dev->umrc.pd);\n\tMLX5_SET(mkc, mkc, free, 1);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, ent->rb_key.access_mode & 0x3);\n\tMLX5_SET(mkc, mkc, access_mode_4_2,\n\t\t(ent->rb_key.access_mode >> 2) & 0x7);\n\n\tMLX5_SET(mkc, mkc, translations_octword_size,\n\t\t get_mkc_octo_size(ent->rb_key.access_mode,\n\t\t\t\t   ent->rb_key.ndescs));\n\tMLX5_SET(mkc, mkc, log_page_size, PAGE_SHIFT);\n}\n\n \nstatic int add_keys(struct mlx5_cache_ent *ent, unsigned int num)\n{\n\tstruct mlx5r_async_create_mkey *async_create;\n\tvoid *mkc;\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < num; i++) {\n\t\tasync_create = kzalloc(sizeof(struct mlx5r_async_create_mkey),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!async_create)\n\t\t\treturn -ENOMEM;\n\t\tmkc = MLX5_ADDR_OF(create_mkey_in, async_create->in,\n\t\t\t\t   memory_key_mkey_entry);\n\t\tset_cache_mkc(ent, mkc);\n\t\tasync_create->ent = ent;\n\n\t\terr = push_mkey(ent, true, NULL);\n\t\tif (err)\n\t\t\tgoto free_async_create;\n\n\t\terr = mlx5_ib_create_mkey_cb(async_create);\n\t\tif (err) {\n\t\t\tmlx5_ib_warn(ent->dev, \"create mkey failed %d\\n\", err);\n\t\t\tgoto err_undo_reserve;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_undo_reserve:\n\txa_lock_irq(&ent->mkeys);\n\tundo_push_reserve_mkey(ent);\n\txa_unlock_irq(&ent->mkeys);\nfree_async_create:\n\tkfree(async_create);\n\treturn err;\n}\n\n \nstatic int create_cache_mkey(struct mlx5_cache_ent *ent, u32 *mkey)\n{\n\tsize_t inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tset_cache_mkc(ent, mkc);\n\n\terr = mlx5_core_create_mkey(ent->dev->mdev, mkey, in, inlen);\n\tif (err)\n\t\tgoto free_in;\n\n\tWRITE_ONCE(ent->dev->cache.last_add, jiffies);\nfree_in:\n\tkfree(in);\n\treturn err;\n}\n\nstatic void remove_cache_mr_locked(struct mlx5_cache_ent *ent)\n{\n\tu32 mkey;\n\n\tlockdep_assert_held(&ent->mkeys.xa_lock);\n\tif (!ent->stored)\n\t\treturn;\n\tmkey = pop_stored_mkey(ent);\n\txa_unlock_irq(&ent->mkeys);\n\tmlx5_core_destroy_mkey(ent->dev->mdev, mkey);\n\txa_lock_irq(&ent->mkeys);\n}\n\nstatic int resize_available_mrs(struct mlx5_cache_ent *ent, unsigned int target,\n\t\t\t\tbool limit_fill)\n\t __acquires(&ent->mkeys) __releases(&ent->mkeys)\n{\n\tint err;\n\n\tlockdep_assert_held(&ent->mkeys.xa_lock);\n\n\twhile (true) {\n\t\tif (limit_fill)\n\t\t\ttarget = ent->limit * 2;\n\t\tif (target == ent->reserved)\n\t\t\treturn 0;\n\t\tif (target > ent->reserved) {\n\t\t\tu32 todo = target - ent->reserved;\n\n\t\t\txa_unlock_irq(&ent->mkeys);\n\t\t\terr = add_keys(ent, todo);\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tusleep_range(3000, 5000);\n\t\t\txa_lock_irq(&ent->mkeys);\n\t\t\tif (err) {\n\t\t\t\tif (err != -EAGAIN)\n\t\t\t\t\treturn err;\n\t\t\t} else\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tremove_cache_mr_locked(ent);\n\t\t}\n\t}\n}\n\nstatic ssize_t size_write(struct file *filp, const char __user *buf,\n\t\t\t  size_t count, loff_t *pos)\n{\n\tstruct mlx5_cache_ent *ent = filp->private_data;\n\tu32 target;\n\tint err;\n\n\terr = kstrtou32_from_user(buf, count, 0, &target);\n\tif (err)\n\t\treturn err;\n\n\t \n\txa_lock_irq(&ent->mkeys);\n\tif (target < ent->in_use) {\n\t\terr = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\ttarget = target - ent->in_use;\n\tif (target < ent->limit || target > ent->limit*2) {\n\t\terr = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\terr = resize_available_mrs(ent, target, false);\n\tif (err)\n\t\tgoto err_unlock;\n\txa_unlock_irq(&ent->mkeys);\n\n\treturn count;\n\nerr_unlock:\n\txa_unlock_irq(&ent->mkeys);\n\treturn err;\n}\n\nstatic ssize_t size_read(struct file *filp, char __user *buf, size_t count,\n\t\t\t loff_t *pos)\n{\n\tstruct mlx5_cache_ent *ent = filp->private_data;\n\tchar lbuf[20];\n\tint err;\n\n\terr = snprintf(lbuf, sizeof(lbuf), \"%ld\\n\", ent->stored + ent->in_use);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn simple_read_from_buffer(buf, count, pos, lbuf, err);\n}\n\nstatic const struct file_operations size_fops = {\n\t.owner\t= THIS_MODULE,\n\t.open\t= simple_open,\n\t.write\t= size_write,\n\t.read\t= size_read,\n};\n\nstatic ssize_t limit_write(struct file *filp, const char __user *buf,\n\t\t\t   size_t count, loff_t *pos)\n{\n\tstruct mlx5_cache_ent *ent = filp->private_data;\n\tu32 var;\n\tint err;\n\n\terr = kstrtou32_from_user(buf, count, 0, &var);\n\tif (err)\n\t\treturn err;\n\n\t \n\txa_lock_irq(&ent->mkeys);\n\tent->limit = var;\n\terr = resize_available_mrs(ent, 0, true);\n\txa_unlock_irq(&ent->mkeys);\n\tif (err)\n\t\treturn err;\n\treturn count;\n}\n\nstatic ssize_t limit_read(struct file *filp, char __user *buf, size_t count,\n\t\t\t  loff_t *pos)\n{\n\tstruct mlx5_cache_ent *ent = filp->private_data;\n\tchar lbuf[20];\n\tint err;\n\n\terr = snprintf(lbuf, sizeof(lbuf), \"%d\\n\", ent->limit);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn simple_read_from_buffer(buf, count, pos, lbuf, err);\n}\n\nstatic const struct file_operations limit_fops = {\n\t.owner\t= THIS_MODULE,\n\t.open\t= simple_open,\n\t.write\t= limit_write,\n\t.read\t= limit_read,\n};\n\nstatic bool someone_adding(struct mlx5_mkey_cache *cache)\n{\n\tstruct mlx5_cache_ent *ent;\n\tstruct rb_node *node;\n\tbool ret;\n\n\tmutex_lock(&cache->rb_lock);\n\tfor (node = rb_first(&cache->rb_root); node; node = rb_next(node)) {\n\t\tent = rb_entry(node, struct mlx5_cache_ent, node);\n\t\txa_lock_irq(&ent->mkeys);\n\t\tret = ent->stored < ent->limit;\n\t\txa_unlock_irq(&ent->mkeys);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&cache->rb_lock);\n\t\t\treturn true;\n\t\t}\n\t}\n\tmutex_unlock(&cache->rb_lock);\n\treturn false;\n}\n\n \nstatic void queue_adjust_cache_locked(struct mlx5_cache_ent *ent)\n{\n\tlockdep_assert_held(&ent->mkeys.xa_lock);\n\n\tif (ent->disabled || READ_ONCE(ent->dev->fill_delay) || ent->is_tmp)\n\t\treturn;\n\tif (ent->stored < ent->limit) {\n\t\tent->fill_to_high_water = true;\n\t\tmod_delayed_work(ent->dev->cache.wq, &ent->dwork, 0);\n\t} else if (ent->fill_to_high_water &&\n\t\t   ent->reserved < 2 * ent->limit) {\n\t\t \n\t\tmod_delayed_work(ent->dev->cache.wq, &ent->dwork, 0);\n\t} else if (ent->stored == 2 * ent->limit) {\n\t\tent->fill_to_high_water = false;\n\t} else if (ent->stored > 2 * ent->limit) {\n\t\t \n\t\tent->fill_to_high_water = false;\n\t\tif (ent->stored != ent->reserved)\n\t\t\tqueue_delayed_work(ent->dev->cache.wq, &ent->dwork,\n\t\t\t\t\t   msecs_to_jiffies(1000));\n\t\telse\n\t\t\tmod_delayed_work(ent->dev->cache.wq, &ent->dwork, 0);\n\t}\n}\n\nstatic void __cache_work_func(struct mlx5_cache_ent *ent)\n{\n\tstruct mlx5_ib_dev *dev = ent->dev;\n\tstruct mlx5_mkey_cache *cache = &dev->cache;\n\tint err;\n\n\txa_lock_irq(&ent->mkeys);\n\tif (ent->disabled)\n\t\tgoto out;\n\n\tif (ent->fill_to_high_water && ent->reserved < 2 * ent->limit &&\n\t    !READ_ONCE(dev->fill_delay)) {\n\t\txa_unlock_irq(&ent->mkeys);\n\t\terr = add_keys(ent, 1);\n\t\txa_lock_irq(&ent->mkeys);\n\t\tif (ent->disabled)\n\t\t\tgoto out;\n\t\tif (err) {\n\t\t\t \n\t\t\tif (err != -EAGAIN) {\n\t\t\t\tmlx5_ib_warn(\n\t\t\t\t\tdev,\n\t\t\t\t\t\"add keys command failed, err %d\\n\",\n\t\t\t\t\terr);\n\t\t\t\tqueue_delayed_work(cache->wq, &ent->dwork,\n\t\t\t\t\t\t   msecs_to_jiffies(1000));\n\t\t\t}\n\t\t}\n\t} else if (ent->stored > 2 * ent->limit) {\n\t\tbool need_delay;\n\n\t\t \n\t\txa_unlock_irq(&ent->mkeys);\n\t\tneed_delay = need_resched() || someone_adding(cache) ||\n\t\t\t     !time_after(jiffies,\n\t\t\t\t\t READ_ONCE(cache->last_add) + 300 * HZ);\n\t\txa_lock_irq(&ent->mkeys);\n\t\tif (ent->disabled)\n\t\t\tgoto out;\n\t\tif (need_delay) {\n\t\t\tqueue_delayed_work(cache->wq, &ent->dwork, 300 * HZ);\n\t\t\tgoto out;\n\t\t}\n\t\tremove_cache_mr_locked(ent);\n\t\tqueue_adjust_cache_locked(ent);\n\t}\nout:\n\txa_unlock_irq(&ent->mkeys);\n}\n\nstatic void delayed_cache_work_func(struct work_struct *work)\n{\n\tstruct mlx5_cache_ent *ent;\n\n\tent = container_of(work, struct mlx5_cache_ent, dwork.work);\n\t__cache_work_func(ent);\n}\n\nstatic int cache_ent_key_cmp(struct mlx5r_cache_rb_key key1,\n\t\t\t     struct mlx5r_cache_rb_key key2)\n{\n\tint res;\n\n\tres = key1.ats - key2.ats;\n\tif (res)\n\t\treturn res;\n\n\tres = key1.access_mode - key2.access_mode;\n\tif (res)\n\t\treturn res;\n\n\tres = key1.access_flags - key2.access_flags;\n\tif (res)\n\t\treturn res;\n\n\t \n\treturn key1.ndescs - key2.ndescs;\n}\n\nstatic int mlx5_cache_ent_insert(struct mlx5_mkey_cache *cache,\n\t\t\t\t struct mlx5_cache_ent *ent)\n{\n\tstruct rb_node **new = &cache->rb_root.rb_node, *parent = NULL;\n\tstruct mlx5_cache_ent *cur;\n\tint cmp;\n\n\t \n\twhile (*new) {\n\t\tcur = rb_entry(*new, struct mlx5_cache_ent, node);\n\t\tparent = *new;\n\t\tcmp = cache_ent_key_cmp(cur->rb_key, ent->rb_key);\n\t\tif (cmp > 0)\n\t\t\tnew = &((*new)->rb_left);\n\t\tif (cmp < 0)\n\t\t\tnew = &((*new)->rb_right);\n\t\tif (cmp == 0) {\n\t\t\tmutex_unlock(&cache->rb_lock);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\t \n\trb_link_node(&ent->node, parent, new);\n\trb_insert_color(&ent->node, &cache->rb_root);\n\n\treturn 0;\n}\n\nstatic struct mlx5_cache_ent *\nmkey_cache_ent_from_rb_key(struct mlx5_ib_dev *dev,\n\t\t\t   struct mlx5r_cache_rb_key rb_key)\n{\n\tstruct rb_node *node = dev->cache.rb_root.rb_node;\n\tstruct mlx5_cache_ent *cur, *smallest = NULL;\n\tint cmp;\n\n\t \n\twhile (node) {\n\t\tcur = rb_entry(node, struct mlx5_cache_ent, node);\n\t\tcmp = cache_ent_key_cmp(cur->rb_key, rb_key);\n\t\tif (cmp > 0) {\n\t\t\tsmallest = cur;\n\t\t\tnode = node->rb_left;\n\t\t}\n\t\tif (cmp < 0)\n\t\t\tnode = node->rb_right;\n\t\tif (cmp == 0)\n\t\t\treturn cur;\n\t}\n\n\treturn (smallest &&\n\t\tsmallest->rb_key.access_mode == rb_key.access_mode &&\n\t\tsmallest->rb_key.access_flags == rb_key.access_flags &&\n\t\tsmallest->rb_key.ats == rb_key.ats) ?\n\t\t       smallest :\n\t\t       NULL;\n}\n\nstatic struct mlx5_ib_mr *_mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev,\n\t\t\t\t\tstruct mlx5_cache_ent *ent,\n\t\t\t\t\tint access_flags)\n{\n\tstruct mlx5_ib_mr *mr;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\txa_lock_irq(&ent->mkeys);\n\tent->in_use++;\n\n\tif (!ent->stored) {\n\t\tqueue_adjust_cache_locked(ent);\n\t\tent->miss++;\n\t\txa_unlock_irq(&ent->mkeys);\n\t\terr = create_cache_mkey(ent, &mr->mmkey.key);\n\t\tif (err) {\n\t\t\txa_lock_irq(&ent->mkeys);\n\t\t\tent->in_use--;\n\t\t\txa_unlock_irq(&ent->mkeys);\n\t\t\tkfree(mr);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t} else {\n\t\tmr->mmkey.key = pop_stored_mkey(ent);\n\t\tqueue_adjust_cache_locked(ent);\n\t\txa_unlock_irq(&ent->mkeys);\n\t}\n\tmr->mmkey.cache_ent = ent;\n\tmr->mmkey.type = MLX5_MKEY_MR;\n\tinit_waitqueue_head(&mr->mmkey.wait);\n\treturn mr;\n}\n\nstatic int get_unchangeable_access_flags(struct mlx5_ib_dev *dev,\n\t\t\t\t\t int access_flags)\n{\n\tint ret = 0;\n\n\tif ((access_flags & IB_ACCESS_REMOTE_ATOMIC) &&\n\t    MLX5_CAP_GEN(dev->mdev, atomic) &&\n\t    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))\n\t\tret |= IB_ACCESS_REMOTE_ATOMIC;\n\n\tif ((access_flags & IB_ACCESS_RELAXED_ORDERING) &&\n\t    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write) &&\n\t    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))\n\t\tret |= IB_ACCESS_RELAXED_ORDERING;\n\n\tif ((access_flags & IB_ACCESS_RELAXED_ORDERING) &&\n\t    (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) ||\n\t     MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_pci_enabled)) &&\n\t    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))\n\t\tret |= IB_ACCESS_RELAXED_ORDERING;\n\n\treturn ret;\n}\n\nstruct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev,\n\t\t\t\t       int access_flags, int access_mode,\n\t\t\t\t       int ndescs)\n{\n\tstruct mlx5r_cache_rb_key rb_key = {\n\t\t.ndescs = ndescs,\n\t\t.access_mode = access_mode,\n\t\t.access_flags = get_unchangeable_access_flags(dev, access_flags)\n\t};\n\tstruct mlx5_cache_ent *ent = mkey_cache_ent_from_rb_key(dev, rb_key);\n\n\tif (!ent)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn _mlx5_mr_cache_alloc(dev, ent, access_flags);\n}\n\nstatic void clean_keys(struct mlx5_ib_dev *dev, struct mlx5_cache_ent *ent)\n{\n\tu32 mkey;\n\n\tcancel_delayed_work(&ent->dwork);\n\txa_lock_irq(&ent->mkeys);\n\twhile (ent->stored) {\n\t\tmkey = pop_stored_mkey(ent);\n\t\txa_unlock_irq(&ent->mkeys);\n\t\tmlx5_core_destroy_mkey(dev->mdev, mkey);\n\t\txa_lock_irq(&ent->mkeys);\n\t}\n\txa_unlock_irq(&ent->mkeys);\n}\n\nstatic void mlx5_mkey_cache_debugfs_cleanup(struct mlx5_ib_dev *dev)\n{\n\tif (!mlx5_debugfs_root || dev->is_rep)\n\t\treturn;\n\n\tdebugfs_remove_recursive(dev->cache.fs_root);\n\tdev->cache.fs_root = NULL;\n}\n\nstatic void mlx5_mkey_cache_debugfs_add_ent(struct mlx5_ib_dev *dev,\n\t\t\t\t\t    struct mlx5_cache_ent *ent)\n{\n\tint order = order_base_2(ent->rb_key.ndescs);\n\tstruct dentry *dir;\n\n\tif (!mlx5_debugfs_root || dev->is_rep)\n\t\treturn;\n\n\tif (ent->rb_key.access_mode == MLX5_MKC_ACCESS_MODE_KSM)\n\t\torder = MLX5_IMR_KSM_CACHE_ENTRY + 2;\n\n\tsprintf(ent->name, \"%d\", order);\n\tdir = debugfs_create_dir(ent->name, dev->cache.fs_root);\n\tdebugfs_create_file(\"size\", 0600, dir, ent, &size_fops);\n\tdebugfs_create_file(\"limit\", 0600, dir, ent, &limit_fops);\n\tdebugfs_create_ulong(\"cur\", 0400, dir, &ent->stored);\n\tdebugfs_create_u32(\"miss\", 0600, dir, &ent->miss);\n}\n\nstatic void mlx5_mkey_cache_debugfs_init(struct mlx5_ib_dev *dev)\n{\n\tstruct dentry *dbg_root = mlx5_debugfs_get_dev_root(dev->mdev);\n\tstruct mlx5_mkey_cache *cache = &dev->cache;\n\n\tif (!mlx5_debugfs_root || dev->is_rep)\n\t\treturn;\n\n\tcache->fs_root = debugfs_create_dir(\"mr_cache\", dbg_root);\n}\n\nstatic void delay_time_func(struct timer_list *t)\n{\n\tstruct mlx5_ib_dev *dev = from_timer(dev, t, delay_timer);\n\n\tWRITE_ONCE(dev->fill_delay, 0);\n}\n\nstruct mlx5_cache_ent *\nmlx5r_cache_create_ent_locked(struct mlx5_ib_dev *dev,\n\t\t\t      struct mlx5r_cache_rb_key rb_key,\n\t\t\t      bool persistent_entry)\n{\n\tstruct mlx5_cache_ent *ent;\n\tint order;\n\tint ret;\n\n\tent = kzalloc(sizeof(*ent), GFP_KERNEL);\n\tif (!ent)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\txa_init_flags(&ent->mkeys, XA_FLAGS_LOCK_IRQ);\n\tent->rb_key = rb_key;\n\tent->dev = dev;\n\tent->is_tmp = !persistent_entry;\n\n\tINIT_DELAYED_WORK(&ent->dwork, delayed_cache_work_func);\n\n\tret = mlx5_cache_ent_insert(&dev->cache, ent);\n\tif (ret) {\n\t\tkfree(ent);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (persistent_entry) {\n\t\tif (rb_key.access_mode == MLX5_MKC_ACCESS_MODE_KSM)\n\t\t\torder = MLX5_IMR_KSM_CACHE_ENTRY;\n\t\telse\n\t\t\torder = order_base_2(rb_key.ndescs) - 2;\n\n\t\tif ((dev->mdev->profile.mask & MLX5_PROF_MASK_MR_CACHE) &&\n\t\t    !dev->is_rep && mlx5_core_is_pf(dev->mdev) &&\n\t\t    mlx5r_umr_can_load_pas(dev, 0))\n\t\t\tent->limit = dev->mdev->profile.mr_cache[order].limit;\n\t\telse\n\t\t\tent->limit = 0;\n\n\t\tmlx5_mkey_cache_debugfs_add_ent(dev, ent);\n\t} else {\n\t\tmod_delayed_work(ent->dev->cache.wq,\n\t\t\t\t &ent->dev->cache.remove_ent_dwork,\n\t\t\t\t msecs_to_jiffies(30 * 1000));\n\t}\n\n\treturn ent;\n}\n\nstatic void remove_ent_work_func(struct work_struct *work)\n{\n\tstruct mlx5_mkey_cache *cache;\n\tstruct mlx5_cache_ent *ent;\n\tstruct rb_node *cur;\n\n\tcache = container_of(work, struct mlx5_mkey_cache,\n\t\t\t     remove_ent_dwork.work);\n\tmutex_lock(&cache->rb_lock);\n\tcur = rb_last(&cache->rb_root);\n\twhile (cur) {\n\t\tent = rb_entry(cur, struct mlx5_cache_ent, node);\n\t\tcur = rb_prev(cur);\n\t\tmutex_unlock(&cache->rb_lock);\n\n\t\txa_lock_irq(&ent->mkeys);\n\t\tif (!ent->is_tmp) {\n\t\t\txa_unlock_irq(&ent->mkeys);\n\t\t\tmutex_lock(&cache->rb_lock);\n\t\t\tcontinue;\n\t\t}\n\t\txa_unlock_irq(&ent->mkeys);\n\n\t\tclean_keys(ent->dev, ent);\n\t\tmutex_lock(&cache->rb_lock);\n\t}\n\tmutex_unlock(&cache->rb_lock);\n}\n\nint mlx5_mkey_cache_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_mkey_cache *cache = &dev->cache;\n\tstruct rb_root *root = &dev->cache.rb_root;\n\tstruct mlx5r_cache_rb_key rb_key = {\n\t\t.access_mode = MLX5_MKC_ACCESS_MODE_MTT,\n\t};\n\tstruct mlx5_cache_ent *ent;\n\tstruct rb_node *node;\n\tint ret;\n\tint i;\n\n\tmutex_init(&dev->slow_path_mutex);\n\tmutex_init(&dev->cache.rb_lock);\n\tdev->cache.rb_root = RB_ROOT;\n\tINIT_DELAYED_WORK(&dev->cache.remove_ent_dwork, remove_ent_work_func);\n\tcache->wq = alloc_ordered_workqueue(\"mkey_cache\", WQ_MEM_RECLAIM);\n\tif (!cache->wq) {\n\t\tmlx5_ib_warn(dev, \"failed to create work queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmlx5_cmd_init_async_ctx(dev->mdev, &dev->async_ctx);\n\ttimer_setup(&dev->delay_timer, delay_time_func, 0);\n\tmlx5_mkey_cache_debugfs_init(dev);\n\tmutex_lock(&cache->rb_lock);\n\tfor (i = 0; i <= mkey_cache_max_order(dev); i++) {\n\t\trb_key.ndescs = 1 << (i + 2);\n\t\tent = mlx5r_cache_create_ent_locked(dev, rb_key, true);\n\t\tif (IS_ERR(ent)) {\n\t\t\tret = PTR_ERR(ent);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tret = mlx5_odp_init_mkey_cache(dev);\n\tif (ret)\n\t\tgoto err;\n\n\tmutex_unlock(&cache->rb_lock);\n\tfor (node = rb_first(root); node; node = rb_next(node)) {\n\t\tent = rb_entry(node, struct mlx5_cache_ent, node);\n\t\txa_lock_irq(&ent->mkeys);\n\t\tqueue_adjust_cache_locked(ent);\n\t\txa_unlock_irq(&ent->mkeys);\n\t}\n\n\treturn 0;\n\nerr:\n\tmutex_unlock(&cache->rb_lock);\n\tmlx5_mkey_cache_debugfs_cleanup(dev);\n\tmlx5_ib_warn(dev, \"failed to create mkey cache entry\\n\");\n\treturn ret;\n}\n\nvoid mlx5_mkey_cache_cleanup(struct mlx5_ib_dev *dev)\n{\n\tstruct rb_root *root = &dev->cache.rb_root;\n\tstruct mlx5_cache_ent *ent;\n\tstruct rb_node *node;\n\n\tif (!dev->cache.wq)\n\t\treturn;\n\n\tmutex_lock(&dev->cache.rb_lock);\n\tcancel_delayed_work(&dev->cache.remove_ent_dwork);\n\tfor (node = rb_first(root); node; node = rb_next(node)) {\n\t\tent = rb_entry(node, struct mlx5_cache_ent, node);\n\t\txa_lock_irq(&ent->mkeys);\n\t\tent->disabled = true;\n\t\txa_unlock_irq(&ent->mkeys);\n\t\tcancel_delayed_work(&ent->dwork);\n\t}\n\tmutex_unlock(&dev->cache.rb_lock);\n\n\t \n\tflush_workqueue(dev->cache.wq);\n\n\tmlx5_mkey_cache_debugfs_cleanup(dev);\n\tmlx5_cmd_cleanup_async_ctx(&dev->async_ctx);\n\n\t \n\tmutex_lock(&dev->cache.rb_lock);\n\tnode = rb_first(root);\n\twhile (node) {\n\t\tent = rb_entry(node, struct mlx5_cache_ent, node);\n\t\tnode = rb_next(node);\n\t\tclean_keys(dev, ent);\n\t\trb_erase(&ent->node, root);\n\t\tkfree(ent);\n\t}\n\tmutex_unlock(&dev->cache.rb_lock);\n\n\tdestroy_workqueue(dev->cache.wq);\n\tdel_timer_sync(&dev->delay_timer);\n}\n\nstruct ib_mr *mlx5_ib_get_dma_mr(struct ib_pd *pd, int acc)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tstruct mlx5_ib_mr *mr;\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_PA);\n\tMLX5_SET(mkc, mkc, length64, 1);\n\tset_mkc_access_pd_addr_fields(mkc, acc | IB_ACCESS_RELAXED_ORDERING, 0,\n\t\t\t\t      pd);\n\n\terr = mlx5_ib_create_mkey(dev, &mr->mmkey, in, inlen);\n\tif (err)\n\t\tgoto err_in;\n\n\tkfree(in);\n\tmr->mmkey.type = MLX5_MKEY_MR;\n\tmr->ibmr.lkey = mr->mmkey.key;\n\tmr->ibmr.rkey = mr->mmkey.key;\n\tmr->umem = NULL;\n\n\treturn &mr->ibmr;\n\nerr_in:\n\tkfree(in);\n\nerr_free:\n\tkfree(mr);\n\n\treturn ERR_PTR(err);\n}\n\nstatic int get_octo_len(u64 addr, u64 len, int page_shift)\n{\n\tu64 page_size = 1ULL << page_shift;\n\tu64 offset;\n\tint npages;\n\n\toffset = addr & (page_size - 1);\n\tnpages = ALIGN(len + offset, page_size) >> page_shift;\n\treturn (npages + 1) / 2;\n}\n\nstatic int mkey_cache_max_order(struct mlx5_ib_dev *dev)\n{\n\tif (MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))\n\t\treturn MKEY_CACHE_LAST_STD_ENTRY;\n\treturn MLX5_MAX_UMR_SHIFT;\n}\n\nstatic void set_mr_fields(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,\n\t\t\t  u64 length, int access_flags, u64 iova)\n{\n\tmr->ibmr.lkey = mr->mmkey.key;\n\tmr->ibmr.rkey = mr->mmkey.key;\n\tmr->ibmr.length = length;\n\tmr->ibmr.device = &dev->ib_dev;\n\tmr->ibmr.iova = iova;\n\tmr->access_flags = access_flags;\n}\n\nstatic unsigned int mlx5_umem_dmabuf_default_pgsz(struct ib_umem *umem,\n\t\t\t\t\t\t  u64 iova)\n{\n\t \n\tumem->iova = iova;\n\treturn PAGE_SIZE;\n}\n\nstatic struct mlx5_ib_mr *alloc_cacheable_mr(struct ib_pd *pd,\n\t\t\t\t\t     struct ib_umem *umem, u64 iova,\n\t\t\t\t\t     int access_flags)\n{\n\tstruct mlx5r_cache_rb_key rb_key = {\n\t\t.access_mode = MLX5_MKC_ACCESS_MODE_MTT,\n\t};\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx5_cache_ent *ent;\n\tstruct mlx5_ib_mr *mr;\n\tunsigned int page_size;\n\n\tif (umem->is_dmabuf)\n\t\tpage_size = mlx5_umem_dmabuf_default_pgsz(umem, iova);\n\telse\n\t\tpage_size = mlx5_umem_find_best_pgsz(umem, mkc, log_page_size,\n\t\t\t\t\t\t     0, iova);\n\tif (WARN_ON(!page_size))\n\t\treturn ERR_PTR(-EINVAL);\n\n\trb_key.ndescs = ib_umem_num_dma_blocks(umem, page_size);\n\trb_key.ats = mlx5_umem_needs_ats(dev, umem, access_flags);\n\trb_key.access_flags = get_unchangeable_access_flags(dev, access_flags);\n\tent = mkey_cache_ent_from_rb_key(dev, rb_key);\n\t \n\tif (!ent) {\n\t\tmutex_lock(&dev->slow_path_mutex);\n\t\tmr = reg_create(pd, umem, iova, access_flags, page_size, false);\n\t\tmutex_unlock(&dev->slow_path_mutex);\n\t\tif (IS_ERR(mr))\n\t\t\treturn mr;\n\t\tmr->mmkey.rb_key = rb_key;\n\t\treturn mr;\n\t}\n\n\tmr = _mlx5_mr_cache_alloc(dev, ent, access_flags);\n\tif (IS_ERR(mr))\n\t\treturn mr;\n\n\tmr->ibmr.pd = pd;\n\tmr->umem = umem;\n\tmr->page_shift = order_base_2(page_size);\n\tset_mr_fields(dev, mr, umem->length, access_flags, iova);\n\n\treturn mr;\n}\n\n \nstatic struct mlx5_ib_mr *reg_create(struct ib_pd *pd, struct ib_umem *umem,\n\t\t\t\t     u64 iova, int access_flags,\n\t\t\t\t     unsigned int page_size, bool populate)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx5_ib_mr *mr;\n\t__be64 *pas;\n\tvoid *mkc;\n\tint inlen;\n\tu32 *in;\n\tint err;\n\tbool pg_cap = !!(MLX5_CAP_GEN(dev->mdev, pg));\n\n\tif (!page_size)\n\t\treturn ERR_PTR(-EINVAL);\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->ibmr.pd = pd;\n\tmr->access_flags = access_flags;\n\tmr->page_shift = order_base_2(page_size);\n\n\tinlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tif (populate)\n\t\tinlen += sizeof(*pas) *\n\t\t\t roundup(ib_umem_num_dma_blocks(umem, page_size), 2);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_1;\n\t}\n\tpas = (__be64 *)MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\tif (populate) {\n\t\tif (WARN_ON(access_flags & IB_ACCESS_ON_DEMAND)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_2;\n\t\t}\n\t\tmlx5_ib_populate_pas(umem, 1UL << mr->page_shift, pas,\n\t\t\t\t     pg_cap ? MLX5_IB_MTT_PRESENT : 0);\n\t}\n\n\t \n\tMLX5_SET(create_mkey_in, in, pg_access, !!(pg_cap));\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tset_mkc_access_pd_addr_fields(mkc, access_flags, iova,\n\t\t\t\t      populate ? pd : dev->umrc.pd);\n\tMLX5_SET(mkc, mkc, free, !populate);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_MTT);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\n\tMLX5_SET64(mkc, mkc, len, umem->length);\n\tMLX5_SET(mkc, mkc, bsf_octword_size, 0);\n\tMLX5_SET(mkc, mkc, translations_octword_size,\n\t\t get_octo_len(iova, umem->length, mr->page_shift));\n\tMLX5_SET(mkc, mkc, log_page_size, mr->page_shift);\n\tif (mlx5_umem_needs_ats(dev, umem, access_flags))\n\t\tMLX5_SET(mkc, mkc, ma_translation_mode, 1);\n\tif (populate) {\n\t\tMLX5_SET(create_mkey_in, in, translations_octword_actual_size,\n\t\t\t get_octo_len(iova, umem->length, mr->page_shift));\n\t}\n\n\terr = mlx5_ib_create_mkey(dev, &mr->mmkey, in, inlen);\n\tif (err) {\n\t\tmlx5_ib_warn(dev, \"create mkey failed\\n\");\n\t\tgoto err_2;\n\t}\n\tmr->mmkey.type = MLX5_MKEY_MR;\n\tmr->mmkey.ndescs = get_octo_len(iova, umem->length, mr->page_shift);\n\tmr->umem = umem;\n\tset_mr_fields(dev, mr, umem->length, access_flags, iova);\n\tkvfree(in);\n\n\tmlx5_ib_dbg(dev, \"mkey = 0x%x\\n\", mr->mmkey.key);\n\n\treturn mr;\n\nerr_2:\n\tkvfree(in);\nerr_1:\n\tkfree(mr);\n\treturn ERR_PTR(err);\n}\n\nstatic struct ib_mr *mlx5_ib_get_dm_mr(struct ib_pd *pd, u64 start_addr,\n\t\t\t\t       u64 length, int acc, int mode)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tstruct mlx5_ib_mr *mr;\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, access_mode_1_0, mode & 0x3);\n\tMLX5_SET(mkc, mkc, access_mode_4_2, (mode >> 2) & 0x7);\n\tMLX5_SET64(mkc, mkc, len, length);\n\tset_mkc_access_pd_addr_fields(mkc, acc, start_addr, pd);\n\n\terr = mlx5_ib_create_mkey(dev, &mr->mmkey, in, inlen);\n\tif (err)\n\t\tgoto err_in;\n\n\tkfree(in);\n\n\tset_mr_fields(dev, mr, length, acc, start_addr);\n\n\treturn &mr->ibmr;\n\nerr_in:\n\tkfree(in);\n\nerr_free:\n\tkfree(mr);\n\n\treturn ERR_PTR(err);\n}\n\nint mlx5_ib_advise_mr(struct ib_pd *pd,\n\t\t      enum ib_uverbs_advise_mr_advice advice,\n\t\t      u32 flags,\n\t\t      struct ib_sge *sg_list,\n\t\t      u32 num_sge,\n\t\t      struct uverbs_attr_bundle *attrs)\n{\n\tif (advice != IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH &&\n\t    advice != IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_WRITE &&\n\t    advice != IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_NO_FAULT)\n\t\treturn -EOPNOTSUPP;\n\n\treturn mlx5_ib_advise_mr_prefetch(pd, advice, flags,\n\t\t\t\t\t sg_list, num_sge);\n}\n\nstruct ib_mr *mlx5_ib_reg_dm_mr(struct ib_pd *pd, struct ib_dm *dm,\n\t\t\t\tstruct ib_dm_mr_attr *attr,\n\t\t\t\tstruct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_ib_dm *mdm = to_mdm(dm);\n\tstruct mlx5_core_dev *dev = to_mdev(dm->device)->mdev;\n\tu64 start_addr = mdm->dev_addr + attr->offset;\n\tint mode;\n\n\tswitch (mdm->type) {\n\tcase MLX5_IB_UAPI_DM_TYPE_MEMIC:\n\t\tif (attr->access_flags & ~MLX5_IB_DM_MEMIC_ALLOWED_ACCESS)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tmode = MLX5_MKC_ACCESS_MODE_MEMIC;\n\t\tstart_addr -= pci_resource_start(dev->pdev, 0);\n\t\tbreak;\n\tcase MLX5_IB_UAPI_DM_TYPE_STEERING_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_PATTERN_SW_ICM:\n\t\tif (attr->access_flags & ~MLX5_IB_DM_SW_ICM_ALLOWED_ACCESS)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tmode = MLX5_MKC_ACCESS_MODE_SW_ICM;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn mlx5_ib_get_dm_mr(pd, start_addr, attr->length,\n\t\t\t\t attr->access_flags, mode);\n}\n\nstatic struct ib_mr *create_real_mr(struct ib_pd *pd, struct ib_umem *umem,\n\t\t\t\t    u64 iova, int access_flags)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx5_ib_mr *mr = NULL;\n\tbool xlt_with_umr;\n\tint err;\n\n\txlt_with_umr = mlx5r_umr_can_load_pas(dev, umem->length);\n\tif (xlt_with_umr) {\n\t\tmr = alloc_cacheable_mr(pd, umem, iova, access_flags);\n\t} else {\n\t\tunsigned int page_size = mlx5_umem_find_best_pgsz(\n\t\t\tumem, mkc, log_page_size, 0, iova);\n\n\t\tmutex_lock(&dev->slow_path_mutex);\n\t\tmr = reg_create(pd, umem, iova, access_flags, page_size, true);\n\t\tmutex_unlock(&dev->slow_path_mutex);\n\t}\n\tif (IS_ERR(mr)) {\n\t\tib_umem_release(umem);\n\t\treturn ERR_CAST(mr);\n\t}\n\n\tmlx5_ib_dbg(dev, \"mkey 0x%x\\n\", mr->mmkey.key);\n\n\tatomic_add(ib_umem_num_pages(umem), &dev->mdev->priv.reg_pages);\n\n\tif (xlt_with_umr) {\n\t\t \n\t\terr = mlx5r_umr_update_mr_pas(mr, MLX5_IB_UPD_XLT_ENABLE);\n\t\tif (err) {\n\t\t\tmlx5_ib_dereg_mr(&mr->ibmr, NULL);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\treturn &mr->ibmr;\n}\n\nstatic struct ib_mr *create_user_odp_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t\t\tu64 iova, int access_flags,\n\t\t\t\t\tstruct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct ib_umem_odp *odp;\n\tstruct mlx5_ib_mr *mr;\n\tint err;\n\n\tif (!IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\terr = mlx5r_odp_create_eq(dev, &dev->odp_pf_eq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (!start && length == U64_MAX) {\n\t\tif (iova != 0)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tif (!(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tmr = mlx5_ib_alloc_implicit_mr(to_mpd(pd), access_flags);\n\t\tif (IS_ERR(mr))\n\t\t\treturn ERR_CAST(mr);\n\t\treturn &mr->ibmr;\n\t}\n\n\t \n\tif (!mlx5r_umr_can_load_pas(dev, length))\n\t\treturn ERR_PTR(-EINVAL);\n\n\todp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,\n\t\t\t      &mlx5_mn_ops);\n\tif (IS_ERR(odp))\n\t\treturn ERR_CAST(odp);\n\n\tmr = alloc_cacheable_mr(pd, &odp->umem, iova, access_flags);\n\tif (IS_ERR(mr)) {\n\t\tib_umem_release(&odp->umem);\n\t\treturn ERR_CAST(mr);\n\t}\n\txa_init(&mr->implicit_children);\n\n\todp->private = mr;\n\terr = mlx5r_store_odp_mkey(dev, &mr->mmkey);\n\tif (err)\n\t\tgoto err_dereg_mr;\n\n\terr = mlx5_ib_init_odp_mr(mr);\n\tif (err)\n\t\tgoto err_dereg_mr;\n\treturn &mr->ibmr;\n\nerr_dereg_mr:\n\tmlx5_ib_dereg_mr(&mr->ibmr, NULL);\n\treturn ERR_PTR(err);\n}\n\nstruct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t\t  u64 iova, int access_flags,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct ib_umem *umem;\n\n\tif (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tmlx5_ib_dbg(dev, \"start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\\n\",\n\t\t    start, iova, length, access_flags);\n\n\tif (access_flags & IB_ACCESS_ON_DEMAND)\n\t\treturn create_user_odp_mr(pd, start, length, iova, access_flags,\n\t\t\t\t\t  udata);\n\tumem = ib_umem_get(&dev->ib_dev, start, length, access_flags);\n\tif (IS_ERR(umem))\n\t\treturn ERR_CAST(umem);\n\treturn create_real_mr(pd, umem, iova, access_flags);\n}\n\nstatic void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)\n{\n\tstruct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;\n\tstruct mlx5_ib_mr *mr = umem_dmabuf->private;\n\n\tdma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);\n\n\tif (!umem_dmabuf->sgt)\n\t\treturn;\n\n\tmlx5r_umr_update_mr_pas(mr, MLX5_IB_UPD_XLT_ZAP);\n\tib_umem_dmabuf_unmap_pages(umem_dmabuf);\n}\n\nstatic struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {\n\t.allow_peer2peer = 1,\n\t.move_notify = mlx5_ib_dmabuf_invalidate_cb,\n};\n\nstruct ib_mr *mlx5_ib_reg_user_mr_dmabuf(struct ib_pd *pd, u64 offset,\n\t\t\t\t\t u64 length, u64 virt_addr,\n\t\t\t\t\t int fd, int access_flags,\n\t\t\t\t\t struct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tstruct mlx5_ib_mr *mr = NULL;\n\tstruct ib_umem_dmabuf *umem_dmabuf;\n\tint err;\n\n\tif (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM) ||\n\t    !IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tmlx5_ib_dbg(dev,\n\t\t    \"offset 0x%llx, virt_addr 0x%llx, length 0x%llx, fd %d, access_flags 0x%x\\n\",\n\t\t    offset, virt_addr, length, fd, access_flags);\n\n\t \n\tif (!mlx5r_umr_can_load_pas(dev, length))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tumem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd,\n\t\t\t\t\t access_flags,\n\t\t\t\t\t &mlx5_ib_dmabuf_attach_ops);\n\tif (IS_ERR(umem_dmabuf)) {\n\t\tmlx5_ib_dbg(dev, \"umem_dmabuf get failed (%ld)\\n\",\n\t\t\t    PTR_ERR(umem_dmabuf));\n\t\treturn ERR_CAST(umem_dmabuf);\n\t}\n\n\tmr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr,\n\t\t\t\taccess_flags);\n\tif (IS_ERR(mr)) {\n\t\tib_umem_release(&umem_dmabuf->umem);\n\t\treturn ERR_CAST(mr);\n\t}\n\n\tmlx5_ib_dbg(dev, \"mkey 0x%x\\n\", mr->mmkey.key);\n\n\tatomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);\n\tumem_dmabuf->private = mr;\n\terr = mlx5r_store_odp_mkey(dev, &mr->mmkey);\n\tif (err)\n\t\tgoto err_dereg_mr;\n\n\terr = mlx5_ib_init_dmabuf_mr(mr);\n\tif (err)\n\t\tgoto err_dereg_mr;\n\treturn &mr->ibmr;\n\nerr_dereg_mr:\n\tmlx5_ib_dereg_mr(&mr->ibmr, NULL);\n\treturn ERR_PTR(err);\n}\n\n \nstatic bool can_use_umr_rereg_access(struct mlx5_ib_dev *dev,\n\t\t\t\t     unsigned int current_access_flags,\n\t\t\t\t     unsigned int target_access_flags)\n{\n\tunsigned int diffs = current_access_flags ^ target_access_flags;\n\n\tif (diffs & ~(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |\n\t\t      IB_ACCESS_REMOTE_READ | IB_ACCESS_RELAXED_ORDERING))\n\t\treturn false;\n\treturn mlx5r_umr_can_reconfig(dev, current_access_flags,\n\t\t\t\t      target_access_flags);\n}\n\nstatic bool can_use_umr_rereg_pas(struct mlx5_ib_mr *mr,\n\t\t\t\t  struct ib_umem *new_umem,\n\t\t\t\t  int new_access_flags, u64 iova,\n\t\t\t\t  unsigned long *page_size)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n\n\t \n\tif (!mr->mmkey.cache_ent)\n\t\treturn false;\n\tif (!mlx5r_umr_can_load_pas(dev, new_umem->length))\n\t\treturn false;\n\n\t*page_size =\n\t\tmlx5_umem_find_best_pgsz(new_umem, mkc, log_page_size, 0, iova);\n\tif (WARN_ON(!*page_size))\n\t\treturn false;\n\treturn (mr->mmkey.cache_ent->rb_key.ndescs) >=\n\t       ib_umem_num_dma_blocks(new_umem, *page_size);\n}\n\nstatic int umr_rereg_pas(struct mlx5_ib_mr *mr, struct ib_pd *pd,\n\t\t\t int access_flags, int flags, struct ib_umem *new_umem,\n\t\t\t u64 iova, unsigned long page_size)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);\n\tint upd_flags = MLX5_IB_UPD_XLT_ADDR | MLX5_IB_UPD_XLT_ENABLE;\n\tstruct ib_umem *old_umem = mr->umem;\n\tint err;\n\n\t \n\terr = mlx5r_umr_revoke_mr(mr);\n\tif (err)\n\t\treturn err;\n\n\tif (flags & IB_MR_REREG_PD) {\n\t\tmr->ibmr.pd = pd;\n\t\tupd_flags |= MLX5_IB_UPD_XLT_PD;\n\t}\n\tif (flags & IB_MR_REREG_ACCESS) {\n\t\tmr->access_flags = access_flags;\n\t\tupd_flags |= MLX5_IB_UPD_XLT_ACCESS;\n\t}\n\n\tmr->ibmr.iova = iova;\n\tmr->ibmr.length = new_umem->length;\n\tmr->page_shift = order_base_2(page_size);\n\tmr->umem = new_umem;\n\terr = mlx5r_umr_update_mr_pas(mr, upd_flags);\n\tif (err) {\n\t\t \n\t\tmr->umem = old_umem;\n\t\treturn err;\n\t}\n\n\tatomic_sub(ib_umem_num_pages(old_umem), &dev->mdev->priv.reg_pages);\n\tib_umem_release(old_umem);\n\tatomic_add(ib_umem_num_pages(new_umem), &dev->mdev->priv.reg_pages);\n\treturn 0;\n}\n\nstruct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,\n\t\t\t\t    u64 length, u64 iova, int new_access_flags,\n\t\t\t\t    struct ib_pd *new_pd,\n\t\t\t\t    struct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ib_mr->device);\n\tstruct mlx5_ib_mr *mr = to_mmr(ib_mr);\n\tint err;\n\n\tif (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tmlx5_ib_dbg(\n\t\tdev,\n\t\t\"start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\\n\",\n\t\tstart, iova, length, new_access_flags);\n\n\tif (flags & ~(IB_MR_REREG_TRANS | IB_MR_REREG_PD | IB_MR_REREG_ACCESS))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif (!(flags & IB_MR_REREG_ACCESS))\n\t\tnew_access_flags = mr->access_flags;\n\tif (!(flags & IB_MR_REREG_PD))\n\t\tnew_pd = ib_mr->pd;\n\n\tif (!(flags & IB_MR_REREG_TRANS)) {\n\t\tstruct ib_umem *umem;\n\n\t\t \n\t\tif (can_use_umr_rereg_access(dev, mr->access_flags,\n\t\t\t\t\t     new_access_flags)) {\n\t\t\terr = mlx5r_umr_rereg_pd_access(mr, new_pd,\n\t\t\t\t\t\t\tnew_access_flags);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\t\t\treturn NULL;\n\t\t}\n\t\t \n\t\tif (!mr->umem || is_odp_mr(mr) || is_dmabuf_mr(mr))\n\t\t\tgoto recreate;\n\n\t\t \n\t\terr = mlx5r_umr_revoke_mr(mr);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tumem = mr->umem;\n\t\tmr->umem = NULL;\n\t\tatomic_sub(ib_umem_num_pages(umem), &dev->mdev->priv.reg_pages);\n\n\t\treturn create_real_mr(new_pd, umem, mr->ibmr.iova,\n\t\t\t\t      new_access_flags);\n\t}\n\n\t \n\tif (!mr->umem || is_odp_mr(mr) || is_dmabuf_mr(mr))\n\t\tgoto recreate;\n\n\tif (!(new_access_flags & IB_ACCESS_ON_DEMAND) &&\n\t    can_use_umr_rereg_access(dev, mr->access_flags, new_access_flags)) {\n\t\tstruct ib_umem *new_umem;\n\t\tunsigned long page_size;\n\n\t\tnew_umem = ib_umem_get(&dev->ib_dev, start, length,\n\t\t\t\t       new_access_flags);\n\t\tif (IS_ERR(new_umem))\n\t\t\treturn ERR_CAST(new_umem);\n\n\t\t \n\t\tif (can_use_umr_rereg_pas(mr, new_umem, new_access_flags, iova,\n\t\t\t\t\t  &page_size)) {\n\t\t\terr = umr_rereg_pas(mr, new_pd, new_access_flags, flags,\n\t\t\t\t\t    new_umem, iova, page_size);\n\t\t\tif (err) {\n\t\t\t\tib_umem_release(new_umem);\n\t\t\t\treturn ERR_PTR(err);\n\t\t\t}\n\t\t\treturn NULL;\n\t\t}\n\t\treturn create_real_mr(new_pd, new_umem, iova, new_access_flags);\n\t}\n\n\t \nrecreate:\n\treturn mlx5_ib_reg_user_mr(new_pd, start, length, iova,\n\t\t\t\t   new_access_flags, udata);\n}\n\nstatic int\nmlx5_alloc_priv_descs(struct ib_device *device,\n\t\t      struct mlx5_ib_mr *mr,\n\t\t      int ndescs,\n\t\t      int desc_size)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(device);\n\tstruct device *ddev = &dev->mdev->pdev->dev;\n\tint size = ndescs * desc_size;\n\tint add_size;\n\tint ret;\n\n\tadd_size = max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);\n\tif (is_power_of_2(MLX5_UMR_ALIGN) && add_size) {\n\t\tint end = max_t(int, MLX5_UMR_ALIGN, roundup_pow_of_two(size));\n\n\t\tadd_size = min_t(int, end - size, add_size);\n\t}\n\n\tmr->descs_alloc = kzalloc(size + add_size, GFP_KERNEL);\n\tif (!mr->descs_alloc)\n\t\treturn -ENOMEM;\n\n\tmr->descs = PTR_ALIGN(mr->descs_alloc, MLX5_UMR_ALIGN);\n\n\tmr->desc_map = dma_map_single(ddev, mr->descs, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(ddev, mr->desc_map)) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mr->descs_alloc);\n\n\treturn ret;\n}\n\nstatic void\nmlx5_free_priv_descs(struct mlx5_ib_mr *mr)\n{\n\tif (!mr->umem && mr->descs) {\n\t\tstruct ib_device *device = mr->ibmr.device;\n\t\tint size = mr->max_descs * mr->desc_size;\n\t\tstruct mlx5_ib_dev *dev = to_mdev(device);\n\n\t\tdma_unmap_single(&dev->mdev->pdev->dev, mr->desc_map, size,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tkfree(mr->descs_alloc);\n\t\tmr->descs = NULL;\n\t}\n}\n\nstatic int cache_ent_find_and_store(struct mlx5_ib_dev *dev,\n\t\t\t\t    struct mlx5_ib_mr *mr)\n{\n\tstruct mlx5_mkey_cache *cache = &dev->cache;\n\tstruct mlx5_cache_ent *ent;\n\tint ret;\n\n\tif (mr->mmkey.cache_ent) {\n\t\txa_lock_irq(&mr->mmkey.cache_ent->mkeys);\n\t\tmr->mmkey.cache_ent->in_use--;\n\t\tgoto end;\n\t}\n\n\tmutex_lock(&cache->rb_lock);\n\tent = mkey_cache_ent_from_rb_key(dev, mr->mmkey.rb_key);\n\tif (ent) {\n\t\tif (ent->rb_key.ndescs == mr->mmkey.rb_key.ndescs) {\n\t\t\tif (ent->disabled) {\n\t\t\t\tmutex_unlock(&cache->rb_lock);\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\t}\n\t\t\tmr->mmkey.cache_ent = ent;\n\t\t\txa_lock_irq(&mr->mmkey.cache_ent->mkeys);\n\t\t\tmutex_unlock(&cache->rb_lock);\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\tent = mlx5r_cache_create_ent_locked(dev, mr->mmkey.rb_key, false);\n\tmutex_unlock(&cache->rb_lock);\n\tif (IS_ERR(ent))\n\t\treturn PTR_ERR(ent);\n\n\tmr->mmkey.cache_ent = ent;\n\txa_lock_irq(&mr->mmkey.cache_ent->mkeys);\n\nend:\n\tret = push_mkey_locked(mr->mmkey.cache_ent, false,\n\t\t\t       xa_mk_value(mr->mmkey.key));\n\txa_unlock_irq(&mr->mmkey.cache_ent->mkeys);\n\treturn ret;\n}\n\nint mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tstruct mlx5_ib_dev *dev = to_mdev(ibmr->device);\n\tint rc;\n\n\t \n\tif (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) &&\n\t    refcount_read(&mr->mmkey.usecount) != 0 &&\n\t    xa_erase(&mr_to_mdev(mr)->odp_mkeys, mlx5_base_mkey(mr->mmkey.key)))\n\t\tmlx5r_deref_wait_odp_mkey(&mr->mmkey);\n\n\tif (ibmr->type == IB_MR_TYPE_INTEGRITY) {\n\t\txa_cmpxchg(&dev->sig_mrs, mlx5_base_mkey(mr->mmkey.key),\n\t\t\t   mr->sig, NULL, GFP_KERNEL);\n\n\t\tif (mr->mtt_mr) {\n\t\t\trc = mlx5_ib_dereg_mr(&mr->mtt_mr->ibmr, NULL);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tmr->mtt_mr = NULL;\n\t\t}\n\t\tif (mr->klm_mr) {\n\t\t\trc = mlx5_ib_dereg_mr(&mr->klm_mr->ibmr, NULL);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tmr->klm_mr = NULL;\n\t\t}\n\n\t\tif (mlx5_core_destroy_psv(dev->mdev,\n\t\t\t\t\t  mr->sig->psv_memory.psv_idx))\n\t\t\tmlx5_ib_warn(dev, \"failed to destroy mem psv %d\\n\",\n\t\t\t\t     mr->sig->psv_memory.psv_idx);\n\t\tif (mlx5_core_destroy_psv(dev->mdev, mr->sig->psv_wire.psv_idx))\n\t\t\tmlx5_ib_warn(dev, \"failed to destroy wire psv %d\\n\",\n\t\t\t\t     mr->sig->psv_wire.psv_idx);\n\t\tkfree(mr->sig);\n\t\tmr->sig = NULL;\n\t}\n\n\t \n\tif (mr->umem && mlx5r_umr_can_load_pas(dev, mr->umem->length))\n\t\tif (mlx5r_umr_revoke_mr(mr) ||\n\t\t    cache_ent_find_and_store(dev, mr))\n\t\t\tmr->mmkey.cache_ent = NULL;\n\n\tif (!mr->mmkey.cache_ent) {\n\t\trc = destroy_mkey(to_mdev(mr->ibmr.device), mr);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (mr->umem) {\n\t\tbool is_odp = is_odp_mr(mr);\n\n\t\tif (!is_odp)\n\t\t\tatomic_sub(ib_umem_num_pages(mr->umem),\n\t\t\t\t   &dev->mdev->priv.reg_pages);\n\t\tib_umem_release(mr->umem);\n\t\tif (is_odp)\n\t\t\tmlx5_ib_free_odp_mr(mr);\n\t}\n\n\tif (!mr->mmkey.cache_ent)\n\t\tmlx5_free_priv_descs(mr);\n\n\tkfree(mr);\n\treturn 0;\n}\n\nstatic void mlx5_set_umr_free_mkey(struct ib_pd *pd, u32 *in, int ndescs,\n\t\t\t\t   int access_mode, int page_shift)\n{\n\tvoid *mkc;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\t \n\tset_mkc_access_pd_addr_fields(mkc, IB_ACCESS_RELAXED_ORDERING, 0, pd);\n\tMLX5_SET(mkc, mkc, free, 1);\n\tMLX5_SET(mkc, mkc, translations_octword_size, ndescs);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, access_mode & 0x3);\n\tMLX5_SET(mkc, mkc, access_mode_4_2, (access_mode >> 2) & 0x7);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\tMLX5_SET(mkc, mkc, log_page_size, page_shift);\n}\n\nstatic int _mlx5_alloc_mkey_descs(struct ib_pd *pd, struct mlx5_ib_mr *mr,\n\t\t\t\t  int ndescs, int desc_size, int page_shift,\n\t\t\t\t  int access_mode, u32 *in, int inlen)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tint err;\n\n\tmr->access_mode = access_mode;\n\tmr->desc_size = desc_size;\n\tmr->max_descs = ndescs;\n\n\terr = mlx5_alloc_priv_descs(pd->device, mr, ndescs, desc_size);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_set_umr_free_mkey(pd, in, ndescs, access_mode, page_shift);\n\n\terr = mlx5_ib_create_mkey(dev, &mr->mmkey, in, inlen);\n\tif (err)\n\t\tgoto err_free_descs;\n\n\tmr->mmkey.type = MLX5_MKEY_MR;\n\tmr->ibmr.lkey = mr->mmkey.key;\n\tmr->ibmr.rkey = mr->mmkey.key;\n\n\treturn 0;\n\nerr_free_descs:\n\tmlx5_free_priv_descs(mr);\n\treturn err;\n}\n\nstatic struct mlx5_ib_mr *mlx5_ib_alloc_pi_mr(struct ib_pd *pd,\n\t\t\t\tu32 max_num_sg, u32 max_num_meta_sg,\n\t\t\t\tint desc_size, int access_mode)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tint ndescs = ALIGN(max_num_sg + max_num_meta_sg, 4);\n\tint page_shift = 0;\n\tstruct mlx5_ib_mr *mr;\n\tu32 *in;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->ibmr.pd = pd;\n\tmr->ibmr.device = pd->device;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tif (access_mode == MLX5_MKC_ACCESS_MODE_MTT)\n\t\tpage_shift = PAGE_SHIFT;\n\n\terr = _mlx5_alloc_mkey_descs(pd, mr, ndescs, desc_size, page_shift,\n\t\t\t\t     access_mode, in, inlen);\n\tif (err)\n\t\tgoto err_free_in;\n\n\tmr->umem = NULL;\n\tkfree(in);\n\n\treturn mr;\n\nerr_free_in:\n\tkfree(in);\nerr_free:\n\tkfree(mr);\n\treturn ERR_PTR(err);\n}\n\nstatic int mlx5_alloc_mem_reg_descs(struct ib_pd *pd, struct mlx5_ib_mr *mr,\n\t\t\t\t    int ndescs, u32 *in, int inlen)\n{\n\treturn _mlx5_alloc_mkey_descs(pd, mr, ndescs, sizeof(struct mlx5_mtt),\n\t\t\t\t      PAGE_SHIFT, MLX5_MKC_ACCESS_MODE_MTT, in,\n\t\t\t\t      inlen);\n}\n\nstatic int mlx5_alloc_sg_gaps_descs(struct ib_pd *pd, struct mlx5_ib_mr *mr,\n\t\t\t\t    int ndescs, u32 *in, int inlen)\n{\n\treturn _mlx5_alloc_mkey_descs(pd, mr, ndescs, sizeof(struct mlx5_klm),\n\t\t\t\t      0, MLX5_MKC_ACCESS_MODE_KLMS, in, inlen);\n}\n\nstatic int mlx5_alloc_integrity_descs(struct ib_pd *pd, struct mlx5_ib_mr *mr,\n\t\t\t\t      int max_num_sg, int max_num_meta_sg,\n\t\t\t\t      u32 *in, int inlen)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tu32 psv_index[2];\n\tvoid *mkc;\n\tint err;\n\n\tmr->sig = kzalloc(sizeof(*mr->sig), GFP_KERNEL);\n\tif (!mr->sig)\n\t\treturn -ENOMEM;\n\n\t \n\terr = mlx5_core_create_psv(dev->mdev, to_mpd(pd)->pdn, 2, psv_index);\n\tif (err)\n\t\tgoto err_free_sig;\n\n\tmr->sig->psv_memory.psv_idx = psv_index[0];\n\tmr->sig->psv_wire.psv_idx = psv_index[1];\n\n\tmr->sig->sig_status_checked = true;\n\tmr->sig->sig_err_exists = false;\n\t \n\t++mr->sig->sigerr_count;\n\tmr->klm_mr = mlx5_ib_alloc_pi_mr(pd, max_num_sg, max_num_meta_sg,\n\t\t\t\t\t sizeof(struct mlx5_klm),\n\t\t\t\t\t MLX5_MKC_ACCESS_MODE_KLMS);\n\tif (IS_ERR(mr->klm_mr)) {\n\t\terr = PTR_ERR(mr->klm_mr);\n\t\tgoto err_destroy_psv;\n\t}\n\tmr->mtt_mr = mlx5_ib_alloc_pi_mr(pd, max_num_sg, max_num_meta_sg,\n\t\t\t\t\t sizeof(struct mlx5_mtt),\n\t\t\t\t\t MLX5_MKC_ACCESS_MODE_MTT);\n\tif (IS_ERR(mr->mtt_mr)) {\n\t\terr = PTR_ERR(mr->mtt_mr);\n\t\tgoto err_free_klm_mr;\n\t}\n\n\t \n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, bsf_en, 1);\n\tMLX5_SET(mkc, mkc, bsf_octword_size, MLX5_MKEY_BSF_OCTO_SIZE);\n\n\terr = _mlx5_alloc_mkey_descs(pd, mr, 4, sizeof(struct mlx5_klm), 0,\n\t\t\t\t     MLX5_MKC_ACCESS_MODE_KLMS, in, inlen);\n\tif (err)\n\t\tgoto err_free_mtt_mr;\n\n\terr = xa_err(xa_store(&dev->sig_mrs, mlx5_base_mkey(mr->mmkey.key),\n\t\t\t      mr->sig, GFP_KERNEL));\n\tif (err)\n\t\tgoto err_free_descs;\n\treturn 0;\n\nerr_free_descs:\n\tdestroy_mkey(dev, mr);\n\tmlx5_free_priv_descs(mr);\nerr_free_mtt_mr:\n\tmlx5_ib_dereg_mr(&mr->mtt_mr->ibmr, NULL);\n\tmr->mtt_mr = NULL;\nerr_free_klm_mr:\n\tmlx5_ib_dereg_mr(&mr->klm_mr->ibmr, NULL);\n\tmr->klm_mr = NULL;\nerr_destroy_psv:\n\tif (mlx5_core_destroy_psv(dev->mdev, mr->sig->psv_memory.psv_idx))\n\t\tmlx5_ib_warn(dev, \"failed to destroy mem psv %d\\n\",\n\t\t\t     mr->sig->psv_memory.psv_idx);\n\tif (mlx5_core_destroy_psv(dev->mdev, mr->sig->psv_wire.psv_idx))\n\t\tmlx5_ib_warn(dev, \"failed to destroy wire psv %d\\n\",\n\t\t\t     mr->sig->psv_wire.psv_idx);\nerr_free_sig:\n\tkfree(mr->sig);\n\n\treturn err;\n}\n\nstatic struct ib_mr *__mlx5_ib_alloc_mr(struct ib_pd *pd,\n\t\t\t\t\tenum ib_mr_type mr_type, u32 max_num_sg,\n\t\t\t\t\tu32 max_num_meta_sg)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->device);\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tint ndescs = ALIGN(max_num_sg, 4);\n\tstruct mlx5_ib_mr *mr;\n\tu32 *in;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tmr->ibmr.device = pd->device;\n\tmr->umem = NULL;\n\n\tswitch (mr_type) {\n\tcase IB_MR_TYPE_MEM_REG:\n\t\terr = mlx5_alloc_mem_reg_descs(pd, mr, ndescs, in, inlen);\n\t\tbreak;\n\tcase IB_MR_TYPE_SG_GAPS:\n\t\terr = mlx5_alloc_sg_gaps_descs(pd, mr, ndescs, in, inlen);\n\t\tbreak;\n\tcase IB_MR_TYPE_INTEGRITY:\n\t\terr = mlx5_alloc_integrity_descs(pd, mr, max_num_sg,\n\t\t\t\t\t\t max_num_meta_sg, in, inlen);\n\t\tbreak;\n\tdefault:\n\t\tmlx5_ib_warn(dev, \"Invalid mr type %d\\n\", mr_type);\n\t\terr = -EINVAL;\n\t}\n\n\tif (err)\n\t\tgoto err_free_in;\n\n\tkfree(in);\n\n\treturn &mr->ibmr;\n\nerr_free_in:\n\tkfree(in);\nerr_free:\n\tkfree(mr);\n\treturn ERR_PTR(err);\n}\n\nstruct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t       u32 max_num_sg)\n{\n\treturn __mlx5_ib_alloc_mr(pd, mr_type, max_num_sg, 0);\n}\n\nstruct ib_mr *mlx5_ib_alloc_mr_integrity(struct ib_pd *pd,\n\t\t\t\t\t u32 max_num_sg, u32 max_num_meta_sg)\n{\n\treturn __mlx5_ib_alloc_mr(pd, IB_MR_TYPE_INTEGRITY, max_num_sg,\n\t\t\t\t  max_num_meta_sg);\n}\n\nint mlx5_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibmw->device);\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tstruct mlx5_ib_mw *mw = to_mmw(ibmw);\n\tunsigned int ndescs;\n\tu32 *in = NULL;\n\tvoid *mkc;\n\tint err;\n\tstruct mlx5_ib_alloc_mw req = {};\n\tstruct {\n\t\t__u32\tcomp_mask;\n\t\t__u32\tresponse_length;\n\t} resp = {};\n\n\terr = ib_copy_from_udata(&req, udata, min(udata->inlen, sizeof(req)));\n\tif (err)\n\t\treturn err;\n\n\tif (req.comp_mask || req.reserved1 || req.reserved2)\n\t\treturn -EOPNOTSUPP;\n\n\tif (udata->inlen > sizeof(req) &&\n\t    !ib_is_udata_cleared(udata, sizeof(req),\n\t\t\t\t udata->inlen - sizeof(req)))\n\t\treturn -EOPNOTSUPP;\n\n\tndescs = req.num_klms ? roundup(req.num_klms, 4) : roundup(1, 4);\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, free, 1);\n\tMLX5_SET(mkc, mkc, translations_octword_size, ndescs);\n\tMLX5_SET(mkc, mkc, pd, to_mpd(ibmw->pd)->pdn);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_KLMS);\n\tMLX5_SET(mkc, mkc, en_rinval, !!((ibmw->type == IB_MW_TYPE_2)));\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\n\terr = mlx5_ib_create_mkey(dev, &mw->mmkey, in, inlen);\n\tif (err)\n\t\tgoto free;\n\n\tmw->mmkey.type = MLX5_MKEY_MW;\n\tibmw->rkey = mw->mmkey.key;\n\tmw->mmkey.ndescs = ndescs;\n\n\tresp.response_length =\n\t\tmin(offsetofend(typeof(resp), response_length), udata->outlen);\n\tif (resp.response_length) {\n\t\terr = ib_copy_to_udata(udata, &resp, resp.response_length);\n\t\tif (err)\n\t\t\tgoto free_mkey;\n\t}\n\n\tif (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {\n\t\terr = mlx5r_store_odp_mkey(dev, &mw->mmkey);\n\t\tif (err)\n\t\t\tgoto free_mkey;\n\t}\n\n\tkfree(in);\n\treturn 0;\n\nfree_mkey:\n\tmlx5_core_destroy_mkey(dev->mdev, mw->mmkey.key);\nfree:\n\tkfree(in);\n\treturn err;\n}\n\nint mlx5_ib_dealloc_mw(struct ib_mw *mw)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(mw->device);\n\tstruct mlx5_ib_mw *mmw = to_mmw(mw);\n\n\tif (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) &&\n\t    xa_erase(&dev->odp_mkeys, mlx5_base_mkey(mmw->mmkey.key)))\n\t\t \n\t\tmlx5r_deref_wait_odp_mkey(&mmw->mmkey);\n\n\treturn mlx5_core_destroy_mkey(dev->mdev, mmw->mmkey.key);\n}\n\nint mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,\n\t\t\t    struct ib_mr_status *mr_status)\n{\n\tstruct mlx5_ib_mr *mmr = to_mmr(ibmr);\n\tint ret = 0;\n\n\tif (check_mask & ~IB_MR_CHECK_SIG_STATUS) {\n\t\tpr_err(\"Invalid status check mask\\n\");\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tmr_status->fail_status = 0;\n\tif (check_mask & IB_MR_CHECK_SIG_STATUS) {\n\t\tif (!mmr->sig) {\n\t\t\tret = -EINVAL;\n\t\t\tpr_err(\"signature status check requested on a non-signature enabled MR\\n\");\n\t\t\tgoto done;\n\t\t}\n\n\t\tmmr->sig->sig_status_checked = true;\n\t\tif (!mmr->sig->sig_err_exists)\n\t\t\tgoto done;\n\n\t\tif (ibmr->lkey == mmr->sig->err_item.key)\n\t\t\tmemcpy(&mr_status->sig_err, &mmr->sig->err_item,\n\t\t\t       sizeof(mr_status->sig_err));\n\t\telse {\n\t\t\tmr_status->sig_err.err_type = IB_SIG_BAD_GUARD;\n\t\t\tmr_status->sig_err.sig_err_offset = 0;\n\t\t\tmr_status->sig_err.key = mmr->sig->err_item.key;\n\t\t}\n\n\t\tmmr->sig->sig_err_exists = false;\n\t\tmr_status->fail_status |= IB_MR_CHECK_SIG_STATUS;\n\t}\n\ndone:\n\treturn ret;\n}\n\nstatic int\nmlx5_ib_map_pa_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,\n\t\t\tint data_sg_nents, unsigned int *data_sg_offset,\n\t\t\tstruct scatterlist *meta_sg, int meta_sg_nents,\n\t\t\tunsigned int *meta_sg_offset)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tunsigned int sg_offset = 0;\n\tint n = 0;\n\n\tmr->meta_length = 0;\n\tif (data_sg_nents == 1) {\n\t\tn++;\n\t\tmr->mmkey.ndescs = 1;\n\t\tif (data_sg_offset)\n\t\t\tsg_offset = *data_sg_offset;\n\t\tmr->data_length = sg_dma_len(data_sg) - sg_offset;\n\t\tmr->data_iova = sg_dma_address(data_sg) + sg_offset;\n\t\tif (meta_sg_nents == 1) {\n\t\t\tn++;\n\t\t\tmr->meta_ndescs = 1;\n\t\t\tif (meta_sg_offset)\n\t\t\t\tsg_offset = *meta_sg_offset;\n\t\t\telse\n\t\t\t\tsg_offset = 0;\n\t\t\tmr->meta_length = sg_dma_len(meta_sg) - sg_offset;\n\t\t\tmr->pi_iova = sg_dma_address(meta_sg) + sg_offset;\n\t\t}\n\t\tibmr->length = mr->data_length + mr->meta_length;\n\t}\n\n\treturn n;\n}\n\nstatic int\nmlx5_ib_sg_to_klms(struct mlx5_ib_mr *mr,\n\t\t   struct scatterlist *sgl,\n\t\t   unsigned short sg_nents,\n\t\t   unsigned int *sg_offset_p,\n\t\t   struct scatterlist *meta_sgl,\n\t\t   unsigned short meta_sg_nents,\n\t\t   unsigned int *meta_sg_offset_p)\n{\n\tstruct scatterlist *sg = sgl;\n\tstruct mlx5_klm *klms = mr->descs;\n\tunsigned int sg_offset = sg_offset_p ? *sg_offset_p : 0;\n\tu32 lkey = mr->ibmr.pd->local_dma_lkey;\n\tint i, j = 0;\n\n\tmr->ibmr.iova = sg_dma_address(sg) + sg_offset;\n\tmr->ibmr.length = 0;\n\n\tfor_each_sg(sgl, sg, sg_nents, i) {\n\t\tif (unlikely(i >= mr->max_descs))\n\t\t\tbreak;\n\t\tklms[i].va = cpu_to_be64(sg_dma_address(sg) + sg_offset);\n\t\tklms[i].bcount = cpu_to_be32(sg_dma_len(sg) - sg_offset);\n\t\tklms[i].key = cpu_to_be32(lkey);\n\t\tmr->ibmr.length += sg_dma_len(sg) - sg_offset;\n\n\t\tsg_offset = 0;\n\t}\n\n\tif (sg_offset_p)\n\t\t*sg_offset_p = sg_offset;\n\n\tmr->mmkey.ndescs = i;\n\tmr->data_length = mr->ibmr.length;\n\n\tif (meta_sg_nents) {\n\t\tsg = meta_sgl;\n\t\tsg_offset = meta_sg_offset_p ? *meta_sg_offset_p : 0;\n\t\tfor_each_sg(meta_sgl, sg, meta_sg_nents, j) {\n\t\t\tif (unlikely(i + j >= mr->max_descs))\n\t\t\t\tbreak;\n\t\t\tklms[i + j].va = cpu_to_be64(sg_dma_address(sg) +\n\t\t\t\t\t\t     sg_offset);\n\t\t\tklms[i + j].bcount = cpu_to_be32(sg_dma_len(sg) -\n\t\t\t\t\t\t\t sg_offset);\n\t\t\tklms[i + j].key = cpu_to_be32(lkey);\n\t\t\tmr->ibmr.length += sg_dma_len(sg) - sg_offset;\n\n\t\t\tsg_offset = 0;\n\t\t}\n\t\tif (meta_sg_offset_p)\n\t\t\t*meta_sg_offset_p = sg_offset;\n\n\t\tmr->meta_ndescs = j;\n\t\tmr->meta_length = mr->ibmr.length - mr->data_length;\n\t}\n\n\treturn i + j;\n}\n\nstatic int mlx5_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\t__be64 *descs;\n\n\tif (unlikely(mr->mmkey.ndescs == mr->max_descs))\n\t\treturn -ENOMEM;\n\n\tdescs = mr->descs;\n\tdescs[mr->mmkey.ndescs++] = cpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR);\n\n\treturn 0;\n}\n\nstatic int mlx5_set_page_pi(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\t__be64 *descs;\n\n\tif (unlikely(mr->mmkey.ndescs + mr->meta_ndescs == mr->max_descs))\n\t\treturn -ENOMEM;\n\n\tdescs = mr->descs;\n\tdescs[mr->mmkey.ndescs + mr->meta_ndescs++] =\n\t\tcpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR);\n\n\treturn 0;\n}\n\nstatic int\nmlx5_ib_map_mtt_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,\n\t\t\t int data_sg_nents, unsigned int *data_sg_offset,\n\t\t\t struct scatterlist *meta_sg, int meta_sg_nents,\n\t\t\t unsigned int *meta_sg_offset)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tstruct mlx5_ib_mr *pi_mr = mr->mtt_mr;\n\tint n;\n\n\tpi_mr->mmkey.ndescs = 0;\n\tpi_mr->meta_ndescs = 0;\n\tpi_mr->meta_length = 0;\n\n\tib_dma_sync_single_for_cpu(ibmr->device, pi_mr->desc_map,\n\t\t\t\t   pi_mr->desc_size * pi_mr->max_descs,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tpi_mr->ibmr.page_size = ibmr->page_size;\n\tn = ib_sg_to_pages(&pi_mr->ibmr, data_sg, data_sg_nents, data_sg_offset,\n\t\t\t   mlx5_set_page);\n\tif (n != data_sg_nents)\n\t\treturn n;\n\n\tpi_mr->data_iova = pi_mr->ibmr.iova;\n\tpi_mr->data_length = pi_mr->ibmr.length;\n\tpi_mr->ibmr.length = pi_mr->data_length;\n\tibmr->length = pi_mr->data_length;\n\n\tif (meta_sg_nents) {\n\t\tu64 page_mask = ~((u64)ibmr->page_size - 1);\n\t\tu64 iova = pi_mr->data_iova;\n\n\t\tn += ib_sg_to_pages(&pi_mr->ibmr, meta_sg, meta_sg_nents,\n\t\t\t\t    meta_sg_offset, mlx5_set_page_pi);\n\n\t\tpi_mr->meta_length = pi_mr->ibmr.length;\n\t\t \n\t\tpi_mr->pi_iova = (iova & page_mask) +\n\t\t\t\t pi_mr->mmkey.ndescs * ibmr->page_size +\n\t\t\t\t (pi_mr->ibmr.iova & ~page_mask);\n\t\t \n\t\tpi_mr->ibmr.length = pi_mr->pi_iova + pi_mr->meta_length - iova;\n\t\tpi_mr->ibmr.iova = iova;\n\t\tibmr->length += pi_mr->meta_length;\n\t}\n\n\tib_dma_sync_single_for_device(ibmr->device, pi_mr->desc_map,\n\t\t\t\t      pi_mr->desc_size * pi_mr->max_descs,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\treturn n;\n}\n\nstatic int\nmlx5_ib_map_klm_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,\n\t\t\t int data_sg_nents, unsigned int *data_sg_offset,\n\t\t\t struct scatterlist *meta_sg, int meta_sg_nents,\n\t\t\t unsigned int *meta_sg_offset)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tstruct mlx5_ib_mr *pi_mr = mr->klm_mr;\n\tint n;\n\n\tpi_mr->mmkey.ndescs = 0;\n\tpi_mr->meta_ndescs = 0;\n\tpi_mr->meta_length = 0;\n\n\tib_dma_sync_single_for_cpu(ibmr->device, pi_mr->desc_map,\n\t\t\t\t   pi_mr->desc_size * pi_mr->max_descs,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tn = mlx5_ib_sg_to_klms(pi_mr, data_sg, data_sg_nents, data_sg_offset,\n\t\t\t       meta_sg, meta_sg_nents, meta_sg_offset);\n\n\tib_dma_sync_single_for_device(ibmr->device, pi_mr->desc_map,\n\t\t\t\t      pi_mr->desc_size * pi_mr->max_descs,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\t \n\tpi_mr->data_iova = 0;\n\tpi_mr->ibmr.iova = 0;\n\tpi_mr->pi_iova = pi_mr->data_length;\n\tibmr->length = pi_mr->ibmr.length;\n\n\treturn n;\n}\n\nint mlx5_ib_map_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,\n\t\t\t int data_sg_nents, unsigned int *data_sg_offset,\n\t\t\t struct scatterlist *meta_sg, int meta_sg_nents,\n\t\t\t unsigned int *meta_sg_offset)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tstruct mlx5_ib_mr *pi_mr = NULL;\n\tint n;\n\n\tWARN_ON(ibmr->type != IB_MR_TYPE_INTEGRITY);\n\n\tmr->mmkey.ndescs = 0;\n\tmr->data_length = 0;\n\tmr->data_iova = 0;\n\tmr->meta_ndescs = 0;\n\tmr->pi_iova = 0;\n\t \n\tn = mlx5_ib_map_pa_mr_sg_pi(ibmr, data_sg, data_sg_nents,\n\t\t\t\t    data_sg_offset, meta_sg, meta_sg_nents,\n\t\t\t\t    meta_sg_offset);\n\tif (n == data_sg_nents + meta_sg_nents)\n\t\tgoto out;\n\t \n\tpi_mr = mr->mtt_mr;\n\tn = mlx5_ib_map_mtt_mr_sg_pi(ibmr, data_sg, data_sg_nents,\n\t\t\t\t     data_sg_offset, meta_sg, meta_sg_nents,\n\t\t\t\t     meta_sg_offset);\n\tif (n == data_sg_nents + meta_sg_nents)\n\t\tgoto out;\n\n\tpi_mr = mr->klm_mr;\n\tn = mlx5_ib_map_klm_mr_sg_pi(ibmr, data_sg, data_sg_nents,\n\t\t\t\t     data_sg_offset, meta_sg, meta_sg_nents,\n\t\t\t\t     meta_sg_offset);\n\tif (unlikely(n != data_sg_nents + meta_sg_nents))\n\t\treturn -ENOMEM;\n\nout:\n\t \n\tibmr->iova = 0;\n\tmr->pi_mr = pi_mr;\n\tif (pi_mr)\n\t\tibmr->sig_attrs->meta_length = pi_mr->meta_length;\n\telse\n\t\tibmr->sig_attrs->meta_length = mr->meta_length;\n\n\treturn 0;\n}\n\nint mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t      unsigned int *sg_offset)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(ibmr);\n\tint n;\n\n\tmr->mmkey.ndescs = 0;\n\n\tib_dma_sync_single_for_cpu(ibmr->device, mr->desc_map,\n\t\t\t\t   mr->desc_size * mr->max_descs,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tif (mr->access_mode == MLX5_MKC_ACCESS_MODE_KLMS)\n\t\tn = mlx5_ib_sg_to_klms(mr, sg, sg_nents, sg_offset, NULL, 0,\n\t\t\t\t       NULL);\n\telse\n\t\tn = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset,\n\t\t\t\tmlx5_set_page);\n\n\tib_dma_sync_single_for_device(ibmr->device, mr->desc_map,\n\t\t\t\t      mr->desc_size * mr->max_descs,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\treturn n;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}