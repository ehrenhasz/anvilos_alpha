{
  "module_name": "wr.c",
  "hash_id": "8bd31d2f5bf775c4a86f41035e5716ebe4a51680ef30220255d4178d87406196",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/wr.c",
  "human_readable_source": "\n \n\n#include <linux/gfp.h>\n#include <linux/mlx5/qp.h>\n#include <linux/mlx5/driver.h>\n#include \"wr.h\"\n#include \"umr.h\"\n\nstatic const u32 mlx5_ib_opcode[] = {\n\t[IB_WR_SEND]\t\t\t\t= MLX5_OPCODE_SEND,\n\t[IB_WR_LSO]\t\t\t\t= MLX5_OPCODE_LSO,\n\t[IB_WR_SEND_WITH_IMM]\t\t\t= MLX5_OPCODE_SEND_IMM,\n\t[IB_WR_RDMA_WRITE]\t\t\t= MLX5_OPCODE_RDMA_WRITE,\n\t[IB_WR_RDMA_WRITE_WITH_IMM]\t\t= MLX5_OPCODE_RDMA_WRITE_IMM,\n\t[IB_WR_RDMA_READ]\t\t\t= MLX5_OPCODE_RDMA_READ,\n\t[IB_WR_ATOMIC_CMP_AND_SWP]\t\t= MLX5_OPCODE_ATOMIC_CS,\n\t[IB_WR_ATOMIC_FETCH_AND_ADD]\t\t= MLX5_OPCODE_ATOMIC_FA,\n\t[IB_WR_SEND_WITH_INV]\t\t\t= MLX5_OPCODE_SEND_INVAL,\n\t[IB_WR_LOCAL_INV]\t\t\t= MLX5_OPCODE_UMR,\n\t[IB_WR_REG_MR]\t\t\t\t= MLX5_OPCODE_UMR,\n\t[IB_WR_MASKED_ATOMIC_CMP_AND_SWP]\t= MLX5_OPCODE_ATOMIC_MASKED_CS,\n\t[IB_WR_MASKED_ATOMIC_FETCH_AND_ADD]\t= MLX5_OPCODE_ATOMIC_MASKED_FA,\n\t[MLX5_IB_WR_UMR]\t\t\t= MLX5_OPCODE_UMR,\n};\n\nint mlx5r_wq_overflow(struct mlx5_ib_wq *wq, int nreq, struct ib_cq *ib_cq)\n{\n\tstruct mlx5_ib_cq *cq;\n\tunsigned int cur;\n\n\tcur = wq->head - wq->tail;\n\tif (likely(cur + nreq < wq->max_post))\n\t\treturn 0;\n\n\tcq = to_mcq(ib_cq);\n\tspin_lock(&cq->lock);\n\tcur = wq->head - wq->tail;\n\tspin_unlock(&cq->lock);\n\n\treturn cur + nreq >= wq->max_post;\n}\n\nstatic __always_inline void set_raddr_seg(struct mlx5_wqe_raddr_seg *rseg,\n\t\t\t\t\t  u64 remote_addr, u32 rkey)\n{\n\trseg->raddr    = cpu_to_be64(remote_addr);\n\trseg->rkey     = cpu_to_be32(rkey);\n\trseg->reserved = 0;\n}\n\nstatic void set_eth_seg(const struct ib_send_wr *wr, struct mlx5_ib_qp *qp,\n\t\t\tvoid **seg, int *size, void **cur_edge)\n{\n\tstruct mlx5_wqe_eth_seg *eseg = *seg;\n\n\tmemset(eseg, 0, sizeof(struct mlx5_wqe_eth_seg));\n\n\tif (wr->send_flags & IB_SEND_IP_CSUM)\n\t\teseg->cs_flags = MLX5_ETH_WQE_L3_CSUM |\n\t\t\t\t MLX5_ETH_WQE_L4_CSUM;\n\n\tif (wr->opcode == IB_WR_LSO) {\n\t\tstruct ib_ud_wr *ud_wr = container_of(wr, struct ib_ud_wr, wr);\n\t\tsize_t left, copysz;\n\t\tvoid *pdata = ud_wr->header;\n\t\tsize_t stride;\n\n\t\tleft = ud_wr->hlen;\n\t\teseg->mss = cpu_to_be16(ud_wr->mss);\n\t\teseg->inline_hdr.sz = cpu_to_be16(left);\n\n\t\t \n\t\tcopysz = min_t(u64, *cur_edge - (void *)eseg->inline_hdr.start,\n\t\t\t       left);\n\t\tmemcpy(eseg->inline_hdr.start, pdata, copysz);\n\t\tstride = ALIGN(sizeof(struct mlx5_wqe_eth_seg) -\n\t\t\t       sizeof(eseg->inline_hdr.start) + copysz, 16);\n\t\t*size += stride / 16;\n\t\t*seg += stride;\n\n\t\tif (copysz < left) {\n\t\t\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\t\t\tleft -= copysz;\n\t\t\tpdata += copysz;\n\t\t\tmlx5r_memcpy_send_wqe(&qp->sq, cur_edge, seg, size,\n\t\t\t\t\t      pdata, left);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t*seg += sizeof(struct mlx5_wqe_eth_seg);\n\t*size += sizeof(struct mlx5_wqe_eth_seg) / 16;\n}\n\nstatic void set_datagram_seg(struct mlx5_wqe_datagram_seg *dseg,\n\t\t\t     const struct ib_send_wr *wr)\n{\n\tmemcpy(&dseg->av, &to_mah(ud_wr(wr)->ah)->av, sizeof(struct mlx5_av));\n\tdseg->av.dqp_dct =\n\t\tcpu_to_be32(ud_wr(wr)->remote_qpn | MLX5_EXTENDED_UD_AV);\n\tdseg->av.key.qkey.qkey = cpu_to_be32(ud_wr(wr)->remote_qkey);\n}\n\nstatic void set_data_ptr_seg(struct mlx5_wqe_data_seg *dseg, struct ib_sge *sg)\n{\n\tdseg->byte_count = cpu_to_be32(sg->length);\n\tdseg->lkey       = cpu_to_be32(sg->lkey);\n\tdseg->addr       = cpu_to_be64(sg->addr);\n}\n\nstatic __be64 frwr_mkey_mask(bool atomic)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_LEN\t\t|\n\t\tMLX5_MKEY_MASK_PAGE_SIZE\t|\n\t\tMLX5_MKEY_MASK_START_ADDR\t|\n\t\tMLX5_MKEY_MASK_EN_RINVAL\t|\n\t\tMLX5_MKEY_MASK_KEY\t\t|\n\t\tMLX5_MKEY_MASK_LR\t\t|\n\t\tMLX5_MKEY_MASK_LW\t\t|\n\t\tMLX5_MKEY_MASK_RR\t\t|\n\t\tMLX5_MKEY_MASK_RW\t\t|\n\t\tMLX5_MKEY_MASK_SMALL_FENCE\t|\n\t\tMLX5_MKEY_MASK_FREE;\n\n\tif (atomic)\n\t\tresult |= MLX5_MKEY_MASK_A;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic __be64 sig_mkey_mask(void)\n{\n\tu64 result;\n\n\tresult = MLX5_MKEY_MASK_LEN\t\t|\n\t\tMLX5_MKEY_MASK_PAGE_SIZE\t|\n\t\tMLX5_MKEY_MASK_START_ADDR\t|\n\t\tMLX5_MKEY_MASK_EN_SIGERR\t|\n\t\tMLX5_MKEY_MASK_EN_RINVAL\t|\n\t\tMLX5_MKEY_MASK_KEY\t\t|\n\t\tMLX5_MKEY_MASK_LR\t\t|\n\t\tMLX5_MKEY_MASK_LW\t\t|\n\t\tMLX5_MKEY_MASK_RR\t\t|\n\t\tMLX5_MKEY_MASK_RW\t\t|\n\t\tMLX5_MKEY_MASK_SMALL_FENCE\t|\n\t\tMLX5_MKEY_MASK_FREE\t\t|\n\t\tMLX5_MKEY_MASK_BSF_EN;\n\n\treturn cpu_to_be64(result);\n}\n\nstatic void set_reg_umr_seg(struct mlx5_wqe_umr_ctrl_seg *umr,\n\t\t\t    struct mlx5_ib_mr *mr, u8 flags, bool atomic)\n{\n\tint size = (mr->mmkey.ndescs + mr->meta_ndescs) * mr->desc_size;\n\n\tmemset(umr, 0, sizeof(*umr));\n\n\tumr->flags = flags;\n\tumr->xlt_octowords = cpu_to_be16(mlx5r_umr_get_xlt_octo(size));\n\tumr->mkey_mask = frwr_mkey_mask(atomic);\n}\n\nstatic void set_linv_umr_seg(struct mlx5_wqe_umr_ctrl_seg *umr)\n{\n\tmemset(umr, 0, sizeof(*umr));\n\tumr->mkey_mask = cpu_to_be64(MLX5_MKEY_MASK_FREE);\n\tumr->flags = MLX5_UMR_INLINE;\n}\n\nstatic u8 get_umr_flags(int acc)\n{\n\treturn (acc & IB_ACCESS_REMOTE_ATOMIC ? MLX5_PERM_ATOMIC       : 0) |\n\t       (acc & IB_ACCESS_REMOTE_WRITE  ? MLX5_PERM_REMOTE_WRITE : 0) |\n\t       (acc & IB_ACCESS_REMOTE_READ   ? MLX5_PERM_REMOTE_READ  : 0) |\n\t       (acc & IB_ACCESS_LOCAL_WRITE   ? MLX5_PERM_LOCAL_WRITE  : 0) |\n\t\tMLX5_PERM_LOCAL_READ | MLX5_PERM_UMR_EN;\n}\n\nstatic void set_reg_mkey_seg(struct mlx5_mkey_seg *seg,\n\t\t\t     struct mlx5_ib_mr *mr,\n\t\t\t     u32 key, int access)\n{\n\tint ndescs = ALIGN(mr->mmkey.ndescs + mr->meta_ndescs, 8) >> 1;\n\n\tmemset(seg, 0, sizeof(*seg));\n\n\tif (mr->access_mode == MLX5_MKC_ACCESS_MODE_MTT)\n\t\tseg->log2_page_size = ilog2(mr->ibmr.page_size);\n\telse if (mr->access_mode == MLX5_MKC_ACCESS_MODE_KLMS)\n\t\t \n\t\tndescs *= 2;\n\n\tseg->flags = get_umr_flags(access) | mr->access_mode;\n\tseg->qpn_mkey7_0 = cpu_to_be32((key & 0xff) | 0xffffff00);\n\tseg->flags_pd = cpu_to_be32(MLX5_MKEY_REMOTE_INVAL);\n\tseg->start_addr = cpu_to_be64(mr->ibmr.iova);\n\tseg->len = cpu_to_be64(mr->ibmr.length);\n\tseg->xlt_oct_size = cpu_to_be32(ndescs);\n}\n\nstatic void set_linv_mkey_seg(struct mlx5_mkey_seg *seg)\n{\n\tmemset(seg, 0, sizeof(*seg));\n\tseg->status = MLX5_MKEY_STATUS_FREE;\n}\n\nstatic void set_reg_data_seg(struct mlx5_wqe_data_seg *dseg,\n\t\t\t     struct mlx5_ib_mr *mr,\n\t\t\t     struct mlx5_ib_pd *pd)\n{\n\tint bcount = mr->desc_size * (mr->mmkey.ndescs + mr->meta_ndescs);\n\n\tdseg->addr = cpu_to_be64(mr->desc_map);\n\tdseg->byte_count = cpu_to_be32(ALIGN(bcount, 64));\n\tdseg->lkey = cpu_to_be32(pd->ibpd.local_dma_lkey);\n}\n\nstatic __be32 send_ieth(const struct ib_send_wr *wr)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\treturn wr->ex.imm_data;\n\n\tcase IB_WR_SEND_WITH_INV:\n\t\treturn cpu_to_be32(wr->ex.invalidate_rkey);\n\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic u8 calc_sig(void *wqe, int size)\n{\n\tu8 *p = wqe;\n\tu8 res = 0;\n\tint i;\n\n\tfor (i = 0; i < size; i++)\n\t\tres ^= p[i];\n\n\treturn ~res;\n}\n\nstatic u8 wq_sig(void *wqe)\n{\n\treturn calc_sig(wqe, (*((u8 *)wqe + 8) & 0x3f) << 4);\n}\n\nstatic int set_data_inl_seg(struct mlx5_ib_qp *qp, const struct ib_send_wr *wr,\n\t\t\t    void **wqe, int *wqe_sz, void **cur_edge)\n{\n\tstruct mlx5_wqe_inline_seg *seg;\n\tsize_t offset;\n\tint inl = 0;\n\tint i;\n\n\tseg = *wqe;\n\t*wqe += sizeof(*seg);\n\toffset = sizeof(*seg);\n\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tsize_t len  = wr->sg_list[i].length;\n\t\tvoid *addr = (void *)(unsigned long)(wr->sg_list[i].addr);\n\n\t\tinl += len;\n\n\t\tif (unlikely(inl > qp->max_inline_data))\n\t\t\treturn -ENOMEM;\n\n\t\twhile (likely(len)) {\n\t\t\tsize_t leftlen;\n\t\t\tsize_t copysz;\n\n\t\t\thandle_post_send_edge(&qp->sq, wqe,\n\t\t\t\t\t      *wqe_sz + (offset >> 4),\n\t\t\t\t\t      cur_edge);\n\n\t\t\tleftlen = *cur_edge - *wqe;\n\t\t\tcopysz = min_t(size_t, leftlen, len);\n\n\t\t\tmemcpy(*wqe, addr, copysz);\n\t\t\tlen -= copysz;\n\t\t\taddr += copysz;\n\t\t\t*wqe += copysz;\n\t\t\toffset += copysz;\n\t\t}\n\t}\n\n\tseg->byte_count = cpu_to_be32(inl | MLX5_INLINE_SEG);\n\n\t*wqe_sz +=  ALIGN(inl + sizeof(seg->byte_count), 16) / 16;\n\n\treturn 0;\n}\n\nstatic u16 prot_field_size(enum ib_signature_type type)\n{\n\tswitch (type) {\n\tcase IB_SIG_TYPE_T10_DIF:\n\t\treturn MLX5_DIF_SIZE;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic u8 bs_selector(int block_size)\n{\n\tswitch (block_size) {\n\tcase 512:\t    return 0x1;\n\tcase 520:\t    return 0x2;\n\tcase 4096:\t    return 0x3;\n\tcase 4160:\t    return 0x4;\n\tcase 1073741824:    return 0x5;\n\tdefault:\t    return 0;\n\t}\n}\n\nstatic void mlx5_fill_inl_bsf(struct ib_sig_domain *domain,\n\t\t\t      struct mlx5_bsf_inl *inl)\n{\n\t \n\tinl->vld_refresh = cpu_to_be16(MLX5_BSF_INL_VALID |\n\t\t\t\t       MLX5_BSF_REFRESH_DIF);\n\tinl->dif_apptag = cpu_to_be16(domain->sig.dif.app_tag);\n\tinl->dif_reftag = cpu_to_be32(domain->sig.dif.ref_tag);\n\t \n\tinl->rp_inv_seed = MLX5_BSF_REPEAT_BLOCK;\n\tinl->sig_type = domain->sig.dif.bg_type == IB_T10DIF_CRC ?\n\t\t\tMLX5_DIF_CRC : MLX5_DIF_IPCS;\n\n\tif (domain->sig.dif.ref_remap)\n\t\tinl->dif_inc_ref_guard_check |= MLX5_BSF_INC_REFTAG;\n\n\tif (domain->sig.dif.app_escape) {\n\t\tif (domain->sig.dif.ref_escape)\n\t\t\tinl->dif_inc_ref_guard_check |= MLX5_BSF_APPREF_ESCAPE;\n\t\telse\n\t\t\tinl->dif_inc_ref_guard_check |= MLX5_BSF_APPTAG_ESCAPE;\n\t}\n\n\tinl->dif_app_bitmask_check =\n\t\tcpu_to_be16(domain->sig.dif.apptag_check_mask);\n}\n\nstatic int mlx5_set_bsf(struct ib_mr *sig_mr,\n\t\t\tstruct ib_sig_attrs *sig_attrs,\n\t\t\tstruct mlx5_bsf *bsf, u32 data_size)\n{\n\tstruct mlx5_core_sig_ctx *msig = to_mmr(sig_mr)->sig;\n\tstruct mlx5_bsf_basic *basic = &bsf->basic;\n\tstruct ib_sig_domain *mem = &sig_attrs->mem;\n\tstruct ib_sig_domain *wire = &sig_attrs->wire;\n\n\tmemset(bsf, 0, sizeof(*bsf));\n\n\t \n\tbasic->bsf_size_sbs = 1 << 7;\n\t \n\tbasic->check_byte_mask = sig_attrs->check_mask;\n\tbasic->raw_data_size = cpu_to_be32(data_size);\n\n\t \n\tswitch (sig_attrs->mem.sig_type) {\n\tcase IB_SIG_TYPE_NONE:\n\t\tbreak;\n\tcase IB_SIG_TYPE_T10_DIF:\n\t\tbasic->mem.bs_selector = bs_selector(mem->sig.dif.pi_interval);\n\t\tbasic->m_bfs_psv = cpu_to_be32(msig->psv_memory.psv_idx);\n\t\tmlx5_fill_inl_bsf(mem, &bsf->m_inl);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tswitch (sig_attrs->wire.sig_type) {\n\tcase IB_SIG_TYPE_NONE:\n\t\tbreak;\n\tcase IB_SIG_TYPE_T10_DIF:\n\t\tif (mem->sig.dif.pi_interval == wire->sig.dif.pi_interval &&\n\t\t    mem->sig_type == wire->sig_type) {\n\t\t\t \n\t\t\tbasic->bsf_size_sbs |= 1 << 4;\n\t\t\tif (mem->sig.dif.bg_type == wire->sig.dif.bg_type)\n\t\t\t\tbasic->wire.copy_byte_mask |= MLX5_CPY_GRD_MASK;\n\t\t\tif (mem->sig.dif.app_tag == wire->sig.dif.app_tag)\n\t\t\t\tbasic->wire.copy_byte_mask |= MLX5_CPY_APP_MASK;\n\t\t\tif (mem->sig.dif.ref_tag == wire->sig.dif.ref_tag)\n\t\t\t\tbasic->wire.copy_byte_mask |= MLX5_CPY_REF_MASK;\n\t\t} else\n\t\t\tbasic->wire.bs_selector =\n\t\t\t\tbs_selector(wire->sig.dif.pi_interval);\n\n\t\tbasic->w_bfs_psv = cpu_to_be32(msig->psv_wire.psv_idx);\n\t\tmlx5_fill_inl_bsf(wire, &bsf->w_inl);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n\nstatic int set_sig_data_segment(const struct ib_send_wr *send_wr,\n\t\t\t\tstruct ib_mr *sig_mr,\n\t\t\t\tstruct ib_sig_attrs *sig_attrs,\n\t\t\t\tstruct mlx5_ib_qp *qp, void **seg, int *size,\n\t\t\t\tvoid **cur_edge)\n{\n\tstruct mlx5_bsf *bsf;\n\tu32 data_len;\n\tu32 data_key;\n\tu64 data_va;\n\tu32 prot_len = 0;\n\tu32 prot_key = 0;\n\tu64 prot_va = 0;\n\tbool prot = false;\n\tint ret;\n\tint wqe_size;\n\tstruct mlx5_ib_mr *mr = to_mmr(sig_mr);\n\tstruct mlx5_ib_mr *pi_mr = mr->pi_mr;\n\n\tdata_len = pi_mr->data_length;\n\tdata_key = pi_mr->ibmr.lkey;\n\tdata_va = pi_mr->data_iova;\n\tif (pi_mr->meta_ndescs) {\n\t\tprot_len = pi_mr->meta_length;\n\t\tprot_key = pi_mr->ibmr.lkey;\n\t\tprot_va = pi_mr->pi_iova;\n\t\tprot = true;\n\t}\n\n\tif (!prot || (data_key == prot_key && data_va == prot_va &&\n\t\t      data_len == prot_len)) {\n\t\t \n\t\tstruct mlx5_klm *data_klm = *seg;\n\n\t\tdata_klm->bcount = cpu_to_be32(data_len);\n\t\tdata_klm->key = cpu_to_be32(data_key);\n\t\tdata_klm->va = cpu_to_be64(data_va);\n\t\twqe_size = ALIGN(sizeof(*data_klm), 64);\n\t} else {\n\t\t \n\t\tstruct mlx5_stride_block_ctrl_seg *sblock_ctrl;\n\t\tstruct mlx5_stride_block_entry *data_sentry;\n\t\tstruct mlx5_stride_block_entry *prot_sentry;\n\t\tu16 block_size = sig_attrs->mem.sig.dif.pi_interval;\n\t\tint prot_size;\n\n\t\tsblock_ctrl = *seg;\n\t\tdata_sentry = (void *)sblock_ctrl + sizeof(*sblock_ctrl);\n\t\tprot_sentry = (void *)data_sentry + sizeof(*data_sentry);\n\n\t\tprot_size = prot_field_size(sig_attrs->mem.sig_type);\n\t\tif (!prot_size) {\n\t\t\tpr_err(\"Bad block size given: %u\\n\", block_size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsblock_ctrl->bcount_per_cycle = cpu_to_be32(block_size +\n\t\t\t\t\t\t\t    prot_size);\n\t\tsblock_ctrl->op = cpu_to_be32(MLX5_STRIDE_BLOCK_OP);\n\t\tsblock_ctrl->repeat_count = cpu_to_be32(data_len / block_size);\n\t\tsblock_ctrl->num_entries = cpu_to_be16(2);\n\n\t\tdata_sentry->bcount = cpu_to_be16(block_size);\n\t\tdata_sentry->key = cpu_to_be32(data_key);\n\t\tdata_sentry->va = cpu_to_be64(data_va);\n\t\tdata_sentry->stride = cpu_to_be16(block_size);\n\n\t\tprot_sentry->bcount = cpu_to_be16(prot_size);\n\t\tprot_sentry->key = cpu_to_be32(prot_key);\n\t\tprot_sentry->va = cpu_to_be64(prot_va);\n\t\tprot_sentry->stride = cpu_to_be16(prot_size);\n\n\t\twqe_size = ALIGN(sizeof(*sblock_ctrl) + sizeof(*data_sentry) +\n\t\t\t\t sizeof(*prot_sentry), 64);\n\t}\n\n\t*seg += wqe_size;\n\t*size += wqe_size / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\tbsf = *seg;\n\tret = mlx5_set_bsf(sig_mr, sig_attrs, bsf, data_len);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\t*seg += sizeof(*bsf);\n\t*size += sizeof(*bsf) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\treturn 0;\n}\n\nstatic void set_sig_mkey_segment(struct mlx5_mkey_seg *seg,\n\t\t\t\t struct ib_mr *sig_mr, int access_flags,\n\t\t\t\t u32 size, u32 length, u32 pdn)\n{\n\tu32 sig_key = sig_mr->rkey;\n\tu8 sigerr = to_mmr(sig_mr)->sig->sigerr_count & 1;\n\n\tmemset(seg, 0, sizeof(*seg));\n\n\tseg->flags = get_umr_flags(access_flags) | MLX5_MKC_ACCESS_MODE_KLMS;\n\tseg->qpn_mkey7_0 = cpu_to_be32((sig_key & 0xff) | 0xffffff00);\n\tseg->flags_pd = cpu_to_be32(MLX5_MKEY_REMOTE_INVAL | sigerr << 26 |\n\t\t\t\t    MLX5_MKEY_BSF_EN | pdn);\n\tseg->len = cpu_to_be64(length);\n\tseg->xlt_oct_size = cpu_to_be32(mlx5r_umr_get_xlt_octo(size));\n\tseg->bsfs_octo_size = cpu_to_be32(MLX5_MKEY_BSF_OCTO_SIZE);\n}\n\nstatic void set_sig_umr_segment(struct mlx5_wqe_umr_ctrl_seg *umr,\n\t\t\t\tu32 size)\n{\n\tmemset(umr, 0, sizeof(*umr));\n\n\tumr->flags = MLX5_FLAGS_INLINE | MLX5_FLAGS_CHECK_FREE;\n\tumr->xlt_octowords = cpu_to_be16(mlx5r_umr_get_xlt_octo(size));\n\tumr->bsf_octowords = cpu_to_be16(MLX5_MKEY_BSF_OCTO_SIZE);\n\tumr->mkey_mask = sig_mkey_mask();\n}\n\nstatic int set_pi_umr_wr(const struct ib_send_wr *send_wr,\n\t\t\t struct mlx5_ib_qp *qp, void **seg, int *size,\n\t\t\t void **cur_edge)\n{\n\tconst struct ib_reg_wr *wr = reg_wr(send_wr);\n\tstruct mlx5_ib_mr *sig_mr = to_mmr(wr->mr);\n\tstruct mlx5_ib_mr *pi_mr = sig_mr->pi_mr;\n\tstruct ib_sig_attrs *sig_attrs = sig_mr->ibmr.sig_attrs;\n\tu32 pdn = to_mpd(qp->ibqp.pd)->pdn;\n\tu32 xlt_size;\n\tint region_len, ret;\n\n\tif (unlikely(send_wr->num_sge != 0) ||\n\t    unlikely(wr->access & IB_ACCESS_REMOTE_ATOMIC) ||\n\t    unlikely(!sig_mr->sig) || unlikely(!qp->ibqp.integrity_en) ||\n\t    unlikely(!sig_mr->sig->sig_status_checked))\n\t\treturn -EINVAL;\n\n\t \n\tregion_len = pi_mr->ibmr.length;\n\n\t \n\tif (sig_attrs->mem.sig_type != IB_SIG_TYPE_NONE)\n\t\txlt_size = 0x30;\n\telse\n\t\txlt_size = sizeof(struct mlx5_klm);\n\n\tset_sig_umr_segment(*seg, xlt_size);\n\t*seg += sizeof(struct mlx5_wqe_umr_ctrl_seg);\n\t*size += sizeof(struct mlx5_wqe_umr_ctrl_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\tset_sig_mkey_segment(*seg, wr->mr, wr->access, xlt_size, region_len,\n\t\t\t     pdn);\n\t*seg += sizeof(struct mlx5_mkey_seg);\n\t*size += sizeof(struct mlx5_mkey_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\tret = set_sig_data_segment(send_wr, wr->mr, sig_attrs, qp, seg, size,\n\t\t\t\t   cur_edge);\n\tif (ret)\n\t\treturn ret;\n\n\tsig_mr->sig->sig_status_checked = false;\n\treturn 0;\n}\n\nstatic int set_psv_wr(struct ib_sig_domain *domain,\n\t\t      u32 psv_idx, void **seg, int *size)\n{\n\tstruct mlx5_seg_set_psv *psv_seg = *seg;\n\n\tmemset(psv_seg, 0, sizeof(*psv_seg));\n\tpsv_seg->psv_num = cpu_to_be32(psv_idx);\n\tswitch (domain->sig_type) {\n\tcase IB_SIG_TYPE_NONE:\n\t\tbreak;\n\tcase IB_SIG_TYPE_T10_DIF:\n\t\tpsv_seg->transient_sig = cpu_to_be32(domain->sig.dif.bg << 16 |\n\t\t\t\t\t\t     domain->sig.dif.app_tag);\n\t\tpsv_seg->ref_tag = cpu_to_be32(domain->sig.dif.ref_tag);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Bad signature type (%d) is given.\\n\",\n\t\t       domain->sig_type);\n\t\treturn -EINVAL;\n\t}\n\n\t*seg += sizeof(*psv_seg);\n\t*size += sizeof(*psv_seg) / 16;\n\n\treturn 0;\n}\n\nstatic int set_reg_wr(struct mlx5_ib_qp *qp,\n\t\t      const struct ib_reg_wr *wr,\n\t\t      void **seg, int *size, void **cur_edge,\n\t\t      bool check_not_free)\n{\n\tstruct mlx5_ib_mr *mr = to_mmr(wr->mr);\n\tstruct mlx5_ib_pd *pd = to_mpd(qp->ibqp.pd);\n\tstruct mlx5_ib_dev *dev = to_mdev(pd->ibpd.device);\n\tint mr_list_size = (mr->mmkey.ndescs + mr->meta_ndescs) * mr->desc_size;\n\tbool umr_inline = mr_list_size <= MLX5_IB_SQ_UMR_INLINE_THRESHOLD;\n\tbool atomic = wr->access & IB_ACCESS_REMOTE_ATOMIC;\n\tu8 flags = 0;\n\n\t \n\tif (!mlx5r_umr_can_reconfig(dev, 0, wr->access)) {\n\t\tmlx5_ib_warn(\n\t\t\tto_mdev(qp->ibqp.device),\n\t\t\t\"Fast update for MR access flags is not possible\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(wr->wr.send_flags & IB_SEND_INLINE)) {\n\t\tmlx5_ib_warn(to_mdev(qp->ibqp.device),\n\t\t\t     \"Invalid IB_SEND_INLINE send flag\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (check_not_free)\n\t\tflags |= MLX5_UMR_CHECK_NOT_FREE;\n\tif (umr_inline)\n\t\tflags |= MLX5_UMR_INLINE;\n\n\tset_reg_umr_seg(*seg, mr, flags, atomic);\n\t*seg += sizeof(struct mlx5_wqe_umr_ctrl_seg);\n\t*size += sizeof(struct mlx5_wqe_umr_ctrl_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\tset_reg_mkey_seg(*seg, mr, wr->key, wr->access);\n\t*seg += sizeof(struct mlx5_mkey_seg);\n\t*size += sizeof(struct mlx5_mkey_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\tif (umr_inline) {\n\t\tmlx5r_memcpy_send_wqe(&qp->sq, cur_edge, seg, size, mr->descs,\n\t\t\t\t      mr_list_size);\n\t\t*size = ALIGN(*size, MLX5_SEND_WQE_BB >> 4);\n\t} else {\n\t\tset_reg_data_seg(*seg, mr, pd);\n\t\t*seg += sizeof(struct mlx5_wqe_data_seg);\n\t\t*size += (sizeof(struct mlx5_wqe_data_seg) / 16);\n\t}\n\treturn 0;\n}\n\nstatic void set_linv_wr(struct mlx5_ib_qp *qp, void **seg, int *size,\n\t\t\tvoid **cur_edge)\n{\n\tset_linv_umr_seg(*seg);\n\t*seg += sizeof(struct mlx5_wqe_umr_ctrl_seg);\n\t*size += sizeof(struct mlx5_wqe_umr_ctrl_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\tset_linv_mkey_seg(*seg);\n\t*seg += sizeof(struct mlx5_mkey_seg);\n\t*size += sizeof(struct mlx5_mkey_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n}\n\nstatic void dump_wqe(struct mlx5_ib_qp *qp, u32 idx, int size_16)\n{\n\t__be32 *p = NULL;\n\tint i, j;\n\n\tpr_debug(\"dump WQE index %u:\\n\", idx);\n\tfor (i = 0, j = 0; i < size_16 * 4; i += 4, j += 4) {\n\t\tif ((i & 0xf) == 0) {\n\t\t\tp = mlx5_frag_buf_get_wqe(&qp->sq.fbc, idx);\n\t\t\tpr_debug(\"WQBB at %p:\\n\", (void *)p);\n\t\t\tj = 0;\n\t\t\tidx = (idx + 1) & (qp->sq.wqe_cnt - 1);\n\t\t}\n\t\tpr_debug(\"%08x %08x %08x %08x\\n\", be32_to_cpu(p[j]),\n\t\t\t be32_to_cpu(p[j + 1]), be32_to_cpu(p[j + 2]),\n\t\t\t be32_to_cpu(p[j + 3]));\n\t}\n}\n\nint mlx5r_begin_wqe(struct mlx5_ib_qp *qp, void **seg,\n\t\t    struct mlx5_wqe_ctrl_seg **ctrl, unsigned int *idx,\n\t\t    int *size, void **cur_edge, int nreq, __be32 general_id,\n\t\t    bool send_signaled, bool solicited)\n{\n\tif (unlikely(mlx5r_wq_overflow(&qp->sq, nreq, qp->ibqp.send_cq)))\n\t\treturn -ENOMEM;\n\n\t*idx = qp->sq.cur_post & (qp->sq.wqe_cnt - 1);\n\t*seg = mlx5_frag_buf_get_wqe(&qp->sq.fbc, *idx);\n\t*ctrl = *seg;\n\t*(uint32_t *)(*seg + 8) = 0;\n\t(*ctrl)->general_id = general_id;\n\t(*ctrl)->fm_ce_se = qp->sq_signal_bits |\n\t\t\t    (send_signaled ? MLX5_WQE_CTRL_CQ_UPDATE : 0) |\n\t\t\t    (solicited ? MLX5_WQE_CTRL_SOLICITED : 0);\n\n\t*seg += sizeof(**ctrl);\n\t*size = sizeof(**ctrl) / 16;\n\t*cur_edge = qp->sq.cur_edge;\n\n\treturn 0;\n}\n\nstatic int begin_wqe(struct mlx5_ib_qp *qp, void **seg,\n\t\t     struct mlx5_wqe_ctrl_seg **ctrl,\n\t\t     const struct ib_send_wr *wr, unsigned int *idx, int *size,\n\t\t     void **cur_edge, int nreq)\n{\n\treturn mlx5r_begin_wqe(qp, seg, ctrl, idx, size, cur_edge, nreq,\n\t\t\t       send_ieth(wr), wr->send_flags & IB_SEND_SIGNALED,\n\t\t\t       wr->send_flags & IB_SEND_SOLICITED);\n}\n\nvoid mlx5r_finish_wqe(struct mlx5_ib_qp *qp, struct mlx5_wqe_ctrl_seg *ctrl,\n\t\t      void *seg, u8 size, void *cur_edge, unsigned int idx,\n\t\t      u64 wr_id, int nreq, u8 fence, u32 mlx5_opcode)\n{\n\tu8 opmod = 0;\n\n\tctrl->opmod_idx_opcode = cpu_to_be32(((u32)(qp->sq.cur_post) << 8) |\n\t\t\t\t\t     mlx5_opcode | ((u32)opmod << 24));\n\tctrl->qpn_ds = cpu_to_be32(size | (qp->trans_qp.base.mqp.qpn << 8));\n\tctrl->fm_ce_se |= fence;\n\tif (unlikely(qp->flags_en & MLX5_QP_FLAG_SIGNATURE))\n\t\tctrl->signature = wq_sig(ctrl);\n\n\tqp->sq.wrid[idx] = wr_id;\n\tqp->sq.w_list[idx].opcode = mlx5_opcode;\n\tqp->sq.wqe_head[idx] = qp->sq.head + nreq;\n\tqp->sq.cur_post += DIV_ROUND_UP(size * 16, MLX5_SEND_WQE_BB);\n\tqp->sq.w_list[idx].next = qp->sq.cur_post;\n\n\t \n\tseg = PTR_ALIGN(seg, MLX5_SEND_WQE_BB);\n\tqp->sq.cur_edge = (unlikely(seg == cur_edge)) ?\n\t\t\t  get_sq_edge(&qp->sq, qp->sq.cur_post &\n\t\t\t\t      (qp->sq.wqe_cnt - 1)) :\n\t\t\t  cur_edge;\n}\n\nstatic void handle_rdma_op(const struct ib_send_wr *wr, void **seg, int *size)\n{\n\tset_raddr_seg(*seg, rdma_wr(wr)->remote_addr, rdma_wr(wr)->rkey);\n\t*seg += sizeof(struct mlx5_wqe_raddr_seg);\n\t*size += sizeof(struct mlx5_wqe_raddr_seg) / 16;\n}\n\nstatic void handle_local_inv(struct mlx5_ib_qp *qp, const struct ib_send_wr *wr,\n\t\t\t     struct mlx5_wqe_ctrl_seg **ctrl, void **seg,\n\t\t\t     int *size, void **cur_edge, unsigned int idx)\n{\n\tqp->sq.wr_data[idx] = IB_WR_LOCAL_INV;\n\t(*ctrl)->imm = cpu_to_be32(wr->ex.invalidate_rkey);\n\tset_linv_wr(qp, seg, size, cur_edge);\n}\n\nstatic int handle_reg_mr(struct mlx5_ib_qp *qp, const struct ib_send_wr *wr,\n\t\t\t struct mlx5_wqe_ctrl_seg **ctrl, void **seg, int *size,\n\t\t\t void **cur_edge, unsigned int idx)\n{\n\tqp->sq.wr_data[idx] = IB_WR_REG_MR;\n\t(*ctrl)->imm = cpu_to_be32(reg_wr(wr)->key);\n\treturn set_reg_wr(qp, reg_wr(wr), seg, size, cur_edge, true);\n}\n\nstatic int handle_psv(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,\n\t\t      const struct ib_send_wr *wr,\n\t\t      struct mlx5_wqe_ctrl_seg **ctrl, void **seg, int *size,\n\t\t      void **cur_edge, unsigned int *idx, int nreq,\n\t\t      struct ib_sig_domain *domain, u32 psv_index,\n\t\t      u8 next_fence)\n{\n\tint err;\n\n\t \n\terr = mlx5r_begin_wqe(qp, seg, ctrl, idx, size, cur_edge, nreq,\n\t\t\t      send_ieth(wr), false, true);\n\tif (unlikely(err)) {\n\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\terr = set_psv_wr(domain, psv_index, seg, size);\n\tif (unlikely(err)) {\n\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\tgoto out;\n\t}\n\tmlx5r_finish_wqe(qp, *ctrl, *seg, *size, *cur_edge, *idx, wr->wr_id,\n\t\t\t nreq, next_fence, MLX5_OPCODE_SET_PSV);\n\nout:\n\treturn err;\n}\n\nstatic int handle_reg_mr_integrity(struct mlx5_ib_dev *dev,\n\t\t\t\t   struct mlx5_ib_qp *qp,\n\t\t\t\t   const struct ib_send_wr *wr,\n\t\t\t\t   struct mlx5_wqe_ctrl_seg **ctrl, void **seg,\n\t\t\t\t   int *size, void **cur_edge,\n\t\t\t\t   unsigned int *idx, int nreq, u8 fence,\n\t\t\t\t   u8 next_fence)\n{\n\tstruct mlx5_ib_mr *mr;\n\tstruct mlx5_ib_mr *pi_mr;\n\tstruct mlx5_ib_mr pa_pi_mr;\n\tstruct ib_sig_attrs *sig_attrs;\n\tstruct ib_reg_wr reg_pi_wr;\n\tint err;\n\n\tqp->sq.wr_data[*idx] = IB_WR_REG_MR_INTEGRITY;\n\n\tmr = to_mmr(reg_wr(wr)->mr);\n\tpi_mr = mr->pi_mr;\n\n\tif (pi_mr) {\n\t\tmemset(&reg_pi_wr, 0,\n\t\t       sizeof(struct ib_reg_wr));\n\n\t\treg_pi_wr.mr = &pi_mr->ibmr;\n\t\treg_pi_wr.access = reg_wr(wr)->access;\n\t\treg_pi_wr.key = pi_mr->ibmr.rkey;\n\n\t\t(*ctrl)->imm = cpu_to_be32(reg_pi_wr.key);\n\t\t \n\t\terr = set_reg_wr(qp, &reg_pi_wr, seg, size, cur_edge, false);\n\t\tif (unlikely(err))\n\t\t\tgoto out;\n\n\t\tmlx5r_finish_wqe(qp, *ctrl, *seg, *size, *cur_edge, *idx,\n\t\t\t\t wr->wr_id, nreq, fence, MLX5_OPCODE_UMR);\n\n\t\terr = begin_wqe(qp, seg, ctrl, wr, idx, size, cur_edge, nreq);\n\t\tif (unlikely(err)) {\n\t\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tmemset(&pa_pi_mr, 0, sizeof(struct mlx5_ib_mr));\n\t\t \n\t\tpa_pi_mr.ibmr.lkey = mr->ibmr.pd->local_dma_lkey;\n\t\tpa_pi_mr.mmkey.ndescs = mr->mmkey.ndescs;\n\t\tpa_pi_mr.data_length = mr->data_length;\n\t\tpa_pi_mr.data_iova = mr->data_iova;\n\t\tif (mr->meta_ndescs) {\n\t\t\tpa_pi_mr.meta_ndescs = mr->meta_ndescs;\n\t\t\tpa_pi_mr.meta_length = mr->meta_length;\n\t\t\tpa_pi_mr.pi_iova = mr->pi_iova;\n\t\t}\n\n\t\tpa_pi_mr.ibmr.length = mr->ibmr.length;\n\t\tmr->pi_mr = &pa_pi_mr;\n\t}\n\t(*ctrl)->imm = cpu_to_be32(mr->ibmr.rkey);\n\t \n\terr = set_pi_umr_wr(wr, qp, seg, size, cur_edge);\n\tif (unlikely(err)) {\n\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\tgoto out;\n\t}\n\tmlx5r_finish_wqe(qp, *ctrl, *seg, *size, *cur_edge, *idx, wr->wr_id,\n\t\t\t nreq, fence, MLX5_OPCODE_UMR);\n\n\tsig_attrs = mr->ibmr.sig_attrs;\n\terr = handle_psv(dev, qp, wr, ctrl, seg, size, cur_edge, idx, nreq,\n\t\t\t &sig_attrs->mem, mr->sig->psv_memory.psv_idx,\n\t\t\t next_fence);\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = handle_psv(dev, qp, wr, ctrl, seg, size, cur_edge, idx, nreq,\n\t\t\t &sig_attrs->wire, mr->sig->psv_wire.psv_idx,\n\t\t\t next_fence);\n\tif (unlikely(err))\n\t\tgoto out;\n\n\tqp->next_fence = MLX5_FENCE_MODE_INITIATOR_SMALL;\n\nout:\n\treturn err;\n}\n\nstatic int handle_qpt_rc(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,\n\t\t\t const struct ib_send_wr *wr,\n\t\t\t struct mlx5_wqe_ctrl_seg **ctrl, void **seg, int *size,\n\t\t\t void **cur_edge, unsigned int *idx, int nreq, u8 fence,\n\t\t\t u8 next_fence, int *num_sge)\n{\n\tint err = 0;\n\n\tswitch (wr->opcode) {\n\tcase IB_WR_RDMA_READ:\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\thandle_rdma_op(wr, seg, size);\n\t\tbreak;\n\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\tcase IB_WR_MASKED_ATOMIC_CMP_AND_SWP:\n\t\tmlx5_ib_warn(dev, \"Atomic operations are not supported yet\\n\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\n\tcase IB_WR_LOCAL_INV:\n\t\thandle_local_inv(qp, wr, ctrl, seg, size, cur_edge, *idx);\n\t\t*num_sge = 0;\n\t\tbreak;\n\n\tcase IB_WR_REG_MR:\n\t\terr = handle_reg_mr(qp, wr, ctrl, seg, size, cur_edge, *idx);\n\t\tif (unlikely(err))\n\t\t\tgoto out;\n\t\t*num_sge = 0;\n\t\tbreak;\n\n\tcase IB_WR_REG_MR_INTEGRITY:\n\t\terr = handle_reg_mr_integrity(dev, qp, wr, ctrl, seg, size,\n\t\t\t\t\t      cur_edge, idx, nreq, fence,\n\t\t\t\t\t      next_fence);\n\t\tif (unlikely(err))\n\t\t\tgoto out;\n\t\t*num_sge = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\nout:\n\treturn err;\n}\n\nstatic void handle_qpt_uc(const struct ib_send_wr *wr, void **seg, int *size)\n{\n\tswitch (wr->opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\thandle_rdma_op(wr, seg, size);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void handle_qpt_hw_gsi(struct mlx5_ib_qp *qp,\n\t\t\t      const struct ib_send_wr *wr, void **seg,\n\t\t\t      int *size, void **cur_edge)\n{\n\tset_datagram_seg(*seg, wr);\n\t*seg += sizeof(struct mlx5_wqe_datagram_seg);\n\t*size += sizeof(struct mlx5_wqe_datagram_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n}\n\nstatic void handle_qpt_ud(struct mlx5_ib_qp *qp, const struct ib_send_wr *wr,\n\t\t\t  void **seg, int *size, void **cur_edge)\n{\n\tset_datagram_seg(*seg, wr);\n\t*seg += sizeof(struct mlx5_wqe_datagram_seg);\n\t*size += sizeof(struct mlx5_wqe_datagram_seg) / 16;\n\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\n\t \n\tif (qp->flags & IB_QP_CREATE_IPOIB_UD_LSO) {\n\t\tstruct mlx5_wqe_eth_pad *pad;\n\n\t\tpad = *seg;\n\t\tmemset(pad, 0, sizeof(struct mlx5_wqe_eth_pad));\n\t\t*seg += sizeof(struct mlx5_wqe_eth_pad);\n\t\t*size += sizeof(struct mlx5_wqe_eth_pad) / 16;\n\t\tset_eth_seg(wr, qp, seg, size, cur_edge);\n\t\thandle_post_send_edge(&qp->sq, seg, *size, cur_edge);\n\t}\n}\n\nvoid mlx5r_ring_db(struct mlx5_ib_qp *qp, unsigned int nreq,\n\t\t   struct mlx5_wqe_ctrl_seg *ctrl)\n{\n\tstruct mlx5_bf *bf = &qp->bf;\n\n\tqp->sq.head += nreq;\n\n\t \n\twmb();\n\n\tqp->db.db[MLX5_SND_DBR] = cpu_to_be32(qp->sq.cur_post);\n\n\t \n\twmb();\n\n\tmlx5_write64((__be32 *)ctrl, bf->bfreg->map + bf->offset);\n\t \n\tbf->offset ^= bf->buf_size;\n}\n\nint mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t      const struct ib_send_wr **bad_wr, bool drain)\n{\n\tstruct mlx5_wqe_ctrl_seg *ctrl = NULL;   \n\tstruct mlx5_ib_dev *dev = to_mdev(ibqp->device);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tstruct mlx5_ib_qp *qp = to_mqp(ibqp);\n\tstruct mlx5_wqe_xrc_seg *xrc;\n\tvoid *cur_edge;\n\tint size;\n\tunsigned long flags;\n\tunsigned int idx;\n\tint err = 0;\n\tint num_sge;\n\tvoid *seg;\n\tint nreq;\n\tint i;\n\tu8 next_fence = 0;\n\tu8 fence;\n\n\tif (unlikely(mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR &&\n\t\t     !drain)) {\n\t\t*bad_wr = wr;\n\t\treturn -EIO;\n\t}\n\n\tif (qp->type == IB_QPT_GSI)\n\t\treturn mlx5_ib_gsi_post_send(ibqp, wr, bad_wr);\n\n\tspin_lock_irqsave(&qp->sq.lock, flags);\n\n\tfor (nreq = 0; wr; nreq++, wr = wr->next) {\n\t\tif (unlikely(wr->opcode >= ARRAY_SIZE(mlx5_ib_opcode))) {\n\t\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnum_sge = wr->num_sge;\n\t\tif (unlikely(num_sge > qp->sq.max_gs)) {\n\t\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = begin_wqe(qp, &seg, &ctrl, wr, &idx, &size, &cur_edge,\n\t\t\t\tnreq);\n\t\tif (err) {\n\t\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (wr->opcode == IB_WR_REG_MR ||\n\t\t    wr->opcode == IB_WR_REG_MR_INTEGRITY) {\n\t\t\tfence = dev->umr_fence;\n\t\t\tnext_fence = MLX5_FENCE_MODE_INITIATOR_SMALL;\n\t\t} else  {\n\t\t\tif (wr->send_flags & IB_SEND_FENCE) {\n\t\t\t\tif (qp->next_fence)\n\t\t\t\t\tfence = MLX5_FENCE_MODE_SMALL_AND_FENCE;\n\t\t\t\telse\n\t\t\t\t\tfence = MLX5_FENCE_MODE_FENCE;\n\t\t\t} else {\n\t\t\t\tfence = qp->next_fence;\n\t\t\t}\n\t\t}\n\n\t\tswitch (qp->type) {\n\t\tcase IB_QPT_XRC_INI:\n\t\t\txrc = seg;\n\t\t\tseg += sizeof(*xrc);\n\t\t\tsize += sizeof(*xrc) / 16;\n\t\t\tfallthrough;\n\t\tcase IB_QPT_RC:\n\t\t\terr = handle_qpt_rc(dev, qp, wr, &ctrl, &seg, &size,\n\t\t\t\t\t    &cur_edge, &idx, nreq, fence,\n\t\t\t\t\t    next_fence, &num_sge);\n\t\t\tif (unlikely(err)) {\n\t\t\t\t*bad_wr = wr;\n\t\t\t\tgoto out;\n\t\t\t} else if (wr->opcode == IB_WR_REG_MR_INTEGRITY) {\n\t\t\t\tgoto skip_psv;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase IB_QPT_UC:\n\t\t\thandle_qpt_uc(wr, &seg, &size);\n\t\t\tbreak;\n\t\tcase IB_QPT_SMI:\n\t\t\tif (unlikely(!dev->port_caps[qp->port - 1].has_smi)) {\n\t\t\t\tmlx5_ib_warn(dev, \"Send SMP MADs is not allowed\\n\");\n\t\t\t\terr = -EPERM;\n\t\t\t\t*bad_wr = wr;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfallthrough;\n\t\tcase MLX5_IB_QPT_HW_GSI:\n\t\t\thandle_qpt_hw_gsi(qp, wr, &seg, &size, &cur_edge);\n\t\t\tbreak;\n\t\tcase IB_QPT_UD:\n\t\t\thandle_qpt_ud(qp, wr, &seg, &size, &cur_edge);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (wr->send_flags & IB_SEND_INLINE && num_sge) {\n\t\t\terr = set_data_inl_seg(qp, wr, &seg, &size, &cur_edge);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tmlx5_ib_warn(dev, \"\\n\");\n\t\t\t\t*bad_wr = wr;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tfor (i = 0; i < num_sge; i++) {\n\t\t\t\thandle_post_send_edge(&qp->sq, &seg, size,\n\t\t\t\t\t\t      &cur_edge);\n\t\t\t\tif (unlikely(!wr->sg_list[i].length))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tset_data_ptr_seg(\n\t\t\t\t\t(struct mlx5_wqe_data_seg *)seg,\n\t\t\t\t\twr->sg_list + i);\n\t\t\t\tsize += sizeof(struct mlx5_wqe_data_seg) / 16;\n\t\t\t\tseg += sizeof(struct mlx5_wqe_data_seg);\n\t\t\t}\n\t\t}\n\n\t\tqp->next_fence = next_fence;\n\t\tmlx5r_finish_wqe(qp, ctrl, seg, size, cur_edge, idx, wr->wr_id,\n\t\t\t\t nreq, fence, mlx5_ib_opcode[wr->opcode]);\nskip_psv:\n\t\tif (0)\n\t\t\tdump_wqe(qp, idx, size);\n\t}\n\nout:\n\tif (likely(nreq))\n\t\tmlx5r_ring_db(qp, nreq, ctrl);\n\n\tspin_unlock_irqrestore(&qp->sq.lock, flags);\n\n\treturn err;\n}\n\nstatic void set_sig_seg(struct mlx5_rwqe_sig *sig, int max_gs)\n{\n\t sig->signature = calc_sig(sig, (max_gs + 1) << 2);\n}\n\nint mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr, bool drain)\n{\n\tstruct mlx5_ib_qp *qp = to_mqp(ibqp);\n\tstruct mlx5_wqe_data_seg *scat;\n\tstruct mlx5_rwqe_sig *sig;\n\tstruct mlx5_ib_dev *dev = to_mdev(ibqp->device);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tunsigned long flags;\n\tint err = 0;\n\tint nreq;\n\tint ind;\n\tint i;\n\n\tif (unlikely(mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR &&\n\t\t     !drain)) {\n\t\t*bad_wr = wr;\n\t\treturn -EIO;\n\t}\n\n\tif (qp->type == IB_QPT_GSI)\n\t\treturn mlx5_ib_gsi_post_recv(ibqp, wr, bad_wr);\n\n\tspin_lock_irqsave(&qp->rq.lock, flags);\n\n\tind = qp->rq.head & (qp->rq.wqe_cnt - 1);\n\n\tfor (nreq = 0; wr; nreq++, wr = wr->next) {\n\t\tif (mlx5r_wq_overflow(&qp->rq, nreq, qp->ibqp.recv_cq)) {\n\t\t\terr = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (unlikely(wr->num_sge > qp->rq.max_gs)) {\n\t\t\terr = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tscat = mlx5_frag_buf_get_wqe(&qp->rq.fbc, ind);\n\t\tif (qp->flags_en & MLX5_QP_FLAG_SIGNATURE)\n\t\t\tscat++;\n\n\t\tfor (i = 0; i < wr->num_sge; i++)\n\t\t\tset_data_ptr_seg(scat + i, wr->sg_list + i);\n\n\t\tif (i < qp->rq.max_gs) {\n\t\t\tscat[i].byte_count = 0;\n\t\t\tscat[i].lkey = dev->mkeys.terminate_scatter_list_mkey;\n\t\t\tscat[i].addr       = 0;\n\t\t}\n\n\t\tif (qp->flags_en & MLX5_QP_FLAG_SIGNATURE) {\n\t\t\tsig = (struct mlx5_rwqe_sig *)scat;\n\t\t\tset_sig_seg(sig, qp->rq.max_gs);\n\t\t}\n\n\t\tqp->rq.wrid[ind] = wr->wr_id;\n\n\t\tind = (ind + 1) & (qp->rq.wqe_cnt - 1);\n\t}\n\nout:\n\tif (likely(nreq)) {\n\t\tqp->rq.head += nreq;\n\n\t\t \n\t\twmb();\n\n\t\t*qp->db.db = cpu_to_be32(qp->rq.head & 0xffff);\n\t}\n\n\tspin_unlock_irqrestore(&qp->rq.lock, flags);\n\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}