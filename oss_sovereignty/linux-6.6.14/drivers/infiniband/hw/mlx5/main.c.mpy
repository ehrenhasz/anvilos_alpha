{
  "module_name": "main.c",
  "hash_id": "3949cc777e58fbc11b53373daabbdb69bb7cd05b1534e79bc75a8756cd55c73d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/main.c",
  "human_readable_source": "\n \n\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/bitmap.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/delay.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n#include <linux/mlx5/port.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/fs.h>\n#include <linux/mlx5/eswitch.h>\n#include <linux/mlx5/driver.h>\n#include <linux/list.h>\n#include <rdma/ib_smi.h>\n#include <rdma/ib_umem_odp.h>\n#include <rdma/lag.h>\n#include <linux/in.h>\n#include <linux/etherdevice.h>\n#include \"mlx5_ib.h\"\n#include \"ib_rep.h\"\n#include \"cmd.h\"\n#include \"devx.h\"\n#include \"dm.h\"\n#include \"fs.h\"\n#include \"srq.h\"\n#include \"qp.h\"\n#include \"wr.h\"\n#include \"restrack.h\"\n#include \"counters.h\"\n#include \"umr.h\"\n#include <rdma/uverbs_std_types.h>\n#include <rdma/uverbs_ioctl.h>\n#include <rdma/mlx5_user_ioctl_verbs.h>\n#include <rdma/mlx5_user_ioctl_cmds.h>\n#include \"macsec.h\"\n\n#define UVERBS_MODULE_NAME mlx5_ib\n#include <rdma/uverbs_named_ioctl.h>\n\nMODULE_AUTHOR(\"Eli Cohen <eli@mellanox.com>\");\nMODULE_DESCRIPTION(\"Mellanox 5th generation network adapters (ConnectX series) IB driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nstruct mlx5_ib_event_work {\n\tstruct work_struct\twork;\n\tunion {\n\t\tstruct mlx5_ib_dev\t      *dev;\n\t\tstruct mlx5_ib_multiport_info *mpi;\n\t};\n\tbool\t\t\tis_slave;\n\tunsigned int\t\tevent;\n\tvoid\t\t\t*param;\n};\n\nenum {\n\tMLX5_ATOMIC_SIZE_QP_8BYTES = 1 << 3,\n};\n\nstatic struct workqueue_struct *mlx5_ib_event_wq;\nstatic LIST_HEAD(mlx5_ib_unaffiliated_port_list);\nstatic LIST_HEAD(mlx5_ib_dev_list);\n \nstatic DEFINE_MUTEX(mlx5_ib_multiport_mutex);\n\nstruct mlx5_ib_dev *mlx5_ib_get_ibdev_from_mpi(struct mlx5_ib_multiport_info *mpi)\n{\n\tstruct mlx5_ib_dev *dev;\n\n\tmutex_lock(&mlx5_ib_multiport_mutex);\n\tdev = mpi->ibdev;\n\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\treturn dev;\n}\n\nstatic enum rdma_link_layer\nmlx5_port_type_cap_to_rdma_ll(int port_type_cap)\n{\n\tswitch (port_type_cap) {\n\tcase MLX5_CAP_PORT_TYPE_IB:\n\t\treturn IB_LINK_LAYER_INFINIBAND;\n\tcase MLX5_CAP_PORT_TYPE_ETH:\n\t\treturn IB_LINK_LAYER_ETHERNET;\n\tdefault:\n\t\treturn IB_LINK_LAYER_UNSPECIFIED;\n\t}\n}\n\nstatic enum rdma_link_layer\nmlx5_ib_port_link_layer(struct ib_device *device, u32 port_num)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(device);\n\tint port_type_cap = MLX5_CAP_GEN(dev->mdev, port_type);\n\n\treturn mlx5_port_type_cap_to_rdma_ll(port_type_cap);\n}\n\nstatic int get_port_state(struct ib_device *ibdev,\n\t\t\t  u32 port_num,\n\t\t\t  enum ib_port_state *state)\n{\n\tstruct ib_port_attr attr;\n\tint ret;\n\n\tmemset(&attr, 0, sizeof(attr));\n\tret = ibdev->ops.query_port(ibdev, port_num, &attr);\n\tif (!ret)\n\t\t*state = attr.state;\n\treturn ret;\n}\n\nstatic struct mlx5_roce *mlx5_get_rep_roce(struct mlx5_ib_dev *dev,\n\t\t\t\t\t   struct net_device *ndev,\n\t\t\t\t\t   struct net_device *upper,\n\t\t\t\t\t   u32 *port_num)\n{\n\tstruct net_device *rep_ndev;\n\tstruct mlx5_ib_port *port;\n\tint i;\n\n\tfor (i = 0; i < dev->num_ports; i++) {\n\t\tport  = &dev->port[i];\n\t\tif (!port->rep)\n\t\t\tcontinue;\n\n\t\tif (upper == ndev && port->rep->vport == MLX5_VPORT_UPLINK) {\n\t\t\t*port_num = i + 1;\n\t\t\treturn &port->roce;\n\t\t}\n\n\t\tif (upper && port->rep->vport == MLX5_VPORT_UPLINK)\n\t\t\tcontinue;\n\n\t\tread_lock(&port->roce.netdev_lock);\n\t\trep_ndev = mlx5_ib_get_rep_netdev(port->rep->esw,\n\t\t\t\t\t\t  port->rep->vport);\n\t\tif (rep_ndev == ndev) {\n\t\t\tread_unlock(&port->roce.netdev_lock);\n\t\t\t*port_num = i + 1;\n\t\t\treturn &port->roce;\n\t\t}\n\t\tread_unlock(&port->roce.netdev_lock);\n\t}\n\n\treturn NULL;\n}\n\nstatic int mlx5_netdev_event(struct notifier_block *this,\n\t\t\t     unsigned long event, void *ptr)\n{\n\tstruct mlx5_roce *roce = container_of(this, struct mlx5_roce, nb);\n\tstruct net_device *ndev = netdev_notifier_info_to_dev(ptr);\n\tu32 port_num = roce->native_port_num;\n\tstruct mlx5_core_dev *mdev;\n\tstruct mlx5_ib_dev *ibdev;\n\n\tibdev = roce->dev;\n\tmdev = mlx5_ib_get_native_port_mdev(ibdev, port_num, NULL);\n\tif (!mdev)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (event) {\n\tcase NETDEV_REGISTER:\n\t\t \n\t\tif (ibdev->is_rep)\n\t\t\tbreak;\n\t\twrite_lock(&roce->netdev_lock);\n\t\tif (ndev->dev.parent == mdev->device)\n\t\t\troce->netdev = ndev;\n\t\twrite_unlock(&roce->netdev_lock);\n\t\tbreak;\n\n\tcase NETDEV_UNREGISTER:\n\t\t \n\t\twrite_lock(&roce->netdev_lock);\n\t\tif (roce->netdev == ndev)\n\t\t\troce->netdev = NULL;\n\t\twrite_unlock(&roce->netdev_lock);\n\t\tbreak;\n\n\tcase NETDEV_CHANGE:\n\tcase NETDEV_UP:\n\tcase NETDEV_DOWN: {\n\t\tstruct net_device *lag_ndev = mlx5_lag_get_roce_netdev(mdev);\n\t\tstruct net_device *upper = NULL;\n\n\t\tif (lag_ndev) {\n\t\t\tupper = netdev_master_upper_dev_get(lag_ndev);\n\t\t\tdev_put(lag_ndev);\n\t\t}\n\n\t\tif (ibdev->is_rep)\n\t\t\troce = mlx5_get_rep_roce(ibdev, ndev, upper, &port_num);\n\t\tif (!roce)\n\t\t\treturn NOTIFY_DONE;\n\t\tif ((upper == ndev ||\n\t\t     ((!upper || ibdev->is_rep) && ndev == roce->netdev)) &&\n\t\t    ibdev->ib_active) {\n\t\t\tstruct ib_event ibev = { };\n\t\t\tenum ib_port_state port_state;\n\n\t\t\tif (get_port_state(&ibdev->ib_dev, port_num,\n\t\t\t\t\t   &port_state))\n\t\t\t\tgoto done;\n\n\t\t\tif (roce->last_port_state == port_state)\n\t\t\t\tgoto done;\n\n\t\t\troce->last_port_state = port_state;\n\t\t\tibev.device = &ibdev->ib_dev;\n\t\t\tif (port_state == IB_PORT_DOWN)\n\t\t\t\tibev.event = IB_EVENT_PORT_ERR;\n\t\t\telse if (port_state == IB_PORT_ACTIVE)\n\t\t\t\tibev.event = IB_EVENT_PORT_ACTIVE;\n\t\t\telse\n\t\t\t\tgoto done;\n\n\t\t\tibev.element.port_num = port_num;\n\t\t\tib_dispatch_event(&ibev);\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tbreak;\n\t}\ndone:\n\tmlx5_ib_put_native_port_mdev(ibdev, port_num);\n\treturn NOTIFY_DONE;\n}\n\nstatic struct net_device *mlx5_ib_get_netdev(struct ib_device *device,\n\t\t\t\t\t     u32 port_num)\n{\n\tstruct mlx5_ib_dev *ibdev = to_mdev(device);\n\tstruct net_device *ndev;\n\tstruct mlx5_core_dev *mdev;\n\n\tmdev = mlx5_ib_get_native_port_mdev(ibdev, port_num, NULL);\n\tif (!mdev)\n\t\treturn NULL;\n\n\tndev = mlx5_lag_get_roce_netdev(mdev);\n\tif (ndev)\n\t\tgoto out;\n\n\t \n\tread_lock(&ibdev->port[port_num - 1].roce.netdev_lock);\n\tndev = ibdev->port[port_num - 1].roce.netdev;\n\tif (ndev)\n\t\tdev_hold(ndev);\n\tread_unlock(&ibdev->port[port_num - 1].roce.netdev_lock);\n\nout:\n\tmlx5_ib_put_native_port_mdev(ibdev, port_num);\n\treturn ndev;\n}\n\nstruct mlx5_core_dev *mlx5_ib_get_native_port_mdev(struct mlx5_ib_dev *ibdev,\n\t\t\t\t\t\t   u32 ib_port_num,\n\t\t\t\t\t\t   u32 *native_port_num)\n{\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(&ibdev->ib_dev,\n\t\t\t\t\t\t\t  ib_port_num);\n\tstruct mlx5_core_dev *mdev = NULL;\n\tstruct mlx5_ib_multiport_info *mpi;\n\tstruct mlx5_ib_port *port;\n\n\tif (!mlx5_core_mp_enabled(ibdev->mdev) ||\n\t    ll != IB_LINK_LAYER_ETHERNET) {\n\t\tif (native_port_num)\n\t\t\t*native_port_num = ib_port_num;\n\t\treturn ibdev->mdev;\n\t}\n\n\tif (native_port_num)\n\t\t*native_port_num = 1;\n\n\tport = &ibdev->port[ib_port_num - 1];\n\tspin_lock(&port->mp.mpi_lock);\n\tmpi = ibdev->port[ib_port_num - 1].mp.mpi;\n\tif (mpi && !mpi->unaffiliate) {\n\t\tmdev = mpi->mdev;\n\t\t \n\t\tif (!mpi->is_master)\n\t\t\tmpi->mdev_refcnt++;\n\t}\n\tspin_unlock(&port->mp.mpi_lock);\n\n\treturn mdev;\n}\n\nvoid mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *ibdev, u32 port_num)\n{\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(&ibdev->ib_dev,\n\t\t\t\t\t\t\t  port_num);\n\tstruct mlx5_ib_multiport_info *mpi;\n\tstruct mlx5_ib_port *port;\n\n\tif (!mlx5_core_mp_enabled(ibdev->mdev) || ll != IB_LINK_LAYER_ETHERNET)\n\t\treturn;\n\n\tport = &ibdev->port[port_num - 1];\n\n\tspin_lock(&port->mp.mpi_lock);\n\tmpi = ibdev->port[port_num - 1].mp.mpi;\n\tif (mpi->is_master)\n\t\tgoto out;\n\n\tmpi->mdev_refcnt--;\n\tif (mpi->unaffiliate)\n\t\tcomplete(&mpi->unref_comp);\nout:\n\tspin_unlock(&port->mp.mpi_lock);\n}\n\nstatic int translate_eth_legacy_proto_oper(u32 eth_proto_oper,\n\t\t\t\t\t   u16 *active_speed, u8 *active_width)\n{\n\tswitch (eth_proto_oper) {\n\tcase MLX5E_PROT_MASK(MLX5E_1000BASE_CX_SGMII):\n\tcase MLX5E_PROT_MASK(MLX5E_1000BASE_KX):\n\tcase MLX5E_PROT_MASK(MLX5E_100BASE_TX):\n\tcase MLX5E_PROT_MASK(MLX5E_1000BASE_T):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_SDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_T):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_CX4):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_KX4):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_KR):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_CR):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_SR):\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_ER):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_QDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_25GBASE_CR):\n\tcase MLX5E_PROT_MASK(MLX5E_25GBASE_KR):\n\tcase MLX5E_PROT_MASK(MLX5E_25GBASE_SR):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_EDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_40GBASE_CR4):\n\tcase MLX5E_PROT_MASK(MLX5E_40GBASE_KR4):\n\tcase MLX5E_PROT_MASK(MLX5E_40GBASE_SR4):\n\tcase MLX5E_PROT_MASK(MLX5E_40GBASE_LR4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_QDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_50GBASE_CR2):\n\tcase MLX5E_PROT_MASK(MLX5E_50GBASE_KR2):\n\tcase MLX5E_PROT_MASK(MLX5E_50GBASE_SR2):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_HDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_56GBASE_R4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_FDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_100GBASE_CR4):\n\tcase MLX5E_PROT_MASK(MLX5E_100GBASE_SR4):\n\tcase MLX5E_PROT_MASK(MLX5E_100GBASE_KR4):\n\tcase MLX5E_PROT_MASK(MLX5E_100GBASE_LR4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_EDR;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int translate_eth_ext_proto_oper(u32 eth_proto_oper, u16 *active_speed,\n\t\t\t\t\tu8 *active_width)\n{\n\tswitch (eth_proto_oper) {\n\tcase MLX5E_PROT_MASK(MLX5E_SGMII_100M):\n\tcase MLX5E_PROT_MASK(MLX5E_1000BASE_X_SGMII):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_SDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_5GBASE_R):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_DDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_10GBASE_XFI_XAUI_1):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_QDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_40GBASE_XLAUI_4_XLPPI_4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_QDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_25GAUI_1_25GBASE_CR_KR):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_EDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_50GAUI_2_LAUI_2_50GBASE_CR2_KR2):\n\t\t*active_width = IB_WIDTH_2X;\n\t\t*active_speed = IB_SPEED_EDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_50GAUI_1_LAUI_1_50GBASE_CR_KR):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_HDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_CAUI_4_100GBASE_CR4_KR4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_EDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_100GAUI_2_100GBASE_CR2_KR2):\n\t\t*active_width = IB_WIDTH_2X;\n\t\t*active_speed = IB_SPEED_HDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_100GAUI_1_100GBASE_CR_KR):\n\t\t*active_width = IB_WIDTH_1X;\n\t\t*active_speed = IB_SPEED_NDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_200GAUI_4_200GBASE_CR4_KR4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_HDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_200GAUI_2_200GBASE_CR2_KR2):\n\t\t*active_width = IB_WIDTH_2X;\n\t\t*active_speed = IB_SPEED_NDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_400GAUI_8):\n\t\t*active_width = IB_WIDTH_8X;\n\t\t*active_speed = IB_SPEED_HDR;\n\t\tbreak;\n\tcase MLX5E_PROT_MASK(MLX5E_400GAUI_4_400GBASE_CR4_KR4):\n\t\t*active_width = IB_WIDTH_4X;\n\t\t*active_speed = IB_SPEED_NDR;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int translate_eth_proto_oper(u32 eth_proto_oper, u16 *active_speed,\n\t\t\t\t    u8 *active_width, bool ext)\n{\n\treturn ext ?\n\t\ttranslate_eth_ext_proto_oper(eth_proto_oper, active_speed,\n\t\t\t\t\t     active_width) :\n\t\ttranslate_eth_legacy_proto_oper(eth_proto_oper, active_speed,\n\t\t\t\t\t\tactive_width);\n}\n\nstatic int mlx5_query_port_roce(struct ib_device *device, u32 port_num,\n\t\t\t\tstruct ib_port_attr *props)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(device);\n\tu32 out[MLX5_ST_SZ_DW(ptys_reg)] = {0};\n\tstruct mlx5_core_dev *mdev;\n\tstruct net_device *ndev, *upper;\n\tenum ib_mtu ndev_ib_mtu;\n\tbool put_mdev = true;\n\tu32 eth_prot_oper;\n\tu32 mdev_port_num;\n\tbool ext;\n\tint err;\n\n\tmdev = mlx5_ib_get_native_port_mdev(dev, port_num, &mdev_port_num);\n\tif (!mdev) {\n\t\t \n\t\tput_mdev = false;\n\t\tmdev = dev->mdev;\n\t\tmdev_port_num = 1;\n\t\tport_num = 1;\n\t}\n\n\t \n\tif (dev->is_rep)\n\t\terr = mlx5_query_port_ptys(mdev, out, sizeof(out), MLX5_PTYS_EN,\n\t\t\t\t\t   1);\n\telse\n\t\terr = mlx5_query_port_ptys(mdev, out, sizeof(out), MLX5_PTYS_EN,\n\t\t\t\t\t   mdev_port_num);\n\tif (err)\n\t\tgoto out;\n\text = !!MLX5_GET_ETH_PROTO(ptys_reg, out, true, eth_proto_capability);\n\teth_prot_oper = MLX5_GET_ETH_PROTO(ptys_reg, out, ext, eth_proto_oper);\n\n\tprops->active_width     = IB_WIDTH_4X;\n\tprops->active_speed     = IB_SPEED_QDR;\n\n\ttranslate_eth_proto_oper(eth_prot_oper, &props->active_speed,\n\t\t\t\t &props->active_width, ext);\n\n\tif (!dev->is_rep && dev->mdev->roce.roce_en) {\n\t\tu16 qkey_viol_cntr;\n\n\t\tprops->port_cap_flags |= IB_PORT_CM_SUP;\n\t\tprops->ip_gids = true;\n\t\tprops->gid_tbl_len = MLX5_CAP_ROCE(dev->mdev,\n\t\t\t\t\t\t   roce_address_table_size);\n\t\tmlx5_query_nic_vport_qkey_viol_cntr(mdev, &qkey_viol_cntr);\n\t\tprops->qkey_viol_cntr = qkey_viol_cntr;\n\t}\n\tprops->max_mtu          = IB_MTU_4096;\n\tprops->max_msg_sz       = 1 << MLX5_CAP_GEN(dev->mdev, log_max_msg);\n\tprops->pkey_tbl_len     = 1;\n\tprops->state            = IB_PORT_DOWN;\n\tprops->phys_state       = IB_PORT_PHYS_STATE_DISABLED;\n\n\t \n\tif (!put_mdev)\n\t\tgoto out;\n\n\tndev = mlx5_ib_get_netdev(device, port_num);\n\tif (!ndev)\n\t\tgoto out;\n\n\tif (dev->lag_active) {\n\t\trcu_read_lock();\n\t\tupper = netdev_master_upper_dev_get_rcu(ndev);\n\t\tif (upper) {\n\t\t\tdev_put(ndev);\n\t\t\tndev = upper;\n\t\t\tdev_hold(ndev);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (netif_running(ndev) && netif_carrier_ok(ndev)) {\n\t\tprops->state      = IB_PORT_ACTIVE;\n\t\tprops->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t}\n\n\tndev_ib_mtu = iboe_get_mtu(ndev->mtu);\n\n\tdev_put(ndev);\n\n\tprops->active_mtu\t= min(props->max_mtu, ndev_ib_mtu);\nout:\n\tif (put_mdev)\n\t\tmlx5_ib_put_native_port_mdev(dev, port_num);\n\treturn err;\n}\n\nint set_roce_addr(struct mlx5_ib_dev *dev, u32 port_num,\n\t\t  unsigned int index, const union ib_gid *gid,\n\t\t  const struct ib_gid_attr *attr)\n{\n\tenum ib_gid_type gid_type;\n\tu16 vlan_id = 0xffff;\n\tu8 roce_version = 0;\n\tu8 roce_l3_type = 0;\n\tu8 mac[ETH_ALEN];\n\tint ret;\n\n\tgid_type = attr->gid_type;\n\tif (gid) {\n\t\tret = rdma_read_gid_l2_fields(attr, &vlan_id, &mac[0]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (gid_type) {\n\tcase IB_GID_TYPE_ROCE:\n\t\troce_version = MLX5_ROCE_VERSION_1;\n\t\tbreak;\n\tcase IB_GID_TYPE_ROCE_UDP_ENCAP:\n\t\troce_version = MLX5_ROCE_VERSION_2;\n\t\tif (gid && ipv6_addr_v4mapped((void *)gid))\n\t\t\troce_l3_type = MLX5_ROCE_L3_TYPE_IPV4;\n\t\telse\n\t\t\troce_l3_type = MLX5_ROCE_L3_TYPE_IPV6;\n\t\tbreak;\n\n\tdefault:\n\t\tmlx5_ib_warn(dev, \"Unexpected GID type %u\\n\", gid_type);\n\t}\n\n\treturn mlx5_core_roce_gid_set(dev->mdev, index, roce_version,\n\t\t\t\t      roce_l3_type, gid->raw, mac,\n\t\t\t\t      vlan_id < VLAN_CFI_MASK, vlan_id,\n\t\t\t\t      port_num);\n}\n\nstatic int mlx5_ib_add_gid(const struct ib_gid_attr *attr,\n\t\t\t   __always_unused void **context)\n{\n\tint ret;\n\n\tret = mlx5r_add_gid_macsec_operations(attr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn set_roce_addr(to_mdev(attr->device), attr->port_num,\n\t\t\t     attr->index, &attr->gid, attr);\n}\n\nstatic int mlx5_ib_del_gid(const struct ib_gid_attr *attr,\n\t\t\t   __always_unused void **context)\n{\n\tint ret;\n\n\tret = set_roce_addr(to_mdev(attr->device), attr->port_num,\n\t\t\t    attr->index, NULL, attr);\n\tif (ret)\n\t\treturn ret;\n\n\tmlx5r_del_gid_macsec_operations(attr);\n\treturn 0;\n}\n\n__be16 mlx5_get_roce_udp_sport_min(const struct mlx5_ib_dev *dev,\n\t\t\t\t   const struct ib_gid_attr *attr)\n{\n\tif (attr->gid_type != IB_GID_TYPE_ROCE_UDP_ENCAP)\n\t\treturn 0;\n\n\treturn cpu_to_be16(MLX5_CAP_ROCE(dev->mdev, r_roce_min_src_udp_port));\n}\n\nstatic int mlx5_use_mad_ifc(struct mlx5_ib_dev *dev)\n{\n\tif (MLX5_CAP_GEN(dev->mdev, port_type) == MLX5_CAP_PORT_TYPE_IB)\n\t\treturn !MLX5_CAP_GEN(dev->mdev, ib_virt);\n\treturn 0;\n}\n\nenum {\n\tMLX5_VPORT_ACCESS_METHOD_MAD,\n\tMLX5_VPORT_ACCESS_METHOD_HCA,\n\tMLX5_VPORT_ACCESS_METHOD_NIC,\n};\n\nstatic int mlx5_get_vport_access_method(struct ib_device *ibdev)\n{\n\tif (mlx5_use_mad_ifc(to_mdev(ibdev)))\n\t\treturn MLX5_VPORT_ACCESS_METHOD_MAD;\n\n\tif (mlx5_ib_port_link_layer(ibdev, 1) ==\n\t    IB_LINK_LAYER_ETHERNET)\n\t\treturn MLX5_VPORT_ACCESS_METHOD_NIC;\n\n\treturn MLX5_VPORT_ACCESS_METHOD_HCA;\n}\n\nstatic void get_atomic_caps(struct mlx5_ib_dev *dev,\n\t\t\t    u8 atomic_size_qp,\n\t\t\t    struct ib_device_attr *props)\n{\n\tu8 tmp;\n\tu8 atomic_operations = MLX5_CAP_ATOMIC(dev->mdev, atomic_operations);\n\tu8 atomic_req_8B_endianness_mode =\n\t\tMLX5_CAP_ATOMIC(dev->mdev, atomic_req_8B_endianness_mode);\n\n\t \n\ttmp = MLX5_ATOMIC_OPS_CMP_SWAP | MLX5_ATOMIC_OPS_FETCH_ADD;\n\tif (((atomic_operations & tmp) == tmp) &&\n\t    (atomic_size_qp & MLX5_ATOMIC_SIZE_QP_8BYTES) &&\n\t    (atomic_req_8B_endianness_mode)) {\n\t\tprops->atomic_cap = IB_ATOMIC_HCA;\n\t} else {\n\t\tprops->atomic_cap = IB_ATOMIC_NONE;\n\t}\n}\n\nstatic void get_atomic_caps_qp(struct mlx5_ib_dev *dev,\n\t\t\t       struct ib_device_attr *props)\n{\n\tu8 atomic_size_qp = MLX5_CAP_ATOMIC(dev->mdev, atomic_size_qp);\n\n\tget_atomic_caps(dev, atomic_size_qp, props);\n}\n\nstatic int mlx5_query_system_image_guid(struct ib_device *ibdev,\n\t\t\t\t\t__be64 *sys_image_guid)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tu64 tmp;\n\tint err;\n\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_system_image_guid(ibdev,\n\t\t\t\t\t\t\t    sys_image_guid);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\t\terr = mlx5_query_hca_vport_system_image_guid(mdev, &tmp);\n\t\tbreak;\n\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\terr = mlx5_query_nic_vport_system_image_guid(mdev, &tmp);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!err)\n\t\t*sys_image_guid = cpu_to_be64(tmp);\n\n\treturn err;\n\n}\n\nstatic int mlx5_query_max_pkeys(struct ib_device *ibdev,\n\t\t\t\tu16 *max_pkeys)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_max_pkeys(ibdev, max_pkeys);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\t*max_pkeys = mlx5_to_sw_pkey_sz(MLX5_CAP_GEN(mdev,\n\t\t\t\t\t\tpkey_table_size));\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int mlx5_query_vendor_id(struct ib_device *ibdev,\n\t\t\t\tu32 *vendor_id)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_vendor_id(ibdev, vendor_id);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\treturn mlx5_core_query_vendor_id(dev->mdev, vendor_id);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int mlx5_query_node_guid(struct mlx5_ib_dev *dev,\n\t\t\t\t__be64 *node_guid)\n{\n\tu64 tmp;\n\tint err;\n\n\tswitch (mlx5_get_vport_access_method(&dev->ib_dev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_node_guid(dev, node_guid);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\t\terr = mlx5_query_hca_vport_node_guid(dev->mdev, &tmp);\n\t\tbreak;\n\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\terr = mlx5_query_nic_vport_node_guid(dev->mdev, &tmp);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!err)\n\t\t*node_guid = cpu_to_be64(tmp);\n\n\treturn err;\n}\n\nstruct mlx5_reg_node_desc {\n\tu8\tdesc[IB_DEVICE_NODE_DESC_MAX];\n};\n\nstatic int mlx5_query_node_desc(struct mlx5_ib_dev *dev, char *node_desc)\n{\n\tstruct mlx5_reg_node_desc in;\n\n\tif (mlx5_use_mad_ifc(dev))\n\t\treturn mlx5_query_mad_ifc_node_desc(dev, node_desc);\n\n\tmemset(&in, 0, sizeof(in));\n\n\treturn mlx5_core_access_reg(dev->mdev, &in, sizeof(in), node_desc,\n\t\t\t\t    sizeof(struct mlx5_reg_node_desc),\n\t\t\t\t    MLX5_REG_NODE_DESC, 0, 0);\n}\n\nstatic int mlx5_ib_query_device(struct ib_device *ibdev,\n\t\t\t\tstruct ib_device_attr *props,\n\t\t\t\tstruct ib_udata *uhw)\n{\n\tsize_t uhw_outlen = (uhw) ? uhw->outlen : 0;\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tint err = -ENOMEM;\n\tint max_sq_desc;\n\tint max_rq_sg;\n\tint max_sq_sg;\n\tu64 min_page_size = 1ull << MLX5_CAP_GEN(mdev, log_pg_sz);\n\tbool raw_support = !mlx5_core_mp_enabled(mdev);\n\tstruct mlx5_ib_query_device_resp resp = {};\n\tsize_t resp_len;\n\tu64 max_tso;\n\n\tresp_len = sizeof(resp.comp_mask) + sizeof(resp.response_length);\n\tif (uhw_outlen && uhw_outlen < resp_len)\n\t\treturn -EINVAL;\n\n\tresp.response_length = resp_len;\n\n\tif (uhw && uhw->inlen && !ib_is_udata_cleared(uhw, 0, uhw->inlen))\n\t\treturn -EINVAL;\n\n\tmemset(props, 0, sizeof(*props));\n\terr = mlx5_query_system_image_guid(ibdev,\n\t\t\t\t\t   &props->sys_image_guid);\n\tif (err)\n\t\treturn err;\n\n\tprops->max_pkeys = dev->pkey_table_len;\n\n\terr = mlx5_query_vendor_id(ibdev, &props->vendor_id);\n\tif (err)\n\t\treturn err;\n\n\tprops->fw_ver = ((u64)fw_rev_maj(dev->mdev) << 32) |\n\t\t(fw_rev_min(dev->mdev) << 16) |\n\t\tfw_rev_sub(dev->mdev);\n\tprops->device_cap_flags    = IB_DEVICE_CHANGE_PHY_PORT |\n\t\tIB_DEVICE_PORT_ACTIVE_EVENT\t\t|\n\t\tIB_DEVICE_SYS_IMAGE_GUID\t\t|\n\t\tIB_DEVICE_RC_RNR_NAK_GEN;\n\n\tif (MLX5_CAP_GEN(mdev, pkv))\n\t\tprops->device_cap_flags |= IB_DEVICE_BAD_PKEY_CNTR;\n\tif (MLX5_CAP_GEN(mdev, qkv))\n\t\tprops->device_cap_flags |= IB_DEVICE_BAD_QKEY_CNTR;\n\tif (MLX5_CAP_GEN(mdev, apm))\n\t\tprops->device_cap_flags |= IB_DEVICE_AUTO_PATH_MIG;\n\tif (MLX5_CAP_GEN(mdev, xrc))\n\t\tprops->device_cap_flags |= IB_DEVICE_XRC;\n\tif (MLX5_CAP_GEN(mdev, imaicl)) {\n\t\tprops->device_cap_flags |= IB_DEVICE_MEM_WINDOW |\n\t\t\t\t\t   IB_DEVICE_MEM_WINDOW_TYPE_2B;\n\t\tprops->max_mw = 1 << MLX5_CAP_GEN(mdev, log_max_mkey);\n\t\t \n\t\tprops->kernel_cap_flags |= IBK_SG_GAPS_REG;\n\t}\n\t \n\tif (!MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))\n\t\tprops->device_cap_flags |= IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tif (MLX5_CAP_GEN(mdev, sho)) {\n\t\tprops->kernel_cap_flags |= IBK_INTEGRITY_HANDOVER;\n\t\t \n\t\tprops->sig_prot_cap = IB_PROT_T10DIF_TYPE_1 |\n\t\t\t\t      IB_PROT_T10DIF_TYPE_2 |\n\t\t\t\t      IB_PROT_T10DIF_TYPE_3;\n\t\tprops->sig_guard_cap = IB_GUARD_T10DIF_CRC |\n\t\t\t\t       IB_GUARD_T10DIF_CSUM;\n\t}\n\tif (MLX5_CAP_GEN(mdev, block_lb_mc))\n\t\tprops->kernel_cap_flags |= IBK_BLOCK_MULTICAST_LOOPBACK;\n\n\tif (MLX5_CAP_GEN(dev->mdev, eth_net_offloads) && raw_support) {\n\t\tif (MLX5_CAP_ETH(mdev, csum_cap)) {\n\t\t\t \n\t\t\tprops->device_cap_flags |= IB_DEVICE_RAW_IP_CSUM;\n\t\t\tprops->raw_packet_caps |= IB_RAW_PACKET_CAP_IP_CSUM;\n\t\t}\n\n\t\tif (MLX5_CAP_ETH(dev->mdev, vlan_cap))\n\t\t\tprops->raw_packet_caps |=\n\t\t\t\tIB_RAW_PACKET_CAP_CVLAN_STRIPPING;\n\n\t\tif (offsetofend(typeof(resp), tso_caps) <= uhw_outlen) {\n\t\t\tmax_tso = MLX5_CAP_ETH(mdev, max_lso_cap);\n\t\t\tif (max_tso) {\n\t\t\t\tresp.tso_caps.max_tso = 1 << max_tso;\n\t\t\t\tresp.tso_caps.supported_qpts |=\n\t\t\t\t\t1 << IB_QPT_RAW_PACKET;\n\t\t\t\tresp.response_length += sizeof(resp.tso_caps);\n\t\t\t}\n\t\t}\n\n\t\tif (offsetofend(typeof(resp), rss_caps) <= uhw_outlen) {\n\t\t\tresp.rss_caps.rx_hash_function =\n\t\t\t\t\t\tMLX5_RX_HASH_FUNC_TOEPLITZ;\n\t\t\tresp.rss_caps.rx_hash_fields_mask =\n\t\t\t\t\t\tMLX5_RX_HASH_SRC_IPV4 |\n\t\t\t\t\t\tMLX5_RX_HASH_DST_IPV4 |\n\t\t\t\t\t\tMLX5_RX_HASH_SRC_IPV6 |\n\t\t\t\t\t\tMLX5_RX_HASH_DST_IPV6 |\n\t\t\t\t\t\tMLX5_RX_HASH_SRC_PORT_TCP |\n\t\t\t\t\t\tMLX5_RX_HASH_DST_PORT_TCP |\n\t\t\t\t\t\tMLX5_RX_HASH_SRC_PORT_UDP |\n\t\t\t\t\t\tMLX5_RX_HASH_DST_PORT_UDP |\n\t\t\t\t\t\tMLX5_RX_HASH_INNER;\n\t\t\tresp.response_length += sizeof(resp.rss_caps);\n\t\t}\n\t} else {\n\t\tif (offsetofend(typeof(resp), tso_caps) <= uhw_outlen)\n\t\t\tresp.response_length += sizeof(resp.tso_caps);\n\t\tif (offsetofend(typeof(resp), rss_caps) <= uhw_outlen)\n\t\t\tresp.response_length += sizeof(resp.rss_caps);\n\t}\n\n\tif (MLX5_CAP_GEN(mdev, ipoib_basic_offloads)) {\n\t\tprops->device_cap_flags |= IB_DEVICE_UD_IP_CSUM;\n\t\tprops->kernel_cap_flags |= IBK_UD_TSO;\n\t}\n\n\tif (MLX5_CAP_GEN(dev->mdev, rq_delay_drop) &&\n\t    MLX5_CAP_GEN(dev->mdev, general_notification_event) &&\n\t    raw_support)\n\t\tprops->raw_packet_caps |= IB_RAW_PACKET_CAP_DELAY_DROP;\n\n\tif (MLX5_CAP_GEN(mdev, ipoib_enhanced_offloads) &&\n\t    MLX5_CAP_IPOIB_ENHANCED(mdev, csum_cap))\n\t\tprops->device_cap_flags |= IB_DEVICE_UD_IP_CSUM;\n\n\tif (MLX5_CAP_GEN(dev->mdev, eth_net_offloads) &&\n\t    MLX5_CAP_ETH(dev->mdev, scatter_fcs) &&\n\t    raw_support) {\n\t\t \n\t\tprops->device_cap_flags |= IB_DEVICE_RAW_SCATTER_FCS;\n\t\tprops->raw_packet_caps |= IB_RAW_PACKET_CAP_SCATTER_FCS;\n\t}\n\n\tif (MLX5_CAP_DEV_MEM(mdev, memic)) {\n\t\tprops->max_dm_size =\n\t\t\tMLX5_CAP_DEV_MEM(mdev, max_memic_size);\n\t}\n\n\tif (mlx5_get_flow_namespace(dev->mdev, MLX5_FLOW_NAMESPACE_BYPASS))\n\t\tprops->device_cap_flags |= IB_DEVICE_MANAGED_FLOW_STEERING;\n\n\tif (MLX5_CAP_GEN(mdev, end_pad))\n\t\tprops->device_cap_flags |= IB_DEVICE_PCI_WRITE_END_PADDING;\n\n\tprops->vendor_part_id\t   = mdev->pdev->device;\n\tprops->hw_ver\t\t   = mdev->pdev->revision;\n\n\tprops->max_mr_size\t   = ~0ull;\n\tprops->page_size_cap\t   = ~(min_page_size - 1);\n\tprops->max_qp\t\t   = 1 << MLX5_CAP_GEN(mdev, log_max_qp);\n\tprops->max_qp_wr\t   = 1 << MLX5_CAP_GEN(mdev, log_max_qp_sz);\n\tmax_rq_sg =  MLX5_CAP_GEN(mdev, max_wqe_sz_rq) /\n\t\t     sizeof(struct mlx5_wqe_data_seg);\n\tmax_sq_desc = min_t(int, MLX5_CAP_GEN(mdev, max_wqe_sz_sq), 512);\n\tmax_sq_sg = (max_sq_desc - sizeof(struct mlx5_wqe_ctrl_seg) -\n\t\t     sizeof(struct mlx5_wqe_raddr_seg)) /\n\t\tsizeof(struct mlx5_wqe_data_seg);\n\tprops->max_send_sge = max_sq_sg;\n\tprops->max_recv_sge = max_rq_sg;\n\tprops->max_sge_rd\t   = MLX5_MAX_SGE_RD;\n\tprops->max_cq\t\t   = 1 << MLX5_CAP_GEN(mdev, log_max_cq);\n\tprops->max_cqe = (1 << MLX5_CAP_GEN(mdev, log_max_cq_sz)) - 1;\n\tprops->max_mr\t\t   = 1 << MLX5_CAP_GEN(mdev, log_max_mkey);\n\tprops->max_pd\t\t   = 1 << MLX5_CAP_GEN(mdev, log_max_pd);\n\tprops->max_qp_rd_atom\t   = 1 << MLX5_CAP_GEN(mdev, log_max_ra_req_qp);\n\tprops->max_qp_init_rd_atom = 1 << MLX5_CAP_GEN(mdev, log_max_ra_res_qp);\n\tprops->max_srq\t\t   = 1 << MLX5_CAP_GEN(mdev, log_max_srq);\n\tprops->max_srq_wr = (1 << MLX5_CAP_GEN(mdev, log_max_srq_sz)) - 1;\n\tprops->local_ca_ack_delay  = MLX5_CAP_GEN(mdev, local_ca_ack_delay);\n\tprops->max_res_rd_atom\t   = props->max_qp_rd_atom * props->max_qp;\n\tprops->max_srq_sge\t   = max_rq_sg - 1;\n\tprops->max_fast_reg_page_list_len =\n\t\t1 << MLX5_CAP_GEN(mdev, log_max_klm_list_size);\n\tprops->max_pi_fast_reg_page_list_len =\n\t\tprops->max_fast_reg_page_list_len / 2;\n\tprops->max_sgl_rd =\n\t\tMLX5_CAP_GEN(mdev, max_sgl_for_optimized_performance);\n\tget_atomic_caps_qp(dev, props);\n\tprops->masked_atomic_cap   = IB_ATOMIC_NONE;\n\tprops->max_mcast_grp\t   = 1 << MLX5_CAP_GEN(mdev, log_max_mcg);\n\tprops->max_mcast_qp_attach = MLX5_CAP_GEN(mdev, max_qp_mcg);\n\tprops->max_total_mcast_qp_attach = props->max_mcast_qp_attach *\n\t\t\t\t\t   props->max_mcast_grp;\n\tprops->max_ah = INT_MAX;\n\tprops->hca_core_clock = MLX5_CAP_GEN(mdev, device_frequency_khz);\n\tprops->timestamp_mask = 0x7FFFFFFFFFFFFFFFULL;\n\n\tif (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {\n\t\tif (dev->odp_caps.general_caps & IB_ODP_SUPPORT)\n\t\t\tprops->kernel_cap_flags |= IBK_ON_DEMAND_PAGING;\n\t\tprops->odp_caps = dev->odp_caps;\n\t\tif (!uhw) {\n\t\t\t \n\t\t\tprops->odp_caps.per_transport_caps.rc_odp_caps &=\n\t\t\t\t~(IB_ODP_SUPPORT_READ |\n\t\t\t\t  IB_ODP_SUPPORT_SRQ_RECV);\n\t\t\tprops->odp_caps.per_transport_caps.uc_odp_caps &=\n\t\t\t\t~(IB_ODP_SUPPORT_READ |\n\t\t\t\t  IB_ODP_SUPPORT_SRQ_RECV);\n\t\t\tprops->odp_caps.per_transport_caps.ud_odp_caps &=\n\t\t\t\t~(IB_ODP_SUPPORT_READ |\n\t\t\t\t  IB_ODP_SUPPORT_SRQ_RECV);\n\t\t\tprops->odp_caps.per_transport_caps.xrc_odp_caps &=\n\t\t\t\t~(IB_ODP_SUPPORT_READ |\n\t\t\t\t  IB_ODP_SUPPORT_SRQ_RECV);\n\t\t}\n\t}\n\n\tif (mlx5_core_is_vf(mdev))\n\t\tprops->kernel_cap_flags |= IBK_VIRTUAL_FUNCTION;\n\n\tif (mlx5_ib_port_link_layer(ibdev, 1) ==\n\t    IB_LINK_LAYER_ETHERNET && raw_support) {\n\t\tprops->rss_caps.max_rwq_indirection_tables =\n\t\t\t1 << MLX5_CAP_GEN(dev->mdev, log_max_rqt);\n\t\tprops->rss_caps.max_rwq_indirection_table_size =\n\t\t\t1 << MLX5_CAP_GEN(dev->mdev, log_max_rqt_size);\n\t\tprops->rss_caps.supported_qpts = 1 << IB_QPT_RAW_PACKET;\n\t\tprops->max_wq_type_rq =\n\t\t\t1 << MLX5_CAP_GEN(dev->mdev, log_max_rq);\n\t}\n\n\tif (MLX5_CAP_GEN(mdev, tag_matching)) {\n\t\tprops->tm_caps.max_num_tags =\n\t\t\t(1 << MLX5_CAP_GEN(mdev, log_tag_matching_list_sz)) - 1;\n\t\tprops->tm_caps.max_ops =\n\t\t\t1 << MLX5_CAP_GEN(mdev, log_max_qp_sz);\n\t\tprops->tm_caps.max_sge = MLX5_TM_MAX_SGE;\n\t}\n\n\tif (MLX5_CAP_GEN(mdev, tag_matching) &&\n\t    MLX5_CAP_GEN(mdev, rndv_offload_rc)) {\n\t\tprops->tm_caps.flags = IB_TM_CAP_RNDV_RC;\n\t\tprops->tm_caps.max_rndv_hdr_size = MLX5_TM_MAX_RNDV_MSG_SIZE;\n\t}\n\n\tif (MLX5_CAP_GEN(dev->mdev, cq_moderation)) {\n\t\tprops->cq_caps.max_cq_moderation_count =\n\t\t\t\t\t\tMLX5_MAX_CQ_COUNT;\n\t\tprops->cq_caps.max_cq_moderation_period =\n\t\t\t\t\t\tMLX5_MAX_CQ_PERIOD;\n\t}\n\n\tif (offsetofend(typeof(resp), cqe_comp_caps) <= uhw_outlen) {\n\t\tresp.response_length += sizeof(resp.cqe_comp_caps);\n\n\t\tif (MLX5_CAP_GEN(dev->mdev, cqe_compression)) {\n\t\t\tresp.cqe_comp_caps.max_num =\n\t\t\t\tMLX5_CAP_GEN(dev->mdev,\n\t\t\t\t\t     cqe_compression_max_num);\n\n\t\t\tresp.cqe_comp_caps.supported_format =\n\t\t\t\tMLX5_IB_CQE_RES_FORMAT_HASH |\n\t\t\t\tMLX5_IB_CQE_RES_FORMAT_CSUM;\n\n\t\t\tif (MLX5_CAP_GEN(dev->mdev, mini_cqe_resp_stride_index))\n\t\t\t\tresp.cqe_comp_caps.supported_format |=\n\t\t\t\t\tMLX5_IB_CQE_RES_FORMAT_CSUM_STRIDX;\n\t\t}\n\t}\n\n\tif (offsetofend(typeof(resp), packet_pacing_caps) <= uhw_outlen &&\n\t    raw_support) {\n\t\tif (MLX5_CAP_QOS(mdev, packet_pacing) &&\n\t\t    MLX5_CAP_GEN(mdev, qos)) {\n\t\t\tresp.packet_pacing_caps.qp_rate_limit_max =\n\t\t\t\tMLX5_CAP_QOS(mdev, packet_pacing_max_rate);\n\t\t\tresp.packet_pacing_caps.qp_rate_limit_min =\n\t\t\t\tMLX5_CAP_QOS(mdev, packet_pacing_min_rate);\n\t\t\tresp.packet_pacing_caps.supported_qpts |=\n\t\t\t\t1 << IB_QPT_RAW_PACKET;\n\t\t\tif (MLX5_CAP_QOS(mdev, packet_pacing_burst_bound) &&\n\t\t\t    MLX5_CAP_QOS(mdev, packet_pacing_typical_size))\n\t\t\t\tresp.packet_pacing_caps.cap_flags |=\n\t\t\t\t\tMLX5_IB_PP_SUPPORT_BURST;\n\t\t}\n\t\tresp.response_length += sizeof(resp.packet_pacing_caps);\n\t}\n\n\tif (offsetofend(typeof(resp), mlx5_ib_support_multi_pkt_send_wqes) <=\n\t    uhw_outlen) {\n\t\tif (MLX5_CAP_ETH(mdev, multi_pkt_send_wqe))\n\t\t\tresp.mlx5_ib_support_multi_pkt_send_wqes =\n\t\t\t\tMLX5_IB_ALLOW_MPW;\n\n\t\tif (MLX5_CAP_ETH(mdev, enhanced_multi_pkt_send_wqe))\n\t\t\tresp.mlx5_ib_support_multi_pkt_send_wqes |=\n\t\t\t\tMLX5_IB_SUPPORT_EMPW;\n\n\t\tresp.response_length +=\n\t\t\tsizeof(resp.mlx5_ib_support_multi_pkt_send_wqes);\n\t}\n\n\tif (offsetofend(typeof(resp), flags) <= uhw_outlen) {\n\t\tresp.response_length += sizeof(resp.flags);\n\n\t\tif (MLX5_CAP_GEN(mdev, cqe_compression_128))\n\t\t\tresp.flags |=\n\t\t\t\tMLX5_IB_QUERY_DEV_RESP_FLAGS_CQE_128B_COMP;\n\n\t\tif (MLX5_CAP_GEN(mdev, cqe_128_always))\n\t\t\tresp.flags |= MLX5_IB_QUERY_DEV_RESP_FLAGS_CQE_128B_PAD;\n\t\tif (MLX5_CAP_GEN(mdev, qp_packet_based))\n\t\t\tresp.flags |=\n\t\t\t\tMLX5_IB_QUERY_DEV_RESP_PACKET_BASED_CREDIT_MODE;\n\n\t\tresp.flags |= MLX5_IB_QUERY_DEV_RESP_FLAGS_SCAT2CQE_DCT;\n\t}\n\n\tif (offsetofend(typeof(resp), sw_parsing_caps) <= uhw_outlen) {\n\t\tresp.response_length += sizeof(resp.sw_parsing_caps);\n\t\tif (MLX5_CAP_ETH(mdev, swp)) {\n\t\t\tresp.sw_parsing_caps.sw_parsing_offloads |=\n\t\t\t\tMLX5_IB_SW_PARSING;\n\n\t\t\tif (MLX5_CAP_ETH(mdev, swp_csum))\n\t\t\t\tresp.sw_parsing_caps.sw_parsing_offloads |=\n\t\t\t\t\tMLX5_IB_SW_PARSING_CSUM;\n\n\t\t\tif (MLX5_CAP_ETH(mdev, swp_lso))\n\t\t\t\tresp.sw_parsing_caps.sw_parsing_offloads |=\n\t\t\t\t\tMLX5_IB_SW_PARSING_LSO;\n\n\t\t\tif (resp.sw_parsing_caps.sw_parsing_offloads)\n\t\t\t\tresp.sw_parsing_caps.supported_qpts =\n\t\t\t\t\tBIT(IB_QPT_RAW_PACKET);\n\t\t}\n\t}\n\n\tif (offsetofend(typeof(resp), striding_rq_caps) <= uhw_outlen &&\n\t    raw_support) {\n\t\tresp.response_length += sizeof(resp.striding_rq_caps);\n\t\tif (MLX5_CAP_GEN(mdev, striding_rq)) {\n\t\t\tresp.striding_rq_caps.min_single_stride_log_num_of_bytes =\n\t\t\t\tMLX5_MIN_SINGLE_STRIDE_LOG_NUM_BYTES;\n\t\t\tresp.striding_rq_caps.max_single_stride_log_num_of_bytes =\n\t\t\t\tMLX5_MAX_SINGLE_STRIDE_LOG_NUM_BYTES;\n\t\t\tif (MLX5_CAP_GEN(dev->mdev, ext_stride_num_range))\n\t\t\t\tresp.striding_rq_caps\n\t\t\t\t\t.min_single_wqe_log_num_of_strides =\n\t\t\t\t\tMLX5_EXT_MIN_SINGLE_WQE_LOG_NUM_STRIDES;\n\t\t\telse\n\t\t\t\tresp.striding_rq_caps\n\t\t\t\t\t.min_single_wqe_log_num_of_strides =\n\t\t\t\t\tMLX5_MIN_SINGLE_WQE_LOG_NUM_STRIDES;\n\t\t\tresp.striding_rq_caps.max_single_wqe_log_num_of_strides =\n\t\t\t\tMLX5_MAX_SINGLE_WQE_LOG_NUM_STRIDES;\n\t\t\tresp.striding_rq_caps.supported_qpts =\n\t\t\t\tBIT(IB_QPT_RAW_PACKET);\n\t\t}\n\t}\n\n\tif (offsetofend(typeof(resp), tunnel_offloads_caps) <= uhw_outlen) {\n\t\tresp.response_length += sizeof(resp.tunnel_offloads_caps);\n\t\tif (MLX5_CAP_ETH(mdev, tunnel_stateless_vxlan))\n\t\t\tresp.tunnel_offloads_caps |=\n\t\t\t\tMLX5_IB_TUNNELED_OFFLOADS_VXLAN;\n\t\tif (MLX5_CAP_ETH(mdev, tunnel_stateless_geneve_rx))\n\t\t\tresp.tunnel_offloads_caps |=\n\t\t\t\tMLX5_IB_TUNNELED_OFFLOADS_GENEVE;\n\t\tif (MLX5_CAP_ETH(mdev, tunnel_stateless_gre))\n\t\t\tresp.tunnel_offloads_caps |=\n\t\t\t\tMLX5_IB_TUNNELED_OFFLOADS_GRE;\n\t\tif (MLX5_CAP_ETH(mdev, tunnel_stateless_mpls_over_gre))\n\t\t\tresp.tunnel_offloads_caps |=\n\t\t\t\tMLX5_IB_TUNNELED_OFFLOADS_MPLS_GRE;\n\t\tif (MLX5_CAP_ETH(mdev, tunnel_stateless_mpls_over_udp))\n\t\t\tresp.tunnel_offloads_caps |=\n\t\t\t\tMLX5_IB_TUNNELED_OFFLOADS_MPLS_UDP;\n\t}\n\n\tif (offsetofend(typeof(resp), dci_streams_caps) <= uhw_outlen) {\n\t\tresp.response_length += sizeof(resp.dci_streams_caps);\n\n\t\tresp.dci_streams_caps.max_log_num_concurent =\n\t\t\tMLX5_CAP_GEN(mdev, log_max_dci_stream_channels);\n\n\t\tresp.dci_streams_caps.max_log_num_errored =\n\t\t\tMLX5_CAP_GEN(mdev, log_max_dci_errored_streams);\n\t}\n\n\tif (uhw_outlen) {\n\t\terr = ib_copy_to_udata(uhw, &resp, resp.response_length);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void translate_active_width(struct ib_device *ibdev, u16 active_width,\n\t\t\t\t   u8 *ib_width)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\n\tif (active_width & MLX5_PTYS_WIDTH_1X)\n\t\t*ib_width = IB_WIDTH_1X;\n\telse if (active_width & MLX5_PTYS_WIDTH_2X)\n\t\t*ib_width = IB_WIDTH_2X;\n\telse if (active_width & MLX5_PTYS_WIDTH_4X)\n\t\t*ib_width = IB_WIDTH_4X;\n\telse if (active_width & MLX5_PTYS_WIDTH_8X)\n\t\t*ib_width = IB_WIDTH_8X;\n\telse if (active_width & MLX5_PTYS_WIDTH_12X)\n\t\t*ib_width = IB_WIDTH_12X;\n\telse {\n\t\tmlx5_ib_dbg(dev, \"Invalid active_width %d, setting width to default value: 4x\\n\",\n\t\t\t    active_width);\n\t\t*ib_width = IB_WIDTH_4X;\n\t}\n\n\treturn;\n}\n\nstatic int mlx5_mtu_to_ib_mtu(int mtu)\n{\n\tswitch (mtu) {\n\tcase 256: return 1;\n\tcase 512: return 2;\n\tcase 1024: return 3;\n\tcase 2048: return 4;\n\tcase 4096: return 5;\n\tdefault:\n\t\tpr_warn(\"invalid mtu\\n\");\n\t\treturn -1;\n\t}\n}\n\nenum ib_max_vl_num {\n\t__IB_MAX_VL_0\t\t= 1,\n\t__IB_MAX_VL_0_1\t\t= 2,\n\t__IB_MAX_VL_0_3\t\t= 3,\n\t__IB_MAX_VL_0_7\t\t= 4,\n\t__IB_MAX_VL_0_14\t= 5,\n};\n\nenum mlx5_vl_hw_cap {\n\tMLX5_VL_HW_0\t= 1,\n\tMLX5_VL_HW_0_1\t= 2,\n\tMLX5_VL_HW_0_2\t= 3,\n\tMLX5_VL_HW_0_3\t= 4,\n\tMLX5_VL_HW_0_4\t= 5,\n\tMLX5_VL_HW_0_5\t= 6,\n\tMLX5_VL_HW_0_6\t= 7,\n\tMLX5_VL_HW_0_7\t= 8,\n\tMLX5_VL_HW_0_14\t= 15\n};\n\nstatic int translate_max_vl_num(struct ib_device *ibdev, u8 vl_hw_cap,\n\t\t\t\tu8 *max_vl_num)\n{\n\tswitch (vl_hw_cap) {\n\tcase MLX5_VL_HW_0:\n\t\t*max_vl_num = __IB_MAX_VL_0;\n\t\tbreak;\n\tcase MLX5_VL_HW_0_1:\n\t\t*max_vl_num = __IB_MAX_VL_0_1;\n\t\tbreak;\n\tcase MLX5_VL_HW_0_3:\n\t\t*max_vl_num = __IB_MAX_VL_0_3;\n\t\tbreak;\n\tcase MLX5_VL_HW_0_7:\n\t\t*max_vl_num = __IB_MAX_VL_0_7;\n\t\tbreak;\n\tcase MLX5_VL_HW_0_14:\n\t\t*max_vl_num = __IB_MAX_VL_0_14;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5_query_hca_port(struct ib_device *ibdev, u32 port,\n\t\t\t       struct ib_port_attr *props)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tstruct mlx5_hca_vport_context *rep;\n\tu16 max_mtu;\n\tu16 oper_mtu;\n\tint err;\n\tu16 ib_link_width_oper;\n\tu8 vl_hw_cap;\n\n\trep = kzalloc(sizeof(*rep), GFP_KERNEL);\n\tif (!rep) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\n\terr = mlx5_query_hca_vport_context(mdev, 0, port, 0, rep);\n\tif (err)\n\t\tgoto out;\n\n\tprops->lid\t\t= rep->lid;\n\tprops->lmc\t\t= rep->lmc;\n\tprops->sm_lid\t\t= rep->sm_lid;\n\tprops->sm_sl\t\t= rep->sm_sl;\n\tprops->state\t\t= rep->vport_state;\n\tprops->phys_state\t= rep->port_physical_state;\n\tprops->port_cap_flags\t= rep->cap_mask1;\n\tprops->gid_tbl_len\t= mlx5_get_gid_table_len(MLX5_CAP_GEN(mdev, gid_table_size));\n\tprops->max_msg_sz\t= 1 << MLX5_CAP_GEN(mdev, log_max_msg);\n\tprops->pkey_tbl_len\t= mlx5_to_sw_pkey_sz(MLX5_CAP_GEN(mdev, pkey_table_size));\n\tprops->bad_pkey_cntr\t= rep->pkey_violation_counter;\n\tprops->qkey_viol_cntr\t= rep->qkey_violation_counter;\n\tprops->subnet_timeout\t= rep->subnet_timeout;\n\tprops->init_type_reply\t= rep->init_type_reply;\n\n\tif (props->port_cap_flags & IB_PORT_CAP_MASK2_SUP)\n\t\tprops->port_cap_flags2 = rep->cap_mask2;\n\n\terr = mlx5_query_ib_port_oper(mdev, &ib_link_width_oper,\n\t\t\t\t      &props->active_speed, port);\n\tif (err)\n\t\tgoto out;\n\n\ttranslate_active_width(ibdev, ib_link_width_oper, &props->active_width);\n\n\tmlx5_query_port_max_mtu(mdev, &max_mtu, port);\n\n\tprops->max_mtu = mlx5_mtu_to_ib_mtu(max_mtu);\n\n\tmlx5_query_port_oper_mtu(mdev, &oper_mtu, port);\n\n\tprops->active_mtu = mlx5_mtu_to_ib_mtu(oper_mtu);\n\n\terr = mlx5_query_port_vl_hw_cap(mdev, &vl_hw_cap, port);\n\tif (err)\n\t\tgoto out;\n\n\terr = translate_max_vl_num(ibdev, vl_hw_cap,\n\t\t\t\t   &props->max_vl_num);\nout:\n\tkfree(rep);\n\treturn err;\n}\n\nint mlx5_ib_query_port(struct ib_device *ibdev, u32 port,\n\t\t       struct ib_port_attr *props)\n{\n\tunsigned int count;\n\tint ret;\n\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\tret = mlx5_query_mad_ifc_port(ibdev, port, props);\n\t\tbreak;\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\t\tret = mlx5_query_hca_port(ibdev, port, props);\n\t\tbreak;\n\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\tret = mlx5_query_port_roce(ibdev, port, props);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tif (!ret && props) {\n\t\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\t\tstruct mlx5_core_dev *mdev;\n\t\tbool put_mdev = true;\n\n\t\tmdev = mlx5_ib_get_native_port_mdev(dev, port, NULL);\n\t\tif (!mdev) {\n\t\t\t \n\t\t\tmdev = dev->mdev;\n\t\t\tport = 1;\n\t\t\tput_mdev = false;\n\t\t}\n\t\tcount = mlx5_core_reserved_gids_count(mdev);\n\t\tif (put_mdev)\n\t\t\tmlx5_ib_put_native_port_mdev(dev, port);\n\t\tprops->gid_tbl_len -= count;\n\t}\n\treturn ret;\n}\n\nstatic int mlx5_ib_rep_query_port(struct ib_device *ibdev, u32 port,\n\t\t\t\t  struct ib_port_attr *props)\n{\n\treturn mlx5_query_port_roce(ibdev, port, props);\n}\n\nstatic int mlx5_ib_rep_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t\t\t  u16 *pkey)\n{\n\t \n\t*pkey = 0xffff;\n\treturn 0;\n}\n\nstatic int mlx5_ib_query_gid(struct ib_device *ibdev, u32 port, int index,\n\t\t\t     union ib_gid *gid)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_gids(ibdev, port, index, gid);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\t\treturn mlx5_query_hca_vport_gid(mdev, 0, port, 0, index, gid);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n}\n\nstatic int mlx5_query_hca_nic_pkey(struct ib_device *ibdev, u32 port,\n\t\t\t\t   u16 index, u16 *pkey)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_core_dev *mdev;\n\tbool put_mdev = true;\n\tu32 mdev_port_num;\n\tint err;\n\n\tmdev = mlx5_ib_get_native_port_mdev(dev, port, &mdev_port_num);\n\tif (!mdev) {\n\t\t \n\t\tput_mdev = false;\n\t\tmdev = dev->mdev;\n\t\tmdev_port_num = 1;\n\t}\n\n\terr = mlx5_query_hca_vport_pkey(mdev, 0, mdev_port_num, 0,\n\t\t\t\t\tindex, pkey);\n\tif (put_mdev)\n\t\tmlx5_ib_put_native_port_mdev(dev, port);\n\n\treturn err;\n}\n\nstatic int mlx5_ib_query_pkey(struct ib_device *ibdev, u32 port, u16 index,\n\t\t\t      u16 *pkey)\n{\n\tswitch (mlx5_get_vport_access_method(ibdev)) {\n\tcase MLX5_VPORT_ACCESS_METHOD_MAD:\n\t\treturn mlx5_query_mad_ifc_pkey(ibdev, port, index, pkey);\n\n\tcase MLX5_VPORT_ACCESS_METHOD_HCA:\n\tcase MLX5_VPORT_ACCESS_METHOD_NIC:\n\t\treturn mlx5_query_hca_nic_pkey(ibdev, port, index, pkey);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int mlx5_ib_modify_device(struct ib_device *ibdev, int mask,\n\t\t\t\t struct ib_device_modify *props)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_reg_node_desc in;\n\tstruct mlx5_reg_node_desc out;\n\tint err;\n\n\tif (mask & ~IB_DEVICE_MODIFY_NODE_DESC)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!(mask & IB_DEVICE_MODIFY_NODE_DESC))\n\t\treturn 0;\n\n\t \n\tmemcpy(&in, props->node_desc, IB_DEVICE_NODE_DESC_MAX);\n\terr = mlx5_core_access_reg(dev->mdev, &in, sizeof(in), &out,\n\t\t\t\t   sizeof(out), MLX5_REG_NODE_DESC, 0, 1);\n\tif (err)\n\t\treturn err;\n\n\tmemcpy(ibdev->node_desc, props->node_desc, IB_DEVICE_NODE_DESC_MAX);\n\n\treturn err;\n}\n\nstatic int set_port_caps_atomic(struct mlx5_ib_dev *dev, u32 port_num, u32 mask,\n\t\t\t\tu32 value)\n{\n\tstruct mlx5_hca_vport_context ctx = {};\n\tstruct mlx5_core_dev *mdev;\n\tu32 mdev_port_num;\n\tint err;\n\n\tmdev = mlx5_ib_get_native_port_mdev(dev, port_num, &mdev_port_num);\n\tif (!mdev)\n\t\treturn -ENODEV;\n\n\terr = mlx5_query_hca_vport_context(mdev, 0, mdev_port_num, 0, &ctx);\n\tif (err)\n\t\tgoto out;\n\n\tif (~ctx.cap_mask1_perm & mask) {\n\t\tmlx5_ib_warn(dev, \"trying to change bitmask 0x%X but change supported 0x%X\\n\",\n\t\t\t     mask, ctx.cap_mask1_perm);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tctx.cap_mask1 = value;\n\tctx.cap_mask1_perm = mask;\n\terr = mlx5_core_modify_hca_vport_context(mdev, 0, mdev_port_num,\n\t\t\t\t\t\t 0, &ctx);\n\nout:\n\tmlx5_ib_put_native_port_mdev(dev, port_num);\n\n\treturn err;\n}\n\nstatic int mlx5_ib_modify_port(struct ib_device *ibdev, u32 port, int mask,\n\t\t\t       struct ib_port_modify *props)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct ib_port_attr attr;\n\tu32 tmp;\n\tint err;\n\tu32 change_mask;\n\tu32 value;\n\tbool is_ib = (mlx5_ib_port_link_layer(ibdev, port) ==\n\t\t      IB_LINK_LAYER_INFINIBAND);\n\n\t \n\tif (!is_ib)\n\t\treturn 0;\n\n\tif (MLX5_CAP_GEN(dev->mdev, ib_virt) && is_ib) {\n\t\tchange_mask = props->clr_port_cap_mask | props->set_port_cap_mask;\n\t\tvalue = ~props->clr_port_cap_mask | props->set_port_cap_mask;\n\t\treturn set_port_caps_atomic(dev, port, change_mask, value);\n\t}\n\n\tmutex_lock(&dev->cap_mask_mutex);\n\n\terr = ib_query_port(ibdev, port, &attr);\n\tif (err)\n\t\tgoto out;\n\n\ttmp = (attr.port_cap_flags | props->set_port_cap_mask) &\n\t\t~props->clr_port_cap_mask;\n\n\terr = mlx5_set_port_caps(dev->mdev, port, tmp);\n\nout:\n\tmutex_unlock(&dev->cap_mask_mutex);\n\treturn err;\n}\n\nstatic void print_lib_caps(struct mlx5_ib_dev *dev, u64 caps)\n{\n\tmlx5_ib_dbg(dev, \"MLX5_LIB_CAP_4K_UAR = %s\\n\",\n\t\t    caps & MLX5_LIB_CAP_4K_UAR ? \"y\" : \"n\");\n}\n\nstatic u16 calc_dynamic_bfregs(int uars_per_sys_page)\n{\n\t \n\tif (uars_per_sys_page == 1  && PAGE_SIZE > 4096)\n\t\treturn MLX5_MIN_DYN_BFREGS;\n\n\treturn MLX5_MAX_DYN_BFREGS;\n}\n\nstatic int calc_total_bfregs(struct mlx5_ib_dev *dev, bool lib_uar_4k,\n\t\t\t     struct mlx5_ib_alloc_ucontext_req_v2 *req,\n\t\t\t     struct mlx5_bfreg_info *bfregi)\n{\n\tint uars_per_sys_page;\n\tint bfregs_per_sys_page;\n\tint ref_bfregs = req->total_num_bfregs;\n\n\tif (req->total_num_bfregs == 0)\n\t\treturn -EINVAL;\n\n\tBUILD_BUG_ON(MLX5_MAX_BFREGS % MLX5_NON_FP_BFREGS_IN_PAGE);\n\tBUILD_BUG_ON(MLX5_MAX_BFREGS < MLX5_NON_FP_BFREGS_IN_PAGE);\n\n\tif (req->total_num_bfregs > MLX5_MAX_BFREGS)\n\t\treturn -ENOMEM;\n\n\tuars_per_sys_page = get_uars_per_sys_page(dev, lib_uar_4k);\n\tbfregs_per_sys_page = uars_per_sys_page * MLX5_NON_FP_BFREGS_PER_UAR;\n\t \n\treq->total_num_bfregs = ALIGN(req->total_num_bfregs, bfregs_per_sys_page);\n\tif (req->num_low_latency_bfregs > req->total_num_bfregs - 1)\n\t\treturn -EINVAL;\n\n\tbfregi->num_static_sys_pages = req->total_num_bfregs / bfregs_per_sys_page;\n\tbfregi->num_dyn_bfregs = ALIGN(calc_dynamic_bfregs(uars_per_sys_page), bfregs_per_sys_page);\n\tbfregi->total_num_bfregs = req->total_num_bfregs + bfregi->num_dyn_bfregs;\n\tbfregi->num_sys_pages = bfregi->total_num_bfregs / bfregs_per_sys_page;\n\n\tmlx5_ib_dbg(dev, \"uar_4k: fw support %s, lib support %s, user requested %d bfregs, allocated %d, total bfregs %d, using %d sys pages\\n\",\n\t\t    MLX5_CAP_GEN(dev->mdev, uar_4k) ? \"yes\" : \"no\",\n\t\t    lib_uar_4k ? \"yes\" : \"no\", ref_bfregs,\n\t\t    req->total_num_bfregs, bfregi->total_num_bfregs,\n\t\t    bfregi->num_sys_pages);\n\n\treturn 0;\n}\n\nstatic int allocate_uars(struct mlx5_ib_dev *dev, struct mlx5_ib_ucontext *context)\n{\n\tstruct mlx5_bfreg_info *bfregi;\n\tint err;\n\tint i;\n\n\tbfregi = &context->bfregi;\n\tfor (i = 0; i < bfregi->num_static_sys_pages; i++) {\n\t\terr = mlx5_cmd_uar_alloc(dev->mdev, &bfregi->sys_pages[i],\n\t\t\t\t\t context->devx_uid);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\tmlx5_ib_dbg(dev, \"allocated uar %d\\n\", bfregi->sys_pages[i]);\n\t}\n\n\tfor (i = bfregi->num_static_sys_pages; i < bfregi->num_sys_pages; i++)\n\t\tbfregi->sys_pages[i] = MLX5_IB_INVALID_UAR_INDEX;\n\n\treturn 0;\n\nerror:\n\tfor (--i; i >= 0; i--)\n\t\tif (mlx5_cmd_uar_dealloc(dev->mdev, bfregi->sys_pages[i],\n\t\t\t\t\t context->devx_uid))\n\t\t\tmlx5_ib_warn(dev, \"failed to free uar %d\\n\", i);\n\n\treturn err;\n}\n\nstatic void deallocate_uars(struct mlx5_ib_dev *dev,\n\t\t\t    struct mlx5_ib_ucontext *context)\n{\n\tstruct mlx5_bfreg_info *bfregi;\n\tint i;\n\n\tbfregi = &context->bfregi;\n\tfor (i = 0; i < bfregi->num_sys_pages; i++)\n\t\tif (i < bfregi->num_static_sys_pages ||\n\t\t    bfregi->sys_pages[i] != MLX5_IB_INVALID_UAR_INDEX)\n\t\t\tmlx5_cmd_uar_dealloc(dev->mdev, bfregi->sys_pages[i],\n\t\t\t\t\t     context->devx_uid);\n}\n\nint mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)\n{\n\tint err = 0;\n\n\tmutex_lock(&dev->lb.mutex);\n\tif (td)\n\t\tdev->lb.user_td++;\n\tif (qp)\n\t\tdev->lb.qps++;\n\n\tif (dev->lb.user_td == 2 ||\n\t    dev->lb.qps == 1) {\n\t\tif (!dev->lb.enabled) {\n\t\t\terr = mlx5_nic_vport_update_local_lb(dev->mdev, true);\n\t\t\tdev->lb.enabled = true;\n\t\t}\n\t}\n\n\tmutex_unlock(&dev->lb.mutex);\n\n\treturn err;\n}\n\nvoid mlx5_ib_disable_lb(struct mlx5_ib_dev *dev, bool td, bool qp)\n{\n\tmutex_lock(&dev->lb.mutex);\n\tif (td)\n\t\tdev->lb.user_td--;\n\tif (qp)\n\t\tdev->lb.qps--;\n\n\tif (dev->lb.user_td == 1 &&\n\t    dev->lb.qps == 0) {\n\t\tif (dev->lb.enabled) {\n\t\t\tmlx5_nic_vport_update_local_lb(dev->mdev, false);\n\t\t\tdev->lb.enabled = false;\n\t\t}\n\t}\n\n\tmutex_unlock(&dev->lb.mutex);\n}\n\nstatic int mlx5_ib_alloc_transport_domain(struct mlx5_ib_dev *dev, u32 *tdn,\n\t\t\t\t\t  u16 uid)\n{\n\tint err;\n\n\tif (!MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))\n\t\treturn 0;\n\n\terr = mlx5_cmd_alloc_transport_domain(dev->mdev, tdn, uid);\n\tif (err)\n\t\treturn err;\n\n\tif ((MLX5_CAP_GEN(dev->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH) ||\n\t    (!MLX5_CAP_GEN(dev->mdev, disable_local_lb_uc) &&\n\t     !MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))\n\t\treturn err;\n\n\treturn mlx5_ib_enable_lb(dev, true, false);\n}\n\nstatic void mlx5_ib_dealloc_transport_domain(struct mlx5_ib_dev *dev, u32 tdn,\n\t\t\t\t\t     u16 uid)\n{\n\tif (!MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))\n\t\treturn;\n\n\tmlx5_cmd_dealloc_transport_domain(dev->mdev, tdn, uid);\n\n\tif ((MLX5_CAP_GEN(dev->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH) ||\n\t    (!MLX5_CAP_GEN(dev->mdev, disable_local_lb_uc) &&\n\t     !MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))\n\t\treturn;\n\n\tmlx5_ib_disable_lb(dev, true, false);\n}\n\nstatic int set_ucontext_resp(struct ib_ucontext *uctx,\n\t\t\t     struct mlx5_ib_alloc_ucontext_resp *resp)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_ib_ucontext *context = to_mucontext(uctx);\n\tstruct mlx5_bfreg_info *bfregi = &context->bfregi;\n\n\tif (MLX5_CAP_GEN(dev->mdev, dump_fill_mkey)) {\n\t\tresp->dump_fill_mkey = dev->mkeys.dump_fill_mkey;\n\t\tresp->comp_mask |=\n\t\t\tMLX5_IB_ALLOC_UCONTEXT_RESP_MASK_DUMP_FILL_MKEY;\n\t}\n\n\tresp->qp_tab_size = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp);\n\tif (dev->wc_support)\n\t\tresp->bf_reg_size = 1 << MLX5_CAP_GEN(dev->mdev,\n\t\t\t\t\t\t      log_bf_reg_size);\n\tresp->cache_line_size = cache_line_size();\n\tresp->max_sq_desc_sz = MLX5_CAP_GEN(dev->mdev, max_wqe_sz_sq);\n\tresp->max_rq_desc_sz = MLX5_CAP_GEN(dev->mdev, max_wqe_sz_rq);\n\tresp->max_send_wqebb = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp_sz);\n\tresp->max_recv_wr = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp_sz);\n\tresp->max_srq_recv_wr = 1 << MLX5_CAP_GEN(dev->mdev, log_max_srq_sz);\n\tresp->cqe_version = context->cqe_version;\n\tresp->log_uar_size = MLX5_CAP_GEN(dev->mdev, uar_4k) ?\n\t\t\t\tMLX5_ADAPTER_PAGE_SHIFT : PAGE_SHIFT;\n\tresp->num_uars_per_page = MLX5_CAP_GEN(dev->mdev, uar_4k) ?\n\t\t\t\t\tMLX5_CAP_GEN(dev->mdev,\n\t\t\t\t\t\t     num_of_uars_per_page) : 1;\n\tresp->tot_bfregs = bfregi->lib_uar_dyn ? 0 :\n\t\t\tbfregi->total_num_bfregs - bfregi->num_dyn_bfregs;\n\tresp->num_ports = dev->num_ports;\n\tresp->cmds_supp_uhw |= MLX5_USER_CMDS_SUPP_UHW_QUERY_DEVICE |\n\t\t\t\t      MLX5_USER_CMDS_SUPP_UHW_CREATE_AH;\n\n\tif (mlx5_ib_port_link_layer(ibdev, 1) == IB_LINK_LAYER_ETHERNET) {\n\t\tmlx5_query_min_inline(dev->mdev, &resp->eth_min_inline);\n\t\tresp->eth_min_inline++;\n\t}\n\n\tif (dev->mdev->clock_info)\n\t\tresp->clock_info_versions = BIT(MLX5_IB_CLOCK_INFO_V1);\n\n\t \n\tif (PAGE_SIZE <= 4096) {\n\t\tresp->comp_mask |=\n\t\t\tMLX5_IB_ALLOC_UCONTEXT_RESP_MASK_CORE_CLOCK_OFFSET;\n\t\tresp->hca_core_clock_offset =\n\t\t\toffsetof(struct mlx5_init_seg,\n\t\t\t\t internal_timer_h) % PAGE_SIZE;\n\t}\n\n\tif (MLX5_CAP_GEN(dev->mdev, ece_support))\n\t\tresp->comp_mask |= MLX5_IB_ALLOC_UCONTEXT_RESP_MASK_ECE;\n\n\tif (rt_supported(MLX5_CAP_GEN(dev->mdev, sq_ts_format)) &&\n\t    rt_supported(MLX5_CAP_GEN(dev->mdev, rq_ts_format)) &&\n\t    rt_supported(MLX5_CAP_ROCE(dev->mdev, qp_ts_format)))\n\t\tresp->comp_mask |=\n\t\t\tMLX5_IB_ALLOC_UCONTEXT_RESP_MASK_REAL_TIME_TS;\n\n\tresp->num_dyn_bfregs = bfregi->num_dyn_bfregs;\n\n\tif (MLX5_CAP_GEN(dev->mdev, drain_sigerr))\n\t\tresp->comp_mask |= MLX5_IB_ALLOC_UCONTEXT_RESP_MASK_SQD2RTS;\n\n\tresp->comp_mask |=\n\t\tMLX5_IB_ALLOC_UCONTEXT_RESP_MASK_MKEY_UPDATE_TAG;\n\n\treturn 0;\n}\n\nstatic int mlx5_ib_alloc_ucontext(struct ib_ucontext *uctx,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tstruct mlx5_ib_alloc_ucontext_req_v2 req = {};\n\tstruct mlx5_ib_alloc_ucontext_resp resp = {};\n\tstruct mlx5_ib_ucontext *context = to_mucontext(uctx);\n\tstruct mlx5_bfreg_info *bfregi;\n\tint ver;\n\tint err;\n\tsize_t min_req_v2 = offsetof(struct mlx5_ib_alloc_ucontext_req_v2,\n\t\t\t\t     max_cqe_version);\n\tbool lib_uar_4k;\n\tbool lib_uar_dyn;\n\n\tif (!dev->ib_active)\n\t\treturn -EAGAIN;\n\n\tif (udata->inlen == sizeof(struct mlx5_ib_alloc_ucontext_req))\n\t\tver = 0;\n\telse if (udata->inlen >= min_req_v2)\n\t\tver = 2;\n\telse\n\t\treturn -EINVAL;\n\n\terr = ib_copy_from_udata(&req, udata, min(udata->inlen, sizeof(req)));\n\tif (err)\n\t\treturn err;\n\n\tif (req.flags & ~MLX5_IB_ALLOC_UCTX_DEVX)\n\t\treturn -EOPNOTSUPP;\n\n\tif (req.comp_mask || req.reserved0 || req.reserved1 || req.reserved2)\n\t\treturn -EOPNOTSUPP;\n\n\treq.total_num_bfregs = ALIGN(req.total_num_bfregs,\n\t\t\t\t    MLX5_NON_FP_BFREGS_PER_UAR);\n\tif (req.num_low_latency_bfregs > req.total_num_bfregs - 1)\n\t\treturn -EINVAL;\n\n\tif (req.flags & MLX5_IB_ALLOC_UCTX_DEVX) {\n\t\terr = mlx5_ib_devx_create(dev, true);\n\t\tif (err < 0)\n\t\t\tgoto out_ctx;\n\t\tcontext->devx_uid = err;\n\t}\n\n\tlib_uar_4k = req.lib_caps & MLX5_LIB_CAP_4K_UAR;\n\tlib_uar_dyn = req.lib_caps & MLX5_LIB_CAP_DYN_UAR;\n\tbfregi = &context->bfregi;\n\n\tif (lib_uar_dyn) {\n\t\tbfregi->lib_uar_dyn = lib_uar_dyn;\n\t\tgoto uar_done;\n\t}\n\n\t \n\terr = calc_total_bfregs(dev, lib_uar_4k, &req, bfregi);\n\tif (err)\n\t\tgoto out_devx;\n\n\tmutex_init(&bfregi->lock);\n\tbfregi->lib_uar_4k = lib_uar_4k;\n\tbfregi->count = kcalloc(bfregi->total_num_bfregs, sizeof(*bfregi->count),\n\t\t\t\tGFP_KERNEL);\n\tif (!bfregi->count) {\n\t\terr = -ENOMEM;\n\t\tgoto out_devx;\n\t}\n\n\tbfregi->sys_pages = kcalloc(bfregi->num_sys_pages,\n\t\t\t\t    sizeof(*bfregi->sys_pages),\n\t\t\t\t    GFP_KERNEL);\n\tif (!bfregi->sys_pages) {\n\t\terr = -ENOMEM;\n\t\tgoto out_count;\n\t}\n\n\terr = allocate_uars(dev, context);\n\tif (err)\n\t\tgoto out_sys_pages;\n\nuar_done:\n\terr = mlx5_ib_alloc_transport_domain(dev, &context->tdn,\n\t\t\t\t\t     context->devx_uid);\n\tif (err)\n\t\tgoto out_uars;\n\n\tINIT_LIST_HEAD(&context->db_page_list);\n\tmutex_init(&context->db_page_mutex);\n\n\tcontext->cqe_version = min_t(__u8,\n\t\t\t\t (__u8)MLX5_CAP_GEN(dev->mdev, cqe_version),\n\t\t\t\t req.max_cqe_version);\n\n\terr = set_ucontext_resp(uctx, &resp);\n\tif (err)\n\t\tgoto out_mdev;\n\n\tresp.response_length = min(udata->outlen, sizeof(resp));\n\terr = ib_copy_to_udata(udata, &resp, resp.response_length);\n\tif (err)\n\t\tgoto out_mdev;\n\n\tbfregi->ver = ver;\n\tbfregi->num_low_latency_bfregs = req.num_low_latency_bfregs;\n\tcontext->lib_caps = req.lib_caps;\n\tprint_lib_caps(dev, context->lib_caps);\n\n\tif (mlx5_ib_lag_should_assign_affinity(dev)) {\n\t\tu32 port = mlx5_core_native_port_num(dev->mdev) - 1;\n\n\t\tatomic_set(&context->tx_port_affinity,\n\t\t\t   atomic_add_return(\n\t\t\t\t   1, &dev->port[port].roce.tx_port_affinity));\n\t}\n\n\treturn 0;\n\nout_mdev:\n\tmlx5_ib_dealloc_transport_domain(dev, context->tdn, context->devx_uid);\n\nout_uars:\n\tdeallocate_uars(dev, context);\n\nout_sys_pages:\n\tkfree(bfregi->sys_pages);\n\nout_count:\n\tkfree(bfregi->count);\n\nout_devx:\n\tif (req.flags & MLX5_IB_ALLOC_UCTX_DEVX)\n\t\tmlx5_ib_devx_destroy(dev, context->devx_uid);\n\nout_ctx:\n\treturn err;\n}\n\nstatic int mlx5_ib_query_ucontext(struct ib_ucontext *ibcontext,\n\t\t\t\t  struct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_ib_alloc_ucontext_resp uctx_resp = {};\n\tint ret;\n\n\tret = set_ucontext_resp(ibcontext, &uctx_resp);\n\tif (ret)\n\t\treturn ret;\n\n\tuctx_resp.response_length =\n\t\tmin_t(size_t,\n\t\t      uverbs_attr_get_len(attrs,\n\t\t\t\tMLX5_IB_ATTR_QUERY_CONTEXT_RESP_UCTX),\n\t\t      sizeof(uctx_resp));\n\n\tret = uverbs_copy_to_struct_or_zero(attrs,\n\t\t\t\t\tMLX5_IB_ATTR_QUERY_CONTEXT_RESP_UCTX,\n\t\t\t\t\t&uctx_resp,\n\t\t\t\t\tsizeof(uctx_resp));\n\treturn ret;\n}\n\nstatic void mlx5_ib_dealloc_ucontext(struct ib_ucontext *ibcontext)\n{\n\tstruct mlx5_ib_ucontext *context = to_mucontext(ibcontext);\n\tstruct mlx5_ib_dev *dev = to_mdev(ibcontext->device);\n\tstruct mlx5_bfreg_info *bfregi;\n\n\tbfregi = &context->bfregi;\n\tmlx5_ib_dealloc_transport_domain(dev, context->tdn, context->devx_uid);\n\n\tdeallocate_uars(dev, context);\n\tkfree(bfregi->sys_pages);\n\tkfree(bfregi->count);\n\n\tif (context->devx_uid)\n\t\tmlx5_ib_devx_destroy(dev, context->devx_uid);\n}\n\nstatic phys_addr_t uar_index2pfn(struct mlx5_ib_dev *dev,\n\t\t\t\t int uar_idx)\n{\n\tint fw_uars_per_page;\n\n\tfw_uars_per_page = MLX5_CAP_GEN(dev->mdev, uar_4k) ? MLX5_UARS_IN_PAGE : 1;\n\n\treturn (dev->mdev->bar_addr >> PAGE_SHIFT) + uar_idx / fw_uars_per_page;\n}\n\nstatic u64 uar_index2paddress(struct mlx5_ib_dev *dev,\n\t\t\t\t int uar_idx)\n{\n\tunsigned int fw_uars_per_page;\n\n\tfw_uars_per_page = MLX5_CAP_GEN(dev->mdev, uar_4k) ?\n\t\t\t\tMLX5_UARS_IN_PAGE : 1;\n\n\treturn (dev->mdev->bar_addr + (uar_idx / fw_uars_per_page) * PAGE_SIZE);\n}\n\nstatic int get_command(unsigned long offset)\n{\n\treturn (offset >> MLX5_IB_MMAP_CMD_SHIFT) & MLX5_IB_MMAP_CMD_MASK;\n}\n\nstatic int get_arg(unsigned long offset)\n{\n\treturn offset & ((1 << MLX5_IB_MMAP_CMD_SHIFT) - 1);\n}\n\nstatic int get_index(unsigned long offset)\n{\n\treturn get_arg(offset);\n}\n\n \nstatic int get_extended_index(unsigned long offset)\n{\n\treturn get_arg(offset) | ((offset >> 16) & 0xff) << 8;\n}\n\n\nstatic void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)\n{\n}\n\nstatic inline char *mmap_cmd2str(enum mlx5_ib_mmap_cmd cmd)\n{\n\tswitch (cmd) {\n\tcase MLX5_IB_MMAP_WC_PAGE:\n\t\treturn \"WC\";\n\tcase MLX5_IB_MMAP_REGULAR_PAGE:\n\t\treturn \"best effort WC\";\n\tcase MLX5_IB_MMAP_NC_PAGE:\n\t\treturn \"NC\";\n\tcase MLX5_IB_MMAP_DEVICE_MEM:\n\t\treturn \"Device Memory\";\n\tdefault:\n\t\treturn \"Unknown\";\n\t}\n}\n\nstatic int mlx5_ib_mmap_clock_info_page(struct mlx5_ib_dev *dev,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tstruct mlx5_ib_ucontext *context)\n{\n\tif ((vma->vm_end - vma->vm_start != PAGE_SIZE) ||\n\t    !(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tif (get_index(vma->vm_pgoff) != MLX5_IB_CLOCK_INFO_V1)\n\t\treturn -EOPNOTSUPP;\n\n\tif (vma->vm_flags & (VM_WRITE | VM_EXEC))\n\t\treturn -EPERM;\n\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\tif (!dev->mdev->clock_info)\n\t\treturn -EOPNOTSUPP;\n\n\treturn vm_insert_page(vma, vma->vm_start,\n\t\t\t      virt_to_page(dev->mdev->clock_info));\n}\n\nstatic void mlx5_ib_mmap_free(struct rdma_user_mmap_entry *entry)\n{\n\tstruct mlx5_user_mmap_entry *mentry = to_mmmap(entry);\n\tstruct mlx5_ib_dev *dev = to_mdev(entry->ucontext->device);\n\tstruct mlx5_var_table *var_table = &dev->var_table;\n\tstruct mlx5_ib_ucontext *context = to_mucontext(entry->ucontext);\n\n\tswitch (mentry->mmap_flag) {\n\tcase MLX5_IB_MMAP_TYPE_MEMIC:\n\tcase MLX5_IB_MMAP_TYPE_MEMIC_OP:\n\t\tmlx5_ib_dm_mmap_free(dev, mentry);\n\t\tbreak;\n\tcase MLX5_IB_MMAP_TYPE_VAR:\n\t\tmutex_lock(&var_table->bitmap_lock);\n\t\tclear_bit(mentry->page_idx, var_table->bitmap);\n\t\tmutex_unlock(&var_table->bitmap_lock);\n\t\tkfree(mentry);\n\t\tbreak;\n\tcase MLX5_IB_MMAP_TYPE_UAR_WC:\n\tcase MLX5_IB_MMAP_TYPE_UAR_NC:\n\t\tmlx5_cmd_uar_dealloc(dev->mdev, mentry->page_idx,\n\t\t\t\t     context->devx_uid);\n\t\tkfree(mentry);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(true);\n\t}\n}\n\nstatic int uar_mmap(struct mlx5_ib_dev *dev, enum mlx5_ib_mmap_cmd cmd,\n\t\t    struct vm_area_struct *vma,\n\t\t    struct mlx5_ib_ucontext *context)\n{\n\tstruct mlx5_bfreg_info *bfregi = &context->bfregi;\n\tint err;\n\tunsigned long idx;\n\tphys_addr_t pfn;\n\tpgprot_t prot;\n\tu32 bfreg_dyn_idx = 0;\n\tu32 uar_index;\n\tint dyn_uar = (cmd == MLX5_IB_MMAP_ALLOC_WC);\n\tint max_valid_idx = dyn_uar ? bfregi->num_sys_pages :\n\t\t\t\tbfregi->num_static_sys_pages;\n\n\tif (bfregi->lib_uar_dyn)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (dyn_uar)\n\t\tidx = get_extended_index(vma->vm_pgoff) + bfregi->num_static_sys_pages;\n\telse\n\t\tidx = get_index(vma->vm_pgoff);\n\n\tif (idx >= max_valid_idx) {\n\t\tmlx5_ib_warn(dev, \"invalid uar index %lu, max=%d\\n\",\n\t\t\t     idx, max_valid_idx);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (cmd) {\n\tcase MLX5_IB_MMAP_WC_PAGE:\n\tcase MLX5_IB_MMAP_ALLOC_WC:\n\tcase MLX5_IB_MMAP_REGULAR_PAGE:\n\t\t \n\t\tprot = pgprot_writecombine(vma->vm_page_prot);\n\t\tbreak;\n\tcase MLX5_IB_MMAP_NC_PAGE:\n\t\tprot = pgprot_noncached(vma->vm_page_prot);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (dyn_uar) {\n\t\tint uars_per_page;\n\n\t\tuars_per_page = get_uars_per_sys_page(dev, bfregi->lib_uar_4k);\n\t\tbfreg_dyn_idx = idx * (uars_per_page * MLX5_NON_FP_BFREGS_PER_UAR);\n\t\tif (bfreg_dyn_idx >= bfregi->total_num_bfregs) {\n\t\t\tmlx5_ib_warn(dev, \"invalid bfreg_dyn_idx %u, max=%u\\n\",\n\t\t\t\t     bfreg_dyn_idx, bfregi->total_num_bfregs);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmutex_lock(&bfregi->lock);\n\t\t \n\t\tif (bfregi->count[bfreg_dyn_idx]) {\n\t\t\tmlx5_ib_warn(dev, \"wrong offset, idx %lu is busy, bfregn=%u\\n\", idx, bfreg_dyn_idx);\n\t\t\tmutex_unlock(&bfregi->lock);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tbfregi->count[bfreg_dyn_idx]++;\n\t\tmutex_unlock(&bfregi->lock);\n\n\t\terr = mlx5_cmd_uar_alloc(dev->mdev, &uar_index,\n\t\t\t\t\t context->devx_uid);\n\t\tif (err) {\n\t\t\tmlx5_ib_warn(dev, \"UAR alloc failed\\n\");\n\t\t\tgoto free_bfreg;\n\t\t}\n\t} else {\n\t\tuar_index = bfregi->sys_pages[idx];\n\t}\n\n\tpfn = uar_index2pfn(dev, uar_index);\n\tmlx5_ib_dbg(dev, \"uar idx 0x%lx, pfn %pa\\n\", idx, &pfn);\n\n\terr = rdma_user_mmap_io(&context->ibucontext, vma, pfn, PAGE_SIZE,\n\t\t\t\tprot, NULL);\n\tif (err) {\n\t\tmlx5_ib_err(dev,\n\t\t\t    \"rdma_user_mmap_io failed with error=%d, mmap_cmd=%s\\n\",\n\t\t\t    err, mmap_cmd2str(cmd));\n\t\tgoto err;\n\t}\n\n\tif (dyn_uar)\n\t\tbfregi->sys_pages[idx] = uar_index;\n\treturn 0;\n\nerr:\n\tif (!dyn_uar)\n\t\treturn err;\n\n\tmlx5_cmd_uar_dealloc(dev->mdev, idx, context->devx_uid);\n\nfree_bfreg:\n\tmlx5_ib_free_bfreg(dev, bfregi, bfreg_dyn_idx);\n\n\treturn err;\n}\n\nstatic unsigned long mlx5_vma_to_pgoff(struct vm_area_struct *vma)\n{\n\tunsigned long idx;\n\tu8 command;\n\n\tcommand = get_command(vma->vm_pgoff);\n\tidx = get_extended_index(vma->vm_pgoff);\n\n\treturn (command << 16 | idx);\n}\n\nstatic int mlx5_ib_mmap_offset(struct mlx5_ib_dev *dev,\n\t\t\t       struct vm_area_struct *vma,\n\t\t\t       struct ib_ucontext *ucontext)\n{\n\tstruct mlx5_user_mmap_entry *mentry;\n\tstruct rdma_user_mmap_entry *entry;\n\tunsigned long pgoff;\n\tpgprot_t prot;\n\tphys_addr_t pfn;\n\tint ret;\n\n\tpgoff = mlx5_vma_to_pgoff(vma);\n\tentry = rdma_user_mmap_entry_get_pgoff(ucontext, pgoff);\n\tif (!entry)\n\t\treturn -EINVAL;\n\n\tmentry = to_mmmap(entry);\n\tpfn = (mentry->address >> PAGE_SHIFT);\n\tif (mentry->mmap_flag == MLX5_IB_MMAP_TYPE_VAR ||\n\t    mentry->mmap_flag == MLX5_IB_MMAP_TYPE_UAR_NC)\n\t\tprot = pgprot_noncached(vma->vm_page_prot);\n\telse\n\t\tprot = pgprot_writecombine(vma->vm_page_prot);\n\tret = rdma_user_mmap_io(ucontext, vma, pfn,\n\t\t\t\tentry->npages * PAGE_SIZE,\n\t\t\t\tprot,\n\t\t\t\tentry);\n\trdma_user_mmap_entry_put(&mentry->rdma_entry);\n\treturn ret;\n}\n\nstatic u64 mlx5_entry_to_mmap_offset(struct mlx5_user_mmap_entry *entry)\n{\n\tu64 cmd = (entry->rdma_entry.start_pgoff >> 16) & 0xFFFF;\n\tu64 index = entry->rdma_entry.start_pgoff & 0xFFFF;\n\n\treturn (((index >> 8) << 16) | (cmd << MLX5_IB_MMAP_CMD_SHIFT) |\n\t\t(index & 0xFF)) << PAGE_SHIFT;\n}\n\nstatic int mlx5_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)\n{\n\tstruct mlx5_ib_ucontext *context = to_mucontext(ibcontext);\n\tstruct mlx5_ib_dev *dev = to_mdev(ibcontext->device);\n\tunsigned long command;\n\tphys_addr_t pfn;\n\n\tcommand = get_command(vma->vm_pgoff);\n\tswitch (command) {\n\tcase MLX5_IB_MMAP_WC_PAGE:\n\tcase MLX5_IB_MMAP_ALLOC_WC:\n\t\tif (!dev->wc_support)\n\t\t\treturn -EPERM;\n\t\tfallthrough;\n\tcase MLX5_IB_MMAP_NC_PAGE:\n\tcase MLX5_IB_MMAP_REGULAR_PAGE:\n\t\treturn uar_mmap(dev, command, vma, context);\n\n\tcase MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES:\n\t\treturn -ENOSYS;\n\n\tcase MLX5_IB_MMAP_CORE_CLOCK:\n\t\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tif (vma->vm_flags & VM_WRITE)\n\t\t\treturn -EPERM;\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\t\t \n\t\tif (PAGE_SIZE > 4096)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tpfn = (dev->mdev->iseg_base +\n\t\t       offsetof(struct mlx5_init_seg, internal_timer_h)) >>\n\t\t\tPAGE_SHIFT;\n\t\treturn rdma_user_mmap_io(&context->ibucontext, vma, pfn,\n\t\t\t\t\t PAGE_SIZE,\n\t\t\t\t\t pgprot_noncached(vma->vm_page_prot),\n\t\t\t\t\t NULL);\n\tcase MLX5_IB_MMAP_CLOCK_INFO:\n\t\treturn mlx5_ib_mmap_clock_info_page(dev, vma, context);\n\n\tdefault:\n\t\treturn mlx5_ib_mmap_offset(dev, vma, ibcontext);\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5_ib_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct mlx5_ib_pd *pd = to_mpd(ibpd);\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct mlx5_ib_alloc_pd_resp resp;\n\tint err;\n\tu32 out[MLX5_ST_SZ_DW(alloc_pd_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(alloc_pd_in)] = {};\n\tu16 uid = 0;\n\tstruct mlx5_ib_ucontext *context = rdma_udata_to_drv_context(\n\t\tudata, struct mlx5_ib_ucontext, ibucontext);\n\n\tuid = context ? context->devx_uid : 0;\n\tMLX5_SET(alloc_pd_in, in, opcode, MLX5_CMD_OP_ALLOC_PD);\n\tMLX5_SET(alloc_pd_in, in, uid, uid);\n\terr = mlx5_cmd_exec_inout(to_mdev(ibdev)->mdev, alloc_pd, in, out);\n\tif (err)\n\t\treturn err;\n\n\tpd->pdn = MLX5_GET(alloc_pd_out, out, pd);\n\tpd->uid = uid;\n\tif (udata) {\n\t\tresp.pdn = pd->pdn;\n\t\tif (ib_copy_to_udata(udata, &resp, sizeof(resp))) {\n\t\t\tmlx5_cmd_dealloc_pd(to_mdev(ibdev)->mdev, pd->pdn, uid);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5_ib_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata)\n{\n\tstruct mlx5_ib_dev *mdev = to_mdev(pd->device);\n\tstruct mlx5_ib_pd *mpd = to_mpd(pd);\n\n\treturn mlx5_cmd_dealloc_pd(mdev->mdev, mpd->pdn, mpd->uid);\n}\n\nstatic int mlx5_ib_mcg_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibqp->device);\n\tstruct mlx5_ib_qp *mqp = to_mqp(ibqp);\n\tint err;\n\tu16 uid;\n\n\tuid = ibqp->pd ?\n\t\tto_mpd(ibqp->pd)->uid : 0;\n\n\tif (mqp->flags & IB_QP_CREATE_SOURCE_QPN) {\n\t\tmlx5_ib_dbg(dev, \"Attaching a multi cast group to underlay QP is not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\terr = mlx5_cmd_attach_mcg(dev->mdev, gid, ibqp->qp_num, uid);\n\tif (err)\n\t\tmlx5_ib_warn(dev, \"failed attaching QPN 0x%x, MGID %pI6\\n\",\n\t\t\t     ibqp->qp_num, gid->raw);\n\n\treturn err;\n}\n\nstatic int mlx5_ib_mcg_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibqp->device);\n\tint err;\n\tu16 uid;\n\n\tuid = ibqp->pd ?\n\t\tto_mpd(ibqp->pd)->uid : 0;\n\terr = mlx5_cmd_detach_mcg(dev->mdev, gid, ibqp->qp_num, uid);\n\tif (err)\n\t\tmlx5_ib_warn(dev, \"failed detaching QPN 0x%x, MGID %pI6\\n\",\n\t\t\t     ibqp->qp_num, gid->raw);\n\n\treturn err;\n}\n\nstatic int init_node_data(struct mlx5_ib_dev *dev)\n{\n\tint err;\n\n\terr = mlx5_query_node_desc(dev, dev->ib_dev.node_desc);\n\tif (err)\n\t\treturn err;\n\n\tdev->mdev->rev_id = dev->mdev->pdev->revision;\n\n\treturn mlx5_query_node_guid(dev, &dev->ib_dev.node_guid);\n}\n\nstatic ssize_t fw_pages_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx5_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", dev->mdev->priv.fw_pages);\n}\nstatic DEVICE_ATTR_RO(fw_pages);\n\nstatic ssize_t reg_pages_show(struct device *device,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx5_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", atomic_read(&dev->mdev->priv.reg_pages));\n}\nstatic DEVICE_ATTR_RO(reg_pages);\n\nstatic ssize_t hca_type_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx5_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"MT%d\\n\", dev->mdev->pdev->device);\n}\nstatic DEVICE_ATTR_RO(hca_type);\n\nstatic ssize_t hw_rev_show(struct device *device,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx5_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%x\\n\", dev->mdev->rev_id);\n}\nstatic DEVICE_ATTR_RO(hw_rev);\n\nstatic ssize_t board_id_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\trdma_device_to_drv_device(device, struct mlx5_ib_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%.*s\\n\", MLX5_BOARD_ID_LEN,\n\t\t\t  dev->mdev->board_id);\n}\nstatic DEVICE_ATTR_RO(board_id);\n\nstatic struct attribute *mlx5_class_attributes[] = {\n\t&dev_attr_hw_rev.attr,\n\t&dev_attr_hca_type.attr,\n\t&dev_attr_board_id.attr,\n\t&dev_attr_fw_pages.attr,\n\t&dev_attr_reg_pages.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group mlx5_attr_group = {\n\t.attrs = mlx5_class_attributes,\n};\n\nstatic void pkey_change_handler(struct work_struct *work)\n{\n\tstruct mlx5_ib_port_resources *ports =\n\t\tcontainer_of(work, struct mlx5_ib_port_resources,\n\t\t\t     pkey_change_work);\n\n\tif (!ports->gsi)\n\t\t \n\t\treturn;\n\n\tmlx5_ib_gsi_pkey_change(ports->gsi);\n}\n\nstatic void mlx5_ib_handle_internal_error(struct mlx5_ib_dev *ibdev)\n{\n\tstruct mlx5_ib_qp *mqp;\n\tstruct mlx5_ib_cq *send_mcq, *recv_mcq;\n\tstruct mlx5_core_cq *mcq;\n\tstruct list_head cq_armed_list;\n\tunsigned long flags_qp;\n\tunsigned long flags_cq;\n\tunsigned long flags;\n\n\tINIT_LIST_HEAD(&cq_armed_list);\n\n\t \n\tspin_lock_irqsave(&ibdev->reset_flow_resource_lock, flags);\n\tlist_for_each_entry(mqp, &ibdev->qp_list, qps_list) {\n\t\tspin_lock_irqsave(&mqp->sq.lock, flags_qp);\n\t\tif (mqp->sq.tail != mqp->sq.head) {\n\t\t\tsend_mcq = to_mcq(mqp->ibqp.send_cq);\n\t\t\tspin_lock_irqsave(&send_mcq->lock, flags_cq);\n\t\t\tif (send_mcq->mcq.comp &&\n\t\t\t    mqp->ibqp.send_cq->comp_handler) {\n\t\t\t\tif (!send_mcq->mcq.reset_notify_added) {\n\t\t\t\t\tsend_mcq->mcq.reset_notify_added = 1;\n\t\t\t\t\tlist_add_tail(&send_mcq->mcq.reset_notify,\n\t\t\t\t\t\t      &cq_armed_list);\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&send_mcq->lock, flags_cq);\n\t\t}\n\t\tspin_unlock_irqrestore(&mqp->sq.lock, flags_qp);\n\t\tspin_lock_irqsave(&mqp->rq.lock, flags_qp);\n\t\t \n\t\tif (!mqp->ibqp.srq) {\n\t\t\tif (mqp->rq.tail != mqp->rq.head) {\n\t\t\t\trecv_mcq = to_mcq(mqp->ibqp.recv_cq);\n\t\t\t\tspin_lock_irqsave(&recv_mcq->lock, flags_cq);\n\t\t\t\tif (recv_mcq->mcq.comp &&\n\t\t\t\t    mqp->ibqp.recv_cq->comp_handler) {\n\t\t\t\t\tif (!recv_mcq->mcq.reset_notify_added) {\n\t\t\t\t\t\trecv_mcq->mcq.reset_notify_added = 1;\n\t\t\t\t\t\tlist_add_tail(&recv_mcq->mcq.reset_notify,\n\t\t\t\t\t\t\t      &cq_armed_list);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tspin_unlock_irqrestore(&recv_mcq->lock,\n\t\t\t\t\t\t       flags_cq);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&mqp->rq.lock, flags_qp);\n\t}\n\t \n\tlist_for_each_entry(mcq, &cq_armed_list, reset_notify) {\n\t\tmcq->comp(mcq, NULL);\n\t}\n\tspin_unlock_irqrestore(&ibdev->reset_flow_resource_lock, flags);\n}\n\nstatic void delay_drop_handler(struct work_struct *work)\n{\n\tint err;\n\tstruct mlx5_ib_delay_drop *delay_drop =\n\t\tcontainer_of(work, struct mlx5_ib_delay_drop,\n\t\t\t     delay_drop_work);\n\n\tatomic_inc(&delay_drop->events_cnt);\n\n\tmutex_lock(&delay_drop->lock);\n\terr = mlx5_core_set_delay_drop(delay_drop->dev, delay_drop->timeout);\n\tif (err) {\n\t\tmlx5_ib_warn(delay_drop->dev, \"Failed to set delay drop, timeout=%u\\n\",\n\t\t\t     delay_drop->timeout);\n\t\tdelay_drop->activate = false;\n\t}\n\tmutex_unlock(&delay_drop->lock);\n}\n\nstatic void handle_general_event(struct mlx5_ib_dev *ibdev, struct mlx5_eqe *eqe,\n\t\t\t\t struct ib_event *ibev)\n{\n\tu32 port = (eqe->data.port.port >> 4) & 0xf;\n\n\tswitch (eqe->sub_type) {\n\tcase MLX5_GENERAL_SUBTYPE_DELAY_DROP_TIMEOUT:\n\t\tif (mlx5_ib_port_link_layer(&ibdev->ib_dev, port) ==\n\t\t\t\t\t    IB_LINK_LAYER_ETHERNET)\n\t\t\tschedule_work(&ibdev->delay_drop.delay_drop_work);\n\t\tbreak;\n\tdefault:  \n\t\treturn;\n\t}\n}\n\nstatic int handle_port_change(struct mlx5_ib_dev *ibdev, struct mlx5_eqe *eqe,\n\t\t\t      struct ib_event *ibev)\n{\n\tu32 port = (eqe->data.port.port >> 4) & 0xf;\n\n\tibev->element.port_num = port;\n\n\tswitch (eqe->sub_type) {\n\tcase MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:\n\tcase MLX5_PORT_CHANGE_SUBTYPE_DOWN:\n\tcase MLX5_PORT_CHANGE_SUBTYPE_INITIALIZED:\n\t\t \n\t\tif (mlx5_ib_port_link_layer(&ibdev->ib_dev, port) ==\n\t\t\t\t\t    IB_LINK_LAYER_ETHERNET)\n\t\t\treturn -EINVAL;\n\n\t\tibev->event = (eqe->sub_type == MLX5_PORT_CHANGE_SUBTYPE_ACTIVE) ?\n\t\t\t\tIB_EVENT_PORT_ACTIVE : IB_EVENT_PORT_ERR;\n\t\tbreak;\n\n\tcase MLX5_PORT_CHANGE_SUBTYPE_LID:\n\t\tibev->event = IB_EVENT_LID_CHANGE;\n\t\tbreak;\n\n\tcase MLX5_PORT_CHANGE_SUBTYPE_PKEY:\n\t\tibev->event = IB_EVENT_PKEY_CHANGE;\n\t\tschedule_work(&ibdev->devr.ports[port - 1].pkey_change_work);\n\t\tbreak;\n\n\tcase MLX5_PORT_CHANGE_SUBTYPE_GUID:\n\t\tibev->event = IB_EVENT_GID_CHANGE;\n\t\tbreak;\n\n\tcase MLX5_PORT_CHANGE_SUBTYPE_CLIENT_REREG:\n\t\tibev->event = IB_EVENT_CLIENT_REREGISTER;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void mlx5_ib_handle_event(struct work_struct *_work)\n{\n\tstruct mlx5_ib_event_work *work =\n\t\tcontainer_of(_work, struct mlx5_ib_event_work, work);\n\tstruct mlx5_ib_dev *ibdev;\n\tstruct ib_event ibev;\n\tbool fatal = false;\n\n\tif (work->is_slave) {\n\t\tibdev = mlx5_ib_get_ibdev_from_mpi(work->mpi);\n\t\tif (!ibdev)\n\t\t\tgoto out;\n\t} else {\n\t\tibdev = work->dev;\n\t}\n\n\tswitch (work->event) {\n\tcase MLX5_DEV_EVENT_SYS_ERROR:\n\t\tibev.event = IB_EVENT_DEVICE_FATAL;\n\t\tmlx5_ib_handle_internal_error(ibdev);\n\t\tibev.element.port_num  = (u8)(unsigned long)work->param;\n\t\tfatal = true;\n\t\tbreak;\n\tcase MLX5_EVENT_TYPE_PORT_CHANGE:\n\t\tif (handle_port_change(ibdev, work->param, &ibev))\n\t\t\tgoto out;\n\t\tbreak;\n\tcase MLX5_EVENT_TYPE_GENERAL_EVENT:\n\t\thandle_general_event(ibdev, work->param, &ibev);\n\t\tfallthrough;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tibev.device = &ibdev->ib_dev;\n\n\tif (!rdma_is_port_valid(&ibdev->ib_dev, ibev.element.port_num)) {\n\t\tmlx5_ib_warn(ibdev, \"warning: event on port %d\\n\",  ibev.element.port_num);\n\t\tgoto out;\n\t}\n\n\tif (ibdev->ib_active)\n\t\tib_dispatch_event(&ibev);\n\n\tif (fatal)\n\t\tibdev->ib_active = false;\nout:\n\tkfree(work);\n}\n\nstatic int mlx5_ib_event(struct notifier_block *nb,\n\t\t\t unsigned long event, void *param)\n{\n\tstruct mlx5_ib_event_work *work;\n\n\twork = kmalloc(sizeof(*work), GFP_ATOMIC);\n\tif (!work)\n\t\treturn NOTIFY_DONE;\n\n\tINIT_WORK(&work->work, mlx5_ib_handle_event);\n\twork->dev = container_of(nb, struct mlx5_ib_dev, mdev_events);\n\twork->is_slave = false;\n\twork->param = param;\n\twork->event = event;\n\n\tqueue_work(mlx5_ib_event_wq, &work->work);\n\n\treturn NOTIFY_OK;\n}\n\nstatic int mlx5_ib_event_slave_port(struct notifier_block *nb,\n\t\t\t\t    unsigned long event, void *param)\n{\n\tstruct mlx5_ib_event_work *work;\n\n\twork = kmalloc(sizeof(*work), GFP_ATOMIC);\n\tif (!work)\n\t\treturn NOTIFY_DONE;\n\n\tINIT_WORK(&work->work, mlx5_ib_handle_event);\n\twork->mpi = container_of(nb, struct mlx5_ib_multiport_info, mdev_events);\n\twork->is_slave = true;\n\twork->param = param;\n\twork->event = event;\n\tqueue_work(mlx5_ib_event_wq, &work->work);\n\n\treturn NOTIFY_OK;\n}\n\nstatic int set_has_smi_cap(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_hca_vport_context vport_ctx;\n\tint err;\n\tint port;\n\n\tif (MLX5_CAP_GEN(dev->mdev, port_type) != MLX5_CAP_PORT_TYPE_IB)\n\t\treturn 0;\n\n\tfor (port = 1; port <= dev->num_ports; port++) {\n\t\tif (!MLX5_CAP_GEN(dev->mdev, ib_virt)) {\n\t\t\tdev->port_caps[port - 1].has_smi = true;\n\t\t\tcontinue;\n\t\t}\n\t\terr = mlx5_query_hca_vport_context(dev->mdev, 0, port, 0,\n\t\t\t\t\t\t   &vport_ctx);\n\t\tif (err) {\n\t\t\tmlx5_ib_err(dev, \"query_hca_vport_context for port=%d failed %d\\n\",\n\t\t\t\t    port, err);\n\t\t\treturn err;\n\t\t}\n\t\tdev->port_caps[port - 1].has_smi = vport_ctx.has_smi;\n\t}\n\n\treturn 0;\n}\n\nstatic void get_ext_port_caps(struct mlx5_ib_dev *dev)\n{\n\tunsigned int port;\n\n\trdma_for_each_port (&dev->ib_dev, port)\n\t\tmlx5_query_ext_port_caps(dev, port);\n}\n\nstatic u8 mlx5_get_umr_fence(u8 umr_fence_cap)\n{\n\tswitch (umr_fence_cap) {\n\tcase MLX5_CAP_UMR_FENCE_NONE:\n\t\treturn MLX5_FENCE_MODE_NONE;\n\tcase MLX5_CAP_UMR_FENCE_SMALL:\n\t\treturn MLX5_FENCE_MODE_INITIATOR_SMALL;\n\tdefault:\n\t\treturn MLX5_FENCE_MODE_STRONG_ORDERING;\n\t}\n}\n\nstatic int mlx5_ib_dev_res_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_ib_resources *devr = &dev->devr;\n\tstruct ib_srq_init_attr attr;\n\tstruct ib_device *ibdev;\n\tstruct ib_cq_init_attr cq_attr = {.cqe = 1};\n\tint port;\n\tint ret = 0;\n\n\tibdev = &dev->ib_dev;\n\n\tif (!MLX5_CAP_GEN(dev->mdev, xrc))\n\t\treturn -EOPNOTSUPP;\n\n\tdevr->p0 = ib_alloc_pd(ibdev, 0);\n\tif (IS_ERR(devr->p0))\n\t\treturn PTR_ERR(devr->p0);\n\n\tdevr->c0 = ib_create_cq(ibdev, NULL, NULL, NULL, &cq_attr);\n\tif (IS_ERR(devr->c0)) {\n\t\tret = PTR_ERR(devr->c0);\n\t\tgoto error1;\n\t}\n\n\tret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn0, 0);\n\tif (ret)\n\t\tgoto error2;\n\n\tret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn1, 0);\n\tif (ret)\n\t\tgoto error3;\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.attr.max_sge = 1;\n\tattr.attr.max_wr = 1;\n\tattr.srq_type = IB_SRQT_XRC;\n\tattr.ext.cq = devr->c0;\n\n\tdevr->s0 = ib_create_srq(devr->p0, &attr);\n\tif (IS_ERR(devr->s0)) {\n\t\tret = PTR_ERR(devr->s0);\n\t\tgoto err_create;\n\t}\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.attr.max_sge = 1;\n\tattr.attr.max_wr = 1;\n\tattr.srq_type = IB_SRQT_BASIC;\n\n\tdevr->s1 = ib_create_srq(devr->p0, &attr);\n\tif (IS_ERR(devr->s1)) {\n\t\tret = PTR_ERR(devr->s1);\n\t\tgoto error6;\n\t}\n\n\tfor (port = 0; port < ARRAY_SIZE(devr->ports); ++port)\n\t\tINIT_WORK(&devr->ports[port].pkey_change_work,\n\t\t\t  pkey_change_handler);\n\n\treturn 0;\n\nerror6:\n\tib_destroy_srq(devr->s0);\nerr_create:\n\tmlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn1, 0);\nerror3:\n\tmlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn0, 0);\nerror2:\n\tib_destroy_cq(devr->c0);\nerror1:\n\tib_dealloc_pd(devr->p0);\n\treturn ret;\n}\n\nstatic void mlx5_ib_dev_res_cleanup(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_ib_resources *devr = &dev->devr;\n\tint port;\n\n\t \n\tfor (port = 0; port < ARRAY_SIZE(devr->ports); ++port)\n\t\tcancel_work_sync(&devr->ports[port].pkey_change_work);\n\n\tib_destroy_srq(devr->s1);\n\tib_destroy_srq(devr->s0);\n\tmlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn1, 0);\n\tmlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn0, 0);\n\tib_destroy_cq(devr->c0);\n\tib_dealloc_pd(devr->p0);\n}\n\nstatic u32 get_core_cap_flags(struct ib_device *ibdev,\n\t\t\t      struct mlx5_hca_vport_context *rep)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(ibdev, 1);\n\tu8 l3_type_cap = MLX5_CAP_ROCE(dev->mdev, l3_type);\n\tu8 roce_version_cap = MLX5_CAP_ROCE(dev->mdev, roce_version);\n\tbool raw_support = !mlx5_core_mp_enabled(dev->mdev);\n\tu32 ret = 0;\n\n\tif (rep->grh_required)\n\t\tret |= RDMA_CORE_CAP_IB_GRH_REQUIRED;\n\n\tif (ll == IB_LINK_LAYER_INFINIBAND)\n\t\treturn ret | RDMA_CORE_PORT_IBA_IB;\n\n\tif (raw_support)\n\t\tret |= RDMA_CORE_PORT_RAW_PACKET;\n\n\tif (!(l3_type_cap & MLX5_ROCE_L3_TYPE_IPV4_CAP))\n\t\treturn ret;\n\n\tif (!(l3_type_cap & MLX5_ROCE_L3_TYPE_IPV6_CAP))\n\t\treturn ret;\n\n\tif (roce_version_cap & MLX5_ROCE_VERSION_1_CAP)\n\t\tret |= RDMA_CORE_PORT_IBA_ROCE;\n\n\tif (roce_version_cap & MLX5_ROCE_VERSION_2_CAP)\n\t\tret |= RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;\n\n\treturn ret;\n}\n\nstatic int mlx5_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t       struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tstruct mlx5_ib_dev *dev = to_mdev(ibdev);\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(ibdev, port_num);\n\tstruct mlx5_hca_vport_context rep = {0};\n\tint err;\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (ll == IB_LINK_LAYER_INFINIBAND) {\n\t\terr = mlx5_query_hca_vport_context(dev->mdev, 0, port_num, 0,\n\t\t\t\t\t\t   &rep);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\timmutable->core_cap_flags = get_core_cap_flags(ibdev, &rep);\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\n\treturn 0;\n}\n\nstatic int mlx5_port_rep_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t\t   struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tint err;\n\n\timmutable->core_cap_flags = RDMA_CORE_PORT_RAW_PACKET;\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\timmutable->core_cap_flags = RDMA_CORE_PORT_RAW_PACKET;\n\n\treturn 0;\n}\n\nstatic void get_dev_fw_str(struct ib_device *ibdev, char *str)\n{\n\tstruct mlx5_ib_dev *dev =\n\t\tcontainer_of(ibdev, struct mlx5_ib_dev, ib_dev);\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%04d\",\n\t\t fw_rev_maj(dev->mdev), fw_rev_min(dev->mdev),\n\t\t fw_rev_sub(dev->mdev));\n}\n\nstatic int mlx5_eth_lag_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tstruct mlx5_flow_namespace *ns = mlx5_get_flow_namespace(mdev,\n\t\t\t\t\t\t\t\t MLX5_FLOW_NAMESPACE_LAG);\n\tstruct mlx5_flow_table *ft;\n\tint err;\n\n\tif (!ns || !mlx5_lag_is_active(mdev))\n\t\treturn 0;\n\n\terr = mlx5_cmd_create_vport_lag(mdev);\n\tif (err)\n\t\treturn err;\n\n\tft = mlx5_create_lag_demux_flow_table(ns, 0, 0);\n\tif (IS_ERR(ft)) {\n\t\terr = PTR_ERR(ft);\n\t\tgoto err_destroy_vport_lag;\n\t}\n\n\tdev->flow_db->lag_demux_ft = ft;\n\tdev->lag_ports = mlx5_lag_get_num_ports(mdev);\n\tdev->lag_active = true;\n\treturn 0;\n\nerr_destroy_vport_lag:\n\tmlx5_cmd_destroy_vport_lag(mdev);\n\treturn err;\n}\n\nstatic void mlx5_eth_lag_cleanup(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\n\tif (dev->lag_active) {\n\t\tdev->lag_active = false;\n\n\t\tmlx5_destroy_flow_table(dev->flow_db->lag_demux_ft);\n\t\tdev->flow_db->lag_demux_ft = NULL;\n\n\t\tmlx5_cmd_destroy_vport_lag(mdev);\n\t}\n}\n\nstatic void mlx5_netdev_notifier_register(struct mlx5_roce *roce,\n\t\t\t\t\t  struct net_device *netdev)\n{\n\tint err;\n\n\tif (roce->tracking_netdev)\n\t\treturn;\n\troce->tracking_netdev = netdev;\n\troce->nb.notifier_call = mlx5_netdev_event;\n\terr = register_netdevice_notifier_dev_net(netdev, &roce->nb, &roce->nn);\n\tWARN_ON(err);\n}\n\nstatic void mlx5_netdev_notifier_unregister(struct mlx5_roce *roce)\n{\n\tif (!roce->tracking_netdev)\n\t\treturn;\n\tunregister_netdevice_notifier_dev_net(roce->tracking_netdev, &roce->nb,\n\t\t\t\t\t      &roce->nn);\n\troce->tracking_netdev = NULL;\n}\n\nstatic int mlx5e_mdev_notifier_event(struct notifier_block *nb,\n\t\t\t\t     unsigned long event, void *data)\n{\n\tstruct mlx5_roce *roce = container_of(nb, struct mlx5_roce, mdev_nb);\n\tstruct net_device *netdev = data;\n\n\tswitch (event) {\n\tcase MLX5_DRIVER_EVENT_UPLINK_NETDEV:\n\t\tif (netdev)\n\t\t\tmlx5_netdev_notifier_register(roce, netdev);\n\t\telse\n\t\t\tmlx5_netdev_notifier_unregister(roce);\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic void mlx5_mdev_netdev_track(struct mlx5_ib_dev *dev, u32 port_num)\n{\n\tstruct mlx5_roce *roce = &dev->port[port_num].roce;\n\n\troce->mdev_nb.notifier_call = mlx5e_mdev_notifier_event;\n\tmlx5_blocking_notifier_register(dev->mdev, &roce->mdev_nb);\n\tmlx5_core_uplink_netdev_event_replay(dev->mdev);\n}\n\nstatic void mlx5_mdev_netdev_untrack(struct mlx5_ib_dev *dev, u32 port_num)\n{\n\tstruct mlx5_roce *roce = &dev->port[port_num].roce;\n\n\tmlx5_blocking_notifier_unregister(dev->mdev, &roce->mdev_nb);\n\tmlx5_netdev_notifier_unregister(roce);\n}\n\nstatic int mlx5_enable_eth(struct mlx5_ib_dev *dev)\n{\n\tint err;\n\n\tif (!dev->is_rep && dev->profile != &raw_eth_profile) {\n\t\terr = mlx5_nic_vport_enable_roce(dev->mdev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = mlx5_eth_lag_init(dev);\n\tif (err)\n\t\tgoto err_disable_roce;\n\n\treturn 0;\n\nerr_disable_roce:\n\tif (!dev->is_rep && dev->profile != &raw_eth_profile)\n\t\tmlx5_nic_vport_disable_roce(dev->mdev);\n\n\treturn err;\n}\n\nstatic void mlx5_disable_eth(struct mlx5_ib_dev *dev)\n{\n\tmlx5_eth_lag_cleanup(dev);\n\tif (!dev->is_rep && dev->profile != &raw_eth_profile)\n\t\tmlx5_nic_vport_disable_roce(dev->mdev);\n}\n\nstatic int mlx5_ib_rn_get_params(struct ib_device *device, u32 port_num,\n\t\t\t\t enum rdma_netdev_t type,\n\t\t\t\t struct rdma_netdev_alloc_params *params)\n{\n\tif (type != RDMA_NETDEV_IPOIB)\n\t\treturn -EOPNOTSUPP;\n\n\treturn mlx5_rdma_rn_get_params(to_mdev(device)->mdev, device, params);\n}\n\nstatic ssize_t delay_drop_timeout_read(struct file *filp, char __user *buf,\n\t\t\t\t       size_t count, loff_t *pos)\n{\n\tstruct mlx5_ib_delay_drop *delay_drop = filp->private_data;\n\tchar lbuf[20];\n\tint len;\n\n\tlen = snprintf(lbuf, sizeof(lbuf), \"%u\\n\", delay_drop->timeout);\n\treturn simple_read_from_buffer(buf, count, pos, lbuf, len);\n}\n\nstatic ssize_t delay_drop_timeout_write(struct file *filp, const char __user *buf,\n\t\t\t\t\tsize_t count, loff_t *pos)\n{\n\tstruct mlx5_ib_delay_drop *delay_drop = filp->private_data;\n\tu32 timeout;\n\tu32 var;\n\n\tif (kstrtouint_from_user(buf, count, 0, &var))\n\t\treturn -EFAULT;\n\n\ttimeout = min_t(u32, roundup(var, 100), MLX5_MAX_DELAY_DROP_TIMEOUT_MS *\n\t\t\t1000);\n\tif (timeout != var)\n\t\tmlx5_ib_dbg(delay_drop->dev, \"Round delay drop timeout to %u usec\\n\",\n\t\t\t    timeout);\n\n\tdelay_drop->timeout = timeout;\n\n\treturn count;\n}\n\nstatic const struct file_operations fops_delay_drop_timeout = {\n\t.owner\t= THIS_MODULE,\n\t.open\t= simple_open,\n\t.write\t= delay_drop_timeout_write,\n\t.read\t= delay_drop_timeout_read,\n};\n\nstatic void mlx5_ib_unbind_slave_port(struct mlx5_ib_dev *ibdev,\n\t\t\t\t      struct mlx5_ib_multiport_info *mpi)\n{\n\tu32 port_num = mlx5_core_native_port_num(mpi->mdev) - 1;\n\tstruct mlx5_ib_port *port = &ibdev->port[port_num];\n\tint comps;\n\tint err;\n\tint i;\n\n\tlockdep_assert_held(&mlx5_ib_multiport_mutex);\n\n\tmlx5_core_mp_event_replay(ibdev->mdev,\n\t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_REMOVED,\n\t\t\t\t  NULL);\n\tmlx5_core_mp_event_replay(mpi->mdev,\n\t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_REMOVED,\n\t\t\t\t  NULL);\n\n\tmlx5_ib_cleanup_cong_debugfs(ibdev, port_num);\n\n\tspin_lock(&port->mp.mpi_lock);\n\tif (!mpi->ibdev) {\n\t\tspin_unlock(&port->mp.mpi_lock);\n\t\treturn;\n\t}\n\n\tmpi->ibdev = NULL;\n\n\tspin_unlock(&port->mp.mpi_lock);\n\tif (mpi->mdev_events.notifier_call)\n\t\tmlx5_notifier_unregister(mpi->mdev, &mpi->mdev_events);\n\tmpi->mdev_events.notifier_call = NULL;\n\tmlx5_mdev_netdev_untrack(ibdev, port_num);\n\tspin_lock(&port->mp.mpi_lock);\n\n\tcomps = mpi->mdev_refcnt;\n\tif (comps) {\n\t\tmpi->unaffiliate = true;\n\t\tinit_completion(&mpi->unref_comp);\n\t\tspin_unlock(&port->mp.mpi_lock);\n\n\t\tfor (i = 0; i < comps; i++)\n\t\t\twait_for_completion(&mpi->unref_comp);\n\n\t\tspin_lock(&port->mp.mpi_lock);\n\t\tmpi->unaffiliate = false;\n\t}\n\n\tport->mp.mpi = NULL;\n\n\tspin_unlock(&port->mp.mpi_lock);\n\n\terr = mlx5_nic_vport_unaffiliate_multiport(mpi->mdev);\n\n\tmlx5_ib_dbg(ibdev, \"unaffiliated port %u\\n\", port_num + 1);\n\t \n\tif (err)\n\t\tmlx5_ib_err(ibdev, \"Failed to unaffiliate port %u\\n\",\n\t\t\t    port_num + 1);\n\n\tibdev->port[port_num].roce.last_port_state = IB_PORT_DOWN;\n}\n\nstatic bool mlx5_ib_bind_slave_port(struct mlx5_ib_dev *ibdev,\n\t\t\t\t    struct mlx5_ib_multiport_info *mpi)\n{\n\tu32 port_num = mlx5_core_native_port_num(mpi->mdev) - 1;\n\tu64 key;\n\tint err;\n\n\tlockdep_assert_held(&mlx5_ib_multiport_mutex);\n\n\tspin_lock(&ibdev->port[port_num].mp.mpi_lock);\n\tif (ibdev->port[port_num].mp.mpi) {\n\t\tmlx5_ib_dbg(ibdev, \"port %u already affiliated.\\n\",\n\t\t\t    port_num + 1);\n\t\tspin_unlock(&ibdev->port[port_num].mp.mpi_lock);\n\t\treturn false;\n\t}\n\n\tibdev->port[port_num].mp.mpi = mpi;\n\tmpi->ibdev = ibdev;\n\tmpi->mdev_events.notifier_call = NULL;\n\tspin_unlock(&ibdev->port[port_num].mp.mpi_lock);\n\n\terr = mlx5_nic_vport_affiliate_multiport(ibdev->mdev, mpi->mdev);\n\tif (err)\n\t\tgoto unbind;\n\n\tmlx5_mdev_netdev_track(ibdev, port_num);\n\n\tmpi->mdev_events.notifier_call = mlx5_ib_event_slave_port;\n\tmlx5_notifier_register(mpi->mdev, &mpi->mdev_events);\n\n\tmlx5_ib_init_cong_debugfs(ibdev, port_num);\n\n\tkey = mpi->mdev->priv.adev_idx;\n\tmlx5_core_mp_event_replay(mpi->mdev,\n\t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_DONE,\n\t\t\t\t  &key);\n\tmlx5_core_mp_event_replay(ibdev->mdev,\n\t\t\t\t  MLX5_DRIVER_EVENT_AFFILIATION_DONE,\n\t\t\t\t  &key);\n\n\treturn true;\n\nunbind:\n\tmlx5_ib_unbind_slave_port(ibdev, mpi);\n\treturn false;\n}\n\nstatic int mlx5_ib_init_multiport_master(struct mlx5_ib_dev *dev)\n{\n\tu32 port_num = mlx5_core_native_port_num(dev->mdev) - 1;\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(&dev->ib_dev,\n\t\t\t\t\t\t\t  port_num + 1);\n\tstruct mlx5_ib_multiport_info *mpi;\n\tint err;\n\tu32 i;\n\n\tif (!mlx5_core_is_mp_master(dev->mdev) || ll != IB_LINK_LAYER_ETHERNET)\n\t\treturn 0;\n\n\terr = mlx5_query_nic_vport_system_image_guid(dev->mdev,\n\t\t\t\t\t\t     &dev->sys_image_guid);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_nic_vport_enable_roce(dev->mdev);\n\tif (err)\n\t\treturn err;\n\n\tmutex_lock(&mlx5_ib_multiport_mutex);\n\tfor (i = 0; i < dev->num_ports; i++) {\n\t\tbool bound = false;\n\n\t\t \n\t\tif (i == port_num) {\n\t\t\tmpi = kzalloc(sizeof(*mpi), GFP_KERNEL);\n\t\t\tif (!mpi) {\n\t\t\t\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\t\t\t\tmlx5_nic_vport_disable_roce(dev->mdev);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tmpi->is_master = true;\n\t\t\tmpi->mdev = dev->mdev;\n\t\t\tmpi->sys_image_guid = dev->sys_image_guid;\n\t\t\tdev->port[i].mp.mpi = mpi;\n\t\t\tmpi->ibdev = dev;\n\t\t\tmpi = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_for_each_entry(mpi, &mlx5_ib_unaffiliated_port_list,\n\t\t\t\t    list) {\n\t\t\tif (dev->sys_image_guid == mpi->sys_image_guid &&\n\t\t\t    (mlx5_core_native_port_num(mpi->mdev) - 1) == i) {\n\t\t\t\tbound = mlx5_ib_bind_slave_port(dev, mpi);\n\t\t\t}\n\n\t\t\tif (bound) {\n\t\t\t\tdev_dbg(mpi->mdev->device,\n\t\t\t\t\t\"removing port from unaffiliated list.\\n\");\n\t\t\t\tmlx5_ib_dbg(dev, \"port %d bound\\n\", i + 1);\n\t\t\t\tlist_del(&mpi->list);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!bound)\n\t\t\tmlx5_ib_dbg(dev, \"no free port found for port %d\\n\",\n\t\t\t\t    i + 1);\n\t}\n\n\tlist_add_tail(&dev->ib_dev_list, &mlx5_ib_dev_list);\n\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\treturn err;\n}\n\nstatic void mlx5_ib_cleanup_multiport_master(struct mlx5_ib_dev *dev)\n{\n\tu32 port_num = mlx5_core_native_port_num(dev->mdev) - 1;\n\tenum rdma_link_layer ll = mlx5_ib_port_link_layer(&dev->ib_dev,\n\t\t\t\t\t\t\t  port_num + 1);\n\tu32 i;\n\n\tif (!mlx5_core_is_mp_master(dev->mdev) || ll != IB_LINK_LAYER_ETHERNET)\n\t\treturn;\n\n\tmutex_lock(&mlx5_ib_multiport_mutex);\n\tfor (i = 0; i < dev->num_ports; i++) {\n\t\tif (dev->port[i].mp.mpi) {\n\t\t\t \n\t\t\tif (i == port_num) {\n\t\t\t\tkfree(dev->port[i].mp.mpi);\n\t\t\t\tdev->port[i].mp.mpi = NULL;\n\t\t\t} else {\n\t\t\t\tmlx5_ib_dbg(dev, \"unbinding port_num: %u\\n\",\n\t\t\t\t\t    i + 1);\n\t\t\t\tlist_add_tail(&dev->port[i].mp.mpi->list,\n\t\t\t\t\t      &mlx5_ib_unaffiliated_port_list);\n\t\t\t\tmlx5_ib_unbind_slave_port(dev,\n\t\t\t\t\t\t\t  dev->port[i].mp.mpi);\n\t\t\t}\n\t\t}\n\t}\n\n\tmlx5_ib_dbg(dev, \"removing from devlist\\n\");\n\tlist_del(&dev->ib_dev_list);\n\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\n\tmlx5_nic_vport_disable_roce(dev->mdev);\n}\n\nstatic int mmap_obj_cleanup(struct ib_uobject *uobject,\n\t\t\t    enum rdma_remove_reason why,\n\t\t\t    struct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_user_mmap_entry *obj = uobject->object;\n\n\trdma_user_mmap_entry_remove(&obj->rdma_entry);\n\treturn 0;\n}\n\nstatic int mlx5_rdma_user_mmap_entry_insert(struct mlx5_ib_ucontext *c,\n\t\t\t\t\t    struct mlx5_user_mmap_entry *entry,\n\t\t\t\t\t    size_t length)\n{\n\treturn rdma_user_mmap_entry_insert_range(\n\t\t&c->ibucontext, &entry->rdma_entry, length,\n\t\t(MLX5_IB_MMAP_OFFSET_START << 16),\n\t\t((MLX5_IB_MMAP_OFFSET_END << 16) + (1UL << 16) - 1));\n}\n\nstatic struct mlx5_user_mmap_entry *\nalloc_var_entry(struct mlx5_ib_ucontext *c)\n{\n\tstruct mlx5_user_mmap_entry *entry;\n\tstruct mlx5_var_table *var_table;\n\tu32 page_idx;\n\tint err;\n\n\tvar_table = &to_mdev(c->ibucontext.device)->var_table;\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_lock(&var_table->bitmap_lock);\n\tpage_idx = find_first_zero_bit(var_table->bitmap,\n\t\t\t\t       var_table->num_var_hw_entries);\n\tif (page_idx >= var_table->num_var_hw_entries) {\n\t\terr = -ENOSPC;\n\t\tmutex_unlock(&var_table->bitmap_lock);\n\t\tgoto end;\n\t}\n\n\tset_bit(page_idx, var_table->bitmap);\n\tmutex_unlock(&var_table->bitmap_lock);\n\n\tentry->address = var_table->hw_start_addr +\n\t\t\t\t(page_idx * var_table->stride_size);\n\tentry->page_idx = page_idx;\n\tentry->mmap_flag = MLX5_IB_MMAP_TYPE_VAR;\n\n\terr = mlx5_rdma_user_mmap_entry_insert(c, entry,\n\t\t\t\t\t       var_table->stride_size);\n\tif (err)\n\t\tgoto err_insert;\n\n\treturn entry;\n\nerr_insert:\n\tmutex_lock(&var_table->bitmap_lock);\n\tclear_bit(page_idx, var_table->bitmap);\n\tmutex_unlock(&var_table->bitmap_lock);\nend:\n\tkfree(entry);\n\treturn ERR_PTR(err);\n}\n\nstatic int UVERBS_HANDLER(MLX5_IB_METHOD_VAR_OBJ_ALLOC)(\n\tstruct uverbs_attr_bundle *attrs)\n{\n\tstruct ib_uobject *uobj = uverbs_attr_get_uobject(\n\t\tattrs, MLX5_IB_ATTR_VAR_OBJ_ALLOC_HANDLE);\n\tstruct mlx5_ib_ucontext *c;\n\tstruct mlx5_user_mmap_entry *entry;\n\tu64 mmap_offset;\n\tu32 length;\n\tint err;\n\n\tc = to_mucontext(ib_uverbs_get_ucontext(attrs));\n\tif (IS_ERR(c))\n\t\treturn PTR_ERR(c);\n\n\tentry = alloc_var_entry(c);\n\tif (IS_ERR(entry))\n\t\treturn PTR_ERR(entry);\n\n\tmmap_offset = mlx5_entry_to_mmap_offset(entry);\n\tlength = entry->rdma_entry.npages * PAGE_SIZE;\n\tuobj->object = entry;\n\tuverbs_finalize_uobj_create(attrs, MLX5_IB_ATTR_VAR_OBJ_ALLOC_HANDLE);\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_VAR_OBJ_ALLOC_MMAP_OFFSET,\n\t\t\t     &mmap_offset, sizeof(mmap_offset));\n\tif (err)\n\t\treturn err;\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_VAR_OBJ_ALLOC_PAGE_ID,\n\t\t\t     &entry->page_idx, sizeof(entry->page_idx));\n\tif (err)\n\t\treturn err;\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_VAR_OBJ_ALLOC_MMAP_LENGTH,\n\t\t\t     &length, sizeof(length));\n\treturn err;\n}\n\nDECLARE_UVERBS_NAMED_METHOD(\n\tMLX5_IB_METHOD_VAR_OBJ_ALLOC,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_VAR_OBJ_ALLOC_HANDLE,\n\t\t\tMLX5_IB_OBJECT_VAR,\n\t\t\tUVERBS_ACCESS_NEW,\n\t\t\tUA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_VAR_OBJ_ALLOC_PAGE_ID,\n\t\t\t   UVERBS_ATTR_TYPE(u32),\n\t\t\t   UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_VAR_OBJ_ALLOC_MMAP_LENGTH,\n\t\t\t   UVERBS_ATTR_TYPE(u32),\n\t\t\t   UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_VAR_OBJ_ALLOC_MMAP_OFFSET,\n\t\t\t    UVERBS_ATTR_TYPE(u64),\n\t\t\t    UA_MANDATORY));\n\nDECLARE_UVERBS_NAMED_METHOD_DESTROY(\n\tMLX5_IB_METHOD_VAR_OBJ_DESTROY,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_VAR_OBJ_DESTROY_HANDLE,\n\t\t\tMLX5_IB_OBJECT_VAR,\n\t\t\tUVERBS_ACCESS_DESTROY,\n\t\t\tUA_MANDATORY));\n\nDECLARE_UVERBS_NAMED_OBJECT(MLX5_IB_OBJECT_VAR,\n\t\t\t    UVERBS_TYPE_ALLOC_IDR(mmap_obj_cleanup),\n\t\t\t    &UVERBS_METHOD(MLX5_IB_METHOD_VAR_OBJ_ALLOC),\n\t\t\t    &UVERBS_METHOD(MLX5_IB_METHOD_VAR_OBJ_DESTROY));\n\nstatic bool var_is_supported(struct ib_device *device)\n{\n\tstruct mlx5_ib_dev *dev = to_mdev(device);\n\n\treturn (MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &\n\t\t\tMLX5_GENERAL_OBJ_TYPES_CAP_VIRTIO_NET_Q);\n}\n\nstatic struct mlx5_user_mmap_entry *\nalloc_uar_entry(struct mlx5_ib_ucontext *c,\n\t\tenum mlx5_ib_uapi_uar_alloc_type alloc_type)\n{\n\tstruct mlx5_user_mmap_entry *entry;\n\tstruct mlx5_ib_dev *dev;\n\tu32 uar_index;\n\tint err;\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdev = to_mdev(c->ibucontext.device);\n\terr = mlx5_cmd_uar_alloc(dev->mdev, &uar_index, c->devx_uid);\n\tif (err)\n\t\tgoto end;\n\n\tentry->page_idx = uar_index;\n\tentry->address = uar_index2paddress(dev, uar_index);\n\tif (alloc_type == MLX5_IB_UAPI_UAR_ALLOC_TYPE_BF)\n\t\tentry->mmap_flag = MLX5_IB_MMAP_TYPE_UAR_WC;\n\telse\n\t\tentry->mmap_flag = MLX5_IB_MMAP_TYPE_UAR_NC;\n\n\terr = mlx5_rdma_user_mmap_entry_insert(c, entry, PAGE_SIZE);\n\tif (err)\n\t\tgoto err_insert;\n\n\treturn entry;\n\nerr_insert:\n\tmlx5_cmd_uar_dealloc(dev->mdev, uar_index, c->devx_uid);\nend:\n\tkfree(entry);\n\treturn ERR_PTR(err);\n}\n\nstatic int UVERBS_HANDLER(MLX5_IB_METHOD_UAR_OBJ_ALLOC)(\n\tstruct uverbs_attr_bundle *attrs)\n{\n\tstruct ib_uobject *uobj = uverbs_attr_get_uobject(\n\t\tattrs, MLX5_IB_ATTR_UAR_OBJ_ALLOC_HANDLE);\n\tenum mlx5_ib_uapi_uar_alloc_type alloc_type;\n\tstruct mlx5_ib_ucontext *c;\n\tstruct mlx5_user_mmap_entry *entry;\n\tu64 mmap_offset;\n\tu32 length;\n\tint err;\n\n\tc = to_mucontext(ib_uverbs_get_ucontext(attrs));\n\tif (IS_ERR(c))\n\t\treturn PTR_ERR(c);\n\n\terr = uverbs_get_const(&alloc_type, attrs,\n\t\t\t       MLX5_IB_ATTR_UAR_OBJ_ALLOC_TYPE);\n\tif (err)\n\t\treturn err;\n\n\tif (alloc_type != MLX5_IB_UAPI_UAR_ALLOC_TYPE_BF &&\n\t    alloc_type != MLX5_IB_UAPI_UAR_ALLOC_TYPE_NC)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!to_mdev(c->ibucontext.device)->wc_support &&\n\t    alloc_type == MLX5_IB_UAPI_UAR_ALLOC_TYPE_BF)\n\t\treturn -EOPNOTSUPP;\n\n\tentry = alloc_uar_entry(c, alloc_type);\n\tif (IS_ERR(entry))\n\t\treturn PTR_ERR(entry);\n\n\tmmap_offset = mlx5_entry_to_mmap_offset(entry);\n\tlength = entry->rdma_entry.npages * PAGE_SIZE;\n\tuobj->object = entry;\n\tuverbs_finalize_uobj_create(attrs, MLX5_IB_ATTR_UAR_OBJ_ALLOC_HANDLE);\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_UAR_OBJ_ALLOC_MMAP_OFFSET,\n\t\t\t     &mmap_offset, sizeof(mmap_offset));\n\tif (err)\n\t\treturn err;\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_UAR_OBJ_ALLOC_PAGE_ID,\n\t\t\t     &entry->page_idx, sizeof(entry->page_idx));\n\tif (err)\n\t\treturn err;\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_UAR_OBJ_ALLOC_MMAP_LENGTH,\n\t\t\t     &length, sizeof(length));\n\treturn err;\n}\n\nDECLARE_UVERBS_NAMED_METHOD(\n\tMLX5_IB_METHOD_UAR_OBJ_ALLOC,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_UAR_OBJ_ALLOC_HANDLE,\n\t\t\tMLX5_IB_OBJECT_UAR,\n\t\t\tUVERBS_ACCESS_NEW,\n\t\t\tUA_MANDATORY),\n\tUVERBS_ATTR_CONST_IN(MLX5_IB_ATTR_UAR_OBJ_ALLOC_TYPE,\n\t\t\t     enum mlx5_ib_uapi_uar_alloc_type,\n\t\t\t     UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_UAR_OBJ_ALLOC_PAGE_ID,\n\t\t\t   UVERBS_ATTR_TYPE(u32),\n\t\t\t   UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_UAR_OBJ_ALLOC_MMAP_LENGTH,\n\t\t\t   UVERBS_ATTR_TYPE(u32),\n\t\t\t   UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_UAR_OBJ_ALLOC_MMAP_OFFSET,\n\t\t\t    UVERBS_ATTR_TYPE(u64),\n\t\t\t    UA_MANDATORY));\n\nDECLARE_UVERBS_NAMED_METHOD_DESTROY(\n\tMLX5_IB_METHOD_UAR_OBJ_DESTROY,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_UAR_OBJ_DESTROY_HANDLE,\n\t\t\tMLX5_IB_OBJECT_UAR,\n\t\t\tUVERBS_ACCESS_DESTROY,\n\t\t\tUA_MANDATORY));\n\nDECLARE_UVERBS_NAMED_OBJECT(MLX5_IB_OBJECT_UAR,\n\t\t\t    UVERBS_TYPE_ALLOC_IDR(mmap_obj_cleanup),\n\t\t\t    &UVERBS_METHOD(MLX5_IB_METHOD_UAR_OBJ_ALLOC),\n\t\t\t    &UVERBS_METHOD(MLX5_IB_METHOD_UAR_OBJ_DESTROY));\n\nADD_UVERBS_ATTRIBUTES_SIMPLE(\n\tmlx5_ib_query_context,\n\tUVERBS_OBJECT_DEVICE,\n\tUVERBS_METHOD_QUERY_CONTEXT,\n\tUVERBS_ATTR_PTR_OUT(\n\t\tMLX5_IB_ATTR_QUERY_CONTEXT_RESP_UCTX,\n\t\tUVERBS_ATTR_STRUCT(struct mlx5_ib_alloc_ucontext_resp,\n\t\t\t\t   dump_fill_mkey),\n\t\tUA_MANDATORY));\n\nstatic const struct uapi_definition mlx5_ib_defs[] = {\n\tUAPI_DEF_CHAIN(mlx5_ib_devx_defs),\n\tUAPI_DEF_CHAIN(mlx5_ib_flow_defs),\n\tUAPI_DEF_CHAIN(mlx5_ib_qos_defs),\n\tUAPI_DEF_CHAIN(mlx5_ib_std_types_defs),\n\tUAPI_DEF_CHAIN(mlx5_ib_dm_defs),\n\n\tUAPI_DEF_CHAIN_OBJ_TREE(UVERBS_OBJECT_DEVICE, &mlx5_ib_query_context),\n\tUAPI_DEF_CHAIN_OBJ_TREE_NAMED(MLX5_IB_OBJECT_VAR,\n\t\t\t\tUAPI_DEF_IS_OBJ_SUPPORTED(var_is_supported)),\n\tUAPI_DEF_CHAIN_OBJ_TREE_NAMED(MLX5_IB_OBJECT_UAR),\n\t{}\n};\n\nstatic void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5_ib_cleanup_multiport_master(dev);\n\tWARN_ON(!xa_empty(&dev->odp_mkeys));\n\tmutex_destroy(&dev->cap_mask_mutex);\n\tWARN_ON(!xa_empty(&dev->sig_mrs));\n\tWARN_ON(!bitmap_empty(dev->dm.memic_alloc_pages, MLX5_MAX_MEMIC_PAGES));\n\tmlx5r_macsec_dealloc_gids(dev);\n}\n\nstatic int mlx5_ib_stage_init_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tint err, i;\n\n\tdev->ib_dev.node_type = RDMA_NODE_IB_CA;\n\tdev->ib_dev.local_dma_lkey = 0  ;\n\tdev->ib_dev.phys_port_cnt = dev->num_ports;\n\tdev->ib_dev.dev.parent = mdev->device;\n\tdev->ib_dev.lag_flags = RDMA_LAG_FLAGS_HASH_ALL_SLAVES;\n\n\tfor (i = 0; i < dev->num_ports; i++) {\n\t\tspin_lock_init(&dev->port[i].mp.mpi_lock);\n\t\trwlock_init(&dev->port[i].roce.netdev_lock);\n\t\tdev->port[i].roce.dev = dev;\n\t\tdev->port[i].roce.native_port_num = i + 1;\n\t\tdev->port[i].roce.last_port_state = IB_PORT_DOWN;\n\t}\n\n\terr = mlx5r_cmd_query_special_mkeys(dev);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5r_macsec_init_gids_and_devlist(dev);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_ib_init_multiport_master(dev);\n\tif (err)\n\t\tgoto err;\n\n\terr = set_has_smi_cap(dev);\n\tif (err)\n\t\tgoto err_mp;\n\n\terr = mlx5_query_max_pkeys(&dev->ib_dev, &dev->pkey_table_len);\n\tif (err)\n\t\tgoto err_mp;\n\n\tif (mlx5_use_mad_ifc(dev))\n\t\tget_ext_port_caps(dev);\n\n\tdev->ib_dev.num_comp_vectors    = mlx5_comp_vectors_max(mdev);\n\n\tmutex_init(&dev->cap_mask_mutex);\n\tINIT_LIST_HEAD(&dev->qp_list);\n\tspin_lock_init(&dev->reset_flow_resource_lock);\n\txa_init(&dev->odp_mkeys);\n\txa_init(&dev->sig_mrs);\n\tatomic_set(&dev->mkey_var, 0);\n\n\tspin_lock_init(&dev->dm.lock);\n\tdev->dm.dev = mdev;\n\treturn 0;\nerr:\n\tmlx5r_macsec_dealloc_gids(dev);\nerr_mp:\n\tmlx5_ib_cleanup_multiport_master(dev);\n\treturn err;\n}\n\nstatic int mlx5_ib_enable_driver(struct ib_device *dev)\n{\n\tstruct mlx5_ib_dev *mdev = to_mdev(dev);\n\tint ret;\n\n\tret = mlx5_ib_test_wc(mdev);\n\tmlx5_ib_dbg(mdev, \"Write-Combining %s\",\n\t\t    mdev->wc_support ? \"supported\" : \"not supported\");\n\n\treturn ret;\n}\n\nstatic const struct ib_device_ops mlx5_ib_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_MLX5,\n\t.uverbs_abi_ver\t= MLX5_IB_UVERBS_ABI_VERSION,\n\n\t.add_gid = mlx5_ib_add_gid,\n\t.alloc_mr = mlx5_ib_alloc_mr,\n\t.alloc_mr_integrity = mlx5_ib_alloc_mr_integrity,\n\t.alloc_pd = mlx5_ib_alloc_pd,\n\t.alloc_ucontext = mlx5_ib_alloc_ucontext,\n\t.attach_mcast = mlx5_ib_mcg_attach,\n\t.check_mr_status = mlx5_ib_check_mr_status,\n\t.create_ah = mlx5_ib_create_ah,\n\t.create_cq = mlx5_ib_create_cq,\n\t.create_qp = mlx5_ib_create_qp,\n\t.create_srq = mlx5_ib_create_srq,\n\t.create_user_ah = mlx5_ib_create_ah,\n\t.dealloc_pd = mlx5_ib_dealloc_pd,\n\t.dealloc_ucontext = mlx5_ib_dealloc_ucontext,\n\t.del_gid = mlx5_ib_del_gid,\n\t.dereg_mr = mlx5_ib_dereg_mr,\n\t.destroy_ah = mlx5_ib_destroy_ah,\n\t.destroy_cq = mlx5_ib_destroy_cq,\n\t.destroy_qp = mlx5_ib_destroy_qp,\n\t.destroy_srq = mlx5_ib_destroy_srq,\n\t.detach_mcast = mlx5_ib_mcg_detach,\n\t.disassociate_ucontext = mlx5_ib_disassociate_ucontext,\n\t.drain_rq = mlx5_ib_drain_rq,\n\t.drain_sq = mlx5_ib_drain_sq,\n\t.device_group = &mlx5_attr_group,\n\t.enable_driver = mlx5_ib_enable_driver,\n\t.get_dev_fw_str = get_dev_fw_str,\n\t.get_dma_mr = mlx5_ib_get_dma_mr,\n\t.get_link_layer = mlx5_ib_port_link_layer,\n\t.map_mr_sg = mlx5_ib_map_mr_sg,\n\t.map_mr_sg_pi = mlx5_ib_map_mr_sg_pi,\n\t.mmap = mlx5_ib_mmap,\n\t.mmap_free = mlx5_ib_mmap_free,\n\t.modify_cq = mlx5_ib_modify_cq,\n\t.modify_device = mlx5_ib_modify_device,\n\t.modify_port = mlx5_ib_modify_port,\n\t.modify_qp = mlx5_ib_modify_qp,\n\t.modify_srq = mlx5_ib_modify_srq,\n\t.poll_cq = mlx5_ib_poll_cq,\n\t.post_recv = mlx5_ib_post_recv_nodrain,\n\t.post_send = mlx5_ib_post_send_nodrain,\n\t.post_srq_recv = mlx5_ib_post_srq_recv,\n\t.process_mad = mlx5_ib_process_mad,\n\t.query_ah = mlx5_ib_query_ah,\n\t.query_device = mlx5_ib_query_device,\n\t.query_gid = mlx5_ib_query_gid,\n\t.query_pkey = mlx5_ib_query_pkey,\n\t.query_qp = mlx5_ib_query_qp,\n\t.query_srq = mlx5_ib_query_srq,\n\t.query_ucontext = mlx5_ib_query_ucontext,\n\t.reg_user_mr = mlx5_ib_reg_user_mr,\n\t.reg_user_mr_dmabuf = mlx5_ib_reg_user_mr_dmabuf,\n\t.req_notify_cq = mlx5_ib_arm_cq,\n\t.rereg_user_mr = mlx5_ib_rereg_user_mr,\n\t.resize_cq = mlx5_ib_resize_cq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_ah, mlx5_ib_ah, ibah),\n\tINIT_RDMA_OBJ_SIZE(ib_counters, mlx5_ib_mcounters, ibcntrs),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, mlx5_ib_cq, ibcq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, mlx5_ib_pd, ibpd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, mlx5_ib_qp, ibqp),\n\tINIT_RDMA_OBJ_SIZE(ib_srq, mlx5_ib_srq, ibsrq),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, mlx5_ib_ucontext, ibucontext),\n};\n\nstatic const struct ib_device_ops mlx5_ib_dev_ipoib_enhanced_ops = {\n\t.rdma_netdev_get_params = mlx5_ib_rn_get_params,\n};\n\nstatic const struct ib_device_ops mlx5_ib_dev_sriov_ops = {\n\t.get_vf_config = mlx5_ib_get_vf_config,\n\t.get_vf_guid = mlx5_ib_get_vf_guid,\n\t.get_vf_stats = mlx5_ib_get_vf_stats,\n\t.set_vf_guid = mlx5_ib_set_vf_guid,\n\t.set_vf_link_state = mlx5_ib_set_vf_link_state,\n};\n\nstatic const struct ib_device_ops mlx5_ib_dev_mw_ops = {\n\t.alloc_mw = mlx5_ib_alloc_mw,\n\t.dealloc_mw = mlx5_ib_dealloc_mw,\n\n\tINIT_RDMA_OBJ_SIZE(ib_mw, mlx5_ib_mw, ibmw),\n};\n\nstatic const struct ib_device_ops mlx5_ib_dev_xrc_ops = {\n\t.alloc_xrcd = mlx5_ib_alloc_xrcd,\n\t.dealloc_xrcd = mlx5_ib_dealloc_xrcd,\n\n\tINIT_RDMA_OBJ_SIZE(ib_xrcd, mlx5_ib_xrcd, ibxrcd),\n};\n\nstatic int mlx5_ib_init_var_table(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tstruct mlx5_var_table *var_table = &dev->var_table;\n\tu8 log_doorbell_bar_size;\n\tu8 log_doorbell_stride;\n\tu64 bar_size;\n\n\tlog_doorbell_bar_size = MLX5_CAP_DEV_VDPA_EMULATION(mdev,\n\t\t\t\t\tlog_doorbell_bar_size);\n\tlog_doorbell_stride = MLX5_CAP_DEV_VDPA_EMULATION(mdev,\n\t\t\t\t\tlog_doorbell_stride);\n\tvar_table->hw_start_addr = dev->mdev->bar_addr +\n\t\t\t\tMLX5_CAP64_DEV_VDPA_EMULATION(mdev,\n\t\t\t\t\tdoorbell_bar_offset);\n\tbar_size = (1ULL << log_doorbell_bar_size) * 4096;\n\tvar_table->stride_size = 1ULL << log_doorbell_stride;\n\tvar_table->num_var_hw_entries = div_u64(bar_size,\n\t\t\t\t\t\tvar_table->stride_size);\n\tmutex_init(&var_table->bitmap_lock);\n\tvar_table->bitmap = bitmap_zalloc(var_table->num_var_hw_entries,\n\t\t\t\t\t  GFP_KERNEL);\n\treturn (var_table->bitmap) ? 0 : -ENOMEM;\n}\n\nstatic void mlx5_ib_stage_caps_cleanup(struct mlx5_ib_dev *dev)\n{\n\tbitmap_free(dev->var_table.bitmap);\n}\n\nstatic int mlx5_ib_stage_caps_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tint err;\n\n\tif (MLX5_CAP_GEN(mdev, ipoib_enhanced_offloads) &&\n\t    IS_ENABLED(CONFIG_MLX5_CORE_IPOIB))\n\t\tib_set_device_ops(&dev->ib_dev,\n\t\t\t\t  &mlx5_ib_dev_ipoib_enhanced_ops);\n\n\tif (mlx5_core_is_pf(mdev))\n\t\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_sriov_ops);\n\n\tdev->umr_fence = mlx5_get_umr_fence(MLX5_CAP_GEN(mdev, umr_fence));\n\n\tif (MLX5_CAP_GEN(mdev, imaicl))\n\t\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_mw_ops);\n\n\tif (MLX5_CAP_GEN(mdev, xrc))\n\t\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_xrc_ops);\n\n\tif (MLX5_CAP_DEV_MEM(mdev, memic) ||\n\t    MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &\n\t    MLX5_GENERAL_OBJ_TYPES_CAP_SW_ICM)\n\t\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_dm_ops);\n\n\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_ops);\n\n\tif (IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS))\n\t\tdev->ib_dev.driver_def = mlx5_ib_defs;\n\n\terr = init_node_data(dev);\n\tif (err)\n\t\treturn err;\n\n\tif ((MLX5_CAP_GEN(dev->mdev, port_type) == MLX5_CAP_PORT_TYPE_ETH) &&\n\t    (MLX5_CAP_GEN(dev->mdev, disable_local_lb_uc) ||\n\t     MLX5_CAP_GEN(dev->mdev, disable_local_lb_mc)))\n\t\tmutex_init(&dev->lb.mutex);\n\n\tif (MLX5_CAP_GEN_64(dev->mdev, general_obj_types) &\n\t\t\tMLX5_GENERAL_OBJ_TYPES_CAP_VIRTIO_NET_Q) {\n\t\terr = mlx5_ib_init_var_table(dev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tdev->ib_dev.use_cq_dim = true;\n\n\treturn 0;\n}\n\nstatic const struct ib_device_ops mlx5_ib_dev_port_ops = {\n\t.get_port_immutable = mlx5_port_immutable,\n\t.query_port = mlx5_ib_query_port,\n};\n\nstatic int mlx5_ib_stage_non_default_cb(struct mlx5_ib_dev *dev)\n{\n\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_port_ops);\n\treturn 0;\n}\n\nstatic const struct ib_device_ops mlx5_ib_dev_port_rep_ops = {\n\t.get_port_immutable = mlx5_port_rep_immutable,\n\t.query_port = mlx5_ib_rep_query_port,\n\t.query_pkey = mlx5_ib_rep_query_pkey,\n};\n\nstatic int mlx5_ib_stage_raw_eth_non_default_cb(struct mlx5_ib_dev *dev)\n{\n\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_port_rep_ops);\n\treturn 0;\n}\n\nstatic const struct ib_device_ops mlx5_ib_dev_common_roce_ops = {\n\t.create_rwq_ind_table = mlx5_ib_create_rwq_ind_table,\n\t.create_wq = mlx5_ib_create_wq,\n\t.destroy_rwq_ind_table = mlx5_ib_destroy_rwq_ind_table,\n\t.destroy_wq = mlx5_ib_destroy_wq,\n\t.get_netdev = mlx5_ib_get_netdev,\n\t.modify_wq = mlx5_ib_modify_wq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_rwq_ind_table, mlx5_ib_rwq_ind_table,\n\t\t\t   ib_rwq_ind_tbl),\n};\n\nstatic int mlx5_ib_roce_init(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tenum rdma_link_layer ll;\n\tint port_type_cap;\n\tu32 port_num = 0;\n\tint err;\n\n\tport_type_cap = MLX5_CAP_GEN(mdev, port_type);\n\tll = mlx5_port_type_cap_to_rdma_ll(port_type_cap);\n\n\tif (ll == IB_LINK_LAYER_ETHERNET) {\n\t\tib_set_device_ops(&dev->ib_dev, &mlx5_ib_dev_common_roce_ops);\n\n\t\tport_num = mlx5_core_native_port_num(dev->mdev) - 1;\n\n\t\t \n\t\tmlx5_mdev_netdev_track(dev, port_num);\n\n\t\terr = mlx5_enable_eth(dev);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t}\n\n\treturn 0;\ncleanup:\n\tmlx5_mdev_netdev_untrack(dev, port_num);\n\treturn err;\n}\n\nstatic void mlx5_ib_roce_cleanup(struct mlx5_ib_dev *dev)\n{\n\tstruct mlx5_core_dev *mdev = dev->mdev;\n\tenum rdma_link_layer ll;\n\tint port_type_cap;\n\tu32 port_num;\n\n\tport_type_cap = MLX5_CAP_GEN(mdev, port_type);\n\tll = mlx5_port_type_cap_to_rdma_ll(port_type_cap);\n\n\tif (ll == IB_LINK_LAYER_ETHERNET) {\n\t\tmlx5_disable_eth(dev);\n\n\t\tport_num = mlx5_core_native_port_num(dev->mdev) - 1;\n\t\tmlx5_mdev_netdev_untrack(dev, port_num);\n\t}\n}\n\nstatic int mlx5_ib_stage_cong_debugfs_init(struct mlx5_ib_dev *dev)\n{\n\tmlx5_ib_init_cong_debugfs(dev,\n\t\t\t\t  mlx5_core_native_port_num(dev->mdev) - 1);\n\treturn 0;\n}\n\nstatic void mlx5_ib_stage_cong_debugfs_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5_ib_cleanup_cong_debugfs(dev,\n\t\t\t\t     mlx5_core_native_port_num(dev->mdev) - 1);\n}\n\nstatic int mlx5_ib_stage_uar_init(struct mlx5_ib_dev *dev)\n{\n\tdev->mdev->priv.uar = mlx5_get_uars_page(dev->mdev);\n\treturn PTR_ERR_OR_ZERO(dev->mdev->priv.uar);\n}\n\nstatic void mlx5_ib_stage_uar_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5_put_uars_page(dev->mdev, dev->mdev->priv.uar);\n}\n\nstatic int mlx5_ib_stage_bfrag_init(struct mlx5_ib_dev *dev)\n{\n\tint err;\n\n\terr = mlx5_alloc_bfreg(dev->mdev, &dev->bfreg, false, false);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_alloc_bfreg(dev->mdev, &dev->fp_bfreg, false, true);\n\tif (err)\n\t\tmlx5_free_bfreg(dev->mdev, &dev->bfreg);\n\n\treturn err;\n}\n\nstatic void mlx5_ib_stage_bfrag_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5_free_bfreg(dev->mdev, &dev->fp_bfreg);\n\tmlx5_free_bfreg(dev->mdev, &dev->bfreg);\n}\n\nstatic int mlx5_ib_stage_ib_reg_init(struct mlx5_ib_dev *dev)\n{\n\tconst char *name;\n\n\tif (!mlx5_lag_is_active(dev->mdev))\n\t\tname = \"mlx5_%d\";\n\telse\n\t\tname = \"mlx5_bond_%d\";\n\treturn ib_register_device(&dev->ib_dev, name, &dev->mdev->pdev->dev);\n}\n\nstatic void mlx5_ib_stage_pre_ib_reg_umr_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5_mkey_cache_cleanup(dev);\n\tmlx5r_umr_resource_cleanup(dev);\n}\n\nstatic void mlx5_ib_stage_ib_reg_cleanup(struct mlx5_ib_dev *dev)\n{\n\tib_unregister_device(&dev->ib_dev);\n}\n\nstatic int mlx5_ib_stage_post_ib_reg_umr_init(struct mlx5_ib_dev *dev)\n{\n\tint ret;\n\n\tret = mlx5r_umr_resource_init(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mlx5_mkey_cache_init(dev);\n\tif (ret)\n\t\tmlx5_ib_warn(dev, \"mr cache init failed %d\\n\", ret);\n\treturn ret;\n}\n\nstatic int mlx5_ib_stage_delay_drop_init(struct mlx5_ib_dev *dev)\n{\n\tstruct dentry *root;\n\n\tif (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))\n\t\treturn 0;\n\n\tmutex_init(&dev->delay_drop.lock);\n\tdev->delay_drop.dev = dev;\n\tdev->delay_drop.activate = false;\n\tdev->delay_drop.timeout = MLX5_MAX_DELAY_DROP_TIMEOUT_MS * 1000;\n\tINIT_WORK(&dev->delay_drop.delay_drop_work, delay_drop_handler);\n\tatomic_set(&dev->delay_drop.rqs_cnt, 0);\n\tatomic_set(&dev->delay_drop.events_cnt, 0);\n\n\tif (!mlx5_debugfs_root)\n\t\treturn 0;\n\n\troot = debugfs_create_dir(\"delay_drop\", mlx5_debugfs_get_dev_root(dev->mdev));\n\tdev->delay_drop.dir_debugfs = root;\n\n\tdebugfs_create_atomic_t(\"num_timeout_events\", 0400, root,\n\t\t\t\t&dev->delay_drop.events_cnt);\n\tdebugfs_create_atomic_t(\"num_rqs\", 0400, root,\n\t\t\t\t&dev->delay_drop.rqs_cnt);\n\tdebugfs_create_file(\"timeout\", 0600, root, &dev->delay_drop,\n\t\t\t    &fops_delay_drop_timeout);\n\treturn 0;\n}\n\nstatic void mlx5_ib_stage_delay_drop_cleanup(struct mlx5_ib_dev *dev)\n{\n\tif (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))\n\t\treturn;\n\n\tcancel_work_sync(&dev->delay_drop.delay_drop_work);\n\tif (!dev->delay_drop.dir_debugfs)\n\t\treturn;\n\n\tdebugfs_remove_recursive(dev->delay_drop.dir_debugfs);\n\tdev->delay_drop.dir_debugfs = NULL;\n}\n\nstatic int mlx5_ib_stage_dev_notifier_init(struct mlx5_ib_dev *dev)\n{\n\tdev->mdev_events.notifier_call = mlx5_ib_event;\n\tmlx5_notifier_register(dev->mdev, &dev->mdev_events);\n\n\tmlx5r_macsec_event_register(dev);\n\n\treturn 0;\n}\n\nstatic void mlx5_ib_stage_dev_notifier_cleanup(struct mlx5_ib_dev *dev)\n{\n\tmlx5r_macsec_event_unregister(dev);\n\tmlx5_notifier_unregister(dev->mdev, &dev->mdev_events);\n}\n\nvoid __mlx5_ib_remove(struct mlx5_ib_dev *dev,\n\t\t      const struct mlx5_ib_profile *profile,\n\t\t      int stage)\n{\n\tdev->ib_active = false;\n\n\t \n\twhile (stage) {\n\t\tstage--;\n\t\tif (profile->stage[stage].cleanup)\n\t\t\tprofile->stage[stage].cleanup(dev);\n\t}\n\n\tkfree(dev->port);\n\tib_dealloc_device(&dev->ib_dev);\n}\n\nint __mlx5_ib_add(struct mlx5_ib_dev *dev,\n\t\t  const struct mlx5_ib_profile *profile)\n{\n\tint err;\n\tint i;\n\n\tdev->profile = profile;\n\n\tfor (i = 0; i < MLX5_IB_STAGE_MAX; i++) {\n\t\tif (profile->stage[i].init) {\n\t\t\terr = profile->stage[i].init(dev);\n\t\t\tif (err)\n\t\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tdev->ib_active = true;\n\treturn 0;\n\nerr_out:\n\t \n\twhile (i) {\n\t\ti--;\n\t\tif (profile->stage[i].cleanup)\n\t\t\tprofile->stage[i].cleanup(dev);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic const struct mlx5_ib_profile pf_profile = {\n\tSTAGE_CREATE(MLX5_IB_STAGE_INIT,\n\t\t     mlx5_ib_stage_init_init,\n\t\t     mlx5_ib_stage_init_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_FS,\n\t\t     mlx5_ib_fs_init,\n\t\t     mlx5_ib_fs_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_CAPS,\n\t\t     mlx5_ib_stage_caps_init,\n\t\t     mlx5_ib_stage_caps_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_NON_DEFAULT_CB,\n\t\t     mlx5_ib_stage_non_default_cb,\n\t\t     NULL),\n\tSTAGE_CREATE(MLX5_IB_STAGE_ROCE,\n\t\t     mlx5_ib_roce_init,\n\t\t     mlx5_ib_roce_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_QP,\n\t\t     mlx5_init_qp_table,\n\t\t     mlx5_cleanup_qp_table),\n\tSTAGE_CREATE(MLX5_IB_STAGE_SRQ,\n\t\t     mlx5_init_srq_table,\n\t\t     mlx5_cleanup_srq_table),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DEVICE_RESOURCES,\n\t\t     mlx5_ib_dev_res_init,\n\t\t     mlx5_ib_dev_res_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DEVICE_NOTIFIER,\n\t\t     mlx5_ib_stage_dev_notifier_init,\n\t\t     mlx5_ib_stage_dev_notifier_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_ODP,\n\t\t     mlx5_ib_odp_init_one,\n\t\t     mlx5_ib_odp_cleanup_one),\n\tSTAGE_CREATE(MLX5_IB_STAGE_COUNTERS,\n\t\t     mlx5_ib_counters_init,\n\t\t     mlx5_ib_counters_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_CONG_DEBUGFS,\n\t\t     mlx5_ib_stage_cong_debugfs_init,\n\t\t     mlx5_ib_stage_cong_debugfs_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_UAR,\n\t\t     mlx5_ib_stage_uar_init,\n\t\t     mlx5_ib_stage_uar_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_BFREG,\n\t\t     mlx5_ib_stage_bfrag_init,\n\t\t     mlx5_ib_stage_bfrag_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_PRE_IB_REG_UMR,\n\t\t     NULL,\n\t\t     mlx5_ib_stage_pre_ib_reg_umr_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_WHITELIST_UID,\n\t\t     mlx5_ib_devx_init,\n\t\t     mlx5_ib_devx_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_IB_REG,\n\t\t     mlx5_ib_stage_ib_reg_init,\n\t\t     mlx5_ib_stage_ib_reg_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_POST_IB_REG_UMR,\n\t\t     mlx5_ib_stage_post_ib_reg_umr_init,\n\t\t     NULL),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DELAY_DROP,\n\t\t     mlx5_ib_stage_delay_drop_init,\n\t\t     mlx5_ib_stage_delay_drop_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_RESTRACK,\n\t\t     mlx5_ib_restrack_init,\n\t\t     NULL),\n};\n\nconst struct mlx5_ib_profile raw_eth_profile = {\n\tSTAGE_CREATE(MLX5_IB_STAGE_INIT,\n\t\t     mlx5_ib_stage_init_init,\n\t\t     mlx5_ib_stage_init_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_FS,\n\t\t     mlx5_ib_fs_init,\n\t\t     mlx5_ib_fs_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_CAPS,\n\t\t     mlx5_ib_stage_caps_init,\n\t\t     mlx5_ib_stage_caps_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_NON_DEFAULT_CB,\n\t\t     mlx5_ib_stage_raw_eth_non_default_cb,\n\t\t     NULL),\n\tSTAGE_CREATE(MLX5_IB_STAGE_ROCE,\n\t\t     mlx5_ib_roce_init,\n\t\t     mlx5_ib_roce_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_QP,\n\t\t     mlx5_init_qp_table,\n\t\t     mlx5_cleanup_qp_table),\n\tSTAGE_CREATE(MLX5_IB_STAGE_SRQ,\n\t\t     mlx5_init_srq_table,\n\t\t     mlx5_cleanup_srq_table),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DEVICE_RESOURCES,\n\t\t     mlx5_ib_dev_res_init,\n\t\t     mlx5_ib_dev_res_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DEVICE_NOTIFIER,\n\t\t     mlx5_ib_stage_dev_notifier_init,\n\t\t     mlx5_ib_stage_dev_notifier_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_COUNTERS,\n\t\t     mlx5_ib_counters_init,\n\t\t     mlx5_ib_counters_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_CONG_DEBUGFS,\n\t\t     mlx5_ib_stage_cong_debugfs_init,\n\t\t     mlx5_ib_stage_cong_debugfs_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_UAR,\n\t\t     mlx5_ib_stage_uar_init,\n\t\t     mlx5_ib_stage_uar_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_BFREG,\n\t\t     mlx5_ib_stage_bfrag_init,\n\t\t     mlx5_ib_stage_bfrag_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_PRE_IB_REG_UMR,\n\t\t     NULL,\n\t\t     mlx5_ib_stage_pre_ib_reg_umr_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_WHITELIST_UID,\n\t\t     mlx5_ib_devx_init,\n\t\t     mlx5_ib_devx_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_IB_REG,\n\t\t     mlx5_ib_stage_ib_reg_init,\n\t\t     mlx5_ib_stage_ib_reg_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_POST_IB_REG_UMR,\n\t\t     mlx5_ib_stage_post_ib_reg_umr_init,\n\t\t     NULL),\n\tSTAGE_CREATE(MLX5_IB_STAGE_DELAY_DROP,\n\t\t     mlx5_ib_stage_delay_drop_init,\n\t\t     mlx5_ib_stage_delay_drop_cleanup),\n\tSTAGE_CREATE(MLX5_IB_STAGE_RESTRACK,\n\t\t     mlx5_ib_restrack_init,\n\t\t     NULL),\n};\n\nstatic int mlx5r_mp_probe(struct auxiliary_device *adev,\n\t\t\t  const struct auxiliary_device_id *id)\n{\n\tstruct mlx5_adev *idev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5_core_dev *mdev = idev->mdev;\n\tstruct mlx5_ib_multiport_info *mpi;\n\tstruct mlx5_ib_dev *dev;\n\tbool bound = false;\n\tint err;\n\n\tmpi = kzalloc(sizeof(*mpi), GFP_KERNEL);\n\tif (!mpi)\n\t\treturn -ENOMEM;\n\n\tmpi->mdev = mdev;\n\terr = mlx5_query_nic_vport_system_image_guid(mdev,\n\t\t\t\t\t\t     &mpi->sys_image_guid);\n\tif (err) {\n\t\tkfree(mpi);\n\t\treturn err;\n\t}\n\n\tmutex_lock(&mlx5_ib_multiport_mutex);\n\tlist_for_each_entry(dev, &mlx5_ib_dev_list, ib_dev_list) {\n\t\tif (dev->sys_image_guid == mpi->sys_image_guid)\n\t\t\tbound = mlx5_ib_bind_slave_port(dev, mpi);\n\n\t\tif (bound) {\n\t\t\trdma_roce_rescan_device(&dev->ib_dev);\n\t\t\tmpi->ibdev->ib_active = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!bound) {\n\t\tlist_add_tail(&mpi->list, &mlx5_ib_unaffiliated_port_list);\n\t\tdev_dbg(mdev->device,\n\t\t\t\"no suitable IB device found to bind to, added to unaffiliated list.\\n\");\n\t}\n\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\n\tauxiliary_set_drvdata(adev, mpi);\n\treturn 0;\n}\n\nstatic void mlx5r_mp_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx5_ib_multiport_info *mpi;\n\n\tmpi = auxiliary_get_drvdata(adev);\n\tmutex_lock(&mlx5_ib_multiport_mutex);\n\tif (mpi->ibdev)\n\t\tmlx5_ib_unbind_slave_port(mpi->ibdev, mpi);\n\telse\n\t\tlist_del(&mpi->list);\n\tmutex_unlock(&mlx5_ib_multiport_mutex);\n\tkfree(mpi);\n}\n\nstatic int mlx5r_probe(struct auxiliary_device *adev,\n\t\t       const struct auxiliary_device_id *id)\n{\n\tstruct mlx5_adev *idev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5_core_dev *mdev = idev->mdev;\n\tconst struct mlx5_ib_profile *profile;\n\tint port_type_cap, num_ports, ret;\n\tenum rdma_link_layer ll;\n\tstruct mlx5_ib_dev *dev;\n\n\tport_type_cap = MLX5_CAP_GEN(mdev, port_type);\n\tll = mlx5_port_type_cap_to_rdma_ll(port_type_cap);\n\n\tnum_ports = max(MLX5_CAP_GEN(mdev, num_ports),\n\t\t\tMLX5_CAP_GEN(mdev, num_vhca_ports));\n\tdev = ib_alloc_device(mlx5_ib_dev, ib_dev);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\tdev->port = kcalloc(num_ports, sizeof(*dev->port),\n\t\t\t     GFP_KERNEL);\n\tif (!dev->port) {\n\t\tib_dealloc_device(&dev->ib_dev);\n\t\treturn -ENOMEM;\n\t}\n\n\tdev->mdev = mdev;\n\tdev->num_ports = num_ports;\n\n\tif (ll == IB_LINK_LAYER_ETHERNET && !mlx5_get_roce_state(mdev))\n\t\tprofile = &raw_eth_profile;\n\telse\n\t\tprofile = &pf_profile;\n\n\tret = __mlx5_ib_add(dev, profile);\n\tif (ret) {\n\t\tkfree(dev->port);\n\t\tib_dealloc_device(&dev->ib_dev);\n\t\treturn ret;\n\t}\n\n\tauxiliary_set_drvdata(adev, dev);\n\treturn 0;\n}\n\nstatic void mlx5r_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx5_ib_dev *dev;\n\n\tdev = auxiliary_get_drvdata(adev);\n\t__mlx5_ib_remove(dev, dev->profile, MLX5_IB_STAGE_MAX);\n}\n\nstatic const struct auxiliary_device_id mlx5r_mp_id_table[] = {\n\t{ .name = MLX5_ADEV_NAME \".multiport\", },\n\t{},\n};\n\nstatic const struct auxiliary_device_id mlx5r_id_table[] = {\n\t{ .name = MLX5_ADEV_NAME \".rdma\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, mlx5r_mp_id_table);\nMODULE_DEVICE_TABLE(auxiliary, mlx5r_id_table);\n\nstatic struct auxiliary_driver mlx5r_mp_driver = {\n\t.name = \"multiport\",\n\t.probe = mlx5r_mp_probe,\n\t.remove = mlx5r_mp_remove,\n\t.id_table = mlx5r_mp_id_table,\n};\n\nstatic struct auxiliary_driver mlx5r_driver = {\n\t.name = \"rdma\",\n\t.probe = mlx5r_probe,\n\t.remove = mlx5r_remove,\n\t.id_table = mlx5r_id_table,\n};\n\nstatic int __init mlx5_ib_init(void)\n{\n\tint ret;\n\n\txlt_emergency_page = (void *)__get_free_page(GFP_KERNEL);\n\tif (!xlt_emergency_page)\n\t\treturn -ENOMEM;\n\n\tmlx5_ib_event_wq = alloc_ordered_workqueue(\"mlx5_ib_event_wq\", 0);\n\tif (!mlx5_ib_event_wq) {\n\t\tfree_page((unsigned long)xlt_emergency_page);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = mlx5_ib_qp_event_init();\n\tif (ret)\n\t\tgoto qp_event_err;\n\n\tmlx5_ib_odp_init();\n\tret = mlx5r_rep_init();\n\tif (ret)\n\t\tgoto rep_err;\n\tret = auxiliary_driver_register(&mlx5r_mp_driver);\n\tif (ret)\n\t\tgoto mp_err;\n\tret = auxiliary_driver_register(&mlx5r_driver);\n\tif (ret)\n\t\tgoto drv_err;\n\treturn 0;\n\ndrv_err:\n\tauxiliary_driver_unregister(&mlx5r_mp_driver);\nmp_err:\n\tmlx5r_rep_cleanup();\nrep_err:\n\tmlx5_ib_qp_event_cleanup();\nqp_event_err:\n\tdestroy_workqueue(mlx5_ib_event_wq);\n\tfree_page((unsigned long)xlt_emergency_page);\n\treturn ret;\n}\n\nstatic void __exit mlx5_ib_cleanup(void)\n{\n\tauxiliary_driver_unregister(&mlx5r_driver);\n\tauxiliary_driver_unregister(&mlx5r_mp_driver);\n\tmlx5r_rep_cleanup();\n\n\tmlx5_ib_qp_event_cleanup();\n\tdestroy_workqueue(mlx5_ib_event_wq);\n\tfree_page((unsigned long)xlt_emergency_page);\n}\n\nmodule_init(mlx5_ib_init);\nmodule_exit(mlx5_ib_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}