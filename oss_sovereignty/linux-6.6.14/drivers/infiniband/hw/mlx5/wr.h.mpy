{
  "module_name": "wr.h",
  "hash_id": "bed92d69b2131fc0a26607bc15a313835f88474725c3fbcd33b73f39ea60c14f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/wr.h",
  "human_readable_source": " \n \n\n#ifndef _MLX5_IB_WR_H\n#define _MLX5_IB_WR_H\n\n#include \"mlx5_ib.h\"\n\nenum {\n\tMLX5_IB_SQ_UMR_INLINE_THRESHOLD = 64,\n};\n\nstruct mlx5_wqe_eth_pad {\n\tu8 rsvd0[16];\n};\n\n\n \nstatic inline void *get_sq_edge(struct mlx5_ib_wq *sq, u32 idx)\n{\n\tvoid *fragment_end;\n\n\tfragment_end = mlx5_frag_buf_get_wqe\n\t\t(&sq->fbc,\n\t\t mlx5_frag_buf_get_idx_last_contig_stride(&sq->fbc, idx));\n\n\treturn fragment_end + MLX5_SEND_WQE_BB;\n}\n\n \nstatic inline void handle_post_send_edge(struct mlx5_ib_wq *sq, void **seg,\n\t\t\t\t\t u32 wqe_sz, void **cur_edge)\n{\n\tu32 idx;\n\n\tif (likely(*seg != *cur_edge))\n\t\treturn;\n\n\tidx = (sq->cur_post + (wqe_sz >> 2)) & (sq->wqe_cnt - 1);\n\t*cur_edge = get_sq_edge(sq, idx);\n\n\t*seg = mlx5_frag_buf_get_wqe(&sq->fbc, idx);\n}\n\n \nstatic inline void mlx5r_memcpy_send_wqe(struct mlx5_ib_wq *sq, void **cur_edge,\n\t\t\t\t\t void **seg, u32 *wqe_sz,\n\t\t\t\t\t const void *src, size_t n)\n{\n\twhile (likely(n)) {\n\t\tsize_t leftlen = *cur_edge - *seg;\n\t\tsize_t copysz = min_t(size_t, leftlen, n);\n\t\tsize_t stride;\n\n\t\tmemcpy(*seg, src, copysz);\n\n\t\tn -= copysz;\n\t\tsrc += copysz;\n\t\tstride = !n ? ALIGN(copysz, 16) : copysz;\n\t\t*seg += stride;\n\t\t*wqe_sz += stride >> 4;\n\t\thandle_post_send_edge(sq, seg, *wqe_sz, cur_edge);\n\t}\n}\n\nint mlx5r_wq_overflow(struct mlx5_ib_wq *wq, int nreq, struct ib_cq *ib_cq);\nint mlx5r_begin_wqe(struct mlx5_ib_qp *qp, void **seg,\n\t\t    struct mlx5_wqe_ctrl_seg **ctrl, unsigned int *idx,\n\t\t    int *size, void **cur_edge, int nreq, __be32 general_id,\n\t\t    bool send_signaled, bool solicited);\nvoid mlx5r_finish_wqe(struct mlx5_ib_qp *qp, struct mlx5_wqe_ctrl_seg *ctrl,\n\t\t      void *seg, u8 size, void *cur_edge, unsigned int idx,\n\t\t      u64 wr_id, int nreq, u8 fence, u32 mlx5_opcode);\nvoid mlx5r_ring_db(struct mlx5_ib_qp *qp, unsigned int nreq,\n\t\t   struct mlx5_wqe_ctrl_seg *ctrl);\nint mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t      const struct ib_send_wr **bad_wr, bool drain);\nint mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr, bool drain);\n\nstatic inline int mlx5_ib_post_send_nodrain(struct ib_qp *ibqp,\n\t\t\t\t\t    const struct ib_send_wr *wr,\n\t\t\t\t\t    const struct ib_send_wr **bad_wr)\n{\n\treturn mlx5_ib_post_send(ibqp, wr, bad_wr, false);\n}\n\nstatic inline int mlx5_ib_post_send_drain(struct ib_qp *ibqp,\n\t\t\t\t\t  const struct ib_send_wr *wr,\n\t\t\t\t\t  const struct ib_send_wr **bad_wr)\n{\n\treturn mlx5_ib_post_send(ibqp, wr, bad_wr, true);\n}\n\nstatic inline int mlx5_ib_post_recv_nodrain(struct ib_qp *ibqp,\n\t\t\t\t\t    const struct ib_recv_wr *wr,\n\t\t\t\t\t    const struct ib_recv_wr **bad_wr)\n{\n\treturn mlx5_ib_post_recv(ibqp, wr, bad_wr, false);\n}\n\nstatic inline int mlx5_ib_post_recv_drain(struct ib_qp *ibqp,\n\t\t\t\t\t  const struct ib_recv_wr *wr,\n\t\t\t\t\t  const struct ib_recv_wr **bad_wr)\n{\n\treturn mlx5_ib_post_recv(ibqp, wr, bad_wr, true);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}