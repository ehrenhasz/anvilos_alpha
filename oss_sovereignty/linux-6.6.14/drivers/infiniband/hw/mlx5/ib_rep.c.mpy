{
  "module_name": "ib_rep.c",
  "hash_id": "d850f489c71d9b901777031e029d0f0a3d6c96c2f50123473c52127b7b003700",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/ib_rep.c",
  "human_readable_source": "\n \n\n#include <linux/mlx5/vport.h>\n#include \"ib_rep.h\"\n#include \"srq.h\"\n\nstatic int\nmlx5_ib_set_vport_rep(struct mlx5_core_dev *dev,\n\t\t      struct mlx5_eswitch_rep *rep,\n\t\t      int vport_index)\n{\n\tstruct mlx5_ib_dev *ibdev;\n\n\tibdev = mlx5_eswitch_uplink_get_proto_dev(dev->priv.eswitch, REP_IB);\n\tif (!ibdev)\n\t\treturn -EINVAL;\n\n\tibdev->port[vport_index].rep = rep;\n\trep->rep_data[REP_IB].priv = ibdev;\n\twrite_lock(&ibdev->port[vport_index].roce.netdev_lock);\n\tibdev->port[vport_index].roce.netdev =\n\t\tmlx5_ib_get_rep_netdev(rep->esw, rep->vport);\n\twrite_unlock(&ibdev->port[vport_index].roce.netdev_lock);\n\n\treturn 0;\n}\n\nstatic void mlx5_ib_register_peer_vport_reps(struct mlx5_core_dev *mdev);\n\nstatic void mlx5_ib_num_ports_update(struct mlx5_core_dev *dev, u32 *num_ports)\n{\n\tstruct mlx5_core_dev *peer_dev;\n\tint i;\n\n\tmlx5_lag_for_each_peer_mdev(dev, peer_dev, i) {\n\t\tu32 peer_num_ports = mlx5_eswitch_get_total_vports(peer_dev);\n\n\t\tif (mlx5_lag_is_mpesw(peer_dev))\n\t\t\t*num_ports += peer_num_ports;\n\t\telse\n\t\t\t \n\t\t\t*num_ports += peer_num_ports - 1;\n\t}\n}\n\nstatic int\nmlx5_ib_vport_rep_load(struct mlx5_core_dev *dev, struct mlx5_eswitch_rep *rep)\n{\n\tu32 num_ports = mlx5_eswitch_get_total_vports(dev);\n\tstruct mlx5_core_dev *lag_master = dev;\n\tconst struct mlx5_ib_profile *profile;\n\tstruct mlx5_core_dev *peer_dev;\n\tstruct mlx5_ib_dev *ibdev;\n\tint new_uplink = false;\n\tint vport_index;\n\tint ret;\n\tint i;\n\n\tvport_index = rep->vport_index;\n\n\tif (mlx5_lag_is_shared_fdb(dev)) {\n\t\tif (mlx5_lag_is_master(dev)) {\n\t\t\tmlx5_ib_num_ports_update(dev, &num_ports);\n\t\t} else {\n\t\t\tif (rep->vport == MLX5_VPORT_UPLINK) {\n\t\t\t\tif (!mlx5_lag_is_mpesw(dev))\n\t\t\t\t\treturn 0;\n\t\t\t\tnew_uplink = true;\n\t\t\t}\n\t\t\tmlx5_lag_for_each_peer_mdev(dev, peer_dev, i) {\n\t\t\t\tu32 peer_n_ports = mlx5_eswitch_get_total_vports(peer_dev);\n\n\t\t\t\tif (mlx5_lag_is_master(peer_dev))\n\t\t\t\t\tlag_master = peer_dev;\n\t\t\t\telse if (!mlx5_lag_is_mpesw(dev))\n\t\t\t\t \n\t\t\t\t\tpeer_n_ports--;\n\n\t\t\t\tif (mlx5_get_dev_index(peer_dev) < mlx5_get_dev_index(dev))\n\t\t\t\t\tvport_index += peer_n_ports;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (rep->vport == MLX5_VPORT_UPLINK && !new_uplink)\n\t\tprofile = &raw_eth_profile;\n\telse\n\t\treturn mlx5_ib_set_vport_rep(lag_master, rep, vport_index);\n\n\tibdev = ib_alloc_device(mlx5_ib_dev, ib_dev);\n\tif (!ibdev)\n\t\treturn -ENOMEM;\n\n\tibdev->port = kcalloc(num_ports, sizeof(*ibdev->port),\n\t\t\t      GFP_KERNEL);\n\tif (!ibdev->port) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_port;\n\t}\n\n\tibdev->is_rep = true;\n\tvport_index = rep->vport_index;\n\tibdev->port[vport_index].rep = rep;\n\tibdev->port[vport_index].roce.netdev =\n\t\tmlx5_ib_get_rep_netdev(lag_master->priv.eswitch, rep->vport);\n\tibdev->mdev = lag_master;\n\tibdev->num_ports = num_ports;\n\n\tret = __mlx5_ib_add(ibdev, profile);\n\tif (ret)\n\t\tgoto fail_add;\n\n\trep->rep_data[REP_IB].priv = ibdev;\n\tif (mlx5_lag_is_shared_fdb(lag_master))\n\t\tmlx5_ib_register_peer_vport_reps(lag_master);\n\n\treturn 0;\n\nfail_add:\n\tkfree(ibdev->port);\nfail_port:\n\tib_dealloc_device(&ibdev->ib_dev);\n\treturn ret;\n}\n\nstatic void *mlx5_ib_rep_to_dev(struct mlx5_eswitch_rep *rep)\n{\n\treturn rep->rep_data[REP_IB].priv;\n}\n\nstatic void\nmlx5_ib_vport_rep_unload(struct mlx5_eswitch_rep *rep)\n{\n\tstruct mlx5_core_dev *mdev = mlx5_eswitch_get_core_dev(rep->esw);\n\tstruct mlx5_ib_dev *dev = mlx5_ib_rep_to_dev(rep);\n\tint vport_index = rep->vport_index;\n\tstruct mlx5_ib_port *port;\n\tint i;\n\n\tif (WARN_ON(!mdev))\n\t\treturn;\n\n\tif (!dev)\n\t\treturn;\n\n\tif (mlx5_lag_is_shared_fdb(mdev) &&\n\t    !mlx5_lag_is_master(mdev)) {\n\t\tif (rep->vport == MLX5_VPORT_UPLINK && !mlx5_lag_is_mpesw(mdev))\n\t\t\treturn;\n\t\tfor (i = 0; i < dev->num_ports; i++) {\n\t\t\tif (dev->port[i].rep == rep)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (WARN_ON(i == dev->num_ports))\n\t\t\treturn;\n\t\tvport_index = i;\n\t}\n\n\tport = &dev->port[vport_index];\n\twrite_lock(&port->roce.netdev_lock);\n\tport->roce.netdev = NULL;\n\twrite_unlock(&port->roce.netdev_lock);\n\trep->rep_data[REP_IB].priv = NULL;\n\tport->rep = NULL;\n\n\tif (rep->vport == MLX5_VPORT_UPLINK) {\n\n\t\tif (mlx5_lag_is_shared_fdb(mdev) && !mlx5_lag_is_master(mdev))\n\t\t\treturn;\n\n\t\tif (mlx5_lag_is_shared_fdb(mdev)) {\n\t\t\tstruct mlx5_core_dev *peer_mdev;\n\t\t\tstruct mlx5_eswitch *esw;\n\n\t\t\tmlx5_lag_for_each_peer_mdev(mdev, peer_mdev, i) {\n\t\t\t\tesw = peer_mdev->priv.eswitch;\n\t\t\t\tmlx5_eswitch_unregister_vport_reps(esw, REP_IB);\n\t\t\t}\n\t\t}\n\t\t__mlx5_ib_remove(dev, dev->profile, MLX5_IB_STAGE_MAX);\n\t}\n}\n\nstatic const struct mlx5_eswitch_rep_ops rep_ops = {\n\t.load = mlx5_ib_vport_rep_load,\n\t.unload = mlx5_ib_vport_rep_unload,\n\t.get_proto_dev = mlx5_ib_rep_to_dev,\n};\n\nstatic void mlx5_ib_register_peer_vport_reps(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_core_dev *peer_mdev;\n\tstruct mlx5_eswitch *esw;\n\tint i;\n\n\tmlx5_lag_for_each_peer_mdev(mdev, peer_mdev, i) {\n\t\tesw = peer_mdev->priv.eswitch;\n\t\tmlx5_eswitch_register_vport_reps(esw, &rep_ops, REP_IB);\n\t}\n}\n\nstruct net_device *mlx5_ib_get_rep_netdev(struct mlx5_eswitch *esw,\n\t\t\t\t\t  u16 vport_num)\n{\n\treturn mlx5_eswitch_get_proto_dev(esw, vport_num, REP_ETH);\n}\n\nstruct mlx5_flow_handle *create_flow_rule_vport_sq(struct mlx5_ib_dev *dev,\n\t\t\t\t\t\t   struct mlx5_ib_sq *sq,\n\t\t\t\t\t\t   u32 port)\n{\n\tstruct mlx5_eswitch *esw = dev->mdev->priv.eswitch;\n\tstruct mlx5_eswitch_rep *rep;\n\n\tif (!dev->is_rep || !port)\n\t\treturn NULL;\n\n\tif (!dev->port[port - 1].rep)\n\t\treturn ERR_PTR(-EINVAL);\n\n\trep = dev->port[port - 1].rep;\n\n\treturn mlx5_eswitch_add_send_to_vport_rule(esw, esw, rep, sq->base.mqp.qpn);\n}\n\nstatic int mlx5r_rep_probe(struct auxiliary_device *adev,\n\t\t\t   const struct auxiliary_device_id *id)\n{\n\tstruct mlx5_adev *idev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5_core_dev *mdev = idev->mdev;\n\tstruct mlx5_eswitch *esw;\n\n\tesw = mdev->priv.eswitch;\n\tmlx5_eswitch_register_vport_reps(esw, &rep_ops, REP_IB);\n\treturn 0;\n}\n\nstatic void mlx5r_rep_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx5_adev *idev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5_core_dev *mdev = idev->mdev;\n\tstruct mlx5_eswitch *esw;\n\n\tesw = mdev->priv.eswitch;\n\tmlx5_eswitch_unregister_vport_reps(esw, REP_IB);\n}\n\nstatic const struct auxiliary_device_id mlx5r_rep_id_table[] = {\n\t{ .name = MLX5_ADEV_NAME \".rdma-rep\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, mlx5r_rep_id_table);\n\nstatic struct auxiliary_driver mlx5r_rep_driver = {\n\t.name = \"rep\",\n\t.probe = mlx5r_rep_probe,\n\t.remove = mlx5r_rep_remove,\n\t.id_table = mlx5r_rep_id_table,\n};\n\nint mlx5r_rep_init(void)\n{\n\treturn auxiliary_driver_register(&mlx5r_rep_driver);\n}\n\nvoid mlx5r_rep_cleanup(void)\n{\n\tauxiliary_driver_unregister(&mlx5r_rep_driver);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}