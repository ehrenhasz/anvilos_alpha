{
  "module_name": "dm.c",
  "hash_id": "18a484128731cc0422125d07b2198788188d79506e33ec1a48be5f203c86c19b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/mlx5/dm.c",
  "human_readable_source": "\n \n\n#include <rdma/uverbs_std_types.h>\n#include \"dm.h\"\n\n#define UVERBS_MODULE_NAME mlx5_ib\n#include <rdma/uverbs_named_ioctl.h>\n\nstatic int mlx5_cmd_alloc_memic(struct mlx5_dm *dm, phys_addr_t *addr,\n\t\t\t\tu64 length, u32 alignment)\n{\n\tstruct mlx5_core_dev *dev = dm->dev;\n\tu64 num_memic_hw_pages = MLX5_CAP_DEV_MEM(dev, memic_bar_size)\n\t\t\t\t\t>> PAGE_SHIFT;\n\tu64 hw_start_addr = MLX5_CAP64_DEV_MEM(dev, memic_bar_start_addr);\n\tu32 max_alignment = MLX5_CAP_DEV_MEM(dev, log_max_memic_addr_alignment);\n\tu32 num_pages = DIV_ROUND_UP(length, PAGE_SIZE);\n\tu32 out[MLX5_ST_SZ_DW(alloc_memic_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(alloc_memic_in)] = {};\n\tu32 mlx5_alignment;\n\tu64 page_idx = 0;\n\tint ret = 0;\n\n\tif (!length || (length & MLX5_MEMIC_ALLOC_SIZE_MASK))\n\t\treturn -EINVAL;\n\n\t \n\tmlx5_alignment = (alignment < MLX5_MEMIC_BASE_ALIGN) ? 0 :\n\t\t\t alignment - MLX5_MEMIC_BASE_ALIGN;\n\tif (mlx5_alignment > max_alignment)\n\t\treturn -EINVAL;\n\n\tMLX5_SET(alloc_memic_in, in, opcode, MLX5_CMD_OP_ALLOC_MEMIC);\n\tMLX5_SET(alloc_memic_in, in, range_size, num_pages * PAGE_SIZE);\n\tMLX5_SET(alloc_memic_in, in, memic_size, length);\n\tMLX5_SET(alloc_memic_in, in, log_memic_addr_alignment,\n\t\t mlx5_alignment);\n\n\twhile (page_idx < num_memic_hw_pages) {\n\t\tspin_lock(&dm->lock);\n\t\tpage_idx = bitmap_find_next_zero_area(dm->memic_alloc_pages,\n\t\t\t\t\t\t      num_memic_hw_pages,\n\t\t\t\t\t\t      page_idx,\n\t\t\t\t\t\t      num_pages, 0);\n\n\t\tif (page_idx < num_memic_hw_pages)\n\t\t\tbitmap_set(dm->memic_alloc_pages,\n\t\t\t\t   page_idx, num_pages);\n\n\t\tspin_unlock(&dm->lock);\n\n\t\tif (page_idx >= num_memic_hw_pages)\n\t\t\tbreak;\n\n\t\tMLX5_SET64(alloc_memic_in, in, range_start_addr,\n\t\t\t   hw_start_addr + (page_idx * PAGE_SIZE));\n\n\t\tret = mlx5_cmd_exec_inout(dev, alloc_memic, in, out);\n\t\tif (ret) {\n\t\t\tspin_lock(&dm->lock);\n\t\t\tbitmap_clear(dm->memic_alloc_pages,\n\t\t\t\t     page_idx, num_pages);\n\t\t\tspin_unlock(&dm->lock);\n\n\t\t\tif (ret == -EAGAIN) {\n\t\t\t\tpage_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\treturn ret;\n\t\t}\n\n\t\t*addr = dev->bar_addr +\n\t\t\tMLX5_GET64(alloc_memic_out, out, memic_start_addr);\n\n\t\treturn 0;\n\t}\n\n\treturn -ENOMEM;\n}\n\nvoid mlx5_cmd_dealloc_memic(struct mlx5_dm *dm, phys_addr_t addr,\n\t\t\t    u64 length)\n{\n\tstruct mlx5_core_dev *dev = dm->dev;\n\tu64 hw_start_addr = MLX5_CAP64_DEV_MEM(dev, memic_bar_start_addr);\n\tu32 num_pages = DIV_ROUND_UP(length, PAGE_SIZE);\n\tu32 in[MLX5_ST_SZ_DW(dealloc_memic_in)] = {};\n\tu64 start_page_idx;\n\tint err;\n\n\taddr -= dev->bar_addr;\n\tstart_page_idx = (addr - hw_start_addr) >> PAGE_SHIFT;\n\n\tMLX5_SET(dealloc_memic_in, in, opcode, MLX5_CMD_OP_DEALLOC_MEMIC);\n\tMLX5_SET64(dealloc_memic_in, in, memic_start_addr, addr);\n\tMLX5_SET(dealloc_memic_in, in, memic_size, length);\n\n\terr =  mlx5_cmd_exec_in(dev, dealloc_memic, in);\n\tif (err)\n\t\treturn;\n\n\tspin_lock(&dm->lock);\n\tbitmap_clear(dm->memic_alloc_pages,\n\t\t     start_page_idx, num_pages);\n\tspin_unlock(&dm->lock);\n}\n\nvoid mlx5_cmd_dealloc_memic_op(struct mlx5_dm *dm, phys_addr_t addr,\n\t\t\t       u8 operation)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_memic_in)] = {};\n\tstruct mlx5_core_dev *dev = dm->dev;\n\n\tMLX5_SET(modify_memic_in, in, opcode, MLX5_CMD_OP_MODIFY_MEMIC);\n\tMLX5_SET(modify_memic_in, in, op_mod, MLX5_MODIFY_MEMIC_OP_MOD_DEALLOC);\n\tMLX5_SET(modify_memic_in, in, memic_operation_type, operation);\n\tMLX5_SET64(modify_memic_in, in, memic_start_addr, addr - dev->bar_addr);\n\n\tmlx5_cmd_exec_in(dev, modify_memic, in);\n}\n\nstatic int mlx5_cmd_alloc_memic_op(struct mlx5_dm *dm, phys_addr_t addr,\n\t\t\t\t   u8 operation, phys_addr_t *op_addr)\n{\n\tu32 out[MLX5_ST_SZ_DW(modify_memic_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(modify_memic_in)] = {};\n\tstruct mlx5_core_dev *dev = dm->dev;\n\tint err;\n\n\tMLX5_SET(modify_memic_in, in, opcode, MLX5_CMD_OP_MODIFY_MEMIC);\n\tMLX5_SET(modify_memic_in, in, op_mod, MLX5_MODIFY_MEMIC_OP_MOD_ALLOC);\n\tMLX5_SET(modify_memic_in, in, memic_operation_type, operation);\n\tMLX5_SET64(modify_memic_in, in, memic_start_addr, addr - dev->bar_addr);\n\n\terr = mlx5_cmd_exec_inout(dev, modify_memic, in, out);\n\tif (err)\n\t\treturn err;\n\n\t*op_addr = dev->bar_addr +\n\t\t   MLX5_GET64(modify_memic_out, out, memic_operation_addr);\n\treturn 0;\n}\n\nstatic int add_dm_mmap_entry(struct ib_ucontext *context,\n\t\t\t     struct mlx5_user_mmap_entry *mentry, u8 mmap_flag,\n\t\t\t     size_t size, u64 address)\n{\n\tmentry->mmap_flag = mmap_flag;\n\tmentry->address = address;\n\n\treturn rdma_user_mmap_entry_insert_range(\n\t\tcontext, &mentry->rdma_entry, size,\n\t\tMLX5_IB_MMAP_DEVICE_MEM << 16,\n\t\t(MLX5_IB_MMAP_DEVICE_MEM << 16) + (1UL << 16) - 1);\n}\n\nstatic void mlx5_ib_dm_memic_free(struct kref *kref)\n{\n\tstruct mlx5_ib_dm_memic *dm =\n\t\tcontainer_of(kref, struct mlx5_ib_dm_memic, ref);\n\tstruct mlx5_ib_dev *dev = to_mdev(dm->base.ibdm.device);\n\n\tmlx5_cmd_dealloc_memic(&dev->dm, dm->base.dev_addr, dm->base.size);\n\tkfree(dm);\n}\n\nstatic int copy_op_to_user(struct mlx5_ib_dm_op_entry *op_entry,\n\t\t\t   struct uverbs_attr_bundle *attrs)\n{\n\tu64 start_offset;\n\tu16 page_idx;\n\tint err;\n\n\tpage_idx = op_entry->mentry.rdma_entry.start_pgoff & 0xFFFF;\n\tstart_offset = op_entry->op_addr & ~PAGE_MASK;\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_DM_MAP_OP_ADDR_RESP_PAGE_INDEX,\n\t\t\t     &page_idx, sizeof(page_idx));\n\tif (err)\n\t\treturn err;\n\n\treturn uverbs_copy_to(attrs,\n\t\t\t      MLX5_IB_ATTR_DM_MAP_OP_ADDR_RESP_START_OFFSET,\n\t\t\t      &start_offset, sizeof(start_offset));\n}\n\nstatic int map_existing_op(struct mlx5_ib_dm_memic *dm, u8 op,\n\t\t\t   struct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_ib_dm_op_entry *op_entry;\n\n\top_entry = xa_load(&dm->ops, op);\n\tif (!op_entry)\n\t\treturn -ENOENT;\n\n\treturn copy_op_to_user(op_entry, attrs);\n}\n\nstatic int UVERBS_HANDLER(MLX5_IB_METHOD_DM_MAP_OP_ADDR)(\n\tstruct uverbs_attr_bundle *attrs)\n{\n\tstruct ib_uobject *uobj = uverbs_attr_get_uobject(\n\t\tattrs, MLX5_IB_ATTR_DM_MAP_OP_ADDR_REQ_HANDLE);\n\tstruct mlx5_ib_dev *dev = to_mdev(uobj->context->device);\n\tstruct ib_dm *ibdm = uobj->object;\n\tstruct mlx5_ib_dm_memic *dm = to_memic(ibdm);\n\tstruct mlx5_ib_dm_op_entry *op_entry;\n\tint err;\n\tu8 op;\n\n\terr = uverbs_copy_from(&op, attrs, MLX5_IB_ATTR_DM_MAP_OP_ADDR_REQ_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (op >= BITS_PER_TYPE(u32))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!(MLX5_CAP_DEV_MEM(dev->mdev, memic_operations) & BIT(op)))\n\t\treturn -EOPNOTSUPP;\n\n\tmutex_lock(&dm->ops_xa_lock);\n\terr = map_existing_op(dm, op, attrs);\n\tif (!err || err != -ENOENT)\n\t\tgoto err_unlock;\n\n\top_entry = kzalloc(sizeof(*op_entry), GFP_KERNEL);\n\tif (!op_entry)\n\t\tgoto err_unlock;\n\n\terr = mlx5_cmd_alloc_memic_op(&dev->dm, dm->base.dev_addr, op,\n\t\t\t\t      &op_entry->op_addr);\n\tif (err) {\n\t\tkfree(op_entry);\n\t\tgoto err_unlock;\n\t}\n\top_entry->op = op;\n\top_entry->dm = dm;\n\n\terr = add_dm_mmap_entry(uobj->context, &op_entry->mentry,\n\t\t\t\tMLX5_IB_MMAP_TYPE_MEMIC_OP, dm->base.size,\n\t\t\t\top_entry->op_addr & PAGE_MASK);\n\tif (err) {\n\t\tmlx5_cmd_dealloc_memic_op(&dev->dm, dm->base.dev_addr, op);\n\t\tkfree(op_entry);\n\t\tgoto err_unlock;\n\t}\n\t \n\tkref_get(&dm->ref);\n\n\terr = copy_op_to_user(op_entry, attrs);\n\tif (err)\n\t\tgoto err_remove;\n\n\terr = xa_insert(&dm->ops, op, op_entry, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_remove;\n\tmutex_unlock(&dm->ops_xa_lock);\n\n\treturn 0;\n\nerr_remove:\n\trdma_user_mmap_entry_remove(&op_entry->mentry.rdma_entry);\nerr_unlock:\n\tmutex_unlock(&dm->ops_xa_lock);\n\n\treturn err;\n}\n\nstatic struct ib_dm *handle_alloc_dm_memic(struct ib_ucontext *ctx,\n\t\t\t\t\t   struct ib_dm_alloc_attr *attr,\n\t\t\t\t\t   struct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_dm *dm_db = &to_mdev(ctx->device)->dm;\n\tstruct mlx5_ib_dm_memic *dm;\n\tu64 start_offset;\n\tu16 page_idx;\n\tint err;\n\tu64 address;\n\n\tif (!MLX5_CAP_DEV_MEM(dm_db->dev, memic))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tdm = kzalloc(sizeof(*dm), GFP_KERNEL);\n\tif (!dm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdm->base.type = MLX5_IB_UAPI_DM_TYPE_MEMIC;\n\tdm->base.size = roundup(attr->length, MLX5_MEMIC_BASE_SIZE);\n\tdm->base.ibdm.device = ctx->device;\n\n\tkref_init(&dm->ref);\n\txa_init(&dm->ops);\n\tmutex_init(&dm->ops_xa_lock);\n\tdm->req_length = attr->length;\n\n\terr = mlx5_cmd_alloc_memic(dm_db, &dm->base.dev_addr,\n\t\t\t\t   dm->base.size, attr->alignment);\n\tif (err) {\n\t\tkfree(dm);\n\t\treturn ERR_PTR(err);\n\t}\n\n\taddress = dm->base.dev_addr & PAGE_MASK;\n\terr = add_dm_mmap_entry(ctx, &dm->mentry, MLX5_IB_MMAP_TYPE_MEMIC,\n\t\t\t\tdm->base.size, address);\n\tif (err) {\n\t\tmlx5_cmd_dealloc_memic(dm_db, dm->base.dev_addr, dm->base.size);\n\t\tkfree(dm);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tpage_idx = dm->mentry.rdma_entry.start_pgoff & 0xFFFF;\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_ALLOC_DM_RESP_PAGE_INDEX,\n\t\t\t     &page_idx, sizeof(page_idx));\n\tif (err)\n\t\tgoto err_copy;\n\n\tstart_offset = dm->base.dev_addr & ~PAGE_MASK;\n\terr = uverbs_copy_to(attrs,\n\t\t\t     MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,\n\t\t\t     &start_offset, sizeof(start_offset));\n\tif (err)\n\t\tgoto err_copy;\n\n\treturn &dm->base.ibdm;\n\nerr_copy:\n\trdma_user_mmap_entry_remove(&dm->mentry.rdma_entry);\n\treturn ERR_PTR(err);\n}\n\nstatic enum mlx5_sw_icm_type get_icm_type(int uapi_type)\n{\n\tswitch (uapi_type) {\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_SW_ICM:\n\t\treturn MLX5_SW_ICM_TYPE_HEADER_MODIFY;\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_PATTERN_SW_ICM:\n\t\treturn MLX5_SW_ICM_TYPE_HEADER_MODIFY_PATTERN;\n\tcase MLX5_IB_UAPI_DM_TYPE_STEERING_SW_ICM:\n\tdefault:\n\t\treturn MLX5_SW_ICM_TYPE_STEERING;\n\t}\n}\n\nstatic struct ib_dm *handle_alloc_dm_sw_icm(struct ib_ucontext *ctx,\n\t\t\t\t\t    struct ib_dm_alloc_attr *attr,\n\t\t\t\t\t    struct uverbs_attr_bundle *attrs,\n\t\t\t\t\t    int type)\n{\n\tstruct mlx5_core_dev *dev = to_mdev(ctx->device)->mdev;\n\tenum mlx5_sw_icm_type icm_type;\n\tstruct mlx5_ib_dm_icm *dm;\n\tu64 act_size;\n\tint err;\n\n\tif (!capable(CAP_SYS_RAWIO) || !capable(CAP_NET_RAW))\n\t\treturn ERR_PTR(-EPERM);\n\n\tswitch (type) {\n\tcase MLX5_IB_UAPI_DM_TYPE_STEERING_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_SW_ICM:\n\t\tif (!(MLX5_CAP_FLOWTABLE_NIC_RX(dev, sw_owner) ||\n\t\t      MLX5_CAP_FLOWTABLE_NIC_TX(dev, sw_owner) ||\n\t\t      MLX5_CAP_FLOWTABLE_NIC_RX(dev, sw_owner_v2) ||\n\t\t      MLX5_CAP_FLOWTABLE_NIC_TX(dev, sw_owner_v2)))\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\tbreak;\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_PATTERN_SW_ICM:\n\t\tif (!MLX5_CAP_FLOWTABLE_NIC_RX(dev, sw_owner_v2) ||\n\t\t    !MLX5_CAP_FLOWTABLE_NIC_TX(dev, sw_owner_v2))\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\n\tdm = kzalloc(sizeof(*dm), GFP_KERNEL);\n\tif (!dm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdm->base.type = type;\n\tdm->base.ibdm.device = ctx->device;\n\n\t \n\tact_size = round_up(attr->length, MLX5_SW_ICM_BLOCK_SIZE(dev));\n\tact_size = roundup_pow_of_two(act_size);\n\n\tdm->base.size = act_size;\n\ticm_type = get_icm_type(type);\n\n\terr = mlx5_dm_sw_icm_alloc(dev, icm_type, act_size, attr->alignment,\n\t\t\t\t   to_mucontext(ctx)->devx_uid,\n\t\t\t\t   &dm->base.dev_addr, &dm->obj_id);\n\tif (err)\n\t\tgoto free;\n\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,\n\t\t\t     &dm->base.dev_addr, sizeof(dm->base.dev_addr));\n\tif (err) {\n\t\tmlx5_dm_sw_icm_dealloc(dev, icm_type, dm->base.size,\n\t\t\t\t       to_mucontext(ctx)->devx_uid,\n\t\t\t\t       dm->base.dev_addr, dm->obj_id);\n\t\tgoto free;\n\t}\n\treturn &dm->base.ibdm;\nfree:\n\tkfree(dm);\n\treturn ERR_PTR(err);\n}\n\nstruct ib_dm *mlx5_ib_alloc_dm(struct ib_device *ibdev,\n\t\t\t       struct ib_ucontext *context,\n\t\t\t       struct ib_dm_alloc_attr *attr,\n\t\t\t       struct uverbs_attr_bundle *attrs)\n{\n\tenum mlx5_ib_uapi_dm_type type;\n\tint err;\n\n\terr = uverbs_get_const_default(&type, attrs,\n\t\t\t\t       MLX5_IB_ATTR_ALLOC_DM_REQ_TYPE,\n\t\t\t\t       MLX5_IB_UAPI_DM_TYPE_MEMIC);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tmlx5_ib_dbg(to_mdev(ibdev), \"alloc_dm req: dm_type=%d user_length=0x%llx log_alignment=%d\\n\",\n\t\t    type, attr->length, attr->alignment);\n\n\tswitch (type) {\n\tcase MLX5_IB_UAPI_DM_TYPE_MEMIC:\n\t\treturn handle_alloc_dm_memic(context, attr, attrs);\n\tcase MLX5_IB_UAPI_DM_TYPE_STEERING_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_PATTERN_SW_ICM:\n\t\treturn handle_alloc_dm_sw_icm(context, attr, attrs, type);\n\tdefault:\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n}\n\nstatic void dm_memic_remove_ops(struct mlx5_ib_dm_memic *dm)\n{\n\tstruct mlx5_ib_dm_op_entry *entry;\n\tunsigned long idx;\n\n\tmutex_lock(&dm->ops_xa_lock);\n\txa_for_each(&dm->ops, idx, entry) {\n\t\txa_erase(&dm->ops, idx);\n\t\trdma_user_mmap_entry_remove(&entry->mentry.rdma_entry);\n\t}\n\tmutex_unlock(&dm->ops_xa_lock);\n}\n\nstatic void mlx5_dm_memic_dealloc(struct mlx5_ib_dm_memic *dm)\n{\n\tdm_memic_remove_ops(dm);\n\trdma_user_mmap_entry_remove(&dm->mentry.rdma_entry);\n}\n\nstatic int mlx5_dm_icm_dealloc(struct mlx5_ib_ucontext *ctx,\n\t\t\t       struct mlx5_ib_dm_icm *dm)\n{\n\tenum mlx5_sw_icm_type type = get_icm_type(dm->base.type);\n\tstruct mlx5_core_dev *dev = to_mdev(dm->base.ibdm.device)->mdev;\n\tint err;\n\n\terr = mlx5_dm_sw_icm_dealloc(dev, type, dm->base.size, ctx->devx_uid,\n\t\t\t\t     dm->base.dev_addr, dm->obj_id);\n\tif (!err)\n\t\tkfree(dm);\n\treturn 0;\n}\n\nstatic int mlx5_ib_dealloc_dm(struct ib_dm *ibdm,\n\t\t\t      struct uverbs_attr_bundle *attrs)\n{\n\tstruct mlx5_ib_ucontext *ctx = rdma_udata_to_drv_context(\n\t\t&attrs->driver_udata, struct mlx5_ib_ucontext, ibucontext);\n\tstruct mlx5_ib_dm *dm = to_mdm(ibdm);\n\n\tswitch (dm->type) {\n\tcase MLX5_IB_UAPI_DM_TYPE_MEMIC:\n\t\tmlx5_dm_memic_dealloc(to_memic(ibdm));\n\t\treturn 0;\n\tcase MLX5_IB_UAPI_DM_TYPE_STEERING_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_SW_ICM:\n\tcase MLX5_IB_UAPI_DM_TYPE_HEADER_MODIFY_PATTERN_SW_ICM:\n\t\treturn mlx5_dm_icm_dealloc(ctx, to_icm(ibdm));\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int UVERBS_HANDLER(MLX5_IB_METHOD_DM_QUERY)(\n\tstruct uverbs_attr_bundle *attrs)\n{\n\tstruct ib_dm *ibdm =\n\t\tuverbs_attr_get_obj(attrs, MLX5_IB_ATTR_QUERY_DM_REQ_HANDLE);\n\tstruct mlx5_ib_dm *dm = to_mdm(ibdm);\n\tstruct mlx5_ib_dm_memic *memic;\n\tu64 start_offset;\n\tu16 page_idx;\n\tint err;\n\n\tif (dm->type != MLX5_IB_UAPI_DM_TYPE_MEMIC)\n\t\treturn -EOPNOTSUPP;\n\n\tmemic = to_memic(ibdm);\n\tpage_idx = memic->mentry.rdma_entry.start_pgoff & 0xFFFF;\n\terr = uverbs_copy_to(attrs, MLX5_IB_ATTR_QUERY_DM_RESP_PAGE_INDEX,\n\t\t\t     &page_idx, sizeof(page_idx));\n\tif (err)\n\t\treturn err;\n\n\tstart_offset = memic->base.dev_addr & ~PAGE_MASK;\n\terr =  uverbs_copy_to(attrs, MLX5_IB_ATTR_QUERY_DM_RESP_START_OFFSET,\n\t\t\t      &start_offset, sizeof(start_offset));\n\tif (err)\n\t\treturn err;\n\n\treturn uverbs_copy_to(attrs, MLX5_IB_ATTR_QUERY_DM_RESP_LENGTH,\n\t\t\t      &memic->req_length,\n\t\t\t      sizeof(memic->req_length));\n}\n\nvoid mlx5_ib_dm_mmap_free(struct mlx5_ib_dev *dev,\n\t\t\t  struct mlx5_user_mmap_entry *mentry)\n{\n\tstruct mlx5_ib_dm_op_entry *op_entry;\n\tstruct mlx5_ib_dm_memic *mdm;\n\n\tswitch (mentry->mmap_flag) {\n\tcase MLX5_IB_MMAP_TYPE_MEMIC:\n\t\tmdm = container_of(mentry, struct mlx5_ib_dm_memic, mentry);\n\t\tkref_put(&mdm->ref, mlx5_ib_dm_memic_free);\n\t\tbreak;\n\tcase MLX5_IB_MMAP_TYPE_MEMIC_OP:\n\t\top_entry = container_of(mentry, struct mlx5_ib_dm_op_entry,\n\t\t\t\t\tmentry);\n\t\tmdm = op_entry->dm;\n\t\tmlx5_cmd_dealloc_memic_op(&dev->dm, mdm->base.dev_addr,\n\t\t\t\t\t  op_entry->op);\n\t\tkfree(op_entry);\n\t\tkref_put(&mdm->ref, mlx5_ib_dm_memic_free);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(true);\n\t}\n}\n\nDECLARE_UVERBS_NAMED_METHOD(\n\tMLX5_IB_METHOD_DM_QUERY,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_QUERY_DM_REQ_HANDLE, UVERBS_OBJECT_DM,\n\t\t\tUVERBS_ACCESS_READ, UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_QUERY_DM_RESP_START_OFFSET,\n\t\t\t    UVERBS_ATTR_TYPE(u64), UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_QUERY_DM_RESP_PAGE_INDEX,\n\t\t\t    UVERBS_ATTR_TYPE(u16), UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_QUERY_DM_RESP_LENGTH,\n\t\t\t    UVERBS_ATTR_TYPE(u64), UA_MANDATORY));\n\nADD_UVERBS_ATTRIBUTES_SIMPLE(\n\tmlx5_ib_dm, UVERBS_OBJECT_DM, UVERBS_METHOD_DM_ALLOC,\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_START_OFFSET,\n\t\t\t    UVERBS_ATTR_TYPE(u64), UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_ALLOC_DM_RESP_PAGE_INDEX,\n\t\t\t    UVERBS_ATTR_TYPE(u16), UA_OPTIONAL),\n\tUVERBS_ATTR_CONST_IN(MLX5_IB_ATTR_ALLOC_DM_REQ_TYPE,\n\t\t\t     enum mlx5_ib_uapi_dm_type, UA_OPTIONAL));\n\nDECLARE_UVERBS_NAMED_METHOD(\n\tMLX5_IB_METHOD_DM_MAP_OP_ADDR,\n\tUVERBS_ATTR_IDR(MLX5_IB_ATTR_DM_MAP_OP_ADDR_REQ_HANDLE,\n\t\t\tUVERBS_OBJECT_DM,\n\t\t\tUVERBS_ACCESS_READ,\n\t\t\tUA_MANDATORY),\n\tUVERBS_ATTR_PTR_IN(MLX5_IB_ATTR_DM_MAP_OP_ADDR_REQ_OP,\n\t\t\t   UVERBS_ATTR_TYPE(u8),\n\t\t\t   UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_DM_MAP_OP_ADDR_RESP_START_OFFSET,\n\t\t\t    UVERBS_ATTR_TYPE(u64),\n\t\t\t    UA_MANDATORY),\n\tUVERBS_ATTR_PTR_OUT(MLX5_IB_ATTR_DM_MAP_OP_ADDR_RESP_PAGE_INDEX,\n\t\t\t    UVERBS_ATTR_TYPE(u16),\n\t\t\t    UA_OPTIONAL));\n\nDECLARE_UVERBS_GLOBAL_METHODS(UVERBS_OBJECT_DM,\n\t\t\t      &UVERBS_METHOD(MLX5_IB_METHOD_DM_MAP_OP_ADDR),\n\t\t\t      &UVERBS_METHOD(MLX5_IB_METHOD_DM_QUERY));\n\nconst struct uapi_definition mlx5_ib_dm_defs[] = {\n\tUAPI_DEF_CHAIN_OBJ_TREE(UVERBS_OBJECT_DM, &mlx5_ib_dm),\n\tUAPI_DEF_CHAIN_OBJ_TREE_NAMED(UVERBS_OBJECT_DM),\n\t{},\n};\n\nconst struct ib_device_ops mlx5_ib_dev_dm_ops = {\n\t.alloc_dm = mlx5_ib_alloc_dm,\n\t.dealloc_dm = mlx5_ib_dealloc_dm,\n\t.reg_dm_mr = mlx5_ib_reg_dm_mr,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}