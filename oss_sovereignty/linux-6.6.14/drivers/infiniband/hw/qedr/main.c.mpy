{
  "module_name": "main.c",
  "hash_id": "651a13934b29b9c15dc863f7a7a0891896e24b45d8a4647493c1ebefd3600a73",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qedr/main.c",
  "human_readable_source": " \n#include <linux/module.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/iw_cm.h>\n#include <rdma/ib_mad.h>\n#include <linux/netdevice.h>\n#include <linux/iommu.h>\n#include <linux/pci.h>\n#include <net/addrconf.h>\n\n#include <linux/qed/qed_chain.h>\n#include <linux/qed/qed_if.h>\n#include \"qedr.h\"\n#include \"verbs.h\"\n#include <rdma/qedr-abi.h>\n#include \"qedr_iw_cm.h\"\n\nMODULE_DESCRIPTION(\"QLogic 40G/100G ROCE Driver\");\nMODULE_AUTHOR(\"QLogic Corporation\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n#define QEDR_WQ_MULTIPLIER_DFT\t(3)\n\nstatic void qedr_ib_dispatch_event(struct qedr_dev *dev, u32 port_num,\n\t\t\t\t   enum ib_event_type type)\n{\n\tstruct ib_event ibev;\n\n\tibev.device = &dev->ibdev;\n\tibev.element.port_num = port_num;\n\tibev.event = type;\n\n\tib_dispatch_event(&ibev);\n}\n\nstatic enum rdma_link_layer qedr_link_layer(struct ib_device *device,\n\t\t\t\t\t    u32 port_num)\n{\n\treturn IB_LINK_LAYER_ETHERNET;\n}\n\nstatic void qedr_get_dev_fw_str(struct ib_device *ibdev, char *str)\n{\n\tstruct qedr_dev *qedr = get_qedr_dev(ibdev);\n\tu32 fw_ver = (u32)qedr->attr.fw_ver;\n\n\tsnprintf(str, IB_FW_VERSION_NAME_MAX, \"%d.%d.%d.%d\",\n\t\t (fw_ver >> 24) & 0xFF, (fw_ver >> 16) & 0xFF,\n\t\t (fw_ver >> 8) & 0xFF, fw_ver & 0xFF);\n}\n\nstatic int qedr_roce_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t\t    struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tint err;\n\n\terr = qedr_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE |\n\t    RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\n\treturn 0;\n}\n\nstatic int qedr_iw_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t\t  struct ib_port_immutable *immutable)\n{\n\tstruct ib_port_attr attr;\n\tint err;\n\n\terr = qedr_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\treturn err;\n\n\timmutable->gid_tbl_len = 1;\n\timmutable->core_cap_flags = RDMA_CORE_PORT_IWARP;\n\timmutable->max_mad_size = 0;\n\n\treturn 0;\n}\n\n \nstatic ssize_t hw_rev_show(struct device *device, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct qedr_dev *dev =\n\t\trdma_device_to_drv_device(device, struct qedr_dev, ibdev);\n\n\treturn sysfs_emit(buf, \"0x%x\\n\", dev->attr.hw_ver);\n}\nstatic DEVICE_ATTR_RO(hw_rev);\n\nstatic ssize_t hca_type_show(struct device *device,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct qedr_dev *dev =\n\t\trdma_device_to_drv_device(device, struct qedr_dev, ibdev);\n\n\treturn sysfs_emit(buf, \"FastLinQ QL%x %s\\n\", dev->pdev->device,\n\t\t\t  rdma_protocol_iwarp(&dev->ibdev, 1) ? \"iWARP\" :\n\t\t\t\t\t\t\t\t\"RoCE\");\n}\nstatic DEVICE_ATTR_RO(hca_type);\n\nstatic struct attribute *qedr_attributes[] = {\n\t&dev_attr_hw_rev.attr,\n\t&dev_attr_hca_type.attr,\n\tNULL\n};\n\nstatic const struct attribute_group qedr_attr_group = {\n\t.attrs = qedr_attributes,\n};\n\nstatic const struct ib_device_ops qedr_iw_dev_ops = {\n\t.get_port_immutable = qedr_iw_port_immutable,\n\t.iw_accept = qedr_iw_accept,\n\t.iw_add_ref = qedr_iw_qp_add_ref,\n\t.iw_connect = qedr_iw_connect,\n\t.iw_create_listen = qedr_iw_create_listen,\n\t.iw_destroy_listen = qedr_iw_destroy_listen,\n\t.iw_get_qp = qedr_iw_get_qp,\n\t.iw_reject = qedr_iw_reject,\n\t.iw_rem_ref = qedr_iw_qp_rem_ref,\n\t.query_gid = qedr_iw_query_gid,\n};\n\nstatic int qedr_iw_register_device(struct qedr_dev *dev)\n{\n\tdev->ibdev.node_type = RDMA_NODE_RNIC;\n\n\tib_set_device_ops(&dev->ibdev, &qedr_iw_dev_ops);\n\n\tmemcpy(dev->ibdev.iw_ifname,\n\t       dev->ndev->name, sizeof(dev->ibdev.iw_ifname));\n\n\treturn 0;\n}\n\nstatic const struct ib_device_ops qedr_roce_dev_ops = {\n\t.alloc_xrcd = qedr_alloc_xrcd,\n\t.dealloc_xrcd = qedr_dealloc_xrcd,\n\t.get_port_immutable = qedr_roce_port_immutable,\n\t.query_pkey = qedr_query_pkey,\n};\n\nstatic void qedr_roce_register_device(struct qedr_dev *dev)\n{\n\tdev->ibdev.node_type = RDMA_NODE_IB_CA;\n\n\tib_set_device_ops(&dev->ibdev, &qedr_roce_dev_ops);\n}\n\nstatic const struct ib_device_ops qedr_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_QEDR,\n\t.uverbs_abi_ver = QEDR_ABI_VERSION,\n\n\t.alloc_mr = qedr_alloc_mr,\n\t.alloc_pd = qedr_alloc_pd,\n\t.alloc_ucontext = qedr_alloc_ucontext,\n\t.create_ah = qedr_create_ah,\n\t.create_cq = qedr_create_cq,\n\t.create_qp = qedr_create_qp,\n\t.create_srq = qedr_create_srq,\n\t.dealloc_pd = qedr_dealloc_pd,\n\t.dealloc_ucontext = qedr_dealloc_ucontext,\n\t.dereg_mr = qedr_dereg_mr,\n\t.destroy_ah = qedr_destroy_ah,\n\t.destroy_cq = qedr_destroy_cq,\n\t.destroy_qp = qedr_destroy_qp,\n\t.destroy_srq = qedr_destroy_srq,\n\t.device_group = &qedr_attr_group,\n\t.get_dev_fw_str = qedr_get_dev_fw_str,\n\t.get_dma_mr = qedr_get_dma_mr,\n\t.get_link_layer = qedr_link_layer,\n\t.map_mr_sg = qedr_map_mr_sg,\n\t.mmap = qedr_mmap,\n\t.mmap_free = qedr_mmap_free,\n\t.modify_qp = qedr_modify_qp,\n\t.modify_srq = qedr_modify_srq,\n\t.poll_cq = qedr_poll_cq,\n\t.post_recv = qedr_post_recv,\n\t.post_send = qedr_post_send,\n\t.post_srq_recv = qedr_post_srq_recv,\n\t.process_mad = qedr_process_mad,\n\t.query_device = qedr_query_device,\n\t.query_port = qedr_query_port,\n\t.query_qp = qedr_query_qp,\n\t.query_srq = qedr_query_srq,\n\t.reg_user_mr = qedr_reg_user_mr,\n\t.req_notify_cq = qedr_arm_cq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_ah, qedr_ah, ibah),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, qedr_cq, ibcq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, qedr_pd, ibpd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, qedr_qp, ibqp),\n\tINIT_RDMA_OBJ_SIZE(ib_srq, qedr_srq, ibsrq),\n\tINIT_RDMA_OBJ_SIZE(ib_xrcd, qedr_xrcd, ibxrcd),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, qedr_ucontext, ibucontext),\n};\n\nstatic int qedr_register_device(struct qedr_dev *dev)\n{\n\tint rc;\n\n\tdev->ibdev.node_guid = dev->attr.node_guid;\n\tmemcpy(dev->ibdev.node_desc, QEDR_NODE_DESC, sizeof(QEDR_NODE_DESC));\n\n\tif (IS_IWARP(dev)) {\n\t\trc = qedr_iw_register_device(dev);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else {\n\t\tqedr_roce_register_device(dev);\n\t}\n\n\tdev->ibdev.phys_port_cnt = 1;\n\tdev->ibdev.num_comp_vectors = dev->num_cnq;\n\tdev->ibdev.dev.parent = &dev->pdev->dev;\n\n\tib_set_device_ops(&dev->ibdev, &qedr_dev_ops);\n\n\trc = ib_device_set_netdev(&dev->ibdev, dev->ndev, 1);\n\tif (rc)\n\t\treturn rc;\n\n\tdma_set_max_seg_size(&dev->pdev->dev, UINT_MAX);\n\treturn ib_register_device(&dev->ibdev, \"qedr%d\", &dev->pdev->dev);\n}\n\n \nstatic int qedr_alloc_mem_sb(struct qedr_dev *dev,\n\t\t\t     struct qed_sb_info *sb_info, u16 sb_id)\n{\n\tstruct status_block *sb_virt;\n\tdma_addr_t sb_phys;\n\tint rc;\n\n\tsb_virt = dma_alloc_coherent(&dev->pdev->dev,\n\t\t\t\t     sizeof(*sb_virt), &sb_phys, GFP_KERNEL);\n\tif (!sb_virt)\n\t\treturn -ENOMEM;\n\n\trc = dev->ops->common->sb_init(dev->cdev, sb_info,\n\t\t\t\t       sb_virt, sb_phys, sb_id,\n\t\t\t\t       QED_SB_TYPE_CNQ);\n\tif (rc) {\n\t\tpr_err(\"Status block initialization failed\\n\");\n\t\tdma_free_coherent(&dev->pdev->dev, sizeof(*sb_virt),\n\t\t\t\t  sb_virt, sb_phys);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void qedr_free_mem_sb(struct qedr_dev *dev,\n\t\t\t     struct qed_sb_info *sb_info, int sb_id)\n{\n\tif (sb_info->sb_virt) {\n\t\tdev->ops->common->sb_release(dev->cdev, sb_info, sb_id,\n\t\t\t\t\t     QED_SB_TYPE_CNQ);\n\t\tdma_free_coherent(&dev->pdev->dev, sizeof(*sb_info->sb_virt),\n\t\t\t\t  (void *)sb_info->sb_virt, sb_info->sb_phys);\n\t}\n}\n\nstatic void qedr_free_resources(struct qedr_dev *dev)\n{\n\tint i;\n\n\tif (IS_IWARP(dev))\n\t\tdestroy_workqueue(dev->iwarp_wq);\n\n\tfor (i = 0; i < dev->num_cnq; i++) {\n\t\tqedr_free_mem_sb(dev, &dev->sb_array[i], dev->sb_start + i);\n\t\tdev->ops->common->chain_free(dev->cdev, &dev->cnq_array[i].pbl);\n\t}\n\n\tkfree(dev->cnq_array);\n\tkfree(dev->sb_array);\n\tkfree(dev->sgid_tbl);\n}\n\nstatic int qedr_alloc_resources(struct qedr_dev *dev)\n{\n\tstruct qed_chain_init_params params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.intended_use\t= QED_CHAIN_USE_TO_CONSUME,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U16,\n\t\t.elem_size\t= sizeof(struct regpair *),\n\t};\n\tstruct qedr_cnq *cnq;\n\t__le16 *cons_pi;\n\tint i, rc;\n\n\tdev->sgid_tbl = kcalloc(QEDR_MAX_SGID, sizeof(union ib_gid),\n\t\t\t\tGFP_KERNEL);\n\tif (!dev->sgid_tbl)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&dev->sgid_lock);\n\txa_init_flags(&dev->srqs, XA_FLAGS_LOCK_IRQ);\n\n\tif (IS_IWARP(dev)) {\n\t\txa_init(&dev->qps);\n\t\tdev->iwarp_wq = create_singlethread_workqueue(\"qedr_iwarpq\");\n\t\tif (!dev->iwarp_wq) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\t \n\tdev->sb_array = kcalloc(dev->num_cnq, sizeof(*dev->sb_array),\n\t\t\t\tGFP_KERNEL);\n\tif (!dev->sb_array) {\n\t\trc = -ENOMEM;\n\t\tgoto err_destroy_wq;\n\t}\n\n\tdev->cnq_array = kcalloc(dev->num_cnq,\n\t\t\t\t sizeof(*dev->cnq_array), GFP_KERNEL);\n\tif (!dev->cnq_array) {\n\t\trc = -ENOMEM;\n\t\tgoto err2;\n\t}\n\n\tdev->sb_start = dev->ops->rdma_get_start_sb(dev->cdev);\n\n\t \n\tparams.num_elems = min_t(u32, QED_RDMA_MAX_CNQ_SIZE,\n\t\t\t\t QEDR_ROCE_MAX_CNQ_SIZE);\n\n\tfor (i = 0; i < dev->num_cnq; i++) {\n\t\tcnq = &dev->cnq_array[i];\n\n\t\trc = qedr_alloc_mem_sb(dev, &dev->sb_array[i],\n\t\t\t\t       dev->sb_start + i);\n\t\tif (rc)\n\t\t\tgoto err3;\n\n\t\trc = dev->ops->common->chain_alloc(dev->cdev, &cnq->pbl,\n\t\t\t\t\t\t   &params);\n\t\tif (rc)\n\t\t\tgoto err4;\n\n\t\tcnq->dev = dev;\n\t\tcnq->sb = &dev->sb_array[i];\n\t\tcons_pi = dev->sb_array[i].sb_virt->pi_array;\n\t\tcnq->hw_cons_ptr = &cons_pi[QED_ROCE_PROTOCOL_INDEX];\n\t\tcnq->index = i;\n\t\tsprintf(cnq->name, \"qedr%d@pci:%s\", i, pci_name(dev->pdev));\n\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"cnq[%d].cons=%d\\n\",\n\t\t\t i, qed_chain_get_cons_idx(&cnq->pbl));\n\t}\n\n\treturn 0;\nerr4:\n\tqedr_free_mem_sb(dev, &dev->sb_array[i], dev->sb_start + i);\nerr3:\n\tfor (--i; i >= 0; i--) {\n\t\tdev->ops->common->chain_free(dev->cdev, &dev->cnq_array[i].pbl);\n\t\tqedr_free_mem_sb(dev, &dev->sb_array[i], dev->sb_start + i);\n\t}\n\tkfree(dev->cnq_array);\nerr2:\n\tkfree(dev->sb_array);\nerr_destroy_wq:\n\tif (IS_IWARP(dev))\n\t\tdestroy_workqueue(dev->iwarp_wq);\nerr1:\n\tkfree(dev->sgid_tbl);\n\treturn rc;\n}\n\nstatic void qedr_pci_set_atomic(struct qedr_dev *dev, struct pci_dev *pdev)\n{\n\tint rc = pci_enable_atomic_ops_to_root(pdev,\n\t\t\t\t\t       PCI_EXP_DEVCAP2_ATOMIC_COMP64);\n\n\tif (rc) {\n\t\tdev->atomic_cap = IB_ATOMIC_NONE;\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"Atomic capability disabled\\n\");\n\t} else {\n\t\tdev->atomic_cap = IB_ATOMIC_GLOB;\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"Atomic capability enabled\\n\");\n\t}\n}\n\nstatic const struct qed_rdma_ops *qed_ops;\n\n#define HILO_U64(hi, lo)\t\t((((u64)(hi)) << 32) + (lo))\n\nstatic irqreturn_t qedr_irq_handler(int irq, void *handle)\n{\n\tu16 hw_comp_cons, sw_comp_cons;\n\tstruct qedr_cnq *cnq = handle;\n\tstruct regpair *cq_handle;\n\tstruct qedr_cq *cq;\n\n\tqed_sb_ack(cnq->sb, IGU_INT_DISABLE, 0);\n\n\tqed_sb_update_sb_idx(cnq->sb);\n\n\thw_comp_cons = le16_to_cpu(*cnq->hw_cons_ptr);\n\tsw_comp_cons = qed_chain_get_cons_idx(&cnq->pbl);\n\n\t \n\trmb();\n\n\twhile (sw_comp_cons != hw_comp_cons) {\n\t\tcq_handle = (struct regpair *)qed_chain_consume(&cnq->pbl);\n\t\tcq = (struct qedr_cq *)(uintptr_t)HILO_U64(cq_handle->hi,\n\t\t\t\tcq_handle->lo);\n\n\t\tif (cq == NULL) {\n\t\t\tDP_ERR(cnq->dev,\n\t\t\t       \"Received NULL CQ cq_handle->hi=%d cq_handle->lo=%d sw_comp_cons=%d hw_comp_cons=%d\\n\",\n\t\t\t       cq_handle->hi, cq_handle->lo, sw_comp_cons,\n\t\t\t       hw_comp_cons);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cq->sig != QEDR_CQ_MAGIC_NUMBER) {\n\t\t\tDP_ERR(cnq->dev,\n\t\t\t       \"Problem with cq signature, cq_handle->hi=%d ch_handle->lo=%d cq=%p\\n\",\n\t\t\t       cq_handle->hi, cq_handle->lo, cq);\n\t\t\tbreak;\n\t\t}\n\n\t\tcq->arm_flags = 0;\n\n\t\tif (!cq->destroyed && cq->ibcq.comp_handler)\n\t\t\t(*cq->ibcq.comp_handler)\n\t\t\t\t(&cq->ibcq, cq->ibcq.cq_context);\n\n\t\t \n\t\tcq->cnq_notif++;\n\n\t\tsw_comp_cons = qed_chain_get_cons_idx(&cnq->pbl);\n\n\t\tcnq->n_comp++;\n\t}\n\n\tqed_ops->rdma_cnq_prod_update(cnq->dev->rdma_ctx, cnq->index,\n\t\t\t\t      sw_comp_cons);\n\n\tqed_sb_ack(cnq->sb, IGU_INT_ENABLE, 1);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void qedr_sync_free_irqs(struct qedr_dev *dev)\n{\n\tu32 vector;\n\tu16 idx;\n\tint i;\n\n\tfor (i = 0; i < dev->int_info.used_cnt; i++) {\n\t\tif (dev->int_info.msix_cnt) {\n\t\t\tidx = i * dev->num_hwfns + dev->affin_hwfn_idx;\n\t\t\tvector = dev->int_info.msix[idx].vector;\n\t\t\tfree_irq(vector, &dev->cnq_array[i]);\n\t\t}\n\t}\n\n\tdev->int_info.used_cnt = 0;\n}\n\nstatic int qedr_req_msix_irqs(struct qedr_dev *dev)\n{\n\tint i, rc = 0;\n\tu16 idx;\n\n\tif (dev->num_cnq > dev->int_info.msix_cnt) {\n\t\tDP_ERR(dev,\n\t\t       \"Interrupt mismatch: %d CNQ queues > %d MSI-x vectors\\n\",\n\t\t       dev->num_cnq, dev->int_info.msix_cnt);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < dev->num_cnq; i++) {\n\t\tidx = i * dev->num_hwfns + dev->affin_hwfn_idx;\n\t\trc = request_irq(dev->int_info.msix[idx].vector,\n\t\t\t\t qedr_irq_handler, 0, dev->cnq_array[i].name,\n\t\t\t\t &dev->cnq_array[i]);\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"Request cnq %d irq failed\\n\", i);\n\t\t\tqedr_sync_free_irqs(dev);\n\t\t} else {\n\t\t\tDP_DEBUG(dev, QEDR_MSG_INIT,\n\t\t\t\t \"Requested cnq irq for %s [entry %d]. Cookie is at %p\\n\",\n\t\t\t\t dev->cnq_array[i].name, i,\n\t\t\t\t &dev->cnq_array[i]);\n\t\t\tdev->int_info.used_cnt++;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic int qedr_setup_irqs(struct qedr_dev *dev)\n{\n\tint rc;\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"qedr_setup_irqs\\n\");\n\n\t \n\trc = dev->ops->rdma_set_rdma_int(dev->cdev, dev->num_cnq);\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = dev->ops->rdma_get_rdma_int(dev->cdev, &dev->int_info);\n\tif (rc) {\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"get_rdma_int failed\\n\");\n\t\treturn rc;\n\t}\n\n\tif (dev->int_info.msix_cnt) {\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"rdma msix_cnt = %d\\n\",\n\t\t\t dev->int_info.msix_cnt);\n\t\trc = qedr_req_msix_irqs(dev);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"qedr_setup_irqs succeeded\\n\");\n\n\treturn 0;\n}\n\nstatic int qedr_set_device_attr(struct qedr_dev *dev)\n{\n\tstruct qed_rdma_device *qed_attr;\n\tstruct qedr_device_attr *attr;\n\tu32 page_size;\n\n\t \n\tqed_attr = dev->ops->rdma_query_device(dev->rdma_ctx);\n\n\t \n\tpage_size = ~qed_attr->page_size_caps + 1;\n\tif (page_size > PAGE_SIZE) {\n\t\tDP_ERR(dev,\n\t\t       \"Kernel PAGE_SIZE is %ld which is smaller than minimum page size (%d) required by qedr\\n\",\n\t\t       PAGE_SIZE, page_size);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tattr = &dev->attr;\n\tattr->vendor_id = qed_attr->vendor_id;\n\tattr->vendor_part_id = qed_attr->vendor_part_id;\n\tattr->hw_ver = qed_attr->hw_ver;\n\tattr->fw_ver = qed_attr->fw_ver;\n\tattr->node_guid = qed_attr->node_guid;\n\tattr->sys_image_guid = qed_attr->sys_image_guid;\n\tattr->max_cnq = qed_attr->max_cnq;\n\tattr->max_sge = qed_attr->max_sge;\n\tattr->max_inline = qed_attr->max_inline;\n\tattr->max_sqe = min_t(u32, qed_attr->max_wqe, QEDR_MAX_SQE);\n\tattr->max_rqe = min_t(u32, qed_attr->max_wqe, QEDR_MAX_RQE);\n\tattr->max_qp_resp_rd_atomic_resc = qed_attr->max_qp_resp_rd_atomic_resc;\n\tattr->max_qp_req_rd_atomic_resc = qed_attr->max_qp_req_rd_atomic_resc;\n\tattr->max_dev_resp_rd_atomic_resc =\n\t    qed_attr->max_dev_resp_rd_atomic_resc;\n\tattr->max_cq = qed_attr->max_cq;\n\tattr->max_qp = qed_attr->max_qp;\n\tattr->max_mr = qed_attr->max_mr;\n\tattr->max_mr_size = qed_attr->max_mr_size;\n\tattr->max_cqe = min_t(u64, qed_attr->max_cqe, QEDR_MAX_CQES);\n\tattr->max_mw = qed_attr->max_mw;\n\tattr->max_mr_mw_fmr_pbl = qed_attr->max_mr_mw_fmr_pbl;\n\tattr->max_mr_mw_fmr_size = qed_attr->max_mr_mw_fmr_size;\n\tattr->max_pd = qed_attr->max_pd;\n\tattr->max_ah = qed_attr->max_ah;\n\tattr->max_pkey = qed_attr->max_pkey;\n\tattr->max_srq = qed_attr->max_srq;\n\tattr->max_srq_wr = qed_attr->max_srq_wr;\n\tattr->dev_caps = qed_attr->dev_caps;\n\tattr->page_size_caps = qed_attr->page_size_caps;\n\tattr->dev_ack_delay = qed_attr->dev_ack_delay;\n\tattr->reserved_lkey = qed_attr->reserved_lkey;\n\tattr->bad_pkey_counter = qed_attr->bad_pkey_counter;\n\tattr->max_stats_queues = qed_attr->max_stats_queues;\n\n\treturn 0;\n}\n\nstatic void qedr_unaffiliated_event(void *context, u8 event_code)\n{\n\tpr_err(\"unaffiliated event not implemented yet\\n\");\n}\n\nstatic void qedr_affiliated_event(void *context, u8 e_code, void *fw_handle)\n{\n#define EVENT_TYPE_NOT_DEFINED\t0\n#define EVENT_TYPE_CQ\t\t1\n#define EVENT_TYPE_QP\t\t2\n#define EVENT_TYPE_SRQ\t\t3\n\tstruct qedr_dev *dev = (struct qedr_dev *)context;\n\tstruct regpair *async_handle = (struct regpair *)fw_handle;\n\tu64 roce_handle64 = ((u64) async_handle->hi << 32) + async_handle->lo;\n\tu8 event_type = EVENT_TYPE_NOT_DEFINED;\n\tstruct ib_event event;\n\tstruct ib_srq *ibsrq;\n\tstruct qedr_srq *srq;\n\tunsigned long flags;\n\tstruct ib_cq *ibcq;\n\tstruct ib_qp *ibqp;\n\tstruct qedr_cq *cq;\n\tstruct qedr_qp *qp;\n\tu16 srq_id;\n\n\tif (IS_ROCE(dev)) {\n\t\tswitch (e_code) {\n\t\tcase ROCE_ASYNC_EVENT_CQ_OVERFLOW_ERR:\n\t\t\tevent.event = IB_EVENT_CQ_ERR;\n\t\t\tevent_type = EVENT_TYPE_CQ;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_SQ_DRAINED:\n\t\t\tevent.event = IB_EVENT_SQ_DRAINED;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_QP_CATASTROPHIC_ERR:\n\t\t\tevent.event = IB_EVENT_QP_FATAL;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_LOCAL_INVALID_REQUEST_ERR:\n\t\t\tevent.event = IB_EVENT_QP_REQ_ERR;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_LOCAL_ACCESS_ERR:\n\t\t\tevent.event = IB_EVENT_QP_ACCESS_ERR;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_SRQ_LIMIT:\n\t\t\tevent.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\t\t\tevent_type = EVENT_TYPE_SRQ;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_SRQ_EMPTY:\n\t\t\tevent.event = IB_EVENT_SRQ_ERR;\n\t\t\tevent_type = EVENT_TYPE_SRQ;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_XRC_DOMAIN_ERR:\n\t\t\tevent.event = IB_EVENT_QP_ACCESS_ERR;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_INVALID_XRCETH_ERR:\n\t\t\tevent.event = IB_EVENT_QP_ACCESS_ERR;\n\t\t\tevent_type = EVENT_TYPE_QP;\n\t\t\tbreak;\n\t\tcase ROCE_ASYNC_EVENT_XRC_SRQ_CATASTROPHIC_ERR:\n\t\t\tevent.event = IB_EVENT_CQ_ERR;\n\t\t\tevent_type = EVENT_TYPE_CQ;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDP_ERR(dev, \"unsupported event %d on handle=%llx\\n\",\n\t\t\t       e_code, roce_handle64);\n\t\t}\n\t} else {\n\t\tswitch (e_code) {\n\t\tcase QED_IWARP_EVENT_SRQ_LIMIT:\n\t\t\tevent.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\t\t\tevent_type = EVENT_TYPE_SRQ;\n\t\t\tbreak;\n\t\tcase QED_IWARP_EVENT_SRQ_EMPTY:\n\t\t\tevent.event = IB_EVENT_SRQ_ERR;\n\t\t\tevent_type = EVENT_TYPE_SRQ;\n\t\t\tbreak;\n\t\tdefault:\n\t\tDP_ERR(dev, \"unsupported event %d on handle=%llx\\n\", e_code,\n\t\t       roce_handle64);\n\t\t}\n\t}\n\tswitch (event_type) {\n\tcase EVENT_TYPE_CQ:\n\t\tcq = (struct qedr_cq *)(uintptr_t)roce_handle64;\n\t\tif (cq) {\n\t\t\tibcq = &cq->ibcq;\n\t\t\tif (ibcq->event_handler) {\n\t\t\t\tevent.device = ibcq->device;\n\t\t\t\tevent.element.cq = ibcq;\n\t\t\t\tibcq->event_handler(&event, ibcq->cq_context);\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(1,\n\t\t\t     \"Error: CQ event with NULL pointer ibcq. Handle=%llx\\n\",\n\t\t\t     roce_handle64);\n\t\t}\n\t\tDP_ERR(dev, \"CQ event %d on handle %p\\n\", e_code, cq);\n\t\tbreak;\n\tcase EVENT_TYPE_QP:\n\t\tqp = (struct qedr_qp *)(uintptr_t)roce_handle64;\n\t\tif (qp) {\n\t\t\tibqp = &qp->ibqp;\n\t\t\tif (ibqp->event_handler) {\n\t\t\t\tevent.device = ibqp->device;\n\t\t\t\tevent.element.qp = ibqp;\n\t\t\t\tibqp->event_handler(&event, ibqp->qp_context);\n\t\t\t}\n\t\t} else {\n\t\t\tWARN(1,\n\t\t\t     \"Error: QP event with NULL pointer ibqp. Handle=%llx\\n\",\n\t\t\t     roce_handle64);\n\t\t}\n\t\tDP_ERR(dev, \"QP event %d on handle %p\\n\", e_code, qp);\n\t\tbreak;\n\tcase EVENT_TYPE_SRQ:\n\t\tsrq_id = (u16)roce_handle64;\n\t\txa_lock_irqsave(&dev->srqs, flags);\n\t\tsrq = xa_load(&dev->srqs, srq_id);\n\t\tif (srq) {\n\t\t\tibsrq = &srq->ibsrq;\n\t\t\tif (ibsrq->event_handler) {\n\t\t\t\tevent.device = ibsrq->device;\n\t\t\t\tevent.element.srq = ibsrq;\n\t\t\t\tibsrq->event_handler(&event,\n\t\t\t\t\t\t     ibsrq->srq_context);\n\t\t\t}\n\t\t} else {\n\t\t\tDP_NOTICE(dev,\n\t\t\t\t  \"SRQ event with NULL pointer ibsrq. Handle=%llx\\n\",\n\t\t\t\t  roce_handle64);\n\t\t}\n\t\txa_unlock_irqrestore(&dev->srqs, flags);\n\t\tDP_NOTICE(dev, \"SRQ event %d on handle %p\\n\", e_code, srq);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int qedr_init_hw(struct qedr_dev *dev)\n{\n\tstruct qed_rdma_add_user_out_params out_params;\n\tstruct qed_rdma_start_in_params *in_params;\n\tstruct qed_rdma_cnq_params *cur_pbl;\n\tstruct qed_rdma_events events;\n\tdma_addr_t p_phys_table;\n\tu32 page_cnt;\n\tint rc = 0;\n\tint i;\n\n\tin_params =  kzalloc(sizeof(*in_params), GFP_KERNEL);\n\tif (!in_params) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tin_params->desired_cnq = dev->num_cnq;\n\tfor (i = 0; i < dev->num_cnq; i++) {\n\t\tcur_pbl = &in_params->cnq_pbl_list[i];\n\n\t\tpage_cnt = qed_chain_get_page_cnt(&dev->cnq_array[i].pbl);\n\t\tcur_pbl->num_pbl_pages = page_cnt;\n\n\t\tp_phys_table = qed_chain_get_pbl_phys(&dev->cnq_array[i].pbl);\n\t\tcur_pbl->pbl_ptr = (u64)p_phys_table;\n\t}\n\n\tevents.affiliated_event = qedr_affiliated_event;\n\tevents.unaffiliated_event = qedr_unaffiliated_event;\n\tevents.context = dev;\n\n\tin_params->events = &events;\n\tin_params->cq_mode = QED_RDMA_CQ_MODE_32_BITS;\n\tin_params->max_mtu = dev->ndev->mtu;\n\tdev->iwarp_max_mtu = dev->ndev->mtu;\n\tether_addr_copy(&in_params->mac_addr[0], dev->ndev->dev_addr);\n\n\trc = dev->ops->rdma_init(dev->cdev, in_params);\n\tif (rc)\n\t\tgoto out;\n\n\trc = dev->ops->rdma_add_user(dev->rdma_ctx, &out_params);\n\tif (rc)\n\t\tgoto out;\n\n\tdev->db_addr = out_params.dpi_addr;\n\tdev->db_phys_addr = out_params.dpi_phys_addr;\n\tdev->db_size = out_params.dpi_size;\n\tdev->dpi = out_params.dpi;\n\n\trc = qedr_set_device_attr(dev);\nout:\n\tkfree(in_params);\n\tif (rc)\n\t\tDP_ERR(dev, \"Init HW Failed rc = %d\\n\", rc);\n\n\treturn rc;\n}\n\nstatic void qedr_stop_hw(struct qedr_dev *dev)\n{\n\tdev->ops->rdma_remove_user(dev->rdma_ctx, dev->dpi);\n\tdev->ops->rdma_stop(dev->rdma_ctx);\n}\n\nstatic struct qedr_dev *qedr_add(struct qed_dev *cdev, struct pci_dev *pdev,\n\t\t\t\t struct net_device *ndev)\n{\n\tstruct qed_dev_rdma_info dev_info;\n\tstruct qedr_dev *dev;\n\tint rc = 0;\n\n\tdev = ib_alloc_device(qedr_dev, ibdev);\n\tif (!dev) {\n\t\tpr_err(\"Unable to allocate ib device\\n\");\n\t\treturn NULL;\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"qedr add device called\\n\");\n\n\tdev->pdev = pdev;\n\tdev->ndev = ndev;\n\tdev->cdev = cdev;\n\n\tqed_ops = qed_get_rdma_ops();\n\tif (!qed_ops) {\n\t\tDP_ERR(dev, \"Failed to get qed roce operations\\n\");\n\t\tgoto init_err;\n\t}\n\n\tdev->ops = qed_ops;\n\trc = qed_ops->fill_dev_info(cdev, &dev_info);\n\tif (rc)\n\t\tgoto init_err;\n\n\tdev->user_dpm_enabled = dev_info.user_dpm_enabled;\n\tdev->rdma_type = dev_info.rdma_type;\n\tdev->num_hwfns = dev_info.common.num_hwfns;\n\n\tif (IS_IWARP(dev) && QEDR_IS_CMT(dev)) {\n\t\trc = dev->ops->iwarp_set_engine_affin(cdev, false);\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"iWARP is disabled over a 100g device Enabling it may impact L2 performance. To enable it run devlink dev param set <dev> name iwarp_cmt value true cmode runtime\\n\");\n\t\t\tgoto init_err;\n\t\t}\n\t}\n\tdev->affin_hwfn_idx = dev->ops->common->get_affin_hwfn_idx(cdev);\n\n\tdev->rdma_ctx = dev->ops->rdma_get_rdma_ctx(cdev);\n\n\tdev->num_cnq = dev->ops->rdma_get_min_cnq_msix(cdev);\n\tif (!dev->num_cnq) {\n\t\tDP_ERR(dev, \"Failed. At least one CNQ is required.\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto init_err;\n\t}\n\n\tdev->wq_multiplier = QEDR_WQ_MULTIPLIER_DFT;\n\n\tqedr_pci_set_atomic(dev, pdev);\n\n\trc = qedr_alloc_resources(dev);\n\tif (rc)\n\t\tgoto init_err;\n\n\trc = qedr_init_hw(dev);\n\tif (rc)\n\t\tgoto alloc_err;\n\n\trc = qedr_setup_irqs(dev);\n\tif (rc)\n\t\tgoto irq_err;\n\n\trc = qedr_register_device(dev);\n\tif (rc) {\n\t\tDP_ERR(dev, \"Unable to allocate register device\\n\");\n\t\tgoto reg_err;\n\t}\n\n\tif (!test_and_set_bit(QEDR_ENET_STATE_BIT, &dev->enet_state))\n\t\tqedr_ib_dispatch_event(dev, QEDR_PORT, IB_EVENT_PORT_ACTIVE);\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"qedr driver loaded successfully\\n\");\n\treturn dev;\n\nreg_err:\n\tqedr_sync_free_irqs(dev);\nirq_err:\n\tqedr_stop_hw(dev);\nalloc_err:\n\tqedr_free_resources(dev);\ninit_err:\n\tib_dealloc_device(&dev->ibdev);\n\tDP_ERR(dev, \"qedr driver load failed rc=%d\\n\", rc);\n\n\treturn NULL;\n}\n\nstatic void qedr_remove(struct qedr_dev *dev)\n{\n\t \n\tib_unregister_device(&dev->ibdev);\n\n\tqedr_stop_hw(dev);\n\tqedr_sync_free_irqs(dev);\n\tqedr_free_resources(dev);\n\n\tif (IS_IWARP(dev) && QEDR_IS_CMT(dev))\n\t\tdev->ops->iwarp_set_engine_affin(dev->cdev, true);\n\n\tib_dealloc_device(&dev->ibdev);\n}\n\nstatic void qedr_close(struct qedr_dev *dev)\n{\n\tif (test_and_clear_bit(QEDR_ENET_STATE_BIT, &dev->enet_state))\n\t\tqedr_ib_dispatch_event(dev, QEDR_PORT, IB_EVENT_PORT_ERR);\n}\n\nstatic void qedr_shutdown(struct qedr_dev *dev)\n{\n\tqedr_close(dev);\n\tqedr_remove(dev);\n}\n\nstatic void qedr_open(struct qedr_dev *dev)\n{\n\tif (!test_and_set_bit(QEDR_ENET_STATE_BIT, &dev->enet_state))\n\t\tqedr_ib_dispatch_event(dev, QEDR_PORT, IB_EVENT_PORT_ACTIVE);\n}\n\nstatic void qedr_mac_address_change(struct qedr_dev *dev)\n{\n\tunion ib_gid *sgid = &dev->sgid_tbl[0];\n\tu8 guid[8], mac_addr[6];\n\tint rc;\n\n\t \n\tether_addr_copy(&mac_addr[0], dev->ndev->dev_addr);\n\tguid[0] = mac_addr[0] ^ 2;\n\tguid[1] = mac_addr[1];\n\tguid[2] = mac_addr[2];\n\tguid[3] = 0xff;\n\tguid[4] = 0xfe;\n\tguid[5] = mac_addr[3];\n\tguid[6] = mac_addr[4];\n\tguid[7] = mac_addr[5];\n\tsgid->global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL);\n\tmemcpy(&sgid->raw[8], guid, sizeof(guid));\n\n\t \n\trc = dev->ops->ll2_set_mac_filter(dev->cdev,\n\t\t\t\t\t  dev->gsi_ll2_mac_address,\n\t\t\t\t\t  dev->ndev->dev_addr);\n\n\tether_addr_copy(dev->gsi_ll2_mac_address, dev->ndev->dev_addr);\n\n\tqedr_ib_dispatch_event(dev, QEDR_PORT, IB_EVENT_GID_CHANGE);\n\n\tif (rc)\n\t\tDP_ERR(dev, \"Error updating mac filter\\n\");\n}\n\n \nstatic void qedr_notify(struct qedr_dev *dev, enum qede_rdma_event event)\n{\n\tswitch (event) {\n\tcase QEDE_UP:\n\t\tqedr_open(dev);\n\t\tbreak;\n\tcase QEDE_DOWN:\n\t\tqedr_close(dev);\n\t\tbreak;\n\tcase QEDE_CLOSE:\n\t\tqedr_shutdown(dev);\n\t\tbreak;\n\tcase QEDE_CHANGE_ADDR:\n\t\tqedr_mac_address_change(dev);\n\t\tbreak;\n\tcase QEDE_CHANGE_MTU:\n\t\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\t\tif (dev->ndev->mtu != dev->iwarp_max_mtu)\n\t\t\t\tDP_NOTICE(dev,\n\t\t\t\t\t  \"Mtu was changed from %d to %d. This will not take affect for iWARP until qedr is reloaded\\n\",\n\t\t\t\t\t  dev->iwarp_max_mtu, dev->ndev->mtu);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Event not supported\\n\");\n\t}\n}\n\nstatic struct qedr_driver qedr_drv = {\n\t.name = \"qedr_driver\",\n\t.add = qedr_add,\n\t.remove = qedr_remove,\n\t.notify = qedr_notify,\n};\n\nstatic int __init qedr_init_module(void)\n{\n\treturn qede_rdma_register_driver(&qedr_drv);\n}\n\nstatic void __exit qedr_exit_module(void)\n{\n\tqede_rdma_unregister_driver(&qedr_drv);\n}\n\nmodule_init(qedr_init_module);\nmodule_exit(qedr_exit_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}