{
  "module_name": "verbs.c",
  "hash_id": "f6e4b2fbd4bb45883a6fbc3ac579241546dd47c92687c59e2fe6645a841feea8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/qedr/verbs.c",
  "human_readable_source": " \n#include <linux/dma-mapping.h>\n#include <linux/crc32.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/udp.h>\n#include <linux/iommu.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/iw_cm.h>\n#include <rdma/ib_umem.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include <linux/qed/common_hsi.h>\n#include \"qedr_hsi_rdma.h\"\n#include <linux/qed/qed_if.h>\n#include \"qedr.h\"\n#include \"verbs.h\"\n#include <rdma/qedr-abi.h>\n#include \"qedr_roce_cm.h\"\n#include \"qedr_iw_cm.h\"\n\n#define QEDR_SRQ_WQE_ELEM_SIZE\tsizeof(union rdma_srq_elm)\n#define\tRDMA_MAX_SGE_PER_SRQ\t(4)\n#define RDMA_MAX_SRQ_WQE_SIZE\t(RDMA_MAX_SGE_PER_SRQ + 1)\n\n#define DB_ADDR_SHIFT(addr)\t\t((addr) << DB_PWM_ADDR_OFFSET_SHIFT)\n\nenum {\n\tQEDR_USER_MMAP_IO_WC = 0,\n\tQEDR_USER_MMAP_PHYS_PAGE,\n};\n\nstatic inline int qedr_ib_copy_to_udata(struct ib_udata *udata, void *src,\n\t\t\t\t\tsize_t len)\n{\n\tsize_t min_len = min_t(size_t, len, udata->outlen);\n\n\treturn ib_copy_to_udata(udata, src, min_len);\n}\n\nint qedr_query_pkey(struct ib_device *ibdev, u32 port, u16 index, u16 *pkey)\n{\n\tif (index >= QEDR_ROCE_PKEY_TABLE_LEN)\n\t\treturn -EINVAL;\n\n\t*pkey = QEDR_ROCE_PKEY_DEFAULT;\n\treturn 0;\n}\n\nint qedr_iw_query_gid(struct ib_device *ibdev, u32 port,\n\t\t      int index, union ib_gid *sgid)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibdev);\n\n\tmemset(sgid->raw, 0, sizeof(sgid->raw));\n\tether_addr_copy(sgid->raw, dev->ndev->dev_addr);\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"QUERY sgid[%d]=%llx:%llx\\n\", index,\n\t\t sgid->global.interface_id, sgid->global.subnet_prefix);\n\n\treturn 0;\n}\n\nint qedr_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibsrq->device);\n\tstruct qedr_device_attr *qattr = &dev->attr;\n\tstruct qedr_srq *srq = get_qedr_srq(ibsrq);\n\n\tsrq_attr->srq_limit = srq->srq_limit;\n\tsrq_attr->max_wr = qattr->max_srq_wr;\n\tsrq_attr->max_sge = qattr->max_sge;\n\n\treturn 0;\n}\n\nint qedr_query_device(struct ib_device *ibdev,\n\t\t      struct ib_device_attr *attr, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibdev);\n\tstruct qedr_device_attr *qattr = &dev->attr;\n\n\tif (!dev->rdma_ctx) {\n\t\tDP_ERR(dev,\n\t\t       \"qedr_query_device called with invalid params rdma_ctx=%p\\n\",\n\t\t       dev->rdma_ctx);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(attr, 0, sizeof(*attr));\n\n\tattr->fw_ver = qattr->fw_ver;\n\tattr->sys_image_guid = qattr->sys_image_guid;\n\tattr->max_mr_size = qattr->max_mr_size;\n\tattr->page_size_cap = qattr->page_size_caps;\n\tattr->vendor_id = qattr->vendor_id;\n\tattr->vendor_part_id = qattr->vendor_part_id;\n\tattr->hw_ver = qattr->hw_ver;\n\tattr->max_qp = qattr->max_qp;\n\tattr->max_qp_wr = max_t(u32, qattr->max_sqe, qattr->max_rqe);\n\tattr->device_cap_flags = IB_DEVICE_CURR_QP_STATE_MOD |\n\t    IB_DEVICE_RC_RNR_NAK_GEN |\n\t    IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tattr->kernel_cap_flags = IBK_LOCAL_DMA_LKEY;\n\n\tif (!rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\tattr->device_cap_flags |= IB_DEVICE_XRC;\n\tattr->max_send_sge = qattr->max_sge;\n\tattr->max_recv_sge = qattr->max_sge;\n\tattr->max_sge_rd = qattr->max_sge;\n\tattr->max_cq = qattr->max_cq;\n\tattr->max_cqe = qattr->max_cqe;\n\tattr->max_mr = qattr->max_mr;\n\tattr->max_mw = qattr->max_mw;\n\tattr->max_pd = qattr->max_pd;\n\tattr->atomic_cap = dev->atomic_cap;\n\tattr->max_qp_init_rd_atom =\n\t    1 << (fls(qattr->max_qp_req_rd_atomic_resc) - 1);\n\tattr->max_qp_rd_atom =\n\t    min(1 << (fls(qattr->max_qp_resp_rd_atomic_resc) - 1),\n\t\tattr->max_qp_init_rd_atom);\n\n\tattr->max_srq = qattr->max_srq;\n\tattr->max_srq_sge = qattr->max_srq_sge;\n\tattr->max_srq_wr = qattr->max_srq_wr;\n\n\tattr->local_ca_ack_delay = qattr->dev_ack_delay;\n\tattr->max_fast_reg_page_list_len = qattr->max_mr / 8;\n\tattr->max_pkeys = qattr->max_pkey;\n\tattr->max_ah = qattr->max_ah;\n\n\treturn 0;\n}\n\nstatic inline void get_link_speed_and_width(int speed, u16 *ib_speed,\n\t\t\t\t\t    u8 *ib_width)\n{\n\tswitch (speed) {\n\tcase 1000:\n\t\t*ib_speed = IB_SPEED_SDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\tcase 10000:\n\t\t*ib_speed = IB_SPEED_QDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\n\tcase 20000:\n\t\t*ib_speed = IB_SPEED_DDR;\n\t\t*ib_width = IB_WIDTH_4X;\n\t\tbreak;\n\n\tcase 25000:\n\t\t*ib_speed = IB_SPEED_EDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\n\tcase 40000:\n\t\t*ib_speed = IB_SPEED_QDR;\n\t\t*ib_width = IB_WIDTH_4X;\n\t\tbreak;\n\n\tcase 50000:\n\t\t*ib_speed = IB_SPEED_HDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t\tbreak;\n\n\tcase 100000:\n\t\t*ib_speed = IB_SPEED_EDR;\n\t\t*ib_width = IB_WIDTH_4X;\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\t*ib_speed = IB_SPEED_SDR;\n\t\t*ib_width = IB_WIDTH_1X;\n\t}\n}\n\nint qedr_query_port(struct ib_device *ibdev, u32 port,\n\t\t    struct ib_port_attr *attr)\n{\n\tstruct qedr_dev *dev;\n\tstruct qed_rdma_port *rdma_port;\n\n\tdev = get_qedr_dev(ibdev);\n\n\tif (!dev->rdma_ctx) {\n\t\tDP_ERR(dev, \"rdma_ctx is NULL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trdma_port = dev->ops->rdma_query_port(dev->rdma_ctx);\n\n\t \n\tif (rdma_port->port_state == QED_RDMA_PORT_UP) {\n\t\tattr->state = IB_PORT_ACTIVE;\n\t\tattr->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t} else {\n\t\tattr->state = IB_PORT_DOWN;\n\t\tattr->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\t}\n\tattr->max_mtu = IB_MTU_4096;\n\tattr->lid = 0;\n\tattr->lmc = 0;\n\tattr->sm_lid = 0;\n\tattr->sm_sl = 0;\n\tattr->ip_gids = true;\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\tattr->active_mtu = iboe_get_mtu(dev->iwarp_max_mtu);\n\t\tattr->gid_tbl_len = 1;\n\t} else {\n\t\tattr->active_mtu = iboe_get_mtu(dev->ndev->mtu);\n\t\tattr->gid_tbl_len = QEDR_MAX_SGID;\n\t\tattr->pkey_tbl_len = QEDR_ROCE_PKEY_TABLE_LEN;\n\t}\n\tattr->bad_pkey_cntr = rdma_port->pkey_bad_counter;\n\tattr->qkey_viol_cntr = 0;\n\tget_link_speed_and_width(rdma_port->link_speed,\n\t\t\t\t &attr->active_speed, &attr->active_width);\n\tattr->max_msg_sz = rdma_port->max_msg_size;\n\tattr->max_vl_num = 4;\n\n\treturn 0;\n}\n\nint qedr_alloc_ucontext(struct ib_ucontext *uctx, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = uctx->device;\n\tint rc;\n\tstruct qedr_ucontext *ctx = get_qedr_ucontext(uctx);\n\tstruct qedr_alloc_ucontext_resp uresp = {};\n\tstruct qedr_alloc_ucontext_req ureq = {};\n\tstruct qedr_dev *dev = get_qedr_dev(ibdev);\n\tstruct qed_rdma_add_user_out_params oparams;\n\tstruct qedr_user_mmap_entry *entry;\n\n\tif (!udata)\n\t\treturn -EFAULT;\n\n\tif (udata->inlen) {\n\t\trc = ib_copy_from_udata(&ureq, udata,\n\t\t\t\t\tmin(sizeof(ureq), udata->inlen));\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"Problem copying data from user space\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tctx->edpm_mode = !!(ureq.context_flags &\n\t\t\t\t    QEDR_ALLOC_UCTX_EDPM_MODE);\n\t\tctx->db_rec = !!(ureq.context_flags & QEDR_ALLOC_UCTX_DB_REC);\n\t}\n\n\trc = dev->ops->rdma_add_user(dev->rdma_ctx, &oparams);\n\tif (rc) {\n\t\tDP_ERR(dev,\n\t\t       \"failed to allocate a DPI for a new RoCE application, rc=%d. To overcome this consider to increase the number of DPIs, increase the doorbell BAR size or just close unnecessary RoCE applications. In order to increase the number of DPIs consult the qedr readme\\n\",\n\t\t       rc);\n\t\treturn rc;\n\t}\n\n\tctx->dpi = oparams.dpi;\n\tctx->dpi_addr = oparams.dpi_addr;\n\tctx->dpi_phys_addr = oparams.dpi_phys_addr;\n\tctx->dpi_size = oparams.dpi_size;\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry) {\n\t\trc = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tentry->io_address = ctx->dpi_phys_addr;\n\tentry->length = ctx->dpi_size;\n\tentry->mmap_flag = QEDR_USER_MMAP_IO_WC;\n\tentry->dpi = ctx->dpi;\n\tentry->dev = dev;\n\trc = rdma_user_mmap_entry_insert(uctx, &entry->rdma_entry,\n\t\t\t\t\t ctx->dpi_size);\n\tif (rc) {\n\t\tkfree(entry);\n\t\tgoto err;\n\t}\n\tctx->db_mmap_entry = &entry->rdma_entry;\n\n\tif (!dev->user_dpm_enabled)\n\t\turesp.dpm_flags = 0;\n\telse if (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\turesp.dpm_flags = QEDR_DPM_TYPE_IWARP_LEGACY;\n\telse\n\t\turesp.dpm_flags = QEDR_DPM_TYPE_ROCE_ENHANCED |\n\t\t\t\t  QEDR_DPM_TYPE_ROCE_LEGACY |\n\t\t\t\t  QEDR_DPM_TYPE_ROCE_EDPM_MODE;\n\n\tif (ureq.context_flags & QEDR_SUPPORT_DPM_SIZES) {\n\t\turesp.dpm_flags |= QEDR_DPM_SIZES_SET;\n\t\turesp.ldpm_limit_size = QEDR_LDPM_MAX_SIZE;\n\t\turesp.edpm_trans_size = QEDR_EDPM_TRANS_SIZE;\n\t\turesp.edpm_limit_size = QEDR_EDPM_MAX_SIZE;\n\t}\n\n\turesp.wids_enabled = 1;\n\turesp.wid_count = oparams.wid_count;\n\turesp.db_pa = rdma_user_mmap_get_offset(ctx->db_mmap_entry);\n\turesp.db_size = ctx->dpi_size;\n\turesp.max_send_wr = dev->attr.max_sqe;\n\turesp.max_recv_wr = dev->attr.max_rqe;\n\turesp.max_srq_wr = dev->attr.max_srq_wr;\n\turesp.sges_per_send_wr = QEDR_MAX_SQE_ELEMENTS_PER_SQE;\n\turesp.sges_per_recv_wr = QEDR_MAX_RQE_ELEMENTS_PER_RQE;\n\turesp.sges_per_srq_wr = dev->attr.max_srq_sge;\n\turesp.max_cqes = QEDR_MAX_CQES;\n\n\trc = qedr_ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (rc)\n\t\tgoto err;\n\n\tctx->dev = dev;\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"Allocating user context %p\\n\",\n\t\t &ctx->ibucontext);\n\treturn 0;\n\nerr:\n\tif (!ctx->db_mmap_entry)\n\t\tdev->ops->rdma_remove_user(dev->rdma_ctx, ctx->dpi);\n\telse\n\t\trdma_user_mmap_entry_remove(ctx->db_mmap_entry);\n\n\treturn rc;\n}\n\nvoid qedr_dealloc_ucontext(struct ib_ucontext *ibctx)\n{\n\tstruct qedr_ucontext *uctx = get_qedr_ucontext(ibctx);\n\n\tDP_DEBUG(uctx->dev, QEDR_MSG_INIT, \"Deallocating user context %p\\n\",\n\t\t uctx);\n\n\trdma_user_mmap_entry_remove(uctx->db_mmap_entry);\n}\n\nvoid qedr_mmap_free(struct rdma_user_mmap_entry *rdma_entry)\n{\n\tstruct qedr_user_mmap_entry *entry = get_qedr_mmap_entry(rdma_entry);\n\tstruct qedr_dev *dev = entry->dev;\n\n\tif (entry->mmap_flag == QEDR_USER_MMAP_PHYS_PAGE)\n\t\tfree_page((unsigned long)entry->address);\n\telse if (entry->mmap_flag == QEDR_USER_MMAP_IO_WC)\n\t\tdev->ops->rdma_remove_user(dev->rdma_ctx, entry->dpi);\n\n\tkfree(entry);\n}\n\nint qedr_mmap(struct ib_ucontext *ucontext, struct vm_area_struct *vma)\n{\n\tstruct ib_device *dev = ucontext->device;\n\tsize_t length = vma->vm_end - vma->vm_start;\n\tstruct rdma_user_mmap_entry *rdma_entry;\n\tstruct qedr_user_mmap_entry *entry;\n\tint rc = 0;\n\tu64 pfn;\n\n\tibdev_dbg(dev,\n\t\t  \"start %#lx, end %#lx, length = %#zx, pgoff = %#lx\\n\",\n\t\t  vma->vm_start, vma->vm_end, length, vma->vm_pgoff);\n\n\trdma_entry = rdma_user_mmap_entry_get(ucontext, vma);\n\tif (!rdma_entry) {\n\t\tibdev_dbg(dev, \"pgoff[%#lx] does not have valid entry\\n\",\n\t\t\t  vma->vm_pgoff);\n\t\treturn -EINVAL;\n\t}\n\tentry = get_qedr_mmap_entry(rdma_entry);\n\tibdev_dbg(dev,\n\t\t  \"Mapping address[%#llx], length[%#zx], mmap_flag[%d]\\n\",\n\t\t  entry->io_address, length, entry->mmap_flag);\n\n\tswitch (entry->mmap_flag) {\n\tcase QEDR_USER_MMAP_IO_WC:\n\t\tpfn = entry->io_address >> PAGE_SHIFT;\n\t\trc = rdma_user_mmap_io(ucontext, vma, pfn, length,\n\t\t\t\t       pgprot_writecombine(vma->vm_page_prot),\n\t\t\t\t       rdma_entry);\n\t\tbreak;\n\tcase QEDR_USER_MMAP_PHYS_PAGE:\n\t\trc = vm_insert_page(vma, vma->vm_start,\n\t\t\t\t    virt_to_page(entry->address));\n\t\tbreak;\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\n\tif (rc)\n\t\tibdev_dbg(dev,\n\t\t\t  \"Couldn't mmap address[%#llx] length[%#zx] mmap_flag[%d] err[%d]\\n\",\n\t\t\t  entry->io_address, length, entry->mmap_flag, rc);\n\n\trdma_user_mmap_entry_put(rdma_entry);\n\treturn rc;\n}\n\nint qedr_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibpd->device;\n\tstruct qedr_dev *dev = get_qedr_dev(ibdev);\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\tu16 pd_id;\n\tint rc;\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"Function called from: %s\\n\",\n\t\t udata ? \"User Lib\" : \"Kernel\");\n\n\tif (!dev->rdma_ctx) {\n\t\tDP_ERR(dev, \"invalid RDMA context\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trc = dev->ops->rdma_alloc_pd(dev->rdma_ctx, &pd_id);\n\tif (rc)\n\t\treturn rc;\n\n\tpd->pd_id = pd_id;\n\n\tif (udata) {\n\t\tstruct qedr_alloc_pd_uresp uresp = {\n\t\t\t.pd_id = pd_id,\n\t\t};\n\t\tstruct qedr_ucontext *context = rdma_udata_to_drv_context(\n\t\t\tudata, struct qedr_ucontext, ibucontext);\n\n\t\trc = qedr_ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"copy error pd_id=0x%x.\\n\", pd_id);\n\t\t\tdev->ops->rdma_dealloc_pd(dev->rdma_ctx, pd_id);\n\t\t\treturn rc;\n\t\t}\n\n\t\tpd->uctx = context;\n\t\tpd->uctx->pd = pd;\n\t}\n\n\treturn 0;\n}\n\nint qedr_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibpd->device);\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT, \"Deallocating PD %d\\n\", pd->pd_id);\n\tdev->ops->rdma_dealloc_pd(dev->rdma_ctx, pd->pd_id);\n\treturn 0;\n}\n\n\nint qedr_alloc_xrcd(struct ib_xrcd *ibxrcd, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibxrcd->device);\n\tstruct qedr_xrcd *xrcd = get_qedr_xrcd(ibxrcd);\n\n\treturn dev->ops->rdma_alloc_xrcd(dev->rdma_ctx, &xrcd->xrcd_id);\n}\n\nint qedr_dealloc_xrcd(struct ib_xrcd *ibxrcd, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibxrcd->device);\n\tu16 xrcd_id = get_qedr_xrcd(ibxrcd)->xrcd_id;\n\n\tdev->ops->rdma_dealloc_xrcd(dev->rdma_ctx, xrcd_id);\n\treturn 0;\n}\nstatic void qedr_free_pbl(struct qedr_dev *dev,\n\t\t\t  struct qedr_pbl_info *pbl_info, struct qedr_pbl *pbl)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\tint i;\n\n\tfor (i = 0; i < pbl_info->num_pbls; i++) {\n\t\tif (!pbl[i].va)\n\t\t\tcontinue;\n\t\tdma_free_coherent(&pdev->dev, pbl_info->pbl_size,\n\t\t\t\t  pbl[i].va, pbl[i].pa);\n\t}\n\n\tkfree(pbl);\n}\n\n#define MIN_FW_PBL_PAGE_SIZE (4 * 1024)\n#define MAX_FW_PBL_PAGE_SIZE (64 * 1024)\n\n#define NUM_PBES_ON_PAGE(_page_size) (_page_size / sizeof(u64))\n#define MAX_PBES_ON_PAGE NUM_PBES_ON_PAGE(MAX_FW_PBL_PAGE_SIZE)\n#define MAX_PBES_TWO_LAYER (MAX_PBES_ON_PAGE * MAX_PBES_ON_PAGE)\n\nstatic struct qedr_pbl *qedr_alloc_pbl_tbl(struct qedr_dev *dev,\n\t\t\t\t\t   struct qedr_pbl_info *pbl_info,\n\t\t\t\t\t   gfp_t flags)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\tstruct qedr_pbl *pbl_table;\n\tdma_addr_t *pbl_main_tbl;\n\tdma_addr_t pa;\n\tvoid *va;\n\tint i;\n\n\tpbl_table = kcalloc(pbl_info->num_pbls, sizeof(*pbl_table), flags);\n\tif (!pbl_table)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = 0; i < pbl_info->num_pbls; i++) {\n\t\tva = dma_alloc_coherent(&pdev->dev, pbl_info->pbl_size, &pa,\n\t\t\t\t\tflags);\n\t\tif (!va)\n\t\t\tgoto err;\n\n\t\tpbl_table[i].va = va;\n\t\tpbl_table[i].pa = pa;\n\t}\n\n\t \n\tpbl_main_tbl = (dma_addr_t *)pbl_table[0].va;\n\tfor (i = 0; i < pbl_info->num_pbls - 1; i++)\n\t\tpbl_main_tbl[i] = pbl_table[i + 1].pa;\n\n\treturn pbl_table;\n\nerr:\n\tfor (i--; i >= 0; i--)\n\t\tdma_free_coherent(&pdev->dev, pbl_info->pbl_size,\n\t\t\t\t  pbl_table[i].va, pbl_table[i].pa);\n\n\tqedr_free_pbl(dev, pbl_info, pbl_table);\n\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int qedr_prepare_pbl_tbl(struct qedr_dev *dev,\n\t\t\t\tstruct qedr_pbl_info *pbl_info,\n\t\t\t\tu32 num_pbes, int two_layer_capable)\n{\n\tu32 pbl_capacity;\n\tu32 pbl_size;\n\tu32 num_pbls;\n\n\tif ((num_pbes > MAX_PBES_ON_PAGE) && two_layer_capable) {\n\t\tif (num_pbes > MAX_PBES_TWO_LAYER) {\n\t\t\tDP_ERR(dev, \"prepare pbl table: too many pages %d\\n\",\n\t\t\t       num_pbes);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tpbl_size = MIN_FW_PBL_PAGE_SIZE;\n\t\tpbl_capacity = NUM_PBES_ON_PAGE(pbl_size) *\n\t\t\t       NUM_PBES_ON_PAGE(pbl_size);\n\n\t\twhile (pbl_capacity < num_pbes) {\n\t\t\tpbl_size *= 2;\n\t\t\tpbl_capacity = pbl_size / sizeof(u64);\n\t\t\tpbl_capacity = pbl_capacity * pbl_capacity;\n\t\t}\n\n\t\tnum_pbls = DIV_ROUND_UP(num_pbes, NUM_PBES_ON_PAGE(pbl_size));\n\t\tnum_pbls++;\t \n\t\tpbl_info->two_layered = true;\n\t} else {\n\t\t \n\t\tnum_pbls = 1;\n\t\tpbl_size = max_t(u32, MIN_FW_PBL_PAGE_SIZE,\n\t\t\t\t roundup_pow_of_two((num_pbes * sizeof(u64))));\n\t\tpbl_info->two_layered = false;\n\t}\n\n\tpbl_info->num_pbls = num_pbls;\n\tpbl_info->pbl_size = pbl_size;\n\tpbl_info->num_pbes = num_pbes;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR,\n\t\t \"prepare pbl table: num_pbes=%d, num_pbls=%d, pbl_size=%d\\n\",\n\t\t pbl_info->num_pbes, pbl_info->num_pbls, pbl_info->pbl_size);\n\n\treturn 0;\n}\n\nstatic void qedr_populate_pbls(struct qedr_dev *dev, struct ib_umem *umem,\n\t\t\t       struct qedr_pbl *pbl,\n\t\t\t       struct qedr_pbl_info *pbl_info, u32 pg_shift)\n{\n\tint pbe_cnt, total_num_pbes = 0;\n\tstruct qedr_pbl *pbl_tbl;\n\tstruct ib_block_iter biter;\n\tstruct regpair *pbe;\n\n\tif (!pbl_info->num_pbes)\n\t\treturn;\n\n\t \n\tif (pbl_info->two_layered)\n\t\tpbl_tbl = &pbl[1];\n\telse\n\t\tpbl_tbl = pbl;\n\n\tpbe = (struct regpair *)pbl_tbl->va;\n\tif (!pbe) {\n\t\tDP_ERR(dev, \"cannot populate PBL due to a NULL PBE\\n\");\n\t\treturn;\n\t}\n\n\tpbe_cnt = 0;\n\n\trdma_umem_for_each_dma_block (umem, &biter, BIT(pg_shift)) {\n\t\tu64 pg_addr = rdma_block_iter_dma_address(&biter);\n\n\t\tpbe->lo = cpu_to_le32(pg_addr);\n\t\tpbe->hi = cpu_to_le32(upper_32_bits(pg_addr));\n\n\t\tpbe_cnt++;\n\t\ttotal_num_pbes++;\n\t\tpbe++;\n\n\t\tif (total_num_pbes == pbl_info->num_pbes)\n\t\t\treturn;\n\n\t\t \n\t\tif (pbe_cnt == (pbl_info->pbl_size / sizeof(u64))) {\n\t\t\tpbl_tbl++;\n\t\t\tpbe = (struct regpair *)pbl_tbl->va;\n\t\t\tpbe_cnt = 0;\n\t\t}\n\t}\n}\n\nstatic int qedr_db_recovery_add(struct qedr_dev *dev,\n\t\t\t\tvoid __iomem *db_addr,\n\t\t\t\tvoid *db_data,\n\t\t\t\tenum qed_db_rec_width db_width,\n\t\t\t\tenum qed_db_rec_space db_space)\n{\n\tif (!db_data) {\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"avoiding db rec since old lib\\n\");\n\t\treturn 0;\n\t}\n\n\treturn dev->ops->common->db_recovery_add(dev->cdev, db_addr, db_data,\n\t\t\t\t\t\t db_width, db_space);\n}\n\nstatic void qedr_db_recovery_del(struct qedr_dev *dev,\n\t\t\t\t void __iomem *db_addr,\n\t\t\t\t void *db_data)\n{\n\tif (!db_data) {\n\t\tDP_DEBUG(dev, QEDR_MSG_INIT, \"avoiding db rec since old lib\\n\");\n\t\treturn;\n\t}\n\n\t \n\tdev->ops->common->db_recovery_del(dev->cdev, db_addr, db_data);\n}\n\nstatic int qedr_copy_cq_uresp(struct qedr_dev *dev,\n\t\t\t      struct qedr_cq *cq, struct ib_udata *udata,\n\t\t\t      u32 db_offset)\n{\n\tstruct qedr_create_cq_uresp uresp;\n\tint rc;\n\n\tmemset(&uresp, 0, sizeof(uresp));\n\n\turesp.db_offset = db_offset;\n\turesp.icid = cq->icid;\n\tif (cq->q.db_mmap_entry)\n\t\turesp.db_rec_addr =\n\t\t\trdma_user_mmap_get_offset(cq->q.db_mmap_entry);\n\n\trc = qedr_ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (rc)\n\t\tDP_ERR(dev, \"copy error cqid=0x%x.\\n\", cq->icid);\n\n\treturn rc;\n}\n\nstatic void consume_cqe(struct qedr_cq *cq)\n{\n\tif (cq->latest_cqe == cq->toggle_cqe)\n\t\tcq->pbl_toggle ^= RDMA_CQE_REQUESTER_TOGGLE_BIT_MASK;\n\n\tcq->latest_cqe = qed_chain_consume(&cq->pbl);\n}\n\nstatic inline int qedr_align_cq_entries(int entries)\n{\n\tu64 size, aligned_size;\n\n\t \n\tsize = (entries + 1) * QEDR_CQE_SIZE;\n\taligned_size = ALIGN(size, PAGE_SIZE);\n\n\treturn aligned_size / QEDR_CQE_SIZE;\n}\n\nstatic int qedr_init_user_db_rec(struct ib_udata *udata,\n\t\t\t\t struct qedr_dev *dev, struct qedr_userq *q,\n\t\t\t\t bool requires_db_rec)\n{\n\tstruct qedr_ucontext *uctx =\n\t\trdma_udata_to_drv_context(udata, struct qedr_ucontext,\n\t\t\t\t\t  ibucontext);\n\tstruct qedr_user_mmap_entry *entry;\n\tint rc;\n\n\t \n\tif (requires_db_rec == 0 || !uctx->db_rec)\n\t\treturn 0;\n\n\t \n\tq->db_rec_data = (void *)get_zeroed_page(GFP_USER);\n\tif (!q->db_rec_data) {\n\t\tDP_ERR(dev, \"get_zeroed_page failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\tgoto err_free_db_data;\n\n\tentry->address = q->db_rec_data;\n\tentry->length = PAGE_SIZE;\n\tentry->mmap_flag = QEDR_USER_MMAP_PHYS_PAGE;\n\trc = rdma_user_mmap_entry_insert(&uctx->ibucontext,\n\t\t\t\t\t &entry->rdma_entry,\n\t\t\t\t\t PAGE_SIZE);\n\tif (rc)\n\t\tgoto err_free_entry;\n\n\tq->db_mmap_entry = &entry->rdma_entry;\n\n\treturn 0;\n\nerr_free_entry:\n\tkfree(entry);\n\nerr_free_db_data:\n\tfree_page((unsigned long)q->db_rec_data);\n\tq->db_rec_data = NULL;\n\treturn -ENOMEM;\n}\n\nstatic inline int qedr_init_user_queue(struct ib_udata *udata,\n\t\t\t\t       struct qedr_dev *dev,\n\t\t\t\t       struct qedr_userq *q, u64 buf_addr,\n\t\t\t\t       size_t buf_len, bool requires_db_rec,\n\t\t\t\t       int access,\n\t\t\t\t       int alloc_and_init)\n{\n\tu32 fw_pages;\n\tint rc;\n\n\tq->buf_addr = buf_addr;\n\tq->buf_len = buf_len;\n\tq->umem = ib_umem_get(&dev->ibdev, q->buf_addr, q->buf_len, access);\n\tif (IS_ERR(q->umem)) {\n\t\tDP_ERR(dev, \"create user queue: failed ib_umem_get, got %ld\\n\",\n\t\t       PTR_ERR(q->umem));\n\t\treturn PTR_ERR(q->umem);\n\t}\n\n\tfw_pages = ib_umem_num_dma_blocks(q->umem, 1 << FW_PAGE_SHIFT);\n\trc = qedr_prepare_pbl_tbl(dev, &q->pbl_info, fw_pages, 0);\n\tif (rc)\n\t\tgoto err0;\n\n\tif (alloc_and_init) {\n\t\tq->pbl_tbl = qedr_alloc_pbl_tbl(dev, &q->pbl_info, GFP_KERNEL);\n\t\tif (IS_ERR(q->pbl_tbl)) {\n\t\t\trc = PTR_ERR(q->pbl_tbl);\n\t\t\tgoto err0;\n\t\t}\n\t\tqedr_populate_pbls(dev, q->umem, q->pbl_tbl, &q->pbl_info,\n\t\t\t\t   FW_PAGE_SHIFT);\n\t} else {\n\t\tq->pbl_tbl = kzalloc(sizeof(*q->pbl_tbl), GFP_KERNEL);\n\t\tif (!q->pbl_tbl) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err0;\n\t\t}\n\t}\n\n\t \n\treturn qedr_init_user_db_rec(udata, dev, q, requires_db_rec);\n\nerr0:\n\tib_umem_release(q->umem);\n\tq->umem = NULL;\n\n\treturn rc;\n}\n\nstatic inline void qedr_init_cq_params(struct qedr_cq *cq,\n\t\t\t\t       struct qedr_ucontext *ctx,\n\t\t\t\t       struct qedr_dev *dev, int vector,\n\t\t\t\t       int chain_entries, int page_cnt,\n\t\t\t\t       u64 pbl_ptr,\n\t\t\t\t       struct qed_rdma_create_cq_in_params\n\t\t\t\t       *params)\n{\n\tmemset(params, 0, sizeof(*params));\n\tparams->cq_handle_hi = upper_32_bits((uintptr_t)cq);\n\tparams->cq_handle_lo = lower_32_bits((uintptr_t)cq);\n\tparams->cnq_id = vector;\n\tparams->cq_size = chain_entries - 1;\n\tparams->dpi = (ctx) ? ctx->dpi : dev->dpi;\n\tparams->pbl_num_pages = page_cnt;\n\tparams->pbl_ptr = pbl_ptr;\n\tparams->pbl_two_level = 0;\n}\n\nstatic void doorbell_cq(struct qedr_cq *cq, u32 cons, u8 flags)\n{\n\tcq->db.data.agg_flags = flags;\n\tcq->db.data.value = cpu_to_le32(cons);\n\twriteq(cq->db.raw, cq->db_addr);\n}\n\nint qedr_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags)\n{\n\tstruct qedr_cq *cq = get_qedr_cq(ibcq);\n\tunsigned long sflags;\n\tstruct qedr_dev *dev;\n\n\tdev = get_qedr_dev(ibcq->device);\n\n\tif (cq->destroyed) {\n\t\tDP_ERR(dev,\n\t\t       \"warning: arm was invoked after destroy for cq %p (icid=%d)\\n\",\n\t\t       cq, cq->icid);\n\t\treturn -EINVAL;\n\t}\n\n\n\tif (cq->cq_type == QEDR_CQ_TYPE_GSI)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&cq->cq_lock, sflags);\n\n\tcq->arm_flags = 0;\n\n\tif (flags & IB_CQ_SOLICITED)\n\t\tcq->arm_flags |= DQ_UCM_ROCE_CQ_ARM_SE_CF_CMD;\n\n\tif (flags & IB_CQ_NEXT_COMP)\n\t\tcq->arm_flags |= DQ_UCM_ROCE_CQ_ARM_CF_CMD;\n\n\tdoorbell_cq(cq, cq->cq_cons - 1, cq->arm_flags);\n\n\tspin_unlock_irqrestore(&cq->cq_lock, sflags);\n\n\treturn 0;\n}\n\nint qedr_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t   struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tstruct qedr_ucontext *ctx = rdma_udata_to_drv_context(\n\t\tudata, struct qedr_ucontext, ibucontext);\n\tstruct qed_rdma_destroy_cq_out_params destroy_oparams;\n\tstruct qed_rdma_destroy_cq_in_params destroy_iparams;\n\tstruct qed_chain_init_params chain_params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.intended_use\t= QED_CHAIN_USE_TO_CONSUME,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U32,\n\t\t.elem_size\t= sizeof(union rdma_cqe),\n\t};\n\tstruct qedr_dev *dev = get_qedr_dev(ibdev);\n\tstruct qed_rdma_create_cq_in_params params;\n\tstruct qedr_create_cq_ureq ureq = {};\n\tint vector = attr->comp_vector;\n\tint entries = attr->cqe;\n\tstruct qedr_cq *cq = get_qedr_cq(ibcq);\n\tint chain_entries;\n\tu32 db_offset;\n\tint page_cnt;\n\tu64 pbl_ptr;\n\tu16 icid;\n\tint rc;\n\n\tDP_DEBUG(dev, QEDR_MSG_INIT,\n\t\t \"create_cq: called from %s. entries=%d, vector=%d\\n\",\n\t\t udata ? \"User Lib\" : \"Kernel\", entries, vector);\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (entries > QEDR_MAX_CQES) {\n\t\tDP_ERR(dev,\n\t\t       \"create cq: the number of entries %d is too high. Must be equal or below %d.\\n\",\n\t\t       entries, QEDR_MAX_CQES);\n\t\treturn -EINVAL;\n\t}\n\n\tchain_entries = qedr_align_cq_entries(entries);\n\tchain_entries = min_t(int, chain_entries, QEDR_MAX_CQES);\n\tchain_params.num_elems = chain_entries;\n\n\t \n\tdb_offset = DB_ADDR_SHIFT(DQ_PWM_OFFSET_UCM_RDMA_CQ_CONS_32BIT);\n\n\tif (udata) {\n\t\tif (ib_copy_from_udata(&ureq, udata, min(sizeof(ureq),\n\t\t\t\t\t\t\t udata->inlen))) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"create cq: problem copying data from user space\\n\");\n\t\t\tgoto err0;\n\t\t}\n\n\t\tif (!ureq.len) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"create cq: cannot create a cq with 0 entries\\n\");\n\t\t\tgoto err0;\n\t\t}\n\n\t\tcq->cq_type = QEDR_CQ_TYPE_USER;\n\n\t\trc = qedr_init_user_queue(udata, dev, &cq->q, ureq.addr,\n\t\t\t\t\t  ureq.len, true, IB_ACCESS_LOCAL_WRITE,\n\t\t\t\t\t  1);\n\t\tif (rc)\n\t\t\tgoto err0;\n\n\t\tpbl_ptr = cq->q.pbl_tbl->pa;\n\t\tpage_cnt = cq->q.pbl_info.num_pbes;\n\n\t\tcq->ibcq.cqe = chain_entries;\n\t\tcq->q.db_addr = ctx->dpi_addr + db_offset;\n\t} else {\n\t\tcq->cq_type = QEDR_CQ_TYPE_KERNEL;\n\n\t\trc = dev->ops->common->chain_alloc(dev->cdev, &cq->pbl,\n\t\t\t\t\t\t   &chain_params);\n\t\tif (rc)\n\t\t\tgoto err0;\n\n\t\tpage_cnt = qed_chain_get_page_cnt(&cq->pbl);\n\t\tpbl_ptr = qed_chain_get_pbl_phys(&cq->pbl);\n\t\tcq->ibcq.cqe = cq->pbl.capacity;\n\t}\n\n\tqedr_init_cq_params(cq, ctx, dev, vector, chain_entries, page_cnt,\n\t\t\t    pbl_ptr, &params);\n\n\trc = dev->ops->rdma_create_cq(dev->rdma_ctx, &params, &icid);\n\tif (rc)\n\t\tgoto err1;\n\n\tcq->icid = icid;\n\tcq->sig = QEDR_CQ_MAGIC_NUMBER;\n\tspin_lock_init(&cq->cq_lock);\n\n\tif (udata) {\n\t\trc = qedr_copy_cq_uresp(dev, cq, udata, db_offset);\n\t\tif (rc)\n\t\t\tgoto err2;\n\n\t\trc = qedr_db_recovery_add(dev, cq->q.db_addr,\n\t\t\t\t\t  &cq->q.db_rec_data->db_data,\n\t\t\t\t\t  DB_REC_WIDTH_64B,\n\t\t\t\t\t  DB_REC_USER);\n\t\tif (rc)\n\t\t\tgoto err2;\n\n\t} else {\n\t\t \n\t\tcq->db.data.icid = cq->icid;\n\t\tcq->db_addr = dev->db_addr + db_offset;\n\t\tcq->db.data.params = DB_AGG_CMD_MAX <<\n\t\t    RDMA_PWM_VAL32_DATA_AGG_CMD_SHIFT;\n\n\t\t \n\t\tcq->toggle_cqe = qed_chain_get_last_elem(&cq->pbl);\n\t\tcq->pbl_toggle = RDMA_CQE_REQUESTER_TOGGLE_BIT_MASK;\n\t\tcq->latest_cqe = NULL;\n\t\tconsume_cqe(cq);\n\t\tcq->cq_cons = qed_chain_get_cons_idx_u32(&cq->pbl);\n\n\t\trc = qedr_db_recovery_add(dev, cq->db_addr, &cq->db.data,\n\t\t\t\t\t  DB_REC_WIDTH_64B, DB_REC_KERNEL);\n\t\tif (rc)\n\t\t\tgoto err2;\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_CQ,\n\t\t \"create cq: icid=0x%0x, addr=%p, size(entries)=0x%0x\\n\",\n\t\t cq->icid, cq, params.cq_size);\n\n\treturn 0;\n\nerr2:\n\tdestroy_iparams.icid = cq->icid;\n\tdev->ops->rdma_destroy_cq(dev->rdma_ctx, &destroy_iparams,\n\t\t\t\t  &destroy_oparams);\nerr1:\n\tif (udata) {\n\t\tqedr_free_pbl(dev, &cq->q.pbl_info, cq->q.pbl_tbl);\n\t\tib_umem_release(cq->q.umem);\n\t\tif (cq->q.db_mmap_entry)\n\t\t\trdma_user_mmap_entry_remove(cq->q.db_mmap_entry);\n\t} else {\n\t\tdev->ops->common->chain_free(dev->cdev, &cq->pbl);\n\t}\nerr0:\n\treturn -EINVAL;\n}\n\n#define QEDR_DESTROY_CQ_MAX_ITERATIONS\t\t(10)\n#define QEDR_DESTROY_CQ_ITER_DURATION\t\t(10)\n\nint qedr_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibcq->device);\n\tstruct qed_rdma_destroy_cq_out_params oparams;\n\tstruct qed_rdma_destroy_cq_in_params iparams;\n\tstruct qedr_cq *cq = get_qedr_cq(ibcq);\n\tint iter;\n\n\tDP_DEBUG(dev, QEDR_MSG_CQ, \"destroy cq %p (icid=%d)\\n\", cq, cq->icid);\n\n\tcq->destroyed = 1;\n\n\t \n\tif (cq->cq_type == QEDR_CQ_TYPE_GSI) {\n\t\tqedr_db_recovery_del(dev, cq->db_addr, &cq->db.data);\n\t\treturn 0;\n\t}\n\n\tiparams.icid = cq->icid;\n\tdev->ops->rdma_destroy_cq(dev->rdma_ctx, &iparams, &oparams);\n\tdev->ops->common->chain_free(dev->cdev, &cq->pbl);\n\n\tif (udata) {\n\t\tqedr_free_pbl(dev, &cq->q.pbl_info, cq->q.pbl_tbl);\n\t\tib_umem_release(cq->q.umem);\n\n\t\tif (cq->q.db_rec_data) {\n\t\t\tqedr_db_recovery_del(dev, cq->q.db_addr,\n\t\t\t\t\t     &cq->q.db_rec_data->db_data);\n\t\t\trdma_user_mmap_entry_remove(cq->q.db_mmap_entry);\n\t\t}\n\t} else {\n\t\tqedr_db_recovery_del(dev, cq->db_addr, &cq->db.data);\n\t}\n\n\t \n\titer = QEDR_DESTROY_CQ_MAX_ITERATIONS;\n\twhile (oparams.num_cq_notif != READ_ONCE(cq->cnq_notif) && iter) {\n\t\tudelay(QEDR_DESTROY_CQ_ITER_DURATION);\n\t\titer--;\n\t}\n\n\titer = QEDR_DESTROY_CQ_MAX_ITERATIONS;\n\twhile (oparams.num_cq_notif != READ_ONCE(cq->cnq_notif) && iter) {\n\t\tmsleep(QEDR_DESTROY_CQ_ITER_DURATION);\n\t\titer--;\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic inline int get_gid_info_from_table(struct ib_qp *ibqp,\n\t\t\t\t\t  struct ib_qp_attr *attr,\n\t\t\t\t\t  int attr_mask,\n\t\t\t\t\t  struct qed_rdma_modify_qp_in_params\n\t\t\t\t\t  *qp_params)\n{\n\tconst struct ib_gid_attr *gid_attr;\n\tenum rdma_network_type nw_type;\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(&attr->ah_attr);\n\tu32 ipv4_addr;\n\tint ret;\n\tint i;\n\n\tgid_attr = grh->sgid_attr;\n\tret = rdma_read_gid_l2_fields(gid_attr, &qp_params->vlan_id, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tnw_type = rdma_gid_attr_network_type(gid_attr);\n\tswitch (nw_type) {\n\tcase RDMA_NETWORK_IPV6:\n\t\tmemcpy(&qp_params->sgid.bytes[0], &gid_attr->gid.raw[0],\n\t\t       sizeof(qp_params->sgid));\n\t\tmemcpy(&qp_params->dgid.bytes[0],\n\t\t       &grh->dgid,\n\t\t       sizeof(qp_params->dgid));\n\t\tqp_params->roce_mode = ROCE_V2_IPV6;\n\t\tSET_FIELD(qp_params->modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_ROCE_MODE, 1);\n\t\tbreak;\n\tcase RDMA_NETWORK_ROCE_V1:\n\t\tmemcpy(&qp_params->sgid.bytes[0], &gid_attr->gid.raw[0],\n\t\t       sizeof(qp_params->sgid));\n\t\tmemcpy(&qp_params->dgid.bytes[0],\n\t\t       &grh->dgid,\n\t\t       sizeof(qp_params->dgid));\n\t\tqp_params->roce_mode = ROCE_V1;\n\t\tbreak;\n\tcase RDMA_NETWORK_IPV4:\n\t\tmemset(&qp_params->sgid, 0, sizeof(qp_params->sgid));\n\t\tmemset(&qp_params->dgid, 0, sizeof(qp_params->dgid));\n\t\tipv4_addr = qedr_get_ipv4_from_gid(gid_attr->gid.raw);\n\t\tqp_params->sgid.ipv4_addr = ipv4_addr;\n\t\tipv4_addr =\n\t\t    qedr_get_ipv4_from_gid(grh->dgid.raw);\n\t\tqp_params->dgid.ipv4_addr = ipv4_addr;\n\t\tSET_FIELD(qp_params->modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_ROCE_MODE, 1);\n\t\tqp_params->roce_mode = ROCE_V2_IPV4;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < 4; i++) {\n\t\tqp_params->sgid.dwords[i] = ntohl(qp_params->sgid.dwords[i]);\n\t\tqp_params->dgid.dwords[i] = ntohl(qp_params->dgid.dwords[i]);\n\t}\n\n\tif (qp_params->vlan_id >= VLAN_CFI_MASK)\n\t\tqp_params->vlan_id = 0;\n\n\treturn 0;\n}\n\nstatic int qedr_check_qp_attrs(struct ib_pd *ibpd, struct qedr_dev *dev,\n\t\t\t       struct ib_qp_init_attr *attrs,\n\t\t\t       struct ib_udata *udata)\n{\n\tstruct qedr_device_attr *qattr = &dev->attr;\n\n\t \n\tif (attrs->qp_type != IB_QPT_RC &&\n\t    attrs->qp_type != IB_QPT_GSI &&\n\t    attrs->qp_type != IB_QPT_XRC_INI &&\n\t    attrs->qp_type != IB_QPT_XRC_TGT) {\n\t\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t\t \"create qp: unsupported qp type=0x%x requested\\n\",\n\t\t\t attrs->qp_type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (attrs->cap.max_send_wr > qattr->max_sqe) {\n\t\tDP_ERR(dev,\n\t\t       \"create qp: cannot create a SQ with %d elements (max_send_wr=0x%x)\\n\",\n\t\t       attrs->cap.max_send_wr, qattr->max_sqe);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attrs->cap.max_inline_data > qattr->max_inline) {\n\t\tDP_ERR(dev,\n\t\t       \"create qp: unsupported inline data size=0x%x requested (max_inline=0x%x)\\n\",\n\t\t       attrs->cap.max_inline_data, qattr->max_inline);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attrs->cap.max_send_sge > qattr->max_sge) {\n\t\tDP_ERR(dev,\n\t\t       \"create qp: unsupported send_sge=0x%x requested (max_send_sge=0x%x)\\n\",\n\t\t       attrs->cap.max_send_sge, qattr->max_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attrs->cap.max_recv_sge > qattr->max_sge) {\n\t\tDP_ERR(dev,\n\t\t       \"create qp: unsupported recv_sge=0x%x requested (max_recv_sge=0x%x)\\n\",\n\t\t       attrs->cap.max_recv_sge, qattr->max_sge);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((attrs->qp_type != IB_QPT_GSI) && (dev->gsi_qp_created) &&\n\t    (attrs->qp_type != IB_QPT_XRC_TGT) &&\n\t    (attrs->qp_type != IB_QPT_XRC_INI)) {\n\t\tstruct qedr_cq *send_cq = get_qedr_cq(attrs->send_cq);\n\t\tstruct qedr_cq *recv_cq = get_qedr_cq(attrs->recv_cq);\n\n\t\tif ((send_cq->cq_type == QEDR_CQ_TYPE_GSI) ||\n\t\t    (recv_cq->cq_type == QEDR_CQ_TYPE_GSI)) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"create qp: consumer QP cannot use GSI CQs.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int qedr_copy_srq_uresp(struct qedr_dev *dev,\n\t\t\t       struct qedr_srq *srq, struct ib_udata *udata)\n{\n\tstruct qedr_create_srq_uresp uresp = {};\n\tint rc;\n\n\turesp.srq_id = srq->srq_id;\n\n\trc = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (rc)\n\t\tDP_ERR(dev, \"create srq: problem copying data to user space\\n\");\n\n\treturn rc;\n}\n\nstatic void qedr_copy_rq_uresp(struct qedr_dev *dev,\n\t\t\t       struct qedr_create_qp_uresp *uresp,\n\t\t\t       struct qedr_qp *qp)\n{\n\t \n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\turesp->rq_db_offset =\n\t\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_IWARP_RQ_PROD);\n\t\turesp->rq_db2_offset = DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_FLAGS);\n\t} else {\n\t\turesp->rq_db_offset =\n\t\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_ROCE_RQ_PROD);\n\t}\n\n\turesp->rq_icid = qp->icid;\n\tif (qp->urq.db_mmap_entry)\n\t\turesp->rq_db_rec_addr =\n\t\t\trdma_user_mmap_get_offset(qp->urq.db_mmap_entry);\n}\n\nstatic void qedr_copy_sq_uresp(struct qedr_dev *dev,\n\t\t\t       struct qedr_create_qp_uresp *uresp,\n\t\t\t       struct qedr_qp *qp)\n{\n\turesp->sq_db_offset = DB_ADDR_SHIFT(DQ_PWM_OFFSET_XCM_RDMA_SQ_PROD);\n\n\t \n\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\turesp->sq_icid = qp->icid;\n\telse\n\t\turesp->sq_icid = qp->icid + 1;\n\n\tif (qp->usq.db_mmap_entry)\n\t\turesp->sq_db_rec_addr =\n\t\t\trdma_user_mmap_get_offset(qp->usq.db_mmap_entry);\n}\n\nstatic int qedr_copy_qp_uresp(struct qedr_dev *dev,\n\t\t\t      struct qedr_qp *qp, struct ib_udata *udata,\n\t\t\t      struct qedr_create_qp_uresp *uresp)\n{\n\tint rc;\n\n\tmemset(uresp, 0, sizeof(*uresp));\n\n\tif (qedr_qp_has_sq(qp))\n\t\tqedr_copy_sq_uresp(dev, uresp, qp);\n\n\tif (qedr_qp_has_rq(qp))\n\t\tqedr_copy_rq_uresp(dev, uresp, qp);\n\n\turesp->atomic_supported = dev->atomic_cap != IB_ATOMIC_NONE;\n\turesp->qp_id = qp->qp_id;\n\n\trc = qedr_ib_copy_to_udata(udata, uresp, sizeof(*uresp));\n\tif (rc)\n\t\tDP_ERR(dev,\n\t\t       \"create qp: failed a copy to user space with qp icid=0x%x.\\n\",\n\t\t       qp->icid);\n\n\treturn rc;\n}\n\nstatic void qedr_reset_qp_hwq_info(struct qedr_qp_hwq_info *qph)\n{\n\tqed_chain_reset(&qph->pbl);\n\tqph->prod = 0;\n\tqph->cons = 0;\n\tqph->wqe_cons = 0;\n\tqph->db_data.data.value = cpu_to_le16(0);\n}\n\nstatic void qedr_set_common_qp_params(struct qedr_dev *dev,\n\t\t\t\t      struct qedr_qp *qp,\n\t\t\t\t      struct qedr_pd *pd,\n\t\t\t\t      struct ib_qp_init_attr *attrs)\n{\n\tspin_lock_init(&qp->q_lock);\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\tkref_init(&qp->refcnt);\n\t\tinit_completion(&qp->iwarp_cm_comp);\n\t\tinit_completion(&qp->qp_rel_comp);\n\t}\n\n\tqp->pd = pd;\n\tqp->qp_type = attrs->qp_type;\n\tqp->max_inline_data = attrs->cap.max_inline_data;\n\tqp->state = QED_ROCE_QP_STATE_RESET;\n\n\tqp->prev_wqe_size = 0;\n\n\tqp->signaled = attrs->sq_sig_type == IB_SIGNAL_ALL_WR;\n\tqp->dev = dev;\n\tif (qedr_qp_has_sq(qp)) {\n\t\tqedr_reset_qp_hwq_info(&qp->sq);\n\t\tqp->sq.max_sges = attrs->cap.max_send_sge;\n\t\tqp->sq_cq = get_qedr_cq(attrs->send_cq);\n\t\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t\t \"SQ params:\\tsq_max_sges = %d, sq_cq_id = %d\\n\",\n\t\t\t qp->sq.max_sges, qp->sq_cq->icid);\n\t}\n\n\tif (attrs->srq)\n\t\tqp->srq = get_qedr_srq(attrs->srq);\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\tqedr_reset_qp_hwq_info(&qp->rq);\n\t\tqp->rq_cq = get_qedr_cq(attrs->recv_cq);\n\t\tqp->rq.max_sges = attrs->cap.max_recv_sge;\n\t\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t\t \"RQ params:\\trq_max_sges = %d, rq_cq_id = %d\\n\",\n\t\t\t qp->rq.max_sges, qp->rq_cq->icid);\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t \"QP params:\\tpd = %d, qp_type = %d, max_inline_data = %d, state = %d, signaled = %d, use_srq=%d\\n\",\n\t\t pd->pd_id, qp->qp_type, qp->max_inline_data,\n\t\t qp->state, qp->signaled, (attrs->srq) ? 1 : 0);\n\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t \"SQ params:\\tsq_max_sges = %d, sq_cq_id = %d\\n\",\n\t\t qp->sq.max_sges, qp->sq_cq->icid);\n}\n\nstatic int qedr_set_roce_db_info(struct qedr_dev *dev, struct qedr_qp *qp)\n{\n\tint rc = 0;\n\n\tif (qedr_qp_has_sq(qp)) {\n\t\tqp->sq.db = dev->db_addr +\n\t\t\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_XCM_RDMA_SQ_PROD);\n\t\tqp->sq.db_data.data.icid = qp->icid + 1;\n\t\trc = qedr_db_recovery_add(dev, qp->sq.db, &qp->sq.db_data,\n\t\t\t\t\t  DB_REC_WIDTH_32B, DB_REC_KERNEL);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\tqp->rq.db = dev->db_addr +\n\t\t\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_ROCE_RQ_PROD);\n\t\tqp->rq.db_data.data.icid = qp->icid;\n\t\trc = qedr_db_recovery_add(dev, qp->rq.db, &qp->rq.db_data,\n\t\t\t\t\t  DB_REC_WIDTH_32B, DB_REC_KERNEL);\n\t\tif (rc && qedr_qp_has_sq(qp))\n\t\t\tqedr_db_recovery_del(dev, qp->sq.db, &qp->sq.db_data);\n\t}\n\n\treturn rc;\n}\n\nstatic int qedr_check_srq_params(struct qedr_dev *dev,\n\t\t\t\t struct ib_srq_init_attr *attrs,\n\t\t\t\t struct ib_udata *udata)\n{\n\tstruct qedr_device_attr *qattr = &dev->attr;\n\n\tif (attrs->attr.max_wr > qattr->max_srq_wr) {\n\t\tDP_ERR(dev,\n\t\t       \"create srq: unsupported srq_wr=0x%x requested (max_srq_wr=0x%x)\\n\",\n\t\t       attrs->attr.max_wr, qattr->max_srq_wr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attrs->attr.max_sge > qattr->max_sge) {\n\t\tDP_ERR(dev,\n\t\t       \"create srq: unsupported sge=0x%x requested (max_srq_sge=0x%x)\\n\",\n\t\t       attrs->attr.max_sge, qattr->max_sge);\n\t}\n\n\tif (!udata && attrs->srq_type == IB_SRQT_XRC) {\n\t\tDP_ERR(dev, \"XRC SRQs are not supported in kernel-space\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void qedr_free_srq_user_params(struct qedr_srq *srq)\n{\n\tqedr_free_pbl(srq->dev, &srq->usrq.pbl_info, srq->usrq.pbl_tbl);\n\tib_umem_release(srq->usrq.umem);\n\tib_umem_release(srq->prod_umem);\n}\n\nstatic void qedr_free_srq_kernel_params(struct qedr_srq *srq)\n{\n\tstruct qedr_srq_hwq_info *hw_srq = &srq->hw_srq;\n\tstruct qedr_dev *dev = srq->dev;\n\n\tdev->ops->common->chain_free(dev->cdev, &hw_srq->pbl);\n\n\tdma_free_coherent(&dev->pdev->dev, sizeof(struct rdma_srq_producers),\n\t\t\t  hw_srq->virt_prod_pair_addr,\n\t\t\t  hw_srq->phy_prod_pair_addr);\n}\n\nstatic int qedr_init_srq_user_params(struct ib_udata *udata,\n\t\t\t\t     struct qedr_srq *srq,\n\t\t\t\t     struct qedr_create_srq_ureq *ureq,\n\t\t\t\t     int access)\n{\n\tstruct scatterlist *sg;\n\tint rc;\n\n\trc = qedr_init_user_queue(udata, srq->dev, &srq->usrq, ureq->srq_addr,\n\t\t\t\t  ureq->srq_len, false, access, 1);\n\tif (rc)\n\t\treturn rc;\n\n\tsrq->prod_umem = ib_umem_get(srq->ibsrq.device, ureq->prod_pair_addr,\n\t\t\t\t     sizeof(struct rdma_srq_producers), access);\n\tif (IS_ERR(srq->prod_umem)) {\n\t\tqedr_free_pbl(srq->dev, &srq->usrq.pbl_info, srq->usrq.pbl_tbl);\n\t\tib_umem_release(srq->usrq.umem);\n\t\tDP_ERR(srq->dev,\n\t\t       \"create srq: failed ib_umem_get for producer, got %ld\\n\",\n\t\t       PTR_ERR(srq->prod_umem));\n\t\treturn PTR_ERR(srq->prod_umem);\n\t}\n\n\tsg = srq->prod_umem->sgt_append.sgt.sgl;\n\tsrq->hw_srq.phy_prod_pair_addr = sg_dma_address(sg);\n\n\treturn 0;\n}\n\nstatic int qedr_alloc_srq_kernel_params(struct qedr_srq *srq,\n\t\t\t\t\tstruct qedr_dev *dev,\n\t\t\t\t\tstruct ib_srq_init_attr *init_attr)\n{\n\tstruct qedr_srq_hwq_info *hw_srq = &srq->hw_srq;\n\tstruct qed_chain_init_params params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.intended_use\t= QED_CHAIN_USE_TO_CONSUME_PRODUCE,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U32,\n\t\t.elem_size\t= QEDR_SRQ_WQE_ELEM_SIZE,\n\t};\n\tdma_addr_t phy_prod_pair_addr;\n\tu32 num_elems;\n\tvoid *va;\n\tint rc;\n\n\tva = dma_alloc_coherent(&dev->pdev->dev,\n\t\t\t\tsizeof(struct rdma_srq_producers),\n\t\t\t\t&phy_prod_pair_addr, GFP_KERNEL);\n\tif (!va) {\n\t\tDP_ERR(dev,\n\t\t       \"create srq: failed to allocate dma memory for producer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\thw_srq->phy_prod_pair_addr = phy_prod_pair_addr;\n\thw_srq->virt_prod_pair_addr = va;\n\n\tnum_elems = init_attr->attr.max_wr * RDMA_MAX_SRQ_WQE_SIZE;\n\tparams.num_elems = num_elems;\n\n\trc = dev->ops->common->chain_alloc(dev->cdev, &hw_srq->pbl, &params);\n\tif (rc)\n\t\tgoto err0;\n\n\thw_srq->num_elems = num_elems;\n\n\treturn 0;\n\nerr0:\n\tdma_free_coherent(&dev->pdev->dev, sizeof(struct rdma_srq_producers),\n\t\t\t  va, phy_prod_pair_addr);\n\treturn rc;\n}\n\nint qedr_create_srq(struct ib_srq *ibsrq, struct ib_srq_init_attr *init_attr,\n\t\t    struct ib_udata *udata)\n{\n\tstruct qed_rdma_destroy_srq_in_params destroy_in_params;\n\tstruct qed_rdma_create_srq_in_params in_params = {};\n\tstruct qedr_dev *dev = get_qedr_dev(ibsrq->device);\n\tstruct qed_rdma_create_srq_out_params out_params;\n\tstruct qedr_pd *pd = get_qedr_pd(ibsrq->pd);\n\tstruct qedr_create_srq_ureq ureq = {};\n\tu64 pbl_base_addr, phy_prod_pair_addr;\n\tstruct qedr_srq_hwq_info *hw_srq;\n\tu32 page_cnt, page_size;\n\tstruct qedr_srq *srq = get_qedr_srq(ibsrq);\n\tint rc = 0;\n\n\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t \"create SRQ called from %s (pd %p)\\n\",\n\t\t (udata) ? \"User lib\" : \"kernel\", pd);\n\n\tif (init_attr->srq_type != IB_SRQT_BASIC &&\n\t    init_attr->srq_type != IB_SRQT_XRC)\n\t\treturn -EOPNOTSUPP;\n\n\trc = qedr_check_srq_params(dev, init_attr, udata);\n\tif (rc)\n\t\treturn -EINVAL;\n\n\tsrq->dev = dev;\n\tsrq->is_xrc = (init_attr->srq_type == IB_SRQT_XRC);\n\thw_srq = &srq->hw_srq;\n\tspin_lock_init(&srq->lock);\n\n\thw_srq->max_wr = init_attr->attr.max_wr;\n\thw_srq->max_sges = init_attr->attr.max_sge;\n\n\tif (udata) {\n\t\tif (ib_copy_from_udata(&ureq, udata, min(sizeof(ureq),\n\t\t\t\t\t\t\t udata->inlen))) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"create srq: problem copying data from user space\\n\");\n\t\t\tgoto err0;\n\t\t}\n\n\t\trc = qedr_init_srq_user_params(udata, srq, &ureq, 0);\n\t\tif (rc)\n\t\t\tgoto err0;\n\n\t\tpage_cnt = srq->usrq.pbl_info.num_pbes;\n\t\tpbl_base_addr = srq->usrq.pbl_tbl->pa;\n\t\tphy_prod_pair_addr = hw_srq->phy_prod_pair_addr;\n\t\tpage_size = PAGE_SIZE;\n\t} else {\n\t\tstruct qed_chain *pbl;\n\n\t\trc = qedr_alloc_srq_kernel_params(srq, dev, init_attr);\n\t\tif (rc)\n\t\t\tgoto err0;\n\n\t\tpbl = &hw_srq->pbl;\n\t\tpage_cnt = qed_chain_get_page_cnt(pbl);\n\t\tpbl_base_addr = qed_chain_get_pbl_phys(pbl);\n\t\tphy_prod_pair_addr = hw_srq->phy_prod_pair_addr;\n\t\tpage_size = QED_CHAIN_PAGE_SIZE;\n\t}\n\n\tin_params.pd_id = pd->pd_id;\n\tin_params.pbl_base_addr = pbl_base_addr;\n\tin_params.prod_pair_addr = phy_prod_pair_addr;\n\tin_params.num_pages = page_cnt;\n\tin_params.page_size = page_size;\n\tif (srq->is_xrc) {\n\t\tstruct qedr_xrcd *xrcd = get_qedr_xrcd(init_attr->ext.xrc.xrcd);\n\t\tstruct qedr_cq *cq = get_qedr_cq(init_attr->ext.cq);\n\n\t\tin_params.is_xrc = 1;\n\t\tin_params.xrcd_id = xrcd->xrcd_id;\n\t\tin_params.cq_cid = cq->icid;\n\t}\n\n\trc = dev->ops->rdma_create_srq(dev->rdma_ctx, &in_params, &out_params);\n\tif (rc)\n\t\tgoto err1;\n\n\tsrq->srq_id = out_params.srq_id;\n\n\tif (udata) {\n\t\trc = qedr_copy_srq_uresp(dev, srq, udata);\n\t\tif (rc)\n\t\t\tgoto err2;\n\t}\n\n\trc = xa_insert_irq(&dev->srqs, srq->srq_id, srq, GFP_KERNEL);\n\tif (rc)\n\t\tgoto err2;\n\n\tDP_DEBUG(dev, QEDR_MSG_SRQ,\n\t\t \"create srq: created srq with srq_id=0x%0x\\n\", srq->srq_id);\n\treturn 0;\n\nerr2:\n\tdestroy_in_params.srq_id = srq->srq_id;\n\n\tdev->ops->rdma_destroy_srq(dev->rdma_ctx, &destroy_in_params);\nerr1:\n\tif (udata)\n\t\tqedr_free_srq_user_params(srq);\n\telse\n\t\tqedr_free_srq_kernel_params(srq);\nerr0:\n\treturn -EFAULT;\n}\n\nint qedr_destroy_srq(struct ib_srq *ibsrq, struct ib_udata *udata)\n{\n\tstruct qed_rdma_destroy_srq_in_params in_params = {};\n\tstruct qedr_dev *dev = get_qedr_dev(ibsrq->device);\n\tstruct qedr_srq *srq = get_qedr_srq(ibsrq);\n\n\txa_erase_irq(&dev->srqs, srq->srq_id);\n\tin_params.srq_id = srq->srq_id;\n\tin_params.is_xrc = srq->is_xrc;\n\tdev->ops->rdma_destroy_srq(dev->rdma_ctx, &in_params);\n\n\tif (ibsrq->uobject)\n\t\tqedr_free_srq_user_params(srq);\n\telse\n\t\tqedr_free_srq_kernel_params(srq);\n\n\tDP_DEBUG(dev, QEDR_MSG_SRQ,\n\t\t \"destroy srq: destroyed srq with srq_id=0x%0x\\n\",\n\t\t srq->srq_id);\n\treturn 0;\n}\n\nint qedr_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,\n\t\t    enum ib_srq_attr_mask attr_mask, struct ib_udata *udata)\n{\n\tstruct qed_rdma_modify_srq_in_params in_params = {};\n\tstruct qedr_dev *dev = get_qedr_dev(ibsrq->device);\n\tstruct qedr_srq *srq = get_qedr_srq(ibsrq);\n\tint rc;\n\n\tif (attr_mask & IB_SRQ_MAX_WR) {\n\t\tDP_ERR(dev,\n\t\t       \"modify srq: invalid attribute mask=0x%x specified for %p\\n\",\n\t\t       attr_mask, srq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr_mask & IB_SRQ_LIMIT) {\n\t\tif (attr->srq_limit >= srq->hw_srq.max_wr) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"modify srq: invalid srq_limit=0x%x (max_srq_limit=0x%x)\\n\",\n\t\t\t       attr->srq_limit, srq->hw_srq.max_wr);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tin_params.srq_id = srq->srq_id;\n\t\tin_params.wqe_limit = attr->srq_limit;\n\t\trc = dev->ops->rdma_modify_srq(dev->rdma_ctx, &in_params);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tsrq->srq_limit = attr->srq_limit;\n\n\tDP_DEBUG(dev, QEDR_MSG_SRQ,\n\t\t \"modify srq: modified srq with srq_id=0x%0x\\n\", srq->srq_id);\n\n\treturn 0;\n}\n\nstatic enum qed_rdma_qp_type qedr_ib_to_qed_qp_type(enum ib_qp_type ib_qp_type)\n{\n\tswitch (ib_qp_type) {\n\tcase IB_QPT_RC:\n\t\treturn QED_RDMA_QP_TYPE_RC;\n\tcase IB_QPT_XRC_INI:\n\t\treturn QED_RDMA_QP_TYPE_XRC_INI;\n\tcase IB_QPT_XRC_TGT:\n\t\treturn QED_RDMA_QP_TYPE_XRC_TGT;\n\tdefault:\n\t\treturn QED_RDMA_QP_TYPE_INVAL;\n\t}\n}\n\nstatic inline void\nqedr_init_common_qp_in_params(struct qedr_dev *dev,\n\t\t\t      struct qedr_pd *pd,\n\t\t\t      struct qedr_qp *qp,\n\t\t\t      struct ib_qp_init_attr *attrs,\n\t\t\t      bool fmr_and_reserved_lkey,\n\t\t\t      struct qed_rdma_create_qp_in_params *params)\n{\n\t \n\tparams->qp_handle_async_lo = lower_32_bits((uintptr_t) qp);\n\tparams->qp_handle_async_hi = upper_32_bits((uintptr_t) qp);\n\n\tparams->signal_all = (attrs->sq_sig_type == IB_SIGNAL_ALL_WR);\n\tparams->fmr_and_reserved_lkey = fmr_and_reserved_lkey;\n\tparams->qp_type = qedr_ib_to_qed_qp_type(attrs->qp_type);\n\tparams->stats_queue = 0;\n\n\tif (pd) {\n\t\tparams->pd = pd->pd_id;\n\t\tparams->dpi = pd->uctx ? pd->uctx->dpi : dev->dpi;\n\t}\n\n\tif (qedr_qp_has_sq(qp))\n\t\tparams->sq_cq_id = get_qedr_cq(attrs->send_cq)->icid;\n\n\tif (qedr_qp_has_rq(qp))\n\t\tparams->rq_cq_id = get_qedr_cq(attrs->recv_cq)->icid;\n\n\tif (qedr_qp_has_srq(qp)) {\n\t\tparams->rq_cq_id = get_qedr_cq(attrs->recv_cq)->icid;\n\t\tparams->srq_id = qp->srq->srq_id;\n\t\tparams->use_srq = true;\n\t} else {\n\t\tparams->srq_id = 0;\n\t\tparams->use_srq = false;\n\t}\n}\n\nstatic inline void qedr_qp_user_print(struct qedr_dev *dev, struct qedr_qp *qp)\n{\n\tDP_DEBUG(dev, QEDR_MSG_QP, \"create qp: successfully created user QP. \"\n\t\t \"qp=%p. \"\n\t\t \"sq_addr=0x%llx, \"\n\t\t \"sq_len=%zd, \"\n\t\t \"rq_addr=0x%llx, \"\n\t\t \"rq_len=%zd\"\n\t\t \"\\n\",\n\t\t qp,\n\t\t qedr_qp_has_sq(qp) ? qp->usq.buf_addr : 0x0,\n\t\t qedr_qp_has_sq(qp) ? qp->usq.buf_len : 0,\n\t\t qedr_qp_has_rq(qp) ? qp->urq.buf_addr : 0x0,\n\t\t qedr_qp_has_sq(qp) ? qp->urq.buf_len : 0);\n}\n\nstatic inline void\nqedr_iwarp_populate_user_qp(struct qedr_dev *dev,\n\t\t\t    struct qedr_qp *qp,\n\t\t\t    struct qed_rdma_create_qp_out_params *out_params)\n{\n\tqp->usq.pbl_tbl->va = out_params->sq_pbl_virt;\n\tqp->usq.pbl_tbl->pa = out_params->sq_pbl_phys;\n\n\tqedr_populate_pbls(dev, qp->usq.umem, qp->usq.pbl_tbl,\n\t\t\t   &qp->usq.pbl_info, FW_PAGE_SHIFT);\n\tif (!qp->srq) {\n\t\tqp->urq.pbl_tbl->va = out_params->rq_pbl_virt;\n\t\tqp->urq.pbl_tbl->pa = out_params->rq_pbl_phys;\n\t}\n\n\tqedr_populate_pbls(dev, qp->urq.umem, qp->urq.pbl_tbl,\n\t\t\t   &qp->urq.pbl_info, FW_PAGE_SHIFT);\n}\n\nstatic void qedr_cleanup_user(struct qedr_dev *dev,\n\t\t\t      struct qedr_ucontext *ctx,\n\t\t\t      struct qedr_qp *qp)\n{\n\tif (qedr_qp_has_sq(qp)) {\n\t\tib_umem_release(qp->usq.umem);\n\t\tqp->usq.umem = NULL;\n\t}\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\tib_umem_release(qp->urq.umem);\n\t\tqp->urq.umem = NULL;\n\t}\n\n\tif (rdma_protocol_roce(&dev->ibdev, 1)) {\n\t\tqedr_free_pbl(dev, &qp->usq.pbl_info, qp->usq.pbl_tbl);\n\t\tqedr_free_pbl(dev, &qp->urq.pbl_info, qp->urq.pbl_tbl);\n\t} else {\n\t\tkfree(qp->usq.pbl_tbl);\n\t\tkfree(qp->urq.pbl_tbl);\n\t}\n\n\tif (qp->usq.db_rec_data) {\n\t\tqedr_db_recovery_del(dev, qp->usq.db_addr,\n\t\t\t\t     &qp->usq.db_rec_data->db_data);\n\t\trdma_user_mmap_entry_remove(qp->usq.db_mmap_entry);\n\t}\n\n\tif (qp->urq.db_rec_data) {\n\t\tqedr_db_recovery_del(dev, qp->urq.db_addr,\n\t\t\t\t     &qp->urq.db_rec_data->db_data);\n\t\trdma_user_mmap_entry_remove(qp->urq.db_mmap_entry);\n\t}\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\tqedr_db_recovery_del(dev, qp->urq.db_rec_db2_addr,\n\t\t\t\t     &qp->urq.db_rec_db2_data);\n}\n\nstatic int qedr_create_user_qp(struct qedr_dev *dev,\n\t\t\t       struct qedr_qp *qp,\n\t\t\t       struct ib_pd *ibpd,\n\t\t\t       struct ib_udata *udata,\n\t\t\t       struct ib_qp_init_attr *attrs)\n{\n\tstruct qed_rdma_create_qp_in_params in_params;\n\tstruct qed_rdma_create_qp_out_params out_params;\n\tstruct qedr_create_qp_uresp uresp = {};\n\tstruct qedr_create_qp_ureq ureq = {};\n\tint alloc_and_init = rdma_protocol_roce(&dev->ibdev, 1);\n\tstruct qedr_ucontext *ctx = NULL;\n\tstruct qedr_pd *pd = NULL;\n\tint rc = 0;\n\n\tqp->create_type = QEDR_QP_CREATE_USER;\n\n\tif (ibpd) {\n\t\tpd = get_qedr_pd(ibpd);\n\t\tctx = pd->uctx;\n\t}\n\n\tif (udata) {\n\t\trc = ib_copy_from_udata(&ureq, udata, min(sizeof(ureq),\n\t\t\t\t\tudata->inlen));\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"Problem copying data from user space\\n\");\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tif (qedr_qp_has_sq(qp)) {\n\t\t \n\t\trc = qedr_init_user_queue(udata, dev, &qp->usq, ureq.sq_addr,\n\t\t\t\t\t  ureq.sq_len, true, 0, alloc_and_init);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\t \n\t\trc = qedr_init_user_queue(udata, dev, &qp->urq, ureq.rq_addr,\n\t\t\t\t\t  ureq.rq_len, true, 0, alloc_and_init);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tmemset(&in_params, 0, sizeof(in_params));\n\tqedr_init_common_qp_in_params(dev, pd, qp, attrs, false, &in_params);\n\tin_params.qp_handle_lo = ureq.qp_handle_lo;\n\tin_params.qp_handle_hi = ureq.qp_handle_hi;\n\n\tif (qp->qp_type == IB_QPT_XRC_TGT) {\n\t\tstruct qedr_xrcd *xrcd = get_qedr_xrcd(attrs->xrcd);\n\n\t\tin_params.xrcd_id = xrcd->xrcd_id;\n\t\tin_params.qp_handle_lo = qp->qp_id;\n\t\tin_params.use_srq = 1;\n\t}\n\n\tif (qedr_qp_has_sq(qp)) {\n\t\tin_params.sq_num_pages = qp->usq.pbl_info.num_pbes;\n\t\tin_params.sq_pbl_ptr = qp->usq.pbl_tbl->pa;\n\t}\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\tin_params.rq_num_pages = qp->urq.pbl_info.num_pbes;\n\t\tin_params.rq_pbl_ptr = qp->urq.pbl_tbl->pa;\n\t}\n\n\tif (ctx)\n\t\tSET_FIELD(in_params.flags, QED_ROCE_EDPM_MODE, ctx->edpm_mode);\n\n\tqp->qed_qp = dev->ops->rdma_create_qp(dev->rdma_ctx,\n\t\t\t\t\t      &in_params, &out_params);\n\n\tif (!qp->qed_qp) {\n\t\trc = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\tqedr_iwarp_populate_user_qp(dev, qp, &out_params);\n\n\tqp->qp_id = out_params.qp_id;\n\tqp->icid = out_params.icid;\n\n\tif (udata) {\n\t\trc = qedr_copy_qp_uresp(dev, qp, udata, &uresp);\n\t\tif (rc)\n\t\t\tgoto err;\n\t}\n\n\t \n\tif (qedr_qp_has_sq(qp)) {\n\t\tqp->usq.db_addr = ctx->dpi_addr + uresp.sq_db_offset;\n\t\tqp->sq.max_wr = attrs->cap.max_send_wr;\n\t\trc = qedr_db_recovery_add(dev, qp->usq.db_addr,\n\t\t\t\t\t  &qp->usq.db_rec_data->db_data,\n\t\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t\t  DB_REC_USER);\n\t\tif (rc)\n\t\t\tgoto err;\n\t}\n\n\tif (qedr_qp_has_rq(qp)) {\n\t\tqp->urq.db_addr = ctx->dpi_addr + uresp.rq_db_offset;\n\t\tqp->rq.max_wr = attrs->cap.max_recv_wr;\n\t\trc = qedr_db_recovery_add(dev, qp->urq.db_addr,\n\t\t\t\t\t  &qp->urq.db_rec_data->db_data,\n\t\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t\t  DB_REC_USER);\n\t\tif (rc)\n\t\t\tgoto err;\n\t}\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\tqp->urq.db_rec_db2_addr = ctx->dpi_addr + uresp.rq_db2_offset;\n\n\t\t \n\t\tqp->urq.db_rec_db2_data.data.icid = cpu_to_le16(qp->icid);\n\t\tqp->urq.db_rec_db2_data.data.value =\n\t\t\tcpu_to_le16(DQ_TCM_IWARP_POST_RQ_CF_CMD);\n\n\t\trc = qedr_db_recovery_add(dev, qp->urq.db_rec_db2_addr,\n\t\t\t\t\t  &qp->urq.db_rec_db2_data,\n\t\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t\t  DB_REC_USER);\n\t\tif (rc)\n\t\t\tgoto err;\n\t}\n\tqedr_qp_user_print(dev, qp);\n\treturn rc;\nerr:\n\trc = dev->ops->rdma_destroy_qp(dev->rdma_ctx, qp->qed_qp);\n\tif (rc)\n\t\tDP_ERR(dev, \"create qp: fatal fault. rc=%d\", rc);\n\nerr1:\n\tqedr_cleanup_user(dev, ctx, qp);\n\treturn rc;\n}\n\nstatic int qedr_set_iwarp_db_info(struct qedr_dev *dev, struct qedr_qp *qp)\n{\n\tint rc;\n\n\tqp->sq.db = dev->db_addr +\n\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_XCM_RDMA_SQ_PROD);\n\tqp->sq.db_data.data.icid = qp->icid;\n\n\trc = qedr_db_recovery_add(dev, qp->sq.db,\n\t\t\t\t  &qp->sq.db_data,\n\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t  DB_REC_KERNEL);\n\tif (rc)\n\t\treturn rc;\n\n\tqp->rq.db = dev->db_addr +\n\t\t    DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_IWARP_RQ_PROD);\n\tqp->rq.db_data.data.icid = qp->icid;\n\tqp->rq.iwarp_db2 = dev->db_addr +\n\t\t\t   DB_ADDR_SHIFT(DQ_PWM_OFFSET_TCM_FLAGS);\n\tqp->rq.iwarp_db2_data.data.icid = qp->icid;\n\tqp->rq.iwarp_db2_data.data.value = DQ_TCM_IWARP_POST_RQ_CF_CMD;\n\n\trc = qedr_db_recovery_add(dev, qp->rq.db,\n\t\t\t\t  &qp->rq.db_data,\n\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t  DB_REC_KERNEL);\n\tif (rc)\n\t\treturn rc;\n\n\trc = qedr_db_recovery_add(dev, qp->rq.iwarp_db2,\n\t\t\t\t  &qp->rq.iwarp_db2_data,\n\t\t\t\t  DB_REC_WIDTH_32B,\n\t\t\t\t  DB_REC_KERNEL);\n\treturn rc;\n}\n\nstatic int\nqedr_roce_create_kernel_qp(struct qedr_dev *dev,\n\t\t\t   struct qedr_qp *qp,\n\t\t\t   struct qed_rdma_create_qp_in_params *in_params,\n\t\t\t   u32 n_sq_elems, u32 n_rq_elems)\n{\n\tstruct qed_rdma_create_qp_out_params out_params;\n\tstruct qed_chain_init_params params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U32,\n\t};\n\tint rc;\n\n\tparams.intended_use = QED_CHAIN_USE_TO_PRODUCE;\n\tparams.num_elems = n_sq_elems;\n\tparams.elem_size = QEDR_SQE_ELEMENT_SIZE;\n\n\trc = dev->ops->common->chain_alloc(dev->cdev, &qp->sq.pbl, &params);\n\tif (rc)\n\t\treturn rc;\n\n\tin_params->sq_num_pages = qed_chain_get_page_cnt(&qp->sq.pbl);\n\tin_params->sq_pbl_ptr = qed_chain_get_pbl_phys(&qp->sq.pbl);\n\n\tparams.intended_use = QED_CHAIN_USE_TO_CONSUME_PRODUCE;\n\tparams.num_elems = n_rq_elems;\n\tparams.elem_size = QEDR_RQE_ELEMENT_SIZE;\n\n\trc = dev->ops->common->chain_alloc(dev->cdev, &qp->rq.pbl, &params);\n\tif (rc)\n\t\treturn rc;\n\n\tin_params->rq_num_pages = qed_chain_get_page_cnt(&qp->rq.pbl);\n\tin_params->rq_pbl_ptr = qed_chain_get_pbl_phys(&qp->rq.pbl);\n\n\tqp->qed_qp = dev->ops->rdma_create_qp(dev->rdma_ctx,\n\t\t\t\t\t      in_params, &out_params);\n\n\tif (!qp->qed_qp)\n\t\treturn -EINVAL;\n\n\tqp->qp_id = out_params.qp_id;\n\tqp->icid = out_params.icid;\n\n\treturn qedr_set_roce_db_info(dev, qp);\n}\n\nstatic int\nqedr_iwarp_create_kernel_qp(struct qedr_dev *dev,\n\t\t\t    struct qedr_qp *qp,\n\t\t\t    struct qed_rdma_create_qp_in_params *in_params,\n\t\t\t    u32 n_sq_elems, u32 n_rq_elems)\n{\n\tstruct qed_rdma_create_qp_out_params out_params;\n\tstruct qed_chain_init_params params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U32,\n\t};\n\tint rc;\n\n\tin_params->sq_num_pages = QED_CHAIN_PAGE_CNT(n_sq_elems,\n\t\t\t\t\t\t     QEDR_SQE_ELEMENT_SIZE,\n\t\t\t\t\t\t     QED_CHAIN_PAGE_SIZE,\n\t\t\t\t\t\t     QED_CHAIN_MODE_PBL);\n\tin_params->rq_num_pages = QED_CHAIN_PAGE_CNT(n_rq_elems,\n\t\t\t\t\t\t     QEDR_RQE_ELEMENT_SIZE,\n\t\t\t\t\t\t     QED_CHAIN_PAGE_SIZE,\n\t\t\t\t\t\t     QED_CHAIN_MODE_PBL);\n\n\tqp->qed_qp = dev->ops->rdma_create_qp(dev->rdma_ctx,\n\t\t\t\t\t      in_params, &out_params);\n\n\tif (!qp->qed_qp)\n\t\treturn -EINVAL;\n\n\t \n\n\tparams.intended_use = QED_CHAIN_USE_TO_PRODUCE;\n\tparams.num_elems = n_sq_elems;\n\tparams.elem_size = QEDR_SQE_ELEMENT_SIZE;\n\tparams.ext_pbl_virt = out_params.sq_pbl_virt;\n\tparams.ext_pbl_phys = out_params.sq_pbl_phys;\n\n\trc = dev->ops->common->chain_alloc(dev->cdev, &qp->sq.pbl, &params);\n\tif (rc)\n\t\tgoto err;\n\n\tparams.intended_use = QED_CHAIN_USE_TO_CONSUME_PRODUCE;\n\tparams.num_elems = n_rq_elems;\n\tparams.elem_size = QEDR_RQE_ELEMENT_SIZE;\n\tparams.ext_pbl_virt = out_params.rq_pbl_virt;\n\tparams.ext_pbl_phys = out_params.rq_pbl_phys;\n\n\trc = dev->ops->common->chain_alloc(dev->cdev, &qp->rq.pbl, &params);\n\tif (rc)\n\t\tgoto err;\n\n\tqp->qp_id = out_params.qp_id;\n\tqp->icid = out_params.icid;\n\n\treturn qedr_set_iwarp_db_info(dev, qp);\n\nerr:\n\tdev->ops->rdma_destroy_qp(dev->rdma_ctx, qp->qed_qp);\n\n\treturn rc;\n}\n\nstatic void qedr_cleanup_kernel(struct qedr_dev *dev, struct qedr_qp *qp)\n{\n\tdev->ops->common->chain_free(dev->cdev, &qp->sq.pbl);\n\tkfree(qp->wqe_wr_id);\n\n\tdev->ops->common->chain_free(dev->cdev, &qp->rq.pbl);\n\tkfree(qp->rqe_wr_id);\n\n\t \n\tif (qp->qp_type == IB_QPT_GSI)\n\t\treturn;\n\n\tqedr_db_recovery_del(dev, qp->sq.db, &qp->sq.db_data);\n\n\tif (!qp->srq) {\n\t\tqedr_db_recovery_del(dev, qp->rq.db, &qp->rq.db_data);\n\n\t\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\t\tqedr_db_recovery_del(dev, qp->rq.iwarp_db2,\n\t\t\t\t\t     &qp->rq.iwarp_db2_data);\n\t}\n}\n\nstatic int qedr_create_kernel_qp(struct qedr_dev *dev,\n\t\t\t\t struct qedr_qp *qp,\n\t\t\t\t struct ib_pd *ibpd,\n\t\t\t\t struct ib_qp_init_attr *attrs)\n{\n\tstruct qed_rdma_create_qp_in_params in_params;\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\tint rc = -EINVAL;\n\tu32 n_rq_elems;\n\tu32 n_sq_elems;\n\tu32 n_sq_entries;\n\n\tmemset(&in_params, 0, sizeof(in_params));\n\tqp->create_type = QEDR_QP_CREATE_KERNEL;\n\n\t \n\tqp->sq.max_wr = min_t(u32, attrs->cap.max_send_wr * dev->wq_multiplier,\n\t\t\t      dev->attr.max_sqe);\n\n\tqp->wqe_wr_id = kcalloc(qp->sq.max_wr, sizeof(*qp->wqe_wr_id),\n\t\t\t\tGFP_KERNEL);\n\tif (!qp->wqe_wr_id) {\n\t\tDP_ERR(dev, \"create qp: failed SQ shadow memory allocation\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tin_params.qp_handle_lo = lower_32_bits((uintptr_t) qp);\n\tin_params.qp_handle_hi = upper_32_bits((uintptr_t) qp);\n\n\t \n\tqp->rq.max_wr = (u16) max_t(u32, attrs->cap.max_recv_wr, 1);\n\n\t \n\tqp->rqe_wr_id = kcalloc(qp->rq.max_wr, sizeof(*qp->rqe_wr_id),\n\t\t\t\tGFP_KERNEL);\n\tif (!qp->rqe_wr_id) {\n\t\tDP_ERR(dev,\n\t\t       \"create qp: failed RQ shadow memory allocation\\n\");\n\t\tkfree(qp->wqe_wr_id);\n\t\treturn -ENOMEM;\n\t}\n\n\tqedr_init_common_qp_in_params(dev, pd, qp, attrs, true, &in_params);\n\n\tn_sq_entries = attrs->cap.max_send_wr;\n\tn_sq_entries = min_t(u32, n_sq_entries, dev->attr.max_sqe);\n\tn_sq_entries = max_t(u32, n_sq_entries, 1);\n\tn_sq_elems = n_sq_entries * QEDR_MAX_SQE_ELEMENTS_PER_SQE;\n\n\tn_rq_elems = qp->rq.max_wr * QEDR_MAX_RQE_ELEMENTS_PER_RQE;\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\trc = qedr_iwarp_create_kernel_qp(dev, qp, &in_params,\n\t\t\t\t\t\t n_sq_elems, n_rq_elems);\n\telse\n\t\trc = qedr_roce_create_kernel_qp(dev, qp, &in_params,\n\t\t\t\t\t\tn_sq_elems, n_rq_elems);\n\tif (rc)\n\t\tqedr_cleanup_kernel(dev, qp);\n\n\treturn rc;\n}\n\nstatic int qedr_free_qp_resources(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t\t  struct ib_udata *udata)\n{\n\tstruct qedr_ucontext *ctx =\n\t\trdma_udata_to_drv_context(udata, struct qedr_ucontext,\n\t\t\t\t\t  ibucontext);\n\tint rc;\n\n\tif (qp->qp_type != IB_QPT_GSI) {\n\t\trc = dev->ops->rdma_destroy_qp(dev->rdma_ctx, qp->qed_qp);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (qp->create_type == QEDR_QP_CREATE_USER)\n\t\tqedr_cleanup_user(dev, ctx, qp);\n\telse\n\t\tqedr_cleanup_kernel(dev, qp);\n\n\treturn 0;\n}\n\nint qedr_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *attrs,\n\t\t   struct ib_udata *udata)\n{\n\tstruct qedr_xrcd *xrcd = NULL;\n\tstruct ib_pd *ibpd = ibqp->pd;\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\tstruct qedr_dev *dev = get_qedr_dev(ibqp->device);\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tint rc = 0;\n\n\tif (attrs->create_flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (attrs->qp_type == IB_QPT_XRC_TGT)\n\t\txrcd = get_qedr_xrcd(attrs->xrcd);\n\telse\n\t\tpd = get_qedr_pd(ibpd);\n\n\tDP_DEBUG(dev, QEDR_MSG_QP, \"create qp: called from %s, pd=%p\\n\",\n\t\t udata ? \"user library\" : \"kernel\", pd);\n\n\trc = qedr_check_qp_attrs(ibpd, dev, attrs, udata);\n\tif (rc)\n\t\treturn rc;\n\n\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t \"create qp: called from %s, event_handler=%p, eepd=%p sq_cq=%p, sq_icid=%d, rq_cq=%p, rq_icid=%d\\n\",\n\t\t udata ? \"user library\" : \"kernel\", attrs->event_handler, pd,\n\t\t get_qedr_cq(attrs->send_cq),\n\t\t get_qedr_cq(attrs->send_cq)->icid,\n\t\t get_qedr_cq(attrs->recv_cq),\n\t\t attrs->recv_cq ? get_qedr_cq(attrs->recv_cq)->icid : 0);\n\n\tqedr_set_common_qp_params(dev, qp, pd, attrs);\n\n\tif (attrs->qp_type == IB_QPT_GSI)\n\t\treturn qedr_create_gsi_qp(dev, attrs, qp);\n\n\tif (udata || xrcd)\n\t\trc = qedr_create_user_qp(dev, qp, ibpd, udata, attrs);\n\telse\n\t\trc = qedr_create_kernel_qp(dev, qp, ibpd, attrs);\n\n\tif (rc)\n\t\treturn rc;\n\n\tqp->ibqp.qp_num = qp->qp_id;\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\trc = xa_insert(&dev->qps, qp->qp_id, qp, GFP_KERNEL);\n\t\tif (rc)\n\t\t\tgoto out_free_qp_resources;\n\t}\n\n\treturn 0;\n\nout_free_qp_resources:\n\tqedr_free_qp_resources(dev, qp, udata);\n\treturn -EFAULT;\n}\n\nstatic enum ib_qp_state qedr_get_ibqp_state(enum qed_roce_qp_state qp_state)\n{\n\tswitch (qp_state) {\n\tcase QED_ROCE_QP_STATE_RESET:\n\t\treturn IB_QPS_RESET;\n\tcase QED_ROCE_QP_STATE_INIT:\n\t\treturn IB_QPS_INIT;\n\tcase QED_ROCE_QP_STATE_RTR:\n\t\treturn IB_QPS_RTR;\n\tcase QED_ROCE_QP_STATE_RTS:\n\t\treturn IB_QPS_RTS;\n\tcase QED_ROCE_QP_STATE_SQD:\n\t\treturn IB_QPS_SQD;\n\tcase QED_ROCE_QP_STATE_ERR:\n\t\treturn IB_QPS_ERR;\n\tcase QED_ROCE_QP_STATE_SQE:\n\t\treturn IB_QPS_SQE;\n\t}\n\treturn IB_QPS_ERR;\n}\n\nstatic enum qed_roce_qp_state qedr_get_state_from_ibqp(\n\t\t\t\t\tenum ib_qp_state qp_state)\n{\n\tswitch (qp_state) {\n\tcase IB_QPS_RESET:\n\t\treturn QED_ROCE_QP_STATE_RESET;\n\tcase IB_QPS_INIT:\n\t\treturn QED_ROCE_QP_STATE_INIT;\n\tcase IB_QPS_RTR:\n\t\treturn QED_ROCE_QP_STATE_RTR;\n\tcase IB_QPS_RTS:\n\t\treturn QED_ROCE_QP_STATE_RTS;\n\tcase IB_QPS_SQD:\n\t\treturn QED_ROCE_QP_STATE_SQD;\n\tcase IB_QPS_ERR:\n\t\treturn QED_ROCE_QP_STATE_ERR;\n\tdefault:\n\t\treturn QED_ROCE_QP_STATE_ERR;\n\t}\n}\n\nstatic int qedr_update_qp_state(struct qedr_dev *dev,\n\t\t\t\tstruct qedr_qp *qp,\n\t\t\t\tenum qed_roce_qp_state cur_state,\n\t\t\t\tenum qed_roce_qp_state new_state)\n{\n\tint status = 0;\n\n\tif (new_state == cur_state)\n\t\treturn 0;\n\n\tswitch (cur_state) {\n\tcase QED_ROCE_QP_STATE_RESET:\n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_INIT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase QED_ROCE_QP_STATE_INIT:\n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_RTR:\n\t\t\t \n\n\t\t\tif (rdma_protocol_roce(&dev->ibdev, 1)) {\n\t\t\t\twritel(qp->rq.db_data.raw, qp->rq.db);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase QED_ROCE_QP_STATE_ERR:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase QED_ROCE_QP_STATE_RTR:\n\t\t \n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_RTS:\n\t\t\tbreak;\n\t\tcase QED_ROCE_QP_STATE_ERR:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase QED_ROCE_QP_STATE_RTS:\n\t\t \n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_SQD:\n\t\t\tbreak;\n\t\tcase QED_ROCE_QP_STATE_ERR:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase QED_ROCE_QP_STATE_SQD:\n\t\t \n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_RTS:\n\t\tcase QED_ROCE_QP_STATE_ERR:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase QED_ROCE_QP_STATE_ERR:\n\t\t \n\t\tswitch (new_state) {\n\t\tcase QED_ROCE_QP_STATE_RESET:\n\t\t\tif ((qp->rq.prod != qp->rq.cons) ||\n\t\t\t    (qp->sq.prod != qp->sq.cons)) {\n\t\t\t\tDP_NOTICE(dev,\n\t\t\t\t\t  \"Error->Reset with rq/sq not empty rq.prod=%x rq.cons=%x sq.prod=%x sq.cons=%x\\n\",\n\t\t\t\t\t  qp->rq.prod, qp->rq.cons, qp->sq.prod,\n\t\t\t\t\t  qp->sq.cons);\n\t\t\t\tstatus = -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tstatus = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn status;\n}\n\nint qedr_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t   int attr_mask, struct ib_udata *udata)\n{\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tstruct qed_rdma_modify_qp_in_params qp_params = { 0 };\n\tstruct qedr_dev *dev = get_qedr_dev(&qp->dev->ibdev);\n\tconst struct ib_global_route *grh = rdma_ah_read_grh(&attr->ah_attr);\n\tenum ib_qp_state old_qp_state, new_qp_state;\n\tenum qed_roce_qp_state cur_state;\n\tint rc = 0;\n\n\tDP_DEBUG(dev, QEDR_MSG_QP,\n\t\t \"modify qp: qp %p attr_mask=0x%x, state=%d\", qp, attr_mask,\n\t\t attr->qp_state);\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\told_qp_state = qedr_get_ibqp_state(qp->state);\n\tif (attr_mask & IB_QP_STATE)\n\t\tnew_qp_state = attr->qp_state;\n\telse\n\t\tnew_qp_state = old_qp_state;\n\n\tif (rdma_protocol_roce(&dev->ibdev, 1)) {\n\t\tif (!ib_modify_qp_is_ok(old_qp_state, new_qp_state,\n\t\t\t\t\tibqp->qp_type, attr_mask)) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"modify qp: invalid attribute mask=0x%x specified for\\n\"\n\t\t\t       \"qpn=0x%x of type=0x%x old_qp_state=0x%x, new_qp_state=0x%x\\n\",\n\t\t\t       attr_mask, qp->qp_id, ibqp->qp_type,\n\t\t\t       old_qp_state, new_qp_state);\n\t\t\trc = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t \n\tif (attr_mask & IB_QP_STATE) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_RDMA_MODIFY_QP_VALID_NEW_STATE, 1);\n\t\tqp_params.new_state = qedr_get_state_from_ibqp(attr->qp_state);\n\t}\n\n\tif (attr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY)\n\t\tqp_params.sqd_async = true;\n\n\tif (attr_mask & IB_QP_PKEY_INDEX) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_PKEY, 1);\n\t\tif (attr->pkey_index >= QEDR_ROCE_PKEY_TABLE_LEN) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tqp_params.pkey = QEDR_ROCE_PKEY_DEFAULT;\n\t}\n\n\tif (attr_mask & IB_QP_QKEY)\n\t\tqp->qkey = attr->qkey;\n\n\tif (attr_mask & IB_QP_ACCESS_FLAGS) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_RDMA_MODIFY_QP_VALID_RDMA_OPS_EN, 1);\n\t\tqp_params.incoming_rdma_read_en = attr->qp_access_flags &\n\t\t\t\t\t\t  IB_ACCESS_REMOTE_READ;\n\t\tqp_params.incoming_rdma_write_en = attr->qp_access_flags &\n\t\t\t\t\t\t   IB_ACCESS_REMOTE_WRITE;\n\t\tqp_params.incoming_atomic_en = attr->qp_access_flags &\n\t\t\t\t\t       IB_ACCESS_REMOTE_ATOMIC;\n\t}\n\n\tif (attr_mask & (IB_QP_AV | IB_QP_PATH_MTU)) {\n\t\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\t\treturn -EINVAL;\n\n\t\tif (attr_mask & IB_QP_PATH_MTU) {\n\t\t\tif (attr->path_mtu < IB_MTU_256 ||\n\t\t\t    attr->path_mtu > IB_MTU_4096) {\n\t\t\t\tpr_err(\"error: Only MTU sizes of 256, 512, 1024, 2048 and 4096 are supported by RoCE\\n\");\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tqp->mtu = min(ib_mtu_enum_to_int(attr->path_mtu),\n\t\t\t\t      ib_mtu_enum_to_int(iboe_get_mtu\n\t\t\t\t\t\t\t (dev->ndev->mtu)));\n\t\t}\n\n\t\tif (!qp->mtu) {\n\t\t\tqp->mtu =\n\t\t\tib_mtu_enum_to_int(iboe_get_mtu(dev->ndev->mtu));\n\t\t\tpr_err(\"Fixing zeroed MTU to qp->mtu = %d\\n\", qp->mtu);\n\t\t}\n\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR, 1);\n\n\t\tqp_params.traffic_class_tos = grh->traffic_class;\n\t\tqp_params.flow_label = grh->flow_label;\n\t\tqp_params.hop_limit_ttl = grh->hop_limit;\n\n\t\tqp->sgid_idx = grh->sgid_index;\n\n\t\trc = get_gid_info_from_table(ibqp, attr, attr_mask, &qp_params);\n\t\tif (rc) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"modify qp: problems with GID index %d (rc=%d)\\n\",\n\t\t\t       grh->sgid_index, rc);\n\t\t\treturn rc;\n\t\t}\n\n\t\trc = qedr_get_dmac(dev, &attr->ah_attr,\n\t\t\t\t   qp_params.remote_mac_addr);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tqp_params.use_local_mac = true;\n\t\tether_addr_copy(qp_params.local_mac_addr, dev->ndev->dev_addr);\n\n\t\tDP_DEBUG(dev, QEDR_MSG_QP, \"dgid=%x:%x:%x:%x\\n\",\n\t\t\t qp_params.dgid.dwords[0], qp_params.dgid.dwords[1],\n\t\t\t qp_params.dgid.dwords[2], qp_params.dgid.dwords[3]);\n\t\tDP_DEBUG(dev, QEDR_MSG_QP, \"sgid=%x:%x:%x:%x\\n\",\n\t\t\t qp_params.sgid.dwords[0], qp_params.sgid.dwords[1],\n\t\t\t qp_params.sgid.dwords[2], qp_params.sgid.dwords[3]);\n\t\tDP_DEBUG(dev, QEDR_MSG_QP, \"remote_mac=[%pM]\\n\",\n\t\t\t qp_params.remote_mac_addr);\n\n\t\tqp_params.mtu = qp->mtu;\n\t\tqp_params.lb_indication = false;\n\t}\n\n\tif (!qp_params.mtu) {\n\t\t \n\t\tif (qp->mtu)\n\t\t\tqp_params.mtu = qp->mtu;\n\t\telse\n\t\t\tqp_params.mtu =\n\t\t\t    ib_mtu_enum_to_int(iboe_get_mtu(dev->ndev->mtu));\n\t}\n\n\tif (attr_mask & IB_QP_TIMEOUT) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_ACK_TIMEOUT, 1);\n\n\t\t \n\t\tif (attr->timeout)\n\t\t\tqp_params.ack_timeout =\n\t\t\t\t\t1 << max_t(int, attr->timeout - 8, 0);\n\t\telse\n\t\t\tqp_params.ack_timeout = 0;\n\n\t\tqp->timeout = attr->timeout;\n\t}\n\n\tif (attr_mask & IB_QP_RETRY_CNT) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_RETRY_CNT, 1);\n\t\tqp_params.retry_cnt = attr->retry_cnt;\n\t}\n\n\tif (attr_mask & IB_QP_RNR_RETRY) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_RNR_RETRY_CNT, 1);\n\t\tqp_params.rnr_retry_cnt = attr->rnr_retry;\n\t}\n\n\tif (attr_mask & IB_QP_RQ_PSN) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_RQ_PSN, 1);\n\t\tqp_params.rq_psn = attr->rq_psn;\n\t\tqp->rq_psn = attr->rq_psn;\n\t}\n\n\tif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC) {\n\t\tif (attr->max_rd_atomic > dev->attr.max_qp_req_rd_atomic_resc) {\n\t\t\trc = -EINVAL;\n\t\t\tDP_ERR(dev,\n\t\t\t       \"unsupported max_rd_atomic=%d, supported=%d\\n\",\n\t\t\t       attr->max_rd_atomic,\n\t\t\t       dev->attr.max_qp_req_rd_atomic_resc);\n\t\t\tgoto err;\n\t\t}\n\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_REQ, 1);\n\t\tqp_params.max_rd_atomic_req = attr->max_rd_atomic;\n\t}\n\n\tif (attr_mask & IB_QP_MIN_RNR_TIMER) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_MIN_RNR_NAK_TIMER, 1);\n\t\tqp_params.min_rnr_nak_timer = attr->min_rnr_timer;\n\t}\n\n\tif (attr_mask & IB_QP_SQ_PSN) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_SQ_PSN, 1);\n\t\tqp_params.sq_psn = attr->sq_psn;\n\t\tqp->sq_psn = attr->sq_psn;\n\t}\n\n\tif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) {\n\t\tif (attr->max_dest_rd_atomic >\n\t\t    dev->attr.max_qp_resp_rd_atomic_resc) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"unsupported max_dest_rd_atomic=%d, supported=%d\\n\",\n\t\t\t       attr->max_dest_rd_atomic,\n\t\t\t       dev->attr.max_qp_resp_rd_atomic_resc);\n\n\t\t\trc = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_RESP, 1);\n\t\tqp_params.max_rd_atomic_resp = attr->max_dest_rd_atomic;\n\t}\n\n\tif (attr_mask & IB_QP_DEST_QPN) {\n\t\tSET_FIELD(qp_params.modify_flags,\n\t\t\t  QED_ROCE_MODIFY_QP_VALID_DEST_QP, 1);\n\n\t\tqp_params.dest_qp = attr->dest_qp_num;\n\t\tqp->dest_qp_num = attr->dest_qp_num;\n\t}\n\n\tcur_state = qp->state;\n\n\t \n\tif ((attr_mask & IB_QP_STATE) && qp->qp_type != IB_QPT_GSI &&\n\t    !udata && qp_params.new_state == QED_ROCE_QP_STATE_ERR)\n\t\tqp->state = QED_ROCE_QP_STATE_ERR;\n\n\tif (qp->qp_type != IB_QPT_GSI)\n\t\trc = dev->ops->rdma_modify_qp(dev->rdma_ctx,\n\t\t\t\t\t      qp->qed_qp, &qp_params);\n\n\tif (attr_mask & IB_QP_STATE) {\n\t\tif ((qp->qp_type != IB_QPT_GSI) && (!udata))\n\t\t\trc = qedr_update_qp_state(dev, qp, cur_state,\n\t\t\t\t\t\t  qp_params.new_state);\n\t\tqp->state = qp_params.new_state;\n\t}\n\nerr:\n\treturn rc;\n}\n\nstatic int qedr_to_ib_qp_acc_flags(struct qed_rdma_query_qp_out_params *params)\n{\n\tint ib_qp_acc_flags = 0;\n\n\tif (params->incoming_rdma_write_en)\n\t\tib_qp_acc_flags |= IB_ACCESS_REMOTE_WRITE;\n\tif (params->incoming_rdma_read_en)\n\t\tib_qp_acc_flags |= IB_ACCESS_REMOTE_READ;\n\tif (params->incoming_atomic_en)\n\t\tib_qp_acc_flags |= IB_ACCESS_REMOTE_ATOMIC;\n\tib_qp_acc_flags |= IB_ACCESS_LOCAL_WRITE;\n\treturn ib_qp_acc_flags;\n}\n\nint qedr_query_qp(struct ib_qp *ibqp,\n\t\t  struct ib_qp_attr *qp_attr,\n\t\t  int attr_mask, struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct qed_rdma_query_qp_out_params params;\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tstruct qedr_dev *dev = qp->dev;\n\tint rc = 0;\n\n\tmemset(&params, 0, sizeof(params));\n\tmemset(qp_attr, 0, sizeof(*qp_attr));\n\tmemset(qp_init_attr, 0, sizeof(*qp_init_attr));\n\n\tif (qp->qp_type != IB_QPT_GSI) {\n\t\trc = dev->ops->rdma_query_qp(dev->rdma_ctx, qp->qed_qp, &params);\n\t\tif (rc)\n\t\t\tgoto err;\n\t\tqp_attr->qp_state = qedr_get_ibqp_state(params.state);\n\t} else {\n\t\tqp_attr->qp_state = qedr_get_ibqp_state(QED_ROCE_QP_STATE_RTS);\n\t}\n\n\tqp_attr->cur_qp_state = qedr_get_ibqp_state(params.state);\n\tqp_attr->path_mtu = ib_mtu_int_to_enum(params.mtu);\n\tqp_attr->path_mig_state = IB_MIG_MIGRATED;\n\tqp_attr->rq_psn = params.rq_psn;\n\tqp_attr->sq_psn = params.sq_psn;\n\tqp_attr->dest_qp_num = params.dest_qp;\n\n\tqp_attr->qp_access_flags = qedr_to_ib_qp_acc_flags(&params);\n\n\tqp_attr->cap.max_send_wr = qp->sq.max_wr;\n\tqp_attr->cap.max_recv_wr = qp->rq.max_wr;\n\tqp_attr->cap.max_send_sge = qp->sq.max_sges;\n\tqp_attr->cap.max_recv_sge = qp->rq.max_sges;\n\tqp_attr->cap.max_inline_data = dev->attr.max_inline;\n\tqp_init_attr->cap = qp_attr->cap;\n\n\tqp_attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\trdma_ah_set_grh(&qp_attr->ah_attr, NULL,\n\t\t\tparams.flow_label, qp->sgid_idx,\n\t\t\tparams.hop_limit_ttl, params.traffic_class_tos);\n\trdma_ah_set_dgid_raw(&qp_attr->ah_attr, &params.dgid.bytes[0]);\n\trdma_ah_set_port_num(&qp_attr->ah_attr, 1);\n\trdma_ah_set_sl(&qp_attr->ah_attr, 0);\n\tqp_attr->timeout = qp->timeout;\n\tqp_attr->rnr_retry = params.rnr_retry;\n\tqp_attr->retry_cnt = params.retry_cnt;\n\tqp_attr->min_rnr_timer = params.min_rnr_nak_timer;\n\tqp_attr->pkey_index = params.pkey_index;\n\tqp_attr->port_num = 1;\n\trdma_ah_set_path_bits(&qp_attr->ah_attr, 0);\n\trdma_ah_set_static_rate(&qp_attr->ah_attr, 0);\n\tqp_attr->alt_pkey_index = 0;\n\tqp_attr->alt_port_num = 0;\n\tqp_attr->alt_timeout = 0;\n\tmemset(&qp_attr->alt_ah_attr, 0, sizeof(qp_attr->alt_ah_attr));\n\n\tqp_attr->sq_draining = (params.state == QED_ROCE_QP_STATE_SQD) ? 1 : 0;\n\tqp_attr->max_dest_rd_atomic = params.max_dest_rd_atomic;\n\tqp_attr->max_rd_atomic = params.max_rd_atomic;\n\tqp_attr->en_sqd_async_notify = (params.sqd_async) ? 1 : 0;\n\n\tDP_DEBUG(dev, QEDR_MSG_QP, \"QEDR_QUERY_QP: max_inline_data=%d\\n\",\n\t\t qp_attr->cap.max_inline_data);\n\nerr:\n\treturn rc;\n}\n\nint qedr_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tstruct qedr_dev *dev = qp->dev;\n\tstruct ib_qp_attr attr;\n\tint attr_mask = 0;\n\n\tDP_DEBUG(dev, QEDR_MSG_QP, \"destroy qp: destroying %p, qp type=%d\\n\",\n\t\t qp, qp->qp_type);\n\n\tif (rdma_protocol_roce(&dev->ibdev, 1)) {\n\t\tif ((qp->state != QED_ROCE_QP_STATE_RESET) &&\n\t\t    (qp->state != QED_ROCE_QP_STATE_ERR) &&\n\t\t    (qp->state != QED_ROCE_QP_STATE_INIT)) {\n\n\t\t\tattr.qp_state = IB_QPS_ERR;\n\t\t\tattr_mask |= IB_QP_STATE;\n\n\t\t\t \n\t\t\tqedr_modify_qp(ibqp, &attr, attr_mask, NULL);\n\t\t}\n\t} else {\n\t\t \n\t\tif (test_and_set_bit(QEDR_IWARP_CM_WAIT_FOR_CONNECT,\n\t\t\t\t     &qp->iwarp_cm_flags))\n\t\t\twait_for_completion(&qp->iwarp_cm_comp);\n\n\t\t \n\t\tif (test_and_set_bit(QEDR_IWARP_CM_WAIT_FOR_DISCONNECT,\n\t\t\t\t     &qp->iwarp_cm_flags))\n\t\t\twait_for_completion(&qp->iwarp_cm_comp);\n\t}\n\n\tif (qp->qp_type == IB_QPT_GSI)\n\t\tqedr_destroy_gsi_qp(dev);\n\n\t \n\tif (rdma_protocol_iwarp(&dev->ibdev, 1))\n\t\txa_erase(&dev->qps, qp->qp_id);\n\n\tqedr_free_qp_resources(dev, qp, udata);\n\n\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\tqedr_iw_qp_rem_ref(&qp->ibqp);\n\t\twait_for_completion(&qp->qp_rel_comp);\n\t}\n\n\treturn 0;\n}\n\nint qedr_create_ah(struct ib_ah *ibah, struct rdma_ah_init_attr *init_attr,\n\t\t   struct ib_udata *udata)\n{\n\tstruct qedr_ah *ah = get_qedr_ah(ibah);\n\n\trdma_copy_ah_attr(&ah->attr, init_attr->ah_attr);\n\n\treturn 0;\n}\n\nint qedr_destroy_ah(struct ib_ah *ibah, u32 flags)\n{\n\tstruct qedr_ah *ah = get_qedr_ah(ibah);\n\n\trdma_destroy_ah_attr(&ah->attr);\n\treturn 0;\n}\n\nstatic void free_mr_info(struct qedr_dev *dev, struct mr_info *info)\n{\n\tstruct qedr_pbl *pbl, *tmp;\n\n\tif (info->pbl_table)\n\t\tlist_add_tail(&info->pbl_table->list_entry,\n\t\t\t      &info->free_pbl_list);\n\n\tif (!list_empty(&info->inuse_pbl_list))\n\t\tlist_splice(&info->inuse_pbl_list, &info->free_pbl_list);\n\n\tlist_for_each_entry_safe(pbl, tmp, &info->free_pbl_list, list_entry) {\n\t\tlist_del(&pbl->list_entry);\n\t\tqedr_free_pbl(dev, &info->pbl_info, pbl);\n\t}\n}\n\nstatic int init_mr_info(struct qedr_dev *dev, struct mr_info *info,\n\t\t\tsize_t page_list_len, bool two_layered)\n{\n\tstruct qedr_pbl *tmp;\n\tint rc;\n\n\tINIT_LIST_HEAD(&info->free_pbl_list);\n\tINIT_LIST_HEAD(&info->inuse_pbl_list);\n\n\trc = qedr_prepare_pbl_tbl(dev, &info->pbl_info,\n\t\t\t\t  page_list_len, two_layered);\n\tif (rc)\n\t\tgoto done;\n\n\tinfo->pbl_table = qedr_alloc_pbl_tbl(dev, &info->pbl_info, GFP_KERNEL);\n\tif (IS_ERR(info->pbl_table)) {\n\t\trc = PTR_ERR(info->pbl_table);\n\t\tgoto done;\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"pbl_table_pa = %pa\\n\",\n\t\t &info->pbl_table->pa);\n\n\t \n\ttmp = qedr_alloc_pbl_tbl(dev, &info->pbl_info, GFP_KERNEL);\n\tif (IS_ERR(tmp)) {\n\t\tDP_DEBUG(dev, QEDR_MSG_MR, \"Extra PBL is not allocated\\n\");\n\t\tgoto done;\n\t}\n\n\tlist_add_tail(&tmp->list_entry, &info->free_pbl_list);\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"extra pbl_table_pa = %pa\\n\", &tmp->pa);\n\ndone:\n\tif (rc)\n\t\tfree_mr_info(dev, info);\n\n\treturn rc;\n}\n\nstruct ib_mr *qedr_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 len,\n\t\t\t       u64 usr_addr, int acc, struct ib_udata *udata)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibpd->device);\n\tstruct qedr_mr *mr;\n\tstruct qedr_pd *pd;\n\tint rc = -ENOMEM;\n\n\tpd = get_qedr_pd(ibpd);\n\tDP_DEBUG(dev, QEDR_MSG_MR,\n\t\t \"qedr_register user mr pd = %d start = %lld, len = %lld, usr_addr = %lld, acc = %d\\n\",\n\t\t pd->pd_id, start, len, usr_addr, acc);\n\n\tif (acc & IB_ACCESS_REMOTE_WRITE && !(acc & IB_ACCESS_LOCAL_WRITE))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(rc);\n\n\tmr->type = QEDR_MR_USER;\n\n\tmr->umem = ib_umem_get(ibpd->device, start, len, acc);\n\tif (IS_ERR(mr->umem)) {\n\t\trc = -EFAULT;\n\t\tgoto err0;\n\t}\n\n\trc = init_mr_info(dev, &mr->info,\n\t\t\t  ib_umem_num_dma_blocks(mr->umem, PAGE_SIZE), 1);\n\tif (rc)\n\t\tgoto err1;\n\n\tqedr_populate_pbls(dev, mr->umem, mr->info.pbl_table,\n\t\t\t   &mr->info.pbl_info, PAGE_SHIFT);\n\n\trc = dev->ops->rdma_alloc_tid(dev->rdma_ctx, &mr->hw_mr.itid);\n\tif (rc) {\n\t\tif (rc == -EINVAL)\n\t\t\tDP_ERR(dev, \"Out of MR resources\\n\");\n\t\telse\n\t\t\tDP_ERR(dev, \"roce alloc tid returned error %d\\n\", rc);\n\n\t\tgoto err1;\n\t}\n\n\t \n\tmr->hw_mr.tid_type = QED_RDMA_TID_REGISTERED_MR;\n\tmr->hw_mr.key = 0;\n\tmr->hw_mr.pd = pd->pd_id;\n\tmr->hw_mr.local_read = 1;\n\tmr->hw_mr.local_write = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\n\tmr->hw_mr.remote_read = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\n\tmr->hw_mr.remote_write = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\n\tmr->hw_mr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\n\tmr->hw_mr.mw_bind = false;\n\tmr->hw_mr.pbl_ptr = mr->info.pbl_table[0].pa;\n\tmr->hw_mr.pbl_two_level = mr->info.pbl_info.two_layered;\n\tmr->hw_mr.pbl_page_size_log = ilog2(mr->info.pbl_info.pbl_size);\n\tmr->hw_mr.page_size_log = PAGE_SHIFT;\n\tmr->hw_mr.length = len;\n\tmr->hw_mr.vaddr = usr_addr;\n\tmr->hw_mr.phy_mr = false;\n\tmr->hw_mr.dma_mr = false;\n\n\trc = dev->ops->rdma_register_tid(dev->rdma_ctx, &mr->hw_mr);\n\tif (rc) {\n\t\tDP_ERR(dev, \"roce register tid returned an error %d\\n\", rc);\n\t\tgoto err2;\n\t}\n\n\tmr->ibmr.lkey = mr->hw_mr.itid << 8 | mr->hw_mr.key;\n\tif (mr->hw_mr.remote_write || mr->hw_mr.remote_read ||\n\t    mr->hw_mr.remote_atomic)\n\t\tmr->ibmr.rkey = mr->hw_mr.itid << 8 | mr->hw_mr.key;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"register user mr lkey: %x\\n\",\n\t\t mr->ibmr.lkey);\n\treturn &mr->ibmr;\n\nerr2:\n\tdev->ops->rdma_free_tid(dev->rdma_ctx, mr->hw_mr.itid);\nerr1:\n\tqedr_free_pbl(dev, &mr->info.pbl_info, mr->info.pbl_table);\nerr0:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nint qedr_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata)\n{\n\tstruct qedr_mr *mr = get_qedr_mr(ib_mr);\n\tstruct qedr_dev *dev = get_qedr_dev(ib_mr->device);\n\tint rc = 0;\n\n\trc = dev->ops->rdma_deregister_tid(dev->rdma_ctx, mr->hw_mr.itid);\n\tif (rc)\n\t\treturn rc;\n\n\tdev->ops->rdma_free_tid(dev->rdma_ctx, mr->hw_mr.itid);\n\n\tif (mr->type != QEDR_MR_DMA)\n\t\tfree_mr_info(dev, &mr->info);\n\n\t \n\tib_umem_release(mr->umem);\n\n\tkfree(mr);\n\n\treturn rc;\n}\n\nstatic struct qedr_mr *__qedr_alloc_mr(struct ib_pd *ibpd,\n\t\t\t\t       int max_page_list_len)\n{\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\tstruct qedr_dev *dev = get_qedr_dev(ibpd->device);\n\tstruct qedr_mr *mr;\n\tint rc = -ENOMEM;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR,\n\t\t \"qedr_alloc_frmr pd = %d max_page_list_len= %d\\n\", pd->pd_id,\n\t\t max_page_list_len);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(rc);\n\n\tmr->dev = dev;\n\tmr->type = QEDR_MR_FRMR;\n\n\trc = init_mr_info(dev, &mr->info, max_page_list_len, 1);\n\tif (rc)\n\t\tgoto err0;\n\n\trc = dev->ops->rdma_alloc_tid(dev->rdma_ctx, &mr->hw_mr.itid);\n\tif (rc) {\n\t\tif (rc == -EINVAL)\n\t\t\tDP_ERR(dev, \"Out of MR resources\\n\");\n\t\telse\n\t\t\tDP_ERR(dev, \"roce alloc tid returned error %d\\n\", rc);\n\n\t\tgoto err1;\n\t}\n\n\t \n\tmr->hw_mr.tid_type = QED_RDMA_TID_FMR;\n\tmr->hw_mr.key = 0;\n\tmr->hw_mr.pd = pd->pd_id;\n\tmr->hw_mr.local_read = 1;\n\tmr->hw_mr.local_write = 0;\n\tmr->hw_mr.remote_read = 0;\n\tmr->hw_mr.remote_write = 0;\n\tmr->hw_mr.remote_atomic = 0;\n\tmr->hw_mr.mw_bind = false;\n\tmr->hw_mr.pbl_ptr = 0;\n\tmr->hw_mr.pbl_two_level = mr->info.pbl_info.two_layered;\n\tmr->hw_mr.pbl_page_size_log = ilog2(mr->info.pbl_info.pbl_size);\n\tmr->hw_mr.length = 0;\n\tmr->hw_mr.vaddr = 0;\n\tmr->hw_mr.phy_mr = true;\n\tmr->hw_mr.dma_mr = false;\n\n\trc = dev->ops->rdma_register_tid(dev->rdma_ctx, &mr->hw_mr);\n\tif (rc) {\n\t\tDP_ERR(dev, \"roce register tid returned an error %d\\n\", rc);\n\t\tgoto err2;\n\t}\n\n\tmr->ibmr.lkey = mr->hw_mr.itid << 8 | mr->hw_mr.key;\n\tmr->ibmr.rkey = mr->ibmr.lkey;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"alloc frmr: %x\\n\", mr->ibmr.lkey);\n\treturn mr;\n\nerr2:\n\tdev->ops->rdma_free_tid(dev->rdma_ctx, mr->hw_mr.itid);\nerr1:\n\tqedr_free_pbl(dev, &mr->info.pbl_info, mr->info.pbl_table);\nerr0:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nstruct ib_mr *qedr_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,\n\t\t\t    u32 max_num_sg)\n{\n\tstruct qedr_mr *mr;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = __qedr_alloc_mr(ibpd, max_num_sg);\n\n\tif (IS_ERR(mr))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn &mr->ibmr;\n}\n\nstatic int qedr_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct qedr_mr *mr = get_qedr_mr(ibmr);\n\tstruct qedr_pbl *pbl_table;\n\tstruct regpair *pbe;\n\tu32 pbes_in_page;\n\n\tif (unlikely(mr->npages == mr->info.pbl_info.num_pbes)) {\n\t\tDP_ERR(mr->dev, \"qedr_set_page fails when %d\\n\", mr->npages);\n\t\treturn -ENOMEM;\n\t}\n\n\tDP_DEBUG(mr->dev, QEDR_MSG_MR, \"qedr_set_page pages[%d] = 0x%llx\\n\",\n\t\t mr->npages, addr);\n\n\tpbes_in_page = mr->info.pbl_info.pbl_size / sizeof(u64);\n\tpbl_table = mr->info.pbl_table + (mr->npages / pbes_in_page);\n\tpbe = (struct regpair *)pbl_table->va;\n\tpbe +=  mr->npages % pbes_in_page;\n\tpbe->lo = cpu_to_le32((u32)addr);\n\tpbe->hi = cpu_to_le32((u32)upper_32_bits(addr));\n\n\tmr->npages++;\n\n\treturn 0;\n}\n\nstatic void handle_completed_mrs(struct qedr_dev *dev, struct mr_info *info)\n{\n\tint work = info->completed - info->completed_handled - 1;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"Special FMR work = %d\\n\", work);\n\twhile (work-- > 0 && !list_empty(&info->inuse_pbl_list)) {\n\t\tstruct qedr_pbl *pbl;\n\n\t\t \n\t\tpbl = list_first_entry(&info->inuse_pbl_list,\n\t\t\t\t       struct qedr_pbl, list_entry);\n\t\tlist_move_tail(&pbl->list_entry, &info->free_pbl_list);\n\t\tinfo->completed_handled++;\n\t}\n}\n\nint qedr_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,\n\t\t   int sg_nents, unsigned int *sg_offset)\n{\n\tstruct qedr_mr *mr = get_qedr_mr(ibmr);\n\n\tmr->npages = 0;\n\n\thandle_completed_mrs(mr->dev, &mr->info);\n\treturn ib_sg_to_pages(ibmr, sg, sg_nents, NULL, qedr_set_page);\n}\n\nstruct ib_mr *qedr_get_dma_mr(struct ib_pd *ibpd, int acc)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibpd->device);\n\tstruct qedr_pd *pd = get_qedr_pd(ibpd);\n\tstruct qedr_mr *mr;\n\tint rc;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmr->type = QEDR_MR_DMA;\n\n\trc = dev->ops->rdma_alloc_tid(dev->rdma_ctx, &mr->hw_mr.itid);\n\tif (rc) {\n\t\tif (rc == -EINVAL)\n\t\t\tDP_ERR(dev, \"Out of MR resources\\n\");\n\t\telse\n\t\t\tDP_ERR(dev, \"roce alloc tid returned error %d\\n\", rc);\n\n\t\tgoto err1;\n\t}\n\n\t \n\tmr->hw_mr.tid_type = QED_RDMA_TID_REGISTERED_MR;\n\tmr->hw_mr.pd = pd->pd_id;\n\tmr->hw_mr.local_read = 1;\n\tmr->hw_mr.local_write = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\n\tmr->hw_mr.remote_read = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\n\tmr->hw_mr.remote_write = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\n\tmr->hw_mr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\n\tmr->hw_mr.dma_mr = true;\n\n\trc = dev->ops->rdma_register_tid(dev->rdma_ctx, &mr->hw_mr);\n\tif (rc) {\n\t\tDP_ERR(dev, \"roce register tid returned an error %d\\n\", rc);\n\t\tgoto err2;\n\t}\n\n\tmr->ibmr.lkey = mr->hw_mr.itid << 8 | mr->hw_mr.key;\n\tif (mr->hw_mr.remote_write || mr->hw_mr.remote_read ||\n\t    mr->hw_mr.remote_atomic)\n\t\tmr->ibmr.rkey = mr->hw_mr.itid << 8 | mr->hw_mr.key;\n\n\tDP_DEBUG(dev, QEDR_MSG_MR, \"get dma mr: lkey = %x\\n\", mr->ibmr.lkey);\n\treturn &mr->ibmr;\n\nerr2:\n\tdev->ops->rdma_free_tid(dev->rdma_ctx, mr->hw_mr.itid);\nerr1:\n\tkfree(mr);\n\treturn ERR_PTR(rc);\n}\n\nstatic inline int qedr_wq_is_full(struct qedr_qp_hwq_info *wq)\n{\n\treturn (((wq->prod + 1) % wq->max_wr) == wq->cons);\n}\n\nstatic int sge_data_len(struct ib_sge *sg_list, int num_sge)\n{\n\tint i, len = 0;\n\n\tfor (i = 0; i < num_sge; i++)\n\t\tlen += sg_list[i].length;\n\n\treturn len;\n}\n\nstatic void swap_wqe_data64(u64 *p)\n{\n\tint i;\n\n\tfor (i = 0; i < QEDR_SQE_ELEMENT_SIZE / sizeof(u64); i++, p++)\n\t\t*p = cpu_to_be64(cpu_to_le64(*p));\n}\n\nstatic u32 qedr_prepare_sq_inline_data(struct qedr_dev *dev,\n\t\t\t\t       struct qedr_qp *qp, u8 *wqe_size,\n\t\t\t\t       const struct ib_send_wr *wr,\n\t\t\t\t       const struct ib_send_wr **bad_wr,\n\t\t\t\t       u8 *bits, u8 bit)\n{\n\tu32 data_size = sge_data_len(wr->sg_list, wr->num_sge);\n\tchar *seg_prt, *wqe;\n\tint i, seg_siz;\n\n\tif (data_size > ROCE_REQ_MAX_INLINE_DATA_SIZE) {\n\t\tDP_ERR(dev, \"Too much inline data in WR: %d\\n\", data_size);\n\t\t*bad_wr = wr;\n\t\treturn 0;\n\t}\n\n\tif (!data_size)\n\t\treturn data_size;\n\n\t*bits |= bit;\n\n\tseg_prt = NULL;\n\twqe = NULL;\n\tseg_siz = 0;\n\n\t \n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tu32 len = wr->sg_list[i].length;\n\t\tvoid *src = (void *)(uintptr_t)wr->sg_list[i].addr;\n\n\t\twhile (len > 0) {\n\t\t\tu32 cur;\n\n\t\t\t \n\t\t\tif (!seg_siz) {\n\t\t\t\twqe = (char *)qed_chain_produce(&qp->sq.pbl);\n\t\t\t\tseg_prt = wqe;\n\t\t\t\tseg_siz = sizeof(struct rdma_sq_common_wqe);\n\t\t\t\t(*wqe_size)++;\n\t\t\t}\n\n\t\t\t \n\t\t\tcur = min_t(u32, len, seg_siz);\n\t\t\tmemcpy(seg_prt, src, cur);\n\n\t\t\t \n\t\t\tseg_prt += cur;\n\t\t\tseg_siz -= cur;\n\n\t\t\t \n\t\t\tsrc += cur;\n\t\t\tlen -= cur;\n\n\t\t\t \n\t\t\tif (!seg_siz)\n\t\t\t\tswap_wqe_data64((u64 *)wqe);\n\t\t}\n\t}\n\n\t \n\tif (seg_siz)\n\t\tswap_wqe_data64((u64 *)wqe);\n\n\treturn data_size;\n}\n\n#define RQ_SGE_SET(sge, vaddr, vlength, vflags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tDMA_REGPAIR_LE(sge->addr, vaddr);\t\t\\\n\t\t(sge)->length = cpu_to_le32(vlength);\t\t\\\n\t\t(sge)->flags = cpu_to_le32(vflags);\t\t\\\n\t} while (0)\n\n#define SRQ_HDR_SET(hdr, vwr_id, num_sge)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tDMA_REGPAIR_LE(hdr->wr_id, vwr_id);\t\t\\\n\t\t(hdr)->num_sges = num_sge;\t\t\t\\\n\t} while (0)\n\n#define SRQ_SGE_SET(sge, vaddr, vlength, vlkey)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tDMA_REGPAIR_LE(sge->addr, vaddr);\t\t\\\n\t\t(sge)->length = cpu_to_le32(vlength);\t\t\\\n\t\t(sge)->l_key = cpu_to_le32(vlkey);\t\t\\\n\t} while (0)\n\nstatic u32 qedr_prepare_sq_sges(struct qedr_qp *qp, u8 *wqe_size,\n\t\t\t\tconst struct ib_send_wr *wr)\n{\n\tu32 data_size = 0;\n\tint i;\n\n\tfor (i = 0; i < wr->num_sge; i++) {\n\t\tstruct rdma_sq_sge *sge = qed_chain_produce(&qp->sq.pbl);\n\n\t\tDMA_REGPAIR_LE(sge->addr, wr->sg_list[i].addr);\n\t\tsge->l_key = cpu_to_le32(wr->sg_list[i].lkey);\n\t\tsge->length = cpu_to_le32(wr->sg_list[i].length);\n\t\tdata_size += wr->sg_list[i].length;\n\t}\n\n\tif (wqe_size)\n\t\t*wqe_size += wr->num_sge;\n\n\treturn data_size;\n}\n\nstatic u32 qedr_prepare_sq_rdma_data(struct qedr_dev *dev,\n\t\t\t\t     struct qedr_qp *qp,\n\t\t\t\t     struct rdma_sq_rdma_wqe_1st *rwqe,\n\t\t\t\t     struct rdma_sq_rdma_wqe_2nd *rwqe2,\n\t\t\t\t     const struct ib_send_wr *wr,\n\t\t\t\t     const struct ib_send_wr **bad_wr)\n{\n\trwqe2->r_key = cpu_to_le32(rdma_wr(wr)->rkey);\n\tDMA_REGPAIR_LE(rwqe2->remote_va, rdma_wr(wr)->remote_addr);\n\n\tif (wr->send_flags & IB_SEND_INLINE &&\n\t    (wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM ||\n\t     wr->opcode == IB_WR_RDMA_WRITE)) {\n\t\tu8 flags = 0;\n\n\t\tSET_FIELD2(flags, RDMA_SQ_RDMA_WQE_1ST_INLINE_FLG, 1);\n\t\treturn qedr_prepare_sq_inline_data(dev, qp, &rwqe->wqe_size, wr,\n\t\t\t\t\t\t   bad_wr, &rwqe->flags, flags);\n\t}\n\n\treturn qedr_prepare_sq_sges(qp, &rwqe->wqe_size, wr);\n}\n\nstatic u32 qedr_prepare_sq_send_data(struct qedr_dev *dev,\n\t\t\t\t     struct qedr_qp *qp,\n\t\t\t\t     struct rdma_sq_send_wqe_1st *swqe,\n\t\t\t\t     struct rdma_sq_send_wqe_2st *swqe2,\n\t\t\t\t     const struct ib_send_wr *wr,\n\t\t\t\t     const struct ib_send_wr **bad_wr)\n{\n\tmemset(swqe2, 0, sizeof(*swqe2));\n\tif (wr->send_flags & IB_SEND_INLINE) {\n\t\tu8 flags = 0;\n\n\t\tSET_FIELD2(flags, RDMA_SQ_SEND_WQE_INLINE_FLG, 1);\n\t\treturn qedr_prepare_sq_inline_data(dev, qp, &swqe->wqe_size, wr,\n\t\t\t\t\t\t   bad_wr, &swqe->flags, flags);\n\t}\n\n\treturn qedr_prepare_sq_sges(qp, &swqe->wqe_size, wr);\n}\n\nstatic int qedr_prepare_reg(struct qedr_qp *qp,\n\t\t\t    struct rdma_sq_fmr_wqe_1st *fwqe1,\n\t\t\t    const struct ib_reg_wr *wr)\n{\n\tstruct qedr_mr *mr = get_qedr_mr(wr->mr);\n\tstruct rdma_sq_fmr_wqe_2nd *fwqe2;\n\n\tfwqe2 = (struct rdma_sq_fmr_wqe_2nd *)qed_chain_produce(&qp->sq.pbl);\n\tfwqe1->addr.hi = upper_32_bits(mr->ibmr.iova);\n\tfwqe1->addr.lo = lower_32_bits(mr->ibmr.iova);\n\tfwqe1->l_key = wr->key;\n\n\tfwqe2->access_ctrl = 0;\n\n\tSET_FIELD2(fwqe2->access_ctrl, RDMA_SQ_FMR_WQE_2ND_REMOTE_READ,\n\t\t   !!(wr->access & IB_ACCESS_REMOTE_READ));\n\tSET_FIELD2(fwqe2->access_ctrl, RDMA_SQ_FMR_WQE_2ND_REMOTE_WRITE,\n\t\t   !!(wr->access & IB_ACCESS_REMOTE_WRITE));\n\tSET_FIELD2(fwqe2->access_ctrl, RDMA_SQ_FMR_WQE_2ND_ENABLE_ATOMIC,\n\t\t   !!(wr->access & IB_ACCESS_REMOTE_ATOMIC));\n\tSET_FIELD2(fwqe2->access_ctrl, RDMA_SQ_FMR_WQE_2ND_LOCAL_READ, 1);\n\tSET_FIELD2(fwqe2->access_ctrl, RDMA_SQ_FMR_WQE_2ND_LOCAL_WRITE,\n\t\t   !!(wr->access & IB_ACCESS_LOCAL_WRITE));\n\tfwqe2->fmr_ctrl = 0;\n\n\tSET_FIELD2(fwqe2->fmr_ctrl, RDMA_SQ_FMR_WQE_2ND_PAGE_SIZE_LOG,\n\t\t   ilog2(mr->ibmr.page_size) - 12);\n\n\tfwqe2->length_hi = 0;\n\tfwqe2->length_lo = mr->ibmr.length;\n\tfwqe2->pbl_addr.hi = upper_32_bits(mr->info.pbl_table->pa);\n\tfwqe2->pbl_addr.lo = lower_32_bits(mr->info.pbl_table->pa);\n\n\tqp->wqe_wr_id[qp->sq.prod].mr = mr;\n\n\treturn 0;\n}\n\nstatic enum ib_wc_opcode qedr_ib_to_wc_opcode(enum ib_wr_opcode opcode)\n{\n\tswitch (opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\treturn IB_WC_RDMA_WRITE;\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_SEND:\n\tcase IB_WR_SEND_WITH_INV:\n\t\treturn IB_WC_SEND;\n\tcase IB_WR_RDMA_READ:\n\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\treturn IB_WC_RDMA_READ;\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\treturn IB_WC_COMP_SWAP;\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\treturn IB_WC_FETCH_ADD;\n\tcase IB_WR_REG_MR:\n\t\treturn IB_WC_REG_MR;\n\tcase IB_WR_LOCAL_INV:\n\t\treturn IB_WC_LOCAL_INV;\n\tdefault:\n\t\treturn IB_WC_SEND;\n\t}\n}\n\nstatic inline bool qedr_can_post_send(struct qedr_qp *qp,\n\t\t\t\t      const struct ib_send_wr *wr)\n{\n\tint wq_is_full, err_wr, pbl_is_full;\n\tstruct qedr_dev *dev = qp->dev;\n\n\t \n\terr_wr = wr->num_sge > qp->sq.max_sges;\n\twq_is_full = qedr_wq_is_full(&qp->sq);\n\tpbl_is_full = qed_chain_get_elem_left_u32(&qp->sq.pbl) <\n\t\t      QEDR_MAX_SQE_ELEMENTS_PER_SQE;\n\tif (wq_is_full || err_wr || pbl_is_full) {\n\t\tif (wq_is_full && !(qp->err_bitmap & QEDR_QP_ERR_SQ_FULL)) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"error: WQ is full. Post send on QP %p failed (this error appears only once)\\n\",\n\t\t\t       qp);\n\t\t\tqp->err_bitmap |= QEDR_QP_ERR_SQ_FULL;\n\t\t}\n\n\t\tif (err_wr && !(qp->err_bitmap & QEDR_QP_ERR_BAD_SR)) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"error: WR is bad. Post send on QP %p failed (this error appears only once)\\n\",\n\t\t\t       qp);\n\t\t\tqp->err_bitmap |= QEDR_QP_ERR_BAD_SR;\n\t\t}\n\n\t\tif (pbl_is_full &&\n\t\t    !(qp->err_bitmap & QEDR_QP_ERR_SQ_PBL_FULL)) {\n\t\t\tDP_ERR(dev,\n\t\t\t       \"error: WQ PBL is full. Post send on QP %p failed (this error appears only once)\\n\",\n\t\t\t       qp);\n\t\t\tqp->err_bitmap |= QEDR_QP_ERR_SQ_PBL_FULL;\n\t\t}\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int __qedr_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t\t    const struct ib_send_wr **bad_wr)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibqp->device);\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tstruct rdma_sq_atomic_wqe_1st *awqe1;\n\tstruct rdma_sq_atomic_wqe_2nd *awqe2;\n\tstruct rdma_sq_atomic_wqe_3rd *awqe3;\n\tstruct rdma_sq_send_wqe_2st *swqe2;\n\tstruct rdma_sq_local_inv_wqe *iwqe;\n\tstruct rdma_sq_rdma_wqe_2nd *rwqe2;\n\tstruct rdma_sq_send_wqe_1st *swqe;\n\tstruct rdma_sq_rdma_wqe_1st *rwqe;\n\tstruct rdma_sq_fmr_wqe_1st *fwqe1;\n\tstruct rdma_sq_common_wqe *wqe;\n\tu32 length;\n\tint rc = 0;\n\tbool comp;\n\n\tif (!qedr_can_post_send(qp, wr)) {\n\t\t*bad_wr = wr;\n\t\treturn -ENOMEM;\n\t}\n\n\twqe = qed_chain_produce(&qp->sq.pbl);\n\tqp->wqe_wr_id[qp->sq.prod].signaled =\n\t\t!!(wr->send_flags & IB_SEND_SIGNALED) || qp->signaled;\n\n\twqe->flags = 0;\n\tSET_FIELD2(wqe->flags, RDMA_SQ_SEND_WQE_SE_FLG,\n\t\t   !!(wr->send_flags & IB_SEND_SOLICITED));\n\tcomp = (!!(wr->send_flags & IB_SEND_SIGNALED)) || qp->signaled;\n\tSET_FIELD2(wqe->flags, RDMA_SQ_SEND_WQE_COMP_FLG, comp);\n\tSET_FIELD2(wqe->flags, RDMA_SQ_SEND_WQE_RD_FENCE_FLG,\n\t\t   !!(wr->send_flags & IB_SEND_FENCE));\n\twqe->prev_wqe_size = qp->prev_wqe_size;\n\n\tqp->wqe_wr_id[qp->sq.prod].opcode = qedr_ib_to_wc_opcode(wr->opcode);\n\n\tswitch (wr->opcode) {\n\tcase IB_WR_SEND_WITH_IMM:\n\t\tif (unlikely(rdma_protocol_iwarp(&dev->ibdev, 1))) {\n\t\t\trc = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_SEND_WITH_IMM;\n\t\tswqe = (struct rdma_sq_send_wqe_1st *)wqe;\n\t\tswqe->wqe_size = 2;\n\t\tswqe2 = qed_chain_produce(&qp->sq.pbl);\n\n\t\tswqe->inv_key_or_imm_data = cpu_to_le32(be32_to_cpu(wr->ex.imm_data));\n\t\tlength = qedr_prepare_sq_send_data(dev, qp, swqe, swqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\tswqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = swqe->wqe_size;\n\t\tqp->prev_wqe_size = swqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = swqe->length;\n\t\tbreak;\n\tcase IB_WR_SEND:\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_SEND;\n\t\tswqe = (struct rdma_sq_send_wqe_1st *)wqe;\n\n\t\tswqe->wqe_size = 2;\n\t\tswqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tlength = qedr_prepare_sq_send_data(dev, qp, swqe, swqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\tswqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = swqe->wqe_size;\n\t\tqp->prev_wqe_size = swqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = swqe->length;\n\t\tbreak;\n\tcase IB_WR_SEND_WITH_INV:\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_SEND_WITH_INVALIDATE;\n\t\tswqe = (struct rdma_sq_send_wqe_1st *)wqe;\n\t\tswqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tswqe->wqe_size = 2;\n\t\tswqe->inv_key_or_imm_data = cpu_to_le32(wr->ex.invalidate_rkey);\n\t\tlength = qedr_prepare_sq_send_data(dev, qp, swqe, swqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\tswqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = swqe->wqe_size;\n\t\tqp->prev_wqe_size = swqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = swqe->length;\n\t\tbreak;\n\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tif (unlikely(rdma_protocol_iwarp(&dev->ibdev, 1))) {\n\t\t\trc = -EINVAL;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_RDMA_WR_WITH_IMM;\n\t\trwqe = (struct rdma_sq_rdma_wqe_1st *)wqe;\n\n\t\trwqe->wqe_size = 2;\n\t\trwqe->imm_data = htonl(cpu_to_le32(wr->ex.imm_data));\n\t\trwqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tlength = qedr_prepare_sq_rdma_data(dev, qp, rwqe, rwqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\trwqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = rwqe->wqe_size;\n\t\tqp->prev_wqe_size = rwqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = rwqe->length;\n\t\tbreak;\n\tcase IB_WR_RDMA_WRITE:\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_RDMA_WR;\n\t\trwqe = (struct rdma_sq_rdma_wqe_1st *)wqe;\n\n\t\trwqe->wqe_size = 2;\n\t\trwqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tlength = qedr_prepare_sq_rdma_data(dev, qp, rwqe, rwqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\trwqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = rwqe->wqe_size;\n\t\tqp->prev_wqe_size = rwqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = rwqe->length;\n\t\tbreak;\n\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\tSET_FIELD2(wqe->flags, RDMA_SQ_RDMA_WQE_1ST_READ_INV_FLG, 1);\n\t\tfallthrough;\t \n\n\tcase IB_WR_RDMA_READ:\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_RDMA_RD;\n\t\trwqe = (struct rdma_sq_rdma_wqe_1st *)wqe;\n\n\t\trwqe->wqe_size = 2;\n\t\trwqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tlength = qedr_prepare_sq_rdma_data(dev, qp, rwqe, rwqe2,\n\t\t\t\t\t\t   wr, bad_wr);\n\t\trwqe->length = cpu_to_le32(length);\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = rwqe->wqe_size;\n\t\tqp->prev_wqe_size = rwqe->wqe_size;\n\t\tqp->wqe_wr_id[qp->sq.prod].bytes_len = rwqe->length;\n\t\tbreak;\n\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\tawqe1 = (struct rdma_sq_atomic_wqe_1st *)wqe;\n\t\tawqe1->wqe_size = 4;\n\n\t\tawqe2 = qed_chain_produce(&qp->sq.pbl);\n\t\tDMA_REGPAIR_LE(awqe2->remote_va, atomic_wr(wr)->remote_addr);\n\t\tawqe2->r_key = cpu_to_le32(atomic_wr(wr)->rkey);\n\n\t\tawqe3 = qed_chain_produce(&qp->sq.pbl);\n\n\t\tif (wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD) {\n\t\t\twqe->req_type = RDMA_SQ_REQ_TYPE_ATOMIC_ADD;\n\t\t\tDMA_REGPAIR_LE(awqe3->swap_data,\n\t\t\t\t       atomic_wr(wr)->compare_add);\n\t\t} else {\n\t\t\twqe->req_type = RDMA_SQ_REQ_TYPE_ATOMIC_CMP_AND_SWAP;\n\t\t\tDMA_REGPAIR_LE(awqe3->swap_data,\n\t\t\t\t       atomic_wr(wr)->swap);\n\t\t\tDMA_REGPAIR_LE(awqe3->cmp_data,\n\t\t\t\t       atomic_wr(wr)->compare_add);\n\t\t}\n\n\t\tqedr_prepare_sq_sges(qp, NULL, wr);\n\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = awqe1->wqe_size;\n\t\tqp->prev_wqe_size = awqe1->wqe_size;\n\t\tbreak;\n\n\tcase IB_WR_LOCAL_INV:\n\t\tiwqe = (struct rdma_sq_local_inv_wqe *)wqe;\n\t\tiwqe->wqe_size = 1;\n\n\t\tiwqe->req_type = RDMA_SQ_REQ_TYPE_LOCAL_INVALIDATE;\n\t\tiwqe->inv_l_key = wr->ex.invalidate_rkey;\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = iwqe->wqe_size;\n\t\tqp->prev_wqe_size = iwqe->wqe_size;\n\t\tbreak;\n\tcase IB_WR_REG_MR:\n\t\tDP_DEBUG(dev, QEDR_MSG_CQ, \"REG_MR\\n\");\n\t\twqe->req_type = RDMA_SQ_REQ_TYPE_FAST_MR;\n\t\tfwqe1 = (struct rdma_sq_fmr_wqe_1st *)wqe;\n\t\tfwqe1->wqe_size = 2;\n\n\t\trc = qedr_prepare_reg(qp, fwqe1, reg_wr(wr));\n\t\tif (rc) {\n\t\t\tDP_ERR(dev, \"IB_REG_MR failed rc=%d\\n\", rc);\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tqp->wqe_wr_id[qp->sq.prod].wqe_size = fwqe1->wqe_size;\n\t\tqp->prev_wqe_size = fwqe1->wqe_size;\n\t\tbreak;\n\tdefault:\n\t\tDP_ERR(dev, \"invalid opcode 0x%x!\\n\", wr->opcode);\n\t\trc = -EINVAL;\n\t\t*bad_wr = wr;\n\t\tbreak;\n\t}\n\n\tif (*bad_wr) {\n\t\tu16 value;\n\n\t\t \n\t\tvalue = le16_to_cpu(qp->sq.db_data.data.value);\n\t\tqed_chain_set_prod(&qp->sq.pbl, value, wqe);\n\n\t\t \n\t\tqp->prev_wqe_size = wqe->prev_wqe_size;\n\t\trc = -EINVAL;\n\t\tDP_ERR(dev, \"POST SEND FAILED\\n\");\n\t}\n\n\treturn rc;\n}\n\nint qedr_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t   const struct ib_send_wr **bad_wr)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibqp->device);\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tunsigned long flags;\n\tint rc = 0;\n\n\t*bad_wr = NULL;\n\n\tif (qp->qp_type == IB_QPT_GSI)\n\t\treturn qedr_gsi_post_send(ibqp, wr, bad_wr);\n\n\tspin_lock_irqsave(&qp->q_lock, flags);\n\n\tif (rdma_protocol_roce(&dev->ibdev, 1)) {\n\t\tif ((qp->state != QED_ROCE_QP_STATE_RTS) &&\n\t\t    (qp->state != QED_ROCE_QP_STATE_ERR) &&\n\t\t    (qp->state != QED_ROCE_QP_STATE_SQD)) {\n\t\t\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\t\t\t*bad_wr = wr;\n\t\t\tDP_DEBUG(dev, QEDR_MSG_CQ,\n\t\t\t\t \"QP in wrong state! QP icid=0x%x state %d\\n\",\n\t\t\t\t qp->icid, qp->state);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twhile (wr) {\n\t\trc = __qedr_post_send(ibqp, wr, bad_wr);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tqp->wqe_wr_id[qp->sq.prod].wr_id = wr->wr_id;\n\n\t\tqedr_inc_sw_prod(&qp->sq);\n\n\t\tqp->sq.db_data.data.value++;\n\n\t\twr = wr->next;\n\t}\n\n\t \n\tsmp_wmb();\n\twritel(qp->sq.db_data.raw, qp->sq.db);\n\n\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\n\treturn rc;\n}\n\nstatic u32 qedr_srq_elem_left(struct qedr_srq_hwq_info *hw_srq)\n{\n\tu32 used;\n\n\t \n\tused = hw_srq->wr_prod_cnt - (u32)atomic_read(&hw_srq->wr_cons_cnt);\n\n\treturn hw_srq->max_wr - used;\n}\n\nint qedr_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t       const struct ib_recv_wr **bad_wr)\n{\n\tstruct qedr_srq *srq = get_qedr_srq(ibsrq);\n\tstruct qedr_srq_hwq_info *hw_srq;\n\tstruct qedr_dev *dev = srq->dev;\n\tstruct qed_chain *pbl;\n\tunsigned long flags;\n\tint status = 0;\n\tu32 num_sge;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\thw_srq = &srq->hw_srq;\n\tpbl = &srq->hw_srq.pbl;\n\twhile (wr) {\n\t\tstruct rdma_srq_wqe_header *hdr;\n\t\tint i;\n\n\t\tif (!qedr_srq_elem_left(hw_srq) ||\n\t\t    wr->num_sge > srq->hw_srq.max_sges) {\n\t\t\tDP_ERR(dev, \"Can't post WR  (%d,%d) || (%d > %d)\\n\",\n\t\t\t       hw_srq->wr_prod_cnt,\n\t\t\t       atomic_read(&hw_srq->wr_cons_cnt),\n\t\t\t       wr->num_sge, srq->hw_srq.max_sges);\n\t\t\tstatus = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\n\t\thdr = qed_chain_produce(pbl);\n\t\tnum_sge = wr->num_sge;\n\t\t \n\t\tSRQ_HDR_SET(hdr, wr->wr_id, num_sge);\n\n\t\tsrq->hw_srq.wr_prod_cnt++;\n\t\thw_srq->wqe_prod++;\n\t\thw_srq->sge_prod++;\n\n\t\tDP_DEBUG(dev, QEDR_MSG_SRQ,\n\t\t\t \"SRQ WR: SGEs: %d with wr_id[%d] = %llx\\n\",\n\t\t\t wr->num_sge, hw_srq->wqe_prod, wr->wr_id);\n\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tstruct rdma_srq_sge *srq_sge = qed_chain_produce(pbl);\n\n\t\t\t \n\t\t\tSRQ_SGE_SET(srq_sge, wr->sg_list[i].addr,\n\t\t\t\t    wr->sg_list[i].length, wr->sg_list[i].lkey);\n\n\t\t\tDP_DEBUG(dev, QEDR_MSG_SRQ,\n\t\t\t\t \"[%d]: len %d key %x addr %x:%x\\n\",\n\t\t\t\t i, srq_sge->length, srq_sge->l_key,\n\t\t\t\t srq_sge->addr.hi, srq_sge->addr.lo);\n\t\t\thw_srq->sge_prod++;\n\t\t}\n\n\t\t \n\t\tdma_wmb();\n\n\t\t \n\t\tsrq->hw_srq.virt_prod_pair_addr->sge_prod = cpu_to_le32(hw_srq->sge_prod);\n\t\t \n\t\tdma_wmb();\n\t\tsrq->hw_srq.virt_prod_pair_addr->wqe_prod = cpu_to_le32(hw_srq->wqe_prod);\n\n\t\twr = wr->next;\n\t}\n\n\tDP_DEBUG(dev, QEDR_MSG_SRQ, \"POST: Elements in S-RQ: %d\\n\",\n\t\t qed_chain_get_elem_left(pbl));\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn status;\n}\n\nint qedr_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t   const struct ib_recv_wr **bad_wr)\n{\n\tstruct qedr_qp *qp = get_qedr_qp(ibqp);\n\tstruct qedr_dev *dev = qp->dev;\n\tunsigned long flags;\n\tint status = 0;\n\n\tif (qp->qp_type == IB_QPT_GSI)\n\t\treturn qedr_gsi_post_recv(ibqp, wr, bad_wr);\n\n\tspin_lock_irqsave(&qp->q_lock, flags);\n\n\twhile (wr) {\n\t\tint i;\n\n\t\tif (qed_chain_get_elem_left_u32(&qp->rq.pbl) <\n\t\t    QEDR_MAX_RQE_ELEMENTS_PER_RQE ||\n\t\t    wr->num_sge > qp->rq.max_sges) {\n\t\t\tDP_ERR(dev, \"Can't post WR  (%d < %d) || (%d > %d)\\n\",\n\t\t\t       qed_chain_get_elem_left_u32(&qp->rq.pbl),\n\t\t\t       QEDR_MAX_RQE_ELEMENTS_PER_RQE, wr->num_sge,\n\t\t\t       qp->rq.max_sges);\n\t\t\tstatus = -ENOMEM;\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tu32 flags = 0;\n\t\t\tstruct rdma_rq_sge *rqe =\n\t\t\t    qed_chain_produce(&qp->rq.pbl);\n\n\t\t\t \n\t\t\tif (!i)\n\t\t\t\tSET_FIELD(flags, RDMA_RQ_SGE_NUM_SGES,\n\t\t\t\t\t  wr->num_sge);\n\n\t\t\tSET_FIELD(flags, RDMA_RQ_SGE_L_KEY_LO,\n\t\t\t\t  wr->sg_list[i].lkey);\n\n\t\t\tRQ_SGE_SET(rqe, wr->sg_list[i].addr,\n\t\t\t\t   wr->sg_list[i].length, flags);\n\t\t}\n\n\t\t \n\t\tif (!wr->num_sge) {\n\t\t\tu32 flags = 0;\n\t\t\tstruct rdma_rq_sge *rqe =\n\t\t\t    qed_chain_produce(&qp->rq.pbl);\n\n\t\t\t \n\t\t\tSET_FIELD(flags, RDMA_RQ_SGE_L_KEY_LO, 0);\n\t\t\tSET_FIELD(flags, RDMA_RQ_SGE_NUM_SGES, 1);\n\n\t\t\tRQ_SGE_SET(rqe, 0, 0, flags);\n\t\t\ti = 1;\n\t\t}\n\n\t\tqp->rqe_wr_id[qp->rq.prod].wr_id = wr->wr_id;\n\t\tqp->rqe_wr_id[qp->rq.prod].wqe_size = i;\n\n\t\tqedr_inc_sw_prod(&qp->rq);\n\n\t\t \n\t\tsmp_wmb();\n\n\t\tqp->rq.db_data.data.value++;\n\n\t\twritel(qp->rq.db_data.raw, qp->rq.db);\n\n\t\tif (rdma_protocol_iwarp(&dev->ibdev, 1)) {\n\t\t\twritel(qp->rq.iwarp_db2_data.raw, qp->rq.iwarp_db2);\n\t\t}\n\n\t\twr = wr->next;\n\t}\n\n\tspin_unlock_irqrestore(&qp->q_lock, flags);\n\n\treturn status;\n}\n\nstatic int is_valid_cqe(struct qedr_cq *cq, union rdma_cqe *cqe)\n{\n\tstruct rdma_cqe_requester *resp_cqe = &cqe->req;\n\n\treturn (resp_cqe->flags & RDMA_CQE_REQUESTER_TOGGLE_BIT_MASK) ==\n\t\tcq->pbl_toggle;\n}\n\nstatic struct qedr_qp *cqe_get_qp(union rdma_cqe *cqe)\n{\n\tstruct rdma_cqe_requester *resp_cqe = &cqe->req;\n\tstruct qedr_qp *qp;\n\n\tqp = (struct qedr_qp *)(uintptr_t)HILO_GEN(resp_cqe->qp_handle.hi,\n\t\t\t\t\t\t   resp_cqe->qp_handle.lo,\n\t\t\t\t\t\t   u64);\n\treturn qp;\n}\n\nstatic enum rdma_cqe_type cqe_get_type(union rdma_cqe *cqe)\n{\n\tstruct rdma_cqe_requester *resp_cqe = &cqe->req;\n\n\treturn GET_FIELD(resp_cqe->flags, RDMA_CQE_REQUESTER_TYPE);\n}\n\n \nstatic union rdma_cqe *get_cqe(struct qedr_cq *cq)\n{\n\treturn cq->latest_cqe;\n}\n\n \nstatic inline void qedr_chk_if_fmr(struct qedr_qp *qp)\n{\n\tif (qp->wqe_wr_id[qp->sq.cons].opcode == IB_WC_REG_MR)\n\t\tqp->wqe_wr_id[qp->sq.cons].mr->info.completed++;\n}\n\nstatic int process_req(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t       struct qedr_cq *cq, int num_entries,\n\t\t       struct ib_wc *wc, u16 hw_cons, enum ib_wc_status status,\n\t\t       int force)\n{\n\tu16 cnt = 0;\n\n\twhile (num_entries && qp->sq.wqe_cons != hw_cons) {\n\t\tif (!qp->wqe_wr_id[qp->sq.cons].signaled && !force) {\n\t\t\tqedr_chk_if_fmr(qp);\n\t\t\t \n\t\t\tgoto next_cqe;\n\t\t}\n\n\t\t \n\t\twc->status = status;\n\t\twc->vendor_err = 0;\n\t\twc->wc_flags = 0;\n\t\twc->src_qp = qp->id;\n\t\twc->qp = &qp->ibqp;\n\n\t\twc->wr_id = qp->wqe_wr_id[qp->sq.cons].wr_id;\n\t\twc->opcode = qp->wqe_wr_id[qp->sq.cons].opcode;\n\n\t\tswitch (wc->opcode) {\n\t\tcase IB_WC_RDMA_WRITE:\n\t\t\twc->byte_len = qp->wqe_wr_id[qp->sq.cons].bytes_len;\n\t\t\tbreak;\n\t\tcase IB_WC_COMP_SWAP:\n\t\tcase IB_WC_FETCH_ADD:\n\t\t\twc->byte_len = 8;\n\t\t\tbreak;\n\t\tcase IB_WC_REG_MR:\n\t\t\tqp->wqe_wr_id[qp->sq.cons].mr->info.completed++;\n\t\t\tbreak;\n\t\tcase IB_WC_RDMA_READ:\n\t\tcase IB_WC_SEND:\n\t\t\twc->byte_len = qp->wqe_wr_id[qp->sq.cons].bytes_len;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tnum_entries--;\n\t\twc++;\n\t\tcnt++;\nnext_cqe:\n\t\twhile (qp->wqe_wr_id[qp->sq.cons].wqe_size--)\n\t\t\tqed_chain_consume(&qp->sq.pbl);\n\t\tqedr_inc_sw_cons(&qp->sq);\n\t}\n\n\treturn cnt;\n}\n\nstatic int qedr_poll_cq_req(struct qedr_dev *dev,\n\t\t\t    struct qedr_qp *qp, struct qedr_cq *cq,\n\t\t\t    int num_entries, struct ib_wc *wc,\n\t\t\t    struct rdma_cqe_requester *req)\n{\n\tint cnt = 0;\n\n\tswitch (req->status) {\n\tcase RDMA_CQE_REQ_STS_OK:\n\t\tcnt = process_req(dev, qp, cq, num_entries, wc, req->sq_cons,\n\t\t\t\t  IB_WC_SUCCESS, 0);\n\t\tbreak;\n\tcase RDMA_CQE_REQ_STS_WORK_REQUEST_FLUSHED_ERR:\n\t\tif (qp->state != QED_ROCE_QP_STATE_ERR)\n\t\t\tDP_DEBUG(dev, QEDR_MSG_CQ,\n\t\t\t\t \"Error: POLL CQ with RDMA_CQE_REQ_STS_WORK_REQUEST_FLUSHED_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t cq->icid, qp->icid);\n\t\tcnt = process_req(dev, qp, cq, num_entries, wc, req->sq_cons,\n\t\t\t\t  IB_WC_WR_FLUSH_ERR, 1);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tqp->state = QED_ROCE_QP_STATE_ERR;\n\t\tcnt = process_req(dev, qp, cq, num_entries, wc,\n\t\t\t\t  req->sq_cons - 1, IB_WC_SUCCESS, 0);\n\t\twc += cnt;\n\t\t \n\t\tif (cnt < num_entries) {\n\t\t\tenum ib_wc_status wc_status;\n\n\t\t\tswitch (req->status) {\n\t\t\tcase RDMA_CQE_REQ_STS_BAD_RESPONSE_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_BAD_RESPONSE_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_BAD_RESP_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_LOCAL_LENGTH_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_LOCAL_LENGTH_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_LOC_LEN_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_LOCAL_QP_OPERATION_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_LOCAL_QP_OPERATION_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_LOC_QP_OP_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_LOCAL_PROTECTION_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_LOCAL_PROTECTION_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_LOC_PROT_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_MEMORY_MGT_OPERATION_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_MEMORY_MGT_OPERATION_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_MW_BIND_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_REMOTE_INVALID_REQUEST_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_REMOTE_INVALID_REQUEST_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_REM_INV_REQ_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_REMOTE_ACCESS_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_REMOTE_ACCESS_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_REM_ACCESS_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_REMOTE_OPERATION_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_REMOTE_OPERATION_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_REM_OP_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_RNR_NAK_RETRY_CNT_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with RDMA_CQE_REQ_STS_RNR_NAK_RETRY_CNT_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_RNR_RETRY_EXC_ERR;\n\t\t\t\tbreak;\n\t\t\tcase RDMA_CQE_REQ_STS_TRANSPORT_RETRY_CNT_ERR:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with ROCE_CQE_REQ_STS_TRANSPORT_RETRY_CNT_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_RETRY_EXC_ERR;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDP_ERR(dev,\n\t\t\t\t       \"Error: POLL CQ with IB_WC_GENERAL_ERR. CQ icid=0x%x, QP icid=0x%x\\n\",\n\t\t\t\t       cq->icid, qp->icid);\n\t\t\t\twc_status = IB_WC_GENERAL_ERR;\n\t\t\t}\n\t\t\tcnt += process_req(dev, qp, cq, 1, wc, req->sq_cons,\n\t\t\t\t\t   wc_status, 1);\n\t\t}\n\t}\n\n\treturn cnt;\n}\n\nstatic inline int qedr_cqe_resp_status_to_ib(u8 status)\n{\n\tswitch (status) {\n\tcase RDMA_CQE_RESP_STS_LOCAL_ACCESS_ERR:\n\t\treturn IB_WC_LOC_ACCESS_ERR;\n\tcase RDMA_CQE_RESP_STS_LOCAL_LENGTH_ERR:\n\t\treturn IB_WC_LOC_LEN_ERR;\n\tcase RDMA_CQE_RESP_STS_LOCAL_QP_OPERATION_ERR:\n\t\treturn IB_WC_LOC_QP_OP_ERR;\n\tcase RDMA_CQE_RESP_STS_LOCAL_PROTECTION_ERR:\n\t\treturn IB_WC_LOC_PROT_ERR;\n\tcase RDMA_CQE_RESP_STS_MEMORY_MGT_OPERATION_ERR:\n\t\treturn IB_WC_MW_BIND_ERR;\n\tcase RDMA_CQE_RESP_STS_REMOTE_INVALID_REQUEST_ERR:\n\t\treturn IB_WC_REM_INV_RD_REQ_ERR;\n\tcase RDMA_CQE_RESP_STS_OK:\n\t\treturn IB_WC_SUCCESS;\n\tdefault:\n\t\treturn IB_WC_GENERAL_ERR;\n\t}\n}\n\nstatic inline int qedr_set_ok_cqe_resp_wc(struct rdma_cqe_responder *resp,\n\t\t\t\t\t  struct ib_wc *wc)\n{\n\twc->status = IB_WC_SUCCESS;\n\twc->byte_len = le32_to_cpu(resp->length);\n\n\tif (resp->flags & QEDR_RESP_IMM) {\n\t\twc->ex.imm_data = cpu_to_be32(le32_to_cpu(resp->imm_data_or_inv_r_Key));\n\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\n\t\tif (resp->flags & QEDR_RESP_RDMA)\n\t\t\twc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\n\n\t\tif (resp->flags & QEDR_RESP_INV)\n\t\t\treturn -EINVAL;\n\n\t} else if (resp->flags & QEDR_RESP_INV) {\n\t\twc->ex.imm_data = le32_to_cpu(resp->imm_data_or_inv_r_Key);\n\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\n\t\tif (resp->flags & QEDR_RESP_RDMA)\n\t\t\treturn -EINVAL;\n\n\t} else if (resp->flags & QEDR_RESP_RDMA) {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void __process_resp_one(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t       struct qedr_cq *cq, struct ib_wc *wc,\n\t\t\t       struct rdma_cqe_responder *resp, u64 wr_id)\n{\n\t \n\twc->opcode = IB_WC_RECV;\n\twc->wc_flags = 0;\n\n\tif (likely(resp->status == RDMA_CQE_RESP_STS_OK)) {\n\t\tif (qedr_set_ok_cqe_resp_wc(resp, wc))\n\t\t\tDP_ERR(dev,\n\t\t\t       \"CQ %p (icid=%d) has invalid CQE responder flags=0x%x\\n\",\n\t\t\t       cq, cq->icid, resp->flags);\n\n\t} else {\n\t\twc->status = qedr_cqe_resp_status_to_ib(resp->status);\n\t\tif (wc->status == IB_WC_GENERAL_ERR)\n\t\t\tDP_ERR(dev,\n\t\t\t       \"CQ %p (icid=%d) contains an invalid CQE status %d\\n\",\n\t\t\t       cq, cq->icid, resp->status);\n\t}\n\n\t \n\twc->vendor_err = 0;\n\twc->src_qp = qp->id;\n\twc->qp = &qp->ibqp;\n\twc->wr_id = wr_id;\n}\n\nstatic int process_resp_one_srq(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t\tstruct qedr_cq *cq, struct ib_wc *wc,\n\t\t\t\tstruct rdma_cqe_responder *resp)\n{\n\tstruct qedr_srq *srq = qp->srq;\n\tu64 wr_id;\n\n\twr_id = HILO_GEN(le32_to_cpu(resp->srq_wr_id.hi),\n\t\t\t le32_to_cpu(resp->srq_wr_id.lo), u64);\n\n\tif (resp->status == RDMA_CQE_RESP_STS_WORK_REQUEST_FLUSHED_ERR) {\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\twc->vendor_err = 0;\n\t\twc->wr_id = wr_id;\n\t\twc->byte_len = 0;\n\t\twc->src_qp = qp->id;\n\t\twc->qp = &qp->ibqp;\n\t\twc->wr_id = wr_id;\n\t} else {\n\t\t__process_resp_one(dev, qp, cq, wc, resp, wr_id);\n\t}\n\tatomic_inc(&srq->hw_srq.wr_cons_cnt);\n\n\treturn 1;\n}\nstatic int process_resp_one(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t    struct qedr_cq *cq, struct ib_wc *wc,\n\t\t\t    struct rdma_cqe_responder *resp)\n{\n\tu64 wr_id = qp->rqe_wr_id[qp->rq.cons].wr_id;\n\n\t__process_resp_one(dev, qp, cq, wc, resp, wr_id);\n\n\twhile (qp->rqe_wr_id[qp->rq.cons].wqe_size--)\n\t\tqed_chain_consume(&qp->rq.pbl);\n\tqedr_inc_sw_cons(&qp->rq);\n\n\treturn 1;\n}\n\nstatic int process_resp_flush(struct qedr_qp *qp, struct qedr_cq *cq,\n\t\t\t      int num_entries, struct ib_wc *wc, u16 hw_cons)\n{\n\tu16 cnt = 0;\n\n\twhile (num_entries && qp->rq.wqe_cons != hw_cons) {\n\t\t \n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\twc->vendor_err = 0;\n\t\twc->wc_flags = 0;\n\t\twc->src_qp = qp->id;\n\t\twc->byte_len = 0;\n\t\twc->wr_id = qp->rqe_wr_id[qp->rq.cons].wr_id;\n\t\twc->qp = &qp->ibqp;\n\t\tnum_entries--;\n\t\twc++;\n\t\tcnt++;\n\t\twhile (qp->rqe_wr_id[qp->rq.cons].wqe_size--)\n\t\t\tqed_chain_consume(&qp->rq.pbl);\n\t\tqedr_inc_sw_cons(&qp->rq);\n\t}\n\n\treturn cnt;\n}\n\nstatic void try_consume_resp_cqe(struct qedr_cq *cq, struct qedr_qp *qp,\n\t\t\t\t struct rdma_cqe_responder *resp, int *update)\n{\n\tif (le16_to_cpu(resp->rq_cons_or_srq_id) == qp->rq.wqe_cons) {\n\t\tconsume_cqe(cq);\n\t\t*update |= 1;\n\t}\n}\n\nstatic int qedr_poll_cq_resp_srq(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t\t struct qedr_cq *cq, int num_entries,\n\t\t\t\t struct ib_wc *wc,\n\t\t\t\t struct rdma_cqe_responder *resp)\n{\n\tint cnt;\n\n\tcnt = process_resp_one_srq(dev, qp, cq, wc, resp);\n\tconsume_cqe(cq);\n\n\treturn cnt;\n}\n\nstatic int qedr_poll_cq_resp(struct qedr_dev *dev, struct qedr_qp *qp,\n\t\t\t     struct qedr_cq *cq, int num_entries,\n\t\t\t     struct ib_wc *wc, struct rdma_cqe_responder *resp,\n\t\t\t     int *update)\n{\n\tint cnt;\n\n\tif (resp->status == RDMA_CQE_RESP_STS_WORK_REQUEST_FLUSHED_ERR) {\n\t\tcnt = process_resp_flush(qp, cq, num_entries, wc,\n\t\t\t\t\t resp->rq_cons_or_srq_id);\n\t\ttry_consume_resp_cqe(cq, qp, resp, update);\n\t} else {\n\t\tcnt = process_resp_one(dev, qp, cq, wc, resp);\n\t\tconsume_cqe(cq);\n\t\t*update |= 1;\n\t}\n\n\treturn cnt;\n}\n\nstatic void try_consume_req_cqe(struct qedr_cq *cq, struct qedr_qp *qp,\n\t\t\t\tstruct rdma_cqe_requester *req, int *update)\n{\n\tif (le16_to_cpu(req->sq_cons) == qp->sq.wqe_cons) {\n\t\tconsume_cqe(cq);\n\t\t*update |= 1;\n\t}\n}\n\nint qedr_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tstruct qedr_dev *dev = get_qedr_dev(ibcq->device);\n\tstruct qedr_cq *cq = get_qedr_cq(ibcq);\n\tunion rdma_cqe *cqe;\n\tu32 old_cons, new_cons;\n\tunsigned long flags;\n\tint update = 0;\n\tint done = 0;\n\n\tif (cq->destroyed) {\n\t\tDP_ERR(dev,\n\t\t       \"warning: poll was invoked after destroy for cq %p (icid=%d)\\n\",\n\t\t       cq, cq->icid);\n\t\treturn 0;\n\t}\n\n\tif (cq->cq_type == QEDR_CQ_TYPE_GSI)\n\t\treturn qedr_gsi_poll_cq(ibcq, num_entries, wc);\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tcqe = cq->latest_cqe;\n\told_cons = qed_chain_get_cons_idx_u32(&cq->pbl);\n\twhile (num_entries && is_valid_cqe(cq, cqe)) {\n\t\tstruct qedr_qp *qp;\n\t\tint cnt = 0;\n\n\t\t \n\t\trmb();\n\n\t\tqp = cqe_get_qp(cqe);\n\t\tif (!qp) {\n\t\t\tWARN(1, \"Error: CQE QP pointer is NULL. CQE=%p\\n\", cqe);\n\t\t\tbreak;\n\t\t}\n\n\t\twc->qp = &qp->ibqp;\n\n\t\tswitch (cqe_get_type(cqe)) {\n\t\tcase RDMA_CQE_TYPE_REQUESTER:\n\t\t\tcnt = qedr_poll_cq_req(dev, qp, cq, num_entries, wc,\n\t\t\t\t\t       &cqe->req);\n\t\t\ttry_consume_req_cqe(cq, qp, &cqe->req, &update);\n\t\t\tbreak;\n\t\tcase RDMA_CQE_TYPE_RESPONDER_RQ:\n\t\t\tcnt = qedr_poll_cq_resp(dev, qp, cq, num_entries, wc,\n\t\t\t\t\t\t&cqe->resp, &update);\n\t\t\tbreak;\n\t\tcase RDMA_CQE_TYPE_RESPONDER_SRQ:\n\t\t\tcnt = qedr_poll_cq_resp_srq(dev, qp, cq, num_entries,\n\t\t\t\t\t\t    wc, &cqe->resp);\n\t\t\tupdate = 1;\n\t\t\tbreak;\n\t\tcase RDMA_CQE_TYPE_INVALID:\n\t\tdefault:\n\t\t\tDP_ERR(dev, \"Error: invalid CQE type = %d\\n\",\n\t\t\t       cqe_get_type(cqe));\n\t\t}\n\t\tnum_entries -= cnt;\n\t\twc += cnt;\n\t\tdone += cnt;\n\n\t\tcqe = get_cqe(cq);\n\t}\n\tnew_cons = qed_chain_get_cons_idx_u32(&cq->pbl);\n\n\tcq->cq_cons += new_cons - old_cons;\n\n\tif (update)\n\t\t \n\t\tdoorbell_cq(cq, cq->cq_cons - 1, cq->arm_flags);\n\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\treturn done;\n}\n\nint qedr_process_mad(struct ib_device *ibdev, int process_mad_flags,\n\t\t     u32 port_num, const struct ib_wc *in_wc,\n\t\t     const struct ib_grh *in_grh, const struct ib_mad *in,\n\t\t     struct ib_mad *out_mad, size_t *out_mad_size,\n\t\t     u16 *out_mad_pkey_index)\n{\n\treturn IB_MAD_RESULT_SUCCESS;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}