{
  "module_name": "erdma_qp.c",
  "hash_id": "a717523bd8ab59c7e86b8f27e66813b0e10acfd7534168210568cc9979e30d71",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/erdma/erdma_qp.c",
  "human_readable_source": "\n\n \n \n \n \n \n\n#include \"erdma_cm.h\"\n#include \"erdma_verbs.h\"\n\nvoid erdma_qp_llp_close(struct erdma_qp *qp)\n{\n\tstruct erdma_qp_attrs qp_attrs;\n\n\tdown_write(&qp->state_lock);\n\n\tswitch (qp->attrs.state) {\n\tcase ERDMA_QP_STATE_RTS:\n\tcase ERDMA_QP_STATE_RTR:\n\tcase ERDMA_QP_STATE_IDLE:\n\tcase ERDMA_QP_STATE_TERMINATE:\n\t\tqp_attrs.state = ERDMA_QP_STATE_CLOSING;\n\t\terdma_modify_qp_internal(qp, &qp_attrs, ERDMA_QP_ATTR_STATE);\n\t\tbreak;\n\tcase ERDMA_QP_STATE_CLOSING:\n\t\tqp->attrs.state = ERDMA_QP_STATE_IDLE;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (qp->cep) {\n\t\terdma_cep_put(qp->cep);\n\t\tqp->cep = NULL;\n\t}\n\n\tup_write(&qp->state_lock);\n}\n\nstruct ib_qp *erdma_get_ibqp(struct ib_device *ibdev, int id)\n{\n\tstruct erdma_qp *qp = find_qp_by_qpn(to_edev(ibdev), id);\n\n\tif (qp)\n\t\treturn &qp->ibqp;\n\n\treturn NULL;\n}\n\nstatic int erdma_modify_qp_state_to_rts(struct erdma_qp *qp,\n\t\t\t\t\tstruct erdma_qp_attrs *attrs,\n\t\t\t\t\tenum erdma_qp_attr_mask mask)\n{\n\tint ret;\n\tstruct erdma_dev *dev = qp->dev;\n\tstruct erdma_cmdq_modify_qp_req req;\n\tstruct tcp_sock *tp;\n\tstruct erdma_cep *cep = qp->cep;\n\tstruct sockaddr_storage local_addr, remote_addr;\n\n\tif (!(mask & ERDMA_QP_ATTR_LLP_HANDLE))\n\t\treturn -EINVAL;\n\n\tif (!(mask & ERDMA_QP_ATTR_MPA))\n\t\treturn -EINVAL;\n\n\tret = getname_local(cep->sock, &local_addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = getname_peer(cep->sock, &remote_addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tqp->attrs.state = ERDMA_QP_STATE_RTS;\n\n\ttp = tcp_sk(qp->cep->sock->sk);\n\n\terdma_cmdq_build_reqhdr(&req.hdr, CMDQ_SUBMOD_RDMA,\n\t\t\t\tCMDQ_OPCODE_MODIFY_QP);\n\n\treq.cfg = FIELD_PREP(ERDMA_CMD_MODIFY_QP_STATE_MASK, qp->attrs.state) |\n\t\t  FIELD_PREP(ERDMA_CMD_MODIFY_QP_CC_MASK, qp->attrs.cc) |\n\t\t  FIELD_PREP(ERDMA_CMD_MODIFY_QP_QPN_MASK, QP_ID(qp));\n\n\treq.cookie = be32_to_cpu(qp->cep->mpa.ext_data.cookie);\n\treq.dip = to_sockaddr_in(remote_addr).sin_addr.s_addr;\n\treq.sip = to_sockaddr_in(local_addr).sin_addr.s_addr;\n\treq.dport = to_sockaddr_in(remote_addr).sin_port;\n\treq.sport = to_sockaddr_in(local_addr).sin_port;\n\n\treq.send_nxt = tp->snd_nxt;\n\t \n\tif (qp->attrs.qp_type == ERDMA_QP_PASSIVE)\n\t\treq.send_nxt += MPA_DEFAULT_HDR_LEN + qp->attrs.pd_len;\n\treq.recv_nxt = tp->rcv_nxt;\n\n\treturn erdma_post_cmd_wait(&dev->cmdq, &req, sizeof(req), NULL, NULL);\n}\n\nstatic int erdma_modify_qp_state_to_stop(struct erdma_qp *qp,\n\t\t\t\t\t struct erdma_qp_attrs *attrs,\n\t\t\t\t\t enum erdma_qp_attr_mask mask)\n{\n\tstruct erdma_dev *dev = qp->dev;\n\tstruct erdma_cmdq_modify_qp_req req;\n\n\tqp->attrs.state = attrs->state;\n\n\terdma_cmdq_build_reqhdr(&req.hdr, CMDQ_SUBMOD_RDMA,\n\t\t\t\tCMDQ_OPCODE_MODIFY_QP);\n\n\treq.cfg = FIELD_PREP(ERDMA_CMD_MODIFY_QP_STATE_MASK, attrs->state) |\n\t\t  FIELD_PREP(ERDMA_CMD_MODIFY_QP_QPN_MASK, QP_ID(qp));\n\n\treturn erdma_post_cmd_wait(&dev->cmdq, &req, sizeof(req), NULL, NULL);\n}\n\nint erdma_modify_qp_internal(struct erdma_qp *qp, struct erdma_qp_attrs *attrs,\n\t\t\t     enum erdma_qp_attr_mask mask)\n{\n\tbool need_reflush = false;\n\tint drop_conn, ret = 0;\n\n\tif (!mask)\n\t\treturn 0;\n\n\tif (!(mask & ERDMA_QP_ATTR_STATE))\n\t\treturn 0;\n\n\tswitch (qp->attrs.state) {\n\tcase ERDMA_QP_STATE_IDLE:\n\tcase ERDMA_QP_STATE_RTR:\n\t\tif (attrs->state == ERDMA_QP_STATE_RTS) {\n\t\t\tret = erdma_modify_qp_state_to_rts(qp, attrs, mask);\n\t\t} else if (attrs->state == ERDMA_QP_STATE_ERROR) {\n\t\t\tqp->attrs.state = ERDMA_QP_STATE_ERROR;\n\t\t\tneed_reflush = true;\n\t\t\tif (qp->cep) {\n\t\t\t\terdma_cep_put(qp->cep);\n\t\t\t\tqp->cep = NULL;\n\t\t\t}\n\t\t\tret = erdma_modify_qp_state_to_stop(qp, attrs, mask);\n\t\t}\n\t\tbreak;\n\tcase ERDMA_QP_STATE_RTS:\n\t\tdrop_conn = 0;\n\n\t\tif (attrs->state == ERDMA_QP_STATE_CLOSING ||\n\t\t    attrs->state == ERDMA_QP_STATE_TERMINATE ||\n\t\t    attrs->state == ERDMA_QP_STATE_ERROR) {\n\t\t\tret = erdma_modify_qp_state_to_stop(qp, attrs, mask);\n\t\t\tdrop_conn = 1;\n\t\t\tneed_reflush = true;\n\t\t}\n\n\t\tif (drop_conn)\n\t\t\terdma_qp_cm_drop(qp);\n\n\t\tbreak;\n\tcase ERDMA_QP_STATE_TERMINATE:\n\t\tif (attrs->state == ERDMA_QP_STATE_ERROR)\n\t\t\tqp->attrs.state = ERDMA_QP_STATE_ERROR;\n\t\tbreak;\n\tcase ERDMA_QP_STATE_CLOSING:\n\t\tif (attrs->state == ERDMA_QP_STATE_IDLE) {\n\t\t\tqp->attrs.state = ERDMA_QP_STATE_IDLE;\n\t\t} else if (attrs->state == ERDMA_QP_STATE_ERROR) {\n\t\t\tret = erdma_modify_qp_state_to_stop(qp, attrs, mask);\n\t\t\tqp->attrs.state = ERDMA_QP_STATE_ERROR;\n\t\t} else if (attrs->state != ERDMA_QP_STATE_CLOSING) {\n\t\t\treturn -ECONNABORTED;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (need_reflush && !ret && rdma_is_kernel_res(&qp->ibqp.res)) {\n\t\tqp->flags |= ERDMA_QP_IN_FLUSHING;\n\t\tmod_delayed_work(qp->dev->reflush_wq, &qp->reflush_dwork,\n\t\t\t\t usecs_to_jiffies(100));\n\t}\n\n\treturn ret;\n}\n\nstatic void erdma_qp_safe_free(struct kref *ref)\n{\n\tstruct erdma_qp *qp = container_of(ref, struct erdma_qp, ref);\n\n\tcomplete(&qp->safe_free);\n}\n\nvoid erdma_qp_put(struct erdma_qp *qp)\n{\n\tWARN_ON(kref_read(&qp->ref) < 1);\n\tkref_put(&qp->ref, erdma_qp_safe_free);\n}\n\nvoid erdma_qp_get(struct erdma_qp *qp)\n{\n\tkref_get(&qp->ref);\n}\n\nstatic int fill_inline_data(struct erdma_qp *qp,\n\t\t\t    const struct ib_send_wr *send_wr, u16 wqe_idx,\n\t\t\t    u32 sgl_offset, __le32 *length_field)\n{\n\tu32 remain_size, copy_size, data_off, bytes = 0;\n\tchar *data;\n\tint i = 0;\n\n\twqe_idx += (sgl_offset >> SQEBB_SHIFT);\n\tsgl_offset &= (SQEBB_SIZE - 1);\n\tdata = get_queue_entry(qp->kern_qp.sq_buf, wqe_idx, qp->attrs.sq_size,\n\t\t\t       SQEBB_SHIFT);\n\n\twhile (i < send_wr->num_sge) {\n\t\tbytes += send_wr->sg_list[i].length;\n\t\tif (bytes > (int)ERDMA_MAX_INLINE)\n\t\t\treturn -EINVAL;\n\n\t\tremain_size = send_wr->sg_list[i].length;\n\t\tdata_off = 0;\n\n\t\twhile (1) {\n\t\t\tcopy_size = min(remain_size, SQEBB_SIZE - sgl_offset);\n\n\t\t\tmemcpy(data + sgl_offset,\n\t\t\t       (void *)(uintptr_t)send_wr->sg_list[i].addr +\n\t\t\t\t       data_off,\n\t\t\t       copy_size);\n\t\t\tremain_size -= copy_size;\n\t\t\tdata_off += copy_size;\n\t\t\tsgl_offset += copy_size;\n\t\t\twqe_idx += (sgl_offset >> SQEBB_SHIFT);\n\t\t\tsgl_offset &= (SQEBB_SIZE - 1);\n\n\t\t\tdata = get_queue_entry(qp->kern_qp.sq_buf, wqe_idx,\n\t\t\t\t\t       qp->attrs.sq_size, SQEBB_SHIFT);\n\t\t\tif (!remain_size)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ti++;\n\t}\n\t*length_field = cpu_to_le32(bytes);\n\n\treturn bytes;\n}\n\nstatic int fill_sgl(struct erdma_qp *qp, const struct ib_send_wr *send_wr,\n\t\t    u16 wqe_idx, u32 sgl_offset, __le32 *length_field)\n{\n\tint i = 0;\n\tu32 bytes = 0;\n\tchar *sgl;\n\n\tif (send_wr->num_sge > qp->dev->attrs.max_send_sge)\n\t\treturn -EINVAL;\n\n\tif (sgl_offset & 0xF)\n\t\treturn -EINVAL;\n\n\twhile (i < send_wr->num_sge) {\n\t\twqe_idx += (sgl_offset >> SQEBB_SHIFT);\n\t\tsgl_offset &= (SQEBB_SIZE - 1);\n\t\tsgl = get_queue_entry(qp->kern_qp.sq_buf, wqe_idx,\n\t\t\t\t      qp->attrs.sq_size, SQEBB_SHIFT);\n\n\t\tbytes += send_wr->sg_list[i].length;\n\t\tmemcpy(sgl + sgl_offset, &send_wr->sg_list[i],\n\t\t       sizeof(struct ib_sge));\n\n\t\tsgl_offset += sizeof(struct ib_sge);\n\t\ti++;\n\t}\n\n\t*length_field = cpu_to_le32(bytes);\n\treturn 0;\n}\n\nstatic int erdma_push_one_sqe(struct erdma_qp *qp, u16 *pi,\n\t\t\t      const struct ib_send_wr *send_wr)\n{\n\tu32 wqe_size, wqebb_cnt, hw_op, flags, sgl_offset;\n\tu32 idx = *pi & (qp->attrs.sq_size - 1);\n\tenum ib_wr_opcode op = send_wr->opcode;\n\tstruct erdma_atomic_sqe *atomic_sqe;\n\tstruct erdma_readreq_sqe *read_sqe;\n\tstruct erdma_reg_mr_sqe *regmr_sge;\n\tstruct erdma_write_sqe *write_sqe;\n\tstruct erdma_send_sqe *send_sqe;\n\tstruct ib_rdma_wr *rdma_wr;\n\tstruct erdma_sge *sge;\n\t__le32 *length_field;\n\tstruct erdma_mr *mr;\n\tu64 wqe_hdr, *entry;\n\tu32 attrs;\n\tint ret;\n\n\tentry = get_queue_entry(qp->kern_qp.sq_buf, idx, qp->attrs.sq_size,\n\t\t\t\tSQEBB_SHIFT);\n\n\t \n\t*entry = 0;\n\n\tqp->kern_qp.swr_tbl[idx] = send_wr->wr_id;\n\tflags = send_wr->send_flags;\n\twqe_hdr = FIELD_PREP(\n\t\tERDMA_SQE_HDR_CE_MASK,\n\t\t((flags & IB_SEND_SIGNALED) || qp->kern_qp.sig_all) ? 1 : 0);\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_SE_MASK,\n\t\t\t      flags & IB_SEND_SOLICITED ? 1 : 0);\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_FENCE_MASK,\n\t\t\t      flags & IB_SEND_FENCE ? 1 : 0);\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_INLINE_MASK,\n\t\t\t      flags & IB_SEND_INLINE ? 1 : 0);\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_QPN_MASK, QP_ID(qp));\n\n\tswitch (op) {\n\tcase IB_WR_RDMA_WRITE:\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\thw_op = ERDMA_OP_WRITE;\n\t\tif (op == IB_WR_RDMA_WRITE_WITH_IMM)\n\t\t\thw_op = ERDMA_OP_WRITE_WITH_IMM;\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, hw_op);\n\t\trdma_wr = container_of(send_wr, struct ib_rdma_wr, wr);\n\t\twrite_sqe = (struct erdma_write_sqe *)entry;\n\n\t\twrite_sqe->imm_data = send_wr->ex.imm_data;\n\t\twrite_sqe->sink_stag = cpu_to_le32(rdma_wr->rkey);\n\t\twrite_sqe->sink_to_h =\n\t\t\tcpu_to_le32(upper_32_bits(rdma_wr->remote_addr));\n\t\twrite_sqe->sink_to_l =\n\t\t\tcpu_to_le32(lower_32_bits(rdma_wr->remote_addr));\n\n\t\tlength_field = &write_sqe->length;\n\t\twqe_size = sizeof(struct erdma_write_sqe);\n\t\tsgl_offset = wqe_size;\n\t\tbreak;\n\tcase IB_WR_RDMA_READ:\n\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\tread_sqe = (struct erdma_readreq_sqe *)entry;\n\t\tif (unlikely(send_wr->num_sge != 1))\n\t\t\treturn -EINVAL;\n\t\thw_op = ERDMA_OP_READ;\n\t\tif (op == IB_WR_RDMA_READ_WITH_INV) {\n\t\t\thw_op = ERDMA_OP_READ_WITH_INV;\n\t\t\tread_sqe->invalid_stag =\n\t\t\t\tcpu_to_le32(send_wr->ex.invalidate_rkey);\n\t\t}\n\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, hw_op);\n\t\trdma_wr = container_of(send_wr, struct ib_rdma_wr, wr);\n\t\tread_sqe->length = cpu_to_le32(send_wr->sg_list[0].length);\n\t\tread_sqe->sink_stag = cpu_to_le32(send_wr->sg_list[0].lkey);\n\t\tread_sqe->sink_to_l =\n\t\t\tcpu_to_le32(lower_32_bits(send_wr->sg_list[0].addr));\n\t\tread_sqe->sink_to_h =\n\t\t\tcpu_to_le32(upper_32_bits(send_wr->sg_list[0].addr));\n\n\t\tsge = get_queue_entry(qp->kern_qp.sq_buf, idx + 1,\n\t\t\t\t      qp->attrs.sq_size, SQEBB_SHIFT);\n\t\tsge->addr = cpu_to_le64(rdma_wr->remote_addr);\n\t\tsge->key = cpu_to_le32(rdma_wr->rkey);\n\t\tsge->length = cpu_to_le32(send_wr->sg_list[0].length);\n\t\twqe_size = sizeof(struct erdma_readreq_sqe) +\n\t\t\t   send_wr->num_sge * sizeof(struct ib_sge);\n\n\t\tgoto out;\n\tcase IB_WR_SEND:\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_SEND_WITH_INV:\n\t\tsend_sqe = (struct erdma_send_sqe *)entry;\n\t\thw_op = ERDMA_OP_SEND;\n\t\tif (op == IB_WR_SEND_WITH_IMM) {\n\t\t\thw_op = ERDMA_OP_SEND_WITH_IMM;\n\t\t\tsend_sqe->imm_data = send_wr->ex.imm_data;\n\t\t} else if (op == IB_WR_SEND_WITH_INV) {\n\t\t\thw_op = ERDMA_OP_SEND_WITH_INV;\n\t\t\tsend_sqe->invalid_stag =\n\t\t\t\tcpu_to_le32(send_wr->ex.invalidate_rkey);\n\t\t}\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, hw_op);\n\t\tlength_field = &send_sqe->length;\n\t\twqe_size = sizeof(struct erdma_send_sqe);\n\t\tsgl_offset = wqe_size;\n\n\t\tbreak;\n\tcase IB_WR_REG_MR:\n\t\twqe_hdr |=\n\t\t\tFIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, ERDMA_OP_REG_MR);\n\t\tregmr_sge = (struct erdma_reg_mr_sqe *)entry;\n\t\tmr = to_emr(reg_wr(send_wr)->mr);\n\n\t\tmr->access = ERDMA_MR_ACC_LR |\n\t\t\t     to_erdma_access_flags(reg_wr(send_wr)->access);\n\t\tregmr_sge->addr = cpu_to_le64(mr->ibmr.iova);\n\t\tregmr_sge->length = cpu_to_le32(mr->ibmr.length);\n\t\tregmr_sge->stag = cpu_to_le32(reg_wr(send_wr)->key);\n\t\tattrs = FIELD_PREP(ERDMA_SQE_MR_ACCESS_MASK, mr->access) |\n\t\t\tFIELD_PREP(ERDMA_SQE_MR_MTT_CNT_MASK,\n\t\t\t\t   mr->mem.mtt_nents);\n\n\t\tif (mr->mem.mtt_nents <= ERDMA_MAX_INLINE_MTT_ENTRIES) {\n\t\t\tattrs |= FIELD_PREP(ERDMA_SQE_MR_MTT_TYPE_MASK, 0);\n\t\t\t \n\t\t\tmemcpy(get_queue_entry(qp->kern_qp.sq_buf, idx + 1,\n\t\t\t\t\t       qp->attrs.sq_size, SQEBB_SHIFT),\n\t\t\t       mr->mem.mtt->buf, MTT_SIZE(mr->mem.mtt_nents));\n\t\t\twqe_size = sizeof(struct erdma_reg_mr_sqe) +\n\t\t\t\t   MTT_SIZE(mr->mem.mtt_nents);\n\t\t} else {\n\t\t\tattrs |= FIELD_PREP(ERDMA_SQE_MR_MTT_TYPE_MASK, 1);\n\t\t\twqe_size = sizeof(struct erdma_reg_mr_sqe);\n\t\t}\n\n\t\tregmr_sge->attrs = cpu_to_le32(attrs);\n\t\tgoto out;\n\tcase IB_WR_LOCAL_INV:\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK,\n\t\t\t\t      ERDMA_OP_LOCAL_INV);\n\t\tregmr_sge = (struct erdma_reg_mr_sqe *)entry;\n\t\tregmr_sge->stag = cpu_to_le32(send_wr->ex.invalidate_rkey);\n\t\twqe_size = sizeof(struct erdma_reg_mr_sqe);\n\t\tgoto out;\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\tatomic_sqe = (struct erdma_atomic_sqe *)entry;\n\t\tif (op == IB_WR_ATOMIC_CMP_AND_SWP) {\n\t\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK,\n\t\t\t\t\t      ERDMA_OP_ATOMIC_CAS);\n\t\t\tatomic_sqe->fetchadd_swap_data =\n\t\t\t\tcpu_to_le64(atomic_wr(send_wr)->swap);\n\t\t\tatomic_sqe->cmp_data =\n\t\t\t\tcpu_to_le64(atomic_wr(send_wr)->compare_add);\n\t\t} else {\n\t\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK,\n\t\t\t\t\t      ERDMA_OP_ATOMIC_FAA);\n\t\t\tatomic_sqe->fetchadd_swap_data =\n\t\t\t\tcpu_to_le64(atomic_wr(send_wr)->compare_add);\n\t\t}\n\n\t\tsge = get_queue_entry(qp->kern_qp.sq_buf, idx + 1,\n\t\t\t\t      qp->attrs.sq_size, SQEBB_SHIFT);\n\t\tsge->addr = cpu_to_le64(atomic_wr(send_wr)->remote_addr);\n\t\tsge->key = cpu_to_le32(atomic_wr(send_wr)->rkey);\n\t\tsge++;\n\n\t\tsge->addr = cpu_to_le64(send_wr->sg_list[0].addr);\n\t\tsge->key = cpu_to_le32(send_wr->sg_list[0].lkey);\n\t\tsge->length = cpu_to_le32(send_wr->sg_list[0].length);\n\n\t\twqe_size = sizeof(*atomic_sqe);\n\t\tgoto out;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (flags & IB_SEND_INLINE) {\n\t\tret = fill_inline_data(qp, send_wr, idx, sgl_offset,\n\t\t\t\t       length_field);\n\t\tif (ret < 0)\n\t\t\treturn -EINVAL;\n\t\twqe_size += ret;\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_SGL_LEN_MASK, ret);\n\t} else {\n\t\tret = fill_sgl(qp, send_wr, idx, sgl_offset, length_field);\n\t\tif (ret)\n\t\t\treturn -EINVAL;\n\t\twqe_size += send_wr->num_sge * sizeof(struct ib_sge);\n\t\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_SGL_LEN_MASK,\n\t\t\t\t      send_wr->num_sge);\n\t}\n\nout:\n\twqebb_cnt = SQEBB_COUNT(wqe_size);\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_WQEBB_CNT_MASK, wqebb_cnt - 1);\n\t*pi += wqebb_cnt;\n\twqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_WQEBB_INDEX_MASK, *pi);\n\n\t*entry = wqe_hdr;\n\n\treturn 0;\n}\n\nstatic void kick_sq_db(struct erdma_qp *qp, u16 pi)\n{\n\tu64 db_data = FIELD_PREP(ERDMA_SQE_HDR_QPN_MASK, QP_ID(qp)) |\n\t\t      FIELD_PREP(ERDMA_SQE_HDR_WQEBB_INDEX_MASK, pi);\n\n\t*(u64 *)qp->kern_qp.sq_db_info = db_data;\n\twriteq(db_data, qp->kern_qp.hw_sq_db);\n}\n\nint erdma_post_send(struct ib_qp *ibqp, const struct ib_send_wr *send_wr,\n\t\t    const struct ib_send_wr **bad_send_wr)\n{\n\tstruct erdma_qp *qp = to_eqp(ibqp);\n\tint ret = 0;\n\tconst struct ib_send_wr *wr = send_wr;\n\tunsigned long flags;\n\tu16 sq_pi;\n\n\tif (!send_wr)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&qp->lock, flags);\n\tsq_pi = qp->kern_qp.sq_pi;\n\n\twhile (wr) {\n\t\tif ((u16)(sq_pi - qp->kern_qp.sq_ci) >= qp->attrs.sq_size) {\n\t\t\tret = -ENOMEM;\n\t\t\t*bad_send_wr = send_wr;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = erdma_push_one_sqe(qp, &sq_pi, wr);\n\t\tif (ret) {\n\t\t\t*bad_send_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\tqp->kern_qp.sq_pi = sq_pi;\n\t\tkick_sq_db(qp, sq_pi);\n\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&qp->lock, flags);\n\n\tif (unlikely(qp->flags & ERDMA_QP_IN_FLUSHING))\n\t\tmod_delayed_work(qp->dev->reflush_wq, &qp->reflush_dwork,\n\t\t\t\t usecs_to_jiffies(100));\n\n\treturn ret;\n}\n\nstatic int erdma_post_recv_one(struct erdma_qp *qp,\n\t\t\t       const struct ib_recv_wr *recv_wr)\n{\n\tstruct erdma_rqe *rqe =\n\t\tget_queue_entry(qp->kern_qp.rq_buf, qp->kern_qp.rq_pi,\n\t\t\t\tqp->attrs.rq_size, RQE_SHIFT);\n\n\trqe->qe_idx = cpu_to_le16(qp->kern_qp.rq_pi + 1);\n\trqe->qpn = cpu_to_le32(QP_ID(qp));\n\n\tif (recv_wr->num_sge == 0) {\n\t\trqe->length = 0;\n\t} else if (recv_wr->num_sge == 1) {\n\t\trqe->stag = cpu_to_le32(recv_wr->sg_list[0].lkey);\n\t\trqe->to = cpu_to_le64(recv_wr->sg_list[0].addr);\n\t\trqe->length = cpu_to_le32(recv_wr->sg_list[0].length);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\t*(u64 *)qp->kern_qp.rq_db_info = *(u64 *)rqe;\n\twriteq(*(u64 *)rqe, qp->kern_qp.hw_rq_db);\n\n\tqp->kern_qp.rwr_tbl[qp->kern_qp.rq_pi & (qp->attrs.rq_size - 1)] =\n\t\trecv_wr->wr_id;\n\tqp->kern_qp.rq_pi++;\n\n\treturn 0;\n}\n\nint erdma_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *recv_wr,\n\t\t    const struct ib_recv_wr **bad_recv_wr)\n{\n\tconst struct ib_recv_wr *wr = recv_wr;\n\tstruct erdma_qp *qp = to_eqp(ibqp);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&qp->lock, flags);\n\n\twhile (wr) {\n\t\tret = erdma_post_recv_one(qp, wr);\n\t\tif (ret) {\n\t\t\t*bad_recv_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\n\tspin_unlock_irqrestore(&qp->lock, flags);\n\n\tif (unlikely(qp->flags & ERDMA_QP_IN_FLUSHING))\n\t\tmod_delayed_work(qp->dev->reflush_wq, &qp->reflush_dwork,\n\t\t\t\t usecs_to_jiffies(100));\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}