{
  "module_name": "erdma_verbs.h",
  "hash_id": "27ff6fc5fe96448df308bc365812633418274671b7f438f07b58001b257f3b39",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/hw/erdma/erdma_verbs.h",
  "human_readable_source": " \n\n \n \n \n\n#ifndef __ERDMA_VERBS_H__\n#define __ERDMA_VERBS_H__\n\n#include \"erdma.h\"\n\n \n#define ERDMA_MAX_PD (128 * 1024)\n#define ERDMA_MAX_SEND_WR 8192\n#define ERDMA_MAX_ORD 128\n#define ERDMA_MAX_IRD 128\n#define ERDMA_MAX_SGE_RD 1\n#define ERDMA_MAX_CONTEXT (128 * 1024)\n#define ERDMA_MAX_SEND_SGE 6\n#define ERDMA_MAX_RECV_SGE 1\n#define ERDMA_MAX_INLINE (sizeof(struct erdma_sge) * (ERDMA_MAX_SEND_SGE))\n#define ERDMA_MAX_FRMR_PA 512\n\nenum {\n\tERDMA_MMAP_IO_NC = 0,  \n};\n\nstruct erdma_user_mmap_entry {\n\tstruct rdma_user_mmap_entry rdma_entry;\n\tu64 address;\n\tu8 mmap_flag;\n};\n\nstruct erdma_ext_db_info {\n\tbool enable;\n\tu16 sdb_off;\n\tu16 rdb_off;\n\tu16 cdb_off;\n};\n\nstruct erdma_ucontext {\n\tstruct ib_ucontext ibucontext;\n\n\tstruct erdma_ext_db_info ext_db;\n\n\tu64 sdb;\n\tu64 rdb;\n\tu64 cdb;\n\n\tstruct rdma_user_mmap_entry *sq_db_mmap_entry;\n\tstruct rdma_user_mmap_entry *rq_db_mmap_entry;\n\tstruct rdma_user_mmap_entry *cq_db_mmap_entry;\n\n\t \n\tstruct list_head dbrecords_page_list;\n\tstruct mutex dbrecords_page_mutex;\n};\n\nstruct erdma_pd {\n\tstruct ib_pd ibpd;\n\tu32 pdn;\n};\n\n \n#define ERDMA_MAX_INLINE_MTT_ENTRIES 4\n#define MTT_SIZE(mtt_cnt) ((mtt_cnt) << 3)  \n#define ERDMA_MR_MAX_MTT_CNT 524288\n#define ERDMA_MTT_ENTRY_SIZE 8\n\n#define ERDMA_MR_TYPE_NORMAL 0\n#define ERDMA_MR_TYPE_FRMR 1\n#define ERDMA_MR_TYPE_DMA 2\n\n#define ERDMA_MR_MTT_0LEVEL 0\n#define ERDMA_MR_MTT_1LEVEL 1\n\n#define ERDMA_MR_ACC_RA BIT(0)\n#define ERDMA_MR_ACC_LR BIT(1)\n#define ERDMA_MR_ACC_LW BIT(2)\n#define ERDMA_MR_ACC_RR BIT(3)\n#define ERDMA_MR_ACC_RW BIT(4)\n\nstatic inline u8 to_erdma_access_flags(int access)\n{\n\treturn (access & IB_ACCESS_REMOTE_READ ? ERDMA_MR_ACC_RR : 0) |\n\t       (access & IB_ACCESS_LOCAL_WRITE ? ERDMA_MR_ACC_LW : 0) |\n\t       (access & IB_ACCESS_REMOTE_WRITE ? ERDMA_MR_ACC_RW : 0) |\n\t       (access & IB_ACCESS_REMOTE_ATOMIC ? ERDMA_MR_ACC_RA : 0);\n}\n\n \nstruct erdma_mtt {\n\tu64 *buf;\n\tsize_t size;\n\n\tbool continuous;\n\tunion {\n\t\tdma_addr_t buf_dma;\n\t\tstruct {\n\t\t\tstruct scatterlist *sglist;\n\t\t\tu32 nsg;\n\t\t\tu32 level;\n\t\t};\n\t};\n\n\tstruct erdma_mtt *low_level;\n};\n\nstruct erdma_mem {\n\tstruct ib_umem *umem;\n\tstruct erdma_mtt *mtt;\n\n\tu32 page_size;\n\tu32 page_offset;\n\tu32 page_cnt;\n\tu32 mtt_nents;\n\n\tu64 va;\n\tu64 len;\n};\n\nstruct erdma_mr {\n\tstruct ib_mr ibmr;\n\tstruct erdma_mem mem;\n\tu8 type;\n\tu8 access;\n\tu8 valid;\n};\n\nstruct erdma_user_dbrecords_page {\n\tstruct list_head list;\n\tstruct ib_umem *umem;\n\tu64 va;\n\tint refcnt;\n};\n\nstruct erdma_uqp {\n\tstruct erdma_mem sq_mem;\n\tstruct erdma_mem rq_mem;\n\n\tdma_addr_t sq_db_info_dma_addr;\n\tdma_addr_t rq_db_info_dma_addr;\n\n\tstruct erdma_user_dbrecords_page *user_dbr_page;\n\n\tu32 rq_offset;\n};\n\nstruct erdma_kqp {\n\tu16 sq_pi;\n\tu16 sq_ci;\n\n\tu16 rq_pi;\n\tu16 rq_ci;\n\n\tu64 *swr_tbl;\n\tu64 *rwr_tbl;\n\n\tvoid __iomem *hw_sq_db;\n\tvoid __iomem *hw_rq_db;\n\n\tvoid *sq_buf;\n\tdma_addr_t sq_buf_dma_addr;\n\n\tvoid *rq_buf;\n\tdma_addr_t rq_buf_dma_addr;\n\n\tvoid *sq_db_info;\n\tvoid *rq_db_info;\n\n\tu8 sig_all;\n};\n\nenum erdma_qp_state {\n\tERDMA_QP_STATE_IDLE = 0,\n\tERDMA_QP_STATE_RTR = 1,\n\tERDMA_QP_STATE_RTS = 2,\n\tERDMA_QP_STATE_CLOSING = 3,\n\tERDMA_QP_STATE_TERMINATE = 4,\n\tERDMA_QP_STATE_ERROR = 5,\n\tERDMA_QP_STATE_UNDEF = 7,\n\tERDMA_QP_STATE_COUNT = 8\n};\n\nenum erdma_qp_attr_mask {\n\tERDMA_QP_ATTR_STATE = (1 << 0),\n\tERDMA_QP_ATTR_LLP_HANDLE = (1 << 2),\n\tERDMA_QP_ATTR_ORD = (1 << 3),\n\tERDMA_QP_ATTR_IRD = (1 << 4),\n\tERDMA_QP_ATTR_SQ_SIZE = (1 << 5),\n\tERDMA_QP_ATTR_RQ_SIZE = (1 << 6),\n\tERDMA_QP_ATTR_MPA = (1 << 7)\n};\n\nenum erdma_qp_flags {\n\tERDMA_QP_IN_FLUSHING = (1 << 0),\n};\n\nstruct erdma_qp_attrs {\n\tenum erdma_qp_state state;\n\tenum erdma_cc_alg cc;  \n\tu32 sq_size;\n\tu32 rq_size;\n\tu32 orq_size;\n\tu32 irq_size;\n\tu32 max_send_sge;\n\tu32 max_recv_sge;\n\tu32 cookie;\n#define ERDMA_QP_ACTIVE 0\n#define ERDMA_QP_PASSIVE 1\n\tu8 qp_type;\n\tu8 pd_len;\n};\n\nstruct erdma_qp {\n\tstruct ib_qp ibqp;\n\tstruct kref ref;\n\tstruct completion safe_free;\n\tstruct erdma_dev *dev;\n\tstruct erdma_cep *cep;\n\tstruct rw_semaphore state_lock;\n\n\tunsigned long flags;\n\tstruct delayed_work reflush_dwork;\n\n\tunion {\n\t\tstruct erdma_kqp kern_qp;\n\t\tstruct erdma_uqp user_qp;\n\t};\n\n\tstruct erdma_cq *scq;\n\tstruct erdma_cq *rcq;\n\n\tstruct erdma_qp_attrs attrs;\n\tspinlock_t lock;\n};\n\nstruct erdma_kcq_info {\n\tvoid *qbuf;\n\tdma_addr_t qbuf_dma_addr;\n\tu32 ci;\n\tu32 cmdsn;\n\tu32 notify_cnt;\n\n\tspinlock_t lock;\n\tu8 __iomem *db;\n\tu64 *db_record;\n};\n\nstruct erdma_ucq_info {\n\tstruct erdma_mem qbuf_mem;\n\tstruct erdma_user_dbrecords_page *user_dbr_page;\n\tdma_addr_t db_info_dma_addr;\n};\n\nstruct erdma_cq {\n\tstruct ib_cq ibcq;\n\tu32 cqn;\n\n\tu32 depth;\n\tu32 assoc_eqn;\n\n\tunion {\n\t\tstruct erdma_kcq_info kern_cq;\n\t\tstruct erdma_ucq_info user_cq;\n\t};\n};\n\n#define QP_ID(qp) ((qp)->ibqp.qp_num)\n\nstatic inline struct erdma_qp *find_qp_by_qpn(struct erdma_dev *dev, int id)\n{\n\treturn (struct erdma_qp *)xa_load(&dev->qp_xa, id);\n}\n\nstatic inline struct erdma_cq *find_cq_by_cqn(struct erdma_dev *dev, int id)\n{\n\treturn (struct erdma_cq *)xa_load(&dev->cq_xa, id);\n}\n\nvoid erdma_qp_get(struct erdma_qp *qp);\nvoid erdma_qp_put(struct erdma_qp *qp);\nint erdma_modify_qp_internal(struct erdma_qp *qp, struct erdma_qp_attrs *attrs,\n\t\t\t     enum erdma_qp_attr_mask mask);\nvoid erdma_qp_llp_close(struct erdma_qp *qp);\nvoid erdma_qp_cm_drop(struct erdma_qp *qp);\n\nstatic inline struct erdma_ucontext *to_ectx(struct ib_ucontext *ibctx)\n{\n\treturn container_of(ibctx, struct erdma_ucontext, ibucontext);\n}\n\nstatic inline struct erdma_pd *to_epd(struct ib_pd *pd)\n{\n\treturn container_of(pd, struct erdma_pd, ibpd);\n}\n\nstatic inline struct erdma_mr *to_emr(struct ib_mr *ibmr)\n{\n\treturn container_of(ibmr, struct erdma_mr, ibmr);\n}\n\nstatic inline struct erdma_qp *to_eqp(struct ib_qp *qp)\n{\n\treturn container_of(qp, struct erdma_qp, ibqp);\n}\n\nstatic inline struct erdma_cq *to_ecq(struct ib_cq *ibcq)\n{\n\treturn container_of(ibcq, struct erdma_cq, ibcq);\n}\n\nstatic inline struct erdma_user_mmap_entry *\nto_emmap(struct rdma_user_mmap_entry *ibmmap)\n{\n\treturn container_of(ibmmap, struct erdma_user_mmap_entry, rdma_entry);\n}\n\nint erdma_alloc_ucontext(struct ib_ucontext *ibctx, struct ib_udata *data);\nvoid erdma_dealloc_ucontext(struct ib_ucontext *ibctx);\nint erdma_query_device(struct ib_device *dev, struct ib_device_attr *attr,\n\t\t       struct ib_udata *data);\nint erdma_get_port_immutable(struct ib_device *dev, u32 port,\n\t\t\t     struct ib_port_immutable *ib_port_immutable);\nint erdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t    struct ib_udata *data);\nint erdma_query_port(struct ib_device *dev, u32 port,\n\t\t     struct ib_port_attr *attr);\nint erdma_query_gid(struct ib_device *dev, u32 port, int idx,\n\t\t    union ib_gid *gid);\nint erdma_alloc_pd(struct ib_pd *ibpd, struct ib_udata *data);\nint erdma_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata);\nint erdma_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *attr,\n\t\t    struct ib_udata *data);\nint erdma_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int mask,\n\t\t   struct ib_qp_init_attr *init_attr);\nint erdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int mask,\n\t\t    struct ib_udata *data);\nint erdma_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata);\nint erdma_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata);\nint erdma_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);\nstruct ib_mr *erdma_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 len,\n\t\t\t\tu64 virt, int access, struct ib_udata *udata);\nstruct ib_mr *erdma_get_dma_mr(struct ib_pd *ibpd, int rights);\nint erdma_dereg_mr(struct ib_mr *ibmr, struct ib_udata *data);\nint erdma_mmap(struct ib_ucontext *ctx, struct vm_area_struct *vma);\nvoid erdma_mmap_free(struct rdma_user_mmap_entry *rdma_entry);\nvoid erdma_qp_get_ref(struct ib_qp *ibqp);\nvoid erdma_qp_put_ref(struct ib_qp *ibqp);\nstruct ib_qp *erdma_get_ibqp(struct ib_device *dev, int id);\nint erdma_post_send(struct ib_qp *ibqp, const struct ib_send_wr *send_wr,\n\t\t    const struct ib_send_wr **bad_send_wr);\nint erdma_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *recv_wr,\n\t\t    const struct ib_recv_wr **bad_recv_wr);\nint erdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);\nstruct ib_mr *erdma_ib_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,\n\t\t\t\tu32 max_num_sg);\nint erdma_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,\n\t\t    unsigned int *sg_offset);\nvoid erdma_port_event(struct erdma_dev *dev, enum ib_event_type reason);\nvoid erdma_set_mtu(struct erdma_dev *dev, u32 mtu);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}