{
  "module_name": "rw.c",
  "hash_id": "f2cecded832a9f34b62bca947283b528d89554e32457b20bf3436c4083d9a286",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/rw.c",
  "human_readable_source": "\n \n#include <linux/memremap.h>\n#include <linux/moduleparam.h>\n#include <linux/slab.h>\n#include <linux/pci-p2pdma.h>\n#include <rdma/mr_pool.h>\n#include <rdma/rw.h>\n\nenum {\n\tRDMA_RW_SINGLE_WR,\n\tRDMA_RW_MULTI_WR,\n\tRDMA_RW_MR,\n\tRDMA_RW_SIG_MR,\n};\n\nstatic bool rdma_rw_force_mr;\nmodule_param_named(force_mr, rdma_rw_force_mr, bool, 0);\nMODULE_PARM_DESC(force_mr, \"Force usage of MRs for RDMA READ/WRITE operations\");\n\n \nstatic inline bool rdma_rw_can_use_mr(struct ib_device *dev, u32 port_num)\n{\n\tif (rdma_protocol_iwarp(dev, port_num))\n\t\treturn true;\n\tif (dev->attrs.max_sgl_rd)\n\t\treturn true;\n\tif (unlikely(rdma_rw_force_mr))\n\t\treturn true;\n\treturn false;\n}\n\n \nstatic inline bool rdma_rw_io_needs_mr(struct ib_device *dev, u32 port_num,\n\t\tenum dma_data_direction dir, int dma_nents)\n{\n\tif (dir == DMA_FROM_DEVICE) {\n\t\tif (rdma_protocol_iwarp(dev, port_num))\n\t\t\treturn true;\n\t\tif (dev->attrs.max_sgl_rd && dma_nents > dev->attrs.max_sgl_rd)\n\t\t\treturn true;\n\t}\n\tif (unlikely(rdma_rw_force_mr))\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline u32 rdma_rw_fr_page_list_len(struct ib_device *dev,\n\t\t\t\t\t   bool pi_support)\n{\n\tu32 max_pages;\n\n\tif (pi_support)\n\t\tmax_pages = dev->attrs.max_pi_fast_reg_page_list_len;\n\telse\n\t\tmax_pages = dev->attrs.max_fast_reg_page_list_len;\n\n\t \n\treturn min_t(u32, max_pages, 256);\n}\n\nstatic inline int rdma_rw_inv_key(struct rdma_rw_reg_ctx *reg)\n{\n\tint count = 0;\n\n\tif (reg->mr->need_inval) {\n\t\treg->inv_wr.opcode = IB_WR_LOCAL_INV;\n\t\treg->inv_wr.ex.invalidate_rkey = reg->mr->lkey;\n\t\treg->inv_wr.next = &reg->reg_wr.wr;\n\t\tcount++;\n\t} else {\n\t\treg->inv_wr.next = NULL;\n\t}\n\n\treturn count;\n}\n\n \nstatic int rdma_rw_init_one_mr(struct ib_qp *qp, u32 port_num,\n\t\tstruct rdma_rw_reg_ctx *reg, struct scatterlist *sg,\n\t\tu32 sg_cnt, u32 offset)\n{\n\tu32 pages_per_mr = rdma_rw_fr_page_list_len(qp->pd->device,\n\t\t\t\t\t\t    qp->integrity_en);\n\tu32 nents = min(sg_cnt, pages_per_mr);\n\tint count = 0, ret;\n\n\treg->mr = ib_mr_pool_get(qp, &qp->rdma_mrs);\n\tif (!reg->mr)\n\t\treturn -EAGAIN;\n\n\tcount += rdma_rw_inv_key(reg);\n\n\tret = ib_map_mr_sg(reg->mr, sg, nents, &offset, PAGE_SIZE);\n\tif (ret < 0 || ret < nents) {\n\t\tib_mr_pool_put(qp, &qp->rdma_mrs, reg->mr);\n\t\treturn -EINVAL;\n\t}\n\n\treg->reg_wr.wr.opcode = IB_WR_REG_MR;\n\treg->reg_wr.mr = reg->mr;\n\treg->reg_wr.access = IB_ACCESS_LOCAL_WRITE;\n\tif (rdma_protocol_iwarp(qp->device, port_num))\n\t\treg->reg_wr.access |= IB_ACCESS_REMOTE_WRITE;\n\tcount++;\n\n\treg->sge.addr = reg->mr->iova;\n\treg->sge.length = reg->mr->length;\n\treturn count;\n}\n\nstatic int rdma_rw_init_mr_wrs(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tu32 port_num, struct scatterlist *sg, u32 sg_cnt, u32 offset,\n\t\tu64 remote_addr, u32 rkey, enum dma_data_direction dir)\n{\n\tstruct rdma_rw_reg_ctx *prev = NULL;\n\tu32 pages_per_mr = rdma_rw_fr_page_list_len(qp->pd->device,\n\t\t\t\t\t\t    qp->integrity_en);\n\tint i, j, ret = 0, count = 0;\n\n\tctx->nr_ops = DIV_ROUND_UP(sg_cnt, pages_per_mr);\n\tctx->reg = kcalloc(ctx->nr_ops, sizeof(*ctx->reg), GFP_KERNEL);\n\tif (!ctx->reg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < ctx->nr_ops; i++) {\n\t\tstruct rdma_rw_reg_ctx *reg = &ctx->reg[i];\n\t\tu32 nents = min(sg_cnt, pages_per_mr);\n\n\t\tret = rdma_rw_init_one_mr(qp, port_num, reg, sg, sg_cnt,\n\t\t\t\toffset);\n\t\tif (ret < 0)\n\t\t\tgoto out_free;\n\t\tcount += ret;\n\n\t\tif (prev) {\n\t\t\tif (reg->mr->need_inval)\n\t\t\t\tprev->wr.wr.next = &reg->inv_wr;\n\t\t\telse\n\t\t\t\tprev->wr.wr.next = &reg->reg_wr.wr;\n\t\t}\n\n\t\treg->reg_wr.wr.next = &reg->wr.wr;\n\n\t\treg->wr.wr.sg_list = &reg->sge;\n\t\treg->wr.wr.num_sge = 1;\n\t\treg->wr.remote_addr = remote_addr;\n\t\treg->wr.rkey = rkey;\n\t\tif (dir == DMA_TO_DEVICE) {\n\t\t\treg->wr.wr.opcode = IB_WR_RDMA_WRITE;\n\t\t} else if (!rdma_cap_read_inv(qp->device, port_num)) {\n\t\t\treg->wr.wr.opcode = IB_WR_RDMA_READ;\n\t\t} else {\n\t\t\treg->wr.wr.opcode = IB_WR_RDMA_READ_WITH_INV;\n\t\t\treg->wr.wr.ex.invalidate_rkey = reg->mr->lkey;\n\t\t}\n\t\tcount++;\n\n\t\tremote_addr += reg->sge.length;\n\t\tsg_cnt -= nents;\n\t\tfor (j = 0; j < nents; j++)\n\t\t\tsg = sg_next(sg);\n\t\tprev = reg;\n\t\toffset = 0;\n\t}\n\n\tif (prev)\n\t\tprev->wr.wr.next = NULL;\n\n\tctx->type = RDMA_RW_MR;\n\treturn count;\n\nout_free:\n\twhile (--i >= 0)\n\t\tib_mr_pool_put(qp, &qp->rdma_mrs, ctx->reg[i].mr);\n\tkfree(ctx->reg);\nout:\n\treturn ret;\n}\n\nstatic int rdma_rw_init_map_wrs(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tstruct scatterlist *sg, u32 sg_cnt, u32 offset,\n\t\tu64 remote_addr, u32 rkey, enum dma_data_direction dir)\n{\n\tu32 max_sge = dir == DMA_TO_DEVICE ? qp->max_write_sge :\n\t\t      qp->max_read_sge;\n\tstruct ib_sge *sge;\n\tu32 total_len = 0, i, j;\n\n\tctx->nr_ops = DIV_ROUND_UP(sg_cnt, max_sge);\n\n\tctx->map.sges = sge = kcalloc(sg_cnt, sizeof(*sge), GFP_KERNEL);\n\tif (!ctx->map.sges)\n\t\tgoto out;\n\n\tctx->map.wrs = kcalloc(ctx->nr_ops, sizeof(*ctx->map.wrs), GFP_KERNEL);\n\tif (!ctx->map.wrs)\n\t\tgoto out_free_sges;\n\n\tfor (i = 0; i < ctx->nr_ops; i++) {\n\t\tstruct ib_rdma_wr *rdma_wr = &ctx->map.wrs[i];\n\t\tu32 nr_sge = min(sg_cnt, max_sge);\n\n\t\tif (dir == DMA_TO_DEVICE)\n\t\t\trdma_wr->wr.opcode = IB_WR_RDMA_WRITE;\n\t\telse\n\t\t\trdma_wr->wr.opcode = IB_WR_RDMA_READ;\n\t\trdma_wr->remote_addr = remote_addr + total_len;\n\t\trdma_wr->rkey = rkey;\n\t\trdma_wr->wr.num_sge = nr_sge;\n\t\trdma_wr->wr.sg_list = sge;\n\n\t\tfor (j = 0; j < nr_sge; j++, sg = sg_next(sg)) {\n\t\t\tsge->addr = sg_dma_address(sg) + offset;\n\t\t\tsge->length = sg_dma_len(sg) - offset;\n\t\t\tsge->lkey = qp->pd->local_dma_lkey;\n\n\t\t\ttotal_len += sge->length;\n\t\t\tsge++;\n\t\t\tsg_cnt--;\n\t\t\toffset = 0;\n\t\t}\n\n\t\trdma_wr->wr.next = i + 1 < ctx->nr_ops ?\n\t\t\t&ctx->map.wrs[i + 1].wr : NULL;\n\t}\n\n\tctx->type = RDMA_RW_MULTI_WR;\n\treturn ctx->nr_ops;\n\nout_free_sges:\n\tkfree(ctx->map.sges);\nout:\n\treturn -ENOMEM;\n}\n\nstatic int rdma_rw_init_single_wr(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tstruct scatterlist *sg, u32 offset, u64 remote_addr, u32 rkey,\n\t\tenum dma_data_direction dir)\n{\n\tstruct ib_rdma_wr *rdma_wr = &ctx->single.wr;\n\n\tctx->nr_ops = 1;\n\n\tctx->single.sge.lkey = qp->pd->local_dma_lkey;\n\tctx->single.sge.addr = sg_dma_address(sg) + offset;\n\tctx->single.sge.length = sg_dma_len(sg) - offset;\n\n\tmemset(rdma_wr, 0, sizeof(*rdma_wr));\n\tif (dir == DMA_TO_DEVICE)\n\t\trdma_wr->wr.opcode = IB_WR_RDMA_WRITE;\n\telse\n\t\trdma_wr->wr.opcode = IB_WR_RDMA_READ;\n\trdma_wr->wr.sg_list = &ctx->single.sge;\n\trdma_wr->wr.num_sge = 1;\n\trdma_wr->remote_addr = remote_addr;\n\trdma_wr->rkey = rkey;\n\n\tctx->type = RDMA_RW_SINGLE_WR;\n\treturn 1;\n}\n\n \nint rdma_rw_ctx_init(struct rdma_rw_ctx *ctx, struct ib_qp *qp, u32 port_num,\n\t\tstruct scatterlist *sg, u32 sg_cnt, u32 sg_offset,\n\t\tu64 remote_addr, u32 rkey, enum dma_data_direction dir)\n{\n\tstruct ib_device *dev = qp->pd->device;\n\tstruct sg_table sgt = {\n\t\t.sgl = sg,\n\t\t.orig_nents = sg_cnt,\n\t};\n\tint ret;\n\n\tret = ib_dma_map_sgtable_attrs(dev, &sgt, dir, 0);\n\tif (ret)\n\t\treturn ret;\n\tsg_cnt = sgt.nents;\n\n\t \n\tfor (;;) {\n\t\tu32 len = sg_dma_len(sg);\n\n\t\tif (sg_offset < len)\n\t\t\tbreak;\n\n\t\tsg = sg_next(sg);\n\t\tsg_offset -= len;\n\t\tsg_cnt--;\n\t}\n\n\tret = -EIO;\n\tif (WARN_ON_ONCE(sg_cnt == 0))\n\t\tgoto out_unmap_sg;\n\n\tif (rdma_rw_io_needs_mr(qp->device, port_num, dir, sg_cnt)) {\n\t\tret = rdma_rw_init_mr_wrs(ctx, qp, port_num, sg, sg_cnt,\n\t\t\t\tsg_offset, remote_addr, rkey, dir);\n\t} else if (sg_cnt > 1) {\n\t\tret = rdma_rw_init_map_wrs(ctx, qp, sg, sg_cnt, sg_offset,\n\t\t\t\tremote_addr, rkey, dir);\n\t} else {\n\t\tret = rdma_rw_init_single_wr(ctx, qp, sg, sg_offset,\n\t\t\t\tremote_addr, rkey, dir);\n\t}\n\n\tif (ret < 0)\n\t\tgoto out_unmap_sg;\n\treturn ret;\n\nout_unmap_sg:\n\tib_dma_unmap_sgtable_attrs(dev, &sgt, dir, 0);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_rw_ctx_init);\n\n \nint rdma_rw_ctx_signature_init(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tu32 port_num, struct scatterlist *sg, u32 sg_cnt,\n\t\tstruct scatterlist *prot_sg, u32 prot_sg_cnt,\n\t\tstruct ib_sig_attrs *sig_attrs,\n\t\tu64 remote_addr, u32 rkey, enum dma_data_direction dir)\n{\n\tstruct ib_device *dev = qp->pd->device;\n\tu32 pages_per_mr = rdma_rw_fr_page_list_len(qp->pd->device,\n\t\t\t\t\t\t    qp->integrity_en);\n\tstruct sg_table sgt = {\n\t\t.sgl = sg,\n\t\t.orig_nents = sg_cnt,\n\t};\n\tstruct sg_table prot_sgt = {\n\t\t.sgl = prot_sg,\n\t\t.orig_nents = prot_sg_cnt,\n\t};\n\tstruct ib_rdma_wr *rdma_wr;\n\tint count = 0, ret;\n\n\tif (sg_cnt > pages_per_mr || prot_sg_cnt > pages_per_mr) {\n\t\tpr_err(\"SG count too large: sg_cnt=%u, prot_sg_cnt=%u, pages_per_mr=%u\\n\",\n\t\t       sg_cnt, prot_sg_cnt, pages_per_mr);\n\t\treturn -EINVAL;\n\t}\n\n\tret = ib_dma_map_sgtable_attrs(dev, &sgt, dir, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tif (prot_sg_cnt) {\n\t\tret = ib_dma_map_sgtable_attrs(dev, &prot_sgt, dir, 0);\n\t\tif (ret)\n\t\t\tgoto out_unmap_sg;\n\t}\n\n\tctx->type = RDMA_RW_SIG_MR;\n\tctx->nr_ops = 1;\n\tctx->reg = kzalloc(sizeof(*ctx->reg), GFP_KERNEL);\n\tif (!ctx->reg) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unmap_prot_sg;\n\t}\n\n\tctx->reg->mr = ib_mr_pool_get(qp, &qp->sig_mrs);\n\tif (!ctx->reg->mr) {\n\t\tret = -EAGAIN;\n\t\tgoto out_free_ctx;\n\t}\n\n\tcount += rdma_rw_inv_key(ctx->reg);\n\n\tmemcpy(ctx->reg->mr->sig_attrs, sig_attrs, sizeof(struct ib_sig_attrs));\n\n\tret = ib_map_mr_sg_pi(ctx->reg->mr, sg, sgt.nents, NULL, prot_sg,\n\t\t\t      prot_sgt.nents, NULL, SZ_4K);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"failed to map PI sg (%u)\\n\",\n\t\t       sgt.nents + prot_sgt.nents);\n\t\tgoto out_destroy_sig_mr;\n\t}\n\n\tctx->reg->reg_wr.wr.opcode = IB_WR_REG_MR_INTEGRITY;\n\tctx->reg->reg_wr.wr.wr_cqe = NULL;\n\tctx->reg->reg_wr.wr.num_sge = 0;\n\tctx->reg->reg_wr.wr.send_flags = 0;\n\tctx->reg->reg_wr.access = IB_ACCESS_LOCAL_WRITE;\n\tif (rdma_protocol_iwarp(qp->device, port_num))\n\t\tctx->reg->reg_wr.access |= IB_ACCESS_REMOTE_WRITE;\n\tctx->reg->reg_wr.mr = ctx->reg->mr;\n\tctx->reg->reg_wr.key = ctx->reg->mr->lkey;\n\tcount++;\n\n\tctx->reg->sge.addr = ctx->reg->mr->iova;\n\tctx->reg->sge.length = ctx->reg->mr->length;\n\tif (sig_attrs->wire.sig_type == IB_SIG_TYPE_NONE)\n\t\tctx->reg->sge.length -= ctx->reg->mr->sig_attrs->meta_length;\n\n\trdma_wr = &ctx->reg->wr;\n\trdma_wr->wr.sg_list = &ctx->reg->sge;\n\trdma_wr->wr.num_sge = 1;\n\trdma_wr->remote_addr = remote_addr;\n\trdma_wr->rkey = rkey;\n\tif (dir == DMA_TO_DEVICE)\n\t\trdma_wr->wr.opcode = IB_WR_RDMA_WRITE;\n\telse\n\t\trdma_wr->wr.opcode = IB_WR_RDMA_READ;\n\tctx->reg->reg_wr.wr.next = &rdma_wr->wr;\n\tcount++;\n\n\treturn count;\n\nout_destroy_sig_mr:\n\tib_mr_pool_put(qp, &qp->sig_mrs, ctx->reg->mr);\nout_free_ctx:\n\tkfree(ctx->reg);\nout_unmap_prot_sg:\n\tif (prot_sgt.nents)\n\t\tib_dma_unmap_sgtable_attrs(dev, &prot_sgt, dir, 0);\nout_unmap_sg:\n\tib_dma_unmap_sgtable_attrs(dev, &sgt, dir, 0);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_rw_ctx_signature_init);\n\n \nstatic void rdma_rw_update_lkey(struct rdma_rw_reg_ctx *reg, bool need_inval)\n{\n\treg->mr->need_inval = need_inval;\n\tib_update_fast_reg_key(reg->mr, ib_inc_rkey(reg->mr->lkey));\n\treg->reg_wr.key = reg->mr->lkey;\n\treg->sge.lkey = reg->mr->lkey;\n}\n\n \nstruct ib_send_wr *rdma_rw_ctx_wrs(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tu32 port_num, struct ib_cqe *cqe, struct ib_send_wr *chain_wr)\n{\n\tstruct ib_send_wr *first_wr, *last_wr;\n\tint i;\n\n\tswitch (ctx->type) {\n\tcase RDMA_RW_SIG_MR:\n\tcase RDMA_RW_MR:\n\t\tfor (i = 0; i < ctx->nr_ops; i++) {\n\t\t\trdma_rw_update_lkey(&ctx->reg[i],\n\t\t\t\tctx->reg[i].wr.wr.opcode !=\n\t\t\t\t\tIB_WR_RDMA_READ_WITH_INV);\n\t\t}\n\n\t\tif (ctx->reg[0].inv_wr.next)\n\t\t\tfirst_wr = &ctx->reg[0].inv_wr;\n\t\telse\n\t\t\tfirst_wr = &ctx->reg[0].reg_wr.wr;\n\t\tlast_wr = &ctx->reg[ctx->nr_ops - 1].wr.wr;\n\t\tbreak;\n\tcase RDMA_RW_MULTI_WR:\n\t\tfirst_wr = &ctx->map.wrs[0].wr;\n\t\tlast_wr = &ctx->map.wrs[ctx->nr_ops - 1].wr;\n\t\tbreak;\n\tcase RDMA_RW_SINGLE_WR:\n\t\tfirst_wr = &ctx->single.wr.wr;\n\t\tlast_wr = &ctx->single.wr.wr;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (chain_wr) {\n\t\tlast_wr->next = chain_wr;\n\t} else {\n\t\tlast_wr->wr_cqe = cqe;\n\t\tlast_wr->send_flags |= IB_SEND_SIGNALED;\n\t}\n\n\treturn first_wr;\n}\nEXPORT_SYMBOL(rdma_rw_ctx_wrs);\n\n \nint rdma_rw_ctx_post(struct rdma_rw_ctx *ctx, struct ib_qp *qp, u32 port_num,\n\t\tstruct ib_cqe *cqe, struct ib_send_wr *chain_wr)\n{\n\tstruct ib_send_wr *first_wr;\n\n\tfirst_wr = rdma_rw_ctx_wrs(ctx, qp, port_num, cqe, chain_wr);\n\treturn ib_post_send(qp, first_wr, NULL);\n}\nEXPORT_SYMBOL(rdma_rw_ctx_post);\n\n \nvoid rdma_rw_ctx_destroy(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\t\t u32 port_num, struct scatterlist *sg, u32 sg_cnt,\n\t\t\t enum dma_data_direction dir)\n{\n\tint i;\n\n\tswitch (ctx->type) {\n\tcase RDMA_RW_MR:\n\t\tfor (i = 0; i < ctx->nr_ops; i++)\n\t\t\tib_mr_pool_put(qp, &qp->rdma_mrs, ctx->reg[i].mr);\n\t\tkfree(ctx->reg);\n\t\tbreak;\n\tcase RDMA_RW_MULTI_WR:\n\t\tkfree(ctx->map.wrs);\n\t\tkfree(ctx->map.sges);\n\t\tbreak;\n\tcase RDMA_RW_SINGLE_WR:\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tib_dma_unmap_sg(qp->pd->device, sg, sg_cnt, dir);\n}\nEXPORT_SYMBOL(rdma_rw_ctx_destroy);\n\n \nvoid rdma_rw_ctx_destroy_signature(struct rdma_rw_ctx *ctx, struct ib_qp *qp,\n\t\tu32 port_num, struct scatterlist *sg, u32 sg_cnt,\n\t\tstruct scatterlist *prot_sg, u32 prot_sg_cnt,\n\t\tenum dma_data_direction dir)\n{\n\tif (WARN_ON_ONCE(ctx->type != RDMA_RW_SIG_MR))\n\t\treturn;\n\n\tib_mr_pool_put(qp, &qp->sig_mrs, ctx->reg->mr);\n\tkfree(ctx->reg);\n\n\tif (prot_sg_cnt)\n\t\tib_dma_unmap_sg(qp->pd->device, prot_sg, prot_sg_cnt, dir);\n\tib_dma_unmap_sg(qp->pd->device, sg, sg_cnt, dir);\n}\nEXPORT_SYMBOL(rdma_rw_ctx_destroy_signature);\n\n \nunsigned int rdma_rw_mr_factor(struct ib_device *device, u32 port_num,\n\t\t\t       unsigned int maxpages)\n{\n\tunsigned int mr_pages;\n\n\tif (rdma_rw_can_use_mr(device, port_num))\n\t\tmr_pages = rdma_rw_fr_page_list_len(device, false);\n\telse\n\t\tmr_pages = device->attrs.max_sge_rd;\n\treturn DIV_ROUND_UP(maxpages, mr_pages);\n}\nEXPORT_SYMBOL(rdma_rw_mr_factor);\n\nvoid rdma_rw_init_qp(struct ib_device *dev, struct ib_qp_init_attr *attr)\n{\n\tu32 factor;\n\n\tWARN_ON_ONCE(attr->port_num == 0);\n\n\t \n\tfactor = 1;\n\n\t \n\tif (attr->create_flags & IB_QP_CREATE_INTEGRITY_EN ||\n\t    rdma_rw_can_use_mr(dev, attr->port_num))\n\t\tfactor += 2;\t \n\n\tattr->cap.max_send_wr += factor * attr->cap.max_rdma_ctxs;\n\n\t \n\tattr->cap.max_send_wr =\n\t\tmin_t(u32, attr->cap.max_send_wr, dev->attrs.max_qp_wr);\n}\n\nint rdma_rw_init_mrs(struct ib_qp *qp, struct ib_qp_init_attr *attr)\n{\n\tstruct ib_device *dev = qp->pd->device;\n\tu32 nr_mrs = 0, nr_sig_mrs = 0, max_num_sg = 0;\n\tint ret = 0;\n\n\tif (attr->create_flags & IB_QP_CREATE_INTEGRITY_EN) {\n\t\tnr_sig_mrs = attr->cap.max_rdma_ctxs;\n\t\tnr_mrs = attr->cap.max_rdma_ctxs;\n\t\tmax_num_sg = rdma_rw_fr_page_list_len(dev, true);\n\t} else if (rdma_rw_can_use_mr(dev, attr->port_num)) {\n\t\tnr_mrs = attr->cap.max_rdma_ctxs;\n\t\tmax_num_sg = rdma_rw_fr_page_list_len(dev, false);\n\t}\n\n\tif (nr_mrs) {\n\t\tret = ib_mr_pool_init(qp, &qp->rdma_mrs, nr_mrs,\n\t\t\t\tIB_MR_TYPE_MEM_REG,\n\t\t\t\tmax_num_sg, 0);\n\t\tif (ret) {\n\t\t\tpr_err(\"%s: failed to allocated %u MRs\\n\",\n\t\t\t\t__func__, nr_mrs);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (nr_sig_mrs) {\n\t\tret = ib_mr_pool_init(qp, &qp->sig_mrs, nr_sig_mrs,\n\t\t\t\tIB_MR_TYPE_INTEGRITY, max_num_sg, max_num_sg);\n\t\tif (ret) {\n\t\t\tpr_err(\"%s: failed to allocated %u SIG MRs\\n\",\n\t\t\t\t__func__, nr_sig_mrs);\n\t\t\tgoto out_free_rdma_mrs;\n\t\t}\n\t}\n\n\treturn 0;\n\nout_free_rdma_mrs:\n\tib_mr_pool_destroy(qp, &qp->rdma_mrs);\n\treturn ret;\n}\n\nvoid rdma_rw_cleanup_mrs(struct ib_qp *qp)\n{\n\tib_mr_pool_destroy(qp, &qp->sig_mrs);\n\tib_mr_pool_destroy(qp, &qp->rdma_mrs);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}