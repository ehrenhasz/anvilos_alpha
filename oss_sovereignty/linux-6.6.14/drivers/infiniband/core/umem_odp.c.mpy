{
  "module_name": "umem_odp.c",
  "hash_id": "9fa16d305709dd33ca5a09706c49e0171267bccbcfa9f88f5ca6203199b8f83b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/umem_odp.c",
  "human_readable_source": " \n\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/pid.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/vmalloc.h>\n#include <linux/hugetlb.h>\n#include <linux/interval_tree.h>\n#include <linux/hmm.h>\n#include <linux/pagemap.h>\n\n#include <rdma/ib_umem_odp.h>\n\n#include \"uverbs.h\"\n\nstatic inline int ib_init_umem_odp(struct ib_umem_odp *umem_odp,\n\t\t\t\t   const struct mmu_interval_notifier_ops *ops)\n{\n\tint ret;\n\n\tumem_odp->umem.is_odp = 1;\n\tmutex_init(&umem_odp->umem_mutex);\n\n\tif (!umem_odp->is_implicit_odp) {\n\t\tsize_t page_size = 1UL << umem_odp->page_shift;\n\t\tunsigned long start;\n\t\tunsigned long end;\n\t\tsize_t ndmas, npfns;\n\n\t\tstart = ALIGN_DOWN(umem_odp->umem.address, page_size);\n\t\tif (check_add_overflow(umem_odp->umem.address,\n\t\t\t\t       (unsigned long)umem_odp->umem.length,\n\t\t\t\t       &end))\n\t\t\treturn -EOVERFLOW;\n\t\tend = ALIGN(end, page_size);\n\t\tif (unlikely(end < page_size))\n\t\t\treturn -EOVERFLOW;\n\n\t\tndmas = (end - start) >> umem_odp->page_shift;\n\t\tif (!ndmas)\n\t\t\treturn -EINVAL;\n\n\t\tnpfns = (end - start) >> PAGE_SHIFT;\n\t\tumem_odp->pfn_list = kvcalloc(\n\t\t\tnpfns, sizeof(*umem_odp->pfn_list), GFP_KERNEL);\n\t\tif (!umem_odp->pfn_list)\n\t\t\treturn -ENOMEM;\n\n\t\tumem_odp->dma_list = kvcalloc(\n\t\t\tndmas, sizeof(*umem_odp->dma_list), GFP_KERNEL);\n\t\tif (!umem_odp->dma_list) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_pfn_list;\n\t\t}\n\n\t\tret = mmu_interval_notifier_insert(&umem_odp->notifier,\n\t\t\t\t\t\t   umem_odp->umem.owning_mm,\n\t\t\t\t\t\t   start, end - start, ops);\n\t\tif (ret)\n\t\t\tgoto out_dma_list;\n\t}\n\n\treturn 0;\n\nout_dma_list:\n\tkvfree(umem_odp->dma_list);\nout_pfn_list:\n\tkvfree(umem_odp->pfn_list);\n\treturn ret;\n}\n\n \nstruct ib_umem_odp *ib_umem_odp_alloc_implicit(struct ib_device *device,\n\t\t\t\t\t       int access)\n{\n\tstruct ib_umem *umem;\n\tstruct ib_umem_odp *umem_odp;\n\tint ret;\n\n\tif (access & IB_ACCESS_HUGETLB)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tumem_odp = kzalloc(sizeof(*umem_odp), GFP_KERNEL);\n\tif (!umem_odp)\n\t\treturn ERR_PTR(-ENOMEM);\n\tumem = &umem_odp->umem;\n\tumem->ibdev = device;\n\tumem->writable = ib_access_writable(access);\n\tumem->owning_mm = current->mm;\n\tumem_odp->is_implicit_odp = 1;\n\tumem_odp->page_shift = PAGE_SHIFT;\n\n\tumem_odp->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\tret = ib_init_umem_odp(umem_odp, NULL);\n\tif (ret) {\n\t\tput_pid(umem_odp->tgid);\n\t\tkfree(umem_odp);\n\t\treturn ERR_PTR(ret);\n\t}\n\treturn umem_odp;\n}\nEXPORT_SYMBOL(ib_umem_odp_alloc_implicit);\n\n \nstruct ib_umem_odp *\nib_umem_odp_alloc_child(struct ib_umem_odp *root, unsigned long addr,\n\t\t\tsize_t size,\n\t\t\tconst struct mmu_interval_notifier_ops *ops)\n{\n\t \n\tstruct ib_umem_odp *odp_data;\n\tstruct ib_umem *umem;\n\tint ret;\n\n\tif (WARN_ON(!root->is_implicit_odp))\n\t\treturn ERR_PTR(-EINVAL);\n\n\todp_data = kzalloc(sizeof(*odp_data), GFP_KERNEL);\n\tif (!odp_data)\n\t\treturn ERR_PTR(-ENOMEM);\n\tumem = &odp_data->umem;\n\tumem->ibdev = root->umem.ibdev;\n\tumem->length     = size;\n\tumem->address    = addr;\n\tumem->writable   = root->umem.writable;\n\tumem->owning_mm  = root->umem.owning_mm;\n\todp_data->page_shift = PAGE_SHIFT;\n\todp_data->notifier.ops = ops;\n\n\t \n\tif (!mmget_not_zero(umem->owning_mm)) {\n\t\tret = -EFAULT;\n\t\tgoto out_free;\n\t}\n\n\todp_data->tgid = get_pid(root->tgid);\n\tret = ib_init_umem_odp(odp_data, ops);\n\tif (ret)\n\t\tgoto out_tgid;\n\tmmput(umem->owning_mm);\n\treturn odp_data;\n\nout_tgid:\n\tput_pid(odp_data->tgid);\n\tmmput(umem->owning_mm);\nout_free:\n\tkfree(odp_data);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(ib_umem_odp_alloc_child);\n\n \nstruct ib_umem_odp *ib_umem_odp_get(struct ib_device *device,\n\t\t\t\t    unsigned long addr, size_t size, int access,\n\t\t\t\t    const struct mmu_interval_notifier_ops *ops)\n{\n\tstruct ib_umem_odp *umem_odp;\n\tint ret;\n\n\tif (WARN_ON_ONCE(!(access & IB_ACCESS_ON_DEMAND)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tumem_odp = kzalloc(sizeof(struct ib_umem_odp), GFP_KERNEL);\n\tif (!umem_odp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tumem_odp->umem.ibdev = device;\n\tumem_odp->umem.length = size;\n\tumem_odp->umem.address = addr;\n\tumem_odp->umem.writable = ib_access_writable(access);\n\tumem_odp->umem.owning_mm = current->mm;\n\tumem_odp->notifier.ops = ops;\n\n\tumem_odp->page_shift = PAGE_SHIFT;\n#ifdef CONFIG_HUGETLB_PAGE\n\tif (access & IB_ACCESS_HUGETLB)\n\t\tumem_odp->page_shift = HPAGE_SHIFT;\n#endif\n\n\tumem_odp->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\tret = ib_init_umem_odp(umem_odp, ops);\n\tif (ret)\n\t\tgoto err_put_pid;\n\treturn umem_odp;\n\nerr_put_pid:\n\tput_pid(umem_odp->tgid);\n\tkfree(umem_odp);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(ib_umem_odp_get);\n\nvoid ib_umem_odp_release(struct ib_umem_odp *umem_odp)\n{\n\t \n\tif (!umem_odp->is_implicit_odp) {\n\t\tmutex_lock(&umem_odp->umem_mutex);\n\t\tib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),\n\t\t\t\t\t    ib_umem_end(umem_odp));\n\t\tmutex_unlock(&umem_odp->umem_mutex);\n\t\tmmu_interval_notifier_remove(&umem_odp->notifier);\n\t\tkvfree(umem_odp->dma_list);\n\t\tkvfree(umem_odp->pfn_list);\n\t}\n\tput_pid(umem_odp->tgid);\n\tkfree(umem_odp);\n}\nEXPORT_SYMBOL(ib_umem_odp_release);\n\n \nstatic int ib_umem_odp_map_dma_single_page(\n\t\tstruct ib_umem_odp *umem_odp,\n\t\tunsigned int dma_index,\n\t\tstruct page *page,\n\t\tu64 access_mask)\n{\n\tstruct ib_device *dev = umem_odp->umem.ibdev;\n\tdma_addr_t *dma_addr = &umem_odp->dma_list[dma_index];\n\n\tif (*dma_addr) {\n\t\t \n\t\t*dma_addr = (*dma_addr & ODP_DMA_ADDR_MASK) | access_mask;\n\t\treturn 0;\n\t}\n\n\t*dma_addr = ib_dma_map_page(dev, page, 0, 1 << umem_odp->page_shift,\n\t\t\t\t    DMA_BIDIRECTIONAL);\n\tif (ib_dma_mapping_error(dev, *dma_addr)) {\n\t\t*dma_addr = 0;\n\t\treturn -EFAULT;\n\t}\n\tumem_odp->npages++;\n\t*dma_addr |= access_mask;\n\treturn 0;\n}\n\n \nint ib_umem_odp_map_dma_and_lock(struct ib_umem_odp *umem_odp, u64 user_virt,\n\t\t\t\t u64 bcnt, u64 access_mask, bool fault)\n\t\t\t__acquires(&umem_odp->umem_mutex)\n{\n\tstruct task_struct *owning_process  = NULL;\n\tstruct mm_struct *owning_mm = umem_odp->umem.owning_mm;\n\tint pfn_index, dma_index, ret = 0, start_idx;\n\tunsigned int page_shift, hmm_order, pfn_start_idx;\n\tunsigned long num_pfns, current_seq;\n\tstruct hmm_range range = {};\n\tunsigned long timeout;\n\n\tif (access_mask == 0)\n\t\treturn -EINVAL;\n\n\tif (user_virt < ib_umem_start(umem_odp) ||\n\t    user_virt + bcnt > ib_umem_end(umem_odp))\n\t\treturn -EFAULT;\n\n\tpage_shift = umem_odp->page_shift;\n\n\t \n\towning_process = get_pid_task(umem_odp->tgid, PIDTYPE_PID);\n\tif (!owning_process || !mmget_not_zero(owning_mm)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_task;\n\t}\n\n\trange.notifier = &umem_odp->notifier;\n\trange.start = ALIGN_DOWN(user_virt, 1UL << page_shift);\n\trange.end = ALIGN(user_virt + bcnt, 1UL << page_shift);\n\tpfn_start_idx = (range.start - ib_umem_start(umem_odp)) >> PAGE_SHIFT;\n\tnum_pfns = (range.end - range.start) >> PAGE_SHIFT;\n\tif (fault) {\n\t\trange.default_flags = HMM_PFN_REQ_FAULT;\n\n\t\tif (access_mask & ODP_WRITE_ALLOWED_BIT)\n\t\t\trange.default_flags |= HMM_PFN_REQ_WRITE;\n\t}\n\n\trange.hmm_pfns = &(umem_odp->pfn_list[pfn_start_idx]);\n\ttimeout = jiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);\n\nretry:\n\tcurrent_seq = range.notifier_seq =\n\t\tmmu_interval_read_begin(&umem_odp->notifier);\n\n\tmmap_read_lock(owning_mm);\n\tret = hmm_range_fault(&range);\n\tmmap_read_unlock(owning_mm);\n\tif (unlikely(ret)) {\n\t\tif (ret == -EBUSY && !time_after(jiffies, timeout))\n\t\t\tgoto retry;\n\t\tgoto out_put_mm;\n\t}\n\n\tstart_idx = (range.start - ib_umem_start(umem_odp)) >> page_shift;\n\tdma_index = start_idx;\n\n\tmutex_lock(&umem_odp->umem_mutex);\n\tif (mmu_interval_read_retry(&umem_odp->notifier, current_seq)) {\n\t\tmutex_unlock(&umem_odp->umem_mutex);\n\t\tgoto retry;\n\t}\n\n\tfor (pfn_index = 0; pfn_index < num_pfns;\n\t\tpfn_index += 1 << (page_shift - PAGE_SHIFT), dma_index++) {\n\n\t\tif (fault) {\n\t\t\t \n\t\t\tWARN_ON(range.hmm_pfns[pfn_index] & HMM_PFN_ERROR);\n\t\t\tWARN_ON(!(range.hmm_pfns[pfn_index] & HMM_PFN_VALID));\n\t\t} else {\n\t\t\tif (!(range.hmm_pfns[pfn_index] & HMM_PFN_VALID)) {\n\t\t\t\tWARN_ON(umem_odp->dma_list[dma_index]);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\taccess_mask = ODP_READ_ALLOWED_BIT;\n\t\t\tif (range.hmm_pfns[pfn_index] & HMM_PFN_WRITE)\n\t\t\t\taccess_mask |= ODP_WRITE_ALLOWED_BIT;\n\t\t}\n\n\t\thmm_order = hmm_pfn_to_map_order(range.hmm_pfns[pfn_index]);\n\t\t \n\t\tif (hmm_order + PAGE_SHIFT < page_shift) {\n\t\t\tret = -EINVAL;\n\t\t\tibdev_dbg(umem_odp->umem.ibdev,\n\t\t\t\t  \"%s: un-expected hmm_order %u, page_shift %u\\n\",\n\t\t\t\t  __func__, hmm_order, page_shift);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = ib_umem_odp_map_dma_single_page(\n\t\t\t\tumem_odp, dma_index, hmm_pfn_to_page(range.hmm_pfns[pfn_index]),\n\t\t\t\taccess_mask);\n\t\tif (ret < 0) {\n\t\t\tibdev_dbg(umem_odp->umem.ibdev,\n\t\t\t\t  \"ib_umem_odp_map_dma_single_page failed with error %d\\n\", ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\tif (!ret)\n\t\tret = dma_index - start_idx;\n\telse\n\t\tmutex_unlock(&umem_odp->umem_mutex);\n\nout_put_mm:\n\tmmput_async(owning_mm);\nout_put_task:\n\tif (owning_process)\n\t\tput_task_struct(owning_process);\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_umem_odp_map_dma_and_lock);\n\nvoid ib_umem_odp_unmap_dma_pages(struct ib_umem_odp *umem_odp, u64 virt,\n\t\t\t\t u64 bound)\n{\n\tdma_addr_t dma_addr;\n\tdma_addr_t dma;\n\tint idx;\n\tu64 addr;\n\tstruct ib_device *dev = umem_odp->umem.ibdev;\n\n\tlockdep_assert_held(&umem_odp->umem_mutex);\n\n\tvirt = max_t(u64, virt, ib_umem_start(umem_odp));\n\tbound = min_t(u64, bound, ib_umem_end(umem_odp));\n\tfor (addr = virt; addr < bound; addr += BIT(umem_odp->page_shift)) {\n\t\tidx = (addr - ib_umem_start(umem_odp)) >> umem_odp->page_shift;\n\t\tdma = umem_odp->dma_list[idx];\n\n\t\t \n\t\tif (dma) {\n\t\t\tunsigned long pfn_idx = (addr - ib_umem_start(umem_odp)) >> PAGE_SHIFT;\n\t\t\tstruct page *page = hmm_pfn_to_page(umem_odp->pfn_list[pfn_idx]);\n\n\t\t\tdma_addr = dma & ODP_DMA_ADDR_MASK;\n\t\t\tib_dma_unmap_page(dev, dma_addr,\n\t\t\t\t\t  BIT(umem_odp->page_shift),\n\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\t\t\tif (dma & ODP_WRITE_ALLOWED_BIT) {\n\t\t\t\tstruct page *head_page = compound_head(page);\n\t\t\t\t \n\t\t\t\tset_page_dirty(head_page);\n\t\t\t}\n\t\t\tumem_odp->dma_list[idx] = 0;\n\t\t\tumem_odp->npages--;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(ib_umem_odp_unmap_dma_pages);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}