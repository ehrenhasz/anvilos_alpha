{
  "module_name": "cma.c",
  "hash_id": "1d5ef515331979f040800064fcab44b20d9732ab7faac2f0800ab1441cd4152b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/cma.c",
  "human_readable_source": "\n \n\n#include <linux/completion.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/mutex.h>\n#include <linux/random.h>\n#include <linux/rbtree.h>\n#include <linux/igmp.h>\n#include <linux/xarray.h>\n#include <linux/inetdevice.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <net/route.h>\n\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/netevent.h>\n#include <net/tcp.h>\n#include <net/ipv6.h>\n#include <net/ip_fib.h>\n#include <net/ip6_route.h>\n\n#include <rdma/rdma_cm.h>\n#include <rdma/rdma_cm_ib.h>\n#include <rdma/rdma_netlink.h>\n#include <rdma/ib.h>\n#include <rdma/ib_cache.h>\n#include <rdma/ib_cm.h>\n#include <rdma/ib_sa.h>\n#include <rdma/iw_cm.h>\n\n#include \"core_priv.h\"\n#include \"cma_priv.h\"\n#include \"cma_trace.h\"\n\nMODULE_AUTHOR(\"Sean Hefty\");\nMODULE_DESCRIPTION(\"Generic RDMA CM Agent\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n#define CMA_CM_RESPONSE_TIMEOUT 20\n#define CMA_MAX_CM_RETRIES 15\n#define CMA_CM_MRA_SETTING (IB_CM_MRA_FLAG_DELAY | 24)\n#define CMA_IBOE_PACKET_LIFETIME 16\n#define CMA_PREFERRED_ROCE_GID_TYPE IB_GID_TYPE_ROCE_UDP_ENCAP\n\nstatic const char * const cma_events[] = {\n\t[RDMA_CM_EVENT_ADDR_RESOLVED]\t = \"address resolved\",\n\t[RDMA_CM_EVENT_ADDR_ERROR]\t = \"address error\",\n\t[RDMA_CM_EVENT_ROUTE_RESOLVED]\t = \"route resolved \",\n\t[RDMA_CM_EVENT_ROUTE_ERROR]\t = \"route error\",\n\t[RDMA_CM_EVENT_CONNECT_REQUEST]\t = \"connect request\",\n\t[RDMA_CM_EVENT_CONNECT_RESPONSE] = \"connect response\",\n\t[RDMA_CM_EVENT_CONNECT_ERROR]\t = \"connect error\",\n\t[RDMA_CM_EVENT_UNREACHABLE]\t = \"unreachable\",\n\t[RDMA_CM_EVENT_REJECTED]\t = \"rejected\",\n\t[RDMA_CM_EVENT_ESTABLISHED]\t = \"established\",\n\t[RDMA_CM_EVENT_DISCONNECTED]\t = \"disconnected\",\n\t[RDMA_CM_EVENT_DEVICE_REMOVAL]\t = \"device removal\",\n\t[RDMA_CM_EVENT_MULTICAST_JOIN]\t = \"multicast join\",\n\t[RDMA_CM_EVENT_MULTICAST_ERROR]\t = \"multicast error\",\n\t[RDMA_CM_EVENT_ADDR_CHANGE]\t = \"address change\",\n\t[RDMA_CM_EVENT_TIMEWAIT_EXIT]\t = \"timewait exit\",\n};\n\nstatic void cma_iboe_set_mgid(struct sockaddr *addr, union ib_gid *mgid,\n\t\t\t      enum ib_gid_type gid_type);\n\nconst char *__attribute_const__ rdma_event_msg(enum rdma_cm_event_type event)\n{\n\tsize_t index = event;\n\n\treturn (index < ARRAY_SIZE(cma_events) && cma_events[index]) ?\n\t\t\tcma_events[index] : \"unrecognized event\";\n}\nEXPORT_SYMBOL(rdma_event_msg);\n\nconst char *__attribute_const__ rdma_reject_msg(struct rdma_cm_id *id,\n\t\t\t\t\t\tint reason)\n{\n\tif (rdma_ib_or_roce(id->device, id->port_num))\n\t\treturn ibcm_reject_msg(reason);\n\n\tif (rdma_protocol_iwarp(id->device, id->port_num))\n\t\treturn iwcm_reject_msg(reason);\n\n\tWARN_ON_ONCE(1);\n\treturn \"unrecognized transport\";\n}\nEXPORT_SYMBOL(rdma_reject_msg);\n\n \nstatic bool rdma_is_consumer_reject(struct rdma_cm_id *id, int reason)\n{\n\tif (rdma_ib_or_roce(id->device, id->port_num))\n\t\treturn reason == IB_CM_REJ_CONSUMER_DEFINED;\n\n\tif (rdma_protocol_iwarp(id->device, id->port_num))\n\t\treturn reason == -ECONNREFUSED;\n\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nconst void *rdma_consumer_reject_data(struct rdma_cm_id *id,\n\t\t\t\t      struct rdma_cm_event *ev, u8 *data_len)\n{\n\tconst void *p;\n\n\tif (rdma_is_consumer_reject(id, ev->status)) {\n\t\t*data_len = ev->param.conn.private_data_len;\n\t\tp = ev->param.conn.private_data;\n\t} else {\n\t\t*data_len = 0;\n\t\tp = NULL;\n\t}\n\treturn p;\n}\nEXPORT_SYMBOL(rdma_consumer_reject_data);\n\n \nstruct iw_cm_id *rdma_iw_cm_id(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (id->device->node_type == RDMA_NODE_RNIC)\n\t\treturn id_priv->cm_id.iw;\n\treturn NULL;\n}\nEXPORT_SYMBOL(rdma_iw_cm_id);\n\n \nstruct rdma_cm_id *rdma_res_to_id(struct rdma_restrack_entry *res)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(res, struct rdma_id_private, res);\n\n\treturn &id_priv->id;\n}\nEXPORT_SYMBOL(rdma_res_to_id);\n\nstatic int cma_add_one(struct ib_device *device);\nstatic void cma_remove_one(struct ib_device *device, void *client_data);\n\nstatic struct ib_client cma_client = {\n\t.name   = \"cma\",\n\t.add    = cma_add_one,\n\t.remove = cma_remove_one\n};\n\nstatic struct ib_sa_client sa_client;\nstatic LIST_HEAD(dev_list);\nstatic LIST_HEAD(listen_any_list);\nstatic DEFINE_MUTEX(lock);\nstatic struct rb_root id_table = RB_ROOT;\n \nstatic DEFINE_SPINLOCK(id_table_lock);\nstatic struct workqueue_struct *cma_wq;\nstatic unsigned int cma_pernet_id;\n\nstruct cma_pernet {\n\tstruct xarray tcp_ps;\n\tstruct xarray udp_ps;\n\tstruct xarray ipoib_ps;\n\tstruct xarray ib_ps;\n};\n\nstatic struct cma_pernet *cma_pernet(struct net *net)\n{\n\treturn net_generic(net, cma_pernet_id);\n}\n\nstatic\nstruct xarray *cma_pernet_xa(struct net *net, enum rdma_ucm_port_space ps)\n{\n\tstruct cma_pernet *pernet = cma_pernet(net);\n\n\tswitch (ps) {\n\tcase RDMA_PS_TCP:\n\t\treturn &pernet->tcp_ps;\n\tcase RDMA_PS_UDP:\n\t\treturn &pernet->udp_ps;\n\tcase RDMA_PS_IPOIB:\n\t\treturn &pernet->ipoib_ps;\n\tcase RDMA_PS_IB:\n\t\treturn &pernet->ib_ps;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstruct id_table_entry {\n\tstruct list_head id_list;\n\tstruct rb_node rb_node;\n};\n\nstruct cma_device {\n\tstruct list_head\tlist;\n\tstruct ib_device\t*device;\n\tstruct completion\tcomp;\n\trefcount_t refcount;\n\tstruct list_head\tid_list;\n\tenum ib_gid_type\t*default_gid_type;\n\tu8\t\t\t*default_roce_tos;\n};\n\nstruct rdma_bind_list {\n\tenum rdma_ucm_port_space ps;\n\tstruct hlist_head\towners;\n\tunsigned short\t\tport;\n};\n\nstatic int cma_ps_alloc(struct net *net, enum rdma_ucm_port_space ps,\n\t\t\tstruct rdma_bind_list *bind_list, int snum)\n{\n\tstruct xarray *xa = cma_pernet_xa(net, ps);\n\n\treturn xa_insert(xa, snum, bind_list, GFP_KERNEL);\n}\n\nstatic struct rdma_bind_list *cma_ps_find(struct net *net,\n\t\t\t\t\t  enum rdma_ucm_port_space ps, int snum)\n{\n\tstruct xarray *xa = cma_pernet_xa(net, ps);\n\n\treturn xa_load(xa, snum);\n}\n\nstatic void cma_ps_remove(struct net *net, enum rdma_ucm_port_space ps,\n\t\t\t  int snum)\n{\n\tstruct xarray *xa = cma_pernet_xa(net, ps);\n\n\txa_erase(xa, snum);\n}\n\nenum {\n\tCMA_OPTION_AFONLY,\n};\n\nvoid cma_dev_get(struct cma_device *cma_dev)\n{\n\trefcount_inc(&cma_dev->refcount);\n}\n\nvoid cma_dev_put(struct cma_device *cma_dev)\n{\n\tif (refcount_dec_and_test(&cma_dev->refcount))\n\t\tcomplete(&cma_dev->comp);\n}\n\nstruct cma_device *cma_enum_devices_by_ibdev(cma_device_filter\tfilter,\n\t\t\t\t\t     void\t\t*cookie)\n{\n\tstruct cma_device *cma_dev;\n\tstruct cma_device *found_cma_dev = NULL;\n\n\tmutex_lock(&lock);\n\n\tlist_for_each_entry(cma_dev, &dev_list, list)\n\t\tif (filter(cma_dev->device, cookie)) {\n\t\t\tfound_cma_dev = cma_dev;\n\t\t\tbreak;\n\t\t}\n\n\tif (found_cma_dev)\n\t\tcma_dev_get(found_cma_dev);\n\tmutex_unlock(&lock);\n\treturn found_cma_dev;\n}\n\nint cma_get_default_gid_type(struct cma_device *cma_dev,\n\t\t\t     u32 port)\n{\n\tif (!rdma_is_port_valid(cma_dev->device, port))\n\t\treturn -EINVAL;\n\n\treturn cma_dev->default_gid_type[port - rdma_start_port(cma_dev->device)];\n}\n\nint cma_set_default_gid_type(struct cma_device *cma_dev,\n\t\t\t     u32 port,\n\t\t\t     enum ib_gid_type default_gid_type)\n{\n\tunsigned long supported_gids;\n\n\tif (!rdma_is_port_valid(cma_dev->device, port))\n\t\treturn -EINVAL;\n\n\tif (default_gid_type == IB_GID_TYPE_IB &&\n\t    rdma_protocol_roce_eth_encap(cma_dev->device, port))\n\t\tdefault_gid_type = IB_GID_TYPE_ROCE;\n\n\tsupported_gids = roce_gid_type_mask_support(cma_dev->device, port);\n\n\tif (!(supported_gids & 1 << default_gid_type))\n\t\treturn -EINVAL;\n\n\tcma_dev->default_gid_type[port - rdma_start_port(cma_dev->device)] =\n\t\tdefault_gid_type;\n\n\treturn 0;\n}\n\nint cma_get_default_roce_tos(struct cma_device *cma_dev, u32 port)\n{\n\tif (!rdma_is_port_valid(cma_dev->device, port))\n\t\treturn -EINVAL;\n\n\treturn cma_dev->default_roce_tos[port - rdma_start_port(cma_dev->device)];\n}\n\nint cma_set_default_roce_tos(struct cma_device *cma_dev, u32 port,\n\t\t\t     u8 default_roce_tos)\n{\n\tif (!rdma_is_port_valid(cma_dev->device, port))\n\t\treturn -EINVAL;\n\n\tcma_dev->default_roce_tos[port - rdma_start_port(cma_dev->device)] =\n\t\t default_roce_tos;\n\n\treturn 0;\n}\nstruct ib_device *cma_get_ib_dev(struct cma_device *cma_dev)\n{\n\treturn cma_dev->device;\n}\n\n \n\nstruct cma_multicast {\n\tstruct rdma_id_private *id_priv;\n\tunion {\n\t\tstruct ib_sa_multicast *sa_mc;\n\t\tstruct {\n\t\t\tstruct work_struct work;\n\t\t\tstruct rdma_cm_event event;\n\t\t} iboe_join;\n\t};\n\tstruct list_head\tlist;\n\tvoid\t\t\t*context;\n\tstruct sockaddr_storage\taddr;\n\tu8\t\t\tjoin_state;\n};\n\nstruct cma_work {\n\tstruct work_struct\twork;\n\tstruct rdma_id_private\t*id;\n\tenum rdma_cm_state\told_state;\n\tenum rdma_cm_state\tnew_state;\n\tstruct rdma_cm_event\tevent;\n};\n\nunion cma_ip_addr {\n\tstruct in6_addr ip6;\n\tstruct {\n\t\t__be32 pad[3];\n\t\t__be32 addr;\n\t} ip4;\n};\n\nstruct cma_hdr {\n\tu8 cma_version;\n\tu8 ip_version;\t \n\t__be16 port;\n\tunion cma_ip_addr src_addr;\n\tunion cma_ip_addr dst_addr;\n};\n\n#define CMA_VERSION 0x00\n\nstruct cma_req_info {\n\tstruct sockaddr_storage listen_addr_storage;\n\tstruct sockaddr_storage src_addr_storage;\n\tstruct ib_device *device;\n\tunion ib_gid local_gid;\n\t__be64 service_id;\n\tint port;\n\tbool has_gid;\n\tu16 pkey;\n};\n\nstatic int cma_comp_exch(struct rdma_id_private *id_priv,\n\t\t\t enum rdma_cm_state comp, enum rdma_cm_state exch)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tif (comp == RDMA_CM_CONNECT || exch == RDMA_CM_CONNECT)\n\t\tlockdep_assert_held(&id_priv->handler_mutex);\n\n\tspin_lock_irqsave(&id_priv->lock, flags);\n\tif ((ret = (id_priv->state == comp)))\n\t\tid_priv->state = exch;\n\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\treturn ret;\n}\n\nstatic inline u8 cma_get_ip_ver(const struct cma_hdr *hdr)\n{\n\treturn hdr->ip_version >> 4;\n}\n\nstatic void cma_set_ip_ver(struct cma_hdr *hdr, u8 ip_ver)\n{\n\thdr->ip_version = (ip_ver << 4) | (hdr->ip_version & 0xF);\n}\n\nstatic struct sockaddr *cma_src_addr(struct rdma_id_private *id_priv)\n{\n\treturn (struct sockaddr *)&id_priv->id.route.addr.src_addr;\n}\n\nstatic inline struct sockaddr *cma_dst_addr(struct rdma_id_private *id_priv)\n{\n\treturn (struct sockaddr *)&id_priv->id.route.addr.dst_addr;\n}\n\nstatic int cma_igmp_send(struct net_device *ndev, union ib_gid *mgid, bool join)\n{\n\tstruct in_device *in_dev = NULL;\n\n\tif (ndev) {\n\t\trtnl_lock();\n\t\tin_dev = __in_dev_get_rtnl(ndev);\n\t\tif (in_dev) {\n\t\t\tif (join)\n\t\t\t\tip_mc_inc_group(in_dev,\n\t\t\t\t\t\t*(__be32 *)(mgid->raw + 12));\n\t\t\telse\n\t\t\t\tip_mc_dec_group(in_dev,\n\t\t\t\t\t\t*(__be32 *)(mgid->raw + 12));\n\t\t}\n\t\trtnl_unlock();\n\t}\n\treturn (in_dev) ? 0 : -ENODEV;\n}\n\nstatic int compare_netdev_and_ip(int ifindex_a, struct sockaddr *sa,\n\t\t\t\t struct id_table_entry *entry_b)\n{\n\tstruct rdma_id_private *id_priv = list_first_entry(\n\t\t&entry_b->id_list, struct rdma_id_private, id_list_entry);\n\tint ifindex_b = id_priv->id.route.addr.dev_addr.bound_dev_if;\n\tstruct sockaddr *sb = cma_dst_addr(id_priv);\n\n\tif (ifindex_a != ifindex_b)\n\t\treturn (ifindex_a > ifindex_b) ? 1 : -1;\n\n\tif (sa->sa_family != sb->sa_family)\n\t\treturn sa->sa_family - sb->sa_family;\n\n\tif (sa->sa_family == AF_INET &&\n\t    __builtin_object_size(sa, 0) >= sizeof(struct sockaddr_in)) {\n\t\treturn memcmp(&((struct sockaddr_in *)sa)->sin_addr,\n\t\t\t      &((struct sockaddr_in *)sb)->sin_addr,\n\t\t\t      sizeof(((struct sockaddr_in *)sa)->sin_addr));\n\t}\n\n\tif (sa->sa_family == AF_INET6 &&\n\t    __builtin_object_size(sa, 0) >= sizeof(struct sockaddr_in6)) {\n\t\treturn ipv6_addr_cmp(&((struct sockaddr_in6 *)sa)->sin6_addr,\n\t\t\t\t     &((struct sockaddr_in6 *)sb)->sin6_addr);\n\t}\n\n\treturn -1;\n}\n\nstatic int cma_add_id_to_tree(struct rdma_id_private *node_id_priv)\n{\n\tstruct rb_node **new, *parent = NULL;\n\tstruct id_table_entry *this, *node;\n\tunsigned long flags;\n\tint result;\n\n\tnode = kzalloc(sizeof(*node), GFP_KERNEL);\n\tif (!node)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irqsave(&id_table_lock, flags);\n\tnew = &id_table.rb_node;\n\twhile (*new) {\n\t\tthis = container_of(*new, struct id_table_entry, rb_node);\n\t\tresult = compare_netdev_and_ip(\n\t\t\tnode_id_priv->id.route.addr.dev_addr.bound_dev_if,\n\t\t\tcma_dst_addr(node_id_priv), this);\n\n\t\tparent = *new;\n\t\tif (result < 0)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse if (result > 0)\n\t\t\tnew = &((*new)->rb_right);\n\t\telse {\n\t\t\tlist_add_tail(&node_id_priv->id_list_entry,\n\t\t\t\t      &this->id_list);\n\t\t\tkfree(node);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tINIT_LIST_HEAD(&node->id_list);\n\tlist_add_tail(&node_id_priv->id_list_entry, &node->id_list);\n\n\trb_link_node(&node->rb_node, parent, new);\n\trb_insert_color(&node->rb_node, &id_table);\n\nunlock:\n\tspin_unlock_irqrestore(&id_table_lock, flags);\n\treturn 0;\n}\n\nstatic struct id_table_entry *\nnode_from_ndev_ip(struct rb_root *root, int ifindex, struct sockaddr *sa)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct id_table_entry *data;\n\tint result;\n\n\twhile (node) {\n\t\tdata = container_of(node, struct id_table_entry, rb_node);\n\t\tresult = compare_netdev_and_ip(ifindex, sa, data);\n\t\tif (result < 0)\n\t\t\tnode = node->rb_left;\n\t\telse if (result > 0)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn data;\n\t}\n\n\treturn NULL;\n}\n\nstatic void cma_remove_id_from_tree(struct rdma_id_private *id_priv)\n{\n\tstruct id_table_entry *data;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&id_table_lock, flags);\n\tif (list_empty(&id_priv->id_list_entry))\n\t\tgoto out;\n\n\tdata = node_from_ndev_ip(&id_table,\n\t\t\t\t id_priv->id.route.addr.dev_addr.bound_dev_if,\n\t\t\t\t cma_dst_addr(id_priv));\n\tif (!data)\n\t\tgoto out;\n\n\tlist_del_init(&id_priv->id_list_entry);\n\tif (list_empty(&data->id_list)) {\n\t\trb_erase(&data->rb_node, &id_table);\n\t\tkfree(data);\n\t}\nout:\n\tspin_unlock_irqrestore(&id_table_lock, flags);\n}\n\nstatic void _cma_attach_to_dev(struct rdma_id_private *id_priv,\n\t\t\t       struct cma_device *cma_dev)\n{\n\tcma_dev_get(cma_dev);\n\tid_priv->cma_dev = cma_dev;\n\tid_priv->id.device = cma_dev->device;\n\tid_priv->id.route.addr.dev_addr.transport =\n\t\trdma_node_get_transport(cma_dev->device->node_type);\n\tlist_add_tail(&id_priv->device_item, &cma_dev->id_list);\n\n\ttrace_cm_id_attach(id_priv, cma_dev->device);\n}\n\nstatic void cma_attach_to_dev(struct rdma_id_private *id_priv,\n\t\t\t      struct cma_device *cma_dev)\n{\n\t_cma_attach_to_dev(id_priv, cma_dev);\n\tid_priv->gid_type =\n\t\tcma_dev->default_gid_type[id_priv->id.port_num -\n\t\t\t\t\t  rdma_start_port(cma_dev->device)];\n}\n\nstatic void cma_release_dev(struct rdma_id_private *id_priv)\n{\n\tmutex_lock(&lock);\n\tlist_del_init(&id_priv->device_item);\n\tcma_dev_put(id_priv->cma_dev);\n\tid_priv->cma_dev = NULL;\n\tid_priv->id.device = NULL;\n\tif (id_priv->id.route.addr.dev_addr.sgid_attr) {\n\t\trdma_put_gid_attr(id_priv->id.route.addr.dev_addr.sgid_attr);\n\t\tid_priv->id.route.addr.dev_addr.sgid_attr = NULL;\n\t}\n\tmutex_unlock(&lock);\n}\n\nstatic inline unsigned short cma_family(struct rdma_id_private *id_priv)\n{\n\treturn id_priv->id.route.addr.src_addr.ss_family;\n}\n\nstatic int cma_set_default_qkey(struct rdma_id_private *id_priv)\n{\n\tstruct ib_sa_mcmember_rec rec;\n\tint ret = 0;\n\n\tswitch (id_priv->id.ps) {\n\tcase RDMA_PS_UDP:\n\tcase RDMA_PS_IB:\n\t\tid_priv->qkey = RDMA_UDP_QKEY;\n\t\tbreak;\n\tcase RDMA_PS_IPOIB:\n\t\tib_addr_get_mgid(&id_priv->id.route.addr.dev_addr, &rec.mgid);\n\t\tret = ib_sa_get_mcmember_rec(id_priv->id.device,\n\t\t\t\t\t     id_priv->id.port_num, &rec.mgid,\n\t\t\t\t\t     &rec);\n\t\tif (!ret)\n\t\t\tid_priv->qkey = be32_to_cpu(rec.qkey);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int cma_set_qkey(struct rdma_id_private *id_priv, u32 qkey)\n{\n\tif (!qkey ||\n\t    (id_priv->qkey && (id_priv->qkey != qkey)))\n\t\treturn -EINVAL;\n\n\tid_priv->qkey = qkey;\n\treturn 0;\n}\n\nstatic void cma_translate_ib(struct sockaddr_ib *sib, struct rdma_dev_addr *dev_addr)\n{\n\tdev_addr->dev_type = ARPHRD_INFINIBAND;\n\trdma_addr_set_sgid(dev_addr, (union ib_gid *) &sib->sib_addr);\n\tib_addr_set_pkey(dev_addr, ntohs(sib->sib_pkey));\n}\n\nstatic int cma_translate_addr(struct sockaddr *addr, struct rdma_dev_addr *dev_addr)\n{\n\tint ret;\n\n\tif (addr->sa_family != AF_IB) {\n\t\tret = rdma_translate_ip(addr, dev_addr);\n\t} else {\n\t\tcma_translate_ib((struct sockaddr_ib *) addr, dev_addr);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct ib_gid_attr *\ncma_validate_port(struct ib_device *device, u32 port,\n\t\t  enum ib_gid_type gid_type,\n\t\t  union ib_gid *gid,\n\t\t  struct rdma_id_private *id_priv)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tconst struct ib_gid_attr *sgid_attr = ERR_PTR(-ENODEV);\n\tint bound_if_index = dev_addr->bound_dev_if;\n\tint dev_type = dev_addr->dev_type;\n\tstruct net_device *ndev = NULL;\n\n\tif (!rdma_dev_access_netns(device, id_priv->id.route.addr.dev_addr.net))\n\t\tgoto out;\n\n\tif ((dev_type == ARPHRD_INFINIBAND) && !rdma_protocol_ib(device, port))\n\t\tgoto out;\n\n\tif ((dev_type != ARPHRD_INFINIBAND) && rdma_protocol_ib(device, port))\n\t\tgoto out;\n\n\t \n\tif (rdma_protocol_iwarp(device, port)) {\n\t\tsgid_attr = rdma_get_gid_attr(device, port, 0);\n\t\tif (IS_ERR(sgid_attr))\n\t\t\tgoto out;\n\n\t\trcu_read_lock();\n\t\tndev = rcu_dereference(sgid_attr->ndev);\n\t\tif (!net_eq(dev_net(ndev), dev_addr->net) ||\n\t\t    ndev->ifindex != bound_if_index)\n\t\t\tsgid_attr = ERR_PTR(-ENODEV);\n\t\trcu_read_unlock();\n\t\tgoto out;\n\t}\n\n\tif (dev_type == ARPHRD_ETHER && rdma_protocol_roce(device, port)) {\n\t\tndev = dev_get_by_index(dev_addr->net, bound_if_index);\n\t\tif (!ndev)\n\t\t\tgoto out;\n\t} else {\n\t\tgid_type = IB_GID_TYPE_IB;\n\t}\n\n\tsgid_attr = rdma_find_gid_by_port(device, gid, gid_type, port, ndev);\n\tdev_put(ndev);\nout:\n\treturn sgid_attr;\n}\n\nstatic void cma_bind_sgid_attr(struct rdma_id_private *id_priv,\n\t\t\t       const struct ib_gid_attr *sgid_attr)\n{\n\tWARN_ON(id_priv->id.route.addr.dev_addr.sgid_attr);\n\tid_priv->id.route.addr.dev_addr.sgid_attr = sgid_attr;\n}\n\n \nstatic int cma_acquire_dev_by_src_ip(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tconst struct ib_gid_attr *sgid_attr;\n\tunion ib_gid gid, iboe_gid, *gidp;\n\tstruct cma_device *cma_dev;\n\tenum ib_gid_type gid_type;\n\tint ret = -ENODEV;\n\tu32 port;\n\n\tif (dev_addr->dev_type != ARPHRD_INFINIBAND &&\n\t    id_priv->id.ps == RDMA_PS_IPOIB)\n\t\treturn -EINVAL;\n\n\trdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr,\n\t\t    &iboe_gid);\n\n\tmemcpy(&gid, dev_addr->src_dev_addr +\n\t       rdma_addr_gid_offset(dev_addr), sizeof(gid));\n\n\tmutex_lock(&lock);\n\tlist_for_each_entry(cma_dev, &dev_list, list) {\n\t\trdma_for_each_port (cma_dev->device, port) {\n\t\t\tgidp = rdma_protocol_roce(cma_dev->device, port) ?\n\t\t\t       &iboe_gid : &gid;\n\t\t\tgid_type = cma_dev->default_gid_type[port - 1];\n\t\t\tsgid_attr = cma_validate_port(cma_dev->device, port,\n\t\t\t\t\t\t      gid_type, gidp, id_priv);\n\t\t\tif (!IS_ERR(sgid_attr)) {\n\t\t\t\tid_priv->id.port_num = port;\n\t\t\t\tcma_bind_sgid_attr(id_priv, sgid_attr);\n\t\t\t\tcma_attach_to_dev(id_priv, cma_dev);\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\tmutex_unlock(&lock);\n\treturn ret;\n}\n\n \nstatic int cma_ib_acquire_dev(struct rdma_id_private *id_priv,\n\t\t\t      const struct rdma_id_private *listen_id_priv,\n\t\t\t      struct cma_req_info *req)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tconst struct ib_gid_attr *sgid_attr;\n\tenum ib_gid_type gid_type;\n\tunion ib_gid gid;\n\n\tif (dev_addr->dev_type != ARPHRD_INFINIBAND &&\n\t    id_priv->id.ps == RDMA_PS_IPOIB)\n\t\treturn -EINVAL;\n\n\tif (rdma_protocol_roce(req->device, req->port))\n\t\trdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr,\n\t\t\t    &gid);\n\telse\n\t\tmemcpy(&gid, dev_addr->src_dev_addr +\n\t\t       rdma_addr_gid_offset(dev_addr), sizeof(gid));\n\n\tgid_type = listen_id_priv->cma_dev->default_gid_type[req->port - 1];\n\tsgid_attr = cma_validate_port(req->device, req->port,\n\t\t\t\t      gid_type, &gid, id_priv);\n\tif (IS_ERR(sgid_attr))\n\t\treturn PTR_ERR(sgid_attr);\n\n\tid_priv->id.port_num = req->port;\n\tcma_bind_sgid_attr(id_priv, sgid_attr);\n\t \n\tmutex_lock(&lock);\n\tcma_attach_to_dev(id_priv, listen_id_priv->cma_dev);\n\tmutex_unlock(&lock);\n\trdma_restrack_add(&id_priv->res);\n\treturn 0;\n}\n\nstatic int cma_iw_acquire_dev(struct rdma_id_private *id_priv,\n\t\t\t      const struct rdma_id_private *listen_id_priv)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tconst struct ib_gid_attr *sgid_attr;\n\tstruct cma_device *cma_dev;\n\tenum ib_gid_type gid_type;\n\tint ret = -ENODEV;\n\tunion ib_gid gid;\n\tu32 port;\n\n\tif (dev_addr->dev_type != ARPHRD_INFINIBAND &&\n\t    id_priv->id.ps == RDMA_PS_IPOIB)\n\t\treturn -EINVAL;\n\n\tmemcpy(&gid, dev_addr->src_dev_addr +\n\t       rdma_addr_gid_offset(dev_addr), sizeof(gid));\n\n\tmutex_lock(&lock);\n\n\tcma_dev = listen_id_priv->cma_dev;\n\tport = listen_id_priv->id.port_num;\n\tgid_type = listen_id_priv->gid_type;\n\tsgid_attr = cma_validate_port(cma_dev->device, port,\n\t\t\t\t      gid_type, &gid, id_priv);\n\tif (!IS_ERR(sgid_attr)) {\n\t\tid_priv->id.port_num = port;\n\t\tcma_bind_sgid_attr(id_priv, sgid_attr);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(cma_dev, &dev_list, list) {\n\t\trdma_for_each_port (cma_dev->device, port) {\n\t\t\tif (listen_id_priv->cma_dev == cma_dev &&\n\t\t\t    listen_id_priv->id.port_num == port)\n\t\t\t\tcontinue;\n\n\t\t\tgid_type = cma_dev->default_gid_type[port - 1];\n\t\t\tsgid_attr = cma_validate_port(cma_dev->device, port,\n\t\t\t\t\t\t      gid_type, &gid, id_priv);\n\t\t\tif (!IS_ERR(sgid_attr)) {\n\t\t\t\tid_priv->id.port_num = port;\n\t\t\t\tcma_bind_sgid_attr(id_priv, sgid_attr);\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\tif (!ret) {\n\t\tcma_attach_to_dev(id_priv, cma_dev);\n\t\trdma_restrack_add(&id_priv->res);\n\t}\n\n\tmutex_unlock(&lock);\n\treturn ret;\n}\n\n \nstatic int cma_resolve_ib_dev(struct rdma_id_private *id_priv)\n{\n\tstruct cma_device *cma_dev, *cur_dev;\n\tstruct sockaddr_ib *addr;\n\tunion ib_gid gid, sgid, *dgid;\n\tunsigned int p;\n\tu16 pkey, index;\n\tenum ib_port_state port_state;\n\tint ret;\n\tint i;\n\n\tcma_dev = NULL;\n\taddr = (struct sockaddr_ib *) cma_dst_addr(id_priv);\n\tdgid = (union ib_gid *) &addr->sib_addr;\n\tpkey = ntohs(addr->sib_pkey);\n\n\tmutex_lock(&lock);\n\tlist_for_each_entry(cur_dev, &dev_list, list) {\n\t\trdma_for_each_port (cur_dev->device, p) {\n\t\t\tif (!rdma_cap_af_ib(cur_dev->device, p))\n\t\t\t\tcontinue;\n\n\t\t\tif (ib_find_cached_pkey(cur_dev->device, p, pkey, &index))\n\t\t\t\tcontinue;\n\n\t\t\tif (ib_get_cached_port_state(cur_dev->device, p, &port_state))\n\t\t\t\tcontinue;\n\n\t\t\tfor (i = 0; i < cur_dev->device->port_data[p].immutable.gid_tbl_len;\n\t\t\t     ++i) {\n\t\t\t\tret = rdma_query_gid(cur_dev->device, p, i,\n\t\t\t\t\t\t     &gid);\n\t\t\t\tif (ret)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (!memcmp(&gid, dgid, sizeof(gid))) {\n\t\t\t\t\tcma_dev = cur_dev;\n\t\t\t\t\tsgid = gid;\n\t\t\t\t\tid_priv->id.port_num = p;\n\t\t\t\t\tgoto found;\n\t\t\t\t}\n\n\t\t\t\tif (!cma_dev && (gid.global.subnet_prefix ==\n\t\t\t\t    dgid->global.subnet_prefix) &&\n\t\t\t\t    port_state == IB_PORT_ACTIVE) {\n\t\t\t\t\tcma_dev = cur_dev;\n\t\t\t\t\tsgid = gid;\n\t\t\t\t\tid_priv->id.port_num = p;\n\t\t\t\t\tgoto found;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&lock);\n\treturn -ENODEV;\n\nfound:\n\tcma_attach_to_dev(id_priv, cma_dev);\n\trdma_restrack_add(&id_priv->res);\n\tmutex_unlock(&lock);\n\taddr = (struct sockaddr_ib *)cma_src_addr(id_priv);\n\tmemcpy(&addr->sib_addr, &sgid, sizeof(sgid));\n\tcma_translate_ib(addr, &id_priv->id.route.addr.dev_addr);\n\treturn 0;\n}\n\nstatic void cma_id_get(struct rdma_id_private *id_priv)\n{\n\trefcount_inc(&id_priv->refcount);\n}\n\nstatic void cma_id_put(struct rdma_id_private *id_priv)\n{\n\tif (refcount_dec_and_test(&id_priv->refcount))\n\t\tcomplete(&id_priv->comp);\n}\n\nstatic struct rdma_id_private *\n__rdma_create_id(struct net *net, rdma_cm_event_handler event_handler,\n\t\t void *context, enum rdma_ucm_port_space ps,\n\t\t enum ib_qp_type qp_type, const struct rdma_id_private *parent)\n{\n\tstruct rdma_id_private *id_priv;\n\n\tid_priv = kzalloc(sizeof *id_priv, GFP_KERNEL);\n\tif (!id_priv)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tid_priv->state = RDMA_CM_IDLE;\n\tid_priv->id.context = context;\n\tid_priv->id.event_handler = event_handler;\n\tid_priv->id.ps = ps;\n\tid_priv->id.qp_type = qp_type;\n\tid_priv->tos_set = false;\n\tid_priv->timeout_set = false;\n\tid_priv->min_rnr_timer_set = false;\n\tid_priv->gid_type = IB_GID_TYPE_IB;\n\tspin_lock_init(&id_priv->lock);\n\tmutex_init(&id_priv->qp_mutex);\n\tinit_completion(&id_priv->comp);\n\trefcount_set(&id_priv->refcount, 1);\n\tmutex_init(&id_priv->handler_mutex);\n\tINIT_LIST_HEAD(&id_priv->device_item);\n\tINIT_LIST_HEAD(&id_priv->id_list_entry);\n\tINIT_LIST_HEAD(&id_priv->listen_list);\n\tINIT_LIST_HEAD(&id_priv->mc_list);\n\tget_random_bytes(&id_priv->seq_num, sizeof id_priv->seq_num);\n\tid_priv->id.route.addr.dev_addr.net = get_net(net);\n\tid_priv->seq_num &= 0x00ffffff;\n\n\trdma_restrack_new(&id_priv->res, RDMA_RESTRACK_CM_ID);\n\tif (parent)\n\t\trdma_restrack_parent_name(&id_priv->res, &parent->res);\n\n\treturn id_priv;\n}\n\nstruct rdma_cm_id *\n__rdma_create_kernel_id(struct net *net, rdma_cm_event_handler event_handler,\n\t\t\tvoid *context, enum rdma_ucm_port_space ps,\n\t\t\tenum ib_qp_type qp_type, const char *caller)\n{\n\tstruct rdma_id_private *ret;\n\n\tret = __rdma_create_id(net, event_handler, context, ps, qp_type, NULL);\n\tif (IS_ERR(ret))\n\t\treturn ERR_CAST(ret);\n\n\trdma_restrack_set_name(&ret->res, caller);\n\treturn &ret->id;\n}\nEXPORT_SYMBOL(__rdma_create_kernel_id);\n\nstruct rdma_cm_id *rdma_create_user_id(rdma_cm_event_handler event_handler,\n\t\t\t\t       void *context,\n\t\t\t\t       enum rdma_ucm_port_space ps,\n\t\t\t\t       enum ib_qp_type qp_type)\n{\n\tstruct rdma_id_private *ret;\n\n\tret = __rdma_create_id(current->nsproxy->net_ns, event_handler, context,\n\t\t\t       ps, qp_type, NULL);\n\tif (IS_ERR(ret))\n\t\treturn ERR_CAST(ret);\n\n\trdma_restrack_set_name(&ret->res, NULL);\n\treturn &ret->id;\n}\nEXPORT_SYMBOL(rdma_create_user_id);\n\nstatic int cma_init_ud_qp(struct rdma_id_private *id_priv, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tqp_attr.qp_state = IB_QPS_INIT;\n\tret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n\tif (ret)\n\t\treturn ret;\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_modify_qp(qp, &qp_attr, IB_QP_STATE);\n\tif (ret)\n\t\treturn ret;\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tqp_attr.sq_psn = 0;\n\tret = ib_modify_qp(qp, &qp_attr, IB_QP_STATE | IB_QP_SQ_PSN);\n\n\treturn ret;\n}\n\nstatic int cma_init_conn_qp(struct rdma_id_private *id_priv, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tqp_attr.qp_state = IB_QPS_INIT;\n\tret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);\n\tif (ret)\n\t\treturn ret;\n\n\treturn ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n}\n\nint rdma_create_qp(struct rdma_cm_id *id, struct ib_pd *pd,\n\t\t   struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct rdma_id_private *id_priv;\n\tstruct ib_qp *qp;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (id->device != pd->device) {\n\t\tret = -EINVAL;\n\t\tgoto out_err;\n\t}\n\n\tqp_init_attr->port_num = id->port_num;\n\tqp = ib_create_qp(pd, qp_init_attr);\n\tif (IS_ERR(qp)) {\n\t\tret = PTR_ERR(qp);\n\t\tgoto out_err;\n\t}\n\n\tif (id->qp_type == IB_QPT_UD)\n\t\tret = cma_init_ud_qp(id_priv, qp);\n\telse\n\t\tret = cma_init_conn_qp(id_priv, qp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tid->qp = qp;\n\tid_priv->qp_num = qp->qp_num;\n\tid_priv->srq = (qp->srq != NULL);\n\ttrace_cm_qp_create(id_priv, pd, qp_init_attr, 0);\n\treturn 0;\nout_destroy:\n\tib_destroy_qp(qp);\nout_err:\n\ttrace_cm_qp_create(id_priv, pd, qp_init_attr, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_create_qp);\n\nvoid rdma_destroy_qp(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\ttrace_cm_qp_destroy(id_priv);\n\tmutex_lock(&id_priv->qp_mutex);\n\tib_destroy_qp(id_priv->id.qp);\n\tid_priv->id.qp = NULL;\n\tmutex_unlock(&id_priv->qp_mutex);\n}\nEXPORT_SYMBOL(rdma_destroy_qp);\n\nstatic int cma_modify_qp_rtr(struct rdma_id_private *id_priv,\n\t\t\t     struct rdma_conn_param *conn_param)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tmutex_lock(&id_priv->qp_mutex);\n\tif (!id_priv->id.qp) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\t \n\tqp_attr.qp_state = IB_QPS_INIT;\n\tret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ib_modify_qp(id_priv->id.qp, &qp_attr, qp_attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tBUG_ON(id_priv->cma_dev->device != id_priv->id.device);\n\n\tif (conn_param)\n\t\tqp_attr.max_dest_rd_atomic = conn_param->responder_resources;\n\tret = ib_modify_qp(id_priv->id.qp, &qp_attr, qp_attr_mask);\nout:\n\tmutex_unlock(&id_priv->qp_mutex);\n\treturn ret;\n}\n\nstatic int cma_modify_qp_rts(struct rdma_id_private *id_priv,\n\t\t\t     struct rdma_conn_param *conn_param)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tmutex_lock(&id_priv->qp_mutex);\n\tif (!id_priv->id.qp) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tif (conn_param)\n\t\tqp_attr.max_rd_atomic = conn_param->initiator_depth;\n\tret = ib_modify_qp(id_priv->id.qp, &qp_attr, qp_attr_mask);\nout:\n\tmutex_unlock(&id_priv->qp_mutex);\n\treturn ret;\n}\n\nstatic int cma_modify_qp_err(struct rdma_id_private *id_priv)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint ret;\n\n\tmutex_lock(&id_priv->qp_mutex);\n\tif (!id_priv->id.qp) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tqp_attr.qp_state = IB_QPS_ERR;\n\tret = ib_modify_qp(id_priv->id.qp, &qp_attr, IB_QP_STATE);\nout:\n\tmutex_unlock(&id_priv->qp_mutex);\n\treturn ret;\n}\n\nstatic int cma_ib_init_qp_attr(struct rdma_id_private *id_priv,\n\t\t\t       struct ib_qp_attr *qp_attr, int *qp_attr_mask)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tint ret;\n\tu16 pkey;\n\n\tif (rdma_cap_eth_ah(id_priv->id.device, id_priv->id.port_num))\n\t\tpkey = 0xffff;\n\telse\n\t\tpkey = ib_addr_get_pkey(dev_addr);\n\n\tret = ib_find_cached_pkey(id_priv->id.device, id_priv->id.port_num,\n\t\t\t\t  pkey, &qp_attr->pkey_index);\n\tif (ret)\n\t\treturn ret;\n\n\tqp_attr->port_num = id_priv->id.port_num;\n\t*qp_attr_mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT;\n\n\tif (id_priv->id.qp_type == IB_QPT_UD) {\n\t\tret = cma_set_default_qkey(id_priv);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tqp_attr->qkey = id_priv->qkey;\n\t\t*qp_attr_mask |= IB_QP_QKEY;\n\t} else {\n\t\tqp_attr->qp_access_flags = 0;\n\t\t*qp_attr_mask |= IB_QP_ACCESS_FLAGS;\n\t}\n\treturn 0;\n}\n\nint rdma_init_qp_attr(struct rdma_cm_id *id, struct ib_qp_attr *qp_attr,\n\t\t       int *qp_attr_mask)\n{\n\tstruct rdma_id_private *id_priv;\n\tint ret = 0;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (rdma_cap_ib_cm(id->device, id->port_num)) {\n\t\tif (!id_priv->cm_id.ib || (id_priv->id.qp_type == IB_QPT_UD))\n\t\t\tret = cma_ib_init_qp_attr(id_priv, qp_attr, qp_attr_mask);\n\t\telse\n\t\t\tret = ib_cm_init_qp_attr(id_priv->cm_id.ib, qp_attr,\n\t\t\t\t\t\t qp_attr_mask);\n\n\t\tif (qp_attr->qp_state == IB_QPS_RTR)\n\t\t\tqp_attr->rq_psn = id_priv->seq_num;\n\t} else if (rdma_cap_iw_cm(id->device, id->port_num)) {\n\t\tif (!id_priv->cm_id.iw) {\n\t\t\tqp_attr->qp_access_flags = 0;\n\t\t\t*qp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS;\n\t\t} else\n\t\t\tret = iw_cm_init_qp_attr(id_priv->cm_id.iw, qp_attr,\n\t\t\t\t\t\t qp_attr_mask);\n\t\tqp_attr->port_num = id_priv->id.port_num;\n\t\t*qp_attr_mask |= IB_QP_PORT;\n\t} else {\n\t\tret = -ENOSYS;\n\t}\n\n\tif ((*qp_attr_mask & IB_QP_TIMEOUT) && id_priv->timeout_set)\n\t\tqp_attr->timeout = id_priv->timeout;\n\n\tif ((*qp_attr_mask & IB_QP_MIN_RNR_TIMER) && id_priv->min_rnr_timer_set)\n\t\tqp_attr->min_rnr_timer = id_priv->min_rnr_timer;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_init_qp_attr);\n\nstatic inline bool cma_zero_addr(const struct sockaddr *addr)\n{\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\treturn ipv4_is_zeronet(((struct sockaddr_in *)addr)->sin_addr.s_addr);\n\tcase AF_INET6:\n\t\treturn ipv6_addr_any(&((struct sockaddr_in6 *)addr)->sin6_addr);\n\tcase AF_IB:\n\t\treturn ib_addr_any(&((struct sockaddr_ib *)addr)->sib_addr);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline bool cma_loopback_addr(const struct sockaddr *addr)\n{\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\treturn ipv4_is_loopback(\n\t\t\t((struct sockaddr_in *)addr)->sin_addr.s_addr);\n\tcase AF_INET6:\n\t\treturn ipv6_addr_loopback(\n\t\t\t&((struct sockaddr_in6 *)addr)->sin6_addr);\n\tcase AF_IB:\n\t\treturn ib_addr_loopback(\n\t\t\t&((struct sockaddr_ib *)addr)->sib_addr);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline bool cma_any_addr(const struct sockaddr *addr)\n{\n\treturn cma_zero_addr(addr) || cma_loopback_addr(addr);\n}\n\nstatic int cma_addr_cmp(const struct sockaddr *src, const struct sockaddr *dst)\n{\n\tif (src->sa_family != dst->sa_family)\n\t\treturn -1;\n\n\tswitch (src->sa_family) {\n\tcase AF_INET:\n\t\treturn ((struct sockaddr_in *)src)->sin_addr.s_addr !=\n\t\t       ((struct sockaddr_in *)dst)->sin_addr.s_addr;\n\tcase AF_INET6: {\n\t\tstruct sockaddr_in6 *src_addr6 = (struct sockaddr_in6 *)src;\n\t\tstruct sockaddr_in6 *dst_addr6 = (struct sockaddr_in6 *)dst;\n\t\tbool link_local;\n\n\t\tif (ipv6_addr_cmp(&src_addr6->sin6_addr,\n\t\t\t\t\t  &dst_addr6->sin6_addr))\n\t\t\treturn 1;\n\t\tlink_local = ipv6_addr_type(&dst_addr6->sin6_addr) &\n\t\t\t     IPV6_ADDR_LINKLOCAL;\n\t\t \n\t\treturn link_local ? (src_addr6->sin6_scope_id !=\n\t\t\t\t     dst_addr6->sin6_scope_id) :\n\t\t\t\t    0;\n\t}\n\n\tdefault:\n\t\treturn ib_addr_cmp(&((struct sockaddr_ib *) src)->sib_addr,\n\t\t\t\t   &((struct sockaddr_ib *) dst)->sib_addr);\n\t}\n}\n\nstatic __be16 cma_port(const struct sockaddr *addr)\n{\n\tstruct sockaddr_ib *sib;\n\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\treturn ((struct sockaddr_in *) addr)->sin_port;\n\tcase AF_INET6:\n\t\treturn ((struct sockaddr_in6 *) addr)->sin6_port;\n\tcase AF_IB:\n\t\tsib = (struct sockaddr_ib *) addr;\n\t\treturn htons((u16) (be64_to_cpu(sib->sib_sid) &\n\t\t\t\t    be64_to_cpu(sib->sib_sid_mask)));\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic inline int cma_any_port(const struct sockaddr *addr)\n{\n\treturn !cma_port(addr);\n}\n\nstatic void cma_save_ib_info(struct sockaddr *src_addr,\n\t\t\t     struct sockaddr *dst_addr,\n\t\t\t     const struct rdma_cm_id *listen_id,\n\t\t\t     const struct sa_path_rec *path)\n{\n\tstruct sockaddr_ib *listen_ib, *ib;\n\n\tlisten_ib = (struct sockaddr_ib *) &listen_id->route.addr.src_addr;\n\tif (src_addr) {\n\t\tib = (struct sockaddr_ib *)src_addr;\n\t\tib->sib_family = AF_IB;\n\t\tif (path) {\n\t\t\tib->sib_pkey = path->pkey;\n\t\t\tib->sib_flowinfo = path->flow_label;\n\t\t\tmemcpy(&ib->sib_addr, &path->sgid, 16);\n\t\t\tib->sib_sid = path->service_id;\n\t\t\tib->sib_scope_id = 0;\n\t\t} else {\n\t\t\tib->sib_pkey = listen_ib->sib_pkey;\n\t\t\tib->sib_flowinfo = listen_ib->sib_flowinfo;\n\t\t\tib->sib_addr = listen_ib->sib_addr;\n\t\t\tib->sib_sid = listen_ib->sib_sid;\n\t\t\tib->sib_scope_id = listen_ib->sib_scope_id;\n\t\t}\n\t\tib->sib_sid_mask = cpu_to_be64(0xffffffffffffffffULL);\n\t}\n\tif (dst_addr) {\n\t\tib = (struct sockaddr_ib *)dst_addr;\n\t\tib->sib_family = AF_IB;\n\t\tif (path) {\n\t\t\tib->sib_pkey = path->pkey;\n\t\t\tib->sib_flowinfo = path->flow_label;\n\t\t\tmemcpy(&ib->sib_addr, &path->dgid, 16);\n\t\t}\n\t}\n}\n\nstatic void cma_save_ip4_info(struct sockaddr_in *src_addr,\n\t\t\t      struct sockaddr_in *dst_addr,\n\t\t\t      struct cma_hdr *hdr,\n\t\t\t      __be16 local_port)\n{\n\tif (src_addr) {\n\t\t*src_addr = (struct sockaddr_in) {\n\t\t\t.sin_family = AF_INET,\n\t\t\t.sin_addr.s_addr = hdr->dst_addr.ip4.addr,\n\t\t\t.sin_port = local_port,\n\t\t};\n\t}\n\n\tif (dst_addr) {\n\t\t*dst_addr = (struct sockaddr_in) {\n\t\t\t.sin_family = AF_INET,\n\t\t\t.sin_addr.s_addr = hdr->src_addr.ip4.addr,\n\t\t\t.sin_port = hdr->port,\n\t\t};\n\t}\n}\n\nstatic void cma_save_ip6_info(struct sockaddr_in6 *src_addr,\n\t\t\t      struct sockaddr_in6 *dst_addr,\n\t\t\t      struct cma_hdr *hdr,\n\t\t\t      __be16 local_port)\n{\n\tif (src_addr) {\n\t\t*src_addr = (struct sockaddr_in6) {\n\t\t\t.sin6_family = AF_INET6,\n\t\t\t.sin6_addr = hdr->dst_addr.ip6,\n\t\t\t.sin6_port = local_port,\n\t\t};\n\t}\n\n\tif (dst_addr) {\n\t\t*dst_addr = (struct sockaddr_in6) {\n\t\t\t.sin6_family = AF_INET6,\n\t\t\t.sin6_addr = hdr->src_addr.ip6,\n\t\t\t.sin6_port = hdr->port,\n\t\t};\n\t}\n}\n\nstatic u16 cma_port_from_service_id(__be64 service_id)\n{\n\treturn (u16)be64_to_cpu(service_id);\n}\n\nstatic int cma_save_ip_info(struct sockaddr *src_addr,\n\t\t\t    struct sockaddr *dst_addr,\n\t\t\t    const struct ib_cm_event *ib_event,\n\t\t\t    __be64 service_id)\n{\n\tstruct cma_hdr *hdr;\n\t__be16 port;\n\n\thdr = ib_event->private_data;\n\tif (hdr->cma_version != CMA_VERSION)\n\t\treturn -EINVAL;\n\n\tport = htons(cma_port_from_service_id(service_id));\n\n\tswitch (cma_get_ip_ver(hdr)) {\n\tcase 4:\n\t\tcma_save_ip4_info((struct sockaddr_in *)src_addr,\n\t\t\t\t  (struct sockaddr_in *)dst_addr, hdr, port);\n\t\tbreak;\n\tcase 6:\n\t\tcma_save_ip6_info((struct sockaddr_in6 *)src_addr,\n\t\t\t\t  (struct sockaddr_in6 *)dst_addr, hdr, port);\n\t\tbreak;\n\tdefault:\n\t\treturn -EAFNOSUPPORT;\n\t}\n\n\treturn 0;\n}\n\nstatic int cma_save_net_info(struct sockaddr *src_addr,\n\t\t\t     struct sockaddr *dst_addr,\n\t\t\t     const struct rdma_cm_id *listen_id,\n\t\t\t     const struct ib_cm_event *ib_event,\n\t\t\t     sa_family_t sa_family, __be64 service_id)\n{\n\tif (sa_family == AF_IB) {\n\t\tif (ib_event->event == IB_CM_REQ_RECEIVED)\n\t\t\tcma_save_ib_info(src_addr, dst_addr, listen_id,\n\t\t\t\t\t ib_event->param.req_rcvd.primary_path);\n\t\telse if (ib_event->event == IB_CM_SIDR_REQ_RECEIVED)\n\t\t\tcma_save_ib_info(src_addr, dst_addr, listen_id, NULL);\n\t\treturn 0;\n\t}\n\n\treturn cma_save_ip_info(src_addr, dst_addr, ib_event, service_id);\n}\n\nstatic int cma_save_req_info(const struct ib_cm_event *ib_event,\n\t\t\t     struct cma_req_info *req)\n{\n\tconst struct ib_cm_req_event_param *req_param =\n\t\t&ib_event->param.req_rcvd;\n\tconst struct ib_cm_sidr_req_event_param *sidr_param =\n\t\t&ib_event->param.sidr_req_rcvd;\n\n\tswitch (ib_event->event) {\n\tcase IB_CM_REQ_RECEIVED:\n\t\treq->device\t= req_param->listen_id->device;\n\t\treq->port\t= req_param->port;\n\t\tmemcpy(&req->local_gid, &req_param->primary_path->sgid,\n\t\t       sizeof(req->local_gid));\n\t\treq->has_gid\t= true;\n\t\treq->service_id = req_param->primary_path->service_id;\n\t\treq->pkey\t= be16_to_cpu(req_param->primary_path->pkey);\n\t\tif (req->pkey != req_param->bth_pkey)\n\t\t\tpr_warn_ratelimited(\"RDMA CMA: got different BTH P_Key (0x%x) and primary path P_Key (0x%x)\\n\"\n\t\t\t\t\t    \"RDMA CMA: in the future this may cause the request to be dropped\\n\",\n\t\t\t\t\t    req_param->bth_pkey, req->pkey);\n\t\tbreak;\n\tcase IB_CM_SIDR_REQ_RECEIVED:\n\t\treq->device\t= sidr_param->listen_id->device;\n\t\treq->port\t= sidr_param->port;\n\t\treq->has_gid\t= false;\n\t\treq->service_id\t= sidr_param->service_id;\n\t\treq->pkey\t= sidr_param->pkey;\n\t\tif (req->pkey != sidr_param->bth_pkey)\n\t\t\tpr_warn_ratelimited(\"RDMA CMA: got different BTH P_Key (0x%x) and SIDR request payload P_Key (0x%x)\\n\"\n\t\t\t\t\t    \"RDMA CMA: in the future this may cause the request to be dropped\\n\",\n\t\t\t\t\t    sidr_param->bth_pkey, req->pkey);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool validate_ipv4_net_dev(struct net_device *net_dev,\n\t\t\t\t  const struct sockaddr_in *dst_addr,\n\t\t\t\t  const struct sockaddr_in *src_addr)\n{\n\t__be32 daddr = dst_addr->sin_addr.s_addr,\n\t       saddr = src_addr->sin_addr.s_addr;\n\tstruct fib_result res;\n\tstruct flowi4 fl4;\n\tint err;\n\tbool ret;\n\n\tif (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr) ||\n\t    ipv4_is_lbcast(daddr) || ipv4_is_zeronet(saddr) ||\n\t    ipv4_is_zeronet(daddr) || ipv4_is_loopback(daddr) ||\n\t    ipv4_is_loopback(saddr))\n\t\treturn false;\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.flowi4_oif = net_dev->ifindex;\n\tfl4.daddr = daddr;\n\tfl4.saddr = saddr;\n\n\trcu_read_lock();\n\terr = fib_lookup(dev_net(net_dev), &fl4, &res, 0);\n\tret = err == 0 && FIB_RES_DEV(res) == net_dev;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic bool validate_ipv6_net_dev(struct net_device *net_dev,\n\t\t\t\t  const struct sockaddr_in6 *dst_addr,\n\t\t\t\t  const struct sockaddr_in6 *src_addr)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tconst int strict = ipv6_addr_type(&dst_addr->sin6_addr) &\n\t\t\t   IPV6_ADDR_LINKLOCAL;\n\tstruct rt6_info *rt = rt6_lookup(dev_net(net_dev), &dst_addr->sin6_addr,\n\t\t\t\t\t &src_addr->sin6_addr, net_dev->ifindex,\n\t\t\t\t\t NULL, strict);\n\tbool ret;\n\n\tif (!rt)\n\t\treturn false;\n\n\tret = rt->rt6i_idev->dev == net_dev;\n\tip6_rt_put(rt);\n\n\treturn ret;\n#else\n\treturn false;\n#endif\n}\n\nstatic bool validate_net_dev(struct net_device *net_dev,\n\t\t\t     const struct sockaddr *daddr,\n\t\t\t     const struct sockaddr *saddr)\n{\n\tconst struct sockaddr_in *daddr4 = (const struct sockaddr_in *)daddr;\n\tconst struct sockaddr_in *saddr4 = (const struct sockaddr_in *)saddr;\n\tconst struct sockaddr_in6 *daddr6 = (const struct sockaddr_in6 *)daddr;\n\tconst struct sockaddr_in6 *saddr6 = (const struct sockaddr_in6 *)saddr;\n\n\tswitch (daddr->sa_family) {\n\tcase AF_INET:\n\t\treturn saddr->sa_family == AF_INET &&\n\t\t       validate_ipv4_net_dev(net_dev, daddr4, saddr4);\n\n\tcase AF_INET6:\n\t\treturn saddr->sa_family == AF_INET6 &&\n\t\t       validate_ipv6_net_dev(net_dev, daddr6, saddr6);\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic struct net_device *\nroce_get_net_dev_by_cm_event(const struct ib_cm_event *ib_event)\n{\n\tconst struct ib_gid_attr *sgid_attr = NULL;\n\tstruct net_device *ndev;\n\n\tif (ib_event->event == IB_CM_REQ_RECEIVED)\n\t\tsgid_attr = ib_event->param.req_rcvd.ppath_sgid_attr;\n\telse if (ib_event->event == IB_CM_SIDR_REQ_RECEIVED)\n\t\tsgid_attr = ib_event->param.sidr_req_rcvd.sgid_attr;\n\n\tif (!sgid_attr)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tndev = rdma_read_gid_attr_ndev_rcu(sgid_attr);\n\tif (IS_ERR(ndev))\n\t\tndev = NULL;\n\telse\n\t\tdev_hold(ndev);\n\trcu_read_unlock();\n\treturn ndev;\n}\n\nstatic struct net_device *cma_get_net_dev(const struct ib_cm_event *ib_event,\n\t\t\t\t\t  struct cma_req_info *req)\n{\n\tstruct sockaddr *listen_addr =\n\t\t\t(struct sockaddr *)&req->listen_addr_storage;\n\tstruct sockaddr *src_addr = (struct sockaddr *)&req->src_addr_storage;\n\tstruct net_device *net_dev;\n\tconst union ib_gid *gid = req->has_gid ? &req->local_gid : NULL;\n\tint err;\n\n\terr = cma_save_ip_info(listen_addr, src_addr, ib_event,\n\t\t\t       req->service_id);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tif (rdma_protocol_roce(req->device, req->port))\n\t\tnet_dev = roce_get_net_dev_by_cm_event(ib_event);\n\telse\n\t\tnet_dev = ib_get_net_dev_by_params(req->device, req->port,\n\t\t\t\t\t\t   req->pkey,\n\t\t\t\t\t\t   gid, listen_addr);\n\tif (!net_dev)\n\t\treturn ERR_PTR(-ENODEV);\n\n\treturn net_dev;\n}\n\nstatic enum rdma_ucm_port_space rdma_ps_from_service_id(__be64 service_id)\n{\n\treturn (be64_to_cpu(service_id) >> 16) & 0xffff;\n}\n\nstatic bool cma_match_private_data(struct rdma_id_private *id_priv,\n\t\t\t\t   const struct cma_hdr *hdr)\n{\n\tstruct sockaddr *addr = cma_src_addr(id_priv);\n\t__be32 ip4_addr;\n\tstruct in6_addr ip6_addr;\n\n\tif (cma_any_addr(addr) && !id_priv->afonly)\n\t\treturn true;\n\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\tip4_addr = ((struct sockaddr_in *)addr)->sin_addr.s_addr;\n\t\tif (cma_get_ip_ver(hdr) != 4)\n\t\t\treturn false;\n\t\tif (!cma_any_addr(addr) &&\n\t\t    hdr->dst_addr.ip4.addr != ip4_addr)\n\t\t\treturn false;\n\t\tbreak;\n\tcase AF_INET6:\n\t\tip6_addr = ((struct sockaddr_in6 *)addr)->sin6_addr;\n\t\tif (cma_get_ip_ver(hdr) != 6)\n\t\t\treturn false;\n\t\tif (!cma_any_addr(addr) &&\n\t\t    memcmp(&hdr->dst_addr.ip6, &ip6_addr, sizeof(ip6_addr)))\n\t\t\treturn false;\n\t\tbreak;\n\tcase AF_IB:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool cma_protocol_roce(const struct rdma_cm_id *id)\n{\n\tstruct ib_device *device = id->device;\n\tconst u32 port_num = id->port_num ?: rdma_start_port(device);\n\n\treturn rdma_protocol_roce(device, port_num);\n}\n\nstatic bool cma_is_req_ipv6_ll(const struct cma_req_info *req)\n{\n\tconst struct sockaddr *daddr =\n\t\t\t(const struct sockaddr *)&req->listen_addr_storage;\n\tconst struct sockaddr_in6 *daddr6 = (const struct sockaddr_in6 *)daddr;\n\n\t \n\treturn (daddr->sa_family == AF_INET6 &&\n\t\t(ipv6_addr_type(&daddr6->sin6_addr) & IPV6_ADDR_LINKLOCAL));\n}\n\nstatic bool cma_match_net_dev(const struct rdma_cm_id *id,\n\t\t\t      const struct net_device *net_dev,\n\t\t\t      const struct cma_req_info *req)\n{\n\tconst struct rdma_addr *addr = &id->route.addr;\n\n\tif (!net_dev)\n\t\t \n\t\treturn (!id->port_num || id->port_num == req->port) &&\n\t\t       (addr->src_addr.ss_family == AF_IB);\n\n\t \n\tif (!cma_is_req_ipv6_ll(req))\n\t\treturn true;\n\t \n\tif (net_eq(dev_net(net_dev), addr->dev_addr.net) &&\n\t    (!!addr->dev_addr.bound_dev_if ==\n\t     (addr->dev_addr.bound_dev_if == net_dev->ifindex)))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic struct rdma_id_private *cma_find_listener(\n\t\tconst struct rdma_bind_list *bind_list,\n\t\tconst struct ib_cm_id *cm_id,\n\t\tconst struct ib_cm_event *ib_event,\n\t\tconst struct cma_req_info *req,\n\t\tconst struct net_device *net_dev)\n{\n\tstruct rdma_id_private *id_priv, *id_priv_dev;\n\n\tlockdep_assert_held(&lock);\n\n\tif (!bind_list)\n\t\treturn ERR_PTR(-EINVAL);\n\n\thlist_for_each_entry(id_priv, &bind_list->owners, node) {\n\t\tif (cma_match_private_data(id_priv, ib_event->private_data)) {\n\t\t\tif (id_priv->id.device == cm_id->device &&\n\t\t\t    cma_match_net_dev(&id_priv->id, net_dev, req))\n\t\t\t\treturn id_priv;\n\t\t\tlist_for_each_entry(id_priv_dev,\n\t\t\t\t\t    &id_priv->listen_list,\n\t\t\t\t\t    listen_item) {\n\t\t\t\tif (id_priv_dev->id.device == cm_id->device &&\n\t\t\t\t    cma_match_net_dev(&id_priv_dev->id,\n\t\t\t\t\t\t      net_dev, req))\n\t\t\t\t\treturn id_priv_dev;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct rdma_id_private *\ncma_ib_id_from_event(struct ib_cm_id *cm_id,\n\t\t     const struct ib_cm_event *ib_event,\n\t\t     struct cma_req_info *req,\n\t\t     struct net_device **net_dev)\n{\n\tstruct rdma_bind_list *bind_list;\n\tstruct rdma_id_private *id_priv;\n\tint err;\n\n\terr = cma_save_req_info(ib_event, req);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t*net_dev = cma_get_net_dev(ib_event, req);\n\tif (IS_ERR(*net_dev)) {\n\t\tif (PTR_ERR(*net_dev) == -EAFNOSUPPORT) {\n\t\t\t \n\t\t\t*net_dev = NULL;\n\t\t} else {\n\t\t\treturn ERR_CAST(*net_dev);\n\t\t}\n\t}\n\n\tmutex_lock(&lock);\n\t \n\trcu_read_lock();\n\tif (*net_dev) {\n\t\t \n\t\tif (((*net_dev)->flags & IFF_UP) == 0) {\n\t\t\tid_priv = ERR_PTR(-EHOSTUNREACH);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!validate_net_dev(*net_dev,\n\t\t\t\t (struct sockaddr *)&req->src_addr_storage,\n\t\t\t\t (struct sockaddr *)&req->listen_addr_storage)) {\n\t\t\tid_priv = ERR_PTR(-EHOSTUNREACH);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tbind_list = cma_ps_find(*net_dev ? dev_net(*net_dev) : &init_net,\n\t\t\t\trdma_ps_from_service_id(req->service_id),\n\t\t\t\tcma_port_from_service_id(req->service_id));\n\tid_priv = cma_find_listener(bind_list, cm_id, ib_event, req, *net_dev);\nerr:\n\trcu_read_unlock();\n\tmutex_unlock(&lock);\n\tif (IS_ERR(id_priv) && *net_dev) {\n\t\tdev_put(*net_dev);\n\t\t*net_dev = NULL;\n\t}\n\treturn id_priv;\n}\n\nstatic inline u8 cma_user_data_offset(struct rdma_id_private *id_priv)\n{\n\treturn cma_family(id_priv) == AF_IB ? 0 : sizeof(struct cma_hdr);\n}\n\nstatic void cma_cancel_route(struct rdma_id_private *id_priv)\n{\n\tif (rdma_cap_ib_sa(id_priv->id.device, id_priv->id.port_num)) {\n\t\tif (id_priv->query)\n\t\t\tib_sa_cancel_query(id_priv->query_id, id_priv->query);\n\t}\n}\n\nstatic void _cma_cancel_listens(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_id_private *dev_id_priv;\n\n\tlockdep_assert_held(&lock);\n\n\t \n\tlist_del_init(&id_priv->listen_any_item);\n\n\twhile (!list_empty(&id_priv->listen_list)) {\n\t\tdev_id_priv =\n\t\t\tlist_first_entry(&id_priv->listen_list,\n\t\t\t\t\t struct rdma_id_private, listen_item);\n\t\t \n\t\tlist_del_init(&dev_id_priv->device_item);\n\t\tlist_del_init(&dev_id_priv->listen_item);\n\t\tmutex_unlock(&lock);\n\n\t\trdma_destroy_id(&dev_id_priv->id);\n\t\tmutex_lock(&lock);\n\t}\n}\n\nstatic void cma_cancel_listens(struct rdma_id_private *id_priv)\n{\n\tmutex_lock(&lock);\n\t_cma_cancel_listens(id_priv);\n\tmutex_unlock(&lock);\n}\n\nstatic void cma_cancel_operation(struct rdma_id_private *id_priv,\n\t\t\t\t enum rdma_cm_state state)\n{\n\tswitch (state) {\n\tcase RDMA_CM_ADDR_QUERY:\n\t\t \n\t\trdma_addr_cancel(&id_priv->id.route.addr.dev_addr);\n\t\tbreak;\n\tcase RDMA_CM_ROUTE_QUERY:\n\t\tcma_cancel_route(id_priv);\n\t\tbreak;\n\tcase RDMA_CM_LISTEN:\n\t\tif (cma_any_addr(cma_src_addr(id_priv)) && !id_priv->cma_dev)\n\t\t\tcma_cancel_listens(id_priv);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void cma_release_port(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_bind_list *bind_list = id_priv->bind_list;\n\tstruct net *net = id_priv->id.route.addr.dev_addr.net;\n\n\tif (!bind_list)\n\t\treturn;\n\n\tmutex_lock(&lock);\n\thlist_del(&id_priv->node);\n\tif (hlist_empty(&bind_list->owners)) {\n\t\tcma_ps_remove(net, bind_list->ps, bind_list->port);\n\t\tkfree(bind_list);\n\t}\n\tmutex_unlock(&lock);\n}\n\nstatic void destroy_mc(struct rdma_id_private *id_priv,\n\t\t       struct cma_multicast *mc)\n{\n\tbool send_only = mc->join_state == BIT(SENDONLY_FULLMEMBER_JOIN);\n\n\tif (rdma_cap_ib_mcast(id_priv->id.device, id_priv->id.port_num))\n\t\tib_sa_free_multicast(mc->sa_mc);\n\n\tif (rdma_protocol_roce(id_priv->id.device, id_priv->id.port_num)) {\n\t\tstruct rdma_dev_addr *dev_addr =\n\t\t\t&id_priv->id.route.addr.dev_addr;\n\t\tstruct net_device *ndev = NULL;\n\n\t\tif (dev_addr->bound_dev_if)\n\t\t\tndev = dev_get_by_index(dev_addr->net,\n\t\t\t\t\t\tdev_addr->bound_dev_if);\n\t\tif (ndev && !send_only) {\n\t\t\tenum ib_gid_type gid_type;\n\t\t\tunion ib_gid mgid;\n\n\t\t\tgid_type = id_priv->cma_dev->default_gid_type\n\t\t\t\t\t   [id_priv->id.port_num -\n\t\t\t\t\t    rdma_start_port(\n\t\t\t\t\t\t    id_priv->cma_dev->device)];\n\t\t\tcma_iboe_set_mgid((struct sockaddr *)&mc->addr, &mgid,\n\t\t\t\t\t  gid_type);\n\t\t\tcma_igmp_send(ndev, &mgid, false);\n\t\t}\n\t\tdev_put(ndev);\n\n\t\tcancel_work_sync(&mc->iboe_join.work);\n\t}\n\tkfree(mc);\n}\n\nstatic void cma_leave_mc_groups(struct rdma_id_private *id_priv)\n{\n\tstruct cma_multicast *mc;\n\n\twhile (!list_empty(&id_priv->mc_list)) {\n\t\tmc = list_first_entry(&id_priv->mc_list, struct cma_multicast,\n\t\t\t\t      list);\n\t\tlist_del(&mc->list);\n\t\tdestroy_mc(id_priv, mc);\n\t}\n}\n\nstatic void _destroy_id(struct rdma_id_private *id_priv,\n\t\t\tenum rdma_cm_state state)\n{\n\tcma_cancel_operation(id_priv, state);\n\n\trdma_restrack_del(&id_priv->res);\n\tcma_remove_id_from_tree(id_priv);\n\tif (id_priv->cma_dev) {\n\t\tif (rdma_cap_ib_cm(id_priv->id.device, 1)) {\n\t\t\tif (id_priv->cm_id.ib)\n\t\t\t\tib_destroy_cm_id(id_priv->cm_id.ib);\n\t\t} else if (rdma_cap_iw_cm(id_priv->id.device, 1)) {\n\t\t\tif (id_priv->cm_id.iw)\n\t\t\t\tiw_destroy_cm_id(id_priv->cm_id.iw);\n\t\t}\n\t\tcma_leave_mc_groups(id_priv);\n\t\tcma_release_dev(id_priv);\n\t}\n\n\tcma_release_port(id_priv);\n\tcma_id_put(id_priv);\n\twait_for_completion(&id_priv->comp);\n\n\tif (id_priv->internal_id)\n\t\tcma_id_put(id_priv->id.context);\n\n\tkfree(id_priv->id.route.path_rec);\n\tkfree(id_priv->id.route.path_rec_inbound);\n\tkfree(id_priv->id.route.path_rec_outbound);\n\n\tput_net(id_priv->id.route.addr.dev_addr.net);\n\tkfree(id_priv);\n}\n\n \nstatic void destroy_id_handler_unlock(struct rdma_id_private *id_priv)\n\t__releases(&idprv->handler_mutex)\n{\n\tenum rdma_cm_state state;\n\tunsigned long flags;\n\n\ttrace_cm_id_destroy(id_priv);\n\n\t \n\tlockdep_assert_held(&id_priv->handler_mutex);\n\tspin_lock_irqsave(&id_priv->lock, flags);\n\tstate = id_priv->state;\n\tid_priv->state = RDMA_CM_DESTROYING;\n\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\tmutex_unlock(&id_priv->handler_mutex);\n\t_destroy_id(id_priv, state);\n}\n\nvoid rdma_destroy_id(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tdestroy_id_handler_unlock(id_priv);\n}\nEXPORT_SYMBOL(rdma_destroy_id);\n\nstatic int cma_rep_recv(struct rdma_id_private *id_priv)\n{\n\tint ret;\n\n\tret = cma_modify_qp_rtr(id_priv, NULL);\n\tif (ret)\n\t\tgoto reject;\n\n\tret = cma_modify_qp_rts(id_priv, NULL);\n\tif (ret)\n\t\tgoto reject;\n\n\ttrace_cm_send_rtu(id_priv);\n\tret = ib_send_cm_rtu(id_priv->cm_id.ib, NULL, 0);\n\tif (ret)\n\t\tgoto reject;\n\n\treturn 0;\nreject:\n\tpr_debug_ratelimited(\"RDMA CM: CONNECT_ERROR: failed to handle reply. status %d\\n\", ret);\n\tcma_modify_qp_err(id_priv);\n\ttrace_cm_send_rej(id_priv);\n\tib_send_cm_rej(id_priv->cm_id.ib, IB_CM_REJ_CONSUMER_DEFINED,\n\t\t       NULL, 0, NULL, 0);\n\treturn ret;\n}\n\nstatic void cma_set_rep_event_data(struct rdma_cm_event *event,\n\t\t\t\t   const struct ib_cm_rep_event_param *rep_data,\n\t\t\t\t   void *private_data)\n{\n\tevent->param.conn.private_data = private_data;\n\tevent->param.conn.private_data_len = IB_CM_REP_PRIVATE_DATA_SIZE;\n\tevent->param.conn.responder_resources = rep_data->responder_resources;\n\tevent->param.conn.initiator_depth = rep_data->initiator_depth;\n\tevent->param.conn.flow_control = rep_data->flow_control;\n\tevent->param.conn.rnr_retry_count = rep_data->rnr_retry_count;\n\tevent->param.conn.srq = rep_data->srq;\n\tevent->param.conn.qp_num = rep_data->remote_qpn;\n\n\tevent->ece.vendor_id = rep_data->ece.vendor_id;\n\tevent->ece.attr_mod = rep_data->ece.attr_mod;\n}\n\nstatic int cma_cm_event_handler(struct rdma_id_private *id_priv,\n\t\t\t\tstruct rdma_cm_event *event)\n{\n\tint ret;\n\n\tlockdep_assert_held(&id_priv->handler_mutex);\n\n\ttrace_cm_event_handler(id_priv, event);\n\tret = id_priv->id.event_handler(&id_priv->id, event);\n\ttrace_cm_event_done(id_priv, event, ret);\n\treturn ret;\n}\n\nstatic int cma_ib_handler(struct ib_cm_id *cm_id,\n\t\t\t  const struct ib_cm_event *ib_event)\n{\n\tstruct rdma_id_private *id_priv = cm_id->context;\n\tstruct rdma_cm_event event = {};\n\tenum rdma_cm_state state;\n\tint ret;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tstate = READ_ONCE(id_priv->state);\n\tif ((ib_event->event != IB_CM_TIMEWAIT_EXIT &&\n\t     state != RDMA_CM_CONNECT) ||\n\t    (ib_event->event == IB_CM_TIMEWAIT_EXIT &&\n\t     state != RDMA_CM_DISCONNECT))\n\t\tgoto out;\n\n\tswitch (ib_event->event) {\n\tcase IB_CM_REQ_ERROR:\n\tcase IB_CM_REP_ERROR:\n\t\tevent.event = RDMA_CM_EVENT_UNREACHABLE;\n\t\tevent.status = -ETIMEDOUT;\n\t\tbreak;\n\tcase IB_CM_REP_RECEIVED:\n\t\tif (state == RDMA_CM_CONNECT &&\n\t\t    (id_priv->id.qp_type != IB_QPT_UD)) {\n\t\t\ttrace_cm_send_mra(id_priv);\n\t\t\tib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);\n\t\t}\n\t\tif (id_priv->id.qp) {\n\t\t\tevent.status = cma_rep_recv(id_priv);\n\t\t\tevent.event = event.status ? RDMA_CM_EVENT_CONNECT_ERROR :\n\t\t\t\t\t\t     RDMA_CM_EVENT_ESTABLISHED;\n\t\t} else {\n\t\t\tevent.event = RDMA_CM_EVENT_CONNECT_RESPONSE;\n\t\t}\n\t\tcma_set_rep_event_data(&event, &ib_event->param.rep_rcvd,\n\t\t\t\t       ib_event->private_data);\n\t\tbreak;\n\tcase IB_CM_RTU_RECEIVED:\n\tcase IB_CM_USER_ESTABLISHED:\n\t\tevent.event = RDMA_CM_EVENT_ESTABLISHED;\n\t\tbreak;\n\tcase IB_CM_DREQ_ERROR:\n\t\tevent.status = -ETIMEDOUT;\n\t\tfallthrough;\n\tcase IB_CM_DREQ_RECEIVED:\n\tcase IB_CM_DREP_RECEIVED:\n\t\tif (!cma_comp_exch(id_priv, RDMA_CM_CONNECT,\n\t\t\t\t   RDMA_CM_DISCONNECT))\n\t\t\tgoto out;\n\t\tevent.event = RDMA_CM_EVENT_DISCONNECTED;\n\t\tbreak;\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tevent.event = RDMA_CM_EVENT_TIMEWAIT_EXIT;\n\t\tbreak;\n\tcase IB_CM_MRA_RECEIVED:\n\t\t \n\t\tgoto out;\n\tcase IB_CM_REJ_RECEIVED:\n\t\tpr_debug_ratelimited(\"RDMA CM: REJECTED: %s\\n\", rdma_reject_msg(&id_priv->id,\n\t\t\t\t\t\t\t\t\t\tib_event->param.rej_rcvd.reason));\n\t\tcma_modify_qp_err(id_priv);\n\t\tevent.status = ib_event->param.rej_rcvd.reason;\n\t\tevent.event = RDMA_CM_EVENT_REJECTED;\n\t\tevent.param.conn.private_data = ib_event->private_data;\n\t\tevent.param.conn.private_data_len = IB_CM_REJ_PRIVATE_DATA_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"RDMA CMA: unexpected IB CM event: %d\\n\",\n\t\t       ib_event->event);\n\t\tgoto out;\n\t}\n\n\tret = cma_cm_event_handler(id_priv, &event);\n\tif (ret) {\n\t\t \n\t\tid_priv->cm_id.ib = NULL;\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\treturn ret;\n\t}\nout:\n\tmutex_unlock(&id_priv->handler_mutex);\n\treturn 0;\n}\n\nstatic struct rdma_id_private *\ncma_ib_new_conn_id(const struct rdma_cm_id *listen_id,\n\t\t   const struct ib_cm_event *ib_event,\n\t\t   struct net_device *net_dev)\n{\n\tstruct rdma_id_private *listen_id_priv;\n\tstruct rdma_id_private *id_priv;\n\tstruct rdma_cm_id *id;\n\tstruct rdma_route *rt;\n\tconst sa_family_t ss_family = listen_id->route.addr.src_addr.ss_family;\n\tstruct sa_path_rec *path = ib_event->param.req_rcvd.primary_path;\n\tconst __be64 service_id =\n\t\tib_event->param.req_rcvd.primary_path->service_id;\n\tint ret;\n\n\tlisten_id_priv = container_of(listen_id, struct rdma_id_private, id);\n\tid_priv = __rdma_create_id(listen_id->route.addr.dev_addr.net,\n\t\t\t\t   listen_id->event_handler, listen_id->context,\n\t\t\t\t   listen_id->ps,\n\t\t\t\t   ib_event->param.req_rcvd.qp_type,\n\t\t\t\t   listen_id_priv);\n\tif (IS_ERR(id_priv))\n\t\treturn NULL;\n\n\tid = &id_priv->id;\n\tif (cma_save_net_info((struct sockaddr *)&id->route.addr.src_addr,\n\t\t\t      (struct sockaddr *)&id->route.addr.dst_addr,\n\t\t\t      listen_id, ib_event, ss_family, service_id))\n\t\tgoto err;\n\n\trt = &id->route;\n\trt->num_pri_alt_paths = ib_event->param.req_rcvd.alternate_path ? 2 : 1;\n\trt->path_rec = kmalloc_array(rt->num_pri_alt_paths,\n\t\t\t\t     sizeof(*rt->path_rec), GFP_KERNEL);\n\tif (!rt->path_rec)\n\t\tgoto err;\n\n\trt->path_rec[0] = *path;\n\tif (rt->num_pri_alt_paths == 2)\n\t\trt->path_rec[1] = *ib_event->param.req_rcvd.alternate_path;\n\n\tif (net_dev) {\n\t\trdma_copy_src_l2_addr(&rt->addr.dev_addr, net_dev);\n\t} else {\n\t\tif (!cma_protocol_roce(listen_id) &&\n\t\t    cma_any_addr(cma_src_addr(id_priv))) {\n\t\t\trt->addr.dev_addr.dev_type = ARPHRD_INFINIBAND;\n\t\t\trdma_addr_set_sgid(&rt->addr.dev_addr, &rt->path_rec[0].sgid);\n\t\t\tib_addr_set_pkey(&rt->addr.dev_addr, be16_to_cpu(rt->path_rec[0].pkey));\n\t\t} else if (!cma_any_addr(cma_src_addr(id_priv))) {\n\t\t\tret = cma_translate_addr(cma_src_addr(id_priv), &rt->addr.dev_addr);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\trdma_addr_set_dgid(&rt->addr.dev_addr, &rt->path_rec[0].dgid);\n\n\tid_priv->state = RDMA_CM_CONNECT;\n\treturn id_priv;\n\nerr:\n\trdma_destroy_id(id);\n\treturn NULL;\n}\n\nstatic struct rdma_id_private *\ncma_ib_new_udp_id(const struct rdma_cm_id *listen_id,\n\t\t  const struct ib_cm_event *ib_event,\n\t\t  struct net_device *net_dev)\n{\n\tconst struct rdma_id_private *listen_id_priv;\n\tstruct rdma_id_private *id_priv;\n\tstruct rdma_cm_id *id;\n\tconst sa_family_t ss_family = listen_id->route.addr.src_addr.ss_family;\n\tstruct net *net = listen_id->route.addr.dev_addr.net;\n\tint ret;\n\n\tlisten_id_priv = container_of(listen_id, struct rdma_id_private, id);\n\tid_priv = __rdma_create_id(net, listen_id->event_handler,\n\t\t\t\t   listen_id->context, listen_id->ps, IB_QPT_UD,\n\t\t\t\t   listen_id_priv);\n\tif (IS_ERR(id_priv))\n\t\treturn NULL;\n\n\tid = &id_priv->id;\n\tif (cma_save_net_info((struct sockaddr *)&id->route.addr.src_addr,\n\t\t\t      (struct sockaddr *)&id->route.addr.dst_addr,\n\t\t\t      listen_id, ib_event, ss_family,\n\t\t\t      ib_event->param.sidr_req_rcvd.service_id))\n\t\tgoto err;\n\n\tif (net_dev) {\n\t\trdma_copy_src_l2_addr(&id->route.addr.dev_addr, net_dev);\n\t} else {\n\t\tif (!cma_any_addr(cma_src_addr(id_priv))) {\n\t\t\tret = cma_translate_addr(cma_src_addr(id_priv),\n\t\t\t\t\t\t &id->route.addr.dev_addr);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\n\tid_priv->state = RDMA_CM_CONNECT;\n\treturn id_priv;\nerr:\n\trdma_destroy_id(id);\n\treturn NULL;\n}\n\nstatic void cma_set_req_event_data(struct rdma_cm_event *event,\n\t\t\t\t   const struct ib_cm_req_event_param *req_data,\n\t\t\t\t   void *private_data, int offset)\n{\n\tevent->param.conn.private_data = private_data + offset;\n\tevent->param.conn.private_data_len = IB_CM_REQ_PRIVATE_DATA_SIZE - offset;\n\tevent->param.conn.responder_resources = req_data->responder_resources;\n\tevent->param.conn.initiator_depth = req_data->initiator_depth;\n\tevent->param.conn.flow_control = req_data->flow_control;\n\tevent->param.conn.retry_count = req_data->retry_count;\n\tevent->param.conn.rnr_retry_count = req_data->rnr_retry_count;\n\tevent->param.conn.srq = req_data->srq;\n\tevent->param.conn.qp_num = req_data->remote_qpn;\n\n\tevent->ece.vendor_id = req_data->ece.vendor_id;\n\tevent->ece.attr_mod = req_data->ece.attr_mod;\n}\n\nstatic int cma_ib_check_req_qp_type(const struct rdma_cm_id *id,\n\t\t\t\t    const struct ib_cm_event *ib_event)\n{\n\treturn (((ib_event->event == IB_CM_REQ_RECEIVED) &&\n\t\t (ib_event->param.req_rcvd.qp_type == id->qp_type)) ||\n\t\t((ib_event->event == IB_CM_SIDR_REQ_RECEIVED) &&\n\t\t (id->qp_type == IB_QPT_UD)) ||\n\t\t(!id->qp_type));\n}\n\nstatic int cma_ib_req_handler(struct ib_cm_id *cm_id,\n\t\t\t      const struct ib_cm_event *ib_event)\n{\n\tstruct rdma_id_private *listen_id, *conn_id = NULL;\n\tstruct rdma_cm_event event = {};\n\tstruct cma_req_info req = {};\n\tstruct net_device *net_dev;\n\tu8 offset;\n\tint ret;\n\n\tlisten_id = cma_ib_id_from_event(cm_id, ib_event, &req, &net_dev);\n\tif (IS_ERR(listen_id))\n\t\treturn PTR_ERR(listen_id);\n\n\ttrace_cm_req_handler(listen_id, ib_event->event);\n\tif (!cma_ib_check_req_qp_type(&listen_id->id, ib_event)) {\n\t\tret = -EINVAL;\n\t\tgoto net_dev_put;\n\t}\n\n\tmutex_lock(&listen_id->handler_mutex);\n\tif (READ_ONCE(listen_id->state) != RDMA_CM_LISTEN) {\n\t\tret = -ECONNABORTED;\n\t\tgoto err_unlock;\n\t}\n\n\toffset = cma_user_data_offset(listen_id);\n\tevent.event = RDMA_CM_EVENT_CONNECT_REQUEST;\n\tif (ib_event->event == IB_CM_SIDR_REQ_RECEIVED) {\n\t\tconn_id = cma_ib_new_udp_id(&listen_id->id, ib_event, net_dev);\n\t\tevent.param.ud.private_data = ib_event->private_data + offset;\n\t\tevent.param.ud.private_data_len =\n\t\t\t\tIB_CM_SIDR_REQ_PRIVATE_DATA_SIZE - offset;\n\t} else {\n\t\tconn_id = cma_ib_new_conn_id(&listen_id->id, ib_event, net_dev);\n\t\tcma_set_req_event_data(&event, &ib_event->param.req_rcvd,\n\t\t\t\t       ib_event->private_data, offset);\n\t}\n\tif (!conn_id) {\n\t\tret = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\tmutex_lock_nested(&conn_id->handler_mutex, SINGLE_DEPTH_NESTING);\n\tret = cma_ib_acquire_dev(conn_id, listen_id, &req);\n\tif (ret) {\n\t\tdestroy_id_handler_unlock(conn_id);\n\t\tgoto err_unlock;\n\t}\n\n\tconn_id->cm_id.ib = cm_id;\n\tcm_id->context = conn_id;\n\tcm_id->cm_handler = cma_ib_handler;\n\n\tret = cma_cm_event_handler(conn_id, &event);\n\tif (ret) {\n\t\t \n\t\tconn_id->cm_id.ib = NULL;\n\t\tmutex_unlock(&listen_id->handler_mutex);\n\t\tdestroy_id_handler_unlock(conn_id);\n\t\tgoto net_dev_put;\n\t}\n\n\tif (READ_ONCE(conn_id->state) == RDMA_CM_CONNECT &&\n\t    conn_id->id.qp_type != IB_QPT_UD) {\n\t\ttrace_cm_send_mra(cm_id->context);\n\t\tib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);\n\t}\n\tmutex_unlock(&conn_id->handler_mutex);\n\nerr_unlock:\n\tmutex_unlock(&listen_id->handler_mutex);\n\nnet_dev_put:\n\tdev_put(net_dev);\n\n\treturn ret;\n}\n\n__be64 rdma_get_service_id(struct rdma_cm_id *id, struct sockaddr *addr)\n{\n\tif (addr->sa_family == AF_IB)\n\t\treturn ((struct sockaddr_ib *) addr)->sib_sid;\n\n\treturn cpu_to_be64(((u64)id->ps << 16) + be16_to_cpu(cma_port(addr)));\n}\nEXPORT_SYMBOL(rdma_get_service_id);\n\nvoid rdma_read_gids(struct rdma_cm_id *cm_id, union ib_gid *sgid,\n\t\t    union ib_gid *dgid)\n{\n\tstruct rdma_addr *addr = &cm_id->route.addr;\n\n\tif (!cm_id->device) {\n\t\tif (sgid)\n\t\t\tmemset(sgid, 0, sizeof(*sgid));\n\t\tif (dgid)\n\t\t\tmemset(dgid, 0, sizeof(*dgid));\n\t\treturn;\n\t}\n\n\tif (rdma_protocol_roce(cm_id->device, cm_id->port_num)) {\n\t\tif (sgid)\n\t\t\trdma_ip2gid((struct sockaddr *)&addr->src_addr, sgid);\n\t\tif (dgid)\n\t\t\trdma_ip2gid((struct sockaddr *)&addr->dst_addr, dgid);\n\t} else {\n\t\tif (sgid)\n\t\t\trdma_addr_get_sgid(&addr->dev_addr, sgid);\n\t\tif (dgid)\n\t\t\trdma_addr_get_dgid(&addr->dev_addr, dgid);\n\t}\n}\nEXPORT_SYMBOL(rdma_read_gids);\n\nstatic int cma_iw_handler(struct iw_cm_id *iw_id, struct iw_cm_event *iw_event)\n{\n\tstruct rdma_id_private *id_priv = iw_id->context;\n\tstruct rdma_cm_event event = {};\n\tint ret = 0;\n\tstruct sockaddr *laddr = (struct sockaddr *)&iw_event->local_addr;\n\tstruct sockaddr *raddr = (struct sockaddr *)&iw_event->remote_addr;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (READ_ONCE(id_priv->state) != RDMA_CM_CONNECT)\n\t\tgoto out;\n\n\tswitch (iw_event->event) {\n\tcase IW_CM_EVENT_CLOSE:\n\t\tevent.event = RDMA_CM_EVENT_DISCONNECTED;\n\t\tbreak;\n\tcase IW_CM_EVENT_CONNECT_REPLY:\n\t\tmemcpy(cma_src_addr(id_priv), laddr,\n\t\t       rdma_addr_size(laddr));\n\t\tmemcpy(cma_dst_addr(id_priv), raddr,\n\t\t       rdma_addr_size(raddr));\n\t\tswitch (iw_event->status) {\n\t\tcase 0:\n\t\t\tevent.event = RDMA_CM_EVENT_ESTABLISHED;\n\t\t\tevent.param.conn.initiator_depth = iw_event->ird;\n\t\t\tevent.param.conn.responder_resources = iw_event->ord;\n\t\t\tbreak;\n\t\tcase -ECONNRESET:\n\t\tcase -ECONNREFUSED:\n\t\t\tevent.event = RDMA_CM_EVENT_REJECTED;\n\t\t\tbreak;\n\t\tcase -ETIMEDOUT:\n\t\t\tevent.event = RDMA_CM_EVENT_UNREACHABLE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tevent.event = RDMA_CM_EVENT_CONNECT_ERROR;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase IW_CM_EVENT_ESTABLISHED:\n\t\tevent.event = RDMA_CM_EVENT_ESTABLISHED;\n\t\tevent.param.conn.initiator_depth = iw_event->ird;\n\t\tevent.param.conn.responder_resources = iw_event->ord;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tevent.status = iw_event->status;\n\tevent.param.conn.private_data = iw_event->private_data;\n\tevent.param.conn.private_data_len = iw_event->private_data_len;\n\tret = cma_cm_event_handler(id_priv, &event);\n\tif (ret) {\n\t\t \n\t\tid_priv->cm_id.iw = NULL;\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\treturn ret;\n\t}\n\nout:\n\tmutex_unlock(&id_priv->handler_mutex);\n\treturn ret;\n}\n\nstatic int iw_conn_req_handler(struct iw_cm_id *cm_id,\n\t\t\t       struct iw_cm_event *iw_event)\n{\n\tstruct rdma_id_private *listen_id, *conn_id;\n\tstruct rdma_cm_event event = {};\n\tint ret = -ECONNABORTED;\n\tstruct sockaddr *laddr = (struct sockaddr *)&iw_event->local_addr;\n\tstruct sockaddr *raddr = (struct sockaddr *)&iw_event->remote_addr;\n\n\tevent.event = RDMA_CM_EVENT_CONNECT_REQUEST;\n\tevent.param.conn.private_data = iw_event->private_data;\n\tevent.param.conn.private_data_len = iw_event->private_data_len;\n\tevent.param.conn.initiator_depth = iw_event->ird;\n\tevent.param.conn.responder_resources = iw_event->ord;\n\n\tlisten_id = cm_id->context;\n\n\tmutex_lock(&listen_id->handler_mutex);\n\tif (READ_ONCE(listen_id->state) != RDMA_CM_LISTEN)\n\t\tgoto out;\n\n\t \n\tconn_id = __rdma_create_id(listen_id->id.route.addr.dev_addr.net,\n\t\t\t\t   listen_id->id.event_handler,\n\t\t\t\t   listen_id->id.context, RDMA_PS_TCP,\n\t\t\t\t   IB_QPT_RC, listen_id);\n\tif (IS_ERR(conn_id)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tmutex_lock_nested(&conn_id->handler_mutex, SINGLE_DEPTH_NESTING);\n\tconn_id->state = RDMA_CM_CONNECT;\n\n\tret = rdma_translate_ip(laddr, &conn_id->id.route.addr.dev_addr);\n\tif (ret) {\n\t\tmutex_unlock(&listen_id->handler_mutex);\n\t\tdestroy_id_handler_unlock(conn_id);\n\t\treturn ret;\n\t}\n\n\tret = cma_iw_acquire_dev(conn_id, listen_id);\n\tif (ret) {\n\t\tmutex_unlock(&listen_id->handler_mutex);\n\t\tdestroy_id_handler_unlock(conn_id);\n\t\treturn ret;\n\t}\n\n\tconn_id->cm_id.iw = cm_id;\n\tcm_id->context = conn_id;\n\tcm_id->cm_handler = cma_iw_handler;\n\n\tmemcpy(cma_src_addr(conn_id), laddr, rdma_addr_size(laddr));\n\tmemcpy(cma_dst_addr(conn_id), raddr, rdma_addr_size(raddr));\n\n\tret = cma_cm_event_handler(conn_id, &event);\n\tif (ret) {\n\t\t \n\t\tconn_id->cm_id.iw = NULL;\n\t\tmutex_unlock(&listen_id->handler_mutex);\n\t\tdestroy_id_handler_unlock(conn_id);\n\t\treturn ret;\n\t}\n\n\tmutex_unlock(&conn_id->handler_mutex);\n\nout:\n\tmutex_unlock(&listen_id->handler_mutex);\n\treturn ret;\n}\n\nstatic int cma_ib_listen(struct rdma_id_private *id_priv)\n{\n\tstruct sockaddr *addr;\n\tstruct ib_cm_id\t*id;\n\t__be64 svc_id;\n\n\taddr = cma_src_addr(id_priv);\n\tsvc_id = rdma_get_service_id(&id_priv->id, addr);\n\tid = ib_cm_insert_listen(id_priv->id.device,\n\t\t\t\t cma_ib_req_handler, svc_id);\n\tif (IS_ERR(id))\n\t\treturn PTR_ERR(id);\n\tid_priv->cm_id.ib = id;\n\n\treturn 0;\n}\n\nstatic int cma_iw_listen(struct rdma_id_private *id_priv, int backlog)\n{\n\tint ret;\n\tstruct iw_cm_id\t*id;\n\n\tid = iw_create_cm_id(id_priv->id.device,\n\t\t\t     iw_conn_req_handler,\n\t\t\t     id_priv);\n\tif (IS_ERR(id))\n\t\treturn PTR_ERR(id);\n\n\tmutex_lock(&id_priv->qp_mutex);\n\tid->tos = id_priv->tos;\n\tid->tos_set = id_priv->tos_set;\n\tmutex_unlock(&id_priv->qp_mutex);\n\tid->afonly = id_priv->afonly;\n\tid_priv->cm_id.iw = id;\n\n\tmemcpy(&id_priv->cm_id.iw->local_addr, cma_src_addr(id_priv),\n\t       rdma_addr_size(cma_src_addr(id_priv)));\n\n\tret = iw_cm_listen(id_priv->cm_id.iw, backlog);\n\n\tif (ret) {\n\t\tiw_destroy_cm_id(id_priv->cm_id.iw);\n\t\tid_priv->cm_id.iw = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic int cma_listen_handler(struct rdma_cm_id *id,\n\t\t\t      struct rdma_cm_event *event)\n{\n\tstruct rdma_id_private *id_priv = id->context;\n\n\t \n\tif (event->event == RDMA_CM_EVENT_DEVICE_REMOVAL)\n\t\treturn -1;\n\n\tid->context = id_priv->id.context;\n\tid->event_handler = id_priv->id.event_handler;\n\ttrace_cm_event_handler(id_priv, event);\n\treturn id_priv->id.event_handler(id, event);\n}\n\nstatic int cma_listen_on_dev(struct rdma_id_private *id_priv,\n\t\t\t     struct cma_device *cma_dev,\n\t\t\t     struct rdma_id_private **to_destroy)\n{\n\tstruct rdma_id_private *dev_id_priv;\n\tstruct net *net = id_priv->id.route.addr.dev_addr.net;\n\tint ret;\n\n\tlockdep_assert_held(&lock);\n\n\t*to_destroy = NULL;\n\tif (cma_family(id_priv) == AF_IB && !rdma_cap_ib_cm(cma_dev->device, 1))\n\t\treturn 0;\n\n\tdev_id_priv =\n\t\t__rdma_create_id(net, cma_listen_handler, id_priv,\n\t\t\t\t id_priv->id.ps, id_priv->id.qp_type, id_priv);\n\tif (IS_ERR(dev_id_priv))\n\t\treturn PTR_ERR(dev_id_priv);\n\n\tdev_id_priv->state = RDMA_CM_ADDR_BOUND;\n\tmemcpy(cma_src_addr(dev_id_priv), cma_src_addr(id_priv),\n\t       rdma_addr_size(cma_src_addr(id_priv)));\n\n\t_cma_attach_to_dev(dev_id_priv, cma_dev);\n\trdma_restrack_add(&dev_id_priv->res);\n\tcma_id_get(id_priv);\n\tdev_id_priv->internal_id = 1;\n\tdev_id_priv->afonly = id_priv->afonly;\n\tmutex_lock(&id_priv->qp_mutex);\n\tdev_id_priv->tos_set = id_priv->tos_set;\n\tdev_id_priv->tos = id_priv->tos;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\tret = rdma_listen(&dev_id_priv->id, id_priv->backlog);\n\tif (ret)\n\t\tgoto err_listen;\n\tlist_add_tail(&dev_id_priv->listen_item, &id_priv->listen_list);\n\treturn 0;\nerr_listen:\n\t \n\t*to_destroy = dev_id_priv;\n\tdev_warn(&cma_dev->device->dev, \"RDMA CMA: %s, error %d\\n\", __func__, ret);\n\treturn ret;\n}\n\nstatic int cma_listen_on_all(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_id_private *to_destroy;\n\tstruct cma_device *cma_dev;\n\tint ret;\n\n\tmutex_lock(&lock);\n\tlist_add_tail(&id_priv->listen_any_item, &listen_any_list);\n\tlist_for_each_entry(cma_dev, &dev_list, list) {\n\t\tret = cma_listen_on_dev(id_priv, cma_dev, &to_destroy);\n\t\tif (ret) {\n\t\t\t \n\t\t\tif (to_destroy)\n\t\t\t\tlist_del_init(&to_destroy->device_item);\n\t\t\tgoto err_listen;\n\t\t}\n\t}\n\tmutex_unlock(&lock);\n\treturn 0;\n\nerr_listen:\n\t_cma_cancel_listens(id_priv);\n\tmutex_unlock(&lock);\n\tif (to_destroy)\n\t\trdma_destroy_id(&to_destroy->id);\n\treturn ret;\n}\n\nvoid rdma_set_service_type(struct rdma_cm_id *id, int tos)\n{\n\tstruct rdma_id_private *id_priv;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tmutex_lock(&id_priv->qp_mutex);\n\tid_priv->tos = (u8) tos;\n\tid_priv->tos_set = true;\n\tmutex_unlock(&id_priv->qp_mutex);\n}\nEXPORT_SYMBOL(rdma_set_service_type);\n\n \nint rdma_set_ack_timeout(struct rdma_cm_id *id, u8 timeout)\n{\n\tstruct rdma_id_private *id_priv;\n\n\tif (id->qp_type != IB_QPT_RC && id->qp_type != IB_QPT_XRC_INI)\n\t\treturn -EINVAL;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tmutex_lock(&id_priv->qp_mutex);\n\tid_priv->timeout = timeout;\n\tid_priv->timeout_set = true;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_set_ack_timeout);\n\n \nint rdma_set_min_rnr_timer(struct rdma_cm_id *id, u8 min_rnr_timer)\n{\n\tstruct rdma_id_private *id_priv;\n\n\t \n\tif (min_rnr_timer & 0xe0)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON(id->qp_type != IB_QPT_RC && id->qp_type != IB_QPT_XRC_TGT))\n\t\treturn -EINVAL;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tmutex_lock(&id_priv->qp_mutex);\n\tid_priv->min_rnr_timer = min_rnr_timer;\n\tid_priv->min_rnr_timer_set = true;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_set_min_rnr_timer);\n\nstatic int route_set_path_rec_inbound(struct cma_work *work,\n\t\t\t\t      struct sa_path_rec *path_rec)\n{\n\tstruct rdma_route *route = &work->id->id.route;\n\n\tif (!route->path_rec_inbound) {\n\t\troute->path_rec_inbound =\n\t\t\tkzalloc(sizeof(*route->path_rec_inbound), GFP_KERNEL);\n\t\tif (!route->path_rec_inbound)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t*route->path_rec_inbound = *path_rec;\n\treturn 0;\n}\n\nstatic int route_set_path_rec_outbound(struct cma_work *work,\n\t\t\t\t       struct sa_path_rec *path_rec)\n{\n\tstruct rdma_route *route = &work->id->id.route;\n\n\tif (!route->path_rec_outbound) {\n\t\troute->path_rec_outbound =\n\t\t\tkzalloc(sizeof(*route->path_rec_outbound), GFP_KERNEL);\n\t\tif (!route->path_rec_outbound)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t*route->path_rec_outbound = *path_rec;\n\treturn 0;\n}\n\nstatic void cma_query_handler(int status, struct sa_path_rec *path_rec,\n\t\t\t      unsigned int num_prs, void *context)\n{\n\tstruct cma_work *work = context;\n\tstruct rdma_route *route;\n\tint i;\n\n\troute = &work->id->id.route;\n\n\tif (status)\n\t\tgoto fail;\n\n\tfor (i = 0; i < num_prs; i++) {\n\t\tif (!path_rec[i].flags || (path_rec[i].flags & IB_PATH_GMP))\n\t\t\t*route->path_rec = path_rec[i];\n\t\telse if (path_rec[i].flags & IB_PATH_INBOUND)\n\t\t\tstatus = route_set_path_rec_inbound(work, &path_rec[i]);\n\t\telse if (path_rec[i].flags & IB_PATH_OUTBOUND)\n\t\t\tstatus = route_set_path_rec_outbound(work,\n\t\t\t\t\t\t\t     &path_rec[i]);\n\t\telse\n\t\t\tstatus = -EINVAL;\n\n\t\tif (status)\n\t\t\tgoto fail;\n\t}\n\n\troute->num_pri_alt_paths = 1;\n\tqueue_work(cma_wq, &work->work);\n\treturn;\n\nfail:\n\twork->old_state = RDMA_CM_ROUTE_QUERY;\n\twork->new_state = RDMA_CM_ADDR_RESOLVED;\n\twork->event.event = RDMA_CM_EVENT_ROUTE_ERROR;\n\twork->event.status = status;\n\tpr_debug_ratelimited(\"RDMA CM: ROUTE_ERROR: failed to query path. status %d\\n\",\n\t\t\t     status);\n\tqueue_work(cma_wq, &work->work);\n}\n\nstatic int cma_query_ib_route(struct rdma_id_private *id_priv,\n\t\t\t      unsigned long timeout_ms, struct cma_work *work)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tstruct sa_path_rec path_rec;\n\tib_sa_comp_mask comp_mask;\n\tstruct sockaddr_in6 *sin6;\n\tstruct sockaddr_ib *sib;\n\n\tmemset(&path_rec, 0, sizeof path_rec);\n\n\tif (rdma_cap_opa_ah(id_priv->id.device, id_priv->id.port_num))\n\t\tpath_rec.rec_type = SA_PATH_REC_TYPE_OPA;\n\telse\n\t\tpath_rec.rec_type = SA_PATH_REC_TYPE_IB;\n\trdma_addr_get_sgid(dev_addr, &path_rec.sgid);\n\trdma_addr_get_dgid(dev_addr, &path_rec.dgid);\n\tpath_rec.pkey = cpu_to_be16(ib_addr_get_pkey(dev_addr));\n\tpath_rec.numb_path = 1;\n\tpath_rec.reversible = 1;\n\tpath_rec.service_id = rdma_get_service_id(&id_priv->id,\n\t\t\t\t\t\t  cma_dst_addr(id_priv));\n\n\tcomp_mask = IB_SA_PATH_REC_DGID | IB_SA_PATH_REC_SGID |\n\t\t    IB_SA_PATH_REC_PKEY | IB_SA_PATH_REC_NUMB_PATH |\n\t\t    IB_SA_PATH_REC_REVERSIBLE | IB_SA_PATH_REC_SERVICE_ID;\n\n\tswitch (cma_family(id_priv)) {\n\tcase AF_INET:\n\t\tpath_rec.qos_class = cpu_to_be16((u16) id_priv->tos);\n\t\tcomp_mask |= IB_SA_PATH_REC_QOS_CLASS;\n\t\tbreak;\n\tcase AF_INET6:\n\t\tsin6 = (struct sockaddr_in6 *) cma_src_addr(id_priv);\n\t\tpath_rec.traffic_class = (u8) (be32_to_cpu(sin6->sin6_flowinfo) >> 20);\n\t\tcomp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;\n\t\tbreak;\n\tcase AF_IB:\n\t\tsib = (struct sockaddr_ib *) cma_src_addr(id_priv);\n\t\tpath_rec.traffic_class = (u8) (be32_to_cpu(sib->sib_flowinfo) >> 20);\n\t\tcomp_mask |= IB_SA_PATH_REC_TRAFFIC_CLASS;\n\t\tbreak;\n\t}\n\n\tid_priv->query_id = ib_sa_path_rec_get(&sa_client, id_priv->id.device,\n\t\t\t\t\t       id_priv->id.port_num, &path_rec,\n\t\t\t\t\t       comp_mask, timeout_ms,\n\t\t\t\t\t       GFP_KERNEL, cma_query_handler,\n\t\t\t\t\t       work, &id_priv->query);\n\n\treturn (id_priv->query_id < 0) ? id_priv->query_id : 0;\n}\n\nstatic void cma_iboe_join_work_handler(struct work_struct *work)\n{\n\tstruct cma_multicast *mc =\n\t\tcontainer_of(work, struct cma_multicast, iboe_join.work);\n\tstruct rdma_cm_event *event = &mc->iboe_join.event;\n\tstruct rdma_id_private *id_priv = mc->id_priv;\n\tint ret;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (READ_ONCE(id_priv->state) == RDMA_CM_DESTROYING ||\n\t    READ_ONCE(id_priv->state) == RDMA_CM_DEVICE_REMOVAL)\n\t\tgoto out_unlock;\n\n\tret = cma_cm_event_handler(id_priv, event);\n\tWARN_ON(ret);\n\nout_unlock:\n\tmutex_unlock(&id_priv->handler_mutex);\n\tif (event->event == RDMA_CM_EVENT_MULTICAST_JOIN)\n\t\trdma_destroy_ah_attr(&event->param.ud.ah_attr);\n}\n\nstatic void cma_work_handler(struct work_struct *_work)\n{\n\tstruct cma_work *work = container_of(_work, struct cma_work, work);\n\tstruct rdma_id_private *id_priv = work->id;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (READ_ONCE(id_priv->state) == RDMA_CM_DESTROYING ||\n\t    READ_ONCE(id_priv->state) == RDMA_CM_DEVICE_REMOVAL)\n\t\tgoto out_unlock;\n\tif (work->old_state != 0 || work->new_state != 0) {\n\t\tif (!cma_comp_exch(id_priv, work->old_state, work->new_state))\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (cma_cm_event_handler(id_priv, &work->event)) {\n\t\tcma_id_put(id_priv);\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\tgoto out_free;\n\t}\n\nout_unlock:\n\tmutex_unlock(&id_priv->handler_mutex);\n\tcma_id_put(id_priv);\nout_free:\n\tif (work->event.event == RDMA_CM_EVENT_MULTICAST_JOIN)\n\t\trdma_destroy_ah_attr(&work->event.param.ud.ah_attr);\n\tkfree(work);\n}\n\nstatic void cma_init_resolve_route_work(struct cma_work *work,\n\t\t\t\t\tstruct rdma_id_private *id_priv)\n{\n\twork->id = id_priv;\n\tINIT_WORK(&work->work, cma_work_handler);\n\twork->old_state = RDMA_CM_ROUTE_QUERY;\n\twork->new_state = RDMA_CM_ROUTE_RESOLVED;\n\twork->event.event = RDMA_CM_EVENT_ROUTE_RESOLVED;\n}\n\nstatic void enqueue_resolve_addr_work(struct cma_work *work,\n\t\t\t\t      struct rdma_id_private *id_priv)\n{\n\t \n\tcma_id_get(id_priv);\n\n\twork->id = id_priv;\n\tINIT_WORK(&work->work, cma_work_handler);\n\twork->old_state = RDMA_CM_ADDR_QUERY;\n\twork->new_state = RDMA_CM_ADDR_RESOLVED;\n\twork->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;\n\n\tqueue_work(cma_wq, &work->work);\n}\n\nstatic int cma_resolve_ib_route(struct rdma_id_private *id_priv,\n\t\t\t\tunsigned long timeout_ms)\n{\n\tstruct rdma_route *route = &id_priv->id.route;\n\tstruct cma_work *work;\n\tint ret;\n\n\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\tcma_init_resolve_route_work(work, id_priv);\n\n\tif (!route->path_rec)\n\t\troute->path_rec = kmalloc(sizeof *route->path_rec, GFP_KERNEL);\n\tif (!route->path_rec) {\n\t\tret = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tret = cma_query_ib_route(id_priv, timeout_ms, work);\n\tif (ret)\n\t\tgoto err2;\n\n\treturn 0;\nerr2:\n\tkfree(route->path_rec);\n\troute->path_rec = NULL;\nerr1:\n\tkfree(work);\n\treturn ret;\n}\n\nstatic enum ib_gid_type cma_route_gid_type(enum rdma_network_type network_type,\n\t\t\t\t\t   unsigned long supported_gids,\n\t\t\t\t\t   enum ib_gid_type default_gid)\n{\n\tif ((network_type == RDMA_NETWORK_IPV4 ||\n\t     network_type == RDMA_NETWORK_IPV6) &&\n\t    test_bit(IB_GID_TYPE_ROCE_UDP_ENCAP, &supported_gids))\n\t\treturn IB_GID_TYPE_ROCE_UDP_ENCAP;\n\n\treturn default_gid;\n}\n\n \nstatic struct net_device *\ncma_iboe_set_path_rec_l2_fields(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_route *route = &id_priv->id.route;\n\tenum ib_gid_type gid_type = IB_GID_TYPE_ROCE;\n\tstruct rdma_addr *addr = &route->addr;\n\tunsigned long supported_gids;\n\tstruct net_device *ndev;\n\n\tif (!addr->dev_addr.bound_dev_if)\n\t\treturn NULL;\n\n\tndev = dev_get_by_index(addr->dev_addr.net,\n\t\t\t\taddr->dev_addr.bound_dev_if);\n\tif (!ndev)\n\t\treturn NULL;\n\n\tsupported_gids = roce_gid_type_mask_support(id_priv->id.device,\n\t\t\t\t\t\t    id_priv->id.port_num);\n\tgid_type = cma_route_gid_type(addr->dev_addr.network,\n\t\t\t\t      supported_gids,\n\t\t\t\t      id_priv->gid_type);\n\t \n\tif (gid_type < ib_network_to_gid_type(addr->dev_addr.network))\n\t\tgid_type = ib_network_to_gid_type(addr->dev_addr.network);\n\troute->path_rec->rec_type = sa_conv_gid_to_pathrec_type(gid_type);\n\n\troute->path_rec->roce.route_resolved = true;\n\tsa_path_set_dmac(route->path_rec, addr->dev_addr.dst_dev_addr);\n\treturn ndev;\n}\n\nint rdma_set_ib_path(struct rdma_cm_id *id,\n\t\t     struct sa_path_rec *path_rec)\n{\n\tstruct rdma_id_private *id_priv;\n\tstruct net_device *ndev;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_RESOLVED,\n\t\t\t   RDMA_CM_ROUTE_RESOLVED))\n\t\treturn -EINVAL;\n\n\tid->route.path_rec = kmemdup(path_rec, sizeof(*path_rec),\n\t\t\t\t     GFP_KERNEL);\n\tif (!id->route.path_rec) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tif (rdma_protocol_roce(id->device, id->port_num)) {\n\t\tndev = cma_iboe_set_path_rec_l2_fields(id_priv);\n\t\tif (!ndev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err_free;\n\t\t}\n\t\tdev_put(ndev);\n\t}\n\n\tid->route.num_pri_alt_paths = 1;\n\treturn 0;\n\nerr_free:\n\tkfree(id->route.path_rec);\n\tid->route.path_rec = NULL;\nerr:\n\tcma_comp_exch(id_priv, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_ADDR_RESOLVED);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_set_ib_path);\n\nstatic int cma_resolve_iw_route(struct rdma_id_private *id_priv)\n{\n\tstruct cma_work *work;\n\n\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\tcma_init_resolve_route_work(work, id_priv);\n\tqueue_work(cma_wq, &work->work);\n\treturn 0;\n}\n\nstatic int get_vlan_ndev_tc(struct net_device *vlan_ndev, int prio)\n{\n\tstruct net_device *dev;\n\n\tdev = vlan_dev_real_dev(vlan_ndev);\n\tif (dev->num_tc)\n\t\treturn netdev_get_prio_tc_map(dev, prio);\n\n\treturn (vlan_dev_get_egress_qos_mask(vlan_ndev, prio) &\n\t\tVLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;\n}\n\nstruct iboe_prio_tc_map {\n\tint input_prio;\n\tint output_tc;\n\tbool found;\n};\n\nstatic int get_lower_vlan_dev_tc(struct net_device *dev,\n\t\t\t\t struct netdev_nested_priv *priv)\n{\n\tstruct iboe_prio_tc_map *map = (struct iboe_prio_tc_map *)priv->data;\n\n\tif (is_vlan_dev(dev))\n\t\tmap->output_tc = get_vlan_ndev_tc(dev, map->input_prio);\n\telse if (dev->num_tc)\n\t\tmap->output_tc = netdev_get_prio_tc_map(dev, map->input_prio);\n\telse\n\t\tmap->output_tc = 0;\n\t \n\tmap->found = true;\n\treturn 1;\n}\n\nstatic int iboe_tos_to_sl(struct net_device *ndev, int tos)\n{\n\tstruct iboe_prio_tc_map prio_tc_map = {};\n\tint prio = rt_tos2priority(tos);\n\tstruct netdev_nested_priv priv;\n\n\t \n\tif (is_vlan_dev(ndev))\n\t\treturn get_vlan_ndev_tc(ndev, prio);\n\n\tprio_tc_map.input_prio = prio;\n\tpriv.data = (void *)&prio_tc_map;\n\trcu_read_lock();\n\tnetdev_walk_all_lower_dev_rcu(ndev,\n\t\t\t\t      get_lower_vlan_dev_tc,\n\t\t\t\t      &priv);\n\trcu_read_unlock();\n\t \n\tif (prio_tc_map.found)\n\t\treturn prio_tc_map.output_tc;\n\telse if (ndev->num_tc)\n\t\treturn netdev_get_prio_tc_map(ndev, prio);\n\telse\n\t\treturn 0;\n}\n\nstatic __be32 cma_get_roce_udp_flow_label(struct rdma_id_private *id_priv)\n{\n\tstruct sockaddr_in6 *addr6;\n\tu16 dport, sport;\n\tu32 hash, fl;\n\n\taddr6 = (struct sockaddr_in6 *)cma_src_addr(id_priv);\n\tfl = be32_to_cpu(addr6->sin6_flowinfo) & IB_GRH_FLOWLABEL_MASK;\n\tif ((cma_family(id_priv) != AF_INET6) || !fl) {\n\t\tdport = be16_to_cpu(cma_port(cma_dst_addr(id_priv)));\n\t\tsport = be16_to_cpu(cma_port(cma_src_addr(id_priv)));\n\t\thash = (u32)sport * 31 + dport;\n\t\tfl = hash & IB_GRH_FLOWLABEL_MASK;\n\t}\n\n\treturn cpu_to_be32(fl);\n}\n\nstatic int cma_resolve_iboe_route(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_route *route = &id_priv->id.route;\n\tstruct rdma_addr *addr = &route->addr;\n\tstruct cma_work *work;\n\tint ret;\n\tstruct net_device *ndev;\n\n\tu8 default_roce_tos = id_priv->cma_dev->default_roce_tos[id_priv->id.port_num -\n\t\t\t\t\trdma_start_port(id_priv->cma_dev->device)];\n\tu8 tos;\n\n\tmutex_lock(&id_priv->qp_mutex);\n\ttos = id_priv->tos_set ? id_priv->tos : default_roce_tos;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\troute->path_rec = kzalloc(sizeof *route->path_rec, GFP_KERNEL);\n\tif (!route->path_rec) {\n\t\tret = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\troute->num_pri_alt_paths = 1;\n\n\tndev = cma_iboe_set_path_rec_l2_fields(id_priv);\n\tif (!ndev) {\n\t\tret = -ENODEV;\n\t\tgoto err2;\n\t}\n\n\trdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr,\n\t\t    &route->path_rec->sgid);\n\trdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.dst_addr,\n\t\t    &route->path_rec->dgid);\n\n\tif (((struct sockaddr *)&id_priv->id.route.addr.dst_addr)->sa_family != AF_IB)\n\t\t \n\t\troute->path_rec->hop_limit = addr->dev_addr.hoplimit;\n\telse\n\t\troute->path_rec->hop_limit = 1;\n\troute->path_rec->reversible = 1;\n\troute->path_rec->pkey = cpu_to_be16(0xffff);\n\troute->path_rec->mtu_selector = IB_SA_EQ;\n\troute->path_rec->sl = iboe_tos_to_sl(ndev, tos);\n\troute->path_rec->traffic_class = tos;\n\troute->path_rec->mtu = iboe_get_mtu(ndev->mtu);\n\troute->path_rec->rate_selector = IB_SA_EQ;\n\troute->path_rec->rate = IB_RATE_PORT_CURRENT;\n\tdev_put(ndev);\n\troute->path_rec->packet_life_time_selector = IB_SA_EQ;\n\t \n\tmutex_lock(&id_priv->qp_mutex);\n\tif (id_priv->timeout_set && id_priv->timeout)\n\t\troute->path_rec->packet_life_time = id_priv->timeout - 1;\n\telse\n\t\troute->path_rec->packet_life_time = CMA_IBOE_PACKET_LIFETIME;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\tif (!route->path_rec->mtu) {\n\t\tret = -EINVAL;\n\t\tgoto err2;\n\t}\n\n\tif (rdma_protocol_roce_udp_encap(id_priv->id.device,\n\t\t\t\t\t id_priv->id.port_num))\n\t\troute->path_rec->flow_label =\n\t\t\tcma_get_roce_udp_flow_label(id_priv);\n\n\tcma_init_resolve_route_work(work, id_priv);\n\tqueue_work(cma_wq, &work->work);\n\n\treturn 0;\n\nerr2:\n\tkfree(route->path_rec);\n\troute->path_rec = NULL;\n\troute->num_pri_alt_paths = 0;\nerr1:\n\tkfree(work);\n\treturn ret;\n}\n\nint rdma_resolve_route(struct rdma_cm_id *id, unsigned long timeout_ms)\n{\n\tstruct rdma_id_private *id_priv;\n\tint ret;\n\n\tif (!timeout_ms)\n\t\treturn -EINVAL;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_RESOLVED, RDMA_CM_ROUTE_QUERY))\n\t\treturn -EINVAL;\n\n\tcma_id_get(id_priv);\n\tif (rdma_cap_ib_sa(id->device, id->port_num))\n\t\tret = cma_resolve_ib_route(id_priv, timeout_ms);\n\telse if (rdma_protocol_roce(id->device, id->port_num)) {\n\t\tret = cma_resolve_iboe_route(id_priv);\n\t\tif (!ret)\n\t\t\tcma_add_id_to_tree(id_priv);\n\t}\n\telse if (rdma_protocol_iwarp(id->device, id->port_num))\n\t\tret = cma_resolve_iw_route(id_priv);\n\telse\n\t\tret = -ENOSYS;\n\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tcma_comp_exch(id_priv, RDMA_CM_ROUTE_QUERY, RDMA_CM_ADDR_RESOLVED);\n\tcma_id_put(id_priv);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_resolve_route);\n\nstatic void cma_set_loopback(struct sockaddr *addr)\n{\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\t((struct sockaddr_in *) addr)->sin_addr.s_addr = htonl(INADDR_LOOPBACK);\n\t\tbreak;\n\tcase AF_INET6:\n\t\tipv6_addr_set(&((struct sockaddr_in6 *) addr)->sin6_addr,\n\t\t\t      0, 0, 0, htonl(1));\n\t\tbreak;\n\tdefault:\n\t\tib_addr_set(&((struct sockaddr_ib *) addr)->sib_addr,\n\t\t\t    0, 0, 0, htonl(1));\n\t\tbreak;\n\t}\n}\n\nstatic int cma_bind_loopback(struct rdma_id_private *id_priv)\n{\n\tstruct cma_device *cma_dev, *cur_dev;\n\tunion ib_gid gid;\n\tenum ib_port_state port_state;\n\tunsigned int p;\n\tu16 pkey;\n\tint ret;\n\n\tcma_dev = NULL;\n\tmutex_lock(&lock);\n\tlist_for_each_entry(cur_dev, &dev_list, list) {\n\t\tif (cma_family(id_priv) == AF_IB &&\n\t\t    !rdma_cap_ib_cm(cur_dev->device, 1))\n\t\t\tcontinue;\n\n\t\tif (!cma_dev)\n\t\t\tcma_dev = cur_dev;\n\n\t\trdma_for_each_port (cur_dev->device, p) {\n\t\t\tif (!ib_get_cached_port_state(cur_dev->device, p, &port_state) &&\n\t\t\t    port_state == IB_PORT_ACTIVE) {\n\t\t\t\tcma_dev = cur_dev;\n\t\t\t\tgoto port_found;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!cma_dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tp = 1;\n\nport_found:\n\tret = rdma_query_gid(cma_dev->device, p, 0, &gid);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ib_get_cached_pkey(cma_dev->device, p, 0, &pkey);\n\tif (ret)\n\t\tgoto out;\n\n\tid_priv->id.route.addr.dev_addr.dev_type =\n\t\t(rdma_protocol_ib(cma_dev->device, p)) ?\n\t\tARPHRD_INFINIBAND : ARPHRD_ETHER;\n\n\trdma_addr_set_sgid(&id_priv->id.route.addr.dev_addr, &gid);\n\tib_addr_set_pkey(&id_priv->id.route.addr.dev_addr, pkey);\n\tid_priv->id.port_num = p;\n\tcma_attach_to_dev(id_priv, cma_dev);\n\trdma_restrack_add(&id_priv->res);\n\tcma_set_loopback(cma_src_addr(id_priv));\nout:\n\tmutex_unlock(&lock);\n\treturn ret;\n}\n\nstatic void addr_handler(int status, struct sockaddr *src_addr,\n\t\t\t struct rdma_dev_addr *dev_addr, void *context)\n{\n\tstruct rdma_id_private *id_priv = context;\n\tstruct rdma_cm_event event = {};\n\tstruct sockaddr *addr;\n\tstruct sockaddr_storage old_addr;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_QUERY,\n\t\t\t   RDMA_CM_ADDR_RESOLVED))\n\t\tgoto out;\n\n\t \n\taddr = cma_src_addr(id_priv);\n\tmemcpy(&old_addr, addr, rdma_addr_size(addr));\n\tmemcpy(addr, src_addr, rdma_addr_size(src_addr));\n\tif (!status && !id_priv->cma_dev) {\n\t\tstatus = cma_acquire_dev_by_src_ip(id_priv);\n\t\tif (status)\n\t\t\tpr_debug_ratelimited(\"RDMA CM: ADDR_ERROR: failed to acquire device. status %d\\n\",\n\t\t\t\t\t     status);\n\t\trdma_restrack_add(&id_priv->res);\n\t} else if (status) {\n\t\tpr_debug_ratelimited(\"RDMA CM: ADDR_ERROR: failed to resolve IP. status %d\\n\", status);\n\t}\n\n\tif (status) {\n\t\tmemcpy(addr, &old_addr,\n\t\t       rdma_addr_size((struct sockaddr *)&old_addr));\n\t\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_RESOLVED,\n\t\t\t\t   RDMA_CM_ADDR_BOUND))\n\t\t\tgoto out;\n\t\tevent.event = RDMA_CM_EVENT_ADDR_ERROR;\n\t\tevent.status = status;\n\t} else\n\t\tevent.event = RDMA_CM_EVENT_ADDR_RESOLVED;\n\n\tif (cma_cm_event_handler(id_priv, &event)) {\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\treturn;\n\t}\nout:\n\tmutex_unlock(&id_priv->handler_mutex);\n}\n\nstatic int cma_resolve_loopback(struct rdma_id_private *id_priv)\n{\n\tstruct cma_work *work;\n\tunion ib_gid gid;\n\tint ret;\n\n\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\tif (!id_priv->cma_dev) {\n\t\tret = cma_bind_loopback(id_priv);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\trdma_addr_get_sgid(&id_priv->id.route.addr.dev_addr, &gid);\n\trdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, &gid);\n\n\tenqueue_resolve_addr_work(work, id_priv);\n\treturn 0;\nerr:\n\tkfree(work);\n\treturn ret;\n}\n\nstatic int cma_resolve_ib_addr(struct rdma_id_private *id_priv)\n{\n\tstruct cma_work *work;\n\tint ret;\n\n\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\tif (!id_priv->cma_dev) {\n\t\tret = cma_resolve_ib_dev(id_priv);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\trdma_addr_set_dgid(&id_priv->id.route.addr.dev_addr, (union ib_gid *)\n\t\t&(((struct sockaddr_ib *) &id_priv->id.route.addr.dst_addr)->sib_addr));\n\n\tenqueue_resolve_addr_work(work, id_priv);\n\treturn 0;\nerr:\n\tkfree(work);\n\treturn ret;\n}\n\nint rdma_set_reuseaddr(struct rdma_cm_id *id, int reuse)\n{\n\tstruct rdma_id_private *id_priv;\n\tunsigned long flags;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tspin_lock_irqsave(&id_priv->lock, flags);\n\tif ((reuse && id_priv->state != RDMA_CM_LISTEN) ||\n\t    id_priv->state == RDMA_CM_IDLE) {\n\t\tid_priv->reuseaddr = reuse;\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_set_reuseaddr);\n\nint rdma_set_afonly(struct rdma_cm_id *id, int afonly)\n{\n\tstruct rdma_id_private *id_priv;\n\tunsigned long flags;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tspin_lock_irqsave(&id_priv->lock, flags);\n\tif (id_priv->state == RDMA_CM_IDLE || id_priv->state == RDMA_CM_ADDR_BOUND) {\n\t\tid_priv->options |= (1 << CMA_OPTION_AFONLY);\n\t\tid_priv->afonly = afonly;\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_set_afonly);\n\nstatic void cma_bind_port(struct rdma_bind_list *bind_list,\n\t\t\t  struct rdma_id_private *id_priv)\n{\n\tstruct sockaddr *addr;\n\tstruct sockaddr_ib *sib;\n\tu64 sid, mask;\n\t__be16 port;\n\n\tlockdep_assert_held(&lock);\n\n\taddr = cma_src_addr(id_priv);\n\tport = htons(bind_list->port);\n\n\tswitch (addr->sa_family) {\n\tcase AF_INET:\n\t\t((struct sockaddr_in *) addr)->sin_port = port;\n\t\tbreak;\n\tcase AF_INET6:\n\t\t((struct sockaddr_in6 *) addr)->sin6_port = port;\n\t\tbreak;\n\tcase AF_IB:\n\t\tsib = (struct sockaddr_ib *) addr;\n\t\tsid = be64_to_cpu(sib->sib_sid);\n\t\tmask = be64_to_cpu(sib->sib_sid_mask);\n\t\tsib->sib_sid = cpu_to_be64((sid & mask) | (u64) ntohs(port));\n\t\tsib->sib_sid_mask = cpu_to_be64(~0ULL);\n\t\tbreak;\n\t}\n\tid_priv->bind_list = bind_list;\n\thlist_add_head(&id_priv->node, &bind_list->owners);\n}\n\nstatic int cma_alloc_port(enum rdma_ucm_port_space ps,\n\t\t\t  struct rdma_id_private *id_priv, unsigned short snum)\n{\n\tstruct rdma_bind_list *bind_list;\n\tint ret;\n\n\tlockdep_assert_held(&lock);\n\n\tbind_list = kzalloc(sizeof *bind_list, GFP_KERNEL);\n\tif (!bind_list)\n\t\treturn -ENOMEM;\n\n\tret = cma_ps_alloc(id_priv->id.route.addr.dev_addr.net, ps, bind_list,\n\t\t\t   snum);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tbind_list->ps = ps;\n\tbind_list->port = snum;\n\tcma_bind_port(bind_list, id_priv);\n\treturn 0;\nerr:\n\tkfree(bind_list);\n\treturn ret == -ENOSPC ? -EADDRNOTAVAIL : ret;\n}\n\nstatic int cma_port_is_unique(struct rdma_bind_list *bind_list,\n\t\t\t      struct rdma_id_private *id_priv)\n{\n\tstruct rdma_id_private *cur_id;\n\tstruct sockaddr  *daddr = cma_dst_addr(id_priv);\n\tstruct sockaddr  *saddr = cma_src_addr(id_priv);\n\t__be16 dport = cma_port(daddr);\n\n\tlockdep_assert_held(&lock);\n\n\thlist_for_each_entry(cur_id, &bind_list->owners, node) {\n\t\tstruct sockaddr  *cur_daddr = cma_dst_addr(cur_id);\n\t\tstruct sockaddr  *cur_saddr = cma_src_addr(cur_id);\n\t\t__be16 cur_dport = cma_port(cur_daddr);\n\n\t\tif (id_priv == cur_id)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!cma_any_port(daddr) &&\n\t\t    !cma_any_port(cur_daddr) &&\n\t\t    (dport != cur_dport))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!cma_any_addr(saddr) &&\n\t\t    !cma_any_addr(cur_saddr) &&\n\t\t    cma_addr_cmp(saddr, cur_saddr))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!cma_any_addr(daddr) &&\n\t\t    !cma_any_addr(cur_daddr) &&\n\t\t    cma_addr_cmp(daddr, cur_daddr))\n\t\t\tcontinue;\n\n\t\treturn -EADDRNOTAVAIL;\n\t}\n\treturn 0;\n}\n\nstatic int cma_alloc_any_port(enum rdma_ucm_port_space ps,\n\t\t\t      struct rdma_id_private *id_priv)\n{\n\tstatic unsigned int last_used_port;\n\tint low, high, remaining;\n\tunsigned int rover;\n\tstruct net *net = id_priv->id.route.addr.dev_addr.net;\n\n\tlockdep_assert_held(&lock);\n\n\tinet_get_local_port_range(net, &low, &high);\n\tremaining = (high - low) + 1;\n\trover = get_random_u32_inclusive(low, remaining + low - 1);\nretry:\n\tif (last_used_port != rover) {\n\t\tstruct rdma_bind_list *bind_list;\n\t\tint ret;\n\n\t\tbind_list = cma_ps_find(net, ps, (unsigned short)rover);\n\n\t\tif (!bind_list) {\n\t\t\tret = cma_alloc_port(ps, id_priv, rover);\n\t\t} else {\n\t\t\tret = cma_port_is_unique(bind_list, id_priv);\n\t\t\tif (!ret)\n\t\t\t\tcma_bind_port(bind_list, id_priv);\n\t\t}\n\t\t \n\t\tif (!ret)\n\t\t\tlast_used_port = rover;\n\t\tif (ret != -EADDRNOTAVAIL)\n\t\t\treturn ret;\n\t}\n\tif (--remaining) {\n\t\trover++;\n\t\tif ((rover < low) || (rover > high))\n\t\t\trover = low;\n\t\tgoto retry;\n\t}\n\treturn -EADDRNOTAVAIL;\n}\n\n \nstatic int cma_check_port(struct rdma_bind_list *bind_list,\n\t\t\t  struct rdma_id_private *id_priv, uint8_t reuseaddr)\n{\n\tstruct rdma_id_private *cur_id;\n\tstruct sockaddr *addr, *cur_addr;\n\n\tlockdep_assert_held(&lock);\n\n\taddr = cma_src_addr(id_priv);\n\thlist_for_each_entry(cur_id, &bind_list->owners, node) {\n\t\tif (id_priv == cur_id)\n\t\t\tcontinue;\n\n\t\tif (reuseaddr && cur_id->reuseaddr)\n\t\t\tcontinue;\n\n\t\tcur_addr = cma_src_addr(cur_id);\n\t\tif (id_priv->afonly && cur_id->afonly &&\n\t\t    (addr->sa_family != cur_addr->sa_family))\n\t\t\tcontinue;\n\n\t\tif (cma_any_addr(addr) || cma_any_addr(cur_addr))\n\t\t\treturn -EADDRNOTAVAIL;\n\n\t\tif (!cma_addr_cmp(addr, cur_addr))\n\t\t\treturn -EADDRINUSE;\n\t}\n\treturn 0;\n}\n\nstatic int cma_use_port(enum rdma_ucm_port_space ps,\n\t\t\tstruct rdma_id_private *id_priv)\n{\n\tstruct rdma_bind_list *bind_list;\n\tunsigned short snum;\n\tint ret;\n\n\tlockdep_assert_held(&lock);\n\n\tsnum = ntohs(cma_port(cma_src_addr(id_priv)));\n\tif (snum < PROT_SOCK && !capable(CAP_NET_BIND_SERVICE))\n\t\treturn -EACCES;\n\n\tbind_list = cma_ps_find(id_priv->id.route.addr.dev_addr.net, ps, snum);\n\tif (!bind_list) {\n\t\tret = cma_alloc_port(ps, id_priv, snum);\n\t} else {\n\t\tret = cma_check_port(bind_list, id_priv, id_priv->reuseaddr);\n\t\tif (!ret)\n\t\t\tcma_bind_port(bind_list, id_priv);\n\t}\n\treturn ret;\n}\n\nstatic enum rdma_ucm_port_space\ncma_select_inet_ps(struct rdma_id_private *id_priv)\n{\n\tswitch (id_priv->id.ps) {\n\tcase RDMA_PS_TCP:\n\tcase RDMA_PS_UDP:\n\tcase RDMA_PS_IPOIB:\n\tcase RDMA_PS_IB:\n\t\treturn id_priv->id.ps;\n\tdefault:\n\n\t\treturn 0;\n\t}\n}\n\nstatic enum rdma_ucm_port_space\ncma_select_ib_ps(struct rdma_id_private *id_priv)\n{\n\tenum rdma_ucm_port_space ps = 0;\n\tstruct sockaddr_ib *sib;\n\tu64 sid_ps, mask, sid;\n\n\tsib = (struct sockaddr_ib *) cma_src_addr(id_priv);\n\tmask = be64_to_cpu(sib->sib_sid_mask) & RDMA_IB_IP_PS_MASK;\n\tsid = be64_to_cpu(sib->sib_sid) & mask;\n\n\tif ((id_priv->id.ps == RDMA_PS_IB) && (sid == (RDMA_IB_IP_PS_IB & mask))) {\n\t\tsid_ps = RDMA_IB_IP_PS_IB;\n\t\tps = RDMA_PS_IB;\n\t} else if (((id_priv->id.ps == RDMA_PS_IB) || (id_priv->id.ps == RDMA_PS_TCP)) &&\n\t\t   (sid == (RDMA_IB_IP_PS_TCP & mask))) {\n\t\tsid_ps = RDMA_IB_IP_PS_TCP;\n\t\tps = RDMA_PS_TCP;\n\t} else if (((id_priv->id.ps == RDMA_PS_IB) || (id_priv->id.ps == RDMA_PS_UDP)) &&\n\t\t   (sid == (RDMA_IB_IP_PS_UDP & mask))) {\n\t\tsid_ps = RDMA_IB_IP_PS_UDP;\n\t\tps = RDMA_PS_UDP;\n\t}\n\n\tif (ps) {\n\t\tsib->sib_sid = cpu_to_be64(sid_ps | ntohs(cma_port((struct sockaddr *) sib)));\n\t\tsib->sib_sid_mask = cpu_to_be64(RDMA_IB_IP_PS_MASK |\n\t\t\t\t\t\tbe64_to_cpu(sib->sib_sid_mask));\n\t}\n\treturn ps;\n}\n\nstatic int cma_get_port(struct rdma_id_private *id_priv)\n{\n\tenum rdma_ucm_port_space ps;\n\tint ret;\n\n\tif (cma_family(id_priv) != AF_IB)\n\t\tps = cma_select_inet_ps(id_priv);\n\telse\n\t\tps = cma_select_ib_ps(id_priv);\n\tif (!ps)\n\t\treturn -EPROTONOSUPPORT;\n\n\tmutex_lock(&lock);\n\tif (cma_any_port(cma_src_addr(id_priv)))\n\t\tret = cma_alloc_any_port(ps, id_priv);\n\telse\n\t\tret = cma_use_port(ps, id_priv);\n\tmutex_unlock(&lock);\n\n\treturn ret;\n}\n\nstatic int cma_check_linklocal(struct rdma_dev_addr *dev_addr,\n\t\t\t       struct sockaddr *addr)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct sockaddr_in6 *sin6;\n\n\tif (addr->sa_family != AF_INET6)\n\t\treturn 0;\n\n\tsin6 = (struct sockaddr_in6 *) addr;\n\n\tif (!(ipv6_addr_type(&sin6->sin6_addr) & IPV6_ADDR_LINKLOCAL))\n\t\treturn 0;\n\n\tif (!sin6->sin6_scope_id)\n\t\t\treturn -EINVAL;\n\n\tdev_addr->bound_dev_if = sin6->sin6_scope_id;\n#endif\n\treturn 0;\n}\n\nint rdma_listen(struct rdma_cm_id *id, int backlog)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN)) {\n\t\tstruct sockaddr_in any_in = {\n\t\t\t.sin_family = AF_INET,\n\t\t\t.sin_addr.s_addr = htonl(INADDR_ANY),\n\t\t};\n\n\t\t \n\t\tret = rdma_bind_addr(id, (struct sockaddr *)&any_in);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,\n\t\t\t\t\t   RDMA_CM_LISTEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (id_priv->reuseaddr) {\n\t\tmutex_lock(&lock);\n\t\tret = cma_check_port(id_priv->bind_list, id_priv, 0);\n\t\tif (!ret)\n\t\t\tid_priv->reuseaddr = 0;\n\t\tmutex_unlock(&lock);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tid_priv->backlog = backlog;\n\tif (id_priv->cma_dev) {\n\t\tif (rdma_cap_ib_cm(id->device, 1)) {\n\t\t\tret = cma_ib_listen(id_priv);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else if (rdma_cap_iw_cm(id->device, 1)) {\n\t\t\tret = cma_iw_listen(id_priv, backlog);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else {\n\t\t\tret = -ENOSYS;\n\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = cma_listen_on_all(id_priv);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tid_priv->backlog = 0;\n\t \n\tcma_comp_exch(id_priv, RDMA_CM_LISTEN, RDMA_CM_ADDR_BOUND);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_listen);\n\nstatic int rdma_bind_addr_dst(struct rdma_id_private *id_priv,\n\t\t\t      struct sockaddr *addr, const struct sockaddr *daddr)\n{\n\tstruct sockaddr *id_daddr;\n\tint ret;\n\n\tif (addr->sa_family != AF_INET && addr->sa_family != AF_INET6 &&\n\t    addr->sa_family != AF_IB)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_IDLE, RDMA_CM_ADDR_BOUND))\n\t\treturn -EINVAL;\n\n\tret = cma_check_linklocal(&id_priv->id.route.addr.dev_addr, addr);\n\tif (ret)\n\t\tgoto err1;\n\n\tmemcpy(cma_src_addr(id_priv), addr, rdma_addr_size(addr));\n\tif (!cma_any_addr(addr)) {\n\t\tret = cma_translate_addr(addr, &id_priv->id.route.addr.dev_addr);\n\t\tif (ret)\n\t\t\tgoto err1;\n\n\t\tret = cma_acquire_dev_by_src_ip(id_priv);\n\t\tif (ret)\n\t\t\tgoto err1;\n\t}\n\n\tif (!(id_priv->options & (1 << CMA_OPTION_AFONLY))) {\n\t\tif (addr->sa_family == AF_INET)\n\t\t\tid_priv->afonly = 1;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\telse if (addr->sa_family == AF_INET6) {\n\t\t\tstruct net *net = id_priv->id.route.addr.dev_addr.net;\n\n\t\t\tid_priv->afonly = net->ipv6.sysctl.bindv6only;\n\t\t}\n#endif\n\t}\n\tid_daddr = cma_dst_addr(id_priv);\n\tif (daddr != id_daddr)\n\t\tmemcpy(id_daddr, daddr, rdma_addr_size(addr));\n\tid_daddr->sa_family = addr->sa_family;\n\n\tret = cma_get_port(id_priv);\n\tif (ret)\n\t\tgoto err2;\n\n\tif (!cma_any_addr(addr))\n\t\trdma_restrack_add(&id_priv->res);\n\treturn 0;\nerr2:\n\tif (id_priv->cma_dev)\n\t\tcma_release_dev(id_priv);\nerr1:\n\tcma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_IDLE);\n\treturn ret;\n}\n\nstatic int cma_bind_addr(struct rdma_cm_id *id, struct sockaddr *src_addr,\n\t\t\t const struct sockaddr *dst_addr)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tstruct sockaddr_storage zero_sock = {};\n\n\tif (src_addr && src_addr->sa_family)\n\t\treturn rdma_bind_addr_dst(id_priv, src_addr, dst_addr);\n\n\t \n\tzero_sock.ss_family = dst_addr->sa_family;\n\tif (IS_ENABLED(CONFIG_IPV6) && dst_addr->sa_family == AF_INET6) {\n\t\tstruct sockaddr_in6 *src_addr6 =\n\t\t\t(struct sockaddr_in6 *)&zero_sock;\n\t\tstruct sockaddr_in6 *dst_addr6 =\n\t\t\t(struct sockaddr_in6 *)dst_addr;\n\n\t\tsrc_addr6->sin6_scope_id = dst_addr6->sin6_scope_id;\n\t\tif (ipv6_addr_type(&dst_addr6->sin6_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tid->route.addr.dev_addr.bound_dev_if =\n\t\t\t\tdst_addr6->sin6_scope_id;\n\t} else if (dst_addr->sa_family == AF_IB) {\n\t\t((struct sockaddr_ib *)&zero_sock)->sib_pkey =\n\t\t\t((struct sockaddr_ib *)dst_addr)->sib_pkey;\n\t}\n\treturn rdma_bind_addr_dst(id_priv, (struct sockaddr *)&zero_sock, dst_addr);\n}\n\n \nstatic int resolve_prepare_src(struct rdma_id_private *id_priv,\n\t\t\t       struct sockaddr *src_addr,\n\t\t\t       const struct sockaddr *dst_addr)\n{\n\tint ret;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_ADDR_QUERY)) {\n\t\t \n\t\tret = cma_bind_addr(&id_priv->id, src_addr, dst_addr);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,\n\t\t\t\t\t   RDMA_CM_ADDR_QUERY)))\n\t\t\treturn -EINVAL;\n\n\t} else {\n\t\tmemcpy(cma_dst_addr(id_priv), dst_addr, rdma_addr_size(dst_addr));\n\t}\n\n\tif (cma_family(id_priv) != dst_addr->sa_family) {\n\t\tret = -EINVAL;\n\t\tgoto err_state;\n\t}\n\treturn 0;\n\nerr_state:\n\tcma_comp_exch(id_priv, RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_BOUND);\n\treturn ret;\n}\n\nint rdma_resolve_addr(struct rdma_cm_id *id, struct sockaddr *src_addr,\n\t\t      const struct sockaddr *dst_addr, unsigned long timeout_ms)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tret = resolve_prepare_src(id_priv, src_addr, dst_addr);\n\tif (ret)\n\t\treturn ret;\n\n\tif (cma_any_addr(dst_addr)) {\n\t\tret = cma_resolve_loopback(id_priv);\n\t} else {\n\t\tif (dst_addr->sa_family == AF_IB) {\n\t\t\tret = cma_resolve_ib_addr(id_priv);\n\t\t} else {\n\t\t\t \n\t\t\tif (id_priv->used_resolve_ip)\n\t\t\t\trdma_addr_cancel(&id->route.addr.dev_addr);\n\t\t\telse\n\t\t\t\tid_priv->used_resolve_ip = 1;\n\t\t\tret = rdma_resolve_ip(cma_src_addr(id_priv), dst_addr,\n\t\t\t\t\t      &id->route.addr.dev_addr,\n\t\t\t\t\t      timeout_ms, addr_handler,\n\t\t\t\t\t      false, id_priv);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tcma_comp_exch(id_priv, RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_BOUND);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_resolve_addr);\n\nint rdma_bind_addr(struct rdma_cm_id *id, struct sockaddr *addr)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\treturn rdma_bind_addr_dst(id_priv, addr, cma_dst_addr(id_priv));\n}\nEXPORT_SYMBOL(rdma_bind_addr);\n\nstatic int cma_format_hdr(void *hdr, struct rdma_id_private *id_priv)\n{\n\tstruct cma_hdr *cma_hdr;\n\n\tcma_hdr = hdr;\n\tcma_hdr->cma_version = CMA_VERSION;\n\tif (cma_family(id_priv) == AF_INET) {\n\t\tstruct sockaddr_in *src4, *dst4;\n\n\t\tsrc4 = (struct sockaddr_in *) cma_src_addr(id_priv);\n\t\tdst4 = (struct sockaddr_in *) cma_dst_addr(id_priv);\n\n\t\tcma_set_ip_ver(cma_hdr, 4);\n\t\tcma_hdr->src_addr.ip4.addr = src4->sin_addr.s_addr;\n\t\tcma_hdr->dst_addr.ip4.addr = dst4->sin_addr.s_addr;\n\t\tcma_hdr->port = src4->sin_port;\n\t} else if (cma_family(id_priv) == AF_INET6) {\n\t\tstruct sockaddr_in6 *src6, *dst6;\n\n\t\tsrc6 = (struct sockaddr_in6 *) cma_src_addr(id_priv);\n\t\tdst6 = (struct sockaddr_in6 *) cma_dst_addr(id_priv);\n\n\t\tcma_set_ip_ver(cma_hdr, 6);\n\t\tcma_hdr->src_addr.ip6 = src6->sin6_addr;\n\t\tcma_hdr->dst_addr.ip6 = dst6->sin6_addr;\n\t\tcma_hdr->port = src6->sin6_port;\n\t}\n\treturn 0;\n}\n\nstatic int cma_sidr_rep_handler(struct ib_cm_id *cm_id,\n\t\t\t\tconst struct ib_cm_event *ib_event)\n{\n\tstruct rdma_id_private *id_priv = cm_id->context;\n\tstruct rdma_cm_event event = {};\n\tconst struct ib_cm_sidr_rep_event_param *rep =\n\t\t\t\t&ib_event->param.sidr_rep_rcvd;\n\tint ret;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (READ_ONCE(id_priv->state) != RDMA_CM_CONNECT)\n\t\tgoto out;\n\n\tswitch (ib_event->event) {\n\tcase IB_CM_SIDR_REQ_ERROR:\n\t\tevent.event = RDMA_CM_EVENT_UNREACHABLE;\n\t\tevent.status = -ETIMEDOUT;\n\t\tbreak;\n\tcase IB_CM_SIDR_REP_RECEIVED:\n\t\tevent.param.ud.private_data = ib_event->private_data;\n\t\tevent.param.ud.private_data_len = IB_CM_SIDR_REP_PRIVATE_DATA_SIZE;\n\t\tif (rep->status != IB_SIDR_SUCCESS) {\n\t\t\tevent.event = RDMA_CM_EVENT_UNREACHABLE;\n\t\t\tevent.status = ib_event->param.sidr_rep_rcvd.status;\n\t\t\tpr_debug_ratelimited(\"RDMA CM: UNREACHABLE: bad SIDR reply. status %d\\n\",\n\t\t\t\t\t     event.status);\n\t\t\tbreak;\n\t\t}\n\t\tret = cma_set_qkey(id_priv, rep->qkey);\n\t\tif (ret) {\n\t\t\tpr_debug_ratelimited(\"RDMA CM: ADDR_ERROR: failed to set qkey. status %d\\n\", ret);\n\t\t\tevent.event = RDMA_CM_EVENT_ADDR_ERROR;\n\t\t\tevent.status = ret;\n\t\t\tbreak;\n\t\t}\n\t\tib_init_ah_attr_from_path(id_priv->id.device,\n\t\t\t\t\t  id_priv->id.port_num,\n\t\t\t\t\t  id_priv->id.route.path_rec,\n\t\t\t\t\t  &event.param.ud.ah_attr,\n\t\t\t\t\t  rep->sgid_attr);\n\t\tevent.param.ud.qp_num = rep->qpn;\n\t\tevent.param.ud.qkey = rep->qkey;\n\t\tevent.event = RDMA_CM_EVENT_ESTABLISHED;\n\t\tevent.status = 0;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"RDMA CMA: unexpected IB CM event: %d\\n\",\n\t\t       ib_event->event);\n\t\tgoto out;\n\t}\n\n\tret = cma_cm_event_handler(id_priv, &event);\n\n\trdma_destroy_ah_attr(&event.param.ud.ah_attr);\n\tif (ret) {\n\t\t \n\t\tid_priv->cm_id.ib = NULL;\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\treturn ret;\n\t}\nout:\n\tmutex_unlock(&id_priv->handler_mutex);\n\treturn 0;\n}\n\nstatic int cma_resolve_ib_udp(struct rdma_id_private *id_priv,\n\t\t\t      struct rdma_conn_param *conn_param)\n{\n\tstruct ib_cm_sidr_req_param req;\n\tstruct ib_cm_id\t*id;\n\tvoid *private_data;\n\tu8 offset;\n\tint ret;\n\n\tmemset(&req, 0, sizeof req);\n\toffset = cma_user_data_offset(id_priv);\n\tif (check_add_overflow(offset, conn_param->private_data_len, &req.private_data_len))\n\t\treturn -EINVAL;\n\n\tif (req.private_data_len) {\n\t\tprivate_data = kzalloc(req.private_data_len, GFP_ATOMIC);\n\t\tif (!private_data)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tprivate_data = NULL;\n\t}\n\n\tif (conn_param->private_data && conn_param->private_data_len)\n\t\tmemcpy(private_data + offset, conn_param->private_data,\n\t\t       conn_param->private_data_len);\n\n\tif (private_data) {\n\t\tret = cma_format_hdr(private_data, id_priv);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\treq.private_data = private_data;\n\t}\n\n\tid = ib_create_cm_id(id_priv->id.device, cma_sidr_rep_handler,\n\t\t\t     id_priv);\n\tif (IS_ERR(id)) {\n\t\tret = PTR_ERR(id);\n\t\tgoto out;\n\t}\n\tid_priv->cm_id.ib = id;\n\n\treq.path = id_priv->id.route.path_rec;\n\treq.sgid_attr = id_priv->id.route.addr.dev_addr.sgid_attr;\n\treq.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));\n\treq.timeout_ms = 1 << (CMA_CM_RESPONSE_TIMEOUT - 8);\n\treq.max_cm_retries = CMA_MAX_CM_RETRIES;\n\n\ttrace_cm_send_sidr_req(id_priv);\n\tret = ib_send_cm_sidr_req(id_priv->cm_id.ib, &req);\n\tif (ret) {\n\t\tib_destroy_cm_id(id_priv->cm_id.ib);\n\t\tid_priv->cm_id.ib = NULL;\n\t}\nout:\n\tkfree(private_data);\n\treturn ret;\n}\n\nstatic int cma_connect_ib(struct rdma_id_private *id_priv,\n\t\t\t  struct rdma_conn_param *conn_param)\n{\n\tstruct ib_cm_req_param req;\n\tstruct rdma_route *route;\n\tvoid *private_data;\n\tstruct ib_cm_id\t*id;\n\tu8 offset;\n\tint ret;\n\n\tmemset(&req, 0, sizeof req);\n\toffset = cma_user_data_offset(id_priv);\n\tif (check_add_overflow(offset, conn_param->private_data_len, &req.private_data_len))\n\t\treturn -EINVAL;\n\n\tif (req.private_data_len) {\n\t\tprivate_data = kzalloc(req.private_data_len, GFP_ATOMIC);\n\t\tif (!private_data)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tprivate_data = NULL;\n\t}\n\n\tif (conn_param->private_data && conn_param->private_data_len)\n\t\tmemcpy(private_data + offset, conn_param->private_data,\n\t\t       conn_param->private_data_len);\n\n\tid = ib_create_cm_id(id_priv->id.device, cma_ib_handler, id_priv);\n\tif (IS_ERR(id)) {\n\t\tret = PTR_ERR(id);\n\t\tgoto out;\n\t}\n\tid_priv->cm_id.ib = id;\n\n\troute = &id_priv->id.route;\n\tif (private_data) {\n\t\tret = cma_format_hdr(private_data, id_priv);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\treq.private_data = private_data;\n\t}\n\n\treq.primary_path = &route->path_rec[0];\n\treq.primary_path_inbound = route->path_rec_inbound;\n\treq.primary_path_outbound = route->path_rec_outbound;\n\tif (route->num_pri_alt_paths == 2)\n\t\treq.alternate_path = &route->path_rec[1];\n\n\treq.ppath_sgid_attr = id_priv->id.route.addr.dev_addr.sgid_attr;\n\t \n\treq.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));\n\treq.qp_num = id_priv->qp_num;\n\treq.qp_type = id_priv->id.qp_type;\n\treq.starting_psn = id_priv->seq_num;\n\treq.responder_resources = conn_param->responder_resources;\n\treq.initiator_depth = conn_param->initiator_depth;\n\treq.flow_control = conn_param->flow_control;\n\treq.retry_count = min_t(u8, 7, conn_param->retry_count);\n\treq.rnr_retry_count = min_t(u8, 7, conn_param->rnr_retry_count);\n\treq.remote_cm_response_timeout = CMA_CM_RESPONSE_TIMEOUT;\n\treq.local_cm_response_timeout = CMA_CM_RESPONSE_TIMEOUT;\n\treq.max_cm_retries = CMA_MAX_CM_RETRIES;\n\treq.srq = id_priv->srq ? 1 : 0;\n\treq.ece.vendor_id = id_priv->ece.vendor_id;\n\treq.ece.attr_mod = id_priv->ece.attr_mod;\n\n\ttrace_cm_send_req(id_priv);\n\tret = ib_send_cm_req(id_priv->cm_id.ib, &req);\nout:\n\tif (ret && !IS_ERR(id)) {\n\t\tib_destroy_cm_id(id);\n\t\tid_priv->cm_id.ib = NULL;\n\t}\n\n\tkfree(private_data);\n\treturn ret;\n}\n\nstatic int cma_connect_iw(struct rdma_id_private *id_priv,\n\t\t\t  struct rdma_conn_param *conn_param)\n{\n\tstruct iw_cm_id *cm_id;\n\tint ret;\n\tstruct iw_cm_conn_param iw_param;\n\n\tcm_id = iw_create_cm_id(id_priv->id.device, cma_iw_handler, id_priv);\n\tif (IS_ERR(cm_id))\n\t\treturn PTR_ERR(cm_id);\n\n\tmutex_lock(&id_priv->qp_mutex);\n\tcm_id->tos = id_priv->tos;\n\tcm_id->tos_set = id_priv->tos_set;\n\tmutex_unlock(&id_priv->qp_mutex);\n\n\tid_priv->cm_id.iw = cm_id;\n\n\tmemcpy(&cm_id->local_addr, cma_src_addr(id_priv),\n\t       rdma_addr_size(cma_src_addr(id_priv)));\n\tmemcpy(&cm_id->remote_addr, cma_dst_addr(id_priv),\n\t       rdma_addr_size(cma_dst_addr(id_priv)));\n\n\tret = cma_modify_qp_rtr(id_priv, conn_param);\n\tif (ret)\n\t\tgoto out;\n\n\tif (conn_param) {\n\t\tiw_param.ord = conn_param->initiator_depth;\n\t\tiw_param.ird = conn_param->responder_resources;\n\t\tiw_param.private_data = conn_param->private_data;\n\t\tiw_param.private_data_len = conn_param->private_data_len;\n\t\tiw_param.qpn = id_priv->id.qp ? id_priv->qp_num : conn_param->qp_num;\n\t} else {\n\t\tmemset(&iw_param, 0, sizeof iw_param);\n\t\tiw_param.qpn = id_priv->qp_num;\n\t}\n\tret = iw_cm_connect(cm_id, &iw_param);\nout:\n\tif (ret) {\n\t\tiw_destroy_cm_id(cm_id);\n\t\tid_priv->cm_id.iw = NULL;\n\t}\n\treturn ret;\n}\n\n \nint rdma_connect_locked(struct rdma_cm_id *id,\n\t\t\tstruct rdma_conn_param *conn_param)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ROUTE_RESOLVED, RDMA_CM_CONNECT))\n\t\treturn -EINVAL;\n\n\tif (!id->qp) {\n\t\tid_priv->qp_num = conn_param->qp_num;\n\t\tid_priv->srq = conn_param->srq;\n\t}\n\n\tif (rdma_cap_ib_cm(id->device, id->port_num)) {\n\t\tif (id->qp_type == IB_QPT_UD)\n\t\t\tret = cma_resolve_ib_udp(id_priv, conn_param);\n\t\telse\n\t\t\tret = cma_connect_ib(id_priv, conn_param);\n\t} else if (rdma_cap_iw_cm(id->device, id->port_num)) {\n\t\tret = cma_connect_iw(id_priv, conn_param);\n\t} else {\n\t\tret = -ENOSYS;\n\t}\n\tif (ret)\n\t\tgoto err_state;\n\treturn 0;\nerr_state:\n\tcma_comp_exch(id_priv, RDMA_CM_CONNECT, RDMA_CM_ROUTE_RESOLVED);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_connect_locked);\n\n \nint rdma_connect(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tret = rdma_connect_locked(id, conn_param);\n\tmutex_unlock(&id_priv->handler_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_connect);\n\n \nint rdma_connect_ece(struct rdma_cm_id *id, struct rdma_conn_param *conn_param,\n\t\t     struct rdma_ucm_ece *ece)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\tid_priv->ece.vendor_id = ece->vendor_id;\n\tid_priv->ece.attr_mod = ece->attr_mod;\n\n\treturn rdma_connect(id, conn_param);\n}\nEXPORT_SYMBOL(rdma_connect_ece);\n\nstatic int cma_accept_ib(struct rdma_id_private *id_priv,\n\t\t\t struct rdma_conn_param *conn_param)\n{\n\tstruct ib_cm_rep_param rep;\n\tint ret;\n\n\tret = cma_modify_qp_rtr(id_priv, conn_param);\n\tif (ret)\n\t\tgoto out;\n\n\tret = cma_modify_qp_rts(id_priv, conn_param);\n\tif (ret)\n\t\tgoto out;\n\n\tmemset(&rep, 0, sizeof rep);\n\trep.qp_num = id_priv->qp_num;\n\trep.starting_psn = id_priv->seq_num;\n\trep.private_data = conn_param->private_data;\n\trep.private_data_len = conn_param->private_data_len;\n\trep.responder_resources = conn_param->responder_resources;\n\trep.initiator_depth = conn_param->initiator_depth;\n\trep.failover_accepted = 0;\n\trep.flow_control = conn_param->flow_control;\n\trep.rnr_retry_count = min_t(u8, 7, conn_param->rnr_retry_count);\n\trep.srq = id_priv->srq ? 1 : 0;\n\trep.ece.vendor_id = id_priv->ece.vendor_id;\n\trep.ece.attr_mod = id_priv->ece.attr_mod;\n\n\ttrace_cm_send_rep(id_priv);\n\tret = ib_send_cm_rep(id_priv->cm_id.ib, &rep);\nout:\n\treturn ret;\n}\n\nstatic int cma_accept_iw(struct rdma_id_private *id_priv,\n\t\t  struct rdma_conn_param *conn_param)\n{\n\tstruct iw_cm_conn_param iw_param;\n\tint ret;\n\n\tif (!conn_param)\n\t\treturn -EINVAL;\n\n\tret = cma_modify_qp_rtr(id_priv, conn_param);\n\tif (ret)\n\t\treturn ret;\n\n\tiw_param.ord = conn_param->initiator_depth;\n\tiw_param.ird = conn_param->responder_resources;\n\tiw_param.private_data = conn_param->private_data;\n\tiw_param.private_data_len = conn_param->private_data_len;\n\tif (id_priv->id.qp)\n\t\tiw_param.qpn = id_priv->qp_num;\n\telse\n\t\tiw_param.qpn = conn_param->qp_num;\n\n\treturn iw_cm_accept(id_priv->cm_id.iw, &iw_param);\n}\n\nstatic int cma_send_sidr_rep(struct rdma_id_private *id_priv,\n\t\t\t     enum ib_cm_sidr_status status, u32 qkey,\n\t\t\t     const void *private_data, int private_data_len)\n{\n\tstruct ib_cm_sidr_rep_param rep;\n\tint ret;\n\n\tmemset(&rep, 0, sizeof rep);\n\trep.status = status;\n\tif (status == IB_SIDR_SUCCESS) {\n\t\tif (qkey)\n\t\t\tret = cma_set_qkey(id_priv, qkey);\n\t\telse\n\t\t\tret = cma_set_default_qkey(id_priv);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\trep.qp_num = id_priv->qp_num;\n\t\trep.qkey = id_priv->qkey;\n\n\t\trep.ece.vendor_id = id_priv->ece.vendor_id;\n\t\trep.ece.attr_mod = id_priv->ece.attr_mod;\n\t}\n\n\trep.private_data = private_data;\n\trep.private_data_len = private_data_len;\n\n\ttrace_cm_send_sidr_rep(id_priv);\n\treturn ib_send_cm_sidr_rep(id_priv->cm_id.ib, &rep);\n}\n\n \nint rdma_accept(struct rdma_cm_id *id, struct rdma_conn_param *conn_param)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tlockdep_assert_held(&id_priv->handler_mutex);\n\n\tif (READ_ONCE(id_priv->state) != RDMA_CM_CONNECT)\n\t\treturn -EINVAL;\n\n\tif (!id->qp && conn_param) {\n\t\tid_priv->qp_num = conn_param->qp_num;\n\t\tid_priv->srq = conn_param->srq;\n\t}\n\n\tif (rdma_cap_ib_cm(id->device, id->port_num)) {\n\t\tif (id->qp_type == IB_QPT_UD) {\n\t\t\tif (conn_param)\n\t\t\t\tret = cma_send_sidr_rep(id_priv, IB_SIDR_SUCCESS,\n\t\t\t\t\t\t\tconn_param->qkey,\n\t\t\t\t\t\t\tconn_param->private_data,\n\t\t\t\t\t\t\tconn_param->private_data_len);\n\t\t\telse\n\t\t\t\tret = cma_send_sidr_rep(id_priv, IB_SIDR_SUCCESS,\n\t\t\t\t\t\t\t0, NULL, 0);\n\t\t} else {\n\t\t\tif (conn_param)\n\t\t\t\tret = cma_accept_ib(id_priv, conn_param);\n\t\t\telse\n\t\t\t\tret = cma_rep_recv(id_priv);\n\t\t}\n\t} else if (rdma_cap_iw_cm(id->device, id->port_num)) {\n\t\tret = cma_accept_iw(id_priv, conn_param);\n\t} else {\n\t\tret = -ENOSYS;\n\t}\n\tif (ret)\n\t\tgoto reject;\n\n\treturn 0;\nreject:\n\tcma_modify_qp_err(id_priv);\n\trdma_reject(id, NULL, 0, IB_CM_REJ_CONSUMER_DEFINED);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_accept);\n\nint rdma_accept_ece(struct rdma_cm_id *id, struct rdma_conn_param *conn_param,\n\t\t    struct rdma_ucm_ece *ece)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\tid_priv->ece.vendor_id = ece->vendor_id;\n\tid_priv->ece.attr_mod = ece->attr_mod;\n\n\treturn rdma_accept(id, conn_param);\n}\nEXPORT_SYMBOL(rdma_accept_ece);\n\nvoid rdma_lock_handler(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\tmutex_lock(&id_priv->handler_mutex);\n}\nEXPORT_SYMBOL(rdma_lock_handler);\n\nvoid rdma_unlock_handler(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\n\tmutex_unlock(&id_priv->handler_mutex);\n}\nEXPORT_SYMBOL(rdma_unlock_handler);\n\nint rdma_notify(struct rdma_cm_id *id, enum ib_event_type event)\n{\n\tstruct rdma_id_private *id_priv;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (!id_priv->cm_id.ib)\n\t\treturn -EINVAL;\n\n\tswitch (id->device->node_type) {\n\tcase RDMA_NODE_IB_CA:\n\t\tret = ib_cm_notify(id_priv->cm_id.ib, event);\n\t\tbreak;\n\tdefault:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_notify);\n\nint rdma_reject(struct rdma_cm_id *id, const void *private_data,\n\t\tu8 private_data_len, u8 reason)\n{\n\tstruct rdma_id_private *id_priv;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (!id_priv->cm_id.ib)\n\t\treturn -EINVAL;\n\n\tif (rdma_cap_ib_cm(id->device, id->port_num)) {\n\t\tif (id->qp_type == IB_QPT_UD) {\n\t\t\tret = cma_send_sidr_rep(id_priv, IB_SIDR_REJECT, 0,\n\t\t\t\t\t\tprivate_data, private_data_len);\n\t\t} else {\n\t\t\ttrace_cm_send_rej(id_priv);\n\t\t\tret = ib_send_cm_rej(id_priv->cm_id.ib, reason, NULL, 0,\n\t\t\t\t\t     private_data, private_data_len);\n\t\t}\n\t} else if (rdma_cap_iw_cm(id->device, id->port_num)) {\n\t\tret = iw_cm_reject(id_priv->cm_id.iw,\n\t\t\t\t   private_data, private_data_len);\n\t} else {\n\t\tret = -ENOSYS;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_reject);\n\nint rdma_disconnect(struct rdma_cm_id *id)\n{\n\tstruct rdma_id_private *id_priv;\n\tint ret;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tif (!id_priv->cm_id.ib)\n\t\treturn -EINVAL;\n\n\tif (rdma_cap_ib_cm(id->device, id->port_num)) {\n\t\tret = cma_modify_qp_err(id_priv);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\t \n\t\ttrace_cm_disconnect(id_priv);\n\t\tif (ib_send_cm_dreq(id_priv->cm_id.ib, NULL, 0)) {\n\t\t\tif (!ib_send_cm_drep(id_priv->cm_id.ib, NULL, 0))\n\t\t\t\ttrace_cm_sent_drep(id_priv);\n\t\t} else {\n\t\t\ttrace_cm_sent_dreq(id_priv);\n\t\t}\n\t} else if (rdma_cap_iw_cm(id->device, id->port_num)) {\n\t\tret = iw_cm_disconnect(id_priv->cm_id.iw, 0);\n\t} else\n\t\tret = -EINVAL;\n\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_disconnect);\n\nstatic void cma_make_mc_event(int status, struct rdma_id_private *id_priv,\n\t\t\t      struct ib_sa_multicast *multicast,\n\t\t\t      struct rdma_cm_event *event,\n\t\t\t      struct cma_multicast *mc)\n{\n\tstruct rdma_dev_addr *dev_addr;\n\tenum ib_gid_type gid_type;\n\tstruct net_device *ndev;\n\n\tif (status)\n\t\tpr_debug_ratelimited(\"RDMA CM: MULTICAST_ERROR: failed to join multicast. status %d\\n\",\n\t\t\t\t     status);\n\n\tevent->status = status;\n\tevent->param.ud.private_data = mc->context;\n\tif (status) {\n\t\tevent->event = RDMA_CM_EVENT_MULTICAST_ERROR;\n\t\treturn;\n\t}\n\n\tdev_addr = &id_priv->id.route.addr.dev_addr;\n\tndev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if);\n\tgid_type =\n\t\tid_priv->cma_dev\n\t\t\t->default_gid_type[id_priv->id.port_num -\n\t\t\t\t\t   rdma_start_port(\n\t\t\t\t\t\t   id_priv->cma_dev->device)];\n\n\tevent->event = RDMA_CM_EVENT_MULTICAST_JOIN;\n\tif (ib_init_ah_from_mcmember(id_priv->id.device, id_priv->id.port_num,\n\t\t\t\t     &multicast->rec, ndev, gid_type,\n\t\t\t\t     &event->param.ud.ah_attr)) {\n\t\tevent->event = RDMA_CM_EVENT_MULTICAST_ERROR;\n\t\tgoto out;\n\t}\n\n\tevent->param.ud.qp_num = 0xFFFFFF;\n\tevent->param.ud.qkey = id_priv->qkey;\n\nout:\n\tdev_put(ndev);\n}\n\nstatic int cma_ib_mc_handler(int status, struct ib_sa_multicast *multicast)\n{\n\tstruct cma_multicast *mc = multicast->context;\n\tstruct rdma_id_private *id_priv = mc->id_priv;\n\tstruct rdma_cm_event event = {};\n\tint ret = 0;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\tif (READ_ONCE(id_priv->state) == RDMA_CM_DEVICE_REMOVAL ||\n\t    READ_ONCE(id_priv->state) == RDMA_CM_DESTROYING)\n\t\tgoto out;\n\n\tret = cma_set_qkey(id_priv, be32_to_cpu(multicast->rec.qkey));\n\tif (!ret) {\n\t\tcma_make_mc_event(status, id_priv, multicast, &event, mc);\n\t\tret = cma_cm_event_handler(id_priv, &event);\n\t}\n\trdma_destroy_ah_attr(&event.param.ud.ah_attr);\n\tWARN_ON(ret);\n\nout:\n\tmutex_unlock(&id_priv->handler_mutex);\n\treturn 0;\n}\n\nstatic void cma_set_mgid(struct rdma_id_private *id_priv,\n\t\t\t struct sockaddr *addr, union ib_gid *mgid)\n{\n\tunsigned char mc_map[MAX_ADDR_LEN];\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *) addr;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) addr;\n\n\tif (cma_any_addr(addr)) {\n\t\tmemset(mgid, 0, sizeof *mgid);\n\t} else if ((addr->sa_family == AF_INET6) &&\n\t\t   ((be32_to_cpu(sin6->sin6_addr.s6_addr32[0]) & 0xFFF0FFFF) ==\n\t\t\t\t\t\t\t\t 0xFF10A01B)) {\n\t\t \n\t\tmemcpy(mgid, &sin6->sin6_addr, sizeof *mgid);\n\t} else if (addr->sa_family == AF_IB) {\n\t\tmemcpy(mgid, &((struct sockaddr_ib *) addr)->sib_addr, sizeof *mgid);\n\t} else if (addr->sa_family == AF_INET6) {\n\t\tipv6_ib_mc_map(&sin6->sin6_addr, dev_addr->broadcast, mc_map);\n\t\tif (id_priv->id.ps == RDMA_PS_UDP)\n\t\t\tmc_map[7] = 0x01;\t \n\t\t*mgid = *(union ib_gid *) (mc_map + 4);\n\t} else {\n\t\tip_ib_mc_map(sin->sin_addr.s_addr, dev_addr->broadcast, mc_map);\n\t\tif (id_priv->id.ps == RDMA_PS_UDP)\n\t\t\tmc_map[7] = 0x01;\t \n\t\t*mgid = *(union ib_gid *) (mc_map + 4);\n\t}\n}\n\nstatic int cma_join_ib_multicast(struct rdma_id_private *id_priv,\n\t\t\t\t struct cma_multicast *mc)\n{\n\tstruct ib_sa_mcmember_rec rec;\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tib_sa_comp_mask comp_mask;\n\tint ret;\n\n\tib_addr_get_mgid(dev_addr, &rec.mgid);\n\tret = ib_sa_get_mcmember_rec(id_priv->id.device, id_priv->id.port_num,\n\t\t\t\t     &rec.mgid, &rec);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!id_priv->qkey) {\n\t\tret = cma_set_default_qkey(id_priv);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tcma_set_mgid(id_priv, (struct sockaddr *) &mc->addr, &rec.mgid);\n\trec.qkey = cpu_to_be32(id_priv->qkey);\n\trdma_addr_get_sgid(dev_addr, &rec.port_gid);\n\trec.pkey = cpu_to_be16(ib_addr_get_pkey(dev_addr));\n\trec.join_state = mc->join_state;\n\n\tcomp_mask = IB_SA_MCMEMBER_REC_MGID | IB_SA_MCMEMBER_REC_PORT_GID |\n\t\t    IB_SA_MCMEMBER_REC_PKEY | IB_SA_MCMEMBER_REC_JOIN_STATE |\n\t\t    IB_SA_MCMEMBER_REC_QKEY | IB_SA_MCMEMBER_REC_SL |\n\t\t    IB_SA_MCMEMBER_REC_FLOW_LABEL |\n\t\t    IB_SA_MCMEMBER_REC_TRAFFIC_CLASS;\n\n\tif (id_priv->id.ps == RDMA_PS_IPOIB)\n\t\tcomp_mask |= IB_SA_MCMEMBER_REC_RATE |\n\t\t\t     IB_SA_MCMEMBER_REC_RATE_SELECTOR |\n\t\t\t     IB_SA_MCMEMBER_REC_MTU_SELECTOR |\n\t\t\t     IB_SA_MCMEMBER_REC_MTU |\n\t\t\t     IB_SA_MCMEMBER_REC_HOP_LIMIT;\n\n\tmc->sa_mc = ib_sa_join_multicast(&sa_client, id_priv->id.device,\n\t\t\t\t\t id_priv->id.port_num, &rec, comp_mask,\n\t\t\t\t\t GFP_KERNEL, cma_ib_mc_handler, mc);\n\treturn PTR_ERR_OR_ZERO(mc->sa_mc);\n}\n\nstatic void cma_iboe_set_mgid(struct sockaddr *addr, union ib_gid *mgid,\n\t\t\t      enum ib_gid_type gid_type)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)addr;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)addr;\n\n\tif (cma_any_addr(addr)) {\n\t\tmemset(mgid, 0, sizeof *mgid);\n\t} else if (addr->sa_family == AF_INET6) {\n\t\tmemcpy(mgid, &sin6->sin6_addr, sizeof *mgid);\n\t} else {\n\t\tmgid->raw[0] =\n\t\t\t(gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) ? 0 : 0xff;\n\t\tmgid->raw[1] =\n\t\t\t(gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) ? 0 : 0x0e;\n\t\tmgid->raw[2] = 0;\n\t\tmgid->raw[3] = 0;\n\t\tmgid->raw[4] = 0;\n\t\tmgid->raw[5] = 0;\n\t\tmgid->raw[6] = 0;\n\t\tmgid->raw[7] = 0;\n\t\tmgid->raw[8] = 0;\n\t\tmgid->raw[9] = 0;\n\t\tmgid->raw[10] = 0xff;\n\t\tmgid->raw[11] = 0xff;\n\t\t*(__be32 *)(&mgid->raw[12]) = sin->sin_addr.s_addr;\n\t}\n}\n\nstatic int cma_iboe_join_multicast(struct rdma_id_private *id_priv,\n\t\t\t\t   struct cma_multicast *mc)\n{\n\tstruct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;\n\tint err = 0;\n\tstruct sockaddr *addr = (struct sockaddr *)&mc->addr;\n\tstruct net_device *ndev = NULL;\n\tstruct ib_sa_multicast ib = {};\n\tenum ib_gid_type gid_type;\n\tbool send_only;\n\n\tsend_only = mc->join_state == BIT(SENDONLY_FULLMEMBER_JOIN);\n\n\tif (cma_zero_addr(addr))\n\t\treturn -EINVAL;\n\n\tgid_type = id_priv->cma_dev->default_gid_type[id_priv->id.port_num -\n\t\t   rdma_start_port(id_priv->cma_dev->device)];\n\tcma_iboe_set_mgid(addr, &ib.rec.mgid, gid_type);\n\n\tib.rec.pkey = cpu_to_be16(0xffff);\n\tif (dev_addr->bound_dev_if)\n\t\tndev = dev_get_by_index(dev_addr->net, dev_addr->bound_dev_if);\n\tif (!ndev)\n\t\treturn -ENODEV;\n\n\tib.rec.rate = IB_RATE_PORT_CURRENT;\n\tib.rec.hop_limit = 1;\n\tib.rec.mtu = iboe_get_mtu(ndev->mtu);\n\n\tif (addr->sa_family == AF_INET) {\n\t\tif (gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) {\n\t\t\tib.rec.hop_limit = IPV6_DEFAULT_HOPLIMIT;\n\t\t\tif (!send_only) {\n\t\t\t\terr = cma_igmp_send(ndev, &ib.rec.mgid,\n\t\t\t\t\t\t    true);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP)\n\t\t\terr = -ENOTSUPP;\n\t}\n\tdev_put(ndev);\n\tif (err || !ib.rec.mtu)\n\t\treturn err ?: -EINVAL;\n\n\tif (!id_priv->qkey)\n\t\tcma_set_default_qkey(id_priv);\n\n\trdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr,\n\t\t    &ib.rec.port_gid);\n\tINIT_WORK(&mc->iboe_join.work, cma_iboe_join_work_handler);\n\tcma_make_mc_event(0, id_priv, &ib, &mc->iboe_join.event, mc);\n\tqueue_work(cma_wq, &mc->iboe_join.work);\n\treturn 0;\n}\n\nint rdma_join_multicast(struct rdma_cm_id *id, struct sockaddr *addr,\n\t\t\tu8 join_state, void *context)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tstruct cma_multicast *mc;\n\tint ret;\n\n\t \n\tif (WARN_ON(id->qp))\n\t\treturn -EINVAL;\n\n\t \n\tif (!id->device || (READ_ONCE(id_priv->state) != RDMA_CM_ADDR_BOUND &&\n\t\t\t    READ_ONCE(id_priv->state) != RDMA_CM_ADDR_RESOLVED))\n\t\treturn -EINVAL;\n\n\tif (id_priv->id.qp_type != IB_QPT_UD)\n\t\treturn -EINVAL;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn -ENOMEM;\n\n\tmemcpy(&mc->addr, addr, rdma_addr_size(addr));\n\tmc->context = context;\n\tmc->id_priv = id_priv;\n\tmc->join_state = join_state;\n\n\tif (rdma_protocol_roce(id->device, id->port_num)) {\n\t\tret = cma_iboe_join_multicast(id_priv, mc);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t} else if (rdma_cap_ib_mcast(id->device, id->port_num)) {\n\t\tret = cma_join_ib_multicast(id_priv, mc);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t} else {\n\t\tret = -ENOSYS;\n\t\tgoto out_err;\n\t}\n\n\tspin_lock(&id_priv->lock);\n\tlist_add(&mc->list, &id_priv->mc_list);\n\tspin_unlock(&id_priv->lock);\n\n\treturn 0;\nout_err:\n\tkfree(mc);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_join_multicast);\n\nvoid rdma_leave_multicast(struct rdma_cm_id *id, struct sockaddr *addr)\n{\n\tstruct rdma_id_private *id_priv;\n\tstruct cma_multicast *mc;\n\n\tid_priv = container_of(id, struct rdma_id_private, id);\n\tspin_lock_irq(&id_priv->lock);\n\tlist_for_each_entry(mc, &id_priv->mc_list, list) {\n\t\tif (memcmp(&mc->addr, addr, rdma_addr_size(addr)) != 0)\n\t\t\tcontinue;\n\t\tlist_del(&mc->list);\n\t\tspin_unlock_irq(&id_priv->lock);\n\n\t\tWARN_ON(id_priv->cma_dev->device != id->device);\n\t\tdestroy_mc(id_priv, mc);\n\t\treturn;\n\t}\n\tspin_unlock_irq(&id_priv->lock);\n}\nEXPORT_SYMBOL(rdma_leave_multicast);\n\nstatic int cma_netdev_change(struct net_device *ndev, struct rdma_id_private *id_priv)\n{\n\tstruct rdma_dev_addr *dev_addr;\n\tstruct cma_work *work;\n\n\tdev_addr = &id_priv->id.route.addr.dev_addr;\n\n\tif ((dev_addr->bound_dev_if == ndev->ifindex) &&\n\t    (net_eq(dev_net(ndev), dev_addr->net)) &&\n\t    memcmp(dev_addr->src_dev_addr, ndev->dev_addr, ndev->addr_len)) {\n\t\tpr_info(\"RDMA CM addr change for ndev %s used by id %p\\n\",\n\t\t\tndev->name, &id_priv->id);\n\t\twork = kzalloc(sizeof *work, GFP_KERNEL);\n\t\tif (!work)\n\t\t\treturn -ENOMEM;\n\n\t\tINIT_WORK(&work->work, cma_work_handler);\n\t\twork->id = id_priv;\n\t\twork->event.event = RDMA_CM_EVENT_ADDR_CHANGE;\n\t\tcma_id_get(id_priv);\n\t\tqueue_work(cma_wq, &work->work);\n\t}\n\n\treturn 0;\n}\n\nstatic int cma_netdev_callback(struct notifier_block *self, unsigned long event,\n\t\t\t       void *ptr)\n{\n\tstruct net_device *ndev = netdev_notifier_info_to_dev(ptr);\n\tstruct cma_device *cma_dev;\n\tstruct rdma_id_private *id_priv;\n\tint ret = NOTIFY_DONE;\n\n\tif (event != NETDEV_BONDING_FAILOVER)\n\t\treturn NOTIFY_DONE;\n\n\tif (!netif_is_bond_master(ndev))\n\t\treturn NOTIFY_DONE;\n\n\tmutex_lock(&lock);\n\tlist_for_each_entry(cma_dev, &dev_list, list)\n\t\tlist_for_each_entry(id_priv, &cma_dev->id_list, device_item) {\n\t\t\tret = cma_netdev_change(ndev, id_priv);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\nout:\n\tmutex_unlock(&lock);\n\treturn ret;\n}\n\nstatic void cma_netevent_work_handler(struct work_struct *_work)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(_work, struct rdma_id_private, id.net_work);\n\tstruct rdma_cm_event event = {};\n\n\tmutex_lock(&id_priv->handler_mutex);\n\n\tif (READ_ONCE(id_priv->state) == RDMA_CM_DESTROYING ||\n\t    READ_ONCE(id_priv->state) == RDMA_CM_DEVICE_REMOVAL)\n\t\tgoto out_unlock;\n\n\tevent.event = RDMA_CM_EVENT_UNREACHABLE;\n\tevent.status = -ETIMEDOUT;\n\n\tif (cma_cm_event_handler(id_priv, &event)) {\n\t\t__acquire(&id_priv->handler_mutex);\n\t\tid_priv->cm_id.ib = NULL;\n\t\tcma_id_put(id_priv);\n\t\tdestroy_id_handler_unlock(id_priv);\n\t\treturn;\n\t}\n\nout_unlock:\n\tmutex_unlock(&id_priv->handler_mutex);\n\tcma_id_put(id_priv);\n}\n\nstatic int cma_netevent_callback(struct notifier_block *self,\n\t\t\t\t unsigned long event, void *ctx)\n{\n\tstruct id_table_entry *ips_node = NULL;\n\tstruct rdma_id_private *current_id;\n\tstruct neighbour *neigh = ctx;\n\tunsigned long flags;\n\n\tif (event != NETEVENT_NEIGH_UPDATE)\n\t\treturn NOTIFY_DONE;\n\n\tspin_lock_irqsave(&id_table_lock, flags);\n\tif (neigh->tbl->family == AF_INET6) {\n\t\tstruct sockaddr_in6 neigh_sock_6;\n\n\t\tneigh_sock_6.sin6_family = AF_INET6;\n\t\tneigh_sock_6.sin6_addr = *(struct in6_addr *)neigh->primary_key;\n\t\tips_node = node_from_ndev_ip(&id_table, neigh->dev->ifindex,\n\t\t\t\t\t     (struct sockaddr *)&neigh_sock_6);\n\t} else if (neigh->tbl->family == AF_INET) {\n\t\tstruct sockaddr_in neigh_sock_4;\n\n\t\tneigh_sock_4.sin_family = AF_INET;\n\t\tneigh_sock_4.sin_addr.s_addr = *(__be32 *)(neigh->primary_key);\n\t\tips_node = node_from_ndev_ip(&id_table, neigh->dev->ifindex,\n\t\t\t\t\t     (struct sockaddr *)&neigh_sock_4);\n\t} else\n\t\tgoto out;\n\n\tif (!ips_node)\n\t\tgoto out;\n\n\tlist_for_each_entry(current_id, &ips_node->id_list, id_list_entry) {\n\t\tif (!memcmp(current_id->id.route.addr.dev_addr.dst_dev_addr,\n\t\t\t   neigh->ha, ETH_ALEN))\n\t\t\tcontinue;\n\t\tINIT_WORK(&current_id->id.net_work, cma_netevent_work_handler);\n\t\tcma_id_get(current_id);\n\t\tqueue_work(cma_wq, &current_id->id.net_work);\n\t}\nout:\n\tspin_unlock_irqrestore(&id_table_lock, flags);\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block cma_nb = {\n\t.notifier_call = cma_netdev_callback\n};\n\nstatic struct notifier_block cma_netevent_cb = {\n\t.notifier_call = cma_netevent_callback\n};\n\nstatic void cma_send_device_removal_put(struct rdma_id_private *id_priv)\n{\n\tstruct rdma_cm_event event = { .event = RDMA_CM_EVENT_DEVICE_REMOVAL };\n\tenum rdma_cm_state state;\n\tunsigned long flags;\n\n\tmutex_lock(&id_priv->handler_mutex);\n\t \n\tspin_lock_irqsave(&id_priv->lock, flags);\n\tstate = id_priv->state;\n\tif (state == RDMA_CM_DESTROYING || state == RDMA_CM_DEVICE_REMOVAL) {\n\t\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\t\tmutex_unlock(&id_priv->handler_mutex);\n\t\tcma_id_put(id_priv);\n\t\treturn;\n\t}\n\tid_priv->state = RDMA_CM_DEVICE_REMOVAL;\n\tspin_unlock_irqrestore(&id_priv->lock, flags);\n\n\tif (cma_cm_event_handler(id_priv, &event)) {\n\t\t \n\t\tcma_id_put(id_priv);\n\t\tmutex_unlock(&id_priv->handler_mutex);\n\t\ttrace_cm_id_destroy(id_priv);\n\t\t_destroy_id(id_priv, state);\n\t\treturn;\n\t}\n\tmutex_unlock(&id_priv->handler_mutex);\n\n\t \n\tcma_cancel_operation(id_priv, state);\n\tcma_id_put(id_priv);\n}\n\nstatic void cma_process_remove(struct cma_device *cma_dev)\n{\n\tmutex_lock(&lock);\n\twhile (!list_empty(&cma_dev->id_list)) {\n\t\tstruct rdma_id_private *id_priv = list_first_entry(\n\t\t\t&cma_dev->id_list, struct rdma_id_private, device_item);\n\n\t\tlist_del_init(&id_priv->listen_item);\n\t\tlist_del_init(&id_priv->device_item);\n\t\tcma_id_get(id_priv);\n\t\tmutex_unlock(&lock);\n\n\t\tcma_send_device_removal_put(id_priv);\n\n\t\tmutex_lock(&lock);\n\t}\n\tmutex_unlock(&lock);\n\n\tcma_dev_put(cma_dev);\n\twait_for_completion(&cma_dev->comp);\n}\n\nstatic bool cma_supported(struct ib_device *device)\n{\n\tu32 i;\n\n\trdma_for_each_port(device, i) {\n\t\tif (rdma_cap_ib_cm(device, i) || rdma_cap_iw_cm(device, i))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int cma_add_one(struct ib_device *device)\n{\n\tstruct rdma_id_private *to_destroy;\n\tstruct cma_device *cma_dev;\n\tstruct rdma_id_private *id_priv;\n\tunsigned long supported_gids = 0;\n\tint ret;\n\tu32 i;\n\n\tif (!cma_supported(device))\n\t\treturn -EOPNOTSUPP;\n\n\tcma_dev = kmalloc(sizeof(*cma_dev), GFP_KERNEL);\n\tif (!cma_dev)\n\t\treturn -ENOMEM;\n\n\tcma_dev->device = device;\n\tcma_dev->default_gid_type = kcalloc(device->phys_port_cnt,\n\t\t\t\t\t    sizeof(*cma_dev->default_gid_type),\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!cma_dev->default_gid_type) {\n\t\tret = -ENOMEM;\n\t\tgoto free_cma_dev;\n\t}\n\n\tcma_dev->default_roce_tos = kcalloc(device->phys_port_cnt,\n\t\t\t\t\t    sizeof(*cma_dev->default_roce_tos),\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!cma_dev->default_roce_tos) {\n\t\tret = -ENOMEM;\n\t\tgoto free_gid_type;\n\t}\n\n\trdma_for_each_port (device, i) {\n\t\tsupported_gids = roce_gid_type_mask_support(device, i);\n\t\tWARN_ON(!supported_gids);\n\t\tif (supported_gids & (1 << CMA_PREFERRED_ROCE_GID_TYPE))\n\t\t\tcma_dev->default_gid_type[i - rdma_start_port(device)] =\n\t\t\t\tCMA_PREFERRED_ROCE_GID_TYPE;\n\t\telse\n\t\t\tcma_dev->default_gid_type[i - rdma_start_port(device)] =\n\t\t\t\tfind_first_bit(&supported_gids, BITS_PER_LONG);\n\t\tcma_dev->default_roce_tos[i - rdma_start_port(device)] = 0;\n\t}\n\n\tinit_completion(&cma_dev->comp);\n\trefcount_set(&cma_dev->refcount, 1);\n\tINIT_LIST_HEAD(&cma_dev->id_list);\n\tib_set_client_data(device, &cma_client, cma_dev);\n\n\tmutex_lock(&lock);\n\tlist_add_tail(&cma_dev->list, &dev_list);\n\tlist_for_each_entry(id_priv, &listen_any_list, listen_any_item) {\n\t\tret = cma_listen_on_dev(id_priv, cma_dev, &to_destroy);\n\t\tif (ret)\n\t\t\tgoto free_listen;\n\t}\n\tmutex_unlock(&lock);\n\n\ttrace_cm_add_one(device);\n\treturn 0;\n\nfree_listen:\n\tlist_del(&cma_dev->list);\n\tmutex_unlock(&lock);\n\n\t \n\tcma_process_remove(cma_dev);\n\tkfree(cma_dev->default_roce_tos);\nfree_gid_type:\n\tkfree(cma_dev->default_gid_type);\n\nfree_cma_dev:\n\tkfree(cma_dev);\n\treturn ret;\n}\n\nstatic void cma_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct cma_device *cma_dev = client_data;\n\n\ttrace_cm_remove_one(device);\n\n\tmutex_lock(&lock);\n\tlist_del(&cma_dev->list);\n\tmutex_unlock(&lock);\n\n\tcma_process_remove(cma_dev);\n\tkfree(cma_dev->default_roce_tos);\n\tkfree(cma_dev->default_gid_type);\n\tkfree(cma_dev);\n}\n\nstatic int cma_init_net(struct net *net)\n{\n\tstruct cma_pernet *pernet = cma_pernet(net);\n\n\txa_init(&pernet->tcp_ps);\n\txa_init(&pernet->udp_ps);\n\txa_init(&pernet->ipoib_ps);\n\txa_init(&pernet->ib_ps);\n\n\treturn 0;\n}\n\nstatic void cma_exit_net(struct net *net)\n{\n\tstruct cma_pernet *pernet = cma_pernet(net);\n\n\tWARN_ON(!xa_empty(&pernet->tcp_ps));\n\tWARN_ON(!xa_empty(&pernet->udp_ps));\n\tWARN_ON(!xa_empty(&pernet->ipoib_ps));\n\tWARN_ON(!xa_empty(&pernet->ib_ps));\n}\n\nstatic struct pernet_operations cma_pernet_operations = {\n\t.init = cma_init_net,\n\t.exit = cma_exit_net,\n\t.id = &cma_pernet_id,\n\t.size = sizeof(struct cma_pernet),\n};\n\nstatic int __init cma_init(void)\n{\n\tint ret;\n\n\t \n\tif (IS_ENABLED(CONFIG_LOCKDEP)) {\n\t\trtnl_lock();\n\t\tmutex_lock(&lock);\n\t\tmutex_unlock(&lock);\n\t\trtnl_unlock();\n\t}\n\n\tcma_wq = alloc_ordered_workqueue(\"rdma_cm\", WQ_MEM_RECLAIM);\n\tif (!cma_wq)\n\t\treturn -ENOMEM;\n\n\tret = register_pernet_subsys(&cma_pernet_operations);\n\tif (ret)\n\t\tgoto err_wq;\n\n\tib_sa_register_client(&sa_client);\n\tregister_netdevice_notifier(&cma_nb);\n\tregister_netevent_notifier(&cma_netevent_cb);\n\n\tret = ib_register_client(&cma_client);\n\tif (ret)\n\t\tgoto err;\n\n\tret = cma_configfs_init();\n\tif (ret)\n\t\tgoto err_ib;\n\n\treturn 0;\n\nerr_ib:\n\tib_unregister_client(&cma_client);\nerr:\n\tunregister_netevent_notifier(&cma_netevent_cb);\n\tunregister_netdevice_notifier(&cma_nb);\n\tib_sa_unregister_client(&sa_client);\n\tunregister_pernet_subsys(&cma_pernet_operations);\nerr_wq:\n\tdestroy_workqueue(cma_wq);\n\treturn ret;\n}\n\nstatic void __exit cma_cleanup(void)\n{\n\tcma_configfs_exit();\n\tib_unregister_client(&cma_client);\n\tunregister_netevent_notifier(&cma_netevent_cb);\n\tunregister_netdevice_notifier(&cma_nb);\n\tib_sa_unregister_client(&sa_client);\n\tunregister_pernet_subsys(&cma_pernet_operations);\n\tdestroy_workqueue(cma_wq);\n}\n\nmodule_init(cma_init);\nmodule_exit(cma_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}