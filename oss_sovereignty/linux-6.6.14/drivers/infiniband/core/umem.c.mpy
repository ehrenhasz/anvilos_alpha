{
  "module_name": "umem.c",
  "hash_id": "2c66e467458a2a19ad82383edf7d1c5f24e1352112f052b627848aedfb45bc20",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/umem.c",
  "human_readable_source": " \n\n#include <linux/mm.h>\n#include <linux/dma-mapping.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/count_zeros.h>\n#include <rdma/ib_umem_odp.h>\n\n#include \"uverbs.h\"\n\nstatic void __ib_umem_release(struct ib_device *dev, struct ib_umem *umem, int dirty)\n{\n\tbool make_dirty = umem->writable && dirty;\n\tstruct scatterlist *sg;\n\tunsigned int i;\n\n\tif (dirty)\n\t\tib_dma_unmap_sgtable_attrs(dev, &umem->sgt_append.sgt,\n\t\t\t\t\t   DMA_BIDIRECTIONAL, 0);\n\n\tfor_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)\n\t\tunpin_user_page_range_dirty_lock(sg_page(sg),\n\t\t\tDIV_ROUND_UP(sg->length, PAGE_SIZE), make_dirty);\n\n\tsg_free_append_table(&umem->sgt_append);\n}\n\n \nunsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,\n\t\t\t\t     unsigned long pgsz_bitmap,\n\t\t\t\t     unsigned long virt)\n{\n\tstruct scatterlist *sg;\n\tunsigned long va, pgoff;\n\tdma_addr_t mask;\n\tint i;\n\n\tumem->iova = va = virt;\n\n\tif (umem->is_odp) {\n\t\tunsigned int page_size = BIT(to_ib_umem_odp(umem)->page_shift);\n\n\t\t \n\t\tif (!(pgsz_bitmap & page_size))\n\t\t\treturn 0;\n\t\treturn page_size;\n\t}\n\n\t \n\tmask = pgsz_bitmap &\n\t       GENMASK(BITS_PER_LONG - 1,\n\t\t       bits_per((umem->length - 1 + virt) ^ virt));\n\t \n\tpgoff = umem->address & ~PAGE_MASK;\n\n\tfor_each_sgtable_dma_sg(&umem->sgt_append.sgt, sg, i) {\n\t\t \n\t\tmask |= (sg_dma_address(sg) + pgoff) ^ va;\n\t\tva += sg_dma_len(sg) - pgoff;\n\t\t \n\t\tif (i != (umem->sgt_append.sgt.nents - 1))\n\t\t\tmask |= va;\n\t\tpgoff = 0;\n\t}\n\n\t \n\tif (mask)\n\t\tpgsz_bitmap &= GENMASK(count_trailing_zeros(mask), 0);\n\treturn pgsz_bitmap ? rounddown_pow_of_two(pgsz_bitmap) : 0;\n}\nEXPORT_SYMBOL(ib_umem_find_best_pgsz);\n\n \nstruct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,\n\t\t\t    size_t size, int access)\n{\n\tstruct ib_umem *umem;\n\tstruct page **page_list;\n\tunsigned long lock_limit;\n\tunsigned long new_pinned;\n\tunsigned long cur_base;\n\tunsigned long dma_attr = 0;\n\tstruct mm_struct *mm;\n\tunsigned long npages;\n\tint pinned, ret;\n\tunsigned int gup_flags = FOLL_LONGTERM;\n\n\t \n\tif (((addr + size) < addr) ||\n\t    PAGE_ALIGN(addr + size) < (addr + size))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!can_do_mlock())\n\t\treturn ERR_PTR(-EPERM);\n\n\tif (access & IB_ACCESS_ON_DEMAND)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tumem = kzalloc(sizeof(*umem), GFP_KERNEL);\n\tif (!umem)\n\t\treturn ERR_PTR(-ENOMEM);\n\tumem->ibdev      = device;\n\tumem->length     = size;\n\tumem->address    = addr;\n\t \n\tumem->iova = addr;\n\tumem->writable   = ib_access_writable(access);\n\tumem->owning_mm = mm = current->mm;\n\tmmgrab(mm);\n\n\tpage_list = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!page_list) {\n\t\tret = -ENOMEM;\n\t\tgoto umem_kfree;\n\t}\n\n\tnpages = ib_umem_num_pages(umem);\n\tif (npages == 0 || npages > UINT_MAX) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tnew_pinned = atomic64_add_return(npages, &mm->pinned_vm);\n\tif (new_pinned > lock_limit && !capable(CAP_IPC_LOCK)) {\n\t\tatomic64_sub(npages, &mm->pinned_vm);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcur_base = addr & PAGE_MASK;\n\n\tif (umem->writable)\n\t\tgup_flags |= FOLL_WRITE;\n\n\twhile (npages) {\n\t\tcond_resched();\n\t\tpinned = pin_user_pages_fast(cur_base,\n\t\t\t\t\t  min_t(unsigned long, npages,\n\t\t\t\t\t\tPAGE_SIZE /\n\t\t\t\t\t\tsizeof(struct page *)),\n\t\t\t\t\t  gup_flags, page_list);\n\t\tif (pinned < 0) {\n\t\t\tret = pinned;\n\t\t\tgoto umem_release;\n\t\t}\n\n\t\tcur_base += pinned * PAGE_SIZE;\n\t\tnpages -= pinned;\n\t\tret = sg_alloc_append_table_from_pages(\n\t\t\t&umem->sgt_append, page_list, pinned, 0,\n\t\t\tpinned << PAGE_SHIFT, ib_dma_max_seg_size(device),\n\t\t\tnpages, GFP_KERNEL);\n\t\tif (ret) {\n\t\t\tunpin_user_pages_dirty_lock(page_list, pinned, 0);\n\t\t\tgoto umem_release;\n\t\t}\n\t}\n\n\tif (access & IB_ACCESS_RELAXED_ORDERING)\n\t\tdma_attr |= DMA_ATTR_WEAK_ORDERING;\n\n\tret = ib_dma_map_sgtable_attrs(device, &umem->sgt_append.sgt,\n\t\t\t\t       DMA_BIDIRECTIONAL, dma_attr);\n\tif (ret)\n\t\tgoto umem_release;\n\tgoto out;\n\numem_release:\n\t__ib_umem_release(device, umem, 0);\n\tatomic64_sub(ib_umem_num_pages(umem), &mm->pinned_vm);\nout:\n\tfree_page((unsigned long) page_list);\numem_kfree:\n\tif (ret) {\n\t\tmmdrop(umem->owning_mm);\n\t\tkfree(umem);\n\t}\n\treturn ret ? ERR_PTR(ret) : umem;\n}\nEXPORT_SYMBOL(ib_umem_get);\n\n \nvoid ib_umem_release(struct ib_umem *umem)\n{\n\tif (!umem)\n\t\treturn;\n\tif (umem->is_dmabuf)\n\t\treturn ib_umem_dmabuf_release(to_ib_umem_dmabuf(umem));\n\tif (umem->is_odp)\n\t\treturn ib_umem_odp_release(to_ib_umem_odp(umem));\n\n\t__ib_umem_release(umem->ibdev, umem, 1);\n\n\tatomic64_sub(ib_umem_num_pages(umem), &umem->owning_mm->pinned_vm);\n\tmmdrop(umem->owning_mm);\n\tkfree(umem);\n}\nEXPORT_SYMBOL(ib_umem_release);\n\n \nint ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,\n\t\t      size_t length)\n{\n\tsize_t end = offset + length;\n\tint ret;\n\n\tif (offset > umem->length || length > umem->length - offset) {\n\t\tpr_err(\"%s not in range. offset: %zd umem length: %zd end: %zd\\n\",\n\t\t       __func__, offset, umem->length, end);\n\t\treturn -EINVAL;\n\t}\n\n\tret = sg_pcopy_to_buffer(umem->sgt_append.sgt.sgl,\n\t\t\t\t umem->sgt_append.sgt.orig_nents, dst, length,\n\t\t\t\t offset + ib_umem_offset(umem));\n\n\tif (ret < 0)\n\t\treturn ret;\n\telse if (ret != length)\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\nEXPORT_SYMBOL(ib_umem_copy_from);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}