{
  "module_name": "ib_core_uverbs.c",
  "hash_id": "2196557597c8c71af5ff40fa2d17d765bfd5fa24a81e8596d5cba000ace2cfc0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/ib_core_uverbs.c",
  "human_readable_source": "\n \n#include <linux/xarray.h>\n#include \"uverbs.h\"\n#include \"core_priv.h\"\n\n \nvoid rdma_umap_priv_init(struct rdma_umap_priv *priv,\n\t\t\t struct vm_area_struct *vma,\n\t\t\t struct rdma_user_mmap_entry *entry)\n{\n\tstruct ib_uverbs_file *ufile = vma->vm_file->private_data;\n\n\tpriv->vma = vma;\n\tif (entry) {\n\t\tkref_get(&entry->ref);\n\t\tpriv->entry = entry;\n\t}\n\tvma->vm_private_data = priv;\n\t \n\n\tmutex_lock(&ufile->umap_lock);\n\tlist_add(&priv->list, &ufile->umaps);\n\tmutex_unlock(&ufile->umap_lock);\n}\nEXPORT_SYMBOL(rdma_umap_priv_init);\n\n \nint rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,\n\t\t      unsigned long pfn, unsigned long size, pgprot_t prot,\n\t\t      struct rdma_user_mmap_entry *entry)\n{\n\tstruct ib_uverbs_file *ufile = ucontext->ufile;\n\tstruct rdma_umap_priv *priv;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tif (vma->vm_end - vma->vm_start != size)\n\t\treturn -EINVAL;\n\n\t \n\tif (WARN_ON(!vma->vm_file ||\n\t\t    vma->vm_file->private_data != ufile))\n\t\treturn -EINVAL;\n\tlockdep_assert_held(&ufile->device->disassociate_srcu);\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tvma->vm_page_prot = prot;\n\tif (io_remap_pfn_range(vma, vma->vm_start, pfn, size, prot)) {\n\t\tkfree(priv);\n\t\treturn -EAGAIN;\n\t}\n\n\trdma_umap_priv_init(priv, vma, entry);\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_user_mmap_io);\n\n \nstruct rdma_user_mmap_entry *\nrdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,\n\t\t\t       unsigned long pgoff)\n{\n\tstruct rdma_user_mmap_entry *entry;\n\n\tif (pgoff > U32_MAX)\n\t\treturn NULL;\n\n\txa_lock(&ucontext->mmap_xa);\n\n\tentry = xa_load(&ucontext->mmap_xa, pgoff);\n\n\t \n\tif (!entry || entry->start_pgoff != pgoff || entry->driver_removed ||\n\t    !kref_get_unless_zero(&entry->ref))\n\t\tgoto err;\n\n\txa_unlock(&ucontext->mmap_xa);\n\n\tibdev_dbg(ucontext->device, \"mmap: pgoff[%#lx] npages[%#zx] returned\\n\",\n\t\t  pgoff, entry->npages);\n\n\treturn entry;\n\nerr:\n\txa_unlock(&ucontext->mmap_xa);\n\treturn NULL;\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_get_pgoff);\n\n \nstruct rdma_user_mmap_entry *\nrdma_user_mmap_entry_get(struct ib_ucontext *ucontext,\n\t\t\t struct vm_area_struct *vma)\n{\n\tstruct rdma_user_mmap_entry *entry;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn NULL;\n\tentry = rdma_user_mmap_entry_get_pgoff(ucontext, vma->vm_pgoff);\n\tif (!entry)\n\t\treturn NULL;\n\tif (entry->npages * PAGE_SIZE != vma->vm_end - vma->vm_start) {\n\t\trdma_user_mmap_entry_put(entry);\n\t\treturn NULL;\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_get);\n\nstatic void rdma_user_mmap_entry_free(struct kref *kref)\n{\n\tstruct rdma_user_mmap_entry *entry =\n\t\tcontainer_of(kref, struct rdma_user_mmap_entry, ref);\n\tstruct ib_ucontext *ucontext = entry->ucontext;\n\tunsigned long i;\n\n\t \n\txa_lock(&ucontext->mmap_xa);\n\tfor (i = 0; i < entry->npages; i++)\n\t\t__xa_erase(&ucontext->mmap_xa, entry->start_pgoff + i);\n\txa_unlock(&ucontext->mmap_xa);\n\n\tibdev_dbg(ucontext->device, \"mmap: pgoff[%#lx] npages[%#zx] removed\\n\",\n\t\t  entry->start_pgoff, entry->npages);\n\n\tif (ucontext->device->ops.mmap_free)\n\t\tucontext->device->ops.mmap_free(entry);\n}\n\n \nvoid rdma_user_mmap_entry_put(struct rdma_user_mmap_entry *entry)\n{\n\tkref_put(&entry->ref, rdma_user_mmap_entry_free);\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_put);\n\n \nvoid rdma_user_mmap_entry_remove(struct rdma_user_mmap_entry *entry)\n{\n\tif (!entry)\n\t\treturn;\n\n\txa_lock(&entry->ucontext->mmap_xa);\n\tentry->driver_removed = true;\n\txa_unlock(&entry->ucontext->mmap_xa);\n\tkref_put(&entry->ref, rdma_user_mmap_entry_free);\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_remove);\n\n \nint rdma_user_mmap_entry_insert_range(struct ib_ucontext *ucontext,\n\t\t\t\t      struct rdma_user_mmap_entry *entry,\n\t\t\t\t      size_t length, u32 min_pgoff,\n\t\t\t\t      u32 max_pgoff)\n{\n\tstruct ib_uverbs_file *ufile = ucontext->ufile;\n\tXA_STATE(xas, &ucontext->mmap_xa, min_pgoff);\n\tu32 xa_first, xa_last, npages;\n\tint err;\n\tu32 i;\n\n\tif (!entry)\n\t\treturn -EINVAL;\n\n\tkref_init(&entry->ref);\n\tentry->ucontext = ucontext;\n\n\t \n\tmutex_lock(&ufile->umap_lock);\n\n\txa_lock(&ucontext->mmap_xa);\n\n\t \n\tnpages = (u32)DIV_ROUND_UP(length, PAGE_SIZE);\n\tentry->npages = npages;\n\twhile (true) {\n\t\t \n\t\txas_find_marked(&xas, max_pgoff, XA_FREE_MARK);\n\t\tif (xas.xa_node == XAS_RESTART)\n\t\t\tgoto err_unlock;\n\n\t\txa_first = xas.xa_index;\n\n\t\t \n\t\tif (check_add_overflow(xa_first, npages, &xa_last))\n\t\t\tgoto err_unlock;\n\n\t\t \n\t\txas_next_entry(&xas, xa_last - 1);\n\t\tif (xas.xa_node == XAS_BOUNDS || xas.xa_index >= xa_last)\n\t\t\tbreak;\n\t}\n\n\tfor (i = xa_first; i < xa_last; i++) {\n\t\terr = __xa_insert(&ucontext->mmap_xa, i, entry, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto err_undo;\n\t}\n\n\t \n\tentry->start_pgoff = xa_first;\n\txa_unlock(&ucontext->mmap_xa);\n\tmutex_unlock(&ufile->umap_lock);\n\n\tibdev_dbg(ucontext->device, \"mmap: pgoff[%#lx] npages[%#x] inserted\\n\",\n\t\t  entry->start_pgoff, npages);\n\n\treturn 0;\n\nerr_undo:\n\tfor (; i > xa_first; i--)\n\t\t__xa_erase(&ucontext->mmap_xa, i - 1);\n\nerr_unlock:\n\txa_unlock(&ucontext->mmap_xa);\n\tmutex_unlock(&ufile->umap_lock);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_insert_range);\n\n \nint rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,\n\t\t\t\tstruct rdma_user_mmap_entry *entry,\n\t\t\t\tsize_t length)\n{\n\treturn rdma_user_mmap_entry_insert_range(ucontext, entry, length, 0,\n\t\t\t\t\t\t U32_MAX);\n}\nEXPORT_SYMBOL(rdma_user_mmap_entry_insert);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}