{
  "module_name": "cq.c",
  "hash_id": "e567164356b79999730b3b93022b30f83d927c9a0dd93b881b8c84ebc105b3fd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/cq.c",
  "human_readable_source": "\n \n#include <linux/err.h>\n#include <linux/slab.h>\n#include <rdma/ib_verbs.h>\n\n#include \"core_priv.h\"\n\n#include <trace/events/rdma_core.h>\n \n#define IB_MAX_SHARED_CQ_SZ\t\t4096U\n\n \n#define IB_POLL_BATCH\t\t\t16\n#define IB_POLL_BATCH_DIRECT\t\t8\n\n \n#define IB_POLL_BUDGET_IRQ\t\t256\n#define IB_POLL_BUDGET_WORKQUEUE\t65536\n\n#define IB_POLL_FLAGS \\\n\t(IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS)\n\nstatic const struct dim_cq_moder\nrdma_dim_prof[RDMA_DIM_PARAMS_NUM_PROFILES] = {\n\t{1,   0, 1,  0},\n\t{1,   0, 4,  0},\n\t{2,   0, 4,  0},\n\t{2,   0, 8,  0},\n\t{4,   0, 8,  0},\n\t{16,  0, 8,  0},\n\t{16,  0, 16, 0},\n\t{32,  0, 16, 0},\n\t{32,  0, 32, 0},\n};\n\nstatic void ib_cq_rdma_dim_work(struct work_struct *w)\n{\n\tstruct dim *dim = container_of(w, struct dim, work);\n\tstruct ib_cq *cq = dim->priv;\n\n\tu16 usec = rdma_dim_prof[dim->profile_ix].usec;\n\tu16 comps = rdma_dim_prof[dim->profile_ix].comps;\n\n\tdim->state = DIM_START_MEASURE;\n\n\ttrace_cq_modify(cq, comps, usec);\n\tcq->device->ops.modify_cq(cq, comps, usec);\n}\n\nstatic void rdma_dim_init(struct ib_cq *cq)\n{\n\tstruct dim *dim;\n\n\tif (!cq->device->ops.modify_cq || !cq->device->use_cq_dim ||\n\t    cq->poll_ctx == IB_POLL_DIRECT)\n\t\treturn;\n\n\tdim = kzalloc(sizeof(struct dim), GFP_KERNEL);\n\tif (!dim)\n\t\treturn;\n\n\tdim->state = DIM_START_MEASURE;\n\tdim->tune_state = DIM_GOING_RIGHT;\n\tdim->profile_ix = RDMA_DIM_START_PROFILE;\n\tdim->priv = cq;\n\tcq->dim = dim;\n\n\tINIT_WORK(&dim->work, ib_cq_rdma_dim_work);\n}\n\nstatic void rdma_dim_destroy(struct ib_cq *cq)\n{\n\tif (!cq->dim)\n\t\treturn;\n\n\tcancel_work_sync(&cq->dim->work);\n\tkfree(cq->dim);\n}\n\nstatic int __poll_cq(struct ib_cq *cq, int num_entries, struct ib_wc *wc)\n{\n\tint rc;\n\n\trc = ib_poll_cq(cq, num_entries, wc);\n\ttrace_cq_poll(cq, num_entries, rc);\n\treturn rc;\n}\n\nstatic int __ib_process_cq(struct ib_cq *cq, int budget, struct ib_wc *wcs,\n\t\t\t   int batch)\n{\n\tint i, n, completed = 0;\n\n\ttrace_cq_process(cq);\n\n\t \n\twhile ((n = __poll_cq(cq, min_t(u32, batch,\n\t\t\t\t\tbudget - completed), wcs)) > 0) {\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tstruct ib_wc *wc = &wcs[i];\n\n\t\t\tif (wc->wr_cqe)\n\t\t\t\twc->wr_cqe->done(cq, wc);\n\t\t\telse\n\t\t\t\tWARN_ON_ONCE(wc->status == IB_WC_SUCCESS);\n\t\t}\n\n\t\tcompleted += n;\n\n\t\tif (n != batch || (budget != -1 && completed >= budget))\n\t\t\tbreak;\n\t}\n\n\treturn completed;\n}\n\n \nint ib_process_cq_direct(struct ib_cq *cq, int budget)\n{\n\tstruct ib_wc wcs[IB_POLL_BATCH_DIRECT];\n\n\treturn __ib_process_cq(cq, budget, wcs, IB_POLL_BATCH_DIRECT);\n}\nEXPORT_SYMBOL(ib_process_cq_direct);\n\nstatic void ib_cq_completion_direct(struct ib_cq *cq, void *private)\n{\n\tWARN_ONCE(1, \"got unsolicited completion for CQ 0x%p\\n\", cq);\n}\n\nstatic int ib_poll_handler(struct irq_poll *iop, int budget)\n{\n\tstruct ib_cq *cq = container_of(iop, struct ib_cq, iop);\n\tstruct dim *dim = cq->dim;\n\tint completed;\n\n\tcompleted = __ib_process_cq(cq, budget, cq->wc, IB_POLL_BATCH);\n\tif (completed < budget) {\n\t\tirq_poll_complete(&cq->iop);\n\t\tif (ib_req_notify_cq(cq, IB_POLL_FLAGS) > 0) {\n\t\t\ttrace_cq_reschedule(cq);\n\t\t\tirq_poll_sched(&cq->iop);\n\t\t}\n\t}\n\n\tif (dim)\n\t\trdma_dim(dim, completed);\n\n\treturn completed;\n}\n\nstatic void ib_cq_completion_softirq(struct ib_cq *cq, void *private)\n{\n\ttrace_cq_schedule(cq);\n\tirq_poll_sched(&cq->iop);\n}\n\nstatic void ib_cq_poll_work(struct work_struct *work)\n{\n\tstruct ib_cq *cq = container_of(work, struct ib_cq, work);\n\tint completed;\n\n\tcompleted = __ib_process_cq(cq, IB_POLL_BUDGET_WORKQUEUE, cq->wc,\n\t\t\t\t    IB_POLL_BATCH);\n\tif (completed >= IB_POLL_BUDGET_WORKQUEUE ||\n\t    ib_req_notify_cq(cq, IB_POLL_FLAGS) > 0)\n\t\tqueue_work(cq->comp_wq, &cq->work);\n\telse if (cq->dim)\n\t\trdma_dim(cq->dim, completed);\n}\n\nstatic void ib_cq_completion_workqueue(struct ib_cq *cq, void *private)\n{\n\ttrace_cq_schedule(cq);\n\tqueue_work(cq->comp_wq, &cq->work);\n}\n\n \nstruct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private, int nr_cqe,\n\t\t\t    int comp_vector, enum ib_poll_context poll_ctx,\n\t\t\t    const char *caller)\n{\n\tstruct ib_cq_init_attr cq_attr = {\n\t\t.cqe\t\t= nr_cqe,\n\t\t.comp_vector\t= comp_vector,\n\t};\n\tstruct ib_cq *cq;\n\tint ret = -ENOMEM;\n\n\tcq = rdma_zalloc_drv_obj(dev, ib_cq);\n\tif (!cq)\n\t\treturn ERR_PTR(ret);\n\n\tcq->device = dev;\n\tcq->cq_context = private;\n\tcq->poll_ctx = poll_ctx;\n\tatomic_set(&cq->usecnt, 0);\n\tcq->comp_vector = comp_vector;\n\n\tcq->wc = kmalloc_array(IB_POLL_BATCH, sizeof(*cq->wc), GFP_KERNEL);\n\tif (!cq->wc)\n\t\tgoto out_free_cq;\n\n\trdma_restrack_new(&cq->res, RDMA_RESTRACK_CQ);\n\trdma_restrack_set_name(&cq->res, caller);\n\n\tret = dev->ops.create_cq(cq, &cq_attr, NULL);\n\tif (ret)\n\t\tgoto out_free_wc;\n\n\trdma_dim_init(cq);\n\n\tswitch (cq->poll_ctx) {\n\tcase IB_POLL_DIRECT:\n\t\tcq->comp_handler = ib_cq_completion_direct;\n\t\tbreak;\n\tcase IB_POLL_SOFTIRQ:\n\t\tcq->comp_handler = ib_cq_completion_softirq;\n\n\t\tirq_poll_init(&cq->iop, IB_POLL_BUDGET_IRQ, ib_poll_handler);\n\t\tib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\n\t\tbreak;\n\tcase IB_POLL_WORKQUEUE:\n\tcase IB_POLL_UNBOUND_WORKQUEUE:\n\t\tcq->comp_handler = ib_cq_completion_workqueue;\n\t\tINIT_WORK(&cq->work, ib_cq_poll_work);\n\t\tib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\n\t\tcq->comp_wq = (cq->poll_ctx == IB_POLL_WORKQUEUE) ?\n\t\t\t\tib_comp_wq : ib_comp_unbound_wq;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto out_destroy_cq;\n\t}\n\n\trdma_restrack_add(&cq->res);\n\ttrace_cq_alloc(cq, nr_cqe, comp_vector, poll_ctx);\n\treturn cq;\n\nout_destroy_cq:\n\trdma_dim_destroy(cq);\n\tcq->device->ops.destroy_cq(cq, NULL);\nout_free_wc:\n\trdma_restrack_put(&cq->res);\n\tkfree(cq->wc);\nout_free_cq:\n\tkfree(cq);\n\ttrace_cq_alloc_error(nr_cqe, comp_vector, poll_ctx, ret);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(__ib_alloc_cq);\n\n \nstruct ib_cq *__ib_alloc_cq_any(struct ib_device *dev, void *private,\n\t\t\t\tint nr_cqe, enum ib_poll_context poll_ctx,\n\t\t\t\tconst char *caller)\n{\n\tstatic atomic_t counter;\n\tint comp_vector = 0;\n\n\tif (dev->num_comp_vectors > 1)\n\t\tcomp_vector =\n\t\t\tatomic_inc_return(&counter) %\n\t\t\tmin_t(int, dev->num_comp_vectors, num_online_cpus());\n\n\treturn __ib_alloc_cq(dev, private, nr_cqe, comp_vector, poll_ctx,\n\t\t\t     caller);\n}\nEXPORT_SYMBOL(__ib_alloc_cq_any);\n\n \nvoid ib_free_cq(struct ib_cq *cq)\n{\n\tint ret;\n\n\tif (WARN_ON_ONCE(atomic_read(&cq->usecnt)))\n\t\treturn;\n\tif (WARN_ON_ONCE(cq->cqe_used))\n\t\treturn;\n\n\tswitch (cq->poll_ctx) {\n\tcase IB_POLL_DIRECT:\n\t\tbreak;\n\tcase IB_POLL_SOFTIRQ:\n\t\tirq_poll_disable(&cq->iop);\n\t\tbreak;\n\tcase IB_POLL_WORKQUEUE:\n\tcase IB_POLL_UNBOUND_WORKQUEUE:\n\t\tcancel_work_sync(&cq->work);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\trdma_dim_destroy(cq);\n\ttrace_cq_free(cq);\n\tret = cq->device->ops.destroy_cq(cq, NULL);\n\tWARN_ONCE(ret, \"Destroy of kernel CQ shouldn't fail\");\n\trdma_restrack_del(&cq->res);\n\tkfree(cq->wc);\n\tkfree(cq);\n}\nEXPORT_SYMBOL(ib_free_cq);\n\nvoid ib_cq_pool_cleanup(struct ib_device *dev)\n{\n\tstruct ib_cq *cq, *n;\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->cq_pools); i++) {\n\t\tlist_for_each_entry_safe(cq, n, &dev->cq_pools[i],\n\t\t\t\t\t pool_entry) {\n\t\t\tWARN_ON(cq->cqe_used);\n\t\t\tlist_del(&cq->pool_entry);\n\t\t\tcq->shared = false;\n\t\t\tib_free_cq(cq);\n\t\t}\n\t}\n}\n\nstatic int ib_alloc_cqs(struct ib_device *dev, unsigned int nr_cqes,\n\t\t\tenum ib_poll_context poll_ctx)\n{\n\tLIST_HEAD(tmp_list);\n\tunsigned int nr_cqs, i;\n\tstruct ib_cq *cq, *n;\n\tint ret;\n\n\tif (poll_ctx > IB_POLL_LAST_POOL_TYPE) {\n\t\tWARN_ON_ONCE(poll_ctx > IB_POLL_LAST_POOL_TYPE);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tnr_cqes = min_t(unsigned int, dev->attrs.max_cqe,\n\t\t\tmax(nr_cqes, IB_MAX_SHARED_CQ_SZ));\n\tnr_cqs = min_t(unsigned int, dev->num_comp_vectors, num_online_cpus());\n\tfor (i = 0; i < nr_cqs; i++) {\n\t\tcq = ib_alloc_cq(dev, NULL, nr_cqes, i, poll_ctx);\n\t\tif (IS_ERR(cq)) {\n\t\t\tret = PTR_ERR(cq);\n\t\t\tgoto out_free_cqs;\n\t\t}\n\t\tcq->shared = true;\n\t\tlist_add_tail(&cq->pool_entry, &tmp_list);\n\t}\n\n\tspin_lock_irq(&dev->cq_pools_lock);\n\tlist_splice(&tmp_list, &dev->cq_pools[poll_ctx]);\n\tspin_unlock_irq(&dev->cq_pools_lock);\n\n\treturn 0;\n\nout_free_cqs:\n\tlist_for_each_entry_safe(cq, n, &tmp_list, pool_entry) {\n\t\tcq->shared = false;\n\t\tib_free_cq(cq);\n\t}\n\treturn ret;\n}\n\n \nstruct ib_cq *ib_cq_pool_get(struct ib_device *dev, unsigned int nr_cqe,\n\t\t\t     int comp_vector_hint,\n\t\t\t     enum ib_poll_context poll_ctx)\n{\n\tstatic unsigned int default_comp_vector;\n\tunsigned int vector, num_comp_vectors;\n\tstruct ib_cq *cq, *found = NULL;\n\tint ret;\n\n\tif (poll_ctx > IB_POLL_LAST_POOL_TYPE) {\n\t\tWARN_ON_ONCE(poll_ctx > IB_POLL_LAST_POOL_TYPE);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tnum_comp_vectors =\n\t\tmin_t(unsigned int, dev->num_comp_vectors, num_online_cpus());\n\t \n\tif (comp_vector_hint < 0) {\n\t\tcomp_vector_hint =\n\t\t\t(READ_ONCE(default_comp_vector) + 1) % num_comp_vectors;\n\t\tWRITE_ONCE(default_comp_vector, comp_vector_hint);\n\t}\n\tvector = comp_vector_hint % num_comp_vectors;\n\n\t \n\twhile (!found) {\n\t\tspin_lock_irq(&dev->cq_pools_lock);\n\t\tlist_for_each_entry(cq, &dev->cq_pools[poll_ctx],\n\t\t\t\t    pool_entry) {\n\t\t\t \n\t\t\tif (vector != cq->comp_vector)\n\t\t\t\tcontinue;\n\t\t\tif (cq->cqe_used + nr_cqe > cq->cqe)\n\t\t\t\tcontinue;\n\t\t\tfound = cq;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (found) {\n\t\t\tfound->cqe_used += nr_cqe;\n\t\t\tspin_unlock_irq(&dev->cq_pools_lock);\n\n\t\t\treturn found;\n\t\t}\n\t\tspin_unlock_irq(&dev->cq_pools_lock);\n\n\t\t \n\t\tret = ib_alloc_cqs(dev, nr_cqe, poll_ctx);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn found;\n}\nEXPORT_SYMBOL(ib_cq_pool_get);\n\n \nvoid ib_cq_pool_put(struct ib_cq *cq, unsigned int nr_cqe)\n{\n\tif (WARN_ON_ONCE(nr_cqe > cq->cqe_used))\n\t\treturn;\n\n\tspin_lock_irq(&cq->device->cq_pools_lock);\n\tcq->cqe_used -= nr_cqe;\n\tspin_unlock_irq(&cq->device->cq_pools_lock);\n}\nEXPORT_SYMBOL(ib_cq_pool_put);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}