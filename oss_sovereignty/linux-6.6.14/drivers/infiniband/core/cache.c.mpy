{
  "module_name": "cache.c",
  "hash_id": "64a7dc7b2e5757da240d74e7c2e3f46efd332708ddede992fb606e78610294ee",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/cache.c",
  "human_readable_source": " \n\n#include <linux/if_vlan.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/netdevice.h>\n#include <net/addrconf.h>\n\n#include <rdma/ib_cache.h>\n\n#include \"core_priv.h\"\n\nstruct ib_pkey_cache {\n\tint             table_len;\n\tu16             table[];\n};\n\nstruct ib_update_work {\n\tstruct work_struct work;\n\tstruct ib_event event;\n\tbool enforce_security;\n};\n\nunion ib_gid zgid;\nEXPORT_SYMBOL(zgid);\n\nenum gid_attr_find_mask {\n\tGID_ATTR_FIND_MASK_GID          = 1UL << 0,\n\tGID_ATTR_FIND_MASK_NETDEV\t= 1UL << 1,\n\tGID_ATTR_FIND_MASK_DEFAULT\t= 1UL << 2,\n\tGID_ATTR_FIND_MASK_GID_TYPE\t= 1UL << 3,\n};\n\nenum gid_table_entry_state {\n\tGID_TABLE_ENTRY_INVALID\t\t= 1,\n\tGID_TABLE_ENTRY_VALID\t\t= 2,\n\t \n\tGID_TABLE_ENTRY_PENDING_DEL\t= 3,\n};\n\nstruct roce_gid_ndev_storage {\n\tstruct rcu_head rcu_head;\n\tstruct net_device *ndev;\n};\n\nstruct ib_gid_table_entry {\n\tstruct kref\t\t\tkref;\n\tstruct work_struct\t\tdel_work;\n\tstruct ib_gid_attr\t\tattr;\n\tvoid\t\t\t\t*context;\n\t \n\tstruct roce_gid_ndev_storage\t*ndev_storage;\n\tenum gid_table_entry_state\tstate;\n};\n\nstruct ib_gid_table {\n\tint\t\t\t\tsz;\n\t \n\t \n\tstruct mutex\t\t\tlock;\n\t \n\trwlock_t\t\t\trwlock;\n\tstruct ib_gid_table_entry\t**data_vec;\n\t \n\tu32\t\t\t\tdefault_gid_indices;\n};\n\nstatic void dispatch_gid_change_event(struct ib_device *ib_dev, u32 port)\n{\n\tstruct ib_event event;\n\n\tevent.device\t\t= ib_dev;\n\tevent.element.port_num\t= port;\n\tevent.event\t\t= IB_EVENT_GID_CHANGE;\n\n\tib_dispatch_event_clients(&event);\n}\n\nstatic const char * const gid_type_str[] = {\n\t \n\t[IB_GID_TYPE_IB]\t= \"IB/RoCE v1\",\n\t[IB_GID_TYPE_ROCE]\t= \"IB/RoCE v1\",\n\t[IB_GID_TYPE_ROCE_UDP_ENCAP]\t= \"RoCE v2\",\n};\n\nconst char *ib_cache_gid_type_str(enum ib_gid_type gid_type)\n{\n\tif (gid_type < ARRAY_SIZE(gid_type_str) && gid_type_str[gid_type])\n\t\treturn gid_type_str[gid_type];\n\n\treturn \"Invalid GID type\";\n}\nEXPORT_SYMBOL(ib_cache_gid_type_str);\n\n \nbool rdma_is_zero_gid(const union ib_gid *gid)\n{\n\treturn !memcmp(gid, &zgid, sizeof(*gid));\n}\nEXPORT_SYMBOL(rdma_is_zero_gid);\n\n \nstatic bool is_gid_index_default(const struct ib_gid_table *table,\n\t\t\t\t unsigned int index)\n{\n\treturn index < 32 && (BIT(index) & table->default_gid_indices);\n}\n\nint ib_cache_gid_parse_type_str(const char *buf)\n{\n\tunsigned int i;\n\tsize_t len;\n\tint err = -EINVAL;\n\n\tlen = strlen(buf);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tif (buf[len - 1] == '\\n')\n\t\tlen--;\n\n\tfor (i = 0; i < ARRAY_SIZE(gid_type_str); ++i)\n\t\tif (gid_type_str[i] && !strncmp(buf, gid_type_str[i], len) &&\n\t\t    len == strlen(gid_type_str[i])) {\n\t\t\terr = i;\n\t\t\tbreak;\n\t\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(ib_cache_gid_parse_type_str);\n\nstatic struct ib_gid_table *rdma_gid_table(struct ib_device *device, u32 port)\n{\n\treturn device->port_data[port].cache.gid;\n}\n\nstatic bool is_gid_entry_free(const struct ib_gid_table_entry *entry)\n{\n\treturn !entry;\n}\n\nstatic bool is_gid_entry_valid(const struct ib_gid_table_entry *entry)\n{\n\treturn entry && entry->state == GID_TABLE_ENTRY_VALID;\n}\n\nstatic void schedule_free_gid(struct kref *kref)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\t\tcontainer_of(kref, struct ib_gid_table_entry, kref);\n\n\tqueue_work(ib_wq, &entry->del_work);\n}\n\nstatic void put_gid_ndev(struct rcu_head *head)\n{\n\tstruct roce_gid_ndev_storage *storage =\n\t\tcontainer_of(head, struct roce_gid_ndev_storage, rcu_head);\n\n\tWARN_ON(!storage->ndev);\n\t \n\tdev_put(storage->ndev);\n\tkfree(storage);\n}\n\nstatic void free_gid_entry_locked(struct ib_gid_table_entry *entry)\n{\n\tstruct ib_device *device = entry->attr.device;\n\tu32 port_num = entry->attr.port_num;\n\tstruct ib_gid_table *table = rdma_gid_table(device, port_num);\n\n\tdev_dbg(&device->dev, \"%s port=%u index=%u gid %pI6\\n\", __func__,\n\t\tport_num, entry->attr.index, entry->attr.gid.raw);\n\n\twrite_lock_irq(&table->rwlock);\n\n\t \n\tif (entry == table->data_vec[entry->attr.index])\n\t\ttable->data_vec[entry->attr.index] = NULL;\n\t \n\twrite_unlock_irq(&table->rwlock);\n\n\tif (entry->ndev_storage)\n\t\tcall_rcu(&entry->ndev_storage->rcu_head, put_gid_ndev);\n\tkfree(entry);\n}\n\nstatic void free_gid_entry(struct kref *kref)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\t\tcontainer_of(kref, struct ib_gid_table_entry, kref);\n\n\tfree_gid_entry_locked(entry);\n}\n\n \nstatic void free_gid_work(struct work_struct *work)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\tcontainer_of(work, struct ib_gid_table_entry, del_work);\n\tstruct ib_device *device = entry->attr.device;\n\tu32 port_num = entry->attr.port_num;\n\tstruct ib_gid_table *table = rdma_gid_table(device, port_num);\n\n\tmutex_lock(&table->lock);\n\tfree_gid_entry_locked(entry);\n\tmutex_unlock(&table->lock);\n}\n\nstatic struct ib_gid_table_entry *\nalloc_gid_entry(const struct ib_gid_attr *attr)\n{\n\tstruct ib_gid_table_entry *entry;\n\tstruct net_device *ndev;\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn NULL;\n\n\tndev = rcu_dereference_protected(attr->ndev, 1);\n\tif (ndev) {\n\t\tentry->ndev_storage = kzalloc(sizeof(*entry->ndev_storage),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!entry->ndev_storage) {\n\t\t\tkfree(entry);\n\t\t\treturn NULL;\n\t\t}\n\t\tdev_hold(ndev);\n\t\tentry->ndev_storage->ndev = ndev;\n\t}\n\tkref_init(&entry->kref);\n\tmemcpy(&entry->attr, attr, sizeof(*attr));\n\tINIT_WORK(&entry->del_work, free_gid_work);\n\tentry->state = GID_TABLE_ENTRY_INVALID;\n\treturn entry;\n}\n\nstatic void store_gid_entry(struct ib_gid_table *table,\n\t\t\t    struct ib_gid_table_entry *entry)\n{\n\tentry->state = GID_TABLE_ENTRY_VALID;\n\n\tdev_dbg(&entry->attr.device->dev, \"%s port=%u index=%u gid %pI6\\n\",\n\t\t__func__, entry->attr.port_num, entry->attr.index,\n\t\tentry->attr.gid.raw);\n\n\tlockdep_assert_held(&table->lock);\n\twrite_lock_irq(&table->rwlock);\n\ttable->data_vec[entry->attr.index] = entry;\n\twrite_unlock_irq(&table->rwlock);\n}\n\nstatic void get_gid_entry(struct ib_gid_table_entry *entry)\n{\n\tkref_get(&entry->kref);\n}\n\nstatic void put_gid_entry(struct ib_gid_table_entry *entry)\n{\n\tkref_put(&entry->kref, schedule_free_gid);\n}\n\nstatic void put_gid_entry_locked(struct ib_gid_table_entry *entry)\n{\n\tkref_put(&entry->kref, free_gid_entry);\n}\n\nstatic int add_roce_gid(struct ib_gid_table_entry *entry)\n{\n\tconst struct ib_gid_attr *attr = &entry->attr;\n\tint ret;\n\n\tif (!attr->ndev) {\n\t\tdev_err(&attr->device->dev, \"%s NULL netdev port=%u index=%u\\n\",\n\t\t\t__func__, attr->port_num, attr->index);\n\t\treturn -EINVAL;\n\t}\n\tif (rdma_cap_roce_gid_table(attr->device, attr->port_num)) {\n\t\tret = attr->device->ops.add_gid(attr, &entry->context);\n\t\tif (ret) {\n\t\t\tdev_err(&attr->device->dev,\n\t\t\t\t\"%s GID add failed port=%u index=%u\\n\",\n\t\t\t\t__func__, attr->port_num, attr->index);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic void del_gid(struct ib_device *ib_dev, u32 port,\n\t\t    struct ib_gid_table *table, int ix)\n{\n\tstruct roce_gid_ndev_storage *ndev_storage;\n\tstruct ib_gid_table_entry *entry;\n\n\tlockdep_assert_held(&table->lock);\n\n\tdev_dbg(&ib_dev->dev, \"%s port=%u index=%d gid %pI6\\n\", __func__, port,\n\t\tix, table->data_vec[ix]->attr.gid.raw);\n\n\twrite_lock_irq(&table->rwlock);\n\tentry = table->data_vec[ix];\n\tentry->state = GID_TABLE_ENTRY_PENDING_DEL;\n\t \n\tif (!rdma_protocol_roce(ib_dev, port))\n\t\ttable->data_vec[ix] = NULL;\n\twrite_unlock_irq(&table->rwlock);\n\n\tif (rdma_cap_roce_gid_table(ib_dev, port))\n\t\tib_dev->ops.del_gid(&entry->attr, &entry->context);\n\n\tndev_storage = entry->ndev_storage;\n\tif (ndev_storage) {\n\t\tentry->ndev_storage = NULL;\n\t\trcu_assign_pointer(entry->attr.ndev, NULL);\n\t\tcall_rcu(&ndev_storage->rcu_head, put_gid_ndev);\n\t}\n\n\tput_gid_entry_locked(entry);\n}\n\n \nstatic int add_modify_gid(struct ib_gid_table *table,\n\t\t\t  const struct ib_gid_attr *attr)\n{\n\tstruct ib_gid_table_entry *entry;\n\tint ret = 0;\n\n\t \n\tif (is_gid_entry_valid(table->data_vec[attr->index]))\n\t\tdel_gid(attr->device, attr->port_num, table, attr->index);\n\n\t \n\tif (rdma_is_zero_gid(&attr->gid))\n\t\treturn 0;\n\n\tentry = alloc_gid_entry(attr);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tif (rdma_protocol_roce(attr->device, attr->port_num)) {\n\t\tret = add_roce_gid(entry);\n\t\tif (ret)\n\t\t\tgoto done;\n\t}\n\n\tstore_gid_entry(table, entry);\n\treturn 0;\n\ndone:\n\tput_gid_entry(entry);\n\treturn ret;\n}\n\n \nstatic int find_gid(struct ib_gid_table *table, const union ib_gid *gid,\n\t\t    const struct ib_gid_attr *val, bool default_gid,\n\t\t    unsigned long mask, int *pempty)\n{\n\tint i = 0;\n\tint found = -1;\n\tint empty = pempty ? -1 : 0;\n\n\twhile (i < table->sz && (found < 0 || empty < 0)) {\n\t\tstruct ib_gid_table_entry *data = table->data_vec[i];\n\t\tstruct ib_gid_attr *attr;\n\t\tint curr_index = i;\n\n\t\ti++;\n\n\t\t \n\t\tif (pempty && empty < 0) {\n\t\t\tif (is_gid_entry_free(data) &&\n\t\t\t    default_gid ==\n\t\t\t\tis_gid_index_default(table, curr_index)) {\n\t\t\t\t \n\t\t\t\tempty = curr_index;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!is_gid_entry_valid(data))\n\t\t\tcontinue;\n\n\t\tif (found >= 0)\n\t\t\tcontinue;\n\n\t\tattr = &data->attr;\n\t\tif (mask & GID_ATTR_FIND_MASK_GID_TYPE &&\n\t\t    attr->gid_type != val->gid_type)\n\t\t\tcontinue;\n\n\t\tif (mask & GID_ATTR_FIND_MASK_GID &&\n\t\t    memcmp(gid, &data->attr.gid, sizeof(*gid)))\n\t\t\tcontinue;\n\n\t\tif (mask & GID_ATTR_FIND_MASK_NETDEV &&\n\t\t    attr->ndev != val->ndev)\n\t\t\tcontinue;\n\n\t\tif (mask & GID_ATTR_FIND_MASK_DEFAULT &&\n\t\t    is_gid_index_default(table, curr_index) != default_gid)\n\t\t\tcontinue;\n\n\t\tfound = curr_index;\n\t}\n\n\tif (pempty)\n\t\t*pempty = empty;\n\n\treturn found;\n}\n\nstatic void make_default_gid(struct  net_device *dev, union ib_gid *gid)\n{\n\tgid->global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL);\n\taddrconf_ifid_eui48(&gid->raw[8], dev);\n}\n\nstatic int __ib_cache_gid_add(struct ib_device *ib_dev, u32 port,\n\t\t\t      union ib_gid *gid, struct ib_gid_attr *attr,\n\t\t\t      unsigned long mask, bool default_gid)\n{\n\tstruct ib_gid_table *table;\n\tint ret = 0;\n\tint empty;\n\tint ix;\n\n\t \n\tif (rdma_is_zero_gid(gid))\n\t\treturn -EINVAL;\n\n\ttable = rdma_gid_table(ib_dev, port);\n\n\tmutex_lock(&table->lock);\n\n\tix = find_gid(table, gid, attr, default_gid, mask, &empty);\n\tif (ix >= 0)\n\t\tgoto out_unlock;\n\n\tif (empty < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_unlock;\n\t}\n\tattr->device = ib_dev;\n\tattr->index = empty;\n\tattr->port_num = port;\n\tattr->gid = *gid;\n\tret = add_modify_gid(table, attr);\n\tif (!ret)\n\t\tdispatch_gid_change_event(ib_dev, port);\n\nout_unlock:\n\tmutex_unlock(&table->lock);\n\tif (ret)\n\t\tpr_warn(\"%s: unable to add gid %pI6 error=%d\\n\",\n\t\t\t__func__, gid->raw, ret);\n\treturn ret;\n}\n\nint ib_cache_gid_add(struct ib_device *ib_dev, u32 port,\n\t\t     union ib_gid *gid, struct ib_gid_attr *attr)\n{\n\tunsigned long mask = GID_ATTR_FIND_MASK_GID |\n\t\t\t     GID_ATTR_FIND_MASK_GID_TYPE |\n\t\t\t     GID_ATTR_FIND_MASK_NETDEV;\n\n\treturn __ib_cache_gid_add(ib_dev, port, gid, attr, mask, false);\n}\n\nstatic int\n_ib_cache_gid_del(struct ib_device *ib_dev, u32 port,\n\t\t  union ib_gid *gid, struct ib_gid_attr *attr,\n\t\t  unsigned long mask, bool default_gid)\n{\n\tstruct ib_gid_table *table;\n\tint ret = 0;\n\tint ix;\n\n\ttable = rdma_gid_table(ib_dev, port);\n\n\tmutex_lock(&table->lock);\n\n\tix = find_gid(table, gid, attr, default_gid, mask, NULL);\n\tif (ix < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tdel_gid(ib_dev, port, table, ix);\n\tdispatch_gid_change_event(ib_dev, port);\n\nout_unlock:\n\tmutex_unlock(&table->lock);\n\tif (ret)\n\t\tpr_debug(\"%s: can't delete gid %pI6 error=%d\\n\",\n\t\t\t __func__, gid->raw, ret);\n\treturn ret;\n}\n\nint ib_cache_gid_del(struct ib_device *ib_dev, u32 port,\n\t\t     union ib_gid *gid, struct ib_gid_attr *attr)\n{\n\tunsigned long mask = GID_ATTR_FIND_MASK_GID\t  |\n\t\t\t     GID_ATTR_FIND_MASK_GID_TYPE |\n\t\t\t     GID_ATTR_FIND_MASK_DEFAULT  |\n\t\t\t     GID_ATTR_FIND_MASK_NETDEV;\n\n\treturn _ib_cache_gid_del(ib_dev, port, gid, attr, mask, false);\n}\n\nint ib_cache_gid_del_all_netdev_gids(struct ib_device *ib_dev, u32 port,\n\t\t\t\t     struct net_device *ndev)\n{\n\tstruct ib_gid_table *table;\n\tint ix;\n\tbool deleted = false;\n\n\ttable = rdma_gid_table(ib_dev, port);\n\n\tmutex_lock(&table->lock);\n\n\tfor (ix = 0; ix < table->sz; ix++) {\n\t\tif (is_gid_entry_valid(table->data_vec[ix]) &&\n\t\t    table->data_vec[ix]->attr.ndev == ndev) {\n\t\t\tdel_gid(ib_dev, port, table, ix);\n\t\t\tdeleted = true;\n\t\t}\n\t}\n\n\tmutex_unlock(&table->lock);\n\n\tif (deleted)\n\t\tdispatch_gid_change_event(ib_dev, port);\n\n\treturn 0;\n}\n\n \nconst struct ib_gid_attr *\nrdma_find_gid_by_port(struct ib_device *ib_dev,\n\t\t      const union ib_gid *gid,\n\t\t      enum ib_gid_type gid_type,\n\t\t      u32 port, struct net_device *ndev)\n{\n\tint local_index;\n\tstruct ib_gid_table *table;\n\tunsigned long mask = GID_ATTR_FIND_MASK_GID |\n\t\t\t     GID_ATTR_FIND_MASK_GID_TYPE;\n\tstruct ib_gid_attr val = {.ndev = ndev, .gid_type = gid_type};\n\tconst struct ib_gid_attr *attr;\n\tunsigned long flags;\n\n\tif (!rdma_is_port_valid(ib_dev, port))\n\t\treturn ERR_PTR(-ENOENT);\n\n\ttable = rdma_gid_table(ib_dev, port);\n\n\tif (ndev)\n\t\tmask |= GID_ATTR_FIND_MASK_NETDEV;\n\n\tread_lock_irqsave(&table->rwlock, flags);\n\tlocal_index = find_gid(table, gid, &val, false, mask, NULL);\n\tif (local_index >= 0) {\n\t\tget_gid_entry(table->data_vec[local_index]);\n\t\tattr = &table->data_vec[local_index]->attr;\n\t\tread_unlock_irqrestore(&table->rwlock, flags);\n\t\treturn attr;\n\t}\n\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn ERR_PTR(-ENOENT);\n}\nEXPORT_SYMBOL(rdma_find_gid_by_port);\n\n \nconst struct ib_gid_attr *rdma_find_gid_by_filter(\n\tstruct ib_device *ib_dev, const union ib_gid *gid, u32 port,\n\tbool (*filter)(const union ib_gid *gid, const struct ib_gid_attr *,\n\t\t       void *),\n\tvoid *context)\n{\n\tconst struct ib_gid_attr *res = ERR_PTR(-ENOENT);\n\tstruct ib_gid_table *table;\n\tunsigned long flags;\n\tunsigned int i;\n\n\tif (!rdma_is_port_valid(ib_dev, port))\n\t\treturn ERR_PTR(-EINVAL);\n\n\ttable = rdma_gid_table(ib_dev, port);\n\n\tread_lock_irqsave(&table->rwlock, flags);\n\tfor (i = 0; i < table->sz; i++) {\n\t\tstruct ib_gid_table_entry *entry = table->data_vec[i];\n\n\t\tif (!is_gid_entry_valid(entry))\n\t\t\tcontinue;\n\n\t\tif (memcmp(gid, &entry->attr.gid, sizeof(*gid)))\n\t\t\tcontinue;\n\n\t\tif (filter(gid, &entry->attr, context)) {\n\t\t\tget_gid_entry(entry);\n\t\t\tres = &entry->attr;\n\t\t\tbreak;\n\t\t}\n\t}\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn res;\n}\n\nstatic struct ib_gid_table *alloc_gid_table(int sz)\n{\n\tstruct ib_gid_table *table = kzalloc(sizeof(*table), GFP_KERNEL);\n\n\tif (!table)\n\t\treturn NULL;\n\n\ttable->data_vec = kcalloc(sz, sizeof(*table->data_vec), GFP_KERNEL);\n\tif (!table->data_vec)\n\t\tgoto err_free_table;\n\n\tmutex_init(&table->lock);\n\n\ttable->sz = sz;\n\trwlock_init(&table->rwlock);\n\treturn table;\n\nerr_free_table:\n\tkfree(table);\n\treturn NULL;\n}\n\nstatic void release_gid_table(struct ib_device *device,\n\t\t\t      struct ib_gid_table *table)\n{\n\tbool leak = false;\n\tint i;\n\n\tif (!table)\n\t\treturn;\n\n\tfor (i = 0; i < table->sz; i++) {\n\t\tif (is_gid_entry_free(table->data_vec[i]))\n\t\t\tcontinue;\n\t\tif (kref_read(&table->data_vec[i]->kref) > 1) {\n\t\t\tdev_err(&device->dev,\n\t\t\t\t\"GID entry ref leak for index %d ref=%u\\n\", i,\n\t\t\t\tkref_read(&table->data_vec[i]->kref));\n\t\t\tleak = true;\n\t\t}\n\t}\n\tif (leak)\n\t\treturn;\n\n\tmutex_destroy(&table->lock);\n\tkfree(table->data_vec);\n\tkfree(table);\n}\n\nstatic void cleanup_gid_table_port(struct ib_device *ib_dev, u32 port,\n\t\t\t\t   struct ib_gid_table *table)\n{\n\tint i;\n\n\tif (!table)\n\t\treturn;\n\n\tmutex_lock(&table->lock);\n\tfor (i = 0; i < table->sz; ++i) {\n\t\tif (is_gid_entry_valid(table->data_vec[i]))\n\t\t\tdel_gid(ib_dev, port, table, i);\n\t}\n\tmutex_unlock(&table->lock);\n}\n\nvoid ib_cache_gid_set_default_gid(struct ib_device *ib_dev, u32 port,\n\t\t\t\t  struct net_device *ndev,\n\t\t\t\t  unsigned long gid_type_mask,\n\t\t\t\t  enum ib_cache_gid_default_mode mode)\n{\n\tunion ib_gid gid = { };\n\tstruct ib_gid_attr gid_attr;\n\tunsigned int gid_type;\n\tunsigned long mask;\n\n\tmask = GID_ATTR_FIND_MASK_GID_TYPE |\n\t       GID_ATTR_FIND_MASK_DEFAULT |\n\t       GID_ATTR_FIND_MASK_NETDEV;\n\tmemset(&gid_attr, 0, sizeof(gid_attr));\n\tgid_attr.ndev = ndev;\n\n\tfor (gid_type = 0; gid_type < IB_GID_TYPE_SIZE; ++gid_type) {\n\t\tif (1UL << gid_type & ~gid_type_mask)\n\t\t\tcontinue;\n\n\t\tgid_attr.gid_type = gid_type;\n\n\t\tif (mode == IB_CACHE_GID_DEFAULT_MODE_SET) {\n\t\t\tmake_default_gid(ndev, &gid);\n\t\t\t__ib_cache_gid_add(ib_dev, port, &gid,\n\t\t\t\t\t   &gid_attr, mask, true);\n\t\t} else if (mode == IB_CACHE_GID_DEFAULT_MODE_DELETE) {\n\t\t\t_ib_cache_gid_del(ib_dev, port, &gid,\n\t\t\t\t\t  &gid_attr, mask, true);\n\t\t}\n\t}\n}\n\nstatic void gid_table_reserve_default(struct ib_device *ib_dev, u32 port,\n\t\t\t\t      struct ib_gid_table *table)\n{\n\tunsigned int i;\n\tunsigned long roce_gid_type_mask;\n\tunsigned int num_default_gids;\n\n\troce_gid_type_mask = roce_gid_type_mask_support(ib_dev, port);\n\tnum_default_gids = hweight_long(roce_gid_type_mask);\n\t \n\tfor (i = 0; i < num_default_gids && i < table->sz; i++)\n\t\ttable->default_gid_indices |= BIT(i);\n}\n\n\nstatic void gid_table_release_one(struct ib_device *ib_dev)\n{\n\tu32 p;\n\n\trdma_for_each_port (ib_dev, p) {\n\t\trelease_gid_table(ib_dev, ib_dev->port_data[p].cache.gid);\n\t\tib_dev->port_data[p].cache.gid = NULL;\n\t}\n}\n\nstatic int _gid_table_setup_one(struct ib_device *ib_dev)\n{\n\tstruct ib_gid_table *table;\n\tu32 rdma_port;\n\n\trdma_for_each_port (ib_dev, rdma_port) {\n\t\ttable = alloc_gid_table(\n\t\t\tib_dev->port_data[rdma_port].immutable.gid_tbl_len);\n\t\tif (!table)\n\t\t\tgoto rollback_table_setup;\n\n\t\tgid_table_reserve_default(ib_dev, rdma_port, table);\n\t\tib_dev->port_data[rdma_port].cache.gid = table;\n\t}\n\treturn 0;\n\nrollback_table_setup:\n\tgid_table_release_one(ib_dev);\n\treturn -ENOMEM;\n}\n\nstatic void gid_table_cleanup_one(struct ib_device *ib_dev)\n{\n\tu32 p;\n\n\trdma_for_each_port (ib_dev, p)\n\t\tcleanup_gid_table_port(ib_dev, p,\n\t\t\t\t       ib_dev->port_data[p].cache.gid);\n}\n\nstatic int gid_table_setup_one(struct ib_device *ib_dev)\n{\n\tint err;\n\n\terr = _gid_table_setup_one(ib_dev);\n\n\tif (err)\n\t\treturn err;\n\n\trdma_roce_rescan_device(ib_dev);\n\n\treturn err;\n}\n\n \nint rdma_query_gid(struct ib_device *device, u32 port_num,\n\t\t   int index, union ib_gid *gid)\n{\n\tstruct ib_gid_table *table;\n\tunsigned long flags;\n\tint res;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\ttable = rdma_gid_table(device, port_num);\n\tread_lock_irqsave(&table->rwlock, flags);\n\n\tif (index < 0 || index >= table->sz) {\n\t\tres = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tif (!is_gid_entry_valid(table->data_vec[index])) {\n\t\tres = -ENOENT;\n\t\tgoto done;\n\t}\n\n\tmemcpy(gid, &table->data_vec[index]->attr.gid, sizeof(*gid));\n\tres = 0;\n\ndone:\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn res;\n}\nEXPORT_SYMBOL(rdma_query_gid);\n\n \nvoid *rdma_read_gid_hw_context(const struct ib_gid_attr *attr)\n{\n\treturn container_of(attr, struct ib_gid_table_entry, attr)->context;\n}\nEXPORT_SYMBOL(rdma_read_gid_hw_context);\n\n \nconst struct ib_gid_attr *rdma_find_gid(struct ib_device *device,\n\t\t\t\t\tconst union ib_gid *gid,\n\t\t\t\t\tenum ib_gid_type gid_type,\n\t\t\t\t\tstruct net_device *ndev)\n{\n\tunsigned long mask = GID_ATTR_FIND_MASK_GID |\n\t\t\t     GID_ATTR_FIND_MASK_GID_TYPE;\n\tstruct ib_gid_attr gid_attr_val = {.ndev = ndev, .gid_type = gid_type};\n\tu32 p;\n\n\tif (ndev)\n\t\tmask |= GID_ATTR_FIND_MASK_NETDEV;\n\n\trdma_for_each_port(device, p) {\n\t\tstruct ib_gid_table *table;\n\t\tunsigned long flags;\n\t\tint index;\n\n\t\ttable = device->port_data[p].cache.gid;\n\t\tread_lock_irqsave(&table->rwlock, flags);\n\t\tindex = find_gid(table, gid, &gid_attr_val, false, mask, NULL);\n\t\tif (index >= 0) {\n\t\t\tconst struct ib_gid_attr *attr;\n\n\t\t\tget_gid_entry(table->data_vec[index]);\n\t\t\tattr = &table->data_vec[index]->attr;\n\t\t\tread_unlock_irqrestore(&table->rwlock, flags);\n\t\t\treturn attr;\n\t\t}\n\t\tread_unlock_irqrestore(&table->rwlock, flags);\n\t}\n\n\treturn ERR_PTR(-ENOENT);\n}\nEXPORT_SYMBOL(rdma_find_gid);\n\nint ib_get_cached_pkey(struct ib_device *device,\n\t\t       u32               port_num,\n\t\t       int               index,\n\t\t       u16              *pkey)\n{\n\tstruct ib_pkey_cache *cache;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\n\tcache = device->port_data[port_num].cache.pkey;\n\n\tif (!cache || index < 0 || index >= cache->table_len)\n\t\tret = -EINVAL;\n\telse\n\t\t*pkey = cache->table[index];\n\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_get_cached_pkey);\n\nvoid ib_get_cached_subnet_prefix(struct ib_device *device, u32 port_num,\n\t\t\t\tu64 *sn_pfx)\n{\n\tunsigned long flags;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\t*sn_pfx = device->port_data[port_num].cache.subnet_prefix;\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n}\nEXPORT_SYMBOL(ib_get_cached_subnet_prefix);\n\nint ib_find_cached_pkey(struct ib_device *device, u32 port_num,\n\t\t\tu16 pkey, u16 *index)\n{\n\tstruct ib_pkey_cache *cache;\n\tunsigned long flags;\n\tint i;\n\tint ret = -ENOENT;\n\tint partial_ix = -1;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\n\tcache = device->port_data[port_num].cache.pkey;\n\tif (!cache) {\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t*index = -1;\n\n\tfor (i = 0; i < cache->table_len; ++i)\n\t\tif ((cache->table[i] & 0x7fff) == (pkey & 0x7fff)) {\n\t\t\tif (cache->table[i] & 0x8000) {\n\t\t\t\t*index = i;\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tpartial_ix = i;\n\t\t\t}\n\t\t}\n\n\tif (ret && partial_ix >= 0) {\n\t\t*index = partial_ix;\n\t\tret = 0;\n\t}\n\nerr:\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_find_cached_pkey);\n\nint ib_find_exact_cached_pkey(struct ib_device *device, u32 port_num,\n\t\t\t      u16 pkey, u16 *index)\n{\n\tstruct ib_pkey_cache *cache;\n\tunsigned long flags;\n\tint i;\n\tint ret = -ENOENT;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\n\tcache = device->port_data[port_num].cache.pkey;\n\tif (!cache) {\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t*index = -1;\n\n\tfor (i = 0; i < cache->table_len; ++i)\n\t\tif (cache->table[i] == pkey) {\n\t\t\t*index = i;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\nerr:\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_find_exact_cached_pkey);\n\nint ib_get_cached_lmc(struct ib_device *device, u32 port_num, u8 *lmc)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\t*lmc = device->port_data[port_num].cache.lmc;\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_get_cached_lmc);\n\nint ib_get_cached_port_state(struct ib_device *device, u32 port_num,\n\t\t\t     enum ib_port_state *port_state)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tread_lock_irqsave(&device->cache_lock, flags);\n\t*port_state = device->port_data[port_num].cache.port_state;\n\tread_unlock_irqrestore(&device->cache_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_get_cached_port_state);\n\n \nconst struct ib_gid_attr *\nrdma_get_gid_attr(struct ib_device *device, u32 port_num, int index)\n{\n\tconst struct ib_gid_attr *attr = ERR_PTR(-ENODATA);\n\tstruct ib_gid_table *table;\n\tunsigned long flags;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn ERR_PTR(-EINVAL);\n\n\ttable = rdma_gid_table(device, port_num);\n\tif (index < 0 || index >= table->sz)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tread_lock_irqsave(&table->rwlock, flags);\n\tif (!is_gid_entry_valid(table->data_vec[index]))\n\t\tgoto done;\n\n\tget_gid_entry(table->data_vec[index]);\n\tattr = &table->data_vec[index]->attr;\ndone:\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn attr;\n}\nEXPORT_SYMBOL(rdma_get_gid_attr);\n\n \nssize_t rdma_query_gid_table(struct ib_device *device,\n\t\t\t     struct ib_uverbs_gid_entry *entries,\n\t\t\t     size_t max_entries)\n{\n\tconst struct ib_gid_attr *gid_attr;\n\tssize_t num_entries = 0, ret;\n\tstruct ib_gid_table *table;\n\tu32 port_num, i;\n\tstruct net_device *ndev;\n\tunsigned long flags;\n\n\trdma_for_each_port(device, port_num) {\n\t\ttable = rdma_gid_table(device, port_num);\n\t\tread_lock_irqsave(&table->rwlock, flags);\n\t\tfor (i = 0; i < table->sz; i++) {\n\t\t\tif (!is_gid_entry_valid(table->data_vec[i]))\n\t\t\t\tcontinue;\n\t\t\tif (num_entries >= max_entries) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tgid_attr = &table->data_vec[i]->attr;\n\n\t\t\tmemcpy(&entries->gid, &gid_attr->gid,\n\t\t\t       sizeof(gid_attr->gid));\n\t\t\tentries->gid_index = gid_attr->index;\n\t\t\tentries->port_num = gid_attr->port_num;\n\t\t\tentries->gid_type = gid_attr->gid_type;\n\t\t\tndev = rcu_dereference_protected(\n\t\t\t\tgid_attr->ndev,\n\t\t\t\tlockdep_is_held(&table->rwlock));\n\t\t\tif (ndev)\n\t\t\t\tentries->netdev_ifindex = ndev->ifindex;\n\n\t\t\tnum_entries++;\n\t\t\tentries++;\n\t\t}\n\t\tread_unlock_irqrestore(&table->rwlock, flags);\n\t}\n\n\treturn num_entries;\nerr:\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(rdma_query_gid_table);\n\n \nvoid rdma_put_gid_attr(const struct ib_gid_attr *attr)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\tcontainer_of(attr, struct ib_gid_table_entry, attr);\n\n\tput_gid_entry(entry);\n}\nEXPORT_SYMBOL(rdma_put_gid_attr);\n\n \nvoid rdma_hold_gid_attr(const struct ib_gid_attr *attr)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\tcontainer_of(attr, struct ib_gid_table_entry, attr);\n\n\tget_gid_entry(entry);\n}\nEXPORT_SYMBOL(rdma_hold_gid_attr);\n\n \nstruct net_device *rdma_read_gid_attr_ndev_rcu(const struct ib_gid_attr *attr)\n{\n\tstruct ib_gid_table_entry *entry =\n\t\t\tcontainer_of(attr, struct ib_gid_table_entry, attr);\n\tstruct ib_device *device = entry->attr.device;\n\tstruct net_device *ndev = ERR_PTR(-EINVAL);\n\tu32 port_num = entry->attr.port_num;\n\tstruct ib_gid_table *table;\n\tunsigned long flags;\n\tbool valid;\n\n\ttable = rdma_gid_table(device, port_num);\n\n\tread_lock_irqsave(&table->rwlock, flags);\n\tvalid = is_gid_entry_valid(table->data_vec[attr->index]);\n\tif (valid) {\n\t\tndev = rcu_dereference(attr->ndev);\n\t\tif (!ndev)\n\t\t\tndev = ERR_PTR(-ENODEV);\n\t}\n\tread_unlock_irqrestore(&table->rwlock, flags);\n\treturn ndev;\n}\nEXPORT_SYMBOL(rdma_read_gid_attr_ndev_rcu);\n\nstatic int get_lower_dev_vlan(struct net_device *lower_dev,\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tu16 *vlan_id = (u16 *)priv->data;\n\n\tif (is_vlan_dev(lower_dev))\n\t\t*vlan_id = vlan_dev_vlan_id(lower_dev);\n\n\t \n\treturn 1;\n}\n\n \nint rdma_read_gid_l2_fields(const struct ib_gid_attr *attr,\n\t\t\t    u16 *vlan_id, u8 *smac)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)vlan_id,\n\t};\n\tstruct net_device *ndev;\n\n\trcu_read_lock();\n\tndev = rcu_dereference(attr->ndev);\n\tif (!ndev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\tif (smac)\n\t\tether_addr_copy(smac, ndev->dev_addr);\n\tif (vlan_id) {\n\t\t*vlan_id = 0xffff;\n\t\tif (is_vlan_dev(ndev)) {\n\t\t\t*vlan_id = vlan_dev_vlan_id(ndev);\n\t\t} else {\n\t\t\t \n\t\t\tnetdev_walk_all_lower_dev_rcu(attr->ndev,\n\t\t\t\t\tget_lower_dev_vlan, &priv);\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn 0;\n}\nEXPORT_SYMBOL(rdma_read_gid_l2_fields);\n\nstatic int config_non_roce_gid_cache(struct ib_device *device,\n\t\t\t\t     u32 port, struct ib_port_attr *tprops)\n{\n\tstruct ib_gid_attr gid_attr = {};\n\tstruct ib_gid_table *table;\n\tint ret = 0;\n\tint i;\n\n\tgid_attr.device = device;\n\tgid_attr.port_num = port;\n\ttable = rdma_gid_table(device, port);\n\n\tmutex_lock(&table->lock);\n\tfor (i = 0; i < tprops->gid_tbl_len; ++i) {\n\t\tif (!device->ops.query_gid)\n\t\t\tcontinue;\n\t\tret = device->ops.query_gid(device, port, i, &gid_attr.gid);\n\t\tif (ret) {\n\t\t\tdev_warn(&device->dev,\n\t\t\t\t \"query_gid failed (%d) for index %d\\n\", ret,\n\t\t\t\t i);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (rdma_protocol_iwarp(device, port)) {\n\t\t\tstruct net_device *ndev;\n\n\t\t\tndev = ib_device_get_netdev(device, port);\n\t\t\tif (!ndev)\n\t\t\t\tcontinue;\n\t\t\tRCU_INIT_POINTER(gid_attr.ndev, ndev);\n\t\t\tdev_put(ndev);\n\t\t}\n\n\t\tgid_attr.index = i;\n\t\ttprops->subnet_prefix =\n\t\t\tbe64_to_cpu(gid_attr.gid.global.subnet_prefix);\n\t\tadd_modify_gid(table, &gid_attr);\n\t}\nerr:\n\tmutex_unlock(&table->lock);\n\treturn ret;\n}\n\nstatic int\nib_cache_update(struct ib_device *device, u32 port, bool update_gids,\n\t\tbool update_pkeys, bool enforce_security)\n{\n\tstruct ib_port_attr       *tprops = NULL;\n\tstruct ib_pkey_cache      *pkey_cache = NULL;\n\tstruct ib_pkey_cache      *old_pkey_cache = NULL;\n\tint                        i;\n\tint                        ret;\n\n\tif (!rdma_is_port_valid(device, port))\n\t\treturn -EINVAL;\n\n\ttprops = kmalloc(sizeof *tprops, GFP_KERNEL);\n\tif (!tprops)\n\t\treturn -ENOMEM;\n\n\tret = ib_query_port(device, port, tprops);\n\tif (ret) {\n\t\tdev_warn(&device->dev, \"ib_query_port failed (%d)\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tif (!rdma_protocol_roce(device, port) && update_gids) {\n\t\tret = config_non_roce_gid_cache(device, port,\n\t\t\t\t\t\ttprops);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tupdate_pkeys &= !!tprops->pkey_tbl_len;\n\n\tif (update_pkeys) {\n\t\tpkey_cache = kmalloc(struct_size(pkey_cache, table,\n\t\t\t\t\t\t tprops->pkey_tbl_len),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (!pkey_cache) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tpkey_cache->table_len = tprops->pkey_tbl_len;\n\n\t\tfor (i = 0; i < pkey_cache->table_len; ++i) {\n\t\t\tret = ib_query_pkey(device, port, i,\n\t\t\t\t\t    pkey_cache->table + i);\n\t\t\tif (ret) {\n\t\t\t\tdev_warn(&device->dev,\n\t\t\t\t\t \"ib_query_pkey failed (%d) for index %d\\n\",\n\t\t\t\t\t ret, i);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t}\n\n\twrite_lock_irq(&device->cache_lock);\n\n\tif (update_pkeys) {\n\t\told_pkey_cache = device->port_data[port].cache.pkey;\n\t\tdevice->port_data[port].cache.pkey = pkey_cache;\n\t}\n\tdevice->port_data[port].cache.lmc = tprops->lmc;\n\tdevice->port_data[port].cache.port_state = tprops->state;\n\n\tdevice->port_data[port].cache.subnet_prefix = tprops->subnet_prefix;\n\twrite_unlock_irq(&device->cache_lock);\n\n\tif (enforce_security)\n\t\tib_security_cache_change(device,\n\t\t\t\t\t port,\n\t\t\t\t\t tprops->subnet_prefix);\n\n\tkfree(old_pkey_cache);\n\tkfree(tprops);\n\treturn 0;\n\nerr:\n\tkfree(pkey_cache);\n\tkfree(tprops);\n\treturn ret;\n}\n\nstatic void ib_cache_event_task(struct work_struct *_work)\n{\n\tstruct ib_update_work *work =\n\t\tcontainer_of(_work, struct ib_update_work, work);\n\tint ret;\n\n\t \n\tret = ib_cache_update(work->event.device, work->event.element.port_num,\n\t\t\t      work->event.event == IB_EVENT_GID_CHANGE,\n\t\t\t      work->event.event == IB_EVENT_PKEY_CHANGE,\n\t\t\t      work->enforce_security);\n\n\t \n\tif (!ret && work->event.event != IB_EVENT_GID_CHANGE)\n\t\tib_dispatch_event_clients(&work->event);\n\n\tkfree(work);\n}\n\nstatic void ib_generic_event_task(struct work_struct *_work)\n{\n\tstruct ib_update_work *work =\n\t\tcontainer_of(_work, struct ib_update_work, work);\n\n\tib_dispatch_event_clients(&work->event);\n\tkfree(work);\n}\n\nstatic bool is_cache_update_event(const struct ib_event *event)\n{\n\treturn (event->event == IB_EVENT_PORT_ERR    ||\n\t\tevent->event == IB_EVENT_PORT_ACTIVE ||\n\t\tevent->event == IB_EVENT_LID_CHANGE  ||\n\t\tevent->event == IB_EVENT_PKEY_CHANGE ||\n\t\tevent->event == IB_EVENT_CLIENT_REREGISTER ||\n\t\tevent->event == IB_EVENT_GID_CHANGE);\n}\n\n \nvoid ib_dispatch_event(const struct ib_event *event)\n{\n\tstruct ib_update_work *work;\n\n\twork = kzalloc(sizeof(*work), GFP_ATOMIC);\n\tif (!work)\n\t\treturn;\n\n\tif (is_cache_update_event(event))\n\t\tINIT_WORK(&work->work, ib_cache_event_task);\n\telse\n\t\tINIT_WORK(&work->work, ib_generic_event_task);\n\n\twork->event = *event;\n\tif (event->event == IB_EVENT_PKEY_CHANGE ||\n\t    event->event == IB_EVENT_GID_CHANGE)\n\t\twork->enforce_security = true;\n\n\tqueue_work(ib_wq, &work->work);\n}\nEXPORT_SYMBOL(ib_dispatch_event);\n\nint ib_cache_setup_one(struct ib_device *device)\n{\n\tu32 p;\n\tint err;\n\n\terr = gid_table_setup_one(device);\n\tif (err)\n\t\treturn err;\n\n\trdma_for_each_port (device, p) {\n\t\terr = ib_cache_update(device, p, true, true, true);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid ib_cache_release_one(struct ib_device *device)\n{\n\tu32 p;\n\n\t \n\trdma_for_each_port (device, p)\n\t\tkfree(device->port_data[p].cache.pkey);\n\n\tgid_table_release_one(device);\n}\n\nvoid ib_cache_cleanup_one(struct ib_device *device)\n{\n\t \n\tflush_workqueue(ib_wq);\n\tgid_table_cleanup_one(device);\n\n\t \n\tflush_workqueue(ib_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}