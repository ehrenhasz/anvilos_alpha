{
  "module_name": "mad.c",
  "hash_id": "42dacbc6ae04d64709481723a8f8820977b7202b3fbe371da4894732f6e671b1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/mad.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/security.h>\n#include <linux/xarray.h>\n#include <rdma/ib_cache.h>\n\n#include \"mad_priv.h\"\n#include \"core_priv.h\"\n#include \"mad_rmpp.h\"\n#include \"smi.h\"\n#include \"opa_smi.h\"\n#include \"agent.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ib_mad.h>\n\n#ifdef CONFIG_TRACEPOINTS\nstatic void create_mad_addr_info(struct ib_mad_send_wr_private *mad_send_wr,\n\t\t\t  struct ib_mad_qp_info *qp_info,\n\t\t\t  struct trace_event_raw_ib_mad_send_template *entry)\n{\n\tstruct ib_ud_wr *wr = &mad_send_wr->send_wr;\n\tstruct rdma_ah_attr attr = {};\n\n\trdma_query_ah(wr->ah, &attr);\n\n\t \n\tentry->sl = attr.sl;\n\tentry->rqpn = wr->remote_qpn;\n\tentry->rqkey = wr->remote_qkey;\n\tentry->dlid = rdma_ah_get_dlid(&attr);\n}\n#endif\n\nstatic int mad_sendq_size = IB_MAD_QP_SEND_SIZE;\nstatic int mad_recvq_size = IB_MAD_QP_RECV_SIZE;\n\nmodule_param_named(send_queue_size, mad_sendq_size, int, 0444);\nMODULE_PARM_DESC(send_queue_size, \"Size of send queue in number of work requests\");\nmodule_param_named(recv_queue_size, mad_recvq_size, int, 0444);\nMODULE_PARM_DESC(recv_queue_size, \"Size of receive queue in number of work requests\");\n\nstatic DEFINE_XARRAY_ALLOC1(ib_mad_clients);\nstatic u32 ib_mad_client_next;\nstatic struct list_head ib_mad_port_list;\n\n \nstatic DEFINE_SPINLOCK(ib_mad_port_list_lock);\n\n \nstatic int method_in_use(struct ib_mad_mgmt_method_table **method,\n\t\t\t struct ib_mad_reg_req *mad_reg_req);\nstatic void remove_mad_reg_req(struct ib_mad_agent_private *priv);\nstatic struct ib_mad_agent_private *find_mad_agent(\n\t\t\t\t\tstruct ib_mad_port_private *port_priv,\n\t\t\t\t\tconst struct ib_mad_hdr *mad);\nstatic int ib_mad_post_receive_mads(struct ib_mad_qp_info *qp_info,\n\t\t\t\t    struct ib_mad_private *mad);\nstatic void cancel_mads(struct ib_mad_agent_private *mad_agent_priv);\nstatic void timeout_sends(struct work_struct *work);\nstatic void local_completions(struct work_struct *work);\nstatic int add_nonoui_reg_req(struct ib_mad_reg_req *mad_reg_req,\n\t\t\t      struct ib_mad_agent_private *agent_priv,\n\t\t\t      u8 mgmt_class);\nstatic int add_oui_reg_req(struct ib_mad_reg_req *mad_reg_req,\n\t\t\t   struct ib_mad_agent_private *agent_priv);\nstatic bool ib_mad_send_error(struct ib_mad_port_private *port_priv,\n\t\t\t      struct ib_wc *wc);\nstatic void ib_mad_send_done(struct ib_cq *cq, struct ib_wc *wc);\n\n \nstatic inline struct ib_mad_port_private *\n__ib_get_mad_port(struct ib_device *device, u32 port_num)\n{\n\tstruct ib_mad_port_private *entry;\n\n\tlist_for_each_entry(entry, &ib_mad_port_list, port_list) {\n\t\tif (entry->device == device && entry->port_num == port_num)\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n \nstatic inline struct ib_mad_port_private *\nib_get_mad_port(struct ib_device *device, u32 port_num)\n{\n\tstruct ib_mad_port_private *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ib_mad_port_list_lock, flags);\n\tentry = __ib_get_mad_port(device, port_num);\n\tspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\n\n\treturn entry;\n}\n\nstatic inline u8 convert_mgmt_class(u8 mgmt_class)\n{\n\t \n\treturn mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE ?\n\t\t0 : mgmt_class;\n}\n\nstatic int get_spl_qp_index(enum ib_qp_type qp_type)\n{\n\tswitch (qp_type) {\n\tcase IB_QPT_SMI:\n\t\treturn 0;\n\tcase IB_QPT_GSI:\n\t\treturn 1;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\nstatic int vendor_class_index(u8 mgmt_class)\n{\n\treturn mgmt_class - IB_MGMT_CLASS_VENDOR_RANGE2_START;\n}\n\nstatic int is_vendor_class(u8 mgmt_class)\n{\n\tif ((mgmt_class < IB_MGMT_CLASS_VENDOR_RANGE2_START) ||\n\t    (mgmt_class > IB_MGMT_CLASS_VENDOR_RANGE2_END))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int is_vendor_oui(char *oui)\n{\n\tif (oui[0] || oui[1] || oui[2])\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int is_vendor_method_in_use(\n\t\tstruct ib_mad_mgmt_vendor_class *vendor_class,\n\t\tstruct ib_mad_reg_req *mad_reg_req)\n{\n\tstruct ib_mad_mgmt_method_table *method;\n\tint i;\n\n\tfor (i = 0; i < MAX_MGMT_OUI; i++) {\n\t\tif (!memcmp(vendor_class->oui[i], mad_reg_req->oui, 3)) {\n\t\t\tmethod = vendor_class->method_table[i];\n\t\t\tif (method) {\n\t\t\t\tif (method_in_use(&method, mad_reg_req))\n\t\t\t\t\treturn 1;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nint ib_response_mad(const struct ib_mad_hdr *hdr)\n{\n\treturn ((hdr->method & IB_MGMT_METHOD_RESP) ||\n\t\t(hdr->method == IB_MGMT_METHOD_TRAP_REPRESS) ||\n\t\t((hdr->mgmt_class == IB_MGMT_CLASS_BM) &&\n\t\t (hdr->attr_mod & IB_BM_ATTR_MOD_RESP)));\n}\nEXPORT_SYMBOL(ib_response_mad);\n\n \nstruct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,\n\t\t\t\t\t   u32 port_num,\n\t\t\t\t\t   enum ib_qp_type qp_type,\n\t\t\t\t\t   struct ib_mad_reg_req *mad_reg_req,\n\t\t\t\t\t   u8 rmpp_version,\n\t\t\t\t\t   ib_mad_send_handler send_handler,\n\t\t\t\t\t   ib_mad_recv_handler recv_handler,\n\t\t\t\t\t   void *context,\n\t\t\t\t\t   u32 registration_flags)\n{\n\tstruct ib_mad_port_private *port_priv;\n\tstruct ib_mad_agent *ret = ERR_PTR(-EINVAL);\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_reg_req *reg_req = NULL;\n\tstruct ib_mad_mgmt_class_table *class;\n\tstruct ib_mad_mgmt_vendor_class_table *vendor;\n\tstruct ib_mad_mgmt_vendor_class *vendor_class;\n\tstruct ib_mad_mgmt_method_table *method;\n\tint ret2, qpn;\n\tu8 mgmt_class, vclass;\n\n\tif ((qp_type == IB_QPT_SMI && !rdma_cap_ib_smi(device, port_num)) ||\n\t    (qp_type == IB_QPT_GSI && !rdma_cap_ib_cm(device, port_num)))\n\t\treturn ERR_PTR(-EPROTONOSUPPORT);\n\n\t \n\tqpn = get_spl_qp_index(qp_type);\n\tif (qpn == -1) {\n\t\tdev_dbg_ratelimited(&device->dev, \"%s: invalid QP Type %d\\n\",\n\t\t\t\t    __func__, qp_type);\n\t\tgoto error1;\n\t}\n\n\tif (rmpp_version && rmpp_version != IB_MGMT_RMPP_VERSION) {\n\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t    \"%s: invalid RMPP Version %u\\n\",\n\t\t\t\t    __func__, rmpp_version);\n\t\tgoto error1;\n\t}\n\n\t \n\tif (mad_reg_req) {\n\t\tif (mad_reg_req->mgmt_class_version >= MAX_MGMT_VERSION) {\n\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t    \"%s: invalid Class Version %u\\n\",\n\t\t\t\t\t    __func__,\n\t\t\t\t\t    mad_reg_req->mgmt_class_version);\n\t\t\tgoto error1;\n\t\t}\n\t\tif (!recv_handler) {\n\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t    \"%s: no recv_handler\\n\", __func__);\n\t\t\tgoto error1;\n\t\t}\n\t\tif (mad_reg_req->mgmt_class >= MAX_MGMT_CLASS) {\n\t\t\t \n\t\t\tif (mad_reg_req->mgmt_class !=\n\t\t\t    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {\n\t\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t\"%s: Invalid Mgmt Class 0x%x\\n\",\n\t\t\t\t\t__func__, mad_reg_req->mgmt_class);\n\t\t\t\tgoto error1;\n\t\t\t}\n\t\t} else if (mad_reg_req->mgmt_class == 0) {\n\t\t\t \n\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t    \"%s: Invalid Mgmt Class 0\\n\",\n\t\t\t\t\t    __func__);\n\t\t\tgoto error1;\n\t\t} else if (is_vendor_class(mad_reg_req->mgmt_class)) {\n\t\t\t \n\t\t\tif (!is_vendor_oui(mad_reg_req->oui)) {\n\t\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t\"%s: No OUI specified for class 0x%x\\n\",\n\t\t\t\t\t__func__,\n\t\t\t\t\tmad_reg_req->mgmt_class);\n\t\t\t\tgoto error1;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (!ib_is_mad_class_rmpp(mad_reg_req->mgmt_class)) {\n\t\t\tif (rmpp_version) {\n\t\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t\"%s: RMPP version for non-RMPP class 0x%x\\n\",\n\t\t\t\t\t__func__, mad_reg_req->mgmt_class);\n\t\t\t\tgoto error1;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (qp_type == IB_QPT_SMI) {\n\t\t\tif ((mad_reg_req->mgmt_class !=\n\t\t\t\t\tIB_MGMT_CLASS_SUBN_LID_ROUTED) &&\n\t\t\t    (mad_reg_req->mgmt_class !=\n\t\t\t\t\tIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {\n\t\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t\"%s: Invalid SM QP type: class 0x%x\\n\",\n\t\t\t\t\t__func__, mad_reg_req->mgmt_class);\n\t\t\t\tgoto error1;\n\t\t\t}\n\t\t} else {\n\t\t\tif ((mad_reg_req->mgmt_class ==\n\t\t\t\t\tIB_MGMT_CLASS_SUBN_LID_ROUTED) ||\n\t\t\t    (mad_reg_req->mgmt_class ==\n\t\t\t\t\tIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {\n\t\t\t\tdev_dbg_ratelimited(&device->dev,\n\t\t\t\t\t\"%s: Invalid GS QP type: class 0x%x\\n\",\n\t\t\t\t\t__func__, mad_reg_req->mgmt_class);\n\t\t\t\tgoto error1;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tif (!send_handler)\n\t\t\tgoto error1;\n\t\tif (registration_flags & IB_MAD_USER_RMPP)\n\t\t\tgoto error1;\n\t}\n\n\t \n\tport_priv = ib_get_mad_port(device, port_num);\n\tif (!port_priv) {\n\t\tdev_dbg_ratelimited(&device->dev, \"%s: Invalid port %u\\n\",\n\t\t\t\t    __func__, port_num);\n\t\tret = ERR_PTR(-ENODEV);\n\t\tgoto error1;\n\t}\n\n\t \n\tif (!port_priv->qp_info[qpn].qp) {\n\t\tdev_dbg_ratelimited(&device->dev, \"%s: QP %d not supported\\n\",\n\t\t\t\t    __func__, qpn);\n\t\tret = ERR_PTR(-EPROTONOSUPPORT);\n\t\tgoto error1;\n\t}\n\n\t \n\tmad_agent_priv = kzalloc(sizeof *mad_agent_priv, GFP_KERNEL);\n\tif (!mad_agent_priv) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto error1;\n\t}\n\n\tif (mad_reg_req) {\n\t\treg_req = kmemdup(mad_reg_req, sizeof *reg_req, GFP_KERNEL);\n\t\tif (!reg_req) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto error3;\n\t\t}\n\t}\n\n\t \n\tmad_agent_priv->qp_info = &port_priv->qp_info[qpn];\n\tmad_agent_priv->reg_req = reg_req;\n\tmad_agent_priv->agent.rmpp_version = rmpp_version;\n\tmad_agent_priv->agent.device = device;\n\tmad_agent_priv->agent.recv_handler = recv_handler;\n\tmad_agent_priv->agent.send_handler = send_handler;\n\tmad_agent_priv->agent.context = context;\n\tmad_agent_priv->agent.qp = port_priv->qp_info[qpn].qp;\n\tmad_agent_priv->agent.port_num = port_num;\n\tmad_agent_priv->agent.flags = registration_flags;\n\tspin_lock_init(&mad_agent_priv->lock);\n\tINIT_LIST_HEAD(&mad_agent_priv->send_list);\n\tINIT_LIST_HEAD(&mad_agent_priv->wait_list);\n\tINIT_LIST_HEAD(&mad_agent_priv->done_list);\n\tINIT_LIST_HEAD(&mad_agent_priv->rmpp_list);\n\tINIT_DELAYED_WORK(&mad_agent_priv->timed_work, timeout_sends);\n\tINIT_LIST_HEAD(&mad_agent_priv->local_list);\n\tINIT_WORK(&mad_agent_priv->local_work, local_completions);\n\trefcount_set(&mad_agent_priv->refcount, 1);\n\tinit_completion(&mad_agent_priv->comp);\n\n\tret2 = ib_mad_agent_security_setup(&mad_agent_priv->agent, qp_type);\n\tif (ret2) {\n\t\tret = ERR_PTR(ret2);\n\t\tgoto error4;\n\t}\n\n\t \n\tret2 = xa_alloc_cyclic(&ib_mad_clients, &mad_agent_priv->agent.hi_tid,\n\t\t\tmad_agent_priv, XA_LIMIT(0, (1 << 24) - 1),\n\t\t\t&ib_mad_client_next, GFP_KERNEL);\n\tif (ret2 < 0) {\n\t\tret = ERR_PTR(ret2);\n\t\tgoto error5;\n\t}\n\n\t \n\tspin_lock_irq(&port_priv->reg_lock);\n\tif (mad_reg_req) {\n\t\tmgmt_class = convert_mgmt_class(mad_reg_req->mgmt_class);\n\t\tif (!is_vendor_class(mgmt_class)) {\n\t\t\tclass = port_priv->version[mad_reg_req->\n\t\t\t\t\t\t   mgmt_class_version].class;\n\t\t\tif (class) {\n\t\t\t\tmethod = class->method_table[mgmt_class];\n\t\t\t\tif (method) {\n\t\t\t\t\tif (method_in_use(&method,\n\t\t\t\t\t\t\t   mad_reg_req))\n\t\t\t\t\t\tgoto error6;\n\t\t\t\t}\n\t\t\t}\n\t\t\tret2 = add_nonoui_reg_req(mad_reg_req, mad_agent_priv,\n\t\t\t\t\t\t  mgmt_class);\n\t\t} else {\n\t\t\t \n\t\t\tvendor = port_priv->version[mad_reg_req->\n\t\t\t\t\t\t    mgmt_class_version].vendor;\n\t\t\tif (vendor) {\n\t\t\t\tvclass = vendor_class_index(mgmt_class);\n\t\t\t\tvendor_class = vendor->vendor_class[vclass];\n\t\t\t\tif (vendor_class) {\n\t\t\t\t\tif (is_vendor_method_in_use(\n\t\t\t\t\t\t\tvendor_class,\n\t\t\t\t\t\t\tmad_reg_req))\n\t\t\t\t\t\tgoto error6;\n\t\t\t\t}\n\t\t\t}\n\t\t\tret2 = add_oui_reg_req(mad_reg_req, mad_agent_priv);\n\t\t}\n\t\tif (ret2) {\n\t\t\tret = ERR_PTR(ret2);\n\t\t\tgoto error6;\n\t\t}\n\t}\n\tspin_unlock_irq(&port_priv->reg_lock);\n\n\ttrace_ib_mad_create_agent(mad_agent_priv);\n\treturn &mad_agent_priv->agent;\nerror6:\n\tspin_unlock_irq(&port_priv->reg_lock);\n\txa_erase(&ib_mad_clients, mad_agent_priv->agent.hi_tid);\nerror5:\n\tib_mad_agent_security_cleanup(&mad_agent_priv->agent);\nerror4:\n\tkfree(reg_req);\nerror3:\n\tkfree(mad_agent_priv);\nerror1:\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_register_mad_agent);\n\nstatic inline void deref_mad_agent(struct ib_mad_agent_private *mad_agent_priv)\n{\n\tif (refcount_dec_and_test(&mad_agent_priv->refcount))\n\t\tcomplete(&mad_agent_priv->comp);\n}\n\nstatic void unregister_mad_agent(struct ib_mad_agent_private *mad_agent_priv)\n{\n\tstruct ib_mad_port_private *port_priv;\n\n\t \n\ttrace_ib_mad_unregister_agent(mad_agent_priv);\n\n\t \n\tcancel_mads(mad_agent_priv);\n\tport_priv = mad_agent_priv->qp_info->port_priv;\n\tcancel_delayed_work(&mad_agent_priv->timed_work);\n\n\tspin_lock_irq(&port_priv->reg_lock);\n\tremove_mad_reg_req(mad_agent_priv);\n\tspin_unlock_irq(&port_priv->reg_lock);\n\txa_erase(&ib_mad_clients, mad_agent_priv->agent.hi_tid);\n\n\tflush_workqueue(port_priv->wq);\n\n\tderef_mad_agent(mad_agent_priv);\n\twait_for_completion(&mad_agent_priv->comp);\n\tib_cancel_rmpp_recvs(mad_agent_priv);\n\n\tib_mad_agent_security_cleanup(&mad_agent_priv->agent);\n\n\tkfree(mad_agent_priv->reg_req);\n\tkfree_rcu(mad_agent_priv, rcu);\n}\n\n \nvoid ib_unregister_mad_agent(struct ib_mad_agent *mad_agent)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\n\tmad_agent_priv = container_of(mad_agent,\n\t\t\t\t      struct ib_mad_agent_private,\n\t\t\t\t      agent);\n\tunregister_mad_agent(mad_agent_priv);\n}\nEXPORT_SYMBOL(ib_unregister_mad_agent);\n\nstatic void dequeue_mad(struct ib_mad_list_head *mad_list)\n{\n\tstruct ib_mad_queue *mad_queue;\n\tunsigned long flags;\n\n\tmad_queue = mad_list->mad_queue;\n\tspin_lock_irqsave(&mad_queue->lock, flags);\n\tlist_del(&mad_list->list);\n\tmad_queue->count--;\n\tspin_unlock_irqrestore(&mad_queue->lock, flags);\n}\n\nstatic void build_smp_wc(struct ib_qp *qp, struct ib_cqe *cqe, u16 slid,\n\t\tu16 pkey_index, u32 port_num, struct ib_wc *wc)\n{\n\tmemset(wc, 0, sizeof *wc);\n\twc->wr_cqe = cqe;\n\twc->status = IB_WC_SUCCESS;\n\twc->opcode = IB_WC_RECV;\n\twc->pkey_index = pkey_index;\n\twc->byte_len = sizeof(struct ib_mad) + sizeof(struct ib_grh);\n\twc->src_qp = IB_QP0;\n\twc->qp = qp;\n\twc->slid = slid;\n\twc->sl = 0;\n\twc->dlid_path_bits = 0;\n\twc->port_num = port_num;\n}\n\nstatic size_t mad_priv_size(const struct ib_mad_private *mp)\n{\n\treturn sizeof(struct ib_mad_private) + mp->mad_size;\n}\n\nstatic struct ib_mad_private *alloc_mad_private(size_t mad_size, gfp_t flags)\n{\n\tsize_t size = sizeof(struct ib_mad_private) + mad_size;\n\tstruct ib_mad_private *ret = kzalloc(size, flags);\n\n\tif (ret)\n\t\tret->mad_size = mad_size;\n\n\treturn ret;\n}\n\nstatic size_t port_mad_size(const struct ib_mad_port_private *port_priv)\n{\n\treturn rdma_max_mad_size(port_priv->device, port_priv->port_num);\n}\n\nstatic size_t mad_priv_dma_size(const struct ib_mad_private *mp)\n{\n\treturn sizeof(struct ib_grh) + mp->mad_size;\n}\n\n \nstatic int handle_outgoing_dr_smp(struct ib_mad_agent_private *mad_agent_priv,\n\t\t\t\t  struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tint ret = 0;\n\tstruct ib_smp *smp = mad_send_wr->send_buf.mad;\n\tstruct opa_smp *opa_smp = (struct opa_smp *)smp;\n\tunsigned long flags;\n\tstruct ib_mad_local_private *local;\n\tstruct ib_mad_private *mad_priv;\n\tstruct ib_mad_port_private *port_priv;\n\tstruct ib_mad_agent_private *recv_mad_agent = NULL;\n\tstruct ib_device *device = mad_agent_priv->agent.device;\n\tu32 port_num;\n\tstruct ib_wc mad_wc;\n\tstruct ib_ud_wr *send_wr = &mad_send_wr->send_wr;\n\tsize_t mad_size = port_mad_size(mad_agent_priv->qp_info->port_priv);\n\tu16 out_mad_pkey_index = 0;\n\tu16 drslid;\n\tbool opa = rdma_cap_opa_mad(mad_agent_priv->qp_info->port_priv->device,\n\t\t\t\t    mad_agent_priv->qp_info->port_priv->port_num);\n\n\tif (rdma_cap_ib_switch(device) &&\n\t    smp->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\n\t\tport_num = send_wr->port_num;\n\telse\n\t\tport_num = mad_agent_priv->agent.port_num;\n\n\t \n\tif (opa && smp->class_version == OPA_SM_CLASS_VERSION) {\n\t\tu32 opa_drslid;\n\n\t\ttrace_ib_mad_handle_out_opa_smi(opa_smp);\n\n\t\tif ((opa_get_smp_direction(opa_smp)\n\t\t     ? opa_smp->route.dr.dr_dlid : opa_smp->route.dr.dr_slid) ==\n\t\t     OPA_LID_PERMISSIVE &&\n\t\t     opa_smi_handle_dr_smp_send(opa_smp,\n\t\t\t\t\t\trdma_cap_ib_switch(device),\n\t\t\t\t\t\tport_num) == IB_SMI_DISCARD) {\n\t\t\tret = -EINVAL;\n\t\t\tdev_err(&device->dev, \"OPA Invalid directed route\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\topa_drslid = be32_to_cpu(opa_smp->route.dr.dr_slid);\n\t\tif (opa_drslid != be32_to_cpu(OPA_LID_PERMISSIVE) &&\n\t\t    opa_drslid & 0xffff0000) {\n\t\t\tret = -EINVAL;\n\t\t\tdev_err(&device->dev, \"OPA Invalid dr_slid 0x%x\\n\",\n\t\t\t       opa_drslid);\n\t\t\tgoto out;\n\t\t}\n\t\tdrslid = (u16)(opa_drslid & 0x0000ffff);\n\n\t\t \n\t\tif (opa_smi_check_local_smp(opa_smp, device) == IB_SMI_DISCARD &&\n\t\t    opa_smi_check_local_returning_smp(opa_smp, device) == IB_SMI_DISCARD)\n\t\t\tgoto out;\n\t} else {\n\t\ttrace_ib_mad_handle_out_ib_smi(smp);\n\n\t\tif ((ib_get_smp_direction(smp) ? smp->dr_dlid : smp->dr_slid) ==\n\t\t     IB_LID_PERMISSIVE &&\n\t\t     smi_handle_dr_smp_send(smp, rdma_cap_ib_switch(device), port_num) ==\n\t\t     IB_SMI_DISCARD) {\n\t\t\tret = -EINVAL;\n\t\t\tdev_err(&device->dev, \"Invalid directed route\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tdrslid = be16_to_cpu(smp->dr_slid);\n\n\t\t \n\t\tif (smi_check_local_smp(smp, device) == IB_SMI_DISCARD &&\n\t\t    smi_check_local_returning_smp(smp, device) == IB_SMI_DISCARD)\n\t\t\tgoto out;\n\t}\n\n\tlocal = kmalloc(sizeof *local, GFP_ATOMIC);\n\tif (!local) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tlocal->mad_priv = NULL;\n\tlocal->recv_mad_agent = NULL;\n\tmad_priv = alloc_mad_private(mad_size, GFP_ATOMIC);\n\tif (!mad_priv) {\n\t\tret = -ENOMEM;\n\t\tkfree(local);\n\t\tgoto out;\n\t}\n\n\tbuild_smp_wc(mad_agent_priv->agent.qp,\n\t\t     send_wr->wr.wr_cqe, drslid,\n\t\t     send_wr->pkey_index,\n\t\t     send_wr->port_num, &mad_wc);\n\n\tif (opa && smp->base_version == OPA_MGMT_BASE_VERSION) {\n\t\tmad_wc.byte_len = mad_send_wr->send_buf.hdr_len\n\t\t\t\t\t+ mad_send_wr->send_buf.data_len\n\t\t\t\t\t+ sizeof(struct ib_grh);\n\t}\n\n\t \n\tret = device->ops.process_mad(device, 0, port_num, &mad_wc, NULL,\n\t\t\t\t      (const struct ib_mad *)smp,\n\t\t\t\t      (struct ib_mad *)mad_priv->mad, &mad_size,\n\t\t\t\t      &out_mad_pkey_index);\n\tswitch (ret) {\n\tcase IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_REPLY:\n\t\tif (ib_response_mad((const struct ib_mad_hdr *)mad_priv->mad) &&\n\t\t    mad_agent_priv->agent.recv_handler) {\n\t\t\tlocal->mad_priv = mad_priv;\n\t\t\tlocal->recv_mad_agent = mad_agent_priv;\n\t\t\t \n\t\t\trefcount_inc(&mad_agent_priv->refcount);\n\t\t} else\n\t\t\tkfree(mad_priv);\n\t\tbreak;\n\tcase IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_CONSUMED:\n\t\tkfree(mad_priv);\n\t\tbreak;\n\tcase IB_MAD_RESULT_SUCCESS:\n\t\t \n\t\tport_priv = ib_get_mad_port(mad_agent_priv->agent.device,\n\t\t\t\t\t    mad_agent_priv->agent.port_num);\n\t\tif (port_priv) {\n\t\t\tmemcpy(mad_priv->mad, smp, mad_priv->mad_size);\n\t\t\trecv_mad_agent = find_mad_agent(port_priv,\n\t\t\t\t\t\t        (const struct ib_mad_hdr *)mad_priv->mad);\n\t\t}\n\t\tif (!port_priv || !recv_mad_agent) {\n\t\t\t \n\t\t\tkfree(mad_priv);\n\t\t\tbreak;\n\t\t}\n\t\tlocal->mad_priv = mad_priv;\n\t\tlocal->recv_mad_agent = recv_mad_agent;\n\t\tbreak;\n\tdefault:\n\t\tkfree(mad_priv);\n\t\tkfree(local);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlocal->mad_send_wr = mad_send_wr;\n\tif (opa) {\n\t\tlocal->mad_send_wr->send_wr.pkey_index = out_mad_pkey_index;\n\t\tlocal->return_wc_byte_len = mad_size;\n\t}\n\t \n\trefcount_inc(&mad_agent_priv->refcount);\n\t \n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\tlist_add_tail(&local->completion_list, &mad_agent_priv->local_list);\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\tqueue_work(mad_agent_priv->qp_info->port_priv->wq,\n\t\t   &mad_agent_priv->local_work);\n\tret = 1;\nout:\n\treturn ret;\n}\n\nstatic int get_pad_size(int hdr_len, int data_len, size_t mad_size)\n{\n\tint seg_size, pad;\n\n\tseg_size = mad_size - hdr_len;\n\tif (data_len && seg_size) {\n\t\tpad = seg_size - data_len % seg_size;\n\t\treturn pad == seg_size ? 0 : pad;\n\t} else\n\t\treturn seg_size;\n}\n\nstatic void free_send_rmpp_list(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tstruct ib_rmpp_segment *s, *t;\n\n\tlist_for_each_entry_safe(s, t, &mad_send_wr->rmpp_list, list) {\n\t\tlist_del(&s->list);\n\t\tkfree(s);\n\t}\n}\n\nstatic int alloc_send_rmpp_list(struct ib_mad_send_wr_private *send_wr,\n\t\t\t\tsize_t mad_size, gfp_t gfp_mask)\n{\n\tstruct ib_mad_send_buf *send_buf = &send_wr->send_buf;\n\tstruct ib_rmpp_mad *rmpp_mad = send_buf->mad;\n\tstruct ib_rmpp_segment *seg = NULL;\n\tint left, seg_size, pad;\n\n\tsend_buf->seg_size = mad_size - send_buf->hdr_len;\n\tsend_buf->seg_rmpp_size = mad_size - IB_MGMT_RMPP_HDR;\n\tseg_size = send_buf->seg_size;\n\tpad = send_wr->pad;\n\n\t \n\tfor (left = send_buf->data_len + pad; left > 0; left -= seg_size) {\n\t\tseg = kmalloc(sizeof(*seg) + seg_size, gfp_mask);\n\t\tif (!seg) {\n\t\t\tfree_send_rmpp_list(send_wr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tseg->num = ++send_buf->seg_count;\n\t\tlist_add_tail(&seg->list, &send_wr->rmpp_list);\n\t}\n\n\t \n\tif (pad)\n\t\tmemset(seg->data + seg_size - pad, 0, pad);\n\n\trmpp_mad->rmpp_hdr.rmpp_version = send_wr->mad_agent_priv->\n\t\t\t\t\t  agent.rmpp_version;\n\trmpp_mad->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_DATA;\n\tib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);\n\n\tsend_wr->cur_seg = container_of(send_wr->rmpp_list.next,\n\t\t\t\t\tstruct ib_rmpp_segment, list);\n\tsend_wr->last_ack_seg = send_wr->cur_seg;\n\treturn 0;\n}\n\nint ib_mad_kernel_rmpp_agent(const struct ib_mad_agent *agent)\n{\n\treturn agent->rmpp_version && !(agent->flags & IB_MAD_USER_RMPP);\n}\nEXPORT_SYMBOL(ib_mad_kernel_rmpp_agent);\n\nstruct ib_mad_send_buf *ib_create_send_mad(struct ib_mad_agent *mad_agent,\n\t\t\t\t\t   u32 remote_qpn, u16 pkey_index,\n\t\t\t\t\t   int rmpp_active, int hdr_len,\n\t\t\t\t\t   int data_len, gfp_t gfp_mask,\n\t\t\t\t\t   u8 base_version)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tint pad, message_size, ret, size;\n\tvoid *buf;\n\tsize_t mad_size;\n\tbool opa;\n\n\tmad_agent_priv = container_of(mad_agent, struct ib_mad_agent_private,\n\t\t\t\t      agent);\n\n\topa = rdma_cap_opa_mad(mad_agent->device, mad_agent->port_num);\n\n\tif (opa && base_version == OPA_MGMT_BASE_VERSION)\n\t\tmad_size = sizeof(struct opa_mad);\n\telse\n\t\tmad_size = sizeof(struct ib_mad);\n\n\tpad = get_pad_size(hdr_len, data_len, mad_size);\n\tmessage_size = hdr_len + data_len + pad;\n\n\tif (ib_mad_kernel_rmpp_agent(mad_agent)) {\n\t\tif (!rmpp_active && message_size > mad_size)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else\n\t\tif (rmpp_active || message_size > mad_size)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\tsize = rmpp_active ? hdr_len : mad_size;\n\tbuf = kzalloc(sizeof *mad_send_wr + size, gfp_mask);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmad_send_wr = buf + size;\n\tINIT_LIST_HEAD(&mad_send_wr->rmpp_list);\n\tmad_send_wr->send_buf.mad = buf;\n\tmad_send_wr->send_buf.hdr_len = hdr_len;\n\tmad_send_wr->send_buf.data_len = data_len;\n\tmad_send_wr->pad = pad;\n\n\tmad_send_wr->mad_agent_priv = mad_agent_priv;\n\tmad_send_wr->sg_list[0].length = hdr_len;\n\tmad_send_wr->sg_list[0].lkey = mad_agent->qp->pd->local_dma_lkey;\n\n\t \n\tif (opa && base_version == OPA_MGMT_BASE_VERSION &&\n\t    data_len < mad_size - hdr_len)\n\t\tmad_send_wr->sg_list[1].length = data_len;\n\telse\n\t\tmad_send_wr->sg_list[1].length = mad_size - hdr_len;\n\n\tmad_send_wr->sg_list[1].lkey = mad_agent->qp->pd->local_dma_lkey;\n\n\tmad_send_wr->mad_list.cqe.done = ib_mad_send_done;\n\n\tmad_send_wr->send_wr.wr.wr_cqe = &mad_send_wr->mad_list.cqe;\n\tmad_send_wr->send_wr.wr.sg_list = mad_send_wr->sg_list;\n\tmad_send_wr->send_wr.wr.num_sge = 2;\n\tmad_send_wr->send_wr.wr.opcode = IB_WR_SEND;\n\tmad_send_wr->send_wr.wr.send_flags = IB_SEND_SIGNALED;\n\tmad_send_wr->send_wr.remote_qpn = remote_qpn;\n\tmad_send_wr->send_wr.remote_qkey = IB_QP_SET_QKEY;\n\tmad_send_wr->send_wr.pkey_index = pkey_index;\n\n\tif (rmpp_active) {\n\t\tret = alloc_send_rmpp_list(mad_send_wr, mad_size, gfp_mask);\n\t\tif (ret) {\n\t\t\tkfree(buf);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\n\tmad_send_wr->send_buf.mad_agent = mad_agent;\n\trefcount_inc(&mad_agent_priv->refcount);\n\treturn &mad_send_wr->send_buf;\n}\nEXPORT_SYMBOL(ib_create_send_mad);\n\nint ib_get_mad_data_offset(u8 mgmt_class)\n{\n\tif (mgmt_class == IB_MGMT_CLASS_SUBN_ADM)\n\t\treturn IB_MGMT_SA_HDR;\n\telse if ((mgmt_class == IB_MGMT_CLASS_DEVICE_MGMT) ||\n\t\t (mgmt_class == IB_MGMT_CLASS_DEVICE_ADM) ||\n\t\t (mgmt_class == IB_MGMT_CLASS_BIS))\n\t\treturn IB_MGMT_DEVICE_HDR;\n\telse if ((mgmt_class >= IB_MGMT_CLASS_VENDOR_RANGE2_START) &&\n\t\t (mgmt_class <= IB_MGMT_CLASS_VENDOR_RANGE2_END))\n\t\treturn IB_MGMT_VENDOR_HDR;\n\telse\n\t\treturn IB_MGMT_MAD_HDR;\n}\nEXPORT_SYMBOL(ib_get_mad_data_offset);\n\nint ib_is_mad_class_rmpp(u8 mgmt_class)\n{\n\tif ((mgmt_class == IB_MGMT_CLASS_SUBN_ADM) ||\n\t    (mgmt_class == IB_MGMT_CLASS_DEVICE_MGMT) ||\n\t    (mgmt_class == IB_MGMT_CLASS_DEVICE_ADM) ||\n\t    (mgmt_class == IB_MGMT_CLASS_BIS) ||\n\t    ((mgmt_class >= IB_MGMT_CLASS_VENDOR_RANGE2_START) &&\n\t     (mgmt_class <= IB_MGMT_CLASS_VENDOR_RANGE2_END)))\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL(ib_is_mad_class_rmpp);\n\nvoid *ib_get_rmpp_segment(struct ib_mad_send_buf *send_buf, int seg_num)\n{\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tstruct list_head *list;\n\n\tmad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private,\n\t\t\t\t   send_buf);\n\tlist = &mad_send_wr->cur_seg->list;\n\n\tif (mad_send_wr->cur_seg->num < seg_num) {\n\t\tlist_for_each_entry(mad_send_wr->cur_seg, list, list)\n\t\t\tif (mad_send_wr->cur_seg->num == seg_num)\n\t\t\t\tbreak;\n\t} else if (mad_send_wr->cur_seg->num > seg_num) {\n\t\tlist_for_each_entry_reverse(mad_send_wr->cur_seg, list, list)\n\t\t\tif (mad_send_wr->cur_seg->num == seg_num)\n\t\t\t\tbreak;\n\t}\n\treturn mad_send_wr->cur_seg->data;\n}\nEXPORT_SYMBOL(ib_get_rmpp_segment);\n\nstatic inline void *ib_get_payload(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tif (mad_send_wr->send_buf.seg_count)\n\t\treturn ib_get_rmpp_segment(&mad_send_wr->send_buf,\n\t\t\t\t\t   mad_send_wr->seg_num);\n\telse\n\t\treturn mad_send_wr->send_buf.mad +\n\t\t       mad_send_wr->send_buf.hdr_len;\n}\n\nvoid ib_free_send_mad(struct ib_mad_send_buf *send_buf)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\n\tmad_agent_priv = container_of(send_buf->mad_agent,\n\t\t\t\t      struct ib_mad_agent_private, agent);\n\tmad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private,\n\t\t\t\t   send_buf);\n\n\tfree_send_rmpp_list(mad_send_wr);\n\tkfree(send_buf->mad);\n\tderef_mad_agent(mad_agent_priv);\n}\nEXPORT_SYMBOL(ib_free_send_mad);\n\nint ib_send_mad(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tstruct ib_mad_qp_info *qp_info;\n\tstruct list_head *list;\n\tstruct ib_mad_agent *mad_agent;\n\tstruct ib_sge *sge;\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tqp_info = mad_send_wr->mad_agent_priv->qp_info;\n\tmad_send_wr->mad_list.mad_queue = &qp_info->send_queue;\n\tmad_send_wr->mad_list.cqe.done = ib_mad_send_done;\n\tmad_send_wr->send_wr.wr.wr_cqe = &mad_send_wr->mad_list.cqe;\n\n\tmad_agent = mad_send_wr->send_buf.mad_agent;\n\tsge = mad_send_wr->sg_list;\n\tsge[0].addr = ib_dma_map_single(mad_agent->device,\n\t\t\t\t\tmad_send_wr->send_buf.mad,\n\t\t\t\t\tsge[0].length,\n\t\t\t\t\tDMA_TO_DEVICE);\n\tif (unlikely(ib_dma_mapping_error(mad_agent->device, sge[0].addr)))\n\t\treturn -ENOMEM;\n\n\tmad_send_wr->header_mapping = sge[0].addr;\n\n\tsge[1].addr = ib_dma_map_single(mad_agent->device,\n\t\t\t\t\tib_get_payload(mad_send_wr),\n\t\t\t\t\tsge[1].length,\n\t\t\t\t\tDMA_TO_DEVICE);\n\tif (unlikely(ib_dma_mapping_error(mad_agent->device, sge[1].addr))) {\n\t\tib_dma_unmap_single(mad_agent->device,\n\t\t\t\t    mad_send_wr->header_mapping,\n\t\t\t\t    sge[0].length, DMA_TO_DEVICE);\n\t\treturn -ENOMEM;\n\t}\n\tmad_send_wr->payload_mapping = sge[1].addr;\n\n\tspin_lock_irqsave(&qp_info->send_queue.lock, flags);\n\tif (qp_info->send_queue.count < qp_info->send_queue.max_active) {\n\t\ttrace_ib_mad_ib_send_mad(mad_send_wr, qp_info);\n\t\tret = ib_post_send(mad_agent->qp, &mad_send_wr->send_wr.wr,\n\t\t\t\t   NULL);\n\t\tlist = &qp_info->send_queue.list;\n\t} else {\n\t\tret = 0;\n\t\tlist = &qp_info->overflow_list;\n\t}\n\n\tif (!ret) {\n\t\tqp_info->send_queue.count++;\n\t\tlist_add_tail(&mad_send_wr->mad_list.list, list);\n\t}\n\tspin_unlock_irqrestore(&qp_info->send_queue.lock, flags);\n\tif (ret) {\n\t\tib_dma_unmap_single(mad_agent->device,\n\t\t\t\t    mad_send_wr->header_mapping,\n\t\t\t\t    sge[0].length, DMA_TO_DEVICE);\n\t\tib_dma_unmap_single(mad_agent->device,\n\t\t\t\t    mad_send_wr->payload_mapping,\n\t\t\t\t    sge[1].length, DMA_TO_DEVICE);\n\t}\n\treturn ret;\n}\n\n \nint ib_post_send_mad(struct ib_mad_send_buf *send_buf,\n\t\t     struct ib_mad_send_buf **bad_send_buf)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_buf *next_send_buf;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\n\t \n\tfor (; send_buf; send_buf = next_send_buf) {\n\t\tmad_send_wr = container_of(send_buf,\n\t\t\t\t\t   struct ib_mad_send_wr_private,\n\t\t\t\t\t   send_buf);\n\t\tmad_agent_priv = mad_send_wr->mad_agent_priv;\n\n\t\tret = ib_mad_enforce_security(mad_agent_priv,\n\t\t\t\t\t      mad_send_wr->send_wr.pkey_index);\n\t\tif (ret)\n\t\t\tgoto error;\n\n\t\tif (!send_buf->mad_agent->send_handler ||\n\t\t    (send_buf->timeout_ms &&\n\t\t     !send_buf->mad_agent->recv_handler)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (!ib_is_mad_class_rmpp(((struct ib_mad_hdr *) send_buf->mad)->mgmt_class)) {\n\t\t\tif (mad_agent_priv->agent.rmpp_version) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tnext_send_buf = send_buf->next;\n\t\tmad_send_wr->send_wr.ah = send_buf->ah;\n\n\t\tif (((struct ib_mad_hdr *) send_buf->mad)->mgmt_class ==\n\t\t    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {\n\t\t\tret = handle_outgoing_dr_smp(mad_agent_priv,\n\t\t\t\t\t\t     mad_send_wr);\n\t\t\tif (ret < 0)\t\t \n\t\t\t\tgoto error;\n\t\t\telse if (ret == 1)\t \n\t\t\t\tcontinue;\n\t\t}\n\n\t\tmad_send_wr->tid = ((struct ib_mad_hdr *) send_buf->mad)->tid;\n\t\t \n\t\tmad_send_wr->timeout = msecs_to_jiffies(send_buf->timeout_ms);\n\t\tmad_send_wr->max_retries = send_buf->retries;\n\t\tmad_send_wr->retries_left = send_buf->retries;\n\t\tsend_buf->retries = 0;\n\t\t \n\t\tmad_send_wr->refcount = 1 + (mad_send_wr->timeout > 0);\n\t\tmad_send_wr->status = IB_WC_SUCCESS;\n\n\t\t \n\t\trefcount_inc(&mad_agent_priv->refcount);\n\t\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\t\tlist_add_tail(&mad_send_wr->agent_list,\n\t\t\t      &mad_agent_priv->send_list);\n\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\n\t\tif (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {\n\t\t\tret = ib_send_rmpp_mad(mad_send_wr);\n\t\t\tif (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)\n\t\t\t\tret = ib_send_mad(mad_send_wr);\n\t\t} else\n\t\t\tret = ib_send_mad(mad_send_wr);\n\t\tif (ret < 0) {\n\t\t\t \n\t\t\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\t\t\tlist_del(&mad_send_wr->agent_list);\n\t\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\t\t\tderef_mad_agent(mad_agent_priv);\n\t\t\tgoto error;\n\t\t}\n\t}\n\treturn 0;\nerror:\n\tif (bad_send_buf)\n\t\t*bad_send_buf = send_buf;\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_post_send_mad);\n\n \nvoid ib_free_recv_mad(struct ib_mad_recv_wc *mad_recv_wc)\n{\n\tstruct ib_mad_recv_buf *mad_recv_buf, *temp_recv_buf;\n\tstruct ib_mad_private_header *mad_priv_hdr;\n\tstruct ib_mad_private *priv;\n\tstruct list_head free_list;\n\n\tINIT_LIST_HEAD(&free_list);\n\tlist_splice_init(&mad_recv_wc->rmpp_list, &free_list);\n\n\tlist_for_each_entry_safe(mad_recv_buf, temp_recv_buf,\n\t\t\t\t\t&free_list, list) {\n\t\tmad_recv_wc = container_of(mad_recv_buf, struct ib_mad_recv_wc,\n\t\t\t\t\t   recv_buf);\n\t\tmad_priv_hdr = container_of(mad_recv_wc,\n\t\t\t\t\t    struct ib_mad_private_header,\n\t\t\t\t\t    recv_wc);\n\t\tpriv = container_of(mad_priv_hdr, struct ib_mad_private,\n\t\t\t\t    header);\n\t\tkfree(priv);\n\t}\n}\nEXPORT_SYMBOL(ib_free_recv_mad);\n\nstatic int method_in_use(struct ib_mad_mgmt_method_table **method,\n\t\t\t struct ib_mad_reg_req *mad_reg_req)\n{\n\tint i;\n\n\tfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS) {\n\t\tif ((*method)->agent[i]) {\n\t\t\tpr_err(\"Method %d already in use\\n\", i);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int allocate_method_table(struct ib_mad_mgmt_method_table **method)\n{\n\t \n\t*method = kzalloc(sizeof **method, GFP_ATOMIC);\n\treturn (*method) ? 0 : (-ENOMEM);\n}\n\n \nstatic int check_method_table(struct ib_mad_mgmt_method_table *method)\n{\n\tint i;\n\n\tfor (i = 0; i < IB_MGMT_MAX_METHODS; i++)\n\t\tif (method->agent[i])\n\t\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic int check_class_table(struct ib_mad_mgmt_class_table *class)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_MGMT_CLASS; i++)\n\t\tif (class->method_table[i])\n\t\t\treturn 1;\n\treturn 0;\n}\n\nstatic int check_vendor_class(struct ib_mad_mgmt_vendor_class *vendor_class)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_MGMT_OUI; i++)\n\t\tif (vendor_class->method_table[i])\n\t\t\treturn 1;\n\treturn 0;\n}\n\nstatic int find_vendor_oui(struct ib_mad_mgmt_vendor_class *vendor_class,\n\t\t\t   const char *oui)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_MGMT_OUI; i++)\n\t\t \n\t\tif (!memcmp(vendor_class->oui[i], oui, 3))\n\t\t\treturn i;\n\n\treturn -1;\n}\n\nstatic int check_vendor_table(struct ib_mad_mgmt_vendor_class_table *vendor)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_MGMT_VENDOR_RANGE2; i++)\n\t\tif (vendor->vendor_class[i])\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void remove_methods_mad_agent(struct ib_mad_mgmt_method_table *method,\n\t\t\t\t     struct ib_mad_agent_private *agent)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < IB_MGMT_MAX_METHODS; i++)\n\t\tif (method->agent[i] == agent)\n\t\t\tmethod->agent[i] = NULL;\n}\n\nstatic int add_nonoui_reg_req(struct ib_mad_reg_req *mad_reg_req,\n\t\t\t      struct ib_mad_agent_private *agent_priv,\n\t\t\t      u8 mgmt_class)\n{\n\tstruct ib_mad_port_private *port_priv;\n\tstruct ib_mad_mgmt_class_table **class;\n\tstruct ib_mad_mgmt_method_table **method;\n\tint i, ret;\n\n\tport_priv = agent_priv->qp_info->port_priv;\n\tclass = &port_priv->version[mad_reg_req->mgmt_class_version].class;\n\tif (!*class) {\n\t\t \n\t\t*class = kzalloc(sizeof **class, GFP_ATOMIC);\n\t\tif (!*class) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto error1;\n\t\t}\n\n\t\t \n\t\tmethod = &(*class)->method_table[mgmt_class];\n\t\tif ((ret = allocate_method_table(method)))\n\t\t\tgoto error2;\n\t} else {\n\t\tmethod = &(*class)->method_table[mgmt_class];\n\t\tif (!*method) {\n\t\t\t \n\t\t\tif ((ret = allocate_method_table(method)))\n\t\t\t\tgoto error1;\n\t\t}\n\t}\n\n\t \n\tif (method_in_use(method, mad_reg_req))\n\t\tgoto error3;\n\n\t \n\tfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)\n\t\t(*method)->agent[i] = agent_priv;\n\n\treturn 0;\n\nerror3:\n\t \n\tremove_methods_mad_agent(*method, agent_priv);\n\t \n\tif (!check_method_table(*method)) {\n\t\t \n\t\tkfree(*method);\n\t\t*method = NULL;\n\t}\n\tret = -EINVAL;\n\tgoto error1;\nerror2:\n\tkfree(*class);\n\t*class = NULL;\nerror1:\n\treturn ret;\n}\n\nstatic int add_oui_reg_req(struct ib_mad_reg_req *mad_reg_req,\n\t\t\t   struct ib_mad_agent_private *agent_priv)\n{\n\tstruct ib_mad_port_private *port_priv;\n\tstruct ib_mad_mgmt_vendor_class_table **vendor_table;\n\tstruct ib_mad_mgmt_vendor_class_table *vendor = NULL;\n\tstruct ib_mad_mgmt_vendor_class *vendor_class = NULL;\n\tstruct ib_mad_mgmt_method_table **method;\n\tint i, ret = -ENOMEM;\n\tu8 vclass;\n\n\t \n\tvclass = vendor_class_index(mad_reg_req->mgmt_class);\n\tport_priv = agent_priv->qp_info->port_priv;\n\tvendor_table = &port_priv->version[\n\t\t\t\tmad_reg_req->mgmt_class_version].vendor;\n\tif (!*vendor_table) {\n\t\t \n\t\tvendor = kzalloc(sizeof *vendor, GFP_ATOMIC);\n\t\tif (!vendor)\n\t\t\tgoto error1;\n\n\t\t*vendor_table = vendor;\n\t}\n\tif (!(*vendor_table)->vendor_class[vclass]) {\n\t\t \n\t\tvendor_class = kzalloc(sizeof *vendor_class, GFP_ATOMIC);\n\t\tif (!vendor_class)\n\t\t\tgoto error2;\n\n\t\t(*vendor_table)->vendor_class[vclass] = vendor_class;\n\t}\n\tfor (i = 0; i < MAX_MGMT_OUI; i++) {\n\t\t \n\t\tif (!memcmp((*vendor_table)->vendor_class[vclass]->oui[i],\n\t\t\t    mad_reg_req->oui, 3)) {\n\t\t\tmethod = &(*vendor_table)->vendor_class[\n\t\t\t\t\t\tvclass]->method_table[i];\n\t\t\tif (!*method)\n\t\t\t\tgoto error3;\n\t\t\tgoto check_in_use;\n\t\t}\n\t}\n\tfor (i = 0; i < MAX_MGMT_OUI; i++) {\n\t\t \n\t\tif (!is_vendor_oui((*vendor_table)->vendor_class[\n\t\t\t\tvclass]->oui[i])) {\n\t\t\tmethod = &(*vendor_table)->vendor_class[\n\t\t\t\tvclass]->method_table[i];\n\t\t\t \n\t\t\tif (!*method) {\n\t\t\t\tret = allocate_method_table(method);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto error3;\n\t\t\t}\n\t\t\tmemcpy((*vendor_table)->vendor_class[vclass]->oui[i],\n\t\t\t       mad_reg_req->oui, 3);\n\t\t\tgoto check_in_use;\n\t\t}\n\t}\n\tdev_err(&agent_priv->agent.device->dev, \"All OUI slots in use\\n\");\n\tgoto error3;\n\ncheck_in_use:\n\t \n\tif (method_in_use(method, mad_reg_req))\n\t\tgoto error4;\n\n\t \n\tfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)\n\t\t(*method)->agent[i] = agent_priv;\n\n\treturn 0;\n\nerror4:\n\t \n\tremove_methods_mad_agent(*method, agent_priv);\n\t \n\tif (!check_method_table(*method)) {\n\t\t \n\t\tkfree(*method);\n\t\t*method = NULL;\n\t}\n\tret = -EINVAL;\nerror3:\n\tif (vendor_class) {\n\t\t(*vendor_table)->vendor_class[vclass] = NULL;\n\t\tkfree(vendor_class);\n\t}\nerror2:\n\tif (vendor) {\n\t\t*vendor_table = NULL;\n\t\tkfree(vendor);\n\t}\nerror1:\n\treturn ret;\n}\n\nstatic void remove_mad_reg_req(struct ib_mad_agent_private *agent_priv)\n{\n\tstruct ib_mad_port_private *port_priv;\n\tstruct ib_mad_mgmt_class_table *class;\n\tstruct ib_mad_mgmt_method_table *method;\n\tstruct ib_mad_mgmt_vendor_class_table *vendor;\n\tstruct ib_mad_mgmt_vendor_class *vendor_class;\n\tint index;\n\tu8 mgmt_class;\n\n\t \n\tif (!agent_priv->reg_req)\n\t\tgoto out;\n\n\tport_priv = agent_priv->qp_info->port_priv;\n\tmgmt_class = convert_mgmt_class(agent_priv->reg_req->mgmt_class);\n\tclass = port_priv->version[\n\t\t\tagent_priv->reg_req->mgmt_class_version].class;\n\tif (!class)\n\t\tgoto vendor_check;\n\n\tmethod = class->method_table[mgmt_class];\n\tif (method) {\n\t\t \n\t\tremove_methods_mad_agent(method, agent_priv);\n\t\t \n\t\tif (!check_method_table(method)) {\n\t\t\t \n\t\t\tkfree(method);\n\t\t\tclass->method_table[mgmt_class] = NULL;\n\t\t\t \n\t\t\tif (!check_class_table(class)) {\n\t\t\t\t \n\t\t\t\tkfree(class);\n\t\t\t\tport_priv->version[\n\t\t\t\t\tagent_priv->reg_req->\n\t\t\t\t\tmgmt_class_version].class = NULL;\n\t\t\t}\n\t\t}\n\t}\n\nvendor_check:\n\tif (!is_vendor_class(mgmt_class))\n\t\tgoto out;\n\n\t \n\tmgmt_class = vendor_class_index(agent_priv->reg_req->mgmt_class);\n\tvendor = port_priv->version[\n\t\t\tagent_priv->reg_req->mgmt_class_version].vendor;\n\n\tif (!vendor)\n\t\tgoto out;\n\n\tvendor_class = vendor->vendor_class[mgmt_class];\n\tif (vendor_class) {\n\t\tindex = find_vendor_oui(vendor_class, agent_priv->reg_req->oui);\n\t\tif (index < 0)\n\t\t\tgoto out;\n\t\tmethod = vendor_class->method_table[index];\n\t\tif (method) {\n\t\t\t \n\t\t\tremove_methods_mad_agent(method, agent_priv);\n\t\t\t \n\t\t\tif (!check_method_table(method)) {\n\t\t\t\t \n\t\t\t\tkfree(method);\n\t\t\t\tvendor_class->method_table[index] = NULL;\n\t\t\t\tmemset(vendor_class->oui[index], 0, 3);\n\t\t\t\t \n\t\t\t\tif (!check_vendor_class(vendor_class)) {\n\t\t\t\t\t \n\t\t\t\t\tkfree(vendor_class);\n\t\t\t\t\tvendor->vendor_class[mgmt_class] = NULL;\n\t\t\t\t\t \n\t\t\t\t\tif (!check_vendor_table(vendor)) {\n\t\t\t\t\t\tkfree(vendor);\n\t\t\t\t\t\tport_priv->version[\n\t\t\t\t\t\t\tagent_priv->reg_req->\n\t\t\t\t\t\t\tmgmt_class_version].\n\t\t\t\t\t\t\tvendor = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\treturn;\n}\n\nstatic struct ib_mad_agent_private *\nfind_mad_agent(struct ib_mad_port_private *port_priv,\n\t       const struct ib_mad_hdr *mad_hdr)\n{\n\tstruct ib_mad_agent_private *mad_agent = NULL;\n\tunsigned long flags;\n\n\tif (ib_response_mad(mad_hdr)) {\n\t\tu32 hi_tid;\n\n\t\t \n\t\thi_tid = be64_to_cpu(mad_hdr->tid) >> 32;\n\t\trcu_read_lock();\n\t\tmad_agent = xa_load(&ib_mad_clients, hi_tid);\n\t\tif (mad_agent && !refcount_inc_not_zero(&mad_agent->refcount))\n\t\t\tmad_agent = NULL;\n\t\trcu_read_unlock();\n\t} else {\n\t\tstruct ib_mad_mgmt_class_table *class;\n\t\tstruct ib_mad_mgmt_method_table *method;\n\t\tstruct ib_mad_mgmt_vendor_class_table *vendor;\n\t\tstruct ib_mad_mgmt_vendor_class *vendor_class;\n\t\tconst struct ib_vendor_mad *vendor_mad;\n\t\tint index;\n\n\t\tspin_lock_irqsave(&port_priv->reg_lock, flags);\n\t\t \n\t\tif (mad_hdr->class_version >= MAX_MGMT_VERSION)\n\t\t\tgoto out;\n\t\tif (!is_vendor_class(mad_hdr->mgmt_class)) {\n\t\t\tclass = port_priv->version[\n\t\t\t\t\tmad_hdr->class_version].class;\n\t\t\tif (!class)\n\t\t\t\tgoto out;\n\t\t\tif (convert_mgmt_class(mad_hdr->mgmt_class) >=\n\t\t\t    ARRAY_SIZE(class->method_table))\n\t\t\t\tgoto out;\n\t\t\tmethod = class->method_table[convert_mgmt_class(\n\t\t\t\t\t\t\tmad_hdr->mgmt_class)];\n\t\t\tif (method)\n\t\t\t\tmad_agent = method->agent[mad_hdr->method &\n\t\t\t\t\t\t\t  ~IB_MGMT_METHOD_RESP];\n\t\t} else {\n\t\t\tvendor = port_priv->version[\n\t\t\t\t\tmad_hdr->class_version].vendor;\n\t\t\tif (!vendor)\n\t\t\t\tgoto out;\n\t\t\tvendor_class = vendor->vendor_class[vendor_class_index(\n\t\t\t\t\t\tmad_hdr->mgmt_class)];\n\t\t\tif (!vendor_class)\n\t\t\t\tgoto out;\n\t\t\t \n\t\t\tvendor_mad = (const struct ib_vendor_mad *)mad_hdr;\n\t\t\tindex = find_vendor_oui(vendor_class, vendor_mad->oui);\n\t\t\tif (index == -1)\n\t\t\t\tgoto out;\n\t\t\tmethod = vendor_class->method_table[index];\n\t\t\tif (method) {\n\t\t\t\tmad_agent = method->agent[mad_hdr->method &\n\t\t\t\t\t\t\t  ~IB_MGMT_METHOD_RESP];\n\t\t\t}\n\t\t}\n\t\tif (mad_agent)\n\t\t\trefcount_inc(&mad_agent->refcount);\nout:\n\t\tspin_unlock_irqrestore(&port_priv->reg_lock, flags);\n\t}\n\n\tif (mad_agent && !mad_agent->agent.recv_handler) {\n\t\tdev_notice(&port_priv->device->dev,\n\t\t\t   \"No receive handler for client %p on port %u\\n\",\n\t\t\t   &mad_agent->agent, port_priv->port_num);\n\t\tderef_mad_agent(mad_agent);\n\t\tmad_agent = NULL;\n\t}\n\n\treturn mad_agent;\n}\n\nstatic int validate_mad(const struct ib_mad_hdr *mad_hdr,\n\t\t\tconst struct ib_mad_qp_info *qp_info,\n\t\t\tbool opa)\n{\n\tint valid = 0;\n\tu32 qp_num = qp_info->qp->qp_num;\n\n\t \n\tif (mad_hdr->base_version != IB_MGMT_BASE_VERSION &&\n\t    (!opa || mad_hdr->base_version != OPA_MGMT_BASE_VERSION)) {\n\t\tpr_err(\"MAD received with unsupported base version %u %s\\n\",\n\t\t       mad_hdr->base_version, opa ? \"(opa)\" : \"\");\n\t\tgoto out;\n\t}\n\n\t \n\tif ((mad_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_LID_ROUTED) ||\n\t    (mad_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {\n\t\tif (qp_num == 0)\n\t\t\tvalid = 1;\n\t} else {\n\t\t \n\t\tif ((mad_hdr->mgmt_class == IB_MGMT_CLASS_CM) &&\n\t\t    (mad_hdr->attr_id != IB_MGMT_CLASSPORTINFO_ATTR_ID) &&\n\t\t    (mad_hdr->method != IB_MGMT_METHOD_SEND))\n\t\t\tgoto out;\n\t\t \n\t\tif (qp_num != 0)\n\t\t\tvalid = 1;\n\t}\n\nout:\n\treturn valid;\n}\n\nstatic int is_rmpp_data_mad(const struct ib_mad_agent_private *mad_agent_priv,\n\t\t\t    const struct ib_mad_hdr *mad_hdr)\n{\n\tstruct ib_rmpp_mad *rmpp_mad;\n\n\trmpp_mad = (struct ib_rmpp_mad *)mad_hdr;\n\treturn !mad_agent_priv->agent.rmpp_version ||\n\t\t!ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent) ||\n\t\t!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &\n\t\t\t\t    IB_MGMT_RMPP_FLAG_ACTIVE) ||\n\t\t(rmpp_mad->rmpp_hdr.rmpp_type == IB_MGMT_RMPP_TYPE_DATA);\n}\n\nstatic inline int rcv_has_same_class(const struct ib_mad_send_wr_private *wr,\n\t\t\t\t     const struct ib_mad_recv_wc *rwc)\n{\n\treturn ((struct ib_mad_hdr *)(wr->send_buf.mad))->mgmt_class ==\n\t\trwc->recv_buf.mad->mad_hdr.mgmt_class;\n}\n\nstatic inline int\nrcv_has_same_gid(const struct ib_mad_agent_private *mad_agent_priv,\n\t\t const struct ib_mad_send_wr_private *wr,\n\t\t const struct ib_mad_recv_wc *rwc)\n{\n\tstruct rdma_ah_attr attr;\n\tu8 send_resp, rcv_resp;\n\tunion ib_gid sgid;\n\tstruct ib_device *device = mad_agent_priv->agent.device;\n\tu32 port_num = mad_agent_priv->agent.port_num;\n\tu8 lmc;\n\tbool has_grh;\n\n\tsend_resp = ib_response_mad((struct ib_mad_hdr *)wr->send_buf.mad);\n\trcv_resp = ib_response_mad(&rwc->recv_buf.mad->mad_hdr);\n\n\tif (send_resp == rcv_resp)\n\t\t \n\t\treturn 0;\n\n\tif (rdma_query_ah(wr->send_buf.ah, &attr))\n\t\t \n\t\treturn 0;\n\n\thas_grh = !!(rdma_ah_get_ah_flags(&attr) & IB_AH_GRH);\n\tif (has_grh != !!(rwc->wc->wc_flags & IB_WC_GRH))\n\t\t \n\t\treturn 0;\n\n\tif (!send_resp && rcv_resp) {\n\t\t \n\t\tif (!has_grh) {\n\t\t\tif (ib_get_cached_lmc(device, port_num, &lmc))\n\t\t\t\treturn 0;\n\t\t\treturn (!lmc || !((rdma_ah_get_path_bits(&attr) ^\n\t\t\t\t\t   rwc->wc->dlid_path_bits) &\n\t\t\t\t\t  ((1 << lmc) - 1)));\n\t\t} else {\n\t\t\tconst struct ib_global_route *grh =\n\t\t\t\t\trdma_ah_read_grh(&attr);\n\n\t\t\tif (rdma_query_gid(device, port_num,\n\t\t\t\t\t   grh->sgid_index, &sgid))\n\t\t\t\treturn 0;\n\t\t\treturn !memcmp(sgid.raw, rwc->recv_buf.grh->dgid.raw,\n\t\t\t\t       16);\n\t\t}\n\t}\n\n\tif (!has_grh)\n\t\treturn rdma_ah_get_dlid(&attr) == rwc->wc->slid;\n\telse\n\t\treturn !memcmp(rdma_ah_read_grh(&attr)->dgid.raw,\n\t\t\t       rwc->recv_buf.grh->sgid.raw,\n\t\t\t       16);\n}\n\nstatic inline int is_direct(u8 class)\n{\n\treturn (class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE);\n}\n\nstruct ib_mad_send_wr_private*\nib_find_send_mad(const struct ib_mad_agent_private *mad_agent_priv,\n\t\t const struct ib_mad_recv_wc *wc)\n{\n\tstruct ib_mad_send_wr_private *wr;\n\tconst struct ib_mad_hdr *mad_hdr;\n\n\tmad_hdr = &wc->recv_buf.mad->mad_hdr;\n\n\tlist_for_each_entry(wr, &mad_agent_priv->wait_list, agent_list) {\n\t\tif ((wr->tid == mad_hdr->tid) &&\n\t\t    rcv_has_same_class(wr, wc) &&\n\t\t     \n\t\t    (is_direct(mad_hdr->mgmt_class) ||\n\t\t     rcv_has_same_gid(mad_agent_priv, wr, wc)))\n\t\t\treturn (wr->status == IB_WC_SUCCESS) ? wr : NULL;\n\t}\n\n\t \n\tlist_for_each_entry(wr, &mad_agent_priv->send_list, agent_list) {\n\t\tif (is_rmpp_data_mad(mad_agent_priv, wr->send_buf.mad) &&\n\t\t    wr->tid == mad_hdr->tid &&\n\t\t    wr->timeout &&\n\t\t    rcv_has_same_class(wr, wc) &&\n\t\t     \n\t\t    (is_direct(mad_hdr->mgmt_class) ||\n\t\t     rcv_has_same_gid(mad_agent_priv, wr, wc)))\n\t\t\t \n\t\t\treturn (wr->status == IB_WC_SUCCESS) ? wr : NULL;\n\t}\n\treturn NULL;\n}\n\nvoid ib_mark_mad_done(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tmad_send_wr->timeout = 0;\n\tif (mad_send_wr->refcount == 1)\n\t\tlist_move_tail(&mad_send_wr->agent_list,\n\t\t\t      &mad_send_wr->mad_agent_priv->done_list);\n}\n\nstatic void ib_mad_complete_recv(struct ib_mad_agent_private *mad_agent_priv,\n\t\t\t\t struct ib_mad_recv_wc *mad_recv_wc)\n{\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tstruct ib_mad_send_wc mad_send_wc;\n\tunsigned long flags;\n\tint ret;\n\n\tINIT_LIST_HEAD(&mad_recv_wc->rmpp_list);\n\tret = ib_mad_enforce_security(mad_agent_priv,\n\t\t\t\t      mad_recv_wc->wc->pkey_index);\n\tif (ret) {\n\t\tib_free_recv_mad(mad_recv_wc);\n\t\tderef_mad_agent(mad_agent_priv);\n\t\treturn;\n\t}\n\n\tlist_add(&mad_recv_wc->recv_buf.list, &mad_recv_wc->rmpp_list);\n\tif (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {\n\t\tmad_recv_wc = ib_process_rmpp_recv_wc(mad_agent_priv,\n\t\t\t\t\t\t      mad_recv_wc);\n\t\tif (!mad_recv_wc) {\n\t\t\tderef_mad_agent(mad_agent_priv);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tif (ib_response_mad(&mad_recv_wc->recv_buf.mad->mad_hdr)) {\n\t\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\t\tmad_send_wr = ib_find_send_mad(mad_agent_priv, mad_recv_wc);\n\t\tif (!mad_send_wr) {\n\t\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\t\t\tif (!ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)\n\t\t\t   && ib_is_mad_class_rmpp(mad_recv_wc->recv_buf.mad->mad_hdr.mgmt_class)\n\t\t\t   && (ib_get_rmpp_flags(&((struct ib_rmpp_mad *)mad_recv_wc->recv_buf.mad)->rmpp_hdr)\n\t\t\t\t\t& IB_MGMT_RMPP_FLAG_ACTIVE)) {\n\t\t\t\t \n\t\t\t\tmad_agent_priv->agent.recv_handler(\n\t\t\t\t\t\t&mad_agent_priv->agent, NULL,\n\t\t\t\t\t\tmad_recv_wc);\n\t\t\t\tderef_mad_agent(mad_agent_priv);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tib_free_recv_mad(mad_recv_wc);\n\t\t\t\tderef_mad_agent(mad_agent_priv);\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else {\n\t\t\tib_mark_mad_done(mad_send_wr);\n\t\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\n\t\t\t \n\t\t\tmad_agent_priv->agent.recv_handler(\n\t\t\t\t\t&mad_agent_priv->agent,\n\t\t\t\t\t&mad_send_wr->send_buf,\n\t\t\t\t\tmad_recv_wc);\n\t\t\tderef_mad_agent(mad_agent_priv);\n\n\t\t\tmad_send_wc.status = IB_WC_SUCCESS;\n\t\t\tmad_send_wc.vendor_err = 0;\n\t\t\tmad_send_wc.send_buf = &mad_send_wr->send_buf;\n\t\t\tib_mad_complete_send_wr(mad_send_wr, &mad_send_wc);\n\t\t}\n\t} else {\n\t\tmad_agent_priv->agent.recv_handler(&mad_agent_priv->agent, NULL,\n\t\t\t\t\t\t   mad_recv_wc);\n\t\tderef_mad_agent(mad_agent_priv);\n\t}\n}\n\nstatic enum smi_action handle_ib_smi(const struct ib_mad_port_private *port_priv,\n\t\t\t\t     const struct ib_mad_qp_info *qp_info,\n\t\t\t\t     const struct ib_wc *wc,\n\t\t\t\t     u32 port_num,\n\t\t\t\t     struct ib_mad_private *recv,\n\t\t\t\t     struct ib_mad_private *response)\n{\n\tenum smi_forward_action retsmi;\n\tstruct ib_smp *smp = (struct ib_smp *)recv->mad;\n\n\ttrace_ib_mad_handle_ib_smi(smp);\n\n\tif (smi_handle_dr_smp_recv(smp,\n\t\t\t\t   rdma_cap_ib_switch(port_priv->device),\n\t\t\t\t   port_num,\n\t\t\t\t   port_priv->device->phys_port_cnt) ==\n\t\t\t\t   IB_SMI_DISCARD)\n\t\treturn IB_SMI_DISCARD;\n\n\tretsmi = smi_check_forward_dr_smp(smp);\n\tif (retsmi == IB_SMI_LOCAL)\n\t\treturn IB_SMI_HANDLE;\n\n\tif (retsmi == IB_SMI_SEND) {  \n\t\tif (smi_handle_dr_smp_send(smp,\n\t\t\t\t\t   rdma_cap_ib_switch(port_priv->device),\n\t\t\t\t\t   port_num) == IB_SMI_DISCARD)\n\t\t\treturn IB_SMI_DISCARD;\n\n\t\tif (smi_check_local_smp(smp, port_priv->device) == IB_SMI_DISCARD)\n\t\t\treturn IB_SMI_DISCARD;\n\t} else if (rdma_cap_ib_switch(port_priv->device)) {\n\t\t \n\t\tmemcpy(response, recv, mad_priv_size(response));\n\t\tresponse->header.recv_wc.wc = &response->header.wc;\n\t\tresponse->header.recv_wc.recv_buf.mad = (struct ib_mad *)response->mad;\n\t\tresponse->header.recv_wc.recv_buf.grh = &response->grh;\n\n\t\tagent_send_response((const struct ib_mad_hdr *)response->mad,\n\t\t\t\t    &response->grh, wc,\n\t\t\t\t    port_priv->device,\n\t\t\t\t    smi_get_fwd_port(smp),\n\t\t\t\t    qp_info->qp->qp_num,\n\t\t\t\t    response->mad_size,\n\t\t\t\t    false);\n\n\t\treturn IB_SMI_DISCARD;\n\t}\n\treturn IB_SMI_HANDLE;\n}\n\nstatic bool generate_unmatched_resp(const struct ib_mad_private *recv,\n\t\t\t\t    struct ib_mad_private *response,\n\t\t\t\t    size_t *resp_len, bool opa)\n{\n\tconst struct ib_mad_hdr *recv_hdr = (const struct ib_mad_hdr *)recv->mad;\n\tstruct ib_mad_hdr *resp_hdr = (struct ib_mad_hdr *)response->mad;\n\n\tif (recv_hdr->method == IB_MGMT_METHOD_GET ||\n\t    recv_hdr->method == IB_MGMT_METHOD_SET) {\n\t\tmemcpy(response, recv, mad_priv_size(response));\n\t\tresponse->header.recv_wc.wc = &response->header.wc;\n\t\tresponse->header.recv_wc.recv_buf.mad = (struct ib_mad *)response->mad;\n\t\tresponse->header.recv_wc.recv_buf.grh = &response->grh;\n\t\tresp_hdr->method = IB_MGMT_METHOD_GET_RESP;\n\t\tresp_hdr->status = cpu_to_be16(IB_MGMT_MAD_STATUS_UNSUPPORTED_METHOD_ATTRIB);\n\t\tif (recv_hdr->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\n\t\t\tresp_hdr->status |= IB_SMP_DIRECTION;\n\n\t\tif (opa && recv_hdr->base_version == OPA_MGMT_BASE_VERSION) {\n\t\t\tif (recv_hdr->mgmt_class ==\n\t\t\t    IB_MGMT_CLASS_SUBN_LID_ROUTED ||\n\t\t\t    recv_hdr->mgmt_class ==\n\t\t\t    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\n\t\t\t\t*resp_len = opa_get_smp_header_size(\n\t\t\t\t\t\t\t(struct opa_smp *)recv->mad);\n\t\t\telse\n\t\t\t\t*resp_len = sizeof(struct ib_mad_hdr);\n\t\t}\n\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\nstatic enum smi_action\nhandle_opa_smi(struct ib_mad_port_private *port_priv,\n\t       struct ib_mad_qp_info *qp_info,\n\t       struct ib_wc *wc,\n\t       u32 port_num,\n\t       struct ib_mad_private *recv,\n\t       struct ib_mad_private *response)\n{\n\tenum smi_forward_action retsmi;\n\tstruct opa_smp *smp = (struct opa_smp *)recv->mad;\n\n\ttrace_ib_mad_handle_opa_smi(smp);\n\n\tif (opa_smi_handle_dr_smp_recv(smp,\n\t\t\t\t   rdma_cap_ib_switch(port_priv->device),\n\t\t\t\t   port_num,\n\t\t\t\t   port_priv->device->phys_port_cnt) ==\n\t\t\t\t   IB_SMI_DISCARD)\n\t\treturn IB_SMI_DISCARD;\n\n\tretsmi = opa_smi_check_forward_dr_smp(smp);\n\tif (retsmi == IB_SMI_LOCAL)\n\t\treturn IB_SMI_HANDLE;\n\n\tif (retsmi == IB_SMI_SEND) {  \n\t\tif (opa_smi_handle_dr_smp_send(smp,\n\t\t\t\t\t   rdma_cap_ib_switch(port_priv->device),\n\t\t\t\t\t   port_num) == IB_SMI_DISCARD)\n\t\t\treturn IB_SMI_DISCARD;\n\n\t\tif (opa_smi_check_local_smp(smp, port_priv->device) ==\n\t\t    IB_SMI_DISCARD)\n\t\t\treturn IB_SMI_DISCARD;\n\n\t} else if (rdma_cap_ib_switch(port_priv->device)) {\n\t\t \n\t\tmemcpy(response, recv, mad_priv_size(response));\n\t\tresponse->header.recv_wc.wc = &response->header.wc;\n\t\tresponse->header.recv_wc.recv_buf.opa_mad =\n\t\t\t\t(struct opa_mad *)response->mad;\n\t\tresponse->header.recv_wc.recv_buf.grh = &response->grh;\n\n\t\tagent_send_response((const struct ib_mad_hdr *)response->mad,\n\t\t\t\t    &response->grh, wc,\n\t\t\t\t    port_priv->device,\n\t\t\t\t    opa_smi_get_fwd_port(smp),\n\t\t\t\t    qp_info->qp->qp_num,\n\t\t\t\t    recv->header.wc.byte_len,\n\t\t\t\t    true);\n\n\t\treturn IB_SMI_DISCARD;\n\t}\n\n\treturn IB_SMI_HANDLE;\n}\n\nstatic enum smi_action\nhandle_smi(struct ib_mad_port_private *port_priv,\n\t   struct ib_mad_qp_info *qp_info,\n\t   struct ib_wc *wc,\n\t   u32 port_num,\n\t   struct ib_mad_private *recv,\n\t   struct ib_mad_private *response,\n\t   bool opa)\n{\n\tstruct ib_mad_hdr *mad_hdr = (struct ib_mad_hdr *)recv->mad;\n\n\tif (opa && mad_hdr->base_version == OPA_MGMT_BASE_VERSION &&\n\t    mad_hdr->class_version == OPA_SM_CLASS_VERSION)\n\t\treturn handle_opa_smi(port_priv, qp_info, wc, port_num, recv,\n\t\t\t\t      response);\n\n\treturn handle_ib_smi(port_priv, qp_info, wc, port_num, recv, response);\n}\n\nstatic void ib_mad_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_mad_port_private *port_priv = cq->cq_context;\n\tstruct ib_mad_list_head *mad_list =\n\t\tcontainer_of(wc->wr_cqe, struct ib_mad_list_head, cqe);\n\tstruct ib_mad_qp_info *qp_info;\n\tstruct ib_mad_private_header *mad_priv_hdr;\n\tstruct ib_mad_private *recv, *response = NULL;\n\tstruct ib_mad_agent_private *mad_agent;\n\tu32 port_num;\n\tint ret = IB_MAD_RESULT_SUCCESS;\n\tsize_t mad_size;\n\tu16 resp_mad_pkey_index = 0;\n\tbool opa;\n\n\tif (list_empty_careful(&port_priv->port_list))\n\t\treturn;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\t \n\t\treturn;\n\t}\n\n\tqp_info = mad_list->mad_queue->qp_info;\n\tdequeue_mad(mad_list);\n\n\topa = rdma_cap_opa_mad(qp_info->port_priv->device,\n\t\t\t       qp_info->port_priv->port_num);\n\n\tmad_priv_hdr = container_of(mad_list, struct ib_mad_private_header,\n\t\t\t\t    mad_list);\n\trecv = container_of(mad_priv_hdr, struct ib_mad_private, header);\n\tib_dma_unmap_single(port_priv->device,\n\t\t\t    recv->header.mapping,\n\t\t\t    mad_priv_dma_size(recv),\n\t\t\t    DMA_FROM_DEVICE);\n\n\t \n\trecv->header.wc = *wc;\n\trecv->header.recv_wc.wc = &recv->header.wc;\n\n\tif (opa && ((struct ib_mad_hdr *)(recv->mad))->base_version == OPA_MGMT_BASE_VERSION) {\n\t\trecv->header.recv_wc.mad_len = wc->byte_len - sizeof(struct ib_grh);\n\t\trecv->header.recv_wc.mad_seg_size = sizeof(struct opa_mad);\n\t} else {\n\t\trecv->header.recv_wc.mad_len = sizeof(struct ib_mad);\n\t\trecv->header.recv_wc.mad_seg_size = sizeof(struct ib_mad);\n\t}\n\n\trecv->header.recv_wc.recv_buf.mad = (struct ib_mad *)recv->mad;\n\trecv->header.recv_wc.recv_buf.grh = &recv->grh;\n\n\t \n\tif (!validate_mad((const struct ib_mad_hdr *)recv->mad, qp_info, opa))\n\t\tgoto out;\n\n\ttrace_ib_mad_recv_done_handler(qp_info, wc,\n\t\t\t\t       (struct ib_mad_hdr *)recv->mad);\n\n\tmad_size = recv->mad_size;\n\tresponse = alloc_mad_private(mad_size, GFP_KERNEL);\n\tif (!response)\n\t\tgoto out;\n\n\tif (rdma_cap_ib_switch(port_priv->device))\n\t\tport_num = wc->port_num;\n\telse\n\t\tport_num = port_priv->port_num;\n\n\tif (((struct ib_mad_hdr *)recv->mad)->mgmt_class ==\n\t    IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {\n\t\tif (handle_smi(port_priv, qp_info, wc, port_num, recv,\n\t\t\t       response, opa)\n\t\t    == IB_SMI_DISCARD)\n\t\t\tgoto out;\n\t}\n\n\t \n\tif (port_priv->device->ops.process_mad) {\n\t\tret = port_priv->device->ops.process_mad(\n\t\t\tport_priv->device, 0, port_priv->port_num, wc,\n\t\t\t&recv->grh, (const struct ib_mad *)recv->mad,\n\t\t\t(struct ib_mad *)response->mad, &mad_size,\n\t\t\t&resp_mad_pkey_index);\n\n\t\tif (opa)\n\t\t\twc->pkey_index = resp_mad_pkey_index;\n\n\t\tif (ret & IB_MAD_RESULT_SUCCESS) {\n\t\t\tif (ret & IB_MAD_RESULT_CONSUMED)\n\t\t\t\tgoto out;\n\t\t\tif (ret & IB_MAD_RESULT_REPLY) {\n\t\t\t\tagent_send_response((const struct ib_mad_hdr *)response->mad,\n\t\t\t\t\t\t    &recv->grh, wc,\n\t\t\t\t\t\t    port_priv->device,\n\t\t\t\t\t\t    port_num,\n\t\t\t\t\t\t    qp_info->qp->qp_num,\n\t\t\t\t\t\t    mad_size, opa);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tmad_agent = find_mad_agent(port_priv, (const struct ib_mad_hdr *)recv->mad);\n\tif (mad_agent) {\n\t\ttrace_ib_mad_recv_done_agent(mad_agent);\n\t\tib_mad_complete_recv(mad_agent, &recv->header.recv_wc);\n\t\t \n\t\trecv = NULL;\n\t} else if ((ret & IB_MAD_RESULT_SUCCESS) &&\n\t\t   generate_unmatched_resp(recv, response, &mad_size, opa)) {\n\t\tagent_send_response((const struct ib_mad_hdr *)response->mad, &recv->grh, wc,\n\t\t\t\t    port_priv->device, port_num,\n\t\t\t\t    qp_info->qp->qp_num, mad_size, opa);\n\t}\n\nout:\n\t \n\tif (response) {\n\t\tib_mad_post_receive_mads(qp_info, response);\n\t\tkfree(recv);\n\t} else\n\t\tib_mad_post_receive_mads(qp_info, recv);\n}\n\nstatic void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv)\n{\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tunsigned long delay;\n\n\tif (list_empty(&mad_agent_priv->wait_list)) {\n\t\tcancel_delayed_work(&mad_agent_priv->timed_work);\n\t} else {\n\t\tmad_send_wr = list_entry(mad_agent_priv->wait_list.next,\n\t\t\t\t\t struct ib_mad_send_wr_private,\n\t\t\t\t\t agent_list);\n\n\t\tif (time_after(mad_agent_priv->timeout,\n\t\t\t       mad_send_wr->timeout)) {\n\t\t\tmad_agent_priv->timeout = mad_send_wr->timeout;\n\t\t\tdelay = mad_send_wr->timeout - jiffies;\n\t\t\tif ((long)delay <= 0)\n\t\t\t\tdelay = 1;\n\t\t\tmod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,\n\t\t\t\t\t &mad_agent_priv->timed_work, delay);\n\t\t}\n\t}\n}\n\nstatic void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_wr_private *temp_mad_send_wr;\n\tstruct list_head *list_item;\n\tunsigned long delay;\n\n\tmad_agent_priv = mad_send_wr->mad_agent_priv;\n\tlist_del(&mad_send_wr->agent_list);\n\n\tdelay = mad_send_wr->timeout;\n\tmad_send_wr->timeout += jiffies;\n\n\tif (delay) {\n\t\tlist_for_each_prev(list_item, &mad_agent_priv->wait_list) {\n\t\t\ttemp_mad_send_wr = list_entry(list_item,\n\t\t\t\t\t\tstruct ib_mad_send_wr_private,\n\t\t\t\t\t\tagent_list);\n\t\t\tif (time_after(mad_send_wr->timeout,\n\t\t\t\t       temp_mad_send_wr->timeout))\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tlist_item = &mad_agent_priv->wait_list;\n\t}\n\n\tlist_add(&mad_send_wr->agent_list, list_item);\n\n\t \n\tif (mad_agent_priv->wait_list.next == &mad_send_wr->agent_list)\n\t\tmod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,\n\t\t\t\t &mad_agent_priv->timed_work, delay);\n}\n\nvoid ib_reset_mad_timeout(struct ib_mad_send_wr_private *mad_send_wr,\n\t\t\t  unsigned long timeout_ms)\n{\n\tmad_send_wr->timeout = msecs_to_jiffies(timeout_ms);\n\twait_for_response(mad_send_wr);\n}\n\n \nvoid ib_mad_complete_send_wr(struct ib_mad_send_wr_private *mad_send_wr,\n\t\t\t     struct ib_mad_send_wc *mad_send_wc)\n{\n\tstruct ib_mad_agent_private\t*mad_agent_priv;\n\tunsigned long\t\t\tflags;\n\tint\t\t\t\tret;\n\n\tmad_agent_priv = mad_send_wr->mad_agent_priv;\n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\tif (ib_mad_kernel_rmpp_agent(&mad_agent_priv->agent)) {\n\t\tret = ib_process_rmpp_send_wc(mad_send_wr, mad_send_wc);\n\t\tif (ret == IB_RMPP_RESULT_CONSUMED)\n\t\t\tgoto done;\n\t} else\n\t\tret = IB_RMPP_RESULT_UNHANDLED;\n\n\tif (mad_send_wc->status != IB_WC_SUCCESS &&\n\t    mad_send_wr->status == IB_WC_SUCCESS) {\n\t\tmad_send_wr->status = mad_send_wc->status;\n\t\tmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\n\t}\n\n\tif (--mad_send_wr->refcount > 0) {\n\t\tif (mad_send_wr->refcount == 1 && mad_send_wr->timeout &&\n\t\t    mad_send_wr->status == IB_WC_SUCCESS) {\n\t\t\twait_for_response(mad_send_wr);\n\t\t}\n\t\tgoto done;\n\t}\n\n\t \n\tlist_del(&mad_send_wr->agent_list);\n\tadjust_timeout(mad_agent_priv);\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\n\tif (mad_send_wr->status != IB_WC_SUCCESS)\n\t\tmad_send_wc->status = mad_send_wr->status;\n\tif (ret == IB_RMPP_RESULT_INTERNAL)\n\t\tib_rmpp_send_handler(mad_send_wc);\n\telse\n\t\tmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\n\t\t\t\t\t\t   mad_send_wc);\n\n\t \n\tderef_mad_agent(mad_agent_priv);\n\treturn;\ndone:\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n}\n\nstatic void ib_mad_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_mad_port_private *port_priv = cq->cq_context;\n\tstruct ib_mad_list_head *mad_list =\n\t\tcontainer_of(wc->wr_cqe, struct ib_mad_list_head, cqe);\n\tstruct ib_mad_send_wr_private\t*mad_send_wr, *queued_send_wr;\n\tstruct ib_mad_qp_info\t\t*qp_info;\n\tstruct ib_mad_queue\t\t*send_queue;\n\tstruct ib_mad_send_wc\t\tmad_send_wc;\n\tunsigned long flags;\n\tint ret;\n\n\tif (list_empty_careful(&port_priv->port_list))\n\t\treturn;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (!ib_mad_send_error(port_priv, wc))\n\t\t\treturn;\n\t}\n\n\tmad_send_wr = container_of(mad_list, struct ib_mad_send_wr_private,\n\t\t\t\t   mad_list);\n\tsend_queue = mad_list->mad_queue;\n\tqp_info = send_queue->qp_info;\n\n\ttrace_ib_mad_send_done_agent(mad_send_wr->mad_agent_priv);\n\ttrace_ib_mad_send_done_handler(mad_send_wr, wc);\n\nretry:\n\tib_dma_unmap_single(mad_send_wr->send_buf.mad_agent->device,\n\t\t\t    mad_send_wr->header_mapping,\n\t\t\t    mad_send_wr->sg_list[0].length, DMA_TO_DEVICE);\n\tib_dma_unmap_single(mad_send_wr->send_buf.mad_agent->device,\n\t\t\t    mad_send_wr->payload_mapping,\n\t\t\t    mad_send_wr->sg_list[1].length, DMA_TO_DEVICE);\n\tqueued_send_wr = NULL;\n\tspin_lock_irqsave(&send_queue->lock, flags);\n\tlist_del(&mad_list->list);\n\n\t \n\tif (send_queue->count-- > send_queue->max_active) {\n\t\tmad_list = container_of(qp_info->overflow_list.next,\n\t\t\t\t\tstruct ib_mad_list_head, list);\n\t\tqueued_send_wr = container_of(mad_list,\n\t\t\t\t\tstruct ib_mad_send_wr_private,\n\t\t\t\t\tmad_list);\n\t\tlist_move_tail(&mad_list->list, &send_queue->list);\n\t}\n\tspin_unlock_irqrestore(&send_queue->lock, flags);\n\n\tmad_send_wc.send_buf = &mad_send_wr->send_buf;\n\tmad_send_wc.status = wc->status;\n\tmad_send_wc.vendor_err = wc->vendor_err;\n\tib_mad_complete_send_wr(mad_send_wr, &mad_send_wc);\n\n\tif (queued_send_wr) {\n\t\ttrace_ib_mad_send_done_resend(queued_send_wr, qp_info);\n\t\tret = ib_post_send(qp_info->qp, &queued_send_wr->send_wr.wr,\n\t\t\t\t   NULL);\n\t\tif (ret) {\n\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\"ib_post_send failed: %d\\n\", ret);\n\t\t\tmad_send_wr = queued_send_wr;\n\t\t\twc->status = IB_WC_LOC_QP_OP_ERR;\n\t\t\tgoto retry;\n\t\t}\n\t}\n}\n\nstatic void mark_sends_for_retry(struct ib_mad_qp_info *qp_info)\n{\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tstruct ib_mad_list_head *mad_list;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp_info->send_queue.lock, flags);\n\tlist_for_each_entry(mad_list, &qp_info->send_queue.list, list) {\n\t\tmad_send_wr = container_of(mad_list,\n\t\t\t\t\t   struct ib_mad_send_wr_private,\n\t\t\t\t\t   mad_list);\n\t\tmad_send_wr->retry = 1;\n\t}\n\tspin_unlock_irqrestore(&qp_info->send_queue.lock, flags);\n}\n\nstatic bool ib_mad_send_error(struct ib_mad_port_private *port_priv,\n\t\tstruct ib_wc *wc)\n{\n\tstruct ib_mad_list_head *mad_list =\n\t\tcontainer_of(wc->wr_cqe, struct ib_mad_list_head, cqe);\n\tstruct ib_mad_qp_info *qp_info = mad_list->mad_queue->qp_info;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tint ret;\n\n\t \n\tmad_send_wr = container_of(mad_list, struct ib_mad_send_wr_private,\n\t\t\t\t   mad_list);\n\tif (wc->status == IB_WC_WR_FLUSH_ERR) {\n\t\tif (mad_send_wr->retry) {\n\t\t\t \n\t\t\tmad_send_wr->retry = 0;\n\t\t\ttrace_ib_mad_error_handler(mad_send_wr, qp_info);\n\t\t\tret = ib_post_send(qp_info->qp, &mad_send_wr->send_wr.wr,\n\t\t\t\t\t   NULL);\n\t\t\tif (!ret)\n\t\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tstruct ib_qp_attr *attr;\n\n\t\t \n\t\tattr = kmalloc(sizeof *attr, GFP_KERNEL);\n\t\tif (attr) {\n\t\t\tattr->qp_state = IB_QPS_RTS;\n\t\t\tattr->cur_qp_state = IB_QPS_SQE;\n\t\t\tret = ib_modify_qp(qp_info->qp, attr,\n\t\t\t\t\t   IB_QP_STATE | IB_QP_CUR_STATE);\n\t\t\tkfree(attr);\n\t\t\tif (ret)\n\t\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\t\"%s - ib_modify_qp to RTS: %d\\n\",\n\t\t\t\t\t__func__, ret);\n\t\t\telse\n\t\t\t\tmark_sends_for_retry(qp_info);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic void cancel_mads(struct ib_mad_agent_private *mad_agent_priv)\n{\n\tunsigned long flags;\n\tstruct ib_mad_send_wr_private *mad_send_wr, *temp_mad_send_wr;\n\tstruct ib_mad_send_wc mad_send_wc;\n\tstruct list_head cancel_list;\n\n\tINIT_LIST_HEAD(&cancel_list);\n\n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\tlist_for_each_entry_safe(mad_send_wr, temp_mad_send_wr,\n\t\t\t\t &mad_agent_priv->send_list, agent_list) {\n\t\tif (mad_send_wr->status == IB_WC_SUCCESS) {\n\t\t\tmad_send_wr->status = IB_WC_WR_FLUSH_ERR;\n\t\t\tmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\n\t\t}\n\t}\n\n\t \n\tlist_splice_init(&mad_agent_priv->wait_list, &cancel_list);\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\n\t \n\tmad_send_wc.status = IB_WC_WR_FLUSH_ERR;\n\tmad_send_wc.vendor_err = 0;\n\n\tlist_for_each_entry_safe(mad_send_wr, temp_mad_send_wr,\n\t\t\t\t &cancel_list, agent_list) {\n\t\tmad_send_wc.send_buf = &mad_send_wr->send_buf;\n\t\tlist_del(&mad_send_wr->agent_list);\n\t\tmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\n\t\t\t\t\t\t   &mad_send_wc);\n\t\tderef_mad_agent(mad_agent_priv);\n\t}\n}\n\nstatic struct ib_mad_send_wr_private*\nfind_send_wr(struct ib_mad_agent_private *mad_agent_priv,\n\t     struct ib_mad_send_buf *send_buf)\n{\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\n\tlist_for_each_entry(mad_send_wr, &mad_agent_priv->wait_list,\n\t\t\t    agent_list) {\n\t\tif (&mad_send_wr->send_buf == send_buf)\n\t\t\treturn mad_send_wr;\n\t}\n\n\tlist_for_each_entry(mad_send_wr, &mad_agent_priv->send_list,\n\t\t\t    agent_list) {\n\t\tif (is_rmpp_data_mad(mad_agent_priv,\n\t\t\t\t     mad_send_wr->send_buf.mad) &&\n\t\t    &mad_send_wr->send_buf == send_buf)\n\t\t\treturn mad_send_wr;\n\t}\n\treturn NULL;\n}\n\nint ib_modify_mad(struct ib_mad_send_buf *send_buf, u32 timeout_ms)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tunsigned long flags;\n\tint active;\n\n\tif (!send_buf)\n\t\treturn -EINVAL;\n\n\tmad_agent_priv = container_of(send_buf->mad_agent,\n\t\t\t\t      struct ib_mad_agent_private, agent);\n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\tmad_send_wr = find_send_wr(mad_agent_priv, send_buf);\n\tif (!mad_send_wr || mad_send_wr->status != IB_WC_SUCCESS) {\n\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n\tactive = (!mad_send_wr->timeout || mad_send_wr->refcount > 1);\n\tif (!timeout_ms) {\n\t\tmad_send_wr->status = IB_WC_WR_FLUSH_ERR;\n\t\tmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\n\t}\n\n\tmad_send_wr->send_buf.timeout_ms = timeout_ms;\n\tif (active)\n\t\tmad_send_wr->timeout = msecs_to_jiffies(timeout_ms);\n\telse\n\t\tib_reset_mad_timeout(mad_send_wr, timeout_ms);\n\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\treturn 0;\n}\nEXPORT_SYMBOL(ib_modify_mad);\n\nstatic void local_completions(struct work_struct *work)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_local_private *local;\n\tstruct ib_mad_agent_private *recv_mad_agent;\n\tunsigned long flags;\n\tint free_mad;\n\tstruct ib_wc wc;\n\tstruct ib_mad_send_wc mad_send_wc;\n\tbool opa;\n\n\tmad_agent_priv =\n\t\tcontainer_of(work, struct ib_mad_agent_private, local_work);\n\n\topa = rdma_cap_opa_mad(mad_agent_priv->qp_info->port_priv->device,\n\t\t\t       mad_agent_priv->qp_info->port_priv->port_num);\n\n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\twhile (!list_empty(&mad_agent_priv->local_list)) {\n\t\tlocal = list_entry(mad_agent_priv->local_list.next,\n\t\t\t\t   struct ib_mad_local_private,\n\t\t\t\t   completion_list);\n\t\tlist_del(&local->completion_list);\n\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\t\tfree_mad = 0;\n\t\tif (local->mad_priv) {\n\t\t\tu8 base_version;\n\t\t\trecv_mad_agent = local->recv_mad_agent;\n\t\t\tif (!recv_mad_agent) {\n\t\t\t\tdev_err(&mad_agent_priv->agent.device->dev,\n\t\t\t\t\t\"No receive MAD agent for local completion\\n\");\n\t\t\t\tfree_mad = 1;\n\t\t\t\tgoto local_send_completion;\n\t\t\t}\n\n\t\t\t \n\t\t\tbuild_smp_wc(recv_mad_agent->agent.qp,\n\t\t\t\t     local->mad_send_wr->send_wr.wr.wr_cqe,\n\t\t\t\t     be16_to_cpu(IB_LID_PERMISSIVE),\n\t\t\t\t     local->mad_send_wr->send_wr.pkey_index,\n\t\t\t\t     recv_mad_agent->agent.port_num, &wc);\n\n\t\t\tlocal->mad_priv->header.recv_wc.wc = &wc;\n\n\t\t\tbase_version = ((struct ib_mad_hdr *)(local->mad_priv->mad))->base_version;\n\t\t\tif (opa && base_version == OPA_MGMT_BASE_VERSION) {\n\t\t\t\tlocal->mad_priv->header.recv_wc.mad_len = local->return_wc_byte_len;\n\t\t\t\tlocal->mad_priv->header.recv_wc.mad_seg_size = sizeof(struct opa_mad);\n\t\t\t} else {\n\t\t\t\tlocal->mad_priv->header.recv_wc.mad_len = sizeof(struct ib_mad);\n\t\t\t\tlocal->mad_priv->header.recv_wc.mad_seg_size = sizeof(struct ib_mad);\n\t\t\t}\n\n\t\t\tINIT_LIST_HEAD(&local->mad_priv->header.recv_wc.rmpp_list);\n\t\t\tlist_add(&local->mad_priv->header.recv_wc.recv_buf.list,\n\t\t\t\t &local->mad_priv->header.recv_wc.rmpp_list);\n\t\t\tlocal->mad_priv->header.recv_wc.recv_buf.grh = NULL;\n\t\t\tlocal->mad_priv->header.recv_wc.recv_buf.mad =\n\t\t\t\t\t\t(struct ib_mad *)local->mad_priv->mad;\n\t\t\trecv_mad_agent->agent.recv_handler(\n\t\t\t\t\t\t&recv_mad_agent->agent,\n\t\t\t\t\t\t&local->mad_send_wr->send_buf,\n\t\t\t\t\t\t&local->mad_priv->header.recv_wc);\n\t\t\tspin_lock_irqsave(&recv_mad_agent->lock, flags);\n\t\t\tderef_mad_agent(recv_mad_agent);\n\t\t\tspin_unlock_irqrestore(&recv_mad_agent->lock, flags);\n\t\t}\n\nlocal_send_completion:\n\t\t \n\t\tmad_send_wc.status = IB_WC_SUCCESS;\n\t\tmad_send_wc.vendor_err = 0;\n\t\tmad_send_wc.send_buf = &local->mad_send_wr->send_buf;\n\t\tmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\n\t\t\t\t\t\t   &mad_send_wc);\n\n\t\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\t\tderef_mad_agent(mad_agent_priv);\n\t\tif (free_mad)\n\t\t\tkfree(local->mad_priv);\n\t\tkfree(local);\n\t}\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n}\n\nstatic int retry_send(struct ib_mad_send_wr_private *mad_send_wr)\n{\n\tint ret;\n\n\tif (!mad_send_wr->retries_left)\n\t\treturn -ETIMEDOUT;\n\n\tmad_send_wr->retries_left--;\n\tmad_send_wr->send_buf.retries++;\n\n\tmad_send_wr->timeout = msecs_to_jiffies(mad_send_wr->send_buf.timeout_ms);\n\n\tif (ib_mad_kernel_rmpp_agent(&mad_send_wr->mad_agent_priv->agent)) {\n\t\tret = ib_retry_rmpp(mad_send_wr);\n\t\tswitch (ret) {\n\t\tcase IB_RMPP_RESULT_UNHANDLED:\n\t\t\tret = ib_send_mad(mad_send_wr);\n\t\t\tbreak;\n\t\tcase IB_RMPP_RESULT_CONSUMED:\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -ECOMM;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tret = ib_send_mad(mad_send_wr);\n\n\tif (!ret) {\n\t\tmad_send_wr->refcount++;\n\t\tlist_add_tail(&mad_send_wr->agent_list,\n\t\t\t      &mad_send_wr->mad_agent_priv->send_list);\n\t}\n\treturn ret;\n}\n\nstatic void timeout_sends(struct work_struct *work)\n{\n\tstruct ib_mad_agent_private *mad_agent_priv;\n\tstruct ib_mad_send_wr_private *mad_send_wr;\n\tstruct ib_mad_send_wc mad_send_wc;\n\tunsigned long flags, delay;\n\n\tmad_agent_priv = container_of(work, struct ib_mad_agent_private,\n\t\t\t\t      timed_work.work);\n\tmad_send_wc.vendor_err = 0;\n\n\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\twhile (!list_empty(&mad_agent_priv->wait_list)) {\n\t\tmad_send_wr = list_entry(mad_agent_priv->wait_list.next,\n\t\t\t\t\t struct ib_mad_send_wr_private,\n\t\t\t\t\t agent_list);\n\n\t\tif (time_after(mad_send_wr->timeout, jiffies)) {\n\t\t\tdelay = mad_send_wr->timeout - jiffies;\n\t\t\tif ((long)delay <= 0)\n\t\t\t\tdelay = 1;\n\t\t\tqueue_delayed_work(mad_agent_priv->qp_info->\n\t\t\t\t\t   port_priv->wq,\n\t\t\t\t\t   &mad_agent_priv->timed_work, delay);\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_del(&mad_send_wr->agent_list);\n\t\tif (mad_send_wr->status == IB_WC_SUCCESS &&\n\t\t    !retry_send(mad_send_wr))\n\t\t\tcontinue;\n\n\t\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n\n\t\tif (mad_send_wr->status == IB_WC_SUCCESS)\n\t\t\tmad_send_wc.status = IB_WC_RESP_TIMEOUT_ERR;\n\t\telse\n\t\t\tmad_send_wc.status = mad_send_wr->status;\n\t\tmad_send_wc.send_buf = &mad_send_wr->send_buf;\n\t\tmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\n\t\t\t\t\t\t   &mad_send_wc);\n\n\t\tderef_mad_agent(mad_agent_priv);\n\t\tspin_lock_irqsave(&mad_agent_priv->lock, flags);\n\t}\n\tspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\n}\n\n \nstatic int ib_mad_post_receive_mads(struct ib_mad_qp_info *qp_info,\n\t\t\t\t    struct ib_mad_private *mad)\n{\n\tunsigned long flags;\n\tint post, ret;\n\tstruct ib_mad_private *mad_priv;\n\tstruct ib_sge sg_list;\n\tstruct ib_recv_wr recv_wr;\n\tstruct ib_mad_queue *recv_queue = &qp_info->recv_queue;\n\n\t \n\tsg_list.lkey = qp_info->port_priv->pd->local_dma_lkey;\n\n\t \n\trecv_wr.next = NULL;\n\trecv_wr.sg_list = &sg_list;\n\trecv_wr.num_sge = 1;\n\n\tdo {\n\t\t \n\t\tif (mad) {\n\t\t\tmad_priv = mad;\n\t\t\tmad = NULL;\n\t\t} else {\n\t\t\tmad_priv = alloc_mad_private(port_mad_size(qp_info->port_priv),\n\t\t\t\t\t\t     GFP_ATOMIC);\n\t\t\tif (!mad_priv) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tsg_list.length = mad_priv_dma_size(mad_priv);\n\t\tsg_list.addr = ib_dma_map_single(qp_info->port_priv->device,\n\t\t\t\t\t\t &mad_priv->grh,\n\t\t\t\t\t\t mad_priv_dma_size(mad_priv),\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\t\tif (unlikely(ib_dma_mapping_error(qp_info->port_priv->device,\n\t\t\t\t\t\t  sg_list.addr))) {\n\t\t\tkfree(mad_priv);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tmad_priv->header.mapping = sg_list.addr;\n\t\tmad_priv->header.mad_list.mad_queue = recv_queue;\n\t\tmad_priv->header.mad_list.cqe.done = ib_mad_recv_done;\n\t\trecv_wr.wr_cqe = &mad_priv->header.mad_list.cqe;\n\n\t\t \n\t\tspin_lock_irqsave(&recv_queue->lock, flags);\n\t\tpost = (++recv_queue->count < recv_queue->max_active);\n\t\tlist_add_tail(&mad_priv->header.mad_list.list, &recv_queue->list);\n\t\tspin_unlock_irqrestore(&recv_queue->lock, flags);\n\t\tret = ib_post_recv(qp_info->qp, &recv_wr, NULL);\n\t\tif (ret) {\n\t\t\tspin_lock_irqsave(&recv_queue->lock, flags);\n\t\t\tlist_del(&mad_priv->header.mad_list.list);\n\t\t\trecv_queue->count--;\n\t\t\tspin_unlock_irqrestore(&recv_queue->lock, flags);\n\t\t\tib_dma_unmap_single(qp_info->port_priv->device,\n\t\t\t\t\t    mad_priv->header.mapping,\n\t\t\t\t\t    mad_priv_dma_size(mad_priv),\n\t\t\t\t\t    DMA_FROM_DEVICE);\n\t\t\tkfree(mad_priv);\n\t\t\tdev_err(&qp_info->port_priv->device->dev,\n\t\t\t\t\"ib_post_recv failed: %d\\n\", ret);\n\t\t\tbreak;\n\t\t}\n\t} while (post);\n\n\treturn ret;\n}\n\n \nstatic void cleanup_recv_queue(struct ib_mad_qp_info *qp_info)\n{\n\tstruct ib_mad_private_header *mad_priv_hdr;\n\tstruct ib_mad_private *recv;\n\tstruct ib_mad_list_head *mad_list;\n\n\tif (!qp_info->qp)\n\t\treturn;\n\n\twhile (!list_empty(&qp_info->recv_queue.list)) {\n\n\t\tmad_list = list_entry(qp_info->recv_queue.list.next,\n\t\t\t\t      struct ib_mad_list_head, list);\n\t\tmad_priv_hdr = container_of(mad_list,\n\t\t\t\t\t    struct ib_mad_private_header,\n\t\t\t\t\t    mad_list);\n\t\trecv = container_of(mad_priv_hdr, struct ib_mad_private,\n\t\t\t\t    header);\n\n\t\t \n\t\tlist_del(&mad_list->list);\n\n\t\tib_dma_unmap_single(qp_info->port_priv->device,\n\t\t\t\t    recv->header.mapping,\n\t\t\t\t    mad_priv_dma_size(recv),\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tkfree(recv);\n\t}\n\n\tqp_info->recv_queue.count = 0;\n}\n\n \nstatic int ib_mad_port_start(struct ib_mad_port_private *port_priv)\n{\n\tint ret, i;\n\tstruct ib_qp_attr *attr;\n\tstruct ib_qp *qp;\n\tu16 pkey_index;\n\n\tattr = kmalloc(sizeof *attr, GFP_KERNEL);\n\tif (!attr)\n\t\treturn -ENOMEM;\n\n\tret = ib_find_pkey(port_priv->device, port_priv->port_num,\n\t\t\t   IB_DEFAULT_PKEY_FULL, &pkey_index);\n\tif (ret)\n\t\tpkey_index = 0;\n\n\tfor (i = 0; i < IB_MAD_QPS_CORE; i++) {\n\t\tqp = port_priv->qp_info[i].qp;\n\t\tif (!qp)\n\t\t\tcontinue;\n\n\t\t \n\t\tattr->qp_state = IB_QPS_INIT;\n\t\tattr->pkey_index = pkey_index;\n\t\tattr->qkey = (qp->qp_num == 0) ? 0 : IB_QP1_QKEY;\n\t\tret = ib_modify_qp(qp, attr, IB_QP_STATE |\n\t\t\t\t\t     IB_QP_PKEY_INDEX | IB_QP_QKEY);\n\t\tif (ret) {\n\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\"Couldn't change QP%d state to INIT: %d\\n\",\n\t\t\t\ti, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr->qp_state = IB_QPS_RTR;\n\t\tret = ib_modify_qp(qp, attr, IB_QP_STATE);\n\t\tif (ret) {\n\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\"Couldn't change QP%d state to RTR: %d\\n\",\n\t\t\t\ti, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr->qp_state = IB_QPS_RTS;\n\t\tattr->sq_psn = IB_MAD_SEND_Q_PSN;\n\t\tret = ib_modify_qp(qp, attr, IB_QP_STATE | IB_QP_SQ_PSN);\n\t\tif (ret) {\n\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\"Couldn't change QP%d state to RTS: %d\\n\",\n\t\t\t\ti, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = ib_req_notify_cq(port_priv->cq, IB_CQ_NEXT_COMP);\n\tif (ret) {\n\t\tdev_err(&port_priv->device->dev,\n\t\t\t\"Failed to request completion notification: %d\\n\",\n\t\t\tret);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < IB_MAD_QPS_CORE; i++) {\n\t\tif (!port_priv->qp_info[i].qp)\n\t\t\tcontinue;\n\n\t\tret = ib_mad_post_receive_mads(&port_priv->qp_info[i], NULL);\n\t\tif (ret) {\n\t\t\tdev_err(&port_priv->device->dev,\n\t\t\t\t\"Couldn't post receive WRs\\n\");\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tkfree(attr);\n\treturn ret;\n}\n\nstatic void qp_event_handler(struct ib_event *event, void *qp_context)\n{\n\tstruct ib_mad_qp_info\t*qp_info = qp_context;\n\n\t \n\tdev_err(&qp_info->port_priv->device->dev,\n\t\t\"Fatal error (%d) on MAD QP (%u)\\n\",\n\t\tevent->event, qp_info->qp->qp_num);\n}\n\nstatic void init_mad_queue(struct ib_mad_qp_info *qp_info,\n\t\t\t   struct ib_mad_queue *mad_queue)\n{\n\tmad_queue->qp_info = qp_info;\n\tmad_queue->count = 0;\n\tspin_lock_init(&mad_queue->lock);\n\tINIT_LIST_HEAD(&mad_queue->list);\n}\n\nstatic void init_mad_qp(struct ib_mad_port_private *port_priv,\n\t\t\tstruct ib_mad_qp_info *qp_info)\n{\n\tqp_info->port_priv = port_priv;\n\tinit_mad_queue(qp_info, &qp_info->send_queue);\n\tinit_mad_queue(qp_info, &qp_info->recv_queue);\n\tINIT_LIST_HEAD(&qp_info->overflow_list);\n}\n\nstatic int create_mad_qp(struct ib_mad_qp_info *qp_info,\n\t\t\t enum ib_qp_type qp_type)\n{\n\tstruct ib_qp_init_attr\tqp_init_attr;\n\tint ret;\n\n\tmemset(&qp_init_attr, 0, sizeof qp_init_attr);\n\tqp_init_attr.send_cq = qp_info->port_priv->cq;\n\tqp_init_attr.recv_cq = qp_info->port_priv->cq;\n\tqp_init_attr.sq_sig_type = IB_SIGNAL_ALL_WR;\n\tqp_init_attr.cap.max_send_wr = mad_sendq_size;\n\tqp_init_attr.cap.max_recv_wr = mad_recvq_size;\n\tqp_init_attr.cap.max_send_sge = IB_MAD_SEND_REQ_MAX_SG;\n\tqp_init_attr.cap.max_recv_sge = IB_MAD_RECV_REQ_MAX_SG;\n\tqp_init_attr.qp_type = qp_type;\n\tqp_init_attr.port_num = qp_info->port_priv->port_num;\n\tqp_init_attr.qp_context = qp_info;\n\tqp_init_attr.event_handler = qp_event_handler;\n\tqp_info->qp = ib_create_qp(qp_info->port_priv->pd, &qp_init_attr);\n\tif (IS_ERR(qp_info->qp)) {\n\t\tdev_err(&qp_info->port_priv->device->dev,\n\t\t\t\"Couldn't create ib_mad QP%d\\n\",\n\t\t\tget_spl_qp_index(qp_type));\n\t\tret = PTR_ERR(qp_info->qp);\n\t\tgoto error;\n\t}\n\t \n\tqp_info->send_queue.max_active = mad_sendq_size;\n\tqp_info->recv_queue.max_active = mad_recvq_size;\n\treturn 0;\n\nerror:\n\treturn ret;\n}\n\nstatic void destroy_mad_qp(struct ib_mad_qp_info *qp_info)\n{\n\tif (!qp_info->qp)\n\t\treturn;\n\n\tib_destroy_qp(qp_info->qp);\n}\n\n \nstatic int ib_mad_port_open(struct ib_device *device,\n\t\t\t    u32 port_num)\n{\n\tint ret, cq_size;\n\tstruct ib_mad_port_private *port_priv;\n\tunsigned long flags;\n\tchar name[sizeof \"ib_mad123\"];\n\tint has_smi;\n\n\tif (WARN_ON(rdma_max_mad_size(device, port_num) < IB_MGMT_MAD_SIZE))\n\t\treturn -EFAULT;\n\n\tif (WARN_ON(rdma_cap_opa_mad(device, port_num) &&\n\t\t    rdma_max_mad_size(device, port_num) < OPA_MGMT_MAD_SIZE))\n\t\treturn -EFAULT;\n\n\t \n\tport_priv = kzalloc(sizeof *port_priv, GFP_KERNEL);\n\tif (!port_priv)\n\t\treturn -ENOMEM;\n\n\tport_priv->device = device;\n\tport_priv->port_num = port_num;\n\tspin_lock_init(&port_priv->reg_lock);\n\tinit_mad_qp(port_priv, &port_priv->qp_info[0]);\n\tinit_mad_qp(port_priv, &port_priv->qp_info[1]);\n\n\tcq_size = mad_sendq_size + mad_recvq_size;\n\thas_smi = rdma_cap_ib_smi(device, port_num);\n\tif (has_smi)\n\t\tcq_size *= 2;\n\n\tport_priv->pd = ib_alloc_pd(device, 0);\n\tif (IS_ERR(port_priv->pd)) {\n\t\tdev_err(&device->dev, \"Couldn't create ib_mad PD\\n\");\n\t\tret = PTR_ERR(port_priv->pd);\n\t\tgoto error3;\n\t}\n\n\tport_priv->cq = ib_alloc_cq(port_priv->device, port_priv, cq_size, 0,\n\t\t\tIB_POLL_UNBOUND_WORKQUEUE);\n\tif (IS_ERR(port_priv->cq)) {\n\t\tdev_err(&device->dev, \"Couldn't create ib_mad CQ\\n\");\n\t\tret = PTR_ERR(port_priv->cq);\n\t\tgoto error4;\n\t}\n\n\tif (has_smi) {\n\t\tret = create_mad_qp(&port_priv->qp_info[0], IB_QPT_SMI);\n\t\tif (ret)\n\t\t\tgoto error6;\n\t}\n\tret = create_mad_qp(&port_priv->qp_info[1], IB_QPT_GSI);\n\tif (ret)\n\t\tgoto error7;\n\n\tsnprintf(name, sizeof(name), \"ib_mad%u\", port_num);\n\tport_priv->wq = alloc_ordered_workqueue(name, WQ_MEM_RECLAIM);\n\tif (!port_priv->wq) {\n\t\tret = -ENOMEM;\n\t\tgoto error8;\n\t}\n\n\tspin_lock_irqsave(&ib_mad_port_list_lock, flags);\n\tlist_add_tail(&port_priv->port_list, &ib_mad_port_list);\n\tspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\n\n\tret = ib_mad_port_start(port_priv);\n\tif (ret) {\n\t\tdev_err(&device->dev, \"Couldn't start port\\n\");\n\t\tgoto error9;\n\t}\n\n\treturn 0;\n\nerror9:\n\tspin_lock_irqsave(&ib_mad_port_list_lock, flags);\n\tlist_del_init(&port_priv->port_list);\n\tspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\n\n\tdestroy_workqueue(port_priv->wq);\nerror8:\n\tdestroy_mad_qp(&port_priv->qp_info[1]);\nerror7:\n\tdestroy_mad_qp(&port_priv->qp_info[0]);\nerror6:\n\tib_free_cq(port_priv->cq);\n\tcleanup_recv_queue(&port_priv->qp_info[1]);\n\tcleanup_recv_queue(&port_priv->qp_info[0]);\nerror4:\n\tib_dealloc_pd(port_priv->pd);\nerror3:\n\tkfree(port_priv);\n\n\treturn ret;\n}\n\n \nstatic int ib_mad_port_close(struct ib_device *device, u32 port_num)\n{\n\tstruct ib_mad_port_private *port_priv;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ib_mad_port_list_lock, flags);\n\tport_priv = __ib_get_mad_port(device, port_num);\n\tif (port_priv == NULL) {\n\t\tspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\n\t\tdev_err(&device->dev, \"Port %u not found\\n\", port_num);\n\t\treturn -ENODEV;\n\t}\n\tlist_del_init(&port_priv->port_list);\n\tspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\n\n\tdestroy_workqueue(port_priv->wq);\n\tdestroy_mad_qp(&port_priv->qp_info[1]);\n\tdestroy_mad_qp(&port_priv->qp_info[0]);\n\tib_free_cq(port_priv->cq);\n\tib_dealloc_pd(port_priv->pd);\n\tcleanup_recv_queue(&port_priv->qp_info[1]);\n\tcleanup_recv_queue(&port_priv->qp_info[0]);\n\t \n\n\tkfree(port_priv);\n\n\treturn 0;\n}\n\nstatic int ib_mad_init_device(struct ib_device *device)\n{\n\tint start, i;\n\tunsigned int count = 0;\n\tint ret;\n\n\tstart = rdma_start_port(device);\n\n\tfor (i = start; i <= rdma_end_port(device); i++) {\n\t\tif (!rdma_cap_ib_mad(device, i))\n\t\t\tcontinue;\n\n\t\tret = ib_mad_port_open(device, i);\n\t\tif (ret) {\n\t\t\tdev_err(&device->dev, \"Couldn't open port %d\\n\", i);\n\t\t\tgoto error;\n\t\t}\n\t\tret = ib_agent_port_open(device, i);\n\t\tif (ret) {\n\t\t\tdev_err(&device->dev,\n\t\t\t\t\"Couldn't open port %d for agents\\n\", i);\n\t\t\tgoto error_agent;\n\t\t}\n\t\tcount++;\n\t}\n\tif (!count)\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n\nerror_agent:\n\tif (ib_mad_port_close(device, i))\n\t\tdev_err(&device->dev, \"Couldn't close port %d\\n\", i);\n\nerror:\n\twhile (--i >= start) {\n\t\tif (!rdma_cap_ib_mad(device, i))\n\t\t\tcontinue;\n\n\t\tif (ib_agent_port_close(device, i))\n\t\t\tdev_err(&device->dev,\n\t\t\t\t\"Couldn't close port %d for agents\\n\", i);\n\t\tif (ib_mad_port_close(device, i))\n\t\t\tdev_err(&device->dev, \"Couldn't close port %d\\n\", i);\n\t}\n\treturn ret;\n}\n\nstatic void ib_mad_remove_device(struct ib_device *device, void *client_data)\n{\n\tunsigned int i;\n\n\trdma_for_each_port (device, i) {\n\t\tif (!rdma_cap_ib_mad(device, i))\n\t\t\tcontinue;\n\n\t\tif (ib_agent_port_close(device, i))\n\t\t\tdev_err(&device->dev,\n\t\t\t\t\"Couldn't close port %u for agents\\n\", i);\n\t\tif (ib_mad_port_close(device, i))\n\t\t\tdev_err(&device->dev, \"Couldn't close port %u\\n\", i);\n\t}\n}\n\nstatic struct ib_client mad_client = {\n\t.name   = \"mad\",\n\t.add = ib_mad_init_device,\n\t.remove = ib_mad_remove_device\n};\n\nint ib_mad_init(void)\n{\n\tmad_recvq_size = min(mad_recvq_size, IB_MAD_QP_MAX_SIZE);\n\tmad_recvq_size = max(mad_recvq_size, IB_MAD_QP_MIN_SIZE);\n\n\tmad_sendq_size = min(mad_sendq_size, IB_MAD_QP_MAX_SIZE);\n\tmad_sendq_size = max(mad_sendq_size, IB_MAD_QP_MIN_SIZE);\n\n\tINIT_LIST_HEAD(&ib_mad_port_list);\n\n\tif (ib_register_client(&mad_client)) {\n\t\tpr_err(\"Couldn't register ib_mad client\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid ib_mad_cleanup(void)\n{\n\tib_unregister_client(&mad_client);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}