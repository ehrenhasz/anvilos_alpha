{
  "module_name": "device.c",
  "hash_id": "a5e292011f740f1f6fe9274ecd370a91f60e3436a4856a71c192e4e10dad86eb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/core/device.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/netdevice.h>\n#include <net/net_namespace.h>\n#include <linux/security.h>\n#include <linux/notifier.h>\n#include <linux/hashtable.h>\n#include <rdma/rdma_netlink.h>\n#include <rdma/ib_addr.h>\n#include <rdma/ib_cache.h>\n#include <rdma/rdma_counter.h>\n\n#include \"core_priv.h\"\n#include \"restrack.h\"\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"core kernel InfiniBand API\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nstruct workqueue_struct *ib_comp_wq;\nstruct workqueue_struct *ib_comp_unbound_wq;\nstruct workqueue_struct *ib_wq;\nEXPORT_SYMBOL_GPL(ib_wq);\nstatic struct workqueue_struct *ib_unreg_wq;\n\n \n\n \nstatic DEFINE_XARRAY_FLAGS(devices, XA_FLAGS_ALLOC);\nstatic DECLARE_RWSEM(devices_rwsem);\n#define DEVICE_REGISTERED XA_MARK_1\n\nstatic u32 highest_client_id;\n#define CLIENT_REGISTERED XA_MARK_1\nstatic DEFINE_XARRAY_FLAGS(clients, XA_FLAGS_ALLOC);\nstatic DECLARE_RWSEM(clients_rwsem);\n\nstatic void ib_client_put(struct ib_client *client)\n{\n\tif (refcount_dec_and_test(&client->uses))\n\t\tcomplete(&client->uses_zero);\n}\n\n \n#define CLIENT_DATA_REGISTERED XA_MARK_1\n\nunsigned int rdma_dev_net_id;\n\n \nstatic DEFINE_XARRAY_FLAGS(rdma_nets, XA_FLAGS_ALLOC);\n \nstatic DECLARE_RWSEM(rdma_nets_rwsem);\n\nbool ib_devices_shared_netns = true;\nmodule_param_named(netns_mode, ib_devices_shared_netns, bool, 0444);\nMODULE_PARM_DESC(netns_mode,\n\t\t \"Share device among net namespaces; default=1 (shared)\");\n \nbool rdma_dev_access_netns(const struct ib_device *dev, const struct net *net)\n{\n\treturn (ib_devices_shared_netns ||\n\t\tnet_eq(read_pnet(&dev->coredev.rdma_net), net));\n}\nEXPORT_SYMBOL(rdma_dev_access_netns);\n\n \nstatic void *xan_find_marked(struct xarray *xa, unsigned long *indexp,\n\t\t\t     xa_mark_t filter)\n{\n\tXA_STATE(xas, xa, *indexp);\n\tvoid *entry;\n\n\trcu_read_lock();\n\tdo {\n\t\tentry = xas_find_marked(&xas, ULONG_MAX, filter);\n\t\tif (xa_is_zero(entry))\n\t\t\tbreak;\n\t} while (xas_retry(&xas, entry));\n\trcu_read_unlock();\n\n\tif (entry) {\n\t\t*indexp = xas.xa_index;\n\t\tif (xa_is_zero(entry))\n\t\t\treturn NULL;\n\t\treturn entry;\n\t}\n\treturn XA_ERROR(-ENOENT);\n}\n#define xan_for_each_marked(xa, index, entry, filter)                          \\\n\tfor (index = 0, entry = xan_find_marked(xa, &(index), filter);         \\\n\t     !xa_is_err(entry);                                                \\\n\t     (index)++, entry = xan_find_marked(xa, &(index), filter))\n\n \nstatic DEFINE_SPINLOCK(ndev_hash_lock);\nstatic DECLARE_HASHTABLE(ndev_hash, 5);\n\nstatic void free_netdevs(struct ib_device *ib_dev);\nstatic void ib_unregister_work(struct work_struct *work);\nstatic void __ib_unregister_device(struct ib_device *device);\nstatic int ib_security_change(struct notifier_block *nb, unsigned long event,\n\t\t\t      void *lsm_data);\nstatic void ib_policy_change_task(struct work_struct *work);\nstatic DECLARE_WORK(ib_policy_change_work, ib_policy_change_task);\n\nstatic void __ibdev_printk(const char *level, const struct ib_device *ibdev,\n\t\t\t   struct va_format *vaf)\n{\n\tif (ibdev && ibdev->dev.parent)\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tibdev->dev.parent,\n\t\t\t\t\"%s %s %s: %pV\",\n\t\t\t\tdev_driver_string(ibdev->dev.parent),\n\t\t\t\tdev_name(ibdev->dev.parent),\n\t\t\t\tdev_name(&ibdev->dev),\n\t\t\t\tvaf);\n\telse if (ibdev)\n\t\tprintk(\"%s%s: %pV\",\n\t\t       level, dev_name(&ibdev->dev), vaf);\n\telse\n\t\tprintk(\"%s(NULL ib_device): %pV\", level, vaf);\n}\n\nvoid ibdev_printk(const char *level, const struct ib_device *ibdev,\n\t\t  const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__ibdev_printk(level, ibdev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(ibdev_printk);\n\n#define define_ibdev_printk_level(func, level)                  \\\nvoid func(const struct ib_device *ibdev, const char *fmt, ...)  \\\n{                                                               \\\n\tstruct va_format vaf;                                   \\\n\tva_list args;                                           \\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);                                    \\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;                                          \\\n\tvaf.va = &args;                                         \\\n\t\t\t\t\t\t\t\t\\\n\t__ibdev_printk(level, ibdev, &vaf);                     \\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);                                           \\\n}                                                               \\\nEXPORT_SYMBOL(func);\n\ndefine_ibdev_printk_level(ibdev_emerg, KERN_EMERG);\ndefine_ibdev_printk_level(ibdev_alert, KERN_ALERT);\ndefine_ibdev_printk_level(ibdev_crit, KERN_CRIT);\ndefine_ibdev_printk_level(ibdev_err, KERN_ERR);\ndefine_ibdev_printk_level(ibdev_warn, KERN_WARNING);\ndefine_ibdev_printk_level(ibdev_notice, KERN_NOTICE);\ndefine_ibdev_printk_level(ibdev_info, KERN_INFO);\n\nstatic struct notifier_block ibdev_lsm_nb = {\n\t.notifier_call = ib_security_change,\n};\n\nstatic int rdma_dev_change_netns(struct ib_device *device, struct net *cur_net,\n\t\t\t\t struct net *net);\n\n \nstruct ib_port_data_rcu {\n\tstruct rcu_head rcu_head;\n\tstruct ib_port_data pdata[];\n};\n\nstatic void ib_device_check_mandatory(struct ib_device *device)\n{\n#define IB_MANDATORY_FUNC(x) { offsetof(struct ib_device_ops, x), #x }\n\tstatic const struct {\n\t\tsize_t offset;\n\t\tchar  *name;\n\t} mandatory_table[] = {\n\t\tIB_MANDATORY_FUNC(query_device),\n\t\tIB_MANDATORY_FUNC(query_port),\n\t\tIB_MANDATORY_FUNC(alloc_pd),\n\t\tIB_MANDATORY_FUNC(dealloc_pd),\n\t\tIB_MANDATORY_FUNC(create_qp),\n\t\tIB_MANDATORY_FUNC(modify_qp),\n\t\tIB_MANDATORY_FUNC(destroy_qp),\n\t\tIB_MANDATORY_FUNC(post_send),\n\t\tIB_MANDATORY_FUNC(post_recv),\n\t\tIB_MANDATORY_FUNC(create_cq),\n\t\tIB_MANDATORY_FUNC(destroy_cq),\n\t\tIB_MANDATORY_FUNC(poll_cq),\n\t\tIB_MANDATORY_FUNC(req_notify_cq),\n\t\tIB_MANDATORY_FUNC(get_dma_mr),\n\t\tIB_MANDATORY_FUNC(reg_user_mr),\n\t\tIB_MANDATORY_FUNC(dereg_mr),\n\t\tIB_MANDATORY_FUNC(get_port_immutable)\n\t};\n\tint i;\n\n\tdevice->kverbs_provider = true;\n\tfor (i = 0; i < ARRAY_SIZE(mandatory_table); ++i) {\n\t\tif (!*(void **) ((void *) &device->ops +\n\t\t\t\t mandatory_table[i].offset)) {\n\t\t\tdevice->kverbs_provider = false;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstruct ib_device *ib_device_get_by_index(const struct net *net, u32 index)\n{\n\tstruct ib_device *device;\n\n\tdown_read(&devices_rwsem);\n\tdevice = xa_load(&devices, index);\n\tif (device) {\n\t\tif (!rdma_dev_access_netns(device, net)) {\n\t\t\tdevice = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!ib_device_try_get(device))\n\t\t\tdevice = NULL;\n\t}\nout:\n\tup_read(&devices_rwsem);\n\treturn device;\n}\n\n \nvoid ib_device_put(struct ib_device *device)\n{\n\tif (refcount_dec_and_test(&device->refcount))\n\t\tcomplete(&device->unreg_completion);\n}\nEXPORT_SYMBOL(ib_device_put);\n\nstatic struct ib_device *__ib_device_get_by_name(const char *name)\n{\n\tstruct ib_device *device;\n\tunsigned long index;\n\n\txa_for_each (&devices, index, device)\n\t\tif (!strcmp(name, dev_name(&device->dev)))\n\t\t\treturn device;\n\n\treturn NULL;\n}\n\n \nstruct ib_device *ib_device_get_by_name(const char *name,\n\t\t\t\t\tenum rdma_driver_id driver_id)\n{\n\tstruct ib_device *device;\n\n\tdown_read(&devices_rwsem);\n\tdevice = __ib_device_get_by_name(name);\n\tif (device && driver_id != RDMA_DRIVER_UNKNOWN &&\n\t    device->ops.driver_id != driver_id)\n\t\tdevice = NULL;\n\n\tif (device) {\n\t\tif (!ib_device_try_get(device))\n\t\t\tdevice = NULL;\n\t}\n\tup_read(&devices_rwsem);\n\treturn device;\n}\nEXPORT_SYMBOL(ib_device_get_by_name);\n\nstatic int rename_compat_devs(struct ib_device *device)\n{\n\tstruct ib_core_device *cdev;\n\tunsigned long index;\n\tint ret = 0;\n\n\tmutex_lock(&device->compat_devs_mutex);\n\txa_for_each (&device->compat_devs, index, cdev) {\n\t\tret = device_rename(&cdev->dev, dev_name(&device->dev));\n\t\tif (ret) {\n\t\t\tdev_warn(&cdev->dev,\n\t\t\t\t \"Fail to rename compatdev to new name %s\\n\",\n\t\t\t\t dev_name(&device->dev));\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&device->compat_devs_mutex);\n\treturn ret;\n}\n\nint ib_device_rename(struct ib_device *ibdev, const char *name)\n{\n\tunsigned long index;\n\tvoid *client_data;\n\tint ret;\n\n\tdown_write(&devices_rwsem);\n\tif (!strcmp(name, dev_name(&ibdev->dev))) {\n\t\tup_write(&devices_rwsem);\n\t\treturn 0;\n\t}\n\n\tif (__ib_device_get_by_name(name)) {\n\t\tup_write(&devices_rwsem);\n\t\treturn -EEXIST;\n\t}\n\n\tret = device_rename(&ibdev->dev, name);\n\tif (ret) {\n\t\tup_write(&devices_rwsem);\n\t\treturn ret;\n\t}\n\n\tstrscpy(ibdev->name, name, IB_DEVICE_NAME_MAX);\n\tret = rename_compat_devs(ibdev);\n\n\tdowngrade_write(&devices_rwsem);\n\tdown_read(&ibdev->client_data_rwsem);\n\txan_for_each_marked(&ibdev->client_data, index, client_data,\n\t\t\t    CLIENT_DATA_REGISTERED) {\n\t\tstruct ib_client *client = xa_load(&clients, index);\n\n\t\tif (!client || !client->rename)\n\t\t\tcontinue;\n\n\t\tclient->rename(ibdev, client_data);\n\t}\n\tup_read(&ibdev->client_data_rwsem);\n\tup_read(&devices_rwsem);\n\treturn 0;\n}\n\nint ib_device_set_dim(struct ib_device *ibdev, u8 use_dim)\n{\n\tif (use_dim > 1)\n\t\treturn -EINVAL;\n\tibdev->use_cq_dim = use_dim;\n\n\treturn 0;\n}\n\nstatic int alloc_name(struct ib_device *ibdev, const char *name)\n{\n\tstruct ib_device *device;\n\tunsigned long index;\n\tstruct ida inuse;\n\tint rc;\n\tint i;\n\n\tlockdep_assert_held_write(&devices_rwsem);\n\tida_init(&inuse);\n\txa_for_each (&devices, index, device) {\n\t\tchar buf[IB_DEVICE_NAME_MAX];\n\n\t\tif (sscanf(dev_name(&device->dev), name, &i) != 1)\n\t\t\tcontinue;\n\t\tif (i < 0 || i >= INT_MAX)\n\t\t\tcontinue;\n\t\tsnprintf(buf, sizeof buf, name, i);\n\t\tif (strcmp(buf, dev_name(&device->dev)) != 0)\n\t\t\tcontinue;\n\n\t\trc = ida_alloc_range(&inuse, i, i, GFP_KERNEL);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t}\n\n\trc = ida_alloc(&inuse, GFP_KERNEL);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trc = dev_set_name(&ibdev->dev, name, rc);\nout:\n\tida_destroy(&inuse);\n\treturn rc;\n}\n\nstatic void ib_device_release(struct device *device)\n{\n\tstruct ib_device *dev = container_of(device, struct ib_device, dev);\n\n\tfree_netdevs(dev);\n\tWARN_ON(refcount_read(&dev->refcount));\n\tif (dev->hw_stats_data)\n\t\tib_device_release_hw_stats(dev->hw_stats_data);\n\tif (dev->port_data) {\n\t\tib_cache_release_one(dev);\n\t\tib_security_release_port_pkey_list(dev);\n\t\trdma_counter_release(dev);\n\t\tkfree_rcu(container_of(dev->port_data, struct ib_port_data_rcu,\n\t\t\t\t       pdata[0]),\n\t\t\t  rcu_head);\n\t}\n\n\tmutex_destroy(&dev->unregistration_lock);\n\tmutex_destroy(&dev->compat_devs_mutex);\n\n\txa_destroy(&dev->compat_devs);\n\txa_destroy(&dev->client_data);\n\tkfree_rcu(dev, rcu_head);\n}\n\nstatic int ib_device_uevent(const struct device *device,\n\t\t\t    struct kobj_uevent_env *env)\n{\n\tif (add_uevent_var(env, \"NAME=%s\", dev_name(device)))\n\t\treturn -ENOMEM;\n\n\t \n\n\treturn 0;\n}\n\nstatic const void *net_namespace(const struct device *d)\n{\n\tconst struct ib_core_device *coredev =\n\t\t\tcontainer_of(d, struct ib_core_device, dev);\n\n\treturn read_pnet(&coredev->rdma_net);\n}\n\nstatic struct class ib_class = {\n\t.name    = \"infiniband\",\n\t.dev_release = ib_device_release,\n\t.dev_uevent = ib_device_uevent,\n\t.ns_type = &net_ns_type_operations,\n\t.namespace = net_namespace,\n};\n\nstatic void rdma_init_coredev(struct ib_core_device *coredev,\n\t\t\t      struct ib_device *dev, struct net *net)\n{\n\t \n\tBUILD_BUG_ON(offsetof(struct ib_device, coredev.dev) !=\n\t\t     offsetof(struct ib_device, dev));\n\n\tcoredev->dev.class = &ib_class;\n\tcoredev->dev.groups = dev->groups;\n\tdevice_initialize(&coredev->dev);\n\tcoredev->owner = dev;\n\tINIT_LIST_HEAD(&coredev->port_list);\n\twrite_pnet(&coredev->rdma_net, net);\n}\n\n \nstruct ib_device *_ib_alloc_device(size_t size)\n{\n\tstruct ib_device *device;\n\tunsigned int i;\n\n\tif (WARN_ON(size < sizeof(struct ib_device)))\n\t\treturn NULL;\n\n\tdevice = kzalloc(size, GFP_KERNEL);\n\tif (!device)\n\t\treturn NULL;\n\n\tif (rdma_restrack_init(device)) {\n\t\tkfree(device);\n\t\treturn NULL;\n\t}\n\n\trdma_init_coredev(&device->coredev, device, &init_net);\n\n\tINIT_LIST_HEAD(&device->event_handler_list);\n\tspin_lock_init(&device->qp_open_list_lock);\n\tinit_rwsem(&device->event_handler_rwsem);\n\tmutex_init(&device->unregistration_lock);\n\t \n\txa_init_flags(&device->client_data, XA_FLAGS_ALLOC);\n\tinit_rwsem(&device->client_data_rwsem);\n\txa_init_flags(&device->compat_devs, XA_FLAGS_ALLOC);\n\tmutex_init(&device->compat_devs_mutex);\n\tinit_completion(&device->unreg_completion);\n\tINIT_WORK(&device->unregistration_work, ib_unregister_work);\n\n\tspin_lock_init(&device->cq_pools_lock);\n\tfor (i = 0; i < ARRAY_SIZE(device->cq_pools); i++)\n\t\tINIT_LIST_HEAD(&device->cq_pools[i]);\n\n\trwlock_init(&device->cache_lock);\n\n\tdevice->uverbs_cmd_mask =\n\t\tBIT_ULL(IB_USER_VERBS_CMD_ALLOC_MW) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_ALLOC_PD) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_ATTACH_MCAST) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CLOSE_XRCD) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_AH) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_CQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_QP) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_SRQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_CREATE_XSRQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DEALLOC_MW) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DEALLOC_PD) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DEREG_MR) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DESTROY_AH) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DESTROY_CQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DESTROY_QP) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DESTROY_SRQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_DETACH_MCAST) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_GET_CONTEXT) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_MODIFY_QP) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_MODIFY_SRQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_OPEN_QP) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_OPEN_XRCD) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_QUERY_DEVICE) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_QUERY_PORT) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_QUERY_QP) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_QUERY_SRQ) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_REG_MR) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_REREG_MR) |\n\t\tBIT_ULL(IB_USER_VERBS_CMD_RESIZE_CQ);\n\treturn device;\n}\nEXPORT_SYMBOL(_ib_alloc_device);\n\n \nvoid ib_dealloc_device(struct ib_device *device)\n{\n\tif (device->ops.dealloc_driver)\n\t\tdevice->ops.dealloc_driver(device);\n\n\t \n\tdown_write(&devices_rwsem);\n\tif (xa_load(&devices, device->index) == device)\n\t\txa_erase(&devices, device->index);\n\tup_write(&devices_rwsem);\n\n\t \n\tfree_netdevs(device);\n\n\tWARN_ON(!xa_empty(&device->compat_devs));\n\tWARN_ON(!xa_empty(&device->client_data));\n\tWARN_ON(refcount_read(&device->refcount));\n\trdma_restrack_clean(device);\n\t \n\tput_device(&device->dev);\n}\nEXPORT_SYMBOL(ib_dealloc_device);\n\n \nstatic int add_client_context(struct ib_device *device,\n\t\t\t      struct ib_client *client)\n{\n\tint ret = 0;\n\n\tif (!device->kverbs_provider && !client->no_kverbs_req)\n\t\treturn 0;\n\n\tdown_write(&device->client_data_rwsem);\n\t \n\tif (!refcount_inc_not_zero(&client->uses))\n\t\tgoto out_unlock;\n\trefcount_inc(&device->refcount);\n\n\t \n\tif (xa_get_mark(&device->client_data, client->client_id,\n\t\t    CLIENT_DATA_REGISTERED))\n\t\tgoto out;\n\n\tret = xa_err(xa_store(&device->client_data, client->client_id, NULL,\n\t\t\t      GFP_KERNEL));\n\tif (ret)\n\t\tgoto out;\n\tdowngrade_write(&device->client_data_rwsem);\n\tif (client->add) {\n\t\tif (client->add(device)) {\n\t\t\t \n\t\t\txa_erase(&device->client_data, client->client_id);\n\t\t\tup_read(&device->client_data_rwsem);\n\t\t\tib_device_put(device);\n\t\t\tib_client_put(client);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\txa_set_mark(&device->client_data, client->client_id,\n\t\t    CLIENT_DATA_REGISTERED);\n\tup_read(&device->client_data_rwsem);\n\treturn 0;\n\nout:\n\tib_device_put(device);\n\tib_client_put(client);\nout_unlock:\n\tup_write(&device->client_data_rwsem);\n\treturn ret;\n}\n\nstatic void remove_client_context(struct ib_device *device,\n\t\t\t\t  unsigned int client_id)\n{\n\tstruct ib_client *client;\n\tvoid *client_data;\n\n\tdown_write(&device->client_data_rwsem);\n\tif (!xa_get_mark(&device->client_data, client_id,\n\t\t\t CLIENT_DATA_REGISTERED)) {\n\t\tup_write(&device->client_data_rwsem);\n\t\treturn;\n\t}\n\tclient_data = xa_load(&device->client_data, client_id);\n\txa_clear_mark(&device->client_data, client_id, CLIENT_DATA_REGISTERED);\n\tclient = xa_load(&clients, client_id);\n\tup_write(&device->client_data_rwsem);\n\n\t \n\tif (client->remove)\n\t\tclient->remove(device, client_data);\n\n\txa_erase(&device->client_data, client_id);\n\tib_device_put(device);\n\tib_client_put(client);\n}\n\nstatic int alloc_port_data(struct ib_device *device)\n{\n\tstruct ib_port_data_rcu *pdata_rcu;\n\tu32 port;\n\n\tif (device->port_data)\n\t\treturn 0;\n\n\t \n\tif (WARN_ON(!device->phys_port_cnt))\n\t\treturn -EINVAL;\n\n\t \n\tif (WARN_ON(device->phys_port_cnt == U32_MAX))\n\t\treturn -EINVAL;\n\n\t \n\tpdata_rcu = kzalloc(struct_size(pdata_rcu, pdata,\n\t\t\t\t\tsize_add(rdma_end_port(device), 1)),\n\t\t\t    GFP_KERNEL);\n\tif (!pdata_rcu)\n\t\treturn -ENOMEM;\n\t \n\tdevice->port_data = pdata_rcu->pdata;\n\n\trdma_for_each_port (device, port) {\n\t\tstruct ib_port_data *pdata = &device->port_data[port];\n\n\t\tpdata->ib_dev = device;\n\t\tspin_lock_init(&pdata->pkey_list_lock);\n\t\tINIT_LIST_HEAD(&pdata->pkey_list);\n\t\tspin_lock_init(&pdata->netdev_lock);\n\t\tINIT_HLIST_NODE(&pdata->ndev_hash_link);\n\t}\n\treturn 0;\n}\n\nstatic int verify_immutable(const struct ib_device *dev, u32 port)\n{\n\treturn WARN_ON(!rdma_cap_ib_mad(dev, port) &&\n\t\t\t    rdma_max_mad_size(dev, port) != 0);\n}\n\nstatic int setup_port_data(struct ib_device *device)\n{\n\tu32 port;\n\tint ret;\n\n\tret = alloc_port_data(device);\n\tif (ret)\n\t\treturn ret;\n\n\trdma_for_each_port (device, port) {\n\t\tstruct ib_port_data *pdata = &device->port_data[port];\n\n\t\tret = device->ops.get_port_immutable(device, port,\n\t\t\t\t\t\t     &pdata->immutable);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (verify_immutable(device, port))\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nconst struct ib_port_immutable*\nib_port_immutable_read(struct ib_device *dev, unsigned int port)\n{\n\tWARN_ON(!rdma_is_port_valid(dev, port));\n\treturn &dev->port_data[port].immutable;\n}\nEXPORT_SYMBOL(ib_port_immutable_read);\n\nvoid ib_get_device_fw_str(struct ib_device *dev, char *str)\n{\n\tif (dev->ops.get_dev_fw_str)\n\t\tdev->ops.get_dev_fw_str(dev, str);\n\telse\n\t\tstr[0] = '\\0';\n}\nEXPORT_SYMBOL(ib_get_device_fw_str);\n\nstatic void ib_policy_change_task(struct work_struct *work)\n{\n\tstruct ib_device *dev;\n\tunsigned long index;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {\n\t\tunsigned int i;\n\n\t\trdma_for_each_port (dev, i) {\n\t\t\tu64 sp;\n\t\t\tib_get_cached_subnet_prefix(dev, i, &sp);\n\t\t\tib_security_cache_change(dev, i, sp);\n\t\t}\n\t}\n\tup_read(&devices_rwsem);\n}\n\nstatic int ib_security_change(struct notifier_block *nb, unsigned long event,\n\t\t\t      void *lsm_data)\n{\n\tif (event != LSM_POLICY_CHANGE)\n\t\treturn NOTIFY_DONE;\n\n\tschedule_work(&ib_policy_change_work);\n\tib_mad_agent_security_change();\n\n\treturn NOTIFY_OK;\n}\n\nstatic void compatdev_release(struct device *dev)\n{\n\tstruct ib_core_device *cdev =\n\t\tcontainer_of(dev, struct ib_core_device, dev);\n\n\tkfree(cdev);\n}\n\nstatic int add_one_compat_dev(struct ib_device *device,\n\t\t\t      struct rdma_dev_net *rnet)\n{\n\tstruct ib_core_device *cdev;\n\tint ret;\n\n\tlockdep_assert_held(&rdma_nets_rwsem);\n\tif (!ib_devices_shared_netns)\n\t\treturn 0;\n\n\t \n\tif (net_eq(read_pnet(&rnet->net),\n\t\t   read_pnet(&device->coredev.rdma_net)))\n\t\treturn 0;\n\n\t \n\tmutex_lock(&device->compat_devs_mutex);\n\tcdev = xa_load(&device->compat_devs, rnet->id);\n\tif (cdev) {\n\t\tret = 0;\n\t\tgoto done;\n\t}\n\tret = xa_reserve(&device->compat_devs, rnet->id, GFP_KERNEL);\n\tif (ret)\n\t\tgoto done;\n\n\tcdev = kzalloc(sizeof(*cdev), GFP_KERNEL);\n\tif (!cdev) {\n\t\tret = -ENOMEM;\n\t\tgoto cdev_err;\n\t}\n\n\tcdev->dev.parent = device->dev.parent;\n\trdma_init_coredev(cdev, device, read_pnet(&rnet->net));\n\tcdev->dev.release = compatdev_release;\n\tret = dev_set_name(&cdev->dev, \"%s\", dev_name(&device->dev));\n\tif (ret)\n\t\tgoto add_err;\n\n\tret = device_add(&cdev->dev);\n\tif (ret)\n\t\tgoto add_err;\n\tret = ib_setup_port_attrs(cdev);\n\tif (ret)\n\t\tgoto port_err;\n\n\tret = xa_err(xa_store(&device->compat_devs, rnet->id,\n\t\t\t      cdev, GFP_KERNEL));\n\tif (ret)\n\t\tgoto insert_err;\n\n\tmutex_unlock(&device->compat_devs_mutex);\n\treturn 0;\n\ninsert_err:\n\tib_free_port_attrs(cdev);\nport_err:\n\tdevice_del(&cdev->dev);\nadd_err:\n\tput_device(&cdev->dev);\ncdev_err:\n\txa_release(&device->compat_devs, rnet->id);\ndone:\n\tmutex_unlock(&device->compat_devs_mutex);\n\treturn ret;\n}\n\nstatic void remove_one_compat_dev(struct ib_device *device, u32 id)\n{\n\tstruct ib_core_device *cdev;\n\n\tmutex_lock(&device->compat_devs_mutex);\n\tcdev = xa_erase(&device->compat_devs, id);\n\tmutex_unlock(&device->compat_devs_mutex);\n\tif (cdev) {\n\t\tib_free_port_attrs(cdev);\n\t\tdevice_del(&cdev->dev);\n\t\tput_device(&cdev->dev);\n\t}\n}\n\nstatic void remove_compat_devs(struct ib_device *device)\n{\n\tstruct ib_core_device *cdev;\n\tunsigned long index;\n\n\txa_for_each (&device->compat_devs, index, cdev)\n\t\tremove_one_compat_dev(device, index);\n}\n\nstatic int add_compat_devs(struct ib_device *device)\n{\n\tstruct rdma_dev_net *rnet;\n\tunsigned long index;\n\tint ret = 0;\n\n\tlockdep_assert_held(&devices_rwsem);\n\n\tdown_read(&rdma_nets_rwsem);\n\txa_for_each (&rdma_nets, index, rnet) {\n\t\tret = add_one_compat_dev(device, rnet);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tup_read(&rdma_nets_rwsem);\n\treturn ret;\n}\n\nstatic void remove_all_compat_devs(void)\n{\n\tstruct ib_compat_device *cdev;\n\tstruct ib_device *dev;\n\tunsigned long index;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each (&devices, index, dev) {\n\t\tunsigned long c_index = 0;\n\n\t\t \n\t\tdown_read(&rdma_nets_rwsem);\n\t\txa_for_each (&dev->compat_devs, c_index, cdev)\n\t\t\tremove_one_compat_dev(dev, c_index);\n\t\tup_read(&rdma_nets_rwsem);\n\t}\n\tup_read(&devices_rwsem);\n}\n\nstatic int add_all_compat_devs(void)\n{\n\tstruct rdma_dev_net *rnet;\n\tstruct ib_device *dev;\n\tunsigned long index;\n\tint ret = 0;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {\n\t\tunsigned long net_index = 0;\n\n\t\t \n\t\tdown_read(&rdma_nets_rwsem);\n\t\txa_for_each (&rdma_nets, net_index, rnet) {\n\t\t\tret = add_one_compat_dev(dev, rnet);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tup_read(&rdma_nets_rwsem);\n\t}\n\tup_read(&devices_rwsem);\n\tif (ret)\n\t\tremove_all_compat_devs();\n\treturn ret;\n}\n\nint rdma_compatdev_set(u8 enable)\n{\n\tstruct rdma_dev_net *rnet;\n\tunsigned long index;\n\tint ret = 0;\n\n\tdown_write(&rdma_nets_rwsem);\n\tif (ib_devices_shared_netns == enable) {\n\t\tup_write(&rdma_nets_rwsem);\n\t\treturn 0;\n\t}\n\n\t \n\txa_for_each (&rdma_nets, index, rnet) {\n\t\tret++;\n\t\tbreak;\n\t}\n\tif (!ret)\n\t\tib_devices_shared_netns = enable;\n\tup_write(&rdma_nets_rwsem);\n\tif (ret)\n\t\treturn -EBUSY;\n\n\tif (enable)\n\t\tret = add_all_compat_devs();\n\telse\n\t\tremove_all_compat_devs();\n\treturn ret;\n}\n\nstatic void rdma_dev_exit_net(struct net *net)\n{\n\tstruct rdma_dev_net *rnet = rdma_net_to_dev_net(net);\n\tstruct ib_device *dev;\n\tunsigned long index;\n\tint ret;\n\n\tdown_write(&rdma_nets_rwsem);\n\t \n\tret = xa_err(xa_store(&rdma_nets, rnet->id, NULL, GFP_KERNEL));\n\tWARN_ON(ret);\n\tup_write(&rdma_nets_rwsem);\n\n\tdown_read(&devices_rwsem);\n\txa_for_each (&devices, index, dev) {\n\t\tget_device(&dev->dev);\n\t\t \n\t\tup_read(&devices_rwsem);\n\n\t\tremove_one_compat_dev(dev, rnet->id);\n\n\t\t \n\t\trdma_dev_change_netns(dev, net, &init_net);\n\n\t\tput_device(&dev->dev);\n\t\tdown_read(&devices_rwsem);\n\t}\n\tup_read(&devices_rwsem);\n\n\trdma_nl_net_exit(rnet);\n\txa_erase(&rdma_nets, rnet->id);\n}\n\nstatic __net_init int rdma_dev_init_net(struct net *net)\n{\n\tstruct rdma_dev_net *rnet = rdma_net_to_dev_net(net);\n\tunsigned long index;\n\tstruct ib_device *dev;\n\tint ret;\n\n\twrite_pnet(&rnet->net, net);\n\n\tret = rdma_nl_net_init(rnet);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (net_eq(net, &init_net))\n\t\treturn 0;\n\n\tret = xa_alloc(&rdma_nets, &rnet->id, rnet, xa_limit_32b, GFP_KERNEL);\n\tif (ret) {\n\t\trdma_nl_net_exit(rnet);\n\t\treturn ret;\n\t}\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {\n\t\t \n\t\tdown_read(&rdma_nets_rwsem);\n\t\tret = add_one_compat_dev(dev, rnet);\n\t\tup_read(&rdma_nets_rwsem);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tup_read(&devices_rwsem);\n\n\tif (ret)\n\t\trdma_dev_exit_net(net);\n\n\treturn ret;\n}\n\n \nstatic int assign_name(struct ib_device *device, const char *name)\n{\n\tstatic u32 last_id;\n\tint ret;\n\n\tdown_write(&devices_rwsem);\n\t \n\tif (strchr(name, '%'))\n\t\tret = alloc_name(device, name);\n\telse\n\t\tret = dev_set_name(&device->dev, name);\n\tif (ret)\n\t\tgoto out;\n\n\tif (__ib_device_get_by_name(dev_name(&device->dev))) {\n\t\tret = -ENFILE;\n\t\tgoto out;\n\t}\n\tstrscpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);\n\n\tret = xa_alloc_cyclic(&devices, &device->index, device, xa_limit_31b,\n\t\t\t&last_id, GFP_KERNEL);\n\tif (ret > 0)\n\t\tret = 0;\n\nout:\n\tup_write(&devices_rwsem);\n\treturn ret;\n}\n\n \nstatic int setup_device(struct ib_device *device)\n{\n\tstruct ib_udata uhw = {.outlen = 0, .inlen = 0};\n\tint ret;\n\n\tib_device_check_mandatory(device);\n\n\tret = setup_port_data(device);\n\tif (ret) {\n\t\tdev_warn(&device->dev, \"Couldn't create per-port data\\n\");\n\t\treturn ret;\n\t}\n\n\tmemset(&device->attrs, 0, sizeof(device->attrs));\n\tret = device->ops.query_device(device, &device->attrs, &uhw);\n\tif (ret) {\n\t\tdev_warn(&device->dev,\n\t\t\t \"Couldn't query the device attributes\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void disable_device(struct ib_device *device)\n{\n\tu32 cid;\n\n\tWARN_ON(!refcount_read(&device->refcount));\n\n\tdown_write(&devices_rwsem);\n\txa_clear_mark(&devices, device->index, DEVICE_REGISTERED);\n\tup_write(&devices_rwsem);\n\n\t \n\tdown_read(&clients_rwsem);\n\tcid = highest_client_id;\n\tup_read(&clients_rwsem);\n\twhile (cid) {\n\t\tcid--;\n\t\tremove_client_context(device, cid);\n\t}\n\n\tib_cq_pool_cleanup(device);\n\n\t \n\tib_device_put(device);\n\twait_for_completion(&device->unreg_completion);\n\n\t \n\tremove_compat_devs(device);\n}\n\n \nstatic int enable_device_and_get(struct ib_device *device)\n{\n\tstruct ib_client *client;\n\tunsigned long index;\n\tint ret = 0;\n\n\t \n\trefcount_set(&device->refcount, 2);\n\tdown_write(&devices_rwsem);\n\txa_set_mark(&devices, device->index, DEVICE_REGISTERED);\n\n\t \n\tdowngrade_write(&devices_rwsem);\n\n\tif (device->ops.enable_driver) {\n\t\tret = device->ops.enable_driver(device);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tdown_read(&clients_rwsem);\n\txa_for_each_marked (&clients, index, client, CLIENT_REGISTERED) {\n\t\tret = add_client_context(device, client);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tup_read(&clients_rwsem);\n\tif (!ret)\n\t\tret = add_compat_devs(device);\nout:\n\tup_read(&devices_rwsem);\n\treturn ret;\n}\n\nstatic void prevent_dealloc_device(struct ib_device *ib_dev)\n{\n}\n\n \nint ib_register_device(struct ib_device *device, const char *name,\n\t\t       struct device *dma_device)\n{\n\tint ret;\n\n\tret = assign_name(device, name);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tWARN_ON(dma_device && !dma_device->dma_parms);\n\tdevice->dma_device = dma_device;\n\n\tret = setup_device(device);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ib_cache_setup_one(device);\n\tif (ret) {\n\t\tdev_warn(&device->dev,\n\t\t\t \"Couldn't set up InfiniBand P_Key/GID cache\\n\");\n\t\treturn ret;\n\t}\n\n\tdevice->groups[0] = &ib_dev_attr_group;\n\tdevice->groups[1] = device->ops.device_group;\n\tret = ib_setup_device_attrs(device);\n\tif (ret)\n\t\tgoto cache_cleanup;\n\n\tib_device_register_rdmacg(device);\n\n\trdma_counter_init(device);\n\n\t \n\tdev_set_uevent_suppress(&device->dev, true);\n\tret = device_add(&device->dev);\n\tif (ret)\n\t\tgoto cg_cleanup;\n\n\tret = ib_setup_port_attrs(&device->coredev);\n\tif (ret) {\n\t\tdev_warn(&device->dev,\n\t\t\t \"Couldn't register device with driver model\\n\");\n\t\tgoto dev_cleanup;\n\t}\n\n\tret = enable_device_and_get(device);\n\tif (ret) {\n\t\tvoid (*dealloc_fn)(struct ib_device *);\n\n\t\t \n\t\tdealloc_fn = device->ops.dealloc_driver;\n\t\tdevice->ops.dealloc_driver = prevent_dealloc_device;\n\t\tib_device_put(device);\n\t\t__ib_unregister_device(device);\n\t\tdevice->ops.dealloc_driver = dealloc_fn;\n\t\tdev_set_uevent_suppress(&device->dev, false);\n\t\treturn ret;\n\t}\n\tdev_set_uevent_suppress(&device->dev, false);\n\t \n\tkobject_uevent(&device->dev.kobj, KOBJ_ADD);\n\tib_device_put(device);\n\n\treturn 0;\n\ndev_cleanup:\n\tdevice_del(&device->dev);\ncg_cleanup:\n\tdev_set_uevent_suppress(&device->dev, false);\n\tib_device_unregister_rdmacg(device);\ncache_cleanup:\n\tib_cache_cleanup_one(device);\n\treturn ret;\n}\nEXPORT_SYMBOL(ib_register_device);\n\n \nstatic void __ib_unregister_device(struct ib_device *ib_dev)\n{\n\t \n\tmutex_lock(&ib_dev->unregistration_lock);\n\tif (!refcount_read(&ib_dev->refcount))\n\t\tgoto out;\n\n\tdisable_device(ib_dev);\n\n\t \n\tfree_netdevs(ib_dev);\n\n\tib_free_port_attrs(&ib_dev->coredev);\n\tdevice_del(&ib_dev->dev);\n\tib_device_unregister_rdmacg(ib_dev);\n\tib_cache_cleanup_one(ib_dev);\n\n\t \n\tif (ib_dev->ops.dealloc_driver &&\n\t    ib_dev->ops.dealloc_driver != prevent_dealloc_device) {\n\t\tWARN_ON(kref_read(&ib_dev->dev.kobj.kref) <= 1);\n\t\tib_dealloc_device(ib_dev);\n\t}\nout:\n\tmutex_unlock(&ib_dev->unregistration_lock);\n}\n\n \nvoid ib_unregister_device(struct ib_device *ib_dev)\n{\n\tget_device(&ib_dev->dev);\n\t__ib_unregister_device(ib_dev);\n\tput_device(&ib_dev->dev);\n}\nEXPORT_SYMBOL(ib_unregister_device);\n\n \nvoid ib_unregister_device_and_put(struct ib_device *ib_dev)\n{\n\tWARN_ON(!ib_dev->ops.dealloc_driver);\n\tget_device(&ib_dev->dev);\n\tib_device_put(ib_dev);\n\t__ib_unregister_device(ib_dev);\n\tput_device(&ib_dev->dev);\n}\nEXPORT_SYMBOL(ib_unregister_device_and_put);\n\n \nvoid ib_unregister_driver(enum rdma_driver_id driver_id)\n{\n\tstruct ib_device *ib_dev;\n\tunsigned long index;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each (&devices, index, ib_dev) {\n\t\tif (ib_dev->ops.driver_id != driver_id)\n\t\t\tcontinue;\n\n\t\tget_device(&ib_dev->dev);\n\t\tup_read(&devices_rwsem);\n\n\t\tWARN_ON(!ib_dev->ops.dealloc_driver);\n\t\t__ib_unregister_device(ib_dev);\n\n\t\tput_device(&ib_dev->dev);\n\t\tdown_read(&devices_rwsem);\n\t}\n\tup_read(&devices_rwsem);\n}\nEXPORT_SYMBOL(ib_unregister_driver);\n\nstatic void ib_unregister_work(struct work_struct *work)\n{\n\tstruct ib_device *ib_dev =\n\t\tcontainer_of(work, struct ib_device, unregistration_work);\n\n\t__ib_unregister_device(ib_dev);\n\tput_device(&ib_dev->dev);\n}\n\n \nvoid ib_unregister_device_queued(struct ib_device *ib_dev)\n{\n\tWARN_ON(!refcount_read(&ib_dev->refcount));\n\tWARN_ON(!ib_dev->ops.dealloc_driver);\n\tget_device(&ib_dev->dev);\n\tif (!queue_work(ib_unreg_wq, &ib_dev->unregistration_work))\n\t\tput_device(&ib_dev->dev);\n}\nEXPORT_SYMBOL(ib_unregister_device_queued);\n\n \nstatic int rdma_dev_change_netns(struct ib_device *device, struct net *cur_net,\n\t\t\t\t struct net *net)\n{\n\tint ret2 = -EINVAL;\n\tint ret;\n\n\tmutex_lock(&device->unregistration_lock);\n\n\t \n\tif (refcount_read(&device->refcount) == 0 ||\n\t    !net_eq(cur_net, read_pnet(&device->coredev.rdma_net))) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tkobject_uevent(&device->dev.kobj, KOBJ_REMOVE);\n\tdisable_device(device);\n\n\t \n\twrite_pnet(&device->coredev.rdma_net, net);\n\n\tdown_read(&devices_rwsem);\n\t \n\tret = device_rename(&device->dev, dev_name(&device->dev));\n\tup_read(&devices_rwsem);\n\tif (ret) {\n\t\tdev_warn(&device->dev,\n\t\t\t \"%s: Couldn't rename device after namespace change\\n\",\n\t\t\t __func__);\n\t\t \n\t\twrite_pnet(&device->coredev.rdma_net, cur_net);\n\t}\n\n\tret2 = enable_device_and_get(device);\n\tif (ret2) {\n\t\t \n\t\tdev_warn(&device->dev,\n\t\t\t \"%s: Couldn't re-enable device after namespace change\\n\",\n\t\t\t __func__);\n\t}\n\tkobject_uevent(&device->dev.kobj, KOBJ_ADD);\n\n\tib_device_put(device);\nout:\n\tmutex_unlock(&device->unregistration_lock);\n\tif (ret)\n\t\treturn ret;\n\treturn ret2;\n}\n\nint ib_device_set_netns_put(struct sk_buff *skb,\n\t\t\t    struct ib_device *dev, u32 ns_fd)\n{\n\tstruct net *net;\n\tint ret;\n\n\tnet = get_net_ns_by_fd(ns_fd);\n\tif (IS_ERR(net)) {\n\t\tret = PTR_ERR(net);\n\t\tgoto net_err;\n\t}\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN)) {\n\t\tret = -EPERM;\n\t\tgoto ns_err;\n\t}\n\n\t \n\tif (!dev->ops.disassociate_ucontext || ib_devices_shared_netns) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto ns_err;\n\t}\n\n\tget_device(&dev->dev);\n\tib_device_put(dev);\n\tret = rdma_dev_change_netns(dev, current->nsproxy->net_ns, net);\n\tput_device(&dev->dev);\n\n\tput_net(net);\n\treturn ret;\n\nns_err:\n\tput_net(net);\nnet_err:\n\tib_device_put(dev);\n\treturn ret;\n}\n\nstatic struct pernet_operations rdma_dev_net_ops = {\n\t.init = rdma_dev_init_net,\n\t.exit = rdma_dev_exit_net,\n\t.id = &rdma_dev_net_id,\n\t.size = sizeof(struct rdma_dev_net),\n};\n\nstatic int assign_client_id(struct ib_client *client)\n{\n\tint ret;\n\n\tdown_write(&clients_rwsem);\n\t \n\tclient->client_id = highest_client_id;\n\tret = xa_insert(&clients, client->client_id, client, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\thighest_client_id++;\n\txa_set_mark(&clients, client->client_id, CLIENT_REGISTERED);\n\nout:\n\tup_write(&clients_rwsem);\n\treturn ret;\n}\n\nstatic void remove_client_id(struct ib_client *client)\n{\n\tdown_write(&clients_rwsem);\n\txa_erase(&clients, client->client_id);\n\tfor (; highest_client_id; highest_client_id--)\n\t\tif (xa_load(&clients, highest_client_id - 1))\n\t\t\tbreak;\n\tup_write(&clients_rwsem);\n}\n\n \nint ib_register_client(struct ib_client *client)\n{\n\tstruct ib_device *device;\n\tunsigned long index;\n\tint ret;\n\n\trefcount_set(&client->uses, 1);\n\tinit_completion(&client->uses_zero);\n\tret = assign_client_id(client);\n\tif (ret)\n\t\treturn ret;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, device, DEVICE_REGISTERED) {\n\t\tret = add_client_context(device, client);\n\t\tif (ret) {\n\t\t\tup_read(&devices_rwsem);\n\t\t\tib_unregister_client(client);\n\t\t\treturn ret;\n\t\t}\n\t}\n\tup_read(&devices_rwsem);\n\treturn 0;\n}\nEXPORT_SYMBOL(ib_register_client);\n\n \nvoid ib_unregister_client(struct ib_client *client)\n{\n\tstruct ib_device *device;\n\tunsigned long index;\n\n\tdown_write(&clients_rwsem);\n\tib_client_put(client);\n\txa_clear_mark(&clients, client->client_id, CLIENT_REGISTERED);\n\tup_write(&clients_rwsem);\n\n\t \n\trcu_read_lock();\n\txa_for_each (&devices, index, device) {\n\t\tif (!ib_device_try_get(device))\n\t\t\tcontinue;\n\t\trcu_read_unlock();\n\n\t\tremove_client_context(device, client->client_id);\n\n\t\tib_device_put(device);\n\t\trcu_read_lock();\n\t}\n\trcu_read_unlock();\n\n\t \n\twait_for_completion(&client->uses_zero);\n\tremove_client_id(client);\n}\nEXPORT_SYMBOL(ib_unregister_client);\n\nstatic int __ib_get_global_client_nl_info(const char *client_name,\n\t\t\t\t\t  struct ib_client_nl_info *res)\n{\n\tstruct ib_client *client;\n\tunsigned long index;\n\tint ret = -ENOENT;\n\n\tdown_read(&clients_rwsem);\n\txa_for_each_marked (&clients, index, client, CLIENT_REGISTERED) {\n\t\tif (strcmp(client->name, client_name) != 0)\n\t\t\tcontinue;\n\t\tif (!client->get_global_nl_info) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tret = client->get_global_nl_info(res);\n\t\tif (WARN_ON(ret == -ENOENT))\n\t\t\tret = -EINVAL;\n\t\tif (!ret && res->cdev)\n\t\t\tget_device(res->cdev);\n\t\tbreak;\n\t}\n\tup_read(&clients_rwsem);\n\treturn ret;\n}\n\nstatic int __ib_get_client_nl_info(struct ib_device *ibdev,\n\t\t\t\t   const char *client_name,\n\t\t\t\t   struct ib_client_nl_info *res)\n{\n\tunsigned long index;\n\tvoid *client_data;\n\tint ret = -ENOENT;\n\n\tdown_read(&ibdev->client_data_rwsem);\n\txan_for_each_marked (&ibdev->client_data, index, client_data,\n\t\t\t     CLIENT_DATA_REGISTERED) {\n\t\tstruct ib_client *client = xa_load(&clients, index);\n\n\t\tif (!client || strcmp(client->name, client_name) != 0)\n\t\t\tcontinue;\n\t\tif (!client->get_nl_info) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tret = client->get_nl_info(ibdev, client_data, res);\n\t\tif (WARN_ON(ret == -ENOENT))\n\t\t\tret = -EINVAL;\n\n\t\t \n\t\tif (!ret && res->cdev)\n\t\t\tget_device(res->cdev);\n\t\tbreak;\n\t}\n\tup_read(&ibdev->client_data_rwsem);\n\n\treturn ret;\n}\n\n \nint ib_get_client_nl_info(struct ib_device *ibdev, const char *client_name,\n\t\t\t  struct ib_client_nl_info *res)\n{\n\tint ret;\n\n\tif (ibdev)\n\t\tret = __ib_get_client_nl_info(ibdev, client_name, res);\n\telse\n\t\tret = __ib_get_global_client_nl_info(client_name, res);\n#ifdef CONFIG_MODULES\n\tif (ret == -ENOENT) {\n\t\trequest_module(\"rdma-client-%s\", client_name);\n\t\tif (ibdev)\n\t\t\tret = __ib_get_client_nl_info(ibdev, client_name, res);\n\t\telse\n\t\t\tret = __ib_get_global_client_nl_info(client_name, res);\n\t}\n#endif\n\tif (ret) {\n\t\tif (ret == -ENOENT)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn ret;\n\t}\n\n\tif (WARN_ON(!res->cdev))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n \nvoid ib_set_client_data(struct ib_device *device, struct ib_client *client,\n\t\t\tvoid *data)\n{\n\tvoid *rc;\n\n\tif (WARN_ON(IS_ERR(data)))\n\t\tdata = NULL;\n\n\trc = xa_store(&device->client_data, client->client_id, data,\n\t\t      GFP_KERNEL);\n\tWARN_ON(xa_is_err(rc));\n}\nEXPORT_SYMBOL(ib_set_client_data);\n\n \nvoid ib_register_event_handler(struct ib_event_handler *event_handler)\n{\n\tdown_write(&event_handler->device->event_handler_rwsem);\n\tlist_add_tail(&event_handler->list,\n\t\t      &event_handler->device->event_handler_list);\n\tup_write(&event_handler->device->event_handler_rwsem);\n}\nEXPORT_SYMBOL(ib_register_event_handler);\n\n \nvoid ib_unregister_event_handler(struct ib_event_handler *event_handler)\n{\n\tdown_write(&event_handler->device->event_handler_rwsem);\n\tlist_del(&event_handler->list);\n\tup_write(&event_handler->device->event_handler_rwsem);\n}\nEXPORT_SYMBOL(ib_unregister_event_handler);\n\nvoid ib_dispatch_event_clients(struct ib_event *event)\n{\n\tstruct ib_event_handler *handler;\n\n\tdown_read(&event->device->event_handler_rwsem);\n\n\tlist_for_each_entry(handler, &event->device->event_handler_list, list)\n\t\thandler->handler(handler, event);\n\n\tup_read(&event->device->event_handler_rwsem);\n}\n\nstatic int iw_query_port(struct ib_device *device,\n\t\t\t   u32 port_num,\n\t\t\t   struct ib_port_attr *port_attr)\n{\n\tstruct in_device *inetdev;\n\tstruct net_device *netdev;\n\n\tmemset(port_attr, 0, sizeof(*port_attr));\n\n\tnetdev = ib_device_get_netdev(device, port_num);\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tport_attr->max_mtu = IB_MTU_4096;\n\tport_attr->active_mtu = ib_mtu_int_to_enum(netdev->mtu);\n\n\tif (!netif_carrier_ok(netdev)) {\n\t\tport_attr->state = IB_PORT_DOWN;\n\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\t} else {\n\t\trcu_read_lock();\n\t\tinetdev = __in_dev_get_rcu(netdev);\n\n\t\tif (inetdev && inetdev->ifa_list) {\n\t\t\tport_attr->state = IB_PORT_ACTIVE;\n\t\t\tport_attr->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\t\t} else {\n\t\t\tport_attr->state = IB_PORT_INIT;\n\t\t\tport_attr->phys_state =\n\t\t\t\tIB_PORT_PHYS_STATE_PORT_CONFIGURATION_TRAINING;\n\t\t}\n\n\t\trcu_read_unlock();\n\t}\n\n\tdev_put(netdev);\n\treturn device->ops.query_port(device, port_num, port_attr);\n}\n\nstatic int __ib_query_port(struct ib_device *device,\n\t\t\t   u32 port_num,\n\t\t\t   struct ib_port_attr *port_attr)\n{\n\tint err;\n\n\tmemset(port_attr, 0, sizeof(*port_attr));\n\n\terr = device->ops.query_port(device, port_num, port_attr);\n\tif (err || port_attr->subnet_prefix)\n\t\treturn err;\n\n\tif (rdma_port_get_link_layer(device, port_num) !=\n\t    IB_LINK_LAYER_INFINIBAND)\n\t\treturn 0;\n\n\tib_get_cached_subnet_prefix(device, port_num,\n\t\t\t\t    &port_attr->subnet_prefix);\n\treturn 0;\n}\n\n \nint ib_query_port(struct ib_device *device,\n\t\t  u32 port_num,\n\t\t  struct ib_port_attr *port_attr)\n{\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tif (rdma_protocol_iwarp(device, port_num))\n\t\treturn iw_query_port(device, port_num, port_attr);\n\telse\n\t\treturn __ib_query_port(device, port_num, port_attr);\n}\nEXPORT_SYMBOL(ib_query_port);\n\nstatic void add_ndev_hash(struct ib_port_data *pdata)\n{\n\tunsigned long flags;\n\n\tmight_sleep();\n\n\tspin_lock_irqsave(&ndev_hash_lock, flags);\n\tif (hash_hashed(&pdata->ndev_hash_link)) {\n\t\thash_del_rcu(&pdata->ndev_hash_link);\n\t\tspin_unlock_irqrestore(&ndev_hash_lock, flags);\n\t\t \n\t\tsynchronize_rcu();\n\t\tspin_lock_irqsave(&ndev_hash_lock, flags);\n\t}\n\tif (pdata->netdev)\n\t\thash_add_rcu(ndev_hash, &pdata->ndev_hash_link,\n\t\t\t     (uintptr_t)pdata->netdev);\n\tspin_unlock_irqrestore(&ndev_hash_lock, flags);\n}\n\n \nint ib_device_set_netdev(struct ib_device *ib_dev, struct net_device *ndev,\n\t\t\t u32 port)\n{\n\tstruct net_device *old_ndev;\n\tstruct ib_port_data *pdata;\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tret = alloc_port_data(ib_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!rdma_is_port_valid(ib_dev, port))\n\t\treturn -EINVAL;\n\n\tpdata = &ib_dev->port_data[port];\n\tspin_lock_irqsave(&pdata->netdev_lock, flags);\n\told_ndev = rcu_dereference_protected(\n\t\tpdata->netdev, lockdep_is_held(&pdata->netdev_lock));\n\tif (old_ndev == ndev) {\n\t\tspin_unlock_irqrestore(&pdata->netdev_lock, flags);\n\t\treturn 0;\n\t}\n\n\tif (old_ndev)\n\t\tnetdev_tracker_free(ndev, &pdata->netdev_tracker);\n\tif (ndev)\n\t\tnetdev_hold(ndev, &pdata->netdev_tracker, GFP_ATOMIC);\n\trcu_assign_pointer(pdata->netdev, ndev);\n\tspin_unlock_irqrestore(&pdata->netdev_lock, flags);\n\n\tadd_ndev_hash(pdata);\n\tif (old_ndev)\n\t\t__dev_put(old_ndev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ib_device_set_netdev);\n\nstatic void free_netdevs(struct ib_device *ib_dev)\n{\n\tunsigned long flags;\n\tu32 port;\n\n\tif (!ib_dev->port_data)\n\t\treturn;\n\n\trdma_for_each_port (ib_dev, port) {\n\t\tstruct ib_port_data *pdata = &ib_dev->port_data[port];\n\t\tstruct net_device *ndev;\n\n\t\tspin_lock_irqsave(&pdata->netdev_lock, flags);\n\t\tndev = rcu_dereference_protected(\n\t\t\tpdata->netdev, lockdep_is_held(&pdata->netdev_lock));\n\t\tif (ndev) {\n\t\t\tspin_lock(&ndev_hash_lock);\n\t\t\thash_del_rcu(&pdata->ndev_hash_link);\n\t\t\tspin_unlock(&ndev_hash_lock);\n\n\t\t\t \n\t\t\trcu_assign_pointer(pdata->netdev, NULL);\n\t\t\tnetdev_put(ndev, &pdata->netdev_tracker);\n\t\t}\n\t\tspin_unlock_irqrestore(&pdata->netdev_lock, flags);\n\t}\n}\n\nstruct net_device *ib_device_get_netdev(struct ib_device *ib_dev,\n\t\t\t\t\tu32 port)\n{\n\tstruct ib_port_data *pdata;\n\tstruct net_device *res;\n\n\tif (!rdma_is_port_valid(ib_dev, port))\n\t\treturn NULL;\n\n\tpdata = &ib_dev->port_data[port];\n\n\t \n\tif (ib_dev->ops.get_netdev)\n\t\tres = ib_dev->ops.get_netdev(ib_dev, port);\n\telse {\n\t\tspin_lock(&pdata->netdev_lock);\n\t\tres = rcu_dereference_protected(\n\t\t\tpdata->netdev, lockdep_is_held(&pdata->netdev_lock));\n\t\tif (res)\n\t\t\tdev_hold(res);\n\t\tspin_unlock(&pdata->netdev_lock);\n\t}\n\n\t \n\tif (res && res->reg_state != NETREG_REGISTERED) {\n\t\tdev_put(res);\n\t\treturn NULL;\n\t}\n\n\treturn res;\n}\n\n \nstruct ib_device *ib_device_get_by_netdev(struct net_device *ndev,\n\t\t\t\t\t  enum rdma_driver_id driver_id)\n{\n\tstruct ib_device *res = NULL;\n\tstruct ib_port_data *cur;\n\n\trcu_read_lock();\n\thash_for_each_possible_rcu (ndev_hash, cur, ndev_hash_link,\n\t\t\t\t    (uintptr_t)ndev) {\n\t\tif (rcu_access_pointer(cur->netdev) == ndev &&\n\t\t    (driver_id == RDMA_DRIVER_UNKNOWN ||\n\t\t     cur->ib_dev->ops.driver_id == driver_id) &&\n\t\t    ib_device_try_get(cur->ib_dev)) {\n\t\t\tres = cur->ib_dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL(ib_device_get_by_netdev);\n\n \nvoid ib_enum_roce_netdev(struct ib_device *ib_dev,\n\t\t\t roce_netdev_filter filter,\n\t\t\t void *filter_cookie,\n\t\t\t roce_netdev_callback cb,\n\t\t\t void *cookie)\n{\n\tu32 port;\n\n\trdma_for_each_port (ib_dev, port)\n\t\tif (rdma_protocol_roce(ib_dev, port)) {\n\t\t\tstruct net_device *idev =\n\t\t\t\tib_device_get_netdev(ib_dev, port);\n\n\t\t\tif (filter(ib_dev, port, idev, filter_cookie))\n\t\t\t\tcb(ib_dev, port, idev, cookie);\n\n\t\t\tif (idev)\n\t\t\t\tdev_put(idev);\n\t\t}\n}\n\n \nvoid ib_enum_all_roce_netdevs(roce_netdev_filter filter,\n\t\t\t      void *filter_cookie,\n\t\t\t      roce_netdev_callback cb,\n\t\t\t      void *cookie)\n{\n\tstruct ib_device *dev;\n\tunsigned long index;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED)\n\t\tib_enum_roce_netdev(dev, filter, filter_cookie, cb, cookie);\n\tup_read(&devices_rwsem);\n}\n\n \nint ib_enum_all_devs(nldev_callback nldev_cb, struct sk_buff *skb,\n\t\t     struct netlink_callback *cb)\n{\n\tunsigned long index;\n\tstruct ib_device *dev;\n\tunsigned int idx = 0;\n\tint ret = 0;\n\n\tdown_read(&devices_rwsem);\n\txa_for_each_marked (&devices, index, dev, DEVICE_REGISTERED) {\n\t\tif (!rdma_dev_access_netns(dev, sock_net(skb->sk)))\n\t\t\tcontinue;\n\n\t\tret = nldev_cb(dev, skb, cb, idx);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tidx++;\n\t}\n\tup_read(&devices_rwsem);\n\treturn ret;\n}\n\n \nint ib_query_pkey(struct ib_device *device,\n\t\t  u32 port_num, u16 index, u16 *pkey)\n{\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tif (!device->ops.query_pkey)\n\t\treturn -EOPNOTSUPP;\n\n\treturn device->ops.query_pkey(device, port_num, index, pkey);\n}\nEXPORT_SYMBOL(ib_query_pkey);\n\n \nint ib_modify_device(struct ib_device *device,\n\t\t     int device_modify_mask,\n\t\t     struct ib_device_modify *device_modify)\n{\n\tif (!device->ops.modify_device)\n\t\treturn -EOPNOTSUPP;\n\n\treturn device->ops.modify_device(device, device_modify_mask,\n\t\t\t\t\t device_modify);\n}\nEXPORT_SYMBOL(ib_modify_device);\n\n \nint ib_modify_port(struct ib_device *device,\n\t\t   u32 port_num, int port_modify_mask,\n\t\t   struct ib_port_modify *port_modify)\n{\n\tint rc;\n\n\tif (!rdma_is_port_valid(device, port_num))\n\t\treturn -EINVAL;\n\n\tif (device->ops.modify_port)\n\t\trc = device->ops.modify_port(device, port_num,\n\t\t\t\t\t     port_modify_mask,\n\t\t\t\t\t     port_modify);\n\telse if (rdma_protocol_roce(device, port_num) &&\n\t\t ((port_modify->set_port_cap_mask & ~IB_PORT_CM_SUP) == 0 ||\n\t\t  (port_modify->clr_port_cap_mask & ~IB_PORT_CM_SUP) == 0))\n\t\trc = 0;\n\telse\n\t\trc = -EOPNOTSUPP;\n\treturn rc;\n}\nEXPORT_SYMBOL(ib_modify_port);\n\n \nint ib_find_gid(struct ib_device *device, union ib_gid *gid,\n\t\tu32 *port_num, u16 *index)\n{\n\tunion ib_gid tmp_gid;\n\tu32 port;\n\tint ret, i;\n\n\trdma_for_each_port (device, port) {\n\t\tif (!rdma_protocol_ib(device, port))\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < device->port_data[port].immutable.gid_tbl_len;\n\t\t     ++i) {\n\t\t\tret = rdma_query_gid(device, port, i, &tmp_gid);\n\t\t\tif (ret)\n\t\t\t\tcontinue;\n\n\t\t\tif (!memcmp(&tmp_gid, gid, sizeof *gid)) {\n\t\t\t\t*port_num = port;\n\t\t\t\tif (index)\n\t\t\t\t\t*index = i;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\nEXPORT_SYMBOL(ib_find_gid);\n\n \nint ib_find_pkey(struct ib_device *device,\n\t\t u32 port_num, u16 pkey, u16 *index)\n{\n\tint ret, i;\n\tu16 tmp_pkey;\n\tint partial_ix = -1;\n\n\tfor (i = 0; i < device->port_data[port_num].immutable.pkey_tbl_len;\n\t     ++i) {\n\t\tret = ib_query_pkey(device, port_num, i, &tmp_pkey);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif ((pkey & 0x7fff) == (tmp_pkey & 0x7fff)) {\n\t\t\t \n\t\t\tif (tmp_pkey & 0x8000) {\n\t\t\t\t*index = i;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (partial_ix < 0)\n\t\t\t\tpartial_ix = i;\n\t\t}\n\t}\n\n\t \n\tif (partial_ix >= 0) {\n\t\t*index = partial_ix;\n\t\treturn 0;\n\t}\n\treturn -ENOENT;\n}\nEXPORT_SYMBOL(ib_find_pkey);\n\n \nstruct net_device *ib_get_net_dev_by_params(struct ib_device *dev,\n\t\t\t\t\t    u32 port,\n\t\t\t\t\t    u16 pkey,\n\t\t\t\t\t    const union ib_gid *gid,\n\t\t\t\t\t    const struct sockaddr *addr)\n{\n\tstruct net_device *net_dev = NULL;\n\tunsigned long index;\n\tvoid *client_data;\n\n\tif (!rdma_protocol_ib(dev, port))\n\t\treturn NULL;\n\n\t \n\tdown_read(&dev->client_data_rwsem);\n\txan_for_each_marked (&dev->client_data, index, client_data,\n\t\t\t     CLIENT_DATA_REGISTERED) {\n\t\tstruct ib_client *client = xa_load(&clients, index);\n\n\t\tif (!client || !client->get_net_dev_by_params)\n\t\t\tcontinue;\n\n\t\tnet_dev = client->get_net_dev_by_params(dev, port, pkey, gid,\n\t\t\t\t\t\t\taddr, client_data);\n\t\tif (net_dev)\n\t\t\tbreak;\n\t}\n\tup_read(&dev->client_data_rwsem);\n\n\treturn net_dev;\n}\nEXPORT_SYMBOL(ib_get_net_dev_by_params);\n\nvoid ib_set_device_ops(struct ib_device *dev, const struct ib_device_ops *ops)\n{\n\tstruct ib_device_ops *dev_ops = &dev->ops;\n#define SET_DEVICE_OP(ptr, name)                                               \\\n\tdo {                                                                   \\\n\t\tif (ops->name)                                                 \\\n\t\t\tif (!((ptr)->name))\t\t\t\t       \\\n\t\t\t\t(ptr)->name = ops->name;                       \\\n\t} while (0)\n\n#define SET_OBJ_SIZE(ptr, name) SET_DEVICE_OP(ptr, size_##name)\n\n\tif (ops->driver_id != RDMA_DRIVER_UNKNOWN) {\n\t\tWARN_ON(dev_ops->driver_id != RDMA_DRIVER_UNKNOWN &&\n\t\t\tdev_ops->driver_id != ops->driver_id);\n\t\tdev_ops->driver_id = ops->driver_id;\n\t}\n\tif (ops->owner) {\n\t\tWARN_ON(dev_ops->owner && dev_ops->owner != ops->owner);\n\t\tdev_ops->owner = ops->owner;\n\t}\n\tif (ops->uverbs_abi_ver)\n\t\tdev_ops->uverbs_abi_ver = ops->uverbs_abi_ver;\n\n\tdev_ops->uverbs_no_driver_id_binding |=\n\t\tops->uverbs_no_driver_id_binding;\n\n\tSET_DEVICE_OP(dev_ops, add_gid);\n\tSET_DEVICE_OP(dev_ops, advise_mr);\n\tSET_DEVICE_OP(dev_ops, alloc_dm);\n\tSET_DEVICE_OP(dev_ops, alloc_hw_device_stats);\n\tSET_DEVICE_OP(dev_ops, alloc_hw_port_stats);\n\tSET_DEVICE_OP(dev_ops, alloc_mr);\n\tSET_DEVICE_OP(dev_ops, alloc_mr_integrity);\n\tSET_DEVICE_OP(dev_ops, alloc_mw);\n\tSET_DEVICE_OP(dev_ops, alloc_pd);\n\tSET_DEVICE_OP(dev_ops, alloc_rdma_netdev);\n\tSET_DEVICE_OP(dev_ops, alloc_ucontext);\n\tSET_DEVICE_OP(dev_ops, alloc_xrcd);\n\tSET_DEVICE_OP(dev_ops, attach_mcast);\n\tSET_DEVICE_OP(dev_ops, check_mr_status);\n\tSET_DEVICE_OP(dev_ops, counter_alloc_stats);\n\tSET_DEVICE_OP(dev_ops, counter_bind_qp);\n\tSET_DEVICE_OP(dev_ops, counter_dealloc);\n\tSET_DEVICE_OP(dev_ops, counter_unbind_qp);\n\tSET_DEVICE_OP(dev_ops, counter_update_stats);\n\tSET_DEVICE_OP(dev_ops, create_ah);\n\tSET_DEVICE_OP(dev_ops, create_counters);\n\tSET_DEVICE_OP(dev_ops, create_cq);\n\tSET_DEVICE_OP(dev_ops, create_flow);\n\tSET_DEVICE_OP(dev_ops, create_qp);\n\tSET_DEVICE_OP(dev_ops, create_rwq_ind_table);\n\tSET_DEVICE_OP(dev_ops, create_srq);\n\tSET_DEVICE_OP(dev_ops, create_user_ah);\n\tSET_DEVICE_OP(dev_ops, create_wq);\n\tSET_DEVICE_OP(dev_ops, dealloc_dm);\n\tSET_DEVICE_OP(dev_ops, dealloc_driver);\n\tSET_DEVICE_OP(dev_ops, dealloc_mw);\n\tSET_DEVICE_OP(dev_ops, dealloc_pd);\n\tSET_DEVICE_OP(dev_ops, dealloc_ucontext);\n\tSET_DEVICE_OP(dev_ops, dealloc_xrcd);\n\tSET_DEVICE_OP(dev_ops, del_gid);\n\tSET_DEVICE_OP(dev_ops, dereg_mr);\n\tSET_DEVICE_OP(dev_ops, destroy_ah);\n\tSET_DEVICE_OP(dev_ops, destroy_counters);\n\tSET_DEVICE_OP(dev_ops, destroy_cq);\n\tSET_DEVICE_OP(dev_ops, destroy_flow);\n\tSET_DEVICE_OP(dev_ops, destroy_flow_action);\n\tSET_DEVICE_OP(dev_ops, destroy_qp);\n\tSET_DEVICE_OP(dev_ops, destroy_rwq_ind_table);\n\tSET_DEVICE_OP(dev_ops, destroy_srq);\n\tSET_DEVICE_OP(dev_ops, destroy_wq);\n\tSET_DEVICE_OP(dev_ops, device_group);\n\tSET_DEVICE_OP(dev_ops, detach_mcast);\n\tSET_DEVICE_OP(dev_ops, disassociate_ucontext);\n\tSET_DEVICE_OP(dev_ops, drain_rq);\n\tSET_DEVICE_OP(dev_ops, drain_sq);\n\tSET_DEVICE_OP(dev_ops, enable_driver);\n\tSET_DEVICE_OP(dev_ops, fill_res_cm_id_entry);\n\tSET_DEVICE_OP(dev_ops, fill_res_cq_entry);\n\tSET_DEVICE_OP(dev_ops, fill_res_cq_entry_raw);\n\tSET_DEVICE_OP(dev_ops, fill_res_mr_entry);\n\tSET_DEVICE_OP(dev_ops, fill_res_mr_entry_raw);\n\tSET_DEVICE_OP(dev_ops, fill_res_qp_entry);\n\tSET_DEVICE_OP(dev_ops, fill_res_qp_entry_raw);\n\tSET_DEVICE_OP(dev_ops, fill_stat_mr_entry);\n\tSET_DEVICE_OP(dev_ops, get_dev_fw_str);\n\tSET_DEVICE_OP(dev_ops, get_dma_mr);\n\tSET_DEVICE_OP(dev_ops, get_hw_stats);\n\tSET_DEVICE_OP(dev_ops, get_link_layer);\n\tSET_DEVICE_OP(dev_ops, get_netdev);\n\tSET_DEVICE_OP(dev_ops, get_numa_node);\n\tSET_DEVICE_OP(dev_ops, get_port_immutable);\n\tSET_DEVICE_OP(dev_ops, get_vector_affinity);\n\tSET_DEVICE_OP(dev_ops, get_vf_config);\n\tSET_DEVICE_OP(dev_ops, get_vf_guid);\n\tSET_DEVICE_OP(dev_ops, get_vf_stats);\n\tSET_DEVICE_OP(dev_ops, iw_accept);\n\tSET_DEVICE_OP(dev_ops, iw_add_ref);\n\tSET_DEVICE_OP(dev_ops, iw_connect);\n\tSET_DEVICE_OP(dev_ops, iw_create_listen);\n\tSET_DEVICE_OP(dev_ops, iw_destroy_listen);\n\tSET_DEVICE_OP(dev_ops, iw_get_qp);\n\tSET_DEVICE_OP(dev_ops, iw_reject);\n\tSET_DEVICE_OP(dev_ops, iw_rem_ref);\n\tSET_DEVICE_OP(dev_ops, map_mr_sg);\n\tSET_DEVICE_OP(dev_ops, map_mr_sg_pi);\n\tSET_DEVICE_OP(dev_ops, mmap);\n\tSET_DEVICE_OP(dev_ops, mmap_free);\n\tSET_DEVICE_OP(dev_ops, modify_ah);\n\tSET_DEVICE_OP(dev_ops, modify_cq);\n\tSET_DEVICE_OP(dev_ops, modify_device);\n\tSET_DEVICE_OP(dev_ops, modify_hw_stat);\n\tSET_DEVICE_OP(dev_ops, modify_port);\n\tSET_DEVICE_OP(dev_ops, modify_qp);\n\tSET_DEVICE_OP(dev_ops, modify_srq);\n\tSET_DEVICE_OP(dev_ops, modify_wq);\n\tSET_DEVICE_OP(dev_ops, peek_cq);\n\tSET_DEVICE_OP(dev_ops, poll_cq);\n\tSET_DEVICE_OP(dev_ops, port_groups);\n\tSET_DEVICE_OP(dev_ops, post_recv);\n\tSET_DEVICE_OP(dev_ops, post_send);\n\tSET_DEVICE_OP(dev_ops, post_srq_recv);\n\tSET_DEVICE_OP(dev_ops, process_mad);\n\tSET_DEVICE_OP(dev_ops, query_ah);\n\tSET_DEVICE_OP(dev_ops, query_device);\n\tSET_DEVICE_OP(dev_ops, query_gid);\n\tSET_DEVICE_OP(dev_ops, query_pkey);\n\tSET_DEVICE_OP(dev_ops, query_port);\n\tSET_DEVICE_OP(dev_ops, query_qp);\n\tSET_DEVICE_OP(dev_ops, query_srq);\n\tSET_DEVICE_OP(dev_ops, query_ucontext);\n\tSET_DEVICE_OP(dev_ops, rdma_netdev_get_params);\n\tSET_DEVICE_OP(dev_ops, read_counters);\n\tSET_DEVICE_OP(dev_ops, reg_dm_mr);\n\tSET_DEVICE_OP(dev_ops, reg_user_mr);\n\tSET_DEVICE_OP(dev_ops, reg_user_mr_dmabuf);\n\tSET_DEVICE_OP(dev_ops, req_notify_cq);\n\tSET_DEVICE_OP(dev_ops, rereg_user_mr);\n\tSET_DEVICE_OP(dev_ops, resize_cq);\n\tSET_DEVICE_OP(dev_ops, set_vf_guid);\n\tSET_DEVICE_OP(dev_ops, set_vf_link_state);\n\n\tSET_OBJ_SIZE(dev_ops, ib_ah);\n\tSET_OBJ_SIZE(dev_ops, ib_counters);\n\tSET_OBJ_SIZE(dev_ops, ib_cq);\n\tSET_OBJ_SIZE(dev_ops, ib_mw);\n\tSET_OBJ_SIZE(dev_ops, ib_pd);\n\tSET_OBJ_SIZE(dev_ops, ib_qp);\n\tSET_OBJ_SIZE(dev_ops, ib_rwq_ind_table);\n\tSET_OBJ_SIZE(dev_ops, ib_srq);\n\tSET_OBJ_SIZE(dev_ops, ib_ucontext);\n\tSET_OBJ_SIZE(dev_ops, ib_xrcd);\n}\nEXPORT_SYMBOL(ib_set_device_ops);\n\n#ifdef CONFIG_INFINIBAND_VIRT_DMA\nint ib_dma_virt_map_sg(struct ib_device *dev, struct scatterlist *sg, int nents)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tsg_dma_address(s) = (uintptr_t)sg_virt(s);\n\t\tsg_dma_len(s) = s->length;\n\t}\n\treturn nents;\n}\nEXPORT_SYMBOL(ib_dma_virt_map_sg);\n#endif  \n\nstatic const struct rdma_nl_cbs ibnl_ls_cb_table[RDMA_NL_LS_NUM_OPS] = {\n\t[RDMA_NL_LS_OP_RESOLVE] = {\n\t\t.doit = ib_nl_handle_resolve_resp,\n\t\t.flags = RDMA_NL_ADMIN_PERM,\n\t},\n\t[RDMA_NL_LS_OP_SET_TIMEOUT] = {\n\t\t.doit = ib_nl_handle_set_timeout,\n\t\t.flags = RDMA_NL_ADMIN_PERM,\n\t},\n\t[RDMA_NL_LS_OP_IP_RESOLVE] = {\n\t\t.doit = ib_nl_handle_ip_res_resp,\n\t\t.flags = RDMA_NL_ADMIN_PERM,\n\t},\n};\n\nstatic int __init ib_core_init(void)\n{\n\tint ret = -ENOMEM;\n\n\tib_wq = alloc_workqueue(\"infiniband\", 0, 0);\n\tif (!ib_wq)\n\t\treturn -ENOMEM;\n\n\tib_unreg_wq = alloc_workqueue(\"ib-unreg-wq\", WQ_UNBOUND,\n\t\t\t\t      WQ_UNBOUND_MAX_ACTIVE);\n\tif (!ib_unreg_wq)\n\t\tgoto err;\n\n\tib_comp_wq = alloc_workqueue(\"ib-comp-wq\",\n\t\t\tWQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_SYSFS, 0);\n\tif (!ib_comp_wq)\n\t\tgoto err_unbound;\n\n\tib_comp_unbound_wq =\n\t\talloc_workqueue(\"ib-comp-unb-wq\",\n\t\t\t\tWQ_UNBOUND | WQ_HIGHPRI | WQ_MEM_RECLAIM |\n\t\t\t\tWQ_SYSFS, WQ_UNBOUND_MAX_ACTIVE);\n\tif (!ib_comp_unbound_wq)\n\t\tgoto err_comp;\n\n\tret = class_register(&ib_class);\n\tif (ret) {\n\t\tpr_warn(\"Couldn't create InfiniBand device class\\n\");\n\t\tgoto err_comp_unbound;\n\t}\n\n\trdma_nl_init();\n\n\tret = addr_init();\n\tif (ret) {\n\t\tpr_warn(\"Couldn't init IB address resolution\\n\");\n\t\tgoto err_ibnl;\n\t}\n\n\tret = ib_mad_init();\n\tif (ret) {\n\t\tpr_warn(\"Couldn't init IB MAD\\n\");\n\t\tgoto err_addr;\n\t}\n\n\tret = ib_sa_init();\n\tif (ret) {\n\t\tpr_warn(\"Couldn't init SA\\n\");\n\t\tgoto err_mad;\n\t}\n\n\tret = register_blocking_lsm_notifier(&ibdev_lsm_nb);\n\tif (ret) {\n\t\tpr_warn(\"Couldn't register LSM notifier. ret %d\\n\", ret);\n\t\tgoto err_sa;\n\t}\n\n\tret = register_pernet_device(&rdma_dev_net_ops);\n\tif (ret) {\n\t\tpr_warn(\"Couldn't init compat dev. ret %d\\n\", ret);\n\t\tgoto err_compat;\n\t}\n\n\tnldev_init();\n\trdma_nl_register(RDMA_NL_LS, ibnl_ls_cb_table);\n\tret = roce_gid_mgmt_init();\n\tif (ret) {\n\t\tpr_warn(\"Couldn't init RoCE GID management\\n\");\n\t\tgoto err_parent;\n\t}\n\n\treturn 0;\n\nerr_parent:\n\trdma_nl_unregister(RDMA_NL_LS);\n\tnldev_exit();\n\tunregister_pernet_device(&rdma_dev_net_ops);\nerr_compat:\n\tunregister_blocking_lsm_notifier(&ibdev_lsm_nb);\nerr_sa:\n\tib_sa_cleanup();\nerr_mad:\n\tib_mad_cleanup();\nerr_addr:\n\taddr_cleanup();\nerr_ibnl:\n\tclass_unregister(&ib_class);\nerr_comp_unbound:\n\tdestroy_workqueue(ib_comp_unbound_wq);\nerr_comp:\n\tdestroy_workqueue(ib_comp_wq);\nerr_unbound:\n\tdestroy_workqueue(ib_unreg_wq);\nerr:\n\tdestroy_workqueue(ib_wq);\n\treturn ret;\n}\n\nstatic void __exit ib_core_cleanup(void)\n{\n\troce_gid_mgmt_cleanup();\n\trdma_nl_unregister(RDMA_NL_LS);\n\tnldev_exit();\n\tunregister_pernet_device(&rdma_dev_net_ops);\n\tunregister_blocking_lsm_notifier(&ibdev_lsm_nb);\n\tib_sa_cleanup();\n\tib_mad_cleanup();\n\taddr_cleanup();\n\trdma_nl_exit();\n\tclass_unregister(&ib_class);\n\tdestroy_workqueue(ib_comp_unbound_wq);\n\tdestroy_workqueue(ib_comp_wq);\n\t \n\tdestroy_workqueue(ib_wq);\n\tdestroy_workqueue(ib_unreg_wq);\n\tWARN_ON(!xa_empty(&clients));\n\tWARN_ON(!xa_empty(&devices));\n}\n\nMODULE_ALIAS_RDMA_NETLINK(RDMA_NL_LS, 4);\n\n \nfs_initcall(ib_core_init);\nmodule_exit(ib_core_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}