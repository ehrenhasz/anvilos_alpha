{
  "module_name": "ib_srpt.c",
  "hash_id": "82765ae46b140d6e99b09f6614963359cf86e99efcc6d410c4e5eacca5376f72",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/srpt/ib_srpt.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/kthread.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/inet.h>\n#include <rdma/ib_cache.h>\n#include <scsi/scsi_proto.h>\n#include <scsi/scsi_tcq.h>\n#include <target/target_core_base.h>\n#include <target/target_core_fabric.h>\n#include \"ib_srpt.h\"\n\n \n#define DRV_NAME\t\t\"ib_srpt\"\n\n#define SRPT_ID_STRING\t\"Linux SRP target\"\n\n#undef pr_fmt\n#define pr_fmt(fmt) DRV_NAME \" \" fmt\n\nMODULE_AUTHOR(\"Vu Pham and Bart Van Assche\");\nMODULE_DESCRIPTION(\"SCSI RDMA Protocol target driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n \n\nstatic u64 srpt_service_guid;\nstatic DEFINE_SPINLOCK(srpt_dev_lock);\t \nstatic LIST_HEAD(srpt_dev_list);\t \n\nstatic unsigned srp_max_req_size = DEFAULT_MAX_REQ_SIZE;\nmodule_param(srp_max_req_size, int, 0444);\nMODULE_PARM_DESC(srp_max_req_size,\n\t\t \"Maximum size of SRP request messages in bytes.\");\n\nstatic int srpt_srq_size = DEFAULT_SRPT_SRQ_SIZE;\nmodule_param(srpt_srq_size, int, 0444);\nMODULE_PARM_DESC(srpt_srq_size,\n\t\t \"Shared receive queue (SRQ) size.\");\n\nstatic int srpt_get_u64_x(char *buffer, const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"0x%016llx\\n\", *(u64 *)kp->arg);\n}\nmodule_param_call(srpt_service_guid, NULL, srpt_get_u64_x, &srpt_service_guid,\n\t\t  0444);\nMODULE_PARM_DESC(srpt_service_guid,\n\t\t \"Using this value for ioc_guid, id_ext, and cm_listen_id instead of using the node_guid of the first HCA.\");\n\nstatic struct ib_client srpt_client;\n \nstatic DEFINE_MUTEX(rdma_cm_mutex);\n \nstatic u16 rdma_cm_port;\nstatic struct rdma_cm_id *rdma_cm_id;\nstatic void srpt_release_cmd(struct se_cmd *se_cmd);\nstatic void srpt_free_ch(struct kref *kref);\nstatic int srpt_queue_status(struct se_cmd *cmd);\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void srpt_process_wait_list(struct srpt_rdma_ch *ch);\n\n \nstatic bool srpt_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state new)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state prev;\n\tbool changed = false;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev = ch->state;\n\tif (new > prev) {\n\t\tch->state = new;\n\t\tchanged = true;\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\treturn changed;\n}\n\n \nstatic void srpt_event_handler(struct ib_event_handler *handler,\n\t\t\t       struct ib_event *event)\n{\n\tstruct srpt_device *sdev =\n\t\tcontainer_of(handler, struct srpt_device, event_handler);\n\tstruct srpt_port *sport;\n\tu8 port_num;\n\n\tpr_debug(\"ASYNC event= %d on device= %s\\n\", event->event,\n\t\t dev_name(&sdev->device->dev));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_PORT_ERR:\n\t\tport_num = event->element.port_num - 1;\n\t\tif (port_num < sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[port_num];\n\t\t\tsport->lid = 0;\n\t\t\tsport->sm_lid = 0;\n\t\t} else {\n\t\t\tWARN(true, \"event %d: port_num %d out of range 1..%d\\n\",\n\t\t\t     event->event, port_num + 1,\n\t\t\t     sdev->device->phys_port_cnt);\n\t\t}\n\t\tbreak;\n\tcase IB_EVENT_PORT_ACTIVE:\n\tcase IB_EVENT_LID_CHANGE:\n\tcase IB_EVENT_PKEY_CHANGE:\n\tcase IB_EVENT_SM_CHANGE:\n\tcase IB_EVENT_CLIENT_REREGISTER:\n\tcase IB_EVENT_GID_CHANGE:\n\t\t \n\t\tport_num = event->element.port_num - 1;\n\t\tif (port_num < sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[port_num];\n\t\t\tif (!sport->lid && !sport->sm_lid)\n\t\t\t\tschedule_work(&sport->work);\n\t\t} else {\n\t\t\tWARN(true, \"event %d: port_num %d out of range 1..%d\\n\",\n\t\t\t     event->event, port_num + 1,\n\t\t\t     sdev->device->phys_port_cnt);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB event %d\\n\", event->event);\n\t\tbreak;\n\t}\n}\n\n \nstatic void srpt_srq_event(struct ib_event *event, void *ctx)\n{\n\tpr_debug(\"SRQ event %d\\n\", event->event);\n}\n\nstatic const char *get_ch_state_name(enum rdma_ch_state s)\n{\n\tswitch (s) {\n\tcase CH_CONNECTING:\n\t\treturn \"connecting\";\n\tcase CH_LIVE:\n\t\treturn \"live\";\n\tcase CH_DISCONNECTING:\n\t\treturn \"disconnecting\";\n\tcase CH_DRAINING:\n\t\treturn \"draining\";\n\tcase CH_DISCONNECTED:\n\t\treturn \"disconnected\";\n\t}\n\treturn \"???\";\n}\n\n \nstatic void srpt_qp_event(struct ib_event *event, struct srpt_rdma_ch *ch)\n{\n\tpr_debug(\"QP event %d on ch=%p sess_name=%s-%d state=%s\\n\",\n\t\t event->event, ch, ch->sess_name, ch->qp->qp_num,\n\t\t get_ch_state_name(ch->state));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_COMM_EST:\n\t\tif (ch->using_rdma_cm)\n\t\t\trdma_notify(ch->rdma_cm.cm_id, event->event);\n\t\telse\n\t\t\tib_cm_notify(ch->ib_cm.cm_id, event->event);\n\t\tbreak;\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tpr_debug(\"%s-%d, state %s: received Last WQE event.\\n\",\n\t\t\t ch->sess_name, ch->qp->qp_num,\n\t\t\t get_ch_state_name(ch->state));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB QP event %d\\n\", event->event);\n\t\tbreak;\n\t}\n}\n\n \nstatic void srpt_set_ioc(u8 *c_list, u32 slot, u8 value)\n{\n\tu16 id;\n\tu8 tmp;\n\n\tid = (slot - 1) / 2;\n\tif (slot & 0x1) {\n\t\ttmp = c_list[id] & 0xf;\n\t\tc_list[id] = (value << 4) | tmp;\n\t} else {\n\t\ttmp = c_list[id] & 0xf0;\n\t\tc_list[id] = (value & 0xf) | tmp;\n\t}\n}\n\n \nstatic void srpt_get_class_port_info(struct ib_dm_mad *mad)\n{\n\tstruct ib_class_port_info *cif;\n\n\tcif = (struct ib_class_port_info *)mad->data;\n\tmemset(cif, 0, sizeof(*cif));\n\tcif->base_version = 1;\n\tcif->class_version = 1;\n\n\tib_set_cpi_resp_time(cif, 20);\n\tmad->mad_hdr.status = 0;\n}\n\n \nstatic void srpt_get_iou(struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_iou_info *ioui;\n\tu8 slot;\n\tint i;\n\n\tioui = (struct ib_dm_iou_info *)mad->data;\n\tioui->change_id = cpu_to_be16(1);\n\tioui->max_controllers = 16;\n\n\t \n\tsrpt_set_ioc(ioui->controller_list, 1, 1);\n\tfor (i = 1, slot = 2; i < 16; i++, slot++)\n\t\tsrpt_set_ioc(ioui->controller_list, slot, 0);\n\n\tmad->mad_hdr.status = 0;\n}\n\n \nstatic void srpt_get_ioc(struct srpt_port *sport, u32 slot,\n\t\t\t struct ib_dm_mad *mad)\n{\n\tstruct srpt_device *sdev = sport->sdev;\n\tstruct ib_dm_ioc_profile *iocp;\n\tint send_queue_depth;\n\n\tiocp = (struct ib_dm_ioc_profile *)mad->data;\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tif (sdev->use_srq)\n\t\tsend_queue_depth = sdev->srq_size;\n\telse\n\t\tsend_queue_depth = min(MAX_SRPT_RQ_SIZE,\n\t\t\t\t       sdev->device->attrs.max_qp_wr);\n\n\tmemset(iocp, 0, sizeof(*iocp));\n\tstrcpy(iocp->id_string, SRPT_ID_STRING);\n\tiocp->guid = cpu_to_be64(srpt_service_guid);\n\tiocp->vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->device_id = cpu_to_be32(sdev->device->attrs.vendor_part_id);\n\tiocp->device_version = cpu_to_be16(sdev->device->attrs.hw_ver);\n\tiocp->subsys_vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->subsys_device_id = 0x0;\n\tiocp->io_class = cpu_to_be16(SRP_REV16A_IB_IO_CLASS);\n\tiocp->io_subclass = cpu_to_be16(SRP_IO_SUBCLASS);\n\tiocp->protocol = cpu_to_be16(SRP_PROTOCOL);\n\tiocp->protocol_version = cpu_to_be16(SRP_PROTOCOL_VERSION);\n\tiocp->send_queue_depth = cpu_to_be16(send_queue_depth);\n\tiocp->rdma_read_depth = 4;\n\tiocp->send_size = cpu_to_be32(srp_max_req_size);\n\tiocp->rdma_size = cpu_to_be32(min(sport->port_attrib.srp_max_rdma_size,\n\t\t\t\t\t  1U << 24));\n\tiocp->num_svc_entries = 1;\n\tiocp->op_cap_mask = SRP_SEND_TO_IOC | SRP_SEND_FROM_IOC |\n\t\tSRP_RDMA_READ_FROM_IOC | SRP_RDMA_WRITE_FROM_IOC;\n\n\tmad->mad_hdr.status = 0;\n}\n\n \nstatic void srpt_get_svc_entries(u64 ioc_guid,\n\t\t\t\t u16 slot, u8 hi, u8 lo, struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_svc_entries *svc_entries;\n\n\tWARN_ON(!ioc_guid);\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2 || lo > hi || hi > 1) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tsvc_entries = (struct ib_dm_svc_entries *)mad->data;\n\tmemset(svc_entries, 0, sizeof(*svc_entries));\n\tsvc_entries->service_entries[0].id = cpu_to_be64(ioc_guid);\n\tsnprintf(svc_entries->service_entries[0].name,\n\t\t sizeof(svc_entries->service_entries[0].name),\n\t\t \"%s%016llx\",\n\t\t SRP_SERVICE_NAME_PREFIX,\n\t\t ioc_guid);\n\n\tmad->mad_hdr.status = 0;\n}\n\n \nstatic void srpt_mgmt_method_get(struct srpt_port *sp, struct ib_mad *rq_mad,\n\t\t\t\t struct ib_dm_mad *rsp_mad)\n{\n\tu16 attr_id;\n\tu32 slot;\n\tu8 hi, lo;\n\n\tattr_id = be16_to_cpu(rq_mad->mad_hdr.attr_id);\n\tswitch (attr_id) {\n\tcase DM_ATTR_CLASS_PORT_INFO:\n\t\tsrpt_get_class_port_info(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOU_INFO:\n\t\tsrpt_get_iou(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOC_PROFILE:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\tsrpt_get_ioc(sp, slot, rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_SVC_ENTRIES:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\thi = (u8) ((slot >> 8) & 0xff);\n\t\tlo = (u8) (slot & 0xff);\n\t\tslot = (u16) ((slot >> 16) & 0xffff);\n\t\tsrpt_get_svc_entries(srpt_service_guid,\n\t\t\t\t     slot, hi, lo, rsp_mad);\n\t\tbreak;\n\tdefault:\n\t\trsp_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\t}\n}\n\n \nstatic void srpt_mad_send_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_wc *mad_wc)\n{\n\trdma_destroy_ah(mad_wc->send_buf->ah, RDMA_DESTROY_AH_SLEEPABLE);\n\tib_free_send_mad(mad_wc->send_buf);\n}\n\n \nstatic void srpt_mad_recv_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_buf *send_buf,\n\t\t\t\t  struct ib_mad_recv_wc *mad_wc)\n{\n\tstruct srpt_port *sport = (struct srpt_port *)mad_agent->context;\n\tstruct ib_ah *ah;\n\tstruct ib_mad_send_buf *rsp;\n\tstruct ib_dm_mad *dm_mad;\n\n\tif (!mad_wc || !mad_wc->recv_buf.mad)\n\t\treturn;\n\n\tah = ib_create_ah_from_wc(mad_agent->qp->pd, mad_wc->wc,\n\t\t\t\t  mad_wc->recv_buf.grh, mad_agent->port_num);\n\tif (IS_ERR(ah))\n\t\tgoto err;\n\n\tBUILD_BUG_ON(offsetof(struct ib_dm_mad, data) != IB_MGMT_DEVICE_HDR);\n\n\trsp = ib_create_send_mad(mad_agent, mad_wc->wc->src_qp,\n\t\t\t\t mad_wc->wc->pkey_index, 0,\n\t\t\t\t IB_MGMT_DEVICE_HDR, IB_MGMT_DEVICE_DATA,\n\t\t\t\t GFP_KERNEL,\n\t\t\t\t IB_MGMT_BASE_VERSION);\n\tif (IS_ERR(rsp))\n\t\tgoto err_rsp;\n\n\trsp->ah = ah;\n\n\tdm_mad = rsp->mad;\n\tmemcpy(dm_mad, mad_wc->recv_buf.mad, sizeof(*dm_mad));\n\tdm_mad->mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\n\tdm_mad->mad_hdr.status = 0;\n\n\tswitch (mad_wc->recv_buf.mad->mad_hdr.method) {\n\tcase IB_MGMT_METHOD_GET:\n\t\tsrpt_mgmt_method_get(sport, mad_wc->recv_buf.mad, dm_mad);\n\t\tbreak;\n\tcase IB_MGMT_METHOD_SET:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\tdefault:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD);\n\t\tbreak;\n\t}\n\n\tif (!ib_post_send_mad(rsp, NULL)) {\n\t\tib_free_recv_mad(mad_wc);\n\t\t \n\t\treturn;\n\t}\n\n\tib_free_send_mad(rsp);\n\nerr_rsp:\n\trdma_destroy_ah(ah, RDMA_DESTROY_AH_SLEEPABLE);\nerr:\n\tib_free_recv_mad(mad_wc);\n}\n\nstatic int srpt_format_guid(char *buf, unsigned int size, const __be64 *guid)\n{\n\tconst __be16 *g = (const __be16 *)guid;\n\n\treturn snprintf(buf, size, \"%04x:%04x:%04x:%04x\",\n\t\t\tbe16_to_cpu(g[0]), be16_to_cpu(g[1]),\n\t\t\tbe16_to_cpu(g[2]), be16_to_cpu(g[3]));\n}\n\n \nstatic int srpt_refresh_port(struct srpt_port *sport)\n{\n\tstruct ib_mad_agent *mad_agent;\n\tstruct ib_mad_reg_req reg_req;\n\tstruct ib_port_modify port_modify;\n\tstruct ib_port_attr port_attr;\n\tint ret;\n\n\tret = ib_query_port(sport->sdev->device, sport->port, &port_attr);\n\tif (ret)\n\t\treturn ret;\n\n\tsport->sm_lid = port_attr.sm_lid;\n\tsport->lid = port_attr.lid;\n\n\tret = rdma_query_gid(sport->sdev->device, sport->port, 0, &sport->gid);\n\tif (ret)\n\t\treturn ret;\n\n\tsrpt_format_guid(sport->guid_name, ARRAY_SIZE(sport->guid_name),\n\t\t\t &sport->gid.global.interface_id);\n\tsnprintf(sport->gid_name, ARRAY_SIZE(sport->gid_name),\n\t\t \"0x%016llx%016llx\",\n\t\t be64_to_cpu(sport->gid.global.subnet_prefix),\n\t\t be64_to_cpu(sport->gid.global.interface_id));\n\n\tif (rdma_protocol_iwarp(sport->sdev->device, sport->port))\n\t\treturn 0;\n\n\tmemset(&port_modify, 0, sizeof(port_modify));\n\tport_modify.set_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\tport_modify.clr_port_cap_mask = 0;\n\n\tret = ib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\n\tif (ret) {\n\t\tpr_warn(\"%s-%d: enabling device management failed (%d). Note: this is expected if SR-IOV is enabled.\\n\",\n\t\t\tdev_name(&sport->sdev->device->dev), sport->port, ret);\n\t\treturn 0;\n\t}\n\n\tif (!sport->mad_agent) {\n\t\tmemset(&reg_req, 0, sizeof(reg_req));\n\t\treg_req.mgmt_class = IB_MGMT_CLASS_DEVICE_MGMT;\n\t\treg_req.mgmt_class_version = IB_MGMT_BASE_VERSION;\n\t\tset_bit(IB_MGMT_METHOD_GET, reg_req.method_mask);\n\t\tset_bit(IB_MGMT_METHOD_SET, reg_req.method_mask);\n\n\t\tmad_agent = ib_register_mad_agent(sport->sdev->device,\n\t\t\t\t\t\t  sport->port,\n\t\t\t\t\t\t  IB_QPT_GSI,\n\t\t\t\t\t\t  &reg_req, 0,\n\t\t\t\t\t\t  srpt_mad_send_handler,\n\t\t\t\t\t\t  srpt_mad_recv_handler,\n\t\t\t\t\t\t  sport, 0);\n\t\tif (IS_ERR(mad_agent)) {\n\t\t\tpr_err(\"%s-%d: MAD agent registration failed (%ld). Note: this is expected if SR-IOV is enabled.\\n\",\n\t\t\t       dev_name(&sport->sdev->device->dev), sport->port,\n\t\t\t       PTR_ERR(mad_agent));\n\t\t\tsport->mad_agent = NULL;\n\t\t\tmemset(&port_modify, 0, sizeof(port_modify));\n\t\t\tport_modify.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\t\t\tib_modify_port(sport->sdev->device, sport->port, 0,\n\t\t\t\t       &port_modify);\n\t\t\treturn 0;\n\t\t}\n\n\t\tsport->mad_agent = mad_agent;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void srpt_unregister_mad_agent(struct srpt_device *sdev, int port_cnt)\n{\n\tstruct ib_port_modify port_modify = {\n\t\t.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP,\n\t};\n\tstruct srpt_port *sport;\n\tint i;\n\n\tfor (i = 1; i <= port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tWARN_ON(sport->port != i);\n\t\tif (sport->mad_agent) {\n\t\t\tib_modify_port(sdev->device, i, 0, &port_modify);\n\t\t\tib_unregister_mad_agent(sport->mad_agent);\n\t\t\tsport->mad_agent = NULL;\n\t\t}\n\t}\n}\n\n \nstatic struct srpt_ioctx *srpt_alloc_ioctx(struct srpt_device *sdev,\n\t\t\t\t\t   int ioctx_size,\n\t\t\t\t\t   struct kmem_cache *buf_cache,\n\t\t\t\t\t   enum dma_data_direction dir)\n{\n\tstruct srpt_ioctx *ioctx;\n\n\tioctx = kzalloc(ioctx_size, GFP_KERNEL);\n\tif (!ioctx)\n\t\tgoto err;\n\n\tioctx->buf = kmem_cache_alloc(buf_cache, GFP_KERNEL);\n\tif (!ioctx->buf)\n\t\tgoto err_free_ioctx;\n\n\tioctx->dma = ib_dma_map_single(sdev->device, ioctx->buf,\n\t\t\t\t       kmem_cache_size(buf_cache), dir);\n\tif (ib_dma_mapping_error(sdev->device, ioctx->dma))\n\t\tgoto err_free_buf;\n\n\treturn ioctx;\n\nerr_free_buf:\n\tkmem_cache_free(buf_cache, ioctx->buf);\nerr_free_ioctx:\n\tkfree(ioctx);\nerr:\n\treturn NULL;\n}\n\n \nstatic void srpt_free_ioctx(struct srpt_device *sdev, struct srpt_ioctx *ioctx,\n\t\t\t    struct kmem_cache *buf_cache,\n\t\t\t    enum dma_data_direction dir)\n{\n\tif (!ioctx)\n\t\treturn;\n\n\tib_dma_unmap_single(sdev->device, ioctx->dma,\n\t\t\t    kmem_cache_size(buf_cache), dir);\n\tkmem_cache_free(buf_cache, ioctx->buf);\n\tkfree(ioctx);\n}\n\n \nstatic struct srpt_ioctx **srpt_alloc_ioctx_ring(struct srpt_device *sdev,\n\t\t\t\tint ring_size, int ioctx_size,\n\t\t\t\tstruct kmem_cache *buf_cache,\n\t\t\t\tint alignment_offset,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct srpt_ioctx **ring;\n\tint i;\n\n\tWARN_ON(ioctx_size != sizeof(struct srpt_recv_ioctx) &&\n\t\tioctx_size != sizeof(struct srpt_send_ioctx));\n\n\tring = kvmalloc_array(ring_size, sizeof(ring[0]), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\tfor (i = 0; i < ring_size; ++i) {\n\t\tring[i] = srpt_alloc_ioctx(sdev, ioctx_size, buf_cache, dir);\n\t\tif (!ring[i])\n\t\t\tgoto err;\n\t\tring[i]->index = i;\n\t\tring[i]->offset = alignment_offset;\n\t}\n\tgoto out;\n\nerr:\n\twhile (--i >= 0)\n\t\tsrpt_free_ioctx(sdev, ring[i], buf_cache, dir);\n\tkvfree(ring);\n\tring = NULL;\nout:\n\treturn ring;\n}\n\n \nstatic void srpt_free_ioctx_ring(struct srpt_ioctx **ioctx_ring,\n\t\t\t\t struct srpt_device *sdev, int ring_size,\n\t\t\t\t struct kmem_cache *buf_cache,\n\t\t\t\t enum dma_data_direction dir)\n{\n\tint i;\n\n\tif (!ioctx_ring)\n\t\treturn;\n\n\tfor (i = 0; i < ring_size; ++i)\n\t\tsrpt_free_ioctx(sdev, ioctx_ring[i], buf_cache, dir);\n\tkvfree(ioctx_ring);\n}\n\n \nstatic enum srpt_command_state srpt_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\t\t  enum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\n\tprevious = ioctx->state;\n\tif (previous != SRPT_STATE_DONE)\n\t\tioctx->state = new;\n\n\treturn previous;\n}\n\n \nstatic bool srpt_test_and_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\tenum srpt_command_state old,\n\t\t\t\t\tenum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\n\tWARN_ON(!ioctx);\n\tWARN_ON(old == SRPT_STATE_DONE);\n\tWARN_ON(new == SRPT_STATE_NEW);\n\n\tprevious = ioctx->state;\n\tif (previous == old)\n\t\tioctx->state = new;\n\n\treturn previous == old;\n}\n\n \nstatic int srpt_post_recv(struct srpt_device *sdev, struct srpt_rdma_ch *ch,\n\t\t\t  struct srpt_recv_ioctx *ioctx)\n{\n\tstruct ib_sge list;\n\tstruct ib_recv_wr wr;\n\n\tBUG_ON(!sdev);\n\tlist.addr = ioctx->ioctx.dma + ioctx->ioctx.offset;\n\tlist.length = srp_max_req_size;\n\tlist.lkey = sdev->lkey;\n\n\tioctx->ioctx.cqe.done = srpt_recv_done;\n\twr.wr_cqe = &ioctx->ioctx.cqe;\n\twr.next = NULL;\n\twr.sg_list = &list;\n\twr.num_sge = 1;\n\n\tif (sdev->use_srq)\n\t\treturn ib_post_srq_recv(sdev->srq, &wr, NULL);\n\telse\n\t\treturn ib_post_recv(ch->qp, &wr, NULL);\n}\n\n \nstatic int srpt_zerolength_write(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_rdma_wr wr = {\n\t\t.wr = {\n\t\t\t.next\t\t= NULL,\n\t\t\t{ .wr_cqe\t= &ch->zw_cqe, },\n\t\t\t.opcode\t\t= IB_WR_RDMA_WRITE,\n\t\t\t.send_flags\t= IB_SEND_SIGNALED,\n\t\t}\n\t};\n\n\tpr_debug(\"%s-%d: queued zerolength write\\n\", ch->sess_name,\n\t\t ch->qp->qp_num);\n\n\treturn ib_post_send(ch->qp, &wr.wr, NULL);\n}\n\nstatic void srpt_zerolength_write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = wc->qp->qp_context;\n\n\tpr_debug(\"%s-%d wc->status %d\\n\", ch->sess_name, ch->qp->qp_num,\n\t\t wc->status);\n\n\tif (wc->status == IB_WC_SUCCESS) {\n\t\tsrpt_process_wait_list(ch);\n\t} else {\n\t\tif (srpt_set_ch_state(ch, CH_DISCONNECTED))\n\t\t\tschedule_work(&ch->release_work);\n\t\telse\n\t\t\tpr_debug(\"%s-%d: already disconnected.\\n\",\n\t\t\t\t ch->sess_name, ch->qp->qp_num);\n\t}\n}\n\nstatic int srpt_alloc_rw_ctxs(struct srpt_send_ioctx *ioctx,\n\t\tstruct srp_direct_buf *db, int nbufs, struct scatterlist **sg,\n\t\tunsigned *sg_cnt)\n{\n\tenum dma_data_direction dir = target_reverse_dma_direction(&ioctx->cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tstruct scatterlist *prev = NULL;\n\tunsigned prev_nents;\n\tint ret, i;\n\n\tif (nbufs == 1) {\n\t\tioctx->rw_ctxs = &ioctx->s_rw_ctx;\n\t} else {\n\t\tioctx->rw_ctxs = kmalloc_array(nbufs, sizeof(*ioctx->rw_ctxs),\n\t\t\tGFP_KERNEL);\n\t\tif (!ioctx->rw_ctxs)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = ioctx->n_rw_ctx; i < nbufs; i++, db++) {\n\t\tstruct srpt_rw_ctx *ctx = &ioctx->rw_ctxs[i];\n\t\tu64 remote_addr = be64_to_cpu(db->va);\n\t\tu32 size = be32_to_cpu(db->len);\n\t\tu32 rkey = be32_to_cpu(db->key);\n\n\t\tret = target_alloc_sgl(&ctx->sg, &ctx->nents, size, false,\n\t\t\t\ti < nbufs - 1);\n\t\tif (ret)\n\t\t\tgoto unwind;\n\n\t\tret = rdma_rw_ctx_init(&ctx->rw, ch->qp, ch->sport->port,\n\t\t\t\tctx->sg, ctx->nents, 0, remote_addr, rkey, dir);\n\t\tif (ret < 0) {\n\t\t\ttarget_free_sgl(ctx->sg, ctx->nents);\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tioctx->n_rdma += ret;\n\t\tioctx->n_rw_ctx++;\n\n\t\tif (prev) {\n\t\t\tsg_unmark_end(&prev[prev_nents - 1]);\n\t\t\tsg_chain(prev, prev_nents + 1, ctx->sg);\n\t\t} else {\n\t\t\t*sg = ctx->sg;\n\t\t}\n\n\t\tprev = ctx->sg;\n\t\tprev_nents = ctx->nents;\n\n\t\t*sg_cnt += ctx->nents;\n\t}\n\n\treturn 0;\n\nunwind:\n\twhile (--i >= 0) {\n\t\tstruct srpt_rw_ctx *ctx = &ioctx->rw_ctxs[i];\n\n\t\trdma_rw_ctx_destroy(&ctx->rw, ch->qp, ch->sport->port,\n\t\t\t\tctx->sg, ctx->nents, dir);\n\t\ttarget_free_sgl(ctx->sg, ctx->nents);\n\t}\n\tif (ioctx->rw_ctxs != &ioctx->s_rw_ctx)\n\t\tkfree(ioctx->rw_ctxs);\n\treturn ret;\n}\n\nstatic void srpt_free_rw_ctxs(struct srpt_rdma_ch *ch,\n\t\t\t\t    struct srpt_send_ioctx *ioctx)\n{\n\tenum dma_data_direction dir = target_reverse_dma_direction(&ioctx->cmd);\n\tint i;\n\n\tfor (i = 0; i < ioctx->n_rw_ctx; i++) {\n\t\tstruct srpt_rw_ctx *ctx = &ioctx->rw_ctxs[i];\n\n\t\trdma_rw_ctx_destroy(&ctx->rw, ch->qp, ch->sport->port,\n\t\t\t\tctx->sg, ctx->nents, dir);\n\t\ttarget_free_sgl(ctx->sg, ctx->nents);\n\t}\n\n\tif (ioctx->rw_ctxs != &ioctx->s_rw_ctx)\n\t\tkfree(ioctx->rw_ctxs);\n}\n\nstatic inline void *srpt_get_desc_buf(struct srp_cmd *srp_cmd)\n{\n\t \n\tBUILD_BUG_ON(!__same_type(srp_cmd->add_data[0], (s8)0) &&\n\t\t     !__same_type(srp_cmd->add_data[0], (u8)0));\n\n\t \n\treturn srp_cmd->add_data + (srp_cmd->add_cdb_len & ~3);\n}\n\n \nstatic int srpt_get_desc_tbl(struct srpt_recv_ioctx *recv_ioctx,\n\t\tstruct srpt_send_ioctx *ioctx,\n\t\tstruct srp_cmd *srp_cmd, enum dma_data_direction *dir,\n\t\tstruct scatterlist **sg, unsigned int *sg_cnt, u64 *data_len,\n\t\tu16 imm_data_offset)\n{\n\tBUG_ON(!dir);\n\tBUG_ON(!data_len);\n\n\t \n\tif (srp_cmd->buf_fmt & 0xf)\n\t\t \n\t\t*dir = DMA_FROM_DEVICE;\n\telse if (srp_cmd->buf_fmt >> 4)\n\t\t \n\t\t*dir = DMA_TO_DEVICE;\n\telse\n\t\t*dir = DMA_NONE;\n\n\t \n\tioctx->cmd.data_direction = *dir;\n\n\tif (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_DIRECT) ||\n\t    ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_DIRECT)) {\n\t\tstruct srp_direct_buf *db = srpt_get_desc_buf(srp_cmd);\n\n\t\t*data_len = be32_to_cpu(db->len);\n\t\treturn srpt_alloc_rw_ctxs(ioctx, db, 1, sg, sg_cnt);\n\t} else if (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_INDIRECT) ||\n\t\t   ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_INDIRECT)) {\n\t\tstruct srp_indirect_buf *idb = srpt_get_desc_buf(srp_cmd);\n\t\tint nbufs = be32_to_cpu(idb->table_desc.len) /\n\t\t\t\tsizeof(struct srp_direct_buf);\n\n\t\tif (nbufs >\n\t\t    (srp_cmd->data_out_desc_cnt + srp_cmd->data_in_desc_cnt)) {\n\t\t\tpr_err(\"received unsupported SRP_CMD request type (%u out + %u in != %u / %zu)\\n\",\n\t\t\t       srp_cmd->data_out_desc_cnt,\n\t\t\t       srp_cmd->data_in_desc_cnt,\n\t\t\t       be32_to_cpu(idb->table_desc.len),\n\t\t\t       sizeof(struct srp_direct_buf));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t*data_len = be32_to_cpu(idb->len);\n\t\treturn srpt_alloc_rw_ctxs(ioctx, idb->desc_list, nbufs,\n\t\t\t\tsg, sg_cnt);\n\t} else if ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_IMM) {\n\t\tstruct srp_imm_buf *imm_buf = srpt_get_desc_buf(srp_cmd);\n\t\tvoid *data = (void *)srp_cmd + imm_data_offset;\n\t\tuint32_t len = be32_to_cpu(imm_buf->len);\n\t\tuint32_t req_size = imm_data_offset + len;\n\n\t\tif (req_size > srp_max_req_size) {\n\t\t\tpr_err(\"Immediate data (length %d + %d) exceeds request size %d\\n\",\n\t\t\t       imm_data_offset, len, srp_max_req_size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (recv_ioctx->byte_len < req_size) {\n\t\t\tpr_err(\"Received too few data - %d < %d\\n\",\n\t\t\t       recv_ioctx->byte_len, req_size);\n\t\t\treturn -EIO;\n\t\t}\n\t\t \n\t\tif ((void *)(imm_buf + 1) > (void *)data) {\n\t\t\tpr_err(\"Received invalid write request\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*data_len = len;\n\t\tioctx->recv_ioctx = recv_ioctx;\n\t\tif ((uintptr_t)data & 511) {\n\t\t\tpr_warn_once(\"Internal error - the receive buffers are not aligned properly.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsg_init_one(&ioctx->imm_sg, data, len);\n\t\t*sg = &ioctx->imm_sg;\n\t\t*sg_cnt = 1;\n\t\treturn 0;\n\t} else {\n\t\t*data_len = 0;\n\t\treturn 0;\n\t}\n}\n\n \nstatic int srpt_init_ch_qp(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr *attr;\n\tint ret;\n\n\tWARN_ON_ONCE(ch->using_rdma_cm);\n\n\tattr = kzalloc(sizeof(*attr), GFP_KERNEL);\n\tif (!attr)\n\t\treturn -ENOMEM;\n\n\tattr->qp_state = IB_QPS_INIT;\n\tattr->qp_access_flags = IB_ACCESS_LOCAL_WRITE;\n\tattr->port_num = ch->sport->port;\n\n\tret = ib_find_cached_pkey(ch->sport->sdev->device, ch->sport->port,\n\t\t\t\t  ch->pkey, &attr->pkey_index);\n\tif (ret < 0)\n\t\tpr_err(\"Translating pkey %#x failed (%d) - using index 0\\n\",\n\t\t       ch->pkey, ret);\n\n\tret = ib_modify_qp(qp, attr,\n\t\t\t   IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PORT |\n\t\t\t   IB_QP_PKEY_INDEX);\n\n\tkfree(attr);\n\treturn ret;\n}\n\n \nstatic int srpt_ch_qp_rtr(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tWARN_ON_ONCE(ch->using_rdma_cm);\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_cm_init_qp_attr(ch->ib_cm.cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_dest_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n \nstatic int srpt_ch_qp_rts(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = ib_cm_init_qp_attr(ch->ib_cm.cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n \nstatic int srpt_ch_qp_err(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tqp_attr.qp_state = IB_QPS_ERR;\n\treturn ib_modify_qp(ch->qp, &qp_attr, IB_QP_STATE);\n}\n\n \nstatic struct srpt_send_ioctx *srpt_get_send_ioctx(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\tint tag, cpu;\n\n\tBUG_ON(!ch);\n\n\ttag = sbitmap_queue_get(&ch->sess->sess_tag_pool, &cpu);\n\tif (tag < 0)\n\t\treturn NULL;\n\n\tioctx = ch->ioctx_ring[tag];\n\tBUG_ON(ioctx->ch != ch);\n\tioctx->state = SRPT_STATE_NEW;\n\tWARN_ON_ONCE(ioctx->recv_ioctx);\n\tioctx->n_rdma = 0;\n\tioctx->n_rw_ctx = 0;\n\tioctx->queue_status_only = false;\n\t \n\tmemset(&ioctx->cmd, 0, sizeof(ioctx->cmd));\n\tmemset(&ioctx->sense_data, 0, sizeof(ioctx->sense_data));\n\tioctx->cmd.map_tag = tag;\n\tioctx->cmd.map_cpu = cpu;\n\n\treturn ioctx;\n}\n\n \nstatic int srpt_abort_cmd(struct srpt_send_ioctx *ioctx)\n{\n\tenum srpt_command_state state;\n\n\tBUG_ON(!ioctx);\n\n\t \n\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEED_DATA:\n\t\tioctx->state = SRPT_STATE_DATA_IN;\n\t\tbreak;\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\tioctx->state = SRPT_STATE_DONE;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(true, \"%s: unexpected I/O context state %d\\n\",\n\t\t\t  __func__, state);\n\t\tbreak;\n\t}\n\n\tpr_debug(\"Aborting cmd with state %d -> %d and tag %lld\\n\", state,\n\t\t ioctx->state, ioctx->cmd.tag);\n\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\tcase SRPT_STATE_MGMT:\n\tcase SRPT_STATE_DONE:\n\t\t \n\t\tbreak;\n\tcase SRPT_STATE_NEED_DATA:\n\t\tpr_debug(\"tag %#llx: RDMA read error\\n\", ioctx->cmd.tag);\n\t\ttransport_generic_request_failure(&ioctx->cmd,\n\t\t\t\t\tTCM_CHECK_CONDITION_ABORT_CMD);\n\t\tbreak;\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\t\t \n\t\ttransport_generic_free_cmd(&ioctx->cmd, 0);\n\t\tbreak;\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\ttransport_generic_free_cmd(&ioctx->cmd, 0);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Unexpected command state (%d)\", state);\n\t\tbreak;\n\t}\n\n\treturn state;\n}\n\n \nstatic void srpt_rdma_read_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = wc->qp->qp_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, rdma_cqe);\n\n\tWARN_ON(ioctx->n_rdma <= 0);\n\tatomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\n\tioctx->n_rdma = 0;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tpr_info(\"RDMA_READ for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t\tsrpt_abort_cmd(ioctx);\n\t\treturn;\n\t}\n\n\tif (srpt_test_and_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA,\n\t\t\t\t\tSRPT_STATE_DATA_IN))\n\t\ttarget_execute_cmd(&ioctx->cmd);\n\telse\n\t\tpr_err(\"%s[%d]: wrong state = %d\\n\", __func__,\n\t\t       __LINE__, ioctx->state);\n}\n\n \nstatic int srpt_build_cmd_rsp(struct srpt_rdma_ch *ch,\n\t\t\t      struct srpt_send_ioctx *ioctx, u64 tag,\n\t\t\t      int status)\n{\n\tstruct se_cmd *cmd = &ioctx->cmd;\n\tstruct srp_rsp *srp_rsp;\n\tconst u8 *sense_data;\n\tint sense_data_len, max_sense_len;\n\tu32 resid = cmd->residual_count;\n\n\t \n\tWARN_ON(status & 1);\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\n\tsense_data = ioctx->sense_data;\n\tsense_data_len = ioctx->cmd.scsi_sense_length;\n\tWARN_ON(sense_data_len > sizeof(ioctx->sense_data));\n\n\tmemset(srp_rsp, 0, sizeof(*srp_rsp));\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\tsrp_rsp->status = status;\n\n\tif (cmd->se_cmd_flags & SCF_UNDERFLOW_BIT) {\n\t\tif (cmd->data_direction == DMA_TO_DEVICE) {\n\t\t\t \n\t\t\tsrp_rsp->flags = SRP_RSP_FLAG_DOUNDER;\n\t\t\tsrp_rsp->data_out_res_cnt = cpu_to_be32(resid);\n\t\t} else if (cmd->data_direction == DMA_FROM_DEVICE) {\n\t\t\t \n\t\t\tsrp_rsp->flags = SRP_RSP_FLAG_DIUNDER;\n\t\t\tsrp_rsp->data_in_res_cnt = cpu_to_be32(resid);\n\t\t}\n\t} else if (cmd->se_cmd_flags & SCF_OVERFLOW_BIT) {\n\t\tif (cmd->data_direction == DMA_TO_DEVICE) {\n\t\t\t \n\t\t\tsrp_rsp->flags = SRP_RSP_FLAG_DOOVER;\n\t\t\tsrp_rsp->data_out_res_cnt = cpu_to_be32(resid);\n\t\t} else if (cmd->data_direction == DMA_FROM_DEVICE) {\n\t\t\t \n\t\t\tsrp_rsp->flags = SRP_RSP_FLAG_DIOVER;\n\t\t\tsrp_rsp->data_in_res_cnt = cpu_to_be32(resid);\n\t\t}\n\t}\n\n\tif (sense_data_len) {\n\t\tBUILD_BUG_ON(MIN_MAX_RSP_SIZE <= sizeof(*srp_rsp));\n\t\tmax_sense_len = ch->max_ti_iu_len - sizeof(*srp_rsp);\n\t\tif (sense_data_len > max_sense_len) {\n\t\t\tpr_warn(\"truncated sense data from %d to %d bytes\\n\",\n\t\t\t\tsense_data_len, max_sense_len);\n\t\t\tsense_data_len = max_sense_len;\n\t\t}\n\n\t\tsrp_rsp->flags |= SRP_RSP_FLAG_SNSVALID;\n\t\tsrp_rsp->sense_data_len = cpu_to_be32(sense_data_len);\n\t\tmemcpy(srp_rsp->data, sense_data, sense_data_len);\n\t}\n\n\treturn sizeof(*srp_rsp) + sense_data_len;\n}\n\n \nstatic int srpt_build_tskmgmt_rsp(struct srpt_rdma_ch *ch,\n\t\t\t\t  struct srpt_send_ioctx *ioctx,\n\t\t\t\t  u8 rsp_code, u64 tag)\n{\n\tstruct srp_rsp *srp_rsp;\n\tint resp_data_len;\n\tint resp_len;\n\n\tresp_data_len = 4;\n\tresp_len = sizeof(*srp_rsp) + resp_data_len;\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\tmemset(srp_rsp, 0, sizeof(*srp_rsp));\n\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\n\tsrp_rsp->flags |= SRP_RSP_FLAG_RSPVALID;\n\tsrp_rsp->resp_data_len = cpu_to_be32(resp_data_len);\n\tsrp_rsp->data[3] = rsp_code;\n\n\treturn resp_len;\n}\n\nstatic int srpt_check_stop_free(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\n\treturn target_put_sess_cmd(&ioctx->cmd);\n}\n\n \nstatic void srpt_handle_cmd(struct srpt_rdma_ch *ch,\n\t\t\t    struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t    struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct se_cmd *cmd;\n\tstruct srp_cmd *srp_cmd;\n\tstruct scatterlist *sg = NULL;\n\tunsigned sg_cnt = 0;\n\tu64 data_len;\n\tenum dma_data_direction dir;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_cmd = recv_ioctx->ioctx.buf + recv_ioctx->ioctx.offset;\n\tcmd = &send_ioctx->cmd;\n\tcmd->tag = srp_cmd->tag;\n\n\tswitch (srp_cmd->task_attr) {\n\tcase SRP_CMD_SIMPLE_Q:\n\t\tcmd->sam_task_attr = TCM_SIMPLE_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ORDERED_Q:\n\tdefault:\n\t\tcmd->sam_task_attr = TCM_ORDERED_TAG;\n\t\tbreak;\n\tcase SRP_CMD_HEAD_OF_Q:\n\t\tcmd->sam_task_attr = TCM_HEAD_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ACA:\n\t\tcmd->sam_task_attr = TCM_ACA_TAG;\n\t\tbreak;\n\t}\n\n\trc = srpt_get_desc_tbl(recv_ioctx, send_ioctx, srp_cmd, &dir,\n\t\t\t       &sg, &sg_cnt, &data_len, ch->imm_data_offset);\n\tif (rc) {\n\t\tif (rc != -EAGAIN) {\n\t\t\tpr_err(\"0x%llx: parsing SRP descriptor table failed.\\n\",\n\t\t\t       srp_cmd->tag);\n\t\t}\n\t\tgoto busy;\n\t}\n\n\trc = target_init_cmd(cmd, ch->sess, &send_ioctx->sense_data[0],\n\t\t\t     scsilun_to_int(&srp_cmd->lun), data_len,\n\t\t\t     TCM_SIMPLE_TAG, dir, TARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tpr_debug(\"target_submit_cmd() returned %d for tag %#llx\\n\", rc,\n\t\t\t srp_cmd->tag);\n\t\tgoto busy;\n\t}\n\n\tif (target_submit_prep(cmd, srp_cmd->cdb, sg, sg_cnt, NULL, 0, NULL, 0,\n\t\t\t       GFP_KERNEL))\n\t\treturn;\n\n\ttarget_submit(cmd);\n\treturn;\n\nbusy:\n\ttarget_send_busy(cmd);\n}\n\nstatic int srp_tmr_to_tcm(int fn)\n{\n\tswitch (fn) {\n\tcase SRP_TSK_ABORT_TASK:\n\t\treturn TMR_ABORT_TASK;\n\tcase SRP_TSK_ABORT_TASK_SET:\n\t\treturn TMR_ABORT_TASK_SET;\n\tcase SRP_TSK_CLEAR_TASK_SET:\n\t\treturn TMR_CLEAR_TASK_SET;\n\tcase SRP_TSK_LUN_RESET:\n\t\treturn TMR_LUN_RESET;\n\tcase SRP_TSK_CLEAR_ACA:\n\t\treturn TMR_CLEAR_ACA;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\n \nstatic void srpt_handle_tsk_mgmt(struct srpt_rdma_ch *ch,\n\t\t\t\t struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t\t struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct srp_tsk_mgmt *srp_tsk;\n\tstruct se_cmd *cmd;\n\tstruct se_session *sess = ch->sess;\n\tint tcm_tmr;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_tsk = recv_ioctx->ioctx.buf + recv_ioctx->ioctx.offset;\n\tcmd = &send_ioctx->cmd;\n\n\tpr_debug(\"recv tsk_mgmt fn %d for task_tag %lld and cmd tag %lld ch %p sess %p\\n\",\n\t\t srp_tsk->tsk_mgmt_func, srp_tsk->task_tag, srp_tsk->tag, ch,\n\t\t ch->sess);\n\n\tsrpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);\n\tsend_ioctx->cmd.tag = srp_tsk->tag;\n\ttcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);\n\trc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,\n\t\t\t       scsilun_to_int(&srp_tsk->lun), srp_tsk, tcm_tmr,\n\t\t\t       GFP_KERNEL, srp_tsk->task_tag,\n\t\t\t       TARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tsend_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;\n\t\tcmd->se_tfo->queue_tm_rsp(cmd);\n\t}\n\treturn;\n}\n\n \nstatic bool\nsrpt_handle_new_iu(struct srpt_rdma_ch *ch, struct srpt_recv_ioctx *recv_ioctx)\n{\n\tstruct srpt_send_ioctx *send_ioctx = NULL;\n\tstruct srp_cmd *srp_cmd;\n\tbool res = false;\n\tu8 opcode;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!recv_ioctx);\n\n\tif (unlikely(ch->state == CH_CONNECTING))\n\t\tgoto push;\n\n\tib_dma_sync_single_for_cpu(ch->sport->sdev->device,\n\t\t\t\t   recv_ioctx->ioctx.dma,\n\t\t\t\t   recv_ioctx->ioctx.offset + srp_max_req_size,\n\t\t\t\t   DMA_FROM_DEVICE);\n\n\tsrp_cmd = recv_ioctx->ioctx.buf + recv_ioctx->ioctx.offset;\n\topcode = srp_cmd->opcode;\n\tif (opcode == SRP_CMD || opcode == SRP_TSK_MGMT) {\n\t\tsend_ioctx = srpt_get_send_ioctx(ch);\n\t\tif (unlikely(!send_ioctx))\n\t\t\tgoto push;\n\t}\n\n\tif (!list_empty(&recv_ioctx->wait_list)) {\n\t\tWARN_ON_ONCE(!ch->processing_wait_list);\n\t\tlist_del_init(&recv_ioctx->wait_list);\n\t}\n\n\tswitch (opcode) {\n\tcase SRP_CMD:\n\t\tsrpt_handle_cmd(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_TSK_MGMT:\n\t\tsrpt_handle_tsk_mgmt(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_I_LOGOUT:\n\t\tpr_err(\"Not yet implemented: SRP_I_LOGOUT\\n\");\n\t\tbreak;\n\tcase SRP_CRED_RSP:\n\t\tpr_debug(\"received SRP_CRED_RSP\\n\");\n\t\tbreak;\n\tcase SRP_AER_RSP:\n\t\tpr_debug(\"received SRP_AER_RSP\\n\");\n\t\tbreak;\n\tcase SRP_RSP:\n\t\tpr_err(\"Received SRP_RSP\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received IU with unknown opcode 0x%x\\n\", opcode);\n\t\tbreak;\n\t}\n\n\tif (!send_ioctx || !send_ioctx->recv_ioctx)\n\t\tsrpt_post_recv(ch->sport->sdev, ch, recv_ioctx);\n\tres = true;\n\nout:\n\treturn res;\n\npush:\n\tif (list_empty(&recv_ioctx->wait_list)) {\n\t\tWARN_ON_ONCE(ch->processing_wait_list);\n\t\tlist_add_tail(&recv_ioctx->wait_list, &ch->cmd_wait_list);\n\t}\n\tgoto out;\n}\n\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = wc->qp->qp_context;\n\tstruct srpt_recv_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_recv_ioctx, ioctx.cqe);\n\n\tif (wc->status == IB_WC_SUCCESS) {\n\t\tint req_lim;\n\n\t\treq_lim = atomic_dec_return(&ch->req_lim);\n\t\tif (unlikely(req_lim < 0))\n\t\t\tpr_err(\"req_lim = %d < 0\\n\", req_lim);\n\t\tioctx->byte_len = wc->byte_len;\n\t\tsrpt_handle_new_iu(ch, ioctx);\n\t} else {\n\t\tpr_info_ratelimited(\"receiving failed for ioctx %p with status %d\\n\",\n\t\t\t\t    ioctx, wc->status);\n\t}\n}\n\n \nstatic void srpt_process_wait_list(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_recv_ioctx *recv_ioctx, *tmp;\n\n\tWARN_ON_ONCE(ch->state == CH_CONNECTING);\n\n\tif (list_empty(&ch->cmd_wait_list))\n\t\treturn;\n\n\tWARN_ON_ONCE(ch->processing_wait_list);\n\tch->processing_wait_list = true;\n\tlist_for_each_entry_safe(recv_ioctx, tmp, &ch->cmd_wait_list,\n\t\t\t\t wait_list) {\n\t\tif (!srpt_handle_new_iu(ch, recv_ioctx))\n\t\t\tbreak;\n\t}\n\tch->processing_wait_list = false;\n}\n\n \nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = wc->qp->qp_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, ioctx.cqe);\n\tenum srpt_command_state state;\n\n\tstate = srpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\n\tWARN_ON(state != SRPT_STATE_CMD_RSP_SENT &&\n\t\tstate != SRPT_STATE_MGMT_RSP_SENT);\n\n\tatomic_add(1 + ioctx->n_rdma, &ch->sq_wr_avail);\n\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tpr_info(\"sending response for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\n\tif (state != SRPT_STATE_DONE) {\n\t\ttransport_generic_free_cmd(&ioctx->cmd, 0);\n\t} else {\n\t\tpr_err(\"IB completion has been received too late for wr_id = %u.\\n\",\n\t\t       ioctx->ioctx.index);\n\t}\n\n\tsrpt_process_wait_list(ch);\n}\n\n \nstatic int srpt_create_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_init_attr *qp_init;\n\tstruct srpt_port *sport = ch->sport;\n\tstruct srpt_device *sdev = sport->sdev;\n\tconst struct ib_device_attr *attrs = &sdev->device->attrs;\n\tint sq_size = sport->port_attrib.srp_sq_size;\n\tint i, ret;\n\n\tWARN_ON(ch->rq_size < 1);\n\n\tret = -ENOMEM;\n\tqp_init = kzalloc(sizeof(*qp_init), GFP_KERNEL);\n\tif (!qp_init)\n\t\tgoto out;\n\nretry:\n\tch->cq = ib_cq_pool_get(sdev->device, ch->rq_size + sq_size, -1,\n\t\t\t\t IB_POLL_WORKQUEUE);\n\tif (IS_ERR(ch->cq)) {\n\t\tret = PTR_ERR(ch->cq);\n\t\tpr_err(\"failed to create CQ cqe= %d ret= %d\\n\",\n\t\t       ch->rq_size + sq_size, ret);\n\t\tgoto out;\n\t}\n\tch->cq_size = ch->rq_size + sq_size;\n\n\tqp_init->qp_context = (void *)ch;\n\tqp_init->event_handler\n\t\t= (void(*)(struct ib_event *, void*))srpt_qp_event;\n\tqp_init->send_cq = ch->cq;\n\tqp_init->recv_cq = ch->cq;\n\tqp_init->sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_init->qp_type = IB_QPT_RC;\n\t \n\tqp_init->cap.max_send_wr = min(sq_size / 2, attrs->max_qp_wr);\n\tqp_init->cap.max_rdma_ctxs = sq_size / 2;\n\tqp_init->cap.max_send_sge = attrs->max_send_sge;\n\tqp_init->cap.max_recv_sge = 1;\n\tqp_init->port_num = ch->sport->port;\n\tif (sdev->use_srq)\n\t\tqp_init->srq = sdev->srq;\n\telse\n\t\tqp_init->cap.max_recv_wr = ch->rq_size;\n\n\tif (ch->using_rdma_cm) {\n\t\tret = rdma_create_qp(ch->rdma_cm.cm_id, sdev->pd, qp_init);\n\t\tch->qp = ch->rdma_cm.cm_id->qp;\n\t} else {\n\t\tch->qp = ib_create_qp(sdev->pd, qp_init);\n\t\tif (!IS_ERR(ch->qp)) {\n\t\t\tret = srpt_init_ch_qp(ch, ch->qp);\n\t\t\tif (ret)\n\t\t\t\tib_destroy_qp(ch->qp);\n\t\t} else {\n\t\t\tret = PTR_ERR(ch->qp);\n\t\t}\n\t}\n\tif (ret) {\n\t\tbool retry = sq_size > MIN_SRPT_SQ_SIZE;\n\n\t\tif (retry) {\n\t\t\tpr_debug(\"failed to create queue pair with sq_size = %d (%d) - retrying\\n\",\n\t\t\t\t sq_size, ret);\n\t\t\tib_cq_pool_put(ch->cq, ch->cq_size);\n\t\t\tsq_size = max(sq_size / 2, MIN_SRPT_SQ_SIZE);\n\t\t\tgoto retry;\n\t\t} else {\n\t\t\tpr_err(\"failed to create queue pair with sq_size = %d (%d)\\n\",\n\t\t\t       sq_size, ret);\n\t\t\tgoto err_destroy_cq;\n\t\t}\n\t}\n\n\tatomic_set(&ch->sq_wr_avail, qp_init->cap.max_send_wr);\n\n\tpr_debug(\"%s: max_cqe= %d max_sge= %d sq_size = %d ch= %p\\n\",\n\t\t __func__, ch->cq->cqe, qp_init->cap.max_send_sge,\n\t\t qp_init->cap.max_send_wr, ch);\n\n\tif (!sdev->use_srq)\n\t\tfor (i = 0; i < ch->rq_size; i++)\n\t\t\tsrpt_post_recv(sdev, ch, ch->ioctx_recv_ring[i]);\n\nout:\n\tkfree(qp_init);\n\treturn ret;\n\nerr_destroy_cq:\n\tch->qp = NULL;\n\tib_cq_pool_put(ch->cq, ch->cq_size);\n\tgoto out;\n}\n\nstatic void srpt_destroy_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tib_destroy_qp(ch->qp);\n\tib_cq_pool_put(ch->cq, ch->cq_size);\n}\n\n \nstatic bool srpt_close_ch(struct srpt_rdma_ch *ch)\n{\n\tint ret;\n\n\tif (!srpt_set_ch_state(ch, CH_DRAINING)) {\n\t\tpr_debug(\"%s: already closed\\n\", ch->sess_name);\n\t\treturn false;\n\t}\n\n\tkref_get(&ch->kref);\n\n\tret = srpt_ch_qp_err(ch);\n\tif (ret < 0)\n\t\tpr_err(\"%s-%d: changing queue pair into error state failed: %d\\n\",\n\t\t       ch->sess_name, ch->qp->qp_num, ret);\n\n\tret = srpt_zerolength_write(ch);\n\tif (ret < 0) {\n\t\tpr_err(\"%s-%d: queuing zero-length write failed: %d\\n\",\n\t\t       ch->sess_name, ch->qp->qp_num, ret);\n\t\tif (srpt_set_ch_state(ch, CH_DISCONNECTED))\n\t\t\tschedule_work(&ch->release_work);\n\t\telse\n\t\t\tWARN_ON_ONCE(true);\n\t}\n\n\tkref_put(&ch->kref, srpt_free_ch);\n\n\treturn true;\n}\n\n \nstatic int srpt_disconnect_ch(struct srpt_rdma_ch *ch)\n{\n\tint ret;\n\n\tif (!srpt_set_ch_state(ch, CH_DISCONNECTING))\n\t\treturn -ENOTCONN;\n\n\tif (ch->using_rdma_cm) {\n\t\tret = rdma_disconnect(ch->rdma_cm.cm_id);\n\t} else {\n\t\tret = ib_send_cm_dreq(ch->ib_cm.cm_id, NULL, 0);\n\t\tif (ret < 0)\n\t\t\tret = ib_send_cm_drep(ch->ib_cm.cm_id, NULL, 0);\n\t}\n\n\tif (ret < 0 && srpt_close_ch(ch))\n\t\tret = 0;\n\n\treturn ret;\n}\n\n \nstatic void srpt_disconnect_ch_sync(struct srpt_rdma_ch *ch)\n{\n\tDECLARE_COMPLETION_ONSTACK(closed);\n\tstruct srpt_port *sport = ch->sport;\n\n\tpr_debug(\"ch %s-%d state %d\\n\", ch->sess_name, ch->qp->qp_num,\n\t\t ch->state);\n\n\tch->closed = &closed;\n\n\tmutex_lock(&sport->mutex);\n\tsrpt_disconnect_ch(ch);\n\tmutex_unlock(&sport->mutex);\n\n\twhile (wait_for_completion_timeout(&closed, 5 * HZ) == 0)\n\t\tpr_info(\"%s(%s-%d state %d): still waiting ...\\n\", __func__,\n\t\t\tch->sess_name, ch->qp->qp_num, ch->state);\n\n}\n\nstatic void __srpt_close_all_ch(struct srpt_port *sport)\n{\n\tstruct srpt_nexus *nexus;\n\tstruct srpt_rdma_ch *ch;\n\n\tlockdep_assert_held(&sport->mutex);\n\n\tlist_for_each_entry(nexus, &sport->nexus_list, entry) {\n\t\tlist_for_each_entry(ch, &nexus->ch_list, list) {\n\t\t\tif (srpt_disconnect_ch(ch) >= 0)\n\t\t\t\tpr_info(\"Closing channel %s-%d because target %s_%d has been disabled\\n\",\n\t\t\t\t\tch->sess_name, ch->qp->qp_num,\n\t\t\t\t\tdev_name(&sport->sdev->device->dev),\n\t\t\t\t\tsport->port);\n\t\t\tsrpt_close_ch(ch);\n\t\t}\n\t}\n}\n\n \nstatic struct srpt_nexus *srpt_get_nexus(struct srpt_port *sport,\n\t\t\t\t\t const u8 i_port_id[16],\n\t\t\t\t\t const u8 t_port_id[16])\n{\n\tstruct srpt_nexus *nexus = NULL, *tmp_nexus = NULL, *n;\n\n\tfor (;;) {\n\t\tmutex_lock(&sport->mutex);\n\t\tlist_for_each_entry(n, &sport->nexus_list, entry) {\n\t\t\tif (memcmp(n->i_port_id, i_port_id, 16) == 0 &&\n\t\t\t    memcmp(n->t_port_id, t_port_id, 16) == 0) {\n\t\t\t\tnexus = n;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!nexus && tmp_nexus) {\n\t\t\tlist_add_tail_rcu(&tmp_nexus->entry,\n\t\t\t\t\t  &sport->nexus_list);\n\t\t\tswap(nexus, tmp_nexus);\n\t\t}\n\t\tmutex_unlock(&sport->mutex);\n\n\t\tif (nexus)\n\t\t\tbreak;\n\t\ttmp_nexus = kzalloc(sizeof(*nexus), GFP_KERNEL);\n\t\tif (!tmp_nexus) {\n\t\t\tnexus = ERR_PTR(-ENOMEM);\n\t\t\tbreak;\n\t\t}\n\t\tINIT_LIST_HEAD(&tmp_nexus->ch_list);\n\t\tmemcpy(tmp_nexus->i_port_id, i_port_id, 16);\n\t\tmemcpy(tmp_nexus->t_port_id, t_port_id, 16);\n\t}\n\n\tkfree(tmp_nexus);\n\n\treturn nexus;\n}\n\nstatic void srpt_set_enabled(struct srpt_port *sport, bool enabled)\n\t__must_hold(&sport->mutex)\n{\n\tlockdep_assert_held(&sport->mutex);\n\n\tif (sport->enabled == enabled)\n\t\treturn;\n\tsport->enabled = enabled;\n\tif (!enabled)\n\t\t__srpt_close_all_ch(sport);\n}\n\nstatic void srpt_drop_sport_ref(struct srpt_port *sport)\n{\n\tif (atomic_dec_return(&sport->refcount) == 0 && sport->freed_channels)\n\t\tcomplete(sport->freed_channels);\n}\n\nstatic void srpt_free_ch(struct kref *kref)\n{\n\tstruct srpt_rdma_ch *ch = container_of(kref, struct srpt_rdma_ch, kref);\n\n\tsrpt_drop_sport_ref(ch->sport);\n\tkfree_rcu(ch, rcu);\n}\n\n \nstatic void srpt_release_channel_work(struct work_struct *w)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tstruct se_session *se_sess;\n\n\tch = container_of(w, struct srpt_rdma_ch, release_work);\n\tpr_debug(\"%s-%d\\n\", ch->sess_name, ch->qp->qp_num);\n\n\tsdev = ch->sport->sdev;\n\tBUG_ON(!sdev);\n\n\tse_sess = ch->sess;\n\tBUG_ON(!se_sess);\n\n\ttarget_stop_session(se_sess);\n\ttarget_wait_for_sess_cmds(se_sess);\n\n\ttarget_remove_session(se_sess);\n\tch->sess = NULL;\n\n\tif (ch->using_rdma_cm)\n\t\trdma_destroy_id(ch->rdma_cm.cm_id);\n\telse\n\t\tib_destroy_cm_id(ch->ib_cm.cm_id);\n\n\tsport = ch->sport;\n\tmutex_lock(&sport->mutex);\n\tlist_del_rcu(&ch->list);\n\tmutex_unlock(&sport->mutex);\n\n\tif (ch->closed)\n\t\tcomplete(ch->closed);\n\n\tsrpt_destroy_ch_ib(ch);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_buf_cache, DMA_TO_DEVICE);\n\n\tkmem_cache_destroy(ch->rsp_buf_cache);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_recv_ring,\n\t\t\t     sdev, ch->rq_size,\n\t\t\t     ch->req_buf_cache, DMA_FROM_DEVICE);\n\n\tkmem_cache_destroy(ch->req_buf_cache);\n\n\tkref_put(&ch->kref, srpt_free_ch);\n}\n\n \nstatic int srpt_cm_req_recv(struct srpt_device *const sdev,\n\t\t\t    struct ib_cm_id *ib_cm_id,\n\t\t\t    struct rdma_cm_id *rdma_cm_id,\n\t\t\t    u8 port_num, __be16 pkey,\n\t\t\t    const struct srp_login_req *req,\n\t\t\t    const char *src_addr)\n{\n\tstruct srpt_port *sport = &sdev->port[port_num - 1];\n\tstruct srpt_nexus *nexus;\n\tstruct srp_login_rsp *rsp = NULL;\n\tstruct srp_login_rej *rej = NULL;\n\tunion {\n\t\tstruct rdma_conn_param rdma_cm;\n\t\tstruct ib_cm_rep_param ib_cm;\n\t} *rep_param = NULL;\n\tstruct srpt_rdma_ch *ch = NULL;\n\tchar i_port_id[36];\n\tu32 it_iu_len;\n\tint i, tag_num, tag_size, ret;\n\tstruct srpt_tpg *stpg;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tit_iu_len = be32_to_cpu(req->req_it_iu_len);\n\n\tpr_info(\"Received SRP_LOGIN_REQ with i_port_id %pI6, t_port_id %pI6 and it_iu_len %d on port %d (guid=%pI6); pkey %#04x\\n\",\n\t\treq->initiator_port_id, req->target_port_id, it_iu_len,\n\t\tport_num, &sport->gid, be16_to_cpu(pkey));\n\n\tnexus = srpt_get_nexus(sport, req->initiator_port_id,\n\t\t\t       req->target_port_id);\n\tif (IS_ERR(nexus)) {\n\t\tret = PTR_ERR(nexus);\n\t\tgoto out;\n\t}\n\n\tret = -ENOMEM;\n\trsp = kzalloc(sizeof(*rsp), GFP_KERNEL);\n\trej = kzalloc(sizeof(*rej), GFP_KERNEL);\n\trep_param = kzalloc(sizeof(*rep_param), GFP_KERNEL);\n\tif (!rsp || !rej || !rep_param)\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (it_iu_len > srp_max_req_size || it_iu_len < 64) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because its length (%d bytes) is out of range (%d .. %d)\\n\",\n\t\t       it_iu_len, 64, srp_max_req_size);\n\t\tgoto reject;\n\t}\n\n\tif (!sport->enabled) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_info(\"rejected SRP_LOGIN_REQ because target port %s_%d has not yet been enabled\\n\",\n\t\t\tdev_name(&sport->sdev->device->dev), port_num);\n\t\tgoto reject;\n\t}\n\n\tif (*(__be64 *)req->target_port_id != cpu_to_be64(srpt_service_guid)\n\t    || *(__be64 *)(req->target_port_id + 8) !=\n\t       cpu_to_be64(srpt_service_guid)) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_UNABLE_ASSOCIATE_CHANNEL);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because it has an invalid target port identifier.\\n\");\n\t\tgoto reject;\n\t}\n\n\tret = -ENOMEM;\n\tch = kzalloc(sizeof(*ch), GFP_KERNEL);\n\tif (!ch) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because out of memory.\\n\");\n\t\tgoto reject;\n\t}\n\n\tkref_init(&ch->kref);\n\tch->pkey = be16_to_cpu(pkey);\n\tch->nexus = nexus;\n\tch->zw_cqe.done = srpt_zerolength_write_done;\n\tINIT_WORK(&ch->release_work, srpt_release_channel_work);\n\tch->sport = sport;\n\tif (rdma_cm_id) {\n\t\tch->using_rdma_cm = true;\n\t\tch->rdma_cm.cm_id = rdma_cm_id;\n\t\trdma_cm_id->context = ch;\n\t} else {\n\t\tch->ib_cm.cm_id = ib_cm_id;\n\t\tib_cm_id->context = ch;\n\t}\n\t \n\tch->rq_size = min(MAX_SRPT_RQ_SIZE, sdev->device->attrs.max_qp_wr);\n\tspin_lock_init(&ch->spinlock);\n\tch->state = CH_CONNECTING;\n\tINIT_LIST_HEAD(&ch->cmd_wait_list);\n\tch->max_rsp_size = ch->sport->port_attrib.srp_max_rsp_size;\n\n\tch->rsp_buf_cache = kmem_cache_create(\"srpt-rsp-buf\", ch->max_rsp_size,\n\t\t\t\t\t      512, 0, NULL);\n\tif (!ch->rsp_buf_cache)\n\t\tgoto free_ch;\n\n\tch->ioctx_ring = (struct srpt_send_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(ch->sport->sdev, ch->rq_size,\n\t\t\t\t      sizeof(*ch->ioctx_ring[0]),\n\t\t\t\t      ch->rsp_buf_cache, 0, DMA_TO_DEVICE);\n\tif (!ch->ioctx_ring) {\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because creating a new QP SQ ring failed.\\n\");\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tgoto free_rsp_cache;\n\t}\n\n\tfor (i = 0; i < ch->rq_size; i++)\n\t\tch->ioctx_ring[i]->ch = ch;\n\tif (!sdev->use_srq) {\n\t\tu16 imm_data_offset = req->req_flags & SRP_IMMED_REQUESTED ?\n\t\t\tbe16_to_cpu(req->imm_data_offset) : 0;\n\t\tu16 alignment_offset;\n\t\tu32 req_sz;\n\n\t\tif (req->req_flags & SRP_IMMED_REQUESTED)\n\t\t\tpr_debug(\"imm_data_offset = %d\\n\",\n\t\t\t\t be16_to_cpu(req->imm_data_offset));\n\t\tif (imm_data_offset >= sizeof(struct srp_cmd)) {\n\t\t\tch->imm_data_offset = imm_data_offset;\n\t\t\trsp->rsp_flags |= SRP_LOGIN_RSP_IMMED_SUPP;\n\t\t} else {\n\t\t\tch->imm_data_offset = 0;\n\t\t}\n\t\talignment_offset = round_up(imm_data_offset, 512) -\n\t\t\timm_data_offset;\n\t\treq_sz = alignment_offset + imm_data_offset + srp_max_req_size;\n\t\tch->req_buf_cache = kmem_cache_create(\"srpt-req-buf\", req_sz,\n\t\t\t\t\t\t      512, 0, NULL);\n\t\tif (!ch->req_buf_cache)\n\t\t\tgoto free_rsp_ring;\n\n\t\tch->ioctx_recv_ring = (struct srpt_recv_ioctx **)\n\t\t\tsrpt_alloc_ioctx_ring(ch->sport->sdev, ch->rq_size,\n\t\t\t\t\t      sizeof(*ch->ioctx_recv_ring[0]),\n\t\t\t\t\t      ch->req_buf_cache,\n\t\t\t\t\t      alignment_offset,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\t\tif (!ch->ioctx_recv_ring) {\n\t\t\tpr_err(\"rejected SRP_LOGIN_REQ because creating a new QP RQ ring failed.\\n\");\n\t\t\trej->reason =\n\t\t\t    cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\t\tgoto free_recv_cache;\n\t\t}\n\t\tfor (i = 0; i < ch->rq_size; i++)\n\t\t\tINIT_LIST_HEAD(&ch->ioctx_recv_ring[i]->wait_list);\n\t}\n\n\tret = srpt_create_ch_ib(ch);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because creating a new RDMA channel failed.\\n\");\n\t\tgoto free_recv_ring;\n\t}\n\n\tstrscpy(ch->sess_name, src_addr, sizeof(ch->sess_name));\n\tsnprintf(i_port_id, sizeof(i_port_id), \"0x%016llx%016llx\",\n\t\t\tbe64_to_cpu(*(__be64 *)nexus->i_port_id),\n\t\t\tbe64_to_cpu(*(__be64 *)(nexus->i_port_id + 8)));\n\n\tpr_debug(\"registering src addr %s or i_port_id %s\\n\", ch->sess_name,\n\t\t i_port_id);\n\n\ttag_num = ch->rq_size;\n\ttag_size = 1;  \n\n\tif (sport->guid_id) {\n\t\tmutex_lock(&sport->guid_id->mutex);\n\t\tlist_for_each_entry(stpg, &sport->guid_id->tpg_list, entry) {\n\t\t\tif (!IS_ERR_OR_NULL(ch->sess))\n\t\t\t\tbreak;\n\t\t\tch->sess = target_setup_session(&stpg->tpg, tag_num,\n\t\t\t\t\t\ttag_size, TARGET_PROT_NORMAL,\n\t\t\t\t\t\tch->sess_name, ch, NULL);\n\t\t}\n\t\tmutex_unlock(&sport->guid_id->mutex);\n\t}\n\n\tif (sport->gid_id) {\n\t\tmutex_lock(&sport->gid_id->mutex);\n\t\tlist_for_each_entry(stpg, &sport->gid_id->tpg_list, entry) {\n\t\t\tif (!IS_ERR_OR_NULL(ch->sess))\n\t\t\t\tbreak;\n\t\t\tch->sess = target_setup_session(&stpg->tpg, tag_num,\n\t\t\t\t\ttag_size, TARGET_PROT_NORMAL, i_port_id,\n\t\t\t\t\tch, NULL);\n\t\t\tif (!IS_ERR_OR_NULL(ch->sess))\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tch->sess = target_setup_session(&stpg->tpg, tag_num,\n\t\t\t\t\t\ttag_size, TARGET_PROT_NORMAL,\n\t\t\t\t\t\ti_port_id + 2, ch, NULL);\n\t\t}\n\t\tmutex_unlock(&sport->gid_id->mutex);\n\t}\n\n\tif (IS_ERR_OR_NULL(ch->sess)) {\n\t\tWARN_ON_ONCE(ch->sess == NULL);\n\t\tret = PTR_ERR(ch->sess);\n\t\tch->sess = NULL;\n\t\tpr_info(\"Rejected login for initiator %s: ret = %d.\\n\",\n\t\t\tch->sess_name, ret);\n\t\trej->reason = cpu_to_be32(ret == -ENOMEM ?\n\t\t\t\tSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES :\n\t\t\t\tSRP_LOGIN_REJ_CHANNEL_LIMIT_REACHED);\n\t\tgoto destroy_ib;\n\t}\n\n\t \n\tatomic_inc(&sport->refcount);\n\n\tmutex_lock(&sport->mutex);\n\n\tif ((req->req_flags & SRP_MTCH_ACTION) == SRP_MULTICHAN_SINGLE) {\n\t\tstruct srpt_rdma_ch *ch2;\n\n\t\tlist_for_each_entry(ch2, &nexus->ch_list, list) {\n\t\t\tif (srpt_disconnect_ch(ch2) < 0)\n\t\t\t\tcontinue;\n\t\t\tpr_info(\"Relogin - closed existing channel %s\\n\",\n\t\t\t\tch2->sess_name);\n\t\t\trsp->rsp_flags |= SRP_LOGIN_RSP_MULTICHAN_TERMINATED;\n\t\t}\n\t} else {\n\t\trsp->rsp_flags |= SRP_LOGIN_RSP_MULTICHAN_MAINTAINED;\n\t}\n\n\tlist_add_tail_rcu(&ch->list, &nexus->ch_list);\n\n\tif (!sport->enabled) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_info(\"rejected SRP_LOGIN_REQ because target %s_%d is not enabled\\n\",\n\t\t\tdev_name(&sdev->device->dev), port_num);\n\t\tmutex_unlock(&sport->mutex);\n\t\tret = -EINVAL;\n\t\tgoto reject;\n\t}\n\n\tmutex_unlock(&sport->mutex);\n\n\tret = ch->using_rdma_cm ? 0 : srpt_ch_qp_rtr(ch, ch->qp);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because enabling RTR failed (error code = %d)\\n\",\n\t\t       ret);\n\t\tgoto reject;\n\t}\n\n\tpr_debug(\"Establish connection sess=%p name=%s ch=%p\\n\", ch->sess,\n\t\t ch->sess_name, ch);\n\n\t \n\trsp->opcode = SRP_LOGIN_RSP;\n\trsp->tag = req->tag;\n\trsp->max_it_iu_len = cpu_to_be32(srp_max_req_size);\n\trsp->max_ti_iu_len = req->req_it_iu_len;\n\tch->max_ti_iu_len = it_iu_len;\n\trsp->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\n\t\t\t\t   SRP_BUF_FORMAT_INDIRECT);\n\trsp->req_lim_delta = cpu_to_be32(ch->rq_size);\n\tatomic_set(&ch->req_lim, ch->rq_size);\n\tatomic_set(&ch->req_lim_delta, 0);\n\n\t \n\tif (ch->using_rdma_cm) {\n\t\trep_param->rdma_cm.private_data = (void *)rsp;\n\t\trep_param->rdma_cm.private_data_len = sizeof(*rsp);\n\t\trep_param->rdma_cm.rnr_retry_count = 7;\n\t\trep_param->rdma_cm.flow_control = 1;\n\t\trep_param->rdma_cm.responder_resources = 4;\n\t\trep_param->rdma_cm.initiator_depth = 4;\n\t} else {\n\t\trep_param->ib_cm.qp_num = ch->qp->qp_num;\n\t\trep_param->ib_cm.private_data = (void *)rsp;\n\t\trep_param->ib_cm.private_data_len = sizeof(*rsp);\n\t\trep_param->ib_cm.rnr_retry_count = 7;\n\t\trep_param->ib_cm.flow_control = 1;\n\t\trep_param->ib_cm.failover_accepted = 0;\n\t\trep_param->ib_cm.srq = 1;\n\t\trep_param->ib_cm.responder_resources = 4;\n\t\trep_param->ib_cm.initiator_depth = 4;\n\t}\n\n\t \n\tmutex_lock(&sport->mutex);\n\tif (sport->enabled && ch->state == CH_CONNECTING) {\n\t\tif (ch->using_rdma_cm)\n\t\t\tret = rdma_accept(rdma_cm_id, &rep_param->rdma_cm);\n\t\telse\n\t\t\tret = ib_send_cm_rep(ib_cm_id, &rep_param->ib_cm);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\tmutex_unlock(&sport->mutex);\n\n\tswitch (ret) {\n\tcase 0:\n\t\tbreak;\n\tcase -EINVAL:\n\t\tgoto reject;\n\tdefault:\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"sending SRP_LOGIN_REQ response failed (error code = %d)\\n\",\n\t\t       ret);\n\t\tgoto reject;\n\t}\n\n\tgoto out;\n\ndestroy_ib:\n\tsrpt_destroy_ch_ib(ch);\n\nfree_recv_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_recv_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->req_buf_cache, DMA_FROM_DEVICE);\n\nfree_recv_cache:\n\tkmem_cache_destroy(ch->req_buf_cache);\n\nfree_rsp_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_buf_cache, DMA_TO_DEVICE);\n\nfree_rsp_cache:\n\tkmem_cache_destroy(ch->rsp_buf_cache);\n\nfree_ch:\n\tif (rdma_cm_id)\n\t\trdma_cm_id->context = NULL;\n\telse\n\t\tib_cm_id->context = NULL;\n\tkfree(ch);\n\tch = NULL;\n\n\tWARN_ON_ONCE(ret == 0);\n\nreject:\n\tpr_info(\"Rejecting login with reason %#x\\n\", be32_to_cpu(rej->reason));\n\trej->opcode = SRP_LOGIN_REJ;\n\trej->tag = req->tag;\n\trej->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\n\t\t\t\t   SRP_BUF_FORMAT_INDIRECT);\n\n\tif (rdma_cm_id)\n\t\trdma_reject(rdma_cm_id, rej, sizeof(*rej),\n\t\t\t    IB_CM_REJ_CONSUMER_DEFINED);\n\telse\n\t\tib_send_cm_rej(ib_cm_id, IB_CM_REJ_CONSUMER_DEFINED, NULL, 0,\n\t\t\t       rej, sizeof(*rej));\n\n\tif (ch && ch->sess) {\n\t\tsrpt_close_ch(ch);\n\t\t \n\t\tret = 0;\n\t}\n\nout:\n\tkfree(rep_param);\n\tkfree(rsp);\n\tkfree(rej);\n\n\treturn ret;\n}\n\nstatic int srpt_ib_cm_req_recv(struct ib_cm_id *cm_id,\n\t\t\t       const struct ib_cm_req_event_param *param,\n\t\t\t       void *private_data)\n{\n\tchar sguid[40];\n\n\tsrpt_format_guid(sguid, sizeof(sguid),\n\t\t\t &param->primary_path->dgid.global.interface_id);\n\n\treturn srpt_cm_req_recv(cm_id->context, cm_id, NULL, param->port,\n\t\t\t\tparam->primary_path->pkey,\n\t\t\t\tprivate_data, sguid);\n}\n\nstatic int srpt_rdma_cm_req_recv(struct rdma_cm_id *cm_id,\n\t\t\t\t struct rdma_cm_event *event)\n{\n\tstruct srpt_device *sdev;\n\tstruct srp_login_req req;\n\tconst struct srp_login_req_rdma *req_rdma;\n\tstruct sa_path_rec *path_rec = cm_id->route.path_rec;\n\tchar src_addr[40];\n\n\tsdev = ib_get_client_data(cm_id->device, &srpt_client);\n\tif (!sdev)\n\t\treturn -ECONNREFUSED;\n\n\tif (event->param.conn.private_data_len < sizeof(*req_rdma))\n\t\treturn -EINVAL;\n\n\t \n\treq_rdma = event->param.conn.private_data;\n\tmemset(&req, 0, sizeof(req));\n\treq.opcode\t\t= req_rdma->opcode;\n\treq.tag\t\t\t= req_rdma->tag;\n\treq.req_it_iu_len\t= req_rdma->req_it_iu_len;\n\treq.req_buf_fmt\t\t= req_rdma->req_buf_fmt;\n\treq.req_flags\t\t= req_rdma->req_flags;\n\tmemcpy(req.initiator_port_id, req_rdma->initiator_port_id, 16);\n\tmemcpy(req.target_port_id, req_rdma->target_port_id, 16);\n\treq.imm_data_offset\t= req_rdma->imm_data_offset;\n\n\tsnprintf(src_addr, sizeof(src_addr), \"%pIS\",\n\t\t &cm_id->route.addr.src_addr);\n\n\treturn srpt_cm_req_recv(sdev, NULL, cm_id, cm_id->port_num,\n\t\t\t\tpath_rec ? path_rec->pkey : 0, &req, src_addr);\n}\n\nstatic void srpt_cm_rej_recv(struct srpt_rdma_ch *ch,\n\t\t\t     enum ib_cm_rej_reason reason,\n\t\t\t     const u8 *private_data,\n\t\t\t     u8 private_data_len)\n{\n\tchar *priv = NULL;\n\tint i;\n\n\tif (private_data_len && (priv = kmalloc(private_data_len * 3 + 1,\n\t\t\t\t\t\tGFP_KERNEL))) {\n\t\tfor (i = 0; i < private_data_len; i++)\n\t\t\tsprintf(priv + 3 * i, \" %02x\", private_data[i]);\n\t}\n\tpr_info(\"Received CM REJ for ch %s-%d; reason %d%s%s.\\n\",\n\t\tch->sess_name, ch->qp->qp_num, reason, private_data_len ?\n\t\t\"; private data\" : \"\", priv ? priv : \" (?)\");\n\tkfree(priv);\n}\n\n \nstatic void srpt_cm_rtu_recv(struct srpt_rdma_ch *ch)\n{\n\tint ret;\n\n\tret = ch->using_rdma_cm ? 0 : srpt_ch_qp_rts(ch, ch->qp);\n\tif (ret < 0) {\n\t\tpr_err(\"%s-%d: QP transition to RTS failed\\n\", ch->sess_name,\n\t\t       ch->qp->qp_num);\n\t\tsrpt_close_ch(ch);\n\t\treturn;\n\t}\n\n\t \n\tif (!srpt_set_ch_state(ch, CH_LIVE)) {\n\t\tpr_err(\"%s-%d: channel transition to LIVE state failed\\n\",\n\t\t       ch->sess_name, ch->qp->qp_num);\n\t\treturn;\n\t}\n\n\t \n\tret = srpt_zerolength_write(ch);\n\tWARN_ONCE(ret < 0, \"%d\\n\", ret);\n}\n\n \nstatic int srpt_cm_handler(struct ib_cm_id *cm_id,\n\t\t\t   const struct ib_cm_event *event)\n{\n\tstruct srpt_rdma_ch *ch = cm_id->context;\n\tint ret;\n\n\tret = 0;\n\tswitch (event->event) {\n\tcase IB_CM_REQ_RECEIVED:\n\t\tret = srpt_ib_cm_req_recv(cm_id, &event->param.req_rcvd,\n\t\t\t\t\t  event->private_data);\n\t\tbreak;\n\tcase IB_CM_REJ_RECEIVED:\n\t\tsrpt_cm_rej_recv(ch, event->param.rej_rcvd.reason,\n\t\t\t\t event->private_data,\n\t\t\t\t IB_CM_REJ_PRIVATE_DATA_SIZE);\n\t\tbreak;\n\tcase IB_CM_RTU_RECEIVED:\n\tcase IB_CM_USER_ESTABLISHED:\n\t\tsrpt_cm_rtu_recv(ch);\n\t\tbreak;\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tsrpt_disconnect_ch(ch);\n\t\tbreak;\n\tcase IB_CM_DREP_RECEIVED:\n\t\tpr_info(\"Received CM DREP message for ch %s-%d.\\n\",\n\t\t\tch->sess_name, ch->qp->qp_num);\n\t\tsrpt_close_ch(ch);\n\t\tbreak;\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tpr_info(\"Received CM TimeWait exit for ch %s-%d.\\n\",\n\t\t\tch->sess_name, ch->qp->qp_num);\n\t\tsrpt_close_ch(ch);\n\t\tbreak;\n\tcase IB_CM_REP_ERROR:\n\t\tpr_info(\"Received CM REP error for ch %s-%d.\\n\", ch->sess_name,\n\t\t\tch->qp->qp_num);\n\t\tbreak;\n\tcase IB_CM_DREQ_ERROR:\n\t\tpr_info(\"Received CM DREQ ERROR event.\\n\");\n\t\tbreak;\n\tcase IB_CM_MRA_RECEIVED:\n\t\tpr_info(\"Received CM MRA event\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized CM event %d\\n\", event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int srpt_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\tstruct rdma_cm_event *event)\n{\n\tstruct srpt_rdma_ch *ch = cm_id->context;\n\tint ret = 0;\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_CONNECT_REQUEST:\n\t\tret = srpt_rdma_cm_req_recv(cm_id, event);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tsrpt_cm_rej_recv(ch, event->status,\n\t\t\t\t event->param.conn.private_data,\n\t\t\t\t event->param.conn.private_data_len);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tsrpt_cm_rtu_recv(ch);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\tif (ch->state < CH_DISCONNECTING)\n\t\t\tsrpt_disconnect_ch(ch);\n\t\telse\n\t\t\tsrpt_close_ch(ch);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\tsrpt_close_ch(ch);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\t\tpr_info(\"Received CM REP error for ch %s-%d.\\n\", ch->sess_name,\n\t\t\tch->qp->qp_num);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized RDMA CM event %d\\n\",\n\t\t       event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int srpt_write_pending(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(se_cmd, struct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tstruct ib_send_wr *first_wr = NULL;\n\tstruct ib_cqe *cqe = &ioctx->rdma_cqe;\n\tenum srpt_command_state new_state;\n\tint ret, i;\n\n\tif (ioctx->recv_ioctx) {\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DATA_IN);\n\t\ttarget_execute_cmd(&ioctx->cmd);\n\t\treturn 0;\n\t}\n\n\tnew_state = srpt_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA);\n\tWARN_ON(new_state == SRPT_STATE_DONE);\n\n\tif (atomic_sub_return(ioctx->n_rdma, &ch->sq_wr_avail) < 0) {\n\t\tpr_warn(\"%s: IB send queue full (needed %d)\\n\",\n\t\t\t\t__func__, ioctx->n_rdma);\n\t\tret = -ENOMEM;\n\t\tgoto out_undo;\n\t}\n\n\tcqe->done = srpt_rdma_read_done;\n\tfor (i = ioctx->n_rw_ctx - 1; i >= 0; i--) {\n\t\tstruct srpt_rw_ctx *ctx = &ioctx->rw_ctxs[i];\n\n\t\tfirst_wr = rdma_rw_ctx_wrs(&ctx->rw, ch->qp, ch->sport->port,\n\t\t\t\tcqe, first_wr);\n\t\tcqe = NULL;\n\t}\n\n\tret = ib_post_send(ch->qp, first_wr, NULL);\n\tif (ret) {\n\t\tpr_err(\"%s: ib_post_send() returned %d for %d (avail: %d)\\n\",\n\t\t\t __func__, ret, ioctx->n_rdma,\n\t\t\t atomic_read(&ch->sq_wr_avail));\n\t\tgoto out_undo;\n\t}\n\n\treturn 0;\nout_undo:\n\tatomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\n\treturn ret;\n}\n\nstatic u8 tcm_to_srp_tsk_mgmt_status(const int tcm_mgmt_status)\n{\n\tswitch (tcm_mgmt_status) {\n\tcase TMR_FUNCTION_COMPLETE:\n\t\treturn SRP_TSK_MGMT_SUCCESS;\n\tcase TMR_FUNCTION_REJECTED:\n\t\treturn SRP_TSK_MGMT_FUNC_NOT_SUPP;\n\t}\n\treturn SRP_TSK_MGMT_FAILED;\n}\n\n \nstatic void srpt_queue_response(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(cmd, struct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tstruct srpt_device *sdev = ch->sport->sdev;\n\tstruct ib_send_wr send_wr, *first_wr = &send_wr;\n\tstruct ib_sge sge;\n\tenum srpt_command_state state;\n\tint resp_len, ret, i;\n\tu8 srp_tm_status;\n\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\t\tioctx->state = SRPT_STATE_CMD_RSP_SENT;\n\t\tbreak;\n\tcase SRPT_STATE_MGMT:\n\t\tioctx->state = SRPT_STATE_MGMT_RSP_SENT;\n\t\tbreak;\n\tdefault:\n\t\tWARN(true, \"ch %p; cmd %d: unexpected command state %d\\n\",\n\t\t\tch, ioctx->ioctx.index, ioctx->state);\n\t\tbreak;\n\t}\n\n\tif (WARN_ON_ONCE(state == SRPT_STATE_CMD_RSP_SENT))\n\t\treturn;\n\n\t \n\tif (ioctx->cmd.data_direction == DMA_FROM_DEVICE &&\n\t    ioctx->cmd.data_length &&\n\t    !ioctx->queue_status_only) {\n\t\tfor (i = ioctx->n_rw_ctx - 1; i >= 0; i--) {\n\t\t\tstruct srpt_rw_ctx *ctx = &ioctx->rw_ctxs[i];\n\n\t\t\tfirst_wr = rdma_rw_ctx_wrs(&ctx->rw, ch->qp,\n\t\t\t\t\tch->sport->port, NULL, first_wr);\n\t\t}\n\t}\n\n\tif (state != SRPT_STATE_MGMT)\n\t\tresp_len = srpt_build_cmd_rsp(ch, ioctx, ioctx->cmd.tag,\n\t\t\t\t\t      cmd->scsi_status);\n\telse {\n\t\tsrp_tm_status\n\t\t\t= tcm_to_srp_tsk_mgmt_status(cmd->se_tmr_req->response);\n\t\tresp_len = srpt_build_tskmgmt_rsp(ch, ioctx, srp_tm_status,\n\t\t\t\t\t\t ioctx->cmd.tag);\n\t}\n\n\tatomic_inc(&ch->req_lim);\n\n\tif (unlikely(atomic_sub_return(1 + ioctx->n_rdma,\n\t\t\t&ch->sq_wr_avail) < 0)) {\n\t\tpr_warn(\"%s: IB send queue full (needed %d)\\n\",\n\t\t\t\t__func__, ioctx->n_rdma);\n\t\tgoto out;\n\t}\n\n\tib_dma_sync_single_for_device(sdev->device, ioctx->ioctx.dma, resp_len,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\tsge.addr = ioctx->ioctx.dma;\n\tsge.length = resp_len;\n\tsge.lkey = sdev->lkey;\n\n\tioctx->ioctx.cqe.done = srpt_send_done;\n\tsend_wr.next = NULL;\n\tsend_wr.wr_cqe = &ioctx->ioctx.cqe;\n\tsend_wr.sg_list = &sge;\n\tsend_wr.num_sge = 1;\n\tsend_wr.opcode = IB_WR_SEND;\n\tsend_wr.send_flags = IB_SEND_SIGNALED;\n\n\tret = ib_post_send(ch->qp, first_wr, NULL);\n\tif (ret < 0) {\n\t\tpr_err(\"%s: sending cmd response failed for tag %llu (%d)\\n\",\n\t\t\t__func__, ioctx->cmd.tag, ret);\n\t\tgoto out;\n\t}\n\n\treturn;\n\nout:\n\tatomic_add(1 + ioctx->n_rdma, &ch->sq_wr_avail);\n\tatomic_dec(&ch->req_lim);\n\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\ttarget_put_sess_cmd(&ioctx->cmd);\n}\n\nstatic int srpt_queue_data_in(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_queue_tm_rsp(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n}\n\n \nstatic void srpt_aborted_task(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\n\tatomic_inc(&ch->req_lim_delta);\n}\n\nstatic int srpt_queue_status(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\n\tBUG_ON(ioctx->sense_data != cmd->sense_buffer);\n\tif (cmd->se_cmd_flags &\n\t    (SCF_TRANSPORT_TASK_SENSE | SCF_EMULATED_TASK_SENSE))\n\t\tWARN_ON(cmd->scsi_status != SAM_STAT_CHECK_CONDITION);\n\tioctx->queue_status_only = true;\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_refresh_port_work(struct work_struct *work)\n{\n\tstruct srpt_port *sport = container_of(work, struct srpt_port, work);\n\n\tsrpt_refresh_port(sport);\n}\n\n \nstatic int srpt_release_sport(struct srpt_port *sport)\n{\n\tDECLARE_COMPLETION_ONSTACK(c);\n\tstruct srpt_nexus *nexus, *next_n;\n\tstruct srpt_rdma_ch *ch;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tsport->freed_channels = &c;\n\n\tmutex_lock(&sport->mutex);\n\tsrpt_set_enabled(sport, false);\n\tmutex_unlock(&sport->mutex);\n\n\twhile (atomic_read(&sport->refcount) > 0 &&\n\t       wait_for_completion_timeout(&c, 5 * HZ) <= 0) {\n\t\tpr_info(\"%s_%d: waiting for unregistration of %d sessions ...\\n\",\n\t\t\tdev_name(&sport->sdev->device->dev), sport->port,\n\t\t\tatomic_read(&sport->refcount));\n\t\trcu_read_lock();\n\t\tlist_for_each_entry(nexus, &sport->nexus_list, entry) {\n\t\t\tlist_for_each_entry(ch, &nexus->ch_list, list) {\n\t\t\t\tpr_info(\"%s-%d: state %s\\n\",\n\t\t\t\t\tch->sess_name, ch->qp->qp_num,\n\t\t\t\t\tget_ch_state_name(ch->state));\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tmutex_lock(&sport->mutex);\n\tlist_for_each_entry_safe(nexus, next_n, &sport->nexus_list, entry) {\n\t\tlist_del(&nexus->entry);\n\t\tkfree_rcu(nexus, rcu);\n\t}\n\tmutex_unlock(&sport->mutex);\n\n\treturn 0;\n}\n\nstruct port_and_port_id {\n\tstruct srpt_port *sport;\n\tstruct srpt_port_id **port_id;\n};\n\nstatic struct port_and_port_id __srpt_lookup_port(const char *name)\n{\n\tstruct ib_device *dev;\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tint i;\n\n\tlist_for_each_entry(sdev, &srpt_dev_list, list) {\n\t\tdev = sdev->device;\n\t\tif (!dev)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < dev->phys_port_cnt; i++) {\n\t\t\tsport = &sdev->port[i];\n\n\t\t\tif (strcmp(sport->guid_name, name) == 0) {\n\t\t\t\tkref_get(&sdev->refcnt);\n\t\t\t\treturn (struct port_and_port_id){\n\t\t\t\t\tsport, &sport->guid_id};\n\t\t\t}\n\t\t\tif (strcmp(sport->gid_name, name) == 0) {\n\t\t\t\tkref_get(&sdev->refcnt);\n\t\t\t\treturn (struct port_and_port_id){\n\t\t\t\t\tsport, &sport->gid_id};\n\t\t\t}\n\t\t}\n\t}\n\n\treturn (struct port_and_port_id){};\n}\n\n \nstatic struct port_and_port_id srpt_lookup_port(const char *name)\n{\n\tstruct port_and_port_id papi;\n\n\tspin_lock(&srpt_dev_lock);\n\tpapi = __srpt_lookup_port(name);\n\tspin_unlock(&srpt_dev_lock);\n\n\treturn papi;\n}\n\nstatic void srpt_free_srq(struct srpt_device *sdev)\n{\n\tif (!sdev->srq)\n\t\treturn;\n\n\tib_destroy_srq(sdev->srq);\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\n\t\t\t     sdev->srq_size, sdev->req_buf_cache,\n\t\t\t     DMA_FROM_DEVICE);\n\tkmem_cache_destroy(sdev->req_buf_cache);\n\tsdev->srq = NULL;\n}\n\nstatic int srpt_alloc_srq(struct srpt_device *sdev)\n{\n\tstruct ib_srq_init_attr srq_attr = {\n\t\t.event_handler = srpt_srq_event,\n\t\t.srq_context = (void *)sdev,\n\t\t.attr.max_wr = sdev->srq_size,\n\t\t.attr.max_sge = 1,\n\t\t.srq_type = IB_SRQT_BASIC,\n\t};\n\tstruct ib_device *device = sdev->device;\n\tstruct ib_srq *srq;\n\tint i;\n\n\tWARN_ON_ONCE(sdev->srq);\n\tsrq = ib_create_srq(sdev->pd, &srq_attr);\n\tif (IS_ERR(srq)) {\n\t\tpr_debug(\"ib_create_srq() failed: %ld\\n\", PTR_ERR(srq));\n\t\treturn PTR_ERR(srq);\n\t}\n\n\tpr_debug(\"create SRQ #wr= %d max_allow=%d dev= %s\\n\", sdev->srq_size,\n\t\t sdev->device->attrs.max_srq_wr, dev_name(&device->dev));\n\n\tsdev->req_buf_cache = kmem_cache_create(\"srpt-srq-req-buf\",\n\t\t\t\t\t\tsrp_max_req_size, 0, 0, NULL);\n\tif (!sdev->req_buf_cache)\n\t\tgoto free_srq;\n\n\tsdev->ioctx_ring = (struct srpt_recv_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(sdev, sdev->srq_size,\n\t\t\t\t      sizeof(*sdev->ioctx_ring[0]),\n\t\t\t\t      sdev->req_buf_cache, 0, DMA_FROM_DEVICE);\n\tif (!sdev->ioctx_ring)\n\t\tgoto free_cache;\n\n\tsdev->use_srq = true;\n\tsdev->srq = srq;\n\n\tfor (i = 0; i < sdev->srq_size; ++i) {\n\t\tINIT_LIST_HEAD(&sdev->ioctx_ring[i]->wait_list);\n\t\tsrpt_post_recv(sdev, NULL, sdev->ioctx_ring[i]);\n\t}\n\n\treturn 0;\n\nfree_cache:\n\tkmem_cache_destroy(sdev->req_buf_cache);\n\nfree_srq:\n\tib_destroy_srq(srq);\n\treturn -ENOMEM;\n}\n\nstatic int srpt_use_srq(struct srpt_device *sdev, bool use_srq)\n{\n\tstruct ib_device *device = sdev->device;\n\tint ret = 0;\n\n\tif (!use_srq) {\n\t\tsrpt_free_srq(sdev);\n\t\tsdev->use_srq = false;\n\t} else if (use_srq && !sdev->srq) {\n\t\tret = srpt_alloc_srq(sdev);\n\t}\n\tpr_debug(\"%s(%s): use_srq = %d; ret = %d\\n\", __func__,\n\t\t dev_name(&device->dev), sdev->use_srq, ret);\n\treturn ret;\n}\n\nstatic void srpt_free_sdev(struct kref *refcnt)\n{\n\tstruct srpt_device *sdev = container_of(refcnt, typeof(*sdev), refcnt);\n\n\tkfree(sdev);\n}\n\nstatic void srpt_sdev_put(struct srpt_device *sdev)\n{\n\tkref_put(&sdev->refcnt, srpt_free_sdev);\n}\n\n \nstatic int srpt_add_one(struct ib_device *device)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tint ret;\n\tu32 i;\n\n\tpr_debug(\"device = %p\\n\", device);\n\n\tsdev = kzalloc(struct_size(sdev, port, device->phys_port_cnt),\n\t\t       GFP_KERNEL);\n\tif (!sdev)\n\t\treturn -ENOMEM;\n\n\tkref_init(&sdev->refcnt);\n\tsdev->device = device;\n\tmutex_init(&sdev->sdev_mutex);\n\n\tsdev->pd = ib_alloc_pd(device, 0);\n\tif (IS_ERR(sdev->pd)) {\n\t\tret = PTR_ERR(sdev->pd);\n\t\tgoto free_dev;\n\t}\n\n\tsdev->lkey = sdev->pd->local_dma_lkey;\n\n\tsdev->srq_size = min(srpt_srq_size, sdev->device->attrs.max_srq_wr);\n\n\tsrpt_use_srq(sdev, sdev->port[0].port_attrib.use_srq);\n\n\tif (!srpt_service_guid)\n\t\tsrpt_service_guid = be64_to_cpu(device->node_guid);\n\n\tif (rdma_port_get_link_layer(device, 1) == IB_LINK_LAYER_INFINIBAND)\n\t\tsdev->cm_id = ib_create_cm_id(device, srpt_cm_handler, sdev);\n\tif (IS_ERR(sdev->cm_id)) {\n\t\tpr_info(\"ib_create_cm_id() failed: %ld\\n\",\n\t\t\tPTR_ERR(sdev->cm_id));\n\t\tret = PTR_ERR(sdev->cm_id);\n\t\tsdev->cm_id = NULL;\n\t\tif (!rdma_cm_id)\n\t\t\tgoto err_ring;\n\t}\n\n\t \n\tpr_debug(\"Target login info: id_ext=%016llx,ioc_guid=%016llx,pkey=ffff,service_id=%016llx\\n\",\n\t\t srpt_service_guid, srpt_service_guid, srpt_service_guid);\n\n\t \n\tret = sdev->cm_id ?\n\t\tib_cm_listen(sdev->cm_id, cpu_to_be64(srpt_service_guid)) :\n\t\t0;\n\tif (ret < 0) {\n\t\tpr_err(\"ib_cm_listen() failed: %d (cm_id state = %d)\\n\", ret,\n\t\t       sdev->cm_id->state);\n\t\tgoto err_cm;\n\t}\n\n\tINIT_IB_EVENT_HANDLER(&sdev->event_handler, sdev->device,\n\t\t\t      srpt_event_handler);\n\tib_register_event_handler(&sdev->event_handler);\n\n\tfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tINIT_LIST_HEAD(&sport->nexus_list);\n\t\tmutex_init(&sport->mutex);\n\t\tsport->sdev = sdev;\n\t\tsport->port = i;\n\t\tsport->port_attrib.srp_max_rdma_size = DEFAULT_MAX_RDMA_SIZE;\n\t\tsport->port_attrib.srp_max_rsp_size = DEFAULT_MAX_RSP_SIZE;\n\t\tsport->port_attrib.srp_sq_size = DEF_SRPT_SQ_SIZE;\n\t\tsport->port_attrib.use_srq = false;\n\t\tINIT_WORK(&sport->work, srpt_refresh_port_work);\n\n\t\tret = srpt_refresh_port(sport);\n\t\tif (ret) {\n\t\t\tpr_err(\"MAD registration failed for %s-%d.\\n\",\n\t\t\t       dev_name(&sdev->device->dev), i);\n\t\t\ti--;\n\t\t\tgoto err_port;\n\t\t}\n\t}\n\n\tspin_lock(&srpt_dev_lock);\n\tlist_add_tail(&sdev->list, &srpt_dev_list);\n\tspin_unlock(&srpt_dev_lock);\n\n\tib_set_client_data(device, &srpt_client, sdev);\n\tpr_debug(\"added %s.\\n\", dev_name(&device->dev));\n\treturn 0;\n\nerr_port:\n\tsrpt_unregister_mad_agent(sdev, i);\n\tib_unregister_event_handler(&sdev->event_handler);\nerr_cm:\n\tif (sdev->cm_id)\n\t\tib_destroy_cm_id(sdev->cm_id);\nerr_ring:\n\tsrpt_free_srq(sdev);\n\tib_dealloc_pd(sdev->pd);\nfree_dev:\n\tsrpt_sdev_put(sdev);\n\tpr_info(\"%s(%s) failed.\\n\", __func__, dev_name(&device->dev));\n\treturn ret;\n}\n\n \nstatic void srpt_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct srpt_device *sdev = client_data;\n\tint i;\n\n\tsrpt_unregister_mad_agent(sdev, sdev->device->phys_port_cnt);\n\n\tib_unregister_event_handler(&sdev->event_handler);\n\n\t \n\tfor (i = 0; i < sdev->device->phys_port_cnt; i++)\n\t\tcancel_work_sync(&sdev->port[i].work);\n\n\tif (sdev->cm_id)\n\t\tib_destroy_cm_id(sdev->cm_id);\n\n\tib_set_client_data(device, &srpt_client, NULL);\n\n\t \n\tspin_lock(&srpt_dev_lock);\n\tlist_del(&sdev->list);\n\tspin_unlock(&srpt_dev_lock);\n\n\tfor (i = 0; i < sdev->device->phys_port_cnt; i++)\n\t\tsrpt_release_sport(&sdev->port[i]);\n\n\tsrpt_free_srq(sdev);\n\n\tib_dealloc_pd(sdev->pd);\n\n\tsrpt_sdev_put(sdev);\n}\n\nstatic struct ib_client srpt_client = {\n\t.name = DRV_NAME,\n\t.add = srpt_add_one,\n\t.remove = srpt_remove_one\n};\n\nstatic int srpt_check_true(struct se_portal_group *se_tpg)\n{\n\treturn 1;\n}\n\nstatic struct srpt_port *srpt_tpg_to_sport(struct se_portal_group *tpg)\n{\n\treturn tpg->se_tpg_wwn->priv;\n}\n\nstatic struct srpt_port_id *srpt_wwn_to_sport_id(struct se_wwn *wwn)\n{\n\tstruct srpt_port *sport = wwn->priv;\n\n\tif (sport->guid_id && &sport->guid_id->wwn == wwn)\n\t\treturn sport->guid_id;\n\tif (sport->gid_id && &sport->gid_id->wwn == wwn)\n\t\treturn sport->gid_id;\n\tWARN_ON_ONCE(true);\n\treturn NULL;\n}\n\nstatic char *srpt_get_fabric_wwn(struct se_portal_group *tpg)\n{\n\tstruct srpt_tpg *stpg = container_of(tpg, typeof(*stpg), tpg);\n\n\treturn stpg->sport_id->name;\n}\n\nstatic u16 srpt_get_tag(struct se_portal_group *tpg)\n{\n\treturn 1;\n}\n\nstatic void srpt_release_cmd(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(se_cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tstruct srpt_recv_ioctx *recv_ioctx = ioctx->recv_ioctx;\n\n\tWARN_ON_ONCE(ioctx->state != SRPT_STATE_DONE &&\n\t\t     !(ioctx->cmd.transport_state & CMD_T_ABORTED));\n\n\tif (recv_ioctx) {\n\t\tWARN_ON_ONCE(!list_empty(&recv_ioctx->wait_list));\n\t\tioctx->recv_ioctx = NULL;\n\t\tsrpt_post_recv(ch->sport->sdev, ch, recv_ioctx);\n\t}\n\n\tif (ioctx->n_rw_ctx) {\n\t\tsrpt_free_rw_ctxs(ch, ioctx);\n\t\tioctx->n_rw_ctx = 0;\n\t}\n\n\ttarget_free_tag(se_cmd->se_sess, se_cmd);\n}\n\n \nstatic void srpt_close_session(struct se_session *se_sess)\n{\n\tstruct srpt_rdma_ch *ch = se_sess->fabric_sess_ptr;\n\n\tsrpt_disconnect_ch_sync(ch);\n}\n\n \nstatic int srpt_get_tcm_cmd_state(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\treturn ioctx->state;\n}\n\nstatic int srpt_parse_guid(u64 *guid, const char *name)\n{\n\tu16 w[4];\n\tint ret = -EINVAL;\n\n\tif (sscanf(name, \"%hx:%hx:%hx:%hx\", &w[0], &w[1], &w[2], &w[3]) != 4)\n\t\tgoto out;\n\t*guid = get_unaligned_be64(w);\n\tret = 0;\nout:\n\treturn ret;\n}\n\n \nstatic int srpt_parse_i_port_id(u8 i_port_id[16], const char *name)\n{\n\tconst char *p;\n\tunsigned len, count, leading_zero_bytes;\n\tint ret;\n\n\tp = name;\n\tif (strncasecmp(p, \"0x\", 2) == 0)\n\t\tp += 2;\n\tret = -EINVAL;\n\tlen = strlen(p);\n\tif (len % 2)\n\t\tgoto out;\n\tcount = min(len / 2, 16U);\n\tleading_zero_bytes = 16 - count;\n\tmemset(i_port_id, 0, leading_zero_bytes);\n\tret = hex2bin(i_port_id + leading_zero_bytes, p, count);\n\nout:\n\treturn ret;\n}\n\n \nstatic int srpt_init_nodeacl(struct se_node_acl *se_nacl, const char *name)\n{\n\tstruct sockaddr_storage sa;\n\tu64 guid;\n\tu8 i_port_id[16];\n\tint ret;\n\n\tret = srpt_parse_guid(&guid, name);\n\tif (ret < 0)\n\t\tret = srpt_parse_i_port_id(i_port_id, name);\n\tif (ret < 0)\n\t\tret = inet_pton_with_scope(&init_net, AF_UNSPEC, name, NULL,\n\t\t\t\t\t   &sa);\n\tif (ret < 0)\n\t\tpr_err(\"invalid initiator port ID %s\\n\", name);\n\treturn ret;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\n\treturn sysfs_emit(page, \"%u\\n\", sport->port_attrib.srp_max_rdma_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RDMA_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < DEFAULT_MAX_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu smaller than DEFAULT_MAX_RDMA_SIZE: %d\\n\",\n\t\t\tval, DEFAULT_MAX_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rdma_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\n\treturn sysfs_emit(page, \"%u\\n\", sport->port_attrib.srp_max_rsp_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RSP_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RSP_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_MAX_RSP_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_MAX_RSP_SIZE: %d\\n\", val,\n\t\t\tMIN_MAX_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rsp_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\n\treturn sysfs_emit(page, \"%u\\n\", sport->port_attrib.srp_sq_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMIN_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_sq_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_use_srq_show(struct config_item *item,\n\t\t\t\t\t    char *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\n\treturn sysfs_emit(page, \"%d\\n\", sport->port_attrib.use_srq);\n}\n\nstatic ssize_t srpt_tpg_attrib_use_srq_store(struct config_item *item,\n\t\t\t\t\t     const char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\tstruct srpt_device *sdev = sport->sdev;\n\tunsigned long val;\n\tbool enabled;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (val != !!val)\n\t\treturn -EINVAL;\n\n\tret = mutex_lock_interruptible(&sdev->sdev_mutex);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = mutex_lock_interruptible(&sport->mutex);\n\tif (ret < 0)\n\t\tgoto unlock_sdev;\n\tenabled = sport->enabled;\n\t \n\tsrpt_set_enabled(sport, false);\n\tsport->port_attrib.use_srq = val;\n\tsrpt_use_srq(sdev, sport->port_attrib.use_srq);\n\tsrpt_set_enabled(sport, enabled);\n\tret = count;\n\tmutex_unlock(&sport->mutex);\nunlock_sdev:\n\tmutex_unlock(&sdev->sdev_mutex);\n\n\treturn ret;\n}\n\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rdma_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rsp_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_sq_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  use_srq);\n\nstatic struct configfs_attribute *srpt_tpg_attrib_attrs[] = {\n\t&srpt_tpg_attrib_attr_srp_max_rdma_size,\n\t&srpt_tpg_attrib_attr_srp_max_rsp_size,\n\t&srpt_tpg_attrib_attr_srp_sq_size,\n\t&srpt_tpg_attrib_attr_use_srq,\n\tNULL,\n};\n\nstatic struct rdma_cm_id *srpt_create_rdma_id(struct sockaddr *listen_addr)\n{\n\tstruct rdma_cm_id *rdma_cm_id;\n\tint ret;\n\n\trdma_cm_id = rdma_create_id(&init_net, srpt_rdma_cm_handler,\n\t\t\t\t    NULL, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(rdma_cm_id)) {\n\t\tpr_err(\"RDMA/CM ID creation failed: %ld\\n\",\n\t\t       PTR_ERR(rdma_cm_id));\n\t\tgoto out;\n\t}\n\n\tret = rdma_bind_addr(rdma_cm_id, listen_addr);\n\tif (ret) {\n\t\tchar addr_str[64];\n\n\t\tsnprintf(addr_str, sizeof(addr_str), \"%pISp\", listen_addr);\n\t\tpr_err(\"Binding RDMA/CM ID to address %s failed: %d\\n\",\n\t\t       addr_str, ret);\n\t\trdma_destroy_id(rdma_cm_id);\n\t\trdma_cm_id = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tret = rdma_listen(rdma_cm_id, 128);\n\tif (ret) {\n\t\tpr_err(\"rdma_listen() failed: %d\\n\", ret);\n\t\trdma_destroy_id(rdma_cm_id);\n\t\trdma_cm_id = ERR_PTR(ret);\n\t}\n\nout:\n\treturn rdma_cm_id;\n}\n\nstatic ssize_t srpt_rdma_cm_port_show(struct config_item *item, char *page)\n{\n\treturn sysfs_emit(page, \"%d\\n\", rdma_cm_port);\n}\n\nstatic ssize_t srpt_rdma_cm_port_store(struct config_item *item,\n\t\t\t\t       const char *page, size_t count)\n{\n\tstruct sockaddr_in  addr4 = { .sin_family  = AF_INET  };\n\tstruct sockaddr_in6 addr6 = { .sin6_family = AF_INET6 };\n\tstruct rdma_cm_id *new_id = NULL;\n\tu16 val;\n\tint ret;\n\n\tret = kstrtou16(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = count;\n\tif (rdma_cm_port == val)\n\t\tgoto out;\n\n\tif (val) {\n\t\taddr6.sin6_port = cpu_to_be16(val);\n\t\tnew_id = srpt_create_rdma_id((struct sockaddr *)&addr6);\n\t\tif (IS_ERR(new_id)) {\n\t\t\taddr4.sin_port = cpu_to_be16(val);\n\t\t\tnew_id = srpt_create_rdma_id((struct sockaddr *)&addr4);\n\t\t\tif (IS_ERR(new_id)) {\n\t\t\t\tret = PTR_ERR(new_id);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tmutex_lock(&rdma_cm_mutex);\n\trdma_cm_port = val;\n\tswap(rdma_cm_id, new_id);\n\tmutex_unlock(&rdma_cm_mutex);\n\n\tif (new_id)\n\t\trdma_destroy_id(new_id);\n\tret = count;\nout:\n\treturn ret;\n}\n\nCONFIGFS_ATTR(srpt_, rdma_cm_port);\n\nstatic struct configfs_attribute *srpt_da_attrs[] = {\n\t&srpt_attr_rdma_cm_port,\n\tNULL,\n};\n\nstatic int srpt_enable_tpg(struct se_portal_group *se_tpg, bool enable)\n{\n\tstruct srpt_port *sport = srpt_tpg_to_sport(se_tpg);\n\n\tmutex_lock(&sport->mutex);\n\tsrpt_set_enabled(sport, enable);\n\tmutex_unlock(&sport->mutex);\n\n\treturn 0;\n}\n\n \nstatic struct se_portal_group *srpt_make_tpg(struct se_wwn *wwn,\n\t\t\t\t\t     const char *name)\n{\n\tstruct srpt_port_id *sport_id = srpt_wwn_to_sport_id(wwn);\n\tstruct srpt_tpg *stpg;\n\tint res = -ENOMEM;\n\n\tstpg = kzalloc(sizeof(*stpg), GFP_KERNEL);\n\tif (!stpg)\n\t\treturn ERR_PTR(res);\n\tstpg->sport_id = sport_id;\n\tres = core_tpg_register(wwn, &stpg->tpg, SCSI_PROTOCOL_SRP);\n\tif (res) {\n\t\tkfree(stpg);\n\t\treturn ERR_PTR(res);\n\t}\n\n\tmutex_lock(&sport_id->mutex);\n\tlist_add_tail(&stpg->entry, &sport_id->tpg_list);\n\tmutex_unlock(&sport_id->mutex);\n\n\treturn &stpg->tpg;\n}\n\n \nstatic void srpt_drop_tpg(struct se_portal_group *tpg)\n{\n\tstruct srpt_tpg *stpg = container_of(tpg, typeof(*stpg), tpg);\n\tstruct srpt_port_id *sport_id = stpg->sport_id;\n\tstruct srpt_port *sport = srpt_tpg_to_sport(tpg);\n\n\tmutex_lock(&sport_id->mutex);\n\tlist_del(&stpg->entry);\n\tmutex_unlock(&sport_id->mutex);\n\n\tsport->enabled = false;\n\tcore_tpg_deregister(tpg);\n\tkfree(stpg);\n}\n\n \nstatic struct se_wwn *srpt_make_tport(struct target_fabric_configfs *tf,\n\t\t\t\t      struct config_group *group,\n\t\t\t\t      const char *name)\n{\n\tstruct port_and_port_id papi = srpt_lookup_port(name);\n\tstruct srpt_port *sport = papi.sport;\n\tstruct srpt_port_id *port_id;\n\n\tif (!papi.port_id)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (*papi.port_id) {\n\t\t \n\t\tWARN_ON_ONCE(true);\n\t\treturn &(*papi.port_id)->wwn;\n\t}\n\tport_id = kzalloc(sizeof(*port_id), GFP_KERNEL);\n\tif (!port_id) {\n\t\tsrpt_sdev_put(sport->sdev);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tmutex_init(&port_id->mutex);\n\tINIT_LIST_HEAD(&port_id->tpg_list);\n\tport_id->wwn.priv = sport;\n\tmemcpy(port_id->name, port_id == sport->guid_id ? sport->guid_name :\n\t       sport->gid_name, ARRAY_SIZE(port_id->name));\n\n\t*papi.port_id = port_id;\n\n\treturn &port_id->wwn;\n}\n\n \nstatic void srpt_drop_tport(struct se_wwn *wwn)\n{\n\tstruct srpt_port_id *port_id = container_of(wwn, typeof(*port_id), wwn);\n\tstruct srpt_port *sport = wwn->priv;\n\n\tif (sport->guid_id == port_id)\n\t\tsport->guid_id = NULL;\n\telse if (sport->gid_id == port_id)\n\t\tsport->gid_id = NULL;\n\telse\n\t\tWARN_ON_ONCE(true);\n\n\tsrpt_sdev_put(sport->sdev);\n\tkfree(port_id);\n}\n\nstatic ssize_t srpt_wwn_version_show(struct config_item *item, char *buf)\n{\n\treturn sysfs_emit(buf, \"\\n\");\n}\n\nCONFIGFS_ATTR_RO(srpt_wwn_, version);\n\nstatic struct configfs_attribute *srpt_wwn_attrs[] = {\n\t&srpt_wwn_attr_version,\n\tNULL,\n};\n\nstatic const struct target_core_fabric_ops srpt_template = {\n\t.module\t\t\t\t= THIS_MODULE,\n\t.fabric_name\t\t\t= \"srpt\",\n\t.tpg_get_wwn\t\t\t= srpt_get_fabric_wwn,\n\t.tpg_get_tag\t\t\t= srpt_get_tag,\n\t.tpg_check_demo_mode_cache\t= srpt_check_true,\n\t.tpg_check_demo_mode_write_protect = srpt_check_true,\n\t.release_cmd\t\t\t= srpt_release_cmd,\n\t.check_stop_free\t\t= srpt_check_stop_free,\n\t.close_session\t\t\t= srpt_close_session,\n\t.sess_get_initiator_sid\t\t= NULL,\n\t.write_pending\t\t\t= srpt_write_pending,\n\t.get_cmd_state\t\t\t= srpt_get_tcm_cmd_state,\n\t.queue_data_in\t\t\t= srpt_queue_data_in,\n\t.queue_status\t\t\t= srpt_queue_status,\n\t.queue_tm_rsp\t\t\t= srpt_queue_tm_rsp,\n\t.aborted_task\t\t\t= srpt_aborted_task,\n\t \n\t.fabric_make_wwn\t\t= srpt_make_tport,\n\t.fabric_drop_wwn\t\t= srpt_drop_tport,\n\t.fabric_make_tpg\t\t= srpt_make_tpg,\n\t.fabric_enable_tpg\t\t= srpt_enable_tpg,\n\t.fabric_drop_tpg\t\t= srpt_drop_tpg,\n\t.fabric_init_nodeacl\t\t= srpt_init_nodeacl,\n\n\t.tfc_discovery_attrs\t\t= srpt_da_attrs,\n\t.tfc_wwn_attrs\t\t\t= srpt_wwn_attrs,\n\t.tfc_tpg_attrib_attrs\t\t= srpt_tpg_attrib_attrs,\n};\n\n \nstatic int __init srpt_init_module(void)\n{\n\tint ret;\n\n\tret = -EINVAL;\n\tif (srp_max_req_size < MIN_MAX_REQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter srp_max_req_size -- must be at least %d.\\n\",\n\t\t       srp_max_req_size, MIN_MAX_REQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tif (srpt_srq_size < MIN_SRPT_SRQ_SIZE\n\t    || srpt_srq_size > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter srpt_srq_size -- must be in the range [%d..%d].\\n\",\n\t\t       srpt_srq_size, MIN_SRPT_SRQ_SIZE, MAX_SRPT_SRQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tret = target_register_template(&srpt_template);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ib_register_client(&srpt_client);\n\tif (ret) {\n\t\tpr_err(\"couldn't register IB client\\n\");\n\t\tgoto out_unregister_target;\n\t}\n\n\treturn 0;\n\nout_unregister_target:\n\ttarget_unregister_template(&srpt_template);\nout:\n\treturn ret;\n}\n\nstatic void __exit srpt_cleanup_module(void)\n{\n\tif (rdma_cm_id)\n\t\trdma_destroy_id(rdma_cm_id);\n\tib_unregister_client(&srpt_client);\n\ttarget_unregister_template(&srpt_template);\n}\n\nmodule_init(srpt_init_module);\nmodule_exit(srpt_cleanup_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}