{
  "module_name": "ipoib_ib.c",
  "hash_id": "d2e1850536611f2efd2fc796f154155fd699ab4144e9169464c51f3595d6f035",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/ipoib/ipoib_ib.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n#include <linux/moduleparam.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <rdma/ib_cache.h>\n\n#include \"ipoib.h\"\n\n#ifdef CONFIG_INFINIBAND_IPOIB_DEBUG_DATA\nstatic int data_debug_level;\n\nmodule_param(data_debug_level, int, 0644);\nMODULE_PARM_DESC(data_debug_level,\n\t\t \"Enable data path debug tracing if > 0\");\n#endif\n\nstruct ipoib_ah *ipoib_create_ah(struct net_device *dev,\n\t\t\t\t struct ib_pd *pd, struct rdma_ah_attr *attr)\n{\n\tstruct ipoib_ah *ah;\n\tstruct ib_ah *vah;\n\n\tah = kmalloc(sizeof(*ah), GFP_KERNEL);\n\tif (!ah)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tah->dev       = dev;\n\tah->last_send = 0;\n\tkref_init(&ah->ref);\n\n\tvah = rdma_create_ah(pd, attr, RDMA_CREATE_AH_SLEEPABLE);\n\tif (IS_ERR(vah)) {\n\t\tkfree(ah);\n\t\tah = (struct ipoib_ah *)vah;\n\t} else {\n\t\tah->ah = vah;\n\t\tipoib_dbg(ipoib_priv(dev), \"Created ah %p\\n\", ah->ah);\n\t}\n\n\treturn ah;\n}\n\nvoid ipoib_free_ah(struct kref *kref)\n{\n\tstruct ipoib_ah *ah = container_of(kref, struct ipoib_ah, ref);\n\tstruct ipoib_dev_priv *priv = ipoib_priv(ah->dev);\n\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\tlist_add_tail(&ah->list, &priv->dead_ahs);\n\tspin_unlock_irqrestore(&priv->lock, flags);\n}\n\nstatic void ipoib_ud_dma_unmap_rx(struct ipoib_dev_priv *priv,\n\t\t\t\t  u64 mapping[IPOIB_UD_RX_SG])\n{\n\tib_dma_unmap_single(priv->ca, mapping[0],\n\t\t\t    IPOIB_UD_BUF_SIZE(priv->max_ib_mtu),\n\t\t\t    DMA_FROM_DEVICE);\n}\n\nstatic int ipoib_ib_post_receive(struct net_device *dev, int id)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint ret;\n\n\tpriv->rx_wr.wr_id   = id | IPOIB_OP_RECV;\n\tpriv->rx_sge[0].addr = priv->rx_ring[id].mapping[0];\n\tpriv->rx_sge[1].addr = priv->rx_ring[id].mapping[1];\n\n\n\tret = ib_post_recv(priv->qp, &priv->rx_wr, NULL);\n\tif (unlikely(ret)) {\n\t\tipoib_warn(priv, \"receive failed for buf %d (%d)\\n\", id, ret);\n\t\tipoib_ud_dma_unmap_rx(priv, priv->rx_ring[id].mapping);\n\t\tdev_kfree_skb_any(priv->rx_ring[id].skb);\n\t\tpriv->rx_ring[id].skb = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic struct sk_buff *ipoib_alloc_rx_skb(struct net_device *dev, int id)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct sk_buff *skb;\n\tint buf_size;\n\tu64 *mapping;\n\n\tbuf_size = IPOIB_UD_BUF_SIZE(priv->max_ib_mtu);\n\n\tskb = dev_alloc_skb(buf_size + IPOIB_HARD_LEN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, sizeof(struct ipoib_pseudo_header));\n\n\tmapping = priv->rx_ring[id].mapping;\n\tmapping[0] = ib_dma_map_single(priv->ca, skb->data, buf_size,\n\t\t\t\t       DMA_FROM_DEVICE);\n\tif (unlikely(ib_dma_mapping_error(priv->ca, mapping[0])))\n\t\tgoto error;\n\n\tpriv->rx_ring[id].skb = skb;\n\treturn skb;\nerror:\n\tdev_kfree_skb_any(skb);\n\treturn NULL;\n}\n\nstatic int ipoib_ib_post_receives(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < ipoib_recvq_size; ++i) {\n\t\tif (!ipoib_alloc_rx_skb(dev, i)) {\n\t\t\tipoib_warn(priv, \"failed to allocate receive buffer %d\\n\", i);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (ipoib_ib_post_receive(dev, i)) {\n\t\t\tipoib_warn(priv, \"ipoib_ib_post_receive failed for buf %d\\n\", i);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void ipoib_ib_handle_rx_wc(struct net_device *dev, struct ib_wc *wc)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tunsigned int wr_id = wc->wr_id & ~IPOIB_OP_RECV;\n\tstruct sk_buff *skb;\n\tu64 mapping[IPOIB_UD_RX_SG];\n\tunion ib_gid *dgid;\n\tunion ib_gid *sgid;\n\n\tipoib_dbg_data(priv, \"recv completion: id %d, status: %d\\n\",\n\t\t       wr_id, wc->status);\n\n\tif (unlikely(wr_id >= ipoib_recvq_size)) {\n\t\tipoib_warn(priv, \"recv completion event with wrid %d (> %d)\\n\",\n\t\t\t   wr_id, ipoib_recvq_size);\n\t\treturn;\n\t}\n\n\tskb  = priv->rx_ring[wr_id].skb;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\t\tipoib_warn(priv,\n\t\t\t\t   \"failed recv event (status=%d, wrid=%d vend_err %#x)\\n\",\n\t\t\t\t   wc->status, wr_id, wc->vendor_err);\n\t\tipoib_ud_dma_unmap_rx(priv, priv->rx_ring[wr_id].mapping);\n\t\tdev_kfree_skb_any(skb);\n\t\tpriv->rx_ring[wr_id].skb = NULL;\n\t\treturn;\n\t}\n\n\tmemcpy(mapping, priv->rx_ring[wr_id].mapping,\n\t       IPOIB_UD_RX_SG * sizeof(*mapping));\n\n\t \n\tif (unlikely(!ipoib_alloc_rx_skb(dev, wr_id))) {\n\t\t++dev->stats.rx_dropped;\n\t\tgoto repost;\n\t}\n\n\tipoib_dbg_data(priv, \"received %d bytes, SLID 0x%04x\\n\",\n\t\t       wc->byte_len, wc->slid);\n\n\tipoib_ud_dma_unmap_rx(priv, mapping);\n\n\tskb_put(skb, wc->byte_len);\n\n\t \n\tdgid = &((struct ib_grh *)skb->data)->dgid;\n\n\tif (!(wc->wc_flags & IB_WC_GRH) || dgid->raw[0] != 0xff)\n\t\tskb->pkt_type = PACKET_HOST;\n\telse if (memcmp(dgid, dev->broadcast + 4, sizeof(union ib_gid)) == 0)\n\t\tskb->pkt_type = PACKET_BROADCAST;\n\telse\n\t\tskb->pkt_type = PACKET_MULTICAST;\n\n\tsgid = &((struct ib_grh *)skb->data)->sgid;\n\n\t \n\tif (wc->slid == priv->local_lid && wc->src_qp == priv->qp->qp_num) {\n\t\tint need_repost = 1;\n\n\t\tif ((wc->wc_flags & IB_WC_GRH) &&\n\t\t    sgid->global.interface_id != priv->local_gid.global.interface_id)\n\t\t\tneed_repost = 0;\n\n\t\tif (need_repost) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto repost;\n\t\t}\n\t}\n\n\tskb_pull(skb, IB_GRH_BYTES);\n\n\tskb->protocol = ((struct ipoib_header *) skb->data)->proto;\n\tskb_add_pseudo_hdr(skb);\n\n\t++dev->stats.rx_packets;\n\tdev->stats.rx_bytes += skb->len;\n\tif (skb->pkt_type == PACKET_MULTICAST)\n\t\tdev->stats.multicast++;\n\n\tskb->dev = dev;\n\tif ((dev->features & NETIF_F_RXCSUM) &&\n\t\t\tlikely(wc->wc_flags & IB_WC_IP_CSUM_OK))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tnapi_gro_receive(&priv->recv_napi, skb);\n\nrepost:\n\tif (unlikely(ipoib_ib_post_receive(dev, wr_id)))\n\t\tipoib_warn(priv, \"ipoib_ib_post_receive failed \"\n\t\t\t   \"for buf %d\\n\", wr_id);\n}\n\nint ipoib_dma_map_tx(struct ib_device *ca, struct ipoib_tx_buf *tx_req)\n{\n\tstruct sk_buff *skb = tx_req->skb;\n\tu64 *mapping = tx_req->mapping;\n\tint i;\n\tint off;\n\n\tif (skb_headlen(skb)) {\n\t\tmapping[0] = ib_dma_map_single(ca, skb->data, skb_headlen(skb),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (unlikely(ib_dma_mapping_error(ca, mapping[0])))\n\t\t\treturn -EIO;\n\n\t\toff = 1;\n\t} else\n\t\toff = 0;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; ++i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tmapping[i + off] = ib_dma_map_page(ca,\n\t\t\t\t\t\t skb_frag_page(frag),\n\t\t\t\t\t\t skb_frag_off(frag),\n\t\t\t\t\t\t skb_frag_size(frag),\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (unlikely(ib_dma_mapping_error(ca, mapping[i + off])))\n\t\t\tgoto partial_error;\n\t}\n\treturn 0;\n\npartial_error:\n\tfor (; i > 0; --i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\tib_dma_unmap_page(ca, mapping[i - !off], skb_frag_size(frag), DMA_TO_DEVICE);\n\t}\n\n\tif (off)\n\t\tib_dma_unmap_single(ca, mapping[0], skb_headlen(skb), DMA_TO_DEVICE);\n\n\treturn -EIO;\n}\n\nvoid ipoib_dma_unmap_tx(struct ipoib_dev_priv *priv,\n\t\t\tstruct ipoib_tx_buf *tx_req)\n{\n\tstruct sk_buff *skb = tx_req->skb;\n\tu64 *mapping = tx_req->mapping;\n\tint i;\n\tint off;\n\n\tif (skb_headlen(skb)) {\n\t\tib_dma_unmap_single(priv->ca, mapping[0], skb_headlen(skb),\n\t\t\t\t    DMA_TO_DEVICE);\n\t\toff = 1;\n\t} else\n\t\toff = 0;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; ++i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tib_dma_unmap_page(priv->ca, mapping[i + off],\n\t\t\t\t  skb_frag_size(frag), DMA_TO_DEVICE);\n\t}\n}\n\n \nstatic void ipoib_qp_state_validate_work(struct work_struct *work)\n{\n\tstruct ipoib_qp_state_validate *qp_work =\n\t\tcontainer_of(work, struct ipoib_qp_state_validate, work);\n\n\tstruct ipoib_dev_priv *priv = qp_work->priv;\n\tstruct ib_qp_attr qp_attr;\n\tstruct ib_qp_init_attr query_init_attr;\n\tint ret;\n\n\tret = ib_query_qp(priv->qp, &qp_attr, IB_QP_STATE, &query_init_attr);\n\tif (ret) {\n\t\tipoib_warn(priv, \"%s: Failed to query QP ret: %d\\n\",\n\t\t\t   __func__, ret);\n\t\tgoto free_res;\n\t}\n\tpr_info(\"%s: QP: 0x%x is in state: %d\\n\",\n\t\t__func__, priv->qp->qp_num, qp_attr.qp_state);\n\n\t \n\tif (qp_attr.qp_state == IB_QPS_SQE) {\n\t\tqp_attr.qp_state = IB_QPS_RTS;\n\n\t\tret = ib_modify_qp(priv->qp, &qp_attr, IB_QP_STATE);\n\t\tif (ret) {\n\t\t\tpr_warn(\"failed(%d) modify QP:0x%x SQE->RTS\\n\",\n\t\t\t\tret, priv->qp->qp_num);\n\t\t\tgoto free_res;\n\t\t}\n\t\tpr_info(\"%s: QP: 0x%x moved from IB_QPS_SQE to IB_QPS_RTS\\n\",\n\t\t\t__func__, priv->qp->qp_num);\n\t} else {\n\t\tpr_warn(\"QP (%d) will stay in state: %d\\n\",\n\t\t\tpriv->qp->qp_num, qp_attr.qp_state);\n\t}\n\nfree_res:\n\tkfree(qp_work);\n}\n\nstatic void ipoib_ib_handle_tx_wc(struct net_device *dev, struct ib_wc *wc)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tunsigned int wr_id = wc->wr_id;\n\tstruct ipoib_tx_buf *tx_req;\n\n\tipoib_dbg_data(priv, \"send completion: id %d, status: %d\\n\",\n\t\t       wr_id, wc->status);\n\n\tif (unlikely(wr_id >= ipoib_sendq_size)) {\n\t\tipoib_warn(priv, \"send completion event with wrid %d (> %d)\\n\",\n\t\t\t   wr_id, ipoib_sendq_size);\n\t\treturn;\n\t}\n\n\ttx_req = &priv->tx_ring[wr_id];\n\n\tipoib_dma_unmap_tx(priv, tx_req);\n\n\t++dev->stats.tx_packets;\n\tdev->stats.tx_bytes += tx_req->skb->len;\n\n\tdev_kfree_skb_any(tx_req->skb);\n\n\t++priv->tx_tail;\n\t++priv->global_tx_tail;\n\n\tif (unlikely(netif_queue_stopped(dev) &&\n\t\t     ((priv->global_tx_head - priv->global_tx_tail) <=\n\t\t      ipoib_sendq_size >> 1) &&\n\t\t     test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags)))\n\t\tnetif_wake_queue(dev);\n\n\tif (wc->status != IB_WC_SUCCESS &&\n\t    wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\tstruct ipoib_qp_state_validate *qp_work;\n\t\tipoib_warn(priv,\n\t\t\t   \"failed send event (status=%d, wrid=%d vend_err %#x)\\n\",\n\t\t\t   wc->status, wr_id, wc->vendor_err);\n\t\tqp_work = kzalloc(sizeof(*qp_work), GFP_ATOMIC);\n\t\tif (!qp_work)\n\t\t\treturn;\n\n\t\tINIT_WORK(&qp_work->work, ipoib_qp_state_validate_work);\n\t\tqp_work->priv = priv;\n\t\tqueue_work(priv->wq, &qp_work->work);\n\t}\n}\n\nstatic int poll_tx(struct ipoib_dev_priv *priv)\n{\n\tint n, i;\n\tstruct ib_wc *wc;\n\n\tn = ib_poll_cq(priv->send_cq, MAX_SEND_CQE, priv->send_wc);\n\tfor (i = 0; i < n; ++i) {\n\t\twc = priv->send_wc + i;\n\t\tif (wc->wr_id & IPOIB_OP_CM)\n\t\t\tipoib_cm_handle_tx_wc(priv->dev, priv->send_wc + i);\n\t\telse\n\t\t\tipoib_ib_handle_tx_wc(priv->dev, priv->send_wc + i);\n\t}\n\treturn n == MAX_SEND_CQE;\n}\n\nint ipoib_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ipoib_dev_priv *priv =\n\t\tcontainer_of(napi, struct ipoib_dev_priv, recv_napi);\n\tstruct net_device *dev = priv->dev;\n\tint done;\n\tint t;\n\tint n, i;\n\n\tdone  = 0;\n\npoll_more:\n\twhile (done < budget) {\n\t\tint max = (budget - done);\n\n\t\tt = min(IPOIB_NUM_WC, max);\n\t\tn = ib_poll_cq(priv->recv_cq, t, priv->ibwc);\n\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tstruct ib_wc *wc = priv->ibwc + i;\n\n\t\t\tif (wc->wr_id & IPOIB_OP_RECV) {\n\t\t\t\t++done;\n\t\t\t\tif (wc->wr_id & IPOIB_OP_CM)\n\t\t\t\t\tipoib_cm_handle_rx_wc(dev, wc);\n\t\t\t\telse\n\t\t\t\t\tipoib_ib_handle_rx_wc(dev, wc);\n\t\t\t} else {\n\t\t\t\tpr_warn(\"%s: Got unexpected wqe id\\n\", __func__);\n\t\t\t}\n\t\t}\n\n\t\tif (n != t)\n\t\t\tbreak;\n\t}\n\n\tif (done < budget) {\n\t\tnapi_complete(napi);\n\t\tif (unlikely(ib_req_notify_cq(priv->recv_cq,\n\t\t\t\t\t      IB_CQ_NEXT_COMP |\n\t\t\t\t\t      IB_CQ_REPORT_MISSED_EVENTS)) &&\n\t\t    napi_reschedule(napi))\n\t\t\tgoto poll_more;\n\t}\n\n\treturn done;\n}\n\nint ipoib_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ipoib_dev_priv *priv = container_of(napi, struct ipoib_dev_priv,\n\t\t\t\t\t\t   send_napi);\n\tstruct net_device *dev = priv->dev;\n\tint n, i;\n\tstruct ib_wc *wc;\n\npoll_more:\n\tn = ib_poll_cq(priv->send_cq, MAX_SEND_CQE, priv->send_wc);\n\n\tfor (i = 0; i < n; i++) {\n\t\twc = priv->send_wc + i;\n\t\tif (wc->wr_id & IPOIB_OP_CM)\n\t\t\tipoib_cm_handle_tx_wc(dev, wc);\n\t\telse\n\t\t\tipoib_ib_handle_tx_wc(dev, wc);\n\t}\n\n\tif (n < budget) {\n\t\tnapi_complete(napi);\n\t\tif (unlikely(ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP |\n\t\t\t\t\t      IB_CQ_REPORT_MISSED_EVENTS)) &&\n\t\t    napi_reschedule(napi))\n\t\t\tgoto poll_more;\n\t}\n\treturn n < 0 ? 0 : n;\n}\n\nvoid ipoib_ib_rx_completion(struct ib_cq *cq, void *ctx_ptr)\n{\n\tstruct ipoib_dev_priv *priv = ctx_ptr;\n\n\tnapi_schedule(&priv->recv_napi);\n}\n\nvoid ipoib_ib_tx_completion(struct ib_cq *cq, void *ctx_ptr)\n{\n\tstruct ipoib_dev_priv *priv = ctx_ptr;\n\n\tnapi_schedule(&priv->send_napi);\n}\n\nstatic inline int post_send(struct ipoib_dev_priv *priv,\n\t\t\t    unsigned int wr_id,\n\t\t\t    struct ib_ah *address, u32 dqpn,\n\t\t\t    struct ipoib_tx_buf *tx_req,\n\t\t\t    void *head, int hlen)\n{\n\tstruct sk_buff *skb = tx_req->skb;\n\n\tipoib_build_sge(priv, tx_req);\n\n\tpriv->tx_wr.wr.wr_id\t= wr_id;\n\tpriv->tx_wr.remote_qpn\t= dqpn;\n\tpriv->tx_wr.ah\t\t= address;\n\n\tif (head) {\n\t\tpriv->tx_wr.mss\t\t= skb_shinfo(skb)->gso_size;\n\t\tpriv->tx_wr.header\t= head;\n\t\tpriv->tx_wr.hlen\t= hlen;\n\t\tpriv->tx_wr.wr.opcode\t= IB_WR_LSO;\n\t} else\n\t\tpriv->tx_wr.wr.opcode\t= IB_WR_SEND;\n\n\treturn ib_post_send(priv->qp, &priv->tx_wr.wr, NULL);\n}\n\nint ipoib_send(struct net_device *dev, struct sk_buff *skb,\n\t       struct ib_ah *address, u32 dqpn)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_tx_buf *tx_req;\n\tint hlen, rc;\n\tvoid *phead;\n\tunsigned int usable_sge = priv->max_send_sge - !!skb_headlen(skb);\n\n\tif (skb_is_gso(skb)) {\n\t\thlen = skb_tcp_all_headers(skb);\n\t\tphead = skb->data;\n\t\tif (unlikely(!skb_pull(skb, hlen))) {\n\t\t\tipoib_warn(priv, \"linear data too small\\n\");\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -1;\n\t\t}\n\t} else {\n\t\tif (unlikely(skb->len > priv->mcast_mtu + IPOIB_ENCAP_LEN)) {\n\t\t\tipoib_warn(priv, \"packet len %d (> %d) too long to send, dropping\\n\",\n\t\t\t\t   skb->len, priv->mcast_mtu + IPOIB_ENCAP_LEN);\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tipoib_cm_skb_too_long(dev, skb, priv->mcast_mtu);\n\t\t\treturn -1;\n\t\t}\n\t\tphead = NULL;\n\t\thlen  = 0;\n\t}\n\tif (skb_shinfo(skb)->nr_frags > usable_sge) {\n\t\tif (skb_linearize(skb) < 0) {\n\t\t\tipoib_warn(priv, \"skb could not be linearized\\n\");\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -1;\n\t\t}\n\t\t \n\t\tif (skb_shinfo(skb)->nr_frags > usable_sge) {\n\t\t\tipoib_warn(priv, \"too many frags after skb linearize\\n\");\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tipoib_dbg_data(priv,\n\t\t       \"sending packet, length=%d address=%p dqpn=0x%06x\\n\",\n\t\t       skb->len, address, dqpn);\n\n\t \n\ttx_req = &priv->tx_ring[priv->tx_head & (ipoib_sendq_size - 1)];\n\ttx_req->skb = skb;\n\tif (unlikely(ipoib_dma_map_tx(priv->ca, tx_req))) {\n\t\t++dev->stats.tx_errors;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -1;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tpriv->tx_wr.wr.send_flags |= IB_SEND_IP_CSUM;\n\telse\n\t\tpriv->tx_wr.wr.send_flags &= ~IB_SEND_IP_CSUM;\n\t \n\tif ((priv->global_tx_head - priv->global_tx_tail) ==\n\t    ipoib_sendq_size - 1) {\n\t\tipoib_dbg(priv, \"TX ring full, stopping kernel net queue\\n\");\n\t\tnetif_stop_queue(dev);\n\t}\n\n\tskb_orphan(skb);\n\tskb_dst_drop(skb);\n\n\tif (netif_queue_stopped(dev))\n\t\tif (ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP |\n\t\t\t\t     IB_CQ_REPORT_MISSED_EVENTS) < 0)\n\t\t\tipoib_warn(priv, \"request notify on send CQ failed\\n\");\n\n\trc = post_send(priv, priv->tx_head & (ipoib_sendq_size - 1),\n\t\t       address, dqpn, tx_req, phead, hlen);\n\tif (unlikely(rc)) {\n\t\tipoib_warn(priv, \"post_send failed, error %d\\n\", rc);\n\t\t++dev->stats.tx_errors;\n\t\tipoib_dma_unmap_tx(priv, tx_req);\n\t\tdev_kfree_skb_any(skb);\n\t\tif (netif_queue_stopped(dev))\n\t\t\tnetif_wake_queue(dev);\n\t\trc = 0;\n\t} else {\n\t\tnetif_trans_update(dev);\n\n\t\trc = priv->tx_head;\n\t\t++priv->tx_head;\n\t\t++priv->global_tx_head;\n\t}\n\treturn rc;\n}\n\nstatic void ipoib_reap_dead_ahs(struct ipoib_dev_priv *priv)\n{\n\tstruct ipoib_ah *ah, *tah;\n\tunsigned long flags;\n\n\tnetif_tx_lock_bh(priv->dev);\n\tspin_lock_irqsave(&priv->lock, flags);\n\n\tlist_for_each_entry_safe(ah, tah, &priv->dead_ahs, list)\n\t\tif ((int) priv->tx_tail - (int) ah->last_send >= 0) {\n\t\t\tlist_del(&ah->list);\n\t\t\trdma_destroy_ah(ah->ah, 0);\n\t\t\tkfree(ah);\n\t\t}\n\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\tnetif_tx_unlock_bh(priv->dev);\n}\n\nvoid ipoib_reap_ah(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv =\n\t\tcontainer_of(work, struct ipoib_dev_priv, ah_reap_task.work);\n\n\tipoib_reap_dead_ahs(priv);\n\n\tif (!test_bit(IPOIB_STOP_REAPER, &priv->flags))\n\t\tqueue_delayed_work(priv->wq, &priv->ah_reap_task,\n\t\t\t\t   round_jiffies_relative(HZ));\n}\n\nstatic void ipoib_start_ah_reaper(struct ipoib_dev_priv *priv)\n{\n\tclear_bit(IPOIB_STOP_REAPER, &priv->flags);\n\tqueue_delayed_work(priv->wq, &priv->ah_reap_task,\n\t\t\t   round_jiffies_relative(HZ));\n}\n\nstatic void ipoib_stop_ah_reaper(struct ipoib_dev_priv *priv)\n{\n\tset_bit(IPOIB_STOP_REAPER, &priv->flags);\n\tcancel_delayed_work(&priv->ah_reap_task);\n\t \n}\n\nstatic int recvs_pending(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint pending = 0;\n\tint i;\n\n\tfor (i = 0; i < ipoib_recvq_size; ++i)\n\t\tif (priv->rx_ring[i].skb)\n\t\t\t++pending;\n\n\treturn pending;\n}\n\nstatic void check_qp_movement_and_print(struct ipoib_dev_priv *priv,\n\t\t\t\t\tstruct ib_qp *qp,\n\t\t\t\t\tenum ib_qp_state new_state)\n{\n\tstruct ib_qp_attr qp_attr;\n\tstruct ib_qp_init_attr query_init_attr;\n\tint ret;\n\n\tret = ib_query_qp(qp, &qp_attr, IB_QP_STATE, &query_init_attr);\n\tif (ret) {\n\t\tipoib_warn(priv, \"%s: Failed to query QP\\n\", __func__);\n\t\treturn;\n\t}\n\t \n\tif (new_state == IB_QPS_ERR && qp_attr.qp_state == IB_QPS_RESET)\n\t\tipoib_dbg(priv, \"Failed modify QP, IB_QPS_RESET to IB_QPS_ERR, acceptable\\n\");\n\telse\n\t\tipoib_warn(priv, \"Failed to modify QP to state: %d from state: %d\\n\",\n\t\t\t   new_state, qp_attr.qp_state);\n}\n\nstatic void ipoib_napi_enable(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tnapi_enable(&priv->recv_napi);\n\tnapi_enable(&priv->send_napi);\n}\n\nstatic void ipoib_napi_disable(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tnapi_disable(&priv->recv_napi);\n\tnapi_disable(&priv->send_napi);\n}\n\nint ipoib_ib_dev_stop_default(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_qp_attr qp_attr;\n\tunsigned long begin;\n\tstruct ipoib_tx_buf *tx_req;\n\tint i;\n\n\tif (test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))\n\t\tipoib_napi_disable(dev);\n\n\tipoib_cm_dev_stop(dev);\n\n\t \n\tqp_attr.qp_state = IB_QPS_ERR;\n\tif (ib_modify_qp(priv->qp, &qp_attr, IB_QP_STATE))\n\t\tcheck_qp_movement_and_print(priv, priv->qp, IB_QPS_ERR);\n\n\t \n\tbegin = jiffies;\n\n\twhile (priv->tx_head != priv->tx_tail || recvs_pending(dev)) {\n\t\tif (time_after(jiffies, begin + 5 * HZ)) {\n\t\t\tipoib_warn(priv,\n\t\t\t\t   \"timing out; %d sends %d receives not completed\\n\",\n\t\t\t\t   priv->tx_head - priv->tx_tail,\n\t\t\t\t   recvs_pending(dev));\n\n\t\t\t \n\t\t\twhile ((int)priv->tx_tail - (int)priv->tx_head < 0) {\n\t\t\t\ttx_req = &priv->tx_ring[priv->tx_tail &\n\t\t\t\t\t\t\t(ipoib_sendq_size - 1)];\n\t\t\t\tipoib_dma_unmap_tx(priv, tx_req);\n\t\t\t\tdev_kfree_skb_any(tx_req->skb);\n\t\t\t\t++priv->tx_tail;\n\t\t\t\t++priv->global_tx_tail;\n\t\t\t}\n\n\t\t\tfor (i = 0; i < ipoib_recvq_size; ++i) {\n\t\t\t\tstruct ipoib_rx_buf *rx_req;\n\n\t\t\t\trx_req = &priv->rx_ring[i];\n\t\t\t\tif (!rx_req->skb)\n\t\t\t\t\tcontinue;\n\t\t\t\tipoib_ud_dma_unmap_rx(priv,\n\t\t\t\t\t\t      priv->rx_ring[i].mapping);\n\t\t\t\tdev_kfree_skb_any(rx_req->skb);\n\t\t\t\trx_req->skb = NULL;\n\t\t\t}\n\n\t\t\tgoto timeout;\n\t\t}\n\n\t\tipoib_drain_cq(dev);\n\n\t\tusleep_range(1000, 2000);\n\t}\n\n\tipoib_dbg(priv, \"All sends and receives done.\\n\");\n\ntimeout:\n\tqp_attr.qp_state = IB_QPS_RESET;\n\tif (ib_modify_qp(priv->qp, &qp_attr, IB_QP_STATE))\n\t\tipoib_warn(priv, \"Failed to modify QP to RESET state\\n\");\n\n\tib_req_notify_cq(priv->recv_cq, IB_CQ_NEXT_COMP);\n\n\treturn 0;\n}\n\nint ipoib_ib_dev_open_default(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint ret;\n\n\tret = ipoib_init_qp(dev);\n\tif (ret) {\n\t\tipoib_warn(priv, \"ipoib_init_qp returned %d\\n\", ret);\n\t\treturn -1;\n\t}\n\n\tret = ipoib_ib_post_receives(dev);\n\tif (ret) {\n\t\tipoib_warn(priv, \"ipoib_ib_post_receives returned %d\\n\", ret);\n\t\tgoto out;\n\t}\n\n\tret = ipoib_cm_dev_open(dev);\n\tif (ret) {\n\t\tipoib_warn(priv, \"ipoib_cm_dev_open returned %d\\n\", ret);\n\t\tgoto out;\n\t}\n\n\tif (!test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))\n\t\tipoib_napi_enable(dev);\n\n\treturn 0;\nout:\n\treturn -1;\n}\n\nint ipoib_ib_dev_open(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tipoib_pkey_dev_check_presence(dev);\n\n\tif (!test_bit(IPOIB_PKEY_ASSIGNED, &priv->flags)) {\n\t\tipoib_warn(priv, \"P_Key 0x%04x is %s\\n\", priv->pkey,\n\t\t\t   (!(priv->pkey & 0x7fff) ? \"Invalid\" : \"not found\"));\n\t\treturn -1;\n\t}\n\n\tipoib_start_ah_reaper(priv);\n\tif (priv->rn_ops->ndo_open(dev)) {\n\t\tpr_warn(\"%s: Failed to open dev\\n\", dev->name);\n\t\tgoto dev_stop;\n\t}\n\n\tset_bit(IPOIB_FLAG_INITIALIZED, &priv->flags);\n\n\treturn 0;\n\ndev_stop:\n\tipoib_stop_ah_reaper(priv);\n\treturn -1;\n}\n\nvoid ipoib_ib_dev_stop(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tpriv->rn_ops->ndo_stop(dev);\n\n\tclear_bit(IPOIB_FLAG_INITIALIZED, &priv->flags);\n\tipoib_stop_ah_reaper(priv);\n}\n\nvoid ipoib_pkey_dev_check_presence(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct rdma_netdev *rn = netdev_priv(dev);\n\n\tif (!(priv->pkey & 0x7fff) ||\n\t    ib_find_pkey(priv->ca, priv->port, priv->pkey,\n\t\t\t &priv->pkey_index)) {\n\t\tclear_bit(IPOIB_PKEY_ASSIGNED, &priv->flags);\n\t} else {\n\t\tif (rn->set_id)\n\t\t\trn->set_id(dev, priv->pkey_index);\n\t\tset_bit(IPOIB_PKEY_ASSIGNED, &priv->flags);\n\t}\n}\n\nvoid ipoib_ib_dev_up(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tipoib_pkey_dev_check_presence(dev);\n\n\tif (!test_bit(IPOIB_PKEY_ASSIGNED, &priv->flags)) {\n\t\tipoib_dbg(priv, \"PKEY is not assigned.\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(IPOIB_FLAG_OPER_UP, &priv->flags);\n\n\tipoib_mcast_start_thread(dev);\n}\n\nvoid ipoib_ib_dev_down(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tipoib_dbg(priv, \"downing ib_dev\\n\");\n\n\tclear_bit(IPOIB_FLAG_OPER_UP, &priv->flags);\n\tnetif_carrier_off(dev);\n\n\tipoib_mcast_stop_thread(dev);\n\tipoib_mcast_dev_flush(dev);\n\n\tipoib_flush_paths(dev);\n}\n\nvoid ipoib_drain_cq(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i, n;\n\n\t \n\tlocal_bh_disable();\n\n\tdo {\n\t\tn = ib_poll_cq(priv->recv_cq, IPOIB_NUM_WC, priv->ibwc);\n\t\tfor (i = 0; i < n; ++i) {\n\t\t\t \n\t\t\tif (priv->ibwc[i].status == IB_WC_SUCCESS)\n\t\t\t\tpriv->ibwc[i].status = IB_WC_WR_FLUSH_ERR;\n\n\t\t\tif (priv->ibwc[i].wr_id & IPOIB_OP_RECV) {\n\t\t\t\tif (priv->ibwc[i].wr_id & IPOIB_OP_CM)\n\t\t\t\t\tipoib_cm_handle_rx_wc(dev, priv->ibwc + i);\n\t\t\t\telse\n\t\t\t\t\tipoib_ib_handle_rx_wc(dev, priv->ibwc + i);\n\t\t\t} else {\n\t\t\t\tpr_warn(\"%s: Got unexpected wqe id\\n\", __func__);\n\t\t\t}\n\t\t}\n\t} while (n == IPOIB_NUM_WC);\n\n\twhile (poll_tx(priv))\n\t\t;  \n\n\tlocal_bh_enable();\n}\n\n \nstatic inline int update_parent_pkey(struct ipoib_dev_priv *priv)\n{\n\tint result;\n\tu16 prev_pkey;\n\n\tprev_pkey = priv->pkey;\n\tresult = ib_query_pkey(priv->ca, priv->port, 0, &priv->pkey);\n\tif (result) {\n\t\tipoib_warn(priv, \"ib_query_pkey port %d failed (ret = %d)\\n\",\n\t\t\t   priv->port, result);\n\t\treturn result;\n\t}\n\n\tpriv->pkey |= 0x8000;\n\n\tif (prev_pkey != priv->pkey) {\n\t\tipoib_dbg(priv, \"pkey changed from 0x%x to 0x%x\\n\",\n\t\t\t  prev_pkey, priv->pkey);\n\t\t \n\t\tpriv->dev->broadcast[8] = priv->pkey >> 8;\n\t\tpriv->dev->broadcast[9] = priv->pkey & 0xff;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n \nstatic inline int update_child_pkey(struct ipoib_dev_priv *priv)\n{\n\tu16 old_index = priv->pkey_index;\n\n\tpriv->pkey_index = 0;\n\tipoib_pkey_dev_check_presence(priv->dev);\n\n\tif (test_bit(IPOIB_PKEY_ASSIGNED, &priv->flags) &&\n\t    (old_index == priv->pkey_index))\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic bool ipoib_dev_addr_changed_valid(struct ipoib_dev_priv *priv)\n{\n\tunion ib_gid search_gid;\n\tunion ib_gid gid0;\n\tint err;\n\tu16 index;\n\tu32 port;\n\tbool ret = false;\n\n\tif (rdma_query_gid(priv->ca, priv->port, 0, &gid0))\n\t\treturn false;\n\n\tnetif_addr_lock_bh(priv->dev);\n\n\t \n\tpriv->local_gid.global.subnet_prefix = gid0.global.subnet_prefix;\n\tdev_addr_mod(priv->dev, 4, (u8 *)&gid0.global.subnet_prefix,\n\t\t     sizeof(gid0.global.subnet_prefix));\n\tsearch_gid.global.subnet_prefix = gid0.global.subnet_prefix;\n\n\tsearch_gid.global.interface_id = priv->local_gid.global.interface_id;\n\n\tnetif_addr_unlock_bh(priv->dev);\n\n\terr = ib_find_gid(priv->ca, &search_gid, &port, &index);\n\n\tnetif_addr_lock_bh(priv->dev);\n\n\tif (search_gid.global.interface_id !=\n\t    priv->local_gid.global.interface_id)\n\t\t \n\t\tgoto out;\n\n\t \n\tif (!test_bit(IPOIB_FLAG_DEV_ADDR_SET, &priv->flags)) {\n\t\tif (!err && port == priv->port) {\n\t\t\tset_bit(IPOIB_FLAG_DEV_ADDR_SET, &priv->flags);\n\t\t\tif (index == 0)\n\t\t\t\tclear_bit(IPOIB_FLAG_DEV_ADDR_CTRL,\n\t\t\t\t\t  &priv->flags);\n\t\t\telse\n\t\t\t\tset_bit(IPOIB_FLAG_DEV_ADDR_CTRL, &priv->flags);\n\t\t\tret = true;\n\t\t} else {\n\t\t\tret = false;\n\t\t}\n\t} else {\n\t\tif (!err && port == priv->port) {\n\t\t\tret = true;\n\t\t} else {\n\t\t\tif (!test_bit(IPOIB_FLAG_DEV_ADDR_CTRL, &priv->flags)) {\n\t\t\t\tmemcpy(&priv->local_gid, &gid0,\n\t\t\t\t       sizeof(priv->local_gid));\n\t\t\t\tdev_addr_mod(priv->dev, 4, (u8 *)&gid0,\n\t\t\t\t\t     sizeof(priv->local_gid));\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\tnetif_addr_unlock_bh(priv->dev);\n\n\treturn ret;\n}\n\nstatic void __ipoib_ib_dev_flush(struct ipoib_dev_priv *priv,\n\t\t\t\tenum ipoib_flush_level level,\n\t\t\t\tint nesting)\n{\n\tstruct ipoib_dev_priv *cpriv;\n\tstruct net_device *dev = priv->dev;\n\tint result;\n\n\tdown_read_nested(&priv->vlan_rwsem, nesting);\n\n\t \n\tlist_for_each_entry(cpriv, &priv->child_intfs, list)\n\t\t__ipoib_ib_dev_flush(cpriv, level, nesting + 1);\n\n\tup_read(&priv->vlan_rwsem);\n\n\tif (!test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags) &&\n\t    level != IPOIB_FLUSH_HEAVY) {\n\t\t \n\t\tif (level == IPOIB_FLUSH_LIGHT)\n\t\t\tipoib_dev_addr_changed_valid(priv);\n\t\tipoib_dbg(priv, \"Not flushing - IPOIB_FLAG_INITIALIZED not set.\\n\");\n\t\treturn;\n\t}\n\n\tif (!test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags)) {\n\t\t \n\t\tif (level == IPOIB_FLUSH_HEAVY) {\n\t\t\tif (!test_bit(IPOIB_FLAG_SUBINTERFACE, &priv->flags))\n\t\t\t\tupdate_parent_pkey(priv);\n\t\t\telse\n\t\t\t\tupdate_child_pkey(priv);\n\t\t} else if (level == IPOIB_FLUSH_LIGHT)\n\t\t\tipoib_dev_addr_changed_valid(priv);\n\t\tipoib_dbg(priv, \"Not flushing - IPOIB_FLAG_ADMIN_UP not set.\\n\");\n\t\treturn;\n\t}\n\n\tif (level == IPOIB_FLUSH_HEAVY) {\n\t\t \n\t\tif (test_bit(IPOIB_FLAG_SUBINTERFACE, &priv->flags)) {\n\t\t\tresult = update_child_pkey(priv);\n\t\t\tif (result) {\n\t\t\t\t \n\t\t\t\tipoib_dbg(priv, \"Not flushing - P_Key index not changed.\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t} else {\n\t\t\tresult = update_parent_pkey(priv);\n\t\t\t \n\t\t\tif (result) {\n\t\t\t\tipoib_dbg(priv, \"Not flushing - P_Key value not changed.\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (level == IPOIB_FLUSH_LIGHT) {\n\t\tint oper_up;\n\t\tipoib_mark_paths_invalid(dev);\n\t\t \n\t\toper_up = test_and_clear_bit(IPOIB_FLAG_OPER_UP, &priv->flags);\n\t\tipoib_mcast_dev_flush(dev);\n\t\tif (oper_up)\n\t\t\tset_bit(IPOIB_FLAG_OPER_UP, &priv->flags);\n\t\tipoib_reap_dead_ahs(priv);\n\t}\n\n\tif (level >= IPOIB_FLUSH_NORMAL)\n\t\tipoib_ib_dev_down(dev);\n\n\tif (level == IPOIB_FLUSH_HEAVY) {\n\t\tif (test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))\n\t\t\tipoib_ib_dev_stop(dev);\n\n\t\tif (ipoib_ib_dev_open(dev))\n\t\t\treturn;\n\n\t\tif (netif_queue_stopped(dev))\n\t\t\tnetif_start_queue(dev);\n\t}\n\n\t \n\tif (test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags)) {\n\t\tif (level >= IPOIB_FLUSH_NORMAL)\n\t\t\tipoib_ib_dev_up(dev);\n\t\tif (ipoib_dev_addr_changed_valid(priv))\n\t\t\tipoib_mcast_restart_task(&priv->restart_task);\n\t}\n}\n\nvoid ipoib_ib_dev_flush_light(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv =\n\t\tcontainer_of(work, struct ipoib_dev_priv, flush_light);\n\n\t__ipoib_ib_dev_flush(priv, IPOIB_FLUSH_LIGHT, 0);\n}\n\nvoid ipoib_ib_dev_flush_normal(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv =\n\t\tcontainer_of(work, struct ipoib_dev_priv, flush_normal);\n\n\t__ipoib_ib_dev_flush(priv, IPOIB_FLUSH_NORMAL, 0);\n}\n\nvoid ipoib_ib_dev_flush_heavy(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv =\n\t\tcontainer_of(work, struct ipoib_dev_priv, flush_heavy);\n\n\trtnl_lock();\n\t__ipoib_ib_dev_flush(priv, IPOIB_FLUSH_HEAVY, 0);\n\trtnl_unlock();\n}\n\nvoid ipoib_ib_dev_cleanup(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tipoib_dbg(priv, \"cleaning up ib_dev\\n\");\n\t \n\tipoib_flush_paths(dev);\n\n\tipoib_mcast_stop_thread(dev);\n\tipoib_mcast_dev_flush(dev);\n\n\t \n\tipoib_reap_dead_ahs(priv);\n\n\tclear_bit(IPOIB_PKEY_ASSIGNED, &priv->flags);\n\n\tpriv->rn_ops->ndo_uninit(dev);\n\n\tif (priv->pd) {\n\t\tib_dealloc_pd(priv->pd);\n\t\tpriv->pd = NULL;\n\t}\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}