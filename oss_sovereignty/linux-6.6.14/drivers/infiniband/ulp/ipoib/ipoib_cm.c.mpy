{
  "module_name": "ipoib_cm.c",
  "hash_id": "96704723a266952df33b48f03ad88d49b82bf54b4aa2d5604f9123b3c0211410",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/ipoib/ipoib_cm.c",
  "human_readable_source": " \n\n#include <rdma/ib_cm.h>\n#include <net/dst.h>\n#include <net/icmp.h>\n#include <linux/icmpv6.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/moduleparam.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n\n#include \"ipoib.h\"\n\nint ipoib_max_conn_qp = 128;\n\nmodule_param_named(max_nonsrq_conn_qp, ipoib_max_conn_qp, int, 0444);\nMODULE_PARM_DESC(max_nonsrq_conn_qp,\n\t\t \"Max number of connected-mode QPs per interface \"\n\t\t \"(applied only if shared receive queue is not available)\");\n\n#ifdef CONFIG_INFINIBAND_IPOIB_DEBUG_DATA\nstatic int data_debug_level;\n\nmodule_param_named(cm_data_debug_level, data_debug_level, int, 0644);\nMODULE_PARM_DESC(cm_data_debug_level,\n\t\t \"Enable data path debug tracing for connected mode if > 0\");\n#endif\n\n#define IPOIB_CM_IETF_ID 0x1000000000000000ULL\n\n#define IPOIB_CM_RX_UPDATE_TIME (256 * HZ)\n#define IPOIB_CM_RX_TIMEOUT     (2 * 256 * HZ)\n#define IPOIB_CM_RX_DELAY       (3 * 256 * HZ)\n#define IPOIB_CM_RX_UPDATE_MASK (0x3)\n\n#define IPOIB_CM_RX_RESERVE     (ALIGN(IPOIB_HARD_LEN, 16) - IPOIB_ENCAP_LEN)\n\nstatic struct ib_qp_attr ipoib_cm_err_attr = {\n\t.qp_state = IB_QPS_ERR\n};\n\n#define IPOIB_CM_RX_DRAIN_WRID 0xffffffff\n\nstatic struct ib_send_wr ipoib_cm_rx_drain_wr = {\n\t.opcode = IB_WR_SEND,\n};\n\nstatic int ipoib_cm_tx_handler(struct ib_cm_id *cm_id,\n\t\t\t       const struct ib_cm_event *event);\n\nstatic void ipoib_cm_dma_unmap_rx(struct ipoib_dev_priv *priv, int frags,\n\t\t\t\t  u64 mapping[IPOIB_CM_RX_SG])\n{\n\tint i;\n\n\tib_dma_unmap_single(priv->ca, mapping[0], IPOIB_CM_HEAD_SIZE, DMA_FROM_DEVICE);\n\n\tfor (i = 0; i < frags; ++i)\n\t\tib_dma_unmap_page(priv->ca, mapping[i + 1], PAGE_SIZE, DMA_FROM_DEVICE);\n}\n\nstatic int ipoib_cm_post_receive_srq(struct net_device *dev, int id)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i, ret;\n\n\tpriv->cm.rx_wr.wr_id = id | IPOIB_OP_CM | IPOIB_OP_RECV;\n\n\tfor (i = 0; i < priv->cm.num_frags; ++i)\n\t\tpriv->cm.rx_sge[i].addr = priv->cm.srq_ring[id].mapping[i];\n\n\tret = ib_post_srq_recv(priv->cm.srq, &priv->cm.rx_wr, NULL);\n\tif (unlikely(ret)) {\n\t\tipoib_warn(priv, \"post srq failed for buf %d (%d)\\n\", id, ret);\n\t\tipoib_cm_dma_unmap_rx(priv, priv->cm.num_frags - 1,\n\t\t\t\t      priv->cm.srq_ring[id].mapping);\n\t\tdev_kfree_skb_any(priv->cm.srq_ring[id].skb);\n\t\tpriv->cm.srq_ring[id].skb = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic int ipoib_cm_post_receive_nonsrq(struct net_device *dev,\n\t\t\t\t\tstruct ipoib_cm_rx *rx,\n\t\t\t\t\tstruct ib_recv_wr *wr,\n\t\t\t\t\tstruct ib_sge *sge, int id)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i, ret;\n\n\twr->wr_id = id | IPOIB_OP_CM | IPOIB_OP_RECV;\n\n\tfor (i = 0; i < IPOIB_CM_RX_SG; ++i)\n\t\tsge[i].addr = rx->rx_ring[id].mapping[i];\n\n\tret = ib_post_recv(rx->qp, wr, NULL);\n\tif (unlikely(ret)) {\n\t\tipoib_warn(priv, \"post recv failed for buf %d (%d)\\n\", id, ret);\n\t\tipoib_cm_dma_unmap_rx(priv, IPOIB_CM_RX_SG - 1,\n\t\t\t\t      rx->rx_ring[id].mapping);\n\t\tdev_kfree_skb_any(rx->rx_ring[id].skb);\n\t\trx->rx_ring[id].skb = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic struct sk_buff *ipoib_cm_alloc_rx_skb(struct net_device *dev,\n\t\t\t\t\t     struct ipoib_cm_rx_buf *rx_ring,\n\t\t\t\t\t     int id, int frags,\n\t\t\t\t\t     u64 mapping[IPOIB_CM_RX_SG],\n\t\t\t\t\t     gfp_t gfp)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct sk_buff *skb;\n\tint i;\n\n\tskb = dev_alloc_skb(ALIGN(IPOIB_CM_HEAD_SIZE + IPOIB_PSEUDO_LEN, 16));\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, IPOIB_CM_RX_RESERVE);\n\n\tmapping[0] = ib_dma_map_single(priv->ca, skb->data, IPOIB_CM_HEAD_SIZE,\n\t\t\t\t       DMA_FROM_DEVICE);\n\tif (unlikely(ib_dma_mapping_error(priv->ca, mapping[0]))) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < frags; i++) {\n\t\tstruct page *page = alloc_page(gfp);\n\n\t\tif (!page)\n\t\t\tgoto partial_error;\n\t\tskb_fill_page_desc(skb, i, page, 0, PAGE_SIZE);\n\n\t\tmapping[i + 1] = ib_dma_map_page(priv->ca, page,\n\t\t\t\t\t\t 0, PAGE_SIZE, DMA_FROM_DEVICE);\n\t\tif (unlikely(ib_dma_mapping_error(priv->ca, mapping[i + 1])))\n\t\t\tgoto partial_error;\n\t}\n\n\trx_ring[id].skb = skb;\n\treturn skb;\n\npartial_error:\n\n\tib_dma_unmap_single(priv->ca, mapping[0], IPOIB_CM_HEAD_SIZE, DMA_FROM_DEVICE);\n\n\tfor (; i > 0; --i)\n\t\tib_dma_unmap_page(priv->ca, mapping[i], PAGE_SIZE, DMA_FROM_DEVICE);\n\n\tdev_kfree_skb_any(skb);\n\treturn NULL;\n}\n\nstatic void ipoib_cm_free_rx_ring(struct net_device *dev,\n\t\t\t\t  struct ipoib_cm_rx_buf *rx_ring)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < ipoib_recvq_size; ++i)\n\t\tif (rx_ring[i].skb) {\n\t\t\tipoib_cm_dma_unmap_rx(priv, IPOIB_CM_RX_SG - 1,\n\t\t\t\t\t      rx_ring[i].mapping);\n\t\t\tdev_kfree_skb_any(rx_ring[i].skb);\n\t\t}\n\n\tvfree(rx_ring);\n}\n\nstatic void ipoib_cm_start_rx_drain(struct ipoib_dev_priv *priv)\n{\n\tstruct ipoib_cm_rx *p;\n\n\t \n\tif (list_empty(&priv->cm.rx_flush_list) ||\n\t    !list_empty(&priv->cm.rx_drain_list))\n\t\treturn;\n\n\t \n\tp = list_entry(priv->cm.rx_flush_list.next, typeof(*p), list);\n\tipoib_cm_rx_drain_wr.wr_id = IPOIB_CM_RX_DRAIN_WRID;\n\tif (ib_post_send(p->qp, &ipoib_cm_rx_drain_wr, NULL))\n\t\tipoib_warn(priv, \"failed to post drain wr\\n\");\n\n\tlist_splice_init(&priv->cm.rx_flush_list, &priv->cm.rx_drain_list);\n}\n\nstatic void ipoib_cm_rx_event_handler(struct ib_event *event, void *ctx)\n{\n\tstruct ipoib_cm_rx *p = ctx;\n\tstruct ipoib_dev_priv *priv = ipoib_priv(p->dev);\n\tunsigned long flags;\n\n\tif (event->event != IB_EVENT_QP_LAST_WQE_REACHED)\n\t\treturn;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\tlist_move(&p->list, &priv->cm.rx_flush_list);\n\tp->state = IPOIB_CM_RX_FLUSH;\n\tipoib_cm_start_rx_drain(priv);\n\tspin_unlock_irqrestore(&priv->lock, flags);\n}\n\nstatic struct ib_qp *ipoib_cm_create_rx_qp(struct net_device *dev,\n\t\t\t\t\t   struct ipoib_cm_rx *p)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_qp_init_attr attr = {\n\t\t.event_handler = ipoib_cm_rx_event_handler,\n\t\t.send_cq = priv->recv_cq,  \n\t\t.recv_cq = priv->recv_cq,\n\t\t.srq = priv->cm.srq,\n\t\t.cap.max_send_wr = 1,  \n\t\t.cap.max_send_sge = 1,  \n\t\t.sq_sig_type = IB_SIGNAL_ALL_WR,\n\t\t.qp_type = IB_QPT_RC,\n\t\t.qp_context = p,\n\t};\n\n\tif (!ipoib_cm_has_srq(dev)) {\n\t\tattr.cap.max_recv_wr  = ipoib_recvq_size;\n\t\tattr.cap.max_recv_sge = IPOIB_CM_RX_SG;\n\t}\n\n\treturn ib_create_qp(priv->pd, &attr);\n}\n\nstatic int ipoib_cm_modify_rx_qp(struct net_device *dev,\n\t\t\t\t struct ib_cm_id *cm_id, struct ib_qp *qp,\n\t\t\t\t unsigned int psn)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tqp_attr.qp_state = IB_QPS_INIT;\n\tret = ib_cm_init_qp_attr(cm_id, &qp_attr, &qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to init QP attr for INIT: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify QP to INIT: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_cm_init_qp_attr(cm_id, &qp_attr, &qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to init QP attr for RTR: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tqp_attr.rq_psn = psn;\n\tret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify QP to RTR: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = ib_cm_init_qp_attr(cm_id, &qp_attr, &qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to init QP attr for RTS: %d\\n\", ret);\n\t\treturn 0;\n\t}\n\tret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify QP to RTS: %d\\n\", ret);\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nstatic void ipoib_cm_init_rx_wr(struct net_device *dev,\n\t\t\t\tstruct ib_recv_wr *wr,\n\t\t\t\tstruct ib_sge *sge)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < priv->cm.num_frags; ++i)\n\t\tsge[i].lkey = priv->pd->local_dma_lkey;\n\n\tsge[0].length = IPOIB_CM_HEAD_SIZE;\n\tfor (i = 1; i < priv->cm.num_frags; ++i)\n\t\tsge[i].length = PAGE_SIZE;\n\n\twr->next    = NULL;\n\twr->sg_list = sge;\n\twr->num_sge = priv->cm.num_frags;\n}\n\nstatic int ipoib_cm_nonsrq_init_rx(struct net_device *dev, struct ib_cm_id *cm_id,\n\t\t\t\t   struct ipoib_cm_rx *rx)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct {\n\t\tstruct ib_recv_wr wr;\n\t\tstruct ib_sge sge[IPOIB_CM_RX_SG];\n\t} *t;\n\tint ret;\n\tint i;\n\n\trx->rx_ring = vzalloc(array_size(ipoib_recvq_size,\n\t\t\t\t\t sizeof(*rx->rx_ring)));\n\tif (!rx->rx_ring)\n\t\treturn -ENOMEM;\n\n\tt = kmalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_1;\n\t}\n\n\tipoib_cm_init_rx_wr(dev, &t->wr, t->sge);\n\n\tspin_lock_irq(&priv->lock);\n\n\tif (priv->cm.nonsrq_conn_qp >= ipoib_max_conn_qp) {\n\t\tspin_unlock_irq(&priv->lock);\n\t\tib_send_cm_rej(cm_id, IB_CM_REJ_NO_QP, NULL, 0, NULL, 0);\n\t\tret = -EINVAL;\n\t\tgoto err_free;\n\t} else\n\t\t++priv->cm.nonsrq_conn_qp;\n\n\tspin_unlock_irq(&priv->lock);\n\n\tfor (i = 0; i < ipoib_recvq_size; ++i) {\n\t\tif (!ipoib_cm_alloc_rx_skb(dev, rx->rx_ring, i, IPOIB_CM_RX_SG - 1,\n\t\t\t\t\t   rx->rx_ring[i].mapping,\n\t\t\t\t\t   GFP_KERNEL)) {\n\t\t\tipoib_warn(priv, \"failed to allocate receive buffer %d\\n\", i);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_count;\n\t\t}\n\t\tret = ipoib_cm_post_receive_nonsrq(dev, rx, &t->wr, t->sge, i);\n\t\tif (ret) {\n\t\t\tipoib_warn(priv, \"ipoib_cm_post_receive_nonsrq \"\n\t\t\t\t   \"failed for buf %d\\n\", i);\n\t\t\tret = -EIO;\n\t\t\tgoto err_count;\n\t\t}\n\t}\n\n\trx->recv_count = ipoib_recvq_size;\n\n\tkfree(t);\n\n\treturn 0;\n\nerr_count:\n\tspin_lock_irq(&priv->lock);\n\t--priv->cm.nonsrq_conn_qp;\n\tspin_unlock_irq(&priv->lock);\n\nerr_free:\n\tkfree(t);\n\nerr_free_1:\n\tipoib_cm_free_rx_ring(dev, rx->rx_ring);\n\n\treturn ret;\n}\n\nstatic int ipoib_cm_send_rep(struct net_device *dev, struct ib_cm_id *cm_id,\n\t\t\t     struct ib_qp *qp,\n\t\t\t     const struct ib_cm_req_event_param *req,\n\t\t\t     unsigned int psn)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_data data = {};\n\tstruct ib_cm_rep_param rep = {};\n\n\tdata.qpn = cpu_to_be32(priv->qp->qp_num);\n\tdata.mtu = cpu_to_be32(IPOIB_CM_BUF_SIZE);\n\n\trep.private_data = &data;\n\trep.private_data_len = sizeof(data);\n\trep.flow_control = 0;\n\trep.rnr_retry_count = req->rnr_retry_count;\n\trep.srq = ipoib_cm_has_srq(dev);\n\trep.qp_num = qp->qp_num;\n\trep.starting_psn = psn;\n\treturn ib_send_cm_rep(cm_id, &rep);\n}\n\nstatic int ipoib_cm_req_handler(struct ib_cm_id *cm_id,\n\t\t\t\tconst struct ib_cm_event *event)\n{\n\tstruct net_device *dev = cm_id->context;\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_rx *p;\n\tunsigned int psn;\n\tint ret;\n\n\tipoib_dbg(priv, \"REQ arrived\\n\");\n\tp = kzalloc(sizeof(*p), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tp->dev = dev;\n\tp->id = cm_id;\n\tcm_id->context = p;\n\tp->state = IPOIB_CM_RX_LIVE;\n\tp->jiffies = jiffies;\n\tINIT_LIST_HEAD(&p->list);\n\n\tp->qp = ipoib_cm_create_rx_qp(dev, p);\n\tif (IS_ERR(p->qp)) {\n\t\tret = PTR_ERR(p->qp);\n\t\tgoto err_qp;\n\t}\n\n\tpsn = get_random_u32() & 0xffffff;\n\tret = ipoib_cm_modify_rx_qp(dev, cm_id, p->qp, psn);\n\tif (ret)\n\t\tgoto err_modify;\n\n\tif (!ipoib_cm_has_srq(dev)) {\n\t\tret = ipoib_cm_nonsrq_init_rx(dev, cm_id, p);\n\t\tif (ret)\n\t\t\tgoto err_modify;\n\t}\n\n\tspin_lock_irq(&priv->lock);\n\tqueue_delayed_work(priv->wq,\n\t\t\t   &priv->cm.stale_task, IPOIB_CM_RX_DELAY);\n\t \n\tp->jiffies = jiffies;\n\tif (p->state == IPOIB_CM_RX_LIVE)\n\t\tlist_move(&p->list, &priv->cm.passive_ids);\n\tspin_unlock_irq(&priv->lock);\n\n\tret = ipoib_cm_send_rep(dev, cm_id, p->qp, &event->param.req_rcvd, psn);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to send REP: %d\\n\", ret);\n\t\tif (ib_modify_qp(p->qp, &ipoib_cm_err_attr, IB_QP_STATE))\n\t\t\tipoib_warn(priv, \"unable to move qp to error state\\n\");\n\t}\n\treturn 0;\n\nerr_modify:\n\tib_destroy_qp(p->qp);\nerr_qp:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int ipoib_cm_rx_handler(struct ib_cm_id *cm_id,\n\t\t\t       const struct ib_cm_event *event)\n{\n\tstruct ipoib_cm_rx *p;\n\tstruct ipoib_dev_priv *priv;\n\n\tswitch (event->event) {\n\tcase IB_CM_REQ_RECEIVED:\n\t\treturn ipoib_cm_req_handler(cm_id, event);\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tib_send_cm_drep(cm_id, NULL, 0);\n\t\tfallthrough;\n\tcase IB_CM_REJ_RECEIVED:\n\t\tp = cm_id->context;\n\t\tpriv = ipoib_priv(p->dev);\n\t\tif (ib_modify_qp(p->qp, &ipoib_cm_err_attr, IB_QP_STATE))\n\t\t\tipoib_warn(priv, \"unable to move qp to error state\\n\");\n\t\tfallthrough;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n \nstatic void skb_put_frags(struct sk_buff *skb, unsigned int hdr_space,\n\t\t\t  unsigned int length, struct sk_buff *toskb)\n{\n\tint i, num_frags;\n\tunsigned int size;\n\n\t \n\tsize = min(length, hdr_space);\n\tskb->tail += size;\n\tskb->len += size;\n\tlength -= size;\n\n\tnum_frags = skb_shinfo(skb)->nr_frags;\n\tfor (i = 0; i < num_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tif (length == 0) {\n\t\t\t \n\t\t\tskb_fill_page_desc(toskb, i, skb_frag_page(frag),\n\t\t\t\t\t   0, PAGE_SIZE);\n\t\t\t--skb_shinfo(skb)->nr_frags;\n\t\t} else {\n\t\t\tsize = min_t(unsigned int, length, PAGE_SIZE);\n\n\t\t\tskb_frag_size_set(frag, size);\n\t\t\tskb->data_len += size;\n\t\t\tskb->truesize += size;\n\t\t\tskb->len += size;\n\t\t\tlength -= size;\n\t\t}\n\t}\n}\n\nvoid ipoib_cm_handle_rx_wc(struct net_device *dev, struct ib_wc *wc)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_rx_buf *rx_ring;\n\tunsigned int wr_id = wc->wr_id & ~(IPOIB_OP_CM | IPOIB_OP_RECV);\n\tstruct sk_buff *skb, *newskb;\n\tstruct ipoib_cm_rx *p;\n\tunsigned long flags;\n\tu64 mapping[IPOIB_CM_RX_SG];\n\tint frags;\n\tint has_srq;\n\tstruct sk_buff *small_skb;\n\n\tipoib_dbg_data(priv, \"cm recv completion: id %d, status: %d\\n\",\n\t\t       wr_id, wc->status);\n\n\tif (unlikely(wr_id >= ipoib_recvq_size)) {\n\t\tif (wr_id == (IPOIB_CM_RX_DRAIN_WRID & ~(IPOIB_OP_CM | IPOIB_OP_RECV))) {\n\t\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\t\tlist_splice_init(&priv->cm.rx_drain_list, &priv->cm.rx_reap_list);\n\t\t\tipoib_cm_start_rx_drain(priv);\n\t\t\tqueue_work(priv->wq, &priv->cm.rx_reap_task);\n\t\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\t} else\n\t\t\tipoib_warn(priv, \"cm recv completion event with wrid %d (> %d)\\n\",\n\t\t\t\t   wr_id, ipoib_recvq_size);\n\t\treturn;\n\t}\n\n\tp = wc->qp->qp_context;\n\n\thas_srq = ipoib_cm_has_srq(dev);\n\trx_ring = has_srq ? priv->cm.srq_ring : p->rx_ring;\n\n\tskb = rx_ring[wr_id].skb;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tipoib_dbg(priv,\n\t\t\t  \"cm recv error (status=%d, wrid=%d vend_err %#x)\\n\",\n\t\t\t  wc->status, wr_id, wc->vendor_err);\n\t\t++dev->stats.rx_dropped;\n\t\tif (has_srq)\n\t\t\tgoto repost;\n\t\telse {\n\t\t\tif (!--p->recv_count) {\n\t\t\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\t\t\tlist_move(&p->list, &priv->cm.rx_reap_list);\n\t\t\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\t\t\tqueue_work(priv->wq, &priv->cm.rx_reap_task);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (unlikely(!(wr_id & IPOIB_CM_RX_UPDATE_MASK))) {\n\t\tif (p && time_after_eq(jiffies, p->jiffies + IPOIB_CM_RX_UPDATE_TIME)) {\n\t\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\t\tp->jiffies = jiffies;\n\t\t\t \n\t\t\tif (p->state == IPOIB_CM_RX_LIVE)\n\t\t\t\tlist_move(&p->list, &priv->cm.passive_ids);\n\t\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\t}\n\t}\n\n\tif (wc->byte_len < IPOIB_CM_COPYBREAK) {\n\t\tint dlen = wc->byte_len;\n\n\t\tsmall_skb = dev_alloc_skb(dlen + IPOIB_CM_RX_RESERVE);\n\t\tif (small_skb) {\n\t\t\tskb_reserve(small_skb, IPOIB_CM_RX_RESERVE);\n\t\t\tib_dma_sync_single_for_cpu(priv->ca, rx_ring[wr_id].mapping[0],\n\t\t\t\t\t\t   dlen, DMA_FROM_DEVICE);\n\t\t\tskb_copy_from_linear_data(skb, small_skb->data, dlen);\n\t\t\tib_dma_sync_single_for_device(priv->ca, rx_ring[wr_id].mapping[0],\n\t\t\t\t\t\t      dlen, DMA_FROM_DEVICE);\n\t\t\tskb_put(small_skb, dlen);\n\t\t\tskb = small_skb;\n\t\t\tgoto copied;\n\t\t}\n\t}\n\n\tfrags = PAGE_ALIGN(wc->byte_len -\n\t\t\t   min_t(u32, wc->byte_len, IPOIB_CM_HEAD_SIZE)) /\n\t\tPAGE_SIZE;\n\n\tnewskb = ipoib_cm_alloc_rx_skb(dev, rx_ring, wr_id, frags,\n\t\t\t\t       mapping, GFP_ATOMIC);\n\tif (unlikely(!newskb)) {\n\t\t \n\t\tipoib_dbg(priv, \"failed to allocate receive buffer %d\\n\", wr_id);\n\t\t++dev->stats.rx_dropped;\n\t\tgoto repost;\n\t}\n\n\tipoib_cm_dma_unmap_rx(priv, frags, rx_ring[wr_id].mapping);\n\tmemcpy(rx_ring[wr_id].mapping, mapping, (frags + 1) * sizeof(*mapping));\n\n\tipoib_dbg_data(priv, \"received %d bytes, SLID 0x%04x\\n\",\n\t\t       wc->byte_len, wc->slid);\n\n\tskb_put_frags(skb, IPOIB_CM_HEAD_SIZE, wc->byte_len, newskb);\n\ncopied:\n\tskb->protocol = ((struct ipoib_header *) skb->data)->proto;\n\tskb_add_pseudo_hdr(skb);\n\n\t++dev->stats.rx_packets;\n\tdev->stats.rx_bytes += skb->len;\n\n\tskb->dev = dev;\n\t \n\tskb->pkt_type = PACKET_HOST;\n\tnetif_receive_skb(skb);\n\nrepost:\n\tif (has_srq) {\n\t\tif (unlikely(ipoib_cm_post_receive_srq(dev, wr_id)))\n\t\t\tipoib_warn(priv, \"ipoib_cm_post_receive_srq failed \"\n\t\t\t\t   \"for buf %d\\n\", wr_id);\n\t} else {\n\t\tif (unlikely(ipoib_cm_post_receive_nonsrq(dev, p,\n\t\t\t\t\t\t\t  &priv->cm.rx_wr,\n\t\t\t\t\t\t\t  priv->cm.rx_sge,\n\t\t\t\t\t\t\t  wr_id))) {\n\t\t\t--p->recv_count;\n\t\t\tipoib_warn(priv, \"ipoib_cm_post_receive_nonsrq failed \"\n\t\t\t\t   \"for buf %d\\n\", wr_id);\n\t\t}\n\t}\n}\n\nstatic inline int post_send(struct ipoib_dev_priv *priv,\n\t\t\t    struct ipoib_cm_tx *tx,\n\t\t\t    unsigned int wr_id,\n\t\t\t    struct ipoib_tx_buf *tx_req)\n{\n\tipoib_build_sge(priv, tx_req);\n\n\tpriv->tx_wr.wr.wr_id\t= wr_id | IPOIB_OP_CM;\n\n\treturn ib_post_send(tx->qp, &priv->tx_wr.wr, NULL);\n}\n\nvoid ipoib_cm_send(struct net_device *dev, struct sk_buff *skb, struct ipoib_cm_tx *tx)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_tx_buf *tx_req;\n\tint rc;\n\tunsigned int usable_sge = tx->max_send_sge - !!skb_headlen(skb);\n\n\tif (unlikely(skb->len > tx->mtu)) {\n\t\tipoib_warn(priv, \"packet len %d (> %d) too long to send, dropping\\n\",\n\t\t\t   skb->len, tx->mtu);\n\t\t++dev->stats.tx_dropped;\n\t\t++dev->stats.tx_errors;\n\t\tipoib_cm_skb_too_long(dev, skb, tx->mtu - IPOIB_ENCAP_LEN);\n\t\treturn;\n\t}\n\tif (skb_shinfo(skb)->nr_frags > usable_sge) {\n\t\tif (skb_linearize(skb) < 0) {\n\t\t\tipoib_warn(priv, \"skb could not be linearized\\n\");\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tif (skb_shinfo(skb)->nr_frags > usable_sge) {\n\t\t\tipoib_warn(priv, \"too many frags after skb linearize\\n\");\n\t\t\t++dev->stats.tx_dropped;\n\t\t\t++dev->stats.tx_errors;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn;\n\t\t}\n\t}\n\tipoib_dbg_data(priv, \"sending packet: head 0x%x length %d connection 0x%x\\n\",\n\t\t       tx->tx_head, skb->len, tx->qp->qp_num);\n\n\t \n\ttx_req = &tx->tx_ring[tx->tx_head & (ipoib_sendq_size - 1)];\n\ttx_req->skb = skb;\n\n\tif (unlikely(ipoib_dma_map_tx(priv->ca, tx_req))) {\n\t\t++dev->stats.tx_errors;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn;\n\t}\n\n\tif ((priv->global_tx_head - priv->global_tx_tail) ==\n\t    ipoib_sendq_size - 1) {\n\t\tipoib_dbg(priv, \"TX ring 0x%x full, stopping kernel net queue\\n\",\n\t\t\t  tx->qp->qp_num);\n\t\tnetif_stop_queue(dev);\n\t}\n\n\tskb_orphan(skb);\n\tskb_dst_drop(skb);\n\n\tif (netif_queue_stopped(dev)) {\n\t\trc = ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP |\n\t\t\t\t      IB_CQ_REPORT_MISSED_EVENTS);\n\t\tif (unlikely(rc < 0))\n\t\t\tipoib_warn(priv, \"IPoIB/CM:request notify on send CQ failed\\n\");\n\t\telse if (rc)\n\t\t\tnapi_schedule(&priv->send_napi);\n\t}\n\n\trc = post_send(priv, tx, tx->tx_head & (ipoib_sendq_size - 1), tx_req);\n\tif (unlikely(rc)) {\n\t\tipoib_warn(priv, \"IPoIB/CM:post_send failed, error %d\\n\", rc);\n\t\t++dev->stats.tx_errors;\n\t\tipoib_dma_unmap_tx(priv, tx_req);\n\t\tdev_kfree_skb_any(skb);\n\n\t\tif (netif_queue_stopped(dev))\n\t\t\tnetif_wake_queue(dev);\n\t} else {\n\t\tnetif_trans_update(dev);\n\t\t++tx->tx_head;\n\t\t++priv->global_tx_head;\n\t}\n}\n\nvoid ipoib_cm_handle_tx_wc(struct net_device *dev, struct ib_wc *wc)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_tx *tx = wc->qp->qp_context;\n\tunsigned int wr_id = wc->wr_id & ~IPOIB_OP_CM;\n\tstruct ipoib_tx_buf *tx_req;\n\tunsigned long flags;\n\n\tipoib_dbg_data(priv, \"cm send completion: id %d, status: %d\\n\",\n\t\t       wr_id, wc->status);\n\n\tif (unlikely(wr_id >= ipoib_sendq_size)) {\n\t\tipoib_warn(priv, \"cm send completion event with wrid %d (> %d)\\n\",\n\t\t\t   wr_id, ipoib_sendq_size);\n\t\treturn;\n\t}\n\n\ttx_req = &tx->tx_ring[wr_id];\n\n\tipoib_dma_unmap_tx(priv, tx_req);\n\n\t \n\t++dev->stats.tx_packets;\n\tdev->stats.tx_bytes += tx_req->skb->len;\n\n\tdev_kfree_skb_any(tx_req->skb);\n\n\tnetif_tx_lock(dev);\n\n\t++tx->tx_tail;\n\t++priv->global_tx_tail;\n\n\tif (unlikely(netif_queue_stopped(dev) &&\n\t\t     ((priv->global_tx_head - priv->global_tx_tail) <=\n\t\t      ipoib_sendq_size >> 1) &&\n\t\t     test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags)))\n\t\tnetif_wake_queue(dev);\n\n\tif (wc->status != IB_WC_SUCCESS &&\n\t    wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\tstruct ipoib_neigh *neigh;\n\n\t\t \n\t\tif (wc->status == IB_WC_RNR_RETRY_EXC_ERR ||\n\t\t    wc->status == IB_WC_RETRY_EXC_ERR)\n\t\t\tipoib_dbg(priv,\n\t\t\t\t  \"%s: failed cm send event (status=%d, wrid=%d vend_err %#x)\\n\",\n\t\t\t\t   __func__, wc->status, wr_id, wc->vendor_err);\n\t\telse\n\t\t\tipoib_warn(priv,\n\t\t\t\t    \"%s: failed cm send event (status=%d, wrid=%d vend_err %#x)\\n\",\n\t\t\t\t   __func__, wc->status, wr_id, wc->vendor_err);\n\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\tneigh = tx->neigh;\n\n\t\tif (neigh) {\n\t\t\tneigh->cm = NULL;\n\t\t\tipoib_neigh_free(neigh);\n\n\t\t\ttx->neigh = NULL;\n\t\t}\n\n\t\tif (test_and_clear_bit(IPOIB_FLAG_INITIALIZED, &tx->flags)) {\n\t\t\tlist_move(&tx->list, &priv->cm.reap_list);\n\t\t\tqueue_work(priv->wq, &priv->cm.reap_task);\n\t\t}\n\n\t\tclear_bit(IPOIB_FLAG_OPER_UP, &tx->flags);\n\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t}\n\n\tnetif_tx_unlock(dev);\n}\n\nint ipoib_cm_dev_open(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint ret;\n\n\tif (!IPOIB_CM_SUPPORTED(dev->dev_addr))\n\t\treturn 0;\n\n\tpriv->cm.id = ib_create_cm_id(priv->ca, ipoib_cm_rx_handler, dev);\n\tif (IS_ERR(priv->cm.id)) {\n\t\tpr_warn(\"%s: failed to create CM ID\\n\", priv->ca->name);\n\t\tret = PTR_ERR(priv->cm.id);\n\t\tgoto err_cm;\n\t}\n\n\tret = ib_cm_listen(priv->cm.id,\n\t\t\t   cpu_to_be64(IPOIB_CM_IETF_ID | priv->qp->qp_num));\n\tif (ret) {\n\t\tpr_warn(\"%s: failed to listen on ID 0x%llx\\n\", priv->ca->name,\n\t\t\tIPOIB_CM_IETF_ID | priv->qp->qp_num);\n\t\tgoto err_listen;\n\t}\n\n\treturn 0;\n\nerr_listen:\n\tib_destroy_cm_id(priv->cm.id);\nerr_cm:\n\tpriv->cm.id = NULL;\n\treturn ret;\n}\n\nstatic void ipoib_cm_free_rx_reap_list(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_rx *rx, *n;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&priv->lock);\n\tlist_splice_init(&priv->cm.rx_reap_list, &list);\n\tspin_unlock_irq(&priv->lock);\n\n\tlist_for_each_entry_safe(rx, n, &list, list) {\n\t\tib_destroy_cm_id(rx->id);\n\t\tib_destroy_qp(rx->qp);\n\t\tif (!ipoib_cm_has_srq(dev)) {\n\t\t\tipoib_cm_free_rx_ring(priv->dev, rx->rx_ring);\n\t\t\tspin_lock_irq(&priv->lock);\n\t\t\t--priv->cm.nonsrq_conn_qp;\n\t\t\tspin_unlock_irq(&priv->lock);\n\t\t}\n\t\tkfree(rx);\n\t}\n}\n\nvoid ipoib_cm_dev_stop(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_rx *p;\n\tunsigned long begin;\n\tint ret;\n\n\tif (!IPOIB_CM_SUPPORTED(dev->dev_addr) || !priv->cm.id)\n\t\treturn;\n\n\tib_destroy_cm_id(priv->cm.id);\n\tpriv->cm.id = NULL;\n\n\tspin_lock_irq(&priv->lock);\n\twhile (!list_empty(&priv->cm.passive_ids)) {\n\t\tp = list_entry(priv->cm.passive_ids.next, typeof(*p), list);\n\t\tlist_move(&p->list, &priv->cm.rx_error_list);\n\t\tp->state = IPOIB_CM_RX_ERROR;\n\t\tspin_unlock_irq(&priv->lock);\n\t\tret = ib_modify_qp(p->qp, &ipoib_cm_err_attr, IB_QP_STATE);\n\t\tif (ret)\n\t\t\tipoib_warn(priv, \"unable to move qp to error state: %d\\n\", ret);\n\t\tspin_lock_irq(&priv->lock);\n\t}\n\n\t \n\tbegin = jiffies;\n\n\twhile (!list_empty(&priv->cm.rx_error_list) ||\n\t       !list_empty(&priv->cm.rx_flush_list) ||\n\t       !list_empty(&priv->cm.rx_drain_list)) {\n\t\tif (time_after(jiffies, begin + 5 * HZ)) {\n\t\t\tipoib_warn(priv, \"RX drain timing out\\n\");\n\n\t\t\t \n\t\t\tlist_splice_init(&priv->cm.rx_flush_list,\n\t\t\t\t\t &priv->cm.rx_reap_list);\n\t\t\tlist_splice_init(&priv->cm.rx_error_list,\n\t\t\t\t\t &priv->cm.rx_reap_list);\n\t\t\tlist_splice_init(&priv->cm.rx_drain_list,\n\t\t\t\t\t &priv->cm.rx_reap_list);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&priv->lock);\n\t\tusleep_range(1000, 2000);\n\t\tipoib_drain_cq(dev);\n\t\tspin_lock_irq(&priv->lock);\n\t}\n\n\tspin_unlock_irq(&priv->lock);\n\n\tipoib_cm_free_rx_reap_list(dev);\n\n\tcancel_delayed_work(&priv->cm.stale_task);\n}\n\nstatic int ipoib_cm_rep_handler(struct ib_cm_id *cm_id,\n\t\t\t\tconst struct ib_cm_event *event)\n{\n\tstruct ipoib_cm_tx *p = cm_id->context;\n\tstruct ipoib_dev_priv *priv = ipoib_priv(p->dev);\n\tstruct ipoib_cm_data *data = event->private_data;\n\tstruct sk_buff_head skqueue;\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\tstruct sk_buff *skb;\n\n\tp->mtu = be32_to_cpu(data->mtu);\n\n\tif (p->mtu <= IPOIB_ENCAP_LEN) {\n\t\tipoib_warn(priv, \"Rejecting connection: mtu %d <= %d\\n\",\n\t\t\t   p->mtu, IPOIB_ENCAP_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_cm_init_qp_attr(cm_id, &qp_attr, &qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to init QP attr for RTR: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tqp_attr.rq_psn = 0  ;\n\tret = ib_modify_qp(p->qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify QP to RTR: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = ib_cm_init_qp_attr(cm_id, &qp_attr, &qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to init QP attr for RTS: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tret = ib_modify_qp(p->qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify QP to RTS: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tskb_queue_head_init(&skqueue);\n\n\tnetif_tx_lock_bh(p->dev);\n\tspin_lock_irq(&priv->lock);\n\tset_bit(IPOIB_FLAG_OPER_UP, &p->flags);\n\tif (p->neigh)\n\t\twhile ((skb = __skb_dequeue(&p->neigh->queue)))\n\t\t\t__skb_queue_tail(&skqueue, skb);\n\tspin_unlock_irq(&priv->lock);\n\tnetif_tx_unlock_bh(p->dev);\n\n\twhile ((skb = __skb_dequeue(&skqueue))) {\n\t\tskb->dev = p->dev;\n\t\tret = dev_queue_xmit(skb);\n\t\tif (ret)\n\t\t\tipoib_warn(priv, \"%s:dev_queue_xmit failed to re-queue packet, ret:%d\\n\",\n\t\t\t\t   __func__, ret);\n\t}\n\n\tret = ib_send_cm_rtu(cm_id, NULL, 0);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to send RTU: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic struct ib_qp *ipoib_cm_create_tx_qp(struct net_device *dev, struct ipoib_cm_tx *tx)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_qp_init_attr attr = {\n\t\t.send_cq\t\t= priv->send_cq,\n\t\t.recv_cq\t\t= priv->recv_cq,\n\t\t.srq\t\t\t= priv->cm.srq,\n\t\t.cap.max_send_wr\t= ipoib_sendq_size,\n\t\t.cap.max_send_sge\t= 1,\n\t\t.sq_sig_type\t\t= IB_SIGNAL_ALL_WR,\n\t\t.qp_type\t\t= IB_QPT_RC,\n\t\t.qp_context\t\t= tx,\n\t\t.create_flags\t\t= 0\n\t};\n\tstruct ib_qp *tx_qp;\n\n\tif (dev->features & NETIF_F_SG)\n\t\tattr.cap.max_send_sge = min_t(u32, priv->ca->attrs.max_send_sge,\n\t\t\t\t\t      MAX_SKB_FRAGS + 1);\n\n\ttx_qp = ib_create_qp(priv->pd, &attr);\n\ttx->max_send_sge = attr.cap.max_send_sge;\n\treturn tx_qp;\n}\n\nstatic int ipoib_cm_send_req(struct net_device *dev,\n\t\t\t     struct ib_cm_id *id, struct ib_qp *qp,\n\t\t\t     u32 qpn,\n\t\t\t     struct sa_path_rec *pathrec)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_data data = {};\n\tstruct ib_cm_req_param req = {};\n\n\tdata.qpn = cpu_to_be32(priv->qp->qp_num);\n\tdata.mtu = cpu_to_be32(IPOIB_CM_BUF_SIZE);\n\n\treq.primary_path\t\t= pathrec;\n\treq.alternate_path\t\t= NULL;\n\treq.service_id\t\t\t= cpu_to_be64(IPOIB_CM_IETF_ID | qpn);\n\treq.qp_num\t\t\t= qp->qp_num;\n\treq.qp_type\t\t\t= qp->qp_type;\n\treq.private_data\t\t= &data;\n\treq.private_data_len\t\t= sizeof(data);\n\treq.flow_control\t\t= 0;\n\n\treq.starting_psn\t\t= 0;  \n\n\t \n\treq.responder_resources\t\t= 4;\n\treq.remote_cm_response_timeout\t= 20;\n\treq.local_cm_response_timeout\t= 20;\n\treq.retry_count\t\t\t= 0;  \n\treq.rnr_retry_count\t\t= 0;  \n\treq.max_cm_retries\t\t= 15;\n\treq.srq\t\t\t\t= ipoib_cm_has_srq(dev);\n\treturn ib_send_cm_req(id, &req);\n}\n\nstatic int ipoib_cm_modify_tx_init(struct net_device *dev,\n\t\t\t\t  struct ib_cm_id *cm_id, struct ib_qp *qp)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_qp_attr qp_attr;\n\tint qp_attr_mask, ret;\n\n\tqp_attr.pkey_index = priv->pkey_index;\n\tqp_attr.qp_state = IB_QPS_INIT;\n\tqp_attr.qp_access_flags = IB_ACCESS_LOCAL_WRITE;\n\tqp_attr.port_num = priv->port;\n\tqp_attr_mask = IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PKEY_INDEX | IB_QP_PORT;\n\n\tret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify tx QP to INIT: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int ipoib_cm_tx_init(struct ipoib_cm_tx *p, u32 qpn,\n\t\t\t    struct sa_path_rec *pathrec)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(p->dev);\n\tunsigned int noio_flag;\n\tint ret;\n\n\tnoio_flag = memalloc_noio_save();\n\tp->tx_ring = vzalloc(array_size(ipoib_sendq_size, sizeof(*p->tx_ring)));\n\tif (!p->tx_ring) {\n\t\tmemalloc_noio_restore(noio_flag);\n\t\tret = -ENOMEM;\n\t\tgoto err_tx;\n\t}\n\n\tp->qp = ipoib_cm_create_tx_qp(p->dev, p);\n\tmemalloc_noio_restore(noio_flag);\n\tif (IS_ERR(p->qp)) {\n\t\tret = PTR_ERR(p->qp);\n\t\tipoib_warn(priv, \"failed to create tx qp: %d\\n\", ret);\n\t\tgoto err_qp;\n\t}\n\n\tp->id = ib_create_cm_id(priv->ca, ipoib_cm_tx_handler, p);\n\tif (IS_ERR(p->id)) {\n\t\tret = PTR_ERR(p->id);\n\t\tipoib_warn(priv, \"failed to create tx cm id: %d\\n\", ret);\n\t\tgoto err_id;\n\t}\n\n\tret = ipoib_cm_modify_tx_init(p->dev, p->id,  p->qp);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to modify tx qp to rtr: %d\\n\", ret);\n\t\tgoto err_modify_send;\n\t}\n\n\tret = ipoib_cm_send_req(p->dev, p->id, p->qp, qpn, pathrec);\n\tif (ret) {\n\t\tipoib_warn(priv, \"failed to send cm req: %d\\n\", ret);\n\t\tgoto err_modify_send;\n\t}\n\n\tipoib_dbg(priv, \"Request connection 0x%x for gid %pI6 qpn 0x%x\\n\",\n\t\t  p->qp->qp_num, pathrec->dgid.raw, qpn);\n\n\treturn 0;\n\nerr_modify_send:\n\tib_destroy_cm_id(p->id);\nerr_id:\n\tp->id = NULL;\n\tib_destroy_qp(p->qp);\nerr_qp:\n\tp->qp = NULL;\n\tvfree(p->tx_ring);\nerr_tx:\n\treturn ret;\n}\n\nstatic void ipoib_cm_tx_destroy(struct ipoib_cm_tx *p)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(p->dev);\n\tstruct ipoib_tx_buf *tx_req;\n\tunsigned long begin;\n\n\tipoib_dbg(priv, \"Destroy active connection 0x%x head 0x%x tail 0x%x\\n\",\n\t\t  p->qp ? p->qp->qp_num : 0, p->tx_head, p->tx_tail);\n\n\tif (p->id)\n\t\tib_destroy_cm_id(p->id);\n\n\tif (p->tx_ring) {\n\t\t \n\t\tbegin = jiffies;\n\t\twhile ((int) p->tx_tail - (int) p->tx_head < 0) {\n\t\t\tif (time_after(jiffies, begin + 5 * HZ)) {\n\t\t\t\tipoib_warn(priv, \"timing out; %d sends not completed\\n\",\n\t\t\t\t\t   p->tx_head - p->tx_tail);\n\t\t\t\tgoto timeout;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t}\n\t}\n\ntimeout:\n\n\twhile ((int) p->tx_tail - (int) p->tx_head < 0) {\n\t\ttx_req = &p->tx_ring[p->tx_tail & (ipoib_sendq_size - 1)];\n\t\tipoib_dma_unmap_tx(priv, tx_req);\n\t\tdev_kfree_skb_any(tx_req->skb);\n\t\tnetif_tx_lock_bh(p->dev);\n\t\t++p->tx_tail;\n\t\t++priv->global_tx_tail;\n\t\tif (unlikely((priv->global_tx_head - priv->global_tx_tail) <=\n\t\t\t     ipoib_sendq_size >> 1) &&\n\t\t    netif_queue_stopped(p->dev) &&\n\t\t    test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags))\n\t\t\tnetif_wake_queue(p->dev);\n\t\tnetif_tx_unlock_bh(p->dev);\n\t}\n\n\tif (p->qp)\n\t\tib_destroy_qp(p->qp);\n\n\tvfree(p->tx_ring);\n\tkfree(p);\n}\n\nstatic int ipoib_cm_tx_handler(struct ib_cm_id *cm_id,\n\t\t\t       const struct ib_cm_event *event)\n{\n\tstruct ipoib_cm_tx *tx = cm_id->context;\n\tstruct ipoib_dev_priv *priv = ipoib_priv(tx->dev);\n\tstruct net_device *dev = priv->dev;\n\tstruct ipoib_neigh *neigh;\n\tunsigned long flags;\n\tint ret;\n\n\tswitch (event->event) {\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tipoib_dbg(priv, \"DREQ received.\\n\");\n\t\tib_send_cm_drep(cm_id, NULL, 0);\n\t\tbreak;\n\tcase IB_CM_REP_RECEIVED:\n\t\tipoib_dbg(priv, \"REP received.\\n\");\n\t\tret = ipoib_cm_rep_handler(cm_id, event);\n\t\tif (ret)\n\t\t\tib_send_cm_rej(cm_id, IB_CM_REJ_CONSUMER_DEFINED,\n\t\t\t\t       NULL, 0, NULL, 0);\n\t\tbreak;\n\tcase IB_CM_REQ_ERROR:\n\tcase IB_CM_REJ_RECEIVED:\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tipoib_dbg(priv, \"CM error %d.\\n\", event->event);\n\t\tnetif_tx_lock_bh(dev);\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\tneigh = tx->neigh;\n\n\t\tif (neigh) {\n\t\t\tneigh->cm = NULL;\n\t\t\tipoib_neigh_free(neigh);\n\n\t\t\ttx->neigh = NULL;\n\t\t}\n\n\t\tif (test_and_clear_bit(IPOIB_FLAG_INITIALIZED, &tx->flags)) {\n\t\t\tlist_move(&tx->list, &priv->cm.reap_list);\n\t\t\tqueue_work(priv->wq, &priv->cm.reap_task);\n\t\t}\n\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\tnetif_tx_unlock_bh(dev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstruct ipoib_cm_tx *ipoib_cm_create_tx(struct net_device *dev, struct ipoib_path *path,\n\t\t\t\t       struct ipoib_neigh *neigh)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ipoib_cm_tx *tx;\n\n\ttx = kzalloc(sizeof(*tx), GFP_ATOMIC);\n\tif (!tx)\n\t\treturn NULL;\n\n\tneigh->cm = tx;\n\ttx->neigh = neigh;\n\ttx->dev = dev;\n\tlist_add(&tx->list, &priv->cm.start_list);\n\tset_bit(IPOIB_FLAG_INITIALIZED, &tx->flags);\n\tqueue_work(priv->wq, &priv->cm.start_task);\n\treturn tx;\n}\n\nvoid ipoib_cm_destroy_tx(struct ipoib_cm_tx *tx)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(tx->dev);\n\tunsigned long flags;\n\tif (test_and_clear_bit(IPOIB_FLAG_INITIALIZED, &tx->flags)) {\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\tlist_move(&tx->list, &priv->cm.reap_list);\n\t\tqueue_work(priv->wq, &priv->cm.reap_task);\n\t\tipoib_dbg(priv, \"Reap connection for gid %pI6\\n\",\n\t\t\t  tx->neigh->daddr + 4);\n\t\ttx->neigh = NULL;\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t}\n}\n\n#define QPN_AND_OPTIONS_OFFSET\t4\n\nstatic void ipoib_cm_tx_start(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv = container_of(work, struct ipoib_dev_priv,\n\t\t\t\t\t\t   cm.start_task);\n\tstruct net_device *dev = priv->dev;\n\tstruct ipoib_neigh *neigh;\n\tstruct ipoib_cm_tx *p;\n\tunsigned long flags;\n\tstruct ipoib_path *path;\n\tint ret;\n\n\tstruct sa_path_rec pathrec;\n\tu32 qpn;\n\n\tnetif_tx_lock_bh(dev);\n\tspin_lock_irqsave(&priv->lock, flags);\n\n\twhile (!list_empty(&priv->cm.start_list)) {\n\t\tp = list_entry(priv->cm.start_list.next, typeof(*p), list);\n\t\tlist_del_init(&p->list);\n\t\tneigh = p->neigh;\n\n\t\tqpn = IPOIB_QPN(neigh->daddr);\n\t\t \n\t\tpath = __path_find(dev, neigh->daddr + QPN_AND_OPTIONS_OFFSET);\n\t\tif (!path) {\n\t\t\tpr_info(\"%s ignore not valid path %pI6\\n\",\n\t\t\t\t__func__,\n\t\t\t\tneigh->daddr + QPN_AND_OPTIONS_OFFSET);\n\t\t\tgoto free_neigh;\n\t\t}\n\t\tmemcpy(&pathrec, &path->pathrec, sizeof(pathrec));\n\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\tnetif_tx_unlock_bh(dev);\n\n\t\tret = ipoib_cm_tx_init(p, qpn, &pathrec);\n\n\t\tnetif_tx_lock_bh(dev);\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\n\t\tif (ret) {\nfree_neigh:\n\t\t\tneigh = p->neigh;\n\t\t\tif (neigh) {\n\t\t\t\tneigh->cm = NULL;\n\t\t\t\tipoib_neigh_free(neigh);\n\t\t\t}\n\t\t\tlist_del(&p->list);\n\t\t\tkfree(p);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\tnetif_tx_unlock_bh(dev);\n}\n\nstatic void ipoib_cm_tx_reap(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv = container_of(work, struct ipoib_dev_priv,\n\t\t\t\t\t\t   cm.reap_task);\n\tstruct net_device *dev = priv->dev;\n\tstruct ipoib_cm_tx *p;\n\tunsigned long flags;\n\n\tnetif_tx_lock_bh(dev);\n\tspin_lock_irqsave(&priv->lock, flags);\n\n\twhile (!list_empty(&priv->cm.reap_list)) {\n\t\tp = list_entry(priv->cm.reap_list.next, typeof(*p), list);\n\t\tlist_del_init(&p->list);\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\tnetif_tx_unlock_bh(dev);\n\t\tipoib_cm_tx_destroy(p);\n\t\tnetif_tx_lock_bh(dev);\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t}\n\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\tnetif_tx_unlock_bh(dev);\n}\n\nstatic void ipoib_cm_skb_reap(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv = container_of(work, struct ipoib_dev_priv,\n\t\t\t\t\t\t   cm.skb_task);\n\tstruct net_device *dev = priv->dev;\n\tstruct sk_buff *skb;\n\tunsigned long flags;\n\tunsigned int mtu = priv->mcast_mtu;\n\n\tnetif_tx_lock_bh(dev);\n\tspin_lock_irqsave(&priv->lock, flags);\n\n\twhile ((skb = skb_dequeue(&priv->cm.skb_queue))) {\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\t\tnetif_tx_unlock_bh(dev);\n\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\tmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));\n\t\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));\n\t\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tmemset(IP6CB(skb), 0, sizeof(*IP6CB(skb)));\n\t\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t}\n#endif\n\t\tdev_kfree_skb_any(skb);\n\n\t\tnetif_tx_lock_bh(dev);\n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t}\n\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\tnetif_tx_unlock_bh(dev);\n}\n\nvoid ipoib_cm_skb_too_long(struct net_device *dev, struct sk_buff *skb,\n\t\t\t   unsigned int mtu)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint e = skb_queue_empty(&priv->cm.skb_queue);\n\n\tskb_dst_update_pmtu(skb, mtu);\n\n\tskb_queue_tail(&priv->cm.skb_queue, skb);\n\tif (e)\n\t\tqueue_work(priv->wq, &priv->cm.skb_task);\n}\n\nstatic void ipoib_cm_rx_reap(struct work_struct *work)\n{\n\tipoib_cm_free_rx_reap_list(container_of(work, struct ipoib_dev_priv,\n\t\t\t\t\t\tcm.rx_reap_task)->dev);\n}\n\nstatic void ipoib_cm_stale_task(struct work_struct *work)\n{\n\tstruct ipoib_dev_priv *priv = container_of(work, struct ipoib_dev_priv,\n\t\t\t\t\t\t   cm.stale_task.work);\n\tstruct ipoib_cm_rx *p;\n\tint ret;\n\n\tspin_lock_irq(&priv->lock);\n\twhile (!list_empty(&priv->cm.passive_ids)) {\n\t\t \n\t\tp = list_entry(priv->cm.passive_ids.prev, typeof(*p), list);\n\t\tif (time_before_eq(jiffies, p->jiffies + IPOIB_CM_RX_TIMEOUT))\n\t\t\tbreak;\n\t\tlist_move(&p->list, &priv->cm.rx_error_list);\n\t\tp->state = IPOIB_CM_RX_ERROR;\n\t\tspin_unlock_irq(&priv->lock);\n\t\tret = ib_modify_qp(p->qp, &ipoib_cm_err_attr, IB_QP_STATE);\n\t\tif (ret)\n\t\t\tipoib_warn(priv, \"unable to move qp to error state: %d\\n\", ret);\n\t\tspin_lock_irq(&priv->lock);\n\t}\n\n\tif (!list_empty(&priv->cm.passive_ids))\n\t\tqueue_delayed_work(priv->wq,\n\t\t\t\t   &priv->cm.stale_task, IPOIB_CM_RX_DELAY);\n\tspin_unlock_irq(&priv->lock);\n}\n\nstatic ssize_t mode_show(struct device *d, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct net_device *dev = to_net_dev(d);\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tif (test_bit(IPOIB_FLAG_ADMIN_CM, &priv->flags))\n\t\treturn sysfs_emit(buf, \"connected\\n\");\n\telse\n\t\treturn sysfs_emit(buf, \"datagram\\n\");\n}\n\nstatic ssize_t mode_store(struct device *d, struct device_attribute *attr,\n\t\t\t  const char *buf, size_t count)\n{\n\tstruct net_device *dev = to_net_dev(d);\n\tint ret;\n\n\tif (!rtnl_trylock()) {\n\t\treturn restart_syscall();\n\t}\n\n\tif (dev->reg_state != NETREG_REGISTERED) {\n\t\trtnl_unlock();\n\t\treturn -EPERM;\n\t}\n\n\tret = ipoib_set_mode(dev, buf);\n\n\t \n\tif (ret != -EBUSY)\n\t\trtnl_unlock();\n\n\treturn (!ret || ret == -EBUSY) ? count : ret;\n}\n\nstatic DEVICE_ATTR_RW(mode);\n\nint ipoib_cm_add_mode_attr(struct net_device *dev)\n{\n\treturn device_create_file(&dev->dev, &dev_attr_mode);\n}\n\nstatic void ipoib_cm_create_srq(struct net_device *dev, int max_sge)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tstruct ib_srq_init_attr srq_init_attr = {\n\t\t.srq_type = IB_SRQT_BASIC,\n\t\t.attr = {\n\t\t\t.max_wr  = ipoib_recvq_size,\n\t\t\t.max_sge = max_sge\n\t\t}\n\t};\n\n\tpriv->cm.srq = ib_create_srq(priv->pd, &srq_init_attr);\n\tif (IS_ERR(priv->cm.srq)) {\n\t\tif (PTR_ERR(priv->cm.srq) != -EOPNOTSUPP)\n\t\t\tpr_warn(\"%s: failed to allocate SRQ, error %ld\\n\",\n\t\t\t       priv->ca->name, PTR_ERR(priv->cm.srq));\n\t\tpriv->cm.srq = NULL;\n\t\treturn;\n\t}\n\n\tpriv->cm.srq_ring = vzalloc(array_size(ipoib_recvq_size,\n\t\t\t\t\t       sizeof(*priv->cm.srq_ring)));\n\tif (!priv->cm.srq_ring) {\n\t\tib_destroy_srq(priv->cm.srq);\n\t\tpriv->cm.srq = NULL;\n\t\treturn;\n\t}\n\n}\n\nint ipoib_cm_dev_init(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\tint max_srq_sge, i;\n\tu8 addr;\n\n\tINIT_LIST_HEAD(&priv->cm.passive_ids);\n\tINIT_LIST_HEAD(&priv->cm.reap_list);\n\tINIT_LIST_HEAD(&priv->cm.start_list);\n\tINIT_LIST_HEAD(&priv->cm.rx_error_list);\n\tINIT_LIST_HEAD(&priv->cm.rx_flush_list);\n\tINIT_LIST_HEAD(&priv->cm.rx_drain_list);\n\tINIT_LIST_HEAD(&priv->cm.rx_reap_list);\n\tINIT_WORK(&priv->cm.start_task, ipoib_cm_tx_start);\n\tINIT_WORK(&priv->cm.reap_task, ipoib_cm_tx_reap);\n\tINIT_WORK(&priv->cm.skb_task, ipoib_cm_skb_reap);\n\tINIT_WORK(&priv->cm.rx_reap_task, ipoib_cm_rx_reap);\n\tINIT_DELAYED_WORK(&priv->cm.stale_task, ipoib_cm_stale_task);\n\n\tskb_queue_head_init(&priv->cm.skb_queue);\n\n\tipoib_dbg(priv, \"max_srq_sge=%d\\n\", priv->ca->attrs.max_srq_sge);\n\n\tmax_srq_sge = min_t(int, IPOIB_CM_RX_SG, priv->ca->attrs.max_srq_sge);\n\tipoib_cm_create_srq(dev, max_srq_sge);\n\tif (ipoib_cm_has_srq(dev)) {\n\t\tpriv->cm.max_cm_mtu = max_srq_sge * PAGE_SIZE - 0x10;\n\t\tpriv->cm.num_frags  = max_srq_sge;\n\t\tipoib_dbg(priv, \"max_cm_mtu = 0x%x, num_frags=%d\\n\",\n\t\t\t  priv->cm.max_cm_mtu, priv->cm.num_frags);\n\t} else {\n\t\tpriv->cm.max_cm_mtu = IPOIB_CM_MTU;\n\t\tpriv->cm.num_frags  = IPOIB_CM_RX_SG;\n\t}\n\n\tipoib_cm_init_rx_wr(dev, &priv->cm.rx_wr, priv->cm.rx_sge);\n\n\tif (ipoib_cm_has_srq(dev)) {\n\t\tfor (i = 0; i < ipoib_recvq_size; ++i) {\n\t\t\tif (!ipoib_cm_alloc_rx_skb(dev, priv->cm.srq_ring, i,\n\t\t\t\t\t\t   priv->cm.num_frags - 1,\n\t\t\t\t\t\t   priv->cm.srq_ring[i].mapping,\n\t\t\t\t\t\t   GFP_KERNEL)) {\n\t\t\t\tipoib_warn(priv, \"failed to allocate \"\n\t\t\t\t\t   \"receive buffer %d\\n\", i);\n\t\t\t\tipoib_cm_dev_cleanup(dev);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tif (ipoib_cm_post_receive_srq(dev, i)) {\n\t\t\t\tipoib_warn(priv, \"ipoib_cm_post_receive_srq \"\n\t\t\t\t\t   \"failed for buf %d\\n\", i);\n\t\t\t\tipoib_cm_dev_cleanup(dev);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t}\n\n\taddr = IPOIB_FLAGS_RC;\n\tdev_addr_mod(dev, 0, &addr, 1);\n\treturn 0;\n}\n\nvoid ipoib_cm_dev_cleanup(struct net_device *dev)\n{\n\tstruct ipoib_dev_priv *priv = ipoib_priv(dev);\n\n\tif (!priv->cm.srq)\n\t\treturn;\n\n\tipoib_dbg(priv, \"Cleanup ipoib connected mode.\\n\");\n\n\tib_destroy_srq(priv->cm.srq);\n\tpriv->cm.srq = NULL;\n\tif (!priv->cm.srq_ring)\n\t\treturn;\n\n\tipoib_cm_free_rx_ring(dev, priv->cm.srq_ring);\n\tpriv->cm.srq_ring = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}