{
  "module_name": "ib_srp.c",
  "hash_id": "427c1b158313ee0a5843494cc3ac5f1099cb9384f9ff1dc53fc925ce4e4e98be",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/srp/ib_srp.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/string.h>\n#include <linux/parser.h>\n#include <linux/random.h>\n#include <linux/jiffies.h>\n#include <linux/lockdep.h>\n#include <linux/inet.h>\n#include <rdma/ib_cache.h>\n\n#include <linux/atomic.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_tcq.h>\n#include <scsi/srp.h>\n#include <scsi/scsi_transport_srp.h>\n\n#include \"ib_srp.h\"\n\n#define DRV_NAME\t\"ib_srp\"\n#define PFX\t\tDRV_NAME \": \"\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"InfiniBand SCSI RDMA Protocol initiator\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nstatic unsigned int srp_sg_tablesize;\nstatic unsigned int cmd_sg_entries;\nstatic unsigned int indirect_sg_entries;\nstatic bool allow_ext_sg;\nstatic bool register_always = true;\nstatic bool never_register;\nstatic int topspin_workarounds = 1;\n\nmodule_param(srp_sg_tablesize, uint, 0444);\nMODULE_PARM_DESC(srp_sg_tablesize, \"Deprecated name for cmd_sg_entries\");\n\nmodule_param(cmd_sg_entries, uint, 0444);\nMODULE_PARM_DESC(cmd_sg_entries,\n\t\t \"Default number of gather/scatter entries in the SRP command (default is 12, max 255)\");\n\nmodule_param(indirect_sg_entries, uint, 0444);\nMODULE_PARM_DESC(indirect_sg_entries,\n\t\t \"Default max number of gather/scatter entries (default is 12, max is \" __stringify(SG_MAX_SEGMENTS) \")\");\n\nmodule_param(allow_ext_sg, bool, 0444);\nMODULE_PARM_DESC(allow_ext_sg,\n\t\t  \"Default behavior when there are more than cmd_sg_entries S/G entries after mapping; fails the request when false (default false)\");\n\nmodule_param(topspin_workarounds, int, 0444);\nMODULE_PARM_DESC(topspin_workarounds,\n\t\t \"Enable workarounds for Topspin/Cisco SRP target bugs if != 0\");\n\nmodule_param(register_always, bool, 0444);\nMODULE_PARM_DESC(register_always,\n\t\t \"Use memory registration even for contiguous memory regions\");\n\nmodule_param(never_register, bool, 0444);\nMODULE_PARM_DESC(never_register, \"Never register memory\");\n\nstatic const struct kernel_param_ops srp_tmo_ops;\n\nstatic int srp_reconnect_delay = 10;\nmodule_param_cb(reconnect_delay, &srp_tmo_ops, &srp_reconnect_delay,\n\t\tS_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(reconnect_delay, \"Time between successive reconnect attempts\");\n\nstatic int srp_fast_io_fail_tmo = 15;\nmodule_param_cb(fast_io_fail_tmo, &srp_tmo_ops, &srp_fast_io_fail_tmo,\n\t\tS_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(fast_io_fail_tmo,\n\t\t \"Number of seconds between the observation of a transport\"\n\t\t \" layer error and failing all I/O. \\\"off\\\" means that this\"\n\t\t \" functionality is disabled.\");\n\nstatic int srp_dev_loss_tmo = 600;\nmodule_param_cb(dev_loss_tmo, &srp_tmo_ops, &srp_dev_loss_tmo,\n\t\tS_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(dev_loss_tmo,\n\t\t \"Maximum number of seconds that the SRP transport should\"\n\t\t \" insulate transport layer errors. After this time has been\"\n\t\t \" exceeded the SCSI host is removed. Should be\"\n\t\t \" between 1 and \" __stringify(SCSI_DEVICE_BLOCK_MAX_TIMEOUT)\n\t\t \" if fast_io_fail_tmo has not been set. \\\"off\\\" means that\"\n\t\t \" this functionality is disabled.\");\n\nstatic bool srp_use_imm_data = true;\nmodule_param_named(use_imm_data, srp_use_imm_data, bool, 0644);\nMODULE_PARM_DESC(use_imm_data,\n\t\t \"Whether or not to request permission to use immediate data during SRP login.\");\n\nstatic unsigned int srp_max_imm_data = 8 * 1024;\nmodule_param_named(max_imm_data, srp_max_imm_data, uint, 0644);\nMODULE_PARM_DESC(max_imm_data, \"Maximum immediate data size.\");\n\nstatic unsigned ch_count;\nmodule_param(ch_count, uint, 0444);\nMODULE_PARM_DESC(ch_count,\n\t\t \"Number of RDMA channels to use for communication with an SRP target. Using more than one channel improves performance if the HCA supports multiple completion vectors. The default value is the minimum of four times the number of online CPU sockets and the number of completion vectors supported by the HCA.\");\n\nstatic int srp_add_one(struct ib_device *device);\nstatic void srp_remove_one(struct ib_device *device, void *client_data);\nstatic void srp_rename_dev(struct ib_device *device, void *client_data);\nstatic void srp_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void srp_handle_qp_err(struct ib_cq *cq, struct ib_wc *wc,\n\t\tconst char *opname);\nstatic int srp_ib_cm_handler(struct ib_cm_id *cm_id,\n\t\t\t     const struct ib_cm_event *event);\nstatic int srp_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t       struct rdma_cm_event *event);\n\nstatic struct scsi_transport_template *ib_srp_transport_template;\nstatic struct workqueue_struct *srp_remove_wq;\n\nstatic struct ib_client srp_client = {\n\t.name   = \"srp\",\n\t.add    = srp_add_one,\n\t.remove = srp_remove_one,\n\t.rename = srp_rename_dev\n};\n\nstatic struct ib_sa_client srp_sa_client;\n\nstatic int srp_tmo_get(char *buffer, const struct kernel_param *kp)\n{\n\tint tmo = *(int *)kp->arg;\n\n\tif (tmo >= 0)\n\t\treturn sysfs_emit(buffer, \"%d\\n\", tmo);\n\telse\n\t\treturn sysfs_emit(buffer, \"off\\n\");\n}\n\nstatic int srp_tmo_set(const char *val, const struct kernel_param *kp)\n{\n\tint tmo, res;\n\n\tres = srp_parse_tmo(&tmo, val);\n\tif (res)\n\t\tgoto out;\n\n\tif (kp->arg == &srp_reconnect_delay)\n\t\tres = srp_tmo_valid(tmo, srp_fast_io_fail_tmo,\n\t\t\t\t    srp_dev_loss_tmo);\n\telse if (kp->arg == &srp_fast_io_fail_tmo)\n\t\tres = srp_tmo_valid(srp_reconnect_delay, tmo, srp_dev_loss_tmo);\n\telse\n\t\tres = srp_tmo_valid(srp_reconnect_delay, srp_fast_io_fail_tmo,\n\t\t\t\t    tmo);\n\tif (res)\n\t\tgoto out;\n\t*(int *)kp->arg = tmo;\n\nout:\n\treturn res;\n}\n\nstatic const struct kernel_param_ops srp_tmo_ops = {\n\t.get = srp_tmo_get,\n\t.set = srp_tmo_set,\n};\n\nstatic inline struct srp_target_port *host_to_target(struct Scsi_Host *host)\n{\n\treturn (struct srp_target_port *) host->hostdata;\n}\n\nstatic const char *srp_target_info(struct Scsi_Host *host)\n{\n\treturn host_to_target(host)->target_name;\n}\n\nstatic int srp_target_is_topspin(struct srp_target_port *target)\n{\n\tstatic const u8 topspin_oui[3] = { 0x00, 0x05, 0xad };\n\tstatic const u8 cisco_oui[3]   = { 0x00, 0x1b, 0x0d };\n\n\treturn topspin_workarounds &&\n\t\t(!memcmp(&target->ioc_guid, topspin_oui, sizeof topspin_oui) ||\n\t\t !memcmp(&target->ioc_guid, cisco_oui, sizeof cisco_oui));\n}\n\nstatic struct srp_iu *srp_alloc_iu(struct srp_host *host, size_t size,\n\t\t\t\t   gfp_t gfp_mask,\n\t\t\t\t   enum dma_data_direction direction)\n{\n\tstruct srp_iu *iu;\n\n\tiu = kmalloc(sizeof *iu, gfp_mask);\n\tif (!iu)\n\t\tgoto out;\n\n\tiu->buf = kzalloc(size, gfp_mask);\n\tif (!iu->buf)\n\t\tgoto out_free_iu;\n\n\tiu->dma = ib_dma_map_single(host->srp_dev->dev, iu->buf, size,\n\t\t\t\t    direction);\n\tif (ib_dma_mapping_error(host->srp_dev->dev, iu->dma))\n\t\tgoto out_free_buf;\n\n\tiu->size      = size;\n\tiu->direction = direction;\n\n\treturn iu;\n\nout_free_buf:\n\tkfree(iu->buf);\nout_free_iu:\n\tkfree(iu);\nout:\n\treturn NULL;\n}\n\nstatic void srp_free_iu(struct srp_host *host, struct srp_iu *iu)\n{\n\tif (!iu)\n\t\treturn;\n\n\tib_dma_unmap_single(host->srp_dev->dev, iu->dma, iu->size,\n\t\t\t    iu->direction);\n\tkfree(iu->buf);\n\tkfree(iu);\n}\n\nstatic void srp_qp_event(struct ib_event *event, void *context)\n{\n\tpr_debug(\"QP event %s (%d)\\n\",\n\t\t ib_event_msg(event->event), event->event);\n}\n\nstatic int srp_init_ib_qp(struct srp_target_port *target,\n\t\t\t  struct ib_qp *qp)\n{\n\tstruct ib_qp_attr *attr;\n\tint ret;\n\n\tattr = kmalloc(sizeof *attr, GFP_KERNEL);\n\tif (!attr)\n\t\treturn -ENOMEM;\n\n\tret = ib_find_cached_pkey(target->srp_host->srp_dev->dev,\n\t\t\t\t  target->srp_host->port,\n\t\t\t\t  be16_to_cpu(target->ib_cm.pkey),\n\t\t\t\t  &attr->pkey_index);\n\tif (ret)\n\t\tgoto out;\n\n\tattr->qp_state        = IB_QPS_INIT;\n\tattr->qp_access_flags = (IB_ACCESS_REMOTE_READ |\n\t\t\t\t    IB_ACCESS_REMOTE_WRITE);\n\tattr->port_num        = target->srp_host->port;\n\n\tret = ib_modify_qp(qp, attr,\n\t\t\t   IB_QP_STATE\t\t|\n\t\t\t   IB_QP_PKEY_INDEX\t|\n\t\t\t   IB_QP_ACCESS_FLAGS\t|\n\t\t\t   IB_QP_PORT);\n\nout:\n\tkfree(attr);\n\treturn ret;\n}\n\nstatic int srp_new_ib_cm_id(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_cm_id *new_cm_id;\n\n\tnew_cm_id = ib_create_cm_id(target->srp_host->srp_dev->dev,\n\t\t\t\t    srp_ib_cm_handler, ch);\n\tif (IS_ERR(new_cm_id))\n\t\treturn PTR_ERR(new_cm_id);\n\n\tif (ch->ib_cm.cm_id)\n\t\tib_destroy_cm_id(ch->ib_cm.cm_id);\n\tch->ib_cm.cm_id = new_cm_id;\n\tif (rdma_cap_opa_ah(target->srp_host->srp_dev->dev,\n\t\t\t    target->srp_host->port))\n\t\tch->ib_cm.path.rec_type = SA_PATH_REC_TYPE_OPA;\n\telse\n\t\tch->ib_cm.path.rec_type = SA_PATH_REC_TYPE_IB;\n\tch->ib_cm.path.sgid = target->sgid;\n\tch->ib_cm.path.dgid = target->ib_cm.orig_dgid;\n\tch->ib_cm.path.pkey = target->ib_cm.pkey;\n\tch->ib_cm.path.service_id = target->ib_cm.service_id;\n\n\treturn 0;\n}\n\nstatic int srp_new_rdma_cm_id(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct rdma_cm_id *new_cm_id;\n\tint ret;\n\n\tnew_cm_id = rdma_create_id(target->net, srp_rdma_cm_handler, ch,\n\t\t\t\t   RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(new_cm_id)) {\n\t\tret = PTR_ERR(new_cm_id);\n\t\tnew_cm_id = NULL;\n\t\tgoto out;\n\t}\n\n\tinit_completion(&ch->done);\n\tret = rdma_resolve_addr(new_cm_id, target->rdma_cm.src_specified ?\n\t\t\t\t&target->rdma_cm.src.sa : NULL,\n\t\t\t\t&target->rdma_cm.dst.sa,\n\t\t\t\tSRP_PATH_REC_TIMEOUT_MS);\n\tif (ret) {\n\t\tpr_err(\"No route available from %pISpsc to %pISpsc (%d)\\n\",\n\t\t       &target->rdma_cm.src, &target->rdma_cm.dst, ret);\n\t\tgoto out;\n\t}\n\tret = wait_for_completion_interruptible(&ch->done);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = ch->status;\n\tif (ret) {\n\t\tpr_err(\"Resolving address %pISpsc failed (%d)\\n\",\n\t\t       &target->rdma_cm.dst, ret);\n\t\tgoto out;\n\t}\n\n\tswap(ch->rdma_cm.cm_id, new_cm_id);\n\nout:\n\tif (new_cm_id)\n\t\trdma_destroy_id(new_cm_id);\n\n\treturn ret;\n}\n\nstatic int srp_new_cm_id(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\n\treturn target->using_rdma_cm ? srp_new_rdma_cm_id(ch) :\n\t\tsrp_new_ib_cm_id(ch);\n}\n\n \nstatic void srp_destroy_fr_pool(struct srp_fr_pool *pool)\n{\n\tint i;\n\tstruct srp_fr_desc *d;\n\n\tif (!pool)\n\t\treturn;\n\n\tfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\n\t\tif (d->mr)\n\t\t\tib_dereg_mr(d->mr);\n\t}\n\tkfree(pool);\n}\n\n \nstatic struct srp_fr_pool *srp_create_fr_pool(struct ib_device *device,\n\t\t\t\t\t      struct ib_pd *pd, int pool_size,\n\t\t\t\t\t      int max_page_list_len)\n{\n\tstruct srp_fr_pool *pool;\n\tstruct srp_fr_desc *d;\n\tstruct ib_mr *mr;\n\tint i, ret = -EINVAL;\n\tenum ib_mr_type mr_type;\n\n\tif (pool_size <= 0)\n\t\tgoto err;\n\tret = -ENOMEM;\n\tpool = kzalloc(struct_size(pool, desc, pool_size), GFP_KERNEL);\n\tif (!pool)\n\t\tgoto err;\n\tpool->size = pool_size;\n\tpool->max_page_list_len = max_page_list_len;\n\tspin_lock_init(&pool->lock);\n\tINIT_LIST_HEAD(&pool->free_list);\n\n\tif (device->attrs.kernel_cap_flags & IBK_SG_GAPS_REG)\n\t\tmr_type = IB_MR_TYPE_SG_GAPS;\n\telse\n\t\tmr_type = IB_MR_TYPE_MEM_REG;\n\n\tfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\n\t\tmr = ib_alloc_mr(pd, mr_type, max_page_list_len);\n\t\tif (IS_ERR(mr)) {\n\t\t\tret = PTR_ERR(mr);\n\t\t\tif (ret == -ENOMEM)\n\t\t\t\tpr_info(\"%s: ib_alloc_mr() failed. Try to reduce max_cmd_per_lun, max_sect or ch_count\\n\",\n\t\t\t\t\tdev_name(&device->dev));\n\t\t\tgoto destroy_pool;\n\t\t}\n\t\td->mr = mr;\n\t\tlist_add_tail(&d->entry, &pool->free_list);\n\t}\n\nout:\n\treturn pool;\n\ndestroy_pool:\n\tsrp_destroy_fr_pool(pool);\n\nerr:\n\tpool = ERR_PTR(ret);\n\tgoto out;\n}\n\n \nstatic struct srp_fr_desc *srp_fr_pool_get(struct srp_fr_pool *pool)\n{\n\tstruct srp_fr_desc *d = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tif (!list_empty(&pool->free_list)) {\n\t\td = list_first_entry(&pool->free_list, typeof(*d), entry);\n\t\tlist_del(&d->entry);\n\t}\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn d;\n}\n\n \nstatic void srp_fr_pool_put(struct srp_fr_pool *pool, struct srp_fr_desc **desc,\n\t\t\t    int n)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tfor (i = 0; i < n; i++)\n\t\tlist_add(&desc[i]->entry, &pool->free_list);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n}\n\nstatic struct srp_fr_pool *srp_alloc_fr_pool(struct srp_target_port *target)\n{\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\n\treturn srp_create_fr_pool(dev->dev, dev->pd, target->mr_pool_size,\n\t\t\t\t  dev->max_pages_per_mr);\n}\n\n \nstatic void srp_destroy_qp(struct srp_rdma_ch *ch)\n{\n\tspin_lock_irq(&ch->lock);\n\tib_process_cq_direct(ch->send_cq, -1);\n\tspin_unlock_irq(&ch->lock);\n\n\tib_drain_qp(ch->qp);\n\tib_destroy_qp(ch->qp);\n}\n\nstatic int srp_create_ch_ib(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tconst struct ib_device_attr *attr = &dev->dev->attrs;\n\tstruct ib_qp_init_attr *init_attr;\n\tstruct ib_cq *recv_cq, *send_cq;\n\tstruct ib_qp *qp;\n\tstruct srp_fr_pool *fr_pool = NULL;\n\tconst int m = 1 + dev->use_fast_reg * target->mr_per_cmd * 2;\n\tint ret;\n\n\tinit_attr = kzalloc(sizeof *init_attr, GFP_KERNEL);\n\tif (!init_attr)\n\t\treturn -ENOMEM;\n\n\t \n\trecv_cq = ib_alloc_cq(dev->dev, ch, target->queue_size + 1,\n\t\t\t\tch->comp_vector, IB_POLL_SOFTIRQ);\n\tif (IS_ERR(recv_cq)) {\n\t\tret = PTR_ERR(recv_cq);\n\t\tgoto err;\n\t}\n\n\tsend_cq = ib_alloc_cq(dev->dev, ch, m * target->queue_size,\n\t\t\t\tch->comp_vector, IB_POLL_DIRECT);\n\tif (IS_ERR(send_cq)) {\n\t\tret = PTR_ERR(send_cq);\n\t\tgoto err_recv_cq;\n\t}\n\n\tinit_attr->event_handler       = srp_qp_event;\n\tinit_attr->cap.max_send_wr     = m * target->queue_size;\n\tinit_attr->cap.max_recv_wr     = target->queue_size + 1;\n\tinit_attr->cap.max_recv_sge    = 1;\n\tinit_attr->cap.max_send_sge    = min(SRP_MAX_SGE, attr->max_send_sge);\n\tinit_attr->sq_sig_type         = IB_SIGNAL_REQ_WR;\n\tinit_attr->qp_type             = IB_QPT_RC;\n\tinit_attr->send_cq             = send_cq;\n\tinit_attr->recv_cq             = recv_cq;\n\n\tch->max_imm_sge = min(init_attr->cap.max_send_sge - 1U, 255U);\n\n\tif (target->using_rdma_cm) {\n\t\tret = rdma_create_qp(ch->rdma_cm.cm_id, dev->pd, init_attr);\n\t\tqp = ch->rdma_cm.cm_id->qp;\n\t} else {\n\t\tqp = ib_create_qp(dev->pd, init_attr);\n\t\tif (!IS_ERR(qp)) {\n\t\t\tret = srp_init_ib_qp(target, qp);\n\t\t\tif (ret)\n\t\t\t\tib_destroy_qp(qp);\n\t\t} else {\n\t\t\tret = PTR_ERR(qp);\n\t\t}\n\t}\n\tif (ret) {\n\t\tpr_err(\"QP creation failed for dev %s: %d\\n\",\n\t\t       dev_name(&dev->dev->dev), ret);\n\t\tgoto err_send_cq;\n\t}\n\n\tif (dev->use_fast_reg) {\n\t\tfr_pool = srp_alloc_fr_pool(target);\n\t\tif (IS_ERR(fr_pool)) {\n\t\t\tret = PTR_ERR(fr_pool);\n\t\t\tshost_printk(KERN_WARNING, target->scsi_host, PFX\n\t\t\t\t     \"FR pool allocation failed (%d)\\n\", ret);\n\t\t\tgoto err_qp;\n\t\t}\n\t}\n\n\tif (ch->qp)\n\t\tsrp_destroy_qp(ch);\n\tif (ch->recv_cq)\n\t\tib_free_cq(ch->recv_cq);\n\tif (ch->send_cq)\n\t\tib_free_cq(ch->send_cq);\n\n\tch->qp = qp;\n\tch->recv_cq = recv_cq;\n\tch->send_cq = send_cq;\n\n\tif (dev->use_fast_reg) {\n\t\tif (ch->fr_pool)\n\t\t\tsrp_destroy_fr_pool(ch->fr_pool);\n\t\tch->fr_pool = fr_pool;\n\t}\n\n\tkfree(init_attr);\n\treturn 0;\n\nerr_qp:\n\tif (target->using_rdma_cm)\n\t\trdma_destroy_qp(ch->rdma_cm.cm_id);\n\telse\n\t\tib_destroy_qp(qp);\n\nerr_send_cq:\n\tib_free_cq(send_cq);\n\nerr_recv_cq:\n\tib_free_cq(recv_cq);\n\nerr:\n\tkfree(init_attr);\n\treturn ret;\n}\n\n \nstatic void srp_free_ch_ib(struct srp_target_port *target,\n\t\t\t   struct srp_rdma_ch *ch)\n{\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tint i;\n\n\tif (!ch->target)\n\t\treturn;\n\n\tif (target->using_rdma_cm) {\n\t\tif (ch->rdma_cm.cm_id) {\n\t\t\trdma_destroy_id(ch->rdma_cm.cm_id);\n\t\t\tch->rdma_cm.cm_id = NULL;\n\t\t}\n\t} else {\n\t\tif (ch->ib_cm.cm_id) {\n\t\t\tib_destroy_cm_id(ch->ib_cm.cm_id);\n\t\t\tch->ib_cm.cm_id = NULL;\n\t\t}\n\t}\n\n\t \n\tif (!ch->qp)\n\t\treturn;\n\n\tif (dev->use_fast_reg) {\n\t\tif (ch->fr_pool)\n\t\t\tsrp_destroy_fr_pool(ch->fr_pool);\n\t}\n\n\tsrp_destroy_qp(ch);\n\tib_free_cq(ch->send_cq);\n\tib_free_cq(ch->recv_cq);\n\n\t \n\tch->target = NULL;\n\n\tch->qp = NULL;\n\tch->send_cq = ch->recv_cq = NULL;\n\n\tif (ch->rx_ring) {\n\t\tfor (i = 0; i < target->queue_size; ++i)\n\t\t\tsrp_free_iu(target->srp_host, ch->rx_ring[i]);\n\t\tkfree(ch->rx_ring);\n\t\tch->rx_ring = NULL;\n\t}\n\tif (ch->tx_ring) {\n\t\tfor (i = 0; i < target->queue_size; ++i)\n\t\t\tsrp_free_iu(target->srp_host, ch->tx_ring[i]);\n\t\tkfree(ch->tx_ring);\n\t\tch->tx_ring = NULL;\n\t}\n}\n\nstatic void srp_path_rec_completion(int status,\n\t\t\t\t    struct sa_path_rec *pathrec,\n\t\t\t\t    unsigned int num_paths, void *ch_ptr)\n{\n\tstruct srp_rdma_ch *ch = ch_ptr;\n\tstruct srp_target_port *target = ch->target;\n\n\tch->status = status;\n\tif (status)\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"Got failed path rec status %d\\n\", status);\n\telse\n\t\tch->ib_cm.path = *pathrec;\n\tcomplete(&ch->done);\n}\n\nstatic int srp_ib_lookup_path(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tint ret;\n\n\tch->ib_cm.path.numb_path = 1;\n\n\tinit_completion(&ch->done);\n\n\tch->ib_cm.path_query_id = ib_sa_path_rec_get(&srp_sa_client,\n\t\t\t\t\t       target->srp_host->srp_dev->dev,\n\t\t\t\t\t       target->srp_host->port,\n\t\t\t\t\t       &ch->ib_cm.path,\n\t\t\t\t\t       IB_SA_PATH_REC_SERVICE_ID |\n\t\t\t\t\t       IB_SA_PATH_REC_DGID\t |\n\t\t\t\t\t       IB_SA_PATH_REC_SGID\t |\n\t\t\t\t\t       IB_SA_PATH_REC_NUMB_PATH\t |\n\t\t\t\t\t       IB_SA_PATH_REC_PKEY,\n\t\t\t\t\t       SRP_PATH_REC_TIMEOUT_MS,\n\t\t\t\t\t       GFP_KERNEL,\n\t\t\t\t\t       srp_path_rec_completion,\n\t\t\t\t\t       ch, &ch->ib_cm.path_query);\n\tif (ch->ib_cm.path_query_id < 0)\n\t\treturn ch->ib_cm.path_query_id;\n\n\tret = wait_for_completion_interruptible(&ch->done);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ch->status < 0)\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Path record query failed: sgid %pI6, dgid %pI6, pkey %#04x, service_id %#16llx\\n\",\n\t\t\t     ch->ib_cm.path.sgid.raw, ch->ib_cm.path.dgid.raw,\n\t\t\t     be16_to_cpu(target->ib_cm.pkey),\n\t\t\t     be64_to_cpu(target->ib_cm.service_id));\n\n\treturn ch->status;\n}\n\nstatic int srp_rdma_lookup_path(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tint ret;\n\n\tinit_completion(&ch->done);\n\n\tret = rdma_resolve_route(ch->rdma_cm.cm_id, SRP_PATH_REC_TIMEOUT_MS);\n\tif (ret)\n\t\treturn ret;\n\n\twait_for_completion_interruptible(&ch->done);\n\n\tif (ch->status != 0)\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Path resolution failed\\n\");\n\n\treturn ch->status;\n}\n\nstatic int srp_lookup_path(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\n\treturn target->using_rdma_cm ? srp_rdma_lookup_path(ch) :\n\t\tsrp_ib_lookup_path(ch);\n}\n\nstatic u8 srp_get_subnet_timeout(struct srp_host *host)\n{\n\tstruct ib_port_attr attr;\n\tint ret;\n\tu8 subnet_timeout = 18;\n\n\tret = ib_query_port(host->srp_dev->dev, host->port, &attr);\n\tif (ret == 0)\n\t\tsubnet_timeout = attr.subnet_timeout;\n\n\tif (unlikely(subnet_timeout < 15))\n\t\tpr_warn(\"%s: subnet timeout %d may cause SRP login to fail.\\n\",\n\t\t\tdev_name(&host->srp_dev->dev->dev), subnet_timeout);\n\n\treturn subnet_timeout;\n}\n\nstatic int srp_send_req(struct srp_rdma_ch *ch, uint32_t max_iu_len,\n\t\t\tbool multich)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct {\n\t\tstruct rdma_conn_param\t  rdma_param;\n\t\tstruct srp_login_req_rdma rdma_req;\n\t\tstruct ib_cm_req_param\t  ib_param;\n\t\tstruct srp_login_req\t  ib_req;\n\t} *req = NULL;\n\tchar *ipi, *tpi;\n\tint status;\n\n\treq = kzalloc(sizeof *req, GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->ib_param.flow_control = 1;\n\treq->ib_param.retry_count = target->tl_retry_count;\n\n\t \n\treq->ib_param.responder_resources = 4;\n\treq->ib_param.rnr_retry_count = 7;\n\treq->ib_param.max_cm_retries = 15;\n\n\treq->ib_req.opcode = SRP_LOGIN_REQ;\n\treq->ib_req.tag = 0;\n\treq->ib_req.req_it_iu_len = cpu_to_be32(max_iu_len);\n\treq->ib_req.req_buf_fmt\t= cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\n\t\t\t\t\t      SRP_BUF_FORMAT_INDIRECT);\n\treq->ib_req.req_flags = (multich ? SRP_MULTICHAN_MULTI :\n\t\t\t\t SRP_MULTICHAN_SINGLE);\n\tif (srp_use_imm_data) {\n\t\treq->ib_req.req_flags |= SRP_IMMED_REQUESTED;\n\t\treq->ib_req.imm_data_offset = cpu_to_be16(SRP_IMM_DATA_OFFSET);\n\t}\n\n\tif (target->using_rdma_cm) {\n\t\treq->rdma_param.flow_control = req->ib_param.flow_control;\n\t\treq->rdma_param.responder_resources =\n\t\t\treq->ib_param.responder_resources;\n\t\treq->rdma_param.initiator_depth = req->ib_param.initiator_depth;\n\t\treq->rdma_param.retry_count = req->ib_param.retry_count;\n\t\treq->rdma_param.rnr_retry_count = req->ib_param.rnr_retry_count;\n\t\treq->rdma_param.private_data = &req->rdma_req;\n\t\treq->rdma_param.private_data_len = sizeof(req->rdma_req);\n\n\t\treq->rdma_req.opcode = req->ib_req.opcode;\n\t\treq->rdma_req.tag = req->ib_req.tag;\n\t\treq->rdma_req.req_it_iu_len = req->ib_req.req_it_iu_len;\n\t\treq->rdma_req.req_buf_fmt = req->ib_req.req_buf_fmt;\n\t\treq->rdma_req.req_flags\t= req->ib_req.req_flags;\n\t\treq->rdma_req.imm_data_offset = req->ib_req.imm_data_offset;\n\n\t\tipi = req->rdma_req.initiator_port_id;\n\t\ttpi = req->rdma_req.target_port_id;\n\t} else {\n\t\tu8 subnet_timeout;\n\n\t\tsubnet_timeout = srp_get_subnet_timeout(target->srp_host);\n\n\t\treq->ib_param.primary_path = &ch->ib_cm.path;\n\t\treq->ib_param.alternate_path = NULL;\n\t\treq->ib_param.service_id = target->ib_cm.service_id;\n\t\tget_random_bytes(&req->ib_param.starting_psn, 4);\n\t\treq->ib_param.starting_psn &= 0xffffff;\n\t\treq->ib_param.qp_num = ch->qp->qp_num;\n\t\treq->ib_param.qp_type = ch->qp->qp_type;\n\t\treq->ib_param.local_cm_response_timeout = subnet_timeout + 2;\n\t\treq->ib_param.remote_cm_response_timeout = subnet_timeout + 2;\n\t\treq->ib_param.private_data = &req->ib_req;\n\t\treq->ib_param.private_data_len = sizeof(req->ib_req);\n\n\t\tipi = req->ib_req.initiator_port_id;\n\t\ttpi = req->ib_req.target_port_id;\n\t}\n\n\t \n\tif (target->io_class == SRP_REV10_IB_IO_CLASS) {\n\t\tmemcpy(ipi,     &target->sgid.global.interface_id, 8);\n\t\tmemcpy(ipi + 8, &target->initiator_ext, 8);\n\t\tmemcpy(tpi,     &target->ioc_guid, 8);\n\t\tmemcpy(tpi + 8, &target->id_ext, 8);\n\t} else {\n\t\tmemcpy(ipi,     &target->initiator_ext, 8);\n\t\tmemcpy(ipi + 8, &target->sgid.global.interface_id, 8);\n\t\tmemcpy(tpi,     &target->id_ext, 8);\n\t\tmemcpy(tpi + 8, &target->ioc_guid, 8);\n\t}\n\n\t \n\tif (srp_target_is_topspin(target)) {\n\t\tshost_printk(KERN_DEBUG, target->scsi_host,\n\t\t\t     PFX \"Topspin/Cisco initiator port ID workaround \"\n\t\t\t     \"activated for target GUID %016llx\\n\",\n\t\t\t     be64_to_cpu(target->ioc_guid));\n\t\tmemset(ipi, 0, 8);\n\t\tmemcpy(ipi + 8, &target->srp_host->srp_dev->dev->node_guid, 8);\n\t}\n\n\tif (target->using_rdma_cm)\n\t\tstatus = rdma_connect(ch->rdma_cm.cm_id, &req->rdma_param);\n\telse\n\t\tstatus = ib_send_cm_req(ch->ib_cm.cm_id, &req->ib_param);\n\n\tkfree(req);\n\n\treturn status;\n}\n\nstatic bool srp_queue_remove_work(struct srp_target_port *target)\n{\n\tbool changed = false;\n\n\tspin_lock_irq(&target->lock);\n\tif (target->state != SRP_TARGET_REMOVED) {\n\t\ttarget->state = SRP_TARGET_REMOVED;\n\t\tchanged = true;\n\t}\n\tspin_unlock_irq(&target->lock);\n\n\tif (changed)\n\t\tqueue_work(srp_remove_wq, &target->remove_work);\n\n\treturn changed;\n}\n\nstatic void srp_disconnect_target(struct srp_target_port *target)\n{\n\tstruct srp_rdma_ch *ch;\n\tint i, ret;\n\n\t \n\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\tch->connected = false;\n\t\tret = 0;\n\t\tif (target->using_rdma_cm) {\n\t\t\tif (ch->rdma_cm.cm_id)\n\t\t\t\trdma_disconnect(ch->rdma_cm.cm_id);\n\t\t} else {\n\t\t\tif (ch->ib_cm.cm_id)\n\t\t\t\tret = ib_send_cm_dreq(ch->ib_cm.cm_id,\n\t\t\t\t\t\t      NULL, 0);\n\t\t}\n\t\tif (ret < 0) {\n\t\t\tshost_printk(KERN_DEBUG, target->scsi_host,\n\t\t\t\t     PFX \"Sending CM DREQ failed\\n\");\n\t\t}\n\t}\n}\n\nstatic int srp_exit_cmd_priv(struct Scsi_Host *shost, struct scsi_cmnd *cmd)\n{\n\tstruct srp_target_port *target = host_to_target(shost);\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tstruct ib_device *ibdev = dev->dev;\n\tstruct srp_request *req = scsi_cmd_priv(cmd);\n\n\tkfree(req->fr_list);\n\tif (req->indirect_dma_addr) {\n\t\tib_dma_unmap_single(ibdev, req->indirect_dma_addr,\n\t\t\t\t    target->indirect_size,\n\t\t\t\t    DMA_TO_DEVICE);\n\t}\n\tkfree(req->indirect_desc);\n\n\treturn 0;\n}\n\nstatic int srp_init_cmd_priv(struct Scsi_Host *shost, struct scsi_cmnd *cmd)\n{\n\tstruct srp_target_port *target = host_to_target(shost);\n\tstruct srp_device *srp_dev = target->srp_host->srp_dev;\n\tstruct ib_device *ibdev = srp_dev->dev;\n\tstruct srp_request *req = scsi_cmd_priv(cmd);\n\tdma_addr_t dma_addr;\n\tint ret = -ENOMEM;\n\n\tif (srp_dev->use_fast_reg) {\n\t\treq->fr_list = kmalloc_array(target->mr_per_cmd, sizeof(void *),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!req->fr_list)\n\t\t\tgoto out;\n\t}\n\treq->indirect_desc = kmalloc(target->indirect_size, GFP_KERNEL);\n\tif (!req->indirect_desc)\n\t\tgoto out;\n\n\tdma_addr = ib_dma_map_single(ibdev, req->indirect_desc,\n\t\t\t\t     target->indirect_size,\n\t\t\t\t     DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(ibdev, dma_addr)) {\n\t\tsrp_exit_cmd_priv(shost, cmd);\n\t\tgoto out;\n\t}\n\n\treq->indirect_dma_addr = dma_addr;\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\n \nstatic void srp_del_scsi_host_attr(struct Scsi_Host *shost)\n{\n\tconst struct attribute_group **g;\n\tstruct attribute **attr;\n\n\tfor (g = shost->hostt->shost_groups; *g; ++g) {\n\t\tfor (attr = (*g)->attrs; *attr; ++attr) {\n\t\t\tstruct device_attribute *dev_attr =\n\t\t\t\tcontainer_of(*attr, typeof(*dev_attr), attr);\n\n\t\t\tdevice_remove_file(&shost->shost_dev, dev_attr);\n\t\t}\n\t}\n}\n\nstatic void srp_remove_target(struct srp_target_port *target)\n{\n\tstruct srp_rdma_ch *ch;\n\tint i;\n\n\tWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\n\n\tsrp_del_scsi_host_attr(target->scsi_host);\n\tsrp_rport_get(target->rport);\n\tsrp_remove_host(target->scsi_host);\n\tscsi_remove_host(target->scsi_host);\n\tsrp_stop_rport_timers(target->rport);\n\tsrp_disconnect_target(target);\n\tkobj_ns_drop(KOBJ_NS_TYPE_NET, target->net);\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\tsrp_free_ch_ib(target, ch);\n\t}\n\tcancel_work_sync(&target->tl_err_work);\n\tsrp_rport_put(target->rport);\n\tkfree(target->ch);\n\ttarget->ch = NULL;\n\n\tspin_lock(&target->srp_host->target_lock);\n\tlist_del(&target->list);\n\tspin_unlock(&target->srp_host->target_lock);\n\n\tscsi_host_put(target->scsi_host);\n}\n\nstatic void srp_remove_work(struct work_struct *work)\n{\n\tstruct srp_target_port *target =\n\t\tcontainer_of(work, struct srp_target_port, remove_work);\n\n\tWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\n\n\tsrp_remove_target(target);\n}\n\nstatic void srp_rport_delete(struct srp_rport *rport)\n{\n\tstruct srp_target_port *target = rport->lld_data;\n\n\tsrp_queue_remove_work(target);\n}\n\n \nstatic int srp_connected_ch(struct srp_target_port *target)\n{\n\tint i, c = 0;\n\n\tfor (i = 0; i < target->ch_count; i++)\n\t\tc += target->ch[i].connected;\n\n\treturn c;\n}\n\nstatic int srp_connect_ch(struct srp_rdma_ch *ch, uint32_t max_iu_len,\n\t\t\t  bool multich)\n{\n\tstruct srp_target_port *target = ch->target;\n\tint ret;\n\n\tWARN_ON_ONCE(!multich && srp_connected_ch(target) > 0);\n\n\tret = srp_lookup_path(ch);\n\tif (ret)\n\t\tgoto out;\n\n\twhile (1) {\n\t\tinit_completion(&ch->done);\n\t\tret = srp_send_req(ch, max_iu_len, multich);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = wait_for_completion_interruptible(&ch->done);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\t \n\t\tret = ch->status;\n\t\tswitch (ret) {\n\t\tcase 0:\n\t\t\tch->connected = true;\n\t\t\tgoto out;\n\n\t\tcase SRP_PORT_REDIRECT:\n\t\t\tret = srp_lookup_path(ch);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase SRP_DLID_REDIRECT:\n\t\t\tbreak;\n\n\t\tcase SRP_STALE_CONN:\n\t\t\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t\t\t     \"giving up on stale connection\\n\");\n\t\t\tret = -ECONNRESET;\n\t\t\tgoto out;\n\n\t\tdefault:\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn ret <= 0 ? ret : -ENODEV;\n}\n\nstatic void srp_inv_rkey_err_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsrp_handle_qp_err(cq, wc, \"INV RKEY\");\n}\n\nstatic int srp_inv_rkey(struct srp_request *req, struct srp_rdma_ch *ch,\n\t\tu32 rkey)\n{\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.next\t\t    = NULL,\n\t\t.num_sge\t    = 0,\n\t\t.send_flags\t    = 0,\n\t\t.ex.invalidate_rkey = rkey,\n\t};\n\n\twr.wr_cqe = &req->reg_cqe;\n\treq->reg_cqe.done = srp_inv_rkey_err_done;\n\treturn ib_post_send(ch->qp, &wr, NULL);\n}\n\nstatic void srp_unmap_data(struct scsi_cmnd *scmnd,\n\t\t\t   struct srp_rdma_ch *ch,\n\t\t\t   struct srp_request *req)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tstruct ib_device *ibdev = dev->dev;\n\tint i, res;\n\n\tif (!scsi_sglist(scmnd) ||\n\t    (scmnd->sc_data_direction != DMA_TO_DEVICE &&\n\t     scmnd->sc_data_direction != DMA_FROM_DEVICE))\n\t\treturn;\n\n\tif (dev->use_fast_reg) {\n\t\tstruct srp_fr_desc **pfr;\n\n\t\tfor (i = req->nmdesc, pfr = req->fr_list; i > 0; i--, pfr++) {\n\t\t\tres = srp_inv_rkey(req, ch, (*pfr)->mr->rkey);\n\t\t\tif (res < 0) {\n\t\t\t\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t\t\t  \"Queueing INV WR for rkey %#x failed (%d)\\n\",\n\t\t\t\t  (*pfr)->mr->rkey, res);\n\t\t\t\tqueue_work(system_long_wq,\n\t\t\t\t\t   &target->tl_err_work);\n\t\t\t}\n\t\t}\n\t\tif (req->nmdesc)\n\t\t\tsrp_fr_pool_put(ch->fr_pool, req->fr_list,\n\t\t\t\t\treq->nmdesc);\n\t}\n\n\tib_dma_unmap_sg(ibdev, scsi_sglist(scmnd), scsi_sg_count(scmnd),\n\t\t\tscmnd->sc_data_direction);\n}\n\n \nstatic struct scsi_cmnd *srp_claim_req(struct srp_rdma_ch *ch,\n\t\t\t\t       struct srp_request *req,\n\t\t\t\t       struct scsi_device *sdev,\n\t\t\t\t       struct scsi_cmnd *scmnd)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tif (req->scmnd &&\n\t    (!sdev || req->scmnd->device == sdev) &&\n\t    (!scmnd || req->scmnd == scmnd)) {\n\t\tscmnd = req->scmnd;\n\t\treq->scmnd = NULL;\n\t} else {\n\t\tscmnd = NULL;\n\t}\n\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\treturn scmnd;\n}\n\n \nstatic void srp_free_req(struct srp_rdma_ch *ch, struct srp_request *req,\n\t\t\t struct scsi_cmnd *scmnd, s32 req_lim_delta)\n{\n\tunsigned long flags;\n\n\tsrp_unmap_data(scmnd, ch, req);\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tch->req_lim += req_lim_delta;\n\tspin_unlock_irqrestore(&ch->lock, flags);\n}\n\nstatic void srp_finish_req(struct srp_rdma_ch *ch, struct srp_request *req,\n\t\t\t   struct scsi_device *sdev, int result)\n{\n\tstruct scsi_cmnd *scmnd = srp_claim_req(ch, req, sdev, NULL);\n\n\tif (scmnd) {\n\t\tsrp_free_req(ch, req, scmnd, 0);\n\t\tscmnd->result = result;\n\t\tscsi_done(scmnd);\n\t}\n}\n\nstruct srp_terminate_context {\n\tstruct srp_target_port *srp_target;\n\tint scsi_result;\n};\n\nstatic bool srp_terminate_cmd(struct scsi_cmnd *scmnd, void *context_ptr)\n{\n\tstruct srp_terminate_context *context = context_ptr;\n\tstruct srp_target_port *target = context->srp_target;\n\tu32 tag = blk_mq_unique_tag(scsi_cmd_to_rq(scmnd));\n\tstruct srp_rdma_ch *ch = &target->ch[blk_mq_unique_tag_to_hwq(tag)];\n\tstruct srp_request *req = scsi_cmd_priv(scmnd);\n\n\tsrp_finish_req(ch, req, NULL, context->scsi_result);\n\n\treturn true;\n}\n\nstatic void srp_terminate_io(struct srp_rport *rport)\n{\n\tstruct srp_target_port *target = rport->lld_data;\n\tstruct srp_terminate_context context = { .srp_target = target,\n\t\t.scsi_result = DID_TRANSPORT_FAILFAST << 16 };\n\n\tscsi_host_busy_iter(target->scsi_host, srp_terminate_cmd, &context);\n}\n\n \nstatic uint32_t srp_max_it_iu_len(int cmd_sg_cnt, bool use_imm_data,\n\t\t\t\t  uint32_t max_it_iu_size)\n{\n\tuint32_t max_iu_len = sizeof(struct srp_cmd) + SRP_MAX_ADD_CDB_LEN +\n\t\tsizeof(struct srp_indirect_buf) +\n\t\tcmd_sg_cnt * sizeof(struct srp_direct_buf);\n\n\tif (use_imm_data)\n\t\tmax_iu_len = max(max_iu_len, SRP_IMM_DATA_OFFSET +\n\t\t\t\t srp_max_imm_data);\n\n\tif (max_it_iu_size)\n\t\tmax_iu_len = min(max_iu_len, max_it_iu_size);\n\n\tpr_debug(\"max_iu_len = %d\\n\", max_iu_len);\n\n\treturn max_iu_len;\n}\n\n \nstatic int srp_rport_reconnect(struct srp_rport *rport)\n{\n\tstruct srp_target_port *target = rport->lld_data;\n\tstruct srp_rdma_ch *ch;\n\tuint32_t max_iu_len = srp_max_it_iu_len(target->cmd_sg_cnt,\n\t\t\t\t\t\tsrp_use_imm_data,\n\t\t\t\t\t\ttarget->max_it_iu_size);\n\tint i, j, ret = 0;\n\tbool multich = false;\n\n\tsrp_disconnect_target(target);\n\n\tif (target->state == SRP_TARGET_SCANNING)\n\t\treturn -ENODEV;\n\n\t \n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\tret += srp_new_cm_id(ch);\n\t}\n\t{\n\t\tstruct srp_terminate_context context = {\n\t\t\t.srp_target = target, .scsi_result = DID_RESET << 16};\n\n\t\tscsi_host_busy_iter(target->scsi_host, srp_terminate_cmd,\n\t\t\t\t    &context);\n\t}\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\t \n\t\tret += srp_create_ch_ib(ch);\n\n\t\tINIT_LIST_HEAD(&ch->free_tx);\n\t\tfor (j = 0; j < target->queue_size; ++j)\n\t\t\tlist_add(&ch->tx_ring[j]->list, &ch->free_tx);\n\t}\n\n\ttarget->qp_in_error = false;\n\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = srp_connect_ch(ch, max_iu_len, multich);\n\t\tmultich = true;\n\t}\n\n\tif (ret == 0)\n\t\tshost_printk(KERN_INFO, target->scsi_host,\n\t\t\t     PFX \"reconnect succeeded\\n\");\n\n\treturn ret;\n}\n\nstatic void srp_map_desc(struct srp_map_state *state, dma_addr_t dma_addr,\n\t\t\t unsigned int dma_len, u32 rkey)\n{\n\tstruct srp_direct_buf *desc = state->desc;\n\n\tWARN_ON_ONCE(!dma_len);\n\n\tdesc->va = cpu_to_be64(dma_addr);\n\tdesc->key = cpu_to_be32(rkey);\n\tdesc->len = cpu_to_be32(dma_len);\n\n\tstate->total_len += dma_len;\n\tstate->desc++;\n\tstate->ndesc++;\n}\n\nstatic void srp_reg_mr_err_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsrp_handle_qp_err(cq, wc, \"FAST REG\");\n}\n\n \nstatic int srp_map_finish_fr(struct srp_map_state *state,\n\t\t\t     struct srp_request *req,\n\t\t\t     struct srp_rdma_ch *ch, int sg_nents,\n\t\t\t     unsigned int *sg_offset_p)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tstruct ib_reg_wr wr;\n\tstruct srp_fr_desc *desc;\n\tu32 rkey;\n\tint n, err;\n\n\tif (state->fr.next >= state->fr.end) {\n\t\tshost_printk(KERN_ERR, ch->target->scsi_host,\n\t\t\t     PFX \"Out of MRs (mr_per_cmd = %d)\\n\",\n\t\t\t     ch->target->mr_per_cmd);\n\t\treturn -ENOMEM;\n\t}\n\n\tWARN_ON_ONCE(!dev->use_fast_reg);\n\n\tif (sg_nents == 1 && target->global_rkey) {\n\t\tunsigned int sg_offset = sg_offset_p ? *sg_offset_p : 0;\n\n\t\tsrp_map_desc(state, sg_dma_address(state->sg) + sg_offset,\n\t\t\t     sg_dma_len(state->sg) - sg_offset,\n\t\t\t     target->global_rkey);\n\t\tif (sg_offset_p)\n\t\t\t*sg_offset_p = 0;\n\t\treturn 1;\n\t}\n\n\tdesc = srp_fr_pool_get(ch->fr_pool);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\trkey = ib_inc_rkey(desc->mr->rkey);\n\tib_update_fast_reg_key(desc->mr, rkey);\n\n\tn = ib_map_mr_sg(desc->mr, state->sg, sg_nents, sg_offset_p,\n\t\t\t dev->mr_page_size);\n\tif (unlikely(n < 0)) {\n\t\tsrp_fr_pool_put(ch->fr_pool, &desc, 1);\n\t\tpr_debug(\"%s: ib_map_mr_sg(%d, %d) returned %d.\\n\",\n\t\t\t dev_name(&req->scmnd->device->sdev_gendev), sg_nents,\n\t\t\t sg_offset_p ? *sg_offset_p : -1, n);\n\t\treturn n;\n\t}\n\n\tWARN_ON_ONCE(desc->mr->length == 0);\n\n\treq->reg_cqe.done = srp_reg_mr_err_done;\n\n\twr.wr.next = NULL;\n\twr.wr.opcode = IB_WR_REG_MR;\n\twr.wr.wr_cqe = &req->reg_cqe;\n\twr.wr.num_sge = 0;\n\twr.wr.send_flags = 0;\n\twr.mr = desc->mr;\n\twr.key = desc->mr->rkey;\n\twr.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t     IB_ACCESS_REMOTE_READ |\n\t\t     IB_ACCESS_REMOTE_WRITE);\n\n\t*state->fr.next++ = desc;\n\tstate->nmdesc++;\n\n\tsrp_map_desc(state, desc->mr->iova,\n\t\t     desc->mr->length, desc->mr->rkey);\n\n\terr = ib_post_send(ch->qp, &wr.wr, NULL);\n\tif (unlikely(err)) {\n\t\tWARN_ON_ONCE(err == -ENOMEM);\n\t\treturn err;\n\t}\n\n\treturn n;\n}\n\nstatic int srp_map_sg_fr(struct srp_map_state *state, struct srp_rdma_ch *ch,\n\t\t\t struct srp_request *req, struct scatterlist *scat,\n\t\t\t int count)\n{\n\tunsigned int sg_offset = 0;\n\n\tstate->fr.next = req->fr_list;\n\tstate->fr.end = req->fr_list + ch->target->mr_per_cmd;\n\tstate->sg = scat;\n\n\tif (count == 0)\n\t\treturn 0;\n\n\twhile (count) {\n\t\tint i, n;\n\n\t\tn = srp_map_finish_fr(state, req, ch, count, &sg_offset);\n\t\tif (unlikely(n < 0))\n\t\t\treturn n;\n\n\t\tcount -= n;\n\t\tfor (i = 0; i < n; i++)\n\t\t\tstate->sg = sg_next(state->sg);\n\t}\n\n\treturn 0;\n}\n\nstatic int srp_map_sg_dma(struct srp_map_state *state, struct srp_rdma_ch *ch,\n\t\t\t  struct srp_request *req, struct scatterlist *scat,\n\t\t\t  int count)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tfor_each_sg(scat, sg, count, i) {\n\t\tsrp_map_desc(state, sg_dma_address(sg), sg_dma_len(sg),\n\t\t\t     target->global_rkey);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int srp_map_idb(struct srp_rdma_ch *ch, struct srp_request *req,\n\t\t       void **next_mr, void **end_mr, u32 idb_len,\n\t\t       __be32 *idb_rkey)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_device *dev = target->srp_host->srp_dev;\n\tstruct srp_map_state state;\n\tstruct srp_direct_buf idb_desc;\n\tstruct scatterlist idb_sg[1];\n\tint ret;\n\n\tmemset(&state, 0, sizeof(state));\n\tmemset(&idb_desc, 0, sizeof(idb_desc));\n\tstate.gen.next = next_mr;\n\tstate.gen.end = end_mr;\n\tstate.desc = &idb_desc;\n\tstate.base_dma_addr = req->indirect_dma_addr;\n\tstate.dma_len = idb_len;\n\n\tif (dev->use_fast_reg) {\n\t\tstate.sg = idb_sg;\n\t\tsg_init_one(idb_sg, req->indirect_desc, idb_len);\n\t\tidb_sg->dma_address = req->indirect_dma_addr;  \n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\tidb_sg->dma_length = idb_sg->length;\t       \n#endif\n\t\tret = srp_map_finish_fr(&state, req, ch, 1, NULL);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tWARN_ON_ONCE(ret < 1);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\t*idb_rkey = idb_desc.key;\n\n\treturn 0;\n}\n\nstatic void srp_check_mapping(struct srp_map_state *state,\n\t\t\t      struct srp_rdma_ch *ch, struct srp_request *req,\n\t\t\t      struct scatterlist *scat, int count)\n{\n\tstruct srp_device *dev = ch->target->srp_host->srp_dev;\n\tstruct srp_fr_desc **pfr;\n\tu64 desc_len = 0, mr_len = 0;\n\tint i;\n\n\tfor (i = 0; i < state->ndesc; i++)\n\t\tdesc_len += be32_to_cpu(req->indirect_desc[i].len);\n\tif (dev->use_fast_reg)\n\t\tfor (i = 0, pfr = req->fr_list; i < state->nmdesc; i++, pfr++)\n\t\t\tmr_len += (*pfr)->mr->length;\n\tif (desc_len != scsi_bufflen(req->scmnd) ||\n\t    mr_len > scsi_bufflen(req->scmnd))\n\t\tpr_err(\"Inconsistent: scsi len %d <> desc len %lld <> mr len %lld; ndesc %d; nmdesc = %d\\n\",\n\t\t       scsi_bufflen(req->scmnd), desc_len, mr_len,\n\t\t       state->ndesc, state->nmdesc);\n}\n\n \nstatic int srp_map_data(struct scsi_cmnd *scmnd, struct srp_rdma_ch *ch,\n\t\t\tstruct srp_request *req)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct scatterlist *scat, *sg;\n\tstruct srp_cmd *cmd = req->cmd->buf;\n\tint i, len, nents, count, ret;\n\tstruct srp_device *dev;\n\tstruct ib_device *ibdev;\n\tstruct srp_map_state state;\n\tstruct srp_indirect_buf *indirect_hdr;\n\tu64 data_len;\n\tu32 idb_len, table_len;\n\t__be32 idb_rkey;\n\tu8 fmt;\n\n\treq->cmd->num_sge = 1;\n\n\tif (!scsi_sglist(scmnd) || scmnd->sc_data_direction == DMA_NONE)\n\t\treturn sizeof(struct srp_cmd) + cmd->add_cdb_len;\n\n\tif (scmnd->sc_data_direction != DMA_FROM_DEVICE &&\n\t    scmnd->sc_data_direction != DMA_TO_DEVICE) {\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Unhandled data direction %d\\n\",\n\t\t\t     scmnd->sc_data_direction);\n\t\treturn -EINVAL;\n\t}\n\n\tnents = scsi_sg_count(scmnd);\n\tscat  = scsi_sglist(scmnd);\n\tdata_len = scsi_bufflen(scmnd);\n\n\tdev = target->srp_host->srp_dev;\n\tibdev = dev->dev;\n\n\tcount = ib_dma_map_sg(ibdev, scat, nents, scmnd->sc_data_direction);\n\tif (unlikely(count == 0))\n\t\treturn -EIO;\n\n\tif (ch->use_imm_data &&\n\t    count <= ch->max_imm_sge &&\n\t    SRP_IMM_DATA_OFFSET + data_len <= ch->max_it_iu_len &&\n\t    scmnd->sc_data_direction == DMA_TO_DEVICE) {\n\t\tstruct srp_imm_buf *buf;\n\t\tstruct ib_sge *sge = &req->cmd->sge[1];\n\n\t\tfmt = SRP_DATA_DESC_IMM;\n\t\tlen = SRP_IMM_DATA_OFFSET;\n\t\treq->nmdesc = 0;\n\t\tbuf = (void *)cmd->add_data + cmd->add_cdb_len;\n\t\tbuf->len = cpu_to_be32(data_len);\n\t\tWARN_ON_ONCE((void *)(buf + 1) > (void *)cmd + len);\n\t\tfor_each_sg(scat, sg, count, i) {\n\t\t\tsge[i].addr   = sg_dma_address(sg);\n\t\t\tsge[i].length = sg_dma_len(sg);\n\t\t\tsge[i].lkey   = target->lkey;\n\t\t}\n\t\treq->cmd->num_sge += count;\n\t\tgoto map_complete;\n\t}\n\n\tfmt = SRP_DATA_DESC_DIRECT;\n\tlen = sizeof(struct srp_cmd) + cmd->add_cdb_len +\n\t\tsizeof(struct srp_direct_buf);\n\n\tif (count == 1 && target->global_rkey) {\n\t\t \n\t\tstruct srp_direct_buf *buf;\n\n\t\tbuf = (void *)cmd->add_data + cmd->add_cdb_len;\n\t\tbuf->va  = cpu_to_be64(sg_dma_address(scat));\n\t\tbuf->key = cpu_to_be32(target->global_rkey);\n\t\tbuf->len = cpu_to_be32(sg_dma_len(scat));\n\n\t\treq->nmdesc = 0;\n\t\tgoto map_complete;\n\t}\n\n\t \n\tindirect_hdr = (void *)cmd->add_data + cmd->add_cdb_len;\n\n\tib_dma_sync_single_for_cpu(ibdev, req->indirect_dma_addr,\n\t\t\t\t   target->indirect_size, DMA_TO_DEVICE);\n\n\tmemset(&state, 0, sizeof(state));\n\tstate.desc = req->indirect_desc;\n\tif (dev->use_fast_reg)\n\t\tret = srp_map_sg_fr(&state, ch, req, scat, count);\n\telse\n\t\tret = srp_map_sg_dma(&state, ch, req, scat, count);\n\treq->nmdesc = state.nmdesc;\n\tif (ret < 0)\n\t\tgoto unmap;\n\n\t{\n\t\tDEFINE_DYNAMIC_DEBUG_METADATA(ddm,\n\t\t\t\"Memory mapping consistency check\");\n\t\tif (DYNAMIC_DEBUG_BRANCH(ddm))\n\t\t\tsrp_check_mapping(&state, ch, req, scat, count);\n\t}\n\n\t \n\tif (state.ndesc == 1) {\n\t\t \n\t\tstruct srp_direct_buf *buf;\n\n\t\tbuf = (void *)cmd->add_data + cmd->add_cdb_len;\n\t\t*buf = req->indirect_desc[0];\n\t\tgoto map_complete;\n\t}\n\n\tif (unlikely(target->cmd_sg_cnt < state.ndesc &&\n\t\t\t\t\t\t!target->allow_ext_sg)) {\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     \"Could not fit S/G list into SRP_CMD\\n\");\n\t\tret = -EIO;\n\t\tgoto unmap;\n\t}\n\n\tcount = min(state.ndesc, target->cmd_sg_cnt);\n\ttable_len = state.ndesc * sizeof (struct srp_direct_buf);\n\tidb_len = sizeof(struct srp_indirect_buf) + table_len;\n\n\tfmt = SRP_DATA_DESC_INDIRECT;\n\tlen = sizeof(struct srp_cmd) + cmd->add_cdb_len +\n\t\tsizeof(struct srp_indirect_buf);\n\tlen += count * sizeof (struct srp_direct_buf);\n\n\tmemcpy(indirect_hdr->desc_list, req->indirect_desc,\n\t       count * sizeof (struct srp_direct_buf));\n\n\tif (!target->global_rkey) {\n\t\tret = srp_map_idb(ch, req, state.gen.next, state.gen.end,\n\t\t\t\t  idb_len, &idb_rkey);\n\t\tif (ret < 0)\n\t\t\tgoto unmap;\n\t\treq->nmdesc++;\n\t} else {\n\t\tidb_rkey = cpu_to_be32(target->global_rkey);\n\t}\n\n\tindirect_hdr->table_desc.va = cpu_to_be64(req->indirect_dma_addr);\n\tindirect_hdr->table_desc.key = idb_rkey;\n\tindirect_hdr->table_desc.len = cpu_to_be32(table_len);\n\tindirect_hdr->len = cpu_to_be32(state.total_len);\n\n\tif (scmnd->sc_data_direction == DMA_TO_DEVICE)\n\t\tcmd->data_out_desc_cnt = count;\n\telse\n\t\tcmd->data_in_desc_cnt = count;\n\n\tib_dma_sync_single_for_device(ibdev, req->indirect_dma_addr, table_len,\n\t\t\t\t      DMA_TO_DEVICE);\n\nmap_complete:\n\tif (scmnd->sc_data_direction == DMA_TO_DEVICE)\n\t\tcmd->buf_fmt = fmt << 4;\n\telse\n\t\tcmd->buf_fmt = fmt;\n\n\treturn len;\n\nunmap:\n\tsrp_unmap_data(scmnd, ch, req);\n\tif (ret == -ENOMEM && req->nmdesc >= target->mr_pool_size)\n\t\tret = -E2BIG;\n\treturn ret;\n}\n\n \nstatic void srp_put_tx_iu(struct srp_rdma_ch *ch, struct srp_iu *iu,\n\t\t\t  enum srp_iu_type iu_type)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tlist_add(&iu->list, &ch->free_tx);\n\tif (iu_type != SRP_IU_RSP)\n\t\t++ch->req_lim;\n\tspin_unlock_irqrestore(&ch->lock, flags);\n}\n\n \nstatic struct srp_iu *__srp_get_tx_iu(struct srp_rdma_ch *ch,\n\t\t\t\t      enum srp_iu_type iu_type)\n{\n\tstruct srp_target_port *target = ch->target;\n\ts32 rsv = (iu_type == SRP_IU_TSK_MGMT) ? 0 : SRP_TSK_MGMT_SQ_SIZE;\n\tstruct srp_iu *iu;\n\n\tlockdep_assert_held(&ch->lock);\n\n\tib_process_cq_direct(ch->send_cq, -1);\n\n\tif (list_empty(&ch->free_tx))\n\t\treturn NULL;\n\n\t \n\tif (iu_type != SRP_IU_RSP) {\n\t\tif (ch->req_lim <= rsv) {\n\t\t\t++target->zero_req_lim;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t--ch->req_lim;\n\t}\n\n\tiu = list_first_entry(&ch->free_tx, struct srp_iu, list);\n\tlist_del(&iu->list);\n\treturn iu;\n}\n\n \nstatic void srp_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srp_iu *iu = container_of(wc->wr_cqe, struct srp_iu, cqe);\n\tstruct srp_rdma_ch *ch = cq->cq_context;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tsrp_handle_qp_err(cq, wc, \"SEND\");\n\t\treturn;\n\t}\n\n\tlockdep_assert_held(&ch->lock);\n\n\tlist_add(&iu->list, &ch->free_tx);\n}\n\n \nstatic int srp_post_send(struct srp_rdma_ch *ch, struct srp_iu *iu, int len)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_send_wr wr;\n\n\tif (WARN_ON_ONCE(iu->num_sge > SRP_MAX_SGE))\n\t\treturn -EINVAL;\n\n\tiu->sge[0].addr   = iu->dma;\n\tiu->sge[0].length = len;\n\tiu->sge[0].lkey   = target->lkey;\n\n\tiu->cqe.done = srp_send_done;\n\n\twr.next       = NULL;\n\twr.wr_cqe     = &iu->cqe;\n\twr.sg_list    = &iu->sge[0];\n\twr.num_sge    = iu->num_sge;\n\twr.opcode     = IB_WR_SEND;\n\twr.send_flags = IB_SEND_SIGNALED;\n\n\treturn ib_post_send(ch->qp, &wr, NULL);\n}\n\nstatic int srp_post_recv(struct srp_rdma_ch *ch, struct srp_iu *iu)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_recv_wr wr;\n\tstruct ib_sge list;\n\n\tlist.addr   = iu->dma;\n\tlist.length = iu->size;\n\tlist.lkey   = target->lkey;\n\n\tiu->cqe.done = srp_recv_done;\n\n\twr.next     = NULL;\n\twr.wr_cqe   = &iu->cqe;\n\twr.sg_list  = &list;\n\twr.num_sge  = 1;\n\n\treturn ib_post_recv(ch->qp, &wr, NULL);\n}\n\nstatic void srp_process_rsp(struct srp_rdma_ch *ch, struct srp_rsp *rsp)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_request *req;\n\tstruct scsi_cmnd *scmnd;\n\tunsigned long flags;\n\n\tif (unlikely(rsp->tag & SRP_TAG_TSK_MGMT)) {\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\tch->req_lim += be32_to_cpu(rsp->req_lim_delta);\n\t\tif (rsp->tag == ch->tsk_mgmt_tag) {\n\t\t\tch->tsk_mgmt_status = -1;\n\t\t\tif (be32_to_cpu(rsp->resp_data_len) >= 4)\n\t\t\t\tch->tsk_mgmt_status = rsp->data[3];\n\t\t\tcomplete(&ch->tsk_mgmt_done);\n\t\t} else {\n\t\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t\t     \"Received tsk mgmt response too late for tag %#llx\\n\",\n\t\t\t\t     rsp->tag);\n\t\t}\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t} else {\n\t\tscmnd = scsi_host_find_tag(target->scsi_host, rsp->tag);\n\t\tif (scmnd) {\n\t\t\treq = scsi_cmd_priv(scmnd);\n\t\t\tscmnd = srp_claim_req(ch, req, NULL, scmnd);\n\t\t}\n\t\tif (!scmnd) {\n\t\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t\t     \"Null scmnd for RSP w/tag %#016llx received on ch %td / QP %#x\\n\",\n\t\t\t\t     rsp->tag, ch - target->ch, ch->qp->qp_num);\n\n\t\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\t\tch->req_lim += be32_to_cpu(rsp->req_lim_delta);\n\t\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\t\t\treturn;\n\t\t}\n\t\tscmnd->result = rsp->status;\n\n\t\tif (rsp->flags & SRP_RSP_FLAG_SNSVALID) {\n\t\t\tmemcpy(scmnd->sense_buffer, rsp->data +\n\t\t\t       be32_to_cpu(rsp->resp_data_len),\n\t\t\t       min_t(int, be32_to_cpu(rsp->sense_data_len),\n\t\t\t\t     SCSI_SENSE_BUFFERSIZE));\n\t\t}\n\n\t\tif (unlikely(rsp->flags & SRP_RSP_FLAG_DIUNDER))\n\t\t\tscsi_set_resid(scmnd, be32_to_cpu(rsp->data_in_res_cnt));\n\t\telse if (unlikely(rsp->flags & SRP_RSP_FLAG_DOUNDER))\n\t\t\tscsi_set_resid(scmnd, be32_to_cpu(rsp->data_out_res_cnt));\n\n\t\tsrp_free_req(ch, req, scmnd,\n\t\t\t     be32_to_cpu(rsp->req_lim_delta));\n\n\t\tscsi_done(scmnd);\n\t}\n}\n\nstatic int srp_response_common(struct srp_rdma_ch *ch, s32 req_delta,\n\t\t\t       void *rsp, int len)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_device *dev = target->srp_host->srp_dev->dev;\n\tunsigned long flags;\n\tstruct srp_iu *iu;\n\tint err;\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tch->req_lim += req_delta;\n\tiu = __srp_get_tx_iu(ch, SRP_IU_RSP);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\tif (!iu) {\n\t\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t\t     \"no IU available to send response\\n\");\n\t\treturn 1;\n\t}\n\n\tiu->num_sge = 1;\n\tib_dma_sync_single_for_cpu(dev, iu->dma, len, DMA_TO_DEVICE);\n\tmemcpy(iu->buf, rsp, len);\n\tib_dma_sync_single_for_device(dev, iu->dma, len, DMA_TO_DEVICE);\n\n\terr = srp_post_send(ch, iu, len);\n\tif (err) {\n\t\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t\t     \"unable to post response: %d\\n\", err);\n\t\tsrp_put_tx_iu(ch, iu, SRP_IU_RSP);\n\t}\n\n\treturn err;\n}\n\nstatic void srp_process_cred_req(struct srp_rdma_ch *ch,\n\t\t\t\t struct srp_cred_req *req)\n{\n\tstruct srp_cred_rsp rsp = {\n\t\t.opcode = SRP_CRED_RSP,\n\t\t.tag = req->tag,\n\t};\n\ts32 delta = be32_to_cpu(req->req_lim_delta);\n\n\tif (srp_response_common(ch, delta, &rsp, sizeof(rsp)))\n\t\tshost_printk(KERN_ERR, ch->target->scsi_host, PFX\n\t\t\t     \"problems processing SRP_CRED_REQ\\n\");\n}\n\nstatic void srp_process_aer_req(struct srp_rdma_ch *ch,\n\t\t\t\tstruct srp_aer_req *req)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_aer_rsp rsp = {\n\t\t.opcode = SRP_AER_RSP,\n\t\t.tag = req->tag,\n\t};\n\ts32 delta = be32_to_cpu(req->req_lim_delta);\n\n\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t     \"ignoring AER for LUN %llu\\n\", scsilun_to_int(&req->lun));\n\n\tif (srp_response_common(ch, delta, &rsp, sizeof(rsp)))\n\t\tshost_printk(KERN_ERR, target->scsi_host, PFX\n\t\t\t     \"problems processing SRP_AER_REQ\\n\");\n}\n\nstatic void srp_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srp_iu *iu = container_of(wc->wr_cqe, struct srp_iu, cqe);\n\tstruct srp_rdma_ch *ch = cq->cq_context;\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_device *dev = target->srp_host->srp_dev->dev;\n\tint res;\n\tu8 opcode;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tsrp_handle_qp_err(cq, wc, \"RECV\");\n\t\treturn;\n\t}\n\n\tib_dma_sync_single_for_cpu(dev, iu->dma, ch->max_ti_iu_len,\n\t\t\t\t   DMA_FROM_DEVICE);\n\n\topcode = *(u8 *) iu->buf;\n\n\tif (0) {\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"recv completion, opcode 0x%02x\\n\", opcode);\n\t\tprint_hex_dump(KERN_ERR, \"\", DUMP_PREFIX_OFFSET, 8, 1,\n\t\t\t       iu->buf, wc->byte_len, true);\n\t}\n\n\tswitch (opcode) {\n\tcase SRP_RSP:\n\t\tsrp_process_rsp(ch, iu->buf);\n\t\tbreak;\n\n\tcase SRP_CRED_REQ:\n\t\tsrp_process_cred_req(ch, iu->buf);\n\t\tbreak;\n\n\tcase SRP_AER_REQ:\n\t\tsrp_process_aer_req(ch, iu->buf);\n\t\tbreak;\n\n\tcase SRP_T_LOGOUT:\n\t\t \n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Got target logout request\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Unhandled SRP opcode 0x%02x\\n\", opcode);\n\t\tbreak;\n\t}\n\n\tib_dma_sync_single_for_device(dev, iu->dma, ch->max_ti_iu_len,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\tres = srp_post_recv(ch, iu);\n\tif (res != 0)\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"Recv failed with error code %d\\n\", res);\n}\n\n \nstatic void srp_tl_err_work(struct work_struct *work)\n{\n\tstruct srp_target_port *target;\n\n\ttarget = container_of(work, struct srp_target_port, tl_err_work);\n\tif (target->rport)\n\t\tsrp_start_tl_fail_timers(target->rport);\n}\n\nstatic void srp_handle_qp_err(struct ib_cq *cq, struct ib_wc *wc,\n\t\tconst char *opname)\n{\n\tstruct srp_rdma_ch *ch = cq->cq_context;\n\tstruct srp_target_port *target = ch->target;\n\n\tif (ch->connected && !target->qp_in_error) {\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"failed %s status %s (%d) for CQE %p\\n\",\n\t\t\t     opname, ib_wc_status_msg(wc->status), wc->status,\n\t\t\t     wc->wr_cqe);\n\t\tqueue_work(system_long_wq, &target->tl_err_work);\n\t}\n\ttarget->qp_in_error = true;\n}\n\nstatic int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)\n{\n\tstruct request *rq = scsi_cmd_to_rq(scmnd);\n\tstruct srp_target_port *target = host_to_target(shost);\n\tstruct srp_rdma_ch *ch;\n\tstruct srp_request *req = scsi_cmd_priv(scmnd);\n\tstruct srp_iu *iu;\n\tstruct srp_cmd *cmd;\n\tstruct ib_device *dev;\n\tunsigned long flags;\n\tu32 tag;\n\tint len, ret;\n\n\tscmnd->result = srp_chkready(target->rport);\n\tif (unlikely(scmnd->result))\n\t\tgoto err;\n\n\tWARN_ON_ONCE(rq->tag < 0);\n\ttag = blk_mq_unique_tag(rq);\n\tch = &target->ch[blk_mq_unique_tag_to_hwq(tag)];\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tiu = __srp_get_tx_iu(ch, SRP_IU_CMD);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\tif (!iu)\n\t\tgoto err;\n\n\tdev = target->srp_host->srp_dev->dev;\n\tib_dma_sync_single_for_cpu(dev, iu->dma, ch->max_it_iu_len,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tcmd = iu->buf;\n\tmemset(cmd, 0, sizeof *cmd);\n\n\tcmd->opcode = SRP_CMD;\n\tint_to_scsilun(scmnd->device->lun, &cmd->lun);\n\tcmd->tag    = tag;\n\tmemcpy(cmd->cdb, scmnd->cmnd, scmnd->cmd_len);\n\tif (unlikely(scmnd->cmd_len > sizeof(cmd->cdb))) {\n\t\tcmd->add_cdb_len = round_up(scmnd->cmd_len - sizeof(cmd->cdb),\n\t\t\t\t\t    4);\n\t\tif (WARN_ON_ONCE(cmd->add_cdb_len > SRP_MAX_ADD_CDB_LEN))\n\t\t\tgoto err_iu;\n\t}\n\n\treq->scmnd    = scmnd;\n\treq->cmd      = iu;\n\n\tlen = srp_map_data(scmnd, ch, req);\n\tif (len < 0) {\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"Failed to map data (%d)\\n\", len);\n\t\t \n\t\tscmnd->result = len == -ENOMEM ?\n\t\t\tDID_OK << 16 | SAM_STAT_TASK_SET_FULL : DID_ERROR << 16;\n\t\tgoto err_iu;\n\t}\n\n\tib_dma_sync_single_for_device(dev, iu->dma, ch->max_it_iu_len,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\tif (srp_post_send(ch, iu, len)) {\n\t\tshost_printk(KERN_ERR, target->scsi_host, PFX \"Send failed\\n\");\n\t\tscmnd->result = DID_ERROR << 16;\n\t\tgoto err_unmap;\n\t}\n\n\treturn 0;\n\nerr_unmap:\n\tsrp_unmap_data(scmnd, ch, req);\n\nerr_iu:\n\tsrp_put_tx_iu(ch, iu, SRP_IU_CMD);\n\n\t \n\treq->scmnd = NULL;\n\nerr:\n\tif (scmnd->result) {\n\t\tscsi_done(scmnd);\n\t\tret = 0;\n\t} else {\n\t\tret = SCSI_MLQUEUE_HOST_BUSY;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int srp_alloc_iu_bufs(struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tint i;\n\n\tch->rx_ring = kcalloc(target->queue_size, sizeof(*ch->rx_ring),\n\t\t\t      GFP_KERNEL);\n\tif (!ch->rx_ring)\n\t\tgoto err_no_ring;\n\tch->tx_ring = kcalloc(target->queue_size, sizeof(*ch->tx_ring),\n\t\t\t      GFP_KERNEL);\n\tif (!ch->tx_ring)\n\t\tgoto err_no_ring;\n\n\tfor (i = 0; i < target->queue_size; ++i) {\n\t\tch->rx_ring[i] = srp_alloc_iu(target->srp_host,\n\t\t\t\t\t      ch->max_ti_iu_len,\n\t\t\t\t\t      GFP_KERNEL, DMA_FROM_DEVICE);\n\t\tif (!ch->rx_ring[i])\n\t\t\tgoto err;\n\t}\n\n\tfor (i = 0; i < target->queue_size; ++i) {\n\t\tch->tx_ring[i] = srp_alloc_iu(target->srp_host,\n\t\t\t\t\t      ch->max_it_iu_len,\n\t\t\t\t\t      GFP_KERNEL, DMA_TO_DEVICE);\n\t\tif (!ch->tx_ring[i])\n\t\t\tgoto err;\n\n\t\tlist_add(&ch->tx_ring[i]->list, &ch->free_tx);\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < target->queue_size; ++i) {\n\t\tsrp_free_iu(target->srp_host, ch->rx_ring[i]);\n\t\tsrp_free_iu(target->srp_host, ch->tx_ring[i]);\n\t}\n\n\nerr_no_ring:\n\tkfree(ch->tx_ring);\n\tch->tx_ring = NULL;\n\tkfree(ch->rx_ring);\n\tch->rx_ring = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic uint32_t srp_compute_rq_tmo(struct ib_qp_attr *qp_attr, int attr_mask)\n{\n\tuint64_t T_tr_ns, max_compl_time_ms;\n\tuint32_t rq_tmo_jiffies;\n\n\t \n\tWARN_ON_ONCE((attr_mask & (IB_QP_TIMEOUT | IB_QP_RETRY_CNT)) !=\n\t\t     (IB_QP_TIMEOUT | IB_QP_RETRY_CNT));\n\n\t \n\tT_tr_ns = 4096 * (1ULL << qp_attr->timeout);\n\tmax_compl_time_ms = qp_attr->retry_cnt * 4 * T_tr_ns;\n\tdo_div(max_compl_time_ms, NSEC_PER_MSEC);\n\trq_tmo_jiffies = msecs_to_jiffies(max_compl_time_ms + 1000);\n\n\treturn rq_tmo_jiffies;\n}\n\nstatic void srp_cm_rep_handler(struct ib_cm_id *cm_id,\n\t\t\t       const struct srp_login_rsp *lrsp,\n\t\t\t       struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct ib_qp_attr *qp_attr = NULL;\n\tint attr_mask = 0;\n\tint ret = 0;\n\tint i;\n\n\tif (lrsp->opcode == SRP_LOGIN_RSP) {\n\t\tch->max_ti_iu_len = be32_to_cpu(lrsp->max_ti_iu_len);\n\t\tch->req_lim       = be32_to_cpu(lrsp->req_lim_delta);\n\t\tch->use_imm_data  = srp_use_imm_data &&\n\t\t\t(lrsp->rsp_flags & SRP_LOGIN_RSP_IMMED_SUPP);\n\t\tch->max_it_iu_len = srp_max_it_iu_len(target->cmd_sg_cnt,\n\t\t\t\t\t\t      ch->use_imm_data,\n\t\t\t\t\t\t      target->max_it_iu_size);\n\t\tWARN_ON_ONCE(ch->max_it_iu_len >\n\t\t\t     be32_to_cpu(lrsp->max_it_iu_len));\n\n\t\tif (ch->use_imm_data)\n\t\t\tshost_printk(KERN_DEBUG, target->scsi_host,\n\t\t\t\t     PFX \"using immediate data\\n\");\n\n\t\t \n\t\ttarget->scsi_host->can_queue\n\t\t\t= min(ch->req_lim - SRP_TSK_MGMT_SQ_SIZE,\n\t\t\t      target->scsi_host->can_queue);\n\t\ttarget->scsi_host->cmd_per_lun\n\t\t\t= min_t(int, target->scsi_host->can_queue,\n\t\t\t\ttarget->scsi_host->cmd_per_lun);\n\t} else {\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Unhandled RSP opcode %#x\\n\", lrsp->opcode);\n\t\tret = -ECONNRESET;\n\t\tgoto error;\n\t}\n\n\tif (!ch->rx_ring) {\n\t\tret = srp_alloc_iu_bufs(ch);\n\t\tif (ret)\n\t\t\tgoto error;\n\t}\n\n\tfor (i = 0; i < target->queue_size; i++) {\n\t\tstruct srp_iu *iu = ch->rx_ring[i];\n\n\t\tret = srp_post_recv(ch, iu);\n\t\tif (ret)\n\t\t\tgoto error;\n\t}\n\n\tif (!target->using_rdma_cm) {\n\t\tret = -ENOMEM;\n\t\tqp_attr = kmalloc(sizeof(*qp_attr), GFP_KERNEL);\n\t\tif (!qp_attr)\n\t\t\tgoto error;\n\n\t\tqp_attr->qp_state = IB_QPS_RTR;\n\t\tret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\n\t\tif (ret)\n\t\t\tgoto error_free;\n\n\t\tret = ib_modify_qp(ch->qp, qp_attr, attr_mask);\n\t\tif (ret)\n\t\t\tgoto error_free;\n\n\t\tqp_attr->qp_state = IB_QPS_RTS;\n\t\tret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\n\t\tif (ret)\n\t\t\tgoto error_free;\n\n\t\ttarget->rq_tmo_jiffies = srp_compute_rq_tmo(qp_attr, attr_mask);\n\n\t\tret = ib_modify_qp(ch->qp, qp_attr, attr_mask);\n\t\tif (ret)\n\t\t\tgoto error_free;\n\n\t\tret = ib_send_cm_rtu(cm_id, NULL, 0);\n\t}\n\nerror_free:\n\tkfree(qp_attr);\n\nerror:\n\tch->status = ret;\n}\n\nstatic void srp_ib_cm_rej_handler(struct ib_cm_id *cm_id,\n\t\t\t\t  const struct ib_cm_event *event,\n\t\t\t\t  struct srp_rdma_ch *ch)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct Scsi_Host *shost = target->scsi_host;\n\tstruct ib_class_port_info *cpi;\n\tint opcode;\n\tu16 dlid;\n\n\tswitch (event->param.rej_rcvd.reason) {\n\tcase IB_CM_REJ_PORT_CM_REDIRECT:\n\t\tcpi = event->param.rej_rcvd.ari;\n\t\tdlid = be16_to_cpu(cpi->redirect_lid);\n\t\tsa_path_set_dlid(&ch->ib_cm.path, dlid);\n\t\tch->ib_cm.path.pkey = cpi->redirect_pkey;\n\t\tcm_id->remote_cm_qpn = be32_to_cpu(cpi->redirect_qp) & 0x00ffffff;\n\t\tmemcpy(ch->ib_cm.path.dgid.raw, cpi->redirect_gid, 16);\n\n\t\tch->status = dlid ? SRP_DLID_REDIRECT : SRP_PORT_REDIRECT;\n\t\tbreak;\n\n\tcase IB_CM_REJ_PORT_REDIRECT:\n\t\tif (srp_target_is_topspin(target)) {\n\t\t\tunion ib_gid *dgid = &ch->ib_cm.path.dgid;\n\n\t\t\t \n\t\t\tmemcpy(dgid->raw, event->param.rej_rcvd.ari, 16);\n\n\t\t\tshost_printk(KERN_DEBUG, shost,\n\t\t\t\t     PFX \"Topspin/Cisco redirect to target port GID %016llx%016llx\\n\",\n\t\t\t\t     be64_to_cpu(dgid->global.subnet_prefix),\n\t\t\t\t     be64_to_cpu(dgid->global.interface_id));\n\n\t\t\tch->status = SRP_PORT_REDIRECT;\n\t\t} else {\n\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t     \"  REJ reason: IB_CM_REJ_PORT_REDIRECT\\n\");\n\t\t\tch->status = -ECONNRESET;\n\t\t}\n\t\tbreak;\n\n\tcase IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID:\n\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t    \"  REJ reason: IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID\\n\");\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase IB_CM_REJ_CONSUMER_DEFINED:\n\t\topcode = *(u8 *) event->private_data;\n\t\tif (opcode == SRP_LOGIN_REJ) {\n\t\t\tstruct srp_login_rej *rej = event->private_data;\n\t\t\tu32 reason = be32_to_cpu(rej->reason);\n\n\t\t\tif (reason == SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE)\n\t\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t\t     PFX \"SRP_LOGIN_REJ: requested max_it_iu_len too large\\n\");\n\t\t\telse\n\t\t\t\tshost_printk(KERN_WARNING, shost, PFX\n\t\t\t\t\t     \"SRP LOGIN from %pI6 to %pI6 REJECTED, reason 0x%08x\\n\",\n\t\t\t\t\t     target->sgid.raw,\n\t\t\t\t\t     target->ib_cm.orig_dgid.raw,\n\t\t\t\t\t     reason);\n\t\t} else\n\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t     \"  REJ reason: IB_CM_REJ_CONSUMER_DEFINED,\"\n\t\t\t\t     \" opcode 0x%02x\\n\", opcode);\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase IB_CM_REJ_STALE_CONN:\n\t\tshost_printk(KERN_WARNING, shost, \"  REJ reason: stale connection\\n\");\n\t\tch->status = SRP_STALE_CONN;\n\t\tbreak;\n\n\tdefault:\n\t\tshost_printk(KERN_WARNING, shost, \"  REJ reason 0x%x\\n\",\n\t\t\t     event->param.rej_rcvd.reason);\n\t\tch->status = -ECONNRESET;\n\t}\n}\n\nstatic int srp_ib_cm_handler(struct ib_cm_id *cm_id,\n\t\t\t     const struct ib_cm_event *event)\n{\n\tstruct srp_rdma_ch *ch = cm_id->context;\n\tstruct srp_target_port *target = ch->target;\n\tint comp = 0;\n\n\tswitch (event->event) {\n\tcase IB_CM_REQ_ERROR:\n\t\tshost_printk(KERN_DEBUG, target->scsi_host,\n\t\t\t     PFX \"Sending CM REQ failed\\n\");\n\t\tcomp = 1;\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase IB_CM_REP_RECEIVED:\n\t\tcomp = 1;\n\t\tsrp_cm_rep_handler(cm_id, event->private_data, ch);\n\t\tbreak;\n\n\tcase IB_CM_REJ_RECEIVED:\n\t\tshost_printk(KERN_DEBUG, target->scsi_host, PFX \"REJ received\\n\");\n\t\tcomp = 1;\n\n\t\tsrp_ib_cm_rej_handler(cm_id, event, ch);\n\t\tbreak;\n\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"DREQ received - connection closed\\n\");\n\t\tch->connected = false;\n\t\tif (ib_send_cm_drep(cm_id, NULL, 0))\n\t\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t\t     PFX \"Sending CM DREP failed\\n\");\n\t\tqueue_work(system_long_wq, &target->tl_err_work);\n\t\tbreak;\n\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"connection closed\\n\");\n\t\tcomp = 1;\n\n\t\tch->status = 0;\n\t\tbreak;\n\n\tcase IB_CM_MRA_RECEIVED:\n\tcase IB_CM_DREQ_ERROR:\n\tcase IB_CM_DREP_RECEIVED:\n\t\tbreak;\n\n\tdefault:\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Unhandled CM event %d\\n\", event->event);\n\t\tbreak;\n\t}\n\n\tif (comp)\n\t\tcomplete(&ch->done);\n\n\treturn 0;\n}\n\nstatic void srp_rdma_cm_rej_handler(struct srp_rdma_ch *ch,\n\t\t\t\t    struct rdma_cm_event *event)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct Scsi_Host *shost = target->scsi_host;\n\tint opcode;\n\n\tswitch (event->status) {\n\tcase IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID:\n\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t    \"  REJ reason: IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID\\n\");\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase IB_CM_REJ_CONSUMER_DEFINED:\n\t\topcode = *(u8 *) event->param.conn.private_data;\n\t\tif (opcode == SRP_LOGIN_REJ) {\n\t\t\tstruct srp_login_rej *rej =\n\t\t\t\t(struct srp_login_rej *)\n\t\t\t\tevent->param.conn.private_data;\n\t\t\tu32 reason = be32_to_cpu(rej->reason);\n\n\t\t\tif (reason == SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE)\n\t\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t\t     PFX \"SRP_LOGIN_REJ: requested max_it_iu_len too large\\n\");\n\t\t\telse\n\t\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t\t    PFX \"SRP LOGIN REJECTED, reason 0x%08x\\n\", reason);\n\t\t} else {\n\t\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t\t     \"  REJ reason: IB_CM_REJ_CONSUMER_DEFINED, opcode 0x%02x\\n\",\n\t\t\t\t     opcode);\n\t\t}\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase IB_CM_REJ_STALE_CONN:\n\t\tshost_printk(KERN_WARNING, shost,\n\t\t\t     \"  REJ reason: stale connection\\n\");\n\t\tch->status = SRP_STALE_CONN;\n\t\tbreak;\n\n\tdefault:\n\t\tshost_printk(KERN_WARNING, shost, \"  REJ reason 0x%x\\n\",\n\t\t\t     event->status);\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\t}\n}\n\nstatic int srp_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t       struct rdma_cm_event *event)\n{\n\tstruct srp_rdma_ch *ch = cm_id->context;\n\tstruct srp_target_port *target = ch->target;\n\tint comp = 0;\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\t\tch->status = 0;\n\t\tcomp = 1;\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\t\tch->status = -ENXIO;\n\t\tcomp = 1;\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tch->status = 0;\n\t\tcomp = 1;\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\t\tch->status = -EHOSTUNREACH;\n\t\tcomp = 1;\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\t\tshost_printk(KERN_DEBUG, target->scsi_host,\n\t\t\t     PFX \"Sending CM REQ failed\\n\");\n\t\tcomp = 1;\n\t\tch->status = -ECONNRESET;\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tcomp = 1;\n\t\tsrp_cm_rep_handler(NULL, event->param.conn.private_data, ch);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tshost_printk(KERN_DEBUG, target->scsi_host, PFX \"REJ received\\n\");\n\t\tcomp = 1;\n\n\t\tsrp_rdma_cm_rej_handler(ch, event);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\tif (ch->connected) {\n\t\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t\t     PFX \"received DREQ\\n\");\n\t\t\trdma_disconnect(ch->rdma_cm.cm_id);\n\t\t\tcomp = 1;\n\t\t\tch->status = 0;\n\t\t\tqueue_work(system_long_wq, &target->tl_err_work);\n\t\t}\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t     PFX \"connection closed\\n\");\n\n\t\tcomp = 1;\n\t\tch->status = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tshost_printk(KERN_WARNING, target->scsi_host,\n\t\t\t     PFX \"Unhandled CM event %d\\n\", event->event);\n\t\tbreak;\n\t}\n\n\tif (comp)\n\t\tcomplete(&ch->done);\n\n\treturn 0;\n}\n\n \nstatic int\nsrp_change_queue_depth(struct scsi_device *sdev, int qdepth)\n{\n\tif (!sdev->tagged_supported)\n\t\tqdepth = 1;\n\treturn scsi_change_queue_depth(sdev, qdepth);\n}\n\nstatic int srp_send_tsk_mgmt(struct srp_rdma_ch *ch, u64 req_tag, u64 lun,\n\t\t\t     u8 func, u8 *status)\n{\n\tstruct srp_target_port *target = ch->target;\n\tstruct srp_rport *rport = target->rport;\n\tstruct ib_device *dev = target->srp_host->srp_dev->dev;\n\tstruct srp_iu *iu;\n\tstruct srp_tsk_mgmt *tsk_mgmt;\n\tint res;\n\n\tif (!ch->connected || target->qp_in_error)\n\t\treturn -1;\n\n\t \n\tmutex_lock(&rport->mutex);\n\tspin_lock_irq(&ch->lock);\n\tiu = __srp_get_tx_iu(ch, SRP_IU_TSK_MGMT);\n\tspin_unlock_irq(&ch->lock);\n\n\tif (!iu) {\n\t\tmutex_unlock(&rport->mutex);\n\n\t\treturn -1;\n\t}\n\n\tiu->num_sge = 1;\n\n\tib_dma_sync_single_for_cpu(dev, iu->dma, sizeof *tsk_mgmt,\n\t\t\t\t   DMA_TO_DEVICE);\n\ttsk_mgmt = iu->buf;\n\tmemset(tsk_mgmt, 0, sizeof *tsk_mgmt);\n\n\ttsk_mgmt->opcode \t= SRP_TSK_MGMT;\n\tint_to_scsilun(lun, &tsk_mgmt->lun);\n\ttsk_mgmt->tsk_mgmt_func = func;\n\ttsk_mgmt->task_tag\t= req_tag;\n\n\tspin_lock_irq(&ch->lock);\n\tch->tsk_mgmt_tag = (ch->tsk_mgmt_tag + 1) | SRP_TAG_TSK_MGMT;\n\ttsk_mgmt->tag = ch->tsk_mgmt_tag;\n\tspin_unlock_irq(&ch->lock);\n\n\tinit_completion(&ch->tsk_mgmt_done);\n\n\tib_dma_sync_single_for_device(dev, iu->dma, sizeof *tsk_mgmt,\n\t\t\t\t      DMA_TO_DEVICE);\n\tif (srp_post_send(ch, iu, sizeof(*tsk_mgmt))) {\n\t\tsrp_put_tx_iu(ch, iu, SRP_IU_TSK_MGMT);\n\t\tmutex_unlock(&rport->mutex);\n\n\t\treturn -1;\n\t}\n\tres = wait_for_completion_timeout(&ch->tsk_mgmt_done,\n\t\t\t\t\tmsecs_to_jiffies(SRP_ABORT_TIMEOUT_MS));\n\tif (res > 0 && status)\n\t\t*status = ch->tsk_mgmt_status;\n\tmutex_unlock(&rport->mutex);\n\n\tWARN_ON_ONCE(res < 0);\n\n\treturn res > 0 ? 0 : -1;\n}\n\nstatic int srp_abort(struct scsi_cmnd *scmnd)\n{\n\tstruct srp_target_port *target = host_to_target(scmnd->device->host);\n\tstruct srp_request *req = scsi_cmd_priv(scmnd);\n\tu32 tag;\n\tu16 ch_idx;\n\tstruct srp_rdma_ch *ch;\n\n\tshost_printk(KERN_ERR, target->scsi_host, \"SRP abort called\\n\");\n\n\ttag = blk_mq_unique_tag(scsi_cmd_to_rq(scmnd));\n\tch_idx = blk_mq_unique_tag_to_hwq(tag);\n\tif (WARN_ON_ONCE(ch_idx >= target->ch_count))\n\t\treturn SUCCESS;\n\tch = &target->ch[ch_idx];\n\tif (!srp_claim_req(ch, req, NULL, scmnd))\n\t\treturn SUCCESS;\n\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t     \"Sending SRP abort for tag %#x\\n\", tag);\n\tif (srp_send_tsk_mgmt(ch, tag, scmnd->device->lun,\n\t\t\t      SRP_TSK_ABORT_TASK, NULL) == 0) {\n\t\tsrp_free_req(ch, req, scmnd, 0);\n\t\treturn SUCCESS;\n\t}\n\tif (target->rport->state == SRP_RPORT_LOST)\n\t\treturn FAST_IO_FAIL;\n\n\treturn FAILED;\n}\n\nstatic int srp_reset_device(struct scsi_cmnd *scmnd)\n{\n\tstruct srp_target_port *target = host_to_target(scmnd->device->host);\n\tstruct srp_rdma_ch *ch;\n\tu8 status;\n\n\tshost_printk(KERN_ERR, target->scsi_host, \"SRP reset_device called\\n\");\n\n\tch = &target->ch[0];\n\tif (srp_send_tsk_mgmt(ch, SRP_TAG_NO_REQ, scmnd->device->lun,\n\t\t\t      SRP_TSK_LUN_RESET, &status))\n\t\treturn FAILED;\n\tif (status)\n\t\treturn FAILED;\n\n\treturn SUCCESS;\n}\n\nstatic int srp_reset_host(struct scsi_cmnd *scmnd)\n{\n\tstruct srp_target_port *target = host_to_target(scmnd->device->host);\n\n\tshost_printk(KERN_ERR, target->scsi_host, PFX \"SRP reset_host called\\n\");\n\n\treturn srp_reconnect_rport(target->rport) == 0 ? SUCCESS : FAILED;\n}\n\nstatic int srp_target_alloc(struct scsi_target *starget)\n{\n\tstruct Scsi_Host *shost = dev_to_shost(starget->dev.parent);\n\tstruct srp_target_port *target = host_to_target(shost);\n\n\tif (target->target_can_queue)\n\t\tstarget->can_queue = target->target_can_queue;\n\treturn 0;\n}\n\nstatic int srp_slave_configure(struct scsi_device *sdev)\n{\n\tstruct Scsi_Host *shost = sdev->host;\n\tstruct srp_target_port *target = host_to_target(shost);\n\tstruct request_queue *q = sdev->request_queue;\n\tunsigned long timeout;\n\n\tif (sdev->type == TYPE_DISK) {\n\t\ttimeout = max_t(unsigned, 30 * HZ, target->rq_tmo_jiffies);\n\t\tblk_queue_rq_timeout(q, timeout);\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t id_ext_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"0x%016llx\\n\", be64_to_cpu(target->id_ext));\n}\n\nstatic DEVICE_ATTR_RO(id_ext);\n\nstatic ssize_t ioc_guid_show(struct device *dev, struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"0x%016llx\\n\", be64_to_cpu(target->ioc_guid));\n}\n\nstatic DEVICE_ATTR_RO(ioc_guid);\n\nstatic ssize_t service_id_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\tif (target->using_rdma_cm)\n\t\treturn -ENOENT;\n\treturn sysfs_emit(buf, \"0x%016llx\\n\",\n\t\t\t  be64_to_cpu(target->ib_cm.service_id));\n}\n\nstatic DEVICE_ATTR_RO(service_id);\n\nstatic ssize_t pkey_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\tif (target->using_rdma_cm)\n\t\treturn -ENOENT;\n\n\treturn sysfs_emit(buf, \"0x%04x\\n\", be16_to_cpu(target->ib_cm.pkey));\n}\n\nstatic DEVICE_ATTR_RO(pkey);\n\nstatic ssize_t sgid_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%pI6\\n\", target->sgid.raw);\n}\n\nstatic DEVICE_ATTR_RO(sgid);\n\nstatic ssize_t dgid_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\tstruct srp_rdma_ch *ch = &target->ch[0];\n\n\tif (target->using_rdma_cm)\n\t\treturn -ENOENT;\n\n\treturn sysfs_emit(buf, \"%pI6\\n\", ch->ib_cm.path.dgid.raw);\n}\n\nstatic DEVICE_ATTR_RO(dgid);\n\nstatic ssize_t orig_dgid_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\tif (target->using_rdma_cm)\n\t\treturn -ENOENT;\n\n\treturn sysfs_emit(buf, \"%pI6\\n\", target->ib_cm.orig_dgid.raw);\n}\n\nstatic DEVICE_ATTR_RO(orig_dgid);\n\nstatic ssize_t req_lim_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\tstruct srp_rdma_ch *ch;\n\tint i, req_lim = INT_MAX;\n\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\treq_lim = min(req_lim, ch->req_lim);\n\t}\n\n\treturn sysfs_emit(buf, \"%d\\n\", req_lim);\n}\n\nstatic DEVICE_ATTR_RO(req_lim);\n\nstatic ssize_t zero_req_lim_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%d\\n\", target->zero_req_lim);\n}\n\nstatic DEVICE_ATTR_RO(zero_req_lim);\n\nstatic ssize_t local_ib_port_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%u\\n\", target->srp_host->port);\n}\n\nstatic DEVICE_ATTR_RO(local_ib_port);\n\nstatic ssize_t local_ib_device_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  dev_name(&target->srp_host->srp_dev->dev->dev));\n}\n\nstatic DEVICE_ATTR_RO(local_ib_device);\n\nstatic ssize_t ch_count_show(struct device *dev, struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%d\\n\", target->ch_count);\n}\n\nstatic DEVICE_ATTR_RO(ch_count);\n\nstatic ssize_t comp_vector_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%d\\n\", target->comp_vector);\n}\n\nstatic DEVICE_ATTR_RO(comp_vector);\n\nstatic ssize_t tl_retry_count_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%d\\n\", target->tl_retry_count);\n}\n\nstatic DEVICE_ATTR_RO(tl_retry_count);\n\nstatic ssize_t cmd_sg_entries_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%u\\n\", target->cmd_sg_cnt);\n}\n\nstatic DEVICE_ATTR_RO(cmd_sg_entries);\n\nstatic ssize_t allow_ext_sg_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct srp_target_port *target = host_to_target(class_to_shost(dev));\n\n\treturn sysfs_emit(buf, \"%s\\n\", target->allow_ext_sg ? \"true\" : \"false\");\n}\n\nstatic DEVICE_ATTR_RO(allow_ext_sg);\n\nstatic struct attribute *srp_host_attrs[] = {\n\t&dev_attr_id_ext.attr,\n\t&dev_attr_ioc_guid.attr,\n\t&dev_attr_service_id.attr,\n\t&dev_attr_pkey.attr,\n\t&dev_attr_sgid.attr,\n\t&dev_attr_dgid.attr,\n\t&dev_attr_orig_dgid.attr,\n\t&dev_attr_req_lim.attr,\n\t&dev_attr_zero_req_lim.attr,\n\t&dev_attr_local_ib_port.attr,\n\t&dev_attr_local_ib_device.attr,\n\t&dev_attr_ch_count.attr,\n\t&dev_attr_comp_vector.attr,\n\t&dev_attr_tl_retry_count.attr,\n\t&dev_attr_cmd_sg_entries.attr,\n\t&dev_attr_allow_ext_sg.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(srp_host);\n\nstatic const struct scsi_host_template srp_template = {\n\t.module\t\t\t\t= THIS_MODULE,\n\t.name\t\t\t\t= \"InfiniBand SRP initiator\",\n\t.proc_name\t\t\t= DRV_NAME,\n\t.target_alloc\t\t\t= srp_target_alloc,\n\t.slave_configure\t\t= srp_slave_configure,\n\t.info\t\t\t\t= srp_target_info,\n\t.init_cmd_priv\t\t\t= srp_init_cmd_priv,\n\t.exit_cmd_priv\t\t\t= srp_exit_cmd_priv,\n\t.queuecommand\t\t\t= srp_queuecommand,\n\t.change_queue_depth             = srp_change_queue_depth,\n\t.eh_timed_out\t\t\t= srp_timed_out,\n\t.eh_abort_handler\t\t= srp_abort,\n\t.eh_device_reset_handler\t= srp_reset_device,\n\t.eh_host_reset_handler\t\t= srp_reset_host,\n\t.skip_settle_delay\t\t= true,\n\t.sg_tablesize\t\t\t= SRP_DEF_SG_TABLESIZE,\n\t.can_queue\t\t\t= SRP_DEFAULT_CMD_SQ_SIZE,\n\t.this_id\t\t\t= -1,\n\t.cmd_per_lun\t\t\t= SRP_DEFAULT_CMD_SQ_SIZE,\n\t.shost_groups\t\t\t= srp_host_groups,\n\t.track_queue_depth\t\t= 1,\n\t.cmd_size\t\t\t= sizeof(struct srp_request),\n};\n\nstatic int srp_sdev_count(struct Scsi_Host *host)\n{\n\tstruct scsi_device *sdev;\n\tint c = 0;\n\n\tshost_for_each_device(sdev, host)\n\t\tc++;\n\n\treturn c;\n}\n\n \nstatic int srp_add_target(struct srp_host *host, struct srp_target_port *target)\n{\n\tstruct srp_rport_identifiers ids;\n\tstruct srp_rport *rport;\n\n\ttarget->state = SRP_TARGET_SCANNING;\n\tsprintf(target->target_name, \"SRP.T10:%016llX\",\n\t\tbe64_to_cpu(target->id_ext));\n\n\tif (scsi_add_host(target->scsi_host, host->srp_dev->dev->dev.parent))\n\t\treturn -ENODEV;\n\n\tmemcpy(ids.port_id, &target->id_ext, 8);\n\tmemcpy(ids.port_id + 8, &target->ioc_guid, 8);\n\tids.roles = SRP_RPORT_ROLE_TARGET;\n\trport = srp_rport_add(target->scsi_host, &ids);\n\tif (IS_ERR(rport)) {\n\t\tscsi_remove_host(target->scsi_host);\n\t\treturn PTR_ERR(rport);\n\t}\n\n\trport->lld_data = target;\n\ttarget->rport = rport;\n\n\tspin_lock(&host->target_lock);\n\tlist_add_tail(&target->list, &host->target_list);\n\tspin_unlock(&host->target_lock);\n\n\tscsi_scan_target(&target->scsi_host->shost_gendev,\n\t\t\t 0, target->scsi_id, SCAN_WILD_CARD, SCSI_SCAN_INITIAL);\n\n\tif (srp_connected_ch(target) < target->ch_count ||\n\t    target->qp_in_error) {\n\t\tshost_printk(KERN_INFO, target->scsi_host,\n\t\t\t     PFX \"SCSI scan failed - removing SCSI host\\n\");\n\t\tsrp_queue_remove_work(target);\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"%s: SCSI scan succeeded - detected %d LUNs\\n\",\n\t\t dev_name(&target->scsi_host->shost_gendev),\n\t\t srp_sdev_count(target->scsi_host));\n\n\tspin_lock_irq(&target->lock);\n\tif (target->state == SRP_TARGET_SCANNING)\n\t\ttarget->state = SRP_TARGET_LIVE;\n\tspin_unlock_irq(&target->lock);\n\nout:\n\treturn 0;\n}\n\nstatic void srp_release_dev(struct device *dev)\n{\n\tstruct srp_host *host =\n\t\tcontainer_of(dev, struct srp_host, dev);\n\n\tkfree(host);\n}\n\nstatic struct attribute *srp_class_attrs[];\n\nATTRIBUTE_GROUPS(srp_class);\n\nstatic struct class srp_class = {\n\t.name    = \"infiniband_srp\",\n\t.dev_groups = srp_class_groups,\n\t.dev_release = srp_release_dev\n};\n\n \nstatic bool srp_conn_unique(struct srp_host *host,\n\t\t\t    struct srp_target_port *target)\n{\n\tstruct srp_target_port *t;\n\tbool ret = false;\n\n\tif (target->state == SRP_TARGET_REMOVED)\n\t\tgoto out;\n\n\tret = true;\n\n\tspin_lock(&host->target_lock);\n\tlist_for_each_entry(t, &host->target_list, list) {\n\t\tif (t != target &&\n\t\t    target->id_ext == t->id_ext &&\n\t\t    target->ioc_guid == t->ioc_guid &&\n\t\t    target->initiator_ext == t->initiator_ext) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&host->target_lock);\n\nout:\n\treturn ret;\n}\n\n \nenum {\n\tSRP_OPT_ERR\t\t= 0,\n\tSRP_OPT_ID_EXT\t\t= 1 << 0,\n\tSRP_OPT_IOC_GUID\t= 1 << 1,\n\tSRP_OPT_DGID\t\t= 1 << 2,\n\tSRP_OPT_PKEY\t\t= 1 << 3,\n\tSRP_OPT_SERVICE_ID\t= 1 << 4,\n\tSRP_OPT_MAX_SECT\t= 1 << 5,\n\tSRP_OPT_MAX_CMD_PER_LUN\t= 1 << 6,\n\tSRP_OPT_IO_CLASS\t= 1 << 7,\n\tSRP_OPT_INITIATOR_EXT\t= 1 << 8,\n\tSRP_OPT_CMD_SG_ENTRIES\t= 1 << 9,\n\tSRP_OPT_ALLOW_EXT_SG\t= 1 << 10,\n\tSRP_OPT_SG_TABLESIZE\t= 1 << 11,\n\tSRP_OPT_COMP_VECTOR\t= 1 << 12,\n\tSRP_OPT_TL_RETRY_COUNT\t= 1 << 13,\n\tSRP_OPT_QUEUE_SIZE\t= 1 << 14,\n\tSRP_OPT_IP_SRC\t\t= 1 << 15,\n\tSRP_OPT_IP_DEST\t\t= 1 << 16,\n\tSRP_OPT_TARGET_CAN_QUEUE= 1 << 17,\n\tSRP_OPT_MAX_IT_IU_SIZE  = 1 << 18,\n\tSRP_OPT_CH_COUNT\t= 1 << 19,\n};\n\nstatic unsigned int srp_opt_mandatory[] = {\n\tSRP_OPT_ID_EXT\t\t|\n\tSRP_OPT_IOC_GUID\t|\n\tSRP_OPT_DGID\t\t|\n\tSRP_OPT_PKEY\t\t|\n\tSRP_OPT_SERVICE_ID,\n\tSRP_OPT_ID_EXT\t\t|\n\tSRP_OPT_IOC_GUID\t|\n\tSRP_OPT_IP_DEST,\n};\n\nstatic const match_table_t srp_opt_tokens = {\n\t{ SRP_OPT_ID_EXT,\t\t\"id_ext=%s\" \t\t},\n\t{ SRP_OPT_IOC_GUID,\t\t\"ioc_guid=%s\" \t\t},\n\t{ SRP_OPT_DGID,\t\t\t\"dgid=%s\" \t\t},\n\t{ SRP_OPT_PKEY,\t\t\t\"pkey=%x\" \t\t},\n\t{ SRP_OPT_SERVICE_ID,\t\t\"service_id=%s\"\t\t},\n\t{ SRP_OPT_MAX_SECT,\t\t\"max_sect=%d\" \t\t},\n\t{ SRP_OPT_MAX_CMD_PER_LUN,\t\"max_cmd_per_lun=%d\" \t},\n\t{ SRP_OPT_TARGET_CAN_QUEUE,\t\"target_can_queue=%d\"\t},\n\t{ SRP_OPT_IO_CLASS,\t\t\"io_class=%x\"\t\t},\n\t{ SRP_OPT_INITIATOR_EXT,\t\"initiator_ext=%s\"\t},\n\t{ SRP_OPT_CMD_SG_ENTRIES,\t\"cmd_sg_entries=%u\"\t},\n\t{ SRP_OPT_ALLOW_EXT_SG,\t\t\"allow_ext_sg=%u\"\t},\n\t{ SRP_OPT_SG_TABLESIZE,\t\t\"sg_tablesize=%u\"\t},\n\t{ SRP_OPT_COMP_VECTOR,\t\t\"comp_vector=%u\"\t},\n\t{ SRP_OPT_TL_RETRY_COUNT,\t\"tl_retry_count=%u\"\t},\n\t{ SRP_OPT_QUEUE_SIZE,\t\t\"queue_size=%d\"\t\t},\n\t{ SRP_OPT_IP_SRC,\t\t\"src=%s\"\t\t},\n\t{ SRP_OPT_IP_DEST,\t\t\"dest=%s\"\t\t},\n\t{ SRP_OPT_MAX_IT_IU_SIZE,\t\"max_it_iu_size=%d\"\t},\n\t{ SRP_OPT_CH_COUNT,\t\t\"ch_count=%u\",\t\t},\n\t{ SRP_OPT_ERR,\t\t\tNULL \t\t\t}\n};\n\n \nstatic int srp_parse_in(struct net *net, struct sockaddr_storage *sa,\n\t\t\tconst char *addr_port_str, bool *has_port)\n{\n\tchar *addr_end, *addr = kstrdup(addr_port_str, GFP_KERNEL);\n\tchar *port_str;\n\tint ret;\n\n\tif (!addr)\n\t\treturn -ENOMEM;\n\tport_str = strrchr(addr, ':');\n\tif (port_str && strchr(port_str, ']'))\n\t\tport_str = NULL;\n\tif (port_str)\n\t\t*port_str++ = '\\0';\n\tif (has_port)\n\t\t*has_port = port_str != NULL;\n\tret = inet_pton_with_scope(net, AF_INET, addr, port_str, sa);\n\tif (ret && addr[0]) {\n\t\taddr_end = addr + strlen(addr) - 1;\n\t\tif (addr[0] == '[' && *addr_end == ']') {\n\t\t\t*addr_end = '\\0';\n\t\t\tret = inet_pton_with_scope(net, AF_INET6, addr + 1,\n\t\t\t\t\t\t   port_str, sa);\n\t\t}\n\t}\n\tkfree(addr);\n\tpr_debug(\"%s -> %pISpfsc\\n\", addr_port_str, sa);\n\treturn ret;\n}\n\nstatic int srp_parse_options(struct net *net, const char *buf,\n\t\t\t     struct srp_target_port *target)\n{\n\tchar *options, *sep_opt;\n\tchar *p;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tunsigned long long ull;\n\tbool has_port;\n\tint opt_mask = 0;\n\tint token;\n\tint ret = -EINVAL;\n\tint i;\n\n\toptions = kstrdup(buf, GFP_KERNEL);\n\tif (!options)\n\t\treturn -ENOMEM;\n\n\tsep_opt = options;\n\twhile ((p = strsep(&sep_opt, \",\\n\")) != NULL) {\n\t\tif (!*p)\n\t\t\tcontinue;\n\n\t\ttoken = match_token(p, srp_opt_tokens, args);\n\t\topt_mask |= token;\n\n\t\tswitch (token) {\n\t\tcase SRP_OPT_ID_EXT:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = kstrtoull(p, 16, &ull);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"invalid id_ext parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->id_ext = cpu_to_be64(ull);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_IOC_GUID:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = kstrtoull(p, 16, &ull);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"invalid ioc_guid parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->ioc_guid = cpu_to_be64(ull);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_DGID:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (strlen(p) != 32) {\n\t\t\t\tpr_warn(\"bad dest GID parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tret = hex2bin(target->ib_cm.orig_dgid.raw, p, 16);\n\t\t\tkfree(p);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_PKEY:\n\t\t\tret = match_hex(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad P_Key parameter '%s'\\n\", p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->ib_cm.pkey = cpu_to_be16(token);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_SERVICE_ID:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = kstrtoull(p, 16, &ull);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad service_id parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->ib_cm.service_id = cpu_to_be64(ull);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_IP_SRC:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = srp_parse_in(net, &target->rdma_cm.src.ss, p,\n\t\t\t\t\t   NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\tpr_warn(\"bad source parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->rdma_cm.src_specified = true;\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_IP_DEST:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = srp_parse_in(net, &target->rdma_cm.dst.ss, p,\n\t\t\t\t\t   &has_port);\n\t\t\tif (!has_port)\n\t\t\t\tret = -EINVAL;\n\t\t\tif (ret < 0) {\n\t\t\t\tpr_warn(\"bad dest parameter '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->using_rdma_cm = true;\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_MAX_SECT:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad max sect parameter '%s'\\n\", p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->scsi_host->max_sectors = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_QUEUE_SIZE:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for queue_size parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1) {\n\t\t\t\tpr_warn(\"bad queue_size parameter '%s'\\n\", p);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->scsi_host->can_queue = token;\n\t\t\ttarget->queue_size = token + SRP_RSP_SQ_SIZE +\n\t\t\t\t\t     SRP_TSK_MGMT_SQ_SIZE;\n\t\t\tif (!(opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\n\t\t\t\ttarget->scsi_host->cmd_per_lun = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_MAX_CMD_PER_LUN:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for max cmd_per_lun parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1) {\n\t\t\t\tpr_warn(\"bad max cmd_per_lun parameter '%s'\\n\",\n\t\t\t\t\tp);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->scsi_host->cmd_per_lun = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_TARGET_CAN_QUEUE:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for max target_can_queue parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1) {\n\t\t\t\tpr_warn(\"bad max target_can_queue parameter '%s'\\n\",\n\t\t\t\t\tp);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->target_can_queue = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_IO_CLASS:\n\t\t\tret = match_hex(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad IO class parameter '%s'\\n\", p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token != SRP_REV10_IB_IO_CLASS &&\n\t\t\t    token != SRP_REV16A_IB_IO_CLASS) {\n\t\t\t\tpr_warn(\"unknown IO class parameter value %x specified (use %x or %x).\\n\",\n\t\t\t\t\ttoken, SRP_REV10_IB_IO_CLASS,\n\t\t\t\t\tSRP_REV16A_IB_IO_CLASS);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->io_class = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_INITIATOR_EXT:\n\t\t\tp = match_strdup(args);\n\t\t\tif (!p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = kstrtoull(p, 16, &ull);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad initiator_ext value '%s'\\n\", p);\n\t\t\t\tkfree(p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->initiator_ext = cpu_to_be64(ull);\n\t\t\tkfree(p);\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_CMD_SG_ENTRIES:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for max cmd_sg_entries parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1 || token > 255) {\n\t\t\t\tpr_warn(\"bad max cmd_sg_entries parameter '%s'\\n\",\n\t\t\t\t\tp);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->cmd_sg_cnt = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_ALLOW_EXT_SG:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"bad allow_ext_sg parameter '%s'\\n\", p);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->allow_ext_sg = !!token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_SG_TABLESIZE:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for max sg_tablesize parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1 || token > SG_MAX_SEGMENTS) {\n\t\t\t\tpr_warn(\"bad max sg_tablesize parameter '%s'\\n\",\n\t\t\t\t\tp);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->sg_tablesize = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_COMP_VECTOR:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for comp_vector parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 0) {\n\t\t\t\tpr_warn(\"bad comp_vector parameter '%s'\\n\", p);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->comp_vector = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_TL_RETRY_COUNT:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for tl_retry_count parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 2 || token > 7) {\n\t\t\t\tpr_warn(\"bad tl_retry_count parameter '%s' (must be a number between 2 and 7)\\n\",\n\t\t\t\t\tp);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->tl_retry_count = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_MAX_IT_IU_SIZE:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for max it_iu_size parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 0) {\n\t\t\t\tpr_warn(\"bad maximum initiator to target IU size '%s'\\n\", p);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->max_it_iu_size = token;\n\t\t\tbreak;\n\n\t\tcase SRP_OPT_CH_COUNT:\n\t\t\tret = match_int(args, &token);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"match_int() failed for channel count parameter '%s', Error %d\\n\",\n\t\t\t\t\tp, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (token < 1) {\n\t\t\t\tpr_warn(\"bad channel count %s\\n\", p);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\ttarget->ch_count = token;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpr_warn(\"unknown parameter or missing value '%s' in target creation request\\n\",\n\t\t\t\tp);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(srp_opt_mandatory); i++) {\n\t\tif ((opt_mask & srp_opt_mandatory[i]) == srp_opt_mandatory[i]) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (ret)\n\t\tpr_warn(\"target creation request is missing one or more parameters\\n\");\n\n\tif (target->scsi_host->cmd_per_lun > target->scsi_host->can_queue\n\t    && (opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\n\t\tpr_warn(\"cmd_per_lun = %d > queue_size = %d\\n\",\n\t\t\ttarget->scsi_host->cmd_per_lun,\n\t\t\ttarget->scsi_host->can_queue);\n\nout:\n\tkfree(options);\n\treturn ret;\n}\n\nstatic ssize_t add_target_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct srp_host *host =\n\t\tcontainer_of(dev, struct srp_host, dev);\n\tstruct Scsi_Host *target_host;\n\tstruct srp_target_port *target;\n\tstruct srp_rdma_ch *ch;\n\tstruct srp_device *srp_dev = host->srp_dev;\n\tstruct ib_device *ibdev = srp_dev->dev;\n\tint ret, i, ch_idx;\n\tunsigned int max_sectors_per_mr, mr_per_cmd = 0;\n\tbool multich = false;\n\tuint32_t max_iu_len;\n\n\ttarget_host = scsi_host_alloc(&srp_template,\n\t\t\t\t      sizeof (struct srp_target_port));\n\tif (!target_host)\n\t\treturn -ENOMEM;\n\n\ttarget_host->transportt  = ib_srp_transport_template;\n\ttarget_host->max_channel = 0;\n\ttarget_host->max_id      = 1;\n\ttarget_host->max_lun     = -1LL;\n\ttarget_host->max_cmd_len = sizeof ((struct srp_cmd *) (void *) 0L)->cdb;\n\ttarget_host->max_segment_size = ib_dma_max_seg_size(ibdev);\n\n\tif (!(ibdev->attrs.kernel_cap_flags & IBK_SG_GAPS_REG))\n\t\ttarget_host->virt_boundary_mask = ~srp_dev->mr_page_mask;\n\n\ttarget = host_to_target(target_host);\n\n\ttarget->net\t\t= kobj_ns_grab_current(KOBJ_NS_TYPE_NET);\n\ttarget->io_class\t= SRP_REV16A_IB_IO_CLASS;\n\ttarget->scsi_host\t= target_host;\n\ttarget->srp_host\t= host;\n\ttarget->lkey\t\t= host->srp_dev->pd->local_dma_lkey;\n\ttarget->global_rkey\t= host->srp_dev->global_rkey;\n\ttarget->cmd_sg_cnt\t= cmd_sg_entries;\n\ttarget->sg_tablesize\t= indirect_sg_entries ? : cmd_sg_entries;\n\ttarget->allow_ext_sg\t= allow_ext_sg;\n\ttarget->tl_retry_count\t= 7;\n\ttarget->queue_size\t= SRP_DEFAULT_QUEUE_SIZE;\n\n\t \n\tscsi_host_get(target->scsi_host);\n\n\tret = mutex_lock_interruptible(&host->add_target_mutex);\n\tif (ret < 0)\n\t\tgoto put;\n\n\tret = srp_parse_options(target->net, buf, target);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!srp_conn_unique(target->srp_host, target)) {\n\t\tif (target->using_rdma_cm) {\n\t\t\tshost_printk(KERN_INFO, target->scsi_host,\n\t\t\t\t     PFX \"Already connected to target port with id_ext=%016llx;ioc_guid=%016llx;dest=%pIS\\n\",\n\t\t\t\t     be64_to_cpu(target->id_ext),\n\t\t\t\t     be64_to_cpu(target->ioc_guid),\n\t\t\t\t     &target->rdma_cm.dst);\n\t\t} else {\n\t\t\tshost_printk(KERN_INFO, target->scsi_host,\n\t\t\t\t     PFX \"Already connected to target port with id_ext=%016llx;ioc_guid=%016llx;initiator_ext=%016llx\\n\",\n\t\t\t\t     be64_to_cpu(target->id_ext),\n\t\t\t\t     be64_to_cpu(target->ioc_guid),\n\t\t\t\t     be64_to_cpu(target->initiator_ext));\n\t\t}\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\tif (!srp_dev->has_fr && !target->allow_ext_sg &&\n\t    target->cmd_sg_cnt < target->sg_tablesize) {\n\t\tpr_warn(\"No MR pool and no external indirect descriptors, limiting sg_tablesize to cmd_sg_cnt\\n\");\n\t\ttarget->sg_tablesize = target->cmd_sg_cnt;\n\t}\n\n\tif (srp_dev->use_fast_reg) {\n\t\tbool gaps_reg = ibdev->attrs.kernel_cap_flags &\n\t\t\t\t IBK_SG_GAPS_REG;\n\n\t\tmax_sectors_per_mr = srp_dev->max_pages_per_mr <<\n\t\t\t\t  (ilog2(srp_dev->mr_page_size) - 9);\n\t\tif (!gaps_reg) {\n\t\t\t \n\t\t\tmr_per_cmd = register_always +\n\t\t\t\t(target->scsi_host->max_sectors + 1 +\n\t\t\t\t max_sectors_per_mr - 1) / max_sectors_per_mr;\n\t\t} else {\n\t\t\tmr_per_cmd = register_always +\n\t\t\t\t(target->sg_tablesize +\n\t\t\t\t srp_dev->max_pages_per_mr - 1) /\n\t\t\t\tsrp_dev->max_pages_per_mr;\n\t\t}\n\t\tpr_debug(\"max_sectors = %u; max_pages_per_mr = %u; mr_page_size = %u; max_sectors_per_mr = %u; mr_per_cmd = %u\\n\",\n\t\t\t target->scsi_host->max_sectors, srp_dev->max_pages_per_mr, srp_dev->mr_page_size,\n\t\t\t max_sectors_per_mr, mr_per_cmd);\n\t}\n\n\ttarget_host->sg_tablesize = target->sg_tablesize;\n\ttarget->mr_pool_size = target->scsi_host->can_queue * mr_per_cmd;\n\ttarget->mr_per_cmd = mr_per_cmd;\n\ttarget->indirect_size = target->sg_tablesize *\n\t\t\t\tsizeof (struct srp_direct_buf);\n\tmax_iu_len = srp_max_it_iu_len(target->cmd_sg_cnt,\n\t\t\t\t       srp_use_imm_data,\n\t\t\t\t       target->max_it_iu_size);\n\n\tINIT_WORK(&target->tl_err_work, srp_tl_err_work);\n\tINIT_WORK(&target->remove_work, srp_remove_work);\n\tspin_lock_init(&target->lock);\n\tret = rdma_query_gid(ibdev, host->port, 0, &target->sgid);\n\tif (ret)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tif (target->ch_count == 0) {\n\t\ttarget->ch_count =\n\t\t\tmin(ch_count ?:\n\t\t\t\tmax(4 * num_online_nodes(),\n\t\t\t\t    ibdev->num_comp_vectors),\n\t\t\t\tnum_online_cpus());\n\t}\n\n\ttarget->ch = kcalloc(target->ch_count, sizeof(*target->ch),\n\t\t\t     GFP_KERNEL);\n\tif (!target->ch)\n\t\tgoto out;\n\n\tfor (ch_idx = 0; ch_idx < target->ch_count; ++ch_idx) {\n\t\tch = &target->ch[ch_idx];\n\t\tch->target = target;\n\t\tch->comp_vector = ch_idx % ibdev->num_comp_vectors;\n\t\tspin_lock_init(&ch->lock);\n\t\tINIT_LIST_HEAD(&ch->free_tx);\n\t\tret = srp_new_cm_id(ch);\n\t\tif (ret)\n\t\t\tgoto err_disconnect;\n\n\t\tret = srp_create_ch_ib(ch);\n\t\tif (ret)\n\t\t\tgoto err_disconnect;\n\n\t\tret = srp_connect_ch(ch, max_iu_len, multich);\n\t\tif (ret) {\n\t\t\tchar dst[64];\n\n\t\t\tif (target->using_rdma_cm)\n\t\t\t\tsnprintf(dst, sizeof(dst), \"%pIS\",\n\t\t\t\t\t&target->rdma_cm.dst);\n\t\t\telse\n\t\t\t\tsnprintf(dst, sizeof(dst), \"%pI6\",\n\t\t\t\t\ttarget->ib_cm.orig_dgid.raw);\n\t\t\tshost_printk(KERN_ERR, target->scsi_host,\n\t\t\t\tPFX \"Connection %d/%d to %s failed\\n\",\n\t\t\t\tch_idx,\n\t\t\t\ttarget->ch_count, dst);\n\t\t\tif (ch_idx == 0) {\n\t\t\t\tgoto free_ch;\n\t\t\t} else {\n\t\t\t\tsrp_free_ch_ib(target, ch);\n\t\t\t\ttarget->ch_count = ch - target->ch;\n\t\t\t\tgoto connected;\n\t\t\t}\n\t\t}\n\t\tmultich = true;\n\t}\n\nconnected:\n\ttarget->scsi_host->nr_hw_queues = target->ch_count;\n\n\tret = srp_add_target(host, target);\n\tif (ret)\n\t\tgoto err_disconnect;\n\n\tif (target->state != SRP_TARGET_REMOVED) {\n\t\tif (target->using_rdma_cm) {\n\t\t\tshost_printk(KERN_DEBUG, target->scsi_host, PFX\n\t\t\t\t     \"new target: id_ext %016llx ioc_guid %016llx sgid %pI6 dest %pIS\\n\",\n\t\t\t\t     be64_to_cpu(target->id_ext),\n\t\t\t\t     be64_to_cpu(target->ioc_guid),\n\t\t\t\t     target->sgid.raw, &target->rdma_cm.dst);\n\t\t} else {\n\t\t\tshost_printk(KERN_DEBUG, target->scsi_host, PFX\n\t\t\t\t     \"new target: id_ext %016llx ioc_guid %016llx pkey %04x service_id %016llx sgid %pI6 dgid %pI6\\n\",\n\t\t\t\t     be64_to_cpu(target->id_ext),\n\t\t\t\t     be64_to_cpu(target->ioc_guid),\n\t\t\t\t     be16_to_cpu(target->ib_cm.pkey),\n\t\t\t\t     be64_to_cpu(target->ib_cm.service_id),\n\t\t\t\t     target->sgid.raw,\n\t\t\t\t     target->ib_cm.orig_dgid.raw);\n\t\t}\n\t}\n\n\tret = count;\n\nout:\n\tmutex_unlock(&host->add_target_mutex);\n\nput:\n\tscsi_host_put(target->scsi_host);\n\tif (ret < 0) {\n\t\t \n\t\tif (target->state != SRP_TARGET_REMOVED)\n\t\t\tkobj_ns_drop(KOBJ_NS_TYPE_NET, target->net);\n\t\tscsi_host_put(target->scsi_host);\n\t}\n\n\treturn ret;\n\nerr_disconnect:\n\tsrp_disconnect_target(target);\n\nfree_ch:\n\tfor (i = 0; i < target->ch_count; i++) {\n\t\tch = &target->ch[i];\n\t\tsrp_free_ch_ib(target, ch);\n\t}\n\n\tkfree(target->ch);\n\tgoto out;\n}\n\nstatic DEVICE_ATTR_WO(add_target);\n\nstatic ssize_t ibdev_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct srp_host *host = container_of(dev, struct srp_host, dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", dev_name(&host->srp_dev->dev->dev));\n}\n\nstatic DEVICE_ATTR_RO(ibdev);\n\nstatic ssize_t port_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct srp_host *host = container_of(dev, struct srp_host, dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", host->port);\n}\n\nstatic DEVICE_ATTR_RO(port);\n\nstatic struct attribute *srp_class_attrs[] = {\n\t&dev_attr_add_target.attr,\n\t&dev_attr_ibdev.attr,\n\t&dev_attr_port.attr,\n\tNULL\n};\n\nstatic struct srp_host *srp_add_port(struct srp_device *device, u32 port)\n{\n\tstruct srp_host *host;\n\n\thost = kzalloc(sizeof *host, GFP_KERNEL);\n\tif (!host)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&host->target_list);\n\tspin_lock_init(&host->target_lock);\n\tmutex_init(&host->add_target_mutex);\n\thost->srp_dev = device;\n\thost->port = port;\n\n\tdevice_initialize(&host->dev);\n\thost->dev.class = &srp_class;\n\thost->dev.parent = device->dev->dev.parent;\n\tif (dev_set_name(&host->dev, \"srp-%s-%u\", dev_name(&device->dev->dev),\n\t\t\t port))\n\t\tgoto put_host;\n\tif (device_add(&host->dev))\n\t\tgoto put_host;\n\n\treturn host;\n\nput_host:\n\tdevice_del(&host->dev);\n\tput_device(&host->dev);\n\treturn NULL;\n}\n\nstatic void srp_rename_dev(struct ib_device *device, void *client_data)\n{\n\tstruct srp_device *srp_dev = client_data;\n\tstruct srp_host *host, *tmp_host;\n\n\tlist_for_each_entry_safe(host, tmp_host, &srp_dev->dev_list, list) {\n\t\tchar name[IB_DEVICE_NAME_MAX + 8];\n\n\t\tsnprintf(name, sizeof(name), \"srp-%s-%u\",\n\t\t\t dev_name(&device->dev), host->port);\n\t\tdevice_rename(&host->dev, name);\n\t}\n}\n\nstatic int srp_add_one(struct ib_device *device)\n{\n\tstruct srp_device *srp_dev;\n\tstruct ib_device_attr *attr = &device->attrs;\n\tstruct srp_host *host;\n\tint mr_page_shift;\n\tu32 p;\n\tu64 max_pages_per_mr;\n\tunsigned int flags = 0;\n\n\tsrp_dev = kzalloc(sizeof(*srp_dev), GFP_KERNEL);\n\tif (!srp_dev)\n\t\treturn -ENOMEM;\n\n\t \n\tmr_page_shift\t\t= max(12, ffs(attr->page_size_cap) - 1);\n\tsrp_dev->mr_page_size\t= 1 << mr_page_shift;\n\tsrp_dev->mr_page_mask\t= ~((u64) srp_dev->mr_page_size - 1);\n\tmax_pages_per_mr\t= attr->max_mr_size;\n\tdo_div(max_pages_per_mr, srp_dev->mr_page_size);\n\tpr_debug(\"%s: %llu / %u = %llu <> %u\\n\", __func__,\n\t\t attr->max_mr_size, srp_dev->mr_page_size,\n\t\t max_pages_per_mr, SRP_MAX_PAGES_PER_MR);\n\tsrp_dev->max_pages_per_mr = min_t(u64, SRP_MAX_PAGES_PER_MR,\n\t\t\t\t\t  max_pages_per_mr);\n\n\tsrp_dev->has_fr = (attr->device_cap_flags &\n\t\t\t   IB_DEVICE_MEM_MGT_EXTENSIONS);\n\tif (!never_register && !srp_dev->has_fr)\n\t\tdev_warn(&device->dev, \"FR is not supported\\n\");\n\telse if (!never_register &&\n\t\t attr->max_mr_size >= 2 * srp_dev->mr_page_size)\n\t\tsrp_dev->use_fast_reg = srp_dev->has_fr;\n\n\tif (never_register || !register_always || !srp_dev->has_fr)\n\t\tflags |= IB_PD_UNSAFE_GLOBAL_RKEY;\n\n\tif (srp_dev->use_fast_reg) {\n\t\tsrp_dev->max_pages_per_mr =\n\t\t\tmin_t(u32, srp_dev->max_pages_per_mr,\n\t\t\t      attr->max_fast_reg_page_list_len);\n\t}\n\tsrp_dev->mr_max_size\t= srp_dev->mr_page_size *\n\t\t\t\t   srp_dev->max_pages_per_mr;\n\tpr_debug(\"%s: mr_page_shift = %d, device->max_mr_size = %#llx, device->max_fast_reg_page_list_len = %u, max_pages_per_mr = %d, mr_max_size = %#x\\n\",\n\t\t dev_name(&device->dev), mr_page_shift, attr->max_mr_size,\n\t\t attr->max_fast_reg_page_list_len,\n\t\t srp_dev->max_pages_per_mr, srp_dev->mr_max_size);\n\n\tINIT_LIST_HEAD(&srp_dev->dev_list);\n\n\tsrp_dev->dev = device;\n\tsrp_dev->pd  = ib_alloc_pd(device, flags);\n\tif (IS_ERR(srp_dev->pd)) {\n\t\tint ret = PTR_ERR(srp_dev->pd);\n\n\t\tkfree(srp_dev);\n\t\treturn ret;\n\t}\n\n\tif (flags & IB_PD_UNSAFE_GLOBAL_RKEY) {\n\t\tsrp_dev->global_rkey = srp_dev->pd->unsafe_global_rkey;\n\t\tWARN_ON_ONCE(srp_dev->global_rkey == 0);\n\t}\n\n\trdma_for_each_port (device, p) {\n\t\thost = srp_add_port(srp_dev, p);\n\t\tif (host)\n\t\t\tlist_add_tail(&host->list, &srp_dev->dev_list);\n\t}\n\n\tib_set_client_data(device, &srp_client, srp_dev);\n\treturn 0;\n}\n\nstatic void srp_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct srp_device *srp_dev;\n\tstruct srp_host *host, *tmp_host;\n\tstruct srp_target_port *target;\n\n\tsrp_dev = client_data;\n\n\tlist_for_each_entry_safe(host, tmp_host, &srp_dev->dev_list, list) {\n\t\t \n\t\tdevice_del(&host->dev);\n\n\t\t \n\t\tspin_lock(&host->target_lock);\n\t\tlist_for_each_entry(target, &host->target_list, list)\n\t\t\tsrp_queue_remove_work(target);\n\t\tspin_unlock(&host->target_lock);\n\n\t\t \n\t\tflush_workqueue(srp_remove_wq);\n\n\t\tput_device(&host->dev);\n\t}\n\n\tib_dealloc_pd(srp_dev->pd);\n\n\tkfree(srp_dev);\n}\n\nstatic struct srp_function_template ib_srp_transport_functions = {\n\t.has_rport_state\t = true,\n\t.reset_timer_if_blocked\t = true,\n\t.reconnect_delay\t = &srp_reconnect_delay,\n\t.fast_io_fail_tmo\t = &srp_fast_io_fail_tmo,\n\t.dev_loss_tmo\t\t = &srp_dev_loss_tmo,\n\t.reconnect\t\t = srp_rport_reconnect,\n\t.rport_delete\t\t = srp_rport_delete,\n\t.terminate_rport_io\t = srp_terminate_io,\n};\n\nstatic int __init srp_init_module(void)\n{\n\tint ret;\n\n\tBUILD_BUG_ON(sizeof(struct srp_aer_req) != 36);\n\tBUILD_BUG_ON(sizeof(struct srp_cmd) != 48);\n\tBUILD_BUG_ON(sizeof(struct srp_imm_buf) != 4);\n\tBUILD_BUG_ON(sizeof(struct srp_indirect_buf) != 20);\n\tBUILD_BUG_ON(sizeof(struct srp_login_req) != 64);\n\tBUILD_BUG_ON(sizeof(struct srp_login_req_rdma) != 56);\n\tBUILD_BUG_ON(sizeof(struct srp_rsp) != 36);\n\n\tif (srp_sg_tablesize) {\n\t\tpr_warn(\"srp_sg_tablesize is deprecated, please use cmd_sg_entries\\n\");\n\t\tif (!cmd_sg_entries)\n\t\t\tcmd_sg_entries = srp_sg_tablesize;\n\t}\n\n\tif (!cmd_sg_entries)\n\t\tcmd_sg_entries = SRP_DEF_SG_TABLESIZE;\n\n\tif (cmd_sg_entries > 255) {\n\t\tpr_warn(\"Clamping cmd_sg_entries to 255\\n\");\n\t\tcmd_sg_entries = 255;\n\t}\n\n\tif (!indirect_sg_entries)\n\t\tindirect_sg_entries = cmd_sg_entries;\n\telse if (indirect_sg_entries < cmd_sg_entries) {\n\t\tpr_warn(\"Bumping up indirect_sg_entries to match cmd_sg_entries (%u)\\n\",\n\t\t\tcmd_sg_entries);\n\t\tindirect_sg_entries = cmd_sg_entries;\n\t}\n\n\tif (indirect_sg_entries > SG_MAX_SEGMENTS) {\n\t\tpr_warn(\"Clamping indirect_sg_entries to %u\\n\",\n\t\t\tSG_MAX_SEGMENTS);\n\t\tindirect_sg_entries = SG_MAX_SEGMENTS;\n\t}\n\n\tsrp_remove_wq = create_workqueue(\"srp_remove\");\n\tif (!srp_remove_wq) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = -ENOMEM;\n\tib_srp_transport_template =\n\t\tsrp_attach_transport(&ib_srp_transport_functions);\n\tif (!ib_srp_transport_template)\n\t\tgoto destroy_wq;\n\n\tret = class_register(&srp_class);\n\tif (ret) {\n\t\tpr_err(\"couldn't register class infiniband_srp\\n\");\n\t\tgoto release_tr;\n\t}\n\n\tib_sa_register_client(&srp_sa_client);\n\n\tret = ib_register_client(&srp_client);\n\tif (ret) {\n\t\tpr_err(\"couldn't register IB client\\n\");\n\t\tgoto unreg_sa;\n\t}\n\nout:\n\treturn ret;\n\nunreg_sa:\n\tib_sa_unregister_client(&srp_sa_client);\n\tclass_unregister(&srp_class);\n\nrelease_tr:\n\tsrp_release_transport(ib_srp_transport_template);\n\ndestroy_wq:\n\tdestroy_workqueue(srp_remove_wq);\n\tgoto out;\n}\n\nstatic void __exit srp_cleanup_module(void)\n{\n\tib_unregister_client(&srp_client);\n\tib_sa_unregister_client(&srp_sa_client);\n\tclass_unregister(&srp_class);\n\tsrp_release_transport(ib_srp_transport_template);\n\tdestroy_workqueue(srp_remove_wq);\n}\n\nmodule_init(srp_init_module);\nmodule_exit(srp_cleanup_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}