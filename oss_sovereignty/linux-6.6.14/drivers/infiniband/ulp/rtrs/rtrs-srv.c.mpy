{
  "module_name": "rtrs-srv.c",
  "hash_id": "9c6a29d7d3c348c0083aefb6d11460d85f2af6bbc395c35d22d2d3102d2ec5c1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/rtrs/rtrs-srv.c",
  "human_readable_source": "\n \n\n#undef pr_fmt\n#define pr_fmt(fmt) KBUILD_MODNAME \" L\" __stringify(__LINE__) \": \" fmt\n\n#include <linux/module.h>\n\n#include \"rtrs-srv.h\"\n#include \"rtrs-log.h\"\n#include <rdma/ib_cm.h>\n#include <rdma/ib_verbs.h>\n#include \"rtrs-srv-trace.h\"\n\nMODULE_DESCRIPTION(\"RDMA Transport Server\");\nMODULE_LICENSE(\"GPL\");\n\n \n#define DEFAULT_MAX_CHUNK_SIZE (128 << 10)\n#define DEFAULT_SESS_QUEUE_DEPTH 512\n#define MAX_HDR_SIZE PAGE_SIZE\n\nstatic struct rtrs_rdma_dev_pd dev_pd;\nconst struct class rtrs_dev_class = {\n\t.name = \"rtrs-server\",\n};\nstatic struct rtrs_srv_ib_ctx ib_ctx;\n\nstatic int __read_mostly max_chunk_size = DEFAULT_MAX_CHUNK_SIZE;\nstatic int __read_mostly sess_queue_depth = DEFAULT_SESS_QUEUE_DEPTH;\n\nstatic bool always_invalidate = true;\nmodule_param(always_invalidate, bool, 0444);\nMODULE_PARM_DESC(always_invalidate,\n\t\t \"Invalidate memory registration for contiguous memory regions before accessing.\");\n\nmodule_param_named(max_chunk_size, max_chunk_size, int, 0444);\nMODULE_PARM_DESC(max_chunk_size,\n\t\t \"Max size for each IO request, when change the unit is in byte (default: \"\n\t\t __stringify(DEFAULT_MAX_CHUNK_SIZE) \"KB)\");\n\nmodule_param_named(sess_queue_depth, sess_queue_depth, int, 0444);\nMODULE_PARM_DESC(sess_queue_depth,\n\t\t \"Number of buffers for pending I/O requests to allocate per session. Maximum: \"\n\t\t __stringify(MAX_SESS_QUEUE_DEPTH) \" (default: \"\n\t\t __stringify(DEFAULT_SESS_QUEUE_DEPTH) \")\");\n\nstatic cpumask_t cq_affinity_mask = { CPU_BITS_ALL };\n\nstatic struct workqueue_struct *rtrs_wq;\n\nstatic inline struct rtrs_srv_con *to_srv_con(struct rtrs_con *c)\n{\n\treturn container_of(c, struct rtrs_srv_con, c);\n}\n\nstatic bool rtrs_srv_change_state(struct rtrs_srv_path *srv_path,\n\t\t\t\t  enum rtrs_srv_state new_state)\n{\n\tenum rtrs_srv_state old_state;\n\tbool changed = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&srv_path->state_lock, flags);\n\told_state = srv_path->state;\n\tswitch (new_state) {\n\tcase RTRS_SRV_CONNECTED:\n\t\tif (old_state == RTRS_SRV_CONNECTING)\n\t\t\tchanged = true;\n\t\tbreak;\n\tcase RTRS_SRV_CLOSING:\n\t\tif (old_state == RTRS_SRV_CONNECTING ||\n\t\t    old_state == RTRS_SRV_CONNECTED)\n\t\t\tchanged = true;\n\t\tbreak;\n\tcase RTRS_SRV_CLOSED:\n\t\tif (old_state == RTRS_SRV_CLOSING)\n\t\t\tchanged = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (changed)\n\t\tsrv_path->state = new_state;\n\tspin_unlock_irqrestore(&srv_path->state_lock, flags);\n\n\treturn changed;\n}\n\nstatic void free_id(struct rtrs_srv_op *id)\n{\n\tif (!id)\n\t\treturn;\n\tkfree(id);\n}\n\nstatic void rtrs_srv_free_ops_ids(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tint i;\n\n\tif (srv_path->ops_ids) {\n\t\tfor (i = 0; i < srv->queue_depth; i++)\n\t\t\tfree_id(srv_path->ops_ids[i]);\n\t\tkfree(srv_path->ops_ids);\n\t\tsrv_path->ops_ids = NULL;\n\t}\n}\n\nstatic void rtrs_srv_rdma_done(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic struct ib_cqe io_comp_cqe = {\n\t.done = rtrs_srv_rdma_done\n};\n\nstatic inline void rtrs_srv_inflight_ref_release(struct percpu_ref *ref)\n{\n\tstruct rtrs_srv_path *srv_path = container_of(ref,\n\t\t\t\t\t\t      struct rtrs_srv_path,\n\t\t\t\t\t\t      ids_inflight_ref);\n\n\tpercpu_ref_exit(&srv_path->ids_inflight_ref);\n\tcomplete(&srv_path->complete_done);\n}\n\nstatic int rtrs_srv_alloc_ops_ids(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_srv_op *id;\n\tint i, ret;\n\n\tsrv_path->ops_ids = kcalloc(srv->queue_depth,\n\t\t\t\t    sizeof(*srv_path->ops_ids),\n\t\t\t\t    GFP_KERNEL);\n\tif (!srv_path->ops_ids)\n\t\tgoto err;\n\n\tfor (i = 0; i < srv->queue_depth; ++i) {\n\t\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\t\tif (!id)\n\t\t\tgoto err;\n\n\t\tsrv_path->ops_ids[i] = id;\n\t}\n\n\tret = percpu_ref_init(&srv_path->ids_inflight_ref,\n\t\t\t      rtrs_srv_inflight_ref_release, 0, GFP_KERNEL);\n\tif (ret) {\n\t\tpr_err(\"Percpu reference init failed\\n\");\n\t\tgoto err;\n\t}\n\tinit_completion(&srv_path->complete_done);\n\n\treturn 0;\n\nerr:\n\trtrs_srv_free_ops_ids(srv_path);\n\treturn -ENOMEM;\n}\n\nstatic inline void rtrs_srv_get_ops_ids(struct rtrs_srv_path *srv_path)\n{\n\tpercpu_ref_get(&srv_path->ids_inflight_ref);\n}\n\nstatic inline void rtrs_srv_put_ops_ids(struct rtrs_srv_path *srv_path)\n{\n\tpercpu_ref_put(&srv_path->ids_inflight_ref);\n}\n\nstatic void rtrs_srv_reg_mr_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context);\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(s, \"REG MR failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tclose_path(srv_path);\n\t\treturn;\n\t}\n}\n\nstatic struct ib_cqe local_reg_cqe = {\n\t.done = rtrs_srv_reg_mr_done\n};\n\nstatic int rdma_write_sg(struct rtrs_srv_op *id)\n{\n\tstruct rtrs_path *s = id->con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tdma_addr_t dma_addr = srv_path->dma_addr[id->msg_id];\n\tstruct rtrs_srv_mr *srv_mr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_rdma_wr imm_wr;\n\tstruct ib_rdma_wr *wr = NULL;\n\tenum ib_send_flags flags;\n\tsize_t sg_cnt;\n\tint err, offset;\n\tbool need_inval;\n\tu32 rkey = 0;\n\tstruct ib_reg_wr rwr;\n\tstruct ib_sge *plist;\n\tstruct ib_sge list;\n\n\tsg_cnt = le16_to_cpu(id->rd_msg->sg_cnt);\n\tneed_inval = le16_to_cpu(id->rd_msg->flags) & RTRS_MSG_NEED_INVAL_F;\n\tif (sg_cnt != 1)\n\t\treturn -EINVAL;\n\n\toffset = 0;\n\n\twr\t\t= &id->tx_wr;\n\tplist\t\t= &id->tx_sg;\n\tplist->addr\t= dma_addr + offset;\n\tplist->length\t= le32_to_cpu(id->rd_msg->desc[0].len);\n\n\t \n\tif (plist->length == 0) {\n\t\trtrs_err(s, \"Invalid RDMA-Write sg list length 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tplist->lkey = srv_path->s.dev->ib_pd->local_dma_lkey;\n\toffset += plist->length;\n\n\twr->wr.sg_list\t= plist;\n\twr->wr.num_sge\t= 1;\n\twr->remote_addr\t= le64_to_cpu(id->rd_msg->desc[0].addr);\n\twr->rkey\t= le32_to_cpu(id->rd_msg->desc[0].key);\n\tif (rkey == 0)\n\t\trkey = wr->rkey;\n\telse\n\t\t \n\t\tWARN_ON_ONCE(rkey != wr->rkey);\n\n\twr->wr.opcode = IB_WR_RDMA_WRITE;\n\twr->wr.wr_cqe   = &io_comp_cqe;\n\twr->wr.ex.imm_data = 0;\n\twr->wr.send_flags  = 0;\n\n\tif (need_inval && always_invalidate) {\n\t\twr->wr.next = &rwr.wr;\n\t\trwr.wr.next = &inv_wr;\n\t\tinv_wr.next = &imm_wr.wr;\n\t} else if (always_invalidate) {\n\t\twr->wr.next = &rwr.wr;\n\t\trwr.wr.next = &imm_wr.wr;\n\t} else if (need_inval) {\n\t\twr->wr.next = &inv_wr;\n\t\tinv_wr.next = &imm_wr.wr;\n\t} else {\n\t\twr->wr.next = &imm_wr.wr;\n\t}\n\t \n\tflags = (atomic_inc_return(&id->con->c.wr_cnt) % s->signal_interval) ?\n\t\t0 : IB_SEND_SIGNALED;\n\n\tif (need_inval) {\n\t\tinv_wr.sg_list = NULL;\n\t\tinv_wr.num_sge = 0;\n\t\tinv_wr.opcode = IB_WR_SEND_WITH_INV;\n\t\tinv_wr.wr_cqe   = &io_comp_cqe;\n\t\tinv_wr.send_flags = 0;\n\t\tinv_wr.ex.invalidate_rkey = rkey;\n\t}\n\n\timm_wr.wr.next = NULL;\n\tif (always_invalidate) {\n\t\tstruct rtrs_msg_rkey_rsp *msg;\n\n\t\tsrv_mr = &srv_path->mrs[id->msg_id];\n\t\trwr.wr.opcode = IB_WR_REG_MR;\n\t\trwr.wr.wr_cqe = &local_reg_cqe;\n\t\trwr.wr.num_sge = 0;\n\t\trwr.mr = srv_mr->mr;\n\t\trwr.wr.send_flags = 0;\n\t\trwr.key = srv_mr->mr->rkey;\n\t\trwr.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t      IB_ACCESS_REMOTE_WRITE);\n\t\tmsg = srv_mr->iu->buf;\n\t\tmsg->buf_id = cpu_to_le16(id->msg_id);\n\t\tmsg->type = cpu_to_le16(RTRS_MSG_RKEY_RSP);\n\t\tmsg->rkey = cpu_to_le32(srv_mr->mr->rkey);\n\n\t\tlist.addr   = srv_mr->iu->dma_addr;\n\t\tlist.length = sizeof(*msg);\n\t\tlist.lkey   = srv_path->s.dev->ib_pd->local_dma_lkey;\n\t\timm_wr.wr.sg_list = &list;\n\t\timm_wr.wr.num_sge = 1;\n\t\timm_wr.wr.opcode = IB_WR_SEND_WITH_IMM;\n\t\tib_dma_sync_single_for_device(srv_path->s.dev->ib_dev,\n\t\t\t\t\t      srv_mr->iu->dma_addr,\n\t\t\t\t\t      srv_mr->iu->size, DMA_TO_DEVICE);\n\t} else {\n\t\timm_wr.wr.sg_list = NULL;\n\t\timm_wr.wr.num_sge = 0;\n\t\timm_wr.wr.opcode = IB_WR_RDMA_WRITE_WITH_IMM;\n\t}\n\timm_wr.wr.send_flags = flags;\n\timm_wr.wr.ex.imm_data = cpu_to_be32(rtrs_to_io_rsp_imm(id->msg_id,\n\t\t\t\t\t\t\t     0, need_inval));\n\n\timm_wr.wr.wr_cqe   = &io_comp_cqe;\n\tib_dma_sync_single_for_device(srv_path->s.dev->ib_dev, dma_addr,\n\t\t\t\t      offset, DMA_BIDIRECTIONAL);\n\n\terr = ib_post_send(id->con->c.qp, &id->tx_wr.wr, NULL);\n\tif (err)\n\t\trtrs_err(s,\n\t\t\t  \"Posting RDMA-Write-Request to QP failed, err: %d\\n\",\n\t\t\t  err);\n\n\treturn err;\n}\n\n \nstatic int send_io_resp_imm(struct rtrs_srv_con *con, struct rtrs_srv_op *id,\n\t\t\t    int errno)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct ib_send_wr inv_wr, *wr = NULL;\n\tstruct ib_rdma_wr imm_wr;\n\tstruct ib_reg_wr rwr;\n\tstruct rtrs_srv_mr *srv_mr;\n\tbool need_inval = false;\n\tenum ib_send_flags flags;\n\tu32 imm;\n\tint err;\n\n\tif (id->dir == READ) {\n\t\tstruct rtrs_msg_rdma_read *rd_msg = id->rd_msg;\n\t\tsize_t sg_cnt;\n\n\t\tneed_inval = le16_to_cpu(rd_msg->flags) &\n\t\t\t\tRTRS_MSG_NEED_INVAL_F;\n\t\tsg_cnt = le16_to_cpu(rd_msg->sg_cnt);\n\n\t\tif (need_inval) {\n\t\t\tif (sg_cnt) {\n\t\t\t\tinv_wr.wr_cqe   = &io_comp_cqe;\n\t\t\t\tinv_wr.sg_list = NULL;\n\t\t\t\tinv_wr.num_sge = 0;\n\t\t\t\tinv_wr.opcode = IB_WR_SEND_WITH_INV;\n\t\t\t\tinv_wr.send_flags = 0;\n\t\t\t\t \n\t\t\t\tinv_wr.ex.invalidate_rkey =\n\t\t\t\t\tle32_to_cpu(rd_msg->desc[0].key);\n\t\t\t} else {\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\tneed_inval = false;\n\t\t\t}\n\t\t}\n\t}\n\n\ttrace_send_io_resp_imm(id, need_inval, always_invalidate, errno);\n\n\tif (need_inval && always_invalidate) {\n\t\twr = &inv_wr;\n\t\tinv_wr.next = &rwr.wr;\n\t\trwr.wr.next = &imm_wr.wr;\n\t} else if (always_invalidate) {\n\t\twr = &rwr.wr;\n\t\trwr.wr.next = &imm_wr.wr;\n\t} else if (need_inval) {\n\t\twr = &inv_wr;\n\t\tinv_wr.next = &imm_wr.wr;\n\t} else {\n\t\twr = &imm_wr.wr;\n\t}\n\t \n\tflags = (atomic_inc_return(&con->c.wr_cnt) % s->signal_interval) ?\n\t\t0 : IB_SEND_SIGNALED;\n\timm = rtrs_to_io_rsp_imm(id->msg_id, errno, need_inval);\n\timm_wr.wr.next = NULL;\n\tif (always_invalidate) {\n\t\tstruct ib_sge list;\n\t\tstruct rtrs_msg_rkey_rsp *msg;\n\n\t\tsrv_mr = &srv_path->mrs[id->msg_id];\n\t\trwr.wr.next = &imm_wr.wr;\n\t\trwr.wr.opcode = IB_WR_REG_MR;\n\t\trwr.wr.wr_cqe = &local_reg_cqe;\n\t\trwr.wr.num_sge = 0;\n\t\trwr.wr.send_flags = 0;\n\t\trwr.mr = srv_mr->mr;\n\t\trwr.key = srv_mr->mr->rkey;\n\t\trwr.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t      IB_ACCESS_REMOTE_WRITE);\n\t\tmsg = srv_mr->iu->buf;\n\t\tmsg->buf_id = cpu_to_le16(id->msg_id);\n\t\tmsg->type = cpu_to_le16(RTRS_MSG_RKEY_RSP);\n\t\tmsg->rkey = cpu_to_le32(srv_mr->mr->rkey);\n\n\t\tlist.addr   = srv_mr->iu->dma_addr;\n\t\tlist.length = sizeof(*msg);\n\t\tlist.lkey   = srv_path->s.dev->ib_pd->local_dma_lkey;\n\t\timm_wr.wr.sg_list = &list;\n\t\timm_wr.wr.num_sge = 1;\n\t\timm_wr.wr.opcode = IB_WR_SEND_WITH_IMM;\n\t\tib_dma_sync_single_for_device(srv_path->s.dev->ib_dev,\n\t\t\t\t\t      srv_mr->iu->dma_addr,\n\t\t\t\t\t      srv_mr->iu->size, DMA_TO_DEVICE);\n\t} else {\n\t\timm_wr.wr.sg_list = NULL;\n\t\timm_wr.wr.num_sge = 0;\n\t\timm_wr.wr.opcode = IB_WR_RDMA_WRITE_WITH_IMM;\n\t}\n\timm_wr.wr.send_flags = flags;\n\timm_wr.wr.wr_cqe   = &io_comp_cqe;\n\n\timm_wr.wr.ex.imm_data = cpu_to_be32(imm);\n\n\terr = ib_post_send(id->con->c.qp, wr, NULL);\n\tif (err)\n\t\trtrs_err_rl(s, \"Posting RDMA-Reply to QP failed, err: %d\\n\",\n\t\t\t     err);\n\n\treturn err;\n}\n\nvoid close_path(struct rtrs_srv_path *srv_path)\n{\n\tif (rtrs_srv_change_state(srv_path, RTRS_SRV_CLOSING))\n\t\tqueue_work(rtrs_wq, &srv_path->close_work);\n\tWARN_ON(srv_path->state != RTRS_SRV_CLOSING);\n}\n\nstatic inline const char *rtrs_srv_state_str(enum rtrs_srv_state state)\n{\n\tswitch (state) {\n\tcase RTRS_SRV_CONNECTING:\n\t\treturn \"RTRS_SRV_CONNECTING\";\n\tcase RTRS_SRV_CONNECTED:\n\t\treturn \"RTRS_SRV_CONNECTED\";\n\tcase RTRS_SRV_CLOSING:\n\t\treturn \"RTRS_SRV_CLOSING\";\n\tcase RTRS_SRV_CLOSED:\n\t\treturn \"RTRS_SRV_CLOSED\";\n\tdefault:\n\t\treturn \"UNKNOWN\";\n\t}\n}\n\n \nbool rtrs_srv_resp_rdma(struct rtrs_srv_op *id, int status)\n{\n\tstruct rtrs_srv_path *srv_path;\n\tstruct rtrs_srv_con *con;\n\tstruct rtrs_path *s;\n\tint err;\n\n\tif (WARN_ON(!id))\n\t\treturn true;\n\n\tcon = id->con;\n\ts = con->c.path;\n\tsrv_path = to_srv_path(s);\n\n\tid->status = status;\n\n\tif (srv_path->state != RTRS_SRV_CONNECTED) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Sending I/O response failed,  server path %s is disconnected, path state %s\\n\",\n\t\t\t    kobject_name(&srv_path->kobj),\n\t\t\t    rtrs_srv_state_str(srv_path->state));\n\t\tgoto out;\n\t}\n\tif (always_invalidate) {\n\t\tstruct rtrs_srv_mr *mr = &srv_path->mrs[id->msg_id];\n\n\t\tib_update_fast_reg_key(mr->mr, ib_inc_rkey(mr->mr->rkey));\n\t}\n\tif (atomic_sub_return(1, &con->c.sq_wr_avail) < 0) {\n\t\trtrs_err(s, \"IB send queue full: srv_path=%s cid=%d\\n\",\n\t\t\t kobject_name(&srv_path->kobj),\n\t\t\t con->c.cid);\n\t\tatomic_add(1, &con->c.sq_wr_avail);\n\t\tspin_lock(&con->rsp_wr_wait_lock);\n\t\tlist_add_tail(&id->wait_list, &con->rsp_wr_wait_list);\n\t\tspin_unlock(&con->rsp_wr_wait_lock);\n\t\treturn false;\n\t}\n\n\tif (status || id->dir == WRITE || !id->rd_msg->sg_cnt)\n\t\terr = send_io_resp_imm(con, id, status);\n\telse\n\t\terr = rdma_write_sg(id);\n\n\tif (err) {\n\t\trtrs_err_rl(s, \"IO response failed: %d: srv_path=%s\\n\", err,\n\t\t\t    kobject_name(&srv_path->kobj));\n\t\tclose_path(srv_path);\n\t}\nout:\n\trtrs_srv_put_ops_ids(srv_path);\n\treturn true;\n}\nEXPORT_SYMBOL(rtrs_srv_resp_rdma);\n\n \nvoid rtrs_srv_set_sess_priv(struct rtrs_srv_sess *srv, void *priv)\n{\n\tsrv->priv = priv;\n}\nEXPORT_SYMBOL(rtrs_srv_set_sess_priv);\n\nstatic void unmap_cont_bufs(struct rtrs_srv_path *srv_path)\n{\n\tint i;\n\n\tfor (i = 0; i < srv_path->mrs_num; i++) {\n\t\tstruct rtrs_srv_mr *srv_mr;\n\n\t\tsrv_mr = &srv_path->mrs[i];\n\n\t\tif (always_invalidate)\n\t\t\trtrs_iu_free(srv_mr->iu, srv_path->s.dev->ib_dev, 1);\n\n\t\tib_dereg_mr(srv_mr->mr);\n\t\tib_dma_unmap_sg(srv_path->s.dev->ib_dev, srv_mr->sgt.sgl,\n\t\t\t\tsrv_mr->sgt.nents, DMA_BIDIRECTIONAL);\n\t\tsg_free_table(&srv_mr->sgt);\n\t}\n\tkfree(srv_path->mrs);\n}\n\nstatic int map_cont_bufs(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_path *ss = &srv_path->s;\n\tint i, err, mrs_num;\n\tunsigned int chunk_bits;\n\tint chunks_per_mr = 1;\n\tstruct ib_mr *mr;\n\tstruct sg_table *sgt;\n\n\t \n\tif (always_invalidate) {\n\t\t \n\t\tmrs_num = srv->queue_depth;\n\t} else {\n\t\tchunks_per_mr =\n\t\t\tsrv_path->s.dev->ib_dev->attrs.max_fast_reg_page_list_len;\n\t\tmrs_num = DIV_ROUND_UP(srv->queue_depth, chunks_per_mr);\n\t\tchunks_per_mr = DIV_ROUND_UP(srv->queue_depth, mrs_num);\n\t}\n\n\tsrv_path->mrs = kcalloc(mrs_num, sizeof(*srv_path->mrs), GFP_KERNEL);\n\tif (!srv_path->mrs)\n\t\treturn -ENOMEM;\n\n\tfor (srv_path->mrs_num = 0; srv_path->mrs_num < mrs_num;\n\t     srv_path->mrs_num++) {\n\t\tstruct rtrs_srv_mr *srv_mr = &srv_path->mrs[srv_path->mrs_num];\n\t\tstruct scatterlist *s;\n\t\tint nr, nr_sgt, chunks;\n\n\t\tsgt = &srv_mr->sgt;\n\t\tchunks = chunks_per_mr * srv_path->mrs_num;\n\t\tif (!always_invalidate)\n\t\t\tchunks_per_mr = min_t(int, chunks_per_mr,\n\t\t\t\t\t      srv->queue_depth - chunks);\n\n\t\terr = sg_alloc_table(sgt, chunks_per_mr, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\tfor_each_sg(sgt->sgl, s, chunks_per_mr, i)\n\t\t\tsg_set_page(s, srv->chunks[chunks + i],\n\t\t\t\t    max_chunk_size, 0);\n\n\t\tnr_sgt = ib_dma_map_sg(srv_path->s.dev->ib_dev, sgt->sgl,\n\t\t\t\t   sgt->nents, DMA_BIDIRECTIONAL);\n\t\tif (!nr_sgt) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free_sg;\n\t\t}\n\t\tmr = ib_alloc_mr(srv_path->s.dev->ib_pd, IB_MR_TYPE_MEM_REG,\n\t\t\t\t nr_sgt);\n\t\tif (IS_ERR(mr)) {\n\t\t\terr = PTR_ERR(mr);\n\t\t\tgoto unmap_sg;\n\t\t}\n\t\tnr = ib_map_mr_sg(mr, sgt->sgl, nr_sgt,\n\t\t\t\t  NULL, max_chunk_size);\n\t\tif (nr != nr_sgt) {\n\t\t\terr = nr < 0 ? nr : -EINVAL;\n\t\t\tgoto dereg_mr;\n\t\t}\n\n\t\tif (always_invalidate) {\n\t\t\tsrv_mr->iu = rtrs_iu_alloc(1,\n\t\t\t\t\tsizeof(struct rtrs_msg_rkey_rsp),\n\t\t\t\t\tGFP_KERNEL, srv_path->s.dev->ib_dev,\n\t\t\t\t\tDMA_TO_DEVICE, rtrs_srv_rdma_done);\n\t\t\tif (!srv_mr->iu) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\trtrs_err(ss, \"rtrs_iu_alloc(), err: %d\\n\", err);\n\t\t\t\tgoto dereg_mr;\n\t\t\t}\n\t\t}\n\t\t \n\t\tfor_each_sg(sgt->sgl, s, nr_sgt, i)\n\t\t\tsrv_path->dma_addr[chunks + i] = sg_dma_address(s);\n\n\t\tib_update_fast_reg_key(mr, ib_inc_rkey(mr->rkey));\n\t\tsrv_mr->mr = mr;\n\t}\n\n\tchunk_bits = ilog2(srv->queue_depth - 1) + 1;\n\tsrv_path->mem_bits = (MAX_IMM_PAYL_BITS - chunk_bits);\n\n\treturn 0;\n\ndereg_mr:\n\tib_dereg_mr(mr);\nunmap_sg:\n\tib_dma_unmap_sg(srv_path->s.dev->ib_dev, sgt->sgl,\n\t\t\tsgt->nents, DMA_BIDIRECTIONAL);\nfree_sg:\n\tsg_free_table(sgt);\nerr:\n\tunmap_cont_bufs(srv_path);\n\n\treturn err;\n}\n\nstatic void rtrs_srv_hb_err_handler(struct rtrs_con *c)\n{\n\tclose_path(to_srv_path(c->path));\n}\n\nstatic void rtrs_srv_init_hb(struct rtrs_srv_path *srv_path)\n{\n\trtrs_init_hb(&srv_path->s, &io_comp_cqe,\n\t\t      RTRS_HB_INTERVAL_MS,\n\t\t      RTRS_HB_MISSED_MAX,\n\t\t      rtrs_srv_hb_err_handler,\n\t\t      rtrs_wq);\n}\n\nstatic void rtrs_srv_start_hb(struct rtrs_srv_path *srv_path)\n{\n\trtrs_start_hb(&srv_path->s);\n}\n\nstatic void rtrs_srv_stop_hb(struct rtrs_srv_path *srv_path)\n{\n\trtrs_stop_hb(&srv_path->s);\n}\n\nstatic void rtrs_srv_info_rsp_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context);\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_iu *iu;\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\trtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(s, \"Sess info response send failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tclose_path(srv_path);\n\t\treturn;\n\t}\n\tWARN_ON(wc->opcode != IB_WC_SEND);\n}\n\nstatic int rtrs_srv_path_up(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_srv_ctx *ctx = srv->ctx;\n\tint up, ret = 0;\n\n\tmutex_lock(&srv->paths_ev_mutex);\n\tup = ++srv->paths_up;\n\tif (up == 1)\n\t\tret = ctx->ops.link_ev(srv, RTRS_SRV_LINK_EV_CONNECTED, NULL);\n\tmutex_unlock(&srv->paths_ev_mutex);\n\n\t \n\tif (!ret)\n\t\tsrv_path->established = true;\n\n\treturn ret;\n}\n\nstatic void rtrs_srv_path_down(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_srv_ctx *ctx = srv->ctx;\n\n\tif (!srv_path->established)\n\t\treturn;\n\n\tsrv_path->established = false;\n\tmutex_lock(&srv->paths_ev_mutex);\n\tWARN_ON(!srv->paths_up);\n\tif (--srv->paths_up == 0)\n\t\tctx->ops.link_ev(srv, RTRS_SRV_LINK_EV_DISCONNECTED, srv->priv);\n\tmutex_unlock(&srv->paths_ev_mutex);\n}\n\nstatic bool exist_pathname(struct rtrs_srv_ctx *ctx,\n\t\t\t   const char *pathname, const uuid_t *path_uuid)\n{\n\tstruct rtrs_srv_sess *srv;\n\tstruct rtrs_srv_path *srv_path;\n\tbool found = false;\n\n\tmutex_lock(&ctx->srv_mutex);\n\tlist_for_each_entry(srv, &ctx->srv_list, ctx_list) {\n\t\tmutex_lock(&srv->paths_mutex);\n\n\t\t \n\t\tif (uuid_equal(&srv->paths_uuid, path_uuid)) {\n\t\t\tmutex_unlock(&srv->paths_mutex);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_for_each_entry(srv_path, &srv->paths_list, s.entry) {\n\t\t\tif (strlen(srv_path->s.sessname) == strlen(pathname) &&\n\t\t\t    !strcmp(srv_path->s.sessname, pathname)) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&srv->paths_mutex);\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&ctx->srv_mutex);\n\treturn found;\n}\n\nstatic int post_recv_path(struct rtrs_srv_path *srv_path);\nstatic int rtrs_rdma_do_reject(struct rdma_cm_id *cm_id, int errno);\n\nstatic int process_info_req(struct rtrs_srv_con *con,\n\t\t\t    struct rtrs_msg_info_req *msg)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct ib_send_wr *reg_wr = NULL;\n\tstruct rtrs_msg_info_rsp *rsp;\n\tstruct rtrs_iu *tx_iu;\n\tstruct ib_reg_wr *rwr;\n\tint mri, err;\n\tsize_t tx_sz;\n\n\terr = post_recv_path(srv_path);\n\tif (err) {\n\t\trtrs_err(s, \"post_recv_path(), err: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tif (strchr(msg->pathname, '/') || strchr(msg->pathname, '.')) {\n\t\trtrs_err(s, \"pathname cannot contain / and .\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (exist_pathname(srv_path->srv->ctx,\n\t\t\t   msg->pathname, &srv_path->srv->paths_uuid)) {\n\t\trtrs_err(s, \"pathname is duplicated: %s\\n\", msg->pathname);\n\t\treturn -EPERM;\n\t}\n\tstrscpy(srv_path->s.sessname, msg->pathname,\n\t\tsizeof(srv_path->s.sessname));\n\n\trwr = kcalloc(srv_path->mrs_num, sizeof(*rwr), GFP_KERNEL);\n\tif (!rwr)\n\t\treturn -ENOMEM;\n\n\ttx_sz  = sizeof(*rsp);\n\ttx_sz += sizeof(rsp->desc[0]) * srv_path->mrs_num;\n\ttx_iu = rtrs_iu_alloc(1, tx_sz, GFP_KERNEL, srv_path->s.dev->ib_dev,\n\t\t\t       DMA_TO_DEVICE, rtrs_srv_info_rsp_done);\n\tif (!tx_iu) {\n\t\terr = -ENOMEM;\n\t\tgoto rwr_free;\n\t}\n\n\trsp = tx_iu->buf;\n\trsp->type = cpu_to_le16(RTRS_MSG_INFO_RSP);\n\trsp->sg_cnt = cpu_to_le16(srv_path->mrs_num);\n\n\tfor (mri = 0; mri < srv_path->mrs_num; mri++) {\n\t\tstruct ib_mr *mr = srv_path->mrs[mri].mr;\n\n\t\trsp->desc[mri].addr = cpu_to_le64(mr->iova);\n\t\trsp->desc[mri].key  = cpu_to_le32(mr->rkey);\n\t\trsp->desc[mri].len  = cpu_to_le32(mr->length);\n\n\t\t \n\t\trwr[mri].wr.next = mri ? &rwr[mri - 1].wr : NULL;\n\t\trwr[mri].wr.opcode = IB_WR_REG_MR;\n\t\trwr[mri].wr.wr_cqe = &local_reg_cqe;\n\t\trwr[mri].wr.num_sge = 0;\n\t\trwr[mri].wr.send_flags = 0;\n\t\trwr[mri].mr = mr;\n\t\trwr[mri].key = mr->rkey;\n\t\trwr[mri].access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_WRITE);\n\t\treg_wr = &rwr[mri].wr;\n\t}\n\n\terr = rtrs_srv_create_path_files(srv_path);\n\tif (err)\n\t\tgoto iu_free;\n\tkobject_get(&srv_path->kobj);\n\tget_device(&srv_path->srv->dev);\n\terr = rtrs_srv_change_state(srv_path, RTRS_SRV_CONNECTED);\n\tif (!err) {\n\t\trtrs_err(s, \"rtrs_srv_change_state(), err: %d\\n\", err);\n\t\tgoto iu_free;\n\t}\n\n\trtrs_srv_start_hb(srv_path);\n\n\t \n\terr = rtrs_srv_path_up(srv_path);\n\tif (err) {\n\t\trtrs_err(s, \"rtrs_srv_path_up(), err: %d\\n\", err);\n\t\tgoto iu_free;\n\t}\n\n\tib_dma_sync_single_for_device(srv_path->s.dev->ib_dev,\n\t\t\t\t      tx_iu->dma_addr,\n\t\t\t\t      tx_iu->size, DMA_TO_DEVICE);\n\n\t \n\terr = rtrs_iu_post_send(&con->c, tx_iu, tx_sz, reg_wr);\n\tif (err) {\n\t\trtrs_err(s, \"rtrs_iu_post_send(), err: %d\\n\", err);\niu_free:\n\t\trtrs_iu_free(tx_iu, srv_path->s.dev->ib_dev, 1);\n\t}\nrwr_free:\n\tkfree(rwr);\n\n\treturn err;\n}\n\nstatic void rtrs_srv_info_req_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context);\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_msg_info_req *msg;\n\tstruct rtrs_iu *iu;\n\tint err;\n\n\tWARN_ON(con->c.cid);\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(s, \"Sess info request receive failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tgoto close;\n\t}\n\tWARN_ON(wc->opcode != IB_WC_RECV);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(s, \"Sess info request is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto close;\n\t}\n\tib_dma_sync_single_for_cpu(srv_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_INFO_REQ) {\n\t\trtrs_err(s, \"Sess info request is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto close;\n\t}\n\terr = process_info_req(con, msg);\n\tif (err)\n\t\tgoto close;\n\nout:\n\trtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1);\n\treturn;\nclose:\n\tclose_path(srv_path);\n\tgoto out;\n}\n\nstatic int post_recv_info_req(struct rtrs_srv_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_iu *rx_iu;\n\tint err;\n\n\trx_iu = rtrs_iu_alloc(1, sizeof(struct rtrs_msg_info_req),\n\t\t\t       GFP_KERNEL, srv_path->s.dev->ib_dev,\n\t\t\t       DMA_FROM_DEVICE, rtrs_srv_info_req_done);\n\tif (!rx_iu)\n\t\treturn -ENOMEM;\n\t \n\terr = rtrs_iu_post_recv(&con->c, rx_iu);\n\tif (err) {\n\t\trtrs_err(s, \"rtrs_iu_post_recv(), err: %d\\n\", err);\n\t\trtrs_iu_free(rx_iu, srv_path->s.dev->ib_dev, 1);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int post_recv_io(struct rtrs_srv_con *con, size_t q_size)\n{\n\tint i, err;\n\n\tfor (i = 0; i < q_size; i++) {\n\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int post_recv_path(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_path *s = &srv_path->s;\n\tsize_t q_size;\n\tint err, cid;\n\n\tfor (cid = 0; cid < srv_path->s.con_num; cid++) {\n\t\tif (cid == 0)\n\t\t\tq_size = SERVICE_CON_QUEUE_DEPTH;\n\t\telse\n\t\t\tq_size = srv->queue_depth;\n\n\t\terr = post_recv_io(to_srv_con(srv_path->s.con[cid]), q_size);\n\t\tif (err) {\n\t\t\trtrs_err(s, \"post_recv_io(), err: %d\\n\", err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void process_read(struct rtrs_srv_con *con,\n\t\t\t struct rtrs_msg_rdma_read *msg,\n\t\t\t u32 buf_id, u32 off)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_srv_ctx *ctx = srv->ctx;\n\tstruct rtrs_srv_op *id;\n\n\tsize_t usr_len, data_len;\n\tvoid *data;\n\tint ret;\n\n\tif (srv_path->state != RTRS_SRV_CONNECTED) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Processing read request failed,  session is disconnected, sess state %s\\n\",\n\t\t\t     rtrs_srv_state_str(srv_path->state));\n\t\treturn;\n\t}\n\tif (msg->sg_cnt != 1 && msg->sg_cnt != 0) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Processing read request failed, invalid message\\n\");\n\t\treturn;\n\t}\n\trtrs_srv_get_ops_ids(srv_path);\n\trtrs_srv_update_rdma_stats(srv_path->stats, off, READ);\n\tid = srv_path->ops_ids[buf_id];\n\tid->con\t\t= con;\n\tid->dir\t\t= READ;\n\tid->msg_id\t= buf_id;\n\tid->rd_msg\t= msg;\n\tusr_len = le16_to_cpu(msg->usr_len);\n\tdata_len = off - usr_len;\n\tdata = page_address(srv->chunks[buf_id]);\n\tret = ctx->ops.rdma_ev(srv->priv, id, data, data_len,\n\t\t\t   data + data_len, usr_len);\n\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Processing read request failed, user module cb reported for msg_id %d, err: %d\\n\",\n\t\t\t     buf_id, ret);\n\t\tgoto send_err_msg;\n\t}\n\n\treturn;\n\nsend_err_msg:\n\tret = send_io_resp_imm(con, id, ret);\n\tif (ret < 0) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Sending err msg for failed RDMA-Write-Req failed, msg_id %d, err: %d\\n\",\n\t\t\t     buf_id, ret);\n\t\tclose_path(srv_path);\n\t}\n\trtrs_srv_put_ops_ids(srv_path);\n}\n\nstatic void process_write(struct rtrs_srv_con *con,\n\t\t\t  struct rtrs_msg_rdma_write *req,\n\t\t\t  u32 buf_id, u32 off)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_srv_ctx *ctx = srv->ctx;\n\tstruct rtrs_srv_op *id;\n\n\tsize_t data_len, usr_len;\n\tvoid *data;\n\tint ret;\n\n\tif (srv_path->state != RTRS_SRV_CONNECTED) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Processing write request failed,  session is disconnected, sess state %s\\n\",\n\t\t\t     rtrs_srv_state_str(srv_path->state));\n\t\treturn;\n\t}\n\trtrs_srv_get_ops_ids(srv_path);\n\trtrs_srv_update_rdma_stats(srv_path->stats, off, WRITE);\n\tid = srv_path->ops_ids[buf_id];\n\tid->con    = con;\n\tid->dir    = WRITE;\n\tid->msg_id = buf_id;\n\n\tusr_len = le16_to_cpu(req->usr_len);\n\tdata_len = off - usr_len;\n\tdata = page_address(srv->chunks[buf_id]);\n\tret = ctx->ops.rdma_ev(srv->priv, id, data, data_len,\n\t\t\t       data + data_len, usr_len);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Processing write request failed, user module callback reports err: %d\\n\",\n\t\t\t     ret);\n\t\tgoto send_err_msg;\n\t}\n\n\treturn;\n\nsend_err_msg:\n\tret = send_io_resp_imm(con, id, ret);\n\tif (ret < 0) {\n\t\trtrs_err_rl(s,\n\t\t\t     \"Processing write request failed, sending I/O response failed, msg_id %d, err: %d\\n\",\n\t\t\t     buf_id, ret);\n\t\tclose_path(srv_path);\n\t}\n\trtrs_srv_put_ops_ids(srv_path);\n}\n\nstatic void process_io_req(struct rtrs_srv_con *con, void *msg,\n\t\t\t   u32 id, u32 off)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_msg_rdma_hdr *hdr;\n\tunsigned int type;\n\n\tib_dma_sync_single_for_cpu(srv_path->s.dev->ib_dev,\n\t\t\t\t   srv_path->dma_addr[id],\n\t\t\t\t   max_chunk_size, DMA_BIDIRECTIONAL);\n\thdr = msg;\n\ttype = le16_to_cpu(hdr->type);\n\n\tswitch (type) {\n\tcase RTRS_MSG_WRITE:\n\t\tprocess_write(con, msg, id, off);\n\t\tbreak;\n\tcase RTRS_MSG_READ:\n\t\tprocess_read(con, msg, id, off);\n\t\tbreak;\n\tdefault:\n\t\trtrs_err(s,\n\t\t\t  \"Processing I/O request failed, unknown message type received: 0x%02x\\n\",\n\t\t\t  type);\n\t\tgoto err;\n\t}\n\n\treturn;\n\nerr:\n\tclose_path(srv_path);\n}\n\nstatic void rtrs_srv_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_srv_mr *mr =\n\t\tcontainer_of(wc->wr_cqe, typeof(*mr), inv_cqe);\n\tstruct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context);\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tu32 msg_id, off;\n\tvoid *data;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(s, \"Failed IB_WR_LOCAL_INV: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tclose_path(srv_path);\n\t}\n\tmsg_id = mr->msg_id;\n\toff = mr->msg_off;\n\tdata = page_address(srv->chunks[msg_id]) + off;\n\tprocess_io_req(con, data, msg_id, off);\n}\n\nstatic int rtrs_srv_inv_rkey(struct rtrs_srv_con *con,\n\t\t\t      struct rtrs_srv_mr *mr)\n{\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.wr_cqe\t\t    = &mr->inv_cqe,\n\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t.ex.invalidate_rkey = mr->mr->rkey,\n\t};\n\tmr->inv_cqe.done = rtrs_srv_inv_rkey_done;\n\n\treturn ib_post_send(con->c.qp, &wr, NULL);\n}\n\nstatic void rtrs_rdma_process_wr_wait_list(struct rtrs_srv_con *con)\n{\n\tspin_lock(&con->rsp_wr_wait_lock);\n\twhile (!list_empty(&con->rsp_wr_wait_list)) {\n\t\tstruct rtrs_srv_op *id;\n\t\tint ret;\n\n\t\tid = list_entry(con->rsp_wr_wait_list.next,\n\t\t\t\tstruct rtrs_srv_op, wait_list);\n\t\tlist_del(&id->wait_list);\n\n\t\tspin_unlock(&con->rsp_wr_wait_lock);\n\t\tret = rtrs_srv_resp_rdma(id, id->status);\n\t\tspin_lock(&con->rsp_wr_wait_lock);\n\n\t\tif (!ret) {\n\t\t\tlist_add(&id->wait_list, &con->rsp_wr_wait_list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&con->rsp_wr_wait_lock);\n}\n\nstatic void rtrs_srv_rdma_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context);\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_srv_path *srv_path = to_srv_path(s);\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tu32 imm_type, imm_payload;\n\tint err;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\trtrs_err(s,\n\t\t\t\t  \"%s (wr_cqe: %p, type: %d, vendor_err: 0x%x, len: %u)\\n\",\n\t\t\t\t  ib_wc_status_msg(wc->status), wc->wr_cqe,\n\t\t\t\t  wc->opcode, wc->vendor_err, wc->byte_len);\n\t\t\tclose_path(srv_path);\n\t\t}\n\t\treturn;\n\t}\n\n\tswitch (wc->opcode) {\n\tcase IB_WC_RECV_RDMA_WITH_IMM:\n\t\t \n\t\tif (WARN_ON(wc->wr_cqe != &io_comp_cqe))\n\t\t\treturn;\n\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\tif (err) {\n\t\t\trtrs_err(s, \"rtrs_post_recv(), err: %d\\n\", err);\n\t\t\tclose_path(srv_path);\n\t\t\tbreak;\n\t\t}\n\t\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data),\n\t\t\t       &imm_type, &imm_payload);\n\t\tif (imm_type == RTRS_IO_REQ_IMM) {\n\t\t\tu32 msg_id, off;\n\t\t\tvoid *data;\n\n\t\t\tmsg_id = imm_payload >> srv_path->mem_bits;\n\t\t\toff = imm_payload & ((1 << srv_path->mem_bits) - 1);\n\t\t\tif (msg_id >= srv->queue_depth || off >= max_chunk_size) {\n\t\t\t\trtrs_err(s, \"Wrong msg_id %u, off %u\\n\",\n\t\t\t\t\t  msg_id, off);\n\t\t\t\tclose_path(srv_path);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (always_invalidate) {\n\t\t\t\tstruct rtrs_srv_mr *mr = &srv_path->mrs[msg_id];\n\n\t\t\t\tmr->msg_off = off;\n\t\t\t\tmr->msg_id = msg_id;\n\t\t\t\terr = rtrs_srv_inv_rkey(con, mr);\n\t\t\t\tif (err) {\n\t\t\t\t\trtrs_err(s, \"rtrs_post_recv(), err: %d\\n\",\n\t\t\t\t\t\t  err);\n\t\t\t\t\tclose_path(srv_path);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdata = page_address(srv->chunks[msg_id]) + off;\n\t\t\t\tprocess_io_req(con, data, msg_id, off);\n\t\t\t}\n\t\t} else if (imm_type == RTRS_HB_MSG_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\trtrs_send_hb_ack(&srv_path->s);\n\t\t} else if (imm_type == RTRS_HB_ACK_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\tsrv_path->s.hb_missed_cnt = 0;\n\t\t} else {\n\t\t\trtrs_wrn(s, \"Unknown IMM type %u\\n\", imm_type);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RDMA_WRITE:\n\tcase IB_WC_SEND:\n\t\t \n\t\tatomic_add(s->signal_interval, &con->c.sq_wr_avail);\n\n\t\tif (!list_empty_careful(&con->rsp_wr_wait_list))\n\t\t\trtrs_rdma_process_wr_wait_list(con);\n\n\t\tbreak;\n\tdefault:\n\t\trtrs_wrn(s, \"Unexpected WC type: %d\\n\", wc->opcode);\n\t\treturn;\n\t}\n}\n\n \nint rtrs_srv_get_path_name(struct rtrs_srv_sess *srv, char *pathname,\n\t\t\t   size_t len)\n{\n\tstruct rtrs_srv_path *srv_path;\n\tint err = -ENOTCONN;\n\n\tmutex_lock(&srv->paths_mutex);\n\tlist_for_each_entry(srv_path, &srv->paths_list, s.entry) {\n\t\tif (srv_path->state != RTRS_SRV_CONNECTED)\n\t\t\tcontinue;\n\t\tstrscpy(pathname, srv_path->s.sessname,\n\t\t\tmin_t(size_t, sizeof(srv_path->s.sessname), len));\n\t\terr = 0;\n\t\tbreak;\n\t}\n\tmutex_unlock(&srv->paths_mutex);\n\n\treturn err;\n}\nEXPORT_SYMBOL(rtrs_srv_get_path_name);\n\n \nint rtrs_srv_get_queue_depth(struct rtrs_srv_sess *srv)\n{\n\treturn srv->queue_depth;\n}\nEXPORT_SYMBOL(rtrs_srv_get_queue_depth);\n\nstatic int find_next_bit_ring(struct rtrs_srv_path *srv_path)\n{\n\tstruct ib_device *ib_dev = srv_path->s.dev->ib_dev;\n\tint v;\n\n\tv = cpumask_next(srv_path->cur_cq_vector, &cq_affinity_mask);\n\tif (v >= nr_cpu_ids || v >= ib_dev->num_comp_vectors)\n\t\tv = cpumask_first(&cq_affinity_mask);\n\treturn v;\n}\n\nstatic int rtrs_srv_get_next_cq_vector(struct rtrs_srv_path *srv_path)\n{\n\tsrv_path->cur_cq_vector = find_next_bit_ring(srv_path);\n\n\treturn srv_path->cur_cq_vector;\n}\n\nstatic void rtrs_srv_dev_release(struct device *dev)\n{\n\tstruct rtrs_srv_sess *srv = container_of(dev, struct rtrs_srv_sess,\n\t\t\t\t\t\t dev);\n\n\tkfree(srv);\n}\n\nstatic void free_srv(struct rtrs_srv_sess *srv)\n{\n\tint i;\n\n\tWARN_ON(refcount_read(&srv->refcount));\n\tfor (i = 0; i < srv->queue_depth; i++)\n\t\t__free_pages(srv->chunks[i], get_order(max_chunk_size));\n\tkfree(srv->chunks);\n\tmutex_destroy(&srv->paths_mutex);\n\tmutex_destroy(&srv->paths_ev_mutex);\n\t \n\tput_device(&srv->dev);\n}\n\nstatic struct rtrs_srv_sess *get_or_create_srv(struct rtrs_srv_ctx *ctx,\n\t\t\t\t\t  const uuid_t *paths_uuid,\n\t\t\t\t\t  bool first_conn)\n{\n\tstruct rtrs_srv_sess *srv;\n\tint i;\n\n\tmutex_lock(&ctx->srv_mutex);\n\tlist_for_each_entry(srv, &ctx->srv_list, ctx_list) {\n\t\tif (uuid_equal(&srv->paths_uuid, paths_uuid) &&\n\t\t    refcount_inc_not_zero(&srv->refcount)) {\n\t\t\tmutex_unlock(&ctx->srv_mutex);\n\t\t\treturn srv;\n\t\t}\n\t}\n\tmutex_unlock(&ctx->srv_mutex);\n\t \n\tif (!first_conn) {\n\t\tpr_err_ratelimited(\"Error: Not the first connection request for this session\\n\");\n\t\treturn ERR_PTR(-ENXIO);\n\t}\n\n\t \n\tsrv = kzalloc(sizeof(*srv), GFP_KERNEL);\n\tif  (!srv)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&srv->paths_list);\n\tmutex_init(&srv->paths_mutex);\n\tmutex_init(&srv->paths_ev_mutex);\n\tuuid_copy(&srv->paths_uuid, paths_uuid);\n\tsrv->queue_depth = sess_queue_depth;\n\tsrv->ctx = ctx;\n\tdevice_initialize(&srv->dev);\n\tsrv->dev.release = rtrs_srv_dev_release;\n\n\tsrv->chunks = kcalloc(srv->queue_depth, sizeof(*srv->chunks),\n\t\t\t      GFP_KERNEL);\n\tif (!srv->chunks)\n\t\tgoto err_free_srv;\n\n\tfor (i = 0; i < srv->queue_depth; i++) {\n\t\tsrv->chunks[i] = alloc_pages(GFP_KERNEL,\n\t\t\t\t\t     get_order(max_chunk_size));\n\t\tif (!srv->chunks[i])\n\t\t\tgoto err_free_chunks;\n\t}\n\trefcount_set(&srv->refcount, 1);\n\tmutex_lock(&ctx->srv_mutex);\n\tlist_add(&srv->ctx_list, &ctx->srv_list);\n\tmutex_unlock(&ctx->srv_mutex);\n\n\treturn srv;\n\nerr_free_chunks:\n\twhile (i--)\n\t\t__free_pages(srv->chunks[i], get_order(max_chunk_size));\n\tkfree(srv->chunks);\n\nerr_free_srv:\n\tkfree(srv);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic void put_srv(struct rtrs_srv_sess *srv)\n{\n\tif (refcount_dec_and_test(&srv->refcount)) {\n\t\tstruct rtrs_srv_ctx *ctx = srv->ctx;\n\n\t\tWARN_ON(srv->dev.kobj.state_in_sysfs);\n\n\t\tmutex_lock(&ctx->srv_mutex);\n\t\tlist_del(&srv->ctx_list);\n\t\tmutex_unlock(&ctx->srv_mutex);\n\t\tfree_srv(srv);\n\t}\n}\n\nstatic void __add_path_to_srv(struct rtrs_srv_sess *srv,\n\t\t\t      struct rtrs_srv_path *srv_path)\n{\n\tlist_add_tail(&srv_path->s.entry, &srv->paths_list);\n\tsrv->paths_num++;\n\tWARN_ON(srv->paths_num >= MAX_PATHS_NUM);\n}\n\nstatic void del_path_from_srv(struct rtrs_srv_path *srv_path)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\n\tif (WARN_ON(!srv))\n\t\treturn;\n\n\tmutex_lock(&srv->paths_mutex);\n\tlist_del(&srv_path->s.entry);\n\tWARN_ON(!srv->paths_num);\n\tsrv->paths_num--;\n\tmutex_unlock(&srv->paths_mutex);\n}\n\n \nstatic int sockaddr_cmp(const struct sockaddr *a, const struct sockaddr *b)\n{\n\tswitch (a->sa_family) {\n\tcase AF_IB:\n\t\treturn memcmp(&((struct sockaddr_ib *)a)->sib_addr,\n\t\t\t      &((struct sockaddr_ib *)b)->sib_addr,\n\t\t\t      sizeof(struct ib_addr)) &&\n\t\t\t(b->sa_family == AF_IB);\n\tcase AF_INET:\n\t\treturn memcmp(&((struct sockaddr_in *)a)->sin_addr,\n\t\t\t      &((struct sockaddr_in *)b)->sin_addr,\n\t\t\t      sizeof(struct in_addr)) &&\n\t\t\t(b->sa_family == AF_INET);\n\tcase AF_INET6:\n\t\treturn memcmp(&((struct sockaddr_in6 *)a)->sin6_addr,\n\t\t\t      &((struct sockaddr_in6 *)b)->sin6_addr,\n\t\t\t      sizeof(struct in6_addr)) &&\n\t\t\t(b->sa_family == AF_INET6);\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n\nstatic bool __is_path_w_addr_exists(struct rtrs_srv_sess *srv,\n\t\t\t\t    struct rdma_addr *addr)\n{\n\tstruct rtrs_srv_path *srv_path;\n\n\tlist_for_each_entry(srv_path, &srv->paths_list, s.entry)\n\t\tif (!sockaddr_cmp((struct sockaddr *)&srv_path->s.dst_addr,\n\t\t\t\t  (struct sockaddr *)&addr->dst_addr) &&\n\t\t    !sockaddr_cmp((struct sockaddr *)&srv_path->s.src_addr,\n\t\t\t\t  (struct sockaddr *)&addr->src_addr))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic void free_path(struct rtrs_srv_path *srv_path)\n{\n\tif (srv_path->kobj.state_in_sysfs) {\n\t\tkobject_del(&srv_path->kobj);\n\t\tkobject_put(&srv_path->kobj);\n\t} else {\n\t\tfree_percpu(srv_path->stats->rdma_stats);\n\t\tkfree(srv_path->stats);\n\t\tkfree(srv_path);\n\t}\n}\n\nstatic void rtrs_srv_close_work(struct work_struct *work)\n{\n\tstruct rtrs_srv_path *srv_path;\n\tstruct rtrs_srv_con *con;\n\tint i;\n\n\tsrv_path = container_of(work, typeof(*srv_path), close_work);\n\n\trtrs_srv_stop_hb(srv_path);\n\n\tfor (i = 0; i < srv_path->s.con_num; i++) {\n\t\tif (!srv_path->s.con[i])\n\t\t\tcontinue;\n\t\tcon = to_srv_con(srv_path->s.con[i]);\n\t\trdma_disconnect(con->c.cm_id);\n\t\tib_drain_qp(con->c.qp);\n\t}\n\n\t \n\tpercpu_ref_kill(&srv_path->ids_inflight_ref);\n\n\t \n\twait_for_completion(&srv_path->complete_done);\n\n\trtrs_srv_destroy_path_files(srv_path);\n\n\t \n\trtrs_srv_path_down(srv_path);\n\n\tunmap_cont_bufs(srv_path);\n\trtrs_srv_free_ops_ids(srv_path);\n\n\tfor (i = 0; i < srv_path->s.con_num; i++) {\n\t\tif (!srv_path->s.con[i])\n\t\t\tcontinue;\n\t\tcon = to_srv_con(srv_path->s.con[i]);\n\t\trtrs_cq_qp_destroy(&con->c);\n\t\trdma_destroy_id(con->c.cm_id);\n\t\tkfree(con);\n\t}\n\trtrs_ib_dev_put(srv_path->s.dev);\n\n\tdel_path_from_srv(srv_path);\n\tput_srv(srv_path->srv);\n\tsrv_path->srv = NULL;\n\trtrs_srv_change_state(srv_path, RTRS_SRV_CLOSED);\n\n\tkfree(srv_path->dma_addr);\n\tkfree(srv_path->s.con);\n\tfree_path(srv_path);\n}\n\nstatic int rtrs_rdma_do_accept(struct rtrs_srv_path *srv_path,\n\t\t\t       struct rdma_cm_id *cm_id)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_msg_conn_rsp msg;\n\tstruct rdma_conn_param param;\n\tint err;\n\n\tparam = (struct rdma_conn_param) {\n\t\t.rnr_retry_count = 7,\n\t\t.private_data = &msg,\n\t\t.private_data_len = sizeof(msg),\n\t};\n\n\tmsg = (struct rtrs_msg_conn_rsp) {\n\t\t.magic = cpu_to_le16(RTRS_MAGIC),\n\t\t.version = cpu_to_le16(RTRS_PROTO_VER),\n\t\t.queue_depth = cpu_to_le16(srv->queue_depth),\n\t\t.max_io_size = cpu_to_le32(max_chunk_size - MAX_HDR_SIZE),\n\t\t.max_hdr_size = cpu_to_le32(MAX_HDR_SIZE),\n\t};\n\n\tif (always_invalidate)\n\t\tmsg.flags = cpu_to_le32(RTRS_MSG_NEW_RKEY_F);\n\n\terr = rdma_accept(cm_id, &param);\n\tif (err)\n\t\tpr_err(\"rdma_accept(), err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_do_reject(struct rdma_cm_id *cm_id, int errno)\n{\n\tstruct rtrs_msg_conn_rsp msg;\n\tint err;\n\n\tmsg = (struct rtrs_msg_conn_rsp) {\n\t\t.magic = cpu_to_le16(RTRS_MAGIC),\n\t\t.version = cpu_to_le16(RTRS_PROTO_VER),\n\t\t.errno = cpu_to_le16(errno),\n\t};\n\n\terr = rdma_reject(cm_id, &msg, sizeof(msg), IB_CM_REJ_CONSUMER_DEFINED);\n\tif (err)\n\t\tpr_err(\"rdma_reject(), err: %d\\n\", err);\n\n\t \n\treturn errno;\n}\n\nstatic struct rtrs_srv_path *\n__find_path(struct rtrs_srv_sess *srv, const uuid_t *sess_uuid)\n{\n\tstruct rtrs_srv_path *srv_path;\n\n\tlist_for_each_entry(srv_path, &srv->paths_list, s.entry) {\n\t\tif (uuid_equal(&srv_path->s.uuid, sess_uuid))\n\t\t\treturn srv_path;\n\t}\n\n\treturn NULL;\n}\n\nstatic int create_con(struct rtrs_srv_path *srv_path,\n\t\t      struct rdma_cm_id *cm_id,\n\t\t      unsigned int cid)\n{\n\tstruct rtrs_srv_sess *srv = srv_path->srv;\n\tstruct rtrs_path *s = &srv_path->s;\n\tstruct rtrs_srv_con *con;\n\n\tu32 cq_num, max_send_wr, max_recv_wr, wr_limit;\n\tint err, cq_vector;\n\n\tcon = kzalloc(sizeof(*con), GFP_KERNEL);\n\tif (!con) {\n\t\terr = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tspin_lock_init(&con->rsp_wr_wait_lock);\n\tINIT_LIST_HEAD(&con->rsp_wr_wait_list);\n\tcon->c.cm_id = cm_id;\n\tcon->c.path = &srv_path->s;\n\tcon->c.cid = cid;\n\tatomic_set(&con->c.wr_cnt, 1);\n\twr_limit = srv_path->s.dev->ib_dev->attrs.max_qp_wr;\n\n\tif (con->c.cid == 0) {\n\t\t \n\t\tmax_send_wr = min_t(int, wr_limit,\n\t\t\t\t    SERVICE_CON_QUEUE_DEPTH * 2 + 2);\n\t\tmax_recv_wr = max_send_wr;\n\t\ts->signal_interval = min_not_zero(srv->queue_depth,\n\t\t\t\t\t\t  (size_t)SERVICE_CON_QUEUE_DEPTH);\n\t} else {\n\t\t \n\t\tif (always_invalidate)\n\t\t\tmax_send_wr =\n\t\t\t\tmin_t(int, wr_limit,\n\t\t\t\t      srv->queue_depth * (1 + 4) + 1);\n\t\telse\n\t\t\tmax_send_wr =\n\t\t\t\tmin_t(int, wr_limit,\n\t\t\t\t      srv->queue_depth * (1 + 2) + 1);\n\n\t\tmax_recv_wr = srv->queue_depth + 1;\n\t}\n\tcq_num = max_send_wr + max_recv_wr;\n\tatomic_set(&con->c.sq_wr_avail, max_send_wr);\n\tcq_vector = rtrs_srv_get_next_cq_vector(srv_path);\n\n\t \n\terr = rtrs_cq_qp_create(&srv_path->s, &con->c, 1, cq_vector, cq_num,\n\t\t\t\t max_send_wr, max_recv_wr,\n\t\t\t\t IB_POLL_WORKQUEUE);\n\tif (err) {\n\t\trtrs_err(s, \"rtrs_cq_qp_create(), err: %d\\n\", err);\n\t\tgoto free_con;\n\t}\n\tif (con->c.cid == 0) {\n\t\terr = post_recv_info_req(con);\n\t\tif (err)\n\t\t\tgoto free_cqqp;\n\t}\n\tWARN_ON(srv_path->s.con[cid]);\n\tsrv_path->s.con[cid] = &con->c;\n\n\t \n\tcm_id->context = &con->c;\n\n\treturn 0;\n\nfree_cqqp:\n\trtrs_cq_qp_destroy(&con->c);\nfree_con:\n\tkfree(con);\n\nerr:\n\treturn err;\n}\n\nstatic struct rtrs_srv_path *__alloc_path(struct rtrs_srv_sess *srv,\n\t\t\t\t\t   struct rdma_cm_id *cm_id,\n\t\t\t\t\t   unsigned int con_num,\n\t\t\t\t\t   unsigned int recon_cnt,\n\t\t\t\t\t   const uuid_t *uuid)\n{\n\tstruct rtrs_srv_path *srv_path;\n\tint err = -ENOMEM;\n\tchar str[NAME_MAX];\n\tstruct rtrs_addr path;\n\n\tif (srv->paths_num >= MAX_PATHS_NUM) {\n\t\terr = -ECONNRESET;\n\t\tgoto err;\n\t}\n\tif (__is_path_w_addr_exists(srv, &cm_id->route.addr)) {\n\t\terr = -EEXIST;\n\t\tpr_err(\"Path with same addr exists\\n\");\n\t\tgoto err;\n\t}\n\tsrv_path = kzalloc(sizeof(*srv_path), GFP_KERNEL);\n\tif (!srv_path)\n\t\tgoto err;\n\n\tsrv_path->stats = kzalloc(sizeof(*srv_path->stats), GFP_KERNEL);\n\tif (!srv_path->stats)\n\t\tgoto err_free_sess;\n\n\tsrv_path->stats->rdma_stats = alloc_percpu(struct rtrs_srv_stats_rdma_stats);\n\tif (!srv_path->stats->rdma_stats)\n\t\tgoto err_free_stats;\n\n\tsrv_path->stats->srv_path = srv_path;\n\n\tsrv_path->dma_addr = kcalloc(srv->queue_depth,\n\t\t\t\t     sizeof(*srv_path->dma_addr),\n\t\t\t\t     GFP_KERNEL);\n\tif (!srv_path->dma_addr)\n\t\tgoto err_free_percpu;\n\n\tsrv_path->s.con = kcalloc(con_num, sizeof(*srv_path->s.con),\n\t\t\t\t  GFP_KERNEL);\n\tif (!srv_path->s.con)\n\t\tgoto err_free_dma_addr;\n\n\tsrv_path->state = RTRS_SRV_CONNECTING;\n\tsrv_path->srv = srv;\n\tsrv_path->cur_cq_vector = -1;\n\tsrv_path->s.dst_addr = cm_id->route.addr.dst_addr;\n\tsrv_path->s.src_addr = cm_id->route.addr.src_addr;\n\n\t \n\tpath.src = &srv_path->s.src_addr;\n\tpath.dst = &srv_path->s.dst_addr;\n\trtrs_addr_to_str(&path, str, sizeof(str));\n\tstrscpy(srv_path->s.sessname, str, sizeof(srv_path->s.sessname));\n\n\tsrv_path->s.con_num = con_num;\n\tsrv_path->s.irq_con_num = con_num;\n\tsrv_path->s.recon_cnt = recon_cnt;\n\tuuid_copy(&srv_path->s.uuid, uuid);\n\tspin_lock_init(&srv_path->state_lock);\n\tINIT_WORK(&srv_path->close_work, rtrs_srv_close_work);\n\trtrs_srv_init_hb(srv_path);\n\n\tsrv_path->s.dev = rtrs_ib_dev_find_or_add(cm_id->device, &dev_pd);\n\tif (!srv_path->s.dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_con;\n\t}\n\terr = map_cont_bufs(srv_path);\n\tif (err)\n\t\tgoto err_put_dev;\n\n\terr = rtrs_srv_alloc_ops_ids(srv_path);\n\tif (err)\n\t\tgoto err_unmap_bufs;\n\n\t__add_path_to_srv(srv, srv_path);\n\n\treturn srv_path;\n\nerr_unmap_bufs:\n\tunmap_cont_bufs(srv_path);\nerr_put_dev:\n\trtrs_ib_dev_put(srv_path->s.dev);\nerr_free_con:\n\tkfree(srv_path->s.con);\nerr_free_dma_addr:\n\tkfree(srv_path->dma_addr);\nerr_free_percpu:\n\tfree_percpu(srv_path->stats->rdma_stats);\nerr_free_stats:\n\tkfree(srv_path->stats);\nerr_free_sess:\n\tkfree(srv_path);\nerr:\n\treturn ERR_PTR(err);\n}\n\nstatic int rtrs_rdma_connect(struct rdma_cm_id *cm_id,\n\t\t\t      const struct rtrs_msg_conn_req *msg,\n\t\t\t      size_t len)\n{\n\tstruct rtrs_srv_ctx *ctx = cm_id->context;\n\tstruct rtrs_srv_path *srv_path;\n\tstruct rtrs_srv_sess *srv;\n\n\tu16 version, con_num, cid;\n\tu16 recon_cnt;\n\tint err = -ECONNRESET;\n\n\tif (len < sizeof(*msg)) {\n\t\tpr_err(\"Invalid RTRS connection request\\n\");\n\t\tgoto reject_w_err;\n\t}\n\tif (le16_to_cpu(msg->magic) != RTRS_MAGIC) {\n\t\tpr_err(\"Invalid RTRS magic\\n\");\n\t\tgoto reject_w_err;\n\t}\n\tversion = le16_to_cpu(msg->version);\n\tif (version >> 8 != RTRS_PROTO_VER_MAJOR) {\n\t\tpr_err(\"Unsupported major RTRS version: %d, expected %d\\n\",\n\t\t       version >> 8, RTRS_PROTO_VER_MAJOR);\n\t\tgoto reject_w_err;\n\t}\n\tcon_num = le16_to_cpu(msg->cid_num);\n\tif (con_num > 4096) {\n\t\t \n\t\tpr_err(\"Too many connections requested: %d\\n\", con_num);\n\t\tgoto reject_w_err;\n\t}\n\tcid = le16_to_cpu(msg->cid);\n\tif (cid >= con_num) {\n\t\t \n\t\tpr_err(\"Incorrect cid: %d >= %d\\n\", cid, con_num);\n\t\tgoto reject_w_err;\n\t}\n\trecon_cnt = le16_to_cpu(msg->recon_cnt);\n\tsrv = get_or_create_srv(ctx, &msg->paths_uuid, msg->first_conn);\n\tif (IS_ERR(srv)) {\n\t\terr = PTR_ERR(srv);\n\t\tpr_err(\"get_or_create_srv(), error %d\\n\", err);\n\t\tgoto reject_w_err;\n\t}\n\tmutex_lock(&srv->paths_mutex);\n\tsrv_path = __find_path(srv, &msg->sess_uuid);\n\tif (srv_path) {\n\t\tstruct rtrs_path *s = &srv_path->s;\n\n\t\t \n\t\tput_srv(srv);\n\n\t\tif (srv_path->state != RTRS_SRV_CONNECTING) {\n\t\t\trtrs_err(s, \"Session in wrong state: %s\\n\",\n\t\t\t\t  rtrs_srv_state_str(srv_path->state));\n\t\t\tmutex_unlock(&srv->paths_mutex);\n\t\t\tgoto reject_w_err;\n\t\t}\n\t\t \n\t\tif (con_num != s->con_num || cid >= s->con_num) {\n\t\t\trtrs_err(s, \"Incorrect request: %d, %d\\n\",\n\t\t\t\t  cid, con_num);\n\t\t\tmutex_unlock(&srv->paths_mutex);\n\t\t\tgoto reject_w_err;\n\t\t}\n\t\tif (s->con[cid]) {\n\t\t\trtrs_err(s, \"Connection already exists: %d\\n\",\n\t\t\t\t  cid);\n\t\t\tmutex_unlock(&srv->paths_mutex);\n\t\t\tgoto reject_w_err;\n\t\t}\n\t} else {\n\t\tsrv_path = __alloc_path(srv, cm_id, con_num, recon_cnt,\n\t\t\t\t    &msg->sess_uuid);\n\t\tif (IS_ERR(srv_path)) {\n\t\t\tmutex_unlock(&srv->paths_mutex);\n\t\t\tput_srv(srv);\n\t\t\terr = PTR_ERR(srv_path);\n\t\t\tpr_err(\"RTRS server session allocation failed: %d\\n\", err);\n\t\t\tgoto reject_w_err;\n\t\t}\n\t}\n\terr = create_con(srv_path, cm_id, cid);\n\tif (err) {\n\t\trtrs_err((&srv_path->s), \"create_con(), error %d\\n\", err);\n\t\trtrs_rdma_do_reject(cm_id, err);\n\t\t \n\t\tgoto close_and_return_err;\n\t}\n\terr = rtrs_rdma_do_accept(srv_path, cm_id);\n\tif (err) {\n\t\trtrs_err((&srv_path->s), \"rtrs_rdma_do_accept(), error %d\\n\", err);\n\t\trtrs_rdma_do_reject(cm_id, err);\n\t\t \n\t\terr = 0;\n\t\tgoto close_and_return_err;\n\t}\n\tmutex_unlock(&srv->paths_mutex);\n\n\treturn 0;\n\nreject_w_err:\n\treturn rtrs_rdma_do_reject(cm_id, err);\n\nclose_and_return_err:\n\tmutex_unlock(&srv->paths_mutex);\n\tclose_path(srv_path);\n\n\treturn err;\n}\n\nstatic int rtrs_srv_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t     struct rdma_cm_event *ev)\n{\n\tstruct rtrs_srv_path *srv_path = NULL;\n\tstruct rtrs_path *s = NULL;\n\tstruct rtrs_con *c = NULL;\n\n\tif (ev->event == RDMA_CM_EVENT_CONNECT_REQUEST)\n\t\t \n\t\treturn rtrs_rdma_connect(cm_id, ev->param.conn.private_data,\n\t\t\t\t\t  ev->param.conn.private_data_len);\n\n\tc = cm_id->context;\n\ts = c->path;\n\tsrv_path = to_srv_path(s);\n\n\tswitch (ev->event) {\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\t \n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\t\trtrs_err(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t  rdma_event_msg(ev->event), ev->status);\n\t\tfallthrough;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tclose_path(srv_path);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Ignoring unexpected CM event %s, err %d\\n\",\n\t\t       rdma_event_msg(ev->event), ev->status);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct rdma_cm_id *rtrs_srv_cm_init(struct rtrs_srv_ctx *ctx,\n\t\t\t\t\t    struct sockaddr *addr,\n\t\t\t\t\t    enum rdma_ucm_port_space ps)\n{\n\tstruct rdma_cm_id *cm_id;\n\tint ret;\n\n\tcm_id = rdma_create_id(&init_net, rtrs_srv_rdma_cm_handler,\n\t\t\t       ctx, ps, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\tret = PTR_ERR(cm_id);\n\t\tpr_err(\"Creating id for RDMA connection failed, err: %d\\n\",\n\t\t       ret);\n\t\tgoto err_out;\n\t}\n\tret = rdma_bind_addr(cm_id, addr);\n\tif (ret) {\n\t\tpr_err(\"Binding RDMA address failed, err: %d\\n\", ret);\n\t\tgoto err_cm;\n\t}\n\tret = rdma_listen(cm_id, 64);\n\tif (ret) {\n\t\tpr_err(\"Listening on RDMA connection failed, err: %d\\n\",\n\t\t       ret);\n\t\tgoto err_cm;\n\t}\n\n\treturn cm_id;\n\nerr_cm:\n\trdma_destroy_id(cm_id);\nerr_out:\n\n\treturn ERR_PTR(ret);\n}\n\nstatic int rtrs_srv_rdma_init(struct rtrs_srv_ctx *ctx, u16 port)\n{\n\tstruct sockaddr_in6 sin = {\n\t\t.sin6_family\t= AF_INET6,\n\t\t.sin6_addr\t= IN6ADDR_ANY_INIT,\n\t\t.sin6_port\t= htons(port),\n\t};\n\tstruct sockaddr_ib sib = {\n\t\t.sib_family\t\t\t= AF_IB,\n\t\t.sib_sid\t= cpu_to_be64(RDMA_IB_IP_PS_IB | port),\n\t\t.sib_sid_mask\t= cpu_to_be64(0xffffffffffffffffULL),\n\t\t.sib_pkey\t= cpu_to_be16(0xffff),\n\t};\n\tstruct rdma_cm_id *cm_ip, *cm_ib;\n\tint ret;\n\n\t \n\tcm_ip = rtrs_srv_cm_init(ctx, (struct sockaddr *)&sin, RDMA_PS_TCP);\n\tif (IS_ERR(cm_ip))\n\t\treturn PTR_ERR(cm_ip);\n\n\tcm_ib = rtrs_srv_cm_init(ctx, (struct sockaddr *)&sib, RDMA_PS_IB);\n\tif (IS_ERR(cm_ib)) {\n\t\tret = PTR_ERR(cm_ib);\n\t\tgoto free_cm_ip;\n\t}\n\n\tctx->cm_id_ip = cm_ip;\n\tctx->cm_id_ib = cm_ib;\n\n\treturn 0;\n\nfree_cm_ip:\n\trdma_destroy_id(cm_ip);\n\n\treturn ret;\n}\n\nstatic struct rtrs_srv_ctx *alloc_srv_ctx(struct rtrs_srv_ops *ops)\n{\n\tstruct rtrs_srv_ctx *ctx;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\tctx->ops = *ops;\n\tmutex_init(&ctx->srv_mutex);\n\tINIT_LIST_HEAD(&ctx->srv_list);\n\n\treturn ctx;\n}\n\nstatic void free_srv_ctx(struct rtrs_srv_ctx *ctx)\n{\n\tWARN_ON(!list_empty(&ctx->srv_list));\n\tmutex_destroy(&ctx->srv_mutex);\n\tkfree(ctx);\n}\n\nstatic int rtrs_srv_add_one(struct ib_device *device)\n{\n\tstruct rtrs_srv_ctx *ctx;\n\tint ret = 0;\n\n\tmutex_lock(&ib_ctx.ib_dev_mutex);\n\tif (ib_ctx.ib_dev_count)\n\t\tgoto out;\n\n\t \n\tctx = ib_ctx.srv_ctx;\n\tret = rtrs_srv_rdma_init(ctx, ib_ctx.port);\n\tif (ret) {\n\t\t \n\t\tpr_err(\"Failed to initialize RDMA connection\");\n\t\tgoto err_out;\n\t}\n\nout:\n\t \n\tib_ctx.ib_dev_count++;\n\nerr_out:\n\tmutex_unlock(&ib_ctx.ib_dev_mutex);\n\treturn ret;\n}\n\nstatic void rtrs_srv_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct rtrs_srv_ctx *ctx;\n\n\tmutex_lock(&ib_ctx.ib_dev_mutex);\n\tib_ctx.ib_dev_count--;\n\n\tif (ib_ctx.ib_dev_count)\n\t\tgoto out;\n\n\t \n\tctx = ib_ctx.srv_ctx;\n\trdma_destroy_id(ctx->cm_id_ip);\n\trdma_destroy_id(ctx->cm_id_ib);\n\nout:\n\tmutex_unlock(&ib_ctx.ib_dev_mutex);\n}\n\nstatic struct ib_client rtrs_srv_client = {\n\t.name\t= \"rtrs_server\",\n\t.add\t= rtrs_srv_add_one,\n\t.remove\t= rtrs_srv_remove_one\n};\n\n \nstruct rtrs_srv_ctx *rtrs_srv_open(struct rtrs_srv_ops *ops, u16 port)\n{\n\tstruct rtrs_srv_ctx *ctx;\n\tint err;\n\n\tctx = alloc_srv_ctx(ops);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&ib_ctx.ib_dev_mutex);\n\tib_ctx.srv_ctx = ctx;\n\tib_ctx.port = port;\n\n\terr = ib_register_client(&rtrs_srv_client);\n\tif (err) {\n\t\tfree_srv_ctx(ctx);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn ctx;\n}\nEXPORT_SYMBOL(rtrs_srv_open);\n\nstatic void close_paths(struct rtrs_srv_sess *srv)\n{\n\tstruct rtrs_srv_path *srv_path;\n\n\tmutex_lock(&srv->paths_mutex);\n\tlist_for_each_entry(srv_path, &srv->paths_list, s.entry)\n\t\tclose_path(srv_path);\n\tmutex_unlock(&srv->paths_mutex);\n}\n\nstatic void close_ctx(struct rtrs_srv_ctx *ctx)\n{\n\tstruct rtrs_srv_sess *srv;\n\n\tmutex_lock(&ctx->srv_mutex);\n\tlist_for_each_entry(srv, &ctx->srv_list, ctx_list)\n\t\tclose_paths(srv);\n\tmutex_unlock(&ctx->srv_mutex);\n\tflush_workqueue(rtrs_wq);\n}\n\n \nvoid rtrs_srv_close(struct rtrs_srv_ctx *ctx)\n{\n\tib_unregister_client(&rtrs_srv_client);\n\tmutex_destroy(&ib_ctx.ib_dev_mutex);\n\tclose_ctx(ctx);\n\tfree_srv_ctx(ctx);\n}\nEXPORT_SYMBOL(rtrs_srv_close);\n\nstatic int check_module_params(void)\n{\n\tif (sess_queue_depth < 1 || sess_queue_depth > MAX_SESS_QUEUE_DEPTH) {\n\t\tpr_err(\"Invalid sess_queue_depth value %d, has to be >= %d, <= %d.\\n\",\n\t\t       sess_queue_depth, 1, MAX_SESS_QUEUE_DEPTH);\n\t\treturn -EINVAL;\n\t}\n\tif (max_chunk_size < MIN_CHUNK_SIZE || !is_power_of_2(max_chunk_size)) {\n\t\tpr_err(\"Invalid max_chunk_size value %d, has to be >= %d and should be power of two.\\n\",\n\t\t       max_chunk_size, MIN_CHUNK_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((ilog2(sess_queue_depth - 1) + 1) +\n\t    (ilog2(max_chunk_size - 1) + 1) > MAX_IMM_PAYL_BITS) {\n\t\tpr_err(\"RDMA immediate size (%db) not enough to encode %d buffers of size %dB. Reduce 'sess_queue_depth' or 'max_chunk_size' parameters.\\n\",\n\t\t       MAX_IMM_PAYL_BITS, sess_queue_depth, max_chunk_size);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int __init rtrs_server_init(void)\n{\n\tint err;\n\n\tpr_info(\"Loading module %s, proto %s: (max_chunk_size: %d (pure IO %ld, headers %ld) , sess_queue_depth: %d, always_invalidate: %d)\\n\",\n\t\tKBUILD_MODNAME, RTRS_PROTO_VER_STRING,\n\t\tmax_chunk_size, max_chunk_size - MAX_HDR_SIZE, MAX_HDR_SIZE,\n\t\tsess_queue_depth, always_invalidate);\n\n\trtrs_rdma_dev_pd_init(0, &dev_pd);\n\n\terr = check_module_params();\n\tif (err) {\n\t\tpr_err(\"Failed to load module, invalid module parameters, err: %d\\n\",\n\t\t       err);\n\t\treturn err;\n\t}\n\terr = class_register(&rtrs_dev_class);\n\tif (err)\n\t\tgoto out_err;\n\n\trtrs_wq = alloc_workqueue(\"rtrs_server_wq\", 0, 0);\n\tif (!rtrs_wq) {\n\t\terr = -ENOMEM;\n\t\tgoto out_dev_class;\n\t}\n\n\treturn 0;\n\nout_dev_class:\n\tclass_unregister(&rtrs_dev_class);\nout_err:\n\treturn err;\n}\n\nstatic void __exit rtrs_server_exit(void)\n{\n\tdestroy_workqueue(rtrs_wq);\n\tclass_unregister(&rtrs_dev_class);\n\trtrs_rdma_dev_pd_deinit(&dev_pd);\n}\n\nmodule_init(rtrs_server_init);\nmodule_exit(rtrs_server_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}