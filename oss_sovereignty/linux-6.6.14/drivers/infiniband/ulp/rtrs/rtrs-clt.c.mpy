{
  "module_name": "rtrs-clt.c",
  "hash_id": "859bebdb28897e346469c958a8a109ca78129a928e0da18d885b1f91ad49f9c3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/ulp/rtrs/rtrs-clt.c",
  "human_readable_source": "\n \n\n#undef pr_fmt\n#define pr_fmt(fmt) KBUILD_MODNAME \" L\" __stringify(__LINE__) \": \" fmt\n\n#include <linux/module.h>\n#include <linux/rculist.h>\n#include <linux/random.h>\n\n#include \"rtrs-clt.h\"\n#include \"rtrs-log.h\"\n#include \"rtrs-clt-trace.h\"\n\n#define RTRS_CONNECT_TIMEOUT_MS 30000\n \n#define RTRS_RECONNECT_BACKOFF 1000\n \n#define RTRS_RECONNECT_SEED 8\n\n#define FIRST_CONN 0x01\n \n#define RTRS_MAX_SEGMENTS          128\n\nMODULE_DESCRIPTION(\"RDMA Transport Client\");\nMODULE_LICENSE(\"GPL\");\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops;\nstatic struct rtrs_rdma_dev_pd dev_pd = {\n\t.ops = &dev_pd_ops\n};\n\nstatic struct workqueue_struct *rtrs_wq;\nstatic const struct class rtrs_clt_dev_class = {\n\t.name = \"rtrs-client\",\n};\n\nstatic inline bool rtrs_clt_is_connected(const struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tbool connected = false;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry)\n\t\tif (READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED) {\n\t\t\tconnected = true;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\n\treturn connected;\n}\n\nstatic struct rtrs_permit *\n__rtrs_get_permit(struct rtrs_clt_sess *clt, enum rtrs_clt_con_type con_type)\n{\n\tsize_t max_depth = clt->queue_depth;\n\tstruct rtrs_permit *permit;\n\tint bit;\n\n\t \n\tdo {\n\t\tbit = find_first_zero_bit(clt->permits_map, max_depth);\n\t\tif (bit >= max_depth)\n\t\t\treturn NULL;\n\t} while (test_and_set_bit_lock(bit, clt->permits_map));\n\n\tpermit = get_permit(clt, bit);\n\tWARN_ON(permit->mem_id != bit);\n\tpermit->cpu_id = raw_smp_processor_id();\n\tpermit->con_type = con_type;\n\n\treturn permit;\n}\n\nstatic inline void __rtrs_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t      struct rtrs_permit *permit)\n{\n\tclear_bit_unlock(permit->mem_id, clt->permits_map);\n}\n\n \nstruct rtrs_permit *rtrs_clt_get_permit(struct rtrs_clt_sess *clt,\n\t\t\t\t\t  enum rtrs_clt_con_type con_type,\n\t\t\t\t\t  enum wait_type can_wait)\n{\n\tstruct rtrs_permit *permit;\n\tDEFINE_WAIT(wait);\n\n\tpermit = __rtrs_get_permit(clt, con_type);\n\tif (permit || !can_wait)\n\t\treturn permit;\n\n\tdo {\n\t\tprepare_to_wait(&clt->permits_wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tpermit = __rtrs_get_permit(clt, con_type);\n\t\tif (permit)\n\t\t\tbreak;\n\n\t\tio_schedule();\n\t} while (1);\n\n\tfinish_wait(&clt->permits_wait, &wait);\n\n\treturn permit;\n}\nEXPORT_SYMBOL(rtrs_clt_get_permit);\n\n \nvoid rtrs_clt_put_permit(struct rtrs_clt_sess *clt,\n\t\t\t struct rtrs_permit *permit)\n{\n\tif (WARN_ON(!test_bit(permit->mem_id, clt->permits_map)))\n\t\treturn;\n\n\t__rtrs_put_permit(clt, permit);\n\n\t \n\tif (waitqueue_active(&clt->permits_wait))\n\t\twake_up(&clt->permits_wait);\n}\nEXPORT_SYMBOL(rtrs_clt_put_permit);\n\n \nstatic\nstruct rtrs_clt_con *rtrs_permit_to_clt_con(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t    struct rtrs_permit *permit)\n{\n\tint id = 0;\n\n\tif (permit->con_type == RTRS_IO_CON)\n\t\tid = (permit->cpu_id % (clt_path->s.irq_con_num - 1)) + 1;\n\n\treturn to_clt_con(clt_path->s.con[id]);\n}\n\n \nstatic bool rtrs_clt_change_state(struct rtrs_clt_path *clt_path,\n\t\t\t\t     enum rtrs_clt_state new_state)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed = false;\n\n\tlockdep_assert_held(&clt_path->state_wq.lock);\n\n\told_state = clt_path->state;\n\tswitch (new_state) {\n\tcase RTRS_CLT_CONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_RECONNECTING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTED:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSING:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CONNECTING:\n\t\tcase RTRS_CLT_CONNECTING_ERR:\n\t\tcase RTRS_CLT_RECONNECTING:\n\t\tcase RTRS_CLT_CONNECTED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_CLOSED:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSING:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RTRS_CLT_DEAD:\n\t\tswitch (old_state) {\n\t\tcase RTRS_CLT_CLOSED:\n\t\t\tchanged = true;\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (changed) {\n\t\tclt_path->state = new_state;\n\t\twake_up_locked(&clt_path->state_wq);\n\t}\n\n\treturn changed;\n}\n\nstatic bool rtrs_clt_change_state_from_to(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state old_state,\n\t\t\t\t\t   enum rtrs_clt_state new_state)\n{\n\tbool changed = false;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (clt_path->state == old_state)\n\t\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_clt_stop_and_destroy_conns(struct rtrs_clt_path *clt_path);\nstatic void rtrs_rdma_error_recovery(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\ttrace_rtrs_rdma_error_recovery(clt_path);\n\n\tif (rtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t   RTRS_CLT_CONNECTED,\n\t\t\t\t\t   RTRS_CLT_RECONNECTING)) {\n\t\tqueue_work(rtrs_wq, &clt_path->err_recovery_work);\n\t} else {\n\t\t \n\t\trtrs_clt_change_state_from_to(clt_path,\n\t\t\t\t\t       RTRS_CLT_CONNECTING,\n\t\t\t\t\t       RTRS_CLT_CONNECTING_ERR);\n\t}\n}\n\nstatic void rtrs_clt_fast_reg_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_REG_MR: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic struct ib_cqe fast_reg_cqe = {\n\t.done = rtrs_clt_fast_reg_done\n};\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait);\n\nstatic void rtrs_clt_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_io_req *req =\n\t\tcontainer_of(wc->wr_cqe, typeof(*req), inv_cqe);\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(con->c.path, \"Failed IB_WR_LOCAL_INV: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\treq->need_inv = false;\n\tif (req->need_inv_comp)\n\t\tcomplete(&req->inv_comp);\n\telse\n\t\t \n\t\tcomplete_rdma_req(req, req->inv_errno, true, false);\n}\n\nstatic int rtrs_inv_rkey(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t};\n\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\n\treturn ib_post_send(con->c.qp, &wr, NULL);\n}\n\nstatic void complete_rdma_req(struct rtrs_clt_io_req *req, int errno,\n\t\t\t      bool notify, bool can_wait)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tif (!req->in_use)\n\t\treturn;\n\tif (WARN_ON(!req->con))\n\t\treturn;\n\tclt_path = to_clt_path(con->c.path);\n\n\tif (req->sg_cnt) {\n\t\tif (req->dir == DMA_FROM_DEVICE && req->need_inv) {\n\t\t\t \n\n\t\t\tif (can_wait) {\n\t\t\t\treq->need_inv_comp = true;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tWARN_ON(!notify);\n\t\t\t\t \n\t\t\t\treq->inv_errno = errno;\n\t\t\t}\n\n\t\t\trefcount_inc(&req->ref);\n\t\t\terr = rtrs_inv_rkey(req);\n\t\t\tif (err) {\n\t\t\t\trtrs_err(con->c.path, \"Send INV WR key=%#x: %d\\n\",\n\t\t\t\t\t  req->mr->rkey, err);\n\t\t\t} else if (can_wait) {\n\t\t\t\twait_for_completion(&req->inv_comp);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tWARN_ON_ONCE(1);\n\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (!refcount_dec_and_test(&req->ref))\n\t\t\t\treturn;\n\t\t}\n\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\tif (!refcount_dec_and_test(&req->ref))\n\t\treturn;\n\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tatomic_dec(&clt_path->stats->inflight);\n\n\treq->in_use = false;\n\treq->con = NULL;\n\n\tif (errno) {\n\t\trtrs_err_rl(con->c.path, \"IO request failed: error=%d path=%s [%s:%u] notify=%d\\n\",\n\t\t\t    errno, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port, notify);\n\t}\n\n\tif (notify)\n\t\treq->conf(req->priv, errno);\n}\n\nstatic int rtrs_post_send_rdma(struct rtrs_clt_con *con,\n\t\t\t\tstruct rtrs_clt_io_req *req,\n\t\t\t\tstruct rtrs_rbuf *rbuf, u32 off,\n\t\t\t\tu32 imm, struct ib_send_wr *wr)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tenum ib_send_flags flags;\n\tstruct ib_sge sge;\n\n\tif (!req->sg_size) {\n\t\trtrs_wrn(con->c.path,\n\t\t\t \"Doing RDMA Write failed, no data supplied\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tsge.addr   = req->iu->dma_addr;\n\tsge.length = req->sg_size;\n\tsge.lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t \n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      req->sg_size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, &sge, 1,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr + off,\n\t\t\t\t\t    imm, flags, wr, NULL);\n}\n\nstatic void process_io_rsp(struct rtrs_clt_path *clt_path, u32 msg_id,\n\t\t\t   s16 errno, bool w_inval)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\tif (WARN_ON(msg_id >= clt_path->queue_depth))\n\t\treturn;\n\n\treq = &clt_path->reqs[msg_id];\n\t \n\treq->need_inv &= !w_inval;\n\tcomplete_rdma_req(req, errno, true, false);\n}\n\nstatic void rtrs_clt_recv_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_iu *iu;\n\tint err;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu,\n\t\t\t  cqe);\n\terr = rtrs_iu_post_recv(&con->c, iu);\n\tif (err) {\n\t\trtrs_err(con->c.path, \"post iu failed %d\\n\", err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n}\n\nstatic void rtrs_clt_rkey_rsp_done(struct rtrs_clt_con *con, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_rkey_rsp *msg;\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tstruct rtrs_iu *iu;\n\tu32 buf_id;\n\tint err;\n\n\tWARN_ON((clt_path->flags & RTRS_MSG_NEW_RKEY_F) == 0);\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(con->c.path, \"rkey response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_RKEY_RSP) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"rkey response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\tbuf_id = le16_to_cpu(msg->buf_id);\n\tif (WARN_ON(buf_id >= clt_path->queue_depth))\n\t\tgoto out;\n\n\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data), &imm_type, &imm_payload);\n\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\tu32 msg_id;\n\n\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\tif (WARN_ON(buf_id != msg_id))\n\t\t\tgoto out;\n\t\tclt_path->rbufs[buf_id].rkey = le32_to_cpu(msg->rkey);\n\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t}\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t      iu->size, DMA_FROM_DEVICE);\n\treturn rtrs_clt_recv_done(con, wc);\nout:\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic struct ib_cqe io_comp_cqe = {\n\t.done = rtrs_clt_rdma_done\n};\n\n \nstatic int rtrs_post_recv_empty_x2(struct rtrs_con *con, struct ib_cqe *cqe)\n{\n\tstruct ib_recv_wr wr_arr[2], *wr;\n\tint i;\n\n\tmemset(wr_arr, 0, sizeof(wr_arr));\n\tfor (i = 0; i < ARRAY_SIZE(wr_arr); i++) {\n\t\twr = &wr_arr[i];\n\t\twr->wr_cqe  = cqe;\n\t\tif (i)\n\t\t\t \n\t\t\twr->next = &wr_arr[i - 1];\n\t}\n\n\treturn ib_post_recv(con->qp, wr, NULL);\n}\n\nstatic void rtrs_clt_rdma_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 imm_type, imm_payload;\n\tbool w_inval = false;\n\tint err;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\trtrs_err(clt_path->clt, \"RDMA failed: %s\\n\",\n\t\t\t\t  ib_wc_status_msg(wc->status));\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\treturn;\n\t}\n\trtrs_clt_update_wc_stats(con);\n\n\tswitch (wc->opcode) {\n\tcase IB_WC_RECV_RDMA_WITH_IMM:\n\t\t \n\t\tif (WARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done))\n\t\t\treturn;\n\t\trtrs_from_imm(be32_to_cpu(wc->ex.imm_data),\n\t\t\t       &imm_type, &imm_payload);\n\t\tif (imm_type == RTRS_IO_RSP_IMM ||\n\t\t    imm_type == RTRS_IO_RSP_W_INV_IMM) {\n\t\t\tu32 msg_id;\n\n\t\t\tw_inval = (imm_type == RTRS_IO_RSP_W_INV_IMM);\n\t\t\trtrs_from_io_rsp_imm(imm_payload, &msg_id, &err);\n\n\t\t\tprocess_io_rsp(clt_path, msg_id, err, w_inval);\n\t\t} else if (imm_type == RTRS_HB_MSG_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\trtrs_send_hb_ack(&clt_path->s);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else if (imm_type == RTRS_HB_ACK_IMM) {\n\t\t\tWARN_ON(con->c.cid);\n\t\t\tclt_path->s.hb_missed_cnt = 0;\n\t\t\tclt_path->s.hb_cur_latency =\n\t\t\t\tktime_sub(ktime_get(), clt_path->s.hb_last_sent);\n\t\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\t\t} else {\n\t\t\trtrs_wrn(con->c.path, \"Unknown IMM type %u\\n\",\n\t\t\t\t  imm_type);\n\t\t}\n\t\tif (w_inval)\n\t\t\t \n\t\t\terr = rtrs_post_recv_empty_x2(&con->c, &io_comp_cqe);\n\t\telse\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\tif (err) {\n\t\t\trtrs_err(con->c.path, \"rtrs_post_recv_empty(): %d\\n\",\n\t\t\t\t  err);\n\t\t\trtrs_rdma_error_recovery(con);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RECV:\n\t\t \n\t\tWARN_ON(!(wc->wc_flags & IB_WC_WITH_INVALIDATE ||\n\t\t\t  wc->wc_flags & IB_WC_WITH_IMM));\n\t\tWARN_ON(wc->wr_cqe->done != rtrs_clt_rdma_done);\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tif (wc->wc_flags & IB_WC_WITH_INVALIDATE)\n\t\t\t\treturn  rtrs_clt_recv_done(con, wc);\n\n\t\t\treturn  rtrs_clt_rkey_rsp_done(con, wc);\n\t\t}\n\t\tbreak;\n\tcase IB_WC_RDMA_WRITE:\n\t\t \n\t\tbreak;\n\n\tdefault:\n\t\trtrs_wrn(clt_path->clt, \"Unexpected WC type: %d\\n\", wc->opcode);\n\t\treturn;\n\t}\n}\n\nstatic int post_recv_io(struct rtrs_clt_con *con, size_t q_size)\n{\n\tint err, i;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tfor (i = 0; i < q_size; i++) {\n\t\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F) {\n\t\t\tstruct rtrs_iu *iu = &con->rsp_ius[i];\n\n\t\t\terr = rtrs_iu_post_recv(&con->c, iu);\n\t\t} else {\n\t\t\terr = rtrs_post_recv_empty(&con->c, &io_comp_cqe);\n\t\t}\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int post_recv_path(struct rtrs_clt_path *clt_path)\n{\n\tsize_t q_size = 0;\n\tint err, cid;\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (cid == 0)\n\t\t\tq_size = SERVICE_CON_QUEUE_DEPTH;\n\t\telse\n\t\t\tq_size = clt_path->queue_depth;\n\n\t\t \n\t\tq_size *= 2;\n\n\t\terr = post_recv_io(to_clt_con(clt_path->s.con[cid]), q_size);\n\t\tif (err) {\n\t\t\trtrs_err(clt_path->clt, \"post_recv_io(), err: %d\\n\",\n\t\t\t\t err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstruct path_it {\n\tint i;\n\tstruct list_head skip_list;\n\tstruct rtrs_clt_sess *clt;\n\tstruct rtrs_clt_path *(*next_path)(struct path_it *it);\n};\n\n \nstatic inline struct rtrs_clt_path *\nrtrs_clt_get_next_path_or_null(struct list_head *head, struct rtrs_clt_path *clt_path)\n{\n\treturn list_next_or_null_rcu(head, &clt_path->s.entry, typeof(*clt_path), s.entry) ?:\n\t\t\t\t     list_next_or_null_rcu(head,\n\t\t\t\t\t\t\t   READ_ONCE((&clt_path->s.entry)->next),\n\t\t\t\t\t\t\t   typeof(*clt_path), s.entry);\n}\n\n \nstatic struct rtrs_clt_path *get_next_path_rr(struct path_it *it)\n{\n\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\tstruct rtrs_clt_path *path;\n\tstruct rtrs_clt_sess *clt;\n\n\tclt = it->clt;\n\n\t \n\n\tppcpu_path = this_cpu_ptr(clt->pcpu_path);\n\tpath = rcu_dereference(*ppcpu_path);\n\tif (!path)\n\t\tpath = list_first_or_null_rcu(&clt->paths_list,\n\t\t\t\t\t      typeof(*path), s.entry);\n\telse\n\t\tpath = rtrs_clt_get_next_path_or_null(&clt->paths_list, path);\n\n\trcu_assign_pointer(*ppcpu_path, path);\n\n\treturn path;\n}\n\n \nstatic struct rtrs_clt_path *get_next_path_min_inflight(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tint min_inflight = INT_MAX;\n\tint inflight;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tinflight = atomic_read(&clt_path->stats->inflight);\n\n\t\tif (inflight < min_inflight) {\n\t\t\tmin_inflight = inflight;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t \n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\n \nstatic struct rtrs_clt_path *get_next_path_min_latency(struct path_it *it)\n{\n\tstruct rtrs_clt_path *min_path = NULL;\n\tstruct rtrs_clt_sess *clt = it->clt;\n\tstruct rtrs_clt_path *clt_path;\n\tktime_t min_latency = KTIME_MAX;\n\tktime_t latency;\n\n\tlist_for_each_entry_rcu(clt_path, &clt->paths_list, s.entry) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (!list_empty(raw_cpu_ptr(clt_path->mp_skip_entry)))\n\t\t\tcontinue;\n\n\t\tlatency = clt_path->s.hb_cur_latency;\n\n\t\tif (latency < min_latency) {\n\t\t\tmin_latency = latency;\n\t\t\tmin_path = clt_path;\n\t\t}\n\t}\n\n\t \n\tif (min_path)\n\t\tlist_add(raw_cpu_ptr(min_path->mp_skip_entry), &it->skip_list);\n\n\treturn min_path;\n}\n\nstatic inline void path_it_init(struct path_it *it, struct rtrs_clt_sess *clt)\n{\n\tINIT_LIST_HEAD(&it->skip_list);\n\tit->clt = clt;\n\tit->i = 0;\n\n\tif (clt->mp_policy == MP_POLICY_RR)\n\t\tit->next_path = get_next_path_rr;\n\telse if (clt->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\tit->next_path = get_next_path_min_inflight;\n\telse\n\t\tit->next_path = get_next_path_min_latency;\n}\n\nstatic inline void path_it_deinit(struct path_it *it)\n{\n\tstruct list_head *skip, *tmp;\n\t \n\tlist_for_each_safe(skip, tmp, &it->skip_list)\n\t\tlist_del_init(skip);\n}\n\n \nstatic void rtrs_clt_init_req(struct rtrs_clt_io_req *req,\n\t\t\t      struct rtrs_clt_path *clt_path,\n\t\t\t      void (*conf)(void *priv, int errno),\n\t\t\t      struct rtrs_permit *permit, void *priv,\n\t\t\t      const struct kvec *vec, size_t usr_len,\n\t\t\t      struct scatterlist *sg, size_t sg_cnt,\n\t\t\t      size_t data_len, int dir)\n{\n\tstruct iov_iter iter;\n\tsize_t len;\n\n\treq->permit = permit;\n\treq->in_use = true;\n\treq->usr_len = usr_len;\n\treq->data_len = data_len;\n\treq->sglist = sg;\n\treq->sg_cnt = sg_cnt;\n\treq->priv = priv;\n\treq->dir = dir;\n\treq->con = rtrs_permit_to_clt_con(clt_path, permit);\n\treq->conf = conf;\n\treq->need_inv = false;\n\treq->need_inv_comp = false;\n\treq->inv_errno = 0;\n\trefcount_set(&req->ref, 1);\n\treq->mp_policy = clt_path->clt->mp_policy;\n\n\tiov_iter_kvec(&iter, ITER_SOURCE, vec, 1, usr_len);\n\tlen = _copy_from_iter(req->iu->buf, usr_len, &iter);\n\tWARN_ON(len != usr_len);\n\n\treinit_completion(&req->inv_comp);\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_req(struct rtrs_clt_path *clt_path,\n\t\t void (*conf)(void *priv, int errno),\n\t\t struct rtrs_permit *permit, void *priv,\n\t\t const struct kvec *vec, size_t usr_len,\n\t\t struct scatterlist *sg, size_t sg_cnt,\n\t\t size_t data_len, int dir)\n{\n\tstruct rtrs_clt_io_req *req;\n\n\treq = &clt_path->reqs[permit->mem_id];\n\trtrs_clt_init_req(req, clt_path, conf, permit, priv, vec, usr_len,\n\t\t\t   sg, sg_cnt, data_len, dir);\n\treturn req;\n}\n\nstatic struct rtrs_clt_io_req *\nrtrs_clt_get_copy_req(struct rtrs_clt_path *alive_path,\n\t\t       struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct kvec vec = {\n\t\t.iov_base = fail_req->iu->buf,\n\t\t.iov_len  = fail_req->usr_len\n\t};\n\n\treq = &alive_path->reqs[fail_req->permit->mem_id];\n\trtrs_clt_init_req(req, alive_path, fail_req->conf, fail_req->permit,\n\t\t\t   fail_req->priv, &vec, fail_req->usr_len,\n\t\t\t   fail_req->sglist, fail_req->sg_cnt,\n\t\t\t   fail_req->data_len, fail_req->dir);\n\treturn req;\n}\n\nstatic int rtrs_post_rdma_write_sg(struct rtrs_clt_con *con,\n\t\t\t\t   struct rtrs_clt_io_req *req,\n\t\t\t\t   struct rtrs_rbuf *rbuf, bool fr_en,\n\t\t\t\t   u32 count, u32 size, u32 imm,\n\t\t\t\t   struct ib_send_wr *wr,\n\t\t\t\t   struct ib_send_wr *tail)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct ib_sge *sge = req->sge;\n\tenum ib_send_flags flags;\n\tstruct scatterlist *sg;\n\tsize_t num_sge;\n\tint i;\n\tstruct ib_send_wr *ptail = NULL;\n\n\tif (fr_en) {\n\t\ti = 0;\n\t\tsge[i].addr   = req->mr->iova;\n\t\tsge[i].length = req->mr->length;\n\t\tsge[i].lkey   = req->mr->lkey;\n\t\ti++;\n\t\tnum_sge = 2;\n\t\tptail = tail;\n\t} else {\n\t\tfor_each_sg(req->sglist, sg, count, i) {\n\t\t\tsge[i].addr   = sg_dma_address(sg);\n\t\t\tsge[i].length = sg_dma_len(sg);\n\t\t\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\t\t}\n\t\tnum_sge = 1 + count;\n\t}\n\tsge[i].addr   = req->iu->dma_addr;\n\tsge[i].length = size;\n\tsge[i].lkey   = clt_path->s.dev->ib_pd->local_dma_lkey;\n\n\t \n\tflags = atomic_inc_return(&con->c.wr_cnt) % clt_path->s.signal_interval ?\n\t\t\t0 : IB_SEND_SIGNALED;\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      req->iu->dma_addr,\n\t\t\t\t      size, DMA_TO_DEVICE);\n\n\treturn rtrs_iu_post_rdma_write_imm(&con->c, req->iu, sge, num_sge,\n\t\t\t\t\t    rbuf->rkey, rbuf->addr, imm,\n\t\t\t\t\t    flags, wr, ptail);\n}\n\nstatic int rtrs_map_sg_fr(struct rtrs_clt_io_req *req, size_t count)\n{\n\tint nr;\n\n\t \n\tnr = ib_map_mr_sg(req->mr, req->sglist, count, NULL, SZ_4K);\n\tif (nr != count)\n\t\treturn nr < 0 ? nr : -EINVAL;\n\tib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));\n\n\treturn nr;\n}\n\nstatic int rtrs_clt_write_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_write *msg;\n\n\tstruct rtrs_rbuf *rbuf;\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_send_wr *wr = NULL;\n\tbool fr_en = false;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s, \"Write request failed, size too big %zu > %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t      req->sg_cnt, req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s, \"Write request failed, map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t \n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_WRITE);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\t \n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\treq->sg_size = tsize;\n\trbuf = &clt_path->rbufs[buf_id];\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t    \"Write request failed, failed to map fast reg. data, err: %d\\n\",\n\t\t\t\t    ret);\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t\t\treturn ret;\n\t\t}\n\t\tinv_wr = (struct ib_send_wr) {\n\t\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t\t.wr_cqe\t\t    = &req->inv_cqe,\n\t\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t\t};\n\t\treq->inv_cqe.done = rtrs_clt_inv_rkey_done;\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\t\tfr_en = true;\n\t\trefcount_inc(&req->ref);\n\t}\n\t \n\trtrs_clt_update_all_stats(req, WRITE);\n\n\tret = rtrs_post_rdma_write_sg(req->con, req, rbuf, fr_en, count,\n\t\t\t\t      req->usr_len + sizeof(*msg),\n\t\t\t\t      imm, wr, &inv_wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Write request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(clt_path->s.dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\nstatic int rtrs_clt_read_req(struct rtrs_clt_io_req *req)\n{\n\tstruct rtrs_clt_con *con = req->con;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rtrs_msg_rdma_read *msg;\n\tstruct rtrs_ib_dev *dev = clt_path->s.dev;\n\n\tstruct ib_reg_wr rwr;\n\tstruct ib_send_wr *wr = NULL;\n\n\tint ret, count = 0;\n\tu32 imm, buf_id;\n\n\tconst size_t tsize = sizeof(*msg) + req->data_len + req->usr_len;\n\n\tif (tsize > clt_path->chunk_size) {\n\t\trtrs_wrn(s,\n\t\t\t  \"Read request failed, message size is %zu, bigger than CHUNK_SIZE %d\\n\",\n\t\t\t  tsize, clt_path->chunk_size);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (req->sg_cnt) {\n\t\tcount = ib_dma_map_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t      req->dir);\n\t\tif (!count) {\n\t\t\trtrs_wrn(s,\n\t\t\t\t  \"Read request failed, dma map failed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\t \n\tmsg = req->iu->buf + req->usr_len;\n\tmsg->type = cpu_to_le16(RTRS_MSG_READ);\n\tmsg->usr_len = cpu_to_le16(req->usr_len);\n\n\tif (count) {\n\t\tret = rtrs_map_sg_fr(req, count);\n\t\tif (ret < 0) {\n\t\t\trtrs_err_rl(s,\n\t\t\t\t     \"Read request failed, failed to map  fast reg. data, err: %d\\n\",\n\t\t\t\t     ret);\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist, req->sg_cnt,\n\t\t\t\t\treq->dir);\n\t\t\treturn ret;\n\t\t}\n\t\trwr = (struct ib_reg_wr) {\n\t\t\t.wr.opcode = IB_WR_REG_MR,\n\t\t\t.wr.wr_cqe = &fast_reg_cqe,\n\t\t\t.mr = req->mr,\n\t\t\t.key = req->mr->rkey,\n\t\t\t.access = (IB_ACCESS_LOCAL_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_WRITE),\n\t\t};\n\t\twr = &rwr.wr;\n\n\t\tmsg->sg_cnt = cpu_to_le16(1);\n\t\tmsg->flags = cpu_to_le16(RTRS_MSG_NEED_INVAL_F);\n\n\t\tmsg->desc[0].addr = cpu_to_le64(req->mr->iova);\n\t\tmsg->desc[0].key = cpu_to_le32(req->mr->rkey);\n\t\tmsg->desc[0].len = cpu_to_le32(req->mr->length);\n\n\t\t \n\t\treq->need_inv = !!RTRS_MSG_NEED_INVAL_F;\n\n\t} else {\n\t\tmsg->sg_cnt = 0;\n\t\tmsg->flags = 0;\n\t}\n\t \n\timm = req->permit->mem_off + req->data_len + req->usr_len;\n\timm = rtrs_to_io_req_imm(imm);\n\tbuf_id = req->permit->mem_id;\n\n\treq->sg_size  = sizeof(*msg);\n\treq->sg_size += le16_to_cpu(msg->sg_cnt) * sizeof(struct rtrs_sg_desc);\n\treq->sg_size += req->usr_len;\n\n\t \n\trtrs_clt_update_all_stats(req, READ);\n\n\tret = rtrs_post_send_rdma(req->con, req, &clt_path->rbufs[buf_id],\n\t\t\t\t   req->data_len, imm, wr);\n\tif (ret) {\n\t\trtrs_err_rl(s,\n\t\t\t    \"Read request failed: error=%d path=%s [%s:%u]\\n\",\n\t\t\t    ret, kobject_name(&clt_path->kobj), clt_path->hca_name,\n\t\t\t    clt_path->hca_port);\n\t\tif (req->mp_policy == MP_POLICY_MIN_INFLIGHT)\n\t\t\tatomic_dec(&clt_path->stats->inflight);\n\t\treq->need_inv = false;\n\t\tif (req->sg_cnt)\n\t\t\tib_dma_unmap_sg(dev->ib_dev, req->sglist,\n\t\t\t\t\treq->sg_cnt, req->dir);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int rtrs_clt_failover_req(struct rtrs_clt_sess *clt,\n\t\t\t\t struct rtrs_clt_io_req *fail_req)\n{\n\tstruct rtrs_clt_path *alive_path;\n\tstruct rtrs_clt_io_req *req;\n\tint err = -ECONNABORTED;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (alive_path = it.next_path(&it)) && it.i < it.clt->paths_num;\n\t     it.i++) {\n\t\tif (READ_ONCE(alive_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\t\treq = rtrs_clt_get_copy_req(alive_path, fail_req);\n\t\tif (req->dir == DMA_TO_DEVICE)\n\t\t\terr = rtrs_clt_write_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_read_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\trtrs_clt_inc_failover_cnt(alive_path->stats);\n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic void fail_all_outstanding_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_io_req *req;\n\tint i, err;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (!req->in_use)\n\t\t\tcontinue;\n\n\t\t \n\t\tcomplete_rdma_req(req, -ECONNABORTED, false, true);\n\n\t\terr = rtrs_clt_failover_req(clt, req);\n\t\tif (err)\n\t\t\t \n\t\t\treq->conf(req->priv, err);\n\t}\n}\n\nstatic void free_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i;\n\n\tif (!clt_path->reqs)\n\t\treturn;\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\tif (req->mr)\n\t\t\tib_dereg_mr(req->mr);\n\t\tkfree(req->sge);\n\t\trtrs_iu_free(req->iu, clt_path->s.dev->ib_dev, 1);\n\t}\n\tkfree(clt_path->reqs);\n\tclt_path->reqs = NULL;\n}\n\nstatic int alloc_path_reqs(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_io_req *req;\n\tint i, err = -ENOMEM;\n\n\tclt_path->reqs = kcalloc(clt_path->queue_depth,\n\t\t\t\t sizeof(*clt_path->reqs),\n\t\t\t\t GFP_KERNEL);\n\tif (!clt_path->reqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < clt_path->queue_depth; ++i) {\n\t\treq = &clt_path->reqs[i];\n\t\treq->iu = rtrs_iu_alloc(1, clt_path->max_hdr_size, GFP_KERNEL,\n\t\t\t\t\t clt_path->s.dev->ib_dev,\n\t\t\t\t\t DMA_TO_DEVICE,\n\t\t\t\t\t rtrs_clt_rdma_done);\n\t\tif (!req->iu)\n\t\t\tgoto out;\n\n\t\treq->sge = kcalloc(2, sizeof(*req->sge), GFP_KERNEL);\n\t\tif (!req->sge)\n\t\t\tgoto out;\n\n\t\treq->mr = ib_alloc_mr(clt_path->s.dev->ib_pd,\n\t\t\t\t      IB_MR_TYPE_MEM_REG,\n\t\t\t\t      clt_path->max_pages_per_mr);\n\t\tif (IS_ERR(req->mr)) {\n\t\t\terr = PTR_ERR(req->mr);\n\t\t\treq->mr = NULL;\n\t\t\tpr_err(\"Failed to alloc clt_path->max_pages_per_mr %d\\n\",\n\t\t\t       clt_path->max_pages_per_mr);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinit_completion(&req->inv_comp);\n\t}\n\n\treturn 0;\n\nout:\n\tfree_path_reqs(clt_path);\n\n\treturn err;\n}\n\nstatic int alloc_permits(struct rtrs_clt_sess *clt)\n{\n\tunsigned int chunk_bits;\n\tint err, i;\n\n\tclt->permits_map = bitmap_zalloc(clt->queue_depth, GFP_KERNEL);\n\tif (!clt->permits_map) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tclt->permits = kcalloc(clt->queue_depth, permit_size(clt), GFP_KERNEL);\n\tif (!clt->permits) {\n\t\terr = -ENOMEM;\n\t\tgoto err_map;\n\t}\n\tchunk_bits = ilog2(clt->queue_depth - 1) + 1;\n\tfor (i = 0; i < clt->queue_depth; i++) {\n\t\tstruct rtrs_permit *permit;\n\n\t\tpermit = get_permit(clt, i);\n\t\tpermit->mem_id = i;\n\t\tpermit->mem_off = i << (MAX_IMM_PAYL_BITS - chunk_bits);\n\t}\n\n\treturn 0;\n\nerr_map:\n\tbitmap_free(clt->permits_map);\n\tclt->permits_map = NULL;\nout_err:\n\treturn err;\n}\n\nstatic void free_permits(struct rtrs_clt_sess *clt)\n{\n\tif (clt->permits_map)\n\t\twait_event(clt->permits_wait,\n\t\t\t   bitmap_empty(clt->permits_map, clt->queue_depth));\n\n\tbitmap_free(clt->permits_map);\n\tclt->permits_map = NULL;\n\tkfree(clt->permits);\n\tclt->permits = NULL;\n}\n\nstatic void query_fast_reg_mode(struct rtrs_clt_path *clt_path)\n{\n\tstruct ib_device *ib_dev;\n\tu64 max_pages_per_mr;\n\tint mr_page_shift;\n\n\tib_dev = clt_path->s.dev->ib_dev;\n\n\t \n\tmr_page_shift      = max(12, ffs(ib_dev->attrs.page_size_cap) - 1);\n\tmax_pages_per_mr   = ib_dev->attrs.max_mr_size;\n\tdo_div(max_pages_per_mr, (1ull << mr_page_shift));\n\tclt_path->max_pages_per_mr =\n\t\tmin3(clt_path->max_pages_per_mr, (u32)max_pages_per_mr,\n\t\t     ib_dev->attrs.max_fast_reg_page_list_len);\n\tclt_path->clt->max_segments =\n\t\tmin(clt_path->max_pages_per_mr, clt_path->clt->max_segments);\n}\n\nstatic bool rtrs_clt_change_state_get_old(struct rtrs_clt_path *clt_path,\n\t\t\t\t\t   enum rtrs_clt_state new_state,\n\t\t\t\t\t   enum rtrs_clt_state *old_state)\n{\n\tbool changed;\n\n\tspin_lock_irq(&clt_path->state_wq.lock);\n\tif (old_state)\n\t\t*old_state = clt_path->state;\n\tchanged = rtrs_clt_change_state(clt_path, new_state);\n\tspin_unlock_irq(&clt_path->state_wq.lock);\n\n\treturn changed;\n}\n\nstatic void rtrs_clt_hb_err_handler(struct rtrs_con *c)\n{\n\tstruct rtrs_clt_con *con = container_of(c, typeof(*con), c);\n\n\trtrs_rdma_error_recovery(con);\n}\n\nstatic void rtrs_clt_init_hb(struct rtrs_clt_path *clt_path)\n{\n\trtrs_init_hb(&clt_path->s, &io_comp_cqe,\n\t\t      RTRS_HB_INTERVAL_MS,\n\t\t      RTRS_HB_MISSED_MAX,\n\t\t      rtrs_clt_hb_err_handler,\n\t\t      rtrs_wq);\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work);\nstatic void rtrs_clt_close_work(struct work_struct *work);\n\nstatic void rtrs_clt_err_recovery_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tstruct rtrs_clt_sess *clt;\n\tint delay_ms;\n\n\tclt_path = container_of(work, struct rtrs_clt_path, err_recovery_work);\n\tclt = clt_path->clt;\n\tdelay_ms = clt->reconnect_delay_sec * 1000;\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,\n\t\t\t   msecs_to_jiffies(delay_ms +\n\t\t\t\t\t    get_random_u32_below(RTRS_RECONNECT_SEED)));\n}\n\nstatic struct rtrs_clt_path *alloc_path(struct rtrs_clt_sess *clt,\n\t\t\t\t\tconst struct rtrs_addr *path,\n\t\t\t\t\tsize_t con_num, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err = -ENOMEM;\n\tint cpu;\n\tsize_t total_con;\n\n\tclt_path = kzalloc(sizeof(*clt_path), GFP_KERNEL);\n\tif (!clt_path)\n\t\tgoto err;\n\n\t \n\ttotal_con = con_num + nr_poll_queues + 1;\n\tclt_path->s.con = kcalloc(total_con, sizeof(*clt_path->s.con),\n\t\t\t\t  GFP_KERNEL);\n\tif (!clt_path->s.con)\n\t\tgoto err_free_path;\n\n\tclt_path->s.con_num = total_con;\n\tclt_path->s.irq_con_num = con_num + 1;\n\n\tclt_path->stats = kzalloc(sizeof(*clt_path->stats), GFP_KERNEL);\n\tif (!clt_path->stats)\n\t\tgoto err_free_con;\n\n\tmutex_init(&clt_path->init_mutex);\n\tuuid_gen(&clt_path->s.uuid);\n\tmemcpy(&clt_path->s.dst_addr, path->dst,\n\t       rdma_addr_size((struct sockaddr *)path->dst));\n\n\t \n\tif (path->src)\n\t\tmemcpy(&clt_path->s.src_addr, path->src,\n\t\t       rdma_addr_size((struct sockaddr *)path->src));\n\tstrscpy(clt_path->s.sessname, clt->sessname,\n\t\tsizeof(clt_path->s.sessname));\n\tclt_path->clt = clt;\n\tclt_path->max_pages_per_mr = RTRS_MAX_SEGMENTS;\n\tinit_waitqueue_head(&clt_path->state_wq);\n\tclt_path->state = RTRS_CLT_CONNECTING;\n\tatomic_set(&clt_path->connected_cnt, 0);\n\tINIT_WORK(&clt_path->close_work, rtrs_clt_close_work);\n\tINIT_WORK(&clt_path->err_recovery_work, rtrs_clt_err_recovery_work);\n\tINIT_DELAYED_WORK(&clt_path->reconnect_dwork, rtrs_clt_reconnect_work);\n\trtrs_clt_init_hb(clt_path);\n\n\tclt_path->mp_skip_entry = alloc_percpu(typeof(*clt_path->mp_skip_entry));\n\tif (!clt_path->mp_skip_entry)\n\t\tgoto err_free_stats;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(per_cpu_ptr(clt_path->mp_skip_entry, cpu));\n\n\terr = rtrs_clt_init_stats(clt_path->stats);\n\tif (err)\n\t\tgoto err_free_percpu;\n\n\treturn clt_path;\n\nerr_free_percpu:\n\tfree_percpu(clt_path->mp_skip_entry);\nerr_free_stats:\n\tkfree(clt_path->stats);\nerr_free_con:\n\tkfree(clt_path->s.con);\nerr_free_path:\n\tkfree(clt_path);\nerr:\n\treturn ERR_PTR(err);\n}\n\nvoid free_path(struct rtrs_clt_path *clt_path)\n{\n\tfree_percpu(clt_path->mp_skip_entry);\n\tmutex_destroy(&clt_path->init_mutex);\n\tkfree(clt_path->s.con);\n\tkfree(clt_path->rbufs);\n\tkfree(clt_path);\n}\n\nstatic int create_con(struct rtrs_clt_path *clt_path, unsigned int cid)\n{\n\tstruct rtrs_clt_con *con;\n\n\tcon = kzalloc(sizeof(*con), GFP_KERNEL);\n\tif (!con)\n\t\treturn -ENOMEM;\n\n\t \n\tcon->cpu  = (cid ? cid - 1 : 0) % nr_cpu_ids;\n\tcon->c.cid = cid;\n\tcon->c.path = &clt_path->s;\n\t \n\tatomic_set(&con->c.wr_cnt, 1);\n\tmutex_init(&con->con_mutex);\n\n\tclt_path->s.con[cid] = &con->c;\n\n\treturn 0;\n}\n\nstatic void destroy_con(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tclt_path->s.con[con->c.cid] = NULL;\n\tmutex_destroy(&con->con_mutex);\n\tkfree(con);\n}\n\nstatic int create_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tu32 max_send_wr, max_recv_wr, cq_num, max_send_sge, wr_limit;\n\tint err, cq_vector;\n\tstruct rtrs_msg_rkey_rsp *rsp;\n\n\tlockdep_assert_held(&con->con_mutex);\n\tif (con->c.cid == 0) {\n\t\tmax_send_sge = 1;\n\t\t \n\t\tif (WARN_ON(clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tclt_path->s.dev = rtrs_ib_dev_find_or_add(con->c.cm_id->device,\n\t\t\t\t\t\t       &dev_pd);\n\t\tif (!clt_path->s.dev) {\n\t\t\trtrs_wrn(clt_path->clt,\n\t\t\t\t  \"rtrs_ib_dev_find_get_or_add(): no memory\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->s.dev_ref = 1;\n\t\tquery_fast_reg_mode(clt_path);\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t \n\t\tmax_send_wr =\n\t\t\tmin_t(int, wr_limit, SERVICE_CON_QUEUE_DEPTH * 2 + 2);\n\t\tmax_recv_wr = max_send_wr;\n\t} else {\n\t\t \n\t\tif (WARN_ON(!clt_path->s.dev))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON(!clt_path->queue_depth))\n\t\t\treturn -EINVAL;\n\n\t\twr_limit = clt_path->s.dev->ib_dev->attrs.max_qp_wr;\n\t\t \n\t\tclt_path->s.dev_ref++;\n\t\tmax_send_wr = min_t(int, wr_limit,\n\t\t\t       \n\t\t\t      clt_path->queue_depth * 4 + 1);\n\t\tmax_recv_wr = min_t(int, wr_limit,\n\t\t\t      clt_path->queue_depth * 3 + 1);\n\t\tmax_send_sge = 2;\n\t}\n\tatomic_set(&con->c.sq_wr_avail, max_send_wr);\n\tcq_num = max_send_wr + max_recv_wr;\n\t \n\tif (clt_path->flags & RTRS_MSG_NEW_RKEY_F || con->c.cid == 0) {\n\t\tcon->rsp_ius = rtrs_iu_alloc(cq_num, sizeof(*rsp),\n\t\t\t\t\t      GFP_KERNEL,\n\t\t\t\t\t      clt_path->s.dev->ib_dev,\n\t\t\t\t\t      DMA_FROM_DEVICE,\n\t\t\t\t\t      rtrs_clt_rdma_done);\n\t\tif (!con->rsp_ius)\n\t\t\treturn -ENOMEM;\n\t\tcon->queue_num = cq_num;\n\t}\n\tcq_vector = con->cpu % clt_path->s.dev->ib_dev->num_comp_vectors;\n\tif (con->c.cid >= clt_path->s.irq_con_num)\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_DIRECT);\n\telse\n\t\terr = rtrs_cq_qp_create(&clt_path->s, &con->c, max_send_sge,\n\t\t\t\t\tcq_vector, cq_num, max_send_wr,\n\t\t\t\t\tmax_recv_wr, IB_POLL_SOFTIRQ);\n\t \n\treturn err;\n}\n\nstatic void destroy_con_cq_qp(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\t \n\tlockdep_assert_held(&con->con_mutex);\n\trtrs_cq_qp_destroy(&con->c);\n\tif (con->rsp_ius) {\n\t\trtrs_iu_free(con->rsp_ius, clt_path->s.dev->ib_dev,\n\t\t\t     con->queue_num);\n\t\tcon->rsp_ius = NULL;\n\t\tcon->queue_num = 0;\n\t}\n\tif (clt_path->s.dev_ref && !--clt_path->s.dev_ref) {\n\t\trtrs_ib_dev_put(clt_path->s.dev);\n\t\tclt_path->s.dev = NULL;\n\t}\n}\n\nstatic void stop_cm(struct rtrs_clt_con *con)\n{\n\trdma_disconnect(con->c.cm_id);\n\tif (con->c.qp)\n\t\tib_drain_qp(con->c.qp);\n}\n\nstatic void destroy_cm(struct rtrs_clt_con *con)\n{\n\trdma_destroy_id(con->c.cm_id);\n\tcon->c.cm_id = NULL;\n}\n\nstatic int rtrs_rdma_addr_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tint err;\n\n\tmutex_lock(&con->con_mutex);\n\terr = create_con_cq_qp(con);\n\tmutex_unlock(&con->con_mutex);\n\tif (err) {\n\t\trtrs_err(s, \"create_con_cq_qp(), err: %d\\n\", err);\n\t\treturn err;\n\t}\n\terr = rdma_resolve_route(con->c.cm_id, RTRS_CONNECT_TIMEOUT_MS);\n\tif (err)\n\t\trtrs_err(s, \"Resolving route failed, err: %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_route_resolved(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_msg_conn_req msg;\n\tstruct rdma_conn_param param;\n\n\tint err;\n\n\tparam = (struct rdma_conn_param) {\n\t\t.retry_count = 7,\n\t\t.rnr_retry_count = 7,\n\t\t.private_data = &msg,\n\t\t.private_data_len = sizeof(msg),\n\t};\n\n\tmsg = (struct rtrs_msg_conn_req) {\n\t\t.magic = cpu_to_le16(RTRS_MAGIC),\n\t\t.version = cpu_to_le16(RTRS_PROTO_VER),\n\t\t.cid = cpu_to_le16(con->c.cid),\n\t\t.cid_num = cpu_to_le16(clt_path->s.con_num),\n\t\t.recon_cnt = cpu_to_le16(clt_path->s.recon_cnt),\n\t};\n\tmsg.first_conn = clt_path->for_new_clt ? FIRST_CONN : 0;\n\tuuid_copy(&msg.sess_uuid, &clt_path->s.uuid);\n\tuuid_copy(&msg.paths_uuid, &clt->paths_uuid);\n\n\terr = rdma_connect_locked(con->c.cm_id, &param);\n\tif (err)\n\t\trtrs_err(clt, \"rdma_connect_locked(): %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int rtrs_rdma_conn_established(struct rtrs_clt_con *con,\n\t\t\t\t       struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tu16 version, queue_depth;\n\tint errno;\n\tu8 len;\n\n\tmsg = ev->param.conn.private_data;\n\tlen = ev->param.conn.private_data_len;\n\tif (len < sizeof(*msg)) {\n\t\trtrs_err(clt, \"Invalid RTRS connection response\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tif (le16_to_cpu(msg->magic) != RTRS_MAGIC) {\n\t\trtrs_err(clt, \"Invalid RTRS magic\\n\");\n\t\treturn -ECONNRESET;\n\t}\n\tversion = le16_to_cpu(msg->version);\n\tif (version >> 8 != RTRS_PROTO_VER_MAJOR) {\n\t\trtrs_err(clt, \"Unsupported major RTRS version: %d, expected %d\\n\",\n\t\t\t  version >> 8, RTRS_PROTO_VER_MAJOR);\n\t\treturn -ECONNRESET;\n\t}\n\terrno = le16_to_cpu(msg->errno);\n\tif (errno) {\n\t\trtrs_err(clt, \"Invalid RTRS message: errno %d\\n\",\n\t\t\t  errno);\n\t\treturn -ECONNRESET;\n\t}\n\tif (con->c.cid == 0) {\n\t\tqueue_depth = le16_to_cpu(msg->queue_depth);\n\n\t\tif (clt_path->queue_depth > 0 && queue_depth != clt_path->queue_depth) {\n\t\t\trtrs_err(clt, \"Error: queue depth changed\\n\");\n\n\t\t\t \n\t\t\tclt_path->reconnect_attempts = -1;\n\t\t\trtrs_err(clt,\n\t\t\t\t\"Disabling auto-reconnect. Trigger a manual reconnect after issue is resolved\\n\");\n\t\t\treturn -ECONNRESET;\n\t\t}\n\n\t\tif (!clt_path->rbufs) {\n\t\t\tclt_path->rbufs = kcalloc(queue_depth,\n\t\t\t\t\t\t  sizeof(*clt_path->rbufs),\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!clt_path->rbufs)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tclt_path->queue_depth = queue_depth;\n\t\tclt_path->s.signal_interval = min_not_zero(queue_depth,\n\t\t\t\t\t\t(unsigned short) SERVICE_CON_QUEUE_DEPTH);\n\t\tclt_path->max_hdr_size = le32_to_cpu(msg->max_hdr_size);\n\t\tclt_path->max_io_size = le32_to_cpu(msg->max_io_size);\n\t\tclt_path->flags = le32_to_cpu(msg->flags);\n\t\tclt_path->chunk_size = clt_path->max_io_size + clt_path->max_hdr_size;\n\n\t\t \n\t\tmutex_lock(&clt->paths_mutex);\n\t\tclt->queue_depth = clt_path->queue_depth;\n\t\tclt->max_io_size = min_not_zero(clt_path->max_io_size,\n\t\t\t\t\t\tclt->max_io_size);\n\t\tmutex_unlock(&clt->paths_mutex);\n\n\t\t \n\t\tclt_path->hca_port = con->c.cm_id->port_num;\n\t\tscnprintf(clt_path->hca_name, sizeof(clt_path->hca_name),\n\t\t\t  clt_path->s.dev->ib_dev->name);\n\t\tclt_path->s.src_addr = con->c.cm_id->route.addr.src_addr;\n\t\t \n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void flag_success_on_conn(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\n\tatomic_inc(&clt_path->connected_cnt);\n\tcon->cm_err = 1;\n}\n\nstatic int rtrs_rdma_conn_rejected(struct rtrs_clt_con *con,\n\t\t\t\t    struct rdma_cm_event *ev)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tconst struct rtrs_msg_conn_rsp *msg;\n\tconst char *rej_msg;\n\tint status, errno;\n\tu8 data_len;\n\n\tstatus = ev->status;\n\trej_msg = rdma_reject_msg(con->c.cm_id, status);\n\tmsg = rdma_consumer_reject_data(con->c.cm_id, ev, &data_len);\n\n\tif (msg && data_len >= sizeof(*msg)) {\n\t\terrno = (int16_t)le16_to_cpu(msg->errno);\n\t\tif (errno == -EBUSY)\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Previous session is still exists on the server, please reconnect later\\n\");\n\t\telse\n\t\t\trtrs_err(s,\n\t\t\t\t  \"Connect rejected: status %d (%s), rtrs errno %d\\n\",\n\t\t\t\t  status, rej_msg, errno);\n\t} else {\n\t\trtrs_err(s,\n\t\t\t  \"Connect rejected but with malformed message: status %d (%s)\\n\",\n\t\t\t  status, rej_msg);\n\t}\n\n\treturn -ECONNRESET;\n}\n\nvoid rtrs_clt_close_conns(struct rtrs_clt_path *clt_path, bool wait)\n{\n\ttrace_rtrs_clt_close_conns(clt_path);\n\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSING, NULL))\n\t\tqueue_work(rtrs_wq, &clt_path->close_work);\n\tif (wait)\n\t\tflush_work(&clt_path->close_work);\n}\n\nstatic inline void flag_error_on_conn(struct rtrs_clt_con *con, int cm_err)\n{\n\tif (con->cm_err == 1) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = to_clt_path(con->c.path);\n\t\tif (atomic_dec_and_test(&clt_path->connected_cnt))\n\n\t\t\twake_up(&clt_path->state_wq);\n\t}\n\tcon->cm_err = cm_err;\n}\n\nstatic int rtrs_clt_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t     struct rdma_cm_event *ev)\n{\n\tstruct rtrs_clt_con *con = cm_id->context;\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tint cm_err = 0;\n\n\tswitch (ev->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\t\tcm_err = rtrs_rdma_addr_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tcm_err = rtrs_rdma_route_resolved(con);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tcm_err = rtrs_rdma_conn_established(con, ev);\n\t\tif (!cm_err) {\n\t\t\t \n\t\t\tflag_success_on_conn(con);\n\t\t\twake_up(&clt_path->state_wq);\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tcm_err = rtrs_rdma_conn_rejected(con, ev);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\t \n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\t\trtrs_wrn(s, \"CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -EHOSTUNREACH;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\t \n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn 0;\n\tdefault:\n\t\trtrs_err(s, \"Unexpected RDMA CM error (CM event: %s, err: %d)\\n\",\n\t\t\t rdma_event_msg(ev->event), ev->status);\n\t\tcm_err = -ECONNRESET;\n\t\tbreak;\n\t}\n\n\tif (cm_err) {\n\t\t \n\t\tflag_error_on_conn(con, cm_err);\n\t\trtrs_rdma_error_recovery(con);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int create_cm(struct rtrs_clt_con *con)\n{\n\tstruct rtrs_path *s = con->c.path;\n\tstruct rtrs_clt_path *clt_path = to_clt_path(s);\n\tstruct rdma_cm_id *cm_id;\n\tint err;\n\n\tcm_id = rdma_create_id(&init_net, rtrs_clt_rdma_cm_handler, con,\n\t\t\t       clt_path->s.dst_addr.ss_family == AF_IB ?\n\t\t\t       RDMA_PS_IB : RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\terr = PTR_ERR(cm_id);\n\t\trtrs_err(s, \"Failed to create CM ID, err: %d\\n\", err);\n\n\t\treturn err;\n\t}\n\tcon->c.cm_id = cm_id;\n\tcon->cm_err = 0;\n\t \n\terr = rdma_set_reuseaddr(cm_id, 1);\n\tif (err != 0) {\n\t\trtrs_err(s, \"Set address reuse failed, err: %d\\n\", err);\n\t\treturn err;\n\t}\n\terr = rdma_resolve_addr(cm_id, (struct sockaddr *)&clt_path->s.src_addr,\n\t\t\t\t(struct sockaddr *)&clt_path->s.dst_addr,\n\t\t\t\tRTRS_CONNECT_TIMEOUT_MS);\n\tif (err) {\n\t\trtrs_err(s, \"Failed to resolve address, err: %d\\n\", err);\n\t\treturn err;\n\t}\n\t \n\terr = wait_event_interruptible_timeout(\n\t\t\tclt_path->state_wq,\n\t\t\tcon->cm_err || clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\tmsecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\tif (err == 0 || err == -ERESTARTSYS) {\n\t\tif (err == 0)\n\t\t\terr = -ETIMEDOUT;\n\t\t \n\t\treturn err;\n\t}\n\tif (con->cm_err < 0)\n\t\treturn con->cm_err;\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTING)\n\t\t \n\t\treturn -ECONNABORTED;\n\n\treturn 0;\n}\n\nstatic void rtrs_clt_path_up(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tint up;\n\n\t \n\n\tmutex_lock(&clt->paths_ev_mutex);\n\tup = ++clt->paths_up;\n\t \n\tif (up > MAX_PATHS_NUM && up == MAX_PATHS_NUM + clt->paths_num)\n\t\tclt->paths_up = clt->paths_num;\n\telse if (up == 1)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_RECONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n\n\t \n\tclt_path->established = true;\n\tclt_path->reconnect_attempts = 0;\n\tclt_path->stats->reconnects.successful_cnt++;\n}\n\nstatic void rtrs_clt_path_down(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tif (!clt_path->established)\n\t\treturn;\n\n\tclt_path->established = false;\n\tmutex_lock(&clt->paths_ev_mutex);\n\tWARN_ON(!clt->paths_up);\n\tif (--clt->paths_up == 0)\n\t\tclt->link_ev(clt->priv, RTRS_CLT_LINK_EV_DISCONNECTED);\n\tmutex_unlock(&clt->paths_ev_mutex);\n}\n\nstatic void rtrs_clt_stop_and_destroy_conns(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *con;\n\tunsigned int cid;\n\n\tWARN_ON(READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTED);\n\n\t \n\tmutex_lock(&clt_path->init_mutex);\n\tmutex_unlock(&clt_path->init_mutex);\n\n\t \n\tsynchronize_rcu();\n\n\trtrs_stop_hb(&clt_path->s);\n\n\t \n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tstop_cm(con);\n\t}\n\tfail_all_outstanding_reqs(clt_path);\n\tfree_path_reqs(clt_path);\n\trtrs_clt_path_down(clt_path);\n\n\t \n\twait_event_timeout(clt_path->state_wq,\n\t\t\t   !atomic_read(&clt_path->connected_cnt),\n\t\t\t   msecs_to_jiffies(RTRS_CONNECT_TIMEOUT_MS));\n\n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\tif (!clt_path->s.con[cid])\n\t\t\tbreak;\n\t\tcon = to_clt_con(clt_path->s.con[cid]);\n\t\tmutex_lock(&con->con_mutex);\n\t\tdestroy_con_cq_qp(con);\n\t\tmutex_unlock(&con->con_mutex);\n\t\tdestroy_cm(con);\n\t\tdestroy_con(con);\n\t}\n}\n\nstatic void rtrs_clt_remove_path_from_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\tstruct rtrs_clt_path *next;\n\tbool wait_for_grace = false;\n\tint cpu;\n\n\tmutex_lock(&clt->paths_mutex);\n\tlist_del_rcu(&clt_path->s.entry);\n\n\t \n\tsynchronize_rcu();\n\n\t \n\n\t \n\tclt->paths_num--;\n\n\t \n\trcu_read_lock();\n\tnext = rtrs_clt_get_next_path_or_null(&clt->paths_list, clt_path);\n\trcu_read_unlock();\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rtrs_clt_path __rcu **ppcpu_path;\n\n\t\tppcpu_path = per_cpu_ptr(clt->pcpu_path, cpu);\n\t\tif (rcu_dereference_protected(*ppcpu_path,\n\t\t\tlockdep_is_held(&clt->paths_mutex)) != clt_path)\n\t\t\t \n\t\t\tcontinue;\n\n\t\t \n\t\tif (try_cmpxchg((struct rtrs_clt_path **)ppcpu_path, &clt_path,\n\t\t\t\tnext))\n\t\t\t \n\t\t\twait_for_grace = true;\n\t}\n\tif (wait_for_grace)\n\t\tsynchronize_rcu();\n\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_add_path_to_arr(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_sess *clt = clt_path->clt;\n\n\tmutex_lock(&clt->paths_mutex);\n\tclt->paths_num++;\n\n\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\tmutex_unlock(&clt->paths_mutex);\n}\n\nstatic void rtrs_clt_close_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\n\tclt_path = container_of(work, struct rtrs_clt_path, close_work);\n\n\tcancel_work_sync(&clt_path->err_recovery_work);\n\tcancel_delayed_work_sync(&clt_path->reconnect_dwork);\n\trtrs_clt_stop_and_destroy_conns(clt_path);\n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CLOSED, NULL);\n}\n\nstatic int init_conns(struct rtrs_clt_path *clt_path)\n{\n\tunsigned int cid;\n\tint err, i;\n\n\t \n\tclt_path->s.recon_cnt++;\n\n\t \n\tfor (cid = 0; cid < clt_path->s.con_num; cid++) {\n\t\terr = create_con(clt_path, cid);\n\t\tif (err)\n\t\t\tgoto destroy;\n\n\t\terr = create_cm(to_clt_con(clt_path->s.con[cid]));\n\t\tif (err)\n\t\t\tgoto destroy;\n\t}\n\terr = alloc_path_reqs(clt_path);\n\tif (err)\n\t\tgoto destroy;\n\n\treturn 0;\n\ndestroy:\n\t \n\tfor (i = 0; i <= cid; i++) {\n\t\tstruct rtrs_clt_con *con;\n\n\t\tif (!clt_path->s.con[i])\n\t\t\tbreak;\n\n\t\tcon = to_clt_con(clt_path->s.con[i]);\n\t\tif (con->c.cm_id) {\n\t\t\tstop_cm(con);\n\t\t\tmutex_lock(&con->con_mutex);\n\t\t\tdestroy_con_cq_qp(con);\n\t\t\tmutex_unlock(&con->con_mutex);\n\t\t\tdestroy_cm(con);\n\t\t}\n\t\tdestroy_con(con);\n\t}\n\t \n\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_info_req_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_iu *iu;\n\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info request send failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\trtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING_ERR, NULL);\n\t\treturn;\n\t}\n\n\trtrs_clt_update_wc_stats(con);\n}\n\nstatic int process_info_rsp(struct rtrs_clt_path *clt_path,\n\t\t\t    const struct rtrs_msg_info_rsp *msg)\n{\n\tunsigned int sg_cnt, total_len;\n\tint i, sgi;\n\n\tsg_cnt = le16_to_cpu(msg->sg_cnt);\n\tif (!sg_cnt || (clt_path->queue_depth % sg_cnt)) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"Incorrect sg_cnt %d, is not multiple\\n\",\n\t\t\t  sg_cnt);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((ilog2(sg_cnt - 1) + 1) + (ilog2(clt_path->chunk_size - 1) + 1) >\n\t    MAX_IMM_PAYL_BITS) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t  \"RDMA immediate size (%db) not enough to encode %d buffers of size %dB\\n\",\n\t\t\t  MAX_IMM_PAYL_BITS, sg_cnt, clt_path->chunk_size);\n\t\treturn -EINVAL;\n\t}\n\ttotal_len = 0;\n\tfor (sgi = 0, i = 0; sgi < sg_cnt && i < clt_path->queue_depth; sgi++) {\n\t\tconst struct rtrs_sg_desc *desc = &msg->desc[sgi];\n\t\tu32 len, rkey;\n\t\tu64 addr;\n\n\t\taddr = le64_to_cpu(desc->addr);\n\t\trkey = le32_to_cpu(desc->key);\n\t\tlen  = le32_to_cpu(desc->len);\n\n\t\ttotal_len += len;\n\n\t\tif (!len || (len % clt_path->chunk_size)) {\n\t\t\trtrs_err(clt_path->clt, \"Incorrect [%d].len %d\\n\",\n\t\t\t\t  sgi,\n\t\t\t\t  len);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfor ( ; len && i < clt_path->queue_depth; i++) {\n\t\t\tclt_path->rbufs[i].addr = addr;\n\t\t\tclt_path->rbufs[i].rkey = rkey;\n\n\t\t\tlen  -= clt_path->chunk_size;\n\t\t\taddr += clt_path->chunk_size;\n\t\t}\n\t}\n\t \n\tif (sgi != sg_cnt || i != clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"Incorrect sg vector, not fully mapped\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (total_len != clt_path->chunk_size * clt_path->queue_depth) {\n\t\trtrs_err(clt_path->clt, \"Incorrect total_len %d\\n\", total_len);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void rtrs_clt_info_rsp_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct rtrs_clt_con *con = to_clt_con(wc->qp->qp_context);\n\tstruct rtrs_clt_path *clt_path = to_clt_path(con->c.path);\n\tstruct rtrs_msg_info_rsp *msg;\n\tenum rtrs_clt_state state;\n\tstruct rtrs_iu *iu;\n\tsize_t rx_sz;\n\tint err;\n\n\tstate = RTRS_CLT_CONNECTING_ERR;\n\n\tWARN_ON(con->c.cid);\n\tiu = container_of(wc->wr_cqe, struct rtrs_iu, cqe);\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\trtrs_err(clt_path->clt, \"Path info response recv failed: %s\\n\",\n\t\t\t  ib_wc_status_msg(wc->status));\n\t\tgoto out;\n\t}\n\tWARN_ON(wc->opcode != IB_WC_RECV);\n\n\tif (wc->byte_len < sizeof(*msg)) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\tib_dma_sync_single_for_cpu(clt_path->s.dev->ib_dev, iu->dma_addr,\n\t\t\t\t   iu->size, DMA_FROM_DEVICE);\n\tmsg = iu->buf;\n\tif (le16_to_cpu(msg->type) != RTRS_MSG_INFO_RSP) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: type %d\\n\",\n\t\t\t  le16_to_cpu(msg->type));\n\t\tgoto out;\n\t}\n\trx_sz  = sizeof(*msg);\n\trx_sz += sizeof(msg->desc[0]) * le16_to_cpu(msg->sg_cnt);\n\tif (wc->byte_len < rx_sz) {\n\t\trtrs_err(clt_path->clt, \"Path info response is malformed: size %d\\n\",\n\t\t\t  wc->byte_len);\n\t\tgoto out;\n\t}\n\terr = process_info_rsp(clt_path, msg);\n\tif (err)\n\t\tgoto out;\n\n\terr = post_recv_path(clt_path);\n\tif (err)\n\t\tgoto out;\n\n\tstate = RTRS_CLT_CONNECTED;\n\nout:\n\trtrs_clt_update_wc_stats(con);\n\trtrs_iu_free(iu, clt_path->s.dev->ib_dev, 1);\n\trtrs_clt_change_state_get_old(clt_path, state, NULL);\n}\n\nstatic int rtrs_send_path_info(struct rtrs_clt_path *clt_path)\n{\n\tstruct rtrs_clt_con *usr_con = to_clt_con(clt_path->s.con[0]);\n\tstruct rtrs_msg_info_req *msg;\n\tstruct rtrs_iu *tx_iu, *rx_iu;\n\tsize_t rx_sz;\n\tint err;\n\n\trx_sz  = sizeof(struct rtrs_msg_info_rsp);\n\trx_sz += sizeof(struct rtrs_sg_desc) * clt_path->queue_depth;\n\n\ttx_iu = rtrs_iu_alloc(1, sizeof(struct rtrs_msg_info_req), GFP_KERNEL,\n\t\t\t       clt_path->s.dev->ib_dev, DMA_TO_DEVICE,\n\t\t\t       rtrs_clt_info_req_done);\n\trx_iu = rtrs_iu_alloc(1, rx_sz, GFP_KERNEL, clt_path->s.dev->ib_dev,\n\t\t\t       DMA_FROM_DEVICE, rtrs_clt_info_rsp_done);\n\tif (!tx_iu || !rx_iu) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\t \n\terr = rtrs_iu_post_recv(&usr_con->c, rx_iu);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_recv(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\trx_iu = NULL;\n\n\tmsg = tx_iu->buf;\n\tmsg->type = cpu_to_le16(RTRS_MSG_INFO_REQ);\n\tmemcpy(msg->pathname, clt_path->s.sessname, sizeof(msg->pathname));\n\n\tib_dma_sync_single_for_device(clt_path->s.dev->ib_dev,\n\t\t\t\t      tx_iu->dma_addr,\n\t\t\t\t      tx_iu->size, DMA_TO_DEVICE);\n\n\t \n\terr = rtrs_iu_post_send(&usr_con->c, tx_iu, sizeof(*msg), NULL);\n\tif (err) {\n\t\trtrs_err(clt_path->clt, \"rtrs_iu_post_send(), err: %d\\n\", err);\n\t\tgoto out;\n\t}\n\ttx_iu = NULL;\n\n\t \n\twait_event_interruptible_timeout(clt_path->state_wq,\n\t\t\t\t\t clt_path->state != RTRS_CLT_CONNECTING,\n\t\t\t\t\t msecs_to_jiffies(\n\t\t\t\t\t\t RTRS_CONNECT_TIMEOUT_MS));\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED) {\n\t\tif (READ_ONCE(clt_path->state) == RTRS_CLT_CONNECTING_ERR)\n\t\t\terr = -ECONNRESET;\n\t\telse\n\t\t\terr = -ETIMEDOUT;\n\t}\n\nout:\n\tif (tx_iu)\n\t\trtrs_iu_free(tx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (rx_iu)\n\t\trtrs_iu_free(rx_iu, clt_path->s.dev->ib_dev, 1);\n\tif (err)\n\t\t \n\t\trtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t      RTRS_CLT_CONNECTING_ERR, NULL);\n\n\treturn err;\n}\n\n \nstatic int init_path(struct rtrs_clt_path *clt_path)\n{\n\tint err;\n\tchar str[NAME_MAX];\n\tstruct rtrs_addr path = {\n\t\t.src = &clt_path->s.src_addr,\n\t\t.dst = &clt_path->s.dst_addr,\n\t};\n\n\trtrs_addr_to_str(&path, str, sizeof(str));\n\n\tmutex_lock(&clt_path->init_mutex);\n\terr = init_conns(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"init_conns() failed: err=%d path=%s [%s:%u]\\n\", err,\n\t\t\t str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\terr = rtrs_send_path_info(clt_path);\n\tif (err) {\n\t\trtrs_err(clt_path->clt,\n\t\t\t \"rtrs_send_path_info() failed: err=%d path=%s [%s:%u]\\n\",\n\t\t\t err, str, clt_path->hca_name, clt_path->hca_port);\n\t\tgoto out;\n\t}\n\trtrs_clt_path_up(clt_path);\n\trtrs_start_hb(&clt_path->s);\nout:\n\tmutex_unlock(&clt_path->init_mutex);\n\n\treturn err;\n}\n\nstatic void rtrs_clt_reconnect_work(struct work_struct *work)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tstruct rtrs_clt_sess *clt;\n\tint err;\n\n\tclt_path = container_of(to_delayed_work(work), struct rtrs_clt_path,\n\t\t\t\treconnect_dwork);\n\tclt = clt_path->clt;\n\n\ttrace_rtrs_clt_reconnect_work(clt_path);\n\n\tif (READ_ONCE(clt_path->state) != RTRS_CLT_RECONNECTING)\n\t\treturn;\n\n\tif (clt_path->reconnect_attempts >= clt->max_reconnect_attempts) {\n\t\t \n\t\trtrs_clt_close_conns(clt_path, false);\n\t\treturn;\n\t}\n\tclt_path->reconnect_attempts++;\n\n\tmsleep(RTRS_RECONNECT_BACKOFF);\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_CONNECTING, NULL)) {\n\t\terr = init_path(clt_path);\n\t\tif (err)\n\t\t\tgoto reconnect_again;\n\t}\n\n\treturn;\n\nreconnect_again:\n\tif (rtrs_clt_change_state_get_old(clt_path, RTRS_CLT_RECONNECTING, NULL)) {\n\t\tclt_path->stats->reconnects.fail_cnt++;\n\t\tqueue_work(rtrs_wq, &clt_path->err_recovery_work);\n\t}\n}\n\nstatic void rtrs_clt_dev_release(struct device *dev)\n{\n\tstruct rtrs_clt_sess *clt = container_of(dev, struct rtrs_clt_sess,\n\t\t\t\t\t\t dev);\n\n\tmutex_destroy(&clt->paths_ev_mutex);\n\tmutex_destroy(&clt->paths_mutex);\n\tkfree(clt);\n}\n\nstatic struct rtrs_clt_sess *alloc_clt(const char *sessname, size_t paths_num,\n\t\t\t\t  u16 port, size_t pdu_sz, void *priv,\n\t\t\t\t  void\t(*link_ev)(void *priv,\n\t\t\t\t\t\t   enum rtrs_clt_link_ev ev),\n\t\t\t\t  unsigned int reconnect_delay_sec,\n\t\t\t\t  unsigned int max_reconnect_attempts)\n{\n\tstruct rtrs_clt_sess *clt;\n\tint err;\n\n\tif (!paths_num || paths_num > MAX_PATHS_NUM)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strlen(sessname) >= sizeof(clt->sessname))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tclt = kzalloc(sizeof(*clt), GFP_KERNEL);\n\tif (!clt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclt->pcpu_path = alloc_percpu(typeof(*clt->pcpu_path));\n\tif (!clt->pcpu_path) {\n\t\tkfree(clt);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tclt->dev.class = &rtrs_clt_dev_class;\n\tclt->dev.release = rtrs_clt_dev_release;\n\tuuid_gen(&clt->paths_uuid);\n\tINIT_LIST_HEAD_RCU(&clt->paths_list);\n\tclt->paths_num = paths_num;\n\tclt->paths_up = MAX_PATHS_NUM;\n\tclt->port = port;\n\tclt->pdu_sz = pdu_sz;\n\tclt->max_segments = RTRS_MAX_SEGMENTS;\n\tclt->reconnect_delay_sec = reconnect_delay_sec;\n\tclt->max_reconnect_attempts = max_reconnect_attempts;\n\tclt->priv = priv;\n\tclt->link_ev = link_ev;\n\tclt->mp_policy = MP_POLICY_MIN_INFLIGHT;\n\tstrscpy(clt->sessname, sessname, sizeof(clt->sessname));\n\tinit_waitqueue_head(&clt->permits_wait);\n\tmutex_init(&clt->paths_ev_mutex);\n\tmutex_init(&clt->paths_mutex);\n\tdevice_initialize(&clt->dev);\n\n\terr = dev_set_name(&clt->dev, \"%s\", sessname);\n\tif (err)\n\t\tgoto err_put;\n\n\t \n\tdev_set_uevent_suppress(&clt->dev, true);\n\terr = device_add(&clt->dev);\n\tif (err)\n\t\tgoto err_put;\n\n\tclt->kobj_paths = kobject_create_and_add(\"paths\", &clt->dev.kobj);\n\tif (!clt->kobj_paths) {\n\t\terr = -ENOMEM;\n\t\tgoto err_del;\n\t}\n\terr = rtrs_clt_create_sysfs_root_files(clt);\n\tif (err) {\n\t\tkobject_del(clt->kobj_paths);\n\t\tkobject_put(clt->kobj_paths);\n\t\tgoto err_del;\n\t}\n\tdev_set_uevent_suppress(&clt->dev, false);\n\tkobject_uevent(&clt->dev.kobj, KOBJ_ADD);\n\n\treturn clt;\nerr_del:\n\tdevice_del(&clt->dev);\nerr_put:\n\tfree_percpu(clt->pcpu_path);\n\tput_device(&clt->dev);\n\treturn ERR_PTR(err);\n}\n\nstatic void free_clt(struct rtrs_clt_sess *clt)\n{\n\tfree_percpu(clt->pcpu_path);\n\n\t \n\tdevice_unregister(&clt->dev);\n}\n\n \nstruct rtrs_clt_sess *rtrs_clt_open(struct rtrs_clt_ops *ops,\n\t\t\t\t const char *pathname,\n\t\t\t\t const struct rtrs_addr *paths,\n\t\t\t\t size_t paths_num, u16 port,\n\t\t\t\t size_t pdu_sz, u8 reconnect_delay_sec,\n\t\t\t\t s16 max_reconnect_attempts, u32 nr_poll_queues)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\tstruct rtrs_clt_sess *clt;\n\tint err, i;\n\n\tif (strchr(pathname, '/') || strchr(pathname, '.')) {\n\t\tpr_err(\"pathname cannot contain / and .\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tclt = alloc_clt(pathname, paths_num, port, pdu_sz, ops->priv,\n\t\t\tops->link_ev,\n\t\t\treconnect_delay_sec,\n\t\t\tmax_reconnect_attempts);\n\tif (IS_ERR(clt)) {\n\t\terr = PTR_ERR(clt);\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < paths_num; i++) {\n\t\tstruct rtrs_clt_path *clt_path;\n\n\t\tclt_path = alloc_path(clt, &paths[i], nr_cpu_ids,\n\t\t\t\t  nr_poll_queues);\n\t\tif (IS_ERR(clt_path)) {\n\t\t\terr = PTR_ERR(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t\tif (!i)\n\t\t\tclt_path->for_new_clt = 1;\n\t\tlist_add_tail_rcu(&clt_path->s.entry, &clt->paths_list);\n\n\t\terr = init_path(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\n\t\terr = rtrs_clt_create_path_files(clt_path);\n\t\tif (err) {\n\t\t\tlist_del_rcu(&clt_path->s.entry);\n\t\t\trtrs_clt_close_conns(clt_path, true);\n\t\t\tfree_percpu(clt_path->stats->pcpu_stats);\n\t\t\tkfree(clt_path->stats);\n\t\t\tfree_path(clt_path);\n\t\t\tgoto close_all_path;\n\t\t}\n\t}\n\terr = alloc_permits(clt);\n\tif (err)\n\t\tgoto close_all_path;\n\n\treturn clt;\n\nclose_all_path:\n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\trtrs_clt_destroy_sysfs_root(clt);\n\tfree_clt(clt);\n\nout:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(rtrs_clt_open);\n\n \nvoid rtrs_clt_close(struct rtrs_clt_sess *clt)\n{\n\tstruct rtrs_clt_path *clt_path, *tmp;\n\n\t \n\trtrs_clt_destroy_sysfs_root(clt);\n\n\t \n\tlist_for_each_entry_safe(clt_path, tmp, &clt->paths_list, s.entry) {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\trtrs_clt_destroy_path_files(clt_path, NULL);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\tfree_permits(clt);\n\tfree_clt(clt);\n}\nEXPORT_SYMBOL(rtrs_clt_close);\n\nint rtrs_clt_reconnect_from_sysfs(struct rtrs_clt_path *clt_path)\n{\n\tenum rtrs_clt_state old_state;\n\tint err = -EBUSY;\n\tbool changed;\n\n\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t RTRS_CLT_RECONNECTING,\n\t\t\t\t\t\t &old_state);\n\tif (changed) {\n\t\tclt_path->reconnect_attempts = 0;\n\t\trtrs_clt_stop_and_destroy_conns(clt_path);\n\t\tqueue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork, 0);\n\t}\n\tif (changed || old_state == RTRS_CLT_RECONNECTING) {\n\t\t \n\t\tflush_delayed_work(&clt_path->reconnect_dwork);\n\t\terr = (READ_ONCE(clt_path->state) ==\n\t\t       RTRS_CLT_CONNECTED ? 0 : -ENOTCONN);\n\t}\n\n\treturn err;\n}\n\nint rtrs_clt_remove_path_from_sysfs(struct rtrs_clt_path *clt_path,\n\t\t\t\t     const struct attribute *sysfs_self)\n{\n\tenum rtrs_clt_state old_state;\n\tbool changed;\n\n\t \n\tdo {\n\t\trtrs_clt_close_conns(clt_path, true);\n\t\tchanged = rtrs_clt_change_state_get_old(clt_path,\n\t\t\t\t\t\t\tRTRS_CLT_DEAD,\n\t\t\t\t\t\t\t&old_state);\n\t} while (!changed && old_state != RTRS_CLT_DEAD);\n\n\tif (changed) {\n\t\trtrs_clt_remove_path_from_arr(clt_path);\n\t\trtrs_clt_destroy_path_files(clt_path, sysfs_self);\n\t\tkobject_put(&clt_path->kobj);\n\t}\n\n\treturn 0;\n}\n\nvoid rtrs_clt_set_max_reconnect_attempts(struct rtrs_clt_sess *clt, int value)\n{\n\tclt->max_reconnect_attempts = (unsigned int)value;\n}\n\nint rtrs_clt_get_max_reconnect_attempts(const struct rtrs_clt_sess *clt)\n{\n\treturn (int)clt->max_reconnect_attempts;\n}\n\n \nint rtrs_clt_request(int dir, struct rtrs_clt_req_ops *ops,\n\t\t     struct rtrs_clt_sess *clt, struct rtrs_permit *permit,\n\t\t     const struct kvec *vec, size_t nr, size_t data_len,\n\t\t     struct scatterlist *sg, unsigned int sg_cnt)\n{\n\tstruct rtrs_clt_io_req *req;\n\tstruct rtrs_clt_path *clt_path;\n\n\tenum dma_data_direction dma_dir;\n\tint err = -ECONNABORTED, i;\n\tsize_t usr_len, hdr_len;\n\tstruct path_it it;\n\n\t \n\tfor (i = 0, usr_len = 0; i < nr; i++)\n\t\tusr_len += vec[i].iov_len;\n\n\tif (dir == READ) {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_read) +\n\t\t\t  sg_cnt * sizeof(struct rtrs_sg_desc);\n\t\tdma_dir = DMA_FROM_DEVICE;\n\t} else {\n\t\thdr_len = sizeof(struct rtrs_msg_rdma_write);\n\t\tdma_dir = DMA_TO_DEVICE;\n\t}\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tif (usr_len + hdr_len > clt_path->max_hdr_size) {\n\t\t\trtrs_wrn_rl(clt_path->clt,\n\t\t\t\t     \"%s request failed, user message size is %zu and header length %zu, but max size is %u\\n\",\n\t\t\t\t     dir == READ ? \"Read\" : \"Write\",\n\t\t\t\t     usr_len, hdr_len, clt_path->max_hdr_size);\n\t\t\terr = -EMSGSIZE;\n\t\t\tbreak;\n\t\t}\n\t\treq = rtrs_clt_get_req(clt_path, ops->conf_fn, permit, ops->priv,\n\t\t\t\t       vec, usr_len, sg, sg_cnt, data_len,\n\t\t\t\t       dma_dir);\n\t\tif (dir == READ)\n\t\t\terr = rtrs_clt_read_req(req);\n\t\telse\n\t\t\terr = rtrs_clt_write_req(req);\n\t\tif (err) {\n\t\t\treq->in_use = false;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(rtrs_clt_request);\n\nint rtrs_clt_rdma_cq_direct(struct rtrs_clt_sess *clt, unsigned int index)\n{\n\t \n\tint cnt = -1;\n\tstruct rtrs_con *con;\n\tstruct rtrs_clt_path *clt_path;\n\tstruct path_it it;\n\n\trcu_read_lock();\n\tfor (path_it_init(&it, clt);\n\t     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) {\n\t\tif (READ_ONCE(clt_path->state) != RTRS_CLT_CONNECTED)\n\t\t\tcontinue;\n\n\t\tcon = clt_path->s.con[index + 1];\n\t\tcnt = ib_process_cq_direct(con->cq, -1);\n\t\tif (cnt)\n\t\t\tbreak;\n\t}\n\tpath_it_deinit(&it);\n\trcu_read_unlock();\n\n\treturn cnt;\n}\nEXPORT_SYMBOL(rtrs_clt_rdma_cq_direct);\n\n \nint rtrs_clt_query(struct rtrs_clt_sess *clt, struct rtrs_attrs *attr)\n{\n\tif (!rtrs_clt_is_connected(clt))\n\t\treturn -ECOMM;\n\n\tattr->queue_depth      = clt->queue_depth;\n\tattr->max_segments     = clt->max_segments;\n\t \n\tattr->max_io_size = min_t(int, clt->max_io_size,\n\t\t\t\t  clt->max_segments * SZ_4K);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rtrs_clt_query);\n\nint rtrs_clt_create_path_from_sysfs(struct rtrs_clt_sess *clt,\n\t\t\t\t     struct rtrs_addr *addr)\n{\n\tstruct rtrs_clt_path *clt_path;\n\tint err;\n\n\tclt_path = alloc_path(clt, addr, nr_cpu_ids, 0);\n\tif (IS_ERR(clt_path))\n\t\treturn PTR_ERR(clt_path);\n\n\tmutex_lock(&clt->paths_mutex);\n\tif (clt->paths_num == 0) {\n\t\t \n\t\tclt_path->for_new_clt = 1;\n\t}\n\n\tmutex_unlock(&clt->paths_mutex);\n\n\t \n\trtrs_clt_add_path_to_arr(clt_path);\n\n\terr = init_path(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\terr = rtrs_clt_create_path_files(clt_path);\n\tif (err)\n\t\tgoto close_path;\n\n\treturn 0;\n\nclose_path:\n\trtrs_clt_remove_path_from_arr(clt_path);\n\trtrs_clt_close_conns(clt_path, true);\n\tfree_percpu(clt_path->stats->pcpu_stats);\n\tkfree(clt_path->stats);\n\tfree_path(clt_path);\n\n\treturn err;\n}\n\nstatic int rtrs_clt_ib_dev_init(struct rtrs_ib_dev *dev)\n{\n\tif (!(dev->ib_dev->attrs.device_cap_flags &\n\t      IB_DEVICE_MEM_MGT_EXTENSIONS)) {\n\t\tpr_err(\"Memory registrations not supported.\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct rtrs_rdma_dev_pd_ops dev_pd_ops = {\n\t.init = rtrs_clt_ib_dev_init\n};\n\nstatic int __init rtrs_client_init(void)\n{\n\tint ret = 0;\n\n\trtrs_rdma_dev_pd_init(0, &dev_pd);\n\tret = class_register(&rtrs_clt_dev_class);\n\tif (ret) {\n\t\tpr_err(\"Failed to create rtrs-client dev class\\n\");\n\t\treturn ret;\n\t}\n\trtrs_wq = alloc_workqueue(\"rtrs_client_wq\", 0, 0);\n\tif (!rtrs_wq) {\n\t\tclass_unregister(&rtrs_clt_dev_class);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit rtrs_client_exit(void)\n{\n\tdestroy_workqueue(rtrs_wq);\n\tclass_unregister(&rtrs_clt_dev_class);\n\trtrs_rdma_dev_pd_deinit(&dev_pd);\n}\n\nmodule_init(rtrs_client_init);\nmodule_exit(rtrs_client_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}