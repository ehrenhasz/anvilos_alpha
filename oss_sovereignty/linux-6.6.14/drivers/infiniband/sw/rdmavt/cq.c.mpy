{
  "module_name": "cq.c",
  "hash_id": "e6bf4ac1b7d9bc2679d8bc70e7767b104cd42cdf36995217b03a9aed11f8a472",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rdmavt/cq.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include \"cq.h\"\n#include \"vt.h\"\n#include \"trace.h\"\n\nstatic struct workqueue_struct *comp_vector_wq;\n\n \nbool rvt_cq_enter(struct rvt_cq *cq, struct ib_wc *entry, bool solicited)\n{\n\tstruct ib_uverbs_wc *uqueue = NULL;\n\tstruct ib_wc *kqueue = NULL;\n\tstruct rvt_cq_wc *u_wc = NULL;\n\tstruct rvt_k_cq_wc *k_wc = NULL;\n\tunsigned long flags;\n\tu32 head;\n\tu32 next;\n\tu32 tail;\n\n\tspin_lock_irqsave(&cq->lock, flags);\n\n\tif (cq->ip) {\n\t\tu_wc = cq->queue;\n\t\tuqueue = &u_wc->uqueue[0];\n\t\thead = RDMA_READ_UAPI_ATOMIC(u_wc->head);\n\t\ttail = RDMA_READ_UAPI_ATOMIC(u_wc->tail);\n\t} else {\n\t\tk_wc = cq->kqueue;\n\t\tkqueue = &k_wc->kqueue[0];\n\t\thead = k_wc->head;\n\t\ttail = k_wc->tail;\n\t}\n\n\t \n\tif (head >= (unsigned)cq->ibcq.cqe) {\n\t\thead = cq->ibcq.cqe;\n\t\tnext = 0;\n\t} else {\n\t\tnext = head + 1;\n\t}\n\n\tif (unlikely(next == tail || cq->cq_full)) {\n\t\tstruct rvt_dev_info *rdi = cq->rdi;\n\n\t\tif (!cq->cq_full)\n\t\t\trvt_pr_err_ratelimited(rdi, \"CQ is full!\\n\");\n\t\tcq->cq_full = true;\n\t\tspin_unlock_irqrestore(&cq->lock, flags);\n\t\tif (cq->ibcq.event_handler) {\n\t\t\tstruct ib_event ev;\n\n\t\t\tev.device = cq->ibcq.device;\n\t\t\tev.element.cq = &cq->ibcq;\n\t\t\tev.event = IB_EVENT_CQ_ERR;\n\t\t\tcq->ibcq.event_handler(&ev, cq->ibcq.cq_context);\n\t\t}\n\t\treturn false;\n\t}\n\ttrace_rvt_cq_enter(cq, entry, head);\n\tif (uqueue) {\n\t\tuqueue[head].wr_id = entry->wr_id;\n\t\tuqueue[head].status = entry->status;\n\t\tuqueue[head].opcode = entry->opcode;\n\t\tuqueue[head].vendor_err = entry->vendor_err;\n\t\tuqueue[head].byte_len = entry->byte_len;\n\t\tuqueue[head].ex.imm_data = entry->ex.imm_data;\n\t\tuqueue[head].qp_num = entry->qp->qp_num;\n\t\tuqueue[head].src_qp = entry->src_qp;\n\t\tuqueue[head].wc_flags = entry->wc_flags;\n\t\tuqueue[head].pkey_index = entry->pkey_index;\n\t\tuqueue[head].slid = ib_lid_cpu16(entry->slid);\n\t\tuqueue[head].sl = entry->sl;\n\t\tuqueue[head].dlid_path_bits = entry->dlid_path_bits;\n\t\tuqueue[head].port_num = entry->port_num;\n\t\t \n\t\tRDMA_WRITE_UAPI_ATOMIC(u_wc->head, next);\n\t} else {\n\t\tkqueue[head] = *entry;\n\t\tk_wc->head = next;\n\t}\n\n\tif (cq->notify == IB_CQ_NEXT_COMP ||\n\t    (cq->notify == IB_CQ_SOLICITED &&\n\t     (solicited || entry->status != IB_WC_SUCCESS))) {\n\t\t \n\t\tcq->notify = RVT_CQ_NONE;\n\t\tcq->triggered++;\n\t\tqueue_work_on(cq->comp_vector_cpu, comp_vector_wq,\n\t\t\t      &cq->comptask);\n\t}\n\n\tspin_unlock_irqrestore(&cq->lock, flags);\n\treturn true;\n}\nEXPORT_SYMBOL(rvt_cq_enter);\n\nstatic void send_complete(struct work_struct *work)\n{\n\tstruct rvt_cq *cq = container_of(work, struct rvt_cq, comptask);\n\n\t \n\tfor (;;) {\n\t\tu8 triggered = cq->triggered;\n\n\t\t \n\t\tlocal_bh_disable();\n\t\tcq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\n\t\tlocal_bh_enable();\n\n\t\tif (cq->triggered == triggered)\n\t\t\treturn;\n\t}\n}\n\n \nint rvt_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct ib_device *ibdev = ibcq->device;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibdev);\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(ibcq);\n\tstruct rvt_cq_wc *u_wc = NULL;\n\tstruct rvt_k_cq_wc *k_wc = NULL;\n\tu32 sz;\n\tunsigned int entries = attr->cqe;\n\tint comp_vector = attr->comp_vector;\n\tint err;\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (entries < 1 || entries > rdi->dparms.props.max_cqe)\n\t\treturn -EINVAL;\n\n\tif (comp_vector < 0)\n\t\tcomp_vector = 0;\n\n\tcomp_vector = comp_vector % rdi->ibdev.num_comp_vectors;\n\n\t \n\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\tsz = sizeof(struct ib_uverbs_wc) * (entries + 1);\n\t\tsz += sizeof(*u_wc);\n\t\tu_wc = vmalloc_user(sz);\n\t\tif (!u_wc)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tsz = sizeof(struct ib_wc) * (entries + 1);\n\t\tsz += sizeof(*k_wc);\n\t\tk_wc = vzalloc_node(sz, rdi->dparms.node);\n\t\tif (!k_wc)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\tcq->ip = rvt_create_mmap_info(rdi, sz, udata, u_wc);\n\t\tif (IS_ERR(cq->ip)) {\n\t\t\terr = PTR_ERR(cq->ip);\n\t\t\tgoto bail_wc;\n\t\t}\n\n\t\terr = ib_copy_to_udata(udata, &cq->ip->offset,\n\t\t\t\t       sizeof(cq->ip->offset));\n\t\tif (err)\n\t\t\tgoto bail_ip;\n\t}\n\n\tspin_lock_irq(&rdi->n_cqs_lock);\n\tif (rdi->n_cqs_allocated == rdi->dparms.props.max_cq) {\n\t\tspin_unlock_irq(&rdi->n_cqs_lock);\n\t\terr = -ENOMEM;\n\t\tgoto bail_ip;\n\t}\n\n\trdi->n_cqs_allocated++;\n\tspin_unlock_irq(&rdi->n_cqs_lock);\n\n\tif (cq->ip) {\n\t\tspin_lock_irq(&rdi->pending_lock);\n\t\tlist_add(&cq->ip->pending_mmaps, &rdi->pending_mmaps);\n\t\tspin_unlock_irq(&rdi->pending_lock);\n\t}\n\n\t \n\tcq->rdi = rdi;\n\tif (rdi->driver_f.comp_vect_cpu_lookup)\n\t\tcq->comp_vector_cpu =\n\t\t\trdi->driver_f.comp_vect_cpu_lookup(rdi, comp_vector);\n\telse\n\t\tcq->comp_vector_cpu =\n\t\t\tcpumask_first(cpumask_of_node(rdi->dparms.node));\n\n\tcq->ibcq.cqe = entries;\n\tcq->notify = RVT_CQ_NONE;\n\tspin_lock_init(&cq->lock);\n\tINIT_WORK(&cq->comptask, send_complete);\n\tif (u_wc)\n\t\tcq->queue = u_wc;\n\telse\n\t\tcq->kqueue = k_wc;\n\n\ttrace_rvt_create_cq(cq, attr);\n\treturn 0;\n\nbail_ip:\n\tkfree(cq->ip);\nbail_wc:\n\tvfree(u_wc);\n\tvfree(k_wc);\n\treturn err;\n}\n\n \nint rvt_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(ibcq);\n\tstruct rvt_dev_info *rdi = cq->rdi;\n\n\tflush_work(&cq->comptask);\n\tspin_lock_irq(&rdi->n_cqs_lock);\n\trdi->n_cqs_allocated--;\n\tspin_unlock_irq(&rdi->n_cqs_lock);\n\tif (cq->ip)\n\t\tkref_put(&cq->ip->ref, rvt_release_mmap_info);\n\telse\n\t\tvfree(cq->kqueue);\n\treturn 0;\n}\n\n \nint rvt_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify_flags)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(ibcq);\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&cq->lock, flags);\n\t \n\tif (cq->notify != IB_CQ_NEXT_COMP)\n\t\tcq->notify = notify_flags & IB_CQ_SOLICITED_MASK;\n\n\tif (notify_flags & IB_CQ_REPORT_MISSED_EVENTS) {\n\t\tif (cq->queue) {\n\t\t\tif (RDMA_READ_UAPI_ATOMIC(cq->queue->head) !=\n\t\t\t\tRDMA_READ_UAPI_ATOMIC(cq->queue->tail))\n\t\t\t\tret = 1;\n\t\t} else {\n\t\t\tif (cq->kqueue->head != cq->kqueue->tail)\n\t\t\t\tret = 1;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&cq->lock, flags);\n\n\treturn ret;\n}\n\n \nint rvt_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(ibcq);\n\tu32 head, tail, n;\n\tint ret;\n\tu32 sz;\n\tstruct rvt_dev_info *rdi = cq->rdi;\n\tstruct rvt_cq_wc *u_wc = NULL;\n\tstruct rvt_cq_wc *old_u_wc = NULL;\n\tstruct rvt_k_cq_wc *k_wc = NULL;\n\tstruct rvt_k_cq_wc *old_k_wc = NULL;\n\n\tif (cqe < 1 || cqe > rdi->dparms.props.max_cqe)\n\t\treturn -EINVAL;\n\n\t \n\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\tsz = sizeof(struct ib_uverbs_wc) * (cqe + 1);\n\t\tsz += sizeof(*u_wc);\n\t\tu_wc = vmalloc_user(sz);\n\t\tif (!u_wc)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tsz = sizeof(struct ib_wc) * (cqe + 1);\n\t\tsz += sizeof(*k_wc);\n\t\tk_wc = vzalloc_node(sz, rdi->dparms.node);\n\t\tif (!k_wc)\n\t\t\treturn -ENOMEM;\n\t}\n\t \n\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\t__u64 offset = 0;\n\n\t\tret = ib_copy_to_udata(udata, &offset, sizeof(offset));\n\t\tif (ret)\n\t\t\tgoto bail_free;\n\t}\n\n\tspin_lock_irq(&cq->lock);\n\t \n\tif (u_wc) {\n\t\told_u_wc = cq->queue;\n\t\thead = RDMA_READ_UAPI_ATOMIC(old_u_wc->head);\n\t\ttail = RDMA_READ_UAPI_ATOMIC(old_u_wc->tail);\n\t} else {\n\t\told_k_wc = cq->kqueue;\n\t\thead = old_k_wc->head;\n\t\ttail = old_k_wc->tail;\n\t}\n\n\tif (head > (u32)cq->ibcq.cqe)\n\t\thead = (u32)cq->ibcq.cqe;\n\tif (tail > (u32)cq->ibcq.cqe)\n\t\ttail = (u32)cq->ibcq.cqe;\n\tif (head < tail)\n\t\tn = cq->ibcq.cqe + 1 + head - tail;\n\telse\n\t\tn = head - tail;\n\tif (unlikely((u32)cqe < n)) {\n\t\tret = -EINVAL;\n\t\tgoto bail_unlock;\n\t}\n\tfor (n = 0; tail != head; n++) {\n\t\tif (u_wc)\n\t\t\tu_wc->uqueue[n] = old_u_wc->uqueue[tail];\n\t\telse\n\t\t\tk_wc->kqueue[n] = old_k_wc->kqueue[tail];\n\t\tif (tail == (u32)cq->ibcq.cqe)\n\t\t\ttail = 0;\n\t\telse\n\t\t\ttail++;\n\t}\n\tcq->ibcq.cqe = cqe;\n\tif (u_wc) {\n\t\tRDMA_WRITE_UAPI_ATOMIC(u_wc->head, n);\n\t\tRDMA_WRITE_UAPI_ATOMIC(u_wc->tail, 0);\n\t\tcq->queue = u_wc;\n\t} else {\n\t\tk_wc->head = n;\n\t\tk_wc->tail = 0;\n\t\tcq->kqueue = k_wc;\n\t}\n\tspin_unlock_irq(&cq->lock);\n\n\tif (u_wc)\n\t\tvfree(old_u_wc);\n\telse\n\t\tvfree(old_k_wc);\n\n\tif (cq->ip) {\n\t\tstruct rvt_mmap_info *ip = cq->ip;\n\n\t\trvt_update_mmap_info(rdi, ip, sz, u_wc);\n\n\t\t \n\t\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\t\tret = ib_copy_to_udata(udata, &ip->offset,\n\t\t\t\t\t       sizeof(ip->offset));\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tspin_lock_irq(&rdi->pending_lock);\n\t\tif (list_empty(&ip->pending_mmaps))\n\t\t\tlist_add(&ip->pending_mmaps, &rdi->pending_mmaps);\n\t\tspin_unlock_irq(&rdi->pending_lock);\n\t}\n\n\treturn 0;\n\nbail_unlock:\n\tspin_unlock_irq(&cq->lock);\nbail_free:\n\tvfree(u_wc);\n\tvfree(k_wc);\n\n\treturn ret;\n}\n\n \nint rvt_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(ibcq);\n\tstruct rvt_k_cq_wc *wc;\n\tunsigned long flags;\n\tint npolled;\n\tu32 tail;\n\n\t \n\tif (cq->ip)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&cq->lock, flags);\n\n\twc = cq->kqueue;\n\ttail = wc->tail;\n\tif (tail > (u32)cq->ibcq.cqe)\n\t\ttail = (u32)cq->ibcq.cqe;\n\tfor (npolled = 0; npolled < num_entries; ++npolled, ++entry) {\n\t\tif (tail == wc->head)\n\t\t\tbreak;\n\t\t \n\t\ttrace_rvt_cq_poll(cq, &wc->kqueue[tail], npolled);\n\t\t*entry = wc->kqueue[tail];\n\t\tif (tail >= cq->ibcq.cqe)\n\t\t\ttail = 0;\n\t\telse\n\t\t\ttail++;\n\t}\n\twc->tail = tail;\n\n\tspin_unlock_irqrestore(&cq->lock, flags);\n\n\treturn npolled;\n}\n\n \nint rvt_driver_cq_init(void)\n{\n\tcomp_vector_wq = alloc_workqueue(\"%s\", WQ_HIGHPRI | WQ_CPU_INTENSIVE,\n\t\t\t\t\t 0, \"rdmavt_cq\");\n\tif (!comp_vector_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nvoid rvt_cq_exit(void)\n{\n\tdestroy_workqueue(comp_vector_wq);\n\tcomp_vector_wq = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}