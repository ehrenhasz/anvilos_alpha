{
  "module_name": "mr.c",
  "hash_id": "cc2227ec874bafa39ddc2d747187c890981e5528fcc54f24cb7191057d2e3672",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rdmavt/mr.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <rdma/ib_umem.h>\n#include <rdma/rdma_vt.h>\n#include \"vt.h\"\n#include \"mr.h\"\n#include \"trace.h\"\n\n \nint rvt_driver_mr_init(struct rvt_dev_info *rdi)\n{\n\tunsigned int lkey_table_size = rdi->dparms.lkey_table_size;\n\tunsigned lk_tab_size;\n\tint i;\n\n\t \n\tif (!lkey_table_size)\n\t\treturn -EINVAL;\n\n\tspin_lock_init(&rdi->lkey_table.lock);\n\n\t \n\tif (lkey_table_size > RVT_MAX_LKEY_TABLE_BITS) {\n\t\trvt_pr_warn(rdi, \"lkey bits %u too large, reduced to %u\\n\",\n\t\t\t    lkey_table_size, RVT_MAX_LKEY_TABLE_BITS);\n\t\trdi->dparms.lkey_table_size = RVT_MAX_LKEY_TABLE_BITS;\n\t\tlkey_table_size = rdi->dparms.lkey_table_size;\n\t}\n\trdi->lkey_table.max = 1 << lkey_table_size;\n\trdi->lkey_table.shift = 32 - lkey_table_size;\n\tlk_tab_size = rdi->lkey_table.max * sizeof(*rdi->lkey_table.table);\n\trdi->lkey_table.table = (struct rvt_mregion __rcu **)\n\t\t\t       vmalloc_node(lk_tab_size, rdi->dparms.node);\n\tif (!rdi->lkey_table.table)\n\t\treturn -ENOMEM;\n\n\tRCU_INIT_POINTER(rdi->dma_mr, NULL);\n\tfor (i = 0; i < rdi->lkey_table.max; i++)\n\t\tRCU_INIT_POINTER(rdi->lkey_table.table[i], NULL);\n\n\trdi->dparms.props.max_mr = rdi->lkey_table.max;\n\treturn 0;\n}\n\n \nvoid rvt_mr_exit(struct rvt_dev_info *rdi)\n{\n\tif (rdi->dma_mr)\n\t\trvt_pr_err(rdi, \"DMA MR not null!\\n\");\n\n\tvfree(rdi->lkey_table.table);\n}\n\nstatic void rvt_deinit_mregion(struct rvt_mregion *mr)\n{\n\tint i = mr->mapsz;\n\n\tmr->mapsz = 0;\n\twhile (i)\n\t\tkfree(mr->map[--i]);\n\tpercpu_ref_exit(&mr->refcount);\n}\n\nstatic void __rvt_mregion_complete(struct percpu_ref *ref)\n{\n\tstruct rvt_mregion *mr = container_of(ref, struct rvt_mregion,\n\t\t\t\t\t      refcount);\n\n\tcomplete(&mr->comp);\n}\n\nstatic int rvt_init_mregion(struct rvt_mregion *mr, struct ib_pd *pd,\n\t\t\t    int count, unsigned int percpu_flags)\n{\n\tint m, i = 0;\n\tstruct rvt_dev_info *dev = ib_to_rvt(pd->device);\n\n\tmr->mapsz = 0;\n\tm = (count + RVT_SEGSZ - 1) / RVT_SEGSZ;\n\tfor (; i < m; i++) {\n\t\tmr->map[i] = kzalloc_node(sizeof(*mr->map[0]), GFP_KERNEL,\n\t\t\t\t\t  dev->dparms.node);\n\t\tif (!mr->map[i])\n\t\t\tgoto bail;\n\t\tmr->mapsz++;\n\t}\n\tinit_completion(&mr->comp);\n\t \n\tif (percpu_ref_init(&mr->refcount, &__rvt_mregion_complete,\n\t\t\t    percpu_flags, GFP_KERNEL))\n\t\tgoto bail;\n\n\tatomic_set(&mr->lkey_invalid, 0);\n\tmr->pd = pd;\n\tmr->max_segs = count;\n\treturn 0;\nbail:\n\trvt_deinit_mregion(mr);\n\treturn -ENOMEM;\n}\n\n \nstatic int rvt_alloc_lkey(struct rvt_mregion *mr, int dma_region)\n{\n\tunsigned long flags;\n\tu32 r;\n\tu32 n;\n\tint ret = 0;\n\tstruct rvt_dev_info *dev = ib_to_rvt(mr->pd->device);\n\tstruct rvt_lkey_table *rkt = &dev->lkey_table;\n\n\trvt_get_mr(mr);\n\tspin_lock_irqsave(&rkt->lock, flags);\n\n\t \n\tif (dma_region) {\n\t\tstruct rvt_mregion *tmr;\n\n\t\ttmr = rcu_access_pointer(dev->dma_mr);\n\t\tif (!tmr) {\n\t\t\tmr->lkey_published = 1;\n\t\t\t \n\t\t\trcu_assign_pointer(dev->dma_mr, mr);\n\t\t\trvt_get_mr(mr);\n\t\t}\n\t\tgoto success;\n\t}\n\n\t \n\tr = rkt->next;\n\tn = r;\n\tfor (;;) {\n\t\tif (!rcu_access_pointer(rkt->table[r]))\n\t\t\tbreak;\n\t\tr = (r + 1) & (rkt->max - 1);\n\t\tif (r == n)\n\t\t\tgoto bail;\n\t}\n\trkt->next = (r + 1) & (rkt->max - 1);\n\t \n\trkt->gen++;\n\t \n\tmr->lkey = (r << (32 - dev->dparms.lkey_table_size)) |\n\t\t((((1 << (24 - dev->dparms.lkey_table_size)) - 1) & rkt->gen)\n\t\t << 8);\n\tif (mr->lkey == 0) {\n\t\tmr->lkey |= 1 << 8;\n\t\trkt->gen++;\n\t}\n\tmr->lkey_published = 1;\n\t \n\trcu_assign_pointer(rkt->table[r], mr);\nsuccess:\n\tspin_unlock_irqrestore(&rkt->lock, flags);\nout:\n\treturn ret;\nbail:\n\trvt_put_mr(mr);\n\tspin_unlock_irqrestore(&rkt->lock, flags);\n\tret = -ENOMEM;\n\tgoto out;\n}\n\n \nstatic void rvt_free_lkey(struct rvt_mregion *mr)\n{\n\tunsigned long flags;\n\tu32 lkey = mr->lkey;\n\tu32 r;\n\tstruct rvt_dev_info *dev = ib_to_rvt(mr->pd->device);\n\tstruct rvt_lkey_table *rkt = &dev->lkey_table;\n\tint freed = 0;\n\n\tspin_lock_irqsave(&rkt->lock, flags);\n\tif (!lkey) {\n\t\tif (mr->lkey_published) {\n\t\t\tmr->lkey_published = 0;\n\t\t\t \n\t\t\trcu_assign_pointer(dev->dma_mr, NULL);\n\t\t\trvt_put_mr(mr);\n\t\t}\n\t} else {\n\t\tif (!mr->lkey_published)\n\t\t\tgoto out;\n\t\tr = lkey >> (32 - dev->dparms.lkey_table_size);\n\t\tmr->lkey_published = 0;\n\t\t \n\t\trcu_assign_pointer(rkt->table[r], NULL);\n\t}\n\tfreed++;\nout:\n\tspin_unlock_irqrestore(&rkt->lock, flags);\n\tif (freed)\n\t\tpercpu_ref_kill(&mr->refcount);\n}\n\nstatic struct rvt_mr *__rvt_alloc_mr(int count, struct ib_pd *pd)\n{\n\tstruct rvt_mr *mr;\n\tint rval = -ENOMEM;\n\tint m;\n\n\t \n\tm = (count + RVT_SEGSZ - 1) / RVT_SEGSZ;\n\tmr = kzalloc(struct_size(mr, mr.map, m), GFP_KERNEL);\n\tif (!mr)\n\t\tgoto bail;\n\n\trval = rvt_init_mregion(&mr->mr, pd, count, 0);\n\tif (rval)\n\t\tgoto bail;\n\t \n\trval = rvt_alloc_lkey(&mr->mr, 0);\n\tif (rval)\n\t\tgoto bail_mregion;\n\tmr->ibmr.lkey = mr->mr.lkey;\n\tmr->ibmr.rkey = mr->mr.lkey;\ndone:\n\treturn mr;\n\nbail_mregion:\n\trvt_deinit_mregion(&mr->mr);\nbail:\n\tkfree(mr);\n\tmr = ERR_PTR(rval);\n\tgoto done;\n}\n\nstatic void __rvt_free_mr(struct rvt_mr *mr)\n{\n\trvt_free_lkey(&mr->mr);\n\trvt_deinit_mregion(&mr->mr);\n\tkfree(mr);\n}\n\n \nstruct ib_mr *rvt_get_dma_mr(struct ib_pd *pd, int acc)\n{\n\tstruct rvt_mr *mr;\n\tstruct ib_mr *ret;\n\tint rval;\n\n\tif (ibpd_to_rvtpd(pd)->user)\n\t\treturn ERR_PTR(-EPERM);\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto bail;\n\t}\n\n\trval = rvt_init_mregion(&mr->mr, pd, 0, 0);\n\tif (rval) {\n\t\tret = ERR_PTR(rval);\n\t\tgoto bail;\n\t}\n\n\trval = rvt_alloc_lkey(&mr->mr, 1);\n\tif (rval) {\n\t\tret = ERR_PTR(rval);\n\t\tgoto bail_mregion;\n\t}\n\n\tmr->mr.access_flags = acc;\n\tret = &mr->ibmr;\ndone:\n\treturn ret;\n\nbail_mregion:\n\trvt_deinit_mregion(&mr->mr);\nbail:\n\tkfree(mr);\n\tgoto done;\n}\n\n \nstruct ib_mr *rvt_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t      u64 virt_addr, int mr_access_flags,\n\t\t\t      struct ib_udata *udata)\n{\n\tstruct rvt_mr *mr;\n\tstruct ib_umem *umem;\n\tstruct sg_page_iter sg_iter;\n\tint n, m;\n\tstruct ib_mr *ret;\n\n\tif (length == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tumem = ib_umem_get(pd->device, start, length, mr_access_flags);\n\tif (IS_ERR(umem))\n\t\treturn (void *)umem;\n\n\tn = ib_umem_num_pages(umem);\n\n\tmr = __rvt_alloc_mr(n, pd);\n\tif (IS_ERR(mr)) {\n\t\tret = (struct ib_mr *)mr;\n\t\tgoto bail_umem;\n\t}\n\n\tmr->mr.user_base = start;\n\tmr->mr.iova = virt_addr;\n\tmr->mr.length = length;\n\tmr->mr.offset = ib_umem_offset(umem);\n\tmr->mr.access_flags = mr_access_flags;\n\tmr->umem = umem;\n\n\tmr->mr.page_shift = PAGE_SHIFT;\n\tm = 0;\n\tn = 0;\n\tfor_each_sgtable_page (&umem->sgt_append.sgt, &sg_iter, 0) {\n\t\tvoid *vaddr;\n\n\t\tvaddr = page_address(sg_page_iter_page(&sg_iter));\n\t\tif (!vaddr) {\n\t\t\tret = ERR_PTR(-EINVAL);\n\t\t\tgoto bail_inval;\n\t\t}\n\t\tmr->mr.map[m]->segs[n].vaddr = vaddr;\n\t\tmr->mr.map[m]->segs[n].length = PAGE_SIZE;\n\t\ttrace_rvt_mr_user_seg(&mr->mr, m, n, vaddr, PAGE_SIZE);\n\t\tif (++n == RVT_SEGSZ) {\n\t\t\tm++;\n\t\t\tn = 0;\n\t\t}\n\t}\n\treturn &mr->ibmr;\n\nbail_inval:\n\t__rvt_free_mr(mr);\n\nbail_umem:\n\tib_umem_release(umem);\n\n\treturn ret;\n}\n\n \nstatic void rvt_dereg_clean_qp_cb(struct rvt_qp *qp, u64 v)\n{\n\tstruct rvt_mregion *mr = (struct rvt_mregion *)v;\n\n\t \n\tif (mr->pd != qp->ibqp.pd)\n\t\treturn;\n\trvt_qp_mr_clean(qp, mr->lkey);\n}\n\n \nstatic void rvt_dereg_clean_qps(struct rvt_mregion *mr)\n{\n\tstruct rvt_dev_info *rdi = ib_to_rvt(mr->pd->device);\n\n\trvt_qp_iter(rdi, (u64)mr, rvt_dereg_clean_qp_cb);\n}\n\n \nstatic int rvt_check_refs(struct rvt_mregion *mr, const char *t)\n{\n\tunsigned long timeout;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(mr->pd->device);\n\n\tif (mr->lkey) {\n\t\t \n\t\trvt_dereg_clean_qps(mr);\n\t\t \n\t\tsynchronize_rcu();\n\t}\n\n\ttimeout = wait_for_completion_timeout(&mr->comp, 5 * HZ);\n\tif (!timeout) {\n\t\trvt_pr_err(rdi,\n\t\t\t   \"%s timeout mr %p pd %p lkey %x refcount %ld\\n\",\n\t\t\t   t, mr, mr->pd, mr->lkey,\n\t\t\t   atomic_long_read(&mr->refcount.data->count));\n\t\trvt_get_mr(mr);\n\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\n \nbool rvt_mr_has_lkey(struct rvt_mregion *mr, u32 lkey)\n{\n\treturn mr && lkey == mr->lkey;\n}\n\n \nbool rvt_ss_has_lkey(struct rvt_sge_state *ss, u32 lkey)\n{\n\tint i;\n\tbool rval = false;\n\n\tif (!ss->num_sge)\n\t\treturn rval;\n\t \n\trval = rvt_mr_has_lkey(ss->sge.mr, lkey);\n\t \n\tfor (i = 0; !rval && i < ss->num_sge - 1; i++)\n\t\trval = rvt_mr_has_lkey(ss->sg_list[i].mr, lkey);\n\treturn rval;\n}\n\n \nint rvt_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct rvt_mr *mr = to_imr(ibmr);\n\tint ret;\n\n\trvt_free_lkey(&mr->mr);\n\n\trvt_put_mr(&mr->mr);  \n\tret = rvt_check_refs(&mr->mr, __func__);\n\tif (ret)\n\t\tgoto out;\n\trvt_deinit_mregion(&mr->mr);\n\tib_umem_release(mr->umem);\n\tkfree(mr);\nout:\n\treturn ret;\n}\n\n \nstruct ib_mr *rvt_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t   u32 max_num_sg)\n{\n\tstruct rvt_mr *mr;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmr = __rvt_alloc_mr(max_num_sg, pd);\n\tif (IS_ERR(mr))\n\t\treturn (struct ib_mr *)mr;\n\n\treturn &mr->ibmr;\n}\n\n \nstatic int rvt_set_page(struct ib_mr *ibmr, u64 addr)\n{\n\tstruct rvt_mr *mr = to_imr(ibmr);\n\tu32 ps = 1 << mr->mr.page_shift;\n\tu32 mapped_segs = mr->mr.length >> mr->mr.page_shift;\n\tint m, n;\n\n\tif (unlikely(mapped_segs == mr->mr.max_segs))\n\t\treturn -ENOMEM;\n\n\tm = mapped_segs / RVT_SEGSZ;\n\tn = mapped_segs % RVT_SEGSZ;\n\tmr->mr.map[m]->segs[n].vaddr = (void *)addr;\n\tmr->mr.map[m]->segs[n].length = ps;\n\tmr->mr.length += ps;\n\ttrace_rvt_mr_page_seg(&mr->mr, m, n, (void *)addr, ps);\n\n\treturn 0;\n}\n\n \nint rvt_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,\n\t\t  int sg_nents, unsigned int *sg_offset)\n{\n\tstruct rvt_mr *mr = to_imr(ibmr);\n\tint ret;\n\n\tmr->mr.length = 0;\n\tmr->mr.page_shift = PAGE_SHIFT;\n\tret = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, rvt_set_page);\n\tmr->mr.user_base = ibmr->iova;\n\tmr->mr.iova = ibmr->iova;\n\tmr->mr.offset = ibmr->iova - (u64)mr->mr.map[0]->segs[0].vaddr;\n\tmr->mr.length = (size_t)ibmr->length;\n\ttrace_rvt_map_mr_sg(ibmr, sg_nents, sg_offset);\n\treturn ret;\n}\n\n \nint rvt_fast_reg_mr(struct rvt_qp *qp, struct ib_mr *ibmr, u32 key,\n\t\t    int access)\n{\n\tstruct rvt_mr *mr = to_imr(ibmr);\n\n\tif (qp->ibqp.pd != mr->mr.pd)\n\t\treturn -EACCES;\n\n\t \n\tif (!mr->mr.lkey || mr->umem)\n\t\treturn -EINVAL;\n\n\tif ((key & 0xFFFFFF00) != (mr->mr.lkey & 0xFFFFFF00))\n\t\treturn -EINVAL;\n\n\tibmr->lkey = key;\n\tibmr->rkey = key;\n\tmr->mr.lkey = key;\n\tmr->mr.access_flags = access;\n\tmr->mr.iova = ibmr->iova;\n\tatomic_set(&mr->mr.lkey_invalid, 0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(rvt_fast_reg_mr);\n\n \nint rvt_invalidate_rkey(struct rvt_qp *qp, u32 rkey)\n{\n\tstruct rvt_dev_info *dev = ib_to_rvt(qp->ibqp.device);\n\tstruct rvt_lkey_table *rkt = &dev->lkey_table;\n\tstruct rvt_mregion *mr;\n\n\tif (rkey == 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tmr = rcu_dereference(\n\t\trkt->table[(rkey >> (32 - dev->dparms.lkey_table_size))]);\n\tif (unlikely(!mr || mr->lkey != rkey || qp->ibqp.pd != mr->pd))\n\t\tgoto bail;\n\n\tatomic_set(&mr->lkey_invalid, 1);\n\trcu_read_unlock();\n\treturn 0;\n\nbail:\n\trcu_read_unlock();\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(rvt_invalidate_rkey);\n\n \nstatic inline bool rvt_sge_adjacent(struct rvt_sge *last_sge,\n\t\t\t\t    struct ib_sge *sge)\n{\n\tif (last_sge && sge->lkey == last_sge->mr->lkey &&\n\t    ((uint64_t)(last_sge->vaddr + last_sge->length) == sge->addr)) {\n\t\tif (sge->lkey) {\n\t\t\tif (unlikely((sge->addr - last_sge->mr->user_base +\n\t\t\t      sge->length > last_sge->mr->length)))\n\t\t\t\treturn false;  \n\t\t} else {\n\t\t\tlast_sge->length += sge->length;\n\t\t}\n\t\tlast_sge->sge_length += sge->length;\n\t\ttrace_rvt_sge_adjacent(last_sge, sge);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nint rvt_lkey_ok(struct rvt_lkey_table *rkt, struct rvt_pd *pd,\n\t\tstruct rvt_sge *isge, struct rvt_sge *last_sge,\n\t\tstruct ib_sge *sge, int acc)\n{\n\tstruct rvt_mregion *mr;\n\tunsigned n, m;\n\tsize_t off;\n\n\t \n\tif (sge->lkey == 0) {\n\t\tstruct rvt_dev_info *dev = ib_to_rvt(pd->ibpd.device);\n\n\t\tif (pd->user)\n\t\t\treturn -EINVAL;\n\t\tif (rvt_sge_adjacent(last_sge, sge))\n\t\t\treturn 0;\n\t\trcu_read_lock();\n\t\tmr = rcu_dereference(dev->dma_mr);\n\t\tif (!mr)\n\t\t\tgoto bail;\n\t\trvt_get_mr(mr);\n\t\trcu_read_unlock();\n\n\t\tisge->mr = mr;\n\t\tisge->vaddr = (void *)sge->addr;\n\t\tisge->length = sge->length;\n\t\tisge->sge_length = sge->length;\n\t\tisge->m = 0;\n\t\tisge->n = 0;\n\t\tgoto ok;\n\t}\n\tif (rvt_sge_adjacent(last_sge, sge))\n\t\treturn 0;\n\trcu_read_lock();\n\tmr = rcu_dereference(rkt->table[sge->lkey >> rkt->shift]);\n\tif (!mr)\n\t\tgoto bail;\n\trvt_get_mr(mr);\n\tif (!READ_ONCE(mr->lkey_published))\n\t\tgoto bail_unref;\n\n\tif (unlikely(atomic_read(&mr->lkey_invalid) ||\n\t\t     mr->lkey != sge->lkey || mr->pd != &pd->ibpd))\n\t\tgoto bail_unref;\n\n\toff = sge->addr - mr->user_base;\n\tif (unlikely(sge->addr < mr->user_base ||\n\t\t     off + sge->length > mr->length ||\n\t\t     (mr->access_flags & acc) != acc))\n\t\tgoto bail_unref;\n\trcu_read_unlock();\n\n\toff += mr->offset;\n\tif (mr->page_shift) {\n\t\t \n\t\tsize_t entries_spanned_by_off;\n\n\t\tentries_spanned_by_off = off >> mr->page_shift;\n\t\toff -= (entries_spanned_by_off << mr->page_shift);\n\t\tm = entries_spanned_by_off / RVT_SEGSZ;\n\t\tn = entries_spanned_by_off % RVT_SEGSZ;\n\t} else {\n\t\tm = 0;\n\t\tn = 0;\n\t\twhile (off >= mr->map[m]->segs[n].length) {\n\t\t\toff -= mr->map[m]->segs[n].length;\n\t\t\tn++;\n\t\t\tif (n >= RVT_SEGSZ) {\n\t\t\t\tm++;\n\t\t\t\tn = 0;\n\t\t\t}\n\t\t}\n\t}\n\tisge->mr = mr;\n\tisge->vaddr = mr->map[m]->segs[n].vaddr + off;\n\tisge->length = mr->map[m]->segs[n].length - off;\n\tisge->sge_length = sge->length;\n\tisge->m = m;\n\tisge->n = n;\nok:\n\ttrace_rvt_sge_new(isge, sge);\n\treturn 1;\nbail_unref:\n\trvt_put_mr(mr);\nbail:\n\trcu_read_unlock();\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(rvt_lkey_ok);\n\n \nint rvt_rkey_ok(struct rvt_qp *qp, struct rvt_sge *sge,\n\t\tu32 len, u64 vaddr, u32 rkey, int acc)\n{\n\tstruct rvt_dev_info *dev = ib_to_rvt(qp->ibqp.device);\n\tstruct rvt_lkey_table *rkt = &dev->lkey_table;\n\tstruct rvt_mregion *mr;\n\tunsigned n, m;\n\tsize_t off;\n\n\t \n\trcu_read_lock();\n\tif (rkey == 0) {\n\t\tstruct rvt_pd *pd = ibpd_to_rvtpd(qp->ibqp.pd);\n\t\tstruct rvt_dev_info *rdi = ib_to_rvt(pd->ibpd.device);\n\n\t\tif (pd->user)\n\t\t\tgoto bail;\n\t\tmr = rcu_dereference(rdi->dma_mr);\n\t\tif (!mr)\n\t\t\tgoto bail;\n\t\trvt_get_mr(mr);\n\t\trcu_read_unlock();\n\n\t\tsge->mr = mr;\n\t\tsge->vaddr = (void *)vaddr;\n\t\tsge->length = len;\n\t\tsge->sge_length = len;\n\t\tsge->m = 0;\n\t\tsge->n = 0;\n\t\tgoto ok;\n\t}\n\n\tmr = rcu_dereference(rkt->table[rkey >> rkt->shift]);\n\tif (!mr)\n\t\tgoto bail;\n\trvt_get_mr(mr);\n\t \n\tif (!READ_ONCE(mr->lkey_published))\n\t\tgoto bail_unref;\n\tif (unlikely(atomic_read(&mr->lkey_invalid) ||\n\t\t     mr->lkey != rkey || qp->ibqp.pd != mr->pd))\n\t\tgoto bail_unref;\n\n\toff = vaddr - mr->iova;\n\tif (unlikely(vaddr < mr->iova || off + len > mr->length ||\n\t\t     (mr->access_flags & acc) == 0))\n\t\tgoto bail_unref;\n\trcu_read_unlock();\n\n\toff += mr->offset;\n\tif (mr->page_shift) {\n\t\t \n\t\tsize_t entries_spanned_by_off;\n\n\t\tentries_spanned_by_off = off >> mr->page_shift;\n\t\toff -= (entries_spanned_by_off << mr->page_shift);\n\t\tm = entries_spanned_by_off / RVT_SEGSZ;\n\t\tn = entries_spanned_by_off % RVT_SEGSZ;\n\t} else {\n\t\tm = 0;\n\t\tn = 0;\n\t\twhile (off >= mr->map[m]->segs[n].length) {\n\t\t\toff -= mr->map[m]->segs[n].length;\n\t\t\tn++;\n\t\t\tif (n >= RVT_SEGSZ) {\n\t\t\t\tm++;\n\t\t\t\tn = 0;\n\t\t\t}\n\t\t}\n\t}\n\tsge->mr = mr;\n\tsge->vaddr = mr->map[m]->segs[n].vaddr + off;\n\tsge->length = mr->map[m]->segs[n].length - off;\n\tsge->sge_length = len;\n\tsge->m = m;\n\tsge->n = n;\nok:\n\treturn 1;\nbail_unref:\n\trvt_put_mr(mr);\nbail:\n\trcu_read_unlock();\n\treturn 0;\n}\nEXPORT_SYMBOL(rvt_rkey_ok);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}