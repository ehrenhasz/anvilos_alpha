{
  "module_name": "qp.c",
  "hash_id": "cc62cadc668f911a4b87087910793dc76d3c0df234dd227cab1d187d2ff5eb92",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rdmavt/qp.c",
  "human_readable_source": "\n \n\n#include <linux/hash.h>\n#include <linux/bitops.h>\n#include <linux/lockdep.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_hdrs.h>\n#include <rdma/opa_addr.h>\n#include <rdma/uverbs_ioctl.h>\n#include \"qp.h\"\n#include \"vt.h\"\n#include \"trace.h\"\n\n#define RVT_RWQ_COUNT_THRESHOLD 16\n\nstatic void rvt_rc_timeout(struct timer_list *t);\nstatic void rvt_reset_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t\t enum ib_qp_type type);\n\n \nstatic const u32 ib_rvt_rnr_table[32] = {\n\t655360,  \n\t10,      \n\t20,      \n\t30,      \n\t40,      \n\t60,      \n\t80,      \n\t120,     \n\t160,     \n\t240,     \n\t320,     \n\t480,     \n\t640,     \n\t960,     \n\t1280,    \n\t1920,    \n\t2560,    \n\t3840,    \n\t5120,    \n\t7680,    \n\t10240,   \n\t15360,   \n\t20480,   \n\t30720,   \n\t40960,   \n\t61440,   \n\t81920,   \n\t122880,  \n\t163840,  \n\t245760,  \n\t327680,  \n\t491520   \n};\n\n \nconst int ib_rvt_state_ops[IB_QPS_ERR + 1] = {\n\t[IB_QPS_RESET] = 0,\n\t[IB_QPS_INIT] = RVT_POST_RECV_OK,\n\t[IB_QPS_RTR] = RVT_POST_RECV_OK | RVT_PROCESS_RECV_OK,\n\t[IB_QPS_RTS] = RVT_POST_RECV_OK | RVT_PROCESS_RECV_OK |\n\t    RVT_POST_SEND_OK | RVT_PROCESS_SEND_OK |\n\t    RVT_PROCESS_NEXT_SEND_OK,\n\t[IB_QPS_SQD] = RVT_POST_RECV_OK | RVT_PROCESS_RECV_OK |\n\t    RVT_POST_SEND_OK | RVT_PROCESS_SEND_OK,\n\t[IB_QPS_SQE] = RVT_POST_RECV_OK | RVT_PROCESS_RECV_OK |\n\t    RVT_POST_SEND_OK | RVT_FLUSH_SEND,\n\t[IB_QPS_ERR] = RVT_POST_RECV_OK | RVT_FLUSH_RECV |\n\t    RVT_POST_SEND_OK | RVT_FLUSH_SEND,\n};\nEXPORT_SYMBOL(ib_rvt_state_ops);\n\n \nstatic int rvt_wss_llc_size(void)\n{\n\t \n\treturn boot_cpu_data.x86_cache_size;\n}\n\n \nstatic void cacheless_memcpy(void *dst, void *src, size_t n)\n{\n\t \n\t__copy_user_nocache(dst, (void __user *)src, n);\n}\n\nvoid rvt_wss_exit(struct rvt_dev_info *rdi)\n{\n\tstruct rvt_wss *wss = rdi->wss;\n\n\tif (!wss)\n\t\treturn;\n\n\t \n\tkfree(wss->entries);\n\twss->entries = NULL;\n\tkfree(rdi->wss);\n\trdi->wss = NULL;\n}\n\n \nint rvt_wss_init(struct rvt_dev_info *rdi)\n{\n\tunsigned int sge_copy_mode = rdi->dparms.sge_copy_mode;\n\tunsigned int wss_threshold = rdi->dparms.wss_threshold;\n\tunsigned int wss_clean_period = rdi->dparms.wss_clean_period;\n\tlong llc_size;\n\tlong llc_bits;\n\tlong table_size;\n\tlong table_bits;\n\tstruct rvt_wss *wss;\n\tint node = rdi->dparms.node;\n\n\tif (sge_copy_mode != RVT_SGE_COPY_ADAPTIVE) {\n\t\trdi->wss = NULL;\n\t\treturn 0;\n\t}\n\n\trdi->wss = kzalloc_node(sizeof(*rdi->wss), GFP_KERNEL, node);\n\tif (!rdi->wss)\n\t\treturn -ENOMEM;\n\twss = rdi->wss;\n\n\t \n\tif (wss_threshold < 1 || wss_threshold > 100)\n\t\twss_threshold = 80;\n\n\t \n\tif (wss_clean_period > 1000000)\n\t\twss_clean_period = 256;\n\n\t \n\tif (wss_clean_period == 0)\n\t\twss_clean_period = 1;\n\n\t \n\tllc_size = rvt_wss_llc_size() * 1024;\n\ttable_size = roundup_pow_of_two(llc_size);\n\n\t \n\tllc_bits = llc_size / PAGE_SIZE;\n\ttable_bits = table_size / PAGE_SIZE;\n\twss->pages_mask = table_bits - 1;\n\twss->num_entries = table_bits / BITS_PER_LONG;\n\n\twss->threshold = (llc_bits * wss_threshold) / 100;\n\tif (wss->threshold == 0)\n\t\twss->threshold = 1;\n\n\twss->clean_period = wss_clean_period;\n\tatomic_set(&wss->clean_counter, wss_clean_period);\n\n\twss->entries = kcalloc_node(wss->num_entries, sizeof(*wss->entries),\n\t\t\t\t    GFP_KERNEL, node);\n\tif (!wss->entries) {\n\t\trvt_wss_exit(rdi);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void wss_advance_clean_counter(struct rvt_wss *wss)\n{\n\tint entry;\n\tint weight;\n\tunsigned long bits;\n\n\t \n\tif (atomic_dec_and_test(&wss->clean_counter)) {\n\t\t \n\t\tatomic_set(&wss->clean_counter, wss->clean_period);\n\n\t\t \n\t\tentry = (atomic_inc_return(&wss->clean_entry) - 1)\n\t\t\t& (wss->num_entries - 1);\n\n\t\t \n\t\tbits = xchg(&wss->entries[entry], 0);\n\t\tweight = hweight64((u64)bits);\n\t\t \n\t\tif (weight)\n\t\t\tatomic_sub(weight, &wss->total_count);\n\t}\n}\n\n \nstatic void wss_insert(struct rvt_wss *wss, void *address)\n{\n\tu32 page = ((unsigned long)address >> PAGE_SHIFT) & wss->pages_mask;\n\tu32 entry = page / BITS_PER_LONG;  \n\tu32 nr = page & (BITS_PER_LONG - 1);\n\n\tif (!test_and_set_bit(nr, &wss->entries[entry]))\n\t\tatomic_inc(&wss->total_count);\n\n\twss_advance_clean_counter(wss);\n}\n\n \nstatic inline bool wss_exceeds_threshold(struct rvt_wss *wss)\n{\n\treturn atomic_read(&wss->total_count) >= wss->threshold;\n}\n\nstatic void get_map_page(struct rvt_qpn_table *qpt,\n\t\t\t struct rvt_qpn_map *map)\n{\n\tunsigned long page = get_zeroed_page(GFP_KERNEL);\n\n\t \n\n\tspin_lock(&qpt->lock);\n\tif (map->page)\n\t\tfree_page(page);\n\telse\n\t\tmap->page = (void *)page;\n\tspin_unlock(&qpt->lock);\n}\n\n \nstatic int init_qpn_table(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt)\n{\n\tu32 offset, i;\n\tstruct rvt_qpn_map *map;\n\tint ret = 0;\n\n\tif (!(rdi->dparms.qpn_res_end >= rdi->dparms.qpn_res_start))\n\t\treturn -EINVAL;\n\n\tspin_lock_init(&qpt->lock);\n\n\tqpt->last = rdi->dparms.qpn_start;\n\tqpt->incr = rdi->dparms.qpn_inc << rdi->dparms.qos_shift;\n\n\t \n\n\t \n\tqpt->nmaps = rdi->dparms.qpn_res_start / RVT_BITS_PER_PAGE;\n\n\t \n\toffset = rdi->dparms.qpn_res_start & RVT_BITS_PER_PAGE_MASK;\n\n\t \n\tmap = &qpt->map[qpt->nmaps];\n\n\trvt_pr_info(rdi, \"Reserving QPNs from 0x%x to 0x%x for non-verbs use\\n\",\n\t\t    rdi->dparms.qpn_res_start, rdi->dparms.qpn_res_end);\n\tfor (i = rdi->dparms.qpn_res_start; i <= rdi->dparms.qpn_res_end; i++) {\n\t\tif (!map->page) {\n\t\t\tget_map_page(qpt, map);\n\t\t\tif (!map->page) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tset_bit(offset, map->page);\n\t\toffset++;\n\t\tif (offset == RVT_BITS_PER_PAGE) {\n\t\t\t \n\t\t\tqpt->nmaps++;\n\t\t\tmap++;\n\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nstatic void free_qpn_table(struct rvt_qpn_table *qpt)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(qpt->map); i++)\n\t\tfree_page((unsigned long)qpt->map[i].page);\n}\n\n \nint rvt_driver_qp_init(struct rvt_dev_info *rdi)\n{\n\tint i;\n\tint ret = -ENOMEM;\n\n\tif (!rdi->dparms.qp_table_size)\n\t\treturn -EINVAL;\n\n\t \n\tif (!rdi->driver_f.free_all_qps ||\n\t    !rdi->driver_f.qp_priv_alloc ||\n\t    !rdi->driver_f.qp_priv_free ||\n\t    !rdi->driver_f.notify_qp_reset ||\n\t    !rdi->driver_f.notify_restart_rc)\n\t\treturn -EINVAL;\n\n\t \n\trdi->qp_dev = kzalloc_node(sizeof(*rdi->qp_dev), GFP_KERNEL,\n\t\t\t\t   rdi->dparms.node);\n\tif (!rdi->qp_dev)\n\t\treturn -ENOMEM;\n\n\t \n\trdi->qp_dev->qp_table_size = rdi->dparms.qp_table_size;\n\trdi->qp_dev->qp_table_bits = ilog2(rdi->dparms.qp_table_size);\n\trdi->qp_dev->qp_table =\n\t\tkmalloc_array_node(rdi->qp_dev->qp_table_size,\n\t\t\t     sizeof(*rdi->qp_dev->qp_table),\n\t\t\t     GFP_KERNEL, rdi->dparms.node);\n\tif (!rdi->qp_dev->qp_table)\n\t\tgoto no_qp_table;\n\n\tfor (i = 0; i < rdi->qp_dev->qp_table_size; i++)\n\t\tRCU_INIT_POINTER(rdi->qp_dev->qp_table[i], NULL);\n\n\tspin_lock_init(&rdi->qp_dev->qpt_lock);\n\n\t \n\tif (init_qpn_table(rdi, &rdi->qp_dev->qpn_table))\n\t\tgoto fail_table;\n\n\tspin_lock_init(&rdi->n_qps_lock);\n\n\treturn 0;\n\nfail_table:\n\tkfree(rdi->qp_dev->qp_table);\n\tfree_qpn_table(&rdi->qp_dev->qpn_table);\n\nno_qp_table:\n\tkfree(rdi->qp_dev);\n\n\treturn ret;\n}\n\n \nstatic void rvt_free_qp_cb(struct rvt_qp *qp, u64 v)\n{\n\tunsigned int *qp_inuse = (unsigned int *)v;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\t \n\trvt_reset_qp(rdi, qp, qp->ibqp.qp_type);\n\n\t \n\t(*qp_inuse)++;\n}\n\n \nstatic unsigned rvt_free_all_qps(struct rvt_dev_info *rdi)\n{\n\tunsigned int qp_inuse = 0;\n\n\tqp_inuse += rvt_mcast_tree_empty(rdi);\n\n\trvt_qp_iter(rdi, (u64)&qp_inuse, rvt_free_qp_cb);\n\n\treturn qp_inuse;\n}\n\n \nvoid rvt_qp_exit(struct rvt_dev_info *rdi)\n{\n\tu32 qps_inuse = rvt_free_all_qps(rdi);\n\n\tif (qps_inuse)\n\t\trvt_pr_err(rdi, \"QP memory leak! %u still in use\\n\",\n\t\t\t   qps_inuse);\n\n\tkfree(rdi->qp_dev->qp_table);\n\tfree_qpn_table(&rdi->qp_dev->qpn_table);\n\tkfree(rdi->qp_dev);\n}\n\nstatic inline unsigned mk_qpn(struct rvt_qpn_table *qpt,\n\t\t\t      struct rvt_qpn_map *map, unsigned off)\n{\n\treturn (map - qpt->map) * RVT_BITS_PER_PAGE + off;\n}\n\n \nstatic int alloc_qpn(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt,\n\t\t     enum ib_qp_type type, u8 port_num, u8 exclude_prefix)\n{\n\tu32 i, offset, max_scan, qpn;\n\tstruct rvt_qpn_map *map;\n\tu32 ret;\n\tu32 max_qpn = exclude_prefix == RVT_AIP_QP_PREFIX ?\n\t\tRVT_AIP_QPN_MAX : RVT_QPN_MAX;\n\n\tif (rdi->driver_f.alloc_qpn)\n\t\treturn rdi->driver_f.alloc_qpn(rdi, qpt, type, port_num);\n\n\tif (type == IB_QPT_SMI || type == IB_QPT_GSI) {\n\t\tunsigned n;\n\n\t\tret = type == IB_QPT_GSI;\n\t\tn = 1 << (ret + 2 * (port_num - 1));\n\t\tspin_lock(&qpt->lock);\n\t\tif (qpt->flags & n)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tqpt->flags |= n;\n\t\tspin_unlock(&qpt->lock);\n\t\tgoto bail;\n\t}\n\n\tqpn = qpt->last + qpt->incr;\n\tif (qpn >= max_qpn)\n\t\tqpn = qpt->incr | ((qpt->last & 1) ^ 1);\n\t \n\toffset = qpn & RVT_BITS_PER_PAGE_MASK;\n\tmap = &qpt->map[qpn / RVT_BITS_PER_PAGE];\n\tmax_scan = qpt->nmaps - !offset;\n\tfor (i = 0;;) {\n\t\tif (unlikely(!map->page)) {\n\t\t\tget_map_page(qpt, map);\n\t\t\tif (unlikely(!map->page))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tif (!test_and_set_bit(offset, map->page)) {\n\t\t\t\tqpt->last = qpn;\n\t\t\t\tret = qpn;\n\t\t\t\tgoto bail;\n\t\t\t}\n\t\t\toffset += qpt->incr;\n\t\t\t \n\t\t\tqpn = mk_qpn(qpt, map, offset);\n\t\t} while (offset < RVT_BITS_PER_PAGE && qpn < RVT_QPN_MAX);\n\t\t \n\t\tif (++i > max_scan) {\n\t\t\tif (qpt->nmaps == RVT_QPNMAP_ENTRIES)\n\t\t\t\tbreak;\n\t\t\tmap = &qpt->map[qpt->nmaps++];\n\t\t\t \n\t\t\toffset = qpt->incr | (offset & 1);\n\t\t} else if (map < &qpt->map[qpt->nmaps]) {\n\t\t\t++map;\n\t\t\t \n\t\t\toffset = qpt->incr | (offset & 1);\n\t\t} else {\n\t\t\tmap = &qpt->map[0];\n\t\t\t \n\t\t\toffset = qpt->incr | ((offset & 1) ^ 1);\n\t\t}\n\t\t \n\t\tWARN_ON(rdi->dparms.qos_shift > 1 &&\n\t\t\toffset & ((BIT(rdi->dparms.qos_shift - 1) - 1) << 1));\n\t\tqpn = mk_qpn(qpt, map, offset);\n\t}\n\n\tret = -ENOMEM;\n\nbail:\n\treturn ret;\n}\n\n \nstatic void rvt_clear_mr_refs(struct rvt_qp *qp, int clr_sends)\n{\n\tunsigned n;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\tif (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))\n\t\trvt_put_ss(&qp->s_rdma_read_sge);\n\n\trvt_put_ss(&qp->r_sge);\n\n\tif (clr_sends) {\n\t\twhile (qp->s_last != qp->s_head) {\n\t\t\tstruct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, qp->s_last);\n\n\t\t\trvt_put_qp_swqe(qp, wqe);\n\t\t\tif (++qp->s_last >= qp->s_size)\n\t\t\t\tqp->s_last = 0;\n\t\t\tsmp_wmb();  \n\t\t}\n\t\tif (qp->s_rdma_mr) {\n\t\t\trvt_put_mr(qp->s_rdma_mr);\n\t\t\tqp->s_rdma_mr = NULL;\n\t\t}\n\t}\n\n\tfor (n = 0; qp->s_ack_queue && n < rvt_max_atomic(rdi); n++) {\n\t\tstruct rvt_ack_entry *e = &qp->s_ack_queue[n];\n\n\t\tif (e->rdma_sge.mr) {\n\t\t\trvt_put_mr(e->rdma_sge.mr);\n\t\t\te->rdma_sge.mr = NULL;\n\t\t}\n\t}\n}\n\n \nstatic bool rvt_swqe_has_lkey(struct rvt_swqe *wqe, u32 lkey)\n{\n\tint i;\n\n\tfor (i = 0; i < wqe->wr.num_sge; i++) {\n\t\tstruct rvt_sge *sge = &wqe->sg_list[i];\n\n\t\tif (rvt_mr_has_lkey(sge->mr, lkey))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool rvt_qp_sends_has_lkey(struct rvt_qp *qp, u32 lkey)\n{\n\tu32 s_last = qp->s_last;\n\n\twhile (s_last != qp->s_head) {\n\t\tstruct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, s_last);\n\n\t\tif (rvt_swqe_has_lkey(wqe, lkey))\n\t\t\treturn true;\n\n\t\tif (++s_last >= qp->s_size)\n\t\t\ts_last = 0;\n\t}\n\tif (qp->s_rdma_mr)\n\t\tif (rvt_mr_has_lkey(qp->s_rdma_mr, lkey))\n\t\t\treturn true;\n\treturn false;\n}\n\n \nstatic bool rvt_qp_acks_has_lkey(struct rvt_qp *qp, u32 lkey)\n{\n\tint i;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\tfor (i = 0; qp->s_ack_queue && i < rvt_max_atomic(rdi); i++) {\n\t\tstruct rvt_ack_entry *e = &qp->s_ack_queue[i];\n\n\t\tif (rvt_mr_has_lkey(e->rdma_sge.mr, lkey))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nvoid rvt_qp_mr_clean(struct rvt_qp *qp, u32 lkey)\n{\n\tbool lastwqe = false;\n\n\tif (qp->ibqp.qp_type == IB_QPT_SMI ||\n\t    qp->ibqp.qp_type == IB_QPT_GSI)\n\t\t \n\t\treturn;\n\tspin_lock_irq(&qp->r_lock);\n\tspin_lock(&qp->s_hlock);\n\tspin_lock(&qp->s_lock);\n\n\tif (qp->state == IB_QPS_ERR || qp->state == IB_QPS_RESET)\n\t\tgoto check_lwqe;\n\n\tif (rvt_ss_has_lkey(&qp->r_sge, lkey) ||\n\t    rvt_qp_sends_has_lkey(qp, lkey) ||\n\t    rvt_qp_acks_has_lkey(qp, lkey))\n\t\tlastwqe = rvt_error_qp(qp, IB_WC_LOC_PROT_ERR);\ncheck_lwqe:\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->s_hlock);\n\tspin_unlock_irq(&qp->r_lock);\n\tif (lastwqe) {\n\t\tstruct ib_event ev;\n\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n}\n\n \nstatic void rvt_remove_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct rvt_ibport *rvp = rdi->ports[qp->port_num - 1];\n\tu32 n = hash_32(qp->ibqp.qp_num, rdi->qp_dev->qp_table_bits);\n\tunsigned long flags;\n\tint removed = 1;\n\n\tspin_lock_irqsave(&rdi->qp_dev->qpt_lock, flags);\n\n\tif (rcu_dereference_protected(rvp->qp[0],\n\t\t\tlockdep_is_held(&rdi->qp_dev->qpt_lock)) == qp) {\n\t\tRCU_INIT_POINTER(rvp->qp[0], NULL);\n\t} else if (rcu_dereference_protected(rvp->qp[1],\n\t\t\tlockdep_is_held(&rdi->qp_dev->qpt_lock)) == qp) {\n\t\tRCU_INIT_POINTER(rvp->qp[1], NULL);\n\t} else {\n\t\tstruct rvt_qp *q;\n\t\tstruct rvt_qp __rcu **qpp;\n\n\t\tremoved = 0;\n\t\tqpp = &rdi->qp_dev->qp_table[n];\n\t\tfor (; (q = rcu_dereference_protected(*qpp,\n\t\t\tlockdep_is_held(&rdi->qp_dev->qpt_lock))) != NULL;\n\t\t\tqpp = &q->next) {\n\t\t\tif (q == qp) {\n\t\t\t\tRCU_INIT_POINTER(*qpp,\n\t\t\t\t     rcu_dereference_protected(qp->next,\n\t\t\t\t     lockdep_is_held(&rdi->qp_dev->qpt_lock)));\n\t\t\t\tremoved = 1;\n\t\t\t\ttrace_rvt_qpremove(qp, n);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&rdi->qp_dev->qpt_lock, flags);\n\tif (removed) {\n\t\tsynchronize_rcu();\n\t\trvt_put_qp(qp);\n\t}\n}\n\n \nint rvt_alloc_rq(struct rvt_rq *rq, u32 size, int node,\n\t\t struct ib_udata *udata)\n{\n\tif (udata) {\n\t\trq->wq = vmalloc_user(sizeof(struct rvt_rwq) + size);\n\t\tif (!rq->wq)\n\t\t\tgoto bail;\n\t\t \n\t\trq->kwq = kzalloc_node(sizeof(*rq->kwq), GFP_KERNEL, node);\n\t\tif (!rq->kwq)\n\t\t\tgoto bail;\n\t\trq->kwq->curr_wq = rq->wq->wq;\n\t} else {\n\t\t \n\t\trq->kwq =\n\t\t\tvzalloc_node(sizeof(struct rvt_krwq) + size, node);\n\t\tif (!rq->kwq)\n\t\t\tgoto bail;\n\t\trq->kwq->curr_wq = rq->kwq->wq;\n\t}\n\n\tspin_lock_init(&rq->kwq->p_lock);\n\tspin_lock_init(&rq->kwq->c_lock);\n\treturn 0;\nbail:\n\trvt_free_rq(rq);\n\treturn -ENOMEM;\n}\n\n \nstatic void rvt_init_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t\tenum ib_qp_type type)\n{\n\tqp->remote_qpn = 0;\n\tqp->qkey = 0;\n\tqp->qp_access_flags = 0;\n\tqp->s_flags &= RVT_S_SIGNAL_REQ_WR;\n\tqp->s_hdrwords = 0;\n\tqp->s_wqe = NULL;\n\tqp->s_draining = 0;\n\tqp->s_next_psn = 0;\n\tqp->s_last_psn = 0;\n\tqp->s_sending_psn = 0;\n\tqp->s_sending_hpsn = 0;\n\tqp->s_psn = 0;\n\tqp->r_psn = 0;\n\tqp->r_msn = 0;\n\tif (type == IB_QPT_RC) {\n\t\tqp->s_state = IB_OPCODE_RC_SEND_LAST;\n\t\tqp->r_state = IB_OPCODE_RC_SEND_LAST;\n\t} else {\n\t\tqp->s_state = IB_OPCODE_UC_SEND_LAST;\n\t\tqp->r_state = IB_OPCODE_UC_SEND_LAST;\n\t}\n\tqp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;\n\tqp->r_nak_state = 0;\n\tqp->r_aflags = 0;\n\tqp->r_flags = 0;\n\tqp->s_head = 0;\n\tqp->s_tail = 0;\n\tqp->s_cur = 0;\n\tqp->s_acked = 0;\n\tqp->s_last = 0;\n\tqp->s_ssn = 1;\n\tqp->s_lsn = 0;\n\tqp->s_mig_state = IB_MIG_MIGRATED;\n\tqp->r_head_ack_queue = 0;\n\tqp->s_tail_ack_queue = 0;\n\tqp->s_acked_ack_queue = 0;\n\tqp->s_num_rd_atomic = 0;\n\tqp->r_sge.num_sge = 0;\n\tatomic_set(&qp->s_reserved_used, 0);\n}\n\n \nstatic void _rvt_reset_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t\t  enum ib_qp_type type)\n\t__must_hold(&qp->s_lock)\n\t__must_hold(&qp->s_hlock)\n\t__must_hold(&qp->r_lock)\n{\n\tlockdep_assert_held(&qp->r_lock);\n\tlockdep_assert_held(&qp->s_hlock);\n\tlockdep_assert_held(&qp->s_lock);\n\tif (qp->state != IB_QPS_RESET) {\n\t\tqp->state = IB_QPS_RESET;\n\n\t\t \n\t\trdi->driver_f.flush_qp_waiters(qp);\n\t\trvt_stop_rc_timers(qp);\n\t\tqp->s_flags &= ~(RVT_S_TIMER | RVT_S_ANY_WAIT);\n\t\tspin_unlock(&qp->s_lock);\n\t\tspin_unlock(&qp->s_hlock);\n\t\tspin_unlock_irq(&qp->r_lock);\n\n\t\t \n\t\trdi->driver_f.stop_send_queue(qp);\n\t\trvt_del_timers_sync(qp);\n\t\t \n\t\trdi->driver_f.quiesce_qp(qp);\n\n\t\t \n\t\trvt_remove_qp(rdi, qp);\n\n\t\t \n\t\tspin_lock_irq(&qp->r_lock);\n\t\tspin_lock(&qp->s_hlock);\n\t\tspin_lock(&qp->s_lock);\n\n\t\trvt_clear_mr_refs(qp, 1);\n\t\t \n\t\trdi->driver_f.notify_qp_reset(qp);\n\t}\n\trvt_init_qp(rdi, qp, type);\n\tlockdep_assert_held(&qp->r_lock);\n\tlockdep_assert_held(&qp->s_hlock);\n\tlockdep_assert_held(&qp->s_lock);\n}\n\n \nstatic void rvt_reset_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp,\n\t\t\t enum ib_qp_type type)\n{\n\tspin_lock_irq(&qp->r_lock);\n\tspin_lock(&qp->s_hlock);\n\tspin_lock(&qp->s_lock);\n\t_rvt_reset_qp(rdi, qp, type);\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->s_hlock);\n\tspin_unlock_irq(&qp->r_lock);\n}\n\n \nstatic void rvt_free_qpn(struct rvt_qpn_table *qpt, u32 qpn)\n{\n\tstruct rvt_qpn_map *map;\n\n\tif ((qpn & RVT_AIP_QP_PREFIX_MASK) == RVT_AIP_QP_BASE)\n\t\tqpn &= RVT_AIP_QP_SUFFIX;\n\n\tmap = qpt->map + (qpn & RVT_QPN_MASK) / RVT_BITS_PER_PAGE;\n\tif (map->page)\n\t\tclear_bit(qpn & RVT_BITS_PER_PAGE_MASK, map->page);\n}\n\n \nstatic u8 get_allowed_ops(enum ib_qp_type type)\n{\n\treturn type == IB_QPT_RC ? IB_OPCODE_RC : type == IB_QPT_UC ?\n\t\tIB_OPCODE_UC : IB_OPCODE_UD;\n}\n\n \nstatic void free_ud_wq_attr(struct rvt_qp *qp)\n{\n\tstruct rvt_swqe *wqe;\n\tint i;\n\n\tfor (i = 0; qp->allowed_ops == IB_OPCODE_UD && i < qp->s_size; i++) {\n\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\tkfree(wqe->ud_wr.attr);\n\t\twqe->ud_wr.attr = NULL;\n\t}\n}\n\n \nstatic int alloc_ud_wq_attr(struct rvt_qp *qp, int node)\n{\n\tstruct rvt_swqe *wqe;\n\tint i;\n\n\tfor (i = 0; qp->allowed_ops == IB_OPCODE_UD && i < qp->s_size; i++) {\n\t\twqe = rvt_get_swqe_ptr(qp, i);\n\t\twqe->ud_wr.attr = kzalloc_node(sizeof(*wqe->ud_wr.attr),\n\t\t\t\t\t       GFP_KERNEL, node);\n\t\tif (!wqe->ud_wr.attr) {\n\t\t\tfree_ud_wq_attr(qp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint rvt_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *init_attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tint ret = -ENOMEM;\n\tstruct rvt_swqe *swq = NULL;\n\tsize_t sz;\n\tsize_t sg_list_sz = 0;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\tvoid *priv = NULL;\n\tsize_t sqsize;\n\tu8 exclude_prefix = 0;\n\n\tif (!rdi)\n\t\treturn -EINVAL;\n\n\tif (init_attr->create_flags & ~IB_QP_CREATE_NETDEV_USE)\n\t\treturn -EOPNOTSUPP;\n\n\tif (init_attr->cap.max_send_sge > rdi->dparms.props.max_send_sge ||\n\t    init_attr->cap.max_send_wr > rdi->dparms.props.max_qp_wr)\n\t\treturn -EINVAL;\n\n\t \n\tif (!init_attr->srq) {\n\t\tif (init_attr->cap.max_recv_sge >\n\t\t    rdi->dparms.props.max_recv_sge ||\n\t\t    init_attr->cap.max_recv_wr > rdi->dparms.props.max_qp_wr)\n\t\t\treturn -EINVAL;\n\n\t\tif (init_attr->cap.max_send_sge +\n\t\t    init_attr->cap.max_send_wr +\n\t\t    init_attr->cap.max_recv_sge +\n\t\t    init_attr->cap.max_recv_wr == 0)\n\t\t\treturn -EINVAL;\n\t}\n\tsqsize =\n\t\tinit_attr->cap.max_send_wr + 1 +\n\t\trdi->dparms.reserved_operations;\n\tswitch (init_attr->qp_type) {\n\tcase IB_QPT_SMI:\n\tcase IB_QPT_GSI:\n\t\tif (init_attr->port_num == 0 ||\n\t\t    init_attr->port_num > ibqp->device->phys_port_cnt)\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\tcase IB_QPT_UC:\n\tcase IB_QPT_RC:\n\tcase IB_QPT_UD:\n\t\tsz = struct_size(swq, sg_list, init_attr->cap.max_send_sge);\n\t\tswq = vzalloc_node(array_size(sz, sqsize), rdi->dparms.node);\n\t\tif (!swq)\n\t\t\treturn -ENOMEM;\n\n\t\tif (init_attr->srq) {\n\t\t\tstruct rvt_srq *srq = ibsrq_to_rvtsrq(init_attr->srq);\n\n\t\t\tif (srq->rq.max_sge > 1)\n\t\t\t\tsg_list_sz = sizeof(*qp->r_sg_list) *\n\t\t\t\t\t(srq->rq.max_sge - 1);\n\t\t} else if (init_attr->cap.max_recv_sge > 1)\n\t\t\tsg_list_sz = sizeof(*qp->r_sg_list) *\n\t\t\t\t(init_attr->cap.max_recv_sge - 1);\n\t\tqp->r_sg_list =\n\t\t\tkzalloc_node(sg_list_sz, GFP_KERNEL, rdi->dparms.node);\n\t\tif (!qp->r_sg_list)\n\t\t\tgoto bail_qp;\n\t\tqp->allowed_ops = get_allowed_ops(init_attr->qp_type);\n\n\t\tRCU_INIT_POINTER(qp->next, NULL);\n\t\tif (init_attr->qp_type == IB_QPT_RC) {\n\t\t\tqp->s_ack_queue =\n\t\t\t\tkcalloc_node(rvt_max_atomic(rdi),\n\t\t\t\t\t     sizeof(*qp->s_ack_queue),\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     rdi->dparms.node);\n\t\t\tif (!qp->s_ack_queue)\n\t\t\t\tgoto bail_qp;\n\t\t}\n\t\t \n\t\ttimer_setup(&qp->s_timer, rvt_rc_timeout, 0);\n\t\thrtimer_init(&qp->s_rnr_timer, CLOCK_MONOTONIC,\n\t\t\t     HRTIMER_MODE_REL);\n\t\tqp->s_rnr_timer.function = rvt_rc_rnr_retry;\n\n\t\t \n\t\tpriv = rdi->driver_f.qp_priv_alloc(rdi, qp);\n\t\tif (IS_ERR(priv)) {\n\t\t\tret = PTR_ERR(priv);\n\t\t\tgoto bail_qp;\n\t\t}\n\t\tqp->priv = priv;\n\t\tqp->timeout_jiffies =\n\t\t\tusecs_to_jiffies((4096UL * (1UL << qp->timeout)) /\n\t\t\t\t1000UL);\n\t\tif (init_attr->srq) {\n\t\t\tsz = 0;\n\t\t} else {\n\t\t\tqp->r_rq.size = init_attr->cap.max_recv_wr + 1;\n\t\t\tqp->r_rq.max_sge = init_attr->cap.max_recv_sge;\n\t\t\tsz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +\n\t\t\t\tsizeof(struct rvt_rwqe);\n\t\t\tret = rvt_alloc_rq(&qp->r_rq, qp->r_rq.size * sz,\n\t\t\t\t\t   rdi->dparms.node, udata);\n\t\t\tif (ret)\n\t\t\t\tgoto bail_driver_priv;\n\t\t}\n\n\t\t \n\t\tspin_lock_init(&qp->r_lock);\n\t\tspin_lock_init(&qp->s_hlock);\n\t\tspin_lock_init(&qp->s_lock);\n\t\tatomic_set(&qp->refcount, 0);\n\t\tatomic_set(&qp->local_ops_pending, 0);\n\t\tinit_waitqueue_head(&qp->wait);\n\t\tINIT_LIST_HEAD(&qp->rspwait);\n\t\tqp->state = IB_QPS_RESET;\n\t\tqp->s_wq = swq;\n\t\tqp->s_size = sqsize;\n\t\tqp->s_avail = init_attr->cap.max_send_wr;\n\t\tqp->s_max_sge = init_attr->cap.max_send_sge;\n\t\tif (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)\n\t\t\tqp->s_flags = RVT_S_SIGNAL_REQ_WR;\n\t\tret = alloc_ud_wq_attr(qp, rdi->dparms.node);\n\t\tif (ret)\n\t\t\tgoto bail_rq_rvt;\n\n\t\tif (init_attr->create_flags & IB_QP_CREATE_NETDEV_USE)\n\t\t\texclude_prefix = RVT_AIP_QP_PREFIX;\n\n\t\tret = alloc_qpn(rdi, &rdi->qp_dev->qpn_table,\n\t\t\t\tinit_attr->qp_type,\n\t\t\t\tinit_attr->port_num,\n\t\t\t\texclude_prefix);\n\t\tif (ret < 0)\n\t\t\tgoto bail_rq_wq;\n\n\t\tqp->ibqp.qp_num = ret;\n\t\tif (init_attr->create_flags & IB_QP_CREATE_NETDEV_USE)\n\t\t\tqp->ibqp.qp_num |= RVT_AIP_QP_BASE;\n\t\tqp->port_num = init_attr->port_num;\n\t\trvt_init_qp(rdi, qp, init_attr->qp_type);\n\t\tif (rdi->driver_f.qp_priv_init) {\n\t\t\tret = rdi->driver_f.qp_priv_init(rdi, qp, init_attr);\n\t\t\tif (ret)\n\t\t\t\tgoto bail_rq_wq;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tinit_attr->cap.max_inline_data = 0;\n\n\t \n\tif (udata && udata->outlen >= sizeof(__u64)) {\n\t\tif (!qp->r_rq.wq) {\n\t\t\t__u64 offset = 0;\n\n\t\t\tret = ib_copy_to_udata(udata, &offset,\n\t\t\t\t\t       sizeof(offset));\n\t\t\tif (ret)\n\t\t\t\tgoto bail_qpn;\n\t\t} else {\n\t\t\tu32 s = sizeof(struct rvt_rwq) + qp->r_rq.size * sz;\n\n\t\t\tqp->ip = rvt_create_mmap_info(rdi, s, udata,\n\t\t\t\t\t\t      qp->r_rq.wq);\n\t\t\tif (IS_ERR(qp->ip)) {\n\t\t\t\tret = PTR_ERR(qp->ip);\n\t\t\t\tgoto bail_qpn;\n\t\t\t}\n\n\t\t\tret = ib_copy_to_udata(udata, &qp->ip->offset,\n\t\t\t\t\t       sizeof(qp->ip->offset));\n\t\t\tif (ret)\n\t\t\t\tgoto bail_ip;\n\t\t}\n\t\tqp->pid = current->pid;\n\t}\n\n\tspin_lock(&rdi->n_qps_lock);\n\tif (rdi->n_qps_allocated == rdi->dparms.props.max_qp) {\n\t\tspin_unlock(&rdi->n_qps_lock);\n\t\tret = -ENOMEM;\n\t\tgoto bail_ip;\n\t}\n\n\trdi->n_qps_allocated++;\n\t \n\tif (init_attr->qp_type == IB_QPT_RC) {\n\t\trdi->n_rc_qps++;\n\t\trdi->busy_jiffies = rdi->n_rc_qps / RC_QP_SCALING_INTERVAL;\n\t}\n\tspin_unlock(&rdi->n_qps_lock);\n\n\tif (qp->ip) {\n\t\tspin_lock_irq(&rdi->pending_lock);\n\t\tlist_add(&qp->ip->pending_mmaps, &rdi->pending_mmaps);\n\t\tspin_unlock_irq(&rdi->pending_lock);\n\t}\n\n\treturn 0;\n\nbail_ip:\n\tif (qp->ip)\n\t\tkref_put(&qp->ip->ref, rvt_release_mmap_info);\n\nbail_qpn:\n\trvt_free_qpn(&rdi->qp_dev->qpn_table, qp->ibqp.qp_num);\n\nbail_rq_wq:\n\tfree_ud_wq_attr(qp);\n\nbail_rq_rvt:\n\trvt_free_rq(&qp->r_rq);\n\nbail_driver_priv:\n\trdi->driver_f.qp_priv_free(rdi, qp);\n\nbail_qp:\n\tkfree(qp->s_ack_queue);\n\tkfree(qp->r_sg_list);\n\tvfree(swq);\n\treturn ret;\n}\n\n \nint rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err)\n{\n\tstruct ib_wc wc;\n\tint ret = 0;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\tlockdep_assert_held(&qp->r_lock);\n\tlockdep_assert_held(&qp->s_lock);\n\tif (qp->state == IB_QPS_ERR || qp->state == IB_QPS_RESET)\n\t\tgoto bail;\n\n\tqp->state = IB_QPS_ERR;\n\n\tif (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {\n\t\tqp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);\n\t\tdel_timer(&qp->s_timer);\n\t}\n\n\tif (qp->s_flags & RVT_S_ANY_WAIT_SEND)\n\t\tqp->s_flags &= ~RVT_S_ANY_WAIT_SEND;\n\n\trdi->driver_f.notify_error_qp(qp);\n\n\t \n\tif (READ_ONCE(qp->s_last) != qp->s_head)\n\t\trdi->driver_f.schedule_send(qp);\n\n\trvt_clear_mr_refs(qp, 0);\n\n\tmemset(&wc, 0, sizeof(wc));\n\twc.qp = &qp->ibqp;\n\twc.opcode = IB_WC_RECV;\n\n\tif (test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags)) {\n\t\twc.wr_id = qp->r_wr_id;\n\t\twc.status = err;\n\t\trvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);\n\t}\n\twc.status = IB_WC_WR_FLUSH_ERR;\n\n\tif (qp->r_rq.kwq) {\n\t\tu32 head;\n\t\tu32 tail;\n\t\tstruct rvt_rwq *wq = NULL;\n\t\tstruct rvt_krwq *kwq = NULL;\n\n\t\tspin_lock(&qp->r_rq.kwq->c_lock);\n\t\t \n\t\tif (qp->ip) {\n\t\t\twq = qp->r_rq.wq;\n\t\t\thead = RDMA_READ_UAPI_ATOMIC(wq->head);\n\t\t\ttail = RDMA_READ_UAPI_ATOMIC(wq->tail);\n\t\t} else {\n\t\t\tkwq = qp->r_rq.kwq;\n\t\t\thead = kwq->head;\n\t\t\ttail = kwq->tail;\n\t\t}\n\t\t \n\t\tif (head >= qp->r_rq.size)\n\t\t\thead = 0;\n\t\tif (tail >= qp->r_rq.size)\n\t\t\ttail = 0;\n\t\twhile (tail != head) {\n\t\t\twc.wr_id = rvt_get_rwqe_ptr(&qp->r_rq, tail)->wr_id;\n\t\t\tif (++tail >= qp->r_rq.size)\n\t\t\t\ttail = 0;\n\t\t\trvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);\n\t\t}\n\t\tif (qp->ip)\n\t\t\tRDMA_WRITE_UAPI_ATOMIC(wq->tail, tail);\n\t\telse\n\t\t\tkwq->tail = tail;\n\t\tspin_unlock(&qp->r_rq.kwq->c_lock);\n\t} else if (qp->ibqp.event_handler) {\n\t\tret = 1;\n\t}\n\nbail:\n\treturn ret;\n}\nEXPORT_SYMBOL(rvt_error_qp);\n\n \nstatic void rvt_insert_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp)\n{\n\tstruct rvt_ibport *rvp = rdi->ports[qp->port_num - 1];\n\tunsigned long flags;\n\n\trvt_get_qp(qp);\n\tspin_lock_irqsave(&rdi->qp_dev->qpt_lock, flags);\n\n\tif (qp->ibqp.qp_num <= 1) {\n\t\trcu_assign_pointer(rvp->qp[qp->ibqp.qp_num], qp);\n\t} else {\n\t\tu32 n = hash_32(qp->ibqp.qp_num, rdi->qp_dev->qp_table_bits);\n\n\t\tqp->next = rdi->qp_dev->qp_table[n];\n\t\trcu_assign_pointer(rdi->qp_dev->qp_table[n], qp);\n\t\ttrace_rvt_qpinsert(qp, n);\n\t}\n\n\tspin_unlock_irqrestore(&rdi->qp_dev->qpt_lock, flags);\n}\n\n \nint rvt_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t  int attr_mask, struct ib_udata *udata)\n{\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tenum ib_qp_state cur_state, new_state;\n\tstruct ib_event ev;\n\tint lastwqe = 0;\n\tint mig = 0;\n\tint pmtu = 0;  \n\tint opa_ah;\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\tspin_lock_irq(&qp->r_lock);\n\tspin_lock(&qp->s_hlock);\n\tspin_lock(&qp->s_lock);\n\n\tcur_state = attr_mask & IB_QP_CUR_STATE ?\n\t\tattr->cur_qp_state : qp->state;\n\tnew_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;\n\topa_ah = rdma_cap_opa_ah(ibqp->device, qp->port_num);\n\n\tif (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,\n\t\t\t\tattr_mask))\n\t\tgoto inval;\n\n\tif (rdi->driver_f.check_modify_qp &&\n\t    rdi->driver_f.check_modify_qp(qp, attr, attr_mask, udata))\n\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_AV) {\n\t\tif (opa_ah) {\n\t\t\tif (rdma_ah_get_dlid(&attr->ah_attr) >=\n\t\t\t\topa_get_mcast_base(OPA_MCAST_NR))\n\t\t\t\tgoto inval;\n\t\t} else {\n\t\t\tif (rdma_ah_get_dlid(&attr->ah_attr) >=\n\t\t\t\tbe16_to_cpu(IB_MULTICAST_LID_BASE))\n\t\t\t\tgoto inval;\n\t\t}\n\n\t\tif (rvt_check_ah(qp->ibqp.device, &attr->ah_attr))\n\t\t\tgoto inval;\n\t}\n\n\tif (attr_mask & IB_QP_ALT_PATH) {\n\t\tif (opa_ah) {\n\t\t\tif (rdma_ah_get_dlid(&attr->alt_ah_attr) >=\n\t\t\t\topa_get_mcast_base(OPA_MCAST_NR))\n\t\t\t\tgoto inval;\n\t\t} else {\n\t\t\tif (rdma_ah_get_dlid(&attr->alt_ah_attr) >=\n\t\t\t\tbe16_to_cpu(IB_MULTICAST_LID_BASE))\n\t\t\t\tgoto inval;\n\t\t}\n\n\t\tif (rvt_check_ah(qp->ibqp.device, &attr->alt_ah_attr))\n\t\t\tgoto inval;\n\t\tif (attr->alt_pkey_index >= rvt_get_npkeys(rdi))\n\t\t\tgoto inval;\n\t}\n\n\tif (attr_mask & IB_QP_PKEY_INDEX)\n\t\tif (attr->pkey_index >= rvt_get_npkeys(rdi))\n\t\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_MIN_RNR_TIMER)\n\t\tif (attr->min_rnr_timer > 31)\n\t\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_PORT)\n\t\tif (qp->ibqp.qp_type == IB_QPT_SMI ||\n\t\t    qp->ibqp.qp_type == IB_QPT_GSI ||\n\t\t    attr->port_num == 0 ||\n\t\t    attr->port_num > ibqp->device->phys_port_cnt)\n\t\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_DEST_QPN)\n\t\tif (attr->dest_qp_num > RVT_QPN_MASK)\n\t\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_RETRY_CNT)\n\t\tif (attr->retry_cnt > 7)\n\t\t\tgoto inval;\n\n\tif (attr_mask & IB_QP_RNR_RETRY)\n\t\tif (attr->rnr_retry > 7)\n\t\t\tgoto inval;\n\n\t \n\tif (attr_mask & IB_QP_PATH_MTU) {\n\t\tpmtu = rdi->driver_f.get_pmtu_from_attr(rdi, qp, attr);\n\t\tif (pmtu < 0)\n\t\t\tgoto inval;\n\t}\n\n\tif (attr_mask & IB_QP_PATH_MIG_STATE) {\n\t\tif (attr->path_mig_state == IB_MIG_REARM) {\n\t\t\tif (qp->s_mig_state == IB_MIG_ARMED)\n\t\t\t\tgoto inval;\n\t\t\tif (new_state != IB_QPS_RTS)\n\t\t\t\tgoto inval;\n\t\t} else if (attr->path_mig_state == IB_MIG_MIGRATED) {\n\t\t\tif (qp->s_mig_state == IB_MIG_REARM)\n\t\t\t\tgoto inval;\n\t\t\tif (new_state != IB_QPS_RTS && new_state != IB_QPS_SQD)\n\t\t\t\tgoto inval;\n\t\t\tif (qp->s_mig_state == IB_MIG_ARMED)\n\t\t\t\tmig = 1;\n\t\t} else {\n\t\t\tgoto inval;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\n\t\tif (attr->max_dest_rd_atomic > rdi->dparms.max_rdma_atomic)\n\t\t\tgoto inval;\n\n\tswitch (new_state) {\n\tcase IB_QPS_RESET:\n\t\tif (qp->state != IB_QPS_RESET)\n\t\t\t_rvt_reset_qp(rdi, qp, ibqp->qp_type);\n\t\tbreak;\n\n\tcase IB_QPS_RTR:\n\t\t \n\t\tqp->r_flags &= ~RVT_R_COMM_EST;\n\t\tqp->state = new_state;\n\t\tbreak;\n\n\tcase IB_QPS_SQD:\n\t\tqp->s_draining = qp->s_last != qp->s_cur;\n\t\tqp->state = new_state;\n\t\tbreak;\n\n\tcase IB_QPS_SQE:\n\t\tif (qp->ibqp.qp_type == IB_QPT_RC)\n\t\t\tgoto inval;\n\t\tqp->state = new_state;\n\t\tbreak;\n\n\tcase IB_QPS_ERR:\n\t\tlastwqe = rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);\n\t\tbreak;\n\n\tdefault:\n\t\tqp->state = new_state;\n\t\tbreak;\n\t}\n\n\tif (attr_mask & IB_QP_PKEY_INDEX)\n\t\tqp->s_pkey_index = attr->pkey_index;\n\n\tif (attr_mask & IB_QP_PORT)\n\t\tqp->port_num = attr->port_num;\n\n\tif (attr_mask & IB_QP_DEST_QPN)\n\t\tqp->remote_qpn = attr->dest_qp_num;\n\n\tif (attr_mask & IB_QP_SQ_PSN) {\n\t\tqp->s_next_psn = attr->sq_psn & rdi->dparms.psn_modify_mask;\n\t\tqp->s_psn = qp->s_next_psn;\n\t\tqp->s_sending_psn = qp->s_next_psn;\n\t\tqp->s_last_psn = qp->s_next_psn - 1;\n\t\tqp->s_sending_hpsn = qp->s_last_psn;\n\t}\n\n\tif (attr_mask & IB_QP_RQ_PSN)\n\t\tqp->r_psn = attr->rq_psn & rdi->dparms.psn_modify_mask;\n\n\tif (attr_mask & IB_QP_ACCESS_FLAGS)\n\t\tqp->qp_access_flags = attr->qp_access_flags;\n\n\tif (attr_mask & IB_QP_AV) {\n\t\trdma_replace_ah_attr(&qp->remote_ah_attr, &attr->ah_attr);\n\t\tqp->s_srate = rdma_ah_get_static_rate(&attr->ah_attr);\n\t\tqp->srate_mbps = ib_rate_to_mbps(qp->s_srate);\n\t}\n\n\tif (attr_mask & IB_QP_ALT_PATH) {\n\t\trdma_replace_ah_attr(&qp->alt_ah_attr, &attr->alt_ah_attr);\n\t\tqp->s_alt_pkey_index = attr->alt_pkey_index;\n\t}\n\n\tif (attr_mask & IB_QP_PATH_MIG_STATE) {\n\t\tqp->s_mig_state = attr->path_mig_state;\n\t\tif (mig) {\n\t\t\tqp->remote_ah_attr = qp->alt_ah_attr;\n\t\t\tqp->port_num = rdma_ah_get_port_num(&qp->alt_ah_attr);\n\t\t\tqp->s_pkey_index = qp->s_alt_pkey_index;\n\t\t}\n\t}\n\n\tif (attr_mask & IB_QP_PATH_MTU) {\n\t\tqp->pmtu = rdi->driver_f.mtu_from_qp(rdi, qp, pmtu);\n\t\tqp->log_pmtu = ilog2(qp->pmtu);\n\t}\n\n\tif (attr_mask & IB_QP_RETRY_CNT) {\n\t\tqp->s_retry_cnt = attr->retry_cnt;\n\t\tqp->s_retry = attr->retry_cnt;\n\t}\n\n\tif (attr_mask & IB_QP_RNR_RETRY) {\n\t\tqp->s_rnr_retry_cnt = attr->rnr_retry;\n\t\tqp->s_rnr_retry = attr->rnr_retry;\n\t}\n\n\tif (attr_mask & IB_QP_MIN_RNR_TIMER)\n\t\tqp->r_min_rnr_timer = attr->min_rnr_timer;\n\n\tif (attr_mask & IB_QP_TIMEOUT) {\n\t\tqp->timeout = attr->timeout;\n\t\tqp->timeout_jiffies = rvt_timeout_to_jiffies(qp->timeout);\n\t}\n\n\tif (attr_mask & IB_QP_QKEY)\n\t\tqp->qkey = attr->qkey;\n\n\tif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\n\t\tqp->r_max_rd_atomic = attr->max_dest_rd_atomic;\n\n\tif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)\n\t\tqp->s_max_rd_atomic = attr->max_rd_atomic;\n\n\tif (rdi->driver_f.modify_qp)\n\t\trdi->driver_f.modify_qp(qp, attr, attr_mask, udata);\n\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->s_hlock);\n\tspin_unlock_irq(&qp->r_lock);\n\n\tif (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)\n\t\trvt_insert_qp(rdi, qp);\n\n\tif (lastwqe) {\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n\tif (mig) {\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_PATH_MIG;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n\treturn 0;\n\ninval:\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock(&qp->s_hlock);\n\tspin_unlock_irq(&qp->r_lock);\n\treturn -EINVAL;\n}\n\n \nint rvt_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\trvt_reset_qp(rdi, qp, ibqp->qp_type);\n\n\twait_event(qp->wait, !atomic_read(&qp->refcount));\n\t \n\trvt_free_qpn(&rdi->qp_dev->qpn_table, qp->ibqp.qp_num);\n\n\tspin_lock(&rdi->n_qps_lock);\n\trdi->n_qps_allocated--;\n\tif (qp->ibqp.qp_type == IB_QPT_RC) {\n\t\trdi->n_rc_qps--;\n\t\trdi->busy_jiffies = rdi->n_rc_qps / RC_QP_SCALING_INTERVAL;\n\t}\n\tspin_unlock(&rdi->n_qps_lock);\n\n\tif (qp->ip)\n\t\tkref_put(&qp->ip->ref, rvt_release_mmap_info);\n\tkvfree(qp->r_rq.kwq);\n\trdi->driver_f.qp_priv_free(rdi, qp);\n\tkfree(qp->s_ack_queue);\n\tkfree(qp->r_sg_list);\n\trdma_destroy_ah_attr(&qp->remote_ah_attr);\n\trdma_destroy_ah_attr(&qp->alt_ah_attr);\n\tfree_ud_wq_attr(qp);\n\tvfree(qp->s_wq);\n\treturn 0;\n}\n\n \nint rvt_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t int attr_mask, struct ib_qp_init_attr *init_attr)\n{\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\tattr->qp_state = qp->state;\n\tattr->cur_qp_state = attr->qp_state;\n\tattr->path_mtu = rdi->driver_f.mtu_to_path_mtu(qp->pmtu);\n\tattr->path_mig_state = qp->s_mig_state;\n\tattr->qkey = qp->qkey;\n\tattr->rq_psn = qp->r_psn & rdi->dparms.psn_mask;\n\tattr->sq_psn = qp->s_next_psn & rdi->dparms.psn_mask;\n\tattr->dest_qp_num = qp->remote_qpn;\n\tattr->qp_access_flags = qp->qp_access_flags;\n\tattr->cap.max_send_wr = qp->s_size - 1 -\n\t\trdi->dparms.reserved_operations;\n\tattr->cap.max_recv_wr = qp->ibqp.srq ? 0 : qp->r_rq.size - 1;\n\tattr->cap.max_send_sge = qp->s_max_sge;\n\tattr->cap.max_recv_sge = qp->r_rq.max_sge;\n\tattr->cap.max_inline_data = 0;\n\tattr->ah_attr = qp->remote_ah_attr;\n\tattr->alt_ah_attr = qp->alt_ah_attr;\n\tattr->pkey_index = qp->s_pkey_index;\n\tattr->alt_pkey_index = qp->s_alt_pkey_index;\n\tattr->en_sqd_async_notify = 0;\n\tattr->sq_draining = qp->s_draining;\n\tattr->max_rd_atomic = qp->s_max_rd_atomic;\n\tattr->max_dest_rd_atomic = qp->r_max_rd_atomic;\n\tattr->min_rnr_timer = qp->r_min_rnr_timer;\n\tattr->port_num = qp->port_num;\n\tattr->timeout = qp->timeout;\n\tattr->retry_cnt = qp->s_retry_cnt;\n\tattr->rnr_retry = qp->s_rnr_retry_cnt;\n\tattr->alt_port_num =\n\t\trdma_ah_get_port_num(&qp->alt_ah_attr);\n\tattr->alt_timeout = qp->alt_timeout;\n\n\tinit_attr->event_handler = qp->ibqp.event_handler;\n\tinit_attr->qp_context = qp->ibqp.qp_context;\n\tinit_attr->send_cq = qp->ibqp.send_cq;\n\tinit_attr->recv_cq = qp->ibqp.recv_cq;\n\tinit_attr->srq = qp->ibqp.srq;\n\tinit_attr->cap = attr->cap;\n\tif (qp->s_flags & RVT_S_SIGNAL_REQ_WR)\n\t\tinit_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\n\telse\n\t\tinit_attr->sq_sig_type = IB_SIGNAL_ALL_WR;\n\tinit_attr->qp_type = qp->ibqp.qp_type;\n\tinit_attr->port_num = qp->port_num;\n\treturn 0;\n}\n\n \nint rvt_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t  const struct ib_recv_wr **bad_wr)\n{\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tstruct rvt_krwq *wq = qp->r_rq.kwq;\n\tunsigned long flags;\n\tint qp_err_flush = (ib_rvt_state_ops[qp->state] & RVT_FLUSH_RECV) &&\n\t\t\t\t!qp->ibqp.srq;\n\n\t \n\tif (!(ib_rvt_state_ops[qp->state] & RVT_POST_RECV_OK) || !wq) {\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\tfor (; wr; wr = wr->next) {\n\t\tstruct rvt_rwqe *wqe;\n\t\tu32 next;\n\t\tint i;\n\n\t\tif ((unsigned)wr->num_sge > qp->r_rq.max_sge) {\n\t\t\t*bad_wr = wr;\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tspin_lock_irqsave(&qp->r_rq.kwq->p_lock, flags);\n\t\tnext = wq->head + 1;\n\t\tif (next >= qp->r_rq.size)\n\t\t\tnext = 0;\n\t\tif (next == READ_ONCE(wq->tail)) {\n\t\t\tspin_unlock_irqrestore(&qp->r_rq.kwq->p_lock, flags);\n\t\t\t*bad_wr = wr;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (unlikely(qp_err_flush)) {\n\t\t\tstruct ib_wc wc;\n\n\t\t\tmemset(&wc, 0, sizeof(wc));\n\t\t\twc.qp = &qp->ibqp;\n\t\t\twc.opcode = IB_WC_RECV;\n\t\t\twc.wr_id = wr->wr_id;\n\t\t\twc.status = IB_WC_WR_FLUSH_ERR;\n\t\t\trvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);\n\t\t} else {\n\t\t\twqe = rvt_get_rwqe_ptr(&qp->r_rq, wq->head);\n\t\t\twqe->wr_id = wr->wr_id;\n\t\t\twqe->num_sge = wr->num_sge;\n\t\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\t\twqe->sg_list[i].addr = wr->sg_list[i].addr;\n\t\t\t\twqe->sg_list[i].length = wr->sg_list[i].length;\n\t\t\t\twqe->sg_list[i].lkey = wr->sg_list[i].lkey;\n\t\t\t}\n\t\t\t \n\t\t\tsmp_store_release(&wq->head, next);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->r_rq.kwq->p_lock, flags);\n\t}\n\treturn 0;\n}\n\n \nstatic inline int rvt_qp_valid_operation(\n\tstruct rvt_qp *qp,\n\tconst struct rvt_operation_params *post_parms,\n\tconst struct ib_send_wr *wr)\n{\n\tint len;\n\n\tif (wr->opcode >= RVT_OPERATION_MAX || !post_parms[wr->opcode].length)\n\t\treturn -EINVAL;\n\tif (!(post_parms[wr->opcode].qpt_support & BIT(qp->ibqp.qp_type)))\n\t\treturn -EINVAL;\n\tif ((post_parms[wr->opcode].flags & RVT_OPERATION_PRIV) &&\n\t    ibpd_to_rvtpd(qp->ibqp.pd)->user)\n\t\treturn -EINVAL;\n\tif (post_parms[wr->opcode].flags & RVT_OPERATION_ATOMIC_SGE &&\n\t    (wr->num_sge == 0 ||\n\t     wr->sg_list[0].length < sizeof(u64) ||\n\t     wr->sg_list[0].addr & (sizeof(u64) - 1)))\n\t\treturn -EINVAL;\n\tif (post_parms[wr->opcode].flags & RVT_OPERATION_ATOMIC &&\n\t    !qp->s_max_rd_atomic)\n\t\treturn -EINVAL;\n\tlen = post_parms[wr->opcode].length;\n\t \n\tif (qp->ibqp.qp_type != IB_QPT_UC &&\n\t    qp->ibqp.qp_type != IB_QPT_RC) {\n\t\tif (qp->ibqp.pd != ud_wr(wr)->ah->pd)\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(struct ib_ud_wr);\n\t}\n\treturn len;\n}\n\n \nstatic inline int rvt_qp_is_avail(\n\tstruct rvt_qp *qp,\n\tstruct rvt_dev_info *rdi,\n\tbool reserved_op)\n{\n\tu32 slast;\n\tu32 avail;\n\tu32 reserved_used;\n\n\t \n\tsmp_mb__before_atomic();\n\tif (unlikely(reserved_op)) {\n\t\t \n\t\treserved_used = atomic_read(&qp->s_reserved_used);\n\t\tif (reserved_used >= rdi->dparms.reserved_operations)\n\t\t\treturn -ENOMEM;\n\t\treturn 0;\n\t}\n\t \n\tif (likely(qp->s_avail))\n\t\treturn 0;\n\t \n\tslast = smp_load_acquire(&qp->s_last);\n\tif (qp->s_head >= slast)\n\t\tavail = qp->s_size - (qp->s_head - slast);\n\telse\n\t\tavail = slast - qp->s_head;\n\n\treserved_used = atomic_read(&qp->s_reserved_used);\n\tavail =  avail - 1 -\n\t\t(rdi->dparms.reserved_operations - reserved_used);\n\t \n\tif ((s32)avail <= 0)\n\t\treturn -ENOMEM;\n\tqp->s_avail = avail;\n\tif (WARN_ON(qp->s_avail >\n\t\t    (qp->s_size - 1 - rdi->dparms.reserved_operations)))\n\t\trvt_pr_err(rdi,\n\t\t\t   \"More avail entries than QP RB size.\\nQP: %u, size: %u, avail: %u\\nhead: %u, tail: %u, cur: %u, acked: %u, last: %u\",\n\t\t\t   qp->ibqp.qp_num, qp->s_size, qp->s_avail,\n\t\t\t   qp->s_head, qp->s_tail, qp->s_cur,\n\t\t\t   qp->s_acked, qp->s_last);\n\treturn 0;\n}\n\n \nstatic int rvt_post_one_wr(struct rvt_qp *qp,\n\t\t\t   const struct ib_send_wr *wr,\n\t\t\t   bool *call_send)\n{\n\tstruct rvt_swqe *wqe;\n\tu32 next;\n\tint i;\n\tint j;\n\tint acc;\n\tstruct rvt_lkey_table *rkt;\n\tstruct rvt_pd *pd;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\tu8 log_pmtu;\n\tint ret;\n\tsize_t cplen;\n\tbool reserved_op;\n\tint local_ops_delayed = 0;\n\n\tBUILD_BUG_ON(IB_QPT_MAX >= (sizeof(u32) * BITS_PER_BYTE));\n\n\t \n\tif (unlikely(wr->num_sge > qp->s_max_sge))\n\t\treturn -EINVAL;\n\n\tret = rvt_qp_valid_operation(qp, rdi->post_parms, wr);\n\tif (ret < 0)\n\t\treturn ret;\n\tcplen = ret;\n\n\t \n\tif ((rdi->post_parms[wr->opcode].flags & RVT_OPERATION_LOCAL)) {\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_REG_MR:\n\t\t\tret = rvt_fast_reg_mr(qp,\n\t\t\t\t\t      reg_wr(wr)->mr,\n\t\t\t\t\t      reg_wr(wr)->key,\n\t\t\t\t\t      reg_wr(wr)->access);\n\t\t\tif (ret || !(wr->send_flags & IB_SEND_SIGNALED))\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\tif ((wr->send_flags & IB_SEND_FENCE) ||\n\t\t\t    atomic_read(&qp->local_ops_pending)) {\n\t\t\t\tlocal_ops_delayed = 1;\n\t\t\t} else {\n\t\t\t\tret = rvt_invalidate_rkey(\n\t\t\t\t\tqp, wr->ex.invalidate_rkey);\n\t\t\t\tif (ret || !(wr->send_flags & IB_SEND_SIGNALED))\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treserved_op = rdi->post_parms[wr->opcode].flags &\n\t\t\tRVT_OPERATION_USE_RESERVE;\n\t \n\tret = rvt_qp_is_avail(qp, rdi, reserved_op);\n\tif (ret)\n\t\treturn ret;\n\tnext = qp->s_head + 1;\n\tif (next >= qp->s_size)\n\t\tnext = 0;\n\n\trkt = &rdi->lkey_table;\n\tpd = ibpd_to_rvtpd(qp->ibqp.pd);\n\twqe = rvt_get_swqe_ptr(qp, qp->s_head);\n\n\t \n\tmemcpy(&wqe->ud_wr, wr, cplen);\n\n\twqe->length = 0;\n\tj = 0;\n\tif (wr->num_sge) {\n\t\tstruct rvt_sge *last_sge = NULL;\n\n\t\tacc = wr->opcode >= IB_WR_RDMA_READ ?\n\t\t\tIB_ACCESS_LOCAL_WRITE : 0;\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\tu32 length = wr->sg_list[i].length;\n\n\t\t\tif (length == 0)\n\t\t\t\tcontinue;\n\t\t\tret = rvt_lkey_ok(rkt, pd, &wqe->sg_list[j], last_sge,\n\t\t\t\t\t  &wr->sg_list[i], acc);\n\t\t\tif (unlikely(ret < 0))\n\t\t\t\tgoto bail_inval_free;\n\t\t\twqe->length += length;\n\t\t\tif (ret)\n\t\t\t\tlast_sge = &wqe->sg_list[j];\n\t\t\tj += ret;\n\t\t}\n\t\twqe->wr.num_sge = j;\n\t}\n\n\t \n\tlog_pmtu = qp->log_pmtu;\n\tif (qp->allowed_ops == IB_OPCODE_UD) {\n\t\tstruct rvt_ah *ah = rvt_get_swqe_ah(wqe);\n\n\t\tlog_pmtu = ah->log_pmtu;\n\t\trdma_copy_ah_attr(wqe->ud_wr.attr, &ah->attr);\n\t}\n\n\tif (rdi->post_parms[wr->opcode].flags & RVT_OPERATION_LOCAL) {\n\t\tif (local_ops_delayed)\n\t\t\tatomic_inc(&qp->local_ops_pending);\n\t\telse\n\t\t\twqe->wr.send_flags |= RVT_SEND_COMPLETION_ONLY;\n\t\twqe->ssn = 0;\n\t\twqe->psn = 0;\n\t\twqe->lpsn = 0;\n\t} else {\n\t\twqe->ssn = qp->s_ssn++;\n\t\twqe->psn = qp->s_next_psn;\n\t\twqe->lpsn = wqe->psn +\n\t\t\t\t(wqe->length ?\n\t\t\t\t\t((wqe->length - 1) >> log_pmtu) :\n\t\t\t\t\t0);\n\t}\n\n\t \n\tif (rdi->driver_f.setup_wqe) {\n\t\tret = rdi->driver_f.setup_wqe(qp, wqe, call_send);\n\t\tif (ret < 0)\n\t\t\tgoto bail_inval_free_ref;\n\t}\n\n\tif (!(rdi->post_parms[wr->opcode].flags & RVT_OPERATION_LOCAL))\n\t\tqp->s_next_psn = wqe->lpsn + 1;\n\n\tif (unlikely(reserved_op)) {\n\t\twqe->wr.send_flags |= RVT_SEND_RESERVE_USED;\n\t\trvt_qp_wqe_reserve(qp, wqe);\n\t} else {\n\t\twqe->wr.send_flags &= ~RVT_SEND_RESERVE_USED;\n\t\tqp->s_avail--;\n\t}\n\ttrace_rvt_post_one_wr(qp, wqe, wr->num_sge);\n\tsmp_wmb();  \n\tqp->s_head = next;\n\n\treturn 0;\n\nbail_inval_free_ref:\n\tif (qp->allowed_ops == IB_OPCODE_UD)\n\t\trdma_destroy_ah_attr(wqe->ud_wr.attr);\nbail_inval_free:\n\t \n\twhile (j) {\n\t\tstruct rvt_sge *sge = &wqe->sg_list[--j];\n\n\t\trvt_put_mr(sge->mr);\n\t}\n\treturn ret;\n}\n\n \nint rvt_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t  const struct ib_send_wr **bad_wr)\n{\n\tstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\tunsigned long flags = 0;\n\tbool call_send;\n\tunsigned nreq = 0;\n\tint err = 0;\n\n\tspin_lock_irqsave(&qp->s_hlock, flags);\n\n\t \n\tif (unlikely(!(ib_rvt_state_ops[qp->state] & RVT_POST_SEND_OK))) {\n\t\tspin_unlock_irqrestore(&qp->s_hlock, flags);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tcall_send = qp->s_head == READ_ONCE(qp->s_last) && !wr->next;\n\n\tfor (; wr; wr = wr->next) {\n\t\terr = rvt_post_one_wr(qp, wr, &call_send);\n\t\tif (unlikely(err)) {\n\t\t\t*bad_wr = wr;\n\t\t\tgoto bail;\n\t\t}\n\t\tnreq++;\n\t}\nbail:\n\tspin_unlock_irqrestore(&qp->s_hlock, flags);\n\tif (nreq) {\n\t\t \n\t\tif (nreq == 1 && call_send)\n\t\t\trdi->driver_f.do_send(qp);\n\t\telse\n\t\t\trdi->driver_f.schedule_send_no_lock(qp);\n\t}\n\treturn err;\n}\n\n \nint rvt_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct rvt_srq *srq = ibsrq_to_rvtsrq(ibsrq);\n\tstruct rvt_krwq *wq;\n\tunsigned long flags;\n\n\tfor (; wr; wr = wr->next) {\n\t\tstruct rvt_rwqe *wqe;\n\t\tu32 next;\n\t\tint i;\n\n\t\tif ((unsigned)wr->num_sge > srq->rq.max_sge) {\n\t\t\t*bad_wr = wr;\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tspin_lock_irqsave(&srq->rq.kwq->p_lock, flags);\n\t\twq = srq->rq.kwq;\n\t\tnext = wq->head + 1;\n\t\tif (next >= srq->rq.size)\n\t\t\tnext = 0;\n\t\tif (next == READ_ONCE(wq->tail)) {\n\t\t\tspin_unlock_irqrestore(&srq->rq.kwq->p_lock, flags);\n\t\t\t*bad_wr = wr;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\twqe = rvt_get_rwqe_ptr(&srq->rq, wq->head);\n\t\twqe->wr_id = wr->wr_id;\n\t\twqe->num_sge = wr->num_sge;\n\t\tfor (i = 0; i < wr->num_sge; i++) {\n\t\t\twqe->sg_list[i].addr = wr->sg_list[i].addr;\n\t\t\twqe->sg_list[i].length = wr->sg_list[i].length;\n\t\t\twqe->sg_list[i].lkey = wr->sg_list[i].lkey;\n\t\t}\n\t\t \n\t\tsmp_store_release(&wq->head, next);\n\t\tspin_unlock_irqrestore(&srq->rq.kwq->p_lock, flags);\n\t}\n\treturn 0;\n}\n\n \nstatic struct ib_sge *rvt_cast_sge(struct rvt_wqe_sge *sge)\n{\n\tBUILD_BUG_ON(offsetof(struct ib_sge, addr) !=\n\t\t     offsetof(struct rvt_wqe_sge, addr));\n\tBUILD_BUG_ON(offsetof(struct ib_sge, length) !=\n\t\t     offsetof(struct rvt_wqe_sge, length));\n\tBUILD_BUG_ON(offsetof(struct ib_sge, lkey) !=\n\t\t     offsetof(struct rvt_wqe_sge, lkey));\n\treturn (struct ib_sge *)sge;\n}\n\n \nstatic int init_sge(struct rvt_qp *qp, struct rvt_rwqe *wqe)\n{\n\tint i, j, ret;\n\tstruct ib_wc wc;\n\tstruct rvt_lkey_table *rkt;\n\tstruct rvt_pd *pd;\n\tstruct rvt_sge_state *ss;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\trkt = &rdi->lkey_table;\n\tpd = ibpd_to_rvtpd(qp->ibqp.srq ? qp->ibqp.srq->pd : qp->ibqp.pd);\n\tss = &qp->r_sge;\n\tss->sg_list = qp->r_sg_list;\n\tqp->r_len = 0;\n\tfor (i = j = 0; i < wqe->num_sge; i++) {\n\t\tif (wqe->sg_list[i].length == 0)\n\t\t\tcontinue;\n\t\t \n\t\tret = rvt_lkey_ok(rkt, pd, j ? &ss->sg_list[j - 1] : &ss->sge,\n\t\t\t\t  NULL, rvt_cast_sge(&wqe->sg_list[i]),\n\t\t\t\t  IB_ACCESS_LOCAL_WRITE);\n\t\tif (unlikely(ret <= 0))\n\t\t\tgoto bad_lkey;\n\t\tqp->r_len += wqe->sg_list[i].length;\n\t\tj++;\n\t}\n\tss->num_sge = j;\n\tss->total_len = qp->r_len;\n\treturn 1;\n\nbad_lkey:\n\twhile (j) {\n\t\tstruct rvt_sge *sge = --j ? &ss->sg_list[j - 1] : &ss->sge;\n\n\t\trvt_put_mr(sge->mr);\n\t}\n\tss->num_sge = 0;\n\tmemset(&wc, 0, sizeof(wc));\n\twc.wr_id = wqe->wr_id;\n\twc.status = IB_WC_LOC_PROT_ERR;\n\twc.opcode = IB_WC_RECV;\n\twc.qp = &qp->ibqp;\n\t \n\trvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);\n\treturn 0;\n}\n\n \nstatic inline u32 get_rvt_head(struct rvt_rq *rq, void *ip)\n{\n\tu32 head;\n\n\tif (ip)\n\t\thead = RDMA_READ_UAPI_ATOMIC(rq->wq->head);\n\telse\n\t\thead = rq->kwq->head;\n\n\treturn head;\n}\n\n \nint rvt_get_rwqe(struct rvt_qp *qp, bool wr_id_only)\n{\n\tunsigned long flags;\n\tstruct rvt_rq *rq;\n\tstruct rvt_krwq *kwq = NULL;\n\tstruct rvt_rwq *wq;\n\tstruct rvt_srq *srq;\n\tstruct rvt_rwqe *wqe;\n\tvoid (*handler)(struct ib_event *, void *);\n\tu32 tail;\n\tu32 head;\n\tint ret;\n\tvoid *ip = NULL;\n\n\tif (qp->ibqp.srq) {\n\t\tsrq = ibsrq_to_rvtsrq(qp->ibqp.srq);\n\t\thandler = srq->ibsrq.event_handler;\n\t\trq = &srq->rq;\n\t\tip = srq->ip;\n\t} else {\n\t\tsrq = NULL;\n\t\thandler = NULL;\n\t\trq = &qp->r_rq;\n\t\tip = qp->ip;\n\t}\n\n\tspin_lock_irqsave(&rq->kwq->c_lock, flags);\n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {\n\t\tret = 0;\n\t\tgoto unlock;\n\t}\n\tkwq = rq->kwq;\n\tif (ip) {\n\t\twq = rq->wq;\n\t\ttail = RDMA_READ_UAPI_ATOMIC(wq->tail);\n\t} else {\n\t\ttail = kwq->tail;\n\t}\n\n\t \n\tif (tail >= rq->size)\n\t\ttail = 0;\n\n\tif (kwq->count < RVT_RWQ_COUNT_THRESHOLD) {\n\t\thead = get_rvt_head(rq, ip);\n\t\tkwq->count = rvt_get_rq_count(rq, head, tail);\n\t}\n\tif (unlikely(kwq->count == 0)) {\n\t\tret = 0;\n\t\tgoto unlock;\n\t}\n\t \n\tsmp_rmb();\n\twqe = rvt_get_rwqe_ptr(rq, tail);\n\t \n\tif (++tail >= rq->size)\n\t\ttail = 0;\n\tif (ip)\n\t\tRDMA_WRITE_UAPI_ATOMIC(wq->tail, tail);\n\telse\n\t\tkwq->tail = tail;\n\tif (!wr_id_only && !init_sge(qp, wqe)) {\n\t\tret = -1;\n\t\tgoto unlock;\n\t}\n\tqp->r_wr_id = wqe->wr_id;\n\n\tkwq->count--;\n\tret = 1;\n\tset_bit(RVT_R_WRID_VALID, &qp->r_aflags);\n\tif (handler) {\n\t\t \n\t\tif (kwq->count < srq->limit) {\n\t\t\tkwq->count =\n\t\t\t\trvt_get_rq_count(rq,\n\t\t\t\t\t\t get_rvt_head(rq, ip), tail);\n\t\t\tif (kwq->count < srq->limit) {\n\t\t\t\tstruct ib_event ev;\n\n\t\t\t\tsrq->limit = 0;\n\t\t\t\tspin_unlock_irqrestore(&rq->kwq->c_lock, flags);\n\t\t\t\tev.device = qp->ibqp.device;\n\t\t\t\tev.element.srq = qp->ibqp.srq;\n\t\t\t\tev.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\t\t\t\thandler(&ev, srq->ibsrq.srq_context);\n\t\t\t\tgoto bail;\n\t\t\t}\n\t\t}\n\t}\nunlock:\n\tspin_unlock_irqrestore(&rq->kwq->c_lock, flags);\nbail:\n\treturn ret;\n}\nEXPORT_SYMBOL(rvt_get_rwqe);\n\n \nvoid rvt_comm_est(struct rvt_qp *qp)\n{\n\tqp->r_flags |= RVT_R_COMM_EST;\n\tif (qp->ibqp.event_handler) {\n\t\tstruct ib_event ev;\n\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_COMM_EST;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n}\nEXPORT_SYMBOL(rvt_comm_est);\n\nvoid rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err)\n{\n\tunsigned long flags;\n\tint lastwqe;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\tlastwqe = rvt_error_qp(qp, err);\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\n\tif (lastwqe) {\n\t\tstruct ib_event ev;\n\n\t\tev.device = qp->ibqp.device;\n\t\tev.element.qp = &qp->ibqp;\n\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t}\n}\nEXPORT_SYMBOL(rvt_rc_error);\n\n \nunsigned long rvt_rnr_tbl_to_usec(u32 index)\n{\n\treturn ib_rvt_rnr_table[(index & IB_AETH_CREDIT_MASK)];\n}\nEXPORT_SYMBOL(rvt_rnr_tbl_to_usec);\n\nstatic inline unsigned long rvt_aeth_to_usec(u32 aeth)\n{\n\treturn ib_rvt_rnr_table[(aeth >> IB_AETH_CREDIT_SHIFT) &\n\t\t\t\t  IB_AETH_CREDIT_MASK];\n}\n\n \nvoid rvt_add_retry_timer_ext(struct rvt_qp *qp, u8 shift)\n{\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\tlockdep_assert_held(&qp->s_lock);\n\tqp->s_flags |= RVT_S_TIMER;\n        \n\tqp->s_timer.expires = jiffies + rdi->busy_jiffies +\n\t\t\t      (qp->timeout_jiffies << shift);\n\tadd_timer(&qp->s_timer);\n}\nEXPORT_SYMBOL(rvt_add_retry_timer_ext);\n\n \nvoid rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth)\n{\n\tu32 to;\n\n\tlockdep_assert_held(&qp->s_lock);\n\tqp->s_flags |= RVT_S_WAIT_RNR;\n\tto = rvt_aeth_to_usec(aeth);\n\ttrace_rvt_rnrnak_add(qp, to);\n\thrtimer_start(&qp->s_rnr_timer,\n\t\t      ns_to_ktime(1000 * to), HRTIMER_MODE_REL_PINNED);\n}\nEXPORT_SYMBOL(rvt_add_rnr_timer);\n\n \nvoid rvt_stop_rc_timers(struct rvt_qp *qp)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\t \n\tif (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {\n\t\tqp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);\n\t\tdel_timer(&qp->s_timer);\n\t\thrtimer_try_to_cancel(&qp->s_rnr_timer);\n\t}\n}\nEXPORT_SYMBOL(rvt_stop_rc_timers);\n\n \nstatic void rvt_stop_rnr_timer(struct rvt_qp *qp)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\t \n\tif (qp->s_flags & RVT_S_WAIT_RNR) {\n\t\tqp->s_flags &= ~RVT_S_WAIT_RNR;\n\t\ttrace_rvt_rnrnak_stop(qp, 0);\n\t}\n}\n\n \nvoid rvt_del_timers_sync(struct rvt_qp *qp)\n{\n\tdel_timer_sync(&qp->s_timer);\n\thrtimer_cancel(&qp->s_rnr_timer);\n}\nEXPORT_SYMBOL(rvt_del_timers_sync);\n\n \nstatic void rvt_rc_timeout(struct timer_list *t)\n{\n\tstruct rvt_qp *qp = from_timer(qp, t, s_timer);\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->r_lock, flags);\n\tspin_lock(&qp->s_lock);\n\tif (qp->s_flags & RVT_S_TIMER) {\n\t\tstruct rvt_ibport *rvp = rdi->ports[qp->port_num - 1];\n\n\t\tqp->s_flags &= ~RVT_S_TIMER;\n\t\trvp->n_rc_timeouts++;\n\t\tdel_timer(&qp->s_timer);\n\t\ttrace_rvt_rc_timeout(qp, qp->s_last_psn + 1);\n\t\tif (rdi->driver_f.notify_restart_rc)\n\t\t\trdi->driver_f.notify_restart_rc(qp,\n\t\t\t\t\t\t\tqp->s_last_psn + 1,\n\t\t\t\t\t\t\t1);\n\t\trdi->driver_f.schedule_send(qp);\n\t}\n\tspin_unlock(&qp->s_lock);\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\n}\n\n \nenum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t)\n{\n\tstruct rvt_qp *qp = container_of(t, struct rvt_qp, s_rnr_timer);\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->s_lock, flags);\n\trvt_stop_rnr_timer(qp);\n\ttrace_rvt_rnrnak_timeout(qp, 0);\n\trdi->driver_f.schedule_send(qp);\n\tspin_unlock_irqrestore(&qp->s_lock, flags);\n\treturn HRTIMER_NORESTART;\n}\nEXPORT_SYMBOL(rvt_rc_rnr_retry);\n\n \nstruct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,\n\t\t\t\t     u64 v,\n\t\t\t\t     void (*cb)(struct rvt_qp *qp, u64 v))\n{\n\tstruct rvt_qp_iter *i;\n\n\ti = kzalloc(sizeof(*i), GFP_KERNEL);\n\tif (!i)\n\t\treturn NULL;\n\n\ti->rdi = rdi;\n\t \n\ti->specials = rdi->ibdev.phys_port_cnt * 2;\n\ti->v = v;\n\ti->cb = cb;\n\n\treturn i;\n}\nEXPORT_SYMBOL(rvt_qp_iter_init);\n\n \nint rvt_qp_iter_next(struct rvt_qp_iter *iter)\n\t__must_hold(RCU)\n{\n\tint n = iter->n;\n\tint ret = 1;\n\tstruct rvt_qp *pqp = iter->qp;\n\tstruct rvt_qp *qp;\n\tstruct rvt_dev_info *rdi = iter->rdi;\n\n\t \n\tfor (; n <  rdi->qp_dev->qp_table_size + iter->specials; n++) {\n\t\tif (pqp) {\n\t\t\tqp = rcu_dereference(pqp->next);\n\t\t} else {\n\t\t\tif (n < iter->specials) {\n\t\t\t\tstruct rvt_ibport *rvp;\n\t\t\t\tint pidx;\n\n\t\t\t\tpidx = n % rdi->ibdev.phys_port_cnt;\n\t\t\t\trvp = rdi->ports[pidx];\n\t\t\t\tqp = rcu_dereference(rvp->qp[n & 1]);\n\t\t\t} else {\n\t\t\t\tqp = rcu_dereference(\n\t\t\t\t\trdi->qp_dev->qp_table[\n\t\t\t\t\t\t(n - iter->specials)]);\n\t\t\t}\n\t\t}\n\t\tpqp = qp;\n\t\tif (qp) {\n\t\t\titer->qp = qp;\n\t\t\titer->n = n;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(rvt_qp_iter_next);\n\n \nvoid rvt_qp_iter(struct rvt_dev_info *rdi,\n\t\t u64 v,\n\t\t void (*cb)(struct rvt_qp *qp, u64 v))\n{\n\tint ret;\n\tstruct rvt_qp_iter i = {\n\t\t.rdi = rdi,\n\t\t.specials = rdi->ibdev.phys_port_cnt * 2,\n\t\t.v = v,\n\t\t.cb = cb\n\t};\n\n\trcu_read_lock();\n\tdo {\n\t\tret = rvt_qp_iter_next(&i);\n\t\tif (!ret) {\n\t\t\trvt_get_qp(i.qp);\n\t\t\trcu_read_unlock();\n\t\t\ti.cb(i.qp, i.v);\n\t\t\trcu_read_lock();\n\t\t\trvt_put_qp(i.qp);\n\t\t}\n\t} while (!ret);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(rvt_qp_iter);\n\n \nvoid rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t       enum ib_wc_status status)\n{\n\tu32 old_last, last;\n\tstruct rvt_dev_info *rdi;\n\n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_OR_FLUSH_SEND))\n\t\treturn;\n\trdi = ib_to_rvt(qp->ibqp.device);\n\n\told_last = qp->s_last;\n\ttrace_rvt_qp_send_completion(qp, wqe, old_last);\n\tlast = rvt_qp_complete_swqe(qp, wqe, rdi->wc_opcode[wqe->wr.opcode],\n\t\t\t\t    status);\n\tif (qp->s_acked == old_last)\n\t\tqp->s_acked = last;\n\tif (qp->s_cur == old_last)\n\t\tqp->s_cur = last;\n\tif (qp->s_tail == old_last)\n\t\tqp->s_tail = last;\n\tif (qp->state == IB_QPS_SQD && last == qp->s_cur)\n\t\tqp->s_draining = 0;\n}\nEXPORT_SYMBOL(rvt_send_complete);\n\n \nvoid rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,\n\t\t  void *data, u32 length,\n\t\t  bool release, bool copy_last)\n{\n\tstruct rvt_sge *sge = &ss->sge;\n\tint i;\n\tbool in_last = false;\n\tbool cacheless_copy = false;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\tstruct rvt_wss *wss = rdi->wss;\n\tunsigned int sge_copy_mode = rdi->dparms.sge_copy_mode;\n\n\tif (sge_copy_mode == RVT_SGE_COPY_CACHELESS) {\n\t\tcacheless_copy = length >= PAGE_SIZE;\n\t} else if (sge_copy_mode == RVT_SGE_COPY_ADAPTIVE) {\n\t\tif (length >= PAGE_SIZE) {\n\t\t\t \n\t\t\twss_insert(wss, sge->vaddr);\n\t\t\tif (length >= (2 * PAGE_SIZE))\n\t\t\t\twss_insert(wss, (sge->vaddr + PAGE_SIZE));\n\n\t\t\tcacheless_copy = wss_exceeds_threshold(wss);\n\t\t} else {\n\t\t\twss_advance_clean_counter(wss);\n\t\t}\n\t}\n\n\tif (copy_last) {\n\t\tif (length > 8) {\n\t\t\tlength -= 8;\n\t\t} else {\n\t\t\tcopy_last = false;\n\t\t\tin_last = true;\n\t\t}\n\t}\n\nagain:\n\twhile (length) {\n\t\tu32 len = rvt_get_sge_length(sge, length);\n\n\t\tWARN_ON_ONCE(len == 0);\n\t\tif (unlikely(in_last)) {\n\t\t\t \n\t\t\tfor (i = 0; i < len; i++)\n\t\t\t\t((u8 *)sge->vaddr)[i] = ((u8 *)data)[i];\n\t\t} else if (cacheless_copy) {\n\t\t\tcacheless_memcpy(sge->vaddr, data, len);\n\t\t} else {\n\t\t\tmemcpy(sge->vaddr, data, len);\n\t\t}\n\t\trvt_update_sge(ss, len, release);\n\t\tdata += len;\n\t\tlength -= len;\n\t}\n\n\tif (copy_last) {\n\t\tcopy_last = false;\n\t\tin_last = true;\n\t\tlength = 8;\n\t\tgoto again;\n\t}\n}\nEXPORT_SYMBOL(rvt_copy_sge);\n\nstatic enum ib_wc_status loopback_qp_drop(struct rvt_ibport *rvp,\n\t\t\t\t\t  struct rvt_qp *sqp)\n{\n\trvp->n_pkt_drops++;\n\t \n\treturn sqp->ibqp.qp_type == IB_QPT_RC ?\n\t\tIB_WC_RETRY_EXC_ERR : IB_WC_SUCCESS;\n}\n\n \nvoid rvt_ruc_loopback(struct rvt_qp *sqp)\n{\n\tstruct rvt_ibport *rvp =  NULL;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(sqp->ibqp.device);\n\tstruct rvt_qp *qp;\n\tstruct rvt_swqe *wqe;\n\tstruct rvt_sge *sge;\n\tunsigned long flags;\n\tstruct ib_wc wc;\n\tu64 sdata;\n\tatomic64_t *maddr;\n\tenum ib_wc_status send_status;\n\tbool release;\n\tint ret;\n\tbool copy_last = false;\n\tint local_ops = 0;\n\n\trcu_read_lock();\n\trvp = rdi->ports[sqp->port_num - 1];\n\n\t \n\n\tqp = rvt_lookup_qpn(ib_to_rvt(sqp->ibqp.device), rvp,\n\t\t\t    sqp->remote_qpn);\n\n\tspin_lock_irqsave(&sqp->s_lock, flags);\n\n\t \n\tif ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||\n\t    !(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_OR_FLUSH_SEND))\n\t\tgoto unlock;\n\n\tsqp->s_flags |= RVT_S_BUSY;\n\nagain:\n\tif (sqp->s_last == READ_ONCE(sqp->s_head))\n\t\tgoto clr_busy;\n\twqe = rvt_get_swqe_ptr(sqp, sqp->s_last);\n\n\t \n\tif (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_NEXT_SEND_OK)) {\n\t\tif (!(ib_rvt_state_ops[sqp->state] & RVT_FLUSH_SEND))\n\t\t\tgoto clr_busy;\n\t\t \n\t\tsend_status = IB_WC_WR_FLUSH_ERR;\n\t\tgoto flush_send;\n\t}\n\n\t \n\tif (sqp->s_last == sqp->s_cur) {\n\t\tif (++sqp->s_cur >= sqp->s_size)\n\t\t\tsqp->s_cur = 0;\n\t}\n\tspin_unlock_irqrestore(&sqp->s_lock, flags);\n\n\tif (!qp) {\n\t\tsend_status = loopback_qp_drop(rvp, sqp);\n\t\tgoto serr_no_r_lock;\n\t}\n\tspin_lock_irqsave(&qp->r_lock, flags);\n\tif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) ||\n\t    qp->ibqp.qp_type != sqp->ibqp.qp_type) {\n\t\tsend_status = loopback_qp_drop(rvp, sqp);\n\t\tgoto serr;\n\t}\n\n\tmemset(&wc, 0, sizeof(wc));\n\tsend_status = IB_WC_SUCCESS;\n\n\trelease = true;\n\tsqp->s_sge.sge = wqe->sg_list[0];\n\tsqp->s_sge.sg_list = wqe->sg_list + 1;\n\tsqp->s_sge.num_sge = wqe->wr.num_sge;\n\tsqp->s_len = wqe->length;\n\tswitch (wqe->wr.opcode) {\n\tcase IB_WR_REG_MR:\n\t\tgoto send_comp;\n\n\tcase IB_WR_LOCAL_INV:\n\t\tif (!(wqe->wr.send_flags & RVT_SEND_COMPLETION_ONLY)) {\n\t\t\tif (rvt_invalidate_rkey(sqp,\n\t\t\t\t\t\twqe->wr.ex.invalidate_rkey))\n\t\t\t\tsend_status = IB_WC_LOC_PROT_ERR;\n\t\t\tlocal_ops = 1;\n\t\t}\n\t\tgoto send_comp;\n\n\tcase IB_WR_SEND_WITH_INV:\n\tcase IB_WR_SEND_WITH_IMM:\n\tcase IB_WR_SEND:\n\t\tret = rvt_get_rwqe(qp, false);\n\t\tif (ret < 0)\n\t\t\tgoto op_err;\n\t\tif (!ret)\n\t\t\tgoto rnr_nak;\n\t\tif (wqe->length > qp->r_len)\n\t\t\tgoto inv_err;\n\t\tswitch (wqe->wr.opcode) {\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\tif (!rvt_invalidate_rkey(qp,\n\t\t\t\t\t\t wqe->wr.ex.invalidate_rkey)) {\n\t\t\t\twc.wc_flags = IB_WC_WITH_INVALIDATE;\n\t\t\t\twc.ex.invalidate_rkey =\n\t\t\t\t\twqe->wr.ex.invalidate_rkey;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\twc.wc_flags = IB_WC_WITH_IMM;\n\t\t\twc.ex.imm_data = wqe->wr.ex.imm_data;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))\n\t\t\tgoto inv_err;\n\t\twc.wc_flags = IB_WC_WITH_IMM;\n\t\twc.ex.imm_data = wqe->wr.ex.imm_data;\n\t\tret = rvt_get_rwqe(qp, true);\n\t\tif (ret < 0)\n\t\t\tgoto op_err;\n\t\tif (!ret)\n\t\t\tgoto rnr_nak;\n\t\t \n\t\tgoto do_write;\n\tcase IB_WR_RDMA_WRITE:\n\t\tcopy_last = rvt_is_user_qp(qp);\n\t\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))\n\t\t\tgoto inv_err;\ndo_write:\n\t\tif (wqe->length == 0)\n\t\t\tbreak;\n\t\tif (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, wqe->length,\n\t\t\t\t\t  wqe->rdma_wr.remote_addr,\n\t\t\t\t\t  wqe->rdma_wr.rkey,\n\t\t\t\t\t  IB_ACCESS_REMOTE_WRITE)))\n\t\t\tgoto acc_err;\n\t\tqp->r_sge.sg_list = NULL;\n\t\tqp->r_sge.num_sge = 1;\n\t\tqp->r_sge.total_len = wqe->length;\n\t\tbreak;\n\n\tcase IB_WR_RDMA_READ:\n\t\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))\n\t\t\tgoto inv_err;\n\t\tif (unlikely(!rvt_rkey_ok(qp, &sqp->s_sge.sge, wqe->length,\n\t\t\t\t\t  wqe->rdma_wr.remote_addr,\n\t\t\t\t\t  wqe->rdma_wr.rkey,\n\t\t\t\t\t  IB_ACCESS_REMOTE_READ)))\n\t\t\tgoto acc_err;\n\t\trelease = false;\n\t\tsqp->s_sge.sg_list = NULL;\n\t\tsqp->s_sge.num_sge = 1;\n\t\tqp->r_sge.sge = wqe->sg_list[0];\n\t\tqp->r_sge.sg_list = wqe->sg_list + 1;\n\t\tqp->r_sge.num_sge = wqe->wr.num_sge;\n\t\tqp->r_sge.total_len = wqe->length;\n\t\tbreak;\n\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\tif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))\n\t\t\tgoto inv_err;\n\t\tif (unlikely(wqe->atomic_wr.remote_addr & (sizeof(u64) - 1)))\n\t\t\tgoto inv_err;\n\t\tif (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, sizeof(u64),\n\t\t\t\t\t  wqe->atomic_wr.remote_addr,\n\t\t\t\t\t  wqe->atomic_wr.rkey,\n\t\t\t\t\t  IB_ACCESS_REMOTE_ATOMIC)))\n\t\t\tgoto acc_err;\n\t\t \n\t\tmaddr = (atomic64_t *)qp->r_sge.sge.vaddr;\n\t\tsdata = wqe->atomic_wr.compare_add;\n\t\t*(u64 *)sqp->s_sge.sge.vaddr =\n\t\t\t(wqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) ?\n\t\t\t(u64)atomic64_add_return(sdata, maddr) - sdata :\n\t\t\t(u64)cmpxchg((u64 *)qp->r_sge.sge.vaddr,\n\t\t\t\t      sdata, wqe->atomic_wr.swap);\n\t\trvt_put_mr(qp->r_sge.sge.mr);\n\t\tqp->r_sge.num_sge = 0;\n\t\tgoto send_comp;\n\n\tdefault:\n\t\tsend_status = IB_WC_LOC_QP_OP_ERR;\n\t\tgoto serr;\n\t}\n\n\tsge = &sqp->s_sge.sge;\n\twhile (sqp->s_len) {\n\t\tu32 len = rvt_get_sge_length(sge, sqp->s_len);\n\n\t\tWARN_ON_ONCE(len == 0);\n\t\trvt_copy_sge(qp, &qp->r_sge, sge->vaddr,\n\t\t\t     len, release, copy_last);\n\t\trvt_update_sge(&sqp->s_sge, len, !release);\n\t\tsqp->s_len -= len;\n\t}\n\tif (release)\n\t\trvt_put_ss(&qp->r_sge);\n\n\tif (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))\n\t\tgoto send_comp;\n\n\tif (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)\n\t\twc.opcode = IB_WC_RECV_RDMA_WITH_IMM;\n\telse\n\t\twc.opcode = IB_WC_RECV;\n\twc.wr_id = qp->r_wr_id;\n\twc.status = IB_WC_SUCCESS;\n\twc.byte_len = wqe->length;\n\twc.qp = &qp->ibqp;\n\twc.src_qp = qp->remote_qpn;\n\twc.slid = rdma_ah_get_dlid(&qp->remote_ah_attr) & U16_MAX;\n\twc.sl = rdma_ah_get_sl(&qp->remote_ah_attr);\n\twc.port_num = 1;\n\t \n\trvt_recv_cq(qp, &wc, wqe->wr.send_flags & IB_SEND_SOLICITED);\n\nsend_comp:\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\n\tspin_lock_irqsave(&sqp->s_lock, flags);\n\trvp->n_loop_pkts++;\nflush_send:\n\tsqp->s_rnr_retry = sqp->s_rnr_retry_cnt;\n\tspin_lock(&sqp->r_lock);\n\trvt_send_complete(sqp, wqe, send_status);\n\tspin_unlock(&sqp->r_lock);\n\tif (local_ops) {\n\t\tatomic_dec(&sqp->local_ops_pending);\n\t\tlocal_ops = 0;\n\t}\n\tgoto again;\n\nrnr_nak:\n\t \n\tif (qp->ibqp.qp_type == IB_QPT_UC)\n\t\tgoto send_comp;\n\trvp->n_rnr_naks++;\n\t \n\tif (sqp->s_rnr_retry == 0) {\n\t\tsend_status = IB_WC_RNR_RETRY_EXC_ERR;\n\t\tgoto serr;\n\t}\n\tif (sqp->s_rnr_retry_cnt < 7)\n\t\tsqp->s_rnr_retry--;\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\n\tspin_lock_irqsave(&sqp->s_lock, flags);\n\tif (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_RECV_OK))\n\t\tgoto clr_busy;\n\trvt_add_rnr_timer(sqp, qp->r_min_rnr_timer <<\n\t\t\t\tIB_AETH_CREDIT_SHIFT);\n\tgoto clr_busy;\n\nop_err:\n\tsend_status = IB_WC_REM_OP_ERR;\n\twc.status = IB_WC_LOC_QP_OP_ERR;\n\tgoto err;\n\ninv_err:\n\tsend_status =\n\t\tsqp->ibqp.qp_type == IB_QPT_RC ?\n\t\t\tIB_WC_REM_INV_REQ_ERR :\n\t\t\tIB_WC_SUCCESS;\n\twc.status = IB_WC_LOC_QP_OP_ERR;\n\tgoto err;\n\nacc_err:\n\tsend_status = IB_WC_REM_ACCESS_ERR;\n\twc.status = IB_WC_LOC_PROT_ERR;\nerr:\n\t \n\trvt_rc_error(qp, wc.status);\n\nserr:\n\tspin_unlock_irqrestore(&qp->r_lock, flags);\nserr_no_r_lock:\n\tspin_lock_irqsave(&sqp->s_lock, flags);\n\tspin_lock(&sqp->r_lock);\n\trvt_send_complete(sqp, wqe, send_status);\n\tspin_unlock(&sqp->r_lock);\n\tif (sqp->ibqp.qp_type == IB_QPT_RC) {\n\t\tint lastwqe;\n\n\t\tspin_lock(&sqp->r_lock);\n\t\tlastwqe = rvt_error_qp(sqp, IB_WC_WR_FLUSH_ERR);\n\t\tspin_unlock(&sqp->r_lock);\n\n\t\tsqp->s_flags &= ~RVT_S_BUSY;\n\t\tspin_unlock_irqrestore(&sqp->s_lock, flags);\n\t\tif (lastwqe) {\n\t\t\tstruct ib_event ev;\n\n\t\t\tev.device = sqp->ibqp.device;\n\t\t\tev.element.qp = &sqp->ibqp;\n\t\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\t\tsqp->ibqp.event_handler(&ev, sqp->ibqp.qp_context);\n\t\t}\n\t\tgoto done;\n\t}\nclr_busy:\n\tsqp->s_flags &= ~RVT_S_BUSY;\nunlock:\n\tspin_unlock_irqrestore(&sqp->s_lock, flags);\ndone:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(rvt_ruc_loopback);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}