{
  "module_name": "rxe_loc.h",
  "hash_id": "5486688396f26ae645011c3bfb81c1b3be8c8867371ccf44a11f0c732d552631",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rxe/rxe_loc.h",
  "human_readable_source": " \n \n\n#ifndef RXE_LOC_H\n#define RXE_LOC_H\n\n \nvoid rxe_init_av(struct rdma_ah_attr *attr, struct rxe_av *av);\nint rxe_av_chk_attr(struct rxe_qp *qp, struct rdma_ah_attr *attr);\nint rxe_ah_chk_attr(struct rxe_ah *ah, struct rdma_ah_attr *attr);\nvoid rxe_av_from_attr(u8 port_num, struct rxe_av *av,\n\t\t     struct rdma_ah_attr *attr);\nvoid rxe_av_to_attr(struct rxe_av *av, struct rdma_ah_attr *attr);\nvoid rxe_av_fill_ip_info(struct rxe_av *av, struct rdma_ah_attr *attr);\nstruct rxe_av *rxe_get_av(struct rxe_pkt_info *pkt, struct rxe_ah **ahp);\n\n \nint rxe_cq_chk_attr(struct rxe_dev *rxe, struct rxe_cq *cq,\n\t\t    int cqe, int comp_vector);\n\nint rxe_cq_from_init(struct rxe_dev *rxe, struct rxe_cq *cq, int cqe,\n\t\t     int comp_vector, struct ib_udata *udata,\n\t\t     struct rxe_create_cq_resp __user *uresp);\n\nint rxe_cq_resize_queue(struct rxe_cq *cq, int new_cqe,\n\t\t\tstruct rxe_resize_cq_resp __user *uresp,\n\t\t\tstruct ib_udata *udata);\n\nint rxe_cq_post(struct rxe_cq *cq, struct rxe_cqe *cqe, int solicited);\n\nvoid rxe_cq_cleanup(struct rxe_pool_elem *elem);\n\n \nstruct rxe_mcg *rxe_lookup_mcg(struct rxe_dev *rxe, union ib_gid *mgid);\nint rxe_attach_mcast(struct ib_qp *ibqp, union ib_gid *mgid, u16 mlid);\nint rxe_detach_mcast(struct ib_qp *ibqp, union ib_gid *mgid, u16 mlid);\nvoid rxe_cleanup_mcg(struct kref *kref);\n\n \nstruct rxe_mmap_info {\n\tstruct list_head\tpending_mmaps;\n\tstruct ib_ucontext\t*context;\n\tstruct kref\t\tref;\n\tvoid\t\t\t*obj;\n\n\tstruct mminfo info;\n};\n\nvoid rxe_mmap_release(struct kref *ref);\n\nstruct rxe_mmap_info *rxe_create_mmap_info(struct rxe_dev *dev, u32 size,\n\t\t\t\t\t   struct ib_udata *udata, void *obj);\n\nint rxe_mmap(struct ib_ucontext *context, struct vm_area_struct *vma);\n\n \nu8 rxe_get_next_key(u32 last_key);\nvoid rxe_mr_init_dma(int access, struct rxe_mr *mr);\nint rxe_mr_init_user(struct rxe_dev *rxe, u64 start, u64 length, u64 iova,\n\t\t     int access, struct rxe_mr *mr);\nint rxe_mr_init_fast(int max_pages, struct rxe_mr *mr);\nint rxe_flush_pmem_iova(struct rxe_mr *mr, u64 iova, unsigned int length);\nint rxe_mr_copy(struct rxe_mr *mr, u64 iova, void *addr,\n\t\tunsigned int length, enum rxe_mr_copy_dir dir);\nint copy_data(struct rxe_pd *pd, int access, struct rxe_dma_info *dma,\n\t      void *addr, int length, enum rxe_mr_copy_dir dir);\nint rxe_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,\n\t\t  int sg_nents, unsigned int *sg_offset);\nint rxe_mr_do_atomic_op(struct rxe_mr *mr, u64 iova, int opcode,\n\t\t\tu64 compare, u64 swap_add, u64 *orig_val);\nint rxe_mr_do_atomic_write(struct rxe_mr *mr, u64 iova, u64 value);\nstruct rxe_mr *lookup_mr(struct rxe_pd *pd, int access, u32 key,\n\t\t\t enum rxe_mr_lookup_type type);\nint mr_check_range(struct rxe_mr *mr, u64 iova, size_t length);\nint advance_dma_data(struct rxe_dma_info *dma, unsigned int length);\nint rxe_invalidate_mr(struct rxe_qp *qp, u32 key);\nint rxe_reg_fast_mr(struct rxe_qp *qp, struct rxe_send_wqe *wqe);\nvoid rxe_mr_cleanup(struct rxe_pool_elem *elem);\n\n \nint rxe_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata);\nint rxe_dealloc_mw(struct ib_mw *ibmw);\nint rxe_bind_mw(struct rxe_qp *qp, struct rxe_send_wqe *wqe);\nint rxe_invalidate_mw(struct rxe_qp *qp, u32 rkey);\nstruct rxe_mw *rxe_lookup_mw(struct rxe_qp *qp, int access, u32 rkey);\nvoid rxe_mw_cleanup(struct rxe_pool_elem *elem);\n\n \nstruct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,\n\t\t\t\tint paylen, struct rxe_pkt_info *pkt);\nint rxe_prepare(struct rxe_av *av, struct rxe_pkt_info *pkt,\n\t\tstruct sk_buff *skb);\nint rxe_xmit_packet(struct rxe_qp *qp, struct rxe_pkt_info *pkt,\n\t\t    struct sk_buff *skb);\nconst char *rxe_parent_name(struct rxe_dev *rxe, unsigned int port_num);\n\n \nint rxe_qp_chk_init(struct rxe_dev *rxe, struct ib_qp_init_attr *init);\nint rxe_qp_from_init(struct rxe_dev *rxe, struct rxe_qp *qp, struct rxe_pd *pd,\n\t\t     struct ib_qp_init_attr *init,\n\t\t     struct rxe_create_qp_resp __user *uresp,\n\t\t     struct ib_pd *ibpd, struct ib_udata *udata);\nint rxe_qp_to_init(struct rxe_qp *qp, struct ib_qp_init_attr *init);\nint rxe_qp_chk_attr(struct rxe_dev *rxe, struct rxe_qp *qp,\n\t\t    struct ib_qp_attr *attr, int mask);\nint rxe_qp_from_attr(struct rxe_qp *qp, struct ib_qp_attr *attr,\n\t\t     int mask, struct ib_udata *udata);\nint rxe_qp_to_attr(struct rxe_qp *qp, struct ib_qp_attr *attr, int mask);\nvoid rxe_qp_error(struct rxe_qp *qp);\nint rxe_qp_chk_destroy(struct rxe_qp *qp);\nvoid rxe_qp_cleanup(struct rxe_pool_elem *elem);\n\nstatic inline int qp_num(struct rxe_qp *qp)\n{\n\treturn qp->ibqp.qp_num;\n}\n\nstatic inline enum ib_qp_type qp_type(struct rxe_qp *qp)\n{\n\treturn qp->ibqp.qp_type;\n}\n\nstatic inline enum ib_qp_state qp_state(struct rxe_qp *qp)\n{\n\treturn qp->attr.qp_state;\n}\n\nstatic inline int qp_mtu(struct rxe_qp *qp)\n{\n\tif (qp->ibqp.qp_type == IB_QPT_RC || qp->ibqp.qp_type == IB_QPT_UC)\n\t\treturn qp->attr.path_mtu;\n\telse\n\t\treturn IB_MTU_4096;\n}\n\nvoid free_rd_atomic_resource(struct resp_res *res);\n\nstatic inline void rxe_advance_resp_resource(struct rxe_qp *qp)\n{\n\tqp->resp.res_head++;\n\tif (unlikely(qp->resp.res_head == qp->attr.max_dest_rd_atomic))\n\t\tqp->resp.res_head = 0;\n}\n\nvoid retransmit_timer(struct timer_list *t);\nvoid rnr_nak_timer(struct timer_list *t);\n\n \nint rxe_srq_chk_init(struct rxe_dev *rxe, struct ib_srq_init_attr *init);\nint rxe_srq_from_init(struct rxe_dev *rxe, struct rxe_srq *srq,\n\t\t      struct ib_srq_init_attr *init, struct ib_udata *udata,\n\t\t      struct rxe_create_srq_resp __user *uresp);\nint rxe_srq_chk_attr(struct rxe_dev *rxe, struct rxe_srq *srq,\n\t\t     struct ib_srq_attr *attr, enum ib_srq_attr_mask mask);\nint rxe_srq_from_attr(struct rxe_dev *rxe, struct rxe_srq *srq,\n\t\t      struct ib_srq_attr *attr, enum ib_srq_attr_mask mask,\n\t\t      struct rxe_modify_srq_cmd *ucmd, struct ib_udata *udata);\nvoid rxe_srq_cleanup(struct rxe_pool_elem *elem);\n\nvoid rxe_dealloc(struct ib_device *ib_dev);\n\nint rxe_completer(struct rxe_qp *qp);\nint rxe_requester(struct rxe_qp *qp);\nint rxe_responder(struct rxe_qp *qp);\n\n \nint rxe_icrc_init(struct rxe_dev *rxe);\nint rxe_icrc_check(struct sk_buff *skb, struct rxe_pkt_info *pkt);\nvoid rxe_icrc_generate(struct sk_buff *skb, struct rxe_pkt_info *pkt);\n\nvoid rxe_resp_queue_pkt(struct rxe_qp *qp, struct sk_buff *skb);\n\nvoid rxe_comp_queue_pkt(struct rxe_qp *qp, struct sk_buff *skb);\n\nstatic inline unsigned int wr_opcode_mask(int opcode, struct rxe_qp *qp)\n{\n\treturn rxe_wr_opcode_info[opcode].mask[qp->ibqp.qp_type];\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}