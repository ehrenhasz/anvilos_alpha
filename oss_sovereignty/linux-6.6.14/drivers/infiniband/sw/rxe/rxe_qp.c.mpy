{
  "module_name": "rxe_qp.c",
  "hash_id": "ffe223e18d332834804fd6c29c0a5524bc3d9a54a2368ab389148069ec183f84",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rxe/rxe_qp.c",
  "human_readable_source": "\n \n\n#include <linux/skbuff.h>\n#include <linux/delay.h>\n#include <linux/sched.h>\n#include <linux/vmalloc.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"rxe.h\"\n#include \"rxe_loc.h\"\n#include \"rxe_queue.h\"\n#include \"rxe_task.h\"\n\nstatic int rxe_qp_chk_cap(struct rxe_dev *rxe, struct ib_qp_cap *cap,\n\t\t\t  int has_srq)\n{\n\tif (cap->max_send_wr > rxe->attr.max_qp_wr) {\n\t\trxe_dbg_dev(rxe, \"invalid send wr = %u > %d\\n\",\n\t\t\t cap->max_send_wr, rxe->attr.max_qp_wr);\n\t\tgoto err1;\n\t}\n\n\tif (cap->max_send_sge > rxe->attr.max_send_sge) {\n\t\trxe_dbg_dev(rxe, \"invalid send sge = %u > %d\\n\",\n\t\t\t cap->max_send_sge, rxe->attr.max_send_sge);\n\t\tgoto err1;\n\t}\n\n\tif (!has_srq) {\n\t\tif (cap->max_recv_wr > rxe->attr.max_qp_wr) {\n\t\t\trxe_dbg_dev(rxe, \"invalid recv wr = %u > %d\\n\",\n\t\t\t\t cap->max_recv_wr, rxe->attr.max_qp_wr);\n\t\t\tgoto err1;\n\t\t}\n\n\t\tif (cap->max_recv_sge > rxe->attr.max_recv_sge) {\n\t\t\trxe_dbg_dev(rxe, \"invalid recv sge = %u > %d\\n\",\n\t\t\t\t cap->max_recv_sge, rxe->attr.max_recv_sge);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tif (cap->max_inline_data > rxe->max_inline_data) {\n\t\trxe_dbg_dev(rxe, \"invalid max inline data = %u > %d\\n\",\n\t\t\t cap->max_inline_data, rxe->max_inline_data);\n\t\tgoto err1;\n\t}\n\n\treturn 0;\n\nerr1:\n\treturn -EINVAL;\n}\n\nint rxe_qp_chk_init(struct rxe_dev *rxe, struct ib_qp_init_attr *init)\n{\n\tstruct ib_qp_cap *cap = &init->cap;\n\tstruct rxe_port *port;\n\tint port_num = init->port_num;\n\n\tswitch (init->qp_type) {\n\tcase IB_QPT_GSI:\n\tcase IB_QPT_RC:\n\tcase IB_QPT_UC:\n\tcase IB_QPT_UD:\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!init->recv_cq || !init->send_cq) {\n\t\trxe_dbg_dev(rxe, \"missing cq\\n\");\n\t\tgoto err1;\n\t}\n\n\tif (rxe_qp_chk_cap(rxe, cap, !!init->srq))\n\t\tgoto err1;\n\n\tif (init->qp_type == IB_QPT_GSI) {\n\t\tif (!rdma_is_port_valid(&rxe->ib_dev, port_num)) {\n\t\t\trxe_dbg_dev(rxe, \"invalid port = %d\\n\", port_num);\n\t\t\tgoto err1;\n\t\t}\n\n\t\tport = &rxe->port;\n\n\t\tif (init->qp_type == IB_QPT_GSI && port->qp_gsi_index) {\n\t\t\trxe_dbg_dev(rxe, \"GSI QP exists for port %d\\n\", port_num);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\treturn -EINVAL;\n}\n\nstatic int alloc_rd_atomic_resources(struct rxe_qp *qp, unsigned int n)\n{\n\tqp->resp.res_head = 0;\n\tqp->resp.res_tail = 0;\n\tqp->resp.resources = kcalloc(n, sizeof(struct resp_res), GFP_KERNEL);\n\n\tif (!qp->resp.resources)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void free_rd_atomic_resources(struct rxe_qp *qp)\n{\n\tif (qp->resp.resources) {\n\t\tint i;\n\n\t\tfor (i = 0; i < qp->attr.max_dest_rd_atomic; i++) {\n\t\t\tstruct resp_res *res = &qp->resp.resources[i];\n\n\t\t\tfree_rd_atomic_resource(res);\n\t\t}\n\t\tkfree(qp->resp.resources);\n\t\tqp->resp.resources = NULL;\n\t}\n}\n\nvoid free_rd_atomic_resource(struct resp_res *res)\n{\n\tres->type = 0;\n}\n\nstatic void cleanup_rd_atomic_resources(struct rxe_qp *qp)\n{\n\tint i;\n\tstruct resp_res *res;\n\n\tif (qp->resp.resources) {\n\t\tfor (i = 0; i < qp->attr.max_dest_rd_atomic; i++) {\n\t\t\tres = &qp->resp.resources[i];\n\t\t\tfree_rd_atomic_resource(res);\n\t\t}\n\t}\n}\n\nstatic void rxe_qp_init_misc(struct rxe_dev *rxe, struct rxe_qp *qp,\n\t\t\t     struct ib_qp_init_attr *init)\n{\n\tstruct rxe_port *port;\n\tu32 qpn;\n\n\tqp->sq_sig_type\t\t= init->sq_sig_type;\n\tqp->attr.path_mtu\t= 1;\n\tqp->mtu\t\t\t= ib_mtu_enum_to_int(qp->attr.path_mtu);\n\n\tqpn\t\t\t= qp->elem.index;\n\tport\t\t\t= &rxe->port;\n\n\tswitch (init->qp_type) {\n\tcase IB_QPT_GSI:\n\t\tqp->ibqp.qp_num\t\t= 1;\n\t\tport->qp_gsi_index\t= qpn;\n\t\tqp->attr.port_num\t= init->port_num;\n\t\tbreak;\n\n\tdefault:\n\t\tqp->ibqp.qp_num\t\t= qpn;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&qp->state_lock);\n\n\tspin_lock_init(&qp->sq.sq_lock);\n\tspin_lock_init(&qp->rq.producer_lock);\n\tspin_lock_init(&qp->rq.consumer_lock);\n\n\tskb_queue_head_init(&qp->req_pkts);\n\tskb_queue_head_init(&qp->resp_pkts);\n\n\tatomic_set(&qp->ssn, 0);\n\tatomic_set(&qp->skb_out, 0);\n}\n\nstatic int rxe_init_sq(struct rxe_qp *qp, struct ib_qp_init_attr *init,\n\t\t       struct ib_udata *udata,\n\t\t       struct rxe_create_qp_resp __user *uresp)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tint wqe_size;\n\tint err;\n\n\tqp->sq.max_wr = init->cap.max_send_wr;\n\twqe_size = max_t(int, init->cap.max_send_sge * sizeof(struct ib_sge),\n\t\t\t init->cap.max_inline_data);\n\tqp->sq.max_sge = wqe_size / sizeof(struct ib_sge);\n\tqp->sq.max_inline = wqe_size;\n\twqe_size += sizeof(struct rxe_send_wqe);\n\n\tqp->sq.queue = rxe_queue_init(rxe, &qp->sq.max_wr, wqe_size,\n\t\t\t\t      QUEUE_TYPE_FROM_CLIENT);\n\tif (!qp->sq.queue) {\n\t\trxe_err_qp(qp, \"Unable to allocate send queue\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\t \n\terr = do_mmap_info(rxe, uresp ? &uresp->sq_mi : NULL, udata,\n\t\t\t   qp->sq.queue->buf, qp->sq.queue->buf_size,\n\t\t\t   &qp->sq.queue->ip);\n\tif (err) {\n\t\trxe_err_qp(qp, \"do_mmap_info failed, err = %d\", err);\n\t\tgoto err_free;\n\t}\n\n\t \n\tinit->cap.max_send_wr = qp->sq.max_wr;\n\tinit->cap.max_send_sge = qp->sq.max_sge;\n\tinit->cap.max_inline_data = qp->sq.max_inline;\n\n\treturn 0;\n\nerr_free:\n\tvfree(qp->sq.queue->buf);\n\tkfree(qp->sq.queue);\n\tqp->sq.queue = NULL;\nerr_out:\n\treturn err;\n}\n\nstatic int rxe_qp_init_req(struct rxe_dev *rxe, struct rxe_qp *qp,\n\t\t\t   struct ib_qp_init_attr *init, struct ib_udata *udata,\n\t\t\t   struct rxe_create_qp_resp __user *uresp)\n{\n\tint err;\n\n\t \n\tskb_queue_head_init(&qp->req_pkts);\n\n\terr = sock_create_kern(&init_net, AF_INET, SOCK_DGRAM, 0, &qp->sk);\n\tif (err < 0)\n\t\treturn err;\n\tqp->sk->sk->sk_user_data = qp;\n\n\t \n\tqp->src_port = RXE_ROCE_V2_SPORT + (hash_32(qp_num(qp), 14) & 0x3fff);\n\n\terr = rxe_init_sq(qp, init, udata, uresp);\n\tif (err)\n\t\treturn err;\n\n\tqp->req.wqe_index = queue_get_producer(qp->sq.queue,\n\t\t\t\t\t       QUEUE_TYPE_FROM_CLIENT);\n\n\tqp->req.opcode\t\t= -1;\n\tqp->comp.opcode\t\t= -1;\n\n\trxe_init_task(&qp->req.task, qp, rxe_requester);\n\trxe_init_task(&qp->comp.task, qp, rxe_completer);\n\n\tqp->qp_timeout_jiffies = 0;  \n\tif (init->qp_type == IB_QPT_RC) {\n\t\ttimer_setup(&qp->rnr_nak_timer, rnr_nak_timer, 0);\n\t\ttimer_setup(&qp->retrans_timer, retransmit_timer, 0);\n\t}\n\treturn 0;\n}\n\nstatic int rxe_init_rq(struct rxe_qp *qp, struct ib_qp_init_attr *init,\n\t\t       struct ib_udata *udata,\n\t\t       struct rxe_create_qp_resp __user *uresp)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tint wqe_size;\n\tint err;\n\n\tqp->rq.max_wr = init->cap.max_recv_wr;\n\tqp->rq.max_sge = init->cap.max_recv_sge;\n\twqe_size = sizeof(struct rxe_recv_wqe) +\n\t\t\t\tqp->rq.max_sge*sizeof(struct ib_sge);\n\n\tqp->rq.queue = rxe_queue_init(rxe, &qp->rq.max_wr, wqe_size,\n\t\t\t\t      QUEUE_TYPE_FROM_CLIENT);\n\tif (!qp->rq.queue) {\n\t\trxe_err_qp(qp, \"Unable to allocate recv queue\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\t \n\terr = do_mmap_info(rxe, uresp ? &uresp->rq_mi : NULL, udata,\n\t\t\t   qp->rq.queue->buf, qp->rq.queue->buf_size,\n\t\t\t   &qp->rq.queue->ip);\n\tif (err) {\n\t\trxe_err_qp(qp, \"do_mmap_info failed, err = %d\", err);\n\t\tgoto err_free;\n\t}\n\n\t \n\tinit->cap.max_recv_wr = qp->rq.max_wr;\n\n\treturn 0;\n\nerr_free:\n\tvfree(qp->rq.queue->buf);\n\tkfree(qp->rq.queue);\n\tqp->rq.queue = NULL;\nerr_out:\n\treturn err;\n}\n\nstatic int rxe_qp_init_resp(struct rxe_dev *rxe, struct rxe_qp *qp,\n\t\t\t    struct ib_qp_init_attr *init,\n\t\t\t    struct ib_udata *udata,\n\t\t\t    struct rxe_create_qp_resp __user *uresp)\n{\n\tint err;\n\n\t \n\tskb_queue_head_init(&qp->resp_pkts);\n\n\tif (!qp->srq) {\n\t\terr = rxe_init_rq(qp, init, udata, uresp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\trxe_init_task(&qp->resp.task, qp, rxe_responder);\n\n\tqp->resp.opcode\t\t= OPCODE_NONE;\n\tqp->resp.msn\t\t= 0;\n\n\treturn 0;\n}\n\n \nint rxe_qp_from_init(struct rxe_dev *rxe, struct rxe_qp *qp, struct rxe_pd *pd,\n\t\t     struct ib_qp_init_attr *init,\n\t\t     struct rxe_create_qp_resp __user *uresp,\n\t\t     struct ib_pd *ibpd,\n\t\t     struct ib_udata *udata)\n{\n\tint err;\n\tstruct rxe_cq *rcq = to_rcq(init->recv_cq);\n\tstruct rxe_cq *scq = to_rcq(init->send_cq);\n\tstruct rxe_srq *srq = init->srq ? to_rsrq(init->srq) : NULL;\n\tunsigned long flags;\n\n\trxe_get(pd);\n\trxe_get(rcq);\n\trxe_get(scq);\n\tif (srq)\n\t\trxe_get(srq);\n\n\tqp->pd = pd;\n\tqp->rcq = rcq;\n\tqp->scq = scq;\n\tqp->srq = srq;\n\n\tatomic_inc(&rcq->num_wq);\n\tatomic_inc(&scq->num_wq);\n\n\trxe_qp_init_misc(rxe, qp, init);\n\n\terr = rxe_qp_init_req(rxe, qp, init, udata, uresp);\n\tif (err)\n\t\tgoto err1;\n\n\terr = rxe_qp_init_resp(rxe, qp, init, udata, uresp);\n\tif (err)\n\t\tgoto err2;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tqp->attr.qp_state = IB_QPS_RESET;\n\tqp->valid = 1;\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\treturn 0;\n\nerr2:\n\trxe_queue_cleanup(qp->sq.queue);\n\tqp->sq.queue = NULL;\nerr1:\n\tatomic_dec(&rcq->num_wq);\n\tatomic_dec(&scq->num_wq);\n\n\tqp->pd = NULL;\n\tqp->rcq = NULL;\n\tqp->scq = NULL;\n\tqp->srq = NULL;\n\n\tif (srq)\n\t\trxe_put(srq);\n\trxe_put(scq);\n\trxe_put(rcq);\n\trxe_put(pd);\n\n\treturn err;\n}\n\n \nint rxe_qp_to_init(struct rxe_qp *qp, struct ib_qp_init_attr *init)\n{\n\tinit->event_handler\t\t= qp->ibqp.event_handler;\n\tinit->qp_context\t\t= qp->ibqp.qp_context;\n\tinit->send_cq\t\t\t= qp->ibqp.send_cq;\n\tinit->recv_cq\t\t\t= qp->ibqp.recv_cq;\n\tinit->srq\t\t\t= qp->ibqp.srq;\n\n\tinit->cap.max_send_wr\t\t= qp->sq.max_wr;\n\tinit->cap.max_send_sge\t\t= qp->sq.max_sge;\n\tinit->cap.max_inline_data\t= qp->sq.max_inline;\n\n\tif (!qp->srq) {\n\t\tinit->cap.max_recv_wr\t\t= qp->rq.max_wr;\n\t\tinit->cap.max_recv_sge\t\t= qp->rq.max_sge;\n\t}\n\n\tinit->sq_sig_type\t\t= qp->sq_sig_type;\n\n\tinit->qp_type\t\t\t= qp->ibqp.qp_type;\n\tinit->port_num\t\t\t= 1;\n\n\treturn 0;\n}\n\nint rxe_qp_chk_attr(struct rxe_dev *rxe, struct rxe_qp *qp,\n\t\t    struct ib_qp_attr *attr, int mask)\n{\n\tif (mask & IB_QP_PORT) {\n\t\tif (!rdma_is_port_valid(&rxe->ib_dev, attr->port_num)) {\n\t\t\trxe_dbg_qp(qp, \"invalid port %d\\n\", attr->port_num);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_CAP && rxe_qp_chk_cap(rxe, &attr->cap, !!qp->srq))\n\t\tgoto err1;\n\n\tif (mask & IB_QP_ACCESS_FLAGS) {\n\t\tif (!(qp_type(qp) == IB_QPT_RC || qp_type(qp) == IB_QPT_UC))\n\t\t\tgoto err1;\n\t\tif (attr->qp_access_flags & ~RXE_ACCESS_SUPPORTED_QP)\n\t\t\tgoto err1;\n\t}\n\n\tif (mask & IB_QP_AV && rxe_av_chk_attr(qp, &attr->ah_attr))\n\t\tgoto err1;\n\n\tif (mask & IB_QP_ALT_PATH) {\n\t\tif (rxe_av_chk_attr(qp, &attr->alt_ah_attr))\n\t\t\tgoto err1;\n\t\tif (!rdma_is_port_valid(&rxe->ib_dev, attr->alt_port_num))  {\n\t\t\trxe_dbg_qp(qp, \"invalid alt port %d\\n\", attr->alt_port_num);\n\t\t\tgoto err1;\n\t\t}\n\t\tif (attr->alt_timeout > 31) {\n\t\t\trxe_dbg_qp(qp, \"invalid alt timeout %d > 31\\n\",\n\t\t\t\t attr->alt_timeout);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_PATH_MTU) {\n\t\tstruct rxe_port *port = &rxe->port;\n\n\t\tenum ib_mtu max_mtu = port->attr.max_mtu;\n\t\tenum ib_mtu mtu = attr->path_mtu;\n\n\t\tif (mtu > max_mtu) {\n\t\t\trxe_dbg_qp(qp, \"invalid mtu (%d) > (%d)\\n\",\n\t\t\t\t ib_mtu_enum_to_int(mtu),\n\t\t\t\t ib_mtu_enum_to_int(max_mtu));\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_MAX_QP_RD_ATOMIC) {\n\t\tif (attr->max_rd_atomic > rxe->attr.max_qp_rd_atom) {\n\t\t\trxe_dbg_qp(qp, \"invalid max_rd_atomic %d > %d\\n\",\n\t\t\t\t attr->max_rd_atomic,\n\t\t\t\t rxe->attr.max_qp_rd_atom);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_TIMEOUT) {\n\t\tif (attr->timeout > 31) {\n\t\t\trxe_dbg_qp(qp, \"invalid timeout %d > 31\\n\",\n\t\t\t\t\tattr->timeout);\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\treturn -EINVAL;\n}\n\n \nstatic void rxe_qp_reset(struct rxe_qp *qp)\n{\n\t \n\trxe_disable_task(&qp->resp.task);\n\trxe_disable_task(&qp->comp.task);\n\trxe_disable_task(&qp->req.task);\n\n\t \n\trxe_requester(qp);\n\trxe_completer(qp);\n\trxe_responder(qp);\n\n\tif (qp->rq.queue)\n\t\trxe_queue_reset(qp->rq.queue);\n\tif (qp->sq.queue)\n\t\trxe_queue_reset(qp->sq.queue);\n\n\t \n\tatomic_set(&qp->ssn, 0);\n\tqp->req.opcode = -1;\n\tqp->req.need_retry = 0;\n\tqp->req.wait_for_rnr_timer = 0;\n\tqp->req.noack_pkts = 0;\n\tqp->resp.msn = 0;\n\tqp->resp.opcode = -1;\n\tqp->resp.drop_msg = 0;\n\tqp->resp.goto_error = 0;\n\tqp->resp.sent_psn_nak = 0;\n\n\tif (qp->resp.mr) {\n\t\trxe_put(qp->resp.mr);\n\t\tqp->resp.mr = NULL;\n\t}\n\n\tcleanup_rd_atomic_resources(qp);\n\n\t \n\trxe_enable_task(&qp->resp.task);\n\trxe_enable_task(&qp->comp.task);\n\trxe_enable_task(&qp->req.task);\n}\n\n \nvoid rxe_qp_error(struct rxe_qp *qp)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tqp->attr.qp_state = IB_QPS_ERR;\n\n\t \n\trxe_sched_task(&qp->resp.task);\n\trxe_sched_task(&qp->comp.task);\n\trxe_sched_task(&qp->req.task);\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n}\n\nstatic void rxe_qp_sqd(struct rxe_qp *qp, struct ib_qp_attr *attr,\n\t\t       int mask)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tqp->attr.sq_draining = 1;\n\trxe_sched_task(&qp->comp.task);\n\trxe_sched_task(&qp->req.task);\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n}\n\n \nstatic int __qp_chk_state(struct rxe_qp *qp, struct ib_qp_attr *attr,\n\t\t\t    int mask)\n{\n\tenum ib_qp_state cur_state;\n\tenum ib_qp_state new_state;\n\n\tcur_state = (mask & IB_QP_CUR_STATE) ?\n\t\t\t\tattr->cur_qp_state : qp->attr.qp_state;\n\tnew_state = (mask & IB_QP_STATE) ?\n\t\t\t\tattr->qp_state : cur_state;\n\n\tif (!ib_modify_qp_is_ok(cur_state, new_state, qp_type(qp), mask))\n\t\treturn -EINVAL;\n\n\tif (mask & IB_QP_STATE && cur_state == IB_QPS_SQD) {\n\t\tif (qp->attr.sq_draining && new_state != IB_QPS_ERR)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const char *const qps2str[] = {\n\t[IB_QPS_RESET]\t= \"RESET\",\n\t[IB_QPS_INIT]\t= \"INIT\",\n\t[IB_QPS_RTR]\t= \"RTR\",\n\t[IB_QPS_RTS]\t= \"RTS\",\n\t[IB_QPS_SQD]\t= \"SQD\",\n\t[IB_QPS_SQE]\t= \"SQE\",\n\t[IB_QPS_ERR]\t= \"ERR\",\n};\n\n \nint rxe_qp_from_attr(struct rxe_qp *qp, struct ib_qp_attr *attr, int mask,\n\t\t     struct ib_udata *udata)\n{\n\tint err;\n\n\tif (mask & IB_QP_CUR_STATE)\n\t\tqp->attr.cur_qp_state = attr->qp_state;\n\n\tif (mask & IB_QP_STATE) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&qp->state_lock, flags);\n\t\terr = __qp_chk_state(qp, attr, mask);\n\t\tif (!err) {\n\t\t\tqp->attr.qp_state = attr->qp_state;\n\t\t\trxe_dbg_qp(qp, \"state -> %s\\n\",\n\t\t\t\t\tqps2str[attr->qp_state]);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tswitch (attr->qp_state) {\n\t\tcase IB_QPS_RESET:\n\t\t\trxe_qp_reset(qp);\n\t\t\tbreak;\n\t\tcase IB_QPS_SQD:\n\t\t\trxe_qp_sqd(qp, attr, mask);\n\t\t\tbreak;\n\t\tcase IB_QPS_ERR:\n\t\t\trxe_qp_error(qp);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_MAX_QP_RD_ATOMIC) {\n\t\tint max_rd_atomic = attr->max_rd_atomic ?\n\t\t\troundup_pow_of_two(attr->max_rd_atomic) : 0;\n\n\t\tqp->attr.max_rd_atomic = max_rd_atomic;\n\t\tatomic_set(&qp->req.rd_atomic, max_rd_atomic);\n\t}\n\n\tif (mask & IB_QP_MAX_DEST_RD_ATOMIC) {\n\t\tint max_dest_rd_atomic = attr->max_dest_rd_atomic ?\n\t\t\troundup_pow_of_two(attr->max_dest_rd_atomic) : 0;\n\n\t\tqp->attr.max_dest_rd_atomic = max_dest_rd_atomic;\n\n\t\tfree_rd_atomic_resources(qp);\n\n\t\terr = alloc_rd_atomic_resources(qp, max_dest_rd_atomic);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (mask & IB_QP_EN_SQD_ASYNC_NOTIFY)\n\t\tqp->attr.en_sqd_async_notify = attr->en_sqd_async_notify;\n\n\tif (mask & IB_QP_ACCESS_FLAGS)\n\t\tqp->attr.qp_access_flags = attr->qp_access_flags;\n\n\tif (mask & IB_QP_PKEY_INDEX)\n\t\tqp->attr.pkey_index = attr->pkey_index;\n\n\tif (mask & IB_QP_PORT)\n\t\tqp->attr.port_num = attr->port_num;\n\n\tif (mask & IB_QP_QKEY)\n\t\tqp->attr.qkey = attr->qkey;\n\n\tif (mask & IB_QP_AV)\n\t\trxe_init_av(&attr->ah_attr, &qp->pri_av);\n\n\tif (mask & IB_QP_ALT_PATH) {\n\t\trxe_init_av(&attr->alt_ah_attr, &qp->alt_av);\n\t\tqp->attr.alt_port_num = attr->alt_port_num;\n\t\tqp->attr.alt_pkey_index = attr->alt_pkey_index;\n\t\tqp->attr.alt_timeout = attr->alt_timeout;\n\t}\n\n\tif (mask & IB_QP_PATH_MTU) {\n\t\tqp->attr.path_mtu = attr->path_mtu;\n\t\tqp->mtu = ib_mtu_enum_to_int(attr->path_mtu);\n\t}\n\n\tif (mask & IB_QP_TIMEOUT) {\n\t\tqp->attr.timeout = attr->timeout;\n\t\tif (attr->timeout == 0) {\n\t\t\tqp->qp_timeout_jiffies = 0;\n\t\t} else {\n\t\t\t \n\t\t\tint j = nsecs_to_jiffies(4096ULL << attr->timeout);\n\n\t\t\tqp->qp_timeout_jiffies = j ? j : 1;\n\t\t}\n\t}\n\n\tif (mask & IB_QP_RETRY_CNT) {\n\t\tqp->attr.retry_cnt = attr->retry_cnt;\n\t\tqp->comp.retry_cnt = attr->retry_cnt;\n\t\trxe_dbg_qp(qp, \"set retry count = %d\\n\", attr->retry_cnt);\n\t}\n\n\tif (mask & IB_QP_RNR_RETRY) {\n\t\tqp->attr.rnr_retry = attr->rnr_retry;\n\t\tqp->comp.rnr_retry = attr->rnr_retry;\n\t\trxe_dbg_qp(qp, \"set rnr retry count = %d\\n\", attr->rnr_retry);\n\t}\n\n\tif (mask & IB_QP_RQ_PSN) {\n\t\tqp->attr.rq_psn = (attr->rq_psn & BTH_PSN_MASK);\n\t\tqp->resp.psn = qp->attr.rq_psn;\n\t\trxe_dbg_qp(qp, \"set resp psn = 0x%x\\n\", qp->resp.psn);\n\t}\n\n\tif (mask & IB_QP_MIN_RNR_TIMER) {\n\t\tqp->attr.min_rnr_timer = attr->min_rnr_timer;\n\t\trxe_dbg_qp(qp, \"set min rnr timer = 0x%x\\n\",\n\t\t\t attr->min_rnr_timer);\n\t}\n\n\tif (mask & IB_QP_SQ_PSN) {\n\t\tqp->attr.sq_psn = (attr->sq_psn & BTH_PSN_MASK);\n\t\tqp->req.psn = qp->attr.sq_psn;\n\t\tqp->comp.psn = qp->attr.sq_psn;\n\t\trxe_dbg_qp(qp, \"set req psn = 0x%x\\n\", qp->req.psn);\n\t}\n\n\tif (mask & IB_QP_PATH_MIG_STATE)\n\t\tqp->attr.path_mig_state = attr->path_mig_state;\n\n\tif (mask & IB_QP_DEST_QPN)\n\t\tqp->attr.dest_qp_num = attr->dest_qp_num;\n\n\treturn 0;\n}\n\n \nint rxe_qp_to_attr(struct rxe_qp *qp, struct ib_qp_attr *attr, int mask)\n{\n\tunsigned long flags;\n\n\t*attr = qp->attr;\n\n\tattr->rq_psn\t\t\t\t= qp->resp.psn;\n\tattr->sq_psn\t\t\t\t= qp->req.psn;\n\n\tattr->cap.max_send_wr\t\t\t= qp->sq.max_wr;\n\tattr->cap.max_send_sge\t\t\t= qp->sq.max_sge;\n\tattr->cap.max_inline_data\t\t= qp->sq.max_inline;\n\n\tif (!qp->srq) {\n\t\tattr->cap.max_recv_wr\t\t= qp->rq.max_wr;\n\t\tattr->cap.max_recv_sge\t\t= qp->rq.max_sge;\n\t}\n\n\trxe_av_to_attr(&qp->pri_av, &attr->ah_attr);\n\trxe_av_to_attr(&qp->alt_av, &attr->alt_ah_attr);\n\n\t \n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (qp->attr.sq_draining) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\tcond_resched();\n\t} else {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t}\n\n\treturn 0;\n}\n\nint rxe_qp_chk_destroy(struct rxe_qp *qp)\n{\n\t \n\tif (atomic_read(&qp->mcg_num)) {\n\t\trxe_dbg_qp(qp, \"Attempt to destroy while attached to multicast group\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void rxe_qp_do_cleanup(struct work_struct *work)\n{\n\tstruct rxe_qp *qp = container_of(work, typeof(*qp), cleanup_work.work);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tqp->valid = 0;\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\tqp->qp_timeout_jiffies = 0;\n\n\tif (qp_type(qp) == IB_QPT_RC) {\n\t\tdel_timer_sync(&qp->retrans_timer);\n\t\tdel_timer_sync(&qp->rnr_nak_timer);\n\t}\n\n\tif (qp->resp.task.func)\n\t\trxe_cleanup_task(&qp->resp.task);\n\n\tif (qp->req.task.func)\n\t\trxe_cleanup_task(&qp->req.task);\n\n\tif (qp->comp.task.func)\n\t\trxe_cleanup_task(&qp->comp.task);\n\n\t \n\trxe_requester(qp);\n\trxe_completer(qp);\n\trxe_responder(qp);\n\n\tif (qp->sq.queue)\n\t\trxe_queue_cleanup(qp->sq.queue);\n\n\tif (qp->srq)\n\t\trxe_put(qp->srq);\n\n\tif (qp->rq.queue)\n\t\trxe_queue_cleanup(qp->rq.queue);\n\n\tif (qp->scq) {\n\t\tatomic_dec(&qp->scq->num_wq);\n\t\trxe_put(qp->scq);\n\t}\n\n\tif (qp->rcq) {\n\t\tatomic_dec(&qp->rcq->num_wq);\n\t\trxe_put(qp->rcq);\n\t}\n\n\tif (qp->pd)\n\t\trxe_put(qp->pd);\n\n\tif (qp->resp.mr)\n\t\trxe_put(qp->resp.mr);\n\n\tfree_rd_atomic_resources(qp);\n\n\tif (qp->sk) {\n\t\tif (qp_type(qp) == IB_QPT_RC)\n\t\t\tsk_dst_reset(qp->sk->sk);\n\n\t\tkernel_sock_shutdown(qp->sk, SHUT_RDWR);\n\t\tsock_release(qp->sk);\n\t}\n}\n\n \nvoid rxe_qp_cleanup(struct rxe_pool_elem *elem)\n{\n\tstruct rxe_qp *qp = container_of(elem, typeof(*qp), elem);\n\n\texecute_in_process_context(rxe_qp_do_cleanup, &qp->cleanup_work);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}