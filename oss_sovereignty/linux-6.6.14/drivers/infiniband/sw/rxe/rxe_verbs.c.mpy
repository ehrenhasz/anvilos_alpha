{
  "module_name": "rxe_verbs.c",
  "hash_id": "d087518fe9261ef9ff63c12991d148d02b596bb580ecd3dd22934c4ef0f62e73",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rxe/rxe_verbs.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <net/addrconf.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"rxe.h\"\n#include \"rxe_queue.h\"\n#include \"rxe_hw_counters.h\"\n\nstatic int post_one_recv(struct rxe_rq *rq, const struct ib_recv_wr *ibwr);\n\n \nstatic int rxe_query_device(struct ib_device *ibdev,\n\t\t\t    struct ib_device_attr *attr,\n\t\t\t    struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tint err;\n\n\tif (udata->inlen || udata->outlen) {\n\t\trxe_dbg_dev(rxe, \"malformed udata\");\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tmemcpy(attr, &rxe->attr, sizeof(*attr));\n\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_query_port(struct ib_device *ibdev,\n\t\t\t  u32 port_num, struct ib_port_attr *attr)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tint err, ret;\n\n\tif (port_num != 1) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_dev(rxe, \"bad port_num = %d\", port_num);\n\t\tgoto err_out;\n\t}\n\n\tmemcpy(attr, &rxe->port.attr, sizeof(*attr));\n\n\tmutex_lock(&rxe->usdev_lock);\n\tret = ib_get_eth_speed(ibdev, port_num, &attr->active_speed,\n\t\t\t       &attr->active_width);\n\n\tif (attr->state == IB_PORT_ACTIVE)\n\t\tattr->phys_state = IB_PORT_PHYS_STATE_LINK_UP;\n\telse if (dev_get_flags(rxe->ndev) & IFF_UP)\n\t\tattr->phys_state = IB_PORT_PHYS_STATE_POLLING;\n\telse\n\t\tattr->phys_state = IB_PORT_PHYS_STATE_DISABLED;\n\n\tmutex_unlock(&rxe->usdev_lock);\n\n\treturn ret;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_query_pkey(struct ib_device *ibdev,\n\t\t\t  u32 port_num, u16 index, u16 *pkey)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tint err;\n\n\tif (index != 0) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_dev(rxe, \"bad pkey index = %d\", index);\n\t\tgoto err_out;\n\t}\n\n\t*pkey = IB_DEFAULT_PKEY_FULL;\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_modify_device(struct ib_device *ibdev,\n\t\t\t     int mask, struct ib_device_modify *attr)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tint err;\n\n\tif (mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\n\t\t     IB_DEVICE_MODIFY_NODE_DESC)) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_dev(rxe, \"unsupported mask = 0x%x\", mask);\n\t\tgoto err_out;\n\t}\n\n\tif (mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID)\n\t\trxe->attr.sys_image_guid = cpu_to_be64(attr->sys_image_guid);\n\n\tif (mask & IB_DEVICE_MODIFY_NODE_DESC) {\n\t\tmemcpy(rxe->ib_dev.node_desc,\n\t\t       attr->node_desc, sizeof(rxe->ib_dev.node_desc));\n\t}\n\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_modify_port(struct ib_device *ibdev, u32 port_num,\n\t\t\t   int mask, struct ib_port_modify *attr)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tstruct rxe_port *port;\n\tint err;\n\n\tif (port_num != 1) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_dev(rxe, \"bad port_num = %d\", port_num);\n\t\tgoto err_out;\n\t}\n\n\t\n\tif (mask & ~(IB_PORT_RESET_QKEY_CNTR)) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_dev(rxe, \"unsupported mask = 0x%x\", mask);\n\t\tgoto err_out;\n\t}\n\n\tport = &rxe->port;\n\tport->attr.port_cap_flags |= attr->set_port_cap_mask;\n\tport->attr.port_cap_flags &= ~attr->clr_port_cap_mask;\n\n\tif (mask & IB_PORT_RESET_QKEY_CNTR)\n\t\tport->attr.qkey_viol_cntr = 0;\n\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic enum rdma_link_layer rxe_get_link_layer(struct ib_device *ibdev,\n\t\t\t\t\t       u32 port_num)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tint err;\n\n\tif (port_num != 1) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_dev(rxe, \"bad port_num = %d\", port_num);\n\t\tgoto err_out;\n\t}\n\n\treturn IB_LINK_LAYER_ETHERNET;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_port_immutable(struct ib_device *ibdev, u32 port_num,\n\t\t\t      struct ib_port_immutable *immutable)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibdev);\n\tstruct ib_port_attr attr = {};\n\tint err;\n\n\tif (port_num != 1) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_dev(rxe, \"bad port_num = %d\", port_num);\n\t\tgoto err_out;\n\t}\n\n\terr = ib_query_port(ibdev, port_num, &attr);\n\tif (err)\n\t\tgoto err_out;\n\n\timmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;\n\timmutable->pkey_tbl_len = attr.pkey_tbl_len;\n\timmutable->gid_tbl_len = attr.gid_tbl_len;\n\timmutable->max_mad_size = IB_MGMT_MAD_SIZE;\n\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\n \nstatic int rxe_alloc_ucontext(struct ib_ucontext *ibuc, struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibuc->device);\n\tstruct rxe_ucontext *uc = to_ruc(ibuc);\n\tint err;\n\n\terr = rxe_add_to_pool(&rxe->uc_pool, uc);\n\tif (err)\n\t\trxe_err_dev(rxe, \"unable to create uc\");\n\n\treturn err;\n}\n\nstatic void rxe_dealloc_ucontext(struct ib_ucontext *ibuc)\n{\n\tstruct rxe_ucontext *uc = to_ruc(ibuc);\n\tint err;\n\n\terr = rxe_cleanup(uc);\n\tif (err)\n\t\trxe_err_uc(uc, \"cleanup failed, err = %d\", err);\n}\n\n \nstatic int rxe_alloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibpd->device);\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\tint err;\n\n\terr = rxe_add_to_pool(&rxe->pd_pool, pd);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to alloc pd\");\n\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_dealloc_pd(struct ib_pd *ibpd, struct ib_udata *udata)\n{\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\tint err;\n\n\terr = rxe_cleanup(pd);\n\tif (err)\n\t\trxe_err_pd(pd, \"cleanup failed, err = %d\", err);\n\n\treturn 0;\n}\n\n \nstatic int rxe_create_ah(struct ib_ah *ibah,\n\t\t\t struct rdma_ah_init_attr *init_attr,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibah->device);\n\tstruct rxe_ah *ah = to_rah(ibah);\n\tstruct rxe_create_ah_resp __user *uresp = NULL;\n\tint err, cleanup_err;\n\n\tif (udata) {\n\t\t \n\t\tif (udata->outlen >= sizeof(*uresp))\n\t\t\turesp = udata->outbuf;\n\t\tah->is_user = true;\n\t} else {\n\t\tah->is_user = false;\n\t}\n\n\terr = rxe_add_to_pool_ah(&rxe->ah_pool, ah,\n\t\t\tinit_attr->flags & RDMA_CREATE_AH_SLEEPABLE);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to create ah\");\n\t\tgoto err_out;\n\t}\n\n\t \n\tah->ah_num = ah->elem.index;\n\n\terr = rxe_ah_chk_attr(ah, init_attr->ah_attr);\n\tif (err) {\n\t\trxe_dbg_ah(ah, \"bad attr\");\n\t\tgoto err_cleanup;\n\t}\n\n\tif (uresp) {\n\t\t \n\t\terr = copy_to_user(&uresp->ah_num, &ah->ah_num,\n\t\t\t\t\t sizeof(uresp->ah_num));\n\t\tif (err) {\n\t\t\terr = -EFAULT;\n\t\t\trxe_dbg_ah(ah, \"unable to copy to user\");\n\t\t\tgoto err_cleanup;\n\t\t}\n\t} else if (ah->is_user) {\n\t\t \n\t\tah->ah_num = 0;\n\t}\n\n\trxe_init_av(init_attr->ah_attr, &ah->av);\n\trxe_finalize(ah);\n\n\treturn 0;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(ah);\n\tif (cleanup_err)\n\t\trxe_err_ah(ah, \"cleanup failed, err = %d\", cleanup_err);\nerr_out:\n\trxe_err_ah(ah, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_modify_ah(struct ib_ah *ibah, struct rdma_ah_attr *attr)\n{\n\tstruct rxe_ah *ah = to_rah(ibah);\n\tint err;\n\n\terr = rxe_ah_chk_attr(ah, attr);\n\tif (err) {\n\t\trxe_dbg_ah(ah, \"bad attr\");\n\t\tgoto err_out;\n\t}\n\n\trxe_init_av(attr, &ah->av);\n\n\treturn 0;\n\nerr_out:\n\trxe_err_ah(ah, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *attr)\n{\n\tstruct rxe_ah *ah = to_rah(ibah);\n\n\tmemset(attr, 0, sizeof(*attr));\n\tattr->type = ibah->type;\n\trxe_av_to_attr(&ah->av, attr);\n\n\treturn 0;\n}\n\nstatic int rxe_destroy_ah(struct ib_ah *ibah, u32 flags)\n{\n\tstruct rxe_ah *ah = to_rah(ibah);\n\tint err;\n\n\terr = rxe_cleanup_ah(ah, flags & RDMA_DESTROY_AH_SLEEPABLE);\n\tif (err)\n\t\trxe_err_ah(ah, \"cleanup failed, err = %d\", err);\n\n\treturn 0;\n}\n\n \nstatic int rxe_create_srq(struct ib_srq *ibsrq, struct ib_srq_init_attr *init,\n\t\t\t  struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibsrq->device);\n\tstruct rxe_pd *pd = to_rpd(ibsrq->pd);\n\tstruct rxe_srq *srq = to_rsrq(ibsrq);\n\tstruct rxe_create_srq_resp __user *uresp = NULL;\n\tint err, cleanup_err;\n\n\tif (udata) {\n\t\tif (udata->outlen < sizeof(*uresp)) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_err_dev(rxe, \"malformed udata\");\n\t\t\tgoto err_out;\n\t\t}\n\t\turesp = udata->outbuf;\n\t}\n\n\tif (init->srq_type != IB_SRQT_BASIC) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_dev(rxe, \"srq type = %d, not supported\",\n\t\t\t\tinit->srq_type);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_srq_chk_init(rxe, init);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"invalid init attributes\");\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_add_to_pool(&rxe->srq_pool, srq);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to create srq, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\trxe_get(pd);\n\tsrq->pd = pd;\n\n\terr = rxe_srq_from_init(rxe, srq, init, udata, uresp);\n\tif (err) {\n\t\trxe_dbg_srq(srq, \"create srq failed, err = %d\", err);\n\t\tgoto err_cleanup;\n\t}\n\n\treturn 0;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(srq);\n\tif (cleanup_err)\n\t\trxe_err_srq(srq, \"cleanup failed, err = %d\", cleanup_err);\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,\n\t\t\t  enum ib_srq_attr_mask mask,\n\t\t\t  struct ib_udata *udata)\n{\n\tstruct rxe_srq *srq = to_rsrq(ibsrq);\n\tstruct rxe_dev *rxe = to_rdev(ibsrq->device);\n\tstruct rxe_modify_srq_cmd cmd = {};\n\tint err;\n\n\tif (udata) {\n\t\tif (udata->inlen < sizeof(cmd)) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_dbg_srq(srq, \"malformed udata\");\n\t\t\tgoto err_out;\n\t\t}\n\n\t\terr = ib_copy_from_udata(&cmd, udata, sizeof(cmd));\n\t\tif (err) {\n\t\t\terr = -EFAULT;\n\t\t\trxe_dbg_srq(srq, \"unable to read udata\");\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = rxe_srq_chk_attr(rxe, srq, attr, mask);\n\tif (err) {\n\t\trxe_dbg_srq(srq, \"bad init attributes\");\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_srq_from_attr(rxe, srq, attr, mask, &cmd, udata);\n\tif (err) {\n\t\trxe_dbg_srq(srq, \"bad attr\");\n\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\trxe_err_srq(srq, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr)\n{\n\tstruct rxe_srq *srq = to_rsrq(ibsrq);\n\tint err;\n\n\tif (srq->error) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_srq(srq, \"srq in error state\");\n\t\tgoto err_out;\n\t}\n\n\tattr->max_wr = srq->rq.queue->buf->index_mask;\n\tattr->max_sge = srq->rq.max_sge;\n\tattr->srq_limit = srq->limit;\n\treturn 0;\n\nerr_out:\n\trxe_err_srq(srq, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,\n\t\t\t     const struct ib_recv_wr **bad_wr)\n{\n\tint err = 0;\n\tstruct rxe_srq *srq = to_rsrq(ibsrq);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&srq->rq.producer_lock, flags);\n\n\twhile (wr) {\n\t\terr = post_one_recv(&srq->rq, wr);\n\t\tif (unlikely(err))\n\t\t\tbreak;\n\t\twr = wr->next;\n\t}\n\n\tspin_unlock_irqrestore(&srq->rq.producer_lock, flags);\n\n\tif (err) {\n\t\t*bad_wr = wr;\n\t\trxe_err_srq(srq, \"returned err = %d\", err);\n\t}\n\n\treturn err;\n}\n\nstatic int rxe_destroy_srq(struct ib_srq *ibsrq, struct ib_udata *udata)\n{\n\tstruct rxe_srq *srq = to_rsrq(ibsrq);\n\tint err;\n\n\terr = rxe_cleanup(srq);\n\tif (err)\n\t\trxe_err_srq(srq, \"cleanup failed, err = %d\", err);\n\n\treturn 0;\n}\n\n \nstatic int rxe_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *init,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibqp->device);\n\tstruct rxe_pd *pd = to_rpd(ibqp->pd);\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\tstruct rxe_create_qp_resp __user *uresp = NULL;\n\tint err, cleanup_err;\n\n\tif (udata) {\n\t\tif (udata->inlen) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_dbg_dev(rxe, \"malformed udata, err = %d\", err);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (udata->outlen < sizeof(*uresp)) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_dbg_dev(rxe, \"malformed udata, err = %d\", err);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tqp->is_user = true;\n\t\turesp = udata->outbuf;\n\t} else {\n\t\tqp->is_user = false;\n\t}\n\n\tif (init->create_flags) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_dev(rxe, \"unsupported create_flags, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_qp_chk_init(rxe, init);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"bad init attr, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_add_to_pool(&rxe->qp_pool, qp);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to create qp, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_qp_from_init(rxe, qp, pd, init, uresp, ibqp->pd, udata);\n\tif (err) {\n\t\trxe_dbg_qp(qp, \"create qp failed, err = %d\", err);\n\t\tgoto err_cleanup;\n\t}\n\n\trxe_finalize(qp);\n\treturn 0;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(qp);\n\tif (cleanup_err)\n\t\trxe_err_qp(qp, \"cleanup failed, err = %d\", cleanup_err);\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t\t int mask, struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibqp->device);\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\tint err;\n\n\tif (mask & ~IB_QP_ATTR_STANDARD_BITS) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_qp(qp, \"unsupported mask = 0x%x, err = %d\",\n\t\t\t   mask, err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_qp_chk_attr(rxe, qp, attr, mask);\n\tif (err) {\n\t\trxe_dbg_qp(qp, \"bad mask/attr, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_qp_from_attr(qp, attr, mask, udata);\n\tif (err) {\n\t\trxe_dbg_qp(qp, \"modify qp failed, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\tif ((mask & IB_QP_AV) && (attr->ah_attr.ah_flags & IB_AH_GRH))\n\t\tqp->src_port = rdma_get_udp_sport(attr->ah_attr.grh.flow_label,\n\t\t\t\t\t\t  qp->ibqp.qp_num,\n\t\t\t\t\t\t  qp->attr.dest_qp_num);\n\n\treturn 0;\n\nerr_out:\n\trxe_err_qp(qp, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\n\t\t\tint mask, struct ib_qp_init_attr *init)\n{\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\n\trxe_qp_to_init(qp, init);\n\trxe_qp_to_attr(qp, attr, mask);\n\n\treturn 0;\n}\n\nstatic int rxe_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)\n{\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\tint err;\n\n\terr = rxe_qp_chk_destroy(qp);\n\tif (err) {\n\t\trxe_dbg_qp(qp, \"unable to destroy qp, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_cleanup(qp);\n\tif (err)\n\t\trxe_err_qp(qp, \"cleanup failed, err = %d\", err);\n\n\treturn 0;\n\nerr_out:\n\trxe_err_qp(qp, \"returned err = %d\", err);\n\treturn err;\n}\n\n \n\n \nstatic int validate_send_wr(struct rxe_qp *qp, const struct ib_send_wr *ibwr,\n\t\t\t    unsigned int *maskp, unsigned int *lengthp)\n{\n\tint num_sge = ibwr->num_sge;\n\tstruct rxe_sq *sq = &qp->sq;\n\tunsigned int mask = 0;\n\tunsigned long length = 0;\n\tint err = -EINVAL;\n\tint i;\n\n\tdo {\n\t\tmask = wr_opcode_mask(ibwr->opcode, qp);\n\t\tif (!mask) {\n\t\t\trxe_err_qp(qp, \"bad wr opcode for qp type\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (num_sge > sq->max_sge) {\n\t\t\trxe_err_qp(qp, \"num_sge > max_sge\");\n\t\t\tbreak;\n\t\t}\n\n\t\tlength = 0;\n\t\tfor (i = 0; i < ibwr->num_sge; i++)\n\t\t\tlength += ibwr->sg_list[i].length;\n\n\t\tif (length > (1UL << 31)) {\n\t\t\trxe_err_qp(qp, \"message length too long\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (mask & WR_ATOMIC_MASK) {\n\t\t\tif (length != 8) {\n\t\t\t\trxe_err_qp(qp, \"atomic length != 8\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (atomic_wr(ibwr)->remote_addr & 0x7) {\n\t\t\t\trxe_err_qp(qp, \"misaligned atomic address\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (ibwr->send_flags & IB_SEND_INLINE) {\n\t\t\tif (!(mask & WR_INLINE_MASK)) {\n\t\t\t\trxe_err_qp(qp, \"opcode doesn't support inline data\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (length > sq->max_inline) {\n\t\t\t\trxe_err_qp(qp, \"inline length too big\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\terr = 0;\n\t} while (0);\n\n\t*maskp = mask;\n\t*lengthp = (int)length;\n\n\treturn err;\n}\n\nstatic int init_send_wr(struct rxe_qp *qp, struct rxe_send_wr *wr,\n\t\t\t const struct ib_send_wr *ibwr)\n{\n\twr->wr_id = ibwr->wr_id;\n\twr->opcode = ibwr->opcode;\n\twr->send_flags = ibwr->send_flags;\n\n\tif (qp_type(qp) == IB_QPT_UD ||\n\t    qp_type(qp) == IB_QPT_GSI) {\n\t\tstruct ib_ah *ibah = ud_wr(ibwr)->ah;\n\n\t\twr->wr.ud.remote_qpn = ud_wr(ibwr)->remote_qpn;\n\t\twr->wr.ud.remote_qkey = ud_wr(ibwr)->remote_qkey;\n\t\twr->wr.ud.ah_num = to_rah(ibah)->ah_num;\n\t\tif (qp_type(qp) == IB_QPT_GSI)\n\t\t\twr->wr.ud.pkey_index = ud_wr(ibwr)->pkey_index;\n\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\twr->ex.imm_data = ibwr->ex.imm_data;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trxe_err_qp(qp, \"bad wr opcode %d for UD/GSI QP\",\n\t\t\t\t\twr->opcode);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\t\twr->ex.imm_data = ibwr->ex.imm_data;\n\t\t\tfallthrough;\n\t\tcase IB_WR_RDMA_READ:\n\t\tcase IB_WR_RDMA_WRITE:\n\t\t\twr->wr.rdma.remote_addr = rdma_wr(ibwr)->remote_addr;\n\t\t\twr->wr.rdma.rkey\t= rdma_wr(ibwr)->rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\twr->ex.imm_data = ibwr->ex.imm_data;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\twr->ex.invalidate_rkey = ibwr->ex.invalidate_rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\t\twr->ex.invalidate_rkey = ibwr->ex.invalidate_rkey;\n\t\t\twr->wr.rdma.remote_addr = rdma_wr(ibwr)->remote_addr;\n\t\t\twr->wr.rdma.rkey\t= rdma_wr(ibwr)->rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\t\twr->wr.atomic.remote_addr =\n\t\t\t\tatomic_wr(ibwr)->remote_addr;\n\t\t\twr->wr.atomic.compare_add =\n\t\t\t\tatomic_wr(ibwr)->compare_add;\n\t\t\twr->wr.atomic.swap = atomic_wr(ibwr)->swap;\n\t\t\twr->wr.atomic.rkey = atomic_wr(ibwr)->rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\twr->ex.invalidate_rkey = ibwr->ex.invalidate_rkey;\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR:\n\t\t\twr->wr.reg.mr = reg_wr(ibwr)->mr;\n\t\t\twr->wr.reg.key = reg_wr(ibwr)->key;\n\t\t\twr->wr.reg.access = reg_wr(ibwr)->access;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND:\n\t\tcase IB_WR_BIND_MW:\n\t\tcase IB_WR_FLUSH:\n\t\tcase IB_WR_ATOMIC_WRITE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trxe_err_qp(qp, \"unsupported wr opcode %d\",\n\t\t\t\t\twr->opcode);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void copy_inline_data_to_wqe(struct rxe_send_wqe *wqe,\n\t\t\t\t    const struct ib_send_wr *ibwr)\n{\n\tstruct ib_sge *sge = ibwr->sg_list;\n\tu8 *p = wqe->dma.inline_data;\n\tint i;\n\n\tfor (i = 0; i < ibwr->num_sge; i++, sge++) {\n\t\tmemcpy(p, ib_virt_dma_to_page(sge->addr), sge->length);\n\t\tp += sge->length;\n\t}\n}\n\nstatic int init_send_wqe(struct rxe_qp *qp, const struct ib_send_wr *ibwr,\n\t\t\t unsigned int mask, unsigned int length,\n\t\t\t struct rxe_send_wqe *wqe)\n{\n\tint num_sge = ibwr->num_sge;\n\tint err;\n\n\terr = init_send_wr(qp, &wqe->wr, ibwr);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (unlikely(mask & WR_LOCAL_OP_MASK)) {\n\t\twqe->mask = mask;\n\t\twqe->state = wqe_state_posted;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(ibwr->send_flags & IB_SEND_INLINE))\n\t\tcopy_inline_data_to_wqe(wqe, ibwr);\n\telse\n\t\tmemcpy(wqe->dma.sge, ibwr->sg_list,\n\t\t       num_sge * sizeof(struct ib_sge));\n\n\twqe->iova = mask & WR_ATOMIC_MASK ? atomic_wr(ibwr)->remote_addr :\n\t\tmask & WR_READ_OR_WRITE_MASK ? rdma_wr(ibwr)->remote_addr : 0;\n\twqe->mask\t\t= mask;\n\twqe->dma.length\t\t= length;\n\twqe->dma.resid\t\t= length;\n\twqe->dma.num_sge\t= num_sge;\n\twqe->dma.cur_sge\t= 0;\n\twqe->dma.sge_offset\t= 0;\n\twqe->state\t\t= wqe_state_posted;\n\twqe->ssn\t\t= atomic_add_return(1, &qp->ssn);\n\n\treturn 0;\n}\n\nstatic int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr)\n{\n\tint err;\n\tstruct rxe_sq *sq = &qp->sq;\n\tstruct rxe_send_wqe *send_wqe;\n\tunsigned int mask;\n\tunsigned int length;\n\tint full;\n\n\terr = validate_send_wr(qp, ibwr, &mask, &length);\n\tif (err)\n\t\treturn err;\n\n\tfull = queue_full(sq->queue, QUEUE_TYPE_FROM_ULP);\n\tif (unlikely(full)) {\n\t\trxe_err_qp(qp, \"send queue full\");\n\t\treturn -ENOMEM;\n\t}\n\n\tsend_wqe = queue_producer_addr(sq->queue, QUEUE_TYPE_FROM_ULP);\n\terr = init_send_wqe(qp, ibwr, mask, length, send_wqe);\n\tif (!err)\n\t\tqueue_advance_producer(sq->queue, QUEUE_TYPE_FROM_ULP);\n\n\treturn err;\n}\n\nstatic int rxe_post_send_kernel(struct rxe_qp *qp,\n\t\t\t\tconst struct ib_send_wr *ibwr,\n\t\t\t\tconst struct ib_send_wr **bad_wr)\n{\n\tint err = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->sq.sq_lock, flags);\n\twhile (ibwr) {\n\t\terr = post_one_send(qp, ibwr);\n\t\tif (err) {\n\t\t\t*bad_wr = ibwr;\n\t\t\tbreak;\n\t\t}\n\t\tibwr = ibwr->next;\n\t}\n\tspin_unlock_irqrestore(&qp->sq.sq_lock, flags);\n\n\tif (!err)\n\t\trxe_sched_task(&qp->req.task);\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (qp_state(qp) == IB_QPS_ERR)\n\t\trxe_sched_task(&qp->comp.task);\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\treturn err;\n}\n\nstatic int rxe_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,\n\t\t\t const struct ib_send_wr **bad_wr)\n{\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\tint err;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\t \n\tif (WARN_ON_ONCE(!qp->valid)) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\trxe_err_qp(qp, \"qp has been destroyed\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(qp_state(qp) < IB_QPS_RTS)) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\t*bad_wr = wr;\n\t\trxe_err_qp(qp, \"qp not ready to send\");\n\t\treturn -EINVAL;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\tif (qp->is_user) {\n\t\t \n\t\trxe_run_task(&qp->req.task);\n\t} else {\n\t\terr = rxe_post_send_kernel(qp, wr, bad_wr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int post_one_recv(struct rxe_rq *rq, const struct ib_recv_wr *ibwr)\n{\n\tint i;\n\tunsigned long length;\n\tstruct rxe_recv_wqe *recv_wqe;\n\tint num_sge = ibwr->num_sge;\n\tint full;\n\tint err;\n\n\tfull = queue_full(rq->queue, QUEUE_TYPE_FROM_ULP);\n\tif (unlikely(full)) {\n\t\terr = -ENOMEM;\n\t\trxe_dbg(\"queue full\");\n\t\tgoto err_out;\n\t}\n\n\tif (unlikely(num_sge > rq->max_sge)) {\n\t\terr = -EINVAL;\n\t\trxe_dbg(\"bad num_sge > max_sge\");\n\t\tgoto err_out;\n\t}\n\n\tlength = 0;\n\tfor (i = 0; i < num_sge; i++)\n\t\tlength += ibwr->sg_list[i].length;\n\n\t \n\tif (length >= (1UL<<31)) {\n\t\terr = -EINVAL;\n\t\trxe_dbg(\"message length too long\");\n\t\tgoto err_out;\n\t}\n\n\trecv_wqe = queue_producer_addr(rq->queue, QUEUE_TYPE_FROM_ULP);\n\n\trecv_wqe->wr_id = ibwr->wr_id;\n\trecv_wqe->dma.length = length;\n\trecv_wqe->dma.resid = length;\n\trecv_wqe->dma.num_sge = num_sge;\n\trecv_wqe->dma.cur_sge = 0;\n\trecv_wqe->dma.sge_offset = 0;\n\tmemcpy(recv_wqe->dma.sge, ibwr->sg_list,\n\t       num_sge * sizeof(struct ib_sge));\n\n\tqueue_advance_producer(rq->queue, QUEUE_TYPE_FROM_ULP);\n\n\treturn 0;\n\nerr_out:\n\trxe_dbg(\"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,\n\t\t\t const struct ib_recv_wr **bad_wr)\n{\n\tint err = 0;\n\tstruct rxe_qp *qp = to_rqp(ibqp);\n\tstruct rxe_rq *rq = &qp->rq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\t \n\tif (WARN_ON_ONCE(!qp->valid)) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\trxe_err_qp(qp, \"qp has been destroyed\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (unlikely((qp_state(qp) < IB_QPS_INIT))) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\t*bad_wr = wr;\n\t\trxe_dbg_qp(qp, \"qp not ready to post recv\");\n\t\treturn -EINVAL;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\tif (unlikely(qp->srq)) {\n\t\t*bad_wr = wr;\n\t\trxe_dbg_qp(qp, \"qp has srq, use post_srq_recv instead\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&rq->producer_lock, flags);\n\n\twhile (wr) {\n\t\terr = post_one_recv(rq, wr);\n\t\tif (unlikely(err)) {\n\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\n\tspin_unlock_irqrestore(&rq->producer_lock, flags);\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (qp_state(qp) == IB_QPS_ERR)\n\t\trxe_sched_task(&qp->resp.task);\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\treturn err;\n}\n\n \nstatic int rxe_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,\n\t\t\t struct ib_udata *udata)\n{\n\tstruct ib_device *dev = ibcq->device;\n\tstruct rxe_dev *rxe = to_rdev(dev);\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tstruct rxe_create_cq_resp __user *uresp = NULL;\n\tint err, cleanup_err;\n\n\tif (udata) {\n\t\tif (udata->outlen < sizeof(*uresp)) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_dbg_dev(rxe, \"malformed udata, err = %d\", err);\n\t\t\tgoto err_out;\n\t\t}\n\t\turesp = udata->outbuf;\n\t}\n\n\tif (attr->flags) {\n\t\terr = -EOPNOTSUPP;\n\t\trxe_dbg_dev(rxe, \"bad attr->flags, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_cq_chk_attr(rxe, NULL, attr->cqe, attr->comp_vector);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"bad init attributes, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_add_to_pool(&rxe->cq_pool, cq);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to create cq, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_cq_from_init(rxe, cq, attr->cqe, attr->comp_vector, udata,\n\t\t\t       uresp);\n\tif (err) {\n\t\trxe_dbg_cq(cq, \"create cq failed, err = %d\", err);\n\t\tgoto err_cleanup;\n\t}\n\n\treturn 0;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(cq);\n\tif (cleanup_err)\n\t\trxe_err_cq(cq, \"cleanup failed, err = %d\", cleanup_err);\nerr_out:\n\trxe_err_dev(rxe, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata)\n{\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tstruct rxe_dev *rxe = to_rdev(ibcq->device);\n\tstruct rxe_resize_cq_resp __user *uresp = NULL;\n\tint err;\n\n\tif (udata) {\n\t\tif (udata->outlen < sizeof(*uresp)) {\n\t\t\terr = -EINVAL;\n\t\t\trxe_dbg_cq(cq, \"malformed udata\");\n\t\t\tgoto err_out;\n\t\t}\n\t\turesp = udata->outbuf;\n\t}\n\n\terr = rxe_cq_chk_attr(rxe, cq, cqe, 0);\n\tif (err) {\n\t\trxe_dbg_cq(cq, \"bad attr, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_cq_resize_queue(cq, cqe, uresp, udata);\n\tif (err) {\n\t\trxe_dbg_cq(cq, \"resize cq failed, err = %d\", err);\n\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\trxe_err_cq(cq, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic int rxe_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\n{\n\tint i;\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tstruct rxe_cqe *cqe;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, flags);\n\tfor (i = 0; i < num_entries; i++) {\n\t\tcqe = queue_head(cq->queue, QUEUE_TYPE_TO_ULP);\n\t\tif (!cqe)\n\t\t\tbreak;\t \n\n\t\tmemcpy(wc++, &cqe->ibwc, sizeof(*wc));\n\t\tqueue_advance_consumer(cq->queue, QUEUE_TYPE_TO_ULP);\n\t}\n\tspin_unlock_irqrestore(&cq->cq_lock, flags);\n\n\treturn i;\n}\n\nstatic int rxe_peek_cq(struct ib_cq *ibcq, int wc_cnt)\n{\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tint count;\n\n\tcount = queue_count(cq->queue, QUEUE_TYPE_TO_ULP);\n\n\treturn (count > wc_cnt) ? wc_cnt : count;\n}\n\nstatic int rxe_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags)\n{\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tint ret = 0;\n\tint empty;\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(&cq->cq_lock, irq_flags);\n\tcq->notify |= flags & IB_CQ_SOLICITED_MASK;\n\tempty = queue_empty(cq->queue, QUEUE_TYPE_TO_ULP);\n\n\tif ((flags & IB_CQ_REPORT_MISSED_EVENTS) && !empty)\n\t\tret = 1;\n\n\tspin_unlock_irqrestore(&cq->cq_lock, irq_flags);\n\n\treturn ret;\n}\n\nstatic int rxe_destroy_cq(struct ib_cq *ibcq, struct ib_udata *udata)\n{\n\tstruct rxe_cq *cq = to_rcq(ibcq);\n\tint err;\n\n\t \n\tif (atomic_read(&cq->num_wq)) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_cq(cq, \"still in use\");\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_cleanup(cq);\n\tif (err)\n\t\trxe_err_cq(cq, \"cleanup failed, err = %d\", err);\n\n\treturn 0;\n\nerr_out:\n\trxe_err_cq(cq, \"returned err = %d\", err);\n\treturn err;\n}\n\n \nstatic struct ib_mr *rxe_get_dma_mr(struct ib_pd *ibpd, int access)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibpd->device);\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\tstruct rxe_mr *mr;\n\tint err;\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = rxe_add_to_pool(&rxe->mr_pool, mr);\n\tif (err) {\n\t\trxe_dbg_dev(rxe, \"unable to create mr\");\n\t\tgoto err_free;\n\t}\n\n\trxe_get(pd);\n\tmr->ibmr.pd = ibpd;\n\tmr->ibmr.device = ibpd->device;\n\n\trxe_mr_init_dma(access, mr);\n\trxe_finalize(mr);\n\treturn &mr->ibmr;\n\nerr_free:\n\tkfree(mr);\n\trxe_err_pd(pd, \"returned err = %d\", err);\n\treturn ERR_PTR(err);\n}\n\nstatic struct ib_mr *rxe_reg_user_mr(struct ib_pd *ibpd, u64 start,\n\t\t\t\t     u64 length, u64 iova, int access,\n\t\t\t\t     struct ib_udata *udata)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibpd->device);\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\tstruct rxe_mr *mr;\n\tint err, cleanup_err;\n\n\tif (access & ~RXE_ACCESS_SUPPORTED_MR) {\n\t\trxe_err_pd(pd, \"access = %#x not supported (%#x)\", access,\n\t\t\t\tRXE_ACCESS_SUPPORTED_MR);\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = rxe_add_to_pool(&rxe->mr_pool, mr);\n\tif (err) {\n\t\trxe_dbg_pd(pd, \"unable to create mr\");\n\t\tgoto err_free;\n\t}\n\n\trxe_get(pd);\n\tmr->ibmr.pd = ibpd;\n\tmr->ibmr.device = ibpd->device;\n\n\terr = rxe_mr_init_user(rxe, start, length, iova, access, mr);\n\tif (err) {\n\t\trxe_dbg_mr(mr, \"reg_user_mr failed, err = %d\", err);\n\t\tgoto err_cleanup;\n\t}\n\n\trxe_finalize(mr);\n\treturn &mr->ibmr;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(mr);\n\tif (cleanup_err)\n\t\trxe_err_mr(mr, \"cleanup failed, err = %d\", cleanup_err);\nerr_free:\n\tkfree(mr);\n\trxe_err_pd(pd, \"returned err = %d\", err);\n\treturn ERR_PTR(err);\n}\n\nstatic struct ib_mr *rxe_rereg_user_mr(struct ib_mr *ibmr, int flags,\n\t\t\t\t       u64 start, u64 length, u64 iova,\n\t\t\t\t       int access, struct ib_pd *ibpd,\n\t\t\t\t       struct ib_udata *udata)\n{\n\tstruct rxe_mr *mr = to_rmr(ibmr);\n\tstruct rxe_pd *old_pd = to_rpd(ibmr->pd);\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\n\t \n\tif (flags & ~RXE_MR_REREG_SUPPORTED) {\n\t\trxe_err_mr(mr, \"flags = %#x not supported\", flags);\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\n\tif (flags & IB_MR_REREG_PD) {\n\t\trxe_put(old_pd);\n\t\trxe_get(pd);\n\t\tmr->ibmr.pd = ibpd;\n\t}\n\n\tif (flags & IB_MR_REREG_ACCESS) {\n\t\tif (access & ~RXE_ACCESS_SUPPORTED_MR) {\n\t\t\trxe_err_mr(mr, \"access = %#x not supported\", access);\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\t}\n\t\tmr->access = access;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct ib_mr *rxe_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,\n\t\t\t\t  u32 max_num_sg)\n{\n\tstruct rxe_dev *rxe = to_rdev(ibpd->device);\n\tstruct rxe_pd *pd = to_rpd(ibpd);\n\tstruct rxe_mr *mr;\n\tint err, cleanup_err;\n\n\tif (mr_type != IB_MR_TYPE_MEM_REG) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_pd(pd, \"mr type %d not supported, err = %d\",\n\t\t\t   mr_type, err);\n\t\tgoto err_out;\n\t}\n\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = rxe_add_to_pool(&rxe->mr_pool, mr);\n\tif (err)\n\t\tgoto err_free;\n\n\trxe_get(pd);\n\tmr->ibmr.pd = ibpd;\n\tmr->ibmr.device = ibpd->device;\n\n\terr = rxe_mr_init_fast(max_num_sg, mr);\n\tif (err) {\n\t\trxe_dbg_mr(mr, \"alloc_mr failed, err = %d\", err);\n\t\tgoto err_cleanup;\n\t}\n\n\trxe_finalize(mr);\n\treturn &mr->ibmr;\n\nerr_cleanup:\n\tcleanup_err = rxe_cleanup(mr);\n\tif (cleanup_err)\n\t\trxe_err_mr(mr, \"cleanup failed, err = %d\", err);\nerr_free:\n\tkfree(mr);\nerr_out:\n\trxe_err_pd(pd, \"returned err = %d\", err);\n\treturn ERR_PTR(err);\n}\n\nstatic int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)\n{\n\tstruct rxe_mr *mr = to_rmr(ibmr);\n\tint err, cleanup_err;\n\n\t \n\tif (atomic_read(&mr->num_mw) > 0) {\n\t\terr = -EINVAL;\n\t\trxe_dbg_mr(mr, \"mr has mw's bound\");\n\t\tgoto err_out;\n\t}\n\n\tcleanup_err = rxe_cleanup(mr);\n\tif (cleanup_err)\n\t\trxe_err_mr(mr, \"cleanup failed, err = %d\", cleanup_err);\n\n\tkfree_rcu_mightsleep(mr);\n\treturn 0;\n\nerr_out:\n\trxe_err_mr(mr, \"returned err = %d\", err);\n\treturn err;\n}\n\nstatic ssize_t parent_show(struct device *device,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct rxe_dev *rxe =\n\t\trdma_device_to_drv_device(device, struct rxe_dev, ib_dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", rxe_parent_name(rxe, 1));\n}\n\nstatic DEVICE_ATTR_RO(parent);\n\nstatic struct attribute *rxe_dev_attributes[] = {\n\t&dev_attr_parent.attr,\n\tNULL\n};\n\nstatic const struct attribute_group rxe_attr_group = {\n\t.attrs = rxe_dev_attributes,\n};\n\nstatic int rxe_enable_driver(struct ib_device *ib_dev)\n{\n\tstruct rxe_dev *rxe = container_of(ib_dev, struct rxe_dev, ib_dev);\n\n\trxe_set_port_state(rxe);\n\tdev_info(&rxe->ib_dev.dev, \"added %s\\n\", netdev_name(rxe->ndev));\n\treturn 0;\n}\n\nstatic const struct ib_device_ops rxe_dev_ops = {\n\t.owner = THIS_MODULE,\n\t.driver_id = RDMA_DRIVER_RXE,\n\t.uverbs_abi_ver = RXE_UVERBS_ABI_VERSION,\n\n\t.alloc_hw_port_stats = rxe_ib_alloc_hw_port_stats,\n\t.alloc_mr = rxe_alloc_mr,\n\t.alloc_mw = rxe_alloc_mw,\n\t.alloc_pd = rxe_alloc_pd,\n\t.alloc_ucontext = rxe_alloc_ucontext,\n\t.attach_mcast = rxe_attach_mcast,\n\t.create_ah = rxe_create_ah,\n\t.create_cq = rxe_create_cq,\n\t.create_qp = rxe_create_qp,\n\t.create_srq = rxe_create_srq,\n\t.create_user_ah = rxe_create_ah,\n\t.dealloc_driver = rxe_dealloc,\n\t.dealloc_mw = rxe_dealloc_mw,\n\t.dealloc_pd = rxe_dealloc_pd,\n\t.dealloc_ucontext = rxe_dealloc_ucontext,\n\t.dereg_mr = rxe_dereg_mr,\n\t.destroy_ah = rxe_destroy_ah,\n\t.destroy_cq = rxe_destroy_cq,\n\t.destroy_qp = rxe_destroy_qp,\n\t.destroy_srq = rxe_destroy_srq,\n\t.detach_mcast = rxe_detach_mcast,\n\t.device_group = &rxe_attr_group,\n\t.enable_driver = rxe_enable_driver,\n\t.get_dma_mr = rxe_get_dma_mr,\n\t.get_hw_stats = rxe_ib_get_hw_stats,\n\t.get_link_layer = rxe_get_link_layer,\n\t.get_port_immutable = rxe_port_immutable,\n\t.map_mr_sg = rxe_map_mr_sg,\n\t.mmap = rxe_mmap,\n\t.modify_ah = rxe_modify_ah,\n\t.modify_device = rxe_modify_device,\n\t.modify_port = rxe_modify_port,\n\t.modify_qp = rxe_modify_qp,\n\t.modify_srq = rxe_modify_srq,\n\t.peek_cq = rxe_peek_cq,\n\t.poll_cq = rxe_poll_cq,\n\t.post_recv = rxe_post_recv,\n\t.post_send = rxe_post_send,\n\t.post_srq_recv = rxe_post_srq_recv,\n\t.query_ah = rxe_query_ah,\n\t.query_device = rxe_query_device,\n\t.query_pkey = rxe_query_pkey,\n\t.query_port = rxe_query_port,\n\t.query_qp = rxe_query_qp,\n\t.query_srq = rxe_query_srq,\n\t.reg_user_mr = rxe_reg_user_mr,\n\t.req_notify_cq = rxe_req_notify_cq,\n\t.rereg_user_mr = rxe_rereg_user_mr,\n\t.resize_cq = rxe_resize_cq,\n\n\tINIT_RDMA_OBJ_SIZE(ib_ah, rxe_ah, ibah),\n\tINIT_RDMA_OBJ_SIZE(ib_cq, rxe_cq, ibcq),\n\tINIT_RDMA_OBJ_SIZE(ib_pd, rxe_pd, ibpd),\n\tINIT_RDMA_OBJ_SIZE(ib_qp, rxe_qp, ibqp),\n\tINIT_RDMA_OBJ_SIZE(ib_srq, rxe_srq, ibsrq),\n\tINIT_RDMA_OBJ_SIZE(ib_ucontext, rxe_ucontext, ibuc),\n\tINIT_RDMA_OBJ_SIZE(ib_mw, rxe_mw, ibmw),\n};\n\nint rxe_register_device(struct rxe_dev *rxe, const char *ibdev_name)\n{\n\tint err;\n\tstruct ib_device *dev = &rxe->ib_dev;\n\n\tstrscpy(dev->node_desc, \"rxe\", sizeof(dev->node_desc));\n\n\tdev->node_type = RDMA_NODE_IB_CA;\n\tdev->phys_port_cnt = 1;\n\tdev->num_comp_vectors = num_possible_cpus();\n\tdev->local_dma_lkey = 0;\n\taddrconf_addr_eui48((unsigned char *)&dev->node_guid,\n\t\t\t    rxe->ndev->dev_addr);\n\n\tdev->uverbs_cmd_mask |= BIT_ULL(IB_USER_VERBS_CMD_POST_SEND) |\n\t\t\t\tBIT_ULL(IB_USER_VERBS_CMD_REQ_NOTIFY_CQ);\n\n\tib_set_device_ops(dev, &rxe_dev_ops);\n\terr = ib_device_set_netdev(&rxe->ib_dev, rxe->ndev, 1);\n\tif (err)\n\t\treturn err;\n\n\terr = rxe_icrc_init(rxe);\n\tif (err)\n\t\treturn err;\n\n\terr = ib_register_device(dev, ibdev_name, NULL);\n\tif (err)\n\t\trxe_dbg_dev(rxe, \"failed with error %d\\n\", err);\n\n\t \n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}