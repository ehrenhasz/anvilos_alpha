{
  "module_name": "rxe_resp.c",
  "hash_id": "1e5914a913dc2e58e0800fea515a6fd655296904e99be9c93377e38ee0f7c8a3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rxe/rxe_resp.c",
  "human_readable_source": "\n \n\n#include <linux/skbuff.h>\n\n#include \"rxe.h\"\n#include \"rxe_loc.h\"\n#include \"rxe_queue.h\"\n\nstatic char *resp_state_name[] = {\n\t[RESPST_NONE]\t\t\t\t= \"NONE\",\n\t[RESPST_GET_REQ]\t\t\t= \"GET_REQ\",\n\t[RESPST_CHK_PSN]\t\t\t= \"CHK_PSN\",\n\t[RESPST_CHK_OP_SEQ]\t\t\t= \"CHK_OP_SEQ\",\n\t[RESPST_CHK_OP_VALID]\t\t\t= \"CHK_OP_VALID\",\n\t[RESPST_CHK_RESOURCE]\t\t\t= \"CHK_RESOURCE\",\n\t[RESPST_CHK_LENGTH]\t\t\t= \"CHK_LENGTH\",\n\t[RESPST_CHK_RKEY]\t\t\t= \"CHK_RKEY\",\n\t[RESPST_EXECUTE]\t\t\t= \"EXECUTE\",\n\t[RESPST_READ_REPLY]\t\t\t= \"READ_REPLY\",\n\t[RESPST_ATOMIC_REPLY]\t\t\t= \"ATOMIC_REPLY\",\n\t[RESPST_ATOMIC_WRITE_REPLY]\t\t= \"ATOMIC_WRITE_REPLY\",\n\t[RESPST_PROCESS_FLUSH]\t\t\t= \"PROCESS_FLUSH\",\n\t[RESPST_COMPLETE]\t\t\t= \"COMPLETE\",\n\t[RESPST_ACKNOWLEDGE]\t\t\t= \"ACKNOWLEDGE\",\n\t[RESPST_CLEANUP]\t\t\t= \"CLEANUP\",\n\t[RESPST_DUPLICATE_REQUEST]\t\t= \"DUPLICATE_REQUEST\",\n\t[RESPST_ERR_MALFORMED_WQE]\t\t= \"ERR_MALFORMED_WQE\",\n\t[RESPST_ERR_UNSUPPORTED_OPCODE]\t\t= \"ERR_UNSUPPORTED_OPCODE\",\n\t[RESPST_ERR_MISALIGNED_ATOMIC]\t\t= \"ERR_MISALIGNED_ATOMIC\",\n\t[RESPST_ERR_PSN_OUT_OF_SEQ]\t\t= \"ERR_PSN_OUT_OF_SEQ\",\n\t[RESPST_ERR_MISSING_OPCODE_FIRST]\t= \"ERR_MISSING_OPCODE_FIRST\",\n\t[RESPST_ERR_MISSING_OPCODE_LAST_C]\t= \"ERR_MISSING_OPCODE_LAST_C\",\n\t[RESPST_ERR_MISSING_OPCODE_LAST_D1E]\t= \"ERR_MISSING_OPCODE_LAST_D1E\",\n\t[RESPST_ERR_TOO_MANY_RDMA_ATM_REQ]\t= \"ERR_TOO_MANY_RDMA_ATM_REQ\",\n\t[RESPST_ERR_RNR]\t\t\t= \"ERR_RNR\",\n\t[RESPST_ERR_RKEY_VIOLATION]\t\t= \"ERR_RKEY_VIOLATION\",\n\t[RESPST_ERR_INVALIDATE_RKEY]\t\t= \"ERR_INVALIDATE_RKEY_VIOLATION\",\n\t[RESPST_ERR_LENGTH]\t\t\t= \"ERR_LENGTH\",\n\t[RESPST_ERR_CQ_OVERFLOW]\t\t= \"ERR_CQ_OVERFLOW\",\n\t[RESPST_ERROR]\t\t\t\t= \"ERROR\",\n\t[RESPST_DONE]\t\t\t\t= \"DONE\",\n\t[RESPST_EXIT]\t\t\t\t= \"EXIT\",\n};\n\n \nvoid rxe_resp_queue_pkt(struct rxe_qp *qp, struct sk_buff *skb)\n{\n\tint must_sched;\n\tstruct rxe_pkt_info *pkt = SKB_TO_PKT(skb);\n\n\tskb_queue_tail(&qp->req_pkts, skb);\n\n\tmust_sched = (pkt->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST) ||\n\t\t\t(skb_queue_len(&qp->req_pkts) > 1);\n\n\tif (must_sched)\n\t\trxe_sched_task(&qp->resp.task);\n\telse\n\t\trxe_run_task(&qp->resp.task);\n}\n\nstatic inline enum resp_states get_req(struct rxe_qp *qp,\n\t\t\t\t       struct rxe_pkt_info **pkt_p)\n{\n\tstruct sk_buff *skb;\n\n\tskb = skb_peek(&qp->req_pkts);\n\tif (!skb)\n\t\treturn RESPST_EXIT;\n\n\t*pkt_p = SKB_TO_PKT(skb);\n\n\treturn (qp->resp.res) ? RESPST_READ_REPLY : RESPST_CHK_PSN;\n}\n\nstatic enum resp_states check_psn(struct rxe_qp *qp,\n\t\t\t\t  struct rxe_pkt_info *pkt)\n{\n\tint diff = psn_compare(pkt->psn, qp->resp.psn);\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\n\tswitch (qp_type(qp)) {\n\tcase IB_QPT_RC:\n\t\tif (diff > 0) {\n\t\t\tif (qp->resp.sent_psn_nak)\n\t\t\t\treturn RESPST_CLEANUP;\n\n\t\t\tqp->resp.sent_psn_nak = 1;\n\t\t\trxe_counter_inc(rxe, RXE_CNT_OUT_OF_SEQ_REQ);\n\t\t\treturn RESPST_ERR_PSN_OUT_OF_SEQ;\n\n\t\t} else if (diff < 0) {\n\t\t\trxe_counter_inc(rxe, RXE_CNT_DUP_REQ);\n\t\t\treturn RESPST_DUPLICATE_REQUEST;\n\t\t}\n\n\t\tif (qp->resp.sent_psn_nak)\n\t\t\tqp->resp.sent_psn_nak = 0;\n\n\t\tbreak;\n\n\tcase IB_QPT_UC:\n\t\tif (qp->resp.drop_msg || diff != 0) {\n\t\t\tif (pkt->mask & RXE_START_MASK) {\n\t\t\t\tqp->resp.drop_msg = 0;\n\t\t\t\treturn RESPST_CHK_OP_SEQ;\n\t\t\t}\n\n\t\t\tqp->resp.drop_msg = 1;\n\t\t\treturn RESPST_CLEANUP;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn RESPST_CHK_OP_SEQ;\n}\n\nstatic enum resp_states check_op_seq(struct rxe_qp *qp,\n\t\t\t\t     struct rxe_pkt_info *pkt)\n{\n\tswitch (qp_type(qp)) {\n\tcase IB_QPT_RC:\n\t\tswitch (qp->resp.opcode) {\n\t\tcase IB_OPCODE_RC_SEND_FIRST:\n\t\tcase IB_OPCODE_RC_SEND_MIDDLE:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_RC_SEND_MIDDLE:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST_WITH_IMMEDIATE:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST_WITH_INVALIDATE:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_ERR_MISSING_OPCODE_LAST_C;\n\t\t\t}\n\n\t\tcase IB_OPCODE_RC_RDMA_WRITE_FIRST:\n\t\tcase IB_OPCODE_RC_RDMA_WRITE_MIDDLE:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_MIDDLE:\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_LAST:\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_LAST_WITH_IMMEDIATE:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_ERR_MISSING_OPCODE_LAST_C;\n\t\t\t}\n\n\t\tdefault:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_RC_SEND_MIDDLE:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST_WITH_IMMEDIATE:\n\t\t\tcase IB_OPCODE_RC_SEND_LAST_WITH_INVALIDATE:\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_MIDDLE:\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_LAST:\n\t\t\tcase IB_OPCODE_RC_RDMA_WRITE_LAST_WITH_IMMEDIATE:\n\t\t\t\treturn RESPST_ERR_MISSING_OPCODE_FIRST;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase IB_QPT_UC:\n\t\tswitch (qp->resp.opcode) {\n\t\tcase IB_OPCODE_UC_SEND_FIRST:\n\t\tcase IB_OPCODE_UC_SEND_MIDDLE:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_UC_SEND_MIDDLE:\n\t\t\tcase IB_OPCODE_UC_SEND_LAST:\n\t\t\tcase IB_OPCODE_UC_SEND_LAST_WITH_IMMEDIATE:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_ERR_MISSING_OPCODE_LAST_D1E;\n\t\t\t}\n\n\t\tcase IB_OPCODE_UC_RDMA_WRITE_FIRST:\n\t\tcase IB_OPCODE_UC_RDMA_WRITE_MIDDLE:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_MIDDLE:\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_LAST:\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_LAST_WITH_IMMEDIATE:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_ERR_MISSING_OPCODE_LAST_D1E;\n\t\t\t}\n\n\t\tdefault:\n\t\t\tswitch (pkt->opcode) {\n\t\t\tcase IB_OPCODE_UC_SEND_MIDDLE:\n\t\t\tcase IB_OPCODE_UC_SEND_LAST:\n\t\t\tcase IB_OPCODE_UC_SEND_LAST_WITH_IMMEDIATE:\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_MIDDLE:\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_LAST:\n\t\t\tcase IB_OPCODE_UC_RDMA_WRITE_LAST_WITH_IMMEDIATE:\n\t\t\t\tqp->resp.drop_msg = 1;\n\t\t\t\treturn RESPST_CLEANUP;\n\t\t\tdefault:\n\t\t\t\treturn RESPST_CHK_OP_VALID;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn RESPST_CHK_OP_VALID;\n\t}\n}\n\nstatic bool check_qp_attr_access(struct rxe_qp *qp,\n\t\t\t\t struct rxe_pkt_info *pkt)\n{\n\tif (((pkt->mask & RXE_READ_MASK) &&\n\t     !(qp->attr.qp_access_flags & IB_ACCESS_REMOTE_READ)) ||\n\t    ((pkt->mask & (RXE_WRITE_MASK | RXE_ATOMIC_WRITE_MASK)) &&\n\t     !(qp->attr.qp_access_flags & IB_ACCESS_REMOTE_WRITE)) ||\n\t    ((pkt->mask & RXE_ATOMIC_MASK) &&\n\t     !(qp->attr.qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))\n\t\treturn false;\n\n\tif (pkt->mask & RXE_FLUSH_MASK) {\n\t\tu32 flush_type = feth_plt(pkt);\n\n\t\tif ((flush_type & IB_FLUSH_GLOBAL &&\n\t\t     !(qp->attr.qp_access_flags & IB_ACCESS_FLUSH_GLOBAL)) ||\n\t\t    (flush_type & IB_FLUSH_PERSISTENT &&\n\t\t     !(qp->attr.qp_access_flags & IB_ACCESS_FLUSH_PERSISTENT)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic enum resp_states check_op_valid(struct rxe_qp *qp,\n\t\t\t\t       struct rxe_pkt_info *pkt)\n{\n\tswitch (qp_type(qp)) {\n\tcase IB_QPT_RC:\n\t\tif (!check_qp_attr_access(qp, pkt))\n\t\t\treturn RESPST_ERR_UNSUPPORTED_OPCODE;\n\n\t\tbreak;\n\n\tcase IB_QPT_UC:\n\t\tif ((pkt->mask & RXE_WRITE_MASK) &&\n\t\t    !(qp->attr.qp_access_flags & IB_ACCESS_REMOTE_WRITE)) {\n\t\t\tqp->resp.drop_msg = 1;\n\t\t\treturn RESPST_CLEANUP;\n\t\t}\n\n\t\tbreak;\n\n\tcase IB_QPT_UD:\n\tcase IB_QPT_GSI:\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\treturn RESPST_CHK_RESOURCE;\n}\n\nstatic enum resp_states get_srq_wqe(struct rxe_qp *qp)\n{\n\tstruct rxe_srq *srq = qp->srq;\n\tstruct rxe_queue *q = srq->rq.queue;\n\tstruct rxe_recv_wqe *wqe;\n\tstruct ib_event ev;\n\tunsigned int count;\n\tsize_t size;\n\tunsigned long flags;\n\n\tif (srq->error)\n\t\treturn RESPST_ERR_RNR;\n\n\tspin_lock_irqsave(&srq->rq.consumer_lock, flags);\n\n\twqe = queue_head(q, QUEUE_TYPE_FROM_CLIENT);\n\tif (!wqe) {\n\t\tspin_unlock_irqrestore(&srq->rq.consumer_lock, flags);\n\t\treturn RESPST_ERR_RNR;\n\t}\n\n\t \n\tif (unlikely(wqe->dma.num_sge > srq->rq.max_sge)) {\n\t\tspin_unlock_irqrestore(&srq->rq.consumer_lock, flags);\n\t\trxe_dbg_qp(qp, \"invalid num_sge in SRQ entry\\n\");\n\t\treturn RESPST_ERR_MALFORMED_WQE;\n\t}\n\tsize = sizeof(*wqe) + wqe->dma.num_sge*sizeof(struct rxe_sge);\n\tmemcpy(&qp->resp.srq_wqe, wqe, size);\n\n\tqp->resp.wqe = &qp->resp.srq_wqe.wqe;\n\tqueue_advance_consumer(q, QUEUE_TYPE_FROM_CLIENT);\n\tcount = queue_count(q, QUEUE_TYPE_FROM_CLIENT);\n\n\tif (srq->limit && srq->ibsrq.event_handler && (count < srq->limit)) {\n\t\tsrq->limit = 0;\n\t\tgoto event;\n\t}\n\n\tspin_unlock_irqrestore(&srq->rq.consumer_lock, flags);\n\treturn RESPST_CHK_LENGTH;\n\nevent:\n\tspin_unlock_irqrestore(&srq->rq.consumer_lock, flags);\n\tev.device = qp->ibqp.device;\n\tev.element.srq = qp->ibqp.srq;\n\tev.event = IB_EVENT_SRQ_LIMIT_REACHED;\n\tsrq->ibsrq.event_handler(&ev, srq->ibsrq.srq_context);\n\treturn RESPST_CHK_LENGTH;\n}\n\nstatic enum resp_states check_resource(struct rxe_qp *qp,\n\t\t\t\t       struct rxe_pkt_info *pkt)\n{\n\tstruct rxe_srq *srq = qp->srq;\n\n\tif (pkt->mask & (RXE_READ_OR_ATOMIC_MASK | RXE_ATOMIC_WRITE_MASK)) {\n\t\t \n\t\tif (likely(qp->attr.max_dest_rd_atomic > 0))\n\t\t\treturn RESPST_CHK_LENGTH;\n\t\telse\n\t\t\treturn RESPST_ERR_TOO_MANY_RDMA_ATM_REQ;\n\t}\n\n\tif (pkt->mask & RXE_RWR_MASK) {\n\t\tif (srq)\n\t\t\treturn get_srq_wqe(qp);\n\n\t\tqp->resp.wqe = queue_head(qp->rq.queue,\n\t\t\t\tQUEUE_TYPE_FROM_CLIENT);\n\t\treturn (qp->resp.wqe) ? RESPST_CHK_LENGTH : RESPST_ERR_RNR;\n\t}\n\n\treturn RESPST_CHK_LENGTH;\n}\n\nstatic enum resp_states rxe_resp_check_length(struct rxe_qp *qp,\n\t\t\t\t\t      struct rxe_pkt_info *pkt)\n{\n\t \n\tif (pkt->mask & RXE_PAYLOAD_MASK && ((qp_type(qp) == IB_QPT_RC) ||\n\t\t\t\t\t     (qp_type(qp) == IB_QPT_UC))) {\n\t\tunsigned int mtu = qp->mtu;\n\t\tunsigned int payload = payload_size(pkt);\n\n\t\tif ((pkt->mask & RXE_START_MASK) &&\n\t\t    (pkt->mask & RXE_END_MASK)) {\n\t\t\tif (unlikely(payload > mtu)) {\n\t\t\t\trxe_dbg_qp(qp, \"only packet too long\");\n\t\t\t\treturn RESPST_ERR_LENGTH;\n\t\t\t}\n\t\t} else if ((pkt->mask & RXE_START_MASK) ||\n\t\t\t   (pkt->mask & RXE_MIDDLE_MASK)) {\n\t\t\tif (unlikely(payload != mtu)) {\n\t\t\t\trxe_dbg_qp(qp, \"first or middle packet not mtu\");\n\t\t\t\treturn RESPST_ERR_LENGTH;\n\t\t\t}\n\t\t} else if (pkt->mask & RXE_END_MASK) {\n\t\t\tif (unlikely((payload == 0) || (payload > mtu))) {\n\t\t\t\trxe_dbg_qp(qp, \"last packet zero or too long\");\n\t\t\t\treturn RESPST_ERR_LENGTH;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (pkt->mask & RXE_RETH_MASK) {\n\t\tif (reth_len(pkt) > (1U << 31)) {\n\t\t\trxe_dbg_qp(qp, \"dma length too long\");\n\t\t\treturn RESPST_ERR_LENGTH;\n\t\t}\n\t}\n\n\tif (pkt->mask & RXE_RDMA_OP_MASK)\n\t\treturn RESPST_CHK_RKEY;\n\telse\n\t\treturn RESPST_EXECUTE;\n}\n\n \nstatic void qp_resp_from_reth(struct rxe_qp *qp, struct rxe_pkt_info *pkt)\n{\n\tunsigned int length = reth_len(pkt);\n\n\tqp->resp.va = reth_va(pkt);\n\tqp->resp.offset = 0;\n\tqp->resp.resid = length;\n\tqp->resp.length = length;\n\tif (pkt->mask & RXE_READ_OR_WRITE_MASK && length == 0)\n\t\tqp->resp.rkey = 0;\n\telse\n\t\tqp->resp.rkey = reth_rkey(pkt);\n}\n\nstatic void qp_resp_from_atmeth(struct rxe_qp *qp, struct rxe_pkt_info *pkt)\n{\n\tqp->resp.va = atmeth_va(pkt);\n\tqp->resp.offset = 0;\n\tqp->resp.rkey = atmeth_rkey(pkt);\n\tqp->resp.resid = sizeof(u64);\n}\n\n \nstatic enum resp_states check_rkey(struct rxe_qp *qp,\n\t\t\t\t   struct rxe_pkt_info *pkt)\n{\n\tstruct rxe_mr *mr = NULL;\n\tstruct rxe_mw *mw = NULL;\n\tu64 va;\n\tu32 rkey;\n\tu32 resid;\n\tu32 pktlen;\n\tint mtu = qp->mtu;\n\tenum resp_states state;\n\tint access = 0;\n\n\t \n\tif (pkt->mask & (RXE_READ_OR_WRITE_MASK | RXE_ATOMIC_WRITE_MASK)) {\n\t\tif (pkt->mask & RXE_RETH_MASK)\n\t\t\tqp_resp_from_reth(qp, pkt);\n\n\t\taccess = (pkt->mask & RXE_READ_MASK) ? IB_ACCESS_REMOTE_READ\n\t\t\t\t\t\t     : IB_ACCESS_REMOTE_WRITE;\n\t} else if (pkt->mask & RXE_FLUSH_MASK) {\n\t\tu32 flush_type = feth_plt(pkt);\n\n\t\tif (pkt->mask & RXE_RETH_MASK)\n\t\t\tqp_resp_from_reth(qp, pkt);\n\n\t\tif (flush_type & IB_FLUSH_GLOBAL)\n\t\t\taccess |= IB_ACCESS_FLUSH_GLOBAL;\n\t\tif (flush_type & IB_FLUSH_PERSISTENT)\n\t\t\taccess |= IB_ACCESS_FLUSH_PERSISTENT;\n\t} else if (pkt->mask & RXE_ATOMIC_MASK) {\n\t\tqp_resp_from_atmeth(qp, pkt);\n\t\taccess = IB_ACCESS_REMOTE_ATOMIC;\n\t} else {\n\t\t \n\t\tWARN_ON(1);\n\t}\n\n\t \n\tif ((pkt->mask & RXE_READ_OR_WRITE_MASK) &&\n\t    (pkt->mask & RXE_RETH_MASK) && reth_len(pkt) == 0) {\n\t\tqp->resp.mr = NULL;\n\t\treturn RESPST_EXECUTE;\n\t}\n\n\tva\t= qp->resp.va;\n\trkey\t= qp->resp.rkey;\n\tresid\t= qp->resp.resid;\n\tpktlen\t= payload_size(pkt);\n\n\tif (rkey_is_mw(rkey)) {\n\t\tmw = rxe_lookup_mw(qp, access, rkey);\n\t\tif (!mw) {\n\t\t\trxe_dbg_qp(qp, \"no MW matches rkey %#x\\n\", rkey);\n\t\t\tstate = RESPST_ERR_RKEY_VIOLATION;\n\t\t\tgoto err;\n\t\t}\n\n\t\tmr = mw->mr;\n\t\tif (!mr) {\n\t\t\trxe_dbg_qp(qp, \"MW doesn't have an MR\\n\");\n\t\t\tstate = RESPST_ERR_RKEY_VIOLATION;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (mw->access & IB_ZERO_BASED)\n\t\t\tqp->resp.offset = mw->addr;\n\n\t\trxe_get(mr);\n\t\trxe_put(mw);\n\t\tmw = NULL;\n\t} else {\n\t\tmr = lookup_mr(qp->pd, access, rkey, RXE_LOOKUP_REMOTE);\n\t\tif (!mr) {\n\t\t\trxe_dbg_qp(qp, \"no MR matches rkey %#x\\n\", rkey);\n\t\t\tstate = RESPST_ERR_RKEY_VIOLATION;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tif (pkt->mask & RXE_FLUSH_MASK) {\n\t\t \n\t\tif (feth_sel(pkt) == IB_FLUSH_MR)\n\t\t\tgoto skip_check_range;\n\t}\n\n\tif (mr_check_range(mr, va + qp->resp.offset, resid)) {\n\t\tstate = RESPST_ERR_RKEY_VIOLATION;\n\t\tgoto err;\n\t}\n\nskip_check_range:\n\tif (pkt->mask & (RXE_WRITE_MASK | RXE_ATOMIC_WRITE_MASK)) {\n\t\tif (resid > mtu) {\n\t\t\tif (pktlen != mtu || bth_pad(pkt)) {\n\t\t\t\tstate = RESPST_ERR_LENGTH;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t} else {\n\t\t\tif (pktlen != resid) {\n\t\t\t\tstate = RESPST_ERR_LENGTH;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif ((bth_pad(pkt) != (0x3 & (-resid)))) {\n\t\t\t\t \n\t\t\t\tstate = RESPST_ERR_LENGTH;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(qp->resp.mr);\n\n\tqp->resp.mr = mr;\n\treturn RESPST_EXECUTE;\n\nerr:\n\tqp->resp.mr = NULL;\n\tif (mr)\n\t\trxe_put(mr);\n\tif (mw)\n\t\trxe_put(mw);\n\n\treturn state;\n}\n\nstatic enum resp_states send_data_in(struct rxe_qp *qp, void *data_addr,\n\t\t\t\t     int data_len)\n{\n\tint err;\n\n\terr = copy_data(qp->pd, IB_ACCESS_LOCAL_WRITE, &qp->resp.wqe->dma,\n\t\t\tdata_addr, data_len, RXE_TO_MR_OBJ);\n\tif (unlikely(err))\n\t\treturn (err == -ENOSPC) ? RESPST_ERR_LENGTH\n\t\t\t\t\t: RESPST_ERR_MALFORMED_WQE;\n\n\treturn RESPST_NONE;\n}\n\nstatic enum resp_states write_data_in(struct rxe_qp *qp,\n\t\t\t\t      struct rxe_pkt_info *pkt)\n{\n\tenum resp_states rc = RESPST_NONE;\n\tint\terr;\n\tint data_len = payload_size(pkt);\n\n\terr = rxe_mr_copy(qp->resp.mr, qp->resp.va + qp->resp.offset,\n\t\t\t  payload_addr(pkt), data_len, RXE_TO_MR_OBJ);\n\tif (err) {\n\t\trc = RESPST_ERR_RKEY_VIOLATION;\n\t\tgoto out;\n\t}\n\n\tqp->resp.va += data_len;\n\tqp->resp.resid -= data_len;\n\nout:\n\treturn rc;\n}\n\nstatic struct resp_res *rxe_prepare_res(struct rxe_qp *qp,\n\t\t\t\t\tstruct rxe_pkt_info *pkt,\n\t\t\t\t\tint type)\n{\n\tstruct resp_res *res;\n\tu32 pkts;\n\n\tres = &qp->resp.resources[qp->resp.res_head];\n\trxe_advance_resp_resource(qp);\n\tfree_rd_atomic_resource(res);\n\n\tres->type = type;\n\tres->replay = 0;\n\n\tswitch (type) {\n\tcase RXE_READ_MASK:\n\t\tres->read.va = qp->resp.va + qp->resp.offset;\n\t\tres->read.va_org = qp->resp.va + qp->resp.offset;\n\t\tres->read.resid = qp->resp.resid;\n\t\tres->read.length = qp->resp.resid;\n\t\tres->read.rkey = qp->resp.rkey;\n\n\t\tpkts = max_t(u32, (reth_len(pkt) + qp->mtu - 1)/qp->mtu, 1);\n\t\tres->first_psn = pkt->psn;\n\t\tres->cur_psn = pkt->psn;\n\t\tres->last_psn = (pkt->psn + pkts - 1) & BTH_PSN_MASK;\n\n\t\tres->state = rdatm_res_state_new;\n\t\tbreak;\n\tcase RXE_ATOMIC_MASK:\n\tcase RXE_ATOMIC_WRITE_MASK:\n\t\tres->first_psn = pkt->psn;\n\t\tres->last_psn = pkt->psn;\n\t\tres->cur_psn = pkt->psn;\n\t\tbreak;\n\tcase RXE_FLUSH_MASK:\n\t\tres->flush.va = qp->resp.va + qp->resp.offset;\n\t\tres->flush.length = qp->resp.length;\n\t\tres->flush.type = feth_plt(pkt);\n\t\tres->flush.level = feth_sel(pkt);\n\t}\n\n\treturn res;\n}\n\nstatic enum resp_states process_flush(struct rxe_qp *qp,\n\t\t\t\t       struct rxe_pkt_info *pkt)\n{\n\tu64 length, start;\n\tstruct rxe_mr *mr = qp->resp.mr;\n\tstruct resp_res *res = qp->resp.res;\n\n\t \n\tif (res && res->replay)\n\t\treturn RESPST_ACKNOWLEDGE;\n\telse if (!res) {\n\t\tres = rxe_prepare_res(qp, pkt, RXE_FLUSH_MASK);\n\t\tqp->resp.res = res;\n\t}\n\n\tif (res->flush.level == IB_FLUSH_RANGE) {\n\t\tstart = res->flush.va;\n\t\tlength = res->flush.length;\n\t} else {  \n\t\tstart = mr->ibmr.iova;\n\t\tlength = mr->ibmr.length;\n\t}\n\n\tif (res->flush.type & IB_FLUSH_PERSISTENT) {\n\t\tif (rxe_flush_pmem_iova(mr, start, length))\n\t\t\treturn RESPST_ERR_RKEY_VIOLATION;\n\t\t \n\t\twmb();\n\t} else if (res->flush.type & IB_FLUSH_GLOBAL) {\n\t\t \n\t\twmb();\n\t}\n\n\tqp->resp.msn++;\n\n\t \n\tqp->resp.psn = (pkt->psn + 1) & BTH_PSN_MASK;\n\tqp->resp.ack_psn = qp->resp.psn;\n\n\tqp->resp.opcode = pkt->opcode;\n\tqp->resp.status = IB_WC_SUCCESS;\n\n\treturn RESPST_ACKNOWLEDGE;\n}\n\nstatic enum resp_states atomic_reply(struct rxe_qp *qp,\n\t\t\t\t     struct rxe_pkt_info *pkt)\n{\n\tstruct rxe_mr *mr = qp->resp.mr;\n\tstruct resp_res *res = qp->resp.res;\n\tint err;\n\n\tif (!res) {\n\t\tres = rxe_prepare_res(qp, pkt, RXE_ATOMIC_MASK);\n\t\tqp->resp.res = res;\n\t}\n\n\tif (!res->replay) {\n\t\tu64 iova = qp->resp.va + qp->resp.offset;\n\n\t\terr = rxe_mr_do_atomic_op(mr, iova, pkt->opcode,\n\t\t\t\t\t  atmeth_comp(pkt),\n\t\t\t\t\t  atmeth_swap_add(pkt),\n\t\t\t\t\t  &res->atomic.orig_val);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tqp->resp.msn++;\n\n\t\t \n\t\tqp->resp.psn = (pkt->psn + 1) & BTH_PSN_MASK;\n\t\tqp->resp.ack_psn = qp->resp.psn;\n\n\t\tqp->resp.opcode = pkt->opcode;\n\t\tqp->resp.status = IB_WC_SUCCESS;\n\t}\n\n\treturn RESPST_ACKNOWLEDGE;\n}\n\nstatic enum resp_states atomic_write_reply(struct rxe_qp *qp,\n\t\t\t\t\t   struct rxe_pkt_info *pkt)\n{\n\tstruct resp_res *res = qp->resp.res;\n\tstruct rxe_mr *mr;\n\tu64 value;\n\tu64 iova;\n\tint err;\n\n\tif (!res) {\n\t\tres = rxe_prepare_res(qp, pkt, RXE_ATOMIC_WRITE_MASK);\n\t\tqp->resp.res = res;\n\t}\n\n\tif (res->replay)\n\t\treturn RESPST_ACKNOWLEDGE;\n\n\tmr = qp->resp.mr;\n\tvalue = *(u64 *)payload_addr(pkt);\n\tiova = qp->resp.va + qp->resp.offset;\n\n\terr = rxe_mr_do_atomic_write(mr, iova, value);\n\tif (err)\n\t\treturn err;\n\n\tqp->resp.resid = 0;\n\tqp->resp.msn++;\n\n\t \n\tqp->resp.psn = (pkt->psn + 1) & BTH_PSN_MASK;\n\tqp->resp.ack_psn = qp->resp.psn;\n\n\tqp->resp.opcode = pkt->opcode;\n\tqp->resp.status = IB_WC_SUCCESS;\n\n\treturn RESPST_ACKNOWLEDGE;\n}\n\nstatic struct sk_buff *prepare_ack_packet(struct rxe_qp *qp,\n\t\t\t\t\t  struct rxe_pkt_info *ack,\n\t\t\t\t\t  int opcode,\n\t\t\t\t\t  int payload,\n\t\t\t\t\t  u32 psn,\n\t\t\t\t\t  u8 syndrome)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tstruct sk_buff *skb;\n\tint paylen;\n\tint pad;\n\tint err;\n\n\t \n\tpad = (-payload) & 0x3;\n\tpaylen = rxe_opcode[opcode].length + payload + pad + RXE_ICRC_SIZE;\n\n\tskb = rxe_init_packet(rxe, &qp->pri_av, paylen, ack);\n\tif (!skb)\n\t\treturn NULL;\n\n\tack->qp = qp;\n\tack->opcode = opcode;\n\tack->mask = rxe_opcode[opcode].mask;\n\tack->paylen = paylen;\n\tack->psn = psn;\n\n\tbth_init(ack, opcode, 0, 0, pad, IB_DEFAULT_PKEY_FULL,\n\t\t qp->attr.dest_qp_num, 0, psn);\n\n\tif (ack->mask & RXE_AETH_MASK) {\n\t\taeth_set_syn(ack, syndrome);\n\t\taeth_set_msn(ack, qp->resp.msn);\n\t}\n\n\tif (ack->mask & RXE_ATMACK_MASK)\n\t\tatmack_set_orig(ack, qp->resp.res->atomic.orig_val);\n\n\terr = rxe_prepare(&qp->pri_av, ack, skb);\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\treturn skb;\n}\n\n \nstatic struct rxe_mr *rxe_recheck_mr(struct rxe_qp *qp, u32 rkey)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tstruct rxe_mr *mr;\n\tstruct rxe_mw *mw;\n\n\tif (rkey_is_mw(rkey)) {\n\t\tmw = rxe_pool_get_index(&rxe->mw_pool, rkey >> 8);\n\t\tif (!mw)\n\t\t\treturn NULL;\n\n\t\tmr = mw->mr;\n\t\tif (mw->rkey != rkey || mw->state != RXE_MW_STATE_VALID ||\n\t\t    !mr || mr->state != RXE_MR_STATE_VALID) {\n\t\t\trxe_put(mw);\n\t\t\treturn NULL;\n\t\t}\n\n\t\trxe_get(mr);\n\t\trxe_put(mw);\n\n\t\treturn mr;\n\t}\n\n\tmr = rxe_pool_get_index(&rxe->mr_pool, rkey >> 8);\n\tif (!mr)\n\t\treturn NULL;\n\n\tif (mr->rkey != rkey || mr->state != RXE_MR_STATE_VALID) {\n\t\trxe_put(mr);\n\t\treturn NULL;\n\t}\n\n\treturn mr;\n}\n\n \nstatic enum resp_states read_reply(struct rxe_qp *qp,\n\t\t\t\t   struct rxe_pkt_info *req_pkt)\n{\n\tstruct rxe_pkt_info ack_pkt;\n\tstruct sk_buff *skb;\n\tint mtu = qp->mtu;\n\tenum resp_states state;\n\tint payload;\n\tint opcode;\n\tint err;\n\tstruct resp_res *res = qp->resp.res;\n\tstruct rxe_mr *mr;\n\n\tif (!res) {\n\t\tres = rxe_prepare_res(qp, req_pkt, RXE_READ_MASK);\n\t\tqp->resp.res = res;\n\t}\n\n\tif (res->state == rdatm_res_state_new) {\n\t\tif (!res->replay || qp->resp.length == 0) {\n\t\t\t \n\t\t\tmr = qp->resp.mr;\n\t\t\tqp->resp.mr = NULL;\n\t\t} else {\n\t\t\tmr = rxe_recheck_mr(qp, res->read.rkey);\n\t\t\tif (!mr)\n\t\t\t\treturn RESPST_ERR_RKEY_VIOLATION;\n\t\t}\n\n\t\tif (res->read.resid <= mtu)\n\t\t\topcode = IB_OPCODE_RC_RDMA_READ_RESPONSE_ONLY;\n\t\telse\n\t\t\topcode = IB_OPCODE_RC_RDMA_READ_RESPONSE_FIRST;\n\t} else {\n\t\t \n\t\tmr = rxe_recheck_mr(qp, res->read.rkey);\n\t\tif (!mr)\n\t\t\treturn RESPST_ERR_RKEY_VIOLATION;\n\n\t\tif (res->read.resid > mtu)\n\t\t\topcode = IB_OPCODE_RC_RDMA_READ_RESPONSE_MIDDLE;\n\t\telse\n\t\t\topcode = IB_OPCODE_RC_RDMA_READ_RESPONSE_LAST;\n\t}\n\n\tres->state = rdatm_res_state_next;\n\n\tpayload = min_t(int, res->read.resid, mtu);\n\n\tskb = prepare_ack_packet(qp, &ack_pkt, opcode, payload,\n\t\t\t\t res->cur_psn, AETH_ACK_UNLIMITED);\n\tif (!skb) {\n\t\tstate = RESPST_ERR_RNR;\n\t\tgoto err_out;\n\t}\n\n\terr = rxe_mr_copy(mr, res->read.va, payload_addr(&ack_pkt),\n\t\t\t  payload, RXE_FROM_MR_OBJ);\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\tstate = RESPST_ERR_RKEY_VIOLATION;\n\t\tgoto err_out;\n\t}\n\n\tif (bth_pad(&ack_pkt)) {\n\t\tu8 *pad = payload_addr(&ack_pkt) + payload;\n\n\t\tmemset(pad, 0, bth_pad(&ack_pkt));\n\t}\n\n\t \n\terr = rxe_xmit_packet(qp, &ack_pkt, skb);\n\tif (err) {\n\t\tstate = RESPST_ERR_RNR;\n\t\tgoto err_out;\n\t}\n\n\tres->read.va += payload;\n\tres->read.resid -= payload;\n\tres->cur_psn = (res->cur_psn + 1) & BTH_PSN_MASK;\n\n\tif (res->read.resid > 0) {\n\t\tstate = RESPST_DONE;\n\t} else {\n\t\tqp->resp.res = NULL;\n\t\tif (!res->replay)\n\t\t\tqp->resp.opcode = -1;\n\t\tif (psn_compare(res->cur_psn, qp->resp.psn) >= 0)\n\t\t\tqp->resp.psn = res->cur_psn;\n\t\tstate = RESPST_CLEANUP;\n\t}\n\nerr_out:\n\tif (mr)\n\t\trxe_put(mr);\n\treturn state;\n}\n\nstatic int invalidate_rkey(struct rxe_qp *qp, u32 rkey)\n{\n\tif (rkey_is_mw(rkey))\n\t\treturn rxe_invalidate_mw(qp, rkey);\n\telse\n\t\treturn rxe_invalidate_mr(qp, rkey);\n}\n\n \nstatic enum resp_states execute(struct rxe_qp *qp, struct rxe_pkt_info *pkt)\n{\n\tenum resp_states err;\n\tstruct sk_buff *skb = PKT_TO_SKB(pkt);\n\tunion rdma_network_hdr hdr;\n\n\tif (pkt->mask & RXE_SEND_MASK) {\n\t\tif (qp_type(qp) == IB_QPT_UD ||\n\t\t    qp_type(qp) == IB_QPT_GSI) {\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tmemset(&hdr.reserved, 0,\n\t\t\t\t\t\tsizeof(hdr.reserved));\n\t\t\t\tmemcpy(&hdr.roce4grh, ip_hdr(skb),\n\t\t\t\t\t\tsizeof(hdr.roce4grh));\n\t\t\t\terr = send_data_in(qp, &hdr, sizeof(hdr));\n\t\t\t} else {\n\t\t\t\terr = send_data_in(qp, ipv6_hdr(skb),\n\t\t\t\t\t\tsizeof(hdr));\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\terr = send_data_in(qp, payload_addr(pkt), payload_size(pkt));\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (pkt->mask & RXE_WRITE_MASK) {\n\t\terr = write_data_in(qp, pkt);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (pkt->mask & RXE_READ_MASK) {\n\t\t \n\t\tqp->resp.msn++;\n\t\treturn RESPST_READ_REPLY;\n\t} else if (pkt->mask & RXE_ATOMIC_MASK) {\n\t\treturn RESPST_ATOMIC_REPLY;\n\t} else if (pkt->mask & RXE_ATOMIC_WRITE_MASK) {\n\t\treturn RESPST_ATOMIC_WRITE_REPLY;\n\t} else if (pkt->mask & RXE_FLUSH_MASK) {\n\t\treturn RESPST_PROCESS_FLUSH;\n\t} else {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t}\n\n\tif (pkt->mask & RXE_IETH_MASK) {\n\t\tu32 rkey = ieth_rkey(pkt);\n\n\t\terr = invalidate_rkey(qp, rkey);\n\t\tif (err)\n\t\t\treturn RESPST_ERR_INVALIDATE_RKEY;\n\t}\n\n\tif (pkt->mask & RXE_END_MASK)\n\t\t \n\t\tqp->resp.msn++;\n\n\t \n\tqp->resp.psn = (pkt->psn + 1) & BTH_PSN_MASK;\n\tqp->resp.ack_psn = qp->resp.psn;\n\n\tqp->resp.opcode = pkt->opcode;\n\tqp->resp.status = IB_WC_SUCCESS;\n\n\tif (pkt->mask & RXE_COMP_MASK)\n\t\treturn RESPST_COMPLETE;\n\telse if (qp_type(qp) == IB_QPT_RC)\n\t\treturn RESPST_ACKNOWLEDGE;\n\telse\n\t\treturn RESPST_CLEANUP;\n}\n\nstatic enum resp_states do_complete(struct rxe_qp *qp,\n\t\t\t\t    struct rxe_pkt_info *pkt)\n{\n\tstruct rxe_cqe cqe;\n\tstruct ib_wc *wc = &cqe.ibwc;\n\tstruct ib_uverbs_wc *uwc = &cqe.uibwc;\n\tstruct rxe_recv_wqe *wqe = qp->resp.wqe;\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tunsigned long flags;\n\n\tif (!wqe)\n\t\tgoto finish;\n\n\tmemset(&cqe, 0, sizeof(cqe));\n\n\tif (qp->rcq->is_user) {\n\t\tuwc->status\t\t= qp->resp.status;\n\t\tuwc->qp_num\t\t= qp->ibqp.qp_num;\n\t\tuwc->wr_id\t\t= wqe->wr_id;\n\t} else {\n\t\twc->status\t\t= qp->resp.status;\n\t\twc->qp\t\t\t= &qp->ibqp;\n\t\twc->wr_id\t\t= wqe->wr_id;\n\t}\n\n\tif (wc->status == IB_WC_SUCCESS) {\n\t\trxe_counter_inc(rxe, RXE_CNT_RDMA_RECV);\n\t\twc->opcode = (pkt->mask & RXE_IMMDT_MASK &&\n\t\t\t\tpkt->mask & RXE_WRITE_MASK) ?\n\t\t\t\t\tIB_WC_RECV_RDMA_WITH_IMM : IB_WC_RECV;\n\t\twc->byte_len = (pkt->mask & RXE_IMMDT_MASK &&\n\t\t\t\tpkt->mask & RXE_WRITE_MASK) ?\n\t\t\t\t\tqp->resp.length : wqe->dma.length - wqe->dma.resid;\n\n\t\t \n\t\tif (qp->rcq->is_user) {\n\t\t\tuwc->wc_flags = IB_WC_GRH;\n\n\t\t\tif (pkt->mask & RXE_IMMDT_MASK) {\n\t\t\t\tuwc->wc_flags |= IB_WC_WITH_IMM;\n\t\t\t\tuwc->ex.imm_data = immdt_imm(pkt);\n\t\t\t}\n\n\t\t\tif (pkt->mask & RXE_IETH_MASK) {\n\t\t\t\tuwc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\t\t\tuwc->ex.invalidate_rkey = ieth_rkey(pkt);\n\t\t\t}\n\n\t\t\tif (pkt->mask & RXE_DETH_MASK)\n\t\t\t\tuwc->src_qp = deth_sqp(pkt);\n\n\t\t\tuwc->port_num\t\t= qp->attr.port_num;\n\t\t} else {\n\t\t\tstruct sk_buff *skb = PKT_TO_SKB(pkt);\n\n\t\t\twc->wc_flags = IB_WC_GRH | IB_WC_WITH_NETWORK_HDR_TYPE;\n\t\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\t\twc->network_hdr_type = RDMA_NETWORK_IPV4;\n\t\t\telse\n\t\t\t\twc->network_hdr_type = RDMA_NETWORK_IPV6;\n\n\t\t\tif (is_vlan_dev(skb->dev)) {\n\t\t\t\twc->wc_flags |= IB_WC_WITH_VLAN;\n\t\t\t\twc->vlan_id = vlan_dev_vlan_id(skb->dev);\n\t\t\t}\n\n\t\t\tif (pkt->mask & RXE_IMMDT_MASK) {\n\t\t\t\twc->wc_flags |= IB_WC_WITH_IMM;\n\t\t\t\twc->ex.imm_data = immdt_imm(pkt);\n\t\t\t}\n\n\t\t\tif (pkt->mask & RXE_IETH_MASK) {\n\t\t\t\twc->wc_flags |= IB_WC_WITH_INVALIDATE;\n\t\t\t\twc->ex.invalidate_rkey = ieth_rkey(pkt);\n\t\t\t}\n\n\t\t\tif (pkt->mask & RXE_DETH_MASK)\n\t\t\t\twc->src_qp = deth_sqp(pkt);\n\n\t\t\twc->port_num\t\t= qp->attr.port_num;\n\t\t}\n\t} else {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\t\trxe_err_qp(qp, \"non-flush error status = %d\",\n\t\t\t\twc->status);\n\t}\n\n\t \n\tif (!qp->srq)\n\t\tqueue_advance_consumer(qp->rq.queue, QUEUE_TYPE_FROM_CLIENT);\n\n\tqp->resp.wqe = NULL;\n\n\tif (rxe_cq_post(qp->rcq, &cqe, pkt ? bth_se(pkt) : 1))\n\t\treturn RESPST_ERR_CQ_OVERFLOW;\n\nfinish:\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (unlikely(qp_state(qp) == IB_QPS_ERR)) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\treturn RESPST_CHK_RESOURCE;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\tif (unlikely(!pkt))\n\t\treturn RESPST_DONE;\n\tif (qp_type(qp) == IB_QPT_RC)\n\t\treturn RESPST_ACKNOWLEDGE;\n\telse\n\t\treturn RESPST_CLEANUP;\n}\n\n\nstatic int send_common_ack(struct rxe_qp *qp, u8 syndrome, u32 psn,\n\t\t\t\t  int opcode, const char *msg)\n{\n\tint err;\n\tstruct rxe_pkt_info ack_pkt;\n\tstruct sk_buff *skb;\n\n\tskb = prepare_ack_packet(qp, &ack_pkt, opcode, 0, psn, syndrome);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\terr = rxe_xmit_packet(qp, &ack_pkt, skb);\n\tif (err)\n\t\trxe_dbg_qp(qp, \"Failed sending %s\\n\", msg);\n\n\treturn err;\n}\n\nstatic int send_ack(struct rxe_qp *qp, u8 syndrome, u32 psn)\n{\n\treturn send_common_ack(qp, syndrome, psn,\n\t\t\tIB_OPCODE_RC_ACKNOWLEDGE, \"ACK\");\n}\n\nstatic int send_atomic_ack(struct rxe_qp *qp, u8 syndrome, u32 psn)\n{\n\tint ret = send_common_ack(qp, syndrome, psn,\n\t\t\tIB_OPCODE_RC_ATOMIC_ACKNOWLEDGE, \"ATOMIC ACK\");\n\n\t \n\tqp->resp.res = NULL;\n\treturn ret;\n}\n\nstatic int send_read_response_ack(struct rxe_qp *qp, u8 syndrome, u32 psn)\n{\n\tint ret = send_common_ack(qp, syndrome, psn,\n\t\t\tIB_OPCODE_RC_RDMA_READ_RESPONSE_ONLY,\n\t\t\t\"RDMA READ response of length zero ACK\");\n\n\t \n\tqp->resp.res = NULL;\n\treturn ret;\n}\n\nstatic enum resp_states acknowledge(struct rxe_qp *qp,\n\t\t\t\t    struct rxe_pkt_info *pkt)\n{\n\tif (qp_type(qp) != IB_QPT_RC)\n\t\treturn RESPST_CLEANUP;\n\n\tif (qp->resp.aeth_syndrome != AETH_ACK_UNLIMITED)\n\t\tsend_ack(qp, qp->resp.aeth_syndrome, pkt->psn);\n\telse if (pkt->mask & RXE_ATOMIC_MASK)\n\t\tsend_atomic_ack(qp, AETH_ACK_UNLIMITED, pkt->psn);\n\telse if (pkt->mask & (RXE_FLUSH_MASK | RXE_ATOMIC_WRITE_MASK))\n\t\tsend_read_response_ack(qp, AETH_ACK_UNLIMITED, pkt->psn);\n\telse if (bth_ack(pkt))\n\t\tsend_ack(qp, AETH_ACK_UNLIMITED, pkt->psn);\n\n\treturn RESPST_CLEANUP;\n}\n\nstatic enum resp_states cleanup(struct rxe_qp *qp,\n\t\t\t\tstruct rxe_pkt_info *pkt)\n{\n\tstruct sk_buff *skb;\n\n\tif (pkt) {\n\t\tskb = skb_dequeue(&qp->req_pkts);\n\t\trxe_put(qp);\n\t\tkfree_skb(skb);\n\t\tib_device_put(qp->ibqp.device);\n\t}\n\n\tif (qp->resp.mr) {\n\t\trxe_put(qp->resp.mr);\n\t\tqp->resp.mr = NULL;\n\t}\n\n\treturn RESPST_DONE;\n}\n\nstatic struct resp_res *find_resource(struct rxe_qp *qp, u32 psn)\n{\n\tint i;\n\n\tfor (i = 0; i < qp->attr.max_dest_rd_atomic; i++) {\n\t\tstruct resp_res *res = &qp->resp.resources[i];\n\n\t\tif (res->type == 0)\n\t\t\tcontinue;\n\n\t\tif (psn_compare(psn, res->first_psn) >= 0 &&\n\t\t    psn_compare(psn, res->last_psn) <= 0) {\n\t\t\treturn res;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic enum resp_states duplicate_request(struct rxe_qp *qp,\n\t\t\t\t\t  struct rxe_pkt_info *pkt)\n{\n\tenum resp_states rc;\n\tu32 prev_psn = (qp->resp.ack_psn - 1) & BTH_PSN_MASK;\n\n\tif (pkt->mask & RXE_SEND_MASK ||\n\t    pkt->mask & RXE_WRITE_MASK) {\n\t\t \n\t\tsend_ack(qp, AETH_ACK_UNLIMITED, prev_psn);\n\t\treturn RESPST_CLEANUP;\n\t} else if (pkt->mask & RXE_FLUSH_MASK) {\n\t\tstruct resp_res *res;\n\n\t\t \n\t\tres = find_resource(qp, pkt->psn);\n\t\tif (res) {\n\t\t\tres->replay = 1;\n\t\t\tres->cur_psn = pkt->psn;\n\t\t\tqp->resp.res = res;\n\t\t\trc = RESPST_PROCESS_FLUSH;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\trc = RESPST_CLEANUP;\n\t\tgoto out;\n\t} else if (pkt->mask & RXE_READ_MASK) {\n\t\tstruct resp_res *res;\n\n\t\tres = find_resource(qp, pkt->psn);\n\t\tif (!res) {\n\t\t\t \n\t\t\trc = RESPST_CLEANUP;\n\t\t\tgoto out;\n\t\t} else {\n\t\t\t \n\t\t\tu64 iova = reth_va(pkt);\n\t\t\tu32 resid = reth_len(pkt);\n\n\t\t\tif (iova < res->read.va_org ||\n\t\t\t    resid > res->read.length ||\n\t\t\t    (iova + resid) > (res->read.va_org +\n\t\t\t\t\t      res->read.length)) {\n\t\t\t\trc = RESPST_CLEANUP;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (reth_rkey(pkt) != res->read.rkey) {\n\t\t\t\trc = RESPST_CLEANUP;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tres->cur_psn = pkt->psn;\n\t\t\tres->state = (pkt->psn == res->first_psn) ?\n\t\t\t\t\trdatm_res_state_new :\n\t\t\t\t\trdatm_res_state_replay;\n\t\t\tres->replay = 1;\n\n\t\t\t \n\t\t\tres->read.va_org = iova;\n\t\t\tres->read.va = iova;\n\t\t\tres->read.resid = resid;\n\n\t\t\t \n\t\t\tqp->resp.res = res;\n\t\t\trc = RESPST_READ_REPLY;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tstruct resp_res *res;\n\n\t\t \n\t\tres = find_resource(qp, pkt->psn);\n\t\tif (res) {\n\t\t\tres->replay = 1;\n\t\t\tres->cur_psn = pkt->psn;\n\t\t\tqp->resp.res = res;\n\t\t\trc = pkt->mask & RXE_ATOMIC_MASK ?\n\t\t\t\t\tRESPST_ATOMIC_REPLY :\n\t\t\t\t\tRESPST_ATOMIC_WRITE_REPLY;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\trc = RESPST_CLEANUP;\n\t\tgoto out;\n\t}\nout:\n\treturn rc;\n}\n\n \nstatic void do_class_ac_error(struct rxe_qp *qp, u8 syndrome,\n\t\t\t      enum ib_wc_status status)\n{\n\tqp->resp.aeth_syndrome\t= syndrome;\n\tqp->resp.status\t\t= status;\n\n\t \n\tqp->resp.goto_error\t= 1;\n}\n\nstatic enum resp_states do_class_d1e_error(struct rxe_qp *qp)\n{\n\t \n\tif (qp->srq) {\n\t\t \n\t\tqp->resp.drop_msg = 1;\n\t\tif (qp->resp.wqe) {\n\t\t\tqp->resp.status = IB_WC_REM_INV_REQ_ERR;\n\t\t\treturn RESPST_COMPLETE;\n\t\t} else {\n\t\t\treturn RESPST_CLEANUP;\n\t\t}\n\t} else {\n\t\t \n\t\tif (qp->resp.wqe) {\n\t\t\tqp->resp.wqe->dma.resid = qp->resp.wqe->dma.length;\n\t\t\tqp->resp.wqe->dma.cur_sge = 0;\n\t\t\tqp->resp.wqe->dma.sge_offset = 0;\n\t\t\tqp->resp.opcode = -1;\n\t\t}\n\n\t\tif (qp->resp.mr) {\n\t\t\trxe_put(qp->resp.mr);\n\t\t\tqp->resp.mr = NULL;\n\t\t}\n\n\t\treturn RESPST_CLEANUP;\n\t}\n}\n\n \nstatic void drain_req_pkts(struct rxe_qp *qp)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_dequeue(&qp->req_pkts))) {\n\t\trxe_put(qp);\n\t\tkfree_skb(skb);\n\t\tib_device_put(qp->ibqp.device);\n\t}\n}\n\n \nstatic int flush_recv_wqe(struct rxe_qp *qp, struct rxe_recv_wqe *wqe)\n{\n\tstruct rxe_cqe cqe = {};\n\tstruct ib_wc *wc = &cqe.ibwc;\n\tstruct ib_uverbs_wc *uwc = &cqe.uibwc;\n\tint err;\n\n\tif (qp->rcq->is_user) {\n\t\tuwc->wr_id = wqe->wr_id;\n\t\tuwc->status = IB_WC_WR_FLUSH_ERR;\n\t\tuwc->qp_num = qp_num(qp);\n\t} else {\n\t\twc->wr_id = wqe->wr_id;\n\t\twc->status = IB_WC_WR_FLUSH_ERR;\n\t\twc->qp = &qp->ibqp;\n\t}\n\n\terr = rxe_cq_post(qp->rcq, &cqe, 0);\n\tif (err)\n\t\trxe_dbg_cq(qp->rcq, \"post cq failed err = %d\", err);\n\n\treturn err;\n}\n\n \nstatic void flush_recv_queue(struct rxe_qp *qp, bool notify)\n{\n\tstruct rxe_queue *q = qp->rq.queue;\n\tstruct rxe_recv_wqe *wqe;\n\tint err;\n\n\tif (qp->srq) {\n\t\tif (notify && qp->ibqp.event_handler) {\n\t\t\tstruct ib_event ev;\n\n\t\t\tev.device = qp->ibqp.device;\n\t\t\tev.element.qp = &qp->ibqp;\n\t\t\tev.event = IB_EVENT_QP_LAST_WQE_REACHED;\n\t\t\tqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (!qp->rq.queue)\n\t\treturn;\n\n\twhile ((wqe = queue_head(q, q->type))) {\n\t\tif (notify) {\n\t\t\terr = flush_recv_wqe(qp, wqe);\n\t\t\tif (err)\n\t\t\t\tnotify = 0;\n\t\t}\n\t\tqueue_advance_consumer(q, q->type);\n\t}\n\n\tqp->resp.wqe = NULL;\n}\n\nint rxe_responder(struct rxe_qp *qp)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tenum resp_states state;\n\tstruct rxe_pkt_info *pkt = NULL;\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (!qp->valid || qp_state(qp) == IB_QPS_ERR ||\n\t\t\t  qp_state(qp) == IB_QPS_RESET) {\n\t\tbool notify = qp->valid && (qp_state(qp) == IB_QPS_ERR);\n\n\t\tdrain_req_pkts(qp);\n\t\tflush_recv_queue(qp, notify);\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\tgoto exit;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\tqp->resp.aeth_syndrome = AETH_ACK_UNLIMITED;\n\n\tstate = RESPST_GET_REQ;\n\n\twhile (1) {\n\t\trxe_dbg_qp(qp, \"state = %s\\n\", resp_state_name[state]);\n\t\tswitch (state) {\n\t\tcase RESPST_GET_REQ:\n\t\t\tstate = get_req(qp, &pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_PSN:\n\t\t\tstate = check_psn(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_OP_SEQ:\n\t\t\tstate = check_op_seq(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_OP_VALID:\n\t\t\tstate = check_op_valid(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_RESOURCE:\n\t\t\tstate = check_resource(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_LENGTH:\n\t\t\tstate = rxe_resp_check_length(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CHK_RKEY:\n\t\t\tstate = check_rkey(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_EXECUTE:\n\t\t\tstate = execute(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_COMPLETE:\n\t\t\tstate = do_complete(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_READ_REPLY:\n\t\t\tstate = read_reply(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_ATOMIC_REPLY:\n\t\t\tstate = atomic_reply(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_ATOMIC_WRITE_REPLY:\n\t\t\tstate = atomic_write_reply(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_PROCESS_FLUSH:\n\t\t\tstate = process_flush(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_ACKNOWLEDGE:\n\t\t\tstate = acknowledge(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_CLEANUP:\n\t\t\tstate = cleanup(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_DUPLICATE_REQUEST:\n\t\t\tstate = duplicate_request(qp, pkt);\n\t\t\tbreak;\n\t\tcase RESPST_ERR_PSN_OUT_OF_SEQ:\n\t\t\t \n\t\t\tsend_ack(qp, AETH_NAK_PSN_SEQ_ERROR, qp->resp.psn);\n\t\t\tstate = RESPST_CLEANUP;\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_TOO_MANY_RDMA_ATM_REQ:\n\t\tcase RESPST_ERR_MISSING_OPCODE_FIRST:\n\t\tcase RESPST_ERR_MISSING_OPCODE_LAST_C:\n\t\tcase RESPST_ERR_UNSUPPORTED_OPCODE:\n\t\tcase RESPST_ERR_MISALIGNED_ATOMIC:\n\t\t\t \n\t\t\tdo_class_ac_error(qp, AETH_NAK_INVALID_REQ,\n\t\t\t\t\t  IB_WC_REM_INV_REQ_ERR);\n\t\t\tstate = RESPST_COMPLETE;\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_MISSING_OPCODE_LAST_D1E:\n\t\t\tstate = do_class_d1e_error(qp);\n\t\t\tbreak;\n\t\tcase RESPST_ERR_RNR:\n\t\t\tif (qp_type(qp) == IB_QPT_RC) {\n\t\t\t\trxe_counter_inc(rxe, RXE_CNT_SND_RNR);\n\t\t\t\t \n\t\t\t\tsend_ack(qp, AETH_RNR_NAK |\n\t\t\t\t\t (~AETH_TYPE_MASK &\n\t\t\t\t\t qp->attr.min_rnr_timer),\n\t\t\t\t\t pkt->psn);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tqp->resp.drop_msg = 1;\n\t\t\t}\n\t\t\tstate = RESPST_CLEANUP;\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_RKEY_VIOLATION:\n\t\t\tif (qp_type(qp) == IB_QPT_RC) {\n\t\t\t\t \n\t\t\t\tdo_class_ac_error(qp, AETH_NAK_REM_ACC_ERR,\n\t\t\t\t\t\t  IB_WC_REM_ACCESS_ERR);\n\t\t\t\tstate = RESPST_COMPLETE;\n\t\t\t} else {\n\t\t\t\tqp->resp.drop_msg = 1;\n\t\t\t\tif (qp->srq) {\n\t\t\t\t\t \n\t\t\t\t\tqp->resp.status = IB_WC_REM_ACCESS_ERR;\n\t\t\t\t\tstate = RESPST_COMPLETE;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tstate = RESPST_CLEANUP;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_INVALIDATE_RKEY:\n\t\t\t \n\t\t\tqp->resp.goto_error = 1;\n\t\t\tqp->resp.status = IB_WC_REM_INV_REQ_ERR;\n\t\t\tstate = RESPST_COMPLETE;\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_LENGTH:\n\t\t\tif (qp_type(qp) == IB_QPT_RC) {\n\t\t\t\t \n\t\t\t\tdo_class_ac_error(qp, AETH_NAK_INVALID_REQ,\n\t\t\t\t\t\t  IB_WC_REM_INV_REQ_ERR);\n\t\t\t\tstate = RESPST_COMPLETE;\n\t\t\t} else if (qp->srq) {\n\t\t\t\t \n\t\t\t\tqp->resp.status = IB_WC_REM_INV_REQ_ERR;\n\t\t\t\tstate = RESPST_COMPLETE;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tqp->resp.drop_msg = 1;\n\t\t\t\tstate = RESPST_CLEANUP;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_MALFORMED_WQE:\n\t\t\t \n\t\t\tdo_class_ac_error(qp, AETH_NAK_REM_OP_ERR,\n\t\t\t\t\t  IB_WC_LOC_QP_OP_ERR);\n\t\t\tstate = RESPST_COMPLETE;\n\t\t\tbreak;\n\n\t\tcase RESPST_ERR_CQ_OVERFLOW:\n\t\t\t \n\t\t\tstate = RESPST_ERROR;\n\t\t\tbreak;\n\n\t\tcase RESPST_DONE:\n\t\t\tif (qp->resp.goto_error) {\n\t\t\t\tstate = RESPST_ERROR;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgoto done;\n\n\t\tcase RESPST_EXIT:\n\t\t\tif (qp->resp.goto_error) {\n\t\t\t\tstate = RESPST_ERROR;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgoto exit;\n\n\t\tcase RESPST_ERROR:\n\t\t\tqp->resp.goto_error = 0;\n\t\t\trxe_dbg_qp(qp, \"moved to error state\\n\");\n\t\t\trxe_qp_error(qp);\n\t\t\tgoto exit;\n\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t}\n\n\t \ndone:\n\tret = 0;\n\tgoto out;\nexit:\n\tret = -EAGAIN;\nout:\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}