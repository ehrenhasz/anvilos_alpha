{
  "module_name": "rxe_req.c",
  "hash_id": "68efbfed680342170eb036729c70491bc6fa249f8e473e6b5016d7314472de8f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/rxe/rxe_req.c",
  "human_readable_source": "\n \n\n#include <linux/skbuff.h>\n#include <crypto/hash.h>\n\n#include \"rxe.h\"\n#include \"rxe_loc.h\"\n#include \"rxe_queue.h\"\n\nstatic int next_opcode(struct rxe_qp *qp, struct rxe_send_wqe *wqe,\n\t\t       u32 opcode);\n\nstatic inline void retry_first_write_send(struct rxe_qp *qp,\n\t\t\t\t\t  struct rxe_send_wqe *wqe, int npsn)\n{\n\tint i;\n\n\tfor (i = 0; i < npsn; i++) {\n\t\tint to_send = (wqe->dma.resid > qp->mtu) ?\n\t\t\t\tqp->mtu : wqe->dma.resid;\n\n\t\tqp->req.opcode = next_opcode(qp, wqe,\n\t\t\t\t\t     wqe->wr.opcode);\n\n\t\tif (wqe->wr.send_flags & IB_SEND_INLINE) {\n\t\t\twqe->dma.resid -= to_send;\n\t\t\twqe->dma.sge_offset += to_send;\n\t\t} else {\n\t\t\tadvance_dma_data(&wqe->dma, to_send);\n\t\t}\n\t}\n}\n\nstatic void req_retry(struct rxe_qp *qp)\n{\n\tstruct rxe_send_wqe *wqe;\n\tunsigned int wqe_index;\n\tunsigned int mask;\n\tint npsn;\n\tint first = 1;\n\tstruct rxe_queue *q = qp->sq.queue;\n\tunsigned int cons;\n\tunsigned int prod;\n\n\tcons = queue_get_consumer(q, QUEUE_TYPE_FROM_CLIENT);\n\tprod = queue_get_producer(q, QUEUE_TYPE_FROM_CLIENT);\n\n\tqp->req.wqe_index\t= cons;\n\tqp->req.psn\t\t= qp->comp.psn;\n\tqp->req.opcode\t\t= -1;\n\n\tfor (wqe_index = cons; wqe_index != prod;\n\t\t\twqe_index = queue_next_index(q, wqe_index)) {\n\t\twqe = queue_addr_from_index(qp->sq.queue, wqe_index);\n\t\tmask = wr_opcode_mask(wqe->wr.opcode, qp);\n\n\t\tif (wqe->state == wqe_state_posted)\n\t\t\tbreak;\n\n\t\tif (wqe->state == wqe_state_done)\n\t\t\tcontinue;\n\n\t\twqe->iova = (mask & WR_ATOMIC_MASK) ?\n\t\t\t     wqe->wr.wr.atomic.remote_addr :\n\t\t\t     (mask & WR_READ_OR_WRITE_MASK) ?\n\t\t\t     wqe->wr.wr.rdma.remote_addr :\n\t\t\t     0;\n\n\t\tif (!first || (mask & WR_READ_MASK) == 0) {\n\t\t\twqe->dma.resid = wqe->dma.length;\n\t\t\twqe->dma.cur_sge = 0;\n\t\t\twqe->dma.sge_offset = 0;\n\t\t}\n\n\t\tif (first) {\n\t\t\tfirst = 0;\n\n\t\t\tif (mask & WR_WRITE_OR_SEND_MASK) {\n\t\t\t\tnpsn = (qp->comp.psn - wqe->first_psn) &\n\t\t\t\t\tBTH_PSN_MASK;\n\t\t\t\tretry_first_write_send(qp, wqe, npsn);\n\t\t\t}\n\n\t\t\tif (mask & WR_READ_MASK) {\n\t\t\t\tnpsn = (wqe->dma.length - wqe->dma.resid) /\n\t\t\t\t\tqp->mtu;\n\t\t\t\twqe->iova += npsn * qp->mtu;\n\t\t\t}\n\t\t}\n\n\t\twqe->state = wqe_state_posted;\n\t}\n}\n\nvoid rnr_nak_timer(struct timer_list *t)\n{\n\tstruct rxe_qp *qp = from_timer(qp, t, rnr_nak_timer);\n\tunsigned long flags;\n\n\trxe_dbg_qp(qp, \"nak timer fired\\n\");\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (qp->valid) {\n\t\t \n\t\tqp->req.need_retry = 1;\n\t\tqp->req.wait_for_rnr_timer = 0;\n\t\trxe_sched_task(&qp->req.task);\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n}\n\nstatic void req_check_sq_drain_done(struct rxe_qp *qp)\n{\n\tstruct rxe_queue *q;\n\tunsigned int index;\n\tunsigned int cons;\n\tstruct rxe_send_wqe *wqe;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (qp_state(qp) == IB_QPS_SQD) {\n\t\tq = qp->sq.queue;\n\t\tindex = qp->req.wqe_index;\n\t\tcons = queue_get_consumer(q, QUEUE_TYPE_FROM_CLIENT);\n\t\twqe = queue_addr_from_index(q, cons);\n\n\t\t \n\t\tdo {\n\t\t\tif (!qp->attr.sq_draining)\n\t\t\t\t \n\t\t\t\tbreak;\n\n\t\t\tif (wqe && ((index != cons) ||\n\t\t\t\t(wqe->state != wqe_state_posted)))\n\t\t\t\t \n\t\t\t\tbreak;\n\n\t\t\tqp->attr.sq_draining = 0;\n\t\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\t\t\tif (qp->ibqp.event_handler) {\n\t\t\t\tstruct ib_event ev;\n\n\t\t\t\tev.device = qp->ibqp.device;\n\t\t\t\tev.element.qp = &qp->ibqp;\n\t\t\t\tev.event = IB_EVENT_SQ_DRAINED;\n\t\t\t\tqp->ibqp.event_handler(&ev,\n\t\t\t\t\tqp->ibqp.qp_context);\n\t\t\t}\n\t\t\treturn;\n\t\t} while (0);\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n}\n\nstatic struct rxe_send_wqe *__req_next_wqe(struct rxe_qp *qp)\n{\n\tstruct rxe_queue *q = qp->sq.queue;\n\tunsigned int index = qp->req.wqe_index;\n\tunsigned int prod;\n\n\tprod = queue_get_producer(q, QUEUE_TYPE_FROM_CLIENT);\n\tif (index == prod)\n\t\treturn NULL;\n\telse\n\t\treturn queue_addr_from_index(q, index);\n}\n\nstatic struct rxe_send_wqe *req_next_wqe(struct rxe_qp *qp)\n{\n\tstruct rxe_send_wqe *wqe;\n\tunsigned long flags;\n\n\treq_check_sq_drain_done(qp);\n\n\twqe = __req_next_wqe(qp);\n\tif (wqe == NULL)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (unlikely((qp_state(qp) == IB_QPS_SQD) &&\n\t\t     (wqe->state != wqe_state_processing))) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\treturn NULL;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\twqe->mask = wr_opcode_mask(wqe->wr.opcode, qp);\n\treturn wqe;\n}\n\n \nstatic int rxe_wqe_is_fenced(struct rxe_qp *qp, struct rxe_send_wqe *wqe)\n{\n\t \n\tif (wqe->wr.opcode == IB_WR_LOCAL_INV)\n\t\treturn qp->req.wqe_index != queue_get_consumer(qp->sq.queue,\n\t\t\t\t\t\tQUEUE_TYPE_FROM_CLIENT);\n\n\t \n\treturn (wqe->wr.send_flags & IB_SEND_FENCE) &&\n\t\tatomic_read(&qp->req.rd_atomic) != qp->attr.max_rd_atomic;\n}\n\nstatic int next_opcode_rc(struct rxe_qp *qp, u32 opcode, int fits)\n{\n\tswitch (opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\t\tif (qp->req.opcode == IB_OPCODE_RC_RDMA_WRITE_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_RC_RDMA_WRITE_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_LAST :\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_ONLY :\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_FIRST;\n\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tif (qp->req.opcode == IB_OPCODE_RC_RDMA_WRITE_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_RC_RDMA_WRITE_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_LAST_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_ONLY_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_RC_RDMA_WRITE_FIRST;\n\n\tcase IB_WR_SEND:\n\t\tif (qp->req.opcode == IB_OPCODE_RC_SEND_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_RC_SEND_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_SEND_LAST :\n\t\t\t\tIB_OPCODE_RC_SEND_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_SEND_ONLY :\n\t\t\t\tIB_OPCODE_RC_SEND_FIRST;\n\n\tcase IB_WR_SEND_WITH_IMM:\n\t\tif (qp->req.opcode == IB_OPCODE_RC_SEND_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_RC_SEND_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_SEND_LAST_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_RC_SEND_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_RC_SEND_ONLY_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_RC_SEND_FIRST;\n\n\tcase IB_WR_FLUSH:\n\t\treturn IB_OPCODE_RC_FLUSH;\n\n\tcase IB_WR_RDMA_READ:\n\t\treturn IB_OPCODE_RC_RDMA_READ_REQUEST;\n\n\tcase IB_WR_ATOMIC_CMP_AND_SWP:\n\t\treturn IB_OPCODE_RC_COMPARE_SWAP;\n\n\tcase IB_WR_ATOMIC_FETCH_AND_ADD:\n\t\treturn IB_OPCODE_RC_FETCH_ADD;\n\n\tcase IB_WR_SEND_WITH_INV:\n\t\tif (qp->req.opcode == IB_OPCODE_RC_SEND_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_RC_SEND_MIDDLE)\n\t\t\treturn fits ? IB_OPCODE_RC_SEND_LAST_WITH_INVALIDATE :\n\t\t\t\tIB_OPCODE_RC_SEND_MIDDLE;\n\t\telse\n\t\t\treturn fits ? IB_OPCODE_RC_SEND_ONLY_WITH_INVALIDATE :\n\t\t\t\tIB_OPCODE_RC_SEND_FIRST;\n\n\tcase IB_WR_ATOMIC_WRITE:\n\t\treturn IB_OPCODE_RC_ATOMIC_WRITE;\n\n\tcase IB_WR_REG_MR:\n\tcase IB_WR_LOCAL_INV:\n\t\treturn opcode;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int next_opcode_uc(struct rxe_qp *qp, u32 opcode, int fits)\n{\n\tswitch (opcode) {\n\tcase IB_WR_RDMA_WRITE:\n\t\tif (qp->req.opcode == IB_OPCODE_UC_RDMA_WRITE_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_UC_RDMA_WRITE_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_LAST :\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_ONLY :\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_FIRST;\n\n\tcase IB_WR_RDMA_WRITE_WITH_IMM:\n\t\tif (qp->req.opcode == IB_OPCODE_UC_RDMA_WRITE_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_UC_RDMA_WRITE_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_LAST_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_ONLY_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_UC_RDMA_WRITE_FIRST;\n\n\tcase IB_WR_SEND:\n\t\tif (qp->req.opcode == IB_OPCODE_UC_SEND_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_UC_SEND_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_SEND_LAST :\n\t\t\t\tIB_OPCODE_UC_SEND_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_SEND_ONLY :\n\t\t\t\tIB_OPCODE_UC_SEND_FIRST;\n\n\tcase IB_WR_SEND_WITH_IMM:\n\t\tif (qp->req.opcode == IB_OPCODE_UC_SEND_FIRST ||\n\t\t    qp->req.opcode == IB_OPCODE_UC_SEND_MIDDLE)\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_SEND_LAST_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_UC_SEND_MIDDLE;\n\t\telse\n\t\t\treturn fits ?\n\t\t\t\tIB_OPCODE_UC_SEND_ONLY_WITH_IMMEDIATE :\n\t\t\t\tIB_OPCODE_UC_SEND_FIRST;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int next_opcode(struct rxe_qp *qp, struct rxe_send_wqe *wqe,\n\t\t       u32 opcode)\n{\n\tint fits = (wqe->dma.resid <= qp->mtu);\n\n\tswitch (qp_type(qp)) {\n\tcase IB_QPT_RC:\n\t\treturn next_opcode_rc(qp, opcode, fits);\n\n\tcase IB_QPT_UC:\n\t\treturn next_opcode_uc(qp, opcode, fits);\n\n\tcase IB_QPT_UD:\n\tcase IB_QPT_GSI:\n\t\tswitch (opcode) {\n\t\tcase IB_WR_SEND:\n\t\t\treturn IB_OPCODE_UD_SEND_ONLY;\n\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\treturn IB_OPCODE_UD_SEND_ONLY_WITH_IMMEDIATE;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic inline int check_init_depth(struct rxe_qp *qp, struct rxe_send_wqe *wqe)\n{\n\tint depth;\n\n\tif (wqe->has_rd_atomic)\n\t\treturn 0;\n\n\tqp->req.need_rd_atomic = 1;\n\tdepth = atomic_dec_return(&qp->req.rd_atomic);\n\n\tif (depth >= 0) {\n\t\tqp->req.need_rd_atomic = 0;\n\t\twqe->has_rd_atomic = 1;\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&qp->req.rd_atomic);\n\treturn -EAGAIN;\n}\n\nstatic inline int get_mtu(struct rxe_qp *qp)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\n\tif ((qp_type(qp) == IB_QPT_RC) || (qp_type(qp) == IB_QPT_UC))\n\t\treturn qp->mtu;\n\n\treturn rxe->port.mtu_cap;\n}\n\nstatic struct sk_buff *init_req_packet(struct rxe_qp *qp,\n\t\t\t\t       struct rxe_av *av,\n\t\t\t\t       struct rxe_send_wqe *wqe,\n\t\t\t\t       int opcode, u32 payload,\n\t\t\t\t       struct rxe_pkt_info *pkt)\n{\n\tstruct rxe_dev\t\t*rxe = to_rdev(qp->ibqp.device);\n\tstruct sk_buff\t\t*skb;\n\tstruct rxe_send_wr\t*ibwr = &wqe->wr;\n\tint\t\t\tpad = (-payload) & 0x3;\n\tint\t\t\tpaylen;\n\tint\t\t\tsolicited;\n\tu32\t\t\tqp_num;\n\tint\t\t\tack_req;\n\n\t \n\tpaylen = rxe_opcode[opcode].length + payload + pad + RXE_ICRC_SIZE;\n\tpkt->paylen = paylen;\n\n\t \n\tskb = rxe_init_packet(rxe, av, paylen, pkt);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tsolicited = (ibwr->send_flags & IB_SEND_SOLICITED) &&\n\t\t\t(pkt->mask & RXE_END_MASK) &&\n\t\t\t((pkt->mask & (RXE_SEND_MASK)) ||\n\t\t\t(pkt->mask & (RXE_WRITE_MASK | RXE_IMMDT_MASK)) ==\n\t\t\t(RXE_WRITE_MASK | RXE_IMMDT_MASK));\n\n\tqp_num = (pkt->mask & RXE_DETH_MASK) ? ibwr->wr.ud.remote_qpn :\n\t\t\t\t\t qp->attr.dest_qp_num;\n\n\tack_req = ((pkt->mask & RXE_END_MASK) ||\n\t\t(qp->req.noack_pkts++ > RXE_MAX_PKT_PER_ACK));\n\tif (ack_req)\n\t\tqp->req.noack_pkts = 0;\n\n\tbth_init(pkt, pkt->opcode, solicited, 0, pad, IB_DEFAULT_PKEY_FULL, qp_num,\n\t\t ack_req, pkt->psn);\n\n\t \n\tif (pkt->mask & RXE_RETH_MASK) {\n\t\tif (pkt->mask & RXE_FETH_MASK)\n\t\t\treth_set_rkey(pkt, ibwr->wr.flush.rkey);\n\t\telse\n\t\t\treth_set_rkey(pkt, ibwr->wr.rdma.rkey);\n\t\treth_set_va(pkt, wqe->iova);\n\t\treth_set_len(pkt, wqe->dma.resid);\n\t}\n\n\t \n\tif (pkt->mask & RXE_FETH_MASK)\n\t\tfeth_init(pkt, ibwr->wr.flush.type, ibwr->wr.flush.level);\n\n\tif (pkt->mask & RXE_IMMDT_MASK)\n\t\timmdt_set_imm(pkt, ibwr->ex.imm_data);\n\n\tif (pkt->mask & RXE_IETH_MASK)\n\t\tieth_set_rkey(pkt, ibwr->ex.invalidate_rkey);\n\n\tif (pkt->mask & RXE_ATMETH_MASK) {\n\t\tatmeth_set_va(pkt, wqe->iova);\n\t\tif (opcode == IB_OPCODE_RC_COMPARE_SWAP) {\n\t\t\tatmeth_set_swap_add(pkt, ibwr->wr.atomic.swap);\n\t\t\tatmeth_set_comp(pkt, ibwr->wr.atomic.compare_add);\n\t\t} else {\n\t\t\tatmeth_set_swap_add(pkt, ibwr->wr.atomic.compare_add);\n\t\t}\n\t\tatmeth_set_rkey(pkt, ibwr->wr.atomic.rkey);\n\t}\n\n\tif (pkt->mask & RXE_DETH_MASK) {\n\t\tif (qp->ibqp.qp_num == 1)\n\t\t\tdeth_set_qkey(pkt, GSI_QKEY);\n\t\telse\n\t\t\tdeth_set_qkey(pkt, ibwr->wr.ud.remote_qkey);\n\t\tdeth_set_sqp(pkt, qp->ibqp.qp_num);\n\t}\n\n\treturn skb;\n}\n\nstatic int finish_packet(struct rxe_qp *qp, struct rxe_av *av,\n\t\t\t struct rxe_send_wqe *wqe, struct rxe_pkt_info *pkt,\n\t\t\t struct sk_buff *skb, u32 payload)\n{\n\tint err;\n\n\terr = rxe_prepare(av, pkt, skb);\n\tif (err)\n\t\treturn err;\n\n\tif (pkt->mask & RXE_WRITE_OR_SEND_MASK) {\n\t\tif (wqe->wr.send_flags & IB_SEND_INLINE) {\n\t\t\tu8 *tmp = &wqe->dma.inline_data[wqe->dma.sge_offset];\n\n\t\t\tmemcpy(payload_addr(pkt), tmp, payload);\n\n\t\t\twqe->dma.resid -= payload;\n\t\t\twqe->dma.sge_offset += payload;\n\t\t} else {\n\t\t\terr = copy_data(qp->pd, 0, &wqe->dma,\n\t\t\t\t\tpayload_addr(pkt), payload,\n\t\t\t\t\tRXE_FROM_MR_OBJ);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tif (bth_pad(pkt)) {\n\t\t\tu8 *pad = payload_addr(pkt) + payload;\n\n\t\t\tmemset(pad, 0, bth_pad(pkt));\n\t\t}\n\t} else if (pkt->mask & RXE_FLUSH_MASK) {\n\t\t \n\t\twqe->dma.resid = 0;\n\t}\n\n\tif (pkt->mask & RXE_ATOMIC_WRITE_MASK) {\n\t\tmemcpy(payload_addr(pkt), wqe->dma.atomic_wr, payload);\n\t\twqe->dma.resid -= payload;\n\t}\n\n\treturn 0;\n}\n\nstatic void update_wqe_state(struct rxe_qp *qp,\n\t\tstruct rxe_send_wqe *wqe,\n\t\tstruct rxe_pkt_info *pkt)\n{\n\tif (pkt->mask & RXE_END_MASK) {\n\t\tif (qp_type(qp) == IB_QPT_RC)\n\t\t\twqe->state = wqe_state_pending;\n\t} else {\n\t\twqe->state = wqe_state_processing;\n\t}\n}\n\nstatic void update_wqe_psn(struct rxe_qp *qp,\n\t\t\t   struct rxe_send_wqe *wqe,\n\t\t\t   struct rxe_pkt_info *pkt,\n\t\t\t   u32 payload)\n{\n\t \n\tint num_pkt = (wqe->dma.resid + payload + qp->mtu - 1) / qp->mtu;\n\n\t \n\tif (num_pkt == 0)\n\t\tnum_pkt = 1;\n\n\tif (pkt->mask & RXE_START_MASK) {\n\t\twqe->first_psn = qp->req.psn;\n\t\twqe->last_psn = (qp->req.psn + num_pkt - 1) & BTH_PSN_MASK;\n\t}\n\n\tif (pkt->mask & RXE_READ_MASK)\n\t\tqp->req.psn = (wqe->first_psn + num_pkt) & BTH_PSN_MASK;\n\telse\n\t\tqp->req.psn = (qp->req.psn + 1) & BTH_PSN_MASK;\n}\n\nstatic void save_state(struct rxe_send_wqe *wqe,\n\t\t       struct rxe_qp *qp,\n\t\t       struct rxe_send_wqe *rollback_wqe,\n\t\t       u32 *rollback_psn)\n{\n\trollback_wqe->state = wqe->state;\n\trollback_wqe->first_psn = wqe->first_psn;\n\trollback_wqe->last_psn = wqe->last_psn;\n\trollback_wqe->dma = wqe->dma;\n\t*rollback_psn = qp->req.psn;\n}\n\nstatic void rollback_state(struct rxe_send_wqe *wqe,\n\t\t\t   struct rxe_qp *qp,\n\t\t\t   struct rxe_send_wqe *rollback_wqe,\n\t\t\t   u32 rollback_psn)\n{\n\twqe->state = rollback_wqe->state;\n\twqe->first_psn = rollback_wqe->first_psn;\n\twqe->last_psn = rollback_wqe->last_psn;\n\twqe->dma = rollback_wqe->dma;\n\tqp->req.psn = rollback_psn;\n}\n\nstatic void update_state(struct rxe_qp *qp, struct rxe_pkt_info *pkt)\n{\n\tqp->req.opcode = pkt->opcode;\n\n\tif (pkt->mask & RXE_END_MASK)\n\t\tqp->req.wqe_index = queue_next_index(qp->sq.queue,\n\t\t\t\t\t\t     qp->req.wqe_index);\n\n\tqp->need_req_skb = 0;\n\n\tif (qp->qp_timeout_jiffies && !timer_pending(&qp->retrans_timer))\n\t\tmod_timer(&qp->retrans_timer,\n\t\t\t  jiffies + qp->qp_timeout_jiffies);\n}\n\nstatic int rxe_do_local_ops(struct rxe_qp *qp, struct rxe_send_wqe *wqe)\n{\n\tu8 opcode = wqe->wr.opcode;\n\tu32 rkey;\n\tint ret;\n\n\tswitch (opcode) {\n\tcase IB_WR_LOCAL_INV:\n\t\trkey = wqe->wr.ex.invalidate_rkey;\n\t\tif (rkey_is_mw(rkey))\n\t\t\tret = rxe_invalidate_mw(qp, rkey);\n\t\telse\n\t\t\tret = rxe_invalidate_mr(qp, rkey);\n\n\t\tif (unlikely(ret)) {\n\t\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase IB_WR_REG_MR:\n\t\tret = rxe_reg_fast_mr(qp, wqe);\n\t\tif (unlikely(ret)) {\n\t\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase IB_WR_BIND_MW:\n\t\tret = rxe_bind_mw(qp, wqe);\n\t\tif (unlikely(ret)) {\n\t\t\twqe->status = IB_WC_MW_BIND_ERR;\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\trxe_dbg_qp(qp, \"Unexpected send wqe opcode %d\\n\", opcode);\n\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\treturn -EINVAL;\n\t}\n\n\twqe->state = wqe_state_done;\n\twqe->status = IB_WC_SUCCESS;\n\tqp->req.wqe_index = queue_next_index(qp->sq.queue, qp->req.wqe_index);\n\n\t \n\trxe_sched_task(&qp->comp.task);\n\n\treturn 0;\n}\n\nint rxe_requester(struct rxe_qp *qp)\n{\n\tstruct rxe_dev *rxe = to_rdev(qp->ibqp.device);\n\tstruct rxe_pkt_info pkt;\n\tstruct sk_buff *skb;\n\tstruct rxe_send_wqe *wqe;\n\tenum rxe_hdr_mask mask;\n\tu32 payload;\n\tint mtu;\n\tint opcode;\n\tint err;\n\tint ret;\n\tstruct rxe_send_wqe rollback_wqe;\n\tu32 rollback_psn;\n\tstruct rxe_queue *q = qp->sq.queue;\n\tstruct rxe_ah *ah;\n\tstruct rxe_av *av;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&qp->state_lock, flags);\n\tif (unlikely(!qp->valid)) {\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\tgoto exit;\n\t}\n\n\tif (unlikely(qp_state(qp) == IB_QPS_ERR)) {\n\t\twqe = __req_next_wqe(qp);\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\tif (wqe)\n\t\t\tgoto err;\n\t\telse\n\t\t\tgoto exit;\n\t}\n\n\tif (unlikely(qp_state(qp) == IB_QPS_RESET)) {\n\t\tqp->req.wqe_index = queue_get_consumer(q,\n\t\t\t\t\t\tQUEUE_TYPE_FROM_CLIENT);\n\t\tqp->req.opcode = -1;\n\t\tqp->req.need_rd_atomic = 0;\n\t\tqp->req.wait_psn = 0;\n\t\tqp->req.need_retry = 0;\n\t\tqp->req.wait_for_rnr_timer = 0;\n\t\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\t\tgoto exit;\n\t}\n\tspin_unlock_irqrestore(&qp->state_lock, flags);\n\n\t \n\tif (unlikely(qp->req.need_retry && !qp->req.wait_for_rnr_timer)) {\n\t\treq_retry(qp);\n\t\tqp->req.need_retry = 0;\n\t}\n\n\twqe = req_next_wqe(qp);\n\tif (unlikely(!wqe))\n\t\tgoto exit;\n\n\tif (rxe_wqe_is_fenced(qp, wqe)) {\n\t\tqp->req.wait_fence = 1;\n\t\tgoto exit;\n\t}\n\n\tif (wqe->mask & WR_LOCAL_OP_MASK) {\n\t\terr = rxe_do_local_ops(qp, wqe);\n\t\tif (unlikely(err))\n\t\t\tgoto err;\n\t\telse\n\t\t\tgoto done;\n\t}\n\n\tif (unlikely(qp_type(qp) == IB_QPT_RC &&\n\t\tpsn_compare(qp->req.psn, (qp->comp.psn +\n\t\t\t\tRXE_MAX_UNACKED_PSNS)) > 0)) {\n\t\tqp->req.wait_psn = 1;\n\t\tgoto exit;\n\t}\n\n\t \n\tif (unlikely(atomic_read(&qp->skb_out) >\n\t\t     RXE_INFLIGHT_SKBS_PER_QP_HIGH)) {\n\t\tqp->need_req_skb = 1;\n\t\tgoto exit;\n\t}\n\n\topcode = next_opcode(qp, wqe, wqe->wr.opcode);\n\tif (unlikely(opcode < 0)) {\n\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\tgoto err;\n\t}\n\n\tmask = rxe_opcode[opcode].mask;\n\tif (unlikely(mask & (RXE_READ_OR_ATOMIC_MASK |\n\t\t\tRXE_ATOMIC_WRITE_MASK))) {\n\t\tif (check_init_depth(qp, wqe))\n\t\t\tgoto exit;\n\t}\n\n\tmtu = get_mtu(qp);\n\tpayload = (mask & (RXE_WRITE_OR_SEND_MASK | RXE_ATOMIC_WRITE_MASK)) ?\n\t\t\twqe->dma.resid : 0;\n\tif (payload > mtu) {\n\t\tif (qp_type(qp) == IB_QPT_UD) {\n\t\t\t \n\n\t\t\t \n\t\t\twqe->first_psn = qp->req.psn;\n\t\t\twqe->last_psn = qp->req.psn;\n\t\t\tqp->req.psn = (qp->req.psn + 1) & BTH_PSN_MASK;\n\t\t\tqp->req.opcode = IB_OPCODE_UD_SEND_ONLY;\n\t\t\tqp->req.wqe_index = queue_next_index(qp->sq.queue,\n\t\t\t\t\t\t       qp->req.wqe_index);\n\t\t\twqe->state = wqe_state_done;\n\t\t\twqe->status = IB_WC_SUCCESS;\n\t\t\trxe_sched_task(&qp->comp.task);\n\t\t\tgoto done;\n\t\t}\n\t\tpayload = mtu;\n\t}\n\n\tpkt.rxe = rxe;\n\tpkt.opcode = opcode;\n\tpkt.qp = qp;\n\tpkt.psn = qp->req.psn;\n\tpkt.mask = rxe_opcode[opcode].mask;\n\tpkt.wqe = wqe;\n\n\t \n\tsave_state(wqe, qp, &rollback_wqe, &rollback_psn);\n\n\tav = rxe_get_av(&pkt, &ah);\n\tif (unlikely(!av)) {\n\t\trxe_dbg_qp(qp, \"Failed no address vector\\n\");\n\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\tgoto err;\n\t}\n\n\tskb = init_req_packet(qp, av, wqe, opcode, payload, &pkt);\n\tif (unlikely(!skb)) {\n\t\trxe_dbg_qp(qp, \"Failed allocating skb\\n\");\n\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\tif (ah)\n\t\t\trxe_put(ah);\n\t\tgoto err;\n\t}\n\n\terr = finish_packet(qp, av, wqe, &pkt, skb, payload);\n\tif (unlikely(err)) {\n\t\trxe_dbg_qp(qp, \"Error during finish packet\\n\");\n\t\tif (err == -EFAULT)\n\t\t\twqe->status = IB_WC_LOC_PROT_ERR;\n\t\telse\n\t\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\tkfree_skb(skb);\n\t\tif (ah)\n\t\t\trxe_put(ah);\n\t\tgoto err;\n\t}\n\n\tif (ah)\n\t\trxe_put(ah);\n\n\t \n\tupdate_wqe_state(qp, wqe, &pkt);\n\tupdate_wqe_psn(qp, wqe, &pkt, payload);\n\n\terr = rxe_xmit_packet(qp, &pkt, skb);\n\tif (err) {\n\t\tif (err != -EAGAIN) {\n\t\t\twqe->status = IB_WC_LOC_QP_OP_ERR;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\trollback_state(wqe, qp, &rollback_wqe, rollback_psn);\n\n\t\t \n\t\tqp->need_req_skb = 1;\n\n\t\trxe_sched_task(&qp->req.task);\n\t\tgoto exit;\n\t}\n\n\tupdate_state(qp, &pkt);\n\n\t \ndone:\n\tret = 0;\n\tgoto out;\nerr:\n\t \n\tqp->req.wqe_index = queue_next_index(qp->sq.queue, qp->req.wqe_index);\n\twqe->state = wqe_state_error;\n\trxe_qp_error(qp);\nexit:\n\tret = -EAGAIN;\nout:\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}