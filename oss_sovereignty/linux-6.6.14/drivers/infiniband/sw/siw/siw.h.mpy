{
  "module_name": "siw.h",
  "hash_id": "2415a958d3d4a13c041feddd5bc88b2f0dc3d6b4e1706b7265fc234ba3c23f3b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/siw/siw.h",
  "human_readable_source": " \n\n \n \n\n#ifndef _SIW_H\n#define _SIW_H\n\n#include <rdma/ib_verbs.h>\n#include <rdma/restrack.h>\n#include <linux/socket.h>\n#include <linux/skbuff.h>\n#include <crypto/hash.h>\n#include <linux/crc32.h>\n#include <linux/crc32c.h>\n\n#include <rdma/siw-abi.h>\n#include \"iwarp.h\"\n\n#define SIW_VENDOR_ID 0x626d74  \n#define SIW_VENDORT_PART_ID 0\n#define SIW_MAX_QP (1024 * 100)\n#define SIW_MAX_QP_WR (1024 * 32)\n#define SIW_MAX_ORD_QP 128\n#define SIW_MAX_IRD_QP 128\n#define SIW_MAX_SGE_PBL 256  \n#define SIW_MAX_SGE_RD 1  \n#define SIW_MAX_CQ (1024 * 100)\n#define SIW_MAX_CQE (SIW_MAX_QP_WR * 100)\n#define SIW_MAX_MR (SIW_MAX_QP * 10)\n#define SIW_MAX_PD SIW_MAX_QP\n#define SIW_MAX_MW 0  \n#define SIW_MAX_SRQ SIW_MAX_QP\n#define SIW_MAX_SRQ_WR (SIW_MAX_QP_WR * 10)\n#define SIW_MAX_CONTEXT SIW_MAX_PD\n\n \n#define SENDPAGE_THRESH PAGE_SIZE\n\n \n#define SQ_USER_MAXBURST 100\n\n \n#define SIW_IRQ_MAXBURST_SQ_ACTIVE 4\n\nstruct siw_dev_cap {\n\tint max_qp;\n\tint max_qp_wr;\n\tint max_ord;  \n\tint max_ird;  \n\tint max_sge;\n\tint max_sge_rd;\n\tint max_cq;\n\tint max_cqe;\n\tint max_mr;\n\tint max_pd;\n\tint max_mw;\n\tint max_srq;\n\tint max_srq_wr;\n\tint max_srq_sge;\n};\n\nstruct siw_pd {\n\tstruct ib_pd base_pd;\n};\n\nstruct siw_device {\n\tstruct ib_device base_dev;\n\tstruct net_device *netdev;\n\tstruct siw_dev_cap attrs;\n\n\tu32 vendor_part_id;\n\tint numa_node;\n\tchar raw_gid[ETH_ALEN];\n\n\t \n\tenum ib_port_state state;\n\n\tspinlock_t lock;\n\n\tstruct xarray qp_xa;\n\tstruct xarray mem_xa;\n\n\tstruct list_head cep_list;\n\tstruct list_head qp_list;\n\n\t \n\tatomic_t num_qp;\n\tatomic_t num_cq;\n\tatomic_t num_pd;\n\tatomic_t num_mr;\n\tatomic_t num_srq;\n\tatomic_t num_ctx;\n\n\tstruct work_struct netdev_down;\n};\n\nstruct siw_ucontext {\n\tstruct ib_ucontext base_ucontext;\n\tstruct siw_device *sdev;\n};\n\n \n#define IWARP_ACCESS_MASK\t\t\t\t\t\\\n\t(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE\t|\t\\\n\t IB_ACCESS_REMOTE_READ)\n\n \n\nstruct siw_page_chunk {\n\tstruct page **plist;\n};\n\nstruct siw_umem {\n\tstruct siw_page_chunk *page_chunk;\n\tint num_pages;\n\tbool writable;\n\tu64 fp_addr;  \n\tstruct mm_struct *owning_mm;\n};\n\nstruct siw_pble {\n\tdma_addr_t addr;  \n\tunsigned int size;  \n\tunsigned long pbl_off;  \n};\n\nstruct siw_pbl {\n\tunsigned int num_buf;\n\tunsigned int max_buf;\n\tstruct siw_pble pbe[];\n};\n\n \nstruct siw_mem {\n\tstruct siw_device *sdev;\n\tstruct kref ref;\n\tu64 va;  \n\tu64 len;  \n\tu32 stag;  \n\tu8 stag_valid;  \n\tu8 is_pbl;  \n\tu8 is_mw;  \n\tenum ib_access_flags perms;  \n\tunion {\n\t\tstruct siw_umem *umem;\n\t\tstruct siw_pbl *pbl;\n\t\tvoid *mem_obj;\n\t};\n\tstruct ib_pd *pd;\n};\n\nstruct siw_mr {\n\tstruct ib_mr base_mr;\n\tstruct siw_mem *mem;\n\tstruct rcu_head rcu;\n};\n\n \nenum siw_access_state {\n\tE_ACCESS_OK,\n\tE_STAG_INVALID,\n\tE_BASE_BOUNDS,\n\tE_ACCESS_PERM,\n\tE_PD_MISMATCH\n};\n\nenum siw_wr_state {\n\tSIW_WR_IDLE,\n\tSIW_WR_QUEUED,  \n\tSIW_WR_INPROGRESS  \n};\n\n \nstruct siw_wqe {\n\t \n\tunion {\n\t\tstruct siw_sqe sqe;\n\t\tstruct siw_rqe rqe;\n\t};\n\tstruct siw_mem *mem[SIW_MAX_SGE];  \n\tenum siw_wr_state wr_status;\n\tenum siw_wc_status wc_status;\n\tu32 bytes;  \n\tu32 processed;  \n};\n\nstruct siw_cq {\n\tstruct ib_cq base_cq;\n\tspinlock_t lock;\n\tstruct siw_cq_ctrl *notify;\n\tstruct siw_cqe *queue;\n\tu32 cq_put;\n\tu32 cq_get;\n\tu32 num_cqe;\n\tstruct rdma_user_mmap_entry *cq_entry;  \n\tu32 id;  \n};\n\nenum siw_qp_state {\n\tSIW_QP_STATE_IDLE,\n\tSIW_QP_STATE_RTR,\n\tSIW_QP_STATE_RTS,\n\tSIW_QP_STATE_CLOSING,\n\tSIW_QP_STATE_TERMINATE,\n\tSIW_QP_STATE_ERROR,\n\tSIW_QP_STATE_COUNT\n};\n\nenum siw_qp_flags {\n\tSIW_RDMA_BIND_ENABLED = (1 << 0),\n\tSIW_RDMA_WRITE_ENABLED = (1 << 1),\n\tSIW_RDMA_READ_ENABLED = (1 << 2),\n\tSIW_SIGNAL_ALL_WR = (1 << 3),\n\tSIW_MPA_CRC = (1 << 4),\n\tSIW_QP_IN_DESTROY = (1 << 5)\n};\n\nenum siw_qp_attr_mask {\n\tSIW_QP_ATTR_STATE = (1 << 0),\n\tSIW_QP_ATTR_ACCESS_FLAGS = (1 << 1),\n\tSIW_QP_ATTR_LLP_HANDLE = (1 << 2),\n\tSIW_QP_ATTR_ORD = (1 << 3),\n\tSIW_QP_ATTR_IRD = (1 << 4),\n\tSIW_QP_ATTR_SQ_SIZE = (1 << 5),\n\tSIW_QP_ATTR_RQ_SIZE = (1 << 6),\n\tSIW_QP_ATTR_MPA = (1 << 7)\n};\n\nstruct siw_srq {\n\tstruct ib_srq base_srq;\n\tspinlock_t lock;\n\tu32 max_sge;\n\tu32 limit;  \n\tstruct siw_rqe *recvq;\n\tu32 rq_put;\n\tu32 rq_get;\n\tu32 num_rqe;  \n\tstruct rdma_user_mmap_entry *srq_entry;  \n\tbool armed:1;  \n\tbool is_kernel_res:1;  \n};\n\nstruct siw_qp_attrs {\n\tenum siw_qp_state state;\n\tu32 sq_size;\n\tu32 rq_size;\n\tu32 orq_size;\n\tu32 irq_size;\n\tu32 sq_max_sges;\n\tu32 rq_max_sges;\n\tenum siw_qp_flags flags;\n\n\tstruct socket *sk;\n};\n\nenum siw_tx_ctx {\n\tSIW_SEND_HDR,  \n\tSIW_SEND_DATA,  \n\tSIW_SEND_TRAILER,  \n\tSIW_SEND_SHORT_FPDU \n};\n\nenum siw_rx_state {\n\tSIW_GET_HDR,  \n\tSIW_GET_DATA_START,  \n\tSIW_GET_DATA_MORE,  \n\tSIW_GET_TRAILER \n};\n\nstruct siw_rx_stream {\n\tstruct sk_buff *skb;\n\tint skb_new;  \n\tint skb_offset;  \n\tint skb_copied;  \n\n\tunion iwarp_hdr hdr;\n\tstruct mpa_trailer trailer;\n\n\tenum siw_rx_state state;\n\n\t \n\tint fpdu_part_rcvd;  \n\tint fpdu_part_rem;  \n\n\t \n\tu32 ddp_msn[RDMAP_UNTAGGED_QN_COUNT];\n\tu32 ddp_stag;\n\tu64 ddp_to;\n\tu32 inval_stag;  \n\n\tstruct shash_desc *mpa_crc_hd;\n\tu8 rx_suspend : 1;\n\tu8 pad : 2;  \n\tu8 rdmap_op : 4;  \n};\n\nstruct siw_rx_fpdu {\n\t \n\tstruct siw_wqe wqe_active;\n\n\tunsigned int pbl_idx;  \n\tunsigned int sge_idx;  \n\tunsigned int sge_off;  \n\n\tchar first_ddp_seg;  \n\tchar more_ddp_segs;  \n\tu8 prev_rdmap_op : 4;  \n};\n\n \nstruct siw_send_pkt {\n\tstruct iwarp_send send;\n\t__be32 crc;\n};\n\nstruct siw_write_pkt {\n\tstruct iwarp_rdma_write write;\n\t__be32 crc;\n};\n\nstruct siw_rreq_pkt {\n\tstruct iwarp_rdma_rreq rreq;\n\t__be32 crc;\n};\n\nstruct siw_rresp_pkt {\n\tstruct iwarp_rdma_rresp rresp;\n\t__be32 crc;\n};\n\nstruct siw_iwarp_tx {\n\tunion {\n\t\tunion iwarp_hdr hdr;\n\n\t\t \n\t\tstruct iwarp_ctrl ctrl;\n\t\tstruct iwarp_ctrl_untagged c_untagged;\n\t\tstruct iwarp_ctrl_tagged c_tagged;\n\n\t\t \n\t\tstruct iwarp_rdma_write rwrite;\n\t\tstruct iwarp_rdma_rreq rreq;\n\t\tstruct iwarp_rdma_rresp rresp;\n\t\tstruct iwarp_terminate terminate;\n\t\tstruct iwarp_send send;\n\t\tstruct iwarp_send_inv send_inv;\n\n\t\t \n\t\tstruct siw_send_pkt send_pkt;\n\t\tstruct siw_write_pkt write_pkt;\n\t\tstruct siw_rreq_pkt rreq_pkt;\n\t\tstruct siw_rresp_pkt rresp_pkt;\n\t} pkt;\n\n\tstruct mpa_trailer trailer;\n\t \n\tu32 ddp_msn[RDMAP_UNTAGGED_QN_COUNT];\n\n\tenum siw_tx_ctx state;\n\tu16 ctrl_len;  \n\tu16 ctrl_sent;\n\tint burst;\n\tint bytes_unsent;  \n\n\tstruct shash_desc *mpa_crc_hd;\n\n\tu8 do_crc : 1;  \n\tu8 use_sendpage : 1;  \n\tu8 tx_suspend : 1;  \n\tu8 pad : 2;  \n\tu8 orq_fence : 1;  \n\tu8 in_syscall : 1;  \n\tu8 zcopy_tx : 1;  \n\tu8 gso_seg_limit;  \n\n\tu16 fpdu_len;  \n\tunsigned int tcp_seglen;  \n\n\tstruct siw_wqe wqe_active;\n\n\tint pbl_idx;  \n\tint sge_idx;  \n\tu32 sge_off;  \n};\n\nstruct siw_qp {\n\tstruct ib_qp base_qp;\n\tstruct siw_device *sdev;\n\tstruct kref ref;\n\tstruct completion qp_free;\n\tstruct list_head devq;\n\tint tx_cpu;\n\tstruct siw_qp_attrs attrs;\n\n\tstruct siw_cep *cep;\n\tstruct rw_semaphore state_lock;\n\n\tstruct ib_pd *pd;\n\tstruct siw_cq *scq;\n\tstruct siw_cq *rcq;\n\tstruct siw_srq *srq;\n\n\tstruct siw_iwarp_tx tx_ctx;  \n\tspinlock_t sq_lock;\n\tstruct siw_sqe *sendq;  \n\tuint32_t sq_get;  \n\tuint32_t sq_put;  \n\tstruct llist_node tx_list;\n\n\tstruct siw_sqe *orq;  \n\tspinlock_t orq_lock;\n\tuint32_t orq_get;  \n\tuint32_t orq_put;  \n\n\tstruct siw_rx_stream rx_stream;\n\tstruct siw_rx_fpdu *rx_fpdu;\n\tstruct siw_rx_fpdu rx_tagged;\n\tstruct siw_rx_fpdu rx_untagged;\n\tspinlock_t rq_lock;\n\tstruct siw_rqe *recvq;  \n\tuint32_t rq_get;  \n\tuint32_t rq_put;  \n\n\tstruct siw_sqe *irq;  \n\tuint32_t irq_get;  \n\tuint32_t irq_put;  \n\tint irq_burst;\n\n\tstruct {  \n\t\tu8 valid;\n\t\tu8 in_tx;\n\t\tu8 layer : 4, etype : 4;\n\t\tu8 ecode;\n\t} term_info;\n\tstruct rdma_user_mmap_entry *sq_entry;  \n\tstruct rdma_user_mmap_entry *rq_entry;  \n\tstruct rcu_head rcu;\n};\n\n \n#define rx_qp(rx) container_of(rx, struct siw_qp, rx_stream)\n#define tx_qp(tx) container_of(tx, struct siw_qp, tx_ctx)\n#define tx_wqe(qp) (&(qp)->tx_ctx.wqe_active)\n#define rx_wqe(rctx) (&(rctx)->wqe_active)\n#define rx_mem(rctx) ((rctx)->wqe_active.mem[0])\n#define tx_type(wqe) ((wqe)->sqe.opcode)\n#define rx_type(wqe) ((wqe)->rqe.opcode)\n#define tx_flags(wqe) ((wqe)->sqe.flags)\n\nstruct iwarp_msg_info {\n\tint hdr_len;\n\tstruct iwarp_ctrl ctrl;\n\tint (*rx_data)(struct siw_qp *qp);\n};\n\nstruct siw_user_mmap_entry {\n\tstruct rdma_user_mmap_entry rdma_entry;\n\tvoid *address;\n};\n\n \nextern const bool zcopy_tx;\nextern const bool try_gso;\nextern const bool loopback_enabled;\nextern const bool mpa_crc_required;\nextern const bool mpa_crc_strict;\nextern const bool siw_tcp_nagle;\nextern u_char mpa_version;\nextern const bool peer_to_peer;\nextern struct task_struct *siw_tx_thread[];\n\nextern struct crypto_shash *siw_crypto_shash;\nextern struct iwarp_msg_info iwarp_pktinfo[RDMAP_TERMINATE + 1];\n\n \nint siw_qp_modify(struct siw_qp *qp, struct siw_qp_attrs *attr,\n\t\t  enum siw_qp_attr_mask mask);\nint siw_qp_mpa_rts(struct siw_qp *qp, enum mpa_v2_ctrl ctrl);\nvoid siw_qp_llp_close(struct siw_qp *qp);\nvoid siw_qp_cm_drop(struct siw_qp *qp, int schedule);\nvoid siw_send_terminate(struct siw_qp *qp);\n\nvoid siw_qp_get_ref(struct ib_qp *qp);\nvoid siw_qp_put_ref(struct ib_qp *qp);\nint siw_qp_add(struct siw_device *sdev, struct siw_qp *qp);\nvoid siw_free_qp(struct kref *ref);\n\nvoid siw_init_terminate(struct siw_qp *qp, enum term_elayer layer,\n\t\t\tu8 etype, u8 ecode, int in_tx);\nenum ddp_ecode siw_tagged_error(enum siw_access_state state);\nenum rdmap_ecode siw_rdmap_error(enum siw_access_state state);\n\nvoid siw_read_to_orq(struct siw_sqe *rreq, struct siw_sqe *sqe);\nint siw_sqe_complete(struct siw_qp *qp, struct siw_sqe *sqe, u32 bytes,\n\t\t     enum siw_wc_status status);\nint siw_rqe_complete(struct siw_qp *qp, struct siw_rqe *rqe, u32 bytes,\n\t\t     u32 inval_stag, enum siw_wc_status status);\nvoid siw_qp_llp_data_ready(struct sock *sk);\nvoid siw_qp_llp_write_space(struct sock *sk);\n\n \nint siw_create_tx_threads(void);\nvoid siw_stop_tx_threads(void);\nint siw_run_sq(void *arg);\nint siw_qp_sq_process(struct siw_qp *qp);\nint siw_sq_start(struct siw_qp *qp);\nint siw_activate_tx(struct siw_qp *qp);\nint siw_get_tx_cpu(struct siw_device *sdev);\nvoid siw_put_tx_cpu(int cpu);\n\n \nint siw_proc_send(struct siw_qp *qp);\nint siw_proc_rreq(struct siw_qp *qp);\nint siw_proc_rresp(struct siw_qp *qp);\nint siw_proc_write(struct siw_qp *qp);\nint siw_proc_terminate(struct siw_qp *qp);\n\nint siw_tcp_rx_data(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t    unsigned int off, size_t len);\n\nstatic inline void set_rx_fpdu_context(struct siw_qp *qp, u8 opcode)\n{\n\tif (opcode == RDMAP_RDMA_WRITE || opcode == RDMAP_RDMA_READ_RESP)\n\t\tqp->rx_fpdu = &qp->rx_tagged;\n\telse\n\t\tqp->rx_fpdu = &qp->rx_untagged;\n\n\tqp->rx_stream.rdmap_op = opcode;\n}\n\nstatic inline struct siw_ucontext *to_siw_ctx(struct ib_ucontext *base_ctx)\n{\n\treturn container_of(base_ctx, struct siw_ucontext, base_ucontext);\n}\n\nstatic inline struct siw_qp *to_siw_qp(struct ib_qp *base_qp)\n{\n\treturn container_of(base_qp, struct siw_qp, base_qp);\n}\n\nstatic inline struct siw_cq *to_siw_cq(struct ib_cq *base_cq)\n{\n\treturn container_of(base_cq, struct siw_cq, base_cq);\n}\n\nstatic inline struct siw_srq *to_siw_srq(struct ib_srq *base_srq)\n{\n\treturn container_of(base_srq, struct siw_srq, base_srq);\n}\n\nstatic inline struct siw_device *to_siw_dev(struct ib_device *base_dev)\n{\n\treturn container_of(base_dev, struct siw_device, base_dev);\n}\n\nstatic inline struct siw_mr *to_siw_mr(struct ib_mr *base_mr)\n{\n\treturn container_of(base_mr, struct siw_mr, base_mr);\n}\n\nstatic inline struct siw_user_mmap_entry *\nto_siw_mmap_entry(struct rdma_user_mmap_entry *rdma_mmap)\n{\n\treturn container_of(rdma_mmap, struct siw_user_mmap_entry, rdma_entry);\n}\n\nstatic inline struct siw_qp *siw_qp_id2obj(struct siw_device *sdev, int id)\n{\n\tstruct siw_qp *qp;\n\n\trcu_read_lock();\n\tqp = xa_load(&sdev->qp_xa, id);\n\tif (likely(qp && kref_get_unless_zero(&qp->ref))) {\n\t\trcu_read_unlock();\n\t\treturn qp;\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstatic inline u32 qp_id(struct siw_qp *qp)\n{\n\treturn qp->base_qp.qp_num;\n}\n\nstatic inline void siw_qp_get(struct siw_qp *qp)\n{\n\tkref_get(&qp->ref);\n}\n\nstatic inline void siw_qp_put(struct siw_qp *qp)\n{\n\tkref_put(&qp->ref, siw_free_qp);\n}\n\nstatic inline int siw_sq_empty(struct siw_qp *qp)\n{\n\tstruct siw_sqe *sqe = &qp->sendq[qp->sq_get % qp->attrs.sq_size];\n\n\treturn READ_ONCE(sqe->flags) == 0;\n}\n\nstatic inline struct siw_sqe *sq_get_next(struct siw_qp *qp)\n{\n\tstruct siw_sqe *sqe = &qp->sendq[qp->sq_get % qp->attrs.sq_size];\n\n\tif (READ_ONCE(sqe->flags) & SIW_WQE_VALID)\n\t\treturn sqe;\n\n\treturn NULL;\n}\n\nstatic inline struct siw_sqe *orq_get_current(struct siw_qp *qp)\n{\n\treturn &qp->orq[qp->orq_get % qp->attrs.orq_size];\n}\n\nstatic inline struct siw_sqe *orq_get_free(struct siw_qp *qp)\n{\n\tstruct siw_sqe *orq_e = &qp->orq[qp->orq_put % qp->attrs.orq_size];\n\n\tif (READ_ONCE(orq_e->flags) == 0)\n\t\treturn orq_e;\n\n\treturn NULL;\n}\n\nstatic inline int siw_orq_empty(struct siw_qp *qp)\n{\n\treturn qp->orq[qp->orq_get % qp->attrs.orq_size].flags == 0 ? 1 : 0;\n}\n\nstatic inline struct siw_sqe *irq_alloc_free(struct siw_qp *qp)\n{\n\tstruct siw_sqe *irq_e = &qp->irq[qp->irq_put % qp->attrs.irq_size];\n\n\tif (READ_ONCE(irq_e->flags) == 0) {\n\t\tqp->irq_put++;\n\t\treturn irq_e;\n\t}\n\treturn NULL;\n}\n\nstatic inline __wsum siw_csum_update(const void *buff, int len, __wsum sum)\n{\n\treturn (__force __wsum)crc32c((__force __u32)sum, buff, len);\n}\n\nstatic inline __wsum siw_csum_combine(__wsum csum, __wsum csum2, int offset,\n\t\t\t\t      int len)\n{\n\treturn (__force __wsum)__crc32c_le_combine((__force __u32)csum,\n\t\t\t\t\t\t   (__force __u32)csum2, len);\n}\n\nstatic inline void siw_crc_skb(struct siw_rx_stream *srx, unsigned int len)\n{\n\tconst struct skb_checksum_ops siw_cs_ops = {\n\t\t.update = siw_csum_update,\n\t\t.combine = siw_csum_combine,\n\t};\n\t__wsum crc = *(u32 *)shash_desc_ctx(srx->mpa_crc_hd);\n\n\tcrc = __skb_checksum(srx->skb, srx->skb_offset, len, crc,\n\t\t\t     &siw_cs_ops);\n\t*(u32 *)shash_desc_ctx(srx->mpa_crc_hd) = crc;\n}\n\n#define siw_dbg(ibdev, fmt, ...)                                               \\\n\tibdev_dbg(ibdev, \"%s: \" fmt, __func__, ##__VA_ARGS__)\n\n#define siw_dbg_qp(qp, fmt, ...)                                               \\\n\tibdev_dbg(&qp->sdev->base_dev, \"QP[%u] %s: \" fmt, qp_id(qp), __func__, \\\n\t\t  ##__VA_ARGS__)\n\n#define siw_dbg_cq(cq, fmt, ...)                                               \\\n\tibdev_dbg(cq->base_cq.device, \"CQ[%u] %s: \" fmt, cq->id, __func__,     \\\n\t\t  ##__VA_ARGS__)\n\n#define siw_dbg_pd(pd, fmt, ...)                                               \\\n\tibdev_dbg(pd->device, \"PD[%u] %s: \" fmt, pd->res.id, __func__,         \\\n\t\t  ##__VA_ARGS__)\n\n#define siw_dbg_mem(mem, fmt, ...)                                             \\\n\tibdev_dbg(&mem->sdev->base_dev,                                        \\\n\t\t  \"MEM[0x%08x] %s: \" fmt, mem->stag, __func__, ##__VA_ARGS__)\n\n#define siw_dbg_cep(cep, fmt, ...)                                             \\\n\tibdev_dbg(&cep->sdev->base_dev, \"CEP[0x%pK] %s: \" fmt,                 \\\n\t\t  cep, __func__, ##__VA_ARGS__)\n\nvoid siw_cq_flush(struct siw_cq *cq);\nvoid siw_sq_flush(struct siw_qp *qp);\nvoid siw_rq_flush(struct siw_qp *qp);\nint siw_reap_cqe(struct siw_cq *cq, struct ib_wc *wc);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}