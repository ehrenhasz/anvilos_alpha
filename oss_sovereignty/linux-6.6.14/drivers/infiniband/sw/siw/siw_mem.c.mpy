{
  "module_name": "siw_mem.c",
  "hash_id": "306f6e57aa678951aea2390e8b18c50ab683fbbef2caab935ad29ec7b6c4476d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/siw/siw_mem.c",
  "human_readable_source": "\n\n \n \n\n#include <linux/gfp.h>\n#include <rdma/ib_verbs.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n#include <linux/resource.h>\n\n#include \"siw.h\"\n#include \"siw_mem.h\"\n\n \nint siw_mem_add(struct siw_device *sdev, struct siw_mem *m)\n{\n\tstruct xa_limit limit = XA_LIMIT(1, 0x00ffffff);\n\tu32 id, next;\n\n\tget_random_bytes(&next, 4);\n\tnext &= 0x00ffffff;\n\n\tif (xa_alloc_cyclic(&sdev->mem_xa, &id, m, limit, &next,\n\t    GFP_KERNEL) < 0)\n\t\treturn -ENOMEM;\n\n\t \n\tm->stag = id << 8;\n\n\tsiw_dbg_mem(m, \"new MEM object\\n\");\n\n\treturn 0;\n}\n\n \nstruct siw_mem *siw_mem_id2obj(struct siw_device *sdev, int stag_index)\n{\n\tstruct siw_mem *mem;\n\n\trcu_read_lock();\n\tmem = xa_load(&sdev->mem_xa, stag_index);\n\tif (likely(mem && kref_get_unless_zero(&mem->ref))) {\n\t\trcu_read_unlock();\n\t\treturn mem;\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic void siw_free_plist(struct siw_page_chunk *chunk, int num_pages,\n\t\t\t   bool dirty)\n{\n\tunpin_user_pages_dirty_lock(chunk->plist, num_pages, dirty);\n}\n\nvoid siw_umem_release(struct siw_umem *umem, bool dirty)\n{\n\tstruct mm_struct *mm_s = umem->owning_mm;\n\tint i, num_pages = umem->num_pages;\n\n\tfor (i = 0; num_pages; i++) {\n\t\tint to_free = min_t(int, PAGES_PER_CHUNK, num_pages);\n\n\t\tsiw_free_plist(&umem->page_chunk[i], to_free,\n\t\t\t       umem->writable && dirty);\n\t\tkfree(umem->page_chunk[i].plist);\n\t\tnum_pages -= to_free;\n\t}\n\tatomic64_sub(umem->num_pages, &mm_s->pinned_vm);\n\n\tmmdrop(mm_s);\n\tkfree(umem->page_chunk);\n\tkfree(umem);\n}\n\nint siw_mr_add_mem(struct siw_mr *mr, struct ib_pd *pd, void *mem_obj,\n\t\t   u64 start, u64 len, int rights)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mem *mem = kzalloc(sizeof(*mem), GFP_KERNEL);\n\tstruct xa_limit limit = XA_LIMIT(1, 0x00ffffff);\n\tu32 id, next;\n\n\tif (!mem)\n\t\treturn -ENOMEM;\n\n\tmem->mem_obj = mem_obj;\n\tmem->stag_valid = 0;\n\tmem->sdev = sdev;\n\tmem->va = start;\n\tmem->len = len;\n\tmem->pd = pd;\n\tmem->perms = rights & IWARP_ACCESS_MASK;\n\tkref_init(&mem->ref);\n\n\tget_random_bytes(&next, 4);\n\tnext &= 0x00ffffff;\n\n\tif (xa_alloc_cyclic(&sdev->mem_xa, &id, mem, limit, &next,\n\t    GFP_KERNEL) < 0) {\n\t\tkfree(mem);\n\t\treturn -ENOMEM;\n\t}\n\n\tmr->mem = mem;\n\t \n\tmem->stag = id << 8;\n\tmr->base_mr.lkey = mr->base_mr.rkey = mem->stag;\n\n\treturn 0;\n}\n\nvoid siw_mr_drop_mem(struct siw_mr *mr)\n{\n\tstruct siw_mem *mem = mr->mem, *found;\n\n\tmem->stag_valid = 0;\n\n\t \n\tsmp_mb();\n\n\tfound = xa_erase(&mem->sdev->mem_xa, mem->stag >> 8);\n\tWARN_ON(found != mem);\n\tsiw_mem_put(mem);\n}\n\nvoid siw_free_mem(struct kref *ref)\n{\n\tstruct siw_mem *mem = container_of(ref, struct siw_mem, ref);\n\n\tsiw_dbg_mem(mem, \"free mem, pbl: %s\\n\", mem->is_pbl ? \"y\" : \"n\");\n\n\tif (!mem->is_mw && mem->mem_obj) {\n\t\tif (mem->is_pbl == 0)\n\t\t\tsiw_umem_release(mem->umem, true);\n\t\telse\n\t\t\tkfree(mem->pbl);\n\t}\n\tkfree(mem);\n}\n\n \nint siw_check_mem(struct ib_pd *pd, struct siw_mem *mem, u64 addr,\n\t\t  enum ib_access_flags perms, int len)\n{\n\tif (!mem->stag_valid) {\n\t\tsiw_dbg_pd(pd, \"STag 0x%08x invalid\\n\", mem->stag);\n\t\treturn -E_STAG_INVALID;\n\t}\n\tif (mem->pd != pd) {\n\t\tsiw_dbg_pd(pd, \"STag 0x%08x: PD mismatch\\n\", mem->stag);\n\t\treturn -E_PD_MISMATCH;\n\t}\n\t \n\tif ((mem->perms & perms) < perms) {\n\t\tsiw_dbg_pd(pd, \"permissions 0x%08x < 0x%08x\\n\",\n\t\t\t   mem->perms, perms);\n\t\treturn -E_ACCESS_PERM;\n\t}\n\t \n\tif (addr < mem->va || addr + len > mem->va + mem->len) {\n\t\tsiw_dbg_pd(pd, \"MEM interval len %d\\n\", len);\n\t\tsiw_dbg_pd(pd, \"[0x%pK, 0x%pK] out of bounds\\n\",\n\t\t\t   (void *)(uintptr_t)addr,\n\t\t\t   (void *)(uintptr_t)(addr + len));\n\t\tsiw_dbg_pd(pd, \"[0x%pK, 0x%pK] STag=0x%08x\\n\",\n\t\t\t   (void *)(uintptr_t)mem->va,\n\t\t\t   (void *)(uintptr_t)(mem->va + mem->len),\n\t\t\t   mem->stag);\n\n\t\treturn -E_BASE_BOUNDS;\n\t}\n\treturn E_ACCESS_OK;\n}\n\n \nint siw_check_sge(struct ib_pd *pd, struct siw_sge *sge, struct siw_mem *mem[],\n\t\t  enum ib_access_flags perms, u32 off, int len)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mem *new = NULL;\n\tint rv = E_ACCESS_OK;\n\n\tif (len + off > sge->length) {\n\t\trv = -E_BASE_BOUNDS;\n\t\tgoto fail;\n\t}\n\tif (*mem == NULL) {\n\t\tnew = siw_mem_id2obj(sdev, sge->lkey >> 8);\n\t\tif (unlikely(!new)) {\n\t\t\tsiw_dbg_pd(pd, \"STag unknown: 0x%08x\\n\", sge->lkey);\n\t\t\trv = -E_STAG_INVALID;\n\t\t\tgoto fail;\n\t\t}\n\t\t*mem = new;\n\t}\n\t \n\tif (unlikely((*mem)->stag != sge->lkey)) {\n\t\tsiw_dbg_mem((*mem), \"STag mismatch: 0x%08x\\n\", sge->lkey);\n\t\trv = -E_STAG_INVALID;\n\t\tgoto fail;\n\t}\n\trv = siw_check_mem(pd, *mem, sge->laddr + off, perms, len);\n\tif (unlikely(rv))\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tif (new) {\n\t\t*mem = NULL;\n\t\tsiw_mem_put(new);\n\t}\n\treturn rv;\n}\n\nvoid siw_wqe_put_mem(struct siw_wqe *wqe, enum siw_opcode op)\n{\n\tswitch (op) {\n\tcase SIW_OP_SEND:\n\tcase SIW_OP_WRITE:\n\tcase SIW_OP_SEND_WITH_IMM:\n\tcase SIW_OP_SEND_REMOTE_INV:\n\tcase SIW_OP_READ:\n\tcase SIW_OP_READ_LOCAL_INV:\n\t\tif (!(wqe->sqe.flags & SIW_WQE_INLINE))\n\t\t\tsiw_unref_mem_sgl(wqe->mem, wqe->sqe.num_sge);\n\t\tbreak;\n\n\tcase SIW_OP_RECEIVE:\n\t\tsiw_unref_mem_sgl(wqe->mem, wqe->rqe.num_sge);\n\t\tbreak;\n\n\tcase SIW_OP_READ_RESPONSE:\n\t\tsiw_unref_mem_sgl(wqe->mem, 1);\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n}\n\nint siw_invalidate_stag(struct ib_pd *pd, u32 stag)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mem *mem = siw_mem_id2obj(sdev, stag >> 8);\n\tint rv = 0;\n\n\tif (unlikely(!mem)) {\n\t\tsiw_dbg_pd(pd, \"STag 0x%08x unknown\\n\", stag);\n\t\treturn -EINVAL;\n\t}\n\tif (unlikely(mem->pd != pd)) {\n\t\tsiw_dbg_pd(pd, \"PD mismatch for STag 0x%08x\\n\", stag);\n\t\trv = -EACCES;\n\t\tgoto out;\n\t}\n\t \n\tmem->stag_valid = 0;\n\n\tsiw_dbg_pd(pd, \"STag 0x%08x now invalid\\n\", stag);\nout:\n\tsiw_mem_put(mem);\n\treturn rv;\n}\n\n \ndma_addr_t siw_pbl_get_buffer(struct siw_pbl *pbl, u64 off, int *len, int *idx)\n{\n\tint i = idx ? *idx : 0;\n\n\twhile (i < pbl->num_buf) {\n\t\tstruct siw_pble *pble = &pbl->pbe[i];\n\n\t\tif (pble->pbl_off + pble->size > off) {\n\t\t\tu64 pble_off = off - pble->pbl_off;\n\n\t\t\tif (len)\n\t\t\t\t*len = pble->size - pble_off;\n\t\t\tif (idx)\n\t\t\t\t*idx = i;\n\n\t\t\treturn pble->addr + pble_off;\n\t\t}\n\t\ti++;\n\t}\n\tif (len)\n\t\t*len = 0;\n\treturn 0;\n}\n\nstruct siw_pbl *siw_pbl_alloc(u32 num_buf)\n{\n\tstruct siw_pbl *pbl;\n\n\tif (num_buf == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpbl = kzalloc(struct_size(pbl, pbe, num_buf), GFP_KERNEL);\n\tif (!pbl)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpbl->max_buf = num_buf;\n\n\treturn pbl;\n}\n\nstruct siw_umem *siw_umem_get(u64 start, u64 len, bool writable)\n{\n\tstruct siw_umem *umem;\n\tstruct mm_struct *mm_s;\n\tu64 first_page_va;\n\tunsigned long mlock_limit;\n\tunsigned int foll_flags = FOLL_LONGTERM;\n\tint num_pages, num_chunks, i, rv = 0;\n\n\tif (!can_do_mlock())\n\t\treturn ERR_PTR(-EPERM);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tfirst_page_va = start & PAGE_MASK;\n\tnum_pages = PAGE_ALIGN(start + len - first_page_va) >> PAGE_SHIFT;\n\tnum_chunks = (num_pages >> CHUNK_SHIFT) + 1;\n\n\tumem = kzalloc(sizeof(*umem), GFP_KERNEL);\n\tif (!umem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmm_s = current->mm;\n\tumem->owning_mm = mm_s;\n\tumem->writable = writable;\n\n\tmmgrab(mm_s);\n\n\tif (writable)\n\t\tfoll_flags |= FOLL_WRITE;\n\n\tmmap_read_lock(mm_s);\n\n\tmlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tif (atomic64_add_return(num_pages, &mm_s->pinned_vm) > mlock_limit) {\n\t\trv = -ENOMEM;\n\t\tgoto out_sem_up;\n\t}\n\tumem->fp_addr = first_page_va;\n\n\tumem->page_chunk =\n\t\tkcalloc(num_chunks, sizeof(struct siw_page_chunk), GFP_KERNEL);\n\tif (!umem->page_chunk) {\n\t\trv = -ENOMEM;\n\t\tgoto out_sem_up;\n\t}\n\tfor (i = 0; num_pages; i++) {\n\t\tint nents = min_t(int, num_pages, PAGES_PER_CHUNK);\n\t\tstruct page **plist =\n\t\t\tkcalloc(nents, sizeof(struct page *), GFP_KERNEL);\n\n\t\tif (!plist) {\n\t\t\trv = -ENOMEM;\n\t\t\tgoto out_sem_up;\n\t\t}\n\t\tumem->page_chunk[i].plist = plist;\n\t\twhile (nents) {\n\t\t\trv = pin_user_pages(first_page_va, nents, foll_flags,\n\t\t\t\t\t    plist);\n\t\t\tif (rv < 0)\n\t\t\t\tgoto out_sem_up;\n\n\t\t\tumem->num_pages += rv;\n\t\t\tfirst_page_va += rv * PAGE_SIZE;\n\t\t\tplist += rv;\n\t\t\tnents -= rv;\n\t\t\tnum_pages -= rv;\n\t\t}\n\t}\nout_sem_up:\n\tmmap_read_unlock(mm_s);\n\n\tif (rv > 0)\n\t\treturn umem;\n\n\t \n\tif (num_pages)\n\t\tatomic64_sub(num_pages, &mm_s->pinned_vm);\n\n\tsiw_umem_release(umem, false);\n\n\treturn ERR_PTR(rv);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}