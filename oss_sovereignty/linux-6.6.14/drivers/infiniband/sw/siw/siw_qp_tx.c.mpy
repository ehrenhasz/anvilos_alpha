{
  "module_name": "siw_qp_tx.c",
  "hash_id": "f20a0a5b229e2e2f6054b57efbea36feb519f5d55c3436d1dc285d0488ac46af",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/siw/siw_qp_tx.c",
  "human_readable_source": "\n\n \n \n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/net.h>\n#include <linux/scatterlist.h>\n#include <linux/highmem.h>\n#include <net/tcp.h>\n\n#include <rdma/iw_cm.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n\n#include \"siw.h\"\n#include \"siw_verbs.h\"\n#include \"siw_mem.h\"\n\n#define MAX_HDR_INLINE\t\t\t\t\t\\\n\t(((uint32_t)(sizeof(struct siw_rreq_pkt) -\t\\\n\t\t     sizeof(struct iwarp_send))) & 0xF8)\n\nstatic struct page *siw_get_pblpage(struct siw_mem *mem, u64 addr, int *idx)\n{\n\tstruct siw_pbl *pbl = mem->pbl;\n\tu64 offset = addr - mem->va;\n\tdma_addr_t paddr = siw_pbl_get_buffer(pbl, offset, NULL, idx);\n\n\tif (paddr)\n\t\treturn ib_virt_dma_to_page(paddr);\n\n\treturn NULL;\n}\n\n \nstatic int siw_try_1seg(struct siw_iwarp_tx *c_tx, void *paddr)\n{\n\tstruct siw_wqe *wqe = &c_tx->wqe_active;\n\tstruct siw_sge *sge = &wqe->sqe.sge[0];\n\tu32 bytes = sge->length;\n\n\tif (bytes > MAX_HDR_INLINE || wqe->sqe.num_sge != 1)\n\t\treturn MAX_HDR_INLINE + 1;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tif (tx_flags(wqe) & SIW_WQE_INLINE) {\n\t\tmemcpy(paddr, &wqe->sqe.sge[1], bytes);\n\t} else {\n\t\tstruct siw_mem *mem = wqe->mem[0];\n\n\t\tif (!mem->mem_obj) {\n\t\t\t \n\t\t\tmemcpy(paddr, ib_virt_dma_to_ptr(sge->laddr), bytes);\n\t\t} else if (c_tx->in_syscall) {\n\t\t\tif (copy_from_user(paddr, u64_to_user_ptr(sge->laddr),\n\t\t\t\t\t   bytes))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\tunsigned int off = sge->laddr & ~PAGE_MASK;\n\t\t\tstruct page *p;\n\t\t\tchar *buffer;\n\t\t\tint pbl_idx = 0;\n\n\t\t\tif (!mem->is_pbl)\n\t\t\t\tp = siw_get_upage(mem->umem, sge->laddr);\n\t\t\telse\n\t\t\t\tp = siw_get_pblpage(mem, sge->laddr, &pbl_idx);\n\n\t\t\tif (unlikely(!p))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tbuffer = kmap_local_page(p);\n\n\t\t\tif (likely(PAGE_SIZE - off >= bytes)) {\n\t\t\t\tmemcpy(paddr, buffer + off, bytes);\n\t\t\t} else {\n\t\t\t\tunsigned long part = bytes - (PAGE_SIZE - off);\n\n\t\t\t\tmemcpy(paddr, buffer + off, part);\n\t\t\t\tkunmap_local(buffer);\n\n\t\t\t\tif (!mem->is_pbl)\n\t\t\t\t\tp = siw_get_upage(mem->umem,\n\t\t\t\t\t\t\t  sge->laddr + part);\n\t\t\t\telse\n\t\t\t\t\tp = siw_get_pblpage(mem,\n\t\t\t\t\t\t\t    sge->laddr + part,\n\t\t\t\t\t\t\t    &pbl_idx);\n\t\t\t\tif (unlikely(!p))\n\t\t\t\t\treturn -EFAULT;\n\n\t\t\t\tbuffer = kmap_local_page(p);\n\t\t\t\tmemcpy(paddr + part, buffer, bytes - part);\n\t\t\t}\n\t\t\tkunmap_local(buffer);\n\t\t}\n\t}\n\treturn (int)bytes;\n}\n\n#define PKT_FRAGMENTED 1\n#define PKT_COMPLETE 0\n\n \nstatic int siw_qp_prepare_tx(struct siw_iwarp_tx *c_tx)\n{\n\tstruct siw_wqe *wqe = &c_tx->wqe_active;\n\tchar *crc = NULL;\n\tint data = 0;\n\n\tswitch (tx_type(wqe)) {\n\tcase SIW_OP_READ:\n\tcase SIW_OP_READ_LOCAL_INV:\n\t\tmemcpy(&c_tx->pkt.ctrl,\n\t\t       &iwarp_pktinfo[RDMAP_RDMA_READ_REQ].ctrl,\n\t\t       sizeof(struct iwarp_ctrl));\n\n\t\tc_tx->pkt.rreq.rsvd = 0;\n\t\tc_tx->pkt.rreq.ddp_qn = htonl(RDMAP_UNTAGGED_QN_RDMA_READ);\n\t\tc_tx->pkt.rreq.ddp_msn =\n\t\t\thtonl(++c_tx->ddp_msn[RDMAP_UNTAGGED_QN_RDMA_READ]);\n\t\tc_tx->pkt.rreq.ddp_mo = 0;\n\t\tc_tx->pkt.rreq.sink_stag = htonl(wqe->sqe.sge[0].lkey);\n\t\tc_tx->pkt.rreq.sink_to =\n\t\t\tcpu_to_be64(wqe->sqe.sge[0].laddr);\n\t\tc_tx->pkt.rreq.source_stag = htonl(wqe->sqe.rkey);\n\t\tc_tx->pkt.rreq.source_to = cpu_to_be64(wqe->sqe.raddr);\n\t\tc_tx->pkt.rreq.read_size = htonl(wqe->sqe.sge[0].length);\n\n\t\tc_tx->ctrl_len = sizeof(struct iwarp_rdma_rreq);\n\t\tcrc = (char *)&c_tx->pkt.rreq_pkt.crc;\n\t\tbreak;\n\n\tcase SIW_OP_SEND:\n\t\tif (tx_flags(wqe) & SIW_WQE_SOLICITED)\n\t\t\tmemcpy(&c_tx->pkt.ctrl,\n\t\t\t       &iwarp_pktinfo[RDMAP_SEND_SE].ctrl,\n\t\t\t       sizeof(struct iwarp_ctrl));\n\t\telse\n\t\t\tmemcpy(&c_tx->pkt.ctrl, &iwarp_pktinfo[RDMAP_SEND].ctrl,\n\t\t\t       sizeof(struct iwarp_ctrl));\n\n\t\tc_tx->pkt.send.ddp_qn = RDMAP_UNTAGGED_QN_SEND;\n\t\tc_tx->pkt.send.ddp_msn =\n\t\t\thtonl(++c_tx->ddp_msn[RDMAP_UNTAGGED_QN_SEND]);\n\t\tc_tx->pkt.send.ddp_mo = 0;\n\n\t\tc_tx->pkt.send_inv.inval_stag = 0;\n\n\t\tc_tx->ctrl_len = sizeof(struct iwarp_send);\n\n\t\tcrc = (char *)&c_tx->pkt.send_pkt.crc;\n\t\tdata = siw_try_1seg(c_tx, crc);\n\t\tbreak;\n\n\tcase SIW_OP_SEND_REMOTE_INV:\n\t\tif (tx_flags(wqe) & SIW_WQE_SOLICITED)\n\t\t\tmemcpy(&c_tx->pkt.ctrl,\n\t\t\t       &iwarp_pktinfo[RDMAP_SEND_SE_INVAL].ctrl,\n\t\t\t       sizeof(struct iwarp_ctrl));\n\t\telse\n\t\t\tmemcpy(&c_tx->pkt.ctrl,\n\t\t\t       &iwarp_pktinfo[RDMAP_SEND_INVAL].ctrl,\n\t\t\t       sizeof(struct iwarp_ctrl));\n\n\t\tc_tx->pkt.send.ddp_qn = RDMAP_UNTAGGED_QN_SEND;\n\t\tc_tx->pkt.send.ddp_msn =\n\t\t\thtonl(++c_tx->ddp_msn[RDMAP_UNTAGGED_QN_SEND]);\n\t\tc_tx->pkt.send.ddp_mo = 0;\n\n\t\tc_tx->pkt.send_inv.inval_stag = cpu_to_be32(wqe->sqe.rkey);\n\n\t\tc_tx->ctrl_len = sizeof(struct iwarp_send_inv);\n\n\t\tcrc = (char *)&c_tx->pkt.send_pkt.crc;\n\t\tdata = siw_try_1seg(c_tx, crc);\n\t\tbreak;\n\n\tcase SIW_OP_WRITE:\n\t\tmemcpy(&c_tx->pkt.ctrl, &iwarp_pktinfo[RDMAP_RDMA_WRITE].ctrl,\n\t\t       sizeof(struct iwarp_ctrl));\n\n\t\tc_tx->pkt.rwrite.sink_stag = htonl(wqe->sqe.rkey);\n\t\tc_tx->pkt.rwrite.sink_to = cpu_to_be64(wqe->sqe.raddr);\n\t\tc_tx->ctrl_len = sizeof(struct iwarp_rdma_write);\n\n\t\tcrc = (char *)&c_tx->pkt.write_pkt.crc;\n\t\tdata = siw_try_1seg(c_tx, crc);\n\t\tbreak;\n\n\tcase SIW_OP_READ_RESPONSE:\n\t\tmemcpy(&c_tx->pkt.ctrl,\n\t\t       &iwarp_pktinfo[RDMAP_RDMA_READ_RESP].ctrl,\n\t\t       sizeof(struct iwarp_ctrl));\n\n\t\t \n\t\tc_tx->pkt.rresp.sink_stag = cpu_to_be32(wqe->sqe.rkey);\n\t\tc_tx->pkt.rresp.sink_to = cpu_to_be64(wqe->sqe.raddr);\n\n\t\tc_tx->ctrl_len = sizeof(struct iwarp_rdma_rresp);\n\n\t\tcrc = (char *)&c_tx->pkt.write_pkt.crc;\n\t\tdata = siw_try_1seg(c_tx, crc);\n\t\tbreak;\n\n\tdefault:\n\t\tsiw_dbg_qp(tx_qp(c_tx), \"stale wqe type %d\\n\", tx_type(wqe));\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (unlikely(data < 0))\n\t\treturn data;\n\n\tc_tx->ctrl_sent = 0;\n\n\tif (data <= MAX_HDR_INLINE) {\n\t\tif (data) {\n\t\t\twqe->processed = data;\n\n\t\t\tc_tx->pkt.ctrl.mpa_len =\n\t\t\t\thtons(c_tx->ctrl_len + data - MPA_HDR_SIZE);\n\n\t\t\t \n\t\t\tdata += -(int)data & 0x3;\n\t\t\t \n\t\t\tcrc += data;\n\t\t\tc_tx->ctrl_len += data;\n\n\t\t\tif (!(c_tx->pkt.ctrl.ddp_rdmap_ctrl & DDP_FLAG_TAGGED))\n\t\t\t\tc_tx->pkt.c_untagged.ddp_mo = 0;\n\t\t\telse\n\t\t\t\tc_tx->pkt.c_tagged.ddp_to =\n\t\t\t\t\tcpu_to_be64(wqe->sqe.raddr);\n\t\t}\n\n\t\t*(u32 *)crc = 0;\n\t\t \n\t\tif (c_tx->mpa_crc_hd) {\n\t\t\tcrypto_shash_init(c_tx->mpa_crc_hd);\n\t\t\tif (crypto_shash_update(c_tx->mpa_crc_hd,\n\t\t\t\t\t\t(u8 *)&c_tx->pkt,\n\t\t\t\t\t\tc_tx->ctrl_len))\n\t\t\t\treturn -EINVAL;\n\t\t\tcrypto_shash_final(c_tx->mpa_crc_hd, (u8 *)crc);\n\t\t}\n\t\tc_tx->ctrl_len += MPA_CRC_SIZE;\n\n\t\treturn PKT_COMPLETE;\n\t}\n\tc_tx->ctrl_len += MPA_CRC_SIZE;\n\tc_tx->sge_idx = 0;\n\tc_tx->sge_off = 0;\n\tc_tx->pbl_idx = 0;\n\n\t \n\tif (c_tx->zcopy_tx && wqe->bytes >= SENDPAGE_THRESH &&\n\t    !(tx_flags(wqe) & SIW_WQE_SIGNALLED))\n\t\tc_tx->use_sendpage = 1;\n\telse\n\t\tc_tx->use_sendpage = 0;\n\n\treturn PKT_FRAGMENTED;\n}\n\n \nstatic int siw_tx_ctrl(struct siw_iwarp_tx *c_tx, struct socket *s,\n\t\t\t      int flags)\n{\n\tstruct msghdr msg = { .msg_flags = flags };\n\tstruct kvec iov = { .iov_base =\n\t\t\t\t    (char *)&c_tx->pkt.ctrl + c_tx->ctrl_sent,\n\t\t\t    .iov_len = c_tx->ctrl_len - c_tx->ctrl_sent };\n\n\tint rv = kernel_sendmsg(s, &msg, &iov, 1,\n\t\t\t\tc_tx->ctrl_len - c_tx->ctrl_sent);\n\n\tif (rv >= 0) {\n\t\tc_tx->ctrl_sent += rv;\n\n\t\tif (c_tx->ctrl_sent == c_tx->ctrl_len)\n\t\t\trv = 0;\n\t\telse\n\t\t\trv = -EAGAIN;\n\t}\n\treturn rv;\n}\n\n \nstatic int siw_tcp_sendpages(struct socket *s, struct page **page, int offset,\n\t\t\t     size_t size)\n{\n\tstruct bio_vec bvec;\n\tstruct msghdr msg = {\n\t\t.msg_flags = (MSG_MORE | MSG_DONTWAIT | MSG_SPLICE_PAGES),\n\t};\n\tstruct sock *sk = s->sk;\n\tint i = 0, rv = 0, sent = 0;\n\n\twhile (size) {\n\t\tsize_t bytes = min_t(size_t, PAGE_SIZE - offset, size);\n\n\t\tif (size + offset <= PAGE_SIZE)\n\t\t\tmsg.msg_flags &= ~MSG_MORE;\n\n\t\ttcp_rate_check_app_limited(sk);\n\t\tbvec_set_page(&bvec, page[i], bytes, offset);\n\t\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, size);\n\ntry_page_again:\n\t\tlock_sock(sk);\n\t\trv = tcp_sendmsg_locked(sk, &msg, size);\n\t\trelease_sock(sk);\n\n\t\tif (rv > 0) {\n\t\t\tsize -= rv;\n\t\t\tsent += rv;\n\t\t\tif (rv != bytes) {\n\t\t\t\toffset += rv;\n\t\t\t\tbytes -= rv;\n\t\t\t\tgoto try_page_again;\n\t\t\t}\n\t\t\toffset = 0;\n\t\t} else {\n\t\t\tif (rv == -EAGAIN || rv == 0)\n\t\t\t\tbreak;\n\t\t\treturn rv;\n\t\t}\n\t\ti++;\n\t}\n\treturn sent;\n}\n\n \nstatic int siw_0copy_tx(struct socket *s, struct page **page,\n\t\t\tstruct siw_sge *sge, unsigned int offset,\n\t\t\tunsigned int size)\n{\n\tint i = 0, sent = 0, rv;\n\tint sge_bytes = min(sge->length - offset, size);\n\n\toffset = (sge->laddr + offset) & ~PAGE_MASK;\n\n\twhile (sent != size) {\n\t\trv = siw_tcp_sendpages(s, &page[i], offset, sge_bytes);\n\t\tif (rv >= 0) {\n\t\t\tsent += rv;\n\t\t\tif (size == sent || sge_bytes > rv)\n\t\t\t\tbreak;\n\n\t\t\ti += PAGE_ALIGN(sge_bytes + offset) >> PAGE_SHIFT;\n\t\t\tsge++;\n\t\t\tsge_bytes = min(sge->length, size - sent);\n\t\t\toffset = sge->laddr & ~PAGE_MASK;\n\t\t} else {\n\t\t\tsent = rv;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn sent;\n}\n\n#define MAX_TRAILER (MPA_CRC_SIZE + 4)\n\nstatic void siw_unmap_pages(struct kvec *iov, unsigned long kmap_mask, int len)\n{\n\tint i;\n\n\t \n\tfor (i = (len-1); i >= 0; i--) {\n\t\tif (kmap_mask & BIT(i)) {\n\t\t\tunsigned long addr = (unsigned long)iov[i].iov_base;\n\n\t\t\tkunmap_local((void *)(addr & PAGE_MASK));\n\t\t}\n\t}\n}\n\n \n#define MAX_ARRAY ((0xffff / PAGE_SIZE) + 1 + (2 * (SIW_MAX_SGE - 1) + 2))\n\n \nstatic int siw_tx_hdt(struct siw_iwarp_tx *c_tx, struct socket *s)\n{\n\tstruct siw_wqe *wqe = &c_tx->wqe_active;\n\tstruct siw_sge *sge = &wqe->sqe.sge[c_tx->sge_idx];\n\tstruct kvec iov[MAX_ARRAY];\n\tstruct page *page_array[MAX_ARRAY];\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_EOR };\n\n\tint seg = 0, do_crc = c_tx->do_crc, is_kva = 0, rv;\n\tunsigned int data_len = c_tx->bytes_unsent, hdr_len = 0, trl_len = 0,\n\t\t     sge_off = c_tx->sge_off, sge_idx = c_tx->sge_idx,\n\t\t     pbl_idx = c_tx->pbl_idx;\n\tunsigned long kmap_mask = 0L;\n\n\tif (c_tx->state == SIW_SEND_HDR) {\n\t\tif (c_tx->use_sendpage) {\n\t\t\trv = siw_tx_ctrl(c_tx, s, MSG_DONTWAIT | MSG_MORE);\n\t\t\tif (rv)\n\t\t\t\tgoto done;\n\n\t\t\tc_tx->state = SIW_SEND_DATA;\n\t\t} else {\n\t\t\tiov[0].iov_base =\n\t\t\t\t(char *)&c_tx->pkt.ctrl + c_tx->ctrl_sent;\n\t\t\tiov[0].iov_len = hdr_len =\n\t\t\t\tc_tx->ctrl_len - c_tx->ctrl_sent;\n\t\t\tseg = 1;\n\t\t}\n\t}\n\n\twqe->processed += data_len;\n\n\twhile (data_len) {  \n\t\tunsigned int sge_len = min(sge->length - sge_off, data_len);\n\t\tunsigned int fp_off = (sge->laddr + sge_off) & ~PAGE_MASK;\n\t\tstruct siw_mem *mem;\n\n\t\tif (!(tx_flags(wqe) & SIW_WQE_INLINE)) {\n\t\t\tmem = wqe->mem[sge_idx];\n\t\t\tis_kva = mem->mem_obj == NULL ? 1 : 0;\n\t\t} else {\n\t\t\tis_kva = 1;\n\t\t}\n\t\tif (is_kva && !c_tx->use_sendpage) {\n\t\t\t \n\t\t\tiov[seg].iov_base =\n\t\t\t\tib_virt_dma_to_ptr(sge->laddr + sge_off);\n\t\t\tiov[seg].iov_len = sge_len;\n\n\t\t\tif (do_crc)\n\t\t\t\tcrypto_shash_update(c_tx->mpa_crc_hd,\n\t\t\t\t\t\t    iov[seg].iov_base,\n\t\t\t\t\t\t    sge_len);\n\t\t\tsge_off += sge_len;\n\t\t\tdata_len -= sge_len;\n\t\t\tseg++;\n\t\t\tgoto sge_done;\n\t\t}\n\n\t\twhile (sge_len) {\n\t\t\tsize_t plen = min((int)PAGE_SIZE - fp_off, sge_len);\n\t\t\tvoid *kaddr;\n\n\t\t\tif (!is_kva) {\n\t\t\t\tstruct page *p;\n\n\t\t\t\tif (mem->is_pbl)\n\t\t\t\t\tp = siw_get_pblpage(\n\t\t\t\t\t\tmem, sge->laddr + sge_off,\n\t\t\t\t\t\t&pbl_idx);\n\t\t\t\telse\n\t\t\t\t\tp = siw_get_upage(mem->umem,\n\t\t\t\t\t\t\t  sge->laddr + sge_off);\n\t\t\t\tif (unlikely(!p)) {\n\t\t\t\t\tsiw_unmap_pages(iov, kmap_mask, seg);\n\t\t\t\t\twqe->processed -= c_tx->bytes_unsent;\n\t\t\t\t\trv = -EFAULT;\n\t\t\t\t\tgoto done_crc;\n\t\t\t\t}\n\t\t\t\tpage_array[seg] = p;\n\n\t\t\t\tif (!c_tx->use_sendpage) {\n\t\t\t\t\tvoid *kaddr = kmap_local_page(p);\n\n\t\t\t\t\t \n\t\t\t\t\tkmap_mask |= BIT(seg);\n\t\t\t\t\tiov[seg].iov_base = kaddr + fp_off;\n\t\t\t\t\tiov[seg].iov_len = plen;\n\n\t\t\t\t\tif (do_crc)\n\t\t\t\t\t\tcrypto_shash_update(\n\t\t\t\t\t\t\tc_tx->mpa_crc_hd,\n\t\t\t\t\t\t\tiov[seg].iov_base,\n\t\t\t\t\t\t\tplen);\n\t\t\t\t} else if (do_crc) {\n\t\t\t\t\tkaddr = kmap_local_page(p);\n\t\t\t\t\tcrypto_shash_update(c_tx->mpa_crc_hd,\n\t\t\t\t\t\t\t    kaddr + fp_off,\n\t\t\t\t\t\t\t    plen);\n\t\t\t\t\tkunmap_local(kaddr);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tu64 va = sge->laddr + sge_off;\n\n\t\t\t\tpage_array[seg] = ib_virt_dma_to_page(va);\n\t\t\t\tif (do_crc)\n\t\t\t\t\tcrypto_shash_update(\n\t\t\t\t\t\tc_tx->mpa_crc_hd,\n\t\t\t\t\t\tib_virt_dma_to_ptr(va),\n\t\t\t\t\t\tplen);\n\t\t\t}\n\n\t\t\tsge_len -= plen;\n\t\t\tsge_off += plen;\n\t\t\tdata_len -= plen;\n\t\t\tfp_off = 0;\n\n\t\t\tif (++seg >= (int)MAX_ARRAY) {\n\t\t\t\tsiw_dbg_qp(tx_qp(c_tx), \"to many fragments\\n\");\n\t\t\t\tsiw_unmap_pages(iov, kmap_mask, seg-1);\n\t\t\t\twqe->processed -= c_tx->bytes_unsent;\n\t\t\t\trv = -EMSGSIZE;\n\t\t\t\tgoto done_crc;\n\t\t\t}\n\t\t}\nsge_done:\n\t\t \n\t\tif (sge_off == sge->length &&\n\t\t    (data_len != 0 || wqe->processed < wqe->bytes)) {\n\t\t\tsge_idx++;\n\t\t\tsge++;\n\t\t\tsge_off = 0;\n\t\t}\n\t}\n\t \n\tif (likely(c_tx->state != SIW_SEND_TRAILER)) {\n\t\tiov[seg].iov_base = &c_tx->trailer.pad[4 - c_tx->pad];\n\t\tiov[seg].iov_len = trl_len = MAX_TRAILER - (4 - c_tx->pad);\n\t} else {\n\t\tiov[seg].iov_base = &c_tx->trailer.pad[c_tx->ctrl_sent];\n\t\tiov[seg].iov_len = trl_len = MAX_TRAILER - c_tx->ctrl_sent;\n\t}\n\n\tif (c_tx->pad) {\n\t\t*(u32 *)c_tx->trailer.pad = 0;\n\t\tif (do_crc)\n\t\t\tcrypto_shash_update(c_tx->mpa_crc_hd,\n\t\t\t\t(u8 *)&c_tx->trailer.crc - c_tx->pad,\n\t\t\t\tc_tx->pad);\n\t}\n\tif (!c_tx->mpa_crc_hd)\n\t\tc_tx->trailer.crc = 0;\n\telse if (do_crc)\n\t\tcrypto_shash_final(c_tx->mpa_crc_hd, (u8 *)&c_tx->trailer.crc);\n\n\tdata_len = c_tx->bytes_unsent;\n\n\tif (c_tx->use_sendpage) {\n\t\trv = siw_0copy_tx(s, page_array, &wqe->sqe.sge[c_tx->sge_idx],\n\t\t\t\t  c_tx->sge_off, data_len);\n\t\tif (rv == data_len) {\n\t\t\trv = kernel_sendmsg(s, &msg, &iov[seg], 1, trl_len);\n\t\t\tif (rv > 0)\n\t\t\t\trv += data_len;\n\t\t\telse\n\t\t\t\trv = data_len;\n\t\t}\n\t} else {\n\t\trv = kernel_sendmsg(s, &msg, iov, seg + 1,\n\t\t\t\t    hdr_len + data_len + trl_len);\n\t\tsiw_unmap_pages(iov, kmap_mask, seg);\n\t}\n\tif (rv < (int)hdr_len) {\n\t\t \n\t\twqe->processed -= data_len;\n\t\tif (rv >= 0) {\n\t\t\tc_tx->ctrl_sent += rv;\n\t\t\trv = -EAGAIN;\n\t\t}\n\t\tgoto done_crc;\n\t}\n\trv -= hdr_len;\n\n\tif (rv >= (int)data_len) {\n\t\t \n\t\tif (data_len > 0 && wqe->processed < wqe->bytes) {\n\t\t\t \n\t\t\tc_tx->sge_idx = sge_idx;\n\t\t\tc_tx->sge_off = sge_off;\n\t\t\tc_tx->pbl_idx = pbl_idx;\n\t\t}\n\t\trv -= data_len;\n\n\t\tif (rv == trl_len)  \n\t\t\trv = 0;\n\t\telse {\n\t\t\tc_tx->state = SIW_SEND_TRAILER;\n\t\t\tc_tx->ctrl_len = MAX_TRAILER;\n\t\t\tc_tx->ctrl_sent = rv + 4 - c_tx->pad;\n\t\t\tc_tx->bytes_unsent = 0;\n\t\t\trv = -EAGAIN;\n\t\t}\n\n\t} else if (data_len > 0) {\n\t\t \n\t\tc_tx->state = SIW_SEND_DATA;\n\t\twqe->processed -= data_len - rv;\n\n\t\tif (rv) {\n\t\t\t \n\t\t\tunsigned int sge_unsent;\n\n\t\t\tc_tx->bytes_unsent -= rv;\n\t\t\tsge = &wqe->sqe.sge[c_tx->sge_idx];\n\t\t\tsge_unsent = sge->length - c_tx->sge_off;\n\n\t\t\twhile (sge_unsent <= rv) {\n\t\t\t\trv -= sge_unsent;\n\t\t\t\tc_tx->sge_idx++;\n\t\t\t\tc_tx->sge_off = 0;\n\t\t\t\tsge++;\n\t\t\t\tsge_unsent = sge->length;\n\t\t\t}\n\t\t\tc_tx->sge_off += rv;\n\t\t}\n\t\trv = -EAGAIN;\n\t}\ndone_crc:\n\tc_tx->do_crc = 0;\ndone:\n\treturn rv;\n}\n\nstatic void siw_update_tcpseg(struct siw_iwarp_tx *c_tx,\n\t\t\t\t     struct socket *s)\n{\n\tstruct tcp_sock *tp = tcp_sk(s->sk);\n\n\tif (tp->gso_segs) {\n\t\tif (c_tx->gso_seg_limit == 0)\n\t\t\tc_tx->tcp_seglen = tp->mss_cache * tp->gso_segs;\n\t\telse\n\t\t\tc_tx->tcp_seglen =\n\t\t\t\ttp->mss_cache *\n\t\t\t\tmin_t(u16, c_tx->gso_seg_limit, tp->gso_segs);\n\t} else {\n\t\tc_tx->tcp_seglen = tp->mss_cache;\n\t}\n\t \n\tc_tx->tcp_seglen &= 0xfffffff8;\n}\n\n \nstatic void siw_prepare_fpdu(struct siw_qp *qp, struct siw_wqe *wqe)\n{\n\tstruct siw_iwarp_tx *c_tx = &qp->tx_ctx;\n\tint data_len;\n\n\tc_tx->ctrl_len =\n\t\tiwarp_pktinfo[__rdmap_get_opcode(&c_tx->pkt.ctrl)].hdr_len;\n\tc_tx->ctrl_sent = 0;\n\n\t \n\tif (!(c_tx->pkt.ctrl.ddp_rdmap_ctrl & DDP_FLAG_TAGGED))\n\t\t \n\t\tc_tx->pkt.c_untagged.ddp_mo = cpu_to_be32(wqe->processed);\n\telse  \n\t\tc_tx->pkt.c_tagged.ddp_to =\n\t\t\tcpu_to_be64(wqe->sqe.raddr + wqe->processed);\n\n\tdata_len = wqe->bytes - wqe->processed;\n\tif (data_len + c_tx->ctrl_len + MPA_CRC_SIZE > c_tx->tcp_seglen) {\n\t\t \n\t\tdata_len = c_tx->tcp_seglen - (c_tx->ctrl_len + MPA_CRC_SIZE);\n\t\tc_tx->pkt.ctrl.ddp_rdmap_ctrl &= ~DDP_FLAG_LAST;\n\t\tc_tx->pad = 0;\n\t} else {\n\t\tc_tx->pkt.ctrl.ddp_rdmap_ctrl |= DDP_FLAG_LAST;\n\t\tc_tx->pad = -data_len & 0x3;\n\t}\n\tc_tx->bytes_unsent = data_len;\n\n\tc_tx->pkt.ctrl.mpa_len =\n\t\thtons(c_tx->ctrl_len + data_len - MPA_HDR_SIZE);\n\n\t \n\tif (c_tx->mpa_crc_hd) {\n\t\tcrypto_shash_init(c_tx->mpa_crc_hd);\n\t\tcrypto_shash_update(c_tx->mpa_crc_hd, (u8 *)&c_tx->pkt,\n\t\t\t\t    c_tx->ctrl_len);\n\t\tc_tx->do_crc = 1;\n\t}\n}\n\n \n\nstatic int siw_check_sgl_tx(struct ib_pd *pd, struct siw_wqe *wqe,\n\t\t\t    enum ib_access_flags perms)\n{\n\tstruct siw_sge *sge = &wqe->sqe.sge[0];\n\tint i, len, num_sge = wqe->sqe.num_sge;\n\n\tif (unlikely(num_sge > SIW_MAX_SGE))\n\t\treturn -EINVAL;\n\n\tfor (i = 0, len = 0; num_sge; num_sge--, i++, sge++) {\n\t\t \n\t\tif (sge->length) {\n\t\t\tint rv = siw_check_sge(pd, sge, &wqe->mem[i], perms, 0,\n\t\t\t\t\t       sge->length);\n\n\t\t\tif (unlikely(rv != E_ACCESS_OK))\n\t\t\t\treturn rv;\n\t\t}\n\t\tlen += sge->length;\n\t}\n\treturn len;\n}\n\n \nstatic int siw_qp_sq_proc_tx(struct siw_qp *qp, struct siw_wqe *wqe)\n{\n\tstruct siw_iwarp_tx *c_tx = &qp->tx_ctx;\n\tstruct socket *s = qp->attrs.sk;\n\tint rv = 0, burst_len = qp->tx_ctx.burst;\n\tenum rdmap_ecode ecode = RDMAP_ECODE_CATASTROPHIC_STREAM;\n\n\tif (unlikely(wqe->wr_status == SIW_WR_IDLE))\n\t\treturn 0;\n\n\tif (!burst_len)\n\t\tburst_len = SQ_USER_MAXBURST;\n\n\tif (wqe->wr_status == SIW_WR_QUEUED) {\n\t\tif (!(wqe->sqe.flags & SIW_WQE_INLINE)) {\n\t\t\tif (tx_type(wqe) == SIW_OP_READ_RESPONSE)\n\t\t\t\twqe->sqe.num_sge = 1;\n\n\t\t\tif (tx_type(wqe) != SIW_OP_READ &&\n\t\t\t    tx_type(wqe) != SIW_OP_READ_LOCAL_INV) {\n\t\t\t\t \n\t\t\t\trv = siw_check_sgl_tx(qp->pd, wqe, 0);\n\t\t\t\tif (rv < 0) {\n\t\t\t\t\tif (tx_type(wqe) ==\n\t\t\t\t\t    SIW_OP_READ_RESPONSE)\n\t\t\t\t\t\tecode = siw_rdmap_error(-rv);\n\t\t\t\t\trv = -EINVAL;\n\t\t\t\t\tgoto tx_error;\n\t\t\t\t}\n\t\t\t\twqe->bytes = rv;\n\t\t\t} else {\n\t\t\t\twqe->bytes = 0;\n\t\t\t}\n\t\t} else {\n\t\t\twqe->bytes = wqe->sqe.sge[0].length;\n\t\t\tif (!rdma_is_kernel_res(&qp->base_qp.res)) {\n\t\t\t\tif (wqe->bytes > SIW_MAX_INLINE) {\n\t\t\t\t\trv = -EINVAL;\n\t\t\t\t\tgoto tx_error;\n\t\t\t\t}\n\t\t\t\twqe->sqe.sge[0].laddr =\n\t\t\t\t\t(u64)(uintptr_t)&wqe->sqe.sge[1];\n\t\t\t}\n\t\t}\n\t\twqe->wr_status = SIW_WR_INPROGRESS;\n\t\twqe->processed = 0;\n\n\t\tsiw_update_tcpseg(c_tx, s);\n\n\t\trv = siw_qp_prepare_tx(c_tx);\n\t\tif (rv == PKT_FRAGMENTED) {\n\t\t\tc_tx->state = SIW_SEND_HDR;\n\t\t\tsiw_prepare_fpdu(qp, wqe);\n\t\t} else if (rv == PKT_COMPLETE) {\n\t\t\tc_tx->state = SIW_SEND_SHORT_FPDU;\n\t\t} else {\n\t\t\tgoto tx_error;\n\t\t}\n\t}\n\nnext_segment:\n\tsiw_dbg_qp(qp, \"wr type %d, state %d, data %u, sent %u, id %llx\\n\",\n\t\t   tx_type(wqe), wqe->wr_status, wqe->bytes, wqe->processed,\n\t\t   wqe->sqe.id);\n\n\tif (--burst_len == 0) {\n\t\trv = -EINPROGRESS;\n\t\tgoto tx_done;\n\t}\n\tif (c_tx->state == SIW_SEND_SHORT_FPDU) {\n\t\tenum siw_opcode tx_type = tx_type(wqe);\n\t\tunsigned int msg_flags;\n\n\t\tif (siw_sq_empty(qp) || !siw_tcp_nagle || burst_len == 1)\n\t\t\t \n\t\t\tmsg_flags = MSG_DONTWAIT;\n\t\telse\n\t\t\tmsg_flags = MSG_DONTWAIT | MSG_MORE;\n\n\t\trv = siw_tx_ctrl(c_tx, s, msg_flags);\n\n\t\tif (!rv && tx_type != SIW_OP_READ &&\n\t\t    tx_type != SIW_OP_READ_LOCAL_INV)\n\t\t\twqe->processed = wqe->bytes;\n\n\t\tgoto tx_done;\n\n\t} else {\n\t\trv = siw_tx_hdt(c_tx, s);\n\t}\n\tif (!rv) {\n\t\t \n\t\tif (unlikely(c_tx->tx_suspend)) {\n\t\t\t \n\t\t\trv = -ECONNABORTED;\n\t\t\tgoto tx_done;\n\t\t}\n\t\tif (c_tx->pkt.ctrl.ddp_rdmap_ctrl & DDP_FLAG_LAST) {\n\t\t\tsiw_dbg_qp(qp, \"WQE completed\\n\");\n\t\t\tgoto tx_done;\n\t\t}\n\t\tc_tx->state = SIW_SEND_HDR;\n\n\t\tsiw_update_tcpseg(c_tx, s);\n\n\t\tsiw_prepare_fpdu(qp, wqe);\n\t\tgoto next_segment;\n\t}\ntx_done:\n\tqp->tx_ctx.burst = burst_len;\n\treturn rv;\n\ntx_error:\n\tif (ecode != RDMAP_ECODE_CATASTROPHIC_STREAM)\n\t\tsiw_init_terminate(qp, TERM_ERROR_LAYER_RDMAP,\n\t\t\t\t   RDMAP_ETYPE_REMOTE_PROTECTION, ecode, 1);\n\telse\n\t\tsiw_init_terminate(qp, TERM_ERROR_LAYER_RDMAP,\n\t\t\t\t   RDMAP_ETYPE_CATASTROPHIC,\n\t\t\t\t   RDMAP_ECODE_UNSPECIFIED, 1);\n\treturn rv;\n}\n\nstatic int siw_fastreg_mr(struct ib_pd *pd, struct siw_sqe *sqe)\n{\n\tstruct ib_mr *base_mr = (struct ib_mr *)(uintptr_t)sqe->base_mr;\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mem *mem;\n\tint rv = 0;\n\n\tsiw_dbg_pd(pd, \"STag 0x%08x\\n\", sqe->rkey);\n\n\tif (unlikely(!base_mr)) {\n\t\tpr_warn(\"siw: fastreg: STag 0x%08x unknown\\n\", sqe->rkey);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(base_mr->rkey >> 8 != sqe->rkey  >> 8)) {\n\t\tpr_warn(\"siw: fastreg: STag 0x%08x: bad MR\\n\", sqe->rkey);\n\t\treturn -EINVAL;\n\t}\n\n\tmem = siw_mem_id2obj(sdev, sqe->rkey  >> 8);\n\tif (unlikely(!mem)) {\n\t\tpr_warn(\"siw: fastreg: STag 0x%08x unknown\\n\", sqe->rkey);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(mem->pd != pd)) {\n\t\tpr_warn(\"siw: fastreg: PD mismatch\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (unlikely(mem->stag_valid)) {\n\t\tpr_warn(\"siw: fastreg: STag 0x%08x already valid\\n\", sqe->rkey);\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\t \n\tmem->stag = sqe->rkey;\n\tmem->perms = sqe->access;\n\n\tsiw_dbg_mem(mem, \"STag 0x%08x now valid\\n\", sqe->rkey);\n\tmem->va = base_mr->iova;\n\tmem->stag_valid = 1;\nout:\n\tsiw_mem_put(mem);\n\treturn rv;\n}\n\nstatic int siw_qp_sq_proc_local(struct siw_qp *qp, struct siw_wqe *wqe)\n{\n\tint rv;\n\n\tswitch (tx_type(wqe)) {\n\tcase SIW_OP_REG_MR:\n\t\trv = siw_fastreg_mr(qp->pd, &wqe->sqe);\n\t\tbreak;\n\n\tcase SIW_OP_INVAL_STAG:\n\t\trv = siw_invalidate_stag(qp->pd, wqe->sqe.rkey);\n\t\tbreak;\n\n\tdefault:\n\t\trv = -EINVAL;\n\t}\n\treturn rv;\n}\n\n \nint siw_qp_sq_process(struct siw_qp *qp)\n{\n\tstruct siw_wqe *wqe = tx_wqe(qp);\n\tenum siw_opcode tx_type;\n\tunsigned long flags;\n\tint rv = 0;\n\n\tsiw_dbg_qp(qp, \"enter for type %d\\n\", tx_type(wqe));\n\nnext_wqe:\n\t \n\tif (unlikely(qp->tx_ctx.tx_suspend)) {\n\t\tsiw_dbg_qp(qp, \"tx suspended\\n\");\n\t\tgoto done;\n\t}\n\ttx_type = tx_type(wqe);\n\n\tif (tx_type <= SIW_OP_READ_RESPONSE)\n\t\trv = siw_qp_sq_proc_tx(qp, wqe);\n\telse\n\t\trv = siw_qp_sq_proc_local(qp, wqe);\n\n\tif (!rv) {\n\t\t \n\t\tswitch (tx_type) {\n\t\tcase SIW_OP_SEND:\n\t\tcase SIW_OP_SEND_REMOTE_INV:\n\t\tcase SIW_OP_WRITE:\n\t\t\tsiw_wqe_put_mem(wqe, tx_type);\n\t\t\tfallthrough;\n\n\t\tcase SIW_OP_INVAL_STAG:\n\t\tcase SIW_OP_REG_MR:\n\t\t\tif (tx_flags(wqe) & SIW_WQE_SIGNALLED)\n\t\t\t\tsiw_sqe_complete(qp, &wqe->sqe, wqe->bytes,\n\t\t\t\t\t\t SIW_WC_SUCCESS);\n\t\t\tbreak;\n\n\t\tcase SIW_OP_READ:\n\t\tcase SIW_OP_READ_LOCAL_INV:\n\t\t\t \n\t\t\tbreak;\n\n\t\tcase SIW_OP_READ_RESPONSE:\n\t\t\tsiw_wqe_put_mem(wqe, tx_type);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN(1, \"undefined WQE type %d\\n\", tx_type);\n\t\t\trv = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\n\t\tspin_lock_irqsave(&qp->sq_lock, flags);\n\t\twqe->wr_status = SIW_WR_IDLE;\n\t\trv = siw_activate_tx(qp);\n\t\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\n\t\tif (rv <= 0)\n\t\t\tgoto done;\n\n\t\tgoto next_wqe;\n\n\t} else if (rv == -EAGAIN) {\n\t\tsiw_dbg_qp(qp, \"sq paused: hd/tr %d of %d, data %d\\n\",\n\t\t\t   qp->tx_ctx.ctrl_sent, qp->tx_ctx.ctrl_len,\n\t\t\t   qp->tx_ctx.bytes_unsent);\n\t\trv = 0;\n\t\tgoto done;\n\t} else if (rv == -EINPROGRESS) {\n\t\trv = siw_sq_start(qp);\n\t\tgoto done;\n\t} else {\n\t\t \n\t\tsiw_dbg_qp(qp, \"wqe type %d processing failed: %d\\n\",\n\t\t\t   tx_type(wqe), rv);\n\n\t\tspin_lock_irqsave(&qp->sq_lock, flags);\n\t\t \n\t\tif ((tx_type == SIW_OP_READ ||\n\t\t     tx_type == SIW_OP_READ_LOCAL_INV) && qp->attrs.orq_size) {\n\t\t\t \n\t\t\tqp->orq_put--;\n\t\t\tqp->orq[qp->orq_put % qp->attrs.orq_size].flags = 0;\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\t\t \n\t\tif (!qp->tx_ctx.tx_suspend)\n\t\t\tsiw_qp_cm_drop(qp, 0);\n\n\t\tswitch (tx_type) {\n\t\tcase SIW_OP_SEND:\n\t\tcase SIW_OP_SEND_REMOTE_INV:\n\t\tcase SIW_OP_SEND_WITH_IMM:\n\t\tcase SIW_OP_WRITE:\n\t\tcase SIW_OP_READ:\n\t\tcase SIW_OP_READ_LOCAL_INV:\n\t\t\tsiw_wqe_put_mem(wqe, tx_type);\n\t\t\tfallthrough;\n\n\t\tcase SIW_OP_INVAL_STAG:\n\t\tcase SIW_OP_REG_MR:\n\t\t\tsiw_sqe_complete(qp, &wqe->sqe, wqe->bytes,\n\t\t\t\t\t SIW_WC_LOC_QP_OP_ERR);\n\n\t\t\tsiw_qp_event(qp, IB_EVENT_QP_FATAL);\n\n\t\t\tbreak;\n\n\t\tcase SIW_OP_READ_RESPONSE:\n\t\t\tsiw_dbg_qp(qp, \"proc. read.response failed: %d\\n\", rv);\n\n\t\t\tsiw_qp_event(qp, IB_EVENT_QP_REQ_ERR);\n\n\t\t\tsiw_wqe_put_mem(wqe, SIW_OP_READ_RESPONSE);\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN(1, \"undefined WQE type %d\\n\", tx_type);\n\t\t\trv = -EINVAL;\n\t\t}\n\t\twqe->wr_status = SIW_WR_IDLE;\n\t}\ndone:\n\treturn rv;\n}\n\nstatic void siw_sq_resume(struct siw_qp *qp)\n{\n\tif (down_read_trylock(&qp->state_lock)) {\n\t\tif (likely(qp->attrs.state == SIW_QP_STATE_RTS &&\n\t\t\t   !qp->tx_ctx.tx_suspend)) {\n\t\t\tint rv = siw_qp_sq_process(qp);\n\n\t\t\tup_read(&qp->state_lock);\n\n\t\t\tif (unlikely(rv < 0)) {\n\t\t\t\tsiw_dbg_qp(qp, \"SQ task failed: err %d\\n\", rv);\n\n\t\t\t\tif (!qp->tx_ctx.tx_suspend)\n\t\t\t\t\tsiw_qp_cm_drop(qp, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tup_read(&qp->state_lock);\n\t\t}\n\t} else {\n\t\tsiw_dbg_qp(qp, \"Resume SQ while QP locked\\n\");\n\t}\n\tsiw_qp_put(qp);\n}\n\nstruct tx_task_t {\n\tstruct llist_head active;\n\twait_queue_head_t waiting;\n};\n\nstatic DEFINE_PER_CPU(struct tx_task_t, siw_tx_task_g);\n\nint siw_create_tx_threads(void)\n{\n\tint cpu, assigned = 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct tx_task_t *tx_task;\n\n\t\t \n\t\tif (cpu % cpumask_weight(topology_sibling_cpumask(cpu)))\n\t\t\tcontinue;\n\n\t\ttx_task = &per_cpu(siw_tx_task_g, cpu);\n\t\tinit_llist_head(&tx_task->active);\n\t\tinit_waitqueue_head(&tx_task->waiting);\n\n\t\tsiw_tx_thread[cpu] =\n\t\t\tkthread_run_on_cpu(siw_run_sq,\n\t\t\t\t\t   (unsigned long *)(long)cpu,\n\t\t\t\t\t   cpu, \"siw_tx/%u\");\n\t\tif (IS_ERR(siw_tx_thread[cpu])) {\n\t\t\tsiw_tx_thread[cpu] = NULL;\n\t\t\tcontinue;\n\t\t}\n\t\tassigned++;\n\t}\n\treturn assigned;\n}\n\nvoid siw_stop_tx_threads(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (siw_tx_thread[cpu]) {\n\t\t\tkthread_stop(siw_tx_thread[cpu]);\n\t\t\twake_up(&per_cpu(siw_tx_task_g, cpu).waiting);\n\t\t\tsiw_tx_thread[cpu] = NULL;\n\t\t}\n\t}\n}\n\nint siw_run_sq(void *data)\n{\n\tconst int nr_cpu = (unsigned int)(long)data;\n\tstruct llist_node *active;\n\tstruct siw_qp *qp;\n\tstruct tx_task_t *tx_task = &per_cpu(siw_tx_task_g, nr_cpu);\n\n\twhile (1) {\n\t\tstruct llist_node *fifo_list = NULL;\n\n\t\twait_event_interruptible(tx_task->waiting,\n\t\t\t\t\t !llist_empty(&tx_task->active) ||\n\t\t\t\t\t\t kthread_should_stop());\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tactive = llist_del_all(&tx_task->active);\n\t\t \n\t\tfifo_list = llist_reverse_order(active);\n\t\twhile (fifo_list) {\n\t\t\tqp = container_of(fifo_list, struct siw_qp, tx_list);\n\t\t\tfifo_list = llist_next(fifo_list);\n\t\t\tqp->tx_list.next = NULL;\n\n\t\t\tsiw_sq_resume(qp);\n\t\t}\n\t}\n\tactive = llist_del_all(&tx_task->active);\n\tif (active) {\n\t\tllist_for_each_entry(qp, active, tx_list) {\n\t\t\tqp->tx_list.next = NULL;\n\t\t\tsiw_sq_resume(qp);\n\t\t}\n\t}\n\treturn 0;\n}\n\nint siw_sq_start(struct siw_qp *qp)\n{\n\tif (tx_wqe(qp)->wr_status == SIW_WR_IDLE)\n\t\treturn 0;\n\n\tif (unlikely(!cpu_online(qp->tx_cpu))) {\n\t\tsiw_put_tx_cpu(qp->tx_cpu);\n\t\tqp->tx_cpu = siw_get_tx_cpu(qp->sdev);\n\t\tif (qp->tx_cpu < 0) {\n\t\t\tpr_warn(\"siw: no tx cpu available\\n\");\n\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\tsiw_qp_get(qp);\n\n\tllist_add(&qp->tx_list, &per_cpu(siw_tx_task_g, qp->tx_cpu).active);\n\n\twake_up(&per_cpu(siw_tx_task_g, qp->tx_cpu).waiting);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}