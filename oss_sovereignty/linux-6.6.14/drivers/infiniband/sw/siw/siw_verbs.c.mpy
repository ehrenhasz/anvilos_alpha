{
  "module_name": "siw_verbs.c",
  "hash_id": "fbe30d1553fb851ea39d13712a9218367500876786da75a203b7a10f7bf776d3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/infiniband/sw/siw/siw_verbs.c",
  "human_readable_source": "\n\n \n \n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/xarray.h>\n#include <net/addrconf.h>\n\n#include <rdma/iw_cm.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_user_verbs.h>\n#include <rdma/uverbs_ioctl.h>\n\n#include \"siw.h\"\n#include \"siw_verbs.h\"\n#include \"siw_mem.h\"\n\nstatic int ib_qp_state_to_siw_qp_state[IB_QPS_ERR + 1] = {\n\t[IB_QPS_RESET] = SIW_QP_STATE_IDLE,\n\t[IB_QPS_INIT] = SIW_QP_STATE_IDLE,\n\t[IB_QPS_RTR] = SIW_QP_STATE_RTR,\n\t[IB_QPS_RTS] = SIW_QP_STATE_RTS,\n\t[IB_QPS_SQD] = SIW_QP_STATE_CLOSING,\n\t[IB_QPS_SQE] = SIW_QP_STATE_TERMINATE,\n\t[IB_QPS_ERR] = SIW_QP_STATE_ERROR\n};\n\nstatic char ib_qp_state_to_string[IB_QPS_ERR + 1][sizeof(\"RESET\")] = {\n\t[IB_QPS_RESET] = \"RESET\", [IB_QPS_INIT] = \"INIT\", [IB_QPS_RTR] = \"RTR\",\n\t[IB_QPS_RTS] = \"RTS\",     [IB_QPS_SQD] = \"SQD\",   [IB_QPS_SQE] = \"SQE\",\n\t[IB_QPS_ERR] = \"ERR\"\n};\n\nvoid siw_mmap_free(struct rdma_user_mmap_entry *rdma_entry)\n{\n\tstruct siw_user_mmap_entry *entry = to_siw_mmap_entry(rdma_entry);\n\n\tkfree(entry);\n}\n\nint siw_mmap(struct ib_ucontext *ctx, struct vm_area_struct *vma)\n{\n\tstruct siw_ucontext *uctx = to_siw_ctx(ctx);\n\tsize_t size = vma->vm_end - vma->vm_start;\n\tstruct rdma_user_mmap_entry *rdma_entry;\n\tstruct siw_user_mmap_entry *entry;\n\tint rv = -EINVAL;\n\n\t \n\tif (vma->vm_start & (PAGE_SIZE - 1)) {\n\t\tpr_warn(\"siw: mmap not page aligned\\n\");\n\t\treturn -EINVAL;\n\t}\n\trdma_entry = rdma_user_mmap_entry_get(&uctx->base_ucontext, vma);\n\tif (!rdma_entry) {\n\t\tsiw_dbg(&uctx->sdev->base_dev, \"mmap lookup failed: %lu, %#zx\\n\",\n\t\t\tvma->vm_pgoff, size);\n\t\treturn -EINVAL;\n\t}\n\tentry = to_siw_mmap_entry(rdma_entry);\n\n\trv = remap_vmalloc_range(vma, entry->address, 0);\n\tif (rv) {\n\t\tpr_warn(\"remap_vmalloc_range failed: %lu, %zu\\n\", vma->vm_pgoff,\n\t\t\tsize);\n\t\tgoto out;\n\t}\nout:\n\trdma_user_mmap_entry_put(rdma_entry);\n\n\treturn rv;\n}\n\nint siw_alloc_ucontext(struct ib_ucontext *base_ctx, struct ib_udata *udata)\n{\n\tstruct siw_device *sdev = to_siw_dev(base_ctx->device);\n\tstruct siw_ucontext *ctx = to_siw_ctx(base_ctx);\n\tstruct siw_uresp_alloc_ctx uresp = {};\n\tint rv;\n\n\tif (atomic_inc_return(&sdev->num_ctx) > SIW_MAX_CONTEXT) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tctx->sdev = sdev;\n\n\turesp.dev_id = sdev->vendor_part_id;\n\n\tif (udata->outlen < sizeof(uresp)) {\n\t\trv = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trv = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\tif (rv)\n\t\tgoto err_out;\n\n\tsiw_dbg(base_ctx->device, \"success. now %d context(s)\\n\",\n\t\tatomic_read(&sdev->num_ctx));\n\n\treturn 0;\n\nerr_out:\n\tatomic_dec(&sdev->num_ctx);\n\tsiw_dbg(base_ctx->device, \"failure %d. now %d context(s)\\n\", rv,\n\t\tatomic_read(&sdev->num_ctx));\n\n\treturn rv;\n}\n\nvoid siw_dealloc_ucontext(struct ib_ucontext *base_ctx)\n{\n\tstruct siw_ucontext *uctx = to_siw_ctx(base_ctx);\n\n\tatomic_dec(&uctx->sdev->num_ctx);\n}\n\nint siw_query_device(struct ib_device *base_dev, struct ib_device_attr *attr,\n\t\t     struct ib_udata *udata)\n{\n\tstruct siw_device *sdev = to_siw_dev(base_dev);\n\n\tif (udata->inlen || udata->outlen)\n\t\treturn -EINVAL;\n\n\tmemset(attr, 0, sizeof(*attr));\n\n\t \n\tattr->atomic_cap = 0;\n\tattr->device_cap_flags = IB_DEVICE_MEM_MGT_EXTENSIONS;\n\tattr->kernel_cap_flags = IBK_ALLOW_USER_UNREG;\n\tattr->max_cq = sdev->attrs.max_cq;\n\tattr->max_cqe = sdev->attrs.max_cqe;\n\tattr->max_fast_reg_page_list_len = SIW_MAX_SGE_PBL;\n\tattr->max_mr = sdev->attrs.max_mr;\n\tattr->max_mw = sdev->attrs.max_mw;\n\tattr->max_mr_size = ~0ull;\n\tattr->max_pd = sdev->attrs.max_pd;\n\tattr->max_qp = sdev->attrs.max_qp;\n\tattr->max_qp_init_rd_atom = sdev->attrs.max_ird;\n\tattr->max_qp_rd_atom = sdev->attrs.max_ord;\n\tattr->max_qp_wr = sdev->attrs.max_qp_wr;\n\tattr->max_recv_sge = sdev->attrs.max_sge;\n\tattr->max_res_rd_atom = sdev->attrs.max_qp * sdev->attrs.max_ird;\n\tattr->max_send_sge = sdev->attrs.max_sge;\n\tattr->max_sge_rd = sdev->attrs.max_sge_rd;\n\tattr->max_srq = sdev->attrs.max_srq;\n\tattr->max_srq_sge = sdev->attrs.max_srq_sge;\n\tattr->max_srq_wr = sdev->attrs.max_srq_wr;\n\tattr->page_size_cap = PAGE_SIZE;\n\tattr->vendor_id = SIW_VENDOR_ID;\n\tattr->vendor_part_id = sdev->vendor_part_id;\n\n\taddrconf_addr_eui48((u8 *)&attr->sys_image_guid,\n\t\t\t    sdev->raw_gid);\n\n\treturn 0;\n}\n\nint siw_query_port(struct ib_device *base_dev, u32 port,\n\t\t   struct ib_port_attr *attr)\n{\n\tstruct siw_device *sdev = to_siw_dev(base_dev);\n\tint rv;\n\n\tmemset(attr, 0, sizeof(*attr));\n\n\trv = ib_get_eth_speed(base_dev, port, &attr->active_speed,\n\t\t\t &attr->active_width);\n\tattr->gid_tbl_len = 1;\n\tattr->max_msg_sz = -1;\n\tattr->max_mtu = ib_mtu_int_to_enum(sdev->netdev->mtu);\n\tattr->active_mtu = ib_mtu_int_to_enum(sdev->netdev->mtu);\n\tattr->phys_state = sdev->state == IB_PORT_ACTIVE ?\n\t\tIB_PORT_PHYS_STATE_LINK_UP : IB_PORT_PHYS_STATE_DISABLED;\n\tattr->port_cap_flags = IB_PORT_CM_SUP | IB_PORT_DEVICE_MGMT_SUP;\n\tattr->state = sdev->state;\n\t \n\treturn rv;\n}\n\nint siw_get_port_immutable(struct ib_device *base_dev, u32 port,\n\t\t\t   struct ib_port_immutable *port_immutable)\n{\n\tstruct ib_port_attr attr;\n\tint rv = siw_query_port(base_dev, port, &attr);\n\n\tif (rv)\n\t\treturn rv;\n\n\tport_immutable->gid_tbl_len = attr.gid_tbl_len;\n\tport_immutable->core_cap_flags = RDMA_CORE_PORT_IWARP;\n\n\treturn 0;\n}\n\nint siw_query_gid(struct ib_device *base_dev, u32 port, int idx,\n\t\t  union ib_gid *gid)\n{\n\tstruct siw_device *sdev = to_siw_dev(base_dev);\n\n\t \n\tmemset(gid, 0, sizeof(*gid));\n\tmemcpy(gid->raw, sdev->raw_gid, ETH_ALEN);\n\n\treturn 0;\n}\n\nint siw_alloc_pd(struct ib_pd *pd, struct ib_udata *udata)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\n\tif (atomic_inc_return(&sdev->num_pd) > SIW_MAX_PD) {\n\t\tatomic_dec(&sdev->num_pd);\n\t\treturn -ENOMEM;\n\t}\n\tsiw_dbg_pd(pd, \"now %d PD's(s)\\n\", atomic_read(&sdev->num_pd));\n\n\treturn 0;\n}\n\nint siw_dealloc_pd(struct ib_pd *pd, struct ib_udata *udata)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\n\tsiw_dbg_pd(pd, \"free PD\\n\");\n\tatomic_dec(&sdev->num_pd);\n\treturn 0;\n}\n\nvoid siw_qp_get_ref(struct ib_qp *base_qp)\n{\n\tsiw_qp_get(to_siw_qp(base_qp));\n}\n\nvoid siw_qp_put_ref(struct ib_qp *base_qp)\n{\n\tsiw_qp_put(to_siw_qp(base_qp));\n}\n\nstatic struct rdma_user_mmap_entry *\nsiw_mmap_entry_insert(struct siw_ucontext *uctx,\n\t\t      void *address, size_t length,\n\t\t      u64 *offset)\n{\n\tstruct siw_user_mmap_entry *entry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tint rv;\n\n\t*offset = SIW_INVAL_UOBJ_KEY;\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->address = address;\n\n\trv = rdma_user_mmap_entry_insert(&uctx->base_ucontext,\n\t\t\t\t\t &entry->rdma_entry,\n\t\t\t\t\t length);\n\tif (rv) {\n\t\tkfree(entry);\n\t\treturn NULL;\n\t}\n\n\t*offset = rdma_user_mmap_get_offset(&entry->rdma_entry);\n\n\treturn &entry->rdma_entry;\n}\n\n \n\nint siw_create_qp(struct ib_qp *ibqp, struct ib_qp_init_attr *attrs,\n\t\t  struct ib_udata *udata)\n{\n\tstruct ib_pd *pd = ibqp->pd;\n\tstruct siw_qp *qp = to_siw_qp(ibqp);\n\tstruct ib_device *base_dev = pd->device;\n\tstruct siw_device *sdev = to_siw_dev(base_dev);\n\tstruct siw_ucontext *uctx =\n\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t  base_ucontext);\n\tunsigned long flags;\n\tint num_sqe, num_rqe, rv = 0;\n\tsize_t length;\n\n\tsiw_dbg(base_dev, \"create new QP\\n\");\n\n\tif (attrs->create_flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (atomic_inc_return(&sdev->num_qp) > SIW_MAX_QP) {\n\t\tsiw_dbg(base_dev, \"too many QP's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_atomic;\n\t}\n\tif (attrs->qp_type != IB_QPT_RC) {\n\t\tsiw_dbg(base_dev, \"only RC QP's supported\\n\");\n\t\trv = -EOPNOTSUPP;\n\t\tgoto err_atomic;\n\t}\n\tif ((attrs->cap.max_send_wr > SIW_MAX_QP_WR) ||\n\t    (attrs->cap.max_recv_wr > SIW_MAX_QP_WR) ||\n\t    (attrs->cap.max_send_sge > SIW_MAX_SGE) ||\n\t    (attrs->cap.max_recv_sge > SIW_MAX_SGE)) {\n\t\tsiw_dbg(base_dev, \"QP size error\\n\");\n\t\trv = -EINVAL;\n\t\tgoto err_atomic;\n\t}\n\tif (attrs->cap.max_inline_data > SIW_MAX_INLINE) {\n\t\tsiw_dbg(base_dev, \"max inline send: %d > %d\\n\",\n\t\t\tattrs->cap.max_inline_data, (int)SIW_MAX_INLINE);\n\t\trv = -EINVAL;\n\t\tgoto err_atomic;\n\t}\n\t \n\tif (attrs->cap.max_send_wr + attrs->cap.max_recv_wr == 0) {\n\t\tsiw_dbg(base_dev, \"QP must have send or receive queue\\n\");\n\t\trv = -EINVAL;\n\t\tgoto err_atomic;\n\t}\n\n\tif (!attrs->send_cq || (!attrs->recv_cq && !attrs->srq)) {\n\t\tsiw_dbg(base_dev, \"send CQ or receive CQ invalid\\n\");\n\t\trv = -EINVAL;\n\t\tgoto err_atomic;\n\t}\n\n\tinit_rwsem(&qp->state_lock);\n\tspin_lock_init(&qp->sq_lock);\n\tspin_lock_init(&qp->rq_lock);\n\tspin_lock_init(&qp->orq_lock);\n\n\trv = siw_qp_add(sdev, qp);\n\tif (rv)\n\t\tgoto err_atomic;\n\n\tnum_sqe = attrs->cap.max_send_wr;\n\tnum_rqe = attrs->cap.max_recv_wr;\n\n\t \n\tif (num_sqe)\n\t\tnum_sqe = roundup_pow_of_two(num_sqe);\n\telse {\n\t\t \n\t\trv = -EINVAL;\n\t\tgoto err_out_xa;\n\t}\n\tif (num_rqe)\n\t\tnum_rqe = roundup_pow_of_two(num_rqe);\n\n\tif (udata)\n\t\tqp->sendq = vmalloc_user(num_sqe * sizeof(struct siw_sqe));\n\telse\n\t\tqp->sendq = vcalloc(num_sqe, sizeof(struct siw_sqe));\n\n\tif (qp->sendq == NULL) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out_xa;\n\t}\n\tif (attrs->sq_sig_type != IB_SIGNAL_REQ_WR) {\n\t\tif (attrs->sq_sig_type == IB_SIGNAL_ALL_WR)\n\t\t\tqp->attrs.flags |= SIW_SIGNAL_ALL_WR;\n\t\telse {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out_xa;\n\t\t}\n\t}\n\tqp->pd = pd;\n\tqp->scq = to_siw_cq(attrs->send_cq);\n\tqp->rcq = to_siw_cq(attrs->recv_cq);\n\n\tif (attrs->srq) {\n\t\t \n\t\tqp->srq = to_siw_srq(attrs->srq);\n\t\tqp->attrs.rq_size = 0;\n\t\tsiw_dbg(base_dev, \"QP [%u]: SRQ attached\\n\",\n\t\t\tqp->base_qp.qp_num);\n\t} else if (num_rqe) {\n\t\tif (udata)\n\t\t\tqp->recvq =\n\t\t\t\tvmalloc_user(num_rqe * sizeof(struct siw_rqe));\n\t\telse\n\t\t\tqp->recvq = vcalloc(num_rqe, sizeof(struct siw_rqe));\n\n\t\tif (qp->recvq == NULL) {\n\t\t\trv = -ENOMEM;\n\t\t\tgoto err_out_xa;\n\t\t}\n\t\tqp->attrs.rq_size = num_rqe;\n\t}\n\tqp->attrs.sq_size = num_sqe;\n\tqp->attrs.sq_max_sges = attrs->cap.max_send_sge;\n\tqp->attrs.rq_max_sges = attrs->cap.max_recv_sge;\n\n\t \n\tqp->tx_ctx.gso_seg_limit = 1;\n\tqp->tx_ctx.zcopy_tx = zcopy_tx;\n\n\tqp->attrs.state = SIW_QP_STATE_IDLE;\n\n\tif (udata) {\n\t\tstruct siw_uresp_create_qp uresp = {};\n\n\t\turesp.num_sqe = num_sqe;\n\t\turesp.num_rqe = num_rqe;\n\t\turesp.qp_id = qp_id(qp);\n\n\t\tif (qp->sendq) {\n\t\t\tlength = num_sqe * sizeof(struct siw_sqe);\n\t\t\tqp->sq_entry =\n\t\t\t\tsiw_mmap_entry_insert(uctx, qp->sendq,\n\t\t\t\t\t\t      length, &uresp.sq_key);\n\t\t\tif (!qp->sq_entry) {\n\t\t\t\trv = -ENOMEM;\n\t\t\t\tgoto err_out_xa;\n\t\t\t}\n\t\t}\n\n\t\tif (qp->recvq) {\n\t\t\tlength = num_rqe * sizeof(struct siw_rqe);\n\t\t\tqp->rq_entry =\n\t\t\t\tsiw_mmap_entry_insert(uctx, qp->recvq,\n\t\t\t\t\t\t      length, &uresp.rq_key);\n\t\t\tif (!qp->rq_entry) {\n\t\t\t\turesp.sq_key = SIW_INVAL_UOBJ_KEY;\n\t\t\t\trv = -ENOMEM;\n\t\t\t\tgoto err_out_xa;\n\t\t\t}\n\t\t}\n\n\t\tif (udata->outlen < sizeof(uresp)) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out_xa;\n\t\t}\n\t\trv = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (rv)\n\t\t\tgoto err_out_xa;\n\t}\n\tqp->tx_cpu = siw_get_tx_cpu(sdev);\n\tif (qp->tx_cpu < 0) {\n\t\trv = -EINVAL;\n\t\tgoto err_out_xa;\n\t}\n\tINIT_LIST_HEAD(&qp->devq);\n\tspin_lock_irqsave(&sdev->lock, flags);\n\tlist_add_tail(&qp->devq, &sdev->qp_list);\n\tspin_unlock_irqrestore(&sdev->lock, flags);\n\n\tinit_completion(&qp->qp_free);\n\n\treturn 0;\n\nerr_out_xa:\n\txa_erase(&sdev->qp_xa, qp_id(qp));\n\tif (uctx) {\n\t\trdma_user_mmap_entry_remove(qp->sq_entry);\n\t\trdma_user_mmap_entry_remove(qp->rq_entry);\n\t}\n\tvfree(qp->sendq);\n\tvfree(qp->recvq);\n\nerr_atomic:\n\tatomic_dec(&sdev->num_qp);\n\treturn rv;\n}\n\n \nint siw_query_qp(struct ib_qp *base_qp, struct ib_qp_attr *qp_attr,\n\t\t int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr)\n{\n\tstruct siw_qp *qp;\n\tstruct siw_device *sdev;\n\n\tif (base_qp && qp_attr && qp_init_attr) {\n\t\tqp = to_siw_qp(base_qp);\n\t\tsdev = to_siw_dev(base_qp->device);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\tqp_attr->cap.max_inline_data = SIW_MAX_INLINE;\n\tqp_attr->cap.max_send_wr = qp->attrs.sq_size;\n\tqp_attr->cap.max_send_sge = qp->attrs.sq_max_sges;\n\tqp_attr->cap.max_recv_wr = qp->attrs.rq_size;\n\tqp_attr->cap.max_recv_sge = qp->attrs.rq_max_sges;\n\tqp_attr->path_mtu = ib_mtu_int_to_enum(sdev->netdev->mtu);\n\tqp_attr->max_rd_atomic = qp->attrs.irq_size;\n\tqp_attr->max_dest_rd_atomic = qp->attrs.orq_size;\n\n\tqp_attr->qp_access_flags = IB_ACCESS_LOCAL_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_WRITE |\n\t\t\t\t   IB_ACCESS_REMOTE_READ;\n\n\tqp_init_attr->qp_type = base_qp->qp_type;\n\tqp_init_attr->send_cq = base_qp->send_cq;\n\tqp_init_attr->recv_cq = base_qp->recv_cq;\n\tqp_init_attr->srq = base_qp->srq;\n\n\tqp_init_attr->cap = qp_attr->cap;\n\n\treturn 0;\n}\n\nint siw_verbs_modify_qp(struct ib_qp *base_qp, struct ib_qp_attr *attr,\n\t\t\tint attr_mask, struct ib_udata *udata)\n{\n\tstruct siw_qp_attrs new_attrs;\n\tenum siw_qp_attr_mask siw_attr_mask = 0;\n\tstruct siw_qp *qp = to_siw_qp(base_qp);\n\tint rv = 0;\n\n\tif (!attr_mask)\n\t\treturn 0;\n\n\tif (attr_mask & ~IB_QP_ATTR_STANDARD_BITS)\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(&new_attrs, 0, sizeof(new_attrs));\n\n\tif (attr_mask & IB_QP_ACCESS_FLAGS) {\n\t\tsiw_attr_mask = SIW_QP_ATTR_ACCESS_FLAGS;\n\n\t\tif (attr->qp_access_flags & IB_ACCESS_REMOTE_READ)\n\t\t\tnew_attrs.flags |= SIW_RDMA_READ_ENABLED;\n\t\tif (attr->qp_access_flags & IB_ACCESS_REMOTE_WRITE)\n\t\t\tnew_attrs.flags |= SIW_RDMA_WRITE_ENABLED;\n\t\tif (attr->qp_access_flags & IB_ACCESS_MW_BIND)\n\t\t\tnew_attrs.flags |= SIW_RDMA_BIND_ENABLED;\n\t}\n\tif (attr_mask & IB_QP_STATE) {\n\t\tsiw_dbg_qp(qp, \"desired IB QP state: %s\\n\",\n\t\t\t   ib_qp_state_to_string[attr->qp_state]);\n\n\t\tnew_attrs.state = ib_qp_state_to_siw_qp_state[attr->qp_state];\n\n\t\tif (new_attrs.state > SIW_QP_STATE_RTS)\n\t\t\tqp->tx_ctx.tx_suspend = 1;\n\n\t\tsiw_attr_mask |= SIW_QP_ATTR_STATE;\n\t}\n\tif (!siw_attr_mask)\n\t\tgoto out;\n\n\tdown_write(&qp->state_lock);\n\n\trv = siw_qp_modify(qp, &new_attrs, siw_attr_mask);\n\n\tup_write(&qp->state_lock);\nout:\n\treturn rv;\n}\n\nint siw_destroy_qp(struct ib_qp *base_qp, struct ib_udata *udata)\n{\n\tstruct siw_qp *qp = to_siw_qp(base_qp);\n\tstruct siw_ucontext *uctx =\n\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t  base_ucontext);\n\tstruct siw_qp_attrs qp_attrs;\n\n\tsiw_dbg_qp(qp, \"state %d\\n\", qp->attrs.state);\n\n\t \n\tqp->attrs.flags |= SIW_QP_IN_DESTROY;\n\tqp->rx_stream.rx_suspend = 1;\n\n\tif (uctx) {\n\t\trdma_user_mmap_entry_remove(qp->sq_entry);\n\t\trdma_user_mmap_entry_remove(qp->rq_entry);\n\t}\n\n\tdown_write(&qp->state_lock);\n\n\tqp_attrs.state = SIW_QP_STATE_ERROR;\n\tsiw_qp_modify(qp, &qp_attrs, SIW_QP_ATTR_STATE);\n\n\tif (qp->cep) {\n\t\tsiw_cep_put(qp->cep);\n\t\tqp->cep = NULL;\n\t}\n\tup_write(&qp->state_lock);\n\n\tkfree(qp->tx_ctx.mpa_crc_hd);\n\tkfree(qp->rx_stream.mpa_crc_hd);\n\n\tqp->scq = qp->rcq = NULL;\n\n\tsiw_qp_put(qp);\n\twait_for_completion(&qp->qp_free);\n\n\treturn 0;\n}\n\n \nstatic int siw_copy_inline_sgl(const struct ib_send_wr *core_wr,\n\t\t\t       struct siw_sqe *sqe)\n{\n\tstruct ib_sge *core_sge = core_wr->sg_list;\n\tvoid *kbuf = &sqe->sge[1];\n\tint num_sge = core_wr->num_sge, bytes = 0;\n\n\tsqe->sge[0].laddr = (uintptr_t)kbuf;\n\tsqe->sge[0].lkey = 0;\n\n\twhile (num_sge--) {\n\t\tif (!core_sge->length) {\n\t\t\tcore_sge++;\n\t\t\tcontinue;\n\t\t}\n\t\tbytes += core_sge->length;\n\t\tif (bytes > SIW_MAX_INLINE) {\n\t\t\tbytes = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tmemcpy(kbuf, ib_virt_dma_to_ptr(core_sge->addr),\n\t\t       core_sge->length);\n\n\t\tkbuf += core_sge->length;\n\t\tcore_sge++;\n\t}\n\tsqe->sge[0].length = max(bytes, 0);\n\tsqe->num_sge = bytes > 0 ? 1 : 0;\n\n\treturn bytes;\n}\n\n \nstatic int siw_sq_flush_wr(struct siw_qp *qp, const struct ib_send_wr *wr,\n\t\t\t   const struct ib_send_wr **bad_wr)\n{\n\tint rv = 0;\n\n\twhile (wr) {\n\t\tstruct siw_sqe sqe = {};\n\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_RDMA_WRITE:\n\t\t\tsqe.opcode = SIW_OP_WRITE;\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ:\n\t\t\tsqe.opcode = SIW_OP_READ;\n\t\t\tbreak;\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\t\tsqe.opcode = SIW_OP_READ_LOCAL_INV;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND:\n\t\t\tsqe.opcode = SIW_OP_SEND;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_IMM:\n\t\t\tsqe.opcode = SIW_OP_SEND_WITH_IMM;\n\t\t\tbreak;\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\tsqe.opcode = SIW_OP_SEND_REMOTE_INV;\n\t\t\tbreak;\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\tsqe.opcode = SIW_OP_INVAL_STAG;\n\t\t\tbreak;\n\t\tcase IB_WR_REG_MR:\n\t\t\tsqe.opcode = SIW_OP_REG_MR;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (!rv) {\n\t\t\tsqe.id = wr->wr_id;\n\t\t\trv = siw_sqe_complete(qp, &sqe, 0,\n\t\t\t\t\t      SIW_WC_WR_FLUSH_ERR);\n\t\t}\n\t\tif (rv) {\n\t\t\tif (bad_wr)\n\t\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\treturn rv;\n}\n\n \nstatic int siw_rq_flush_wr(struct siw_qp *qp, const struct ib_recv_wr *wr,\n\t\t\t   const struct ib_recv_wr **bad_wr)\n{\n\tstruct siw_rqe rqe = {};\n\tint rv = 0;\n\n\twhile (wr) {\n\t\trqe.id = wr->wr_id;\n\t\trv = siw_rqe_complete(qp, &rqe, 0, 0, SIW_WC_WR_FLUSH_ERR);\n\t\tif (rv) {\n\t\t\tif (bad_wr)\n\t\t\t\t*bad_wr = wr;\n\t\t\tbreak;\n\t\t}\n\t\twr = wr->next;\n\t}\n\treturn rv;\n}\n\n \nint siw_post_send(struct ib_qp *base_qp, const struct ib_send_wr *wr,\n\t\t  const struct ib_send_wr **bad_wr)\n{\n\tstruct siw_qp *qp = to_siw_qp(base_qp);\n\tstruct siw_wqe *wqe = tx_wqe(qp);\n\n\tunsigned long flags;\n\tint rv = 0;\n\n\tif (wr && !rdma_is_kernel_res(&qp->base_qp.res)) {\n\t\tsiw_dbg_qp(qp, \"wr must be empty for user mapped sq\\n\");\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!down_read_trylock(&qp->state_lock)) {\n\t\tif (qp->attrs.state == SIW_QP_STATE_ERROR) {\n\t\t\t \n\t\t\trv = siw_sq_flush_wr(qp, wr, bad_wr);\n\t\t} else {\n\t\t\tsiw_dbg_qp(qp, \"QP locked, state %d\\n\",\n\t\t\t\t   qp->attrs.state);\n\t\t\t*bad_wr = wr;\n\t\t\trv = -ENOTCONN;\n\t\t}\n\t\treturn rv;\n\t}\n\tif (unlikely(qp->attrs.state != SIW_QP_STATE_RTS)) {\n\t\tif (qp->attrs.state == SIW_QP_STATE_ERROR) {\n\t\t\t \n\t\t\trv = siw_sq_flush_wr(qp, wr, bad_wr);\n\t\t} else {\n\t\t\tsiw_dbg_qp(qp, \"QP out of state %d\\n\",\n\t\t\t\t   qp->attrs.state);\n\t\t\t*bad_wr = wr;\n\t\t\trv = -ENOTCONN;\n\t\t}\n\t\tup_read(&qp->state_lock);\n\t\treturn rv;\n\t}\n\tspin_lock_irqsave(&qp->sq_lock, flags);\n\n\twhile (wr) {\n\t\tu32 idx = qp->sq_put % qp->attrs.sq_size;\n\t\tstruct siw_sqe *sqe = &qp->sendq[idx];\n\n\t\tif (sqe->flags) {\n\t\t\tsiw_dbg_qp(qp, \"sq full\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tif (wr->num_sge > qp->attrs.sq_max_sges) {\n\t\t\tsiw_dbg_qp(qp, \"too many sge's: %d\\n\", wr->num_sge);\n\t\t\trv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsqe->id = wr->wr_id;\n\n\t\tif ((wr->send_flags & IB_SEND_SIGNALED) ||\n\t\t    (qp->attrs.flags & SIW_SIGNAL_ALL_WR))\n\t\t\tsqe->flags |= SIW_WQE_SIGNALLED;\n\n\t\tif (wr->send_flags & IB_SEND_FENCE)\n\t\t\tsqe->flags |= SIW_WQE_READ_FENCE;\n\n\t\tswitch (wr->opcode) {\n\t\tcase IB_WR_SEND:\n\t\tcase IB_WR_SEND_WITH_INV:\n\t\t\tif (wr->send_flags & IB_SEND_SOLICITED)\n\t\t\t\tsqe->flags |= SIW_WQE_SOLICITED;\n\n\t\t\tif (!(wr->send_flags & IB_SEND_INLINE)) {\n\t\t\t\tsiw_copy_sgl(wr->sg_list, sqe->sge,\n\t\t\t\t\t     wr->num_sge);\n\t\t\t\tsqe->num_sge = wr->num_sge;\n\t\t\t} else {\n\t\t\t\trv = siw_copy_inline_sgl(wr, sqe);\n\t\t\t\tif (rv <= 0) {\n\t\t\t\t\trv = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tsqe->flags |= SIW_WQE_INLINE;\n\t\t\t\tsqe->num_sge = 1;\n\t\t\t}\n\t\t\tif (wr->opcode == IB_WR_SEND)\n\t\t\t\tsqe->opcode = SIW_OP_SEND;\n\t\t\telse {\n\t\t\t\tsqe->opcode = SIW_OP_SEND_REMOTE_INV;\n\t\t\t\tsqe->rkey = wr->ex.invalidate_rkey;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase IB_WR_RDMA_READ_WITH_INV:\n\t\tcase IB_WR_RDMA_READ:\n\t\t\t \n\t\t\tif (unlikely(wr->num_sge != 1)) {\n\t\t\t\trv = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsiw_copy_sgl(wr->sg_list, &sqe->sge[0], 1);\n\t\t\t \n\t\t\tsqe->raddr = rdma_wr(wr)->remote_addr;\n\t\t\tsqe->rkey = rdma_wr(wr)->rkey;\n\t\t\tsqe->num_sge = 1;\n\n\t\t\tif (wr->opcode == IB_WR_RDMA_READ)\n\t\t\t\tsqe->opcode = SIW_OP_READ;\n\t\t\telse\n\t\t\t\tsqe->opcode = SIW_OP_READ_LOCAL_INV;\n\t\t\tbreak;\n\n\t\tcase IB_WR_RDMA_WRITE:\n\t\t\tif (!(wr->send_flags & IB_SEND_INLINE)) {\n\t\t\t\tsiw_copy_sgl(wr->sg_list, &sqe->sge[0],\n\t\t\t\t\t     wr->num_sge);\n\t\t\t\tsqe->num_sge = wr->num_sge;\n\t\t\t} else {\n\t\t\t\trv = siw_copy_inline_sgl(wr, sqe);\n\t\t\t\tif (unlikely(rv < 0)) {\n\t\t\t\t\trv = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tsqe->flags |= SIW_WQE_INLINE;\n\t\t\t\tsqe->num_sge = 1;\n\t\t\t}\n\t\t\tsqe->raddr = rdma_wr(wr)->remote_addr;\n\t\t\tsqe->rkey = rdma_wr(wr)->rkey;\n\t\t\tsqe->opcode = SIW_OP_WRITE;\n\t\t\tbreak;\n\n\t\tcase IB_WR_REG_MR:\n\t\t\tsqe->base_mr = (uintptr_t)reg_wr(wr)->mr;\n\t\t\tsqe->rkey = reg_wr(wr)->key;\n\t\t\tsqe->access = reg_wr(wr)->access & IWARP_ACCESS_MASK;\n\t\t\tsqe->opcode = SIW_OP_REG_MR;\n\t\t\tbreak;\n\n\t\tcase IB_WR_LOCAL_INV:\n\t\t\tsqe->rkey = wr->ex.invalidate_rkey;\n\t\t\tsqe->opcode = SIW_OP_INVAL_STAG;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tsiw_dbg_qp(qp, \"ib wr type %d unsupported\\n\",\n\t\t\t\t   wr->opcode);\n\t\t\trv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsiw_dbg_qp(qp, \"opcode %d, flags 0x%x, wr_id 0x%pK\\n\",\n\t\t\t   sqe->opcode, sqe->flags,\n\t\t\t   (void *)(uintptr_t)sqe->id);\n\n\t\tif (unlikely(rv < 0))\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_wmb();\n\t\tsqe->flags |= SIW_WQE_VALID;\n\n\t\tqp->sq_put++;\n\t\twr = wr->next;\n\t}\n\n\t \n\tif (wqe->wr_status != SIW_WR_IDLE) {\n\t\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\t\tgoto skip_direct_sending;\n\t}\n\trv = siw_activate_tx(qp);\n\tspin_unlock_irqrestore(&qp->sq_lock, flags);\n\n\tif (rv <= 0)\n\t\tgoto skip_direct_sending;\n\n\tif (rdma_is_kernel_res(&qp->base_qp.res)) {\n\t\trv = siw_sq_start(qp);\n\t} else {\n\t\tqp->tx_ctx.in_syscall = 1;\n\n\t\tif (siw_qp_sq_process(qp) != 0 && !(qp->tx_ctx.tx_suspend))\n\t\t\tsiw_qp_cm_drop(qp, 0);\n\n\t\tqp->tx_ctx.in_syscall = 0;\n\t}\nskip_direct_sending:\n\n\tup_read(&qp->state_lock);\n\n\tif (rv >= 0)\n\t\treturn 0;\n\t \n\tsiw_dbg_qp(qp, \"error %d\\n\", rv);\n\n\t*bad_wr = wr;\n\treturn rv;\n}\n\n \nint siw_post_receive(struct ib_qp *base_qp, const struct ib_recv_wr *wr,\n\t\t     const struct ib_recv_wr **bad_wr)\n{\n\tstruct siw_qp *qp = to_siw_qp(base_qp);\n\tunsigned long flags;\n\tint rv = 0;\n\n\tif (qp->srq || qp->attrs.rq_size == 0) {\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\tif (!rdma_is_kernel_res(&qp->base_qp.res)) {\n\t\tsiw_dbg_qp(qp, \"no kernel post_recv for user mapped rq\\n\");\n\t\t*bad_wr = wr;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!down_read_trylock(&qp->state_lock)) {\n\t\tif (qp->attrs.state == SIW_QP_STATE_ERROR) {\n\t\t\t \n\t\t\trv = siw_rq_flush_wr(qp, wr, bad_wr);\n\t\t} else {\n\t\t\tsiw_dbg_qp(qp, \"QP locked, state %d\\n\",\n\t\t\t\t   qp->attrs.state);\n\t\t\t*bad_wr = wr;\n\t\t\trv = -ENOTCONN;\n\t\t}\n\t\treturn rv;\n\t}\n\tif (qp->attrs.state > SIW_QP_STATE_RTS) {\n\t\tif (qp->attrs.state == SIW_QP_STATE_ERROR) {\n\t\t\t \n\t\t\trv = siw_rq_flush_wr(qp, wr, bad_wr);\n\t\t} else {\n\t\t\tsiw_dbg_qp(qp, \"QP out of state %d\\n\",\n\t\t\t\t   qp->attrs.state);\n\t\t\t*bad_wr = wr;\n\t\t\trv = -ENOTCONN;\n\t\t}\n\t\tup_read(&qp->state_lock);\n\t\treturn rv;\n\t}\n\t \n\tspin_lock_irqsave(&qp->rq_lock, flags);\n\n\twhile (wr) {\n\t\tu32 idx = qp->rq_put % qp->attrs.rq_size;\n\t\tstruct siw_rqe *rqe = &qp->recvq[idx];\n\n\t\tif (rqe->flags) {\n\t\t\tsiw_dbg_qp(qp, \"RQ full\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tif (wr->num_sge > qp->attrs.rq_max_sges) {\n\t\t\tsiw_dbg_qp(qp, \"too many sge's: %d\\n\", wr->num_sge);\n\t\t\trv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\trqe->id = wr->wr_id;\n\t\trqe->num_sge = wr->num_sge;\n\t\tsiw_copy_sgl(wr->sg_list, rqe->sge, wr->num_sge);\n\n\t\t \n\t\tsmp_wmb();\n\n\t\trqe->flags = SIW_WQE_VALID;\n\n\t\tqp->rq_put++;\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&qp->rq_lock, flags);\n\n\tup_read(&qp->state_lock);\n\n\tif (rv < 0) {\n\t\tsiw_dbg_qp(qp, \"error %d\\n\", rv);\n\t\t*bad_wr = wr;\n\t}\n\treturn rv > 0 ? 0 : rv;\n}\n\nint siw_destroy_cq(struct ib_cq *base_cq, struct ib_udata *udata)\n{\n\tstruct siw_cq *cq = to_siw_cq(base_cq);\n\tstruct siw_device *sdev = to_siw_dev(base_cq->device);\n\tstruct siw_ucontext *ctx =\n\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t  base_ucontext);\n\n\tsiw_dbg_cq(cq, \"free CQ resources\\n\");\n\n\tsiw_cq_flush(cq);\n\n\tif (ctx)\n\t\trdma_user_mmap_entry_remove(cq->cq_entry);\n\n\tatomic_dec(&sdev->num_cq);\n\n\tvfree(cq->queue);\n\treturn 0;\n}\n\n \n\nint siw_create_cq(struct ib_cq *base_cq, const struct ib_cq_init_attr *attr,\n\t\t  struct ib_udata *udata)\n{\n\tstruct siw_device *sdev = to_siw_dev(base_cq->device);\n\tstruct siw_cq *cq = to_siw_cq(base_cq);\n\tint rv, size = attr->cqe;\n\n\tif (attr->flags)\n\t\treturn -EOPNOTSUPP;\n\n\tif (atomic_inc_return(&sdev->num_cq) > SIW_MAX_CQ) {\n\t\tsiw_dbg(base_cq->device, \"too many CQ's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tif (size < 1 || size > sdev->attrs.max_cqe) {\n\t\tsiw_dbg(base_cq->device, \"CQ size error: %d\\n\", size);\n\t\trv = -EINVAL;\n\t\tgoto err_out;\n\t}\n\tsize = roundup_pow_of_two(size);\n\tcq->base_cq.cqe = size;\n\tcq->num_cqe = size;\n\n\tif (udata)\n\t\tcq->queue = vmalloc_user(size * sizeof(struct siw_cqe) +\n\t\t\t\t\t sizeof(struct siw_cq_ctrl));\n\telse\n\t\tcq->queue = vzalloc(size * sizeof(struct siw_cqe) +\n\t\t\t\t    sizeof(struct siw_cq_ctrl));\n\n\tif (cq->queue == NULL) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tget_random_bytes(&cq->id, 4);\n\tsiw_dbg(base_cq->device, \"new CQ [%u]\\n\", cq->id);\n\n\tspin_lock_init(&cq->lock);\n\n\tcq->notify = (struct siw_cq_ctrl *)&cq->queue[size];\n\n\tif (udata) {\n\t\tstruct siw_uresp_create_cq uresp = {};\n\t\tstruct siw_ucontext *ctx =\n\t\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t\t  base_ucontext);\n\t\tsize_t length = size * sizeof(struct siw_cqe) +\n\t\t\tsizeof(struct siw_cq_ctrl);\n\n\t\tcq->cq_entry =\n\t\t\tsiw_mmap_entry_insert(ctx, cq->queue,\n\t\t\t\t\t      length, &uresp.cq_key);\n\t\tif (!cq->cq_entry) {\n\t\t\trv = -ENOMEM;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\turesp.cq_id = cq->id;\n\t\turesp.num_cqe = size;\n\n\t\tif (udata->outlen < sizeof(uresp)) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t\trv = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (rv)\n\t\t\tgoto err_out;\n\t}\n\treturn 0;\n\nerr_out:\n\tsiw_dbg(base_cq->device, \"CQ creation failed: %d\", rv);\n\n\tif (cq->queue) {\n\t\tstruct siw_ucontext *ctx =\n\t\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t\t  base_ucontext);\n\t\tif (ctx)\n\t\t\trdma_user_mmap_entry_remove(cq->cq_entry);\n\t\tvfree(cq->queue);\n\t}\n\tatomic_dec(&sdev->num_cq);\n\n\treturn rv;\n}\n\n \nint siw_poll_cq(struct ib_cq *base_cq, int num_cqe, struct ib_wc *wc)\n{\n\tstruct siw_cq *cq = to_siw_cq(base_cq);\n\tint i;\n\n\tfor (i = 0; i < num_cqe; i++) {\n\t\tif (!siw_reap_cqe(cq, wc))\n\t\t\tbreak;\n\t\twc++;\n\t}\n\treturn i;\n}\n\n \nint siw_req_notify_cq(struct ib_cq *base_cq, enum ib_cq_notify_flags flags)\n{\n\tstruct siw_cq *cq = to_siw_cq(base_cq);\n\n\tsiw_dbg_cq(cq, \"flags: 0x%02x\\n\", flags);\n\n\tif ((flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED)\n\t\t \n\t\tsmp_store_mb(cq->notify->flags, SIW_NOTIFY_SOLICITED);\n\telse\n\t\t \n\t\tsmp_store_mb(cq->notify->flags, SIW_NOTIFY_ALL);\n\n\tif (flags & IB_CQ_REPORT_MISSED_EVENTS)\n\t\treturn cq->cq_put - cq->cq_get;\n\n\treturn 0;\n}\n\n \nint siw_dereg_mr(struct ib_mr *base_mr, struct ib_udata *udata)\n{\n\tstruct siw_mr *mr = to_siw_mr(base_mr);\n\tstruct siw_device *sdev = to_siw_dev(base_mr->device);\n\n\tsiw_dbg_mem(mr->mem, \"deregister MR\\n\");\n\n\tatomic_dec(&sdev->num_mr);\n\n\tsiw_mr_drop_mem(mr);\n\tkfree_rcu(mr, rcu);\n\n\treturn 0;\n}\n\n \nstruct ib_mr *siw_reg_user_mr(struct ib_pd *pd, u64 start, u64 len,\n\t\t\t      u64 rnic_va, int rights, struct ib_udata *udata)\n{\n\tstruct siw_mr *mr = NULL;\n\tstruct siw_umem *umem = NULL;\n\tstruct siw_ureq_reg_mr ureq;\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\n\tunsigned long mem_limit = rlimit(RLIMIT_MEMLOCK);\n\tint rv;\n\n\tsiw_dbg_pd(pd, \"start: 0x%pK, va: 0x%pK, len: %llu\\n\",\n\t\t   (void *)(uintptr_t)start, (void *)(uintptr_t)rnic_va,\n\t\t   (unsigned long long)len);\n\n\tif (atomic_inc_return(&sdev->num_mr) > SIW_MAX_MR) {\n\t\tsiw_dbg_pd(pd, \"too many mr's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tif (!len) {\n\t\trv = -EINVAL;\n\t\tgoto err_out;\n\t}\n\tif (mem_limit != RLIM_INFINITY) {\n\t\tunsigned long num_pages =\n\t\t\t(PAGE_ALIGN(len + (start & ~PAGE_MASK))) >> PAGE_SHIFT;\n\t\tmem_limit >>= PAGE_SHIFT;\n\n\t\tif (num_pages > mem_limit - current->mm->locked_vm) {\n\t\t\tsiw_dbg_pd(pd, \"pages req %lu, max %lu, lock %lu\\n\",\n\t\t\t\t   num_pages, mem_limit,\n\t\t\t\t   current->mm->locked_vm);\n\t\t\trv = -ENOMEM;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tumem = siw_umem_get(start, len, ib_access_writable(rights));\n\tif (IS_ERR(umem)) {\n\t\trv = PTR_ERR(umem);\n\t\tsiw_dbg_pd(pd, \"getting user memory failed: %d\\n\", rv);\n\t\tumem = NULL;\n\t\tgoto err_out;\n\t}\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\trv = siw_mr_add_mem(mr, pd, umem, start, len, rights);\n\tif (rv)\n\t\tgoto err_out;\n\n\tif (udata) {\n\t\tstruct siw_uresp_reg_mr uresp = {};\n\t\tstruct siw_mem *mem = mr->mem;\n\n\t\tif (udata->inlen < sizeof(ureq)) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t\trv = ib_copy_from_udata(&ureq, udata, sizeof(ureq));\n\t\tif (rv)\n\t\t\tgoto err_out;\n\n\t\tmr->base_mr.lkey |= ureq.stag_key;\n\t\tmr->base_mr.rkey |= ureq.stag_key;\n\t\tmem->stag |= ureq.stag_key;\n\t\turesp.stag = mem->stag;\n\n\t\tif (udata->outlen < sizeof(uresp)) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t\trv = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (rv)\n\t\t\tgoto err_out;\n\t}\n\tmr->mem->stag_valid = 1;\n\n\treturn &mr->base_mr;\n\nerr_out:\n\tatomic_dec(&sdev->num_mr);\n\tif (mr) {\n\t\tif (mr->mem)\n\t\t\tsiw_mr_drop_mem(mr);\n\t\tkfree_rcu(mr, rcu);\n\t} else {\n\t\tif (umem)\n\t\t\tsiw_umem_release(umem, false);\n\t}\n\treturn ERR_PTR(rv);\n}\n\nstruct ib_mr *siw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t   u32 max_sge)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mr *mr = NULL;\n\tstruct siw_pbl *pbl = NULL;\n\tint rv;\n\n\tif (atomic_inc_return(&sdev->num_mr) > SIW_MAX_MR) {\n\t\tsiw_dbg_pd(pd, \"too many mr's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tif (mr_type != IB_MR_TYPE_MEM_REG) {\n\t\tsiw_dbg_pd(pd, \"mr type %d unsupported\\n\", mr_type);\n\t\trv = -EOPNOTSUPP;\n\t\tgoto err_out;\n\t}\n\tif (max_sge > SIW_MAX_SGE_PBL) {\n\t\tsiw_dbg_pd(pd, \"too many sge's: %d\\n\", max_sge);\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tpbl = siw_pbl_alloc(max_sge);\n\tif (IS_ERR(pbl)) {\n\t\trv = PTR_ERR(pbl);\n\t\tsiw_dbg_pd(pd, \"pbl allocation failed: %d\\n\", rv);\n\t\tpbl = NULL;\n\t\tgoto err_out;\n\t}\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\trv = siw_mr_add_mem(mr, pd, pbl, 0, max_sge * PAGE_SIZE, 0);\n\tif (rv)\n\t\tgoto err_out;\n\n\tmr->mem->is_pbl = 1;\n\n\tsiw_dbg_pd(pd, \"[MEM %u]: success\\n\", mr->mem->stag);\n\n\treturn &mr->base_mr;\n\nerr_out:\n\tatomic_dec(&sdev->num_mr);\n\n\tif (!mr) {\n\t\tkfree(pbl);\n\t} else {\n\t\tif (mr->mem)\n\t\t\tsiw_mr_drop_mem(mr);\n\t\tkfree_rcu(mr, rcu);\n\t}\n\tsiw_dbg_pd(pd, \"failed: %d\\n\", rv);\n\n\treturn ERR_PTR(rv);\n}\n\n \nstatic int siw_set_pbl_page(struct ib_mr *base_mr, u64 buf_addr)\n{\n\treturn 0;\n}\n\nint siw_map_mr_sg(struct ib_mr *base_mr, struct scatterlist *sl, int num_sle,\n\t\t  unsigned int *sg_off)\n{\n\tstruct scatterlist *slp;\n\tstruct siw_mr *mr = to_siw_mr(base_mr);\n\tstruct siw_mem *mem = mr->mem;\n\tstruct siw_pbl *pbl = mem->pbl;\n\tstruct siw_pble *pble;\n\tunsigned long pbl_size;\n\tint i, rv;\n\n\tif (!pbl) {\n\t\tsiw_dbg_mem(mem, \"no PBL allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\tpble = pbl->pbe;\n\n\tif (pbl->max_buf < num_sle) {\n\t\tsiw_dbg_mem(mem, \"too many SGE's: %d > %d\\n\",\n\t\t\t    num_sle, pbl->max_buf);\n\t\treturn -ENOMEM;\n\t}\n\tfor_each_sg(sl, slp, num_sle, i) {\n\t\tif (sg_dma_len(slp) == 0) {\n\t\t\tsiw_dbg_mem(mem, \"empty SGE\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (i == 0) {\n\t\t\tpble->addr = sg_dma_address(slp);\n\t\t\tpble->size = sg_dma_len(slp);\n\t\t\tpble->pbl_off = 0;\n\t\t\tpbl_size = pble->size;\n\t\t\tpbl->num_buf = 1;\n\t\t} else {\n\t\t\t \n\t\t\tif (pble->addr + pble->size == sg_dma_address(slp)) {\n\t\t\t\tpble->size += sg_dma_len(slp);\n\t\t\t} else {\n\t\t\t\tpble++;\n\t\t\t\tpbl->num_buf++;\n\t\t\t\tpble->addr = sg_dma_address(slp);\n\t\t\t\tpble->size = sg_dma_len(slp);\n\t\t\t\tpble->pbl_off = pbl_size;\n\t\t\t}\n\t\t\tpbl_size += sg_dma_len(slp);\n\t\t}\n\t\tsiw_dbg_mem(mem,\n\t\t\t\"sge[%d], size %u, addr 0x%p, total %lu\\n\",\n\t\t\ti, pble->size, ib_virt_dma_to_ptr(pble->addr),\n\t\t\tpbl_size);\n\t}\n\trv = ib_sg_to_pages(base_mr, sl, num_sle, sg_off, siw_set_pbl_page);\n\tif (rv > 0) {\n\t\tmem->len = base_mr->length;\n\t\tmem->va = base_mr->iova;\n\t\tsiw_dbg_mem(mem,\n\t\t\t\"%llu bytes, start 0x%pK, %u SLE to %u entries\\n\",\n\t\t\tmem->len, (void *)(uintptr_t)mem->va, num_sle,\n\t\t\tpbl->num_buf);\n\t}\n\treturn rv;\n}\n\n \nstruct ib_mr *siw_get_dma_mr(struct ib_pd *pd, int rights)\n{\n\tstruct siw_device *sdev = to_siw_dev(pd->device);\n\tstruct siw_mr *mr = NULL;\n\tint rv;\n\n\tif (atomic_inc_return(&sdev->num_mr) > SIW_MAX_MR) {\n\t\tsiw_dbg_pd(pd, \"too many mr's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tmr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tif (!mr) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\trv = siw_mr_add_mem(mr, pd, NULL, 0, ULONG_MAX, rights);\n\tif (rv)\n\t\tgoto err_out;\n\n\tmr->mem->stag_valid = 1;\n\n\tsiw_dbg_pd(pd, \"[MEM %u]: success\\n\", mr->mem->stag);\n\n\treturn &mr->base_mr;\n\nerr_out:\n\tif (rv)\n\t\tkfree(mr);\n\n\tatomic_dec(&sdev->num_mr);\n\n\treturn ERR_PTR(rv);\n}\n\n \nint siw_create_srq(struct ib_srq *base_srq,\n\t\t   struct ib_srq_init_attr *init_attrs, struct ib_udata *udata)\n{\n\tstruct siw_srq *srq = to_siw_srq(base_srq);\n\tstruct ib_srq_attr *attrs = &init_attrs->attr;\n\tstruct siw_device *sdev = to_siw_dev(base_srq->device);\n\tstruct siw_ucontext *ctx =\n\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t  base_ucontext);\n\tint rv;\n\n\tif (init_attrs->srq_type != IB_SRQT_BASIC)\n\t\treturn -EOPNOTSUPP;\n\n\tif (atomic_inc_return(&sdev->num_srq) > SIW_MAX_SRQ) {\n\t\tsiw_dbg_pd(base_srq->pd, \"too many SRQ's\\n\");\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tif (attrs->max_wr == 0 || attrs->max_wr > SIW_MAX_SRQ_WR ||\n\t    attrs->max_sge > SIW_MAX_SGE || attrs->srq_limit > attrs->max_wr) {\n\t\trv = -EINVAL;\n\t\tgoto err_out;\n\t}\n\tsrq->max_sge = attrs->max_sge;\n\tsrq->num_rqe = roundup_pow_of_two(attrs->max_wr);\n\tsrq->limit = attrs->srq_limit;\n\tif (srq->limit)\n\t\tsrq->armed = true;\n\n\tsrq->is_kernel_res = !udata;\n\n\tif (udata)\n\t\tsrq->recvq =\n\t\t\tvmalloc_user(srq->num_rqe * sizeof(struct siw_rqe));\n\telse\n\t\tsrq->recvq = vcalloc(srq->num_rqe, sizeof(struct siw_rqe));\n\n\tif (srq->recvq == NULL) {\n\t\trv = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tif (udata) {\n\t\tstruct siw_uresp_create_srq uresp = {};\n\t\tsize_t length = srq->num_rqe * sizeof(struct siw_rqe);\n\n\t\tsrq->srq_entry =\n\t\t\tsiw_mmap_entry_insert(ctx, srq->recvq,\n\t\t\t\t\t      length, &uresp.srq_key);\n\t\tif (!srq->srq_entry) {\n\t\t\trv = -ENOMEM;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\turesp.num_rqe = srq->num_rqe;\n\n\t\tif (udata->outlen < sizeof(uresp)) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t\trv = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\n\t\tif (rv)\n\t\t\tgoto err_out;\n\t}\n\tspin_lock_init(&srq->lock);\n\n\tsiw_dbg_pd(base_srq->pd, \"[SRQ]: success\\n\");\n\n\treturn 0;\n\nerr_out:\n\tif (srq->recvq) {\n\t\tif (ctx)\n\t\t\trdma_user_mmap_entry_remove(srq->srq_entry);\n\t\tvfree(srq->recvq);\n\t}\n\tatomic_dec(&sdev->num_srq);\n\n\treturn rv;\n}\n\n \nint siw_modify_srq(struct ib_srq *base_srq, struct ib_srq_attr *attrs,\n\t\t   enum ib_srq_attr_mask attr_mask, struct ib_udata *udata)\n{\n\tstruct siw_srq *srq = to_siw_srq(base_srq);\n\tunsigned long flags;\n\tint rv = 0;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\tif (attr_mask & IB_SRQ_MAX_WR) {\n\t\t \n\t\trv = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\tif (attr_mask & IB_SRQ_LIMIT) {\n\t\tif (attrs->srq_limit) {\n\t\t\tif (unlikely(attrs->srq_limit > srq->num_rqe)) {\n\t\t\t\trv = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsrq->armed = true;\n\t\t} else {\n\t\t\tsrq->armed = false;\n\t\t}\n\t\tsrq->limit = attrs->srq_limit;\n\t}\nout:\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn rv;\n}\n\n \nint siw_query_srq(struct ib_srq *base_srq, struct ib_srq_attr *attrs)\n{\n\tstruct siw_srq *srq = to_siw_srq(base_srq);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&srq->lock, flags);\n\n\tattrs->max_wr = srq->num_rqe;\n\tattrs->max_sge = srq->max_sge;\n\tattrs->srq_limit = srq->limit;\n\n\tspin_unlock_irqrestore(&srq->lock, flags);\n\n\treturn 0;\n}\n\n \nint siw_destroy_srq(struct ib_srq *base_srq, struct ib_udata *udata)\n{\n\tstruct siw_srq *srq = to_siw_srq(base_srq);\n\tstruct siw_device *sdev = to_siw_dev(base_srq->device);\n\tstruct siw_ucontext *ctx =\n\t\trdma_udata_to_drv_context(udata, struct siw_ucontext,\n\t\t\t\t\t  base_ucontext);\n\n\tif (ctx)\n\t\trdma_user_mmap_entry_remove(srq->srq_entry);\n\tvfree(srq->recvq);\n\tatomic_dec(&sdev->num_srq);\n\treturn 0;\n}\n\n \nint siw_post_srq_recv(struct ib_srq *base_srq, const struct ib_recv_wr *wr,\n\t\t      const struct ib_recv_wr **bad_wr)\n{\n\tstruct siw_srq *srq = to_siw_srq(base_srq);\n\tunsigned long flags;\n\tint rv = 0;\n\n\tif (unlikely(!srq->is_kernel_res)) {\n\t\tsiw_dbg_pd(base_srq->pd,\n\t\t\t   \"[SRQ]: no kernel post_recv for mapped srq\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\t \n\tspin_lock_irqsave(&srq->lock, flags);\n\n\twhile (wr) {\n\t\tu32 idx = srq->rq_put % srq->num_rqe;\n\t\tstruct siw_rqe *rqe = &srq->recvq[idx];\n\n\t\tif (rqe->flags) {\n\t\t\tsiw_dbg_pd(base_srq->pd, \"SRQ full\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tif (unlikely(wr->num_sge > srq->max_sge)) {\n\t\t\tsiw_dbg_pd(base_srq->pd,\n\t\t\t\t   \"[SRQ]: too many sge's: %d\\n\", wr->num_sge);\n\t\t\trv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\trqe->id = wr->wr_id;\n\t\trqe->num_sge = wr->num_sge;\n\t\tsiw_copy_sgl(wr->sg_list, rqe->sge, wr->num_sge);\n\n\t\t \n\t\tsmp_wmb();\n\n\t\trqe->flags = SIW_WQE_VALID;\n\n\t\tsrq->rq_put++;\n\t\twr = wr->next;\n\t}\n\tspin_unlock_irqrestore(&srq->lock, flags);\nout:\n\tif (unlikely(rv < 0)) {\n\t\tsiw_dbg_pd(base_srq->pd, \"[SRQ]: error %d\\n\", rv);\n\t\t*bad_wr = wr;\n\t}\n\treturn rv;\n}\n\nvoid siw_qp_event(struct siw_qp *qp, enum ib_event_type etype)\n{\n\tstruct ib_event event;\n\tstruct ib_qp *base_qp = &qp->base_qp;\n\n\t \n\tif (qp->attrs.flags & SIW_QP_IN_DESTROY)\n\t\treturn;\n\n\tevent.event = etype;\n\tevent.device = base_qp->device;\n\tevent.element.qp = base_qp;\n\n\tif (base_qp->event_handler) {\n\t\tsiw_dbg_qp(qp, \"reporting event %d\\n\", etype);\n\t\tbase_qp->event_handler(&event, base_qp->qp_context);\n\t}\n}\n\nvoid siw_cq_event(struct siw_cq *cq, enum ib_event_type etype)\n{\n\tstruct ib_event event;\n\tstruct ib_cq *base_cq = &cq->base_cq;\n\n\tevent.event = etype;\n\tevent.device = base_cq->device;\n\tevent.element.cq = base_cq;\n\n\tif (base_cq->event_handler) {\n\t\tsiw_dbg_cq(cq, \"reporting CQ event %d\\n\", etype);\n\t\tbase_cq->event_handler(&event, base_cq->cq_context);\n\t}\n}\n\nvoid siw_srq_event(struct siw_srq *srq, enum ib_event_type etype)\n{\n\tstruct ib_event event;\n\tstruct ib_srq *base_srq = &srq->base_srq;\n\n\tevent.event = etype;\n\tevent.device = base_srq->device;\n\tevent.element.srq = base_srq;\n\n\tif (base_srq->event_handler) {\n\t\tsiw_dbg_pd(srq->base_srq.pd,\n\t\t\t   \"reporting SRQ event %d\\n\", etype);\n\t\tbase_srq->event_handler(&event, base_srq->srq_context);\n\t}\n}\n\nvoid siw_port_event(struct siw_device *sdev, u32 port, enum ib_event_type etype)\n{\n\tstruct ib_event event;\n\n\tevent.event = etype;\n\tevent.device = &sdev->base_dev;\n\tevent.element.port_num = port;\n\n\tsiw_dbg(&sdev->base_dev, \"reporting port event %d\\n\", etype);\n\n\tib_dispatch_event(&event);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}