{
  "module_name": "binder_alloc.c",
  "hash_id": "7778428e9bac44dad878dc2245f2464cfd430bce7160a3f63b671d8ddba89eff",
  "original_prompt": "Ingested from linux-6.6.14/drivers/android/binder_alloc.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/list.h>\n#include <linux/sched/mm.h>\n#include <linux/module.h>\n#include <linux/rtmutex.h>\n#include <linux/rbtree.h>\n#include <linux/seq_file.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/list_lru.h>\n#include <linux/ratelimit.h>\n#include <asm/cacheflush.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <linux/sizes.h>\n#include \"binder_alloc.h\"\n#include \"binder_trace.h\"\n\nstruct list_lru binder_alloc_lru;\n\nstatic DEFINE_MUTEX(binder_alloc_mmap_lock);\n\nenum {\n\tBINDER_DEBUG_USER_ERROR             = 1U << 0,\n\tBINDER_DEBUG_OPEN_CLOSE             = 1U << 1,\n\tBINDER_DEBUG_BUFFER_ALLOC           = 1U << 2,\n\tBINDER_DEBUG_BUFFER_ALLOC_ASYNC     = 1U << 3,\n};\nstatic uint32_t binder_alloc_debug_mask = BINDER_DEBUG_USER_ERROR;\n\nmodule_param_named(debug_mask, binder_alloc_debug_mask,\n\t\t   uint, 0644);\n\n#define binder_alloc_debug(mask, x...) \\\n\tdo { \\\n\t\tif (binder_alloc_debug_mask & mask) \\\n\t\t\tpr_info_ratelimited(x); \\\n\t} while (0)\n\nstatic struct binder_buffer *binder_buffer_next(struct binder_buffer *buffer)\n{\n\treturn list_entry(buffer->entry.next, struct binder_buffer, entry);\n}\n\nstatic struct binder_buffer *binder_buffer_prev(struct binder_buffer *buffer)\n{\n\treturn list_entry(buffer->entry.prev, struct binder_buffer, entry);\n}\n\nstatic size_t binder_alloc_buffer_size(struct binder_alloc *alloc,\n\t\t\t\t       struct binder_buffer *buffer)\n{\n\tif (list_is_last(&buffer->entry, &alloc->buffers))\n\t\treturn alloc->buffer + alloc->buffer_size - buffer->user_data;\n\treturn binder_buffer_next(buffer)->user_data - buffer->user_data;\n}\n\nstatic void binder_insert_free_buffer(struct binder_alloc *alloc,\n\t\t\t\t      struct binder_buffer *new_buffer)\n{\n\tstruct rb_node **p = &alloc->free_buffers.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_buffer *buffer;\n\tsize_t buffer_size;\n\tsize_t new_buffer_size;\n\n\tBUG_ON(!new_buffer->free);\n\n\tnew_buffer_size = binder_alloc_buffer_size(alloc, new_buffer);\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: add free buffer, size %zd, at %pK\\n\",\n\t\t      alloc->pid, new_buffer_size, new_buffer);\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tbuffer = rb_entry(parent, struct binder_buffer, rb_node);\n\t\tBUG_ON(!buffer->free);\n\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\t\tif (new_buffer_size < buffer_size)\n\t\t\tp = &parent->rb_left;\n\t\telse\n\t\t\tp = &parent->rb_right;\n\t}\n\trb_link_node(&new_buffer->rb_node, parent, p);\n\trb_insert_color(&new_buffer->rb_node, &alloc->free_buffers);\n}\n\nstatic void binder_insert_allocated_buffer_locked(\n\t\tstruct binder_alloc *alloc, struct binder_buffer *new_buffer)\n{\n\tstruct rb_node **p = &alloc->allocated_buffers.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_buffer *buffer;\n\n\tBUG_ON(new_buffer->free);\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tbuffer = rb_entry(parent, struct binder_buffer, rb_node);\n\t\tBUG_ON(buffer->free);\n\n\t\tif (new_buffer->user_data < buffer->user_data)\n\t\t\tp = &parent->rb_left;\n\t\telse if (new_buffer->user_data > buffer->user_data)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\trb_link_node(&new_buffer->rb_node, parent, p);\n\trb_insert_color(&new_buffer->rb_node, &alloc->allocated_buffers);\n}\n\nstatic struct binder_buffer *binder_alloc_prepare_to_free_locked(\n\t\tstruct binder_alloc *alloc,\n\t\tuintptr_t user_ptr)\n{\n\tstruct rb_node *n = alloc->allocated_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tvoid __user *uptr;\n\n\tuptr = (void __user *)user_ptr;\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(buffer->free);\n\n\t\tif (uptr < buffer->user_data)\n\t\t\tn = n->rb_left;\n\t\telse if (uptr > buffer->user_data)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\t \n\t\t\tif (!buffer->allow_user_free)\n\t\t\t\treturn ERR_PTR(-EPERM);\n\t\t\tbuffer->allow_user_free = 0;\n\t\t\treturn buffer;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstruct binder_buffer *binder_alloc_prepare_to_free(struct binder_alloc *alloc,\n\t\t\t\t\t\t   uintptr_t user_ptr)\n{\n\tstruct binder_buffer *buffer;\n\n\tmutex_lock(&alloc->mutex);\n\tbuffer = binder_alloc_prepare_to_free_locked(alloc, user_ptr);\n\tmutex_unlock(&alloc->mutex);\n\treturn buffer;\n}\n\nstatic int binder_update_page_range(struct binder_alloc *alloc, int allocate,\n\t\t\t\t    void __user *start, void __user *end)\n{\n\tvoid __user *page_addr;\n\tunsigned long user_page_addr;\n\tstruct binder_lru_page *page;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = NULL;\n\tbool need_mm = false;\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: %s pages %pK-%pK\\n\", alloc->pid,\n\t\t     allocate ? \"allocate\" : \"free\", start, end);\n\n\tif (end <= start)\n\t\treturn 0;\n\n\ttrace_binder_update_page_range(alloc, allocate, start, end);\n\n\tif (allocate == 0)\n\t\tgoto free_range;\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tpage = &alloc->pages[(page_addr - alloc->buffer) / PAGE_SIZE];\n\t\tif (!page->page_ptr) {\n\t\t\tneed_mm = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (need_mm && mmget_not_zero(alloc->mm))\n\t\tmm = alloc->mm;\n\n\tif (mm) {\n\t\tmmap_write_lock(mm);\n\t\tvma = alloc->vma;\n\t}\n\n\tif (!vma && need_mm) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf failed to map pages in userspace, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\tgoto err_no_vma;\n\t}\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tint ret;\n\t\tbool on_lru;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\tif (page->page_ptr) {\n\t\t\ttrace_binder_alloc_lru_start(alloc, index);\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru, &page->lru);\n\t\t\tWARN_ON(!on_lru);\n\n\t\t\ttrace_binder_alloc_lru_end(alloc, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (WARN_ON(!vma))\n\t\t\tgoto err_page_ptr_cleared;\n\n\t\ttrace_binder_alloc_page_start(alloc, index);\n\t\tpage->page_ptr = alloc_page(GFP_KERNEL |\n\t\t\t\t\t    __GFP_HIGHMEM |\n\t\t\t\t\t    __GFP_ZERO);\n\t\tif (!page->page_ptr) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed for page at %pK\\n\",\n\t\t\t\talloc->pid, page_addr);\n\t\t\tgoto err_alloc_page_failed;\n\t\t}\n\t\tpage->alloc = alloc;\n\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\tuser_page_addr = (uintptr_t)page_addr;\n\t\tret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\",\n\t\t\t       alloc->pid, user_page_addr);\n\t\t\tgoto err_vm_insert_page_failed;\n\t\t}\n\n\t\tif (index + 1 > alloc->pages_high)\n\t\t\talloc->pages_high = index + 1;\n\n\t\ttrace_binder_alloc_page_end(alloc, index);\n\t}\n\tif (mm) {\n\t\tmmap_write_unlock(mm);\n\t\tmmput_async(mm);\n\t}\n\treturn 0;\n\nfree_range:\n\tfor (page_addr = end - PAGE_SIZE; 1; page_addr -= PAGE_SIZE) {\n\t\tbool ret;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\ttrace_binder_free_lru_start(alloc, index);\n\n\t\tret = list_lru_add(&binder_alloc_lru, &page->lru);\n\t\tWARN_ON(!ret);\n\n\t\ttrace_binder_free_lru_end(alloc, index);\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t\tcontinue;\n\nerr_vm_insert_page_failed:\n\t\t__free_page(page->page_ptr);\n\t\tpage->page_ptr = NULL;\nerr_alloc_page_failed:\nerr_page_ptr_cleared:\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t}\nerr_no_vma:\n\tif (mm) {\n\t\tmmap_write_unlock(mm);\n\t\tmmput_async(mm);\n\t}\n\treturn vma ? -ENOMEM : -ESRCH;\n}\n\nstatic inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n\t\tstruct vm_area_struct *vma)\n{\n\t \n\tsmp_store_release(&alloc->vma, vma);\n}\n\nstatic inline struct vm_area_struct *binder_alloc_get_vma(\n\t\tstruct binder_alloc *alloc)\n{\n\t \n\treturn smp_load_acquire(&alloc->vma);\n}\n\nstatic bool debug_low_async_space_locked(struct binder_alloc *alloc, int pid)\n{\n\t \n\tstruct rb_node *n;\n\tstruct binder_buffer *buffer;\n\tsize_t total_alloc_size = 0;\n\tsize_t num_buffers = 0;\n\n\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL;\n\t\t n = rb_next(n)) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tif (buffer->pid != pid)\n\t\t\tcontinue;\n\t\tif (!buffer->async_transaction)\n\t\t\tcontinue;\n\t\ttotal_alloc_size += binder_alloc_buffer_size(alloc, buffer);\n\t\tnum_buffers++;\n\t}\n\n\t \n\tif (num_buffers > 50 || total_alloc_size > alloc->buffer_size / 4) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t     \"%d: pid %d spamming oneway? %zd buffers allocated for a total size of %zd\\n\",\n\t\t\t      alloc->pid, pid, num_buffers, total_alloc_size);\n\t\tif (!alloc->oneway_spam_detected) {\n\t\t\talloc->oneway_spam_detected = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic struct binder_buffer *binder_alloc_new_buf_locked(\n\t\t\t\tstruct binder_alloc *alloc,\n\t\t\t\tsize_t data_size,\n\t\t\t\tsize_t offsets_size,\n\t\t\t\tsize_t extra_buffers_size,\n\t\t\t\tint is_async,\n\t\t\t\tint pid)\n{\n\tstruct rb_node *n = alloc->free_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tsize_t buffer_size;\n\tstruct rb_node *best_fit = NULL;\n\tvoid __user *has_page_addr;\n\tvoid __user *end_page_addr;\n\tsize_t size, data_offsets_size;\n\tint ret;\n\n\t \n\tif (!binder_alloc_get_vma(alloc)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\treturn ERR_PTR(-ESRCH);\n\t}\n\n\tdata_offsets_size = ALIGN(data_size, sizeof(void *)) +\n\t\tALIGN(offsets_size, sizeof(void *));\n\n\tif (data_offsets_size < data_size || data_offsets_size < offsets_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid size %zd-%zd\\n\",\n\t\t\t\talloc->pid, data_size, offsets_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tsize = data_offsets_size + ALIGN(extra_buffers_size, sizeof(void *));\n\tif (size < data_offsets_size || size < extra_buffers_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid extra_buffers_size %zd\\n\",\n\t\t\t\talloc->pid, extra_buffers_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tsize = max(size, sizeof(void *));\n\n\tif (is_async && alloc->free_async_space < size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t     \"%d: binder_alloc_buf size %zd failed, no async space left\\n\",\n\t\t\t      alloc->pid, size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(!buffer->free);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\t\tif (size < buffer_size) {\n\t\t\tbest_fit = n;\n\t\t\tn = n->rb_left;\n\t\t} else if (size > buffer_size)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\tbest_fit = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (best_fit == NULL) {\n\t\tsize_t allocated_buffers = 0;\n\t\tsize_t largest_alloc_size = 0;\n\t\tsize_t total_alloc_size = 0;\n\t\tsize_t free_buffers = 0;\n\t\tsize_t largest_free_size = 0;\n\t\tsize_t total_free_size = 0;\n\n\t\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tallocated_buffers++;\n\t\t\ttotal_alloc_size += buffer_size;\n\t\t\tif (buffer_size > largest_alloc_size)\n\t\t\t\tlargest_alloc_size = buffer_size;\n\t\t}\n\t\tfor (n = rb_first(&alloc->free_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tfree_buffers++;\n\t\t\ttotal_free_size += buffer_size;\n\t\t\tif (buffer_size > largest_free_size)\n\t\t\t\tlargest_free_size = buffer_size;\n\t\t}\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf size %zd failed, no address space\\n\",\n\t\t\t\t   alloc->pid, size);\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"allocated: %zd (num: %zd largest: %zd), free: %zd (num: %zd largest: %zd)\\n\",\n\t\t\t\t   total_alloc_size, allocated_buffers,\n\t\t\t\t   largest_alloc_size, total_free_size,\n\t\t\t\t   free_buffers, largest_free_size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\tif (n == NULL) {\n\t\tbuffer = rb_entry(best_fit, struct binder_buffer, rb_node);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t}\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got buffer %pK size %zd\\n\",\n\t\t      alloc->pid, size, buffer, buffer_size);\n\n\thas_page_addr = (void __user *)\n\t\t(((uintptr_t)buffer->user_data + buffer_size) & PAGE_MASK);\n\tWARN_ON(n && buffer_size != size);\n\tend_page_addr =\n\t\t(void __user *)PAGE_ALIGN((uintptr_t)buffer->user_data + size);\n\tif (end_page_addr > has_page_addr)\n\t\tend_page_addr = has_page_addr;\n\tret = binder_update_page_range(alloc, 1, (void __user *)\n\t\tPAGE_ALIGN((uintptr_t)buffer->user_data), end_page_addr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (buffer_size != size) {\n\t\tstruct binder_buffer *new_buffer;\n\n\t\tnew_buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\t\tif (!new_buffer) {\n\t\t\tpr_err(\"%s: %d failed to alloc new buffer struct\\n\",\n\t\t\t       __func__, alloc->pid);\n\t\t\tgoto err_alloc_buf_struct_failed;\n\t\t}\n\t\tnew_buffer->user_data = (u8 __user *)buffer->user_data + size;\n\t\tlist_add(&new_buffer->entry, &buffer->entry);\n\t\tnew_buffer->free = 1;\n\t\tbinder_insert_free_buffer(alloc, new_buffer);\n\t}\n\n\trb_erase(best_fit, &alloc->free_buffers);\n\tbuffer->free = 0;\n\tbuffer->allow_user_free = 0;\n\tbinder_insert_allocated_buffer_locked(alloc, buffer);\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got %pK\\n\",\n\t\t      alloc->pid, size, buffer);\n\tbuffer->data_size = data_size;\n\tbuffer->offsets_size = offsets_size;\n\tbuffer->async_transaction = is_async;\n\tbuffer->extra_buffers_size = extra_buffers_size;\n\tbuffer->pid = pid;\n\tbuffer->oneway_spam_suspect = false;\n\tif (is_async) {\n\t\talloc->free_async_space -= size;\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC_ASYNC,\n\t\t\t     \"%d: binder_alloc_buf size %zd async free %zd\\n\",\n\t\t\t      alloc->pid, size, alloc->free_async_space);\n\t\tif (alloc->free_async_space < alloc->buffer_size / 10) {\n\t\t\t \n\t\t\tbuffer->oneway_spam_suspect = debug_low_async_space_locked(alloc, pid);\n\t\t} else {\n\t\t\talloc->oneway_spam_detected = false;\n\t\t}\n\t}\n\treturn buffer;\n\nerr_alloc_buf_struct_failed:\n\tbinder_update_page_range(alloc, 0, (void __user *)\n\t\t\t\t PAGE_ALIGN((uintptr_t)buffer->user_data),\n\t\t\t\t end_page_addr);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nstruct binder_buffer *binder_alloc_new_buf(struct binder_alloc *alloc,\n\t\t\t\t\t   size_t data_size,\n\t\t\t\t\t   size_t offsets_size,\n\t\t\t\t\t   size_t extra_buffers_size,\n\t\t\t\t\t   int is_async,\n\t\t\t\t\t   int pid)\n{\n\tstruct binder_buffer *buffer;\n\n\tmutex_lock(&alloc->mutex);\n\tbuffer = binder_alloc_new_buf_locked(alloc, data_size, offsets_size,\n\t\t\t\t\t     extra_buffers_size, is_async, pid);\n\tmutex_unlock(&alloc->mutex);\n\treturn buffer;\n}\n\nstatic void __user *buffer_start_page(struct binder_buffer *buffer)\n{\n\treturn (void __user *)((uintptr_t)buffer->user_data & PAGE_MASK);\n}\n\nstatic void __user *prev_buffer_end_page(struct binder_buffer *buffer)\n{\n\treturn (void __user *)\n\t\t(((uintptr_t)(buffer->user_data) - 1) & PAGE_MASK);\n}\n\nstatic void binder_delete_free_buffer(struct binder_alloc *alloc,\n\t\t\t\t      struct binder_buffer *buffer)\n{\n\tstruct binder_buffer *prev, *next = NULL;\n\tbool to_free = true;\n\n\tBUG_ON(alloc->buffers.next == &buffer->entry);\n\tprev = binder_buffer_prev(buffer);\n\tBUG_ON(!prev->free);\n\tif (prev_buffer_end_page(prev) == buffer_start_page(buffer)) {\n\t\tto_free = false;\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t   \"%d: merge free, buffer %pK share page with %pK\\n\",\n\t\t\t\t   alloc->pid, buffer->user_data,\n\t\t\t\t   prev->user_data);\n\t}\n\n\tif (!list_is_last(&buffer->entry, &alloc->buffers)) {\n\t\tnext = binder_buffer_next(buffer);\n\t\tif (buffer_start_page(next) == buffer_start_page(buffer)) {\n\t\t\tto_free = false;\n\t\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\t   \"%d: merge free, buffer %pK share page with %pK\\n\",\n\t\t\t\t\t   alloc->pid,\n\t\t\t\t\t   buffer->user_data,\n\t\t\t\t\t   next->user_data);\n\t\t}\n\t}\n\n\tif (PAGE_ALIGNED(buffer->user_data)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t   \"%d: merge free, buffer start %pK is page aligned\\n\",\n\t\t\t\t   alloc->pid, buffer->user_data);\n\t\tto_free = false;\n\t}\n\n\tif (to_free) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t   \"%d: merge free, buffer %pK do not share page with %pK or %pK\\n\",\n\t\t\t\t   alloc->pid, buffer->user_data,\n\t\t\t\t   prev->user_data,\n\t\t\t\t   next ? next->user_data : NULL);\n\t\tbinder_update_page_range(alloc, 0, buffer_start_page(buffer),\n\t\t\t\t\t buffer_start_page(buffer) + PAGE_SIZE);\n\t}\n\tlist_del(&buffer->entry);\n\tkfree(buffer);\n}\n\nstatic void binder_free_buf_locked(struct binder_alloc *alloc,\n\t\t\t\t   struct binder_buffer *buffer)\n{\n\tsize_t size, buffer_size;\n\n\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\tsize = ALIGN(buffer->data_size, sizeof(void *)) +\n\t\tALIGN(buffer->offsets_size, sizeof(void *)) +\n\t\tALIGN(buffer->extra_buffers_size, sizeof(void *));\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_free_buf %pK size %zd buffer_size %zd\\n\",\n\t\t      alloc->pid, buffer, size, buffer_size);\n\n\tBUG_ON(buffer->free);\n\tBUG_ON(size > buffer_size);\n\tBUG_ON(buffer->transaction != NULL);\n\tBUG_ON(buffer->user_data < alloc->buffer);\n\tBUG_ON(buffer->user_data > alloc->buffer + alloc->buffer_size);\n\n\tif (buffer->async_transaction) {\n\t\talloc->free_async_space += buffer_size;\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC_ASYNC,\n\t\t\t     \"%d: binder_free_buf size %zd async free %zd\\n\",\n\t\t\t      alloc->pid, size, alloc->free_async_space);\n\t}\n\n\tbinder_update_page_range(alloc, 0,\n\t\t(void __user *)PAGE_ALIGN((uintptr_t)buffer->user_data),\n\t\t(void __user *)(((uintptr_t)\n\t\t\t  buffer->user_data + buffer_size) & PAGE_MASK));\n\n\trb_erase(&buffer->rb_node, &alloc->allocated_buffers);\n\tbuffer->free = 1;\n\tif (!list_is_last(&buffer->entry, &alloc->buffers)) {\n\t\tstruct binder_buffer *next = binder_buffer_next(buffer);\n\n\t\tif (next->free) {\n\t\t\trb_erase(&next->rb_node, &alloc->free_buffers);\n\t\t\tbinder_delete_free_buffer(alloc, next);\n\t\t}\n\t}\n\tif (alloc->buffers.next != &buffer->entry) {\n\t\tstruct binder_buffer *prev = binder_buffer_prev(buffer);\n\n\t\tif (prev->free) {\n\t\t\tbinder_delete_free_buffer(alloc, buffer);\n\t\t\trb_erase(&prev->rb_node, &alloc->free_buffers);\n\t\t\tbuffer = prev;\n\t\t}\n\t}\n\tbinder_insert_free_buffer(alloc, buffer);\n}\n\nstatic void binder_alloc_clear_buf(struct binder_alloc *alloc,\n\t\t\t\t   struct binder_buffer *buffer);\n \nvoid binder_alloc_free_buf(struct binder_alloc *alloc,\n\t\t\t    struct binder_buffer *buffer)\n{\n\t \n\tif (buffer->clear_on_free) {\n\t\tbinder_alloc_clear_buf(alloc, buffer);\n\t\tbuffer->clear_on_free = false;\n\t}\n\tmutex_lock(&alloc->mutex);\n\tbinder_free_buf_locked(alloc, buffer);\n\tmutex_unlock(&alloc->mutex);\n}\n\n \nint binder_alloc_mmap_handler(struct binder_alloc *alloc,\n\t\t\t      struct vm_area_struct *vma)\n{\n\tint ret;\n\tconst char *failure_string;\n\tstruct binder_buffer *buffer;\n\n\tif (unlikely(vma->vm_mm != alloc->mm)) {\n\t\tret = -EINVAL;\n\t\tfailure_string = \"invalid vma->vm_mm\";\n\t\tgoto err_invalid_mm;\n\t}\n\n\tmutex_lock(&binder_alloc_mmap_lock);\n\tif (alloc->buffer_size) {\n\t\tret = -EBUSY;\n\t\tfailure_string = \"already mapped\";\n\t\tgoto err_already_mapped;\n\t}\n\talloc->buffer_size = min_t(unsigned long, vma->vm_end - vma->vm_start,\n\t\t\t\t   SZ_4M);\n\tmutex_unlock(&binder_alloc_mmap_lock);\n\n\talloc->buffer = (void __user *)vma->vm_start;\n\n\talloc->pages = kcalloc(alloc->buffer_size / PAGE_SIZE,\n\t\t\t       sizeof(alloc->pages[0]),\n\t\t\t       GFP_KERNEL);\n\tif (alloc->pages == NULL) {\n\t\tret = -ENOMEM;\n\t\tfailure_string = \"alloc page array\";\n\t\tgoto err_alloc_pages_failed;\n\t}\n\n\tbuffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\tif (!buffer) {\n\t\tret = -ENOMEM;\n\t\tfailure_string = \"alloc buffer struct\";\n\t\tgoto err_alloc_buf_struct_failed;\n\t}\n\n\tbuffer->user_data = alloc->buffer;\n\tlist_add(&buffer->entry, &alloc->buffers);\n\tbuffer->free = 1;\n\tbinder_insert_free_buffer(alloc, buffer);\n\talloc->free_async_space = alloc->buffer_size / 2;\n\n\t \n\tbinder_alloc_set_vma(alloc, vma);\n\n\treturn 0;\n\nerr_alloc_buf_struct_failed:\n\tkfree(alloc->pages);\n\talloc->pages = NULL;\nerr_alloc_pages_failed:\n\talloc->buffer = NULL;\n\tmutex_lock(&binder_alloc_mmap_lock);\n\talloc->buffer_size = 0;\nerr_already_mapped:\n\tmutex_unlock(&binder_alloc_mmap_lock);\nerr_invalid_mm:\n\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t   \"%s: %d %lx-%lx %s failed %d\\n\", __func__,\n\t\t\t   alloc->pid, vma->vm_start, vma->vm_end,\n\t\t\t   failure_string, ret);\n\treturn ret;\n}\n\n\nvoid binder_alloc_deferred_release(struct binder_alloc *alloc)\n{\n\tstruct rb_node *n;\n\tint buffers, page_count;\n\tstruct binder_buffer *buffer;\n\n\tbuffers = 0;\n\tmutex_lock(&alloc->mutex);\n\tBUG_ON(alloc->vma);\n\n\twhile ((n = rb_first(&alloc->allocated_buffers))) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\n\t\t \n\t\tBUG_ON(buffer->transaction);\n\n\t\tif (buffer->clear_on_free) {\n\t\t\tbinder_alloc_clear_buf(alloc, buffer);\n\t\t\tbuffer->clear_on_free = false;\n\t\t}\n\t\tbinder_free_buf_locked(alloc, buffer);\n\t\tbuffers++;\n\t}\n\n\twhile (!list_empty(&alloc->buffers)) {\n\t\tbuffer = list_first_entry(&alloc->buffers,\n\t\t\t\t\t  struct binder_buffer, entry);\n\t\tWARN_ON(!buffer->free);\n\n\t\tlist_del(&buffer->entry);\n\t\tWARN_ON_ONCE(!list_empty(&alloc->buffers));\n\t\tkfree(buffer);\n\t}\n\n\tpage_count = 0;\n\tif (alloc->pages) {\n\t\tint i;\n\n\t\tfor (i = 0; i < alloc->buffer_size / PAGE_SIZE; i++) {\n\t\t\tvoid __user *page_addr;\n\t\t\tbool on_lru;\n\n\t\t\tif (!alloc->pages[i].page_ptr)\n\t\t\t\tcontinue;\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru,\n\t\t\t\t\t      &alloc->pages[i].lru);\n\t\t\tpage_addr = alloc->buffer + i * PAGE_SIZE;\n\t\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t     \"%s: %d: page %d at %pK %s\\n\",\n\t\t\t\t     __func__, alloc->pid, i, page_addr,\n\t\t\t\t     on_lru ? \"on lru\" : \"active\");\n\t\t\t__free_page(alloc->pages[i].page_ptr);\n\t\t\tpage_count++;\n\t\t}\n\t\tkfree(alloc->pages);\n\t}\n\tmutex_unlock(&alloc->mutex);\n\tif (alloc->mm)\n\t\tmmdrop(alloc->mm);\n\n\tbinder_alloc_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d buffers %d, pages %d\\n\",\n\t\t     __func__, alloc->pid, buffers, page_count);\n}\n\nstatic void print_binder_buffer(struct seq_file *m, const char *prefix,\n\t\t\t\tstruct binder_buffer *buffer)\n{\n\tseq_printf(m, \"%s %d: %pK size %zd:%zd:%zd %s\\n\",\n\t\t   prefix, buffer->debug_id, buffer->user_data,\n\t\t   buffer->data_size, buffer->offsets_size,\n\t\t   buffer->extra_buffers_size,\n\t\t   buffer->transaction ? \"active\" : \"delivered\");\n}\n\n \nvoid binder_alloc_print_allocated(struct seq_file *m,\n\t\t\t\t  struct binder_alloc *alloc)\n{\n\tstruct rb_node *n;\n\n\tmutex_lock(&alloc->mutex);\n\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL; n = rb_next(n))\n\t\tprint_binder_buffer(m, \"  buffer\",\n\t\t\t\t    rb_entry(n, struct binder_buffer, rb_node));\n\tmutex_unlock(&alloc->mutex);\n}\n\n \nvoid binder_alloc_print_pages(struct seq_file *m,\n\t\t\t      struct binder_alloc *alloc)\n{\n\tstruct binder_lru_page *page;\n\tint i;\n\tint active = 0;\n\tint lru = 0;\n\tint free = 0;\n\n\tmutex_lock(&alloc->mutex);\n\t \n\tif (binder_alloc_get_vma(alloc) != NULL) {\n\t\tfor (i = 0; i < alloc->buffer_size / PAGE_SIZE; i++) {\n\t\t\tpage = &alloc->pages[i];\n\t\t\tif (!page->page_ptr)\n\t\t\t\tfree++;\n\t\t\telse if (list_empty(&page->lru))\n\t\t\t\tactive++;\n\t\t\telse\n\t\t\t\tlru++;\n\t\t}\n\t}\n\tmutex_unlock(&alloc->mutex);\n\tseq_printf(m, \"  pages: %d:%d:%d\\n\", active, lru, free);\n\tseq_printf(m, \"  pages high watermark: %zu\\n\", alloc->pages_high);\n}\n\n \nint binder_alloc_get_allocated_count(struct binder_alloc *alloc)\n{\n\tstruct rb_node *n;\n\tint count = 0;\n\n\tmutex_lock(&alloc->mutex);\n\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tmutex_unlock(&alloc->mutex);\n\treturn count;\n}\n\n\n \nvoid binder_alloc_vma_close(struct binder_alloc *alloc)\n{\n\tbinder_alloc_set_vma(alloc, NULL);\n}\n\n \nenum lru_status binder_alloc_free_page(struct list_head *item,\n\t\t\t\t       struct list_lru_one *lru,\n\t\t\t\t       spinlock_t *lock,\n\t\t\t\t       void *cb_arg)\n\t__must_hold(lock)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct binder_lru_page *page = container_of(item,\n\t\t\t\t\t\t    struct binder_lru_page,\n\t\t\t\t\t\t    lru);\n\tstruct binder_alloc *alloc;\n\tuintptr_t page_addr;\n\tsize_t index;\n\tstruct vm_area_struct *vma;\n\n\talloc = page->alloc;\n\tif (!mutex_trylock(&alloc->mutex))\n\t\tgoto err_get_alloc_mutex_failed;\n\n\tif (!page->page_ptr)\n\t\tgoto err_page_already_freed;\n\n\tindex = page - alloc->pages;\n\tpage_addr = (uintptr_t)alloc->buffer + index * PAGE_SIZE;\n\n\tmm = alloc->mm;\n\tif (!mmget_not_zero(mm))\n\t\tgoto err_mmget;\n\tif (!mmap_read_trylock(mm))\n\t\tgoto err_mmap_read_lock_failed;\n\tvma = vma_lookup(mm, page_addr);\n\tif (vma && vma != binder_alloc_get_vma(alloc))\n\t\tgoto err_invalid_vma;\n\n\tlist_lru_isolate(lru, item);\n\tspin_unlock(lock);\n\n\tif (vma) {\n\t\ttrace_binder_unmap_user_start(alloc, index);\n\n\t\tzap_page_range_single(vma, page_addr, PAGE_SIZE, NULL);\n\n\t\ttrace_binder_unmap_user_end(alloc, index);\n\t}\n\tmmap_read_unlock(mm);\n\tmmput_async(mm);\n\n\ttrace_binder_unmap_kernel_start(alloc, index);\n\n\t__free_page(page->page_ptr);\n\tpage->page_ptr = NULL;\n\n\ttrace_binder_unmap_kernel_end(alloc, index);\n\n\tspin_lock(lock);\n\tmutex_unlock(&alloc->mutex);\n\treturn LRU_REMOVED_RETRY;\n\nerr_invalid_vma:\n\tmmap_read_unlock(mm);\nerr_mmap_read_lock_failed:\n\tmmput_async(mm);\nerr_mmget:\nerr_page_already_freed:\n\tmutex_unlock(&alloc->mutex);\nerr_get_alloc_mutex_failed:\n\treturn LRU_SKIP;\n}\n\nstatic unsigned long\nbinder_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\treturn list_lru_count(&binder_alloc_lru);\n}\n\nstatic unsigned long\nbinder_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\treturn list_lru_walk(&binder_alloc_lru, binder_alloc_free_page,\n\t\t\t    NULL, sc->nr_to_scan);\n}\n\nstatic struct shrinker binder_shrinker = {\n\t.count_objects = binder_shrink_count,\n\t.scan_objects = binder_shrink_scan,\n\t.seeks = DEFAULT_SEEKS,\n};\n\n \nvoid binder_alloc_init(struct binder_alloc *alloc)\n{\n\talloc->pid = current->group_leader->pid;\n\talloc->mm = current->mm;\n\tmmgrab(alloc->mm);\n\tmutex_init(&alloc->mutex);\n\tINIT_LIST_HEAD(&alloc->buffers);\n}\n\nint binder_alloc_shrinker_init(void)\n{\n\tint ret = list_lru_init(&binder_alloc_lru);\n\n\tif (ret == 0) {\n\t\tret = register_shrinker(&binder_shrinker, \"android-binder\");\n\t\tif (ret)\n\t\t\tlist_lru_destroy(&binder_alloc_lru);\n\t}\n\treturn ret;\n}\n\nvoid binder_alloc_shrinker_exit(void)\n{\n\tunregister_shrinker(&binder_shrinker);\n\tlist_lru_destroy(&binder_alloc_lru);\n}\n\n \nstatic inline bool check_buffer(struct binder_alloc *alloc,\n\t\t\t\tstruct binder_buffer *buffer,\n\t\t\t\tbinder_size_t offset, size_t bytes)\n{\n\tsize_t buffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\treturn buffer_size >= bytes &&\n\t\toffset <= buffer_size - bytes &&\n\t\tIS_ALIGNED(offset, sizeof(u32)) &&\n\t\t!buffer->free &&\n\t\t(!buffer->allow_user_free || !buffer->transaction);\n}\n\n \nstatic struct page *binder_alloc_get_page(struct binder_alloc *alloc,\n\t\t\t\t\t  struct binder_buffer *buffer,\n\t\t\t\t\t  binder_size_t buffer_offset,\n\t\t\t\t\t  pgoff_t *pgoffp)\n{\n\tbinder_size_t buffer_space_offset = buffer_offset +\n\t\t(buffer->user_data - alloc->buffer);\n\tpgoff_t pgoff = buffer_space_offset & ~PAGE_MASK;\n\tsize_t index = buffer_space_offset >> PAGE_SHIFT;\n\tstruct binder_lru_page *lru_page;\n\n\tlru_page = &alloc->pages[index];\n\t*pgoffp = pgoff;\n\treturn lru_page->page_ptr;\n}\n\n \nstatic void binder_alloc_clear_buf(struct binder_alloc *alloc,\n\t\t\t\t   struct binder_buffer *buffer)\n{\n\tsize_t bytes = binder_alloc_buffer_size(alloc, buffer);\n\tbinder_size_t buffer_offset = 0;\n\n\twhile (bytes) {\n\t\tunsigned long size;\n\t\tstruct page *page;\n\t\tpgoff_t pgoff;\n\n\t\tpage = binder_alloc_get_page(alloc, buffer,\n\t\t\t\t\t     buffer_offset, &pgoff);\n\t\tsize = min_t(size_t, bytes, PAGE_SIZE - pgoff);\n\t\tmemset_page(page, pgoff, 0, size);\n\t\tbytes -= size;\n\t\tbuffer_offset += size;\n\t}\n}\n\n \nunsigned long\nbinder_alloc_copy_user_to_buffer(struct binder_alloc *alloc,\n\t\t\t\t struct binder_buffer *buffer,\n\t\t\t\t binder_size_t buffer_offset,\n\t\t\t\t const void __user *from,\n\t\t\t\t size_t bytes)\n{\n\tif (!check_buffer(alloc, buffer, buffer_offset, bytes))\n\t\treturn bytes;\n\n\twhile (bytes) {\n\t\tunsigned long size;\n\t\tunsigned long ret;\n\t\tstruct page *page;\n\t\tpgoff_t pgoff;\n\t\tvoid *kptr;\n\n\t\tpage = binder_alloc_get_page(alloc, buffer,\n\t\t\t\t\t     buffer_offset, &pgoff);\n\t\tsize = min_t(size_t, bytes, PAGE_SIZE - pgoff);\n\t\tkptr = kmap_local_page(page) + pgoff;\n\t\tret = copy_from_user(kptr, from, size);\n\t\tkunmap_local(kptr);\n\t\tif (ret)\n\t\t\treturn bytes - size + ret;\n\t\tbytes -= size;\n\t\tfrom += size;\n\t\tbuffer_offset += size;\n\t}\n\treturn 0;\n}\n\nstatic int binder_alloc_do_buffer_copy(struct binder_alloc *alloc,\n\t\t\t\t       bool to_buffer,\n\t\t\t\t       struct binder_buffer *buffer,\n\t\t\t\t       binder_size_t buffer_offset,\n\t\t\t\t       void *ptr,\n\t\t\t\t       size_t bytes)\n{\n\t \n\tif (!check_buffer(alloc, buffer, buffer_offset, bytes))\n\t\treturn -EINVAL;\n\n\twhile (bytes) {\n\t\tunsigned long size;\n\t\tstruct page *page;\n\t\tpgoff_t pgoff;\n\n\t\tpage = binder_alloc_get_page(alloc, buffer,\n\t\t\t\t\t     buffer_offset, &pgoff);\n\t\tsize = min_t(size_t, bytes, PAGE_SIZE - pgoff);\n\t\tif (to_buffer)\n\t\t\tmemcpy_to_page(page, pgoff, ptr, size);\n\t\telse\n\t\t\tmemcpy_from_page(ptr, page, pgoff, size);\n\t\tbytes -= size;\n\t\tpgoff = 0;\n\t\tptr = ptr + size;\n\t\tbuffer_offset += size;\n\t}\n\treturn 0;\n}\n\nint binder_alloc_copy_to_buffer(struct binder_alloc *alloc,\n\t\t\t\tstruct binder_buffer *buffer,\n\t\t\t\tbinder_size_t buffer_offset,\n\t\t\t\tvoid *src,\n\t\t\t\tsize_t bytes)\n{\n\treturn binder_alloc_do_buffer_copy(alloc, true, buffer, buffer_offset,\n\t\t\t\t\t   src, bytes);\n}\n\nint binder_alloc_copy_from_buffer(struct binder_alloc *alloc,\n\t\t\t\t  void *dest,\n\t\t\t\t  struct binder_buffer *buffer,\n\t\t\t\t  binder_size_t buffer_offset,\n\t\t\t\t  size_t bytes)\n{\n\treturn binder_alloc_do_buffer_copy(alloc, false, buffer, buffer_offset,\n\t\t\t\t\t   dest, bytes);\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}