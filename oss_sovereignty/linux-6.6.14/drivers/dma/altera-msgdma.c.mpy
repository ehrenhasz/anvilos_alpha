{
  "module_name": "altera-msgdma.c",
  "hash_id": "f98897281f6441141bc0e33a43e0e5f862b2b68c4aaf6e7045b44ac2317d33db",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/altera-msgdma.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/of_dma.h>\n\n#include \"dmaengine.h\"\n\n#define MSGDMA_MAX_TRANS_LEN\t\tU32_MAX\n#define MSGDMA_DESC_NUM\t\t\t1024\n\n \nstruct msgdma_extended_desc {\n\tu32 read_addr_lo;\n\tu32 write_addr_lo;\n\tu32 len;\n\tu32 burst_seq_num;\n\tu32 stride;\n\tu32 read_addr_hi;\n\tu32 write_addr_hi;\n\tu32 control;\n};\n\n \n#define MSGDMA_DESC_CTL_SET_CH(x)\t((x) & 0xff)\n#define MSGDMA_DESC_CTL_GEN_SOP\t\tBIT(8)\n#define MSGDMA_DESC_CTL_GEN_EOP\t\tBIT(9)\n#define MSGDMA_DESC_CTL_PARK_READS\tBIT(10)\n#define MSGDMA_DESC_CTL_PARK_WRITES\tBIT(11)\n#define MSGDMA_DESC_CTL_END_ON_EOP\tBIT(12)\n#define MSGDMA_DESC_CTL_END_ON_LEN\tBIT(13)\n#define MSGDMA_DESC_CTL_TR_COMP_IRQ\tBIT(14)\n#define MSGDMA_DESC_CTL_EARLY_IRQ\tBIT(15)\n#define MSGDMA_DESC_CTL_TR_ERR_IRQ\tGENMASK(23, 16)\n#define MSGDMA_DESC_CTL_EARLY_DONE\tBIT(24)\n\n \n#define MSGDMA_DESC_CTL_GO\t\tBIT(31)\n\n \n#define MSGDMA_DESC_CTL_TX_FIRST\t(MSGDMA_DESC_CTL_GEN_SOP |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_ERR_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GO)\n\n#define MSGDMA_DESC_CTL_TX_MIDDLE\t(MSGDMA_DESC_CTL_TR_ERR_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GO)\n\n#define MSGDMA_DESC_CTL_TX_LAST\t\t(MSGDMA_DESC_CTL_GEN_EOP |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_COMP_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_ERR_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GO)\n\n#define MSGDMA_DESC_CTL_TX_SINGLE\t(MSGDMA_DESC_CTL_GEN_SOP |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GEN_EOP |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_COMP_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_ERR_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GO)\n\n#define MSGDMA_DESC_CTL_RX_SINGLE\t(MSGDMA_DESC_CTL_END_ON_EOP |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_END_ON_LEN |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_COMP_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_EARLY_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_TR_ERR_IRQ |\t\\\n\t\t\t\t\t MSGDMA_DESC_CTL_GO)\n\n \n#define MSGDMA_DESC_STRIDE_RD\t\t0x00000001\n#define MSGDMA_DESC_STRIDE_WR\t\t0x00010000\n#define MSGDMA_DESC_STRIDE_RW\t\t0x00010001\n\n \n#define MSGDMA_CSR_STATUS\t\t0x00\t \n#define MSGDMA_CSR_CONTROL\t\t0x04\t \n#define MSGDMA_CSR_RW_FILL_LEVEL\t0x08\t \n\t\t\t\t\t\t \n#define MSGDMA_CSR_RESP_FILL_LEVEL\t0x0c\t \n#define MSGDMA_CSR_RW_SEQ_NUM\t\t0x10\t \n\t\t\t\t\t\t \n\n \n#define MSGDMA_CSR_STAT_BUSY\t\t\tBIT(0)\n#define MSGDMA_CSR_STAT_DESC_BUF_EMPTY\t\tBIT(1)\n#define MSGDMA_CSR_STAT_DESC_BUF_FULL\t\tBIT(2)\n#define MSGDMA_CSR_STAT_RESP_BUF_EMPTY\t\tBIT(3)\n#define MSGDMA_CSR_STAT_RESP_BUF_FULL\t\tBIT(4)\n#define MSGDMA_CSR_STAT_STOPPED\t\t\tBIT(5)\n#define MSGDMA_CSR_STAT_RESETTING\t\tBIT(6)\n#define MSGDMA_CSR_STAT_STOPPED_ON_ERR\t\tBIT(7)\n#define MSGDMA_CSR_STAT_STOPPED_ON_EARLY\tBIT(8)\n#define MSGDMA_CSR_STAT_IRQ\t\t\tBIT(9)\n#define MSGDMA_CSR_STAT_MASK\t\t\tGENMASK(9, 0)\n#define MSGDMA_CSR_STAT_MASK_WITHOUT_IRQ\tGENMASK(8, 0)\n\n#define DESC_EMPTY\t(MSGDMA_CSR_STAT_DESC_BUF_EMPTY | \\\n\t\t\t MSGDMA_CSR_STAT_RESP_BUF_EMPTY)\n\n \n#define MSGDMA_CSR_CTL_STOP\t\t\tBIT(0)\n#define MSGDMA_CSR_CTL_RESET\t\t\tBIT(1)\n#define MSGDMA_CSR_CTL_STOP_ON_ERR\t\tBIT(2)\n#define MSGDMA_CSR_CTL_STOP_ON_EARLY\t\tBIT(3)\n#define MSGDMA_CSR_CTL_GLOBAL_INTR\t\tBIT(4)\n#define MSGDMA_CSR_CTL_STOP_DESCS\t\tBIT(5)\n\n \n#define MSGDMA_CSR_WR_FILL_LEVEL_GET(v)\t\t(((v) & 0xffff0000) >> 16)\n#define MSGDMA_CSR_RD_FILL_LEVEL_GET(v)\t\t((v) & 0x0000ffff)\n#define MSGDMA_CSR_RESP_FILL_LEVEL_GET(v)\t((v) & 0x0000ffff)\n\n#define MSGDMA_CSR_SEQ_NUM_GET(v)\t\t(((v) & 0xffff0000) >> 16)\n\n \n#define MSGDMA_RESP_BYTES_TRANSFERRED\t0x00\n#define MSGDMA_RESP_STATUS\t\t0x04\n\n \n#define MSGDMA_RESP_EARLY_TERM\tBIT(8)\n#define MSGDMA_RESP_ERR_MASK\t0xff\n\n \nstruct msgdma_sw_desc {\n\tstruct dma_async_tx_descriptor async_tx;\n\tstruct msgdma_extended_desc hw_desc;\n\tstruct list_head node;\n\tstruct list_head tx_list;\n};\n\n \nstruct msgdma_device {\n\tspinlock_t lock;\n\tstruct device *dev;\n\tstruct tasklet_struct irq_tasklet;\n\tstruct list_head pending_list;\n\tstruct list_head free_list;\n\tstruct list_head active_list;\n\tstruct list_head done_list;\n\tu32 desc_free_cnt;\n\tbool idle;\n\n\tstruct dma_device dmadev;\n\tstruct dma_chan\tdmachan;\n\tdma_addr_t hw_desq;\n\tstruct msgdma_sw_desc *sw_desq;\n\tunsigned int npendings;\n\n\tstruct dma_slave_config slave_cfg;\n\n\tint irq;\n\n\t \n\tvoid __iomem *csr;\n\n\t \n\tvoid __iomem *desc;\n\n\t \n\tvoid __iomem *resp;\n};\n\n#define to_mdev(chan)\tcontainer_of(chan, struct msgdma_device, dmachan)\n#define tx_to_desc(tx)\tcontainer_of(tx, struct msgdma_sw_desc, async_tx)\n\n \nstatic struct msgdma_sw_desc *msgdma_get_descriptor(struct msgdma_device *mdev)\n{\n\tstruct msgdma_sw_desc *desc;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mdev->lock, flags);\n\tdesc = list_first_entry(&mdev->free_list, struct msgdma_sw_desc, node);\n\tlist_del(&desc->node);\n\tspin_unlock_irqrestore(&mdev->lock, flags);\n\n\tINIT_LIST_HEAD(&desc->tx_list);\n\n\treturn desc;\n}\n\n \nstatic void msgdma_free_descriptor(struct msgdma_device *mdev,\n\t\t\t\t   struct msgdma_sw_desc *desc)\n{\n\tstruct msgdma_sw_desc *child, *next;\n\n\tmdev->desc_free_cnt++;\n\tlist_add_tail(&desc->node, &mdev->free_list);\n\tlist_for_each_entry_safe(child, next, &desc->tx_list, node) {\n\t\tmdev->desc_free_cnt++;\n\t\tlist_move_tail(&child->node, &mdev->free_list);\n\t}\n}\n\n \nstatic void msgdma_free_desc_list(struct msgdma_device *mdev,\n\t\t\t\t  struct list_head *list)\n{\n\tstruct msgdma_sw_desc *desc, *next;\n\n\tlist_for_each_entry_safe(desc, next, list, node)\n\t\tmsgdma_free_descriptor(mdev, desc);\n}\n\n \nstatic void msgdma_desc_config(struct msgdma_extended_desc *desc,\n\t\t\t       dma_addr_t dst, dma_addr_t src, size_t len,\n\t\t\t       u32 stride)\n{\n\t \n\tdesc->read_addr_lo = lower_32_bits(src);\n\tdesc->write_addr_lo = lower_32_bits(dst);\n\n\t \n\tdesc->read_addr_hi = upper_32_bits(src);\n\tdesc->write_addr_hi = upper_32_bits(dst);\n\n\tdesc->len = len;\n\tdesc->stride = stride;\n\tdesc->burst_seq_num = 0;\t \n\n\t \n\tdesc->control = MSGDMA_DESC_CTL_TR_ERR_IRQ | MSGDMA_DESC_CTL_GO |\n\t\tMSGDMA_DESC_CTL_END_ON_LEN;\n}\n\n \nstatic void msgdma_desc_config_eod(struct msgdma_extended_desc *desc)\n{\n\tdesc->control |= MSGDMA_DESC_CTL_TR_COMP_IRQ;\n}\n\n \nstatic dma_cookie_t msgdma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct msgdma_device *mdev = to_mdev(tx->chan);\n\tstruct msgdma_sw_desc *new;\n\tdma_cookie_t cookie;\n\tunsigned long flags;\n\n\tnew = tx_to_desc(tx);\n\tspin_lock_irqsave(&mdev->lock, flags);\n\tcookie = dma_cookie_assign(tx);\n\n\tlist_add_tail(&new->node, &mdev->pending_list);\n\tspin_unlock_irqrestore(&mdev->lock, flags);\n\n\treturn cookie;\n}\n\n \nstatic struct dma_async_tx_descriptor *\nmsgdma_prep_memcpy(struct dma_chan *dchan, dma_addr_t dma_dst,\n\t\t   dma_addr_t dma_src, size_t len, ulong flags)\n{\n\tstruct msgdma_device *mdev = to_mdev(dchan);\n\tstruct msgdma_sw_desc *new, *first = NULL;\n\tstruct msgdma_extended_desc *desc;\n\tsize_t copy;\n\tu32 desc_cnt;\n\tunsigned long irqflags;\n\n\tdesc_cnt = DIV_ROUND_UP(len, MSGDMA_MAX_TRANS_LEN);\n\n\tspin_lock_irqsave(&mdev->lock, irqflags);\n\tif (desc_cnt > mdev->desc_free_cnt) {\n\t\tspin_unlock_irqrestore(&mdev->lock, irqflags);\n\t\tdev_dbg(mdev->dev, \"mdev %p descs are not available\\n\", mdev);\n\t\treturn NULL;\n\t}\n\tmdev->desc_free_cnt -= desc_cnt;\n\tspin_unlock_irqrestore(&mdev->lock, irqflags);\n\n\tdo {\n\t\t \n\t\tnew = msgdma_get_descriptor(mdev);\n\n\t\tcopy = min_t(size_t, len, MSGDMA_MAX_TRANS_LEN);\n\t\tdesc = &new->hw_desc;\n\t\tmsgdma_desc_config(desc, dma_dst, dma_src, copy,\n\t\t\t\t   MSGDMA_DESC_STRIDE_RW);\n\t\tlen -= copy;\n\t\tdma_src += copy;\n\t\tdma_dst += copy;\n\t\tif (!first)\n\t\t\tfirst = new;\n\t\telse\n\t\t\tlist_add_tail(&new->node, &first->tx_list);\n\t} while (len);\n\n\tmsgdma_desc_config_eod(desc);\n\tasync_tx_ack(&first->async_tx);\n\tfirst->async_tx.flags = flags;\n\n\treturn &first->async_tx;\n}\n\n \nstatic struct dma_async_tx_descriptor *\nmsgdma_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\n\t\t     unsigned int sg_len, enum dma_transfer_direction dir,\n\t\t     unsigned long flags, void *context)\n\n{\n\tstruct msgdma_device *mdev = to_mdev(dchan);\n\tstruct dma_slave_config *cfg = &mdev->slave_cfg;\n\tstruct msgdma_sw_desc *new, *first = NULL;\n\tvoid *desc = NULL;\n\tsize_t len, avail;\n\tdma_addr_t dma_dst, dma_src;\n\tu32 desc_cnt = 0, i;\n\tstruct scatterlist *sg;\n\tu32 stride;\n\tunsigned long irqflags;\n\n\tfor_each_sg(sgl, sg, sg_len, i)\n\t\tdesc_cnt += DIV_ROUND_UP(sg_dma_len(sg), MSGDMA_MAX_TRANS_LEN);\n\n\tspin_lock_irqsave(&mdev->lock, irqflags);\n\tif (desc_cnt > mdev->desc_free_cnt) {\n\t\tspin_unlock_irqrestore(&mdev->lock, irqflags);\n\t\tdev_dbg(mdev->dev, \"mdev %p descs are not available\\n\", mdev);\n\t\treturn NULL;\n\t}\n\tmdev->desc_free_cnt -= desc_cnt;\n\tspin_unlock_irqrestore(&mdev->lock, irqflags);\n\n\tavail = sg_dma_len(sgl);\n\n\t \n\twhile (true) {\n\t\t \n\t\tnew = msgdma_get_descriptor(mdev);\n\n\t\tdesc = &new->hw_desc;\n\t\tlen = min_t(size_t, avail, MSGDMA_MAX_TRANS_LEN);\n\n\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\tdma_src = sg_dma_address(sgl) + sg_dma_len(sgl) - avail;\n\t\t\tdma_dst = cfg->dst_addr;\n\t\t\tstride = MSGDMA_DESC_STRIDE_RD;\n\t\t} else {\n\t\t\tdma_src = cfg->src_addr;\n\t\t\tdma_dst = sg_dma_address(sgl) + sg_dma_len(sgl) - avail;\n\t\t\tstride = MSGDMA_DESC_STRIDE_WR;\n\t\t}\n\t\tmsgdma_desc_config(desc, dma_dst, dma_src, len, stride);\n\t\tavail -= len;\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\t\telse\n\t\t\tlist_add_tail(&new->node, &first->tx_list);\n\n\t\t \n\t\tif (avail == 0) {\n\t\t\tif (sg_len == 0)\n\t\t\t\tbreak;\n\t\t\tsgl = sg_next(sgl);\n\t\t\tif (sgl == NULL)\n\t\t\t\tbreak;\n\t\t\tsg_len--;\n\t\t\tavail = sg_dma_len(sgl);\n\t\t}\n\t}\n\n\tmsgdma_desc_config_eod(desc);\n\tfirst->async_tx.flags = flags;\n\n\treturn &first->async_tx;\n}\n\nstatic int msgdma_dma_config(struct dma_chan *dchan,\n\t\t\t     struct dma_slave_config *config)\n{\n\tstruct msgdma_device *mdev = to_mdev(dchan);\n\n\tmemcpy(&mdev->slave_cfg, config, sizeof(*config));\n\n\treturn 0;\n}\n\nstatic void msgdma_reset(struct msgdma_device *mdev)\n{\n\tu32 val;\n\tint ret;\n\n\t \n\tiowrite32(MSGDMA_CSR_STAT_MASK, mdev->csr + MSGDMA_CSR_STATUS);\n\tiowrite32(MSGDMA_CSR_CTL_RESET, mdev->csr + MSGDMA_CSR_CONTROL);\n\n\tret = readl_poll_timeout(mdev->csr + MSGDMA_CSR_STATUS, val,\n\t\t\t\t (val & MSGDMA_CSR_STAT_RESETTING) == 0,\n\t\t\t\t 1, 10000);\n\tif (ret)\n\t\tdev_err(mdev->dev, \"DMA channel did not reset\\n\");\n\n\t \n\tiowrite32(MSGDMA_CSR_STAT_MASK, mdev->csr + MSGDMA_CSR_STATUS);\n\n\t \n\tiowrite32(MSGDMA_CSR_CTL_STOP_ON_ERR | MSGDMA_CSR_CTL_STOP_ON_EARLY |\n\t\t  MSGDMA_CSR_CTL_GLOBAL_INTR, mdev->csr + MSGDMA_CSR_CONTROL);\n\n\tmdev->idle = true;\n};\n\nstatic void msgdma_copy_one(struct msgdma_device *mdev,\n\t\t\t    struct msgdma_sw_desc *desc)\n{\n\tvoid __iomem *hw_desc = mdev->desc;\n\n\t \n\twhile (ioread32(mdev->csr + MSGDMA_CSR_STATUS) &\n\t       MSGDMA_CSR_STAT_DESC_BUF_FULL)\n\t\tmdelay(1);\n\n\t \n\tmemcpy((void __force *)hw_desc, &desc->hw_desc,\n\t       sizeof(desc->hw_desc) - sizeof(u32));\n\n\t \n\tmdev->idle = false;\n\twmb();\n\tiowrite32(desc->hw_desc.control, hw_desc +\n\t\t  offsetof(struct msgdma_extended_desc, control));\n\twmb();\n}\n\n \nstatic void msgdma_copy_desc_to_fifo(struct msgdma_device *mdev,\n\t\t\t\t     struct msgdma_sw_desc *desc)\n{\n\tstruct msgdma_sw_desc *sdesc, *next;\n\n\tmsgdma_copy_one(mdev, desc);\n\n\tlist_for_each_entry_safe(sdesc, next, &desc->tx_list, node)\n\t\tmsgdma_copy_one(mdev, sdesc);\n}\n\n \nstatic void msgdma_start_transfer(struct msgdma_device *mdev)\n{\n\tstruct msgdma_sw_desc *desc;\n\n\tif (!mdev->idle)\n\t\treturn;\n\n\tdesc = list_first_entry_or_null(&mdev->pending_list,\n\t\t\t\t\tstruct msgdma_sw_desc, node);\n\tif (!desc)\n\t\treturn;\n\n\tlist_splice_tail_init(&mdev->pending_list, &mdev->active_list);\n\tmsgdma_copy_desc_to_fifo(mdev, desc);\n}\n\n \nstatic void msgdma_issue_pending(struct dma_chan *chan)\n{\n\tstruct msgdma_device *mdev = to_mdev(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mdev->lock, flags);\n\tmsgdma_start_transfer(mdev);\n\tspin_unlock_irqrestore(&mdev->lock, flags);\n}\n\n \nstatic void msgdma_chan_desc_cleanup(struct msgdma_device *mdev)\n{\n\tstruct msgdma_sw_desc *desc, *next;\n\n\tlist_for_each_entry_safe(desc, next, &mdev->done_list, node) {\n\t\tstruct dmaengine_desc_callback cb;\n\n\t\tlist_del(&desc->node);\n\n\t\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\t\tif (dmaengine_desc_callback_valid(&cb)) {\n\t\t\tspin_unlock(&mdev->lock);\n\t\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\t\t\tspin_lock(&mdev->lock);\n\t\t}\n\n\t\t \n\t\tmsgdma_free_descriptor(mdev, desc);\n\t}\n}\n\n \nstatic void msgdma_complete_descriptor(struct msgdma_device *mdev)\n{\n\tstruct msgdma_sw_desc *desc;\n\n\tdesc = list_first_entry_or_null(&mdev->active_list,\n\t\t\t\t\tstruct msgdma_sw_desc, node);\n\tif (!desc)\n\t\treturn;\n\tlist_del(&desc->node);\n\tdma_cookie_complete(&desc->async_tx);\n\tlist_add_tail(&desc->node, &mdev->done_list);\n}\n\n \nstatic void msgdma_free_descriptors(struct msgdma_device *mdev)\n{\n\tmsgdma_free_desc_list(mdev, &mdev->active_list);\n\tmsgdma_free_desc_list(mdev, &mdev->pending_list);\n\tmsgdma_free_desc_list(mdev, &mdev->done_list);\n}\n\n \nstatic void msgdma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct msgdma_device *mdev = to_mdev(dchan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mdev->lock, flags);\n\tmsgdma_free_descriptors(mdev);\n\tspin_unlock_irqrestore(&mdev->lock, flags);\n\tkfree(mdev->sw_desq);\n}\n\n \nstatic int msgdma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct msgdma_device *mdev = to_mdev(dchan);\n\tstruct msgdma_sw_desc *desc;\n\tint i;\n\n\tmdev->sw_desq = kcalloc(MSGDMA_DESC_NUM, sizeof(*desc), GFP_NOWAIT);\n\tif (!mdev->sw_desq)\n\t\treturn -ENOMEM;\n\n\tmdev->idle = true;\n\tmdev->desc_free_cnt = MSGDMA_DESC_NUM;\n\n\tINIT_LIST_HEAD(&mdev->free_list);\n\n\tfor (i = 0; i < MSGDMA_DESC_NUM; i++) {\n\t\tdesc = mdev->sw_desq + i;\n\t\tdma_async_tx_descriptor_init(&desc->async_tx, &mdev->dmachan);\n\t\tdesc->async_tx.tx_submit = msgdma_tx_submit;\n\t\tlist_add_tail(&desc->node, &mdev->free_list);\n\t}\n\n\treturn MSGDMA_DESC_NUM;\n}\n\n \nstatic void msgdma_tasklet(struct tasklet_struct *t)\n{\n\tstruct msgdma_device *mdev = from_tasklet(mdev, t, irq_tasklet);\n\tu32 count;\n\tu32 __maybe_unused size;\n\tu32 __maybe_unused status;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mdev->lock, flags);\n\n\tif (mdev->resp) {\n\t\t \n\t\tcount = ioread32(mdev->csr + MSGDMA_CSR_RESP_FILL_LEVEL);\n\t\tdev_dbg(mdev->dev, \"%s (%d): response count=%d\\n\",\n\t\t\t__func__, __LINE__, count);\n\t} else {\n\t\tcount = 1;\n\t}\n\n\twhile (count--) {\n\t\t \n\t\tif (mdev->resp) {\n\t\t\tsize = ioread32(mdev->resp +\n\t\t\t\t\tMSGDMA_RESP_BYTES_TRANSFERRED);\n\t\t\tstatus = ioread32(mdev->resp +\n\t\t\t\t\tMSGDMA_RESP_STATUS);\n\t\t}\n\n\t\tmsgdma_complete_descriptor(mdev);\n\t\tmsgdma_chan_desc_cleanup(mdev);\n\t}\n\n\tspin_unlock_irqrestore(&mdev->lock, flags);\n}\n\n \nstatic irqreturn_t msgdma_irq_handler(int irq, void *data)\n{\n\tstruct msgdma_device *mdev = data;\n\tu32 status;\n\n\tstatus = ioread32(mdev->csr + MSGDMA_CSR_STATUS);\n\tif ((status & MSGDMA_CSR_STAT_BUSY) == 0) {\n\t\t \n\t\tspin_lock(&mdev->lock);\n\t\tmdev->idle = true;\n\t\tmsgdma_start_transfer(mdev);\n\t\tspin_unlock(&mdev->lock);\n\t}\n\n\ttasklet_schedule(&mdev->irq_tasklet);\n\n\t \n\tiowrite32(MSGDMA_CSR_STAT_IRQ, mdev->csr + MSGDMA_CSR_STATUS);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void msgdma_dev_remove(struct msgdma_device *mdev)\n{\n\tif (!mdev)\n\t\treturn;\n\n\tdevm_free_irq(mdev->dev, mdev->irq, mdev);\n\ttasklet_kill(&mdev->irq_tasklet);\n\tlist_del(&mdev->dmachan.device_node);\n}\n\nstatic int request_and_map(struct platform_device *pdev, const char *name,\n\t\t\t   struct resource **res, void __iomem **ptr,\n\t\t\t   bool optional)\n{\n\tstruct resource *region;\n\tstruct device *device = &pdev->dev;\n\n\t*res = platform_get_resource_byname(pdev, IORESOURCE_MEM, name);\n\tif (*res == NULL) {\n\t\tif (optional) {\n\t\t\t*ptr = NULL;\n\t\t\tdev_info(device, \"optional resource %s not defined\\n\",\n\t\t\t\t name);\n\t\t\treturn 0;\n\t\t}\n\t\tdev_err(device, \"mandatory resource %s not defined\\n\", name);\n\t\treturn -ENODEV;\n\t}\n\n\tregion = devm_request_mem_region(device, (*res)->start,\n\t\t\t\t\t resource_size(*res), dev_name(device));\n\tif (region == NULL) {\n\t\tdev_err(device, \"unable to request %s\\n\", name);\n\t\treturn -EBUSY;\n\t}\n\n\t*ptr = devm_ioremap(device, region->start,\n\t\t\t\t    resource_size(region));\n\tif (*ptr == NULL) {\n\t\tdev_err(device, \"ioremap of %s failed!\", name);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int msgdma_probe(struct platform_device *pdev)\n{\n\tstruct msgdma_device *mdev;\n\tstruct dma_device *dma_dev;\n\tstruct resource *dma_res;\n\tint ret;\n\n\tmdev = devm_kzalloc(&pdev->dev, sizeof(*mdev), GFP_NOWAIT);\n\tif (!mdev)\n\t\treturn -ENOMEM;\n\n\tmdev->dev = &pdev->dev;\n\n\t \n\tret = request_and_map(pdev, \"csr\", &dma_res, &mdev->csr, false);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = request_and_map(pdev, \"desc\", &dma_res, &mdev->desc, false);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = request_and_map(pdev, \"resp\", &dma_res, &mdev->resp, true);\n\tif (ret)\n\t\treturn ret;\n\n\tplatform_set_drvdata(pdev, mdev);\n\n\t \n\tmdev->irq = platform_get_irq(pdev, 0);\n\tif (mdev->irq < 0)\n\t\treturn -ENXIO;\n\n\tret = devm_request_irq(&pdev->dev, mdev->irq, msgdma_irq_handler,\n\t\t\t       0, dev_name(&pdev->dev), mdev);\n\tif (ret)\n\t\treturn ret;\n\n\ttasklet_setup(&mdev->irq_tasklet, msgdma_tasklet);\n\n\tdma_cookie_init(&mdev->dmachan);\n\n\tspin_lock_init(&mdev->lock);\n\n\tINIT_LIST_HEAD(&mdev->active_list);\n\tINIT_LIST_HEAD(&mdev->pending_list);\n\tINIT_LIST_HEAD(&mdev->done_list);\n\tINIT_LIST_HEAD(&mdev->free_list);\n\n\tdma_dev = &mdev->dmadev;\n\n\t \n\tdma_cap_zero(dma_dev->cap_mask);\n\tdma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\n\tdma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\n\n\tdma_dev->src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tdma_dev->dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tdma_dev->directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM) |\n\t\tBIT(DMA_MEM_TO_MEM);\n\tdma_dev->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\n\t \n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\t \n\tdma_dev->device_tx_status = dma_cookie_status;\n\tdma_dev->device_issue_pending = msgdma_issue_pending;\n\tdma_dev->dev = &pdev->dev;\n\n\tdma_dev->copy_align = DMAENGINE_ALIGN_4_BYTES;\n\tdma_dev->device_prep_dma_memcpy = msgdma_prep_memcpy;\n\tdma_dev->device_prep_slave_sg = msgdma_prep_slave_sg;\n\tdma_dev->device_config = msgdma_dma_config;\n\n\tdma_dev->device_alloc_chan_resources = msgdma_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = msgdma_free_chan_resources;\n\n\tmdev->dmachan.device = dma_dev;\n\tlist_add_tail(&mdev->dmachan.device_node, &dma_dev->channels);\n\n\t \n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tdev_warn(&pdev->dev, \"unable to set coherent mask to 64\");\n\t\tgoto fail;\n\t}\n\n\tmsgdma_reset(mdev);\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\t\t of_dma_xlate_by_chan_id, dma_dev);\n\tif (ret == -EINVAL)\n\t\tdev_warn(&pdev->dev, \"device was not probed from DT\");\n\telse if (ret && ret != -ENODEV)\n\t\tgoto fail;\n\n\tdev_notice(&pdev->dev, \"Altera mSGDMA driver probe success\\n\");\n\n\treturn 0;\n\nfail:\n\tmsgdma_dev_remove(mdev);\n\n\treturn ret;\n}\n\n \nstatic int msgdma_remove(struct platform_device *pdev)\n{\n\tstruct msgdma_device *mdev = platform_get_drvdata(pdev);\n\n\tif (pdev->dev.of_node)\n\t\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&mdev->dmadev);\n\tmsgdma_dev_remove(mdev);\n\n\tdev_notice(&pdev->dev, \"Altera mSGDMA driver removed\\n\");\n\n\treturn 0;\n}\n\n#ifdef CONFIG_OF\nstatic const struct of_device_id msgdma_match[] = {\n\t{ .compatible = \"altr,socfpga-msgdma\", },\n\t{ }\n};\n\nMODULE_DEVICE_TABLE(of, msgdma_match);\n#endif\n\nstatic struct platform_driver msgdma_driver = {\n\t.driver = {\n\t\t.name = \"altera-msgdma\",\n\t\t.of_match_table = of_match_ptr(msgdma_match),\n\t},\n\t.probe = msgdma_probe,\n\t.remove = msgdma_remove,\n};\n\nmodule_platform_driver(msgdma_driver);\n\nMODULE_ALIAS(\"platform:altera-msgdma\");\nMODULE_DESCRIPTION(\"Altera mSGDMA driver\");\nMODULE_AUTHOR(\"Stefan Roese <sr@denx.de>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}