{
  "module_name": "rcar-dmac.c",
  "hash_id": "c281c3a3ed906f3b2ae4c479796cf6944ddee3470778237ace9abc6bae51161c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/sh/rcar-dmac.c",
  "human_readable_source": "\n \n\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/of_platform.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n#include \"../dmaengine.h\"\n\n \nstruct rcar_dmac_xfer_chunk {\n\tstruct list_head node;\n\n\tdma_addr_t src_addr;\n\tdma_addr_t dst_addr;\n\tu32 size;\n};\n\n \nstruct rcar_dmac_hw_desc {\n\tu32 sar;\n\tu32 dar;\n\tu32 tcr;\n\tu32 reserved;\n} __attribute__((__packed__));\n\n \nstruct rcar_dmac_desc {\n\tstruct dma_async_tx_descriptor async_tx;\n\tenum dma_transfer_direction direction;\n\tunsigned int xfer_shift;\n\tu32 chcr;\n\n\tstruct list_head node;\n\tstruct list_head chunks;\n\tstruct rcar_dmac_xfer_chunk *running;\n\tunsigned int nchunks;\n\n\tstruct {\n\t\tbool use;\n\t\tstruct rcar_dmac_hw_desc *mem;\n\t\tdma_addr_t dma;\n\t\tsize_t size;\n\t} hwdescs;\n\n\tunsigned int size;\n\tbool cyclic;\n};\n\n#define to_rcar_dmac_desc(d)\tcontainer_of(d, struct rcar_dmac_desc, async_tx)\n\n \nstruct rcar_dmac_desc_page {\n\tstruct list_head node;\n\n\tunion {\n\t\tDECLARE_FLEX_ARRAY(struct rcar_dmac_desc, descs);\n\t\tDECLARE_FLEX_ARRAY(struct rcar_dmac_xfer_chunk, chunks);\n\t};\n};\n\n#define RCAR_DMAC_DESCS_PER_PAGE\t\t\t\t\t\\\n\t((PAGE_SIZE - offsetof(struct rcar_dmac_desc_page, descs)) /\t\\\n\tsizeof(struct rcar_dmac_desc))\n#define RCAR_DMAC_XFER_CHUNKS_PER_PAGE\t\t\t\t\t\\\n\t((PAGE_SIZE - offsetof(struct rcar_dmac_desc_page, chunks)) /\t\\\n\tsizeof(struct rcar_dmac_xfer_chunk))\n\n \nstruct rcar_dmac_chan_slave {\n\tphys_addr_t slave_addr;\n\tunsigned int xfer_size;\n};\n\n \nstruct rcar_dmac_chan_map {\n\tdma_addr_t addr;\n\tenum dma_data_direction dir;\n\tstruct rcar_dmac_chan_slave slave;\n};\n\n \nstruct rcar_dmac_chan {\n\tstruct dma_chan chan;\n\tvoid __iomem *iomem;\n\tunsigned int index;\n\tint irq;\n\n\tstruct rcar_dmac_chan_slave src;\n\tstruct rcar_dmac_chan_slave dst;\n\tstruct rcar_dmac_chan_map map;\n\tint mid_rid;\n\n\tspinlock_t lock;\n\n\tstruct {\n\t\tstruct list_head free;\n\t\tstruct list_head pending;\n\t\tstruct list_head active;\n\t\tstruct list_head done;\n\t\tstruct list_head wait;\n\t\tstruct rcar_dmac_desc *running;\n\n\t\tstruct list_head chunks_free;\n\n\t\tstruct list_head pages;\n\t} desc;\n};\n\n#define to_rcar_dmac_chan(c)\tcontainer_of(c, struct rcar_dmac_chan, chan)\n\n \nstruct rcar_dmac {\n\tstruct dma_device engine;\n\tstruct device *dev;\n\tvoid __iomem *dmac_base;\n\tvoid __iomem *chan_base;\n\n\tunsigned int n_channels;\n\tstruct rcar_dmac_chan *channels;\n\tu32 channels_mask;\n\n\tDECLARE_BITMAP(modules, 256);\n};\n\n#define to_rcar_dmac(d)\t\tcontainer_of(d, struct rcar_dmac, engine)\n\n#define for_each_rcar_dmac_chan(i, dmac, chan)\t\t\t\t\t\t\\\n\tfor (i = 0, chan = &(dmac)->channels[0]; i < (dmac)->n_channels; i++, chan++)\t\\\n\t\tif (!((dmac)->channels_mask & BIT(i))) continue; else\n\n \nstruct rcar_dmac_of_data {\n\tu32 chan_offset_base;\n\tu32 chan_offset_stride;\n};\n\n \n\n#define RCAR_DMAISTA\t\t\t0x0020\n#define RCAR_DMASEC\t\t\t0x0030\n#define RCAR_DMAOR\t\t\t0x0060\n#define RCAR_DMAOR_PRI_FIXED\t\t(0 << 8)\n#define RCAR_DMAOR_PRI_ROUND_ROBIN\t(3 << 8)\n#define RCAR_DMAOR_AE\t\t\t(1 << 2)\n#define RCAR_DMAOR_DME\t\t\t(1 << 0)\n#define RCAR_DMACHCLR\t\t\t0x0080\t \n#define RCAR_DMADPSEC\t\t\t0x00a0\n\n#define RCAR_DMASAR\t\t\t0x0000\n#define RCAR_DMADAR\t\t\t0x0004\n#define RCAR_DMATCR\t\t\t0x0008\n#define RCAR_DMATCR_MASK\t\t0x00ffffff\n#define RCAR_DMATSR\t\t\t0x0028\n#define RCAR_DMACHCR\t\t\t0x000c\n#define RCAR_DMACHCR_CAE\t\t(1 << 31)\n#define RCAR_DMACHCR_CAIE\t\t(1 << 30)\n#define RCAR_DMACHCR_DPM_DISABLED\t(0 << 28)\n#define RCAR_DMACHCR_DPM_ENABLED\t(1 << 28)\n#define RCAR_DMACHCR_DPM_REPEAT\t\t(2 << 28)\n#define RCAR_DMACHCR_DPM_INFINITE\t(3 << 28)\n#define RCAR_DMACHCR_RPT_SAR\t\t(1 << 27)\n#define RCAR_DMACHCR_RPT_DAR\t\t(1 << 26)\n#define RCAR_DMACHCR_RPT_TCR\t\t(1 << 25)\n#define RCAR_DMACHCR_DPB\t\t(1 << 22)\n#define RCAR_DMACHCR_DSE\t\t(1 << 19)\n#define RCAR_DMACHCR_DSIE\t\t(1 << 18)\n#define RCAR_DMACHCR_TS_1B\t\t((0 << 20) | (0 << 3))\n#define RCAR_DMACHCR_TS_2B\t\t((0 << 20) | (1 << 3))\n#define RCAR_DMACHCR_TS_4B\t\t((0 << 20) | (2 << 3))\n#define RCAR_DMACHCR_TS_16B\t\t((0 << 20) | (3 << 3))\n#define RCAR_DMACHCR_TS_32B\t\t((1 << 20) | (0 << 3))\n#define RCAR_DMACHCR_TS_64B\t\t((1 << 20) | (1 << 3))\n#define RCAR_DMACHCR_TS_8B\t\t((1 << 20) | (3 << 3))\n#define RCAR_DMACHCR_DM_FIXED\t\t(0 << 14)\n#define RCAR_DMACHCR_DM_INC\t\t(1 << 14)\n#define RCAR_DMACHCR_DM_DEC\t\t(2 << 14)\n#define RCAR_DMACHCR_SM_FIXED\t\t(0 << 12)\n#define RCAR_DMACHCR_SM_INC\t\t(1 << 12)\n#define RCAR_DMACHCR_SM_DEC\t\t(2 << 12)\n#define RCAR_DMACHCR_RS_AUTO\t\t(4 << 8)\n#define RCAR_DMACHCR_RS_DMARS\t\t(8 << 8)\n#define RCAR_DMACHCR_IE\t\t\t(1 << 2)\n#define RCAR_DMACHCR_TE\t\t\t(1 << 1)\n#define RCAR_DMACHCR_DE\t\t\t(1 << 0)\n#define RCAR_DMATCRB\t\t\t0x0018\n#define RCAR_DMATSRB\t\t\t0x0038\n#define RCAR_DMACHCRB\t\t\t0x001c\n#define RCAR_DMACHCRB_DCNT(n)\t\t((n) << 24)\n#define RCAR_DMACHCRB_DPTR_MASK\t\t(0xff << 16)\n#define RCAR_DMACHCRB_DPTR_SHIFT\t16\n#define RCAR_DMACHCRB_DRST\t\t(1 << 15)\n#define RCAR_DMACHCRB_DTS\t\t(1 << 8)\n#define RCAR_DMACHCRB_SLM_NORMAL\t(0 << 4)\n#define RCAR_DMACHCRB_SLM_CLK(n)\t((8 | (n)) << 4)\n#define RCAR_DMACHCRB_PRI(n)\t\t((n) << 0)\n#define RCAR_DMARS\t\t\t0x0040\n#define RCAR_DMABUFCR\t\t\t0x0048\n#define RCAR_DMABUFCR_MBU(n)\t\t((n) << 16)\n#define RCAR_DMABUFCR_ULB(n)\t\t((n) << 0)\n#define RCAR_DMADPBASE\t\t\t0x0050\n#define RCAR_DMADPBASE_MASK\t\t0xfffffff0\n#define RCAR_DMADPBASE_SEL\t\t(1 << 0)\n#define RCAR_DMADPCR\t\t\t0x0054\n#define RCAR_DMADPCR_DIPT(n)\t\t((n) << 24)\n#define RCAR_DMAFIXSAR\t\t\t0x0010\n#define RCAR_DMAFIXDAR\t\t\t0x0014\n#define RCAR_DMAFIXDPBASE\t\t0x0060\n\n \n#define RCAR_GEN4_DMACHCLR\t\t0x0100\n\n \n#define RCAR_DMAC_MEMCPY_XFER_SIZE\t4\n\n \n\nstatic void rcar_dmac_write(struct rcar_dmac *dmac, u32 reg, u32 data)\n{\n\tif (reg == RCAR_DMAOR)\n\t\twritew(data, dmac->dmac_base + reg);\n\telse\n\t\twritel(data, dmac->dmac_base + reg);\n}\n\nstatic u32 rcar_dmac_read(struct rcar_dmac *dmac, u32 reg)\n{\n\tif (reg == RCAR_DMAOR)\n\t\treturn readw(dmac->dmac_base + reg);\n\telse\n\t\treturn readl(dmac->dmac_base + reg);\n}\n\nstatic u32 rcar_dmac_chan_read(struct rcar_dmac_chan *chan, u32 reg)\n{\n\tif (reg == RCAR_DMARS)\n\t\treturn readw(chan->iomem + reg);\n\telse\n\t\treturn readl(chan->iomem + reg);\n}\n\nstatic void rcar_dmac_chan_write(struct rcar_dmac_chan *chan, u32 reg, u32 data)\n{\n\tif (reg == RCAR_DMARS)\n\t\twritew(data, chan->iomem + reg);\n\telse\n\t\twritel(data, chan->iomem + reg);\n}\n\nstatic void rcar_dmac_chan_clear(struct rcar_dmac *dmac,\n\t\t\t\t struct rcar_dmac_chan *chan)\n{\n\tif (dmac->chan_base)\n\t\trcar_dmac_chan_write(chan, RCAR_GEN4_DMACHCLR, 1);\n\telse\n\t\trcar_dmac_write(dmac, RCAR_DMACHCLR, BIT(chan->index));\n}\n\nstatic void rcar_dmac_chan_clear_all(struct rcar_dmac *dmac)\n{\n\tstruct rcar_dmac_chan *chan;\n\tunsigned int i;\n\n\tif (dmac->chan_base) {\n\t\tfor_each_rcar_dmac_chan(i, dmac, chan)\n\t\t\trcar_dmac_chan_write(chan, RCAR_GEN4_DMACHCLR, 1);\n\t} else {\n\t\trcar_dmac_write(dmac, RCAR_DMACHCLR, dmac->channels_mask);\n\t}\n}\n\n \n\nstatic bool rcar_dmac_chan_is_busy(struct rcar_dmac_chan *chan)\n{\n\tu32 chcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\n\n\treturn !!(chcr & (RCAR_DMACHCR_DE | RCAR_DMACHCR_TE));\n}\n\nstatic void rcar_dmac_chan_start_xfer(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc = chan->desc.running;\n\tu32 chcr = desc->chcr;\n\n\tWARN_ON_ONCE(rcar_dmac_chan_is_busy(chan));\n\n\tif (chan->mid_rid >= 0)\n\t\trcar_dmac_chan_write(chan, RCAR_DMARS, chan->mid_rid);\n\n\tif (desc->hwdescs.use) {\n\t\tstruct rcar_dmac_xfer_chunk *chunk =\n\t\t\tlist_first_entry(&desc->chunks,\n\t\t\t\t\t struct rcar_dmac_xfer_chunk, node);\n\n\t\tdev_dbg(chan->chan.device->dev,\n\t\t\t\"chan%u: queue desc %p: %u@%pad\\n\",\n\t\t\tchan->index, desc, desc->nchunks, &desc->hwdescs.dma);\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\trcar_dmac_chan_write(chan, RCAR_DMAFIXSAR,\n\t\t\t\t     chunk->src_addr >> 32);\n\t\trcar_dmac_chan_write(chan, RCAR_DMAFIXDAR,\n\t\t\t\t     chunk->dst_addr >> 32);\n\t\trcar_dmac_chan_write(chan, RCAR_DMAFIXDPBASE,\n\t\t\t\t     desc->hwdescs.dma >> 32);\n#endif\n\t\trcar_dmac_chan_write(chan, RCAR_DMADPBASE,\n\t\t\t\t     (desc->hwdescs.dma & 0xfffffff0) |\n\t\t\t\t     RCAR_DMADPBASE_SEL);\n\t\trcar_dmac_chan_write(chan, RCAR_DMACHCRB,\n\t\t\t\t     RCAR_DMACHCRB_DCNT(desc->nchunks - 1) |\n\t\t\t\t     RCAR_DMACHCRB_DRST);\n\n\t\t \n\t\trcar_dmac_chan_write(chan, RCAR_DMADAR,\n\t\t\t\t     chunk->dst_addr & 0xffffffff);\n\n\t\t \n\t\trcar_dmac_chan_write(chan, RCAR_DMADPCR, RCAR_DMADPCR_DIPT(1));\n\n\t\tchcr |= RCAR_DMACHCR_RPT_SAR | RCAR_DMACHCR_RPT_DAR\n\t\t     |  RCAR_DMACHCR_RPT_TCR | RCAR_DMACHCR_DPB;\n\n\t\t \n\t\tif (!desc->cyclic)\n\t\t\tchcr |= RCAR_DMACHCR_DPM_ENABLED | RCAR_DMACHCR_IE;\n\t\t \n\t\telse if (desc->async_tx.callback)\n\t\t\tchcr |= RCAR_DMACHCR_DPM_INFINITE | RCAR_DMACHCR_DSIE;\n\t\t \n\t\telse\n\t\t\tchcr |= RCAR_DMACHCR_DPM_INFINITE;\n\t} else {\n\t\tstruct rcar_dmac_xfer_chunk *chunk = desc->running;\n\n\t\tdev_dbg(chan->chan.device->dev,\n\t\t\t\"chan%u: queue chunk %p: %u@%pad -> %pad\\n\",\n\t\t\tchan->index, chunk, chunk->size, &chunk->src_addr,\n\t\t\t&chunk->dst_addr);\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\trcar_dmac_chan_write(chan, RCAR_DMAFIXSAR,\n\t\t\t\t     chunk->src_addr >> 32);\n\t\trcar_dmac_chan_write(chan, RCAR_DMAFIXDAR,\n\t\t\t\t     chunk->dst_addr >> 32);\n#endif\n\t\trcar_dmac_chan_write(chan, RCAR_DMASAR,\n\t\t\t\t     chunk->src_addr & 0xffffffff);\n\t\trcar_dmac_chan_write(chan, RCAR_DMADAR,\n\t\t\t\t     chunk->dst_addr & 0xffffffff);\n\t\trcar_dmac_chan_write(chan, RCAR_DMATCR,\n\t\t\t\t     chunk->size >> desc->xfer_shift);\n\n\t\tchcr |= RCAR_DMACHCR_DPM_DISABLED | RCAR_DMACHCR_IE;\n\t}\n\n\trcar_dmac_chan_write(chan, RCAR_DMACHCR,\n\t\t\t     chcr | RCAR_DMACHCR_DE | RCAR_DMACHCR_CAIE);\n}\n\nstatic int rcar_dmac_init(struct rcar_dmac *dmac)\n{\n\tu16 dmaor;\n\n\t \n\trcar_dmac_chan_clear_all(dmac);\n\trcar_dmac_write(dmac, RCAR_DMAOR,\n\t\t\tRCAR_DMAOR_PRI_FIXED | RCAR_DMAOR_DME);\n\n\tdmaor = rcar_dmac_read(dmac, RCAR_DMAOR);\n\tif ((dmaor & (RCAR_DMAOR_AE | RCAR_DMAOR_DME)) != RCAR_DMAOR_DME) {\n\t\tdev_warn(dmac->dev, \"DMAOR initialization failed.\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \n\nstatic dma_cookie_t rcar_dmac_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct rcar_dmac_chan *chan = to_rcar_dmac_chan(tx->chan);\n\tstruct rcar_dmac_desc *desc = to_rcar_dmac_desc(tx);\n\tunsigned long flags;\n\tdma_cookie_t cookie;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tcookie = dma_cookie_assign(tx);\n\n\tdev_dbg(chan->chan.device->dev, \"chan%u: submit #%d@%p\\n\",\n\t\tchan->index, tx->cookie, desc);\n\n\tlist_add_tail(&desc->node, &chan->desc.pending);\n\tdesc->running = list_first_entry(&desc->chunks,\n\t\t\t\t\t struct rcar_dmac_xfer_chunk, node);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn cookie;\n}\n\n \n\n \nstatic int rcar_dmac_desc_alloc(struct rcar_dmac_chan *chan, gfp_t gfp)\n{\n\tstruct rcar_dmac_desc_page *page;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\tunsigned int i;\n\n\tpage = (void *)get_zeroed_page(gfp);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < RCAR_DMAC_DESCS_PER_PAGE; ++i) {\n\t\tstruct rcar_dmac_desc *desc = &page->descs[i];\n\n\t\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->chan);\n\t\tdesc->async_tx.tx_submit = rcar_dmac_tx_submit;\n\t\tINIT_LIST_HEAD(&desc->chunks);\n\n\t\tlist_add_tail(&desc->node, &list);\n\t}\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_splice_tail(&list, &chan->desc.free);\n\tlist_add_tail(&page->node, &chan->desc.pages);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn 0;\n}\n\n \nstatic void rcar_dmac_desc_put(struct rcar_dmac_chan *chan,\n\t\t\t       struct rcar_dmac_desc *desc)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_splice_tail_init(&desc->chunks, &chan->desc.chunks_free);\n\tlist_add(&desc->node, &chan->desc.free);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic void rcar_dmac_desc_recycle_acked(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc, *_desc;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\t \n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_splice_init(&chan->desc.wait, &list);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tlist_for_each_entry_safe(desc, _desc, &list, node) {\n\t\tif (async_tx_test_ack(&desc->async_tx)) {\n\t\t\tlist_del(&desc->node);\n\t\t\trcar_dmac_desc_put(chan, desc);\n\t\t}\n\t}\n\n\tif (list_empty(&list))\n\t\treturn;\n\n\t \n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_splice(&list, &chan->desc.wait);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic struct rcar_dmac_desc *rcar_dmac_desc_get(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc;\n\tunsigned long flags;\n\tint ret;\n\n\t \n\trcar_dmac_desc_recycle_acked(chan);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\twhile (list_empty(&chan->desc.free)) {\n\t\t \n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\t\tret = rcar_dmac_desc_alloc(chan, GFP_NOWAIT);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\t}\n\n\tdesc = list_first_entry(&chan->desc.free, struct rcar_dmac_desc, node);\n\tlist_del(&desc->node);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn desc;\n}\n\n \nstatic int rcar_dmac_xfer_chunk_alloc(struct rcar_dmac_chan *chan, gfp_t gfp)\n{\n\tstruct rcar_dmac_desc_page *page;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\tunsigned int i;\n\n\tpage = (void *)get_zeroed_page(gfp);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < RCAR_DMAC_XFER_CHUNKS_PER_PAGE; ++i) {\n\t\tstruct rcar_dmac_xfer_chunk *chunk = &page->chunks[i];\n\n\t\tlist_add_tail(&chunk->node, &list);\n\t}\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_splice_tail(&list, &chan->desc.chunks_free);\n\tlist_add_tail(&page->node, &chan->desc.pages);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn 0;\n}\n\n \nstatic struct rcar_dmac_xfer_chunk *\nrcar_dmac_xfer_chunk_get(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_xfer_chunk *chunk;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\twhile (list_empty(&chan->desc.chunks_free)) {\n\t\t \n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\t\tret = rcar_dmac_xfer_chunk_alloc(chan, GFP_NOWAIT);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\t}\n\n\tchunk = list_first_entry(&chan->desc.chunks_free,\n\t\t\t\t struct rcar_dmac_xfer_chunk, node);\n\tlist_del(&chunk->node);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn chunk;\n}\n\nstatic void rcar_dmac_realloc_hwdesc(struct rcar_dmac_chan *chan,\n\t\t\t\t     struct rcar_dmac_desc *desc, size_t size)\n{\n\t \n\tsize = PAGE_ALIGN(size);\n\n\tif (desc->hwdescs.size == size)\n\t\treturn;\n\n\tif (desc->hwdescs.mem) {\n\t\tdma_free_coherent(chan->chan.device->dev, desc->hwdescs.size,\n\t\t\t\t  desc->hwdescs.mem, desc->hwdescs.dma);\n\t\tdesc->hwdescs.mem = NULL;\n\t\tdesc->hwdescs.size = 0;\n\t}\n\n\tif (!size)\n\t\treturn;\n\n\tdesc->hwdescs.mem = dma_alloc_coherent(chan->chan.device->dev, size,\n\t\t\t\t\t       &desc->hwdescs.dma, GFP_NOWAIT);\n\tif (!desc->hwdescs.mem)\n\t\treturn;\n\n\tdesc->hwdescs.size = size;\n}\n\nstatic int rcar_dmac_fill_hwdesc(struct rcar_dmac_chan *chan,\n\t\t\t\t struct rcar_dmac_desc *desc)\n{\n\tstruct rcar_dmac_xfer_chunk *chunk;\n\tstruct rcar_dmac_hw_desc *hwdesc;\n\n\trcar_dmac_realloc_hwdesc(chan, desc, desc->nchunks * sizeof(*hwdesc));\n\n\thwdesc = desc->hwdescs.mem;\n\tif (!hwdesc)\n\t\treturn -ENOMEM;\n\n\tlist_for_each_entry(chunk, &desc->chunks, node) {\n\t\thwdesc->sar = chunk->src_addr;\n\t\thwdesc->dar = chunk->dst_addr;\n\t\thwdesc->tcr = chunk->size >> desc->xfer_shift;\n\t\thwdesc++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void rcar_dmac_chcr_de_barrier(struct rcar_dmac_chan *chan)\n{\n\tu32 chcr;\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < 1024; i++) {\n\t\tchcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\n\t\tif (!(chcr & RCAR_DMACHCR_DE))\n\t\t\treturn;\n\t\tudelay(1);\n\t}\n\n\tdev_err(chan->chan.device->dev, \"CHCR DE check error\\n\");\n}\n\nstatic void rcar_dmac_clear_chcr_de(struct rcar_dmac_chan *chan)\n{\n\tu32 chcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\n\n\t \n\trcar_dmac_chan_write(chan, RCAR_DMACHCR, (chcr & ~RCAR_DMACHCR_DE));\n\n\t \n\trcar_dmac_chcr_de_barrier(chan);\n}\n\nstatic void rcar_dmac_chan_halt(struct rcar_dmac_chan *chan)\n{\n\tu32 chcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\n\n\tchcr &= ~(RCAR_DMACHCR_DSE | RCAR_DMACHCR_DSIE | RCAR_DMACHCR_IE |\n\t\t  RCAR_DMACHCR_TE | RCAR_DMACHCR_DE |\n\t\t  RCAR_DMACHCR_CAE | RCAR_DMACHCR_CAIE);\n\trcar_dmac_chan_write(chan, RCAR_DMACHCR, chcr);\n\trcar_dmac_chcr_de_barrier(chan);\n}\n\nstatic void rcar_dmac_chan_reinit(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc, *_desc;\n\tunsigned long flags;\n\tLIST_HEAD(descs);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\t \n\tlist_splice_init(&chan->desc.pending, &descs);\n\tlist_splice_init(&chan->desc.active, &descs);\n\tlist_splice_init(&chan->desc.done, &descs);\n\tlist_splice_init(&chan->desc.wait, &descs);\n\n\tchan->desc.running = NULL;\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tlist_for_each_entry_safe(desc, _desc, &descs, node) {\n\t\tlist_del(&desc->node);\n\t\trcar_dmac_desc_put(chan, desc);\n\t}\n}\n\nstatic void rcar_dmac_stop_all_chan(struct rcar_dmac *dmac)\n{\n\tstruct rcar_dmac_chan *chan;\n\tunsigned int i;\n\n\t \n\tfor_each_rcar_dmac_chan(i, dmac, chan) {\n\t\t \n\t\tspin_lock_irq(&chan->lock);\n\t\trcar_dmac_chan_halt(chan);\n\t\tspin_unlock_irq(&chan->lock);\n\t}\n}\n\nstatic int rcar_dmac_chan_pause(struct dma_chan *chan)\n{\n\tunsigned long flags;\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\n\tspin_lock_irqsave(&rchan->lock, flags);\n\trcar_dmac_clear_chcr_de(rchan);\n\tspin_unlock_irqrestore(&rchan->lock, flags);\n\n\treturn 0;\n}\n\n \n\nstatic void rcar_dmac_chan_configure_desc(struct rcar_dmac_chan *chan,\n\t\t\t\t\t  struct rcar_dmac_desc *desc)\n{\n\tstatic const u32 chcr_ts[] = {\n\t\tRCAR_DMACHCR_TS_1B, RCAR_DMACHCR_TS_2B,\n\t\tRCAR_DMACHCR_TS_4B, RCAR_DMACHCR_TS_8B,\n\t\tRCAR_DMACHCR_TS_16B, RCAR_DMACHCR_TS_32B,\n\t\tRCAR_DMACHCR_TS_64B,\n\t};\n\n\tunsigned int xfer_size;\n\tu32 chcr;\n\n\tswitch (desc->direction) {\n\tcase DMA_DEV_TO_MEM:\n\t\tchcr = RCAR_DMACHCR_DM_INC | RCAR_DMACHCR_SM_FIXED\n\t\t     | RCAR_DMACHCR_RS_DMARS;\n\t\txfer_size = chan->src.xfer_size;\n\t\tbreak;\n\n\tcase DMA_MEM_TO_DEV:\n\t\tchcr = RCAR_DMACHCR_DM_FIXED | RCAR_DMACHCR_SM_INC\n\t\t     | RCAR_DMACHCR_RS_DMARS;\n\t\txfer_size = chan->dst.xfer_size;\n\t\tbreak;\n\n\tcase DMA_MEM_TO_MEM:\n\tdefault:\n\t\tchcr = RCAR_DMACHCR_DM_INC | RCAR_DMACHCR_SM_INC\n\t\t     | RCAR_DMACHCR_RS_AUTO;\n\t\txfer_size = RCAR_DMAC_MEMCPY_XFER_SIZE;\n\t\tbreak;\n\t}\n\n\tdesc->xfer_shift = ilog2(xfer_size);\n\tdesc->chcr = chcr | chcr_ts[desc->xfer_shift];\n}\n\n \nstatic struct dma_async_tx_descriptor *\nrcar_dmac_chan_prep_sg(struct rcar_dmac_chan *chan, struct scatterlist *sgl,\n\t\t       unsigned int sg_len, dma_addr_t dev_addr,\n\t\t       enum dma_transfer_direction dir, unsigned long dma_flags,\n\t\t       bool cyclic)\n{\n\tstruct rcar_dmac_xfer_chunk *chunk;\n\tstruct rcar_dmac_desc *desc;\n\tstruct scatterlist *sg;\n\tunsigned int nchunks = 0;\n\tunsigned int max_chunk_size;\n\tunsigned int full_size = 0;\n\tbool cross_boundary = false;\n\tunsigned int i;\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tu32 high_dev_addr;\n\tu32 high_mem_addr;\n#endif\n\n\tdesc = rcar_dmac_desc_get(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdesc->async_tx.flags = dma_flags;\n\tdesc->async_tx.cookie = -EBUSY;\n\n\tdesc->cyclic = cyclic;\n\tdesc->direction = dir;\n\n\trcar_dmac_chan_configure_desc(chan, desc);\n\n\tmax_chunk_size = RCAR_DMATCR_MASK << desc->xfer_shift;\n\n\t \n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tdma_addr_t mem_addr = sg_dma_address(sg);\n\t\tunsigned int len = sg_dma_len(sg);\n\n\t\tfull_size += len;\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\tif (i == 0) {\n\t\t\thigh_dev_addr = dev_addr >> 32;\n\t\t\thigh_mem_addr = mem_addr >> 32;\n\t\t}\n\n\t\tif ((dev_addr >> 32 != high_dev_addr) ||\n\t\t    (mem_addr >> 32 != high_mem_addr))\n\t\t\tcross_boundary = true;\n#endif\n\t\twhile (len) {\n\t\t\tunsigned int size = min(len, max_chunk_size);\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\t\t \n\t\t\tif (dev_addr >> 32 != (dev_addr + size - 1) >> 32) {\n\t\t\t\tsize = ALIGN(dev_addr, 1ULL << 32) - dev_addr;\n\t\t\t\tcross_boundary = true;\n\t\t\t}\n\t\t\tif (mem_addr >> 32 != (mem_addr + size - 1) >> 32) {\n\t\t\t\tsize = ALIGN(mem_addr, 1ULL << 32) - mem_addr;\n\t\t\t\tcross_boundary = true;\n\t\t\t}\n#endif\n\n\t\t\tchunk = rcar_dmac_xfer_chunk_get(chan);\n\t\t\tif (!chunk) {\n\t\t\t\trcar_dmac_desc_put(chan, desc);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\tif (dir == DMA_DEV_TO_MEM) {\n\t\t\t\tchunk->src_addr = dev_addr;\n\t\t\t\tchunk->dst_addr = mem_addr;\n\t\t\t} else {\n\t\t\t\tchunk->src_addr = mem_addr;\n\t\t\t\tchunk->dst_addr = dev_addr;\n\t\t\t}\n\n\t\t\tchunk->size = size;\n\n\t\t\tdev_dbg(chan->chan.device->dev,\n\t\t\t\t\"chan%u: chunk %p/%p sgl %u@%p, %u/%u %pad -> %pad\\n\",\n\t\t\t\tchan->index, chunk, desc, i, sg, size, len,\n\t\t\t\t&chunk->src_addr, &chunk->dst_addr);\n\n\t\t\tmem_addr += size;\n\t\t\tif (dir == DMA_MEM_TO_MEM)\n\t\t\t\tdev_addr += size;\n\n\t\t\tlen -= size;\n\n\t\t\tlist_add_tail(&chunk->node, &desc->chunks);\n\t\t\tnchunks++;\n\t\t}\n\t}\n\n\tdesc->nchunks = nchunks;\n\tdesc->size = full_size;\n\n\t \n\tdesc->hwdescs.use = !cross_boundary && nchunks > 1;\n\tif (desc->hwdescs.use) {\n\t\tif (rcar_dmac_fill_hwdesc(chan, desc) < 0)\n\t\t\tdesc->hwdescs.use = false;\n\t}\n\n\treturn &desc->async_tx;\n}\n\n \n\nstatic int rcar_dmac_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tint ret;\n\n\tINIT_LIST_HEAD(&rchan->desc.chunks_free);\n\tINIT_LIST_HEAD(&rchan->desc.pages);\n\n\t \n\tret = rcar_dmac_xfer_chunk_alloc(rchan, GFP_KERNEL);\n\tif (ret < 0)\n\t\treturn -ENOMEM;\n\n\tret = rcar_dmac_desc_alloc(rchan, GFP_KERNEL);\n\tif (ret < 0)\n\t\treturn -ENOMEM;\n\n\treturn pm_runtime_get_sync(chan->device->dev);\n}\n\nstatic void rcar_dmac_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tstruct rcar_dmac *dmac = to_rcar_dmac(chan->device);\n\tstruct rcar_dmac_chan_map *map = &rchan->map;\n\tstruct rcar_dmac_desc_page *page, *_page;\n\tstruct rcar_dmac_desc *desc;\n\tLIST_HEAD(list);\n\n\t \n\tspin_lock_irq(&rchan->lock);\n\trcar_dmac_chan_halt(rchan);\n\tspin_unlock_irq(&rchan->lock);\n\n\t \n\tsynchronize_irq(rchan->irq);\n\n\tif (rchan->mid_rid >= 0) {\n\t\t \n\t\tclear_bit(rchan->mid_rid, dmac->modules);\n\t\trchan->mid_rid = -EINVAL;\n\t}\n\n\tlist_splice_init(&rchan->desc.free, &list);\n\tlist_splice_init(&rchan->desc.pending, &list);\n\tlist_splice_init(&rchan->desc.active, &list);\n\tlist_splice_init(&rchan->desc.done, &list);\n\tlist_splice_init(&rchan->desc.wait, &list);\n\n\trchan->desc.running = NULL;\n\n\tlist_for_each_entry(desc, &list, node)\n\t\trcar_dmac_realloc_hwdesc(rchan, desc, 0);\n\n\tlist_for_each_entry_safe(page, _page, &rchan->desc.pages, node) {\n\t\tlist_del(&page->node);\n\t\tfree_page((unsigned long)page);\n\t}\n\n\t \n\tif (map->slave.xfer_size) {\n\t\tdma_unmap_resource(chan->device->dev, map->addr,\n\t\t\t\t   map->slave.xfer_size, map->dir, 0);\n\t\tmap->slave.xfer_size = 0;\n\t}\n\n\tpm_runtime_put(chan->device->dev);\n}\n\nstatic struct dma_async_tx_descriptor *\nrcar_dmac_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,\n\t\t\t  dma_addr_t dma_src, size_t len, unsigned long flags)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tstruct scatterlist sgl;\n\n\tif (!len)\n\t\treturn NULL;\n\n\tsg_init_table(&sgl, 1);\n\tsg_set_page(&sgl, pfn_to_page(PFN_DOWN(dma_src)), len,\n\t\t    offset_in_page(dma_src));\n\tsg_dma_address(&sgl) = dma_src;\n\tsg_dma_len(&sgl) = len;\n\n\treturn rcar_dmac_chan_prep_sg(rchan, &sgl, 1, dma_dest,\n\t\t\t\t      DMA_MEM_TO_MEM, flags, false);\n}\n\nstatic int rcar_dmac_map_slave_addr(struct dma_chan *chan,\n\t\t\t\t    enum dma_transfer_direction dir)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tstruct rcar_dmac_chan_map *map = &rchan->map;\n\tphys_addr_t dev_addr;\n\tsize_t dev_size;\n\tenum dma_data_direction dev_dir;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_addr = rchan->src.slave_addr;\n\t\tdev_size = rchan->src.xfer_size;\n\t\tdev_dir = DMA_TO_DEVICE;\n\t} else {\n\t\tdev_addr = rchan->dst.slave_addr;\n\t\tdev_size = rchan->dst.xfer_size;\n\t\tdev_dir = DMA_FROM_DEVICE;\n\t}\n\n\t \n\tif (dev_addr == map->slave.slave_addr &&\n\t    dev_size == map->slave.xfer_size &&\n\t    dev_dir == map->dir)\n\t\treturn 0;\n\n\t \n\tif (map->slave.xfer_size)\n\t\tdma_unmap_resource(chan->device->dev, map->addr,\n\t\t\t\t   map->slave.xfer_size, map->dir, 0);\n\tmap->slave.xfer_size = 0;\n\n\t \n\tmap->addr = dma_map_resource(chan->device->dev, dev_addr, dev_size,\n\t\t\t\t     dev_dir, 0);\n\n\tif (dma_mapping_error(chan->device->dev, map->addr)) {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"chan%u: failed to map %zx@%pap\", rchan->index,\n\t\t\tdev_size, &dev_addr);\n\t\treturn -EIO;\n\t}\n\n\tdev_dbg(chan->device->dev, \"chan%u: map %zx@%pap to %pad dir: %s\\n\",\n\t\trchan->index, dev_size, &dev_addr, &map->addr,\n\t\tdev_dir == DMA_TO_DEVICE ? \"DMA_TO_DEVICE\" : \"DMA_FROM_DEVICE\");\n\n\tmap->slave.slave_addr = dev_addr;\n\tmap->slave.xfer_size = dev_size;\n\tmap->dir = dev_dir;\n\n\treturn 0;\n}\n\nstatic struct dma_async_tx_descriptor *\nrcar_dmac_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t\tunsigned int sg_len, enum dma_transfer_direction dir,\n\t\t\tunsigned long flags, void *context)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\n\t \n\tif (rchan->mid_rid < 0 || !sg_len || !sg_dma_len(sgl)) {\n\t\tdev_warn(chan->device->dev,\n\t\t\t \"%s: bad parameter: len=%d, id=%d\\n\",\n\t\t\t __func__, sg_len, rchan->mid_rid);\n\t\treturn NULL;\n\t}\n\n\tif (rcar_dmac_map_slave_addr(chan, dir))\n\t\treturn NULL;\n\n\treturn rcar_dmac_chan_prep_sg(rchan, sgl, sg_len, rchan->map.addr,\n\t\t\t\t      dir, flags, false);\n}\n\n#define RCAR_DMAC_MAX_SG_LEN\t32\n\nstatic struct dma_async_tx_descriptor *\nrcar_dmac_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr,\n\t\t\t  size_t buf_len, size_t period_len,\n\t\t\t  enum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tstruct dma_async_tx_descriptor *desc;\n\tstruct scatterlist *sgl;\n\tunsigned int sg_len;\n\tunsigned int i;\n\n\t \n\tif (rchan->mid_rid < 0 || buf_len < period_len) {\n\t\tdev_warn(chan->device->dev,\n\t\t\t\"%s: bad parameter: buf_len=%zu, period_len=%zu, id=%d\\n\",\n\t\t\t__func__, buf_len, period_len, rchan->mid_rid);\n\t\treturn NULL;\n\t}\n\n\tif (rcar_dmac_map_slave_addr(chan, dir))\n\t\treturn NULL;\n\n\tsg_len = buf_len / period_len;\n\tif (sg_len > RCAR_DMAC_MAX_SG_LEN) {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"chan%u: sg length %d exceeds limit %d\",\n\t\t\trchan->index, sg_len, RCAR_DMAC_MAX_SG_LEN);\n\t\treturn NULL;\n\t}\n\n\t \n\tsgl = kmalloc_array(sg_len, sizeof(*sgl), GFP_NOWAIT);\n\tif (!sgl)\n\t\treturn NULL;\n\n\tsg_init_table(sgl, sg_len);\n\n\tfor (i = 0; i < sg_len; ++i) {\n\t\tdma_addr_t src = buf_addr + (period_len * i);\n\n\t\tsg_set_page(&sgl[i], pfn_to_page(PFN_DOWN(src)), period_len,\n\t\t\t    offset_in_page(src));\n\t\tsg_dma_address(&sgl[i]) = src;\n\t\tsg_dma_len(&sgl[i]) = period_len;\n\t}\n\n\tdesc = rcar_dmac_chan_prep_sg(rchan, sgl, sg_len, rchan->map.addr,\n\t\t\t\t      dir, flags, true);\n\n\tkfree(sgl);\n\treturn desc;\n}\n\nstatic int rcar_dmac_device_config(struct dma_chan *chan,\n\t\t\t\t   struct dma_slave_config *cfg)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\n\t \n\trchan->src.slave_addr = cfg->src_addr;\n\trchan->dst.slave_addr = cfg->dst_addr;\n\trchan->src.xfer_size = cfg->src_addr_width;\n\trchan->dst.xfer_size = cfg->dst_addr_width;\n\n\treturn 0;\n}\n\nstatic int rcar_dmac_chan_terminate_all(struct dma_chan *chan)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rchan->lock, flags);\n\trcar_dmac_chan_halt(rchan);\n\tspin_unlock_irqrestore(&rchan->lock, flags);\n\n\t \n\n\trcar_dmac_chan_reinit(rchan);\n\n\treturn 0;\n}\n\nstatic unsigned int rcar_dmac_chan_get_residue(struct rcar_dmac_chan *chan,\n\t\t\t\t\t       dma_cookie_t cookie)\n{\n\tstruct rcar_dmac_desc *desc = chan->desc.running;\n\tstruct rcar_dmac_xfer_chunk *running = NULL;\n\tstruct rcar_dmac_xfer_chunk *chunk;\n\tenum dma_status status;\n\tunsigned int residue = 0;\n\tunsigned int dptr = 0;\n\tunsigned int chcrb;\n\tunsigned int tcrb;\n\tunsigned int i;\n\n\tif (!desc)\n\t\treturn 0;\n\n\t \n\tstatus = dma_cookie_status(&chan->chan, cookie, NULL);\n\tif (status == DMA_COMPLETE)\n\t\treturn 0;\n\n\t \n\tif (cookie != desc->async_tx.cookie) {\n\t\tlist_for_each_entry(desc, &chan->desc.done, node) {\n\t\t\tif (cookie == desc->async_tx.cookie)\n\t\t\t\treturn 0;\n\t\t}\n\t\tlist_for_each_entry(desc, &chan->desc.pending, node) {\n\t\t\tif (cookie == desc->async_tx.cookie)\n\t\t\t\treturn desc->size;\n\t\t}\n\t\tlist_for_each_entry(desc, &chan->desc.active, node) {\n\t\t\tif (cookie == desc->async_tx.cookie)\n\t\t\t\treturn desc->size;\n\t\t}\n\n\t\t \n\t\tWARN(1, \"No descriptor for cookie!\");\n\t\treturn 0;\n\t}\n\n\t \n\tfor (i = 0; i < 3; i++) {\n\t\tchcrb = rcar_dmac_chan_read(chan, RCAR_DMACHCRB) &\n\t\t\t\t\t    RCAR_DMACHCRB_DPTR_MASK;\n\t\ttcrb = rcar_dmac_chan_read(chan, RCAR_DMATCRB);\n\t\t \n\t\tif (chcrb == (rcar_dmac_chan_read(chan, RCAR_DMACHCRB) &\n\t\t\t      RCAR_DMACHCRB_DPTR_MASK))\n\t\t\tbreak;\n\t}\n\tWARN_ONCE(i >= 3, \"residue might be not continuous!\");\n\n\t \n\tif (desc->hwdescs.use) {\n\t\tdptr = chcrb >> RCAR_DMACHCRB_DPTR_SHIFT;\n\t\tif (dptr == 0)\n\t\t\tdptr = desc->nchunks;\n\t\tdptr--;\n\t\tWARN_ON(dptr >= desc->nchunks);\n\t} else {\n\t\trunning = desc->running;\n\t}\n\n\t \n\tlist_for_each_entry_reverse(chunk, &desc->chunks, node) {\n\t\tif (chunk == running || ++dptr == desc->nchunks)\n\t\t\tbreak;\n\n\t\tresidue += chunk->size;\n\t}\n\n\t \n\tresidue += tcrb << desc->xfer_shift;\n\n\treturn residue;\n}\n\nstatic enum dma_status rcar_dmac_tx_status(struct dma_chan *chan,\n\t\t\t\t\t   dma_cookie_t cookie,\n\t\t\t\t\t   struct dma_tx_state *txstate)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tenum dma_status status;\n\tunsigned long flags;\n\tunsigned int residue;\n\tbool cyclic;\n\n\tstatus = dma_cookie_status(chan, cookie, txstate);\n\tif (status == DMA_COMPLETE || !txstate)\n\t\treturn status;\n\n\tspin_lock_irqsave(&rchan->lock, flags);\n\tresidue = rcar_dmac_chan_get_residue(rchan, cookie);\n\tcyclic = rchan->desc.running ? rchan->desc.running->cyclic : false;\n\tspin_unlock_irqrestore(&rchan->lock, flags);\n\n\t \n\tif (!residue && !cyclic)\n\t\treturn DMA_COMPLETE;\n\n\tdma_set_residue(txstate, residue);\n\n\treturn status;\n}\n\nstatic void rcar_dmac_issue_pending(struct dma_chan *chan)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rchan->lock, flags);\n\n\tif (list_empty(&rchan->desc.pending))\n\t\tgoto done;\n\n\t \n\tlist_splice_tail_init(&rchan->desc.pending, &rchan->desc.active);\n\n\t \n\tif (!rchan->desc.running) {\n\t\tstruct rcar_dmac_desc *desc;\n\n\t\tdesc = list_first_entry(&rchan->desc.active,\n\t\t\t\t\tstruct rcar_dmac_desc, node);\n\t\trchan->desc.running = desc;\n\n\t\trcar_dmac_chan_start_xfer(rchan);\n\t}\n\ndone:\n\tspin_unlock_irqrestore(&rchan->lock, flags);\n}\n\nstatic void rcar_dmac_device_synchronize(struct dma_chan *chan)\n{\n\tstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\n\n\tsynchronize_irq(rchan->irq);\n}\n\n \n\nstatic irqreturn_t rcar_dmac_isr_desc_stage_end(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc = chan->desc.running;\n\tunsigned int stage;\n\n\tif (WARN_ON(!desc || !desc->cyclic)) {\n\t\t \n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tstage = (rcar_dmac_chan_read(chan, RCAR_DMACHCRB) &\n\t\t RCAR_DMACHCRB_DPTR_MASK) >> RCAR_DMACHCRB_DPTR_SHIFT;\n\trcar_dmac_chan_write(chan, RCAR_DMADPCR, RCAR_DMADPCR_DIPT(stage));\n\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic irqreturn_t rcar_dmac_isr_transfer_end(struct rcar_dmac_chan *chan)\n{\n\tstruct rcar_dmac_desc *desc = chan->desc.running;\n\tirqreturn_t ret = IRQ_WAKE_THREAD;\n\n\tif (WARN_ON_ONCE(!desc)) {\n\t\t \n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tif (!desc->hwdescs.use) {\n\t\t \n\t\tif (!list_is_last(&desc->running->node, &desc->chunks)) {\n\t\t\tdesc->running = list_next_entry(desc->running, node);\n\t\t\tif (!desc->cyclic)\n\t\t\t\tret = IRQ_HANDLED;\n\t\t\tgoto done;\n\t\t}\n\n\t\t \n\t\tif (desc->cyclic) {\n\t\t\tdesc->running =\n\t\t\t\tlist_first_entry(&desc->chunks,\n\t\t\t\t\t\t struct rcar_dmac_xfer_chunk,\n\t\t\t\t\t\t node);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tlist_move_tail(&desc->node, &chan->desc.done);\n\n\t \n\tif (!list_empty(&chan->desc.active))\n\t\tchan->desc.running = list_first_entry(&chan->desc.active,\n\t\t\t\t\t\t      struct rcar_dmac_desc,\n\t\t\t\t\t\t      node);\n\telse\n\t\tchan->desc.running = NULL;\n\ndone:\n\tif (chan->desc.running)\n\t\trcar_dmac_chan_start_xfer(chan);\n\n\treturn ret;\n}\n\nstatic irqreturn_t rcar_dmac_isr_channel(int irq, void *dev)\n{\n\tu32 mask = RCAR_DMACHCR_DSE | RCAR_DMACHCR_TE;\n\tstruct rcar_dmac_chan *chan = dev;\n\tirqreturn_t ret = IRQ_NONE;\n\tbool reinit = false;\n\tu32 chcr;\n\n\tspin_lock(&chan->lock);\n\n\tchcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\n\tif (chcr & RCAR_DMACHCR_CAE) {\n\t\tstruct rcar_dmac *dmac = to_rcar_dmac(chan->chan.device);\n\n\t\t \n\t\trcar_dmac_chan_clear(dmac, chan);\n\t\trcar_dmac_chcr_de_barrier(chan);\n\t\treinit = true;\n\t\tgoto spin_lock_end;\n\t}\n\n\tif (chcr & RCAR_DMACHCR_TE)\n\t\tmask |= RCAR_DMACHCR_DE;\n\trcar_dmac_chan_write(chan, RCAR_DMACHCR, chcr & ~mask);\n\tif (mask & RCAR_DMACHCR_DE)\n\t\trcar_dmac_chcr_de_barrier(chan);\n\n\tif (chcr & RCAR_DMACHCR_DSE)\n\t\tret |= rcar_dmac_isr_desc_stage_end(chan);\n\n\tif (chcr & RCAR_DMACHCR_TE)\n\t\tret |= rcar_dmac_isr_transfer_end(chan);\n\nspin_lock_end:\n\tspin_unlock(&chan->lock);\n\n\tif (reinit) {\n\t\tdev_err(chan->chan.device->dev, \"Channel Address Error\\n\");\n\n\t\trcar_dmac_chan_reinit(chan);\n\t\tret = IRQ_HANDLED;\n\t}\n\n\treturn ret;\n}\n\nstatic irqreturn_t rcar_dmac_isr_channel_thread(int irq, void *dev)\n{\n\tstruct rcar_dmac_chan *chan = dev;\n\tstruct rcar_dmac_desc *desc;\n\tstruct dmaengine_desc_callback cb;\n\n\tspin_lock_irq(&chan->lock);\n\n\t \n\tif (chan->desc.running && chan->desc.running->cyclic) {\n\t\tdesc = chan->desc.running;\n\t\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\n\t\tif (dmaengine_desc_callback_valid(&cb)) {\n\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\t\t\tspin_lock_irq(&chan->lock);\n\t\t}\n\t}\n\n\t \n\twhile (!list_empty(&chan->desc.done)) {\n\t\tdesc = list_first_entry(&chan->desc.done, struct rcar_dmac_desc,\n\t\t\t\t\tnode);\n\t\tdma_cookie_complete(&desc->async_tx);\n\t\tlist_del(&desc->node);\n\n\t\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\t\tif (dmaengine_desc_callback_valid(&cb)) {\n\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\t \n\t\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\t\t\tspin_lock_irq(&chan->lock);\n\t\t}\n\n\t\tlist_add_tail(&desc->node, &chan->desc.wait);\n\t}\n\n\tspin_unlock_irq(&chan->lock);\n\n\t \n\trcar_dmac_desc_recycle_acked(chan);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic bool rcar_dmac_chan_filter(struct dma_chan *chan, void *arg)\n{\n\tstruct rcar_dmac *dmac = to_rcar_dmac(chan->device);\n\tstruct of_phandle_args *dma_spec = arg;\n\n\t \n\tif (chan->device->device_config != rcar_dmac_device_config)\n\t\treturn false;\n\n\treturn !test_and_set_bit(dma_spec->args[0], dmac->modules);\n}\n\nstatic struct dma_chan *rcar_dmac_of_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t   struct of_dma *ofdma)\n{\n\tstruct rcar_dmac_chan *rchan;\n\tstruct dma_chan *chan;\n\tdma_cap_mask_t mask;\n\n\tif (dma_spec->args_count != 1)\n\t\treturn NULL;\n\n\t \n\tdma_cap_zero(mask);\n\tdma_cap_set(DMA_SLAVE, mask);\n\n\tchan = __dma_request_channel(&mask, rcar_dmac_chan_filter, dma_spec,\n\t\t\t\t     ofdma->of_node);\n\tif (!chan)\n\t\treturn NULL;\n\n\trchan = to_rcar_dmac_chan(chan);\n\trchan->mid_rid = dma_spec->args[0];\n\n\treturn chan;\n}\n\n \n\n#ifdef CONFIG_PM\nstatic int rcar_dmac_runtime_suspend(struct device *dev)\n{\n\treturn 0;\n}\n\nstatic int rcar_dmac_runtime_resume(struct device *dev)\n{\n\tstruct rcar_dmac *dmac = dev_get_drvdata(dev);\n\n\treturn rcar_dmac_init(dmac);\n}\n#endif\n\nstatic const struct dev_pm_ops rcar_dmac_pm = {\n\t \n\tSET_NOIRQ_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,\n\t\t\t\t      pm_runtime_force_resume)\n\tSET_RUNTIME_PM_OPS(rcar_dmac_runtime_suspend, rcar_dmac_runtime_resume,\n\t\t\t   NULL)\n};\n\n \n\nstatic int rcar_dmac_chan_probe(struct rcar_dmac *dmac,\n\t\t\t\tstruct rcar_dmac_chan *rchan)\n{\n\tstruct platform_device *pdev = to_platform_device(dmac->dev);\n\tstruct dma_chan *chan = &rchan->chan;\n\tchar pdev_irqname[5];\n\tchar *irqname;\n\tint ret;\n\n\trchan->mid_rid = -EINVAL;\n\n\tspin_lock_init(&rchan->lock);\n\n\tINIT_LIST_HEAD(&rchan->desc.free);\n\tINIT_LIST_HEAD(&rchan->desc.pending);\n\tINIT_LIST_HEAD(&rchan->desc.active);\n\tINIT_LIST_HEAD(&rchan->desc.done);\n\tINIT_LIST_HEAD(&rchan->desc.wait);\n\n\t \n\tsprintf(pdev_irqname, \"ch%u\", rchan->index);\n\trchan->irq = platform_get_irq_byname(pdev, pdev_irqname);\n\tif (rchan->irq < 0)\n\t\treturn -ENODEV;\n\n\tirqname = devm_kasprintf(dmac->dev, GFP_KERNEL, \"%s:%u\",\n\t\t\t\t dev_name(dmac->dev), rchan->index);\n\tif (!irqname)\n\t\treturn -ENOMEM;\n\n\t \n\tchan->device = &dmac->engine;\n\tdma_cookie_init(chan);\n\n\tlist_add_tail(&chan->device_node, &dmac->engine.channels);\n\n\tret = devm_request_threaded_irq(dmac->dev, rchan->irq,\n\t\t\t\t\trcar_dmac_isr_channel,\n\t\t\t\t\trcar_dmac_isr_channel_thread, 0,\n\t\t\t\t\tirqname, rchan);\n\tif (ret) {\n\t\tdev_err(dmac->dev, \"failed to request IRQ %u (%d)\\n\",\n\t\t\trchan->irq, ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n#define RCAR_DMAC_MAX_CHANNELS\t32\n\nstatic int rcar_dmac_parse_of(struct device *dev, struct rcar_dmac *dmac)\n{\n\tstruct device_node *np = dev->of_node;\n\tint ret;\n\n\tret = of_property_read_u32(np, \"dma-channels\", &dmac->n_channels);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"unable to read dma-channels property\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tif (dmac->n_channels <= 0 ||\n\t    dmac->n_channels >= RCAR_DMAC_MAX_CHANNELS) {\n\t\tdev_err(dev, \"invalid number of channels %u\\n\",\n\t\t\tdmac->n_channels);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tdmac->channels_mask = GENMASK(dmac->n_channels - 1, 0);\n\tof_property_read_u32(np, \"dma-channel-mask\", &dmac->channels_mask);\n\n\t \n\tdmac->channels_mask &= GENMASK(dmac->n_channels - 1, 0);\n\n\treturn 0;\n}\n\nstatic int rcar_dmac_probe(struct platform_device *pdev)\n{\n\tconst enum dma_slave_buswidth widths = DMA_SLAVE_BUSWIDTH_1_BYTE |\n\t\tDMA_SLAVE_BUSWIDTH_2_BYTES | DMA_SLAVE_BUSWIDTH_4_BYTES |\n\t\tDMA_SLAVE_BUSWIDTH_8_BYTES | DMA_SLAVE_BUSWIDTH_16_BYTES |\n\t\tDMA_SLAVE_BUSWIDTH_32_BYTES | DMA_SLAVE_BUSWIDTH_64_BYTES;\n\tconst struct rcar_dmac_of_data *data;\n\tstruct rcar_dmac_chan *chan;\n\tstruct dma_device *engine;\n\tvoid __iomem *chan_base;\n\tstruct rcar_dmac *dmac;\n\tunsigned int i;\n\tint ret;\n\n\tdata = of_device_get_match_data(&pdev->dev);\n\tif (!data)\n\t\treturn -EINVAL;\n\n\tdmac = devm_kzalloc(&pdev->dev, sizeof(*dmac), GFP_KERNEL);\n\tif (!dmac)\n\t\treturn -ENOMEM;\n\n\tdmac->dev = &pdev->dev;\n\tplatform_set_drvdata(pdev, dmac);\n\tret = dma_set_max_seg_size(dmac->dev, RCAR_DMATCR_MASK);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dma_set_mask_and_coherent(dmac->dev, DMA_BIT_MASK(40));\n\tif (ret)\n\t\treturn ret;\n\n\tret = rcar_dmac_parse_of(&pdev->dev, dmac);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (device_iommu_mapped(&pdev->dev))\n\t\tdmac->channels_mask &= ~BIT(0);\n\n\tdmac->channels = devm_kcalloc(&pdev->dev, dmac->n_channels,\n\t\t\t\t      sizeof(*dmac->channels), GFP_KERNEL);\n\tif (!dmac->channels)\n\t\treturn -ENOMEM;\n\n\t \n\tdmac->dmac_base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(dmac->dmac_base))\n\t\treturn PTR_ERR(dmac->dmac_base);\n\n\tif (!data->chan_offset_base) {\n\t\tdmac->chan_base = devm_platform_ioremap_resource(pdev, 1);\n\t\tif (IS_ERR(dmac->chan_base))\n\t\t\treturn PTR_ERR(dmac->chan_base);\n\n\t\tchan_base = dmac->chan_base;\n\t} else {\n\t\tchan_base = dmac->dmac_base + data->chan_offset_base;\n\t}\n\n\tfor_each_rcar_dmac_chan(i, dmac, chan) {\n\t\tchan->index = i;\n\t\tchan->iomem = chan_base + i * data->chan_offset_stride;\n\t}\n\n\t \n\tpm_runtime_enable(&pdev->dev);\n\tret = pm_runtime_resume_and_get(&pdev->dev);\n\tif (ret < 0) {\n\t\tdev_err(&pdev->dev, \"runtime PM get sync failed (%d)\\n\", ret);\n\t\tgoto err_pm_disable;\n\t}\n\n\tret = rcar_dmac_init(dmac);\n\tpm_runtime_put(&pdev->dev);\n\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to reset device\\n\");\n\t\tgoto err_pm_disable;\n\t}\n\n\t \n\tengine = &dmac->engine;\n\n\tdma_cap_set(DMA_MEMCPY, engine->cap_mask);\n\tdma_cap_set(DMA_SLAVE, engine->cap_mask);\n\n\tengine->dev\t\t= &pdev->dev;\n\tengine->copy_align\t= ilog2(RCAR_DMAC_MEMCPY_XFER_SIZE);\n\n\tengine->src_addr_widths\t= widths;\n\tengine->dst_addr_widths\t= widths;\n\tengine->directions\t= BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\n\tengine->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\n\tengine->device_alloc_chan_resources\t= rcar_dmac_alloc_chan_resources;\n\tengine->device_free_chan_resources\t= rcar_dmac_free_chan_resources;\n\tengine->device_prep_dma_memcpy\t\t= rcar_dmac_prep_dma_memcpy;\n\tengine->device_prep_slave_sg\t\t= rcar_dmac_prep_slave_sg;\n\tengine->device_prep_dma_cyclic\t\t= rcar_dmac_prep_dma_cyclic;\n\tengine->device_config\t\t\t= rcar_dmac_device_config;\n\tengine->device_pause\t\t\t= rcar_dmac_chan_pause;\n\tengine->device_terminate_all\t\t= rcar_dmac_chan_terminate_all;\n\tengine->device_tx_status\t\t= rcar_dmac_tx_status;\n\tengine->device_issue_pending\t\t= rcar_dmac_issue_pending;\n\tengine->device_synchronize\t\t= rcar_dmac_device_synchronize;\n\n\tINIT_LIST_HEAD(&engine->channels);\n\n\tfor_each_rcar_dmac_chan(i, dmac, chan) {\n\t\tret = rcar_dmac_chan_probe(dmac, chan);\n\t\tif (ret < 0)\n\t\t\tgoto err_pm_disable;\n\t}\n\n\t \n\tret = of_dma_controller_register(pdev->dev.of_node, rcar_dmac_of_xlate,\n\t\t\t\t\t NULL);\n\tif (ret < 0)\n\t\tgoto err_pm_disable;\n\n\t \n\tret = dma_async_device_register(engine);\n\tif (ret < 0)\n\t\tgoto err_dma_free;\n\n\treturn 0;\n\nerr_dma_free:\n\tof_dma_controller_free(pdev->dev.of_node);\nerr_pm_disable:\n\tpm_runtime_disable(&pdev->dev);\n\treturn ret;\n}\n\nstatic int rcar_dmac_remove(struct platform_device *pdev)\n{\n\tstruct rcar_dmac *dmac = platform_get_drvdata(pdev);\n\n\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&dmac->engine);\n\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}\n\nstatic void rcar_dmac_shutdown(struct platform_device *pdev)\n{\n\tstruct rcar_dmac *dmac = platform_get_drvdata(pdev);\n\n\trcar_dmac_stop_all_chan(dmac);\n}\n\nstatic const struct rcar_dmac_of_data rcar_dmac_data = {\n\t.chan_offset_base\t= 0x8000,\n\t.chan_offset_stride\t= 0x80,\n};\n\nstatic const struct rcar_dmac_of_data rcar_gen4_dmac_data = {\n\t.chan_offset_base\t= 0x0,\n\t.chan_offset_stride\t= 0x1000,\n};\n\nstatic const struct of_device_id rcar_dmac_of_ids[] = {\n\t{\n\t\t.compatible = \"renesas,rcar-dmac\",\n\t\t.data = &rcar_dmac_data,\n\t}, {\n\t\t.compatible = \"renesas,rcar-gen4-dmac\",\n\t\t.data = &rcar_gen4_dmac_data,\n\t}, {\n\t\t.compatible = \"renesas,dmac-r8a779a0\",\n\t\t.data = &rcar_gen4_dmac_data,\n\t},\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, rcar_dmac_of_ids);\n\nstatic struct platform_driver rcar_dmac_driver = {\n\t.driver\t\t= {\n\t\t.pm\t= &rcar_dmac_pm,\n\t\t.name\t= \"rcar-dmac\",\n\t\t.of_match_table = rcar_dmac_of_ids,\n\t},\n\t.probe\t\t= rcar_dmac_probe,\n\t.remove\t\t= rcar_dmac_remove,\n\t.shutdown\t= rcar_dmac_shutdown,\n};\n\nmodule_platform_driver(rcar_dmac_driver);\n\nMODULE_DESCRIPTION(\"R-Car Gen2 DMA Controller Driver\");\nMODULE_AUTHOR(\"Laurent Pinchart <laurent.pinchart@ideasonboard.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}