{
  "module_name": "shdma-base.c",
  "hash_id": "44367b58071d2e207e93923806e932fc3f53c06d05a26bf5f0a83f7b5787a463",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/sh/shdma-base.c",
  "human_readable_source": "\n \n\n#include <linux/delay.h>\n#include <linux/shdma-base.h>\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n#include \"../dmaengine.h\"\n\n \nenum shdma_desc_status {\n\tDESC_IDLE,\n\tDESC_PREPARED,\n\tDESC_SUBMITTED,\n\tDESC_COMPLETED,\t \n\tDESC_WAITING,\t \n};\n\n#define NR_DESCS_PER_CHANNEL 32\n\n#define to_shdma_chan(c) container_of(c, struct shdma_chan, dma_chan)\n#define to_shdma_dev(d) container_of(d, struct shdma_dev, dma_dev)\n\n \nstatic unsigned int slave_num = 256;\nmodule_param(slave_num, uint, 0444);\n\n \nstatic unsigned long *shdma_slave_used;\n\n \nstatic void shdma_chan_xfer_ld_queue(struct shdma_chan *schan)\n{\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tstruct shdma_desc *sdesc;\n\n\t \n\tif (ops->channel_busy(schan))\n\t\treturn;\n\n\t \n\tlist_for_each_entry(sdesc, &schan->ld_queue, node)\n\t\tif (sdesc->mark == DESC_SUBMITTED) {\n\t\t\tops->start_xfer(schan, sdesc);\n\t\t\tbreak;\n\t\t}\n}\n\nstatic dma_cookie_t shdma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct shdma_desc *chunk, *c, *desc =\n\t\tcontainer_of(tx, struct shdma_desc, async_tx);\n\tstruct shdma_chan *schan = to_shdma_chan(tx->chan);\n\tdma_async_tx_callback callback = tx->callback;\n\tdma_cookie_t cookie;\n\tbool power_up;\n\n\tspin_lock_irq(&schan->chan_lock);\n\n\tpower_up = list_empty(&schan->ld_queue);\n\n\tcookie = dma_cookie_assign(tx);\n\n\t \n\tlist_for_each_entry_safe(chunk, c, desc->node.prev, node) {\n\t\t \n\t\tif (chunk != desc && (chunk->mark == DESC_IDLE ||\n\t\t\t\t      chunk->async_tx.cookie > 0 ||\n\t\t\t\t      chunk->async_tx.cookie == -EBUSY ||\n\t\t\t\t      &chunk->node == &schan->ld_free))\n\t\t\tbreak;\n\t\tchunk->mark = DESC_SUBMITTED;\n\t\tif (chunk->chunks == 1) {\n\t\t\tchunk->async_tx.callback = callback;\n\t\t\tchunk->async_tx.callback_param = tx->callback_param;\n\t\t} else {\n\t\t\t \n\t\t\tchunk->async_tx.callback = NULL;\n\t\t}\n\t\tchunk->cookie = cookie;\n\t\tlist_move_tail(&chunk->node, &schan->ld_queue);\n\n\t\tdev_dbg(schan->dev, \"submit #%d@%p on %d\\n\",\n\t\t\ttx->cookie, &chunk->async_tx, schan->id);\n\t}\n\n\tif (power_up) {\n\t\tint ret;\n\t\tschan->pm_state = SHDMA_PM_BUSY;\n\n\t\tret = pm_runtime_get(schan->dev);\n\n\t\tspin_unlock_irq(&schan->chan_lock);\n\t\tif (ret < 0)\n\t\t\tdev_err(schan->dev, \"%s(): GET = %d\\n\", __func__, ret);\n\n\t\tpm_runtime_barrier(schan->dev);\n\n\t\tspin_lock_irq(&schan->chan_lock);\n\n\t\t \n\t\tif (schan->pm_state != SHDMA_PM_ESTABLISHED) {\n\t\t\tstruct shdma_dev *sdev =\n\t\t\t\tto_shdma_dev(schan->dma_chan.device);\n\t\t\tconst struct shdma_ops *ops = sdev->ops;\n\t\t\tdev_dbg(schan->dev, \"Bring up channel %d\\n\",\n\t\t\t\tschan->id);\n\t\t\t \n\t\t\tops->setup_xfer(schan, schan->slave_id);\n\n\t\t\tif (schan->pm_state == SHDMA_PM_PENDING)\n\t\t\t\tshdma_chan_xfer_ld_queue(schan);\n\t\t\tschan->pm_state = SHDMA_PM_ESTABLISHED;\n\t\t}\n\t} else {\n\t\t \n\t\tschan->pm_state = SHDMA_PM_PENDING;\n\t}\n\n\tspin_unlock_irq(&schan->chan_lock);\n\n\treturn cookie;\n}\n\n \nstatic struct shdma_desc *shdma_get_desc(struct shdma_chan *schan)\n{\n\tstruct shdma_desc *sdesc;\n\n\tlist_for_each_entry(sdesc, &schan->ld_free, node)\n\t\tif (sdesc->mark != DESC_PREPARED) {\n\t\t\tBUG_ON(sdesc->mark != DESC_IDLE);\n\t\t\tlist_del(&sdesc->node);\n\t\t\treturn sdesc;\n\t\t}\n\n\treturn NULL;\n}\n\nstatic int shdma_setup_slave(struct shdma_chan *schan, dma_addr_t slave_addr)\n{\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tint ret, match;\n\n\tif (schan->dev->of_node) {\n\t\tmatch = schan->hw_req;\n\t\tret = ops->set_slave(schan, match, slave_addr, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t} else {\n\t\tmatch = schan->real_slave_id;\n\t}\n\n\tif (schan->real_slave_id < 0 || schan->real_slave_id >= slave_num)\n\t\treturn -EINVAL;\n\n\tif (test_and_set_bit(schan->real_slave_id, shdma_slave_used))\n\t\treturn -EBUSY;\n\n\tret = ops->set_slave(schan, match, slave_addr, false);\n\tif (ret < 0) {\n\t\tclear_bit(schan->real_slave_id, shdma_slave_used);\n\t\treturn ret;\n\t}\n\n\tschan->slave_id = schan->real_slave_id;\n\n\treturn 0;\n}\n\nstatic int shdma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tstruct shdma_desc *desc;\n\tstruct shdma_slave *slave = chan->private;\n\tint ret, i;\n\n\t \n\tif (slave) {\n\t\t \n\t\tschan->real_slave_id = slave->slave_id;\n\t\tret = shdma_setup_slave(schan, 0);\n\t\tif (ret < 0)\n\t\t\tgoto esetslave;\n\t} else {\n\t\t \n\t\tschan->slave_id = -EINVAL;\n\t}\n\n\tschan->desc = kcalloc(NR_DESCS_PER_CHANNEL,\n\t\t\t      sdev->desc_size, GFP_KERNEL);\n\tif (!schan->desc) {\n\t\tret = -ENOMEM;\n\t\tgoto edescalloc;\n\t}\n\tschan->desc_num = NR_DESCS_PER_CHANNEL;\n\n\tfor (i = 0; i < NR_DESCS_PER_CHANNEL; i++) {\n\t\tdesc = ops->embedded_desc(schan->desc, i);\n\t\tdma_async_tx_descriptor_init(&desc->async_tx,\n\t\t\t\t\t     &schan->dma_chan);\n\t\tdesc->async_tx.tx_submit = shdma_tx_submit;\n\t\tdesc->mark = DESC_IDLE;\n\n\t\tlist_add(&desc->node, &schan->ld_free);\n\t}\n\n\treturn NR_DESCS_PER_CHANNEL;\n\nedescalloc:\n\tif (slave)\nesetslave:\n\t\tclear_bit(slave->slave_id, shdma_slave_used);\n\tchan->private = NULL;\n\treturn ret;\n}\n\n \nbool shdma_chan_filter(struct dma_chan *chan, void *arg)\n{\n\tstruct shdma_chan *schan;\n\tstruct shdma_dev *sdev;\n\tint slave_id = (long)arg;\n\tint ret;\n\n\t \n\tif (chan->device->device_alloc_chan_resources !=\n\t    shdma_alloc_chan_resources)\n\t\treturn false;\n\n\tschan = to_shdma_chan(chan);\n\tsdev = to_shdma_dev(chan->device);\n\n\t \n\tif (schan->dev->of_node) {\n\t\tret = sdev->ops->set_slave(schan, slave_id, 0, true);\n\t\tif (ret < 0)\n\t\t\treturn false;\n\n\t\tschan->real_slave_id = schan->slave_id;\n\t\treturn true;\n\t}\n\n\tif (slave_id < 0) {\n\t\t \n\t\tdev_warn(sdev->dma_dev.dev, \"invalid slave ID passed to dma_request_slave\\n\");\n\t\treturn true;\n\t}\n\n\tif (slave_id >= slave_num)\n\t\treturn false;\n\n\tret = sdev->ops->set_slave(schan, slave_id, 0, true);\n\tif (ret < 0)\n\t\treturn false;\n\n\tschan->real_slave_id = slave_id;\n\n\treturn true;\n}\nEXPORT_SYMBOL(shdma_chan_filter);\n\nstatic dma_async_tx_callback __ld_cleanup(struct shdma_chan *schan, bool all)\n{\n\tstruct shdma_desc *desc, *_desc;\n\t \n\tbool head_acked = false;\n\tdma_cookie_t cookie = 0;\n\tdma_async_tx_callback callback = NULL;\n\tstruct dmaengine_desc_callback cb;\n\tunsigned long flags;\n\tLIST_HEAD(cyclic_list);\n\n\tmemset(&cb, 0, sizeof(cb));\n\tspin_lock_irqsave(&schan->chan_lock, flags);\n\tlist_for_each_entry_safe(desc, _desc, &schan->ld_queue, node) {\n\t\tstruct dma_async_tx_descriptor *tx = &desc->async_tx;\n\n\t\tBUG_ON(tx->cookie > 0 && tx->cookie != desc->cookie);\n\t\tBUG_ON(desc->mark != DESC_SUBMITTED &&\n\t\t       desc->mark != DESC_COMPLETED &&\n\t\t       desc->mark != DESC_WAITING);\n\n\t\t \n\t\tif (!all && desc->mark == DESC_SUBMITTED &&\n\t\t    desc->cookie != cookie)\n\t\t\tbreak;\n\n\t\tif (tx->cookie > 0)\n\t\t\tcookie = tx->cookie;\n\n\t\tif (desc->mark == DESC_COMPLETED && desc->chunks == 1) {\n\t\t\tif (schan->dma_chan.completed_cookie != desc->cookie - 1)\n\t\t\t\tdev_dbg(schan->dev,\n\t\t\t\t\t\"Completing cookie %d, expected %d\\n\",\n\t\t\t\t\tdesc->cookie,\n\t\t\t\t\tschan->dma_chan.completed_cookie + 1);\n\t\t\tschan->dma_chan.completed_cookie = desc->cookie;\n\t\t}\n\n\t\t \n\t\tif (desc->mark == DESC_COMPLETED && tx->callback) {\n\t\t\tdesc->mark = DESC_WAITING;\n\t\t\tdmaengine_desc_get_callback(tx, &cb);\n\t\t\tcallback = tx->callback;\n\t\t\tdev_dbg(schan->dev, \"descriptor #%d@%p on %d callback\\n\",\n\t\t\t\ttx->cookie, tx, schan->id);\n\t\t\tBUG_ON(desc->chunks != 1);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tx->cookie > 0 || tx->cookie == -EBUSY) {\n\t\t\tif (desc->mark == DESC_COMPLETED) {\n\t\t\t\tBUG_ON(tx->cookie < 0);\n\t\t\t\tdesc->mark = DESC_WAITING;\n\t\t\t}\n\t\t\thead_acked = async_tx_test_ack(tx);\n\t\t} else {\n\t\t\tswitch (desc->mark) {\n\t\t\tcase DESC_COMPLETED:\n\t\t\t\tdesc->mark = DESC_WAITING;\n\t\t\t\tfallthrough;\n\t\t\tcase DESC_WAITING:\n\t\t\t\tif (head_acked)\n\t\t\t\t\tasync_tx_ack(&desc->async_tx);\n\t\t\t}\n\t\t}\n\n\t\tdev_dbg(schan->dev, \"descriptor %p #%d completed.\\n\",\n\t\t\ttx, tx->cookie);\n\n\t\tif (((desc->mark == DESC_COMPLETED ||\n\t\t      desc->mark == DESC_WAITING) &&\n\t\t     async_tx_test_ack(&desc->async_tx)) || all) {\n\n\t\t\tif (all || !desc->cyclic) {\n\t\t\t\t \n\t\t\t\tdesc->mark = DESC_IDLE;\n\t\t\t\tlist_move(&desc->node, &schan->ld_free);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tdesc->mark = DESC_SUBMITTED;\n\t\t\t\tlist_move_tail(&desc->node, &cyclic_list);\n\t\t\t}\n\n\t\t\tif (list_empty(&schan->ld_queue)) {\n\t\t\t\tdev_dbg(schan->dev, \"Bring down channel %d\\n\", schan->id);\n\t\t\t\tpm_runtime_put(schan->dev);\n\t\t\t\tschan->pm_state = SHDMA_PM_ESTABLISHED;\n\t\t\t} else if (schan->pm_state == SHDMA_PM_PENDING) {\n\t\t\t\tshdma_chan_xfer_ld_queue(schan);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (all && !callback)\n\t\t \n\t\tschan->dma_chan.completed_cookie = schan->dma_chan.cookie;\n\n\tlist_splice_tail(&cyclic_list, &schan->ld_queue);\n\n\tspin_unlock_irqrestore(&schan->chan_lock, flags);\n\n\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\treturn callback;\n}\n\n \nstatic void shdma_chan_ld_cleanup(struct shdma_chan *schan, bool all)\n{\n\twhile (__ld_cleanup(schan, all))\n\t\t;\n}\n\n \nstatic void shdma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct shdma_dev *sdev = to_shdma_dev(chan->device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tLIST_HEAD(list);\n\n\t \n\tspin_lock_irq(&schan->chan_lock);\n\tops->halt_channel(schan);\n\tspin_unlock_irq(&schan->chan_lock);\n\n\t \n\n\t \n\tif (!list_empty(&schan->ld_queue))\n\t\tshdma_chan_ld_cleanup(schan, true);\n\n\tif (schan->slave_id >= 0) {\n\t\t \n\t\tclear_bit(schan->slave_id, shdma_slave_used);\n\t\tchan->private = NULL;\n\t}\n\n\tschan->real_slave_id = 0;\n\n\tspin_lock_irq(&schan->chan_lock);\n\n\tlist_splice_init(&schan->ld_free, &list);\n\tschan->desc_num = 0;\n\n\tspin_unlock_irq(&schan->chan_lock);\n\n\tkfree(schan->desc);\n}\n\n \nstatic struct shdma_desc *shdma_add_desc(struct shdma_chan *schan,\n\tunsigned long flags, dma_addr_t *dst, dma_addr_t *src, size_t *len,\n\tstruct shdma_desc **first, enum dma_transfer_direction direction)\n{\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tstruct shdma_desc *new;\n\tsize_t copy_size = *len;\n\n\tif (!copy_size)\n\t\treturn NULL;\n\n\t \n\tnew = shdma_get_desc(schan);\n\tif (!new) {\n\t\tdev_err(schan->dev, \"No free link descriptor available\\n\");\n\t\treturn NULL;\n\t}\n\n\tops->desc_setup(schan, new, *src, *dst, &copy_size);\n\n\tif (!*first) {\n\t\t \n\t\tnew->async_tx.cookie = -EBUSY;\n\t\t*first = new;\n\t} else {\n\t\t \n\t\tnew->async_tx.cookie = -EINVAL;\n\t}\n\n\tdev_dbg(schan->dev,\n\t\t\"chaining (%zu/%zu)@%pad -> %pad with %p, cookie %d\\n\",\n\t\tcopy_size, *len, src, dst, &new->async_tx,\n\t\tnew->async_tx.cookie);\n\n\tnew->mark = DESC_PREPARED;\n\tnew->async_tx.flags = flags;\n\tnew->direction = direction;\n\tnew->partial = 0;\n\n\t*len -= copy_size;\n\tif (direction == DMA_MEM_TO_MEM || direction == DMA_MEM_TO_DEV)\n\t\t*src += copy_size;\n\tif (direction == DMA_MEM_TO_MEM || direction == DMA_DEV_TO_MEM)\n\t\t*dst += copy_size;\n\n\treturn new;\n}\n\n \nstatic struct dma_async_tx_descriptor *shdma_prep_sg(struct shdma_chan *schan,\n\tstruct scatterlist *sgl, unsigned int sg_len, dma_addr_t *addr,\n\tenum dma_transfer_direction direction, unsigned long flags, bool cyclic)\n{\n\tstruct scatterlist *sg;\n\tstruct shdma_desc *first = NULL, *new = NULL  ;\n\tLIST_HEAD(tx_list);\n\tint chunks = 0;\n\tunsigned long irq_flags;\n\tint i;\n\n\tfor_each_sg(sgl, sg, sg_len, i)\n\t\tchunks += DIV_ROUND_UP(sg_dma_len(sg), schan->max_xfer_len);\n\n\t \n\tspin_lock_irqsave(&schan->chan_lock, irq_flags);\n\n\t \n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tdma_addr_t sg_addr = sg_dma_address(sg);\n\t\tsize_t len = sg_dma_len(sg);\n\n\t\tif (!len)\n\t\t\tgoto err_get_desc;\n\n\t\tdo {\n\t\t\tdev_dbg(schan->dev, \"Add SG #%d@%p[%zu], dma %pad\\n\",\n\t\t\t\ti, sg, len, &sg_addr);\n\n\t\t\tif (direction == DMA_DEV_TO_MEM)\n\t\t\t\tnew = shdma_add_desc(schan, flags,\n\t\t\t\t\t\t&sg_addr, addr, &len, &first,\n\t\t\t\t\t\tdirection);\n\t\t\telse\n\t\t\t\tnew = shdma_add_desc(schan, flags,\n\t\t\t\t\t\taddr, &sg_addr, &len, &first,\n\t\t\t\t\t\tdirection);\n\t\t\tif (!new)\n\t\t\t\tgoto err_get_desc;\n\n\t\t\tnew->cyclic = cyclic;\n\t\t\tif (cyclic)\n\t\t\t\tnew->chunks = 1;\n\t\t\telse\n\t\t\t\tnew->chunks = chunks--;\n\t\t\tlist_add_tail(&new->node, &tx_list);\n\t\t} while (len);\n\t}\n\n\tif (new != first)\n\t\tnew->async_tx.cookie = -ENOSPC;\n\n\t \n\tlist_splice_tail(&tx_list, &schan->ld_free);\n\n\tspin_unlock_irqrestore(&schan->chan_lock, irq_flags);\n\n\treturn &first->async_tx;\n\nerr_get_desc:\n\tlist_for_each_entry(new, &tx_list, node)\n\t\tnew->mark = DESC_IDLE;\n\tlist_splice(&tx_list, &schan->ld_free);\n\n\tspin_unlock_irqrestore(&schan->chan_lock, irq_flags);\n\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *shdma_prep_memcpy(\n\tstruct dma_chan *chan, dma_addr_t dma_dest, dma_addr_t dma_src,\n\tsize_t len, unsigned long flags)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct scatterlist sg;\n\n\tif (!chan || !len)\n\t\treturn NULL;\n\n\tBUG_ON(!schan->desc_num);\n\n\tsg_init_table(&sg, 1);\n\tsg_set_page(&sg, pfn_to_page(PFN_DOWN(dma_src)), len,\n\t\t    offset_in_page(dma_src));\n\tsg_dma_address(&sg) = dma_src;\n\tsg_dma_len(&sg) = len;\n\n\treturn shdma_prep_sg(schan, &sg, 1, &dma_dest, DMA_MEM_TO_MEM,\n\t\t\t     flags, false);\n}\n\nstatic struct dma_async_tx_descriptor *shdma_prep_slave_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len,\n\tenum dma_transfer_direction direction, unsigned long flags, void *context)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tint slave_id = schan->slave_id;\n\tdma_addr_t slave_addr;\n\n\tif (!chan)\n\t\treturn NULL;\n\n\tBUG_ON(!schan->desc_num);\n\n\t \n\tif (slave_id < 0 || !sg_len) {\n\t\tdev_warn(schan->dev, \"%s: bad parameter: len=%d, id=%d\\n\",\n\t\t\t __func__, sg_len, slave_id);\n\t\treturn NULL;\n\t}\n\n\tslave_addr = ops->slave_addr(schan);\n\n\treturn shdma_prep_sg(schan, sgl, sg_len, &slave_addr,\n\t\t\t     direction, flags, false);\n}\n\n#define SHDMA_MAX_SG_LEN 32\n\nstatic struct dma_async_tx_descriptor *shdma_prep_dma_cyclic(\n\tstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\tsize_t period_len, enum dma_transfer_direction direction,\n\tunsigned long flags)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\n\tstruct dma_async_tx_descriptor *desc;\n\tconst struct shdma_ops *ops = sdev->ops;\n\tunsigned int sg_len = buf_len / period_len;\n\tint slave_id = schan->slave_id;\n\tdma_addr_t slave_addr;\n\tstruct scatterlist *sgl;\n\tint i;\n\n\tif (!chan)\n\t\treturn NULL;\n\n\tBUG_ON(!schan->desc_num);\n\n\tif (sg_len > SHDMA_MAX_SG_LEN) {\n\t\tdev_err(schan->dev, \"sg length %d exceeds limit %d\",\n\t\t\t\tsg_len, SHDMA_MAX_SG_LEN);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (slave_id < 0 || (buf_len < period_len)) {\n\t\tdev_warn(schan->dev,\n\t\t\t\"%s: bad parameter: buf_len=%zu, period_len=%zu, id=%d\\n\",\n\t\t\t__func__, buf_len, period_len, slave_id);\n\t\treturn NULL;\n\t}\n\n\tslave_addr = ops->slave_addr(schan);\n\n\t \n\tsgl = kmalloc_array(sg_len, sizeof(*sgl), GFP_KERNEL);\n\tif (!sgl)\n\t\treturn NULL;\n\n\tsg_init_table(sgl, sg_len);\n\n\tfor (i = 0; i < sg_len; i++) {\n\t\tdma_addr_t src = buf_addr + (period_len * i);\n\n\t\tsg_set_page(&sgl[i], pfn_to_page(PFN_DOWN(src)), period_len,\n\t\t\t    offset_in_page(src));\n\t\tsg_dma_address(&sgl[i]) = src;\n\t\tsg_dma_len(&sgl[i]) = period_len;\n\t}\n\n\tdesc = shdma_prep_sg(schan, sgl, sg_len, &slave_addr,\n\t\t\t     direction, flags, true);\n\n\tkfree(sgl);\n\treturn desc;\n}\n\nstatic int shdma_terminate_all(struct dma_chan *chan)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tstruct shdma_dev *sdev = to_shdma_dev(chan->device);\n\tconst struct shdma_ops *ops = sdev->ops;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&schan->chan_lock, flags);\n\tops->halt_channel(schan);\n\n\tif (ops->get_partial && !list_empty(&schan->ld_queue)) {\n\t\t \n\t\tstruct shdma_desc *desc = list_first_entry(&schan->ld_queue,\n\t\t\t\t\t\t\t   struct shdma_desc, node);\n\t\tdesc->partial = ops->get_partial(schan, desc);\n\t}\n\n\tspin_unlock_irqrestore(&schan->chan_lock, flags);\n\n\tshdma_chan_ld_cleanup(schan, true);\n\n\treturn 0;\n}\n\nstatic int shdma_config(struct dma_chan *chan,\n\t\t\tstruct dma_slave_config *config)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\n\t \n\tif (!config)\n\t\treturn -EINVAL;\n\n\t \n\treturn shdma_setup_slave(schan,\n\t\t\t\t config->direction == DMA_DEV_TO_MEM ?\n\t\t\t\t config->src_addr : config->dst_addr);\n}\n\nstatic void shdma_issue_pending(struct dma_chan *chan)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\n\tspin_lock_irq(&schan->chan_lock);\n\tif (schan->pm_state == SHDMA_PM_ESTABLISHED)\n\t\tshdma_chan_xfer_ld_queue(schan);\n\telse\n\t\tschan->pm_state = SHDMA_PM_PENDING;\n\tspin_unlock_irq(&schan->chan_lock);\n}\n\nstatic enum dma_status shdma_tx_status(struct dma_chan *chan,\n\t\t\t\t\tdma_cookie_t cookie,\n\t\t\t\t\tstruct dma_tx_state *txstate)\n{\n\tstruct shdma_chan *schan = to_shdma_chan(chan);\n\tenum dma_status status;\n\tunsigned long flags;\n\n\tshdma_chan_ld_cleanup(schan, false);\n\n\tspin_lock_irqsave(&schan->chan_lock, flags);\n\n\tstatus = dma_cookie_status(chan, cookie, txstate);\n\n\t \n\tif (status != DMA_COMPLETE) {\n\t\tstruct shdma_desc *sdesc;\n\t\tstatus = DMA_ERROR;\n\t\tlist_for_each_entry(sdesc, &schan->ld_queue, node)\n\t\t\tif (sdesc->cookie == cookie) {\n\t\t\t\tstatus = DMA_IN_PROGRESS;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&schan->chan_lock, flags);\n\n\treturn status;\n}\n\n \nbool shdma_reset(struct shdma_dev *sdev)\n{\n\tconst struct shdma_ops *ops = sdev->ops;\n\tstruct shdma_chan *schan;\n\tunsigned int handled = 0;\n\tint i;\n\n\t \n\tshdma_for_each_chan(schan, sdev, i) {\n\t\tstruct shdma_desc *sdesc;\n\t\tLIST_HEAD(dl);\n\n\t\tif (!schan)\n\t\t\tcontinue;\n\n\t\tspin_lock(&schan->chan_lock);\n\n\t\t \n\t\tops->halt_channel(schan);\n\n\t\tlist_splice_init(&schan->ld_queue, &dl);\n\n\t\tif (!list_empty(&dl)) {\n\t\t\tdev_dbg(schan->dev, \"Bring down channel %d\\n\", schan->id);\n\t\t\tpm_runtime_put(schan->dev);\n\t\t}\n\t\tschan->pm_state = SHDMA_PM_ESTABLISHED;\n\n\t\tspin_unlock(&schan->chan_lock);\n\n\t\t \n\t\tlist_for_each_entry(sdesc, &dl, node) {\n\t\t\tstruct dma_async_tx_descriptor *tx = &sdesc->async_tx;\n\n\t\t\tsdesc->mark = DESC_IDLE;\n\t\t\tdmaengine_desc_get_callback_invoke(tx, NULL);\n\t\t}\n\n\t\tspin_lock(&schan->chan_lock);\n\t\tlist_splice(&dl, &schan->ld_free);\n\t\tspin_unlock(&schan->chan_lock);\n\n\t\thandled++;\n\t}\n\n\treturn !!handled;\n}\nEXPORT_SYMBOL(shdma_reset);\n\nstatic irqreturn_t chan_irq(int irq, void *dev)\n{\n\tstruct shdma_chan *schan = dev;\n\tconst struct shdma_ops *ops =\n\t\tto_shdma_dev(schan->dma_chan.device)->ops;\n\tirqreturn_t ret;\n\n\tspin_lock(&schan->chan_lock);\n\n\tret = ops->chan_irq(schan, irq) ? IRQ_WAKE_THREAD : IRQ_NONE;\n\n\tspin_unlock(&schan->chan_lock);\n\n\treturn ret;\n}\n\nstatic irqreturn_t chan_irqt(int irq, void *dev)\n{\n\tstruct shdma_chan *schan = dev;\n\tconst struct shdma_ops *ops =\n\t\tto_shdma_dev(schan->dma_chan.device)->ops;\n\tstruct shdma_desc *sdesc;\n\n\tspin_lock_irq(&schan->chan_lock);\n\tlist_for_each_entry(sdesc, &schan->ld_queue, node) {\n\t\tif (sdesc->mark == DESC_SUBMITTED &&\n\t\t    ops->desc_completed(schan, sdesc)) {\n\t\t\tdev_dbg(schan->dev, \"done #%d@%p\\n\",\n\t\t\t\tsdesc->async_tx.cookie, &sdesc->async_tx);\n\t\t\tsdesc->mark = DESC_COMPLETED;\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\tshdma_chan_xfer_ld_queue(schan);\n\tspin_unlock_irq(&schan->chan_lock);\n\n\tshdma_chan_ld_cleanup(schan, false);\n\n\treturn IRQ_HANDLED;\n}\n\nint shdma_request_irq(struct shdma_chan *schan, int irq,\n\t\t\t   unsigned long flags, const char *name)\n{\n\tint ret = devm_request_threaded_irq(schan->dev, irq, chan_irq,\n\t\t\t\t\t    chan_irqt, flags, name, schan);\n\n\tschan->irq = ret < 0 ? ret : irq;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(shdma_request_irq);\n\nvoid shdma_chan_probe(struct shdma_dev *sdev,\n\t\t\t   struct shdma_chan *schan, int id)\n{\n\tschan->pm_state = SHDMA_PM_ESTABLISHED;\n\n\t \n\tschan->dma_chan.device = &sdev->dma_dev;\n\tdma_cookie_init(&schan->dma_chan);\n\n\tschan->dev = sdev->dma_dev.dev;\n\tschan->id = id;\n\n\tif (!schan->max_xfer_len)\n\t\tschan->max_xfer_len = PAGE_SIZE;\n\n\tspin_lock_init(&schan->chan_lock);\n\n\t \n\tINIT_LIST_HEAD(&schan->ld_queue);\n\tINIT_LIST_HEAD(&schan->ld_free);\n\n\t \n\tlist_add_tail(&schan->dma_chan.device_node,\n\t\t\t&sdev->dma_dev.channels);\n\tsdev->schan[id] = schan;\n}\nEXPORT_SYMBOL(shdma_chan_probe);\n\nvoid shdma_chan_remove(struct shdma_chan *schan)\n{\n\tlist_del(&schan->dma_chan.device_node);\n}\nEXPORT_SYMBOL(shdma_chan_remove);\n\nint shdma_init(struct device *dev, struct shdma_dev *sdev,\n\t\t    int chan_num)\n{\n\tstruct dma_device *dma_dev = &sdev->dma_dev;\n\n\t \n\tif (!sdev->ops ||\n\t    !sdev->desc_size ||\n\t    !sdev->ops->embedded_desc ||\n\t    !sdev->ops->start_xfer ||\n\t    !sdev->ops->setup_xfer ||\n\t    !sdev->ops->set_slave ||\n\t    !sdev->ops->desc_setup ||\n\t    !sdev->ops->slave_addr ||\n\t    !sdev->ops->channel_busy ||\n\t    !sdev->ops->halt_channel ||\n\t    !sdev->ops->desc_completed)\n\t\treturn -EINVAL;\n\n\tsdev->schan = kcalloc(chan_num, sizeof(*sdev->schan), GFP_KERNEL);\n\tif (!sdev->schan)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\t \n\tdma_dev->device_alloc_chan_resources\n\t\t= shdma_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = shdma_free_chan_resources;\n\tdma_dev->device_prep_dma_memcpy = shdma_prep_memcpy;\n\tdma_dev->device_tx_status = shdma_tx_status;\n\tdma_dev->device_issue_pending = shdma_issue_pending;\n\n\t \n\tdma_dev->device_prep_slave_sg = shdma_prep_slave_sg;\n\tdma_dev->device_prep_dma_cyclic = shdma_prep_dma_cyclic;\n\tdma_dev->device_config = shdma_config;\n\tdma_dev->device_terminate_all = shdma_terminate_all;\n\n\tdma_dev->dev = dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(shdma_init);\n\nvoid shdma_cleanup(struct shdma_dev *sdev)\n{\n\tkfree(sdev->schan);\n}\nEXPORT_SYMBOL(shdma_cleanup);\n\nstatic int __init shdma_enter(void)\n{\n\tshdma_slave_used = bitmap_zalloc(slave_num, GFP_KERNEL);\n\tif (!shdma_slave_used)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nmodule_init(shdma_enter);\n\nstatic void __exit shdma_exit(void)\n{\n\tbitmap_free(shdma_slave_used);\n}\nmodule_exit(shdma_exit);\n\nMODULE_DESCRIPTION(\"SH-DMA driver base library\");\nMODULE_AUTHOR(\"Guennadi Liakhovetski <g.liakhovetski@gmx.de>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}