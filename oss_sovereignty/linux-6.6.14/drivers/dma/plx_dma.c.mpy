{
  "module_name": "plx_dma.c",
  "hash_id": "f048f5cc09e01892ebb1d929486af7392932029e7fbee772e69e4bc7fcb43793",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/plx_dma.c",
  "human_readable_source": "\n \n\n#include \"dmaengine.h\"\n\n#include <linux/circ_buf.h>\n#include <linux/dmaengine.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\nMODULE_DESCRIPTION(\"PLX ExpressLane PEX PCI Switch DMA Engine\");\nMODULE_VERSION(\"0.1\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Logan Gunthorpe\");\n\n#define PLX_REG_DESC_RING_ADDR\t\t\t0x214\n#define PLX_REG_DESC_RING_ADDR_HI\t\t0x218\n#define PLX_REG_DESC_RING_NEXT_ADDR\t\t0x21C\n#define PLX_REG_DESC_RING_COUNT\t\t\t0x220\n#define PLX_REG_DESC_RING_LAST_ADDR\t\t0x224\n#define PLX_REG_DESC_RING_LAST_SIZE\t\t0x228\n#define PLX_REG_PREF_LIMIT\t\t\t0x234\n#define PLX_REG_CTRL\t\t\t\t0x238\n#define PLX_REG_CTRL2\t\t\t\t0x23A\n#define PLX_REG_INTR_CTRL\t\t\t0x23C\n#define PLX_REG_INTR_STATUS\t\t\t0x23E\n\n#define PLX_REG_PREF_LIMIT_PREF_FOUR\t\t8\n\n#define PLX_REG_CTRL_GRACEFUL_PAUSE\t\tBIT(0)\n#define PLX_REG_CTRL_ABORT\t\t\tBIT(1)\n#define PLX_REG_CTRL_WRITE_BACK_EN\t\tBIT(2)\n#define PLX_REG_CTRL_START\t\t\tBIT(3)\n#define PLX_REG_CTRL_RING_STOP_MODE\t\tBIT(4)\n#define PLX_REG_CTRL_DESC_MODE_BLOCK\t\t(0 << 5)\n#define PLX_REG_CTRL_DESC_MODE_ON_CHIP\t\t(1 << 5)\n#define PLX_REG_CTRL_DESC_MODE_OFF_CHIP\t\t(2 << 5)\n#define PLX_REG_CTRL_DESC_INVALID\t\tBIT(8)\n#define PLX_REG_CTRL_GRACEFUL_PAUSE_DONE\tBIT(9)\n#define PLX_REG_CTRL_ABORT_DONE\t\t\tBIT(10)\n#define PLX_REG_CTRL_IMM_PAUSE_DONE\t\tBIT(12)\n#define PLX_REG_CTRL_IN_PROGRESS\t\tBIT(30)\n\n#define PLX_REG_CTRL_RESET_VAL\t(PLX_REG_CTRL_DESC_INVALID | \\\n\t\t\t\t PLX_REG_CTRL_GRACEFUL_PAUSE_DONE | \\\n\t\t\t\t PLX_REG_CTRL_ABORT_DONE | \\\n\t\t\t\t PLX_REG_CTRL_IMM_PAUSE_DONE)\n\n#define PLX_REG_CTRL_START_VAL\t(PLX_REG_CTRL_WRITE_BACK_EN | \\\n\t\t\t\t PLX_REG_CTRL_DESC_MODE_OFF_CHIP | \\\n\t\t\t\t PLX_REG_CTRL_START | \\\n\t\t\t\t PLX_REG_CTRL_RESET_VAL)\n\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_64B\t\t0\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_128B\t1\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_256B\t2\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_512B\t3\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_1KB\t\t4\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_2KB\t\t5\n#define PLX_REG_CTRL2_MAX_TXFR_SIZE_4B\t\t7\n\n#define PLX_REG_INTR_CRTL_ERROR_EN\t\tBIT(0)\n#define PLX_REG_INTR_CRTL_INV_DESC_EN\t\tBIT(1)\n#define PLX_REG_INTR_CRTL_ABORT_DONE_EN\t\tBIT(3)\n#define PLX_REG_INTR_CRTL_PAUSE_DONE_EN\t\tBIT(4)\n#define PLX_REG_INTR_CRTL_IMM_PAUSE_DONE_EN\tBIT(5)\n\n#define PLX_REG_INTR_STATUS_ERROR\t\tBIT(0)\n#define PLX_REG_INTR_STATUS_INV_DESC\t\tBIT(1)\n#define PLX_REG_INTR_STATUS_DESC_DONE\t\tBIT(2)\n#define PLX_REG_INTR_CRTL_ABORT_DONE\t\tBIT(3)\n\nstruct plx_dma_hw_std_desc {\n\t__le32 flags_and_size;\n\t__le16 dst_addr_hi;\n\t__le16 src_addr_hi;\n\t__le32 dst_addr_lo;\n\t__le32 src_addr_lo;\n};\n\n#define PLX_DESC_SIZE_MASK\t\t0x7ffffff\n#define PLX_DESC_FLAG_VALID\t\tBIT(31)\n#define PLX_DESC_FLAG_INT_WHEN_DONE\tBIT(30)\n\n#define PLX_DESC_WB_SUCCESS\t\tBIT(30)\n#define PLX_DESC_WB_RD_FAIL\t\tBIT(29)\n#define PLX_DESC_WB_WR_FAIL\t\tBIT(28)\n\n#define PLX_DMA_RING_COUNT\t\t2048\n\nstruct plx_dma_desc {\n\tstruct dma_async_tx_descriptor txd;\n\tstruct plx_dma_hw_std_desc *hw;\n\tu32 orig_size;\n};\n\nstruct plx_dma_dev {\n\tstruct dma_device dma_dev;\n\tstruct dma_chan dma_chan;\n\tstruct pci_dev __rcu *pdev;\n\tvoid __iomem *bar;\n\tstruct tasklet_struct desc_task;\n\n\tspinlock_t ring_lock;\n\tbool ring_active;\n\tint head;\n\tint tail;\n\tstruct plx_dma_hw_std_desc *hw_ring;\n\tdma_addr_t hw_ring_dma;\n\tstruct plx_dma_desc **desc_ring;\n};\n\nstatic struct plx_dma_dev *chan_to_plx_dma_dev(struct dma_chan *c)\n{\n\treturn container_of(c, struct plx_dma_dev, dma_chan);\n}\n\nstatic struct plx_dma_desc *to_plx_desc(struct dma_async_tx_descriptor *txd)\n{\n\treturn container_of(txd, struct plx_dma_desc, txd);\n}\n\nstatic struct plx_dma_desc *plx_dma_get_desc(struct plx_dma_dev *plxdev, int i)\n{\n\treturn plxdev->desc_ring[i & (PLX_DMA_RING_COUNT - 1)];\n}\n\nstatic void plx_dma_process_desc(struct plx_dma_dev *plxdev)\n{\n\tstruct dmaengine_result res;\n\tstruct plx_dma_desc *desc;\n\tu32 flags;\n\n\tspin_lock(&plxdev->ring_lock);\n\n\twhile (plxdev->tail != plxdev->head) {\n\t\tdesc = plx_dma_get_desc(plxdev, plxdev->tail);\n\n\t\tflags = le32_to_cpu(READ_ONCE(desc->hw->flags_and_size));\n\n\t\tif (flags & PLX_DESC_FLAG_VALID)\n\t\t\tbreak;\n\n\t\tres.residue = desc->orig_size - (flags & PLX_DESC_SIZE_MASK);\n\n\t\tif (flags & PLX_DESC_WB_SUCCESS)\n\t\t\tres.result = DMA_TRANS_NOERROR;\n\t\telse if (flags & PLX_DESC_WB_WR_FAIL)\n\t\t\tres.result = DMA_TRANS_WRITE_FAILED;\n\t\telse\n\t\t\tres.result = DMA_TRANS_READ_FAILED;\n\n\t\tdma_cookie_complete(&desc->txd);\n\t\tdma_descriptor_unmap(&desc->txd);\n\t\tdmaengine_desc_get_callback_invoke(&desc->txd, &res);\n\t\tdesc->txd.callback = NULL;\n\t\tdesc->txd.callback_result = NULL;\n\n\t\tplxdev->tail++;\n\t}\n\n\tspin_unlock(&plxdev->ring_lock);\n}\n\nstatic void plx_dma_abort_desc(struct plx_dma_dev *plxdev)\n{\n\tstruct dmaengine_result res;\n\tstruct plx_dma_desc *desc;\n\n\tplx_dma_process_desc(plxdev);\n\n\tspin_lock_bh(&plxdev->ring_lock);\n\n\twhile (plxdev->tail != plxdev->head) {\n\t\tdesc = plx_dma_get_desc(plxdev, plxdev->tail);\n\n\t\tres.residue = desc->orig_size;\n\t\tres.result = DMA_TRANS_ABORTED;\n\n\t\tdma_cookie_complete(&desc->txd);\n\t\tdma_descriptor_unmap(&desc->txd);\n\t\tdmaengine_desc_get_callback_invoke(&desc->txd, &res);\n\t\tdesc->txd.callback = NULL;\n\t\tdesc->txd.callback_result = NULL;\n\n\t\tplxdev->tail++;\n\t}\n\n\tspin_unlock_bh(&plxdev->ring_lock);\n}\n\nstatic void __plx_dma_stop(struct plx_dma_dev *plxdev)\n{\n\tunsigned long timeout = jiffies + msecs_to_jiffies(1000);\n\tu32 val;\n\n\tval = readl(plxdev->bar + PLX_REG_CTRL);\n\tif (!(val & ~PLX_REG_CTRL_GRACEFUL_PAUSE))\n\t\treturn;\n\n\twritel(PLX_REG_CTRL_RESET_VAL | PLX_REG_CTRL_GRACEFUL_PAUSE,\n\t       plxdev->bar + PLX_REG_CTRL);\n\n\twhile (!time_after(jiffies, timeout)) {\n\t\tval = readl(plxdev->bar + PLX_REG_CTRL);\n\t\tif (val & PLX_REG_CTRL_GRACEFUL_PAUSE_DONE)\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t}\n\n\tif (!(val & PLX_REG_CTRL_GRACEFUL_PAUSE_DONE))\n\t\tdev_err(plxdev->dma_dev.dev,\n\t\t\t\"Timeout waiting for graceful pause!\\n\");\n\n\twritel(PLX_REG_CTRL_RESET_VAL | PLX_REG_CTRL_GRACEFUL_PAUSE,\n\t       plxdev->bar + PLX_REG_CTRL);\n\n\twritel(0, plxdev->bar + PLX_REG_DESC_RING_COUNT);\n\twritel(0, plxdev->bar + PLX_REG_DESC_RING_ADDR);\n\twritel(0, plxdev->bar + PLX_REG_DESC_RING_ADDR_HI);\n\twritel(0, plxdev->bar + PLX_REG_DESC_RING_NEXT_ADDR);\n}\n\nstatic void plx_dma_stop(struct plx_dma_dev *plxdev)\n{\n\trcu_read_lock();\n\tif (!rcu_dereference(plxdev->pdev)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__plx_dma_stop(plxdev);\n\n\trcu_read_unlock();\n}\n\nstatic void plx_dma_desc_task(struct tasklet_struct *t)\n{\n\tstruct plx_dma_dev *plxdev = from_tasklet(plxdev, t, desc_task);\n\n\tplx_dma_process_desc(plxdev);\n}\n\nstatic struct dma_async_tx_descriptor *plx_dma_prep_memcpy(struct dma_chan *c,\n\t\tdma_addr_t dma_dst, dma_addr_t dma_src, size_t len,\n\t\tunsigned long flags)\n\t__acquires(plxdev->ring_lock)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(c);\n\tstruct plx_dma_desc *plxdesc;\n\n\tspin_lock_bh(&plxdev->ring_lock);\n\tif (!plxdev->ring_active)\n\t\tgoto err_unlock;\n\n\tif (!CIRC_SPACE(plxdev->head, plxdev->tail, PLX_DMA_RING_COUNT))\n\t\tgoto err_unlock;\n\n\tif (len > PLX_DESC_SIZE_MASK)\n\t\tgoto err_unlock;\n\n\tplxdesc = plx_dma_get_desc(plxdev, plxdev->head);\n\tplxdev->head++;\n\n\tplxdesc->hw->dst_addr_lo = cpu_to_le32(lower_32_bits(dma_dst));\n\tplxdesc->hw->dst_addr_hi = cpu_to_le16(upper_32_bits(dma_dst));\n\tplxdesc->hw->src_addr_lo = cpu_to_le32(lower_32_bits(dma_src));\n\tplxdesc->hw->src_addr_hi = cpu_to_le16(upper_32_bits(dma_src));\n\n\tplxdesc->orig_size = len;\n\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\tlen |= PLX_DESC_FLAG_INT_WHEN_DONE;\n\n\tplxdesc->hw->flags_and_size = cpu_to_le32(len);\n\tplxdesc->txd.flags = flags;\n\n\t \n\n\treturn &plxdesc->txd;\n\nerr_unlock:\n\t \n\t__acquire(plxdev->ring_lock);\n\n\tspin_unlock_bh(&plxdev->ring_lock);\n\treturn NULL;\n}\n\nstatic dma_cookie_t plx_dma_tx_submit(struct dma_async_tx_descriptor *desc)\n\t__releases(plxdev->ring_lock)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(desc->chan);\n\tstruct plx_dma_desc *plxdesc = to_plx_desc(desc);\n\tdma_cookie_t cookie;\n\n\tcookie = dma_cookie_assign(desc);\n\n\t \n\twmb();\n\n\tplxdesc->hw->flags_and_size |= cpu_to_le32(PLX_DESC_FLAG_VALID);\n\n\tspin_unlock_bh(&plxdev->ring_lock);\n\n\treturn cookie;\n}\n\nstatic enum dma_status plx_dma_tx_status(struct dma_chan *chan,\n\t\tdma_cookie_t cookie, struct dma_tx_state *txstate)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(chan);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tplx_dma_process_desc(plxdev);\n\n\treturn dma_cookie_status(chan, cookie, txstate);\n}\n\nstatic void plx_dma_issue_pending(struct dma_chan *chan)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(chan);\n\n\trcu_read_lock();\n\tif (!rcu_dereference(plxdev->pdev)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t \n\twmb();\n\n\twritew(PLX_REG_CTRL_START_VAL, plxdev->bar + PLX_REG_CTRL);\n\n\trcu_read_unlock();\n}\n\nstatic irqreturn_t plx_dma_isr(int irq, void *devid)\n{\n\tstruct plx_dma_dev *plxdev = devid;\n\tu32 status;\n\n\tstatus = readw(plxdev->bar + PLX_REG_INTR_STATUS);\n\n\tif (!status)\n\t\treturn IRQ_NONE;\n\n\tif (status & PLX_REG_INTR_STATUS_DESC_DONE && plxdev->ring_active)\n\t\ttasklet_schedule(&plxdev->desc_task);\n\n\twritew(status, plxdev->bar + PLX_REG_INTR_STATUS);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int plx_dma_alloc_desc(struct plx_dma_dev *plxdev)\n{\n\tstruct plx_dma_desc *desc;\n\tint i;\n\n\tplxdev->desc_ring = kcalloc(PLX_DMA_RING_COUNT,\n\t\t\t\t    sizeof(*plxdev->desc_ring), GFP_KERNEL);\n\tif (!plxdev->desc_ring)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < PLX_DMA_RING_COUNT; i++) {\n\t\tdesc = kzalloc(sizeof(*desc), GFP_KERNEL);\n\t\tif (!desc)\n\t\t\tgoto free_and_exit;\n\n\t\tdma_async_tx_descriptor_init(&desc->txd, &plxdev->dma_chan);\n\t\tdesc->txd.tx_submit = plx_dma_tx_submit;\n\t\tdesc->hw = &plxdev->hw_ring[i];\n\n\t\tplxdev->desc_ring[i] = desc;\n\t}\n\n\treturn 0;\n\nfree_and_exit:\n\tfor (i = 0; i < PLX_DMA_RING_COUNT; i++)\n\t\tkfree(plxdev->desc_ring[i]);\n\tkfree(plxdev->desc_ring);\n\treturn -ENOMEM;\n}\n\nstatic int plx_dma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(chan);\n\tsize_t ring_sz = PLX_DMA_RING_COUNT * sizeof(*plxdev->hw_ring);\n\tint rc;\n\n\tplxdev->head = plxdev->tail = 0;\n\tplxdev->hw_ring = dma_alloc_coherent(plxdev->dma_dev.dev, ring_sz,\n\t\t\t\t\t     &plxdev->hw_ring_dma, GFP_KERNEL);\n\tif (!plxdev->hw_ring)\n\t\treturn -ENOMEM;\n\n\trc = plx_dma_alloc_desc(plxdev);\n\tif (rc)\n\t\tgoto out_free_hw_ring;\n\n\trcu_read_lock();\n\tif (!rcu_dereference(plxdev->pdev)) {\n\t\trcu_read_unlock();\n\t\trc = -ENODEV;\n\t\tgoto out_free_hw_ring;\n\t}\n\n\twritel(PLX_REG_CTRL_RESET_VAL, plxdev->bar + PLX_REG_CTRL);\n\twritel(lower_32_bits(plxdev->hw_ring_dma),\n\t       plxdev->bar + PLX_REG_DESC_RING_ADDR);\n\twritel(upper_32_bits(plxdev->hw_ring_dma),\n\t       plxdev->bar + PLX_REG_DESC_RING_ADDR_HI);\n\twritel(lower_32_bits(plxdev->hw_ring_dma),\n\t       plxdev->bar + PLX_REG_DESC_RING_NEXT_ADDR);\n\twritel(PLX_DMA_RING_COUNT, plxdev->bar + PLX_REG_DESC_RING_COUNT);\n\twritel(PLX_REG_PREF_LIMIT_PREF_FOUR, plxdev->bar + PLX_REG_PREF_LIMIT);\n\n\tplxdev->ring_active = true;\n\n\trcu_read_unlock();\n\n\treturn PLX_DMA_RING_COUNT;\n\nout_free_hw_ring:\n\tdma_free_coherent(plxdev->dma_dev.dev, ring_sz, plxdev->hw_ring,\n\t\t\t  plxdev->hw_ring_dma);\n\treturn rc;\n}\n\nstatic void plx_dma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct plx_dma_dev *plxdev = chan_to_plx_dma_dev(chan);\n\tsize_t ring_sz = PLX_DMA_RING_COUNT * sizeof(*plxdev->hw_ring);\n\tstruct pci_dev *pdev;\n\tint irq = -1;\n\tint i;\n\n\tspin_lock_bh(&plxdev->ring_lock);\n\tplxdev->ring_active = false;\n\tspin_unlock_bh(&plxdev->ring_lock);\n\n\tplx_dma_stop(plxdev);\n\n\trcu_read_lock();\n\tpdev = rcu_dereference(plxdev->pdev);\n\tif (pdev)\n\t\tirq = pci_irq_vector(pdev, 0);\n\trcu_read_unlock();\n\n\tif (irq > 0)\n\t\tsynchronize_irq(irq);\n\n\ttasklet_kill(&plxdev->desc_task);\n\n\tplx_dma_abort_desc(plxdev);\n\n\tfor (i = 0; i < PLX_DMA_RING_COUNT; i++)\n\t\tkfree(plxdev->desc_ring[i]);\n\n\tkfree(plxdev->desc_ring);\n\tdma_free_coherent(plxdev->dma_dev.dev, ring_sz, plxdev->hw_ring,\n\t\t\t  plxdev->hw_ring_dma);\n\n}\n\nstatic void plx_dma_release(struct dma_device *dma_dev)\n{\n\tstruct plx_dma_dev *plxdev =\n\t\tcontainer_of(dma_dev, struct plx_dma_dev, dma_dev);\n\n\tput_device(dma_dev->dev);\n\tkfree(plxdev);\n}\n\nstatic int plx_dma_create(struct pci_dev *pdev)\n{\n\tstruct plx_dma_dev *plxdev;\n\tstruct dma_device *dma;\n\tstruct dma_chan *chan;\n\tint rc;\n\n\tplxdev = kzalloc(sizeof(*plxdev), GFP_KERNEL);\n\tif (!plxdev)\n\t\treturn -ENOMEM;\n\n\trc = request_irq(pci_irq_vector(pdev, 0), plx_dma_isr, 0,\n\t\t\t KBUILD_MODNAME, plxdev);\n\tif (rc)\n\t\tgoto free_plx;\n\n\tspin_lock_init(&plxdev->ring_lock);\n\ttasklet_setup(&plxdev->desc_task, plx_dma_desc_task);\n\n\tRCU_INIT_POINTER(plxdev->pdev, pdev);\n\tplxdev->bar = pcim_iomap_table(pdev)[0];\n\n\tdma = &plxdev->dma_dev;\n\tINIT_LIST_HEAD(&dma->channels);\n\tdma_cap_set(DMA_MEMCPY, dma->cap_mask);\n\tdma->copy_align = DMAENGINE_ALIGN_1_BYTE;\n\tdma->dev = get_device(&pdev->dev);\n\n\tdma->device_alloc_chan_resources = plx_dma_alloc_chan_resources;\n\tdma->device_free_chan_resources = plx_dma_free_chan_resources;\n\tdma->device_prep_dma_memcpy = plx_dma_prep_memcpy;\n\tdma->device_issue_pending = plx_dma_issue_pending;\n\tdma->device_tx_status = plx_dma_tx_status;\n\tdma->device_release = plx_dma_release;\n\n\tchan = &plxdev->dma_chan;\n\tchan->device = dma;\n\tdma_cookie_init(chan);\n\tlist_add_tail(&chan->device_node, &dma->channels);\n\n\trc = dma_async_device_register(dma);\n\tif (rc) {\n\t\tpci_err(pdev, \"Failed to register dma device: %d\\n\", rc);\n\t\tgoto put_device;\n\t}\n\n\tpci_set_drvdata(pdev, plxdev);\n\n\treturn 0;\n\nput_device:\n\tput_device(&pdev->dev);\n\tfree_irq(pci_irq_vector(pdev, 0),  plxdev);\nfree_plx:\n\tkfree(plxdev);\n\n\treturn rc;\n}\n\nstatic int plx_dma_probe(struct pci_dev *pdev,\n\t\t\t const struct pci_device_id *id)\n{\n\tint rc;\n\n\trc = pcim_enable_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(48));\n\tif (rc)\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\tif (rc)\n\t\treturn rc;\n\n\trc = pcim_iomap_regions(pdev, 1, KBUILD_MODNAME);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);\n\tif (rc <= 0)\n\t\treturn rc;\n\n\tpci_set_master(pdev);\n\n\trc = plx_dma_create(pdev);\n\tif (rc)\n\t\tgoto err_free_irq_vectors;\n\n\tpci_info(pdev, \"PLX DMA Channel Registered\\n\");\n\n\treturn 0;\n\nerr_free_irq_vectors:\n\tpci_free_irq_vectors(pdev);\n\treturn rc;\n}\n\nstatic void plx_dma_remove(struct pci_dev *pdev)\n{\n\tstruct plx_dma_dev *plxdev = pci_get_drvdata(pdev);\n\n\tfree_irq(pci_irq_vector(pdev, 0),  plxdev);\n\n\trcu_assign_pointer(plxdev->pdev, NULL);\n\tsynchronize_rcu();\n\n\tspin_lock_bh(&plxdev->ring_lock);\n\tplxdev->ring_active = false;\n\tspin_unlock_bh(&plxdev->ring_lock);\n\n\t__plx_dma_stop(plxdev);\n\tplx_dma_abort_desc(plxdev);\n\n\tplxdev->bar = NULL;\n\tdma_async_device_unregister(&plxdev->dma_dev);\n\n\tpci_free_irq_vectors(pdev);\n}\n\nstatic const struct pci_device_id plx_dma_pci_tbl[] = {\n\t{\n\t\t.vendor\t\t= PCI_VENDOR_ID_PLX,\n\t\t.device\t\t= 0x87D0,\n\t\t.subvendor\t= PCI_ANY_ID,\n\t\t.subdevice\t= PCI_ANY_ID,\n\t\t.class\t\t= PCI_CLASS_SYSTEM_OTHER << 8,\n\t\t.class_mask\t= 0xFFFFFFFF,\n\t},\n\t{0}\n};\nMODULE_DEVICE_TABLE(pci, plx_dma_pci_tbl);\n\nstatic struct pci_driver plx_dma_pci_driver = {\n\t.name           = KBUILD_MODNAME,\n\t.id_table       = plx_dma_pci_tbl,\n\t.probe          = plx_dma_probe,\n\t.remove\t\t= plx_dma_remove,\n};\nmodule_pci_driver(plx_dma_pci_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}